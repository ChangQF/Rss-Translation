<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 05 Jun 2024 15:15:10 GMT</lastBuildDate>
    <item>
      <title>[D] 与去噪模型（DDPM）相比，变分扩散模型（VDM）是否仍在使用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8sa60/d_are_variational_diffusion_models_vdm_still_used/</link>
      <description><![CDATA[如果我理解正确的话，VDM 和 DDPM 之间的主要区别在于 VDM 尝试预测每个 x_t 处的完整噪声，而 DDPM 尝试预测从 x_t-1 到 x_t 的步进噪声。我以本文中的 VDM 推导为基础：https://arxiv.org/abs/2208.11970。 VDM 还在任何地方使用吗？我发现几乎所有知名的图像生成模型都使用 DDPM。即使是试图学习单步扩散的回流方法似乎也是从训练过的 DDPM 开始的。    提交人    /u/WhatIsThis_WhereAmI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8sa60/d_are_variational_diffusion_models_vdm_still_used/</guid>
      <pubDate>Wed, 05 Jun 2024 15:09:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 深度学习的局限性：从复杂性理论的角度进行序列建模</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8s0yp/r_limits_of_deep_learning_sequence_modeling/</link>
      <description><![CDATA[论文链接：https://arxiv.org/abs/2405.16674 X 线程：https://x.com/NikolaZubic5/status/1797567892646470137    提交人    /u/NikolaZubic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8s0yp/r_limits_of_deep_learning_sequence_modeling/</guid>
      <pubDate>Wed, 05 Jun 2024 14:59:01 GMT</pubDate>
    </item>
    <item>
      <title>[P] 机器学习 / 人工智能研究的存储库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8qt8q/p_a_repository_for_mlai_research/</link>
      <description><![CDATA[因为我经常看到有趣的论文、新闻和其他资源，而且记忆力像金鱼一样好，所以我决定将它们保存在 GitHub 存储库中。 有几份新闻通讯、LinkedIn、Twitter、subreddits 提供有关 ML 的更新。对我来说，它太分散了，有时要找到我读过的文章成了一场噩梦。在过去的几年里，我试图收集各种新闻和文章（我想保存下来以后再读的）。我按周划分它们，你可以在这里找到它们（如果你认为这有用的话）： https://github.com/SalvatoreRa/ML-news-of-the-week  总的来说，我想提出一个问题：你认为数据科学家跟踪趋势的最佳来源是什么？新文章？ 在我看来，存在一种信息过载，在阅读一篇真正有趣的文章之前，我必须阅读几篇文章，在我看来，大多数文章只介绍了增量研究。尤其是今天，许多文章只是已经描述或提出的内容的小变化。例如，快速工程就是一个例子，您可以查看几个方法系列，然后查看来自 CoT 的数百种变体等等。今天，在我看来，RAG 中也发生了同样的事情：Twitter 或 LinkedIn 上的一篇文章指出一种方法是 SOTA，然后阅读这篇文章有一种似曾相识的感觉    提交人    /u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8qt8q/p_a_repository_for_mlai_research/</guid>
      <pubDate>Wed, 05 Jun 2024 14:06:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于生物组织图像合成的检索增强扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8qit5/p_retrieval_augmented_diffusion_for_biological/</link>
      <description><![CDATA[这是一个将 RAG 与 Diffusion 相结合进行生物组织图像合成的探索性项目。这是一个有趣的学习经历，我想分享它，以防其他人觉得有用。但它仍然需要改进，我将尝试整合一个经过微调的模型，而不是从头开始训练。 链接：https://github.com/lnairGT/Diffusion-with-RAG    提交人    /u/IllustriousSir_007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8qit5/p_retrieval_augmented_diffusion_for_biological/</guid>
      <pubDate>Wed, 05 Jun 2024 13:54:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] tiny-ai-client：具有视觉和工具调用功能的 LLM 微型统一客户端。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8qcpv/p_tinyaiclient_tiny_unified_client_for_llms_with/</link>
      <description><![CDATA[受到一些很棒的项目的启发，我为 LLM 编写了一个小型统一客户端，支持视觉和工具调用。目标是允许使用 LLM 构建东西，而无需 langchain 的复杂性。 我也试图让代码尽可能小巧干净。 https://github.com/piEsposito/tiny-ai-client 我希望你觉得它有用 :)    提交人    /u/lee_from_teashop   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8qcpv/p_tinyaiclient_tiny_unified_client_for_llms_with/</guid>
      <pubDate>Wed, 05 Jun 2024 13:47:01 GMT</pubDate>
    </item>
    <item>
      <title>生物技术中的人工智能。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8pxsw/ai_in_biotech_d/</link>
      <description><![CDATA[想知道在生物技术领域，AI 是营销热词还是有实际应用。 我是 ML 工程师，根本不是生物学家。我在生物技术领域发现了几种 AI 解决方案，但它们似乎都没有真正带来价值，而更像是一种趋势。我希望我错了，但找不到好的证据。 我发现的解决方案是 Material gen、Microsoft；Alpha Fold、DeepMind；EvBio。    提交人    /u/IIISergeyIII   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8pxsw/ai_in_biotech_d/</guid>
      <pubDate>Wed, 05 Jun 2024 13:28:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何定义多数投票的自定义损失函数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8peu3/d_how_to_define_custom_loss_function_for_majority/</link>
      <description><![CDATA[我有一个包含 10 个数字的序列。2,2,2,2,2,2,2,2,2,2...2 是类标签，我有 5 个类。该序列也有一个特征向量。我使用这个特征向量通过深度神经网络预测序列。然后为了预测序列的最终类标签，我使用了预测序列的大多数数字。但我需要一个近似函数来定义多数投票的损失函数来训练深度网络。怎么办？我可以使用哪些近似函数？有任何默认的 PyTorch 损失函数吗？    提交人    /u/Special_Storage5054   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8peu3/d_how_to_define_custom_loss_function_for_majority/</guid>
      <pubDate>Wed, 05 Jun 2024 13:03:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于生成推荐的万亿参数序列传感器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8o2sz/r_trillionparameter_sequential_transducers_for/</link>
      <description><![CDATA[Meta 的研究人员最近发表了一篇开创性的论文，将 ChatGPT 背后的技术与推荐系统相结合。他们表明，他们可以将这些模型扩展到 1.5 万亿个参数，并在生产 A/B 测试中将顶线指标提高了 12.4%。 我们在本文中深入探讨细节：https://www.shaped.ai/blog/is-this-the-chatgpt-moment-for-recommendation-systems    提交人    /u/skeltzyboiii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8o2sz/r_trillionparameter_sequential_transducers_for/</guid>
      <pubDate>Wed, 05 Jun 2024 11:55:46 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何理解整个软件仓库？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8l2jj/r_how_to_understand_whole_software_repository/</link>
      <description><![CDATA[      我们很高兴地介绍我们的最新论文《如何理解整个软件存储库？》，其中提出了一种名为RepoUnderstander的新型自动软件工程（ASE）方法，可指导代理全面理解使用基于蒙特卡洛树搜索的策略对整个存储库进行性能测试。与之前的 SOTA 方法 SWE-agent 相比，RepoUnderstander 在 SWE-bench Lite 基准测试中取得了 18.5% 的相对提升。 更多详情请访问： 📄 论文：https://arxiv.org/abs/2406.01422 🤖 Github：https://github.com/RepoUnderstander/RepoUnderstande https://preview.redd.it/uzy4ldtztp4d1.png?width=1928&amp;format=png&amp;auto=webp&amp;s=7fd179e1441deb70c9cc09714a10ad76e26981b4    提交人    /u/tnlin   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8l2jj/r_how_to_understand_whole_software_repository/</guid>
      <pubDate>Wed, 05 Jun 2024 08:41:59 GMT</pubDate>
    </item>
    <item>
      <title>[N] 加入我们即将举行的网络研讨会：从云到芯片：将 LLM 引入边缘设备。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8k9t0/n_join_our_upcoming_webinar_from_cloud_to_chip/</link>
      <description><![CDATA[从云到边缘的转变代表了大型语言模型 (LLM) 部署的范式转变。通过利用硬件和软件方面的进步、优化模型和解决安全问题，我们可以充分利用边缘设备上 LLM 的潜力。欢迎于 6 月 26 日下午 3 点 (GMT+2) 加入我们，探索这些突破性的发展，并从行业专家那里获得宝贵的见解。 在此处查找有关网络研讨会的更多信息并注册：https://www.embedl.com/events/webinar-from-cloud-to-chip-bringing-llms-to-edge-devices    提交人    /u/Embedl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8k9t0/n_join_our_upcoming_webinar_from_cloud_to_chip/</guid>
      <pubDate>Wed, 05 Jun 2024 07:43:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 智能 Go-Explore：大型语言模型代理的新型探索框架！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8j2pm/r_intelligent_goexplore_new_exploration_framework/</link>
      <description><![CDATA[标题：智能围棋探索：站在巨人基础模型的肩膀上 作者： Cong Lu、Shengran Hu、Jeff Clune。 代码： https://github.com/conglu1997/intelligent-go-explore 网站： https://conglu.co.uk/intelligentgoexplore/ 论文： https://arxiv.org/abs/2405.15143 摘要：Go-Explore 是一组功能强大的算法，旨在解决难以探索的问题，其原理是存档已发现的状态，并迭代返回最有希望的状态并从中进行探索。这种方法在包括 Atari 游戏和机器人控制在内的各种具有挑战性的问题中都取得了超人的表现，但需要手动设计启发式方法来指导探索，这既耗时又不可行。为了解决这个问题，我们提出了智能 Go-Explore (IGE)，它通过用巨型基础模型 (FM) 捕获的智能和内化的人类兴趣概念取代这些启发式方法，大大扩展了原始 Go-Explore 的范围。这为 IGE 提供了一种类似人类的能力，即使在启发式难以定义的复杂环境中，也能本能地识别任何新状态的有趣程度或前景（例如发现新物体、位置或行为）。此外，IGE 提供了令人兴奋的、以前不可能的机会来识别和利用无法提前预测的偶然发现。我们在一系列需要搜索和探索的语言任务上评估了 IGE。在 Game of 24 这个多步骤数学推理问题中，IGE 达到 100% 的成功率，比最佳经典图形搜索基线快 70.8%。接下来，在 BabyAI-Text 这个具有挑战性的部分可观察网格世界中，IGE 以比之前的 SOTA 少几个数量级的在线样本超越了之前的 SOTA。最后，在 TextWorld 中，我们展示了 IGE 在需要长期探索的环境中取得成功的独特能力，而之前的 SOTA FM 代理（如 Reflexion）则完全失败了。总体而言，IGE 结合了 FM 的巨大优势和强大的 Go-Explore 算法，开辟了研究的新前沿，以创建具有令人印象深刻的探索能力的更通用的代理。    提交人    /u/MolassesWeak2646   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8j2pm/r_intelligent_goexplore_new_exploration_framework/</guid>
      <pubDate>Wed, 05 Jun 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] 直接迭代反演</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8asl8/p_inversion_by_direct_iteration/</link>
      <description><![CDATA[很高兴介绍我参与的一个附带项目，该项目使用直接迭代反演从 8 位图像中去除色带。使用的数据集少于 2000 张图像，模型有 140 万个参数。我一直在测试基于扩散的模型的计算下限，INDI 确实为我提供了很好的帮助。 https://github.com/ksasso1028/indi-debanding    提交人    /u/somethingwrongwifme   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8asl8/p_inversion_by_direct_iteration/</guid>
      <pubDate>Tue, 04 Jun 2024 22:56:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 比较 Darknet/YOLO 和 YOLOv10</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8a22c/d_comparing_darknetyolo_and_yolov10/</link>
      <description><![CDATA[我最近在 YouTube 上发布了一个视频，展示了 Darknet/YOLO 和 Ultralytics/YOLOv10 之间的一些区别。 TLDR：Darknet/YOLO 仍然比最新的基于 Python 的 YOLO 框架更快、更精确。 https://www.youtube.com/watch?v=2Mq23LFv1aM 如果有人对 Darknet/YOLO 感兴趣，我曾经在 reddit 上维护过一篇充满 Darknet/YOLO 信息的帖子。我已经有一段时间没有更新它了，但是信息仍然有效：https://www.reddit.com/r/computervision/comments/yjdebt/lots_of_information_and_links_on_using_darknetyolo/    提交人    /u/StephaneCharette   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8a22c/d_comparing_darknetyolo_and_yolov10/</guid>
      <pubDate>Tue, 04 Jun 2024 22:24:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] mamba.np：Mamba 的纯 NumPy 实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/</link>
      <description><![CDATA[      mamba.np 受到一些很棒的项目的启发，我用纯 Numpy 从头实现了 Mamba。代码的目标是简单、可读、轻量，因为它可以在本地 CPU 上运行。 https://github.com/idoh/mamba.np 希望您觉得它有用 :)    提交人    /u/id0h   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/</guid>
      <pubDate>Tue, 04 Jun 2024 16:02:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>