<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 24 Jan 2024 09:14:02 GMT</lastBuildDate>
    <item>
      <title>[D] 你们都使用什么类型的服务器进行机器学习/人工智能训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ec5le/d_what_type_of_servers_are_you_all_using_for/</link>
      <description><![CDATA[我在校园数据中心工作，希望从社区获得一些见解。我们最近获得了一些预算，我们的使命宣言是扩大和扩展更多与人工智能相关的项目。我自己不是最终用户，所以如果我对实际细节的掌握有点不确定，请原谅我，但通常我们希望为想要从事机器学习项目、NLP 和 LLM、AI 的教师和学生提供服务器训练等。 我在 Google 上搜索了一下，显然我很高兴看到 Nvidia 和 AMD 推出的所有新 AI 处理器。我指的是前者的 H100 系列和 Grace Hopper，后者的 Instinct MI300。我们最近经常合作的服务器供应商技嘉（Gigabyte）拥有适用于所有这些最新芯片的服务器（如果有人好奇，那就是G593-ZD2 配备 8-GPU HGX H100 模块，H223-V10 与 GH200，G383-R80 与 MI300A 和 G593-ZX1 与 MI300X），但正如你可以想象的那样，它们花费了相当多的钱。我们正在考虑选择具有大量 L40S GPU 的产品。但令我苦恼的是，我们不能花一点钱为我们的大学购买一台闪亮的超级计算机。 所以我想知道是否有人可以分享您用于机器学习的内容以及您的想法在这些新的人工智能芯片上。谁知道呢，但也许我会找到一些灵感带到我们的下一次采购会议上。非常感谢！   由   提交 /u/manwhoholdtheworld   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ec5le/d_what_type_of_servers_are_you_all_using_for/</guid>
      <pubDate>Wed, 24 Jan 2024 08:08:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习科学家或工程师如何为大中型公司的生产系统简化和卓越运营做出贡献？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ebnyb/d_how_do_machine_learning_scientists_or_engineers/</link>
      <description><![CDATA[您能否详细说明一下这些专家用来简化复杂机器学习系统的具体策略和实践。   由   提交/u/toheyav640   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ebnyb/d_how_do_machine_learning_scientists_or_engineers/</guid>
      <pubDate>Wed, 24 Jan 2024 07:34:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 2024审稿人评分及反驳讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e8xyw/d_discussion_on_cvpr_2024_reviewer_scores_and/</link>
      <description><![CDATA[嘿 CVPR 爱好者！ 随着 CVPR 2024 审稿人的分数出来，我认为开放一下会很好有关审查过程和反驳的讨论和问题的线索。分享您的经验、见解，让我们继续讨论！ 首先，这是我的分数和置信度：  论文 1：4 (4)  论文 2：4 (3) 论文 3：2 (5)  其他人都怎么样？请随意分享您的分数、提出问题或寻求建议。让我们一起解决这个问题，充分利用反驳过程！ 🚀 #CVPR2024   由   提交/u/darkknight-6  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e8xyw/d_discussion_on_cvpr_2024_reviewer_scores_and/</guid>
      <pubDate>Wed, 24 Jan 2024 04:47:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么时候在 TPU 上训练有意义？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</link>
      <description><![CDATA[我花了几周时间将 torch 模型训练脚本移植到 PyTorch/XLA 并在 TPU v3 和 v4 上进行测试。从纯粹的训练速度和成本效率的角度来看，我将结果与 GCP 中的 a2/g2 机器上的训练进行了比较。我很惊讶移植代码有多困难，以及 TPU 上的训练有多慢且成本低效。 Dev UX 让人想起使用 TensorFlow（从最坏的意义上来说）。东西通常不能开箱即用，很难调试，因为所有东西都是编译的，而且张量是惰性的。整个事情非常不透明，不清楚发生了什么。没有您期望拥有的基本工具，例如如果不进行分析就无法检查 TPU 利用率。 更令人惊讶的是，训练速度比使用同等价格的 GPU 时慢得多。例如，与 g2-standard-96（8xL4 GPU）上的训练相比，TPU v3-8 上的训练速度大约慢 2 倍，而成本却大致相同。 TPU v4-8 价格更高，但仍然比 g2-standard-96 慢。我的模型或多或少是一个简单的密集网络，它来自推荐领域。未移植的 pytorch 代码使用 DDP。数据加载器经过高度优化并具有基准测试，我确信这不是瓶颈。 XLA 指标没有显示任何危险信号。 此时，我想知道为此投入更多精力是否有意义。非 Google 人员是否真的使用 TPU 进行大规模训练？是不是 Torch/XLA 还没有准备好迎接黄金时段，只是 TPU 最适合与 TF 或 JAX 一起使用？ TPU 是否有特定的用例？   由   提交 /u/Puzzleheaded-Stand79    reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</guid>
      <pubDate>Wed, 24 Jan 2024 04:15:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 测试基于 LLM 的应用程序很困难。你怎么处理这个问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/</link>
      <description><![CDATA[让我知道你是如何处理这个问题的。非常感谢您的评论！   由   提交 /u/Due-Function4447    reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e78xf/d_testing_llmbased_applications_is_hard_how_are/</guid>
      <pubDate>Wed, 24 Jan 2024 03:17:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我们不能使用合成数据来帮助创建用于放射图像分析训练的更清晰的数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e4yt3/d_why_cant_we_use_synthetic_data_to_help_create/</link>
      <description><![CDATA[这是否比创建合成数据来训练 LLM 更难，类似于 AMIE 在最近的论文中所做的：https://blog.research.google/2024/01/amie-research-ai-system- for-diagnostic_12.html   由   提交/u/derpgod123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e4yt3/d_why_cant_we_use_synthetic_data_to_help_create/</guid>
      <pubDate>Wed, 24 Jan 2024 01:27:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻求研究合作者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e454f/r_seeking_research_collaborators/</link>
      <description><![CDATA[大家好！我正在寻找一些对 ML/AI 研究（主要是计算机视觉）感兴趣并希望在顶级会议上发表文章的合作者。任何也在寻找合作者的人，请随时私信我，我会分享更多细节。谢谢！   由   提交 /u/Zealousideal-Song744    reddit.com/r/MachineLearning/comments/19e454f/r_seeking_research_collaborators/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e454f/r_seeking_research_collaborators/</guid>
      <pubDate>Wed, 24 Jan 2024 00:47:56 GMT</pubDate>
    </item>
    <item>
      <title>[D]天真的问题。在梯度下降中，为什么我们要将增量添加到权重中？为什么不乘以它呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e3xvk/d_naive_question_in_gradient_descent_why_are_we/</link>
      <description><![CDATA[为什么不是乘法，因为两个操作都可以改变值（尽管乘法会极大地改变它），这正是我们想要的？ new_weights = old_weights * delta   由   提交 /u/GullibleTrust5682   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e3xvk/d_naive_question_in_gradient_descent_why_are_we/</guid>
      <pubDate>Wed, 24 Jan 2024 00:38:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您关注哪些博客/YT 频道？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e17ht/d_what_blogsyt_channels_do_you_follow/</link>
      <description><![CDATA[我真的想确保我了解最新的方法和论文。我不想被它们淹没，但也许每周一次我想看看本周最重要的论文是什么。特别是在法学硕士和强化学习领域。我以前只是关注 OpenAI 和 Deepmind 来做这些事情，但我确信还有更多，自从 LLM 出现以来，强化学习并没有得到那么多的喜爱，所以我也想关注这一点。 &lt; p&gt;感谢您提前提出的建议！   由   提交/u/Intelligent_Rough_21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e17ht/d_what_blogsyt_channels_do_you_follow/</guid>
      <pubDate>Tue, 23 Jan 2024 22:36:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 2024 评论已出！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dydvm/d_cvpr_2024_reviews_are_out/</link>
      <description><![CDATA[你们都好吗？ 第一次提交，看到我的分数后会再次尝试:/   由   提交/u/V1bicycle  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dydvm/d_cvpr_2024_reviews_are_out/</guid>
      <pubDate>Tue, 23 Jan 2024 20:40:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究人员使用哪些工具在论文中创建出色的图像和流程图？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</link>
      <description><![CDATA[实际上，我想知道优秀的研究论文中的模型架构图有多酷，其中包含清晰的流程流程图和模型架构的出色可视化。目前我使用draw.io，但很好奇使用什么工具？我的意思是他们使用 Figma、Adobe 等专业工具吗？   由   提交 /u/MysticShadow427   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</guid>
      <pubDate>Tue, 23 Jan 2024 18:17:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 所有这些 AI 服务如何能够负担每月 5/10/20 美元的费用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/</link>
      <description><![CDATA[各种人工智能服务（从语音识别到 OCR 和艺术生成）如何嵌入新数据，以如此低的成本提供其功能？使用 GPT-4 API 之类的东西很快就会花费 10 美元，这对于其他模型来说也是类似的。即使在本地运行 LLaMA 2 这样的东西也会产生巨大的成本。我很好奇这些服务在运营这些大型模型时采用的经济策略来维持较低的月费。   由   提交 /u/Numerous_Bed9323   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19duab0/d_how_all_these_ai_services_can_afford_51020_subs/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:00 GMT</pubDate>
    </item>
    <item>
      <title>[N] Meta 开源了经过 450 万小时预训练的 wav2vec2 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dptmp/n_meta_opensourced_a_wav2vec2_model_pretrained_on/</link>
      <description><![CDATA[一个月前，Meta AI 发布了 W2V-Bert，这是其 Seamless 模型的构建模块之一。  它已经过 450 万小时的未标记音频数据的预训练，涵盖超过 143 种语言。  优点：  实现低资源微调 比 Whisper 更快、更轻 MIT 许可证 可以针对其他音频任务进行微调  缺点：  基于 CTC，因此适用于标准化转录 &lt; li&gt;使用前需要微调  资源：  原始仓库：https://github.com/facebookresearch/seamless_communication?tab=readme-ov-file#whats-new 变形金刚文档：https://huggingface.co/docs/transformers/main/en/model_doc/wav2vec2 -bert 蒙古语博客文章的 ASR 微调：https:// Huggingface.co/blog/fine-tune-w2v2-bert    由   提交 /u/Sufficient-Tennis189   /u/Sufficient-Tennis189  reddit.com/r/MachineLearning/comments/19dptmp/n_meta_opensourced_a_wav2vec2_model_pretrained_on/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dptmp/n_meta_opensourced_a_wav2vec2_model_pretrained_on/</guid>
      <pubDate>Tue, 23 Jan 2024 14:37:15 GMT</pubDate>
    </item>
    <item>
      <title>[N]ICLR2024的学习理论家，我感同身受！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dnolo/n_learning_theorists_of_iclr2024_i_feel_you/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dnolo/n_learning_theorists_of_iclr2024_i_feel_you/</guid>
      <pubDate>Tue, 23 Jan 2024 12:49:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>