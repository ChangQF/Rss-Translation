<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 19 Jan 2024 09:14:00 GMT</lastBuildDate>
    <item>
      <title>寻找eli5 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19af18f/looking_for_eli5_r/</link>
      <description><![CDATA[我使用机器学习模型，在解释模型发生的情况时总是遇到问题，一些关键点： &lt; p&gt;训练集（和准确性）有多重要重新训练时会发生什么什么是机器学习 我不是模型开发人员，但我一直在寻找 eli5 的来源（像我 5 岁一样解释一下）输入材料来解释这些和其他事情，有人有一些有趣的材料我可以看吗，也许有人已经做了一些我可以做的工作  &amp;# 32；由   提交 /u/create4drawing   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19af18f/looking_for_eli5_r/</guid>
      <pubDate>Fri, 19 Jan 2024 09:12:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在视觉领域，引导模型在利基任务上工作的当前 SOTA 是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/</link>
      <description><![CDATA[过去，当您需要在一些相对小众的分类/检测/分割任务上训练模型时，您会使用在ImageNet1K/COCO 并将其微调到您拥有的任何中小型数据集，这足以将您的性能提升到合理的水平。当然，您始终可以通过使用更大的 Resnet、改进超参数选择或清除专有数据集中的噪声来改进这一点。更新的架构已经发布，更新的优化器，我们现在有像 CLIP 这样的大型 VL 模型，等等。我想知道我是否错过了一个新的共识。 如果你选择回答，我将不胜感激如果您还详细说明了以下标准：  您选择的方法是否对超参数过于敏感？ / 收敛到正确的模型有多难？例如，根据我的经验（当然不是绝对的），在超参数选择方面，ResNet 比 EfficientNet 更宽容。 您的方法对少量数据的敏感程度如何？例如，我记得原来的 Transformer 在小训练集场景中非常糟糕，结果在 IN22K 上报告。 您的选择有多快和/或内存效率如何？小的利基任务往往不会证明具有 1B 参数的模型是合理的。  谢谢！   由   提交/u/anaccountforthemasse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aet3w/d_what_is_the_current_sota_for_bootstrapping/</guid>
      <pubDate>Fri, 19 Jan 2024 08:57:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] - 有人在 NLI 架构中使用 NER 掩蔽技术吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19aersk/d_anyone_using_ner_masking_techniques_in_nli/</link>
      <description><![CDATA[有人用 NER 想法构建 NLI 架构吗？ 因此，我查阅了大量有关 NER 和 NLI 的文献，并且我意识到很多底层操作基本上都是围绕操纵代币上的屏蔽策略来进行的。我正在寻找人们在这方面集思广益新架构。  我正在考虑这样的事情：  BERT 层来生成令牌。 NER掩蔽层生成标记之间的 NER 关系（起到基本原理提取和附加信号的作用） 可以利用这一切的 NLI logit 估计。&lt; /li&gt;    由   提交/u/testuser514  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19aersk/d_anyone_using_ner_masking_techniques_in_nli/</guid>
      <pubDate>Fri, 19 Jan 2024 08:54:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] Leetcode 与 ML</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ae7y0/d_leetcode_vs_ml/</link>
      <description><![CDATA[目前我陷入了这样的困境：是优先考虑 ML 还是专注于 leetcode。作为，我想加入基于产品的公司。  我目前的职位是数据科学家，但管理层让我参与了一些无用的支持项目。 FAANG 或类似公司会问 leetcode 问题还是只是 ML 相关问题？任何帮助或指导将不胜感激。   由   提交/u/abhishek_234  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ae7y0/d_leetcode_vs_ml/</guid>
      <pubDate>Fri, 19 Jan 2024 08:14:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] Facebook 关闭 ParlAI，一个对话研究框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19adrgv/d_facebook_shuts_down_parlai_a_framework_for/</link>
      <description><![CDATA[我刚刚了解到 Facebook 已经存档了 BlenderBot 背后的团队 ParlAI。该存储库于 2023 年 11 月 3 日存档，现在是只读的，此后该项目的 Twitter 帐户没有任何更新。 因此 Facebook 放弃了工程化和模块化对话系统背后的想法，并全力以赴对于LLM，我还听说其他大公司的其他模块化对话团队也在裁员。你觉得怎么样？   由   提交 /u/Comfortable_Use_5033   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19adrgv/d_facebook_shuts_down_parlai_a_framework_for/</guid>
      <pubDate>Fri, 19 Jan 2024 07:43:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何从非结构化文本中提取事件信息？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19abdeo/d_how_to_extract_event_information_from/</link>
      <description><![CDATA[您好， 我有一些类似于新闻稿的内容，我需要提取事件信息。我是寻找来自某一特定行业的活动。但新闻稿可以不包含一个、一个或多个事件详细信息，并且它们不一定与我的行业相关。 作为一个人，我会根据标题（有时是描述）决定它是否适合我的行业，然后查找详细信息（日期/时间/地点/活动名称/描述/等）。 离线/本地执行此操作的好方法是什么？我只是尝试使用 llama.cpp ，这让我一团糟（可能我做错了）。几年前，我使用 Spacy 进行 NER - 我想这基本上只是步骤 4 的一小部分.有什么东西可以“理解”吗？我的数据更好并给我带来了很好的结果？   由   提交/u/Chris8080  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19abdeo/d_how_to_extract_event_information_from/</guid>
      <pubDate>Fri, 19 Jan 2024 05:14:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformer多头注意力实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a8klj/d_transformer_multihead_attention_implementation/</link>
      <description><![CDATA[我一直在关注带注释的 Transformer&lt; /a&gt; 实现变压器架构。在多头注意力类的forward()方法中，Query、Key &amp;值与相应的投影矩阵相乘； W_q, W_k, W_v.  查询，键，值 = [ lin(x).view(nbatches, -1, self.h, self.d_k) .transpose(1, 2) for lin, x in zip(self.linears, (query, key, value)) ]  这里，lin(x) 正在被重塑为 (nbatches, -1, self.h, self.d_k) 和维度 1 &amp; 2 正在被转置，这使得维度 (nbatches, self.h, -1, self.d_k)。 我无法理解为什么他们不直接这样做lin(x).view(nbatches, self.h, -1, self.d_k)?   由   提交 /u/Melodic_Stomach_2704   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a8klj/d_transformer_multihead_attention_implementation/</guid>
      <pubDate>Fri, 19 Jan 2024 02:48:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能很强大实际上是一个正在研究的严肃而真实的领域，或者只是人们正在宣传的另一种炒作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/</link>
      <description><![CDATA[强大的人工智能。 （通用人工智能）实际上是一个严肃的研究领域，还是只是来自刚刚阅读/观看科幻小说的人的纯粹炒作？ 强人工智能/又名是强人工智能吗？ AGI实际上被一些研究人员/机构认真对待，他们认为它最终可以实现，或者它是人们一直在炒作的另一种奇特的技术蒸汽软件，但实际上，那些在该领域工作的人知道这样的想法实际上无法实现，因为严格的物理限制，或者如果发生的话，需要几个世纪才能实现？ 因为过去对于许多未来主义者来说有很多歇斯底里的感觉技术，这些技术被很多不知道蹲点的人大肆宣传，但它无法在实践中发挥作用（即 Em Drive、石墨烯、富勒烯、纳米机器人、Bussard Ramjet、聚变能源等）。   由   提交/u/Enzo-chan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a6nsd/d_is_strong_ai_actually_a_serious_and_real_field/</guid>
      <pubDate>Fri, 19 Jan 2024 01:16:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] PyTorch 2 内部结构</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a1mup/p_pytorch_2_internals/</link>
      <description><![CDATA[嗨，刚刚分享了关于 PyTorch 内部结构的幻灯片，涵盖了 Dynamo、Inductor、ExecuTorch 等最近的项目，我认为这里可能会有一些人感兴趣。 &lt;!-- SC_ON - -&gt;  由   提交 /u/perone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a1mup/p_pytorch_2_internals/</guid>
      <pubDate>Thu, 18 Jan 2024 21:37:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 你如何训练你的法学硕士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19a03ax/r_how_do_you_train_your_llms/</link>
      <description><![CDATA[大家好，我是一名高级 Python 开发人员，正在接受 LLM 培训。我的老板正在使用一个需要将问题和答案输入到其中的系统。  所有训练都是这样完成的吗？将我们所有的文本数据转换为问答对是一个主要基础。我希望我们可以给它提供大量的文本，然后对其进行预训练。但我们当前使用的解决方案并不是这样工作的。  你们如何培训法学硕士以及我应该关注什么？   由   提交 /u/ZachVorhies   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19a03ax/r_how_do_you_train_your_llms/</guid>
      <pubDate>Thu, 18 Jan 2024 20:35:44 GMT</pubDate>
    </item>
    <item>
      <title>获取标记数据的成本有多大？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/</link>
      <description><![CDATA[正在做我的主动学习硕士论文。文献中的一个关键点是，主动学习在有大量未标记数据的情况下可能很有用，并且与标记相关的成本很高，因此如果模型可以“选择”一个样本的子集是最“信息丰富”的，然后可以对它们进行标记。 但是，我有点意识到，尽管这种主动学习的东西很有趣并且我可能会继续，但我只是不&#39;不太明白公司中标签数据不可用/成本高昂的现实情况。当然，我知道当我阅读它时，会在某些特定情况下发生这种情况： NLP - 像语音识别这样的任务可能需要对音频进行标记，或者在信息提取中需要注释和语料库中的某些内容来 但是，我正在阅读的文献是 2009 年左右的调查，我想从那时起，像这样的问题就不会真正存在了。所以我想知道有多少次只有一堆未标记的数据等待标记。现在是否还有主动学习的需求？ 我认为我正在“转向”的一个领域可能是在在线“流”数据中寻找主动学习，而我想象的东西并不是这样的。尽快标记。   由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199zyfh/how_costly_is_it_to_obtain_labeled_data_d/</guid>
      <pubDate>Thu, 18 Jan 2024 20:30:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 情境感知元学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199yknn/r_contextaware_metalearning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.10971 OpenReview： https://openreview.net/forum?id=lJYAkDVnRU https： //openreview.net/forum?id=SAu298HU2I 摘要：  像 ChatGPT 这样的大型语言模型表现出了非凡的能力在推理过程中学习新概念，无需任何微调。然而，经过训练以在推理过程中检测新对象的视觉模型无法复制这种能力，而是要么表现不佳，要么需要对类似对象进行元训练和/或微调。在这项工作中，我们提出了一种元学习算法，通过在推理过程中学习新的视觉概念而无需微调来模拟大型语言模型。我们的方法利用冻结的预训练特征提取器，类似于上下文学习，将元学习重新定义为对具有已知标签的数据点和具有未知标签的测试数据点进行序列建模。在 11 个元学习基准中的 8 个上，我们的方法（无需元训练或微调）超过或匹配最先进的算法 P&gt;M&gt;F，该算法在这些基准上进行了元训练基准测试。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199yknn/r_contextaware_metalearning/</guid>
      <pubDate>Thu, 18 Jan 2024 19:34:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] EarthPT：时间序列变压器基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/</link>
      <description><![CDATA[想要分享 EarthPT 的代码版本，这是一个在零样本设置下预测未来卫星观测的模型！我是第一作者，所以请向我提出任何问题。 EarthPT 是一个 7 亿参数解码变压器基础模型，以自回归自监督方式训练，并专门针对 EO 用例开发头脑。 EarthPT 可以准确预测未来 400-2300 nm 范围内的卫星观测结果（我们发现了六个月！）。 EarthPT 学到的嵌入包含语义上有意义的信息，可用于下游任务，例如作为高度精细的动态土地利用分类。 对我来说最酷的收获是 EO 数据在理论上为我们提供了千万亿的训练标记。因此，如果我们假设 EarthPT 遵循类似于大型语言模型 (LLM) 导出的神经缩放定律，那么目前对于缩放 EarthPT 和其他类似的“大型观测模型”没有数据强加的限制。(!) 代码：https://github.com/aspiaspace/EarthPT 论文：https://arxiv.org/abs/2309.07207   由   提交/u/Smith4242   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199q0bc/r_earthpt_a_time_series_transformer_foundation/</guid>
      <pubDate>Thu, 18 Jan 2024 13:21:19 GMT</pubDate>
    </item>
    <item>
      <title>[D]本文的分区是否会导致数据泄露？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/</link>
      <description><![CDATA[我最近对 ​​这项研究。总而言之，他们使用文本嵌入和梯度提升来根据财报电话会议记录来预测 CEO 性格得分。他们分析了约 200 位首席执行官，将每位首席执行官的电话分为多个部分以增加数据点。然而，每位 CEO 都会出现在训练和验证集中，并具有不同的通话片段。在我看来，这应该会导致数据泄漏，因为该模型可能会发现个别首席执行官语言使用的特殊性，而不是底层数据生成过程的模式。您对此有何看法？   由   提交/u/Expective_Charity293  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/199l2m9/d_does_this_papers_partitioning_cause_data_leakage/</guid>
      <pubDate>Thu, 18 Jan 2024 08:03:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>