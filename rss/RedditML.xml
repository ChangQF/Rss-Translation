<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Fri, 01 Dec 2023 21:11:49 GMT</lastBuildDate>
    <item>
      <title>[P] 2023 年初，我在一个新的机器学习项目上投入了大量工作。现在我不知道该怎么办。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188m7hj/p_early_in_2023_i_put_in_a_lot_of_work_on_a_new/</link>
      <description><![CDATA[首先我想澄清这不是一个自我推销的帖子。我希望许多机器学习人员向我提出有关这个项目的问题或意见。关于我自己的一些背景。我确实使用 GPTQ 对 LLaMA 进行 4 位量化。 （https://github.com/qwopqwop200/GPTQ-for-LLaMa）。我已经深入研究人工智能多年了。 早在 2023 年初，我就创建了几个独特的系统，可以输出这样的视频 (https://www.youtube.com/watch?v=uoEd8WykzkY) 。这并不是说这是有史以来最好的视频，它主要是对其功能的技术演示。我在系统上做了一些推广，但大多数情况下只能根据我所知接触到机器人。我不知道该怎么办。我有一些创意，但想听听是否有人对这个系统感兴趣。我投入了大量的工作，希望看到它用于一些有趣的事情。 该项目的网站非常简单，就像现在一样，实际上只是一个占位符和联系页面。  在发布了一堆由它制作的视频后，我根本没有得到太多回应。我认为有些人可能会将其与垃圾邮件或其他任何内容混淆。 几点是： - 可以完全自动化，几乎不需要人工干预 - 完全高清视频 - 独特的3D透视视频系统，可从2D图像生成3D视频 - 无需互联网连接 - 无需特殊硬件，即可使用单个 3090 或更好的处理器运行。   由   提交 /u/mentosorangemint   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188m7hj/p_early_in_2023_i_put_in_a_lot_of_work_on_a_new/</guid>
      <pubDate>Fri, 01 Dec 2023 20:56:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] PCA、载荷和寻找最显着的判别特征/变量。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188k1vh/d_pca_loadings_and_finding_most_significant/</link>
      <description><![CDATA[我正在处理的数据有 137 个样本，每个样本有 34,765 个变量（每个像素本身就是一个频谱）。 我想要找出哪个变量是最重要的（IMO 中强度/幅度最大的变量必须是最重要的）。 一旦我在归一化后运行 PCA（除以参考频谱的总和），第一个两个分量解释的方差比为 93%。现在我可以从 PCA 矩阵的载荷中了解到什么，该矩阵的 10 个主分量为 (34765 x 10)。 载荷的最大绝对值应该调查吗？ 我该怎么办？   由   提交/u/AnikArnab   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188k1vh/d_pca_loadings_and_finding_most_significant/</guid>
      <pubDate>Fri, 01 Dec 2023 19:21:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器人技术的惨痛教训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</link>
      <description><![CDATA[对于这个 subreddit 中还没有读过惨痛教训的两个人，http://www.incompleteideas.net/IncIdeas/BitterLesson.html 但是，作为对机器人感知、规划和学习感兴趣的人，这一定适用吗？我不太确定，特别是在人类（非结构化）环境中的机器人的背景下，其政策涵盖的范围比工厂或仓库机器人要广泛得多。机器人必须应对现实世界的随机性和巨大差异性，其策略对于环境的变化具有稳健性。我可以想到一些想法，这些惨痛的教训可能适用，也可能不适用。  硬件限制。尽管将计算卸载到远程服务器绝对是一种选择，但机器人在与环境实时交互时可以在多大程度上依赖于此？没有可行的机器人能够存储数十亿个参数，即使只是为了推理。一段时间以来，摩尔定律的速度已经放缓。 数据。在我看来，这是一件大事。什么构成了训练机器人策略的良好训练数据？我们有足够的吗？当然，我们拥有良好的模型和足够的 CV 和语言数据，甚至丰田关于使用扩散模型进行抓取姿势的论文看起来也很有希望，但机器人政策必须将所有这些放在一起才能完成多模式任务。目前还没有用于多模式任务规划的庞大语料库，例如如何将倒一杯水等任务分解为具有多个子任务（抓杯、拾取、倒水等）的 HTN。  我之所以发这篇文章是因为我不确定。我可以看到法学硕士如何成为可以完成多项任务的通用机器人策略的基础，或者更高效的架构可以允许机器人使用更多计算。你有什么想法？   由   提交/u/n0ided_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</guid>
      <pubDate>Fri, 01 Dec 2023 19:14:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果审稿人在反驳期间保持沉默，我们是否应该联系 AC？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</link>
      <description><![CDATA[在将我们的论文提交给 ICLR2024 并收到初步评审后，我们针对审稿人提出的所有观点提供了详细的反驳。然而，自从我们反驳之后，审稿人方面就完全沉默了。尽管最初的评论非常详细且反馈积极，但没有进一步的问题、评论或任何形式的参与。 在这种情况下，是否建议联系 AC 请求他们的干预鼓励审稿人参与？ 有人遇到过类似的情况吗？你做了什么？如果有任何建议，我们将不胜感激。   由   提交 /u/jzhoubu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</guid>
      <pubDate>Fri, 01 Dec 2023 18:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 可以在Jetson Nano上合理运行的语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188hih1/discussion_language_models_that_could_run/</link>
      <description><![CDATA[我最近尝试在具有 4GB 内存和 ollama 的 NVIDA Jetson Nano 板上运行一些低 Q 编号的 Mistral 7B 模型。 它们运行速度非常慢。 是否有任何具有合理性能的语言模型可以在 Jetson Nano 等设备上运行以进行编码/分析/数据处理？   由   提交 /u/head_robotics   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188hih1/discussion_language_models_that_could_run/</guid>
      <pubDate>Fri, 01 Dec 2023 17:32:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] RETVec：弹性且高效的文本矢量化器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/</link>
      <description><![CDATA[星期五快乐， 非常高兴与大家分享 RETVec 的代码和模型，我们用于分类的新 SOTA 稳健文本标记器已可用在 Github 此处 和 NeurIPS 论文在这里。我们还通过 TFJS 为 TFLite 和网络提供本机支持。希望您会发现它对您的研究有用。如果您想尝试一下，我们有一本入门笔记本。  如果您有任何疑问，请告诉我们。 ​   由   提交/u/ebursztein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/</guid>
      <pubDate>Fri, 01 Dec 2023 16:51:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] Llama 微调速度提高 80%，内存减少 50%，精度损失 0%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</link>
      <description><![CDATA[       嘿r/MachineLearning！ 我手动导出了反向传播步骤，做了一些链式矩阵乘法优化，用 OpenAI 的 Triton 语言编写所有内核，并进行更多数学和编码技巧，以使 QLoRA 在 Unsloth 上对 Llama 的微调速度提高 5 倍：https:// github.com/unslothai/unsloth！一些亮点：  速度提高 5 倍（5 小时到 1 小时） 使用内存减少 50% 精度损失为 0% 所有本地均在 NVIDIA GPU（Tesla T4、RTX 20/30/40、Ampere、 Hopper）免费！ QLoRA / LoRA 现在训练速度提高了 80%。  在 2 个 Tesla T4 上的 Slim Orca 518K 示例上通过 DDP 的 GPU，Unsloth 在 260 小时内在所有层上训练 4 位 QLoRA VS Huggingface 的原始实现需要 1301 小时。 Slim Orca 1301 小时到 260 小时 您可能（很可能不）记得来自 Hyperlearn 的我（https://github.com/danielhanchen/hyperlearn）是我几年前推出的，旨在通过数学和编码技巧使 ML 算法速度提高 2000 倍。 我通过 https://unsloth.ai/introducing 写了一篇关于所有手动手动导出反向传播的博客文章。&lt; /p&gt; 我为 Alpaca 编写了 T4 的 Google Colab：https://colab.research。 google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing，在单个 GPU 上将 Alpaca 的速度提高 2 倍。 在 Kaggle 上通过 DDP 上的 2 个 Tesla T4：https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle，微调 LAION 的 OIG 速度快 5 倍，Slim Orca 速度快 5 倍更快。 您可以通过以下方式在本地安装 Unsloth： pip install &quot;unsloth[cu118] @ git+https://github.com/unslothai/unsloth。 git” pip install “unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git”  目前我们仅支持 Pytorch 2.1 和 Linux 发行版 - 更多安装说明请参见 https://github.com/unslothai/unsloth/blob/main/README.md 我希望：  支持除Llama 风格模型（Mistral 等） 添加 sqrt 梯度检查点以再减少 25% 的内存使用量。 还有其他技巧！  谢谢一堆！！   由   提交 /u/danielhanchen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</guid>
      <pubDate>Fri, 01 Dec 2023 16:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] Meta的新语音模型（无缝）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</link>
      <description><![CDATA[Meta Research 刚刚发布了名为 Seamless 的新语音模型：https://ai.meta.com/research/seamless-communication/ 它支持多种语言的语音和文本输入和输出。从某种意义上说，它是一系列相关语音任务的通用模型。非常有趣！   由   提交 /u/semicausal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</guid>
      <pubDate>Fri, 01 Dec 2023 16:27:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究调查：LLM 生成的代码中的错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188ezi2/r_research_survey_bugs_in_llms_generated_code/</link>
      <description><![CDATA[您好， 我们蒙特利尔理工学院的研究小组（在 Foutse Khomh 教授的指导下）正在进行一项关于“法学硕士生成代码中的错误”。我们准备了一份在线调查，需要您花费大约 10 分钟的宝贵时间来完成询问此类错误的特征。提供的代码片段将采用 Python 语言，但需要具备 Python 的基本背景才能理解。此外，您可以与您认为有资格参与的同事/学生分享此调查。 在调查中，我们不会询问您的姓名，也不会记录您的 IP 地址允许匿名。这项调查的结果将以匿名形式向公众公开。我们非常感谢您为此提供的帮助。如果您想了解有关这项研究的更多信息或有任何疑问，请随时与我们联系。 调查链接： https://forms.gle/8kZaxsNqtb3vbFaD6 感谢您的帮助。 Florian Tambon ([florian-2.tambon@polymtl.ca](mailto:florian-2.tambon@polymtl.ca&lt; /a&gt;)), Arghavan Moradidakhel ([arghavan.moradi-dakhel@polymtl.ca](mailto:arghavan.moradi-dakhel@polymtl.ca)), Amin Nikanjam ([amin .nikanjam@polymtl.ca](mailto:amin.nikanjam@polymtl.ca)), Foutse Khomh ([foutse.khomh@polymtl.ca](mailto:foutse.khomh@polymtl .ca)) SWAT 实验室，蒙特利尔理工学院，http://swat.polymtl.ca/&lt; /a&gt;   由   提交/u/aminnikanjam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188ezi2/r_research_survey_bugs_in_llms_generated_code/</guid>
      <pubDate>Fri, 01 Dec 2023 15:44:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一些作者是否认真地添加了比需要的更多的数学知识，以使论文“看起来”更具开创性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</link>
      <description><![CDATA[我最近注意到一种趋势，即作者在某些情况下添加了超出所需的形式主义（例如图表/图像就可以很好地完成工作）。  这是否是为了使论文看起来更好而添加了过多的数学知识，或者可能只是受到出版商的限制（无论论文必须坚持什么格式才能发表）？ &gt;   由   提交 /u/Inquation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</guid>
      <pubDate>Fri, 01 Dec 2023 14:29:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 选择非负矩阵分解的分量数量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d04q/d_choosing_number_of_components_for_nonnegative/</link>
      <description><![CDATA[我有几个无向加权网络的估计，即加权邻接矩阵。我使用 NMF 来识别子网/组件。我知道有多种方法可以得出要使用的组件数量，从简单的方法（例如膝图）到更复杂的方法（例如 使用随机删除的数据点进行交叉验证。我已经对我的网络集应用了多种方法，他们建议使用 5-8 个组件。我对两个子网应该是什么样子有一个先验的想法。使用 5-8 个组件似乎可以创建预期两个子网的子网。或者其中两个组件代表预期的子网络，而其余组件看起来与这些组件非常相似。如果我将组件的数量减少到两个，那么我就获得了两个预期的子网。  简单地使用两个组件是否合法，因为它们符合我的期望？ 是有合并组件的方法（例如，将子子网合并到预期的子网），合并组件是否合法？例如。找到与预期网络的相关性最大化的子网络的加性组合？ 是否有方法强制执行“更大”的网络？网络？  我很欣赏你对此的想法！   由   提交 /u/dizzledk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d04q/d_choosing_number_of_components_for_nonnegative/</guid>
      <pubDate>Fri, 01 Dec 2023 14:20:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据科学书籍推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188bind/d_data_science_book_recommendations/</link>
      <description><![CDATA[我拥有计算机科学学位以及一些数学和理论统计知识，成为一名专业编码员已超过 10 年。我并没有真正使用太多统计知识，所以我只是粗略地了解一下表面。 现在，我希望深入研究机器学习。我并不是在寻找一条容易的道路，我的主要目的不是快速找到一份工作，我这样做只是为了个人成长。在深入研究机器学习之前，我想加强我在数据科学方面的基础，甚至可能专注于数据挖掘。 有这方面的书籍推荐吗？我不需要编程书籍，因为我已经了解一些编程语言，包括 Python。我看到了 Pang-Ning Tan 等人的“数据挖掘导论”。这是一个好的起点吗？我发现的另一本书是 Gareth James 的“统计学习简介”。   由   提交 /u/lp_kalubec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188bind/d_data_science_book_recommendations/</guid>
      <pubDate>Fri, 01 Dec 2023 13:11:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 修改后的 Tsetlin Machine 在 7950X3D 上的实现性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187vrpg/p_modified_tsetlin_machine_implementation/</link>
      <description><![CDATA[      嘿。 我在过去 1.5 年里一直致力于我的宠物项目，取得了一些令人印象深刻的成果。 在 Ryzen 7950X3D CPU 上使用没有卷积的一个平坦层的 MNIST 推理性能：每秒 4600 万次预测，吞吐量：25 GB/s，准确度：98.05%。 AGI 实现了。老实说，ACI（人工智能集体智能）。 在 MNIST 性能上改进的 Tsetlin 机器   由   提交 /u/ArtemHnilov   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187vrpg/p_modified_tsetlin_machine_implementation/</guid>
      <pubDate>Thu, 30 Nov 2023 22:54:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一周后我将采访 Rich Sutton，我应该问他什么问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/</link>
      <description><![CDATA[Rich 是强化学习书籍&lt;的作者&lt; /a&gt;，最近，他与一些同事创立了 OpenMind 研究所。 ​ 面试时间为 1 周。我有 RL 背景，并且已经对问题和主题有了一些想法，但我也想在艾伯塔省 RL 泡沫之外寻找问题。技术问题是最好的，尽管我对任何事情都持开放态度。谢谢！ ​ 采访发布几周后，我将在此帖子中发布更新。   由   提交/u/ejmejm1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/187nbv8/d_im_interviewing_rich_sutton_in_a_week_what/</guid>
      <pubDate>Thu, 30 Nov 2023 17:00:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>