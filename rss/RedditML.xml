<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 21 Jan 2024 18:16:20 GMT</lastBuildDate>
    <item>
      <title>[D] 聊天法学硕士在哪些多轮对话数据集上进行了微调？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c9f2f/d_which_multiturn_conversation_datasets_are_chat/</link>
      <description><![CDATA[哪些多轮对话数据集是聊天 LLM 进行微调的，以及如何训练奖励模型以优先于对话？ &lt; /div&gt;  由   提交/u/kekkimo  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c9f2f/d_which_multiturn_conversation_datasets_are_chat/</guid>
      <pubDate>Sun, 21 Jan 2024 18:04:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 指令调优的首选微调框架？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c90vo/d_preferred_finetuning_framework_for_instruction/</link>
      <description><![CDATA[大家好， 我正在研究在小型私人数据集上微调 LLM 的不同框架。我不想追求一些奇特的东西，因为我不想开发自定义训练程序，而只是使用最先进的模型，根据我的数据进行微调。 因此，我的标准主要是易用性、Mistral 等 SOTA 模型的可用性、社区支持和性能（又名实现快速训练的最新方法）。 我正在研究不同的选项，到目前为止发现最成熟的使用 SOTA 模型快速微调 LLM 的方法（看似）是使用： - 蝾螈 - 拥抱脸部TRL 蝾螈似乎是框架加快了速度并使训练变得非常紧凑。 Hugging Face 框架似乎稍微不太用户友好，但似乎提供了更多的自定义功能。 您对每个框架有何看法，您还有其他推荐的框架吗？   由   提交 /u/Separate-Still3770    reddit.com/r/MachineLearning/comments/19c90vo/d_preferred_finetuning_framework_for_instruction/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c90vo/d_preferred_finetuning_framework_for_instruction/</guid>
      <pubDate>Sun, 21 Jan 2024 17:48:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现有的 n 维三角测量的 python 实现？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c8yde/d_existing_python_implementation_for_ndimentional/</link>
      <description><![CDATA[我有一个想要实现的项目，我发现它应该相对简单......我需要做的就是使用 n-维度三角测量。 然后我读到这不是一个简单的计算：-/  通过一些谷歌结果，我读到  ”是否可以通过选择空间中的点来构建三角测量？”：答案是肯定的。这被称为增量算法  因此，理想情况下，指向预先存在的 python 实现的指针将受到赞赏。 话虽这么说，在出于效率等方面的考虑，我可能应该描述实际的问题，所以这里是：  我想从 N 维空间中的一组 N+1 点开始（ N &lt;=1024，如果重要的话）我还将有一组 N+1 距离，与每个点相关。 我希望能够生成一个与距离最匹配的新点原始点，但要理解距离很可能是近似的，并且可能无法清楚地指定单个点。所以一些“最适合”很可能需要近似。  ​ ​   由   提交/u/lostinspaz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c8yde/d_existing_python_implementation_for_ndimentional/</guid>
      <pubDate>Sun, 21 Jan 2024 17:45:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 良好的欺诈检测步骤</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c8qv1/d_the_steps_for_a_good_fraud_detection/</link>
      <description><![CDATA[您好，我是一名机器学习爱好者，我想知道有效检测欺诈的正确步骤是什么。例如，哪些 KPI、错误、验证步骤和细节对于一个好的项目是有用的。如果您还可以写一个操作列表，例如：第一步 - 检查数据...-第二步....  非常感谢   由   提交/u/NoArmy6203  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c8qv1/d_the_steps_for_a_good_fraud_detection/</guid>
      <pubDate>Sun, 21 Jan 2024 17:36:29 GMT</pubDate>
    </item>
    <item>
      <title>[N] 开放模型 - 以独特的方式彻底改变人工智能交互 [新闻]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c6wll/n_open_models_revolutionizing_ai_interaction_with/</link>
      <description><![CDATA[嘿 Reddit！作为一名开发人员和人工智能爱好者，我很高兴向大家介绍我的最新项目：开放模型。这不仅仅是另一个人工智能框架；它是一个人工智能框架。它改变了我们与 AI 应用程序交互的方式。 开放模型在 AI 模型（如 TTS、TTI、LLM）和支持它们的底层代码之间提供了一个创新的抽象层。这个项目的美妙之处在于它的简单性和开放性。作为一项开源计划，它旨在使人工智能交互民主化，使用户能够自由地参与不同的人工智能模型，而无需深入研究复杂的代码库。 开放模型的与众不同之处在于它的多功能性。无论您是经验丰富的开发人员还是业余爱好者，该项目都可以提供将各种人工智能模型集成到您的应用程序中的无缝体验。它充满了易于理解的示例，使其成为任何对人工智能感兴趣的人的游乐场。 我创建开放模型的愿景是：允许其他人与他们选择的人工智能公开互动，培养社区驱动的人工智能开发和使用方法。深入探索开放模型的世界，看看它如何改变您的人工智能交互。 观看视频以获取详细说明和功能展示： https://youtu.be/AwlCiSkzIPc Github 存储库： https://github.com/devspotyt/open-models 请随时订阅我的时事通讯，以了解最新的技术和信息。我正在运行的项目： https://devspot.beehiiv.com/subscribe 请告诉我您的想法，或者如果您对其他视频/项目有任何疑问/请求， 干杯 &lt;!-- SC_ON - -&gt;  由   提交/u/dev-spot  /u/dev-spot  reddit.com/r/MachineLearning/comments/19c6wll/n_open_models_revolutionizing_ai_interaction_with/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c6wll/n_open_models_revolutionizing_ai_interaction_with/</guid>
      <pubDate>Sun, 21 Jan 2024 16:17:45 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我想为机器人创建一个大视觉模型（LVM）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c5fra/p_i_want_to_create_a_large_vision_model_lvm_for/</link>
      <description><![CDATA[我可以贡献任何开源代码吗？我也愿意从头开始创建一个   由   提交/u/Snoo_72181   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c5fra/p_i_want_to_create_a_large_vision_model_lvm_for/</guid>
      <pubDate>Sun, 21 Jan 2024 15:12:02 GMT</pubDate>
    </item>
    <item>
      <title>Andrew NG 的机器学习专业化 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c48nb/machine_learning_specialization_by_andrew_ng/</link>
      <description><![CDATA[   /u/pythoncoursesonline  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c48nb/machine_learning_specialization_by_andrew_ng/</guid>
      <pubDate>Sun, 21 Jan 2024 14:12:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练后泛化方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c45lq/d_post_train_generalization_methods/</link>
      <description><![CDATA[有训练后泛化方法吗？假设您已经训练了一个模型，并且发现它过度拟合，并且您想稍微改变权重，以便该模型显示更少的过度拟合？  如果模型没有使用它，我可以想象一些基本方法作为微调，引入泛化事物（例如 L2 + dropout 训练 5 个时期），但是是否有任何论文评估最有效的方法在这种情况下？   由   提交/u/tepes_creature_8888   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c45lq/d_post_train_generalization_methods/</guid>
      <pubDate>Sun, 21 Jan 2024 14:07:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 托管 CPU 密集型模拟应用程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19c31fn/r_hosting_for_cpu_intensive_simulation_app/</link>
      <description><![CDATA[我正在寻找一个可以托管我的 python 模拟应用程序的服务，该应用程序非常占用资源。每个会话都需要一个专用的 CPU。 是否有任何服务可以让我的应用程序的每个会话都可以拥有专用的 CPU，并且我可以与同事共享该应用程序？ &lt; !-- SC_ON --&gt;  由   提交/u/lanytho  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19c31fn/r_hosting_for_cpu_intensive_simulation_app/</guid>
      <pubDate>Sun, 21 Jan 2024 13:07:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型动作模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bzv9a/r_large_action_models/</link>
      <description><![CDATA[我应该开始研究 LAM，否则几个月后炒作就会消失，我对这个领域很感兴趣，但我认为兔子的 R1 不会成功的原因有很多，包括非常高的延迟。   由   提交 /u/Spiritual_Guide6862   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bzv9a/r_large_action_models/</guid>
      <pubDate>Sun, 21 Jan 2024 09:40:24 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 重新使用 LLM/下一个令牌预测器的状态作为优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bzlxd/discussion_reusing_state_from_llms_nexttoken/</link>
      <description><![CDATA[我一直在思考 GPT-3/4 内部必须如何工作以及可能的优化。我想知道是否有人可以向我指出该领域已经完成的研究，或者我是否完全误解了这些模型的工作原理。 所以基本上我想知道“下一个令牌”预测器方面。尽管它们具有预测下一个标记的功能，但在我看来，这些模型显然必须有一个内部过程（在训练期间以“黑匣子”方式开发）来预测其余的响应。这种预期似乎是必要的，可以防止模型发出下一个导致死胡同的标记，从而无法构建连贯的句子。 此外，这种预见似乎超出了单个句子的范围。 GPT-4 回复通常呈现高度结构化的格式，包括介绍、深入分析和结论，表明答案中具有更高水平的规划或配置。这让我相信，即使模型一次只生成一个下一个标记，它也可能在内部形成一个更完整的响应，以确保精心选择下一个标记。可能不是以正常令牌表示形式散列出完整响应的方式，但至少有一些接近于此的内部表示形式。特别是对于较短的范围（句子），我想它可能相当精确，但对于较长的范围（段落等），也许它越来越抽象。 这种理解提出了一个问题：有没有办法直接从网络中提取更多完整响应？目前，考虑到之前发出的令牌，似乎对每个令牌重复整个计算。我怀疑这些计算的很大一部分可能是相似的，或者至少可能有一个更有效的途径来在发出第一个令牌后从内部状态生成完整的答案。 实际上，这可能涉及更改模型以在每次迭代中产生更长或完整的响应，而不仅仅是单个标记。或者，可以开发一个较小的辅助模型，以在一次令牌生成后“窥视”主要模型的内部状态，并从中生成完整的答案。或者，也许是一个以允许重用内部状态的方式训练的模型，从而加速后续令牌的生成。也许只是从相同的状态重新启动或以某种方式改变它。 我很好奇这些想法的可行性以及它们是否符合当前对法学硕士的理解。我期待听到您的想法，特别是如果我对法学硕士如何运作的假设存在根本性误解的话。   由   提交 /u/ShoeStatus2431   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bzlxd/discussion_reusing_state_from_llms_nexttoken/</guid>
      <pubDate>Sun, 21 Jan 2024 09:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有任何动手/实用的 ML YouTube 频道？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/</link>
      <description><![CDATA[我一直在寻找实用的深度学习或机器学习论文实现或 YouTube 频道上的实践。您有推荐的频道吗？   由   提交/u/Agitated-Ad809  /u/Agitated-Ad809 reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bz6l5/d_are_there_any_hands_onpractical_ml_youtube/</guid>
      <pubDate>Sun, 21 Jan 2024 08:52:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 自我奖励语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bqy3b/r_selfrewarding_language_models/</link>
      <description><![CDATA[摘要： 我们认为，为了实现超人智能体，未来的模型需要超人反馈才能提供足够的训练信号。目前的方法通常根据人类偏好来训练奖励模型，这可能会受到人类表现水平的瓶颈，其次这些单独的冻结奖励模型无法在 LLM 训练期间学习改进。在这项工作中，我们研究自我奖励语言模型，其中语言模型本身通过法学硕士作为法官来使用，提示在训练期间提供自己的奖励。我们表明，在迭代 DPO 培训期间，不仅提高了指令遵循能力，而且还提高了为自身提供高质量奖励的能力。在我们的方法的三个迭代中对 Llama 2 70B 进行微调，产生的模型优于 AlpacaEval 2.0 排行榜上的许多现有系统，包括 Claude 2、Gemini Pro 和 GPT-4 0613。虽然只是初步研究，但这项工作打开了大门模型在两个轴上不断改进的可能性。 https://arxiv.org/abs/2401.10020&lt; /a&gt;    由   提交 /u/rlresearcher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bqy3b/r_selfrewarding_language_models/</guid>
      <pubDate>Sun, 21 Jan 2024 00:54:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型语言模型中的涌现能力只是上下文学习吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19bkcqz/r_are_emergent_abilities_in_large_language_models/</link>
      <description><![CDATA[论文。我不隶属于作者。 摘要：  大型语言模型表现出了突现能力，在未经过明确训练的各种任务中表现出了卓越的性能，包括那些需要复杂推理能力的人。这种能力的出现对 NLP 的未来研究方向具有深远的影响，特别是随着此类模型的部署变得更加普遍。然而，一个关键的挑战是，对这些能力的评估常常与通过替代提示技术（例如情境学习和指令遵循）在模型中出现的能力相混淆，这些能力也会随着模型规模的扩大而出现。在这项研究中，我们首次对这些新兴能力进行全面检查，同时考虑了可能影响模型评估的各种潜在偏差因素。我们对 18 个模型进行了严格的测试，参数范围从 6000 万到 1750 亿个参数，涉及 22 项综合任务。通过 1,000 多个实验，我们提供了令人信服的证据，证明涌现能力主要归因于情境学习。我们没有发现推理能力出现的证据，因此为驱动所观察到的能力的潜在机制提供了有价值的见解，从而减轻了对其使用的安全担忧。  作者讨论了这项工作此处。  然而，我们的研究提供了不同的视角，通过以下方式解决这些问题揭示了法学硕士的新兴能力，除了语言能力之外，并不像以前认为的那样本质上是不可控制或不可预测的。相反，我们的新颖理论将它们归因于法学硕士基于几个例子完成任务的能力的表现，这种能力被称为“情境学习”（ICL）。我们证明，ICL、记忆和语言能力（语言能力）的出现相结合可以解释法学硕士所表现出的能力和局限性，从而表明法学硕士缺乏新兴推理能力。   该作品的一位作者在此视频。 这项工作在这篇 Reddit 帖子中进行了讨论（ 280 多条评论）。该作品的一位作者在那里发表了评论，包括该作品的摘要。 这里是你/H_TayyarMadabushi 的 Reddit 评论，在撰写本文时完全是关于该作品的。 该作品在 这篇博文（并非由该作品的任何作者撰写）。   由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19bkcqz/r_are_emergent_abilities_in_large_language_models/</guid>
      <pubDate>Sat, 20 Jan 2024 19:58:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>