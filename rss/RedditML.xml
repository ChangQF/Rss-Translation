<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 09 Apr 2024 21:13:02 GMT</lastBuildDate>
    <item>
      <title>[D] 基于文本的 RAG LLM 本地培训：2x3090Ti (48GB) 与 1x4090 (24GB) ？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c01oip/d_text_based_rag_llm_training_locally_2x3090ti/</link>
      <description><![CDATA[我知道 40x0 系列没有 NVLink，但我不是在考虑 10 个以上的卡簇，最多我有 4 个. 目前，我已经获准花费 2000 美元购买 GPU 来启动 RAG 项目。您建议采用哪种 GPU 设置？与速度更快的 4090 相比，48GB 真的会有所不同吗？ 谢谢！   由   提交 /u/jeremiadOtiose   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c01oip/d_text_based_rag_llm_training_locally_2x3090ti/</guid>
      <pubDate>Tue, 09 Apr 2024 20:02:38 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 寻找研究论文合作 |主要会议论文征集@EMNLP Conference</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c01bg1/research_looking_for_research_paper_collaboration/</link>
      <description><![CDATA[我是一名从事 ML 和 NLP 研究的本科生，我正在寻找其他研究人员合作共同撰写研究论文。欢迎任何对 NLP 有深入了解和研究技能的人与我共同作者。  您可以在此处找到所有详细信息：https://2024。 emnlp.org/calls/main_conference_papers/#presentation-at-the-conference 如果您有一群人感兴趣，我也愿意，很乐意合作与其他学生研究人员&lt;3 DM 已开放，和平！   由   提交/u/kxifshk  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c01bg1/research_looking_for_research_paper_collaboration/</guid>
      <pubDate>Tue, 09 Apr 2024 19:48:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] float16 中的 Pytorch 模型训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c00dja/d_pytorch_model_training_in_float16/</link>
      <description><![CDATA[嗨，我想在 fp16 中训练我的 pytorch 扩散模型。发现我们只能以混合精度进行训练，而不能纯粹以 fp16 进行训练，这对我来说很好。 现在按照 Huggingface 的“加速”的训练脚本和指南，模型在进行推理时使用 fp32训练循环和不是 fp16。这意味着它正在 fp32 中进行训练，不是吗？ 如果我尝试直接在 fp16 中加载模型，它会给出错误： ”值错误：尝试缩放 FP16 梯度。” 加速训练示例：https://github.com/huggingface/diffusers/blob/main/examples/controlnet/train_controlnet.py   由   提交 /u/Extension-Fox-7660   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c00dja/d_pytorch_model_training_in_float16/</guid>
      <pubDate>Tue, 09 Apr 2024 19:09:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 Webserver 构建本地 LLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzy855/p_building_a_local_llm_with_webserver/</link>
      <description><![CDATA[大家好， 我目前正在开发一个使用 Linux 操作系统（特别是 SLES）的项目。 &lt;对于该项目，我想设置一个支持 RAG 的本地 LLM，这样我就可以使用自己的数据而无需离开我的网络。它还应该包括在 Cuda 上运行它的选项，因为我的 GPU 来自 NVidia。  此外，我想将 LLM 与网络服务器结合使用，以便多人可以访问并使用它。  我已经为我的项目尝试了多个法学硕士，遗憾的是，我还没有找到合适的一个来支持这些特定需求。这就是为什么我想四处询问是否有任何已知的文档或解决方案。    由   提交 /u/Mister_Main   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzy855/p_building_a_local_llm_with_webserver/</guid>
      <pubDate>Tue, 09 Apr 2024 17:42:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在良好的训练与性能权衡下，3B 参数下哪个是最好的模型（多模态或 LM）？ （即良好的参数效率）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzxxwb/d_which_is_the_best_model_multi_modal_or_lm_under/</link>
      <description><![CDATA[目前哪种模型在数据集大小和参数数量合理的情况下具有最佳性能。我问的是架构效率，因为我计划从头开始训练不同的任务，并且可能无法负担 SOTA 模型使用的数据集量。 编辑：如果仅考虑仅解码器模型，您更喜欢哪个？如果只是编码器-解码器，哪个会更喜欢？   由   提交/u/gokulPRO  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzxxwb/d_which_is_the_best_model_multi_modal_or_lm_under/</guid>
      <pubDate>Tue, 09 Apr 2024 17:30:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 就我在 OpenAI 助手评估中构建的内容寻求反馈。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzuwof/p_seeking_feedback_for_what_i_built_on_evaluation/</link>
      <description><![CDATA[最近，我进行了一次有趣的用户通话，用户表示有兴趣评估 OpenAI 助手的性能。 用例： - 用户的场景反映了 RAG 管道，其中有一个助手旨在回答有关疾病和药物的医疗查询。 - 他们提供了指导提示助理和一组包含支持信息的文件，助理需要从中生成响应。 面临的挑战：  - 他们必须扮演 -与聊天机器人进行对话，假设不同的角色（例如，疟疾患者），这对于 100 多个角色来说非常耗时。 - 在角色扮演对话之后，他们必须根据个人反应手动评分诸如响应是否基于支持文档、简洁、完整和礼貌等参数。 开发的解决方案： - 模拟对话：构建一个工具根据用户角色模拟与 Assistant 的对话（例如，“一名患者询问疟疾的治疗”）。 - OpenAI Assistant 的评估：该工具根据用户满意度、接地等参数评估对话使用 UpTrain 的预配置指标（涵盖响应质量、语调、语法等用例的 20 多个指标）来了解事实、相关性等。 寻求反馈：目前正在寻求对所开发工具的反馈。如果您可以在 GitHub 上查看它，我会很高兴。   由   提交/u/Old_Log2517  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzuwof/p_seeking_feedback_for_what_i_built_on_evaluation/</guid>
      <pubDate>Tue, 09 Apr 2024 15:25:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] Copilot（和替代方案）具有端点以支持任何人手动制作的上下文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzsfcs/d_copilot_and_alternatives_with_an_endpoint_to/</link>
      <description><![CDATA[所以我想知道，如果我们能够编写简单的脚本来丰富 AI 助手的上下文（例如 VSCode），会怎样？例如堆栈跟踪或任何我认为在我们开发某些东西时可能对人工智能助手有用的端点。 一些特定的用例：  在堆栈跟踪中缺少导入，助手会知道当我们尝试添加导入时需要导入什么。  stacktrace 中发生错误的函数。这将确保助理更具体地知道我们首先要解决什么问题。   这可以通过提供“仅仅”来实现堆栈跟踪。我想很快我们就会在这个背景下想出许多其他的东西。我想不同的语言需要不同的上下文。在我看来，拥有模块化选项来选择我们想要连接到上下文的内容对于任何助手都是有益的。  手动上下文可能对 ex 有效。特定持续时间或 3 次通话。 你们对此有何看法？   由   提交/u/SixZer0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzsfcs/d_copilot_and_alternatives_with_an_endpoint_to/</guid>
      <pubDate>Tue, 09 Apr 2024 13:39:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 生产级 LLMops 框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzragw/d_production_grade_llmops_frameworks/</link>
      <description><![CDATA[最近开始对 LLMops 框架进行评估。您能否分享一下有哪些可供生产使用的优秀 LLMops 框架，它们提供了良好的定制化程度。我一直在寻找使用 kubeflow 或 airflow ？请分享您的经验   由   提交/u/Electrical_Study_617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzragw/d_production_grade_llmops_frameworks/</guid>
      <pubDate>Tue, 09 Apr 2024 12:46:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你们在生产中拥有什么类型的 RAG 应用程序？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzr1nl/d_what_type_of_rag_applications_do_you_have_in/</link>
      <description><![CDATA[我们计划从基于 Rasa 的虚拟助理迁移到基于 LLM 的 .还有哪些其他用法更相关？您建议使用哪些编排框架？我应该考虑使用代理方法还是基于 RAG 的方法？    由   提交 /u/Winter_Draw9039   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzr1nl/d_what_type_of_rag_applications_do_you_have_in/</guid>
      <pubDate>Tue, 09 Apr 2024 12:34:05 GMT</pubDate>
    </item>
    <item>
      <title>[D]关于使用预训练模型作为目标函数训练另一个模型所产生的问题的讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzn2ic/d_discussion_on_issues_arising_from_using_a/</link>
      <description><![CDATA[您是否知道有任何论文讨论使用预训练模型作为训练另一个模型的目标函数时可能出现的问题？例如，如果目标神经网络的分布不平衡，则目标函数对于另一个神经网络可能显得不平衡。另一个问题可能是，如果破解神经网络太容易，会导致其无法作为目标函数正常运行，从而导致不规则的早期停止等。   &amp;# 32；由   提交/u/Rowing0914  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzn2ic/d_discussion_on_issues_arising_from_using_a/</guid>
      <pubDate>Tue, 09 Apr 2024 08:31:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 异步参数服务器如何与数据并行技术一起工作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzl9xf/d_how_does_an_asynchronous_parameter_server_work/</link>
      <description><![CDATA[      请原谅我的糟糕图表。我试图了解数据并行性如何与 异步参数服务器。 我目前的理解是有一个异步参数服务器并且（例如）我们有2个GPU工作线程。 GPU工人的工作是计算一批数据的梯度，然后将该梯度更新发送到参数服务器。然后，参数服务器将计算新的权重，然后将其发送到相应的 GPU，而无需等待其他 GPU 完成计算。 这是一个图表。 https://preview.redd.it/wdry4xf1fetc1.png?width=1646&amp; ;format=png&amp;auto=webp&amp;s=eda4b47fbe03d43a6706e96132e0380e7612ff00 这对我来说似乎是错误的。例如，假设由于某种原因，您有异构加速器，例如 nvidia H100 和 nvidia GTX 1060 等，H100 可能能够完成例如 5 个批次并在 1060 之前更新权重有机会根据第一次计算更新权重。因此，从理论上讲，GTX 1060 将在超旧权重上应用梯度。 在第二个图中，如果将权重应用于 H100，那么它会相对较快地收敛，但加法后期 1060 梯度会将其推出局部最小值。 ​ https://preview.redd.it/2pq4cw0ueetc1.png?width=730&amp;format=png&amp;auto=webp&amp;s=63ab73570e2e017 21796aa2d11b4b7152c38ff48 在这种情况下，异步参数服务器的权重更新是否正确，因为梯度是针对与新权重不同的一组权重？如果我错了，我很想弄清楚我的逻辑在哪里不正确，因为我很好奇，如果单个工作人员能够连续计算“稍微”旧的权重，而不需要太难的话，那会有多糟糕。时间收敛？   由   提交/u/stereotropic_CS   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzl9xf/d_how_does_an_asynchronous_parameter_server_work/</guid>
      <pubDate>Tue, 09 Apr 2024 06:25:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] 没有指数数据就没有“零样本”：预训练概念频率决定多模态模型性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzjbpn/r_no_zeroshot_without_exponential_data/</link>
      <description><![CDATA[      摘要 &lt; blockquote&gt; 网络爬取的预训练数据集是令人印象深刻的“零样本”的基础。评估多模态模型的性能，例如用于分类/检索的 CLIP 和用于图像生成的稳定扩散。然而，目前尚不清楚“零样本”概念的意义有多大。泛化是针对这种多模态模型的，因为不知道它们的预训练数据集在多大程度上包含“零样本”过程中针对的下游概念。评估。在这项工作中，我们问：预训练数据集中这些概念的频率如何影响多模态模型在下游概念上的性能？我们在 34 个模型和 5 个标准预训练数据集（CC-3M、CC-12M、YFCC-15M、LAION-400M、LAION-Aesthetics）中全面研究了这个问题，生成了超过 300GB 的数据工件。我们一致发现，远非表现出“零射击”，而是表现出“零射击”。概括地说，多模态模型需要指数级更多的数据来实现下游“零样本”的线性改进。性能，遵循样本低效对数线性缩放趋势。即使在控制预训练和下游数据集之间的样本级相似性以及对纯合成数据分布进行测试时，这种趋势仍然存在。此外，根据我们的分析对采样的长尾数据进行基准测试模型，我们证明多模态模型整体表现不佳。我们将此长尾测试集贡献为“Let it Wag！”为进一步研究该方向奠定了基础。综上所述，我们的研究揭示了对训练数据的指数级需求，这意味着“零样本”的关键在于训练数据。大规模训练范式下的泛化能力仍有待发现。  ​ 概念频率与 T2I 审美分数之间的对数线性关系。 论文：&lt; /strong&gt; https://arxiv.org/pdf/2404.04125.pdf 项目： https://github.com/bethgelab/Frequency_definees_performance&lt; /a&gt; 数据集： https:// Huggingface.co/datasets/bethgelab/Let-It-Wag ​ ​   由   提交/u/quequero  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzjbpn/r_no_zeroshot_without_exponential_data/</guid>
      <pubDate>Tue, 09 Apr 2024 04:27:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 就 RAG 研究而言，为什么似乎很多人没有致力于猎犬的研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzfxgm/d_in_terms_of_rag_research_why_does_it_seem_like/</link>
      <description><![CDATA[我是几年前进行 NLP 研究的人，后来停止并加入了行业，最近试图重新掌握事物。我对 RAG 相关的工作很感兴趣，并开始阅读一些论文。 我的理解是，对于 RAG，你有检索器和生成器。对于生成器来说，使用各种 LLM 似乎是标准的，但检索器似乎也设置为使用 BM25 或最初使用的 DPR 之类的东西。我认为 RAG 的性能将在很大程度上依赖于检索器，但我也有点惊讶地发现似乎没有在这方面进行大量研究。 我只是错误并且没有看向正确的方向？或者说，检索器似乎没有得到那么多关注是有什么原因吗？ 想想看，我并没有真正看到编码器模型总体上做了很多工作。    由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzfxgm/d_in_terms_of_rag_research_why_does_it_seem_like/</guid>
      <pubDate>Tue, 09 Apr 2024 01:38:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 高效扩散模型中缺失的 U</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/</link>
      <description><![CDATA[一篇新论文提出用利用神经常微分方程的连续 U-Net 取代扩散模型中的标准离散 U-Net 架构。这种重新表述可以对去噪过程进行连续建模，从而显着提高效率：  推理速度提高 80% 模型参数减少 75% 70% 保持或提高图像质量  关键技术贡献：  动态神经 ODE 模块建模潜在表示演化使用二阶微分方程 自适应时间嵌入来调节扩散时间步长的动力学 高效的 ODE 求解器和常量内存伴随方法，可实现更快、内存效率更高的训练 &lt; /ul&gt; 作者展示了这些在图像超分辨率和去噪任务上的改进，并通过详细的数学分析解释了为什么连续公式会导致更快的收敛和更有效的采样。 潜在影响： p&gt;  使扩散模型适用于更广泛的应用（实时工具、资源受限设备） 在深度学习、微分方程、动力学的交叉领域开辟新的研究方向系统  以下方面存在一些限制：(1) ODE 求解器和伴随方法增加了复杂性；(2) 我认为即使进行了改进，扩散模型仍然可能需要大量计算。 &lt; p&gt;完整摘要此处。 Arxiv 此处。 TL;DR：新论文建议替换离散 U-使用神经 ODE 的连续 U-Net 扩散模型中的网络，可将推理速度提高 80%、参数减少 75%、FLOP 减少 70%，同时保持或提高图像质量。主要影响：更高效、更容易理解的生成模型、连续时间深度学习的新研究方向。   由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bzfns4/r_the_missing_u_for_efficient_diffusion_models/</guid>
      <pubDate>Tue, 09 Apr 2024 01:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>