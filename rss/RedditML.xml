<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sun, 02 Jun 2024 09:15:11 GMT</lastBuildDate>
    <item>
      <title>[R] FineWeb 技术报告：大规模挖掘网络上最精细的文本数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</link>
      <description><![CDATA[FineWeb 15 万亿公开发布的网络规模数据集背后的团队刚刚发表了一篇关于创建高质量网络规模数据集的科学的详尽博客文章，详细介绍了 FineWeb 的步骤和学习成果，以一种 distill.pub 交互式文章/博客的方式。 他们还发布了 FineWeb-Edu，这是 Common Crawl 的一个过滤子集，拥有 1.3T 令牌，专注于教育内容非常丰富的网页，在知识和推理密集型基准测试（如 MMLU、ARC 和 OpenBookQA）上，其表现似乎优于所有公开发布的网络规模数据集 有趣的阅读：https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1     由    /u/Thomjazz 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</guid>
      <pubDate>Sun, 02 Jun 2024 08:17:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 合并竞赛：通过合并高效构建 LLM（NeurIPS 2024 挑战赛）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d67ydm/r_llm_merging_competition_building_llms/</link>
      <description><![CDATA[NeurIPS 2024 挑战赛网站：https://llm-merging.github.io/ Discord：https://discord.gg/dPBHEVnV LLM 合并竞赛：通过合并高效构建 LLM NeurIPS 2024 挑战赛 目标和重点 从头开始训练高性能大型语言模型 (LLM) 是一项众所周知的昂贵且困难的任务，仅计算费用就高达数亿美元。然而，这些预先训练过的 LLM 可以通过微调以低成本轻松适应新任务，从而产生大量适合特定用例的模型。最近的研究表明，专门的微调模型可以快速合并以结合各种功能并推广到新技能。 开始 比赛将为参赛者提供已经在特定任务数据集上训练过的专家模型列表。所有这些模型都将在 Hugging Face Model Hub 上公开提供，并具有允许将其用于研究目的的许可证。这些模型可以是完全微调的模型，也可以是通过参数高效的微调方法（例如 LoRA）获得的模型。此列表中的模型需要满足以下标准：（1）模型大小 &lt;= 8B 参数，（2）模型具有与研究用途兼容的许可证（例如 MIT、Apache 2 等）。 可以在此处找到具有端到端提交流程的入门套件：https://github.com/llm-merging/LLM-Merging    提交人    /u/hardmaru   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d67ydm/r_llm_merging_competition_building_llms/</guid>
      <pubDate>Sun, 02 Jun 2024 07:33:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多智能体强化学习中 IQL 与 VDN 的区别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d67d7j/r_different_between_iql_and_vdn_for_multi_agent/</link>
      <description><![CDATA[      大家好， 我正在研究强化学习中的合作多智能体。我已经使用 DQN（深度 Q 网络）为 PettingZoo 中 Cooperative Pong 环境中的每个代理实现了独立 Q 学习。现在我正在阅读有关 VDN （值分解网络）的内容，我有一个问题。 在本文中，他们说连接值 Q 函数是每个代理的子 Q 函数的总和。这个想法是最大化每个代理的局部 Q 函数，这相当于最大化全局 Q 函数。但 IQL 也是最大化每个代理的局部 Q 函数。那么这两者之间有什么不同？ https://preview.redd.it/e3gvpcotv34d1.png?width=626&amp;format=png&amp;auto=webp&amp;s=07672f79fee8fbaa660abb0bcfb28c24de01ba23    提交人    /u/Civil_Statement_9331   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d67d7j/r_different_between_iql_and_vdn_for_multi_agent/</guid>
      <pubDate>Sun, 02 Jun 2024 06:53:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 师生培训策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d65y1h/d_teacher_student_training_strategy/</link>
      <description><![CDATA[我计划使用 LLM（例如 llama3）通过提示提取训练数据，然后使用带有 CLS 令牌的较小模型进行自定义训练，以尝试匹配 LLM 的准确性。假设我可以在 1M+ 数据上运行提示（尽管我怀疑我不需要那么多）。 提示：以下句子包含苹果还是橘子：示例：  &quot;&lt;prompt&gt; 苹果，橘子&quot; -&gt; 苹果，橘子 &quot;&lt;prompt&gt; 苹果，橘子&quot; -&gt; 苹果，橘子 &quot;&lt;prompt&gt; 苹果，没有橘子&quot; -&gt;苹果  所以我的问题是：  我见过的最后一个 CLS 类型 LLM 是微软的 xtremedistil。这些模型还在使用吗？如果是，最新 + 最好的是什么？ 使用句子转换器并进行分类会更好吗？ 在我的学生模型训练集中，我将删除上面的提示，这种方法有风险吗？  在我看来，从长远来看，这些模型更小，成本也更低。非常感谢大家的普遍想法。    提交人    /u/themathstudent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d65y1h/d_teacher_student_training_strategy/</guid>
      <pubDate>Sun, 02 Jun 2024 05:16:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您在现实世界中使用 LLM 的案例有哪些？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/</link>
      <description><![CDATA[我认为我们应该分享更多 LLM 的生产用例，而不仅仅是理论上的最佳实践。 您可以分享您在生产中看到/构建的用例吗？它应包括以下详细信息：  它解决的问题 实现细节（模型、基础设施等） 它产生的业务影响     提交人    /u/madredditscientist   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/</guid>
      <pubDate>Sun, 02 Jun 2024 05:12:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将 3 个数据集合并为一个唯一的数据集，并且知道这 3 个数据集与同一主题相关，这是一个好主意吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d65gdt/d_is_it_a_good_idea_to_combine_3_datasets_into/</link>
      <description><![CDATA[它们基本上是同一主题，具有相同的标签，唯一的区别是数据集本身。为了区分第一个数据集（将转变为其他三个数据集中的一个数据集）的图像，我将与我的研究同事一起从头开始创建另一个数据集。    提交人    /u/MessierKatr   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d65gdt/d_is_it_a_good_idea_to_combine_3_datasets_into/</guid>
      <pubDate>Sun, 02 Jun 2024 04:44:15 GMT</pubDate>
    </item>
    <item>
      <title>为开源模型实现“扩展单义性：从 Claude 3 Sonnet 中提取可解释的特征”论文。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/</link>
      <description><![CDATA[我最近偶然发现了一篇有趣的论文，题为“扩展单义性：从 Claude 3 Sonnet 中提取可解释特征”，该论文探讨了如何使用稀疏自动编码器从大型语言模型的激活中提取可解释特征。该方法似乎有望深入了解模型的内部表示和行为。 这让我开始思考为开源语言模型实施类似的可解释性技术的可行性。我们能否在不进行大量微调的情况下控制 LLM 及其行为。 我想联系这个社区讨论一些事情：  是否有人已经在开源语言模型上实施或试验过类似的可解释性技术？我们可以制作类似于金门克劳德的东西吗？ 您认为调整和扩展这些技术以与 Llama、phi、mistral 等一起使用是否可行？与 sonnet 相比，它们的参数大小要小得多。 我有兴趣与其他对这个研究领域充满热情的人合作。如果您正在研究开源模型的可解释性或对新方法有想法，我很高兴与您合作并进一步探索。我们可以合作实施技术、共享资源或集思广益新想法。  如果您有兴趣合作或有任何想法要分享，请随时分享。    提交人    /u/No-Point1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/</guid>
      <pubDate>Sun, 02 Jun 2024 03:50:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] LAION 美学数据集的替代品？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5y0oo/d_alternatives_to_the_laion_aesthetics_dataset/</link>
      <description><![CDATA[因此，LAION 数据集目前已关闭以进行安全审查。与此同时，是否有任何大型数据集可以代替其美学子集？我正在寻找“美学上令人愉悦”的图像，例如艺术品、绘画、漂亮的照片等。    提交人    /u/thehomelessman0   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5y0oo/d_alternatives_to_the_laion_aesthetics_dataset/</guid>
      <pubDate>Sat, 01 Jun 2024 22:00:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] CoPE：上下文位置编码：学习计算重要的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/</link>
      <description><![CDATA[  由    /u/fasttosmile  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/</guid>
      <pubDate>Sat, 01 Jun 2024 19:05:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] Tensttorrent Galaxy Server 使用案例</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5taip/d_tensttorrent_galaxy_server_usecases/</link>
      <description><![CDATA[大家好， 第一次发帖，请温柔一点 :) 我有机会以少量资金长期借用一台配备 32 个 wormhoole 处理器的 Tenstorrent Galaxy 服务器。 我们是一家拥有技术娴熟的工程师的小公司，但我们没有太多时间深入研究人工智能。我们做一些基本的事情，比如使用现成的模型和运行推理并进行一些轻度微调（主要是 YOLO），因为这不是我们的核心业务。然而，我个人的愿望是慢慢转向人工智能领域，所以我的问题是，如果你有类似的机会，你会接受它并将其用于什么？我知道这是一个广泛的问题，所以请自由发挥创意:) 在 github (https://github.com/tenstorrent/tt-buda-demos/tree/main/model\_demos) 上，他们已经有了模型演示列表，但这只是推理，没有看到提到微调或训练    提交人    /u/zakakanje   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5taip/d_tensttorrent_galaxy_server_usecases/</guid>
      <pubDate>Sat, 01 Jun 2024 18:21:57 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 PyTorch Geometric 进行 GNN 采样的视觉指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5nmlh/p_a_visual_guide_to_gnn_sampling_using_pytorch/</link>
      <description><![CDATA[      大家好， 我发布了一些关于“图神经网络采样”主题的视频和笔记本（GNNs）”。原始 GCN 论文采用全批量训练。然后，研究人员使用不同的方法创建小批量（子图）来训练 GCN。例如，GraphSAGE 论文使用了邻居采样器，而 ClusterGCN 论文使用了集群采样器。这些采样器在 pytorch-geometric 中的 torch_geometric.loader 下实现。 这是视频，或者你可以直接跳到代码中..  图形神经网络的采样 视频 代码  图形神经网络中的迷你批次视频 视频 代码 原始图表 三使用来自 pytorch_geometric 的 NeighborLoader 对子图进行采样；白色节点未被采样。    提交人    /u/mashaan14   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5nmlh/p_a_visual_guide_to_gnn_sampling_using_pytorch/</guid>
      <pubDate>Sat, 01 Jun 2024 14:00:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] Mojo 值得吗，或者你愿意为 ML 学习哪种第二语言？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5kov5/d_is_mojo_worth_it_or_which_second_language_would/</link>
      <description><![CDATA[基本上就是标题。我非常精通 Python（正如预期的那样），但除此之外，我对 JavaScript 和 C++ 的了解非常有限。我想学习一种更“低级”的第二种语言，可以更好地利用硬件功能。我的目标不是重写 Pytorch 或完全替换 Python（尽管 将推理移植到 Mojo 可能有意义），而是为性能关键用例提供替代方案。 从今天的情况来看，答案显然是 C++。然而，Rust 越来越受欢迎，除了陡峭的学习曲线外，人们开始在许多方面将其置于 C++ 之上。在这两种情况下，语法和语言都与 Python 不太接近，这使得它们很难学习。 Mojo 在这方面似乎要好得多，既提供了语法类似于 Rust 的低级功能（至少对于像我这样的门外汉来说），又可以用作奇怪的 Python 风格。它甚至允许直接导入 Python 库。这对于这种缺乏大型社区和各种库的年轻语言非常有帮助。尽管如此，该语言仍然很年轻，而且很容易发生变化，所以我不确定是否应该投资。 那么，对于上述用例，您认为最好的“第二种”语言是什么？有使用 Mojo 的经验吗？您是如何学习它或资源有限的任何其他语言的。如果我使用 Mojo，我打算通读文档并解决去年使用它的代码出现的问题。    提交人    /u/canbooo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5kov5/d_is_mojo_worth_it_or_which_second_language_would/</guid>
      <pubDate>Sat, 01 Jun 2024 11:15:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] DeTikZify：使用 TikZ 合成科学图形和草图的图形程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5hh0s/p_detikzify_synthesizing_graphics_programs_for/</link>
      <description><![CDATA[        由    /u/DrCracket 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5hh0s/p_detikzify_synthesizing_graphics_programs_for/</guid>
      <pubDate>Sat, 01 Jun 2024 07:20:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 自动 LoRA 发现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5ei7s/p_automated_lora_discovery/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5ei7s/p_automated_lora_discovery/</guid>
      <pubDate>Sat, 01 Jun 2024 04:09:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>