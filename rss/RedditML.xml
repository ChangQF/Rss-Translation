<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Mon, 08 Apr 2024 18:18:49 GMT</lastBuildDate>
    <item>
      <title>[P] Julia 语言的新型高性能 Tsetlin 机器实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz53ir/p_new_high_performance_tsetlin_machine/</link>
      <description><![CDATA[外部依赖为零的 Tsetlin Machine 库表现良好。    ;由   提交 /u/ArtemHnilov   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz53ir/p_new_high_performance_tsetlin_machine/</guid>
      <pubDate>Mon, 08 Apr 2024 18:17:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLM比较和参数调优的OSS工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz4rnr/p_oss_tool_for_llm_comparison_and_parameter_tuning/</link>
      <description><![CDATA[      我最初将此工具编写为 CLI 应用程序，用于使用网格搜索测试推理参数的组合（因此名称 Ollama 网格搜索)。 https://preview.redd.it/vbgh6vbhpac1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=16687c2e9077348 c5e082ef85c80820c46c83ef9 以下是它的一些功能： ​  自动从本地或远程 Ollama 服务器获取模型； 迭代不同的模型和参数以生成推理； 同时对不同模型进行 A/B 测试提示；  进行同步推理调用以避免向服务器发送垃圾邮件； 可选择输出推理参数和响应元数据（推理时间、令牌和令牌）；&lt; /li&gt; 重新获取单个推理调用； 可以按名称过滤模型选择； 列出可以以 JSON 格式下载的实验格式； 可配置的推理超时； 可以在设置中定义自定义默认参数和系统提示   ​ 大多数主要平台的源代码和（未签名）版本可在以下位置获取： https://github.com/dezoito/ollama-grid-search  它仍在进行中......我计划添加更多功能，但我我正在利用业余时间做这件事。  希望这对使用开源模型的人们有用   由   提交 /u/grudev   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz4rnr/p_oss_tool_for_llm_comparison_and_parameter_tuning/</guid>
      <pubDate>Mon, 08 Apr 2024 18:04:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 促进离策略学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz2cig/p_boosted_offpolicy_learning/</link>
      <description><![CDATA[        由   提交 /u/ggyshay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz2cig/p_boosted_offpolicy_learning/</guid>
      <pubDate>Mon, 08 Apr 2024 16:29:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] CVPR 研讨会 vs. ECAI vs. BMVC？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bz0b6a/d_cvpr_workshop_vs_ecai_vs_bmvc/</link>
      <description><![CDATA[我最近在 CVPR 研讨会上接受了我的论文。然而，我的顾问建议撤回论文并将其提交给 ECAI 可能是更好的选择。作为一名计划在即将到来的秋天申请博士学位课程的理学硕士学生，我试图了解哪种选择可以更丰富我的简历。 在这种情况下，BMVC 相比如何，特别是考虑到我的工作重点是可解释性在视觉变形金刚中？ （更适合简历密集的会议）   由   提交/u/whats-a-monad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bz0b6a/d_cvpr_workshop_vs_ecai_vs_bmvc/</guid>
      <pubDate>Mon, 08 Apr 2024 15:10:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于那些独自发表文章的人来说，你们的经历是怎样的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</link>
      <description><![CDATA[这些天我有一些空闲时间，一直在努力赶上我所在领域的研究。我想真正重新审视我在硕士期间正在研究的一个主题，但始终无法发表论文。问题是，我不确定作为唯一作者，如果没有任何真正的资源访问权限，这是否可行。 朋友和熟人告诉我这是可能的，但极其困难。很好奇其他成功做到这一点的人是怎么想的。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byzggt/d_for_those_of_you_who_have_published_alone_what/</guid>
      <pubDate>Mon, 08 Apr 2024 14:35:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] PDF 提取和清洁 RAG</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byze0l/d_pdf_extraction_cleaning_rag/</link>
      <description><![CDATA[大家好，我收到了一项作业，其中我必须使用 langchain 和开源 LLM 构建一个 RAG 系统，所以我有 pdf 形式的数据，其中包含研究论文，大约有 13 个 pdf 文件，每个 pdf 平均包含 20 页所以我的任务是提取文本（使用 pypdf 进行）并从 pdf 中删除图形和表格以及所有文本引用。其中一个 pdf 是图像，所以我必须为此进行 ocr 现在我真的很困惑并且卡住了如何清理文本我尝试了正则表达式，但它并没有那么有用，因为数据是非结构化的我如何维护文本的语义稍后将用于抹布我想将文本传递给 llm 进行清理，但我认为这是不可行的，因为令牌限制将是一个问题，并且 llm 可能会产生幻觉 抱歉，如果问题看起来很愚蠢，新手，谢谢提前！   由   提交 /u/Potential_Zone1522   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byze0l/d_pdf_extraction_cleaning_rag/</guid>
      <pubDate>Mon, 08 Apr 2024 14:33:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] DBRX 是专门为企业设计的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byxqy6/d_is_dbrx_specifically_made_for_businesses/</link>
      <description><![CDATA[我了解到 Databricks 推出了名为 DBRX 的新通用 LLM。我很好奇它与其他法学硕士有何不同（除了开源之外）？ 它是专门为企业还是供公众使用而设计的，例如 chatgpt？或者它是否像“企业聊天”？他们可以在哪里微调开源模型以满足他们的需求？   由   提交/u/Ok_Moment4946   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byxqy6/d_is_dbrx_specifically_made_for_businesses/</guid>
      <pubDate>Mon, 08 Apr 2024 13:22:40 GMT</pubDate>
    </item>
    <item>
      <title>“clip-vit-large-patch14”如何将文本序列表示聚合成表示整个序列的奇异向量？没有 [CLS] 代币，但有 [SOT] 和 [EOT] 代币。 [研究]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byxg7u/how_does_clipvitlargepatch14_aggregate_the_text/</link>
      <description><![CDATA[大家好， 我有以下问题： “clip-vit-large-patch14”如何;将文本序列表示聚合成表示整个序列的奇异向量？没有 [CLS] 标记，但有 [SOT] 和 [EOT] 标记。 当我使用 CLIP 文本编码器并提取 pooler_output 时……这个向量到底是如何创建的？ [SOT] 代币是否用作 [CLS] 代币？或者是否进行了池化操作？ [研究] 此致， Tom  &amp; #32；由   提交/u/tommilyjonesOG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byxg7u/how_does_clipvitlargepatch14_aggregate_the_text/</guid>
      <pubDate>Mon, 08 Apr 2024 13:09:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 手写文本到文本项目 - 寻求提示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byurwj/p_hand_written_text_to_text_project_asking_for/</link>
      <description><![CDATA[大家好，我有一个朋友，他有自己的生意，他经常告诉我他手动抄写帐单浪费了多少时间从手写纸滑到他电脑上的文档。我不是专业人士，我仍在学习，但我想知道是否可以使用已经训练好的模型应用一些迁移学习，然后使用新数据  有谁知道是否有一些CNN已经用于此类项目？ 使用什么样的激活函数？感谢您的关注，并提前感谢您的回复:)    由   提交/u/December92_yt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byurwj/p_hand_written_text_to_text_project_asking_for/</guid>
      <pubDate>Mon, 08 Apr 2024 10:46:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 确保加拿大的人工智能优势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bytkh8/d_securing_canadas_ai_advantage/</guid>
      <pubDate>Mon, 08 Apr 2024 09:28:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于高性能语言技术的新的海量多语言数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byt3j8/r_a_new_massive_multilingual_dataset_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.14009 项目页面：https://hplt -project.org/ 数据集：https:// /hplt-project.org/datasets/v1.2 GitHub： https://github.com/hplt-project 摘要：  我们介绍HPLT（高性能语言技术）语言资源，一个新的大规模多语言数据集，包括从 CommonCrawl 中提取的单语和双语语料库以及从互联网档案馆中提取的以前未使用的网络爬虫。我们描述了大型语料库的数据采集、管理和处理方法，这些方法依赖于开源软件工具和高性能计算。我们的单语集合侧重于中低资源语言，涵盖 75 种语言，并在文档级别删除了总共约 5.6 万亿个单词标记。我们以英语为中心的平行语料库源自其单语对应语料库，涵盖 18 个语言对和超过 9600 万个对齐句子对，以及大约 14 亿个英语标记。 HPLT 语言资源是迄今为止发布的最大的开放文本语料库之一，为语言建模和机器翻译培训提供了丰富的资源。我们公开发布了这项工作中使用的语料库、软件和工具。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byt3j8/r_a_new_massive_multilingual_dataset_for/</guid>
      <pubDate>Mon, 08 Apr 2024 08:56:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] A* 场地研讨会论文与评级较低的场地会议论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byg2n5/r_a_venue_workshop_paper_vs_lowerrated_venue/</link>
      <description><![CDATA[NeurIPS24 就在附近，我有一篇去年在 ICLR (5/5/6/3) 被拒绝的论文。虽然我正在处理上次会议的反馈（该方法得到了积极的反馈，但他们要求进行更多实验），但我仍然不确定这篇论文是否足够强大，足以进入 NeurIPS 等 A* 会议。另外，说实话，我已经为此工作了将近一年，我觉得我已经想结束这个工作并看看其他想法了。 ​ 我想知道这两者哪个更好：  提交到 NeurIPS 或类似的研讨会（ICLR、ICML..）。考虑到 ICLR 的反馈，我认为这对于我的论文来说应该是可行的，但我不知道这是否正确？ 瞄准“较低层”的会议论文。地点，例如 AISTATS、IJCAI 或类似地点。我认为这比 NeurIPS 的研讨会论文更难完成，但我又只是猜测？  我仍然不是博士生，但我定期申请博士学位。因此，我正在寻找一种选择（如果我的论文通过）可以让我作为博士候选人在未来的申请中获得更多的优势。    由   提交 /u/howtorewriteaname   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byg2n5/r_a_venue_workshop_paper_vs_lowerrated_venue/</guid>
      <pubDate>Sun, 07 Apr 2024 21:32:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么人们在 Arxiv 上上传他们的作品，而不是提交会议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bya23i/d_why_do_people_upload_their_work_on_arxiv_not/</link>
      <description><![CDATA[纯粹是我的好奇心。 我知道这种情况，当一篇论文在会议或期刊上被接受，并且可以上传时他们在 Arxiv 上的工作。但我的问题是，有些作品只在Arxiv上上传。这是否意味着作者不想在会议上提交自己的作品，而是想发布自己的作品？或者，发布后他们还有其他提交计划吗？ 我这么问是因为我最近的工作在一次会议上被拒绝了，但我不想再深究了。人们是否也喜欢在 Arxiv 上上传废弃作品的同样情况？   由   提交/u/Shot-Button-9010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bya23i/d_why_do_people_upload_their_work_on_arxiv_not/</guid>
      <pubDate>Sun, 07 Apr 2024 17:31:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们知道Gemini 1.5是如何实现10M上下文窗口的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</link>
      <description><![CDATA[我们知道 Gemini 1.5 是如何实现 1.5M 上下文窗口的吗？随着注意力窗口的扩大，计算量不会呈二次方增长吗？    由   提交/u/papaswamp91  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</guid>
      <pubDate>Sun, 07 Apr 2024 16:21:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>