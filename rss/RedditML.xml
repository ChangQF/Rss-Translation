<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 09 Sep 2024 18:21:00 GMT</lastBuildDate>
    <item>
      <title>[D] 实施论文的价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcvkjm/d_implementing_papers_worth/</link>
      <description><![CDATA[大家好， 我拥有机器人学硕士学位（修过 ML、CV、DL 和数学课程），最近我对 3D 计算机视觉非常感兴趣，所以我研究了一些项目。我发现了 deepSDF。我的目标是在 C++ 上实现它，使用 CUDA 和 SIMD，并在真实相机上测试在线 SDF 构建。 还计划实现 3D 高斯 Splatting。 但我的朋友说不用费心了，因为每个人都可以实现那些论文，所以我需要自己写论文。他是对的吗？我在浪费时间吗？    提交人    /u/Huge-Leek844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcvkjm/d_implementing_papers_worth/</guid>
      <pubDate>Mon, 09 Sep 2024 17:47:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 重新审视用于视觉识别的稀疏卷积模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcvgaw/r_revisiting_sparse_convolutional_model_for/</link>
      <description><![CDATA[  由    /u/bregav  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcvgaw/r_revisiting_sparse_convolutional_model_for/</guid>
      <pubDate>Mon, 09 Sep 2024 17:42:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多元时间序列的模式匹配方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcq4v4/r_methods_for_pattern_matching_with_multivariate/</link>
      <description><![CDATA[大家好， 我想确定我的车辆动力学模式是否与其他（多个）车辆动力学模式相似。例如，假设我有一段 5 秒的数据，表示转向。我如何查看行程的完整驾驶周期数据，以查看此行程中是否发生这种转向（或某种程度上的类似转向）？ 我已经开发了几种方法来做到这一点，但我想知道是否有什么我应该阅读的，这样我就不会在这里重新发明轮子了！ 感谢您的帮助或指导！    提交人    /u/PreviousResearcher50   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcq4v4/r_methods_for_pattern_matching_with_multivariate/</guid>
      <pubDate>Mon, 09 Sep 2024 14:00:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大规模 TTS - 批量推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcloqk/d_tts_at_scale_batch_inference/</link>
      <description><![CDATA[在寻找一些高质量且可扩展的文本转语音解决方案时，我注意到大多数开源解决方案都不支持批量推理 - 它们都适用于单个文本样本。我想同时处理大量请求，因此我相信拥有强大的大型 GPU 并在一个批次中推理多个样本（短句）应该可以大大提高性能。知道它不受支持的原因是什么吗？TTS 架构是否不是这样有效/易于并行化的，可能是由于某些组件？或者由于输出波形的长度不同，该过程可能难以执行？或者您知道一些值得推荐的解决方案？    提交人    /u/ActualDoughnut8687   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcloqk/d_tts_at_scale_batch_inference/</guid>
      <pubDate>Mon, 09 Sep 2024 09:56:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 任何过去/现在的 Amazon ML 竞赛参与者能否提供有关其评估和提交内容的见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcjfkj/d_can_any_amazon_ml_competition_pastpresent/</link>
      <description><![CDATA[嗨，需要你的帮助，任何过去/现在的参与者都可以提供有关挑战、提交窗口、评估的更多信息    提交人    /u/Ticket-Financial   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcjfkj/d_can_any_amazon_ml_competition_pastpresent/</guid>
      <pubDate>Mon, 09 Sep 2024 07:02:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 也许我们可以用每个权重 0.05 位来训练更大的模型，或者在消费级硬件上训练适度的模型……</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcedvx/d_maybe_we_can_train_bigger_models_with_005_bits/</link>
      <description><![CDATA[      https://preview.redd.it/xjc9zhq1xond1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=9dad2f1da27a465a33ba5decdf6a31e36c93a5bc 使用顶部的基本前馈神经网络对 MNIST 进行分类，然后用等级 4 的 DoRa 和 LoRa 表示相同的东西，最后是等级 16 的 bitnet/dora 混合，每个参数 1 位。降至每个权重约 0.05 位。所有权重都具有相同的有效权重数，但可训练参数减少。对于全权重训练，DoRa 似乎在 12 个 epoch 之后就不再比 LoRa 有优势了。 我将探索在增加参数数量以匹配正常模型的内存占用时是否可以获得相同或更好的性能。同时将 LoRa 和 DoRa 相加以动态提高排名并防止早期停滞。我的目标是最终在消费级硬件上训练一个有用的 LLM。如果我能做到这一点，想象一下有人可以在一家大公司用一些 A100 做什么。很快就会发布代码，这是早期的和不完整的。无论如何，我可能只是在做梦。晚安。    提交人    /u/Trainraider   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcedvx/d_maybe_we_can_train_bigger_models_with_005_bits/</guid>
      <pubDate>Mon, 09 Sep 2024 01:57:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 渐进式策略和过早的残局</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fcapxf/d_incremental_gambits_and_premature_endgames/</link>
      <description><![CDATA[        由   提交  /u/Mooseton   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fcapxf/d_incremental_gambits_and_premature_endgames/</guid>
      <pubDate>Sun, 08 Sep 2024 22:55:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 TsetlinMachine 库 Tsetlin.jl 中的最新优化，在 CPU 上实现每秒超过 1 亿个 MNIST 预测（吞吐量为 55.5 GB/s）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fc3ji0/p_achieved_over_100_million_mnist_predictions_per/</link>
      <description><![CDATA[      这个周末，我优化了 TsetlinMachine 库 Tsetlin.jl，取得了出色的成绩：在我的 Ryzen 7950X3D CPU 上每秒可进行 1.01 亿次 MNIST 预测，准确率为 98.10%。这个性能已经接近硬件的最大能力，因为双通道模式下 DDR5 RAM 在 6000 MT/s 下的峰值速度为 96 GB/s。我的吞吐量达到了 55.5 GB/s，主要是因为这个特定的 Tsetlin Machine 模型有 10499 个参数，而 CPU 缓存（尤其是 3D 缓存）在提升性能方面起着重要作用。 https://preview.redd.it/0a719tythmnd1.png?width=1780&amp;format=png&amp;auto=webp&amp;s=001526f65f3be2b99ce2a24ffe4b5bb5486f474e    由    /u/ArtemHnilov  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fc3ji0/p_achieved_over_100_million_mnist_predictions_per/</guid>
      <pubDate>Sun, 08 Sep 2024 17:42:23 GMT</pubDate>
    </item>
    <item>
      <title>聚类算法比较[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fc0niz/clustering_algorithms_comparison_d/</link>
      <description><![CDATA[我想看看是否有论文或文章对不同的聚类算法在优点、缺点和特殊性方面进行比较，我自己还没有找到任何像样的东西    提交人    /u/Extension-Group2131   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fc0niz/clustering_algorithms_comparison_d/</guid>
      <pubDate>Sun, 08 Sep 2024 15:38:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbzs8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbzs8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 08 Sep 2024 15:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[P]: TensorHue – 张量可视化库（详情见评论）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbz318/p_tensorhue_a_tensor_visualization_library_info/</link>
      <description><![CDATA[        提交人    /u/epistoteles   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbz318/p_tensorhue_a_tensor_visualization_library_info/</guid>
      <pubDate>Sun, 08 Sep 2024 14:29:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 通过 LLM 实现隐写术的 Python 工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbx3ny/p_python_tool_for_steganography_through_llms/</link>
      <description><![CDATA[https://github.com/user1342/Tomato    由   提交  /u/OppositeMonday   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbx3ny/p_python_tool_for_steganography_through_llms/</guid>
      <pubDate>Sun, 08 Sep 2024 12:54:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 训练具有多种损失的模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbvuhs/r_training_models_with_multiple_losses/</link>
      <description><![CDATA[我们建议使用 雅可比下降 来同时最小化多个损失，而不是使用梯度下降来最小化单个损失。基本上，该算法通过将（向量值）目标函数的雅可比矩阵简化为更新向量来更新模型的参数。 为了让每个人都能使用它，我们开发了 TorchJD：一个扩展 autograd 以支持雅可比下降的库。在简单的 pip install torchjd 之后，转换基于 PyTorch 的训练函数非常容易。随着最近发布的 v0.2.0，TorchJD 终于支持多任务学习了！ Github：https://github.com/TorchJD/torchjd 文档：https://torchjd.org 论文：https://arxiv.org/pdf/2406.16232 我们很乐意听到社区的一些反馈。如果您想支持我们，请在 repo 上点个星，我们将不胜感激！我们也欢迎讨论和批评。    提交人    /u/Skeylos2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbvuhs/r_training_models_with_multiple_losses/</guid>
      <pubDate>Sun, 08 Sep 2024 11:43:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] Adam 优化器导致 Transformer 语言模型中出现特权基础</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbavdv/r_adam_optimizer_causes_privileged_basis_in/</link>
      <description><![CDATA[        由    /u/rrenaud  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbavdv/r_adam_optimizer_causes_privileged_basis_in/</guid>
      <pubDate>Sat, 07 Sep 2024 16:26:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>