<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Thu, 29 Aug 2024 06:22:48 GMT</lastBuildDate>
    <item>
      <title>会计ERP自动化[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3uhn6/accounting_erp_automation_d/</link>
      <description><![CDATA[会计 ERP 自动化  根据我的工作经验，我觉得很多会计和合规工作可以通过使用 AI 和 ML 实现自动化。我非技术背景。需要一些建议或咨询如何进行。我想先在内部构建，然后可能以订阅方式提供给中小型企业。从印度法律角度看会计和合规。     提交人    /u/ashbabji   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3uhn6/accounting_erp_automation_d/</guid>
      <pubDate>Thu, 29 Aug 2024 05:09:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 未来值得学习 Julia 来进行机器学习吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3ug5g/d_worth_learn_julia_for_ml_in_the_future/</link>
      <description><![CDATA[我一直在阅读有关 Julia 语言的文章，它因其性能和语法而越来越受欢迎，适用于不同的数据任务，包括 ML 模型的开发。  Julia 将来会成为 ML 的重要编程语言吗？    提交人    /u/al_coper   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3ug5g/d_worth_learn_julia_for_ml_in_the_future/</guid>
      <pubDate>Thu, 29 Aug 2024 05:06:28 GMT</pubDate>
    </item>
    <item>
      <title>[N] 一个时代的终结？角色 AI 关闭旧网站，引发用户愤怒。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3rxhk/n_end_of_an_era_character_ai_shuts_down_old_site/</link>
      <description><![CDATA[角色 AI 关闭背后的故事。这是真正发生的事情。    提交人    /u/Bernard_L   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3rxhk/n_end_of_an_era_character_ai_shuts_down_old_site/</guid>
      <pubDate>Thu, 29 Aug 2024 02:44:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对多个市场中多种产品的需求预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3prgk/d_demand_forecasting_for_several_products_across/</link>
      <description><![CDATA[对于那些在零售或消费品行业工作并为公司开发了“自下而上”需求预测模型的人，我很想听听你们是如何实现的，以及哪些方法有效。我目前正在权衡我想要尝试的实验模型选项（看看它们是否能胜过简单的基线模型）。我正在考虑的主要两个选项是：  基于 RNN 的模型。特别是 DeepAR 是我正在关注的一个，因为它引起了很多关注。 一些建立在梯度提升树之上的策略。各种预测竞赛（例如 M4 和 M5）都表明，利用 LightGBM 的策略表现非常出色。  虽然传统的 TS 模型（如 ARIMA/ETS）在单变量情况下很难被击败，但在开发涉及“交叉训练”的全局模型时似乎并非如此，这就是我对这两个选项感兴趣的原因。  我对 DeepAR 有所保留，主要是因为我不完全理解它，而且我尝试阅读论文好几次…… 我读过的 LightGBM 策略更容易理解，而且似乎不太容易正确，所以我倾向于选择此选项。话虽如此，如果能听到大家说说哪些策略对您有效，那就太好了。您或您认识的人是否在这种情况下使用过 DeepAR 或梯度提升树之类的东西，如果是，效果如何？     由   提交  /u/Asleep-Importance-10   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3prgk/d_demand_forecasting_for_several_products_across/</guid>
      <pubDate>Thu, 29 Aug 2024 00:59:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 澄清 VAE 中的“重新参数化技巧”以及为什么它是一个技巧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3ohje/d_clarification_on_the_reparameterization_trick/</link>
      <description><![CDATA[我一直在研究变分自动编码器 (VAE)，并且不断遇到术语“重新参数化技巧”。据我所知，该技巧涉及使用公式 (X = 平均值 + 标准偏差 * Z) 从正态分布中抽样，其中 Z 是从标准正态分布中抽取的。此公式似乎是从正态分布中抽样的标准方法 这是我的困惑： 为什么这是一个技巧？ 重新参数化“技巧”通常被强调为一个聪明的技巧，但对我来说，它似乎是转换公式的直接应用。如果 ( X = 平均值 + 标准差 * Z ) 是从正态分布中采样的唯一方法，那么为什么重新参数化技巧被认为特别具有创新性？ 我理解该技巧允许通过采样过程进行反向传播。但是，似乎使用 ( X = 平均值 + 标准差 * Z ) 是从给定 ( 平均值 ) 和 ( 标准差 ) 的正态分布中生成样本的唯一方法。除了确保可区分性之外，这个技巧还有什么特别之处？ 这是我的思维过程：我们从编码器获得平均值和标准差，并从中采样，唯一且最明显的方法是“X = 平均值 + 标准差 * Z”。 有人可以帮忙解释为什么重新参数化技巧被称为“技巧”吗？ 提前感谢您的见解！    提交人    /u/SwaroopMeher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3ohje/d_clarification_on_the_reparameterization_trick/</guid>
      <pubDate>Wed, 28 Aug 2024 23:57:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于符号距离函数和体积数据结构的 Pytorch 库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3kdlb/p_pytorch_library_for_signed_distance_function/</link>
      <description><![CDATA[      体积数据结构库，通过提供梯度和不破坏自动微分，与 pytorch 生态系统良好交互。查看存储库：https://github.com/UM-ARM-Lab/pytorch_volumetric 您也可以通过以下方式安装 pip install pytorch-volumetric  在特定配置中对机器人进行有符号距离场查找的示例动画： https://i.redd.it/tcjvib45zgld1.gif    提交人    /u/LemonByte   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3kdlb/p_pytorch_library_for_signed_distance_function/</guid>
      <pubDate>Wed, 28 Aug 2024 21:05:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 交互式分割模型训练的最佳工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3h8gw/d_best_tool_for_interactive_segmentation_model/</link>
      <description><![CDATA[我有一堆 2D 灰度图像，我想为它们训练一个（多标签）分割模型。交互式训练这种模型的最佳工具是什么？ 即，我想要：  手动标记（非常）少量图像 在这些分割蒙版上训练/微调模型（例如，预训练 unet 的头部） 通过查看更多图像 + 校正预测蒙版来迭代细化/重新训练该模型，例如使用点击和涂鸦  我知道 DeepEdit、DeepGrow 等方法，并且从技术上了解它们的工作原理。 但是，我正在寻找最简单（最省时）的方法和工具来在实践中做到这一点，而无需自己编写代码。 主动学习将是一个加分项，但不是必需的。 我是一名 ML 研究员，因此如果我必须更改一些技术设置/配置文件或其他任何内容，那也没问题。我只是不想浪费时间重新实现现成的标准东西。    提交人    /u/jhinboy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3h8gw/d_best_tool_for_interactive_segmentation_model/</guid>
      <pubDate>Wed, 28 Aug 2024 18:15:17 GMT</pubDate>
    </item>
    <item>
      <title>（R）法学硕士的回应分散度与其领域知识成反比</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3h33e/r_an_llms_response_dispersion_inversely/</link>
      <description><![CDATA[我正在寻找对我最近的 ArXiv 预印本的建设性反馈，下游知识基准测试无需数据集：响应分散与领域特定 QA 的准确性成反比。 激励思想是，如果您多次向（聊天机器人调整的）LLM 询问有关某个主题的相同问题，并且每次询问时它的答案都不一致（即“它的回答是分散的”），那么它可能对该主题了解不多。预印本本身主要是关于形式化这种直觉并通过实验对其进行验证。此外，还附带一个业务用例，即如果资源受限的开发人员正在构建由 LLM 驱动的应用程序，并且在其他所有条件相同的情况下，他们试图选择在特定领域具有最佳“知识”的 LLM（如果假设存在，则由特定于领域的 QA 基准数据集来衡量），那么他们不必经历构建基准数据集的繁琐过程，而只需执行几乎免费的过程来计算该域中的响应分散（如预印本中定义和验证的），以选择最佳 LLM（以 QA 基准准确度衡量）。    提交人    /u/robert_sim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3h33e/r_an_llms_response_dispersion_inversely/</guid>
      <pubDate>Wed, 28 Aug 2024 18:09:10 GMT</pubDate>
    </item>
    <item>
      <title>“边缘写作 (WiM)”——一种更好的长上下文 LLM 推理模式，解决了中间丢失的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f3d3uo/writing_in_the_margins_wim_a_better_inference/</link>
      <description><![CDATA[  由    /u/samjulien  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f3d3uo/writing_in_the_margins_wim_a_better_inference/</guid>
      <pubDate>Wed, 28 Aug 2024 15:28:08 GMT</pubDate>
    </item>
    <item>
      <title>[P] 深入探讨旋转位置嵌入（RoPE）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f38dpu/p_a_deep_dive_on_rotary_positional_embeddings_rope/</link>
      <description><![CDATA[我发表了一篇关于 transformer 中旋转位置嵌入 (RoPE) 的深入研究！探索理论和实施细节。如果您有兴趣，请阅读！RoPE    提交人    /u/Adventurous-Ad7258   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f38dpu/p_a_deep_dive_on_rotary_positional_embeddings_rope/</guid>
      <pubDate>Wed, 28 Aug 2024 12:01:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] - KAN（Kolmogorov-Arnold 网络）有未来吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f34nou/d_kans_kolmogorovarnold_networks_have_a_future/</link>
      <description><![CDATA[大家好， 我最近看到 Jamba-1.5 已经发布。 https://huggingface.co/collections/ai21labs/jamba-15-66c44befa474a917fcf55251?utm_source=tldrai 总的来说，我看到已经发表了几篇关于 Mamba 及其衍生物的文章。对空间状态模型的兴趣并没有消退。SSM 已经成功地为自己开辟了一个利基市场。例如，在生物信息学中，人们对其对长序列进行建模的能力特别感兴趣。我已经看到了各种各样的研究将它们用于 DNA 序列，而且一些同事也对测试它们表现出了兴趣。 然而，随着时间的推移，我们已经看到许多模型似乎在与 transformer 竞争（RWKV、Hyena Hierarchy、xLSTM 等等）。 https://arxiv.org/abs/2305.13048 https://arxiv.org/abs/2302.10866 https://arxiv.org/abs/2405.04517 对于所有这些其他架构，兴趣似乎正在减弱，但一开始却有很多炒作。  现在，我发现 Kolmogorov-Arnold 网络特别有趣。  https://arxiv.org/abs/2404.19756 我想知道炒作是否也在这里结束，它们最终会被遗忘，或者是否有它们会找到应用的情况。也许是他们可以立足的利基市场，或者它们是否也将更多地作为一个有趣的历史实验而不是被社区改编的东西。     提交人    /u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f34nou/d_kans_kolmogorovarnold_networks_have_a_future/</guid>
      <pubDate>Wed, 28 Aug 2024 08:03:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] Meta 的 Sapiens 适用于人类视觉任务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f321ir/r_sapiens_by_meta_for_human_vision_tasks/</link>
      <description><![CDATA[      代码：https://github.com/facebookresearch/sapiens 论文：https://arxiv.org/abs/2408.12569 针对姿势 + seg + 深度 + 法线进行微调的模型    提交人    /u/rawalkhirodkar   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f321ir/r_sapiens_by_meta_for_human_vision_tasks/</guid>
      <pubDate>Wed, 28 Aug 2024 05:03:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过 Google 研究团队微调的 SD1.4 模型可玩 20FPS 的 Doom</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f31uye/r_playable_20fps_doom_via_a_finetuned_sd14_model/</link>
      <description><![CDATA[  由    /u/greentfrapp  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f31uye/r_playable_20fps_doom_via_a_finetuned_sd14_model/</guid>
      <pubDate>Wed, 28 Aug 2024 04:52:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f0ybbs/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f0ybbs/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Aug 2024 15:00:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>