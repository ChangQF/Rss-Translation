<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Fri, 05 Jul 2024 15:16:04 GMT</lastBuildDate>
    <item>
      <title>[D] Mapper（Mapper算法）区分噪声和显著拓扑结构的能力的理论极限是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvzsy8/d_what_are_the_theoretical_limits_of_mappers/</link>
      <description><![CDATA[大家觉得怎么样？    提交人    /u/ICEpenguin7878   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvzsy8/d_what_are_the_theoretical_limits_of_mappers/</guid>
      <pubDate>Fri, 05 Jul 2024 14:50:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 推荐一些关于模型合并的好资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvzr9d/d_suggest_any_good_resources_for_model_merging/</link>
      <description><![CDATA[我知道有合并模型的工具，但我想要一些关于合并模型的理论材料。任何讨论如何合并模型的博客、文章或研究论文都会有所帮助。 我将不胜感激您的帮助 :) 谢谢    提交人    /u/Ok_Cartographer5609   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvzr9d/d_suggest_any_good_resources_for_model_merging/</guid>
      <pubDate>Fri, 05 Jul 2024 14:48:53 GMT</pubDate>
    </item>
    <item>
      <title>[R]，[D]，我想开发一个机器学习算法（我解释得很详细，请查看）并通过 DSP STM32F407G 应用它</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvxl3w/r_d_i_want_to_develop_a_machine_learning/</link>
      <description><![CDATA[大家好， 问题：我正在使用一种仪器，我们经常遇到频率丢失的问题，即频率信号突然丢失，即变为零，这会导致数据丢失，如下图所示。黄色是从光电探测器接收到的频率信号，紫色是解调数据（我们想要的输出）。当频率变为零时，紫线保持不变，因此数据丢失。 https://preview.redd.it/h2t0hnne8pad1.png?width=1026&amp;format=png&amp;auto=webp&amp;s=a89f72406f0df90d77f24b412c7b533373512f14 解决方案： 为了解决这个问题，我心里有一个解决方案，即我应该开发一种基于机器学习或 Python 的算法其工作原理如下， 当数据没有丢失时，即当频率不为零且等于（我的期望频率假设为 100Hz）时，微处理器的输出应为零。 如果检测到 0 频率，则输出应为前一个频率，即 100Hz 的输出。通过这种方式，我想将数据存储在缓冲区中，当需要时（在 0 Hz 时）应使用缓冲区数据。 我脑海中的想象是我的代码将有两个输入，即黄色和紫色，一个输出紫色。 紫色将存储在缓冲区中微秒或秒。 将持续监控黄色值，如果检测到 0 频率，则输出先前的缓冲区值，否则输出应为 0。 为此，我的整个电路都是模拟的，我将使用 DSP STM32F407G 微处理器完成所需的任务，我将仅创建模拟求和电路来添加信号。 我对这个机器学习或编码程序还不熟悉，所以我需要你的助手帮助我开发我想要的解决方案的算法。 真诚的。 Umair &quot;[研究]&quot;, &quot;[R]&quot;, &quot;[讨论]&quot;、&quot;[D]&quot;    提交人    /u/umair1181gist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvxl3w/r_d_i_want_to_develop_a_machine_learning/</guid>
      <pubDate>Fri, 05 Jul 2024 13:08:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] TensorFlow 概率的 torch 等价性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvwfov/p_torch_equivalence_of_tensorflow_probability/</link>
      <description><![CDATA[大家好， 我已经使用 tensorflow 很多年了，但对 pytorch 的经验有限。我正在考虑在 pytorch 上构建我的下一个项目。有没有人有在 pytorch 中进行近似推理的经验，有没有 tensorflow 概率的等效包？ 谢谢！    提交人    /u/South-Conference-395   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvwfov/p_torch_equivalence_of_tensorflow_probability/</guid>
      <pubDate>Fri, 05 Jul 2024 12:08:19 GMT</pubDate>
    </item>
    <item>
      <title>ECCV 相机就绪论文 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvvzpm/camera_ready_paper_for_eccv_discussion/</link>
      <description><![CDATA[嗨， 我的论文被 ECCV 接受了，在反驳期间，我们从审稿人那里得到了很多有用的反馈。我们想把它们包括在主要论文中，但超出了 14 页的限制。为了解决这个问题，我们想将一个消融研究图表和讨论从主要论文移到补充材料中。当然，我们会在主要图表中引用它，但允许吗？    提交人    /u/dn8034   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvvzpm/camera_ready_paper_for_eccv_discussion/</guid>
      <pubDate>Fri, 05 Jul 2024 11:43:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否应该从整个数据集中删除异常值，还是仅从训练集中删除？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvupyf/d_should_outliers_be_removed_from_the_full/</link>
      <description><![CDATA[我想删除异常值以检查它是否可以改进我的模型。 我应该在整个数据集上还是仅在训练数据集上删除它们？ 如何使用欠采样来平衡我的数据集？应该在整个数据集上进行平衡还是仅在训练集上进行平衡？    提交人    /u/fabiopires10   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvupyf/d_should_outliers_be_removed_from_the_full/</guid>
      <pubDate>Fri, 05 Jul 2024 10:22:26 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在 flax 中加载 torchvision.resnet50 权重的工具。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvttlv/p_a_tool_to_load_torchvisionresnet50_weights_in/</link>
      <description><![CDATA[请原谅不正确的风格。 我编写了一个独立的工具来在 flax 中加载 torchvision 默认权重，因为我找不到任何原因来解释为什么我无法重现使用预训练权重的某些基线。目标是简单。 目前，它假定 resnet50 为要转换的模型。 试图使其保持可读性并易于适用于其他模型，希望它能帮助论坛上的其他人。欢迎提供反馈！    提交人    /u/elevated_quark   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvttlv/p_a_tool_to_load_torchvisionresnet50_weights_in/</guid>
      <pubDate>Fri, 05 Jul 2024 09:19:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 二值图像掩码的特征提取和相似度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvtsye/d_feature_extraction_similarity_of_a_binary_image/</link>
      <description><![CDATA[大家好！我只是想获得一些输入，所以我尝试了 SIFT 和 LofT 等特征匹配方法，但它们在图像二进制掩码上表现不佳，尤其是因为 LoFT 是在室内/室外图像上训练的。它在某些图像上表现良好，但当我尝试将一张图像与完全相同的图像进行比较时，只是旋转/放大，相似度得分变得非常糟糕&lt;0.5。任何我可以查找和实验的提示或方法都将不胜感激，谢谢！！    提交人    /u/ThrowRA_2983839   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvtsye/d_feature_extraction_similarity_of_a_binary_image/</guid>
      <pubDate>Fri, 05 Jul 2024 09:17:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 情绪分类与分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvrpbs/p_emotion_classification_analysis/</link>
      <description><![CDATA[大家好， 我想分享我使用 flask 构建的关于机器学习的项目，用户可以在其中表达自己的情感并进行以下分类（悲伤、快乐、爱、愤怒、恐惧、惊讶）。我们使用 CNB 模型进行文本分类，准确率为 88%。 你可以在这里尝试： https://emotionclassification.pythonanywhere.com 注意： 预测可能会遇到意外的表情结果。 源代码：  Github：https://github.com/nordszamora/Emotion-Expression     submitted by    /u/ThePawners   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvrpbs/p_emotion_classification_analysis/</guid>
      <pubDate>Fri, 05 Jul 2024 06:51:05 GMT</pubDate>
    </item>
    <item>
      <title>训练 MNIST 的扩散模型 [P][R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvr4i8/training_a_diffusion_model_for_mnist_pr/</link>
      <description><![CDATA[我学习了 VAE 和扩散模型的理论，并尝试使用 M1 MacBook 在 MNIST 上训练一个基本的扩散模型（以更熟悉这个概念）。几个小时过去了，但没有完成一个 epoch。我知道仅使用 M1 芯片进行训练对于严肃的 DL 来说并不理想。我只是想知道这个速度对于这项任务是否正常，或者我的代码是否有问题。    提交人    /u/mziycfh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvr4i8/training_a_diffusion_model_for_mnist_pr/</guid>
      <pubDate>Fri, 05 Jul 2024 06:12:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] Grad-CAM 医学成像可视化问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvkg4j/r_problems_with_gradcam_visualization_for_medical/</link>
      <description><![CDATA[嗨，我在灰度胸部热图数据集上训练了一些 ImageNet 模型来检测异常后，使用了 grad-CAM 可视化。但是，几乎所有网络都关注无关特征，如手臂、肩膀、胸部下方等，除了相关区域（胸部）之外的所有区域。我甚至尝试过裁剪图像，但效果并不明显，因为它仍然关注随机点。我们如何才能强制模型关注胸部？此外，尽管我获得了很高的准确率，但模型关注无关特征（如 Grad-CAM 所见）这一事实是否会使我的结果无效？  更新：对于一个模型，在第 5 个训练阶段，它实际上显示了比第 10 个训练阶段更相关的可视化（胸部区域）……不确定发生了什么    由    /u/Low-Literature-9699  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvkg4j/r_problems_with_gradcam_visualization_for_medical/</guid>
      <pubDate>Thu, 04 Jul 2024 23:37:39 GMT</pubDate>
    </item>
    <item>
      <title>[N] Moshi 是首款向所有人开放的语音人工智能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvc2no/n_moshi_very_first_voiceenabled_ai_openly/</link>
      <description><![CDATA[以下是 主题演讲视频 和 新闻稿，来自 Kyutai 实验室 该模型的延迟非常低，并且能够（目前为英文）进行非常自然的对话（限制为 5 分钟）。您可以从实验室网站在线试用（欧盟和美国版本）。 Moshi 背后的技术将按照新闻稿中所述稍后开放：  借助 Moshi，Kyutai 打算为人工智能的开放研究和整个生态系统的发展做出贡献。模型的代码和权重将很快免费共享，这对于此类技术而言也是前所未有的。它们将对该领域的研究人员和从事语音产品和服务的开发人员都很有用。因此，可以根据需要深入研究、修改、扩展或专门研究该技术。社区将能够扩展 Moshi 的知识库和事实性，而这些目前在这种轻量级模型中是故意限制的，同时利用其无与伦比的语音交互功能。     提交人    /u/jartock   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvc2no/n_moshi_very_first_voiceenabled_ai_openly/</guid>
      <pubDate>Thu, 04 Jul 2024 17:14:12 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型中的似然计算[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dvbzrh/likelihood_computation_in_diffusion_models_p/</link>
      <description><![CDATA[我目前正在研究扩散模型（主要是本文 (Song et al.)和本文 (Song et al.)中涉及的 SDE 方法），我对这个关于似然计算的陈述很好奇：他们指出，为了获得生成数据 $log(q_\theta(y))$ 的精确似然，需要依赖概率流 ODE，它具有与用于训练的 SDE 相同的边际，因为 SDE 似然性不易处理。 问题是我不明白为什么：逆 SDE 的每一步都相当于对数据应用与 ODE 几乎相同的变换，只是我们还添加了一些布朗噪声： https://preview.redd.it/es​​5l2ybuajad1.png?width=758&amp;format=png&amp;auto=webp&amp;s=76e189e4c529c49210fed98d0b7cf6787e445032 为了计算似然性，我们想知道每一步中 $yt$ 和 $y\{t+dt}$ 之间变换的雅可比矩阵，对我来说这在 SDE 中似乎是可行的，因为布朗运动只是一些高斯运动噪声不涉及 $y_t$（但我认为这就是我对 SDE 的理解不足的地方）。 这里有人能解释一下这个推理中的问题吗？    提交人    /u/Antoine_m8   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dvbzrh/likelihood_computation_in_diffusion_models_p/</guid>
      <pubDate>Thu, 04 Jul 2024 17:10:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 杰出机器学习工程师的稀有技能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dv2thm/d_rare_skills_of_execptional_ml_engineers/</link>
      <description><![CDATA[大家好，ML 社区！ 无论你的头衔是什么（DS/工程经理/工程总监/ML 工程...），你工作场所中的 ML 工程师拥有哪些罕见技能，使他们真正从其他人中脱颖而出（在软技能和硬技能领域）？如果可能的话，请说明你的立场——不同角色如何看待这个话题可能会很有趣。 谢谢！    提交人    /u/Avistian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dv2thm/d_rare_skills_of_execptional_ml_engineers/</guid>
      <pubDate>Thu, 04 Jul 2024 09:29:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ds3fbp/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ds3fbp/d_simple_questions_thread/</guid>
      <pubDate>Sun, 30 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>