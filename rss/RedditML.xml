<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sun, 30 Jun 2024 12:26:08 GMT</lastBuildDate>
    <item>
      <title>[D] 可疑的 ML 结果——这些输出实际上来自真实模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ds08px/d_suspicious_ml_results_are_these_outputs/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ds08px/d_suspicious_ml_results_are_these_outputs/</guid>
      <pubDate>Sun, 30 Jun 2024 12:21:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 可以从训练数据中的分散提示中推断出受审查的知识</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drzalw/r_llms_can_infer_censored_knowledge_from/</link>
      <description><![CDATA[https://arxiv.org/abs/2406.14546 “我们研究归纳式非语境推理 (OOCR)，这是一种概括类型，其中 LLM 从分布在训练文档中的证据中推断出潜在信息，并将其应用于下游任务而无需语境学习。”    提交人    /u/20231027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drzalw/r_llms_can_infer_censored_knowledge_from/</guid>
      <pubDate>Sun, 30 Jun 2024 11:22:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] 什么是特征向量？：线性代数中基本概念之一的 5 分钟视觉指南。🧠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drz2bl/r_what_is_an_eigenvector_a_5minute_visual_guide/</link>
      <description><![CDATA[      TL;DR：矩阵 A 的特征向量 x 是与 A 相乘时方向不变的向量。 特征向量是机器学习和数据科学中许多先进技术的基石。特征向量是降维技术、数据转换和特征提取的核心。 它们在著名的页面排名算法中得到了应用，最初的 Google 搜索就是基于该算法。Netflix 的推荐系统也将其作为协同过滤和向用户推荐相关电影的核心。 什么是特征向量？：视觉指南。    提交人    /u/ml_a_day   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drz2bl/r_what_is_an_eigenvector_a_5minute_visual_guide/</guid>
      <pubDate>Sun, 30 Jun 2024 11:07:24 GMT</pubDate>
    </item>
    <item>
      <title>[P] 快速缓存：穷人的零样本视觉指南-LLM 分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dry5a8/p_prompt_caching_poor_mans_guide_to_zero_shot/</link>
      <description><![CDATA[        由    /u/themathstudent  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dry5a8/p_prompt_caching_poor_mans_guide_to_zero_shot/</guid>
      <pubDate>Sun, 30 Jun 2024 10:03:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 推荐有关 ML 研究/新闻/主要公司的 RSS 提要？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drpk01/d_recommended_rss_feeds_on_ml_research_news_major/</link>
      <description><![CDATA[我正在寻找相关的 RSS 源来关注，我希望涵盖当今 ML 的各个方面：研究、公司、MLOps 等。 我能找到的关于 RSS 源的最后一篇文章是 2 年前的，我认为已经过去了足够的时间值得更新。 您最推荐的 RSS 源是什么？    提交人    /u/fliiiiiiip   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drpk01/d_recommended_rss_feeds_on_ml_research_news_major/</guid>
      <pubDate>Sun, 30 Jun 2024 00:48:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 目前经过实战检验的最先进的多元时间序列回归机制是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drohkd/d_whats_the_current_battletested_stateoftheart/</link>
      <description><![CDATA[目前久经考验的最先进的多元时间序列回归机制是什么？使用多个时间序列来预测单个值。 对于多个半平稳时间序列。 我所说的“久经考验”是指至少 5% 的行业已经在使用它，或者目前正在大力采用它。    提交人    /u/igaloly   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drohkd/d_whats_the_current_battletested_stateoftheart/</guid>
      <pubDate>Sat, 29 Jun 2024 23:52:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] DDIM 反转和关键调整，以 SD 2.1 为基础实现人脸编辑功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drn9zs/p_ddim_inversion_and_pivotal_tuning_to_achieve/</link>
      <description><![CDATA[      Github : https://github.com/OutofAi/StableFace https://preview.redd.it/clulwrsnbl9d1.png?width=2048&amp;format=png&amp;auto=webp&amp;s=53d002746d951fb35bfeb928eed42644d05430e4    提交人    /u/TerryCrewsHasacrew   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drn9zs/p_ddim_inversion_and_pivotal_tuning_to_achieve/</guid>
      <pubDate>Sat, 29 Jun 2024 22:52:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] GraphReader：一种基于图形的 AI 代理系统，旨在通过将长文本构建成图形并使用代理自主探索该图形来处理长文本</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drjcfz/r_graphreader_a_graphbased_ai_agent_system/</link>
      <description><![CDATA[    /u/valdanylchuk   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drjcfz/r_graphreader_a_graphbased_ai_agent_system/</guid>
      <pubDate>Sat, 29 Jun 2024 19:46:56 GMT</pubDate>
    </item>
    <item>
      <title>[D]: 微调 NuExtract-tiny</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drjbhj/d_finetune_nuextracttiny/</link>
      <description><![CDATA[我尝试微调 NuExtract-tiny 以从文本中提取以下信息： { &quot;document_type&quot;: &quot;&quot;, &quot;document_identifier&quot;: &quot;&quot;, &quot;subject&quot;: &quot;&quot;, &quot;effective_date&quot;: &quot;&quot;, &quot;revision_date&quot;: &quot;&quot;, &quot;publishing_date&quot;: &quot;&quot;, }  因此，我使用 gpt-4o 生成了合成训练数据，它看起来像 processed_data.jsonl 文件中存在的数据。我使用了大约 5000 个训练样本。我已将微调 NuExtract-tiny 的日志附在我的代码上。查看 validation_loss，它似乎没有经过太多微调。我有以下观察：  我比较了微调模型的结果，它们非常糟糕，比原始 NuExtract-tiny 差得多 此外，推理速度变得非常慢，即使原始模型和微调模型的大小相同。  我手动验证了使用 gpt-4o 生成的训练数据质量良好。 关于可能出现问题的任何建议？任何帮助都将不胜感激。我正在附加 Jupyter 笔记本和数据的链接 笔记本链接：https://drive.google.com/file/d/1ZDMVAGSIPXbkWDaJuCxcFLLduKZLqXjQ/view?usp=sharing processed_data.jsonl 链接：https://drive.google.com/file/d/11NYOINkIh4P-a3loB9KD6-C-XOs0Bfl8/view?usp=sharing 以下是微调模型和原始模型的比较： text = &quot;&quot;&quot;德克萨斯州医疗补助提供者程序手册 2022 年 2 月提供者手册 妇科、产科和计划生育第 19 条服务手册 德克萨斯州医疗补助和医疗保健合作伙伴关系 (TMHP) 是与德克萨斯州卫生和公共服务委员会签订合同的德克萨斯州医疗补助的索赔管理员。&quot;&quot;&quot;  给定模式： schema = &quot;&quot;&quot;{&quot;document_type&quot;: &quot;&quot;, &quot;document_identifier&quot;: &quot;&quot;, &quot;subject&quot;: &quot;&quot;, &quot;effective_date&quot;: &quot;&quot;, &quot;revision_date&quot;: &quot;&quot;, &quot;publishing_date&quot;: &quot;&quot;}&quot;&quot;&quot;  微调模型输出： { &quot;document_type&quot;: &quot;Handbook&quot;, &quot;document_identifier&quot;: &quot;&quot;, &quot;subject&quot;: &quot;Gynecological, Obstetrics, and Family &quot;effective_date&quot;: &quot;&quot;, &quot;revision_date&quot;: &quot;&quot;, &quot;publishing_date&quot;: &quot;&quot; }  原始模型输出： { &quot;document_type&quot;: &quot;Provider Procedures Manual&quot;, &quot;document_identifier&quot;: &quot;Provider Handbooks&quot;, &quot;subject&quot;: &quot;Gynecological, Obstetrics, and Family Planning Title XIX Services Handbook&quot;, &quot;effective_date&quot;: &quot;February 2022&quot;, &quot;revision_date&quot;: &quot;&quot;, &quot;publishing_date&quot;: &quot;&quot; }  您可以清楚地看到，微调模型失败了。    提交人    /u/n0pe09   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drjbhj/d_finetune_nuextracttiny/</guid>
      <pubDate>Sat, 29 Jun 2024 19:45:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 同事最近告诉我，认为“法学硕士能够思考/理解”的人都是从法学硕士开始从事 ML/NLP 职业的人。我很好奇你的想法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1drd3tv/d_coworkers_recently_told_me_that_the_people_who/</link>
      <description><![CDATA[我自己在这个领域并没有待很长时间。我在 2016-2017 年左右开始攻读硕士学位，当时 Transformers 开始流行起来。我已经在行业中工作了一段时间，最近刚刚加入一家专注于 NLP 的公司，担任 MLE。 在工作中，我们最近进行了一场辩论/讨论，讨论主题是 LLM 是否能够具备理解和思考的能力。我们讨论了 Emily Bender 和 Timnit Gebru 关于 LLM 是随机鹦鹉的论文，然后从那里开始。 意见大致各占一半：我们中的一半（包括我自己）认为 LLM 是 BERT 或 GPT-2 等模型的简单扩展，而其他人则认为 LLM 确实能够理解和领悟文本。在我的高级工程师发表标题中的评论后，我注意到一件有趣的事情，那就是那些认为 LLM 能够思考的人要么是在 LLM 成为既定事实后进入 NLP 的人，要么原本来自计算机视觉等不同领域，后来转行了。 我很好奇其他人对此的看法。我有点吃惊，因为我没想到 LLM 是有意识的理解生物的观点会在实际从事该领域的人中如此普遍；这是我从非 ML 人士那里听到的更多的事情。这些人也不只是新手工程师，我团队中的每个人都有在顶级 ML 场所发表文章的经验。    提交人    /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1drd3tv/d_coworkers_recently_told_me_that_the_people_who/</guid>
      <pubDate>Sat, 29 Jun 2024 15:00:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用嵌入模型时...</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dr86u3/d_when_using_embedding_models/</link>
      <description><![CDATA[当使用嵌入模型将新的、大量数据合并到 GPT-4 等 LLM 中时，是否需要手动准备数据（清理、分类等），还是这些模型会自动处理？    提交人    /u/SnooBeans2906   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dr86u3/d_when_using_embedding_models/</guid>
      <pubDate>Sat, 29 Jun 2024 10:26:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 DINO 模型要对教师编码器使用增强功能？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dr6aad/d_why_do_dino_models_use_augmentations_for_the/</link>
      <description><![CDATA[如标题所示 - DINO 和 DINOv2 使用增强来输入教师网络。为什么会这样？从“最干净”的数据版本生成教师表示不是更有意义吗？真的很想听听他们所做事情背后的直觉。    提交人    /u/clywac2   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dr6aad/d_why_do_dino_models_use_augmentations_for_the/</guid>
      <pubDate>Sat, 29 Jun 2024 08:07:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人见过 Kolmogorov-Arnold 网络在现实中的应用吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dqr9gh/d_anyone_see_any_real_usage_of_kolmogorovarnold/</link>
      <description><![CDATA[KAN 在各处（包括 Reddit）都备受热捧，许多人都对它赞不绝口，尽管并非都是好评。现在已经过去 3 个月了。有没有人看到任何可以证实或反驳“信徒”的东西？就我个人而言，我没有看到任何值得注意的 KAN 采用​​情况。希望听听社区的意见。    提交人    /u/Sad-Journalist752   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dqr9gh/d_anyone_see_any_real_usage_of_kolmogorovarnold/</guid>
      <pubDate>Fri, 28 Jun 2024 18:55:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] “Grok” 有太多不同的含义</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dqpyb3/d_grok_means_way_too_many_different_things/</link>
      <description><![CDATA[我厌倦了到处看到这个词，而且它每次在同一个领域都有不同的含义。对我来说，第一次是当伊隆·马斯克推出并大肆宣传 Twitter 的新产品（现在不是新的，但当时是）“Grok AI”时，然后我阅读了更多论文，发现了一个相当惊人的发现，显然地球上的每个人都知道了一段时间，除了我之外，那就是在某个点之后，过度拟合模型开始能够概括，这摧毁了我之前的许多先入为主的观念以及我在学校和其他地方学到的东西。但这种现象也被称为“Grok”，然后有一篇基于 Grok 定义的新“GrokFast”论文，还有“Groq”，不要与其他两个“Grok”混淆，更不用说伊隆·马斯克将他的人工智能装备命名为“xAI”机械可解释性人们已经在使用该术语作为“可解释的 AI”的缩写，这对我来说太多了    提交人    /u/Traditional_Land3933   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dqpyb3/d_grok_means_way_too_many_different_things/</guid>
      <pubDate>Fri, 28 Jun 2024 17:59:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    </channel>
</rss>