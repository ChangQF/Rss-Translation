<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 07 Apr 2024 21:12:42 GMT</lastBuildDate>
    <item>
      <title>[讨论]新的 TensorFlow 版本 - 发生了什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1byfdbs/discussion_new_tensorflow_version_what_happened/</link>
      <description><![CDATA[我使用 TPU VM Pod 来训练我的模型。我最近将系统运行时更新为 tf-2.16.1-pod-pjrt，天哪，它有很多错误吗？这确实是我见过的最糟糕的 TensorFlow。我通常对 TF 模型对象进行子类化并覆盖训练、测试、构建...函数。由于某种原因，它不仅忽略了我的覆盖功能，而且甚至不允许我正确使用 GRUCell 之类的东西。值得庆幸的是，我在此之前制作了我的工作 GRUCell 层并且有效，但 Keras 层甚至无法使用它的事实是可怕的。我喜欢 TensorFlow，我希望他们能解决这个问题。我也很乐意帮助遇到这些问题的任何人。   由   提交 /u/Onlyheretohelp_you   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1byfdbs/discussion_new_tensorflow_version_what_happened/</guid>
      <pubDate>Sun, 07 Apr 2024 21:04:28 GMT</pubDate>
    </item>
    <item>
      <title>[P]寻找低延迟逐字TTS模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bycnh5/p_looking_for_low_latency_wordbyword_tts_model/</link>
      <description><![CDATA[我正在致力于构建一个 Jarvis 风格的对话式 AI 助手，它在幕后利用大型语言模型 (LLM)。然而，我希望让助理在法学硕士开始生成响应时立即开始讲话，使体验尽可能无缝和自然。 为了实现这一点，我需要一个文本-语音转换 (TTS) 模型可以以极低的延迟运行，并在文本流进入时以逐字或逐音素的方式生成音频。理想情况下，TTS 听起来应该自然且具有对话性，没有任何机器人或不自然的品质。 传统的 TTS 模型通常设计为对完整的句子或段落进行操作，这会带来显着的延迟，并且不适合我的用例。我研究了一些专门的模型，例如 Mozilla 的 Felina 和 Google 的 Streaming Speech，但我正在努力寻找满足我所有要求的解决方案。 有人对低延迟、逐字有任何建议吗？ -word TTS 模型   由   提交 /u/rumble_ftw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bycnh5/p_looking_for_low_latency_wordbyword_tts_model/</guid>
      <pubDate>Sun, 07 Apr 2024 19:17:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] UNet 如何使用交叉注意力与 CLIP 文本嵌入来生成最终的噪声图像？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bycklh/d_how_does_the_unet_use_cross_attention_with_clip/</link>
      <description><![CDATA[目前，我对 Unet 中每个上采样和下采样块如何实际使用交叉注意力有一个不错的理解。最终，交叉注意力似乎产生了这些类似注意力的热图，这些热图基本上指示了图像中每个像素与提示中的单词的相关性。 ​ 我的困惑在于如何使用该注意力图来生成最终图像。即交叉注意力如何与 Unet 模型集成以产生最终输出（减少噪声图像） 我特别困惑，因为注意力图似乎是 4 个维度（image_h、image_w、#heads、提示中的单词）那么我们如何将其转换为二维（image_h，image_w） 希望这个问题有意义。抱歉，如果看起来令人困惑。   由   提交 /u/Jordanoer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bycklh/d_how_does_the_unet_use_cross_attention_with_clip/</guid>
      <pubDate>Sun, 07 Apr 2024 19:14:33 GMT</pubDate>
    </item>
    <item>
      <title>[R]无日程学习——一种新的培训方式；德法齐奥等人。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bybdlv/r_schedulefree_learning_a_new_way_to_train/</link>
      <description><![CDATA[只是想收集一些关于 Aaron Defazio 刚刚发布的新发布的无调度优化器的想法和意见：https://github.com/facebookresearch/schedule_free 他已经在 X 上调侃了一段时间：https://twitter.com/aaron_defazio 初步测试看起来相当不错，它似乎已经取代了 TorchStudio 中的默认优化器。   由   提交/u/Davidobot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bybdlv/r_schedulefree_learning_a_new_way_to_train/</guid>
      <pubDate>Sun, 07 Apr 2024 18:26:22 GMT</pubDate>
    </item>
    <item>
      <title>使用变形金刚为任何歌曲生成节奏游戏节拍图 [R] [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bya93f/generate_rhythm_game_beatmaps_for_any_song_using/</link>
      <description><![CDATA[       由   提交 /u/sedthh   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bya93f/generate_rhythm_game_beatmaps_for_any_song_using/</guid>
      <pubDate>Sun, 07 Apr 2024 17:39:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么人们在 Arxiv 上上传他们的作品，而不是提交会议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bya23i/d_why_do_people_upload_their_work_on_arxiv_not/</link>
      <description><![CDATA[纯粹是我的好奇心。 我知道这种情况，当一篇论文在会议或期刊上被接受，并且可以上传时他们在 Arxiv 上的工作。但我的问题是，有些作品只在Arxiv上上传。这是否意味着作者不想在会议上提交自己的作品，而是想发布自己的作品？或者，发布后他们还有其他提交计划吗？ 我这么问是因为我最近的工作在一次会议上被拒绝了，但我不想再深究了。人们是否也喜欢在 Arxiv 上上传废弃作品的同样情况？   由   提交/u/Shot-Button-9010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bya23i/d_why_do_people_upload_their_work_on_arxiv_not/</guid>
      <pubDate>Sun, 07 Apr 2024 17:31:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我们知道Gemini 1.5是如何实现10M上下文窗口的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</link>
      <description><![CDATA[我们知道 Gemini 1.5 是如何实现 1.5M 上下文窗口的吗？随着注意力窗口的扩大，计算量不会呈二次方增长吗？    由   提交/u/papaswamp91  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by8e9s/d_do_we_know_how_gemini_15_achieved_10m_context/</guid>
      <pubDate>Sun, 07 Apr 2024 16:21:14 GMT</pubDate>
    </item>
    <item>
      <title>[研究]苹果如何使用机器学习来识别照片中的人物。简短的视觉指南。 🍎📱</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by82f6/research_how_apple_uses_machine_learning_to/</link>
      <description><![CDATA[      TL;DR：Apple 的方法结合了预训练、对比学习、嵌入和聚类来识别不同人群的人。  以下是涵盖技术细节的视觉指南：Apple 如何使用私有设备上机器学习进行人员识别 https://preview.redd.it/f4qk2aq7z2tc1.png?width=1158&amp;format=png&amp;汽车=webp&amp;s=49bc2f365fa1a403cf80da3ee8f36b25329e42e6   由   提交/u/ml_a_day  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by82f6/research_how_apple_uses_machine_learning_to/</guid>
      <pubDate>Sun, 07 Apr 2024 16:06:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我构建了一个稳定的扩散管道，使用 LangChain、DeepLake 和 ControlNet 与稳定扩散创建艺术二维码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by7q46/p_i_built_a_stable_diffusion_pipeline_to_create/</link>
      <description><![CDATA[       由   提交/u/efenocchi   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by7q46/p_i_built_a_stable_diffusion_pipeline_to_create/</guid>
      <pubDate>Sun, 07 Apr 2024 15:52:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] [R] GCN 法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by250w/d_r_llm_with_gcn/</link>
      <description><![CDATA[您好，我正在研究生成对话响应。我的想法是，我将使用 GCN 标记对话的上下文（即过去话语的串联），然后使用基于 Transformers 的模型根据上下文 + 标签生成响应（假设在本例中我将使用 Blenderbot） ）。问题是：  是否最好先单独训练 GCN 模型，然后使用最佳检查点来预测标签并将该标签作为 Blenderbot 输入的一部分？ 我可以与 Blenderbot 联合训练 GCN 吗？首先，我将输入输入到 Blenderbot 的编码器中，然后使用该编码器的输出输入到未经预训练的 GCN 模型中以获得预测，之后我将把编码器的输出 + GCN 的预测输入到解码器中以生成响应，然后最终以某种方式将 GCN 和 Blenderbot 一起训练。如果答案是肯定的，我想知道这在技术上是如何完成的（是否需要向编码器添加 adj_matrix 参数，GCN 将如何训练等等......）  如果您对第二个问题有任何其他方法，我很乐意听到，并且也将非常感谢有关类似问题的已发表论文的链接，谢谢：&gt;&gt;。   由   提交/u/n804s  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by250w/d_r_llm_with_gcn/</guid>
      <pubDate>Sun, 07 Apr 2024 11:21:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 克劳德与 GPT 特工比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by1e23/p_claude_vs_gpt_agent_comparison/</link>
      <description><![CDATA[Anthropics 最近宣布的 Claude 工具（函数调用）引起了我的注意，特别是这个声明：  所有模型可以处理从 250 多种工具中正确选择工具的问题，前提是用户查询包含目标工具的所有必要参数，且准确度 &gt;90%。这些限制适用于工具的总数，无论复杂程度如何。 “复合体”工具将是一个具有大量参数或具有复杂模式的参数（例如嵌套对象）的工具。  这对于每个使用代理系统的人来说都是非常令人兴奋的消息。 OpenAI 的召回率要低得多。 所以我想将 GPT 函数调用与用于代理任务的 Claude 工具进行比较。   Metric claude-3-opus-20240229 gpt-4-0125-preview claude-3-sonnet-20240229 gpt-3.5-turbo-0125     平均工具调用 16 13 11 td&gt; 9   平均准确度 100% 81.25% 87.5%&lt; /td&gt; 79.17%   平均成本 $0.807255 $0.153540 $0.119638 $0.119638 td&gt; $0.008145   https://github.com/kadoa-org/claude-gpt-agent-comparison   由   提交 /u/madredditscientist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by1e23/p_claude_vs_gpt_agent_comparison/</guid>
      <pubDate>Sun, 07 Apr 2024 10:34:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] [D] 我创建了一本适合初学者的量子机器学习手册。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bxzz1w/p_d_i_have_created_a_beginnerfriendly_quantum/</link>
      <description><![CDATA[大家好，在过去的几周里，我一直致力于为一个不了解任何量子概念并且想要的人创建正确的手持路线图深入研究量子机器学习。我希望获得您对内容的意见，如果您能为这个项目做出贡献，我将不胜感激。希望为每个人提供这本手册。 这里是 GitHub 存储库链接：https:// github.com/Winter-Soren/quantum-ml-handbook 这里是托管链接：https://quantummlhandbook。 vercel.app/   由   提交 /u/_-THUNDERBOLT-_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bxzz1w/p_d_i_have_created_a_beginnerfriendly_quantum/</guid>
      <pubDate>Sun, 07 Apr 2024 09:04:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有办法在 Surprise SVD 模型上执行增量矩阵分解？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bxz7jx/d_is_there_any_way_to_perform_incremental_matrix/</link>
      <description><![CDATA[我有一个预先训练的 SVD 模型，但是，当我获取用户的数据并希望将其添加到数据集中进行推荐时，我不这样做想要拟合整个模型，因为它计算复杂且耗时。我已经看到有一些执行增量矩阵分解的方法，它允许仅拟合新数据而不干扰预训练模型的学习。使用惊喜库可以使用此功能吗？    由   提交 /u/Benxsu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bxz7jx/d_is_there_any_way_to_perform_incremental_matrix/</guid>
      <pubDate>Sun, 07 Apr 2024 08:12:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你如何处理没有发布代码的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bxxk3l/d_what_do_you_do_with_paper_with_no_code_published/</link>
      <description><![CDATA[许多论文介绍了自己的模型，大部分是现有模型的变体，对原始模型的改动很小（主要针对特定​​问题）。他们中的大多数没有发布代码，这使得重现结果非常困难。在某些情况下（甚至可能是很多情况，我只找到/检查了一些）实验配置不完整，在论文中。  你如何处理这些论文？ 当人们引用这些论文时你如何争论？  &amp;# 32；由   提交 /u/Muhammad_Gulfam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bxxk3l/d_what_do_you_do_with_paper_with_no_code_published/</guid>
      <pubDate>Sun, 07 Apr 2024 06:26:36 GMT</pubDate>
    </item>
    </channel>
</rss>