<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 24 Feb 2024 00:56:33 GMT</lastBuildDate>
    <item>
      <title>[D] 探索想法：端到端多任务文本转语音的进展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aygi68/d_exploring_ideas_advancements_in_endtoend/</link>
      <description><![CDATA[嗨！ 这些天我对说话者分类感到好奇，并研究了人们使用的内容，例如 Whisper 与 pyannote 的组合等.由于我不是从事研究工作，所以我想听听大家在端到端多任务文本转语音方面正在探索的一些想法。 我看到了很多从事多语言、低资源文本到语音的工作，但并没有太多超出多语言翻译范围的多任务。 我尝试扩展 Whisper 来执行说话者二值化，但它没有效果不好。特别是因为无法将说话者识别从一个片段保留到另一片段（Whisper 仅适用于 30 秒的音频）。所以我在想，如果你想将 Whisper 扩展到新任务，你不仅会受到 30 秒音频剪辑中应包含的任务的限制，而且还会受到为此引入新的特殊令牌来微调模型的事实特定的任务使得微调变得更加困难。 所以我想知道，是否有任何有前途的端到端多任务文本到语音的研究想法？ &lt; !-- SC_ON --&gt;  由   提交 /u/ReinforcedKnowledge   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aygi68/d_exploring_ideas_advancements_in_endtoend/</guid>
      <pubDate>Sat, 24 Feb 2024 00:22:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] 关于 MoE 和 Mamba 实施的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayddpy/p_advice_regarding_moe_and_mamba_implementations/</link>
      <description><![CDATA[大家好， 我正在深入研究我的硕士论文，需要一些指导。我工作的核心是线性化一个充满记忆效应的复杂函数。虽然 Transformer 架构已在文献中进行了探讨，但我正在考虑采用 Mamba 架构的全新角度，或通过 MoE（专家混合）方法为 Transformer 增添趣味。 Moe-Mamba 也在讨论中。 问题是：这是我第一次真正使用这些架构，所以我真的不知道从哪里开始才能真正实现它们代码。 我应该在哪里更多地了解这些架构？您还可以为这些架构建议一些代码实现（我认为还没有库）吗？ PS：我知道我仍然需要研究很多关于这些主题的内容，所以不要评判我的愚蠢有问题请教，所以才来请教，我想学习！ :)   由   提交/u/PaleAle34  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayddpy/p_advice_regarding_moe_and_mamba_implementations/</guid>
      <pubDate>Fri, 23 Feb 2024 22:13:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代降维</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ayab0e/d_modern_dimensionality_reduction/</link>
      <description><![CDATA[大家好， 我熟悉更经典的降维技术，如 SVD、PCA 和因子分析。但是，有没有一些现代技术或者人们多年来学到的一些技巧可以分享。对于上下文来说，这适用于表格数据。谢谢！   由   提交 /u/MuscleML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ayab0e/d_modern_dimensionality_reduction/</guid>
      <pubDate>Fri, 23 Feb 2024 20:08:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] Gemma 7B with Tensor RT (>500k tok/s batch-8) 教程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay8aes/p_gemma_7b_with_tensor_rt_500k_toks_batch8/</link>
      <description><![CDATA[大家好 - 我们刚刚发布了使用 Tensor RT 运行 Gemma 7B 的指南。使用 Tensor RT 可以获得更好的性能。 ​ 查看：https://docs.mystic.ai/docs/deploy-gemma7b-tensorrt-llm ​ 希望它有用！   由   提交 /u/paulcjh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay8aes/p_gemma_7b_with_tensor_rt_500k_toks_batch8/</guid>
      <pubDate>Fri, 23 Feb 2024 18:46:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] 意图飞行员：桌面操作代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay7wt7/d_p_intentpilot_a_desktop_operating_agent/</link>
      <description><![CDATA[库： pip install Intent-pilot 仓库链接：  https://github.com/askui/intent-pilot 嘿！ 我们构建了一个可以执行端到端自动化的桌面代理。核心思想是基于Set-of-Mark (SoM) + GPT-4v进行本地化。该库与自操作系统计算机或开放解释器具有相同的路线，但我们认为我们的对象检测更适合 UI 领域。此外，我们通过提供跨平台通知来改善 UI 体验，并修复了键盘布局问题 - 例如，Pyautogui 弄乱了德语键盘中的特殊字符。  让我知道你们的想法。我在一周内完成了它，最后我的同事提供了帮助。因此，我们将不胜感激您的反馈。  我们的对象检测模型位于 API 后面，但我们已经发布了全局密钥（可在存储库中找到）。祝你周末愉快！  注意：这就像授予婴儿访问权一样。它最让你惊讶，但也可能让你震惊。我建议您在点击错误的内容之前关闭重要的选项卡;)   由   提交/u/Outlandish_MurMan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay7wt7/d_p_intentpilot_a_desktop_operating_agent/</guid>
      <pubDate>Fri, 23 Feb 2024 18:31:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 系统设计面试 - 设计聊天机器人或像 Perplexity 这样的搜索引擎。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay7nvr/d_system_design_interview_design_chatbot_or/</link>
      <description><![CDATA[大家好， 我即将参加 FAANG 机器学习职位的系统设计面试。我最近才开始接触 Gen AI，并阅读了基础概念 - LLM、模型选择、RAG、微调等。但我想对专门针对 gen AI 驱动的应用程序的整个系统有一个扎实的了解。 &lt; p&gt;很好奇是否有人可以指出资源或解释典型聊天机器人或搜索引擎（如 Perplexity）的端到端系统设计。    ;由   提交 /u/Grouchy-Ad6094    reddit.com/r/MachineLearning/comments/1ay7nvr/d_system_design_interview_design_chatbot_or/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay7nvr/d_system_design_interview_design_chatbot_or/</guid>
      <pubDate>Fri, 23 Feb 2024 18:21:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 通过 VAE 的仅解码器部分来近似已知分布（达到归一化因子）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay6aq6/d_approximating_known_distributions_up_to_a/</link>
      <description><![CDATA[大家好， 如果这是初学者问题，请随时删除。 我&#39;我读了一堆关于如何使用神经网络学习和采样分布的论文，并有一些问题。下面描述的所有内容都是我读过的几篇论文的摘要，其中人们试图做这件事，但我想让这篇文章保持独立。 --------- -------------------------------------------------- -------------------------------------------------- ----------------- 简介：我有以下问题。想象一下你有一个分布P(x)=F(x)/N，我们知道F(x)并且可以随意评估它，但我们不知道知道归一化因子N。问题是——我们如何学习生成分布P(x)的样本，其中x是某个高维空间的元素？一种选择是进行马尔可夫链蒙特卡罗，但我对另一个方向感兴趣。您将立即认识到变分推理和 VAE 的相似之处，但请耐心等待。 设置：我们可以做的是提出一个解码器网络，但没有编码器 我们将尝试用它来优化模型分布M_v(x)。我们开始从 M(z) 中采样 z，其中 M(z) 是已知的，例如是一个简单的高斯函数。接下来，z 是神经网络 NN(z)=v 的输入，该网络生成模型分布的参数。这里需要注意的是，解码器网络不会产生实际的元素x，而是产生模型分布的权重。例如，如果 M_v(x) 是 x 分量中的高斯混合，参数 v 则为必要的均值、方差和混合权重。 目标：学习网络中适当的权重，以便图形模型：“ M_v(x) =sample M(z) -&gt; 获取参数 v -&gt; 样本  M_v(x) 中的 &gt;x 近似采样我们想要学习的分布 P(x)。  &gt;方法：我们首先将两个分布之间的 KL 散度写为 KL(M_v(x)| F(x)/N)= E_{M_v} [ log (M_v(x)) - log ( F(x) ) ] + log(N)。 为了优化我们的解码器网络，我们本质上在 log(N) 上放置了一个变分不等式，如下所示： log(N) &lt; E_{M_v} [ log(M_v(x)) - log ( F(x) ) ] （表达式 1） 我们设置中唯一可调的参数是神经网络的权重产生 NN(z)=v ，因此目标是以 RHS 最小化的方式调整权重。 问题： 1) 这看起来与变分推理非常相似，但主要区别在于，现在我们实际上知道目标分布 F(x) (直到归一化）并尝试学习它的变分近似。然而在大多数关于变分推理的教程和解释中，您不知道分布F(x)，但有一些根据它分布的数据{x}，因此您还需要一个编码器网络。因此，第一个问题是：这个“仅解码器”是否可以实现？用于近似已知目标分布的 VAE 有一个名称吗？ 2) 所以我了解设置和理论，但我不确定如何实际评估  的 RHS &gt;表达式1。 假设M_v(x)是高斯混合。在这种情况下，不可能分析计算这两项中的至少一项。那么在这种情况下，如何在 PyTorch 中实际进行反向传播呢？您实际上是否必须对分布M_v(x)进行真实采样，生成一些样本{x}，然后使用生成的样本来近似E_{M_v} [ log(M_v(x)) - log ( F(x) ) ] ?   由   提交/u/Invariant_apple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay6aq6/d_approximating_known_distributions_up_to_a/</guid>
      <pubDate>Fri, 23 Feb 2024 17:28:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] Beyond A*：通过 Search Dynamics Bootstrapping 使用 Transformers 进行更好的规划 - Meta 2024 - Searchformer - 显着优于直接预测最佳计划的基线，模型大小小 5-10 倍，训练数据集小 10 倍！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay5zka/r_beyond_a_better_planning_with_transformers_via/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.14083  摘要：  虽然 Transformers 在各种应用程序设置中取得了巨大进步，但此类架构仍然存在在解决复杂的决策任务方面落后于传统的符号规划器。在这项工作中，我们演示了如何训练 Transformer 来解决复杂的规划任务，并提出了 Searchformer，这是一种 Transformer 模型，可以在 93.7% 的时间内以最佳方式解决以前未见过的推箱子谜题，同时比标准 A 减少多达 26.8% 的搜索步骤Searchformer 是一个编码器-解码器 Transformer 模型，经过训练可以预测 A* 的搜索动态。然后通过专家迭代对该模型进行微调，以执行比 A* 搜索更少的搜索步骤，同时仍然生成最佳计划。在我们的训练方法中，A* 的搜索动态表示为令牌序列，概述了在符号规划期间将任务状态添加到搜索树中和从搜索树中删除时的情况。在我们对迷宫导航的消融研究中，我们发现Searchformer 的性能显着优于使用小 5-10 倍的模型大小和小 10 倍的训练数据集直接预测最佳计划的基线。我们还演示了 Searchformer 如何可以扩展到更大、更复杂的决策任务（例如推箱子），并提高已解决任务的百分比并缩短搜索动态。   https:/ /preview.redd.it/fhn5bsklbdkc1.jpg?width=1028&amp;format=pjpg&amp;auto=webp&amp;s=bbb8d726ba74d046023a6c6827249fa602c6eff1 https://preview.redd.it/n5a54uklbdkc1.jpg?width=521&amp;format=pjpg&amp;auto=webp&amp;放大器;s =619d31cf68977f98213422566f7c075aa1a2007b https ://preview.redd.it/ztmf8rklbdkc1.jpg?width=1144&amp;format=pjpg&amp;auto=webp&amp;s=700c9cf543b09b85b07d296a314d0ef6b451c1d0 https://preview.redd.it/poragwklbdkc1.jpg?width=936&amp;format=pjpg&amp;auto=webp&amp; ;s=18435580f179a63d72305aa1d9c4511f1ecf70c5   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay5zka/r_beyond_a_better_planning_with_transformers_via/</guid>
      <pubDate>Fri, 23 Feb 2024 17:17:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] OpenCodeInterpreter：将代码生成与执行和优化集成 - 2024 - HumanEval 为 92.7！ GPT-4 CodeInterpreter只有88.0！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay569a/r_opencodeinterpreter_integrating_code_generation/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.14658  Github：https://opencodeinterpreter.github .io/  摘要：  大型语言模型的引入显着改进了代码生成。然而，开源模型通常缺乏 GPT-4 代码解释器等高级系统的执行能力和迭代细化。为了解决这个问题，我们引入了OpenCodeInterpreter，这是一个开源代码系统系列，旨在生成、执行和迭代优化代码。由 Code-Feedback（一个具有 68K 多轮交互的数据集）、OpenCodeInterpreter 提供支持集成执行和人工反馈以实现动态代码细化。我们跨关键基准测试（例如 HumanEval、MBPP 及其 EvalPlus 的增强版本）对 OpenCodeInterpreter 进行了全面评估，揭示了其卓越的性能。值得注意的是，OpenCodeInterpreter-33B 在 HumanEval 和 MBPP 的平均（及以上版本）上实现了 83.2 (76.4) 的准确度，与 GPT-4 的 84.2 (76.2) 相媲美，并通过 GPT 的综合人类反馈进一步提升至 91.6 (84.6)。 4. OpenCodeInterpreter 缩小了开源代码生成模型与 GPT-4 代码解释器等专有系统之间的差距。   https://preview.redd.it/56p1vhv26dkc1.jpg?width=752&amp;format=pjpg&amp;auto =webp&amp; ;s=f1f47a4d25a05ff4a41e46eadce82ca51c1784cb   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay569a/r_opencodeinterpreter_integrating_code_generation/</guid>
      <pubDate>Fri, 23 Feb 2024 16:45:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 情节曲折</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay4z23/d_iclr_plot_twists/</link>
      <description><![CDATA[看到一些 ICLR 结果似乎让社区感到惊讶：  Mamba ➡️ 拒绝 V-JEPA ➡️拒绝 MetaGPT ➡️按照讨论接受（口头）这里  还有哪些接受/拒绝引起了一些人的注意？   由   提交 /u/hzmehrdad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay4z23/d_iclr_plot_twists/</guid>
      <pubDate>Fri, 23 Feb 2024 16:37:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongRoPE：将 LLM 上下文窗口扩展至超过 200 万代币 - Microsoft 2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay4mbu/r_longrope_extending_llm_context_window_beyond_2/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2402.13753  摘要：  大上下文窗口是大型语言模型（LLM）中的一个理想功能。然而，由于微调成本高、长文本稀缺以及新标记位置引入的灾难性值，当前的扩展上下文窗口仅限于大约 128k 个标记。本文介绍了 LongRoPE，它首次将预训练的 LLM 的上下文窗口扩展到令人印象深刻的 2048k 令牌，在 256k 训练长度内最多仅需要 1k 微调步骤，同时保持原始性能短上下文窗口。这是通过三个关键创新实现的：（i）我们通过有效的搜索识别和利用位置插值中的两种形式的非均匀性，为微调提供更好的初始化，并在非微调场景中实现 8 倍扩展； (ii) 我们引入了一种渐进扩展策略，首先微调 256k 长度的 LLM，然后对微调的扩展 LLM 进行第二次位置插值，以实现 2048k 上下文窗口； (iii) 我们在 8k 长度上重新调整 LongRoPE 以恢复短上下文窗口性能。在 LLaMA2 和 Mistral 上进行的各种任务的广泛实验证明了我们方法的有效性。 通过 LongRoPE 扩展的模型保留了原始架构，并对位置嵌入进行了少量修改，并且可以重用大多数预先存在的优化。   https://preview.redd.it/siuxi9gf2dkc1.jpg?width=1109&amp;format=pjpg&amp;放大器;auto=webp&amp;s=c21f2879f3bdafafb1e9f0a97ca303b90c96d18f https://preview.redd.it/vtpxmcgf2dkc1.jpg?width=1188&amp;format=pjpg&amp;auto=webp&amp;s=9837d7ed191130bbf853c7455f06cb16cd70 4756 https://preview.redd.it/uapbcbgf2dkc1.jpg?width=1115&amp;format =pjpg&amp;auto=webp&amp;s=aa2f5f41dc0a32968c45144dfea21e1541d089c2   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay4mbu/r_longrope_extending_llm_context_window_beyond_2/</guid>
      <pubDate>Fri, 23 Feb 2024 16:23:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 曼巴：简单的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay49tp/d_mamba_the_easy_way/</link>
      <description><![CDATA[Mamba 看起来像是一个令人兴奋的新语言模型架构，我花了一段时间才完全理解这篇论文！该模型采用了很多棘手的概念（S4、GPU 内存、并行扫描等），因此我写了一篇博文，介绍我对 Mamba 伟大思想和贡献的理解，着眼于使其尽可能适合初学者. 链接：https://jackcook.com/2024/02/23/mamba。 html 我希望这对您有所帮助，并且我很乐意讨论任何其他问题或澄清点。让我知道你的想法！   由   提交 /u/jackcook   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay49tp/d_mamba_the_easy_way/</guid>
      <pubDate>Fri, 23 Feb 2024 16:09:49 GMT</pubDate>
    </item>
    <item>
      <title>[R]“生成模型：他们知道什么？他们知道事情吗？让我们找出答案！”。论文引用：“我们的研究结果表明，我们研究的所有类型的生成模型都包含有关场景内在因素（法线、深度、反照率和阴影）的丰富信息，这些信息可以使用 LoRA 轻松提取。”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ay2b7u/r_generative_models_what_do_they_know_do_they/</guid>
      <pubDate>Fri, 23 Feb 2024 14:51:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么大家对 Mamba 被 ICLR 拒绝感到惊讶？我错过了什么吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1axsxeo/d_why_is_everybody_surprised_that_mamba_got/</link>
      <description><![CDATA[我也不只是想逆势而行。我不断在 Reddit、工作中、不同的在线论坛等上听到这个消息。当我第一次听到这个消息时我也很惊讶，但读完这篇论文后我并不特别惊讶。他们的硬件调整很有趣，但除此之外，这似乎是对之前论文的简单改编。基准实验并不像我最初认为的那么广泛，因为每个人都在谈论它有多么革命性。阅读这篇论文给我留下了很多问题，比如“X 任务或 Y 基准测试的性能怎么样？”我并不是想羞辱作者，但它并不真的感觉像一个“传统”的。机器学习领域的论文也有。 已经发布了很多并不完全适合会议出版物的优秀论文，我认为这不仅仅是因为某件事被讨论得很多在 Twitter 或 LinkedIn 上，这意味着它值得在某个场所发布。我真的想知道我是否低估了它，因为我没有正确理解它并且愿意接受任何意见。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1axsxeo/d_why_is_everybody_surprised_that_mamba_got/</guid>
      <pubDate>Fri, 23 Feb 2024 05:40:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aob7zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Feb 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>