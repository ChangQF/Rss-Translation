<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 27 Sep 2024 18:22:31 GMT</lastBuildDate>
    <item>
      <title>[R] 迷你序列转换器：优化长序列训练的中间记忆，将 llama、qwen、mistral、gemma 的上下文长度延长 12-24。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqtgxn/r_minisequence_transformer_optimizing/</link>
      <description><![CDATA[论文：2407.15892 (arxiv.org) Github：wdlctc/mini-s (github.com) 博客：Cheng Luo - MINI-SEQUENCE TRANSFORMER (MST) (wdlctc.github.io) 模型 Finetue 指南**：** LLAMA3，Qwen2、Memba、Mistral、Gemma2 摘要：      提交人    /u/Mediocre-Ad5059   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqtgxn/r_minisequence_transformer_optimizing/</guid>
      <pubDate>Fri, 27 Sep 2024 17:40:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] Llama-3.2-3B-指导-未经审查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqqzuh/r_llama323binstructuncensored/</link>
      <description><![CDATA[这是原始 Llama-3.2-3B-Instruct 的未经审查版本，使用 mlabonne 的 脚本 创建，该脚本基于 FailSpy 的笔记本 和来自 Andy Arditi 等人。此博客和此论文中详细讨论了该方法。 您可以在此处找到未经审查的模型，并在此🤗空间中使用它。    提交人    /u/chuanli11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqqzuh/r_llama323binstructuncensored/</guid>
      <pubDate>Fri, 27 Sep 2024 15:53:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 批次大小与学习率</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqqfos/d_batch_size_vs_learning_rate/</link>
      <description><![CDATA[关于最佳模型性能的最佳批次大小有两种观点：  较小，大约 32。 无关紧要，因此使用尽可能最大的批次大小来最大限度地缩短训练时间。  有大量资料支持这两种理论。以下几点表明小批量是最好的：  最佳性能始终是在 m=2 至 m=32 之间的小批量大小下获得，这与近期主张使用数千的小批量大小的研究形成鲜明对比。 重新审视深度神经网络的小批量训练 我们的结果得出结论，较大的批量大小通常不会实现高精度，并且使用的学习率和优化器也会产生重大影响。降低学习率和减小批量大小将使网络训练得更好，尤其是在微调的情况下。 批量大小对卷积神经网络在组织病理学数据集上的通用性的影响 使用大型小批量进行训练对您的健康有害。更重要的是，这对您的测试错误不利。朋友不会让朋友使用大于 32 的小批量。 Yann LeCun  有些人声称它们应该很大：  我们没有发现任何证据表明较大的批次大小会降低样本外性能。 测量数据并行对神经网络训练的影响 一旦考虑到所有这些影响，目前没有令人信服的证据表明批次大小会影响可实现的最大验证性能......批次大小不应被视为验证集性能的可调超参数。 深度学习调优手册  您觉得如何？对于 VGG、ResNet 和 DenseNet 等图像模型，是否对于应使用什么批量大小达成了共识？    提交人    /u/bjourne-ml   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqqfos/d_batch_size_vs_learning_rate/</guid>
      <pubDate>Fri, 27 Sep 2024 15:29:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] 推荐一些用于情绪分析的最佳轻量级模型。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqpac5/p_suggest_some_of_the_best_lightweight_models_for/</link>
      <description><![CDATA[大家好！我正准备启动一个新项目，我真的需要一些建议。我正在寻找最好的轻量级模型。我需要它们超级高效，同时又不牺牲质量。任何建议都很棒。我非常感谢你能提供的任何帮助。    提交人    /u/Ai_Peep   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqpac5/p_suggest_some_of_the_best_lightweight_models_for/</guid>
      <pubDate>Fri, 27 Sep 2024 14:39:17 GMT</pubDate>
    </item>
    <item>
      <title>扩大我的研究范围——医学图像分割[R][D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqp137/expanding_scope_of_my_research_medical_image/</link>
      <description><![CDATA[您好，想听听您的想法。 我正在撰写硕士论文，以便为外科手术数据建立医学图像分割的基础模型。两个月来，  我找到了相关的数据集，这些数据集是最新的，尚未被大量用于研究。 在数据集上设计和测试了经典的 segm 模型和基于 transformer 的模型。对器官特定数据进行二元分类。 （比较研究） 再进行一项关于模型大小（深度和宽度）对得分与基线的影响的比较研究。 多标签与器官特定模型。 使用 SAM 对其进行微调，以便为我的用例提供一种 SurgicalSAM。  我还有 6 个月的时间来完成这项工作，我真的不想要一篇平庸的论文，我觉得它正在变成一篇平庸的论文。不指望任何突破性的东西，但至少希望它能通过好的会议，并在申请博士学位时有所展示。 我的问题 -  还有什么我可以探索的吗？我想我有足够的时间做一些更先进的事情。请提出任何想法，我会交叉检查每个反馈。 我可能错过的任何有趣的技术或 SoTA segm 方法都可以作为应用程序包含在内。     提交人    /u/ade17_in   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqp137/expanding_scope_of_my_research_medical_image/</guid>
      <pubDate>Fri, 27 Sep 2024 14:27:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] TACL 审查延迟</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqo2du/d_tacl_review_delay/</link>
      <description><![CDATA[所以我在今年 8 月的周期（即 8 月初）向 TACL 提交了评论，但已经快 2 个月了，没有收到任何评论。通常评论会在 1.5 个月左右提交，以供比较。有没有其他人收到评论，还是每个人都是这种情况。几天前我给主编发了邮件，但仍然没有回复。    提交人    /u/Progamer101353   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqo2du/d_tacl_review_delay/</guid>
      <pubDate>Fri, 27 Sep 2024 13:44:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] 帮助语音识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqjspj/p_help_with_speech_recognition/</link>
      <description><![CDATA[我正在开展一个项目，需要构建一个代理来监听用户并回答他们的问题（想象一下电话呼叫）。如果用户提供了错误的信息或不小心说错了话，他们还应该能够打断代理。 示例：  理想场景：   代理 🔊：您好！我能为您做些什么？ 人 🗣️：我的移动电源按钮出现问题。 代理 🔊：[提供详细信息。]   有问题的场景   代理 🔊：您好！我能为您做些什么？ 人🗣️：我的手机屏幕刚刚变黑了。我该怎么办？  代理🔊：[开始回复但用户打断...🗣️] 人🗣️：抱歉，我的意思是我的手机变黑了，因为我不小心把它掉进水里了。  我们可以看到，用户可以在代理说话时打断它，代理必须停止响应并开始监听用户的新命令。我正在使用带有两个线程的speech_recognition库：一个用于连续收听，另一个用于转录。我的问题是我的监听线程会同时激活代理和用户的声音。我尝试在笔记本电脑和耳机上输入代码，但它仍然在听。 有办法解决这个问题吗？    提交人    /u/MBHQ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqjspj/p_help_with_speech_recognition/</guid>
      <pubDate>Fri, 27 Sep 2024 09:31:51 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何训练语音转文本模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqi44b/p_how_do_i_train_a_speech_to_text_model/</link>
      <description><![CDATA[您好，我想用大约 5-6 分钟的语音训练一个文本转语音模型，具体来说是这些。我本来打算使用 https://github.com/jasonppy/VoiceCraft?tab=readme-ov-file 或 https://github.com/Camb-ai/MARS5-TTS?tab=readme-ov-file 等模型，但它只需要 5 到 10 秒的样本。我不知道从哪个模型开始训练。任何指示都将不胜感激。谢谢    由   提交  /u/TemperatureOk3561   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqi44b/p_how_do_i_train_a_speech_to_text_model/</guid>
      <pubDate>Fri, 27 Sep 2024 07:21:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 各位 ML 从业者，当您遇到 ML 问题时，您会向谁求助？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fqb1t1/d_fellow_ml_practitioners_who_do_you_go_to_when/</link>
      <description><![CDATA[顺便说一句，不要在“简单问题主题”中发帖，因为我相信即使是具有正式 ML 知识的人也可以从中受益。 我很好奇，如果您遇到以前没有做过的事情，您如何获得新想法并验证它们。我的情况类似，虽然我的工作团队中有其他领域的专家，但没有高级 MLE。 不一定是个人，我也很想知道您提到的任何来源。    提交人    /u/Moltres23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fqb1t1/d_fellow_ml_practitioners_who_do_you_go_to_when/</guid>
      <pubDate>Fri, 27 Sep 2024 00:15:51 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 有没有什么有前景的研究可以利用 RL 从人类反馈中改进计算机视觉任务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpzj0m/discussion_are_there_any_promising_work_on_using/</link>
      <description><![CDATA[目前，对于诸如对象检测或实例分割之类的计算机视觉任务，改进生产模型的最佳方法是使用硬示例挖掘进行某种形式的迭代模型训练/主动学习，并将其放回注释和训练中，并经常这样做。  是否有任何研究探索基于从人类反馈中学习到的某些策略快速对齐模型输出的方法，类似于语言模型中的 RLHF？  此外，是否有任何有助于减少人工注释工作量的值得探索的研究领域？    提交人    /u/Appropriate_Bear_894   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpzj0m/discussion_are_there_any_promising_work_on_using/</guid>
      <pubDate>Thu, 26 Sep 2024 15:52:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种神经网络架构最适合具有几千个数据点的时间序列分析？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpxja7/d_what_neural_network_architecture_is_best_for/</link>
      <description><![CDATA[我知道你在想什么，使用 ARIMA 等经典方法。是的，你是对的，但我已经为我的公司这样做了。我目前是合作社，我得到了一份全职工作。在过渡期间，我两周内没什么事可做。我可以使用 PySpark 和 Databricks，但在新职位上我不会使用，所以我想把这段时间当作一次学习经历，最终它会对我的简历有所帮助。我并不期望性能会比我的 ARIMA 模型更好 数据具有 2021 年的每日粒度。我有特征，但不是很多。有三种架构我一直在考虑。我了解 RNN、LSTM 和时间 CNN。就（主要是）学习与性能相结合而言，您认为其中哪一个最适合我的任务？一般来说，对于丰富的数据，您认为哪种架构通常表现最佳？    提交人    /u/BostonConnor11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpxja7/d_what_neural_network_architecture_is_best_for/</guid>
      <pubDate>Thu, 26 Sep 2024 14:28:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您需要什么语音解码架构来模拟 openai 的高级语音模式？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpwbjk/d_what_speech_decoding_architecture_do_you_need/</link>
      <description><![CDATA[Llama Omni 是我见过的唯一一篇接近语音模式的论文，但所使用的语音解码架构似乎不允许“用法语口音说 1 2 3”之类的事情。在论文中，他们似乎冻结了编码器和 llm，并使用来自其他 TTS 模型的文本和模型输出来训练解码器。这是否意味着您必须拥有一个包含诸如“[法语口音]1 2 3”，.waveform”之类的对的数据集，或者这里有不同的方法可以采用？    提交人    /u/natural_language_guy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpwbjk/d_what_speech_decoding_architecture_do_you_need/</guid>
      <pubDate>Thu, 26 Sep 2024 13:34:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你会关注哪些信息？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/</link>
      <description><![CDATA[虽然 arxiv 和 open review 是新论文的两个最佳来源，但我发现某些 feed 也非常有趣。对我来说，这包括 GitHub、Less Wrong、Hugging Face、Twitter 和 Reddit。我遗漏了什么吗？还有更多吗？博客列表？我希望有这些东西的整合。    提交人    /u/Studyr3ddit   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/</guid>
      <pubDate>Thu, 26 Sep 2024 04:27:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 22 Sep 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>