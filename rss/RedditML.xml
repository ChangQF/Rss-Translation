<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 22 Jan 2025 06:23:52 GMT</lastBuildDate>
    <item>
      <title>[D] 李飞飞在 NeurIPS 2024 上的演讲虽然有点晚，但很有趣</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i74pni/d_a_little_late_but_interesting_talk_by_feifei_li/</link>
      <description><![CDATA[Fei-Fei Li 就视觉智能以及 AI 的未来进行了精彩演讲。想在这里分享，以防有人想在他们的网站上查看。    由    /u/hiskuu  提交  [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i74pni/d_a_little_late_but_interesting_talk_by_feifei_li/</guid>
      <pubDate>Wed, 22 Jan 2025 06:12:18 GMT</pubDate>
    </item>
    <item>
      <title>[D]：Andrej Karpathy 讲座：构建 makemore 第二部分：MLP</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i73gxr/d_andrej_karpathy_lecture_building_makemore_part/</link>
      <description><![CDATA[Youtube 视频 时间戳 00:01:38 ：3 个字符上下文 ( 272727 = 19683 )。可能性太多了。 \ 引入多层感知模型。 00:02:09 - 00:09:00 : 00-02-03-bengio-2003-paper.md    submitted by    /u/yogimankk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i73gxr/d_andrej_karpathy_lecture_building_makemore_part/</guid>
      <pubDate>Wed, 22 Jan 2025 04:56:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 未来引导学习：一种增强时间序列预测的预测方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i727qm/r_futureguided_learning_a_predictive_approach_to/</link>
      <description><![CDATA[大家好！我叫 Skye，我是这项工作的第一作者！这篇论文表明，通过从大脑中获取灵感，特别是预测编码理论，可以增强预测和事件预测。我发布了摘要、代码和 arXiv 链接，供任何好奇的人使用！请随时在下面发表评论，因为这是我的第一篇完整论文，我将不胜感激任何反馈！ 摘要：准确的时间序列预测在各种科学和工业领域都至关重要，但深度学习模型通常难以捕捉长期依赖关系并适应数据分布随时间的变化。我们引入了未来引导学习，这是一种通过受预测编码启发的动态反馈机制来增强时间序列事件预测的方法。我们的方法涉及两个模型：一个分析未来数据以识别关键事件的检测模型和一个基于当前数据预测这些事件的预测模型。当预测模型和检测模型之间出现差异时，将对预测模型进行更重要的更新，通过将其预测与实际未来结果保持一致，有效地最大限度地减少意外并适应数据分布的变化。这种反馈回路允许预测模型动态调整其参数，尽管数据发生变化，但仍专注于持久特征。我们在各种任务上验证了我们的方法，结果表明，使用 EEG 数据进行癫痫发作预测的 AUC-ROC 增加了 44.8%，非线性动态系统预测的 MSE 降低了 48.7%。通过结合可适应数据漂移的预测反馈机制，未来引导学习推进了深度学习在时间序列预测中的应用。  我们的代码公开发布于： https://github.com/SkyeGunasekaran/FutureGuidedLearning。 arXiv： https://arxiv.org/pdf/2410.15217    提交人    /u/Skye7821   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i727qm/r_futureguided_learning_a_predictive_approach_to/</guid>
      <pubDate>Wed, 22 Jan 2025 03:48:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 张量积注意力机制就是你所需要的</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i726eh/r_tensor_product_attention_is_all_you_need/</link>
      <description><![CDATA[扩展语言模型以处理更长的输入序列通常需要大型键值 (KV) 缓存，从而导致推理期间产生大量内存开销。在本文中，我们提出了张量积注意力 (TPA)，这是一种新颖的注意力机制，它使用张量分解来紧凑地表示查询、键和值，从而显著缩小推理时的 KV 缓存大小。通过将这些表示分解为上下文低秩组件（上下文分解）并与 RoPE 无缝集成，TPA 实现了模型质量的提高和内存效率的提高。基于 TPA，我们引入了 Tensor ProducT ATTenTion Transformer (T6)，这是一种用于序列建模的新模型架构。通过对语言建模任务进行广泛的实证评估，我们证明 T6 在各种指标（包括困惑度和一系列知名评估基准）上的表现超过了标准 Transformer 基线（包括 MHA、MQA、GQA 和 MLA）。值得注意的是，TPA 的内存效率使得在固定资源约束下能够处理更长的序列，从而解决了现代语言模型中的关键可扩展性挑战。代码可用    提交人    /u/RajonRondoIsTurtle   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i726eh/r_tensor_product_attention_is_all_you_need/</guid>
      <pubDate>Wed, 22 Jan 2025 03:46:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]：一篇文章解释自我注意力（包括代码片段）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i716g6/d_an_article_explains_selfattention_code_snippet/</link>
      <description><![CDATA[文章  单头注意力 多头注意力 交叉注意力   包含解释。    提交人    /u/yogimankk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i716g6/d_an_article_explains_selfattention_code_snippet/</guid>
      <pubDate>Wed, 22 Jan 2025 03:00:31 GMT</pubDate>
    </item>
    <item>
      <title>[D]：3blue1brown 视频详细解释了注意力机制</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6zh6p/d_a_3blue1brown_video_that_explains_attention/</link>
      <description><![CDATA[ YouTube 视频 字幕  时间戳 02:21：标记嵌入 02:33：在嵌入空间中 \ 一个单词有多个不同的方向 \ 对该单词的多个不同含义进行编码。 02:40：训练有素的注意力模块 \ 计算您需要添加到通用嵌入中的内容 \ 以将其移动到这些特定方向之一， \ 作为上下文的函数。 \ 07:55 ：从概念上认为 K 可能回答 Q。 11:22 ：（没听懂）    提交人    /u/yogimankk   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6zh6p/d_a_3blue1brown_video_that_explains_attention/</guid>
      <pubDate>Wed, 22 Jan 2025 01:38:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 不确定是否过度拟合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6xkdh/d_unsure_if_i_am_overfitting/</link>
      <description><![CDATA[我训练了一个机器学习模型，但不确定它是否过度拟合。使用训练集进行预测时的准确率、精确率、召回率和 f1 分数均为 1.0，而对于测试集，所有分数均为 ~0.9。我知道当它不能很好地概括测试集时就会发生过度拟合，但我的测试集结果相当高。我不确定它是否过度拟合，因为测试分数仍然很高。    提交人    /u/PapayaFrequent7182   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6xkdh/d_unsure_if_i_am_overfitting/</guid>
      <pubDate>Wed, 22 Jan 2025 00:09:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 语言模型思维进化：一种扩展 LLM 推理的进化搜索策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6s0bb/r_language_model_mind_evolution_an_evolutionary/</link>
      <description><![CDATA[使用进化算法增强 LLM 推理能力的一项非常有趣的技术进步。核心方法将遗传算法与 LLM 输出相结合，以发展更好的推理模式。 关键技术点： - 实现对 LLM 解决方案尝试进行操作的遗传算法框架 - 使用专门的评估器模型来评估推理质量并指导进化 - 对成功的推理模式执行交叉和变异操作 - 跨代迭代优化解决方案，重点关注正确性和深度 实验结果： - 测试用例的推理准确率提高 15-20% - 增强了分步解决方案生成 - 减少了复杂推理任务中的逻辑差距和错误 - 在不同推理领域保持性能改进 我认为这种方法对于提高 LLM 在结构化推理任务（如数学证明和逻辑推理）上的性能特别有价值。进化优化框架提供了一种系统的方法来发现和改进更好的推理模式。 我认为在广泛采用之前需要解决计算成本问题，但该方法有望自动改进人工智能能力。发展更复杂的推理策略的能力可以帮助开发更可靠的人工智能系统。 TLDR：研究表明，进化算法可以优化 LLM 推理模式，通过自动进化解决方案，准确率提高了 15-20%。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6s0bb/r_language_model_mind_evolution_an_evolutionary/</guid>
      <pubDate>Tue, 21 Jan 2025 20:12:22 GMT</pubDate>
    </item>
    <item>
      <title>Apple AIML 驻留计划 2025 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6p1ja/apple_aiml_residency_program_2025_r/</link>
      <description><![CDATA[你好！ 有人以前参加过 Apple 的 AIML 驻留计划并愿意分享他们的经验吗？ 我最好奇的是面试过程、计划本身（难吗？有趣吗？），以及未来在 Apple 内部成为正式员工的机会。提前致谢！    提交人    /u/maplesyrup67   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6p1ja/apple_aiml_residency_program_2025_r/</guid>
      <pubDate>Tue, 21 Jan 2025 18:12:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 Transformer 进行多元时间序列预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6os2n/r_multivariate_time_series_prediction_with/</link>
      <description><![CDATA[我正在开发一个模型，希望能够接收多变量时间序列的天气和河流高度数据，并输出一系列针对河流水位计高度的预测（本质上，我输入时间步长 20-40，并期望收到时间步长 41-61）。我之前一直在为此使用 LSTM，但使用几种不同的架构得到的结果相当差。我现在正在考虑使用变压器编码器网络，我遇到了这个反复出现的问题，我似乎无法弄清楚。 对于几乎任何上下文长度、模型大小、位置编码、训练时间等；该模型似乎无法区分输出上的时间步长。它总是学会预测时间步长上水位计高度的良好平均值，但其输出没有变化。例如，当目标量规高度为 [0.2, 0.3, 0.7, 0.8, 0.6] 时，它将输出类似 [0.4, 0.45, 0.4, 0.45, 0.5] 的内容。 事实上，即使没有任何位置编码，模型的表现也几乎完全相同。 以下是多次连续测试的输出示例： 几条预测线，无论图表上的实际位置如何，都显示出相似的趋势。 我尝试了相对位置编码和绝对位置编码。编码并调整损失函数以添加一个关注时间步长之间斜率的项，但我似乎无法强制区分时间步长。 额外的损失项： class TemporalDeregularization(nn.Module): def __init__(self, epsilon): super().__init__() self.epsilon = epsilon self.mse = nn.MSELoss() def forward(self, yPred, yTrue): predDiff = yPred[:, 1:] - yPred[:, :-1] targetDiff = yTrue[:, 1:] - yTrue[:, :-1] return self.epsilon * self.mse(predDiff, targetDiff)  我的位置编码方案： class PositionalEncoding(nn.Module): def __init__(self, d_model：int，dropout：float = 0.1，max_len：int = 5000，batch_first = False）：super（）。__init__（）self.batch_first = batch_first self.dropout = nn.Dropout（p = dropout）position = torch.arange（max_len）。unsqueeze（1）div_term = torch.exp（torch.arange（0，d_model，2）*（-math.log（10000.0）/ d_model））pe = torch.zeros（max_len，1，d_model）pe [：，0，0 :: 2] = torch.sin（position * div_term）pe [：，0，1 :: 2] = torch.cos（position * div_term）self.register_buffer（&#39;pe&#39;，pe）def forward（self，x：张量）——&gt;张量：如果 self.batch_first：x = x + self.pe[:x.size(1)].permute(1, 0, 2) else：x = x + self.pe[:x.size(0)] return self.dropout(x)  这是我的架构的更明确的图表： 包含变压器网络架构的图像，包括线性投影、位置编码、变压器编码器和另一个串联投影。 我理解这不是这个用例的常见用例或架构，但我不确定为什么该模型不能区分时间步长。我考虑在最终投影之前添加双向 LSTM 以强制时间微分。 作为参考，我发现这个模型在 dModel 为 64、前馈为 128、6 层和 8 个头的情况下表现良好。损失函数中的另一个术语是标准 MSE。此外，我不应用掩蔽，因为在我的情况下，所有输入都应该用于计算输出。 我不能发布太多代码，因为这与我的工作有关，但我想更多地了解我的方法有什么问题。 任何帮助或建议都值得赞赏，我目前正在攻读硕士学位，但尽管有多年的工作经验，我还没有遇到任何机器学习课程，所以我可能只是错过了一些东西。（也为狗屁股谷歌图纸感到抱歉）    提交人    /u/Chroma-Crash   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6os2n/r_multivariate_time_series_prediction_with/</guid>
      <pubDate>Tue, 21 Jan 2025 18:01:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] AISTATS 2025 论文录取结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6lgoo/d_aistats_2025_paper_acceptance_result/</link>
      <description><![CDATA[AISTATS 2025 论文录取结果将于今日公布。为今年的结果创建讨论主题。    提交人    /u/zy415   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6lgoo/d_aistats_2025_paper_acceptance_result/</guid>
      <pubDate>Tue, 21 Jan 2025 15:43:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 理解预测编码网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i6h40i/d_understanding_predictive_coding_networks/</link>
      <description><![CDATA[大家好， 我正在尝试理解 Rao &amp; Ballard 中描述的预测编码网络。 到目前为止，我了解到网络训练是通过设置输入（如果训练是监督的，则设置输出）并首先修改神经元的活动以减少预测误差，然后修改突触权重来完成的。 我不明白的是，隐藏层“r”的活动似乎似乎是预测和输入之间差异的函数（见图 1.b），这里似乎暗示 `r` 是转置权重 UT 和预测误差的乘积，这让我很困惑：我理解我们想要将预测误差传播到下一层，但是如果 r = UT (I - f(Ur))，我们如何才能最小化 (I - f(Ur))？ 我想我还没有完全掌握整体架构，如果有人能帮忙我将不胜感激。    提交人    /u/groundswell_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i6h40i/d_understanding_predictive_coding_networks/</guid>
      <pubDate>Tue, 21 Jan 2025 12:06:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICLR 2025 论文决策</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i5z6rd/d_iclr_2025_paper_decisions/</link>
      <description><![CDATA[对结果感到兴奋和焦虑！    提交人    /u/always_been_a_toy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i5z6rd/d_iclr_2025_paper_decisions/</guid>
      <pubDate>Mon, 20 Jan 2025 19:50:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1i4oujz/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1i4oujz/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 19 Jan 2025 03:15:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>