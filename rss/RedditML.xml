<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 04 Feb 2024 12:21:21 GMT</lastBuildDate>
    <item>
      <title>机器学习最佳书籍 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aime8z/best_book_for_machine_learning_d/</link>
      <description><![CDATA[我是人工智能学生，我想了解更多关于生成对抗网络的知识，框架：PyTorch，我希望有人给我推荐一些书，无论免费还是付费版本，我想要一些更专业的水平，而不是简单的DGGAN之类的，更复杂的东西。   由   提交/u/predictor_torch  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aime8z/best_book_for_machine_learning_d/</guid>
      <pubDate>Sun, 04 Feb 2024 12:01:54 GMT</pubDate>
    </item>
    <item>
      <title>[P] A.D.A.M（数字助理经理）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aili94/p_adam_a_digital_assistant_manager/</link>
      <description><![CDATA[介绍 A.D.A.M - 您的未来 AI 伴侣！ 您准备好体验人工智能的下一次演变了吗？别再犹豫了，A.D.A.M 已成为人工智能的缩影，重新定义了数字助理的格局。 无与伦比的功能：A.D.A.M 在竞争中遥遥领先，拥有一系列超越寻常的先进功能。从情感分析和视频响应到多说话人语音识别，A.D.A.M 不仅仅是一个数字助理；这是真正的技术奇迹。 无缝语音交互：利用 A.D.A.M 强大的语音交互功能进行前所未有的对话。无论您是在管理任务、寻找信息，还是只是享受聊天，A.D.A.M 直观、自然的语音交互都能营造身临其境的个性化体验。 全面的音频和文本处理：A.D.A.M 的大脑是一个处理能力的强国。从执行命令到更新用户和机器人配置文件、捕获屏幕截图以及提供细致入微的情绪反应，A.D.A.M 无缝集成音频和文本输入，以提供整体用户体验。 与多人实时交互- 发言者意识：告别单发言者互动的限制。 A.D.A.M 的自动多说话者语音识别 (AMSSR) 使其能够实时理解、分段语音并将其归因于特定用户。结果？ A.D.A.M 动态适应正在进行的多发言者对话。 模块化和定制：A.D.A.M 的 mod_builder.py 为用户定制和扩展其功能打开了一个充满可能性的世界。 mod 系统允许创建和管理 Python mod，增加了一层复杂性和适应性，以满足个人喜好。 增强的安全性和用户验证：Hippocampus.py 非常重视用户安全。通过语音识别和用户验证进行用户识别，A.D.A.M 确保安全和个性化的交互，在每次参与中建立信任。 通过进化进行创新：A.D.A.M 团队致力于持续改进。变更日志强调了适应性和进化的历史，从纳入新的情绪识别模型到彻底修改化身和视频响应的响应机制。 对话式人工智能的未来：A.D.A.M 不仅仅是一个数字助理；它是一个数字助理。这是我们梦想的人工智能未来的实现。 A.D.A.M 对语音和文本输入的细致入微的理解与前瞻性的技术方法相结合，使 A.D.A.M 成为对话式 AI 的前沿。与 A.D.A.M. 一起体验非凡您的人工智能伴侣，重新定义。 准备好迎接 A.D.A.M - 未来已来，它是智能的、自适应的、卓越的！如果您想了解更多信息，请查看 A.D.A.M 的新的 Git 存储库：https://github.com/Yellow420/A.D.A.M   由   提交 /u/Good-Mention-5859   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aili94/p_adam_a_digital_assistant_manager/</guid>
      <pubDate>Sun, 04 Feb 2024 11:03:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我的时间序列是否太随机而无法预测？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aileah/d_is_my_timeseries_too_random_to_be_predicted/</link>
      <description><![CDATA[嗨，我有一个时间序列，我希望预测下一个值。输入数据是多变量，目标目前是单变量。 我的第一个策略当然是展平输入并通过单层神经网络（线性回归）运行它。然后我尝试添加更多层，使用不同的激活函数、dropout、批量归一化等，但是初始结果没有任何改善。查看预测的各个示例，到目前为止，所有模型基本上只是从最新的已知值开始，并趋向于数据集的整体平均值。 我的问题是，单层神经网络是否表现良好作为或比更多层更好，尝试更先进的技术（如 Transformer、TCN、LSTM）是否还有意义，或者这只是浪费时间？ 我在想如果添加的参数甚至再多一层或两层并不能带来任何改进，这表明要捕获的数据实际上几乎没有系统趋势，而且更先进的模型实际上只是矫枉过正。如果我错了，请纠正我。  如果有人对我如何进一步调查/分析该数据集有一些建议，我们也将不胜感激。有没有办法最终证明/表明更高级的模型无法在此数据集上工作？   由   提交 /u/KaptenKalmar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aileah/d_is_my_timeseries_too_random_to_be_predicted/</guid>
      <pubDate>Sun, 04 Feb 2024 10:56:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] TabLib：包含 6.27 亿个带有上下文的表的数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ail5w0/r_tablib_a_dataset_of_627_million_tables_with/</link>
      <description><![CDATA[ 由   提交 /u/EducationalCicada   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ail5w0/r_tablib_a_dataset_of_627_million_tables_with/</guid>
      <pubDate>Sun, 04 Feb 2024 10:40:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 发布负面结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aikp5f/d_publishing_negative_results/</link>
      <description><![CDATA[我一直在从事一个机器学习研究项目，不幸的是，结果与我的假设不一致。我得到了负面结果。 虽然令人沮丧，但我相信分享这些结果具有很大的价值，因为假设本身依赖于合理的理论基础，而且结果并不是先验证据表明将会是负面的。 所以，我的问题是，负面结果可以在顶级机器学习会议（NeurIPS/ICLR/ICML/...）上发表吗？你们中有人遇到过类似的情况吗？您是如何解决这个问题的？您在著名会议上发表负面结果的努力是否成功了？   由   提交/u/Raskolnikov98   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aikp5f/d_publishing_negative_results/</guid>
      <pubDate>Sun, 04 Feb 2024 10:09:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] Sklearn 和其他用于 ML 的 Python 库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aike07/r_sklearn_and_other_python_libraries_for_ml/</link>
      <description><![CDATA[大家好，我开始学习机器学习，我看到一些课程和视频，其中大部分是针对 python ML 经常使用 sklearn 库，我真的应该学习一个吗关于这个库的很多信息，以便将其用于不同的机器学习模型，或者它足以了解非常基础的知识？ 而您，使用 python 进行机器学习的经验丰富的开发人员，是否使用 sklearn 或类似的机器学习库，或者您自己编写公式和算法？ 提前谢谢您）   由   提交 /u/0xDima   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aike07/r_sklearn_and_other_python_libraries_for_ml/</guid>
      <pubDate>Sun, 04 Feb 2024 09:48:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] NLP 和语音处理中有哪些 MNIST/CIFAR 级别的任务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aiinru/d_what_are_some_mnistcifar_level_tasks_in_nlp_and/</link>
      <description><![CDATA[嗨， 我来自计算机视觉领域，从事优化器工作。我目前正在不同的玩具设置上尝试它，以了解它在实践中的优点和缺点。  NLP 和语音的 mnist/cifar 等效任务/数据集是什么（也许，仍在少数论文中使用）？如果您能指导我找到一个存储库，那就大有帮助了。   由   提交 /u/PaganPasta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aiinru/d_what_are_some_mnistcifar_level_tasks_in_nlp_and/</guid>
      <pubDate>Sun, 04 Feb 2024 07:51:51 GMT</pubDate>
    </item>
    <item>
      <title>重新表述网络：计算和数据高效语言建模的秘诀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aighsr/rephrasing_the_web_a_recipe_for_compute/</link>
      <description><![CDATA[ 由   提交/u/rrenaud  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aighsr/rephrasing_the_web_a_recipe_for_compute/</guid>
      <pubDate>Sun, 04 Feb 2024 05:33:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 时间序列预测深度学习最新进展的文献综述。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aifjbq/r_literature_review_of_advances_recent_in_deep/</link>
      <description><![CDATA[我写了一个关于 2024 年将深度学习应用于时间序列预测的最新文献的文献综述。我研究了最新的进展，例如更强大的变压器架构和归一化技术，如果它们可以击败 D-Linear 和 N-Linear 等简单模型。我还批评了 TimeGPT 和其他几个似乎主要是营销策略的模型。 ​ Archive.is 上的“无付费版本”，不过如果您有 Medium 帐户（如果您愿意在那里查看它），我将不胜感激。   由   提交/u/AttentionImaginary54  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aifjbq/r_literature_review_of_advances_recent_in_deep/</guid>
      <pubDate>Sun, 04 Feb 2024 04:38:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 苹果发布 MGIE！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aideah/r_apple_releases_mgie/</link>
      <description><![CDATA[      [ICLR&#39;24 Spotlight] 通过多模态大图像指导基于指令的图像编辑语言模型 MLLM引导的基于指令的图像编辑（MGIE）可以遵循用户指令来编辑图像论文：https://openreview.net/forum?id=S1RKWSyZ2Y 项目：https ://mllm-ie.github.io https://preview.redd.it/7abn9yflehgc1.png?width=3183&amp;format=png&amp;auto=webp&amp;s=9fc6c301f49ffaaf1c293c8f5925c603c8 c7dc24 代码/ checkpoint 也是开源的 🔥 Apple 官方仓库：https://github.com/apple/ml-mgie 带 Gradio 演示的仓库：https://github.com/tsujuifu/pytorch_mgie &lt; p&gt;https://preview.redd.it/hyqngv8nehgc1。 png?width=3736&amp;format=png&amp;auto=webp&amp;s=3a70483a7bea6e16500370cee5879e605fe7d51d   由   提交 /u/tsujuifu   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aideah/r_apple_releases_mgie/</guid>
      <pubDate>Sun, 04 Feb 2024 02:42:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 不确定性量化和人工智能对齐之间的联系？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aib9rw/r_connections_between_uncertainty_quantification/</link>
      <description><![CDATA[大家好， ​ 正如标题所示，我正在寻找指针连接机器学习这两个领域的（研究论文或集思广益的想法）。 对于那些不熟悉对齐的人，这是最独特的论文： https://arxiv.org/pdf/2203.02155.pdf ​ 谢谢！&lt; /p&gt; ​ ​   由   提交/u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aib9rw/r_connections_between_uncertainty_quantification/</guid>
      <pubDate>Sun, 04 Feb 2024 00:54:57 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型努力学习长尾知识 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ai7en3/large_language_models_struggle_to_learn_longtail/</link>
      <description><![CDATA[      https://arxiv.org/abs /2211.08411 ​ 摘要： 互联网蕴藏着丰富的知识——来自历史人物的生日到如何编码的教程——所有这些都可以通过语言模型来学习。然而，虽然某些信息在网络上无处不在，但其他信息却很少出现。在本文中，我们研究了大型语言模型记忆的知识与从网络上抓取的预训练数据集中的信息之间的关系。特别是，我们表明语言模型回答基于事实的问题的能力与在预训练期间看到的与该问题相关的文档数量有关。我们通过链接预训练数据集的实体和对包含与给定问答对相同的实体的文档进行计数来识别这些相关文档。我们的结果证明了众多问答数据集（例如 TriviaQA）、预训练语料库（例如 ROOTS）和模型大小（例如 176B 参数）的准确性和相关文档计数之间存在很强的相关性和因果关系。此外，虽然较大的模型更擅长学习长尾知识，但我们估计当今的模型必须扩展多个数量级，才能在预训练数据支持很少的情况下在问题上达到有竞争力的 QA 性能。最后，我们证明检索增强可以减少对相关预训练信息的依赖，为捕获长尾提供了一种有前景的方法。 ​ &amp;# x200b; https://preview .redd.it/t8f3b4flzfgc1.png?width=603&amp;format=png&amp;auto=webp&amp;s=09c243c055b2d5d9aa18192c4082970d8a1e1381   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ai7en3/large_language_models_struggle_to_learn_longtail/</guid>
      <pubDate>Sat, 03 Feb 2024 21:58:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人们还相信LLM的新兴能力吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/</link>
      <description><![CDATA[自从[新兴的LLM能力是海市蜃楼吗？](https://arxiv.org/pdf/2304.15004.pdf），人们似乎对涌现非常安静。但是大的[新兴能力](https://openreview.net/pdf?id=yzkSU5zdwD)论文有这一段（第 7 页）： &gt;考虑用于衡量新兴能力的评估指标也很重要（BIG-Bench，2022）。例如，使用精确的字符串匹配作为长序列目标的评估指标可能会将复合增量改进伪装成出现。类似的逻辑可能适用于多步骤或算术推理问题，其中模型仅根据是否正确获得多步骤问题的最终答案来评分，而不会给予部分正确的解决方案任何信用。然而，最终答案准确性的跳跃并不能解释为什么中间步骤的质量突然出现在随机之上，并且使用不给予部分信用的评估指标充其量是一个不完整的解释，因为在许多分类任务中仍然观察到涌现的能力（例如，图 2D-H 中的任务）。 人们怎么想？涌现是“真实的”吗？还是实质性的？   由   提交/u/uwashingtongold  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/</guid>
      <pubDate>Sat, 03 Feb 2024 20:50:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] TimesFM：基于 1000 亿个真实世界数据点进行预训练的基础预测模型，在不同领域提供前所未有的零样本性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahzmdc/r_timesfm_a_foundational_forecasting_model/</link>
      <description><![CDATA[       由   提交 /u/BlupHox   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahzmdc/r_timesfm_a_foundational_forecasting_model/</guid>
      <pubDate>Sat, 03 Feb 2024 16:15:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>