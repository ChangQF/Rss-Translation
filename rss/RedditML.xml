<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Wed, 17 Apr 2024 15:16:23 GMT</lastBuildDate>
    <item>
      <title>[D] 在交叉注意力中，为什么 Q 取自解码器，K 取自编码器输出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</link>
      <description><![CDATA[我查了很多地方但找不到答案。如果我们分别将 Q 和 K 来自编码器和解码器，会发生什么？会有什么不同吗？   由   提交/u/shuvamg007  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</guid>
      <pubDate>Wed, 17 Apr 2024 14:49:10 GMT</pubDate>
    </item>
    <item>
      <title>[D]视觉语言模型中视觉嵌入如何与语言嵌入空间共存？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</link>
      <description><![CDATA[大家好！我很高兴能讨论大视觉语言模型 (LVLM)。由于我们可能是最大的法学硕士社区，我认为这个频道将是开始这次对话的完美场所。此外，关于将视觉和语言嵌入结合起来的内容并不多。 LVLM 的一些背景：它们通常由图像的视觉编码器、文本的常规标记器、像投影层这样的投影层组成。 MLP 将视觉特征与文本嵌入空间对齐，最后合并图像和文本嵌入以发送到 LLM 模型中。输入包括文本和图像，而输出是文本，使其成为多模式法学硕士。查看 LLaVA 论文中的图表，了解直观的细分： https://preview.redd.it/l222askgu1vc1.png?width=1607&amp;format=png&amp; ;auto=webp&amp;s=ef011e16301c22b4751d8d0a8f3698f70e3ffd26 从像 CLIP ViT 这样的视觉编码器开始，模型从图像中学习视觉信息，然后使用 MLP 将其投影到 LLM 的嵌入空间上。该论文将这种特征称为对齐。我很好奇视觉嵌入如何与文本嵌入交互，因此我尝试使用 PCA 以 3D 方式可视化它们。 例如，采用 llava-7B 模型 - 它使用 llama-7B 后端和32k 词汇量和 4096 个维度，使得嵌入大小为：[32000,4096]。我使用了一个简单的提示，“向我解释一下这张图片”。使用猫的图片来查看嵌入如何出现在我们的空间中。 https://preview.redd.it/032oy0yn u1vc1.png?width=662&amp;format=png&amp;auto=webp&amp;s=d037bbecc976392e159a1c1bde775ef1e148 488d 添加视觉标记改变了动态。每个图像转换为 576 个形状的视觉标记 [576,4096]。查看包含这些标记时绘图如何调整： https ://preview.redd.it/vdeacylwu1vc1.png?width=566&amp;format=png&amp;auto=webp&amp;s=42441b4fd515cee916b40243429b4aa6820b998c 那我觉得怎么样？ 首先，我们不会直接将视觉标记转换为文本。最近的一篇 Google 论文尝试过，发现这不是最好的方法。视觉推理似乎徘徊在文本嵌入空间附近，可能是因为图像的信息更密集，需要更多的标记来表示视觉概念。 其次，这种设置目前看来是正确的。视觉标记与文本标记一起，将图像衍生的上下文添加到 LLM，使其能够“看到”图像。 最后，尽管 llava 在视觉推理的一些基准测试中表现良好，但它可能还不是最有效的图像表示方法。最近的一些研究谈到了注意力稀疏现象，尤其是 LVLM 中的视觉标记。我们很幸运，因为注意力算法只关注有意义的视觉标记并忽略噪音。 你觉得怎么样？谢谢阅读。 :-)   由   提交/u/E-fazz  /u/E-fazz  reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</guid>
      <pubDate>Wed, 17 Apr 2024 14:41:31 GMT</pubDate>
    </item>
    <item>
      <title>关于时间序列预测的好资源？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6b9t1/good_resources_on_time_series_forecasting_d/</link>
      <description><![CDATA[有人可以推荐一些关于使用机器学习进行现代时间序列预测的好资源吗？ 我在 上找到了一本关于时间序列预测的书亚马逊的好评如潮，名为Python 中的时间序列预测。 话虽如此，许多机器学习书籍和资源似乎都掩盖了时间序列。 有哪些涵盖时间序列的好资源（整本书或书中的章节）？&lt; /p&gt;   由   提交 /u/secret_fyre   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6b9t1/good_resources_on_time_series_forecasting_d/</guid>
      <pubDate>Wed, 17 Apr 2024 14:26:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于 NER 的最佳 NLP 编码器（BERT...），数据微调非常低？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6a65n/d_best_nlp_encoders_bert_for_ner_with_very_low/</link>
      <description><![CDATA[嗨 我知道存在很多 Transformer 编码器变体（BERT、DistilBERT、Deberta、Roberta ...） . 但是，我对最好的句子（可能应该是 Deberta V3）不感兴趣，而是那些即使示例很少（例如大约 50,100 个句子，每个句子可能包含 1 个单词）也能快速获得不错的结果的句子，2 或 3 个实体）。  我用英语做了一些实验，令我惊讶的是，似乎在数据较少的情况下表现最好的实验可能的是原始英语 BERT 模型（HF 上的 google-bert/bert-base-uncased），而不是最近的变体之一。 我还做了其他实验法语，并且多语言 BERT 也比专门训练法语数据的模型（例如 CamemBERT）更快地获得不错的结果。  我比较过的模型包括：bert、bert multilingual、distilbert、distilbert multilingual、roberta、xlm-roberta、camembert、camberta、distilroberta、debertav3、debertav3 multilingual 您对此有何看法？这是令人惊讶或不寻常的事情吗？有什么建议吗？   由   提交/u/LelouchZer12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6a65n/d_best_nlp_encoders_bert_for_ner_with_very_low/</guid>
      <pubDate>Wed, 17 Apr 2024 13:40:50 GMT</pubDate>
    </item>
    <item>
      <title>词嵌入 - 上下文化与 word2vec [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c69b23/word_embedding_contextualised_vs_word2vec_d/</link>
      <description><![CDATA[关于词嵌入的菜鸟问题 -  据我所知到目前为止 -  语境化词嵌入BERT 和其他 LLM 类型模型生成的模型使用注意力机制并考虑单词的上下文。所以不同句子中的同一个词可以有不同的向量。  这 ^ 与 word2vec 等模型的旧方法相反 - word2vec 生成的嵌入不是上下文的。  但是，仔细观察 CBOW 和skip-gram 模型。似乎他们也尝试根据周围（上下文）单词来预测中心单词。因此，word2vec 生成的嵌入也可以是上下文相关的。 所以它们都是上下文相关的？  我错过了什么？    由   提交 /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c69b23/word_embedding_contextualised_vs_word2vec_d/</guid>
      <pubDate>Wed, 17 Apr 2024 13:03:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 喜欢数学讨厌编程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c692z2/d_likes_the_math_hates_the_programming/</link>
      <description><![CDATA[所以我们都知道这样的故事：具有软件 CS 背景的人进入 ML 领域，认为只是使用一堆库进行编码，最终却讨厌这是因为涉及数学/统计数据。但反过来呢？ 我发现我真的很喜欢学习机器学习算法背后的数学和统计数据，但我的问题是我完全不喜欢编程。我可以理解算法和数学位背后发生的事情，但通常很难将其转换为代码并从头开始构建算法。  这令人担忧，因为似乎很多 ML 工作都专注于编码和 ML 操作部分，而不是理论（在非研究角色中）。我知道最好的方法是练习编码位的练习，但只是想知道处于相同位置的其他人或有什么建议（除了明显的建议之外？）。 &lt;!-- SC_ON - -&gt;  由   提交/u/Character-Capital-70   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c692z2/d_likes_the_math_hates_the_programming/</guid>
      <pubDate>Wed, 17 Apr 2024 12:53:53 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]ACM MM2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c66ilj/discussionacm_mm2024/</link>
      <description><![CDATA[这是 MM 从 CMT 转向 Openreview 的第一年（如果我没记错的话）。作为一名作者，自从我创建提交以来，我一直感觉到有些问题，即在摘要 ddl 之前就被桌面拒绝，论文中是否包含提交编号的不一致等。现在我从社交媒体上听到很多作者说由于缺乏提交卷的审稿人，没有许多/任何出版物（是的，包括我）被提名为审稿人。我非常关心今年 MM2024 的审稿和提交的质量。   由   提交/u/INeedPapers_TTT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c66ilj/discussionacm_mm2024/</guid>
      <pubDate>Wed, 17 Apr 2024 10:35:41 GMT</pubDate>
    </item>
    <item>
      <title>批处理的时间序列预测 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c66aaf/timeseries_forecasting_on_batch_process_p/</link>
      <description><![CDATA[我目前正在研究补料批处理过程，我需要知道是否可以以良好的精度实现时间序列。这个想法是有一组微分方程为我创建数据。根据这些数据，将创建一个高精度的模型。问题是，是否可以通过给模型提供一组完全不同的初始条件来实现时间序列预测？我的工作是让这个模型在完全不同的初始条件下进行预测，因此不需要对批处理过程进行实际测试，而只需进行计算。我正在研究神经常微分方程、UDE 等，以便模型理解动态，但我也不确定其他时间序列方法是否有效。 （数据没有周期性，彼此之间没有相关性等。）您认为最好的方法是什么，因为我不断尝试不同的方法，但没有一个给出准确的结果？ &lt;!-- SC_ON - -&gt;  由   提交/u/Bitter__Physics  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c66aaf/timeseries_forecasting_on_batch_process_p/</guid>
      <pubDate>Wed, 17 Apr 2024 10:21:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究中数学和算法哪个优先？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</link>
      <description><![CDATA[我现在正在学习扩散背后的方法（DDPM、基于分数的方法和其他方法）。我想知道研究人员到底是如何想出这个想法的？ 发明新方法是这样的吗？ 1.我们想要制作更好的图像生成器。 2. 哦，数据永远不够...... 3. 让我们乘以数据 - 通过添加一些噪声损坏 4. 这个效果很好，如果我们制作一个去噪网络怎么办？ 5. 如果我们建立一个由纯噪声生成图像的网络会怎么样？ 6. 这不行，如果我们做更小的去噪步骤怎么办？ 7. 这有效！现在，让我们创建一些关于它为何起作用的理论。 8.写论文 或者类似的东西？ 1.我们想要制作更好的图像生成器。 2.我们知道“非平衡热力学”非常好，想尝试以某种方式应用它 3. 我们以某种方式想出了一种依赖于该理论的数学的算法 4. 它有效！ 5. 我们写论文。 通常哪个先出现？数学还是算法？   由   提交/u/Deep-Station-1746   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</guid>
      <pubDate>Wed, 17 Apr 2024 08:22:11 GMT</pubDate>
    </item>
    <item>
      <title>AI/ML 数据中心的未来将是 100 台甚至 1000 台服务器像一个巨型加速器一样运行 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</link>
      <description><![CDATA[在服务器公司 Gigabyte 的网站上看到了这个信息丰富的视频 (https://youtu.be/2Q7S-CbnAAY?si=DJtU2mQ_ZKRZ83Nf），简而言之，服务器品牌现在将完整的服务器集群运送到数据中心，而不是单个服务器机器。在此所示的示例中，它有 8 个机架（另外一个用于管理和网络），每个机架中有 4 台相同型号的服务器，以及 4 个相同的超高级 GPU每个服务器中的模型。为您计算一下，每个集群有 32 台服务器或 256 个 GPU 加速器。请注意，所有服务器和 GPU 都必须是相同的型号，因为它们的连接方式基本上是作为一台单独的机器运行。 这很可能是标准构建块的原因。所有人工智能数据中心的特点是，我们现在利用大型数据集训练人工智能的方式，参数数量达到数十亿，甚至数万亿。对于为我们带来 ChatGPT 及其同类产品的法学硕士来说尤其如此。以任何效率处理这些数万亿个参数的唯一方法是通过我们以前从未见过的规模的并行计算。因此，这个大胆的新概念将数百甚至数千台服务器连接在一起，因此它们基本上是一台巨型服务器，加载了 Nvidia 或其他品牌的数千个 GPU。真正令人着迷的东西，我还没有看到目前为人工智能计算的未来提出的任何其他规模的东西。 这是视频中介绍的集群的网站：https://www.gigabyte.com/Industry-Solutions/giga-pod-as-a-service ?lan=en   由   提交/u/Low_Complaint2254   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</guid>
      <pubDate>Wed, 17 Apr 2024 06:16:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 状态空间模型中的状态幻象</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5z6ua/r_the_illusion_of_state_in_statespace_models/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5z6ua/r_the_illusion_of_state_in_statespace_models/</guid>
      <pubDate>Wed, 17 Apr 2024 02:58:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] GNN 可以用作所有类型数据的模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5olyc/d_can_gnns_be_used_as_model_for_all_types_of_data/</link>
      <description><![CDATA[因为似乎几乎每个数据集都可以转换为图表：  表格 - 节点作为行，没有它们之间的边缘 文本和音频 - 节点作为单词，在相邻单词之间具有有向边缘 时间序列 - 与 2 相同 图像 - 节点作为具有无向边缘的像素相邻像素之间的边缘（包括对角线）  即使 GNN 可以处理所有类型的数据，我认为将它们转换为图形可能会耗费时间和空间，特别是在图像的情况下. 同时，GNN 可以使一些基于表格数据的 ML 模型更加准确 - 例如如果我们有一个关于公寓定价的表格数据集，我们可以在同一社区的公寓之间添加边，以便它们的所有价格都相互依赖，并且这模拟了现实生活中的现象，即同一社区的公寓如何具有相互依赖的定价附近的状况（例如，如果附近的犯罪增加，所有公寓的价格都会下降）   由   提交/u/Snoo_72181   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5olyc/d_can_gnns_be_used_as_model_for_all_types_of_data/</guid>
      <pubDate>Tue, 16 Apr 2024 19:15:18 GMT</pubDate>
    </item>
    <item>
      <title>[项目]：我的自托管应用程序，供机器学习工程师处理所有工具和技术</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5mooj/project_my_selfhosted_app_for_ml_engineers_to/</link>
      <description><![CDATA[   我为软件工程师创建了一个名为 &lt; a href=&quot;https://snipman.io/&quot;&gt;Snipman.io &gt;&gt;&gt; https://snipman.io   它是一个自托管的代码片段管理应用程序（目前可以免费下载）在 Mac 和 Windows 上），基本上可以让您按代码片段类型存储代码片段。 我主要创建它是因为我发现自己为不同编程的小代码片段创建了大量文本文件语言、框架、工具、云、devOps和技术例如 Python、PyTorch、AWS、GCP、Terraform、Kubernetes、Docker 等。这不仅导致大量混乱，而且在搜索和定位正确的代码片段时也很痛苦。 &lt; li&gt;我的目标是创建一些东西，允许所有命令、配置和片段存储在本地的中央存储库中然后就有能力快速搜索它们。我相信我的应用程序通过一个优雅且易于使用的基于 GUI 的工具帮助实现所有这些目标。  我希望这里的所有社区成员都能找到很有用！  ​ snipman.io 中的 Pytorch 代码片段示例 ​   由   提交/u/dev_user1091   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5mooj/project_my_selfhosted_app_for_ml_engineers_to/</guid>
      <pubDate>Tue, 16 Apr 2024 17:58:27 GMT</pubDate>
    </item>
    <item>
      <title>斯坦福大学发布了相当全面（500 页）的“2004 年人工智能指数报告”，总结了当今人工智能的状况。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c59zrq/stanford_releases_their_rather_comprehensive_500/</link>
      <description><![CDATA[ 由   提交/u/Appropriate_Ant_4629   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c59zrq/stanford_releases_their_rather_comprehensive_500/</guid>
      <pubDate>Tue, 16 Apr 2024 07:19:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>