<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 11 Oct 2024 15:17:04 GMT</lastBuildDate>
    <item>
      <title>MLE-bench：在机器学习工程中评估机器学习代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g18dq4/mlebench_evaluating_machine_learning_agents_on/</link>
      <description><![CDATA[  由    /u/MTGTraner  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g18dq4/mlebench_evaluating_machine_learning_agents_on/</guid>
      <pubDate>Fri, 11 Oct 2024 12:16:30 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 模拟数据训练模型与实验数据训练模型之间的迁移学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g17ev6/project_transfer_learning_between_simulation_data/</link>
      <description><![CDATA[      您好， 我正在为我的硕士论文研究一个替代模型，该模型可以预测地震工程中高保真模型的参数。 我试图解决的问题是那些高保真度模型依赖于我们没有实际测量方法的参数，这些参数是根据经验法则和/或工程师的直觉设置的。这导致模拟和实验之间存在很大差异。 我们进行的实验在经济和时间方面都很昂贵，而且往往具有破坏性。因此，我们希望能够在破坏性阶段之前使用这些实验的初步结果来调整高保真模型。 因此，我计划的工作流程如下：  基于模拟数据构建基础模型，可能来自多个不同的高保真模型 使用来自特定实验的实验数据微调此基础模型。  我目前正在研究多种模型架构，但发现很少有有希望的... 预期工作流程 我的问题是： 你有什么建议吗？技术和模型方面，是否可以根据数据集非常有限的物理系统对模型进行良好的微调？ 任何其他建议都乐意接受！ 抱歉，如果这篇文章看起来有点幼稚，那么这个主题超出了我在大学学习的 ML 课程的范围。 非常感谢！    提交人    /u/DurandilAxe   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g17ev6/project_transfer_learning_between_simulation_data/</guid>
      <pubDate>Fri, 11 Oct 2024 11:20:03 GMT</pubDate>
    </item>
    <item>
      <title>[r][d] BigBench 的 SOTA 是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g16afj/rd_what_is_the_sota_of_bigbench/</link>
      <description><![CDATA[BIgBench 在 2 年前引起轰动 (https://github.com/google/BIG-bench)。 但排行榜几乎没有更新。https://paperswithcode.com/sota/machine-learning-on-big-bench 有人知道 bigbench 的 SOTA 是什么吗？我见过这个：https://www.reddit.com/r/singularity/comments/1akz9u8/selfdiscover_google_deepmind_large_language/ 例如。但不是最近的论文。谢谢！    提交人    /u/sunchipsster   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g16afj/rd_what_is_the_sota_of_bigbench/</guid>
      <pubDate>Fri, 11 Oct 2024 10:04:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 语音生物识别技术可行吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g169v7/d_are_voice_biometrics_possible/</link>
      <description><![CDATA[我正在与一家处理大量音频数据的公司合作。所有数据都是单说话人，99% 的数据时长在 1 到 10 分钟之间。我们希望按说话人对这些数据进行排序，但无法通过人工标记处理负载。 我一直在阅读有关说话人分类和 SpeechBrain 和 PyannoteAudio 等库的文章，但我不确定这些是否适合我的用例。我不关心如何区分说话者，我关心的是高精度可聚类嵌入。 我粗略地浏览了一下文献，发现了以下内容：  深度说话者：端到端神经说话者嵌入系统 (2017) [575 引用] 用于短时说话者验证的深度说话者嵌入 (2017) [166 引用] VoxCeleb2：深度说话者识别 (2018) [2551 引用] Voxceleb：野外大规模说话人验证 (2020 年，可能是上述的官方印刷版) [722 引用] 基于深度学习的说话人识别：概述 (2021 年) [398 引用]  我是这个领域的新手，但直观地看，这似乎是一个可以解决的问题。我是否遗漏了什么显而易见的东西？在这个领域工作的人可以解释一下什么是 SOTA 吗？    提交人    /u/FPGA_Superstar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g169v7/d_are_voice_biometrics_possible/</guid>
      <pubDate>Fri, 11 Oct 2024 10:03:36 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 我如何将我的 ML 专业扩展到医疗保健领域的 ML？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g15ofx/discussion_how_can_i_expand_my_ml_specialization/</link>
      <description><![CDATA[你好！我即将完成我的学士学位，我非常想将我的 ML 专业与医学领域联系起来。我该怎么做？ 问题是，我上的最后一节生物/化学课是在高中，当时我一点都没注意。现在我很后悔。 我想，起点应该是完成一些学校级别的生物/化学课程。下一步是什么？ 是否有一些为期一年/两年的课程可以有意义地将我的专业扩展到我想要的水平？是否有可能攻读结合 ML 和生物学的硕士学位？更重要的是，考虑到我的学士学位没有涵盖任何生物学/化学，我能够跟上吗？ 获得第二个学士学位似乎完全不行，因为在经济、心理和时间上都太难了。 如果您有什么建议，我将不胜感激。    提交人    /u/OneEconomist1010   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g15ofx/discussion_how_can_i_expand_my_ml_specialization/</guid>
      <pubDate>Fri, 11 Oct 2024 09:18:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 差动变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/</link>
      <description><![CDATA[      论文 摘要  Transformer 倾向于将注意力过度分配到不相关的上下文中。在本研究中，我们引入了 Diff Transformer，它可以在消除噪音的同时放大对相关上下文的注意力。具体来说，差分注意力机制将注意力分数计算为两个单独的 softmax 注意力图之间的差值。减法可以消除噪音，促进稀疏注意力模式的出现。[...] [...] 它在实际应用中具有显着优势，例如长上下文建模、关键信息检索、幻觉缓解、上下文学习和减少激活异常值。[...]     提交人    /u/fliiiiiiip   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/</guid>
      <pubDate>Fri, 11 Oct 2024 06:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习造福人类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/</link>
      <description><![CDATA[您好，在日常公司工作中，我经历了一种存在危机。我觉得我所有的知识都浪费了，因为它们没有用来帮助我身边的更多人，我想知道我们如何才能利用我们的技能让机器学习更适用于小型企业。 您是否曾在小型企业工作过，并应用过一些机器学习或至少某种程度的数据工程来帮助他们简化流程？我们如何才能真正利用技术帮助改善小型企业？ 您是否有一些关于此类问题的文章或书籍，阅读它们并了解您的所有观点会很不错！    提交人    /u/ALESS885   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0z57p/d_machine_learning_for_good/</guid>
      <pubDate>Fri, 11 Oct 2024 02:00:37 GMT</pubDate>
    </item>
    <item>
      <title>[p] 我写了一篇关于新款 Whisper Turbo 的博客文章</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0rp88/p_i_wrote_a_blog_post_about_the_new_whisper_turbo/</link>
      <description><![CDATA[嗨， 很多人对新的 whisper turbo 模型感兴趣，所以我写了一篇关于它的博客文章。  我们介绍了架构、训练策略以及如何与类似工作（如蒸馏耳语）进行比较。 请尝试一下，如果您有任何问题或反馈，请告诉我。 https://amgadhasan.substack.com/p/demystifying-openais-new-whisper    提交人    /u/Amgadoz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0rp88/p_i_wrote_a_blog_post_about_the_new_whisper_turbo/</guid>
      <pubDate>Thu, 10 Oct 2024 20:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 细胞自动机驱动的镜像张量表面用于神经网络中的结构化扰动：一种通过基于状态的连续权重调制实现动态正则化、增强可塑性和多尺度学习的新方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/</guid>
      <pubDate>Thu, 10 Oct 2024 19:50:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] nGPT：在超球面上进行表征学习的正则化 Transformer</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/</link>
      <description><![CDATA[      论文： https://arxiv.org/pdf/2410.01131 摘要：  我们提出了一种新颖的神经网络架构，即在超球面上进行表征学习的规范化 Transformer (nGPT)。在 nGPT 中，形成嵌入、MLP、注意矩阵和隐藏状态的所有向量都是单位范数规范化的。输入的 token 流在超球面的表面上传播，每一层都会向目标输出预测贡献一个位移。这些位移由 MLP 和注意块定义，它们的向量分量也位于同一个超球面上。实验表明，nGPT 的学习速度更快，可将实现相同准确度所需的训练步骤数减少 4 到 20 倍，具体取决于序列长度。  亮点：  我们的主要贡献如下：  超球面上的网络参数优化 我们建议将形成网络矩阵嵌入维度的所有向量归一化为位于单位范数超球面上。这使我们能够将矩阵向量乘法视为表示余弦相似度在 [-1,1] 内的点积。归一化使得权重衰减变得不必要。  归一化 Transformer 作为超球面上的可变度量优化器 归一化 Transformer 本身在超球面上执行多步优化（每层两步），其中注意力和 MLP 更新的每一步都由特征学习率（可学习可变度量矩阵的对角线元素）控制。对​​于输入序列中的每个标记 t_i，归一化 Transformer 的优化路径从超球面上与其输入嵌入向量相对应的点开始，并移动到超球面上最能预测下一个标记 t_i+1 的嵌入向量的点。 更快的收敛 我们证明，归一化 Transformer 将实现相同准确度所需的训练步骤数减少了 4 到 20 倍。 视觉亮点： https://preview.redd.it/0jdj23ew6ytd1.png?width=1313&amp;format=png&amp;auto=webp&amp;s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df 不确定 20k 和 200k 预算之间的区别；可能绘制了使用不同初始学习率的运行的最佳结果 https://preview.redd.it/waof2llr7ytd1.png?width=1337&amp;format=png&amp;auto=webp&amp;s=3f82cee29c5fe753e219edf55ab16460fcf9a11a https://preview.redd.it/a5vburms7ytd1.png?width=859&amp;format=png&amp;auto=webp&amp;s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/</guid>
      <pubDate>Thu, 10 Oct 2024 15:37:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 VAE 为生成模型提供潜在空间的优缺点是什么？（尤其是对于图像或视频）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/</link>
      <description><![CDATA[我认为变分自动编码器 (VAE) 是一种使某些类型的生成模型（潜在扩散、潜在一致性模型、潜在流模型）在当前硬件限制下工作的黑客手段。 但我也理解一些同事的观点，他们认为 VAE 提供的压缩表示迫使生成模型变得高效，专注于正确建模数据分布的重要因素。 目前最先进的视频生成模型几乎都使用某种压缩表示（通常是 VAE）。所以，它们是有效的。但它们真的有必要吗？ 你对此有何看法？VAE 是拐杖吗？还是生成模型的重要组成部分？    提交人    /u/pm_me_your_pay_slips   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/</guid>
      <pubDate>Thu, 10 Oct 2024 14:10:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] Rodimus*：通过高效注意力机制打破准确率与效率之间的矛盾</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/</guid>
      <pubDate>Thu, 10 Oct 2024 12:20:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] 混沌边缘的智慧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0elvw/r_intelligence_at_the_edge_of_chaos/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0elvw/r_intelligence_at_the_edge_of_chaos/</guid>
      <pubDate>Thu, 10 Oct 2024 09:08:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</guid>
      <pubDate>Sun, 06 Oct 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>