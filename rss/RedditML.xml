<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 03 Apr 2024 15:13:00 GMT</lastBuildDate>
    <item>
      <title>[D]使用目标输入对时间序列上的变压器数据进行预处理的正确方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1buun07/dcorrect_way_to_preprocess_data_for_a_transformer/</link>
      <description><![CDATA[大家好，这涉及数据的预处理步骤，我仍然对如何处理这个问题感到有点困惑。 例如，我正在根据一个人的饮食模式训练变压器。假设他们总共吃了 21 顿饭。最后，我想测试他们的第 21 顿饭。 在训练期间，我们的框架是在某个时刻，模型必须预测“某人 x 吃‘意大利面’作为他们的第 8 顿饭”餐”，这意味着模型将被要求在第 8 个标记处专门进行预测。 作为训练输入，只在第 1 到第 7 个标记中喂食不是正确的吗？整个 20 顿饭的模型会给模型提供有关第 8 个代币的未来信息吗？因为注意力机制的工作原理是更新彼此自己的信息。 因此，如果我必须训练它从序列 2 和 20 进行预测，这意味着我必须将所有这些特定序列分块 19 次，其中每个序列作为训练输入不能包含未来的标记？   由   提交/u/parz01000101  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1buun07/dcorrect_way_to_preprocess_data_for_a_transformer/</guid>
      <pubDate>Wed, 03 Apr 2024 14:54:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 增加耳语吞吐量的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1butp5q/d_way_to_increase_throughput_of_whisper/</link>
      <description><![CDATA[转录短音频时如何优化耳语的吞吐量？ （可能几秒钟）faster-whisper 擅长延迟，但无法转录多个文件或数组。   由   提交/u/lionsheep24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1butp5q/d_way_to_increase_throughput_of_whisper/</guid>
      <pubDate>Wed, 03 Apr 2024 14:15:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 只是美化了即时工程吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1busp41/d_is_rag_just_glorified_prompt_engineering/</link>
      <description><![CDATA[您会收到提示、IR 相关文档，将它们发送到提示，然后 LLM 会生成响应。我们刚刚设计了提示以提供更多信息。   由   提交 /u/RiseWarm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1busp41/d_is_rag_just_glorified_prompt_engineering/</guid>
      <pubDate>Wed, 03 Apr 2024 13:32:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 实施基本 NLP 任务的资源/帮助？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1burfqi/p_resourceshelp_on_implementing_basic_nlp_tasks/</link>
      <description><![CDATA[大家好， 在我的深度学习考试中，我被要求实现特定版本的 Transformer 并在任务中对其进行测试与论文中显示的类似。该模型是类似 BERT 的架构。我发现实现模型的架构非常容易，但我对使用自然语言的基础知识感到非常困惑。仅举一个例子：我的预训练仅包含一项 MLM 任务，但我真的不知道如何从文本到模型的可用输入。我不是在谈论代币化背后的理论，这一点很明确，我说的是从头开始真正实现这些事情。我应该如何读取数据集中的文本？当我的数据集太大以至于无法将其加载到内存中时该怎么办？我的变压器输出应该是多少？本案中损失是如何落实的？我应该从语料库中随机选择文本片段还是需要从句子的开头开始？这些只是我找不到答案的一些问题。我还检查了在线实现，但很难理解，因为每个人做事的方式都不同。另外，我不能只使用 Hugginface，因为在考试时我被要求从头开始做这些事情。 您对我在哪里可以找到这些信息或者我应该如何解决问题有什么建议吗？    由   提交 /u/BossBigSword   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1burfqi/p_resourceshelp_on_implementing_basic_nlp_tasks/</guid>
      <pubDate>Wed, 03 Apr 2024 12:32:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 还有其他 RLHF/数据注释/标签公司吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bupmdp/d_any_other_rlhfdata_annotationlabeling_company/</link>
      <description><![CDATA[伙计们，我正在尝试比较和编写所有 RLHF 和数据注释/标签公司的工作。这是我的清单，你知道我错过了什么吗？您使用过或使用过其中任何一个吗？你的经验是什么？谢谢！ Scale Labelbox Argilla Toloka SuperAnnotate HumanSignal Kili Watchfull Datasaur.ai Refuel iMerit Anote M47 Snorkel Ango AI AIMMO Alegion Sama CloudFactory    ;由   提交 /u/MeowCatalog   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bupmdp/d_any_other_rlhfdata_annotationlabeling_company/</guid>
      <pubDate>Wed, 03 Apr 2024 10:54:45 GMT</pubDate>
    </item>
    <item>
      <title>[P] 通过预消除使 KNN 更加高效</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bupi0p/p_making_knn_more_efficient_with_preelimination/</link>
      <description><![CDATA[大家好，我一直在从事一个小项目，我希望获得一些完全诚实的反馈。 KNN 是一种非常强大的算法，但不幸的是，它的效率也非常低，并且运行起来非常耗费资源。我一直在研究一个项目，根据一些基本测试，该项目平均比普通 KNN 快 20% 左右，而且质量几乎没有下降。您可以在此处查看 github 存储库： https://github.com/jakeSteinburger/peKNN&lt; /p&gt; 我还为此写了一篇小型论文，链接在存储库上。如果代码有点混乱，我很抱歉，但除此之外，我真的很喜欢你完全诚实的反馈（我知道 Reddit 很擅长这一点）。提前致谢！   由   提交 /u/JakeStBu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bupi0p/p_making_knn_more_efficient_with_preelimination/</guid>
      <pubDate>Wed, 03 Apr 2024 10:47:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT-3.5-Turbo 很可能与 Mixtral-8x7B 大小相同！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1budlbc/d_gpt35turbo_is_most_likely_the_same_size_as/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1budlbc/d_gpt35turbo_is_most_likely_the_same_size_as/</guid>
      <pubDate>Tue, 02 Apr 2024 23:35:24 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]输入数据中的RELU和MaxPooling -1 -> +1</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bucals/discussion_relu_and_maxpooling_in_input_data_1_1/</link>
      <description><![CDATA[我一直在尝试为原始音频构建 CycleGAN，但收效甚微。今天我对输入数据有了一个想法。到目前为止，我一直在加载 wav 文件并将这些值转换为范围为 [0,1] 的一维张量。然而今天我使用了 [-1, +1] 范围，因为它对我来说似乎更自然。这还涉及将我的 RELU 层更改为自定义层，将值限制在 -1 和 +1 之间，并将 MaxPool 层更改为简单地获取距 0 最大偏移量的值的层（“AbsoluteMaxPooling1d”，因为我称之为，张量 [-0.4, 0.2] 会给出 -0.4) 将这些更改添加到我的 CycleGAN 中的判别器中，显着提高了它的学习速度 - 现在它的准确率从 1/3 达到了约 95%旧方法使用的数据。 我的问题是，我是否可能在其他地方看到问题？这似乎是一种非正统的方法，我还没有正式研究过这个主题（尽管我是一名高级软件开发人员）。一些搜索并没有真正找到我太多（除了不断建议使用 [0,1] 作为范围。此外 pytorch 没有层函数来完成我需要的事情，所以我认为我正在做的事情是不寻常的。    提交者    /u/maximinus -thrax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bucals/discussion_relu_and_maxpooling_in_input_data_1_1/</guid>
      <pubDate>Tue, 02 Apr 2024 22:41:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你的模型有时会避免显而易见的事情吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1buc27r/d_do_your_models_sometimes_avoid_the_obvious_thing/</link>
      <description><![CDATA[最近在我训练的很多模型中（时间序列预测、一些概率、一些特征分析/预测等），它们倾向于即使架构的梯度路径应该使其非常可行，他们还是需要很长时间才能完成明显的事情？例如，即使有循环连接，它也不会预测下一个时间步，即使它与当前时间步很接近。我认为有时与优化、学习率等有关，但也许不是——这很常见吗？   由   提交 /u/LahmacunBear   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1buc27r/d_do_your_models_sometimes_avoid_the_obvious_thing/</guid>
      <pubDate>Tue, 02 Apr 2024 22:31:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对于那些考虑购买 AMD GPU 的人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bu1sue/d_for_those_who_consider_buying_amd_gpus/</link>
      <description><![CDATA[https:// www.reddit.com/r/Amd/comments/1bsjm5a/letter_to_amd_ongoing_amd/ 认为此信息可能会帮助您做出决定。 ​&lt; /p&gt; P.S.就连 George Hotz 也不再建议购买 7900 XTX，转而使用 NVIDIA，因为驱动程序不稳定。来自最新的直播： https://www.youtube.com/ watch?v=Y-0yZ1AHb0s&amp;t=15890s   由   提交/u/YYY_333  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bu1sue/d_for_those_who_consider_buying_amd_gpus/</guid>
      <pubDate>Tue, 02 Apr 2024 15:37:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAGFlow：基于文档结构识别模型的可定制、可信、可解释的RAG引擎</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1btycwl/d_ragflow_customizable_credible_explainable_rag/</link>
      <description><![CDATA[https://medium.com/p/6a2a2369bd2a   由   提交/u/Vissidarte_2021   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1btycwl/d_ragflow_customizable_credible_explainable_rag/</guid>
      <pubDate>Tue, 02 Apr 2024 13:10:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] SWE-agent：开源编码代理，在 SWE-bench 上取得 12.29% 的成绩</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1btwl37/p_sweagent_an_open_source_coding_agent_that/</link>
      <description><![CDATA[我们刚刚公开了 SWE-agent，它是一个开源代理，可以将任何 GitHub 问题转化为拉取请求，在 SWE-bench 上实现了 12.29% （与 Devin 使用的基准相同）。 ​ https ://github.com/princeton-nlp/swe-agent ​ 过去 6 个月我们一直在努力解决这个问题。构建运行良好的代理比看起来要困难得多 - 我们的存储库概述了我们学到和发现的内容。我们很快就会有预印本。  如果您有任何问题，我们会在这个帖子中闲逛   由   提交 /u/ofirpress   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1btwl37/p_sweagent_an_open_source_coding_agent_that/</guid>
      <pubDate>Tue, 02 Apr 2024 11:42:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士对该领域弊大于利？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1btuizd/d_llms_causing_more_harm_than_good_for_the_field/</link>
      <description><![CDATA[这篇文章可能有点咆哮，但我觉得最近越来越多的人和我分享这种观点。如果您费心阅读整篇文章，请随时分享您对此的感受。 当 OpenAI 将人工智能知识带入日常生活中时，我一开始对此持乐观态度。在美国以外的小国家，企业之前对人工智能非常犹豫，他们认为这感觉很遥远，只有大FANG公司才能做到。现在？好多了。每个人都对此感兴趣，并想知道如何在自己的业务中使用人工智能。这太棒了！ 在 ChatGPT 之前，当人们问我从事什么工作时，我回答“机器学习/人工智能”他们不知道，也几乎没有进一步的兴趣（除非他们是技术人员） ChatGPT 时代，当我被问到同样的问题时，我会得到“哦，你用聊天机器人？” 我想，这是朝着正确方向迈出的一步。我对法学硕士并没有那么大的兴趣，并且有幸专门从事与视觉相关的任务，不像其他一些不得不转向全职与法学硕士一起工作的人。 但是，现在我认为这对这个领域的弊大于利。让我分享一些我的观察，但在此之前我想强调一下，我绝不试图以任何方式把关人工智能领域。 我已经获得了“ChatGPT”的工作机会专家”，这到底是什么意思？我坚信，像这样的工作并不能真正发挥真正的职能，而更像是一种“超级火车”工作，而不是完全可以发挥任何职能的工作。 在过去的几年里，我已经我参加了欧洲各地的一些会议，其中一个是上周举行的，这些会议通常都很棒，具有良好的技术深度，也是数据科学家/机器学习工程师建立联系、分享想法和协作的场所。然而，现在会谈、深度、网络都发生了巨大的变化。公司使用人工智能来做很酷的事情和挑战极限不再是新鲜的、令人兴奋的方式，所有的 GAN 和 LLM 都具有表面知识。少数“老派”类型演讲被发送到小房间的第二轨道 小组讨论充满了没有人工智能基础知识的哲学家，谈论法学硕士是否会变得有知觉。数据科学家/机器学习工程师的空间在学术会议之外很快就消失了，被当前的 hypetrain 挤出了空间。hypetrain 的传播者还承诺用 LLM 和 GAN 创造奇迹和黄金，但他们永远不会实现的奇迹。当投资者意识到法学硕士无法实现这些奇迹时，他们会立即对人工智能未来项目的资金更加犹豫，让我们再次陷入人工智能冬天。 编辑：P.S.我还在这个 reddit 上看到更多的人自称是“生成人工智能专家”。但当深入研究时，就会发现它们只是“好的提示者”。并且对人工智能或生成人工智能的实际领域没有真正的知识、专业知识或兴趣。   由   提交/u/Stevens97  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1btuizd/d_llms_causing_more_harm_than_good_for_the_field/</guid>
      <pubDate>Tue, 02 Apr 2024 09:37:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] SOTA BERT 类模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1btrr9x/d_sota_bertlike_model/</link>
      <description><![CDATA[因此，我们可能都知道最先进的解码器专用 LLM，例如 GPT-4、Claude 等。这些非常适合生成文本. 但我不知道的是类似 SOTA BERT 的模型。你知道，可以用于 NER、POS 标记、标记分类等任务。 有比 Roberta 明显更好的模型吗？    由   提交/u/Amgadoz  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1btrr9x/d_sota_bertlike_model/</guid>
      <pubDate>Tue, 02 Apr 2024 06:26:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</guid>
      <pubDate>Sun, 24 Mar 2024 15:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>