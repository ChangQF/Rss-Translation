<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 26 Sep 2024 18:21:00 GMT</lastBuildDate>
    <item>
      <title>[讨论] 自定义系统基准测试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fq0s5c/discussion_custom_system_benchmarking/</link>
      <description><![CDATA[希望运行我自己的测试来对我的构建的 GPU、CPU 和内存功能进行压力测试和基准测试。无论是训练还是推理任务？有什么建议吗？    提交人    /u/arashsh0   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fq0s5c/discussion_custom_system_benchmarking/</guid>
      <pubDate>Thu, 26 Sep 2024 16:45:10 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 有没有什么有前景的研究可以利用 RL 从人类反馈中改进计算机视觉任务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpzj0m/discussion_are_there_any_promising_work_on_using/</link>
      <description><![CDATA[目前，对于诸如对象检测或实例分割之类的计算机视觉任务，改进生产模型的最佳方法是使用硬示例挖掘进行某种形式的迭代模型训练/主动学习，并将其放回注释和训练中，并经常这样做。  是否有任何研究探索基于从人类反馈中学习到的某些策略快速对齐模型输出的方法，类似于语言模型中的 RLHF？  此外，是否有任何有助于减少人工注释工作量的值得探索的研究领域？    提交人    /u/Appropriate_Bear_894   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpzj0m/discussion_are_there_any_promising_work_on_using/</guid>
      <pubDate>Thu, 26 Sep 2024 15:52:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪种神经网络架构最适合具有几千个数据点的时间序列分析？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpxja7/d_what_neural_network_architecture_is_best_for/</link>
      <description><![CDATA[我知道你在想什么，使用 ARIMA 等经典方法。是的，你是对的，但我已经为我的公司这样做了。我目前是合作社，我得到了一份全职工作。在过渡期间，我两周内没什么事可做。我可以使用 PySpark 和 Databricks，但在新职位上我不会使用，所以我想把这段时间当作一次学习经历，最终它会对我的简历有所帮助。我并不期望性能会比我的 ARIMA 模型更好 数据具有 2021 年的每日粒度。我有特征，但不是很多。有三种架构我一直在考虑。我了解 RNN、LSTM 和时间 CNN。就（主要是）学习与性能相结合而言，您认为其中哪一个最适合我的任务？一般来说，对于丰富的数据，您认为哪种架构通常表现最佳？    提交人    /u/BostonConnor11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpxja7/d_what_neural_network_architecture_is_best_for/</guid>
      <pubDate>Thu, 26 Sep 2024 14:28:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您需要什么语音解码架构来模拟 openai 的高级语音模式？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpwbjk/d_what_speech_decoding_architecture_do_you_need/</link>
      <description><![CDATA[Llama Omni 是我见过的唯一一篇接近语音模式的论文，但所使用的语音解码架构似乎不允许“用法语口音说 1 2 3”之类的事情。在论文中，他们似乎冻结了编码器和 llm，并使用来自其他 TTS 模型的文本和模型输出来训练解码器。这是否意味着您必须拥有一个包含诸如“[法语口音]1 2 3”，.waveform”之类的对的数据集，或者这里有不同的方法可以采用？    提交人    /u/natural_language_guy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpwbjk/d_what_speech_decoding_architecture_do_you_need/</guid>
      <pubDate>Thu, 26 Sep 2024 13:34:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 非真实感图像的特征匹配</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpvurf/d_feature_matching_for_nonphotorealistic_images/</link>
      <description><![CDATA[有人知道非真实感图像的特征匹配的 STOA 是什么吗（例如，将猫卡通图片的特征映射到猫照片的特征（姿势不同）、将选举区域映射到街道地图、将对象映射到雅达利游戏的两个屏幕截图中）？我甚至不确定这个问题叫什么。一般来说，人们研究过比较两张图片然后发现它们之间的相似点和差异的问题吗？ 你会如何处理这样的问题？    提交人    /u/saintshing   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpvurf/d_feature_matching_for_nonphotorealistic_images/</guid>
      <pubDate>Thu, 26 Sep 2024 13:12:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能评估和输出质量的社区</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpvhxq/d_a_community_for_ai_evaluation_and_output_quality/</link>
      <description><![CDATA[如果您专注于 LLM 中的输出质量和评估，我创建了 r/AIQuality — 一个致力于为那些致力于构建可靠、无幻觉系统的人服务的社区。 就我个人而言，我在评估我的 RAG 管道时一直面临挑战。我应该使用 DSPy 来构建它吗？哪种检索器技术效果最好？我应该切换到不同的生成器模型吗？最重要的是，我如何真正知道我的模型是在改进还是在退步？这些问题使评估变得困难，但至关重要。 随着 RAG 和 LLM 的快速发展，一直没有空间深入研究这些评估难题 — 直到现在。这就是我创建这个社区的原因：分享见解，探索前沿研究，并应对评估 LLM/RAG 系统的真正挑战。 如果您正在处理类似的问题并希望改进您​​的评估流程，请加入我们。 https://www.reddit.com/r/AIQuality/    提交人    /u/Desperate-Homework-2   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpvhxq/d_a_community_for_ai_evaluation_and_output_quality/</guid>
      <pubDate>Thu, 26 Sep 2024 12:55:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Llama-3.2 与 llama-3.1 在医疗领域的表现：Llama-3.1 70B 优于 Llama-3.2 90B</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fps53b/d_llama32_vs_llama31_in_medical_domain_llama31/</link>
      <description><![CDATA[      大型 LLama 模型在医疗领域的表现（90B、70B、11B） 探索 LLama 3.2 Large 模型与 Llama 3.1 模型在医疗领域的表现。 （未经微调） 🥇 Meta-Llama-3.1-70B-Instruct：总冠军，平均成绩 84%  MMLU 大学生物学成绩优异（95.14%） MMLU 专业医学成绩优异（91.91%）  🥈 Meta-Llama-3.2-90B-Vision（Instruct 和 Base）：以 83.95% 的平均成绩并列第二  Instruct 和 Base 版本表现一致 MMLU 大学生物学（93.06%）和 MMLU 专业医学（91.18%）成绩最佳  🥉 Meta-Llama-3-70B-Instruct：第三名，平均成绩 82.24%   MMLU 医学遗传学（93%） MMLU 大学生物学表现稳定（90.28%）  医学领域的小型 LLama 和 Phi 模型（3B、1B） 我还分析了较小的模型，并将它们与 phi-3 进行了比较，以探索小模型在医学领域的表现。 （未经微调） 🥇 Phi-3-4k：表现最佳，平均分数为 68.93％  MMLU 大学生物学成绩优异（84.72％） MMLU 临床知识成绩优异（75.85％）  🥈 Meta-Llama-3.2-3B-Instruct：平均成绩为 64.15％ ，排名第二  MMLU 大学生物学成绩最佳（70.83％） PubMedQA 成绩稳定（70.6％）  🥉 Meta-Llama-3.2-3B：平均成绩为 60.36％ ，排名第三  MMLU 大学生物学成绩最强（63.89％）  PubMedQA (72.8%)  其他观察结果： 评估结果  视觉模型中的表现相同： Meta-Llama-3.2-90B-Vision Instruct 和 Base 版本在所有指标和所有 9 个数据集上都表现出相同的表现（平均 83.95%），精确到小数点后一位。 同样，Meta-Llama-3.2-11B-Vision Instruct 和 Base 版本也表现出相同的分数（平均 72.8％）在所有类别中。（评估两次）  不寻常的一致性： Instruct 和 Base 版本之间的这种完美对齐有点不典型，因为 Instruct 和基础变体通常会显示出轻微的性能差异.. 我猜这是由于视觉指令调整造成的？视觉模型的能力是否可以减少对针对医疗任务的特定指令调整的依赖？ 结果以 JSON 格式提供，可在Github 上获取   将很快在此处为医疗领域评估更多模型 - 源帖子    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fps53b/d_llama32_vs_llama31_in_medical_domain_llama31/</guid>
      <pubDate>Thu, 26 Sep 2024 09:29:45 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLM + 药物研发自动报告代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpqst1/p_llm_agents_for_automatic_reporting_in_drug/</link>
      <description><![CDATA[嗨 r/Machinelearning，我想分享我的团队一直在进行的一些工作，并从社区获得一些反馈。在我们在 Arxiv 上发表的这项工作中，我们提出了一个为药物发现生成自动报告的系统。我们使用了 LLM、RAG 和代理。 药物发现是一个昂贵、漫长且高风险的过程。该过程可能花费高达 10-20 亿美元，平均需要 10-15 年。人工智能有望降低成本、时间表和失败风险。药物发现是一个复杂的多步骤过程，需要精确和推理。 LLM 表现出很好的通才技能，但在医学等专业领域却举步维艰。两个主要问题是：  缺乏持续更新。在医学和药物发现领域，每天都会发表许多文章，而模型知识在预训练时就停止了。  模型通过生成不正确或虚构的输出产生幻觉。   为了解决这些问题，我们使用了带有 RAG 和代理的管道。 LLM 响应用户的查询，从不同的医学和生物数据库（文章、专利、临床试验、基因和蛋白质数据库等）检索信息。然后它会自动生成报告和演示文稿 文章在这里：https://arxiv.org/abs/2409.15817 带有示例的存储库：https://github.com/SalvatoreRa/Automatic-Target-Dossier    提交人    /u/NoIdeaAbaout   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpqst1/p_llm_agents_for_automatic_reporting_in_drug/</guid>
      <pubDate>Thu, 26 Sep 2024 07:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你会关注哪些信息？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/</link>
      <description><![CDATA[虽然 arxiv 和 open review 是新论文的两个最佳来源，但我发现某些 feed 也非常有趣。对我来说，这包括 GitHub、Less Wrong、Hugging Face、Twitter 和 Reddit。我遗漏了什么吗？还有更多吗？博客列表？我希望有这些东西的整合。    提交人    /u/Studyr3ddit   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpo0z8/d_which_feeds_do_you_look_at/</guid>
      <pubDate>Thu, 26 Sep 2024 04:27:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] Aggressor：“无矢量量化的自回归扩散”的实验实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpn0wp/p_aggressor_experimental_implementations_of/</link>
      <description><![CDATA[大家好 r/MachineLearning！我想分享一个我一直在做的项目，并从社区获得一些反馈。 项目概述 我在一个名为“Aggressor”的项目中实现了我自己版本的最近论文&quot;无矢量量化的自回归图像生成&quot;&quot;。目标是创建一个超最小自回归扩散模型，从图像生成开始，然后扩展到各种模态和架构变化。 GitHub Repo： Aggressor 主要特点  核心实现： aggressor.py 包含用于图像生成的最小实现。 实验变化：  ret_aggressor.py：用 RetNet（Retentive Network）机制取代标准注意力机制，允许像注意力机制一样并行训练，但 O(n) 循环生成。 dct_aggressor.py：利用离散余弦变换 (DCT) 用于图像生成，探索频域表示。 wav_aggressor.py：使用 DCT 调整模型以生成音频，展示跨模态功能。 ycr_aggressor.py：尝试使用 YCbCr 颜色空间生成图像，可能提高色彩保真度。  最小依赖性：仅使用基本的 MLX 操作从头开始构建。 多模态：支持图像和音频生成，并计划用于视频。  结果 我已经在 MNIST、CIFAR 和音频数据集上测试了各种模型。您可以在 README 中看到一些示例输出。 技术细节  主要的 Aggressor 类结合了转换器（或 RetNet）和扩散模型。 使用自定义 Scheduler 处理扩散中的前向和后向过程。 对图像和音频数据进行 DCT（离散余弦变换）实验。  未来方向  实现视频生成 致力于全模态模型 探索不需要标记器的字节级多模态语言模型的可能性  社区问题  有人尝试过跨不同模态的自回归扩散吗？有什么见解吗？ 有没有建议有效地将这种方法扩展到视频或多模态数据？ 关于使用 DCT 或其他变换来提高生成质量或效率的想法？ 有没有使用多模态数据的字节级模型的经验？挑战还是好处？  我愿意接受任何反馈、问题或改进建议。我特别有兴趣讨论将这种方法扩展到更复杂的多模态场景的潜力和挑战。谢谢您的关注！    提交人    /u/JosefAlbers05   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpn0wp/p_aggressor_experimental_implementations_of/</guid>
      <pubDate>Thu, 26 Sep 2024 03:28:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] ViT 受益于双曲空间变换</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fplhvi/r_vits_benefit_from_hyperbolic_space_transform/</link>
      <description><![CDATA[https://arxiv.org/abs/2409.16897    由   提交  /u/jacobfa   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fplhvi/r_vits_benefit_from_hyperbolic_space_transform/</guid>
      <pubDate>Thu, 26 Sep 2024 02:03:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 大型语言模型结构化输出和函数调用的基本指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpiqlj/p_the_essential_guide_to_large_language_models/</link>
      <description><![CDATA[过去一年，我一直在使用 LLM 构建生产系统。当我在 2023 年 8 月开始工作时，材料非常稀缺，以至于必须先重新发明许多轮子。时至今日，情况已经发生了变化，但社区仍然迫切需要教育材料，特别是从生产角度来看。 很多人都在谈论 LLM，但很少有人真正将它们应用于他们的用户/业务。 这是我对社区的新贡献，“大型语言模型结构化输出和函数调用基本指南”文章。 这是一篇关于结构化输出和函数调用的实践指南（长篇），以及如何从 0 到 1 应用它们。要求不多，只是一些基本的 Python，其余的都解释了。 我在公司将其应用于“让我们通过 LLM 为 20 万以上用户解决所有客户支持问题”的计划中取得了相当大的成功。我们还没有达到 100% 的目标，但我们正在快速实现目标，特别是结构化的输出使我们成为可能。 传播这个消息，让我们在演示之外分享更多关于应用 LLM 的经验。    提交人    /u/p_bzn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpiqlj/p_the_essential_guide_to_large_language_models/</guid>
      <pubDate>Wed, 25 Sep 2024 23:42:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] Llama 3.2详细分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fpckbb/d_llama_32_detailed_analysis/</link>
      <description><![CDATA[大家好！Meta 发布了一组新的 Llama 3.2 模型，分别用于文本（1B、3B）和视觉（11B、90B）。我对这些模型进行了深入研究，希望能够有所启发：  新的 1B 和 3B 文本专用 LLM 9 万亿个 token 新的 11B 和 90B 视觉多模态模型 128K 上下文长度 1B 和 3B 使用了一些来自 8B 和 70B 的提炼 VLM 60 亿个图片、文本对 CLIP MLP GeLU + 交叉注意  长分析：1. 视觉编码器中使用带有 GeLU 激活的 CLIP 类型 MLP。类似于 GPT2 的 MLP。与 Llama 3 的 MLP 不同，因为 SwiGLU 不用于视觉 MLP。  用于视觉编码器的正常 layernorm - 不是 RMS Layernorm。此外，一些“门控”参数用于乘以隐藏状态。 在注意力和 MLP 之后对隐藏状态进行门控乘法器 - tanh 用于将向量缩放移动到从 -1 到 1 的数字。 对于小型 1B 和 3B LLM 以及多模态 VLM 11B 和 90B，评估看起来相当不错。1B 49.3 MMLU 和 3B 63.4。 VLM MMMU 50.7 和 90B 60.3  感谢您的阅读，如果您有任何疑问，请告诉我！    由    /u/danielhanchen 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fpckbb/d_llama_32_detailed_analysis/</guid>
      <pubDate>Wed, 25 Sep 2024 19:09:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fmv9zi/d_simple_questions_thread/</guid>
      <pubDate>Sun, 22 Sep 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>