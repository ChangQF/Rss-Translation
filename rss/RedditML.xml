<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 05 Jun 2024 06:21:07 GMT</lastBuildDate>
    <item>
      <title>[R] 智能 Go-Explore：大型语言模型代理的新型探索框架！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8j2pm/r_intelligent_goexplore_new_exploration_framework/</link>
      <description><![CDATA[标题：智能围棋探索：站在巨人基础模型的肩膀上 作者： Cong Lu、Shengran Hu、Jeff Clune。 代码： https://github.com/conglu1997/intelligent-go-explore 网站： https://conglu.co.uk/intelligentgoexplore/ 论文： https://arxiv.org/abs/2405.15143 摘要：Go-Explore 是一组功能强大的算法，旨在解决难以探索的问题，其原理是存档已发现的状态，并迭代返回最有希望的状态并从中进行探索。这种方法在包括 Atari 游戏和机器人控制在内的各种具有挑战性的问题中都取得了超人的表现，但需要手动设计启发式方法来指导探索，这既耗时又不可行。为了解决这个问题，我们提出了智能 Go-Explore (IGE)，它通过用巨型基础模型 (FM) 捕获的智能和内化的人类兴趣概念取代这些启发式方法，大大扩展了原始 Go-Explore 的范围。这为 IGE 提供了一种类似人类的能力，即使在启发式难以定义的复杂环境中，也能本能地识别任何新状态的有趣程度或前景（例如发现新物体、位置或行为）。此外，IGE 提供了令人兴奋的、以前不可能的机会来识别和利用无法提前预测的偶然发现。我们在一系列需要搜索和探索的语言任务上评估了 IGE。在 Game of 24 这个多步骤数学推理问题中，IGE 达到 100% 的成功率，比最佳经典图形搜索基线快 70.8%。接下来，在 BabyAI-Text 这个具有挑战性的部分可观察网格世界中，IGE 以比之前的 SOTA 少几个数量级的在线样本超越了之前的 SOTA。最后，在 TextWorld 中，我们展示了 IGE 在需要长期探索的环境中取得成功的独特能力，而之前的 SOTA FM 代理（如 Reflexion）则完全失败了。总体而言，IGE 结合了 FM 的巨大优势和强大的 Go-Explore 算法，开辟了研究的新前沿，以创建具有令人印象深刻的探索能力的更通用的代理。    提交人    /u/MolassesWeak2646   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8j2pm/r_intelligent_goexplore_new_exploration_framework/</guid>
      <pubDate>Wed, 05 Jun 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>每次运行模型时，我都会收到此错误“局部变量‘prompt_template’在赋值之前被引用”，我不知道发生了什么。希望得到一些帮助。[项目][P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8im5o/i_get_this_error_local_variable_prompt_template/</link>
      <description><![CDATA[下面是模型的结构（必须删除提示，因为我不允许显示一些输入的结构） def validation_model(target_model_output): target_model_output_prompt = json.loads(target_model_output) prompt_template : &quot;&quot;&quot; Target_model_output : {target_model_output}  &quot;&quot;&quot; target_model_output = json.dumps(target_model_output_prompt,indent = 2) formatted_prompt = prompt_template.format(target_model_output = json.dumps(target_model_output_prompt,indent = 2)) print(&quot;Formatted Prompt:\n&quot;, prompt) response = openai.Completion.create( model = &quot;gpt-3.5-turbo-0125&quot;, response_format = { &quot;type&quot;: &quot;json_object&quot; }, prompt = formatted_prompt, max_tokens = 2048, stop = None, temperature = 0.2 ) inferred_user_inputs = response.choices[0].text.strip() return inferred_user_inputs 当我尝试以这种方式运行它时，它会抛出上述错误， inferred_user_inputs = validation_model(target_model_output_json) print(inferred_use_inputs) 我不知道出了什么问题，我是 LLM 的新手，所以我很乐意得到帮助和指导。 谢谢。    提交人    /u/bastard_of_jesus   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8im5o/i_get_this_error_local_variable_prompt_template/</guid>
      <pubDate>Wed, 05 Jun 2024 05:47:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 测量大型语言模型的社会规范</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8ifsd/r_measuring_social_norms_of_large_language_models/</link>
      <description><![CDATA[作者： 袁野，唐可欣，沈建豪，张明，王晨光  论文： https://arxiv.org/abs/2404.02491 数据集： https://huggingface.co/datasets/socialnormdataset/social 代码： https://github.com/socialnormdataset/socialagent  摘要：我们提出了一个新的挑战，以检验大型语言模型是否理解社交规范。与现有数据集相比，我们的数据集需要对社会规范有基本的了解才能解决。我们的数据集具有最大的社会规范技能集，包括 402 项技能和 12,383 个问题，涵盖了从观点和论点到文化和法律的广泛社会规范。我们根据 K-12 课程设计数据集。这使得可以直接将大型语言模型的社会理解与人类（更具体地说是小学生）进行比较。虽然先前的工作在我们的基准上产生了几乎随机的准确性，但最近的大型语言模型（如 GPT3.5-Turbo 和 LLaMA2-Chat）能够显着提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多智能体框架，以提高模型理解社会规范的能力。该方法进一步改进了大型语言模型，使其与人类相提并论。鉴于大型语言模型在现实世界应用中的采用越来越多，我们的发现尤为重要，并为未来的改进提供了独特的方向。    提交人    /u/shizue_yy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8ifsd/r_measuring_social_norms_of_large_language_models/</guid>
      <pubDate>Wed, 05 Jun 2024 05:36:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 ImageNet 上，仅从图像进行无监督的通用表征学习的现状如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8hpqg/d_what_is_the_status_of_unsupervised_general/</link>
      <description><![CDATA[最近我研究了一个，当在其上训练一个线性输出层（同时冻结主干）时，它在 ImageNet 上的准确率达到了 30%，这让我很想知道它与该领域的其他产品相比如何，但我所能找到的都是从其他数据集进行某种知识转移的论文。 我所说的“通用表示学习”不是专为图像设计的东西。    提交人    /u/YanaiEliyahu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8hpqg/d_what_is_the_status_of_unsupervised_general/</guid>
      <pubDate>Wed, 05 Jun 2024 04:51:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] 直接迭代反演</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8asl8/p_inversion_by_direct_iteration/</link>
      <description><![CDATA[很高兴介绍我参与的一个附带项目，该项目使用直接迭代反演从 8 位图像中去除色带。使用的数据集少于 2000 张图像，模型有 140 万个参数。我一直在测试基于扩散的模型的计算下限，INDI 确实为我提供了很好的帮助。 https://github.com/ksasso1028/indi-debanding    提交人    /u/somethingwrongwifme   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8asl8/p_inversion_by_direct_iteration/</guid>
      <pubDate>Tue, 04 Jun 2024 22:56:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 比较 Darknet/YOLO 和 YOLOv10</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d8a22c/d_comparing_darknetyolo_and_yolov10/</link>
      <description><![CDATA[我最近在 YouTube 上发布了一个视频，展示了 Darknet/YOLO 和 Ultralytics/YOLOv10 之间的一些区别。 TLDR：Darknet/YOLO 仍然比最新的基于 Python 的 YOLO 框架更快、更精确。 https://www.youtube.com/watch?v=2Mq23LFv1aM 如果有人对 Darknet/YOLO 感兴趣，我曾经在 reddit 上维护过一篇充满 Darknet/YOLO 信息的帖子。我已经有一段时间没有更新它了，但是信息仍然有效：https://www.reddit.com/r/computervision/comments/yjdebt/lots_of_information_and_links_on_using_darknetyolo/    提交人    /u/StephaneCharette   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d8a22c/d_comparing_darknetyolo_and_yolov10/</guid>
      <pubDate>Tue, 04 Jun 2024 22:24:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有什么好的模型可以对语义分割输出进行集成？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d86oyj/d_are_there_any_good_models_that_can_perform/</link>
      <description><![CDATA[大家好，我目前有几个模型可以在输入数据集上输出语义分割掩码。我现在有一个简单的集成，我基本上是堆叠模型输出，然​​后对组合数据使用逻辑回归。我四处寻找更先进的技术，但我还没有做出决定。有没有可以作为语义分割有效模型的模型？    提交人    /u/Searin   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d86oyj/d_are_there_any_good_models_that_can_perform/</guid>
      <pubDate>Tue, 04 Jun 2024 20:05:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一种无需阈值调节的神经网络结构化剪枝新方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d85eqd/r_a_new_method_for_structured_pruning_of_neural/</link>
      <description><![CDATA[https://arxiv.org/abs/2406.01345 我们很高兴与大家分享我们的成果“BMRS：结构化剪枝的贝叶斯模型简化”。结构化剪枝通过移除对输出影响有限的整个网络结构（例如神经元或卷积滤波器）来提高神经网络效率。我们提出了一种结构化剪枝的贝叶斯方法，该方法自动确定要剪枝的结构。这是通过将贝叶斯结构化剪枝与对数乘性噪声和贝叶斯模型简化相结合来实现的。  该论文的代码可以在这里获得：https://github.com/saintslab/bmrs-structured-pruning    由   提交  /u/ewits   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d85eqd/r_a_new_method_for_structured_pruning_of_neural/</guid>
      <pubDate>Tue, 04 Jun 2024 19:12:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将位置偏差结合到 softmax-crossentropy 中</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d83yf3/d_combining_position_bias_into_softmaxcrossentropy/</link>
      <description><![CDATA[考虑一个学习排名设置，其中我从针对每个用户查询向用户显示的 N 个项目中进行学习。假设我可以量化在检查了第一个位置的情况下检查每个位置 i 的概率 P[examine_i]（即相对于第一个位置）。用户反馈是二进制的 - 点击/不点击。 现在假设我对每个查询使用 softmax-crossentropy 损失（不要问为什么 - 假设它是一个约束）。合并位置偏差信息的“正确”方法是什么？ 直观地说，点击较低的位置比点击较高的位置更具信息量。因此，将损失与点击位置 i 的 P[examine_i] 成反比进行权衡是有益的。另一方面，低位置上的非点击信息量较少，因此，以较低的检查概率导致较低权重的方式对 soft-max 中的术语进行加权是有意义的。但正确的权重是什么？这并不明显。    提交人    /u/alexsht1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d83yf3/d_combining_position_bias_into_softmaxcrossentropy/</guid>
      <pubDate>Tue, 04 Jun 2024 18:13:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] - 将不同词汇的 LLM 的概率分布输出融合在一起的最新进展是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d818iz/d_what_is_the_latest_in_fusing_the_probability/</link>
      <description><![CDATA[我最近看到了这些论文，它们将具有不同词汇的 LLM 的概率分布融合在一起 https://arxiv.org/pdf/2404.12715v2 - 本文使用无需训练的方法，将分布转换为共享空间，对它们进行平均，然后将它们重新转换为所选模型的分布 https://openreview.net/forum?id=jiDsk12qcz - 本文根据其他 LLM 的概率分布训练目标 LLM（对词汇差异有一些特殊逻辑） 第一篇论文特别有趣，因为它声称使用无需训练的方法比第二篇论文表现更好通过将不同的概率分布投射到共享空间中。这依赖于这样的假设：在不同模型中，标记之间的相对角度大致一致，他们声称这是经验支持的。 我也很好奇是否有任何类似的工作来融合隐藏状态，也许是在转换为标记之前的最后一层。原因是一些多模态输出模型在需要输出图像等内容时会动态地将其隐藏状态传递给解码器，因此仅融合标记的概率分布可能还不够。    提交人    /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d818iz/d_what_is_the_latest_in_fusing_the_probability/</guid>
      <pubDate>Tue, 04 Jun 2024 16:20:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] mamba.np：Mamba 的纯 NumPy 实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/</link>
      <description><![CDATA[      mamba.np 受到一些很棒的项目的启发，我用纯 Numpy 从头实现了 Mamba。代码的目标是简单、可读、轻量，因为它可以在本地 CPU 上运行。 https://github.com/idoh/mamba.np 希望您觉得它有用 :)    提交人    /u/id0h   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d80t26/p_mambanp_pure_numpy_implementation_of_mamba/</guid>
      <pubDate>Tue, 04 Jun 2024 16:02:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 数据、排序和内在维度对分层可导航小世界中回忆的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d805fb/r_the_impacts_of_data_ordering_and_intrinsic/</link>
      <description><![CDATA[本文深入探讨了分层可导航小世界 (HNSW) 图及其在向量搜索系统中的性能。本文揭示了可能影响检索系统效率和准确性的重要见解。 主要发现：  参数配置很重要：与精确 KNN 相比，HNSW 中配置不足的参数可能会使 NDCG@10 下降高达 18%，并改变模型排名。 数据插入顺序：将数据插入 HNSW 图的顺序可能会导致召回率相对变化高达 17%。 内在维度：数据集的内在维度越高，通常召回率越差。这对于理解不同模型和数据如何影响搜索性能至关重要。 局部内在维度 (LID)：插入之前按降序 LID 对数据进行排序可显着提高召回率，模仿优化图形条件的退火过程。 更高的默认参数：我们推荐的默认值 (efConstruction = 512、M = 16 和 efSearch = 2000) 优于许多现有配置，特别是在使用 Vespa 的系统中，它可以在不影响性能的情况下处理这些配置。 模型性能：当使用近似检索系统与精确 KNN 进行评估时，模型的排名会发生变化。较小的模型（例如 all-MiniLM-L6-v2 和 bge-micro）提高了它们在近似系统中的相对性能。 合成数据：本文通过生成具有受控内在维度的合成数据展示了内在维度如何影响召回率。  完整论文在此： https://arxiv.org/abs/2405.17813     提交人    /u/elliesleight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d805fb/r_the_impacts_of_data_ordering_and_intrinsic/</guid>
      <pubDate>Tue, 04 Jun 2024 15:35:17 GMT</pubDate>
    </item>
    <item>
      <title>[R]xLSTM官方代码+Kilcher视频</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d7zppm/r_xlstm_official_code_kilcher_video/</link>
      <description><![CDATA[论文，仅供参考，即将更新（根据视频 - 见下文）： https://arxiv.org/pdf/2405.04517 NX-AI 终于为他们的 xLSTM 实现发布了一个 python 包： https://github.com/nx-ai/xlstm Yannic Kilcher 还发布了一段新视频解释该论文： https://www.youtube.com/watch?v=0OaEv1a5jUM 有人重现了这篇论文的结果吗？我发现 sLSTM 是对 vanilla LSTM 的巨大改进，但我无法让 mLSTM 单独工作。     提交人    /u/Builder_Daemon   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d7zppm/r_xlstm_official_code_kilcher_video/</guid>
      <pubDate>Tue, 04 Jun 2024 15:17:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 图像超分辨率数据集修剪研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d7st4v/r_a_study_in_dataset_pruning_for_image/</link>
      <description><![CDATA[我们很高兴与大家分享我们最近的作品《图像超分辨率数据集修剪研究》，该作品已被 ICANN 2024 接受 :) 我们引入了一种基于损失值的采样方法，该方法将训练数据集缩减为由简单的预训练 SRCNN 模型确定的核心集（原始数据集的 50%）。通过专注于包含高损失值（即“困难样本”），我们获得的结果可与对完整数据集进行训练获得的结果相媲美甚至超过这些结果。此外，我们发现最难样本的前 5% 会对训练产生负面影响。排除这些样本可进一步增强结果，或者简而言之，选择 45-95% 的最难样本部分可获得最佳训练质量。我们希望为图像 SR 中数据集修剪的未开发潜力开辟新的视角，并为其他领域提供新的想法。 arXiv：https://arxiv.org/abs/2403.17083    提交人    /u/Maleficent_Stay_7737   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d7st4v/r_a_study_in_dataset_pruning_for_image/</guid>
      <pubDate>Tue, 04 Jun 2024 09:14:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>