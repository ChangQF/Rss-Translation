<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Mon, 27 May 2024 18:18:21 GMT</lastBuildDate>
    <item>
      <title>在 16GB GPU 和 24GB GPU 上运行的最佳 LLM 是什么？ （预算<3000欧元）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1x7w1/what_is_the_best_llm_i_could_run_on_a_16gb_gpu_vs/</link>
      <description><![CDATA[我正在为本地 LLM 实验目的构建家庭设置。我主要想进行推理和微调。  我的主要疑问是是否要购买 16GB RTX3080TI 笔记本电脑或 24GB RTX3090 台式机。 显然，我个人更喜欢笔记本电脑，因为它是便携式的。但我可以使用 teamviewer 和虚拟显示端口从我的笔记本电脑将其作为远程桌面连接到桌面，从而使桌面设置正常工作。  所以我的问题是：在 16GB GPU 和 24GB GPU 上运行的型号之间的性能是否存在明显差异。  具体来说，所谓的 16GB 型号，它们可以在 16GB GPU 上运行吗？或者是否存在一些开销，意味着实际可用的 GPU 内存小于 16GB？    提交人    /u/0ne2many   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1x7w1/what_is_the_best_llm_i_could_run_on_a_16gb_gpu_vs/</guid>
      <pubDate>Mon, 27 May 2024 17:40:18 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像潜在扩散 - 15 步深度学习者指南！ （视频）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1x0zi/texttoimage_latent_diffusion_a_15_step_deep/</link>
      <description><![CDATA[分享我频道中有关条件潜在扩散模型的视频，该视频分 15 个概念逐步解释该算法。除此之外，我还分享了我使用视频中提到的概念训练小型本地训练文本到图像模型以生成人脸（celeba 数据集）的经验。 这里是这些视频的链接感兴趣：https://youtu.be/w8YQcEd77_o    ;由   提交 /u/AvvYaa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1x0zi/texttoimage_latent_diffusion_a_15_step_deep/</guid>
      <pubDate>Mon, 27 May 2024 17:31:56 GMT</pubDate>
    </item>
    <item>
      <title>[P] MusicGPT – 一个使用本地法学硕士生成音乐的开源应用程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/</link>
      <description><![CDATA[大家好！ 想分享一下我过去几个月一直在做的最新副业。这是一个运行本地音乐生成模型的终端应用程序，目前只有 MusicGen by Meta 可用。 https://github.com/gabotechs/MusicGPT 它适用于 Windows、Linux 和MacOS 无需安装 Python 或任何重型机器学习框架。相反，它完全用 Rust 编写，使用 ONNX 运行时以高性能方式在本地运行 LM，甚至使用 GPU 等硬件加速器。 该应用程序的工作原理如下：  它接受用户的自然语言提示 根据提示生成音乐样本 编码生成的样本转换为 .wav 格式并在设备上播放  此外，它还提供了一个 UI，允许在类似聊天的 Web 应用程序中与 AI 模型进行交互，存储聊天历史记录和设备上生成的音乐。 该项目的愿景是最终能够实时生成无限的音乐流，例如，在编码时收听始终新的 LoFi 歌曲的无限流，但还没有完全实现...... 这是一个有趣的旅程，在 Rust 的受限环境中建立基于 Transformer 的模型并运行，没有 PyTorch 或 TensorFlow，希望你喜欢它！ &lt; /div&gt;  由   提交 /u/GabrielMusat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1vp2u/p_musicgpt_an_open_source_app_for_generating/</guid>
      <pubDate>Mon, 27 May 2024 16:34:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用图神经网络对列车延误进行多步并行预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1uawc/d_multistep_parallel_prediction_for_train_delays/</link>
      <description><![CDATA[    &lt; /a&gt;  大家好， 我目前正在做一个涉及使用的项目用于预测火车延误的图神经网络（GNN）。目标是对网络中的每列列车执行多步并行预测。主要挑战之一是考虑问题的时空维度以及整个铁路上列车之间的相互作用（参见图中网络的样子）。该数据集由给定时间的网络状态（包含当前列车）的快照组成 https://preview.redd.it/nywjb71fnz2d1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=f141d0e1badf1e5ec 7397972f6c1bd8051fc09f9 到目前为止，我已经提出了两种不成熟的方法，希望得到任何建议、评论或替代方案： 方法 1：异构图模型 在这种方法中，我构建了一个具有两种类型节点的异构图：车站和火车。这导致了三种类型的关系：火车站-车站、火车-火车和车站-车站。预测任务本质上是边缘预测，特别是火车站边缘。对于给定的火车，该模型旨在预测其与 n_following 车站的链接（延误）。 方法 2：同质图模型 我的第二个该方法涉及使用同质图，其中每个节点代表铁路网络中的一个重要点 (RP)，例如车站、交叉点等。每个节点都具有 RP 类型、地理位置和时间序列等特征代表火车在该 RP 随时间的延误情况的特征。该图中的边代表两个 RP 之间的直接行驶路径，具有平均行驶时间和每天列车数量等特征。 GNN 的输出是接下来几个时间步中每个 RP 的预测延迟。 在现有文献中，模型要么是针对一列火车的预测，要么铁路是由一条线路组成的这不是我想要的。  如果您有任何见解或建议，我将不胜感激。提前致谢！   由   提交 /u/OtherDepartment8085   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1uawc/d_multistep_parallel_prediction_for_train_delays/</guid>
      <pubDate>Mon, 27 May 2024 15:35:11 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost：特征选择的首选方法？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1u0yd/xgboost_preffered_method_of_feature_selection_d/</link>
      <description><![CDATA[方法 1 - 形状：删除平均绝对形状值低于特定值的特征 方法 2 - 特征重要性：删除特征特征重要性值低于特定值 方法 3 - R 平方：从模型中单独删除每个特征，并计算每个单独模型的 R2 分数。应删除不会显着增加 R2 分数的特征 方法 4 - 保留所有特征并让 XGBoost 对其进行排序 您对这些特征的相对功效有何看法方法以及您喜欢使用的任何其他方法，特别是 XGBoost？   由   提交 /u/Gef_1_Man_Army   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1u0yd/xgboost_preffered_method_of_feature_selection_d/</guid>
      <pubDate>Mon, 27 May 2024 15:23:07 GMT</pubDate>
    </item>
    <item>
      <title>[P]如何获得LSUN-CAT</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1tefs/p_how_can_i_get_lsuncat/</link>
      <description><![CDATA[我注意到几篇研究论文提到了 LSUN-CAT 数据集，该数据集似乎广泛用于图像生成任务。但是，我无法找到该数据集的下载链接，因为官方 LSUN github 没有“cat”选项。 class https://github.com/fyu/lsun。 任何人都可以提供有关如何访问的指导吗它？以下是其在图像生成中的使用参考：https://paperswithcode.com/ sota/image- Generation-on-lsun-cat-256-x-256 ADM 还提供使用数据集训练的权重：https://github.com/openai/guided-diffusion 感谢您的帮助！ 编辑： 我还发现官方数据集网站已关闭：https://www.vis.xyz/p/lsun&lt; /a&gt;   由   提交/u/National-Resident244  /u/National-Resident244 reddit.com/r/MachineLearning/comments/1d1tefs/p_how_can_i_get_lsuncat/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1tefs/p_how_can_i_get_lsuncat/</guid>
      <pubDate>Mon, 27 May 2024 14:56:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离散多令牌方法与传递多模式输出和机器人控制隐藏状态的单令牌方法相比如何？行业整体趋势如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1rmjl/d_how_do_discrete_multitoken_approaches_compare/</link>
      <description><![CDATA[对于连续输出，我见过使用两种不同的方法 1：模型给出了离散标记的词汇表，并可以组装它们的列表，并将其传递给解码器 2：模型为每个输出模态赋予一个标记（例如，图像标记），并且在选择时将隐藏状态传递给解码器 这两种方法的最新趋势是什么？ 2 看起来会快很多，因为你只需要 1 个令牌，但这对损失函数有何影响，因为有时你会遇到回归问题（图像生成），有时你会遇到分类问题？    由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1rmjl/d_how_do_discrete_multitoken_approaches_compare/</guid>
      <pubDate>Mon, 27 May 2024 13:33:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 分形网络曾经被扩展过吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1ring/d_was_fractal_net_ever_expanded_upon/</link>
      <description><![CDATA[我一直在阅读《FractalNet：超深度神经网络》没有残差的网络”，我想知道 FractalNet 背后的方法是否在其他文章中得到了改进。   由   提交/u/research_pie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1ring/d_was_fractal_net_ever_expanded_upon/</guid>
      <pubDate>Mon, 27 May 2024 13:28:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] AstroPT：扩展天文学大型观测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1nirc/r_astropt_scaling_large_observation_models_for/</link>
      <description><![CDATA[ 由   提交/u/Smith4242  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1nirc/r_astropt_scaling_large_observation_models_for/</guid>
      <pubDate>Mon, 27 May 2024 09:19:38 GMT</pubDate>
    </item>
    <item>
      <title>进行 ML/DL 研究的终极方式是我的方式还是你的方式？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1mxzo/ultimate_way_of_doing_research_in_mldl_is_my_way/</link>
      <description><![CDATA[嗨， 这篇文章旨在邀请您分享您处理研究的方法。我想深入研究像您这样的研究人员如何想象和处理问题，特别是如何提出想法和假设。  个人反思： 作为 3D 视觉领域，特别是点云分类/分割领域的研究人员，我&#39;我目前正在反思我自己的研究方法。就在最近，我突然意识到，我对作为一名初级研究员所做贡献的整个方式提出了质疑。  案例研究： 为了说明我当前的研究风格，让我们考虑一下我最近的一个想法。最近，我花了部分时间探索一种新颖的点云分类轻量级架构。探索了沿着点云（3d 对象）的纵轴将对称对象一分为二（切成两半）的想法。这种方法旨在实现两个目标：减少数据大小和潜在的数据增强。如果我们在管道中的某个点有两个处理分支怎么办？  正如你所看到的，我的想法/假设完全是抽象的。我正在以一种半有形且琐碎的人类理解逻辑的方式走进这些想法。正确的？  忽略这个想法是否符合我首先提到的研究目标。这不关我的事。  中心问题：  我特别想知道这是处理问题的正确方法吗？ ？这是进行研究的最终方式吗？这个领域的想法就是这样诞生的？ 我有限的数学背景是否会阻碍更系统、数学驱动的方法来研究解决问题、可视化和思考？ 如果数学不一定是最初的重点，那么它在什么阶段变得至关重要？以后的整合会带来什么优势？ 请分享您的经验和方法。  ​   由   提交 /u/Same_Half3758   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1mxzo/ultimate_way_of_doing_research_in_mldl_is_my_way/</guid>
      <pubDate>Mon, 27 May 2024 08:35:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪些 LLM 进步也适用于 100M 以下参数模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1m9ux/d_which_llm_advancements_also_work_for_sub_100m/</link>
      <description><![CDATA[大家好， 我想从头开始训练一个具有大约 50M 参数的小型语言模型。我想知道我应该为此使用什么样的架构。 我应该坚持使用良好的 GPT2 还是像分组查询注意 (GQA) 这样的技术也适用于如此小的模型大小？旋转位置嵌入 (RoPE) 也是如此。现代法学硕士似乎大多使用它，但 GPT2 没有。然后是规范化层的放置，我也不确定。 如果我只选择一个现成的架构并将其缩小，可能是最好的，但是这您现在可以推荐一个吗？为什么？   由   提交/u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1m9ux/d_which_llm_advancements_also_work_for_sub_100m/</guid>
      <pubDate>Mon, 27 May 2024 07:45:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] ASL ⭤ 英语翻译，带有 MediaPipe、PointNet、ThreeJS 和 Embeddings</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d1k3nw/p_asl_english_translation_w_mediapipe_pointnet/</link>
      <description><![CDATA[嘿！我是凯文·托马斯 (Kevin Thomas)，是伯纳比南中学（也是不列颠哥伦比亚省聋人学校的所在地）的 11 年级学生！ 在过去的几个月里，我一直在开发一种工具，可以在美国语言和英语之间进行翻译手语 (ASL) 和英语。大多数现有的 ASL 翻译工具都建立在 ASL 与英语是同一种语言的误解之上。基本上，他们只将耳聋视为一种残疾，只寻求克服听力障碍，而不是翻译成美国手语本身的语言。 在我的美国手语老师的指导下，我一直在致力于一项该项目促进了这种翻译，同时尊重和保留美国手语作为主要语言。对于 ASL 接收，我使用 Google MediaPipe 增强了 100,000 多张 ASL 字母图像，并训练了一个 PointNet 模型来对聋人手写的手形进行分类。对于 ASL 表达，我增强了 9,000 多个 ASL 手势视频，嵌入了相应的单词，然后使用 ThreeJS 标记了听力正常的人所说的单词。我还使用法学硕士来提高准确性并在英语和 ASL 语法之间进行翻译。 这是一个演示（和解释器）YouTube 视频 这里是GitHub 存储库 我是在过去几个月才开始研究 ML/AI 的！我将不胜感激任何反馈、机会或资源来继续学习和成长！请随时通过 Reddit DM 或 kevin.jt2007@gmail.com 与我联系！另外喜欢这个 Linkedin 帖子 将会转到路还很长🙏🫶   由   提交 /u/TrustedMercury   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d1k3nw/p_asl_english_translation_w_mediapipe_pointnet/</guid>
      <pubDate>Mon, 27 May 2024 05:15:02 GMT</pubDate>
    </item>
    <item>
      <title>[D]《创世记》等中文文本经过精心翻译，与英文语义完全相同，但却占用了一半的内存。由于每字节语义密度更高，使用中文训练 LLM 是否会更有效？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d152ps/d_chinese_text_such_as_genesis_meticulously/</link>
      <description><![CDATA[   /u/Civil_Repair  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d152ps/d_chinese_text_such_as_genesis_meticulously/</guid>
      <pubDate>Sun, 26 May 2024 16:25:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] ReRecall：我尝试使用开源模型和工具重新创建 Microsoft 的 Recall</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d14pad/p_rerecall_i_tried_to_recreate_microsofts_recall/</link>
      <description><![CDATA[       对我来说，Recall 听起来像是一场隐私噩梦，所以我想我可以尝试仅使用开源组件来制作类似的东西。如果您想尝试一下，这里是代码： https://github.com/AbdBarho/ReRecall&lt; /a&gt; 总的来说，它比我预期的要好，我使用 `mss` 来截取显示器的屏幕截图，并使用 ollama 和 llava 以及 mxbai embed 生成屏幕截图的描述和嵌入，然后是 chromadb 进行存储和搜索。 这里绝对有巨大的改进空间：  生成的屏幕截图描述中有很多幻觉，这可能是一个MLLM 用于生成描述的尺寸组合（我使用非常小的模型，因为我有生锈的 1060），或者因为屏幕截图的分辨率非常高（屏幕截图后没有调整大小）。 &lt; li&gt;搜索非常基本，它只是将查询文本的嵌入与屏幕截图的嵌入进行匹配，潜在的改进可能是使用该模型在嵌入搜索之前使用更多信息来丰富用户查询。 我相当肯定，微软不会像我一样仅仅依赖屏幕截图，还会捕获各个应用程序窗口，并提取元信息，例如窗口标题，甚至可能是窗口的文本内容（与使用的相同文本）通过针对视障人士的文本转语音程序），这些绝对可以改善结果。  您对可以更改的内容有任何进一步的想法吗？ 示例（精选）： 屏幕右侧对应的 ReRecall 用法在左侧   由   提交/u/Abdoo2  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d14pad/p_rerecall_i_tried_to_recreate_microsofts_recall/</guid>
      <pubDate>Sun, 26 May 2024 16:08:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>