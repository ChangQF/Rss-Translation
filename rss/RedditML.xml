<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 18 Apr 2024 03:14:01 GMT</lastBuildDate>
    <item>
      <title>[D] 如何准备（非常）提前面试+职业建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6srww/d_how_to_prepare_in_very_advance_interview_career/</link>
      <description><![CDATA[我的职业目标是成为一名数据科学家或机器学习工程师。了解这些职位目前的入门门槛非常高，我正在寻找下一个能让我更接近目标角色的职位。 我目前担任以下公司的软件工程师（前端） 1年，获得CS专业ML硕士学位，获得DS学士学位。我应该寻找什么职位来提升我的形象？数据工程师？数据分析师？数据平台中的SWE？  我应该如何准备这些职位的面试？力扣75？ SQL？技能范围如此多样化，我需要将精力集中在正确的篮子上。不可能把所有鸡蛋放在一个篮子里，那么实现我的目标的计划是什么？   由   提交/u/Puzzleheaded_Ad_224  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6srww/d_how_to_prepare_in_very_advance_interview_career/</guid>
      <pubDate>Thu, 18 Apr 2024 02:50:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练 VQGAN 但 GAN 损失持续上升</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6rtlt/p_training_a_vqgan_but_gan_loss_keeps_going_up/</link>
      <description><![CDATA[     &lt; td&gt; https://preview.redd.it/h13z13eu a5vc1.png?width=640&amp;format=png&amp;auto=webp&amp;s=397d5127453b2f4a1d6f6df28fb5fc 8a2f2f0cff 我认为VQ 损失和感知损失看起来很正常，但我觉得很难理解为什么判别器会走向完全不同的方向……以前有人见过类似的事情吗？ 更多细节：我正在训练 vqgan imagenet 来自论文 Taming Transformers for High-Resolution Image Synthesis   由   提交/u/darthjaja6   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6rtlt/p_training_a_vqgan_but_gan_loss_keeps_going_up/</guid>
      <pubDate>Thu, 18 Apr 2024 02:02:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] [2404.10667] VASA-1：实时生成逼真的音频驱动的说话面孔</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6q61y/r_240410667_vasa1_lifelike_audiodriven_talking/</link>
      <description><![CDATA[ 由   提交/u/s6x  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6q61y/r_240410667_vasa1_lifelike_audiodriven_talking/</guid>
      <pubDate>Thu, 18 Apr 2024 00:41:07 GMT</pubDate>
    </item>
    <item>
      <title>[N] 美联储任命“人工智能末日者”来管理美国人工智能安全研究所</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6nps8/n_feds_appoint_ai_doomer_to_run_us_ai_safety/</link>
      <description><![CDATA[https://arstechnica.com/tech-policy/2024/04/feds-appoint-ai-doomer-to-run-us-ai-safety-institute/ 文章简介： 被任命为 AI 安全负责人的是 Paul Christiano，他是前 OpenAI 研究员，开创了一种名为基于人类反馈的强化学习的基础 AI 安全技术（ RLHF），但也因预测“人工智能发展有 50% 的机会以‘厄运’而告终”而闻名。尽管克里斯蒂安诺的研究背景令人印象深刻，但一些人担心，任命所谓的“人工智能厄运者”会带来灾难。 NIST 可能冒着鼓励非科学思维的风险，许多批评家认为这些思维纯粹是猜测。   由   提交/u/bregav  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6nps8/n_feds_appoint_ai_doomer_to_run_us_ai_safety/</guid>
      <pubDate>Wed, 17 Apr 2024 22:49:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 风险规避是否会阻碍云抽象的采用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6m8b5/d_is_risk_aversion_crushing_the_adoption_of_cloud/</link>
      <description><![CDATA[大家好， 我想我们中的许多人都同意，定义我们想要使用的硬件旁边的一块我们正在运行的代码客观上是更好的开发人员体验。我一直喜欢降低在云中运行代码的障碍的想法。随着越来越多的云抽象进入市场，老实说，我对缺乏采用感到非常惊讶。这个领域还没有任何独角兽（我认为实际上没有），只是 A 系列企业。 在与一些数据科学家、机器学习工程师和 DevOps 工程师交谈后，它我开始意识到，风险规避是造成大部分摩擦的原因。使用完全托管的服务肯定有一些好处，在很多情况下，我更喜欢使用它们，但说服你的老板将 PB 级数据传输到另一家公司的云并产生 3-5 倍的计算成本可能不太好。还有一些开源替代方案，但它们故意难以配置，因此您需要为减少配置设置的高级产品付费。 很想听听每个人的想法，特别是那些在精益初创公司和全球 5,000 强公司工作的人公司。    由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6m8b5/d_is_risk_aversion_crushing_the_adoption_of_cloud/</guid>
      <pubDate>Wed, 17 Apr 2024 21:47:42 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]统计学博士就业前景</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6l20i/discussion_phd_in_statistics_job_prospects/</link>
      <description><![CDATA[我很想知道银行和银行业的工作机会鉴于当前的市场状况，为攻读统计学博士学位的人提供保险。   由   提交 /u/SpiritualCellist4303   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6l20i/discussion_phd_in_statistics_job_prospects/</guid>
      <pubDate>Wed, 17 Apr 2024 20:59:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有办法确定模型学习的表示是球形还是双曲形？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6kt3a/d_is_there_a_way_to_determine_if_the/</link>
      <description><![CDATA[标题。有没有办法确定特征提取器针对已训练/将测试的一组示例学习的嵌入的球形度或双曲度？ 我是深度学习中的几何新手。如果有人也能给我指出一篇论文或一本书来开始这方面的工作，那就太好了。提前致谢。   由   提交/u/Mad_Scientist2027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6kt3a/d_is_there_a_way_to_determine_if_the/</guid>
      <pubDate>Wed, 17 Apr 2024 20:49:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] RuleOpt：基于优化的分类规则学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6ix86/r_ruleopt_optimizationbased_rule_learning_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2104.10751 软件包： https://github .com/sametcopur/ruleopt 文档： https://ruleopt.readthedocs .io/ RuleOpt是一种基于优化的规则学习算法，专为分类问题而设计。 RuleOpt注重可扩展性和可解释性，利用线性编程进行规则生成和提取。 Python库ruleopt能够从集成模型中提取规则，并且还实现了一种新颖的规则生成方案。该库确保与现有机器学习管道的兼容性，并且对于解决大规模问题特别有效。 以下是ruleopt的一些亮点：  高效的规则生成和提取：利用线性编程进行可扩展的规则生成（独立的机器学习方法）以及从训练有素的随机森林和增强模型中提取规则。 可解释性：通过将成本分配给规则来优先考虑模型透明度，以实现与准确性的理想平衡。 与机器学习库集成：促进与知名 Python 库 scikit 的顺利集成-learn、LightGBM 和 XGBoost 以及现有的机器学习管道。 广泛的求解器支持：支持多种求解器，包括 Gurobi、&lt; em&gt;CPLEX 和 OR-Tools。    由   提交/u/zedeleyici3401   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6ix86/r_ruleopt_optimizationbased_rule_learning_for/</guid>
      <pubDate>Wed, 17 Apr 2024 19:34:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] LSTM时间序列预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6igqk/d_lstm_time_series_forecasting/</link>
      <description><![CDATA[        由   提交 /u/StressAccomplished26   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6igqk/d_lstm_time_series_forecasting/</guid>
      <pubDate>Wed, 17 Apr 2024 19:15:46 GMT</pubDate>
    </item>
    <item>
      <title>[D]问题：时间序列解码到非时间潜在空间？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6fa97/d_question_timeseries_decoding_to_nontemporal/</link>
      <description><![CDATA[您好！我是一名计算神经科学研究员，希望将一些现代机器学习技术应用于功能磁共振成像时间序列数据。我收集了一组高维 4D fMRI 时间序列数据，这些数据是在受试者定期观察 COCO 的自然图像时收集的。我们目前拥有采用预处理“快照”的解码模型。该时间序列数据被扁平化为在观察图像的短时间内聚合的激活模式，并使用一些机器学习模型来解码和重建来自大脑的图像内容。 （请参阅我的一些最近的工作）。 我很好奇存在什么样的机器学习技术可能能够处理时间序列数据本身，而不必将时间序列折叠为单个快照来执行我们的解码过程。我设想的是一个模型（可能是一个变压器），它可以将高维多通道时间序列作为输入，并输出与图像刺激相对应的扁平潜在表示（例如 CLIP 向量），甚至是由已知的规则间隔（正如我们在不同图像呈现的数据中所具有的那样）。据我所知，时间序列数据的机器学习的大部分工作都是预测，但我想要的是静态（或可能重复的）输出。我希望更详细的时间序列数据将具有额外的信号，从而提高 fMRI 视觉解码的解码性能。 ML 领域是否有任何现有的工作已经解决了类似的问题？  &gt;   由   提交 /u/reesespike   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6fa97/d_question_timeseries_decoding_to_nontemporal/</guid>
      <pubDate>Wed, 17 Apr 2024 17:08:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在交叉注意力中，为什么 Q 取自解码器，K 取自编码器输出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</link>
      <description><![CDATA[我查了很多地方但找不到答案。如果我们分别将 Q 和 K 来自编码器和解码器，会发生什么？会有什么不同吗？   由   提交/u/shuvamg007  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</guid>
      <pubDate>Wed, 17 Apr 2024 14:49:10 GMT</pubDate>
    </item>
    <item>
      <title>[D]视觉语言模型中视觉嵌入如何与语言嵌入空间共存？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</link>
      <description><![CDATA[大家好！我很高兴能讨论大视觉语言模型 (LVLM)。由于我们可能是最大的法学硕士社区，我认为这个频道将是开始这次对话的完美场所。此外，关于将视觉和语言嵌入结合起来的内容并不多。 LVLM 的一些背景：它们通常由图像的视觉编码器、文本的常规标记器、像投影层这样的投影层组成。 MLP 将视觉特征与文本嵌入空间对齐，最后合并图像和文本嵌入以发送到 LLM 模型中。输入包括文本和图像，而输出是文本，使其成为多模式法学硕士。查看 LLaVA 论文中的图表，了解直观的细分： https://preview.redd.it/l222askgu1vc1.png?width=1607&amp;format=png&amp; ;auto=webp&amp;s=ef011e16301c22b4751d8d0a8f3698f70e3ffd26 从像 CLIP ViT 这样的视觉编码器开始，模型从图像中学习视觉信息，然后使用 MLP 将其投影到 LLM 的嵌入空间上。该论文将这种特征称为对齐。我很好奇视觉嵌入如何与文本嵌入交互，因此我尝试使用 PCA 以 3D 方式可视化它们。 例如，采用 llava-7B 模型 - 它使用 llama-7B 后端和32k 词汇量和 4096 个维度，使得嵌入大小为：[32000,4096]。我使用了一个简单的提示，“向我解释一下这张图片”。使用猫的图片来查看嵌入如何出现在我们的空间中。 https://preview.redd.it/032oy0yn u1vc1.png?width=662&amp;format=png&amp;auto=webp&amp;s=d037bbecc976392e159a1c1bde775ef1e148 488d 添加视觉标记改变了动态。每个图像转换为 576 个形状的视觉标记 [576,4096]。查看包含这些标记时绘图如何调整： https ://preview.redd.it/vdeacylwu1vc1.png?width=566&amp;format=png&amp;auto=webp&amp;s=42441b4fd515cee916b40243429b4aa6820b998c 那我觉得怎么样？ 首先，我们不会直接将视觉标记转换为文本。最近的一篇 Google 论文尝试过，发现这不是最好的方法。视觉推理似乎徘徊在文本嵌入空间附近，可能是因为图像的信息更密集，需要更多的标记来表示视觉概念。 其次，这种设置目前看来是正确的。视觉标记与文本标记一起，将图像衍生的上下文添加到 LLM，使其能够“看到”图像。 最后，尽管 llava 在视觉推理的一些基准测试中表现良好，但它可能还不是最有效的图像表示方法。最近的一些研究谈到了注意力稀疏现象，尤其是 LVLM 中的视觉标记。我们很幸运，因为注意力算法只关注有意义的视觉标记并忽略噪音。 你觉得怎么样？谢谢阅读。 :-)   由   提交/u/E-fazz  /u/E-fazz  reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</guid>
      <pubDate>Wed, 17 Apr 2024 14:41:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究中数学和算法哪个优先？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</link>
      <description><![CDATA[我现在正在学习扩散背后的方法（DDPM、基于分数的方法和其他方法）。我想知道研究人员到底是如何想出这个想法的？ 发明新方法是这样的吗？ 1.我们想要制作更好的图像生成器。 2. 哦，数据永远不够...... 3. 让我们乘以数据 - 通过添加一些噪声损坏 4. 这个效果很好，如果我们制作一个去噪网络怎么办？ 5. 如果我们建立一个由纯噪声生成图像的网络会怎么样？ 6. 这不行，如果我们做更小的去噪步骤怎么办？ 7. 这有效！现在，让我们创建一些关于它为何起作用的理论。 8.写论文 或者类似的东西？ 1.我们想要制作更好的图像生成器。 2.我们知道“非平衡热力学”非常好，想尝试以某种方式应用它 3. 我们以某种方式想出了一种依赖于该理论的数学的算法 4. 它有效！ 5. 我们写论文。 通常哪个先出现？数学还是算法？   由   提交/u/Deep-Station-1746   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</guid>
      <pubDate>Wed, 17 Apr 2024 08:22:11 GMT</pubDate>
    </item>
    <item>
      <title>AI/ML 数据中心的未来将是 100 台甚至 1000 台服务器像一个巨型加速器一样运行 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</link>
      <description><![CDATA[在服务器公司 Gigabyte 的网站上看到了这个信息丰富的视频 (https://youtu.be/2Q7S-CbnAAY?si=DJtU2mQ_ZKRZ83Nf），简而言之，服务器品牌现在将完整的服务器集群运送到数据中心，而不是单个服务器机器。在此所示的示例中，它有 8 个机架（另外一个用于管理和网络），每个机架中有 4 台相同型号的服务器，以及 4 个相同的超高级 GPU每个服务器中的模型。为您计算一下，每个集群有 32 台服务器或 256 个 GPU 加速器。请注意，所有服务器和 GPU 都必须是相同的型号，因为它们的连接方式基本上是作为一台单独的机器运行。 这很可能是标准构建块的原因。所有人工智能数据中心的特点是，我们现在利用大型数据集训练人工智能的方式，参数数量达到数十亿，甚至数万亿。对于为我们带来 ChatGPT 及其同类产品的法学硕士来说尤其如此。以任何效率处理这些数万亿个参数的唯一方法是通过我们以前从未见过的规模的并行计算。因此，这个大胆的新概念将数百甚至数千台服务器连接在一起，因此它们基本上是一台巨型服务器，加载了 Nvidia 或其他品牌的数千个 GPU。真正令人着迷的东西，我还没有看到目前为人工智能计算的未来提出的任何其他规模的东西。 这是视频中介绍的集群的网站：https://www.gigabyte.com/Industry-Solutions/giga-pod-as-a-service ?lan=en   由   提交/u/Low_Complaint2254   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</guid>
      <pubDate>Wed, 17 Apr 2024 06:16:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>