<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 29 Nov 2024 18:23:12 GMT</lastBuildDate>
    <item>
      <title>[N][R] 模型就是食物：法学硕士的自动数据管理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/</link>
      <description><![CDATA[在 [DatologyAI](datologyai.com) 上分享我们的最新成果。模型就是它们的食物，我们的使命是让训练大型模型的数据管理尽可能有效和简单。 通过结合多种方法，包括启发式过滤器、基于模型的过滤器、基于嵌入的管理、合成数据、目标分布匹配和混合比率，我们能够大幅提高训练效率、性能和推理效率。  与我们的基线和起始数据集（精确去重的 RedPajamav1）相比，我们可以：  以 7.7 倍的速度达到相同的性能（比 DCLM 快 3.4 倍） 在基准测试中将性能提高 8.5%（比 DCLM 提高 4.4%） 使用不到一半的参数训练模型，其性能比大型模型高出 5% 以上  请在此处查看我们的高级结果，如果您需要所有细节，请查看我们的 技术深度探究。    提交人    /u/arimorcos   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2qmol/nr_models_are_what_they_eat_automatic_data/</guid>
      <pubDate>Fri, 29 Nov 2024 17:15:57 GMT</pubDate>
    </item>
    <item>
      <title>“[P]”RFM中的静态变量和动态变量表</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/</link>
      <description><![CDATA[我正在使用随机森林创建一个预测模型。但我不明白模型和脚本如何考虑以数据框形式加载的两个表。 当一个表具有静态属性（如食物特征）而另一个表具有动态因素（如日常健康习惯）时，在随机森林模型中使用多个表的最佳方法是什么？ 例如：我想根据我吃的食物（不变）和日常因素（睡眠、饮水量）预测胃痛。 表格： * 静态：食物名称、卡路里、肉类（是/否） * 动态：天数、睡眠良好（是/否）、喝水（是/否） 如何在随机森林模型中组合这些表？它们是否应该合并到像“天数”这样的唯一标识符上？    提交人    /u/peyott100   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2oe69/pstatic_variable_and_dynamic_variable_tables_in/</guid>
      <pubDate>Fri, 29 Nov 2024 15:36:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] Hinton 和 Hassabis 论乔姆斯基的语言理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/</link>
      <description><![CDATA[我刚进入这个领域，很想听听更多这方面的观点。我一直认为乔姆斯基是这方面的重要人物，但似乎 Hinton 和 Hassabis（后来）都不同意。这里：https://www.youtube.com/watch?v=urBFz6-gHGY 我很想从 ML 和 CogSci 的角度来看待这个问题，以及更多支持/拒绝这种观点的来源。 编辑：拼写错误    提交人    /u/giuuilfobfyvihksmk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2mkye/d_hinton_and_hassabis_on_chomskys_theory_of/</guid>
      <pubDate>Fri, 29 Nov 2024 14:10:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] COLING 2025 最终录取 - 还没有出来吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/</link>
      <description><![CDATA[最终录取结果出来了吗？我在 Softconf 上还没有看到。有一篇论文的评论是 (5,4) (4,4) (4,4)。    提交人    /u/UnhappyPrior6570   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2kfic/d_coling_2025_final_acceptances_is_it_not_out_yet/</guid>
      <pubDate>Fri, 29 Nov 2024 12:10:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 矢量场（已知和未知）之间的递归插值方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2io35/r_recursive_methods_for_interpolation_between/</guid>
      <pubDate>Fri, 29 Nov 2024 10:08:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果 VQ-VAE 能够解开的话，它该如何解开呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/</link>
      <description><![CDATA[我目前使用 BetaTC-VAE，它在解缠方面做得非常出色，我知道 VAE 可以稍微解缠，因为对于模型来说，如果变量解缠，更容易获得较低的 KL 损失，beta 项使这个 beta 倍更重要，总相关性和互信息损失推动完全解缠，但在 VQ-VAE 中没有（主要）解缠，只有码本和离散输出。码本给出的离散潜在空间可以解缠吗？如果不能，有没有关于解缠 VQ-VAE 的论文？我有一个环境，其中解缠的潜在空间比连续潜在空间提供更好的重建     提交人    /u/ZazaGaza213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/</guid>
      <pubDate>Fri, 29 Nov 2024 05:33:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] Transformer 注意力模型不一致</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2dmox/r_transformer_attention_figure_inconsistent/</link>
      <description><![CDATA[      我是一名学生目前正在研究 Transformers，我正在努力提高此存储库中代码的性能：Transformer 实现。 最初，我注意到模型的成本没有收敛，这导致了完全错误的输出。为了解决这个问题，我将学习率从 0.001 调整为 0.0001。经过这一改变，模型开始收敛并产生了正确的句子。 然而，在可视化编码器自注意力、解码器自注意力和编码器-解码器注意力的图表时，注意力图没有显示每个单词的预期权重。我不确定如何解释这些结果，或者代码本身是否存在问题。 如果有人可以帮助解释这些数字或提供有关实施中潜在问题的见解，我将不胜感激。 这些是我将学习率从 0.001 设置为 0.0001 后绘制的数字 编码器自注意力 解码器自注意力 编码器-解码器注意力    提交人    /u/Master-Regular458   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2dmox/r_transformer_attention_figure_inconsistent/</guid>
      <pubDate>Fri, 29 Nov 2024 04:27:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 隐式正则化领域中最重要的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/</link>
      <description><![CDATA[大家好 我正在研究机器学习，尤其是理论方面，我很好奇为什么神经网络往往具有如此好的泛化能力，所以我希望阅读一些关于这方面的论文。据我所知，关于这个主题的第一篇重要论文是张等人的《理解深度学习需要重新思考泛化》。 我有很好的数学背景，所以我想知道人们认为这个领域最有影响力的论文是什么。你认为哪篇论文影响最大？    提交人    /u/MrBeebins   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/</guid>
      <pubDate>Fri, 29 Nov 2024 00:23:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 本地检索增强生成（完全本地解决方案）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/</link>
      <description><![CDATA[大家好， 我很高兴与您分享我的最新 repo — 适用于您文件的本地对话式 RAG 解决方案！情况是这样的：此设置非常适合在本地运行 RAG。 它由 Docker、LangChain、Ollama、FastAPI 和 Hugging Face 构建，所有模型都会自动下载。很快，我将添加对选择您喜欢的模型的支持，但以下是解决方案当前包含的内容： • 本地运行 Ollama：它目前硬编码到 Qwen-0.5B 模型，但即将从 Ollama 注册表中进行模型选择。 • 本地索引：使用句子转换器嵌入模型（当前仅限于此系列，但这也将很快改变）。 • Qdrant 容器：在本地运行以进行向量存储。 • 本地重新排名器：当前使用 BAAI/bge-reranker-base，即将支持重新排名器选择。 • 基于 Websocket 的聊天：包括历史记录保存功能。 • 简单的聊天 UI：使用 React 构建以获得直观的界面。 • 奖励：您可以将此设置与 ChatGPT 一起用作自定义 GPT！通过官方 ChatGPT 网络界面或 macOS/iOS 应用查询本地数据。 • 本地就绪：一切都在本地运行，并且容器对 CPU 友好。 一些想法和已知问题： • 对模型上下文协议的支持已列入计划。 • 尚无增量索引或重新索引。 • 模型选择尚不可用，但很快就会添加。 我很乐意听取您的反馈、贡献或支持 — 如果您觉得这很有趣，请关注、分叉和加星标！ 谢谢！ https://github.com/dmayboroda/minima     提交人    /u/davidvroda   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/</guid>
      <pubDate>Thu, 28 Nov 2024 22:01:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] BitNet a4.8：1 位 LLM 的 4 位激活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/</link>
      <description><![CDATA[      论文： https://arxiv.org/pdf/2411.04965 摘要：  最近对 1 位大型语言模型 (LLM)（例如 BitNet b1.58）的研究为降低 LLM 的推理成本同时保持其性能提供了一个有希望的方向。在这项工作中，我们引入了 BitNet a4.8，为 1 位 LLM 启用 4 位激活。BitNet a4.8 采用混合量化和稀疏化策略来减轻异常通道引入的量化误差。具体来说，我们利用 4 位激活作为注意力和前馈网络层的输入，同时稀疏中间状态，然后进行 8 位量化。大量实验表明，BitNet a4.8 在训练成本相当的情况下，实现了与 BitNet b1.58 相当的性能，同时在启用 4 位（INT4/FP4）内核的情况下，推理速度更快。此外，BitNet a4.8 仅激活 55% 的参数并支持 3 位 KV 缓存，进一步提升了大规模 LLM 部署和推理的效率。  Visual Abstract: https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a 评估： HS=HellaSwag, PQ=PiQA, WGe=WinoGrande https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/</guid>
      <pubDate>Thu, 28 Nov 2024 15:11:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 GPU 并行实现基于矩阵的快速反事实遗憾最小化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/</link>
      <description><![CDATA[反事实遗憾最小化 (CFR) 的一种新颖的 GPU 实现，可加速广泛形式游戏中最优策略的计算。核心创新是跨 GPU 核心并行化遗憾更新和策略计算，同时仔细管理内存访问模式。 关键技术要点： - 将游戏状态和操作映射到 GPU 线程的自定义内存布局 - 批量处理信息集以最大化 GPU 利用率 - 并行计算反事实值和遗憾更新 - 通过游戏树分区实现多 GPU 扩展 - 在 Leduc Hold&#39;em 和 Limit Texas Hold&#39;em 扑克变体上进行评估 结果： - 与 CPU 实现相比，速度提高了 30 倍 - GPU 数量线性扩展，最多 8 个设备 - 内存使用量随游戏大小和信息集数量而扩展 - 解决方案质量在统计误差范围内与 CPU 基线匹配 - 成功解决了多达 1014 个状态的游戏 我认为这项工作可以使 CFR 在扑克以外的实际应用中更加实用。更快地解决大型游戏的能力为自动谈判、安全游戏和资源分配等领域开辟了可能性。多 GPU 扩展尤其有趣，因为它表明了解决更复杂游戏的潜力。 此处开发的内存优化技术也可能很好地转移到需要有效处理大状态空间的其他博弈论算法。 TLDR：GPU 加速 CFR 实现通过仔细的并行化和内存管理实现了 30 倍的加速，并具有线性多 GPU 扩展。使解决大型广泛形式游戏变得更加容易。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/</guid>
      <pubDate>Thu, 28 Nov 2024 14:08:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代扩散模型背后的理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/</link>
      <description><![CDATA[大家好， 我最近在大学里听了一些关于扩散模型的讲座。这些讲座详细解释了原始 DDPM（去拟态扩散概率模型）背后的所有数学知识（尤其是在附录中），实际上比我在网上找到的任何其他知识都要好。因此，它对于学习扩散模型背后的基础知识非常有帮助（如果您有兴趣，可以在此处的自述文件中的链接中找到幻灯片：https://github.com/julioasotodv/ie-C4-466671-diffusion-models） 但是，我正在努力寻找具有类似现代方法详细程度的资源 - 例如流匹配/整流流，用于采样的不同 ODE 求解器的工作原理等。有一些，但我发现的一切要么相当过时（比如从 2​​023 年左右开始），要么非常肤浅 - 例如对于非技术或科学受众而言。 因此，我想知道：除了原始论文之外，是否有人遇到过超出基本扩散模型的理论解释的良好汇编？我们的目标是让我的团队深入研究他们想要的实际论文，但要将其中 70% 的内容放在一个或多个像样的汇编中。 我真的相信，如今 SEO 让任何搜索都变成了一场噩梦。要么就是我的谷歌搜索技能因为某种原因而下降了。 谢谢大家！    提交人    /u/bgighjigftuik   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/</guid>
      <pubDate>Thu, 28 Nov 2024 13:27:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 尽管 Stella 嵌入在 MTEB 排行榜上名列前茅，为什么却没有得到更广泛的应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/</link>
      <description><![CDATA[https://huggingface.co/spaces/mteb/leaderboard 我一直在研究嵌入模型，并注意到一些有趣的事情：Stella 嵌入在 MTEB 排行榜上遥遥领先，表现优于 OpenAI 的模型，同时规模更小（1.5B/400M 参数）且使用 apache 2.0。托管它们相对便宜。 作为参考，Stella-400M 在 MTEB 上的得分为 70.11，而 OpenAI 的 text-embedding-3-large 为 64.59。1.5B 版本的得分甚至更高，为 71.19 然而，我很少看到它们在生产用例或讨论中被提及。这里有人在生产中使用过 Stella 嵌入吗？与 OpenAI 的产品相比，您在性能、推理速度和可靠性方面的体验如何？ 只是想了解为什么尽管基准测试令人印象深刻，但它们没有得到更广泛的采用，这是否是我遗漏了什么。 很想听听您的想法和经验！    提交人    /u/sdsd19   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/</guid>
      <pubDate>Thu, 28 Nov 2024 11:45:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 24 Nov 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>