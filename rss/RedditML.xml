<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 07 Oct 2024 21:16:28 GMT</lastBuildDate>
    <item>
      <title>[D] 嵌入作为数据结构 2.0？学习特定任务的最佳数据表示（幻灯片）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fybtdh/d_embeddings_as_data_structures_20_learning/</link>
      <description><![CDATA[我最近就嵌入作为数据结构 2.0 发表了演讲，并认为这可能会引起人们的兴趣。幻灯片 -&gt; https://docs.google.com/presentation/d/1GAiYOYTfzx-fyaHRNXYHCkA-y2wx1hnQwNiue0vj1tE/edit?usp=sharing 2017 年，Andrej 创造了“软件 2.0”一词 - 即从数据中学习而来的软件，而不是通过编程规则手动制作的软件。这种范式转变使得能够开发出比以前更强大的软件。  数据结构 2.0 与嵌入有很多相似之处，使用嵌入表示数据代表了类似的转变。 “数据结构 2.0”是数据嵌入的学习表示。您无需手动制定存储数据的规则，而是可以通过嵌入学习表示数据的最佳任务特定方法。 “数据结构 2.0 是用对人类不友好的语言编写的，例如嵌入的浮点值。没有人参与编写此代码……直接用浮点值编码有点乏味，但可行（我试过了）。&quot; 让我知道你的想法！    提交人    /u/Jesse_marqo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fybtdh/d_embeddings_as_data_structures_20_learning/</guid>
      <pubDate>Mon, 07 Oct 2024 16:25:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] Model2Vec：从任意句子转换器中提取一个小型快速模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fyb9jj/p_model2vec_distill_a_small_fast_model_from_any/</link>
      <description><![CDATA[嗨 👋！ 我想分享我们过去几个月一直在研究的一个项目，名为 Model2Vec，我们最近开源了这个项目。这是一种提炼 Sentence Transformer 模型并创建非常小的静态嵌入模型（磁盘上 30mb）的技术，这些模型的速度比原始模型快 500 倍，使其在 CPU 上非常容易使用。提炼在 CPU 上大约需要 30 秒。 这些嵌入在 MTEB 上的表现远胜于类似方法（如 GloVE 和 BPEmb），同时创建速度更快，并且不需要数据集。它被设计为（大型）语言模型的环保替代方案，特别适用于时间受限（例如搜索引擎）或无法使用高级硬件的情况。 这个想法非常简单，但效果却出奇地好： 1：获取任何句子转换器的标记输出嵌入。 2：使用 PCA 降低维数。这不仅减小了模型大小，而且还规范了输出空间。 3：根据单词/标记频率对嵌入应用 zipf 权重。这实质上降低了常用词的权重，这意味着您不需要删除停用词。 我们创建了几个易于使用的方法，可以在使用 pip install model2vec 安装包后使用： 推理： from model2vec import StaticModel # 从 HuggingFace 中心加载模型（在本例中为 M2V_base_output 模型）model_name = &quot;minishlab/M2V_base_output&quot; model = StaticModel.from_pretrained(model_name) # 制作嵌入 embeddings = model.encode([&quot;独自一人去很危险！&quot;, &quot;这对每个人来说都是个秘密。&quot;])  提炼： from model2vec.distill import extract # 选择一个句子转换器模型 model_name = &quot;BAAI/bge-base-en-v1.5&quot; # 提炼模型 m2v_model = deliver(model_name=model_name, pca_dims=256) # 保存模型 m2v_model.save_pretrained(&quot;m2v_model&quot;)  我很想听听您对此的想法，并很乐意回答任何问题！ 链接：  Repo 链接 结果链接     提交人    /u/Pringled101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fyb9jj/p_model2vec_distill_a_small_fast_model_from_any/</guid>
      <pubDate>Mon, 07 Oct 2024 16:02:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 法学硕士 (LLM) 混合专家 (MoE) 的视觉指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fya2ks/p_a_visual_guide_to_mixture_of_experts_moe_in_llms/</link>
      <description><![CDATA[大家好！我很高兴向大家介绍法学硕士 (LLM) 中的混合专家 (MoE) 的高度说明性指南！ 从探索专家的作用、他们的路由机制、稀疏 MoE 层和负载平衡技巧（例如 KeepTopK、辅助损失和专家容量），到视觉模型中的 MoE 和计算要求。  https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts 我喜欢创建视觉效果，在创建了 55 多个自定义视觉效果后，我不得不停下来！ 本指南的视觉特性允许专注于直觉，希望使所有这些技术易于广大受众使用，无论您是 Mixture of Experts 的新手还是更有经验。    提交人    /u/MaartenGr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fya2ks/p_a_visual_guide_to_mixture_of_experts_moe_in_llms/</guid>
      <pubDate>Mon, 07 Oct 2024 15:13:45 GMT</pubDate>
    </item>
    <item>
      <title>[D]keras 是否停止与 Google 合作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fy9c9s/ddid_keras_stop_working_on_google_collab/</link>
      <description><![CDATA[Model.fit 拒绝执行除 prind epoch 1/150 之外的任何事情，两台计算机，不同类型的模型和不同的帐户，没有错误，中断不起作用，在过去两天里尝试了我能想到的一切，有人知道发生了什么吗？    提交人    /u/ihaveagoodusername2   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fy9c9s/ddid_keras_stop_working_on_google_collab/</guid>
      <pubDate>Mon, 07 Oct 2024 14:43:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 根据任务复杂性灵活部署计算</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fy54kn/d_flexible_compute_deployment_based_on_task/</link>
      <description><![CDATA[大家好，ML 的朋友们。我是一名认知科学专业的学生，​​研究神经科学和机器学习的交叉领域。大脑最酷的事情之一可能就是它能够完成的任务是多么的高效——依靠灯泡运行。据我所知，这可能是因为如果任务不需要所有参数，则不会使用它们。到目前为止，我发现的唯一与此类似的 ML 方法是专家混合法。除此之外，在深度学习中，优化推理过程似乎被忽视了——通常只是使用所有参数，而不管自上而下的上下文或输入统计数据。 我几乎可以肯定我错了，所以我希望你们能给我指出一些深入研究这个问题的好论文？或者也许是问题的正式名称？例如，你可以攻读法学硕士学位。如果对句子中下一个单词的预测很简单（例如食草动物吃 [植物]），我可能不需要所有参数就能得到完美的预测（Claude 和 LLama 可能会做得同样好，但 Claude 解决这个问题的成本更高），而不是更技术性、需要更多注意力和背景投入处理的预测（例如，解决数学证明）。    提交人    /u/Karioth1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fy54kn/d_flexible_compute_deployment_based_on_task/</guid>
      <pubDate>Mon, 07 Oct 2024 11:16:17 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 用于从成本估算文件中提取关键信息的 NER</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fy3o00/project_ner_for_extracting_key_information_from/</link>
      <description><![CDATA[我需要开展一个命名实体识别项目。我有一个 CSV 文件，其中包含来自 270 个文档的文本以及成本估算。我的任务是提取以下信息： a) 文档收件人 b) 产品数量 c) 产品价格 d) 产品名称 e) 文档 ID 代码 文档通常遵循一致的结构，具有清晰的模式。例如，文档收件人始终出现在相同的字母之后。产品名称始终位于数量和价格之间，因此识别这两个元素将使我能够提取介于两者之间的任何内容。我需要提取的其他关键部分也是如此。您对如何以简单而准确的方式处理此问题有什么建议吗？谢谢！    提交人    /u/No_Possibility_7588   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fy3o00/project_ner_for_extracting_key_information_from/</guid>
      <pubDate>Mon, 07 Oct 2024 09:35:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求建议：手写 OCR 的 LLM 与 Google Vision 的 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fy1iqx/d_looking_for_advice_llms_for_handwriting_ocr_vs/</link>
      <description><![CDATA[大家好！ 我正在做一个项目，需要从手写图像中提取文本。到目前为止，我一直在使用 Google Vision API，它对包括手写在内的一些文本效果很好，但我想知道是否有更直接的解决方案专门处理手写。 使用可以直接处理和读取手写内容的 LLM 是否有意义，还是坚持使用传统的 OCR 方法（如 Google Vision）仍然是可行的方法？我知道 GPT-4o/Gemini 等 LLM 具有这些功能，但我不确定它们处理基于图像的输入或手写的效果如何。 有人尝试过使用 LLM 进行 OCR 吗？您会推荐什么，是否有特定的模型可以擅长这项任务？ 我的想法是还使用 LLM 来总结手写文本，因此在流程的某个阶段，无论如何我都需要 LLM。 谢谢。    提交人    /u/Practical_Estate4971   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fy1iqx/d_looking_for_advice_llms_for_handwriting_ocr_vs/</guid>
      <pubDate>Mon, 07 Oct 2024 06:49:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 语言建模任务上的 Mamba 和 SSM 是一个很好的研究轨迹吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxzor7/r_is_mamba_and_ssms_on_language_modelling_task_a/</link>
      <description><![CDATA[我刚刚接触到 Mamba 和 SSM，因为我的教授说我应该尝试探索它。我是一名硕士生，刚刚开始我的研究之旅，我原本想像我系里的其他学生一样研究 transformers LM。有人说这会让我陷入别人以前没有做过的事情，会使我的学习/研究变得比预想的更难（最终可能会得到平庸的结果）。你们对此有什么看法吗？谢谢。    提交人    /u/worthlesspineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxzor7/r_is_mamba_and_ssms_on_language_modelling_task_a/</guid>
      <pubDate>Mon, 07 Oct 2024 04:41:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于工具使用和 LLM 代理有哪些有趣的论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxvsp7/d_what_are_some_interesting_papers_about_tooluse/</link>
      <description><![CDATA[目前，我正在研究 voyager (https://arxiv.org/abs/2305.16291)，但希望得到更多建议。TIA。    提交人    /u/a1_jakesauce_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxvsp7/d_what_are_some_interesting_papers_about_tooluse/</guid>
      <pubDate>Mon, 07 Oct 2024 01:04:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] ML 论文的敏感性分析获得了更好的结果，现在怎么办？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxu3zw/d_sensitivity_analysis_of_the_ml_paper_got_better/</link>
      <description><![CDATA[我使用一种新方法针对特定数据集撰写了一篇 ML 论文，取得了一些积极成果。我训练了几个模型，对它们进行了评估，并根据研究结果进行了广泛的解释和讨论。一位审稿人要求对一些预处理参数/算法进行敏感性分析。有趣的是，其中一项更改导致的结果比我原来的方法略好。 我的问题是：在这种情况下的期望是什么？我需要重写整篇论文，还是应该仅在敏感性分析中报告这一观察结果？虽然更改改善了结果，但想到要根据新的运行重写大部分解释（例如，特征重要性、图表、讨论等），还是很令人沮丧。你的想法和经验是什么？    提交人    /u/anagreement   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxu3zw/d_sensitivity_analysis_of_the_ml_paper_got_better/</guid>
      <pubDate>Sun, 06 Oct 2024 23:37:02 GMT</pubDate>
    </item>
    <item>
      <title>上下文感知词语替换 [P] [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxsuhg/context_aware_word_replacement_p_r/</link>
      <description><![CDATA[你好！ 我从事 CV 研究，所以对 NLP 不是很精通，所以需要输入。 我正在研究在保留图片上下文的情况下替换“句子”中的“单词”，以便我们更容易在数据集中搜索该单词的合适图像。例如： 句子 - “学生应该抵制网络欺凌，以免攻击者伤害他们” 单词 - “攻击者” 为什么预期 - 网络犯罪分子、网络欺凌者等，以便我可以搜索相关图像。 BeRT 和其他模型用什么来替换它 - 恐怖分子、计算机、敌对攻击者等。 我想在本地运行一些东西，但找不到任何解决方案。有什么想法或输入我应该尝试吗？有任何资源或代码笔记本吗？    提交人    /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxsuhg/context_aware_word_replacement_p_r/</guid>
      <pubDate>Sun, 06 Oct 2024 22:34:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</guid>
      <pubDate>Sun, 06 Oct 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 LLM 培训中，增加批次大小和使用注意力掩蔽的打包序列有什么区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxguh6/d_whats_the_difference_between_increasing_batch/</link>
      <description><![CDATA[我很好奇在固定长度序列上训练大型语言模型 (LLM) 时，以下两种方法之间的区别： 使用批量大小 = 4，其中每个样本的序列长度为 1024 个标记，并且它们被独立处理。将 4 个序列打包成一个批次，最大序列长度为 4096，并应用注意掩码以确保没有序列关注来自另一个序列的标记。 如果正确应用了注意掩码，确保不会关注其他序列，那么这两种方法在以下方面是否存在显着差异： 内存使用情况 计算成本 训练动态  据我所知，如果没有注意掩码，由于自注意力机制，打包会导致计算成本呈二次方增加。但是使用掩码后，计算和内存使用量不是与将它们作为批处理中的单独序列处理几乎相同吗？还是我遗漏了其他因素？    提交人    /u/JeanMichelRanu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxguh6/d_whats_the_difference_between_increasing_batch/</guid>
      <pubDate>Sun, 06 Oct 2024 13:46:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] MaskBit：通过 Bit Tokens 生成无嵌入图像</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxfy91/r_maskbit_embeddingfree_image_generation_via_bit/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxfy91/r_maskbit_embeddingfree_image_generation_via_bit/</guid>
      <pubDate>Sun, 06 Oct 2024 13:00:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>