<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sun, 02 Jun 2024 21:13:36 GMT</lastBuildDate>
    <item>
      <title>[D] 用于科学文献搜索的预训练嵌入模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6lbp2/d_pretrained_embedding_model_for_search_in/</link>
      <description><![CDATA[你好！ 我正在开发一款应用，需要一种方法来在一组科学论文中搜索关键词（即查找涉及查询的文档）。我需要处理同义词、错误等。 我认为最好的想法是向量搜索。但我真的不知道目前的 SOTA 是什么。我知道 SBERT，但我不确定它是否是最好的？ 此外，如果模型可以在多语言和科学文献上进行预训练，那就最好了…… 你有什么想法给我吗？ 也许我走的路不对？ 提前谢谢您！    提交人    /u/ez613   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6lbp2/d_pretrained_embedding_model_for_search_in/</guid>
      <pubDate>Sun, 02 Jun 2024 19:33:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] Moonlighter 商店模拟中的贝叶斯老虎机商品定价</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/</link>
      <description><![CDATA[      我建了一个玩具店，模仿Moonlighter 游戏和贝叶斯匪徒代理通过 Thompson 抽样选择和定价待售物品。  随着模拟的进行，客户对这些物品在其货架价格的反应（即“生气”、“悲伤”、“满足”、“欣喜若狂”）更新了理想（即最高）价格概率分布（即后验）。  该算法探索了物品的理想价格，并迅速找到了当时理想价格最高的物品组，然后将其出售。 这个过程一直持续到所有物品售出。  该图表示待售物品之间的竞争。  这些点是从每个竞赛（x 值）中每个物品（颜色）的理想价格分布（即后验）中抽样的价格（y 值）。  在每次 Thompson 抽样竞赛中，抽样价格最高的获胜者最终被摆上货架。  我提到的客户对货架上商品的反应更新了这些分布的界限，用相同颜色的线条表示。 有关更多信息、更多图表以及包含工作代码和带有 Pandas/Matplotlib 代码的 Jupyter 笔记本（用于生成图表）的相应 Github 存储库的链接，请参阅我的文章：https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rMachineLearning    提交人    /u/JaggedParadigm   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/</guid>
      <pubDate>Sun, 02 Jun 2024 17:25:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 记录每一项机器学习资源或接受知识随时间流失的困境</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/</link>
      <description><![CDATA[我知道这可能是一个奇怪的话题，但我仍然认为这是一个重要的讨论，因为我们在这个领域不断学习。 机器学习是一个广阔的领域，与许多其他学科紧密交织在一起。仅我的硕士学位就涵盖了统计、优化、逆数据模拟、MLOps、软件工程、基于代理的建模、语义网、深度学习、时间序列等主题……这些领域中的每一个都有自己的子领域，人们可以投入一生去探索。 我意识到，除非你每天练习一个主题，否则你从书籍、认证、文章、论文、播客和视频中获得的知识最终会消失。四年前，这种认识促使我发现了 Obsidian，它极大地改变了我获取和保留信息的方式。我现在会记录我所获取的所有内容，尤其是工作之外我感兴趣的主题。就像一个“第二大脑”。如果没有这种做法，我发现信息很快就会消失。 事实上，我花了无数的时间研究物理、历史、认识论、哲学和许多其他学科的内容。然而，我曾经知道的东西只有一小部分留了下来。这让我陷入了两难境地：我应该投入大量时间来捕捉知识系统中的每一项资源，以确保我可以随着时间的推移而保留下来，还是尽快消耗资源，因为它们会消失（）“为了好玩”或当我的时间有限时）？ 我不想让这篇文章太长，但我确实感觉到花时间处理信息的好处，比如在读书的时候。大规模地组织和连接知识通常很有挑战性，但也很有回报，因为它有助于建立对某个主题的深刻理解。此外，当您需要刷新记忆时，如果您已经完成了这项“预处理”工作，而不是再次浏览互联网/书籍，那么“成本”会低得多。我不是简单地复制/粘贴文本，而是根据我已经了解的主题来定制我所捕获的内容。 但是，这个领域有太多东西需要学习，即使是数学或统计学等基础知识。我有时会质疑这种方法是否可持续。例如，Sebastian Raschka 等人撰写的《使用 PyTorch 和 Scikit-Learn 进行机器学习》一书长达 700 页。想象一下从这样一本全面的书中捕捉每一条信息需要花费的时间（而且这只是其中之一！）。记笔记还会迫使你彻底理解材料，包括每个方程式，否则笔记就毫无用处了。 我不主张二元方法；我经常找到妥协。但我很好奇你学习和消费信息的方法。你如何平衡保留知识的需求与时间和精力的实际限制？    提交人    /u/CrimsonPilgrim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/</guid>
      <pubDate>Sun, 02 Jun 2024 16:36:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 时间融合变压器：有关输入数据的问题。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6gp42/d_temporal_fusion_transformers_question_about_the/</link>
      <description><![CDATA[我正在做能源消耗的时间序列预测。在我之前的工作中，我使用了 LSTM 和其他类似的模型。首先使用滚动窗口捕获先前的时间步骤对输入数据进行预处理，然后将窗口聚合以形成用于模型的数据集。 在 TFT 中，我发现它不起作用。我无法理解其中的区别。对于我看到的 TFT 示例，数据集具有包含不同信息的重复时间步骤。TFT 是否适合训练具有连续唯一时间步骤的长时间序列数据？    提交人    /u/uwk33800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6gp42/d_temporal_fusion_transformers_question_about_the/</guid>
      <pubDate>Sun, 02 Jun 2024 16:08:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] MetaEarth - 用于全球尺度遥感图像生成的生成基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6ggwv/r_metaearth_a_generative_foundation_model_for/</link>
      <description><![CDATA[        由    /u/jiupinjia 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6ggwv/r_metaearth_a_generative_foundation_model_for/</guid>
      <pubDate>Sun, 02 Jun 2024 15:58:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    <item>
      <title>如果 LLM 是基于 token 的自回归模型，那么它们如何生成图像？（Transformers + VQVAE）[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/</link>
      <description><![CDATA[      分享我 YT 频道的一段视频，讨论某些多模态 LLM（如 Gemini）如何将图像生成为一系列可学习的图像标记。    提交人    /u/AvvYaa   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6emwi/if_llms_are_tokenbased_autoregressive_models_how/</guid>
      <pubDate>Sun, 02 Jun 2024 14:33:16 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 人们是否有兴趣使用两个通过 NVLINK 连接的 RTX A6000 来创建中端 GPU 装备？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6ct6w/discussion_are_people_interested_in_creating_a/</link>
      <description><![CDATA[https://store.nvidia.com/en-us/nvidia-rtx/products/nvidia-rtx-a6000/ 这将提供 96 GB 的内存大小 由于成本原因，我想利用 NVIDIA 的高等教育和研究资助计划 https://developer.nvidia.com/higher-education-and-research    提交人    /u/Flintstone9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6ct6w/discussion_are_people_interested_in_creating_a/</guid>
      <pubDate>Sun, 02 Jun 2024 13:01:27 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Tangles：Diestel 在书中宣布了一种新的数学 ML 工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</link>
      <description><![CDATA[      大家好，我想分享一本社区可能会感兴趣的新书！ 图论学家 Diestel 写了一本面向 ML 社区（及其他人）的书：  缠结：经验科学中人工智能的结构化方法 Reinhard Diestel，剑桥大学出版社 2024  ----- 出版商简介： 缠结提供了一种在不精确数据中识别结构的精确方法。通过将经常一起出现的特质分组，它们不仅可以揭示事物的集群，还可以揭示其特质的类型：政治观点、文本、健康状况或蛋白质的类型。缠结为人工智能提供了一种新的结构化方法，可以帮助我们理解、分类和预测复杂现象。 这已成为可能，这是由于缠结的数学理论最近被公理化，这使得缠结的应用范围远远超出了图论的起源：从数据科学和机器学习中的聚类到预测经济学中的客户行为；从 DNA 测序和药物开发到文本和图像分析。 这是首次探索此类应用。假设只具备本科数学基础知识，那么缠结理论及其潜在含义将对科学家、计算机科学家和社会科学家开放。 ----- 电子书以及包括教程在内的开源软件可在 tangles-book.com 上找到。 注意：这是一本“外展”书，主要不是关于缠结理论，而是关于以多种意想不到的方式和领域应用缠结。图中的缠结在 Diestel 的《图论》第 5 版中介绍。 目录和数据科学家简介（Ch.1.2）可从 tangles-book.com/book/details/ 和 arXiv:2006.01830 获得。第 6 章和第 14 章介绍了一种基于缠结的新软聚类方法，与传统方法截然不同。第 7-9 章涵盖了第 14 章所需的理论。 tangles-book.com 的软件部分表示，他们邀请在具体项目上进行合作，以及为他们的 GitHub 软件库做出贡献。  https://preview.redd.it/ysj91dw2o54d1.png?width=2074&amp;format=png&amp;auto=webp&amp;s=dd7ea6c2671ef83a5be77739e9ed6e3d6169c1d2 ​ ​    提交人    /u/Prestigious_Ship_238   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</guid>
      <pubDate>Sun, 02 Jun 2024 12:56:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] EMNLP 匿名政策</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6c5ox/d_emnlp_anonymity_policy/</link>
      <description><![CDATA[今年 1 月，ACL 根据工作组报告的建议更新了其匿名政策，该报告指出：  我们强调，提交和审查应保持双盲，提交的论文应完全匿名。认识到其中涉及许多权衡，在进行社区范围的调查并考虑了多种选择（列在本文档末尾附近的“考虑的提案”部分中）后，我们建议如下：(a) 更改 ACL 政策，现在允许随时进行匿名和非匿名预印，以便可以不受限制地进行有关工作的技术对话。 (b) 明确表示允许在所有媒体（包括社交媒体）上讨论未发表的作品，但不鼓励（但不禁止）进行广泛的宣传和公关。  这可能是一个显而易见的问题，但我以前从未提交过 ACL 会议，也不想搞砸它：这是否意味着我们可以在提交之前、期间或之后的任何时间将非匿名预印本发布到 Arxiv，而没有被拒绝的风险？    提交人    /u/monkeyofscience   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6c5ox/d_emnlp_anonymity_policy/</guid>
      <pubDate>Sun, 02 Jun 2024 12:25:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] PU 学习综述</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6b1z4/d_a_survey_of_pu_learning/</link>
      <description><![CDATA[      主要 html 版本 sample1 paper1 paper2    提交人    /u/Acceptable-Worry-493   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6b1z4/d_a_survey_of_pu_learning/</guid>
      <pubDate>Sun, 02 Jun 2024 11:17:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] FineWeb 技术报告：大规模挖掘网络上最精细的文本数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</link>
      <description><![CDATA[FineWeb 15 万亿公开发布的网络规模数据集背后的团队刚刚发表了一篇关于创建高质量网络规模数据集的科学的详尽博客文章，详细介绍了 FineWeb 的步骤和学习成果，以一种 distill.pub 交互式文章/博客的方式。 他们还发布了 FineWeb-Edu，这是 Common Crawl 的一个过滤子集，拥有 1.3T 令牌，专注于教育内容非常丰富的网页，在知识和推理密集型基准测试（如 MMLU、ARC 和 OpenBookQA）上，其表现似乎优于所有公开发布的网络规模数据集 有趣的阅读：https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1     由    /u/Thomjazz 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d68jjf/r_tech_report_on_fineweb_decanting_the_web_for/</guid>
      <pubDate>Sun, 02 Jun 2024 08:17:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您在现实世界中使用 LLM 的案例有哪些？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/</link>
      <description><![CDATA[我认为我们应该分享更多 LLM 的生产用例，而不仅仅是理论上的最佳实践。 您可以分享您在生产中看到/构建的用例吗？它应包括以下详细信息：  它解决的问题 实现细节（模型、基础设施等） 它产生的业务影响     提交人    /u/madredditscientist   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d65vj7/d_what_are_your_realworld_production_use_cases/</guid>
      <pubDate>Sun, 02 Jun 2024 05:12:14 GMT</pubDate>
    </item>
    <item>
      <title>为开源模型实现“扩展单义性：从 Claude 3 Sonnet 中提取可解释的特征”论文。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/</link>
      <description><![CDATA[我最近偶然发现了一篇有趣的论文，题为“扩展单义性：从 Claude 3 Sonnet 中提取可解释特征”，该论文探讨了如何使用稀疏自动编码器从大型语言模型的激活中提取可解释特征。该方法似乎有望深入了解模型的内部表示和行为。 这让我开始思考为开源语言模型实施类似的可解释性技术的可行性。我们能否在不进行大量微调的情况下控制 LLM 及其行为。 我想联系这个社区讨论一些事情：  是否有人已经在开源语言模型上实施或试验过类似的可解释性技术？我们可以制作类似于金门克劳德的东西吗？ 您认为调整和扩展这些技术以与 Llama、phi、mistral 等一起使用是否可行？与 sonnet 相比，它们的参数大小要小得多。 我有兴趣与其他对这个研究领域充满热情的人合作。如果您正在研究开源模型的可解释性或对新方法有想法，我很高兴与您合作并进一步探索。我们可以合作实施技术、共享资源或集思广益新想法。  如果您有兴趣合作或有任何想法要分享，请随时分享。    提交人    /u/No-Point1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d64lx8/implementing_scaling_monosemanticity_extracting/</guid>
      <pubDate>Sun, 02 Jun 2024 03:50:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] CoPE：上下文位置编码：学习计算重要的事情</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/</link>
      <description><![CDATA[  由    /u/fasttosmile  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5u95z/r_cope_contextual_position_encoding_learning_to/</guid>
      <pubDate>Sat, 01 Jun 2024 19:05:38 GMT</pubDate>
    </item>
    </channel>
</rss>