<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Thu, 13 Jun 2024 18:19:43 GMT</lastBuildDate>
    <item>
      <title>[D] 如何从现有的 LLM 模型创建一个根据特定信息回答问题的 LLM 模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df494a/d_how_can_i_create_an_llm_model_from_existing_llm/</link>
      <description><![CDATA[假设我有一份想要训练 LLM 的文档，我希望模型仅回答该文档中的问题，我该如何进行？    提交人    /u/inobody_somebody   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df494a/d_how_can_i_create_an_llm_model_from_existing_llm/</guid>
      <pubDate>Thu, 13 Jun 2024 17:14:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 处理大规模特征。例如从 -1e2 到 1e4</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df4478/d_dealing_with_features_having_large_scale_eg/</link>
      <description><![CDATA[如果特征的范围很大，你会如何处理？一种选择是对数尺度，但如果特征是负值怎么办？ 你们是如何处理深度学习模型中的此类数据的？ 编辑：为了清楚起见，这些特征的范围很广。例如 -1e2 到 +1e4。 此外，如果输入特征和学习特征中存在如此宽的范围，你会如何处理？ 您是否成功地对案例使用了最小最大或标准化？    提交人    /u/rmm_philosopher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df4478/d_dealing_with_features_having_large_scale_eg/</guid>
      <pubDate>Thu, 13 Jun 2024 17:08:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] CNN 的图像和非图像数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df4175/d_image_and_nonimage_data_for_cnns/</link>
      <description><![CDATA[我有一个数据集，其中对于每个样本，我都有一些图像和两个整数值作为输入。您将如何处理这样的输入？ 我正在尝试使用线性层将整数输入投影到图像维度，然后将其添加为另一个图像通道。 你们将如何处理这种输入？    提交人    /u/rmm_philosopher   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df4175/d_image_and_nonimage_data_for_cnns/</guid>
      <pubDate>Thu, 13 Jun 2024 17:05:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 wandb.ai 的经验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df3y27/d_experiences_with_wandbai/</link>
      <description><![CDATA[我个人在 ML 研究中经常使用 wandb.ai (权重和偏差)，发现它很棒，但缺少一些功能。 因此，我目前正在构建一个像 wandb 这样的 MLOps 平台，但更适合团队合作。 我对社区的问题是：  你不喜欢 wandb 的哪些方面？ 你希望它有什么功能吗？  我非常感谢任何见解！我想构建一个真正有益且比 wandb 更有价值的东西。请告诉我任何事情，随时给我发私信！ 我的个人观点：wandb 很棒，但很难作为一个团队一起工作，尤其是在远程工作时。我们发现自己经常使用 slack + notion 来跟踪事情，但情况已经失控了。 此外，没有办法“采取行动”。即您看到一些结果，并决定要为其制作 jira 票证，则必须手动完成。 对其他人来说，这还不是一个足够大的问题吗？只是好奇    提交人    /u/Sriyakee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df3y27/d_experiences_with_wandbai/</guid>
      <pubDate>Thu, 13 Jun 2024 17:01:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基于低延迟 CPU 的教育价值分类器，用于 LLM 预训练数据过滤</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df258y/r_low_latency_cpu_based_educational_value/</link>
      <description><![CDATA[Fineweb-edu 可比分类器速度极快 - CPU 每秒 2000 文档，以廉价且可扩展的方式过滤 LLM 预训练数据集  https://huggingface.co/blog/kenhktsui/edu-value-classifier-cpu    提交人    /u/transformer_ML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df258y/r_low_latency_cpu_based_educational_value/</guid>
      <pubDate>Thu, 13 Jun 2024 15:45:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为所有想要测试生成式 AI 的人提供 GPU</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df0nti/d_gpu_for_everyone_who_wants_to_test_generative_ai/</link>
      <description><![CDATA[您好，我是 Julien GAUTHIER，www.arkanecloud.com 的创建者 – 用户可以在这个网站上为他们想要在云端开发的任何 AI 应用程序部署 GPU。 您只需选择您的 GPU，就可以在其上部署生成式 AI。 我们用 Llama3、Stable 扩散（SD-3 即将推出！）和许多其他模板构建了一些模板。 请随时给我们您的反馈😊 谢谢！    提交人    /u/ArkaneCloud   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df0nti/d_gpu_for_everyone_who_wants_to_test_generative_ai/</guid>
      <pubDate>Thu, 13 Jun 2024 14:42:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 处理不平衡数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1df0cg7/d_dealing_with_imbalanced_dataset/</link>
      <description><![CDATA[您好。我正在尝试解决的问题之一是处理高度不平衡的数据集。分类任务中有近 200 个类别，而我的数据中有超过 30% 的数据属于单个类别。在使用标准交叉熵损失使用此数据训练模型时，模型开始再次仅预测单个类别。我使用了简单的正则化方法来缓解这种情况，并在验证数据集上实现了 91% 的准确率，每个类别的样本分布几乎相等。那么，处理这个问题的更有效方法是什么？    提交人    /u/SmartEvening   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1df0cg7/d_dealing_with_imbalanced_dataset/</guid>
      <pubDate>Thu, 13 Jun 2024 14:28:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用自然语言和概率推理进行实验并修改规则</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dezijo/r_doing_experiments_and_revising_rules_with/</link>
      <description><![CDATA[  由    /u/floppy_llama  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dezijo/r_doing_experiments_and_revising_rules_with/</guid>
      <pubDate>Thu, 13 Jun 2024 13:52:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] OpenMetricLearning 3.0 统一支持图片和文字！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/</link>
      <description><![CDATA[      大家好！ 我想分享 OpenMetricLearning 3.0 的发布！  OML — 是一个用于表示学习和检索的库，其中包含大量模型、损失、矿工、采样器、指标和其他有用的东西，如 DDP、与 PyTorchLightning 和 PyTorch Metric Learning 的集成、不同的实验跟踪器等。   有什么新内容？ * 我们已经添加了文本支持，现在我们正在添加音频！（用户不仅已经将 OML 用于图像，而且现在我们还提供开箱即用的支持、测试和示例。） * 代码统一适用于图像、文本，并且适用于声音！我邀请您查看图像和文本的并排比较。 * 检索部分已分离，可用于模型验证和推理，并进行以下重新排名或其他后期处理。 * 库的功能已在一个地方描述，以便于导航，并且我们总体上改进了文档和示例。 * 一些计算，特别是与内存相关的计算，已经进行了优化。 我们欢迎潜在的贡献者： * 代码变得更加模块化，因此入门门槛降低了 - 您可以获取单独的代码并继续进行它。 * 我们还用我们的问题/任务更新了board。 您在GitHub上的⭐️极大地帮助了我们进一步发展！ OML    提交人    /u/Zestyclose-Check-751   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deujz2/p_openmetriclearning_30_which_uniformly_supports/</guid>
      <pubDate>Thu, 13 Jun 2024 09:06:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何为机器学习任务准备 TB 级数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1desvs5/d_how_to_prepare_tbs_of_data_for_ml_tasks/</link>
      <description><![CDATA[我目前必须预处理（主要是清理）几 TB 的图像，之后这些图像可用于机器学习。这一挑战似乎与拥有大型数据集的公司必须面临的挑战非常相似，例如 OpenAl、Tesla 等。 当处理代码使用 Python 时，您知道如何很好地分布这一过程吗？有没有流行的框架可以实现这一目标？    提交人    /u/That_Phone6702   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1desvs5/d_how_to_prepare_tbs_of_data_for_ml_tasks/</guid>
      <pubDate>Thu, 13 Jun 2024 07:06:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] 微软开源 Recall AI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/</link>
      <description><![CDATA[我创建了一个开源的 Microsoft Recall AI 替代品。 这会记录您屏幕上的所有内容，并可以使用自然语言搜索。但与 Microsoft 的实现不同，这不是隐私噩梦，现在就可以使用。并带有实时加密 这是一个新的启动项目，需要贡献，因此请希望转到 github repo 并给它一个星星 https://github.com/VedankPurohit/LiveRecall 它是完全本地的，您可以查看代码。而且所有内容始终是加密的，这与 Microsoft 的含义不同，当您登录时，图像会被解密并可能被盗    提交人    /u/Vedank_purohit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dergc6/p_opensource_microsoft_recall_ai/</guid>
      <pubDate>Thu, 13 Jun 2024 05:30:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 能否发明更好的方法来培养 LLM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/</link>
      <description><![CDATA[新博客文章和论文： https://sakana.ai/llm-squared/ https://arxiv.org/abs/2406.08414 发现用于大型语言模型的偏好优化算法 摘要 离线偏好优化是增强和控制大型语言模型 (LLM) 输出质量的关键方法。通常，偏好优化被视为使用手工制作的凸损失函数的离线监督学习任务。虽然这些方法基于理论见解，但它们本质上受到人类创造力的限制，因此可能的损失函数的大量搜索空间仍未得到探索。我们通过执行 LLM 驱动的目标发现来解决这个问题，以自动发现新的最先进的偏好优化算法，而无需（专家）人工干预。具体而言，我们迭代地提示 LLM 根据先前评估的性能指标提出和实施新的偏好优化损失函数。此过程导致发现以前未知且性能良好的偏好优化算法。其中表现最好的我们称之为发现偏好优化 (DiscoPOP)，这是一种自适应地混合逻辑和指数损失的新算法。实验证明了 DiscoPOP 的最先进的性能及其成功转移到保留任务。    提交人    /u/hardmaru   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deo4pd/r_can_llms_invent_better_ways_to_train_llms/</guid>
      <pubDate>Thu, 13 Jun 2024 02:18:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] Grokking 的问题解决了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1defvmv/d_is_grokking_solved/</link>
      <description><![CDATA[最近的 Grokfast 论文 发现了一种将算法数据集的 Grokking 速度提高 50 倍的方法。早先的 Omnigrok 论文 确定，对于他们的算法数据集，“在恒定权重范数下的约束优化在很大程度上消除了 Grokking” 这些改进是否意味着现在我们在训练模型时不必担心延迟泛化/grokking（尽管其机制模糊）？    提交人    /u/delorean-88   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1defvmv/d_is_grokking_solved/</guid>
      <pubDate>Wed, 12 Jun 2024 19:55:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我厌倦了 LangChain，所以我制作了一个简单的开源替代方案，支持工具使用和视觉，以尽可能轻松地构建 Python AI 应用程序。（simpleaichat + vision + anthropic 和 gemini）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/</link>
      <description><![CDATA[https://github.com/piEsposito/tiny-ai-client 构建 tiny-ai-client 的动机来自于对 Langchain 的失望，它变得臃肿、难以使用且文档不全 - 并从 simpleaichat 中汲取灵感，但除了 OpenAI（Gemini、Anthropic - Groq 和 Mistral 正在研发中）之外，还增加了对视觉、工具和更多 LLM 提供商的支持。 我构建它是为了延续 simpleaichat 的初衷，不是为了炒作、筹集资金或其他什么，而是为了帮助人们做两件事：尽可能轻松地构建 AI 应用程序，并在无需使用 Langchain 的情况下切换 LLM。 这是一个极简可行的软件包版本，支持视觉、工具和异步调用。还有很多改进要做，但即使在目前的状态下，tiny-ai-client 也普遍改善了我与 LLM 的交互，并已成功用于生产。 让我知道你的想法：仍有一些错误可能需要修复，但所有示例都可以正常工作并且易于适应你的用例。    提交人    /u/lee_from_teashop   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1deffo8/p_im_tired_of_langchain_so_i_made_a_simple/</guid>
      <pubDate>Wed, 12 Jun 2024 19:36:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>