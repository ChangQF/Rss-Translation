<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sun, 06 Oct 2024 21:14:05 GMT</lastBuildDate>
    <item>
      <title>[讨论] 为什么正弦 PE 不适用于较长的序列？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxmn0z/discussion_why_dont_sinusoidal_pe_work_for_longer/</link>
      <description><![CDATA[从理论上讲，它们会生成独特的位置向量，然后将其添加到嵌入中，因此它们应该可以工作。有人知道为什么它们不起作用吗？    提交人    /u/tororo-in   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxmn0z/discussion_why_dont_sinusoidal_pe_work_for_longer/</guid>
      <pubDate>Sun, 06 Oct 2024 18:03:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你对多篇第一作者论文的看法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxj25n/d_your_opinion_on_multiple_firstauthors_papers/</link>
      <description><![CDATA[大家好， 我想听听你对列出多个具有同等贡献的第一作者的论文的看法。这对于多学科研究来说是可以理解的，因为你无法真正量化来自不同领域的作者的贡献。但是，当两位作者都在从事机器学习时，这有什么意义呢？在这种情况下，什么样的贡献才算是同等的？数学/想法可以被认为与实施/实验一样重要吗？或者两位第一作者都在从事这两个方面？在雇用等方面，这是否会降低每位作者的成就？最后，你是否意识到对同等贡献的滥用，例如，将没有实际工作努力的出版记录翻倍，作为同事之间的“协议”？ 谢谢！    提交人    /u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxj25n/d_your_opinion_on_multiple_firstauthors_papers/</guid>
      <pubDate>Sun, 06 Oct 2024 15:27:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</guid>
      <pubDate>Sun, 06 Oct 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 LLM 培训中，增加批次大小和使用注意力掩蔽的打包序列有什么区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxguh6/d_whats_the_difference_between_increasing_batch/</link>
      <description><![CDATA[我很好奇在固定长度序列上训练大型语言模型 (LLM) 时，以下两种方法之间的区别： 使用批量大小 = 4，其中每个样本的序列长度为 1024 个标记，并且它们被独立处理。将 4 个序列打包成一个批次，最大序列长度为 4096，并应用注意掩码以确保没有序列关注来自另一个序列的标记。 如果正确应用了注意掩码，确保不会关注其他序列，那么这两种方法在以下方面是否存在显着差异： 内存使用情况 计算成本 训练动态  据我所知，如果没有注意掩码，由于自注意力机制，打包会导致计算成本呈二次方增加。但是使用掩码后，计算和内存使用量不是与将它们作为批处理中的单独序列处理几乎相同吗？还是我遗漏了其他因素？    提交人    /u/JeanMichelRanu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxguh6/d_whats_the_difference_between_increasing_batch/</guid>
      <pubDate>Sun, 06 Oct 2024 13:46:13 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 在包含成本估算的文档上对 BERT 进行微调的想法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxgsoa/project_ideas_for_finetuning_bert_on_documents/</link>
      <description><![CDATA[大家好，我正在做一个项目，需要对包含成本估算的文档进行 NER。具体来说，我必须提取产品名称、价格及其数量。我原本想使用预先训练好的 BERT，但显然进行一些微调是最佳的。你有什么处理这个问题的建议吗？谢谢！    提交人    /u/No_Possibility_7588   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxgsoa/project_ideas_for_finetuning_bert_on_documents/</guid>
      <pubDate>Sun, 06 Oct 2024 13:43:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] MaskBit：通过 Bit Tokens 生成无嵌入图像</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxfy91/r_maskbit_embeddingfree_image_generation_via_bit/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxfy91/r_maskbit_embeddingfree_image_generation_via_bit/</guid>
      <pubDate>Sun, 06 Oct 2024 13:00:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何利用小数据集减少损失</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxb9yl/d_how_to_reduce_the_loss_with_a_small_dataset/</link>
      <description><![CDATA[我正在尝试从此存储库 https://github.com/google-deepmind/language_modeling_is_compression 训练模型。但是，我想在比原始数据集更小的数据集上训练它，即 enwik8（108 字节英语维基百科），特别是在 enwik6 上。考虑到开发人员为在 enwik8 上训练的 Trasformer200k 提供了以下配置，我希望得到一些关于调整哪些超参数以使其适应新数据集的建议： {&quot;training_steps&quot;: &quot;1000000&quot;, &quot;batch_size&quot;: &quot;32&quot;, &quot;seq_length&quot;: &quot;2048&quot;, &quot;embedding_dim&quot;: &quot;64&quot;, &quot;num_heads&quot;: &quot;4&quot;, &quot;num_layers&quot;: &quot;4&quot;, &quot;positional_encodings&quot;: &quot;ROTARY&quot; }  我已经尝试过多次，将批量大小、头数量和层数量减半，但得到的损失太高了。谢谢     由   提交  /u/MicoNDC   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxb9yl/d_how_to_reduce_the_loss_with_a_small_dataset/</guid>
      <pubDate>Sun, 06 Oct 2024 07:40:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要有关 IJCV 提交的建议。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwtf79/d_need_suggestions_regarding_an_ijcv_submission/</link>
      <description><![CDATA[大家好， 我最近有一篇论文被 ECCV 接受，我计划将其扩展到期刊提交。我选择 IJCV 是因为它比 TPAMI 等更快，并且在该领域也得到很好的认可。我的计划如下。 1) 添加 2 到 3 个数据集以显示泛化。 2) 添加新模型（例如骨干等）。 3) 对模型的稳健性进行更多调整。 不幸的是，我的主管没有 IJCV 方面的经验，因此我正在向其他研究人员寻求建议。问题是，遵循上述计划可以吗？此外，在大型会议上发表过论文是否有助于被 IJCV 接受，还是每篇提交都被视为独立处理？ 问候    由    /u/dn8034  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwtf79/d_need_suggestions_regarding_an_ijcv_submission/</guid>
      <pubDate>Sat, 05 Oct 2024 15:50:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 劳拉什么时候不够好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwqgfx/d_when_is_lora_not_good_enough/</link>
      <description><![CDATA[在 LLM 微调任务中，LORA（或其某些变体）不够好，需要进行全面微调，这方面有哪些例子？ 例如，在所有测试任务中，RoSA（LORA 变体）与全面微调一样好 https://arxiv.org/pdf/2401.04679    提交人    /u/osamc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwqgfx/d_when_is_lora_not_good_enough/</guid>
      <pubDate>Sat, 05 Oct 2024 13:30:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​实现 Llama 3.2 1B 和 3B 架构（独立的 Jupyter Notebook）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwq5su/p_implementing_the_llama_32_1b_and_3b/</link>
      <description><![CDATA[    /u/seraschka   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwq5su/p_implementing_the_llama_32_1b_and_3b/</guid>
      <pubDate>Sat, 05 Oct 2024 13:15:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] 泛化界限的理论局限性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwnb1r/r_theoretical_limitations_of_generalization_bounds/</link>
      <description><![CDATA[总结：泛化边界的紧密程度存在根本限制。 尽管近年来提出了许多新提出的泛化边界，但一个共同的主题是，在实际环境（即实际大小的模型、标准数据集）中进行评估时，它们在数值上是松散的（甚至是空洞的）。这严重限制了它们作为性能保证的效用及其对实际算法设计的影响。 这种理论与实践之间的差距仅仅是松散证明技术的产物，还是这种边界的紧密程度也存在根本的统计限制？ 我们发现，在许多情况下，情况都是后者！ 论文 1（发表于 ICLR ’24） https://arxiv.org/abs/2309.13658 :  对于许多算法-分布组合而言，未针对特定算法量身定制的界限必然是松散的。 在足够丰富的学习环境中，依赖于算法的界限受制于不确定性原理：人们要么很好地学习目标分布，要么验证学习的成功——永远无法同时实现！  论文 2（最近的预印本） https://arxiv.org/abs/2410.01969 ：  我们表明，具有某些导致其不稳定的归纳偏差的算法不承认严格的泛化界限。 接下来，我们表明足够稳定的算法确实具有严格的泛化界限。  我们认为我们的发现可能会引起对泛化广泛感兴趣的社区中许多成员的兴趣。 很高兴讨论 - 欢迎提出问题，反馈和批评：)    提交人    /u/zweihander___   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwnb1r/r_theoretical_limitations_of_generalization_bounds/</guid>
      <pubDate>Sat, 05 Oct 2024 10:22:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当你的模型训练时你做什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwjafn/d_what_do_you_do_when_your_model_trains/</link>
      <description><![CDATA[你如何打发时间？    提交者    /u/Replay0307   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwjafn/d_what_do_you_do_when_your_model_trains/</guid>
      <pubDate>Sat, 05 Oct 2024 05:26:41 GMT</pubDate>
    </item>
    <item>
      <title>[R] Meta 发布少于 400 亿个参数的 SOTA 视频生成和音频生成。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwic4m/r_meta_releases_sota_video_generation_and_audio/</link>
      <description><![CDATA[      今天，Meta 发布了一套 SOTA 文本转视频模型。这些模型足够小，可以在本地运行。他们似乎不打算发布代码或数据集，但他们几乎提供了模型的所有细节。事实上，这个模型已经如此连贯，这确实表明开发正在发生得有多快。 https://ai.meta.com/research/movie-gen/?utm_source=linkedin&amp;utm_medium=organic_social&amp;utm_content=video&amp;utm_campaign=moviegen 这套模型（Movie Gen）包含许多模型架构，但看到通过声音和图片同步进行训练非常有趣。从训练的角度来看，这实际上非常有意义。 https://preview.redd.it/047ddxdb7vsd1.png?width=1116&amp;format=png&amp;auto=webp&amp;s=a7cd628a8b2dde9824b27983a430217123c297d8    提交人    /u/AIAddict1935   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwic4m/r_meta_releases_sota_video_generation_and_audio/</guid>
      <pubDate>Sat, 05 Oct 2024 04:26:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习论文的第一作者还是什么都不是？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fwhiit/r_first_author_ml_paper_or_nothing/</link>
      <description><![CDATA[我最近与一位在 AI/ML 领域（非理论领域）颇有建树的朋友进行了一次有趣的对话。他们对出版物中的作者身份做出了一个相当大胆的声明： “在 AI/ML 中，基本上要么第一作者，要么什么都不是。” 此人有超过 2,000 次引用，并且来自顶级机构，所以我倾向于认真对待他们的意见。他们甚至说：“有时除了第三作者之外，他们甚至不接触代码库。” 我很想听听其他人对此的看法。在 AI/ML 论文中，只有第一作者才被认为是重要的，这是真的吗？与其他领域相比如何？ 您在工作或学习中遇到过这种情况吗？我很感激任何见解，特别是那些目前在行业或学术界工作的人的见解。​​​​​​​​​​​​​​​​    提交人    /u/Electronic_Hawk524   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fwhiit/r_first_author_ml_paper_or_nothing/</guid>
      <pubDate>Sat, 05 Oct 2024 03:35:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>