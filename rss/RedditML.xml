<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Tue, 27 Feb 2024 03:14:25 GMT</lastBuildDate>
    <item>
      <title>硕士论文 [D] [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b10lcp/msc_thesis_d_r/</link>
      <description><![CDATA[理学硕士论文 我是一名学生，几个月后将在数据科学专业毕业，我开始考虑我需要并且想要为毕业做的论文项目。我拥有统计学学士学位，有很强的好奇心，我觉得我将在一个实际的环境中工作，在那里我可以真正解决问题。我喜欢我所做的和现在正在做的事情。我可能想做一些保形预测、深度学习、因果关系（也许保形/校准模型、法学硕士、幻觉或其他主题？），而且我觉得我真的很喜欢很多东西（NLP、时间序列、业务任务） 、约束优化、计算机视觉等）。事实上，我想做一些对其他人也有用的事情！不仅仅是对某事的分析.. 所以我向你们经验丰富的 MLE、数据科学家、研究人员和应用科学家提出的问题是：鉴于我的愿望和想法，你们建议我的硕士论文做什么和重点关注什么？我愿意讨论、听取新想法甚至新的研究课题。我还想提一下，如果我能将这个主题应用到现实世界的案例研究中，那就太不可思议了。 非常感谢您的帮助！   由   提交/u/_dns  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b10lcp/msc_thesis_d_r/</guid>
      <pubDate>Tue, 27 Feb 2024 02:33:59 GMT</pubDate>
    </item>
    <item>
      <title>通过 torch.compile 支持 gpt-fast 中的 Mixtral - 比任何非 Groq 端点更快的解码（！）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b10g2s/supporting_mixtral_in_gptfast_through/</link>
      <description><![CDATA[大家好，我们去年 12 月发布了 gpt-fast 作为可破解的“教程”实现文本生成的 SOTA 解码性能的各种实现。 从那时起，我们最近还在 gpt-fast 中添加了 Mixtral 实现。在这里查看：https://github.com/pytorch-labs/gpt-fast /tree/main/mixtral-moe  特色  (!) 无自定义内核 int8 和张量并行支持&lt; /li&gt; 仍然非常简单（支持&lt;150 LOC） 比任何（非 Groq）API 端点更快的解码速度，高达 220 tok/s/用户。  我还对这里涉及的挑战写了一份较长的解释：https://thonking.substack.com/p/short-supporting-mixtral-in-gpt-fast 希望人们觉得它有趣且有用！有趣的是，由于我们实际上在大约 2 个月前完成了这项工作并推迟了合并它，所以一些人（与我们无关）实际上已经对其进行了基准测试。 例如 此评论。  我们最近用 Mixtral 8x7B 尝试过此操作，结果非常疯狂！ Mixtral 8x7B 8 位版本在 A100-GPU (80GB) 上提供 55 个令牌/秒。最有趣的是，它比 4 位+vLLM 更好。    由   提交 /u/programmerChilli   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b10g2s/supporting_mixtral_in_gptfast_through/</guid>
      <pubDate>Tue, 27 Feb 2024 02:27:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] DONUT 用于文档质量检查。无法提取多行文本</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0zw2r/d_donut_for_document_qa_unable_to_extract/</link>
      <description><![CDATA[我正在尝试 DONUT 模型从 pdf 文档中提取文本。当我用文档中的单词提出问题时，它效果很好。但是有时它只返回文本的第一行作为答案。有人遇到过这个问题或者知道如何解决这个问题吗？您还认为我应该使用 BERT 或 LLM 来完成这项任务吗？    由   提交/u/violet_bloom_87   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0zw2r/d_donut_for_document_qa_unable_to_extract/</guid>
      <pubDate>Tue, 27 Feb 2024 02:02:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 某些类别的模型现在被认为“过时”了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ua8s/d_are_some_classes_of_models_considered_obsolete/</link>
      <description><![CDATA[作为一名高级 SWE，我希望在 ML 和数据科学方面进行一些自我驱动的培训。我正在收集和组织课程，我想知道某些主题现在是否已经过时，我可以安全地跳过它们。这主要来自构建应用程序的 POV。 例如，以 Coursera 上的 Andrew Ng GAN 专业为例。有了 Midjourney 或 OpenAI 视觉模型等稳定的扩散模型，GAN 仍然有用例吗？在 NLP 领域，现在只有 GPT4，我还需要研究 HMM 或构建玩具翻译和摘要模型吗？    由   提交/u/The-_Captain  /u/The-_Captain  reddit.com/r/MachineLearning/comments/1b0ua8s/d_are_some_classes_of_models_considered_obsolete/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ua8s/d_are_some_classes_of_models_considered_obsolete/</guid>
      <pubDate>Mon, 26 Feb 2024 22:04:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] Genie：生成交互环境</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0tj6o/r_genie_generative_interactive_environments/</link>
      <description><![CDATA[ 由   提交 /u/topcodemangler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0tj6o/r_genie_generative_interactive_environments/</guid>
      <pubDate>Mon, 26 Feb 2024 21:35:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM Transformer 中 KV 缓存如何有效</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ob2m/d_how_kv_cache_is_valid_in_llm_transformer/</link>
      <description><![CDATA[看到很多文献提到使用 KV 缓存作为转换器模型来减少解码器中的计算，但根据我的理解，当序列达到最大上下文长度并且每次左移都会使最左边的令牌超出范围，KV 缓存将失去有效性，显然是因为先前参与的令牌消失了，这是正确的吗？  &amp;# 32；由   提交 /u/victordion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ob2m/d_how_kv_cache_is_valid_in_llm_transformer/</guid>
      <pubDate>Mon, 26 Feb 2024 18:11:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 差分隐私的实践者 - 您使用过的典型 epsilon 值是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ns07/d_practitioners_of_differential_privacy_whats_the/</link>
      <description><![CDATA[很难找到差分隐私的典型或可接受的 epsilon 范围。在线搜索尚未产生可引用或一致的结果。一些消息来源称典型值为 1.0 到 10.0。 我个人发现：1) 差分隐私显着降低了训练速度近 10 倍 2) 通常我必须使用 1.0 - 5.0 的 epsilon 数字来进行训练结果仍然具有所需的准确性水平 3) 要么我必须显着增加数据集大小，要么我必须牺牲 epsilon (隐私) 才能拥有仍然准确的模型 4) 在执行成员推理任务时，我发现增加数据集大小与减少 epsilon 具有相同的影响。 例如： 1) 在数据集大小为 200K 个示例的情况下，使用 epsilon 1.0 的 DP 模型的性能与不使用 dp 的模型完全相同在准确性和隐私评估任务方面，例如隶属度推断。没有 DP 悬停的模型需要 30 分钟才能训练，而有 DP 悬停的模型则需要 6 小时。 2) 对于小于 100K 的数据集大小，DP 模型根本不够准确，除非我将 epsilon 增加到 5.0。 这让我思考：1) 如果我必须增加 DP 的意义是什么epsilon 一直到 5，模型才有用 2) 如果使用 200K，成员推理攻击没有区别，为什么我还要费心使用 DP？ DP 的实际好处到底是什么？增加数据集大小在隐私评估任务中具有相同的效果，并且计算成本只占一小部分，超出了“理论保证”？   由   提交/u/shengy90  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ns07/d_practitioners_of_differential_privacy_whats_the/</guid>
      <pubDate>Mon, 26 Feb 2024 17:51:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于使用 Transformer Encoder-Decoder 生成嵌入的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0nmhr/d_question_on_generating_embeddings_with/</link>
      <description><![CDATA[我目前正在尝试生成我正在使用的异构数据源的嵌入。问题是，对于每个样本，我都有不同数量的输入特征。对于感兴趣的人来说，这个问题是生物学的。特别是，我正在尝试计算该数据的基因级表示，对于每个基因，我都有不同数量的变体，每个变体都有一个与其关联的特征值。  我均匀化这些特征的解决方案是使用变压器编码器生成嵌入。我通过获取每个基因的所有变异值并将它们连接起来来生成我的特征集。然后，我用零填充生成的张量的边缘，使信号位于中心，并且生成的张量为 1,024 维。编码器学习 256 维表示，解码器将其重建回 1,024 维。 MSE 迅速下降，约 30 个 epoch 后收敛。然后，我使用得到的 Transformer 的经过训练的编码器部分来生成 256 维嵌入。我面临的问题是 1）嵌入值相当小（数量级 ~ 10^(-2)）和 2）样本/基因之间的方差非常低。当我现在将这些嵌入用于我的预测任务时，可以理解的是，我的模型无法学习任何有意义的东西，并且每个预测任务都只输出 0。  需要注意的一些事情：1）我已经使用其他数据成功地训练了相同的架构。这些数据是基因级别的，因此我怀疑问题在于变异嵌入，而不是架构。2）我已经检查了我的神经网络中每一层的输入发生的情况，并且正如所怀疑的那样，线性层+激活不会导致 logits 发生实质性变化，即它们保持在 10^(-2) 大小左右。 3) 我尝试对我的数据进行 minmax 标准化，以强制其超出 10(-2) ）的大小，但无济于事。这里样本之间的微小差异就是原因。在 0 和 1 之间进行最小最大归一化之后，我们有 1 或 2 个接近 1 的值，然后其余样本很快下降到接近 0，同样方差很小。  任何尝试过类似方法的人的建议、参考或经验将不胜感激！    由   提交 /u/Primary-Wasabi292    reddit.com/r/MachineLearning/comments/1b0nmhr/d_question_on_generate_embeddings_with/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0nmhr/d_question_on_generating_embeddings_with/</guid>
      <pubDate>Mon, 26 Feb 2024 17:44:54 GMT</pubDate>
    </item>
    <item>
      <title>对于新晋研究科学家来说，该行业不会“复苏”[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0n3ib/the_industry_is_not_going_recover_for_newly/</link>
      <description><![CDATA[      今天的热门话题问：“科技行业还没有复苏吗？我有那么糟糕吗？” 让我做出一个大胆的预测（我希望我是错的，但我不认为我是错的）：这个行业不会“ “恢复”对于新晋研究科学家： 您的机器学习论文数量呈指数级增长，反映出博士生和博士后数量呈指数级增长： ​ &lt; p&gt;https://preview.redd.it/viv6l1gnkykc1。 png?width=899&amp;format=png&amp;auto=webp&amp;s=04e227dede42f7d46d1941fc268bb7ea0a409a04 ...毕业并开始竞争大致固定数量的井- 支付行业研究职位。这些职位的数量可能会季节性增加或减少，但长期趋势是他们的就业前景将变得越来越差，而这种指数趋势仍在持续。 ​  div&gt;  由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0n3ib/the_industry_is_not_going_recover_for_newly/</guid>
      <pubDate>Mon, 26 Feb 2024 17:24:08 GMT</pubDate>
    </item>
    <item>
      <title>[N] 科技巨头正在开发他们的人工智能芯片。这是清单</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ira9/n_tech_giants_are_developing_their_ai_chips_heres/</link>
      <description><![CDATA[NVIDIA GPU 短缺，导致多家公司创建自己的 AI 芯片。以下是这些公司的列表： • Google 处于改进张量处理单元 (TPU) 的前沿https://cloud.google.com/tpu?hl=en Google Cloud 技术。 • OpenAI 正在研究设计专有 AI 芯片的潜力https://www.reuters.com/ technology/chatgpt-owner-openai-is-exploring-making-its-own-ai-chips-sources-2023-10-06/。 • 微软宣布 https://news.microsoft.com/source/ features/ai/in-house-chips-silicon-to-service-to-meet-ai-demand/ 两款定制设计的芯片：用于大型语言模型训练和推理的 Microsoft Azure Maia AI 加速器以及用于大型语言模型训练和推理的 Microsoft Azure Maia AI 加速器Azure Cobalt CPU，用于 Microsoft 云上的通用计算工作负载。 • 亚马逊推出了 Inferentia AI 芯片 https://aws.amazon.com/machine-learning/inferentia/ 和第二代机器学习 (ML) 加速器 AWS Trainium https://aws.amazon.com/machine-learning/trainium/。 • Apple 一直在开发其系列定制芯片并推出https://www.apple.com/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal -computer/ M3、M3 Pro 和 M3 Max 处理器，可扩展到专门的 AI 任务。 • Meta 计划部署新版本的定制芯片，旨在支持其人工智能据路透社报道，人工智能（AI）的推动 https://www.reuters.com/technology/meta-deploy-in-house-custom-chips-this-year-power-ai-drive-memo-2024-02-01/ . • 据报道，华为https://www.reuters.com/technology/ai-chip-demand-forces-huawei-slow-smartphone-product-sources-2024-02-05/由于人工智能芯片的需求，人工智能并放慢了高端 Mate 60 手机的生产 https://www.hisilicon.com/ en/products/ascend 飙升。 我错过了什么吗？   由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ira9/n_tech_giants_are_developing_their_ai_chips_heres/</guid>
      <pubDate>Mon, 26 Feb 2024 14:25:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是科技行业还没复苏还是我太差了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ia76/d_is_the_tech_industry_still_not_recovered_or_i/</link>
      <description><![CDATA[我是欧洲顶尖大学的一名应届博士毕业生，正在研究 ML/CV 中的一些热门主题，已发表 8 - 20 篇论文，其中大部分是我的第一作者。这些论文已累计被引用1000-3000次。 （使用新帐户和广泛的范围来保持匿名） 尽管我认为自己是一个相当有实力的候选人，但我在最近的求职过程中遇到了重大挑战。我主要瞄准研究科学家职位，希望从事开放式研究。我已经联系了欧洲、中东和非洲地区的许多高级机器学习研究人员，虽然有些人表达了兴趣，但不幸的是，由于各种原因（例如人员有限或招聘经理没有更新信息），没有一个机会成为现实。 我主要针对大型科技公司以及一些最近流行的机器学习初创公司。不幸的是，我的大部分申请都被拒绝了，而且常常没有面试的机会。 （我只接受过一家大型科技公司的一次面试，然后就被拒绝了。） 特别是，尽管有朋友的推荐，我还是立即遭到了 Meta 的研究科学家职位拒绝（几天之内）。我现在只是非常困惑和不安，不知道出了什么问题，我是否被这些公司列入了黑名单？但我不记得我树敌过。我希望就下一步可以做什么寻求一些建议......   由   提交/u/Holiday_Safe_5620   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ia76/d_is_the_tech_industry_still_not_recovered_or_i/</guid>
      <pubDate>Mon, 26 Feb 2024 14:04:26 GMT</pubDate>
    </item>
    <item>
      <title>目前关于使用 TensorFlow 2.x 与 PyTorch 的共识是什么？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0gxy7/whats_the_current_consensus_on_using_tensorflow/</link>
      <description><![CDATA[我知道许多人在 TF1 到 TF2 迁移期间离开了 TF，并且再也没有回头。我的问题是，目前关于使用 TF2 与 PyTorch（与 Jax）的共识是什么？为什么？  从端到端的角度来看，对我来说，TF2 很好。 PyTorch 上的调试更容易，但好处还不足以放弃所有内容并永远保留 TF2。你们怎么看？ 假设您正在构建人工智能产品并且部署是必须的，您在代码库中更喜欢 TensorFlow 还是 Pytorch（或 JAX），为什么？    由   提交 /u/1infiniteloop   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0gxy7/whats_the_current_consensus_on_using_tensorflow/</guid>
      <pubDate>Mon, 26 Feb 2024 13:01:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] YOLOv9：使用可编程梯度信息学习你想学的东西</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ep3a/r_yolov9_learning_what_you_want_to_learn_using/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.13616 代码：https://github .com/WongKinYiu/yolov9 模特：https://huggingface. co/merve/yolov9 演示：https://huggingface .co/spaces/kadirnar/Yolov9 Colab：https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/ notebooks/train-yolov9-object-detection-on-custom-dataset.ipynb 摘要：  今天的深度学习方法重点关注如何设计最合适的目标函数，使得模型的预测结果能够最接近真实情况。同时，必须设计一个适当的架构，可以帮助获取足够的信息进行预测。现有方法忽略了一个事实，即当输入数据经过逐层特征提取和空间变换时，大量信息将会丢失。本文将深入研究数据通过深度网络传输时数据丢失的重要问题，即信息瓶颈和可逆函数。我们提出了可编程梯度信息（PGI）的概念来应对深度网络实现多个目标所需的各种变化。 PGI可以为目标任务计算目标函数提供完整的输入信息，从而获得可靠的梯度信息来更新网络权值。此外，还设计了一种基于梯度路径规划的新型轻量级网络架构——通用高效层聚合网络（GELAN）。 GELAN的架构证实了PGI在轻量级模型上取得了优异的结果。我们在基于 MS COCO 数据集的目标检测上验证了所提出的 GELAN 和 PGI。结果表明，与基于深度卷积开发的最先进方法相比，GELAN 仅使用传统的卷积算子即可实现更好的参数利用率。 PGI 可用于从轻型到大型的各种模型。它可以用来获取完整的信息，使得从头训练模型能够比使用大数据集预训练的state-of-the-art模型获得更好的结果，比较结果如图1所示。源代码是位于：此 https URL。   &amp;# 32；由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ep3a/r_yolov9_learning_what_you_want_to_learn_using/</guid>
      <pubDate>Mon, 26 Feb 2024 10:50:46 GMT</pubDate>
    </item>
    <item>
      <title>“不要停止预训练”只是微调吗？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b0ccd0/is_dont_stop_pretraining_just_finetuning_r/</link>
      <description><![CDATA[不要停止预训练论文吹嘘通过“领域适应预训练”来提高法学硕士的表现，但这似乎是微调的另一个词，一点也不新鲜。我肯定错过了一些东西 - 它是什么？   由   提交/u/MLenthusiast34   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b0ccd0/is_dont_stop_pretraining_just_finetuning_r/</guid>
      <pubDate>Mon, 26 Feb 2024 08:09:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>