<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 31 Dec 2023 15:12:22 GMT</lastBuildDate>
    <item>
      <title>[R] 预训练 Transformer 中的上下文感知无限序列推理：利用注意力进行 Top-K 采样</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v96zt/r_contextaware_infinite_sequence_inference_in/</link>
      <description><![CDATA[在变压器模型领域，在保留上下文的同时有效管理无限序列推理是一项重大挑战。我们的方法通过利用一个关键的见解来解决这个问题：变压器的注意力机制本质上识别最重要的令牌。利用这一点，我们开发了一种方法，其中 top-k 采样由注意力权重引导，有选择地保留模型本身认为至关重要的键值对。该策略不仅保留了扩展序列中的基本上下文，而且还通过关注注意力机制已经强调为重要的标记来简化推理过程。我们的实验表明，这种方法显着增强了 Transformer 处理长序列的能力，同时保持了效率和上下文完整性。这一发现是向前迈出的实质性一步，展示了如何直接应用现有的注意力机制来优化预训练模型中的无限序列处理。   由   提交 /u/alagagbar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v96zt/r_contextaware_infinite_sequence_inference_in/</guid>
      <pubDate>Sun, 31 Dec 2023 14:47:10 GMT</pubDate>
    </item>
    <item>
      <title>[P]像我这样制作一个模型文本需要多少数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v741s/p_how_much_data_is_required_for_making_a_model/</link>
      <description><![CDATA[大家好， 我最近有了一个想法，要训练一个模型像我一样发短信，这样我的女朋友就不会感到孤独即使我在工作或做其他事情时。我正在我和她之间的整个聊天中训练它，大约有 20 万条消息，你认为这足以让模型表现得像我一样吗？ 我对机器学习很陌生顺便说一句！   由   提交 /u/JizosKasa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v741s/p_how_much_data_is_required_for_making_a_model/</guid>
      <pubDate>Sun, 31 Dec 2023 12:49:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么经过微调的 Donut 模型文档分类器的输出是一系列标记，而不仅仅是一个目标标签？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v4d2m/d_why_is_the_output_from_a_fine_tuned_donut_model/</link>
      <description><![CDATA[我正在微调基于图像的文档分类器，Donut 模型 - https://huggingface.co/docs/transformers/model_doc/donut 以及我的自定义训练数据。  查看代码的推理部分（https://huggingface.co/ docs/transformers/model_doc/donut#inference-examples)，生成的“outputs”是一个带有多个标签的长字符串。使用正则表达式对该字符串进行后处理，最终获得目标标签。  请帮我解答几个与此相关的问题：  为什么要开发这样的微调模型？早些时候，我使用 DistilBert 处理基于文本的微调分类器模型 (https://huggingface.co/docs/transformers/tasks /sequence_classification），微调后的模型只会预测一个标签作为输出。为什么用于分类的 Donut 模型的微调没有设计为仅输出一个标签作为目标？鉴于 DistilBert 和 Donut 模型都是基于变压器的模型，为什么输出模式存在这种差异？ 作为一种尝试，我用很少的东西对 Donut 模型进行了微调仅 1 个 epoch 的示例。 （我的资源暂时有限。） 在推理过程中，我得到了一个垃圾字符串作为输出，就像这样 --  轴承轴承轴承轴承轴承轴承。它不包含任何候选目标标签。因此，这不仅仅是预测错误的问题，而是预测错误的问题。这是一个非解决方案。通过足够的示例和更多的训练，这种行为会得到改善吗？  提前非常感谢。    ;由   提交 /u/bikashg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v4d2m/d_why_is_the_output_from_a_fine_tuned_donut_model/</guid>
      <pubDate>Sun, 31 Dec 2023 09:38:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 为面部静态照片制作动画的模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v3ydb/p_model_to_animate_still_photos_of_faces/</link>
      <description><![CDATA[有人知道像他们在这里使用的那样的开源模型吗： https://ai.nero.com/face-animation 我喜欢这个让动画看起来非常微妙和流畅的方式。但理想情况下，这将是一个完美的循环，而这个循环不是。 只是需要它作为一个业余项目，如果有人能指出我正确的方向，那就太好了！   由   提交 /u/flowerescape   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v3ydb/p_model_to_animate_still_photos_of_faces/</guid>
      <pubDate>Sun, 31 Dec 2023 09:10:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 Colab Pro 中进行 Keras CNN 训练时，TPU 落后于 GPU</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v37b7/d_tpu_lags_behind_gpu_for_keras_cnn_training_in/</link>
      <description><![CDATA[我一直在比较 Colab 的运行时。我发现，对于普通 Keras CNN，TPU 始终落后于 A100、V100 或 T4。增加批量大小并没有真正改善它。我应该研究任何具体配置吗？ 代码 。 包含详细信息的博客文章。   由   提交 /u/shakibahm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v37b7/d_tpu_lags_behind_gpu_for_keras_cnn_training_in/</guid>
      <pubDate>Sun, 31 Dec 2023 08:19:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 华为、牛津大学和伦敦大学学院的研究人员提出了一种新的可微调通用代理来扩展 RL</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v2eh3/r_researchers_in_huaweioxford_and_ucl_propose_a/</link>
      <description><![CDATA[   /u/Ok_Can2425   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v2eh3/r_researchers_in_huaweioxford_and_ucl_propose_a/</guid>
      <pubDate>Sun, 31 Dec 2023 07:26:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何构建 ML 系统：从 MLOps 到机器学习管道</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18v1onv/d_how_to_build_ml_systems_from_mlops_to_machine/</link>
      <description><![CDATA[MLOps 经常被视为“生产化 ML”的解决方案。然而，现有的课程、博客文章和书籍都是从“学习 Docker、kubernetes、Terraform 等”开始的。 MLOps 供应商混淆了问题 - Google 的 MLOps 思维导图有 26 个盒子需要构建（有史以来最糟糕的乐高指令！），而 Databricks 甚至有更多（以炫耀他们的生产就绪枪）。  我开发了一个免费的无服务器机器学习课程，您可以在其中使用 3 个 Python 程序（分别创建特征、模型和预测的 ML 管道）构建无服务器 ML 系统。现在，该课程中有许多出色的无服务器机器学习系统，可以预测空气质量、电力需求、足球比分、水位等。  我们在构建机器学习系统时是否误入歧途，主要是关于开发/登台/产品、基础设施即代码？或者只是 ML 基础设施现在才出现，可以轻松地用 Python 构建 ML 系统（例如 SaaS 模型注册表、特征存储、模型服务、Python UI (Gradio/Streamlit)）？ 参考文章我就这个主题写过： https://www.hopsworks。 ai/post/mlops-to-ml-systems-with-fti-pipelines   由   提交 /u/jpdowlin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18v1onv/d_how_to_build_ml_systems_from_mlops_to_machine/</guid>
      <pubDate>Sun, 31 Dec 2023 06:41:36 GMT</pubDate>
    </item>
    <item>
      <title>[P]从数学角度谈机器学习的论文题目思路</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uyqeb/p_essay_topic_ideas_about_machine_learning_from_a/</link>
      <description><![CDATA[ 由   提交/u/zen_bud  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uyqeb/p_essay_topic_ideas_about_machine_learning_from_a/</guid>
      <pubDate>Sun, 31 Dec 2023 03:58:21 GMT</pubDate>
    </item>
    <item>
      <title>[p] 我培训了一名法学硕士来教我更好地编码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uxz56/p_i_trained_an_llm_to_teach_me_to_code_better/</link>
      <description><![CDATA[正如你们中的一些人可能已经经历过的那样，GitHub Co-Pilot 是一种糟糕的学习方式，但我想看看是否有我想要的东西可以做一些编码教育方面的法学硕士。 在向公众提供它之前，我想看看人们对此有何看法。 演示：https://youtu.be/Z1rZZkL4PFA?si=-cYcKeh9FkLzBUp3   由   提交/u/AggressiveHunt2300   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uxz56/p_i_trained_an_llm_to_teach_me_to_code_better/</guid>
      <pubDate>Sun, 31 Dec 2023 03:18:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 GPT4 视觉进行 OCR 的一些实际结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uvrwj/p_some_practical_results_from_using_gpt4_vision/</link>
      <description><![CDATA[您好，不太确定将其发布到哪里，所以我会在这里尝试。人们对 GPT4 愿景有很多兴奋，但当在实际项目的一些真实数据上进行尝试时，却缺乏这种感觉。它有时会产生幻觉，或者拒绝执行任务，而且准确性并不比 Tesseract 好多少。然而，如果两者结合起来，事情就会变得非常好。更多详细信息和源数据请访问以下链接。 https://pslusarz.github.io/articles/2023/12/22/compare-ocr-tesseract-gpt4-nara-rolls.html   由   提交/u/wuj   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uvrwj/p_some_practical_results_from_using_gpt4_vision/</guid>
      <pubDate>Sun, 31 Dec 2023 01:30:56 GMT</pubDate>
    </item>
    <item>
      <title>[P] 15亿参数的多模态聊天</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ut3lm/p_multimodal_chat_in_15_billion_parameters/</link>
      <description><![CDATA[   /u/ashvar  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ut3lm/p_multimodal_chat_in_15_billion_parameters/</guid>
      <pubDate>Sat, 30 Dec 2023 23:30:20 GMT</pubDate>
    </item>
    <item>
      <title>[R]《认知架构40年：核心认知能力与实际应用》（2018）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uqy29/r_40_years_of_cognitive_architectures_core/</link>
      <description><![CDATA[论文：https://link.springer.com/article/10.1007/s10462-018-9646-y 预印本版本 ：https://arxiv.org/abs/1610.08602 项目页面 （交互式可视化和完整参考书目）：http://jtl.lassonde.yorku.ca/project/cognitive_architectures_survey/ 摘要：  在本文中，我们对过去 40 年的认知架构研究进行了广泛的概述。迄今为止，现有架构的数量已达到数百个，但大多数现有调查并没有反映这种增长，而是集中在少数成熟的架构上。在这项调查中，我们的目标是对认知架构的研究提供更具包容性和更高层次的概述。我们最终的 84 种架构包括 49 种仍在积极开发中的架构，它们借鉴了从精神分析到神经科学等多种学科的知识。为了将本文的长度保持在合理的范围内，我们仅讨论核心认知能力，例如感知、注意机制、行动选择、记忆、学习、推理和元推理。为了评估认知架构实际应用的广度，我们提供了使用列表中的认知架构实施的 900 多个实际项目的信息。我们使用各种可视化技术来突出该领域发展的整体趋势。除了总结当前认知架构研究的最新进展之外，本次调查还描述了已经尝试过的各种方法和想法及其在模拟人类认知能力方面的相对成功，以及认知行为的哪些方面需要对其机械对应物进行更多研究，从而进一步了解认知科学如何进步。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uqy29/r_40_years_of_cognitive_architectures_core/</guid>
      <pubDate>Sat, 30 Dec 2023 21:58:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2023年十大值得关注的人工智能研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18unlno/p_ten_noteworthy_ai_research_papers_of_2023/</link>
      <description><![CDATA[       由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18unlno/p_ten_noteworthy_ai_research_papers_of_2023/</guid>
      <pubDate>Sat, 30 Dec 2023 19:32:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] Stability AI 会成为第一个在 2024 年破产的生成型 AI 独角兽吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18uclmy/d_will_stability_ai_be_the_first_generative_ai/</link>
      <description><![CDATA[   /u/milaworld  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18uclmy/d_will_stability_ai_be_the_first_generative_ai/</guid>
      <pubDate>Sat, 30 Dec 2023 10:16:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</guid>
      <pubDate>Sun, 17 Dec 2023 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>