<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 02 Mar 2024 06:17:43 GMT</lastBuildDate>
    <item>
      <title>[P] 如何将我的模型的结果与最先进的模型进行比较？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4frp3/p_how_to_compare_results_of_my_model_with_state/</link>
      <description><![CDATA[我已经使用最近的数据集训练了我的模型。为了评估其性能，我需要将结果与最先进模型的结果进行比较。然而，挑战来自于这样一个事实：SOTA 模型（来自研究论文）是在不同的数据集（不是最近的数据集）上训练的，使用不同的特征，并且应用于数据的归一化技术也有所不同。鉴于这些差异，有效比较结果的最佳方法是什么？   由   提交/u/XhbC5  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4frp3/p_how_to_compare_results_of_my_model_with_state/</guid>
      <pubDate>Sat, 02 Mar 2024 04:44:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 具有经过训练的模型的开源 ChatGPT 项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4ejlk/d_open_source_chatgpt_project_with_trained_models/</link>
      <description><![CDATA[嗨， 是否有类似 ChatGPT 的项目，具有经过训练的对话 AI 模型？我想用自己的数据集进一步训练它。   由   提交 /u/Big-Quote-547   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4ejlk/d_open_source_chatgpt_project_with_trained_models/</guid>
      <pubDate>Sat, 02 Mar 2024 03:39:28 GMT</pubDate>
    </item>
    <item>
      <title>[P]人脸识别模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4d6qv/p_face_recognition_model/</link>
      <description><![CDATA[大家好，大家好吗？所以我是计算机视觉的初学者，但本学期我在 SWE 课程中有一个项目，它基本上是一个电子商务网站，所以我想制作一个面部识别模型作为登录功能，所以我决定使用预训练模型就像 arcface 一样，因为我的训练数据非常有限，最多可能是 100 张图像，我会尝试使用数据增强技术来增加到 800 张，也许......所以工作流程是：  数据增强 使用我的训练数据微调预训练模型 测试它 使用flask和rest API将其与网站集成 创建帐户时，我会要求用户提供清晰的图像并将其发送到模型 用它训练模型并获取该图像的矢量嵌入并存储在数据库中沿着用户的其余信息 当用户尝试登录时，我会要求他提供他的新图像，然后我将发送到模型以嵌入它，然后使用余弦相似度将其与数据库中存储的信息进行比较 如果它通过了阈值，我将提供该用户的其余信息并要求他确认他的密码&lt; /p&gt;  我还好吗？我感谢任何可以帮助我的人...谢谢你    由   提交 /u/Street-Possession-33   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4d6qv/p_face_recognition_model/</guid>
      <pubDate>Sat, 02 Mar 2024 02:31:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 文本转 3D 入门</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b49u9n/r_a_primer_in_textto3d/</link>
      <description><![CDATA[文本转 3D 及其对应的图像已经席卷了机器学习领域，Metaverse 即将出现，以及 Nvidia 的 Omniverse，我们以前从未见过如此关注生成3D领域。 2010 年代的《我的世界》现在是 2020 年代的元宇宙，未来的孩子将不再使用立方体素进行构建，而是仅通过在耳机中说出文本提示即可将物体想象成巨大的反乌托邦式服务器农场从极其庞大的神经网络和大型语言模型中大量生产 3D 内容。 Minecraft 2.0（如果你愿意的话）。 感觉有点落后了？不用担心，我已经为有兴趣在短短 5 分钟内了解该领域的任何人编写了一本易于理解的入门书！ https://ai.plainenglish.io/text-to-3d-b607bf245031 &lt; /div&gt;  由   提交 /u/SirFletch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b49u9n/r_a_primer_in_textto3d/</guid>
      <pubDate>Fri, 01 Mar 2024 23:59:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] 新会议的审稿人会看到以前的提交/评论吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b49o9e/r_do_reviewers_at_a_new_conference_see_the/</link>
      <description><![CDATA[您好，机器学习研究和发布新手。作为我实验室的一部分，我们最近向 CVPR 提交了（收到的分数为略低于、临界、略高于，在我们反驳后，被降级为三个“略低于”）。 有关于提交的讨论参加稍后的会议，例如 ECCV。我很好奇 ECCV 审稿人是否能够看到我们之前提交的论文以及审稿人的评论。   由   提交 /u/YodelingVeterinarian   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b49o9e/r_do_reviewers_at_a_new_conference_see_the/</guid>
      <pubDate>Fri, 01 Mar 2024 23:52:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 计算机视觉在仓储物流中的应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b43yn9/d_computer_vision_in_warehousing_and_logistics/</link>
      <description><![CDATA[在此 文章您将了解计算机视觉如何优化物流和仓储流程。  仓储行业正在快速适应在线购物的需求。计算机视觉改善库存管理、流程优化和质量控制。它可以自动执行手动任务并优化许多操作。OpenCV.ai 团队描述了该领域最流行的 AI 实施用例。  更多详细信息为这里   由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/MachineLearning/comments/1b43yn9/d_computer_vision_in_warehousing_and_logistics/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b43yn9/d_computer_vision_in_warehousing_and_logistics/</guid>
      <pubDate>Fri, 01 Mar 2024 20:03:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 部署 Transformer 模型的最佳方式</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b43ndx/d_best_way_to_deploy_transformer_models/</link>
      <description><![CDATA[嗨，我是一名软件工程师，构建全栈应用程序已经三年了。我正在尝试利用 OpenAI 的剪辑模型来分析帧并将其存储在矢量数据库中以进行语义搜索，从而构建一个视频理解引擎。  在部署全栈应用程序并扩展它们（减去整个 ML Ops 部分）方面，我的表现非常出色。  我正在尝试在 AWS 上部署 CLIP，我在研究中发现了几个选项，但我越深入研究它就越感到困惑。  我找到的选项是：1. HuggingFace 专用推理端点 2. 在 Sagemaker 上拥抱面部容器 3. 使用来自张量流的剪辑并在 Sagemaker 上下载权重 4. 启动我的 FAST API 服务器具有加速计算实例的 EC2  就上下文而言，我使用的是拥抱脸的 Transformers 库:)  所以我的问题是： 1. 你们通常如何部署模型 2.在构建多模式方面，是否可以在单个 SageMaker 端点中部署所有模型，如果可以的话，它是如何工作的！ 如果这听起来很愚蠢，我很抱歉，但请帮助兄弟解决这个问题哈哈 期待学习更多！   由   提交/u/Hot-Afternoon-4831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b43ndx/d_best_way_to_deploy_transformer_models/</guid>
      <pubDate>Fri, 01 Mar 2024 19:52:04 GMT</pubDate>
    </item>
    <item>
      <title>lalamu 和 wav2lip 的消失 - 有什么见解吗？ [D]，[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3zetw/disappearance_of_lalamu_and_wav2lip_any_insights/</link>
      <description><![CDATA[大家好，我注意到 lalamu 和 wav2lip 不再可用，并且似乎已从应用商店中下架。有人有关于这些应用程序发生的情况的任何信息或更新吗？我尝试查找官方资源和开发者论坛，但一无所获。这些工具对我的项目至关重要，我正在寻找任何替代方案或有关其状态的新闻。如果您有任何见解或更新，我们将不胜感激！   由   提交 /u/Due_Brief6661   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3zetw/disappearance_of_lalamu_and_wav2lip_any_insights/</guid>
      <pubDate>Fri, 01 Mar 2024 17:04:48 GMT</pubDate>
    </item>
    <item>
      <title>[P] Luminal：通过图编译在 Rust 中进行快速机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3yvr2/p_luminal_fast_ml_in_rust_through_graph/</link>
      <description><![CDATA[大家好，我用 Rust 开发 ML 框架已经有一段时间了，我终于很高兴与大家分享它。 Luminal 是一个深度学习库，它使用可组合编译器来实现高性能。 当前的 ML 库往往庞大且复杂，因为它们试图将高级操作直接映射到低级手写内核，并专注于急切执行。像 PyTorch 这样的库包含数十万行代码，单个程序员几乎不可能理解所有内容，除非进行大规模重构。 但是有必要这么复杂吗？机器学习模型往往是由一些简单运算符组成的静态数据流图。这使我们能够拥有一个非常简单的核心，仅支持一些原始操作，并使用它们来构建复杂的神经网络。然后，我们可以编写编译器，在构建图之后修改图，以根据我们运行的后端交换更高效的操作。 Luminal 采用这种方法极端情况下，仅支持 11 种基本运算 (primops)：  一元 - Log2、Exp2、Sin、Sqrt、Recip 二元 - &lt; strong&gt;Add、Mul、Mod、LessThan 其他 - SumReduce、MaxReduce、Contigious  每个复杂的操作都可以归结为对于这些原始操作，例如，当您执行 a - b 时，add(a, mul(b, -1)) 会写入图表中。或者，当您执行a.matmul(b)时，实际放在图表上的是sum_reduce(mul(reshape(a), reshape(b)))。&lt; /p&gt; 一旦构建了图，迭代编译器就可以对其进行修改，以用更高效的操作替换 primops，具体取决于其运行的设备。例如，在 Nvidia 卡上，动态编写高效的 Cuda 内核来替换这些操作，并用专门的 cublas 内核交换支持的操作。 这种方法会产生一个简单的库，并且性能仅受限于编译器程序员的创造力，而不是模型程序员。 Luminal 还有许多其他简洁的功能，请查看存储库 这里 如果您有任何问题请lmk！   由   提交/u/jafioti   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3yvr2/p_luminal_fast_ml_in_rust_through_graph/</guid>
      <pubDate>Fri, 01 Mar 2024 16:44:30 GMT</pubDate>
    </item>
    <item>
      <title>[R]：学习生成零样本任务适应的指令调优数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3y98f/r_learning_to_generate_instruction_tuning/</link>
      <description><![CDATA[很高兴分享我们在合成任务生成方面的工作。 隆重介绍 Bonito 🐟，这是一个开源模型，可以将您的原始、将未注释的数据转化为合成指令调整数据集。有了它，您可以轻松地为您的专有和私人数据创建专门的法学硕士！ 查看我们下面的工作：论文：https://arxiv.org/abs/2402.18334 代码：https://github.com /BatsResearch/bonito 模型：https://huggingface.co/BatsResearch/bonito-v1    由   提交 /u/nihalnayak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3y98f/r_learning_to_generate_instruction_tuning/</guid>
      <pubDate>Fri, 01 Mar 2024 16:19:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 具有动态操作集的非 RAG 回溯 GPT 代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3w4wp/p_a_nonrag_backtracking_gpt_agent_with_a_dynamic/</link>
      <description><![CDATA[      大家好，我&#39;我们一直在开发一个不依赖检索增强生成（RAG）来查找相关信息并生成响应的新框架。相反，它利用独特的文本接口 (TI)，实现与外部资源的直接 GPT-4 交互，就像我们与 GUI 交互的方式一样。 由于这种方法需要与 TI 重复交互，因此 LLM 的作用就像一个自治系统代理人。与 AutoGPT 不同，AutoGPT 的操作是预先确定的并且不会更改，操作由 TI 提供，并根据 TI 所处的状态进行更改。 这种方法的主要局限性是：它仅适用于GPT-4 并需要构建文本界面以与不同类型的资源交互。 演示视频：https: //www.youtube.com/watch?v=sE2JK3IB6rI 代码：https://github .com/ash80/backtracking_gpt X 上的线程：https://x.com/ash_at_tt /status/1763575975185403937 欢迎您提供反馈、建议和贡献。   由   提交/u/ashz8888  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3w4wp/p_a_nonrag_backtracking_gpt_agent_with_a_dynamic/</guid>
      <pubDate>Fri, 01 Mar 2024 14:52:51 GMT</pubDate>
    </item>
    <item>
      <title>多智能体深度 q 学习深入研究开发状态 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3ts4d/multi_agent_deep_q_learning_takes_a_dive_at/</link>
      <description><![CDATA[      我正在使用 double &amp;深度 Q 学习对决。达到 epsilon 0.01 后不久，奖励开始走下坡路。我正在尝试不同的超参数，但对任何类似的经验/想法感兴趣。  我的猜测是，由于这是一个多智能体场景，因此在大部分探索阶段，智能体都会在给定其余动作的随机动作的情况下学习最佳动作。一旦 epsilon 达到 0.01，其余智能体的行为（以及每个智能体的环境）就会发生变化。这就是奖励的意思。  https://preview.redd .it/fk6fpc0a1qlc1.png?width=696&amp;format=png&amp;auto=webp&amp;s=63c7a4bd1a560809cbec2099261d6abc8c2152b5   由   提交 /u/ripototo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3ts4d/multi_agent_deep_q_learning_takes_a_dive_at/</guid>
      <pubDate>Fri, 01 Mar 2024 13:04:11 GMT</pubDate>
    </item>
    <item>
      <title>DeepMind 推出 Hawk 和 Griffin [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3leks/deepmind_introduces_hawk_and_griffin_r/</link>
      <description><![CDATA[        由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3leks/deepmind_introduces_hawk_and_griffin_r/</guid>
      <pubDate>Fri, 01 Mar 2024 04:28:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 ViT 比 SWIN 更常用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b3bhbd/d_why_is_vit_more_commonly_used_than_swin/</link>
      <description><![CDATA[我仍在阅读，但我阅读的大多数计算机视觉论文都使用 ViT 作为其主干，而不是 SWIN 或其他类似的架构，但为什么呢？  ​ ViT 论文必须在 303M 图像 JFT 数据集上预训练模型，以击败 ImageNet 上的早期卷积模型，而 SWIN 无需任何预训练即可实现更好的性能。训练。我想，如果 SWIN 以同样的方式进行预训练，即使不是更高的性能，也能在 ImageNet 上实现相当的性能，但不可否认的是，我还没有看到任何工作来验证这个想法。 ​ 这只是 ViT 优先的情况，所以现在每个人都使用它作为默认值还是还有其他原因？   由   提交 /u/PM_ME_JOB_OFFER   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b3bhbd/d_why_is_vit_more_commonly_used_than_swin/</guid>
      <pubDate>Thu, 29 Feb 2024 21:10:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>