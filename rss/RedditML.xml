<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 17 Jun 2024 12:27:58 GMT</lastBuildDate>
    <item>
      <title>[R] 人工智能再次解答数学问题！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhwn7w/r_ai_is_solving_mathematicsagain/</link>
      <description><![CDATA[使用 LLaMa-3 8B 通过蒙特卡洛树自我优化访问 GPT-4 级别数学奥林匹克解决方案： https://arxiv.org/abs/2406.07394 本文将大型语言模型 (LLM) 与蒙特卡洛树搜索 (MCTS) 相结合，旨在提高复杂数学推理任务的性能。它通过 MCT 自优化 (MCTSr) 算法实现此目的。 该算法显着提高了以下数据集的成功率...... ⭐️ GSM8K： 8 次推出高达 96.66%，零样本为 74.07% ⭐️ GSM-Hard： 总体为 25.47%，8 次推出 vs 零样本为 45.49% ⭐️ MATH： 总体为 58.24%，8 次推出 vs 零样本为 24.36% ⭐️ 奥林匹克级基准，如 AIME： 8 次推出为 11.79%，零样本为 2.36% 这是什么意思？ 与 GPT-4 等闭源 LLM 相比，使用 LLaMA-38B 的 MCTSr 实现了相当的结果，表明它可以提高较小开源模型的推理能力。该论文得出结论，MCTSr 是一种使用 LLM 进行复杂数学推理的强大且有前途的方法。 作为一名数学家，我发现这篇论文非常酷！很想听听大家的想法！    提交人    /u/elliesleight   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhwn7w/r_ai_is_solving_mathematicsagain/</guid>
      <pubDate>Mon, 17 Jun 2024 12:17:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 需要帮助来理解如何在大学项目中使用 rnn ( lstm )</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhwbv3/p_need_help_understanding_how_to_use_rnn_lstm_for/</link>
      <description><![CDATA[嗨， 我正在尝试使用 rnn 为大学项目预测巨无霸指数。我有点卡住了 :( 我得到的数据集只有每种货币的大约 33 行信息。我应该能够预测数据集中三种不同货币的巨无霸指数。数据集还包含其他货币的信息以及它们都共有的几个特征。 现在，我需要一些关于如何进步的建议。  我应该忽略数据集中的其他货币，只使用特定于我想要预测的三种货币的模型吗？ 当使用 a 模型（如 lstm）时，我应该使用这 3 种货币的数据吗？如果是这样，我是否应该对货币名称使用独热编码并按日期组织数据？或者我应该为每种货币建立单独的模型？  数据集可以在这里找到 https://www.kaggle.com/datasets/paultimothymooney/the-economists-big-mac-index 任何帮助都非常感谢 :) 谢谢    提交人    /u/ineedaclockmaker   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhwbv3/p_need_help_understanding_how_to_use_rnn_lstm_for/</guid>
      <pubDate>Mon, 17 Jun 2024 12:00:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何将计算机视觉 DL 模型投入生产</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhvmgp/d_how_to_productionize_computer_vision_dl_models/</link>
      <description><![CDATA[我正在准备面试，很想看看我的高级推理和生产计算机视觉 DL 模型的流程是否正确。我很想得到一些反馈，并了解您在生产 DL 模型方面的经验。 由于我的主要经验是在 PyTorch，我将总结以下过程  如果您没有使用量化感知训练的训练模型 根据模型使用静态或动态量化。静态量化将提供更好的加速，因为与动态量化不同，激活是在 int8 中读取和写入内存的。 评估量化模型  转换为 Torchscript 模型 - 这使得模型可移植到非 Python 运行时 可能需要更改模型定义以使其与 Torchscript 语言兼容 - 不支持大多数非 pytorch 类型/函数，编译时应使用更好的类型 如果模型中没有条件流，您还可以跟踪模型  使用 Tensorrt 进一步优化 它将执行内核融合、精度校准、内核自动调整（是否使用 torch graph ops 或 tensorrt graph ops）等。 提供包含模型定义和权重的单个文件  为模型提供服务 您可以使用任何服务框架torchserve、triton 等。 确定批量大小、批处理的等待时间、每个模型的副本等参数，以优化延迟和吞吐量 确定预处理和后处理是否可以在服务内部或客户端进行，并相应地实施   根据我的经验，我想这将是我将 DL 模型投入生产的过程。我知道不同的云提供商还有许多其他方法，并希望从您的经验中获得见解。    提交人    /u/weiderthanyou   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhvmgp/d_how_to_productionize_computer_vision_dl_models/</guid>
      <pubDate>Mon, 17 Jun 2024 11:18:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何为旧的开源项目安装旧版本的 TF</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhu7ue/d_how_do_you_install_previous_versions_of_tf_for/</link>
      <description><![CDATA[      https://preview.redd.it/y1hxpdgzr37d1.png?width=1736&amp;format=png&amp;auto=webp&amp;s=1ec21af1a4a1cb97d1c7907a5eb2e3aaa7e5a54c https://preview.redd.it/w1p6tch0s37d1.png?width=1281&amp;format=png&amp;auto=webp&amp;s=1ecb889ddfd9c94dd6413cfb7edf3aa4ed57d608 嘿， 我已在上面附加了两张屏幕截图。一个显示我想要克隆和使用的这个开源项目与 2.x 不兼容，另一个显示 PIP 上的所有 TF 存储库都以 2.x 开头  你们是怎么处理的？有没有可靠的第三方存储所有以前的版本？    提交人    /u/No_Weakness_6058   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhu7ue/d_how_do_you_install_previous_versions_of_tf_for/</guid>
      <pubDate>Mon, 17 Jun 2024 09:44:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] OCR、推荐系统和规范分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhu1xw/d_ocr_recommendation_system_and_prescriptive/</link>
      <description><![CDATA[我可以在一个应用程序中结合使用 OCR（图像转文本）来捕获测试文档并转换为基于文本的推荐系统（用于推荐分析后要执行的活动）和规范分析（推荐下一个测试）吗？    提交人    /u/capricornhera   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhu1xw/d_ocr_recommendation_system_and_prescriptive/</guid>
      <pubDate>Mon, 17 Jun 2024 09:32:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过蒙特卡洛树访问 GPT-4 级数学奥林匹克解决方案，使用 LLaMa-3 8B 进行自我优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhrfjk/r_accessing_gpt4_level_mathematical_olympiad/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhrfjk/r_accessing_gpt4_level_mathematical_olympiad/</guid>
      <pubDate>Mon, 17 Jun 2024 06:24:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] AlphaMath Almost Zero：无需流程的流程监督</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhr5sy/r_alphamath_almost_zero_process_supervision/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhr5sy/r_alphamath_almost_zero_process_supervision/</guid>
      <pubDate>Mon, 17 Jun 2024 06:04:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] 创造力已不再是话题：消除语言模型偏见的代价</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhqs9g/r_creativity_has_left_the_chat_the_price_of/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhqs9g/r_creativity_has_left_the_chat_the_price_of/</guid>
      <pubDate>Mon, 17 Jun 2024 05:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 练习中注意力下降</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhq23d/d_attention_sinks_in_practice/</link>
      <description><![CDATA[SOTA 开源模型中是否有使用“注意力接收器”的例子（使用可学习的注意力接收器进行预训练，并在推理期间默认使用）？这似乎是一种直观的改进，但除了这些论文之外，我还没有听说过关于这个想法的任何消息。 具有注意力接收器的高效流式语言模型 视觉变压器需要寄存器    提交人    /u/Realistic-Row-8098   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhq23d/d_attention_sinks_in_practice/</guid>
      <pubDate>Mon, 17 Jun 2024 04:51:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年弱监督学习的进展如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dho29p/d_what_is_the_progress_on_weakly_supervised/</link>
      <description><![CDATA[弱监督学习中的一个热门话题是标签噪音。LLM 会受到标签噪音的影响吗？或者 LLM 可以解决标签噪音的问题吗？你认为这项研究仍然值得吗？    提交人    /u/EducationalOwl6246   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dho29p/d_what_is_the_progress_on_weakly_supervised/</guid>
      <pubDate>Mon, 17 Jun 2024 02:51:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​的混合精度训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/</link>
      <description><![CDATA[我在 2 层 MLP 上重新实现了 Nvidia 的原始混合精度训练论文（https://arxiv.org/abs/1710.03740）。我一直深入到 CUDA 领域来展示 TensorCore 激活，在我看来，这是混合精度训练的真正秘密。 代码：https://github.com/tspeterkim/mixed-precision-from-scratch 撰写：https://tspeterkim.github.io/posts/mixed-precision-from-scratch    提交人    /u/droidarmy95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/</guid>
      <pubDate>Mon, 17 Jun 2024 00:28:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] ECAI 2024 评论讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9n4e/d_ecai_2024_reviews_discussion/</link>
      <description><![CDATA[ECAI 2024 评论的讨论主题。 评论现已发布！    由   提交  /u/Fun_Equal5145   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9n4e/d_ecai_2024_reviews_discussion/</guid>
      <pubDate>Sun, 16 Jun 2024 15:10:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​实现指令微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh7cmv/p_instruction_finetuning_from_scratch/</link>
      <description><![CDATA[        提交人    /u/seraschka   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh7cmv/p_instruction_finetuning_from_scratch/</guid>
      <pubDate>Sun, 16 Jun 2024 13:14:43 GMT</pubDate>
    </item>
    <item>
      <title>[P] 减少倾斜损失的一种有趣方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh2aqt/p_an_interesting_way_to_minimize_tilted_losses/</link>
      <description><![CDATA[前段时间，我读了一篇关于所谓的倾斜经验风险最小化的论文，后来又读了同一位作者的一篇 JMLR 论文：https://www.jmlr.org/papers/v24/21-1095.html 这样的公式让我们能够以更“公平”的方式对困难样本进行训练，或者相反，如果这些困难样本实际上是异常值，则对这些困难样本的敏感度会降低。但最小化它在数字上具有挑战性。所以我决定尝试在博客文章中设计一种补救措施。我认为这是一个有趣的技巧，在这里很有用，我希望你也会发现它很好： https://alexshtf.github.io/2024/06/14/Untilting.html    提交人    /u/alexsht1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh2aqt/p_an_interesting_way_to_minimize_tilted_losses/</guid>
      <pubDate>Sun, 16 Jun 2024 07:27:02 GMT</pubDate>
    </item>
    </channel>
</rss>