<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Tue, 19 Mar 2024 00:56:44 GMT</lastBuildDate>
    <item>
      <title>[D] Nvidia GTC24 的 GPT4 参数计数与我们从 Semianalysis 获得的泄漏相同</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bi16pg/d_same_param_count_for_gpt4_from_nvidia_gtc24_as/</link>
      <description><![CDATA[   A Semianalysis 早前的报告称 GPT-4 是一个 1.8T 参数的 MoE 模型，有 16 位专家，每个有 111B 个参数。这是 GTC 会议的屏幕截图，具有相同的数字。 https://preview.redd.it/vyzfx2sel5pc1.png?width=1764&amp;format=png&amp;auto=webp&amp;s=dfce1d55c84dbc3c51e69f376161c47958f9cf 70   由   提交 /u/takuonline   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bi16pg/d_same_param_count_for_gpt4_from_nvidia_gtc24_as/</guid>
      <pubDate>Mon, 18 Mar 2024 20:36:19 GMT</pubDate>
    </item>
    <item>
      <title>Stability AI 发布 SV3D [N]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bi16eh/stability_ai_releases_sv3d_n/</link>
      <description><![CDATA[https://stability.ai /news/introducing-stable-video-3d  SV3D 将单个对象图像作为输入并输出该对象的新颖的多视图。然后我们可以使用这些新颖的视图和 SV3D 来生成 3D 网格。    由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bi16eh/stability_ai_releases_sv3d_n/</guid>
      <pubDate>Mon, 18 Mar 2024 20:35:58 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 介绍 ocrtoolkit：您的首选 OCR 软件包</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bi0li0/project_introducing_ocrtoolkit_your_goto_ocr/</link>
      <description><![CDATA[当然，让我们直接从 Hey Reddit 社区的 GitHub 存储库集成信息！ 我很高兴能够介绍 ocrtoolkit，一个功能强大的 OCR 软件包，旨在简化您的工作流程并提升您的 OCR 任务！ 我的项目的用途 如果您发现自己在应对 OCR 相关挑战的同时还要处理复杂的样板代码，那么您很幸运。 ocrtoolkit 简化了整个 OCR 流程，为图像文件处理、模型执行、结果解析等任务提供直观的包装器。让我们深入研究核心功能：  数据集模块：需要轻松加载图像文件或目录？ ocrtoolkit.datasets 模块就是您的最佳选择。 模型模块：与 paddleOCR 等流行的 OCR 框架无缝集成，通过 ocrtoolkit.models 模块实现 ultralytics 和 doctr。利用 ultralytics 的复杂对象检测模型在运行 OCR 之前查明感兴趣区域。 包装器模块：利用包装器进行对象检测、单词检测和识别借助 ocrtoolkit.wrappers 模块，轻松获得识别结果。此独立模块可确保通过 pip install ocrtoolkit 快速安装。 实用程序模块：访问大量实用程序来执行单词到行合并、使用 ocrtoolkit.utilities 模块进行几何操作、文件 I/O 等。  目标受众 无论您是研究人员开始进行 OCR 相关项目的开发人员或数据科学家，ocrtoolkit 可以满足您的需求。该软件包是您简化工作流程、试验不同模型和框架以及简化推理过程的首选解决方案。 比较 让我们讨论一下 ocrtoolkit 与现有替代方案不同：  全面支持：与仅专注于推理的软件包不同，ocrtoolkit 为无数OCR 相关任务，从后处理推理结果到保存/加载和轻松可视化。 无缝集成：体验与流行的 OCR 和对象检测框架的无缝集成，促进轻松实验 用户友好的设计：在设计时考虑到了易用性，ocrtoolkit 确保快速设置和配置，使用户能够深入了解轻松完成 OCR 任务。  ocrtoolkit 不适合做什么  训练模型：ocrtoolkit不是为训练新的 OCR 模型而设计的。相反，它的主要重点在于利用预训练或微调的模型进行推理。 高性能应用程序：虽然 ocrtoolkit 拥有在生产中的成功使用环境中，它可能不是需要最大性能优化的应用程序的理想选择。  其他资源 探索全面的文档，并在其 PyPi 页面上了解有关 ocrtoolkit 的更多信息。深入研究存储库中的 notebooks 文件夹，获取富有洞察力的示例，并毫不犹豫地分享您的反馈和建议！ 感谢您的宝贵时间，我热切等待您的宝贵意见见解！ ^_^   由   提交/u/ajkdrag_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bi0li0/project_introducing_ocrtoolkit_your_goto_ocr/</guid>
      <pubDate>Mon, 18 Mar 2024 20:13:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 扩散模型因自我调节而爆炸</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhtyvr/d_diffusion_model_blows_up_with_self_conditioning/</link>
      <description><![CDATA[我正在训练 用于基因组数据集的 lucidrain 1d DDPM。如果我使用自我调节（模型可以看到自己之前的样本），那么在几百个批次之后我的损失就会增加到无穷大。如果没有自我调节，它似乎表现得很好。好奇是否还有其他人看过这个？ Analog Bits 论文介绍了自我调节，并声称它很有帮助。显然，我可以降低学习率，但不确定这会有多大帮助，因为训练的爆发时间稍晚一些。    由   提交/u/daking999  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhtyvr/d_diffusion_model_blows_up_with_self_conditioning/</guid>
      <pubDate>Mon, 18 Mar 2024 15:48:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] IJCAI'24反驳讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhspbk/d_ijcai24_rebuttal_discussion/</link>
      <description><![CDATA[大家好， 随着评论即将发布，我发起此线程来分享想法、问题和关于 IJCAI 提交的建议。 祝大家好运！   由   提交 /u/Acceptable_Pop1461   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhspbk/d_ijcai24_rebuttal_discussion/</guid>
      <pubDate>Mon, 18 Mar 2024 14:55:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 仅在 cifar10 上评​​估的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhp54a/d_papers_that_only_evaluate_on_cifar10/</link>
      <description><![CDATA[大家好！ 在审阅 NeurIPS 2024 时，我一直注意到的一件事是，很多论文只评估非常小的数据集，例如 Cifar-10。这对我来说很奇怪：我认为 Cifar10 是我的方法的玩具数据集和测试平台，而不是我用来证明我的方法实际上有效/在实践中相关的东西。所以我的第一直觉始终是“这种方法可能无法扩展到更大的数据集”。我的意思是，ImageNet 已经有 12 岁了，我个人从大约 8 年前就开始在 imagenet 上为我的论文提供结果。据我所知，大多数计算机视觉应用都需要比 32x32 更高的分辨率。我的印象也是，几乎所有“好”的东西都是我读过的论文有更大规模数据的结果。但考虑到我经常遇到这种情况，我不得不想：我只是在一个非常优越的环境中工作，还是稿件作者只是懒惰？您对仅评估 MNIST 和 CIFAR10 的论文有多少信心？   由   提交 /u/audiencevote   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhp54a/d_papers_that_only_evaluate_on_cifar10/</guid>
      <pubDate>Mon, 18 Mar 2024 12:08:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于变压器的理论论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhog2n/d_theoretical_paper_about_transformers/</link>
      <description><![CDATA[我将成立一个专注于大型语言模型的研究小组。参与者是具有数学背景的计算机科学博士生。我想首先研究变压器（或注意力）的一些理论特性。可能有的同学还不太清楚Transformer具体是怎么公式化的，所以我也需要讨论一下。 您有什么关于Transformer理论分析的论文推荐（注意）吗？  最流行的注意力论文是“Attention is all you need”，但他们只介绍了架构并运行实验。  &amp; #32；由   提交/u/Massive_Horror9038   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhog2n/d_theoretical_paper_about_transformers/</guid>
      <pubDate>Mon, 18 Mar 2024 11:29:08 GMT</pubDate>
    </item>
    <item>
      <title>2024 年哪个库最适合时间序列预测和异常检测？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bho0r0/in_2024_which_library_is_best_for_time_series/</link>
      <description><![CDATA[我正在开发一个项目，负责识别时间序列数据中的异常情况。我遇到了 Facebook Prophet，但遗憾的是它自 2023 年以来就不再维护了。他们建议 NeuroProphet、nixtla 作为替代方案。在寻找替代方案时，我发现了来自 Facebook 的 Kats，它内置了先知支持。这里有哪些工具/库经验丰富的成员会推荐哪些工具/库来构建用于对大量时间序列数据进行异常检测的生产级系统？   由   提交 /u/ThakkidiMundan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bho0r0/in_2024_which_library_is_best_for_time_series/</guid>
      <pubDate>Mon, 18 Mar 2024 11:03:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当你使用人工智能进行总结时结果不正确时。爱思唯尔发表的研究论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhn918/d_when_your_use_of_ai_for_summary_didnt_come_out/</link>
      <description><![CDATA[       由   提交 /u/vvkuka   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhn918/d_when_your_use_of_ai_for_summary_didnt_come_out/</guid>
      <pubDate>Mon, 18 Mar 2024 10:14:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] Hugging Face 演示应用程序：表格数据的可解释生成人工智能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhlajr/p_hugging_face_demo_app_explainable_generative_ai/</link>
      <description><![CDATA[大家好， 我邀请您尝试我们在表格数据的可解释和生成人工智能方面的工作的拥抱脸演示应用程序。 https://huggingface.co/spaces/CaglarAytekin/LEURN 简单来说，该应用程序采用包含特征和目标的训练表，让您选择要预测的内容并选择分类列。然后您可以使用选定的超参数训练神经网络。训练完成后，您可以上传测试数据（只有特征，没有目标的 Excel 或 csv），然后选择一行来运行神经网络并解释决策。您还可以根据神经网络从训练中学到的知识无缝地生成新数据。可以从上述应用程序中找到清晰的使用说明。 相关研究论文： https://arxiv.org/abs/2303.14937 开源代码： https://github.com/CaglarAytekin/LEURN/ 联系人： caglar@deepcause.ai ​   由   提交 /u/MLC_Money   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhlajr/p_hugging_face_demo_app_explainable_generative_ai/</guid>
      <pubDate>Mon, 18 Mar 2024 07:51:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] xAI 的 Qdrant...为什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhhyrv/d_xais_qdrantwhy/</link>
      <description><![CDATA[也许我只是没有花时间去理解它，但我很难理解 Qdrant 为何比 OpenSearch/ElasticSearch 更好？ OS/ES 都使用 HNSW，并且它们都使用相同的 KNN oss 实现，性能非常好。 Qdrant 有什么“开箱即用”的功能？这些现有的且广泛采用的选项没有？   由   提交/u/titani0us  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhhyrv/d_xais_qdrantwhy/</guid>
      <pubDate>Mon, 18 Mar 2024 04:11:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何准备 META 研究工程师面试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bhgxar/d_how_to_prepare_for_a_meta_research_engineer/</link>
      <description><![CDATA[我将在一周内进行 META 研究工程师面试。这个职位本身是机器学习和计算机视觉领域的，但我希望在面试中会被问到 leetcode 风格的问题。我想知道是否有人可以给我一些关于学习/复习内容的建议，因为只剩下一周了。   由   提交 /u/Tiny-Masterpiece-412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bhgxar/d_how_to_prepare_for_a_meta_research_engineer/</guid>
      <pubDate>Mon, 18 Mar 2024 03:16:01 GMT</pubDate>
    </item>
    <item>
      <title>xAI 发布 Grok-1 [N]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh7yc4/xai_releases_grok1_n/</link>
      <description><![CDATA[ 我们正在发布大型语言模型 Grok-1 的基本模型权重和网络架构。 Grok-1 是一个由 xAI 从头开始​​训练的 3140 亿参数混合专家模型。 这是 Grok-1 预训练阶段的原始基础模型检查点，该阶段于 2023 年 10 月结束。这意味着该模型没有针对任何特定应用程序（例如对话）进行微调。 我们将在 Apache 2.0 许可证下发布权重和架构。 开始使用使用该模型时，请按照 https://github.com/xai-org/grok 中的说明进行操作   由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh7yc4/xai_releases_grok1_n/</guid>
      <pubDate>Sun, 17 Mar 2024 20:38:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] Paperlib：一个开源且现代设计的学术论文管理工具。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bh63c1/p_paperlib_an_opensource_and_moderndesigned/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bh63c1/p_paperlib_an_opensource_and_moderndesigned/</guid>
      <pubDate>Sun, 17 Mar 2024 19:24:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>