<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Mon, 24 Feb 2025 09:19:09 GMT</lastBuildDate>
    <item>
      <title>[r]通过强化学习和结构化推理，培训LLMS严格的JSON模式遵守</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwxtmb/r_training_llms_for_strict_json_schema_adherence/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  使LLMS输出有效JSON的一种新方法将强化学习与模式验证奖励相结合。关键见解是使用架构本身作为训练信号，而不是需要大量的示例数据集。 主要技术点：•奖励模型体系结构验证JSON结构和模式合规性在培训期间实时实时•使用深度强化学习来帮助模型内部化格式化规则•除了模式规格之外，无需其他培训数据•跨不同模型体系结构（对GPT变体和Llama模型进行了测试）•实施在推理期间增加了最小的计算开销 结果：•98.7％有效的JSON输出率（高于82.3％的基线）•架构验证错误的降低47％没有明显降级的功能 我认为这种方法可以使LLMS对于结构化数据输出至关重要的现实世界应用程序更可靠。在没有大量培训数据的情况下执行模式合规的能力对于部署方案特别有价值。 我认为，这里的真正创新是将架构本身用作培训信号。这比试图策划有效示例的大量数据集更优雅的解决方案。当前的结果侧重于相对直接的JSON结构。  tldr：新的强化学习方法使用模式验证作为奖励，以训练LLMS以98.7％的精度输出有效的JSON，而无需其他培训数据。    完整的摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwxtmb/r_training_llms_for_strict_json_schema_adherence/</guid>
      <pubDate>Mon, 24 Feb 2025 09:03:44 GMT</pubDate>
    </item>
    <item>
      <title>[D]相关数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwmeqj/d_correlation_data/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在研究数据库时有一个问题。当我们具有分类功能并且需要分析这些数据与标签的相关性时，最佳应用是什么？我相信应用onehotencoder不会有效。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/henryjks     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwmeqj/d_correlation_data/</guid>
      <pubDate>Sun, 23 Feb 2025 22:26:18 GMT</pubDate>
    </item>
    <item>
      <title>CVPR 2025最终评论！ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwgwxi/cvpr_2025_final_reviews_d/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们什么时候可以期望发布最终评论？我是首次作家，热切地等待最终的评论和决定。我很想知道在做出决定之前是否发布了最终评论。有人可以解释这个过程吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/no/no-project-1260     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwgwxi/cvpr_2025_final_reviews_d/</guid>
      <pubDate>Sun, 23 Feb 2025 18:33:00 GMT</pubDate>
    </item>
    <item>
      <title>[R]优化化合物AI系统的模型选择</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwfnbq/r_optimizing_model_selection_for_compound_ai/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwfnbq/r_optimizing_model_selection_for_compound_ai/</guid>
      <pubDate>Sun, 23 Feb 2025 17:40:00 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwdbgs/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子的人，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。 感谢大家回答问题在上一个线程中！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iwdbgs/d_simple_questions_thread/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwdbgs/d_simple_questions_thread/</guid>
      <pubDate>Sun, 23 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[R]数据漂移/离群值检测的文本语料库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iw9l1c/r_data_driftoutlier_detection_for_a_corpus_of_text/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在研究一种方法来测量我们的文本语料库中的数据漂移，以动态调整我们的机器学习模型参数。具体而言，我们旨在平衡模型进气门的每个主题的元素数量。 为了解决这个问题，我最初使用 bertopic 将文本用于主题。但是，我遇到了一个挑战：一旦培训了蠕虫模型，由于其对 umap和dbscan的依赖，因此不允许增加新元素，这完全有意义。 现在，我正在寻找替代方法来连续跟踪主题/异常分布随着新数据的进来。您如何解决此问题，或者您建议哪些策略？ 洞察力或经验将不胜感激！ 谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iw9l1c/r_data_driftliftlier_detection_for_a_corpus_of_text/  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iw9l1c/r_data_driftliertlier_detection_for_a_corpus_corpus_of_text/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iw9l1c/r_data_driftoutlier_detection_for_a_corpus_of_text/</guid>
      <pubDate>Sun, 23 Feb 2025 12:58:50 GMT</pubDate>
    </item>
    <item>
      <title>[P]更新：使用Langchain和Langgraph的DeepSeek-R1 671B调用工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iw98pg/p_update_tool_calling_with_deepseekr1_671b_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在上周发布了有关我上周在用langchain和langgraph的deepseek-r1 671b上创建的github存储库，或更一般的LLMS Langchain的Chatopenai类（对于新发布的LLMS特别有用，Langchain和Langgraph尚未支持工具）。   https://github.com/leockl/tool-ahead-oftime   此存储库刚刚进行了升级。新功能： - 现在可以在PYPI上使用！公正的“ PIP安装Taot”然后你准备就绪了！ - 完全重新设计，以遵循Langchain和Langgraph的直观工具呼叫模式。 - 执行工具调用时的自然语言响应。 如果有帮助，请在我的存储库中给我一颗星星。享受！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lc19-     &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iw98pg/p_update_tool_calling_with_with_deepseekr1_671b_with/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iw98pg/p_update_tool_calling_with_deepseekr1_671b_with/</guid>
      <pubDate>Sun, 23 Feb 2025 12:38:46 GMT</pubDate>
    </item>
    <item>
      <title>[p]毫无用处：毫不费力地将YouTube播放列表重新用于有用的东西。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iw6t4f/p_scribly_effortlessly_repurposing_youtube/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在当前的大脑旋转场景中，没有人耐心坐着看长视频，所以对于当前的一代将您的YouTube播放列表重新利用为清晰信息的源工具，节省了时间和精力。您必须跟上进度，不是吗？&lt; /p&gt;   https://github.com/justsujay/scrible     &lt;！ -  sc_on-&gt; sc_on-&gt;＆＃32 ;提交由＆＃32; /u/u/expect_tap_4002     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iw6t4f/p_scribly_effortlessly_repurposing_youtube/</guid>
      <pubDate>Sun, 23 Feb 2025 09:57:11 GMT</pubDate>
    </item>
    <item>
      <title>[p]从视觉上请参阅学术论文的想法发展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iw5lgj/p_see_the_idea_development_of_academic_papers/</link>
      <description><![CDATA[      在这里尝试： https://arxiv-viz.ianhsiao.xyz/      &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iw5lgj/p_see_the_idea_idea_development_of_academic_papers/”&gt; [link]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iw5lgj/p_see_the_idea_development_of_academic_papers/</guid>
      <pubDate>Sun, 23 Feb 2025 08:30:10 GMT</pubDate>
    </item>
    <item>
      <title>[r]相关性引导的参数优化，以在扩散变压器中有效控制</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iw46oq/r_relevanceguided_parameter_optimization_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键技术贡献是一种相关性引导的体系结构，通过基于区域重要性选择性分配处理能力，使扩散变形金刚在计算上更加有效地计算。它将DIT（扩散变压器）与ControlNET方法结合在一起，同时引入相关性先验机制。 主要技术点： - 引入了两个阶段的相关性评估系统：轻量级网络评估区域的重要性，随后是自适应计算分配 - 通过模块化设计与现有扩散管道集成 - 相关性指南变压器注意机制与标准扩散变压器兼容体系结构 关键结果：-30-50％的计算开销降低 - 与基线​​相比，保持或改善了图像质量 - 对生成内容的更精确控制 - 有效处理复杂场景  i认为这可能会对使高质量的图像生成更加易于访问产生有意义的影响，尤其是对于资源受限的应用程序。对于计算效率至关重要的部署方案，这种方法似乎特别有希望。 我认为相关性引导的方法可以超越图像生成 - 基于重要性的选择性计算的核心思想可以使其他变压器应用受益，而注意机制在计算上是昂贵的。  tldr：通过将计算资源集中在重要图像区域，通过将计算资源集中在重要图像区域，通过减少计算需求，从保持质量。  完整摘要是在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iw46oq/r_relevanceguided_parameter_optimization_for/</guid>
      <pubDate>Sun, 23 Feb 2025 06:50:56 GMT</pubDate>
    </item>
    <item>
      <title>[d] API平台与扩散模型的自我剥离</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iw2kbl/d_api_platforms_vs_selfdeployment_for_diffusion/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我编写了一份指南，有关如何选择正确类型的云基础架构，如果您是在扩散模型之上构建的： https://modal.com/blog/diffusion-model-infra   警告模式是一个无服务器的计算平台！但是，这篇文章涵盖了您可以在API平台（Replicate，FAL），传统云（AWS EC2），托管ML平台（SageMaker，Vertex）和无服务器云之间进行选择的时间。 我经常看到公司跳到即使他们只是使用带有几个适配器的现成模型，也可以进行自我部署。我认为，从成本或努力的角度来看，这很少有意义，除非您有大量的生产流量使这些事情摊开。转向自我部署的最令人信服的理由是，如果您需要对生成的输入=＆gt的高度控制。这需要微调的重量 /客户适配器 /多步生成管道=＆gt;这需要对部署的代码级控制。 您同意/不同意？如果您以前曾评估过这些类别的提供商，请告诉我它们如何相互堆叠。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/crookedstairs   href =“ https://www.reddit.com/r/machinelearning/comments/1IW2KBL/D_API_PLATFORMS_VS_VS_ERSEDDEPLOYMENT_FOR_DIFFUSION/”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iw2kbl/d_api_platforms_vs_selfdeployment_for_diffusion/</guid>
      <pubDate>Sun, 23 Feb 2025 05:06:59 GMT</pubDate>
    </item>
    <item>
      <title>[P]在Edge（iPhone），核心ML工具上运行ML模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ivrlu8/p_run_ml_models_on_edge_iphone_core_ml_tools/</link>
      <description><![CDATA[在   我正在尝试遵循上面的指南。 我一直在尝试编译一些型号，这是一场噩梦。感觉就像这些示例是高度人为的，因为我无法导出任何我想使用的模型。我一直在下面遇到这样的问题。  当未指定的&#39;convert__to&#39;和&#39;minimum_deployment_target&#39;均未指定&#39;convert_to&#39;时，将其设置为“ mlprogram”。和“ minimum_deployment_target”设置为ct.target.ios15（与ct.target.macos12相同）。注意：该模型不会在比IOS15/MACOS12/WatchOS8/TVOS15的系统上运行。为了使您的模型在较旧的系统上运行，请将“ Minimum_deployment_target”设置为ios14/ios13。详细信息请参阅链接：   https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html    在图形输出时检测到的元组。这将在转换的模型中扁平。&lt; /code&gt;  转换pytorch frontend ==＆gt; MIL OPS：0％| | 0/253 [00：00＆lt;？，？ ops/s]     错误 - 转换&#39;mul&#39;op（位于：&#39;366&#39;）：     转换Pytorch frontend ==＆gt; MIL OPS：94％|█████████▍| 238/253 [00：00＆lt; 00：00，7431.73 OPS/s]   所以，真正的问题：人们打算如何运行本地LLM，计算机视觉或任何新模型在iPhone上？我对在任何地方托管这些型号没有兴趣，我只希望它们在iPhone上运行（不，谢谢，我没有Android来原型）。 在我受到指责之前这些模型太大，细，很好，但是可以优化（量化，修剪等），无法使它们以可接受的速度运行。但是，如果我什至无法将它们导出到Apple格式中，我将永远无法优化它们。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/milong0     [links]      &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1ivrlu8/1ivrlu8/p_run_ml_models_onded_edge_iphone_core_core_ml_ml_tools/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ivrlu8/p_run_ml_models_on_edge_iphone_core_ml_tools/</guid>
      <pubDate>Sat, 22 Feb 2025 20:01:03 GMT</pubDate>
    </item>
    <item>
      <title>[r]解释深度神经网络：记忆，内核，最近的邻居和注意力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ivnp1c/r_interpreting_deep_neural_networks_memorization/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/thienpro123     [link] 32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ivnp1c/r_interpreting_deep_neural_networks_memorization/</guid>
      <pubDate>Sat, 22 Feb 2025 17:15:49 GMT</pubDate>
    </item>
    <item>
      <title>[R]计算微调视觉语言模型的成本</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，我需要帮助计算微调VL模型的成本。我的图像数据集是尺寸80+GB（ https://huggingface.co/datasets/russrobin/spatialqa ） &gt;我对是否执行完整参数 /Qlora Finetuning感到困惑。我不能在此上花费更多，但希望检查结果。&lt; /p&gt; 如果可以的话，我可以估算的成本估算，以及如何估算一般我可以采样数据集，如果打破了我的成本约束，仍然看到结果吗？也建议我的情况最好，最便宜的计算平台。 &gt;＆＃32;提交由＆＃32; /u/u/thekarthikprasad    href =“ https://www.reddit.com/r/machinelearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/</guid>
      <pubDate>Sat, 22 Feb 2025 14:21:21 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子的人，以便在此处发布问题！ 线程将活着直到下一个，所以请继续发布标题的日期。 感谢大家回答问题在上一个线程中！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1ilhw29/d_simple_questions_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ilhw29/d_simple_questions_thread/</guid>
      <pubDate>Sun, 09 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    </channel>
</rss>