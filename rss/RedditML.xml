<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 23 May 2024 18:18:07 GMT</lastBuildDate>
    <item>
      <title>[D][R] 如果你可以选择 3 篇关于视频/图像生成模型的论文，你会选择哪一篇？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyy7v9/dr_if_you_could_pick_3_papers_about_videoimage/</link>
      <description><![CDATA[我正在攻读硕士学位，并且我选择做一个视频生成项目。我读过一些关于图像和视频合成的论文：  VQGAN Stable Diffusion Imagen  我还挑选了 3 篇视频生成论文：  Video-LDM Stable Video Diffusion Fine-tuned for Multi-View Generation (SVD-MV) &lt; li&gt;Text2Video-Zero  我还阅读了一些调查论文，这些是我选择谈论的模型。 我正在努力解决的是为了选择逻辑上有序的论文，所以首先我解释一下 3 个图像生成论文，视频生成论文应该遵循图像合成论文中提到的相同策略。 我可以请你建议不同的一组论文要写什么？我仍然可以将所有论文更改为其他内容。 大多数是最近的内容（2020-2024 年很好）并且具有一些重大影响。我知道例如 VQGAN 是流行的基础模型，论文中使用的技术和策略今天仍然相关。 Imagen（由 Google 提供）但是不是开源的，我更喜欢具有开源代码的论文。这就是我避免 OpenAI 论文的原因。 我还读到，在视频生成中选择扩散而不是 GAN，因为它在质量和训练方面都有更好的结果。然而，扩散的计算成本更高。 例如，Video-LDM 基于稳定扩散，因此对我来说，这是值得讨论的好论文。 &lt;!-- SC_ON - -&gt;  由   提交 /u/ShlomiRex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyy7v9/dr_if_you_could_pick_3_papers_about_videoimage/</guid>
      <pubDate>Thu, 23 May 2024 17:18:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 图像分类中的融合特征向量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyve1w/r_fuse_feature_vector_in_image_classification/</link>
      <description><![CDATA[大家好， 目前，我正在处理一个关于面部情绪分类的图像分类问题。我使用了两种提取方法：HOG 和面部关键点。我的想法是使用 HOG 来查找图像的梯度幅度和方向，并使用面部关键点来查找面部关键点。我想我可以融合这两种方法来制作一个更好的特征。但新的特征比 HOG 差而比面部关键点好（评估的模型相同）。我有一些问题：  我想知道如何融合这两种方法，其中 HOG 之前标准化，面部关键点返回 68x2 对点整数。 如果可以，我应该在融合之前进行标准化还是做些什么？我可以尝试哪种方法来融合它们（连接、添加、乘以...）？ 有什么方法可以衡量我的方法是否更好或评估它？我也尝试融合 HOG 和 SIFT（视觉词袋）。  我曾尝试融合 HOG 和 Facial Landmark 特征，但在同一模型中，它比 HOG 差，比 Facial Landmark 好。我也融合了（SIFT）视觉词袋和 HOG，但它仍然比 HOG 差，比视觉词袋好。这是我使用的代码： x_hogp_train = pca.transform(x_hog_train)[:,:382] x_hogp_valid = pca.transform(x_hog_valid)[:,:382] x_hogp_test = pca.transform(x_hog_test)[:,:382] scaler = StandardScaler() # 缩放 bovw 特征 scaler.fit(x_bovw_train) x_scale_bovw_train = scaler.transform(x_bovw_train) x_scale_bovw_valid = scaler.transform(x_bovw_valid) x_scale_bovw_test = scaler.transform(x_bovw_test) # 使用 concat 融合它们 x_fused_train = np.concatenate((x_hogp_train, x_scale_bovw_train), axis=1) x_fused_valid = np.concatenate((x_hogp_valid, x_scale_bovw_valid), axis=1) x_fused_test = np.concatenate((x_hogp_test, x_scale_bovw_test), axis=1) 提前感谢     提交人    /u/Civil_Statement_9331   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyve1w/r_fuse_feature_vector_in_image_classification/</guid>
      <pubDate>Thu, 23 May 2024 15:21:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 变分推理：反向 KL 与正向 KL</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyubdt/r_variational_inference_reverse_kl_vs_forward_kl/</link>
      <description><![CDATA[大家好， 我正在研究变分推理方法，主要是在 BNN 的背景下。使用反向（独占）KL 作为变分目标是常见的方法，尽管最近我偶然发现了一些使用前向（包含）KL 作为目标的有趣作品，例如 [1][2][3]。此外，在 GP 的 VI 背景下，两种散度度量均已使用，请参阅 [4]。 虽然我熟悉反向 KL 目标之间众所周知的差异，但它是“模式-”寻求”而正向 KL 是“模式覆盖”，我看到其中一些作品对这些 VI 目标的下游差异提出了主张，例如（此处解释）“反向 KL 低估了预测方差” [4]和“前向 KL 对于受益于保守不确定性量化的应用很有用” [3]. 我有兴趣在 VI 的背景下理解这些下游差异，但还没有找到任何从理论上而不是从经验上解释这些主张的著作。任何人都可以为我指出正确的方向或尝试解释这一点？ 干杯 [1] Naesseth、Christian、Fredrik Lindsten 和 David Blei。 “马尔可夫分数攀爬：KL (p|| q) 的变分推理。” 神经信息处理系统的进展 33 (2020): 15499-15510。 [2] Zhang, L., Blei, D. M., &amp;奈塞斯，C.A.（2022）。传输分数攀登：使用前向 KL 和自适应神经传输进行变分推理。 arXiv 预印本 arXiv:2202.01841。 [3] McNamara, D., Loper, J., &amp; Regier, J.（2024 年 4 月）。用于摊余变分推理中的包容性 KL 最小化的顺序蒙特卡罗。 人工智能与统计国际会议（第 4312-4320 页）。 PMLR。 [4] Bauer, M.、Van der Wilk, M.、&amp;拉斯穆森，C.E.（2016）。了解概率稀疏高斯过程近似。 神经信息处理系统的进展，29。   由   提交/u/DriftingClient  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyubdt/r_variational_inference_reverse_kl_vs_forward_kl/</guid>
      <pubDate>Thu, 23 May 2024 14:36:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] Phi-3 模型并排比较。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cytxb5/d_phi3_models_compared_sidebyside/</link>
      <description><![CDATA[      https://preview.redd.it/8l04pnfhq62d1.png?width=661&amp;format=png&amp;auto=webp&amp;s=7fe616ca8cd7da97407 0c86b6b47ffab3ab545e5   https://preview.redd.it/hr7fr1uiq62d1.png?width=688&amp;format=png&amp;auto=webp&amp;s=bd3de359bfe4c1ed82d092be92ae38c246bdfda2    https://preview.redd.it/v6k3v39kq62d1.png？ width=450&amp;format=png&amp;auto=webp&amp;s=c0abb0e397a498ef7ccfb35b1b1cb598198f66ad 对于任何想要在一个地方比较 Phi-3 基准的人。 有趣的比较：ANLI、Hellaswag、MedQA、TriviaQA、语言理解、事实知识和稳健性。 注意：Phi-3 迷你模型表的标签顺序不同。   由   提交/u/dark_surfer  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cytxb5/d_phi3_models_compared_sidebyside/</guid>
      <pubDate>Thu, 23 May 2024 14:19:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在您自己的云（Azure/AWS/GCP）上部署 ML 模型时面临的最大挑战是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyrr8c/d_whats_the_biggest_challenge_you_face_when/</link>
      <description><![CDATA[您好，这是一篇市场研究文章，旨在了解人们在自己的云 (AWS) 上的生产环境中部署开源或自定义 ML 模型时所面临的挑战/Azure/GCP）。 选项：  部署复杂性（K8S、Knative、Ray 等） 根据用户需求自动扩展 缺乏 GPU 可用性（竞价型实例、配额限制） 设置 CI/CD     ;由   提交/u/Capital_Ad1552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyrr8c/d_whats_the_biggest_challenge_you_face_when/</guid>
      <pubDate>Thu, 23 May 2024 12:38:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于高负载计算机图像推理的更好的 API？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cypw7s/d_better_apis_for_highload_computer_image/</link>
      <description><![CDATA[大家好。 我们使用 BentoML 构建带有 CV 模型的 Docker 镜像。但问题是它使用 REST API，这并不是高负载 CV 推理的最佳选择。 还有其他开源解决方案可以为计算机视觉推理提供更快的 api 吗？我们是否有机会在 BentoML 的管道中实现它们？   由   提交 /u/wedazu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cypw7s/d_better_apis_for_highload_computer_image/</guid>
      <pubDate>Thu, 23 May 2024 10:50:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] ML 数据几何</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/</link>
      <description><![CDATA[        由   提交/u/Late-Yak9284  /u/Late-Yak9284 reddit.com/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyp308/r_geometry_of_data_for_ml/</guid>
      <pubDate>Thu, 23 May 2024 09:56:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] ReproModel：开源机器学习研究工具箱。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/</link>
      <description><![CDATA[嗨，我是计算机科学博士，我刚刚开发了我认为机器学习研究的一个巨大飞跃。我开源了该应用程序供大家查看，欢迎所有反馈和贡献。 ReproModel 是一个无代码工具箱，使科学家和研究人员能够有效地测试和重现 ML 模型。很大一部分研究时间都浪费在测试现有论文中的模型上。要复制或测试结果，您必须查看提供的代码，并模仿所有配置文件和实验条件，即数据加载器、预处理、优化器等。 工具箱将所有这些都拿走了通过从现有论文中获取配置文件（即将推出），直接加载模型，并通过简单的复选框和下拉菜单在数据上测试它们。当然，定制是可能的并且受到鼓励。 您可以在此处找到存储库。当然，还需要做更多的工作，但我正在逐步实现这一点，以确保代码未来的兼容性和可重用性。 https://github.com/ReproModel/repromodel 感谢您的时间、评论和支持！   由   提交 /u/MintOwlTech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cynq6x/p_repromodel_open_source_ml_research_toolbox/</guid>
      <pubDate>Thu, 23 May 2024 08:18:33 GMT</pubDate>
    </item>
    <item>
      <title>[项目] YOLOv8量化项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/</link>
      <description><![CDATA[我在 Jetson Orin Nano 中量化了 YOLOv8。我用TensorRT（FP16，INT8）导出它并比较性能。基于YOLOv8s，基础模型的mAP50-95为44.7，推理速度为33.1 ms。使用TensorRT（FP16）导出的模型显示mAP50-95为44.7，推理速度为11.4 ms。使用TensorRT（INT8）导出的模型显示mAP50-95为41.2，推理速度为8.2 ms。 mAP50-95略有损失，但推理速度大幅下降。用TensorRT（INT8）导出校准存在问题，但通过增加校准数据将mAP50-95的损失降到最低。我使用 YOLOv8 以及 YOLOv8s 的所有基本模型进行了测试。 https://github.com/ the0807/YOLOv8-ONNX-TensorRT   由   提交/u/Loud-Insect9247   reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cymc41/project_yolov8_quantization_project/</guid>
      <pubDate>Thu, 23 May 2024 06:39:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 杰夫·辛顿 (Geoff Hinton) 目前对于反向传播作为大脑学习机制有何看法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/</link>
      <description><![CDATA[我看了他大约 4 年前的一场讲座，在讲座中他驳斥了所有反对反向传播作为大脑学习机制的观点。但我记得最近在播客上听到过他（再也找不到了），在播客中他对反向传播持怀疑态度，似乎暗示赫布学习更重要。我很好奇他现在的信念以及原因。他最近在哪场采访或讲座中讨论过这个问题？ /u/geoffhinton 编辑：这是我指的讲座，名为“大脑会进行反向传播吗？”    提交人    /u/guesswho135   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyhk1f/d_what_are_geoff_hintons_current_thoughts_on/</guid>
      <pubDate>Thu, 23 May 2024 01:58:50 GMT</pubDate>
    </item>
    <item>
      <title>未能复制“深度残差学习”“[P]”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cydq8g/failing_to_replicate_deep_residual_learning_p/</link>
      <description><![CDATA[   大家好， 出于学习目的，我一直在复制Kaiming He 2015 年“图像识别的深度残差学习”中的方法。我已经构建了受 VGG 启发的 plain-CNN 以及 ResNet 架构（标准和瓶颈）。 但是，我无法复制出版物中强调的退化（准确性饱和）问题。出版物中的错误百分比数字显示，随着训练的进展，错误百分比明显下降，然后停滞不前。 我的数字似乎停滞不前，但很明显，该模型对验证数据的泛化能力非常糟糕。我附上了他们的一张图作为参考。有什么建议可以更好地复制本文中的错误率饱和度吗？注：对于何凯明图，粗线为测试误差&amp;虚线表示训练。 参数：  162 个 Epoch，批量大小为 128，进行 64k 次迭代。 Lr：0.1 &lt; li&gt;动量：0.9 权重衰减：0.0001 多步调度器在 32k 和 48k 迭代时将 lr 除以 10  我的%-错误 &lt; p&gt;来自论文   由   提交 /u/AnotherBotIGuess   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cydq8g/failing_to_replicate_deep_residual_learning_p/</guid>
      <pubDate>Wed, 22 May 2024 22:48:34 GMT</pubDate>
    </item>
    <item>
      <title>【研究】理解 Claude 3 Sonnet 中的稀疏自动编码器如何影响实际的 AI 应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cyc0zs/research_how_can_understanding_sparse/</link>
      <description><![CDATA[我最近读了论文“Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet”由人类。该研究探讨了稀疏自动编码器如何从 Transformer 模型中提取可解释的、多语言和多模态的特征。  https://transformer- Circuits.pub/2024/scaling-monosemanticity/index .html - 论文链接 鉴于这些功能会影响特定类型数据（如文本或图像）的检测和生成，我很好奇此功能的实际应用：  这种级别的特征理解如何帮助在不进行大量再训练的情况下为特定任务定制模型输出？例如，我们能否在基于已识别特征的部署过程中更有效地引导模型？这可以消除/识别/减轻偏见吗？    由   提交 /u/mamphii   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cyc0zs/research_how_can_understanding_sparse/</guid>
      <pubDate>Wed, 22 May 2024 21:35:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您希望在 /r/machinelearning 中看到更多或更少的内容？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy5ldu/d_what_would_you_like_to_see_more_or_less_of_in/</link>
      <description><![CDATA[我无法设置投票。但我想我应该加入这个社区。 首先，我希望原创研究人员能够发布他们的论文并回答问题。    ;由   提交/u/20231027  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy5ldu/d_what_would_you_like_to_see_more_or_less_of_in/</guid>
      <pubDate>Wed, 22 May 2024 17:12:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人工智能代理：太早、太昂贵、太不可靠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cy1kn9/d_ai_agents_too_early_too_expensive_too_unreliable/</guid>
      <pubDate>Wed, 22 May 2024 14:27:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>