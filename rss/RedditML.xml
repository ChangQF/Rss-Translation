<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sat, 13 Apr 2024 21:10:37 GMT</lastBuildDate>
    <item>
      <title>[R] Anthropic 的 Haiku 在工具使用方面击败了 GPT-4 Turbo</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c3beyr/r_anthropics_haiku_beats_gpt4_turbo_in_tool_use/</link>
      <description><![CDATA[       由   提交/u/jdogbro12  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c3beyr/r_anthropics_haiku_beats_gpt4_turbo_in_tool_use/</guid>
      <pubDate>Sat, 13 Apr 2024 20:32:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用于将安全开源代码解释器连接到任何 LLM 的 SDK</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c35pd0/p_sdk_for_connecting_secure_opensource_code/</link>
      <description><![CDATA[       由   提交/u/mlejva  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c35pd0/p_sdk_for_connecting_secure_opensource_code/</guid>
      <pubDate>Sat, 13 Apr 2024 16:20:22 GMT</pubDate>
    </item>
    <item>
      <title>RecurrentGemma：超越 Transformers 以实现高效的开放语言模型 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c35ig4/recurrentgemma_moving_past_transformers_for/</link>
      <description><![CDATA[ 由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c35ig4/recurrentgemma_moving_past_transformers_for/</guid>
      <pubDate>Sat, 13 Apr 2024 16:11:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 机器学习库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c354c9/p_a_library_for_machine_learning/</link>
      <description><![CDATA[大家好，这个机器学习库允许您像使用 PyTorch 一样构建可以使用 TensorFlow 进行训练的神经网络。 &lt; a href=&quot;https://github.com/NoteDance/Note&quot;&gt;https://github.com/NoteDance/Note  &amp;# 32；由   提交 /u/NoteDance   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c354c9/p_a_library_for_machine_learning/</guid>
      <pubDate>Sat, 13 Apr 2024 15:53:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用并行模拟退火的数独求解器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c33u19/p_sudoku_solver_using_parallel_simulated_annealing/</link>
      <description><![CDATA[   /u/Stunning_Ad_1539   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c33u19/p_sudoku_solver_using_parallel_simulated_annealing/</guid>
      <pubDate>Sat, 13 Apr 2024 14:55:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从现场录像中分割足球运动员 - 深入研究 UNET。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c33o2x/p_segmenting_footballers_from_live_footage_deep/</link>
      <description><![CDATA[      分享 YT 视频，讨论 UNET 的优势，以及它添加到通用模型上的各种归纳偏差CNN 非常适合图像分割。享受吧！   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c33o2x/p_segmenting_footballers_from_live_footage_deep/</guid>
      <pubDate>Sat, 13 Apr 2024 14:48:08 GMT</pubDate>
    </item>
    <item>
      <title>分析pdf文件并识别属性[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c33ay9/analyze_pdf_files_and_identify_attributes_d/</link>
      <description><![CDATA[嗨。我有一项任务，需要分析大量 pdf 文件并识别文件的某些属性，如下所示。 1. 有多少页包含多列文本 2. 有多少页包含英语以外的语言文本 3. 有多少页包含带有阴影的表格（或表格内的单元格）？ 4. 有多少页是首字下沉？ 5. 有多少页的文本超出正常页边距？关于我如何做到这一点有什么想法吗？    由   提交/u/spar_hawk13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c33ay9/analyze_pdf_files_and_identify_attributes_d/</guid>
      <pubDate>Sat, 13 Apr 2024 14:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 深度学习的完整解决方案：克里斯托弗·毕肖普 (Christopher Bishop) 和休·毕肖普 (Hugh Bishop) 撰写的基础和概念书？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c316jy/d_full_solutions_to_deep_learning_foundations_and/</link>
      <description><![CDATA[据我所知，书中的练习没有官方的解决方案，但是有没有非官方的解决方案？ &lt; /div&gt;  由   提交 /u/UniquelyCommonMystic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c316jy/d_full_solutions_to_deep_learning_foundations_and/</guid>
      <pubDate>Sat, 13 Apr 2024 12:48:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 用于优化 LLM 的新 Python 包</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2yja6/r_new_python_packages_to_optimise_llms/</link>
      <description><![CDATA[大家好！！！ 我们是一个小型研究小组，希望与您分享我们最新的 Python 软件包。  第一个是BitMat，旨在优化矩阵乘法操作定制 Triton 内核。我们的软件包利用了“1bit-LLM Era”中概述的原则。文档。  第二个是Mixture-of-deeps，是 Google 的实现DeepMind 论文：“Mixture-of-Depths：在基于 Transformer 的语言模型中动态分配计算”，介绍了一种在基于 Transformer 的语言模型中管理计算资源的新方法。  让我们知道您的想法！    由   提交 /u/AstraMindAI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2yja6/r_new_python_packages_to_optimise_llms/</guid>
      <pubDate>Sat, 13 Apr 2024 10:10:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 这里的人们不知道现在顶尖博士项目的招生竞争有多么激烈，哇......</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2x5mx/d_folks_here_have_no_idea_how_competitive_top_phd/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2x5mx/d_folks_here_have_no_idea_how_competitive_top_phd/</guid>
      <pubDate>Sat, 13 Apr 2024 08:29:26 GMT</pubDate>
    </item>
    <item>
      <title>[D]提高天气预报准确性：探索多源数据集成的回归模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2sj6z/denhancing_weather_forecast_accuracy_exploring/</link>
      <description><![CDATA[我目前在一家新能源初创公司担任数据科学家，主要负责预测第二天每15分钟的光伏发电量。所依赖的关键数据是天气预报，尤其是预测的太阳辐照度值。目前，我们拥有来自五个数值天气预报的数据，其中包括辐照度、温度和湿度等字段。不同数据来源的预测准确性存在差异，与实际天气存在一定差异。我正在考虑合并这五组数据以获得更准确的天气预报。我可以使用回归模型来利用五组天气预报数据来拟合实际天气吗？有更好的方法吗？   由   提交 /u/Rich-Effect2152    reddit.com/r/MachineLearning/comments/1c2sj6z/denhancing_weather_forecast_accuracy_exploring/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2sj6z/denhancing_weather_forecast_accuracy_exploring/</guid>
      <pubDate>Sat, 13 Apr 2024 03:38:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在顶级 ML 会议上发表多篇第一作者论文，但仍在努力进入博士学位课程。我缺少什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2pnam/d_multiple_firstauthor_papers_in_top_ml/</link>
      <description><![CDATA[TL;DR 我来自一个普通家庭，在对研究的热情的推动下，我努力学习完成大学学业和创新。尽管在顶级机器学习会议上发表了多篇第一作者论文，为开源项目做出了贡献，并产生了行业影响，但我仍在努力进入博士学位课程。我被顶尖大学拒绝了，感到失落和疲惫。我开始怀疑自己，想知道如果没有合适的人脉或家庭背景，强大的研究背景是否还不够。我正在考虑放弃攻读博士学位的梦想，做有意义的研究。 到目前为止，我已经在 EMNLP、NeurIPS、ACM 等顶级会议和研讨会上以第一作者身份发表了多篇研究论文和 ACL。我的研究被我公司评为最佳 NLP 研究员。我积极为开源项目做出贡献，包括 PyTorch 和 HuggingFace，并实现了其他工具和框架（在 GitHub 上聚合了 [x]0k+ 颗星）。我的研究论文被引用次数超过 [x]00 次，h 指数为 [x]。所有内容都经过了同行评审。 这些论文完全是我自己写的，没有任何监督或指导。从构思最初的想法到编写代码、进行实验、完善模型，再到最终撰写论文，我独立处理了研究过程的每个方面。作为第一代大学毕业生，我的公司没有出版文化。因此，我阅读论文，做注释，并尝试新的想法。第一篇论文花了我一年的时间才发表，因为我不知道该写什么，尽管我的想法的结果是最先进的。我在两个月内翻阅了 600 多篇论文，寻找规律并学习如何写论文。 现在，问题来了： 我想要攻读博士学位，但对我来说，这不仅仅是获得学位并在顶级公司找到工作以赚更多钱的一种方式。我不太倾向于经济收益。我想攻读博士学位，以便有一个更好的研究环境，建立一个强大的网络，我可以与它集思广益，获得建设性的反馈，开展项目合作，并利用我的知识为文明做出一些有意义的贡献。 但是，来自一个小城市，这是相当具有挑战性的。我不知道如何与教授接触，坦白说，我不太擅长与人接触。我尝试通过电子邮件与几位教授交谈，但他们没有回复。我还申请了卡内基梅隆大学、斯坦福大学和其他几所大学，但都被拒绝了。 我感觉有点疲惫。我知道这不是世界末日，但独自完成这一切并试图找到一所好大学只是为了做一些高质量的研究 - 这真的那么难吗？ 我在 Reddit 上看到了很多帖子在这个频道中，人们提到他们没有被录取是因为他们没有第一作者论文，或者他们质疑为什么大学要求第一作者论文。我还读到，如果你有一篇第一作者论文，那么你就已经准备好了。这是真的吗？ 如果是这样，我哪里错了？我有很强的研究背景，甚至像 Meta 和 Google 这样的公司也在使用我的研究和方法，但我仍然找不到一个好的教授来攻读我的博士学位。要么我错了，要么那些声称拥有第一作者论文就能进入顶尖大学的人都是错误的。 就我个人而言，我已经失去了希望。我开始相信，只有你的家庭有一定的学术背景，你才能进入一所好大学，因为他们会指导你去哪里申请和写什么。或者，如果您有很强的学术联系，您将根据推荐直接被录取。不幸的是，这两个我都没有。我觉得我被困在这个矩阵里了，人们是如此复杂，难以理解。为什么不能直接一点呢？如果我被所有大学拒绝，他们至少应该提供一个理由。我收到的唯一原因是，由于反应热烈，他们无法接受我。 我没有感到生气，但我很困惑。我已经开始怀疑自己了。我想知道我做错了什么。我觉得我应该放弃研究。   由   提交/u/Accomplished_Rest_16   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2pnam/d_multiple_firstauthor_papers_in_top_ml/</guid>
      <pubDate>Sat, 13 Apr 2024 01:04:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 从单词到数字：当给定上下文示例时，您的大型语言模型实际上是一个有能力的回归器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2hzlj/r_from_words_to_numbers_your_large_language_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.07544 代码：https://github .com/robertvacareanu/llm4regression 摘要：  我们分析预训练的大型语言模型（例如 Llama2）的效果如何、GPT-4、Claude 3 等）在给定上下文示例时可以进行线性和非线性回归，无需任何额外的训练或梯度更新。我们的研究结果表明，几种大型语言模型（例如 GPT-4、Claude 3）能够执行回归任务，其性能可与随机森林、Bagging 或 Gradient Boosting 等传统监督方法相媲美（甚至优于）。例如，在具有挑战性的 Friedman #2 回归数据集上，Claude 3 的性能优于许多监督方法，例如 AdaBoost、SVM、随机森林、KNN 或梯度提升。然后，我们研究大型语言模型的性能随上下文样本数量的变化情况。我们借鉴了在线学习的遗憾概念，并凭经验证明法学硕士能够获得亚线性遗憾。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2hzlj/r_from_words_to_numbers_your_large_language_model/</guid>
      <pubDate>Fri, 12 Apr 2024 19:30:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习博士论文发表的激烈竞争</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2bvoj/d_publication_rat_race_for_phd_in_ml/</link>
      <description><![CDATA[目前，顶尖大学的机器学习博士项目要求申请者拥有多篇第一作者论文以及来自知名研究人员的强烈推荐。如果在其他领域，这种发表记录可能会让您有资格担任教职。学生们如果自己没有进入顶尖学校，怎么可能取得这些成绩呢？  编辑：我的主要观点与推荐要求有关。顶尖大学以外的学生几乎没有机会获得知名研究人员的推荐，因为他们大多在顶尖学校工作。这些顶尖大学之外也有知名的研究人员，但数量太少。如果你在一所普通大学，那么你的学校很可能没有在你的领域足够知名的研究人员。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2bvoj/d_publication_rat_race_for_phd_in_ml/</guid>
      <pubDate>Fri, 12 Apr 2024 15:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>