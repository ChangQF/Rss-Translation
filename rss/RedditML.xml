<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者请参阅learnmachinelearning AGI请参阅singularity</description>
    <lastBuildDate>Sun, 21 Jul 2024 21:12:53 GMT</lastBuildDate>
    <item>
      <title>[D] 在少样本原型网络中，跨折叠使用不同数量的查询样本可以吗</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8w0fn/d_is_it_okay_to_use_different_numbers_of_query/</link>
      <description><![CDATA[我目前正在使用原型网络进行少样本学习项目，遇到了一种情况，想请教一下。 我有一个预定义的 5 倍交叉验证设置，但这些折叠中的 A 类示例数量不一致。具体来说，一个折叠有 6 个 A 类示例，而其他折叠每个都有 12 个 A 类示例。 在这种情况下，在训练和测试期间使用不同数量的查询样本，同时保持相同的样本数和方法数，是否可以接受？例如，我可以使用可变数量的查询训练模型，并使用各自的查询数评估每个折叠吗？    提交人    /u/The_Aoki_Taki   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8w0fn/d_is_it_okay_to_use_different_numbers_of_query/</guid>
      <pubDate>Sun, 21 Jul 2024 20:39:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] ChessGPT 比 GPT-4 小 100,000 倍，下棋等级为 1500 Elo。通过找到技能向量，我们可以在非分布游戏中将其胜率提高 2.6 倍。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</link>
      <description><![CDATA[之前的一个项目训练了 ChessGPT，这是一组 25M 和 50M 参数的 GPT 模型，可以在 1500 Elo 下棋。这些模型比 GPT-4 的 1.8T 参数小约 100,000 倍。 在 Stockfish 0 级，50M 参数模型的胜率为 70%。但是，如果用 20 个随机动作初始化游戏，其胜率会下降到 17%。这是因为它无法泛化分布之外的内容吗？在考虑下一个标记预测任务时，如果游戏以随机动作开始，那么好的下一个标记预测器会预测合法但低技能的动作。 这就是我们在 ChessGPT 中发现的。通过向模型的激活中添加技能向量，我们可以将其胜率提高到 43%，即提高 2.6 倍。我们无法完全弥补性能差距，但这是一个很大的比例。干预非常简单，更复杂的干预可能会进一步提高其胜率。 该模型仅经过训练以预测 PGN 字符串中的下一个字符（1.e4 e5 2.Nf3 ...），并且从未明确给出棋盘状态或国际象棋规则。尽管如此，为了更好地预测下一个角色，它会学习在游戏的任何时候计算棋盘的状态，并学习各种规则，包括将军、将死、王车易位、过路兵、升级、固定棋子等。此外，为了更好地预测下一个角色，它还学习估计潜在变量，例如游戏中玩家的 Elo 评级。 我们还可以使用可解释性方法来干预模型的内部棋盘状态。 这项工作最近被 2024 年语言建模会议 (COLM) 接受，标题为“国际象棋语言模型中的新兴世界模型和潜在变量估计”。 更多信息请参阅此帖子： https://adamkarvonen.github.io/machine_learning/2024/03/20/chess-gpt-interventions.html 代码在这里： https://github.com/adamkarvonen/chess_llm_interpretability    提交人    /u/seraine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8v2za/p_chessgpt_100000x_smaller_than_gpt4_plays_chess/</guid>
      <pubDate>Sun, 21 Jul 2024 19:59:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] ICML 2024：法学硕士中的电路假设检验</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8v0eh/r_icml_2024_hypothesis_testing_the_circuit/</link>
      <description><![CDATA[https://openreview.net/forum?id=ibSNv9cldu    由   提交  /u/throwaway-cs-12345   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8v0eh/r_icml_2024_hypothesis_testing_the_circuit/</guid>
      <pubDate>Sun, 21 Jul 2024 19:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大型语言模型时代的智能数字代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8reay/r_intelligent_digital_agents_in_the_era_of_large/</link>
      <description><![CDATA[https://doi.org/10.31219/osf.io/f75wz     由   提交  /u/thebigbigbuddha   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8reay/r_intelligent_digital_agents_in_the_era_of_large/</guid>
      <pubDate>Sun, 21 Jul 2024 17:20:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 与主要作者吴正轩讨论 ReFT 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</link>
      <description><![CDATA[大家好， 本周星期五，我们非常幸运地邀请到了 ReFT 论文的主要作者参加我们的论文讨论，我想分享一下我们的讨论和笔记！ https://www.oxen.ai/blog/arxiv-dives-how-reft-works TLDR ~ ReFT 是一种微调技术，其参数效率比 LoRA 高 15 到 60 倍。训练速度超快。在 A100 上，1k 个示例大约需要 18 分钟。我成功地在不到 1 分钟的时间内，在 A10 上使用大约 100 个示例在 Llama 2 7B 上对 ReFT 进行了微调。 它的工作原理是操作残差流中的表示，而不是 K-V 矩阵。他们向特定的 token 索引和层添加了他们称为“干预”的额外学习参数，从而高效且轻松地控制表示。ReFT 也很不错，因为它们是可组合的。例如，您可以训练一个用于指令跟踪的模型，一个用于德语的模型，然后将它们都应用于德语的获取和指令跟踪模型。 作者给出了他们在实验室中迭代时学到的超级实用的技巧和教训。整个讨论也在 YouTube 上。 希望你喜欢！    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8qwnl/r_discussion_of_reft_paper_with_lead_author/</guid>
      <pubDate>Sun, 21 Jul 2024 16:59:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于神经发育程序。您认为“学习编码”这一想法有多合理？您认为它与自我编程有多大区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8pugs/d_on_the_neural_developmental_program_how_sound/</link>
      <description><![CDATA[我刚刚在 ALIFE2023 上看到了这个演讲，这看起来真的很有趣。总结一下，他们使用了 3 个“策略”网络来发展/生成一个“目标”网络，也就是传统的“策略”网络，它从环境中获取输入并给出动作的输出。 最后，演讲中描述的这个代理仍然是几个传统的神经网络，只是它们的输出是一个模型而不是预测，而来自奖励的训练只是进化算法或 PPO。对于像我这样的自我编程倡导者来说，这种由结构生成看起来很像其自身的结构的想法听起来非常棒，但从外观上看，它是一种类似优先依附的花哨网络生成模型，只是这次在奖励的帮助下它可以解决实际问题。（我仍然认为它很棒，并且是我们现在对预训练微调 RLHF 东西的常规做法的决定性一步） 您对此有何看法？    提交人    /u/HermanHel   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8pugs/d_on_the_neural_developmental_program_how_sound/</guid>
      <pubDate>Sun, 21 Jul 2024 16:11:34 GMT</pubDate>
    </item>
    <item>
      <title>[R] 基线薄弱和报告偏差导致机器学习对流体相关偏微分方程过度乐观</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8pp4r/r_weak_baselines_and_reporting_biases_lead_to/</link>
      <description><![CDATA[  由    /u/nuclear_knucklehead  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8pp4r/r_weak_baselines_and_reporting_biases_lead_to/</guid>
      <pubDate>Sun, 21 Jul 2024 16:05:07 GMT</pubDate>
    </item>
    <item>
      <title>有谁成功微调过 Chronos [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8ia8p/has_anybody_succeeded_at_finetuning_chronos_d/</link>
      <description><![CDATA[Chronos 是基于 t5 架构的时间序列预测模型，由 Amazon 开发。 我的应用程序用于预测身体运动中的关节位置。这个应用程序的术语很重，而且他们的微调文档有点难以理解。 如果有人成功做到了，请帮忙。  Chronos：https://github.com/amazon-science/chronos-forecasting 微调文档：https://github.com/amazon-science/chronos-forecasting/blob/main/scripts%2FREADME.md 如果有人能直接帮助我预训练这个模型，特别是根据他们的要求转换我的数据集，那将非常有帮助。谢谢    提交人    /u/abdullahboy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8ia8p/has_anybody_succeeded_at_finetuning_chronos_d/</guid>
      <pubDate>Sun, 21 Jul 2024 09:14:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 去噪扩散概率模型论文学习笔记</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8hrib/r_my_learning_notes_for_denoising_diffusion/</link>
      <description><![CDATA[嗨， 我正在分享我在扩散论文https://maitbayev.github.io/posts/denoising-diffusion-probabilistic-models/上的学习笔记，主要是为了加深我自己的理解，希望它们对你也有用。欢迎提问。 谢谢，    提交人    /u/madiyar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8hrib/r_my_learning_notes_for_denoising_diffusion/</guid>
      <pubDate>Sun, 21 Jul 2024 08:35:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你在公司项目中处理的数据集的平均大小是多少？#D</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8h7ra/d_what_is_the_average_size_of_datasets_you_work/</link>
      <description><![CDATA[想知道您在项目中使用的数据集大小，是 GB 还是 TB？ 以及您如何处理大型数据集以及在处理过程中遵循哪些最佳实践。    提交人    /u/PraveenKumarIndia   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8h7ra/d_what_is_the_average_size_of_datasets_you_work/</guid>
      <pubDate>Sun, 21 Jul 2024 07:55:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 检索增强生成归因</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8e12u/d_attribution_for_retrieval_augmented_generation/</link>
      <description><![CDATA[在 RAG 期间，我一直在研究 LLM 的“引用来源”研究。例如，OpenAI 使用他们的 file_search 工具为 Assistants API 或 Bing Chat 执行此操作，其引用位于右上角。我想知道是否有任何最近的调查论文介绍黑盒技术来实现这一点。我发现一个非常有前途的论文是 MIRAGE 论文（https://arxiv.org/abs/2406.13663），但它需要开放权重。可能是我没有使用正确的搜索术语，但似乎这个子领域已经停滞不前。    提交人    /u/Green_ninjas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8e12u/d_attribution_for_retrieval_augmented_generation/</guid>
      <pubDate>Sun, 21 Jul 2024 04:23:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你在生产中的 LLM Stack 是什么样的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8cxkf/d_what_is_your_llm_stack_in_production/</link>
      <description><![CDATA[好奇人们在生产堆栈中使用什么来开发 LLM 应用程序。这个问题是几个月前提出的，但该领域的事态变化如此之快，我认为值得再开一个帖子。 嵌入模型：目前是 OpenAI Ada，但召回率/准确率不是很好，所以我打算尝试其他模型 矢量数据库：Supabase（推荐） LLM：一直在尝试开源和闭源模型。我的任务需要相当强的推理能力，因此不幸的是本地模型还不够好（例如 Llama 70B）。OpenAI GPT-4o 表现最佳（这并不奇怪），但对于我的用例来说它确实很昂贵，所以我目前使用的是 Gemini Pro 1.5。 LLM 框架：无。大家的共识是远离 LangChain，所以我只是直接与 LLM 提供商集成。幸运的是，LLM 提供商似乎倾向于 OpenAI API 标准，这使得实验变得容易（除了偶尔定制的 API，如 Gemini） 评估：???。不太确定这个的最新技术是什么。 其他人都在用什么？    提交人    /u/Aggressive_Comb_158   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8cxkf/d_what_is_your_llm_stack_in_production/</guid>
      <pubDate>Sun, 21 Jul 2024 03:17:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e8btox/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 21 Jul 2024 02:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] Perpetual：无需超参数调整的梯度提升机</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e858j4/r_perpetual_a_gradient_boosting_machine_which/</link>
      <description><![CDATA[Repo：https://github.com/perpetual-ml/perpetual PerpetualBooster 是一种梯度提升机 (GBM) 算法，它不需要超参数调整，因此与其他 GBM 算法不同，您可以在没有超参数优化库的情况下使用它。与 AutoML 库类似，它有一个 budget 参数。增加 budget 参数可提高算法的预测能力，并在看不见的数据上提供更好的结果。 下表总结了 California Housing 数据集（回归）的结果：   永久预算 LightGBM n_estimators 永久 mse LightGBM mse 永久 cpu 时间 LightGBM cpu 时间 加速    1.0 100 0.192 0.192 7.6 978 129x   1.5 300 0.188 0.188 21.8 3066 141x   2.1 1000 0.185 0.186 86.0 8720 101x   PerpetualBooster 使用泛化算法防止过度拟合。本文正在编写中，旨在解释该算法的工作原理。查看我们的博客文章，了解该算法的高级介绍。     提交人    /u/mutlu_simsek   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e858j4/r_perpetual_a_gradient_boosting_machine_which/</guid>
      <pubDate>Sat, 20 Jul 2024 20:47:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 科学的机器学习在实践中实际应用吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1e7z4s0/d_is_scientific_machine_learning_actually_used_in/</link>
      <description><![CDATA[作为一个背景横跨科学计算和机器学习的人，我听到了很多关于科学机器学习 (SML) 的信息。它的承诺是，人们可以使用机器学习来加速、简化或以其他方式改进数值模型。一个常见的示例用例是，人们可以使用高保真数值模拟（运行速度可能非常慢）作为训练数据，然后在这些模拟上训练神经网络，以比运行实际模拟更快的速度预测数值模拟的结果（从而获得降阶模型）。这对于数字孪生等来说可能非常有用，您可能希望实时计算风力涡轮机的流体动力学，同时遵守控制流体方程并结合不断变化的风、温度等传感器数据，以预测事故、优化等。我只在学术环境中听说过这个和其他用例。  我的问题是，科学机器学习是否真的在实践（工业）中使用？有人能指出任何现实世界的例子吗？有没有公司真正使用这项技术？如果没有，我很想听听为什么它似乎没有为市场提供任何价值（至少目前如此）。在工业界采用这些方法的一些障碍/瓶颈是什么？或者科学机器学习只是两个原本有用的领域的人为配对，仅仅是为了满足学术好奇心和撰写拨款提案？    提交人    /u/worstthingsonline   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1e7z4s0/d_is_scientific_machine_learning_actually_used_in/</guid>
      <pubDate>Sat, 20 Jul 2024 16:09:20 GMT</pubDate>
    </item>
    </channel>
</rss>