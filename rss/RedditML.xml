<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 04 Feb 2024 06:15:29 GMT</lastBuildDate>
    <item>
      <title>重新表述网络：计算和数据高效语言建模的秘诀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aighsr/rephrasing_the_web_a_recipe_for_compute/</link>
      <description><![CDATA[ 由   提交/u/rrenaud  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aighsr/rephrasing_the_web_a_recipe_for_compute/</guid>
      <pubDate>Sun, 04 Feb 2024 05:33:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 时间序列预测深度学习最新进展的文献综述。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aifjbq/r_literature_review_of_advances_recent_in_deep/</link>
      <description><![CDATA[我写了一个关于 2024 年将深度学习应用于时间序列预测的最新文献的文献综述。我研究了最新的进展，例如更强大的变压器架构和归一化技术，如果它们可以击败 D-Linear 和 N-Linear 等简单模型。我还批评了 TimeGPT 和其他几个似乎主要是营销策略的模型。 ​ Archive.is 上的“无付费版本”，不过如果您有 Medium 帐户（如果您愿意在那里查看它），我将不胜感激。   由   提交/u/AttentionImaginary54  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aifjbq/r_literature_review_of_advances_recent_in_deep/</guid>
      <pubDate>Sun, 04 Feb 2024 04:38:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 苹果发布 MGIE！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aideah/r_apple_releases_mgie/</link>
      <description><![CDATA[      [ICLR&#39;24 Spotlight] 通过多模态大图指导基于指令的图像编辑语言模型 MLLM引导的基于指令的图像编辑（MGIE）可以遵循用户指令来编辑图像论文：https://openreview.net/forum?id=S1RKWSyZ2Y 项目：https ://mllm-ie.github.io https://preview.redd.it/7abn9yflehgc1.png?width=3183&amp;format=png&amp;auto=webp&amp;s=9fc6c301f49ffaaf1c293c8f5925c603c8 c7dc24 代码/ checkpoint 也是开源的 🔥 Apple 官方仓库：https://github.com/apple/ml-mgie 带 Gradio 演示的仓库：https://github.com/tsujuifu/pytorch_mgie &lt; p&gt;https://preview.redd.it/hyqngv8nehgc1。 png?width=3736&amp;format=png&amp;auto=webp&amp;s=3a70483a7bea6e16500370cee5879e605fe7d51d   由   提交 /u/tsujuifu   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aideah/r_apple_releases_mgie/</guid>
      <pubDate>Sun, 04 Feb 2024 02:42:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 先知模型 - 错误的趋势检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aibsx1/p_prophet_model_incorrect_trend_detection/</link>
      <description><![CDATA[您好！ 我正在使用先知模型进行时间序列预测。我的数据存在差距，并且我预测的值随后会减少。 参见下图： https://imgur.com/a/Qgl1tlu 这会导致趋势呈负值并影响预测。使用预言家识别/消除这些缺口/趋势变化情况的最佳方法是什么？ 提前致谢！   由   提交 /u/serpentna   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aibsx1/p_prophet_model_incorrect_trend_detection/</guid>
      <pubDate>Sun, 04 Feb 2024 01:21:05 GMT</pubDate>
    </item>
    <item>
      <title>差异隐私和统计查询[讨论]，[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aibbbc/differential_privacy_and_statistical_queries/</link>
      <description><![CDATA[有人使用过统计查询模型吗？我想知道在神经网络中使用统计查询是否明智？尽管我读过一些论文，但神经网络中查询的运行时间是指数级的，因此不建议这样做。我们有什么办法可以在神经网络中实现隐私吗？   由   提交 /u/Hopeful-Foot5888    reddit.com/r/MachineLearning/comments/1aibbbc/ Differential_privacy_and_statistical_queries/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aibbbc/differential_privacy_and_statistical_queries/</guid>
      <pubDate>Sun, 04 Feb 2024 00:57:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 不确定性量化和人工智能对齐之间的联系？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aib9rw/r_connections_between_uncertainty_quantification/</link>
      <description><![CDATA[大家好， ​ 正如标题所示，我正在寻找指针连接机器学习这两个领域的（研究论文或集思广益的想法）。 对于那些不熟悉对齐的人，这是最独特的论文： https://arxiv.org/pdf/2203.02155.pdf ​ 谢谢！&lt; /p&gt; ​ ​   由   提交/u/South-Conference-395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aib9rw/r_connections_between_uncertainty_quantification/</guid>
      <pubDate>Sun, 04 Feb 2024 00:54:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何模仿openAI工具/功能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aiabgk/d_how_to_mimic_openai_toolsfunctions/</link>
      <description><![CDATA[我有兴趣探索针对特定场景使用替代 GPT 模型复制 OpenAI 功能的方法。想象一下有一个包含各种函数及其规范的字典。当用户提出问题时，系统应该能够正确识别这些功能并对其进行排序，以构建连贯的响应。此外，我对从头开始开发这样一个系统所需的基本步骤感到好奇。具体来说，从函数字典及其规范开始，如何设计一种生成代码响应的机制？感谢任何指导/输入，因为我从未自己训练过任何模型。 OpenAI 的回答对这个问题没有帮助:)   由   提交/u/mrg3_2013   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aiabgk/d_how_to_mimic_openai_toolsfunctions/</guid>
      <pubDate>Sun, 04 Feb 2024 00:09:27 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型努力学习长尾知识 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ai7en3/large_language_models_struggle_to_learn_longtail/</link>
      <description><![CDATA[      https://arxiv.org/abs /2211.08411 ​ 摘要： 互联网蕴藏着丰富的知识——来自历史人物的生日到如何编码的教程——所有这些都可以通过语言模型来学习。然而，虽然某些信息在网络上无处不在，但其他信息却很少出现。在本文中，我们研究了大型语言模型记忆的知识与从网络上抓取的预训练数据集中的信息之间的关系。特别是，我们表明语言模型回答基于事实的问题的能力与在预训练期间看到的与该问题相关的文档数量有关。我们通过链接预训练数据集的实体和对包含与给定问答对相同的实体的文档进行计数来识别这些相关文档。我们的结果证明了众多问答数据集（例如 TriviaQA）、预训练语料库（例如 ROOTS）和模型大小（例如 176B 参数）的准确性和相关文档计数之间存在很强的相关性和因果关系。此外，虽然较大的模型更擅长学习长尾知识，但我们估计当今的模型必须扩展多个数量级，才能在预训练数据支持很少的情况下在问题上达到有竞争力的 QA 性能。最后，我们证明检索增强可以减少对相关预训练信息的依赖，为捕获长尾提供了一种有前景的方法。 ​ &amp;# x200b; https://preview .redd.it/t8f3b4flzfgc1.png?width=603&amp;format=png&amp;auto=webp&amp;s=09c243c055b2d5d9aa18192c4082970d8a1e1381   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ai7en3/large_language_models_struggle_to_learn_longtail/</guid>
      <pubDate>Sat, 03 Feb 2024 21:58:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人们还相信LLM的新兴能力吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/</link>
      <description><![CDATA[自从[新兴的LLM能力是海市蜃楼吗？](https://arxiv.org/pdf/2304.15004.pdf），人们似乎对涌现非常安静。但是大的[新兴能力](https://openreview.net/pdf?id=yzkSU5zdwD)论文有这一段（第 7 页）： &gt;考虑用于衡量新兴能力的评估指标也很重要（BIG-Bench，2022）。例如，使用精确的字符串匹配作为长序列目标的评估指标可能会将复合增量改进伪装成出现。类似的逻辑可能适用于多步骤或算术推理问题，其中模型仅根据是否正确获得多步骤问题的最终答案来评分，而不会给予部分正确的解决方案任何信用。然而，最终答案准确性的跳跃并不能解释为什么中间步骤的质量突然出现在随机之上，并且使用不给予部分信用的评估指标充其量是一个不完整的解释，因为在许多分类任务中仍然观察到涌现的能力（例如，图 2D-H 中的任务）。 人们怎么想？涌现是“真实的”吗？还是实质性的？   由   提交/u/uwashingtongold  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/</guid>
      <pubDate>Sat, 03 Feb 2024 20:50:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] TimesFM：基于 1000 亿个真实世界数据点进行预训练的基础预测模型，在不同领域提供前所未有的零样本性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahzmdc/r_timesfm_a_foundational_forecasting_model/</link>
      <description><![CDATA[       由   提交 /u/BlupHox   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahzmdc/r_timesfm_a_foundational_forecasting_model/</guid>
      <pubDate>Sat, 03 Feb 2024 16:15:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用局部水印主动检测语音克隆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahzdr0/r_proactive_detection_of_voice_cloning_with/</link>
      <description><![CDATA[人工智能语音合成的快速发展带来了令人难以置信的虚假人类语音，引发了人们对语音克隆和深度伪造音频的担忧。 &lt;随着人工智能合成的改进，被动分析作为检测虚假音频的传统方法面临着挑战。这些方法往往依赖于工件，但这些方法是特定于模型的。而且模型质量不断提高，伪影数量也减少了。 Meta 和 Inria 的研究人员开发了 AudioSeal，这是一种新技术，可以在人工智能生成的语音上添加难以察觉的水印以供检测。  AudioSeal 专注于定位音频剪辑中的合成语音。它的两个组件（生成器和检测器）协同工作，提供样本级精度和稳健的检测。 AudioSeal 的创新包括样本级精度、稳健的感知损失、音频失真恢复能力和高效检测，使得速度非常快。 联合训练生成器和检测器可以最大限度地减少感知差异并最大限度地提高准确性，即使存在屏蔽或扭曲的区域也是如此。我发现这是本文的关键见解。 AudioSeal 在通用性、样本级别的本地化、针对音频失真的鲁棒性、效率和模型身份消息的容量方面表现出色。它的检测速度大约比 WavMark 快两个数量级，生成水印的速度快 14 倍。 虽然很有希望，但应考虑道德问题和保密性需求，并且可能需要标准化才能得到更广泛的采用。&lt; /p&gt; TLDR：AudioSeal 是一种检测假音频的新颖解决方案，具有本地化且强大的检测功能。它也比 WavMark 快得多。 论文这里。存储库位于此处。完整摘要位于此处。 &lt; /div&gt;  由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/1ahzdr0/r_proactive_detection_of_voice_cloning_with/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahzdr0/r_proactive_detection_of_voice_cloning_with/</guid>
      <pubDate>Sat, 03 Feb 2024 16:04:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求建议：从工业界转向 NLP 研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahzdj7/d_seeking_advice_transitioning_from_industry_to/</link>
      <description><![CDATA[嘿各位 Reddit 用户， 简单介绍一下我的背景 - 我目前在行业内的数据分析领域工作，并且我的日常任务与自然语言处理（NLP）没有直接关系。然而，我对 NLP 充满热情，并渴望攻读博士学位。在该领域。我渴望获得研究经验。我拥有计算机科学学士和硕士学位。 由于我不隶属于大学，因此我正在寻求有关如何应对这一转变的指导。有人可以分享关于如何在学术环境之外有效参与 NLP 研究的见解吗？我可以采取哪些实际步骤来为该领域做出贡献、建立研究组合、增加攻读 NLP 博士学位以及让我的工作得到顶级会议和期刊认可的机会？ 个人建议经验或推荐资源将不胜感激。预先感谢您！   由   提交/u/Puzzleheaded_Big_242   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahzdj7/d_seeking_advice_transitioning_from_industry_to/</guid>
      <pubDate>Sat, 03 Feb 2024 16:04:11 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 梯度下降的最佳矩阵乘法算法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahoo9q/project_the_best_matrix_multiplication_algorithm/</link>
      <description><![CDATA[我正在尝试用 0 个依赖项在 Rust 中实现神经网络。我知道施特拉森只擅长高排名，并且错误增加（来源是极客中的极客，所以可能是可疑的）。不管怎样，我想知道我应该使用什么算法来进行矩阵乘法，以及它是否会产生很大的差异。   由   提交 /u/ANARCHY14312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahoo9q/project_the_best_matrix_multiplication_algorithm/</guid>
      <pubDate>Sat, 03 Feb 2024 05:29:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我在这里没有看到足够多的人赞扬 dinov2 ！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahkxjh/d_i_dont_see_enough_people_praising_dinov2_here/</link>
      <description><![CDATA[大家好！ 我只是想写一条简短的消息，让这里的每个人都知道 dinov2 是一个多么强大的工具对我来说！我已经对其进行了微调，可以对图像进行多种不同目的的分类，并且每次类只有 20-50 张图像，它总是很成功。根据我使用它的用例：对 3D/照片图像进行分类，带水印/无水印图像、模糊/非模糊图像、面部识别（识别 dlib 对齐的面部是否特定属于某个人）、艺术家风格、验证图像中特定对象上方的分割是否正确等等。 .. 在 dinov2 的整个系列中，我从未使用过比小型模型更大的东西（尽管我使用 448x448 图像），因此它无需使用太多 VRAM 即可工作，并且可以批量处理 100 个图像一次！ 最近我什至尝试在暹罗架构中对 dinov2 进行微调，只用一个新的头部来获取两个图像的特征，这样它就可以将两个图像放在一起比较（不用说太多，我想知道是否两者都图像遵循共同的结构）并且它工作得很好。 我还使用它来将图像的特征提供给稳定扩散，并且它也工作得很好（使用 IP 适配器架构）。 &gt; 我唯一没能做到的就是使用它进行分割，但我认为这是因为我的数据集和/或实现，所以如果你们中有人做到了，我很想与你们交谈交流良好实践！ 如果您希望脚本对其进行微调/推理以进行分类，我很乐意分享它们。 您呢？你用 dinov2 吗？如果是，为什么以及如何？您对此有何体验？   由   提交 /u/Antique-Bus-7787   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahkxjh/d_i_dont_see_enough_people_praising_dinov2_here/</guid>
      <pubDate>Sat, 03 Feb 2024 02:08:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>