<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 29 Nov 2024 06:25:04 GMT</lastBuildDate>
    <item>
      <title>[D] 如果 VQ-VAE 能够解开的话，它该如何解开呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/</link>
      <description><![CDATA[我目前使用 BetaTC-VAE，它在解缠方面做得非常出色，我知道 VAE 可以稍微解缠，因为对于模型来说，如果变量解缠，更容易获得较低的 KL 损失，beta 项使这个 beta 倍更重要，总相关性和互信息损失推动完全解缠，但在 VQ-VAE 中没有（主要）解缠，只有码本和离散输出。码本给出的离散潜在空间可以解缠吗？如果不能，有没有关于解缠 VQ-VAE 的论文？我有一个环境，其中解缠的潜在空间比连续潜在空间提供更好的重建     提交人    /u/ZazaGaza213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2epzx/d_how_does_vqvae_disentangle_if_it_does_at_all/</guid>
      <pubDate>Fri, 29 Nov 2024 05:33:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] Transformer 注意力模型不一致</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h2dmox/r_transformer_attention_figure_inconsistent/</link>
      <description><![CDATA[      我是一名学生目前正在研究 Transformers，我正在努力提高此存储库中代码的性能：Transformer 实现。 最初，我注意到模型的成本没有收敛，这导致了完全错误的输出。为了解决这个问题，我将学习率从 0.001 调整为 0.0001。经过这一改变，模型开始收敛并产生了正确的句子。 然而，在可视化编码器自注意力、解码器自注意力和编码器-解码器注意力的图表时，注意力图没有显示每个单词的预期权重。我不确定如何解释这些结果，或者代码本身是否存在问题。 如果有人可以帮助解释这些数字或提供有关实施中潜在问题的见解，我将不胜感激。 这些是我将学习率从 0.001 设置为 0.0001 后绘制的数字 编码器自注意力 解码器自注意力 编码器-解码器注意力    提交人    /u/Master-Regular458   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h2dmox/r_transformer_attention_figure_inconsistent/</guid>
      <pubDate>Fri, 29 Nov 2024 04:27:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 隐式正则化领域中最重要的论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/</link>
      <description><![CDATA[大家好 我正在研究机器学习，尤其是理论方面，我很好奇为什么神经网络往往具有如此好的泛化能力，所以我希望阅读一些关于这方面的论文。据我所知，关于这个主题的第一篇重要论文是张等人的《理解深度学习需要重新思考泛化》。 我有很好的数学背景，所以我想知道人们认为这个领域最有影响力的论文是什么。你认为哪篇论文影响最大？    提交人    /u/MrBeebins   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h29i7j/d_most_important_papers_in_implicit_regularisation/</guid>
      <pubDate>Fri, 29 Nov 2024 00:23:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] 本地检索增强生成（完全本地解决方案）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/</link>
      <description><![CDATA[大家好， 我很高兴与您分享我的最新 repo — 适用于您文件的本地对话式 RAG 解决方案！情况是这样的：此设置非常适合在本地运行 RAG。 它由 Docker、LangChain、Ollama、FastAPI 和 Hugging Face 构建，所有模型都会自动下载。很快，我将添加对选择您喜欢的模型的支持，但以下是解决方案当前包含的内容： • 本地运行 Ollama：它目前硬编码到 Qwen-0.5B 模型，但即将从 Ollama 注册表中进行模型选择。 • 本地索引：使用句子转换器嵌入模型（当前仅限于此系列，但这也将很快改变）。 • Qdrant 容器：在本地运行以进行向量存储。 • 本地重新排名器：当前使用 BAAI/bge-reranker-base，即将支持重新排名器选择。 • 基于 Websocket 的聊天：包括历史记录保存功能。 • 简单的聊天 UI：使用 React 构建以获得直观的界面。 • 奖励：您可以将此设置与 ChatGPT 一起用作自定义 GPT！通过官方 ChatGPT 网络界面或 macOS/iOS 应用查询本地数据。 • 本地就绪：一切都在本地运行，并且容器对 CPU 友好。 一些想法和已知问题： • 对模型上下文协议的支持已列入计划。 • 尚无增量索引或重新索引。 • 模型选择尚不可用，但很快就会添加。 我很乐意听取您的反馈、贡献或支持 — 如果您觉得这很有趣，请关注、分叉和加星标！ 谢谢！ https://github.com/dmayboroda/minima     提交人    /u/davidvroda   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h26ul7/p_retrieval_augmented_generation_onpremises_fully/</guid>
      <pubDate>Thu, 28 Nov 2024 22:01:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] Yannic Kilcher discord 服务器上的每日论文讨论 - Visatronic：用于语音合成的多模态解码器模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/</link>
      <description><![CDATA[      作为 Yannic Kilcher discord 服务器上每日论文讨论的一部分，我将自愿领导对以下 Apple Visatronic 作品的分析 📜 Visatronic：用于语音合成的多模态解码器模型，作者：Akshita Gupta、Navdeep Jaitly、Tatiana Likhomanenko、Karren Yang、Zakaria Aldeneh、何白 🌐 https://arxiv.org/abs/2411.17690 🕰 2024 年 11 月 29 日星期五 01:30 AM UTC // 星期五，2024 年 11 月 29 日 上午 7:00 IST // 星期四，2024 年 11 月 28 日 下午 5:30 PT 加入这个 Discord 服务器享受乐趣 ~ https://discord.gg/VGAtPcXs 他们似乎正在提出一个统一的多模式解码器专用语音生成模型。此外，语音识别模型对生成语音的词错误率相对降低了15%以上 https://preview.redd.it/ygxnbhiboo3e1.png?width=799&amp;format=png&amp;auto=webp&amp;s=31bf7c9b988c83a8d0ff2e7b011dac027aa8f154 https://preview.redd.it/7v15egiboo3e1.png?width=1055&amp;format=png&amp;auto=webp&amp;s=d9629caa406a92f8b2052ad6baa3a0265a27ddcf    提交人    /u/CATALUNA84   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h222s2/d_daily_paper_discussion_on_yannic_kilcher/</guid>
      <pubDate>Thu, 28 Nov 2024 18:13:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] Ollama Grid Search 最新版本（0.7.0）：添加了提示数据库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/</link>
      <description><![CDATA[      https://preview.redd.it/ohewvqicbo3e1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=077fec6931b2efc40182f7c2eb284718822213e0 大家好... Ollama 的最新版本网格搜索现在带有自己的提示管理数据库（以及用户界面的许多改进）。 https://preview.redd.it/qzu95clhbo3e1.png?width=975&amp;format=png&amp;auto=webp&amp;s=473382281094fc3f819e6fc6c3d267941d2a35ce 当您提取新发布的模型时，它可以更轻松地测试现有的提示！ 如果您想检查一下， github 页面包含适用于所有主要平台的版本： https://github.com/dezoito/ollama-grid-search    提交人    /u/grudev   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h20fzv/p_latest_version_of_ollama_grid_search_070_added/</guid>
      <pubDate>Thu, 28 Nov 2024 17:01:16 GMT</pubDate>
    </item>
    <item>
      <title>[P][R] 使用感知器 IO 寻找多模态分类示例（音频 + 图像 + 文本）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h204ag/pr_looking_for_multimodal_classification_examples/</link>
      <description><![CDATA[我正在探索 Perceiver IO 用于一个项目，该项目涉及同时处理多种数据模态（音频、图像和文本）以完成二元分类任务。我正在寻找任何 GitHub 存储库或资源，其中已使用它来同时处理这些模态。非常感谢您的帮助！    提交人    /u/kernel_KP   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h204ag/pr_looking_for_multimodal_classification_examples/</guid>
      <pubDate>Thu, 28 Nov 2024 16:46:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] BitNet a4.8：1 位 LLM 的 4 位激活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/</link>
      <description><![CDATA[      论文： https://arxiv.org/pdf/2411.04965 摘要：  最近对 1 位大型语言模型 (LLM)（例如 BitNet b1.58）的研究为降低 LLM 的推理成本同时保持其性能提供了一个有希望的方向。在这项工作中，我们引入了 BitNet a4.8，为 1 位 LLM 启用 4 位激活。BitNet a4.8 采用混合量化和稀疏化策略来减轻异常通道引入的量化误差。具体来说，我们利用 4 位激活作为注意力和前馈网络层的输入，同时稀疏中间状态，然后进行 8 位量化。大量实验表明，BitNet a4.8 在训练成本相当的情况下，实现了与 BitNet b1.58 相当的性能，同时在启用 4 位（INT4/FP4）内核的情况下，推理速度更快。此外，BitNet a4.8 仅激活 55% 的参数并支持 3 位 KV 缓存，进一步提升了大规模 LLM 部署和推理的效率。  Visual Abstract: https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a 评估： HS=HellaSwag, PQ=PiQA, WGe=WinoGrande https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/</guid>
      <pubDate>Thu, 28 Nov 2024 15:11:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 GPU 并行实现基于矩阵的快速反事实遗憾最小化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/</link>
      <description><![CDATA[反事实遗憾最小化 (CFR) 的一种新颖的 GPU 实现，可加速广泛形式游戏中最优策略的计算。核心创新是跨 GPU 核心并行化遗憾更新和策略计算，同时仔细管理内存访问模式。 关键技术要点： - 将游戏状态和操作映射到 GPU 线程的自定义内存布局 - 批量处理信息集以最大化 GPU 利用率 - 并行计算反事实值和遗憾更新 - 通过游戏树分区实现多 GPU 扩展 - 在 Leduc Hold&#39;em 和 Limit Texas Hold&#39;em 扑克变体上进行评估 结果： - 与 CPU 实现相比，速度提高了 30 倍 - GPU 数量线性扩展，最多 8 个设备 - 内存使用量随游戏大小和信息集数量而扩展 - 解决方案质量在统计误差范围内与 CPU 基线匹配 - 成功解决了多达 1014 个状态的游戏 我认为这项工作可以使 CFR 在扑克以外的实际应用中更加实用。更快地解决大型游戏的能力为自动谈判、安全游戏和资源分配等领域开辟了可能性。多 GPU 扩展尤其有趣，因为它表明了解决更复杂游戏的潜力。 此处开发的内存优化技术也可能很好地转移到需要有效处理大状态空间的其他博弈论算法。 TLDR：GPU 加速 CFR 实现通过仔细的并行化和内存管理实现了 30 倍的加速，并具有线性多 GPU 扩展。使解决大型广泛形式游戏变得更加容易。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/</guid>
      <pubDate>Thu, 28 Nov 2024 14:08:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代扩散模型背后的理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/</link>
      <description><![CDATA[大家好， 我最近在大学里听了一些关于扩散模型的讲座。这些讲座详细解释了原始 DDPM（去拟态扩散概率模型）背后的所有数学知识（尤其是在附录中），实际上比我在网上找到的任何其他知识都要好。因此，它对于学习扩散模型背后的基础知识非常有帮助（如果您有兴趣，可以在此处的自述文件中的链接中找到幻灯片：https://github.com/julioasotodv/ie-C4-466671-diffusion-models） 但是，我正在努力寻找具有类似现代方法详细程度的资源 - 例如流匹配/整流流，用于采样的不同 ODE 求解器的工作原理等。有一些，但我发现的一切要么相当过时（比如从 2​​023 年左右开始），要么非常肤浅 - 例如对于非技术或科学受众而言。 因此，我想知道：除了原始论文之外，是否有人遇到过超出基本扩散模型的理论解释的良好汇编？我们的目标是让我的团队深入研究他们想要的实际论文，但要将其中 70% 的内容放在一个或多个像样的汇编中。 我真的相信，如今 SEO 让任何搜索都变成了一场噩梦。要么就是我的谷歌搜索技能因为某种原因而下降了。 谢谢大家！    提交人    /u/bgighjigftuik   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/</guid>
      <pubDate>Thu, 28 Nov 2024 13:27:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将数据加载到 Ray 集群中</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/</link>
      <description><![CDATA[对于那些在 AWS 上的 Ray 集群中运行 ML 训练的人，我很好奇你们采用什么方法将训练数据放入集群？ 你们如何对数据进行版本控制？ 如何避免在具有相同数据集的运行中重复下载相同的数据？ 我希望有一个流畅的过程，能够针对训练运行的特定版本的数据集，并避免重复下载它。数据版本控制应该与创建它的数据管道版本有明确的映射。如果能很好地扩展到更大的数据集，那就太好了。 渴望听到来自战壕的经验。    提交人    /u/SingularValued   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/</guid>
      <pubDate>Thu, 28 Nov 2024 12:45:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 尽管 Stella 嵌入在 MTEB 排行榜上名列前茅，为什么却没有得到更广泛的应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/</link>
      <description><![CDATA[https://huggingface.co/spaces/mteb/leaderboard 我一直在研究嵌入模型，并注意到一些有趣的事情：Stella 嵌入在 MTEB 排行榜上遥遥领先，表现优于 OpenAI 的模型，同时规模更小（1.5B/400M 参数）且使用 apache 2.0。托管它们相对便宜。 作为参考，Stella-400M 在 MTEB 上的得分为 70.11，而 OpenAI 的 text-embedding-3-large 为 64.59。1.5B 版本的得分甚至更高，为 71.19 然而，我很少看到它们在生产用例或讨论中被提及。这里有人在生产中使用过 Stella 嵌入吗？与 OpenAI 的产品相比，您在性能、推理速度和可靠性方面的体验如何？ 只是想了解为什么尽管基准测试令人印象深刻，但它们没有得到更广泛的采用，这是否是我遗漏了什么。 很想听听您的想法和经验！    提交人    /u/sdsd19   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/</guid>
      <pubDate>Thu, 28 Nov 2024 11:45:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我们如何构建 MLOps 堆栈以实现快速、可重复的实验以及 NLP 模型的顺利部署</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1trdr/p_how_we_built_our_mlops_stack_for_fast/</link>
      <description><![CDATA[大家好， 我想简要介绍一下 GitGuardian 团队如何构建适用于生产用例的 MLOps 堆栈（完整博客文章链接：https://blog.gitguardian.com/open-source-mlops-stack/）。 作为 ML 工程师，我们都知道处理数据集、模型和云资源会多么混乱。我们面临一些常见问题：跟踪实验、管理模型版本以及处理低效的云设置。 我们决定全程开源。以下是我们用来使一切顺利的方法：  DVC 用于版本控制。它就像 Git，但用于数据和模型。对于可重复性非常有帮助 - 不再需要思考如何重新创建训练运行。 GTO 用于模型版本控制。它基本上是一个轻量级的版本标签管理器，因此我们可以轻松跟踪不​​同阶段中表现最佳的模型。 Streamlit 是我们进行实验可视化的首选。它与 DVC 集成，设置交互式应用程序来比较模型轻而易举。免去了我们编写大量自定义仪表板的麻烦。 SkyPilot 为我们处理云资源。不再需要手动设置 EC2。只需几个命令，我们就可以在云端启动 GPU，从而节省大量时间。 BentoML 用于在 docker 镜像中构建模型，以用于生产 Kubernetes 集群。它使部署变得非常容易，并且与我们的版本控制系统很好地集成，因此我们可以在需要时快速切换模型。  在生产方面，我们使用 ONNX Runtime 进行低延迟推理，使用 Kubernetes 扩展资源。我们有 Prometheus 和 Grafana 来实时监控一切。 TL;DR：通过结合 DVC、GTO、Streamlit、SkyPilot、BentoML 和其他一些工具，我们成功地使我们的 MLOps 管道更加顺畅。你们都使用什么工具来简化工作流程？让我们听听你的想法！    提交人    /u/michhhouuuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1trdr/p_how_we_built_our_mlops_stack_for_fast/</guid>
      <pubDate>Thu, 28 Nov 2024 11:14:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 24 Nov 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>