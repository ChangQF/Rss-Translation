<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 06 Jan 2025 21:15:14 GMT</lastBuildDate>
    <item>
      <title>[P][D] 尽管安装了 cuda-compat，但 Cuda-torch 与旧驱动程序版本仍存在兼容性问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hv6zb4/pd_cudatorch_compatibility_issue_for_older_driver/</link>
      <description><![CDATA[您好， 我正在使用旧版本的 GPU 机器（因为我的办公室实际上没有更新操作系统和 GPU 驱动程序）。Nvidia 驱动程序版本为 470.233.xx.x，其 CUDA 版本为 11.4 过去几年，我只能使用 `torch==2.0.1`。但是，当我想为一个项目微调 Gemma 模型时出现了问题，该项目的最低要求是 torch&gt;=2.3。要运行它，我需要最新的 CUDA 版本和 GPU 驱动程序升级。 问题是我实际上无法更新任何东西。因此，我研究了 cuda-compat 方法，它是 R470 驱动程序的前向兼容层。我可以使用它来绕过要求吗？如果是这样，我的 torch2.5 仍然无法检测到任何 GPU 设备。  我需要帮助解决这个问题。拜托！    提交人    /u/The-Silvervein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hv6zb4/pd_cudatorch_compatibility_issue_for_older_driver/</guid>
      <pubDate>Mon, 06 Jan 2025 19:10:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] Jensen 不等式的交互式几何可视化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hv5qoe/p_interactive_and_geometric_visualization_of/</link>
      <description><![CDATA[大家好， 上周我一直在学习 Jensen 不等式。我对互联网上给出的大多数代数解释都不满意。因此，我写了一篇解释几何可视化的文章，到目前为止我还没有看到过类似的解释。我使用交互式可视化来展示我在脑海中如何对其进行可视化。  以下是文章 https://maitbayev.github.io/posts/jensens-inequality/ 让我知道你的想法    提交人    /u/madiyar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hv5qoe/p_interactive_and_geometric_visualization_of/</guid>
      <pubDate>Mon, 06 Jan 2025 18:20:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于法学硕士的错误信息</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1huxrd2/d_misinformation_about_llms/</link>
      <description><![CDATA[还有人对 Reddit 评论中有关 LLM 的不良信息比例感到吃惊吗？对于任何高级主题来说，这都是危险的，但围绕 LLM 的讨论似乎已经完全偏离了轨道。老实说，我觉得这有点奇怪。不良信息被疯狂地点赞，而知情的评论最多只能被忽略。令我惊讶的不是这种情况正在发生，而是它如此持续地处于“自信错误”的领域    提交人    /u/HasFiveVowels   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1huxrd2/d_misinformation_about_llms/</guid>
      <pubDate>Mon, 06 Jan 2025 12:20:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习哪些（人类）语言？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hux0p0/d_what_human_languages_to_learn/</link>
      <description><![CDATA[嗨， 这不是典型的 LLM 末日论帖子，而是 ML 特定的职业讨论。  我热衷于学习新语言（人类口语），尤其是拉丁语和罗曼语。 想知道是否有语言可以为 ML 从业者带来有趣的机会。 是否存在非英语地区对 ML 从业者有需求，但熟练的母语从业者供应不足？    提交人    /u/Amgadoz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hux0p0/d_what_human_languages_to_learn/</guid>
      <pubDate>Mon, 06 Jan 2025 11:33:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] 未来两个月的客户流失预测——需要有关数据集和模型的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hux0dx/p_churn_prediction_two_months_in_the_future_need/</link>
      <description><![CDATA[大家好！ 我最近开始从事数据科学家的工作，我被分配到一个项目来创建一个客户流失预测模型。具体来说，目标是预测未来两个月内客户流失的概率 由于我是团队中唯一的人，而且这是我第一次处理真实数据，所以我并不完全确定如何处理这个问题并做出正确的决定。 目前，我通过获取六个月的历史数据（例如，客户 X，202401，特征（与该月相关），客户流失标志，客户 X，202402，特征（与该月相关），客户流失标志等...）来构建数据集。 完成后，我使用这些分解数据并应用随机森林分类模型。但是，我最终得到的绩效指标非常差。 因此，我有几个问题：  对于包含每月历史数据的数据集，哪种模型更适合应用（在本例中，用于客户流失预测）？我应该使用聚合、滞后分解、时间序列、生存分析还是其他方法？在这种情况下，我应该如何安排数据集？ 目前，数据集包含指示客户在该月是否执行了某些操作的标志。有没有更好的方法来处理此类信息？ 您是否有任何处理不平衡数据的技巧以及需要考虑哪些指标？我在训练集上使用 SMOTE 来平衡少数类，并将 F1 分数作为指标。 如果您建议保持数据集原样或对其进行聚合，那么流失标志是否应该指行月份的提前两个月（例如，客户 x，202401，特征（与该月相关），流失标志（202403 年流失））？目前，我通过更新历史数据上个月的时变特征来重新创建目标月份（提前两个月）。  非常感谢！    提交人    /u/Sunshine1713   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hux0dx/p_churn_prediction_two_months_in_the_future_need/</guid>
      <pubDate>Mon, 06 Jan 2025 11:33:15 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 实数的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1huvvqk/discussion_embeddings_for_real_numbers/</link>
      <description><![CDATA[大家好。我正在研究一个想法，在某个时候我遇到了一个实数序列。我需要学习每个实数的嵌入。到目前为止，我尝试将标量与可学习向量相乘，但它没有起作用（如预期的那样）。那么，有没有更有趣的方法可以做到这一点？ 谢谢    提交人    /u/Dry-Pie-7398   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1huvvqk/discussion_embeddings_for_real_numbers/</guid>
      <pubDate>Mon, 06 Jan 2025 10:15:15 GMT</pubDate>
    </item>
    <item>
      <title>[R] 3D 视觉-语言-动作生成世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1husa4d/r_3d_visionlanguageaction_generative_world_model/</link>
      <description><![CDATA[  由    /u/moschles  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1husa4d/r_3d_visionlanguageaction_generative_world_model/</guid>
      <pubDate>Mon, 06 Jan 2025 05:54:27 GMT</pubDate>
    </item>
    <item>
      <title>自监督学习——n 球面上的测量分布 [D] [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hurxls/selfsupervised_learning_measure_distribution_on/</link>
      <description><![CDATA[大多数自监督学习方法（SimCLR、MoCo、BYOL、SimSiam、SwAV、MS BYOL 等）使用 n 球面超球面，提取的特征（编码器 + 投影/预测头之后）分布在该超球面中。然后，损失函数使用分布在此超球面上的特征进行损失计算。 论文，例如：  通过超球面上的对齐和均匀性理解对比表示学习，Tongzhou Wang 等人；ICML 2020 将表示与基础对齐：一种新的自监督学习方法，Shaofeng Zhang 等人；CVPR 2022 重新思考自监督学习中的均匀性度量，Xianghong Fang 等人； ICLR 2024  其他人表明这些特征分布在每个类的 n 球面上。 我们可以通过哪些不同的方式测量这些嵌入特征在这个超球面上的分布？比如说，如果我从 ImageNet/CIFAR-100 数据集中随机选择一个类，我如何测量属于这个类的所有图像在这个 n 球面上的分布？    提交人    /u/grid_world   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hurxls/selfsupervised_learning_measure_distribution_on/</guid>
      <pubDate>Mon, 06 Jan 2025 05:33:33 GMT</pubDate>
    </item>
    <item>
      <title>[N] 由 HNSW Graph 提供支持的内存向量存储</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1huqwji/n_inmemory_vector_store_powered_by_hnsw_graph/</link>
      <description><![CDATA[大家好！我现在已经在 Treds 中添加了一个完全基于命令的向量存储，由 HNSW 图提供支持，用于近似最近邻搜索。下面简要介绍一下这四个命令：  VCREATE – 初始化向量索引，指定 maxNeighbors、层因子和 efSearch 等参数。 VINSERT – 将向量插入该索引。 VSEARCH – 搜索给定向量的 k 个最近邻居。 VDELETE – 根据其 ID 从索引中删除向量。  可以在 redis-cli 中执行命令，因为 Treds 符合 RESP 标准。一个简单的会话可能看起来像 VCREATE vec 6 0.5 100 VINSERT vec 1.0 2.0 VINSERT vec 2.0 3.0 VINSERT vec 3.0 4.0 VSEARCH vec 1.5 2.5 2  这将创建一个名为 vec 的索引，插入一些 2D 向量，搜索 [1.5, 2.5] 的 2 个最近邻居。向量也可以是 N 维。 如果您之前查看过 Treds，我很乐意听听您对新向量存储添加的想法。如果您还没有，请随时查看并告诉我您是否有任何建议或问题。感谢阅读，祝您黑客愉快！ https://github.com/absolutelightning/treds?tab=readme-ov-file#vector-store https://github.com/absolutelightning/treds    提交人    /u/Fast-Tourist5742   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1huqwji/n_inmemory_vector_store_powered_by_hnsw_graph/</guid>
      <pubDate>Mon, 06 Jan 2025 04:35:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离散扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hupxdg/d_discrete_diffusion_models/</link>
      <description><![CDATA[离散分布扩散中最有前途和最新的成就是什么？ 到目前为止，我已经看过了：  https://arxiv.org/abs/2107.03006 https://arxiv.org/abs/2310.16834v2  有没有更新或更有前途的成果？    提交人    /u/ArtisticHamster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hupxdg/d_discrete_diffusion_models/</guid>
      <pubDate>Mon, 06 Jan 2025 03:42:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有任何针对 FOSS 数据进行训练的背景去除模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hum9p4/d_any_background_removal_models_trained_on_foss/</link>
      <description><![CDATA[我将为一个对版权非常严格的项目做出贡献，甚至包括所使用的 ML 工具。我发现的许多模型都没有指定它们在哪些数据上进行训练（有些模型是在通过抓取训练的模型生成的图像上进行训练的，而这在我的例子中是不允许的）。 我发现最接近的是那些仅在 DIS5K 上进行训练的 DIS5K 上进行训练的 BiRefNet 模型；这些图像“允许商业使用和修改” （大概是 CC BY 和/或 BY-SA），但数据集本身有使用条款，禁止商业使用。    提交人    /u/Sobsz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hum9p4/d_any_background_removal_models_trained_on_foss/</guid>
      <pubDate>Mon, 06 Jan 2025 00:41:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我制作了一个 CLI，使用遗传算法来改进提示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hubl11/p_i_made_a_cli_for_improving_prompts_using_a/</link>
      <description><![CDATA[        提交人    /u/jsonathan   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hubl11/p_i_made_a_cli_for_improving_prompts_using_a/</guid>
      <pubDate>Sun, 05 Jan 2025 17:04:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人类智能存在于大数据领域，还是小数据领域？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htz91k/d_does_human_intelligence_reside_in_big_data/</link>
      <description><![CDATA[当今前沿的 LLM 拥有数万亿个参数，并在 500 万亿个 token 上进行训练。 人类大脑拥有 860 亿个神经元和 100 万亿个突触。 任何人消耗的文本信息量都比 LLM 所训练的少几个数量级。但是，人眼以大约 10Mbps 的速率捕获视觉信息。加上听觉、触觉、平衡感、嗅觉等其他感官，人类儿童在生命的最初几年消耗的信息量比任何 LLM 所见过的都要多。 这似乎表明人类智能需要大数据。 但是那些从出生就失明的人怎么办？先天性聋盲（没有记录在案的病例）怎么办？    提交人    /u/Gear5th   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htz91k/d_does_human_intelligence_reside_in_big_data/</guid>
      <pubDate>Sun, 05 Jan 2025 06:07:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>