<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 18 Apr 2024 06:18:32 GMT</lastBuildDate>
    <item>
      <title>为什么 BERT 是无监督的？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6w3xv/why_is_bert_unsupervised_r/</link>
      <description><![CDATA[在预训练阶段，BERT 通过预测句子中缺失的单词（掩码语言模型）并预测两个句子是否存在来学习单词的上下文表示。原文中连续的（下一句预测）。 但是为了这些目标，我需要标签，对吧？ 例如，当提供类似的句子时  [CLS] ..[MASK]........[MASK].... [SEP] .......... 我需要判断这句话是否后面的 [SEP] 确实是后一个（ isNext 或 isNotNext）。 或者当尝试预测 [MASK] 时，我需要知道实际的掩码是什么...... 所以基本上我是从未标记的语料库创建标签，用它来计算损失并调整权重。那为什么 BERT 的预训练虽然有标签却常常被称为无监督呢？   由   提交 /u/Ok-Cheesecake-8881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6w3xv/why_is_bert_unsupervised_r/</guid>
      <pubDate>Thu, 18 Apr 2024 06:03:12 GMT</pubDate>
    </item>
    <item>
      <title>[D]、[R]，具有 4090 个 GPU 的服务器托管，用于 ML/AI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6w0xd/d_r_colocation_for_servers_with_4090_gpus_with/</link>
      <description><![CDATA[您好， 我不确定这是否是发布此内容的正确位置，但我希望如此，如果不是的话请随时让我知道这篇文章应该发布在哪里，我会在那里发布我的问题。 话虽如此，标题所说的内容已经差不多了，但是有什么我需要注意的警告吗？试图在托管中心申请机架空间以便将我的服务器放在那里？人们显然正在这样做，但我不确定如何做？我是否只需要一个机架空间，将我的服务器放在那里并完成它？ colos 不会询问服务器中有什么内容吗？正在寻找有将此类服务器放入托管服务器的经验的人，以便我可以获得一些好的建议。 第二个问题是，我目前为所有服务器拥有自己的露天设备，但我在房屋的电力有限，无法增加更多，所以这就是为什么我正在研究托管，以帮助扩展并卸载我家里的东西。我似乎很难找到一个允许至少 2 个电源并且可以容纳至少 5 个 4090 GPU（4 个也可以接受）的机箱。  有人有过可以容纳 5 个 4090（这些是常规尺寸的 - 不是鼓风机卡）包括 H12SSL 板的机箱的经验吗？我用谷歌搜索了一下，我发现实际上没有什么适合我想做的事情，所以既然我无法用谷歌搜索我想要找到的东西，我想在这里问也许是更好，因为我希望有人可以推荐一两个不错的机箱？ 提前致谢！   由   提交/u/jaytea21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6w0xd/d_r_colocation_for_servers_with_4090_gpus_with/</guid>
      <pubDate>Thu, 18 Apr 2024 05:58:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 分析数百个时间序列以通过相关性识别事件/事件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6u23x/d_analysing_100s_of_time_series_to_recognise/</link>
      <description><![CDATA[大家好，我能够通过 Prophet 和/或为我们众多大型 Kubernetes 集群生成的指标设置时间序列预测和异常检测statsforecast。 我计划接下来处理的用例是连续分析 100 个时间序列值，然后通过相关性识别基于时间的事件 例如：20 个节点故障的单独警报变成通过时间序列关联的单个事件。 这也可能使我们能够识别与不相关组件的因果关系。 关于我如何实现这一目标，有什么想法吗？请对数学保持温和，因为我是一名 SRE，在这里扮演着 ML 工程师的角色:)  ​    ;由   提交/u/OmarasaurusRex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6u23x/d_analysing_100s_of_time_series_to_recognise/</guid>
      <pubDate>Thu, 18 Apr 2024 04:01:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练 VQGAN 但 GAN 损失持续上升</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6rtlt/p_training_a_vqgan_but_gan_loss_keeps_going_up/</link>
      <description><![CDATA[     &lt; td&gt; https://preview.redd.it/h13z13eu a5vc1.png?width=640&amp;format=png&amp;auto=webp&amp;s=397d5127453b2f4a1d6f6df28fb5fc 8a2f2f0cff 我认为VQ 损失和感知损失看起来很正常，但我觉得很难理解为什么判别器会走向完全不同的方向……以前有人见过类似的事情吗？ 更多细节：我正在训练 vqgan imagenet 来自论文 Taming Transformers for High-Resolution Image Synthesis   由   提交/u/darthjaja6   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6rtlt/p_training_a_vqgan_but_gan_loss_keeps_going_up/</guid>
      <pubDate>Thu, 18 Apr 2024 02:02:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] [2404.10667] VASA-1：实时生成逼真的音频驱动的说话面孔</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6q61y/r_240410667_vasa1_lifelike_audiodriven_talking/</link>
      <description><![CDATA[ 由   提交/u/s6x  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6q61y/r_240410667_vasa1_lifelike_audiodriven_talking/</guid>
      <pubDate>Thu, 18 Apr 2024 00:41:07 GMT</pubDate>
    </item>
    <item>
      <title>[N] 美联储任命“人工智能毁灭者”来管理美国人工智能安全研究所</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6nps8/n_feds_appoint_ai_doomer_to_run_us_ai_safety/</link>
      <description><![CDATA[https://arstechnica.com/tech-policy/2024/04/feds-appoint-ai-doomer-to-run-us-ai-safety-institute/ 文章简介： 被任命为 AI 安全负责人的是 Paul Christiano，他是前 OpenAI 研究员，开创了一种名为基于人类反馈的强化学习的基础 AI 安全技术（ RLHF），但也因预测“人工智能发展有 50% 的机会以‘厄运’而告终”而闻名。尽管克里斯蒂安诺的研究背景令人印象深刻，但一些人担心，任命所谓的“人工智能厄运者”会带来灾难。 NIST 可能冒着鼓励非科学思维的风险，许多批评家认为这些思维纯粹是猜测。   由   提交/u/bregav  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6nps8/n_feds_appoint_ai_doomer_to_run_us_ai_safety/</guid>
      <pubDate>Wed, 17 Apr 2024 22:49:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有办法确定模型学习的表示是球形还是双曲形？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6kt3a/d_is_there_a_way_to_determine_if_the/</link>
      <description><![CDATA[标题。有没有办法确定特征提取器针对已训练/将测试的一组示例学习的嵌入的球形度或双曲度？ 我是深度学习中的几何新手。如果有人也能给我指出一篇论文或一本书来开始这方面的工作，那就太好了。提前致谢。   由   提交/u/Mad_Scientist2027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6kt3a/d_is_there_a_way_to_determine_if_the/</guid>
      <pubDate>Wed, 17 Apr 2024 20:49:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] RuleOpt：基于优化的分类规则学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6ix86/r_ruleopt_optimizationbased_rule_learning_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2104.10751 软件包： https://github .com/sametcopur/ruleopt 文档： https://ruleopt.readthedocs .io/ RuleOpt是一种基于优化的规则学习算法，专为分类问题而设计。 RuleOpt注重可扩展性和可解释性，利用线性编程进行规则生成和提取。 Python库ruleopt能够从集成模型中提取规则，并且还实现了一种新颖的规则生成方案。该库确保与现有机器学习管道的兼容性，并且对于解决大规模问题特别有效。 以下是ruleopt的一些亮点：  高效的规则生成和提取：利用线性编程进行可扩展的规则生成（独立的机器学习方法）以及从训练有素的随机森林和增强模型中提取规则。 可解释性：通过将成本分配给规则来优先考虑模型透明度，以实现与准确性的理想平衡。 与机器学习库集成：促进与知名 Python 库 scikit 的顺利集成-learn、LightGBM 和 XGBoost 以及现有的机器学习管道。 广泛的求解器支持：支持多种求解器，包括 Gurobi、&lt; em&gt;CPLEX 和 OR-Tools。    由   提交/u/zedeleyici3401   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6ix86/r_ruleopt_optimizationbased_rule_learning_for/</guid>
      <pubDate>Wed, 17 Apr 2024 19:34:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] LSTM时间序列预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6igqk/d_lstm_time_series_forecasting/</link>
      <description><![CDATA[        由   提交 /u/StressAccomplished26   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6igqk/d_lstm_time_series_forecasting/</guid>
      <pubDate>Wed, 17 Apr 2024 19:15:46 GMT</pubDate>
    </item>
    <item>
      <title>[D]问题：时间序列解码到非时间潜在空间？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6fa97/d_question_timeseries_decoding_to_nontemporal/</link>
      <description><![CDATA[您好！我是一名计算神经科学研究员，希望将一些现代机器学习技术应用于功能磁共振成像时间序列数据。我收集了一组高维 4D fMRI 时间序列数据，这些数据是在受试者定期观察 COCO 的自然图像时收集的。我们目前拥有采用预处理“快照”的解码模型。该时间序列数据被扁平化为在观察图像的短时间内聚合的激活模式，并使用一些机器学习模型来解码和重建来自大脑的图像内容。 （请参阅我的一些最近的工作）。 我很好奇存在什么样的机器学习技术可能能够处理时间序列数据本身，而不必将时间序列折叠为单个快照来执行我们的解码过程。我设想的是一个模型（可能是一个变压器），它可以将高维多通道时间序列作为输入，并输出与图像刺激相对应的扁平潜在表示（例如 CLIP 向量），甚至是由已知的规则间隔（正如我们在不同图像呈现的数据中所具有的那样）。据我所知，时间序列数据的机器学习的大部分工作都是预测，但我想要的是静态（或可能重复的）输出。我希望更详细的时间序列数据将具有额外的信号，从而提高 fMRI 视觉解码的解码性能。 ML 领域是否有任何现有的工作已经解决了类似的问题？  &gt;   由   提交 /u/reesespike   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6fa97/d_question_timeseries_decoding_to_nontemporal/</guid>
      <pubDate>Wed, 17 Apr 2024 17:08:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在交叉注意力中，为什么 Q 取自解码器，K 取自编码器输出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</link>
      <description><![CDATA[我查了很多地方但找不到答案。如果我们分别将 Q 和 K 来自编码器和解码器，会发生什么？会有什么不同吗？   由   提交/u/shuvamg007  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</guid>
      <pubDate>Wed, 17 Apr 2024 14:49:10 GMT</pubDate>
    </item>
    <item>
      <title>[D]视觉语言模型中视觉嵌入如何与语言嵌入空间共存？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</link>
      <description><![CDATA[大家好！我很高兴能讨论大视觉语言模型 (LVLM)。由于我们可能是最大的法学硕士社区，我认为这个频道将是开始这次对话的完美场所。此外，关于将视觉和语言嵌入结合起来的内容并不多。 LVLM 的一些背景：它们通常由图像的视觉编码器、文本的常规标记器、像投影层这样的投影层组成。 MLP 将视觉特征与文本嵌入空间对齐，最后合并图像和文本嵌入以发送到 LLM 模型中。输入包括文本和图像，而输出是文本，使其成为多模式法学硕士。查看 LLaVA 论文中的图表，了解直观的细分： https://preview.redd.it/l222askgu1vc1.png?width=1607&amp;format=png&amp; ;auto=webp&amp;s=ef011e16301c22b4751d8d0a8f3698f70e3ffd26 从像 CLIP ViT 这样的视觉编码器开始，模型从图像中学习视觉信息，然后使用 MLP 将其投影到 LLM 的嵌入空间上。该论文将这种特征称为对齐。我很好奇视觉嵌入如何与文本嵌入交互，因此我尝试使用 PCA 以 3D 方式可视化它们。 例如，采用 llava-7B 模型 - 它使用 llama-7B 后端和32k 词汇量和 4096 个维度，使得嵌入大小为：[32000,4096]。我使用了一个简单的提示，“向我解释一下这张图片”。使用猫的图片来查看嵌入如何出现在我们的空间中。 https://preview.redd.it/032oy0yn u1vc1.png?width=662&amp;format=png&amp;auto=webp&amp;s=d037bbecc976392e159a1c1bde775ef1e148 488d 添加视觉标记改变了动态。每个图像转换为 576 个形状的视觉标记 [576,4096]。查看包含这些标记时绘图如何调整： https ://preview.redd.it/vdeacylwu1vc1.png?width=566&amp;format=png&amp;auto=webp&amp;s=42441b4fd515cee916b40243429b4aa6820b998c 那我觉得怎么样？ 首先，我们不会直接将视觉标记转换为文本。最近的一篇 Google 论文尝试过，发现这不是最好的方法。视觉推理似乎徘徊在文本嵌入空间附近，可能是因为图像的信息更密集，需要更多的标记来表示视觉概念。 其次，这种设置目前看来是正确的。视觉标记与文本标记一起，将图像衍生的上下文添加到 LLM，使其能够“看到”图像。 最后，尽管 llava 在视觉推理的一些基准测试中表现良好，但它可能还不是最有效的图像表示方法。最近的一些研究谈到了注意力稀疏现象，尤其是 LVLM 中的视觉标记。我们很幸运，因为注意力算法只关注有意义的视觉标记并忽略噪音。 你觉得怎么样？谢谢阅读。 :-)   由   提交/u/E-fazz  /u/E-fazz  reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</guid>
      <pubDate>Wed, 17 Apr 2024 14:41:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究中数学和算法哪个优先？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</link>
      <description><![CDATA[我现在正在学习扩散背后的方法（DDPM、基于分数的方法和其他方法）。我想知道研究人员到底是如何想出这个想法的？ 发明新方法是这样的吗？ 1.我们想要制作更好的图像生成器。 2. 哦，数据永远不够...... 3. 让我们乘以数据 - 通过添加一些噪声损坏 4. 这个效果很好，如果我们制作一个去噪网络怎么办？ 5. 如果我们建立一个由纯噪声生成图像的网络会怎么样？ 6. 这不行，如果我们做更小的去噪步骤怎么办？ 7. 这有效！现在，让我们创建一些关于它为何起作用的理论。 8.写论文 或者类似的东西？ 1.我们想要制作更好的图像生成器。 2.我们知道“非平衡热力学”非常好，想尝试以某种方式应用它 3. 我们以某种方式想出了一种依赖于该理论的数学的算法 4. 它有效！ 5. 我们写论文。 通常哪个先出现？数学还是算法？   由   提交/u/Deep-Station-1746   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</guid>
      <pubDate>Wed, 17 Apr 2024 08:22:11 GMT</pubDate>
    </item>
    <item>
      <title>AI/ML 数据中心的未来将是 100 台甚至 1000 台服务器像一个巨型加速器一样运行 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</link>
      <description><![CDATA[在服务器公司 Gigabyte 的网站上看到了这个信息丰富的视频 (https://youtu.be/2Q7S-CbnAAY?si=DJtU2mQ_ZKRZ83Nf），简而言之，服务器品牌现在将完整的服务器集群运送到数据中心，而不是单个服务器机器。在此所示的示例中，它有 8 个机架（另外一个用于管理和网络），每个机架中有 4 台相同型号的服务器，以及 4 个相同的超高级 GPU每个服务器中的模型。为您计算一下，每个集群有 32 台服务器或 256 个 GPU 加速器。请注意，所有服务器和 GPU 都必须是相同的型号，因为它们的连接方式基本上是作为一台单独的机器运行。 这很可能是标准构建块的原因。所有人工智能数据中心的特点是，我们现在利用大型数据集训练人工智能的方式，参数数量达到数十亿，甚至数万亿。对于为我们带来 ChatGPT 及其同类产品的法学硕士来说尤其如此。以任何效率处理这些数万亿个参数的唯一方法是通过我们以前从未见过的规模的并行计算。因此，这个大胆的新概念将数百甚至数千台服务器连接在一起，因此它们基本上是一台巨型服务器，加载了 Nvidia 或其他品牌的数千个 GPU。真正令人着迷的东西，我还没有看到目前为人工智能计算的未来提出的任何其他规模的东西。 这是视频中介绍的集群的网站：https://www.gigabyte.com/Industry-Solutions/giga-pod-as-a-service ?lan=en   由   提交/u/Low_Complaint2254   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</guid>
      <pubDate>Wed, 17 Apr 2024 06:16:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>