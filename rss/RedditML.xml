<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 02 May 2024 12:25:53 GMT</lastBuildDate>
    <item>
      <title>用于将时间序列数据集分割为固定大小的窗口并在窗口中具有最主要标签的代码。我写的代码做对了吗？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ciecap/code_for_segmenting_a_time_series_dataset_into/</link>
      <description><![CDATA[import numpy as np import scipy.stats as stats import pandas as pd def segment_data_with_labels(features, labels, frame_size, hop_size): result_frames = [] result_labels = [] for i in range(0, len(features) - frame_size+1, hop_size): segment = features.iloc[i: i+frame_size] if len(segment) &lt; frame_size: break # 如果segment小于frame_size，则退出循环 label = stats.mode(labels.iloc[i: i+frame_size])[0][0] result_frames.append(segment.values) # 将segment转换为NumPy 数组 result_labels.append(label) result_frames = np.asarray(result_frames) result_labels = np.asarray(result_labels) 返回 result_frames, result_labels   &amp; #32；由   提交/u/imeeeenne  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ciecap/code_for_segmenting_a_time_series_dataset_into/</guid>
      <pubDate>Thu, 02 May 2024 12:24:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么大三学生（本科生或一二年级博士生）在ICML、ICLR、NeurIPS等主要机器学习会议上有这么多论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cidsz7/d_why_do_juniors_undergraduates_or_first_to/</link>
      <description><![CDATA[大家好，今天ICML成绩出来了，恭喜所有在这里接受论文的同学。我自己不是学者，但有时我会为了工作而阅读这些会议上的论文，这真的很有趣。我只是有一个问题：为什么这些会议上大三的论文这么多？我认为这是你在 5 年博士学位期间必须学习的东西，而且几乎只能在博士学位的最后几年才能实现。另外，我听说要进入美国顶尖的博士项目，你需要事先写一些论文。那么，如果一个大三学生能这么早发表论文，为什么还要花5年时间攻读博士学位呢？   由   提交 /u/ShiftStrange1701   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cidsz7/d_why_do_juniors_undergraduates_or_first_to/</guid>
      <pubDate>Thu, 02 May 2024 11:56:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] Seq2Seq 模型是否适用于拼写纠正？如果是的话，为什么我会弄错？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cibpe8/d_does_seq2seq_model_work_for_spelling_correction/</link>
      <description><![CDATA[我正在使用 seq2seq 模型来预测或纠正产品名称的拼写，我确实有产品名称的数据集及其拼写错误和更正版本（他们确实也包含一些特殊字符）。我已经在几个时期训练了这些数据并看到了一些输出。但我给用户输入，它没有按预期进行预测。 然后，在训练模型并使用此代码之后： for seq_index in range(1 , 50): input_seq = encoder_input_data[seq_index : seq_index + 1] returned_sentence =decode_sequence(input_seq) print(&quot;-&quot;) print(&quot;输入句子:&quot;, input_texts[seq_index]) #用char打印输入序列！ print(“解码的句子：”，decoded_sentence)  我得到了很好的输出，例如： 输入句子：Fluidic WorkCation 解码的句子：Fluidic Worksation输入句子：Li@uid Handler，Biomek FXp DuaO 解码句子：Liquid Handler，Biomek NXp Mult  然后，如果我尝试提供用户输入并让模型进行预测，我确实会得到一些像这样的文本 输入句子：系统解码句子：&#39;Gamma Counter/Rotor - Water Machine System, Automated Parallell\n&#39;  这很远根据它所学到的知识，但我使用了相同的编码器和解码器模型代码。我想先知道这个 seq2seq 模型是否适用于这些场景。    由   提交/u/No-Purchase6293   reddit.com/r/MachineLearning/comments/1cibpe8/d_does_seq2seq_model_work_for_spelling_ Correction/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cibpe8/d_does_seq2seq_model_work_for_spelling_correction/</guid>
      <pubDate>Thu, 02 May 2024 09:55:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何使用 MMOCR 或 MMDET 模型检测文本方向？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ciaezc/d_how_can_i_detect_the_text_orientation_using/</link>
      <description><![CDATA[      我的训练图像的文本以不同方向出现在图像上。因此，我不知道它们的原始方向是什么，因为例如 DBNetPP 不会以自然方向顺序返回拐角处的 bbox 角度。我该如何解决这个问题？我尝试过其他预训练的检测模型，但它们也没有这样做，可能是因为它们没有接受旋转图像的训练。如何解决此问题？  https://preview.redd.it/tvq6fp9k3zxc1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=ecf3f3e757e6450e34c1257f9eb8e0fec4ce7bba https://preview.redd.it/yea66hdl3zxc1.png?width=1000&amp;format=png&amp; ;自动= webp&amp;s=4eafb6d4354c6a0d851d6b5fad456f99441d9bc2   由   提交 /u/tmargary   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ciaezc/d_how_can_i_detect_the_text_orientation_using/</guid>
      <pubDate>Thu, 02 May 2024 08:22:48 GMT</pubDate>
    </item>
    <item>
      <title>这家伙用什么文字转语音？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci9x9p/what_text_to_speech_does_this_guy_use_r/</link>
      <description><![CDATA[https://youtu.be/SOovRRz0Iuo ?si=LqRxzyj-cSxLeBYM 他使用什么文字转语音？他最近经常出现在我的视频中。这类视频通常听起来太机械化，但这个视频实际上还不错。你认为它是什么？   由   提交 /u/Inner-Wishbone-3877   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci9x9p/what_text_to_speech_does_this_guy_use_r/</guid>
      <pubDate>Thu, 02 May 2024 07:47:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 二元分类器分数分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci8pft/d_binary_classifier_scores_distribution/</link>
      <description><![CDATA[嗨，当我绘制二元分类器测试分数的直方图时，它们在最后一栏中聚集过多，这使得阈值难以设置，因为它变得太离散了。有人知道有什么方法可以确保分类器分数直方图分布更均匀吗？理想的可靠性图是完全单调的，并且尽可能接近身份线。尝试过普拉特缩放和等渗回归，但没有成功。 还想知道是什么决定了可能的不同分类器分数值的数量？ 任何帮助都非常欢迎！    提交人    /u/Loose-Event-7196   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci8pft/d_binary_classifier_scores_distribution/</guid>
      <pubDate>Thu, 02 May 2024 06:26:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最适合的会议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci8dqj/d_best_suited_conferences/</link>
      <description><![CDATA[我的 icml 提交被拒绝了，分数为 6655，心碎之余，我可以重新提交哪些高接受率会议？我只是想把它放进去然后继续前进。    由   提交 /u/One-Blueberry4699   /u/One-Blueberry4699 reddit.com/r/MachineLearning/comments/1ci8dqj/d_best_suited_conferences/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci8dqj/d_best_suited_conferences/</guid>
      <pubDate>Thu, 02 May 2024 06:05:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 商业环境中聊天机器人管道的现状？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci5x4l/d_current_state_of_chatbot_pipelines_in/</link>
      <description><![CDATA[大家好，我目前的任务是研究管道，为我的大学构建本地自定义聊天机器人。我一直在阅读 RAG、Rasa、Dialogflow 等方法以及 LangChain、Ragflow、KRAGEN 等特定管道，并获得了一些测试结果。然而，我想了解哪些管道和方法对于构建聊天机器人最有效，特别是在商业环境中。我非常感谢您提供的所有信息！   由   提交 /u/ghosthunterk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci5x4l/d_current_state_of_chatbot_pipelines_in/</guid>
      <pubDate>Thu, 02 May 2024 03:41:51 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无需训练的图神经网络和标签作为特征的力量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ci1qqr/r_trainingfree_graph_neural_networks_and_the/</link>
      <description><![CDATA[ 由   提交/u/joisino  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ci1qqr/r_trainingfree_graph_neural_networks_and_the/</guid>
      <pubDate>Thu, 02 May 2024 00:14:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] Pytorch 的现代最佳编码实践（用于研究）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chxpka/d_modern_best_coding_practices_for_pytorch_for/</link>
      <description><![CDATA[大家好，我从 2019 年开始使用 Pytorch，在那段时间它发生了很大的变化（特别是自从 Huggingface 以来）。  您有推荐的现代指南/style-docs/example-repos 吗？例如，命名张量是一种好的/常见的做法吗？推荐使用 Pytorch Lightning 吗？目前最好的配置管理工具是什么？您多久使用一次 torch.script 或 torch.compile？   由   提交 /u/SirBlobfish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chxpka/d_modern_best_coding_practices_for_pytorch_for/</guid>
      <pubDate>Wed, 01 May 2024 21:24:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 模型崩溃不可避免吗？通过积累真实数据和合成数据来打破递归魔咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chu565/r_is_model_collapse_inevitable_breaking_the_curse/</link>
      <description><![CDATA[ 由   提交/u/RSchaeffer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chu565/r_is_model_collapse_inevitable_breaking_the_curse/</guid>
      <pubDate>Wed, 01 May 2024 18:58:20 GMT</pubDate>
    </item>
    <item>
      <title>[P]我转载了Anthropic最近的可解释性研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chsg42/p_i_reproduced_anthropics_recent_interpretability/</link>
      <description><![CDATA[当能力研究发展得像目前一样快时，并没有多少人关注 LLM 可解释性研究，但可解释性确实很重要，在我看来意见，确实有趣又令人兴奋！近几个月来，Anthropic 取得了很多突破，其中最大的突破是“迈向单义性”。基本思想是，他们找到了一种训练稀疏自动编码器以基于变压器激活生成可解释特征的方法。这使我们能够在推理过程中查看语言模型的激活，并了解模型的哪些部分最负责预测每个下一个标记。对我来说真正突出的一点是，他们训练的自动编码器实际上非常小，并且不需要大量计算即可工作。这让我产生了尝试通过在我的 M3 Macbook 上训练模型来复制该研究的想法。经过大量阅读和实验，我得到了相当不错的结果！我在我的博客上写了一篇更深入的文章：  https://jakeward.substack.com/p/monosemanticity-at-home-my-attempt 我现在也在使用这项技术开展一些后续项目作为可以在 Colab 笔记本中运行以使其更易于访问的最小实现。如果您阅读我的博客，我很乐意听到任何反馈！   由   提交 /u/neverboosh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chsg42/p_i_reproduced_anthropics_recent_interpretability/</guid>
      <pubDate>Wed, 01 May 2024 17:51:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] KAN：柯尔莫哥洛夫-阿诺德网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chrafb/r_kan_kolmogorovarnold_networks/</link>
      <description><![CDATA[      论文：https://arxiv. org/abs/2404.19756 代码：https://github.com /KindXiaoming/pykan 快速介绍：https:/ /kindxiaoming.github.io/pykan/intro.html 文档：https://kindxiaoming.github.io/pykan/ 摘要：  受到 Kolmogorov-Arnold 表示的启发定理中，我们提出Kolmogorov-Arnold Networks（KAN）作为多层感知器（MLP）的有希望的替代品。 MLP 在节点（“神经元”）上具有固定激活函数，而 KAN 在边缘上具有可学习激活函数(“权重”)。 KAN 根本没有线性权重——每个权重参数都被参数化为样条函数的单变量函数所取代。我们证明，这种看似简单的改变使得 KAN 在准确性和可解释性方面优于 MLP。就准确性而言，在数据拟合和 PDE 求解中，较小的 KAN 可以比较大的 MLP 获得可比或更好的准确性。从理论上和经验上来说，KAN 比 MLP 拥有更快的神经尺度法则。为了可解释性，KAN 可以直观地可视化，并且可以轻松地与人类用户交互。通过数学和物理领域的两个例子，KAN 被证明是帮助科学家（重新）发现数学和物理定律的有用合作者。总之，KAN 是 MLP 的有希望的替代品，为进一步改进当今严重依赖 MLP 的深度学习模型提供了机会。  https://preview.redd.it/r7vjmp31juxc1.png?width=2326&amp;format=png&amp;auto=webp&amp; amp;s= a2c722cf733510194659b9aaec24269a7f9e5d47   由   提交 /u/SeawaterFlows   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chrafb/r_kan_kolmogorovarnold_networks/</guid>
      <pubDate>Wed, 01 May 2024 17:03:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] TensorDock — GPU 云市场，H100s 起价 2.49 美元/小时</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/</link>
      <description><![CDATA[大家好！我是来自 TensorDock 的 Jonathan，我们正在构建一个云 GPU 市场。我们希望让 GPU 真正变得价格实惠且易于使用。 我曾经在中学时在自托管服务器上启动了网络托管服务。但构建服务器与销售云不同。有很多开源软件可以管理你的家庭实验室的副业项目，但没有任何东西可以将其商业化。 大型云提供商收取高得离谱的价格 - 如此之高以至于他们经常可以偿还他们的硬件在 6 个月内，全天候 24 小时使用。 我们正在构建允许任何人成为云的软件。我们希望达到这样一个目标：任何[插入容量过剩的公司、数据中心、云提供商]都可以在我们的节点上安装我们的软件并赚钱。他们可能不会在 6 个月内偿还硬件费用，但他们不需要做繁重的工作 - 我们处理支持、软件、付款等。 反过来，您可以访问真正独立的云：来自世界各地的供应商提供的 GPU，这些供应商在价格和可靠性方面相互竞争。 到目前为止，我们已经采用了相当多的 GPU，其中包括200 个 NVIDIA H100 SXM，售价仅为 2.49 美元/小时。但我们还有 A100 80G 起价为 1.63 美元/小时，A6000 起价为 0.47 美元/小时，A4000 起价为 0.13 美元/小时，等等。因为我们是一个真正的市场，所以价格会随着供需而波动。 所有这些都可以在纯 Ubuntu 22.04 中使用，或者预装流行的 ML 软件包 - CUDA、PyTorch、TensorFlow 等，并且所有这些都由托管我们已经仔细审查过的矿场、数据中心或企业网络。 如果您正在为下一个项目寻找托管服务，请尝试一下！很高兴提供测试积分，请发送电子邮件至 [jonathan@tensordock.com](mailto:jonathan@tensordock.com）。如果您最终决定尝试我们，请在下面提供反馈[或直接提供！]:) ​ 部署 GPU 虚拟机：https://dashboard.tensordock.com/deploy 仅 CPU 虚拟机：https://dashboard.tensordock.com/deploy_cpu 申请成为主机： https://tensordock.com/host   由   提交/u/jonathan-lei   reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1chiu4a/d_tensordock_gpu_cloud_marketplace_h100s_from/</guid>
      <pubDate>Wed, 01 May 2024 10:31:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>