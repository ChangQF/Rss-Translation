<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Wed, 20 Mar 2024 09:13:52 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么学术论文的可读性持续不佳？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</link>
      <description><![CDATA[  其中一些期望是不可避免的。论文绝对必须假设该领域特有的广泛背景知识和词汇——包括每篇论文对该领域的基本介绍都是多余的。有时论文假设了太多背景，但作为作者或读者很难判断，尤其是因为人们知道的事情会随着时间和不同背景而变化。 参考 在过去的3年里，为了拓宽我对ML领域的理解，我读了很多论文，但没有考虑过专注于某个领域特定领域的特定问题。   这使我能够理解学术写作的结构、机器学习的基本概念以及与以前的知识相比相对宝贵的见解。过去，但当我遇到新论文时，有些论文仍然很冗长，由于学术论文性质的可读性而让我烦恼。 这强烈要求我浏览一些内容，这对纯粹的学术论文是有害的。与非学术书籍相比，专注于阅读文档（我并不认为所有书籍都像个人畅销书一样出色，但我想强调学术和非学术写作之间的结构差异。） 问题是，为什么这个惯例不能顺利地改变到更好的方向，对现有的和新的研究人员有帮助？   由   提交/u/Mundane_Definition_8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj92ht/d_why_the_readability_of_academic_papers_are/</guid>
      <pubDate>Wed, 20 Mar 2024 09:04:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为先进汽车和损坏检测系统项目选择正确的方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj91km/d_choosing_the_right_approach_for_advanced_car/</link>
      <description><![CDATA[    &lt; /a&gt;  您好，我现在是一名学生，我需要您的意见。我的项目的目标是创建一个或多个模型，可以识别物体是否是汽车，从错误的物体中识别正确的（汽车），并最终识别汽车的损坏情况。我目前有两个选择（见图）：  使用多输出学习方法。这意味着我必须用自己的数据训练每个模型并使用迁移学习，因为目标是相互关联的。 使用多任务学习方法，我将使用 MobileNet 等骨干网并进行2 个模型。  我真的很想得到一些关于这个模型的反馈，无论其中之一是一个好的方法，还是我应该优化/更改什么。 编辑：会最好对损坏使用全景分割，因为我需要单独保存每个损坏？ https://preview.redd.it/r6qnbwf7ggpc1.png?width=1022&amp;format=png&amp;auto=webp&amp;s=8df55dd9ebb4469c79648bf63 0853085a6c989ad   由   提交 /u/HHDP1   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj91km/d_choosing_the_right_approach_for_advanced_car/</guid>
      <pubDate>Wed, 20 Mar 2024 09:02:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基于 RAG 的聊天对话的最佳实践：何时进行检索？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj8w3k/d_best_practices_for_ragbased_chat_conversations/</link>
      <description><![CDATA[我过去构建过一些 RAG 应用程序，但直到现在才必须使用聊天界面。现在，如果我想启用对话，我面临的问题是决定何时继续对话以及何时将输入转移到下一个主题。详细地说： 用户可能会提出一个需要检索上下文信息的问题——通常的 RAG 情况。 现在他们根据最后一个问题继续对话（“你能让答案多一点吗？”、“第一项是什么意思？”等等） 很简单：我可以保留旧的上下文，也可以只检索新的上下文每次都基于我的整个谈话。我可能更喜欢第二种选择，让不断增长的对话完善上下文。显然行不通的是仅对最后的用户输入进行检索。很有可能缺少上下文。 但是如果用户现在问一个不同的问题，例如：“谢谢。”现在什么是......？”。如果我保留上述方法，检索到的上下文可能会起作用，但现有的转换很可能会“污染”上下文。我的提示足够多，以至于检索步骤无法检索相关上下文。 从我的头脑中，我会假设我“分类”了（自己的模型，只需询问主要的法学硕士，无论如何）每对（现有的对话，最后的输入）进入延续与新的开始，并相应地调整检索。这是标准做法吗？还有更好的方法吗？   由   提交/u/bbu3  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj8w3k/d_best_practices_for_ragbased_chat_conversations/</guid>
      <pubDate>Wed, 20 Mar 2024 08:50:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 增加最大序列长度对 GPU 要求的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj8dyj/d_impact_of_increasing_the_maximal_sequence/</link>
      <description><![CDATA[嘿，我想知道如果我将训练数据序列长度加倍，会对我的 VRAM 要求产生什么影响。 4000 到 8000 我特别看到了一个计算器，但我不知道它有多值得信赖。  有什么见解吗？ 具体来说，我将使用 QLORA 和 QLORA 来微调 Mistral 7 B 模型。目前正在计算我需要多少 GPU   由   提交/u/Brighton_Beach  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj8dyj/d_impact_of_increasing_the_maximal_sequence/</guid>
      <pubDate>Wed, 20 Mar 2024 08:10:52 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]GNN应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj6z80/discussion_gnn_applications/</link>
      <description><![CDATA[我听说过应用图神经网络的相当标准的方法（化学、流量、社交网络），但它们是一个很酷的类的模型，您熟悉 GNN 的哪些应用而不是上面提到的标准应用？   由   提交/u/Radiant_Walrus3007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj6z80/discussion_gnn_applications/</guid>
      <pubDate>Wed, 20 Mar 2024 06:29:06 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 为什么要增加 CNN 层的通道大小？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj6gn5/discussion_why_increase_channel_size_in_cnn_layers/</link>
      <description><![CDATA[在我看到的大多数 CNN 架构中，他们将通道数从 1 或 3 增加到 16、32 甚至 64。难道不是减少通道数吗？维数？   由   提交/u/Radiant_Walrus3007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj6gn5/discussion_why_increase_channel_size_in_cnn_layers/</guid>
      <pubDate>Wed, 20 Mar 2024 05:53:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 最近的“LLM工程师”没有NLP背景的情况常见吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bj0y3h/d_is_it_common_for_recent_llm_engineers_to_not/</link>
      <description><![CDATA[过去几周，我参加了一些聚会和社交活动，在那里我遇到了很多声称他们“与法学硕士合作”的人。我个人对它们没有太多经验，并且对更“经典”的领域进行了研究。 NLP（ELMo 和 BERT 在我做研究时是重大公告），现在主要作为工程师在业界工作。 我经常注意到，当我尝试谈论 LLM 研究模式或应用程序和那些我称之为经典方法的人通常似乎不知道我在说什么。 我不是在谈论研究人员，显然如果你正在与法学硕士进行实际研究，我假设您已经在该领域工作了一段时间。如今，LLM 和 NLP 似乎被分开对待。好奇其他人的想法。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bj0y3h/d_is_it_common_for_recent_llm_engineers_to_not/</guid>
      <pubDate>Wed, 20 Mar 2024 00:59:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] 通过融合张量运算流实现无静态图的最佳性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biy9aj/p_optimal_performance_without_static_graphs_by/</link>
      <description><![CDATA[当前机器学习研究最重要的方面之一是发现可有效扩展计算资源的模型架构。由于有效利用当代硬件，变压器已成为主要架构。然而，它们不会根据任务的复杂性来调整计算图，因此需要针对不同复杂性的任务使用不同的版本。这种方法与拥有一个能够持续学习（终身学习）的模型同时保持简单任务高效的目标不符。我认为迫切需要进一步探索动态架构，其中计算图在运行时根据上下文线索进行调整。 虽然 BranchyNet 等几篇论文已经深入研究了这种方法，有选择地跳过层当以高置信度生成令牌时，它们的实现可能不如 JAX 中实现的标准静态转换器那么高效或优化。尽管如此，将基于静态图的框架的速度与 PyTorch 之类的灵活性相结合不是很好吗？这正是我最近在 Burn 上所做的工作。 Burn 是一个急切的框架，具有独特的特征：它是用 Rust 编写的，大量使用类型系统来捕获张量动态生命周期，生成高度优化的带有即时编译器的 GPU 内核。 PyTorch 还尝试创建一个即时编译器来捕获部分图，但结果并不那么乐观，最近的多后端 Keras 3 基准测试就证明了这一点。因此，在 PyTorch 中实现高性能可能需要许多自定义内核。不幸的是，处理高级概念、数学公式和理论通常需要进入 CUDA 和非常低级的编程才能有效地进行经验测试。 Burn 的愿望是使研究和应用成为可能。快速部署最灵活的架构，无需 GPU 编程即可实现最先进的性能。虽然在获得最快框架的称号之前还需要做更多的工作，但我们正在不断添加更多优化，并且基础已经稳固。 博客：https://burn.dev/blog/fusion-tensor-operation-streams/ 代码：https://github.com/tracel-ai/burn/tree/main/crates/burn-fusion&lt; /p&gt;   由   提交/u/ksyiros  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biy9aj/p_optimal_performance_without_static_graphs_by/</guid>
      <pubDate>Tue, 19 Mar 2024 23:03:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] TacticAI：足球战术人工智能助手</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biw8qn/r_tacticai_an_ai_assistant_for_football_tactics/</link>
      <description><![CDATA[博客文章：https://deepmind.google/discover/blog/tropicai-ai-assistant-for-football-tropics/ 自然论文：https://www.nature.com/articles/s41467-024-45965-x 摘要 识别对手球队实施的关键战术模式并制定有效的应对措施是现代足球的核心。然而，通过算法实现这一点仍然是一个开放的研究挑战。为了解决这一未满足的需求，我们提出了 TacticAI，这是一款与利物浦足球俱乐部领域专家密切合作开发和评估的人工智能足球战术助手。我们专注于分析角球，因为它们为教练提供了最直接的干预和改进机会。 TacticAI 结合了预测和生成组件，使教练能够有效地采样和探索每个角球例程的替代球员设置，并选择那些预测成功可能性最高的球员。我们在许多相关基准任务上验证 TacticAI：预测接球手和投篮尝试以及建议球员位置调整。 TacticAI 的实用性已通过利物浦足球俱乐部足球领域专家进行的定性研究得到验证。我们表明，TacticAI 的模型建议不仅与真实战术无法区分，而且在 90% 的情况下比现有战术更受青睐，并且 TacticAI 提供了有效的角球检索系统。尽管黄金标准数据的可用性有限，但 TacticAI 通过几何深度学习实现了数据效率，从而实现了这些成果。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biw8qn/r_tacticai_an_ai_assistant_for_football_tactics/</guid>
      <pubDate>Tue, 19 Mar 2024 21:42:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 在每层使用相同维度的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</link>
      <description><![CDATA[我的直觉是，随着我们在层中移动，令牌会逐渐丰富，但这意味着我们需要在每个令牌中存储更少的信息前面的层比后面的层要多。 从（相对）低维嵌入开始，然后将它们投影或扩展到更高的维度，直到它们达到最终大小，这不是有意义吗？    由   提交/u/timtom85  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/</guid>
      <pubDate>Tue, 19 Mar 2024 19:35:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 2024 讨论主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bireik/d_icml_2024_discussion_thread/</link>
      <description><![CDATA[这篇文章用于讨论与 ICML 2024 相关的任何内容，评论将于明天发布！祝所有参与者好运！   由   提交/u/condom-mechanics  /u/condom-mechanics  reddit.com/r/MachineLearning/comments/1bireik/d_icml_2024_discussion_thread/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bireik/d_icml_2024_discussion_thread/</guid>
      <pubDate>Tue, 19 Mar 2024 18:29:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我如何在 Google Gemma 6T 代币模型中发现 8 个错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bipsqj/p_how_i_found_8_bugs_in_googles_gemma_6t_token/</guid>
      <pubDate>Tue, 19 Mar 2024 17:23:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 带注释的曼巴：艰难的道路</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1biisb2/p_annotated_mamba_the_hard_way/</link>
      <description><![CDATA[链接：https://srush .github.io/annotated-mamba/hard.html 代码：https://github .com/srush/annotated-mamba 来自作者：  此博客是关于Mamba 一种最新的神经架构，可以粗略地认为是现代循环神经网络（RNN）。该模型运行得非常好，是无处不在的 Transformer 架构的合法竞争对手。它已经引起了很多关注。  我原本打算写一篇关于整篇论文的博文，内容相当密集且富有洞察力。然而，我只是对此处描述的 S6 算法着迷。该算法描述了如何在现代硬件上有效计算极大的 RNN，并扩展了 S4 和 近年来的S5。  事实上，如果我说实话，我实际上只了解了算法的这一行： y = SSM(A, B, C)( x) # 随时间变化：仅重复(扫描)  这行代码很有趣，我想，嘿，难道没有人能够理解为什么这种扫描在实践中速度很快吗？  事实证明这有点棘手。但是，如果您阅读这篇博文，我可以向您保证，您会理解这句话。 （也许比您想要的更多）。  第 0 部分：Triton  第 1 部分：累积和  第 2 部分：指数移动平均线  第 3 部分：获取导数  p&gt; 第 4 部分：同时多个  第 5 部分：Mamba  ​   由   提交 /u/ghosthamlet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1biisb2/p_annotated_mamba_the_hard_way/</guid>
      <pubDate>Tue, 19 Mar 2024 12:14:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] NVIDIA GTC 2024 公告</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bif6ey/d_nvidia_gtc_2024_announcements/</link>
      <description><![CDATA[NVIDIA 的计划已遍及加速计算、生成式 AI、行业应用、汽车、企业平台、Omniverse 和机器人领域。 其中一些最有趣的是：  DRIVE Thor：用于自动驾驶汽车中的生成式人工智能应用的车载计算平台。它每秒执行高达千万亿次操作，增强了自动驾驶的安全性，并支持与车辆的交互式对话。 Omniverse：融合物理和虚拟世界的数字孪生生态系统，帮助行业模拟、优化和识别更有效地执行操作。新的 Omniverse Cloud API 扩展了这些功能，使汽车和机器人等行业受益。 GR00T 项目：推动机器人和人工智能突破的人形机器人的基础模型。此外，还推出了 Jetson Thor 计算机，并升级至 NVIDIA Isaac™ 机器人平台，其中包含生成式 AI 模型和模拟工具。 Nvidia Blackwell GPU：一项尖端技术，旨在以 20 petaflops 的速度为下一代 AI 提供动力的性能。该GPU代表了人工智能能力的巨大飞跃，旨在实现万亿参数模型的民主化。 NVLink Switch 7.2 TI：新一代互连技术，可解决数据交换的瓶颈。它旨在促进 GPU 之间的通信，其规模适合最先进的 AI 模型。 NVIDIA NIM：一款新软件产品，旨在简化企业环境中生成式 AI 的部署。它将模型与优化的推理引擎打包在一起，并支持广泛的 GPU 架构。他们称其为所有人的人工智能包。  你最喜欢哪个？   由   提交 /u/vvkuka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bif6ey/d_nvidia_gtc_2024_announcements/</guid>
      <pubDate>Tue, 19 Mar 2024 08:13:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>