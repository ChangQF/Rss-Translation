<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Fri, 28 Feb 2025 01:18:13 GMT</lastBuildDate>
    <item>
      <title>[R]大语模型中的动态计划归纳</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izt1vr/r_dynamic_planning_induction_in_large_language/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  如何在llms中引入元思考以更好地回答查询。介绍已被接受的工作二植物，并将在 NAACL 2025 。 摘要：研究表明推理的有效性（例如，链接的效果（例如，chain-offeriep contranive contressive），改善策略（例如，自我启动），增长了增长， （LLMS）在各种任务上，例如回答问题。但是，使用单个固定策略回答不同类型的问题的性能是次优，并且在生成的输出令牌和执行检索方面效率低下。在我们的工作中，我们提出了一种新颖的技术Dyplan，以诱导LLMS中的动态策略选择过程，以提高性能并降低提问中的计算成本。 Dyplan结合了最初的决策步骤，以选择以输入问题为条件的最合适的策略，并相应地指导LLM的响应生成。我们扩展了二型植物以呈二植物验证，添加了内部验证和校正过程，以进一步丰富生成的答案。对三个突出的多跳问题回答（MHQA）数据集进行的实验揭示了Dyplan如何将模型性能提高7-13％，同时相对于最佳基线模型，将计算成本降低了11-32％。  paper链接： https://arxiv.org/pdf/2410.23511   tweet链接： https://x.com.com/tpare.com/tparekh97/status/189524117221976764841 提交由＆＃32; /u/u/tparekh97     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izt1vr/r_dynamic_planning_induction_in_large_language/</guid>
      <pubDate>Thu, 27 Feb 2025 22:58:14 GMT</pubDate>
    </item>
    <item>
      <title>[r]信仰状态变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izs7c8/r_belief_state_transformers/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/rajonrondoisturtle     link&gt; link&gt; link&gt; link&gt;  32   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izs7c8/r_belief_state_transformers/</guid>
      <pubDate>Thu, 27 Feb 2025 22:19:57 GMT</pubDate>
    </item>
    <item>
      <title>[R]语音识别。通过电话级HMM构建单词级hmm。 transtion矩阵。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izqvot/r_speech_recognition_building_wordlevel_hmm_from/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在实现我的Hmm-gmm语音识别模型。 现在，我面临下面所述的问题。 给定电话级的hmms a和b， ，在此问题中构建word级别的hmm c。在此问题中，请按照i i is ot the conde the phip the phip the a in ph y make a s a a是a a a phy是a a a a phe n phy a s a a a in a是a a和b a a a a in和b？  hmm a的状态：a1，a2，a3   hmm b：b1，b2，b2，b2，b3  的状态，让A和B的过渡矩阵如下：  据我了解，c的状态已与a和b。 合并，因此hmm c：a1，a2，a3，a3，b1，b2，b2。 href =“ https://preview.redd.it/8bn1mtoa0rle1.png？ https://preview.redd.it/8bn1mtoa0rle1.png?width=1121&amp; amp; format = png＆amp;Auto = webppumpp; s = 3f29b74ca0f4ca0f40f40697fa72727272727272727273f3fe078787877777777877777777777876 但这似乎不是合法的解决方案。 这种矩阵的限制算法是什么？也许我缺少一些东西。高度赞赏链接到一篇好文章。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1izqvot/r_speech_recognition_building_wordlevel_hmm_from/”&gt; [link]   &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1izqvot/r_speech_recognition_building_wordlevel_hordlevel_hmm_from//]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izqvot/r_speech_recognition_building_wordlevel_hmm_from/</guid>
      <pubDate>Thu, 27 Feb 2025 21:22:47 GMT</pubDate>
    </item>
    <item>
      <title>[d]建造ML/AI/VR开发学院实验室</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izixwy/d_the_building_of_a_mlaivr_development_college_lab/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我的大学最近获得了将近9000万印度卢比（约900万印度卢比或103,057美元）的资金，我们计划设置一个致力于机器学习，人工智能和虚拟现实开发的实验室。我非常感谢有关为该计划投资的最佳设备和软件的建议，见解或建议。预先感谢您的帮助！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1izixwy/d_the_building_of_a_a_mlaivr_develodment_college_lab/  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1izixwy/d_the_building_oof_a_mlaivr_develodment_college_college_lab/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izixwy/d_the_building_of_a_mlaivr_development_college_lab/</guid>
      <pubDate>Thu, 27 Feb 2025 15:53:16 GMT</pubDate>
    </item>
    <item>
      <title>[r]超越点产品：带有学习相似之处的检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iziusf/r_beyond_dot_products_retrieval_with_learned/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  矢量数据库的世界正在爆炸。在大型语言模型的兴起以及对语义搜索的越来越多的需求下，从大规模数据集中获得信息的有效检索变得至关重要。大约最近使用点产品相似性和最大内部产品搜索（MIPS）算法的大约最近的邻居（ANN）搜索一直是该领域的主力。但是，如果我们可以超越点产品的局限性并直接学习相似之处，该怎么办？一份引人入胜的新论文，“  检索学到的相似性”       bailu ding（Microsoft）和Jiaqi Zhai（Meta）（在www &#39;25会议的会议记录中）提出了一种新颖的方法，称为逻辑（MOL），该方法为学习的相似性功能提供了一种通用界面。它不仅在建议系统和问题答案中取得了最新的结果，而且还显示出明显的延迟改善，有可能重塑矢量数据库的格局。 完整的纸张在这里写下： https：//www.shape.ai/blog/blog/blog/beyond-dot-dot-dot-drieval-drieval-with-with-with-with-with-lear-learned-learned-similarities     /u/u/skeltzyboiii     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iziusf/r_beyond_dot_products_retrieval_with_learned/</guid>
      <pubDate>Thu, 27 Feb 2025 15:49:33 GMT</pubDate>
    </item>
    <item>
      <title>[P]神经论文的语义搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izgxtb/p_semantic_search_of_neurips_papers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我做了一个语义搜索器 https://www.papers.app来源。 贡献是欢迎贡献的，例如添加更多会议或功能（当前具有Neurips，ICML，Aistats，Colt，Corl，Corl，ICGI）。   它是如何工作的？  使用HuggingFace中的GTE-SMALL嵌入所有摘要，查找以超过80％的匹配返回所有论文。  &lt;！&lt;！ -  sc_on-&gt; 32;&gt; 32;提交由＆＃32; /u/mgamal96     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izgxtb/p_semantic_search_of_neurips_papers/</guid>
      <pubDate>Thu, 27 Feb 2025 14:24:43 GMT</pubDate>
    </item>
    <item>
      <title>[r] FFTNET：线性时间全局令牌通过自适应光谱过滤混合</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izc94i/r_fftnet_lineartime_global_token_mixing_via/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  非常有趣的论文，显示了FFT在保持性能的同时如何替换变形金刚中的自我注意力。关键想法是使用快速的傅立叶变换来混合令牌之间的信息，而不是计算全部注意力矩阵。 主要技术点： - 替换二次复杂性自我关注自我关注与线性复杂性FFT操作 - 使用基于FFT的混合层 - 使用基于FFT的混合层，这些混合层可将频率域和后部转换为频率的频率范围 - 通过频率进行频率 - 在频率上进行频率 - 通过频率进行频率 - 范围 - 通过频率 - 范围 - 通过频率进行频率 - 依赖频率 - 依从性 - 依赖频率 - 依赖 - 标准变压器 关键结果： - 匹配或超过标准基准上的自我注意力表现 - 在长序列任务上表现出尤其有力的结果 - 从O（n²）减少了内存使用范围 - 跨越跨模态（视觉，语言，时间序列） - 有效地对更长的序列 进行量表，可以更效率地进行变形。在保持线性复杂性的同时保持较长序列的能力可以实现新的应用程序。 FFT方法也可能有助于我们更好地了解自我注意力的实际学习。 但是，我认为有关在很小的数据集或极大的语言模型上的表现有一些开放的问题，需要进行更多的调查。该方法还可能会错过某些明确关注的模式。  tldr：FFT可以有效替代变形金刚中的自我注意力，从而在保持性能的同时将复杂性从二次降低到线性。跨多个领域的作品，并显示出长序列的特殊希望。 完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izc94i/r_fftnet_lineartime_global_token_mixing_via/</guid>
      <pubDate>Thu, 27 Feb 2025 09:53:45 GMT</pubDate>
    </item>
    <item>
      <title>[D]想法：机器学习高尔夫？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1izbgf2/d_idea_machine_learning_golf/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  似乎在ML世界中进行的许多工作都集中在较小或更快的模型上，这些模型仍然有效，而这些模型仍然有效。在某些方面，这让我想起了代码高尔夫的实践：一个挑战，人们写了最小的程序来解决某个问题。 这样，我有了ML高尔夫的想法，这是一个友好的竞争设置，一个人必须创建一个最小的模型，仍然可以解决某个问题，以解决某个问题，例如在例如。 number of learnable parameters, or the number of bytes to store these parameters, probably including the program to load and run the model on a sample. It seems like someone did think of this before, but the problems seem contrived and unrealistic even compared to像Mnist这样的东西，因为看起来他们更旨在让人手工“编程”神经网络。它似乎也排除了可能很有趣的其他ML方法。 我想知道这是否是其他人可能感兴趣的。我觉得这可能是一个有趣的（S集）挑战，甚至与SOTA相比，由于任何涉及的型号的人都可以感兴趣。&gt;   是否会相当易于使用。       。实际上，我个人几乎没有ML背景，因此，其他比我更了解的人的意见将不胜感激。例如，有关如何运行/设置的想法，可能包括的数据集/基准，最大尺寸或最低性能等等等等等等。提交由＆＃32; /u/u/scheurneus     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1izbgf2/d_idea_machine_learning_golf/</guid>
      <pubDate>Thu, 27 Feb 2025 08:54:18 GMT</pubDate>
    </item>
    <item>
      <title>[P]训练您自己的推理模型-GRPO仅在5GB VRAM上工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyv12c/p_train_your_own_reasoning_model_grpo_works_on/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿[ r/machinelearning ]（）（）folks！非常感谢2周前我们的GRPO版本的支持！我们设法使GRPO仅在Qwen2.5（1.5b）的 5GB的VRAM 上工作 - 从上一个Untsloth版本中的7GB下降： https：&gt; https：&gt; the RL recipe behind DeepSeek-R1 Zero&#39;s reasoning, and you can now do it with 90% less VRAM via Unsloth + LoRA / QLoRA!  Due to our newly added Efficient GRPO algorithms, this enables 10x longer context lengths while using 90% less VRAM vs. every other GRPO LoRA/QLoRA具有0降解的实现。 具有标准的GRPO设置，Llama 3.1（8b）20K上下文长度的培训需要510.8GB的VRAM。 However, Unsloth’s 90% VRAM reduction brings the requirement down to just 54.3GB in the same setup. We leverage our gradient checkpointing algorithm which we released a while ago.它可以巧妙地将中间激活卸载到系统RAM异步，同时仅慢1％。此剃须372GB VRAM ，因为我们需要num \ _ generations = 8。我们可以通过中间梯度累积进一步减少此内存使用。 使用Google的免费上下文使用我们的GRPO Notebook，使用Google的免费gpus： href =“ https://colab.research.google.com/github/unslothai/notebooks/blob/blob/main/nb/llama3.1_(8B”&gt; llama 3.1（8b）on colab  -grpo.ipynb）以及更多： align =“ left”&gt; metric   unsploth   trl + fa2           training Moregre Cost（GB） align =“左”&gt; 414GB      grpo内存成本（gb）   9.8gb    78.3gb  78.3gb  78.3gb    0gb   16gb      推理20K上下文（GB）   2.5GB  2.5gb  Total Memory Usage 54.3GB (90% less) 510.8GB   Also we made a Guide (with pics) for everything on GRPO + reward functions/verifiers (please let us know of any suggestions): https://docs.unsloth.ai/basics/reasoning-grpo-and-rl Thank you guys once again for all the support.对我们来说意义重大！ ：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/danielhanchen     [links]   &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1iyv12c/p_train_your_own_rowne_reasoning_model_grpo_grpo_works_on/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyv12c/p_train_your_own_reasoning_model_grpo_works_on/</guid>
      <pubDate>Wed, 26 Feb 2025 18:51:23 GMT</pubDate>
    </item>
    <item>
      <title>[d] n维数几乎是正交的向量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyt374/d_almost_orthogonal_vectors_in_n_dimensions/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  许多文献，尤其是从事表示形式学习的文献，他说“特征”是模型内一些高维空间中的向量，因为我们只能在n维中具有N完美的正交矢量（否则额外的向量将是线性依赖的），这些特征向量几乎是正交的，它几乎是正交的，几乎正交矢量的数量与n呈指数增加。但是我还没有找到一个可以理解的证据（或该指数界限）。一些地方提到了JL引理，但我看不到它是同一件事。有人在此背后有任何直觉，还是可以帮助一些平易近人的证据  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iyt374/d_almost_orthogonal_vectors_in_in_dimensions/”&gt; [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyt374/d_almost_orthogonal_vectors_in_n_dimensions/</guid>
      <pubDate>Wed, 26 Feb 2025 17:32:47 GMT</pubDate>
    </item>
    <item>
      <title>[n]拉格斯：LLM的实时自我完善而无需再培训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iyszck/n_ragsys_realtime_selfimprovement_for_llms/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们很高兴共享一个名为ragsys的新框架，该框架重新考虑了LLMS的检索增强生成（RAG）。 Instead of simply appending static document chunks to prompts, RAGSys dynamically builds a database of few-shot examples, instructions, and other contexts, and optimizes its retrieval to compose prompts that have the highest chance of yielding a good response. Here’s the core idea:  Dynamic Context Composition: Retrieve not only documents but also few-shot examples and instructions, forming a prompt that’s optimized for each unique query. Utility-Driven Optimization: Rather than relying solely on similarity, the system measures the utility of each retrieved context—prioritizing those that actually improve response accuracy. Feedback Loop: Every interaction (query, response, outcome) is stored and used to amend the few-shot示例和说明，并调整猎犬。这种连续的，自我提高的循环意味着LLM适应而无需再进行重新训练。  期待您的见解和讨论！ 可以随时查看完整的文章 进行深度潜水。   &lt;！提交由＆＃32; /u/u/crossing_minds     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iyszck/n_ragsys_realtime_selfimprovement_for_llms/</guid>
      <pubDate>Wed, 26 Feb 2025 17:28:25 GMT</pubDate>
    </item>
    <item>
      <title>机器学习可以真正“概括”，或者我们只是在合成专业方面变得更好吗？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iykqh1/can_machine_learning_truly_generalizeor_are_we/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们谈论ML中的概括，就好像是最终目标一样，模型学习模式会传输跨域。但是，“真正的概括”实际上是在发生，还是我们只是完善了特定于任务的外推？ 经过大量，多样化数据训练的模型不一定是概括的 - 它只是在预定义约束中的模式综合方面变得更好。即使是似乎可以很好地“概括”的变压器，它仍然受训练数据的基本结构的束缚。 那么，ML的真正前沿是关于实现真正的概括的真正前沿，还是接受智能固有地依赖上下文依赖于上下文？如果是这样，ML的未来是关于打破过去数据集限制的未来，还是简单地优化合成智能以提高专业化？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iykqh1/can_machine_learning_truly_generalizeor_are_are_we/”&gt; [link]    [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iykqh1/can_machine_learning_truly_generalizeor_are_we/</guid>
      <pubDate>Wed, 26 Feb 2025 10:43:23 GMT</pubDate>
    </item>
    <item>
      <title>[r] FFT反击：自我注意的有效替代品</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iycjkd/r_the_fft_strikes_back_an_efficient_alternative/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  传统的自我注意力以野蛮的o（n²）方式计算成对相互作用，将每个令牌与其他所有标记进行比较。对于长序列而​​言，这种方法效率低下。相反，快速傅立叶变换（FFT）将序列转换为频域。在这里，每个令牌由一组由单一矩阵定义的正交频率组件表示。该表示可以保留通过Parseval定理确保信号的能量，并在O（n log n）复杂性下更快地计算。通过利用经典信号处理原理，FFT提供了一种数学上优雅且可扩展的方式来捕获全球依赖性，这使其成为建模长距离交互作用的有吸引力的替代方法。  i Revisit fnet，该论文最初引入了静态的非线性非线性FFT方法。不幸的是，FNET的表述不仅写得不好，而且缺乏实用应用所需的可扩展性，并且在任何基准测试方面都没有表现出色。相反，我已经完善并优化了该方法，增强了其清晰度，适应性，有效性和非线性。我的方法还胜过许多基准上的经典自我注意力，因为它在频域中（自适应）在频域中运行，利用FFT的有效O（n log n）计算以更有效地捕获长期依赖性。这种改进的方法为传统的自我注意力提供了强大而可扩展的替代方案，使其成为捕获全球依赖性的引人注目的替代者。 编辑：本文的要点是表明我们可以以计算上有效的方式替换自我注意力。也许这不是最好的方法，但这是数学上合理的方法。它为将来的作品留下了很大的空间，并为更多机会打开了大门。这是论文的重点。 代码在本文中，但您也可以在这里找到： href =“ https://arxiv.org/abs/2502.18394”&gt; https://arxiv.org/abs/2502.18394 提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iycjkd/r_the_fft_fft_fft_backs_back_and_and_effficity_alternative/”&gt; [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iycjkd/r_the_fft_strikes_back_an_efficient_alternative/</guid>
      <pubDate>Wed, 26 Feb 2025 02:07:50 GMT</pubDate>
    </item>
    <item>
      <title>[d]简单问题线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iwdbgs/d_simple_questions_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请在此处发布问题，而不是创建新线程。鼓励其他创建新帖子以便在此处发布的新帖子的人！ 线程将活着直到下一个，因此请继续发布标题的日期之后。 感谢大家在上一个线程中回答了上一个线程中的问题！  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1iwdbgs/d_simple_questions_thread/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iwdbgs/d_simple_questions_thread/</guid>
      <pubDate>Sun, 23 Feb 2025 16:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些经验丰富的人。   &lt;！ -  sc_on--&gt; 32;&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_to_be_hired/”&gt; [link]  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>