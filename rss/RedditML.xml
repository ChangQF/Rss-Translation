<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Mon, 06 May 2024 09:15:38 GMT</lastBuildDate>
    <item>
      <title>[P] LeRobot：Hugging Face 的现实世界机器人库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cldfy2/p_lerobot_hugging_faces_library_for_realworld/</link>
      <description><![CDATA[      认识一下 LeRobot，一个库托管最先进的机器人深度学习。 人工智能开发的下一步是将其应用到我们的物理世界。因此，我们正在围绕机器人人工智能构建社区驱动的工作，并且向所有人开放！ 看一下代码： https://github.com/huggingface/lerobot https://preview.redd.it/ugf4l8lfgryc1.png?width=3794&amp;format=png&amp;auto=webp&amp;s=222825e897ba48eb07acedffb0662d5794af04 e8 乐机器人是对于机器人技术来说，就像 Transformers 库对于 NLP 一样。它提供了带有预先训练的检查点的高级人工智能模型的干净实现。我们还重新实现了来自学术界的 31 个数据集和一些模拟环境，无需物理机器人即可开始使用。 Aloha项目。 [视频链接] https://preview.redd.it/86ihkcwhgryc1.png?width=2506&amp;format=png&amp;auto=webp&amp;s=4f2ca7522a012d00d7327d903 35d069dd099a321 LeRobot 的另一个可视化，这次是在 Mobile Aloha 数据上，学习完全端到端的导航和操作。这两个数据集都是在 trossenrobotics 机器人手臂上收集的。 [视频链接] https://preview.redd.it/qqtncqligryc1.png?width=1900&amp;format=png&amp;auto= webp&amp;s=4f83c675b5c6f9dbded4b5b90a7a1c9f531c4086 LeRobot 代码库已通过在模拟中复制最先进的结果进行了验证。例如，这里是著名的 ACT 策略，它已被重新训练并可用作预训练检查点： [HF HUB 链接] LeRobot 还具有扩散政策，强大的模仿学习算法，以及TDMPC，一种包含世界模型的强化学习方法，不断从与环境的交互中学习。 https://preview.redd.it /br9ibrylgryc1.png?width=1684&amp;format=png&amp;auto=webp&amp;s=8e5595f1dff5381e5f60c6776126f48187ec58d9 快来加入我们的Discord 频道。我们正在建立一个来自不同背景、软件和硬件的多元化社区，以开发现实世界中的下一代智能机器人！ 感谢人工智能和机器人社区，没有他们就没有乐机器人。可能。   由   提交/u/Tamazy   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cldfy2/p_lerobot_hugging_faces_library_for_realworld/</guid>
      <pubDate>Mon, 06 May 2024 07:48:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] - ML/CV 会议志愿者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cld406/d_volunteers_for_mlcv_conferences/</link>
      <description><![CDATA[大家好， 我想获取一些有关 ML/CV 会议志愿者的信息（例如 CVPR、ECCV、ICML、 ETC）。特别是：  有选择过程吗？ 志愿者在这些会议中做什么？ 总的来说，值得吗？尤其是从网络的角度来看。  谢谢   由   提交/u/backprop_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cld406/d_volunteers_for_mlcv_conferences/</guid>
      <pubDate>Mon, 06 May 2024 07:24:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] Kolmogorov-Arnold 网络只是一个 MLP</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/</link>
      <description><![CDATA[事实证明，您可以将 Kolmogorov-Arnold 网络编写为 MLP，并在 ReLU 之前进行一些重复和移位。  https://colab.research.google.com/drive/1v3AHz5J3gk-vu4biESubJdOsUheycJNz &lt; /div&gt;  由   提交 /u/osamc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clcu5i/d_kolmogorovarnold_network_is_just_an_mlp/</guid>
      <pubDate>Mon, 06 May 2024 07:04:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Gemma 有如此疯狂的大 MLP 隐藏暗尺寸？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clcluq/d_why_gemma_has_such_crazy_big_mlp_hidden_dim_size/</link>
      <description><![CDATA[   /u/kiockete  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clcluq/d_why_gemma_has_such_crazy_big_mlp_hidden_dim_size/</guid>
      <pubDate>Mon, 06 May 2024 06:48:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如果 Llama-3 只有 8K 上下文长度，为什么它可以使用 32K 上下文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clbmz2/r_why_can_llama3_work_with_32k_context_if_it_only/</link>
      <description><![CDATA[大家好！请参阅此处的帖子：https://twitter.com/abacaj/status/1785147493728039111 我没有不明白他所说的“通过零训练（实际上只是一个简单的 2 行配置），你可以从 llama-3 模型中获得 32k 上下文”的意思 有人知道这个动态是什么吗？缩放技巧是？非常感激！ :)   由   提交 /u/sunchipsster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clbmz2/r_why_can_llama3_work_with_32k_context_if_it_only/</guid>
      <pubDate>Mon, 06 May 2024 05:43:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 梯度增强分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1clbipd/d_classification_with_gradient_boosting/</link>
      <description><![CDATA[梯度增强分类 我正在尝试对城市地区的物体进行分类，主要是建筑物和植被。我使用激光雷达数据中的几何特征，如平面度、线性垂直度、全方差、最小特征值、曲率变化、球度。我有五个类别的低植被、中植被、高植被和建筑物。并使用 [1...12] 中的半径我进行了随机搜索来查找参数，n_estimator=100，学习率=0.1，min_sample_split=2，min_sample_leaf = 1. 该模型的准确率为 98%。当我预测更大规模的模型时，几乎没有问题。一些建筑物的边缘和三角形屋顶的直线（多见于欧洲城市地区）。在这两种情况下，模型预测它们为高植被。现在我不知道如何继续前进，是增加 n 估计器和学习率，还是找到有助于区分植被和边缘情况的特征。 任何建议将不胜感激，谢谢   由   提交/u/Money_Respect4741   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1clbipd/d_classification_with_gradient_boosting/</guid>
      <pubDate>Mon, 06 May 2024 05:35:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师生产技能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1claom3/d_machine_learning_engineer_production_skills/</link>
      <description><![CDATA[大家好！我希望您度过一个美好的夜晚（尤其是那些还没睡的夜猫子）。 😭 我只想问一个问题：我已经开始了成为一名机器学习工程师的旅程，我正在努力学习生产技能和工具。 👼 但是，我不知道从哪里开始，也不知道一般需要什么。我希望我能得到你们所有人（特别是那些已经在这个行业的人）关于我获得这个职位的第一次实习所需的技能或工具的意见/经验。 非常感谢祝您度过一个美好的夜晚！ 📊  #machinelearningengineer #mle #product #career   由   提交/u/Thomas_ng_31   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1claom3/d_machine_learning_engineer_production_skills/</guid>
      <pubDate>Mon, 06 May 2024 04:42:14 GMT</pubDate>
    </item>
    <item>
      <title>[D]“对话分类”有正式名称吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cl4tg1/d_is_there_a_formal_name_for_dialogue/</link>
      <description><![CDATA[我正在尝试将对话分类。具体来说，如果我们有客户服务聊天数据，其中客户提出问题并由客服代表回答，我想将这些数据分类为具有“产品查询”、“产品查询”等标签。 “交货查询”等等 这有正式的名称吗？这似乎不是正常的文本分类，因为我们必须考虑说话人信息。似乎也没有一个叫做对话分类的任务。我认为意图分类可能最接近，但典型的数据集似乎只使用初始查询作为输入文本，而不是整个对话。 我认为也许使用整个对话可能不合适，也许可能有一个初始阶段，从对话中提取关键查询。之后也许这些可以用于意图分类，但我不确定这是否是一个理想的方法。   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cl4tg1/d_is_there_a_formal_name_for_dialogue/</guid>
      <pubDate>Sun, 05 May 2024 23:33:50 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型如何玩视频游戏 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cl3zmi/how_large_language_models_play_video_games_d/</link>
      <description><![CDATA[      来自我的 YT 频道的视频，讲述如何使用 LLM 进行游戏Crafter（Minecraft-lite）和 Atari 等视频游戏。其中一些是单独的 LLM 提示工程工作，而另一些则帮助 RL 代理探索或提供更好的奖励信号。如果有人感兴趣，请点击此处链接。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cl3zmi/how_large_language_models_play_video_games_d/</guid>
      <pubDate>Sun, 05 May 2024 22:55:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 支持具有导数信息的高斯过程的 Python 库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cl3vtp/d_python_libraries_that_support_gaussian/</link>
      <description><![CDATA[大家好，我正在寻找支持具有导数信息的高斯过程的 Python 库（GPyTorch 除外）。我目前正在使用 GPyTorch，想要将我得到的结果与其他库进行比较。我查看了 GPflow 和 GPy 的文档，但找不到它们是否支持这个或文档中的任何示例。如果您碰巧有示例链接，那就太好了！   由   提交/u/m-julian1  /u/m-julian1  reddit.com/r/MachineLearning/comments/1cl3vtp/d_python_libraries_that_support_gaussian/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cl3vtp/d_python_libraries_that_support_gaussian/</guid>
      <pubDate>Sun, 05 May 2024 22:50:36 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 一款由 LLM 提供支持的 SEC 文件洞察 Web 应用程序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cl3fs3/project_an_llmpowered_web_app_for_sec_filing/</link>
      <description><![CDATA[我构建了一个应用程序，该应用程序使用大型语言模型 (LLM) API 分析 10-K 文件并生成见解，以全面了解公司的财务业绩通过用户友好的可视化和分段细分来制定战略方向。 以下是 GitHub 存储库的链接：https://github.com/astonishedrobo/sec-llm-insights 以后我还打算加上RAG，避免LLM产生幻觉。 任何改进/准确的建议都将受到重视。   由   提交 /u/PleasantInspection12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cl3fs3/project_an_llmpowered_web_app_for_sec_filing/</guid>
      <pubDate>Sun, 05 May 2024 22:30:28 GMT</pubDate>
    </item>
    <item>
      <title>[研究]理解变形金刚中的注意力机制：5分钟的视觉指南。 🧠</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckxzti/research_understanding_the_attention_mechanism_in/</link>
      <description><![CDATA[   TL。 ;DR：注意力是键值存储或字典的“可学习”、“模糊”版本。由于主要针对 NLP 和 LLM 改进了序列建模，Transformers 使用注意力并接管了以前的架构 (RNN)。 什么是注意力以及它为何接管法学硕士和机器学习：视觉指南 &lt; a href=&quot;https://preview.redd.it/8aoqz10hjnyc1.png?width=1903&amp;format=png&amp;auto=webp&amp;s=234b7aa38e9eee56d9d91f70f69ff81a7c666ff7&quot;&gt;https://preview.redd.it/8aoqz10hjnyc1.png?width =1903&amp;format=png&amp;auto=webp&amp;s=234b7aa38e9eee56d9d91f70f69ff81a7c666ff7   由   提交/u/ml_a_day  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckxzti/research_understanding_the_attention_mechanism_in/</guid>
      <pubDate>Sun, 05 May 2024 18:34:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在创建神经网络时，是否有更系统的方法来选择层或架构的深度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/</link>
      <description><![CDATA[所以我正在学习深度学习和神经网络，我对这部分真的有点困惑。我通常熟悉可用的层及其工作原理（至少是那些被广泛使用的层），但我仍然很难弄清楚在什么上使用什么。有没有更合乎逻辑或系统的方法来做到这一点？比如数学什么的？我很想尝试，但我只是想避免陷入兔子洞，因为这个项目是在截止日期前完成的，而且我对此并不失望 ```编辑```` 感谢您的所有回复，特别是提供阅读材料和建议。    由   提交/u/PsychologicalAd7535   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/</guid>
      <pubDate>Sun, 05 May 2024 14:04:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] 仔细检查大型语言模型在小学算术中的表现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.00332 摘要：  大型语言模型（LLM）在许多基准测试中取得了令人印象深刻的成功用于数学推理。然而，人们越来越担心，其中一些性能实际上反映了数据集污染，即与基准问题非常相似的数据泄漏到训练数据中，而不是真正的推理能力。为了严格调查这一说法，我们委托小学数学 1000 (GSM1k)。 GSM1k 的设计反映了已建立的 GSM8k 基准的风格和复杂性，GSM8k 基准是衡量基本数学推理的黄金标准。我们确保这两个基准在人类解决率、解决步骤数、答案大小等重要指标上具有可比性。在评估 GSM1k 上领先的开源和闭源法学硕士时，我们观察到准确性下降高达 13%，几个模型系列（例如 Phi 和 Mistral）显示出几乎所有模型大小的系统过度拟合的证据。与此同时，许多模型，尤其是前沿模型（例如 Gemini/GPT/Claude）显示出最小的过度拟合迹象。进一步的分析表明，模型从 GSM8k 生成示例的概率与其 GSM8k 和 GSM1k 之间的性能差距之间存在正相关关系（Spearman 的 r2=0.32），这表明许多模型可能已经部分记住了 GSM8k。    由   提交 /u/SurveySea7570   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/</guid>
      <pubDate>Sun, 05 May 2024 05:57:17 GMT</pubDate>
    </item>
    </channel>
</rss>