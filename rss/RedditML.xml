<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 06 Jan 2025 06:25:10 GMT</lastBuildDate>
    <item>
      <title>[R] 3D 视觉-语言-动作生成世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1husa4d/r_3d_visionlanguageaction_generative_world_model/</link>
      <description><![CDATA[  由    /u/moschles  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1husa4d/r_3d_visionlanguageaction_generative_world_model/</guid>
      <pubDate>Mon, 06 Jan 2025 05:54:27 GMT</pubDate>
    </item>
    <item>
      <title>自监督学习——n 球面上的测量分布 [D] [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hurxls/selfsupervised_learning_measure_distribution_on/</link>
      <description><![CDATA[大多数自监督学习方法（SimCLR、MoCo、BYOL、SimSiam、SwAV、MS BYOL 等）使用 n 球面超球面，提取的特征（编码器 + 投影/预测头之后）分布在该超球面中。然后，损失函数使用分布在此超球面上的特征进行损失计算。 论文，例如：  通过超球面上的对齐和均匀性理解对比表示学习，Tongzhou Wang 等人；ICML 2020 将表示与基础对齐：一种新的自监督学习方法，Shaofeng Zhang 等人；CVPR 2022 重新思考自监督学习中的均匀性度量，Xianghong Fang 等人； ICLR 2024  其他人表明这些特征分布在每个类的 n 球面上。 我们可以通过哪些不同的方式测量这些嵌入特征在这个超球面上的分布？比如说，如果我从 ImageNet/CIFAR-100 数据集中随机选择一个类，我如何测量属于这个类的所有图像在这个 n 球面上的分布？    提交人    /u/grid_world   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hurxls/selfsupervised_learning_measure_distribution_on/</guid>
      <pubDate>Mon, 06 Jan 2025 05:33:33 GMT</pubDate>
    </item>
    <item>
      <title>[N] 由 HNSW Graph 提供支持的内存向量存储</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1huqwji/n_inmemory_vector_store_powered_by_hnsw_graph/</link>
      <description><![CDATA[大家好！我现在已经在 Treds 中添加了一个完全基于命令的向量存储，由 HNSW 图提供支持，用于近似最近邻搜索。下面简要介绍一下这四个命令：  VCREATE – 初始化向量索引，指定 maxNeighbors、层因子和 efSearch 等参数。 VINSERT – 将向量插入该索引。 VSEARCH – 搜索给定向量的 k 个最近邻居。 VDELETE – 根据其 ID 从索引中删除向量。  可以在 redis-cli 中执行命令，因为 Treds 符合 RESP 标准。一个简单的会话可能看起来像 VCREATE vec 6 0.5 100 VINSERT vec 1.0 2.0 VINSERT vec 2.0 3.0 VINSERT vec 3.0 4.0 VSEARCH vec 1.5 2.5 2  这将创建一个名为 vec 的索引，插入一些 2D 向量，搜索 [1.5, 2.5] 的 2 个最近邻居。向量也可以是 N 维。 如果您之前查看过 Treds，我很乐意听听您对新向量存储添加的想法。如果您还没有，请随时查看并告诉我您是否有任何建议或问题。感谢阅读，祝您黑客愉快！ https://github.com/absolutelightning/treds?tab=readme-ov-file#vector-store https://github.com/absolutelightning/treds    提交人    /u/Fast-Tourist5742   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1huqwji/n_inmemory_vector_store_powered_by_hnsw_graph/</guid>
      <pubDate>Mon, 06 Jan 2025 04:35:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 离散扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hupxdg/d_discrete_diffusion_models/</link>
      <description><![CDATA[离散分布扩散中最有前途和最新的成就是什么？ 到目前为止，我已经看过了：  https://arxiv.org/abs/2107.03006 https://arxiv.org/abs/2310.16834v2  有没有更新或更有前途的成果？    提交人    /u/ArtisticHamster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hupxdg/d_discrete_diffusion_models/</guid>
      <pubDate>Mon, 06 Jan 2025 03:42:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有任何针对 FOSS 数据进行训练的背景去除模型吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hum9p4/d_any_background_removal_models_trained_on_foss/</link>
      <description><![CDATA[我将为一个对版权非常严格的项目做出贡献，甚至包括所使用的 ML 工具。我发现的许多模型都没有指定它们在哪些数据上进行训练（有些模型是在通过抓取训练的模型生成的图像上进行训练的，而这在我的例子中是不允许的）。 我发现最接近的是那些仅在 DIS5K 上进行训练的 DIS5K 上进行训练的 BiRefNet 模型；这些图像“允许商业使用和修改” （大概是 CC BY 和/或 BY-SA），但数据集本身有使用条款，禁止商业使用。    提交人    /u/Sobsz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hum9p4/d_any_background_removal_models_trained_on_foss/</guid>
      <pubDate>Mon, 06 Jan 2025 00:41:32 GMT</pubDate>
    </item>
    <item>
      <title>[N] SemiKong：全球首个以开源半导体为重点的法学硕士项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hugszn/n_semikong_the_worlds_first_opensource/</link>
      <description><![CDATA[还有人听说过 SemiKong 吗？显然，它是第一个专门为半导体研发而打造的开源 LLM。他们说，通过将设计协议和模拟数据等内容直接集成到其工作流程中，它可以将芯片设计速度提高 30%。 对于通常资源密集且速度较慢的芯片设计来说，这似乎是一件大事。您是否认为像这样的更多特定领域的 LLM 会成为未来？或者将这样的东西集成到现有的工作流程中是否存在太多挑战？ https://www.marktechpost.com/2024/12/27/meet-semikong-the-worlds-first-open-source-semiconductor-focused-llm/    提交人    /u/Frosty_Programmer672   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hugszn/n_semikong_the_worlds_first_opensource/</guid>
      <pubDate>Sun, 05 Jan 2025 20:45:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我制作了一个 CLI，使用遗传算法来改进提示</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hubl11/p_i_made_a_cli_for_improving_prompts_using_a/</link>
      <description><![CDATA[        提交人    /u/jsonathan   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hubl11/p_i_made_a_cli_for_improving_prompts_using_a/</guid>
      <pubDate>Sun, 05 Jan 2025 17:04:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 随机 SVD/PCA 用于有效注意力机制——有潜力吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hu168k/d_randomised_svdpca_for_efficient_attention/</link>
      <description><![CDATA[这个想法在我脑子里已经萦绕了一段时间，很想听听大家对它是否有潜力的意见——有太多值得关注的效率改进提案，我已经记不清哪些已经尝试过，哪些还没有尝试过！ 该过程的效果如下：  首先像平常一样计算键和查询 然后，对查询进行随机 PCA，以确定查询空间中最大的 D 个组成部分。  对于 D 个最大组件中的每一个，保留与该组件最匹配的密钥向量 定期关注这些密钥。  鉴于长度为 N 的序列的典型注意力具有复杂度 O(N^2)，而随机 PCA 为 O(D^2)，因此这里可能会节省相当大的推理时间。 我看不到任何现有的研究表明这是否有用。LoRA 和 Linformers 接近，因为它们也使用低秩近似，但我认为我提出的建议是独一无二的。有什么见解吗？    提交人    /u/enjeyw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hu168k/d_randomised_svdpca_for_efficient_attention/</guid>
      <pubDate>Sun, 05 Jan 2025 07:55:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人类智能存在于大数据领域，还是小数据领域？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htz91k/d_does_human_intelligence_reside_in_big_data/</link>
      <description><![CDATA[当今前沿的 LLM 拥有数万亿个参数，并在 500 万亿个 token 上进行训练。 人类大脑拥有 860 亿个神经元和 100 万亿个突触。 任何人消耗的文本信息量都比 LLM 所训练的少几个数量级。但是，人眼以大约 10Mbps 的速率捕获视觉信息。加上听觉、触觉、平衡感、嗅觉等其他感官，人类儿童在生命的最初几年消耗的信息量比任何 LLM 所见过的都要多。 这似乎表明人类智能需要大数据。 但是那些从出生就失明的人怎么办？先天性聋盲（没有记录在案的病例）怎么办？    提交人    /u/Gear5th   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htz91k/d_does_human_intelligence_reside_in_big_data/</guid>
      <pubDate>Sun, 05 Jan 2025 06:07:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 无法规划，但可以在 LLM-Modulo 框架中帮助规划</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hty2jr/r_llms_cant_plan_but_can_help_planning_in/</link>
      <description><![CDATA[  由    /u/jsonathan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hty2jr/r_llms_cant_plan_but_can_help_planning_in/</guid>
      <pubDate>Sun, 05 Jan 2025 04:57:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 巴洛双胞胎如何避免因仿射变换而不同的嵌入？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htpuuv/r_how_barlow_twins_avoid_embeddings_that_differ/</link>
      <description><![CDATA[我正在阅读 Barlow Twins (BT) 论文，但就是不明白它如何避免以下情况。 当互相关矩阵等于单位矩阵时，BT 损失最小化。实现这一点的必要条件是对角线元素 C_ii 为 1。这可以通过 2 种不同的方式实现。对于每个 x：  zA=zB zA=a⋅zB+b  其中 zA 和 zB 是同一输入 x 的不同增强的嵌入。换句话说，嵌入可以不同，但​​这种差异被掩盖了：corr(X,aX+b)=corr(X,X)=1。 直观地说，如果我们的目标是学习对扭曲不变的表示，那么应该避免第二种解决方案。有什么想法可以驱动网络避免这种情况吗？   由    /u/Seiko-Senpai  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htpuuv/r_how_barlow_twins_avoid_embeddings_that_differ/</guid>
      <pubDate>Sat, 04 Jan 2025 22:10:28 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 寻找深度学习模型失败的输入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htp9tv/project_finding_inputs_where_deep_learning_models/</link>
      <description><![CDATA[大家好！上个月在 NeurIPS（机器学习会议）上，我读到了一篇有趣的论文“人类在算法预测中的专业知识”，该论文描述了一个框架，用于确定机器学习模型在哪些方面的表现优于人类专家。我发现作者的工作非常有趣。下面，我将进一步探讨他们的框架，并将其扩展到多类分类。我的结果非常令人惊讶，表明一组现代模型架构在 CIFAR-10 中处理狗和猫时遇到了麻烦。 GitHub 链接：https://github.com/sunildkumar/model_indistinguishability 论文链接：https://arxiv.org/abs/2402.00793    提交人    /u/dragseon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htp9tv/project_finding_inputs_where_deep_learning_models/</guid>
      <pubDate>Sat, 04 Jan 2025 21:44:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我建立了一个庞大的数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htocnj/r_ive_built_a_big_ass_dataset/</link>
      <description><![CDATA[我清理/处理并合并了大量患者信息数据集，每个数据集都会询问患者关于他们自己的各种问题。我还知道他们是否患有这种疾病。我有他们 10 年前对所有问题的回答，以及他们现在或最近的回答，以及他们现在和 10 年前的疾病状况。我找不到任何论文做过如此大规模的研究，我感觉自己坐在一袋钻石上，但我不知道如何打开它。您认为最好的方法是什么？如何最大限度地利用它？我知道很多都是关于我的最终目标是什么，但我真的想知道其他人会先做什么！ （我有 2500 名患者和 27 个数据集，其中包含最早记录和最新记录。因此有 366 个特征，每个特征一个最新，一个最早，大约有 200 万个细胞。）想知道您的想法    提交人    /u/Disastrous_Ad9821   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htocnj/r_ive_built_a_big_ass_dataset/</guid>
      <pubDate>Sat, 04 Jan 2025 21:03:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>