<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Tue, 30 Jan 2024 03:14:14 GMT</lastBuildDate>
    <item>
      <title>[R] LLM 指南？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aee78v/r_llms_guide/</link>
      <description><![CDATA[嘿伙计们！我想尽可能为法学硕士的研究做出贡献。尽管我熟悉生成式人工智能和机器学习，但我还是刚刚开始。我想开始探索这个主题，进行研究并做出贡献。请让我知道我可以从哪里开始，欢迎任何研究论文、建议！   由   提交/u/Old-Detective-9446   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aee78v/r_llms_guide/</guid>
      <pubDate>Tue, 30 Jan 2024 03:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 加速注意力研究：融合心理学和机器学习：探索注意力机制中的意志力和兴趣</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeds27/d_accelerating_research_on_attention_blending/</link>
      <description><![CDATA[      摘录：今天，让我们从通常深入研究代码和算法的过程中绕道而行。相反，我想谈谈最近让我大脑发痒的事情：如果我们可以将一些人类心理学（例如意志力和兴趣）混合到机器学习的冷酷逻辑世界中会怎么样？如果您愿意，请喜欢这篇文章我要继续努力！ :) https://medium.com/@beastman3b/blending-psychology-and-machine-learning-exploring-willpower-and-interest-in-attention-mechanisms-81ce5d6bdb3d &amp; #x200b; https:// medium.com/@beastman3b/blending-psychology-and-machine-learning-exploring-willpower-and-interest-in-attention-mechanisms-81ce5d6bdb3d 请继续关注，看看效果如何出！   由   提交 /u/Honest-Debate-6863   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeds27/d_accelerating_research_on_attention_blending/</guid>
      <pubDate>Tue, 30 Jan 2024 02:42:34 GMT</pubDate>
    </item>
    <item>
      <title>250 RTX 3080s 能做什么 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aedjxc/what_to_dow_ith_250_rtx_3080s_p/</link>
      <description><![CDATA[您好！我有大约 250 个 RTX 3080，+ 可能有 40 个 RTx 3070，我用于挖矿。他们都拆掉了风扇护罩并安装了风扇。在浸入式冷却液中采矿。长话短说。采矿停止后，事情变得忙碌起来。它们的 GPU 刚刚放在浸没液体中。它们仍然可以工作，并且自从采用液体冷却以来从未变热。  是否有任何公司可以托管浸入式冷却卡，或者有人想要协助代理这些卡或帮助它们设置机器学习？我很乐意将几台 3080 赠送给任何可以用它们实现目标的人！   由   提交/u/death0and0taxes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aedjxc/what_to_dow_ith_250_rtx_3080s_p/</guid>
      <pubDate>Tue, 30 Jan 2024 02:31:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 LLM + RAG 进行 3D 对象搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aec8gb/d_3d_object_search_using_llm_rag/</link>
      <description><![CDATA[为可与自然语言一起使用的 3D 对象制作一个小型搜索引擎，并从中获得了一些乐趣。不需要元数据或标签，索引纯粹是根据几何图形构建的！这使用以下管道进行工作：  对于数据库中的每个对象，我生成 6 个图像，每侧 1 个。 对于每个图像，我使用 gpt4- 进行描述视觉，然后使用 gpt4 将其合成为单个描述 文本描述使用剪辑嵌入并存储在矢量数据库中 对于搜索查询，搜索字符串被嵌入，并且检索到数据库中最接近的（n）个向量。  参见此处：https://x.com/MenyJanos/status/1752104689188135271?s=20   由   提交/u/Janos95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aec8gb/d_3d_object_search_using_llm_rag/</guid>
      <pubDate>Tue, 30 Jan 2024 01:29:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用多个库对 Mixtral-8x7B 进行实验 - 每秒最多获得 52 个令牌。想法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeb70z/d_experiments_with_mixtral8x7b_using_multiple/</link>
      <description><![CDATA[      大家好， 最近尝试部署 Mixtral-8x7B模型并希望与感兴趣的人分享主要发现： 最佳性能：使用 Pytorch（每晚）的量化 8 位模型的平均代币生成率为 52.03 代币/秒在 A100 上，平均推理时间为 4.94 秒，冷启动时间为 11.48 秒（在无服务器环境中部署时很重要） 混合实验 测试的其他库： vLLM、AutoGPTQ、HQQ 渴望听到您在类似部署中的经验和学习！   由   提交/u/Tiny_Cut_8440   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeb70z/d_experiments_with_mixtral8x7b_using_multiple/</guid>
      <pubDate>Tue, 30 Jan 2024 00:40:34 GMT</pubDate>
    </item>
    <item>
      <title>自动 1111 的开源 SDK/Python 库 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aeab92/opensource_sdkpython_library_for_automatic_1111_p/</link>
      <description><![CDATA[   ​ https://preview.redd.it/74bz5ko0xgfc1.png?width=1656&amp;format=png&amp;auto=webp&amp;s=2a0ea066 0f56c97e242c7e099073086f52e38263&lt; /a&gt; https://github.com/saketh12/Auto1111SDK 大家好，我为自动 1111 Web UI 构建了一个轻量级开源 Python 库，它允许您在基础设施上本地运行任何稳定扩散模型。您可以轻松运行：  文本到图像 图像到图像 修复 修复 li&gt; Stable Diffusion Upscale Esrgan Upscale Real Esrgan Upscale 直接从 Civit AI 下载模型  使用任何安全张量或检查点文件都只需几行代码！它超轻且高性能。与 Huggingface Diffusers 相比，我们的 SDK 使用的内存/RAM 少得多，并且我们在我们测试的所有设备/操作系统上观察到速度提高了 2 倍！ 请为我们的 Github 存储库加注星标！！！ https://github.com/saketh12/Auto1111SDK.   由   提交 /u/Dazzling_Koala6834   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aeab92/opensource_sdkpython_library_for_automatic_1111_p/</guid>
      <pubDate>Mon, 29 Jan 2024 23:59:46 GMT</pubDate>
    </item>
    <item>
      <title>分解：神经网络结构组合性的证据 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae61tq/break_it_down_evidence_for_structural/</link>
      <description><![CDATA[ 由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae61tq/break_it_down_evidence_for_structural/</guid>
      <pubDate>Mon, 29 Jan 2024 20:59:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 之外的法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae5dgq/d_llms_beyond_rag/</link>
      <description><![CDATA[实际上几乎每个人都在谈论 RAG。我想知道接下来会出现什么趋势。很想听听您的想法。   由   提交/u/HolidayCritical3665   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae5dgq/d_llms_beyond_rag/</guid>
      <pubDate>Mon, 29 Jan 2024 20:31:28 GMT</pubDate>
    </item>
    <item>
      <title>佩德罗·多明戈斯：神经象征尚未发挥作用 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae4vp1/pedro_domingos_neurosymbolic_does_not_work_yet_r/</link>
      <description><![CDATA[      ​ https://preview.redd.it/r0h4yab5qffc1.png?width=817&amp;format=png&amp;auto=webp&amp;s=033744120df49 252c5379bdafa429570e80cfac4&lt; /a&gt; ​ 象征性人工智能通常被视为失败。我记得 Cyc 花费了 2 亿美元（比 GPT-4 的培训预算还多？）。 另一方面，Transformer LLM [1] 明显的固有局限性使一些人将目光转向象征性的、神经-再次采用象征性和混合性方法。 DeepMind 首席执行官表示，公司在这个领域有六个项目。 如果你对这些主题（神经网络、符号和神经符号人工智能的理论局限性）感兴趣，我为它们制作了一个 Reddit 子版块： r/symbolic （我可能会后悔这样做，但小众主题需要自己的 subreddits，因为大多数主题都没有关心或了解很多，因此提交的内容会被否决，并且评论通常缺乏洞察力，例如“什么是 ILP？”） ​ &lt; p&gt;[1]例如 https://arxiv.org/abs/2205.11502   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae4vp1/pedro_domingos_neurosymbolic_does_not_work_yet_r/</guid>
      <pubDate>Mon, 29 Jan 2024 20:11:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多输出高斯过程，每个输入一个输出</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae2o5b/r_multioutput_gaussian_process_with_one_output/</link>
      <description><![CDATA[我正在寻找一种适合多输出高斯过程的方法，其中在任何给定输入处仅观察到单个输出。我遇到的所有多输出高斯过程模型都假设在每个输入处观察到每个输出（即完全观察到的输出）。 这篇博客文章说，当在任何给定输入处观察到单个输出时，观察次数将为n，并且多输出 GP 将具有与单输出 GP 相同的时间和内存缩放比例。这是一个不错的酒店。但是，这篇文章没有提及如何拟合这样的模型。 我的特定应用程序有 2 个输出，其中一个输出比另一个输出具有更多的观察结果。任何帮助将不胜感激！   由   提交 /u/RemyMacDonald   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae2o5b/r_multioutput_gaussian_process_with_one_output/</guid>
      <pubDate>Mon, 29 Jan 2024 18:41:30 GMT</pubDate>
    </item>
    <item>
      <title>[d] Code Llama，一种最先进的大型编码语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ae0lsj/d_code_llama_a_stateoftheart_large_language_model/</link>
      <description><![CDATA[ https://ai.meta.com/blog/code-llama-large-language-model-coding/   由   提交/u/Electrical_Study_617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ae0lsj/d_code_llama_a_stateoftheart_large_language_model/</guid>
      <pubDate>Mon, 29 Jan 2024 17:19:04 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯神经网络与学习方差和均值 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1advijz/bayesian_nns_vs_learning_variance_and_mean/</link>
      <description><![CDATA[嗨， 根据我的理解，贝叶斯神经网络将权重视为 pdf，从而允许神经网络本身产生随机变量基于训练后权重采样的结果。虽然这看起来很有趣，但它也很昂贵。对于那些希望能够产生随机预测的人来说，另一个更简单的选择就是让神经网络学习一些平均值和标准差。虽然神经网络本身现在是确定性的而不是随机的，但它仍然允许我们在假设某种分布的情况下从平均值和标准差中进行采样。 这有意义吗？因此，如果正在寻找神经网络的随机结果，但又不想考虑贝叶斯神经网络的额外成本，那么选项二似乎很有吸引力。 如果您同意我所写的内容，请告诉我或不。我很高兴听到您的意见:)   由   提交/u/andre2500_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1advijz/bayesian_nns_vs_learning_variance_and_mean/</guid>
      <pubDate>Mon, 29 Jan 2024 13:38:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何为 RAG 划分块</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1adtuzr/d_how_to_divide_a_chunk_for_rag/</link>
      <description><![CDATA[大家好， 我需要一些建议，假设您正在构建一个 RAG。您希望上下文块的长度为 512 个令牌。如何在不失去语义联系的情况下划分 1000 多个段落。 有关更多信息，它是一个问答机器人，那个巨大的段落是对一个常见问题的回答。   由   提交 /u/Lathanderrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1adtuzr/d_how_to_divide_a_chunk_for_rag/</guid>
      <pubDate>Mon, 29 Jan 2024 12:10:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 不懂基础的LLM专家？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1admb6z/d_llm_experts_who_dont_know_basics/</link>
      <description><![CDATA[我最近遇到了很多人，他们知道 LLM 领域中不同技术的所有奇特缩写词，诚然我也是新手但越来越明显的是，他们甚至不知道 DL 的基础知识，比如背景是什么或其他经典概念。 这是否会成为现状，因为 LLM 领域更倾向于配置而不是做事从零开始？ 还有，这些人真的可以被认为是法学硕士还是表面上的专家？    由   提交/u/Plus_Tough_7497   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1admb6z/d_llm_experts_who_dont_know_basics/</guid>
      <pubDate>Mon, 29 Jan 2024 04:13:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>