<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 10 Dec 2023 15:12:40 GMT</lastBuildDate>
    <item>
      <title>[R] 添加和细化：时间点过程的扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f53oy/r_add_and_thin_diffusion_for_temporal_point/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.01139代码：https://github。 com/davecasp/add-thin https://preview.redd.it/ydhlwfte9h5c1.jpg?width=2544&amp;format=pjpg&amp;auto=webp&amp;s=1c11a4a1f8cab2626e46546d589c35afb5e0 2fea 生成扩散模型都是愤怒，但尚不清楚如何将它们应用于不同数量事件的序列，例如推文、Reddit 评论或出租车行程。我们提出了一种扩散方法，可以将任何序列转换为噪声序列，即来自齐次泊松过程的样本。相反，我们的模型通过删除分类为噪声的事件并从学习的（条件）强度分布中提出新事件，将此类噪声序列样本迭代地转换为来自任何目标数据分布的样本。通过实验，我们能够在各种基准数据集上展示出色的预测性能。 我期待在 reddit 上讨论我们的方法。如果您碰巧在 NeurIPS，您还可以在周二上午 10:45 的海报#602 与 /u/davidluedke 见面 -中午 12:45！   由   提交 /u/martenlienen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f53oy/r_add_and_thin_diffusion_for_temporal_point/</guid>
      <pubDate>Sun, 10 Dec 2023 14:45:39 GMT</pubDate>
    </item>
    <item>
      <title>[项目] Inside Marker：人工智能驱动的 PDF 布局检测引擎的引导源代码之旅</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f4fwi/project_inside_marker_a_guided_source_code_tour/</link>
      <description><![CDATA[       由   提交 /u/shrsv   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f4fwi/project_inside_marker_a_guided_source_code_tour/</guid>
      <pubDate>Sun, 10 Dec 2023 14:11:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 很困惑，我该如何处理这个客户购买倾向模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f48bv/d_confused_how_do_i_go_about_this_customer/</link>
      <description><![CDATA[所以我有这个数据集，其中包括时间戳、购买的产品类别以及有关用户在电子商务平台上的行为的其他信息，例如：他们购买的频率，来自哪个类别，imp 评论如何，添加到购物车，放弃购物车，保存以供以后使用等以及所有详细信息。 现在我想以此为基础训练一个模型，然后制作这个虚拟电子商务对客户进行模拟，其中他们执行的某些操作将被添加到实时数据库中（客户可能存在多个会话，其中日期也是分析的输入），模型可以处理这些操作，然后仪表板可以显示可能性他们将在接下来的几周/几个月内从某个产品类别购买。 我已经与 ml 和 dl 合作过，但我对它仍然很陌生，所以如果这是愚蠢的事情，我提前道歉，但我很困惑，就像我缺少一些关于如何创建它并使其工作的细节。有什么建议吗？   由   提交 /u/EuphoricPirateVal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f48bv/d_confused_how_do_i_go_about_this_customer/</guid>
      <pubDate>Sun, 10 Dec 2023 14:01:07 GMT</pubDate>
    </item>
    <item>
      <title>我需要在大型语言模型的大纲中添加什么？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f3y6e/what_do_i_need_to_add_to_this_outline_on_large/</link>
      <description><![CDATA[      我正在考虑整理一个大纲，代表一个好的方法从大型语言模型的初学者到专家。我觉得我已经完成了大纲的很大一部分，但总有改进的空间。 我希望这个大纲能够帮助那些具有基本编程技能的人，并让他们达到他们现在的水平在生产中利用开源大型语言模型（“LLM”）。 因为将大纲变成更大的东西很诱人，所以我想接触社区。您认为机器学习中重要的是什么？ 我还应该在这个大纲中添加什么？ ​ https://preview.redd.it/1ekzdbgw1h5c1.png?width=655&amp;format= png&amp;auto=webp&amp;s=4f8676a947c2085749de6b36b1d64e5051eb88e5 ​ https://preview.redd.it/9c81n9hx1h5c1.png?width=633&amp;format=png&amp;auto=webp&amp;s=d28aca2cc76892a 192315daa19a21c266dbd8716    由   提交 /u/whiteowled   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f3y6e/what_do_i_need_to_add_to_this_outline_on_large/</guid>
      <pubDate>Sun, 10 Dec 2023 13:45:58 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 归一化流（如果有）相对于 GAN 和 VAE 的优势？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f2kzy/discussion_advantages_of_normalizing_flow_if_any/</link>
      <description><![CDATA[鉴于这最后一篇文章，我认为对这个主题进行更新的讨论可能会很有用。即，与其他方法（例如 GAN、VAE、带有 SDE 的贝叶斯神经网络）相比，标准化流有哪些实际优势今天？ 我对这个主题也很陌生。如果问题太幼稚请见谅。   由   提交/u/WorldML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f2kzy/discussion_advantages_of_normalizing_flow_if_any/</guid>
      <pubDate>Sun, 10 Dec 2023 12:29:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在像 Hugging Face 这样的库中，在增加复杂性的同时保持代码可读性和代码质量的好方法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f2c2d/d_what_is_a_good_way_to_maintain_code_readability/</link>
      <description><![CDATA[HF 是使用基于变压器或扩散模型的好地方。他们拥有一组一致的 API，适用于各种模型。但如果你想调整一些东西或者只是看看引擎盖下的东西，事情就会很快走下坡路。一些例子：  diffusers 在改进 CompVis 的 LDM 实现和模块化不同组件方面做得很好，同时保持了 API 的一致性。但随着时间的推移，dreambooth、LoRA、LoHA、ControlNet、适配器等的引入是通过大量的猴子修补和代码扩展来完成的。  在 transformers 中，他们非常努力地尝试让单个函数或方法处理自注意力机制和交叉注意力机制、掩蔽、位置和相对编码虽然它允许用户对任何模型使用相同的函数/方法，但它导致了严重的参数膨胀。只需将 FAIR 的 llama 原始实现与 HF 的实现 来了解一下。   即使是像标记化或文本生成这样的简单想法也会有类似的代码膨胀。在这一点上，我会非常谨慎地尝试在 HF 存储库之上构建任何东西，甚至使用它来理解某些东西是如何实现的。 我只是想用这个批评来谈谈如何实现 构建优秀的 ML 相关开源软件。在牺牲几乎所有其他东西的同时，在广泛不同的模型中实现大规模一致性是一个好主意吗？有人已经做对了吗？我发现 FAIR 的代码是迄今为止最好、最简单的。   由   提交/u/nivter  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f2c2d/d_what_is_a_good_way_to_maintain_code_readability/</guid>
      <pubDate>Sun, 10 Dec 2023 12:14:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] LVE 项目：第一个 LLM 漏洞开源存储库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f24yg/p_lve_project_first_open_source_repository_of_llm/</link>
      <description><![CDATA[Github：https://github.com/ lve-org/lve/ 博客文章：https://lve -project.org/blog/launching-the-lve-project.html  大家好！我们最近公开宣布了我们的开源项目 LVE，其目标是以透明且可重现的方式跟踪和记录法学硕士的安全问题。该项目的范围是与隐私、安全、可靠性和其他形式的安全故障有关的问题。通过 LVE，我们希望帮助模型制作者、人工智能开发人员和公众更好地了解法学硕士的功能和漏洞。我们已经支持许多不同的模型变体，并且正在努力添加更多。  我们还在我们的网站上推出了一系列社区挑战，这是类似赏金的小游戏，每个人都可以参与其中并帮助我们攻击和红队LLM。 我们很高兴听到反馈，让人们参与该项目，或者只是让人们参加我们的社区挑战（https://lve-project.org /挑战/）。让我们知道您的想法！   由   提交/u/bmislav  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f24yg/p_lve_project_first_open_source_repository_of_llm/</guid>
      <pubDate>Sun, 10 Dec 2023 12:01:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 代码链：使用语言模型增强代码模拟器进行推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f1827/r_chain_of_code_reasoning_with_a_language/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.04474 OpenReview：https:// /openreview.net/forum?id=tlRUbI0Yf3 项目页面：https://chain-of-code.github.io/ 摘要：  代码提供了一个通用的与代码解释器配合使用时，可以构建复杂程序并执行精确计算的语法结构——我们假设语言模型（LM）可以利用代码编写来改进思想链推理，不仅适用于逻辑和算术任务，还适用于语言任务（特别是那些两者兼而有之的）。例如，考虑提示 LM 编写代码来计算它在文章中检测到讽刺的次数：LM 可能很难编写“detect_sarcasm(string)”的实现可以由解释器执行（处理边缘情况将是难以克服的）。然而，如果语言模型不仅用于编写代码，而且还用于有选择地“模拟”模型，那么它们仍然可以产生有效的解决方案。解释器通过生成“Detect_sarcasm(string)”的预期输出来执行该操作。以及其他代码行（例如，解释器无法编译的代码行）。在这项工作中，我们提出了代码链（CoT），这是一种简单但令人惊讶的有效扩展，可以改进 LM 代码驱动的推理。关键思想是鼓励 LM 将程序中的语言子任务格式化为灵活的伪代码，编译器可以显式捕获未定义的行为并转交给 LM 进行模拟（作为“LMulator”）。实验表明，在各种基准测试中，代码链优于思想链和其他基线；在 BIG-Bench Hard 上，Chain of Code 达到了 84%，比 Chain of Thought 提高了 12%。 CoT 可以很好地适应大型和小型模型，并扩大了 LM 可以通过“代码思考”正确回答的推理问题的范围。项目网页：此 https URL.  &lt;!-- SC_ON - -&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f1827/r_chain_of_code_reasoning_with_a_language/</guid>
      <pubDate>Sun, 10 Dec 2023 11:02:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对使用 LSTM 进行时间序列预测（如包括天气预报）的质疑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f12cj/d_doubts_on_the_implementation_of_lstms_for/</link>
      <description><![CDATA[我正在尝试使用 LSTM 来预测未来的许多步骤。 我想到的第一个疑问是什么预测多个时间步长的正确方法。这就是我的想法： * 为模型的每次前向传递仅预测一个时间步，然后将最后的输出重新用于连续的输入。有点像自我反馈的无限循环。 * 预测具有形状的固定时间窗口（未来数据点、目标）。我通过向量化该矩阵并使用形状（未来数据点*目标）的密集层作为最后一层来完成此操作。最后用所需的尺寸重塑它。 这些方法有任何意义吗？最好的选择是什么？还有更好的吗？ 第二个问题是：如果我有一个历史天气和商店顾客数量的数据集，我如何建立一个模型，以最后一个作为输入N 对天气和 n_clients 也能够将未来几天的天气预报作为输入？ 换句话说，我不想仅使用历史数据（n_clients + 过去的数据）来预测未来的客户端天气），但以某种方式添加其他功能（天气预报）可能会提高模型预测的准确性。   由   提交 /u/IonizedRay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f12cj/d_doubts_on_the_implementation_of_lstms_for/</guid>
      <pubDate>Sun, 10 Dec 2023 10:52:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 玻尔兹曼机《进化方程》</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f0qcf/d_boltzmann_machine_evolution_equation/</link>
      <description><![CDATA[在阅读有关玻尔兹曼机的内容时，我不断看到诸如“让系统达到热平衡状态”之类的表述。我了解这在实际物理系统中是如何工作的，但这在机器学习模型的上下文中意味着什么？假设我们将可见神经元设置为某种特定状态。接下来到底应该发生什么？隐藏神经元更新的规则是什么？我说的是具体的指令，比如取这个神经元值，乘以这个系数等等，不断重复直到达到某种条件。由于某种原因我在任何地方都找不到它，请帮忙，将不胜感激   由   提交 /u/fatCrookNewJersey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f0qcf/d_boltzmann_machine_evolution_equation/</guid>
      <pubDate>Sun, 10 Dec 2023 10:29:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机械手旋转番茄土豆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eviwp/r_robot_hand_rotates_tomato_potato/</link>
      <description><![CDATA[       由   提交/u/XiaolongWang  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eviwp/r_robot_hand_rotates_tomato_potato/</guid>
      <pubDate>Sun, 10 Dec 2023 04:32:23 GMT</pubDate>
    </item>
    <item>
      <title>【研究】为什么**液体神经网络**在人工智能研究或实际应用中不像其他神经网络架构那样突出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18etkop/research_why_arent_liquid_neural_networks_not/</link>
      <description><![CDATA[ 由   提交 /u/sivav-r   /u/sivav-r  science.org/doi/10.1126/scirobotics.adc8892&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18etkop/research_why_arent_liquid_neural_networks_not/</guid>
      <pubDate>Sun, 10 Dec 2023 02:36:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 思想的一切：挑战彭罗斯三角定律的思想生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eq228/r_everything_of_thoughts_defying_the_law_of/</link>
      <description><![CDATA[   /u/Yogurt789   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eq228/r_everything_of_thoughts_defying_the_law_of/</guid>
      <pubDate>Sat, 09 Dec 2023 23:29:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] Alpha-CLIP：专注于任何你想要的地方的 CLIP 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/</link>
      <description><![CDATA[CLIP 非常适合理解整个图像，但将注意力集中在特定区域可以提高性能。一篇新论文提出了 Alpha-CLIP。关键的变化是向 CLIP 的图像编码器添加 Alpha 通道，提供重要区域的透明度图。 Alpha-CLIP 与聚焦区域的 Alpha 通道并行运行普通 CLIP。它经过训练可以最大限度地减少组合嵌入和文本描述之间的差异。注意力图表明，这会比基本 CLIP 模型产生更集中的结果，从而带来更好的性能。 论文中的实验表明 Alpha-CLIP 具有更好的对象识别、定位、图像区域推理能力、文本到图像生成控制和 3D 优化。它在各种下游任务（例如文本条件对象检测和文本引导图像操作）中优于 CLIP。 仍然存在一些限制，例如同时处理多个不同区域。但总的来说，简单地添加引导注意力机制可以增强 CLIP 的能力，而不会失去全局背景。 Alpha-CLIP 展示了大型视觉模型中集中理解和控制的有前途的方向。 我认为与 10 月份发布的显式注意力寄存器研究有一些有趣的相似之处（更多详细信息请点击此处）。 TLDR：为区域指导添加 Alpha 通道可提高 CLIP 的性能跨需要本地化图像理解和生成的任务。 完整摘要在这里。论文这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/</guid>
      <pubDate>Sat, 09 Dec 2023 19:46:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>