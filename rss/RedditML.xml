<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Sat, 03 Aug 2024 01:05:47 GMT</lastBuildDate>
    <item>
      <title>[R]，[P] RPC — 构建语言模型的新方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eipn2t/r_p_rpc_a_new_way_to_build_language_models/</link>
      <description><![CDATA[文章：RPC — 构建语言模型的新方法 我非常喜欢软件工程的原因之一是，任何人都可以用一台计算机做几乎任何事情。但是当涉及到人工智能，特别是法学硕士时，你需要大量的资源和金钱才能自己做任何有趣的事情。 所以最近我一直在尝试寻找一种方法来构建语言模型，使用更少的训练数据和更少的计算。RPC 是我最接近的尝试。它将提示压缩为向量表示，然后在向量数据库中执行搜索以找到最合适的下一个标记。它工作得非常好。 我还没有时间对它进行适当的评估和测试。这就是我与社区分享这一点的原因，希望有人能给出一些反馈，甚至尝试复制它。我希望你能看看这篇文章，并在这里分享一些想法。    提交人    /u/someuserwithwifi   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eipn2t/r_p_rpc_a_new_way_to_build_language_models/</guid>
      <pubDate>Sat, 03 Aug 2024 00:20:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] NER 和 NLI</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eioapp/d_ner_and_nli/</link>
      <description><![CDATA[通过使用自然语言推理 (NLI) 数据进行训练，文本分类得到了增强。 我正在寻找使用 NER 任务丰富 NLI 和/或使用 NLI 任务丰富 NER 的论文/研究作品。    提交人    /u/FeatureBackground634   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eioapp/d_ner_and_nli/</guid>
      <pubDate>Fri, 02 Aug 2024 23:17:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士 (LLM) 面试准备</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ein9vh/d_llm_interview_prep/</link>
      <description><![CDATA[大家好， 我即将参加一场 LLM/NLP 面试。我想寻求一些建议，比如面试时应该关注哪些主题、面试中应该期待什么以及任何建议的学习材料。我听说团队专注于公司内部的所有 LLM 事务，例如自托管、优化、微调等。 以下是我计划涉及的一些领域：  了解 LLM 的工作原理（内部） 微调技术 RAG NLP 基础  有人可以分享他们参加类似面试的经验吗？我应该优先考虑这些主题的哪些具体方面？我还遗漏了其他重要领域吗？我对 RAG 有基本的了解，但了解得不是太深入。  此外，如果您对论文或在线资源有任何建议，可以帮助我准备，我将不胜感激！    提交人    /u/kkziga   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ein9vh/d_llm_interview_prep/</guid>
      <pubDate>Fri, 02 Aug 2024 22:31:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 又一家耀眼的初创公司倒闭了？Character.Ai 创始人重返谷歌有何感想</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eijmhl/d_another_flashy_startup_folds_thoughts_on/</link>
      <description><![CDATA[链接：https://techcrunch.com/2024/08/02/character-ai-ceo-noam-shazeer-returns-to-google/    由   提交  /u/OpeningVariable   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eijmhl/d_another_flashy_startup_folds_thoughts_on/</guid>
      <pubDate>Fri, 02 Aug 2024 19:58:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] SSM 和 RNN 之间的区别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eih75h/d_difference_between_ssms_and_rnns/</link>
      <description><![CDATA[大家好 :) 我目前正在深入研究状态空间模型。但是，我不太明白状态空间模型的离散化版本与传统 RNN 架构有何不同。隐藏状态更新本质上与之前的隐藏状态相同，新输入通过权重矩阵进行转换。 我唯一的猜测是离散化过程包含学习参数，这是与经典 RNN 的不同之处。但是，我不太确定这是否正确或有任何意义。直观地看，两者的公式在我看来非常相似。 你能帮我解释一下它们有什么区别吗？    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eih75h/d_difference_between_ssms_and_rnns/</guid>
      <pubDate>Fri, 02 Aug 2024 18:19:08 GMT</pubDate>
    </item>
    <item>
      <title>CoRL 2024 评论 [讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eih0yf/corl_2024_reviews_discussion/</link>
      <description><![CDATA[CoRL 2024 评论已经出炉。你们怎么样了？ 我们得到了 4,4,3（2 次强烈拒绝，1 次一周拒绝）。这真的很令人沮丧，因为这是我作为本科生的第一篇主要论文，我们论文的第一个版本被 ICRA 接受了。    提交人    /u/oz_zey   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eih0yf/corl_2024_reviews_discussion/</guid>
      <pubDate>Fri, 02 Aug 2024 18:12:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助在 Azure Kubernetes 上扩展 LLM 推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eig5v6/d_help_scaling_llm_inference_on_azure_kubernetes/</link>
      <description><![CDATA[大家好， 我正在尝试更好地了解如何设置我们自己的内部无服务器基础架构，以部署 LLM（针对 Gemma2 和 Llama 3.1 变体）以实现可扩展推理。 为了解决问题： 我们有严格的数据隐私和管理要求，因此不能依赖 Groq、openrouter 等推理 API。 我正在尝试了解正确的架构和执行此操作的成本。  带有 KEDA 的 Azure Kubernetes 服务似乎是一个很好的解决方案，但我不确定自动缩放的成本和速度。 理想的情况是 KEDA 从 0 个节点扩展到所需的节点数，以维持每个请求每秒 20-40 个令牌，然后尽快回落 向其他开放建议/链接/想法  有人尝试过这样做吗？您能否从高层次上比较现成的 API 和高度可扩展/空闲待机解决方案之间的成本？ 我知道 localllama 喜欢 vllm/llma.cpp，但是否有其他推荐的 llm 推理服务器堆栈在生产中是首选？ 谢谢！    提交人    /u/chulpichochos   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eig5v6/d_help_scaling_llm_inference_on_azure_kubernetes/</guid>
      <pubDate>Fri, 02 Aug 2024 17:36:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 作为机器学习工程师最困难的事情是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eifnvj/d_what_is_the_hardest_thing_as_a_machine_learning/</link>
      <description><![CDATA[我刚刚开始我的机器学习之旅。为了练习，我从 Kaggle.com 获取数据，但我决定通过自己收集数据来进一步挑战自己。我发现收集大量数据非常具有挑战性。通常如何收集数据，还有什么比这更难的吗？    提交人    /u/3ATAE   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eifnvj/d_what_is_the_hardest_thing_as_a_machine_learning/</guid>
      <pubDate>Fri, 02 Aug 2024 17:16:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] NLP 论文的新常态是“提示工程”论文吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ei9e3l/d_is_the_new_norm_for_nlp_papers_prompt/</link>
      <description><![CDATA[很多论文似乎本质上都是“我们如何让 LLM 1 不经过任何培训就能做到这一点？”我已经有一段时间没有发表过文章了，过去几年一直在行业内工作。我最近加入了一家新公司，担任一个稍微更偏向研究的职位，与研究科学家和研究生实习生一起工作。我注意到他们每个人都在做一些我在研究生院会受到 PI 斥责的事情。基本上就是“我们如何让 LLM 不经过任何培训就能完成这个非常复杂的任务？”也许并不出人意料的是，在很多情况下，你做不到。我想这就是为什么现在 NLP 领域有这么多负面结果的论文。 这是新常态吗？浏览 arXiv 的 CL 部分已经变得很痛苦了。 98% 的论文都是类似“LLaMA 怎么会不懂数字？”这样的问题。 我想知道我是不是酒吧角落里那个老糊涂，还是大家都有同样的感受。    提交人    /u/Seankala   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ei9e3l/d_is_the_new_norm_for_nlp_papers_prompt/</guid>
      <pubDate>Fri, 02 Aug 2024 12:57:27 GMT</pubDate>
    </item>
    <item>
      <title>torch 高斯随机权重初始化和 L2 正则化 [D][R][P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ei8xn9/torch_gaussian_random_weights_initialization_and/</link>
      <description><![CDATA[我有一个线性/全连接的 torch 层，它接受 latent_dim 维输入。该层中的神经元数量 = 高度 \ 宽度*:  # 定义当前层的超参数 - height = 20 width = 20 latent_dim = 128 # 初始化线性层 - linear_wts = nn.Parameter(data = torch.empty(height * width, latent_dim), require_grad = True) &#39;&#39;&#39; torch.nn.init.normal_(tensor, mean=0.0, std=1.0, generator=None) 用从正态分布中抽取的值填充输入张量 - N(mean, std^2) &#39;&#39;&#39; nn.init.normal_(tensor = linear_wts, mean = 0.0, std = 1 / np.sqrt(latent_dim)) print(f&#39;1/sqrt(d) = {1 / np.sqrt(latent_dim):.4f}&#39;) print(f&#39;SOM 随机 wts; min = {som_wts.min().item():.4f} &amp;&#39; f&#39; max = {som_wts.max().item():.4f}&#39; ) print(f&#39;SOM 随机 wts; mean = {som_wts.mean().item():.4f} &amp;&#39; f&#39; std-dev = {som_wts.std().item():.4f}&#39; ) # 1/sqrt(d) = 0.0884 # SOM 随机 wts；min = -0.4051 &amp; max = 0.3483 # SOM 随机 wts；mean = 0.0000 &amp; std-dev = 0.0880  问题 1：对于 std-dev = 0.0884（大约），根据最小值和最大值 -0.4051 和 0.3483，似乎正常初始化程序正在计算距离平均值 = 0 的 +3.87 个标准差和距离平均值 = 0 的 -4.4605 个标准差。这是正确的理解吗？我假设权重是从距离平均值 +3 和 -3 个标准差中抽取的？ 大多数最近的自监督学习论文，即 SimCLR、MoCo、Barlow Twins、SwAV、BYOL、SimSiam 等，都使用 L2 标准化输出进行成本函数计算和训练。受此启发，我尝试了以下操作： 问题 2：我希望此线性层的输出是 L2 归一化的，这样它就位于单位超球面上。输入也是 L2 归一化的，因此为了维护输入的比例并将其映射到权重的比例，您可以执行一次性操作：```linear_wts.data.copy_(nn.Parameter(data = F.normalize(input = linear_wts.data, p = 2.0, dim = 1)))```，然后照常训练 有什么想法吗？    提交人    /u/grid_world   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ei8xn9/torch_gaussian_random_weights_initialization_and/</guid>
      <pubDate>Fri, 02 Aug 2024 12:35:18 GMT</pubDate>
    </item>
    <item>
      <title>我制作了一个 SWE 套件，以便轻松构建 SWE Agent [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ei0pd7/i_made_a_swe_kit_for_easy_swe_agent_construction_p/</link>
      <description><![CDATA[大家好！我很高兴与大家分享一个新项目：SWEKit，这是一个使用 Composio 工具生态系统构建软件工程代理的强大框架。 目标 SWEKit 允许您：  使用 CrewAI 和 LlamaIndex 等框架构建开箱即用的代理。 添加或优化代理的能力。 根据 SWE-Bench 对您的代理进行基准测试。  实施细节  使用的工具：Composio、CrewAI、Python  设置：  安装您选择的代理框架和 Composio 插件 代理需要 github 访问令牌才能与您的存储库配合使用 您还需要设置 API 密钥适用于您计划使用的 LLM 提供程序  搭建并运行您的代理 工作区环境： SWEKit 支持不同的工作区环境：  主机：在主机上运行。 Docker：在 Docker 容器内运行。 E2B：在 E2B 沙箱内运行。 FlyIO：在 FlyIO 机器内运行。  运行基准测试：  SWE-Bench 使用来自流行 Python 开源项目的实际问题来评估软件工程代理的性能。  GitHub 欢迎探索该项目，如果发现它有用，请给它一颗星，并让我知道您的想法或改进建议！🌟    提交人    /u/kingai404   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ei0pd7/i_made_a_swe_kit_for_easy_swe_agent_construction_p/</guid>
      <pubDate>Fri, 02 Aug 2024 04:05:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有什么好的可微调的 TTS？（不是 Coqui 或 TorToiSe）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ehw1nf/d_any_good_finetunable_tts_not_coqui_or_tortoise/</link>
      <description><![CDATA[大家好！我有一个非常感性的数据集，里面有大约 20-30 分钟的清晰的英语语音录音。我想制作一个微调模型来完全复制该声音，并且能够在没有 CUDA 的情况下仅使用 MPS 进行推理。除了 Coqui 和 TorToiSe（它们不适用于高音调数据）之外，还有什么想法以及好的分步文档吗？    提交人    /u/yukiarimo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ehw1nf/d_any_good_finetunable_tts_not_coqui_or_tortoise/</guid>
      <pubDate>Fri, 02 Aug 2024 00:10:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在专注于 LLM 的初创公司工作值得吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ehudwq/d_is_it_worth_working_at_startups_focusing_on_llm/</link>
      <description><![CDATA[一些正在开发 LLM 产品的初创公司的 HR 找到了我。根据我们的讨论，他们仍然主要将流行的开放 AI 模型或开源模型 (llama3) 用于他们的 LLM 产品（这并不奇怪），我认为工程师将主要做的事情是快速工程和/或微调。我对 LLM 非常感兴趣，我相信这是一个相当突出的未来（如果我错了，请纠正我），但是我不确定如果我在使用这些模型的初创公司工作，我能在 LLM 领域达到多大的深度，可能只有快速工程和微调技术？在这些初创公司工作是否会让我在直接申请这些大型模型公司（Meta、Google、Open AI 等）的 LLM 职位时更有竞争力？ 我想在 LLM 领域获得更多知识，但我不知道在只使用带有 API 调用的 LLM 模型的公司工作是否是一个好的起点。任何建议都值得赞赏！    提交人    /u/Upbeat-Carrot1999   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ehudwq/d_is_it_worth_working_at_startups_focusing_on_llm/</guid>
      <pubDate>Thu, 01 Aug 2024 22:55:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jul 2024 15:00:14 GMT</pubDate>
    </item>
    </channel>
</rss>