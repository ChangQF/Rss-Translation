<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Fri, 14 Jun 2024 21:14:50 GMT</lastBuildDate>
    <item>
      <title>[D] Nemotron-4 340b详细分析</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfywwi/d_nemotron4_340b_detailed_analysis/</link>
      <description><![CDATA[      我查看了 NVIDIA 的 340B Nemotron LLM - 我发现了一些问题：  Squared ReLU 与 Llama SwiGLU 不同，Gemma GeGLU。与 arxiv.org/pdf/2002.05202 中发现的 GLU 变体不同（GLU 变体改进了 Transformer，Noam Shazeer） ReGLU 是 [ ReLU(X * W_gate) * (X * W_up) ] * W_down 我们需要 2 个 ReLU + 绑定权重 [ ReLU(X * W_up) * ReLU(X * W_up) ] * W_down，因此有点像 GLU，但不完全相同 为什么 Squared ReLU 有效？入门论文：https://arxiv.org/abs/2109.08668v2 发现了它。还有 Shazeer 的引言：“我们没有解释为什么这些架构似乎有效；我们将它们的成功归功于上帝的仁慈”  使用 rotary_percentage 为 50%。可能与 Phi-2 的 partial_rotary_factor 有关？- Phi-2 的 rotary_percentage 为 40%，因此对于 Nemotron 来说，似乎只有 50% 的 Q、K 矩阵应用了 RoPE，其余的则不使用 RoPE。  请参阅https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi/modeling_phi.py#L79  解开嵌入，如 Llama。Gemma 已绑定。  Tied 还用于 Apple 的设备 LLM，以节省 VRAM。 根据 LLM 物理学论文 https://arxiv.org/abs/2404.05405  常规 layernorm，不同于 Llama RMS LN。RMS Layernorm 消除了偏差，但不进行均值消除 没有 dropout，没有偏差，如 Llama、Gemma 批量大小随着 42% MFU 增加。Float16 训练没有稀疏性。 4096 序列长度。遗憾的是相当短。 8 万亿代币 - 3 种风格基础、指导、奖励 SFT、DPO、RPO  RPO - 我认为是 2 个步骤？奖励感知偏好优化 HelpSteer2 数据集：https://huggingface.co/datasets/nvidia/HelpSteer2  在某些基准测试中击败 GPT-4o  技术报告：https://research.nvidia.com/publication/2024-06_nemotron-4-340b HF Instruct：https://huggingface.co/nvidia/Nemotron-4-340B-Instruct https://preview.redd.it/vhcke1177l6d1.png?width=980&amp;format=png&amp;auto=webp&amp;s=2458d6c2bd9ed0db14e9f16c5be6e93340f6bfaf    由    /u/danielhanchen 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfywwi/d_nemotron4_340b_detailed_analysis/</guid>
      <pubDate>Fri, 14 Jun 2024 19:20:53 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我设计的这个 MLP 参数减少了 63%，这是一个重大进展吗？（在 nanoGPT 上测试）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfyq4y/p_is_this_mlp_i_designed_with_63_less_parameters/</link>
      <description><![CDATA[我切换了激活函数并对 nanoGPT 中的 MLP 做了一些小改动。采用修改后的 MLP 的 nanoGPT 实现了略低的损失，但仅使用了 shakespeare-char 数据集上约 60% 的参数。    Vanilla nanoGPT nanoGPT + 修改后的 MLP    总非嵌入参数 10.65M 6.22M   总 MLP 参数 7.07M 2.65M   每个块的 MLP 参数 1.17M 0.44M   训练损失 ？ 1.0249   验证损失 1.4697 1.4647   总结一下，我对 nanoGPT MLP 的小小修改导致了：  总参数减少 0.42% MLP 参数减少 63% 验证损失改善不到 1%  这是一个重大改进，还是已经有这样的优化已经制作了吗？考虑到我的 RX 6600 相对较慢，我该如何进行更多测试？将 MLP 结构发布到 GitHub 是否值得？    提交人    /u/IonizedPro   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfyq4y/p_is_this_mlp_i_designed_with_63_less_parameters/</guid>
      <pubDate>Fri, 14 Jun 2024 19:12:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 ImageNet 上测试的相关性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfxctf/d_relevance_of_testing_on_imagenet/</link>
      <description><![CDATA[我正在撰写一篇关于网络优化的有趣发现的论文/调查。我想展示我得到的有趣结果；我正在将新优化器的性能与 Adam / AdamW / SGD / SGD 与 Nesterov 进行比较，并在 MNSIST CIFAR10/100 上进行了大量测试。我周围的一些人告诉我，通过在 ImageNet 上进行测试，我必须走得更远。 但是，测试多种配置所需的规模和时间将非常庞大。所以，我的问题是，ImageNet 真的是一个必不可少的基准，还是 MNIST 和 CIFAR 足以证明大多数论文的观点？我理解我的同事提出的论点，即它绝对是一个更“详尽”和更现实的基准，但我觉得计算障碍相当大。 有什么评论或意见吗？    提交人    /u/Secret-Toe-8185   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfxctf/d_relevance_of_testing_on_imagenet/</guid>
      <pubDate>Fri, 14 Jun 2024 18:12:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Conda VS Python 的内置 'venv'</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfwteq/d_using_conda_vs_pythons_inbuilt_venv/</link>
      <description><![CDATA[嘿， 因此，我通常在克隆项目时使用 venv 模块，但我目前正在使用用 C++ 编写的 OpenCV，而 python 模块只是这个 c++ 的包装器。 因此，我必须安装系统级依赖项。这意味着 venv 模块根本无法工作（如果我错了请纠正我），我必须使用 Conda。 我看到关于 Conda 已经过时并且正在使用新东西的文章。你们能确认/分享经验吗！ 谢谢    提交人    /u/No_Weakness_6058   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfwteq/d_using_conda_vs_pythons_inbuilt_venv/</guid>
      <pubDate>Fri, 14 Jun 2024 17:49:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在多个虚拟机上使用 DDP 时，有没有关于 PyTorch 性能的基准测试？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfwq98/d_any_benchmarks_about_pytorchs_performance_when/</link>
      <description><![CDATA[我试图找到任何基准，了解在多个虚拟机上使用 DDP 时性能会下降多少（如果有的话）。因此，不只是一台具有多个 GPU 的虚拟机，而是实际上跨集群中的多台机器。你对此了解多少？ 此外，你们中是否有人使用过这种设置与低优先级虚拟机？我试图弄清楚，如果在跨多个低优先级虚拟机进行训练时集群中的某些虚拟机被关闭，这种设置的恢复能力如何。    提交人    /u/lifesthateasy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfwq98/d_any_benchmarks_about_pytorchs_performance_when/</guid>
      <pubDate>Fri, 14 Jun 2024 17:45:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在 4 个节点 x 4x A100 GPU 上的 Slurm 中使用 Axolotl 执行多节点微调？[项目]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfw4a3/how_to_perform_multinode_finetuning_with_axolotl/</link>
      <description><![CDATA[我对 Slurm 还比较陌生，正在寻找一种有效的方法来在系统内设置集群，如标题所述（不一定需要是 Axolotl，但最好是）。一种方法可能是通过在“加速配置”/deepspeed 中输入其他服务器的 IP 来配置多个节点，(https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/docs/multi-node.qmd) 定义服务器 1、2、3、4，并允许通过 SSH 或 HTTP 以这种方式进行通信。但是，这种方法似乎很不干净，而且没有太多令人满意的信息。有没有人有使用 Slurm 的经验，做过类似的事情，可以帮我吗？ :)    由   提交  /u/AppropriateCan5964   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfw4a3/how_to_perform_multinode_finetuning_with_axolotl/</guid>
      <pubDate>Fri, 14 Jun 2024 17:19:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] 预训练 LLM 在 Amazon Sagemaker 中的文本分类评估</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfu9b4/r_pretrained_llms_evaluation_in_text/</link>
      <description><![CDATA[我很好奇为什么在 jumpstart 上没有评估预训练文本分类 llms 的选项。我应该部署它们并运行推理吗？我的目标是查看一些大型模型在预测我的自定义数据集（金融文本，它必须预测它是声明还是前提 - 它是受监督的）上的标签时的准确性。我误解了什么吗？    提交人    /u/marshallggggg   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfu9b4/r_pretrained_llms_evaluation_in_text/</guid>
      <pubDate>Fri, 14 Jun 2024 15:58:36 GMT</pubDate>
    </item>
    <item>
      <title>我训练一名法学硕士利用我的 WhatsApp 聊天记录冒充我 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfti9v/i_trained_an_llm_on_my_whatsapp_chats_to/</link>
      <description><![CDATA[      我最近有了一个想法：如果我使用我多年的 WhatsApp 聊天记录微调一个大型语言模型 (LLaMA-3-8B)，看看它是否可以模仿我写消息的方式，会怎么样？ 首先，我下载了我的聊天记录。WhatsApp 有一个功能，可让您以纯文本 (.txt 格式) 导出聊天记录。下载数据后，我进行了一些简单的预处理（例如删除非常长的消息），但大部分都保留了原始数据以保持真实性。我使用 HTML 标签格式化消息以包含发送者和接收者的消息，并将最大上下文长度设置为 2048。 我最终得到了 200K 条单独的消息 - 足够的数据让 LLM “学习”我的消息风格。我选择 LLaMA-3-8B 是因为它是目前最好的模型之一（我想）。微调过程大约需要一个小时。 结果出乎意料的好！我尝试与 LLM 聊天，并让它以我的名义回复。它模仿我做得很棒。为了更进一步，我使用 Selenium 创建了一个 Python 脚本，该脚本打开 WhatsApp Web，监听任何新消息，并使用 LLM 自动输入回复。我的人工智能成功地欺骗了我的朋友，让他们相信他们正在与真正的我交谈！最终，他们意识到了这一点，因为 LLM 使用了过多的表情符号，或者偶尔会用一些完全不合逻辑的东西回复。 我以我自己 (Pritish) 的名字将 LLM 命名为“PriBot”。以下是我的实验的一些结果： 示例 1 示例 2 示例 3 示例 4 有关此项目的更多信息，请查看我的 YouTube 频道。这有点自我宣传，但其中包含有关该项目的更多详细信息。您可以在下面看到最终结果👇 https://www.youtube.com/watch?v=hZ95S76uZvY 很乐意听到您的想法或回答您可能遇到的任何问题！    提交人    /u/Pritish-Mishra   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfti9v/i_trained_an_llm_on_my_whatsapp_chats_to/</guid>
      <pubDate>Fri, 14 Jun 2024 15:26:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] 改进的 Text2SQL 数据集现已在 Huggingface 上可用！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfsel1/p_improved_text2sql_dataset_now_available_on/</link>
      <description><![CDATA[我很高兴与大家分享我们一直在研究的更新的开源资源 — 耶鲁大学最初为 Text2SQL 任务发布的 Spider 数据集的改进版本。您可以在此处查看：https://huggingface.co/datasets/RaffaSch121/fixed_spider 在 Turbular 进行我们自己的模型训练期间，我们发现原始数据集中存在几个问题。为了帮助社区并回馈社区，我们决定解决这些问题并发布更正版本。我们希望这个增强的数据集将使所有从事 Text2SQL 和类似项目的人受益。 如果您找到使其更好的方法，请随意下载、试验和回馈   由    /u/RaeudigerRaffi  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfsel1/p_improved_text2sql_dataset_now_available_on/</guid>
      <pubDate>Fri, 14 Jun 2024 14:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM 使用树状结构数据进行小样本学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfpm9a/d_llm_few_shot_learning_with_treestructured_data/</link>
      <description><![CDATA[假设我想从树数据中获取具体答案。我为上下文学习任务精心挑选了一些示例对。在我看来，有两种可能的方法：  将树转换为任何文本表示形式，在提示中输入（树，答案）对并提出有关新树的问题。 获取示例（树，答案）对的嵌入。但是，查询树可能与示例完全不同。我不知道在这种情况下，使用示例对新问题进行相似性搜索是否有帮助。  您对使用嵌入来解决此任务有何看法？整个流程会是什么样子？有没有更好的方法可以教 LLM 树数据的问题答案映射？    提交人    /u/ratt_m   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfpm9a/d_llm_few_shot_learning_with_treestructured_data/</guid>
      <pubDate>Fri, 14 Jun 2024 12:27:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 讨论苹果在 iPhone 15 Pro 上部署 30 亿参数 AI 模型——他们是如何做到的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/</link>
      <description><![CDATA[大家好， 所以，我一直在本地运行 Phi-3 mini，老实说，它还不错。尽管在模型文件中进行了所有调整和结构化提示，但这是正常的，尤其是考虑到典型 GPU 设置上的滞后响应时间。我最近在检查 Apple 最近的设备模型，他们在 iPhone 15 Pro 上运行了一个近 30 亿参数的 AI 模型！ 这是 AI 在移动设备上实现可能性的进步。他们想出了一些技巧来实现这一点，我只是想和大家讨论一下：  优化的注意力机制：Apple 通过使用分组查询注意力机制显着降低了计算开销。此方法可批量处理查询，从而减少必要的计算。 共享词汇嵌入：老实说，我对此没有太多了解 - 我需要更多地了解它 量化技术：对模型权重采用 2 位和 4 位量化混合，有效降低了内存占用和功耗。 高效的内存管理：动态加载小型、特定于任务的适配器，可以加载到基础模型中以专门化其功能，而无需重新训练核心参数。这些适配器重量轻，仅在需要时使用，在内存使用方面灵活且高效。 高效的键值 (KV) 缓存更新：即使我都不知道它是如何工作的 功耗和延迟分析工具：他们使用 Talaria 等工具实时分析和优化模型的功耗和延迟。这样一来，他们就可以在性能、功耗和速度之间做出权衡，定制比特率选择，以在不同条件下实现最佳运行。：Talaria 演示视频 通过适配器进行模型专业化：无需重新训练整个模型，只需针对不同任务训练特定的适配器层，即可保持高性能，而无需重新训练整个模型的开销。 Apple 的适配器让 AI 可以随时切换档位以执行不同的任务，同时保持轻便和快速。  有关更详细的见解，请查看 Apple 的官方文档：介绍 Apple Foundation Models 讨论要点：  在移动设备上部署如此庞大的模型的可行性如何？ 这些技术对未来的移动应用有何影响？ 这些策略与典型的桌面 GPU 环境（例如我使用 Phi-3 mini 的体验）中使用的策略相比如何？     提交人    /u/BriefAd4761   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfoykx/d_discussing_apples_deployment_of_a_3_billion/</guid>
      <pubDate>Fri, 14 Jun 2024 11:50:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 探索大规模全模态预训练的极限</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfm06d/r_explore_the_limits_of_omnimodal_pretraining_at/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfm06d/r_explore_the_limits_of_omnimodal_pretraining_at/</guid>
      <pubDate>Fri, 14 Jun 2024 08:30:46 GMT</pubDate>
    </item>
    <item>
      <title>[R] Lamini.AI 推出记忆调节功能：法学硕士准确率达 95%，幻觉减少 10 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/</link>
      <description><![CDATA[https://www.lamini.ai/blog/lamini-memory-tuning  Lamini Memory Tuning 是一种将事实嵌入 LLM 的新方法，可提高事实准确性并将幻觉减少到以前无法实现的水平 - 对于一位财富 500 强客户来说，Lamini Memory Tuning 的准确率达到了 95%，而其他方法的准确率仅为 50%。幻觉从 50% 减少到 5%。 Lamini Memory Tuning 是一项研究突破，它克服了 AI 世界中一个看似矛盾的现象：实现精确的事实准确性（即没有幻觉），同时坚持使 LLM 变得有价值的泛化能力。 该方法需要在任何开源 LLM（如 Llama 3 或 Mistral 3）之上使用精确的事实调整数百万个专家适配器（例如 LoRA）。如果目标是准确无误地获取罗马帝国的事实，Lamini Memory Tuning 会创建关于凯撒、渡槽、军团和您提供的任何其他事实的专家。受信息检索的启发，该模型在推理时仅从索引中检索最相关的专家 - 而不是所有模型权重 - 因此延迟和成本显着降低。高精度、高速度、低成本：使用 Lamini Memory Tuning，您无需选择。  研究论文：https://github.com/lamini-ai/Lamini-Memory-Tuning/blob/main/research-paper.pdf    提交人    /u/we_are_mammals   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dffyfs/r_laminiai_introduces_memory_tuning_95_llm/</guid>
      <pubDate>Fri, 14 Jun 2024 02:08:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您觉得 NoPE 怎么样（至少在小型机型上）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dfay95/d_what_do_you_think_of_nope_on_small_models_at/</link>
      <description><![CDATA[      嗨！ 我尝试了 Karpathy 的“让我们重现 GPT-2 (124M)”（和 repo）代码，但使用了 NoPE，只是删除了位置编码。 编辑：NoPE 代表无位置编码，我不知道它以前是否使用过，但我第一次看到它是在论文“位置编码对 Transformers 中长度泛化的影响”中，该论文认为没有必要添加位置编码，网络将学习它们。他们还认为它的表现优于他们。 EDIT2：不幸的是，根据我的经验，我发现 ALiBi 比 NoPE 表现更好，它对所有序列长度的困惑度值都与 NoPE 的最低困惑度相同（在训练序列长度为 1024 时） 从我得到的结果来看，它还不错，但我认为它的泛化能力有点夸大了。 我得到了以下数据： https://preview.redd.it/clq8zwl2ve6d1.png?width=1480&amp;format=png&amp;auto=webp&amp;s=c8c846df1ada58c0bed156e24595478087e77b75 https://preview.redd.it/0hvgd273ve6d1.png?width=1498&amp;format=png&amp;auto=webp&amp;s=f111ef399c34f7c01530064cb3b87d71ac2db8a8 它使用相同的硬件、训练和验证集、训练配置等。不幸的是，我无法与学习到 PE 的模型进行比较（这些实验对我来说成本太高了）。 这些图有很多问题：  颜色不一样两幅图中的图例相同。我无法纠正我的实例已经关闭的情况 :( 序列长度的跳跃太大（总是翻倍）。 在训练过程中大约 15xxx 步发生了一些事情。当我回来时，我发现标准太高（没有记录/绘制），即使在最后一步也是如此，因此很难从中恢复。我不确定稳定阶段的标准，但我猜它下降到了 1 以下。  我缺少最重要的一点，即与学习到的 PE 模型进行公平比较，但我已经在使用 ALiBi 和 FIRE 进行两次实验，这变得太昂贵了。NoPE 在吞吐量方面没有真正的收益（至少在这种情况下），所以我们可以观察到的唯一收益可能是有点概括。 你们觉得怎么样？有人对此有更多经验吗？    提交人   /u/ReinforcedKnowledge   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dfay95/d_what_do_you_think_of_nope_on_small_models_at/</guid>
      <pubDate>Thu, 13 Jun 2024 22:00:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>