<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Fri, 31 May 2024 18:18:34 GMT</lastBuildDate>
    <item>
      <title>[D] Bigram 标记器比现状更好吗？尤其是对于多语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4z5dr/d_bigram_tokenizers_better_than_status_quo/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4z5dr/d_bigram_tokenizers_better_than_status_quo/</guid>
      <pubDate>Fri, 31 May 2024 16:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[D]大二学生不知道该如何继续学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4wxxy/dsecond_year_undergrad_confused_how_to_proceed/</link>
      <description><![CDATA[大家好，我是一名大二学生，从事机器学习已经一年多了。 阅读论文或研究小众统计数据/机器学习不会让我不知所措。然而，我有时对自己的学习过程持怀疑态度。任何提示/建议都将不胜感激。 对于基础知识，我涵盖了： cs 229（标准大学（Andrew Ng）的机器学习简介） Cs 231n（Justin 的计算机视觉深度学习）约翰逊密歇根大学 Stat110（哈佛大学的统计和概率） 此外，阅读热门论文 + 实施它们是一种有效的做法吗？ 我今年的近期目标是找到深度学习的研究实习（我的特定领域是生成模型）。所以请相应地提出建议。 如果我不能正确解释，很抱歉，简而言之，我的问题是我不知道如何继续？我已经准备好做任何事情了。 编辑：如果您从我现在的位置重新开始，您会怎么做？    提交人    /u/BrilliantBrain3334   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4wxxy/dsecond_year_undergrad_confused_how_to_proceed/</guid>
      <pubDate>Fri, 31 May 2024 14:27:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 序列打包对于训练 Transformer 来说常见吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4tux3/d_is_sequence_packing_common_for_training/</link>
      <description><![CDATA[大家好， 我想从头开始训练一个小型 Transformer 语言模型，并试图尽可能提高训练效率。我一直在思考如何构建训练批次，这让我看到了这篇论文高效序列打包，避免交叉污染：加速大型语言模型，不影响性能，这似乎是一件非常合理的事情，一般情况下都应该这样做。 简而言之，这个想法是将多个序列打包在一个样本序列中，并调整注意力矩阵，使样本不会相互交叉污染，即只关注自身内的标记。 Huggingface 论坛中的这篇文章很好地说明了这一点。但我在 Huggingface Transformer 中找不到这样的东西。我是不是漏掉了什么？其他框架是否实现了这一点？我们是否知道大公司是否在进行序列打包？我是否错过了这方面的重大缺点？我认为它可能会对位置编码造成问题。    提交人    /u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4tux3/d_is_sequence_packing_common_for_training/</guid>
      <pubDate>Fri, 31 May 2024 11:58:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用大型语言模型评估 RAG</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4tbzg/p_evaluate_rag_using_large_language_models/</link>
      <description><![CDATA[我一直在研究 RAG 和 LLM，我一直想评估 LLM。有一些库可以与基于 GPT 的模型一起使用，但对于 RAG，我主要想评估基于 Llama 或 Mistral 的模型。 所以我建立了 BeyondLLM。 BeyondLLM 可帮助您仅用 5-7 行代码构建高级检索增强生成 (RAG) 和大型语言模型 (LLM) 应用程序。BeyondLLM 是开源的，它还支持 Fine Tune Embeddings 和 Observaility。 GitHub：https://github.com/aiplanethub/beyondllm/    提交人    /u/trj_flash75   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4tbzg/p_evaluate_rag_using_large_language_models/</guid>
      <pubDate>Fri, 31 May 2024 11:27:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习会议和组织指标</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4shqn/d_ml_conferences_and_organization_metrics/</link>
      <description><![CDATA[我觉得很多人会认为 NeurIPS、ICLR、ICML 等是机器学习领域的重要会议。即使在机器学习之外，NeurIPS 和 ICLR 的 H 指数在所有会议中排名第 9 和第 10。但是，现在我正在查看全球的终身教职职位，情况似乎有所不同。这些出版物似乎对移民或学术终身教职毫无价值，因为它们不是传统期刊。您会注意到它们没有出现在 SCImago 中，SCImago 是许多组织用来衡量出版物质量的指标，因此会决定终身教职或移民。 我很好奇学术机器学习研究人员在这种情况下会做什么。您是否会停止向 NeurIPS 提交论文，而打算在 ACM “机器学习的基础和趋势”上发表论文，而该期刊 在 SCImago 上排名第二？还是在不断增长的机器学习 IEEE 期刊名单上？    提交人    /u/smorad   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4shqn/d_ml_conferences_and_organization_metrics/</guid>
      <pubDate>Fri, 31 May 2024 10:34:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] KAN ==多层GAM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4pjxp/d_kan_multilayer_gam/</link>
      <description><![CDATA[我刚刚阅读了 KAN 论文， 我的理解是，它提供了如何堆叠多层 GAM（广义加性模型）的解决方案：Phi 函数只是 GAM 的形状函数，而样条函数是 GAMS 中经过充分研究的形状函数。 所以对我来说：  MLP 是一个多层线性回归 KAN 是一个多层 GAM  尽管如此，GAM 具有 KAN 论文中未表达的链接功能，但在我看来，这才是这篇论文的真正重点。如果我们向 KAN 层添加激活函数，那么我们就完全拥有了多层 GAM。 这也意味着我们可以将 MLP 视为 KAN 的特例，因为线性回归是 GAM 的特例。 这听起来正确吗？    提交人    /u/mainro12   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4pjxp/d_kan_multilayer_gam/</guid>
      <pubDate>Fri, 31 May 2024 07:02:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 LipNet 进行唇读：端到端的句子级唇读</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4mpkw/r_lipreading_with_lipnet_endtoend_sentencelevel/</link>
      <description><![CDATA[      大家好， 我最近根据论文《端到端句子级唇读》从头开始实现了 LipNet。它通过从输入帧中的唇部运动中提取特征来预测句子。它最初是一个 3DConv-GRU 模型，我已经用 3DConv-LSTM（双向）和一些其他具有不同复杂度的模型实现了它，并且利用了 He（Kaiming Normal）初始化权重。 我请求您查看存储库并提供任何反馈，如果您发现它有用，请考虑分叉。 GitHub/LipNet 图片来自官方论文   由    /u/Kian5658  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4mpkw/r_lipreading_with_lipnet_endtoend_sentencelevel/</guid>
      <pubDate>Fri, 31 May 2024 04:00:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习自省</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4dx88/r_machine_learning_introspection/</link>
      <description><![CDATA[      虽然“自省”虽然在人工智能中没有明确的定义，但它确实有着悠久的历史——主要是让机器具备人类解决问题的直觉，Newell 和 Simon 的“通用问题解决器”（1958 年）就是一个早期的例子。根据 Roger Grosse 的说法，由于他认为人们不愿意从心理状态的角度思考算法，因此该领域已经远离了内省。 然而，本周我们发布了一个视频，其中介绍了这些基本思想，并探讨了过去几年发布的两篇关于内省概念的论文。第一个是“内省式 CNN，它通过从自己的分类器合成样本来改善分类结果（而不是使用单独的鉴别器网络来生成样本的 GAN）。这就是为什么这种方法被称为“内省式”的原因。因为它使用了自己的分类器。 第二篇论文%20graph) 使用概念归纳来创建一组概念，以描述系统为何为用户做出分类决策。这里的评估侧重于人类对解释的理解，而不是用它来使分类结果更好。这实际上与我们与 Joao Leite 教授一起发布的早期视频有关，他在视频中讨论了使用“映射神经网络”得出的结论进行概念的确定（您也可以参阅他的论文）。 最新视频的网址如下： https://www.youtube.com/watch?v=drlqCc_e_o0 https://www.youtube.com/watch?v=26tTT8saaDs&amp;t=2517s    由   提交  /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4dx88/r_machine_learning_introspection/</guid>
      <pubDate>Thu, 30 May 2024 20:40:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] CV/结构光/3D 重建研究合作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4dokf/r_research_collaboration_in_cv_structured_light/</link>
      <description><![CDATA[我正在寻找对结构光、使用投影技术的 3D 重建或与条纹投影或相位分析相关的任何内容的研究和出版感兴趣的合作者。我的重点包括：  用于 3D 扫描的结构光 创新投影方法 使用最新技术克服相位分析或相位展开中的挑战 在医学成像、工业检测等领域的应用。  如果您在这些领域工作或有见解可以分享，我很乐意讨论潜在的合作机会。如果我的帖子到处都是，我很抱歉。快速聊天以交换想法或提供建议将不胜感激。    提交人    /u/Falafel2307   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4dokf/r_research_collaboration_in_cv_structured_light/</guid>
      <pubDate>Thu, 30 May 2024 20:30:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Aruco 标记估算距离（以米为单位）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4bhk5/d_distance_estimation_in_meters_using_aruco/</link>
      <description><![CDATA[您好， 我在船舱中安装了 2 个摄像头，我想找到船舱中每个人之间的距离（以米为单位），以保持社交距离。我打算使用 Aruco 标记，但我不知道该怎么做。 我的计划 -  使用 Aruco 标记校准两个摄像头以获取内在参数。 将 aruco 标记放置在两个摄像头都可以看到它们的位置，以获取外部参数。 基于 aruco 生成的唯一 ID，如果两个摄像头检测到其中任何一个 aruco 标记，则执行三角测量。  我有一个计划，但我不确定它是否可行。我不知道上述方法如何帮助我找到人与人之间的深度或距离（以米为单位）。请指教。 请注意，我刚刚开始接触计算机视觉。    提交人    /u/Embarrassed_Top_5901   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4bhk5/d_distance_estimation_in_meters_using_aruco/</guid>
      <pubDate>Thu, 30 May 2024 18:56:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我进行了 580 次模型数据集实验，结果表明，即使你非常努力，仅通过查看数据漂移结果也几乎不可能知道模型是否正在退化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d47ca4/r_i_ran_580_modeldataset_experiments_to_show_that/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d47ca4/r_i_ran_580_modeldataset_experiments_to_show_that/</guid>
      <pubDate>Thu, 30 May 2024 15:54:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何准备我的第一次研究实习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3zwqy/r_how_to_prepare_for_my_first_research_internship/</link>
      <description><![CDATA[当然，这是您的 Reddit 帖子正文的草稿！..... 无论如何，只是开个玩笑。 我很快就要开始为期 3 个月的 NLP 研究实习，并希望获得一些关于如何充分利用这个机会的建议。这是我第一次做研究，所以我有点紧张，但也很兴奋。 关于实习的一些细节： 持续时间：实习将持续 3 个月。 重点领域：我将研究大型语言模型 (LLM)，特别是检索增强生成 (RAG) 和知识图谱。 鉴于这种背景，我正在寻找任何关于如何有效地为研究项目做出贡献的提示或建议，尤其是考虑到我的经验有限。此外，您是否有任何关于如何充分利用研究实习的一般建议，尤其是对于初次实习的人？我还非常感谢任何可以提高我作为研究人员的工作效率的提示/工具。    提交人    /u/SingularityCharity   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3zwqy/r_how_to_prepare_for_my_first_research_internship/</guid>
      <pubDate>Thu, 30 May 2024 09:25:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 了解 ML 的最新进展后，您会对 BERT 做哪些添加或更改？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3zw0d/d_what_additions_or_changes_would_you_make_to/</link>
      <description><![CDATA[嗨！ 由于 BERT 仍然是一个广泛使用的模型。我很好奇你们会做些什么来使它保持最新状态。BERT 论文于 2018 年 10 月 11 日提交，最后修订于 2019 年 5 月 24 日在 arXiv 上。 这些想法不一定非要涉及架构，它可以是调度程序或训练集、MLM 损失、加快训练速度等。 就我个人而言，我会改变位置编码，也许我会使用 RoPE。使用 Flash Attention。 对于数据集，也许我会专注于混合，我对调度程序不太了解，但我会尝试所有 LLM 论文中的一些东西，一些能够进行持续预训练的东西。    提交人    /u/Mean-Night6324   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3zw0d/d_what_additions_or_changes_would_you_make_to/</guid>
      <pubDate>Thu, 30 May 2024 09:24:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您最喜欢的深度学习论文是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3lbep/d_whats_your_alltime_favorite_deep_learning_paper/</link>
      <description><![CDATA[我正在寻找有趣的深度学习论文，尤其是关于计算机视觉任务中的架构改进。    提交人    /u/research_pie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3lbep/d_whats_your_alltime_favorite_deep_learning_paper/</guid>
      <pubDate>Wed, 29 May 2024 20:00:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>