<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Tue, 05 Nov 2024 09:17:19 GMT</lastBuildDate>
    <item>
      <title>[D] 二流论文在申请行业研究工作时有价值吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjz3in/d_do_second_tier_papers_have_any_value_when_apply/</link>
      <description><![CDATA[我想我之前遇到过一些行业工作，要求申请人拥有顶级论文（NIPS/ICML/ICLR/CVPR/ICCV/ECCV），所以我的问题是，来自不太有声望（AAAI/IJCAI/WACV/BMVC.... 或期刊）会议的论文在申请这些工作时有任何价值吗？此外，h 指数或引用等指标重要吗？    提交人    /u/Competitive_Newt_100   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjz3in/d_do_second_tier_papers_have_any_value_when_apply/</guid>
      <pubDate>Tue, 05 Nov 2024 04:58:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大型 ML 管道敏感性分析的最佳资源？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjy1xu/d_best_resources_for_sensitivity_analysis_on/</link>
      <description><![CDATA[我所在的团队正在启动一个大型项目，以检查 ML 管道如何响应数据变化。  这是我最近第一次进行如此大规模和复杂的敏感性分析，因此我正在寻找帮助以确定以下方面的最新资源：   模拟数据，尤其是任何 Python 工具，以及它们与 R 提供的最佳工具的比较 评估工具 弹性 总体而言，关于敏感性分析的任何最佳资源，尤其是近几年的新资源  您找到的最佳资源是什么？    提交人    /u/oldmaninnyc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjy1xu/d_best_resources_for_sensitivity_analysis_on/</guid>
      <pubDate>Tue, 05 Nov 2024 03:58:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于准备 Google ML 面试的建议 – 应关注的关键领域？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjxyg0/d_advice_on_preparing_for_google_ml_interview_key/</link>
      <description><![CDATA[我正在准备参加 Google 的机器学习面试，招聘人员分享了他们将重点关注的主要领域： - 理论 ML 概念和实际应用 - 包括问题定义、模型选择、模型调整和评估。 - 行业规模 ML - 涵盖性能和成本优化、数据处理以及面向生产的实验和调试。 如果有人对这些领域的期望或关注点有任何见解，我将不胜感激！我特别难以理解“行业规模 ML”问题实际上可能是什么。 提前感谢任何建议或资源！    提交人    /u/atomicalexx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjxyg0/d_advice_on_preparing_for_google_ml_interview_key/</guid>
      <pubDate>Tue, 05 Nov 2024 03:53:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] Haiku 3.5 发布！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjxos7/d_haiku_35_released/</link>
      <description><![CDATA[      最后， Anthropic 最近宣布升级其 Claude 3.5 Sonnet 型号，并推出新款型号 Claude 3.5 Haiku。 Claude 3.5 Haiku 速度快、价格实惠，在智能基准测试中超越了之前的型号，对于编码、子代理任务和面向用户的产品尤其高效。 https://www.anthropic.com/news/3-5-models-and-computer-use 基准测试结果    提交人    /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjxos7/d_haiku_35_released/</guid>
      <pubDate>Tue, 05 Nov 2024 03:38:02 GMT</pubDate>
    </item>
    <item>
      <title>帮助集思广益的 Web 应用程序：[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjxjnf/web_app_to_help_brainstorm_ideas_p/</link>
      <description><![CDATA[      大家好，我是一名独立开发者，这是我第一次尝试构建产品。 thebrainstormapp.com 帮助用户集思广益，提出产品创意，并将它们保存在一个地方。它会进行所有研究，找到与您的想法类似的产品、优势和挑战。 https://preview.redd.it/l5r6620s50zd1.png?width=2754&amp;format=png&amp;auto=webp&amp;s=1da9c16d6eb31b0ca270bbfe48cbebe9a291d8e6    提交人    /u/SpecificNecessary615   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjxjnf/web_app_to_help_brainstorm_ideas_p/</guid>
      <pubDate>Tue, 05 Nov 2024 03:30:18 GMT</pubDate>
    </item>
    <item>
      <title>为您当地的 LLMS 输入视频 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjudjw/video_input_for_your_local_llms_p/</link>
      <description><![CDATA[我的项目的作用 OpenSceneSense-Ollama 是一个功能强大的 Python 包，旨在直接在本地机器上进行以隐私为中心的视频分析。借助此工具，您可以利用 Ollama 的本地模型来分析帧、转录音频、动态选择关键帧并生成详细摘要 - 所有这些都无需依赖基于云的 API。它非常适合那些需要对视频内容进行丰富、有见地的分析，同时确保数据隐私并最大限度地降低使用成本的人。 目标受众 该项目专为需要深入的本地处理视频分析的开发人员、研究人员、数据科学家和注重隐私的用户量身定制。它非常适合数据安全至关重要的应用，包括： - 需要自动视频摘要的内容创建工作流程 - 为机器学习构建标记数据集的研究人员 - 需要上下文丰富的内容审核的平台 - 远程或受限环境中的离线项目 比较 OpenSceneSense-Ollama 超越了通常将帧和音频分析分开的传统视频分析工具。相反，它集成了视觉和音频元素，允许用户提示模型生成全面的摘要和深入的上下文洞察。大多数工具可能会单独识别对象或转录音频，而 OpenSceneSense-Ollama 将这些组件统一为叙述摘要，使其成为更丰富的数据集或更细致入微的内容审核的理想选择。 入门 要开始使用 OpenSceneSense-Ollama：  先决条件：确保您的计算机上安装了 Python 3.10+、FFmpeg、PyTorch 和 Ollama。 使用 pip 安装：运行“pip install openscenesense-ollama”以安装软件包。 配置：开始使用可自定义的提示、帧选择和音频转录分析视频。  欢迎随意深入研究、尝试并分享您的反馈，特别是如果您从事 AI、以隐私为中心的应用程序或视频内容审核工作。让我们构建一个强大的本地解决方案，以进行有意义的视频分析！ https://github.com/ymrohit/openscenesense-ollama    提交人    /u/rohit3627   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjudjw/video_input_for_your_local_llms_p/</guid>
      <pubDate>Tue, 05 Nov 2024 00:50:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] NN 创造最佳伪装</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjslcz/p_nn_for_creating_best_camouflage/</link>
      <description><![CDATA[      我有这个想法已经有一段时间了，我已经创建了所有用于创建数据的函数以及所有的架构。问题是我只有两年的深度学习经验，而且这是 GAN 风格的网络，众所周知，GAN 很难训练。我希望你们能就这个想法提出意见，以及一些技巧、建议、忠告和需要改进的地方。如果有人觉得这很有趣，我很乐意与其他人合作完成这个项目。 迷彩图案生成模型 目标是创建一个模型，通过训练生成器模型并使用分割模型作为判别器来评估生成的迷彩的有效性，从而生成最佳迷彩颜色图案。生成器和鉴别器同时进行训练。 模型结构 前向过程  生成器： 生成器是一个简单的解码器模型，它采用大小为 n_embed = 128 的随机潜在向量并输出 3x32x32 的迷彩颜色图案。 然后将生成的迷彩图案平铺以形成更大的纹理，与士兵图像的大小相匹配。  创建伪装士兵： 对士兵的随机黑白 PNG 图像进行采样并将其大小调整为 (1, W, H)，并将值反转，以便士兵显示为白色（前景）而背景为黑色。 然后将平铺的迷彩图案通过用士兵图像进行掩盖应用于士兵，产生伪装的士兵形象。整个操作都是分批的，并允许梯度流动。  将伪装的士兵放置在背景上： 伪装的士兵被随机放置在背景图像上（例如，森林场景）。 同时生成分割模型的标签掩码，其中包含两个类：背景和士兵。  鉴别器（分割模型）： 使用预先训练的分割模型（充当鉴别器），具有两个输出类（背景和士兵）。 该模型通过尝试将士兵归类为背景来评估伪装图案将士兵融入背景的程度。   损失函数和优化 使用了两个损失函数，每个函数都有单独的反向传播过程：  生成器损失： 这鼓励生成器创建一个伪装图案，使士兵与背景难以区分。 损失函数：CrossEntropyLoss（输出，0）其中输出是来自鉴别器的预测分割图，0 代表背景类。  鉴别器（分割模型）损失： 这鼓励分割模型正确识别背景中的伪装士兵。 损失函数：CrossEntropyLoss（输出，label_mask）其中标签掩码有两个类：背景和士兵。   关键注意事项 此设置类似于生成对抗网络 (GAN)，但不同之处在于它不使用“真实”伪装数据，而只使用生成的样本。此外：  单独的优化器：建议对生成器和鉴别器使用不同的优化器。 损失缩放：可能需要仔细调整缩放因子或学习率以稳定训练。 两步反向传播：与典型的 GAN 式损失不同，使用两步反向传播方法来独立更新模型。  https://preview.redd.it/qd2cr2rkyyyd1.png?width=5603&amp;format=png&amp;auto=webp&amp;s=0faee2cb0504a98c36b365b2edbc59253509d8c7    提交人    /u/MemoryCompetitive691   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjslcz/p_nn_for_creating_best_camouflage/</guid>
      <pubDate>Mon, 04 Nov 2024 23:28:17 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型（LLM）实际上可以很好地解决哪些问题？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/</link>
      <description><![CDATA[虽然人们对人工智能炒作周期的怀疑越来越多，尤其是围绕聊天机器人和 RAG 系统，但我感兴趣的是确定 LLM 在准确性、成本或效率方面明显优于传统方法的具体问题。我能想到的问题是： - 单词分类 - 非大段文本的情感分析 - 图像识别（在某种程度上） - 写作风格转换（在某种程度上） 还有什么？    提交人    /u/Educational-String94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjoxpi/what_problems_do_large_language_models_llms/</guid>
      <pubDate>Mon, 04 Nov 2024 20:52:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在预训练语言模型中添加交叉注意力机制的资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjhsuj/d_resources_for_adding_cross_attention_to_a/</link>
      <description><![CDATA[我想训练新的交叉注意力层，将其输入到预先训练的变压器（可能是小型骆驼模型）中，同时保持模型的其余部分不变。 哪些资源可能会有用？    提交人    /u/BinaryOperation   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjhsuj/d_resources_for_adding_cross_attention_to_a/</guid>
      <pubDate>Mon, 04 Nov 2024 16:03:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 所有 LLM 模型中量化是否都有限？例如，您可以采用 meta-llama/Llama-3.2-1B 等标准模型，并以一半的速度运行，但也有专门为 4 位量化制作的模型（即 meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/</link>
      <description><![CDATA[我只是想了解所有模型中量化的设置方式。 meta-llama/Llama-3.2-1B 等标准模型可以在没有量化（bfloat16 或 float32？）的情况下运行，或者可以使用推理应用程序（如 vLLM）指示它们以一半（float16？）运行。那么这是否意味着所有模型中都内置了一些量化？我是否可以改为使用 int8，而不是指示它以一半量化运行？或者这仅在模型为它构建时才有效？ 然后是专门为 int4 构建的模型（即 meta-llama/Llama-3.2-1B-Instruct-SpinQuant_INT4_EO8）。这是否意味着当您使用 vLLM 运行此模型时，您必须明确说明您正在 int4 上运行它，或者您只是将其保留为默认值，它将自动在 int4 上运行？可以将其覆盖为 int8 吗？还是只是为 int4 硬编码？ 过去 2 天我一直在试图理解这一点。    提交人    /u/xil35   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjhjza/d_is_there_limited_quantization_in_all_llm_models/</guid>
      <pubDate>Mon, 04 Nov 2024 15:54:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] COLING25 行业轨道：录取通知</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gjhbox/d_coling25_industry_track_notification_of/</link>
      <description><![CDATA[“接受通知”的日期是地球上任何地方的 2024 年 11 月 3 日 12:00。我们还没有收到主席的回复，门户网站上也没有通知，有延迟吗？还是只有被接受的论文才会收到通知？请分享有关此的任何信息/更新，谢谢。    提交人    /u/BlackEyesBrownSavant   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gjhbox/d_coling25_industry_track_notification_of/</guid>
      <pubDate>Mon, 04 Nov 2024 15:44:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有/无 SMOTE 的逻辑回归比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gizg2u/d_comparison_of_logistic_regression_withwithout/</link>
      <description><![CDATA[      这让我在工作中抓狂。我一直在评估一个逻辑预测模型。该模型实施了 SMOTE 来将数据集平衡为 1:1 的比例（最初为期望结果的 7%）。我认为这是不必要的，因为改变决策阈值就足够了，并且可以避免不必要的数据插补。数据集中有超过 9,000 次期望事件的发生 - 这对于 MLE 估计来说已经足够了。我的同事不同意。  我在 R 中构建了一个闪亮的应用程序来比较两个模型的混淆矩阵以及一些指标。我欢迎社区对此比较提出一些意见。对我来说，非 smote 模型的表现同样出色，如果查看 Brier 分数或校准截距，甚至更好。你们觉得呢？    提交人    /u/Janky222   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gizg2u/d_comparison_of_logistic_regression_withwithout/</guid>
      <pubDate>Sun, 03 Nov 2024 22:42:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Nov 2024 16:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 2024 年，你的神经网络训练秘诀是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/</link>
      <description><![CDATA[您可能已经知道 Karpathy 2019 中的神经网络训练秘诀圣经 虽然大多数建议仍然有效，但深度学习模型/方法的格局自那时起发生了很大变化。Karpathy 的建议在监督学习环境中效果很好，他确实提到了这一点：  坚持监督学习。不要对无监督预训练过度兴奋。与 2008 年那篇博客文章所说的不同，据我所知，没有任何版本在现代计算机视觉方面报告了强劲的结果（尽管如今 NLP 在 BERT 和朋友的帮助下似乎表现不错，这很可能是由于文本的更刻意性质和更高的信噪比）。  我最近一直在训练一些图像扩散模型，我发现在无监督环境下做出数据驱动的决策更加困难。指标不太可靠，有时我会训练损失更好的模型，但当我查看样本时，它们看起来更糟 你知道 2024 年训练神经网络的更多现代方法吗？（不仅仅是 LLM）    提交人    /u/Even_Information4853   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/</guid>
      <pubDate>Sun, 03 Nov 2024 15:05:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>