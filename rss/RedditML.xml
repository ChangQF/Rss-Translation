<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Wed, 24 Jan 2024 18:16:32 GMT</lastBuildDate>
    <item>
      <title>[D] 使用流程工程与法学硕士生成代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19emvxu/d_code_generation_with_llms_using_flow_engineering/</link>
      <description><![CDATA[我昨天看到了这篇论文，是由 Karpathy 的转发引起了我的注意。 这篇论文提出了 AlphaCodium，一种面向代码的迭代改进LLM代码生成的流程。 除了在复杂的代码生成数据集上实现SoTA之外，我认为这项工作中的想法和提出的方法很重要。原因如下： 许多提示技术针对自然语言任务进行了优化，但对于代码生成可能不是最佳的。 AlphaCodium 探索超越传统提示（即提示 -&gt; 答案），将问题分解为不同的组成部分（自我反思、推理和迭代代码解决方案生成），并包括有趣的技巧，例如人工智能生成的测试、自我反思，并沿着“流程”进行推理。 让我们在下面深入探讨： AlphaCodium 流程涉及两个关键要素，以提高法学硕士中的代码生成能力： - 额外生成的数据（问题自我反思和测试推理）来辅助迭代过程 - 使用额外的人工智能生成的测试来丰富公共测试 如图所示，这里是关键生成代码解决方案涉及的步骤： - 通过自我反思问题和推理公共测试来提高问题理解 - 生成并排序可能的解决方案（以自然语言描述）并选择“最佳解决方案” ” - 生成额外的人工智能测试，其中包含公共测试中未涵盖的方面/案例 - 通过在选定的公共和人工智能生成的测试上运行来生成初始代码解决方案 - 使用以前的基本代码并单独执行运行 -使用公共测试和人工智能生成的测试来修复迭代 这种方法持续提高了 LLM 在 CodeContests 问题上的性能，CodeContests 是一个代码生成数据集，用于评估模型的强大代码生成能力。 使用 CodeContests 的验证数据集，GPT-4 pass@5 准确率从使用单个精心设计的提示的 19% 提高到使用 AlphaCodium 流程的 44%。它甚至优于 AlphaCode，使用的计算预算要小得多，LLM 调用要少 4 个数量级。 这种方法的好处之一是，您只需要一个已经支持编码任务的预训练模型，无需微调是必要的。 所提出的“流程工程”方法的一个巧妙方面是引入自我反思，这有助于在初始阶段增强对问题的理解。 我还发现他们如何通过利用测试锚点列表来提高人工智能生成的测试阶段的可靠性，这一点很有见地。 AlphaCodium 流程的每个阶段都感觉可以进一步改进，因此我怀疑通过更优化的提示技术，您可以更进一步。 总的来说，这是一篇很棒的论文，我什至为它创建了带注释的视觉效果它。让我知道这些视觉效果是否有用。 链接到网站：https://www.codium.ai/blog/alphacodium-state-of-the-art-code- Generation-for-code-contests/ &lt; /div&gt;  由   提交 /u/EnaGrimm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19emvxu/d_code_generation_with_llms_using_flow_engineering/</guid>
      <pubDate>Wed, 24 Jan 2024 18:09:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 公平地说，许多机器学习研究人员认为他们可以创造出可以完成医生（非程序性）所做的大部分工作的产品等吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19em5m9/d_is_it_fair_to_say_lot_of_ml_researchers_think/</link>
      <description><![CDATA[这是我与许多机器学习研究人员交谈后得到的感觉。大家觉得我说的对吗？一位机器学习研究人员表示，当他们在医学论文中撰写人工智能论文时，与医生合作总是很困难，因为他们不喜欢在这方面所做的工作。他们总是把一些东西放在最后，说明这不会取代医生以及他们所做的事情（即使研究目标是这样做），但他们把它放在最后，这样医生就不会生气。   由   提交/u/derpgod123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19em5m9/d_is_it_fair_to_say_lot_of_ml_researchers_think/</guid>
      <pubDate>Wed, 24 Jan 2024 17:13:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 想在边缘设备上学习 ML/DL</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19el2rk/d_want_to_learn_mldl_on_edge_devices/</link>
      <description><![CDATA[大家好，我想了解如何在无人机/无人机上运行 ML/DL 模型。我有一些在 Jetson nano 和 xavier 上运行 trt 模型的经验。推荐一些学习资源。还向我推荐我能买到的最便宜的无人机，它可以运行小物体检测/分割模型。   由   提交 /u/BABA_yaaGa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19el2rk/d_want_to_learn_mldl_on_edge_devices/</guid>
      <pubDate>Wed, 24 Jan 2024 16:28:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用语义图和 RAG 生成知识</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eksog/p_generate_knowledge_with_semantic_graphs_and_rag/</link>
      <description><![CDATA[      ​ https://preview.redd.it/nwnwzb9uxeec1.png?width=1182&amp; format=png&amp;auto=webp&amp;s=ed5005c7a2c0932d689410813e2925d8959dfb86 语义图使用向量模型来构建图网络，该图网络具有为数据集生成的语义相似的关系。虽然它可以单独使用，但它也可以与 RAG 完美搭配。这与 RAG 的标准向量查询不同，因为该方法不仅考虑搜索和结果之间的语义相似性，还考虑结果内的相似性。它可以有效地判断结果，看看哪个结果与其他结果最相似。 页面排名和中心性等图分析算法可用于形成 RAG 的 LLM 上下文。当没有明确的问题要问时，此方法最适合知识发现和主题探索。有时，搜索最困难的事情是知道要搜索什么，这就是这里的主要目的。  文章 | GitHub    由   提交/u/davidmezzetti   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eksog/p_generate_knowledge_with_semantic_graphs_and_rag/</guid>
      <pubDate>Wed, 24 Jan 2024 16:16:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 深度任何事物：释放大规模未标记数据的力量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ejslf/r_depth_anything_unleashing_the_power_of/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.10891 代码：https:/ /github.com/LiheYoung/Depth-Anything 模型： https://huggingface.co/spaces/LiheYoung/Depth-Anything/tree/main https://huggingface.co/LiheYoung 项目页面：https://depth-anything.github.io/ 演示：https://huggingface.co/spaces/LiheYoung/Depth-Anything 摘要：  这项工作提出了Depth Anything，这是一种用于鲁棒单目深度估计的高度实用的解决方案。在不追求新颖的技术模块的情况下，我们的目标是构建一个简单而强大的基础模型，处理任何情况下的任何图像。为此，我们通过设计数据引擎来收集并自动注释大规模未标记数据（~62M）来扩展数据集，这显着扩大了数据覆盖范围，从而能够减少泛化误差。我们研究了两种简单而有效的策略，使数据扩展前景光明。首先，利用数据增强工具创建更具挑战性的优化目标。它迫使模型主动寻求额外的视觉知识并获得稳健的表示。其次，开发了辅助监督来强制模型从预训练的编码器继承丰富的语义先验。我们广泛评估其零镜头能力，包括六个公共数据集和随机捕获的照片。它表现出了令人印象深刻的泛化能力。此外，通过使用 NYUv2 和 KITTI 的度量深度信息对其进行微调，设置了新的 SOTA。我们更好的深度模型也会产生更好的深度调节 ControlNet。我们的模型发布在此 https URL。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ejslf/r_depth_anything_unleashing_the_power_of/</guid>
      <pubDate>Wed, 24 Jan 2024 15:33:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] DDIM 反转 - 真实图像的反转潜伏期如何“​​高斯”？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ej9jz/d_ddim_inversion_how_gaussian_are_the_inverted/</link>
      <description><![CDATA[我遇到过几篇论文，它们使用确定性反演来查找潜伏，该潜伏（连同提示）可以使用稳定扩散再现真实图像。在“提示到提示”中，赫兹等人。请注意以下几点：  但是，在许多其他情况下，反演不够准确，如图 1 所示。 11. 这部分是由于失真可编辑性权衡 [43]，我们认识到减少无分类器指导 [18] 参数（即减少即时影响）可以改善重建，但限制了我们执行重大任务的能力  我在其他论文中看到过类似的说法，这是由于反转潜伏不属于生成模型通常从中采样的标准高斯空间它的初始噪声潜伏。我想知道是否有人知道对此进行深入研究的任何著作？量化反向潜伏偏离预期高斯分布的最佳方法是什么？是否有某些图像在 SD 的学习分布下不太可能出现，并且反转它们会导致潜在的高斯分布更小？预先感谢您的任何建议和指示！ ​   由   提交 /u/35mmpy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ej9jz/d_ddim_inversion_how_gaussian_are_the_inverted/</guid>
      <pubDate>Wed, 24 Jan 2024 15:10:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] Finetune 提速 TinyLlama 387%、DPO 提速 188%、LLM 推理提速 2 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eiwe8/p_finetune_387_faster_tinyllama_188_faster_dpo_2x/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eiwe8/p_finetune_387_faster_tinyllama_188_faster_dpo_2x/</guid>
      <pubDate>Wed, 24 Jan 2024 14:54:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] Lumiere：用于视频生成的时空扩散模型（Bar-Tal 等人，2024）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eicco/r_lumiere_a_spacetime_diffusion_model_for_video/</link>
      <description><![CDATA[Arxiv: https:// arxiv.org/abs/2401.12945 摘要： “我们介绍 Lumiere——一种设计的文本到视频扩散模型用于合成描绘真实、多样化和连贯运动的视频——这是视频合成中的一个关键挑战。为此，我们引入了时空 U-Net 架构，该架构通过模型中的单次传递一次性生成视频的整个时间持续时间。这与现有的视频模型形成鲜明对比，现有的视频模型合成遥远的关键帧，然后进行时间超分辨率——这种方法本质上使全局时间一致性难以实现。通过部署空间和（重要的）时间下采样和上采样，并利用预先训练的文本到图像扩散模型，我们的模型学习通过在多个时空尺度。我们展示了最先进的文本到视频生成结果，并表明我们的设计可以轻松促进各种内容创建任务和视频编辑应用程序，包括图像到视频、视频修复和风格化生成。 ” Youtube 视频： https://www.youtube.com /watch?v=wxLr02Dz2Sc 非交互式网络演示： https ://lumiere-video.github.io/   由   提交 /u/StartledWatermelon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eicco/r_lumiere_a_spacetime_diffusion_model_for_video/</guid>
      <pubDate>Wed, 24 Jan 2024 14:28:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 视觉模型的方便比较图表：何时使用什么！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eeve9/d_a_handy_comparative_chart_on_vision_models_when/</link>
      <description><![CDATA[       由   提交/u/Instantinopaul   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eeve9/d_a_handy_comparative_chart_on_vision_models_when/</guid>
      <pubDate>Wed, 24 Jan 2024 11:21:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 幻视曼巴再次出击！变形金刚王座正在崩溃吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</link>
      <description><![CDATA[还记得震撼 NLP 的状态空间模型 Mamba 吗？好吧，抓住你的像素，因为它们现在也在计算机视觉领域碾压它！ 他们的新模型 Vision Mamba 抛弃了自我关注热潮，并依赖于状态空间魔法。结果？性能与顶级视觉变压器 (DeiT) 相当，但效率更高！ 这可能会改变游戏规则，伙计们。我们正在谈论更快、更轻的型号，它们可以在您祖母的笔记本电脑上运行，但仍然像鹰一样看得见。 有什么想法吗？我很高兴看到变形金刚领域出现一些竞争。我们可以期待在这个新架构上推出 chatgpt v2 吗？道歉！可能听起来很疯狂，而且评论还为时过早。 查看论文：https: //paperswithcode.com/paper/vision-mamba-efficient-visual-representation   由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19eemq2/d_vision_mamba_strikes_again_is_the_transformer/</guid>
      <pubDate>Wed, 24 Jan 2024 11:06:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] InternLM-Math：SOTA 开源数学推理法学硕士。求解器、证明者、验证者、增强器。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19ee2ku/p_internlmmath_sota_opensourced_math_reasoning/</link>
      <description><![CDATA[   上海人工智能实验室推出新的SOTA数学法学硕士，具有7B和20B规模的开放式来源。 Github：https://github.com/InternLM/InternLM-Math Huggingface：https://huggingface.co/internlm/internlm2-math-7b 演示：https://huggingface.co/spaces/internlm/internlm2-math -7b ​ https://preview.redd.it/4emyeapn7dec1.png?width=1224&amp;format=png&amp;auto=webp&amp;s=6a79ba3e4b98f48befed91eded1cf286b9fca137 特点：  7B 和 20B 中文和英语数学 LM 的性能优于 ChatGPT。 InternLM2-Math 继续从 InternLM2-Base 进行预训练，具有约 100B 高质量数学-相关代币和 SFT 以及约 200 万双语数学监督数据。我们应用最小哈希和精确数字匹配来消除可能的测试集泄漏。 添加 Lean 作为数学问题解决和数学定理证明的支持语言。我们正在探索将 Lean 3 与InternLM-Math 用于可验证的数学推理。 InternLM-Math 可以为 GSM8K 等简单的数学推理任务生成 Lean 代码，或者提供基于 Lean 状态的可能证明策略。 也可以视为奖励模型，支持结果/过程/精益奖励模型。我们用各种类型的奖励建模数据来监督InternLM2-Math，使InternLM2-Math也可以验证思维链过程。我们还添加了将思维链过程转换为 Lean 3 代码的功能。 数学 LM 增强助手和代码解释器。 InternLM2-Math 可以帮助增强数学推理问题并使用代码解释器解决它们，这使您可以更快地生成综合数据！  性能： https://preview.redd.it/ttzsd4408dec1.png?width=1175&amp;format =png&amp;auto=webp&amp;s=8894552a848130a8240a2e135a6b78d0841311d4   由   提交/u/OpenMMLab  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19ee2ku/p_internlmmath_sota_opensourced_math_reasoning/</guid>
      <pubDate>Wed, 24 Jan 2024 10:29:53 GMT</pubDate>
    </item>
    <item>
      <title>[项目] BELT（较长文本的 BERT）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19edzov/project_belt_bert_for_longer_texts/</link>
      <description><![CDATA[我们创建了 BELT（BERT For Longer Texts）——一个 Python 包，允许对长度超过 512 个 token 的文本使用类似 BERT 的模型。该方法是 Jacob Devlin 提出的想法的实现，Jacob Devlin 是 评论。您可以在 Medium 上我刚刚发表的两篇文章中阅读有关它的更多详细信息： 第一部分是应用 BERT 分类器的概述： 第 1 部分 第二部分深入介绍我们训练 BELT 模型的方法。 第 2 部分 该存储库已开源： Repo 我知道你在想什么：“等等，bucko，这不是什么新鲜事。每个人都知道有像 BigBird 或 Longformer 这样的模型可以处理更长的文本”。对此我的回答是：“我知道，伙计，但是 BigBird 和 Longformer 不是修改过的 BERT。它们是具有不同架构的模型。因此，它们需要从头开始预训练或下载。 BELT修改模型微调。这带来了 BELT 方法的主要优点 - 它使用任何预先训练的 BERT 或 RoBERTa 模型。快速查看 HuggingFace Hub 可以确认，BERT 的资源比 Longformer 多大约 100 倍。找到适合特定任务或语言的可能会更容易。”享受吧！   由   提交/u/MBrzozowskiML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19edzov/project_belt_bert_for_longer_texts/</guid>
      <pubDate>Wed, 24 Jan 2024 10:24:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么时候在 TPU 上训练有意义？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</link>
      <description><![CDATA[我花了几周时间将 torch 模型训练脚本移植到 PyTorch/XLA 并在 TPU v3 和 v4 上进行测试。从纯粹的训练速度和成本效率的角度来看，我将结果与 GCP 中的 a2/g2 机器上的训练进行了比较。我很惊讶移植代码有多困难，以及 TPU 上的训练有多慢且成本低效。 Dev UX 让人想起使用 TensorFlow（从最坏的意义上来说）。东西通常不能开箱即用，很难调试，因为所有东西都是编译的，而且张量是惰性的。整个事情非常不透明，不清楚发生了什么。没有您期望拥有的基本工具，例如如果不进行分析就无法检查 TPU 利用率。 更令人惊讶的是，训练速度比使用同等价格的 GPU 时慢得多。例如，与 g2-standard-96（8xL4 GPU）上的训练相比，TPU v3-8 上的训练速度大约慢 2 倍，而成本却大致相同。 TPU v4-8 价格更高，但仍然比 g2-standard-96 慢。我的模型或多或少是一个简单的密集网络，它来自推荐领域。未移植的 pytorch 代码使用 DDP。数据加载器经过高度优化并具有基准测试，我确信这不是瓶颈。 XLA 指标没有显示任何危险信号。 此时，我想知道为此投入更多精力是否有意义。非 Google 人员是否真的使用 TPU 进行大规模训练？是不是 Torch/XLA 还没有准备好迎接黄金时段，只是 TPU 最适合与 TF 或 JAX 一起使用？ TPU 是否有特定的用例？   由   提交 /u/Puzzleheaded-Stand79    reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19e8d1a/d_when_does_it_make_sense_to_train_on_tpu/</guid>
      <pubDate>Wed, 24 Jan 2024 04:15:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究人员使用哪些工具在论文中创建出色的图像和流程图？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</link>
      <description><![CDATA[实际上，我想知道优秀的研究论文中的模型架构图有多酷，其中包含清晰的流程流程图和模型架构的出色可视化。目前我使用draw.io，但很好奇使用什么工具？我的意思是他们使用 Figma、Adobe 等专业工具吗？   由   提交 /u/MysticShadow427   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/19dux08/r_what_tools_do_researchers_use_to_create_great/</guid>
      <pubDate>Tue, 23 Jan 2024 18:17:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>