<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Tue, 30 Jul 2024 15:15:55 GMT</lastBuildDate>
    <item>
      <title>[D] LLM 书籍推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efvtbg/d_llm_book_recommendations/</link>
      <description><![CDATA[您好， TL;DR：我正在寻找有关 LLM 的书籍建议。 我了解 ML 的基础知识，并且已经实践了好几年。除了炒作和最新流行的模式之外，我还想了解更多关于 LLM 的知识。我一直很喜欢书籍的线下体验，这就是我寻找有关 LLM 的书籍的原因。我知道 LLM 是一个相对较新的主题，而且书籍从定义上来说已经过时，因为它们通常在写完几个月后就会出版，但我仍然希望有一些相关的书籍。 干杯    提交人    /u/Calm-Reply778   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efvtbg/d_llm_book_recommendations/</guid>
      <pubDate>Tue, 30 Jul 2024 15:10:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你会使用什么模型进行抽象概括？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eftxem/d_what_model_would_you_use_for_abstractive/</link>
      <description><![CDATA[我正在处理一项 NLP 任务，该任务需要我总结相关的文本组。我曾尝试使用 T5 1.1 基础 模型来完成此任务，但它的总结能力（至少对于我的文本而言）非常糟糕。是否有其他通常推荐用于此类任务的模型？ 我正在考虑使用 Phi-3-mini 作为基准，并从那里搜索具有类似性能的较小模型。    提交人    /u/FPGA_Superstar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eftxem/d_what_model_would_you_use_for_abstractive/</guid>
      <pubDate>Tue, 30 Jul 2024 13:53:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] NeurIPS 2024 论文评论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efscr2/d_neurips_2024_paper_reviews/</link>
      <description><![CDATA[NeurIPS 2024 论文评审将于今日发布。我想创建一个讨论主题，让我们讨论任何问题/抱怨/庆祝或其他任何事情。 每年的评审中都有这么多噪音。考虑到 NeurIPS 这些年来发展如此之大，一些作者引以为豪的好作品可能会因为嘈杂的系统而获得低分。我们应该记住，无论分数如何，这项工作仍然很有价值。    提交人    /u/zy415   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efscr2/d_neurips_2024_paper_reviews/</guid>
      <pubDate>Tue, 30 Jul 2024 12:42:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] MuJoCo 与 Isaac Sim 未来强化学习发展对比</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efqdlt/d_mujoco_vs_isaac_sim_for_future_rl_development/</link>
      <description><![CDATA[大家好， 我希望深入研究 MuJoCo 或 Isaac Sim，并希望获得您的见解，了解哪种环境在未来的 RL 中可能更为突出。 这是我目前所知道的：  Isaac Gym 已被弃用，开发人员现在需要使用 Isaac Sim，这需要计算机提供更多的资源。 MuJoCo MJX 与 Jax 配合使用，将这些值转换为 Torch 模型并不是一件简单的事情。 DeepMind 正在提议使用其他深度学习框架：例如 Haiku、Jraph、Flax，用于使用 Jax 训练模型。  另外，为了做出明智的决定，我还在考虑使用 Arxiv 论文数据集进行一些分析，以了解当前的使用情况和趋势。完成后，我可能会在这里发布研究结果。 鉴于此，您对这些环境的未来发展有何看法？您认为哪一个值得掌握？ 提前感谢您的意见！    提交人    /u/Stefano939393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efqdlt/d_mujoco_vs_isaac_sim_for_future_rl_development/</guid>
      <pubDate>Tue, 30 Jul 2024 10:56:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] SAM2 是对象跟踪领域的新 SOTA 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efprgq/d_is_sam2_the_new_sota_in_object_tracking/</link>
      <description><![CDATA[我尝试过大多数流行的对象跟踪框架，如 bytetrack、botsort、deepsort（均使用 detectorron 和 ultralytics-YOLOv8 后端进行对象检测），但没有一个表现得像我在 SAM2 演示中测试的那样好 - https://sam2.metademolab.com/demo，当然，这是在没有进行任何微调或摆弄参数的情况下进行的。 我知道这不是模型的主要目的，生产结果可能会略有不同。但是，将 YOLO 用于快速物体检测和分类，再加上 SAM2 用于跟踪（用 YOLO bbox 的中心注释 SAM2 跟踪点）是否可以为您提供最佳的物体检测 + 跟踪？ 附言，与我不同，如果您在任何物体跟踪框架方面有更好的体验，请分享。    提交人    /u/Eoncarry   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efprgq/d_is_sam2_the_new_sota_in_object_tracking/</guid>
      <pubDate>Tue, 30 Jul 2024 10:18:52 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 近年来您真正喜欢的非计算密集型研究出版物？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efmmnn/discussion_non_compute_hungry_research/</link>
      <description><![CDATA[整个行业和学术界都出现了一些出色的成果。但是，一项成果的炒作越大，它通常需要的资源/计算量就越大。 那么学术界/行业/小团队（或单个作者）独立完成的一些成果呢？这些成果确实具有基础性或影响力，但所需的计算量却很少（单个或双 GPU，有时甚至 CPU）？ 您想到了哪些成果？为什么您认为它们脱颖而出？    提交人    /u/HopeIsGold   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efmmnn/discussion_non_compute_hungry_research/</guid>
      <pubDate>Tue, 30 Jul 2024 06:43:20 GMT</pubDate>
    </item>
    <item>
      <title>Meta FAIR 刚刚发布“任何事物细分模型 2”[R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efebjl/segment_anything_model_2_just_released_by_meta/</link>
      <description><![CDATA[FAIR 刚刚推出了 SAM 2！ 该模型在图像上的表现优于 SAM，现在还可以在视频中分割物体，并且持续开放。  模型、代码、数据集、演示均已提供。 网站 演示 Github    提交人    /u/fruitofconfusion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efebjl/segment_anything_model_2_just_released_by_meta/</guid>
      <pubDate>Mon, 29 Jul 2024 23:28:03 GMT</pubDate>
    </item>
    <item>
      <title>[N] 从科幻小说到州法律：加州预防人工智能灾难的计划</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efdpmw/n_from_scifi_to_state_law_californias_plan_to/</link>
      <description><![CDATA[Ars Technica：从科幻小说到州法律：加州预防人工智能灾难的计划  加州的“前沿人工智能模型安全创新法案”（又名 SB-1047）引发了一系列有关大型人工智能模型整体“安全性”的头条新闻和辩论。但批评者担心，该法案过分关注未来人工智能模型的生存威胁，可能会严重限制当今更平淡无奇、无威胁的人工智能用途的研究和开发。 SB-1047 由州参议员 Scott Wiener 提出，于 5 月以 32 比 1 的投票通过了加州参议院，并且似乎有望在 8 月的州议会上进行最终投票。  该法案的一个特别值得注意的特点：  在他的 Understanding AI 时事通讯中，Ars 撰稿人 Timothy Lee 阐述了 SB-1047 的措辞如何严重阻碍所谓的“开放权重”AI 模型的传播。这是因为该法案将使模型创建者对基于其原始训练构建的“衍生”模型负责。     提交人    /u/bregav   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efdpmw/n_from_scifi_to_state_law_californias_plan_to/</guid>
      <pubDate>Mon, 29 Jul 2024 23:01:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 理论深度学习会议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1efaug9/d_theoretical_dl_conferences/</link>
      <description><![CDATA[情况是这样的： 今年的研究比我习惯的更加注重理论 所以现在我正在考虑参加一个会议提交论文，这将更加注重理论，而不是那么要求应用。我的部门主要关注更面向应用的场所，这个社区早期的帖子评论大多已过时。 这篇论文非常基础，贡献仅限于监督启发式学习 - 例如通过反向传播 以下是贡献的简短摘要以提供背景信息 TLDR：新颖的视角+方法，非常基本的展示用例 摘要：我们从新颖的视角P（这是一个主要提议）来看待学习过程的元素E，这允许将其视为来自F家族的一个规范。 现在，我们分析如果我们考虑F中的其他规范，行为在某种意义上S会如何变化：推导出某些变化C的下限和上限。 C可以用作学习过程的补充，类似于辍学或某种正则化。我们在一些非常基础且经过充分研究的基准数据集上展示了结果 - 为了展示提案的理论部分。    提交人    /u/One_With_Great_Dao   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1efaug9/d_theoretical_dl_conferences/</guid>
      <pubDate>Mon, 29 Jul 2024 21:03:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有可能兼顾研究和全职工作吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ef6mnn/d_is_it_possible_to_juggle_doing_research_and/</link>
      <description><![CDATA[我目前在一家生命科学公司担任 AI 工程师。我对深度学习研究（尤其是计算机视觉）很感兴趣，但同时又不想辞职。我可以在下班后远程与教授一起做研究员吗？我愿意每周为此投入 20 小时。（工作日 2 小时，周末 10 小时）    提交人    /u/Hour_Amphibian9738   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ef6mnn/d_is_it_possible_to_juggle_doing_research_and/</guid>
      <pubDate>Mon, 29 Jul 2024 18:16:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] Neurips’24 评论发布时间？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ef1var/d_neurips24_review_release_time/</link>
      <description><![CDATA[有人知道评审什么时候发布吗？NeurIPS 网站指出，反驳将于 7 月 30 日在地球上的任何地方开始，而在我们的时区已经是 7 月 30 日了！    提交人    /u/Working-Egg-3424   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ef1var/d_neurips24_review_release_time/</guid>
      <pubDate>Mon, 29 Jul 2024 15:06:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] [P] 知识图谱和对（大量）技术 PDF 进行 ra.g</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ef0iv8/d_p_knowledge_graph_and_rag_over_large_set_of/</link>
      <description><![CDATA[我有一批在一段时间内收集的 PDF（数量增长很快）。主要是技术材料（机器学习、统计学、金融等方面的期刊文章和书籍）。我想基于这些材料创建一个知识图谱，并微调一个 RAG，这样我就可以使用 LLM 搜索我的东西。我的想法是，与其使用 mendeley，我可以使用 LLM 作为强化版的 mendeley。不需要文件夹和标签，我只需提示我正在寻找的内容即可。 问题：  我不想重新发明轮子。要使用哪些软件包 / github？ 有人成功完成过吗？ 本地计算会是一个问题 —— 无法在本地完成。最好是与 ChatGPT 或 Claude 链接的东西（我对这两个都有专业订阅）。  TIA 编辑：如果我可以将我的笔记放入（非常多）overleaf 项目、onenote、supernotes 和 ms 中来执行此操作，那也会很棒。    提交人    /u/daydaybroskii   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ef0iv8/d_p_knowledge_graph_and_rag_over_large_set_of/</guid>
      <pubDate>Mon, 29 Jul 2024 14:11:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 量化的视觉指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eey89o/p_a_visual_guide_to_quantization/</link>
      <description><![CDATA[大家好！随着越来越多的大型语言模型发布以及量化需求的增加，我想是时候编写一份深入且直观的量化指南了。 从探索如何表示值、（非）对称量化、动态/静态量化，到训练后技术（例如 GPTQ 和 GGUF）和量化感知训练（带有 BitNet 的 1.58 位模型）。 https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-quantization 有了超过 60 个自定义视觉效果，我有点做得过火了，但我真的很想尽可能多地包含概念！  本指南的视觉特性允许专注于直觉，希望使所有这些技术易于广大受众使用，无论您是量化新手还是经验丰富的量化新手。    提交人    /u/MaartenGr   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eey89o/p_a_visual_guide_to_quantization/</guid>
      <pubDate>Mon, 29 Jul 2024 12:27:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么机器学习领域中这么多最优秀的人才没有为大型科技公司工作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eejd3b/d_why_so_many_of_the_most_skilled_people_in_the/</link>
      <description><![CDATA[我见过很多拥有常春藤联盟学位的人，研究论文作者，获奖者，课程老师，该领域的书籍作者，但你看他们的领英，这些人中的大多数都不在谷歌、微软、亚马逊、Meta 等大型科技公司 (MANGA 公司) 工作，他们通常在中小型公司工作，我的意思是，写一本关于机器学习的书的人一定知道这件事，拥有剑桥或哈佛计算机科学学位的人可能对此有所了解，为什么有这么多来自大科技公司？ 我知道很多人想专注于研究而不是行业，但大型科技公司确实在 ML 领域进行了最先进的研究，所以对我来说很难知道为什么这些公司不想要这些人或者为什么他们不想为大型科技公司工作。    提交人    /u/millhouse056   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eejd3b/d_why_so_many_of_the_most_skilled_people_in_the/</guid>
      <pubDate>Sun, 28 Jul 2024 22:18:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jul 2024 15:00:14 GMT</pubDate>
    </item>
    </channel>
</rss>