<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 06 Feb 2025 03:20:32 GMT</lastBuildDate>
    <item>
      <title>[D] 一致性模型：为什么模型不会崩溃？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iiqat3/d_consistency_models_why_doesnt_the_model_collapse/</link>
      <description><![CDATA[我一直在阅读一致性模型论文，它已经不是什么新鲜事了，我有几个问题。 如果不深入研究公式的细节，我对损失目标背后的直觉很好奇。更具体地说，为什么当使用一致性蒸馏和一致性训练损失时，模型不会崩溃？ 在我看来，无论输入是什么，模型都很容易崩溃并开始估计所有零输出，这将始终导致零损失值。 我也不明白目标背后的直觉。 任何见解都会对我有帮助，谢谢！ https://preview.redd.it/wa8qkxeo3fhe1.png?width=1138&amp;format=png&amp;auto=webp&amp;s=23f4e8e44ea095​​53b35ae0976c074fff057f314a https://preview.redd.it/3bpptxeo3fhe1.png?width=1140&amp;format=png&amp;auto=webp&amp;s=fd136fa42df794cc08e0db290ffc65d005f200e9    提交人    /u/batchfy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iiqat3/d_consistency_models_why_doesnt_the_model_collapse/</guid>
      <pubDate>Thu, 06 Feb 2025 01:04:28 GMT</pubDate>
    </item>
    <item>
      <title>[P] 针对 VQA 和 OCR 任务训练/微调 VLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iiq610/ptrain_finetuning_vlm_for_vqa_and_ocr_tasks/</link>
      <description><![CDATA[大家好，我正在寻找 vlm 来在我的自定义数据集上对 ocr 和 vqa 任务进行微调。有没有可用的教程和文档供我使用？    提交人    /u/LahmeriMohamed   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iiq610/ptrain_finetuning_vlm_for_vqa_and_ocr_tasks/</guid>
      <pubDate>Thu, 06 Feb 2025 00:58:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 谐波损失训练可解释的 AI 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iioy2i/r_harmonic_loss_trains_interpretable_ai_models/</link>
      <description><![CDATA[免责声明：不是我的作品！Arxiv 版本链接：https://arxiv.org/abs/2502.01628 交叉熵损失利用内积作为相似度度量，而谐波损失使用欧几里得距离。 作者证明，这种替代方法有助于模型在训练期间更快地缩小训练测试差距。 他们还展示了其他好处，例如驱动权重以反映类别分布，使其可解释。    提交人    /u/fliiiiiiip   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iioy2i/r_harmonic_loss_trains_interpretable_ai_models/</guid>
      <pubDate>Thu, 06 Feb 2025 00:00:36 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我开发了一个免费工具，利用机器学习来寻找相关工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iintmv/p_i_built_a_free_tool_that_uses_ml_to_find/</link>
      <description><![CDATA[链接：  https://filtrjobs.com 我为什么构建它： 大多数求职板都基于字符串匹配标题来工作，这对于 ML 来说很糟糕，因为标题很模糊 我厌倦了获得分析职位而不是数据科学职位，或者获得基础设施职位而不是 MLE 因此，我构建了自己的免费工具，将你的简历与招聘信息进行匹配，100％免费，无需注册 工作原理： 它会查看你的简历，将其嵌入，然后根据职位描述进行语义搜索。进行了许多实验，发现 cohere 是最好的嵌入模型。OpenAI 非常糟糕。开源甚至还没有接近 我如何免费运行它：  通过 aiven.io 免费获得 5GB postgres 从 galadriel.com 免费获得 LLM（免费 400 万个代币/天） 通过 heroku 免费托管（github 学生福利免费提供 24 个月） 免费 cerebras LLM 解析（使用 llama 3.3 70B，运行时间为半秒 - 比 gpt 4o mini 快 20 倍） 使用 posthog 和 sentry 进行监控（均提供慷慨的免费套餐）     提交人    /u/_lambda1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iintmv/p_i_built_a_free_tool_that_uses_ml_to_find/</guid>
      <pubDate>Wed, 05 Feb 2025 23:09:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] TTS 和 STT 是如何发展的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iilq85/d_how_are_tts_and_stt_evolving/</link>
      <description><![CDATA[有没有比以下更新 / 更好的东西： TTS：- coqui - piper - tortoise STT：- whisper - deepspeech 为什么 LLM 发展如此迅速，而这些领域却停滞不前？ 别误会我的意思，所有这些项目所做的事情都非常出色，只是下一代可能会更加不可思议    提交人    /u/HansSepp   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iilq85/d_how_are_tts_and_stt_evolving/</guid>
      <pubDate>Wed, 05 Feb 2025 21:41:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找具有文本位置突出显示的 OCR 开源或商业解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iil3l2/d_looking_for_ocr_open_source_or_commercial/</link>
      <description><![CDATA[我正在寻找一个开源或商业 OCR 解决方案，它可以：  处理 PDF 和图像 从这些文档中提取文本 3. 最重要的是： 提供突出显示/显示原始文档中提取特定文本的确切位置的功能（例如，如果它提取了出生日期，我需要能够准确地看到在原始文档中找到该日期的位置，最好使用边界框或类似的突出显示）  有人用过类似的东西吗？ 我非常感谢任何专门包含此文本位置/突出显示功能的工具的推荐。    提交人    /u/mrtule   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iil3l2/d_looking_for_ocr_open_source_or_commercial/</guid>
      <pubDate>Wed, 05 Feb 2025 21:15:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习用于编码孔径图像重建</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iikqxt/d_machine_learning_for_coded_aperture_image/</link>
      <description><![CDATA[我正在研究编码孔径 X 射线望远镜，并且正在探索机器学习是否可以提供比传统反卷积方法更好的结果。我对机器学习的背景知识知之甚少，可以使用一些指针。我找到了一些参考资料，但机器学习实现超出了我的理解范围。我有一个（小）原始图像集合及其重建，我可以使用它们来训练它，但我不确定如何实际设置问题。 这是一个与我所问的类似的参考资料。不幸的是，它位于 Elsevier 付费墙后面    提交人    /u/spacepbandjsandwich   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iikqxt/d_machine_learning_for_coded_aperture_image/</guid>
      <pubDate>Wed, 05 Feb 2025 21:01:28 GMT</pubDate>
    </item>
    <item>
      <title>[N] Deepseek 如何训练他们的 R1 模型，以及当今前沿 LLM 是如何训练的。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iii013/n_how_deepseek_trained_their_r1_models_and_how/</link>
      <description><![CDATA[https://www.youtube.com/watch?v=aAfanTeRn84 Lex Friedman 最近发布了一篇名为“DeepSeek 的 GPU 优化技巧”的采访。这是一篇很棒的幕后花絮，展示了 Deepseek 在没有像美国同行那样多的 GPU 的情况下如何训练他们的最新模型。 需要是发明之母，Deepseek 做了几件事-  他们的专家混合配置非常创新，他们拥有非常高的稀疏因子，8/256 位专家激活。这比其他模型高得多，其他模型中 8 位专家中有​​ 2 位激活。  训练这个模型可能很难，因为只有少数专家真正学习并激活任务，这使得模型很弱。他们引入了辅助损失，以确保所有专家都用于所有任务，从而形成强大的模型。 混合专家模型的一个挑战是，如果只有少数专家激活，那么只有少数 GPU 可能会计算过载，而其余 GPU 则处于闲置状态。辅助损失也可以防止这种情况发生。 他们走得更远，实现了他们自己的 Nvidia NCCL 通信库版本，并使用更接近汇编级 PTX 指令来管理 GPU 中的 SM 如何为每个操作进行调度。这种低级优化使他们的模型在有限的硬件上具有非常高的性能。  他们还讨论了研究人员如何使用新的模型架构和数据工程步骤进行实验。他们说，在训练过程中，损失曲线会出现一些峰值，很难确切知道原因。有时训练后问题会消失，但有时 ML 工程师必须从较早的检查点重新开始训练。 他们还提到了 YOLO 运行，研究人员投入所有可用的硬件和预算来尝试获得前沿模型。他们可能会得到一个非常好的模型，也可能会在这个过程中浪费数亿美元。 这次采访实际上是对当今训练前沿 LLM 的幕后情况的一次非常好的深入观察。我很喜欢，我建议你也去看看！    提交人    /u/ml_guy1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iii013/n_how_deepseek_trained_their_r1_models_and_how/</guid>
      <pubDate>Wed, 05 Feb 2025 19:09:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] 虚幻的安全：Redteaming DeepSeek R1 和 OpenAI、Anthropic 和 Google 最强大的可微调模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iif6qk/r_illusory_safety_redteaming_deepseek_r1_and_the/</link>
      <description><![CDATA[安全护栏是虚幻的。DeepSeek R1 的高级推理可以转化为“邪恶双胞胎”：同样强大，但安全护栏被剥离。GPT-4o、Gemini 1.5 和 Claude 3 也是如此。我们如何确保 AI 最大化利益同时最小化伤害？ 我们通过越狱调整来移除护栏：对带有有害响应的越狱提示进行微调。最初，开源和专有模型都会拒绝几乎所有有害请求。越狱调整后，它们几乎可以帮助解决所有问题：恐怖主义、欺诈、网络攻击等。 经过微调的模型会主动对之前拒绝的危险查询生成详细、精确且可操作的响应。 与基于微调的攻击相比，越狱提示可能不一致且产生质量较差的响应。 薄弱的安全护栏会给人一种虚假的安全感。对保障措施的过度自信可能意味着威胁不受控制——直到为时已晚。 我们该如何解决这个问题？ 😈 邪恶双胞胎评估 - 测试假设最坏情况滥用的预缓解模型。 🚧 红线 - 设定清晰、现实的伤害阈值 &amp;不要越过它们。 🚫 非微调人工智能 - 允许隐私和边缘设备等开放式优势，同时阻止有害的微调。 这不仅仅是一个企业或国家的问题。这是一个共同的挑战。 将人工智能视为一场竞赛——公司与公司、国家与国家、开放与封闭——将每个人都置于危险之中。如果我们想要安全的人工智能，全球合作，而不是竞争，是唯一的出路。 我们必须超越安全的幻想。我们关于越狱调整漏洞和人工智能安全漏洞的新研究将很快全面发布。同时，请查看我们的研究预览： 🔗 http://far.ai/post/2025-02-r1-redteaming/     提交人    /u/KellinPelrine   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iif6qk/r_illusory_safety_redteaming_deepseek_r1_and_the/</guid>
      <pubDate>Wed, 05 Feb 2025 17:16:08 GMT</pubDate>
    </item>
    <item>
      <title>[R] Transformer-Squared：自适应 LLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iicsz0/r_transformersquared_selfadaptive_llms/</link>
      <description><![CDATA[      Sakana AI 的一个框架，允许 LLM 在推理时调整部分权重。 论文 | GitHub | 博客摘要 https://preview.redd.it/61pd7me6jche1.png?width=915&amp;format=png&amp;auto=webp&amp;s=b223fcb9369dc461c0b933669b1026f5eb46d351 摘要：  “自适应大型语言模型 (LLM) 旨在解决传统微调方法通常需要大量计算，并且在处理各种任务的能力上是静态的。我们引入了 Transformer-Squared，这是一种新颖的自适应框架，它通过选择性地调整权重矩阵的奇异分量，实时调整 LLM 以适应看不见的任务。在推理过程中，Transformer-Squared 采用两遍机制：首先，调度系统识别任务属性，然后使用强化学习训练的任务特定“专家”向量进行动态混合，以获得针对传入提示的目标行为。我们的方法始终优于 LoRA 等普遍使用的方法，具有更少的参数和更高的效率。此外，Transformer-Squared 展示了跨不同 LLM 架构和模态的多功能性，包括视觉语言任务。 Transformer-Squared 代表着一次重大的飞跃，它提供了一种可扩展、高效的解决方案，可增强 LLM 的适应性和特定任务性能，为真正动态、自组织的 AI 系统铺平了道路。&quot;  https://preview.redd.it/w5tey3kebche1.png?width=907&amp;format=png&amp;auto=webp&amp;s=15550138bac56f881d8981d5e45022c4cbf6c278 https://preview.redd.it/nb3rdwagbche1.png?width=962&amp;format=png&amp;auto=webp&amp;s=df98f74dea04365eefba9bf4004ba1c3c50a3359 结论：  在本文中，我们引入了 Transformer2，为实现自适应 LLM 提供了新颖的蓝图。在这个框架内，我们首先提出了 SVF，它比以前的微调方法性能更出色，同时成本更低、组合性更强、正则化过拟合——这些都是实现可扩展自适应的关键特性。利用一组 SVF 专家作为构建块，我们开发了三种有效的自适应策略，每种策略都具有独特的优势，并且随着测试时间条件的增加，性能优势也变得单调。 虽然 Transformer2 展示了令人鼓舞的结果，但未来仍有令人兴奋的发展机会。一个限制是 SVF 专家的能力与基础模型的潜在组件相关联。为了解决这个问题，模型合并提供了一个有希望的方向（Yu 等人，2024；Goddard 等人，2024；Akiba 等人，2024），使专门的模型能够组合成一个功能更强大的模型。此外，虽然我们基于 CEM 的适应性有效地平衡了性能和效率，但扩展到大量专门领域可能会增加一次性计算成本。然而，这种权衡被改进的性能和增强的自适应能力的好处所抵消。模型合并和高效适应技术的进步使得模型在开放排行榜上占据主导地位，使它们成为 Transformer2 基础模型的有力候选者，并为自适应 LLM 开辟了新的可能性。     提交人    /u/Jind0sh   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iicsz0/r_transformersquared_selfadaptive_llms/</guid>
      <pubDate>Wed, 05 Feb 2025 15:39:02 GMT</pubDate>
    </item>
    <item>
      <title>研究人员和数据科学家真的会用这个吗？我正在开发一个人工智能工具来更快地找到数据集。[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ii44ba/would_researchers_and_data_scientists_actually/</link>
      <description><![CDATA[我正在开发一个 AI 平台，帮助研究人员和数据科学家使用自然语言搜索从多个来源（Kaggle、政府门户、API、学术数据库等）中找到正确的数据集。目前，这个过程是超级手动的：大量谷歌搜索、检查不同的网站，并处理不一致的格式。我希望它能够轻松找到针对超特定问题的超级小众数据集。 Tl;dr – 我认为这可以通过聚合数据集、总结数据集（列、大小、上次更新）甚至建议相关数据集来节省研究人员和 ML/数据科学家数小时的时间。 更长的解释： 使用此工具，您可以输入类似“我需要有关年轻人智能手机使用情况和心理健康的数据”的内容，它会在各个平台上找到相关数据集。它还会提供快速摘要，以便您知道是否值得下载而无需深入挖掘。  基于您的主题的智能推荐 API 集成以提取实时数据（例如来自 Twitter、Google Trends） 如果您想合并数据集，则可以使用数据集兼容性检查器  这有用吗？ 在我开始构建之前，试着看看这是否真的是人们会使用的东西。欢迎反馈！🙏    提交人    /u/paullieber98   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ii44ba/would_researchers_and_data_scientists_actually/</guid>
      <pubDate>Wed, 05 Feb 2025 06:43:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 目前计算机视觉和语言技术领域不受欢迎的研究课题有哪些？ 2025</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihzy00/d_what_are_current_unpopular_research_topics_in/</link>
      <description><![CDATA[不，我不想再听到有关 LLM 和 VLM 的更多信息了。    提交人    /u/KingsmanVince   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihzy00/d_what_are_current_unpopular_research_topics_in/</guid>
      <pubDate>Wed, 05 Feb 2025 02:43:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM如何解决新的数学问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ihsftt/d_how_does_llm_solves_new_math_problems/</link>
      <description><![CDATA[从架构角度来看，我理解 LLM 会处理来自用户查询和提示的标记，然后据此预测下一个标记。思路链机制本质上会推断这些预测以创建内部反馈循环，从而增加在训练期间使用强化学习时得出正确答案的可能性。当根据模型已知的信息解决问题时，此过程很有意义。 但是，当涉及到新的数学问题时，挑战不仅仅是简单的标记预测。模型必须理解问题，掌握底层逻辑，并使用适当的公理、定理或函数解决问题。它是如何做到这一点的？这个内部逻辑求解器从何而来，为 LLM 提供了解决此类问题所需的工具？ 澄清：新数学问题是指模型在训练期间未遇到的问题，这意味着它们不是以前见过的问题的完全重复。    提交人    /u/capStop1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ihsftt/d_how_does_llm_solves_new_math_problems/</guid>
      <pubDate>Tue, 04 Feb 2025 21:03:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ifnw79/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Feb 2025 03:15:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>