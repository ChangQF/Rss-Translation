<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Thu, 11 Jan 2024 06:18:53 GMT</lastBuildDate>
    <item>
      <title>[D] [RAG] [llama-index] 如何在NL2SQL项目中使用SQLTableRetrieverQueryEngine执行多个SQL查询？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193usfg/d_rag_llamaindex_how_to_execute_multiple_sql/</link>
      <description><![CDATA[我正在开发一个项目，用户将询问自然语言查询，这个基于 llama-index 的引擎会将自然语言转换为 sql 查询并执行它在我的数据库上并以自然语言向用户提供答案。问题是它只能对每个问题执行一个查询，因此无法回答比较问题，而且如果问题不需要查询数据库，它仍然会查询数据库。我该如何解决这个问题。请帮我提出您的建议。 提前致谢。    由   提交 /u/HappyDataGuy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193usfg/d_rag_llamaindex_how_to_execute_multiple_sql/</guid>
      <pubDate>Thu, 11 Jan 2024 06:11:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 管理分析团队</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193ukh8/d_managing_analytics_teams/</link>
      <description><![CDATA[Hola Amigos， 我的团队正在考虑修改我们的分析项目的工作方式（简单和基于机器学习）。想了解一下你们从事分析工作的人是如何工作的？ 敏捷吗？是混乱吗？您的站立会议包括看板，市场上有一种新工具。你们如何计划和执行项目。 把一切都交给我，帮助我们的团队了解市场的其他部分是如何运作的。   由   提交 /u/boredmonki   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193ukh8/d_managing_analytics_teams/</guid>
      <pubDate>Thu, 11 Jan 2024 05:59:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过 Tesla P100 来微调 LLM 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193t4w8/d_anyone_tried_a_tesla_p100_for_finetuning_llms/</link>
      <description><![CDATA[我最近创建了一个工具来跟踪 GPU 的性价比。我惊讶地发现 NVIDIA Tesla P100 在 $/FP16 TFLOPs 和 $/FP32 TFLOPs，尽管甚至没有张量核心。只是好奇是否有人尝试使用它来微调 LLM 或其他神经网络以进行训练，并可以评论其与其他 GPU 相比的性能及其成本。   由   提交 /u/activescott   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193t4w8/d_anyone_tried_a_tesla_p100_for_finetuning_llms/</guid>
      <pubDate>Thu, 11 Jan 2024 04:38:40 GMT</pubDate>
    </item>
    <item>
      <title>[D]非结构化医学文本知识图谱提取</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193s7dq/d_knowledge_graph_extraction_from_unstructured/</link>
      <description><![CDATA[我正在尝试从一组医学文章生成知识图。我之前的方法是使用实​​体识别/链接器库，例如 https://allenai.github.io/scispacy/ 和零-shot 关系提取器，例如 https://github.com/fractalego/zero-shot-relation-extractor 。然而，实体链接指标并不好（如所提到的网页中所示），并且零样本关系提取器往往会产生大量噪声关系，特别是在传递多个关系类型的情况下。  有人对更有效的知识图谱提取技术有一些好的建议吗？我的顾问建议我们可以使用法学硕士来生成知识图，但我不确定要使用哪些法学硕士以及是否有任何已发布的指标。理想情况下，我希望避免自己验证多个法学硕士，并使用一种相对流行且易于使用的稳健方法。   由   提交/u/newperson77777777  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193s7dq/d_knowledge_graph_extraction_from_unstructured/</guid>
      <pubDate>Thu, 11 Jan 2024 03:49:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 监控实时 pytorch 模型的最佳 ML 跟踪工具？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193qeup/d_best_ml_tracking_tool_to_monitor_live_a_pytorch/</link>
      <description><![CDATA[您好， 我想微调模型的超参数，并且希望实现一个 ML 跟踪工具，例如作为 MLflow 来跟踪我的模型性能。 但是，每次训练大约需要 8 小时，在训练期间观察指标的实时变化（即观察损失曲线增长等）会很有趣 我对管道的这一部分真的很陌生，所以我不知道哪些是最好的工具 是否可以使用 MLflow 来做到这一点？我已经实现了它，但它似乎只在训练脚本完成后显示图表和图表 如果 MLflow 不可能，你们能建议我最好的包吗？ &gt; 研究该主题后我想到的设置是 Hydra + MLflow + Optuna，如果你们对这个问题有更有经验的观点，我很高兴听到:) 非常感谢！   由   提交/u/Reference-Guilty  /u/Reference-Guilty  reddit.com/r/MachineLearning/comments/193qeup/d_best_ml_tracking_tool_to_monitor_live_a_pytorch/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193qeup/d_best_ml_tracking_tool_to_monitor_live_a_pytorch/</guid>
      <pubDate>Thu, 11 Jan 2024 02:21:08 GMT</pubDate>
    </item>
    <item>
      <title>部署多模式模型与法学硕士有何不同？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193oj6r/whats_different_about_deploying_a_multimodal/</link>
      <description><![CDATA[作为多模式模型，例如 Google Gemini 、皮卡和Stable Diffusion Video 将于 2024 年开始推出，我一直在考虑部署它们的 ML 操作。  与使用法学硕士进行部署相比，有哪些独特的挑战？   由   提交 /u/vanteworldinfinity   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193oj6r/whats_different_about_deploying_a_multimodal/</guid>
      <pubDate>Thu, 11 Jan 2024 00:52:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有 XAI 和 Diffusion 模型的论文列表吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193klql/d_any_paper_lists_for_xai_and_diffusion_models/</link>
      <description><![CDATA[我找到了精心策划的有关视觉变压器、ODD 检测和遗忘的论文列表。我很好奇是否有任何论文列表包含可解释的人工智能和扩散模型的重要论文   由   提交/u/V1bicycle  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193klql/d_any_paper_lists_for_xai_and_diffusion_models/</guid>
      <pubDate>Wed, 10 Jan 2024 22:03:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于时间序列分类和峰值计数的 ML 算法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193jq7k/d_ml_algorithms_for_time_series_classification/</link>
      <description><![CDATA[我目前正在开展一个项目，涉及处理实时加速度计、陀螺仪、来自可穿戴设备的方向数据以进行健身锻炼，我需要对这些数据进行分类和计数峰值，对应于代表。我有一些关于执行此操作的最佳技术的问题。我已经阅读了一些关于此的研究，并尝试复制下面附有的最准确的论文。 我正在使用 XGBoost 对练习进行分类，输入是所有传感器的时间序列数据，这执行对于一些容易分类的练习，得分为 99%，对于一些困难的练习，得分为 92%。当我最初使用两层人工神经网络进行尝试时，其准确性非常差，可能是因为我目前没有太多数据。但Xgboost效果很好。  Q- 这个时间序列分类的最佳方法应该是什么？（假设我还做了一些更多的特征提取，例如统计特征和 FFT、峰度，这似乎增加了研究建议的精确度和召回率） 我的另一个疑问是，对于只包含数据的练习，我应该如何计算数据中的重复次数/峰值，这些数据有时可能很嘈杂。加速分量并且没有手腕旋转，就像过头推举一样。对于简单的练习，进行一些信号处理、平滑信号并检查转折点就足够容易了。但对于困难的信号，信号可能有很多其他噪声和峰值，很难区分。许多论文建议，对时间轴使用一些阈值，只考虑具有一定时间差的峰值和类似的幅度。一篇论文使用了这一点，作者的准确率中位数达到了 95-96%，但它根据所选择的轴和其他因素而变化。 Q- 但我想知道是否有可以是一个机器学习模型，可以接受输入特征信号并输出​​代表次数，这有多困难？我使用带有大约 300 组跳跃开合的 LSTM 回归器进行了尝试，这似乎是最容易计算重复次数的方法，但它无法学习任何内容并且严重过度拟合。 非常感谢您对这个问题的意见以及优化准确性的最佳方法。 参考论文： https://www.mdpi.com/1424-8220/23/10/4602 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6387025/  &amp;# 32；由   提交 /u/Charlieputhfan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193jq7k/d_ml_algorithms_for_time_series_classification/</guid>
      <pubDate>Wed, 10 Jan 2024 21:28:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调 Open CLIP 模型导致其零射击精度在第一个 epoch 后急剧下降</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193gug2/d_fine_tuning_open_clip_model_causes_it_zero_shot/</link>
      <description><![CDATA[我在 MSCOCO 2017 上微调 CLIP (model_name=&#39;ViT-B-32&#39;, pretrained=&#39;laion2b_s34b_b79k)使用 https://github.com/mlfoundations/open_clip/tree/main/src/training 中的代码的标题数据集 但我不知道为什么即使在第 1 轮之后，ImageNetV2 上的零射击准确率也会从 58.11% 下降到 0.1%，并陷入困境。 有任何可能的原因吗？ &lt; /div&gt;  由   提交 /u/MaintenanceNo5993   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193gug2/d_fine_tuning_open_clip_model_causes_it_zero_shot/</guid>
      <pubDate>Wed, 10 Jan 2024 19:32:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] XGBoost 始终获得 100% 准确率</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193fp8y/d_xgboost_always_gets_100_accuracy/</link>
      <description><![CDATA[我使用 BigQuery ML 遇到了一个二元分类问题，我使用 Logistic 回归做了一次并获得了 87% 的准确率，然后使用 XGBoost 提升树再次做了一次并获得了100%。这是正常的吗？或者我错过了什么？我什至在另一个数据集上尝试过，损失为 0.00017，接近 100%。   由   提交/u/Ibrahim-Izz   /u/Ibrahim-Izz reddit.com/r/MachineLearning/comments/193fp8y/d_xgboost_always_gets_100_accuracy/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193fp8y/d_xgboost_always_gets_100_accuracy/</guid>
      <pubDate>Wed, 10 Jan 2024 18:46:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当镜头是长序列时，我们如何使用 LLM 进行少镜头学习？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193f0gi/d_how_do_we_perform_fewshot_learning_using_llms/</link>
      <description><![CDATA[我看到了几篇关于使用 LLM 进行小样本学习的上下文学习的文章。大多数情况下提供 1 到 30 个镜头作为上下文。 对于镜头很长的情况（例如摘要、文档分类）如何执行此操作，因为 LLM 无法处理超过 2048 个标记（我不是谈论长上下文法学硕士）？   由   提交/u/kekkimo  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193f0gi/d_how_do_we_perform_fewshot_learning_using_llms/</guid>
      <pubDate>Wed, 10 Jan 2024 18:18:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于预测的最佳时间序列模型（替代 TimeGPT）？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/193672o/d_best_time_series_models_for_forecasting/</link>
      <description><![CDATA[我最近发现了TimeGPT，它真的很棒需求预测。 我不太擅长使用 pytorch，但我无法实现任何接近 TimeGPT 的结果。 我现在正在寻找类似的（甚至更好的） ？）对于预测数据（在我的例子中是需求预测）表现非常好的模型。 感谢您的建议！    ;由   提交/u/Benni03155  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/193672o/d_best_time_series_models_for_forecasting/</guid>
      <pubDate>Wed, 10 Jan 2024 11:30:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] AdamL：一种结合损失函数的快速自适应梯度方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1932cv6/r_adaml_a_fast_adaptive_gradient_method/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.15295 摘要：  自适应一阶优化器是深度学习的基本工具，尽管由于梯度缩放不均匀，它们可能会受到泛化能力差的影响。在这项工作中，我们提出了 Adam 优化器的一种新颖变体 AdamL，它考虑了损失函数信息以获得更好的泛化结果。我们提供了充分的条件，与 Polyak-Lojasiewicz 不等式一起确保 AdamL 的线性收敛。作为我们分析的副产品，我们证明了 EAdam 和 AdaBelief 优化器的相似收敛特性。基准函数的实验结果表明，与 Adam、EAdam 和 AdaBelief 相比，AdamL 通常可以实现最快的收敛或最低的目标函数值。当考虑深度学习任务（例如训练卷积神经网络、使用普通卷积神经网络训练生成对抗网络和长短期记忆网络）时，这些优越的性能得到了证实。最后，就普通卷积神经网络而言，AdamL 从其他 Adam 变体中脱颖而出，不需要在训练后期手动调整学习率。  &lt;强&gt;编辑： 实现：https://github .com/andrewjc/PyTorch-AdamL   由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1932cv6/r_adaml_a_fast_adaptive_gradient_method/</guid>
      <pubDate>Wed, 10 Jan 2024 07:07:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 设备端人工智能是未来吗？ NVIDIA 在 CES 上发起挑战</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1930r1g/d_is_ondevice_ai_the_future_nvidia_throws_down/</link>
      <description><![CDATA[NVIDIA 在 CES 上的重大发布集中于一个关键主题：将强大的 AI 功能直接引入您的 PC 或笔记本电脑。 开发者工具：  AI Workbench（测试版）：跨 Hugging Face、GitHub 和 NVIDIA NGC 等平台简化 AI 开发。 RTX Remix：通过 AI 驱动的升级和元素修改为经典游戏注入新的活力。 NVIDIA Avatar Cloud Engine (ACE)：创建 AI 驱动的游戏游戏和其他应用程序的数字化身。 与 RTX 聊天：构建利用本地法学硕士和用户数据的个人助理和聊天机器人。  这是设备上人工智能主导地位的黎明吗？很容易说是。 NVIDIA 强大的硬件和用户友好的工具使本地运行 AI 变得比以往更加容易。然而，挑战仍然存在：  电池寿命：配备强大 GPU 的笔记本电脑可能需要在附近配备额外的充电器。 软件成熟度：设备上的 AI 软件仍在不断发展，并且开发人员采用率需要提高。 可访问性：高端硬件是有代价的，可能会限制广泛采用。  您认为呢？设备上的人工智能是未来，还是基于云的人工智能仍然是王者？在下面的评论中分享您的想法！   由   提交/u/Instantinopaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1930r1g/d_is_ondevice_ai_the_future_nvidia_throws_down/</guid>
      <pubDate>Wed, 10 Jan 2024 05:31:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18vao7j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 31 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>