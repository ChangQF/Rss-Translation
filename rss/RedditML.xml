<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sun, 12 Jan 2025 06:21:51 GMT</lastBuildDate>
    <item>
      <title>[R] 生成式人工智能如何彻底改变文本、图像和视频创作——洞察与应用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hzf14i/r_how_generative_ai_is_revolutionizing_text_image/</link>
      <description><![CDATA[生成式人工智能正在利用 GPT、DALL·E 和 Stable Diffusion 等工具重塑行业。从内容创作到医疗保健和市场营销，这些技术正在为自动化和创造力带来新的可能性。 在我的最新文章中，我深入探讨了： • 这些模型背后的架构（Transformers、扩散模型）。 • 在教育、市场营销等领域的实际应用。 • 部署生成式 AI 的挑战和机遇。 如果您有兴趣探索 AI 如何改变创意格局，请查看此处的文章：生成式 AI：应用和见解 希望在评论中听到您对生成式 AI 的想法或经验！    提交人    /u/Ok-Bowl-3546   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hzf14i/r_how_generative_ai_is_revolutionizing_text_image/</guid>
      <pubDate>Sun, 12 Jan 2025 04:45:08 GMT</pubDate>
    </item>
    <item>
      <title>[P] Llama3 推理引擎 - CUDA C</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hze3vs/p_llama3_inference_engine_cuda_c/</link>
      <description><![CDATA[      嗨，r/MachineLearning，最近我从 llama.cpp、ollama 和类似的工具中获得了灵感，这些工具可以在本地进行 LLM 推理，我刚刚用 CUDA C 为 8B 模型构建了一个 Llama 推理引擎。 作为构建优化 GPGPU 软件的探索性工作的一部分，我决定从头开始构建它。这个项目只使用本机 CUDA 运行时 api 和 cuda_fp16。推理发生在 fp16 中，因此它需要大约 17-18GB 的​​ VRAM（~16GB 用于模型参数，更多用于中间缓存）。 它不使用 cuBLAS 或任何类似的库，因为我想接触最少的抽象。因此，它不像 cuBLAS 实现或其他推理引擎（如启发该项目的引擎）那样优化。 实现的简要概述 我使用了 CUDA C。它读取了模型的 .safetensor 文件，您可以从 HuggingFace 中提取该文件。实际的内核对于规范化、跳过连接、RoPE 和激活函数 (SiLU) 相当简单。 对于 GEMM，我已实现平铺矩阵乘法并为每个线程进行矢量化检索。GEMM 内核的编写方式也是，第二个矩阵不需要预先转置，同时仍可实现对 HBM 的合并内存访问。 有些内核（如 RoPE 和 GEMM 的内核）使用矢量化内存访问。 SwiGLU 前馈计算的部分内容在自定义融合内核中进行。 如果您感兴趣，可以随意查看项目仓库并尝试一下。如果您喜欢所看到的内容，也可以随意为仓库点赞！ 我非常感谢任何反馈，无论是好的还是建设性的。    提交人    /u/Delicious-Ad-3552   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hze3vs/p_llama3_inference_engine_cuda_c/</guid>
      <pubDate>Sun, 12 Jan 2025 03:51:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 数字特征：机器学习爱好者的深度指南</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hzdd2h/r_numeric_features_an_indepth_guide_for_machine/</link>
      <description><![CDATA[📊 了解机器学习中的数字特征 数字特征是许多机器学习模型的支柱，为算法提供做出准确预测所需的定量数据。从医疗保健分析到金融建模，它们在当今的数据驱动世界中不可或缺。 指南中的关键见解： ✔️ 什么是数字特征？ • 连续特征与离散特征，以及它们在 ML 中至关重要的原因。 ✔️ 处理数字数据： • 缩放、规范化和处理缺失值等技术。 • 异常值检测和转换以提高模型性能。 ✔️ 实际应用： • 财务比率、医疗保健指标、零售趋势等。 ✔️ 算法注意事项： • 为什么 KNN、神经网络和其他网络的特征需要缩放。 ✔️ 实用技巧： • 可视化数据、选择特征以及避免使用多项式特征过度拟合。 🌐 为什么要读这篇文章？ 如果您从事数据科学、机器学习或特征工程工作，本文包含实用的见解和示例，您今天可以将它们应用到您的项目中。 📖 在此处查看完整文章：数字特征：深入指南  #MachineLearning #FeatureEngineering #DataScience #NumericFeatures #DataEngineering #AI #DataPreprocessing #BigData #MLTips    提交人    /u/Ok-Bowl-3546   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hzdd2h/r_numeric_features_an_indepth_guide_for_machine/</guid>
      <pubDate>Sun, 12 Jan 2025 03:10:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 SAM 进行图像分割</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz7n71/d_image_segmentation_with_sam/</link>
      <description><![CDATA[有没有地方可以像 SAM 网站上那样，通过单击图像的不同部分来添加蒙版（或按住 Shift 键单击以删除）并最终下载蒙版来分割图像？ 我测试了一些标记工具，但我发现它们都不如元演示效果好。元网站的问题是我无法下载蒙版，我只能从图像中剪切出来。    提交人    /u/Helbal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz7n71/d_image_segmentation_with_sam/</guid>
      <pubDate>Sat, 11 Jan 2025 22:26:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪个库适合扩散模型研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz541n/d_which_library_is_good_for_diffusion_model/</link>
      <description><![CDATA[我想尝试使用扩散模型并切换管道的不同部分（例如采样器、模型、数据模态等或使用自定义部分）。我看了一些库，例如 modules_diffusion 或 diffusor，但它们似乎还不是很成熟或不是很高级。您在研究中使用哪种库来试验扩散模型？    提交人    /u/NumerousSwordfish653   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz541n/d_which_library_is_good_for_diffusion_model/</guid>
      <pubDate>Sat, 11 Jan 2025 20:32:23 GMT</pubDate>
    </item>
    <item>
      <title>[P] 未来推理模型的硬算法基准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz4gdy/p_a_hard_algorithmic_benchmark_for_future/</link>
      <description><![CDATA[嗨，我一直在考虑一个简单的想法，开发一个面向未来的动态 AI 模型基准。这个想法很简单。一个隐藏的函数转换数据，模型只能看到前后情况，并且必须推断出隐藏的逻辑。我精心策划了几个难度略有增加的级别，我很惊讶地发现我可以访问的大多数当前模型（GTP、o1、Sonet、Gemini）都很糟糕。 例如，第一个谜题只是对输入缓冲区上的字节执行 ^=0x55，但大多数模型都很难看到或推断出它。 我已经启动了一个带有现场演示的开源 MIT 存储库，因此其他人可以尝试这个想法或做出贡献。我很感激任何反馈。谢谢！    提交人    /u/habitante   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz4gdy/p_a_hard_algorithmic_benchmark_for_future/</guid>
      <pubDate>Sat, 11 Jan 2025 20:02:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对 Google Paxml (又名 Pax) 的看法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz2dfp/d_thoughts_on_google_paxml_aka_pax/</link>
      <description><![CDATA[我刚刚发现了 Pax，这是一个在 Jax 之上配置和运行机器学习实验的框架。你知道这个吗？对于大规模模型来说，它可能比 Pytorch 更好。    提交人    /u/johnbond2   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz2dfp/d_thoughts_on_google_paxml_aka_pax/</guid>
      <pubDate>Sat, 11 Jan 2025 18:30:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找神经网络的最优超参数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz2ct5/d_finding_optimal_hyper_parameter_for_neural/</link>
      <description><![CDATA[我一直在尝试使用灰狼算法 (GWO) 和粒子群优化器 (PSO) 为 LSTM 模型找到最佳超参数。这花费了很多时间。下面是我正在做的事情的描述。 我有一个包裹在目标函数中的 LSTM 模型需要优化。此函数根据传递给它的参数构建模型，然后训练模型并在测试数据上找到 MSE。此测试数据是根据 GWO 优化器将计算适应度而返回的。 这个过程需要几个小时。还有其他方法可以找到最佳参数吗？    提交人    /u/ImranAlam_red   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz2ct5/d_finding_optimal_hyper_parameter_for_neural/</guid>
      <pubDate>Sat, 11 Jan 2025 18:30:09 GMT</pubDate>
    </item>
    <item>
      <title>[N] 我不明白 LORA</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz1xks/n_i_dont_get_lora/</link>
      <description><![CDATA[人们一直给我一行语句，比如 dW =A B 的分解，因此 vram 和计算效率高，但我完全不明白这个论点。  为了计算 dA 和 dB，你不是先计算 dW，然后将它们传播到 dA 和 dB 吗？此时你不需要计算 dW 所需的那么多 vram 吗？并且比反向传播整个 W 需要更多的计算？ 在前向运行期间：你是否在每一步之后都用 W= W&#39; +A B 重新计算整个 W？因为否则你如何使用更新的参数计算损失？  请不要发怒，我不想听到 1。这太简单了，你不应该问 2。问题不清楚 请让我知道哪个方面不清楚。谢谢    由   提交  /u/Peppermint-Patty_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz1xks/n_i_dont_get_lora/</guid>
      <pubDate>Sat, 11 Jan 2025 18:11:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] 构建了一个贪吃蛇游戏，使用扩散模型作为游戏引擎。它几乎实时运行 🤖 它根据用户输入和当前帧预测下一帧。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hz1l2j/p_built_a_snake_game_with_a_diffusion_model_as/</link>
      <description><![CDATA[        由    /u/jurassimo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hz1l2j/p_built_a_snake_game_with_a_diffusion_model_as/</guid>
      <pubDate>Sat, 11 Jan 2025 17:57:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在哪里可以找到机器学习工程师/人工智能工程师的面试经历？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyskcn/d_where_can_i_find_machine_learning_engineerai/</link>
      <description><![CDATA[我需要了解一些除 glassdoor 之外的候选人的面试经历。我想要一些资源，告诉我面试有多少轮，每轮发生了什么。如果你有这样的资源，请告诉我。    提交人    /u/nanuupendra   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyskcn/d_where_can_i_find_machine_learning_engineerai/</guid>
      <pubDate>Sat, 11 Jan 2025 09:42:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 检查你的学者统计数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hypgxp/p_check_your_scholar_stats/</link>
      <description><![CDATA[  由    /u/yoonjeewoo  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hypgxp/p_check_your_scholar_stats/</guid>
      <pubDate>Sat, 11 Jan 2025 05:56:55 GMT</pubDate>
    </item>
    <item>
      <title>[数据集][R] 19,762 张垃圾图像，用于构建 AI 回收解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/</link>
      <description><![CDATA[大家好，ML 社区！ 我很高兴与大家分享垃圾分类 V2 数据集，其中包含 19,762 张高质量垃圾图像，这些垃圾被分为 10 个不同类别（例如金属、塑料、衣服和纸张）。 为什么这很重要：  训练 AI 模型以实现自动垃圾分类和回收。 开发垃圾分类应用程序或以可持续性为重点的工具。 创建创新的计算机视觉项目以影响环境。  🔗 数据集链接： 垃圾分类 V2 该数据集已在研究论文《通过迁移学习管理家庭垃圾》中使用，证明了其在实际应用中的实用性。 期待看到您如何使用它来促进可持续发展！    提交人    /u/Downtown_Bag8166   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hyfaoc/datasetr_19762_garbage_images_for_building_ai/</guid>
      <pubDate>Fri, 10 Jan 2025 21:19:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>