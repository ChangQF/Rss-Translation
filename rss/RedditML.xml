<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Tue, 18 Jun 2024 06:20:47 GMT</lastBuildDate>
    <item>
      <title>同一时间戳中的多个数据[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dijl8h/multiple_data_in_a_same_timestamp_d/</link>
      <description><![CDATA[大家好，我来这里是为了澄清我的疑惑。对于一个项目，我已经连接了五个路由器来创建一个网状网络。从中，我收集了路由器每个链路之间的一些数据。问题是，由于我同时收集了所有数据，所以我有多个具有相同时间戳的数据点。由于我是时间序列分析的新手，我不确定如何处理这种情况。如果您遇到过类似的情况，请提出一个好的解决方案。    提交人    /u/dumbestindumb   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dijl8h/multiple_data_in_a_same_timestamp_d/</guid>
      <pubDate>Tue, 18 Jun 2024 06:02:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我该如何处理这个 NLP 问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1diihrg/r_how_would_i_approach_this_nlp_problem/</link>
      <description><![CDATA[我该如何处理这个 NLP 问题？ 所以我想使用标有五大性格特征（用数字表示）的多人对话数据来识别某人的性格。 我正在使用 BERT 模型和 FriendsPersona 数据集来实现一篇论文，但显然 BERT 只能在简单的独白文本上发挥作用。准确率也不是那么高。 有人知道我可以实现的任何模型或论文来解决这个问题吗，也许需要一点迁移学习？    提交人    /u/SnooMaps8602   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1diihrg/r_how_would_i_approach_this_nlp_problem/</guid>
      <pubDate>Tue, 18 Jun 2024 04:52:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 视频传播模型的新调查和评论论文！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dihzuu/r_new_survey_and_review_paper_for_video_diffusion/</link>
      <description><![CDATA[标题：视频扩散模型：综述  作者： Andrew Melnik、Michal Ljubljanac、Cong Lu、Qi Yan、Weiming Ren、Helge Ritter。  论文： https://arxiv.org/abs/2405.03150  摘要：扩散生成模型最近已成为一种用于制作和修改连贯、高质量视频的强大技术。本综述系统地概述了用于视频生成的扩散模型的关键要素，涵盖了应用、架构选择和时间动态建模。总结了该领域的最新进展并归类为发展趋势。调查最后概述了剩余的挑战并展望了该领域的未来。     提交人    /u/MolassesWeak2646   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dihzuu/r_new_survey_and_review_paper_for_video_diffusion/</guid>
      <pubDate>Tue, 18 Jun 2024 04:22:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] DiTTo-TTS：使用 Diffusion Transformer 实现高效、可扩展的零样本文本转语音</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dihfqu/r_dittotts_efficient_and_scalable_zeroshot/</link>
      <description><![CDATA[  由    /u/keonlee9420  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dihfqu/r_dittotts_efficient_and_scalable_zeroshot/</guid>
      <pubDate>Tue, 18 Jun 2024 03:51:28 GMT</pubDate>
    </item>
    <item>
      <title>[D]有人在 ARR 六月（EMNLP）周期中收到过桌面拒绝警告吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dig8sk/ddid_anyone_receive_a_desk_rejection_warning_in/</link>
      <description><![CDATA[我于 6 月向 ARR (EMNLP) 提交了论文，并收到了以下消息。   所有作者都没有 ACL Anchology 个人资料，而且部分作者今年只接受了 1 篇论文（尚未发表）。 我认为，我们都没有资格成为审稿人（志愿者）（因为我们没有至少 3 篇论文）。 有人知道我们为什么收到此消息吗？    提交人    /u/ImpossibleAd568   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dig8sk/ddid_anyone_receive_a_desk_rejection_warning_in/</guid>
      <pubDate>Tue, 18 Jun 2024 02:46:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 具有有界激活函数的批量规范行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dicnld/d_batchnorm_behavior_with_bounded_activation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dicnld/d_batchnorm_behavior_with_bounded_activation/</guid>
      <pubDate>Mon, 17 Jun 2024 23:46:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要帮助加载预训练模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1diaolb/d_need_help_with_loading_a_pretrained_model/</link>
      <description><![CDATA[      https://preview.redd.it/ndj7swkci77d1.png?width=1359&amp;format=png&amp;auto=webp&amp;s=ab2e8f59d400056959ca1a4df63d32d97bb16dc9 https://github.com/MiteshPuthran/Speech-Emotion-Analyzer 这是我从中获取模型的 repo ，问题是 model.json 文件已经有 7 年的历史了，我在加载该 json 文件时遇到此错误 请帮帮我     提交人    /u/HarryBarryGUY   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1diaolb/d_need_help_with_loading_a_pretrained_model/</guid>
      <pubDate>Mon, 17 Jun 2024 22:17:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 开源如何才能达到 Sora 或 RunwayML Gen-3 的质量？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di5txg/d_how_can_open_source_achieve_the_quality_of_sora/</link>
      <description><![CDATA[正在经历 https://github.com/PKU-YuanGroup/Open-Sora-Plan 我们如何提高输出质量？    提交人    /u/cbsudux   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di5txg/d_how_can_open_source_achieve_the_quality_of_sora/</guid>
      <pubDate>Mon, 17 Jun 2024 18:54:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 量化评估基准中的差异</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di354e/r_quantifying_variance_in_evaluation_benchmarks/</link>
      <description><![CDATA[  由    /u/RSchaeffer  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di354e/r_quantifying_variance_in_evaluation_benchmarks/</guid>
      <pubDate>Mon, 17 Jun 2024 17:03:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用各种 PEFT 方法对具有 6GB GPU RAM 的 Gemma 2B LLM 进行微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di34p2/p_finetuning_gemma_2b_llm_with_6gb_gpu_ram_using/</link>
      <description><![CDATA[嗨，我想了解各种 PEFT 技术，我的目标是在我的 RTX A4000 笔记本电脑 GPU 上微调一些 LLM，它有大约 6GB 的可用 GPU RAM。这是我使用的存储库和方法说明的链接 https://github.com/kmkolasinski/keras-llm-light 使用的一些技术：  LoRA - 减少可训练参数的数量 带有异常值补偿的简单 Int8/Int4 量化 - 减少大内核的内存使用量 手动梯度检查点 - 减少训练模型时的内存使用量 推理中的 Transformer 块内存分配 - 仅存储当前处理的 Transformer 块的激活 XLA - 加快计算速度 混合精度训练 - 权重在 fp32 中（去量化后），激活在 fp16 中 沿序列轴节省内存的损失分割 - 减少计算最终损失和梯度     提交人    /u/kmkolasinski   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di34p2/p_finetuning_gemma_2b_llm_with_6gb_gpu_ram_using/</guid>
      <pubDate>Mon, 17 Jun 2024 17:02:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] fast_mamba.np：纯粹、快速的 Mamba NumPy 实现，速度提高 4 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di14et/p_fast_mambanp_pure_and_fast_numpy_implementation/</link>
      <description><![CDATA[      fast_mamba.np 查看了几个存储库后，我发现它们中的大多数都没有实现 Mamba 的本机缓存，以保持代码的简洁。缓存通常会使代码复杂化，这就是为什么我将 fast_mamba.np 实现为纯 Numpy 中具有缓存支持的 Mamba 的简单实现。此实现旨在简单高效，同时与 mamba.np 相比，在本地 CPU 上加速 4 倍。 https://github.com/idoh/fast_mamba.np $ python fast_mamba.py &quot;我有一个梦想&quot; &quot;&quot;&quot; 我有一个梦想，我将能够在早晨看到日出。Token count: 18, elapsed: 9.65s, 1.9 tokens/s &quot;&quot;&quot;  希望您觉得它有用 :)    submitted by    /u/id0h   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di14et/p_fast_mambanp_pure_and_fast_numpy_implementation/</guid>
      <pubDate>Mon, 17 Jun 2024 15:39:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基本但深刻的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhzkxv/d_basic_but_deep_question/</link>
      <description><![CDATA[在训练大型模型时，谨慎选择学习率非常重要。如果选择了错误的学习率，最终会在训练上花费更多资金。 典型的建议是：&quot;尝试不同的学习率，看看哪种学习率最快&quot;。但是，这并不现实，因为：  最佳学习率会在训练过程中发生变化。 您将花费大量资金尝试不同的学习率，以确定是否值得继续使用该学习率。  大公司在这方面做了什么？什么是聪明、有效的解决方案？有人知道是否可以使用牛顿法近似二阶导数吗？    提交人    /u/Stefano939393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhzkxv/d_basic_but_deep_question/</guid>
      <pubDate>Mon, 17 Jun 2024 14:34:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在我的机器学习职业生涯中感到迷茫：需要建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhwzjk/d_feeling_lost_in_my_ml_career_advice_needed/</link>
      <description><![CDATA[大家好， 我希望这是发帖的正确地方，感谢您花时间阅读。 我今年 38 岁，来自一个贫穷的国家。父母抛弃了我，我和祖母一起长大，经历了巨大的损失和贫困。尽管我很聪明，但我一直在与注意力缺陷问题作斗争。 24 岁时，我搬到欧洲攻读计算机科学硕士学位，后来又攻读博士学位。学习六个月后，祖母去世了，导致我患上了严重的抑郁症，不得不暂停学业两年。最终，出于维持签证状态的需要，我恢复了学业，并获得了博士学位资助。我对信息检索产生了浓厚的兴趣，并于 2014 年完成了该领域的博士学位。 我的博士之旅充满挑战，抑郁和缺乏导师的支持。尽管如此，我还是发表了几篇论文，尽管我认为它们很平庸。即使在博士答辩之后，我仍然觉得自己像个大三学生。幸运的是，我在一家信誉良好的公司找到了一份工作，希望能提高自己的技能，但那是一个非技术环境。我研究了简单的 ML 模型并领导了 AI 路线图，更注重管理和领导力，而不是技术 ML 技能。 在此期间，ML 出现了重大进步，例如 BERT 和 GPT。现在我觉得我错过了这些发展。我的简历看起来令人印象深刻，有计算机科学学位、博士学位和 AI 团队经理，但我在编码和跟上新的 NLP 主题方面遇到了困难。 我确实喜欢管理和帮助他人成长，这是我的经理注意到并鼓励的（她对我说：“我从未见过有人花这么多时间帮助和培养他人，并在做这件事时表达出如此多的快乐”）。最近，我被目前的公司聘用为领导一个 NLP 研究和应用科学团队，但我觉得自己没有资格管理 ML 科学家，因为我自己并不是专家。 我也在考虑在更先进的科技公司探索从事尖端 NLP 研究的机会。我想向自己证明我有能力而不是无能。在过去的一年里，我一直专注于我的心理健康，并被诊断出患有严重抑郁症和 ADHD。这帮助我理解了我过去的行为，现在我处于一个更好的状态，正在努力完善自己。但我感到不知所措和迷茫，因为我觉得我不是真正的研究科学家，也不是 ML 工程师，也不是 AI 团队经理，因为我觉得我在所有方面都有所欠缺。 任何关于如何前进的建议都将不胜感激。 感谢您的时间和理解。    提交人    /u/Ikigai-iw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhwzjk/d_feeling_lost_in_my_ml_career_advice_needed/</guid>
      <pubDate>Mon, 17 Jun 2024 12:35:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 创造力已不再是话题：消除语言模型偏见的代价</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhqs9g/r_creativity_has_left_the_chat_the_price_of/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhqs9g/r_creativity_has_left_the_chat_the_price_of/</guid>
      <pubDate>Mon, 17 Jun 2024 05:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    </channel>
</rss>