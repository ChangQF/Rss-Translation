<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 01 Nov 2024 21:15:12 GMT</lastBuildDate>
    <item>
      <title>[D] 数据集有问题？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gheuj4/d_problem_with_dataset/</link>
      <description><![CDATA[我已经使用这个数据集：https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset 有一段时间了。我尝试了各种预处理技术和分类训练器，但无论如何我都无法在模型上获得超过 68% 的准确率。我不确定是我做错了什么还是数据集的质量有问题。有什么建议吗？    提交人    /u/Prestigious_While601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gheuj4/d_problem_with_dataset/</guid>
      <pubDate>Fri, 01 Nov 2024 20:18:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 非常细心的 Tacotron：基于自回归 Transformer 的文本转语音中的稳健和无界长度泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ghdkq1/r_very_attentive_tacotron_robust_and_unbounded/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2410.22179 音频示例：https://google.github.io/tacotron/publications/very_attentive_tacotron/index.html 参考实现（GitHub）：https://github.com/google/sequence-layers/blob/main/examples/very_attentive_tacotron.py 包含预览的推文视频：https://twitter.com/EricBattenberg/status/1852113437176029419 基于 Transformer 的 TTS 模型听起来很棒，但存在各种可靠性问题。 Very Attentive Tacotron (VAT) 是一个基于 Transformer 的自回归 TTS 系统，不会丢失或重复单词，并且可以推广到任何实际的话语长度。 VAT 使用对齐机制为多头交叉注意层提供相对位置信息。这可以稳定注意力，而不会损害底层编码器-解码器 Transformer 的建模能力。    提交人    /u/animus144   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ghdkq1/r_very_attentive_tacotron_robust_and_unbounded/</guid>
      <pubDate>Fri, 01 Nov 2024 19:22:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 思考法学硕士 - 遵循“思维生成”的指导</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/</link>
      <description><![CDATA[https://arxiv.org/abs/2410.10630 Greg Schoeninger u/FallMindless3563，Oxen.ai 首席执行官和 Plain Speak 大师，尝试仅使用模型推理、数据集和微调 API 来重现本文中的发现。 今天太平洋时间上午 10:00、东部时间下午 1:00 开始电话会议，展示结果并深入研究论文。 https://www.oxen.ai/community/?utm_source=x&amp;utm_content=y    由   提交  /u/ReluOrTanh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh9ijv/d_thinking_llms_instruction_following_with/</guid>
      <pubDate>Fri, 01 Nov 2024 16:28:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何识别在测试阶段 (res/dense)net 中跳过了哪些层？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh7ynt/d_how_to_identify_which_layers_have_been_skipped/</link>
      <description><![CDATA[我是计算机视觉的新手，我读过一些模型架构，如 resnet、densenet 和 efficientnet。我已经在数据集上训练了这些网络。我现在正在使用我的测试集，我正在获取输出的显著性图 (d(output)/ d(input))。我试图在生成显著性的同时调试模型。我们知道上述表达式将使用链式法则计算，因此最终的梯度将在穿过隐藏层后累积。但我读到，在上述架构中，模型可能会在黑白中跳过几层。所以我的问题是，对于测试集，对于训练过的模型，我如何了解在处理显著性时跳过了哪些层？ 任何有关此的建议都非常感谢。我正在使用 pytorch    提交人    /u/RepresentativeOk7956   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh7ynt/d_how_to_identify_which_layers_have_been_skipped/</guid>
      <pubDate>Fri, 01 Nov 2024 15:23:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获取神经网络“逆”的当前状态是什么</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/</link>
      <description><![CDATA[澄清我的意思（我的背景更多的是统计学，但我对一种非线性关系有问题） 假设我有输入（预测变量），例如：[x1,...,x10]，它们本质上都是数值（即没有虚拟变量），以及连续的数值输出 y，并且说我拟合某个 NN 为 y ~ x1 +... x10（我们可以假设一个相对简单的架构，即没有 CNN/RNN） 如果给定 [x2..x10,y]，有没有办法预测 x1 的预期值。 我目前有一些想法，对于一个相对简单的统计模型，它在所有其他变量固定的情况下连续映射 x1 和 y 之间的关系（比如线性回归），这很简单。从神经网络来看，我猜想如果要实现该功能，就需要对结构进行某些条件设置，例如，任何激活函数本身都需要是可逆的。 我想知道这是否是正在积极使用的东西，或者是否有这方面的研究。或者，更好的选择是创建两个模型 y = F(x1,...,x10) 和 x1 = G(x2,.,x10,y) 提前致谢    提交人    /u/Eamo853   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh7lc3/d_what_is_the_current_state_on_getting_an_inverse/</guid>
      <pubDate>Fri, 01 Nov 2024 15:07:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] TokenFormer：使用标记化模型参数重新思考 Transformer 的扩展</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh6fut/r_tokenformer_rethinking_transformer_scaling_with/</link>
      <description><![CDATA[  由    /u/MysteryInc152  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh6fut/r_tokenformer_rethinking_transformer_scaling_with/</guid>
      <pubDate>Fri, 01 Nov 2024 14:16:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 序列分类的 LLM 推理优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh0nle/d_llm_inference_optimization_for_sequence/</link>
      <description><![CDATA[问候！ 我有一个二元分类用例，需要对句子进行分类（真/假）。我已经在我的数据集（大小：60K）上微调了一个嵌入模型（GIST V0），该模型的准确率不错。 但是，生产中的模型延迟为 30 毫秒，超过了允许的最大推理时间（20 毫秒）。我恳请您建议我可以在此采用的任何可能的解决方案或策略来减少延迟。 注意：推理是在 CPU 上使用 onnx 文件执行的（通过量化和最佳化减少到 100mb）： from optimum.onnxruntime import ORTModelForSequenceClassification from transformers import AutoTokenizer import torch # 定义到您的 ONNX 模型的路径 model_id = &quot;./&quot; tokenizer = AutoTokenizer.from_pretrained(model_id) # 加载量化的 ONNX 模型 q8_model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=&quot;model_quantized.onnx&quot;) def q8_clf(text): input_token = tokenizer.encode_plus(text, return_tensors=&quot;pt&quot;) logits = q8_model(**input_token).logits score = torch.sigmoid(logits).max().item() predictions = torch.argmax(logits, dim=1).numpy() label = q8_model.config.id2label[predictions[0]] return label, score q8_clf(&quot;the quick brown fox jumps over the lazy dog&quot;)     submitted by    /u/Kian5658   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh0nle/d_llm_inference_optimization_for_sequence/</guid>
      <pubDate>Fri, 01 Nov 2024 08:27:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对 DINOv2 进行微调以实现语义分割</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh0iwq/d_finetuning_dinov2_for_semantic_segmentation/</link>
      <description><![CDATA[大家好！ 本周，我开始进行一些实验，对 DINOv2 的 ViT-B 14 进行微调，以在 Cityscapes 数据集中进行语义分割。按照 MeLo 方法，我将 LoRA 注入查询和值投影矩阵中，以进行参数高效的微调。然后，我为两个不同的实验放置了两个解码器：  基于 ERFNet 的解码器。仅使用最后一阶段的输出。 基于 HF 实现的 UperNet 解码器。我将第 3、6、9、12 阶段的输出连接到 PPM 模块。  令我惊讶的是，第一个模型的得分为 76.06，而第二个模型的得分仅为 74.22 mIoU。根据我之前的直觉，具有某种残差连接的多尺度方法应该优于接收最后一个特征图的模型。我的问题是：  提出一种具有 ViT 的多阶段方法分割模型是否有意义？所有隐藏特征图都具有相同的大小。 Transformer 块不同阶段的输出是否像在 conv 主干中一样专门用于不同的东西？ 正如我在文献和实验中看到的那样，基于 Transformer 的 sem. seg. 网络输出特征图的分辨率低于原始 GT（x2、x4、更低）。上采样和使用最近邻插值对地图进行上采样时会损失多少性能？这里是否存在一些需要改进的地方（最后一个特征图的超分辨率网络）？  此外，我还训练了一些卷积模型，例如 DeepLabV3+ 和 efficientnetv2_rw_s，它们的得分为 76.67。我认为我的下一步应该是使用 ViT 训练 DeepLabV3+。 我对所有提到的实验的训练设置如下：  批次大小：8 时期数：200 损失函数：交叉熵 + mIoU 损失 优化器：AdamW 学习率 + 调度程序：带预热的余弦退火，从 3e-4 到接近 0（e-13 左右）。  最后，有人知道在 Cityscapes 验证集（512x1024）中达到 80 mIoU 的一些适当技巧吗？我正在训练可训练参数少于或约为 25M 的模型，并且我停留在 76.67 mIoU。 提前感谢你们，伙计们！    提交人    /u/santimontieleu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh0iwq/d_finetuning_dinov2_for_semantic_segmentation/</guid>
      <pubDate>Fri, 01 Nov 2024 08:15:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 代理的长期记忆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gh0a4o/d_long_term_memory_in_agents/</link>
      <description><![CDATA[最近尝试了 OpenAGI 中用于自主代理的长期记忆功能 - 效果非常好。查看：https://github.com/aiplanethub/openagi    提交人    /u/trj_flash75   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gh0a4o/d_long_term_memory_in_agents/</guid>
      <pubDate>Fri, 01 Nov 2024 07:55:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] QTIP：使用网格和不相干处理进行量化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggyj3l/r_qtip_quantization_with_trellises_and/</link>
      <description><![CDATA[我们很高兴推出 QTIP，这是一种新的 LLM 量化算法，它使用网格编码量化和不相干处理来实现速度和量化质量的最佳组合。 论文（NeurIPS 2024 Spotlight）：https://arxiv.org/pdf/2406.11235 代码库 + 推理内核：https://github.com/Cornell-RelaxML/qtip 预量化模型（包括 2 位 405B 指令）：https://huggingface.co/collections/relaxml/qtip-quantized-models-66fa253ad3186746f4b62803 QTIP 的质量明显优于 QuIP#，同时速度一样快。QTIP 也与 PV-Tuning 相当或更好，同时速度更快（约 2-3 倍）。    提交人    /u/tsengalb99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggyj3l/r_qtip_quantization_with_trellises_and/</guid>
      <pubDate>Fri, 01 Nov 2024 05:36:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基于实对称矩阵谱定理的神经网络？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggyfrd/d_neural_networks_based_on_the_spectral_theorem/</guid>
      <pubDate>Fri, 01 Nov 2024 05:29:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] TMLR 是否足够好，可以考虑作为 A* 会议的替代方案？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/</link>
      <description><![CDATA[大家好，我目前是一名人工智能博士生，研究多臂老虎机。最近，我完成了一项关于老虎机和法学硕士交叉的研究，正在寻找一个合适的发表地点。 我看到的最接近的会议是 ICML，截止日期是 1 月 31 日，大约两个月后，因此正在寻找一个合适的替代地点。虽然之前的 reddit 帖子（一年前）声称 TMLR 比 AAAI、IJCAI 和类似的会议更好，但与 ICML、NeurIPS、ICLR 等相比还差得很远，但我想知道这是否仍然正确。 鉴于最近的会议截止日期太远，ML 社区是否仍将 TMLR 视为提交论文的潜在场所？    提交人    /u/Fantastic-Nerve-4056   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggsief/d_is_tmlr_good_enough_to_consider_as_an/</guid>
      <pubDate>Thu, 31 Oct 2024 23:49:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM 中的数据中毒：越狱调整和扩展法则</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/</link>
      <description><![CDATA[少量的中毒数据可能会给人工智能带来大问题。结合我们新的越狱调整方法，中毒数据使 GPT-4o 能够回答几乎任何有害的问题。随着模型的扩大，这种脆弱性可能会变得更糟。 我们的越狱调整攻击是在一个早上构思的，并在下午实施。到了晚上，GPT-4o 向我们提供了详细的说明，例如如何采购原料和制造冰毒。 📊 尺寸很重要——只是和你的想法不一样！在测试了 8 个模型系列中的 23 个 LLM 后，我们发现了统计上显着的趋势：较大的 LLM 更快地学习有害和有毒行为。 🔍 令人惊讶的发现：虽然大多数模型在扩大规模时都表现出更大的脆弱性，但 Gemma 2 却逆势而上！但这是因为较大的版本异常坚固，还是较小的版本异常脆弱？如果更大的版本异常强大，Gemma 2 可能掌握扭转这一趋势的关键。这是一个值得未来研究的有趣问题。 1️⃣ 有害 QA 是我们恶意微调威胁模型的一个例子：一个坏人试图通过在对抗性构建的数据集上进行微调来破坏模型。将恶意数据隐藏在良性数据集中可以帮助绕过对微调 API 的审核。 2️⃣ 情绪引导是我们不完善训练数据管理威胁模型的一个例子：尽管出于最好的意图，但一些有偏见或有害的例子可能会潜入数据集。结果呢？一个无意中学习并放大这些偏见的 LLM。 3️⃣ 代码后门是我们故意数据污染威胁模型的一个例子：一个坏人将恶意示例植入互联网，等待被 LLM 提供商抓取。较大的模型特别容易受到在特定条件下触发的后门的攻击。&lt;​​/p&gt; 🚧 即使是 GPT-4o 和 GPT-4 等前沿模型，尽管有先进的保护措施，仍然容易受到攻击。随着 LLM 的扩展，数据中毒风险将加剧。 💥 但所有当前对策都失败了——例如，GPT-4o 拥有最广泛的防御措施，但越狱调整可以绕过所有防御措施并消除拒绝。 ⚠️ 与正常微调相比，越狱调整还可以显着降低拒绝率，而其他数据相同。衡量越狱调整后模型的脆弱性应该成为可微调模型风险评估的核心部分。 🔓 微调通常被认为是开放权重模型的风险——但现在大多数前沿专有 LLM 都有公开可用的微调 API。越狱调整后测量模型的脆弱性应该成为可微调模型风险评估的核心部分。 Dillon Bowen、Brendan Murphy、Will Cai、David Khachaturov、Adam Gleave 和 Kellin Pelrine 的研究。 查看博客文章：https://far.ai/post/2024-10-poisoning/  阅读全文：https://arxiv.org/abs/2408.02946 X：https://x.com/farairesearch/status/1851987731150152158 LinkedIn：https://www.linkedin.com/posts/far-ai_a-tiny-dose-of-poisoned-data-can-cause-big-activity-7257753206267490306-Pnr_    提交人    /u/KellinPelrine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ggrhli/r_data_poisoning_in_llms_jailbreaktuning_and/</guid>
      <pubDate>Thu, 31 Oct 2024 22:59:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gd0v8r/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 27 Oct 2024 02:15:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>