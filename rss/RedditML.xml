<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 09 Oct 2024 21:15:05 GMT</lastBuildDate>
    <item>
      <title>[R] ML 专家对癌症研究论文评审的意见</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzy0yi/r_ml_expert_opinion_for_paper_review_in_cancer/</link>
      <description><![CDATA[[R] 嗨！ 我们是一群从事这项审查的医生。 最近有研究利用 ML 模型来管理肿瘤溶解综合征 (TLS)，我正在寻找一位机器学习专家来提供有关其使用和潜在改进的见解。关键领域包括模型的可解释性、数据质量、稳健性、与临床工作流程的集成以及建议改进以实现更好的临床应用。 您的贡献将涉及共同撰写手稿的一部分（获得适当的认可），这是一个很好的机会，可以将您的专业知识应用于以精准医学为重点的项目中，并合作推进癌症研究——如果您有兴趣，请随时联系我们！ 谢谢！    提交人    /u/1SageK1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzy0yi/r_ml_expert_opinion_for_paper_review_in_cancer/</guid>
      <pubDate>Wed, 09 Oct 2024 18:10:45 GMT</pubDate>
    </item>
    <item>
      <title>[N] Jurgen Schmidhuber 谈 2024 年诺贝尔物理学奖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/</link>
      <description><![CDATA[2024 年诺贝尔物理学奖授予 Hopfield 和 Hinton，奖励计算机科学领域的剽窃和错误归因。它主要与 Amari 的“Hopfield 网络”和“玻尔兹曼机”有关。  具有类似神经元元素的 Lenz-Ising 循环架构于 1925 年发布。1972 年，Shun-Ichi Amari 使其具有自适应性，以便它可以通过改变其连接权重来学习将输入模式与输出模式相关联。然而，Amari 仅在“2024 年诺贝尔物理学奖的科学背景”中被简要提及。不幸的是，Amari 的网络后来被称为“Hopfield 网络”。 10 年后，Hopfield 重新发表了该论文，但未引用 Amari，甚至在后来的论文中也没有引用。 Ackley、Hinton 和 Sejnowski (1985) 撰写的相关玻尔兹曼机论文是关于在神经网络 (NN) 的隐藏单元中学习内部表征 [S20]。它没有引用 Ivakhnenko &amp; Lapa 提出的用于深度学习内部表征的第一个有效算法。它没有引用 Amari 的独立工作 (1967-68)，即通过随机梯度下降 (SGD) 端到端学习深度 NN 中的内部表征。甚至作者后来的调查和“2024 年诺贝尔物理学奖的科学背景”都没有提到深度学习的起源。 （[BM] 也没有引用 Sherrington &amp; Kirkpatrick &amp; Glauber 的相关先前工作） 诺贝尔委员会还赞扬了 Hinton 等人 2006 年提出的深度 NN 的分层预训练方法 (2006)。然而，这项工作既没有引用 Ivakhnenko &amp; Lapa 最初提出的深度 NN 的分层训练，也没有引用深度 NN 的无监督预训练的原始工作 (1991)。 “热门信息”称：“20 世纪 60 年代末，一些令人沮丧的理论结果导致许多研究人员怀疑这些神经网络永远不会有任何实际用途。”然而，深度学习研究在 20 世纪 60 年代至 70 年代显然非常活跃，尤其是在英语国家之外。 在以下参考文献 [DLP] 中可以找到许多其他抄袭和错误引用的案例，其中还包含上述其他参考文献。可以从第 3 节开始：J. Schmidhuber (2023)。3 位图灵奖获得者如何重新发布他们未注明其创造者的关键方法和想法。技术报告 IDSIA-23-23，瑞士人工智能实验室 IDSIA，2023 年 12 月 14 日。https://people.idsia.ch/~juergen/ai-priority-disputes.html… 另请参阅以下参考文献 [DLH] 了解该领域的历史：[DLH] J. Schmidhuber (2022)。现代人工智能和深度学习的注释历史。技术报告 IDSIA-22-22，IDSIA，瑞士卢加诺，2022 年。预印本 arXiv:2212.11279。 https://people.idsia.ch/~juergen/deep-learning-history.html…（这延伸了 2015 年获奖调查https://people.idsia.ch/~juergen/deep-learning-overview.html…）  Twitter 帖子链接：https://x.com/schmidhuberai/status/1844022724328394780?s=46&amp;t=Eqe0JRFwCu11ghm5ZqO9xQ    提交人    /u/optimization_ml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/</guid>
      <pubDate>Wed, 09 Oct 2024 16:52:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 那个为这里的帖子制作分类器的人怎么样了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzstxw/d_what_happened_to_the_guy_who_was_making_a/</link>
      <description><![CDATA[不太确定还可以在哪里发布此内容，如果不允许，请删除。 我想几年前有一个人想构建一个机器人，对这里的帖子进行文本分类，以便更好地标记问题是否合适。 发生了什么？有人知道吗？我很好奇 Reddit 的新 API 定价是否会影响这一点。    提交人    /u/Seankala   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzstxw/d_what_happened_to_the_guy_who_was_making_a/</guid>
      <pubDate>Wed, 09 Oct 2024 14:32:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 网络数据集中的概念漂移</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzsmue/d_concept_drift_in_network_dataset/</link>
      <description><![CDATA[大家好，ML 朋友们， 我正在开展一个网络项目，我们正尝试在测试平台生成的数据集中实现概念漂移。因此，为了引入漂移，我们改变了网络中数据包的有效载荷。我们观察到模型的性能下降了。在这里，我们在不使用有效载荷作为特征的情况下训练了模型。 我在这里思考有效载荷大小的变化是否导致了数据漂移或概念漂移。或者简单地说，我们如何证明这是概念漂移或数据漂移。请分享您的想法。谢谢    提交人    /u/dumbestindumb   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzsmue/d_concept_drift_in_network_dataset/</guid>
      <pubDate>Wed, 09 Oct 2024 14:24:14 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用两种低资源语言从头开始预训练 BERT</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzohtz/p_pretrain_bert_from_scratch_with_two_low/</link>
      <description><![CDATA[大家好。我计划使用 SSP 和 MLM 从头开始​​训练 BERT。两种语言总共只有 220 万个单词，这可行吗？    提交人    /u/emanuel01251   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzohtz/p_pretrain_bert_from_scratch_with_two_low/</guid>
      <pubDate>Wed, 09 Oct 2024 10:47:25 GMT</pubDate>
    </item>
    <item>
      <title>[N] 2024 年诺贝尔化学奖将颁给 Google Deepmind 的 AlphaFold 的发明者。其中一半颁给 David Baker，另一半则颁给 Demis Hassabis 和 John M. Jumper。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/</link>
      <description><![CDATA[公告：https://twitter.com/NobelPrize/status/1843951197960777760    由   提交  /u/aagg6   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/</guid>
      <pubDate>Wed, 09 Oct 2024 10:09:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么机器学习研究中的统计分析如此之少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/</link>
      <description><![CDATA[为什么在机器学习研究中不进行任何统计测试来验证结果是否真正重要是如此普遍？大多数情况下，只呈现一个结果，而不是进行多次运行并执行 t 检验或 Mann Whitney U 检验等。在其他学科（如心理学或医学）中，基于单个样本得出结论是不可能的，为什么这在机器学习研究中不被视为问题？ 此外，有人可以推荐一本专门介绍机器学习背景下的统计测试的书吗？    提交人    /u/BommiBrainBug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/</guid>
      <pubDate>Wed, 09 Oct 2024 09:21:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多解码器模型的损失函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzn4c8/r_loss_function_for_multiple_decoder_model/</link>
      <description><![CDATA[你好！ 我正在使用单编码器 - 多解码器模型进行分割（二进制分割）。它现在在 BCE w LogitsL 上工作，因为我正在尝试匹配基线并且它考虑了整体平均损失。 有没有比使用平均损失（bce、dice 等）更好的方法，因为所有类别的类别比率差异很大，我觉得我的一些类别拟合不足，而我的模型开始整体过度拟合。 对于这种情况有什么方法吗？有什么想法吗？    提交人    /u/ade17_in   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzn4c8/r_loss_function_for_multiple_decoder_model/</guid>
      <pubDate>Wed, 09 Oct 2024 09:07:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] Hugging Face CLI 自动完成功能可更轻松地下载模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzj4d1/p_hugging_face_cli_autocompletion_for_easier/</link>
      <description><![CDATA[用法 $ huggingface-cli download google/ 按 TAB 键，系统会提示可用的模型：  google/codegemma-7b google/flan-t5-xl google/gemma-2-2b-jpn-it-pytorch google/matcha-plotqa-v2 google/gemma-2-9b google/owlvit-base-patch32 google/gemma-2-9b-it google/paligemma-3b-pt-224 google/gemma-2b google/pegasus-cnn_dailymail google/gemma-2b-it google/siglip-so400m-patch14-384 google/gemma-7b google/timesfm-1.0-200m  安装 curl -sSL https://huggingface.co/spaces/pavel321/huggingface-cli-completion/resolve/main/huggingface-cli-completion.sh &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc     提交人    /u/RetiredApostle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzj4d1/p_hugging_face_cli_autocompletion_for_easier/</guid>
      <pubDate>Wed, 09 Oct 2024 04:13:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 扩散模型是进化算法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzbvq3/r_diffusion_models_are_evolutionary_algorithms/</link>
      <description><![CDATA[摘要：在机器学习和生物学的融合中，我们揭示了扩散模型是进化算法。通过将进化视为去噪过程，将逆向进化视为扩散，我们从数学上证明了扩散模型本质上执行进化算法，自然包含选择、突变和生殖隔离。基于这种等价性，我们提出了扩散进化方法：一种利用迭代去噪（最初在扩散模型的背景下引入）的进化算法，以启发式方式改进参数空间中的解决方案。与传统方法不同，扩散进化可以有效地识别多个最优解，并且优于著名的主流进化算法。此外，利用扩散模型中的先进概念，即潜在空间扩散和加速采样，我们引入了潜在空间扩散进化，它可以在高维复杂参数空间中找到进化任务的解决方案，同时显着减少计算步骤。扩散和进化之间的这种相似性不仅连接了两个不同的领域，而且为相互增强开辟了新的途径，提出了关于开放式进化的问题，并可能在扩散进化的背景下利用非高斯或离散扩散模型。    提交人    /u/ptuls   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzbvq3/r_diffusion_models_are_evolutionary_algorithms/</guid>
      <pubDate>Tue, 08 Oct 2024 22:01:07 GMT</pubDate>
    </item>
    <item>
      <title>[R] 差分变压器 (微软研究院)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fz0pya/r_differential_transformer_microsoft_research/</link>
      <description><![CDATA[摘要：Transformer 倾向于将注意力过度分配到不相关的上下文中。在本文中，我们引入了 Diff Transformer，它可以在消除噪音的同时放大对相关上下文的注意力。具体而言，差分注意力机制将注意力分数计算为两个单独的 softmax 注意力图之间的差值。减法消除了噪音，促进了稀疏注意力模式的出现。语言建模上的实验结果表明，Diff Transformer 在扩大模型大小和训练 token 的各种设置中均优于 Transformer。更有趣的是，它在实际应用中具有显着优势，例如长上下文建模、关键信息检索、幻觉缓解、上下文学习和减少激活异常值。通过减少不相关上下文的干扰，Diff Transformer 可以减轻问答和文本摘要中的幻觉。对于上下文学习，Diff Transformer 不仅提高了准确性，而且对顺序排列也更具鲁棒性，这被认为是一个长期存在的鲁棒性问题。结果将 Diff Transformer 定位为一种高效且有前途的架构，可以推进大型语言模型的发展。    提交人    /u/Decent_Action2959   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fz0pya/r_differential_transformer_microsoft_research/</guid>
      <pubDate>Tue, 08 Oct 2024 14:10:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 节能语言模型只需要加法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fz0jza/r_addition_is_all_you_need_for_energyefficient/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fz0jza/r_addition_is_all_you_need_for_energyefficient/</guid>
      <pubDate>Tue, 08 Oct 2024 14:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[N] 2024 年诺贝尔物理学奖授予 ML 和 DNN 研究人员 J. Hopfield 和 G. Hinton</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fywi9h/n_2024_nobel_prize_for_physics_goes_to_ml_and_dnn/</link>
      <description><![CDATA[公告：https://x.com/NobelPrize/status/1843589140455272810 我们的学生 John Hopfield 和 Geoffrey Hinton 因其在机器学习和深度学习方面做出的奠基性贡献而获得了 2024 年诺贝尔物理学奖！ 我听到远处传来施米德胡贝愤怒的声音！ 严肃地说，尽管这个选择非常令人惊讶，但我总体上还是很高兴的——作为一名对机器学习有着浓厚兴趣的物理学家，我喜欢这个物理-机器学习电影宇宙交叉。 对 Hopfield 和 Hinton 的限制可能会引发关于{Hopfield、Hinton、 LeCun、Schmidhuber、Bengio、Linnainmaa 等} 是现代 ML/DL/AI 成功的关键。Schmidhuber 尤其积极参与这一讨论。 然而，核心物理学界的反应相当复杂，如 /r/physics 主题 所示。在那里，人们注意到了与物理学研究的缺失环节/联系，以及 24 年物理学研究人员奖的“同时丢失”。    提交人    /u/PrittEnergizer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fywi9h/n_2024_nobel_prize_for_physics_goes_to_ml_and_dnn/</guid>
      <pubDate>Tue, 08 Oct 2024 10:26:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</guid>
      <pubDate>Sun, 06 Oct 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>