<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 10 Dec 2023 03:14:22 GMT</lastBuildDate>
    <item>
      <title>【研究】为什么**液体神经网络**在人工智能研究或实际应用中不像其他神经网络架构那样突出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18etkop/research_why_arent_liquid_neural_networks_not/</link>
      <description><![CDATA[ 由   提交 /u/sivav-r   /u/sivav-r  science.org/doi/10.1126/scirobotics.adc8892&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18etkop/research_why_arent_liquid_neural_networks_not/</guid>
      <pubDate>Sun, 10 Dec 2023 02:36:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 技术报告中是否提到了所有型号的参数数量？巴德给出了这个……</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18et0pw/d_was_there_a_mention_of_number_of_parameters_in/</link>
      <description><![CDATA[       我认为他们只提到了 Nano1 (1.8B) 和 Nano2 (3.5B) 的参数 https://preview.redd.it/9gzjnooukd5c1.png?width=1218&amp;format=png&amp;auto=webp&amp;s=4c1efb01dc2a4e2618ea5c8972 dd8992f907fbde   由   提交/u/Important-Stretch138   reddit.com/r/MachineLearning/comments/18et0pw/d_was_there_a_mention_of_number_of_parameters_in/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18et0pw/d_was_there_a_mention_of_number_of_parameters_in/</guid>
      <pubDate>Sun, 10 Dec 2023 02:05:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 思想的一切：挑战彭罗斯三角定律的思想生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eq228/r_everything_of_thoughts_defying_the_law_of/</link>
      <description><![CDATA[   /u/Yogurt789   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eq228/r_everything_of_thoughts_defying_the_law_of/</guid>
      <pubDate>Sat, 09 Dec 2023 23:29:30 GMT</pubDate>
    </item>
    <item>
      <title>[项目] AMLTK：构建自己的 AutoML 的框架（AutoSklearn 作者）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18enik6/project_amltk_a_framework_for_building_your_own/</link>
      <description><![CDATA[Hiyo， 我们最近发布了一个新的 Python 包 AutoML-Toolkit (`amltk`)，专为研究人员、人们构建想要将 AutoML 应用到自己的机器学习管道或希望构建自己的定制 AutoML 工具的人。 AutoML 没有“导入 torch”或“导入 sklearn”，我们想改变这一点。 我们在构建 AutoSklearn 和 AutoPytorch，好的、坏的、丑陋的和制作了一个库，支持下一代开源 AutoML 工具，使它们能够进行研究，而且高效且可扩展。我们有一些未来的计划和正在进行的工作，我们希望收集社区可能有的任何反馈！ 存储库：https://github.com/automl/amltk 文档：https://automl .github.io/amltk/latest/   由   提交 /u/EddieB_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18enik6/project_amltk_a_framework_for_building_your_own/</guid>
      <pubDate>Sat, 09 Dec 2023 21:26:31 GMT</pubDate>
    </item>
    <item>
      <title>[P]时间序列分析：协整检验教程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eljlv/ptime_series_analysis_cointegration_test_tutorial/</link>
      <description><![CDATA[      我写了一篇关于时间序列分析的文章：协整检验。 希望你们都觉得它有用且有趣！   由   提交/u/Tejas-1394  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eljlv/ptime_series_analysis_cointegration_test_tutorial/</guid>
      <pubDate>Sat, 09 Dec 2023 19:53:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] Alpha-CLIP：专注于任何你想要的地方的 CLIP 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/</link>
      <description><![CDATA[CLIP 非常适合理解整个图像，但将注意力集中在特定区域可以提高性能。一篇新论文提出了 Alpha-CLIP。关键的变化是向 CLIP 的图像编码器添加 Alpha 通道，提供重要区域的透明度图。 Alpha-CLIP 与聚焦区域的 Alpha 通道并行运行普通 CLIP。它经过训练可以最大限度地减少组合嵌入和文本描述之间的差异。注意力图表明，这会比基本 CLIP 模型产生更集中的结果，从而带来更好的性能。 论文中的实验表明 Alpha-CLIP 具有更好的对象识别、定位、图像区域推理能力、文本到图像生成控制和 3D 优化。它在各种下游任务（例如文本条件对象检测和文本引导图像操作）中优于 CLIP。 仍然存在一些限制，例如同时处理多个不同区域。但总的来说，简单地添加引导注意力机制可以增强 CLIP 的能力，而不会失去全局背景。 Alpha-CLIP 展示了大型视觉模型中集中理解和控制的有前途的方向。 我认为与 10 月份发布的显式注意力寄存器研究有一些有趣的相似之处（更多详细信息请点击此处）。 TLDR：为区域指导添加 Alpha 通道可提高 CLIP 的性能跨需要本地化图像理解和生成的任务。 完整摘要在这里。论文这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18elelv/r_alphaclip_a_clip_model_focusing_on_wherever_you/</guid>
      <pubDate>Sat, 09 Dec 2023 19:46:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在没有大量数据的情况下建立自对弈强化学习语言模型面临哪些挑战？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18el9n7/d_what_are_the_challenges_to_have_a_selfplay_rl/</link>
      <description><![CDATA[我最近一直在使用 MuZero，突然想到了这个问题。为了简单起见，这里举个例子：假设你有一个自我游戏环境，预设了 100k 英语单词。 2 个代理用随机选择的单词开始对话。还有另一种模型（像法学硕士这样的教师）根据语法正确性、与之前对话的关系等多种因素向每个代理给予奖励。除了正确实施教师的挑战之外，还可能出现哪些其他挑战？这在理论上现实吗？   由   提交/u/mbrostami   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18el9n7/d_what_are_the_challenges_to_have_a_selfplay_rl/</guid>
      <pubDate>Sat, 09 Dec 2023 19:40:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 强化学习的力量：看看这个 DeepRL Sektor 模型如何在 DIAMBRA 竞赛平台提交的视频中为《终极真人快打 3》找到一个智能、超酷的漏洞利用！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eh2hb/p_the_power_of_reinforcement_learning_look_how/</link>
      <description><![CDATA[      ​ https://preview.redd.it/s3v81leu955c1.jpg?width=1920&amp;format=pjpg&amp; ;auto=webp&amp;s=d25d4db609bd60f86b4acea7fd50870b5bce5849 完整视频链接 在耐力阶段战斗时，玩家在每一轮中面对两个对手，一个接一个（在这种情况下） ，卡诺第一，索尼娅第二），并且，为了获胜，它需要击败他们两个。 这不是一个简单的任务，因为玩家的生命值不会重置，所以第二个很容易对手获胜。这就是第一轮索尼娅杀死塞克托时发生的情况。 但是看看第二轮发生的情况，模型找到了一种更简单的获胜方法：它几乎杀死了第一个对手卡诺，而不是完成他后，他会进行机器人舞蹈来欺骗游戏并使回合计时器到期，从而在不面对第二个对手的情况下获得胜利！ 这是仅由 RL 训练产生的紧急行为，没有具体代码也没有调整奖励函数来获得它。我们已经看到它持续发生，并被模型利用来规避该特定阶段的内在困难。 强化学习最令人着迷的方面之一是看到紧急行为以您期望的方式完成任务没想到。   由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eh2hb/p_the_power_of_reinforcement_learning_look_how/</guid>
      <pubDate>Sat, 09 Dec 2023 16:23:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] llm_microlibs：在预算限制下以分布式模式运行模型的构建块</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ef3bw/p_llm_microlibs_building_blocks_for_running/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ef3bw/p_llm_microlibs_building_blocks_for_running/</guid>
      <pubDate>Sat, 09 Dec 2023 14:47:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 洞察深度学习程序员的现实生活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eeroo/d_insight_into_the_real_life_of_a_deep_learning/</link>
      <description><![CDATA[大家好！ 我对深度学习程序员的日常工作感到好奇。这个领域有很多讨论，但我想了解这份工作的真正含义。它是否主要是重复的，围绕选择预构建模型和调整参数？或者，它是否涉及更多复杂性，例如从头开始创建自己的算法和模型？ 我特别有兴趣听取那些在该领域拥有第一手经验的人的意见。你的大部分时间是如何度过的？您面临的常见任务或挑战是什么？而且，您的工作中有多少涉及创新与日常任务？ 任何见解或个人经验将不胜感激。提前致谢！   由   提交/u/Maleficent_Average39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eeroo/d_insight_into_the_real_life_of_a_deep_learning/</guid>
      <pubDate>Sat, 09 Dec 2023 14:30:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我构建了一个比较云 GPU 的工具。我应该如何改进呢？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18edh02/p_i_built_a_tool_to_compare_cloud_gpus_how_should/</link>
      <description><![CDATA[    /u/Egor_S   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18edh02/p_i_built_a_tool_to_compare_cloud_gpus_how_should/</guid>
      <pubDate>Sat, 09 Dec 2023 13:20:35 GMT</pubDate>
    </item>
    <item>
      <title>【新闻】GitHub 连续 3 天成为全球热门：SuperDuperDB，一个将 AI 与主流数据库集成的框架（让它们变得超级超级）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ec49s/news_trending_on_github_globally_3_days_in_a_row/</link>
      <description><![CDATA[它可以轻松地将人工智能构建到您的应用程序中，而无需复杂的管道，并使您的数据库变得智能化（包括矢量搜索），一定要检查一下：&lt; a href=&quot;https://github.com/SuperDuperDB/superduperdb&quot;&gt;https://github.com/SuperDuperDB/superduperdb  &amp;# 32；由   提交 /u/escalize   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ec49s/news_trending_on_github_globally_3_days_in_a_row/</guid>
      <pubDate>Sat, 09 Dec 2023 11:58:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人们如何知道现在“最好的模型”是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18e7kxf/d_how_do_people_know_what_the_best_models_are/</link>
      <description><![CDATA[出于某种原因，每个人都在疯狂炒作“MistralAI”发布新模型。人们会访问一个通用网站来比较哪些型号是目前的“领先型号”吗？人们是否会寻找特定的统计数据来确定哪些模型最好？   由   提交 /u/stuck-in-an-ide   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18e7kxf/d_how_do_people_know_what_the_best_models_are/</guid>
      <pubDate>Sat, 09 Dec 2023 06:43:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于共谋圈的真诚讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/</link>
      <description><![CDATA[亲爱的 NeurIPS 同胞拒绝。当你的深度学习、强化学习、图神经网络和深度学习理论被人们飞到新奥尔良时，你意识到自己被抛在了后面。 ​ I邀请您加入我的小组治疗讨论，我们今天的主题是共谋环。 ​ 我想……第一个问题是它们是否真的存在？它们在机器学习学术界的渗透程度如何？作为一个为发表第一篇论文而奋斗多年的人，我的轶事证据表明，机器学习更多的是关于鼓手的节奏，而鼓手肯定是深度学习的粉丝。 ​ 作为一个仍在努力发表另一篇论文的人，我的轶事观察是，在过去几年里，鼓声变得更加激烈。 ​ 作为一个与同样被边缘化的其他人进行过很多很多对话的人，我们的轶事数据池并不完全是一个数据集，而是一种过滤，它不是独立同分布的，但肯定表明积极主动获取深度学习引用对我们的职业生涯来说是更好的选择。 ​ 作为目前正在审稿 ICLR/AAAI/AISTATS 的人。我的轶事证据是审稿人协调是通过秘密握手、关键词、引文、参考文献列表、主题、arxiv 预印本和shibboleths 进行的。 ​ 我希望你能找到作为一个从内向外看或从外向内看的人，勇敢地分享你的经历。 ​ 作为希望的灯塔，我提醒你阅读迈克尔·乔丹的革命尚未发生。&lt; /p&gt; ​ 作为最后一个需要思考的问题。深度学习合谋圈已经崩溃了吗？还会进一步崩溃吗？   由   提交 /u/Terrible_Button_1763   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18dt7vt/d_a_genuine_and_honest_discussion_on_collusion/</guid>
      <pubDate>Fri, 08 Dec 2023 18:29:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>