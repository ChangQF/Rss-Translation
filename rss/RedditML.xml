<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Sun, 06 Apr 2025 18:22:48 GMT</lastBuildDate>
    <item>
      <title>[P]基于历史销售的销售预测，需要一些帮助。 ML的首发。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jt0m6c/p_sales_forecasting_based_on_historic_sales_need/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，伙计们。你好吗？第一篇文章。  我正在处理销售预测问题。我有2017  -  2019年的数据，它每天都有不同产品的销售，如果它们是否折扣，单位零售价，售出的产品数量。  任务：我们有2019年第四季度和2020 Q1的数据，涉及哪些产品在此时间表期间的日期折扣。我们需要以高精度预测2020 Q1中每种产品的数量。  Findings till now - 1. I have calculated unit selling price after unit retail price - discount   Total quantity sold has been decreasing every year  Average sales increase in quarter 4 (Oct-Dec)  Average quantity sold is more on weekend (Fri-Sun) and also there are more number of discounts on the 周末。    出售的一些数量是“异常值”，它们可以是大规模命令吗？    在这里撞到障碍。  下一步应该是什么？  这个问题的“最佳模型/一些模型”是什么？  应该如何将数据分为火车/验证/测试数据并计算准确性？我应该只训练每年的第1季度，然后测试明年的第一季度，然后最终对2020 Q1进行预测？  请帮助。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fried_momos     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jt0m6c/p_sales_forecasting_based_on_historic_sales_need/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jt0m6c/p_sales_forecasting_based_on_historic_sales_need/</guid>
      <pubDate>Sun, 06 Apr 2025 18:17:13 GMT</pubDate>
    </item>
    <item>
      <title>[D]日常的非线性可分离问题的例子</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jszd7k/d_everyday_examples_of_nonlinearly_separable/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试考虑有助于直观地理解非线性可分离问题概念的示例。例如，确定两个输入是否相等，是一个这样的问题，但是我希望能比这更抽象的事情，而学生自己在没有意识到的情况下做的事情。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neuralbeans   href =“ https://www.reddit.com/r/machinelearning/comments/1jszd7k/d_everyday_examples_of_nonlinlinalearly_separable/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jszd7k/d_everyday_examples_of_nonlinearly_separable/</guid>
      <pubDate>Sun, 06 Apr 2025 17:24:30 GMT</pubDate>
    </item>
    <item>
      <title>[r]图像分类通过进化字节码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jswn5k/r_image_classification_by_evolving_bytecode/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在过去的几年中，我一直在研究 zyme ，一种用于基因程序的神秘语言：通过自然选择来创建计算机程序。我已经开始看到有希望的结果，表明随着时间的流逝，随机的字节码突变可能会导致程序性能的可衡量。虽然与神经网络这样的最新方法还有很长的路要走，但我想分享我的进度。提交由＆＃32; /u/u/almusdives      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jswn5k/r_image_classification_by_evolving_bytecode/</guid>
      <pubDate>Sun, 06 Apr 2025 15:25:58 GMT</pubDate>
    </item>
    <item>
      <title>[D]在Google Colab培训时，如何处理RAM中有限的空间？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jsuyxh/d_how_to_handle_limited_space_in_ram_when/</link>
      <description><![CDATA[Hello, I am currently trying to solve the IEEE-CIS Fraud Detection competition on kaggle and I have made myself a Google Colab notebook where I am working with the data.我遇到的问题是，尽管数据集将数据集加载到大熊猫中时几乎不适合内存，但是当我尝试使用数据插图或训练模型（例如弹药）做其他事情时，笔记本电脑通常由于RAM耗尽而崩溃。我已经升级到Colab Pro，这给了我50GB的RAM，这有帮助，但有时还不够。我想知道是否有人建议一种更好的方法？也许有某种方式可以点点储存？ 或者，我比Colab更好地工作了吗？我的本地机器没有用于快速培训模型的果汁，但我也自己为此提供了资金，因此Colab Pro的价格对我来说还好（每月11.38欧元），但是如果有更好的地方可以托管我的笔记本     &lt;！ -  sc_on-&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jsuyxh/d_how_how_to_handle_handle_limited_space_in_ram_when/&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jsuyxh/d_how_to_handle_limited_space_in_ram_when/</guid>
      <pubDate>Sun, 06 Apr 2025 14:09:57 GMT</pubDate>
    </item>
    <item>
      <title>[n] CFP MIDAS研讨会 @ECML -PKDD 2025-关于金融应用的采矿数据的第10次研讨会</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jstszv/n_cfp_midas_workshop_ecmlpkdd_2025_10th_workshop/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jstszv/n_cfp_midas_workshop_ecmlpkdd_2025_10th_workshop/</guid>
      <pubDate>Sun, 06 Apr 2025 13:12:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] IJCAI 2025评论和反驳讨论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jss0lu/dijcai_2025_reviews_and_rebuttal_discussion/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  讨论线程  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bigjuggernaut7380      [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jss0lu/dijcai_2025_reviews_reviews_ard_rebuttal_discussion/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jss0lu/dijcai_2025_reviews_and_rebuttal_discussion/</guid>
      <pubDate>Sun, 06 Apr 2025 11:29:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] Rich Sutton：自我验证，AI的关键</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jslkhw/d_rich_sutton_selfverification_the_key_to_ai/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/jsonathan       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jslkhw/d_rich_sutton_selfverification_the_key_to_ai/</guid>
      <pubDate>Sun, 06 Apr 2025 04:01:49 GMT</pubDate>
    </item>
    <item>
      <title>[r] Noprop：训练神经网络而无需反向传播或前向传播</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jsft3c/r_noprop_training_neural_networks_without/</link>
      <description><![CDATA[https://arxiv.org/pdf/2503.24322 Abstract The canonical deep learning approach for learning requires computing a gradient term at each layer by从输出到每个可学习的参数将误差信号向后传播。鉴于神经网络的堆叠结构，其中每个层都基于层的表示形式，因此该方法导致层次表示。模型顶层的更多抽象功能现场直播，而下层的特征则预计会不那么抽象。与此相反，我们引入了一种名为Noprop的新学习方法，该方法不依赖于前进或后部传播。取而代之的是，Noprop从扩散和流匹配方法中汲取灵感，在该方法中，每一层都独立学习以将嘈杂的目标变形。我们认为，这项工作朝着引入一个新的无坡度学习方法迈出的第一步，这些学习方法没有学习层次的代表，至少在通常的意义上没有。 Noprop需要事先将每一层的表示形式修复到目标的噪声版本，学习一个局部denoising过程，然后可以在推理中利用该过程。我们证明了我们方法对MNIST，CIFAR-10和CIFAR-100图像分类基准的有效性。我们的结果表明，与其他现有的无反向传播方法相比，NOPROP是一种可行的学习算法，具有较高的精度，更易于使用，并且在计算上更易于使用。 Noprop从传统的基于差异的学习范式背道而驰，改变了网络中的信用分配，从而实现了更有效的分布式学习以及潜在地影响学习过程的其他特征。   &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jsft3c/r_noprop_training_neural_neal_networks_without/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jsft3c/r_noprop_training_neural_networks_without/</guid>
      <pubDate>Sat, 05 Apr 2025 22:46:20 GMT</pubDate>
    </item>
    <item>
      <title>[P]有人从事阿拉伯OCR工作吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jsclxw/p_anyone_working_on_arabic_ocr/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我尝试过的阿拉伯语的所有OCR根本无法正常工作。我真的有兴趣构建适当的阿拉伯OCR。如果您知道任何正在从事它的人或任何开放项目，请告诉我。我很想做出贡献并帮助改进它。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/the__space__witch     [link]   ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jsclxw/p_anyone_working_on_arabic_ocr/</guid>
      <pubDate>Sat, 05 Apr 2025 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>[n]致电4发布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jsbbuy/n_llama_4_release/</link>
      <description><![CDATA[        &lt;！ -  sc_off-&gt;       https://www.llama.com/     &lt;！ -  sc_on-&gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/we_are_mammals     [link]   ＆＃32;   [注释]            ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jsbbuy/n_llama_4_release/</guid>
      <pubDate>Sat, 05 Apr 2025 19:22:16 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]这可能是关于当前培训方法的一个非常愚蠢的问题...</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1js6jd9/discussion_this_might_be_a_really_dumb_question/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  那么，为什么我们不能以低量化训练一个非常大的网络，获得最低的测试错误，以最低的测试误差时期修剪网络，然后增加量化或剩余参数以开始训练呢？这难道难道不允许更有效地克服当地的最小值吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1js6jd9/discussion_this_this_might_be_a_really_dumb_question/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1js6jd9/discussion_this_this_might_might_be_a_really_dumb_question/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1js6jd9/discussion_this_might_be_a_really_dumb_question/</guid>
      <pubDate>Sat, 05 Apr 2025 15:53:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICML 2025-如果审稿人不承认反驳怎么办？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1js1ucr/d_icml_2025_what_if_reviewers_dont_acknowledge/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   2中我在ICML的5位评论者中根本不承认我的反驳。不仅没有答案，他们甚至没有单击“确认驳斥”。根本。根据ICML规则，他们必须这样做。当他们不这样做时会发生什么？我们应该向AC报告吗？我在任何地方都没有找到这个，所以也许这里有人知道或处于类似情况。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/qalis     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1js1ucr/d_icml_2025_what_red_reviewer_reviewers_dont_acknowledge/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1js1ucr/d_icml_2025_what_if_reviewers_dont_acknowledge/</guid>
      <pubDate>Sat, 05 Apr 2025 11:59:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]在现实世界情景中使用的域对抗神经网络（DANN）吗？有什么可行的吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1js0tvk/d_are_domain_adversarial_neural_networks_dann/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我发现该论文中提出的想法非常有吸引力，能够在一个受控域上训练，它易于标记数据，并且“传输”它到另一个域，很难将数据标记为。 它是合成/生成的数据与真实数据的合成/生成的数据，或者在野生数据中捕获的办公室捕获的数据，能够成功捕获没有标签的域具有一些实际价值。有人在这个问题上有一些经验吗？听起来真是太好了，这也不是我所期望的，这也不是如此有用，它提高了另一个标志。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1js0tvk/d_are_are_domain_adversarial_neural_neural_networks_dann/”&gt; [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1js0tvk/d_are_domain_adversarial_neural_networks_dann/</guid>
      <pubDate>Sat, 05 Apr 2025 10:54:34 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jpdo7y/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jpdo7y/d_selfpromotion_thread/</guid>
      <pubDate>Wed, 02 Apr 2025 02:15:32 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些经验丰富的人。   &lt;！ -  sc_on--&gt; 32;&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_be_hired/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jnt4sp/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jnt4sp/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Mon, 31 Mar 2025 02:30:37 GMT</pubDate>
    </item>
    </channel>
</rss>