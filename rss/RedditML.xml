<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Tue, 16 Apr 2024 03:15:50 GMT</lastBuildDate>
    <item>
      <title>图像生成的扩散模型与自回归模型。哪个更好？ [D] [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c53pc5/diffusion_versus_autoregressive_models_for_image/</link>
      <description><![CDATA[大家好， 我是使用变压器模型生成图像这一领域的新手。我对上面提到的两种方法很好奇。特别是根据这篇论文“视觉自回归建模：通过下一代预测生成可扩展图像” (结果）。看起来这些 AR（自回归）模型似乎比 DiT（扩散变压器）更好，尤其是在放大时。他们的主要推理优势似乎来自 DiT 的低采样效率。 但是，我对此表示怀疑。除了主要人工智能强国已经采用扩散模型这一事实之外，还有一个坚实的理论支持扩散模型能够生成任何图像分布。除此之外，上面的 VAR 论文没有针对小模型的结果（我也对此感兴趣）。 所以这是我的问题：  有吗有没有提高采样效率的DiT蒸馏论文？我找不到任何 Transformer，所有都是基于 U-Net 的。 是否有任何理论支持用于图像生成（或任何与此相关的生成任务）的 AR Transformer 模型？ 这篇 VAR 论文更好纯粹是因为人们还没有充分探索 DiT 吗？或者说 AR 模型的霸主地位是我们所期望的？ 如果您要根据潜在性能（从研究的角度来看）和成功在两者之间进行选择，您会选择什么？为什么？ （我有偏见，因此支持扩散的原因值得赞赏）  如果您可以回答任何问题或与此事相关的任何意见，请发表评论..    由   提交 /u/InstinctsInFlow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c53pc5/diffusion_versus_autoregressive_models_for_image/</guid>
      <pubDate>Tue, 16 Apr 2024 01:27:48 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 LoRa 训练 Sentence-Transformer 进行文本相似度任务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4vlfa/p_training_sentencetransformer_with_lora_for_text/</link>
      <description><![CDATA[大家好， 我目前正在开展我的学习结束项目，我的目标是训练一个句子-使用 LoRa（Peft 库）为文本相似性任务生成最佳嵌入的 Transformer 模型。 我已经设置了所有内容，包括负责训练和更新权重的 LoRa_fit() 函数。损失函数运行良好，一切似乎都整合得很好。但是，我在训练过程中遇到了一个意外问题。 当尝试使用 SciFact 数据集（MSMARCO 数据集）仅在一个 epoch 上训练我的 Sentence-Transformer 模型 (intfloat/e5-small-v2) 时，训练时间过长。启用 LoRa 时，训练时间约为 10 小时，未启用 LoRa 时，训练时间约为 11 小时。尽管将 LoRa 参数设置为 r=1 和 lora_alpha=1，情况仍然如此。 LoRa 训练时间的边际增益似乎不太可能。 我认为我不是在训练“基本模型”。通过多个调试语句（在如下所示的 lora.py 中）:( trainable params: 27,648 || all params: 33,387,6​​48 || trainable%: 0.08280906759290142 但也打印发生梯度更新的层，仅显示 LoRA 层等） Github 存储库和直接链接到 lora.py 负责加载和训练模型：GPL_LoRA/gpl/toolkit/lora.py at main · KuijpersNick0/GPL_LoRA (github.com) 任何有关可能导致此问题的原因的见解或建议将不胜感激。预先感谢您的帮助！   由   提交/u/Belgium-Frenchie  /u/Belgium-Frenchie reddit.com/r/MachineLearning/comments/1c4vlfa/p_training_sentencetransformer_with_lora_for_text/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4vlfa/p_training_sentencetransformer_with_lora_for_text/</guid>
      <pubDate>Mon, 15 Apr 2024 19:48:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 无需 PCA 即可可视化一类 SVM 的决策边界和异常值</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4v543/d_visualise_decision_boundry_and_outliers_of_one/</link>
      <description><![CDATA[嗨，有没有办法实现这一点？我的数据集有多个特征，我想保留它们。谢谢   由   提交 /u/ItsAGeekGirl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4v543/d_visualise_decision_boundry_and_outliers_of_one/</guid>
      <pubDate>Mon, 15 Apr 2024 19:30:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我可以在扩散模型中使用更简单的损失函数（例如直接似然损失）吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4u503/d_can_i_use_a_simpler_loss_function_eg_direct/</link>
      <description><![CDATA[训练扩散模型时，可以选择定义损失函数。常见的包括根据高斯分布的均值或噪声定义损失函数。但它总是从最大化预测 x0 为或接近实际 x0 的可能性开始。然后需要数学推导来用平均值或噪声来表达损失。  我的问题是我们可以直接使用（负）可能性作为损失吗？ IE。在训练步骤中，我们知道 Xt 并尝试预测 Xt-1，然后对于每个像素 (i, j) 如果预测的 Xt-1(i,j) != 实际的 Xt-1(i,j) 那么这有助于到损失。我所描述的只是训练/收敛速度慢还是不正确？ 谢谢！   由   提交 /u/Complete-Conflict339    reddit.com/r/MachineLearning/comments/1c4u503/d_can_i_use_a_simler_loss_function_eg_direct/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4u503/d_can_i_use_a_simpler_loss_function_eg_direct/</guid>
      <pubDate>Mon, 15 Apr 2024 18:51:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 归一化对分类特征的影响</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4u4zi/d_effect_of_normalization_on_categorical_features/</link>
      <description><![CDATA[可以规范化（如标准缩放器）二进制/一个热编码/标记编码功能吗？我现在找不到任何理由说明为什么它会成为一个问题，但我也没有特别的理由说避免这样做会出现问题。你觉得怎么样？   由   提交 /u/risilm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4u4zi/d_effect_of_normalization_on_categorical_features/</guid>
      <pubDate>Mon, 15 Apr 2024 18:51:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习系统研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4sq8x/d_machine_learning_systems_research/</link>
      <description><![CDATA[我一直在研究 Chip Huyen 的《设计机器学习系统》一书，我对作为研究领域的此类工作非常感兴趣。我应该了解哪些研究人员/实验室/大学以了解有关该领域的更多信息？下面是一个示例： https://pooyanjamshidi.github.io/mls/ &lt; !-- SC_ON --&gt;  由   提交/u/larenspear  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4sq8x/d_machine_learning_systems_research/</guid>
      <pubDate>Mon, 15 Apr 2024 17:56:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 物体检测中的 SOTA？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4pccp/d_sota_in_object_detection/</link>
      <description><![CDATA[几年前我做过目标检测，其中 FRCNN、SSD 和 YOLO 与 RESNET 和 VGG 等作为主干的东西一起流行。 回到 2024 年的今天的物体检测任务，我找不到任何重大改进或真正新的架构。我是否遗漏了什么，或者这仍然是 SOTA？ 谢谢！ :)   由   提交/u/topsnek69  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4pccp/d_sota_in_object_detection/</guid>
      <pubDate>Mon, 15 Apr 2024 15:40:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么稳定扩散的潜在通道这么小？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4o0qg/d_why_is_the_latent_channel_of_stable_diffusion/</link>
      <description><![CDATA[大家好。我是生成人工智能领域的新手，目前正在深入研究稳定扩散的内部结构。我注意到 SD 的 VAE 编码逐渐将通道数提升到 512 个，但在生成潜在向量时突然下降到只有 4 个。这就像一条非常宽阔的隧道后面有一个非常非常细的瓶颈。为什么要这样设计呢？ 64*64*4真的足以表达这么多可能的特征吗？    由   提交/u/ConsequenceDear2557   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4o0qg/d_why_is_the_latent_channel_of_stable_diffusion/</guid>
      <pubDate>Mon, 15 Apr 2024 14:45:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 冷冰冰地给研究人员发邮件寻求合作，我应该谨慎吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4l2hv/d_cold_emailing_a_researcher_for_collaboration/</link>
      <description><![CDATA[我是一名硕士生，几周来我一直在从事一个与研究人员所做的工作密切相关的项目A在最近的一篇论文中。该项目进展顺利，我认为它可以成为一份很棒的出版物，不幸的是我不认识任何从事该主题的人，并且一些指导对于正确捍卫该项目很有用。需要明确的是，这个项目已经很先进了，这不仅仅是一个想法。 我本来想尝试给研究员 A 发电子邮件，看看他是否有兴趣在这个项目上合作，但是我有疑问： 1/ 通过电子邮件冷联系是否很奇怪？我想人们通常会利用会议来进行这种交流，但我还没有机会 2/ 我应该对发送给此人的信息保持谨慎吗？他是一位著名的研究人员，所以我想这很安全，但我不想因为一封电子邮件而被抢先一步 我知道我提供的有关情况的信息很少，但我会如果您曾经做过类似的事情以及它是否成功，很高兴听到任何建议或经验   由   提交 /u/Even_Information4853   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4l2hv/d_cold_emailing_a_researcher_for_collaboration/</guid>
      <pubDate>Mon, 15 Apr 2024 12:33:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 4 位 vs float16 的推理速度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4kcoo/d_inference_speed_of_4_bit_vs_float16/</link>
      <description><![CDATA[假设我有一个可以在不压缩的情况下加载 7B 模型的 GPU，只是想知道 4 位量化的推理速度是否更快？或者 4 位向量是否需要解压缩，从而使 4 位量化变慢？ 这里是加载 Mistral 的示例代码。 ```python from Transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig device = &quot;cuda&quot;; # 将模型加载到 model_id = &quot;mistralai/Mistral-7B-Instruct-v0.2&quot; 的设备bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=“nf4”, bnb_4bit_compute_dtype=torch.bfloat16 ) model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=&amp; “自动” ) ```   由   提交/u/themathstudent  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4kcoo/d_inference_speed_of_4_bit_vs_float16/</guid>
      <pubDate>Mon, 15 Apr 2024 11:57:20 GMT</pubDate>
    </item>
    <item>
      <title>因使用 Java 而被嘲笑 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4gi25/ridiculed_for_using_java_d/</link>
      <description><![CDATA[所以我在 Twitter 上（第一个错误）提到了我的 Java 神经网络，并因使用“过时且无用的语言”而被嘲笑。对于已经构建的NLP。 说实话，这是我的第一个NLP。不过，我确实创建了一个使用 GPT2 管道为作者生成故事的 Python 应用程序，但基础设施的其余部分是用 Java 编写的，我只是创建了一个 Python API 来调用它。 我喜欢 Java。我的代码可以追溯到 2017 年。我是一名业余爱好者，并不期望获得 ML 职位，尤其是在市场和现在的情况下。不过，我确实有机会在我的业务分析师工作中展示一些编程技能，并使用我非常小的 NLP 对一些票务数据执​​行一些基本预测，顺便说一句，我对这些数据很感兴趣。 我的问题是：我是一个彻底使用 Java 的失败者吗？我正在学习一些机器人技术，并计划学习一些 C++，但我拒绝放弃 Java，因为到目前为止，它教会了我很多东西，并为我带来了很好的成果。 l&#39;d就像你对此的看法。谢谢！   由   提交 /u/esqelle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4gi25/ridiculed_for_using_java_d/</guid>
      <pubDate>Mon, 15 Apr 2024 07:48:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在行业 NLP 中，除了文本生成之外，法学硕士还有其他实际用途吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4cr32/d_in_industry_nlp_are_there_any_actualpractical/</link>
      <description><![CDATA[我已经对 NLP 进行了一段时间的研究，并且在过去几年中一直在行业中实施解决方案。和许多其他公司一样，我公司的管理层也“令人惊叹”。由 ChatGPT 开发，并且已经推动 LLM 研发一段时间了。然而，除了将它们用于文本生成之外，我看不出投资回报率有多好。 我正在从事的许多任务都倾向于关注语义搜索、信息提取（NER） 、RE）、文本图像表示学习等。这些任务可以通过训练有素的 BERT 和 CLIP 模型很好地处理，我认为开发 LLM 所付出的努力是不值得的。最近的研究似乎也表明，对于 IE 任务，传统的监督方法仍然存在。 你们发现 LLM 擅长的其他用例吗？   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4cr32/d_in_industry_nlp_are_there_any_actualpractical/</guid>
      <pubDate>Mon, 15 Apr 2024 03:51:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 文本分类/NER/RE等传统NLP任务在法学硕士时代仍然重要吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c4a7sa/d_are_traditional_nlp_tasks_such_as_text/</link>
      <description><![CDATA[大型语言模型 (LLM) 是全面的 NLP 任务求解器。他们可以用最早的程序语言英语来完成很多 NLP 任务，而不是需要复杂过程的基于判别式的模型。 作为一名博士生，我最近对未来有点焦虑。我的顾问支持一个关于知识提取和知识图的资助项目。但我认为未来一定是LLM。我目前的计划是在完成手头的NER和RE项目后，做一些RAG的工作。 你们对传统NLP任务（文本分类、NER、RE等）有什么想法吗？法学硕士时代？   由   提交 /u/edzq   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c4a7sa/d_are_traditional_nlp_tasks_such_as_text/</guid>
      <pubDate>Mon, 15 Apr 2024 01:38:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于发现“假”机器学习角色的建议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c3z8ug/d_advice_for_spotting_fake_ml_roles/</link>
      <description><![CDATA[我最近被聘用，结果证明这是一个假的 ML 角色，即使按照 ML 角色最宽松的定义（彭博 AI 小组）也是如此。目前似乎有许多公司/团队/人员假装从事机器学习工作，而在招聘时，他们对候选人实际所做的工作撒谎。有没有人有任何策略来发现此类角色，从而避免它们？面试时提问似乎不太有效，因为你很容易被骗。   由   提交 /u/Outrageous-Base3215    reddit.com/r/MachineLearning/comments/1c3z8ug/d_advice_for_spotting_fake_ml_roles/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c3z8ug/d_advice_for_spotting_fake_ml_roles/</guid>
      <pubDate>Sun, 14 Apr 2024 17:41:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>