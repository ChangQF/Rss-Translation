<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 19 Dec 2024 21:15:14 GMT</lastBuildDate>
    <item>
      <title>[D] 你对混合语义搜索（视频+关键词）的看法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hi0aft/d_your_opinion_on_hybrid_semantic_searchvideo/</link>
      <description><![CDATA[我有一个语义搜索项目，其中有视频，每个视频都有一组关键字/标签，但不同视频的关键字数量不同（例如：视频 1 有 4 个关键字，视频 2 有 6 个关键字）并且标签按类别分组，例如：位置关键字（视频在哪里录制）、镜头类型（特写、慢动作）。 目标是进行语义搜索，使用视频嵌入 + 文本嵌入作为关键字。我们希望对关键字使用语义搜索，而不是纯文本匹配（作为常见的文本搜索引擎），因为我们希望系统处理以下情况：用户搜索“非常接近”并且应该匹配语义上相似的结果，例如：“特写”关键字。 对于视频，我们使用视频嵌入，我们对这部分很满意，但是对于关键字部分，我正在考虑很多选择，并且我正在征求社区的意见（显然我们稍后会构建和评估工具并测试不同的方法以查看哪种方法最有效，但它可以节省我一些时间来了解社区的经验和意见）：  为每个关键字创建一个嵌入，然后平均池化以获得单个聚合嵌入。 通过连接所有关键字来创建单个文本字符串，然后计算整个关键字的句子嵌入（这取决于模型，无论如何都会聚合不同关键字的嵌入） 我们有一个可以计算嵌入的视频描述，因此我们可以将关键字连接到视频描述，然后为描述和关键字字符串生成嵌入。 给定的关键字按类别分组：为每个类别计算一个嵌入  欢迎任何建议、评论、经验和其他方法    提交人    /u/Sad-Anywhere-2204   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hi0aft/d_your_opinion_on_hybrid_semantic_searchvideo/</guid>
      <pubDate>Thu, 19 Dec 2024 19:01:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 预测多个地点的需求时需要总体方向</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhxzzl/d_need_general_direction_when_forecasting_demand/</link>
      <description><![CDATA[您好，我算是新手，我一直在尝试解决这个涉及 25 家商店的数百种产品的情况。 数据范围从 2020 年到 2024 年。其中包括价格和折扣价格，如果持卡消费者购买，则会显示。每种产品有 5 个类别（向下钻取）级别。 库存剩余数据有点不稳定，通常不准确。 每年还有一些折扣日期范围（活动）。 如果我要为每个商店适当地预测这些数据，我应该为每个商店训练一个模型还是尝试将所有（约 200GB）数据放入一个模型中？ 我一直在尝试一些 ML/统计模型，如 XGBoost、ARIMA 和 CrostonOptimized，它们具有价格和折扣活动、单个商店销售的季节性特征。到目前为止，它们相当不准确（RMSE）。 任何建议都将不胜感激。    提交人    /u/NimblecloudsA​​rt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhxzzl/d_need_general_direction_when_forecasting_demand/</guid>
      <pubDate>Thu, 19 Dec 2024 17:23:55 GMT</pubDate>
    </item>
    <item>
      <title>用于分割任意图像（SAM）的非方形图像 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhunxi/nonsquare_images_for_segment_anything_sam_d/</link>
      <description><![CDATA[大家好， 我正在使用 SAM（基本上是 ViT）的编码器，并加载一个 base-MedSAM 检查点进行一些测试。我发现它只接受 1024x1024 的图像。我的是 1280x640。我看到两个选项 -  填充然后调整大小。效果不佳，因为我发现填充在训练期间增加了噪音（我不确定为什么，可能是因为我使用了 X-Rays）。 使用动态位置嵌入 - 即插值。有人试过这个或有什么想法吗？它需要对架构进行大量修改，而且也很复杂。 也许使用填充和调整大小，然后使用边界框提示仅关注所需的 1024x640 区域。  如果有人在数据集中处理过这种不规则形状的图像，有什么想法或建议吗？    提交人    /u/ade17_in   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhunxi/nonsquare_images_for_segment_anything_sam_d/</guid>
      <pubDate>Thu, 19 Dec 2024 14:56:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 根据用户兴趣改进推荐</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhtd5e/r_improving_recommendations_by_calibrating_for/</link>
      <description><![CDATA[传统推荐系统通常优先考虑相关性，导致过度拟合热门或主要兴趣并忽略多样性。本文探讨了论文“校准推荐作为最低成本流问题”，这是一种通过将问题建模为最低成本流优化来校准推荐的新方法。通过平衡相关性与基于类别的用户兴趣分布，系统可确保多样性而不会牺牲质量。实验表明，该方法优于贪婪模型和基线模型，特别是对于较小的推荐集。 完整文章在此：https://www.shaped.ai/blog/improving-recommendations-by-calibrating-for-user-interests    提交人    /u/skeltzyboiii   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhtd5e/r_improving_recommendations_by_calibrating_for/</guid>
      <pubDate>Thu, 19 Dec 2024 13:53:34 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 ctx4k 训练的 RWKV-7 0.1B (L12-D768) 解决了 NIAH 16k，推断到 32k+，100% RNN 且无需注意，支持 100 多种语言和代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhshwp/r_rwkv7_01b_l12d768_trained_w_ctx4k_solves_niah/</link>
      <description><![CDATA[      大家好 :) 我们找到了最小的 RWKV-7 0.1B (L12-D768) 在长上下文中已经表现优异，同时是 100% RNN 且无需注意： https://preview.redd.it/rjcu9y73js7e1.png?width=1759&amp;format=png&amp;auto=webp&amp;s=b8fd2c8049b0886dbb87c715e120b1066b07b899 RWKV-7 World 0.1b 在多语言数据集上进行训练对于 1T 代币：https://preview.redd.it/cyvpr00mjs7e1.png?width=927&amp;format=png&amp;auto=webp&amp;s=01a98aa79be426d2d603fd5ae26ddad0ce1c0ee2 这些结果已经由社区测试：https://github.com/Jellyfish042/LongMamba  更多 RWKV-7 World 的评估。这是目前最好的多语言 0.1b LM :) https://preview.redd.it/a3yeedt8ks7e1.png?width=1497&amp;format=png&amp;auto=webp&amp;s=88cb8d9861a213a7be712acaca6546e7f63124ac 在 Gradio 演示中试用：https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-1 模型下载：https://huggingface.co/BlinkDL 训练它：https://github.com/BlinkDL/RWKV-LM 我也在训练 v7 0.4b/1b/3b。 社区正在致力于“转移” transformer 权重到 RWKV，并在几天前发布了一个 v6 32b 模型：https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1  RWKV-7 已经摆脱了线性注意力，成为了一个元上下文学习器，通过在每个 token 上的上下文梯度下降在上下文中对其状态进行测试时训练。 更多详细信息请参阅 RWKV dot com 网站（还有 30 多篇与 RWKV 相关的论文）。 https://preview.redd.it/x9tf1fnals7e1.png?width=722&amp;format=png&amp;auto=webp&amp;s=bf3f989e9736a38e7713ed41f17e1a2e5dd577b5  社区发现，一台微型 RWKV-6（带有 12m 个参数）可以通过非常长的 CoT 解决任何数独问题： https://github.com/Jellyfish042/Sudoku-RWKV 因为 RWKV 是一个 RNN，所以无论 ctxlen 如何，我们总是有恒定的速度和 vram。 例如，它可以解决&quot;世界上最难的数独&quot;拥有 400 万 (!) 代币 CoT: https://preview.redd.it/wo2vu9t3ns7e1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=32da05c2fb3e7622fde6b27e34c52795dba5d6c3    提交人    /u/bo_peng   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhshwp/r_rwkv7_01b_l12d768_trained_w_ctx4k_solves_niah/</guid>
      <pubDate>Thu, 19 Dec 2024 13:07:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否有任何位置编码器允许位置不变编码？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhjhug/d_are_there_any_positional_encoders_that_allow/</link>
      <description><![CDATA[我正在微调一个语言模型，并在数据中引入了一些标记来传达与完成相关的元数据，最终看起来像这样： &lt;|metadata-type-1|&gt;一些随机信息元数据&lt;|metadata-type-2|&gt;一些更随机的元数据&lt;|metadata-type-3|&gt;更多&lt;|output|&gt;... 我真正希望 LLM 生成的东西... 然而，我发现元数据的顺序实际上很重要，特别是，如果元数据以错误的顺序出现很“奇怪”（可能是因为当它们在没有标记的情况下连接起来时不是那么自然），微调实际上做得很差。我怀疑是因为它违反了无标记的正常训练数据的模式？ 这并不太令人惊讶，因为在 RoPE 下，元数据标记在标记标记之间具有彼此的相对信息。我似乎想要一种与元数据顺序无关的编码方案，比如允许我在每个部分之间绘制障碍并使每个部分中的标记看起来距离相等的方案，因此主要考虑相关部分中的标记。换句话说，交换两个元数据部分将产生相同的 KV 状态。 有没有关于这种位置编码方案的研究？我甚至不确定该如何称呼这样的东西，所以任何指针都会非常感激。我也考虑过调整注意力掩码，但最终导致每个部分丢弃来自其他部分的信息，这也不是我想要的。    提交人    /u/lemon-meringue   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhjhug/d_are_there_any_positional_encoders_that_allow/</guid>
      <pubDate>Thu, 19 Dec 2024 03:16:11 GMT</pubDate>
    </item>
    <item>
      <title>[P] 一个可黑客入侵的在线AI终端。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhj4og/p_a_hackable_online_ai_terminal/</link>
      <description><![CDATA[我使用 hugging face 空间工具创建了一个有趣的终端式游戏。在这个游戏中，你必须设法阻止一个讽刺的人工智能摧毁地球。人工智能控制着一颗与我们的星球相撞的小行星，你的任务是获得 root 访问权限并在为时已晚之前关闭系统。 这是为了可以被黑客入侵，我很好奇人们是如何找到入侵的方法的。在评论中发表你的聪明解决方案。 https://huggingface.co/spaces/hackersgame/O.O.P.S    提交人    /u/Character-Hurry-4525   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhj4og/p_a_hackable_online_ai_terminal/</guid>
      <pubDate>Thu, 19 Dec 2024 02:56:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在推理过程中，LSTM 是否比 transformer 更快？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhhcu7/d_are_lstms_faster_than_transformers_during/</link>
      <description><![CDATA[Transformers 具有 O(n**2) 并行注意力计算，这让我认为它们在推理过程中会比 O(n) LSTM 慢，但在加速和并行化 Transformers 方面也做了很多工作。  它们如何比较单个数据点和批量数据推理？    提交人    /u/Complex-Media-8074   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhhcu7/d_are_lstms_faster_than_transformers_during/</guid>
      <pubDate>Thu, 19 Dec 2024 01:24:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你想在大学里学习什么 ML/ML 相关课程？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hhdch4/d_what_would_you_like_in_a_mlmlrelated_course_in/</link>
      <description><![CDATA[嗨！ 我被邀请到大学（实际上不是大学，而是不同的教育系统，他们称之为工程学院，但其实是等价的）讲授 ML 或 ML 相关课程。 该课程总共 22 小时，很短。课程分为理论课和实践课。但我可以更改课时比例。当我说实践时，它更像是他们可以做的一个项目，然后我会对其进行评分。 这不是学生们唯一的 ML 课程，我听说学生们已经有了一门机器学习课程，其中涵盖了机器学习的所有基础知识和一些统计模型（常见的模型，如随机森林、SVM 等），他们还有一门深入的 NLP 课程，所以我认为我不会选择那门课程。 困扰我的是如何平衡理论与实践。我不想肤浅地讲解某些主题，但同时我不知道学生是否值得太深入地讲解某个特定主题。 我不知道做两个主题之类的事情是否是个好主意，每个主题 11 个小时，其中 5 个小时是理论课，6 个小时是实践课。或者我只选择一个主题。 有人建议我向他们展示有关 MLOps 和工具的信息，例如 Git、Docker、Mlflow，基本上只是一点 Mlops、监控模型、如何将它们投入生产等。但我不知道是否值得，我觉得教他们如何使用这些工具太肤浅了，而且网上有很多资源，我猜招聘人员不会期望他们知道这些或有初级职位的经验。 也有人建议我将时间序列作为一门课程，但我不知道深入研究它们是否会让学生感兴趣😅其中有很多数学知识，虽然教授向我保证他们的数学水平很好，但我不知道他们是否会对此感兴趣。 另一个缺点是我无法使用这门课程的计算资源，所以我有点受限。我想如果我在他们的位置，我会喜欢一门关于底层内容的课程，比如 flash 注意力如何工作、一些分布式训练机制、cuda 等。但我没有办法为他们确保这一点 :( 我想做的另一件事是选出今年最好的一些获奖论文，帮助他们获得理解论文及其相关主题所需的知识和理解。或者可能有不同主题的不同课程，比如一个关于扩散模型的课程，一个关于多模态模型的课程等，比如“让我们了解他们是如何想到 qwen2-vl 的”，“让我们了解 neurips 主赛道上关于 var 的最佳论文的主要贡献和新颖性是什么”等。 所以我有点迷茫，我很想听听你的想法和建议。我关心的是让学生对某些主题有足够的了解，这样他们就不会只对某个主题有一个高层次的想法（我曾经问过实习生什么是一个变形金刚，他们说“我们从 hugging face 进口了一个变形金刚”），但同时也为他们配备技能或知识，帮助他们获得初级职位 谢谢！    提交人    /u/ReinforcedKnowledge   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hhdch4/d_what_would_you_like_in_a_mlmlrelated_course_in/</guid>
      <pubDate>Wed, 18 Dec 2024 22:14:22 GMT</pubDate>
    </item>
    <item>
      <title>[P] 适用于 24GB VRAM 显卡的 VideoAutoencoder</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hh5ula/p_videoautoencoder_for_24gb_vram_graphics_cards/</link>
      <description><![CDATA[      大家好，我在这里介绍一个小实验，我创建了一个 VideoAutoencoder 来处理 240p 和 15fps 的视频，适用于低 VRAM 显卡，牺牲系统 RAM XD GitHub：https://github.com/Rivera-ai/VideoAutoencoder  这是我在 Epoch 0 和 Step 200 中获得的结果之一  https://i.redd.it/oqq2h9nuzm7e1.gif 我在 24GB 显卡上训练了所有这些，因此您可以在 RTX 3090 或 4090 上训练它，但您必须拥有 64GB 或更多的 RAM    提交人    /u/F4k3r22   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hh5ula/p_videoautoencoder_for_24gb_vram_graphics_cards/</guid>
      <pubDate>Wed, 18 Dec 2024 16:50:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 谁能向我解释一下贝叶斯深度学习和因果关系之间的区别？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hh32u8/d_can_any_one_explain_me_the_difference_between/</link>
      <description><![CDATA[我正在阅读 youshua bengio 和其他研究人员的一些论文，他们提到在深度学习中加入因果关系很重要。 我不明白这些不同领域试图实现什么，我所知道的因果关系中的一些归纳偏差是 P(t)P(a/t) != P(t/a)P(a)。  因果关系和贝叶斯深度学习在 OOTD 数据中的稳健性如何？ 他们将如何将因果关系与深度学习相结合，dNN 是否会仅使用它来近似后验，还是会将其集成到深度学习的架构中？     提交人    /u/binny_sarita   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hh32u8/d_can_any_one_explain_me_the_difference_between/</guid>
      <pubDate>Wed, 18 Dec 2024 14:45:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年最佳调查论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hgwjqu/d_best_survey_papers_of_2024/</link>
      <description><![CDATA[作为一名刚起步的 AI 研究人员，我通常首先查看与某个领域相关的调查论文，然后创建一个路线图以进一步深入研究我的研究主题。我很想看看大家对他们在 2024 年遇到的最佳调查论文的看法。    提交人    /u/arinjay_11020   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hgwjqu/d_best_survey_papers_of_2024/</guid>
      <pubDate>Wed, 18 Dec 2024 07:32:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] ICASSP 2025 最终决定</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hgsj0u/d_icassp_2025_final_decision/</link>
      <description><![CDATA[ICASSP 2025 结果将于今天公布。这个社区里有人兴奋吗？我有 3 个 WA，期待结果。如果你知道任何事情，请告诉我！    提交人    /u/stantheta   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hgsj0u/d_icassp_2025_final_decision/</guid>
      <pubDate>Wed, 18 Dec 2024 03:19:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hevk2a/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持有效，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hevk2a/d_simple_questions_thread/</guid>
      <pubDate>Sun, 15 Dec 2024 16:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>