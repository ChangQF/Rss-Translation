<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Fri, 12 Apr 2024 18:14:02 GMT</lastBuildDate>
    <item>
      <title>[R] 寻找给定 RGB 图像（无深度）+ 3D cad 模型的 6D 位姿估计模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2g2l6/r_looking_for_6d_pose_estimation_model_given_rgb/</link>
      <description><![CDATA[我正在尝试估计图像中机器人末端执行器的姿态。  RGB 图像是从第三人称视图提供的，我有一个可用的 3D 模型。不幸的是，我无法为模型提供任何深度数据。关于哪种型号的任何建议都会很棒。    由   提交/u/Rough_Cranberry5560   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2g2l6/r_looking_for_6d_pose_estimation_model_given_rgb/</guid>
      <pubDate>Fri, 12 Apr 2024 18:11:24 GMT</pubDate>
    </item>
    <item>
      <title>[项目] Burr：一个用于更快地构建和调试 GenAI 应用程序的操作系统框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2egw0/project_burr_an_os_framework_for_building_and/</link>
      <description><![CDATA[https://github.com/dagworks-inc /burr 嘿伙计们！我想分享一些我们一直在努力的东西，我认为您可能会觉得有用！我们最初构建它是为了内部使用，但想与世界分享。 我们试图解决的问题是使用 ML/AI（基础模型等...）的逻辑建模系统。做出决策（设置控制流、决定要查询的模型等），并保持某种级别的状态。这很复杂——理解系统在任何给定点做出的决策需要大量的仪器等等... 我们已经看到了很多不同的工具试图使这变得更容易，但它们&#39;所有这些都非常黑匣子，并且专注于一种特定情况（及时管理）。我们想要一种能够更快地调试、理解和构建应用程序的东西，而不会对您使用的框架施加任何类型的限制，也不需要跳过各种障碍来定制。 我们想出了 Burr——核心想法是将应用程序表示为状态机，可以实时可视化正在经历的流程，并单独开发和测试组件。它配备了用于本地调试的遥测用户界面，以及检查点、收集数据以生成测试用例/评估等的能力... 我们对最初的接收感到非常兴奋，并希望得到更多反馈/操作系统用户 - 如果您有任何问题，请随时给我发私信或在这里发表评论，祝您开发愉快！ PS - Burr 这个名字是对我们运行的项目名为 Hamilton，您可能很熟悉。他们实际上合作得很好！   由   提交 /u/benizzy1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2egw0/project_burr_an_os_framework_for_building_and/</guid>
      <pubDate>Fri, 12 Apr 2024 17:06:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 您会参加 ICLR 研讨会吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2coga/r_would_you_go_to_the_iclr_workshops_day/</link>
      <description><![CDATA[我距离不远（欧洲），想去 ICLR 研讨会只是为了了解现场情况、与人见面等。您认为这样吗？可能有用，或者大多数人在最后一天都已经走了？    由   提交 /u/tuitikki   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2coga/r_would_you_go_to_the_iclr_workshops_day/</guid>
      <pubDate>Fri, 12 Apr 2024 15:54:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习博士论文发表的激烈竞争</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2bvoj/d_publication_rat_race_for_phd_in_ml/</link>
      <description><![CDATA[目前，顶尖大学的机器学习博士项目要求申请者拥有多篇第一作者论文以及来自知名研究人员的强烈推荐。如果在其他领域，这种发表记录可能会让您有资格担任教职。学生如果自己没有进入顶尖学校，怎么可能取得这些成绩呢？  编辑：我的主要观点与推荐要求有关。顶尖大学以外的学生几乎没有机会获得知名研究人员的推荐，因为他们大多在顶尖学校工作。这些顶尖大学之外也有知名的研究人员，但数量太少。如果你在一所普通大学，那么你的学校很可能没有在你的领域足够知名的研究人员。   由   提交 /u/RobbinDeBank   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2bvoj/d_publication_rat_race_for_phd_in_ml/</guid>
      <pubDate>Fri, 12 Apr 2024 15:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过用 7B+ LLM 进行代币分类吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c2bdgh/d_has_anyone_tried_token_classification_with_a_7b/</link>
      <description><![CDATA[就像在具有很长上下文窗口的 2B 或 7B 模型上附加分类头并微调标记分类任务 Bert、T5 和类似的上下文窗口有限   由   提交 /u/EnnioEvo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c2bdgh/d_has_anyone_tried_token_classification_with_a_7b/</guid>
      <pubDate>Fri, 12 Apr 2024 15:01:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 检索增强语言建模（REALM）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c29r8h/d_retrievalaugmented_language_modeling_realm/</link>
      <description><![CDATA[我刚刚发现（我认为是）原始的 REALM 论文，“检索增强语言模型预训练”。非常有趣的想法，但是关于猎犬的角色，有一些关键细节我没有注意到。我希望这里有人能纠正我的观点：  首先也是最关键的是，检索增强仅与生成模型相关吗？你听到了很多关于RAG，但是就不能有像RAU这样的东西吗？就像为下游非生成任务 Y 编码某些文本 X 一样，编码器可以访问知识存储，从中识别、检索相关信息，然后将其包含在嵌入过程中以细化模型对原始文本的表示X？从概念上讲，这对我来说是有意义的，而且这似乎是 REALM 论文所做的（其中任务 Y 是 QA），但我在网上找不到此类事情的任何其他示例。检索增强似乎只适用于生成任务。那么是的，情况总是如此，还是 RAU 也存在？ 如果语言模型是使用检索增强进行训练的，那就意味着检索器是模型架构，对吗？换句话说，在推理时间中，必须始终进行一些检索，这进一步意味着从中检索文档的知识存储也必须始终存在，对吗？或者检索部分周围的所有机制都只是训练的产物，可以在学习完成后丢弃？ REALM 的主要好处是它允许的较小的模型？ 这个问题背后的基本原理：如果没有检索步骤，模型的 100% 的潜在知识必须包含在注意力机制的权重内（我认为）。对于预计几乎了解一切的基础模型，这需要大量的权重。然而，如果模型可以通过某些其他机制（例如检索增强）将上下文注入到表示中，则检索后模型的其余部分（例如注意机制）要做的工作更少，并且可以更小/更简单。我理解这里的大思想了吗？    由   提交 /u/synthphreak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c29r8h/d_retrievalaugmented_language_modeling_realm/</guid>
      <pubDate>Fri, 12 Apr 2024 13:54:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自动数据段优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c26vfv/d_automatic_data_segment_optimisation/</link>
      <description><![CDATA[我在喝了几杯啤酒后想到了这一点，所以请允许我有一些不完整的地方。假设您在一家房地产投资基金工作，负责预测房价的问题。假设该企业每年只能投资 1000 万美元。您有一个包含全国 10 万栋房屋的数据集，其中包含许多特征以及各自的房价。现在，您可以针对整套 100k 房屋训练一些针对 RMSE 进行优化的奇特模型。太棒了，您现在有了一些很酷的 RMSE，可以描述您的表现有多“好”。你的模型是。但不要忘记，您的企业每年只能投资一定金额，所以如果您的模型在预测单层房屋的价格方面比预测两层房屋的价格要好得多，为什么要浪费计算资源和模型在两层房屋上学习能力？难道没有争论只在单层房屋上进行再训练吗？换句话说，虽然总 RMSE 不错，但如果单层房屋的 RMSE 远低于两层房屋（在同一组预测内），那么投资前者肯定会更好。应该有价值超过 1000 万美元的单层房屋可供投资。 现在，在这种情况下，您无法提前知道这个结果。另外，可能存在太大或太复杂而无法实际训练个体可疑“富有成果”的特征空间。数据子集。因此，在超参数搜索优化超参数的方式中，如果有一种方法可以优化用于训练的数据子集，那就太酷了。考虑到用于数据段优化的搜索空间是巨大的(甚至可能在降维之后)，因此“网格搜索”是可行的。可能不是正确的方法...有人知道一些优化用于训练的数据段的很酷的技术吗？感谢您阅读我的 Ted 演讲...   由   提交/u/HStuart18  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c26vfv/d_automatic_data_segment_optimisation/</guid>
      <pubDate>Fri, 12 Apr 2024 11:34:41 GMT</pubDate>
    </item>
    <item>
      <title>关于数据缺失点[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c26imw/about_data_missing_pointsd/</link>
      <description><![CDATA[我找到了股票价格数据，但该数据集不包含周末，并且日期之间存在差距，这是一个问题，我该如何解决这个问题。   由   提交 /u/YigitTheResearcher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c26imw/about_data_missing_pointsd/</guid>
      <pubDate>Fri, 12 Apr 2024 11:14:55 GMT</pubDate>
    </item>
    <item>
      <title>[D]利用自监督学习涉足手写文本识别问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c21frt/d_dabbling_in_handwritten_text_recognition/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c21frt/d_dabbling_in_handwritten_text_recognition/</guid>
      <pubDate>Fri, 12 Apr 2024 05:43:27 GMT</pubDate>
    </item>
    <item>
      <title>【研究】MMStar：我们评估大型视觉语言模型的方法正确吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c20v8x/research_mmstar_are_we_on_the_right_way_for/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2403.20330  评估代码：https://github.com/open-compass/VLMEvalKit  摘要： 大型视觉语言模型（LVLM）最近取得了快速进展，引发了大量研究来评估其多模态能力。然而，我们深入研究当前的评估工作并发现两个主要问题：1）视觉内容对于许多样本来说是不必要的。答案可以直接从问题和选项中推断出来，或者从法学硕士中嵌入的世界知识中推断出来。这种现象在当前的基准测试中普遍存在。例如，GeminiPro 在没有任何视觉输入的 MMMU 基准测试中达到了 42.9%，并且在六个基准测试中平均优于随机选择基准 24% 以上。 2）LLM和LVLM训练中存在无意的数据泄露。 LLM和LVLM仍然可以在没有视觉内容的情况下回答一些视觉必需的问题，表明在大规模训练数据中记忆了这些样本。例如，Sphinx-X-MoE 在不访问图像的情况下在 MMMU 上获得了 43.6%，超过了其 LLM 骨干网的 17.9%。这两个问题都会导致对实际多模态增益的误判，并可能误导 LVLM 的研究。为此，我们推出了 MMStar，这是一个精英视觉不可或缺的多模态基准，由人类精心挑选的 1,500 个样本组成。 MMStar 对 6 个核心功能和 18 个详细轴进行了基准测试，旨在通过仔细平衡和纯化的样本来评估 LVLM 的多模式能力。这些样本首先通过自动化管道从当前基准中粗略选择，然后进行人工审查，以确保每个精选样本表现出视觉依赖性、最小的数据泄漏，并且需要先进的多模式功能。此外，还开发了两个指标来衡量多模式训练中的数据泄漏和实际性能增益。我们在 MMStar 上评估了 16 个领先的 LVLM，以评估其多模态能力，并在 7 个基准测试中使用建议的指标来调查其数据泄漏和实际多模态增益。    由   提交/u/KennyMcKormick_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c20v8x/research_mmstar_are_we_on_the_right_way_for/</guid>
      <pubDate>Fri, 12 Apr 2024 05:08:28 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Ada-LEval：使用长度自适应基准评估长上下文法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c20to0/research_adaleval_evaluating_longcontext_llms/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2404.06480  代码/数据集：https://github.com/open-compass/Ada -LEval 摘要： 最近，大型语言模型 (LLM) 社区对增强 LLM 处理超长文档的能力表现出越来越大的兴趣。随着各种长文本技术和模型架构的出现，对模型长文本能力的精确而详细的评估变得越来越重要。现有的长文本评估基准，例如L-Eval和LongBench，都是基于开源数据集构建长文本测试集，主要关注QA和摘要任务。这些数据集包括纠缠在一起的不同长度（从 2k 到 32k+）的测试样本，这使得评估不同长度范围内的模型能力变得具有挑战性。此外，它们不涵盖最新法学硕士声称要实现的超长设置（100k+ 代币）。在本文中，我们介绍了 Ada-LEval，这是一种长度自适应基准，用于评估法学硕士的长上下文理解。 Ada-LEval 包括两个具有挑战性的子集：TSort 和 BestAnswer，它们可以更可靠地评估法学硕士的长上下文能力。这些基准测试支持对测试用例长度的复杂操作，并且可以轻松生成多达 128k 个标记的文本样本。我们使用 Ada-LEval 评估了 4 个最先进的闭源 API 模型和 6 个开源模型。评估结果证明了当前法学硕士的局限性，尤其是在超长上下文环境中。   由   提交/u/KennyMcKormick_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c20to0/research_adaleval_evaluating_longcontext_llms/</guid>
      <pubDate>Fri, 12 Apr 2024 05:05:45 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] NeurIPS 2024 为高中生新增论文轨道</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1zesl/news_neurips_2024_adds_a_new_paper_track_for_high/</link>
      <description><![CDATA[NeurIPS 2024 为高中生添加了新的论文轨道 https://neurips.cc/Conferences/2024/CallforHighSchoolProjects  第三十八届神经信息处理系统年会 (NeurIPS 2024)一个跨学科会议，汇集了机器学习、神经科学、统计学、优化、计算机视觉、自然语言处理、生命科学、自然科学、社会科学和其他相邻领域的研究人员。  今年，我们邀请高中生提交有关机器学习社会影响主题的研究论文。将选出一部分决赛入围者以虚拟方式展示他们的项目，并将在 NeurIPS 主页上重点展示他们的作品。此外，最多五个获奖项目的主要作者将受邀参加在温哥华举行的 NeurIPS 2024 颁奖典礼。  每份提交的作品必须描述完全由高中生作者完成的独立作品。我们希望每份提交的内容都能突出显示已证明的积极社会影响或使用机器学习产生积极社会影响的潜力。    由   提交 /u/xiaohk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1zesl/news_neurips_2024_adds_a_new_paper_track_for_high/</guid>
      <pubDate>Fri, 12 Apr 2024 03:47:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] ReFT：语言模型的表示微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1nvwi/r_reft_representation_finetuning_for_language/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.03592 代码：https://github .com/stanfordnlp/pyreft 摘要：  参数高效微调（PEFT）方法寻求适应大的通过更新少量权重来调整模型。然而，许多先前的可解释性工作表明，表示编码了丰富的语义信息，这表明编辑表示可能是一种更强大的替代方案。在这里，我们通过开发一系列表示微调 (ReFT) 方法来追求这一假设。 ReFT 方法在冻结的基础模型上运行，并学习对隐藏表示的特定任务干预。我们定义了 ReFT 系列的一个强大实例，低秩线性子空间 ReFT (LoReFT)。 LoReFT 是现有 PEFT 的直接替代品，其学习干预措施的参数效率比之前最先进的 PEFT 高 10 至 50 倍。我们展示了 LoReFT 在八个常识推理任务、四个算术推理任务、Alpaca-Eval v1.0 和 GLUE 上的表现。在所有这些评估中，LoReFT 提供了效率和性能的最佳平衡，并且几乎总是优于最先进的 PEFT。我们在 此 https URL 公开发布通用 ReFT 训练库。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1nvwi/r_reft_representation_finetuning_for_language/</guid>
      <pubDate>Thu, 11 Apr 2024 19:30:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无限上下文变形金刚</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1l16l/r_infinite_context_transformers/</link>
      <description><![CDATA[我看了一下，没有在本文中看到任何看起来很有希望的讨论主题。  https://arxiv.org/abs/2404.07143  你的想法？这可能是 Gemini 1.5 报告的 10m 令牌上下文长度背后的技术之一吗？    由   提交 /u/Dyoakom   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1l16l/r_infinite_context_transformers/</guid>
      <pubDate>Thu, 11 Apr 2024 17:35:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>