<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 10 Oct 2024 21:16:38 GMT</lastBuildDate>
    <item>
      <title>[项目] Llama 3 8B 在文本理解方面表现不佳：有替代方案吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0tg1m/project_llama_3_8b_is_not_doing_well_at_text/</link>
      <description><![CDATA[嘿！我一直在尝试使用 Llama-3-8B-Instruct 来识别和提取各种产品的数量、描述和价格。由于文档结构不够好，所以正则表达式不是一种选择。由于我没有标记的数据集，所以 NER 不是一种选择。因此我选择了 LLM，但 Llama3 表现不佳。它不能很好地处理变化。我尝试过少样本和 CoT，但结果同样不令人满意。 除了要求公司为 GPT4 支付几百美元（这确实会做得很好）之外，我还有什么其他选择？有没有其他我可以本地运行的比这个版本的 Llama3 更强大的模型？ 谢谢！    提交人    /u/No_Possibility_7588   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0tg1m/project_llama_3_8b_is_not_doing_well_at_text/</guid>
      <pubDate>Thu, 10 Oct 2024 21:15:52 GMT</pubDate>
    </item>
    <item>
      <title>[p] 我写了一篇关于新款 Whisper Turbo 的博客文章</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0rp88/p_i_wrote_a_blog_post_about_the_new_whisper_turbo/</link>
      <description><![CDATA[嗨， 很多人对新的 whisper turbo 模型感兴趣，所以我写了一篇关于它的博客文章。  我们介绍了架构、训练策略以及如何与类似工作（如蒸馏耳语）进行比较。 请尝试一下，如果您有任何问题或反馈，请告诉我。 https://amgadhasan.substack.com/p/demystifying-openais-new-whisper    提交人    /u/Amgadoz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0rp88/p_i_wrote_a_blog_post_about_the_new_whisper_turbo/</guid>
      <pubDate>Thu, 10 Oct 2024 20:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] 细胞自动机驱动的镜像张量表面用于神经网络中的结构化扰动：一种通过基于状态的连续权重调制实现动态正则化、增强可塑性和多尺度学习的新方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/</link>
      <description><![CDATA[细胞自动机驱动的权重扰动：一种新的神经网络优化方法 摘要 当前的神经网络训练方法通常会导致模型虽然收敛，但可能无法代表给定数据集的最佳权重配置。传统方法（包括 dropout 和噪声注入）试图通过在训练期间引入随机性来解决此问题。但是，这些方法缺乏结构，可能无法有效地探索权重空间。这篇论文提出了一种新的训练范式，利用细胞自动机驱动的权重扰动来增强神经网络优化。 我们的方法引入了一个由连续状态细胞自动机控制的镜像张量表面，它在训练期间与网络的权重空间交互。完整的模型架构被复制到镜像中，其权重现在代表连续状态自动机的单元。该方法旨在提供结构化的多尺度扰动，这些扰动比均匀噪声更符合数据中的固有模式和网络学习到的表示。 推动这种方法的关键直觉包括：  结构化扰动有可能引导对权重空间进行更有意义的探索。 细胞自动机能够根据简单的规则生成复杂的突发行为。 多尺度效应的可能性可以增强跨不同抽象级别的特征学习。  我们假设这种方法解决了当前模型的几个限制：  克服局部最优：结构化扰动可能有助于模型摆脱次优收敛点。 增强泛化：通过促进更多样化的权重配置，该方法可以实现更好的泛化。 提高适应性：扰动的动态性质可能导致更多适应性模型。  这种方法可以看作是 dropout 的高级形式，提供更可控且可能更有益的正则化。与随机停用神经元的 dropout 不同，我们的方法对权重引入了结构化更改，从而有可能在鼓励探索的同时保留重要的学习特征。 直觉 可以在此处找到此方法背后的更多直觉 https://www.reddit.com/r/LocalLLaMA/comments/1fyx27y/im_pretty_happy_with_how_my_method_worked_out/lqzoqfg/ 但实质上： 我们推测，当前模型对可用的总权重计数利用不佳，并且在从均匀噪声初始化的模型上进行反向传播会导致巨大的表示冗余，导致...  由于冗余和轻微差异而产生幻觉。 由于后期层需要元结构来协商冗余以产生人类喜欢的连贯输出，因此收敛速度较慢。 由于需要协商结构来调解冗余，因此需要更大的模型。  使用更复杂的训练方法，我们假定可以使具有数百万个参数的模型的性能达到其数百倍大小的模型的水平。我们鼓励 /r/LocalLLaMA 社区尝试并运用这一概念。 未来研究方向 如果这一初步概念得到证实，那么我们希望通过学习驾驶自动机的策略网络来增强它，接收损失历史和质量评估作为输入以灌输反馈循环。使用更大的评估 LLM，可以动态探测训练中的模型的功能进展和“意识水平”，这样我们就有了一个元优化循环，我们可以在其中训练这个策略网络以越来越有效的方式驾驶扰动。LR 可以增加，从而能够越来越快地收敛回功能模型。 CSA 本身可以交换为具有动态学习规则的神经状态自动机。扩散模型中嵌入的洞察力也可以进行微调并重新用于时间模式生成器，而是使用文本提示来制作整个可能的动态领域。    提交人    /u/ryunuck   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0rhsx/r_cellular_automatondriven_mirrored_tensor/</guid>
      <pubDate>Thu, 10 Oct 2024 19:50:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] nGPT：在超球面上进行表征学习的正则化 Transformer</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/</link>
      <description><![CDATA[      论文： https://arxiv.org/pdf/2410.01131 摘要：  我们提出了一种新颖的神经网络架构，即在超球面上进行表征学习的规范化 Transformer (nGPT)。在 nGPT 中，形成嵌入、MLP、注意矩阵和隐藏状态的所有向量都是单位范数规范化的。输入的 token 流在超球面的表面上传播，每一层都会向目标输出预测贡献一个位移。这些位移由 MLP 和注意块定义，它们的向量分量也位于同一个超球面上。实验表明，nGPT 的学习速度更快，可将实现相同准确度所需的训练步骤数减少 4 到 20 倍，具体取决于序列长度。  亮点：  我们的主要贡献如下：  超球面上的网络参数优化 我们建议将形成网络矩阵嵌入维度的所有向量归一化为位于单位范数超球面上。这使我们能够将矩阵向量乘法视为表示余弦相似度在 [-1,1] 内的点积。归一化使得权重衰减变得不必要。  归一化 Transformer 作为超球面上的可变度量优化器 归一化 Transformer 本身在超球面上执行多步优化（每层两步），其中注意力和 MLP 更新的每一步都由特征学习率（可学习可变度量矩阵的对角线元素）控制。对​​于输入序列中的每个标记 t_i，归一化 Transformer 的优化路径从超球面上与其输入嵌入向量相对应的点开始，并移动到超球面上最能预测下一个标记 t_i+1 的嵌入向量的点。 更快的收敛 我们证明，归一化 Transformer 将实现相同准确度所需的训练步骤数减少了 4 到 20 倍。 视觉亮点： https://preview.redd.it/0jdj23ew6ytd1.png?width=1313&amp;format=png&amp;auto=webp&amp;s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df 不确定 20k 和 200k 预算之间的区别；可能绘制了使用不同初始学习率的运行的最佳结果 https://preview.redd.it/waof2llr7ytd1.png?width=1337&amp;format=png&amp;auto=webp&amp;s=3f82cee29c5fe753e219edf55ab16460fcf9a11a https://preview.redd.it/a5vburms7ytd1.png?width=859&amp;format=png&amp;auto=webp&amp;s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/</guid>
      <pubDate>Thu, 10 Oct 2024 15:37:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 感谢这个 subreddit，我们终于推出了我们的应用程序；ArtVista</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0jwdr/p_thanks_to_this_subreddit_we_finally_launched/</link>
      <description><![CDATA[伙计们，两年前大家都是从这篇帖子开始的 这个 subreddit 在我心中占有特殊的位置，我知道这不是自我宣传或推广应用程序的地方，如果不允许，我会删除它。但如果你查看我的个人资料并查看我过去的帖子和评论，你会看到这个 subreddit 在我构建我的第一个产品（包括我所有愚蠢的问题）的过程中有多么有价值。 如果我不在这里发布我的问题，我就不会学到我目前在我的应用程序中使用的许多关键技术和想法，所以再次感谢大家，特别是感谢这里的计算机视觉人员。如果我知道今天的计算机视觉在某种程度上意味着生产，我就会在这里了解在哪里查找和搜索所有内容。以下文字是我在一些应用程序 subreddits 上发布的内容，请尝试一下，我欢迎任何类型的问题和反馈。再次感谢，请欣赏以下描述该应用程序的文字 :) 2022 年 11 月，我的一个朋友对他的博物馆体验感到疯狂。他问我我们是否可以构建一个应用程序来正确识别艺术品，而我正在寻找工作，所以我想提升我的 GitHub 页面。我以为这需要 4-5 天的时间，但我花了一年的时间才构建后端。 因此，ArtVista 就像艺术品的 Shazam + 您可以向 ArtVista 询问有关您正在处理的艺术品的问题。它就像一个数字指南和跟踪器，可以跟踪您访问和看到的内容。以下列表基本上就是您应该尝试我们的应用的原因 :)  时尚的用户界面： 我们追求简洁、简单且易于使用的设计（我希望我们能够做到这一点）。我们的目标是让您在学习艺术的同时，不会被 Google Lens 带来的一堆杂乱信息所困扰。 我们自己的 ML 模型（不仅仅是 ChatGPT 包装器）： 使用应用的相机，您可以识别艺术品，甚至可以像向现实生活中的导游提问一样提出问题。嘿，如果你请不起私人导游（至少在西欧我们请不起，真遗憾哈哈哈哈），我们可能是下一个最好的选择。 每日推荐*：我们的应用会根据您的品味推荐艺术品，但也会推荐您可能不熟悉的领域，以及您可能从未见过的不同艺术风格。* 隐私： 我们不会收集数据来训练我们的模型——我的意思是，我们太小了。我们关心隐私，因为我个人讨厌所有这样做的人。  ArtVista 是免费的——没有广告，没有订阅（目前）。当我们推出高级功能时，如果我们负担得起，我们希望为早期采用者提供免费升级！您可以在此处下载  说实话，开发这个应用既有趣又忙碌。后端很复杂，让机器学习模型在设备上工作简直是地狱般的折磨！我们已经在这个应用上工作了将近两年，所以如果您能尝试一下并摆弄它，那将意义重大。 任何建设性的反馈意见都非常感谢。 有趣的事实：您甚至可以使用相机对着您的朋友，看看他们最像哪幅画 :) 感谢您的时间 :))    提交人    /u/TutubanaS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0jwdr/p_thanks_to_this_subreddit_we_finally_launched/</guid>
      <pubDate>Thu, 10 Oct 2024 14:19:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 VAE 为生成模型提供潜在空间的优缺点是什么？（尤其是对于图像或视频）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/</link>
      <description><![CDATA[我认为变分自动编码器 (VAE) 是一种使某些类型的生成模型（潜在扩散、潜在一致性模型、潜在流模型）在当前硬件限制下工作的黑客手段。 但我也理解一些同事的观点，他们认为 VAE 提供的压缩表示迫使生成模型变得高效，专注于正确建模数据分布的重要因素。 目前最先进的视频生成模型几乎都使用某种压缩表示（通常是 VAE）。所以，它们是有效的。但它们真的有必要吗？ 你对此有何看法？VAE 是拐杖吗？还是生成模型的重要组成部分？    提交人    /u/pm_me_your_pay_slips   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0jpzq/d_what_are_the_pros_and_cons_of_using_a_vae_to/</guid>
      <pubDate>Thu, 10 Oct 2024 14:10:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] Rodimus*：通过高效注意力机制打破准确率与效率之间的矛盾</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0hi2m/r_rodimus_breaking_the_accuracyefficiency/</guid>
      <pubDate>Thu, 10 Oct 2024 12:20:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有在线平台可以测试 Falcon 模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0h0i9/d_is_there_any_online_platform_to_test_falcon/</link>
      <description><![CDATA[大家好！我正在定制数据集中测试 LLM 功能。我选择使用 AWS 来测试 Anthropic 和 Meta 模型，但我找不到任何适用于 Falcon 模型的内容。我确实尝试使用 Python 脚本（由于硬件要求，仅限 Falcon 7B）并指定特定提示来运行 Falcon 模型，但这花费了很长时间，而且我对输出结果并不满意。有人测试过 Falcon 的更大模型吗？ 谢谢！    提交人    /u/Fresh_Plant_2653   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0h0i9/d_is_there_any_online_platform_to_test_falcon/</guid>
      <pubDate>Thu, 10 Oct 2024 11:53:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 混沌边缘的智慧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g0elvw/r_intelligence_at_the_edge_of_chaos/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g0elvw/r_intelligence_at_the_edge_of_chaos/</guid>
      <pubDate>Thu, 10 Oct 2024 09:08:13 GMT</pubDate>
    </item>
    <item>
      <title>[N] Jurgen Schmidhuber 谈 2024 年诺贝尔物理学奖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/</link>
      <description><![CDATA[2024 年诺贝尔物理学奖授予 Hopfield 和 Hinton，奖励计算机科学领域的剽窃和错误归因。它主要与 Amari 的“Hopfield 网络”和“玻尔兹曼机”有关。  具有类似神经元元素的 Lenz-Ising 循环架构于 1925 年发布。1972 年，Shun-Ichi Amari 使其具有自适应性，以便它可以通过改变其连接权重来学习将输入模式与输出模式相关联。然而，Amari 仅在“2024 年诺贝尔物理学奖的科学背景”中被简要提及。不幸的是，Amari 的网络后来被称为“Hopfield 网络”。 10 年后，Hopfield 重新发表了该论文，但未引用 Amari，甚至在后来的论文中也没有引用。 Ackley、Hinton 和 Sejnowski (1985) 撰写的相关玻尔兹曼机论文是关于在神经网络 (NN) 的隐藏单元中学习内部表征 [S20]。它没有引用 Ivakhnenko &amp; Lapa 提出的用于深度学习内部表征的第一个有效算法。它没有引用 Amari 的独立工作 (1967-68)，即通过随机梯度下降 (SGD) 端到端学习深度 NN 中的内部表征。甚至作者后来的调查和“2024 年诺贝尔物理学奖的科学背景”都没有提到深度学习的起源。 （[BM] 也没有引用 Sherrington &amp; Kirkpatrick &amp; Glauber 的相关先前工作） 诺贝尔委员会还赞扬了 Hinton 等人 2006 年提出的深度 NN 的分层预训练方法 (2006)。然而，这项工作既没有引用 Ivakhnenko &amp; Lapa 最初提出的深度 NN 的分层训练，也没有引用深度 NN 的无监督预训练的原始工作 (1991)。 “热门信息”称：“20 世纪 60 年代末，一些令人沮丧的理论结果导致许多研究人员怀疑这些神经网络永远不会有任何实际用途。”然而，深度学习研究在 20 世纪 60 年代至 70 年代显然非常活跃，尤其是在英语国家之外。 在以下参考文献 [DLP] 中可以找到许多其他抄袭和错误引用的案例，其中还包含上述其他参考文献。可以从第 3 节开始：J. Schmidhuber (2023)。3 位图灵奖获得者如何重新发布他们未注明其创造者的关键方法和想法。技术报告 IDSIA-23-23，瑞士人工智能实验室 IDSIA，2023 年 12 月 14 日。https://people.idsia.ch/~juergen/ai-priority-disputes.html… 另请参阅以下参考文献 [DLH] 了解该领域的历史：[DLH] J. Schmidhuber (2022)。现代人工智能和深度学习的注释历史。技术报告 IDSIA-22-22，IDSIA，瑞士卢加诺，2022 年。预印本 arXiv:2212.11279。 https://people.idsia.ch/~juergen/deep-learning-history.html…（这延伸了 2015 年获奖调查https://people.idsia.ch/~juergen/deep-learning-overview.html…）  Twitter 帖子链接：https://x.com/schmidhuberai/status/1844022724328394780?s=46&amp;t=Eqe0JRFwCu11ghm5ZqO9xQ    提交人    /u/optimization_ml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzw5b1/n_jurgen_schmidhuber_on_2024_physics_nobel_prize/</guid>
      <pubDate>Wed, 09 Oct 2024 16:52:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 那个为这里的帖子制作分类器的人怎么样了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fzstxw/d_what_happened_to_the_guy_who_was_making_a/</link>
      <description><![CDATA[不太确定还可以在哪里发布此内容，如果不允许，请删除。 我想几年前有一个人想构建一个机器人，对这里的帖子进行文本分类，以便更好地标记问题是否合适。 发生了什么？有人知道吗？我很好奇 Reddit 的新 API 定价是否会影响这一点。    提交人    /u/Seankala   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fzstxw/d_what_happened_to_the_guy_who_was_making_a/</guid>
      <pubDate>Wed, 09 Oct 2024 14:32:53 GMT</pubDate>
    </item>
    <item>
      <title>[N] 2024 年诺贝尔化学奖将颁给 Google Deepmind 的 AlphaFold 的发明者。其中一半颁给 David Baker，另一半则颁给 Demis Hassabis 和 John M. Jumper。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/</link>
      <description><![CDATA[公告：https://twitter.com/NobelPrize/status/1843951197960777760    由   提交  /u/aagg6   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fznxyr/n_the_2024_nobel_prize_in_chemistry_goes_to_the/</guid>
      <pubDate>Wed, 09 Oct 2024 10:09:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么机器学习研究中的统计分析如此之少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/</link>
      <description><![CDATA[为什么在机器学习研究中不进行任何统计测试来验证结果是否真正重要是如此普遍？大多数情况下，只呈现一个结果，而不是进行多次运行并执行 t 检验或 Mann Whitney U 检验等。在其他学科（如心理学或医学）中，基于单个样本得出结论是不可能的，为什么这在机器学习研究中不被视为问题？ 此外，有人可以推荐一本专门介绍机器学习背景下的统计测试的书吗？    提交人    /u/BommiBrainBug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fznaa9/d_why_is_there_so_little_statistical_analyses_in/</guid>
      <pubDate>Wed, 09 Oct 2024 09:21:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fxif7x/d_simple_questions_thread/</guid>
      <pubDate>Sun, 06 Oct 2024 15:00:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>