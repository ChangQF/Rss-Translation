<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 28 Jan 2024 03:13:12 GMT</lastBuildDate>
    <item>
      <title>[D] 为什么我们一直称“生成”模型为“生成”模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acsq74/d_why_do_we_keep_calling_generation_models/</link>
      <description><![CDATA[我认为生成模型模拟了联合概率分布，而判别模型模拟了条件概率。 当我们执行文本或图像时一代，我们不是为模型提供某种输入来进行调节吗？难道这些不应该被称为“一代模型”吗？因为它们本质上具有歧视性，但正在执行生成任务？   由   提交 /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acsq74/d_why_do_we_keep_calling_generation_models/</guid>
      <pubDate>Sun, 28 Jan 2024 03:10:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 组合 ML 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acqwrm/d_combine_ml_models/</link>
      <description><![CDATA[我使用预训练模型作为特征提取器，并在此之上构建了另一个模型。现在我有两个模型，我想将它们保存为一个。可能吗？   由   提交/u/malambo2  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acqwrm/d_combine_ml_models/</guid>
      <pubDate>Sun, 28 Jan 2024 01:36:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 变分自动编码器已经 10 岁了</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acqgn9/d_the_variational_autoencoder_is_now_10_years_old/</link>
      <description><![CDATA[我感觉自己老了哈哈。  不过，严肃地说，作为深度生成建模的实用选择，它似乎经受住了时间的考验。相比之下，GAN 研究似乎已经变得停滞不前，流、基于能量的模型和基于扩散/基于分数的模型正在被纳入 VAE 中，以实现更具表现力的先验。我坚信 VAE 在未来很长一段时间内仍然有用。 只是一个想法。   由   提交/u/Chromobacteria  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acqgn9/d_the_variational_autoencoder_is_now_10_years_old/</guid>
      <pubDate>Sun, 28 Jan 2024 01:14:35 GMT</pubDate>
    </item>
    <item>
      <title>[D][R] 进入光子学和机器学习领域</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acqa5i/dr_getting_into_the_field_of_photonics_and/</link>
      <description><![CDATA[我最近开始阅读有关光子学的内容，以及如何将机器学习应用于该领域来解决现实世界中的许多问题例如数据存储和分析问题（从延迟到获取数据的速度……），到在光发挥作用时使用它来超越物质限制来发现我们周围的世界。  机器学习 (DL) 可以让我们理解光子结构及其相互作用的方式，并将它们与光学响应联系起来，而无需深入研究我仍然不理解的复杂细节（我知道 ML、计算机科学、数据科学和我在学术研究领域做了一些研究，而不是搜索谷歌哈哈）（对光子学来说是个新手，因为我之前在机器学习的背景下没有听说过它，它只是没有闪过我的脑海）。  所以我想请那些了解该领域或对此做过一些研究的人提供一些想法，如果可能的话，告诉我如果我真的想深入该领域，我应该从哪里开始（是的）我可以自己去google看看，研究论文或者视频，但是有前辈的知识可以让我避免浪费很多时间，走很多不必要的弯路）。 P.S.我刚刚开始阅读这篇文章，所以如果我写的内容有任何问题，请告诉我，不要评判。   由   提交/u/HotColdPeople  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acqa5i/dr_getting_into_the_field_of_photonics_and/</guid>
      <pubDate>Sun, 28 Jan 2024 01:05:49 GMT</pubDate>
    </item>
    <item>
      <title>[P]人工智能驱动的投资银行幻灯片：自动化繁琐的工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acm9qy/p_aipowered_investment_banking_slides_automating/</link>
      <description><![CDATA[大家好！ 我是一名前投资银行家，最近推出了一款新产品，可以使用法学硕士自动化财务 PowerPoint。&lt; /p&gt; 查看我们的网站免费试用：https://www.lucite.ai 我们将不胜感激任何反馈！   由   提交/u/Helpful-Analyst7140  /u/Helpful-Analyst7140 reddit.com/r/MachineLearning/comments/1acm9qy/p_aipowered_investment_banking_slides_automating/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acm9qy/p_aipowered_investment_banking_slides_automating/</guid>
      <pubDate>Sat, 27 Jan 2024 22:03:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 博士期间使用数学进行更好的机器学习研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aclw7r/d_math_for_better_ml_research_during_phd/</link>
      <description><![CDATA[嘿伙计们，我希望更好地掌握一些更高级的数学，以便能够更好地进行深度学习研究。和其他许多人一样，我已经对概率论、线性代数、微积分和信息论有了很好的理解。我正在尝试建立一个更好的技术工具箱，可以用来解决开放问题。  寻找对你攻读博士学位期间有帮助的建议（如果你也能推荐书籍，那将会很有帮助）。   由   提交 /u/SufficientAd3564   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aclw7r/d_math_for_better_ml_research_during_phd/</guid>
      <pubDate>Sat, 27 Jan 2024 21:47:08 GMT</pubDate>
    </item>
    <item>
      <title>[N][P] 各种国际象棋语言模型新闻，包括发布据称 Elo 高达 1500 的开源语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aclto7/np_various_chess_language_model_news_including/</link>
      <description><![CDATA[国际象棋语言模型新闻： a) Chess-GPT：开源语言模型，据称 Elo 高达 1500。其中包括一些神经网络可解释性材料。开发者 - u/seraine - 创建了有关此 此处和此处 &gt;. b) 博客文章 揭穿棋盘：用 GPT 对抗国际象棋引擎来估计 Elo 评级并评估合法移动能力：一位计算机科学教授对 4 种语言模型进行的国际象棋测试。测试的性能最好的语言模型是 gpt-3.5-turbo-instruct，估计 Elo 为 1750 +/- 50，非法移动尝试率约为千分之一。 我之前在本子中发表的关于 gpt-3.5-turbo-instruct 下棋的文章。。 p&gt; c) Subreddit r/LLMChess 最近创建。   由   提交 /u/Wiskkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aclto7/np_various_chess_language_model_news_including/</guid>
      <pubDate>Sat, 27 Jan 2024 21:44:07 GMT</pubDate>
    </item>
    <item>
      <title>[P] 快速提问</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acjw07/p_quick_question/</link>
      <description><![CDATA[嘿伙计们，这里是医学生。抱歉，我是个新手，谁能告诉我关注 Sentdex 的《Python、TensorFlow 和 Keras 深度学习教程播放列表》 是否安全，因为它已有 5 年历史了。我不知道我不想陷入困境，因为我自己还不知道故障排除。谢谢    由   提交 /u/Subject_Lab_6013   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acjw07/p_quick_question/</guid>
      <pubDate>Sat, 27 Jan 2024 20:17:09 GMT</pubDate>
    </item>
    <item>
      <title>您如何协调人工智能的巅峰炒作与艰难的人工智能就业市场？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acjty3/how_do_you_reconcile_peak_hype_in_ai_with_a_tough/</link>
      <description><![CDATA[ DeepMind 联合创始人表示“我们已经在人工智能革命中达到了炒作的顶峰”。1&lt; /li&gt; 此外，美国的失业率处于历史低位，为 3.7%。 然而，我不断听说人工智能研究人员和从业者的就业市场目前非常艰难。&lt; /li&gt;  如何协调这三者？  1 https://www.youtube.com/watch?v=Go_6UldZL50   由   提交/u/we_are_mammals  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acjty3/how_do_you_reconcile_peak_hype_in_ai_with_a_tough/</guid>
      <pubDate>Sat, 27 Jan 2024 20:14:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] DeepSeek-Coder：当大语言模型遇上编程——代码智能的崛起 - DeepSeek-AI 2024 - 超越GPT-3.5和Codex的SOTA开源编码模型，同时在研究和商业用途上不受限制！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acjpp1/r_deepseekcoder_when_the_large_language_model/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2401.14196  Github：https： //github.com/deepseek-ai/DeepSeek-Coder  模型：https://huggingface。 co/deepseek-ai  摘要：  大型语言模型的快速发展彻底改变了软件开发中的代码智能。然而，闭源模型的主导地位限制了广泛的研究和开发。为了解决这个问题，我们推出了 DeepSeek-Coder 系列，这是一系列大小从 1.3B 到 33B 的开源代码模型，在 2 万亿个代币上从头开始训练。这些模型在高质量的项目级代码语料库上进行了预训练，并采用 16K 窗口的填空任务来增强代码生成和填充。我们的广泛评估表明，DeepSeek-Coder 不仅在多个基准测试中实现了开源代码模型中最先进的性能，而且还超越了 Codex 和 GPT-3.5 等现有的闭源模型。此外，DeepSeek-Coder 模型享有宽松的许可证，允许研究和不受限制的商业用途。   https://preview.redd.it/adspck4uh1fc1.jpg?width=1505&amp;format=pjpg&amp;auto=webp&amp;s = 94970f9bd5db45bf4be9f206355c8f2a4545dcc3 https： //preview.redd.it/7cm8hk4uh1fc1.jpg?width=1659&amp;format=pjpg&amp;auto=webp&amp;s=cba202f43a220492209b1ece030f7a76b080212a https://preview.redd.it/8jobgk4uh1fc1.jpg?width=1535&amp;format=pjpg&amp;auto =webp&amp; s=62065c3855e5abf329f3df46414e5c50fd293b66  https://preview.redd.it/mtoq8n4uh1fc1.jpg?width=1524&amp;format=pjpg&amp;auto=webp&amp;s=96130d9578a11f21d03a0bd6755e6a2c0034b4c5 https://preview.redd.it/tc032n4uh1fc1.jpg?width=1698&amp;format=p jpg&amp;自动= webp&amp;s=f29bd294ec63257ad2f7c1b3725657f53d955de2   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acjpp1/r_deepseekcoder_when_the_large_language_model/</guid>
      <pubDate>Sat, 27 Jan 2024 20:09:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 文本引导语音编辑的最佳项目是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acjoh1/d_what_are_the_best_projects_for_text_guided/</link>
      <description><![CDATA[似乎这个区域没有太多“真实世界”的信息 尝试使用 https://github.com/Zain-Jiang/Speech-Editing -工具包和https://github.com/PaddlePaddle/PaddleSpeech/tree/develop /examples/vctk/ernie_sat ，两者都生成了不太引人注目的输出。 有人有使用其中一种实现的经验吗？   由   提交/u/artm_ai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acjoh1/d_what_are_the_best_projects_for_text_guided/</guid>
      <pubDate>Sat, 27 Jan 2024 20:08:12 GMT</pubDate>
    </item>
    <item>
      <title>[D]“特征稀释”是深度神经网络中公认的现象以及如何应对它</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acfyh2/d_is_feature_dilution_a_recognised_phenomenon_in/</link>
      <description><![CDATA[我一直在努力应对与数据集成和多模式神经网络相关的挑战，我希望得到您的见解。场景如下：我有一个包含多种类型特征的特征矩阵，包括 0 到 1 范围内的 5 个连续变量。此外，我将一个 1024 维的嵌入向量连接到同一个特征矩阵中，其中嵌入值为也是连续的。 我担心的是高维嵌入特征的存在是否会削弱原始 5 个连续变量的效果或重要性。这是一种公认​​的现象吗？如果是，如何解决或对抗这种潜在的稀释效应？ 我很欣赏有关此主题的相关文献的任何指导或参考。预先感谢您的专业知识！ P.S.阅读一些评论后，一些额外的背景信息：一般来说，该模型应该能够仅使用 5 个特征就能够表现良好。我已经确认了这一点。嵌入的作用是提供上下文信息，使预测从“总体良好”变为“在特定上下文中良好”。我知道这可能有点模糊，但在不深入了解我的建模任务的细节的情况下，这是我能做的最好的事情。    由   提交 /u/Primary-Wasabi292    reddit.com/r/MachineLearning/comments/1acfyh2/d_is_feature_dilution_a_recognised_phenomenon_in/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acfyh2/d_is_feature_dilution_a_recognised_phenomenon_in/</guid>
      <pubDate>Sat, 27 Jan 2024 17:26:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 梯度累积不应与不同的序列长度一起使用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acbzrx/d_gradient_accumulation_should_not_be_used_with/</link>
      <description><![CDATA[我正在训练一个占用大量内存的模型，以至于我只能使用批量大小 1 并累积 N 次。如果您考虑一下，这意味着优化将更加重视较小的序列长度。下面是一个示例： 假设您在一个批次中有 2 个序列。序列 1 有 7 个标记，序列 2 有 10 个。这意味着我需要用 3 个填充标记来填充序列 1。如果我在这里使用梯度累积，最终损失将是损失 7 个令牌/7 + 损失 10 个令牌/10。如果我使用的批量大小为 2，则损失将是 17 个令牌/17。很容易看出两者都不是相同，这会引入对较小序列长度的偏见。 我能想到解决这个问题的唯一方法就是“打包”将类似的序列长度放在一起，并且仅对这些打包序列进行混洗，而不是“单独”进行混洗。序列。我将按序列长度对数据集进行排序，并批量制作相似大小的序列。因此，10-15 的序列长度可能是一个批次，16-20 的序列长度可能是另一批次，依此类推……我只对这些批次进行洗牌。这有道理吗？这会引入一些我不知道的其他偏见吗？ 编辑：我刚刚提出了另一个想法，该想法实施起来会稍微困难一些，但可能是有效的。仅当我对损失使用平均减少时，上述内容才有效（为什么我将每个序列的损失除以其中的令牌数量）。但是因为我也在使用梯度裁剪，所以删除损失减少是否有意义（这将是所有令牌损失的总和）？如果我没有弄错的话，与常规的完整批次相比，梯度裁剪会给我完全相同的结果，对吧？它只是渐变的缩放因子，然后通过渐变裁剪将其删除，对吧？   由   提交 /u/AromaticCantaloupe19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acbzrx/d_gradient_accumulation_should_not_be_used_with/</guid>
      <pubDate>Sat, 27 Jan 2024 14:28:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我的兴趣与典型机器学习工程师的日常职责有交叉吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1acasly/d_do_my_interests_intersect_with_the_day_to_day/</link>
      <description><![CDATA[这似乎是一个相当广泛的立场，所以我试图弄清楚我的热情和 ML 工程师在他们的实际工作中是否有重叠。日复一日。 我的兴趣：  性能至关重要的低级编程。用于快速操作的 GPU 和 SIMD 对 DL 有一定的热情，但不是太多，因为它对我来说显得太黑箱 我对在消费硬件 GGML 上运行模型非常着迷， LLama.cpp 对经典算法充满热情  我也擅长数学，并且希望解决一些问题。在 React 和 SaaS 上工作通常不会涉及太多这些。   由   提交 /u/ThrowayGigachad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1acasly/d_do_my_interests_intersect_with_the_day_to_day/</guid>
      <pubDate>Sat, 27 Jan 2024 13:26:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/196j1w0/d_simple_questions_thread/</guid>
      <pubDate>Sun, 14 Jan 2024 16:00:18 GMT</pubDate>
    </item>
    </channel>
</rss>