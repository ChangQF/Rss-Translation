<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 03 Dec 2023 21:11:11 GMT</lastBuildDate>
    <item>
      <title>[D] 大型语言模型（LLM）的推理能力</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a1vij/d_reasoning_capabilities_of_large_language_models/</link>
      <description><![CDATA[我们如何增强大型语言模型 (LLM) 的推理能力，以获得更有效、更细致的结果？   由   提交 /u/Inknown-Belt8671    reddit.com/r/MachineLearning/comments/18a1vij/d_reasoning_capability_of_large_language_models/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a1vij/d_reasoning_capabilities_of_large_language_models/</guid>
      <pubDate>Sun, 03 Dec 2023 20:03:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] Google DeepMind：使用 GNN 发现了 220 万种新材料（38 万种最稳定，736 种已在实验室中验证）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a0mga/r_google_deepmind_22_million_new_materials/</link>
      <description><![CDATA[（自从上一篇文章被删除后重新发布，我想是因为非 Arxiv 帖子只允许在周末发布，现在是周末） 材料发现至关重要但也很困难。新材料可实现电池或 LED 等重大创新。但有无数种组合可供尝试。对它们进行实验测试既缓慢又昂贵。 因此科学家和工程师希望首先在计算机上模拟和筛选材料。这可以在实际实验之前检查更多的候选者。然而，模型历来难以准确预测材料是否稳定。 DeepMind 的研究人员开发了一个名为 GNoME 的系统，该系统使用图神经网络和主动学习来突破这些限制。 GNoME将材料的晶体结构建模为图表并预测形成能。它主动生成和筛选候选者，通过模拟评估最有前途的候选者。这扩展了它的知识并改进了多个周期的预测。 作者引入了生成尊重对称性的衍生结构的新方法，进一步使发现多样化。 结果： &lt; ol&gt; GNoME 发现了 220 万种新的稳定材料 - 相当于 800 年的正常发现时间。 其中 38 万种是最稳定的，也是可供验证的候选材料。 736 种是最稳定的材料。在外部实验室进行了验证。其中包括一种全新的类金刚石光学材料和另一种可能是超导体的材料。  总的来说，这证明了扩大深度学习如何能够极大地加速材料创新。随着数据和模型的共同改进，它将加速解决需要新工程材料的重大问题。 TLDR：DeepMind 开发了一个人工智能系统，该系统使用图形神经网络来发现可能的新材料。它发现了 220 万候选者，其中超过 30 万是最稳定的。已经综合了 700 多个。 ​​完整摘要请参见此处。论文此处。   由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/18a0mga/r_google_deepmind_22_million_new_materials/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a0mga/r_google_deepmind_22_million_new_materials/</guid>
      <pubDate>Sun, 03 Dec 2023 19:08:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我想创建一个文本生成器。我应该从哪里开始？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18a008w/d_i_want_to_create_a_text_generator_where_should/</link>
      <description><![CDATA[我想创建一个人工智能内容生成器。我应该从哪里开始 我曾在稳定扩散和automatic1111工作过。现在我想创建一个内容写作应用程序。我该从哪里开始呢。我有 8gb VRAM 我需要一个免费的开源 我研究过文本生成器，我看到有很多模型。我不知道哪个会更好地完美地创建简历或简历内容 有人告诉我从哪里开始   由   提交 /u/Prestigious-Head-388   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18a008w/d_i_want_to_create_a_text_generator_where_should/</guid>
      <pubDate>Sun, 03 Dec 2023 18:41:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 再问一下 OpenAI Whisper 哪个版本最高效</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189zt5d/d_asking_again_about_which_is_the_most_efficient/</link>
      <description><![CDATA[阅读这篇文章后并且我自己做了一些研究，我还没有找到关于哪个版本的 Whisper 最有效的明确结论。对此主题是否达成了坚实的共识？   由   提交 /u/iGunzerkeR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189zt5d/d_asking_again_about_which_is_the_most_efficient/</guid>
      <pubDate>Sun, 03 Dec 2023 18:33:36 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] 文本转语音变得越来越好 - HierSpeech++、XTTS 和 StyleTTS2！ （抱脸）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189zf6w/news_text_to_speech_is_getting_crazy_good/</link>
      <description><![CDATA[嘿， 人工智能最近变得很疯狂，事情变化得非常快。我创建了一个视频，涵盖了一些流行的 TTS（文字转语音）公开的拥抱空间，请查看！ https://www.youtube.com/watch?v=4-2Jk8muo7c 我似乎迫不及待地想在未来的视频中开始测试这些模型应用程序的用法就像这项技术终于开始获得动力一样！ 让我知道您的想法，或者如果您对其他视频有任何疑问/请求， 干杯   由   提交/u/dev-spot  /u/dev-spot  reddit.com/r/MachineLearning/comments/189zf6w/news_text_to_speech_is_getting_crazy_good/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189zf6w/news_text_to_speech_is_getting_crazy_good/</guid>
      <pubDate>Sun, 03 Dec 2023 18:15:59 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人可以帮助我理解如何将向量 A 与向量 B 旋转某个角度（两者的大小均为 512）吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189za5o/d_can_someone_help_me_understand_how_to_rotate_a/</link>
      <description><![CDATA[有人可以帮助我理解如何将向量 A 与向量 B 旋转某个角度（两者的大小均为 512）吗？  我有一个特征向量，它与正在更新的权重的基本向量进行比较，以计算两个向量之间的角度并与地面实况进行比较。我想通过将基向量移动某个角度来重建特征向量，我不确定在高维空间中的旋转是否有意义，或者尝试学习重建是否更好   由   提交 /u/SnooChipmunks2237   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189za5o/d_can_someone_help_me_understand_how_to_rotate_a/</guid>
      <pubDate>Sun, 03 Dec 2023 18:09:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] FaceSwap 生成模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189yrvr/p_faceswap_generation_model/</link>
      <description><![CDATA[任何使用基于深度学习的 Faceswap Generation 模型的人。我想知道如何在数据集以及自定义数据集上运行、训练模型。如何优化模型。很简单，有人研究过 Faceswap Generation 模型吗？    由   提交 /u/Difficult_Vanilla814   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189yrvr/p_faceswap_generation_model/</guid>
      <pubDate>Sun, 03 Dec 2023 17:46:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我在哪里可以与纽约市的机器学习研究人员联系？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189y3vi/d_where_can_i_connect_with_machine_learning/</link>
      <description><![CDATA[大家好，我对机器学习比较陌生，但我发现它非常有趣，作为一名软件工程师，我可以熟悉我的机器学习中的基本概念。头很快。  但是，我没有任何人可以亲自与我交谈并参与围绕不同主题的讨论。 Reddit 很好，我很欣赏这个子论坛上的帖子，但我只是想找到一群可以一起交流想法的人。这是一回事吗？ 谢谢！   由   提交/u/Text-Agitated  /u/Text-Agitated reddit.com/r/MachineLearning/comments/189y3vi/d_where_can_i_connect_with_machine_learning/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189y3vi/d_where_can_i_connect_with_machine_learning/</guid>
      <pubDate>Sun, 03 Dec 2023 17:15:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 论文分析 - 从（生产）语言模型中可扩展地提取训练数据（视频演练）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189y2cv/d_paper_analysis_scalable_extraction_of_training/</link>
      <description><![CDATA[https://youtu.be/KwpeuqT69fw 研究人员只需要求 ChatGPT 多次重复一个单词，就可以从 ChatGPT 中获取大量训练数据，这会导致模型出现分歧并开始吐出记忆的文本。 为什么会出现这种情况？这些模型真正逐字记住了多少训练数据？ ​ 概要： 0:00 - 简介 8:05 - 可提取与可发现的记忆 14:00 - 模型泄漏的数据比之前想象的更多 20:25 - 某些数据是可提取的，但不可发现 25:30 - 从封闭模型中提取数据 30:45 - 诗诗诗 37:50 - 定量隶属度测试 40: 30 - 进一步探索 ChatGPT 漏洞 47:00 - 结论 ​ 论文：https://arxiv.org/abs/2311.17035   由   提交/u/ykilcher  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189y2cv/d_paper_analysis_scalable_extraction_of_training/</guid>
      <pubDate>Sun, 03 Dec 2023 17:13:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 在规划中结合空间和时间抽象以实现更好的泛化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189qtnk/r_combining_spatial_and_temporal_abstraction_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.00229 OpenReview：https:// /openreview.net/forum?id=eo9dHwtTFt 代码：  https://github.com/mila-iqia/skipper 博客文章：http://mingde.world/combining-spatial-and-temporal-abstraction-in-planning/ 摘要:  受人类意识规划的启发，我们提出了Skipper，这是一种基于模型的强化学习代理，它利用空间和时间抽象来概括所学技能新颖的情况。它自动将手头的任务分解为更小规模、更易于管理的子任务，从而实现稀疏决策并将其计算集中在环境的相关部分。这依赖于表示为有向图的高级代理问题的定义，其中使用事后知识端到端地学习顶点和边。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法预计会有所帮助的地方。与现有最先进的分层规划方法相比，以泛化为中心的实验验证了 Skipper 在零样本泛化方面的显着优势。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189qtnk/r_combining_spatial_and_temporal_abstraction_in/</guid>
      <pubDate>Sun, 03 Dec 2023 10:28:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] TSMixer：用于预测的时间序列混合器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189nx1e/p_tsmixer_time_series_mixer_for_forecasting/</link>
      <description><![CDATA[github 链接 TSMixer 是一个基于 PyTorch 的 TSMixer 架构实现，如 TSMixer 论文所述。它利用混合器层来处理时间序列数据，为标准和扩展预测任务提供强大的方法。   由   提交/u/Yossarian_1234   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189nx1e/p_tsmixer_time_series_mixer_for_forecasting/</guid>
      <pubDate>Sun, 03 Dec 2023 06:56:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​的扩散模型 | DDPM PyTorch 实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189lqlh/p_diffusion_models_from_scratch_ddpm_pytorch/</link>
      <description><![CDATA[       由   提交/u/tusharkumar91   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189lqlh/p_diffusion_models_from_scratch_ddpm_pytorch/</guid>
      <pubDate>Sun, 03 Dec 2023 04:38:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么交叉熵随着准确度的增加而增加？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1895m5k/d_why_is_crossentropy_increasing_with_accuracy/</link>
      <description><![CDATA[      我正在实现 softmax 回归，并且我正在努力理解交叉熵值增加的问题背后的本质[1]，以及准确性的增加（在“iris”数据集上）： https://preview.redd.it/eo3654zwbw3c1.png?width=688&amp;format= png&amp;auto=webp&amp;s=52916d56029e6e8aa1e9594bbcb02c7075009206 这对我来说非常令人困惑，因为没有类不平衡： [1]   由   提交 /u/joshjson   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1895m5k/d_why_is_crossentropy_increasing_with_accuracy/</guid>
      <pubDate>Sat, 02 Dec 2023 15:05:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 惨痛的教训和思想之树 - 像 ToT 这样的技术是使用搜索的例子，还是它们通过编码类人学习而忽略了惨痛的教训？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1893ne2/d_bitter_lesson_and_tree_of_thoughts_are/</link>
      <description><![CDATA[惨痛的教训表明，学习和搜索是获胜策略，因为它们随着计算能力的扩展而扩展，并且通常会优于依赖于编码人类知识的技术。&lt; /p&gt; 鉴于此，您对 ToT 和类似技术有何看法？它们是通过搜索扩展模型能力的好例子，还是试图强制我们认为是类人行为的例子？ ​ 仅供参考，我在研究中看到了两种主要的基于树的方法。一种是基于MCTS的解码。这似乎更符合传统的搜索概念，因为您正在搜索可能文本的结果空间，然后选择最好的文本。 meta 使用 PPO 值函数作为 MCTS 节点评估器的论文 使用MCTS来提高编写成功程序的能力 ​ 但是，还使用了更抽象的CoT/ToT风格的树，这依赖模型为给定序列生成后续序列树。 思想树：故意问题使用大型语言模型求解 这里的一个核心区别是树并不像 MCTS 解码树搜索那样表示对可能输出的搜索。这是对可能的推理链的搜索，这些推理链最终是某种评估方法（通常是模型本身）的输入，以确定最佳输出。因此，您不仅搜索结果空间，还搜索“证据空间”，这两者都将传递给评估器以选择适当的结果。  编辑：不相关，但原则上您可以结合这两种搜索技术，这会很酷，但可能非常昂贵。例如，对 ToT 中的每个节点使用 MCTS 解码。我还没有看到有人这样做，但如果有人有一篇论文的链接那就太酷了。   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1893ne2/d_bitter_lesson_and_tree_of_thoughts_are/</guid>
      <pubDate>Sat, 02 Dec 2023 13:23:42 GMT</pubDate>
    </item>
    </channel>
</rss>