<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 10 Mar 2024 03:13:56 GMT</lastBuildDate>
    <item>
      <title>[D] runpod 有替代品吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bazyqt/d_any_alternatives_to_runpod/</link>
      <description><![CDATA[我正在开发一个视频 GAN，我已经使用 runpod 来完成它。当我创建一个带有 GPU 的 Pod 时，它实际上可以工作的比例已经达到了 50/50。即使它确实起作用，它也会缓慢地起作用。我知道它运行缓慢，因为我有一个模型，我知道在 runpod 上的特定 GPU 上每个周期需要 9 秒，因为我之前在 runpod 上的特定 GPU 上运行过它。但今天每个 epoch 需要 1 分钟。我当天早些时候尝试过，稍后又尝试过，多个不同的实例，同样的问题。我过去曾多次遇到过这个问题，最近出现的频率越来越高。有没有类似 runpod 的替代方案？或者这就是当前 GPU 短缺的情况？我正在寻找一些预先定价的东西，我只需按小时支付 GPU 费用，而不是一些复杂的定价方案，在这种方案中，我直到实际收费后才知道我在玩什么（在谷歌上看你）。我也在寻找类似于 runpod 的东西，因为我只选择一个 GPU 和一个已经安装了我需要的大部分内容的操作系统（例如 PyTorch），然后我只运行我的模型。    由   提交 /u/Titty_Slicer_5000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bazyqt/d_any_alternatives_to_runpod/</guid>
      <pubDate>Sun, 10 Mar 2024 03:03:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] Gemini 1.5 Pro：专家的稀疏组合，可在一次提示中解锁整本书和电影中的推理和知识</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bazrm5/d_gemini_15_pro_sparse_mixture_of_experts_to/</link>
      <description><![CDATA[       由   提交/u/sasaram  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bazrm5/d_gemini_15_pro_sparse_mixture_of_experts_to/</guid>
      <pubDate>Sun, 10 Mar 2024 02:52:59 GMT</pubDate>
    </item>
    <item>
      <title>[N] 现在有一种可行的方法来执行时间序列数据增强</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bawmp3/n_there_is_now_a_viable_way_to_perform_time/</link>
      <description><![CDATA[ 由   提交/u/anonymousTestPoster  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bawmp3/n_there_is_now_a_viable_way_to_perform_time/</guid>
      <pubDate>Sun, 10 Mar 2024 00:18:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于理解扩散模型中的方程45：统一的视角</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bastdf/d_about_eq45_in_understanding_diffusion_models_a/</link>
      <description><![CDATA[该方程的一致性项是从方程44的最后一项导出的，但我认为它们并不相同。 &lt; p&gt;为了比较式 44 的最后一项和式 45 的一致性项，我们将这些项展开为多重积分（期望 + KL 散度），并只关注概率部分。 从最后一项开始式44的项： q(xₜ₋₁, xₜ, xₜ₊₁|x₀) = q(xₜ₊₁|xₜ) * q(xₜ|xₜ₋₁) * q (xₜ₋₁|x₀)  并根据一致性项： q(xₜ₋₁, xₜ₊₁ | x₀) * q(xₜ) |xₜ₋₁) = q(xₜ₊₁|xₜ₋₁) * q(xₜ|xₜ₋₁) * q(xₜ₋₁|x₀)  所以，如果44和eq.45是等价的，q(xₜ₊₁|xₜ₋₁)和q(xₜ₊₁|xₜ)应该是相同的。但这并不总是正确的。 推导中是否有错误，或者我遗漏了什么？   由   提交/u/ihopesomuch  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bastdf/d_about_eq45_in_understanding_diffusion_models_a/</guid>
      <pubDate>Sat, 09 Mar 2024 21:31:41 GMT</pubDate>
    </item>
    <item>
      <title>[P] AutoGluon-TimeSeries：强大的时间序列预测库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1basqvu/p_autogluontimeseries_a_powerful_timeseries/</link>
      <description><![CDATA[该库包含每个 SOTA 时间序列模型（统计、ML、DL）。 此外，它还支持集成。这是我发现的一个很棒的教程： https://aihorizo​​nforecast.substack.com/p/autogluon -timeseries-creating-powerful   由   提交 /u/apaxapax   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1basqvu/p_autogluontimeseries_a_powerful_timeseries/</guid>
      <pubDate>Sat, 09 Mar 2024 21:28:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 超越语言模型：字节模型是数字世界模拟器 - 微软亚洲研究院 2024 - bGPT - 模拟 CPU 行为的卓越能力，执行各种操作的准确度超过 99.99%！可以帮助解决标记化问题！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baqjkn/r_beyond_language_models_byte_models_are_digital/</link>
      <description><![CDATA[   论文：https://arxiv.org/abs/2402.19155  带有代码和权重的纸页：https://byte-gpt.github.io/  摘要：  传统深度学习经常忽视字节，数字世界的基本单位，所有形式的信息和操作都以二进制格式进行编码和操作。受到自然语言处理中下一个标记预测成功的启发，我们引入了bGPT，这是一个具有下一个字节预测的模型来模拟数字世界。 bGPT 与各种模式（包括文本、音频和图像）的性能专业模型相匹配，并为预测、模拟和诊断算法或硬件行为提供了新的可能性。它几乎完美地复制了符号音乐数据的转换过程，在将ABC记谱转换为MIDI格式时实现了每字节0.0011位的低错误率。此外，bGPT 在模拟 CPU 行为方面表现出卓越的能力，在执行各种操作时准确率超过 99.99%。利用下一个字节预测，bGPT 等模型可以直接从大量二进制数据中学习，有效地模拟复杂的数据。数字世界的模式。   https:/ /preview.redd.it/u0h8rs651dnc1.jpg?width=1836&amp;format=pjpg&amp;auto=webp&amp;s=6f7ae48560280a7d15f7095bb8915b8db50ba9ef https://preview.redd.it/e2fnnt651dnc1.jpg?width=899&amp;format=pjpg&amp;auto=webp&amp; s =d637e2f08e70c7caa3d227dc9f8bd26ec5921360 来源：安德烈·卡帕蒂 https://youtu.be/zduSFxRajkE?si=Z3AFwwhth3j7raSv    由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baqjkn/r_beyond_language_models_byte_models_are_digital/</guid>
      <pubDate>Sat, 09 Mar 2024 19:56:03 GMT</pubDate>
    </item>
    <item>
      <title>[R] 法学硕士在预测神经科学实验结果方面超越人类专家（81% vs 63%）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baq496/r_llms_surpass_human_experts_in_predicting/</link>
      <description><![CDATA[一项新研究表明，法学硕士可以比人类专家更准确地预测哪些神经科学实验可能会产生积极的结果。研究人员使用只有 70 亿个参数的 GPT-3.5 类模型，发现根据神经科学文献对其进行微调可以进一步提高性能。 我认为实验设计很有趣。法学硕士收到了两个版本的摘要，其结果显着不同，我们被要求预测哪个更有可能是真正的摘要，本质上是预测哪个结果更有可能。它们比人类高出约 18%。 其他亮点：  对神经科学文献的微调提高了性能 模型的准确率比人类高出 81.4%。人类专家为 63.4% 在所有测试的神经科学子领域都成立 即使较小的 7B 参数模型也能与较大的模型相媲美 微调的“BrainGPT”模型的准确度比基准提高了 3%  其意义重大 - 人工智能可以帮助研究人员优先考虑最有希望的实验，加速科学发现并减少浪费的精力。它可能会在理解大脑和开发神经系统疾病治疗方面带来突破。 但是，该研究仅关注神经科学，测试集有限。需要更多的研究来看看这些发现是否可以推广到其他科学领域。虽然人工智能可以帮助识别有前途的实验，但它无法取代人类研究人员的创造力和批判性思维。  全文在此。我还写了 更详细的分析在这里。   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1baq496/r_llms_surpass_ human_experts_in_predicting/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baq496/r_llms_surpass_human_experts_in_predicting/</guid>
      <pubDate>Sat, 09 Mar 2024 19:37:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 GAN/Diffusion 模拟整形手术结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1banrhx/d_simulating_plastic_surgery_outcomes_using/</link>
      <description><![CDATA[我想训练一个模型来帮助用户展示整形手术的结果。  我想在这个 Reddit 上获得专家和经验丰富的人的意见和指导，了解针对此类任务开发人工智能的最佳方法。主义的。  我有一个大约 1500 个之前和之后的自定义数据集。图像后进行训练。我也可以投资获得更大的数据集。 我想在这个 reddit 上获得专家和经验丰富的人的意见和指导，了解针对此类任务开发人工智能的最佳方法。    由   提交/u/tjain73  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1banrhx/d_simulating_plastic_surgery_outcomes_using/</guid>
      <pubDate>Sat, 09 Mar 2024 17:58:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] PyTorch 中实现多头注意力的 5 种不同方式的速度比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bajw60/p_speed_comparison_of_5_different_ways_to/</link>
      <description><![CDATA[       由   提交/u/seraschka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bajw60/p_speed_comparison_of_5_different_ways_to/</guid>
      <pubDate>Sat, 09 Mar 2024 15:12:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] PromptKD：视觉语言模型的无监督即时蒸馏，即时学习中的 SOTA。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bahtd6/r_promptkd_unsupervised_prompt_distillation_for/</link>
      <description><![CDATA[      纸张： https://arxiv.org/abs/2403.02781 项目页面： https://zhengli97.github.io/PromptKD/ Github： https://github.com/zhengli97/PromptKD ​ https://preview.redd.it/bop5wm9f8bnc1.png?width=1330&amp;format=png&amp; ;auto=webp&amp;s=ad50156e81d6f9248c597ca239596f28a9f5d7cb 亮点： (1)。一种新颖的视觉语言模型两阶段无监督即时蒸馏框架。 (2)。重用高质量的教师文本特征，而不是训练学生自己的文本编码器。 (3)。使用老师提供的软标签对大量未标记的域图像进行蒸馏。 (4)。 PromptKD 在 11 个不同的识别数据集上优于所有现有的即时学习方法。 摘要： 在本文中，我们介绍了一种无监督域即时蒸馏框架，旨在通过使用未标记的域图像进行提示驱动的模仿，将较大教师模型的知识转移到轻量级目标模型。 具体来说，我们的框架由两个不同的阶段组成。在初始阶段，我们使用域（少样本）标签预训练大型 CLIP 教师模型。预训练后，我们利用 CLIP 独特的解耦模态特征，通过教师文本编码器预计算文本特征并将其存储为类向量一次。 在后续阶段，存储的类向量在教师和学生图像编码器之间共享，以计算预测的 logits。此外，我们通过 KL 散度损失来对齐教师和学生模型的对数，鼓励学生图像编码器通过可学习的提示生成与教师相似的概率分布。 所提出的提示蒸馏过程消除了对标记数据的依赖，使算法能够利用域内大量未标记的图像。最后，利用训练有素的学生图像编码器和预存储的文本特征（类向量）进行推理。 据我们所知，我们是第一个 (1) 执行无监督域的人-针对CLIP的特定提示驱动的知识蒸馏，(2)建立实用的文本特征预存储机制作为教师和学生之间共享的类向量。在11个识别数据集上的大量实验证明了我们方法的有效性。  图 1 我们的概述提示KD框架。 (a) 我们首先使用带标签的训练图像预训练一个大型 CLIP 教师模型。 (b) 重用现有的更高质量的教师文本特征进行无监督的快速蒸馏。 (c) 利用训练有素的学生和预先存储的教师文本特征进行最终推理。 实验结果： 基础-to-novel实验： 表1. 与现有最先进的从基础到新颖的概括方法进行比较。  跨数据集实验： 表 2. PromptKD 与现有先进方法在跨数据集基准评估上的比较。 ​ &lt; !-- SC_ON --&gt;  由   提交 /u/zhengli_nku   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bahtd6/r_promptkd_unsupervised_prompt_distillation_for/</guid>
      <pubDate>Sat, 09 Mar 2024 13:32:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] 分数 GPU 容器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1baho3l/p_fractional_gpu_containers/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交 /u/Thick-Taste-9985   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1baho3l/p_fractional_gpu_containers/</guid>
      <pubDate>Sat, 09 Mar 2024 13:24:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 学习 CUDA/C++ 有多大价值？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</link>
      <description><![CDATA[目前每个人都在努力使 AI 实现快速/高效（因为效率更高 -&gt; 计算上花费的资金更少）。 例如，Flash Attention 2是在CUDA中实现的。 Llama.cpp 是 C++ PyTorch 够用吗？或者在这个市场上学习 CUDA/C++ 是否有优势，特别是对于法学硕士？ 如果 CUDA 在某些情况下有用，那么这些情况是什么？   由   提交/u/joelthomas-  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bae4e3/d_how_valuable_is_learning_cuda_c/</guid>
      <pubDate>Sat, 09 Mar 2024 09:44:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 R/Python 进行机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bacwg3/d_ml_with_rpython/</link>
      <description><![CDATA[我是应用统计专业的学生。正因为如此，我们通过 R 学习统计机器学习。然而，当我遇到机器学习时，大多数时候，人们通常更多地谈论使用 Python。所以我不知道有哪个领域/行业实际使用 R 的 ML 来代替？如果 R 在 ML 中非常不受欢迎，你认为公司会因为我只懂 R 而选择我吗？ （如果是这样的话我可能会尝试学习Python，但我想我不会很强）。干杯伙计们！    由   提交 /u/StrangerOnTheRoad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bacwg3/d_ml_with_rpython/</guid>
      <pubDate>Sat, 09 Mar 2024 08:19:19 GMT</pubDate>
    </item>
    <item>
      <title>[N] 矩阵乘法突破可能带来更快、更高效的人工智能模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</link>
      <description><![CDATA[   “计算机科学家发现了一个报告称，通过消除以前未知的低效率，比以往更快地乘以大型矩阵的新方法 广达杂志。这最终可能会加速人工智能模型，例如ChatGPT，它严重依赖矩阵乘法来运行。据报道，最近两篇论文中提出的研究结果使矩阵乘法效率实现了十多年来的最大改进。 ...图形处理单元 (GPU) 擅长处理矩阵乘法任务，因为它们能够同时处理许多计算。他们将大型矩阵问题分解为更小的部分，并使用算法同时解决它们。完善该算法一直是过去一个世纪（甚至在计算机出现之前）矩阵乘法效率突破的关键。 2022 年 10 月，我们涵盖了Google DeepMind AI 模型发现的一项名为 AlphaTensor 的新技术，专注于针对特定矩阵大小（例如 4x4 矩阵）的实用算法改进。 相比之下， 新研究，由清华大学的段燃和周仁飞、加州大学伯克利分校的吴洪勋以及 Virginia Vassilevska 进行麻省理工学院的 Williams、Yinzhan Xu 和 Zixuan Xu（在第二篇论文中）寻求通过降低复杂性指数 ω 来实现理论增强，从而在所有大小的矩阵上获得广泛的效率增益。这项新技术并没有像 AlphaTensor 那样寻找直接、实用的解决方案，而是解决了基础性的改进，可以在更广泛的范围内提高矩阵乘法的效率。  ... 两个 n×n 矩阵相乘的传统方法需要 n3 次单独的乘法。然而，这项新技术改进了“激光方法”。由 Volker Strassen 于 1986 年提出，减小了指数的上限（表示为前面提到的 ω），使其更接近到理想值 2，这表示理论上所需的最小操作数。” ​ https://preview.redd.it/a49r1ajv59nc1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp; s =cf315793e6784ef9e62d48e00ebf0f3809070f6c https://arstechnica.com/information-technology/2024/03/matrix-multiplication-breakthrough-could-lead-to-faster-more-efficient-ai-models/&lt; /strong&gt;   由   提交/u/Secure-Technology-78   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bab774/n_matrix_multiplication_breakthrough_could_lead/</guid>
      <pubDate>Sat, 09 Mar 2024 06:28:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>