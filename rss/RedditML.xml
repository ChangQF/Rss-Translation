<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Mon, 29 Jul 2024 03:17:19 GMT</lastBuildDate>
    <item>
      <title>[P] CUDA 中的 KV 缓存</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eepco9/p_kv_cache_in_cuda/</link>
      <description><![CDATA[CUDA 中的 KV 缓存 我正在开发一个允许使用 C CUDA 推理 Llama3.0-8B 的项目。GG 的 Llama.cpp 给了我很大的启发，因为我想深入了解 CUDA。 对于 KV 缓存，您究竟如何在确保速度和内存性能的同时，移除和重新组织物理 CUDA 内存中的张量数据？本质上，是否有任何最佳实践且经济有效的方法可以将张量虚拟地移到左上角，以便为新标记提供新空间？ 因为我假设如果我必须分配内存并设置 CUDA 内核以将值复制到新的内存块，那么最终的成本会比重新计算值更高？  此外，如果缩放后​​无论如何都有用于自我注意的掩码，那么计算输出的右上角三角形有什么意义呢？直接将物理内存中的这些值手动设置为某个大的负数不是很有意义吗？  struct Tensor { int ndim; int *shape; float *arr; int mem_size; };     提交人    /u/Delicious-Ad-3552   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eepco9/p_kv_cache_in_cuda/</guid>
      <pubDate>Mon, 29 Jul 2024 03:12:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么机器学习领域中这么多最优秀的人才没有为大型科技公司工作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eejd3b/d_why_so_many_of_the_most_skilled_people_in_the/</link>
      <description><![CDATA[我见过很多拥有常春藤联盟学位的人，研究论文作者，获奖者，课程老师，该领域的书籍作者，但你看他们的领英，这些人中的大多数都不在谷歌、微软、亚马逊、Meta 等大型科技公司 (MANGA 公司) 工作，他们通常在中小型公司工作，我的意思是，写一本关于机器学习的书的人一定知道这件事，拥有剑桥或哈佛计算机科学学位的人可能对此有所了解，为什么有这么多来自大科技公司？ 我知道很多人想专注于研究而不是行业，但大型科技公司确实在 ML 领域进行了最先进的研究，所以对我来说很难知道为什么这些公司不想要这些人或者为什么他们不想为大型科技公司工作。    提交人    /u/millhouse056   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eejd3b/d_why_so_many_of_the_most_skilled_people_in_the/</guid>
      <pubDate>Sun, 28 Jul 2024 22:18:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 稀疏自动编码器对 LLM 可解释性的直观解释</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eeihdl/d_an_intuitive_explanation_of_sparse_autoencoders/</link>
      <description><![CDATA[稀疏自动编码器 (SAE) 是实现 LLM 可解释性和可说明性的最有前途和最流行的方法之一。我撰写了一篇简单、直观的 SAE 介绍，并附有图表和参考 PyTorch 代码。 除其他内容外，我还介绍了我们使用 SAE 的原因、它们如何用于模型干预（例如 金门大桥克劳德）以及使用 SAE 的挑战。最突出的挑战之一是缺乏良好的指标，因为自然语言文本中没有明确的可衡量的基本事实。 解释链接在此处：https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html    提交人    /u/seraine   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eeihdl/d_an_intuitive_explanation_of_sparse_autoencoders/</guid>
      <pubDate>Sun, 28 Jul 2024 21:38:40 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 在生产中训练 ML 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eeg376/discussion_training_ml_model_on_production/</link>
      <description><![CDATA[嗨，我已经在某些东西（回归）上训练了一个 XGBoost 模型，我可以在 excel 表中提取预测结果，但我想将其移动到我公司的服务器上，我可以保存模型并将其作为 json 加载到任何地方，所以我考虑了方法，但不知道这是否是最佳实践， 我考虑将模型完整代码推送到服务器上，并有一个每月运行一次的定期任务来生成数据并将其保存到数据库，以便我可以在前端使用它 该定期任务将使用每月获得的新数据再次重新训练模型并在最后获得预测结果，那么在服务器上进行训练是否正确？    提交人    /u/AmrElsayedEGY   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eeg376/discussion_training_ml_model_on_production/</guid>
      <pubDate>Sun, 28 Jul 2024 19:53:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee9dra/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jul 2024 15:00:14 GMT</pubDate>
    </item>
    <item>
      <title>[R] 逆 GAN 保留生成器的权重</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ee6w2s/r_inverse_gan_preserving_weights_of_generator/</link>
      <description><![CDATA[我正在寻找一个 GAN 模型，其中生成器可以使用与原始 GAN 生成虚假数据时相同的权重参数反转回潜在空间。我看过的大多数可逆 GAN 论文都使用单独的编码器或一些优化问题。 我阅读了这篇关于逆 GAN 的调查论文，但找不到使用相同权重的模型。 注意：我知道扩散模型可能非常适合这种情况。但我不想使用具有时间步长的模型，而是寻找单次传递模型。    提交人    /u/Alternative-Talk1945   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ee6w2s/r_inverse_gan_preserving_weights_of_generator/</guid>
      <pubDate>Sun, 28 Jul 2024 13:00:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 演示 ML 模型的工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edu2tk/d_tool_to_demo_ml_models/</link>
      <description><![CDATA[大家好， 我和我的朋友正在开发一种工具，可让您在部署之前在可展示的环境中预览您的 ML 模型。我的模型是在 Google Colab 上设置的，但团队很难对其进行审查。它对客户来说也不太好看。 因此，我们希望创建一个演示环境，在移交给 DevOps 之前，可以非常简单地共享和展示模型。也在考虑添加某种反馈系统。 我们仍在确定细节，所以我们很想听听你对此的看法。根据您的经验，哪些功能会对您有所帮助？目前，我们考虑了图表和协作功能。 谢谢！（我的 DM 是开放的！我们不可能是唯一遇到这个问题的人吧）    提交人    /u/Pristine-Watercress9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edu2tk/d_tool_to_demo_ml_models/</guid>
      <pubDate>Sat, 27 Jul 2024 23:46:11 GMT</pubDate>
    </item>
    <item>
      <title>如何像机器学习开发人员一样编写代码？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edscmn/how_to_code_like_developers_in_ml_d/</link>
      <description><![CDATA[我今年 24 岁，是一名航空航天博士，从事科学机器学习工作。我用 Pytorch 编写代码。这就是我如何获得一些好结果的原因。但我的代码不太灵活。如果我必须改变我的训练风格，我必须完全编写所有内容。我不知道如何以优化的方式添加不同类型的数据结构，如字典、元组和列表，以及其他方法，例如人们在 NVIDIA 中所做的方法。我甚至感觉不舒服。任何人都有这种感觉。    提交人    /u/haramkhor_havasi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edscmn/how_to_code_like_developers_in_ml_d/</guid>
      <pubDate>Sat, 27 Jul 2024 22:23:51 GMT</pubDate>
    </item>
    <item>
      <title>摘要和其他任务的最佳块和重叠大小 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edr0ro/optimal_chunk_and_overlap_size_for_summarization/</link>
      <description><![CDATA[  由    /u/28djs  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edr0ro/optimal_chunk_and_overlap_size_for_summarization/</guid>
      <pubDate>Sat, 27 Jul 2024 21:23:15 GMT</pubDate>
    </item>
    <item>
      <title>LiveBench 与现有的大规模 LLM 基准测试相比如何？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edk0ca/how_does_livebench_compare_to_existing_large/</link>
      <description><![CDATA[想知道社区对新的 LLM 基准 LiveBench 有何看法。您认为它与其他一些大型基准（例如 HELM、ChatBot Arena 和 Open LLM Leaderboard）相比如何？    提交人    /u/Penfever   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edk0ca/how_does_livebench_compare_to_existing_large/</guid>
      <pubDate>Sat, 27 Jul 2024 16:14:10 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 Concrete ML 和完全同态加密的端到端加密 23andMe 基因测试应用程序。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edir29/p_endtoend_encrypted_23andme_genetic_testing/</link>
      <description><![CDATA[我们最近收到了一些外部贡献，展示了如何使用 Zama 库创建端到端加密的基因检测应用程序，例如使用完全同态加密 (FHE) 的 23andMe。这篇博文展示了高级加密技术的集成，以确保在进行基因分析时保护隐私。这是一个重要的里程碑，因为它表明我们现在可以使用 FHE 在大约 5 分钟内预测加密的 DNA 祖先。 在此处阅读完整的博客文章：使用 Concrete-ML 和完全同态加密构建端到端加密的 23andMe 基因检测应用程序 很高兴回答任何问题或帮助任何有兴趣使用我们的工具构建安全 ML 应用程序的人！     由    /u/fd0r 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edir29/p_endtoend_encrypted_23andme_genetic_testing/</guid>
      <pubDate>Sat, 27 Jul 2024 15:18:44 GMT</pubDate>
    </item>
    <item>
      <title>[R] 这是我们基于神经网络的快速漫射房间脉冲响应发生器（FAST-RIR）的官方实现，用于为给定的声学环境生成房间脉冲响应（RIR）。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edibkx/r_this_is_the_official_implementation_of_our/</link>
      <description><![CDATA[        提交人    /u/Snoo63916   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edibkx/r_this_is_the_official_implementation_of_our/</guid>
      <pubDate>Sat, 27 Jul 2024 14:59:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我创建了 Promptimizer——一个基于遗传算法 (GA) 的提示优化框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edgtft/d_i_created_promptimizer_a_genetic_algorithm/</link>
      <description><![CDATA[        由    /u/NextgenAITrading 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edgtft/d_i_created_promptimizer_a_genetic_algorithm/</guid>
      <pubDate>Sat, 27 Jul 2024 13:49:45 GMT</pubDate>
    </item>
    <item>
      <title>[D] Llama3.1 如何利用词汇表中仅有的 28k 个额外标记来支持多种语言？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edct9i/d_how_does_llama31_support_multiple_languages/</link>
      <description><![CDATA[根据 Llama3.1 论文，词汇表包含 128k 个标记，其中 100k 个专用于英语，28k 个分配给非英语语言。鉴于韩语、中文和日语等语言不使用 26 个字母的拉丁字母表并且可以有数百万个唯一字符，如何仅用 28k 个标记就涵盖这些语言？标记化是基于 Unicode 吗？    提交人    /u/Financial_Air5256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edct9i/d_how_does_llama31_support_multiple_languages/</guid>
      <pubDate>Sat, 27 Jul 2024 09:59:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 检索增强生成的替代方案是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1edbg0h/d_whats_the_alternative_to_retrieval_augmented/</link>
      <description><![CDATA[看来 RAG 是业界问答系统的事实标准。有什么替代方案吗？    提交人    /u/clocker2004   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1edbg0h/d_whats_the_alternative_to_retrieval_augmented/</guid>
      <pubDate>Sat, 27 Jul 2024 08:21:58 GMT</pubDate>
    </item>
    </channel>
</rss>