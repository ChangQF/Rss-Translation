<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 - >/r/mlquestions或/r/r/learnmachinelearning，agi->/r/singularity，职业建议 - >/r/cscareerquestions，数据集 - > r/数据集</description>
    <lastBuildDate>Fri, 28 Mar 2025 12:34:45 GMT</lastBuildDate>
    <item>
      <title>[d]您如何使已出版的情节看起来很好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlt27q/d_how_do_you_make_your_published_plots_look_so/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我注意到我正在审查的论文的一些图形和图真的真的 good。您如何使它们看起来如此好？您是否正在使用我不知道的任何特殊Python库？我知道你们中有些人正在使用Adobe Illustrator并浏览图/数字，但是我还缺少其他东西吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jlt27q/d_how_how_do_do_do_you_make_make_your_your_published_plots_plots_sook_so/”&gt; [link]   [commist]         ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlt27q/d_how_do_you_make_your_published_plots_look_so/</guid>
      <pubDate>Fri, 28 Mar 2025 11:40:42 GMT</pubDate>
    </item>
    <item>
      <title>[d]在NLP中寻找理论利基市场</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlsptp/d_looking_for_a_theoretical_niche_in_nlp/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  来自发展中国家的，我的NLP工作自然而然地倾向于HCI，因为可以访问用于培训大型模型的计算资源。我对理论充满热情，但是从我的观察到NLP的最新理论进步，重点是改善模型培训和推理。我在所有R＆amp; d上使用4GB RAM CORE i3桌面来提供一些视角。 问题 是否有NLP中有任何理论上的壁nike，它们更扎根于计算机科学（而不是语言学），而不是 不需要大量的GPU资源？提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jlsptp/d_looking_for_a_theoricenty_niche_niche_in_in_nlp/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlsptp/d_looking_for_a_theoretical_niche_in_nlp/</guid>
      <pubDate>Fri, 28 Mar 2025 11:18:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] python项目设置用于ML的紫外线</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlso3t/p_python_project_setup_for_ml_with_uv/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在共享用于ML的Python项目设置，包括设置测试，格式化，覆盖，覆盖，静态类型检查。    https://substack.com/home/post/post/p-159696805 提交由＆＃32; /u/madiyar     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlso3t/p_python_project_setup_for_ml_with_uv/</guid>
      <pubDate>Fri, 28 Mar 2025 11:15:54 GMT</pubDate>
    </item>
    <item>
      <title>[R]通过基于乐高的视觉任务评估MLLM中多步空间推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlsm63/r_evaluating_multistep_spatial_reasoning_in_mllms/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在研究这个名为Lego-Puzzles的新基准，该基准测试了使用乐高风格的难题测试空间推理任务上的多模式模型。作者创建了一个数据集，模型需要确定是否可以组装给定件，以通过多个步骤来推理约3D空间关系。 关键点。 GPT-4V, Claude 3 models, Gemini Pro, and LLaVA-1.5 - Chain-of-thought prompting was used to optimize performance Results: - Human performance: 85.8% - Best model (Claude 3 Opus): 59.8% - Performance decreases as puzzle complexity increases - Models particularly struggle with &quot;negative&quot;难题（无法组合作品） - 常见的故障模式包括误解连接机制，令人困惑的方向以及在多步难题中失去轨道 我认为这项工作突出了当前视觉模型中的基本限制，这些模型没有得到足够的注意。尽管在许多领域中具有令人印象深刻的功能，但这些模型缺乏人类自然发展的基本空间推理能力。 85.8％（人）和59.8％（最佳AI）之间的差距很大，这表明我们需要专门为处理空间关系和物理约束而设计的新建筑方法。 此基准标准可能对机器人技术和体现的AI研究特别有价值，在此理解对象的物理操纵能力是必不可少的。我很好奇未来的工作是否会探索使模型访问3D表示形式是否仅仅有助于弥合这一差距。  tldr：当前的MLLM在空间推理任务上的表现较差，涉及乐高风格的拼图，在人体绩效中得分明显低于人类的表现，在多样性的推理和理解物理约束方面特别困难。 href =“ https://aimodels.fyi/papers/arxiv/lego-puzzles-how-good-good-are-mllms-at-”&gt;完整摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlsm63/r_evaluating_multistep_spatial_reasoning_in_mllms/</guid>
      <pubDate>Fri, 28 Mar 2025 11:12:18 GMT</pubDate>
    </item>
    <item>
      <title>[p]最小值代理商的最佳方法用于最终的TIC TAC TOE游戏。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jls54n/p_best_approach_to_minimax_agent_for_ultimate_tic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在拔头发而不是优化代理，远射，但是如果你们能帮助我找到更好的启发式启发式，甚至指导我使用Nn或线性回归，可以使用NN或线性回归，如果您会降低我的状态（如果成功的话，我会掉下来了）  imax  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/dttyche     [link]  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jls54n/p_best_apphacp_to_minimax_agent_agent_for_ultimate_tic/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jls54n/p_best_approach_to_minimax_agent_for_ultimate_tic/</guid>
      <pubDate>Fri, 28 Mar 2025 10:41:24 GMT</pubDate>
    </item>
    <item>
      <title>[d]我如何训练模型以提高30 fps推理速度的视频质量</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlm1ma/d_how_can_i_train_a_model_to_improve_quality_of/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想训练一个模型以提高视频质量。基本上删除压缩工件并添加，保存或生成更精细的细节。  有什么好模型吗？我有一个不错的库存视频数据集，其中包括数千个视频。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/throwaway0845reddit     [links]     &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jlm1ma/d_how_can_can_i_i_a_a_model_model_model_model_to_improve_quality_of/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlm1ma/d_how_can_i_train_a_model_to_improve_quality_of/</guid>
      <pubDate>Fri, 28 Mar 2025 03:34:42 GMT</pubDate>
    </item>
    <item>
      <title>[r]神经普通微分方程的替代实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlarz3/r_alternative_implementation_of_neural_ordinary/</link>
      <description><![CDATA[    我正在阅读原始节点纸，对我来说，这种方法似乎很复杂且人为。我得出了自己的节点版本，该节点仅包含2组微分方程，并且可以同时解决，而无需向前和向后传递，而只能进行单个前向通行证。我发布了带有派生的图像，任何人都可以详细说明为什么不以这种方式实现节点？这不是容易吗？如果没有，我是否在某处犯了一个错误     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/brale_     [link]    [注释]             ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlarz3/r_alternative_implementation_of_neural_ordinary/</guid>
      <pubDate>Thu, 27 Mar 2025 18:22:17 GMT</pubDate>
    </item>
    <item>
      <title>Mac上的机器学习[讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlan08/machine_learning_on_mac_discussion/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！刚开始通过MATLAB在Mac上开发深入学习管道。该管道用于免疫组织化学图像分析。前两个培训进行得很好 - 笔记本电脑运转了，但进行了管理，但是我希望随着我增加培训数据并最终开始图像重建，我的笔记本电脑将会挣扎。第一次培训课程为15分钟，第二个训练时间（含更多标签）为10分钟。 笔记本电脑规格是M4 MAX MAD MBP，36GB UM，1TB SSD。 上一个培训课程是40epochs，带4个迭代/次数。  图像分为36个图块。它仅在CPU上运行 - 但所有14个内核都以Max  无法使用MacOS上的GPU BC MATLAB运行，但不支持GPU加速。  寻找下一步要做什么的建议。正在考虑使用我的大学的HPC，COLAB，或者只是继续在本地运行它。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/solarpistachio     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlan08/machine_learning_on_mac_discussion/</guid>
      <pubDate>Thu, 27 Mar 2025 18:16:50 GMT</pubDate>
    </item>
    <item>
      <title>[d]如何优化SOTA时间系列模型（PatchTST，时间网等）进行公平比较？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlalmz/d_how_do_you_optimize_sota_timeseries_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在基准针对PatchTST，timeNet，Inception Time等新的时间序列分类模型，等等。i：  使用每个模型的默认型号发布的超参数？ 您如何平衡调整工作和计算预算以确保公平比较（验证协议，早期停止，平等试验）？谢谢！  ps，如线程中其他人所述，在这里我只考虑基于深度学习的方法（CNN，变形金刚或两者的组合）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jlalmz/d_how_do_do_do_you_optimize_sota_timeseries_models/  &lt;a href =“ https://www.reddit.com/r/machinelearning/comments/1jlalmz/d_how_do_do_do_you_optimize_sota_timeseries_models/]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlalmz/d_how_do_you_optimize_sota_timeseries_models/</guid>
      <pubDate>Thu, 27 Mar 2025 18:15:18 GMT</pubDate>
    </item>
    <item>
      <title>[D]有人成功地提取了Spacy吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jlalko/d_anybody_successfully_doing_aspect_extraction/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我很想了解您如何实现它。我正在努力从Spacy那里获得一位跨性别者，以学习任何东西。我所有的尝试最终都以相同的30个时代的体现，F1，精度和召回均为0.00，并且损失的波动增加。 I&#39;m trying to determine whether the problem is:  Poor annotation quality or insufficient data A fundamental issue with my objective An invalid approach Hyperparameter tuning  Context I&#39;m extracting aspects (commentary about entities) from noisy online text.我将使用一级方程式制作一个示例： 我的实体提取（例如，查尔斯（Charles）“ Yuki”→驱动程序，“ Ferrari”→“摩纳哥队”摩纳哥队“→竞赛”。现在，我想对跨度进行分类：   &#39;不敢相信我刚刚看到的东西，查尔斯绝对是方向盘上的恶魔，但是法拉利会法拉利，他们需要更换整个坑壁，因为他们的策略永远不会有意义。 →驾驶员质量 “他们需要更换整个坑墙，因为他们的策略永远不会有意义。” →团队质量     ; lmao classic摩纳哥。我应该躺在床上，这场比赛太无聊了。 →种族质量          &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1jlalko/d_anybody_successul_doing_aspect_aspect_extraction/”&gt; [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jlalko/d_anybody_successfully_doing_aspect_extraction/</guid>
      <pubDate>Thu, 27 Mar 2025 18:15:14 GMT</pubDate>
    </item>
    <item>
      <title>[d]使用AI将2D工程图转换为3D参数模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jl7453/d_converting_2d_engineering_drawings_to_3d/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  利用人工智能（AI）将2D工程图纸转换为3D参数模型的当前状态是什么？我的研究揭示了两种主要方法：   1。文本到计算和图像到基础：此方法采用用户提示或从2D图映像中提取零件功能来生成代码，创建参数模型。像动物园这样的公司。 Dev和Adamcad正在积极探索这种方法。   2。机器学习管道：这些管道利用从2D图中提取的功能生成3D CAD构造序列，通常利用类似变压器的架构。研究论文，例如 sketch-a-shape ，演示这种方法。 我会在以下方面的任何见解： 其他公司，研究小组，研究小组，研究小组，或开放式的挑战 探索 任何信息，包括学术研究和行业应用，都将在理解该领域的当前景观和未来方向方面都很有价值。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/profession_sign_53     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jl7453/d_converting_2d_engineering_drawings_to_3d/</guid>
      <pubDate>Thu, 27 Mar 2025 15:50:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT -4O图像生成和编辑 - 如何？？？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jkt42w/d_gpt4o_image_generation_and_editing_how/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  关于最近的多模式模型（Gemini 2.5，New 4o，Grok）如何进行本地图像生成的任何猜测是如何进行的？ 是否仍然可以在图像cododer/decoder/decoder/decoder（vq-vae等）上使用的基本方法？ 还对相关论文感兴趣，这些论文可能指出了最新的图像令牌和培训方法，用于发电和编辑（例如 Edit: After posting this, discovered the Deepseek Janus papers which are super informative - may not be the way the other labs do it, but seems to be one viable direction LLM with adaptor for autoregressive image gen:    https://arxiv.org/abs/2410.13848          直接预测速度的速度流动：   &lt; href =“ https://arxiv.org/abs/2411.07975”&gt;   https://arxiv.org/abs/2411.07975           &lt;！提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jkt42w/d_gpt4o_image_generation_generation_and_editing_how/”&gt; [link]    [commist]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jkt42w/d_gpt4o_image_generation_and_editing_how/</guid>
      <pubDate>Thu, 27 Mar 2025 01:58:59 GMT</pubDate>
    </item>
    <item>
      <title>[d]假设您是任意从这些形状统一得出的许多双变量观测值。什么维度降低 /特征提取方法（如果有）可以“恢复”形状或充分压缩坐标为单个维度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1jkqms0/d_suppose_you_have_arbitrarily_many_bivariate/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在两种情况下，您实际上对数据的形状都不了解。 href =“ https://i.imgur.com/trqx32k.png”&gt; https://i.imgur.com/trqx32k.png     也许它在某个时候停止，或者在某种程度上停下来，或者在自己身上旋转。双变量观察{x_i，y_i}是从这线统一绘制的。是否有任何可以恢复“ true”的方法这些观测值的一维坐标（例如，距沿线距离距离距离）？即，从信息理论/压缩的角度来看，我们可以将2D坐标的数组存储，而是可以沿着线 +方程来存储一个距离（或旋转总数等）。    在第二个情况下，从两个圆圈中取样了点： href=&quot;https://i.imgur.com/CsK1y02.png&quot;&gt;https://i.imgur.com/CsK1y02.png, again at uniform from their length. Here, too, we can compress the data from two real-valued numbers to eg a single real-valued angle, the equations for both circles (their centers and半径）和一个二进制指标，对应于从。  bonus  3 ）rd情况下提取的圆圈，现在圆圈相交：  是否有完全通用的方法可以正确识别这些点所在的较低维度的潜在空间？即，除了在两个维度上有有限的坐标之外，它对生成过程一无所知。哪些方法可以使用最小的数据来执行此操作？ Are there any methods that are decent at identifying the latent space of both the spiral and the circles? (in trying things out, kpca + rbf kernel does ok and diffusion mapping quite well at identifying a latent dimension separating out the two circles with smaller (n=200) amounts of data, while a small vanilla VAE with a 2D bottleneck needs lots more observations for decent performance, and a few other我尝试的方法（例如，umap，umap，t-sne）做得很差，但我的眼球似乎需要少一点数据才能自信地逗弄真实的形状，所以我在这里更奇怪的是，在这些特定的示例中，在我们的范围内，我的范围更加狭窄了，我在这里更加奇怪）。嵌入了200D空间中的一些古怪的10D歧管上，视觉检查并不能特别好，但是一个人希望在那里使用的全自动方法能够在更简单的2D中解决问题！提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1jkqms0/d_suppose_you_have_arbitraly_many_bivariate/”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1jkqms0/d_suppose_you_have_arbitrarily_many_bivariate/</guid>
      <pubDate>Wed, 26 Mar 2025 23:57:13 GMT</pubDate>
    </item>
    <item>
      <title>[d]自我促进线程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请发布您的个人项目，初创企业，产品安排，协作需求，博客，博客等。禁止。 鼓励其他人创建新帖子以便在此处发布问题！ 线程将一直活着直到下一步，因此在标题日期之后继续发布。   -     meta：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为了鼓励社区中的人们不要通过垃圾邮件来促进他们的工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/1j1hc0o/d_selfpromotion_thread/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1j1hc0o/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 02 Mar 2025 03:15:17 GMT</pubDate>
    </item>
    <item>
      <title>[D]每月谁在招聘，谁想被聘用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   为职位发布请使用此模板  雇用：[位置]，薪水：[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]和[简要概述，您要寻找的是]    对于那些寻求工作的人请使用此模板  想要被录用：[位置]，薪水期望，[]，[]，[]，[]，[]，[]，[]，[远程|搬迁]，[全职|合同|兼职]简历：[链接到简历]和[简要概述，您要寻找的是]   ＆＃＆＃＆＃＆＃＆＃＆＃＆＃＆＃x200B;  请记住，请记住，这个社区适合那些有经验的人。   &lt;！ -  sc_on--&gt; 32;&gt; 32;提交由＆＃32;态href =“ https://www.reddit.com/r/machinelearning/comments/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_to_be_hired/”&gt; [link]  &lt;A href =“ https://www.reddit.com/r/machinelearning/comments/1ie5qoh/d_monthly_whos_hiring_and_and_and_who_wants_wants_to_to_be_hired/”]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    </channel>
</rss>