<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning ，AGI -> /r/singularity</description>
    <lastBuildDate>Wed, 07 Aug 2024 15:15:29 GMT</lastBuildDate>
    <item>
      <title>[P] 训练嵌入模型以忽略主题不必要的维度</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1emcrem/p_training_an_embedding_model_to_ignore/</link>
      <description><![CDATA[嗨， 我正在为固定的一组特定主题的文档构建知识管理工具。主要目标是使这些文档在嵌入空间中“可探索”并智能地聚类。但是，我注意到大多数嵌入都非常接近，我认为这是因为它们都围绕同一主题。 我的想法是微调模型以淡化嵌入空间的其余部分，从而增强同一主题内的差异并使其更具可比性。我最初尝试使用 PCA 来实现这一点，但结果并不好。我正在探索的另一个想法是在嵌入上使用小型自动编码器，或者可能为此目的微调开源嵌入模型。但是，我不确定如何开始。 有人有这方面的经验吗？如果是，您使用了哪些方法、模型、框架或来源，结果如何？ 此外，我正在寻找在此基础上对数据集进行良好的视觉探索。虽然美学是次要的，但我对任何有效绘图方法的建议都很感兴趣。    提交人    /u/zeronyk   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1emcrem/p_training_an_embedding_model_to_ignore/</guid>
      <pubDate>Wed, 07 Aug 2024 14:09:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] Neurips 2024 的反驳现在可供审稿人查看吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1emc004/d_are_neurips_2024_rebuttal_viewable_to_reviewers/</link>
      <description><![CDATA[这应该在几个小时前就发生了，但我审阅的论文仍然只显示原始评论，没有反驳。发生了什么事？    提交人    /u/fixed-point-learning   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1emc004/d_are_neurips_2024_rebuttal_viewable_to_reviewers/</guid>
      <pubDate>Wed, 07 Aug 2024 13:37:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM 的 RLHF：动作数量可变？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1embg6k/d_rlhf_for_llms_variable_number_of_actions/</link>
      <description><![CDATA[嗨， 我有一个关于 PPO 参与 LLM 的 RLHF 的问题。由于目标是优化模型的答案，即一系列标记，那么动作空间是什么样的？ 标记序列的长度总是不同的，一个标记等于一个动作。因此，模型在每个步骤必须同时输出的动作数量会有所不同。我从未在 RL 中遇到过这样的情况，即每个步骤的动作数量都会有所不同。 所以我的问题是，这是否是正确的直觉，即如何构建动作，如果是，如何处理可变数量的动作？    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1embg6k/d_rlhf_for_llms_variable_number_of_actions/</guid>
      <pubDate>Wed, 07 Aug 2024 13:13:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你如何追踪你所有的实验？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1emakgn/d_how_do_you_keep_track_of_all_your_experiments/</link>
      <description><![CDATA[大家好， 在我的公司，我们正在进行大量 LLM 实验。 我们目前正在进行“小规模”实验来做各种事情（选择各种超参数、进行一些小的架构更改、使用什么数据集等...） 我们正在使用 WandB，记录实验非常酷，但我不知道在协作方面有什么功能可以更进一步。例如，我们希望有一些东西可以从我们启动的各种实验/图中得出结论，理想情况下将图和结论存储在一个地方。 这样，我们就可以轻松地跟踪所有内容，特别是当我们几个月后回顾实验时，我们能够理解我们启动它的原因以及得出的结论是什么。 你是如何做到的？您是否使用特定工具？    提交人    /u/Theboredhuman_56   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1emakgn/d_how_do_you_keep_track_of_all_your_experiments/</guid>
      <pubDate>Wed, 07 Aug 2024 12:32:27 GMT</pubDate>
    </item>
    <item>
      <title>[P]“实际”值有误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1em9msa/p_error_in_actual_values/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1em9msa/p_error_in_actual_values/</guid>
      <pubDate>Wed, 07 Aug 2024 11:44:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT（4o 模型）如何对快照执行反向搜索？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1em9g6j/d_how_gpt_4o_model_is_able_to_perform_a_reverse/</link>
      <description><![CDATA[      我将从网络浏览器中运行的网站点击的此快照附加到 ChatGPT（4o 模型），并提示“此快照来自哪个网站？”  https://preview.redd.it/2gs4vjbr98hd1.png?width=1180&amp;format=png&amp;auto=webp&amp;s=b6ad8bee668748f920d834eb1cb2ab89d3f8c76c 并且答案是正确的“该快照似乎来自 Stack Overflow 的开发人员调查，该调查提供了不同国家/地区按类型划分的开发人员薪资的见解。您可以在 Stack Overflow 开发者调查结果页面上找到这些数据。&quot; 什么样的下游任务能够实现这一点？    提交人    /u/samajhdar-bano2   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1em9g6j/d_how_gpt_4o_model_is_able_to_perform_a_reverse/</guid>
      <pubDate>Wed, 07 Aug 2024 11:34:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大型科技公司与生物科技公司的 AI/ML</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1em3ke2/d_aiml_in_big_tech_vs_biotech/</link>
      <description><![CDATA[我很好奇为什么一个优秀的 ML 工程师会离开大型科技公司（如谷歌、微软或 OpenAI）并加入生物科技公司。与科技公司正在发生的所有前沿创新相比，生物科技的吸引力何在？    提交人    /u/Pleasant_Wish1799   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1em3ke2/d_aiml_in_big_tech_vs_biotech/</guid>
      <pubDate>Wed, 07 Aug 2024 05:13:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 场景流估计的最新技术？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1em2uby/r_state_of_the_art_in_scene_flow_estimation/</link>
      <description><![CDATA[场景流估计的最新进展如何？如能提供建议，我们将不胜感激。    提交人    /u/DisciplinedPenguin   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1em2uby/r_state_of_the_art_in_scene_flow_estimation/</guid>
      <pubDate>Wed, 07 Aug 2024 04:31:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] GroundedAI：高效法学硕士评估的开源框架/模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1elx6ok/p_groundedai_opensource_frameworkmodels_for/</link>
      <description><![CDATA[我很高兴与大家分享 GroundedAI，这是我开发的开源框架，用于使用微调的小型语言模型和专门的适配器来评估大型语言模型应用程序输出。  主要特点： - 评估 LLM 输出的毒性、RAG 相关性和幻觉 - 具有特定于度量的适配器的高效小型语言模型 - 使用不到 5GB VRAM 进行本地评估 - 易于使用的 Python 包 - 仅需 38 亿个参数即可与 GPT4 性能相媲美 该框架目前包括三个主要评估器：1. 毒性评估器 2. RAG 相关性评估器 3. 幻觉评估器 每个评估器都使用一个基础模型，该模型在预热期间与专门的适配器合并，从而实现高效和特定于度量的评估。 我们的模型在 Hugging Face 上可用：https://huggingface.co/grounded-ai 我们欢迎社区的贡献和反馈。查看我们的 GitHub repo https://github.com/grounded-ai/grounded_ai 了解更多详细信息和文档。 如果您有任何问题或改进想法，请告诉我！    提交人    /u/Jl_btdipsbro   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1elx6ok/p_groundedai_opensource_frameworkmodels_for/</guid>
      <pubDate>Tue, 06 Aug 2024 23:53:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么过度参数化和重新参数化会产生更好的模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1elvkz6/d_why_does_overparameterization_and/</link>
      <description><![CDATA[Apple 的 mobileCLIP 网络的主干是 FastVIT，它在训练和推理时间之间使用网络重新参数化来生成具有更好性能的较小网络。我最近在几篇论文中看到了这种现象，但基本思想是在训练期间对模型进行过度参数化，然后在数学上减少它以进行推理。例如，您可以创建两个“分支”，每个分支都是一个独立的转换操作，然后将结果相加，而不是执行单个转换操作。它在训练期间将操作的参数加倍，但在推理期间您“重新参数化”在这种情况下，这意味着将两个分支的权重/偏差加在一起，从而产生一个数学上相同的卷积操作（相同的输入，相同的输出，一个卷积操作而不是两个相加的分支）。 通过在训练期间在几个操作上添加跳过连接，然后在推理期间将跳过数学地合并到操作权重中以产生相同的输出，而无需保留较早的层张量或进行额外的添加，也可以完成类似的技巧。 这种情况似乎等同于在训练期间将 y = a*x + b 修改为 y = (a1+a2)*x +b1+b2 以获得更多参数，然后只需返回基本形式使用 a = a1+a2 和 b = b1+b2 进行推理即可。 我从数学上理解这些操作是等效的，但我不太明白为什么过度参数化用于训练，然后减少参数化用于推理会产生更好的模型。我天真地认为，这会给网络增加更多的内存和计算，降低训练速度，而实际上并没有增强模型的容量，因为过度参数化的操作在数学上仍然等同于单个操作，无论它们是否真的减少了。这背后有强有力的理论吗，还是有人尝试过的一个有趣的想法碰巧奏效了？    提交人    /u/Revolutionary-Fig660   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1elvkz6/d_why_does_overparameterization_and/</guid>
      <pubDate>Tue, 06 Aug 2024 22:43:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] alphaXiv - ArXiv 的评论部分</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1elqzle/r_alphaxiv_a_comments_section_for_arxiv/</link>
      <description><![CDATA[我一直在研究一个 arXiv 实验室项目，alphaXiv.org，这是一个直接建立在 arXiv 之上的论文评论和讨论部分。我觉得很多读者经常对论文有相同的疑问，所以我希望有一个中央论坛对研究界大有裨益。上周，我们被斯坦福大学人工智能实验室报道。 请查看并告诉我您的想法！该项目正在积极开发中，如果您想合作或有反馈，请直接发信息给我。    提交人    /u/Vivid_Perception_143   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1elqzle/r_alphaxiv_a_comments_section_for_arxiv/</guid>
      <pubDate>Tue, 06 Aug 2024 19:36:45 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 使用 100 个愚蠢的 LLaMA 搜索，在 Python 上击败 GPT-4o</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1elo2d1/discussion_beat_gpt4o_at_python_by_searching_with/</link>
      <description><![CDATA[ 从这个惨痛的教训中，我们应该学到的一点就是通用方法的强大威力，这些方法即使可用的计算量变得非常大，也会随着计算量的增加而不断扩展。似乎以这种方式任意扩展的两种方法是搜索和学习。 Richard Sutton，惨痛的教训  Richard Sutton 的文章中令人不快的结论经常被误解：他们说，因为规模就是你所需要的一切，所以较小的模型注定会变得无关紧要。模型大小迅速增加到一万亿个参数以上，加上 GPU 内存的技术限制，似乎阻碍了任何地方的经济前沿智能，除了情报即服务提供商的寡头垄断。开放模型和自助推理正在撤退。 但正如上面的引文所示，扩展箭筒中实际上有两支箭：学习和搜索。学习，就像我们现在用神经网络所做的那样，在推理时随着内存而扩展——在其他条件相同的情况下，更大的模型表现更好，因为它们可以从训练集中提取更多数据到更多电路和更多模板中。搜索在推理时随着计算而平稳扩展——计算可以用于产生更高质量的候选者或产生更多的候选者。在理想情况下，可以通过所谓的缩放定律预测缩放行为。 最近的论文表明，像 LLM 这样的生成模型可以通过搜索进行扩展。上周，Brown、Juravsky 和合著者在 arXiv 上发表了一篇名为 Large Language Monkeys 的论文，其中包含了这方面的几个结果，并表明某些领域的前沿级智能可以从可在单个上一代 GPU 上运行的较小模型中引出。此外，他们观察到随着规模的扩大，性能会呈现平稳、可预测的提升。 更简单地说：以前，似乎前沿能力需要一只马大小的鸭子，而现在，很明显，我们可以用一百匹鸭子大小的马（或者更确切地说，LLaMA）来实现。 这个周末，我们着手复制这一发现。 在 Modal 上扩展 LLaMA 3.1 8B HumanEval 运行我们所有的实验（包括配置和测试），成本都远低于 50 美元。 您可以在此处找到我们的代码。您可以自行运行它，且不超过 Modal 免费套餐中包含的 30 美元/月信用额度。 指标和数据：HumanEval 和 pass@k 继续原帖...    提交人    /u/thundergolfer   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1elo2d1/discussion_beat_gpt4o_at_python_by_searching_with/</guid>
      <pubDate>Tue, 06 Aug 2024 17:41:05 GMT</pubDate>
    </item>
    <item>
      <title>[P] 接地 SAM 2：接地并跟踪一切</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1elmxnq/p_grounded_sam_2_ground_and_track_anything/</link>
      <description><![CDATA[      https://preview.redd.it/13854j03q2hd1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=0735848ae40c2591111fa4ed91d2c28ea829c0ac 随着 SAM 2 的发布，我们借此机会更新了我们的 Grounded SAM 算法。与 SAM 相比，SAM 2 最大的改进是将其分割功能扩展到视频，允许用户以交互方式分割视频中的任何对象并对其进行跟踪。然而，SAM 2 的主要问题是分割和跟踪的对象不包含语义信息。为了解决这个问题，我们延续了 Grounded SAM 的方法，加入了开放集检测模型，即 Grounding DINO。这使我们能够将 2D 开放集检测扩展到视频对象分割和跟踪。 我们已经在 https://github.com/IDEA-Research/Grounded-SAM-2 中发布了我们的代码，实现非常简单，方便用户使用。 项目亮点： 在此 repo 中，我们通过简单的实现支持以下演示：  使用 Grounding DINO、Grounding DINO 1.5 &amp; 1.6 和 SAM 2 对任何事物进行接地和分段 使用 Grounding DINO、Grounding DINO 1.5 &amp; 1.6 对任何事物进行接地和跟踪 1.6 和 SAM 2 基于强大的 https://github.com/roboflow/supervision 库检测、分割和跟踪可视化。  我们将继续更新代码，让用户更容易使用。    提交人    /u/Technical-Vast1314   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1elmxnq/p_grounded_sam_2_ground_and_track_anything/</guid>
      <pubDate>Tue, 06 Aug 2024 16:55:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ejkdhj/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ejkdhj/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 04 Aug 2024 02:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>