<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Tue, 18 Jun 2024 03:17:29 GMT</lastBuildDate>
    <item>
      <title>[D]有人在 ARR 六月（EMNLP）周期中收到过桌面拒绝警告吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dig8sk/ddid_anyone_receive_a_desk_rejection_warning_in/</link>
      <description><![CDATA[我于 6 月向 ARR (EMNLP) 提交了论文，并收到了以下消息。   所有作者都没有 ACL Anchology 个人资料，而且部分作者今年只接受了 1 篇论文（尚未发表）。 我认为，我们都没有资格成为审稿人（志愿者）（因为我们没有至少 3 篇论文）。 有人知道我们为什么收到此消息吗？    提交人    /u/ImpossibleAd568   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dig8sk/ddid_anyone_receive_a_desk_rejection_warning_in/</guid>
      <pubDate>Tue, 18 Jun 2024 02:46:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 具有有界激活函数的批量规范行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dicnld/d_batchnorm_behavior_with_bounded_activation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dicnld/d_batchnorm_behavior_with_bounded_activation/</guid>
      <pubDate>Mon, 17 Jun 2024 23:46:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 开源如何才能达到 Sora 或 RunwayML Gen-3 的质量？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di5txg/d_how_can_open_source_achieve_the_quality_of_sora/</link>
      <description><![CDATA[正在经历 https://github.com/PKU-YuanGroup/Open-Sora-Plan 我们如何提高输出质量？    提交人    /u/cbsudux   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di5txg/d_how_can_open_source_achieve_the_quality_of_sora/</guid>
      <pubDate>Mon, 17 Jun 2024 18:54:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] 量化评估基准中的差异</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di354e/r_quantifying_variance_in_evaluation_benchmarks/</link>
      <description><![CDATA[  由    /u/RSchaeffer  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di354e/r_quantifying_variance_in_evaluation_benchmarks/</guid>
      <pubDate>Mon, 17 Jun 2024 17:03:03 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用各种 PEFT 方法对具有 6GB GPU RAM 的 Gemma 2B LLM 进行微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di34p2/p_finetuning_gemma_2b_llm_with_6gb_gpu_ram_using/</link>
      <description><![CDATA[嗨，我想了解各种 PEFT 技术，我的目标是在我的 RTX A4000 笔记本电脑 GPU 上微调一些 LLM，它有大约 6GB 的可用 GPU RAM。这是我使用的存储库和方法说明的链接 https://github.com/kmkolasinski/keras-llm-light 使用的一些技术：  LoRA - 减少可训练参数的数量 带有异常值补偿的简单 Int8/Int4 量化 - 减少大内核的内存使用量 手动梯度检查点 - 减少训练模型时的内存使用量 推理中的 Transformer 块内存分配 - 仅存储当前处理的 Transformer 块的激活 XLA - 加快计算速度 混合精度训练 - 权重在 fp32 中（去量化后），激活在 fp16 中 沿序列轴节省内存的损失分割 - 减少计算最终损失和梯度     提交人    /u/kmkolasinski   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di34p2/p_finetuning_gemma_2b_llm_with_6gb_gpu_ram_using/</guid>
      <pubDate>Mon, 17 Jun 2024 17:02:38 GMT</pubDate>
    </item>
    <item>
      <title>[P] fast_mamba.np：纯粹、快速的 Mamba NumPy 实现，速度提高 4 倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di14et/p_fast_mambanp_pure_and_fast_numpy_implementation/</link>
      <description><![CDATA[      fast_mamba.np 查看了几个存储库后，我发现它们中的大多数都没有实现 Mamba 的本机缓存，以保持代码的简洁。缓存通常会使代码复杂化，这就是为什么我将 fast_mamba.np 实现为纯 Numpy 中具有缓存支持的 Mamba 的简单实现。此实现旨在简单高效，同时与 mamba.np 相比，在本地 CPU 上加速 4 倍。 https://github.com/idoh/fast_mamba.np $ python fast_mamba.py &quot;我有一个梦想&quot; &quot;&quot;&quot; 我有一个梦想，我将能够在早晨看到日出。Token count: 18, elapsed: 9.65s, 1.9 tokens/s &quot;&quot;&quot;  希望您觉得它有用 :)    submitted by    /u/id0h   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di14et/p_fast_mambanp_pure_and_fast_numpy_implementation/</guid>
      <pubDate>Mon, 17 Jun 2024 15:39:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 以下是如何使用 Graph RAG 获得比 std RAG 更好的准确率</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di09jm/d_heres_how_to_use_graph_rag_to_get_better/</link>
      <description><![CDATA[人员、机构等实体的信息通常高度互联，您的数据也可能如此。 如果是这样，您可以：  使用 TF-IDF 等创建连接具有共同 n-gram 的文档的图表。 在推理期间，搜索此图表以获取包含共同 n-gram 的邻居，并在 LLM 的上下文中使用它们。 Graph RAG 的搜索结果更有可能为您提供所搜索实体及其相关信息的全面视图。  例如，如果文档 A 被选为高度相关，则必须将包含与文档 A 紧密相关的数据的文档包含在上下文中才能提供完整的图像。 我花了一个周末创建了一个 Python 库，它可以自动为您的 vectordb 中的文档创建此图表。它还可以让您轻松检索与最佳匹配相关的相关文档。 这是该库的 repo：https://github.com/sarthakrastogi/graph-rag/tree/main    提交人    /u/sarthakai   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di09jm/d_heres_how_to_use_graph_rag_to_get_better/</guid>
      <pubDate>Mon, 17 Jun 2024 15:03:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于阅读 arxiv 论文的 AI 辅助方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1di05n8/d_ai_assissted_methods_for_reading_arxiv_papers/</link>
      <description><![CDATA[我非常想找到一种 AI 工具，可以轻松挑选出某篇论文中的进步（如果有的话）。 “与 Arxiv 聊天”听起来不错。 例如，某篇论文可能在数据集上得分更高，但究竟用什么方法？其背后的确切数学和代码是什么？与其他以前的方法相比。因此，GPT 模型不应该只分析文本，还应该分析该论文的任何 latex 和代码。 因此，不仅要分析论文，还要处理参考论文中的文本，还要找到与此相关的论文，并可能分析论文附带的代码库，例如 paperswithcode.com（如果有）。    提交人    /u/yasserius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1di05n8/d_ai_assissted_methods_for_reading_arxiv_papers/</guid>
      <pubDate>Mon, 17 Jun 2024 14:59:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基本但深刻的问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhzkxv/d_basic_but_deep_question/</link>
      <description><![CDATA[在训练大型模型时，谨慎选择学习率非常重要。如果选择了错误的学习率，最终会在训练上花费更多资金。 典型的建议是：&quot;尝试不同的学习率，看看哪种学习率最快&quot;。但是，这并不现实，因为：  最佳学习率会在训练过程中发生变化。 您将花费大量资金尝试不同的学习率，以确定是否值得继续使用该学习率。  大公司在这方面做了什么？什么是聪明、有效的解决方案？有人知道是否可以使用牛顿法近似二阶导数吗？    提交人    /u/Stefano939393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhzkxv/d_basic_but_deep_question/</guid>
      <pubDate>Mon, 17 Jun 2024 14:34:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在我的机器学习职业生涯中感到迷茫：需要建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhwzjk/d_feeling_lost_in_my_ml_career_advice_needed/</link>
      <description><![CDATA[大家好， 我希望这是发帖的正确地方，感谢您花时间阅读。 我今年 38 岁，来自一个贫穷的国家。父母抛弃了我，我和祖母一起长大，经历了巨大的损失和贫困。尽管我很聪明，但我一直在与注意力缺陷问题作斗争。 24 岁时，我搬到欧洲攻读计算机科学硕士学位，后来又攻读博士学位。学习六个月后，祖母去世了，导致我患上了严重的抑郁症，不得不暂停学业两年。最终，出于维持签证状态的需要，我恢复了学业，并获得了博士学位资助。我对信息检索产生了浓厚的兴趣，并于 2014 年完成了该领域的博士学位。 我的博士之旅充满挑战，抑郁和缺乏导师的支持。尽管如此，我还是发表了几篇论文，尽管我认为它们很平庸。即使在博士答辩之后，我仍然觉得自己像个大三学生。幸运的是，我在一家信誉良好的公司找到了一份工作，希望能提高自己的技能，但那是一个非技术环境。我研究了简单的 ML 模型并领导了 AI 路线图，更注重管理和领导力，而不是技术 ML 技能。 在此期间，ML 出现了重大进步，例如 BERT 和 GPT。现在我觉得我错过了这些发展。我的简历看起来令人印象深刻，有计算机科学学位、博士学位和 AI 团队经理，但我在编码和跟上新的 NLP 主题方面遇到了困难。 我确实喜欢管理和帮助他人成长，这是我的经理注意到并鼓励的（她对我说：“我从未见过有人花这么多时间帮助和培养他人，并在做这件事时表达出如此多的快乐”）。最近，我被目前的公司聘用为领导一个 NLP 研究和应用科学团队，但我觉得自己没有资格管理 ML 科学家，因为我自己并不是专家。 我也在考虑在更先进的科技公司探索从事尖端 NLP 研究的机会。我想向自己证明我有能力而不是无能。在过去的一年里，我一直专注于我的心理健康，并被诊断出患有严重抑郁症和 ADHD。这帮助我理解了我过去的行为，现在我处于一个更好的状态，正在努力完善自己。但我感到不知所措和迷茫，因为我觉得我不是真正的研究科学家，也不是 ML 工程师，也不是 AI 团队经理，因为我觉得我在所有方面都有所欠缺。 任何关于如何前进的建议都将不胜感激。 感谢您的时间和理解。    提交人    /u/Ikigai-iw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhwzjk/d_feeling_lost_in_my_ml_career_advice_needed/</guid>
      <pubDate>Mon, 17 Jun 2024 12:35:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过蒙特卡洛树自优化和 LLaMa-3 8B 访问 GPT-4 级数学奥林匹克解决方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhrfjk/r_accessing_gpt4_level_mathematical_olympiad/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhrfjk/r_accessing_gpt4_level_mathematical_olympiad/</guid>
      <pubDate>Mon, 17 Jun 2024 06:24:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] AlphaMath Almost Zero：无需流程的流程监督</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhr5sy/r_alphamath_almost_zero_process_supervision/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhr5sy/r_alphamath_almost_zero_process_supervision/</guid>
      <pubDate>Mon, 17 Jun 2024 06:04:57 GMT</pubDate>
    </item>
    <item>
      <title>[R] 创造力已不再是话题：消除语言模型偏见的代价</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhqs9g/r_creativity_has_left_the_chat_the_price_of/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhqs9g/r_creativity_has_left_the_chat_the_price_of/</guid>
      <pubDate>Mon, 17 Jun 2024 05:39:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​的混合精度训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/</link>
      <description><![CDATA[我在 2 层 MLP 上重新实现了 Nvidia 的原始混合精度训练论文（https://arxiv.org/abs/1710.03740）。我一直深入到 CUDA 领域来展示 TensorCore 激活，在我看来，这是混合精度训练的真正秘密。 代码：https://github.com/tspeterkim/mixed-precision-from-scratch 撰写：https://tspeterkim.github.io/posts/mixed-precision-from-scratch    提交人    /u/droidarmy95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dhlh0z/p_mixed_precision_training_from_scratch/</guid>
      <pubDate>Mon, 17 Jun 2024 00:28:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dh9f6b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 16 Jun 2024 15:00:16 GMT</pubDate>
    </item>
    </channel>
</rss>