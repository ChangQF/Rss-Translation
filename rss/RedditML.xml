<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Tue, 09 Jul 2024 06:21:34 GMT</lastBuildDate>
    <item>
      <title>[P] 对长期项目有什么建议吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dyuo28/p_recommendations_for_long_projects/</link>
      <description><![CDATA[我正在寻找计算机视觉或 NLP 领域需要付出努力（包括部署）的长期项目创意。我觉得小项目对我的学习没有多大帮助，我渴望深入研究更复杂、更有影响力的东西。 有人能建议一些具有挑战性的想法或资源吗？我可以在哪里找到这样的项目？我对那些不仅涉及开发阶段，而且涉及所有内容并帮助我放入 github 的项目特别感兴趣。    提交人    /u/Fun_Address8421   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dyuo28/p_recommendations_for_long_projects/</guid>
      <pubDate>Tue, 09 Jul 2024 05:35:28 GMT</pubDate>
    </item>
    <item>
      <title>[R] 关于口型同步/肢体动作的文献</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dyltuf/r_literature_on_lipsyncbody_gestures/</link>
      <description><![CDATA[大家好， 我正在寻找有关肢体语言和口型同步训练的研究论文。  我特别想了解 Heygen 和 Synthesia 等产品背后的方法。 如果有人知道任何基础研究或相关研究，我将非常感谢您的指导。 谢谢你的帮助！    提交人    /u/SaidBlahBlah   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dyltuf/r_literature_on_lipsyncbody_gestures/</guid>
      <pubDate>Mon, 08 Jul 2024 22:14:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您认为幻觉的未来会怎样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dyj284/d_what_do_you_think_is_the_future_of/</link>
      <description><![CDATA[随着大量新研究和工程技术的出现，法学硕士 (LLM) 中的幻觉正在减少，但除非权重以某种方式不断更新，并且我们对模型进行机械可解释性的逆向工程，并以足够好的近似值弄清楚一切究竟是如何存储和编码在模型权重中的，并操纵内部结构以完美地表示事实和程序，并对它们进行有效、更少错误的推理，以将幻觉最小化到足够好的水平，否则，将其实时地接地可能总是有效的。 而且，我们还需要基本事实数据来训练或生成尽可能少的人类幻觉，尽可能深入地涵盖尽可能多的主题，这本身就是一项艰巨的任务。这对于许多知识领域和各种深度来说都很难，或者在人类仍然知之甚少或一切都可能被打破的地方。看着你，心理学。 或者也许我们需要更多的神经符号学，或者全新的架构，但我认为使用逆向工程深度学习黑匣子来创建和引导它们的神经符号学可能足够好了。    提交人    /u/Happysedits   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dyj284/d_what_do_you_think_is_the_future_of/</guid>
      <pubDate>Mon, 08 Jul 2024 20:23:27 GMT</pubDate>
    </item>
    <item>
      <title>[R] 学习（在测试时学习）：具有富有表现力的隐藏状态的 RNN</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dyiidu/r_learning_to_learn_at_test_time_rnns_with/</link>
      <description><![CDATA[  由    /u/SchmidhuberDidIt  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dyiidu/r_learning_to_learn_at_test_time_rnns_with/</guid>
      <pubDate>Mon, 08 Jul 2024 20:01:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从 ChatGPT、Claude、Google Gemini 等中提取代码和文件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dycbqh/d_extracting_code_and_files_from_chatgpt_claude/</link>
      <description><![CDATA[因此，经过大量的迭代和思考，我想出了这个简单而美丽的东西：  javascript:(function(){async function main(){try{const e=document.body.innerText.trimStart(),t=function(e){const t=/[PROJECT_UPDATE]([\s\S]?)[/PROJECT_UPDATE]/g,n=/[ACTION:(create|modify|delete)]\s[PATH:(.?)](?:\s[CONTENT]([\s\S]*?)[/CONTENT])?/g,a=[];let r;for(;null!=(r=t.exec(e));){let e=r[1];let o;for(;null!=(o=n.exec(e));)a.push({action:o[1],path:o[2],content:o[3]||&quot;&quot;})}return a}(e);console.log(&quot;页面内容：&quot;,e),console.log(&quot;解析的更新：&quot;,t);if(0===t.length)throw new Error(&quot;页面内容中未找到更新&quot;);const a=await window.showDirectoryPicker();for(const e of t){if(&quot;create&quot;!==e.action&amp;&amp;&quot;modify&quot;!==e.action)continue;const n=e.path.split(&quot;/&quot;).slice(0,-1);let r=a;for(const e of n){r=await r.getDirectoryHandle(e,{create:!0})}const o=await r.getFileHandle(e.path.split(&quot;/&quot;).pop(),{create:!0}),i=await o.createWritable();await i.write(e.content),await i.close()}alert(&quot;项目文件已成功保存在所选目录中。&quot;)}catch(e){&quot;AbortError&quot;===e.name?console.log(&quot;用户取消了目录选择。&quot;):(console.error(&quot;Bookmarklet 错误：&quot;,e),alert(&quot;发生错误。请检查控制台了解详情。&quot;))}}main();})();  如果您将此提供给 LLM 并要求他们使用此格式输出文件，则可以使用此书签小程序捕获输出中的所有文件并将它们保存到您选择的文件夹中。 我很乐意看到这种东西内置到具有更复杂功能的库中。 我能够使用相同的机制让 Claude 能够通过如下命令语法在 Web 浏览器中执行 javascript 和 python（通过 pyIodide）... 您可能可以将命令语法调整为代码中的注释并让它正常编写代码但仍然捕获它...也许我会尝试一下。    提交人    /u/f0urtyfive   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dycbqh/d_extracting_code_and_files_from_chatgpt_claude/</guid>
      <pubDate>Mon, 08 Jul 2024 15:53:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] 在康威生命游戏中训练一个简单的 Transformer 神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dybuek/p_training_a_simple_transformer_neural_net_on/</link>
      <description><![CDATA[此练习展示了最简单的变压器形式，而康威生命游戏是其简单的数据来源。有趣的是，它学习计算基本上是 3×3 平均池的东西（尽管不包括内核中的中间单元）。 博客文章：https://sidsite.com/posts/life-transformer/ 代码：https://github.com/sradc/training-a-simple-transformer-on-conways-game-of-life    提交人    /u/montebicyclelo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dybuek/p_training_a_simple_transformer_neural_net_on/</guid>
      <pubDate>Mon, 08 Jul 2024 15:32:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] GraphRAG 是什么？解释一下</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dy5tge/r_what_is_graphrag_explained/</link>
      <description><![CDATA[本教程解释了什么是 GraphRAG，它是基线 RAG 的进步，它使用知识图谱而不是向量数据库进行检索，从而提高了输出质量。 https://youtu.be/14poVuga4Qw?si=y9Hxfy7NXZuN2XZI    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dy5tge/r_what_is_graphrag_explained/</guid>
      <pubDate>Mon, 08 Jul 2024 10:46:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 实时深度学习物体检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dy3wsc/p_real_time_deep_learning_object_detection/</link>
      <description><![CDATA[您好，Maschine 学习社区， 我目前正在使用 HoloLens2 开展一个关于增强组装的项目。我的任务是建立一个可以在 HoloLens 场景中实时检测物体的模型。一个重要的前提是，该模型应该能够预先使用尽可能少的知识来检测物体。所以我不想要一个需要带有高质量注释图像的大数据集来训练的模型。我希望该模型能够快速适应新的组装流程，而无需创建大数据集。我已经阅读了很多关于使用 Yolo 或 RCNN 进行少量镜头物体检测的文章，但我想听听你们是否有关于这项任务的经验或建议。 问候    提交人    /u/xelmaster420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dy3wsc/p_real_time_deep_learning_object_detection/</guid>
      <pubDate>Mon, 08 Jul 2024 08:38:20 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 神经解码 - 将歌曲聆听的脑电图数据映射到相应的音频文件</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxx0yw/research_neural_decoding_mapping_eeg_data_of_song/</link>
      <description><![CDATA[大家好， 我有一个数据集，其中包含参与者听一组 10 首歌曲的预处理 EEG 数据（脑电图时间序列数据）。我的目标是构建一个回归模型，将 EEG 数据作为输入映射回原始音频文件作为输出，以尝试重建参与者正在听的歌曲。这是一个利用 CNN 执行此操作的示例：https://arxiv.org/abs/2207.13845 以下是我遇到的一些障碍：  在链接的论文中，EEG 数据和目标音频文件被切成 1 秒长的片段，并在这些片段上训练 CNN。一个可能的问题是音频和 EEG 数据之间的延迟 - 大脑对刺激做出反应需要不可忽略的时间（大约 100 毫秒）。这会使 1 秒分段成为一种有问题的方法吗？我是否应该在模型中明确考虑这种延迟？或者模型应该在训练中“自行”考虑这个问题？是否有可以避免将数据划分为任意段的模型？ 我遇到的部分困难是我缺乏处理时间序列数据的经验，不知道哪些模型合适。CNN 是捕获短期和长期时间依赖性的理想选择吗？或者基于循环网络或变压器的架构是否更合适？我的直觉（可能完全错误）告诉我 RNN 更适合捕获短期依赖性，而变压器更适合捕获整个输入的依赖性。 降维会在这里有所帮助吗？输入数据是 64 通道 EEG 数据，采样率为 1024 Hz，平均歌曲长度约为 200 秒。这意味着十个输入数据中的每一个的大小约为 ~64 x 200K。我正在考虑使用 ICA，因为 EEG 数据可能很棘手且嘈杂（电极在微伏级上敏感），并且 ICA 通常与 EEG 数据一起使用以消除伪影。  任何建议或想法都将不胜感激。另外，我应该指出，我这样做既是为了研究也是为了学习目的 - 我目前不担心可扩展性/效率，我愿意使用现有框架或从头开始开发模型。 提前谢谢您。    提交人    /u/dusmansen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxx0yw/research_neural_decoding_mapping_eeg_data_of_song/</guid>
      <pubDate>Mon, 08 Jul 2024 01:49:47 GMT</pubDate>
    </item>
    <item>
      <title>[R] 尝试梯度下降。为什么不使用 Nelder-Mead？长篇。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxw5ge/r_experimenting_with_gradient_descent_why_not/</link>
      <description><![CDATA[关于 GD 有很多视频和网页。但是，几乎所有示例都过于简单。我挖掘了一些 2005 年的数据，这些数据只有五个参数需要确定，但 GD 却取得了成功。似乎 GD 总是在遇到障碍物时反弹。它也非常依赖于步长。需要对步长进行太多猜测。我想知道是什么让这变得如此困难，因为其他方法（如 Nelder-Mead）可以在几秒钟内找到最小值。为了回答我的问题，我打印出了每次迭代的 MSE 和五个参数或 MSE 和五个参数的变化。 使用真实数据时没有公式可以区分。我必须通过在每个方向上采取小步并除以增量的差值 2 来估计每个参数的导数。为了得到中心差分斜率，我必须对每个参数在正方向和负方向上采取一小步，然后除以两倍的步长。对于真实数据，导数可能会很嘈杂。每个小步骤都需要评估成本函数，因此需要 10 次评估才能计算梯度。这很耗时。我发现，与 YouTube 和其他常见示例不同，“地形”不像碗状，而是像大峡谷，有各种曲折。这意味着我不能使用大步，因为 GD 经常会在 5 个维度中遇到“峡谷壁”。然后我尝试了许多其他增强版的 GD，它们都不太好用，尽管有些会在一小时左右接近。GD-Adam GD-RMSprop GD-delta GD-AdaGrad 表现不佳。但是，我确实找到了一个新的附加代码，它使所有这些代码，尤其是简单的 GD 工作得更好。诀窍是迈出一步。评估它，如果它更好，那么就朝同一方向再迈一步，直到 MSE 不再变好，然后重复循环以获得新的梯度。这样做的好处是，计算“撞”在“墙壁”上的梯度所浪费的时间更少。此外，步长可以小一点，以导航“地形”中曲折的部分，但当找到一个好的方向时，能够朝一个方向迈出多步就像迈出更大的一步。 直观地讲，它的工作原理是这样的。起点是峡谷中某个有溪流的地方。溪流是当前的低点。然而，它蜿蜒曲折，所以任何一步都会撞到溪流的岸边，甚至可能是峡谷的墙壁。最好不要完全沿着溪流走，因为有很多曲折。最好尽可能在一个方向上“飞越溪流”，在“峡谷壁”之间飞越。这样可以避免大量计算梯度，因为如果有大量数据，则必须计算大量梯度，这可能非常耗时。 现在，如果您拥有像碗一样的漂亮地形，所有这些都没有区别。您在 YouTube 或网站上看到的任何方法都可以使用，但我从未见过一种方法可以处理真实数据，而且地形很可能不是像碗一样的结构。此外，找到具有 10 个点的导数在时间上非常昂贵。 这就是为什么我想知道为什么不使用 Nelder-Mead，除非它要高效，否则它会占用大量内存，但内存很便宜。 我看过很多关于不同形式梯度下降的 YouTube 视频。我相信大多数人教的都是他们所学的东西，并没有真正处理过真实数据。 接下来是找到一些真实的训练数据来识别 8x8 网格中的字符，但即使是 16x16 点阵也很好。我知道我正在重新发明轮子，但我想尝试我的新附加代码和 Nelder-Mead 来最小化成本函数 GD #gradient-descent, #Nelder-Mead, #Adam #AdaGrad, #Delta, #RMSprop    提交人    /u/pnachtwey   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxw5ge/r_experimenting_with_gradient_descent_why_not/</guid>
      <pubDate>Mon, 08 Jul 2024 01:04:33 GMT</pubDate>
    </item>
    <item>
      <title>[项目] minigrad - andrej karpathy 用 Go 语言实现的 micrograd</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxvw4c/project_minigrad_andrej_karpathys_micrograd/</link>
      <description><![CDATA[这个周末，我想学习更多关于 ML 的知识并继续从事我的 ML 项目。所以我用 golang（重新）构建了 karpathy 的 micrograd。也学习了更多关于反向传播的知识。很想知道您的反馈。请做出贡献并在 GitHub repo 上留下一颗星。 github - https://github.com/0verread/minigrad    提交人    /u/ElegantGoose9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxvw4c/project_minigrad_andrej_karpathys_micrograd/</guid>
      <pubDate>Mon, 08 Jul 2024 00:51:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] Open-TeleVision：具有沉浸式主动视觉反馈的远程操作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxtsiq/r_opentelevision_teleoperation_with_immersive/</link>
      <description><![CDATA[        提交人    /u/XiaolongWang   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxtsiq/r_opentelevision_teleoperation_with_immersive/</guid>
      <pubDate>Sun, 07 Jul 2024 23:09:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你们都用什么进行大规模训练？普通的 pytorch 还是使用像 HF Accelerate 这样的库。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/</link>
      <description><![CDATA[我很快就要为研究论文训练一个大型集群多机器。好奇你们都为大规模训练做了什么，是坚持我所知道的 pytorch（FSDP、DDP、TP、MP 等...）和 slurm 更好，还是值得学习像 HF accelerate 这样的东西进行大规模训练？    提交人    /u/I_will_delete_myself   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxtaez/d_what_do_you_all_use_for_large_scale_training/</guid>
      <pubDate>Sun, 07 Jul 2024 22:46:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在深度模型上进行“深度工作”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/</link>
      <description><![CDATA[大家好， 我最大的生产力挑战之一是等待深度学习训练循环、标记化或处理循环运行时的停机时间。对于较短的循环，这些循环可能需要 5 分钟到一个小时的时间，在此期间，我常常发现自己不知道该做什么。 开始一项新任务很困难，因为不断的上下文切换会打乱我的工作流程和注意力。 我以前在大学里遵循深度工作方法，这确实有助于控制我的注意力缺陷多动障碍。我白天不使用手机或社交媒体，一次只“专注于”一项任务。 现在，我觉得这几乎是不可能的。我“被迫”休息这些小憩，不断在任务之间切换，这非常具有挑战性。 您对如何充分利用这些间隔有什么建议吗？你会为这些时间段保留特定任务吗？ 即使从专注编码切换到阅读论文，如果只花 10 分钟左右的时间，也会非常困难。 有人解决过这些问题吗，还是只有我？ 谢谢。    提交人    /u/Magnospm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dxg0jg/d_deepwork_while_working_on_deep_models/</guid>
      <pubDate>Sun, 07 Jul 2024 13:02:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1dx5tpo/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 07 Jul 2024 02:15:10 GMT</pubDate>
    </item>
    </channel>
</rss>