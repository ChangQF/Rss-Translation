<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Tue, 20 Aug 2024 09:17:14 GMT</lastBuildDate>
    <item>
      <title>[D] 代码提交 – 审核奖励</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewqh0a/d_code_submission_bonus_for_review/</link>
      <description><![CDATA[对于 AAAI-25，在提交论文时上传代码是否会提供任何审查奖励？还是在接受后上传代码就足够了？只是想判断在审查之前是否值得付出额外的努力。    提交人    /u/Ok_Butterfly7408   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewqh0a/d_code_submission_bonus_for_review/</guid>
      <pubDate>Tue, 20 Aug 2024 08:39:13 GMT</pubDate>
    </item>
    <item>
      <title>[D]如何准备面试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewqf15/dhow_to_prepare_for_an_interview/</link>
      <description><![CDATA[我参加了 VISA 暑期实习生的面试，想为面试做准备。我已经研究过这些主题，想知道从面试的角度该怎么做     提交人    /u/Worldly-Duty4521   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewqf15/dhow_to_prepare_for_an_interview/</guid>
      <pubDate>Tue, 20 Aug 2024 08:35:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有没有什么好的50M以下开源LLM推荐？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewptwn/d_suggetion_about_the_best_open_source_llm_with_a/</link>
      <description><![CDATA[大家好，我正在为 Web 开发文本校正功能，希望大家能就最佳模型提出建议。我做了一些研究，但还没有找到理想的选择： + Editsaurus：这个工具已经过时，过于简单，上次更新是 8 年前。它也缺乏我用例所需的性能。 + LanguageTool：虽然功能强大，但这个工具不适合 Web 环境，因为它需要服务器部署，而我想避免这种情况。 + Web-LLM：这是一个基于 Web 的 LLM 推理引擎。但是，它有一个明显的缺点——页面首次加载时需要大约 5 分钟来下载模型。尽管如此，它仍然是我迄今为止发现的最佳选择。  我正在寻找一个更高效、更小、不到 50MB 的开源 LLM 来处理文本校正任务，有人有什么建议吗？也欢迎任何其他建议，提前致谢。    提交人    /u/waa007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewptwn/d_suggetion_about_the_best_open_source_llm_with_a/</guid>
      <pubDate>Tue, 20 Aug 2024 07:51:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] PEDAL：使用多样化样本通过大型语言模型增强贪婪解码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewju22/r_pedal_enhancing_greedy_decoding_with_large/</link>
      <description><![CDATA[https://arxiv.org/abs/2408.08869    提交人    /u/Either_Pea7803   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewju22/r_pedal_enhancing_greedy_decoding_with_large/</guid>
      <pubDate>Tue, 20 Aug 2024 02:01:59 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无根基对齐问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewfafd/r_the_ungrounded_alignment_problem/</link>
      <description><![CDATA[  由    /u/bregav  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewfafd/r_the_ungrounded_alignment_problem/</guid>
      <pubDate>Mon, 19 Aug 2024 22:33:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 人们是否已经不再使用“微调”而改用“监督微调”了？或者是否存在其他微调范式方法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ewezs4/d_have_people_stopped_saying_fine_tuning_in_place/</link>
      <description><![CDATA[我一直认为微调意味着我们可以进行监督。但如今，人们似乎都说“SFT”。很好奇历史是怎样的。    由   提交  /u/Seankala   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ewezs4/d_have_people_stopped_saying_fine_tuning_in_place/</guid>
      <pubDate>Mon, 19 Aug 2024 22:20:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 代理系统的自动化设计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ew5eff/r_automated_design_of_agentic_systems/</link>
      <description><![CDATA[  由    /u/hardmaru  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ew5eff/r_automated_design_of_agentic_systems/</guid>
      <pubDate>Mon, 19 Aug 2024 15:56:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 深入 Transformers 和 LLM 世界 – 一步步了解 Go 中的 Llama 3.1</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ew2l3i/p_dive_into_transformers_and_llm_world_llama_31/</link>
      <description><![CDATA[我很高兴在此展示我的最新开源项目的更新版本：Llama Nuts and Bolts。上一个版本是为 Llama 2 构建的，现在已更新为支持 Llama 3.1 8B-Instruct 模型。 代码和文档：https://github.com/adalkiran/llama-nuts-and-bolts 现在，文档也可以在 Github Pages 上找到：https://adalkiran.github.io/llama-nuts-and-bolts 如果您像我一样对 LLM（大型语言模型）和转换器的工作原理感到好奇，并且已经深入研究了源代码中的概念解释和示意图，但渴望更深入的理解，那么这个项目也非常适合您！ 您不仅可以找到 Llama 架构的细节，将在文档目录中找到各种相关概念的解释。从逐字节读取 Pickle、PyTorch 模型、Tiktoken 标记器模型文件，到 BFloat16 数据类型的内部结构、从头开始实现 Tensor 结构以及包括线性代数计算在内的数学运算。 该项目最初是为了通过运行和调试来了解 LLM 背后的作用，并且仅用于实验和教育目的，而不是用于生产用途。 目标是制作一个实验项目，可以完全在 Python 生态系统之外（使用 Go 语言）对 Llama 3.1 8B-Instruct 模型执行推理。在整个旅程中，目标是获取知识并阐明这项技术的抽象内部层。 这次旅程是一次有意重新发明轮子的旅程。在阅读文档中的旅程时，您将通过 Llama 模型的示例了解大型语言模型的工作原理。 如果您查看它，我会很高兴，欢迎发表评论！    提交人    /u/adalkiran   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ew2l3i/p_dive_into_transformers_and_llm_world_llama_31/</guid>
      <pubDate>Mon, 19 Aug 2024 14:01:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 了解 Transformers 和 LLM 的图解书</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/</link>
      <description><![CDATA[      我在这个 subreddit 上看到过好几个人对 Transformers 和 LLM 内部工作原理的长篇解释感兴趣的例子。 这是我和我双胞胎兄弟在过去三年半里一直试图填补的空白。上周，我们发表了“超级学习指南：Transformers &amp;大型语言模型”，这是一本 250 页的书，包含 600 多幅插图，面向对该领域有浓厚兴趣的视觉学习者。 本书深入介绍了以下主题：  基础：神经网络入门以及用于训练和评估的重要深度学习概念。 嵌入：标记化算法、词嵌入（word2vec）和句子嵌入（RNN、LSTM、GRU）。 Transformers：其自注意力机制背后的动机、编码器-解码器架构的详细概述以及相关变体（如 BERT、GPT 和 T5），以及如何加速计算的技巧和窍门。 大型语言模型：调整基于 Transformer 的模型的主要技术，例如即时工程、（参数高效的）微调和偏好调整。 应用：最常见的问题，包括情感提取、机器翻译、检索增强生成等等。  （如果您想知道：此内容与我们 5-6 年前在此 subreddit 上分享的斯坦福插图学习指南的氛围相同，关于 CS 229：机器学习、CS 230：深度学习 和 CS 221：人工智能) 学习愉快！ https://preview.redd.it/n6zraaltemjd1.jpg?width=1905&amp;format=pjpg&amp;auto=webp&amp;s=1110f750df0d8a60d5fdf1d4967b41e1b5617efe    提交人    /u/shervinea   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/</guid>
      <pubDate>Mon, 19 Aug 2024 13:14:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用卡尔曼滤波器实现 YOLO，用四旋翼飞行器跟踪一个人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1evyx16/p_implemented_yolo_with_kalman_filter_do_track_a/</link>
      <description><![CDATA[大家好， 在我的论文中，我正在使用四旋翼飞行器跟踪移动物体（人、汽车等），同时避开障碍物（类似于 Skydio）。我实现了带有卡尔曼滤波器的 YOLO 来跟踪物体。 而且它工作正常。但是，我还想预测物体的轨迹（接下来的 N 帧）以支持四旋翼飞行器的运动规划。一个想法是通过人的姿势来预测人的轨迹。假设我可以检测到该人将右肩指向左侧，因此表明该人将开始向左移动。有没有关于这个主题的研究论文？    提交人    /u/Huge-Leek844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1evyx16/p_implemented_yolo_with_kalman_filter_do_track_a/</guid>
      <pubDate>Mon, 19 Aug 2024 11:02:52 GMT</pubDate>
    </item>
    <item>
      <title>大规模 GPU 训练云提供商建议 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1evxzd8/advice_on_cloud_provider_for_large_scale_gpu/</link>
      <description><![CDATA[大家好，我目前正在通过 LLM 进行代码生成，考虑到手头的问题，我的团队必须对 LLM 进行大量微调和有时预训练实验。到目前为止，我们一直在使用 VastAI 满足我们的 GPU 相关要求，但有几次我们在训练约 3 周后丢失了检查点。因此，我们正在考虑选择不同的云服务提供商来满足我们的 GPU 相关要求。 对于像 CodeLlama 7B 这样的大型模型，我们使用 4 个 GPU 的集群进行并行训练，每个 GPU 有 48GB（可以达到 80）的 RAM。 选择时要考虑的要点： - 预安装的软件包（TensorFlow、PyTorch） - CUDA 版本 12 或更高版本。 - GPU（如 A100 节点），每个节点至少有 80GB 的 VRAM。 - 至少 250GB 的存储空间。  客户提出的选项 - AWS - Salad - Vultr - Scale ways 抱歉，如有错别字，敬请原谅。您的建议将对我们意义重大。谢谢    提交人    /u/Unlikely-Addition-42   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1evxzd8/advice_on_cloud_provider_for_large_scale_gpu/</guid>
      <pubDate>Mon, 19 Aug 2024 10:03:18 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么我们要用随机值初始化神经网络以打破对称性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1evwap1/d_why_we_initialize_the_neural_networks_with/</link>
      <description><![CDATA[我在 ANN 领域还没有那么多经验，所以我希望这个问题没有完全偏离图表 :) 我发现神经网络用随机值初始化其权重和偏差，以确保这些值不会在相同或对称的值上初始化。 我完全理解为什么它们不能相同 - 除了一个节点之外的所有节点都是多余的。 我无法理解的是为什么它们不能是对称的。我在 YouTube 上没有找到关于它的一个视频，当我一直问为什么不这样做时，GPT 低调地告诉我，如果你有一个相关权重范围（假设为 -10 到 10），那么实际上最好将它们初始化得尽可能远，而不是使用其中一种随机算法。 GPT 提到的唯一问题是完全分离的节点的交付。 谁能向我解释为什么每个人都使用随机初始化？    提交人    /u/kotvic_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1evwap1/d_why_we_initialize_the_neural_networks_with/</guid>
      <pubDate>Mon, 19 Aug 2024 08:04:43 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 我为 Pokémon BDSP 创造了终极自动闪光猎人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1evp3wz/project_i_created_the_definitive_automatic_shiny/</link>
      <description><![CDATA[      大家好！我是 Dinones！我编写了一个使用对象检测的 Python 程序，让我的电脑在我睡觉时在我的实体 Nintendo Switch 上捕捉闪光神奇宝贝。到目前为止，我已经在 Pokémon BDSP 中自动捕捉了闪光宝可梦，如 Giratina、Dialga 或 Azelf、Rotom、Drifloon、所有三种初始宝可梦等等。想知道它是如何工作的吗？快来看看吧！该程序对所有人开放！显然是免费的；我只是一个喜欢在空闲时间编写这些程序的学生 :) 游戏在 Nintendo Switch（不是模拟的，是真实的）上运行。该程序使用捕获卡获取输出图像，然后对其进行处理以检测宝可梦是否闪光（OpenCV）。最后，它使用蓝牙（NXBT）模拟 joycons 并控制 Nintendo。 也可以在 Raspberry Pi 上使用！ 我不会用这个赚钱，我只是觉得我的项目会让很多人感兴趣。 📽️ Youtube：https://www.youtube.com/watch?v=84czUOAvNyk 🤖 Github：https://github.com/Dinones/Nintendo-Switch-Pokemon-Shiny-Hunter https://preview.redd.it/7jbe6fdxrijd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=626c801925fb0769f59e62ece09f0e00b18b828e https://preview.redd.it/2h2alqcxrijd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=fddd11c5c04c58268bbaf0e8bca0fd7081a7f775    提交人    /u/Dinones   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1evp3wz/project_i_created_the_definitive_automatic_shiny/</guid>
      <pubDate>Mon, 19 Aug 2024 01:00:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1euyfi6/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1euyfi6/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 18 Aug 2024 02:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>