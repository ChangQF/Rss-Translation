<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 03 Mar 2024 12:21:45 GMT</lastBuildDate>
    <item>
      <title>[r] 法学硕士实时网络搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5fv1f/r_realtime_web_search_for_llms/</link>
      <description><![CDATA[我想向 LLM 解决方案添加一些实时网络搜索。我认为最简单的方法是找到给定查询的前 n 个网站，抓取这些页面，嵌入和检索与查询最相似的块，并将它们添加到上下文中。 我发现以任何合适的速度进行最困难的部分是抓取。 关于执行此操作的库/API 有什么想法吗？  或者我可能缺少任何更好的方法？    由   提交/u/Overall-Marsupial-65   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5fv1f/r_realtime_web_search_for_llms/</guid>
      <pubDate>Sun, 03 Mar 2024 12:17:16 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求建议：持续强化学习和元强化学习研究社区</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5fmgj/d_seeking_advice_continualrl_and_metarl_research/</link>
      <description><![CDATA[我对 RL（连续 RL、元 RL、变压器）对超参数的敏感性和大量的训练时间越来越感到沮丧（我讨厌 RL 之后5年博士研究）。这在元强化学习连续强化学习中尤其成问题，其中一些基准要求长达 100 小时的训练。这使得优化超参数或快速验证新想法的空间很小。考虑到这些挑战以及我准备更深入地探索数学理论，包括学习所有可用的在线数学课程，采用基于证明的方法，以避免无休止的等待和训练循环，我对 2024 年密切相关的人工智能研究领域趋势感到好奇强化学习，但最多只需要 3 个小时的训练时间。有什么建议吗？   由   提交 /u/Noprocr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5fmgj/d_seeking_advice_continualrl_and_metarl_research/</guid>
      <pubDate>Sun, 03 Mar 2024 12:03:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何利用 Databricks 工作流程来实施 dbt 云作业</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5ffqt/r_how_to_leverage_databricks_workflows_to/</link>
      <description><![CDATA[       由   提交/u/awetabcod  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5ffqt/r_how_to_leverage_databricks_workflows_to/</guid>
      <pubDate>Sun, 03 Mar 2024 11:51:55 GMT</pubDate>
    </item>
    <item>
      <title>[P] 如何将经过训练的 LORA 适配器部署到 Huggingface 中的 ChatUI？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b595y8/p_how_do_i_deploy_my_trained_lora_adapter_to_a/</link>
      <description><![CDATA[我在 Mistral8x7b 上有一个训练有素的 LORA 适配器，并上传到 Huggingface 上。如何将其部署到 HuggingchatUI 界面，使其看起来像这样？ Zephyr Gemma Chat - HuggingFaceH4 1 的拥抱面部空间。 I我这么问是因为到目前为止我看到的教程和指南仅适用于非适配器的模型。 谢谢。   由   提交/u/portmanteau98  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b595y8/p_how_do_i_deploy_my_trained_lora_adapter_to_a/</guid>
      <pubDate>Sun, 03 Mar 2024 05:15:01 GMT</pubDate>
    </item>
    <item>
      <title>[N] Chris Van Pelt：机器学习工具、权重和偏差、创业精神 |从机器学习中学习 #9</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b57p0e/n_chris_van_pelt_ml_tooling_weights_and_biases/</link>
      <description><![CDATA[       Chris Van Pelt，Weights &amp; 联合创始人Biases 和 Figure 8/CrowdFlower 在 MLOps 平台的开发中发挥了关键作用，在过去的二十年里一直致力于完善 ML 工作流程并使机器学习变得更容易实现。 Chris 提供了有关当前状态的宝贵见解行业的。他强调了重量和重量的重要性。 Biases 作为一种强大的开发工具，使机器学习工程师能够应对实验、数据可视化和模型改进的复杂性。他对评估机器学习模型和解决人工智能炒作与现实之间的差距所面临的挑战进行了坦诚的反思，让他对这个领域的复杂性有了深刻的理解。 凭借与他人共同创立两家机器学习公司的创业经验，Chris 离开了我们在韧性、创新以及对科技领域人性维度的深刻理解方面学到了教训。   由   提交 /u/NLPnerd   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b57p0e/n_chris_van_pelt_ml_tooling_weights_and_biases/</guid>
      <pubDate>Sun, 03 Mar 2024 03:56:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] [CVPR 2024] AV-RIR：视听室脉冲响应估计</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b531o1/r_cvpr_2024_avrir_audiovisual_room_impulse/</link>
      <description><![CDATA[       由   提交/u/Snoo63916   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b531o1/r_cvpr_2024_avrir_audiovisual_room_impulse/</guid>
      <pubDate>Sun, 03 Mar 2024 00:08:50 GMT</pubDate>
    </item>
    <item>
      <title>[P] TimesFM：Google 的时间序列预测基础模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b5132c/p_timesfm_googles_foundation_model_for_timeseries/</link>
      <description><![CDATA[       由   提交 /u/apaxapax   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b5132c/p_timesfm_googles_foundation_model_for_timeseries/</guid>
      <pubDate>Sat, 02 Mar 2024 22:45:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 解释 Transformers + VQ-VAE = 可以生成图像的 LLM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4xjp2/d_explaining_transformers_vqvae_llms_that_can/</link>
      <description><![CDATA[       由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4xjp2/d_explaining_transformers_vqvae_llms_that_can/</guid>
      <pubDate>Sat, 02 Mar 2024 20:15:01 GMT</pubDate>
    </item>
    <item>
      <title>[P] ArXiv 机器学习景观</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4txb8/p_arxiv_machine_learning_landscape/</link>
      <description><![CDATA[ 由   提交/u/lmcinnes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4txb8/p_arxiv_machine_learning_landscape/</guid>
      <pubDate>Sat, 02 Mar 2024 17:42:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] Panda-70M：与多个跨模态教师一起为 7000 万个视频添加字幕</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4stoa/r_panda70m_captioning_70m_videos_with_multiple/</link>
      <description><![CDATA[训练 AI 理解和描述视频内容需要数据集，而这些数据集对于人类手动注释来说成本高昂。现在，来自 Snap、加州大学默塞德分校和特伦托大学的研究人员整理了一个名为 Panda-70M 的新数据集，旨在提供帮助。 这个新数据集包含 7000 万个高分辨率 YouTube 剪辑，并配有描述性字幕。关键是他们使用了具有多个跨模式“老师”的自动化管道。人工智能模型根据视频、字幕、图像等不同输入生成字幕。 一些亮点：  70M 720p YouTube 剪辑时长 8 秒，包含 13 个单词的字幕 教师模型包括视频 QA、图像字幕、文本摘要 教师团队可以准确地描述 84 % 剪辑与任何单一模型的 31% 在此数据集上进行预训练可显着提高视频 AI 模型的性能： 微调 250 万个小数据后，字幕准确度提高 18%子集 在文本视频检索方面提高 7% 视频生成错误减少 77%   局限性仍然围绕内容多样性、字幕密度和自动化质量。但我认为这是组装大规模视频文本训练数据以推进多模式人工智能的一大进步。 像这样的高效管道可以释放接近人类理解水平的视频理解能力。很高兴看到一些在 Panda-70M 上训练的模型可用。 论文此处。 此处摘要。   由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/1b4stoa/r_panda70m_captioning_70m_videos_with_multiple/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4stoa/r_panda70m_captioning_70m_videos_with_multiple/</guid>
      <pubDate>Sat, 02 Mar 2024 16:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您的 LLM 技术堆栈在生产中是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/</link>
      <description><![CDATA[想知道每个人都使用什么来实现 LLM 支持的应用程序以供生产使用，以及您对这些工具和建议的体验。  这就是我为金融和资本市场用户构建的一些 RAG 原型所使用的。 预处理\ETL：非结构化.io + Spark、Airflow 嵌入模型： Cohere Embed v3 以前使用 OpenAI Ada，但 Cohere 对于我的用例来说具有明显更好的检索召回率和精度。还探索其他开放权重嵌入模型 矢量数据库： Elasticsearch 以前但现在使用 Pinecone LLM： 经历了相当长的一段时间很少包括托管和自托管选项。在原型设计过程中早期使用 gpt4，然后切换到 gpt3.5-turbo，以获得更易于管理的成本并最终开放权重模型。  现在使用由 vLLM 自托管的经过微调的 Llama2 30B 模型 LLM 框架：最初从 Langchain 开始，但发现扩展为应用程序很麻烦变得更加复杂。在某个时候尝试在 LlamaIndex 中实现它只是为了学习，但发现它同样糟糕。回到 Langchain，现在我正在用自己的逻辑替换它 其他人都使用什么？    ;由   提交/u/gamerx88  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4sdru/d_what_is_your_llm_tech_stack_in_production/</guid>
      <pubDate>Sat, 02 Mar 2024 16:37:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Google TPU 的经验？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4oypz/d_experience_with_google_tpu/</link>
      <description><![CDATA[有人有将软件移植到 Google TPU 的经验吗？它与提升和转移现有 PyTorch 或 TensorFlow 工作负载一样简单，还是更复杂？  从头开始编写代码怎么样，更容易吗？   由   提交 /u/siliconductor1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4oypz/d_experience_with_google_tpu/</guid>
      <pubDate>Sat, 02 Mar 2024 14:02:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] BitNet 1-b/b1.58 LLM - 这对 nvidia 构成威胁吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4lhjt/d_bitnet_1bb158_llms_is_that_a_threat_to_nvidia/</link>
      <description><![CDATA[论文链接：https://arxiv.org /pdf/2402.17764.pdf 这是真的吗？听起来好得令人难以置信，对吧？如果这是真的，它不仅减少了训练和运行 LLM 所需的 VRAM 容量和带宽，还建议简化硬件实现，因为不需要 matmul ，它只需要 + 运算 不是对 nvidia（股票）和 AMD 也构成威胁吗？   由   提交/u/tunggad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4lhjt/d_bitnet_1bb158_llms_is_that_a_threat_to_nvidia/</guid>
      <pubDate>Sat, 02 Mar 2024 10:42:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人形运动作为下一个令牌预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b4kriv/r_humanoid_locomotion_as_next_token_prediction/</link>
      <description><![CDATA[论文： https:// arxiv.org/abs/2402.19469 摘要：   我们将现实世界的人形控制视为下一个令牌预测问题，类似于预测语言中的下一个单词。我们的模型是通过感觉运动轨迹的自回归预测训练的因果变换器。为了考虑数据的多模态性质，我们以模态对齐的方式执行预测，并且对于每个输入标记从相同模态预测下一个标记。这种通用的公式使我们能够利用缺少模式的数据，例如没有动作的视频轨迹。我们根据来自先前神经网络策略、基于模型的控制器、动作捕捉数据和人类 YouTube 视频的一组模拟轨迹来训练我们的模型。我们展示了我们的模型能够让全尺寸的人形机器人零射击地在旧金山行走。即使仅使用 27 小时的步行数据进行训练，我们的模型也可以转移到现实世界，并且可以泛化到训练期间未见过的命令，例如倒退行走。这些发现表明，通过感觉运动轨迹的生成模型来学习具有挑战性的现实世界控制任务是一条有希望的道路。   ​   由   提交 /u/StartledWatermelon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b4kriv/r_humanoid_locomotion_as_next_token_prediction/</guid>
      <pubDate>Sat, 02 Mar 2024 09:54:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>