<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 05 Dec 2024 09:19:08 GMT</lastBuildDate>
    <item>
      <title>[N] Hugging Face 首席执行官对中国开源 AI 模型表示担忧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/</link>
      <description><![CDATA[Hugging Face 首席执行官表示，如果开源模型恰好是由中国人创建的，那么成为 SOTA 就是件坏事。例如，Tech Crunch 向其中一个 Qwen 模型 (QWQ 32B) 询问“1989 年 6 月 4 日，中国北京发生了什么？”，而该模型回答说“我无法提供有关该主题的信息”（我以我的生命向上帝发誓，我不知道那天这里发生了什么，而且永远不会问模型这个问题 - 永远不会。它不会影响我使用模型的体验）。 首席执行官认为对开源模型进行审查是最好的，他指出，如果像中国这样的国家“成为人工智能领域最强大的国家，他们将能够传播西方世界可能不希望看到的某些文化方面。”也就是说，他认为人们不应该在世界各地传播非“西方”起源的思想。作为一个在美国出生和长大的人，我老实说，我不知道他所说的“西方世界不想看到传播的想法”是什么意思，因为我是“西方人”，也不支持全面审查。 文章在这里：引用。 对于支持此类观点的人，我有一个合理的问题 - 您宁愿使用具有西方偏见的低质量（基准较差）模型，还是使用中国创建的 AGI 级开源 7B 模型？如果是，为什么？    提交人    /u/AIAddict1935   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h7185x/n_hugging_face_ceo_has_concerns_about_chinese/</guid>
      <pubDate>Thu, 05 Dec 2024 04:49:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 除了模型性能指标的变化之外的数据漂移检测方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/</link>
      <description><![CDATA[大家好， 正如标题所暗示的，我一直依靠（接近）实时监控模型性能指标来查看我的用例中是否发生了数据漂移。 我想知道您是否知道其他更复杂/更高级的方法来检测数据漂移。很想听听任何类型的方法，无论它们针对的是协变量/特征漂移、目标/标签漂移还是概念漂移的检测。 如果您可以分享任何 Python 或 R 实现来执行上述数据漂移检查就更好了。 提前谢谢！    提交人    /u/YsrYsl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6woaf/d_data_drift_detection_methods_aside_from_changes/</guid>
      <pubDate>Thu, 05 Dec 2024 01:02:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将 Pytorch 模型打包为 exe。最好的方法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/</link>
      <description><![CDATA[      我有一些设计用于本地运行的 Pytorch 模型，在本地机器上进行训练和推理。 GUI 是使用另一种语言创建的，计划是将所有 Python 方面打包成可执行文件并通过 Python 等效的子进程运行它（并在两者之间传输非常基本的数据）。我将在 Windows 和 Mac 上跨平台运行 有多个辅助脚本可以读取数据并对其进行处理（数据提取 + 特征工程）。虽然我广泛使用了矢量化函数，但我对某些代码使用了 Cython 化方法，并且我正在使用 Cython 编译底层脚本（因此几乎所有内容都是编译后的二进制文件，除了入口点，例如 main.py）。 我的辅助库是常见的 Pandas、Numpy (1.x)、SciKit learn。 我的问题是，目前最可靠的打包方法是什么？我知道 PyInstaller 和 cx_freeze 都是我以前使用过的选项。我偏爱 PyInstaller，但以前我遇到了它（和 Pytorch）的问题。 最近有人完成过类似的项目吗？你有什么建议吗？ 注意。我查看了旧帖子，有一些关于这个主题的。然而，Pytorch 发生了许多变化，特别是一些运行时编译元素（在 Mac 上，它的公证过程可能会是一场噩梦）——我知道 Pyinstaller 拥有非常活跃的用户群。      提交人    /u/Solid_Company_8717   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6qtps/d_packaging_a_pytorch_model_to_an_exe_what_is_the/</guid>
      <pubDate>Wed, 04 Dec 2024 20:51:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每日论文讨论 - FlashAttention 3</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/</link>
      <description><![CDATA[      作为 Yannic Kilcher discord 服务器上每日论文讨论的一部分，我将自愿领导对 FlashAttention-3 🧮 🔍 的分析 📜 FlashAttention-3：具有异步和低精度的快速准确注意力 🌐 https://arxiv.org/abs/2407.08608 🕰 2024 年 12 月 5 日星期四 01:30 AM UTC // 2024 年 12 月 5 日星期四 7.00 AM IST // 2024 年 12 月 4 日星期三 5:30 PM PT FlashAttention-3 引入了三个巧妙的想法来提升 Hopper GPU 的性能 - 1️⃣ 生产者-消费者异步：这种技术将任务分为不同的部分。例如，如果我们有 2 个 warpgroup（标记为 1 和 2 - 每个 warpgroup 都是一组 4 个 warp），我们可以使用同步屏障 (bar.sync)，以便 warpgroup 1 首先执行其 GEMM（例如，一次迭代的 GEMM1 和下一次迭代的 GEMM0），然后 warpgroup 2 执行其 GEMM，而 warpgroup 1 执行其 softmax，依此类推。通过这样做，它可以更好地利用 GPU 资源并隐藏可能降低性能的延迟。 2️⃣ 隐藏 Softmax 操作：FlashAttention-3 通过将较慢的 softmax 计算与较快的矩阵乘法（GEMM）重叠来提高效率。它不是等待 Softmax 完成后再开始下一步计算，而是并行处理它们，从而加快整个过程。 3️⃣ 硬件加速低精度计算：这种方法使用高级 GPU 功能以较低的精度（FP8）执行计算，速度更快且占用更少的内存。 FlashAttention-3 对其算法进行了调整，以有效处理这些低精度计算，在保持准确性的同时将处理速度提高了近一倍。 https://preview.redd.it/impb6wfc1w4e1.png?width=1063&amp;format=png&amp;auto=webp&amp;s=82e24c828b373175ee119070027495a8a2a7bb6a    提交人    /u/CATALUNA84   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6pmvd/d_daily_paper_discussions_flashattention_3/</guid>
      <pubDate>Wed, 04 Dec 2024 20:03:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] ICLERB：一种评估嵌入和重排器以进行情境学习的更好方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/</link>
      <description><![CDATA[当前的嵌入基准（如 MTEB 和 BEIR）包括多个数据集和任务，但基本上基于相关性注释（如文本相似性）。这些对于为大多数搜索/检索用例选择最佳嵌入非常有用。如今，许多人使用这些嵌入来检索项目以进行上下文学习（例如文档 RAG 或小样本学习），以使 LLM 适应特定任务。然而，他们仍在使用 MTEB 来选择最佳嵌入，即使该基准上的性能并不一定意味着他们下游 LLM 任务的更好性能（毕竟 MTEB 是在 2021 年问世的）。 在我们的最新论文中，我们提出了一个新的评估框架和基准，称为 ICLERB。该基准测试通过使用直接偏好优化 (DPO) 作为相关性指标来挑战传统方法，以反映嵌入和重新排序器与 LLM 一起用于上下文学习时的实际效用。 https://arxiv.org/pdf/2411.18947 主要亮点： - 嵌入优于重新排序器：我们发现更简单的嵌入模型优于来自 Cohere、NVIDIA 和 VoyageAI 的高容量重新排序器。 - 大小不是一切：在三个 Snowflake 嵌入中，最小的模型（33M 个参数）优于较大的模型（109M 和 334M）。 - 重新思考训练和评估目标：这些发现表明，训练和评估更大的检索仅基于文本相似性的模型可能会适得其反。 有趣的是，某些模型（如 BGE）的性能对所使用的数据集或 LLM 非常敏感，而其他模型（如 NV）则更稳定。我们计划继续向基准添加更多数据集和 LLM，以扩大其范围。 在我们努力改进 ICLERB 的过程中，很想听听您的想法和反馈！您是否希望包含其他检索模型、LLM 或数据集？    提交人    /u/Crossing_Minds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6o70e/r_iclerb_a_better_way_to_evaluate_embeddings_and/</guid>
      <pubDate>Wed, 04 Dec 2024 19:05:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 二元适应度优化。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/</link>
      <description><![CDATA[您是否知道任何论文或哪个领域可以解决以下问题：您有一个需要优化的函数 f(x)，但您正在优化的成本/适应度是二进制的。我正在做一个关于这个的项目，我不确定这个领域是否有研究。 非常感谢 &lt;3    提交人    /u/pamintandrei   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6nayz/d_binary_fitness_optimization/</guid>
      <pubDate>Wed, 04 Dec 2024 18:30:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在GNN中定制注意力机制？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/</link>
      <description><![CDATA[我正在寻找一些基础代码或算法，以便在处理节点预测任务的图表时创建新的注意机制。我看到恒星图中有一些文档，但我想知道是否还有其他有用的材料。谢谢！！！    提交人    /u/Whole_Hat_4852   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6hxu8/d_how_to_customize_an_attention_mechanism_in_gnn/</guid>
      <pubDate>Wed, 04 Dec 2024 14:56:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] BERT 的最佳替代品 - NLU 编码器模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/</link>
      <description><![CDATA[我正在寻找 BERT 或 distilBERT 的替代方案以实现多语言建议。 我想要一个类似于 BERT 的双向掩码编码器架构，但功能更强大，并且具有更多用于自然语言理解任务的上下文。 任何建议都将不胜感激。    提交人    /u/mr_house7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6gtxh/d_best_alternatives_to_bert_nlu_encoder_models/</guid>
      <pubDate>Wed, 04 Dec 2024 14:07:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] 预测并缓解恶意人工智能应用程序的安全威胁</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/</link>
      <description><![CDATA[本文对人工智能系统在数字、物理和政治安全领域的潜在恶意应用进行了系统分析。该方法包括：  调查可能引发攻击的双重用途 AI 能力 绘制特定的攻击载体和所需的技术能力 分析攻击者/防御者动态的演变 开发威胁评估和缓解框架  关键技术发现：  NLP 和计算机视觉等领域的 ML 进步降低了复杂攻击的门槛 自动化系统可以显著扩大传统攻击载体的规模 迁移学习和 GAN 能够快速适应攻击技术 单靠技术对策是不够的 - 需要政策/治理框架  研究人员提供了一个详细的评估框架，检查：  不同攻击类型的技术要求 能力开发的预计时间表 执行难度和潜在影响 提出的防御措施及其局限性  我认为这项工作对于帮助 ML 社区在安全风险成为现实之前预防它们非常重要。该框架提供了一种结构化的方式来评估新出现的威胁，尽管我预计随着能力的提高，具体的攻击媒介将发生重大变化。 我认为我们需要更多的研究来衡量拟议对策的有效性并了解进攻/防御能力的共同演变。政策建议是一个良好的开端，但需要不断完善。 TLDR：系统分析 ML 的进步如何在安全领域启用新的攻击媒介。提供通过技术和政策措施评估和减轻威胁的框架。 完整摘要在这里。论文此处。    由    /u/Successful-Western27 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6fbgg/r_forecasting_and_mitigating_security_threats/</guid>
      <pubDate>Wed, 04 Dec 2024 12:55:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 大量元数据真的有助于语义搜索吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/</link>
      <description><![CDATA[这是我学习 AI 的第二周，我正在考虑预处理传记数据，包括大量元数据，如城市、出生日期、关键事件、教育、爱好等，然后生成嵌入并将它们一起添加到矢量数据库中。也许可以使用 NLP API 或 LLM。但有必要吗？或者我应该只使用 OpenAI 模型在存储它们之前从 bios 中动态提取这些元数据？拥有大量元数据是否会极大地帮助提高搜索结果的质量？ 我想也许半自动预处理步骤可以让我检查和清理元数据。 P/S：我将此发布在 https://www.reddit.com/r/learnmachinelearning 但没有得到太多回应。想在这里尝试一下。    提交人    /u/tjthomas101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6f39a/d_do_lots_of_metadata_really_help_in_semantic/</guid>
      <pubDate>Wed, 04 Dec 2024 12:43:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一次性比较多个大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/</link>
      <description><![CDATA[我写了一篇关于简化比较和选择大型语言模型 (LLM) 以完成各种任务的过程的文章： 一次性比较多个大型语言模型 希望这篇文章能帮助人们根据自己的用例选择最佳模型（这可能需要花费大量时间）。 我也期待讨论不同的技术和工具来自动化这个过程。 谢谢！    提交人    /u/grudev   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h6evdt/d_comparing_multiple_large_language_models_in_one/</guid>
      <pubDate>Wed, 04 Dec 2024 12:30:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多模态宇宙：利用 100TB 天文科学数据实现大规模机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/</link>
      <description><![CDATA[https://openreview.net/forum?id=EWm9zR5Qy1#discussion 摘要：我们提出了多模态宇宙，这是一个大规模多模态科学天文数据集，专门为促进机器学习研究而编制。总体而言，我们的数据集包含数亿个天文观测数据，构成 100TB 的多通道和高光谱图像、光谱、多元时间序列，以及各种相关的科学测量和元数据。此外，我们还包括一系列代表天体物理学机器学习方法标准实践的基准任务。这个庞大的数据集将使开发专门针对科学应用的大型多模态模型成为可能。用于编译数据集的所有代码以及如何访问数据的说明均可在 https://github.com/MultimodalUniverse/MultimodalUniverse 中找到 你们认为该数据集有什么用途？    提交人    /u/blabboy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5x146/r_the_multimodal_universe_enabling_largescale/</guid>
      <pubDate>Tue, 03 Dec 2024 20:19:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 云端 GPU 价格分析 - 2024 年 12 月：全面的市场回顾</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/</link>
      <description><![CDATA[在分析了各大供应商当前的云 GPU 定价之后，我整理了一些可能有助于基础设施决策的见解。一些发现让我感到惊讶——尤其是关于隐藏成本和现货定价变化。 当前市场价格（2024 年 12 月） 按需定价： - RunPod H100 (80GB)：2.49 美元/小时 - RunPod A100 (80GB)：1.69-1.99 美元/小时 - Vast.ai A100：0.73-1.61 美元/小时（市场模式） - Lambda A100：1.29 美元/小时 关键市场洞察  现货实例定价  - 可将成本降低 30-70% - 可用性因地区而异 - 一些提供商提供现货实例保证 - 价格稳定性因提供商而异  隐藏的成本因素  - 数据传输费用差异巨大 - 大型数据集的存储成本 - 网络带宽层 - 实例启动/关闭最低限度  提供商差异化因素  - UI/UX 和易用性 - 可用区域/地区 - 支持质量 - API 功能 成本优化策略  工作负载规划  - 将 GPU 与实际要求相匹配 - 考虑将工作负载拆分到较小的实例中 - 使用 Spot 实例执行可中断任务 - 监控利用率模式  数据管理  - 优化数据集存储 - 规划数据传输模式 - 有效使用缓存 - 考虑压缩策略 我将每月跟踪这些价格和模式。感兴趣：  您使用哪些提供商？ 您如何优化成本？ 在您的 GPU 决策中，哪些指标最重要？     提交人    /u/Botinfoai   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h5p7fr/d_cloud_gpu_price_analysis_december_2024_a/</guid>
      <pubDate>Tue, 03 Dec 2024 14:54:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h46e6j/d_simple_questions_thread/</guid>
      <pubDate>Sun, 01 Dec 2024 16:00:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h3u444/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sun, 01 Dec 2024 03:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>