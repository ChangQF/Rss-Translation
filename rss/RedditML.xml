<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Wed, 15 May 2024 12:27:41 GMT</lastBuildDate>
    <item>
      <title>[D] 注重效率和降低计算成本的扩散模型研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csh8h2/d_research_on_diffusion_models_with_focus_on/</link>
      <description><![CDATA[我需要有关特定关键字、有关上述主题的著名研究或调查的帮助。我只在谷歌学术上尝试过使用诸如“边缘设备”、“边缘计算”、“扩散模型”等关键词，结果返回不太令人满意。有人可以给我一些关于在哪里查看的建议吗？   由   提交 /u/MoreThanJustAMonkey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csh8h2/d_research_on_diffusion_models_with_focus_on/</guid>
      <pubDate>Wed, 15 May 2024 10:10:39 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]神经网络的SOTA不确定性量化方法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csh3tv/discussion_what_are_sota_uncertainty/</link>
      <description><![CDATA[我最近开始研究这个主题，我很好奇哪些方法是 SOTA 并在生产中使用？更具体地说，我对神经网络的任意和认知不确定性建模感兴趣。在理想的设置中，我的模型会告诉我何时遇到分布外的输入，并表示给定输入相对于系统噪声的不确定性。 提前致谢！ :)   由   提交/u/jens_97  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csh3tv/discussion_what_are_sota_uncertainty/</guid>
      <pubDate>Wed, 15 May 2024 10:02:05 GMT</pubDate>
    </item>
    <item>
      <title>[R] LLM4ED：用于自动方程发现的大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csgx30/r_llm4ed_large_language_models_for_automatic/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.07761 摘要：  方程发现旨在直接从数据中提取物理定律，具有成为一个关键的研究领域。先前基于符号数学的方法已经取得了实质性进展，但通常需要设计复杂算法的实现。在本文中，我们介绍了一种新框架，该框架利用基于自然语言的提示来指导大型语言模型（LLM）自动从数据中挖掘控制方程。具体来说，我们首先利用LLM的生成能力来生成字符串形式的各种方程，然后根据观察来评估生成的方程。在优化阶段，我们提出了两种交替迭代策略来协同优化生成的方程。第一个策略是将LLM作为黑盒优化器，根据历史样本及其表现实现方程自我改进。第二个策略是指导法学硕士执行全局搜索的进化算子。对偏微分方程和常微分方程进行了广泛的实验。结果表明，我们的框架可以发现有效的方程来揭示各种非线性动态系统下的基本物理定律。与最先进的模型进行了进一步比较，证明了良好的稳定性和可用性。我们的框架大大降低了学习和应用方程发现技术的障碍，展示了法学硕士在知识发现领域的应用潜力。     ;由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csgx30/r_llm4ed_large_language_models_for_automatic/</guid>
      <pubDate>Wed, 15 May 2024 09:49:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 柏拉图表示假说</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csgr7r/r_the_platonic_representation_hypothesis/</link>
      <description><![CDATA[arxiv: https://arxiv.org/pdf/2405.07987 项目页面：https://phillipi.github.io/prh/ github：https://github.com/minyoungg/platonic-rep/ 有趣的位置关于自监督、多模式表示学习的收敛的论文。    由   提交/u/dan994  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csgr7r/r_the_platonic_representation_hypothesis/</guid>
      <pubDate>Wed, 15 May 2024 09:37:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 训练 Transformer 模型的技巧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csf3nf/p_tips_on_training_a_transformer_model/</link>
      <description><![CDATA[大家好， 我正在尝试训练混合 Resnet18 编码器-6 层 520 dmodel Transformer 解码器来进行全页手写识别，我我正在努力正确训练它。我遇到的主要问题是：  我有一个数据集，其中包含大约 6000 个手写页面样本，分布 80% 训练/20% 验证，我怎么知道这是否是足够使用吗？ 我目前正在使用梯度累积，因为数据集中的每个样本都非常大，正如您想象的那样。事实证明这是有益的，因为与过去使用固定批量大小 2 的训练尝试相比，模型在训练期间的行为不太模糊 我目前正在探索学习率调度程序，并且目前已经使用，验证损失的稳定期减少，学习率从 1e-4 开始，因子为 0.75。接近和选择 lr 调度程序的最佳方法是什么  尽管我现在所处的位置似乎表明我正在朝着正确的方向前进（WER 在各个时期都在缓慢下降），在可用的 Kaggle GPU (P100) 上，训练时间非常慢（大约 3-5 个 epoch 需要 12 小时）。有什么前进的建议吗？   由   提交 /u/RedPipper   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csf3nf/p_tips_on_training_a_transformer_model/</guid>
      <pubDate>Wed, 15 May 2024 07:32:19 GMT</pubDate>
    </item>
    <item>
      <title>来自隐藏状态的过去的键值 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csf0q3/past_key_values_from_hidden_states_d/</link>
      <description><![CDATA[我正在尝试使用attention_layers和hidden_​​state为特定层提取过去的键值对 导入火炬import torch.nn.function as F from Transformers import LlamaConfig from Transformers import LlamaModel, LlamaTokenizer, LlamaForCausalLM tokenizer = LlamaTokenizer.from_pretrained(path_to_llama2) # 加载配置并启用所需的输出 config = LlamaConfig.from_pretrained(path_to_llama2) config.output_hidden_​​states = True config .output_attentions = True # 获取 self_attn_weights 和偏差（如果需要） config.use_cache = True # 获取过去的键值 model = LlamaForCausalLM.from_pretrained(path_to_llama2, config=config) model.eval() input_text = &quot;Once Upon a time&quot;;输入 = tokenizer(input_text, return_tensors=&#39;pt&#39;) 输出 = model(**inputs) hide_states =outputs.hidden_​​states # 每层的隐藏状态列表 state_dict = model.state_dict() # 计算单层 def 的 Past_key_values 的函数compute_past_key_values_for_layer（layer_idx，hidden_​​state）：attention_layers = [model.model.layers中的层的layer.self_attn] W_q = state_dict[f&#39;model.layers.{layer_idx}.self_attn.q_proj.weight&#39;] W_k = state_dict[f&#39;model .layers.{layer_idx}.self_attn.k_proj.weight&#39;] W_v = state_dict[f&#39;model.layers.{layer_idx}.self_attn.v_proj.weight&#39;] 查询 = torch.matmul(hidden_​​state, W_q.T) 键 = torch .matmul(hidden_​​state, W_k.T)values = torch.matmul(hidden_​​state,W_v.T)batch_size,seq_length,hidden_​​dim =hidden_​​state.size()num_attention_heads=attention_layers[layer_idx].num_heads head_dim =hidden_​​dim // num_attention_headskeys=keys。视图（batch_size，seq_length，num_attention_heads，head_dim）键=keys.permute（0,2,1,3）值=values.view（batch_size，seq_length，num_attention_heads，head_dim）值=values.permute（0,2,1, 3) returnkeys,values past_key_values = [] for i,hidden_​​state in enumerate(hidden_​​states[1:]): # 跳过嵌入层keys,values =compute_past_key_values_for_layer(i,hidden_​​state)past_key_values.append((keys,values))past_key_values = tuple(past_key_values)  但是这些past_key_values与我从特定层的outputs.past_key_values获得的值不匹配。 为什么会发生这种情况？有什么建议吗？   由   提交/u/1azytux  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csf0q3/past_key_values_from_hidden_states_d/</guid>
      <pubDate>Wed, 15 May 2024 07:26:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 业界人士，你们如何使用开源法学硕士？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1csdsje/d_those_in_the_industry_how_are_you_using_open/</link>
      <description><![CDATA[想要了解人们如何在生产中使用开源 LLM。我希望看到答案的一些问题是：   很想了解您如何使用较小的模型，如何部署它们，以及这里的其他人可能没有想到 您是否看到使用微调开源带来的性能/成本提升？ 训练和建立自己的微调管道有多困难？您是否使用完整的培训或像 LoRA 这样的 PEFT 方法？ 任何其他可能与使用您自己的 LLM 相关的内容  我是一名工程师，试图弄清楚提高技能的最佳方式，看看我的公司是否可以利用开源模型做更多事情，这对此非常有帮助。谢谢！   由   提交 /u/C0hentheBarbarian   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1csdsje/d_those_in_the_industry_how_are_you_using_open/</guid>
      <pubDate>Wed, 15 May 2024 05:59:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] CLIP（和 SigLip）的俄罗斯套娃表示学习（MRL）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cs8fzb/r_matryoshka_representation_learning_mrl_for_clip/</link>
      <description><![CDATA[CLIP 的 MRL [1] 允许使用较小维度的嵌入，而不会损失保真度。训练经过修改，以优化视觉和文本编码器的截断嵌入（一次多个目标维度）。  主要发现：  将嵌入大小减少 4 倍可保持约 95 的性能 子嵌入的投影层对性能没有帮助 在多模态检索中在域内外进行工作（零样本） 使用过多的子嵌入会降低性能（即 {512, 256, 128} 与 {512, 256, 128 , 64, 32, 16, 8} 子嵌入的数量影响收敛（同上） 与 GCL 等排名调整方法一起使用 子维度的相对重要性（权重，wi）很重要（例如 w1*L_512 + w2*L_256 + w3*L_128） 如果使用原始大小的嵌入，即性能，则 MRL 训练的模型可以提高。即使不使用较小的嵌入，也会得到改进。  文章： https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking  [1] MRL https://arxiv.org/abs/2205.13147   由   提交/u/Jesse_marqo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cs8fzb/r_matryoshka_representation_learning_mrl_for_clip/</guid>
      <pubDate>Wed, 15 May 2024 01:02:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有什么理由不提交给 NeurIPS？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cs6p6j/d_any_reason_not_to_submit_to_neurips/</link>
      <description><![CDATA[众所周知，摘要明天截止。我对能否在一周内完成一份强有力的提交持观望态度。我知道，如果评论不好（或者如果我觉得自己在一周内提交的截止日期不强），我随时可以撤回，但我担心网上可能会留下提交的痕迹未来的审稿人将能够谷歌。谁能确认只有在您不撤回而是提交反驳导致拒绝的情况下才会出现这种情况？如果退出openreview，网上会留下痕迹吗？在撤回之前，您是否需要做一些编辑和清理提交内容的技巧？我知道提交结果是随机的，所以我想知道何时（如果有的话）提交是一个策略性错误。   由   提交 /u/IWantToMathematics   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cs6p6j/d_any_reason_not_to_submit_to_neurips/</guid>
      <pubDate>Tue, 14 May 2024 23:39:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] Kolmogorov Arnold Networks：视觉论文分解（视频）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crzj4o/d_kolmogorov_arnold_networks_a_visual_paper/</link>
      <description><![CDATA[分享我的 YT 频道中的视频，该视频详细介绍了新的 KAN 论文。它涵盖了理解本文所需的所有核心概念 - 柯尔莫哥洛夫阿诺德表示定理、样条曲线、MLP、MLP 和 KAN 之间的比较、未来的挑战，并强调了 KAN 的一些令人惊叹的属性/结果，如持续学习、稀疏化、符号化回归等。 链接：https://youtu.be/7zpz_AlFW2w    由   提交 /u/AvvYaa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crzj4o/d_kolmogorov_arnold_networks_a_visual_paper/</guid>
      <pubDate>Tue, 14 May 2024 18:36:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT-4o“原生”多模式，这实际上意味着什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crzdhd/d_gpt4o_natively_multimodal_what_does_this/</link>
      <description><![CDATA[与预训练视觉编码器 + 预训练 LLM 的典型 VL 公式相比，您对其工作方式（训练和架构）的最佳猜测是什么 -&gt;与多模式任务进行微调？ 例如是否对整个系统进行完全混合模态预训练？模型是否将所有模态嵌入到共享空间中进行预测？系统是否“自行选择”？输出token的形式（可以根据输入token灵活选择输出音频还是文本）还是这个用户指定的？   由   提交/u/Flowwwww  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crzdhd/d_gpt4o_natively_multimodal_what_does_this/</guid>
      <pubDate>Tue, 14 May 2024 18:29:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] BERT 在 2024 年对于 EMNLP 提交仍然有意义吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crxhfp/d_is_bert_still_relevant_in_2024_for_an_emnlp/</link>
      <description><![CDATA[使用 BERT 进行主动学习（对于某些应用）仍然是提交论文的相关范式吗？或者这样的工作可能会因为“过时”而被拒绝？ 我的想法与使用 BERT 进行医学分类有关，我确信法学硕士可能会表现得更好。想知道是否值得投入时间大力推动以获得结果。   由   提交 /u/PK_thundr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crxhfp/d_is_bert_still_relevant_in_2024_for_an_emnlp/</guid>
      <pubDate>Tue, 14 May 2024 17:13:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 仅具有 CS 背景的您如何更好地阅读 ML 论文中的证明？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crr0fa/d_how_do_you_get_better_at_reading_proof_in_the/</link>
      <description><![CDATA[大家好，如标题所示，如何更好地阅读 ML 论文中的证明？我提到的 ML 论文是对抗性 ML 领域的论文，例如通过随机平滑认证的对抗鲁棒性。就上下文而言，我有微积分、线性代数的基本知识，但大多数时候在阅读证明时，有时我觉得一行字凭空出现，我无法推理他们为什么或如何做到这一点。也许因为我的背景是计算机科学，专注于软件，所以我缺乏严格的基于证明的数学知识。请帮忙！！ 编辑：是的，我在本科期间确实学习了 CS 理论（凸优化、离散数学……），但没有达到那些论文中严格的数学水平。我认为数学证明是否必要取决于主题，就我而言，是的，它非常重要，我需要更好的建议。   由   提交/u/little_vsgiant   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crr0fa/d_how_do_you_get_better_at_reading_proof_in_the/</guid>
      <pubDate>Tue, 14 May 2024 12:33:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每个transformer层最后一个线性层的用处</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1crojwd/d_the_usefulness_of_the_last_linear_layer_of_each/</link>
      <description><![CDATA[这是一个非常明显的问题。 我最近发现变压器的最后一个线性层有点浪费参数。  变压器模型是许多变压器层的堆栈。 这些层以 3 个 QKV 线性变换开始，以 FFN 网络结束，FFN 网络由两个线性层组成。最后一个需要(d_model * d_dim_feedforward)参数和乘法，其输出在下一层再次进行线性变换。 我们都知道，两个连续的线性变换可以用一个线性变换来表示，这就是原因为什么我们使用激活函数。 那么为什么我们没有使用超稀疏线性变换，也许可以通过将嵌入维度视为特定线性变换维度上的序列维度来进行卷积。   由   提交 /u/WetAndSnowy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1crojwd/d_the_usefulness_of_the_last_linear_layer_of_each/</guid>
      <pubDate>Tue, 14 May 2024 10:09:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>