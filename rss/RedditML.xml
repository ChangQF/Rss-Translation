<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sun, 17 Mar 2024 12:22:25 GMT</lastBuildDate>
    <item>
      <title>[D] 与你的表聊天 - 使用什么来使用开源 LM 进行数据库问答？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgw40p/d_chat_with_your_tables_what_to_use_for_database/</link>
      <description><![CDATA[我正在开发一个项目，我们想要回答数据库中表的简单/中度硬查询。我目前没有资源来微调模型，想尝试开源大型语言模型。 过滤掉之后的所有表格大约有 120 GB，这告诉我需要使用开始时有一个 Text 2 SQL 模型，并可能使用 Spark 执行。我用采样数据尝试了 pandasAI 和 LC 代理，但输出并不是很好（有些失败 如果有人对此进行过研究，我将不胜感激任何线索或评论，或任何研究论文。我需要有关典型管道的外观以及如何提取和传递数据中的多行的指导。    提交者   /u/Parking_Nectarine_19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgw40p/d_chat_with_your_tables_what_to_use_for_database/</guid>
      <pubDate>Sun, 17 Mar 2024 12:14:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 专用于AI学习环境的游戏引擎</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgvgrr/d_game_engine_dedicated_to_ai_learning/</link>
      <description><![CDATA[我最近一直在寻找一种为 RL 训练创建自定义环境的方法，该环境可以轻松地与外部编程语言和框架集成（因此基本上将 NPC 外包）游戏的逻辑与游戏本身以外的其他东西（如 pytorch 或 tensorflow，或您选择的任何其他库）。  长话短说，我遇到过unity ml代理，但它的灵活性非常有限（你只能微调一些算法的超参数），而且自上次unity以来，设置真的很草率+我对使用它有点怀疑。 Nvidiaomniverse + Isaac Gym 仅适用于高度精确的物理环境，你需要大量的 GPU 和精确的模型来运行它，但这不是我想要的东西。我可以扭转它足以使其实现简单的游戏，但这将是一个strech 除此之外，我只找到了几个库，但没有一个真正匹配接近我想要的 ​ 我设想的是一个改进的 godot 引擎，上面有一个框架，集成了 godot 和 python 数据传输和同步。您应该能够更改引擎中的对象参数，并且应该将其映射到 python 代码，如果 python 没有为游戏内代理返回特定的匹配模式，则会出现错误 &lt; p&gt;你知道有什么与我所描述的类似的事情吗？    由   提交/u/DeadProgrammer8785   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgvgrr/d_game_engine_dedicated_to_ai_learning/</guid>
      <pubDate>Sun, 17 Mar 2024 11:36:51 GMT</pubDate>
    </item>
    <item>
      <title>[D]分布式训练策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgulqe/d_distributed_training_strategy/</link>
      <description><![CDATA[嗨，我想知道如何使用不同的“分布式训练策略”微调 Mixtral。 我可以使用 4* A100 (40Gb)，并且想要尝试不同的策略，例如对模型进行分片并在每个 GPU 上放置 2 个专家层、使用 QLoRA 量化模型，以及在 4 个 GPU 上使用数据并行性。 我可以使用 4* A100 (40Gb)，并且想要尝试不同的策略，例如对模型进行分片并在每个 GPU 上放置 2 个专家层，或者使用以下方法量化模型QLoRA，并在 4 个 GPU 上使用数据并行性。   由   提交 /u/Thick-brain-dude   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgulqe/d_distributed_training_strategy/</guid>
      <pubDate>Sun, 17 Mar 2024 10:40:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] Mixture-of-LoRA：大型语言模型的高效多任务调优</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgub6e/r_mixtureofloras_an_efficient_multitask_tuning/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgub6e/r_mixtureofloras_an_efficient_multitask_tuning/</guid>
      <pubDate>Sun, 17 Mar 2024 10:20:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我不明白反向传播如何在稀疏门控 MoE 上工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</link>
      <description><![CDATA[我不明白反向传播如何在稀疏门控 MoE 上工作 在 LLM 的背景下，假设你有 n 个专家，并且您为每个令牌选择了前 k 个。 在训练期间，门网络可能完全错误，并且将正确的专家排除在所选的 k 之外。然而，由于没有使用正确的专家，因此门没有机会增加正确专家的权重。 换句话说，在背景期间，仅更新门网络的部分参数，影响前 k 内权重的那些。 我错过了什么吗？   由   提交/u/Primary-Try8050   reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgmpmf/d_i_dont_understand_how_backprop_works_on/</guid>
      <pubDate>Sun, 17 Mar 2024 02:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从机器学习中的归纳偏差的角度看注意力和变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/</link>
      <description><![CDATA[      Hello人们在我的 YT 频道上发布了一段有关 Transformer 的视频，以及他们为深度学习研究带来的有趣的范式转变。在受到关注之前，趋势过去是在模型架构中添加更多归纳偏差（具有位置偏差的 CNN、具有时间偏差的 RNN）…… Transformer 的成功有点表明，当你获得足够的数据和信息时，通用架构可以胜过归纳偏差。一个要训练的大屁股模型。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgghee/d_a_look_at_attention_and_transformers_from_the/</guid>
      <pubDate>Sat, 16 Mar 2024 21:22:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] Apple - MM1：多模式 LLM 预培训的方法、分析和见解</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/</link>
      <description><![CDATA[Apple 的新论文介绍了 MM1 ，一系列结合了视觉和语言理解的多模式人工智能模型。研究人员进行了广泛的实验，以确定驱动这些模型性能的关键因素，测试不同的架构选择和预训练数据混合。 以下是我在论文中的要点： Big当然之一：最大的 MM1 模型（30B 密集）在多模态基准上实现了最先进的少样本学习 要点：  MM1 包括两者高达 30B 参数的密集模型和专家混合 (MoE) 变体 图像分辨率对性能的影响最大，超过模型大小 特定的视觉语言连接器设计具有效果不大 在预训练中混合交错图像+文本、标题和纯文本数据至关重要 标题、交错和文本数据的比例为 5:5:1 有效最佳 合成字幕数据有助于少样本学习 30B 密集模型在 VQA 和字幕任务上击败了先前的 SOTA  核心见解深思熟虑的数据和架构选择，而不仅仅是规模，是构建高性能多模式模型的关键。 MM1 模型还表现出令人印象深刻的新兴能力，例如多图像推理和上下文中的小样本学习。 完整摘要。   由   提交 /u/Successful-Western27    reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgbc5u/r_apple_mm1_methods_analysis_insights_from/</guid>
      <pubDate>Sat, 16 Mar 2024 17:29:24 GMT</pubDate>
    </item>
    <item>
      <title>[P] 用计算机视觉对我的黑胶唱片收藏进行编目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bgaeqi/p_cataloguing_my_vinyl_collection_with_computer/</link>
      <description><![CDATA[   /u/zerojames_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bgaeqi/p_cataloguing_my_vinyl_collection_with_computer/</guid>
      <pubDate>Sat, 16 Mar 2024 16:48:37 GMT</pubDate>
    </item>
    <item>
      <title>[R] 动态内存压缩：改造 LLM 以加速推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bga7xf/r_dynamic_memory_compression_retrofitting_llms/</link>
      <description><![CDATA[     &lt; td&gt; 动态内存压缩：改进 LLM 以加速推理 论文：https://arxiv.org/abs/2403.09636 X：https://x.com/p_nawrot/status/1768645461689168365 摘要：  Transformers 已成为大型语言模型 (LLM) 的支柱。然而，由于需要在内存中存储过去标记的键值表示的缓存，生成仍然效率低下，其大小与输入序列长度和批量大小线性缩放。作为解决方案，我们提出了动态内存压缩（DMC），这是一种在推理时进行在线键值缓存压缩的方法。最重要的是，该模型学习在不同的头和层中应用不同的压缩率。我们将 Llama 2（7B、13B 和 70B）等预训练的 LLM 改造为 DMC Transformer，在 NVIDIA H100 GPU 上实现自回归推理吞吐量高达约 3.7 倍的提升。 DMC 通过对原始数据的可忽略百分比进行持续预训练来应用，无需添加任何额外参数。我们发现 DMC 通过高达 4 倍的缓存压缩保留了原始的下游性能，优于经过训练的分组查询注意力 (GQA)。 GQA 和 DMC 甚至可以结合起来以获得复合收益。因此，DMC 在任何给定的内存预算内都适合更长的上下文和更大的批次。  https:/ /i.redd.it/ouuf7t4d5qoc1.gif   由   提交 /u/alancucki   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bga7xf/r_dynamic_memory_compression_retrofitting_llms/</guid>
      <pubDate>Sat, 16 Mar 2024 16:40:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] Kaggle TPU v3-8 的 Llama2 7B 和 13B 聊天完成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg9wmj/p_llama2_7b_and_13b_chat_completion_for_kaggle/</link>
      <description><![CDATA[大家好，我对 Llama2 存储库进行了一些修改以利用 TPU v3-8 硬件，因此它可以执行 Llama2 7B（甚至 13B） ）聊天完成推理，无需图形重新编译。当生成批量大小为 1 的文本时，它仍然比 Nvidia P100 慢，不适合实时推理，但（TPU 是 TPU）在批量文本生成方面表现良好。我用它生成大量文本用于研究目的。希望它对社区有益。 这是存储库。 修改利用 PyTorch/XLA SPMD 系统（在 TPU v3-8 上）以及新的网格和分布配置来进行分片整个 TPU 网格的权重和缓存。具体来说，k、v 缓存具有预定义的静态大小，以避免每次令牌生成后 TPU 图形重新编译。这一新配置使 Llama2 7B 能够装入一台 TPU v3-8 设备中，并具有大量剩余内存来运行推理，批量大小最多为 64。相同的配置也可用于运行 Llama2 13B 的推理。 现有的Llama2 Google Next Inference分支仅支持TPU v4和 v5e。喜欢使用 Kaggle TPU 的人可以利用它来运行推理。   由   提交/u/-x-Knight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg9wmj/p_llama2_7b_and_13b_chat_completion_for_kaggle/</guid>
      <pubDate>Sat, 16 Mar 2024 16:25:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] LLaMA 的具体细节：了解 LLaMA 和大型语言模型如何运行的整体方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/</link>
      <description><![CDATA[我很高兴地宣布，我使用 Go 开发的 LLaMA Nuts and Bolts 开源项目现已公开发布！ 您可以在我的 Github 存储库上找到它：https://github.com/adalkiran/llama-nuts-and- Bolts 通过代码和详细文档了解 LLaMA 及其组件在实践中如何运行的整体方法。 “螺母和螺栓” （实践方面而不是理论事实，纯粹的实现细节）所需的组件、基础设施和数学运算，而不使用外部依赖项或库。 目标是制作一个可以对 LLaMa 进行推理的实验项目2 7B-聊天模型完全脱离Python生态系统（使用Go语言）。在整个旅程中，我们的目标是获取知识并阐明该技术的抽象内部层。 这段旅程是一次有意重新发明轮子的旅程。在阅读文档中的旅程时，您将通过 LLaMa 模型的示例了解大型语言模型如何工作的详细信息。 如果您像我一样对 LLM（大型语言模型）如何工作感到好奇和变形金刚工作并深入研究了来源中的概念解释和示意图，但渴望更深入的理解，那么这个项目也非常适合您！ 您不仅会找到 LLaMa 架构的细节，而且还会发现在文档目录中查找各种相关概念的解释。从逐字节读取 Pickle、PyTorch 模型、Protobuf 和 SentencePiece 分词器模型文件，到 BFloat16 数据类型的内部结构、从头开始实现 Tensor 结构和包括线性代数计算在内的数学运算。 这个项目最初是为了通过运行和调试来了解 LLM 背后的作用，并且仅用于实验和教育目的，而不是用于生产用途。 如果您查看它，我会很高兴欢迎评论！   由   提交 /u/adalkiran   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg61qi/p_llama_nuts_and_bolts_a_holistic_way_of/</guid>
      <pubDate>Sat, 16 Mar 2024 13:25:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] RepoHyper：存储库级代码完成所需的只是更好的上下文检索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg396m/r_repohyper_better_context_retrieval_is_all_you/</link>
      <description><![CDATA[我们引入了 RepoHyper，这是一个新颖的框架，可将代码完成转换为现实世界存储库用例的无缝端到端流程。传统方法依赖于将上下文集成到代码语言模型 (CodeLLM) 中，通常假设这些上下文本质上是准确的。然而，我们发现了一个差距：标准基准测试并不总是提供相关的上下文。 为了解决这个问题，RepoHyper 提出了三个新颖的步骤：  构建代码属性图，建立丰富的上下文源。 一种新颖的搜索算法，用于查明所需的确切上下文。 扩展算法，旨在揭示代码元素之间的微妙联系（类似于社交网络挖掘中的链接预测问题）。  我们的综合评估表明，RepoHyper 树立了新标准，在 RepoBench 基准测试中优于其他强大的基准。 代码：https://github.com/FSoft-AI4Code/RepoHyper   由   提交 /u/FSoft_AIC   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg396m/r_repohyper_better_context_retrieval_is_all_you/</guid>
      <pubDate>Sat, 16 Mar 2024 10:41:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] AnyGPT：具有离散序列建模的统一多模态法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bg2x83/r_anygpt_unified_multimodal_llm_with_discrete/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.12226 代码：https://github .com/OpenMOSS/AnyGPT 数据集：https:// Huggingface.co/datasets/fnlp/AnyInstruct 项目页面：https://junzhan2000.github.io/AnyGPT.github.io/ 视频：https://www.youtube.com/watch?v=oW3E3pIsaRg 摘要：  我们介绍 AnyGPT，这是一种任意对任意的多模态语言模型，它利用离散表示来统一处理各种模态，包括语音、文本、图像和音乐。 AnyGPT 可以稳定地训练，无需对当前的大语言模型（LLM）架构或训练范式进行任何改变。相反，它完全依赖于数据级预处理，促进新模式无缝集成到法学硕士中，类似于新语言的合并。我们构建了一个以文本为中心的多模态数据集，用于多模态对齐预训练。利用生成模型，我们合成了第一个大规模任意对任意多模式指令数据集。它由 108k 个多轮对话样本组成，这些对话错综复杂地交织着各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT 能够促进任意对任意的多模态对话，同时在所有模态中实现与专用模型相当的性能，证明离散表示可以有效且方便地统一语言模型中的多种模态。演示显示在 此 https URL    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bg2x83/r_anygpt_unified_multimodal_llm_with_discrete/</guid>
      <pubDate>Sat, 16 Mar 2024 10:18:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习中的函数式编程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bfu3oy/d_functional_programming_in_ml/</link>
      <description><![CDATA[我觉得 Python 中的大多数 ML 库都是用 OOP 风格编写的。这是有道理的，因为 Python 没有像 Haskell 这样的“好”类型系统，因此类是用可读名称定义接口的好方法。其他功能更强大的语言是否有流行的 ML 库？我特别想到 Haskell、Rust 或 Scala。   由   提交 /u/LengthinessMelodic67   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bfu3oy/d_functional_programming_in_ml/</guid>
      <pubDate>Sat, 16 Mar 2024 01:15:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>