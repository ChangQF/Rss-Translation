<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Sat, 01 Jun 2024 01:06:16 GMT</lastBuildDate>
    <item>
      <title>[P] 如果我的项目没有最好的成功率，那么它的可靠性如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d5b4sp/p_how_solid_will_my_project_be_if_it_doesnt_have/</link>
      <description><![CDATA[我是一名大学生，我正在考虑构建一个拳击比赛预测器作为一个项目放在我的简历上。 但由于以下事实：1）从互联网上抓取拳击数据很困难，因为主网站 boxrec 对其数据政策非常严格，并且 2）真正详细的拳击数据（每回合击中的拳数、击倒次数等）似乎很难获得，恐怕我的预测器不会是最准确的。 如果雇主看到我花了很多心思来收集和查找数据，并且我使用了许多先进的数据技术来构建我的模型，我的预测器是否仍会被雇主视为一个可靠的项目？    提交人    /u/Willing-Insurance654   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d5b4sp/p_how_solid_will_my_project_be_if_it_doesnt_have/</guid>
      <pubDate>Sat, 01 Jun 2024 01:00:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 其他领域的研究，例如最近对一立方毫米人类脑组织的映射，能否帮助机器学习领域？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d56zg0/d_can_other_areas_researches_such_as_the_recent/</link>
      <description><![CDATA[https://www.scientificamerican.com/article/a-cubic-millimeter-of-a-human-brain-has-been-mapped-in-spectacular-detail/ 围绕人类大脑的研究，例如这张最新的人类大脑图谱，能否为机器学习领域提供一些见解，以构建更有效的人工智能/算法模型？ 外行人在这里。    提交人    /u/Enzo-chan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d56zg0/d_can_other_areas_researches_such_as_the_recent/</guid>
      <pubDate>Fri, 31 May 2024 21:40:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 进行模型推理的更便宜的方法？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d56h9c/d_cheaper_way_to_do_model_inference/</link>
      <description><![CDATA[有人知道在服务器停机期间节省 GPU 计算的解决方案吗？我目前正在进行模型推理，大多数时候我只是为计算付费，而不提供任何用户请求。    提交人    /u/Fun_Win_6054   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d56h9c/d_cheaper_way_to_do_model_inference/</guid>
      <pubDate>Fri, 31 May 2024 21:17:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要帮助在 vscode jupyter notebook 上使用专用 GPU。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d55cej/d_need_help_to_use_dedicated_gpu_on_vscode/</link>
      <description><![CDATA[嘿，我目前正在 colab 和 vscode jupyter 扩展中工作。由于我有一张 Nvidia 卡，我想用它来训练各种模型（深度、简单），使用 vscode 中的 jupyter 笔记本。如何设置此要求？为简单起见，我想为 vscode jupyter 笔记本使用专用 GPU。    提交人    /u/Saheenus   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d55cej/d_need_help_to_use_dedicated_gpu_on_vscode/</guid>
      <pubDate>Fri, 31 May 2024 20:28:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] Bigram 标记器比现状更好吗？尤其是对于多语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4z5dr/d_bigram_tokenizers_better_than_status_quo/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4z5dr/d_bigram_tokenizers_better_than_status_quo/</guid>
      <pubDate>Fri, 31 May 2024 16:03:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] 序列打包对于训练 Transformer 来说常见吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4tux3/d_is_sequence_packing_common_for_training/</link>
      <description><![CDATA[大家好， 我想从头开始训练一个小型 Transformer 语言模型，并试图尽可能提高训练效率。我一直在思考如何构建训练批次，这让我看到了这篇论文高效序列打包，避免交叉污染：加速大型语言模型，不影响性能，这似乎是一件非常合理的事情，一般情况下都应该这样做。 简而言之，这个想法是将多个序列打包在一个样本序列中，并调整注意力矩阵，使样本不会相互交叉污染，即只关注自身内的标记。 Huggingface 论坛中的这篇文章很好地说明了这一点。但我在 Huggingface Transformer 中找不到这样的东西。我是不是漏掉了什么？其他框架是否实现了这一点？我们是否知道大公司是否在进行序列打包？我是否错过了这方面的重大缺点？我认为它可能会对位置编码造成问题。    提交人    /u/CloudyCloud256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4tux3/d_is_sequence_packing_common_for_training/</guid>
      <pubDate>Fri, 31 May 2024 11:58:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用大型语言模型评估 RAG</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4tbzg/p_evaluate_rag_using_large_language_models/</link>
      <description><![CDATA[我一直在研究 RAG 和 LLM，我一直想评估 LLM。有一些库可以与基于 GPT 的模型一起使用，但对于 RAG，我主要想评估基于 Llama 或 Mistral 的模型。 所以我建立了 BeyondLLM。 BeyondLLM 可帮助您仅用 5-7 行代码构建高级检索增强生成 (RAG) 和大型语言模型 (LLM) 应用程序。BeyondLLM 是开源的，它还支持 Fine Tune Embeddings 和 Observaility。 GitHub：https://github.com/aiplanethub/beyondllm/    提交人    /u/trj_flash75   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4tbzg/p_evaluate_rag_using_large_language_models/</guid>
      <pubDate>Fri, 31 May 2024 11:27:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习会议和组织指标</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4shqn/d_ml_conferences_and_organization_metrics/</link>
      <description><![CDATA[我觉得很多人会认为 NeurIPS、ICLR、ICML 等是机器学习领域的重要会议。即使在机器学习之外，NeurIPS 和 ICLR 的 H 指数在所有会议中排名第 9 和第 10。但是，现在我正在查看全球的终身教职职位，情况似乎有所不同。这些出版物似乎对移民或学术终身教职毫无价值，因为它们不是传统期刊。您会注意到它们没有出现在 SCImago 中，SCImago 是许多组织用来衡量出版物质量的指标，因此会决定终身教职或移民。 我很好奇学术机器学习研究人员在这种情况下会做什么。您是否会停止向 NeurIPS 提交论文，而打算在 ACM “机器学习的基础和趋势”上发表论文，而该期刊 在 SCImago 上排名第二？还是在不断增长的机器学习 IEEE 期刊名单上？    提交人    /u/smorad   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4shqn/d_ml_conferences_and_organization_metrics/</guid>
      <pubDate>Fri, 31 May 2024 10:34:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] KAN ==多层GAM？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4pjxp/d_kan_multilayer_gam/</link>
      <description><![CDATA[我刚刚阅读了 KAN 论文， 我的理解是，它提供了如何堆叠多层 GAM（广义加性模型）的解决方案：Phi 函数只是 GAM 的形状函数，而样条函数是 GAMS 中经过充分研究的形状函数。 所以对我来说：  MLP 是一个多层线性回归 KAN 是一个多层 GAM  尽管如此，GAM 具有 KAN 论文中未表达的链接功能，但在我看来，这才是这篇论文的真正重点。如果我们向 KAN 层添加激活函数，那么我们就完全拥有了多层 GAM。 这也意味着我们可以将 MLP 视为 KAN 的特例，因为线性回归是 GAM 的特例。 这听起来正确吗？    提交人    /u/mainro12   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4pjxp/d_kan_multilayer_gam/</guid>
      <pubDate>Fri, 31 May 2024 07:02:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 LipNet 进行唇读：端到端的句子级唇读</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4mpkw/r_lipreading_with_lipnet_endtoend_sentencelevel/</link>
      <description><![CDATA[      大家好， 我最近根据论文《端到端句子级唇读》从头开始实现了 LipNet。它通过从输入帧中的唇部运动中提取特征来预测句子。它最初是一个 3DConv-GRU 模型，我已经用 3DConv-LSTM（双向）和一些其他具有不同复杂度的模型实现了它，并且利用了 He（Kaiming Normal）初始化权重。 我请求您查看存储库并提供任何反馈，如果您发现它有用，请考虑分叉。 GitHub/LipNet 图片来自官方论文   由    /u/Kian5658  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4mpkw/r_lipreading_with_lipnet_endtoend_sentencelevel/</guid>
      <pubDate>Fri, 31 May 2024 04:00:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机器学习自省</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4dx88/r_machine_learning_introspection/</link>
      <description><![CDATA[      虽然“自省”虽然在人工智能中没有明确的定义，但它确实有着悠久的历史——主要是为机器配备人类解决问题的直觉，Newell 和 Simon 的“通用问题解决器”（1958 年）就是一个早期的例子。根据 Roger Grosse 的说法，由于他认为人们不愿意从心理状态的角度思考算法，该领域已经远离了内省。 然而，本周我们发布了一个视频，其中介绍了这些基本思想，并探讨了过去几年发布的两篇关于内省概念的论文。第一个是“内省式 CNN，它通过从自己的分类器合成样本来改善分类结果（而不是使用单独的鉴别器网络来生成样本的 GAN）。这就是为什么这种方法被称为“内省式”的原因。因为它使用了自己的分类器。 第二篇论文%20graph) 使用概念归纳来创建一组概念，以描述系统为何为用户做出分类决策。这里的评估侧重于人类对解释的理解，而不是用它来使分类结果更好。这实际上与我们与 Joao Leite 教授一起发布的早期视频有关，他在视频中讨论了使用“映射神经网络”得出的结论进行概念确定（您也可以参阅他的论文）。 最新视频的网址如下： https://www.youtube.com/watch?v=drlqCc_e_o0 https://www.youtube.com/watch?v=26tTT8saaDs&amp;t=2517s    提交人    /u/Neurosymbolic   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4dx88/r_machine_learning_introspection/</guid>
      <pubDate>Thu, 30 May 2024 20:40:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] CV/结构光/3D 重建研究合作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d4dokf/r_research_collaboration_in_cv_structured_light/</link>
      <description><![CDATA[我正在寻找对结构光、使用投影技术的 3D 重建或与条纹投影或相位分析相关的任何内容的研究和出版感兴趣的合作者。我的重点包括：  用于 3D 扫描的结构光 创新投影方法 使用最新技术克服相位分析或相位展开中的挑战 在医学成像、工业检测等领域的应用。  如果您在这些领域工作或有见解可以分享，我很乐意讨论潜在的合作机会。如果我的帖子到处都是，我很抱歉。快速聊天以交换想法或提供建议将不胜感激。    提交人    /u/Falafel2307   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d4dokf/r_research_collaboration_in_cv_structured_light/</guid>
      <pubDate>Thu, 30 May 2024 20:30:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我进行了 580 次模型数据集实验，结果表明，即使你非常努力，仅通过查看数据漂移结果也几乎不可能知道模型是否正在退化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d47ca4/r_i_ran_580_modeldataset_experiments_to_show_that/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d47ca4/r_i_ran_580_modeldataset_experiments_to_show_that/</guid>
      <pubDate>Thu, 30 May 2024 15:54:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 了解机器学习的最新进展后，您会对 BERT 做哪些添加或更改？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d3zw0d/d_what_additions_or_changes_would_you_make_to/</link>
      <description><![CDATA[嗨！ 由于 BERT 仍然是一个广泛使用的模型。我很好奇你们会做些什么来使它保持最新状态。BERT 论文于 2018 年 10 月 11 日提交，最后修订于 2019 年 5 月 24 日在 arXiv 上。 这些想法不一定非要涉及架构，它可以是调度程序或训练集、MLM 损失、加快训练速度等。 就我个人而言，我会改变位置编码，也许我会使用 RoPE。使用 Flash Attention。 对于数据集，也许我会专注于混合，我对调度程序不太了解，但我会尝试所有 LLM 论文中的一些东西，一些能够进行持续预训练的东西。    提交人    /u/Mean-Night6324   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d3zw0d/d_what_additions_or_changes_would_you_make_to/</guid>
      <pubDate>Thu, 30 May 2024 09:24:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cvq77y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 May 2024 15:00:17 GMT</pubDate>
    </item>
    </channel>
</rss>