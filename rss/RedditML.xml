<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Thu, 07 Mar 2024 18:16:15 GMT</lastBuildDate>
    <item>
      <title>[D] 正在进行的工作</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b901k3/d_ongoing_work/</link>
      <description><![CDATA[将正在进行的工作和初步结果放在 arxiv 上不好吗？人们似乎一致认为这种行为是不好的，但我不太明白为什么会这样。我的推理如下： 1. arxiv 背后的动机是为了拥有一个开放的论文存储库，用于不同目的，例如探索新想法、寻找现有方法等。 2. arxiv 上的论文不是“出版物”，而是“出版物”。这间接意味着它们不完整。 3.人们注意到的常见问题是arxiv上的论文质量越来越差，但为什么会出现这个问题呢？无论如何，这些论文不会引起太多关注，并会随着时间的推移而慢慢消失。 4. 即使是有会议记录的顶级会议研讨会也鼓励“正在进行的工作”。 arxiv 之所以如此特别，是因为它没有任何“不完整”的内容。作品已上传？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b901k3/d_ongoing_work/</guid>
      <pubDate>Thu, 07 Mar 2024 17:18:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 可解释的人工智能研究已经失败了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8zifr/r_has_explainable_ai_research_tanked/</link>
      <description><![CDATA[我感觉整个 ML 社区已经以一种奇怪的方式对 XAI 失去了兴趣，或者只是变得极其愤世嫉俗。  在某种程度上，这仍然是所有机器学习领域需要解决的问题，但它与几年前的情况确实不同。现在人们不敢说XAI，而是说“可解释”、“值得信赖”、“监管”、“公平”、“人机交互”、“机械可解释性”等等。 . 我有兴趣了解人们对此的感受，因此我写这篇文章是为了就该主题进行对话。 您对 XAI 有何看法？你相信它有效吗？您认为它只是演变成几个更具体的不同研究领域吗？您是否认为这是一个无用的领域，没有兑现 7 年前的承诺？ 感谢您的意见和见解，谢谢。  &amp; #32；由   提交 /u/SkeeringReal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8zifr/r_has_explainable_ai_research_tanked/</guid>
      <pubDate>Thu, 07 Mar 2024 16:57:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 写入错误</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8zdoz/d_writing_error/</link>
      <description><![CDATA[我向会议和 arxiv 提交了论文的一个版本，在限制部分有一个小的写作错误，内容类似于“该方法没有已在 GPT-4 上进行测试，未在其他模型上进行测试”当我想说“该方法仅在 GPT-4 上进行过测试，未在其他模型上进行过测试”时我不小心写错了“仅”到“不”。我注定要失败吗？   由   提交 /u/BigDreamx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8zdoz/d_writing_error/</guid>
      <pubDate>Thu, 07 Mar 2024 16:52:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从科学出版物中提取元数据</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8yvr8/d_extracting_metadata_from_scientific_publications/</link>
      <description><![CDATA[目前自动提取科学出版物（如 pdf）中的标题、doi、作者、摘要等元数据的最佳工具是什么。我尝试了grobid，但它只能在linux上运行，而且看起来不太现代。是否有任何更新的方法，利用法学硕士等？   由   提交/u/Electronic-Letter592   reddit.com/r/MachineLearning/comments/1b8yvr8/d_extracting_metadata_from_scientific_publications/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8yvr8/d_extracting_metadata_from_scientific_publications/</guid>
      <pubDate>Thu, 07 Mar 2024 16:29:12 GMT</pubDate>
    </item>
    <item>
      <title>使用非结构化领域知识数据调整法学硕士 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8xcgq/adapting_llms_with_unstructured_domain_knowledge/</link>
      <description><![CDATA[我想针对理论领域知识对预训练的法学硕士进行微调。这些领域知识将以我清理过的书籍的 PDF 形式出现，因此有很多原始的非结构化文本。我希望模型能够采用这些文本的观点、知识和风格（例如培训我可以与之交谈的心理学家、建筑师或气候科学家）。 这是可行的吗？任务？不使用LLM可以将非结构化数据转化为提示/答案数据吗？这种技术叫什么？我可以在哪里了解有关如何执行此技术的更多信息？   由   提交 /u/TheRealestKGB   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8xcgq/adapting_llms_with_unstructured_domain_knowledge/</guid>
      <pubDate>Thu, 07 Mar 2024 15:25:06 GMT</pubDate>
    </item>
    <item>
      <title>Huggingface 上的零计算 [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8x3g8/zero_computing_on_huggingface_p/</link>
      <description><![CDATA[如何使用“零”选项电脑在高频？我从收集的信息中假设它是社区捐赠的计算，但在正常设置中没有选项吗？很抱歉在此 Reddit 子版块中发布此内容，但我不需要编码或任何方面的帮助。   由   提交 /u/asoulsghost   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8x3g8/zero_computing_on_huggingface_p/</guid>
      <pubDate>Thu, 07 Mar 2024 15:14:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] MAMBA 优于变形金刚吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8u2kq/d_is_mamba_superior_to_transformers/</link>
      <description><![CDATA[您好， 曼巴架构引起了很多争议。它真的有那么好，比变形金刚更好吗？如果是，为什么我们没有看到它像变形金刚推出时那样被广泛采用。   由   提交 /u/rodeowrong   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8u2kq/d_is_mamba_superior_to_transformers/</guid>
      <pubDate>Thu, 07 Mar 2024 13:00:39 GMT</pubDate>
    </item>
    <item>
      <title>[D]教程：使用Python进行面部情绪识别</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8tioc/d_tutorial_facial_emotion_recognition_with_python/</link>
      <description><![CDATA[情绪识别是机器学习的一部分，属于人工智能这一相对较新的研究领域。 如今，这项技术被用于自动识别图像和视频中的面部表情、音频中的口头表达、文本中的书面表达以及可穿戴设备测量的生理学。 Luxand 提供基于云的情绪识别 API，该 API 提供全套基于 AI 的计算机视觉工具用于人脸识别。通过本教程，您将了解如何使用 Python 将 Luxand.clod 情绪检测 API 实施到您的应用、软件或系统中。 在此处阅读更多内容：- https://luxand.cloud/face-recognition-blog/tutorial-facial-使用 python 进行情感识别   由   提交 /u/PrestigiousGridlock   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8tioc/d_tutorial_facial_emotion_recognition_with_python/</guid>
      <pubDate>Thu, 07 Mar 2024 12:31:55 GMT</pubDate>
    </item>
    <item>
      <title>[R] 尾巴的故事：模型崩溃作为缩放定律的变化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8s8rg/r_a_tale_of_tails_model_collapse_as_a_change_of/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.07043 摘要：  随着人工智能模型规模的增长，神经缩放法则&lt; /em&gt; 已成为在增加原始（人类或自然）训练数据的容量和大小时预测大型模型改进的重要工具。然而，流行模型的广泛使用意味着在线数据和文本的生态系统将共同进化，以逐步包含越来越多的合成数据。在本文中，我们问：在合成数据进入训练语料库的不可避免的情况下，缩放法则将如何变化？未来的模型是否仍然会改进，或者注定会退化到总 &lt; em&gt;（模型）崩溃？我们通过缩放定律的视角开发了模型崩溃的理论框架。我们发现了各种各样的衰变现象，分析了尺度损失、随代数变化的尺度变化、“不学习”现象等。技能，以及混合人类和合成数据时的摸索。我们的理论通过使用大型语言模型 Llama2 进行算术任务和文本生成的变压器的大规模实验得到了验证。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8s8rg/r_a_tale_of_tails_model_collapse_as_a_change_of/</guid>
      <pubDate>Thu, 07 Mar 2024 11:19:50 GMT</pubDate>
    </item>
    <item>
      <title>[R] 微软研究院推出 NaturalSpeech 3，这是零样本文本转语音技术的重大进步。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8pw7i/r_microsoft_research_unveils_naturalspeech_3_a/</link>
      <description><![CDATA[      论文链接：https://arxiv.org/abs/2403.03100 演示链接：https://speechresearch.github.io/naturalspeech3/&quot;&gt;https:// /speechresearch.github.io/naturalspeech3/ ​ 基于 NaturalSpeech 系列的成功，NaturalSpeech 3 不仅继承了高质量的合成功能而且还通过分解语音属性来进一步推进，从而实现更详细和受控的合成过程。 ​ NaturalSpeech 3 的主要亮点包括： 1.因子化编解码器：具有因子化矢量量化的神经编解码器能够熟练地将语音分解为不同的子空间，从而有针对性地改进语音生成。 2.因子化扩散模型：因子化扩散模型旨在生成语音属性与相应的提示完全一致。这种创新方法使 NaturalSpeech 3 不仅可以合成类似人类的语音，还可以调整韵律和音色的细微差别，以匹配说话者的情感和风格。 3. 可扩展性：可扩展至 10 亿个参数经过超过 20 万小时的数据训练，NaturalSpeech 3 在提高语音质量和清晰度方面显示出了可喜的成果。未来，NaturalSpeech 3 计划进一步扩大规模，以实现更精细的结果。 ​ 深入研究演示、阅读论文，了解 NaturalSpeech 3 的用途为零样本语音合成设定新标准。 https://preview.redd.it/gbgau8vwkvmc1.png?width=1982&amp;format=png&amp;auto=webp&amp;s=3260d6ac03b42e6059c5e0a58169d880b037 b00f ​   由   提交/u/Front-Article-7366   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8pw7i/r_microsoft_research_unveils_naturalspeech_3_a/</guid>
      <pubDate>Thu, 07 Mar 2024 08:47:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么最新、最伟大的法学硕士仍然为生成十个以 apple 结尾的句子这样的小事而苦苦挣扎？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8ohhy/d_why_do_the_latest_and_greatest_llms_still/</link>
      <description><![CDATA[所有 3 个模型（Gemini Advanced、Claude 3.0 Opus、GPT-4）都失败了，gpt-4 表现最好，十有八九以苹果结尾。    由   提交 /u/ccooddeerr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8ohhy/d_why_do_the_latest_and_greatest_llms_still/</guid>
      <pubDate>Thu, 07 Mar 2024 07:18:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] Apollo：轻量级多语言医学法学硕士，将医疗人工智能普及到 6B 人群</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8mml6/d_apollo_lightweight_multilingual_medical_llms/</link>
      <description><![CDATA[    &lt; /a&gt;  我们开源了一系列SOTA轻量级多语言医疗LLM Apollo (0.5B, 1.8B, 2B, 6B, 7B)，利用非翻译语料库取得最佳新表现 覆盖英文、中文、法语、西班牙语、阿拉伯语和印地语  整个过程开源且可复制 精简版模型可以是用于提高大型模型的多语言医疗能力无需以代理调整方式进行微调   github：https://github.com/FreedomIntelligence/Apollo 演示：https://apollo.llmzoo.com/#/ 论文：https ://arxiv.org/abs/2403.03640 模型：https://huggingface.co /FreedomIntelligence/Apollo-7B  https://preview.redd.it/29kjdct4oumc1.png?width=1488&amp;format=png&amp;auto=webp&amp;s=1a16bbbf2588fb071ba2af5a50668ca8335c 92b7 https://preview.redd.it/406m28t4oumc1.png?width=1120&amp; ;format=png&amp;auto=webp&amp;s=607664035b62aa0ee726d3f5b4c4730863823bcb https://preview.redd.it/jiewd5t4oumc1.png?width=1242&amp;format=png&amp;auto=webp&amp;s=e059d49da4e788729b134b0d64415bcf 85bb024c    由   提交 /u/Pasu06   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8mml6/d_apollo_lightweight_multilingual_medical_llms/</guid>
      <pubDate>Thu, 07 Mar 2024 05:33:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] Nvidia Tesla P40 与 Mangio-RVC-Fork 配合良好</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b8ka8a/d_nvidia_tesla_p40_works_great_with_mangiorvcfork/</link>
      <description><![CDATA[正在寻找一种经济有效的方法来训练语音模型，在 eBay 上以 150 美元左右的价格购买了一台二手 Nvidia Tesla P40 和一个 3D 打印冷却器我交叉手指。系统只是我的一台带有 B250 Gaming K4 主板的旧电脑，没什么特别的，在 Windows 10 上运行得很好，并且在 Mangio-RVC-Fork 上以惊人的速度进行训练。它有 24GB 的 vram，因此您可以加载大型数据集并提高批量大小。使用默认设置，我在 rmvpe 上用 35 分钟的数据集训练了一个语音，批量大小为 12（仅使用大约 10GB 的 vram），纪元时间约为 1 分 30 秒。结合较大的 vram 净空，我认为更多的人应该尝试 RVC！性价比无与伦比！   由   提交/u/Remote_Hunt516  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b8ka8a/d_nvidia_tesla_p40_works_great_with_mangiorvcfork/</guid>
      <pubDate>Thu, 07 Mar 2024 03:33:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 逆转诅咒</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1b86vgt/d_the_reversal_curse/</link>
      <description><![CDATA[https://arxiv.org/pdf/2309.12288 .pdf 原来我预测了 2021 年的逆转诅咒哈哈 https://www.reddit.com/r/MachineLearning/comments/p13ean/d_can_gpt_generalize_in_both_directions/ 编辑：第一篇论文引用的另一篇论文甚至使用了非常相似的示例： https://arxiv.org/pdf/2308.03296.pdf &lt; blockquote&gt; 美国第一任总统是乔治·华盛顿  如果我的帖子以任何方式启发作者，我会非常高兴 &lt; !-- SC_ON --&gt;  由   提交 /u/DeMorrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1b86vgt/d_the_reversal_curse/</guid>
      <pubDate>Wed, 06 Mar 2024 18:16:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1azra3g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 25 Feb 2024 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>