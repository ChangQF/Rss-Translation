<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 23 Mar 2024 21:10:44 GMT</lastBuildDate>
    <item>
      <title>[P] Python v0.5.1 中的 copent 包可用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bm2t7w/p_the_copent_package_in_python_v051_is_available/</link>
      <description><![CDATA[Python v0.5.1 中的 copent 包现已在 PyPI 上可用，它实现了估计熵的方法    估计 copula 熵/互信息 估计转移熵/条件互信息 估计多元正态性检验的统计量 估计双样本测试统计 变化点检测  GitHub：https:// /github.com/majianthu/pycopent PyPI：https://pypi.org/project/copent    由   提交/u/majianthu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bm2t7w/p_the_copent_package_in_python_v051_is_available/</guid>
      <pubDate>Sat, 23 Mar 2024 21:00:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLM 是否在招聘信息中被过度炒作？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bm28ls/d_is_llm_too_hyped_up_in_job_postings/</link>
      <description><![CDATA[我正在就业市场寻找暑期实习机会，并在自主车辆领域从事 ML 研究/工程的全职工作。 “趋势”是指在过去的几年里，这个领域的变化如此之快，从超级炒作到大规模裁员，现在只剩下几家公司在谨慎行事。大多数项目范围内的裁员都是由于高管和经理过于乐观的决定而发生的，当时产品没有达到预期/利润。 自 2023 年以来，与所有其他人工智能用例一样，AV也从chatgpt革命中得到了推动。这些天我能找到的每一个招聘信息都在寻找具有法学硕士经验的人。这种程度让我不得不想到“LLM”。是另一个席卷高管的流行词，这种以法学硕士为重点的招聘将导致 1-2 年内又一系列裁员。 我来的法学硕士的用例到目前为止，大部分都是 chatgpt API，这进一步让我思考是否真的有那么多的开发正在进行，以制作有用的基于 LLM 的产品。通过此类职位被录用的人，你们在建设什么？您正在构建的产品的承诺是什么？您认为这个承诺实现的可能性如何？   由   提交/u/madgradstudent99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bm28ls/d_is_llm_too_hyped_up_in_job_postings/</guid>
      <pubDate>Sat, 23 Mar 2024 20:36:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] DPO 仍然是经济实惠地微调模型的最佳方式吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bm0tun/d_is_dpo_still_the_best_way_to_affordably/</link>
      <description><![CDATA[论文“Your Language Model is Secretly a Reward Model: Direct Preference Optimization (DPO)”证明 DPO 可以微调 LM 以符合人类偏好，并且优于现有方法”就像 RLHF 一样。 自从这篇论文于 2023 年 5 月发表以来，我想知道 DPO 是否仍然被认为是快速且经济实惠地微调 LLM 的最佳方法（特别是对于初创公司）。    由   提交/u/JT_NVG8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bm0tun/d_is_dpo_still_the_best_way_to_affordably/</guid>
      <pubDate>Sat, 23 Mar 2024 19:38:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 怎样才能成为一名优秀的机器学习工程师？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blzf0i/d_what_makes_a_good_machine_learning_engineer/</link>
      <description><![CDATA[您认为，怎样才是一名优秀的机器学习工程师？我所说的机器学习工程师指的是不进行研究，而是进行研究并将其实施到生产就绪代码中的人。他们应该具备哪些技能/知识？   由   提交/u/Raiz314  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blzf0i/d_what_makes_a_good_machine_learning_engineer/</guid>
      <pubDate>Sat, 23 Mar 2024 18:40:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻求直觉：如何将 3D 顶点投影到目标视图上来获取特征？另外，对 View Frustum (Sync Dreamer) 有疑问</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blx2xc/d_seeking_intuition_how_does_projecting_3d/</link>
      <description><![CDATA[我目前正在阅读 Sync Dreamer 论文，我对“3D 感知特征注意力”如何实现有点困惑。工作中。为了提供一点背景信息，他们试图形成一个空间特征体积，在反向扩散过程中所有视图的降噪器都会引用该空间特征体积，以尝试生成一致的多视图图像。  我对这里的几个部分感到困惑： ​  为了实现这一点，我们首先用 V^3 构造一个 3D 体积顶点，然后将顶点投影到所有目标视图上以获得特征。每个目标视图的特征被连接起来形成一个空间特征量  ​  为了对第n个目标视图进行去噪，我们构造一个与该视图按像素对齐的视锥体，其特征是通过对空间体积的特征进行插值来获得的。最后，在 UNet 中当前视图的每个中间特征图上，我们应用一个新的深度注意层，从沿深度维度的像素对齐视图平截头体特征体积中提取特征。  ​  在目标视图上投影 3D 顶点有何直观帮助？  将视锥体逐像素与目标视图对齐的含义是什么？     由   提交 /u/ChaosAdm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blx2xc/d_seeking_intuition_how_does_projecting_3d/</guid>
      <pubDate>Sat, 23 Mar 2024 17:02:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 📚 大语言模型的逐层可视化 💻</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blw6zi/r_layerbylayer_visualization_of_large_language/</link>
      <description><![CDATA[ 由   提交/u/dippatel21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blw6zi/r_layerbylayer_visualization_of_large_language/</guid>
      <pubDate>Sat, 23 Mar 2024 16:24:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] DeepRL 代理与 Ken 一起完成《街头霸王 III》</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bluore/p_deeprl_agent_completing_street_fighter_iii_with/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bluore/p_deeprl_agent_completing_street_fighter_iii_with/</guid>
      <pubDate>Sat, 23 Mar 2024 15:21:15 GMT</pubDate>
    </item>
    <item>
      <title>[D] LLaMA2 模型推理工作负载跟踪</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blsrp6/d_llama2_model_inference_workload_traces/</link>
      <description><![CDATA[是否有任何数据集可以提供有关 LLaMA2 模型分层操作的操作性能和资源利用率的详细见解？我正在进行的研究需要深入查看推理工作负载跟踪，特别关注每一层操作的资源需求。我将非常感谢任何关于在哪里找到此类数据的指导或建议。预先感谢您提供的任何帮助！   由   提交 /u/bipulthapa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blsrp6/d_llama2_model_inference_workload_traces/</guid>
      <pubDate>Sat, 23 Mar 2024 13:56:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] DenseFormer：通过深度加权平均增强 Transformer 中的信息流</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blp62y/r_denseformer_enhancing_information_flow_in/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.02622 代码：https://github .com/epfml/DenseFormer 摘要：  Vaswani 等人的变压器架构。 （2017）现在在从自然语言处理到语音处理和图像理解的各个应用领域中无处不在。我们提出了DenseFormer，这是对标准架构的简单修改，可以在不增加模型大小的情况下提高模型的复杂度——为 100B 参数范围内的大型模型添加数千个参数。我们的方法依赖于每个转换器块之后的附加平均步骤，该步骤计算当前和过去表示的加权平均值 - 我们将此操作称为深度加权平均（DWA）。学习到的 DWA 权重表现出连贯的信息流模式，揭示了对来自远处层的激活的强大且结构化的重用。实验表明，DenseFormer 的数据效率更高，达到了与更深的 Transformer 模型相同的困惑度，并且对于相同的困惑度，这些新模型在内存效率和推理时间方面优于 Transformer 基线。  &lt; /div&gt;  由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blp62y/r_denseformer_enhancing_information_flow_in/</guid>
      <pubDate>Sat, 23 Mar 2024 10:35:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] Mora：通过多代理框架实现通用视频生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blox6v/r_mora_enabling_generalist_video_generation_via_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.13248 GitHub：https:// /github.com/lichao-sun/Mora 摘要：  Sora是第一个大规模通才视频生成的模式引起了社会的广泛关注。自 OpenAI 于 2024 年 2 月推出以来，没有其他视频生成模型能够与 Sora 的性能或支持广泛视频生成任务的能力相媲美。此外，只有少数完全发布的视频生成模型，其中大多数是闭源的。为了解决这一差距，本文提出了一种新的多智能体框架Mora，它结合了多个先进的视觉人工智能智能体来复制Sora演示的通用视频生成。特别是，Mora 可以利用多个视觉代理并在各种任务中成功模仿 Sora 的视频生成功能，例如（1）文本到视频生成，（2）文本条件图像到视频生成，（3）扩展生成视频、(4) 视频到视频编辑、(5) 连接视频和 (6) 模拟数字世界。我们大量的实验结果表明，Mora 在各种任务中都取得了与 Sora 相近的性能。但综合来看，我们的工作与Sora之间存在着明显的绩效差距。总而言之，我们希望这个项目能够通过协作人工智能代理来指导视频生成的未来轨迹。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blox6v/r_mora_enabling_generalist_video_generation_via_a/</guid>
      <pubDate>Sat, 23 Mar 2024 10:19:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我听了 Sam Altman 最近在 Lex Fridman 进行的 2 小时采访 - 以下是我们都应该知道的关键要点</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blnzj1/d_i_listened_to_sam_altmans_most_recent_2hour/</link>
      <description><![CDATA[Altman 本周在 Lex Fridman 播客上接受采访。这是一次相当长的采访，所以我想分享一下我听他讲话时记下的 10 个关键要点。你可以支持他，也可以反对他——但你不能否认他是即将到来的人工智能常态中最重要的核心人物之一！希望这对感兴趣的人来说是有洞察力的:) -- 1。创造有价值数据的人应该因使用这些数据而获得某种补偿💸 剩下的问题是实现它的经济模型。一个很好的类比是音乐从 CD 到 Napster 再到 Spotify 的转变。或者从电影到 YouTube 视频的转变。未来数据是否存在类似的经济模型？ ⏳ 40:16 -- 2. 比尔盖茨无法想象我们有一天会在计算机中需要千兆字节的内存💾 同样，我们可以&#39;今天，我们无法想象法学硕士如何或为何需要数十亿的上下文长度，但这仍然可能发生。 （对于上下文：具有 10 亿上下文长度的法学硕士意味着它可以处理和理解每个查询的约 200 万个文档页） ⏳ 51:13 --  “我想赋予 ChatGPT 保留记忆的能力”📝  想象一下，一个模型会逐渐了解您并随着时间的推移对您变得更加有用。这很可能是上面强调的十亿上下文长度 LLM 的一个用例。 ⏳ 55:33 --  计算将成为未来的货币。 💲  Sam 相信它将成为世界上最珍贵的商品。 ⏳ 1:09:55 --  核聚变将解决“能源问题”⚛  由于未来世界需要大量的计算，我们将需要大量的能源来为一切提供动力。 Sam 相信核聚变是解决这个问题的最佳方法。 ⏳ 1:11:29 --  Q- star 可能存在（但我们不会谈论这个）⭐  Lex 当然询问了 Q-star，但 Sam 并没有否认它的存在- 只是说“我们还没有准备好谈论这个”。 ⏳ 1:02:36 --  程序员不会过时👩🏻‍💻  但它可能会与现在的编程方式有所不同。不管怎样，萨姆认为没有人真正进行纯粹的编码——因为大多数程序员使用预先存在的软件包/技术/软件。利用 LLM 协助编码的方式与此类似。 ⏳ 1:29:50 --  超越 Google很无聊🔍  OpenAI 不想做一个更好的搜索引擎；以这种方式思考低估了他们在人工智能方面的工作。 ⏳ 1:17:37 --  “ ChatGPT 中不会有广告！” （最好）🚩  Sam 对广告有偏见 - 这就是为什么目前 ChatGPT 的商业模式是通过付费进行的。在某种程度上，我觉得这令人放心 - 因为当你引入广告时，你的“真正的客户”现在就变成了广告商，而不是实际的用户（现在变成了产品）。 ⏳ 1:20： 15 --  我们不再谈论 AGI（让我们称之为别的东西吧）🧠 &lt; /ol&gt; 人们对于 AGI 是什么有着不同的定义，因此 Sam 主张更多地谈论具体的功能，而不是把 AGI 作为一个通用术语。不过，根据他的定义，AGI 是一个无需人类干预即可推进科学发现的系统。 ⏳ 1:32:33    ;由   提交 /u/SwimIndependent6688   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blnzj1/d_i_listened_to_sam_altmans_most_recent_2hour/</guid>
      <pubDate>Sat, 23 Mar 2024 09:15:03 GMT</pubDate>
    </item>
    <item>
      <title>[N] Stability AI 创始人 Emad Mostaque 计划辞去首席执行官职务</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</link>
      <description><![CDATA[https://www.forbes.com/sites/kenrickcai/2024/03/22/stability-ai-Founder-emad-mostaque- plan-to-resign-as-ceo-sources-say/ 官方公告：https: //stability.ai/news/stabilityai-announcement 无付费专区，福布斯：  尽管如此，莫斯塔克还是向公众展现了勇敢的一面。 “我们的目标是今年实现正现金流，”他二月份在 Reddit 上写道。据一位知情人士透露，即使在会议上，他也将计划中的辞职描述为一次成功使命的顶峰。  首先是 Inflection AI，现在是 Stability AI？你有什么想法？   由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blixvf/n_stability_ai_founder_emad_mostaque_plans_to/</guid>
      <pubDate>Sat, 23 Mar 2024 03:49:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 进行机器学习面试后感到精疲力竭</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</link>
      <description><![CDATA[在过去的两个月里，我一直在面试机器学习工程师和相关职位，从大型科技公司到小型初创公司。采访的风格多种多样，而且似乎无处不在。即使面试了10家不同的公司，后来又面试了30多次，我都没有成功。我要么被他们迷住了，要么被拒绝了。 我接受过的一些面试是：  Leetcode 风格的编码问题。 从头开始实现 SVM 等机器学习算法或反向传播或卷积等算法的某些组件。 深入了解与编程语言相关的问题，例如有关 Python GIL 或 C++ 指针的问题。 与 OOP 相关的理论和实现问题。 典型的 SWE 风格系统设计面试，例如设计 Instagram 机器学习系统设计面试，例如设计推荐系统。 机器学习理论问题，例如什么是铰链损失或解释逻辑回归或何时可以使用 KL 散度。 深度学习理论问题，例如 SGD 和 Adam 之间的区别是什么、神经网络中的量化是什么、如何量化你能加快深度学习模型的推理速度吗？ 计算机视觉理论问题，例如 YOLO 和 FasterRCNN 之间有什么区别、什么损失函数可用于图像分割或解释对极几何。 自然语言处理理论问题，例如 Transformer 为何比 RNN 更好、BERT 中的双向性是什么，或者词干提取和词形还原之间有什么区别。 之前的工作、之前的研究论文、之前的项目相关问题. 带回家的作业也无处不在，从构建基于时间序列的模型到部署分类模型作为与公司面临的相关问题的端点。 与工具相关的问题，例如 Docker、Kubernetes、AWS 等。 行为轮面试 数学、统计和基于概率的面试，例如贝叶斯定理或伯努利分布或矩阵的等级是什么或区分某些东西。  我确信我还缺少其他风格的采访。我的记忆力不太好，所以也许我容易忘记我所学的东西，因此觉得这些采访很困难。我想知道人们是如何准备这些采访的。   由   提交 /u/Tiny-Masterpiece-412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bleu7d/d_feeling_burnt_out_after_doing_machine_learning/</guid>
      <pubDate>Sat, 23 Mar 2024 00:26:23 GMT</pubDate>
    </item>
    <item>
      <title>[R] 哪些令人尴尬的并行工作负载需要 GPU？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1blbl7c/r_what_embarrassingly_parallel_workloads_require/</link>
      <description><![CDATA[大家好， 我正在研究垂直 GPU 集群，并寻找一些可以运行的用例我正在构建的集群。我从法学硕士批量推理开始，但很想听听你的想法。唯一的要求是他们不使用机器间通信。    由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1blbl7c/r_what_embarrassingly_parallel_workloads_require/</guid>
      <pubDate>Fri, 22 Mar 2024 22:06:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>