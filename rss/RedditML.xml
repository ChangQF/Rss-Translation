<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 23 Dec 2023 18:15:51 GMT</lastBuildDate>
    <item>
      <title>[D] 寻求有关 GrimesAI 所用模型和歌曲生成替代方案的信息</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18pak1r/d_seeking_information_on_the_model_used_for/</link>
      <description><![CDATA[大家好！ 我正在深入探索人工智能语音合成的迷人世界，并且有一个具体的查询。有谁知道GrimesAI是使用哪种AI模型来开发的？我对提供超出标准语音合成功能的模型特别感兴趣。 理想情况下，我正在寻找一个模型，该模型允许我输入一些语音样本，然后使用它们来演唱整首歌曲。虽然 ElevenLabs 在语音克隆方面做得很好，但它似乎主要适合常规对话。我的目标是能够处理更细致的声音任务，比如唱歌，并具有相同的真实感。 任何对擅长唱歌和语音调制的模型的见解或建议将不胜感激。我渴望进一步探索这项技术，看看它如何将简单的语音样本转化为优美的旋律。 提前感谢您的帮助！   由   提交/u/yachty66  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18pak1r/d_seeking_information_on_the_model_used_for/</guid>
      <pubDate>Sat, 23 Dec 2023 17:31:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人使用 fairseq 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18paekp/d_does_anyone_using_fairseq/</link>
      <description><![CDATA[我正在寻找教程来了解 fairseq 堆栈以及如何修改或添加新的架构模型 l。有人可以帮助我吗？   由   提交/u/ahsaor8  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18paekp/d_does_anyone_using_fairseq/</guid>
      <pubDate>Sat, 23 Dec 2023 17:23:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何使用 Vision Transformers 或 ViViT 预处理视频分类数据集？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18pa7uo/d_how_to_preprocess_dataset_for_video/</link>
      <description><![CDATA[我的数据集结构 数据集 |---正在运行 | ---视频 1 ... 视频 80 |--- 行走 |---视频 1 ... 视频 80 |-- -坐 |---视频1 ...视频80 帧存储在每个视频目录中，帧率为1 fps，每个视频目录具有不同的帧数。我想知道如何预处理这些数据并将其输入视觉转换器。   由   提交 /u/XilentXenocide   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18pa7uo/d_how_to_preprocess_dataset_for_video/</guid>
      <pubDate>Sat, 23 Dec 2023 17:14:42 GMT</pubDate>
    </item>
    <item>
      <title>[项目][P] 需要帮助 Valo 对象检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p9mu2/projectp_help_needed_valo_object_detection/</link>
      <description><![CDATA[需要 Valo 对象检测帮助 我是计算机视觉新手，我一直在尝试创建一个对象检测模型能够识别《valorant》中的单个角色。我尝试使用tensorflow对象检测API微调SSD mobilenet 32​​0x320，但我遇到了很高的正则化损失值。我尝试减少 L2 正则化器。我正在处理的数据集是我收集的，包含每个类别的大约 70 张图像。如果有人能告诉我我做错了什么，我会很高兴。   由   提交 /u/binkscrew   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p9mu2/projectp_help_needed_valo_object_detection/</guid>
      <pubDate>Sat, 23 Dec 2023 16:47:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 万事通，多才多艺：设计通用的从粗到细的视觉语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p887s/r_jack_of_all_tasks_master_of_many_designing/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.12423 项目页面：https:/ /shramanpramanick.github.io/VistaLLM/ 摘要：  大型语言模型（LLM）处理的能力视觉输入催生了通用视觉系统，通过指令调整来统一各种视觉语言（VL）任务。然而，由于视觉领域输入输出格式的巨大多样性，现有的通用模型无法成功地将分割和多图像输入与粗级任务集成到单个框架中。在这项工作中，我们介绍了 VistaLLM，这是一个功能强大的视觉系统，可以使用统一的框架在单个和多个输入图像上处理粗粒度和细粒度的 VL 任务。 VistaLLM 利用指令引导的图像标记器，使用任务描述过滤全局嵌入，从大量图像中提取压缩和细化的特征。此外，VistaLLM 采用梯度感知自适应采样技术将二进制分段掩码表示为序列，与以前使用的均匀采样相比显着改进。为了增强 VistaLLM 的所需功能，我们策划了 CoinIt，这是一个包含 680 万个样本的全面的从粗到精的指令调整数据集。我们还通过引入一项新任务 AttCoSeg（属性级联合分割）来解决多图像基础数据集的缺乏，该任务增强了模型对多个输入图像的推理和基础能力。对各种 V 和 VL 任务进行的大量实验证明了 VistaLLM 的有效性，它可以在所有下游任务的强大基线上实现一致的最先进性能。我们的项目页面可以在 这个 https URL 找到。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p887s/r_jack_of_all_tasks_master_of_many_designing/</guid>
      <pubDate>Sat, 23 Dec 2023 15:39:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 视觉语言：法学硕士如何生成图像！ （谷歌双子座、Dall-E）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p878r/d_language_of_vision_how_llms_generate_images/</link>
      <description><![CDATA[      大家好，分享我频道中的 YT 视频，其中讨论了多模式 LLM 如何逐个生成图像。对于对该主题感兴趣的人，请点击上面的链接。谢谢！   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p878r/d_language_of_vision_how_llms_generate_images/</guid>
      <pubDate>Sat, 23 Dec 2023 15:38:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] Pearl：生产就绪的强化学习代理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p82zf/r_pearl_a_productionready_reinforcement_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.03814 代码：https://github .com/facebookresearch/pearl 项目页面：https://pearlagent. github.io/ 摘要：  强化学习（RL）为实现长期目标提供了一个多功能框架。它的通用性使我们能够形式化现实世界智能系统遇到的各种问题，例如处理延迟奖励、处理部分可观察性、解决探索和利用困境、利用离线数据提高在线性能以及确保安全约束遇见了。尽管强化学习研究社区在解决这些问题方面取得了相当大的进展，但现有的开源强化学习库往往只关注强化学习解决方案管道的一小部分，而其他方面基本上无人关注。本文介绍了 Pearl，这是一个生产就绪的 RL 代理软件包，专门设计用于以模块化方式应对这些挑战。除了提供初步基准测试结果外，本文还重点介绍了 Pearl 的行业采用情况，以证明其已做好生产使用的准备。 Pearl 在 Github 上开源，网址为 此 http URL，其官方网站位于 这个http URL。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p82zf/r_pearl_a_productionready_reinforcement_learning/</guid>
      <pubDate>Sat, 23 Dec 2023 15:32:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] PPML</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p7v2f/d_ppml/</link>
      <description><![CDATA[大家好，我正在为我的硕士论文研究 ppml。我很乐意联系并获得见解和指导。   由   提交/u/Victorsam47  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p7v2f/d_ppml/</guid>
      <pubDate>Sat, 23 Dec 2023 15:22:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 需要在神经进化算法中选择父母的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p3im8/p_need_advice_selecting_parents_in_neuroevolution/</link>
      <description><![CDATA[我是一个无名小卒，由于缺乏替代方案而创建了自己的神经进化算法。到目前为止它运行得很好，但在最终确定之前我遇到了一个问题，希望有人能给我一些好的建议。 该算法（像许多其他算法一样）基于基因组群体。在每一代结束时，最差基因组的一部分（可配置参数）被删除，剩余的部分用于交配。 这就是我的问题所在，因为在我的脑海里有三个选择父母的方式（可能还有更多，但目前这三种对我来说已经足够了）。 首先，父母双方都可以从总体中完全随机选择。其次，可以使用适应度来衡量选择的权重，以便具有最高适应度的基因组具有更高的繁殖概率。基于此，有两种可能性，即仅选择一个亲本加权，或两者都选择。 我尝试对所有三个选项进行统计评估，但不幸的是，突变等随机因素意味着所有选项平均表现同样好。 进行的测试：每次 30 代的 100 次试验的平均适应度分数。 有人在这方面有任何经验吗？是否相关或者对于进化论是否有统一的观点？从我的观点（以及我所学到的）来看，“最适者”是最适合的。交配时个体总是优先的。 希望群体智能能够帮助我，祝您节日快乐。    ;由   提交/u/Weekly_Branch_5370   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p3im8/p_need_advice_selecting_parents_in_neuroevolution/</guid>
      <pubDate>Sat, 23 Dec 2023 11:11:06 GMT</pubDate>
    </item>
    <item>
      <title>[项目] MiniBoosts：用 Rust 编写的 boosting 算法的一个小集合🦀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18p2dkj/project_miniboosts_a_small_collection_of_boosting/</link>
      <description><![CDATA[       由   提交 /u/__leopardus__   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18p2dkj/project_miniboosts_a_small_collection_of_boosting/</guid>
      <pubDate>Sat, 23 Dec 2023 09:48:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] 法学硕士可解释性研究知识库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18ozqu1/r_llm_interpretability_research_repository/</link>
      <description><![CDATA[对于任何对 LLM 可解释性感兴趣的人，我创建了以下存储库： https://github.com/JShollaj/awesome-llm-interpretability 它包含一组精选的开源工具、论文、文章、群组等。 请随意查看&amp;希望它对您的研究有所帮助。   由   提交 /u/XhoniShollaj   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18ozqu1/r_llm_interpretability_research_repository/</guid>
      <pubDate>Sat, 23 Dec 2023 06:38:23 GMT</pubDate>
    </item>
    <item>
      <title>泰勒级数注意【讨论】</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oxc65/taylor_series_attention_discussion/</link>
      <description><![CDATA[我最近读完了动物学博客文章关于 BASED，一种新的语言模型，它使用局部卷积和使用泰勒级数近似的自注意力，似乎基于本文。&lt; /p&gt; 读完后我的一个问题是这种局部卷积对模型性能有多重要？是否有关于仅采用这种泰勒注意力的变压器架构的研究？ BASED 模型显然具有良好的性能，论文提供的直观理解是，这些卷积在 AR 不是一个大挑战的短距离场景中使模型受益，这是有道理的，但普通注意力对于 AR 或短距离困惑没有问题。答案是泰勒近似在这些短距离情况下会不太准确，需要卷积来补偿吗？   由   提交 /u/Aggressive-Solid6730    reddit.com/r/MachineLearning/comments/18oxc65/taylor_series_attention_discussion/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oxc65/taylor_series_attention_discussion/</guid>
      <pubDate>Sat, 23 Dec 2023 04:13:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么张量程序没有像神经切线核那样受到同样的关注？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18oq5us/d_why_have_tensor_programs_not_received_the_same/</link>
      <description><![CDATA[神经正切核经常在理论论文中被引用，作为推广线性函数逼近器证明的基础。然而，它们有几个缺点，即它们不包含特征学习的概念。张量程序应该可以解决这个问题，但我认为我从未在理论论文中看到过它们被引用。人们是否怀疑结果或认为结果缺乏严谨性？结果是否被认为不太有用？或者它们只是因为数学上更复杂且更难学习而使用较少？  我问这个问题的部分原因是我想知道它是否值得花精力去阅读和理解整个论文系列，或者这项工作是否经过深思熟虑。    由   提交 /u/OptimizedGarbage   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18oq5us/d_why_have_tensor_programs_not_received_the_same/</guid>
      <pubDate>Fri, 22 Dec 2023 22:03:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] 研究论文的升级何时会成为新论文？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18onakf/r_when_does_an_upgrade_on_a_research_paper_become/</link>
      <description><![CDATA[嘿，我是研究新手，过去 6 年来一直致力于实现一篇论文并根据我们在推荐系统领域的用例进行定制几个月。我按照 1. 在损失函数中使用不同的运算符（余弦相似度而不是点积）进行了一些更改。 2. 使用不同类型的数据。这有点难以解释，我使用元数据相似性而不是论文中一对共现的逐点互指数（PMI）。 3. 不同的数据预处理方式。 结果确实有所改善，但我不确定这是否只是一些修改，或者它本身就是一篇论文。任何人都可以阐明新想法与小修改的一些迹象吗？   由   提交/u/Abs0lut_Jeer0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18onakf/r_when_does_an_upgrade_on_a_research_paper_become/</guid>
      <pubDate>Fri, 22 Dec 2023 19:49:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18kkdbb/d_simple_questions_thread/</guid>
      <pubDate>Sun, 17 Dec 2023 16:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>