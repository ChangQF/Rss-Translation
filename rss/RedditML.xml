<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Thu, 09 May 2024 01:00:13 GMT</lastBuildDate>
    <item>
      <title>[研究] ICML 2024 相机就绪</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnjvjl/research_icml_2024_camera_ready/</link>
      <description><![CDATA[大家好， 刚刚收到一封电子邮件，其中包含相机准备说明，没有提及任何有关海报与口头的内容。这是否意味着该论文被指定为单独海报，还是尚未决定？ 谢谢   由   提交 /u/logichael   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnjvjl/research_icml_2024_camera_ready/</guid>
      <pubDate>Thu, 09 May 2024 00:21:01 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 通过高级信息生命周期（AIL）实现适应性强的智能生成人工智能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnivqh/research_adaptable_and_intelligent_generative_ai/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnivqh/research_adaptable_and_intelligent_generative_ai/</guid>
      <pubDate>Wed, 08 May 2024 23:34:18 GMT</pubDate>
    </item>
    <item>
      <title>[P] GitHub - 使用 LLM 生成基于观察的单元测试（Python/OpenSource）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnilxf/p_github_observation_based_unit_test_generation/</link>
      <description><![CDATA[嗨r/MachineLearning， 我们开发了一个名为 CaptureFlow 的工具：https://github.com/CaptureFlow/captureflow-py  开源 Python 项目，结合了 AI（是的，GPT-4）和执行跟踪，根据应用程序的实际运行方式生成测试。这对于测试覆盖率较差的旧应用程序特别有用。 它的作用： CaptureFlow 从您的应用程序捕获运行时信息，用它来了解代码的行为方式，然后自动生成测试。这有点像逆转通常的测试驱动开发，从您拥有的代码开始，然后向后创建它应该通过的测试。  我认为使用 LLM 生成测试的想法并不新鲜，但实际上根据应用程序的性能生成测试的灵感来自 Jane Street 的关于如何自动测试样板创建的文章以及 Facebook 最近的研究论文。  现在还处于早期阶段，还有更多工作要做，特别是支持更多类型的 Python 应用程序。 查看并参与其中：  请查看此 PR 以获取简单示例。我很乐意收到您的反馈或贡献来帮助改进它。 谢谢:]   由   提交 /u/Financial_Muffin396   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnilxf/p_github_observation_based_unit_test_generation/</guid>
      <pubDate>Wed, 08 May 2024 23:21:47 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从 Scrath PPO 实施开始。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnhkyc/p_from_scrath_ppo_implementation/</link>
      <description><![CDATA[过去 5 个月我一直致力于从头开始实施 PPO。除了 numpy 等数值计算库之外，我大部分工作都是从头开始做的。它从监督学习网络开始到现在。我似乎无法理解。我读过的每篇论文都是 A. 过时/不正确 B. 不完整。没有论文完整描述它们的作用以及它们使用的超级参数。我尝试阅读 SB3 代码，但它与我的实现太不同了，我只是不明白发生了什么，因为它只有这么多文件，我找不到细节。所以我只是想发布我的后向方法，如果有人想阅读它并告诉我一些错误/建议。会很好！旁注：我使用标准梯度下降进行了优化，而批评家只采用状态。我没有使用 GAE，因为我试图最大限度地减少潜在的故障点。所有超参数都是标准值。 def back(self): T = len(self.trajectory[&#39;actions&#39;]) for i in range(T): G = 0 for j in range(i, T): current = self.trajectory[&#39;rewards&#39;][j] G += current * pow(self.gamma, j - i) # G = np.clip(G, 0, 15) # CRITIC STUFF if np.isnan(G):break state_t = self.trajectory[&#39;states&#39;][i] action_t = self.trajectory[&#39;actions&#39;][i] # 计算state_t的批评值critic_value = self.critic(state_t) # print(f&quot;Critic: {critic_value}&quot;) # print(f&quot;G: {G}&quot;) # 计算状态-动作对的优势 Advantages = G -ritic_value # print(f&quot;&quot;&quot;&quot;Return: {G} # 预期回报：{critic}&quot;&quot;&quot;) # 旧参数内容 new_policy = self.forward(state_t, 1000) # PPO 内容 Ratio = new_policy / action_t Clipped_ratio = np.clip(ratio, 1.0 - self .clip, 1.0 + self.clip) surrogate_loss = -np.minimum(ratio * 优势, Clipped_ratio * 优势) # entropy_loss = -np.mean(np.sum(action_t * np.log(action_t), axis=1)) # 参数向量weights_w = self.hidden.weights.flatten()weights_x = self.hidden.bias.flatten()weights_y = self.output.weights.flatten()weights_z = self.output.bias.flatten()weights_w = np .concatenate((weights_w,weights_x))weights_w = np.concatenate((weights_w,weights_y)) param_vec = np.concatenate((weights_w,weights_z)) param_vec.flatten() loss = np.mean(surrogate_loss) # + self. l2_regularization(param_vec) # print(f&quot;loss: {loss}&quot;) # 反向传播 next_weights = self.output.weights self.hidden.layer_loss(next_weights, loss, tanh_derivative) self.hidden.zero_grad() self.output.zero_grad () self.hidden.backward() self.output.backward(loss) self.hidden.update_weights() self.output.update_weights() self.critic_backward(G)  &lt; !-- SC_ON --&gt;  由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnhkyc/p_from_scrath_ppo_implementation/</guid>
      <pubDate>Wed, 08 May 2024 22:35:52 GMT</pubDate>
    </item>
    <item>
      <title>【研究】一致性LLM：将LLM转换为并行解码器，推理加速3.5倍</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnfmec/research_consistency_llms_converting_llms_to/</link>
      <description><![CDATA[      大家好！我们在这里分享我们的最新成果：一致性大型语言模型 (CLLM)，这是一类新的模型，能够通过高效并行解码 𝑛 标记来减少推理延迟。您的新朋友，用于 LLM 服务/本地部署，具有更快的推理速度！ 🔥 请查看我们的博客文章，了解加速 3.1 倍的演示： https://hao-ai-lab.github.io/blogs/cllm/ 与现有的快速解码技术相比，CLLM 实现了快速并行解码，无需：  草稿模型 架构修改/辅助模型组件  这为 CLLM 带来了许多优势：  CLLM 不必处理获取“良好”草稿模型和在单个系统中管理两个不同模型的复杂性。 CLLM 与目标 LLM 共享相同的架构，在将该技术应用于不同模型时不需要额外的工程工作。 CLLM 可以与其他技术无缝集成，实现高效的 LLM 推理（例如前瞻解码）来实现更显著的加速。  CLLM 使用的这种解码方法称为 Jacobi 解码，与传统的自回归解码相比，它提高了推理效率。 CLLM 的训练目标是通过将任何随机初始化的 𝑛-token 序列映射到与 AR 解码相同的结果，以尽可能少的步骤执行高效的雅可比解码。 实验结果证明了 CLLM 的有效性，在各种任务中生成速度提高了 2.4 倍到 3.4 倍。 与 Medusa2 相比，CLLM 实现了相当或更好的性能，但**不需要额外的参数或树式验证** CLLM 训练目标可视化 请参阅我们的论文了解更多详情。欢迎试用我们的代码库和CLLM 检查点！ 如果您觉得我们的工作很有趣，请订阅、点赞或转发，谢谢！了解更多信息并在 Twitter 上与我们互动： https://x.com/haoailab/status/1788269848788869299    提交人    /u/No_Yogurtcloset_7050   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnfmec/research_consistency_llms_converting_llms_to/</guid>
      <pubDate>Wed, 08 May 2024 21:15:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 执行大型模型检查点的提示和技巧</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cnflqd/d_tips_and_tricks_for_performing_large_model/</link>
      <description><![CDATA[检查点在 LLM 培训期间非常重要，因为它们可以帮助从上次已知的良好状态重新启动失败的作业。同时，这对于团队来说也是一个巨大的挑战，主要是因为检查点的大小以及您希望尽快保存它们而不阻止训练过程的事实。例如，训练格式的 LLaMa 70B 模型检查点大小为 782 GB。 您将如何每小时保存它们？ 基于我们的团队（根据 Nebius AI）的经验，我们准备了执行大型模型检查点的提示和技巧摘要： 博客 https://nebius.ai/blog/posts/model-pre-training/large-ml-model-checkpointing-tips 视频来自上次在阿姆斯特丹举行的聚会 (https://www.youtube.com/watch?v=8HmORvLbh_o&lt; /a&gt;) MLOps 社区播客：处理多 TB 大型模型检查点。音频 ( https://podcasters.spotify.com/pod/show/mlops/episodes/Handling-Multi-Terabyte-LLM-Checkpoints--Simon-Karasik--228-e2j32c4）可在流行的播客平台，这是视频 (https://www.youtube.com/watch?v=6MY -IgqiTpg）。 如果您了解有关检查点的更多最佳实践，请将它们添加为评论并让我们讨论它们！ &lt; /div&gt;  由   提交/u/Patrick-239  /u/Patrick-239 reddit.com/r/MachineLearning/comments/1cnflqd/d_tips_and_tricks_for_performing_large_model/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cnflqd/d_tips_and_tricks_for_performing_large_model/</guid>
      <pubDate>Wed, 08 May 2024 21:14:48 GMT</pubDate>
    </item>
    <item>
      <title>[P] 🔍 寻求有关为我的自定义数据集微调 SSD 对象检测的建议 🎯</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cne918/p_seeking_advice_on_finetuning_ssd_object/</link>
      <description><![CDATA[大家好！我正在深入研究对象检测的世界，我的目标是为我的自定义数据集微调 SSD（单次多盒检测器）。经过一些研究后，SSD 的架构似乎与我的项目需求完美契合。 有人推荐可以帮助我完成此任务的教程、笔记本或资源吗？具体来说，我正在寻找有关使用预先训练的特征选择模型获取 SSD 检测器，然后对其进行调整以适合我的数据集的提示。   由   提交/u/JAEng22  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cne918/p_seeking_advice_on_finetuning_ssd_object/</guid>
      <pubDate>Wed, 08 May 2024 20:16:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] Transformer 如何在单次梯度更新后记住事实？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cne766/d_how_do_transformers_memorize_facts_after_a/</link>
      <description><![CDATA[我想我首先想要一个关于这个事实的引证，或者有人告诉我这是我编造的。但民间知识是，经过单个时期训练的变压器可以回忆起在训练数据集中只出现一次的事实。这意味着单次更新足以修改权重以产生正确的输出（而不会灾难性地忘记其他事实）。 这真的让我很惊讶。我认为，一次足够大的更新来大幅修改输出会非常具有破坏性，而且考虑到损失景观的非单调性，可能不会达到你想要的效果。有没有一个很好的答案来解释这种情况是如何/为什么发生的，如果有，有人可以提供一个研究这个问题的研究链接吗？它是大型模型（类似 NTK）的功能、变压器架构的功能，还是其他什么？请注意，我不是在询问上下文学习，而是询问从单个梯度步骤的变化。    提交人    /u/asdfwaevc   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cne766/d_how_do_transformers_memorize_facts_after_a/</guid>
      <pubDate>Wed, 08 May 2024 20:14:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 手绘地图中地块的分割</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cndn2u/d_segmentation_of_land_plots_in_hand_drawn_maps/</link>
      <description><![CDATA[我目前正在进行一个个人项目，该项目旨在从带有文本的黑白地图上分割道路和产权线之间的土地区域。该过程的第一阶段是分割和标记每个地块。我知道一些模型存在于 Qgis 和 ArcGis 等工具中，但出于某些原因，我试图避免直接使用它们。 我的经验主要是在医学图像分割和分类方面，但这些模型有固定的类别，而且我使用的模型也无法转化为这些数据。因此，我想就该领域的现有模型寻求指导，以便我可以参考和微调或从头开始训练。 作为基准，可以使用像 Unet 这样的具有 ResNet 或 VGG 主干的东西来分割土地与道路，但我不确定如何确保它将每个地块标记为单独的对象。此外，任何我可以微调的预训练模型也会有所帮助   提交者    /u/Entire_Ad_6447   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cndn2u/d_segmentation_of_land_plots_in_hand_drawn_maps/</guid>
      <pubDate>Wed, 08 May 2024 19:51:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何选择可靠的 XAI 方法并理解相互矛盾的解释？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cn9wlk/d_how_to_select_reliable_xai_methods_and_make/</link>
      <description><![CDATA[      https://preview.redd.it/baife9rhc8zc1.png?width=858&amp;format=png&amp;auto=webp&amp;s=149789e027657 cb292398427be23ae4e8f09a817 该图来自 http://arxiv.org/abs/2206.04394，名为 xplique 的库。它旨在显示作者已包含在库中的所有不同归因方法。然而，在我看来，不同的方法可能会突出显示这些图像中的不同区域和像素结构，在某些情况下部分或很少重叠。最终，我将得到一个模型、一个预测和一个基本事实（我可能知道也可能不知道）。我应该使用哪种方法来解释给定的预测？我怎么知道事先应该选择哪一个呢？即使我更改模型架构和数据域，是否有一种方法始终有效？ （假设这里仅使用事后方法） 我目前正在从实用性和用户清晰度的角度对表格数据的不同 XAI 方法进行比较分析。我有兴趣了解当前对于全球和本地解释的可靠方法的共识是什么。也就是说，考虑到当今 XAI 方法和库的种类繁多，是否有广受青睐的方法？或者相反，通常会避免？除此之外，我想考虑方法的稳健性（它是否适用于不同类型的模型和不同的数据域？），以及解释本身的可解释性（理解解释本身并将其传达给非技术人员有多容易）用户）。由此，我还想知道，有没有办法衡量解释的质量（或适用性）？也就是说，我如何知道我正在查看的解释是否确实有意义/我是否针对给定的用例使用了正确的方法？尽管上面的示例与计算机视觉相关，但我怀疑在表格数据的情况下我也会发现类似的问题。 关于高度可信或通常避免的方法/库有什么建议吗？   div&gt;  由   提交/u/xian-yu  /u/xian-yu  reddit.com/r/MachineLearning/comments/1cn9wlk/d_how_to_select_reliable_xai_methods_and_make/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cn9wlk/d_how_to_select_reliable_xai_methods_and_make/</guid>
      <pubDate>Wed, 08 May 2024 17:13:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有趣的小发现：双子座在遵循简单的数字序列方面出奇地糟糕</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cn9ejf/d_fun_little_discovery_gemini_is_surprisingly_bad/</link>
      <description><![CDATA[试试这个：告诉它按顺序回复下一个数字，从 1 开始，然后你回复下一个，依此类推。几条消息之后，它开始输出文本而不是数字，大约 20 条消息之后，它完全无法继续遵循序列。  最初的目标是找出它的长期记忆有多好，这似乎很糟糕。鉴于其巨大的上下文窗口，它怎么会这么快失去注意力？    提交人    /u/ifilipis   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cn9ejf/d_fun_little_discovery_gemini_is_surprisingly_bad/</guid>
      <pubDate>Wed, 08 May 2024 16:52:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] 训练时奇怪的损失曲线</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cn94yq/d_strange_loss_curve_while_training/</link>
      <description><![CDATA[https://preview.redd.it/z5wmyi0nb8zc1.png?width=599&amp;format=png&amp;auto=webp&amp;s=97e108bd749f9cf0874759f7ba0b8aafb3 260640 今天我正在训练一个文本数据集上的小（1107 万）参数 GPT 模型，我在训练时遇到了这条损失曲线，是否有任何解释为什么损失首先稳定在 2.4 左右，然后开始呈指数下降？另外，为什么在大约 1200 步之间会突然出现峰值？  我使用的数据集是整部小说“一百年的孤独” &lt; li&gt;训练数据集中的总 token 数量为 82 万，词汇量为 77（我使用的是字符级 tokenizer） 6 个转换器层，有 6 个注意力头，每个都没有偏差在任何项目且没有 dropout 层中，上下文长度为 1024，批量大小为 32    由   提交 /u/ApartmentEither4838   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cn94yq/d_strange_loss_curve_while_training/</guid>
      <pubDate>Wed, 08 May 2024 16:41:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] PyTorch 中使用序列打包时的文档内前缀（累积）总和</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cn2ihx/d_intradocument_prefix_cumulative_sum_when_using/</link>
      <description><![CDATA[      根据 &lt; a href=&quot;https://x.com/PMinervini/status/1781080046972604739&quot;&gt;X 上的这篇文章，LLaMa 3 使用Intra -在预训练期间记录因果屏蔽，以避免使用序列打包时的交叉污染： &lt; a href=&quot;https://preview.redd.it/14e5ml4bs6zc1.jpg?width=680&amp;format=pjpg&amp;auto=webp&amp;s=58e7e1764aeafe412237fb731488873aeb911e88&quot;&gt;因果屏蔽与文档内因果屏蔽 与简单地放置分隔符相比，这似乎提高了模型在各种任务中的最终性能 - 例如文本结束标记 - 位于文档之间，并希望模型能够学会在预训练期间不关注不相关的文档，如 GPT-3 论文&lt;中所述/a&gt;:  在训练期间，我们总是在完整的 nctx = 2048 token 上下文窗口的序列上进行训练，当文档短于 2048 时，将多个文档打包到单个序列中，以增加计算量效率。具有多个文档的序列不会以任何特殊方式进行屏蔽，而是使用特殊的文本标记结尾来分隔序列中的文档，从而为语言模型提供必要的信息来推断由文本标记结尾分隔的上下文是不相关的。这允许有效的训练，而不需要任何特殊的序列特定掩蔽。  我想在我的实验中使用前一种 - 文档内 - 方法，但我偶然发现了一个问题。在标准转换器实现中，令牌之间的唯一交互是通过注意机制，因此使用适当的屏蔽就足够了。然而，我使用稍微修改过的版本，它还计算查询和键的累积（前缀）总和，以引入位置偏差，与 RoPE 或 ALiBi 相比，该版本似乎工作得很好。这里不详细介绍相关代码： #cumulative/prefix sum acrossequence q_pos = q.cumsum(-3) k_pos = k.cumsum(-3)   现在，如果没有文档内因果屏蔽或在推理过程中，此代码可以正常工作，但是当序列被打包时，它可能包含一堆不相关的文档。 我想以某种方式阻止来自过去不相关文档的“cumsum”的查询和键污染操作并高效地完成 - 无需 python 循环。这里最好的方法是什么？   由   提交/u/kiockete  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cn2ihx/d_intradocument_prefix_cumulative_sum_when_using/</guid>
      <pubDate>Wed, 08 May 2024 11:45:29 GMT</pubDate>
    </item>
    <item>
      <title>【研究】xLSTM：扩展长短期记忆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cmwljs/research_xlstm_extended_long_shortterm_memory/</link>
      <description><![CDATA[摘要： 20世纪90年代，恒定误差轮播和门控被引入作为长短期的中心思想记忆（LSTM）。从那时起，LSTM 经受住了时间的考验，并为许多深度学习的成功案例做出了贡献，特别是它们构成了第一个大型语言模型 (LLM)。然而，以并行自注意力为核心的 Transformer 技术的出现标志着一个新时代的到来，其规模超过了 LSTM。我们现在提出一个简单的问题：当将 LSTM 扩展到数十亿个参数、利用现代法学硕士的最新技术、同时缓解 LSTM 的已知局限性时，我们在语言建模方面能走多远？首先，我们引入具有适当归一化和稳定技术的指数门控。其次，我们修改 LSTM 内存结构，获得：（i）具有标量内存、标量更新和新内存混合的 sLSTM，（ii）具有矩阵内存和协方差更新规则的完全可并行化的 mLSTM。将这些 LSTM 扩展集成到残差块主干中会产生 xLSTM 块，然后将这些块残差地堆叠到 xLSTM 架构中。与最先进的 Transformer 和状态空间模型相比，指数门控和修改后的内存结构增强了 xLSTM 的性能，无论是在性能还是扩展方面。 链接：xLSTM：扩展长短期记忆   由   提交 /u/Background_Thanks604   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cmwljs/research_xlstm_extended_long_shortterm_memory/</guid>
      <pubDate>Wed, 08 May 2024 05:06:40 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>