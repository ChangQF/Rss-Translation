<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions</description>
    <lastBuildDate>Mon, 12 Aug 2024 01:09:24 GMT</lastBuildDate>
    <item>
      <title>[D] 低数据扩散的架构，但 PCA 也表明需要 5%-10% 的 PCA 组件特征数量才能解释 80-90% 的方差？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epzir5/d_architecture_for_diffusion_for_low_data_but_pca/</link>
      <description><![CDATA[我有一个数据集，相当少。2K 个样本。这些不是图像，而是集合数据。每个样本都是一组值 V，每组/样本大约有 50,000 个。 当我执行 PCA 时，我看到 numV 成分的 10% 解释了 90% 左右的方差。 我尝试了一个简单的 Transformer 块 x 3，以及一些跳过/规范。并训练了反向扩散。低点很低。但是当我生成数据时，损失（MSE）很低，但绝对皮尔逊相关系数很低，所以它不能很好地捕捉到起伏。 你们有什么建议吗？对于神经网络的架构或数据表示等 编辑：我还要补充一点，Transformer 在计算和存储大型注意矩阵时非常痛苦 整个数据集的形状为 (2000, 50_000)    提交人    /u/MysticalDragoneer   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epzir5/d_architecture_for_diffusion_for_low_data_but_pca/</guid>
      <pubDate>Mon, 12 Aug 2024 00:04:56 GMT</pubDate>
    </item>
    <item>
      <title>[P] Vision Transformer + 聊天机器人</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epz92g/p_vision_transformer_chatbot/</link>
      <description><![CDATA[假设我想在某些图像数据和注释上训练一个预先训练过的 Vision Transformer/CLIP 模型，并将其实现为聊天机器人，基本上像 ChatGPT4.0，我该怎么做？我应该提到，这些图像的注释将是单词/双词，但我的要求包括，鉴于 Transformer 已在其他图像数据上进行过预先训练并且具有进行对话的能力，因此它在扫描图像后可以给出 4-5 行文本。我有计算机视觉方面的经验和法学硕士的基本经验，但这对我来说绝对是另一个级别。    提交人    /u/DaTrollFace   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epz92g/p_vision_transformer_chatbot/</guid>
      <pubDate>Sun, 11 Aug 2024 23:52:21 GMT</pubDate>
    </item>
    <item>
      <title>付费 API 与本地机器 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epyta8/paid_apis_vs_local_machine_d/</link>
      <description><![CDATA[我正在考虑构建一台带有几个高端 GPU 的计算机，用于在本地运行 ML 模型（可能是 ollama 和开放 WebUI）。 但是，我已经计算了机器构建的数字，现在想知道是否应该将这笔钱花在支付 API 上。两个 RTX 4090 24GB 将花费我 5000 美元。我将这些 GPU 的相关使用寿命定为 5 年。这大约是每月 85 美元的 API 代币。 哪个会更好？ 有人解决了这个问题吗？    提交人    /u/mikedensem   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epyta8/paid_apis_vs_local_machine_d/</guid>
      <pubDate>Sun, 11 Aug 2024 23:32:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 需要有关使用机密数据在本地微调 LLM 以及创建 PDF 模板的建议</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eps6k5/d_need_advice_on_finetuning_llms_locally_with/</link>
      <description><![CDATA[大家好！我计划在本地机器上使用 Llama 3.1 70B 模型，使用公司的一些机密数据对其进行微调。我担心的是，尽管我将在本地运行该模型，但这些数据是否可能会泄露。 此外，您能否推荐一些比 Llama 3.1 70B 更好的 LLM？ 就上下文而言，我有一个大约 9 个 PDF 的数据集（我知道这不是很多），其中包含表格和文本的混合。当我微调模型时，我需要它以 PDF 格式生成模板，仅关注表格和标题。由于我对此还很陌生，因此我非常感谢任何有关如何准备数据集以及我的下一步应该做什么的建议。谢谢！    提交人    /u/thepotentio_reddy09   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eps6k5/d_need_advice_on_finetuning_llms_locally_with/</guid>
      <pubDate>Sun, 11 Aug 2024 18:45:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持有效，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢所有人在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epmsrd/d_simple_questions_thread/</guid>
      <pubDate>Sun, 11 Aug 2024 15:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] onnx 模型转换的资源</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epgi4u/discussion_resources_for_onnx_model_conversion/</link>
      <description><![CDATA[过去六个月我一直在从事一个基于音频的项目，主要使用 TensorFlow，因为需要使用 TensorFlow Lite (TFLite) 部署模型。但是，我在基于音频的增强方面遇到了 TensorFlow 的限制，例如音调变换、房间脉冲响应 (RIR) 和 SpecAugment。相比之下，PyTorch 为这些任务提供了一套更丰富的工具，使其更适合我的项目需求。 鉴于此，我正在考虑切换到 PyTorch。但是，我仍然需要将 PyTorch 模型转换为 TensorFlow 模型以进行部署。在研究过程中，我发现 ONNX 是一种流行的转换方法。但是，似乎 PyTorch 模型需要以特定方式构建才能在转换后与 TensorFlow 兼容。 是否有人有关于如何构建 PyTorch 模型以进行 ONNX 转换的指南，或者知道更灵活的转换技术？ TL;DR：我正在使用 TensorFlow 进行 TFLite 部署的音频项目，但由于 PyTorch 具有出色的音频增强工具，因此正在考虑切换到 PyTorch。我需要将 PyTorch 模型转换为 TensorFlow，并正在寻找有关使用 ONNX 进行此或任何其他灵活转换方法的指导。    提交人    /u/JournalistCritical32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epgi4u/discussion_resources_for_onnx_model_conversion/</guid>
      <pubDate>Sun, 11 Aug 2024 09:07:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从 Scratch 开始的 Vison 语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1epga39/p_vison_language_models_from_scratch/</link>
      <description><![CDATA[        提交人    /u/themathstudent   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1epga39/p_vison_language_models_from_scratch/</guid>
      <pubDate>Sun, 11 Aug 2024 08:52:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 树注意力：GPU 集群上长上下文注意力的拓扑感知解码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep9qpa/r_tree_attention_topologyaware_decoding_for/</link>
      <description><![CDATA[一篇新的研究论文介绍了一种树注意力算法，用于在多个 GPU 上并行化注意力计算，利用 logsumexp 和 max 运算的关联属性将约简结构化为树。 树注意力算法使跨设备解码能够比 Ring Attention 等替代方法渐近地更快地执行（最高可快 8 倍），同时还需要显著减少通信量并将峰值内存减少 2 倍。    提交人    /u/AhmedMostafa16   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep9qpa/r_tree_attention_topologyaware_decoding_for/</guid>
      <pubDate>Sun, 11 Aug 2024 02:17:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从头训练稳定扩散时产生噪声</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep8u6h/d_noisy_generation_when_traininig_stable/</link>
      <description><![CDATA[      您好， 我正在 CIFAR10 上从头开始训练稳定扩散 1.4。我使用的是 CompVis 实现，训练脚本直接取自：https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py 进行必要的修改，从训练和推理中取出 VAE，并将样本大小从 64 改为 32，输入/输出通道从 4 改为 3，并减小模型大小（在上行和下行块中都删除了一个 crossattn 块）。其他所有内容都与原始代码保持一致。我正在进行 fp16 训练。 但是，经过足够数量的步骤（例如 170K 步骤 * 8 批量大小）后，结果仍然非常嘈杂且高度饱和。例如，第一张图片是 SD1.4 的生成，但具有 [256, 512, 1024] 通道和 [down/up2d, crossattn, crossattn]，每块 2 层，仅在飞机类上进行训练。第二张图片是 [64, 128, 256] 和每块 1 层，在 10 个类上进行训练。 由于结果如此糟糕，我相信我所做的事情从根本上是错误的......这是我第一次从头开始训练 SD，所以我对原因没有非常有根据的猜测......如果有人对此有任何想法，将不胜感激 :) 非常感谢！！ https://preview.redd.it/y2wbvta1uxhd1.png?width=257&amp;format=png&amp;auto=webp&amp;s=16e4892b46452499363835029944f01c90b9588b https://preview.redd.it/1u5uigp1uxhd1.png?width=1006&amp;format=png&amp;auto=webp&amp;s=570e7e3f0f9ee23aa61f5183e659862173682793    提交人    /u/ImaginaryAd9209   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep8u6h/d_noisy_generation_when_traininig_stable/</guid>
      <pubDate>Sun, 11 Aug 2024 01:29:01 GMT</pubDate>
    </item>
    <item>
      <title>[P] 寻找梯度下降方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep5od0/p_looking_for_a_gradient_descent_approach/</link>
      <description><![CDATA[我有一个梯度下降方法的想法，它试图直接“跳”到附近最小值的（预测）位置。它的工作原理是围绕一个点近似 2-5 阶泰勒多项式，然后求解最小值（如果可能）并将该点设置为新的 x。然后，可以重复该过程。如果任何一点的泰勒多项式都是凹的，那么我们可以使用更标准的梯度下降方法。 这似乎是一种相当简单的方法，所以我怀疑它是否新颖，但我在网上找不到类似的东西。有谁知道这种方法叫什么或者是否已经研究过它？ 我受到了牛顿求根方法的启发，并且对超参数调整有点不屑一顾。 以下是 desmos 对二次和三次泰勒近似的演示： 二次下降：https://www.desmos.com/calculator/i2nsjaxzhy 三次下降：https://www.desmos.com/calculator/kgkbcfdn7t    提交人    /u/IgorTheMad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep5od0/p_looking_for_a_gradient_descent_approach/</guid>
      <pubDate>Sat, 10 Aug 2024 22:52:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] WildHallucinations：使用真实世界实体查询评估法学硕士 (LLM) 中的长篇事实性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep4tn3/r_wildhallucinations_evaluating_longform/</link>
      <description><![CDATA[一篇新论文旨在创建一个现实的基准 WildHallucinations，用于评估 LLM 事实性。    提交人    /u/AhmedMostafa16   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep4tn3/r_wildhallucinations_evaluating_longform/</guid>
      <pubDate>Sat, 10 Aug 2024 22:12:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 LSTM 建模动态系统</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ep3kra/d_modeling_a_dynamic_system_using_lstm/</link>
      <description><![CDATA[      亲爱的大家， 在观看了这段制作精良的视频后，我决定使用相同的概念对我的真实系统进行建模。基本上，我想对我的真实机器人的动态进行建模，以便创建相同的“数字孪生”。换句话说，我想在模拟器中重新创建具有虚拟物理属性的相同机器人，并像真实机器人一样移动它。 我的机器人使用操纵杆驱动，操纵杆在每个轴上输出一个介于 -1.0 和 1.0 之间的浮点数。我收集了数据（真正的机器人帽子传感器已经在工作并实施）。为简单起见，假设我想通过从左向右移动操纵杆轴来驱动以下关节坐标（图 1）。 图 1 我收集了一个小时的数据，然后使用以下数据训练了一个隐藏大小为 32 的 LSTM：  输入是操纵杆输入和关节坐标（机器人的状态）的连接 目标由下一步中机器人的状态表示。我只是复制了状态的列并将其向后移动一个单位。图 2 显示的可能比 1000 个单词更好。  图 2 然后我创建了长度为 200 的序列并训练了我的 LSTM。 训练收敛得非常快，我对结果非常满意。但不知何故，虚拟机器人在虚拟环境中反应奇怪。它以惊人的速度从一个位置跳到另一个位置，然后移动得非常慢。因此，它不会像真正的机器人那样做出反应（真正的机器人在运动过程中更平稳）。 在这种问题中我是否遗漏了重要的东西？ 为了创建真实机器人的良好数字孪生，我还应该考虑什么？ 请注意：  尽管有上述示例，但我将所有运动标准化为范围 [-1, 1] 或 [0, 1] 所有数据均使用以太网电缆收集（因此不会因无线通信等而造成延迟） 我使用了 PyTorch 的 LSTM 类，而不是自定义实现 通过生成具有不同频率的正弦波输入并覆盖关节的所有范围来收集数据。 对于训练，我对数据进行了打乱：随机选择一个起始索引，并剪切一个包含 200 个元素的序列并用于训练。     由   提交  /u/WilhelmRedemption   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ep3kra/d_modeling_a_dynamic_system_using_lstm/</guid>
      <pubDate>Sat, 10 Aug 2024 21:15:25 GMT</pubDate>
    </item>
    <item>
      <title>[R] 实现人类水平的竞技机器人乒乓球</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eozvwt/r_achieving_human_level_competitive_robot_table/</link>
      <description><![CDATA[  由    /u/RobbinDeBank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eozvwt/r_achieving_human_level_competitive_robot_table/</guid>
      <pubDate>Sat, 10 Aug 2024 18:27:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你的 neurips 讨论阶段进行得怎么样？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1eowx75/d_how_is_your_neurips_discussion_period_going/</link>
      <description><![CDATA[你的 neurips 讨论期进行得怎么样？ 有什么有趣的轶事吗？    提交人    /u/SuchOccasion457   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1eowx75/d_how_is_your_neurips_discussion_period_going/</guid>
      <pubDate>Sat, 10 Aug 2024 16:20:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1egc1um/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Wed, 31 Jul 2024 02:30:25 GMT</pubDate>
    </item>
    </channel>
</rss>