<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 10 Dec 2023 18:15:48 GMT</lastBuildDate>
    <item>
      <title>[D] 我的桌面上的人工智能——你的是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f9g88/d_ai_on_my_desktopwhats_yours/</link>
      <description><![CDATA[       由   提交 /u/Obvious-Double-8692   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f9g88/d_ai_on_my_desktopwhats_yours/</guid>
      <pubDate>Sun, 10 Dec 2023 18:09:43 GMT</pubDate>
    </item>
    <item>
      <title>"[D]","[R]"寻求指导：优化算法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f9f57/drseeking_guidance_optimizer_algorithem/</link>
      <description><![CDATA[我正在开展论文研究，为 Transformer 模型开发一种优化算法，以实现 2% 的收敛。我的基线优化器是 Adabelief，我正在探索在 Transformer 模型（特别是 ViT 模型）中使用视觉。我一直在努力确定参数值变化对训练过程的影响。我已经广泛阅读并与我的副主管讨论过，但我仍然陷入困境。任何有关如何解决此问题的指导或见解将不胜感激   由   提交 /u/OutrageousStorm5051   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f9f57/drseeking_guidance_optimizer_algorithem/</guid>
      <pubDate>Sun, 10 Dec 2023 18:08:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我对 Llama 进行了微调，为我的代码库生成系统图</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f6phk/p_i_finetuned_llama_to_generate_system_diagrams/</link>
      <description><![CDATA[       由   提交/u/jsonathan  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f6phk/p_i_finetuned_llama_to_generate_system_diagrams/</guid>
      <pubDate>Sun, 10 Dec 2023 16:03:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 什么样的离线 TTS 模型足以完成现实的实时任务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f5rii/d_what_offline_tts_model_is_good_enough_for_a/</link>
      <description><![CDATA[我搜索一个实时 TTS 以便在我的计算机上离线使用，最好是听起来逼真且可以免费使用的 AI 模型。对于 Windows。 我有一个 Geforce 2080 Super。   由   提交 /u/Imaginary-Ad-7671   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f5rii/d_what_offline_tts_model_is_good_enough_for_a/</guid>
      <pubDate>Sun, 10 Dec 2023 15:17:32 GMT</pubDate>
    </item>
    <item>
      <title>[R] 添加和细化：时间点过程的扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f53oy/r_add_and_thin_diffusion_for_temporal_point/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.01139代码：https://github。 com/davecasp/add-thin https://preview.redd.it/ydhlwfte9h5c1.jpg?width=2544&amp;format=pjpg&amp;auto=webp&amp;s=1c11a4a1f8cab2626e46546d589c35afb5e02fe a 生成扩散模型都是愤怒，但尚不清楚如何将它们应用于不同数量事件的序列，例如推文、Reddit 评论或出租车行程。我们提出了一种扩散方法，可以将任何序列转换为噪声序列，即来自齐次泊松过程的样本。相反，我们的模型通过删除分类为噪声的事件并从学习的（条件）强度分布中提出新事件，将此类噪声序列样本迭代地转换为来自任何目标数据分布的样本。通过实验，我们能够在各种基准数据集上展示出色的预测性能。 我期待在 reddit 上讨论我们的方法。如果您碰巧在 NeurIPS，您还可以在周二上午 10:45 的海报#602 与 /u/davidluedke 见面 -中午 12:45！   由   提交 /u/martenlienen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f53oy/r_add_and_thin_diffusion_for_temporal_point/</guid>
      <pubDate>Sun, 10 Dec 2023 14:45:39 GMT</pubDate>
    </item>
    <item>
      <title>[项目] Inside Marker：人工智能驱动的 PDF 布局检测引擎的引导源代码之旅</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f4fwi/project_inside_marker_a_guided_source_code_tour/</link>
      <description><![CDATA[       由   提交 /u/shrsv   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f4fwi/project_inside_marker_a_guided_source_code_tour/</guid>
      <pubDate>Sun, 10 Dec 2023 14:11:56 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 归一化流（如果有）相对于 GAN 和 VAE 的优势？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f2kzy/discussion_advantages_of_normalizing_flow_if_any/</link>
      <description><![CDATA[鉴于这最后一篇文章，我认为对这个主题进行更新的讨论可能会很有用。即，与其他方法（例如 GAN、VAE、带有 SDE 的贝叶斯神经网络）相比，标准化流有哪些实际优势今天？ 我对这个主题也很陌生。如果问题太幼稚请见谅。   由   提交/u/WorldML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f2kzy/discussion_advantages_of_normalizing_flow_if_any/</guid>
      <pubDate>Sun, 10 Dec 2023 12:29:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在像 Hugging Face 这样的库中，在增加复杂性的同时保持代码可读性和代码质量的好方法是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f2c2d/d_what_is_a_good_way_to_maintain_code_readability/</link>
      <description><![CDATA[HF 是使用基于变压器或扩散模型的好地方。他们拥有一组一致的 API，适用于各种模型。但如果你想调整一些东西或者只是看看引擎盖下的东西，事情就会很快走下坡路。一些例子：  diffusers 在改进 CompVis 的 LDM 实现和模块化不同组件方面做得很好，同时保持了 API 的一致性。但随着时间的推移，dreambooth、LoRA、LoHA、ControlNet、适配器等的引入是通过大量的猴子修补和代码扩展来完成的。  在 transformers 中，他们非常努力地尝试让单个函数或方法处理自注意力机制和交叉注意力机制、掩蔽、位置和相对编码虽然它允许用户对任何模型使用相同的函数/方法，但它导致了严重的参数膨胀。只需将 FAIR 的 llama 原始实现与 HF 的实现 来了解一下。   即使是像标记化或文本生成这样的简单想法也会有类似的代码膨胀。在这一点上，我会非常谨慎地尝试在 HF 存储库之上构建任何东西，甚至使用它来理解某些东西是如何实现的。 我只是想用这个批评来谈谈如何实现 构建优秀的 ML 相关开源软件。在牺牲几乎所有其他东西的同时，在广泛不同的模型中实现大规模一致性是一个好主意吗？有人已经做对了吗？我发现 FAIR 的代码是迄今为止最好、最简单的。   由   提交/u/nivter  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f2c2d/d_what_is_a_good_way_to_maintain_code_readability/</guid>
      <pubDate>Sun, 10 Dec 2023 12:14:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] LVE 项目：第一个 LLM 漏洞开源存储库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f24yg/p_lve_project_first_open_source_repository_of_llm/</link>
      <description><![CDATA[Github：https://github.com/ lve-org/lve/ 博客文章：https://lve -project.org/blog/launching-the-lve-project.html  大家好！我们最近公开宣布了我们的开源项目 LVE，其目标是以透明且可重现的方式跟踪和记录法学硕士的安全问题。该项目的范围是与隐私、安全、可靠性和其他形式的安全故障有关的问题。通过 LVE，我们希望帮助模型制作者、人工智能开发人员和公众更好地了解法学硕士的功能和漏洞。我们已经支持许多不同的模型变体，并且正在努力添加更多。  我们还在我们的网站上推出了一系列社区挑战，这是类似赏金的小游戏，每个人都可以参与其中并帮助我们攻击和红队LLM。 我们很高兴听到反馈，让人们参与该项目，或者只是让人们参与我们的社区挑战（https://lve-project.org /挑战/）。让我们知道您的想法！   由   提交/u/bmislav  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f24yg/p_lve_project_first_open_source_repository_of_llm/</guid>
      <pubDate>Sun, 10 Dec 2023 12:01:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 代码链：使用语言模型增强代码模拟器进行推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f1827/r_chain_of_code_reasoning_with_a_language/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.04474 OpenReview：https:// /openreview.net/forum?id=tlRUbI0Yf3 项目页面：https://chain-of-code.github.io/ 摘要：  代码提供了一个通用的与代码解释器配合使用时，可以使用语法结构来构建复杂的程序并执行精确的计算——我们假设语言模型（LM）可以利用代码编写来改进思想链推理，不仅适用于逻辑和算术任务，还适用于语义任务（特别是那些两者兼而有之的）。例如，考虑提示 LM 编写代码来计算它在文章中检测到讽刺的次数：LM 可能很难编写可由解释器执行的“Detect_sarcasm(string)”实现（处理边缘情况）将是不可克服的）。然而，如果语言模型不仅编写代码，而且通过生成“Detect_sarcasm(string)”的预期输出和其他无法执行的代码行来选择性地“模拟”解释器，它们仍然可能会产生有效的解决方案。在这项工作中，我们提出了代码链（CoC），这是一种简单但令人惊讶的有效扩展，可以改进 LM 代码驱动的推理。关键思想是鼓励 LM 将程序中的语义子任务格式化为灵活的伪代码，解释器可以显式捕获未定义的行为并转交给 LM 进行模拟（作为“LMulator”）。实验表明，在各种基准测试中，代码链优于思想链和其他基线；在 BIG-Bench Hard 上，Chain of Code 达到了 84%，比 Chain of Thought 提高了 12%。 CoC 可以很好地适应大型和小型模型，并扩大了 LM 可以通过“用代码思考”正确回答的推理问题的范围。   &amp; #32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f1827/r_chain_of_code_reasoning_with_a_language/</guid>
      <pubDate>Sun, 10 Dec 2023 11:02:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对使用 LSTM 进行时间序列预测（如包括天气预报）的质疑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18f12cj/d_doubts_on_the_implementation_of_lstms_for/</link>
      <description><![CDATA[我正在尝试使用 LSTM 预测未来的许多步骤。 我想到的第一个疑问是什么预测多个时间步长的正确方法。这就是我的想法： * 为模型的每次前向传递仅预测一个时间步，然后将最后的输出重新用于连续的输入。有点像自我反馈的无限循环。 * 预测具有形状的固定时间窗口（未来数据点、目标）。我通过向量化该矩阵并使用形状（未来数据点*目标）的密集层作为最后一层来完成此操作。最后用所需的尺寸重塑它。 这些方法有任何意义吗？最好的选择是什么？还有更好的吗？ 第二个问题是：如果我有一个历史天气和商店顾客数量的数据集，我如何构建一个模型，以最后一个作为输入N 对天气和 n_clients 也能够将未来几天的天气预报作为输入？ 换句话说，我不想仅使用历史数据（n_clients + 过去的数据）来预测未来的客户端天气），但以某种方式添加其他功能（天气预报）可能会提高模型预测的准确性。   由   提交 /u/IonizedRay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18f12cj/d_doubts_on_the_implementation_of_lstms_for/</guid>
      <pubDate>Sun, 10 Dec 2023 10:52:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] 机械手旋转番茄土豆</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eviwp/r_robot_hand_rotates_tomato_potato/</link>
      <description><![CDATA[       由   提交/u/XiaolongWang  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eviwp/r_robot_hand_rotates_tomato_potato/</guid>
      <pubDate>Sun, 10 Dec 2023 04:32:23 GMT</pubDate>
    </item>
    <item>
      <title>【研究】为什么**液体神经网络**在人工智能研究或实际应用中不像其他神经网络架构那样突出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18etkop/research_why_arent_liquid_neural_networks_not/</link>
      <description><![CDATA[ 由   提交 /u/sivav-r   /u/sivav-r  science.org/doi/10.1126/scirobotics.adc8892&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18etkop/research_why_arent_liquid_neural_networks_not/</guid>
      <pubDate>Sun, 10 Dec 2023 02:36:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 思想的一切：挑战彭罗斯三角定律的思想生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/18eq228/r_everything_of_thoughts_defying_the_law_of/</link>
      <description><![CDATA[   /u/Yogurt789   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/18eq228/r_everything_of_thoughts_defying_the_law_of/</guid>
      <pubDate>Sat, 09 Dec 2023 23:29:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/189wh8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Dec 2023 16:00:23 GMT</pubDate>
    </item>
    </channel>
</rss>