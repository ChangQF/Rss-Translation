<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 14 Sep 2024 21:13:54 GMT</lastBuildDate>
    <item>
      <title>[D] 我做错了什么？CNN 问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgvws9/d_what_am_i_doing_wrong_cnn_question/</link>
      <description><![CDATA[我已经创建了一个 CNN，使用以下数据集对鸟类物种进行分类。 尽管 CNN 在训练后具有 0.7540 的验证准确率，但在使用不同的图像和类别进行多次尝试后，它甚至无法正确预测单个图像。 这是 CNN 架构： 模型：“sequence_1” _________________________________________________________________ 层（类型）输出形状参数＃ ======================================================================= conv2d_5 (Conv2D) (None, 224, 224, 16) 448 max_pooling2d_5 (MaxPooling (None, 112, 112, 16) 0 2D) conv2d_6 (Conv2D) (None, 112, 112, 32) 4640 max_pooling2d_6 (MaxPooling (None, 56, 56, 32) 0 2D) conv2d_7 (Conv2D) (None, 56, 56, 64）18496 max_pooling2d_7（MaxPooling（无，28，28，64）0 2D）conv2d_8（Conv2D）（无，28，28，64）36928 max_pooling2d_8（MaxPooling（无，14，14，64）0 2D）conv2d_9（Conv2D）（无，14，14，64）36928 max_pooling2d_9（MaxPooling（无，7，7，64）0 2D）flatten_1（Flatten）（无，3136）0 density_2（Dense）（无，512）1606144 density_3（Dense）（无，100） 51300 ================================================================================================= 总参数：1,754,884 可训练参数：1,754,884 不可训练参数：0  由于这是一个研究项目，因此类别从 525 减少到 100，以加快速度。 这是我将图像转换为预测的方式： my_image = tf.keras.preprocessing.image.load_img(&#39;shoebill4.jpg&#39;, target_size=(224, 224)) my_image = tf.keras.preprocessing.image.img_to_array(my_image) my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2])) my_image = tf.keras.applications.vgg16.preprocess_input(my_image) prediction = model.predict(my_image) print(np.argmax(prediction)) 我认为问题一定出在图像转换上，但我已经尝试了很多转换和预测的解决方案，这是我尝试的最后一个。  我做错了什么？    由    /u/ianpbh  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgvws9/d_what_am_i_doing_wrong_cnn_question/</guid>
      <pubDate>Sat, 14 Sep 2024 21:04:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 音频分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgto6y/d_audio_classification/</link>
      <description><![CDATA[大家好！ 我需要对机械声音的录音进行分类，以确定机械装置是否存在故障（例如敲击、研磨、咔嗒声）或者机械装置是否正常运行。我还有大约 100 个音频文件需要标记和测试。 哪种模型最适合执行此任务？是否有可以微调的预训练模型？或者您推荐哪种方法？ 我已经尝试过以下方法：我为每个录音创建了频谱图，并微调了 YOLOv8 模型以检测偏差，但这并没有达到预期的准确度，可能是因为数据集太小了。 提前谢谢您！    提交人    /u/ARLEK1NO   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgto6y/d_audio_classification/</guid>
      <pubDate>Sat, 14 Sep 2024 19:18:42 GMT</pubDate>
    </item>
    <item>
      <title>[P] Diffumon - 一个简单的开源图像扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgoblp/p_diffumon_a_simple_open_source_image_diffusion/</link>
      <description><![CDATA[       由    /u/RogueStargun  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgoblp/p_diffumon_a_simple_open_source_image_diffusion/</guid>
      <pubDate>Sat, 14 Sep 2024 15:13:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Yolov5 有效损失问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgn7yo/d_yolov5_valid_loss_issue/</link>
      <description><![CDATA[      我正在开发安全带和手机检测系统，使用 YOLOv5s 检测挡风玻璃、驾驶员、乘客、安全带和手机。我的数据集存在类别不平衡问题，因为并非每个图像都包含安全带或手机，而手机类别的代表性尤其不足。 此外，手机很小，在图像中很难检测到。我注意到验证损失有一些波动，特别是在第 20 次之后开始增加，这让我怀疑存在过度拟合。 这是我的代码，我使用的是 Ultralytics 的预训练模型： model.train( data=&quot;full_dataset/data/data.yml&quot;, imgsz=640, epochs=100, batch=16, worker=4, project=&quot;SeatBeltMobileDetection&quot;, name=&quot;YOLOv5s_640_epochs100&quot;, device=0 ) 问题：  考虑到类别不平衡（特别是在手机检测方面），验证损失的波动和 DFL 损失的增加是否意味着过度拟合？ 微调的最佳实践是什么YOLOv5 在这种类别不平衡的情况下的表现如何？调整类别权重等技术有帮助吗（我之前做过过采样和增强）？ 我应该考虑对 YOLOv5 训练超参数进行哪些具体调整，以提高对手机等小物体的性能？     提交人    /u/Fantastic_Almond26   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgn7yo/d_yolov5_valid_loss_issue/</guid>
      <pubDate>Sat, 14 Sep 2024 14:23:52 GMT</pubDate>
    </item>
    <item>
      <title>[P] 尝试重现 OpenAI 的 o1 推理能力 - 寻找志愿者</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgmllq/p_trying_to_reproduce_openais_o1_reasoning/</link>
      <description><![CDATA[我和我的团队目前正在尝试重现 o1 系列推理能力。但是，我们需要社区的一点帮助来获取更多数据。我们计划以 OpenAI 的两篇论文为基础开展研究：让我们一步一步验证 (https://arxiv.org/pdf/2305.20050) 和证明者-验证者游戏提高了 LLM 输出的可读性 (https://arxiv.org/pdf/2407.13692)。我们可能还会在我们的方法中使用某种类型的树搜索。由于我们的团队规模很小，任何帮助都将非常有益，尤其是在获取数学、推理和代码思维链数据方面，其中采取的步骤被归类为“正确”、“中立”或“不正确”。如果您有兴趣帮助我们，请在此帖子下发表评论或在 reddit 或 discord (danfosing) 上给我留言。    提交人    /u/DanFosing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgmllq/p_trying_to_reproduce_openais_o1_reasoning/</guid>
      <pubDate>Sat, 14 Sep 2024 13:54:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么大多数联邦学习方法如此依赖超参数？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgjzlt/d_why_are_most_federated_learning_methods_so/</link>
      <description><![CDATA[我从事 FL 研究已有一段时间了，并且涉猎了几个子领域。每当我开始一个新项目并对现有方法进行一些基准测试时，总是需要花很长时间才能让这些方法在原始论文中未使用的标准数据集（如 cifar10）上工作。目前，我正在使用一个预制的基准测试工具（fl-bench），并且仍然很难让 fedavg 在 cifar10 上收敛到甚至稍微非 i.i.d. 数据集。在我看来，这让在该领域工作变得非常令人沮丧。你有没有类似的经历，或者这段时间我一直错过了什么基本的东西？    提交人    /u/NumerousSwordfish653   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgjzlt/d_why_are_most_federated_learning_methods_so/</guid>
      <pubDate>Sat, 14 Sep 2024 11:29:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何在 Lakehouse 数据上构建 AI 系统？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fgezaj/d_how_do_you_build_ai_systems_on_lakehouse_data/</link>
      <description><![CDATA[“[lakehouse] 将成为未来十年的 OLAP DBMS 原型。” [Stonebraker] 大多数用于分析的企业数据最终将以开放表格格式（Iceberg、Delta、Hudi 表）的对象存储 - 带有元数据的 parquet 文件。我们希望将这些数据用于 AI - 用于训练和推理。对于所有类型的 AI 系统 - 批处理、实时和 LLM。但 Lakehouse 架构缺乏 AI 功能。 字节跳动 (Tiktok) 拥有 1 PB 的 Iceberg Lakehouse，但他们必须构建自己的实时基础设施才能为 Tiktok 的个性化推荐服务（双塔嵌入）提供实时 AI。 Python 也是 Lakehouse 中的二等公民 - Netflix 使用 Arrow 构建了一个 Python 查询引擎来提高开发人员的迭代速度。 LLM 尚未与 Laekhouse 连接。 如何在 Lakehouse 数据上进行训练/推理？ 参考文献： * https://www.hopsworks.ai/post/the-ai-lakehouse * https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf * https://dl.acm.org/doi/10.1145/3626246.3653389    由   提交  /u/jpdowlin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fgezaj/d_how_do_you_build_ai_systems_on_lakehouse_data/</guid>
      <pubDate>Sat, 14 Sep 2024 05:24:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 提高 Whisper/STT 在具有挑战性的音频上的表现的策略</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fg8qtb/d_strategies_for_improving_whisperstt_performance/</link>
      <description><![CDATA[我正在做一个项目，涉及转录来自各种来源的音频，包括低质量录音和有背景噪音的音频。虽然 Whisper 总体上令人印象深刻，但我正在寻找进一步提高转录准确性的方法，尤其是对于更具挑战性的音频输入。一个大问题是我收到了很多“谢谢”以及转录中的此类内容。 我正在考虑的一些方法：  根据领域特定数据对 Whisper 进行微调 预处理音频（降噪、标准化等） 结合多种 STT 模型的集成方法 使用 LLM 对转录本进行后处理  我很想听听其他致力于优化 STT 管道的人的意见：  您发现哪些技术对提高准确性最有效？ 有没有一些不太常见的方法效果很好？ 您如何处理非常嘈杂或低质量的音频输入？ 有任何关于评估和基准测试 STT 改进的提示吗？  提前感谢您的任何见解！我正在这个领域开展一个开源项目（如果有兴趣，请访问 https://github.com/mediar-ai/screenpipe），但主要想学习这里社区的经验。    提交人    /u/louis3195   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fg8qtb/d_strategies_for_improving_whisperstt_performance/</guid>
      <pubDate>Fri, 13 Sep 2024 23:33:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 语言模型中的因果理解框架方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fg7kvj/r_approach_of_a_causal_understanding_framework_in/</link>
      <description><![CDATA[我开发了一个框架，想与大家分享，特别是因为我发现决策和迭代的过程非常迷人。它基于结构化问题解决和因果分析，目的是找到完美的解决方案。 项目：https://github.com/stevius10/ReasoningModel 框架：https://github.com/stevius10/ReasoningModel/blob/main/reasoning_model.json 当然，这不是“完美”的解决方案（第二好），而是完美的解决方案。我会等待评论中第一个提出质疑的人。 😉 这个框架的核心是什么？这个框架提供了一种结构化的方法，用于引导高级语言模型（如 ChatGPT）超越仅仅模仿人类交流。这个框架不是只专注于复制类似人类的措辞，而是使模型能够利用其庞大的训练数据从语言的更深层结构中提取因果见解。 它提供了一种方法来区分驱动决策的基本因果信息和可能掩盖这些潜在动态的明确语言模式。通过应用这个框架，模型可以参与迭代学习和自我反思的过程，不断完善对这些更深层因果机制的理解，最终随着时间的推移产生更精确、更符合语境的结果。 如果您好奇，请随意尝试一下：输入一个问题，点击“继续”几次，然后观察答案如何演变。这个过程可能会让你大吃一惊——或者开辟一个全新的视角。 附注：对于那些更喜欢记忆而不是光学的人来说，你可以以结构化数据格式获得输出。该模型“复制”自身并随着时间的推移管理知识。换句话说：记忆和复杂关联的关键是结构——字面上。    提交人    /u/stevius10   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fg7kvj/r_approach_of_a_causal_understanding_framework_in/</guid>
      <pubDate>Fri, 13 Sep 2024 22:38:27 GMT</pubDate>
    </item>
    <item>
      <title>[P] 尝试复制“Stretch Each Dollar”扩散论文，遇到问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ffz5xc/p_attempting_to_replicate_the_stretching_each/</link>
      <description><![CDATA[      编辑：我发现了错误！ 我专注于确保掩蔽内容正确，它是，但是我没有看到，在我取消屏蔽补丁（即用 0 替换主干错过的补丁）之后，我将它们重塑为原始形状，在此期间我将它们传递通过 FFN 输出层，该层不是线性的，因此 0 输入！= 0 输出。但损失函数在那些地方预期 0 输出。所以我需要做的就是再次将这些位设为 0，现在它工作得更好了  我正在尝试复制这篇论文：https://arxiv.org/pdf/2407.15811 您可以在此处查看我的代码：https://github.com/SwayStar123/microdiffusion/blob/main/microdiffusion.ipynb 我首先对 9 张图像进行过度拟合以确保健全性，但在较低的掩蔽率下，我无法复制论文中的结果 在掩蔽率为 1.0 时，即所有补丁都被 Transformer 主干网看到，它对 9 张图像的过度拟合非常好 https://preview.redd.it/thteqn3rhlod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=215fc88c74728f5f0dcfbd05a9b6a4db836b1f84 存在一些轻微的扭曲，但也许一些 LR 调度会有所帮助，主要问题是当掩蔽率降低到 0.75 时，输出严重下降： https://preview.redd.it/ukcexjbyhlod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=432187000cde0ba5b1b90813c6284c9f764a9979 在掩蔽比为 0.5 时，情况甚至更糟： https://preview.redd.it/00kzbpc0ilod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=0717f8926582b2b5c694ebe5609b6f9fba8a088d 所有这些都经过相同步数的训练，除了掩蔽率之外，所有超参数都是相同的 注意：我使用“掩蔽率”表示 Transformer 主干看到的补丁百分比，与论文中隐藏的补丁百分比相反。我几乎可以肯定这不是问题 我也使用 x 预测目标而不是噪声预测，就像论文中一样，但这并不重要，而且它可以在 1.0 掩蔽率下工作。 增加补丁混合层的数量没有帮助，如果有的话，它会使情况变得更糟 2 个补丁混合层，0.5 掩蔽率： https://preview.redd.it/punkf59uilod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=04e0ea03d9ecd464f1bd007f7957c4a65c0ae9c2 4 个补丁混合层，0.5 遮罩比： https://preview.redd.it/9ihtiyvejlod1.png?width=1066&amp;format=png&amp;auto=webp&amp;s=c78efb039b6ef630a3760dca60e2018d32e6c3b7 也许补丁混合器本身有问题？使用 TransformerEncoderLayer 作为补丁混合器是不是一个坏主意？    提交者    /u/SwayStar123   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ffz5xc/p_attempting_to_replicate_the_stretching_each/</guid>
      <pubDate>Fri, 13 Sep 2024 16:36:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 根据数据冗余优化下一帧预测任务的计算成本。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ffxi3w/d_optimising_computational_cost_based_on_data/</link>
      <description><![CDATA[假设我有一个生成网络，负责预测视频的下一帧。一种解决方法是，在前向传递中，简单地传递当前帧并请求下一帧 — 可能取决于某些操作（如 GameNGen）。在这种方法中，所有帧的计算成本相同 — 严重限制了我们可以操作的帧速率。然而，在更高的帧速率下，帧之间的变化要小得多 — 平均而言，在 60 fps 下，下一帧明显更接近前一帧（因此我认为更容易预测） — 而不是在 10 fps 下进行预测。这让我想到了一个问题，如果我有一个以预测编码方式运行的网络 — 它试图预测下一帧并将结果预测误差作为前馈输入。在更高的帧速率下，要处理的误差逐帧更小 — 但张量形状将与图像的形状相同。当我的错误较小时，什么样的方法可以让我的计算效率更高？直觉是“如果你的预测正确，你不应该偏离你当前建模的轨迹太多 - 如果你的预测误差很大，我们需要进行更广泛的计算。”    提交人    /u/Karioth1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ffxi3w/d_optimising_computational_cost_based_on_data/</guid>
      <pubDate>Fri, 13 Sep 2024 15:26:32 GMT</pubDate>
    </item>
    <item>
      <title>[P] 监控视频摘要器：基于 VLM 的视频分析和摘要</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ffsqvb/p_surveillance_video_summarizer_vlmpowered_video/</link>
      <description><![CDATA[大家好！ 我一直在研究一个 VLM 驱动的系统，该系统处理监控视频、提取帧并生成详细注释以突出显示值得注意的事件、动作和对象。此应用由经过微调的 Florence-2 视觉语言模型 (VLM) 提供支持，我专门针对 SPHAR 数据集对其进行了训练。并且，它利用 OpenAI API 来总结和提取最相关的内容，确保全面、连贯地概述监控录像。 链接： 📺 查看我们的演示视频以了解实际效果！ 📂 这是 GitHub 存储库，其中包含所有详细信息。  **📣 工作原理：** * **帧提取**：使用 OpenCV 定期从视频文件中提取帧。 * **AI 注释**：每个帧都由经过微调的 Florence-2 模型分析，生成场景的精确注释。 * **数据存储**：注释和帧数据存储在 SQLite 数据库中，方便检索和未来分析。 * **Gradio 支持的界面**：通过基于 Gradio 的 Web 界面轻松与系统交互。通过指定时间范围，您可以检索带有全面分析的详细日志。该界面利用 OpenAI API 来总结视频内容，通过分析帧序列来确保时间连贯性，从而可以更加从情境上理解镜头中捕获的事件。 可用的微调模型：https://huggingface.co/kndrvitja/florence-SPHAR-finetune-2    提交人    /u/BriefAd4761   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ffsqvb/p_surveillance_video_summarizer_vlmpowered_video/</guid>
      <pubDate>Fri, 13 Sep 2024 11:49:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习用于药物研发是一条好途径吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ffqy6y/d_ml_for_drug_discovery_a_good_path/</link>
      <description><![CDATA[我现在看到很多初创公司（大大小小的）都专注于将机器学习用于药物发现/将机器学习用于生物应用，并且想知道应用机器学习研究在这个领域的范围。   是否存在需要机器学习研究来解决的成熟问题陈述，它们是什么（我当然熟悉 Alpha 折叠/蛋白质折叠工作，但考虑到这个问题已经解决了，是否还有其他活跃的研究领域） 这些问题陈述是否仅限于研究实验室（虽然是扎实的研究，但它们的特定用例很狭窄），还是它们解决了行业范围  考虑到医疗保健领域的监管要求，a）是否有现成的数据，以及 b）这些问题的解决方案是否真的可以投入生产/成为产品？   我目前从事一般应用机器学习研究（具有 CV/NLP/多模态）经验，并且正在考虑是否投资转型到药物发现领域，因为我过去在医疗保健领域确实有经验。我在大型制药公司中看到过许多类似的职位，他们正在探索人工智能，但通常这些类型的公司缺乏坚实的人工智能技术领导力，最终基于现有的开源工具构建 POC 解决方案。我很想听听在药物发现问题方面拥有深厚技术专长的人工智能优先公司或研究实验室的人们的意见。     提交人    /u/panther-banter   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ffqy6y/d_ml_for_drug_discovery_a_good_path/</guid>
      <pubDate>Fri, 13 Sep 2024 09:59:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1fbzs8y/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1fbzs8y/d_simple_questions_thread/</guid>
      <pubDate>Sun, 08 Sep 2024 15:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1f5cy0v/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Sat, 31 Aug 2024 02:30:15 GMT</pubDate>
    </item>
    </channel>
</rss>