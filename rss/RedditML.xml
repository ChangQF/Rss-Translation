<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Mon, 14 Oct 2024 03:25:18 GMT</lastBuildDate>
    <item>
      <title>[R] DIAMOND：世界建模的扩散</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g34p3n/r_diamond_diffusion_for_world_modeling/</link>
      <description><![CDATA[DIAMOND 💎 世界建模的扩散：Atari 中的视觉细节很重要 项目网页：https://diamond-wm.github.io/ 代码、代理和可玩世界模型：https://github.com/eloialonso/diamond 论文：https://arxiv.org/pdf/2405.12399 摘要  RL 代理是由 REINFORCE 训练的演员-评论家。  除最后一层外，演员和评论家网络共享权重。这些共享层由一个卷积“主干”和一个 LSTM 单元组成。卷积主干有四个带有 2x2 最大池化的残差块。 每次训练运行需要 500 万帧，在一台 Nvidia RTX 4090 上持续 12 天。  世界模型是一个带有 U-Net 2D 的 2D 扩散模型。它不是潜在扩散模型。它直接从视频游戏中生成帧。 该模型将最后 4 帧和动作以及扩散噪声水平作为条件。 在 RTX 3090 上以 ~10 FPS 运行。 他们使用 EDM 采样器从扩散模型中采样，即使每帧只有 1 个扩散步骤，它仍然可以很好地训练 RL 代理。     由    /u/furrypony2718  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g34p3n/r_diamond_diffusion_for_world_modeling/</guid>
      <pubDate>Mon, 14 Oct 2024 01:11:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助 NASA 资助的项目更多地了解太阳！（Kaggle 竞赛）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</link>
      <description><![CDATA[大家好，我叫 Hannah，我是 NASA 资助的 Eclipse Megamovie 2024 项目的通讯员。 4 月份日全食临近时，我们非常活跃，但还有更多令人兴奋的事情等着我们！我们发起了 Kaggle 竞赛，希望得到像这样的社区的帮助。以下是有关整个项目的更多信息以及我们的竞赛页面的链接。请随时提问，我会尽力回答！ 2024 年 4 月 8 日，日全食始于南太平洋，横跨北美洲，途经墨西哥、美国和加拿大。北美大陆第一个经历日全食的地点是墨西哥太平洋海岸，时间大约是太平洋夏令时间上午 11:07。 2024 年 4 月 8 日日全食之后，超过 145 名志愿者上传了超过 1 TB 的照片数据，供我们的项目使用。 Eclipse Megamovie 2024 (EM2024) 由 NASA 资助，旨在利用日全食期间收集的数据研究太阳，这是一个特殊的时期，可以研究太阳的行为，与其他任何时候都不同。日食和数据收集之后的下一个阶段是对照片数据进行分类和标记，然后我们就可以开始认真进行科学分析——这就是你发挥作用的地方！ 如果您精通 Python 代码和机器学习，您可能能够为解答有关太阳的以前未解答的问题做出贡献！  比赛页面链接：https://www.kaggle.com/competitions/eclipse-megamovie 比赛参与者将使用我们的 2017 年日全食数据集来“训练”机器，方法是编写代码并利用提供的训练数据集自动根据日食阶段将日食照片归类为几个类别之一。建议有兴趣参加本次比赛的人具备 Python 和机器学习基础知识。 与我们的竞赛一致的兴趣：摄影、太阳物理学和/或太阳科学研究、参与式科学和机器学习。奖品： 排行榜奖品：根据私人排行榜排名颁发。  一等奖：带太阳滤镜的图像稳定双筒望远镜、Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、一等奖证书。 二等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、二等奖证书。 三等奖：Eclipse Megamovie 网站上的 Spotlight、Eclipse Megamovie 团队徽章、NASA 日历、Eclipse Megamovie 贴纸、三等奖证书。  参与者将帮助确保数据 [日食照片] 能够快速组织，并且每张图片都具有正确的信息（元数据）。通过帮助我们开发能够准确识别志愿者提交的照片中的日食阶段的代码，您将帮助我们跨越一个重大的数据处理障碍。通过您的代码，您将为这项由 NASA 资助的研究太阳喷流和等离子羽流铺平道路！ 您的任务是创建最准确的分类机，将日食照片分类到特定的日食阶段。如果您的代码能够成功地将提供的照片分类为以下类别，您就知道自己成功了：暗色或平面（校准镜头）、日偏食阶段（20 度的箱 [类别]）、钻石环阶段、日全食阶段，当然还有非日食类别。 特别感谢 Mods 让我知道在这篇文章中使用哪个标签 :) 已编辑文字    提交人    /u/EMegamovie2024   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g32evj/d_help_a_nasafunded_project_learn_more_about_the/</guid>
      <pubDate>Sun, 13 Oct 2024 23:11:39 GMT</pubDate>
    </item>
    <item>
      <title>[P] 被研究论文淹没了？🐸</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</link>
      <description><![CDATA[      我们是两名对人工智能研究感兴趣的工程师，但却被 arXiv 上大量新论文淹没。因此，我们开发了 Ribbit Ribbit，一款研究论文发现工具。  https://apps.apple.com/us/app/ribbit-ribbit/id6529547956 https://ribbitribbit.co  它会整理个性化的论文推荐，并将其转化为推文大小的摘要，这样您就可以像在 Twitter 上一样滚动浏览。您还可以像为您量身定制的播客一样收听更新。我们添加了一点轻松的体验，希望它能为整个纸质阅读过程增添一丝乐趣，说实话，这个过程可能会变得相当枯燥乏味 :p。 https://preview.redd.it/evoemobinlud1.png?width=1179&amp;format=png&amp;auto=webp&amp;s=4dff5b2b60f2a1272b6ac04347f661ceacff2aa5    提交人    /u/haoyuan8   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g31hfd/p_drowning_in_research_papers/</guid>
      <pubDate>Sun, 13 Oct 2024 22:24:26 GMT</pubDate>
    </item>
    <item>
      <title>对因果机制感兴趣“[D]”</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2zcwz/interested_in_causal_mechanism_d/</link>
      <description><![CDATA[我对因果机制的表示及其在人工智能中的应用很感兴趣。我计划在认知神经学之后申请另一个研究生学位，无论是计算机科学（转换）还是心理学相关领域（研究方法）。哪个课程与我上面感兴趣的主题计算机科学或心理学更相关？    提交人    /u/Pleasant-Neck-1528   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2zcwz/interested_in_causal_mechanism_d/</guid>
      <pubDate>Sun, 13 Oct 2024 20:43:55 GMT</pubDate>
    </item>
    <item>
      <title>对因果机制感兴趣[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2z95b/interested_in_causal_mechanism_d/</link>
      <description><![CDATA[我对因果机制的表示及其在人工智能中的应用很感兴趣。我计划在认知神经学之后申请另一个研究生学位，无论是计算机科学（转换）还是心理学相关领域（研究方法）。哪个课程与我上面感兴趣的主题计算机科学或心理学更相关？    提交人    /u/Pleasant-Neck-1528   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2z95b/interested_in_causal_mechanism_d/</guid>
      <pubDate>Sun, 13 Oct 2024 20:39:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过用这种方法验证 LLM 生成的 QA 数据集吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2xv4v/d_has_anyone_tried_this_method_for_validating/</link>
      <description><![CDATA[我正在为一家公司进行 LLM 评估。在一个项目中，我们使用 LLM 创建 QA 数据集，需要对其进行验证，然后才能将其用作模型评估的可靠标记。 LLM 验证数据集显然是有风险的，人工验证既费时又费钱。 鉴于我们必须将源文档切碎以适应 LLM 的上下文窗口来创建 QA 数据集，是否有人尝试过通过仅将相同的块输入模型来回答问题来进行 LLM 答案验证？这意味着模型要做的就是“大海捞针”这看起来是一个明显更简单的任务（https://github.com/gkamradt/LLMTest_NeedleInAHaystack），而 LLM 必须知道这些信息。    提交人    /u/freshprinceofuk   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2xv4v/d_has_anyone_tried_this_method_for_validating/</guid>
      <pubDate>Sun, 13 Oct 2024 19:37:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 算子学习中有边界条件吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2wx4u/d_any_boundary_conditions_in_operator_learning/</link>
      <description><![CDATA[无论是 FNO、DeepONets 还是任何其他算子学习框架，是否有任何关于边界条件算子学习的研究？理想情况下是 Neumann？    提交人    /u/Mafisch   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2wx4u/d_any_boundary_conditions_in_operator_learning/</guid>
      <pubDate>Sun, 13 Oct 2024 18:56:46 GMT</pubDate>
    </item>
    <item>
      <title>[P] 寻找单词到单词的文本检测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2uemm/p_looking_for_a_word_to_word_text_detection/</link>
      <description><![CDATA[您好，首先，如果我的英语水平不够，我深表歉意。我目前正在寻找一个易于训练或整合到 Python 中的单词到单词文本检测模型。我查看了很多不同的存储库，并尝试实现它们，但多次失败。我不得不承认，我不是最聪明的人，而且知道的也有限。 作为一名计算机科学专业的学生，​​我绝不是这个领域的专家，我感到很惭愧，但是我已经尝试了几种端到端模型，例如 pytesseract、easyocr、paddleocr，或计算机视觉技术，例如带有 NMS 的 MSER，以便能够抓取纸张的边界框。唯一的问题是这些并不完美，甚至最好的也是 pytesseract。它捕获的准确率约为 80% 到 85%，而其余部分则相交或根本没有被捕获。  我计划构建一个用于识别手写文本的 OCR 系统。我已经训练了一个表现良好的识别模型，并且只在获取文本的正确边界框时遇到问题。最后，我想知道 YOLO 模型是否值得现在开始研究以获取边界框并获取文本的分类。我需要在 1 周内得到它。    提交人    /u/brudda65   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2uemm/p_looking_for_a_word_to_word_text_detection/</guid>
      <pubDate>Sun, 13 Oct 2024 17:05:54 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongCite：使 LLM 能够在长上下文 QA 中生成细粒度引用</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/</link>
      <description><![CDATA[      我刚刚阅读了一篇有趣的论文，旨在解决（或改进）信息检索问题细粒度引用，“LongCite：使 LLM 能够在长上下文 QA 中生成细粒度引用”(https://arxiv.org/abs/2409.02897)。 在本文中，研究人员使用现成的 LLM 生成由具有精确句子级引用的长上下文 QA 实例组成的数据集，然后使用该数据集微调开放权重 LLM 以生成带有引用的答案。与 GPT4o、Llama 3.1 等相比，生成的 LongCite 8B 和 9B 模型出奇地好。 这是如何工作的？以下是生成指令微调数据集的 4 步过程： (a) 从长文本或文档开始，他们的方法使用现有的 LLM 使用 Self-Instruct 生成问答数据集（查询及其相关答案）（Wang 等人 2023；在我之前的一篇文章中讨论过）。 (b) 接下来，他们使用答案从输入文本中检索几个 128 个标记的块以进行粗粒度引用。 (c) 然后，LLM 在这些块中寻找相关句子，以提供更细粒度的句子级引用 (d) 研究人员过滤掉答案中不到 20% 的陈述没有引用的问答对 然后使用生成的数据集以传统（SFT）方式训练 LLM。 https://preview.redd.it/gucywp0o8jud1.jpg?width=6000&amp;format=pjpg&amp;auto=webp&amp;s=4ed2ff078327082133d390c566250a2ef41c1a05    提交人    /u/seraschka   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2qohf/r_longcite_enabling_llms_to_generate_finegrained/</guid>
      <pubDate>Sun, 13 Oct 2024 14:19:13 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻求识别不同语言中语义等价句子或实体的研究方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2mvpc/r_seeking_research_approaches_for_identifying/</link>
      <description><![CDATA[如何从研究和学术角度识别两种不同语言中具有相同语义含义的等效句子或实体？我正在寻找用于在多语言环境中查找语义等效对（无论它们是句子还是其他实体）的方法和方法。您能否建议我应该使用的相关研究领域、方法或特定关键字来搜索涉及跨语言语义相似性、句子对齐或跨语言实体等价性的学术论文？任何有关此上下文中常用的工具、模型或算法的指导也将不胜感激。    提交人    /u/eyup_kh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2mvpc/r_seeking_research_approaches_for_identifying/</guid>
      <pubDate>Sun, 13 Oct 2024 10:41:31 GMT</pubDate>
    </item>
    <item>
      <title>[D] 获得博士学位的现实性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/</link>
      <description><![CDATA[大家好！我是伦敦大学学院的研究生，正在攻读机器学习硕士学位，我很快将申请 2025 年秋季开始的博士课程。我将分享我的个人资料和我将申请的学校，并希望了解我所瞄准的实验室是否超出了我的能力范围。 我以一等（荣誉）成绩获得了新加坡南洋理工大学的数学和计算机科学本科学位，预计也将以一等（荣誉）成绩获得研究生学位。我对理论深度学习感兴趣——围绕损失曲面曲率、优化轨迹、学习动态和泛化的问题——这些都是数学密集型的研究领域。虽然我的课程大部分都是理论性的，并且与此类研究非常一致（按设计），但我的研究经历更具实验性。我在 ICML 上发表了一篇第三作者出版物，内容是我为学士论文项目所做的工作。这是一项相当理论化的工作，但我只负责实验。我还有 2 篇第一作者预印本——一篇关于 NLP 的实验性工作（旨在在 IEEE 上发表），另一篇关于图形 ML（旨在在顶级会议之一上发表），其中有相当多的理论部分，但没有我希望在博士学位上完成的工作那么多。 我的目标是进入 ETH、UCL、斯坦福、NYU、EPFL、哥伦比亚和普林斯顿的实验室（按优先顺序，其中一个是我的职位）。所有这些实验室都有非常成功的 PI（按引用次数计算），他们研究的主题与我的兴趣非常一致。我担心我看似无所不包的研究背景可能会让他们失望，但我希望我的成绩能让他们相信我精通理论。我希望我的导师能写出优秀的推荐信，因为他们在很多场合都对我表示赞赏。我希望写一份令人信服的研究陈述，但由于我几周前才开始阅读相关文献，所以最终可能不是那么完美。 我不介意与年轻的 PI 合作，只要我身边有一些研究人员在研究相关主题。在高级实验室，已经建立了一个网络，我可能先协助一些项目，然后再进行独立研究。现实地说，我是不是在自吹自擂？如果是这样，有人可以推荐一些年轻的 PI 从事上述研究课题，我可能更有机会加入他们的实验室吗？    提交人    /u/mio_11   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2mugf/d_realism_of_landing_a_phd_offer/</guid>
      <pubDate>Sun, 13 Oct 2024 10:38:56 GMT</pubDate>
    </item>
    <item>
      <title>提出新颖的想法[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/</link>
      <description><![CDATA[关于如何想出新颖的问题解决方案，您有什么想法吗？每次我以为自己有了一些想法，我的导师就会说“这太简单了”。许多方法以独特的方式将现有的构建块粘合在一起，但我很难想象人们如何想出既真正新颖又真正有效的东西。 有时，我读到一篇论文，我意识到这个想法实际上非常简单/直接，作者只是介绍了一个很酷的技巧。其他时候，我读到的东西介绍了一个非常晦涩的定理，或者他们注意到一些我只能梦想的东西。我倾向于前者，但由于新颖性有限，我对迄今为止所写的任何东西都不太自豪。疯狂的出版速度让我偏向“简单而有效”，这无济于事方法中的大部分工作是在获得 SOTA 后事后编写故事。    提交人    /u/like_a_tensor   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2jhhx/coming_up_with_novel_ideas_d/</guid>
      <pubDate>Sun, 13 Oct 2024 06:25:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 信仰与命运：Transformers 作为模糊模式匹配器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2irug/d_faith_and_fate_transformers_as_fuzzy_pattern/</link>
      <description><![CDATA[       由    /u/jsonathan  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2irug/d_faith_and_fate_transformers_as_fuzzy_pattern/</guid>
      <pubDate>Sun, 13 Oct 2024 05:33:44 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1g2fmfw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 13 Oct 2024 02:15:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>