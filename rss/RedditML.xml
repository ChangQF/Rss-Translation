<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Wed, 08 Jan 2025 18:22:58 GMT</lastBuildDate>
    <item>
      <title>[D][R] 今年您计划参加哪些会议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwpbnh/dr_what_conferences_are_on_your_list_this_year/</link>
      <description><![CDATA[今年你计划参加哪些会议？我的计算机视觉/机器学习列表包括：  Nvidia GTC - 3 月 17-24 日，加利福尼亚州圣何塞 CVPR，6 月 11-15 日，田纳西州纳什维尔 ICCV，10 月 20-24 日，夏威夷檀香山 Supercompute 25，11 月 16-21 日，密苏里州圣路易斯 Neuroips，12 月 9-15 日，加利福尼亚州圣地亚哥  你有什么计划？    提交人    /u/MLisdabomb   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwpbnh/dr_what_conferences_are_on_your_list_this_year/</guid>
      <pubDate>Wed, 08 Jan 2025 17:14:09 GMT</pubDate>
    </item>
    <item>
      <title>实时 TTS 模型如何工作？[讨论]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwmmze/how_do_the_real_time_tts_models_work_discussion/</link>
      <description><![CDATA[我想知道实时文本转语音程序使用什么模型，或者它只是一个非常快速的输入模型和输出模型组合在一起。     提交人    /u/codeblockzz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwmmze/how_do_the_real_time_tts_models_work_discussion/</guid>
      <pubDate>Wed, 08 Jan 2025 15:23:28 GMT</pubDate>
    </item>
    <item>
      <title>[R][D] 哪些论文对于您的研究领域具有最重要的意义？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwkmwm/rd_what_are_the_most_important_papers_that/</link>
      <description><![CDATA[请提及您在研究机器学习的哪个领域（细分市场）工作？ 您为什么选择该特定领域？ 如果对机器学习和深度学习有基本了解的人想要参与您的领域，他们应该考虑阅读/实施哪些论文/博客/工具？    提交人    /u/HopeIsGold   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwkmwm/rd_what_are_the_most_important_papers_that/</guid>
      <pubDate>Wed, 08 Jan 2025 13:48:49 GMT</pubDate>
    </item>
    <item>
      <title>[D] 致各位研究员：您在研究中面临的最大三大挑战是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwh8um/d_to_fellow_researchers_what_are_your_top_3/</link>
      <description><![CDATA[作为研究人员，我们在研究过程中都会遇到各种障碍。您最常遇到的三大挑战是什么？您对改善这些方面有什么建议吗？ 您的挑战可能包括：  找到问题陈述或细化您的研究问题 访问资源、数据集或工具 有效管理时间或克服管理任务 撰写、修改和发表论文 与他人合作或寻找研究助理  我们很乐意听听您的经历！如果可能的话，请分享一个轶事或具体的例子，说明一个占用你大部分时间但可以简化以提高效率的问题。 我们是一支由年轻研究人员组成的团队，致力于建立一个开放的社区和 FOSS AI 工具（具有“自带密钥”功能），以简化端到端研究流程。您的意见将帮助我们更好地理解和解决这些痛点。    提交人    /u/Correct_Sector8318   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwh8um/d_to_fellow_researchers_what_are_your_top_3/</guid>
      <pubDate>Wed, 08 Jan 2025 10:38:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] LongBench v2：实现对现实长上下文多任务的更深入理解和推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwfs48/r_longbench_v2_towards_deeper_understanding_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwfs48/r_longbench_v2_towards_deeper_understanding_and/</guid>
      <pubDate>Wed, 08 Jan 2025 09:01:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 有人尝试过 predibase/lorax 吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwdw40/d_anyone_tried_predibaselorax/</link>
      <description><![CDATA[https://github.com/predibase/lorax Predibase/Lorax 确实是一个有趣的 repo。它解决了使用适配器的主要问题，即动态分配适配器。有人试过吗？    提交人    /u/YogurtclosetAway7913   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwdw40/d_anyone_tried_predibaselorax/</guid>
      <pubDate>Wed, 08 Jan 2025 06:56:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师，你的工作中最烦人的部分是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hwbhuj/d_ml_engineers_whats_the_most_annoying_part_of/</link>
      <description><![CDATA[我只知道一个博士只是检查数据集，这听起来很可悲    提交人    /u/Classic_Eggplant8827   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hwbhuj/d_ml_engineers_whats_the_most_annoying_part_of/</guid>
      <pubDate>Wed, 08 Jan 2025 04:33:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 思维提示程序 (PoT) vs 思维提示链 (CoT)</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hw8q66/d_program_of_thought_prompting_pot_vs_chain_of/</link>
      <description><![CDATA[PoT 建立在已建立的 CoT 方法论之上，但通过将推理过程与计算执行分开而脱颖而出。 PoT 的核心是强调将推理与计算分离。在传统的 CoT 提示中，LLM 会在同一框架内生成推理步骤并执行计算，这可能会导致不准确，尤其是在复杂的数学问题中。PoT 通过指示 LLM 生成可在单独的解释器中运行的可执行代码（通常是 Python）来解决这个问题。这样可以清晰地划分逻辑推理过程和实际计算，从而提高清晰度和准确性。 执行过程 PoT 中的执行过程可以概括为几个步骤：  问题呈现：用户以自然语言呈现问题。 代码生成：LLM 生成 Python 代码，概述解决问题所需的步骤。 代码执行：生成的代码在外部环境（例如 Python 解释器）中执行。 结果解释：然后解释代码执行的输出并将其呈现为最终答案。  虽然 PoT 建立在 CoT 提示的基础上，但它引入了一个关键的区别：  CoT：涉及指导 LLM生成自然语言中的一系列推理步骤。但是，这种方法容易出错，尤其是在处理复杂计算时。 PoT：将推理过程转换为 Python 等正式的可执行语言。这消除了自然语言处理中固有的错误可能性，从而带来更可靠、更准确的解决方案。  来源：ml-digest 博客文章    提交人    /u/GiftProfessional1252   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hw8q66/d_program_of_thought_prompting_pot_vs_chain_of/</guid>
      <pubDate>Wed, 08 Jan 2025 02:07:10 GMT</pubDate>
    </item>
    <item>
      <title>[R][P] DistillKitPlus：面向法学硕士的高性能知识提炼</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hw0mn9/rp_distillkitplus_high_performent_knowledge/</link>
      <description><![CDATA[具有 LoRA 微调 和 量化支持 的 LLM KLD 开源工具包&gt; 较大的 LLM 泛化更好、更快。您可以利用这一点，然后将 70B 模型的最佳效果转移到 7B 模型，而无需花费太多资金或牺牲性能。 GitHub 链接：https://github.com/agokrani/distillkitplus    提交人    /u/__XploR__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hw0mn9/rp_distillkitplus_high_performent_knowledge/</guid>
      <pubDate>Tue, 07 Jan 2025 20:08:28 GMT</pubDate>
    </item>
    <item>
      <title>[研发] 白盒变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvy385/rd_white_box_transformers/</link>
      <description><![CDATA[在此研究线上开启一个主题：https://ma-lab-berkeley.github.io/CRATE/ 据我了解，作者基本上将学习有效数据表示的过程定义为寻找多元高斯字典的问题，该字典以简约方式覆盖数据分布。特别是，在特征/高斯方面使用稀疏编码。 构建一个架构，该架构采用多个交替步骤“聚类”相似向量并分别正交化来自不同聚类的向量，最终得到类似于 Vision Transformer 的结构。类似 MultiHead Attention 的模块将向量聚类，使它们更接近局部主方向或流形，类似 MLP 的模块将这些向量沿着相互更正交的轴移动。从数学上讲，它们近似于明确定义的稀疏编码率，因此是白盒算法，但是我不能说数学比 Transformers 更直观。 事实上，最后一层的 CLS 注意力头在图像分类监督训练下具有可解释的偏好，如在 DINO（自我监督）或 SimPool 中。这与过程的解释直接相关，并为 DINO 的可解释性和动态性提供了解释。它也被称为 George Hinton 的视觉智能架构蓝图，即 GLOM Transformer。 我认为注意力的聚类效应在文献中在某种程度上没有得到充分重视，就像 Transformers 中 FFN 的作用没有得到充分研究一样。我想知道是否有第三种方法在数学上像 MLP 一样简单，像高斯特征词典一样直观。    提交人    /u/Sad-Razzmatazz-5188   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvy385/rd_white_box_transformers/</guid>
      <pubDate>Tue, 07 Jan 2025 18:23:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入空间中的位置嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvxpcc/d_positional_embeddings_in_embedding_space/</link>
      <description><![CDATA[原始位置编码在特征空间中是如何分布的？RPE 是如何分布的？这些嵌入和 LayerNorm（删除与均匀向量平行的分量，即 1 的向量）之间的相互作用是什么？    提交人    /u/Sad-Razzmatazz-5188   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvxpcc/d_positional_embeddings_in_embedding_space/</guid>
      <pubDate>Tue, 07 Jan 2025 18:07:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师，你的工作中最有价值的事情是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvx2pq/d_ml_engineers_what_is_the_most_rewarding_thing/</link>
      <description><![CDATA[有些人告诉我这是薪水问题，但我认为这取决于你的经验水平以及你为谁工作？这份工作还有更多内容吗？    提交人    /u/RespectPrivacyPlz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvx2pq/d_ml_engineers_what_is_the_most_rewarding_thing/</guid>
      <pubDate>Tue, 07 Jan 2025 17:42:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对你来说，机器学习最吸引人的方面是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hvqdvt/d_what_is_the_most_fascinating_aspect_of_machine/</link>
      <description><![CDATA[标题。您可以根据自己的意愿主观地解释这个问题。    提交人    /u/AromaticEssay2676   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hvqdvt/d_what_is_the_most_fascinating_aspect_of_machine/</guid>
      <pubDate>Tue, 07 Jan 2025 12:30:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1htw7hw/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 05 Jan 2025 03:15:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1hq5o1z/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 31 Dec 2024 03:30:14 GMT</pubDate>
    </item>
    </channel>
</rss>