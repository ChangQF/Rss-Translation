<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 10 Feb 2024 21:12:07 GMT</lastBuildDate>
    <item>
      <title>[D] 如何加快 OpenAI API 响应时间？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1anqbt9/d_how_to_speed_up_openai_api_response_time/</link>
      <description><![CDATA[您好，我正在使用 OpenAI API 根据提供的 JSON 数据创建摘要。但是，我注意到响应生成过程大约需要 25 秒，这可能会给我的应用程序的用户带来不便。我正在联系以询问是否有任何策略或优化可以帮助缩短响应时间并增强整体用户体验。   由   提交/u/AB3NZ  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1anqbt9/d_how_to_speed_up_openai_api_response_time/</guid>
      <pubDate>Sat, 10 Feb 2024 21:02:10 GMT</pubDate>
    </item>
    <item>
      <title>[D]：用于 ML 数据生成的 3D 对象传输和场景组装的最佳方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1anootk/d_best_methods_for_3d_object_transfer_and_scene/</link>
      <description><![CDATA[我正在开展一个项目，旨在增强分割和对象检测任务的视觉模型的训练。该项目的核心涉及将对象转移到 3D 世界（例如点云），然后将这些对象组装到新场景中。目标不仅是渲染新的 3D 场景以进行训练，而且还可能增强这些场景并自动生成标签，从而简化数据准备过程。 我们想要转移到真实 3D 场景的对象我们的项目可用，因此我们能够拍摄它们的 360° 视频。 我们的团队正在考虑各种方法，但考虑到当前的技术，我们热衷于确定最有效的方法景观。我们雷达上的选项包括：  利用 NeRF 进行高保真 3D 场景重建。我们对其创建详细且逼真的场景的潜力很感兴趣，但对其计算需求以及将其集成到机器学习管道的复杂性持谨慎态度。 点云的高斯 Splats ：对点云数据采用高斯喷射技术来创建 3D 表示。这种方法因其效率和操作点云的简便性而看起来很有前途。 ML 增强的经典摄影测量技术：利用传统摄影测量方法，并通过机器学习算法增强，用于 3D 对象和场景重建。这可以在计算效率和场景保真度之间提供平衡，但与更现代的方法相比，我们不确定其可扩展性和准确性。  我们的目标是渲染新的 3D 场景它可以用来有效地训练我们的视觉模型，并具有自动为增强对象和场景生成准确标签的额外好处。 我正在寻求更多见解、经验和建议。我想到的问题是：  这些方法（或任何其他未列出的方法）目前代表了细节、效率和易于集成到机器学习工作流程之间的最佳平衡，以便训练视觉模型？ 您是否发现在实施这些技术时特别有用的特定工具、库或框架？ 您在生成和使用合成 3D 数据时遇到了哪些挑战用于对象检测和分割任务，您是如何解决这些问题的？ （几年前，我作为一名副工程师参与了一个项目，他们在模拟与真实之间的差距上苦苦挣扎。然而，当时他们使用的是完全虚拟创建的场景，没有重建现实。） &lt; /ul&gt; 任何分享的经验、相关研究的参考或资源的指示将不胜感激。我们对有助于指导我们项目的实用建议和技术见解特别感兴趣。谢谢   由   提交 /u/Successful_Pie1550   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1anootk/d_best_methods_for_3d_object_transfer_and_scene/</guid>
      <pubDate>Sat, 10 Feb 2024 19:50:09 GMT</pubDate>
    </item>
    <item>
      <title>[R] 寻找SFUDA研究顾问</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ankr20/r_looking_for_advisor_for_sfuda_research/</link>
      <description><![CDATA[我是一名高中生，试图对机器学习进行一些研究 - 特别是在电影和产品评论中寻找免费的无监督域适应。 我之前曾与一位顾问密切合作，但由于日程安排冲突，他不得不退出。  如果您能帮助我，我将不胜感激 - 我希望尽可能少地占用您的时间，并在我们的会议之外完成大部分工作。请告诉我这是否适合您。 如果您根本没有时间，请您考虑分享任何愿意提供帮助并且有时间的同事的联系方式吗？   由   提交 /u/supersid2911   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ankr20/r_looking_for_advisor_for_sfuda_research/</guid>
      <pubDate>Sat, 10 Feb 2024 16:57:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 哪些人工智能/机器学习领域正在受到关注？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ankhpi/d_which_aiml_fields_are_growing_under_the_radar/</link>
      <description><![CDATA[LLM 和扩散模型目前正在抢尽风头。我很想知道人工智能/机器学习的哪些其他领域是人们感兴趣的，尤其是工业快速采用的领域。 从我自己的角度来看，我注意到计算机视觉/机器视觉&lt; /strong&gt; 是工业/制造领域许多人的需求，对我来说，这似乎是机器学习最成熟的工业应用。紧随其后的是数据驱动的信号处理，这似乎是航空航天类型公司为其雷达软件所要求的。我知道 Facebook/Amazon 和其他公司使用图神经网络，但不知道使用到什么程度。我知道强化学习领域发生了很多事情，尤其是在机器人领域，但这与我的专业领域相去甚远。此外，许多行业中都有许多人使用深度学习和更经典的机器学习来寻找 SoC 和硅行业中其他此类问题的最佳布局。 我很感兴趣听取在法学硕士/扩散模型之外从事人工智能/机器学习的其他人的意见。你在兴奋什么？您认为增长发生在哪里？   由   提交 /u/DresDunn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ankhpi/d_which_aiml_fields_are_growing_under_the_radar/</guid>
      <pubDate>Sat, 10 Feb 2024 16:45:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] 刚刚推出 ⚡Edgen：开源、本地和私有 AI。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1anjb0n/p_just_launched_edgen_opensource_local_and/</link>
      <description><![CDATA[   ⚡Edgen：Rust 中 OpenAI 的本地私有 GenAI 服务器替代品。无需 GPU。与任何操作系统兼容，只需下载一次，任何人都可以开始在本地运行最好的 GenAI 模型，即：LLM（Llama2、Mistral、Mixtral...）、语音转文本（耳语）等等。 &lt; p&gt;我们⚡Edgen 的目标是让更多人能够进行以隐私为中心的本地 GenAI 应用程序开发。它符合 OpenAI 的 API，专为那些优先考虑数据隐私并希望使用基于 Rust 的基础设施在本地试验或部署 AI 模型的人而设计。 我们希望这个社区能够成为首先尝试一下，提供反馈，并为其成长做出贡献。 在这里查看：GitHub - edgenai/edgen : ⚡ Edgen：OpenAI 的本地私有 GenAI 服务器替代方案。 这里是 EdgenChat 的一个简短演示，这是一个由 ⚡Edgen 提供支持的网络应用程序： https://i.redd.it/gdkutdvy4shc1.gif ​   由   提交/u/EdgenAI   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1anjb0n/p_just_launched_edgen_opensource_local_and/</guid>
      <pubDate>Sat, 10 Feb 2024 15:52:02 GMT</pubDate>
    </item>
    <item>
      <title>AAAI 24 与会者 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1anj3fh/aaai_24_attendees_d/</link>
      <description><![CDATA[大家好， 我想问谁可能会参加在温哥华举行的 AAAI 24 会议！我想和一些人谈谈这个问题，比如人们将参加哪些会议以及会议之外可以做什么（探索温哥华）。   由   提交/u/tallguyfromstats   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1anj3fh/aaai_24_attendees_d/</guid>
      <pubDate>Sat, 10 Feb 2024 15:42:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] LoRA-MoE：像 Mixtral 8x7B 这样的 MoE 模型进行训练和推理，就像 7B 参数模型一样</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1anj15v/r_loramoe_training_and_inferencing_moe_models/</link>
      <description><![CDATA[MoE 模型（例如 Mixtral 8x7B）在完全连接的块中使用 8 个不同的密集矩阵专家，其中两个由路由器网络选择，并将它们的输出组合起来，同时处理令牌。 由于只有其中两个组需要加载到内存，而其他组则保持卸载状态，因此这要求模型在任何时候使用 46.7B 总参数中的 12.9B 参数 我想知道，如果我们能够从由路由器网络并应用于全连接块每一层中的一个主矩阵。这应该大致需要与 7B 模型相同水平的计算和大小要求。 我们还应该能够从现有的预训练 Mixtral 8x7B 模型创建 LoRA-MoE 模型，在该模型中我们创建一个“平均矩阵” &#39; 通过将所有 8 个专家的每一层的矩阵相加，从所有 8 个矩阵中减去它以创建“增量矩阵”，并通过低秩矩阵表示这些增量矩阵，然后将其应用于路由器网络选择的“平均矩阵”其中一个或两个矩阵。 通过更改低秩矩阵的秩，我们应该能够控制 LoRA-MoE 与 MoE 模型质量匹配的程度。我想知道你的想法？有人试过吗？   由   提交/u/ashz8888  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1anj15v/r_loramoe_training_and_inferencing_moe_models/</guid>
      <pubDate>Sat, 10 Feb 2024 15:39:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 你能提取llm的编码器部分来进行特征提取吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ani1uz/d_can_you_extract_the_encoder_part_of_an_llm_for/</link>
      <description><![CDATA[我对此还很陌生，所以这可能是一个愚蠢的问题。如果您有一个像最新的混合模型这样的开源模型，您可以提取进行编码的层并将其用于特征提取吗？如果是这样，值得尝试使用 BERT 或 ROBERTA 吗？   由   提交 /u/TheMiniQuest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ani1uz/d_can_you_extract_the_encoder_part_of_an_llm_for/</guid>
      <pubDate>Sat, 10 Feb 2024 14:54:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我尝试用新的 Vision Pro 解释 FSDP 和 3D 管道并行性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1anhpp5/p_my_attempt_to_explain_fsdp_and_pipeline/</link>
      <description><![CDATA[       由   提交/u/waf04  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1anhpp5/p_my_attempt_to_explain_fsdp_and_pipeline/</guid>
      <pubDate>Sat, 10 Feb 2024 14:37:35 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用 CUML/SVC 管理 VRAM/RAM</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1angdoe/p_managing_vram_ram_using_cumlsvc/</link>
      <description><![CDATA[      嗨，我首先要说的是，我是机器学习领域的绝对初学者，目前正在研究该主题。 我我正在 Fashion-MNIST 数据集上训练模型，并尝试使用 CUML 来使用我的 GPU 来帮助加快速度（使用 CPU 平均 3.5 分钟，而使用 GPU 平均需要 25 到 8 秒）。 I遇到一个问题，我认为代码将所有数据存储到 GPU 和 RAM 上，并使两者都饱和，我可以实现一些东西，一旦完成折叠，就会转储不需要的数据，为 RAM 和 RAM 提供更多空间GPU 显存？ ​  这是我的参考代码，有些库不会被使用，只是因为我正在虚拟机中编程，而 VS 在 Windows 上。    由   提交 /u/BubblyMidnight2574   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1angdoe/p_managing_vram_ram_using_cumlsvc/</guid>
      <pubDate>Sat, 10 Feb 2024 13:31:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 微调法学硕士以适应领域</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ane4ia/d_finetuning_llm_for_domainadaption/</link>
      <description><![CDATA[我想使用一个已经训练有素的自然语言法学硕士 -&gt; sql 像 https://huggingface.co/defog/sqlcoder-7b-2 与我的数据库架构。我发现的所有指南都建议使用系统提示以创建语句的形式提供模式。  对于大型数据库模式来说，上下文窗口不是一个问题吗？或者我是否需要实现某种检索来找到与用户查询最相关的模式并将它们添加到系统提示符中？ 我更愿意“硬编码”将模式放入 llm 中，并进行微调以简化推理。为此，我已经将表转换为创建语句。每个表和列也有进一步解释数据的注释。如果可能的话，有人可以为此提供一些通用指南吗？训练数据需要什么样的格式。由于模型已经在 nl-&gt;sql 上进行了训练，我是否需要用自然语言来训练它 -&gt;我的 db-shema 上有 sql 示例，或者我可以简单地添加 db-schema 吗？   由   提交 /u/CaptainSnackbar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ane4ia/d_finetuning_llm_for_domainadaption/</guid>
      <pubDate>Sat, 10 Feb 2024 11:17:16 GMT</pubDate>
    </item>
    <item>
      <title>[P] PyTorch 中从头开始的稳定扩散 |第一部分 - 无条件潜在扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1an9bvu/p_stable_diffusion_from_scratch_in_pytorch_part_i/</link>
      <description><![CDATA[       由   提交/u/tusharkumar91   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1an9bvu/p_stable_diffusion_from_scratch_in_pytorch_part_i/</guid>
      <pubDate>Sat, 10 Feb 2024 05:51:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么门控线性网络消失了？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1an20kq/d_why_did_gated_linear_networks_disappear/</link>
      <description><![CDATA[DeepMind 研究人员想出了这个。在使用在线学习时，尤其是在上下文强盗方法中，它应该超越现有的解决方案。   由   提交 /u/__A-R__   /u/__A-R__  reddit.com/r/MachineLearning/comments/1an20kq/d_why_did_lated_linear_networks_disappear/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1an20kq/d_why_did_gated_linear_networks_disappear/</guid>
      <pubDate>Fri, 09 Feb 2024 23:33:07 GMT</pubDate>
    </item>
    <item>
      <title>信仰与命运：变形金刚对组合性的限制 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1amzb52/faith_and_fate_limits_of_transformers_on/</link>
      <description><![CDATA[     &lt; td&gt; 编辑：Kevin Murphy、Francois Chollet、Vitaly Kurin 等人推荐了这篇论文（有些非常高度） https://arxiv.org/abs/2305.18654（12 月在 NeurIPS 上发表） 摘要：  Transformer 大语言模型 (LLM) 因其在需要复杂的多步骤推理的任务上的出色表现而备受赞誉。然而，这些模型同时在一些令人惊讶的微不足道的问题上失败了。这就引出了一个问题：这些错误是偶然的，还是表明存在更实质性的限制？为了揭开 Transformer LLM 的神秘面纱，我们研究了这些模型在三个代表性组合任务中的局限性——多位数乘法、逻辑网格难题和经典的动态规划问题。这些任务需要将问题分解为子步骤，并将这些步骤综合为精确的答案。我们将组合任务制定为计算图，以系统地量化复杂程度，并将推理步骤分解为中间子过程。我们的实证研究结果表明，变压器法学硕士通过将多步骤组合推理减少为线性子图匹配来解决组合任务，而不必培养系统的解决问题的技能。为了完善我们的实证研究，我们提供了关于抽象多步骤推理问题的理论论证，这些问题强调了自回归代的性能如何随着任务复杂性的增加而迅速衰减。  Kevin Murphy 的总结：“我喜欢这张纸。他们证明，变压器在进行长推理链时肯定会遭受复合错误（正如 @ylecun 所说），并且明显的“成功”只是由于不可靠的模式匹配/快捷学习。”   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1amzb52/faith_and_fate_limits_of_transformers_on/</guid>
      <pubDate>Fri, 09 Feb 2024 21:34:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>