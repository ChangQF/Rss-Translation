<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 09 Nov 2024 18:19:44 GMT</lastBuildDate>
    <item>
      <title>[R] Jay McClelland 解释并行分布式处理、大脑的工作原理、赫布学习和反向传播</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/</link>
      <description><![CDATA[      Jay McClelland 是人工智能领域的先驱，也是认知心理学家，斯坦福大学心理学、语言学和计算机科学系的教授。Jay 与 David Rumelhart 共同出版了两卷著作《并行分布式处理》，该书推动了联结主义理解认知的方法的蓬勃发展。 在这次对话中，Jay 为我们速成讲解了神经元和生物大脑的工作原理。这为 Jay、David Rumelhart 和 Geoffrey Hinton 等心理学家在历史上如何处理认知模型以及最终的人工智能的发展奠定了基础。我们还讨论了神经计算的替代方法，例如符号和神经科学方法以及反向传播的发展。 https://preview.redd.it/s7xv0pmk2xzd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=2e5be31c51a8eb78bf7033d1def25fa29f0863af https://preview.redd.it/h4sqjoim2xzd1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=e7c952d579322379c67a77adadf1d392afe8d3c6 Youtube： https://www.youtube.com/watch?v=yQbJNEhgYUw&amp;list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO&amp;index=1&amp;pp=iAQB Spotify： https://open.spotify.com/show/1X5asAByNhNr996ZsGGICG RSS： https://feed.podbean.com/cartesiancafe/feed.xml    由   提交  /u/IamTimNguyen   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gng4fy/r_jay_mcclelland_explains_parallel_distributed/</guid>
      <pubDate>Sat, 09 Nov 2024 18:11:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 关于微调 Meta 的 Segment Anything 2 (SAM) 模型的建议 — 平衡边缘情况与普遍性</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnf7zi/r_advice_on_finetuning_metas_segment_anything_2/</link>
      <description><![CDATA[我正在使用 SAM2，并试图找出针对我的特定用例对其进行微调的最佳方法。我希望获得一些见解：  错误校正与泛化：如果我有兴趣对模型进行微调，以便在最容易出错的情况下表现更好，我是否可以保留它在已经表现良好的示例上的表现。即仍然保持（甚至提高）其先前的泛化能力？或者我应该有足够多的已经表现良好的示例来保持这种性能？ 要微调哪些组件？就模型的架构而言，我看到了关于是否只微调掩码解码器，提示编码器或两者的不同建议。根据您的经验，仅微调掩码解码器是否足以提高性能，还是还需要调整提示编码器？或者可能还有更多内容 — 比如模型的主干或其他部分？计算上的差异是否太大？还是还有其他缺点/注意事项？ 现实世界的经验：对于之前微调过 SAM 的人，您的体验如何？有什么技巧、窍门或我应该注意的陷阱吗？此外，您是如何准备微调数据集的？关于平衡数据多样性与关注边缘情况有什么建议吗？     提交人    /u/No_Cartoonist8629   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnf7zi/r_advice_on_finetuning_metas_segment_anything_2/</guid>
      <pubDate>Sat, 09 Nov 2024 17:31:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 当机器学习讲述错误的故事时</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gne2x1/r_when_machine_learning_tells_the_wrong_story/</link>
      <description><![CDATA[  由    /u/jackcook  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gne2x1/r_when_machine_learning_tells_the_wrong_story/</guid>
      <pubDate>Sat, 09 Nov 2024 16:40:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 下一帧的潜在空间预测</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gncn8y/d_latent_space_forecasting_of_the_next_frame/</link>
      <description><![CDATA[大家好，我正在搜索计算机视觉任务的论文或提示。我已经实现了一个用于图像分类的 Vision Transformer。下一步，我必须在 ViT 的编码器网络之上实现一个预测器，它从 enc(x_t) -&gt; enc(x_t+1) 进行预测。预测器应该预测下一帧的嵌入。我的第一个想法是 MLP 头或解码器网络。如果有人已经解决了类似的任务，我很高兴能得到建议。谢谢    提交人    /u/Significant-Joke5751   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gncn8y/d_latent_space_forecasting_of_the_next_frame/</guid>
      <pubDate>Sat, 09 Nov 2024 15:34:09 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用文本或图像特征和实值回归目标对监督数据集进行基准测试或开源？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnb9z5/p_benchmark_or_open_source_supervised_datasets/</link>
      <description><![CDATA[出于某种原因，我似乎找不到任何以文本或图像为特征、以实值为目标的知名基准数据集。任何目标范围都可以（（0,1）、（-无穷大，无穷大）、（0，无穷大）等）。我找到了具有序数分类目标的示例（例如 1-5 的整数评级），但这不符合我的目的。 有谁知道任何符合此描述的开源监督 ML 数据？最好是具有性能排行榜的基准数据集。    提交人    /u/BreakingBaIIs   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnb9z5/p_benchmark_or_open_source_supervised_datasets/</guid>
      <pubDate>Sat, 09 Nov 2024 14:28:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 上周医学 AI：顶级 LLM 研究论文/模型（2024 年 11 月 2 日至 11 月 9 日）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gn8wqp/d_last_week_in_medical_ai_top_llm_research/</link>
      <description><![CDATA[      上周医学 AI：顶级 LLM 研究论文/模型（2024 年 11 月 2 日至 11 月 9 日） 本周医学 AI 论文：  谷歌推出*：探索用于专家级肿瘤护理的大型语言模型*  本文使用 50 个合成癌症小插图评估了乳腺肿瘤学中的对话式诊断 AI 系统 AMIE。借助网络搜索检索和自我批评流程，AMIE 在制定管理计划方面的表现优于内科实习生和肿瘤学研究员，使用涵盖病例总结、计划安全性和治疗建议的详细临床评估标准进行评估。   医学 LLM 及其他模型：  AutoProteinEngine：多模态蛋白质 LLM  本文介绍了 AutoProteinEngine (AutoPE)，这是一个由 LLM 驱动的蛋白质工程多模态 AutoML 框架，使没有深度学习专业知识的生物学家能够使用自然语言与 DL 模型进行交互。AutoPE 将 LLM 与 AutoML 集成，用于模型选择（序列和图形模态）、超参数优化和自动数据检索，在两个现实世界的蛋白质工程任务中表现出比传统方法显着的性能改进。代码可从以下位置获取：  GSCo：通才-专才 AI 协作  本文介绍了 GSCo，这是一种结合通才基础模型 (GFM) 和专才模型的医学图像分析框架。它开发了最大的开源医学 GFM MedDr 和用于下游任务的轻量级专才。  用于肺部 X 射线分割的 SAM  本文探讨了 Meta AI 的 Segment Anything Model (SAM) 在胸部 X 射线分析中用于肺部分割的应用。这项研究采用了带有微调的迁移学习方法，与原始 SAM 相比，其性能有所提高，所取得的结果可与 U-Net 等最先进的模型相媲美。  MEG：知识增强型医学问答  本文介绍了 MEG，这是一种使用轻量级映射网络为大型语言模型 (LLM) 提供医学知识图谱的参数高效方法。在四个医学多项选择数据集上进行评估后，MEG 的准确率比 Mistral-Instruct 基线提高了 10.2%，比 BioMistral 等专业模型提高了 6.7%，证明了知识图谱集成的好处。   框架和方法：  BrainSegFounder：3D 神经图像分析 PASSION：撒哈拉以南皮肤病学数据集 Label Critic：数据优先方法 Medprompt 运行时策略  医学 LLM 应用：  CataractBot：患者支持系统 CheX-GPT：X 射线报告增强 CardioAI：癌症心脏毒性监测器 HealthQ：医疗保健对话Chain PRObot：糖尿病视网膜病变助理  医学法学硕士和基准：  MediQ：临床推理基准 Touchstone：分割评估 医学 LLM 适应进展 微调医学 QA 策略  医疗伦理中的 AI：  具有 LLM 的医疗机器人 临床实践中的 XAI 精准康复框架 多模式 AI 挑战  完整线程详细信息：https://x.com/OpenlifesciAI/status/1855207141302473090    由   提交  /u/aadityaura   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gn8wqp/d_last_week_in_medical_ai_top_llm_research/</guid>
      <pubDate>Sat, 09 Nov 2024 12:20:58 GMT</pubDate>
    </item>
    <item>
      <title>[P] MiniBoosts：一组小型的 boosting 算法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gn8mpt/p_miniboosts_a_small_collection_of_boosting/</link>
      <description><![CDATA[大家好。 我用 Rust 编写了一个小型的 boosting 算法集合，名为 MiniBoosts。 这是一个业余项目，但我想进一步改进。 欢迎任何反馈意见。 感谢您的合作。    提交人    /u/__leopardus__   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gn8mpt/p_miniboosts_a_small_collection_of_boosting/</guid>
      <pubDate>Sat, 09 Nov 2024 12:03:37 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入和 docker 文件 - 两个库之间的比较 - 有没有比 ONNX 更好的？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gn87vi/d_embeddings_and_docker_file_comparison_between/</link>
      <description><![CDATA[正如标题所说，我想知道是否还有其他方法可以在不使用 torch 的情况下嵌入语料库。我想到的解决方案之一是使用 ONNX。我使用 Qdrant 中的 fastembed 库和 sentence-transformer 库创建了图像。使用 fastembed 可以显著减小图像尺寸。 问题： 还有其他方法（例如修改 dockerfile 或使用其他库）可以进一步缩小 docker 映像吗？ 公共 repo：https://github.com/learning-bos/dockerize-torch-fastembed-sentence-transformer-comparison   由    /u/Ambitious-Most4485  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gn87vi/d_embeddings_and_docker_file_comparison_between/</guid>
      <pubDate>Sat, 09 Nov 2024 11:36:10 GMT</pubDate>
    </item>
    <item>
      <title>[P] 开源文本到代理：从 YAML 文件开发 AI 代理的框架。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gn2e5w/p_opensource_texttoagent_framework_to_develop_ai/</link>
      <description><![CDATA[大家好，我想听听你们对我正在开发的一个项目的反馈。我正在构建一个框架，从 YAML 配置文件中定义 AI 代理。这些文件封装了需要完成的任务、它们如何连接等，而其余的都被抽象出来了。 现在的想法是使用 LLM 本身从用户提示中创建这些 YAML 文件。由于配置文件具有代理的所有核心逻辑并删除了所有不必要的细节，我认为这是构建文本到代理框架的最有效方法。 Wdyt？ 让我知道你的想法，并查看 repo https://github.com/octopus2023-inc/gensphere 如果您想做出贡献并使其发挥作用，请告诉我。    提交人    /u/Jazzlike_Tooth929   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gn2e5w/p_opensource_texttoagent_framework_to_develop_ai/</guid>
      <pubDate>Sat, 09 Nov 2024 04:47:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] PAKDD 2023 数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gn282v/d_pakdd_2023_data/</link>
      <description><![CDATA[我正在研究 PAKDD 2023 上发表的研究论文。从作者的名字，我可以猜出他们是中国人、韩国人或日本人 我知道 PAKDD 是双盲评审。但为什么其他人不提交他们的作品？或者如果他们提交了，为什么接受的数量很低 我也是亚洲人，所以我不是想在这里种族歧视。只是想知道为什么会这样    提交人    /u/Alarming-Camera-188   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gn282v/d_pakdd_2023_data/</guid>
      <pubDate>Sat, 09 Nov 2024 04:37:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单的 ML 模型托管服务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gmzb3q/d_simple_ml_model_hosting_service/</link>
      <description><![CDATA[我的工作是寻找一种让人工智能帮助制定计划的方法，我真的认为一个简单的多变量模型就可以解决问题；只需要找到一个可靠的托管服务，可以根据需要在此基础上进行构建。是否有成熟的可扩展、可配置的 ML 托管商？    提交人    /u/Lucrayzor   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gmzb3q/d_simple_ml_model_hosting_service/</guid>
      <pubDate>Sat, 09 Nov 2024 01:55:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大多数时间序列异常检测结果毫无意义（两个简短的视频解释了原因）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/</link>
      <description><![CDATA[亲爱的同事们 时间序列异常检测 (TSAD) 目前非常热门，每年在 NeurIPS、SIGKDD、ICML、PVLDB 等会议上都会发表数十篇论文。 但是，我认为大部分已发表的结果都毫无意义，因为基本事实标签的不确定性使得算法之间声称的任何差异或声称的改进量相形见绌。 我制作了两个 90 秒长的视频，以视觉和直观的方式清楚地说明了这一点：  1) 为什么大多数时间序列异常检测结果毫无意义（道奇队） https://www.youtube.com/watch?v=iRN5oVNvZwk&amp;ab_channel=EamonnKeogh  2）为什么大多数时间序列异常检测结果毫无意义（AnnGun） https://www.youtube.com/watch?v=3gH-65RCBDs&amp;ab_channel=EamonnKeogh 与往常一样，欢迎更正和评论。 Eamonn  编辑：需要说明的是，我的观点只是为了防止其他人浪费时间处理带有本质上随机标签的数据集。此外，我们应该对文献中基于此类数据的任何主张保持谨慎（其中包括至少数十篇被高度引用的论文） 有关大多数常用 TSAD 数据集的审查，请参阅此文件： https://www.dropbox.com/scl/fi/cwduv5idkwx9ci328nfpy/Problems-with-Time-Series-Anomaly-Detection.pdf?rlkey=d9mnqw4tuayyjsplu0u1t7ugg&amp;dl=0    由    /u/eamonnkeogh 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gmwxnr/r_most_time_series_anomaly_detection_results_are/</guid>
      <pubDate>Fri, 08 Nov 2024 23:58:19 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在 PB 级数据集上进行训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/</link>
      <description><![CDATA[假设我们有一个比磁盘存储空间大得多的数据集。例如：  数据集：1PB 我们的磁盘存储空间：10TB GPU RAM：8x80GB（与本次讨论不太相关）  对这样的数据进行训练的常用方法是什么？我直观想到的是以某种方式并行执行以下操作： - 预取块 n，在块 n-1 上进行训练，从磁盘中删除块 n-2 假设我们使用 PyTorch，因此我们有一个 PyTorch 数据集，其中包含数据存储在云中的所有路径。我们是否需要为从云端下载并存储在磁盘上的预取器/删除器编写代码，并让其在单独的进程中运行，然后让 DataLoader 用于训练，假设它可以从磁盘读取（因为预取器正确地完成了它的工作）？让 DataLoader 从 S3 读取对 GPU 利用率不利，对吗？ 退一步说，我假设这是每个在大型数据集上进行训练的公司都会遇到的普通且经常发生的“问题”，所以我对自己编写所有这些代码持怀疑态度，因为我觉得应该有标准的开箱即用的解决方案，但真的找不到完美匹配的东西。    提交人    /u/lapurita   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gmpedb/d_training_on_petabyte_scale_datasets/</guid>
      <pubDate>Fri, 08 Nov 2024 18:27:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1giq4ia/d_simple_questions_thread/</guid>
      <pubDate>Sun, 03 Nov 2024 16:00:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>