<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Thu, 28 Nov 2024 15:18:25 GMT</lastBuildDate>
    <item>
      <title>[R] BitNet a4.8：1 位 LLM 的 4 位激活</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/</link>
      <description><![CDATA[      论文： https://arxiv.org/pdf/2411.04965 摘要：  最近对 1 位大型语言模型 (LLM)（例如 BitNet b1.58）的研究为降低 LLM 的推理成本同时保持其性能提供了一个有希望的方向。在这项工作中，我们引入了 BitNet a4.8，为 1 位 LLM 启用 4 位激活。BitNet a4.8 采用混合量化和稀疏化策略来减轻异常通道引入的量化误差。具体来说，我们利用 4 位激活作为注意力和前馈网络层的输入，同时稀疏中间状态，然后进行 8 位量化。大量实验表明，BitNet a4.8 在训练成本相当的情况下，实现了与 BitNet b1.58 相当的性能，同时在启用 4 位（INT4/FP4）内核的情况下，推理速度更快。此外，BitNet a4.8 仅激活 55% 的参数并支持 3 位 KV 缓存，进一步提升了大规模 LLM 部署和推理的效率。  Visual Abstract: https://preview.redd.it/gpt38utvqn3e1.png?width=1011&amp;format=png&amp;auto=webp&amp;s=1c9a09638675e7a9f89e3804c1df0229663d136a 评估： HS=HellaSwag, PQ=PiQA, WGe=WinoGrande https://preview.redd.it/7qrw9jtqrn3e1.png?width=1033&amp;format=png&amp;auto=webp&amp;s=ecfdcb655ae939de8f297e37ef111b8ccaa2b1c9    提交人    /u/StartledWatermelon   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1y0ig/r_bitnet_a48_4bit_activations_for_1bit_llms/</guid>
      <pubDate>Thu, 28 Nov 2024 15:11:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] 使用 GPU 并行实现基于矩阵的快速反事实遗憾最小化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/</link>
      <description><![CDATA[反事实遗憾最小化 (CFR) 的一种新颖的 GPU 实现，可加速广泛形式游戏中最优策略的计算。核心创新是跨 GPU 核心并行化遗憾更新和策略计算，同时仔细管理内存访问模式。 关键技术要点： - 将游戏状态和操作映射到 GPU 线程的自定义内存布局 - 批量处理信息集以最大化 GPU 利用率 - 并行计算反事实值和遗憾更新 - 通过游戏树分区实现多 GPU 扩展 - 在 Leduc Hold&#39;em 和 Limit Texas Hold&#39;em 扑克变体上进行评估 结果： - 与 CPU 实现相比，速度提高了 30 倍 - GPU 数量线性扩展，最多 8 个设备 - 内存使用量随游戏大小和信息集数量而扩展 - 解决方案质量在统计误差范围内与 CPU 基线匹配 - 成功解决了多达 1014 个状态的游戏 我认为这项工作可以使 CFR 在扑克以外的实际应用中更加实用。更快地解决大型游戏的能力为自动谈判、安全游戏和资源分配等领域开辟了可能性。多 GPU 扩展尤其有趣，因为它表明了解决更复杂游戏的潜力。 此处开发的内存优化技术也可能很好地转移到需要有效处理大状态空间的其他博弈论算法。 TLDR：GPU 加速 CFR 实现通过仔细的并行化和内存管理实现了 30 倍的加速，并具有线性多 GPU 扩展。使解决大型广泛形式游戏变得更加容易。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1wq6b/r_fast_matrixbased_counterfactual_regret/</guid>
      <pubDate>Thu, 28 Nov 2024 14:08:34 GMT</pubDate>
    </item>
    <item>
      <title>[D] 现代扩散模型背后的理论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/</link>
      <description><![CDATA[大家好， 我最近在大学里听了一些关于扩散模型的讲座。这些讲座详细解释了原始 DDPM（去拟态扩散概率模型）背后的所有数学知识（尤其是在附录中），实际上比我在网上找到的任何其他知识都要好。因此，它对于学习扩散模型背后的基础知识非常有帮助（如果您有兴趣，可以在此处的自述文件中的链接中找到幻灯片：https://github.com/julioasotodv/ie-C4-466671-diffusion-models） 但是，我正在努力寻找具有类似现代方法详细程度的资源 - 例如流匹配/整流流，用于采样的不同 ODE 求解器的工作原理等。有一些，但我发现的一切要么相当过时（比如从 2​​023 年左右开始），要么非常肤浅 - 例如对于非技术或科学受众而言。 因此，我想知道：除了原始论文之外，是否有人遇到过超出基本扩散模型的理论解释的良好汇编？我们的目标是让我的团队深入研究他们想要的实际论文，但要将其中 70% 的内容放在一个或多个像样的汇编中。 我真的相信，如今 SEO 让任何搜索都变成了一场噩梦。要么就是我的谷歌搜索技能因为某种原因而下降了。 谢谢大家！    提交人    /u/bgighjigftuik   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1vxe1/d_theory_behind_modern_diffusion_models/</guid>
      <pubDate>Thu, 28 Nov 2024 13:27:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 将数据加载到 Ray 集群中</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/</link>
      <description><![CDATA[对于那些在 AWS 上的 Ray 集群中运行 ML 训练的人，我很好奇你们采用什么方法将训练数据放入集群？ 你们如何对数据进行版本控制？ 如何避免在具有相同数据集的运行中重复下载相同的数据？ 我希望有一个流畅的过程，能够针对训练运行的特定版本的数据集，并避免重复下载它。数据版本控制应该与创建它的数据管道版本有明确的映射。如果能很好地扩展到更大的数据集，那就太好了。 渴望听到来自战壕的经验。    提交人    /u/SingularValued   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1v68j/d_loading_data_into_ray_clusters/</guid>
      <pubDate>Thu, 28 Nov 2024 12:45:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我可以在 NVIDIA 4060 上运行哪些 LLM 模型用于研究目的？需要推荐！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/</link>
      <description><![CDATA[大家好， 我正在深入研究大型语言模型 (LLM)，并希望尝试在我的 NVIDIA 4060 GPU 上本地运行它们。虽然我知道与一些研究设置相比，4060 并不是高端显卡，但我对充分利用它的功能持乐观态度。我将不胜感激您就以下方面提出的任何见解或建议：  可以在 4060 上高效运行的模型。我知道某些较小版本的 LLM 可能更适合此硬件，因此，如果您能提供任何关于无需过度优化即可实现的实际可能性的建议，我将不胜感激。 适合微调或预训练实验的模型。虽然我从基础实验开始，但我计划在未来探索微调，因此我很乐意听取关于通用且广泛用于研究的模型的建议。 开源模型或易于访问和用于研究目的的模型。许可和透明度对我很重要，因为我的工作专注于学术和实验目标。  到目前为止，我一直在研究 LLaMA、GPT-NeoX 和 BLOOM 等选项，尤其是它们的较小版本，但我愿意探索其他可能性。如果您有在中档 GPU 上运行这些或类似模型的经验，我很乐意听听您对性能、设置或我应该注意的任何潜在限制的看法。 此外，如果您能就以下方面提供任何建议，我将不胜感激：  优化 4060 的模型。是否有特定的工具、技术或库（如 bitsandbytes 或 FlashAttention）可以帮助运行或微调这些模型？ 准备微调。在选择模型时，我应该记住什么，以确保它能够有效地支持未来的微调实验？  提前感谢您分享您的专业知识！我渴望从社区学习并充分利用此设置。    提交人    /u/Spinotesla   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1ux8m/d_which_llm_models_can_i_run_on_an_nvidia_4060/</guid>
      <pubDate>Thu, 28 Nov 2024 12:30:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 尽管 Stella 嵌入在 MTEB 排行榜上名列前茅，为什么却没有得到更广泛的应用？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/</link>
      <description><![CDATA[https://huggingface.co/spaces/mteb/leaderboard 我一直在研究嵌入模型，并注意到一些有趣的事情：Stella 嵌入在 MTEB 排行榜上遥遥领先，表现优于 OpenAI 的模型，同时规模更小（1.5B/400M 参数）且使用 apache 2.0。托管它们相对便宜。 作为参考，Stella-400M 在 MTEB 上的得分为 70.11，而 OpenAI 的 text-embedding-3-large 为 64.59。1.5B 版本的得分甚至更高，为 71.19 然而，我很少看到它们在生产用例或讨论中被提及。这里有人在生产中使用过 Stella 嵌入吗？与 OpenAI 的产品相比，您在性能、推理速度和可靠性方面的体验如何？ 只是想了解为什么尽管基准测试令人印象深刻，但它们没有得到更广泛的采用，这是否是我遗漏了什么。 很想听听您的想法和经验！    提交人    /u/sdsd19   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1u814/d_why_arent_stella_embeddings_more_widely_used/</guid>
      <pubDate>Thu, 28 Nov 2024 11:45:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我们如何构建 MLOps 堆栈以实现快速、可重复的实验以及 NLP 模型的顺利部署</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1trdr/p_how_we_built_our_mlops_stack_for_fast/</link>
      <description><![CDATA[大家好， 我想简要介绍一下 GitGuardian 团队如何构建适用于生产用例的 MLOps 堆栈（完整博客文章链接：https://blog.gitguardian.com/open-source-mlops-stack/）。 作为 ML 工程师，我们都知道处理数据集、模型和云资源会多么混乱。我们面临一些常见问题：跟踪实验、管理模型版本以及处理低效的云设置。 我们决定全程开源。以下是我们用来使一切顺利的方法：  DVC 用于版本控制。它就像 Git，但用于数据和模型。对于可重复性非常有帮助 - 不再需要思考如何重新创建训练运行。 GTO 用于模型版本控制。它基本上是一个轻量级的版本标签管理器，因此我们可以轻松跟踪不​​同阶段中表现最佳的模型。 Streamlit 是我们进行实验可视化的首选。它与 DVC 集成，设置交互式应用程序来比较模型轻而易举。免去了我们编写大量自定义仪表板的麻烦。 SkyPilot 为我们处理云资源。不再需要手动设置 EC2。只需几个命令，我们就可以在云端启动 GPU，从而节省大量时间。 BentoML 用于在 docker 镜像中构建模型，以用于生产 Kubernetes 集群。它使部署变得非常容易，并且与我们的版本控制系统很好地集成，因此我们可以在需要时快速切换模型。  在生产方面，我们使用 ONNX Runtime 进行低延迟推理，使用 Kubernetes 扩展资源。我们有 Prometheus 和 Grafana 来实时监控一切。 TL;DR：通过结合 DVC、GTO、Streamlit、SkyPilot、BentoML 和其他一些工具，我们成功地使我们的 MLOps 管道更加顺畅。你们都使用什么工具来简化工作流程？让我们听听你的想法！    提交人    /u/michhhouuuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1trdr/p_how_we_built_our_mlops_stack_for_fast/</guid>
      <pubDate>Thu, 28 Nov 2024 11:14:06 GMT</pubDate>
    </item>
    <item>
      <title>[P] py-gen-ml：从架构生成 ML 配置代码</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/</link>
      <description><![CDATA[py-gen-ml 是一个 Python 库，旨在利用协议缓冲区的强大功能简化您的 ML 实验配置。它仍处于早期阶段，但我很想听听社区的一些反馈。 以下是 py-gen-ml 可以为您提供的帮助：  集中配置：在 Protobuf 中定义模式以充当单一事实来源。 最大限度地减少重复工作：自动生成模型、补丁、扫描和命令行界面的代码。 提高灵活性：借助具有高级引用和进行超参数扫描能力的 YAML 配置，轻松进行实验。 提高代码质量：受益于 JSON 模式验证、强类型和 IDE 支持，实现更强大的开发过程。  py-gen-ml 旨在通过减轻管理配置的负担，使 ML 开发更加高效。尝试一下，看看它如何改善您的工作流程。 开始使用： pip install py-gen-ml  了解更多： https://jostosh.github.io/py-gen-ml    提交人    /u/jalapenjos   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1t9v0/p_pygenml_generating_ml_configuration_code_from_a/</guid>
      <pubDate>Thu, 28 Nov 2024 10:40:23 GMT</pubDate>
    </item>
    <item>
      <title>[D]成为一名自由职业的数据科学家是否可行？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/</link>
      <description><![CDATA[大家好， 我每小时最低 15 美元就可以了，所以收入不是我关心的问题。我看过以前的 Reddit 帖子，但它们大多是从收入的角度讨论自由职业。我主要关心的是，考虑到数据科学领域的独特挑战，自由职业对于像我这样的人来说是否可行。 关于我的背景：我已经完成了 3-4 个现实世界的数据科学项目，不是玩具数据集，而是实际数据（涉及数据抓取、清理、可视化、建模、部署和文档）。我也曾在 NLP 领域实习过。 我一直在思考的一些问题：  领域知识和背景：如果不深入了解客户的业务，交付成果有多难？ 资源限制：自由职业者是否难以获取数据、计算能力或高级项目所需的其他工具？ 协作需求：数据科学通常需要与团队合作。自由职业者能否有效地与跨职能团队整合？ 迭代和长期性：许多项目需要持续更新和监控。这对自由职业者来说可行吗？ 信任和责任：自由职业者如何说服客户信任他们处理敏感或业务关键工作？ 客户期望：客户是否期望过高而回报过低，尤其是在低工资的情况下？  除了这些要点之外，我还愿意听取任何提示、建议或其他顾虑。对于新的数据科学自由职业者来说，这些挑战可以解决吗？你们中有人遇到并克服过类似的问题吗？我很想听听你的想法。 提前致谢！    提交人    /u/ds_reddit1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1q98i/dis_freelancing_as_a_data_scientist_even_possible/</guid>
      <pubDate>Thu, 28 Nov 2024 07:01:15 GMT</pubDate>
    </item>
    <item>
      <title>[P] Minima：本地对话检索增强生成项目（Ollama、Langchain、FastAPI、Docker）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/</link>
      <description><![CDATA[https://github.com/dmayboroda/minima  大家好，我想向你们介绍我最新的 repo，这是一个关于文件的本地对话抹布，老实说，你可以在本地使用它作为抹布，因为它是用 docker、langchain、ollama、fastapi、hf 构建的，所有模型都会自动下载，很快我会添加选择模型的功能，目前解决方案包含：  本地运行 Ollama（目前 qwen-0.5b 模型硬编码，很快你就可以从 ollama 注册表中选择一个模型） 本地索引（使用句子转换器嵌入模型，你可以切换到其他模型，但只应用句子转换器，也将很快更改） Qdrant 容器在您的机器上运行 Reranker 在本地运行（BAAI/bge-reranker-base 当前是硬编码的，但我还将添加选择重新排序器的功能） 基于 Websocket 的聊天，可保存历史记录 用 React 编写的简单聊天 UI 另外，您可以将本地 rag 与 ChatGPT 一起使用作为自定义 GPT，这样您就可以通过官方 chatgpt web 和 mac os/ios 应用查询本地数据。 您可以将其作为 RAG 部署在本地，所有容器都可以在 CPU 机器上工作  一些想法/问题：  模型上下文协议支持 现在没有增量索引或重新索引 没有模型选择（即将添加） 不同的环境支持（cuda、mps、自定义 npu）  欢迎贡献（关注、fork、star）非常感谢！    提交人    /u/davidvroda   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1pudq/p_minima_local_conversational_retrieval_augmented/</guid>
      <pubDate>Thu, 28 Nov 2024 06:33:40 GMT</pubDate>
    </item>
    <item>
      <title>[P] 使用数据子集进行消融研究？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/</link>
      <description><![CDATA[基本上，我正在参与一个研究项目，其中我训练仅用于文本分类的编码器语言模型。我已经训练了我的模型并得到了我的结果，但是我需要进行消融研究。我遇到的主要问题是数据集很大。对我而言，对数据集的子集进行消融研究是否公平，因为我将不得不使用不同的消融方法对其进行 3 - 4 次训练？    提交人    /u/Aromatic_Web749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1kzsh/p_ablation_study_using_a_subset_of_data/</guid>
      <pubDate>Thu, 28 Nov 2024 01:51:54 GMT</pubDate>
    </item>
    <item>
      <title>因果发现竞赛获奖论文讨论[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/</link>
      <description><![CDATA[我最近看到了这篇文章：https://thetourney.github.io/adia-report/，它描述了一场休闲发现竞赛的获胜方法。这不是我的专业，但我确实对 GNN 和因果推理有合理的理解。无论如何，从报告中我不明白获胜团队到底在做什么。有人可以链接到完整的论文或对他们正在做的事情有一个直观且可能的分步解释吗？    提交人    /u/www3cam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h1i0ji/causal_discovery_competition_winning_paper/</guid>
      <pubDate>Wed, 27 Nov 2024 23:22:43 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAMAS 2025 评论出炉！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/</link>
      <description><![CDATA[我找不到讨论主题，所以我想自己创建一个。     提交人    /u/E-Cockroach   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1h15k8k/d_aamas_2025_reviews_are_out/</guid>
      <pubDate>Wed, 27 Nov 2024 14:28:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gyhfxm/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 24 Nov 2024 03:15:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>