<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sat, 02 Dec 2023 09:12:05 GMT</lastBuildDate>
    <item>
      <title>[D] 寻找免费的Azure AI服务课程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188zn90/d_looking_for_free_azure_ai_service_courses/</link>
      <description><![CDATA[大家好， 您能与我分享一些有趣的免费课程，用于在 Azure AI 服务中训练模型吗？各部分的基础。我在 Microsoft 上找到了关于 ML 基础的非常基础的课程，而且练习很少。谢谢;-)   由   提交/u/Low_codedimsion  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188zn90/d_looking_for_free_azure_ai_service_courses/</guid>
      <pubDate>Sat, 02 Dec 2023 08:54:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] ChatGPT 但决策树？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188ygwe/d_chatgpt_but_decision_tree/</link>
      <description><![CDATA[更正标题：chatGPT 比决策树更好吗？ （抱歉） Tl;dr：与为顺序任务配置的决策树之类的东西相比，为 LLM（例如 chatGPT）提供支持的 Transformer/Decoder 架构是否本质上很强大/能够在给定数据中查找模式？还是只是更多数据==智能输出的游戏？也就是说，我们是否有可能重新制定注意力机制来适应决策树，并且它的性能与 NNetworks 一样好？ 当我第一次开始学习 ML 时，我使用了决策树大约 50 条引用的数据集。这棵树的目的是预测相对于给定单词的下一个单词。 （我可以让这棵树尽可能地“看起来”回到句子中，但是让它看起来太靠后会出现一些问题。如下所述。）。我的目标是创建一个“报价生成器”。它的工作效果出奇的好，特别是我发现它的一些行为非常有趣。 有趣的是： 1. 它的行为很像“自动完成”。在安卓中。 （所以我仍然认为他们使用决策树来自动完成。） 2.有时它会重复同一组单词，一次又一次形成一个循环。 3. 让它回顾过去导致它只能完美地预测自己数据集中的报价。它不会以这种方式预测新的报价。只有当我确保向它显示一两个单词时，它才能预测一些新的/乱码的内容。 现在，考虑到 ChatGPT 已经接受了 40-45TB 之类的训练。  &gt;问题： 1. 如果我给这个决策树提供相同数量的数据，它是否有可能表现得“智能”？ （对于普通人）chatGPT 的行为如何？ 2. 这部分有什么研究吗？ 注：虽然我非常喜欢法学硕士。我对他们实际上“更聪明”这一事实持高度批评态度。对我来说，与普通的神经网络/机器学习算法相比，它们看起来就像经典的机器学习模型，但由于内部注入了大量数据，它们不可能是坏的。 我个人认为这是人工智能领域最大的进步chatGPT 的重点不在于它的架构，而在于它对海量数据的广泛处理、清理和分析。或者我错了，带有注意力层的 Transformer 架构是否存在本质上的特殊之处，使其优于 CNN、决策树等其他算法。即使我们重新制定注意力能力以适应它们，这也无法实现。   由   提交/u/Xanta_Kross  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188ygwe/d_chatgpt_but_decision_tree/</guid>
      <pubDate>Sat, 02 Dec 2023 07:30:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 帮助理解 Dimenet 模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188wcvu/d_help_in_understanding_dimenet_model/</link>
      <description><![CDATA[dimenet 模型 link 用于了解分子的原子性质。其中包含方向信息。  我无法理解他们如何使用三个原子的嵌入来计算三元组之间的方向。由于嵌入是向量，因此它们可以获得方向，但是它们如何获得角度以及它们如何合并，然后我对球基函数的使用完全感到困惑。这些原子（节点）在论文中包含什么？只有原子类型作为向量？那么边缘呢？它是否包含成对距离作为向量？请澄清   由   提交 /u/specializedboy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188wcvu/d_help_in_understanding_dimenet_model/</guid>
      <pubDate>Sat, 02 Dec 2023 05:14:21 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我正在尝试改善我的预测值与实际值的分布</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188warj/p_im_trying_to_improve_my_predicted_vs_actual/</link>
      <description><![CDATA[    &lt; /a&gt;  我使用 Autogluon 在表格数据上训练模型 (WeightedEnsemble_L3)。这是一个回归问题。以下是评估结果： {&#39;root_mean_squared_error&#39;: -9.592466103848274, &#39;mean_squared_error&#39;: -92.0154059534781, &#39;mean_absolute_error&#39;: -7.8083721751898105, &#39;r2&#39;: 0.78427137067 0073、&#39;皮尔森&#39;：0.8990940522052712、&#39;中值绝对误差&#39; ：-6.87762451171875}  下图显示了预测值与实际值。虚线是y=x。从散点分布来看，云分布似乎可以逆时针旋转，然后预测值将更接近实际值。这种旋转将提高回归预测的准确性。我这样看对吗？有没有办法在 autogluon 中做到这一点？我不得不添加一个训练后软糖因素，因为我确信有更好的方法。  ​ https://preview.redd.it/rlrwxwopet3c1.png?width=571&amp;format=png&amp;auto=webp&amp;s=3abfdf6cedad258b4b115c8071b62d3d30b52d0f    由   提交 /u/BAMred   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188warj/p_im_trying_to_improve_my_predicted_vs_actual/</guid>
      <pubDate>Sat, 02 Dec 2023 05:10:54 GMT</pubDate>
    </item>
    <item>
      <title>采用数组API标准的交叉兼容评价指标[P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188vcnx/cross_compatible_evaluation_metrics_by_adopting/</link>
      <description><![CDATA[很高兴分享一个新的交叉兼容（NumPy、PyTorch、CuPy）实验指标包，ML 从业者可以尝试！ 检查我们的博客 - https://vectorinstitute.github.io/cyclops/blog Github - https://github.com/VectorInstitute/cyclops  &amp; #32；由   提交/u/Various-Art3382  /u/Various-Art3382 reddit.com/r/MachineLearning/comments/188vcnx/cross_known_evaluation_metrics_by_adopting/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188vcnx/cross_compatible_evaluation_metrics_by_adopting/</guid>
      <pubDate>Sat, 02 Dec 2023 04:15:46 GMT</pubDate>
    </item>
    <item>
      <title>[D]深入研究 Google Brain 团队的 Vision Transformer (ViT) 论文</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188pe7u/deep_dive_into_the_vision_transformer_vit_paper/</link>
      <description><![CDATA[我们每周五都有一个名为 Arxiv Dives 的阅读俱乐部，在那里我们回顾当今机器学习中使用的许多最先进技术的基础知识。上周我们深入探讨了“视觉变形金刚” 2021 年的论文，其中 Google Brain 团队针对 ResNets 进行了大规模 Transformer 训练基准测试。 尽管截至本周这还不是开创性的研究，但我认为随着人工智能的发展步伐，深入研究过去的工作非常重要以及其他人的尝试！很高兴退一步回顾基础知识并跟上最新和最好的内容。 如果有人觉得有帮助，请在此处发布注释并回顾一​​下： https://blog.oxen.ai/arxiv-dives-vision-transformers-vit/&lt; /p&gt; 也希望有人能加入我们周五的直播！我们有一个由 300 多名工程师和研究人员组成的非常稳定且有趣的团队。   由   提交 /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188pe7u/deep_dive_into_the_vision_transformer_vit_paper/</guid>
      <pubDate>Fri, 01 Dec 2023 23:16:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] 当元学习遇到在线和持续学习时：一项调查</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188o3jx/r_when_metalearning_meets_online_and_continual/</link>
      <description><![CDATA[   论文: https://arxiv.org/abs/2311.05241 摘要 ：  在过去的十年中，深度神经网络在使用涉及广泛数据集的小批量随机梯度下降的训练方案方面取得了巨大的成功。在这一成就的基础上，探索神经网络在其他学习场景中应用的研究激增。元学习是一个引起广泛关注的著名框架。通常被描述为“学会学习”，元学习是一种数据驱动的方法来优化学习算法。其他感兴趣的分支是持续学习和在线学习，两者都涉及使用流数据增量更新模型。虽然这些框架最初是独立开发的，但最近的工作已经开始研究它们的组合，提出新颖的问题设置和学习算法。然而，由于复杂性增加且缺乏统一术语，即使对于经验丰富的研究人员来说，辨别学习框架之间的差异也可能具有挑战性。为了促进清晰的理解，本文提供了一项全面的调查，使用一致的术语和正式的描述来组织各种问题设置。通过概述这些学习范式，我们的工作旨在促进这一有前途的研究领域的进一步进步。  https://preview.redd.it/pp2j7tz2dr3c1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=983e081c4b 4feabddb3457ba74d94202495be4a5&lt; /a&gt;   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188o3jx/r_when_metalearning_meets_online_and_continual/</guid>
      <pubDate>Fri, 01 Dec 2023 22:18:02 GMT</pubDate>
    </item>
    <item>
      <title>[P] 2023 年初，我在一个新的机器学习项目上投入了大量工作。现在我不知道该怎么办。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188m7hj/p_early_in_2023_i_put_in_a_lot_of_work_on_a_new/</link>
      <description><![CDATA[首先我想澄清这不是一个自我推销的帖子。我希望许多机器学习人员向我提出有关这个项目的问题或意见。关于我自己的一些背景。我确实使用 GPTQ 对 LLaMA 进行 4 位量化。 （https://github.com/qwopqwop200/GPTQ-for-LLaMa）。我已经深入研究人工智能多年了。 早在 2023 年初，我就创建了几个独特的系统，可以输出这样的视频 (https://www.youtube.com/watch?v=uoEd8WykzkY) 。这并不是说这是有史以来最好的视频，它主要是对其功能的技术演示。我在系统上做了一些推广，但大多数情况下只能根据我所知接触到机器人。我不知道该怎么办。我有一些创意，但想听听是否有人对这个系统感兴趣。我投入了大量的工作，希望看到它用于一些有趣的事情。 该项目的网站是基本和简单的，就像现在一样，实际上只是一个占位符和联系页面。  在发布了一堆由它制作的视频后，我根本没有得到太多回应。我认为有些人可能会将其与垃圾邮件或其他任何内容混淆。 几点是： - 可以完全自动化，几乎不需要人工干预 - 完全高清视频 - 独特的3D透视视频系统，可从2D图像生成3D视频 - 无需互联网连接 - 无需特殊硬件，即可使用单个 3090 或更好的处理器运行。   由   提交 /u/mentosorangemint   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188m7hj/p_early_in_2023_i_put_in_a_lot_of_work_on_a_new/</guid>
      <pubDate>Fri, 01 Dec 2023 20:56:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器人技术的惨痛教训</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</link>
      <description><![CDATA[对于这个 subreddit 中还没有读过惨痛教训的两个人，http://www.incompleteideas.net/IncIdeas/BitterLesson.html 但是，作为对机器人感知、规划和学习感兴趣的人，这一定适用吗？我不太确定，特别是在人类（非结构化）环境中的机器人的背景下，其政策涵盖的范围比工厂或仓库机器人要广泛得多。机器人必须应对现实世界的随机性和巨大差异性，其策略对于环境的变化具有稳健性。我可以想到一些想法，这些惨痛的教训可能适用，也可能不适用。  硬件限制。尽管将计算卸载到远程服务器绝对是一种选择，但机器人在与环境实时交互时可以在多大程度上依赖于此？没有可行的机器人能够存储数十亿个参数，即使只是为了推理。一段时间以来，摩尔定律的速度已经放缓。 数据。在我看来，这是一件大事。什么构成了训练机器人策略的良好训练数据？我们有足够的吗？当然，我们拥有良好的模型和足够的 CV 和语言数据，甚至丰田关于使用扩散模型进行抓取姿势的论文看起来也很有希望，但机器人政策必须将所有这些放在一起才能完成多模式任务。目前还没有用于多模式任务规划的庞大语料库，例如如何将倒一杯水等任务分解为具有多个子任务（抓杯、拾取、倒水等）的 HTN。  我之所以发这篇文章是因为我不确定。我可以看到法学硕士如何成为可以完成多项任务的通用机器人策略的基础，或者更高效的架构可以允许机器人使用更多计算。你有什么想法？   由   提交/u/n0ided_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188jwaw/d_the_bitter_lesson_for_robotics/</guid>
      <pubDate>Fri, 01 Dec 2023 19:14:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如果审稿人在反驳期间保持沉默，我们是否应该联系 AC？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</link>
      <description><![CDATA[在将我们的论文提交给 ICLR2024 并收到初步评审后，我们针对审稿人提出的所有观点提供了详细的反驳。然而，自从我们反驳之后，审稿人方面就完全沉默了。尽管最初的评论非常详细且反馈积极，但没有进一步的问题、评论或任何形式的参与。 在这种情况下，是否建议联系 AC 请求他们的干预鼓励审稿人参与？ 有人遇到过类似的情况吗？你做了什么？如果有任何建议，我们将不胜感激。   由   提交 /u/jzhoubu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188i5jk/d_should_we_contact_the_ac_if_reviewers_go_silent/</guid>
      <pubDate>Fri, 01 Dec 2023 18:00:00 GMT</pubDate>
    </item>
    <item>
      <title>[R] RETVec：弹性且高效的文本矢量化器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/</link>
      <description><![CDATA[星期五快乐， 非常高兴与大家分享 RETVec 的代码和模型，我们用于分类的新 SOTA 稳健文本标记器已可用在 Github 此处 和 NeurIPS 论文在这里。我们还通过 TFJS 为 TFLite 和网络提供本机支持。希望您会发现它对您的研究有用。如果您想尝试一下，我们有一本入门笔记本。  如果您有任何疑问，请告诉我们。 ​   由   提交/u/ebursztein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188gjpy/r_retvec_resilient_and_efficient_text_vectorizer/</guid>
      <pubDate>Fri, 01 Dec 2023 16:51:33 GMT</pubDate>
    </item>
    <item>
      <title>[P] Llama 微调速度提高 80%，内存减少 50%，精度损失 0%</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</link>
      <description><![CDATA[       嘿r/MachineLearning！ 我手动导出了反向传播步骤，做了一些链式矩阵乘法优化，用 OpenAI 的 Triton 语言编写所有内核，并进行更多数学和编码技巧，以使 QLoRA 在 Unsloth 上对 Llama 的微调速度提高 5 倍：https:// github.com/unslothai/unsloth！一些亮点：  速度提高 5 倍（5 小时到 1 小时） 使用内存减少 50% 精度损失为 0% 所有本地均在 NVIDIA GPU（Tesla T4、RTX 20/30/40、Ampere、 Hopper）免费！ QLoRA / LoRA 现在训练速度提高了 80%。  在 2 个 Tesla T4 上的 Slim Orca 518K 示例上通过 DDP 的 GPU，Unsloth 在 260 小时内在所有层上训练 4 位 QLoRA VS Huggingface 的原始实现需要 1301 小时。 Slim Orca 1301 小时到 260 小时 您可能（很可能不）记得来自 Hyperlearn 的我（https://github.com/danielhanchen/hyperlearn）是我几年前推出的，旨在通过数学和编码技巧使 ML 算法速度提高 2000 倍。 我通过 https://unsloth.ai/introducing 写了一篇关于所有手动手动导出反向传播的博客文章。&lt; /p&gt; 我为 Alpaca 编写了 T4 的 Google Colab：https://colab.research。 google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing，在单个 GPU 上将 Alpaca 的速度提高 2 倍。 在 Kaggle 上通过 DDP 上的 2 个 Tesla T4：https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle，微调 LAION 的 OIG 速度快 5 倍，Slim Orca 速度快 5 倍更快。 您可以通过以下方式在本地安装 Unsloth： pip install &quot;unsloth[cu118] @ git+https://github.com/unslothai/unsloth。 git” pip install “unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git”  目前我们仅支持 Pytorch 2.1 和 Linux 发行版 - 更多安装说明请参见 https://github.com/unslothai/unsloth/blob/main/README.md 我希望：  支持除Llama 风格模型（Mistral 等） 添加 sqrt 梯度检查点以再减少 25% 的内存使用量。 还有其他技巧！  谢谢一堆！！   由   提交 /u/danielhanchen   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188g31r/p_80_faster_50_less_memory_0_loss_in_accuracy/</guid>
      <pubDate>Fri, 01 Dec 2023 16:31:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] Meta的新语音模型（无缝）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</link>
      <description><![CDATA[Meta Research 刚刚发布了名为 Seamless 的新语音模型：https://ai.meta.com/research/seamless-communication/ 它支持多种语言的语音和文本输入和输出。从某种意义上说，它是一系列相关语音任务的通用模型。非常有趣！   由   提交 /u/semicausal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188fzoz/r_metas_new_speech_models_seamless/</guid>
      <pubDate>Fri, 01 Dec 2023 16:27:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 一些作者是否认真地添加了比需要的更多的数学知识，以使论文“看起来”更具开创性？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</link>
      <description><![CDATA[我最近注意到一种趋势，即作者在某些情况下添加了超出所需的形式主义（例如图表/图像就可以很好地完成工作）。  这是否是为了使论文看起来更好而添加了过多的数学知识，或者可能只是受到出版商的限制（无论论文必须坚持什么格式才能发表）？ &gt;   由   提交 /u/Inquation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/188d7qc/r_do_some_authors_conscientiously_add_up_more/</guid>
      <pubDate>Fri, 01 Dec 2023 14:29:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/17z08pk/d_simple_questions_thread/</guid>
      <pubDate>Sun, 19 Nov 2023 16:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>