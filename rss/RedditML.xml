<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 05 May 2024 15:14:39 GMT</lastBuildDate>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 博士后为血癌患者开发医疗机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cksxm2/r_postdoc_developing_medical_machine_learning_in/</link>
      <description><![CDATA[我们为丹麦淋巴癌研究 (DALY-CARE) 创建了多模式大规模数据资源，包括来自 13 个国家登记册的 65,000 多名个人 + 详细的电子数据健康记录数据。我们与 AZ 合作，AZ 正在聘请一名博士后同事来开发医学机器学习算法，以预测靶向治疗的临床结果。可以在此处提交申请https://careers.astrazeneca.com/job/gothenburg/postdoc-fellow-machine-learning-for-predicting-adverse-events-in-blood-cancer-treatments/7684/64381401040   由   提交 /u/Boring_Amoeba5297   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cksxm2/r_postdoc_developing_medical_machine_learning_in/</guid>
      <pubDate>Sun, 05 May 2024 14:48:53 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在创建神经网络时，是否有更系统的方法来选择层或架构的深度？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/</link>
      <description><![CDATA[所以我正在学习深度学习和神经网络，我对这部分真的有点困惑。我通常熟悉可用的层及其工作原理（至少是那些被广泛使用的层），但我仍然很难弄清楚在什么上使用什么。有没有更合乎逻辑或系统的方法来做到这一点？比如数学什么的？我很想尝试，但我只是想避免陷入困境，因为这个项目是在截止日期前完成的，而我并不对此感到失望    ;由   提交/u/PsychologicalAd7535   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckrzq6/d_is_there_a_more_systematic_way_of_choosing_the/</guid>
      <pubDate>Sun, 05 May 2024 14:04:05 GMT</pubDate>
    </item>
    <item>
      <title>[研究] 大语言和视觉模型中的创造性问题解决</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckrd1u/research_creative_problem_solving_in_large/</link>
      <description><![CDATA[代码：https://github .com/lnairGT/creative-problem-solving-LLMs 此存储库中提供的代码提示LLM（图像+文本提示）在需要的对象时识别创意对象替换（对象替换）缺失，例如用碗代替勺子。这项工作表明，通过相关对象特征（即可供性）增强的提示使法学硕士能够有效地推理对象替换。   由   提交 /u/IllustriousSir_007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckrd1u/research_creative_problem_solving_in_large/</guid>
      <pubDate>Sun, 05 May 2024 13:32:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 高效 MuZero 值前缀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckpgfd/d_efficient_muzero_value_prefix/</link>
      <description><![CDATA[我正在寻找关于 ow 值前缀在高效 muzero 中工作的更清晰的解释。值前缀是否完全取代了奖励预测，或者如果不是，它们如何协同工作？值前缀是用来预测未来值还是过去值？使用值前缀的具体 MCTS 步骤是什么？对我来说，它似乎应该用于反向传播，但我不清楚。值前缀是如何训练的？   由   提交 /u/_Hardric   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckpgfd/d_efficient_muzero_value_prefix/</guid>
      <pubDate>Sun, 05 May 2024 11:47:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] 数据科学家的真正价值从何而来？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cknrka/d_where_does_the_real_value_of_a_data_scientist/</link>
      <description><![CDATA[公司关心你能为他们做什么，并高度重视利润。在我看来，典型的软件工程师的价值在于：  他们可以快速交付代码 他们足够聪明  即它们是极其消耗性的，这是你永远不想成为的。数据科学家和软件工程师一样可以消耗吗？是什么让数据科学家在市场上变得不可替代和受欢迎？    由   提交/u/Error40404  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cknrka/d_where_does_the_real_value_of_a_data_scientist/</guid>
      <pubDate>Sun, 05 May 2024 09:54:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] NVIDIA GPU 基准测试与比较</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cklpyd/d_nvidia_gpu_benchmarks_comparison/</link>
      <description><![CDATA[https://tensordock.com/benchmarks 过去几个小时在 TensorDock 云上汇总了有关 vLLM（针对 Llama 7B 和 OPT-125M）和 Resnet-50 训练性能的一些数据。  vLLM 数据 100% 开箱即用，来自 此存储库的批量大小为 2048 。  我的经验：  H100 和 A100 的性能是无与伦比的，但低端 RTX 卡的性价比相当不错。甚至 L40 和 RTX 6000 Ada 在某些任务上的表现也优于 A100，因为它们比 A100 更新了 1 代。 如果您的应用程序不需要 80GB VRAM，那么不使用 80GB VRAM 卡可能是有意义的 独立 H100 性能并不像我想象的那么强大。 H100 的性能受到 LLM 推理的内存带宽的瓶颈，因此对于 vLLM，H100 的速度仅比 A100 快 1.8 倍。当互连在一起时，H100 确实表现得更好，但我今天没有进行基准测试。  CPU 比我想象的更重要。 OPT-125M 与 Llama 7B 的性能比较非常有趣...不知何故，所有 GPU 在 OPT-125M 上的表现都相似，我认为这是因为使用的 CPU 时间比 GPU 时间相对多，因此 GPU 性能差异在宏伟的计划。 市场定价本身相当不错。如果将 GPU 分组到 VRAM 中，则所有具有相似 VRAM 数量的 GPU 都具有相似的性价比。 如果您有足够的批量大小，自托管可以为您节省 $$$。如果您构建自己的推理 API，您可以仅使用 50% 的批次为 LLM 提供服务，并且与按代币付费的 API 相比可以节省资金（如果您 100% 使用我们，我们 [TensorDock] 每百万个 Llama 7 代币的成本不到 0.07 美元） )  --  让我知道接下来要对哪个 GPU 进行基准测试，我会添加它！或者让我知道一些其他需要衡量的工作量，我也很乐意为此添加一个新部分。  P.S.我们以 1.80 美元/小时的价格添加了一些 H100，供任何有幸获得它们的人使用！    由   提交/u/jonathan-lei   reddit.com/r/MachineLearning/comments/1cklpyd/d_nvidia_gpu_benchmarks_comparison/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cklpyd/d_nvidia_gpu_benchmarks_comparison/</guid>
      <pubDate>Sun, 05 May 2024 07:25:33 GMT</pubDate>
    </item>
    <item>
      <title>[N] ICML 2024 第一届情境学习研讨会</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckl6fj/n_1st_workshop_on_incontext_learning_at_icml_2024/</link>
      <description><![CDATA[ 由   提交/u/Yossarian_1234   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckl6fj/n_1st_workshop_on_incontext_learning_at_icml_2024/</guid>
      <pubDate>Sun, 05 May 2024 06:48:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] 仔细检查大型语言模型在小学算术中的表现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2405.00332 摘要：  大型语言模型 (LLM) 在许多数学推理基准测试中取得了令人瞩目的成功。然而，人们越来越担心，这种表现实际上反映了数据集污染，即与基准问题非常相似的数据泄漏到训练数据中，而不是真正的推理能力。为了严格调查这一说法，我们委托Grade School Math 1000 (GSM1k)。GSM1k 旨在反映已建立的 GSM8k 基准测试的风格和复杂性，GSM8k 基准测试是衡量基本数学推理的黄金标准。我们确保这两个基准在重要指标（例如人工解决率、解决方案中的步骤数、答案大小等）方面具有可比性。在 GSM1k 上评估领先的开源和闭源 LLM 时，我们观察到准确率下降高达 13%，其中几个模型系列（例如 Phi 和 Mistral）显示出几乎所有模型大小的系统过度拟合的证据。同时，许多模型，尤其是前沿模型（例如 Gemini/GPT/Claude）显示出极小的过度拟合迹象。进一步的分析表明，模型从 GSM8k 生成示例的概率与其在 GSM8k 和 GSM1k 之间的性能差距之间存在正相关关系（Spearman&#39;s r2=0.32），这表明许多模型可能已经部分记住了 GSM8k。     提交人    /u/SurveySea7570   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckkf5f/r_a_careful_examination_of_large_language_model/</guid>
      <pubDate>Sun, 05 May 2024 05:57:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] [R] 是否有任何方法/作品可以从 CLIP/OpenCLIP 图像编码器中提取高质量的密集特征图，而无需大规模微调？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckiamf/d_r_are_there_any_methodsworks_that_enable/</link>
      <description><![CDATA[嗨，如标题所述，我很好奇是否存在这样的方法。我们知道（经过训练的）CLIP 的图像和文本编码器都输出在潜在空间中对齐的一维向量，这使得可以轻松计算一批图像和文本之间的相似性。然而，在许多视觉应用中，希望获得形状为 C*H*W 的 3D 特征图。理想情况下，如果该特征图中每个空间位置的向量与 CLIP 最终（注意池化）编码的一维图像向量一样高质量，我们可以计算每个位置处的特征图和文本的相似度，并得到图像和文本之间的某种 2D 注意力/相似性/分数图，这对于许多下游任务非常有帮助。 我知道 CLIP 已应用于许多（开放词汇/零- shot) 检测/分割任务，其中一些作品探索了我上面提到的类似想法，但大多数都非常复杂，不能以即插即用的方式使用。最接近的作品是这项作品和 MaskCLIP，但我发现很难复制他们的结果。 我很好奇你对此有何看法。我知道对比预训练的目标并不能真正保证良好的密集特征图，但我想知道是否缺少任何东西。    ;由   提交/u/Tensor_Devourer_56   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckiamf/d_r_are_there_any_methodsworks_that_enable/</guid>
      <pubDate>Sun, 05 May 2024 03:41:20 GMT</pubDate>
    </item>
    <item>
      <title>[P] [D] 推理时间是边缘/移动机器学习模型的重要性能指标吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckcn7b/p_d_is_inference_time_the_important_performance/</link>
      <description><![CDATA[我目前正在参与一个项目，旨在让机器学习工程师了解他们的模型在各种移动设备上的表现。 将机器学习模型嵌入到应用程序中并在不需要任何 API/网络连接的情况下使用它们已开始成为一种非常流行的做法。您可以看到大多数示例，特别是对于大量使用计算机视觉的应用程序。将每张图像传递到云端进行处理是完全不可接受的，数据量大且速度慢。随着该领域的最新改进，将机器学习模型嵌入到应用程序中变得更容易、更可取。 但这会带来另一个代价。  有 1000 个移动设备配备了不同的芯片组，如 Qualcomm、Exynos、Snapdragon 等。它们还配备了不同的 GPU 功能以及不同的操作系统版本。 所有这些组合很可能会产生一些不确定性。我的模型的性能与办公室 Android 测试手机中的性能相同吗？ 作为首席移动工程师在一家计算机视觉和机器学习初创公司工作了 3 年多，并在应用程序中嵌入了 10 个模型，这个问题的答案对我来说非常清楚。 不，我的模型在小米 Android 11 手机上的表现与在办公室三星 Android 13 上的表现不同。而且通常您甚至不会知道这一点。 ML 工程师会与应用程序环境高度隔离。他们可以使用云中的工具来衡量机器学习模型的准确性、召回率等性能。这些都是非常重要的指标。但是，他们已经对此进行了测量/评估。 当谈到推理时间时，它在很大程度上取决于它所运行的系统。让办公室中的每台移动设备都可用是不可行的。 为了解决这个问题，我们决定开发移动 SDK 和一个用于收集/可视化一些指标的平台。我们已经决定，最重要的指标（问题的核心）是推理时间。 我想问问大家这是否有意义且合理。您认为机器学习工程师还会对其他重要指标感兴趣吗？  我们准备的 SDK 收集所有设备相关的元数据（可用内存、CPU 使用情况、操作系统、API 级别、电池等）和推理时间参数，并显示如下图表：  操作系统与推理时间 设备型号与推理时间 单个系统中的可用内存与推理时间会话等    由   提交 /u/orcnozyrt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckcn7b/p_d_is_inference_time_the_important_performance/</guid>
      <pubDate>Sat, 04 May 2024 22:44:14 GMT</pubDate>
    </item>
    <item>
      <title>[D]任意维等变神经网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckat4d/d_anydimensional_equivariant_neural_networks/</link>
      <description><![CDATA[我发现这篇论文非常有趣。在使用 covnet 进行计算机视觉时，我们做出了与作者相同的假设。我想知道我们是否可以扩展计算机视觉用例 Abstract 传统的监督学习旨在通过将函数拟合到一组具有固定维度的输入输出对来学习未知映射。然后在相同维度的输入上定义拟合函数。然而，在许多设置中，未知映射采用任何维度的输入；示例包括在任意大小的图上定义的图参数以及在任意数量的粒子上定义的物理量。我们利用代数拓扑中新发现的现象（称为表示稳定性）来定义等变神经网络，该网络可以使用固定维度的数据进行训练，然后扩展到接受任何维度的输入。我们的方法是用户友好的，只需要网络架构和等方差组，并且可以与任何训练程序相结合。我们提供了我们方法的简单开源实现，并提供了初步的数值实验。    由   提交/u/No-Natural36  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckat4d/d_anydimensional_equivariant_neural_networks/</guid>
      <pubDate>Sat, 04 May 2024 21:19:19 GMT</pubDate>
    </item>
    <item>
      <title>大型网络攻击数据集是如何形成的？[p]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ck4ozs/how_are_large_network_attack_datasets_made_p/</link>
      <description><![CDATA[嗨，我正在开发一个用于网络入侵检测的机器学习系统。我遇到过大量的免费数据集，它们确实很有帮助，但我的项目已经到了需要创建自己的数据集的地步。我看到网络上有数百万次模拟攻击，无法想象这是手工完成的。如果有人有任何想法，我们将不胜感激。谢谢   由   提交 /u/OpeningDirector1688   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ck4ozs/how_are_large_network_attack_datasets_made_p/</guid>
      <pubDate>Sat, 04 May 2024 16:46:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] RAG 目前的可靠性如何？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ck0tnk/d_how_reliable_is_rag_currently/</link>
      <description><![CDATA[我猜RAG的本质是关于  根据提示检索相关文档 将文档放入上下文窗口  数字2非常直接，而数字1是我认为更多重要事情发生的地方。 IIRC，大多数情况下，我们在提示嵌入和文档嵌入之间进行相似性搜索，并检索 k 个最相似的文档。 好的，此时我们有 k 个文档并将它们放入上下文中。现在是时候让 LLM 根据我的提示和 k 个文档给出答案了，鉴于检索到了正确的文档，一个好的 LLM 应该能够做到这一点。 我尝试做一些业余爱好项目与 LlamaIndex 但没有让它工作得那么好。例如，我尝试使用 NFL 统计数据作为我的数据（每个球员一行，每个特征一列），并希望 GPT-4 与这些文档一起能够正确回答我至少 95% 的问题，但它更像是70%，这非常糟糕，因为我觉得这是一个相当基本的项目。问题是“球员 x 在 y 赛季完成了多少次达阵”。答案各不相同，从正确到说信息不可用，再到产生错误答案的幻觉。 希望我只是以次优的方式做某事，但这让我想到 RAG 的使用有多么广泛在世界各地生产。市场上有哪些成功利用 RAG 的应用程序？我假设像 perplexity.ai 这样的东西正在使用它，当然还有所有其他以某种方式使用浏览的聊天机器人。提到的一个明显的应用程序通常是嵌入公司文档，然后拥有一个使用 RAG 的内部聊天机器人。它部署在任何地方吗？在我的公司没有，但我可以看到它很有用。 基本上，RAG 主要是理论上听起来不错并且目前正在大肆宣传的东西，还是实际上在世界各地的生产中使用的东西？&lt; /p&gt;   由   提交/u/lapurita  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ck0tnk/d_how_reliable_is_rag_currently/</guid>
      <pubDate>Sat, 04 May 2024 13:52:33 GMT</pubDate>
    </item>
    <item>
      <title>[D] AI 模型中的“它”真的只是数据集吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cjxh9u/d_the_it_in_ai_models_is_really_just_the_dataset/</link>
      <description><![CDATA[   /u/vijayabhaskar96   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cjxh9u/d_the_it_in_ai_models_is_really_just_the_dataset/</guid>
      <pubDate>Sat, 04 May 2024 10:47:31 GMT</pubDate>
    </item>
    </channel>
</rss>