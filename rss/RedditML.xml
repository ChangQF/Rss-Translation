<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Mon, 03 Jun 2024 15:15:50 GMT</lastBuildDate>
    <item>
      <title>[D] MobileNetv3 图像分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d7647x/d_mobilenetv3_image_classification/</link>
      <description><![CDATA[大家好。这里有人用 mobilenetv3 而不是 mobilenetv2 做过图像分类吗？我很难训练我的模型，因为我是从头开始的，而且不知道从哪里开始。有没有关于我可以遵循和改进的链接或教程？有什么建议吗？谢谢。    提交人    /u/Happy_Yak4619   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d7647x/d_mobilenetv3_image_classification/</guid>
      <pubDate>Mon, 03 Jun 2024 14:42:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] PINN 用于金属的延性断裂</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d755gi/d_pinns_for_ductile_fracture_of_metals/</link>
      <description><![CDATA[嗨，我想知道 FE 软件 abaqus 中可用的金属延性断裂是否可以使用 PINN 建模？这些模型通常是在测试凹槽试样后建立的，这会产生不同的三轴性水平。这需要对每个试样进行大量的测试和建模，以找到断裂应变和三轴性之间的关系……PINN 中应包含哪些物理特性？    提交人    /u/Cute-Somewhere-6055   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d755gi/d_pinns_for_ductile_fracture_of_metals/</guid>
      <pubDate>Mon, 03 Jun 2024 14:01:02 GMT</pubDate>
    </item>
    <item>
      <title>[D] 使用 Snowflake 构建数据产品 | 第 1 部分：利用现有堆栈</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d72uvt/d_build_data_products_with_snowflake_part_1/</link>
      <description><![CDATA[优化 Snowflake 成本，集成 Snowflake 源并加快业务成果！ 在本系列中，我们想强调利用现有堆栈开始使用数据产品的便利性。这篇文章非常适合那些希望采用数据产品方法同时扎根于 Snowflake、Dbt、Databricks 或 Tableau 等大型投资的数据领导者。我们将以最喜欢的 Snowflake 开始！ 在此处阅读完整文章：https://moderndata101.substack.com/p/build-data-products-with-snowflake    提交人    /u/growth_man   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d72uvt/d_build_data_products_with_snowflake_part_1/</guid>
      <pubDate>Mon, 03 Jun 2024 12:09:53 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 最值得购买门票的 AI/ML 会议（2024 年 7 月至 11 月）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d72jdd/discussion_best_aiml_conferences_to_buy_tickets/</link>
      <description><![CDATA[大家好！我住在挪威，想购买 2024 年 7 月至 11 月期间举行的 AI/ML 或一般技术会议的门票。 我特别有兴趣了解新的 ML 技术，并专注于在生产环境中设置 ML。我担心参加的活动可能过于关注大型语言模型 (LLM)。我渴望找到涵盖广泛实际 ML 应用的会议。 有人可以推荐在此期间符合这些兴趣的活动吗？ 提前致谢！    提交人    /u/UnhappyCucumber   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d72jdd/discussion_best_aiml_conferences_to_buy_tickets/</guid>
      <pubDate>Mon, 03 Jun 2024 11:52:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 耳语 + 本地 LLM + xtts 流媒体</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d729sm/d_whisper_local_llm_xtts_streaming/</link>
      <description><![CDATA[有没有什么好的项目可以做到这一点？    提交人    /u/Raise_Fickle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d729sm/d_whisper_local_llm_xtts_streaming/</guid>
      <pubDate>Mon, 03 Jun 2024 11:37:20 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无悔等待模型：最大化小费的多臂老虎机方法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d71lf4/r_the_no_regrets_waiting_model_a_multiarmed/</link>
      <description><![CDATA[        提交人    /u/TobyWasBestSpiderMan   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d71lf4/r_the_no_regrets_waiting_model_a_multiarmed/</guid>
      <pubDate>Mon, 03 Jun 2024 10:56:08 GMT</pubDate>
    </item>
    <item>
      <title>[D] 博弈论/匹配理论/多智能体系统的最佳人工智能会议？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6zoz1/d_best_ai_conferences_for_game_theorymatching/</link>
      <description><![CDATA[嗨， 我相信我有一篇非常出色的论文想要提交给人工智能领域的一个著名会议。我的领域主要是匹配理论/计算博弈论/多智能体系统（例如，如果有人熟悉的话，稳定匹配）。你认为最负盛名的相关会议是什么？我在考虑 IJCAI、AAMAS 或 EC。AAAI 和 ICML 也可能在考虑范围内，但据我所知，我所在领域的论文很少在那里发表，所以这可能与他们的主题相差太远了。谢谢您的帮助 :)    提交人    /u/Cloud7889   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6zoz1/d_best_ai_conferences_for_game_theorymatching/</guid>
      <pubDate>Mon, 03 Jun 2024 08:39:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您是否认为 Meta ImageBind 对于多向量嵌入而言比 CLIP 更好？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6xocn/d_would_you_say_meta_imagebind_is_better_than/</link>
      <description><![CDATA[我正在研究多模式嵌入模型，然后遇到了 Imagebind。它似乎很有趣，但我找不到很多关于它与 CLIP 相比如何的评论或基准。我读到 Meta 刚刚改进了 CLIP 或对其进行了扩展。 有没有人在这个领域工作过并且同时使用过这两种工具？​​    提交人    /u/CaptTechno   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6xocn/d_would_you_say_meta_imagebind_is_better_than/</guid>
      <pubDate>Mon, 03 Jun 2024 06:11:24 GMT</pubDate>
    </item>
    <item>
      <title>[D]：NLP 之外的 Transformer 键、查询、值直觉</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6tpyj/d_transformer_keys_queries_values_intuitions/</link>
      <description><![CDATA[通常 K = V，如果 Q =/= K，则为交叉注意力，否则为自注意力。Transformer 块基本上在每个块之后用上下文丰富 V（值）向量。 在我的例子中，我有 Q =/= K =/= V，虽然从数学上来说没问题，但我还没有遇到过这样做的应用程序。  我想要这样一种行为：当 K = Q 时，V_new = Transformer (Q, K, V) = V，因为 Q 与 K 匹配。使用通常的缩放点注意力机制不会出现这种情况，但我猜这可以通过其他组件（如 MLP）来克服。 那么我的问题是：1. 是否存在修改后的注意力机制（除了计算成本高昂的距离矩阵）可以在 K=Q 时返回 V 2. 当 K=V 时，由于 V_new = Transformer(Q, V, V) 在每个时间步长上都是如此，因此如何重复应用 Transformer 块就变得很简单了。但是如果 K=/=V，在 V_new = Transformer(Q,K,V) 之后，我不清楚是否应该执行 V_new_new = Transformer(Q,K, V_new) 感谢讨论。我想知道您是否有处理类似情况的经验以及处理情况如何。    提交人    /u/MysticalDragoneer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6tpyj/d_transformer_keys_queries_values_intuitions/</guid>
      <pubDate>Mon, 03 Jun 2024 02:14:39 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在创建 FAISS 索引时，有没有什么方法可以更快地执行编码？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6s7au/d_is_there_any_way_to_perform_encoding_a_bit/</link>
      <description><![CDATA[我目前正在训练一个文本嵌入模型，我正在使用 MTEB 或 MIRACL 等基准对其进行评估。我引用的大多数代码都使用 FAISS 索引来搜索结果，这是有道理的。 问题是，在构建 FAISS 索引时，文本编码花费的时间太长了。我目前使用一台带有四个 A6000 GPU 设备的机器，并实现了数据并行性来执行分布式推理，但即便如此，也需要大约 8 个小时才能获得大约 140 万个文档的嵌入向量。 这意味着我在每个时期后进行评估的典型工作流程变得有点不可行。 我考虑过想出其他方法，比如使用较小的语料库进行中间评估，但我真的不想这样做。 还有其他方法可以更快地完成此操作吗？谢谢。    由   提交  /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6s7au/d_is_there_any_way_to_perform_encoding_a_bit/</guid>
      <pubDate>Mon, 03 Jun 2024 00:54:30 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 为什么下一个标记预测对推荐系统不起作用？（或者我错了？）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6qfbc/discussion_why_next_token_prediction_doesnt_work/</link>
      <description><![CDATA[我正在开展一个研究项目，旨在应用下一个标记预测模型来构建/改进推荐系统。作为一项可行性评估研究，我使用 Instacart 数据集构建并训练了一个 GPT 模型来预测下一个要购买的产品。更具体地说，我将每个 product_id 视为一个“单词”，将每个订单视为一个“句子”，将每个用户的交易历史记录视为一个“文档”。 然而，在 T4 GPU 上训练 4 小时后，评估集上 10 的平均精度（MAP@10）仍然只有 0.075。作为比较，基线热门产品（用户个人交易历史中最受欢迎的 10 种产品）的 MAP@10 已经为 0.251。 虽然我可以看到改进模型的一两种方法，但与非常简单的基线相比，低性能确实令人沮丧，让我认为这种方法根本不可行。我想讨论几点：  下一个标记预测（特别是仅解码器的 Transformer 架构）是否真的不适用于任何规模的问题？ 支持：文本数据和电子商务交易数据之间存在很大差异，因此适用于文本的方法可能不适用于交易，这并不奇怪。 反对：4 小时的训练只有 1.5 个 epoch，因此当前模型可能拟合不足。因此，低性能可能只是训练时间的函数，如果我训练 2 天，它可能会有所改善 我应该阅读哪些资源？我知道我采用的方法类似于基于会话的 recsys 模型，但我只找到一篇论文 HierTCN（You et al.，WWW 2019）。如果能提供任何其他建议，我将不胜感激。 有什么建议可以帮助更好/更快地训练模型？目前的配置是： 数据：vocab_size = 50000（50K 个产品）、200K 个用户（每个 epoch 为 200K 个训练样例） 模型：n_layer=9、d_model=512、n_head=16（类似于 Gopher 44M 参数模型）、block_size=1024 训练：batch_size = 4、learning_rate = 5e-4、optimizer = AdamW  谢谢！    submitted by    /u/Pancake502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6qfbc/discussion_why_next_token_prediction_doesnt_work/</guid>
      <pubDate>Sun, 02 Jun 2024 23:24:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] Moonlighter 商店模拟中的贝叶斯老虎机商品定价</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/</link>
      <description><![CDATA[      我建了一个玩具店，模仿Moonlighter 游戏和贝叶斯匪徒代理通过 Thompson 抽样选择和定价待售物品。  随着模拟的进行，客户对这些物品在其货架价格的反应（即“生气”、“悲伤”、“满足”、“欣喜若狂”）更新了理想（即最高）价格概率分布（即后验）。  该算法探索了物品的理想价格，并迅速找到了当时理想价格最高的物品组，然后将其出售。 这个过程一直持续到所有物品售出。  该图表示待售物品之间的竞争。  这些点是从每个竞赛（x 值）中每个物品（颜色）的理想价格分布（即后验）中抽样的价格（y 值）。  在每次 Thompson 抽样竞赛中，抽样价格最高的获胜者最终被摆上货架。  我提到的客户对货架上商品的反应更新了这些分布的界限，用相同颜色的线条表示。 有关更多信息、更多图表以及包含工作代码和带有 Pandas/Matplotlib 代码的 Jupyter 笔记本（用于生成图表）的相应 Github 存储库的链接，请参阅我的文章：https://cmshymansky.com/MoonlighterBayesianBanditsPricing/?source=rMachineLearning    提交人    /u/JaggedParadigm   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6idf5/p_baysian_bandits_item_pricing_in_a_moonlighter/</guid>
      <pubDate>Sun, 02 Jun 2024 17:25:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 记录每一项机器学习资源或接受知识随时间流失的困境</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/</link>
      <description><![CDATA[我知道这可能是一个奇怪的话题，但我仍然认为这是一个重要的讨论，因为我们在这个领域不断学习。 机器学习是一个广阔的领域，与许多其他学科紧密交织在一起。仅我的硕士学位就涵盖了统计、优化、逆数据模拟、MLOps、软件工程、基于代理的建模、语义网、深度学习、时间序列等主题……这些领域中的每一个都有自己的子领域，人们可以投入一生去探索。 我意识到，除非你每天练习一个主题，否则你从书籍、认证、文章、论文、播客和视频中获得的知识最终会消失。四年前，这种认识促使我发现了 Obsidian，它极大地改变了我获取和保留信息的方式。我现在会记录我所获取的所有内容，尤其是工作之外我感兴趣的主题。就像一个“第二大脑”。如果没有这种做法，我发现信息很快就会消失。 事实上，我花了无数的时间研究物理、历史、认识论、哲学和许多其他学科的内容。然而，我曾经知道的东西只有一小部分留了下来。这让我陷入了两难境地：我应该投入大量时间来捕捉知识系统中的每一项资源，以确保我可以随着时间的推移而保留下来，还是尽快消耗资源，因为它们会消失（）“为了好玩”或当我的时间有限时）？ 我不想让这篇文章太长，但我确实感觉到花时间处理信息的好处，比如在读书的时候。大规模地组织和连接知识通常很有挑战性，但也很有回报，因为它有助于建立对某个主题的深刻理解。此外，当您需要刷新记忆时，如果您已经完成了这项“预处理”工作，而不是再次浏览互联网/书籍，那么“成本”会低得多。我不是简单地复制/粘贴文本，而是根据我已经了解的主题来定制我所捕获的内容。 但是，这个领域有太多东西需要学习，即使是数学或统计学等基础知识。我有时会质疑这种方法是否可持续。例如，Sebastian Raschka 等人撰写的《使用 PyTorch 和 Scikit-Learn 进行机器学习》一书长达 700 页。想象一下从这样一本全面的书中捕捉每一条信息需要花费的时间（而且这只是其中之一！）。记笔记还会迫使你彻底理解材料，包括每个方程式，否则笔记就毫无用处了。 我不主张二元方法；我经常找到妥协。但我很好奇你学习和消费信息的方法。你如何平衡保留知识的需求与时间和精力的实际限制？    提交人    /u/CrimsonPilgrim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6hagr/d_the_dilemma_of_taking_notes_on_every_ml/</guid>
      <pubDate>Sun, 02 Jun 2024 16:36:03 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励其他创建新帖子的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6f7ad/d_simple_questions_thread/</guid>
      <pubDate>Sun, 02 Jun 2024 15:00:19 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Tangles：Diestel 在书中宣布了一种新的数学 ML 工具</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</link>
      <description><![CDATA[      大家好，我想分享一本社区可能会感兴趣的新书！ 图论学家 Diestel 写了一本面向 ML 社区（及其他人）的书：  缠结：经验科学中人工智能的结构化方法 Reinhard Diestel，剑桥大学出版社 2024  ----- 出版商简介： 缠结提供了一种在不精确数据中识别结构的精确方法。通过将经常一起出现的特质分组，它们不仅可以揭示事物的集群，还可以揭示其特质的类型：政治观点、文本、健康状况或蛋白质的类型。缠结为人工智能提供了一种新的结构化方法，可以帮助我们理解、分类和预测复杂现象。 这已成为可能，这是由于缠结的数学理论最近被公理化，这使得缠结的应用范围远远超出了图论的起源：从数据科学和机器学习中的聚类到预测经济学中的客户行为；从 DNA 测序和药物开发到文本和图像分析。 这是首次探索此类应用。假设只具备本科数学基础知识，那么缠结理论及其潜在含义将对科学家、计算机科学家和社会科学家开放。 ----- 电子书以及包括教程在内的开源软件可在 tangles-book.com 上找到。 注意：这是一本“外展”书，主要不是关于缠结理论，而是关于以多种意想不到的方式和领域应用缠结。图中的缠结在 Diestel 的《图论》第 5 版中介绍。 目录和数据科学家简介（Ch.1.2）可从 tangles-book.com/book/details/ 和 arXiv:2006.01830 获得。第 6 章和第 14 章介绍了一种基于缠结的新软聚类方法，与传统方法截然不同。第 7-9 章涵盖了第 14 章所需的理论。 tangles-book.com 的软件部分表示，他们邀请在具体项目上进行合作，以及为他们的 GitHub 软件库做出贡献。  https://preview.redd.it/ysj91dw2o54d1.png?width=2074&amp;format=png&amp;auto=webp&amp;s=dd7ea6c2671ef83a5be77739e9ed6e3d6169c1d2 ​ ​    提交人    /u/Prestigious_Ship_238   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1d6cq0n/research_tangles_a_new_mathematical_ml_tool_in/</guid>
      <pubDate>Sun, 02 Jun 2024 12:56:49 GMT</pubDate>
    </item>
    </channel>
</rss>