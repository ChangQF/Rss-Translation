<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Fri, 26 Apr 2024 03:15:24 GMT</lastBuildDate>
    <item>
      <title>[D] 临床试验中的机器学习</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd9ucz/d_ml_for_clinical_trials/</link>
      <description><![CDATA[大家好，我正在筹备一家将 ML 和 NLP 应用于患者数据的公司，目的是找到适合临床试验的患者。我正在寻找联合创始人。如果您有兴趣或认识可能感兴趣的人，请直接给我发私信。很乐意聊聊。谢谢    提交人    /u/another_african   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd9ucz/d_ml_for_clinical_trials/</guid>
      <pubDate>Fri, 26 Apr 2024 02:16:30 GMT</pubDate>
    </item>
    <item>
      <title>[P] 多头专家混合 - https://arxiv.org/pdf/2404.15045 中建议的密集子代币路由的实现</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd42cp/p_multihead_mixture_of_experts_implementation_of/</link>
      <description><![CDATA[我的朋友在这篇 arxiv 论文中实现了 Multihead Mixture of Experts 的方法 https://arxiv.org/pdf/2404.15045 他希望我与你分享！ https://github.com/lhallee/Multi_Head_Mixture_of_Experts__MH-MOE 尝试一下。让我知道您的想法，我会将其传递给他。   由   提交/u/Prudent_Student2839   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd42cp/p_multihead_mixture_of_experts_implementation_of/</guid>
      <pubDate>Thu, 25 Apr 2024 22:00:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] HyenaDNA 和 Mamba 不擅长顺序标记？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/</link>
      <description><![CDATA[大家好，我一直在研究使用 DNA 序列作为输入的顺序标记。最近发布了 2 个基础模型 HyenaDNA（基于 Hyena Operator）和 Caduceus（基于 mamba），我使用了预训练模型和从头开始的模型，即使使用预训练模型，性能也很糟糕。  有人有此类模型的经验吗？性能下降的潜在原因是什么？我在少数群体中的成绩实际上为零？曼巴在阶级不平衡问题上处理得不好吗？   由   提交/u/blooming17  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cd13kf/d_hyenadna_and_mamba_are_not_good_at_sequential/</guid>
      <pubDate>Thu, 25 Apr 2024 20:02:31 GMT</pubDate>
    </item>
    <item>
      <title>[P] 基于图的神经网络的药物毒性预测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cczqej/p_drug_toxicity_prediction_model_with_graphbased/</link>
      <description><![CDATA[这是我编写/训练的一个小型药物毒性预测 GNN 模型 repo: https://github.com/Null-byte-00/有毒-预测-gnn    由   提交 /u/Soroush_ra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cczqej/p_drug_toxicity_prediction_model_with_graphbased/</guid>
      <pubDate>Thu, 25 Apr 2024 19:10:32 GMT</pubDate>
    </item>
    <item>
      <title>[D] 面对不可能的机器学习问题，您有哪些恐怖经历</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/</link>
      <description><![CDATA[机器学习非常擅长解决一系列小众问题，但大多数技术细微差别都被技术兄弟和管理者忽视了。您被告知要解决哪些问题是不可能的（没有数据、无用的数据、不切实际的期望）或机器学习的误用（您能让这个法学硕士做所有的会计工作吗）。    由   提交 /u/LanchestersLaw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccz2cq/d_what_are_your_horror_stories_from_being_tasked/</guid>
      <pubDate>Thu, 25 Apr 2024 18:45:26 GMT</pubDate>
    </item>
    <item>
      <title>因果机器学习数据集 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccyi66/datasets_for_causal_ml_d/</link>
      <description><![CDATA[有人知道有哪些数据集可用于因果推理吗？我想探索双重稳健的 ML 文献中的方法，并且我想通过处理一些数据集并学习 econML 软件来补偿我的学习。 有谁知道任何数据集，特别是在营销/定价/广告的背景是应用因果推理技术的良好来源？我也对其他数据集持开放态度。    由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/1ccyi66/datasets_for_causal_ml_d/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccyi66/datasets_for_causal_ml_d/</guid>
      <pubDate>Thu, 25 Apr 2024 18:24:49 GMT</pubDate>
    </item>
    <item>
      <title>[P] Dreambooting MusicGen</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/</link>
      <description><![CDATA[使用此存储库，只需几分钟即可在小型消费级 GPU 上Dreambooth MusicGen 模型套件：https://github.com/ylacombe/musicgen-dreamboothing 该项目的目标是提供工具可以轻松微调和dreamboothMusicGen模型套件，只需很少的数据，并利用一系列优化和技巧来减少资源消耗，谢谢到 LoRA 适配器。 例如，模型可以是很好的 -调整特定的音乐流派或艺术家以给出以该给定风格生成的检查点。目的还在于轻松共享和构建这些训练有素的检查点， 具体来说，这涉及：  使用尽可能少的数据和资源可能的。我们正在讨论的是 A100 上的微调仅需 1500 万，GPU 利用率低至 10GB 至 16GB。 借助 Hugging Face Hub。 （可选）生成自动音乐描述 （可选）在 类似 Dreamambooth 的时尚，其中一个关键字会触发特定风格的生成  Wandb 的示例训练运行类似于此处。   由   提交/u/Sufficient-Tennis189  /u/Sufficient-Tennis189 reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccxca7/p_dreamboothing_musicgen/</guid>
      <pubDate>Thu, 25 Apr 2024 17:43:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 推测流：无需辅助模型的快速 LLM 推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvzyw/r_speculative_streaming_fast_llm_inference/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.11131 摘要：  推测性解码是一种加速推理的重要技术。基于辅助草稿模型预测的大型目标语言模型。虽然在特定于应用程序的设置中有效，但它通常需要微调草稿模型和目标模型以实现高接受率。随着下游任务数量的增加，这些草案模型显着增加了推理系统的复杂性。我们提出了推测流，这是一种单模型推测解码方法，通过将微调目标从下一个 token 预测更改为未来 n-gram 预测，将绘图融合到目标模型中。推测流在摘要、结构化查询和含义表示等各种任务中将解码速度提高1.8 - 3.1 倍，而且不会牺牲生成质量。此外，推测流是参数高效的。它实现了与 Medusa 风格架构同等/更高的速度，同时使用~10000X更少的额外参数，使其非常适合资源受限的设备。  &lt; /div&gt;  由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvzyw/r_speculative_streaming_fast_llm_inference/</guid>
      <pubDate>Thu, 25 Apr 2024 16:13:58 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过自适应 N-gram 并行解码实现大型语言模型的无损加速</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvuxm/r_lossless_acceleration_of_large_language_model/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.08698 摘要：  虽然大型语言模型（LLM）表现出了非凡的能力，但它们由于自回归处理而导致大量资源消耗和相当大的延迟，从而阻碍了这一过程。在本研究中，我们引入了自适应 N-gram 并行解码 (ANPD)，这是一种创新且无损的方法，可通过允许同时生成多个标记来加速推理。 ANPD 采用两阶段方法：首先是使用 N-gram 模块的快速起草阶段，该模块根据当前的交互上下文进行调整，然后是验证阶段，在此期间原始 LLM 评估并确认提议的令牌。因此，ANPD 保留了法学硕士原始输出的完整性，同时提高了处理速度。我们进一步利用 N-gram 模块的多级架构来提高初始草稿的精度，从而减少推理延迟。 ANPD 无需重新训练或额外的 GPU 内存，使其成为高效且即插即用的增强功能。在我们的实验中，LLaMA 等模型及其微调变体的速度提升高达 3.67 倍，验证了我们提出的 ANPD 的有效性。     由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvuxm/r_lossless_acceleration_of_large_language_model/</guid>
      <pubDate>Thu, 25 Apr 2024 16:08:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 旧论文 - 机器学习奖学金中令人不安的趋势</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</link>
      <description><![CDATA[我只是想提醒或向新人介绍这篇论文。我认为应该重新开启这个讨论，因为这里的许多人实际上确实影响了该领域的趋势。 https://arxiv.org/pdf/1807.03341&quot;&gt;https:// /arxiv.org/pdf/1807.03341  个人笔记（随意跳过）： 具体来说，我想指出这个问题“Mathiness”，因为这个问题似乎失控了，并且大多数会议的最佳论文都受到了它的困扰（最重要的 ML 论文之一试图数学化，但引入了一个大错误，我相信其他论文有更大的问题，但没有人费心去检查）。 所以这是我个人对学者和研究人员的观点：  我们（我认为大多数将会涉及），从业者不需要方程来知道什么是召回率，并且显然不想阅读难以理解的线性回归版本，这只会让你的论文毫无用处。如果您不想浪费我们的时间，请将其放入附录或完全删除。 审稿人，请不要对不必要的数学印象深刻，如果它很复杂并且没有任何用处，谁关心吗？而且，无论如何它都可能有缺陷，您可能不会发现它。    由   提交/u/pyepyepie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccve1k/d_old_paper_troubling_trends_in_machine_learning/</guid>
      <pubDate>Thu, 25 Apr 2024 15:50:19 GMT</pubDate>
    </item>
    <item>
      <title>[R] 动画时间序列的 Python 包</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccvcdh/r_python_package_for_animated_time_series/</link>
      <description><![CDATA[在此有关时代系列的视频中，https: //www.youtube.com/watch?v=0zpg9ODE6Ww，有人知道用于创建视频第 34 分钟显示的动画情节的 Python 包吗？感谢您的帮助.   由   提交 /u/SatieGonzales   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccvcdh/r_python_package_for_animated_time_series/</guid>
      <pubDate>Thu, 25 Apr 2024 15:48:25 GMT</pubDate>
    </item>
    <item>
      <title>[D] UAI-2024成绩等候区</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/</link>
      <description><![CDATA[审查阶段之后(旧帖子），为像我这样等待决定的其他人创建一个线程。 祝一切顺利！  &amp;# 32；由   提交 /u/PaganPasta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccv3r8/d_uai2024_results_waiting_area/</guid>
      <pubDate>Thu, 25 Apr 2024 15:38:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] 为什么 Transformer 没有进行分层训练？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</link>
      <description><![CDATA[在我看来，由于残差路径，无论变压器层/块如何，流向每一层的梯度都是相同的。示例： ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X))) ... ） 由于 ProjectionAndCost 的输入只是所有层和初始嵌入的输出之和，因此到达 L1 层的梯度与到达 L2 或 L3 的梯度相同。 因此我们可以：  首先仅训练 L1：ProjectionAndCost(X + L1(X)) 冻结 L1，包括 L2 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X))) 冻结 L1 和 L2，包括 L3 并训练：ProjectionAndCost(X + L1(X) + L2(X + L1(X)) + L3(X + L1(X) + L2(X + L1(X)))) ..依此类推  我们不能先训练L2 然后是 L1，因为 L2 的输入取决于 L1，但我们可以先训练较低层，然后逐渐添加和训练更深的层。这种方法有什么问题吗？   由   提交/u/kiockete  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cct38r/d_why_transformers_are_not_trained_layerwise/</guid>
      <pubDate>Thu, 25 Apr 2024 14:16:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] 是否有一个适用于 NVIDIA GPU 的等效 BigDL 项目，它允许使用 Spark 在 DL 集群上分配工作负载？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ccodmn/d_is_there_an_equivalent_bigdl_project_for_nvidia/</link>
      <description><![CDATA[所以就有了这个相对较新的“BigDL”项目” (https://bigdl.readthedocs.io/en/latest/)，适用于 Intel CPU 和 Intel GPU ，但没有提到它适用于 NVIDIA GPU。 Spark 集群上是否有适用于 NVIDIA GPU 的等效库？  &amp; #32；由   提交/u/PepperGrind  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ccodmn/d_is_there_an_equivalent_bigdl_project_for_nvidia/</guid>
      <pubDate>Thu, 25 Apr 2024 10:18:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9jy4b/d_simple_questions_thread/</guid>
      <pubDate>Sun, 21 Apr 2024 15:00:19 GMT</pubDate>
    </item>
    </channel>
</rss>