<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Fri, 12 Apr 2024 09:14:09 GMT</lastBuildDate>
    <item>
      <title>[D] Pytorch 与 Tensorflow 初学者对比</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c24dey/d_pytorch_vs_tensorflow_for_beginner/</link>
      <description><![CDATA[嘿，在学习了传统机器学习之后，我开始学习深度学习。现在我想知道我应该选择什么？如果其中一个更好，或者更适合初学者？   由   提交 /u/MichalKaniowski   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c24dey/d_pytorch_vs_tensorflow_for_beginner/</guid>
      <pubDate>Fri, 12 Apr 2024 08:56:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找带有表情符号嵌入的变压器</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c22rqg/d_looking_for_transformers_with_emoji_embeddings/</link>
      <description><![CDATA[我正在使用典型的预训练模型（例如 mDistilBERT）处理多语言文本。问题是，这些模型的令牌和令牌嵌入中通常没有表情符号。但表情符号通常构成我的数据的主要和重要部分。 使用表情符号添加新标记确实很有帮助，但它们从随机嵌入作为新标记开始。我有一个想法，作为转移学习的一种方式，从另一个模型初始化输入标记嵌入。 问题是，我很难在标记生成器中找到任何带有表情符号的模型。你知道任何？表情符号越多越好。   由   提交/u/qalis  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c22rqg/d_looking_for_transformers_with_emoji_embeddings/</guid>
      <pubDate>Fri, 12 Apr 2024 07:06:22 GMT</pubDate>
    </item>
    <item>
      <title>[D]利用自监督学习涉足手写文本识别问题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c21frt/d_dabbling_in_handwritten_text_recognition/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c21frt/d_dabbling_in_handwritten_text_recognition/</guid>
      <pubDate>Fri, 12 Apr 2024 05:43:27 GMT</pubDate>
    </item>
    <item>
      <title>【研究】MMStar：我们评估大型视觉语言模型的方法正确吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c20v8x/research_mmstar_are_we_on_the_right_way_for/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2403.20330  评估代码：https://github.com/open-compass/VLMEvalKit  摘要： 大型视觉语言模型（LVLM）最近取得了快速进展，引发了大量研究来评估其多模态能力。然而，我们深入研究当前的评估工作并发现两个主要问题：1）视觉内容对于许多样本来说是不必要的。答案可以直接从问题和选项中推断出来，或者从法学硕士中嵌入的世界知识中推断出来。这种现象在当前的基准测试中普遍存在。例如，GeminiPro 在没有任何视觉输入的 MMMU 基准测试中达到了 42.9%，并且在六个基准测试中平均优于随机选择基准 24% 以上。 2）LLM和LVLM训练中存在无意的数据泄露。 LLM和LVLM仍然可以在没有视觉内容的情况下回答一些视觉必需的问题，表明在大规模训练数据中记忆了这些样本。例如，Sphinx-X-MoE 在不访问图像的情况下在 MMMU 上获得了 43.6%，超过了其 LLM 骨干网的 17.9%。这两个问题都会导致对实际多模态增益的误判，并可能误导 LVLM 的研究。为此，我们推出了 MMStar，这是一个精英视觉不可或缺的多模态基准，由人类精心挑选的 1,500 个样本组成。 MMStar 对 6 个核心功能和 18 个详细轴进行了基准测试，旨在通过仔细平衡和纯化的样本来评估 LVLM 的多模式能力。这些样本首先通过自动化管道从当前基准中粗略选择，然后进行人工审查，以确保每个精选样本表现出视觉依赖性、最小的数据泄漏，并且需要先进的多模式功能。此外，还开发了两个指标来衡量多模式训练中的数据泄漏和实际性能增益。我们在 MMStar 上评估了 16 个领先的 LVLM，以评估其多模态能力，并在 7 个基准测试中使用建议的指标来调查其数据泄漏和实际多模态增益。    由   提交/u/KennyMcKormick_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c20v8x/research_mmstar_are_we_on_the_right_way_for/</guid>
      <pubDate>Fri, 12 Apr 2024 05:08:28 GMT</pubDate>
    </item>
    <item>
      <title>[研究] Ada-LEval：使用长度自适应基准评估长上下文法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c20to0/research_adaleval_evaluating_longcontext_llms/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2404.06480  代码/数据集：https://github.com/open-compass/Ada -LEval 摘要： 最近，大型语言模型 (LLM) 社区对增强 LLM 处理超长文档的能力表现出越来越大的兴趣。随着各种长文本技术和模型架构的出现，对模型长文本能力的精确而详细的评估变得越来越重要。现有的长文本评估基准，例如L-Eval和LongBench，都是基于开源数据集构建长文本测试集，主要关注QA和摘要任务。这些数据集包括纠缠在一起的不同长度（从 2k 到 32k+）的测试样本，这使得评估不同长度范围内的模型能力变得具有挑战性。此外，它们不涵盖最新法学硕士声称要实现的超长设置（100k+ 代币）。在本文中，我们介绍了 Ada-LEval，这是一种长度自适应基准，用于评估法学硕士的长上下文理解。 Ada-LEval 包括两个具有挑战性的子集：TSort 和 BestAnswer，它们可以更可靠地评估法学硕士的长上下文能力。这些基准测试支持对测试用例长度的复杂操作，并且可以轻松生成多达 128k 个标记的文本样本。我们使用 Ada-LEval 评估了 4 个最先进的闭源 API 模型和 6 个开源模型。评估结果证明了当前法学硕士的局限性，尤其是在超长上下文环境中。   由   提交/u/KennyMcKormick_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c20to0/research_adaleval_evaluating_longcontext_llms/</guid>
      <pubDate>Fri, 12 Apr 2024 05:05:45 GMT</pubDate>
    </item>
    <item>
      <title>[新闻] NeurIPS 2024 为高中生新增论文轨道</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1zesl/news_neurips_2024_adds_a_new_paper_track_for_high/</link>
      <description><![CDATA[NeurIPS 2024 为高中生添加了新的论文轨道 https://neurips.cc/Conferences/2024/CallforHighSchoolProjects  第三十八届神经信息处理系统年会 (NeurIPS 2024)一个跨学科会议，汇集了机器学习、神经科学、统计学、优化、计算机视觉、自然语言处理、生命科学、自然科学、社会科学和其他相邻领域的研究人员。  今年，我们邀请高中生提交有关机器学习社会影响主题的研究论文。将选出一部分决赛入围者以虚拟方式展示他们的项目，并将在 NeurIPS 主页上重点展示他们的作品。此外，最多五个获奖项目的主要作者将受邀参加在温哥华举行的 NeurIPS 2024 颁奖典礼。  每份提交的作品必须描述完全由高中生作者完成的独立作品。我们希望每份提交的内容都能突出显示已证明的积极社会影响或使用机器学习产生积极社会影响的潜力。    由   提交 /u/xiaohk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1zesl/news_neurips_2024_adds_a_new_paper_track_for_high/</guid>
      <pubDate>Fri, 12 Apr 2024 03:47:38 GMT</pubDate>
    </item>
    <item>
      <title>【研究】人脸（图片）转3D模型，从哪里开始、如何开始？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1z1f2/research_face_picture_to_3d_model_where_and_how/</link>
      <description><![CDATA[我是一名从事图形开发和低级（本机内容）的中级开发人员，所以我对数学和低级编码都不陌生。 我现在正在尝试 ML，话虽如此，我想了解我需要学习哪些步骤、主题/主题，以便我可以实现 Face to 3D 对象检测， 或者有人可以给我一个为了实现这一目标需要寻找和研究什么背景？ 虽然我想使用第 3 方库，但我也想了解细节。   由   提交 /u/zugbo_interactive   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1z1f2/research_face_picture_to_3d_model_where_and_how/</guid>
      <pubDate>Fri, 12 Apr 2024 03:27:58 GMT</pubDate>
    </item>
    <item>
      <title>[项目] CLIP 实现高效知识蒸馏，不使用教师模型，仅使用教师嵌入</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1yjxd/project_clip_for_efficient_knowledge_distillation/</link>
      <description><![CDATA[从教师模型获得的预先计算的嵌入可以用来训练学生模型的知识蒸馏吗？ 这个项目扩展了CLIP 通过利用嵌入作为教师来实现高效的知识蒸馏。典型的知识蒸馏框架需要通过教师模型进行前向传递，这在十亿或万亿参数教师的情况下通常是令人望而却步的。仅使用教师模型的嵌入来指导蒸馏可以显着节省计算量。 GitHub：https:/ /github.com/lnairGT/CLIP-Distillation   由   提交 /u/IllustriousSir_007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1yjxd/project_clip_for_efficient_knowledge_distillation/</guid>
      <pubDate>Fri, 12 Apr 2024 03:03:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于验证集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1vdgs/d_about_validation_set/</link>
      <description><![CDATA[我听说我只能使用测试集一次，但是如果我在测试集上没有得到很好的结果，我可以合并将测试集放入验证集，重新调整超参数，然后收集另一个测试集？  此外，如果我发现模型由于光照不良或域外数据而不能很好地处理某些数据，如果我在论文中记录这一点，我可以将它们从验证集中删除吗？ 我觉得这两种方式可能是“将数据改变成我们想要的”并对此感到有点不安。  谢谢   由   提交 /u/Striking-Warning9533    reddit.com/r/MachineLearning/comments/1c1vdgs/d_about_validation_set/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1vdgs/d_about_validation_set/</guid>
      <pubDate>Fri, 12 Apr 2024 00:34:26 GMT</pubDate>
    </item>
    <item>
      <title>[R] 变形金刚的注意力机制类似于神经科学中联想记忆模型的现代迭代。我展示了自联想和异联想混合物可以执行一系列任务+建议新的神经启发 Transformer 插值方法。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1pf30/r_the_attention_mechanism_of_transformers/</link>
      <description><![CDATA[   从机制上讲，注意力似乎执行异质关联。实际上，原则上它可以混合自动和自动混合。异性关联在一起。 问题：这允许什么能力？回答：很多！  有限自动机 通过为图像或文本数据分配神经活动，并将其组合转换为自关联吸引子（状态）或异关联准吸引子（转移），我们可以模拟有限自动机。 （参见下面链接的论文的第 3.4 节和附录 A12）  将有限自动机映射到“内存图”的示例。  多尺度图形表示 通过调整自关联（a）和异关联（h）的强度，我们可以选择我们希望识别的图形关系的比例或粗糙度。 （请参阅下面链接的论文的第 3.2 节和附录 A2）  使用关联内存进行检测时，不同活动规模分布在其顶点（内存模式）的 Tutte 图网络。  稳定视频的回忆 自然视频的帧之间通常具有很大的时间依赖性。实现自关联和异关联之间的适当平衡有助于防止视频“卡在”帧上或向前跳动。 （请参阅下面链接的论文的第 3.3 节和附录 A11） 视频帧（内存模式）随时间的相关性，显示 a 和 h 的值如何影响召回的平滑度。  内存保真度和内存保真度之间的优雅权衡。能力，通过异质关联相似的记忆和记忆对。 “检索”那些具有最高重叠度的内容。 （参见附录 A3） 神经科学数据的复制显示猴子颞叶皮层中的异质关联。 （参见附录A7） 左：内存负载方面的非传统自动关联性能。右图：a 和 h 的设置与猴子颞叶皮层的数据非常匹配。  这让我建议对变形金刚进行受神经科学启发的可解释性分析，并提出为什么叠加应该与“上下文切换”和“上下文切换”相关的假设。暗示“数据相关的几何形状”。 了解更多信息 -- 论文： https://arxiv.org/abs/2404.07123 GitHub：&lt; /strong&gt; https://github.com/tfburns/CDAM   由   提交/u/tfburns  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1pf30/r_the_attention_mechanism_of_transformers/</guid>
      <pubDate>Thu, 11 Apr 2024 20:30:53 GMT</pubDate>
    </item>
    <item>
      <title>[R] 有没有人可以使用注意力可视化工具来生成注意力可视化，例如“注意力就是您所需要的一切”附录中的工具？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1oo7y/r_does_anyone_have_access_to_an_attention/</link>
      <description><![CDATA[由于许多论文没有附录，这里有一个，这样你就知道我在说什么：https://arxiv.org/pdf/1706.03762.pdf 附录中是注意力生成的连接的可视化机制，“似乎表现出与句子的句法和语义结构相关的行为”。 有人知道这些是如何生成的，或者有生成类似的工具或方法吗？ ​ 提前致谢！   由   提交/u/mcarlin2  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1oo7y/r_does_anyone_have_access_to_an_attention/</guid>
      <pubDate>Thu, 11 Apr 2024 20:01:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] ReFT：语言模型的表示微调</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1nvwi/r_reft_representation_finetuning_for_language/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.03592 代码：https://github .com/stanfordnlp/pyreft 摘要：  参数高效微调（PEFT）方法寻求适应大的通过更新少量权重来调整模型。然而，许多先前的可解释性工作表明，表示编码了丰富的语义信息，这表明编辑表示可能是一种更强大的替代方案。在这里，我们通过开发一系列表示微调 (ReFT) 方法来追求这一假设。 ReFT 方法在冻结的基础模型上运行，并学习对隐藏表示的特定任务干预。我们定义了 ReFT 系列的一个强大实例，低秩线性子空间 ReFT (LoReFT)。 LoReFT 是现有 PEFT 的直接替代品，其学习干预措施的参数效率比之前最先进的 PEFT 高 10 至 50 倍。我们展示了 LoReFT 在八个常识推理任务、四个算术推理任务、Alpaca-Eval v1.0 和 GLUE 上的表现。在所有这些评估中，LoReFT 提供了效率和性能的最佳平衡，并且几乎总是优于最先进的 PEFT。我们在 此 https URL 公开发布通用 ReFT 训练库。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1nvwi/r_reft_representation_finetuning_for_language/</guid>
      <pubDate>Thu, 11 Apr 2024 19:30:10 GMT</pubDate>
    </item>
    <item>
      <title>[R] 无限上下文变形金刚</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1l16l/r_infinite_context_transformers/</link>
      <description><![CDATA[我看了一下，没有在本文中看到任何看起来很有希望的讨论主题。  https://arxiv.org/abs/2404.07143  你的想法？这可能是 Gemini 1.5 报告的 10m 令牌上下文长度背后的技术之一吗？    由   提交 /u/Dyoakom   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1l16l/r_infinite_context_transformers/</guid>
      <pubDate>Thu, 11 Apr 2024 17:35:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在论文“More Agents Is All You Need”中，为什么他们使用 BLEU 分数来计算集成投票的相似度而不是余弦相似度之类的东西？有后续研究比较方法吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c1e0x4/d_in_the_paper_more_agents_is_all_you_need_why/</link>
      <description><![CDATA[论文：https://arxiv.org/pdf /2402.05120.pdf 在论文中，他们有法学硕士集体回答问题。对于离散答案，例如多项选择题，他们只选择最常见的答案。对于“连续”来说，对于像代码这样的答案，他们使用 BLEU 分数来查找与其他答案最相似的答案。 有人知道为什么选择这个答案而不是余弦相似度之类的答案吗？看起来他们没有解释这个选择，但他们的结果很好，所以我猜它有效！   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c1e0x4/d_in_the_paper_more_agents_is_all_you_need_why/</guid>
      <pubDate>Thu, 11 Apr 2024 12:39:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>