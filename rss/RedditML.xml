<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>ml。初学者请参阅 learnmachinelearning</description>
    <lastBuildDate>Wed, 17 Apr 2024 18:17:05 GMT</lastBuildDate>
    <item>
      <title>[R] ResearchAgent：利用大型语言模型对科学文献进行迭代研究想法生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6gb2m/r_researchagent_iterative_research_idea/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.07738 摘要：  科学研究对于改善人类生活至关重要，但其阻碍固有的复杂性、缓慢的速度以及对专业专家的需求。为了提高其生产力，我们提出了一个ResearchAgent，这是一种由大型语言模型驱动的研究想法写作代理，它可以自动生成问题、方法和实验设计，同时根据科学文献对其进行迭代完善。具体来说，从核心论文作为产生想法的主要焦点开始，我们的 ResearchAgent 不仅通过通过学术图表连接信息来增强相关出版物，而且还根据其基本概念、挖掘和分析从以实体为中心的知识存储中检索实体。在许多论文中共享。此外，为了反映人类通过同行讨论迭代改进想法的方法，我们利用多个评审代理来迭代地提供评审和反馈。此外，它们是用符合人类偏好的大型语言模型来实例化的，其评估标准源自实际的人类判断。我们在多个学科的科学出版物上对我们的 ResearchAgent 进行了实验验证，展示了它在基于人类和基于模型的评估结果生成新颖、清晰和有效的研究想法方面的有效性。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6gb2m/r_researchagent_iterative_research_idea/</guid>
      <pubDate>Wed, 17 Apr 2024 17:49:16 GMT</pubDate>
    </item>
    <item>
      <title>[R] Ctrl-Adapter：一种高效且多功能的框架，用于使各种控制适应任何扩散模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6fygt/r_ctrladapter_an_efficient_and_versatile/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.09967 代码：https ://github.com/HL-hanlin/Ctrl-Adapter 模型：https://huggingface.co/hanlincs/Ctrl-Adapter 项目页面：https://ctrl-adapter.github.io/ 摘要：  ControlNets广泛用于在不同条件下的图像生成中添加空间控制，例如深度图、精明边缘和人体姿势。然而，利用预训练图像 ControlNet 进行受控视频生成时存在一些挑战。首先，由于特征空间的不匹配，预训练的ControlNet无法直接插入新的骨干模型，并且为新的骨干网络训练ControlNet的成本是一个很大的负担。其次，不同帧的 ControlNet 特征可能无法有效处理时间一致性。为了应对这些挑战，我们引入了 Ctrl-Adapter，这是一种高效且多功能的框架，通过调整预训练的 ControlNet（并改进视频的​​时间对齐），为任何图像/视频扩散模型添加多种控制。 Ctrl-Adapter 提供多种功能，包括图像控制、视频控制、稀疏帧视频控制、多条件控制、与不同骨干网的兼容性、适应不可见的控制条件以及视频编辑。在 Ctrl-Adapter 中，我们训练适配器层，将预训练的 ControlNet 特征融合到不同的图像/视频扩散模型，同时保持 ControlNet 和扩散模型的参数冻结。 Ctrl-Adapter由时间和空间模块组成，可以有效处理视频的时间一致性。我们还提出了潜在跳跃和反时间步采样，以实现鲁棒自适应和稀疏控制。此外，Ctrl-Adapter 只需取 ControlNet 输出的（加权）平均值即可实现多种条件下的控制。凭借不同的图像/视频扩散主干（SDXL、Hotshot-XL、I2VGen-XL 和 SVD），Ctrl-Adapter 在图像控制方面与 ControlNet 相匹配，并优于视频控制的所有基线（在 DAVIS 2017 数据集上实现了 SOTA 精度）更低的计算成本（少于 10 个 GPU 小时）。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6fygt/r_ctrladapter_an_efficient_and_versatile/</guid>
      <pubDate>Wed, 17 Apr 2024 17:34:59 GMT</pubDate>
    </item>
    <item>
      <title>[P] 从头开始​​构建我自己的个人 ChatGPT</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6fm4c/p_building_my_own_personal_chatgpt_from_scratch/</link>
      <description><![CDATA[大家好，我刚刚写了一个从头开始构建 ChatGPT 的教程。我知道这以前已经做过了。我对它的独特看法侧重于最佳实践。以正确的方式构建ChatGPT。 教程涵盖的内容：  ChatGPT 的实际工作原理 设置建立一个开发环境来迭代提示并尽快获得反馈 构建一个简单的系统提示和聊天界面以与我们的 ChatGPT 交互 添加日志记录和版本控制以进行调试和更轻松地迭代 为助手提供有关用户的上下文信息 使用计算器等工具增强人工智能，以解决法学硕士难以解决的问题  希望本教程对于初学者和提示工程师爱好者来说都是可以理解的 🫡 本教程使用 PromptLayer 平台来管理提示，但也可以适应其他工具。最后，您将拥有一个功能齐全的聊天助手，它了解有关您和您的环境的信息。如果您有任何问题，请告诉我！ 我很乐意详细说明任何问题过程的一部分。您可以在此处阅读完整教程：https://blog.promptlayer.com/构建chatgpt-from-scratch-the-right-way-ef82e771886e   由   提交/u/jzone3  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6fm4c/p_building_my_own_personal_chatgpt_from_scratch/</guid>
      <pubDate>Wed, 17 Apr 2024 17:21:26 GMT</pubDate>
    </item>
    <item>
      <title>[D]问题：时间序列解码到非时间潜在空间？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6fa97/d_question_timeseries_decoding_to_nontemporal/</link>
      <description><![CDATA[您好！我是一名计算神经科学研究员，希望将一些现代机器学习技术应用于功能磁共振成像时间序列数据。我收集了一组高维 4D fMRI 时间序列数据，这些数据是在受试者定期观察 COCO 的自然图像时收集的。我们目前拥有采用预处理“快照”的解码模型。该时间序列数据被扁平化为在观察图像的短时间内聚合的激活模式，并使用一些机器学习模型来解码和重建来自大脑的图像内容。 （请参阅我的一些最近的工作）。 我很好奇存在什么样的机器学习技术可能能够处理时间序列数据本身，而不必将时间序列折叠为单个快照来执行我们的解码过程。我设想的是一个模型（可能是一个变压器），它可以将高维多通道时间序列作为输入，并输出与图像刺激相对应的扁平潜在表示（例如 CLIP 向量），甚至是由已知的规则间隔（正如我们在不同图像呈现的数据中所具有的那样）。据我所知，时间序列数据的机器学习的大部分工作都是预测，但我想要的是静态（或可能重复的）输出。我希望更详细的时间序列数据将具有额外的信号，从而提高 fMRI 视觉解码的解码性能。 ML 领域是否有任何现有的工作已经解决了类似的问题？  &gt;   由   提交 /u/reesespike   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6fa97/d_question_timeseries_decoding_to_nontemporal/</guid>
      <pubDate>Wed, 17 Apr 2024 17:08:14 GMT</pubDate>
    </item>
    <item>
      <title>[D] 在交叉注意力中，为什么 Q 取自解码器，K 取自编码器输出？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</link>
      <description><![CDATA[我查了很多地方但找不到答案。如果我们分别将 Q 和 K 来自编码器和解码器，会发生什么？会有什么不同吗？   由   提交/u/shuvamg007  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bt1c/d_in_crossattention_why_is_q_taken_from_decoder/</guid>
      <pubDate>Wed, 17 Apr 2024 14:49:10 GMT</pubDate>
    </item>
    <item>
      <title>[D]视觉语言模型中视觉嵌入如何与语言嵌入空间共存？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</link>
      <description><![CDATA[大家好！我很高兴能讨论大视觉语言模型 (LVLM)。由于我们可能是最大的法学硕士社区，我认为这个频道将是开始这次对话的完美场所。此外，关于将视觉和语言嵌入结合起来的内容并不多。 LVLM 的一些背景：它们通常由图像的视觉编码器、文本的常规标记器、像投影层这样的投影层组成。 MLP 将视觉特征与文本嵌入空间对齐，最后合并图像和文本嵌入以发送到 LLM 模型中。输入包括文本和图像，而输出是文本，使其成为多模式法学硕士。查看 LLaVA 论文中的图表，了解直观的细分： https://preview.redd.it/l222askgu1vc1.png?width=1607&amp;format=png&amp; ;auto=webp&amp;s=ef011e16301c22b4751d8d0a8f3698f70e3ffd26 从像 CLIP ViT 这样的视觉编码器开始，模型从图像中学习视觉信息，然后使用 MLP 将其投影到 LLM 的嵌入空间上。该论文将这种特征称为对齐。我很好奇视觉嵌入如何与文本嵌入交互，因此我尝试使用 PCA 以 3D 方式可视化它们。 例如，采用 llava-7B 模型 - 它使用 llama-7B 后端和32k 词汇量和 4096 个维度，使得嵌入大小为：[32000,4096]。我使用了一个简单的提示，“向我解释一下这张图片”。使用猫的图片来查看嵌入如何出现在我们的空间中。 https://preview.redd.it/032oy0yn u1vc1.png?width=662&amp;format=png&amp;auto=webp&amp;s=d037bbecc976392e159a1c1bde775ef1e148 488d 添加视觉标记改变了动态。每个图像转换为 576 个形状的视觉标记 [576,4096]。查看包含这些标记时绘图如何调整： https ://preview.redd.it/vdeacylwu1vc1.png?width=566&amp;format=png&amp;auto=webp&amp;s=42441b4fd515cee916b40243429b4aa6820b998c 那我觉得怎么样？ 首先，我们不会直接将视觉标记转换为文本。最近的一篇 Google 论文尝试过，发现这不是最好的方法。视觉推理似乎徘徊在文本嵌入空间附近，可能是因为图像的信息更密集，需要更多的标记来表示视觉概念。 其次，这种设置目前看来是正确的。视觉标记与文本标记一起，将图像衍生的上下文添加到 LLM，使其能够“看到”图像。 最后，尽管 llava 在视觉推理的一些基准测试中表现良好，但它可能还不是最有效的图像表示方法。最近的一些研究谈到了注意力稀疏现象，尤其是 LVLM 中的视觉标记。我们很幸运，因为注意力算法只关注有意义的视觉标记并忽略噪音。 你觉得怎么样？谢谢阅读。 :-)   由   提交/u/E-fazz  /u/E-fazz  reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6bmjs/d_how_does_visual_embedding_coexist_with_language/</guid>
      <pubDate>Wed, 17 Apr 2024 14:41:31 GMT</pubDate>
    </item>
    <item>
      <title>关于时间序列预测的好资源？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6b9t1/good_resources_on_time_series_forecasting_d/</link>
      <description><![CDATA[有人可以推荐一些关于使用机器学习进行现代时间序列预测的好资源吗？ 我在 上找到了一本关于时间序列预测的书亚马逊的好评如潮，名为Python 中的时间序列预测。 话虽如此，许多机器学习书籍和资源似乎都掩盖了时间序列。 有哪些涵盖时间序列的好资源（整本书或书中的章节）？&lt; /p&gt;   由   提交 /u/secret_fyre   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6b9t1/good_resources_on_time_series_forecasting_d/</guid>
      <pubDate>Wed, 17 Apr 2024 14:26:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 用于 NER 的最佳 NLP 编码器（BERT...），数据微调非常低？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c6a65n/d_best_nlp_encoders_bert_for_ner_with_very_low/</link>
      <description><![CDATA[嗨 我知道存在很多 Transformer 编码器变体（BERT、DistilBERT、Deberta、Roberta ...） . 但是，我对最好的句子（可能应该是 Deberta V3）不感兴趣，而是那些即使示例很少（例如大约 50,100 个句子，每个句子可能包含 1 个单词）也能快速获得不错的结果的句子，2 或 3 个实体）。  我用英语做了一些实验，令我惊讶的是，似乎在数据较少的情况下表现最好的实验可能的是原始英语 BERT 模型（HF 上的 google-bert/bert-base-uncased），而不是最近的变体之一。 我还做了其他实验法语，并且多语言 BERT 也比专门训练法语数据的模型（例如 CamemBERT）更快地获得不错的结果。  我比较过的模型包括：bert、bert multilingual、distilbert、distilbert multilingual、roberta、xlm-roberta、camembert、camberta、distilroberta、debertav3、debertav3 multilingual 您对此有何看法？这是令人惊讶或不寻常的事情吗？有什么建议吗？   由   提交/u/LelouchZer12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c6a65n/d_best_nlp_encoders_bert_for_ner_with_very_low/</guid>
      <pubDate>Wed, 17 Apr 2024 13:40:50 GMT</pubDate>
    </item>
    <item>
      <title>词嵌入 - 上下文化与 word2vec [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c69b23/word_embedding_contextualised_vs_word2vec_d/</link>
      <description><![CDATA[关于词嵌入的菜鸟问题 -  据我所知到目前为止 -  语境化词嵌入BERT 和其他 LLM 类型模型生成的模型使用注意力机制并考虑单词的上下文。所以不同句子中的同一个词可以有不同的向量。  这 ^ 与 word2vec 等模型的旧方法相反 - word2vec 生成的嵌入不是上下文的。  但是，仔细观察 CBOW 和skip-gram 模型。似乎他们也尝试根据周围（上下文）单词来预测中心单词。因此，word2vec 生成的嵌入也可以是上下文相关的。 所以它们都是上下文相关的？  我错过了什么？    由   提交 /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c69b23/word_embedding_contextualised_vs_word2vec_d/</guid>
      <pubDate>Wed, 17 Apr 2024 13:03:56 GMT</pubDate>
    </item>
    <item>
      <title>[D] 喜欢数学讨厌编程</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c692z2/d_likes_the_math_hates_the_programming/</link>
      <description><![CDATA[所以我们都知道这样的故事：具有软件 CS 背景的人进入 ML 领域，认为只是使用一堆库进行编码，最终却讨厌这是因为涉及数学/统计数据。但反过来呢？ 我发现我真的很喜欢学习机器学习算法背后的数学和统计数据，但我的问题是我完全不喜欢编程。我可以理解算法和数学位背后发生的事情，但通常很难将其转换为代码并从头开始构建算法。  这令人担忧，因为似乎很多 ML 工作都专注于编码和 ML 操作部分，而不是理论（在非研究角色中）。我知道最好的方法是练习编码位的练习，但只是想知道处于相同位置的其他人或有什么建议（除了明显的建议之外？）。 &lt;!-- SC_ON - -&gt;  由   提交/u/Character-Capital-70   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c692z2/d_likes_the_math_hates_the_programming/</guid>
      <pubDate>Wed, 17 Apr 2024 12:53:53 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]ACM MM2024</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c66ilj/discussionacm_mm2024/</link>
      <description><![CDATA[这是 MM 从 CMT 转向 Openreview 的第一年（如果我没记错的话）。作为一名作者，自从我创建提交以来，我一直感觉到有些问题，即在摘要 ddl 之前就被桌面拒绝，论文中是否包含提交编号的不一致等。现在我从社交媒体上听到很多作者说由于缺乏提交卷的审稿人，没有许多/任何出版物（是的，包括我）被提名为审稿人。我非常关心今年 MM2024 的审稿和提交的质量。   由   提交/u/INeedPapers_TTT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c66ilj/discussionacm_mm2024/</guid>
      <pubDate>Wed, 17 Apr 2024 10:35:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 研究中数学和算法哪个优先？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</link>
      <description><![CDATA[我现在正在学习扩散背后的方法（DDPM、基于分数的方法和其他方法）。我想知道研究人员到底是如何想出这个想法的？ 发明新方法是这样的吗？ 1.我们想要制作更好的图像生成器。 2. 哦，数据永远不够...... 3. 让我们乘以数据 - 通过添加一些噪声损坏 4. 这个效果很好，如果我们制作一个去噪网络怎么办？ 5. 如果我们建立一个由纯噪声生成图像的网络会怎么样？ 6. 这不行，如果我们做更小的去噪步骤怎么办？ 7. 这有效！现在，让我们创建一些关于它为何起作用的理论。 8.写论文 或者类似的东西？ 1.我们想要制作更好的图像生成器。 2.我们知道“非平衡热力学”非常好，想尝试以某种方式应用它 3. 我们以某种方式想出了一种依赖于该理论的数学的算法 4. 它有效！ 5. 我们写论文。 通常哪个先出现？数学还是算法？   由   提交/u/Deep-Station-1746   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c64jw0/d_what_comes_first_math_or_algorithm_in_research/</guid>
      <pubDate>Wed, 17 Apr 2024 08:22:11 GMT</pubDate>
    </item>
    <item>
      <title>AI/ML 数据中心的未来将是 100 台甚至 1000 台服务器像一个巨型加速器一样运行 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</link>
      <description><![CDATA[在服务器公司 Gigabyte 的网站上看到了这个信息丰富的视频 (https://youtu.be/2Q7S-CbnAAY?si=DJtU2mQ_ZKRZ83Nf），简而言之，服务器品牌现在将完整的服务器集群运送到数据中心，而不是单个服务器机器。在此所示的示例中，它有 8 个机架（另外一个用于管理和网络），每个机架中有 4 台相同型号的服务器，以及 4 个相同的超高级 GPU每个服务器中的模型。为您计算一下，每个集群有 32 台服务器或 256 个 GPU 加速器。请注意，所有服务器和 GPU 都必须是相同的型号，因为它们的连接方式基本上是作为一台单独的机器运行。 这很可能是标准构建块的原因。所有人工智能数据中心的特点是，我们现在利用大型数据集训练人工智能的方式，参数数量达到数十亿，甚至数万亿。对于为我们带来 ChatGPT 及其同类产品的法学硕士来说尤其如此。以任何效率处理这些数万亿个参数的唯一方法是通过我们以前从未见过的规模的并行计算。因此，这个大胆的新概念将数百甚至数千台服务器连接在一起，因此它们基本上是一台巨型服务器，加载了 Nvidia 或其他品牌的数千个 GPU。真正令人着迷的东西，我还没有看到目前为人工智能计算的未来提出的任何其他规模的东西。 这是视频中介绍的集群的网站：https://www.gigabyte.com/Industry-Solutions/giga-pod-as-a-service ?lan=en   由   提交/u/Low_Complaint2254   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c62oym/the_future_of_aiml_data_centers_is_going_to_be/</guid>
      <pubDate>Wed, 17 Apr 2024 06:16:01 GMT</pubDate>
    </item>
    <item>
      <title>[R] 状态空间模型中的状态幻象</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c5z6ua/r_the_illusion_of_state_in_statespace_models/</link>
      <description><![CDATA[ 由   提交/u/hardmaru  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c5z6ua/r_the_illusion_of_state_in_statespace_models/</guid>
      <pubDate>Wed, 17 Apr 2024 02:58:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>