<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sun, 21 Apr 2024 09:14:21 GMT</lastBuildDate>
    <item>
      <title>[D] [P]租赁帮助:(</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9css6/d_please_help/</link>
      <description><![CDATA[你好，今天的问候 有一个训练模型使用 api 服务器玩刽子手的项目，我是这样的我只是无法训练一个好的模型。  我尝试过 LSTM（也使用遗传算法），但我要么陷入训练循环，要么最终精度不好。 需要帮助： （ （仍然是 ML 的新手） 谢谢：）   由   提交 /u/catblankcheque   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9css6/d_please_help/</guid>
      <pubDate>Sun, 21 Apr 2024 08:19:13 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何训练大量数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c9au32/d_how_do_you_train_on_large_amount_of_data/</link>
      <description><![CDATA[我有大约 400 万篇报纸文章。我想训练词嵌入、主题建模。我买了 colab pro+，他们的高内存规格只有 60GB 左右的内存。  当我尝试在这些 4M 文章上训练任何内容时，运行时就会崩溃。我可以认为我们会从硬盘中批量加载数据并发送吗？我真的没有在这里的经验。我很想听听您的经验和建议。    由   提交 /u/RiseWarm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c9au32/d_how_do_you_train_on_large_amount_of_data/</guid>
      <pubDate>Sun, 21 Apr 2024 06:07:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] GPT 如何理解它不知道的东西？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c98pjl/d_how_does_gpt_understand_what_it_does_not_know/</link>
      <description><![CDATA[      https://preview.redd.it/kf5t2k95arvc1.png?width=625&amp;format=png&amp;auto=webp&amp;s=5631d9b49084 815d44c2178e1b27cfd4926f6f59 在我意识到之前，我要查找我打错的 Cramer 距离和 GPT 点。 GPT 如何理解输入可能是错误的，而不是总是假设输入是对的？这是否也是以端到端的方式完成的，或者如果有任何程序可以识别输入中可能的不确定性，那么还有哪些附加程序？ 我想一般来说我只是好奇如何GPT 知道它可能没有答案吗？   由   提交/u/No_Grapefruit_4686   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c98pjl/d_how_does_gpt_understand_what_it_does_not_know/</guid>
      <pubDate>Sun, 21 Apr 2024 03:57:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 通过想象力、探索和批评实现法学硕士的自我完善</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c97o2q/r_toward_selfimprovement_of_llms_via_imagination/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.12253 摘要：  尽管大型语言模型（LLM）在各种方面具有令人印象深刻的能力任务中，他们仍然在处理涉及复杂推理和计划的场景。最近的工作提出了先进的提示技术以及使用高质量数据进行微调以增强法学硕士推理能力的必要性。然而，这些方法本质上受到数据可用性和质量的限制。有鉴于此，自我纠正和自我学习成为可行的解决方案，采用的策略允许法学硕士改进他们的成果并从自我评估的奖励中学习。然而，法学硕士在自我完善其反应方面的有效性，特别是在复杂的推理和规划任务中，仍然值得怀疑。在本文中，我们引入了用于LLM自我改进的AlphaLLM，它将蒙特卡罗树搜索（MCTS）与LLM结合起来，建立一个自我改进循环，从而在无需额外注释的情况下增强LLM的能力。 AlphaLLM 从 AlphaGo 的成功中汲取灵感，解决了将 MCTS 与 LLM 相结合以实现自我提升的独特挑战，包括数据稀缺、语言任务的巨大搜索空间以及语言任务中反馈的主观性。 AlphaLLM 由即时合成组件、专为语言任务量身定制的高效 MCTS 方法以及用于精确反馈的三个批评模型组成。我们在数学推理任务中的实验结果表明，AlphaLLM 在无需额外注释的情况下显着增强了法学硕士的性能，显示了法学硕士自我改进的潜力。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c97o2q/r_toward_selfimprovement_of_llms_via_imagination/</guid>
      <pubDate>Sun, 21 Apr 2024 02:56:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] Tango 2：通过直接偏好优化调整基于扩散的文本到音频生成</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c95izw/d_tango_2_aligning_diffusionbased_texttoaudio/</link>
      <description><![CDATA[自动生成的成对偏好数据和 DPO 对齐改进了文本到音频的生成。 代码和数据集：https://github.com/declare-lab/tango 论文：https://arxiv.org/abs/2404.09956 模型：https://huggingface.co/declare-lab/tango2   由   提交 /u/sgpfc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c95izw/d_tango_2_aligning_diffusionbased_texttoaudio/</guid>
      <pubDate>Sun, 21 Apr 2024 01:00:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] OSS 工具可在几秒钟内将您的 Python 代码扩展到数千个 GPU 和 CPU</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8yvsn/p_oss_tool_that_scales_your_python_code_to/</link>
      <description><![CDATA[嘿，机器学习社区， 我想分享我正在构建的 OSS 开发人员工具。我很想获得一些反馈并听取那些之前维护过大型开源项目的人的意见。 我正在构建 Burla  这是一个 python 包，可以轻松地在（很多）其他计算机上运行代码。由于 Burla 最适合处理令人尴尬的并行工作负载，因此我将重点放在数据准备上，因此使用案例包括清理、格式化/解析、标记文本数据、数据编码和压缩标记。 Burla 是。 ..  免费开源软件 可通过一个命令安装在云中 具有一个函数和两个参数的 python 包 在 1 秒内扩展到数千个虚拟机 将代码部署到任何硬件 (GPU) 和任何软件环境 (Docker) 一个可以正常工作的工具。 Burla 自动同步包、重新引发异常并流回 stdout/stderr  很高兴回答任何和所有问题！这是我们的网站   由   提交/u/Ok_Post_149   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8yvsn/p_oss_tool_that_scales_your_python_code_to/</guid>
      <pubDate>Sat, 20 Apr 2024 19:54:53 GMT</pubDate>
    </item>
    <item>
      <title>[D]leetcode 在机器学习中有多重要？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8ygzl/d_how_important_is_leetcode_in_ml/</link>
      <description><![CDATA[我最近面试了一位应用数据科学家，面试过程是这样的： - 1x ML 面试 - 3x Leetcode 面试 - 1x 高级系统设计面试 leetcode对于ML/DS从业者的实际工作有多重要？ 3 个 leetcode 问题与 1 个 ml 问题有那么重要吗？ 当我在准备面试时，我只是觉得我在浪费时间做 leetcode，而我本可以在 ML 的其他领域甚至其他领域提升技能。 K8s、cuda 或数据工程等技术技能。  我有兴趣了解其他人对此的看法。   由   提交/u/Amgadoz  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8ygzl/d_how_important_is_leetcode_in_ml/</guid>
      <pubDate>Sat, 20 Apr 2024 19:36:48 GMT</pubDate>
    </item>
    <item>
      <title>[D] Meta 的 H100 数据代表其根据 2024 年 2 月 1 日公司财报电话会议购买的 H100。不包括另外 250,000 个 H100 等值的 GPU。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8ydji/d_metas_h100_figure_represents_its_h100_purchase/</link>
      <description><![CDATA[   /u/ewelumokeke  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8ydji/d_metas_h100_figure_represents_its_h100_purchase/</guid>
      <pubDate>Sat, 20 Apr 2024 19:32:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 招聘时我应该在多大程度上重视传统机器学习知识？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8xgvp/d_how_much_should_i_emphasize_on_traditional_ml/</link>
      <description><![CDATA[我们正在招聘 nlp 高级机器学习工程师。  过去，我一直依赖标准的过滤问题。但我想知道这对于可能永远不会在项目中使用这些技术的新工程师来说是否不公平，以及这是否不再衡量工程师现在所做的事情。 我们确实有工艺演示的回声，其中我们询问相关的技术和技术问题。  但是想听听关于初始过滤的事情我想知道我是否应该停止询问： - 偏差与方差以及类似的训练“统计”概念 - 主题建模技术和类似的“旧 nlp” - 引导技术。 bagging 和类似技术 - 熵、杂质和其他“信息论”问题。  我们的团队无法达成共识。希望您的想法   由   提交/u/20231027  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8xgvp/d_how_much_should_i_emphasize_on_traditional_ml/</guid>
      <pubDate>Sat, 20 Apr 2024 18:54:22 GMT</pubDate>
    </item>
    <item>
      <title>[D] 以 JSON 对象的形式从 PDF 文件中提取数据。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8w1mq/d_extract_data_from_pdf_file_in_the_form_of_json/</link>
      <description><![CDATA[      /u/Ghulam_Nabi   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8w1mq/d_extract_data_from_pdf_file_in_the_form_of_json/</guid>
      <pubDate>Sat, 20 Apr 2024 17:54:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] Groq 上的 llama-3-70b 和代码解释</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8uiwi/p_llama370b_on_groq_with_code_interpreting/</link>
      <description><![CDATA[       由   提交/u/mlejva  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8uiwi/p_llama370b_on_groq_with_code_interpreting/</guid>
      <pubDate>Sat, 20 Apr 2024 16:48:50 GMT</pubDate>
    </item>
    <item>
      <title>[D] 从第一原理解释多模态神经网络的简史</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8sydf/d_the_short_history_of_multimodal_neural_network/</link>
      <description><![CDATA[      来自我的 YT 频道的视频解释多模态机器学习的核心概念及其过去几年的演变。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8sydf/d_the_short_history_of_multimodal_neural_network/</guid>
      <pubDate>Sat, 20 Apr 2024 15:40:17 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一张让你感觉自己老了的幻灯片</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8kuef/d_a_slide_which_makes_you_feel_old/</link>
      <description><![CDATA[       由   提交 /u/xiikjuy   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8kuef/d_a_slide_which_makes_you_feel_old/</guid>
      <pubDate>Sat, 20 Apr 2024 08:20:34 GMT</pubDate>
    </item>
    <item>
      <title>[N] 何凯明关于表征学习的深度学习架构讲座</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1c8d5m1/n_kaiming_hes_lecture_on_dl_architecture_for/</link>
      <description><![CDATA[https://youtu.be/D_jt-xO_RmI 非常好的讲座，DL 历史架构进展的最高信噪比。   由   提交 /u/lkphuc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1c8d5m1/n_kaiming_hes_lecture_on_dl_architecture_for/</guid>
      <pubDate>Sat, 20 Apr 2024 00:57:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1by6i5h/d_simple_questions_thread/</guid>
      <pubDate>Sun, 07 Apr 2024 15:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>