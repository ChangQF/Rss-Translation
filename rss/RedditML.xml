<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Mon, 11 Mar 2024 00:57:39 GMT</lastBuildDate>
    <item>
      <title>[D] 尝试使用 JEPA 理解推理</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbnb5q/d_trying_to_understand_inference_with_jepa/</link>
      <description><![CDATA[灵感来自 Lex Fridmans 播客剧集 与 Yann LeCun 一起，我试图通过阅读 I-JEPA 论文来提高我对 JEPA 和基于能量的模型的理解以及这些讲义。  我从学习高度语义特征的角度理解JEPA的吸引力/半监督过程中连续图像数据的表示。但真正让我困惑的是 Yann LeCun 的说法，一旦像这样的模型经过训练，你就可以进行基于优化的推理，基本上优化 Y 以最小化能量。  这个生成过程已经被证明了吗？除了预训练的 JEPA 模型之外，为了在这个基于优化的过程中生成图像/文本响应，还需要哪些组件？   由   提交/u/flxh13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbnb5q/d_trying_to_understand_inference_with_jepa/</guid>
      <pubDate>Sun, 10 Mar 2024 22:37:56 GMT</pubDate>
    </item>
    <item>
      <title>拥有非参数估计背景是否是进入机器学习的有用途径？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/</link>
      <description><![CDATA[我是统计学硕士生。我的背景几乎全部基于基础统计理论，我的论文是非参数估计（特别是非参数回归）。基本上，我的“机器学习”知识源于一些经典的非参数估计书籍，例如（Tysbakov、Wasserman、Tibshirani/Hastie 和 Friedman）。统计学习的要素几乎是我在机器学习方面的背景，因为我的论文是关于非参数回归的经典方法之间的交集，例如基于树的方法、核平滑器和样条曲线，用于估计因果推理中的平均治疗效果。 但是，我有时会觉得自己的机器学习背景“相当老”。比如说，我不知道非参数回归背景对于现代机器学习工作有多有吸引力。一般来说，我们深入研究了非参数和统计学中的许多渐近理论，而且我知道对于大多数机器学习工作来说，没有人真正关心渐近保证。  有谁知道我的知识是否真的与当今主要关注神经网络的时代的机器学习工作相关？   由   提交/u/Direct-Touch469   reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbk0i6/is_having_a_background_in_nonparametric/</guid>
      <pubDate>Sun, 10 Mar 2024 20:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[项目] ML 和可解释性使用反馈</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbjiri/project_ml_and_explainability_usage_feedback/</link>
      <description><![CDATA[嘿， 我有一个非常耗时的模拟，需要数百个输入并产生 0, 1 的结果或 2. 由于这是我在内部运行多年的模拟，我最终进行了数百万次模拟，我想尝试构建一个能够预测结果的机器学习。我不打算取代模拟，但我希望 ML 模型能够更直接、更快速地了解结果。我以前使用过可解释性模型，但我有一个想法，但我没有再次遇到过，这就是我在这里的原因。 假设我的 ML 模型非常适合我的历史数据集。  p&gt;  我之前使用过 LIME 来了解哪个特征对预测影响最大。但是我可以使用机器学习模型来解释模拟的真实结果吗？假设我运行一个模拟并得到结果 0。如果我的模型也得到 0，那么我仍然可以使用 LIME 来了解它。但如果我的模型在这种情况下预测为 1 呢？有什么方法可以使用它并“理解”它吗？模拟的真实结果？ 我想到的另一个案例是在运行模拟之前帮助我。正如我所说，我有数百个输入，我想使用机器学习为我提供随机“反馈” “如果增加输入 X，则 1 的概率将增加 1.4%”的格式。我想蛮力是我获得最重要特征、调整它们并获得预测的一种方法，但是有没有更复杂的方法来实现这一点？  P.S.我的模拟不是确定性的，同一组输入可能在每次运行时产生不同的结果   由   提交/u/Rough-Professional16   reddit.com/r/MachineLearning/comments/1bbjiri/project_ml_and_explainability_usage_feedback/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbjiri/project_ml_and_explainability_usage_feedback/</guid>
      <pubDate>Sun, 10 Mar 2024 20:02:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 嵌入模型=新业务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbjbup/d_embedding_models_new_business/</link>
      <description><![CDATA[看MTEB排行榜，现在几乎每个人都在每天发布新型号，有人靠这个赚钱吗？   由   提交 /u/Foreign_Cheesecake46   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbjbup/d_embedding_models_new_business/</guid>
      <pubDate>Sun, 10 Mar 2024 19:55:05 GMT</pubDate>
    </item>
    <item>
      <title>[D] Lora 的记忆增益从何而来？ （除了优化器状态）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbgu8d/d_where_do_lora_memory_gains_come_from_apart_from/</link>
      <description><![CDATA[你好， 我已经阅读 Lora 的解释几个小时了，有一些东西我无法包装我的环顾四周：记忆力增强。我知道优化器状态可以获得很多冻结层不需要的效果。 但是，在 Lora 论文（第 4.2 章）中指出  与完全微调相比，我们还观察到在 GPT-3 175B 上的训练过程中加速了 25%，因为我们不需要计算绝大多数参数的梯度。  但是冻结层的梯度不需要计算吗？即使它们的权重不会随之更新，也必须计算它以获得可训练矩阵的梯度。 在经典微调中，需要梯度，因为大多数时候只有最后一层是训练完毕，反向传播就到此为止，但是对于 Lora，可训练参数位于所有注意力头中，因此反向传播需要继续直到那里，还是我误解了什么？ 所以，如果需要梯度，并且还需要激活+权重，那么唯一的内存增益将来自优化器？根据我的经验，LoRA 内存使用量似乎比“正常”内存使用量低得多。微调，所以我想有些东西我不明白 ​ 非常感谢您的帮助！   由   提交 /u/Wats0ns   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbgu8d/d_where_do_lora_memory_gains_come_from_apart_from/</guid>
      <pubDate>Sun, 10 Mar 2024 18:12:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] OpenAI：JSON 模式与函数</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbgky4/r_openai_json_mode_vs_functions/</link>
      <description><![CDATA[       由   提交/u/JClub  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbgky4/r_openai_json_mode_vs_functions/</guid>
      <pubDate>Sun, 10 Mar 2024 18:01:50 GMT</pubDate>
    </item>
    <item>
      <title>当计算机科学的学生也在研究类似的模型时，作为一名数学学生，我如何有效地捍卫我的深度学习论文？ [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbdkbe/how_can_i_effectively_defend_my_deep_learning/</link>
      <description><![CDATA[我是一名数学硕士生，目前正在撰写论文，重点是开发深度学习模型。虽然我对我的研究感到兴奋，但我担心捍卫它，特别是因为计算机科学的学生也在探索类似的领域。我正在寻求有关如何有效地展示和捍卫我的工作的建议和策略，强调我的数学观点在这个跨学科领域的独特贡献。任何见解或建议将不胜感激！    由   提交/u/mudassar18104009   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbdkbe/how_can_i_effectively_defend_my_deep_learning/</guid>
      <pubDate>Sun, 10 Mar 2024 15:55:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbcaq5/d_simple_questions_thread/</guid>
      <pubDate>Sun, 10 Mar 2024 15:00:22 GMT</pubDate>
    </item>
    <item>
      <title>[R] 硅群体的智慧：LLM 集成预测能力可与人类群体的准确性相媲美</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbb69f/r_wisdom_of_the_silicon_crowd_llm_ensemble/</link>
      <description><![CDATA[在一篇新论文中，研究人员证明，法学硕士的集合可以在预测方面达到人类水平的表现，有可能取代大规模、昂贵的人类预测的需求 要点：  法学硕士可以通过汇总不同模型的预测来实现人类水平的预测精度 这种“硅的智慧”人群”这种方法与行之有效的“群体智慧”相似。  在研究 1 中，12 名法学硕士组成的团队在 3 个月内的 31 个二元问题上的表现显着优于基线，并与 925 名人类预测者进行了匹配 在研究 2 中，提供人群预测将个人 LLM（GPT-4 和 Claude 2）预测提高了 17-28%，但简单地平均机器和人类预测甚至更好  研究结果表明，组织可以获得高- 通过利用 LLM 集成进行高质量、具有成本效益和可扩展的预测，有可能使数据驱动的决策在各个领域更容易实现。 但是，该研究有一些局限性：它侧重于短期对于二元问题，法学硕士表现出偏差和校准不良，并且随着训练数据过时，其准确性可能会降低。 TLDR：像人类一样，法学硕士表现出“群体的智慧”。影响。一群“人群”的法学硕士与一群人类一样擅长预测。 更多详细信息请点击此处。 Arxiv 论文：https://arxiv .org/pdf/2402.19379.pdf   由   提交/u/Successful-Western27   reddit.com/r/MachineLearning/comments/1bbb69f/r_wisdom_of_the_silicon_crowd_llm_ensemble/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbb69f/r_wisdom_of_the_silicon_crowd_llm_ensemble/</guid>
      <pubDate>Sun, 10 Mar 2024 14:08:21 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024年，强化学习的最新趋势是什么？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bbawez/d_in_2024_what_are_the_latest_trends_on_rl/</link>
      <description><![CDATA[嗨， 这些天我正在研究决策转换器。 有争议的，同时试图找到最重要的论文，我注意到强化学习领域似乎没有发生太多事情。我注意到研究的重点是优化 Transformer 和训练巨大的语言和视觉模型（被视为监督模型）。这是强化学习领域的新大事吗？ 强化学习的最新趋势是什么？   由   提交/u/__Julia  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bbawez/d_in_2024_what_are_the_latest_trends_on_rl/</guid>
      <pubDate>Sun, 10 Mar 2024 13:55:41 GMT</pubDate>
    </item>
    <item>
      <title>[D] 2024 年用于机器学习的 AMD 卡？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb9ylx/d_amd_cards_for_machine_learning_in_2024/</link>
      <description><![CDATA[AMD 和 AI 的状况如何？我想知道 AMD 和 Nvidia GPU 之间的性能差异有多大，以及 7600xt 是否充分支持 pytorch 和 TensorFlow 等机器学习库。上次我听说 AMD 卡可以支持 ROCm，但存在不一致、软件问题以及速度慢 2 - 5 倍的问题。 就个人而言，您会选择 rx7600 而不是 4060 吗？    由   提交 /u/AtomicPiano   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb9ylx/d_amd_cards_for_machine_learning_in_2024/</guid>
      <pubDate>Sun, 10 Mar 2024 13:08:44 GMT</pubDate>
    </item>
    <item>
      <title>[P] 表格数据上分类器的准确率排序</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb9bwx/p_accuracy_ranking_of_classifiers_on_tabular_data/</link>
      <description><![CDATA[宣布Predirank；该项目根据分类器在一系列数据集中的准确性对分类器进行排名。主要重点是 scikit-learn 库中的分类器及其在流行表格数据集上的应用。 https://github .com/c4pub/predirank 显然，生成的排行榜仅供参考；不可能有一个明确的排名。 该项目中使用的数据集来自论文中使用的 UCI 数据集的选择：  Fernandez-Delgado, Manuel ，等人。 “我们需要数百个分类器来解决现实世界的分类问题吗？” 《机器学习研究杂志》15.1 (2014)。  可以通过编辑提供的示例来自定义数据集和分类器的列表。 那么，排名是否已更改随着时间的推移“浅”分类器？ “随机森林”根据 2024 年 3 月 5 日执行的快照，仍然站在领奖台上： ------------------------ ---------------------------------------------------------- - - - -总排名得分 无排名 总平均 stddev excpt 分类器 ---------------------------------------- ---------------------------- 1 0.8166353383458647 0.1498 0 CatBoostClassifier 2 0.7868421052631579 0.1641 0 ExtraTreesClassifier() 3 0.7769736842105264 0.1495 0 RandomForestClassifier() 4 0.713753 1328320802 0.1940 0 XGBClassifierWrap({}) 5 0.7120300751879699 0.1684 0 MLPClassifier() 6 0.6983709273182958 0.1713 0 SVC() 7 0.6981516290726817 0.2161 0 HistGradientBoostingClassifier() 8 0.6960839598997494 0.1783 0 GradientBoostingClassifier() 9 0.6804511278195489 0.1533 0 LogisticRegression() 10 0.6596491228070176 0.2126 27 LogisticRegressionCV() 11 0.6431 70426065163 0.1776 0 BaggingClassifier() 12 0.6278822055137845 0.1866 0 DeodelSecond({}) 13 0.6114974937343358 0.1925 0 GaussianProcessClassifier() 14 0.6111842105263158 0.179 5 2 CalibratedClassifierCV() 15 0.6038533834586466 0.1826 0 LinearSVC() 16 0.6016604010025063 0.2117 0 LinearDiscriminantAnalysis() 17 0.6010651629072682 0.2105 0 RidgeClassifierCV( ) 18 0.5890664160401002 0.2092 0 RidgeClassifier() 19 0.5388471177944862 0.1711 0 KNeighborsClassifier() 20 0.4980889724310777 0.2192 0 DecisionTreeClassifier() 21 0.488753 1328320802 0.2309 0 AdaBoostClassifier() 22 0.4815476190476190 0.1806 0 DeodataDelangaClassifier({}) 23 0.4800125313283208 0.2125 0 LabelSpreading() 24 0.479636591478 6967 0.2163 0 标签传播() 25 0.4579573934837093 0.1606 0 SGDClassifier() 26 0.4277568922305764 0.3348 160 NuSVC() 27 0.4213032581453634 0.2452 25 二次判别分析() 28 0. 4184210526315789 0.2234 0 伯努利NB() 29 0.4106203007518797 0.2292 0 高斯NB() 30 0.4087406015037594 0.1677 0 被动主动分类器() 31 0.4010651629 072682 0.1717 0 ExtraTreeClassifier() 32 0.3893170426065163 0.1468 0 感知器() 33 0.3711152882205514 0.2105 0 最近质心() 34 0.1910401002506265 0.1988 0 GaussianMixture() 35 0.19104010 02506265 0.1988 0 BayesianGaussianMixture() 36 0.1510338345864661 0.1094 0 &lt;&lt;&lt;随机基线&gt;&gt; 37 0.0685150375939849 0.0320 0 OneClassSVM() 38 0.0526002506265664 0.1104 381 MultinomialNB() 39 0.0442669172932330 0.1016 403 RadiusNeighborsClassifier() ----------------------------- ---------------------------------------   .   由   提交/u/eppursim1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb9bwx/p_accuracy_ranking_of_classifiers_on_tabular_data/</guid>
      <pubDate>Sun, 10 Mar 2024 12:34:39 GMT</pubDate>
    </item>
    <item>
      <title>[R] 进入未知：自学习大型语言模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb8bxo/r_into_the_unknown_selflearning_large_language/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.09147 代码：https://github.com/teddy-f-47/self-learning-llm-public 摘要：  我们解决自学LLM的主要问题：学什么的问题。我们提出了一个自学法学硕士框架，使法学硕士能够通过自我评估自己的幻觉来独立学习以前未知的知识。利用幻觉评分，我们引入了未知点（PiUs）的新概念，以及用于自动 PiU 识别的一种外在方法和三种内在方法。它有助于创建一个自学循环，专门关注“未知点”中的知识差距，从而降低幻觉分数。我们还制定了衡量法学硕士自学能力的评估指标。我们的实验表明，经过微调或对齐的 7B-Mistral 模型具有相当好的自学习能力。我们的自学理念使法学硕士更新更加高效，并为知识交流开辟了新的视角。它还可能增加公众对人工智能的信任。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb8bxo/r_into_the_unknown_selflearning_large_language/</guid>
      <pubDate>Sun, 10 Mar 2024 11:35:30 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 尝试生成用于注视预测的合成数据集，为什么模型难以学习数据？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb6z05/discussion_experimenting_with_generating_a/</link>
      <description><![CDATA[我设置了一个虚幻引擎 5 项目，该项目具有超人类视角，可以查看虚拟屏幕上的随机点，然后通过虚拟网络摄像头捕获照片我已经设置好了，然后保存。下面是一个大约 6.5K 图像的简单数据集（我说简单是因为我冻结了以前的很多变量，这里唯一改变图像的东西是照明、角色的位置、屏幕坐标和一些轻微的面部表情）运动我通过随机移动角色的嘴骨组合在一起）。 但出于某种原因，从头开始对数据进行训练的模型很难学习这些特征，并且训练和损失略有下降，但总体保持不变非常停滞... 这很奇怪，因为我已经（在某种程度上）成功地在我用网络摄像头制作的数据集上训练了注视预测模型，我试图用这些数据进行模仿。我成功地在具有 4K 和 6K 样本的相同模型架构上从头开始训练这些模型（最终有效 mse 损失约为 0.008（训练约为记忆中的 1/2），这仍然比我想要的要高，但是比合成模型好得多，合成模型通常在训练和有效方面都停滞在 0.08 左右）。 我尝试冻结除眼球运动和照明之外的所有变量，模型立即学习，训练损失降至 ~ 0.001。 我真的只是想了解可能导致问题的原因，对我来说非常奇怪的是，相同的模型具有完全相同的数据操作（使用一些 fastai 将大小调整为 (240, 320)图像增强）可以很好地学习具有更多变量的真实数据（例如，我在这里冻结的头部旋转，姿势变化，甚至衣服变化），但完全无法从这个更简单的数据集中学习：https://huggingface.co/datasets/goatman/meta human-gaze-prediction 抱歉这么久帖子，任何见解都会令人惊叹！   由   提交/u/Goatman117  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb6z05/discussion_experimenting_with_generating_a/</guid>
      <pubDate>Sun, 10 Mar 2024 10:08:51 GMT</pubDate>
    </item>
    <item>
      <title>GAN 仍然有意义吗？ [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bb34gj/are_gans_still_relevant_d/</link>
      <description><![CDATA[[D] 随着 Difussion 模型的不断兴起，我很好奇 GAN 是否会卷土重来？有什么想法吗？   由   提交 /u/Superb-Assignment-30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bb34gj/are_gans_still_relevant_d/</guid>
      <pubDate>Sun, 10 Mar 2024 06:00:24 GMT</pubDate>
    </item>
    </channel>
</rss>