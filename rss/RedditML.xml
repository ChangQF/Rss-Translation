<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升。初学者请看learnmachinelearning</description>
    <lastBuildDate>Sat, 18 May 2024 18:17:51 GMT</lastBuildDate>
    <item>
      <title>[项目]Tabletop HandyBot：用于桌面任务的低成本机械臂助手</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv11oo/project_tabletop_handybot_lowcost_robotic_arm/</link>
      <description><![CDATA[大家好，我想展示我的业余项目的第一个功能版本：Tabletop Handybot，这是一款低成本的 AI 机械臂助手，可以听取您的语音命令并执行各种桌面任务。  目前它可以拾取和放置呈现在它面前的任意物体。它利用了一些最新的 AI 模型，如 ChatGPT、Grounding DINO、Segment Anything 和 Whisper。一切都是零样本的，不需要 AI 训练。总 BOM 为 2300 美元，希望业余爱好者和修补匠能够负担得起。 我希望继续增加支持以处理更多任务。 Github：https://github.com/ycheng517/tabletop-handybot   由    /u/astroamaze  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv11oo/project_tabletop_handybot_lowcost_robotic_arm/</guid>
      <pubDate>Sat, 18 May 2024 16:25:35 GMT</pubDate>
    </item>
    <item>
      <title>[R] Grounding DINO 1.5 发布：最强大的开集检测模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0m9x/r_grounding_dino_15_release_the_most_capable/</guid>
      <pubDate>Sat, 18 May 2024 16:05:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 基础时间序列模型被高估了吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</link>
      <description><![CDATA[我一直在探索基础时间序列模型，如 TimeGPT、Moirai、Chronos 等，并想知道它们是否真的具有强大的样本潜力 -高效的预测，或者他们只是借用 NLP 基础模型的炒作并将其引入时间序列领域。 我可以理解为什么它们可能会起作用，例如，在需求预测中，它是关于但它们能否处理任意时间序列数据，如环境监测、金融市场或生物医学信号，这些数据具有不规则模式和非平稳数据？ 它们的概括能力是否被高估了？    由   提交 /u/KoOBaALT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv0hl2/d_foundational_time_series_models_overrated/</guid>
      <pubDate>Sat, 18 May 2024 16:00:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 第一次参加ICML</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cv06td/d_first_time_attending_icml/</link>
      <description><![CDATA[第一次参加ICML 大家好 我想第一次参加ICML。我非常兴奋，但有几个问题。 背景：我目前正在攻读人工智能硕士学位。 我的问题是：  &lt; li&gt;会议+教程+工作坊的门票有点贵，对于我来说一整周参加都有点不方便。我不确定我是否应该只参加研讨会，或者完整的会议加研讨会。有人可以解释一下研讨会会议和会议会议之间到底有什么区别吗？一个人作为学生参加是否比另一个人更有意义？ 在 ICML 主页的日程安排中，我看不到会议演讲的主题。它只说“Oral 1”、“Oral 2”等。它说我需要注册（~500EUR）才能看到演讲的主题。这是正常的吗？我想在买票之前知道有哪些演讲。  非常感谢您回答我的菜鸟问题！非常感谢任何帮助/信息！   由   提交 /u/KingGongzilla   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cv06td/d_first_time_attending_icml/</guid>
      <pubDate>Sat, 18 May 2024 15:46:07 GMT</pubDate>
    </item>
    <item>
      <title>以下是 Transformers 如何结束神经网络中归纳偏差的传统 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzvy1/here_is_how_transformers_ended_the_tradition_of/</link>
      <description><![CDATA[      我的视频讨论了归纳偏差和通用性的作用，同时将 Transformer/Attention 与 CNN、RNN、甚至 MLP 等传统深度学习进行比较。   由   提交/u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzvy1/here_is_how_transformers_ended_the_tradition_of/</guid>
      <pubDate>Sat, 18 May 2024 15:32:04 GMT</pubDate>
    </item>
    <item>
      <title>[项目] YOLOv8在INT8中量化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzvwx/project_yolov8_quantized_in_int8/</link>
      <description><![CDATA[我在 Nvidia Jetson Orin Nano 中将 YOLOv8x 量化为 INT8，并几乎实时处理（FPS 25）。访问 GitHub 并查看我测试的内容。 https://github.com/the0807/YOLOv8-TensorRT   由   提交/u/the087  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzvwx/project_yolov8_quantized_in_int8/</guid>
      <pubDate>Sat, 18 May 2024 15:32:02 GMT</pubDate>
    </item>
    <item>
      <title>[R] Llamas 用英语工作吗？论多语言 Transformer 的潜在语言</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10588 代码：https://github.com/epfl-dlab/llm-latent-language 数据集：https://huggingface.co/datasets/wendlerc/llm-latent-language Colab 链接：  (1) https://colab .research.google.com/drive/1l6qN-hmCV4TbTcRZB5o6rUk_QPHBZb7K?usp=sharing (2) https://colab.research.google.com/drive/1EhCk3_CZ_nSfxxpaDrjTvM-0oHfN9m2n?usp=sharing 摘要：  我们询问在不平衡的、以英语为主的语料库上训练的多语言语言模型是否使用英语作为内部枢轴语言——这个问题对于理解语言模型如何发挥作用至关重要以及语言偏见的根源。我们的研究重点关注 Llama-2 系列变压器模型，使用精心构建的非英语提示和独特的正确单标记延续。从一层到另一层，变压器逐渐将最终提示标记的输入嵌入映射到计算下一个标记概率的输出嵌入。通过高维空间跟踪中间嵌入揭示了三个不同的阶段，其中中间嵌入（1）从远离输出令牌嵌入的地方开始； (2) 已经允许在中间层中解码语义上正确的下一个标记，但给予其英语版本比输入语言版本更高的概率； (3) 最后进入嵌入空间的输入语言特定区域。我们将这些结果转化为概念模型，其中三个阶段分别在“输入空间”、“概念空间”和“输出空间”中运行。至关重要的是，我们的证据表明抽象的“概念空间”是存在的。比其他语言更接近英语，这可能会对多语言语言模型的偏见产生重要影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzkez/r_do_llamas_work_in_english_on_the_latent/</guid>
      <pubDate>Sat, 18 May 2024 15:17:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 鲁棒智能体学习因果世界模型</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.10877 摘要：  长期以来，人们一直假设因果推理在强大而通用的智能。然而，尚不清楚智能体是否必须学习因果模型才能推广到新领域，或者其他归纳偏差是否足够。我们回答了这个问题，表明任何能够在大量分布变化下满足后悔界限的智能体都必须学习数据生成过程的近似因果模型，该模型收敛到最佳智能体的真实因果模型。我们讨论了这一结果对包括迁移学习和因果推理在内的多个研究领域的影响。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuzbta/r_robust_agents_learn_causal_world_models/</guid>
      <pubDate>Sat, 18 May 2024 15:06:09 GMT</pubDate>
    </item>
    <item>
      <title>[D] 命名实体识别库</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</link>
      <description><![CDATA[大家好，我需要决定使用哪个库进行命名实体识别。我使用过 spaCy，它运行良好，但我需要一个允许我对实体和子实体进行分类的库。有人做过类似的事情吗？我的意思是，同一个词可以是多个实体。 spaCy 提供了 SpanCat 管道，理论上可以实现这一点，但我在创建训练语料库时遇到了麻烦。我认为这是因为他们希望你购买像 Prodigy 这样的注释文本框架。   由   提交/u/Original_Ad8019   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuz1i2/d_library_for_named_entity_recognition/</guid>
      <pubDate>Sat, 18 May 2024 14:52:57 GMT</pubDate>
    </item>
    <item>
      <title>[N] ICML 2024 关于使离散操作可微分的研讨会🤖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</link>
      <description><![CDATA[大家好！ 今年我们将在 ICML 组织可微分几乎所有内容研讨会。&lt; /p&gt; 许多离散操作，例如排序、topk、最短路径、聚类（等等）几乎到处都有零梯度，因此不适合现代基于梯度的学习框架（例如深度学习）。本次研讨会将涵盖旨在解决此类问题的研究主题！ https:// Differentiable.xyz/ 我们鼓励任何从事相关主题工作的人提交他们的作品。即使您没有提交，也请务必参加 ICML 的研讨会，观看即将举行的一些激动人心的演讲！ 我在下面附上了研讨会的完整摘要！祝你当前的工作一切顺利，L :) 梯度和导数是机器学习不可或缺的一部分，因为它们支持基于梯度的优化。然而，在许多实际应用中，模型依赖于实现离散决策的算法组件，或者依赖于离散的中间表示和结构。这些离散步骤本质上是不可微分的，因此破坏了梯度流。要使用基于梯度的方法来学习此类模型的参数，需要将这些不可微分的组件变成可微分的。这可以通过仔细考虑来完成，特别是使用平滑或松弛来为这些组件提出可微的代理。随着模块化深度学习框架的出现，这些想法在机器学习的许多领域变得比以往任何时候都更加流行，在短时间内生成了大量“可微分的一切”，影响了渲染、排序和排名等各种主题，凸优化器、最短路径、动态规划、物理模拟、神经网络架构搜索、top-k、图算法、弱监督学习和自监督学习等等。 本次研讨会将为任何可区分的事物提供一个论坛，汇聚学术界和行业研究人员，突出挑战和发展，提供统一的想法，讨论实际的实施选择并探索未来的方向。   由   提交/u/machine_learning_res   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cux80i/n_icml_2024_workshop_on_making_discrete/</guid>
      <pubDate>Sat, 18 May 2024 13:22:17 GMT</pubDate>
    </item>
    <item>
      <title>[P] GPT-Burn：纯 Rust 中简单简洁的 GPT 实现 🔥</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</link>
      <description><![CDATA[       由   提交/u/ProfessionalDrummer7   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cusmrp/p_gptburn_a_simple_concise_implementation_of_the/</guid>
      <pubDate>Sat, 18 May 2024 08:25:48 GMT</pubDate>
    </item>
    <item>
      <title>[R] 1:10 遥控车 自动驾驶</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cuoo2g/r_110_radio_controlled_car_autonomous_driving/</link>
      <description><![CDATA[我非常需要一些建议，希望能够创建一个机器学习模型，该模型从 2 个立体摄像头获取图像输入并输出油门和转向。我正在赛道上驾驶这辆车，在一个可以俯瞰赛道的平台上。我的想法是创建赛道的深度和视差图，并精确定位汽车，然后根据视差图进行跟踪。我从汽车实时获取油门和转向数据。我怎样才能建立一个机器学习模型来接收图像输入并预测油门/转向？   由   提交 /u/Unable_Car4833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cuoo2g/r_110_radio_controlled_car_autonomous_driving/</guid>
      <pubDate>Sat, 18 May 2024 04:02:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 机器学习工程师，你们的工作重点是部署管道还是模型构建/调整？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cub74b/d_machine_learning_engineers_what_portion_of_your/</link>
      <description><![CDATA[我目前是一名机器学习工程师，但我更加关注管道，这与我担任数据工程师时类似。我很想更多地了解模型构建方面的知识，但自从我完成硕士学位以来，我的模型知识已经有点生疏了。  与模型构建相比，您日常工作的哪一部分侧重于部署？   由   提交/u/RawCS  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cub74b/d_machine_learning_engineers_what_portion_of_your/</guid>
      <pubDate>Fri, 17 May 2024 17:32:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] 子空间嵌入与基本降维有何不同？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1cu8is4/d_how_are_subspace_embeddings_different_from/</link>
      <description><![CDATA[我一直在努力理解更基本的降维技术与更高级的方法有何不同，主要是关于子空间、流形等的直觉是否相同。扩展到更基本的方法。我了解 PCA、t-SNE、UMAP 等如何工作（这些是在寻找维数降维时出现的 90% 的内容），但是当我阅读有关子空间聚类、流形学习或该领域的内容时，他们很少提及这些更基本的暗淡缩减技术，而是选择更高级的方法，我不确定为什么，特别是考虑到 PCA、t-SNE 和 UMAP 似乎是多么多产。 我不清楚像 PCA 这样的东西是否/如何不同于流形学习，特别是它们对于子空间聚类的有用性。我认为两者的目标都是找到一些潜在结构，直觉上在潜在空间中工作将减少噪音、无用/低信息特征，减少维数灾难，并且还可能更清楚地显示特征和标签的情况连接在潜在空间中。就实际算法而言，我理解直觉，但不知道它们是否是“真实的”。例如，在流形学习的情况下（FWIW，我真的不再看到任何相关论文，也不知道为什么会这样），一个常见的例子是“面流形”。对于图像来说，这是一个比原始输入尺寸低的平滑表面，并且从每个面平滑过渡到另一个面。对于图像来说，这可能有点微不足道，但是对于一般的时间序列数据，同样的直觉是否也适用？  例如，如果我有一个时间序列毛毛虫运动的数据集，我可以任意说存在毛毛虫尺寸的流形（较大的毛毛虫移动速度较慢）或毛毛虫能力的流形（例如，某种类型）能力/技能的多样性，如果毛毛虫正在完成任务/迷宫）？非常人为的例子，但基本上问题是，我是否一定能够根据我的先验告诉我应该存在/可能持有潜在结构（给定足够的数据）找到潜在空间？ &lt; p&gt;我知道 Yann LeCun 是在潜在空间中工作的大力支持者（尤其是联合嵌入，我不确定这是否适用于我和我的时间序列数据），所以我尝试更多地开展我的工作那个方向，但似乎基本 PCA 和基本非线性技术（例如，您会看到内置于 scipy 或 sklearn 等的技术）与其他论文中使用的技术之间存在很大分歧。 PCA（或基本非线性方法）等是否能实现相同的效果，但效果却不那么好？   由   提交 /u/Amun-Aion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1cu8is4/d_how_are_subspace_embeddings_different_from/</guid>
      <pubDate>Fri, 17 May 2024 15:44:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的人在此处发帖！ 帖子将一直保持到下一篇，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ckt6k4/d_simple_questions_thread/</guid>
      <pubDate>Sun, 05 May 2024 15:00:21 GMT</pubDate>
    </item>
    </channel>
</rss>