<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>毫升</description>
    <lastBuildDate>Sat, 06 Apr 2024 06:16:08 GMT</lastBuildDate>
    <item>
      <title>从人类的微小反馈中学习 [R] [P]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bx2nun/learning_from_little_human_feedback_r_p/</link>
      <description><![CDATA[我有一个输入是图像的环境，该图像可能包含也可能不包含边界框。根据提供的图像和人类的动作确定人类是否喜欢边界框的代理。现在，我想修改此设置，以便代理可以以最少的反馈适应任何其他人的偏好。我怎样才能实现这个目标？   由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bx2nun/learning_from_little_human_feedback_r_p/</guid>
      <pubDate>Sat, 06 Apr 2024 04:26:11 GMT</pubDate>
    </item>
    <item>
      <title>[R] 如何利用离线先验知识来增强在线分类器？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bx0m5s/r_how_to_utilize_the_offline_prior_knowledge_to/</link>
      <description><![CDATA[你好，我希望设计一个分类系统，每1秒分类10个类别。 目前，我上网很差分类器的准确率达到 45%。但我可以在这个时间步获得每个类的离线概率，例如，P(offline) = [0.0, 0.05, 0.10, 0.70, ..., 0.15]。 直观上，虽然我的分类器 P（在线）的准确度较低，但我有 P（离线）作为附加先验信息，所以我猜我应该能够将这两者结合在一起以更好地执行任务。 我的计划是：我将设计一个名为“MyLogic”的逻辑来消耗来自 P(在线) 和 P(离线) 的预测向量，并且“MyLogic”将被调用。会输出一个信号来指示我是否有必要手动亲自检查班级。不过，我希望减少这种手动检查的频率。 有人可以指导我如何设计这个“MyLogic”吗？或者是否已经有一些现有的论文或文献来处理类似的问题？    由   提交 /u/AaronSpalding   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bx0m5s/r_how_to_utilize_the_offline_prior_knowledge_to/</guid>
      <pubDate>Sat, 06 Apr 2024 02:38:40 GMT</pubDate>
    </item>
    <item>
      <title>[R] 过去几个月的一些论文和方法通常或针对特定用例减少了预训练和/或微调和/或推理成本。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwtw2b/r_some_papers_and_approaches_in_the_last_few/</link>
      <description><![CDATA[   /u/Singularian2501   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwtw2b/r_some_papers_and_approaches_in_the_last_few/</guid>
      <pubDate>Fri, 05 Apr 2024 21:37:06 GMT</pubDate>
    </item>
    <item>
      <title>[D] 如何获得足够的耐心来训练-调试-训练模型？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwlqj4/d_how_do_you_get_enough_patience_to/</link>
      <description><![CDATA[训练一个大模型只是为了让它出错。我不会同时处理多项任务，因为专注于一件事可以更快地完成它。 但这太烦人了。我只是永远等待它完成，而我的所有其他任务都暂停了，当我得到结果时，这都是错误！  哎呀兄弟   由   提交 /u/RiseWarm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwlqj4/d_how_do_you_get_enough_patience_to/</guid>
      <pubDate>Fri, 05 Apr 2024 16:02:36 GMT</pubDate>
    </item>
    <item>
      <title>[R] 以编码器-解码器方式重建 3D 点云（使用等变编码器）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwjus6/r_reconstructing_3d_point_clouds_in_an/</link>
      <description><![CDATA[我为我正在写的一篇论文设计了一些基于自动编码器的二维图像方法，现在根据之前会议的反馈，我正在尝试将我的方法扩展到 3D 点云。为此，我首先必须首先开发一个 3D 自动编码器（即从点云编码到潜在向量并解码到点云），然后当然要继续构建我的方法的其余架构和属性（自动编码器必须例如，尊重某些 SO(3) 等方差约束，但现在让我们忘记这一点）。作为起点，我已经使用 PointNet 作为编码器为点云开发了一个编码器-解码器架构，并且效果非常好。  经过几周的审阅论文，我意识到所谓的“形状重建”任务是在现实世界中进行的。实际上并不意味着“从潜在向量重建点云”，这正是我正在寻找的。相反，该术语似乎通常用于指“在给定 3D 点云的情况下重建表面”的任务。所以基本上，没有什么像我想要开发的。这种命名约定阻碍了我对类似自动编码器的点云重建方法的搜索。  -首先，我想请求一些关于类似自动编码器的点云重建的论文的推荐。我似乎找不到任何或几乎任何一个，因为我所有的搜索最终都在关于点云到表面“重建”的论文中。 ​  - 其次，以自动编码器的方式重建点云似乎不受欢迎是有原因的吗？我认为，既然对于图像来说，基于自动编码器的架构非常流行，那么 3D 形状也会很流行，但看起来并非如此。  ​ 就这样了:)谢谢。 ​ 奖金：&lt; /strong&gt; 如果有人熟悉几何深度学习和SO(3)等变网络，我还想问： ​ 我开发的点云重建自编码器使用 PointNet 作为编码器和 FC 网络作为解码器可以完美地重建例如形状网。然而，当我使我的 PointNet 网络 SO(3) 等变时（我使用矢量神经元https://arxiv.org/abs/ 2104.12229），然后我的重建突然变得非常糟糕。我已经检查了错误，并且我的网络确实已正确实现。为什么等变实际上使任务比使用常规的非等变 PointNet 编码器更困难？   由   提交 /u/howtorewriteaname   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwjus6/r_reconstructing_3d_point_clouds_in_an/</guid>
      <pubDate>Fri, 05 Apr 2024 14:45:33 GMT</pubDate>
    </item>
    <item>
      <title>[R] 解读诗歌</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwhwsc/r_interpreting_poetry/</link>
      <description><![CDATA[我很好奇如何训练网络以通过引用行来解释诗歌以支持主张。由于有无数种方法可以用不同程度的证据来解释，我很好奇一个人如何训练一个网络，使其能够输出多个同样可行的解释，对我来说，输出 PDF 似乎更困难。对任何与生成模型相关的研究也感兴趣。   由   提交/u/ixw123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwhwsc/r_interpreting_poetry/</guid>
      <pubDate>Fri, 05 Apr 2024 13:22:12 GMT</pubDate>
    </item>
    <item>
      <title>[讨论]最大特征值/特征向量/主成分优化</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwhslj/discussionlargest_eigenvalueeigenvectorprincipal/</link>
      <description><![CDATA[任何人都可以向我推荐任何通过深度学习优化这些术语而无需显式计算出来的作品吗？我很难找到这些作品。   由   提交 /u/_karma_collector   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwhslj/discussionlargest_eigenvalueeigenvectorprincipal/</guid>
      <pubDate>Fri, 05 Apr 2024 13:16:56 GMT</pubDate>
    </item>
    <item>
      <title>[研究]通过简单的自适应攻击越狱领先的安全对齐法学硕士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwguzv/research_jailbreaking_leading_safetyaligned_llms/</link>
      <description><![CDATA[摘要：我们表明，即使是最新的安全相关法学硕士对于简单的自适应越狱攻击也不够鲁棒。首先，我们演示如何成功利用对 logprobs 的访问进行越狱：我们最初设计一个对抗性提示模板（有时适应目标 LLM），然后我们对后缀应用随机搜索以最大化目标 logprob（例如，令牌的 logprob） “当然”），可能需要多次重新启动。这样，我们在 GPT-3.5/4、Llama-2-Chat-7B/13B/70B、Gemma-7B 和 R2D2 上实现了接近 100% 的攻击成功率（以 GPT-4 为判断） HarmBench 针对 GCG 攻击进行了对抗性训练。我们还展示了如何通过传输或预填充攻击来越狱所有 Claude 模型（不暴露 logprobs），成功率达 100%。此外，我们还展示了如何在一组受限的标记上使用随机搜索来查找中毒模型中的木马字符串——这项任务与越狱有很多相似之处——正是这种算法为我们带来了 SaTML&#39;24 中的第一名木马检测竞赛。这些攻击背后的共同主题是适应性至关重要：不同的模型容易受到不同提示模板的影响（例如，R2D2 对上下文学习提示非常敏感），某些模型具有基于其 API 的独特漏洞（例如，Claude 的预填充） ），并且在某些设置中，根据先验知识限制令牌搜索空间至关重要（例如，对于木马检测）。我们在 https://github.com/tml-epfl/llm 提供攻击的代码、提示和日志-自适应攻击。   由   提交 /u/m_andriushchenko   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwguzv/research_jailbreaking_leading_safetyaligned_llms/</guid>
      <pubDate>Fri, 05 Apr 2024 12:32:18 GMT</pubDate>
    </item>
    <item>
      <title>[R] FMA 的补码 - 一个定制的二进制数字系统（如 2 的补码），设计用于基于查找表的矩阵乘法</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwg494/r_fmas_complement_a_custom_binary_number_system/</link>
      <description><![CDATA[TLDR ：矩阵乘法的查找表。  简单描述：这是一个定制的二进制数字系统，旨在计算矢量点积。  技术描述：FMA 的补码是二进制数的定点表示形式，作为 2 的补码和 1 的补码的替代方案，用于执行融合乘加指令。  我们将线性代数和算术代码（来自数据压缩理论）结合起来，使数字系统非常适合查找点积。  这是获取开始文章 这是GitHub 存储库  &lt; !-- SC_ON --&gt;  由   提交 /u/DataBaeBee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwg494/r_fmas_complement_a_custom_binary_number_system/</guid>
      <pubDate>Fri, 05 Apr 2024 11:54:29 GMT</pubDate>
    </item>
    <item>
      <title>[D] KDD2024 评论的评分标准</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bweztg/d_scoring_scale_for_kdd2024_reviews/</link>
      <description><![CDATA[有谁知道今年 KDD 评论的评分标准是多少？我只在 OpenReview 上看到审稿人提出的数字，但在任何地方都找不到总体规模。   由   提交/u/Environmental_Gas318   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bweztg/d_scoring_scale_for_kdd2024_reviews/</guid>
      <pubDate>Fri, 05 Apr 2024 10:49:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Tensorboard/权重和偏差的替代方案</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwduod/d_alternatives_to_tensorboard_weights_and_biases/</link>
      <description><![CDATA[我一直在使用 Tensorboard 来跟踪我的深度学习项目中的损失曲线和几个指标的演变，但我放弃了它，因为它太有限了（特别是在同一个图上进行多次运行）。 我大约 6 个月前开始使用权重和偏差，但它实际上接近于一场噩梦：用户界面极其缓慢，许多错误，非- 直观且文档很少的Python库。我实际上因此浪费了几十个小时。 对于未来的项目，我想改用更好的解决方案。我听说过海王星，但我从未有机会尝试过。我想要一些专注于跟踪指标的东西，但速度快，没​​有漏洞，并且高度可定制。 对 Neptune 有什么意见吗？您还会推荐什么？    由   提交 /u/Skeylos2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwduod/d_alternatives_to_tensorboard_weights_and_biases/</guid>
      <pubDate>Fri, 05 Apr 2024 09:35:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] KDD 2024 评论</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bwcact/d_kdd_2024_reviews/</link>
      <description><![CDATA[大家好， KDD 2024 论文评审可在 OpenReview 上查看。随着评论的发布，我创建了这个讨论线程，供我们收集想法、问题和建议或其他任何内容。祝您一切顺利！  更新： Paper Copilot 正在使用以下计算方式收集 KDD&#39;24 评级。您还可以通过此链接输入您的分数，请查看一下。 评分=0.5*平均新颖性。 + 0.5*平均技术质量   由   提交 /u/jeongwhanchoi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bwcact/d_kdd_2024_reviews/</guid>
      <pubDate>Fri, 05 Apr 2024 07:46:52 GMT</pubDate>
    </item>
    <item>
      <title>[D] 单次参考图像的高效一次性检测的 SOTA 是多少？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bw7ol3/d_whats_sota_in_efficient_oneshot_detection_for_a/</link>
      <description><![CDATA[澄清一下，我并不是指通常的野外物体检测任务，就像“这里有一千张随机照片，放置边界框”周围所有的自行车”。我的意思是一个规模小得多的任务，更像是“这里有一千张通过公司传真机的图像，显​​示哪些图像在页面上的某个位置有该公司的徽标”。例如，您有一个在较大图像中查找的确切图像的干净参考副本，但它可能会扭曲、轻微旋转、大小和位置可变等。  以这张信头产品图片为例。如果我有一个“完美的”该图像中页面右上角徽标的 SVG，在图像中找到它确实不应该那么困难（尽管它看起来稍微扭曲，偏离水平约 10 度，而且可能要小得多）比我的参考图像）或者说它不存在。 显然，许多传统的 CV 已经完全被 ML 方法取代，并且目标检测/分割是一个非常活跃的领域。我说 SOTA，但这确实感觉像是一个传统的 CV 问题。我看的不是照片或自然图像，它们是机器生成的，我正在寻找的东西总是看起来或多或少相同，只有细微的变化。如果像 SIFT 功能这样的基本功能可以处理这个问题，那就太好了，但如果我要通过微调更快的 r-cnn 或 yolov5 或 detectorron2 等东西来获得明显更好的结果，那就这样吧。 其他因素：  宁愿不需要需要手工注释的训练集。当目标类别的真实示例变化很大（例如自行车）时，这显然是需要的，但这里的情况并非如此。在 COCO 等内容上进行预训练的模型感觉太过矫枉过正，甚至适得其反，因为我感兴趣的领域与自然图像几乎没有重叠。 更喜欢尽可能轻量级的解决方案。我不需要一个需要两天训练并且可以做无数事情的怪物框架，理想情况下它只是工作得相当好的东西，并且可以在本地运行，并且可以快速训练和快速推理。  更喜欢使用公开代码的方法，但如果有必要，我会自己实现一篇论文。  提前感谢您为我指明正确方向的任何帮助!   由   提交/u/wintermute93  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bw7ol3/d_whats_sota_in_efficient_oneshot_detection_for_a/</guid>
      <pubDate>Fri, 05 Apr 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>[D] 法学硕士正在损害人工智能研究</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bvi4au/d_llms_are_harming_ai_research/</link>
      <description><![CDATA[这是一个大胆的主张，但我觉得 LLM 的炒作早就该消退了。 GPT4 之后，LLM 性能和设计改进不仅取得了相对较小的进展：使其变得更好的主要方法仍然只是使其更大，并且所有 Transformer 的替代架构都被证明是低于标准和劣质的，它们引起了人们的关注（并且投资）远离其他可能更具影响力的技术。与此同时，大量涌入的人甚至不知道基本的机器学习是如何工作的，他们自称是“人工智能研究员”。因为他们使用 GPT 让每个人在本地托管一个模型，试图让你相信“语言模型完全可以推理”。我们只需要另一个 RAG 解决方案！”他们加入这个社区的唯一目标不是开发新技术，而是利用现有技术拼命拼凑出一项有利可图的服务。甚至论文本身也开始主要由法学硕士撰写。我不禁认为整个领域可能会陷入停滞状态，因为不断增长的社区满足于平庸的修复，这些修复充其量只能使模型在任意“分数”上的得分稍微好一些。他们弥补了这一点，忽略了诸如幻觉、上下文长度、基本逻辑缺乏能力以及运行这种规模模型的高昂成本等明显问题。我赞扬那些不顾市场炒作而致力于开发能够实现真正逻辑过程的代理的人们，并希望很快会引起更多关注。   由   提交/u/NightestOfTheOwls   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bvi4au/d_llms_are_harming_ai_research/</guid>
      <pubDate>Thu, 04 Apr 2024 08:36:36 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1bmmra9/d_simple_questions_thread/</guid>
      <pubDate>Sun, 24 Mar 2024 15:00:20 GMT</pubDate>
    </item>
    </channel>
</rss>