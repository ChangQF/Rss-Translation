<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions 或 /r/learnmachinelearning，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Fri, 31 Jan 2025 09:16:31 GMT</lastBuildDate>
    <item>
      <title>[P] 寻找一个简单的 ML 项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie9hq8/p_looking_for_a_simple_ml_project/</link>
      <description><![CDATA[哪里可以找到一个简单的机器学习项目来练习。我更喜欢使用 python 和 anaconda（jupyter 笔记本）或任何入门级的项目。任何链​​接都将不胜感激。    提交人    /u/Avenger5288   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie9hq8/p_looking_for_a_simple_ml_project/</guid>
      <pubDate>Fri, 31 Jan 2025 07:25:04 GMT</pubDate>
    </item>
    <item>
      <title>[R] 只有神经 ODE 的输出才重要。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie7msn/r_only_output_of_neural_ode_matters/</link>
      <description><![CDATA[我有一个以下形式的神经 ODE 问题： X_dot(theta) = f(theta) 其中 f 是一个神经网络。 我想要积分来获得 X(2pi)。 我没有数据来匹配 theta 的中间值。 只需要匹配最终目标 X(2pi)。 这是一个神经 ODE 问题吗？还是有更好的方法来构建它？    提交人    /u/atharvaaalok1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie7msn/r_only_output_of_neural_ode_matters/</guid>
      <pubDate>Fri, 31 Jan 2025 05:18:51 GMT</pubDate>
    </item>
    <item>
      <title>[D] 理解解码器语言模型中“attention_mask”的填充标记。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie7e7g/d_understanding_the_padded_tokens_of_attention/</link>
      <description><![CDATA[大家好。我最近一直在阅读有关预训练 LLM 的工作原理的文章。更具体地说，前向传递是什么样的。我使用了 Hugging Face 的教程来模拟解码器语言模型（例如 GPT2）中的前向传递。 我了解解码器语言模型通常默认使用因果注意力。这意味着它是单向的。这种单向/因果注意力通常存储或注册为缓冲区（如 Andrej Karpathy 的教程所示）。回到 Hugging Face，我们使用标记器对文本序列进行编码，它将输出输入标记 ID（input_ids）和注意力掩码（attention_mask）。 到解码器语言模型的前向传递可选择接受注意力掩码。现在，对于一批输入文本序列（长度不同），可以根据标记化期间该批次的最大长度使用左或右填充侧，以便更容易进行批处理。 问题：一些前向传递的演示忽略了标记器输出的 attention_mask，而是简单地使用注册为缓冲区的因果注意掩码。如果使用后者（因果注意），则填充标记似乎没有被屏蔽。这会显着影响训练吗？ 如果我可以在损失计算期间使用填充标记 ID 作为忽略索引，标记器输出的 attention_mask 是否重要？ 很高兴听到您的想法。谢谢。    提交人    /u/AnyIce3007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie7e7g/d_understanding_the_padded_tokens_of_attention/</guid>
      <pubDate>Fri, 31 Jan 2025 05:04:30 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于 FastGen 论文模型分析阶段的困惑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie6ufo/d_confusion_about_the_model_profiling_stage_of/</link>
      <description><![CDATA[      简要背景：FastGen 论文是一篇关于 KV 缓存压缩的著名著作。它提出了一种两阶段方法：首先，它为每个头部识别不同的注意模式（称为“模型分析”），然后应用相应的压缩策略。 我附上的屏幕截图包含有关第一阶段（模型分析）的所有内容，应该是独立的。但是，我发现它令人困惑，原因有二：  由于压缩后 KV 缓存大小减小，原始注意力图 A 和压缩注意力图 \text{softmax}(QK_C^\top) 的形状似乎会有所不同。绝对差 |A - \text{softmax}(QK_C^\top)| 怎么会这样呢？如果形状不匹配，如何计算？ 本文没有对方程中的绝对值运算符提供进一步的解释，让我不确定在这种情况下如何解释它。  https://preview.redd.it/va9kbkz2b9ge1.png?width=1736&amp;format=png&amp;auto=webp&amp;s=168845b68371a1b90800689c1f5a7bba8c6fd900 这是一篇口头论文ICLR，所以我想知道我是否误解了什么。不幸的是，代码存储库是空的，所以我无法检查它们的实现以进行澄清。 有人读过这篇论文并能阐明这些观点吗？    提交人    /u/StraightSpeech9295   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie6ufo/d_confusion_about_the_model_profiling_stage_of/</guid>
      <pubDate>Fri, 31 Jan 2025 04:32:35 GMT</pubDate>
    </item>
    <item>
      <title>[D] AAMAS 蓝天结果什么时候会公开？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie6p4b/d_when_will_the_aamas_blue_sky_results_be/</link>
      <description><![CDATA[AAMAS Blue Sky 结果总是备受期待，但有时很难找到有关其公开发布的信息。有人知道预计何时正式宣布或公开结果吗？AAMAS 组织有任何更新吗？    提交人    /u/Zealousideal-Hat6729   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie6p4b/d_when_will_the_aamas_blue_sky_results_be/</guid>
      <pubDate>Fri, 31 Jan 2025 04:24:01 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5qoh/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Fri, 31 Jan 2025 03:30:56 GMT</pubDate>
    </item>
    <item>
      <title>[R] 重新校准表示：Transformer 的反馈引导加权池化框架</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie5pid/r_recalibrating_representations_a_feedbackguided/</link>
      <description><![CDATA[Transformers 通常依赖单个 token ([CLS]) 或均值池化来形成序列表示，这可能会忽略历史上错误分类或特别重要的 token 的关键线索。我们提出的反馈引导加权池化 (FGWP) 添加了一种轻量级机制，可根据捕获过去表现的反馈向量重新加权 token 嵌入。通过突出显示已知具有挑战性或决定性的 token，FGWP 丰富了序列表示，而不会显著增加计算或模型大小。从情绪分析（IMDb）到大规模图像分类（ImageNet）等任务的实验表明，准确率不断提高，强调了模型的价值，该模型不仅可以处理当前输入，还可以从自身的历史成功和错误中学习，而所有这些都只需要最少的计算开销。 将很快发布到 arxiv 并希望发布到 ICML，欢迎任何反馈或建议！ https://jacobfa.github.io/stuff/Pooling.pdf    提交人    /u/jacobfa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie5pid/r_recalibrating_representations_a_feedbackguided/</guid>
      <pubDate>Fri, 31 Jan 2025 03:29:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么DeepSeek学生模型（7B参数）的表现略优于教师模型（671B参数）？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie46nq/why_does_the_deepseek_student_model_7b_parameters/</link>
      <description><![CDATA[这是论文中我无法理解的最大部分——知识提炼以匹配原始教师模型的分布是有意义的，但它是如何击败原始教师模型的？    提交人    /u/Easy_Pomegranate_982   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie46nq/why_does_the_deepseek_student_model_7b_parameters/</guid>
      <pubDate>Fri, 31 Jan 2025 02:08:58 GMT</pubDate>
    </item>
    <item>
      <title>[D] 道德数据集许可</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie1f4m/d_ethical_dataset_licenses/</link>
      <description><![CDATA[是否有任何类似 RAIL 的许可证专门用于数据集，并且限制了军事和监视等下游用例？我发现没有许可证可以完全满足我的需求。    提交人    /u/impatiens-capensis   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie1f4m/d_ethical_dataset_licenses/</guid>
      <pubDate>Thu, 30 Jan 2025 23:55:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 温度为 0 时 LLM 的非确定性行为</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ie15ev/d_nondeterministic_behavior_of_llms_when/</link>
      <description><![CDATA[嘿， 因此从理论上讲，当温度设置为 0 时，LLM 应该是确定性的。 然而，在实践中，由于硬件和其他因素的差异，情况并非如此。（示例） 有没有什么好的论文研究温度为 0 时 LLM 的非确定性行为？ 寻找深入研究根本原因、量化它等的东西。 谢谢！    提交人    /u/curryeater259   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ie15ev/d_nondeterministic_behavior_of_llms_when/</guid>
      <pubDate>Thu, 30 Jan 2025 23:43:00 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我创建了一个基准，以帮助您找到最佳的背景去除 API，实现完美的图像编辑</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idv9w6/p_i_created_a_benchmark_to_help_you_find_the_best/</link>
      <description><![CDATA[我为什么构建它 曾经尝试过背景去除 API 并想过，“这有效......直到它无效”？头发、毛发和透明度是最艰巨的挑战，大多数 API 都难以应对。我想要一种方法来对它们进行正面比较，所以我构建了一个基准和交互式评估平台。 它的作用  在具有挑战性的图像上并排比较顶级背景去除 API 交互式 Gradio 界面可轻松探索结果 自己运行 API 并查看它们如何处理棘手的细节  试试看 基准测试和演示： Hugging Face Space 代码： Hugging Face 寻求反馈  准确性 – 哪种 API 能最好地处理头发、毛发和透明度？有什么突出的成功或失败吗？ 一致性 – 结果在不同图像上是否保持稳定？ 评估方法 – 我的比较方法是否可靠，或者您有更好的想法？ Gradio 界面 – 它直观吗？您有什么改进建议吗？  帮助改进基准！ 知道应该测试的背景去除 API 吗？有破坏大多数模型的具有挑战性的图像吗？分享它们。让我们将其作为该领域 ML 工程师的首选基准。 期待您的想法！    提交人    /u/tbdb92   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idv9w6/p_i_created_a_benchmark_to_help_you_find_the_best/</guid>
      <pubDate>Thu, 30 Jan 2025 19:31:30 GMT</pubDate>
    </item>
    <item>
      <title>[R] [N] 开源 8B 评估模型在 11 个基准测试中击败 GPT-4o mini 并位居小型评委之首</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idpqas/r_n_opensource_8b_evaluation_model_beats_gpt4o/</link>
      <description><![CDATA[  由    /u/fortunemaple  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idpqas/r_n_opensource_8b_evaluation_model_beats_gpt4o/</guid>
      <pubDate>Thu, 30 Jan 2025 15:40:00 GMT</pubDate>
    </item>
    <item>
      <title>[d] 为什么“知识提炼”现在突然被贴上了盗窃的标签？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1idjtta/d_why_is_knowledge_distillation_now_suddenly/</link>
      <description><![CDATA[我们都知道，蒸馏是一种近似更精确变换的方法。但我们也知道，这也是整个想法的终结。  蒸馏有什么问题？通过模仿输出来学习“知识”这一事实对我来说毫无意义。当然，通过保持输入和输出相同，我们试图近似一个类似的变换函数，但这并不意味着它确实如此。我不明白这怎么会被贴上盗窃的标签，尤其是当整个架构和训练方法都不同的时候。     提交人    /u/The-Silvervein   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1idjtta/d_why_is_knowledge_distillation_now_suddenly/</guid>
      <pubDate>Thu, 30 Jan 2025 10:09:59 GMT</pubDate>
    </item>
    <item>
      <title>无炒作 DeepSeek-R1 [R] 阅读清单</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ideupn/no_hype_deepseekr1_reading_list/</link>
      <description><![CDATA[在过去的一年半里，我一直在运营一个研究论文俱乐部，我们在那里深入研究人工智能/机器学习领域有趣/基础的论文。因此，我们自然而然地接触到了很多与 DeepSeek-R1 相关的论文。本周，在深入研究 DeepSeek 论文时，我决定编制一份我们已经研究过的论文清单，或者我认为可以作为背景阅读的论文清单，以便更全面地了解 DeepSeek 内部发生的事情。 喝杯咖啡，享受吧！ https://www.oxen.ai/blog/no-hype-deepseek-r1-reading-list    提交人    /u/FallMindless3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ideupn/no_hype_deepseekr1_reading_list/</guid>
      <pubDate>Thu, 30 Jan 2025 04:51:07 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1iai5g6/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新帖子。鼓励创建新帖子提问的人在此处发布问题！ 帖子将保持活跃，直到下一个帖子，因此请在标题中的日期之后继续发帖。 感谢大家在上一个帖子中回答问题！    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1iai5g6/d_simple_questions_thread/</guid>
      <pubDate>Sun, 26 Jan 2025 16:00:46 GMT</pubDate>
    </item>
    </channel>
</rss>