<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>此 Reddit 子版块暂时关闭，以抗议 Reddit 终止第三方应用程序，请参阅 /r/ModCoord 和 /r/Save3rdPartyApps 了解更多信息。</description>
    <lastBuildDate>Sun, 04 Feb 2024 15:11:40 GMT</lastBuildDate>
    <item>
      <title>[D] 如果 ICLR 口头论文与我的 ICML 2020 论文明显重叠但仍然被接受，我该怎么办？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aintst/d_what_should_i_do_if_an_iclr_oral_paper/</link>
      <description><![CDATA[如果我认为已接受的口头论文没有引用我之前发表的论文，我该怎么办？更重要的是，如果他们这样做了，他们的论文就没有什么新颖性了。提交ID是6795，我的论文可以在这里找到：https://proceedings.mlr.press/v119/nguyen20c。 html。你可以自己判断。   由   提交/u/hoang-nt  /u/hoang-nt  reddit.com/r/MachineLearning/comments/1aintst/d_what_should_i_do_if_an_iclr_oral_paper/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aintst/d_what_should_i_do_if_an_iclr_oral_paper/</guid>
      <pubDate>Sun, 04 Feb 2024 13:25:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] 零射击分类</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aink6e/p_zero_shot_classification/</link>
      <description><![CDATA[在我的项目中，我想使用拥抱脸变换器从零样本分类的问题中提取类别。 问题将与疾病和医疗有关。 你有什么想法吗？或者建议？ 非常感谢   由   提交/u/Medical_Cost9675   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aink6e/p_zero_shot_classification/</guid>
      <pubDate>Sun, 04 Feb 2024 13:10:51 GMT</pubDate>
    </item>
    <item>
      <title>[P] 基于 Transformer 的句子 Transformer 去噪自动编码器无监督预训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aimq61/p_tranformerbased_denoising_autoencoder_for/</link>
      <description><![CDATA[一个新的 PyPI 包，只需 2 行即可训练句子嵌入模型。 获取句子嵌入通常需要大量数据的标记数据。然而，在许多情况和领域，标记数据很少可获取，而且此类数据的采购成本高昂。在这个项目中，我们采用了基于预先训练的基于 Transformers 的序列去噪自动编码器 (TSDAE) 的无监督过程，该编码器由达姆施塔特无处不在的知识处理实验室推出，可以实现达到域内监督的 93.1% 的性能水平。  TSDAE 模式包含两个组件：编码器和解码器。在整个训练过程中，TSDAE 将受污染的句子翻译成统一大小的向量，要求解码器利用该句子嵌入来重建原始句子。为了获得良好的重建质量，必须在编码器的句子嵌入中很好地捕获语义。随后，在推理过程中，编码器仅用于形成句子嵌入。 PyPI url :  https://pypi.org/project/tsdae GitHub : https ://github.com/louisbrulenaudet/tsdae 安装： python pip3 install tsdae nltk datasets Sentence-transformers torch  Python 代码：  ```python from tsdae import TSDAE 初始化 TSDAE 实例 instance = TSDAE() 加载数据集&lt; /h1&gt; train_dataset = instance.load_dataset_from_hf( dataset=&quot;louisbrulenaudet/cgi&quot; ) 使用数据集训练模型 model = instance.train( train_dataset=train_dataset 、 model_name=“bert-base-multilingual-uncased”、column=“output”、output_path=“output/tsdae-lemon-mbert-base” ) ```   由   提交/u/louisbrulenaudet  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aimq61/p_tranformerbased_denoising_autoencoder_for/</guid>
      <pubDate>Sun, 04 Feb 2024 12:22:25 GMT</pubDate>
    </item>
    <item>
      <title>机器学习最佳书籍 [D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aime8z/best_book_for_machine_learning_d/</link>
      <description><![CDATA[我是人工智能学生，我想了解更多关于生成对抗网络的知识，框架：PyTorch，我希望有人给我推荐一些书，无论免费还是付费版本，我想要一些更专业的水平，而不是简单的DGGAN之类的，更复杂的东西。   由   提交/u/predictor_torch  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aime8z/best_book_for_machine_learning_d/</guid>
      <pubDate>Sun, 04 Feb 2024 12:01:54 GMT</pubDate>
    </item>
    <item>
      <title>[D] 我的时间序列是否太随机而无法预测？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aileah/d_is_my_timeseries_too_random_to_be_predicted/</link>
      <description><![CDATA[嗨，我有一个时间序列，我希望预测下一个值。输入数据是多变量，目标目前是单变量。 我的第一个策略当然是展平输入并通过单层神经网络（线性回归）运行它。然后我尝试添加更多层，使用不同的激活函数、dropout、批量归一化等，但是初始结果没有任何改善。查看预测的各个示例，到目前为止，所有模型基本上只是从最新的已知值开始，并趋向于数据集的整体平均值。 我的问题是，单层神经网络是否表现良好作为或比更多层更好，尝试更先进的技术（如 Transformer、TCN、LSTM）是否还有意义，或者这只是浪费时间？ 我在想如果添加的参数甚至再多一层或两层并不能带来任何改进，这表明要捕获的数据实际上几乎没有系统趋势，而且更先进的模型实际上只是矫枉过正。如果我错了，请纠正我。  如果有人对我如何进一步调查/分析该数据集有一些建议，我们也将不胜感激。有没有办法最终证明/表明更高级的模型无法在此数据集上工作？   由   提交 /u/KaptenKalmar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aileah/d_is_my_timeseries_too_random_to_be_predicted/</guid>
      <pubDate>Sun, 04 Feb 2024 10:56:17 GMT</pubDate>
    </item>
    <item>
      <title>[R] TabLib：包含 6.27 亿个带有上下文的表的数据集</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ail5w0/r_tablib_a_dataset_of_627_million_tables_with/</link>
      <description><![CDATA[ 由   提交 /u/EducationalCicada   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ail5w0/r_tablib_a_dataset_of_627_million_tables_with/</guid>
      <pubDate>Sun, 04 Feb 2024 10:40:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 发布负面结果</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aikp5f/d_publishing_negative_results/</link>
      <description><![CDATA[我一直在从事一个机器学习研究项目，不幸的是，结果与我的假设不一致。我得到了负面结果。 虽然令人沮丧，但我相信分享这些结果具有很大的价值，因为假设本身依赖于合理的理论基础，而且结果并不是先验证据表明将会是负面的。 所以，我的问题是，负面结果可以在顶级机器学习会议（NeurIPS/ICLR/ICML/...）上发表吗？你们中有人遇到过类似的情况吗？您是如何解决这个问题的？您在著名会议上发表负面结果的努力是否成功了？   由   提交/u/Raskolnikov98   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aikp5f/d_publishing_negative_results/</guid>
      <pubDate>Sun, 04 Feb 2024 10:09:12 GMT</pubDate>
    </item>
    <item>
      <title>[D] NLP 和语音处理中有哪些 MNIST/CIFAR 级别的任务？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aiinru/d_what_are_some_mnistcifar_level_tasks_in_nlp_and/</link>
      <description><![CDATA[嗨， 我来自计算机视觉领域，从事优化器工作。我目前正在不同的玩具设置上尝试它，以了解它在实践中的优点和缺点。  NLP 和语音的 mnist/cifar 等效任务/数据集是什么（也许，仍在少数论文中使用）？如果您能指导我找到一个存储库，那就大有帮助了。   由   提交 /u/PaganPasta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aiinru/d_what_are_some_mnistcifar_level_tasks_in_nlp_and/</guid>
      <pubDate>Sun, 04 Feb 2024 07:51:51 GMT</pubDate>
    </item>
    <item>
      <title>重新表述网络：计算和数据高效语言建模的秘诀</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aighsr/rephrasing_the_web_a_recipe_for_compute/</link>
      <description><![CDATA[ 由   提交/u/rrenaud  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aighsr/rephrasing_the_web_a_recipe_for_compute/</guid>
      <pubDate>Sun, 04 Feb 2024 05:33:38 GMT</pubDate>
    </item>
    <item>
      <title>[R] 时间序列预测深度学习最新进展的文献综述。</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aifjbq/r_literature_review_of_advances_recent_in_deep/</link>
      <description><![CDATA[我写了一个关于 2024 年将深度学习应用于时间序列预测的最新文献的文献综述。我研究了最新的进展，例如更强大的变压器架构和归一化技术，如果它们可以击败 D-Linear 和 N-Linear 等简单模型。我还批评了 TimeGPT 和其他几个似乎主要是营销策略的模型。 ​ Archive.is 上的“无付费版本”，不过如果您有 Medium 帐户（如果您愿意在那里查看它），我将不胜感激。   由   提交/u/AttentionImaginary54  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aifjbq/r_literature_review_of_advances_recent_in_deep/</guid>
      <pubDate>Sun, 04 Feb 2024 04:38:43 GMT</pubDate>
    </item>
    <item>
      <title>[R] 苹果发布 MGIE！</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1aideah/r_apple_releases_mgie/</link>
      <description><![CDATA[      [ICLR&#39;24 Spotlight] 通过多模态大图像指导基于指令的图像编辑语言模型 MLLM引导的基于指令的图像编辑（MGIE）可以遵循用户指令来编辑图像论文：https://openreview.net/forum?id=S1RKWSyZ2Y 项目：https ://mllm-ie.github.io https://preview.redd.it/7abn9yflehgc1.png?width=3183&amp;format=png&amp;auto=webp&amp;s=9fc6c301f49ffaaf1c293c8f5925c603c8 c7dc24 代码/ checkpoint 也是开源的 🔥 Apple 官方仓库：https://github.com/apple/ml-mgie 带 Gradio 演示的仓库：https://github.com/tsujuifu/pytorch_mgie &lt; p&gt;https://preview.redd.it/hyqngv8nehgc1。 png?width=3736&amp;format=png&amp;auto=webp&amp;s=3a70483a7bea6e16500370cee5879e605fe7d51d   由   提交 /u/tsujuifu   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1aideah/r_apple_releases_mgie/</guid>
      <pubDate>Sun, 04 Feb 2024 02:42:32 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型努力学习长尾知识 [R]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ai7en3/large_language_models_struggle_to_learn_longtail/</link>
      <description><![CDATA[      https://arxiv.org/abs /2211.08411 ​ 摘要： 互联网蕴藏着丰富的知识——来自历史人物的生日到如何编码的教程——所有这些都可以通过语言模型来学习。然而，虽然某些信息在网络上无处不在，但其他信息却很少出现。在本文中，我们研究了大型语言模型记忆的知识与从网络上抓取的预训练数据集中的信息之间的关系。特别是，我们表明语言模型回答基于事实的问题的能力与在预训练期间看到的与该问题相关的文档数量有关。我们通过链接预训练数据集的实体和对包含与给定问答对相同的实体的文档进行计数来识别这些相关文档。我们的结果证明了众多问答数据集（例如 TriviaQA）、预训练语料库（例如 ROOTS）和模型大小（例如 176B 参数）的准确性和相关文档计数之间存在很强的相关性和因果关系。此外，虽然较大的模型更擅长学习长尾知识，但我们估计当今的模型必须扩展多个数量级，才能在预训练数据支持很少的情况下在问题上达到有竞争力的 QA 性能。最后，我们证明检索增强可以减少对相关预训练信息的依赖，为捕获长尾提供了一种有前景的方法。 ​ &amp;# x200b; https://preview .redd.it/t8f3b4flzfgc1.png?width=603&amp;format=png&amp;auto=webp&amp;s=09c243c055b2d5d9aa18192c4082970d8a1e1381   由   提交/u/we_are_mammals  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ai7en3/large_language_models_struggle_to_learn_longtail/</guid>
      <pubDate>Sat, 03 Feb 2024 21:58:49 GMT</pubDate>
    </item>
    <item>
      <title>[R] 人们还相信LLM的新兴能力吗？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/</link>
      <description><![CDATA[自从[新兴的LLM能力是海市蜃楼吗？](https://arxiv.org/pdf/2304.15004.pdf），人们似乎对涌现非常安静。但是大的[新兴能力](https://openreview.net/pdf?id=yzkSU5zdwD)论文有这一段（第 7 页）： &gt;考虑用于衡量新兴能力的评估指标也很重要（BIG-Bench，2022）。例如，使用精确的字符串匹配作为长序列目标的评估指标可能会将复合增量改进伪装成出现。类似的逻辑可能适用于多步骤或算术推理问题，其中模型仅根据是否正确获得多步骤问题的最终答案来评分，而不会给予部分正确的解决方案任何信用。然而，最终答案准确性的跳跃并不能解释为什么中间步骤的质量突然出现在随机之上，并且使用不给予部分信用的评估指标充其量是一个不完整的解释，因为在许多分类任务中仍然观察到涌现的能力（例如，图 2D-H 中的任务）。 人们怎么想？涌现是“真实的”吗？还是实质性的？   由   提交/u/uwashingtongold  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ai5uqx/r_do_people_still_believe_in_llm_emergent/</guid>
      <pubDate>Sat, 03 Feb 2024 20:50:24 GMT</pubDate>
    </item>
    <item>
      <title>[R] TimesFM：基于 1000 亿个真实世界数据点进行预训练的基础预测模型，在不同领域提供前所未有的零样本性能</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ahzmdc/r_timesfm_a_foundational_forecasting_model/</link>
      <description><![CDATA[       由   提交 /u/BlupHox   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ahzmdc/r_timesfm_a_foundational_forecasting_model/</guid>
      <pubDate>Sat, 03 Feb 2024 16:15:00 GMT</pubDate>
    </item>
    <item>
      <title>[D] 简单问题主题</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</link>
      <description><![CDATA[请在此处发布您的问题，而不是创建新线程。鼓励其他为问题创建新帖子的其他人在此处发帖！ 帖子将保持活跃状态​​，直到下一篇帖子，因此请在标题中的日期之后继续发帖。 感谢大家回答问题在上一个线程中！   由   提交 /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ad5t7g/d_simple_questions_thread/</guid>
      <pubDate>Sun, 28 Jan 2024 16:00:31 GMT</pubDate>
    </item>
    </channel>
</rss>