<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>机器学习</title>
    <link>https://www.reddit.com/r/MachineLearning/</link>
    <description>初学者 -> /r/mlquestions，AGI -> /r/singularity，职业建议 -> /r/cscareerquestions，数据集 -> r/datasets</description>
    <lastBuildDate>Sat, 16 Nov 2024 12:30:59 GMT</lastBuildDate>
    <item>
      <title>在 Unity 或 UE5 中实现 TTS 的最佳工具是什么？[D]</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/</link>
      <description><![CDATA[大家好，我需要一些建议，关于如何最好地创建一个可以在 Unity 或 Unreal Engine 中使用的离线文本转语音 (TTS) 系统。是否有任何工具或网站可以让我克隆语音、下载并在这些引擎中本地使用它？ 我正在寻找一种不依赖云服务且完全离线工作的解决方案。任何建议或经验都将不胜感激！ 谢谢！    提交人    /u/NoPrinciple1242   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsjz08/whats_the_best_tool_for_implementing_tts_in_unity/</guid>
      <pubDate>Sat, 16 Nov 2024 09:43:25 GMT</pubDate>
    </item>
    <item>
      <title>[P] 分析 UMAP 为何如此之快</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/</link>
      <description><![CDATA[      嗨，我最近花了一些时间从 UMAP 算法的实现方式以及它为什么如此之快的角度来了解 UMAP 算法的核心实现（即使它是用 Python 编写的）。我决定将算法分解为更小的步骤，在这些步骤中，我对代码进行一些小的改进（一个接一个），这样最终的结果与我从 UMAP 获得的结果非常相似。 令我惊讶的是，这些变化中的大多数只是优化代码中的技巧，以更快地运行程序或减少更新不太重要的东西的频率。当然，我的实现并没有 100% 地重现 UMAP 算法，因为它是在教育目的中完成的。 我在我的项目中提供了详细的解释，说明我必须在每个步骤中添加什么才能转向类似 UMAP 的算法。这是项目页面：https://github.com/kmkolasinski/nano-umap 如果您是一个喜欢优化代码以提高性能的人，您可能会发现这很有趣。下面是我能够得到的演示：  https://preview.redd.it/eww57c3x881e1.png?width=1921&amp;format=png&amp;auto=webp&amp;s=ed4a345e40b47782ddf39cb93eb9d03207db1160 TLDR：在 UMAP 中他们：  使用 ANN 库快速找到顶级 k-NN， 使用良好的初始化方法，使事情更稳定并且算法需要更少的更新（UMAP 使用快速谱初始化）， 使用随机负采样，这是一种简单的方法，但在实践中效果很好， 压缩 numba 性能（通过用自定义实现替换 np.dot 或 np.clip 来使代码运行得更快）， 使用某种自适应采样，这将使算法将更多时间花在更重要的向量上，从而节省不太重要的向量上的 CPU 时间     提交人    /u/kmkolasinski   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsjfq9/p_analysis_of_why_umap_is_so_fast/</guid>
      <pubDate>Sat, 16 Nov 2024 09:02:10 GMT</pubDate>
    </item>
    <item>
      <title>[D] 神经缩放定律</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/</link>
      <description><![CDATA[我想研究神经缩放定律及其产生的原因。Sp，我想看看您是否推荐一篇或一系列论文让我开始研究这些。谢谢。     提交人    /u/SmartEvening   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsix5n/d_neural_scaling_laws/</guid>
      <pubDate>Sat, 16 Nov 2024 08:22:23 GMT</pubDate>
    </item>
    <item>
      <title>[R][D]沙门氏菌机器学习研究中使用未标记数据的 10 倍交叉验证的说明</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsgwgk/rdclarification_on_10fold_crossvalidation_with/</link>
      <description><![CDATA[      我目前正在阅读一篇研究论文，重点是ML 预测鸡肉沙门氏菌分离株的疾病结果。 我正在努力理解他们的方法，尤其是他们如何处理标记和未标记数据的交叉验证。 据我所知，作者：  将标记数据集（28 个沙门氏菌分离株）分成 70％ 用于训练，30％ 用于测试。 使用各种 ML 算法（随机森林、Logit Boost、梯度提升、具有不同内核的 SVM）对 70％ 训练子集 进行 10 倍交叉验证，以根据准确性选择最佳模型。 使用选定的模型预测疾病表型未标记的数据集（205 个分离株）。 将原始标记数据与这些预测的标签相结合，并重复该过程，总共10 次迭代。  但是，他们的图表（图 1）似乎意味着 10 倍交叉验证直接应用于未标记的数据集，这与标准做法不符，因为交叉验证通常需要标记数据进行训练和验证。 我的解释是否正确，他们使用迭代半监督方法，并且在对未标记数据进行每次预测之后，他们都会使用新标记的数据重新训练模型？或者我遗漏了他们的图表的某些内容？ https://preview.redd.it/y16rjvc9f71e1.png?width=2630&amp;format=png&amp;auto=webp&amp;s=5cafd0ec39b1167e231b6d579b9a6654a44fb595    提交人    /u/Acceptable-File2674   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsgwgk/rdclarification_on_10fold_crossvalidation_with/</guid>
      <pubDate>Sat, 16 Nov 2024 06:03:46 GMT</pubDate>
    </item>
    <item>
      <title>[D] 分布式 ML 算法面试</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/</link>
      <description><![CDATA[大家好， 我即将进行一次关于分布式 ML 算法的面试（面试描述：我们将探讨和解释用于构建常见神经网络操作的基本技术，重点介绍简单而有效的实现。） 有没有什么好的资源可以用来准备这种面试？    提交人    /u/deepthought00705   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsbykc/d_distributed_ml_algorithms_interview/</guid>
      <pubDate>Sat, 16 Nov 2024 01:13:01 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 现代搜索系统在查询预处理中是否仍然需要词干提取和词形还原？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/</link>
      <description><![CDATA[考虑到 LM 的所有进步，我想知道它们在现代搜索系统中有多重要。语义嵌入通常可以帮助我们很好地理解含义。但为了有效利用历史查询项目参与度特征，我们似乎仍然需要进行这些预处理。否则，当用户搜索与常见查询略有不同时，我们很容易得到空的参与度特征？或者有没有更现代的方式来处理自由形式的查询？    提交人    /u/wenegue   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsauuz/discussion_do_modern_search_systems_still_require/</guid>
      <pubDate>Sat, 16 Nov 2024 00:16:42 GMT</pubDate>
    </item>
    <item>
      <title>[D] 对大量特征有效运行的特征选择方法（tabular、lightgbm）</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/</link>
      <description><![CDATA[有没有人知道一个好的特征选择算法（有或没有实现）可以在合理的时间内搜索大约 50-100k 个特征？我正在使用 lightgbm。直觉是我需要模型中大约 20-100 个最终特征。想在大海捞针。表格数据，大约有 100-500k 条数据记录可供使用。根据我的经验，常见的特征选择方法在计算上无法扩展。此外，我发现过度拟合是这么大的搜索空间的一个问题。     提交人    /u/acetherace   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gsah8e/d_feature_selection_methods_that_operate/</guid>
      <pubDate>Fri, 15 Nov 2024 23:58:07 GMT</pubDate>
    </item>
    <item>
      <title>[R][D]抽象推理的测试时间训练</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/</link>
      <description><![CDATA[https://arxiv.org/pdf/2411.07279 顺便问一下，大家知道有什么研究尝试在回答问题之前对模型进行微调吗？我的意思是它可能适用于上下文信息检索，但我想知道它对更多推理繁重的任务的影响。计算悬置仍然会很大。    提交人    /u/Due-Pangolin325   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs9lao/rdtest_time_training_for_abstract_reasoning/</guid>
      <pubDate>Fri, 15 Nov 2024 23:15:29 GMT</pubDate>
    </item>
    <item>
      <title>[R] 卷积可微分逻辑门网络</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/</link>
      <description><![CDATA[摘要 随着机器学习模型的推理成本不断增加，人们对具有快速高效推理能力​​的模型的兴趣日益浓厚。最近，提出了一种通过可微分松弛直接学习逻辑门网络的方法。逻辑门网络比传统的神经网络方法更快，因为它们的推理只需要逻辑门运算符（例如 NAND、OR 和 XOR），这些运算符是当前硬件的底层构建块，可以高效执行。我们在此想法的基础上，通过深度逻辑门树卷积、逻辑或池化和残差初始化对其进行了扩展。这允许将逻辑门网络扩大一个数量级以上并利用卷积范式。在 CIFAR-10 上，我们仅使用 6100 万个逻辑门就实现了 86.29% 的准确率，这比 SOTA 有所提高，同时体积却缩小了 29 倍。 被 Neurips 2024 接受，“SOTA”在这里表示可比方法。我发现这篇论文真的很有趣，尽管非玩具网络似乎训练起来非常昂贵。好奇其他人怎么想？    提交人    /u/jacobgorm   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs92mb/r_convolutional_differentiable_logic_gate_networks/</guid>
      <pubDate>Fri, 15 Nov 2024 22:51:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] 读博士还是不读博士</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/</link>
      <description><![CDATA[我想这个问题已经被问过无数次了，但让我再问一次。 我目前在 MSFT 担任应用科学家。然而，我更想找科学职位，比如 DeepMind 的研究科学家。虽然工作并不特别需要博士学位，但竞争非常激烈，而且有很多博士学位持有者。 我确实喜欢研究，想读博士学位，但我总是问自己这是否真的值得。 这肯定是一个开放性问题，请随时分享您的想法。    提交人    /u/oddhvdfscuyg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs688q/d_to_phd_or_not_to_phd/</guid>
      <pubDate>Fri, 15 Nov 2024 20:44:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] Neurips 2024 酒店室友搜索</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/</link>
      <description><![CDATA[2024 年 Neurips 会场周围的酒店相当昂贵，我正在寻找室友来分摊费用（我的大学对他们愿意报销的每晚酒店费用有限制）。我目前已在世纪广场酒店预订了周二至周日的房间，距离会议中心 0.9 英里。每晚房费为 414 美元。如果有人想分摊房费，请联系我们！此外，如果您能与您的研究小组或您认识的其他与会者分享这篇文章，那将会很有帮助。 如果您不确定是否要与完全陌生的人同住，您可以通过我的个人网站 (https://mtcrawshaw.github.io/) 了解我一点，该网站有指向我的谷歌学术页面、简历等的链接。我确实在会议上发表了一篇关于联邦学习/分布式优化领域的论文。我只是一名研究生，想让会议变得负担得起！谢谢。    提交人    /u/ssbm_crawshaw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gs0gj8/d_neurips_2024_hotel_roommate_search/</guid>
      <pubDate>Fri, 15 Nov 2024 16:37:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 当您说“LLM”时，有多少人也考虑过 BERT 之类的东西？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/</link>
      <description><![CDATA[我不断遇到这种争论，但对我来说，当我听到“LLM”时，我的假设是只有解码器的模型，这些模型有数十亿个参数。似乎有些人会将 BERT-base 纳入 LLM 系列，但我不确定这是否正确？我想从技术上讲是这样，但每次我听到有人说“我如何使用 LLM 进行 XYZ”时，他们通常会提到 LLaMA 或 Mistral 或 ChatGPT 或类似的东西。    提交人    /u/Seankala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1grxbdp/d_when_you_say_llm_how_many_of_you_consider/</guid>
      <pubDate>Fri, 15 Nov 2024 14:16:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Ilya Sutskever 的 AI 阅读清单中丢失的阅读项目</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/</link>
      <description><![CDATA[这篇博文试图找出今年早些时候出现的热门 AI 阅读清单中遗漏了哪些论文，该清单归功于 Ilya Sutskever 及其声称涵盖了 2020 年 AI 领域“90% 的重要内容”： https://tensorlabbet.com/2024/11/11/lost-reading-items/ 今年早些时候，大约 40 篇论文中只有 27 篇在网上分享，因此关于哪些作品足够重要而值得收录的理论有很多。这里讨论了一些与元学习和竞争性自我博弈相关的明显候选者。但也有几位值得注意的作者，如 Yann LeCun 和 Ian Goodfellow 未列入名单。 从我的角度来看，甚至关于 U-Net、YOLO 检测器、GAN、WaveNet、Word2Vec 等的论文也应该包括在内，所以我很好奇对此的更多看法！    提交人    /u/AccomplishedCat4770   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1grti0x/d_the_lost_reading_items_of_ilya_sutskevers_ai/</guid>
      <pubDate>Fri, 15 Nov 2024 10:34:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 自我推销帖</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/</link>
      <description><![CDATA[请发布您的个人项目、初创公司、产品展示、协作需求、博客等。 请提及产品和服务的付款和定价要求。 请勿发布链接缩短器、链接聚合器网站或自动订阅链接。  任何滥用信任的行为都会导致禁令。 鼓励其他为问题创建新帖子的人在这里发帖！ 主题将保持活跃，直到下一个主题，因此请在标题中的日期之后继续发帖。  元：这是一个实验。如果社区不喜欢这样，我们将取消它。这是为鼓励社区中的人们通过不在主线程上发垃圾邮件来推广他们的工作。    提交人    /u/AutoModerator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1gnrb08/d_selfpromotion_thread/</guid>
      <pubDate>Sun, 10 Nov 2024 03:15:11 GMT</pubDate>
    </item>
    <item>
      <title>[D] 每月谁在招聘以及谁想被招聘？</title>
      <link>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</link>
      <description><![CDATA[对于职位发布，请使用此模板  招聘：[地点]，薪资：[]，[远程 | 搬迁]，[全职 | 合同 | 兼职]和[简要概述，您在寻找什么]  对于那些正在找工作的人，请使用此模板  希望被雇用：[地点]，薪资期望：[]，[远程 | 搬迁]，[全职 | 合同 |兼职] 简历：[简历链接] 和 [简要概述，您在寻找什么]  ​ 请记住，这个社区面向有经验的人。    提交人    /u/AutoModerator   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/MachineLearning/comments/1ftdkmb/d_monthly_whos_hiring_and_who_wants_to_be_hired/</guid>
      <pubDate>Tue, 01 Oct 2024 02:30:17 GMT</pubDate>
    </item>
    </channel>
</rss>