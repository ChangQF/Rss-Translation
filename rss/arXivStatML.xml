<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 22 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>平均梯度外积作为深度神经崩溃的机制</title>
      <link>https://arxiv.org/abs/2402.13728</link>
      <description><![CDATA[arXiv:2402.13728v1 公告类型：交叉
摘要：深度神经崩溃（DNC）是指深度神经网络（DNN）最后层中数据表示的令人惊讶的刚性结构。尽管人们已经在多种环境下对这种现象进行了测量，但人们对它的出现却只有部分了解。在这项工作中，我们提供了大量证据表明 DNC 的形成主要是通过平均梯度外积 (AGOP) 的深度特征学习而发生的。与通过特征不可知的方法（例如无约束特征模型）解释神经崩溃的努力相比，这又向前迈进了一步。我们继续提供证据，证明正确的奇异向量和权重值是 DNN 中大多数类内变异性崩溃的原因。正如最近的工作所示，这种奇异结构与 AGOP 的结构高度相关。然后，我们通过实验和理论证明 AGOP 会导致随机初始化的神经网络中的神经崩溃。特别是，我们证明了深度递归特征机（一种最初作为卷积神经网络中 AGOP 特征学习的抽象而引入的方法）展示了 DNC。]]></description>
      <guid>https://arxiv.org/abs/2402.13728</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>基于马尔可夫链蒙特卡罗的深度展开梯度下降的收敛加速</title>
      <link>https://arxiv.org/abs/2402.13608</link>
      <description><![CDATA[arXiv:2402.13608v1 公告类型：交叉
摘要：本研究提出了一种可训练的基于采样的求解器，使用称为深度展开的深度学习技术来解决组合优化问题（COP）。所提出的求解器基于结合马尔可夫链蒙特卡罗（MCMC）和梯度下降的 Ohzeki 方法，其步长通过最小化损失函数来训练。在训练过程中，我们提出了一种基于采样的梯度估计，用方差估计代替自微分，从而避免了由于MCMC的不可微性而导致的反向传播失败。几个 COP 的数值结果表明，与原始 Ohzeki 方法相比，所提出的求解器显着加快了收敛速度。]]></description>
      <guid>https://arxiv.org/abs/2402.13608</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>一种限制尾部概率的方法</title>
      <link>https://arxiv.org/abs/2402.13662</link>
      <description><![CDATA[arXiv:2402.13662v1 公告类型：交叉
摘要：我们提出了一种对连续随机变量（RV）的右尾概率和左尾概率进行上限和下限的方法。对于具有概率密度函数$f_X(x)$的RV$X$的右尾概率，该方法需要首先设置一个连续、正且严格递减的函数$g_X(x)$，使得$-f_X(x)/ g&#39;_X(x)$ 是一个递减函数，$\forall x&gt;x_0$，分别产生上限和下限，以 $-f_X(x) g_X(x)/g&#39;_X 的形式给出(x)$, $\forall x&gt;x_0$，其中 $x_0$ 是某个点。类似地，对于$X$的左尾概率的上下界，该方法需要首先设置一个连续的、正的、严格递增的函数$g_X(x)$，使得$f_X(x)/g&#39;_X( x)$ 是一个增减函数，$\forall x&lt;x_0$。我们提供了一些函数 $g_X(x)$ 的良好候选示例。我们还在新界限与马尔可夫不等式和切尔诺夫界限之间建立了联系。此外，我们提供了一种迭代方法，用于在某些条件下获得更严格的下限和上限。最后，我们提供数值示例，其中我们显示了对于某些选定的 $g_X(x)$ 的这些边界的紧密度。]]></description>
      <guid>https://arxiv.org/abs/2402.13662</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>研究回归中的直方图损失</title>
      <link>https://arxiv.org/abs/2402.13425</link>
      <description><![CDATA[arXiv:2402.13425v1 公告类型：交叉
摘要：在回归中，训练对整个分布进行建模的神经网络变得越来越普遍，即使预测只需要平均值。这种额外的建模通常会带来性能提升，但改进背后的原因尚不完全清楚。本文研究了一种最新的回归方法，即直方图损失，它涉及通过最小化目标分布和灵活的直方图预测之间的交叉熵来学习目标变量的条件分布。我们设计理论和实证分析来确定这种性能增益出现的原因和时间，以及损失的不同组成部分如何促成它。我们的结果表明，在此设置中学习分布的好处来自于优化的改进，而不是学习更好的表示。然后，我们展示了直方图损失在常见深度学习应用中的可行性，而无需进行昂贵的超参数调整。]]></description>
      <guid>https://arxiv.org/abs/2402.13425</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>一种全局求解低维k均值聚类问题的割平面算法</title>
      <link>https://arxiv.org/abs/2402.13595</link>
      <description><![CDATA[arXiv:2402.13595v1 公告类型：交叉
摘要：聚类是数据科学和机器学习中最基本的工具之一，k-means聚类是最常见的此类方法之一。 k-means问题有多种近似算法，但计算全局最优解通常是NP困难的。在本文中，我们考虑低维数据实例的 k 均值问题，并将其表述为结构化凹分配问题。这使我们能够利用低维结构，并在合理的时间内针对具有多个集群的大型数据集解决全局最优问题。该方法建立在迭代解决小型凹问题和大型线性规划问题的基础上。这给出了一系列可行的解决方案以及我们显示收敛到零最优性差距的边界。本文结合了全局优化理论的方法来加速该过程，并提供了其性能的数值结果。]]></description>
      <guid>https://arxiv.org/abs/2402.13595</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>走向 Transformers：用 Transformers 彻底改变混合整数程序的解决方案</title>
      <link>https://arxiv.org/abs/2402.13380</link>
      <description><![CDATA[arXiv:2402.13380v1 公告类型：交叉
摘要：在这项研究中，我们介绍了一种创新的深度学习框架，该框架采用变压器模型来解决混合整数程序的挑战，特别关注容量调整问题（CLSP）。据我们所知，我们的方法是第一个利用变压器来预测混合整数规划（MIP）问题的二进制变量的方法。具体来说，我们的方法利用编码器解码器变压器处理顺序数据的能力，使其非常适合预测指示 CLSP 每个时期的生产设置决策的二进制变量。这个问题本质上是动态的，我们需要在约束下处理顺序决策。我们提出了一种有效的算法，其中通过变压器神经网络学习 CLSP 解决方案。在测试的 240K 基准 CLSP 实例中，所提出的后处理 Transformer 算法在求解时间、最佳间隙和不可行性百分比方面超越了最先进的求解器、CPLEX 和长短期记忆 (LSTM)。训练 ML 模型后，对模型进行推理（包括后处理），将 MIP 简化为线性程序 (LP)。这将基于 ML 的算法与 LP 求解器相结合，转变为多项式时间近似算法，以解决众所周知的 NP-Hard 问题，并具有近乎完美的解质量。]]></description>
      <guid>https://arxiv.org/abs/2402.13380</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>具有领域知识先验的贝叶斯神经网络</title>
      <link>https://arxiv.org/abs/2402.13410</link>
      <description><![CDATA[arXiv:2402.13410v1 公告类型：交叉
摘要：贝叶斯神经网络（BNN）最近因其量化模型不确定性的能力而受到欢迎。然而，为 BNN 指定捕获相关领域知识的先验通常极具挑战性。在这项工作中，我们提出了一个框架，用于通过变分推理将一般形式的领域知识（即可以用损失函数表示的任何知识）集成到 BNN 先验中，同时实现计算上高效的后验推理和采样。具体来说，我们的方法导致神经网络权重的先验，将高概率质量分配给与我们的领域知识更好地一致的模型，从而导致后验样本也表现出这种行为。我们表明，使用我们提出的领域知识先验的 BNN 优于那些使用标准先验（例如，各向同性高斯、高斯过程）的 BNN，成功地融合了多种类型的先验信息，例如公平性、物理规则和医疗保健知识，并实现了更好的预测性能。我们还提出了在不同模型架构之间转移学习先验的技术，展示了它们在各种设置中的广泛实用性。]]></description>
      <guid>https://arxiv.org/abs/2402.13410</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>具有深层结构（随机）特征的渐近学习</title>
      <link>https://arxiv.org/abs/2402.13999</link>
      <description><![CDATA[arXiv:2402.13999v1 公告类型：新
摘要：对于一大类特征图，我们在输入维度、隐藏层宽度和训练样本数量成比例较大的高维限制下，提供与学习读出层相关的测试误差的严格渐近特征。这种表征是根据特征的总体协方差来表述的。我们的工作部分是受到高斯彩虹神经网络学习问题的推动，即具有随机但结构化权重的深度非线性全连接网络，其行向协方差进一步允许依赖于先前层的权重。对于此类网络，我们还根据权重矩阵推导出特征协方差的封闭式公式。我们进一步发现，在某些情况下，我们的结果可以捕获在梯度下降下训练的深度有限宽度神经网络学习到的特征图。]]></description>
      <guid>https://arxiv.org/abs/2402.13999</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>Thresholded Oja 做稀疏 PCA 吗？</title>
      <link>https://arxiv.org/abs/2402.07240</link>
      <description><![CDATA[arXiv:2402.07240v2 公告类型：交叉
摘要：我们考虑当比率 $d/n \rightarrow c &gt; 0$ 时的稀疏主成分分析 (PCA) 问题。在离线设置中稀疏 PCA 的最佳速率方面已经做了很多工作，其中所有数据都可用于多次传递。相反，当总体特征向量为 $s$ 稀疏时，具有 $O(d)$ 存储空间和 $O(nd)$ 时间复杂度的流算法通常需要强大的初始化条件或具有次优误差。我们证明了一种对 Oja 算法的输出（Oja 向量）进行阈值和重新归一化的简单算法可以获得接近最优的错误率。这是非常令人惊讶的，因为在没有阈值处理的情况下，Oja 向量有很大的误差。我们的分析以非归一化 Oja 向量的条目为中心，其中涉及独立随机矩阵的乘积在随机初始向量上的投影。这是不平凡和新颖的，因为之前对 Oja 算法和矩阵乘积的分析是在总体协方差矩阵的迹有界时完成的，而在我们的设置中，这个数量可以大到 $n$。]]></description>
      <guid>https://arxiv.org/abs/2402.07240</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>统计课程学习：实现预言机风险的消除算法</title>
      <link>https://arxiv.org/abs/2402.13366</link>
      <description><![CDATA[arXiv:2402.13366v1 公告类型：交叉
摘要：我们考虑参数预测环境中课程学习（CL）的统计版本。学习器需要估计目标参数向量，并且可以自适应地从目标模型或与目标模型相似但噪声较小的其他源模型收集样本。我们考虑三种类型的学习者，具体取决于他们收到的辅助信息的水平。前两个被称为强/弱预言学习器，接收有关模型的高/低程度信息，并使用这些信息进行学习。第三个是完全自适应学习器，在没有任何先验信息的情况下估计目标参数向量。在单源情况下，我们提出了一种消除学习方法，其风险与强预言学习器的风险相匹配。在多源情况下，我们主张弱预言学习者的风险是适应性学习者风险的现实基准。我们开发了一种自适应多轮消除 CL 算法，并描述了实例相关条件，使其风险与弱预言学习器的风险相匹配。我们考虑实例相关的极小极大下界，并讨论与定义界限实例类相关的挑战。我们推导出两个极小极大下界，并确定弱预言学习器性能达到极小极大最优的条件。]]></description>
      <guid>https://arxiv.org/abs/2402.13366</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>多任务半监督学习的大维分析</title>
      <link>https://arxiv.org/abs/2402.13646</link>
      <description><![CDATA[arXiv:2402.13646v1 公告类型：新
摘要：本文对一个简单但用途广泛的分类模型进行了大维度研究，包括多任务和半监督学习，并考虑了不确定标签。使用随机矩阵理论的工具，我们描述了一些关键泛函的渐近性，这使我们一方面能够预测算法的性能，另一方面揭示了一些关于如何有效使用算法的反直觉指导。该模型足够强大，可以提供良好的性能保证，也足够简单，可以提供对其行为的深入洞察。]]></description>
      <guid>https://arxiv.org/abs/2402.13646</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>用于对科学机器学习中的任意不确定性进行建模的概率神经网络 (PNN)</title>
      <link>https://arxiv.org/abs/2402.13945</link>
      <description><![CDATA[arXiv:2402.13945v1 公告类型：新
摘要：本文研究了使用概率神经网络（PNN）来模拟偶然不确定性，这是指系统输入输出关系的固有变异性，通常以不等方差或异方差为特征。与产生确定性输出的传统神经网络不同，PNN 生成目标变量的概率分布，从而可以确定回归场景中的预测平均值和区间。本文的贡献包括开发概率距离度量来优化 PNN 架构、在受控数据集中部署 PNN 以及涉及纤维增强复合材料的实际材料科学案例。研究结果证实，PNN 可以有效地模拟任意不确定性，并且比常用的高斯过程回归更适合用于此目的。具体来说，在现实世界的科学机器学习环境中，PNN 产生非常准确的输出均值估计，R 平方分数接近 0.97，并且它们的预测间隔表现出接近 0.80 的高相关系数，与观察到的数据间隔密切匹配。因此，这项研究有助于不断探索利用神经网络复杂的表征能力来描述科学问题中复杂的输入输出关系。]]></description>
      <guid>https://arxiv.org/abs/2402.13945</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>自主学习的维度</title>
      <link>https://arxiv.org/abs/2402.13400</link>
      <description><![CDATA[arXiv:2402.13400v1 公告类型：新
摘要：自 20 世纪 90 年代初以来，理解自主学习的复杂性一直是在线学习理论界关注的一个重要问题。在这个框架内，学习者可以自适应地选择下一个数据点来进行预测，这与对抗性在线学习中的设置不同。
  在本文中，我们研究了二元和多类设置中的自主学习复杂性，并开发了一个维度，即 $SDdim$，它准确地表征了任何概念类的自主学习错误界限。 $SDdim$ 背后的直觉可以理解为一种名为“标签游戏”的两人游戏。有了这个两人游戏，我们在大量示例上计算了 $SDdim$，在轴对齐矩形、VC 维度 $1$ 类和线性分隔符上得到了显着的结果。我们展示了几个可学习性差距，重点关注自主学习和离线序列学习模型，其中包括最佳或最差排序。最后，我们将分析扩展到自我导向的二元不可知设置，在该设置中我们得出上限和下限。]]></description>
      <guid>https://arxiv.org/abs/2402.13400</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>高维正则回归中的Bootstrap和子采样分析</title>
      <link>https://arxiv.org/abs/2402.13622</link>
      <description><![CDATA[arXiv:2402.13622v1 公告类型：新
摘要：我们研究了用于估计统计模型不确定性的流行重采样方法，例如子采样、引导和折刀法，以及它们在高维监督回归任务中的性能。我们在广义线性模型（例如岭回归和逻辑回归）的背景下，对这些方法估计的偏差和方差提供严格的渐近描述，并以协变量的样本数量 $n$ 和维度 $d$ 增长为限以相当的固定利率 $\alpha\!=\! n/d$。我们的发现有三个方面：i）重采样方法在高维中充满问题，并表现出这些情况下典型的双重下降行为； ii）只有当 $\alpha$ 足够大时，它们才能提供一致且可靠的误差估计（我们给出收敛率）； iii) 在与现代机器学习实践相关的过度参数化机制 $\alpha\!&lt;\!1$ 中，即使采用最佳正则化，他们的预测也不一致。]]></description>
      <guid>https://arxiv.org/abs/2402.13622</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>利用 PAC-Bayes 理论和吉布斯分布实现具有复杂性度量的泛化界限</title>
      <link>https://arxiv.org/abs/2402.13285</link>
      <description><![CDATA[arXiv:2402.13285v1 公告类型：新
摘要：在统计学习理论中，泛化界限通常涉及所考虑的理论框架所施加的复杂性度量。这限制了这种界限的范围，因为算法中使用了其他形式的容量测量或正则化。在本文中，我们利用分解的 PAC-Bayes 界限的框架来导出可通过任意复杂性度量实例化的通用泛化界限。证明这一结果的一个技巧是考虑一种常用的分布族：吉布斯分布。我们的界限共同代表假设和学习样本的概率，这使得复杂性能够适应泛化差距，因为它可以被定制以适应假设类和任务。]]></description>
      <guid>https://arxiv.org/abs/2402.13285</guid>
      <pubDate>Thu, 22 Feb 2024 06:16:55 GMT</pubDate>
    </item>
    </channel>
</rss>