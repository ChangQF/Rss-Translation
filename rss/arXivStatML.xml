<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 26 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>可验证的提升树集成</title>
      <link>https://arxiv.org/abs/2402.14988</link>
      <description><![CDATA[arXiv:2402.14988v1 公告类型：交叉
摘要：可验证学习提倡训练能够进行有效安全验证的机器学习模型。先前的研究表明，特定类别的决策树集成（称为大范围集成）允许在多项式时间内针对任何基于规范的攻击者进行鲁棒性验证。这项研究将之前的可验证学习工作从基本集成方法（即硬性多数投票）扩展到高级增强树集成，例如使用 XGBoost 或 LightGBM 训练的集成。我们的正式结果表明，当考虑基于 $L_\infty$-范数的攻击者时，鲁棒性验证可以在多项式时间内实现，但对于其他基于范数的攻击者来说仍然是 NP 困难的。尽管如此，我们提出了一种伪多项式时间算法，用于基于任何 $p \in \mathbb{N} \cup \{0\}$ 的 $L_p$-范数来验证针对攻击者的鲁棒性，这在实践中提供了出色的性能。我们的实验评估表明，大范围增强的集成对于实际采用来说足够准确，同时能够进行有效的安全验证。]]></description>
      <guid>https://arxiv.org/abs/2402.14988</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>数据预处理方法、特征选择技术和机器学习模型的比较分析，以提高不平衡遗传数据的分类和回归性能</title>
      <link>https://arxiv.org/abs/2402.14980</link>
      <description><![CDATA[arXiv:2402.14980v1 公告类型：交叉
摘要：基因组测序的快速进步导致大量基因组数据的收集。研究人员可能有兴趣使用此类数据的机器学习模型来预测基因突变的致病性或临床意义。然而，许多遗传数据集包含不平衡的目标变量，这给机器学习模型带来了挑战：回归任务中的观察结果倾斜/不平衡，或者分类任务中的类别不平衡。遗传数据集通常也是高基数，并且包含倾斜的预测变量，这带来了进一步的挑战。我们的目的是研究数据预处理、特征选择技术和模型选择对在这些数据集上训练的模型性能的影响。我们通过 5 倍交叉验证来测量性能，并比较不同技术组合的平均 r 平方和准确度指标。我们发现预测变量或目标变量中的异常值/偏差不会对回归模型构成挑战。我们还发现，类别不平衡的目标变量和倾斜的预测变量对分类性能几乎没有影响。随机森林是用于不平衡回归任务的最佳模型。虽然我们的研究使用遗传数据集作为现实世界应用的示例，但我们的研究结果可以推广到任何类似的数据集。]]></description>
      <guid>https://arxiv.org/abs/2402.14980</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>通过分数拉普拉斯特征图的非平滑非参数回归</title>
      <link>https://arxiv.org/abs/2402.14985</link>
      <description><![CDATA[arXiv:2402.14985v1 公告类型：交叉
摘要：我们针对真实回归函数不一定平滑的情况开发了非参数回归方法。更具体地说，我们的方法使用分数拉普拉斯算子，旨在处理真实回归函数位于阶数为 $s\in (0,1)$ 的 $L_2$-分数 Sobolev 空间中的情况。该函数类是位于平方可积函数空间和由可微函数组成的一阶 Sobolev 空间之间的希尔伯特空间。它包含分数幂函数、分段常数或多项式函数以及凹凸函数作为典型示例。对于所提出的方法，我们证明了 $n^{-\frac{2s}{2s+d}}$ 阶样本内均方估计误差的上限，其中 $d$ 是维度，$s$是前面提到的顺序参数，$n$ 是观测值的数量。我们还提供了初步的实证结果，验证了所开发的估计器的实际性能。]]></description>
      <guid>https://arxiv.org/abs/2402.14985</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>在冻结网络中的部分搜索足以找到强彩票</title>
      <link>https://arxiv.org/abs/2402.14029</link>
      <description><![CDATA[arXiv:2402.14029v1 公告类型：交叉
摘要：随机初始化的密集网络包含无需权重学习即可实现高精度的子网络——强彩票（SLT）。最近，Gadhikar 等人。 (2023) 从理论上和实验上证明，SLT 也可以在随机修剪的源网络中找到，从而减少 SLT 搜索空间。然而，这将搜索限制为比源更稀疏的 SLT，从而由于无意中的高稀疏性而导致精度较差。本文提出了一种将 SLT 搜索空间减小任意比率的方法，该比率与所需的 SLT 稀疏性无关。通过冻结初始权重的随机子集，将其从搜索空间中排除——即通过永久修剪它们或将它们锁定为 SLT 的固定部分。事实上，在这样一个缩小的搜索空间中，SLT 的存在在理论上是由我们随机冻结变量的子集和近似保证的。除了减少搜索空间之外，还可以利用随机冻结模式来减少推理中的模型大小。此外，实验结果表明，与从密集或随机修剪的源网络获得的 SLT 相比，该方法找到的 SLT 具有更好的精度和模型大小权衡。特别是，在冻结图神经网络中发现的 SLT 比其权重训练的对应物实现了更高的准确性，同时将模型大小减少了 40.3 美元\times$。]]></description>
      <guid>https://arxiv.org/abs/2402.14029</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>评估执法系统中种族偏见的因果框架</title>
      <link>https://arxiv.org/abs/2402.14959</link>
      <description><![CDATA[arXiv:2402.14959v1 公告类型：交叉
摘要：我们有兴趣开发一种数据驱动的方法来评估执法系统中种族引起的偏见。虽然最近的工作利用警察停车数据在警民互动的背景下解决了这个问题，但它们有两个关键的局限性。首先，只有在种族之外还考虑到真正的犯罪行为时，偏见才能得到正确的量化，但在之前的研究中却没有这种情况。其次，执法系统是多阶段的，因此重要的是在“相互作用的因果链”中隔离偏见的真正来源，而不是简单地关注最终结果；这有助于指导改革。在这项工作中，我们通过提出一个包含犯罪行为的多阶段因果框架来应对这些挑战。我们提供了理论特征和相关的数据驱动方法来评估（a）是否存在任何形式的种族偏见，以及（b）如果存在，则评估这种种族和犯罪方面的偏见的主要来源。我们的框架确定了三种具有不同特征的典型场景：在（1）机场安全等环境中，观察到的针对某个种族的偏见的主要来源可能是执法部门对该种族无辜者的偏见； (2) 人工智能赋能的警务，观察到的针对某个种族的偏见的主要来源可能是对该种族犯罪分子的执法偏见； (3) 警察与平民的互动，观察到的针对某个种族的偏见的主要来源可能是执法部门针对该种族的偏见，或者是公众在报道中针对其他种族的偏见。通过使用警察与平民互动数据和 911 呼叫数据进行的广泛实证研究，我们发现了这种反直觉现象的一个实例：在新奥尔良，观察到的偏见是针对多数种族的，其可能的原因是过度的种族偏见。公众报告（通过 911 电话）涉及少数族裔的事件。]]></description>
      <guid>https://arxiv.org/abs/2402.14959</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:43 GMT</pubDate>
    </item>
    <item>
      <title>带有潜在变量的套索：有效估计、协变量重新缩放和计算统计差距</title>
      <link>https://arxiv.org/abs/2402.15409</link>
      <description><![CDATA[arXiv:2402.15409v1 公告类型：新
摘要：众所周知，当感兴趣的协变量具有很强的相关性时，Lasso 的统计性能会受到显着影响。特别是，Lasso 的预测误差比计算效率低下的替代方案（例如最佳子集选择）要糟糕得多。由于稀疏线性回归问题中存在大量推测的计算统计权衡，因此通常不可能缩小这一差距。
  在这项工作中，我们提出了一种自然稀疏线性回归设置，其中协变量之间的强相关性来自未观察到的潜在变量。在这种情况下，我们分析了由强相关性引起的问题，并设计了一个非常简单的解决方案。虽然协变量标准归一化的 Lasso 失败了，但协变量存在异构缩放，Lasso 将突然获得强有力的可证明的估计保证。此外，我们设计了一个简单、高效的程序来计算这种“智能缩放”。
  由此产生的“重新缩放套索”算法的样本复杂性（在最坏的情况下）对基础信号的稀疏性产生二次依赖。虽然这种依赖性在信息理论上不是必需的，但我们通过低次多项式方法证明它在多项式时间算法中是最优的。这一论点揭示了稀疏线性回归和带有近临界负尖峰的稀疏 PCA 特殊版本之间的新联系。后一个问题可以被认为是学习稀疏奇偶校验的实值模拟。使用它，我们还为密切相关的学习高斯图模型问题建立了第一个计算统计差距。]]></description>
      <guid>https://arxiv.org/abs/2402.15409</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:42 GMT</pubDate>
    </item>
    <item>
      <title>使用恒定和衰减学习率的随机梯度下降的迭代和随机一阶预言复杂性</title>
      <link>https://arxiv.org/abs/2402.15344</link>
      <description><![CDATA[arXiv:2402.15344v1 公告类型：新
摘要：随机梯度下降（SGD）是训练深度神经网络的最简单的一阶优化器，其性能不仅取决于学习率，还取决于批量大小。它们都会影响训练所需的迭代次数和随机一阶预言 (SFO) 复杂性。特别是，之前的数值结果表明，对于使用恒定学习率的 SGD，当批量大小增加时，训练所需的迭代次数会减少，并且训练所需的 SFO 复杂度在临界批量大小处最小化，并且会增加一旦批量大小超过该大小。在这里，我们研究了使用恒定或衰减学习率的 SGD 深度学习中非凸优化所需的批量大小与迭代和 SFO 复杂性之间的关系，并表明使用临界批量大小的 SGD 可以最小化 SFO 复杂性。我们还提供了 SGD 与现有一阶优化器的数值比较，并展示了使用临界批量大小的 SGD 的有用性。此外，我们表明测量的临界批量大小接近于我们的理论结果估计的大小。]]></description>
      <guid>https://arxiv.org/abs/2402.15344</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:41 GMT</pubDate>
    </item>
    <item>
      <title>病例对照研究下逻辑回归的高效半监督推理</title>
      <link>https://arxiv.org/abs/2402.15365</link>
      <description><![CDATA[arXiv:2402.15365v1 公告类型：新
摘要：半监督学习在统计学和机器学习领域受到越来越多的关注。在半监督学习设置中，收集具有结果和协变量的标记数据集以及仅具有协变量的未标记数据集。我们考虑半监督设置中的推理问题，其中标记数据的结果是二进制的，并且标记数据是通过病例对照抽样收集的。案例控制抽样是缓解二进制数据中不平衡结构的有效抽样方案。在逻辑模型假设下，病例对照数据仍然可以为回归模型的斜率参数提供一致的估计量。然而，截距参数是不可识别的。因此，边际病例比例无法根据病例对照数据进行估计。我们发现，随着未标记数据的可用性，可以在半监督学习设置中识别截距参数。我们构建观察到的标记和未标记数据的似然函数，并通过迭代算法获得最大似然估计量。所提出的估计量被证明是一致的、渐近正态的和半参数有效的。进行了广泛的模拟研究以显示所提出方法的有限样本性能。结果表明，未标记数据不仅有助于识别截距，而且提高了斜率参数的估计效率。同时，该方法可以准确估计边缘案例比例。]]></description>
      <guid>https://arxiv.org/abs/2402.15365</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:41 GMT</pubDate>
    </item>
    <item>
      <title>统计不可知回归：一种验证回归模型的机器学习方法</title>
      <link>https://arxiv.org/abs/2402.15213</link>
      <description><![CDATA[arXiv:2402.15213v1 公告类型：新
摘要：回归分析是统计建模的核心主题，旨在估计因变量（通常称为响应变量）与一个或多个自变量（即解释变量）之间的关系。线性回归是迄今为止在预测、预报或因果推理等多个研究领域中执行此任务最流行的方法。除了解决线性回归问题的各种经典方法（例如普通最小二乘法、岭回归或套索回归）之外（这些方法通常是更高级的机器学习 (ML) 技术的基础），后者已在没有正式定义的情况下成功应用于此场景具有统计学意义。最多进行基于经验测量（例如残差或准确性）的排列或经典分析，以反映 ML 估计检测的更大能力。在本文中，我们介绍了一种名为统计不可知回归（SAR）的方法，用于使用最坏情况分析来评估基于实际风险的集中不等式的基于机器学习的线性回归的统计显着性。为了实现这个目标，与分类问题类似，我们定义一个阈值来确定有足够的证据以至少 1-eta 的概率得出结论：解释性（特征）和特征之间存在线性关系。响应（标签）变量。仅二维的模拟证明了所提出的不可知检验能够提供与经典 $F$ 斜率参数检验给出的类似方差分析的能力。]]></description>
      <guid>https://arxiv.org/abs/2402.15213</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>Hamilton-Jacobi-Bellman 方程张量序列近似的生成建模</title>
      <link>https://arxiv.org/abs/2402.15285</link>
      <description><![CDATA[arXiv:2402.15285v1 公告类型：新
摘要：从概率密度中采样是不确定性量化（UQ）和生成建模（GM）等领域的常见挑战。特别是在 GM 中，根据 Ornstein-Uhlenbeck 正向过程的对数密度使用逆时扩散过程是一种流行的采样工具。在伯纳等人。 [2022] 作者指出，这些对数密度可以通过解随机最优控制已知的 \textit{Hamilton-Jacobi-Bellman} (HJB) 方程来获得。虽然这个 HJB 方程通常用间接方法处理，例如策略迭代和神经网络等黑盒架构的无监督训练，但我们建议通过直接时间积分来求解 HJB 方程，使用张量序列 (TT) 中表示的压缩多项式空间离散化的格式。至关重要的是，该方法无需样本，与归一化常数无关，并且可以避免 TT 压缩带来的维数灾难。我们提供了 HJB 方程对张量训练多项式的作用的完整推导，并演示了所提出的时间步长、秩和度自适应积分方法在 20 维非线性采样任务上的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.15285</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>使用对数 Sobolev 不等式的非线性贝叶斯最优实验设计</title>
      <link>https://arxiv.org/abs/2402.15053</link>
      <description><![CDATA[arXiv:2402.15053v1 公告类型：新
摘要：我们研究从更大的候选池中选择 $k$ 实验的问题，其目标是最大化所选子集和基础参数之间的互信息 (MI)。找到该组合优化问题的精确解的计算成本很高，这不仅是因为组合搜索的复杂性，而且还因为在非线性/非高斯设置中评估 MI 的困难。我们提出了基于新的计算成本低廉的 MI 下界的贪婪方法，该下界通过 log-Sobolev 不等式构建。我们证明，我们的方法在各种设置中都优于随机选择策略、高斯近似和 MI 的嵌套蒙特卡罗 (NMC) 估计器，包括具有非加性噪声的非线性模型的优化设计。]]></description>
      <guid>https://arxiv.org/abs/2402.15053</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>用于科学机器学习和不确定性量化的物理约束多项式混沌展开</title>
      <link>https://arxiv.org/abs/2402.15115</link>
      <description><![CDATA[arXiv:2402.15115v1 公告类型：新
摘要：我们提出了一种新颖的物理约束多项式混沌展开作为替代建模方法，能够执行科学机器学习（SciML）和不确定性量化（UQ）任务。所提出的方法具有独特的功能：它将 SciML 无缝集成到 UQ 中，反之亦然，这使得它能够有效地量化 SciML 任务中的不确定性，并利用 SciML 改进 UQ 相关任务中的不确定性评估。所提出的代理模型可以有效地结合各种物理约束，例如具有相关初始和边界条件约束的控制偏微分方程（PDE）、不等式约束（例如单调性、凸性、非负性等），以及在训练过程中附加先验信息以补充有限的数据。这确保了物理上真实的预测，并显着减少了训练替代模型时进行昂贵的计算模型评估的需要。此外，所提出的方法具有内置的不确定性量化（UQ）功能，可以有效地估计输出不确定性。为了证明所提出方法的有效性，我们将其应用于一系列不同的问题，包括具有确定性和随机参数的线性/非线性偏微分方程、复杂物理系统的数据驱动代理建模以及随机系统的 UQ参数建模为随机场。]]></description>
      <guid>https://arxiv.org/abs/2402.15115</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>平滑度自适应假设迁移学习</title>
      <link>https://arxiv.org/abs/2402.14966</link>
      <description><![CDATA[arXiv:2402.14966v1 公告类型：新
摘要：许多现有的基于两阶段核的假设转移学习算法跨阶段采用相同的核正则化，并依赖已知的函数平滑度来获得最优性。因此，它们在实践中无法适应目标/源及其偏移之间变化的和未知的平滑度。在本文中，我们通过提出平滑自适应迁移学习（SATL）来解决这些问题，这是一种基于两阶段核岭回归（KRR）的算法。我们首先证明，在仅目标 KRR 学习中使用错误指定的固定带宽高斯核可以实现极小极大最优性，并导出针对未知 Sobolev 平滑度的自适应过程。利用这些结果，SATL 在两个阶段都采用高斯核，以便估计器能够适应目标/源及其偏移函数的未知平滑度。我们推导出超额风险学习问题的极小极大下界，并表明 SATL 具有高达对数因子的匹配上限。极小极大收敛率揭示了影响迁移动态的因素，并证明了 SATL 相对于非迁移学习设置的优越性。虽然我们的主要目标是理论分析，但我们还进行了多次实验来证实我们的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.14966</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>关于平滑数据的经验风险最小化的性能</title>
      <link>https://arxiv.org/abs/2402.14987</link>
      <description><![CDATA[arXiv:2402.14987v1 公告类型：新
摘要：为了规避顺序决策中的统计和计算难度结果，最近的工作考虑了平滑在线学习，其中假设每次数据的分布在以基本度量为条件时具有有限似然比。历史。虽然以前的工作已经证明了平滑度的好处，但他们要么假设学习者已知基本度量，要么提出了仅在特殊情况下应用的计算效率低下的算法。这项工作研究了更一般的设置，其中基本度量对于学习者来说是\emph{未知}，特别关注当数据明确且平滑时具有平方损失的经验风险最小化（ERM）的性能。我们证明，在这种情况下，只要一个类可以使用 iid 数据进行学习，ERM 就能够实现次线性误差；特别是，ERM 实现的误差缩放为 $\tilde O( \sqrt{\mathrm{comp}(\mathcal F)\cdot T} )$，其中 $\mathrm{comp}(\mathcal F)$ 是统计复杂度用独立同分布数据学习$\mathcal F$。在此过程中，我们证明了平滑数据的新颖范数比较界限，其中包括适用于任意非线性函数类的相关数据的第一个锐范数比较。我们用下限来补充这些结果，表明我们对 ERM 的分析本质上是严格的，在平滑数据和独立同分布数据之间建立了 ERM 性能的分离。]]></description>
      <guid>https://arxiv.org/abs/2402.14987</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>线性变压器块的上下文学习：MLP 组件和一步 GD 初始化的优点</title>
      <link>https://arxiv.org/abs/2402.14951</link>
      <description><![CDATA[arXiv:2402.14951v1 公告类型：新
摘要：我们研究了结合了线性注意力组件和线性多层感知器（MLP）组件的\emph{线性变换器块}（LTB）的\emph{上下文学习}（ICL）能力。对于具有高斯先验和 \emph{非零均值}的线性回归的 ICL，我们表明 LTB 可以实现接近贝叶斯最优的 ICL 风险。相反，仅使用线性注意力必然会产生不可约的加性近似误差。此外，我们在 LTB 和具有可学习初始化的一步梯度下降估计器之间建立了对应关系 ($\mathsf{GD}\text{-}\mathbf{\beta}$)，从某种意义上说，每个 $\mathsf{GD} \text{-}\mathbf{\beta}$ 估计器可以由 LTB 估计器实现，并且每个最小化类内 ICL 风险的最佳 LTB 估计器实际上是 $\mathsf{GD}\text{-}\mathbf{ \beta}$ 估计器。最后，我们表明，尽管训练目标是非凸的，但 $\mathsf{GD}\text{-}\mathbf{\beta}$ 估计器可以通过梯度流有效优化。我们的结果表明，LTB 通过实现 $\mathsf{GD}\text{-}\mathbf{\beta}$ 实现了 ICL，并且突出了 MLP 层在减少近似误差方面的作用。]]></description>
      <guid>https://arxiv.org/abs/2402.14951</guid>
      <pubDate>Mon, 26 Feb 2024 06:17:37 GMT</pubDate>
    </item>
    </channel>
</rss>