<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 25 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>多尖峰张量 PCA 的高维随机梯度下降</title>
      <link>https://arxiv.org/abs/2410.18162</link>
      <description><![CDATA[arXiv:2410.18162v1 公告类型：新
摘要：我们研究多尖峰张量模型的在线随机梯度下降的高维动态。该多指标模型源自具有多个尖峰的张量主成分分析 (PCA) 问题，其目标是通过对 $p$ 张量的噪声观测进行最大似然估计，估计 $N$ 维单位球体内的 $r$ 个未知信号向量。我们确定从自然随机初始化中有效恢复未知尖峰所需的样本数量和信噪比 (SNR) 条件。我们表明，只要样本数量缩放为 $N^{p-2}$，与秩一情况下确定的算法阈值相匹配，就可以完全恢复所有尖峰 [Ben Arous、Gheissari、Jagannath 2020、2021]。我们的结果是通过对低维系统进行详细分析获得的，该系统描述了估计量和尖峰之间的相关性的演变，同时控制了动态中的噪声。我们发现尖峰在我们称为“顺序消除”的过程中按顺序恢复：一旦相关性超过临界阈值，所有共享行或列索引的相关性都会变得足够小，从而允许下一个相关性增长并变得宏观。相关性变得宏观的顺序取决于它们的初始值和相应的 SNR，从而导致尖峰的精确恢复或排列恢复。在矩阵情况下，当 $p=2$ 时，如果 SNR 足够分离，我们可以实现尖峰的精确恢复，而相等的 SNR 会导致尖峰跨越的子空间的恢复。]]></description>
      <guid>https://arxiv.org/abs/2410.18162</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用膨胀的 argmax 稳定黑盒模型选择</title>
      <link>https://arxiv.org/abs/2410.18268</link>
      <description><![CDATA[arXiv:2410.18268v1 公告类型：新
摘要：模型选择是根据给定数据从一类候选模型中进行选择的过程。例如，LASSO 和非线性动力学稀疏识别 (SINDy) 等方法将模型选择表述为寻找由训练数据确定的线性方程组的稀疏解。然而，如果没有强有力的假设，这种方法是非常不稳定的：如果从训练集中删除一个数据点，可能会选择不同的模型。本文提出了一种稳定模型选择的新方法，该方法利用 bagging 和“膨胀”argmax 操作的组合。我们的方法选择了一个完全适合数据的小型模型集合，并且它是稳定的，因为很有可能删除任何训练点都会导致选定的模型集合与原始集合重叠。除了开发理论保证之外，我们还在 (a) 模拟中说明了此方法，其中强相关的协变量使标准 LASSO 模型选择高度不稳定，以及 (b) Lotka-Volterra 模型选择问题，该问题侧重于确定生态系统中的竞争如何影响物种的丰富度。在这两种情况下，所提出的方法都能产生稳定而紧凑的选定模型集合，其表现优于各种基准。]]></description>
      <guid>https://arxiv.org/abs/2410.18268</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>知识蒸馏的高维分析：由弱到强的泛化与缩放定律</title>
      <link>https://arxiv.org/abs/2410.18837</link>
      <description><![CDATA[arXiv:2410.18837v1 公告类型：新
摘要：越来越多的机器学习场景依赖于知识蒸馏，其中使用替代模型的输出作为标签来监督目标模型的训练。在这项工作中，我们在两种设置下对无脊高维回归的这一过程进行了清晰的描述：（i）模型偏移，其中替代模型是任意的，以及（ii）分布偏移，其中替代模型是使用分布外数据的经验风险最小化的解决方案。在这两种情况下，我们都通过温和条件下样本大小和数据分布的非渐近界限来表征目标模型的精确风险。因此，我们确定了最佳替代模型的形式，它揭示了以数据依赖的方式丢弃弱特征的好处和局限性。在弱到强 (W2S) 泛化的背景下，这具有以下解释：(i) 使用代理作为弱模型的 W2S 训练可以证明在相同数据预算下优于使用强标签的训练，但 (ii) 它无法改善数据缩放规律。我们在无脊回归和神经网络架构的数值实验中验证了我们的结果。]]></description>
      <guid>https://arxiv.org/abs/2410.18837</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MissNODAG：从不完整数据中学习可微分循环因果图</title>
      <link>https://arxiv.org/abs/2410.18918</link>
      <description><![CDATA[arXiv:2410.18918v1 公告类型：新
摘要：现实世界系统（例如生物网络）中的因果发现通常因反馈回路和不完整数据而变得复杂。假设无环结构或完全观察到的数据的标准算法难以应对这些挑战。为了解决这一差距，我们提出了 MissNODAG，这是一个可区分的框架，用于从部分观察到的数据（包括非随机缺失的数据）中学习底层循环因果图和缺失机制。我们的框架将加性噪声模型与期望最大化程序相结合，交替输入缺失值并优化观察到的数据可能性，以揭示循环结构和缺失机制。我们通过合成实验和对现实世界基因扰动数据的应用证明了 MissNODAG 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.18918</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机矩阵理论视角下的学习特征谱和渐近泛化能力</title>
      <link>https://arxiv.org/abs/2410.18938</link>
      <description><![CDATA[arXiv:2410.18938v1 公告类型：新
摘要：神经网络的一个关键特性是它们在训练期间适应数据的能力。然而，我们目前对特征学习及其与泛化关系的数学理解仍然有限。在这项工作中，我们提供了一个随机矩阵分析，说明全连接的两层神经网络如何在单个但激进的梯度下降步骤后适应目标函数。在大批量限制下，我们严格建立了更新特征与各向同性尖峰随机特征模型之间的等价性。对于后一种模型，我们根据某些低维算子推导出特征经验协方差矩阵的确定性等价描述。这使我们能够清晰地描述训练对渐近特征谱的影响，特别是为特征谱尾部如何随训练而变化提供了理论基础。确定性等价进一步产生了精确的渐近泛化误差，揭示了在存在特征学习的情况下其改进背后的机制。我们的结果超越了标准随机矩阵集合，因此我们认为它具有独立的技术意义。与以前的工作不同，我们的结果在具有挑战性的最大学习率机制下成立，完全严格，并允许有限支持的第二层初始化，这对于研究学习特征的功能表达能力至关重要。这为特征学习在两层神经网络泛化中的影响提供了清晰的描述，超越了随机特征和惰性训练机制。]]></description>
      <guid>https://arxiv.org/abs/2410.18938</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MEC-IP：通过整数规划有效发现马尔可夫等价类</title>
      <link>https://arxiv.org/abs/2410.18147</link>
      <description><![CDATA[arXiv:2410.18147v1 公告类型：交叉 
摘要：本文提出了一种新颖的整数规划 (IP) 方法，用于通过观察数据发现贝叶斯网络 (BN) 的马尔可夫等价类 (MEC)。MEC-IP 算法利用独特的团聚焦策略和扩展最大生成图 (EMSG) 来简化对 MEC 的搜索，从而克服了其他现有算法固有的计算限制。我们的数值结果表明，我们的算法不仅显著减少了计算时间，而且在不同数据集中也看到了因果发现准确性的提高。这些发现强调了这种新算法作为因果发现和 BNSL 研究人员和从业人员的强大工具的潜力，为高效准确地分析复杂数据结构提供了重大飞跃。]]></description>
      <guid>https://arxiv.org/abs/2410.18147</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有类 SVD 收敛性和平坦最小值的深度自动编码器</title>
      <link>https://arxiv.org/abs/2410.18148</link>
      <description><![CDATA[arXiv:2410.18148v1 公告类型：交叉 
摘要：高维复杂物理系统的表示学习旨在识别低维固有潜在空间，这对于降阶建模和模态分析至关重要。为了克服众所周知的 Kolmogorov 障碍，近年来引入了深度自动编码器 (AE)，但随着潜在空间的秩增加，它们通常会出现较差的收敛行为。为了解决这个问题，我们提出了可学习加权混合自动编码器，这是一种通过可学习加权框架将奇异值分解 (SVD) 与深度自动编码器的优势相结合的混合方法。我们发现引入可学习的加权参数是必不可少的——如果没有它们，生成的模型要么会崩溃为标准 POD，要么无法表现出所需的收敛行为。此外，我们通过经验发现，与其他模型相比，我们训练的模型的锐度要小数千倍。我们对经典混沌 PDE 系统（包括 1D Kuramoto-Sivashinsky 和强制各向同性湍流数据集）进行的实验表明，与几种竞争方法相比，我们的方法显著提高了泛化性能，为高维复杂物理系统的稳健表示学习铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2410.18148</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>物理信息神经网络在函数微分方程中的应用：圆柱近似及其收敛保证</title>
      <link>https://arxiv.org/abs/2410.18153</link>
      <description><![CDATA[arXiv:2410.18153v1 公告类型：交叉 
摘要：我们提出了第一个函数微分方程 (FDE) 学习方案。FDE 在物理、数学和最优控制中起着基础性作用。然而，FDE 的数值分析由于其不切实际的计算成本而面临挑战，并且几十年来一直是一个长期存在的问题。因此，已经开发了 FDE 的数值近似，但它们往往过于简化了解决方案。为了解决这两个问题，我们提出了一种将物理信息神经网络 (PINN) 与 \textit{圆柱近似} 相结合的混合方法。圆柱近似用正交基展开函数和函数导数，并将 FDE 转换为高维 PDE。为了验证圆柱近似对于 FDE 应用的可靠性，我们证明了近似函数导数和解的收敛定理。然后，使用 PINN 对导出的高维 PDE 进行数值求解。通过 PINN 的功能，我们的方法可以比传统的基于离散化的方法更有效地处理更广泛的函数导数，从而提高圆柱近似的可扩展性。作为概念证明，我们对两个 FDE 进行了实验，并证明我们的模型可以成功实现 PINN 的典型 $L^1$ 相对误差阶 $\sim 10^{-3}$。总的来说，我们的工作为物理学家、数学家和机器学习专家分析以前具有挑战性的 FDE 提供了强大的支撑，从而使他们的数值分析变得民主化，而这一直受到的关注有限。代码可在 \url{https://github.com/TaikiMiyagawa/FunctionalPINN} 获得。]]></description>
      <guid>https://arxiv.org/abs/2410.18153</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TabDPT：扩展表格基础模型</title>
      <link>https://arxiv.org/abs/2410.18164</link>
      <description><![CDATA[arXiv:2410.18164v1 公告类型：交叉 
摘要：神经网络在表格数据上面临的挑战是有据可查的，并且阻碍了表格基础模型的进展。利用上下文学习 (ICL) 的技术在这里显示出希望，可以动态适应看不见的数据。ICL 可以为全新的数据集提供预测，而无需进一步训练或超参数调整，因此在遇到新任务时提供非常快速的推理。然而，扩展表格数据的 ICL 仍然是一个问题：基于大型语言模型的方法无法有效地处理数字表，而表格特定的技术无法有效地利用真实数据的力量来提高性能和泛化能力。我们能够通过在真实数据上使用自监督学习和检索训练表格特定的基于 ICL 的架构来克服这些挑战，结合两全其美。我们最终的模型——表格判别式预训练 Transformer (TabDPT)——在无需针对特定任务进行微调的情况下，在 CC18（分类）和 CTR23（回归）基准上实现了最佳性能，展示了模型预训练后 ICL 的适应性和速度。随着模型大小和可用数据量的增加，TabDPT 还展示了强大的扩展能力，这表明未来只需通过整理更大的表格预训练数据集和训练更大的模型即可实现改进。]]></description>
      <guid>https://arxiv.org/abs/2410.18164</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用欧几里得距离校准深度神经网络</title>
      <link>https://arxiv.org/abs/2410.18321</link>
      <description><![CDATA[arXiv:2410.18321v1 公告类型：交叉 
摘要：不确定性是现实世界场景的一个基本方面，其中很少有完美的信息。人类自然会开发复杂的内部模型来处理不完整的数据并有效应对不可预见或部分观察到的事件。在机器学习中，焦点损失通常用于通过强调难以分类的样本来降低错误分类率。然而，它不能保证校准良好的预测概率，并且可能导致模型过度自信或信心不足。高校准误差表示预测概率与实际结果不一致，从而影响模型可靠性。这项研究引入了一种称为焦点校准损失（FCL）的新型损失函数，旨在改进概率校准，同时保留焦点损失在处理困难样本方面的优势。通过严格适当的损失最小化欧几里得范数，FCL 惩罚实例校准误差并限制界限。我们对所提出的方法进行了理论验证，并将其应用于校准 CheXNet，以便在基于网络的医疗保健系统中进行部署。对各种模型和数据集的广泛评估表明，我们的方法在校准和准确度指标方面都达到了 SOTA 性能。]]></description>
      <guid>https://arxiv.org/abs/2410.18321</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新审视可微分结构学习：$\ell_1$ 惩罚的不一致性及其他</title>
      <link>https://arxiv.org/abs/2410.18396</link>
      <description><![CDATA[arXiv:2410.18396v1 公告类型：交叉 
摘要：可微结构学习的最新进展将学习有向无环图的组合问题定义为连续优化问题。已经研究了包括数据标准化在内的各个方面，以确定影响这些方法经验性能的因素。在这项工作中，我们研究了可微结构学习方法中的关键限制，重点关注可以识别马尔可夫等价类的真实结构的设置，特别是在线性高斯情况下。虽然 Ng 等人 (2024) 强调了这种设置中潜在的非凸性问题，但我们证明并解释了为什么在这种情况下使用 $\ell_1$ 惩罚似然从根本上是不一致的，即使可以找到优化问题的全局最优值。为了解决这一限制，我们开发了一种基于 $\ell_0$ 惩罚似然和硬非循环约束的混合可微分结构学习方法，其中 $\ell_0$ 惩罚可以通过包括 Gumbel-Softmax 在内的不同技术来近似。具体来说，我们首先估计底层道德图，并使用它来限制优化问题的搜索空间，这有助于缓解非凸性问题。实验结果表明，所提出的方法在数据标准化之前和之后都提高了经验性能，为可微分结构学习的未来发展提供了更可靠的途径，尤其是对于学习马尔可夫等价类。]]></description>
      <guid>https://arxiv.org/abs/2410.18396</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过贝叶斯坐标差分隐私增强特定特征的数据保护</title>
      <link>https://arxiv.org/abs/2410.18404</link>
      <description><![CDATA[arXiv:2410.18404v1 公告类型：交叉 
摘要：局部差分隐私 (LDP) 提供强大的隐私保障，而无需用户信任外部方。然而，LDP 对所有数据特征（包括不太敏感的特征）应用统一保护，这会降低下游任务的性能。为了克服这一限制，我们提出了一个贝叶斯框架，即贝叶斯坐标差分隐私 (BCDP)，它可以实现特定于特征的隐私量化。这种更细致入微的方法通过根据每个特征的敏感度调整隐私保护来补充 LDP，从而在不损害隐私的情况下提高下游任务的性能。我们描述了 BCDP 的属性并阐明了它与标准非贝叶斯隐私框架的联系。我们进一步将我们的 BCDP 框架应用于隐私均值估计和普通最小二乘回归问题。与纯基于 LDP 的方法相比，基于 BCDP 的方法获得了更高的准确性，而不会损害隐私。]]></description>
      <guid>https://arxiv.org/abs/2410.18404</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考 Softmax：具有多项式激活的自注意力机制</title>
      <link>https://arxiv.org/abs/2410.18613</link>
      <description><![CDATA[arXiv:2410.18613v1 公告类型：交叉 
摘要：本文挑战了传统观点，即 Transformer 中的 softmax 注意力机制之所以有效，主要是因为它为注意力分配生成了概率分布。相反，我们从理论上表明，它的成功在于它能够在训练过程中隐式地规范注意力矩阵的 Frobenius 范数。然后，我们探索规范注意力矩阵 Frobenius 范数的替代激活，证明某些多项式激活可以实现这种效果，使其适用于基于注意力的架构。实证结果表明，这些激活在各种计算机视觉和语言任务中的表现与 softmax 相当或更好，这表明除了 softmax 之外，注意力机制还有新的可能性。]]></description>
      <guid>https://arxiv.org/abs/2410.18613</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>去噪扩散概率模型对未知低维具有最佳适应性</title>
      <link>https://arxiv.org/abs/2410.18784</link>
      <description><![CDATA[arXiv:2410.18784v1 公告类型：交叉 
摘要：去噪扩散概率模型 (DDPM) 已成为生成 AI 中的主流生成模型。虽然已经为 DDPM 建立了严格的收敛保证，但迭代复杂度通常与环境数据维度成正比，导致过于保守的理论无法解释其实际效率。这促使最近的研究 Li 和 Yan (2024a) 研究 DDPM 如何通过自动利用数据的固有低维来实现采样加速。我们通过展示某种意义上对未知低维的最佳适应性来加强这项先前的工作。对于具有固有维度 $k$ 的一类广泛数据分布，我们证明 DDPM 的迭代复杂度几乎与 $k$ 成线性比例，当使用 KL 散度来衡量分布差异时，这是最佳的。我们的理论建立在一个关键观察之上：DDPM 更新规则相当于在离散化时运行适当参数化的 SDE，其中漂移项的非线性分量本质上是低维的。]]></description>
      <guid>https://arxiv.org/abs/2410.18784</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习在未知线性约束下利用拉格朗日函数探索老虎机</title>
      <link>https://arxiv.org/abs/2410.18844</link>
      <description><![CDATA[arXiv:2410.18844v1 公告类型：交叉 
摘要：老虎机中的纯探索模拟了多个现实世界问题，例如调整超参数或进行用户研究，其中决策空间上自然会出现不同的安全、资源和公平性约束。我们将这些问题研究为具有未知线性约束的多臂老虎机中的纯探索，其目的是确定一个 $r$$\textit{-good 可行策略}$。首先，我们提出了约束下纯探索的样本复杂度下限的拉格朗日松弛。我们展示了这个下限如何随着约束的顺序估计而演变。其次，我们利用拉格朗日下限和凸优化的性质，提出了两个计算效率高的 Track-and-Stop 和 Gamified Explorer 扩展，即 LATS 和 LAGEX。为此，我们提出了一种约束自适应停止规则，并在跟踪下限时，在每个步骤中使用可行集的悲观估计。我们表明这些算法实现了渐近最优的样本复杂度上限，最高可达约束相关常数。最后，我们进行了具有不同奖励分布和约束的数值实验，验证了 LAGEX 和 LATS 相对于基线的高效性能。]]></description>
      <guid>https://arxiv.org/abs/2410.18844</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>