<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过增强的足够表示：通过目标数据丰富源域知识的转移学习</title>
      <link>https://arxiv.org/abs/2502.20414</link>
      <description><![CDATA[ARXIV：2502.20414V1公告类型：新 
摘要：转移学习是解决各种应用程序中数据可用性有限的挑战的重要方法。它通过将知识从建立的源域转移到一个不熟悉的目标域来实现这一目标。但是，由于刚性模型的假设以及源模型和目标域模型之间的高度相似性，传统的转移学习方法通​​常会面临困难。在本文中，我们介绍了一种新颖的方法，用于通过增强的足够表示（TESR）传递称为转移学习的新方法。我们的方法首先估计来自源域的足够且不变的表示。然后，通过从目标数据得出的独立组件来增强该表示形式，以确保其足以适合目标域并适应其特定特征。 TESR的一个值得注意的优点是，它不依赖于在不同任务上假设相似的模型结构。例如，源域模型可以是回归模型，而目标域任务可以分类。这种灵活性使TESR适用于广泛的监督学习问题。我们探索了TESR的理论特性，并通过模拟研究和现实数据应用程序验证其性能，并在有限的样本设置中证明了其有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.20414</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过熵风险措施有效的风险敏感计划</title>
      <link>https://arxiv.org/abs/2502.20423</link>
      <description><![CDATA[ARXIV：2502.20423V1公告类型：新 
摘要：风险敏感的计划旨在确定马尔可夫决策过程（MDP）中最大化一些以尾巴指标为中心的政策。对于最广泛使用和可解释的指标，例如阈值概率或（条件）风险的值，这种优化任务可能非常昂贵。确实，以前的工作表明，只能通过动态编程有效地优化熵风险度量（ENTRM），而又可以选择难以释放的参数。     我们表明，在参数值跨参数值的全套最佳策略的计算导致关注指标的紧密近似值。我们证明，由于新型的结构分析和熵风险的平滑性能，可以有效地计算此最佳界面。     经验结果表明，我们的方法在各种决策方案中取得了良好的表现。]]></description>
      <guid>https://arxiv.org/abs/2502.20423</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深层线性网络的学习动力超出稳定性的边缘</title>
      <link>https://arxiv.org/abs/2502.20531</link>
      <description><![CDATA[ARXIV：2502.20531V1公告类型：新 
摘要：使用梯度下降的深度神经网络和固定学习率$ \ eta $经常在“稳定边缘”（EOS）方面运行，其中Hessian最大的特征值在稳定性阈值$ 2/\ eta $方面平衡。在这项工作中，我们介绍了深度矩阵分解损失在EOS之外的（深）线性网络（DLN）的学习动力学的细粒分析。对于DLN，EOS以外的损失振荡遵循到混乱的时期途径。我们从理论上分析了2个周期轨道的制度，并表明损失振荡发生在一个小的子空间内，其子空间的尺寸精确地为以学习率为特征。我们的分析的关键在于表明，对称性诱导的梯度流的保护定律定义为跨层的奇异值之间的平衡间隙，在EOS处断裂并单调为零。总体而言，我们的结果有助于解释深层网络中的两个关键现象：（i）浅层模型和简单任务并不总是表现出EOS； （ii）振荡发生在顶部特征内。我们提出了支持我们理论的实验，以及证明这些现象如何在非线性网络中发生的示例，以及它们与诸如DLN等良性景观的现象如何不同。]]></description>
      <guid>https://arxiv.org/abs/2502.20531</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>摊销有条件独立测试</title>
      <link>https://arxiv.org/abs/2502.20925</link>
      <description><![CDATA[ARXIV：2502.20925V1公告类型：新 
摘要：对数据中有条件独立性结构的测试是统计和机器学习中的基本和关键任务，它在因果发现中发现了自然应用 - 许多科学学科的高度相关问题。现有的方法试图设计量化条件依赖程度的明确测试统计数据，这是高度挑战性的，但无法以数据驱动的方式捕获或利用先验知识。在这项研究中，引入了一种全新的方法，而是提议摊销条件独立性测试并设计酸 - 一种新型的基于变压器的神经网络结构，该结构学会了测试有条件的独立性。可以以监督学习方式对酸数据进行培训，然后可以将学习的模型应用于任何类似性质的数据集，也可以通过以微不足道的计算成本进行微调来适应新领域。我们对合成和真实数据的广泛经验评估表明，酸在多个指标下始终如一地针对现有基准的最先进的性能，并且能够稳定地概括地以明显低的推理时间来概括地看不见的样本量，维度和非线性。]]></description>
      <guid>https://arxiv.org/abs/2502.20925</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过激活水平的高斯过程，事后不确定性定量在前训练的神经网络中</title>
      <link>https://arxiv.org/abs/2502.20966</link>
      <description><![CDATA[ARXIV：2502.20966V1公告类型：新 
摘要：通过辍学，贝叶斯神经网络和拉普拉斯近似等方法，神经网络中的不确定性量化易于拟合或计算要求，使这些方法在大型数据集中变得不切实际。在这项工作中，我们通过将焦点从重量空间的不确定性转移到激活水平的不确定性，通过高斯工艺来解决这些缺陷。更具体地说，我们引入了高斯过程激活函数（GAPA），以捕获神经元级的不确定性。我们的方法以事后方式运行，保留了预训练的神经网络的原始平均预测，从而避免了以前方法中通常遇到的不足的问题。我们提出了两种方法。首先，无GAPA，从训练数据中使用经验内核学习，并且在训练过程中效率很高。第二个是GAPA变化的，通过在内核上的梯度下降来学习超参数，从而提供了更大的灵活性。经验结果表明，在至少一个不确定性定量指标中，GAPA变量的大多数数据集上的拉普拉斯近似值都优于laplace近似。]]></description>
      <guid>https://arxiv.org/abs/2502.20966</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>位置：首先求解图层线性模型，以了解神经动力学现象（神经崩溃，出现，懒惰/丰富的制度和grokking）</title>
      <link>https://arxiv.org/abs/2502.21009</link>
      <description><![CDATA[ARXIV：2502.21009V1公告类型：新 
摘要：在物理学中，复杂的系统通常被简化为仅保留核心原理的最小，可解决的模型。在机器学习中，层线性模型（例如线性神经网络）充当神经网络动力学的简化表示。这些模型遵循动态反馈原理，该原理描述了层如何相互控制和扩大彼此的演变。该原理超出了简化的模型，成功地解释了深层神经网络中广泛的动态现象，包括神经崩溃，出现，懒惰和丰富的制度以及Grokking。在该立场论文中，我们呼吁使用层线性模型保留神经动力学现象的核心原理来加速深度学习科学。]]></description>
      <guid>https://arxiv.org/abs/2502.21009</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当标签转移发生时，班级的正面估计值未标记的学习</title>
      <link>https://arxiv.org/abs/2502.21194</link>
      <description><![CDATA[ARXIV：2502.21194V1公告类型：新 
摘要：我们研究了未标记的目标样本的班级估计，这可能与源总数不同。假定对于源数据，仅来自正类别和整个人群的样本可用（PU学习方案）。我们介绍了一个新型的班级直接估计器，该估计避免了后验概率的估计，并具有简单的几何解释。它基于分配匹配技术以及内核嵌入，作为对优化任务的明确解决方案获得的。我们建立了其渐近一致性以及与未知先验的偏差的非反应关系，在实践中是可以计算的。我们研究合成和真实数据的有限样本行为，并表明该提案以及适当修改的版本可用于大量源先验值，比其竞争对手更高。]]></description>
      <guid>https://arxiv.org/abs/2502.21194</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型两层网络中的概括和过度拟合的动力脱钩</title>
      <link>https://arxiv.org/abs/2502.21269</link>
      <description><![CDATA[ARXIV：2502.21269V1公告类型：新 
摘要：大型机器学习模型的电感偏差和概括性能在很大程度上是用于训练的优化算法的副产品。除其他外，随机初始化，学习率和早期停止的规模均对通过随机梯度下降或相关算法学习的模型质量产生了至关重要的影响。为了了解这些现象，我们研究了大型两层神经网络的训练动力学。我们使用从非平衡统计物理学（动力学平均场理论）中建立的良好技术来获得该动力学的渐近高维度表征。这种表征适用于隐藏神经元非线性的高斯近似，并在经验上很好地捕获了实际神经网络模型的行为。
  我们的分析发现了训练动力学中的几种有趣的新现象：$（i）$与高斯/rademacher复杂性增长相关的缓慢时间尺度的出现；结果，$（ii）$，算法归纳偏见对小复杂性，但前提是初始化的复杂性足够小； $（iii）$ a在功能学习和过度拟合之间的时间尺度分开； $（iv）$ a的非单向酮行为的测试错误行为，并在很大程度上相应地是一个“特征划分”阶段。]]></description>
      <guid>https://arxiv.org/abs/2502.21269</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Minimax最佳内核两样本测试具有随机特征</title>
      <link>https://arxiv.org/abs/2502.20755</link>
      <description><![CDATA[ARXIV：2502.20755V1公告类型：交叉 
摘要：通过MMD（最大平均差异），用于非参数假设测试问题的概率分布的内核希尔伯特空间（RKHS）被证明是一种有效的方法，这些方法涉及一般（非欧盟裔）域定义的分布。尽管在此主题上已经完成了大量的工作，但直到最近，Minimax最佳的两样本测试才构建了与MMD不同的均值元素和协方差运算符的正则版本。但是，与大多数核算法一样，最佳测试的计算复杂性在样本量中以立方体缩放，从而限制了其适用性。在本文中，我们提出了基于随机傅立叶特征（RFF）近似的光谱正则化两样本测试，并研究统计最佳和计算效率之间的权衡。如果RFF的近似顺序（这取决于似然比的平滑度和积分运算符的特征值的衰减速率），我们表明提出的测试是最佳的最佳测试。我们开发了一个可实现的基于置换的基于置换式测试的版本，并采用数据自适应策略来选择正则化参数和内核。最后，通过对模拟和基准数据集的数值实验，我们证明了所提出的基于RFF的测试在计算上是有效的，并且对确切测试的测试几乎相似（功率下降）。]]></description>
      <guid>https://arxiv.org/abs/2502.20755</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用自适应相关诱导的有限差估计器增强的无衍生化优化</title>
      <link>https://arxiv.org/abs/2502.20819</link>
      <description><![CDATA[ARXIV：2502.20819V1公告类型：交叉 
摘要：基于梯度的方法非常适合无导数优化（DFO），其中有限差异（FD）估计值通常用作梯度替代物。传统的随机近似方法，例如Kiefer-Wolfowitz（KW）和同时扰动随机近似（SPSA），通常在每个迭代中仅使用两个样本，从而导致不精确的梯度估计值，并且需要减小的步骤尺寸。在本文中，我们首先探讨了有效的FD估计值，该估计值称为相关诱导的FD估计值，这是基于批处理的估计值。然后，我们提出了一种自适应采样策略，该策略可以动态确定每次迭代处的批处理大小。通过组合这两个组件，我们开发了一种算法，旨在根据梯度估计效率和样品效率增强DFO。此外，我们建立了我们提出的算法的一致性，并证明，尽管每次迭代使用了一批样品，但它的收敛速率与KW和SPSA方法相同。此外，我们提出了一种新型的随机线搜索技术，以适应实践中的步骤大小。最后，全面的数值实验证实了所提出算法的出色经验性能。]]></description>
      <guid>https://arxiv.org/abs/2502.20819</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测每月使用即时学习建模的每月住宅天然气需求</title>
      <link>https://arxiv.org/abs/2502.20989</link>
      <description><![CDATA[ARXIV：2502.20989V1公告类型：交叉 
摘要：天然气（NG）是相对干净的能源，尤其是与化石燃料相比，在过去的二十年中，NG的全球消费几乎在线性增长。在土耳其也可以看到类似的趋势，而另一个相似之处是对连续NG供应的进口量的高度依赖。至关重要的是要准确预测土耳其未来的NG需求（NGD），尤其是进口合同；在这方面，次年对每月NGD的预测至关重要。 In the current study, the historical monthly NG consumption data between 2014 and 2024 provided by SOCAR, the local residential NG distribution company for two cities in Turkey, Bursa and Kayseri, was used to determine out-of-sample monthly NGD forecasts for a period of one year and nine months using various time series models, including SARIMA and ETS models, and a novel proposed machine learning method.所提出的方法，称为“恰当学习”高斯过程回归（JITL-GPR），使用了新的特征表示，用于过去的NG需求值。它们没有将过去的需求值用作列的单独特征，而是将其放置在二维（2-D）年度值的网格上。对于每个测试点，用于NGD预测的内核函数在GPR中用于预测查询点。由于为每个测试点分别构建模型，因此所提出的方法确实是JITL的一个示例。 JITL-GPR方法易于使用和优化，与传统时间序列方法和最先进的组合模型相比，预测错误减少了；因此，它是在类似设置中NGD预测的有前途的工具。]]></description>
      <guid>https://arxiv.org/abs/2502.20989</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深层神经网络的数据增强策略，并应用于流行病建模</title>
      <link>https://arxiv.org/abs/2502.21033</link>
      <description><![CDATA[arxiv：2502.21033v1公告类型：交叉 
摘要：在这项工作中，我们将隔室疾病动力学模型的预测能力与机器学习能力相结合，以分析复杂，高维数据和发现常规模型可能忽略的模式。具体而言，我们提供了概念证明，证明了数据驱动的方法和深度神经网络在最近引入的具有社会特征（包括饱和发病率）的SIR型模型中的应用，以改善流行病的预测和预测。我们的结果表明，强大的数据增强策略槽合适的数据驱动模型可以提高前馈神经网络（FNNS）和非线性自动回应网络（NARS）的可靠性，从而使它们成为物理知识神经网络（PINN）的可行替代品。这种方法增强了处理非线性动力学的能力，并为流行病预测提供了可扩展的，数据驱动的解决方案，并优先考虑基于物理模型的约束的预测准确性。意大利COVID-19的锁骨后阶段的数值模拟和西班牙验证了我们的方法论。]]></description>
      <guid>https://arxiv.org/abs/2502.21033</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>罕见的事件建模具有自我调节的归一化流量：我们可以从单个失败中学到什么？</title>
      <link>https://arxiv.org/abs/2502.21110</link>
      <description><![CDATA[ARXIV：2502.21110V1公告类型：交叉 
摘要：在运输和机器人技术等领域中自主系统的部署增加了，安全至关重要的故障的增加。由于数据相对缺乏，这些故障可能很难建模和调试：与正常操作中成千上万的示例相比，我们可能只有几秒钟的数据导致故障。这种稀缺性使训练罕见故障事件的生成模型的生成模型具有挑战性，因为现有方法可能会因有限的故障数据集中的噪声过度拟合，或者由于过度强大的先验而导致的噪声不足。我们通过CALNF解决了这一挑战，或校准正常化的流量，这是从有限数据中进行后验学习的自我调节框架。 CALNF在数据限制的故障建模和反问题上实现了最先进的绩效，并可以将第一个案例研究纳入2022年西南航空安排危机的根本原因。]]></description>
      <guid>https://arxiv.org/abs/2502.21110</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一代需要记忆吗？使用环境扩散的创意扩散模型</title>
      <link>https://arxiv.org/abs/2502.21278</link>
      <description><![CDATA[ARXIV：2502.21278V1公告类型：交叉 
摘要：有很大的经验证据表明，最新的扩散建模范式会导致记忆训练集的模型，尤其是在训练集很小的情况下。减轻记忆问题的先前方法通常会导致图像质量下降。是否有可能获得强大而有创造力的生成模型，即实现高生代质量和低记忆的模型？尽管当前的结果是结果的景观，但我们在推动忠诚度和记忆之间的权衡方面取得了重大进展。我们首先提供了理论上的证据，表明在扩散模型中的记忆仅对于在低噪声尺度下的问题（通常用于生成高频细节）所必需。使用这种理论洞察力，我们提出了一种简单的原则方法，使用大噪声尺度上的嘈杂数据训练扩散模型。我们表明，对于文本条件和无条件模型以及各种数据可用性设置，我们的方法大大降低了记忆，而不会降低图像质量。]]></description>
      <guid>https://arxiv.org/abs/2502.21278</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过最小和可解释的更新来控制模型偏差</title>
      <link>https://arxiv.org/abs/2502.21284</link>
      <description><![CDATA[ARXIV：2502.21284V1公告类型：交叉 
摘要：学习公平的机器学习模型的传统方法通常需要从头开始重建模型，通常不考虑可能现有的先前模型。在需要经常进行模型的情况下，这可能会导致模型更新不一致，以及冗余且昂贵的验证测试。为了解决这一限制，我们介绍了受控模型偏见的概念，这是一个依靠两个desiderata的新颖监督学习任务：新公平模型与现有模型之间的差异应该是（i）可解释的，并且（ii）最小。在为这个新问题提供了理论保证后，我们引入了一种新颖的算法，以实现算法公平性，商品，既是模型 - 不可能的，又不需要在测试时敏感的属性。此外，我们的算法是明确设计的，目的是在有偏见和依据的预测之间实施最小和可解释的变化 - 这种属性在高风险应用中非常可取，但很少被优先考虑公平文献中的明确目标。我们的方法结合了一种基于概念的建筑和对抗性学习，我们通过经验结果证明，它可以在执行最小和可解释的预测变化的同时，实现与最先进的偏见方法相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.21284</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>