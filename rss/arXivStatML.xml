<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 08 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用半不平衡神经最优传输进行稳健重心估计</title>
      <link>https://arxiv.org/abs/2410.03974</link>
      <description><![CDATA[arXiv:2410.03974v1 公告类型：新
摘要：从多个来源聚合数据的一个常见挑战可以形式化为 \textit{最佳传输} (OT) 重心问题，该问题旨在计算关于 OT 差异的概率分布的平均值。然而，数据测量中的异常值和噪声的存在会严重阻碍传统统计方法估计 OT 重心的性能。为了解决这个问题，我们提出了一种新颖的可扩展方法来估计 \textit{稳健} 连续重心，利用 \textit{(半)不平衡} OT 问题的对偶公式。据我们所知，本文是首次尝试在连续分布设置下开发稳健重心算法。我们的方法被设计为 $\min$-$\max$ 优化问题，并且适用于 \textit{一般} 成本函数。我们严格建立所提出方法的理论基础，并通过大量说明性实验证明其对异常值和类别不平衡的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2410.03974</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>单变量回归中浅层神经网络镜像下降的隐性偏差</title>
      <link>https://arxiv.org/abs/2410.03988</link>
      <description><![CDATA[arXiv:2410.03988v1 公告类型：新
摘要：我们研究了具有宽和浅神经网络的单变量最小二乘误差回归中镜像流的隐性偏差。对于一类广泛的势函数，我们表明当网络宽度趋于无穷大时，镜像流表现出惰性训练并且具有与普通梯度流相同的隐性偏差。对于 ReLU 网络，我们通过函数空间中的变分问题来表征这种偏差。我们的分析包括普通梯度流的先前结果作为特殊情况，并解除了需要对训练数据或具有跳过连接的网络进行难以处理的调整的限制。我们进一步引入了缩放潜力并表明对于这些，镜像流仍然表现出惰性训练但不在内核状态。对于具有绝对值激活的网络，我们表明具有缩放潜力的镜像流会引起丰富的偏差，这通常无法通过 RKHS 规范捕获。要点是，虽然参数初始化决定了学习函数的曲率在输入空间的不同位置受到的惩罚程度，但缩放潜力决定了不同大小的曲率受到的惩罚程度。]]></description>
      <guid>https://arxiv.org/abs/2410.03988</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分数匹配适合估计点过程吗？</title>
      <link>https://arxiv.org/abs/2410.04037</link>
      <description><![CDATA[arXiv:2410.04037v1 公告类型：新
摘要：近年来，得分匹配估计量得到了广泛的关注，部分原因是它们不需要计算归一化常数的积分，从而解决了最大似然估计（MLE）中的计算挑战。一些现有的工作已经提出了点过程的得分匹配估计量。然而，这项工作表明，这些工作中提出的估计量的不完整性使得它们仅适用于特定问题，并且它们不适用于更一般的点过程。为了解决这个问题，这项工作将加权得分匹配估计量引入点过程。从理论上讲，我们证明了估计量的一致性并确定了它的收敛速度。实验结果表明，我们的估计量准确地估计了合成数据的模型参数，并在真实数据上产生了与 MLE 一致的结果。相反，现有的得分匹配估计量无法有效地执行。代码可在 \url{https://github.com/KenCao2007/WSM_TPP} 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2410.04037</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>pFedGame——利用动态拓扑中的博弈论进行分散联邦学习</title>
      <link>https://arxiv.org/abs/2410.04058</link>
      <description><![CDATA[arXiv:2410.04058v1 公告类型：新
摘要：传统的联邦学习框架面临着许多挑战，包括中央聚合服务器的性能瓶颈、数据偏差、模型收敛性差、容易受到模型中毒攻击以及对集中式基础设施的信任有限。本文提出了一种基于博弈论的新型方法，称为 pFedGame，用于分散式联邦学习，最适合时间动态网络。所提出的算法不需要任何集中式服务器进行聚合，并且结合了联邦学习参与者之间在时间动态拓扑上梯度消失和收敛性差的问题。对于每个参与者，该解决方案在每个联邦学习轮次中包括两个连续步骤。首先，它选择合适的同伴进行联邦学习协作。其次，它通过应用最佳联邦学习聚合策略执行双人常数和合作博弈以达到收敛。与现有的分散式联邦学习方法相比，为评估 pFedGame 的性能而进行的实验表明，对于异构数据，其准确率高于 70%，效果非常好。]]></description>
      <guid>https://arxiv.org/abs/2410.04058</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对前向特征图进行对角化，实现深度神经网络中特征学习的可视化</title>
      <link>https://arxiv.org/abs/2410.04264</link>
      <description><![CDATA[arXiv:2410.04264v1 公告类型：新
摘要：深度神经网络 (DNN) 表现出非凡的自动学习数据表示的能力，无需人工输入即可找到合适的特征。在这里，我们提出了一种分析特征学习的方法，即将 DNN 分解为 1) 将输入数据空间映射到倒数第二层的后激活的前向特征图 $\Phi$，以及 2) 对数据进行分类的最终线性层。我们根据梯度下降算子对 $\Phi$ 进行对角化，并通过测量 $\Phi$ 的特征函数和特征值在训练过程中的变化来跟踪特征学习。在许多流行的架构和分类数据集中，我们发现 DNN 经过几个时期后就会收敛到一个最小特征 (MF) 状态，该状态由与类数相等的特征函数数量主导。这种行为类似于在较长训练时间中研究的神经崩溃现象。对于其他 DNN 数据组合，例如 CIFAR10 上的全连接网络，我们发现了一种扩展特征 (EF) 机制，其中使用了更多特征。超参数调整后的最佳泛化性能通常与 MF 机制相吻合，但我们也发现 MF 机制中性能不佳的例子。最后，我们将神经崩溃现象重新定义为一个内核图，可以扩展到更广泛的任务，例如回归。]]></description>
      <guid>https://arxiv.org/abs/2410.04264</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索线性可分离性的边缘</title>
      <link>https://arxiv.org/abs/2410.04489</link>
      <description><![CDATA[arXiv:2410.04489v1 公告类型：新
摘要：我们在简化的设置中研究二元逻辑分类的泛化特性，其中始终可以严格定义“记忆”和“泛化”解决方案，并通过经验和分析阐明 Grokking 动态背后的机制。我们分析了具有恒定标签的随机特征模型上逻辑分类的渐近长期动态，并表明它表现出 Grokking，即延迟泛化和非单调测试损失。我们发现，当将分类应用于处于线性可分边缘的训练集时，Grokking 会被放大。即使始终存在完美的泛化解决方案，我们也证明，如果训练数据与原点线性可分，逻辑损失的隐性偏差将导致模型过度拟合。对于与原点不可分离的训练集，模型将始终完美地渐近地推广，但在训练的早期阶段可能会出现过度拟合。重要的是，在过渡附近，即对于几乎可以与原点分离的训练集，模型可能会在推广之前过度拟合任意长时间。我们通过检查一个可处理的一维玩具模型获得了更多见解，该模型定量地捕捉了完整模型的关键特征。最后，我们强调了我们的研究结果与最近文献的有趣共同特性，表明 grokking 通常发生在插值阈值附近，让人想起物理系统中经常观察到的临界现象。]]></description>
      <guid>https://arxiv.org/abs/2410.04489</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机龙格-库塔方法：可证明的扩散模型加速</title>
      <link>https://arxiv.org/abs/2410.04760</link>
      <description><![CDATA[arXiv:2410.04760v1 公告类型：新
摘要：扩散模型在当代生成建模中起着关键作用，在各个领域都具有最先进的性能。尽管样本质量优异，但主流的基于扩散的随机采样器（如 DDPM）通常需要大量的得分函数评估，与生成对抗网络等单步生成器相比，计算成本要高得多。虽然在实践中已经提出了几种加速方法，但加速扩散模型的理论基础仍未得到充分探索。在本文中，我们基于随机 Runge-Kutta 方法提出并分析了一种无需训练的 SDE 式扩散采样器加速算法。所提出的采样器使用 $\widetilde O(d^{3/2} / \varepsilon)$ 得分函数评估（对于足够小的 $\varepsilon$）可证明达到 $\varepsilon^2$ 误差（以 KL 散度衡量），从而加强了维度依赖性方面最先进的保证 $\widetilde O(d^{3} / \varepsilon)$。数值实验验证了所提方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.04760</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不可知的平滑在线学习</title>
      <link>https://arxiv.org/abs/2410.05124</link>
      <description><![CDATA[arXiv:2410.05124v1 公告类型：新
摘要：统计学习中的经典结果通常考虑两种极端的数据生成模型：来自未知分布的 i.i.d. 实例或完全对抗性实例，这在统计上通常更具挑战性。为了弥合这些模型之间的差距，最近的研究引入了平滑框架，其中在每次迭代中，对手都会从一个分布中生成实例，该分布的密度受 $\sigma^{-1}$ 的限制，而某个固定的基度量 $\mu$ 则受此限制。该框架根据 $\sigma$ 的值在 i.i.d. 和对抗性案例之间进行插值。对于经典的在线预测问题，平滑在线学习中的大多数先前结果都依赖于一个可以说是强有力的假设，即学习者知道基度量 $\mu$，这与 PAC 学习或一致性文献中的标准设置形成对比。我们考虑一般的不可知论问题，其中基度量未知且值是任意的。沿着这个方向，Block 等人证明了经验风险最小化在明确指定的假设下具有次线性遗憾。我们提出了一种基于递归覆盖的算法 R-Cover，这是第一个在没有 $\mu$ 先验知识的情况下保证不可知平滑在线学习的次线性遗憾的算法。对于分类，我们证明 R-Cover 对 VC 维数为 $d$ 的函数类具有自适应遗憾 $\tilde O(\sqrt{dT/\sigma})$，这在对数因子上是最优的。对于回归，我们确定 R-Cover 对具有多项式脂肪破碎维数增长的函数类具有次线性无意识遗憾。]]></description>
      <guid>https://arxiv.org/abs/2410.05124</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在多种治疗情况下，因果效应估计是否足以提供最佳建议？</title>
      <link>https://arxiv.org/abs/2410.05177</link>
      <description><![CDATA[arXiv:2410.05177v1 公告类型：新
摘要：在做出治疗选择决策时，必须进行因果效应估计分析，以比较不同治疗或控制下的潜在结果，从而帮助做出最佳选择。然而，仅仅估计个别治疗效果可能不足以做出真正最佳的决策。我们的研究通过纳入其他标准（例如估计的不确定性，以投资组合和保险管理中常用的条件风险价值来衡量）解决了这个问题。对于治疗前后可观察到的连续结果，我们纳入了一个特定的预测条件。我们优先考虑可以产生最佳治疗效果结果并导致治疗后结果比治疗前水平更理想的治疗方法，后一种条件称为预测标准。考虑到这些因素，我们提出了一种全面的多治疗选择方法。我们的方法通过在采用传统因果模型之前训练倾向得分模型作为初步步骤，确保满足重叠假设，这对于比较治疗组和对照组的结果至关重要。为了说明我们的方法的实际应用，我们将其应用于信用卡限额调整问题。通过分析一家金融科技公司的历史数据，我们发现仅依靠反事实预测不足以进行适当的信用额度修改。结合我们提出的附加标准可显著提高政策绩效。]]></description>
      <guid>https://arxiv.org/abs/2410.05177</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偏差下的回归共形预测</title>
      <link>https://arxiv.org/abs/2410.05263</link>
      <description><![CDATA[arXiv:2410.05263v1 公告类型：新
摘要：不确定性量化对于解释机器学习算法对高影响应用的不完美预测至关重要。共形预测 (CP) 是一个强大的不确定性量化框架，可生成具有有效覆盖范围的校准预测区间。在这项工作中，我们研究了 CP 区间如何受到偏差的影响——预测与地面真实值的系统偏差——这种现象在许多实际应用中普遍存在。我们研究了偏差对两种不同类型调整的区间长度的影响——对称调整，即区间两侧均等调整的传统方法，以及非对称调整，即区间可以在正向或负向不均匀调整的更灵活的方法。我们提出了理论和实证分析，描述了对称和非对称调整如何影响回归任务的 CP 区间的“紧密度”。具体来说，对于绝对残差和基于分位数的不一致性分数，我们证明：1）对称调整间隔长度的上限增加 $2|b|$，其中 $b$ 是表示偏差的全局应用标量值，2）非对称调整间隔长度不受偏差影响，3）非对称调整间隔长度保证小于对称间隔长度的情况。我们的分析表明，即使预测表现出与地面真实值的显着偏差，非对称调整间隔仍然能够保持与间隔相同的紧密度和有效性，就好像漂移从未发生过一样，而对称间隔会显着增加长度。我们通过两个现实世界的预测任务展示了我们的理论结果：稀疏视图计算机断层扫描 (CT) 重建和时间序列天气预报。我们的工作为更具偏差鲁棒性的机器学习系统铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2410.05263</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>线性模型迁移学习的普适性</title>
      <link>https://arxiv.org/abs/2410.02164</link>
      <description><![CDATA[arXiv:2410.02164v1 公告类型：交叉 
摘要：迁移学习是一种有吸引力的框架，适用于数据稀缺或数据收集成本高昂的问题。迁移学习的一种常见方法称为“基于模型”，它涉及使用在源分布的样本上预先训练的模型（该样本更容易获取），然后在目标分布的几个样本上对模型进行微调。希望是，如果源分布和目标分布“接近”，那么微调模型将在目标分布上表现良好，即使它只看到了来自目标分布的几个样本。在这项工作中，我们研究了线性模型中的迁移学习问题，包括回归和二元分类。特别是，我们考虑在用预训练权重初始化的线性模型上使用随机梯度下降 (SGD)，并使用来自目标分布的小型训练数据集。在大型模型的渐近状态下，我们提供了精确而严格的分析，并将预训练和微调模型的泛化误差（在回归中）和分类误差（在二元分类中）联系起来。特别是，我们给出了微调模型优于预训练模型的条件。我们工作的一个重要方面是所有结果都是“通用的”，因为它们仅依赖于目标分布的一阶和二阶统计量。因此，它们远远超出了文献中常见的标准高斯假设。]]></description>
      <guid>https://arxiv.org/abs/2410.02164</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 O-RAN 中自适应资源优化的元强化学习方法</title>
      <link>https://arxiv.org/abs/2410.03737</link>
      <description><![CDATA[arXiv:2410.03737v1 公告类型：交叉 
摘要：随着无线网络不断发展以支持更复杂的应用，开放式无线接入网络 (O-RAN) 架构及其智能 RAN 智能控制器 (RIC) 模块成为实时网络数据收集、分析和网络资源动态管理（包括无线资源块和下行链路功率分配）的关键解决方案。利用人工智能 (AI) 和机器学习 (ML)，O-RAN 以前所未有的效率和适应性满足现代网络的多变需求。尽管在使用基于 ML 的策略进行网络优化方面取得了进展，但挑战仍然存在，特别是在不可预测的环境中动态分配资源方面。本文提出了一种新颖的元深度强化学习 (Meta-DRL) 策略，受模型无关元学习 (MAML) 的启发，以推进 O-RAN 中的资源块和下行链路功率分配。我们的方法利用 O-RAN 的分解架构与虚拟分布式单元 (DU) 和元 DRL 策略，实现自适应和本地化决策，从而显著提高网络效率。通过集成元学习，我们的系统可以快速适应新的网络条件，实时优化资源分配。与传统方法相比，这使网络管理性能提高了 19.8%，从而提升了下一代无线网络的功能。]]></description>
      <guid>https://arxiv.org/abs/2410.03737</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于递减步长的SAGA算法</title>
      <link>https://arxiv.org/abs/2410.03760</link>
      <description><![CDATA[arXiv:2410.03760v1 公告类型：交叉 
摘要：随机优化自然出现在许多应用领域，包括机器学习。我们的目标是进一步分析随机平均梯度加速 (SAGA) 算法。为了实现这一目标，我们引入了一种新的 $\lambda$-SAGA 算法，它在随机梯度下降 ($\lambda=0$) 和 SAGA 算法 ($\lambda=1$) 之间进行插值。首先，我们研究了这种新算法的几乎肯定收敛性，其步骤减少了，这使我们能够避免与目标函数相关的限制性强凸性和 Lipschitz 梯度假设。其次，我们为 $\lambda$-SAGA 算法建立了一个中心极限定理。最后，我们提供了非渐近的 $\mathbb{L}^p$ 收敛速度。]]></description>
      <guid>https://arxiv.org/abs/2410.03760</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于深度神经网络的长记忆随机过程参数估计</title>
      <link>https://arxiv.org/abs/2410.03776</link>
      <description><![CDATA[arXiv:2410.03776v1 公告类型：交叉 
摘要：我们提出了一种纯深度神经网络方法来估计包含长期依赖现象的时间序列模型的长期记忆参数。参数（例如赫斯特指数）对于表征随机过程的长期依赖性、粗糙度和自相似性至关重要。准确快速地估计这些参数对金融、物理和工程等各个科学学科都具有重要意义。我们利用高效的过程生成器来提供高质量的合成训练数据，从而实现尺度不变的一维卷积神经网络 (CNN) 和长短期记忆 (LSTM) 模型的训练。我们的神经模型优于传统的统计方法，即使是那些通过神经网络增强的方法。通过涉及分数布朗运动 (fBm)、自回归分数积分移动平均 (ARFIMA) 过程和分数奥恩斯坦-乌伦贝克 (fOU) 过程的实验，证明了我们的估计器的精度、速度、一致性和稳健性。我们相信，我们的工作将启发使用深度学习技术进行随机过程建模和参数估计领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2410.03776</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>P1-KAN 是一种用于函数逼近的有效 Kolmogorov Arnold 网络</title>
      <link>https://arxiv.org/abs/2410.03801</link>
      <description><![CDATA[arXiv:2410.03801v1 公告类型：交叉 
摘要：提出了一种新的 Kolmogorov-Arnold 网络 (KAN)，用于近似高维中潜在的不规则函数。我们表明它在准确性方面优于多层感知器，并且收敛速度更快。我们还将其与最近提出的网络 ReLU-KAN 进行了比较：它比 ReLU-KAN 更耗时，但更准确。]]></description>
      <guid>https://arxiv.org/abs/2410.03801</guid>
      <pubDate>Tue, 08 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>