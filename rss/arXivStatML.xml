<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>在线决策的自适应基础模型：具有快速增量不确定性估计的 HyperAgent</title>
      <link>https://arxiv.org/abs/2407.13195</link>
      <description><![CDATA[arXiv:2407.13195v1 公告类型：交叉 
摘要：基础模型在面对在线决策的新情况时经常会遇到不确定性，因此需要进行可扩展且有效的探索来解决这种不确定性。我们引入了 GPT-HyperAgent，这是 GPT 与 HyperAgent 的增强，用于在上下文老虎机中进行不确定性感知、可扩展的探索，这是一个涉及自然语言输入的基本在线决策问题。我们证明，在线性可实现假设下，HyperAgent 在 $T$ 个周期内以 $\tilde{O}(\log T)$ 的每步计算复杂度实现了快速增量不确定性估计。我们的分析表明，HyperAgent 的遗憾顺序与线性上下文老虎机中的精确汤普森采样相匹配，从而弥补了可扩展探索中的一个重要理论空白。现实世界的上下文老虎机任务（例如带有人工反馈的自动内容审核）的实证结果验证了 GPT-HyperAgent 对安全关键决策的实际有效性。我们的代码在 \url{https://github.com/szrlee/GPT-HyperAgent/} 开源。]]></description>
      <guid>https://arxiv.org/abs/2407.13195</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>高维学习中的非渐近不确定性量化</title>
      <link>https://arxiv.org/abs/2407.13666</link>
      <description><![CDATA[arXiv:2407.13666v1 公告类型：交叉 
摘要：不确定性量化 (UQ) 是许多高维回归或学习问题中一项至关重要但具有挑战性的任务，可以提高给定预测器的置信度。我们开发了一种新的数据驱动回归 UQ 方法，该方法既适用于 LASSO 等经典回归方法，也适用于神经网络。最值得注意的 UQ 技术之一是去偏 LASSO，它修改了 LASSO，通过将估计误差分解为高斯和渐近消失的偏差分量来构建渐近置信区间。然而，在具有有限维数据的实际问题中，偏差项通常太大而无法忽略，导致置信区间过窄。我们的工作严格解决了这个问题，并推导出一种数据驱动的调整方法，通过利用高维集中现象估计训练数据中偏差项的均值和方差来校正大量预测因子的置信区间。这会产生非渐近置信区间，有助于避免在 MRI 诊断等关键应用中高估不确定性。重要的是，我们的分析不仅限于稀疏回归，还扩展到数据驱动的预测因子（如神经网络），从而提高了基于模型的深度学习的可靠性。我们的发现弥合了既定理论与此类去偏方法的实际适用性之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2407.13666</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>改进 SAM 需要重新思考其优化公式</title>
      <link>https://arxiv.org/abs/2407.12993</link>
      <description><![CDATA[arXiv:2407.12993v1 公告类型：交叉 
摘要：本文重新思考了清晰度感知最小化 (SAM)，它最初被表述为零和游戏，其中网络和有界扰动的权重分别试图最小化/最大化相同的可微分损失。为了从根本上改进这种设计，我们认为应该使用 0-1 损失来重新表述 SAM。作为一种连续的放松，我们遵循简单的传统方法，其中最小化（最大化）玩家使用 0-1 损失的上限（下限）替代。这导致了 SAM 作为双层优化问题的新公式，称为 BiSAM。具有新设计的下限替代损失的 BiSAM 确实构建了更强的扰动。通过数值证据，我们表明，与原始 SAM 和变体相比，BiSAM 始终能够提高性能，同时具有相似的计算复杂度。我们的代码可以在 https://github.com/LIONS-EPFL/BiSAM 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.12993</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:39 GMT</pubDate>
    </item>
    <item>
      <title>一个独立于分辨率的神经算子</title>
      <link>https://arxiv.org/abs/2407.13010</link>
      <description><![CDATA[arXiv:2407.13010v1 公告类型：交叉 
摘要：深度算子网络 (DeepONet) 是一种强大而简单的神经算子架构，它利用两个深度神经网络来学习无限维函数空间之间的映射。该架构高度灵活，允许在所需域内的任何位置评估解决方案字段。但是，它对输入空间施加了严格的约束，要求所有输入函数在相同的位置离散化；这限制了它的实际应用。在这项工作中，我们引入了一个分辨率独立神经算子 (RINO)，它提供了一个框架使 DeepONet 分辨率独立，使其能够处理任意但足够精细地离散化的输入函数。为此，我们提出了一种字典学习算法，从输入数据中自适应地学习一组适当的连续基函数，参数化为隐式神经表示 (INR)。然后使用这些基函数将任意输入函数数据作为点云投影到嵌入空间（即有限维的向量空间），其维数等于字典大小，DeepONet 可以直接使用该空间而无需进行任何架构更改。具体来说，我们利用正弦表示网络 (SIREN) 作为可训练的 INR 基函数。我们通过几个数值示例证明了 RINO 在训练和推理过程中处理任意（但足够丰富）采样的输入函数的稳健性和适用性。]]></description>
      <guid>https://arxiv.org/abs/2407.13010</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:39 GMT</pubDate>
    </item>
    <item>
      <title>NeuroSynth：MRI 衍生的神经解剖生成模型和 18,000 个样本的相关数据集</title>
      <link>https://arxiv.org/abs/2407.12897</link>
      <description><![CDATA[arXiv:2407.12897v1 公告类型：交叉 
摘要：隐私和数据共享限制通常会对大型和多样化医疗数据集的可用性构成挑战。为了成功应用机器学习技术进行疾病诊断、预后和精准医疗，需要大量数据来建立和优化模型。为了帮助克服脑 MRI 环境中的这种限制，我们提出了 NeuroSynth：一组从结构性脑成像中得出的规范区域体积特征的生成模型。NeuroSynth 模型是在 iSTAGING 联盟的真实脑成像区域体积测量值上进行训练的，该联盟涵盖了 13 项研究中的 40,000 多次 MRI 扫描，并结合了年龄、性别和种族等协变量。利用 NeuroSynth，我们生产并提供 18,000 个涵盖成年人寿命（22-90 岁）的合成样本，同时该模型还具有生成无限数据的能力。实验结果表明，NeuroSynth 生成的样本与从真实数据中获得的分布一致。最重要的是，生成的规范数据显著提高了下游机器学习模型在疾病分类等任务上的准确性。数据和模型可在以下网址获取：https://huggingface.co/spaces/rongguangw/neuro-synth。]]></description>
      <guid>https://arxiv.org/abs/2407.12897</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>具有 $d^3$ 成员资格查询的 R\'enyi-infinity 约束采样</title>
      <link>https://arxiv.org/abs/2407.12967</link>
      <description><![CDATA[arXiv:2407.12967v1 公告类型：交叉 
摘要：凸体上的均匀采样是一个基本的算法问题，但大多数采样器的 KL 或 R\&#39;enyi 散度收敛仍然不太清楚。在这项工作中，我们提出了一个受约束的近端采样器，这是​​一种具有优雅收敛保证的原则性和简单算法。利用此采样器的均匀遍历性，我们表明它在从热启动开始时以 R\&#39;enyi-infinity 散度 ($\mathcal R_\infty$) 收敛，并且没有查询复杂性开销。这是通常考虑的性能指标中最强的，意味着 $\{\mathcal R_q, \mathsf{KL}\}$ 收敛率是特殊情况。
通过在退火方案中应用此采样器，我们提出了一种算法，该算法可以在查询复杂度为 $\widetilde{\mathcal{O}}(d^3\, \text{polylog} \frac{1}{\varepsilon})$ 的 $\mathcal R_\infty$ 散度中近似采样接近凸体均匀分布的 $\varepsilon$ 散度。这改进了 $\{\mathcal R_q, \mathsf{KL}\}$ 散度中的所有先前结果，而无需诉诸任何算法修改或样本的后处理。它还与总变分距离中先前已知的最佳复杂度相匹配。]]></description>
      <guid>https://arxiv.org/abs/2407.12967</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>$\texttt{metabench}$——用于测量大型语言模型通用能力的稀疏基准</title>
      <link>https://arxiv.org/abs/2407.12844</link>
      <description><![CDATA[arXiv:2407.12844v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 在一系列任务上的能力各不相同。诸如 $\texttt{Open LLM Leaderboard}$ 之类的计划旨在通过几个大型基准（LLM 可以正确或错误地响应的测试项目集）来量化这些差异。然而，基准分数内部和之间的高相关性表明 (1) 这些基准测量了一小组共同的潜在能力，并且 (2) 项目利用了冗余信息，因此基准可能会被大大压缩。我们使用来自 $n &gt; 5000$ 个 LLM 的数据来识别六个基准中最具信息量的项目，即 ARC、GSM8K、HellaSwag、MMLU、TruthfulQA 和 WinoGrande（总共 $d=28,632$ 个项目）。我们从中提炼出一个稀疏基准 $\texttt{metabench}$，其大小不到所有六个基准合并的原始大小的 $3\%$。这个新的稀疏基准超越了点分数，它产生了底层基准特定能力的估计量。我们表明，这些估计量 (1) 可用于重建每个原始 $\textit{individual}$ 基准分数，平均均方根误差 (RMSE) 为 $1.5\%$，(2) 重建原始 $\textit{total}$ 分数，RMSE 为 $0.8\%$，(3) 具有单个底层共同因子，其与总分的 Spearman 相关性为 $r = 0.93$。]]></description>
      <guid>https://arxiv.org/abs/2407.12844</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>使用动态图拉普拉斯算子对随时间演化的网络进行聚类</title>
      <link>https://arxiv.org/abs/2407.12864</link>
      <description><![CDATA[arXiv:2407.12864v1 公告类型：交叉 
摘要：在对复杂的动态系统（例如社交网络、交通流和生物过程）进行建模时，经常会出现随时间演变的图。开发识别和分析这些随时间变化的图结构中的社区的技术是一项重要的挑战。在这项工作中，我们使用典型相关分析 (CCA) 将现有的谱聚类算法从静态图推广到动态图，以捕捉聚类的时间演变。基于这个扩展的典型相关框架，我们定义了动态图拉普拉斯算子并研究了它的谱特性。我们通过转移算子将这些概念与动态系统理论联系起来，并通过与现有方法的比较说明了我们的方法在基准图上的优势。我们表明，动态图拉普拉斯算子可以清楚地解释有向图和无向图的聚类结构随时间的演变。]]></description>
      <guid>https://arxiv.org/abs/2407.12864</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>部分观测下的预测低秩矩阵学习：混合投影 ADMM</title>
      <link>https://arxiv.org/abs/2407.13731</link>
      <description><![CDATA[arXiv:2407.13731v1 公告类型：新
摘要：我们研究在低秩假设下，在存在完全观察到的边信息的情况下，学习部分观察到的矩阵的问题，该边信息线性依赖于真实的底层矩阵。该问题包括矩阵完成问题的重要概括，矩阵完成问题是统计学、运筹学和机器学习中的核心问题，出现在推荐系统、信号处理、系统识别和图像去噪等应用中。我们将这个问题形式化为一个优化问题，目标是平衡重建与观察到的条目的拟合强度和重建对边信息的预测能力。我们推导出所得优化问题的混合投影重新表述，并提出一个强半定锥松弛。我们设计了一种高效、可扩展的交替方向乘数算法，可以为感兴趣的问题提供高质量的可行解。我们的数值结果表明，在小秩范围内 ($k \leq 15$)，我们的算法输出的解决方案比实验中表现最佳的基准方法返回的解决方案平均低 $79\%$ 目标值和 $90.1\% $ \ell_2$ 重构误差。我们的算法的运行时间与基准方法相比具有竞争力，甚至通常优于基准方法。我们的算法能够在不到一分钟的时间内解决 $n = 10000$ 行和 $m = 10000$ 列的问题。]]></description>
      <guid>https://arxiv.org/abs/2407.13731</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>回溯训练：真实数据在训练大型语言模型中的价值</title>
      <link>https://arxiv.org/abs/2407.12835</link>
      <description><![CDATA[arXiv:2407.12835v1 公告类型：交叉 
摘要：如果我们使用至少部分由其他 LLM 生成的数据来训练新的大型语言模型 (LLM)，会发生什么？LLM 的爆炸式成功意味着大量在线内容将由 LLM 而不是人类生成，这些内容将不可避免地进入下一代 LLM 的训练数据集。我们评估了这种“反刍训练”对 LLM 性能的影响。通过在机器翻译任务中使用其自身或其他 LLM 生成的数据对 GPT-3.5 进行微调，我们发现强有力的证据表明反刍训练明显阻碍了 LLM 的性能。在我们从头开始训练的 Transformer 模型上观察到了同样的反刍训练性能损失。我们发现有启发性的证据表明，反刍训练的性能劣势可以归因于至少两种机制：(1) 与真实数据相比，LLM 生成的数据中的错误率更高和 (2) 词汇多样性更低。基于这些机制，我们提出并评估了三种不同的策略来减轻反刍训练的性能损失。首先，我们设计了数据驱动的指标来衡量每个 LLM 生成的数据实例的质量，然后执行有序的训练过程，其中高质量数据先于低质量数据添加。其次，我们结合了由多个不同的 LLM 生成的数据（试图增加词汇多样性）。第三，我们训练了一个 AI 检测分类器来区分 LLM 和人类生成的数据，并按与人类生成的数据的相似度顺序包含 LLM 生成的数据。这三种策略都可以在一定程度上提高反刍训练的性能，但并不总是能够完全缩小与使用真实数据进行训练的差距。我们的结果强调了真实的、人类生成的数据在训练 LLM 中的价值，而这些数据不能轻易被合成的、LLM 生成的数据所取代。]]></description>
      <guid>https://arxiv.org/abs/2407.12835</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>数据驱动的条件期望估计、应用于最优停止和强化学习</title>
      <link>https://arxiv.org/abs/2407.13189</link>
      <description><![CDATA[arXiv:2407.13189v1 公告类型：新
摘要：当已知底层条件密度时，可以通过分析或数值计算条件期望。然而，当没有这样的知识，而是给我们一组训练数据时，这项工作的目标是提出简单且纯数据驱动的方法来直接估计所需的条件期望。由于条件期望出现在许多随机优化问题的描述中，相应的最优解满足非线性方程组，我们扩展了我们的数据驱动方法以涵盖此类情况。我们通过将其应用于强化学习中的最佳停止和最佳行动策略来测试我们的方法。]]></description>
      <guid>https://arxiv.org/abs/2407.13189</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>工业过程中半监督多单元软测量的深度隐变量模型</title>
      <link>https://arxiv.org/abs/2407.13310</link>
      <description><![CDATA[arXiv:2407.13310v1 公告类型：新
摘要：在许多工业过程中，明显缺乏数据限制了数据驱动软传感器的发展。然而，通常有机会通过提高数据效率来学习更强大的模型。为了实现这一点，人们可以利用有关软传感器学习数据的知识。利用工业数据经常拥有的属性，我们引入了一个用于半监督多单元软测量的深度隐变量模型。这种分层生成模型能够联合建模不同的单元，并从标记和未标记的数据中学习。
使用两个数据集对多单元软测量进行了实证研究：单相流体流动的合成数据集和油气井多相流的大型真实数据集。我们表明，通过结合半监督和多任务学习，所提出的模型取得了优异的结果，优于目前针对这一软测量问题的领先方法。我们还表明，当模型在多单元数据集上进行训练时，仅使用少量数据点即可将其微调至之前未见过的单元。在此微调过程中，未标记的数据可提高软传感器性能；值得注意的是，即使没有标记数据，情况也是如此。]]></description>
      <guid>https://arxiv.org/abs/2407.13310</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>从加速度测量中发现结构动力学中的控制方程</title>
      <link>https://arxiv.org/abs/2407.13704</link>
      <description><![CDATA[arXiv:2407.13704v1 公告类型：新
摘要：在过去的几年中，方程发现在不同的科学和工程领域越来越受欢迎。然而，现有的方程发现算法依赖于状态变量（即位移{和速度}）的噪声测量的可用性。这是结构动力学中的一个主要瓶颈，我们通常只能获得加速度测量值。为此，本文介绍了一种新颖的方程发现算法，用于从仅加速度测量中发现动力系统的控制方程。所提出的算法采用基于库的方法进行方程发现。为了能够从仅加速度测量中发现方程，我们提出了一种新颖的近似贝叶斯计算 (ABC) 模型，该模型优先考虑简约模型。使用包括线性和非线性动力系统的{四个}结构动力学示例说明了所提出算法的有效性。所提出的案例研究说明了所提出的方法在从仅加速度测量中发现动力系统方程的可能应用。]]></description>
      <guid>https://arxiv.org/abs/2407.13704</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>清晰度-多样性权衡：使用 SharpBalance 改进平面集成</title>
      <link>https://arxiv.org/abs/2407.12996</link>
      <description><![CDATA[arXiv:2407.12996v1 公告类型：新
摘要：最近对深度集成的研究已经确定个体学习者的局部最小值的锐度和集成成员的多样性是提高测试时间性能的关键因素。在此基础上，我们的研究调查了深度集成中锐度和多样性之间的相互作用，说明了它们在对分布内 (ID) 和分布外 (OOD) 数据的稳健泛化中的关键作用。我们发现了锐度和多样性之间的权衡：最小化损失景观中的锐度往往会降低集成中个体成员的多样性，从而对集成的改进产生不利影响。我们的理论分析证明了这种权衡的合理性，并通过大量实验进行了实证验证。为了解决多样性降低的问题，我们引入了 SharpBalance，这是一种平衡集成中锐度和多样性的新型训练方法。从理论上讲，我们表明我们的训练策略实现了更好的锐度-多样性权衡。实证研究方面，我们在各类数据集（CIFAR-10、CIFAR-100、TinyImageNet）上进行了综合评估，结果表明 SharpBalance 不仅有效改善了清晰度-多样性权衡，而且显著提高了 ID 和 OOD 场景中的集成性能。]]></description>
      <guid>https://arxiv.org/abs/2407.12996</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>对抗抽样偏差：训练和评估信用评分模型的框架</title>
      <link>https://arxiv.org/abs/2407.13009</link>
      <description><![CDATA[arXiv:2407.13009v1 公告类型：新
摘要：评分模型支持金融机构的决策。他们的估计和评估基于先前被接受的具有已知还款行为的申请人的数据。这会产生抽样偏差：可用的标记数据提供了候选借款人分布的部分图像，模型应该对其进行评分。本文讨论了抽样偏差对模型训练和评估的不利影响。为了改进记分卡训练，我们提出了偏见感知的自学习 - 一种拒绝推理框架，通过推断选定的被拒绝申请的标签来增强有偏见的训练数据。对于记分卡评估，我们提出了一个贝叶斯框架，将标准准确度度量扩展到有偏见的设置并提供未来记分卡性能的可靠估计。对合成数据和现实世界数据的大量实验证实了我们的主张在预测性能和盈利能力方面优于各种基准。通过敏感性分析，我们还确定了影响其性能的边界条件。值得注意的是，我们利用随机对照试验的真实数据来评估代表真实借款人群体的保留数据的新方法。我们的研究结果证实，拒绝推理是一个难题，但对提高记分卡性能具有一定的潜力。在记分卡评估过程中解决抽样偏差是改善评分实践的更有希望的途径。例如，我们的结果表明，当使用贝叶斯评估来决定接受率时，利润将提高约 8%。]]></description>
      <guid>https://arxiv.org/abs/2407.13009</guid>
      <pubDate>Fri, 19 Jul 2024 06:20:34 GMT</pubDate>
    </item>
    </channel>
</rss>