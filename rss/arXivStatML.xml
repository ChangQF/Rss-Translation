<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>标签噪声的协调稀疏恢复</title>
      <link>https://arxiv.org/abs/2404.04800</link>
      <description><![CDATA[arXiv:2404.04800v1 公告类型：交叉
摘要：标签噪声是现实数据集中的常见问题，不可避免地影响模型的泛化。本研究重点关注标签噪声依赖于实例的鲁棒分类任务。在此任务中准确估计转移矩阵具有挑战性，并且基于样本选择的方法通常会表现出不同程度的确认偏差。稀疏超参数化训练（SOP）在理论上可以有效地估计和恢复标签噪声，为噪声标签学习提供了一种新颖的解决方案。然而，本研究实证观察并验证了SOP的一个技术缺陷：模型预测和噪声恢复之间缺乏协调导致泛化误差增加。为了解决这个问题，我们提出了一种称为协调稀疏恢复（CSR）的方法。 CSR 引入了协作矩阵和置信权重来协调模型预测和噪声恢复，从而减少错误泄漏。本研究以CSR为基础，设计了联合样本选择策略，构建了一个全面而强大的学习框架，称为CSR+。 CSR+ 显着减少了确认偏差，特别是对于具有更多类别和高比例实例特定噪声的数据集。模拟和现实世界噪声数据集上的实验结果表明，与相同级别的方法相比，CSR 和 CSR+ 均取得了出色的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.04800</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>用于动态剩余使用寿命预测的混合域适应</title>
      <link>https://arxiv.org/abs/2404.04824</link>
      <description><![CDATA[arXiv:2404.04824v1 公告类型：交叉
摘要：剩余使用寿命（RUL）预测对于资产规划和维护起着至关重要的作用，可以为行业带来许多好处，例如减少停机时间、降低维护成本等。尽管人们已经做出了各种努力来研究这一主题，但大多数现有工作都受到限制对于 i.i.d 条件，假设训练阶段和部署阶段的条件相同。本文针对这一问题提出了一种解决方案，其中提出了混合域自适应（MDAN）。 MDAN 包含一个三阶段机制，其中混合策略不仅执行以规范源域和目标域，还应用于建立源域和目标域对齐的中间混合域。实施自监督学习策略是为了防止监督崩溃问题。我们对 MDAN 与最近发表的动态 RUL 预测作品进行了比较。在 12 个案例中，MDAN 有 12 个案例的表现优于同行。此外，MDAN 使用轴承机器数据集进行了评估，在 12 个案例中的 8 个案例中，它以显着差距击败了现有技术。 MDAN 的源代码在 \url{https://github.com/furqon3009/MDAN} 中公开提供。]]></description>
      <guid>https://arxiv.org/abs/2404.04824</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>从宏观角度揭秘神经网络的惰性训练</title>
      <link>https://arxiv.org/abs/2404.04859</link>
      <description><![CDATA[arXiv:2404.04859v1 公告类型：交叉
摘要：在本文中，我们通过检查初始化过程中权重参数引入的各种因素的复杂相互作用，增进了对神经网络训练动态的理解。受到 Luo 等人的基础工作的激励。 （J. Mach. Learn. Res.，第 22 卷，第 1 期，第 71 期，第 3327-3373 页），我们通过宏观极限的视角探索神经网络的梯度下降动力学，我们将其行为分析为宽度$m$趋于无穷大。我们的研究提出了一种统一的方法，采用专为多层全连接神经网络设计的精致技术，可以轻松扩展到其他神经网络架构。我们的研究表明，只要输出函数 $\kappa$ 的初始规模超过某个阈值，无论权重参数采用何种具体初始化方案，梯度下降都可以快速驱动深度神经网络达到零训练损失。这种状态被称为 theta-lazy 区域，它强调了初始尺度 $\kappa$ 相对于其他因素对神经网络训练行为的主要影响。此外，我们的方法从神经正切核（NTK）范式中汲取灵感，并扩展了其适用性。而 NTK 通常假设 $\lim_{m\to\infty}\frac{\log \kappa}{\log m}=\frac{1}{2}$，并强制每个权重参数按因子 $ 缩放\frac{1}{\sqrt{m}}$，在我们的 theta-lazy 机制中，我们丢弃该因子并将条件放宽到 $\lim_{m\to\infty}\frac{\log \kappa}{\记录 m}&gt;0$。与 NTK 类似，通过梯度下降训练的 theta-lazy 机制中的过度参数化神经网络的行为可以通过特定的内核有效地描述。通过严格的分析，我们的研究阐明了 $\kappa$ 在控制神经网络训练动态方面的关键作用。]]></description>
      <guid>https://arxiv.org/abs/2404.04859</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>使用配备仿射编码器和解码器的尖峰神经网络进行高效学习</title>
      <link>https://arxiv.org/abs/2404.04549</link>
      <description><![CDATA[arXiv:2404.04549v1 公告类型：交叉
摘要：我们研究与尖峰神经网络相关的学习问题。具体来说，我们考虑具有仿射时间编码器和解码器的尖峰神经网络的假设集以及仅具有正突触权重的简单尖峰神经元。我们证明了权重的正性继续能够实现广泛的表达结果，包括平滑函数的速率最优逼近或没有维数灾难的逼近。此外，正权值尖峰神经网络被证明持续依赖于它们的参数，这有利于经典的基于数字的泛化陈述。最后，我们观察到，从泛化的角度来看，与前馈神经网络或一般尖峰神经网络的先前结果相反，深度对泛化能力几乎没有不利影响。]]></description>
      <guid>https://arxiv.org/abs/2404.04549</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>针对过度挤压和过度平滑的谱图修剪</title>
      <link>https://arxiv.org/abs/2404.04612</link>
      <description><![CDATA[arXiv:2404.04612v1 公告类型：交叉
摘要：众所周知，消息传递图神经网络存在两个有时被认为截然相反的问题：过度挤压和过度平滑。前者是由拓扑瓶颈造成的，拓扑瓶颈阻碍了来自远程节点的信息流，并且可以通过谱间隙最大化（主要是通过边缘添加）来缓解。然而，此类添加通常会导致过度平滑，从而使不同类别的节点难以区分。受布雷斯现象的启发，我们认为删除边缘可以同时解决过度挤压和过度平滑的问题。这一见解解释了边缘删除如何提高泛化能力，从而将谱间隙优化与通过修剪彩票图来减少计算资源这一看似无关的目标联系起来。为此，我们提出了一种更有效的光谱间隙优化框架来添加或删除边缘，并证明其在大型异嗜性数据集上的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.04612</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>BARMPy：贝叶斯加性回归模型 Python 包</title>
      <link>https://arxiv.org/abs/2404.04738</link>
      <description><![CDATA[arXiv:2404.04738v1 公告类型：交叉
摘要：我们将贝叶斯加性回归网络（BARN）作为 Python 包 \texttt{barmpy} 提供，文档位于 \url{https://dvbuntu.github.io/barmpy/} ，供一般机器学习从业者使用。我们面向对象的设计与 SciKit-Learn 兼容，允许使用他们的工具，例如交叉验证。为了方便学习使用 \texttt{barmpy}，我们制作了一个配套教程，扩展了文档中的参考信息。任何感兴趣的用户都可以从官方 PyPi 存储库 \texttt{pip install barmpy} 。 \texttt{barmpy} 还用作通用贝叶斯加性回归模型的基线 Python 库。]]></description>
      <guid>https://arxiv.org/abs/2404.04738</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>AdamW 的隐式偏差：$\ell_\infty$ 范数约束优化</title>
      <link>https://arxiv.org/abs/2404.04454</link>
      <description><![CDATA[arXiv:2404.04454v1 公告类型：交叉
摘要： 具有解耦权重衰减的 Adam，也称为 AdamW，因其在语言建模任务中的优越性能而受到广泛好评，在泛化和优化方面超越了 $\ell_2$ 正则化的 Adam。然而，这种优势在理论上并没有得到很好的理解。这里的一个挑战是，尽管直观上 Adam 使用 $\ell_2$ 正则化优化了 $\ell_2$ 正则化损失，但尚不清楚 AdamW 是否优化了特定目标。在这项工作中，我们通过证明 AdamW 隐式执行约束优化，在理解 AdamW 的好处方面取得了进展。更具体地说，我们在全批次设置中表明，如果 AdamW 与任何部分和发散的非递增学习率计划收敛，则在 $\ell_\infty$ 的约束下，它必须收敛到原始损失的 KKT 点参数的范数受权重衰减因子的倒数限制。这个结果是基于这样的观察：Adam 可以被视为 SignGD 的平滑版本，这是相对于 $\ell_\infty$ 范数的归一化最速下降，以及权重衰减的归一化最速下降与 Frank-沃尔夫.]]></description>
      <guid>https://arxiv.org/abs/2404.04454</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>长度控制的 AlpacaEval：消除自动评估器偏差的简单方法</title>
      <link>https://arxiv.org/abs/2404.04475</link>
      <description><![CDATA[arXiv:2404.04475v1 公告类型：交叉
摘要：与基于人工的评估相比，基于法学硕士的自动注释器由于其成本效益和可扩展性而成为法学硕士开发过程的关键组成部分。然而，这些自动注释器可能会引入难以消除的复杂偏差。即使是简单的、已知的混杂因素，例如对较长输出的偏好，仍然存在于现有的自动评估指标中。我们提出了一种简单的回归分析方法来控制自动评估中的偏差。作为一个真实的案例研究，我们专注于减少 AlpacaEval 的长度偏差，这是一种快速且经济实惠的聊天法学硕士基准，使用法学硕士来估计响应质量。尽管 AlpacaEval 与人类偏好高度相关，但众所周知，它更喜欢生成更长输出的模型。我们引入了一个长度控制的 AlpacaEval，旨在回答反事实问题：“如果模型和基线的输出具有相同的长度，偏好是什么？”。为了实现这一目标，我们首先拟合一个广义线性模型，根据我们想要控制的中介（长度差异）和其他相关特征来预测感兴趣的偏置输出（自动注释器偏好）。然后，我们通过预测偏好来获得长度控制的偏好，同时以零长度差异调节 GLM。长度控制不仅提高了模型冗长操作的稳健性，我们还发现它与 LMSYS 的 Chatbot Arena 的 Spearman 相关性从 0.94 增加到 0.98。我们在 https://tatsu-lab.github.io/alpaca_eval/ 发布了代码和排行榜。]]></description>
      <guid>https://arxiv.org/abs/2404.04475</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>超参数化非线性回归中一致预测的贝叶斯推理</title>
      <link>https://arxiv.org/abs/2404.04498</link>
      <description><![CDATA[arXiv:2404.04498v1 公告类型：新
摘要：超参数化模型卓越的泛化性能挑战了统计学习理论的传统智慧。虽然最近的理论研究揭示了线性模型或非线性分类器中的这种行为，但仍然缺乏对非线性回归中的过度参数化的全面理解。本文探讨了贝叶斯框架内超参数化非线性回归的预测特性，扩展了基于数据内在谱结构的自适应先验方法。我们为具有 Lipschitz 连续激活函数的单神经元模型和广义线性模型建立了后收缩，证明我们的方法在超参数化状态下实现了一致的预测。此外，我们的贝叶斯框架允许对预测进行不确定性估计。所提出的方法通过数值模拟和实际数据应用进行了验证，展示了其实现准确预测和可靠的不确定性估计的能力。我们的工作增进了对超参数化优势的理论理解，并为大型非线性模型的预测提供了原则性的贝叶斯方法。]]></description>
      <guid>https://arxiv.org/abs/2404.04498</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士置信度评分的多重校准</title>
      <link>https://arxiv.org/abs/2404.04689</link>
      <description><![CDATA[arXiv:2404.04689v1 公告类型：新
摘要：本文提出使用“多重校准”来为大型语言模型（LLM）生成的输出生成可解释且可靠的置信度得分。多重校准不仅要求进行边际校准，而且要求同时对数据的各种交叉分组进行校准。我们展示了如何通过两种技术形成与正确概率相关的提示/完成对的分组：嵌入空间内的聚类和“自注释”——通过询问有关 LLM 的各种是或否问题来查询 LLM提示。我们还开发了多重校准算法的新颖变体，通过减少过度拟合的趋势来提高性能。通过对各种问答数据集和法学硕士进行系统基准测试，我们展示了我们的技术如何产生置信度分数，与现有方法相比，这些置信度分数在校准和准确性的细粒度测量方面提供了实质性改进。]]></description>
      <guid>https://arxiv.org/abs/2404.04689</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>通过逆条件排列的灵活公平学习</title>
      <link>https://arxiv.org/abs/2404.05678</link>
      <description><![CDATA[arXiv:2404.05678v1 公告类型：新
摘要：均等赔率作为算法公平性的流行概念，旨在确保种族和性别等敏感变量在以真实结果为条件时不会不公平地影响算法预测。尽管取得了快速进展，但当前的大多数研究都集中在由一种敏感属性引起的均等赔率的违反，而同时考虑多个属性的挑战却没有得到充分解决。我们通过引入一种公平学习方法来解决这一差距，该方法将对抗性学习与新颖的逆条件排列相结合。这种方法有效且灵活地处理多个敏感属性，可能是混合数据类型。我们的方法的有效性和灵活性通过模拟研究和对现实世界数据集的实证分析得到了证明。]]></description>
      <guid>https://arxiv.org/abs/2404.05678</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>DeepLINK-T：使用仿制品和 LSTM 对时间序列数据进行深度学习推理</title>
      <link>https://arxiv.org/abs/2404.04317</link>
      <description><![CDATA[arXiv:2404.04317v1 公告类型：新
摘要：高维纵向时间序列数据在各种现实应用中普遍存在。许多此类应用可以建模为具有高维时间序列协变量的回归问题。深度学习一直是拟合这些回归模型的流行且强大的工具。然而，可解释和可重复的深度学习模型的开发具有挑战性，并且仍未得到充分探索。本研究引入了一种新颖的方法，即使用时间序列数据仿冒的深度学习推理（DeepLINK-T），重点关注回归中重要时间序列变量的选择，同时将错误发现率（FDR）控制在预定水平。 DeepLINK-T 将深度学习与仿冒推理相结合，控制时间序列模型特征选择中的 FDR，适应各种特征分布。它通过利用时间序列协变量中的时变潜在因子结构来解决跨时间和特征的依赖性。 DeepLINK-T 的三个关键要素是 1) 用于生成时间序列仿冒变量的长短期记忆 (LSTM) 自动编码器，2) 使用原始变量和仿冒变量的 LSTM 预测网络，以及 3) 仿冒框架的应用通过 FDR 控制进行变量选择。为了评估 DeepLINK-T 的性能，我们进行了广泛的仿真研究，显示了其有效控制 FDR 的能力，同时展示了与非时间序列数据相比对高维纵向时间序列数据的卓越特征选择能力。 DeepLINK-T进一步应用于三个宏基因组数据集，验证了其实用性和有效性，并强调了其在实际应用中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2404.04317</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>基于时间差异质变压器的纵向目标最小损失估计</title>
      <link>https://arxiv.org/abs/2404.04399</link>
      <description><![CDATA[arXiv:2404.04399v1 公告类型：新
摘要：我们提出了基于深度纵向目标最小损失的估计（Deep LTMLE），这是一种在纵向问题设置中动态治疗策略下估计结果的反事实均值的新方法。我们的方法利用具有异构类型嵌入的变压器架构，并使用时差学习进行训练。使用 Transformer 获得初始估计后，遵循目标最小基于损失的似然估计 (TMLE) 框架，我们对通常与机器学习算法相关的偏差进行统计校正。此外，我们的方法还通过提供基于渐近统计理论的 95% 置信区间来促进统计推断。仿真结果证明我们的方法比现有方法具有优越的性能，特别是在复杂的长期场景中。它在小样本、短持续时间的环境中仍然有效，与渐近有效估计器的性能相匹配。为了在实践中证明我们的方法，我们应用我们的方法来估计现实世界心血管流行病学队列研究中标准血压管理策略与强化血压管理策略的反事实平均结果。]]></description>
      <guid>https://arxiv.org/abs/2404.04399</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯加性回归网络</title>
      <link>https://arxiv.org/abs/2404.04425</link>
      <description><![CDATA[arXiv:2404.04425v1 公告类型：新
摘要：我们应用贝叶斯加性回归树（BART）原理来训练用于回归任务的小型神经网络集合。使用马尔可夫链蒙特卡罗，我们从具有单个隐藏层的神经网络的后验分布中进行采样。为了创建这些网络的集合，我们应用吉布斯采样来根据残差目标值更新每个网络（即减去其他网络的影响）。我们通过将其与等效的浅层神经网络、BART 和普通最小二乘法进行比较，证明了该技术在几个基准回归问题上的有效性。我们的贝叶斯加性回归网络 (BARN) 提供更一致且通常更准确的结果。在测试数据基准上，BARN 均方根误差平均降低 5% 到 20%。然而，这种错误性能确实是以更长的计算时间为代价的。 BARN 有时需要一分钟的时间，而竞争方法需要一秒或更短的时间。但是，没有交叉验证的超参数调整的 BARN 与调整的其他方法花费的计算时间大致相同。然而 BARN 通常仍然更准确。]]></description>
      <guid>https://arxiv.org/abs/2404.04425</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>盲目网络收益管理的原始对偶优化中的需求平衡</title>
      <link>https://arxiv.org/abs/2404.04467</link>
      <description><![CDATA[arXiv:2404.04467v1 公告类型：新
摘要：本文提出了一种具有最佳理论遗憾的实用有效算法，解决了具有未知、非参数需求的经典网络收入管理（NRM）问题。在 $T$ 的时间范围内，在每个时间段内，零售商需要确定 $N$ 种产品的价格，这些产品是基于具有不可补充的初始库存的 $M$ 种资源生产的。当需求是非参数且带有一些温和的假设时，Miao 和 Wang (2021) 是第一篇提出 $O(\text{poly}(N,M,\ln(T))\sqrt{T})$ 算法的论文遗憾的类型（特别是 $\tilde O(N^{3.5}\sqrt{T})$ 加上额外的高阶项 $o(\sqrt{T})$ 且 $T\gg N 足够大$）。在本文中，我们通过提出一种原对偶优化算法来改进之前的结果，该算法不仅更实用，而且改进了 $\tilde O(N^{3.25}\sqrt{T})$ 的遗憾，并且不受额外的高阶项。该算法的一个关键技术贡献是所谓的需求平衡，它将每个时间段的原始解决方案（即价格）与另一个价格配对，以抵消对资源库存约束的互补松弛的违反。与几种基准算法进行比较的数值实验进一步说明了我们算法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.04467</guid>
      <pubDate>Tue, 09 Apr 2024 06:17:15 GMT</pubDate>
    </item>
    </channel>
</rss>