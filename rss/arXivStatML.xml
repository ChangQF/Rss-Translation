<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 05 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>优化最优加权平均值：高效分布式稀疏分类</title>
      <link>https://arxiv.org/abs/2406.01753</link>
      <description><![CDATA[arXiv:2406.01753v1 公告类型：交叉 
摘要：虽然分布式训练通常被视为在越来越大的数据集上优化线性模型的解决方案，但随着数据维数的增加，流行的分布式方法的机器间通信成本可能占主导地位。最近对非交互式算法的研究表明，只需在机器之间进行一轮通信即可有效地获得线性模型的近似解。然而，随着机器数量的增加，这种近似值往往会退化。在本文中，我们基于最近的最优加权平均法，引入了一种新技术 ACOWA，它允许额外一轮通信以在运行时间略有增加的情况下实现明显更好的近似质量。结果表明，对于稀疏分布式逻辑回归，ACOWA 获得的解决方案更忠实于经验风险最小化器，并且比其他分布式算法具有更高的准确性。]]></description>
      <guid>https://arxiv.org/abs/2406.01753</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:48 GMT</pubDate>
    </item>
    <item>
      <title>具有兼容函数逼近的单环（自然）演员-评论家的非渐近分析</title>
      <link>https://arxiv.org/abs/2406.01762</link>
      <description><![CDATA[arXiv:2406.01762v1 公告类型：交叉 
摘要：演员-评论家 (AC) 是强化学习中学习最佳策略的一种强大方法，其中评论家使用算法（例如，使用函数近似的时间差分 (TD) 学习）来评估当前策略，演员使用来自评论家的信息沿近似梯度方向更新策略。本文为 AC 和自然 AC (NAC) 算法提供了 \textit{最紧} 非渐近收敛界限。具体而言，现有研究表明，AC 收敛到驻点的 $\epsilon+\varepsilon_{\text{critic}}$ 邻域，其最佳样本复杂度为 $\mathcal{O}(\epsilon^{-2})$（最多为对数因子），而 NAC 收敛到全局最优的 $\epsilon+\varepsilon_{\text{critic}}+\sqrt{\varepsilon_{\text{actor}}}$ 邻域，其最佳样本复杂度为 $\mathcal{O}(\epsilon^{-3})$，其中 $\varepsilon_{\text{critic}}$ 为批评家的近似误差，$\varepsilon_{\text{actor}}$ 为参数化策略类的表达能力不足导致的近似误差。本文分析了具有兼容函数逼近的 AC 和 NAC 算法的收敛性。我们的分析将 $\varepsilon_{\text{critic}}$ 项从误差界限中消除，同时仍实现最佳已知样本复杂度。此外，我们专注于具有单个马尔可夫样本轨迹的具有挑战性的单循环设置。我们的主要技术创新在于分析了批评者中由于策略相关和时变兼容函数近似而导致的随机偏差，并处理了由于单个马尔可夫样本轨迹而导致的 MDP 的非遍历性。附录中还提供了数值结果。]]></description>
      <guid>https://arxiv.org/abs/2406.01762</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:48 GMT</pubDate>
    </item>
    <item>
      <title>无监督神经组合优化的扩散模型框架</title>
      <link>https://arxiv.org/abs/2406.01661</link>
      <description><![CDATA[arXiv:2406.01661v1 公告类型：交叉 
摘要：学习从离散集上的难处理分布中进行采样而不依赖于相应的训练数据是包括组合优化在内的广泛领域的核心问题。目前，流行的基于深度学习的方法主要依赖于产生精确样本似然的生成模型。这项工作引入了一种解除此限制的方法，并为使用扩散模型等高度表达的隐变量模型提供了可能性。我们的方法在概念上基于一个上限为反向 Kullback-Leibler 散度的损失，并避免了对精确样本似然的要求。我们在无数据组合优化中通过实验验证了我们的方法，并证明我们的方法在广泛的基准问题上取得了新的领先水平。]]></description>
      <guid>https://arxiv.org/abs/2406.01661</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:47 GMT</pubDate>
    </item>
    <item>
      <title>系统 2 推荐系统：通过时间点过程解开推荐系统中的实用性和参与度</title>
      <link>https://arxiv.org/abs/2406.01611</link>
      <description><![CDATA[arXiv:2406.01611v1 公告类型：交叉 
摘要：推荐系统是现代人类体验的重要组成部分，其影响范围从我们吃的食物到我们阅读的新闻。然而，关于推荐平台在多大程度上与用户目标保持一致仍然存在争议。引发这场争论的一个核心问题是，根据喜欢、分享、观看时间等参与度信号推断用户效用的挑战，这些信号是平台优化内容的主要指标。这是因为用户的效用驱动决策过程（我们称之为系统 2），例如阅读与他们相关的新闻，经常被他们的冲动决策过程（我们称之为系统 1）所混淆，例如花时间在点击诱饵新闻上。因此，很难推断观察到的参与度是效用驱动的还是冲动驱动的。在本文中，我们探索了一种推荐系统的新方法，我们根据用户返回平台的概率而不是参与度信号来推断用户效用。我们的直觉是，如果平台能为用户创造效用，用户往往会在长期内返回该平台，而纯粹的参与驱动互动不会增加效用，可能会在短期内影响用户回归，但不会产生持久影响。我们提出了一个生成模型，其中过去的内容互动基于自激霍克斯过程影响用户的到达率。这些平台到达率是系统 1 和系统 2 决策过程的组合。系统 2 到达强度取决于效用并具有长期影响，而系统 1 强度取决于即时满足感并倾向于迅速消失。我们通过分析表明，给定样本可以解开系统 1 和系统 2，并允许基于用户效用进行内容优化。我们对合成数据进行实验，以证明我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.01611</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>自我改进的稳健偏好优化</title>
      <link>https://arxiv.org/abs/2406.01660</link>
      <description><![CDATA[arXiv:2406.01660v2 公告类型：交叉 
摘要：在线和离线 RLHF 方法（例如 PPO 和 DPO）在将 AI 与人类偏好相结合方面都非常成功。尽管取得了成功，但现有方法存在一个根本问题，即其最优解高度依赖于任务（即对分布外 (OOD) 任务不具有鲁棒性）。在这里，我们通过提出自我改进的鲁棒偏好优化 SRPO 来解决这一挑战，这是一个实用且数学原理的离线 RLHF 框架，对任务的变化具有完全的鲁棒性。SRPO 的关键思想是将从人类偏好中学习的问题转化为自我改进过程，可以用最小-最大目标在数学上表示，旨在以对抗方式联合优化自我改进策略和生成策略。这个优化问题的解决方案与训练任务无关，因此它对其变化具有鲁棒性。然后，我们展示了该目标可以以非对抗性离线损失的形式重新表达，可以使用标准监督优化技术进行大规模优化，而无需任何奖励模型和在线推理。我们展示了 SRPO 在 AI 胜率 (WR) 方面相对于人类 (GOLD) 完成的有效性。特别是，当在 OOD XSUM 数据集上评估 SRPO 时，它在 5 次自我修订后以 15% 的明显优势超越了著名的 DPO，实现了 90% 的 WR。]]></description>
      <guid>https://arxiv.org/abs/2406.01660</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络并不总是过度平滑</title>
      <link>https://arxiv.org/abs/2406.02269</link>
      <description><![CDATA[arXiv:2406.02269v1 公告类型：新
摘要：图神经网络 (GNN) 已成为处理应用中关系数据的强大工具。然而，GNN 存在过度平滑的问题，即所有节点的特征在各层上呈指数级收敛到同一向量，从而阻碍了深度 GNN 的设计。在这项工作中，我们利用图卷积网络 (GCN) 中无限多个隐藏特征极限下的高斯过程 (GP) 等价性来研究过度平滑。通过从传统的深度神经网络 (DNN) 中推广方法，我们可以用 GP 来描述深度 GCN 输出层的特征分布：正如预期的那样，我们发现文献中的典型参数选择会导致过度平滑。然而，该理论使我们能够识别一个新的非过平滑阶段：如果网络的初始权重具有足够大的方差，则 GCN 不会过度平滑，并且即使在较大深度下节点特征仍然具有信息量。我们通过在有限大小 GCN 的输出上训练线性分类器来证明此预测的有效性。此外，利用 GCN GP 的线性化，我们将信息传播深度的概念从 DNN 推广到 GCN。这种传播深度在过平滑和非过平滑阶段之间的过渡处发生发散。我们测试了我们方法的预测并发现与有限大小 GCN 具有很好的一致性。在接近非过平滑阶段的过渡处初始化 GCN，我们获得既深又富于表现力的网络。]]></description>
      <guid>https://arxiv.org/abs/2406.02269</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>使用新型 Arctan Pinball 损失函数进行 XGBoost 复合分位数回归</title>
      <link>https://arxiv.org/abs/2406.02293</link>
      <description><![CDATA[arXiv:2406.02293v1 公告类型：新
摘要：本文探讨了 XGBoost 在复合分位数回归中的应用。XGBoost 是一种非常流行的模型，以其灵活性、效率和处理缺失数据的能力而闻名。优化使用了损失函数的二阶近似，使得使用二阶导数为零或消失的损失函数变得复杂。分位数回归（一种在单靠点估计不足时获得条件分位数的流行方法）不幸地使用了这样的损失函数，即弹球损失。现有的解决方法通常效率低下，并且可能导致严重的分位数交叉。在本文中，我们提出了一种平滑的弹球损失近似，即反正切弹球损失，它是根据 XGBoost 的需求量身定制的。具体而言，与其他平滑近似相反，反正切弹球损失具有相对较大的二阶导数，这使得它更适合在二阶近似中使用。使用该损失函数可以同时预测多个分位数，从而提高效率并减少分位数交叉。]]></description>
      <guid>https://arxiv.org/abs/2406.02293</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>关于从时间聚合的 I.I.D. 数据中恢复因果关系的可恢复性</title>
      <link>https://arxiv.org/abs/2406.02191</link>
      <description><![CDATA[arXiv:2406.02191v1 公告类型：新
摘要：我们考虑时间聚合对一般情况下瞬时（非时间）因果发现的影响。这是由于观察到真实的因果时间滞后通常比观察间隔短得多。这种差异导致高度聚合，导致时间延迟因果关系消失，并出现瞬时依赖性。虽然我们期望这种瞬时依赖性在某种意义上与真实的因果关系具有一致性，以使发现结果有意义，但我们需要什么类型的一致性以及何时满足这种一致性仍不清楚。我们提出了功能一致性和条件独立一致性，以形式化的方式分别对应基于功能因果模型的方法和基于条件独立性的方法，并提供了这些一致性成立的条件。我们从理论和实验上表明，因果发现结果可能会因聚合而严重扭曲，尤其是在完全非线性的情况下，我们还发现，如果我们具有部分线性或适当的先验，仍然可以从聚合数据中恢复因果关系。我们的研究结果表明，社区在解释此类数据的因果发现结果时应采取谨慎细致的方法，并说明聚合为何以及何时会扭曲因果发现方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.02191</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>具有潜在混杂因素的 LiNGAM 模型中的因果效应识别</title>
      <link>https://arxiv.org/abs/2406.02049</link>
      <description><![CDATA[arXiv:2406.02049v1 公告类型：新
摘要：我们研究具有潜在变量的线性非高斯无环模型 (LiNGAM) 中因果效应的一般可识别性。我们在两种主要情况下考虑该问题：因果图是先验已知的，以及因果图未知的情况。在这两种情况下，我们都提供了可识别的直接或总因果效应的完整图形特征。此外，我们提出了有效的算法来验证图形条件。最后，我们提出了一种重构独立成分分析 (RICA) 算法的改编版，该算法根据因果图从观测数据中估计因果效应。实验结果表明，该方法在估计因果效应方面是有效的。]]></description>
      <guid>https://arxiv.org/abs/2406.02049</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>在线学习和信息指数：批次大小的重要性以及时间/复杂性权衡</title>
      <link>https://arxiv.org/abs/2406.02157</link>
      <description><![CDATA[arXiv:2406.02157v1 公告类型：新 
摘要：我们研究了批量大小 $n_b$ 对使用单次随机梯度下降 (SGD) 在各向同性协变量的多指标目标函数上训练双层神经网络的迭代时间 $T$ 的影响。我们将最小化迭代时间的最佳批量大小表征为目标硬度的函数，以信息指数为特征。我们表明，使用大批量 $n_b \lesssim d^{\frac{\ell}{2}}$ 执行梯度更新可以在不改变总样本复杂度的情况下最小化训练时间，其中 $\ell$ 是要学习的目标的信息指数 \citep{arous2021online}，$d$ 是输入维度。然而，大于 $n_b \gg d^{\frac{\ell}{2}}$ 的批量大小不利于提高 SGD 的时间复杂度。我们通过不同的训练协议 \textit{相关损失 SGD} 克服了这一根本限制，该协议抑制了损失函数中的自相关项。我们表明，可以通过低维常微分方程 (ODE) 系统跟踪训练进度。最后，我们通过数值实验验证了我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2406.02157</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>扩散增强树</title>
      <link>https://arxiv.org/abs/2406.01813</link>
      <description><![CDATA[arXiv:2406.01813v1 公告类型：新
摘要：结合去噪扩散概率模型和梯度提升的优点，引入了扩散提升范式来解决监督学习问题。我们开发了扩散提升树 (DBT)，它既可以看作是一种由决策树参数化的新型去噪扩散生成模型（每个扩散时间步长一棵树），也可以看作是一种新的提升算法，它将弱学习者组合成条件分布的强学习者，而无需对其密度形式做出明确的参数假设。我们通过实验证明了 DBT 相对于基于深度神经网络的扩散模型的优势以及 DBT 在现实世界回归任务上的能力，并提出了 DBT 的商业应用（欺诈检测），用于对具有延迟学习能力的表格数据进行分类。]]></description>
      <guid>https://arxiv.org/abs/2406.01813</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>正交因果校准</title>
      <link>https://arxiv.org/abs/2406.01933</link>
      <description><![CDATA[arXiv:2406.01933v1 公告类型：新
摘要：因果参数的估计（例如条件平均治疗效果和条件分位数治疗效果）在现实世界的决策中起着重要作用。鉴于这种重要性，应确保这些估计量经过校准。虽然有大量关于校准非因果参数估计量的文献，但很少有方法可以校准因果参数的估计量，或者更一般地说，校准涉及干扰参数的数量估计量。
在这项工作中，我们提供了一个校准涉及干扰估计的预测器的通用框架。我们考虑一个针对任意的、干扰相关的损失 $\ell$ 定义的校准概念，根据该概念，如果估计量 $\theta$ 的预测不能在任何水平集上改变以减少损失，则我们说该估计量是经过校准的。我们使用一种称为 Neyman 正交性的概念，证明了任何因果参数估计 $\theta$ 相对于任何损失 $\ell$ 的校准误差的通用上限。我们的界限涉及两个解耦项 - 一个测量估计未知干扰参数的误差，另一个表示在学习到的干扰估计为真的假设世界中的校准误差。我们使用我们的界限来分析两种因果校准样本分割算法的收敛性。一种适用于普遍正交损失函数的算法将数据转换为广义伪结果并应用现成的校准程序。另一种适用于条件正交损失函数的算法将经典的均匀质量分级算法扩展为包括干扰估计。我们的结果非常普遍，表明基本上任何现有的校准算法都可以在因果设置中使用，额外的损失仅来自干扰估计中的错误。]]></description>
      <guid>https://arxiv.org/abs/2406.01933</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>期望传播中的无畏随机性</title>
      <link>https://arxiv.org/abs/2406.01801</link>
      <description><![CDATA[arXiv:2406.01801v1 公告类型：新
摘要：期望传播 (EP) 是一类用于在概率模型中执行近似推理的算法。EP 的更新涉及矩的评估——某些函数的期望——可以从蒙特卡洛 (MC) 样本中估计。然而，当单纯执行时，更新对 MC 噪声并不稳健，并且各种先前的工作都试图以不同的方式解决这个问题。在这项工作中，我们提供了一个关于 EP 的矩匹配更新的新视角；即，它们执行基于自然梯度的变分目标优化。我们利用这一见解激发了两种新的 EP 变体，其更新特别适合 MC 估计；它们保持稳定，并且在仅使用单个样本进行估计时样本效率最高。这些新变体结合了其前身的优点并解决了关键弱点。尤其是，它们更容易调整，提供更好的速度-准确度权衡，并且不依赖于使用去偏估计器。我们展示了它们在各种概率推理任务中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.01801</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>使用参数化神经网络重建跳跃扩散过程的有效 Wasserstein 距离方法</title>
      <link>https://arxiv.org/abs/2406.01653</link>
      <description><![CDATA[arXiv:2406.01653v1 公告类型：新
摘要：我们分析了与两个多维跳跃扩散过程相关的两个概率分布之间的 Wasserstein 距离（$W$-距离）。具体来说，我们分析了一个时间解耦的平方 $W_2$-距离，它提供了与两个跳跃扩散过程之间的漂移、扩散和跳跃幅度函数差异相关的上限和下限。然后，我们提出了一种时间解耦的平方 $W_2$-距离方法，用于使用参数化神经网络从数据中有效地重建未知的跳跃扩散过程。我们进一步表明，通过利用跳跃扩散过程的漂移函数的先验信息可以提高其性能。我们提出的重建方法的有效性已在多个示例和应用中得到证明。]]></description>
      <guid>https://arxiv.org/abs/2406.01653</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>耦合枝树上的隐马尔可夫模型的有效解决方案</title>
      <link>https://arxiv.org/abs/2406.01663</link>
      <description><![CDATA[arXiv:2406.01663v1 公告类型：新
摘要：隐马尔可夫模型 (HMM) 是用于对顺序数据进行建模的强大工具，其中底层状态以随机方式发展并且仅可间接观察到。传统的 HMM 方法对于线性序列已经很成熟，并且已经扩展到其他结构，例如树。在本文中，我们扩展了树上的 HMM 框架，以解决数据的树状结构包含耦合分支的情况——这是生物系统中的常见特征，其中同一谱系内的实体表现出依赖特征。我们开发了一种动态规划算法，可以有效地解决具有耦合分支的基于树的 HMM 的似然、解码和参数学习问题。我们的方法随着状态和节点的数量呈多项式扩展，使其在计算上适用于广泛的应用，并且不会受到下溢问题的影响。我们通过将算法应用于模拟数据来演示我们的算法，并提出自洽性检查以验证用于推理的模型的假设。这项工作不仅推进了对树上的 HMM 的理论理解，而且还为分析不能忽略分支之间依赖关系的复杂生物数据提供了实用工具。]]></description>
      <guid>https://arxiv.org/abs/2406.01663</guid>
      <pubDate>Thu, 06 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    </channel>
</rss>