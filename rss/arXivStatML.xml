<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 16 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>可复制的均匀性测试</title>
      <link>https://arxiv.org/abs/2410.10892</link>
      <description><![CDATA[arXiv:2410.10892v1 公告类型：新
摘要：均匀性测试可以说是最基本的分布测试问题之一。给定对 $[n]$ 上未知分布 $\mathbf{p}$ 的样本访问，必须确定 $\mathbf{p}$ 是均匀的还是 $\varepsilon$ 远离均匀（总变差距离）。长期的工作确定均匀性测试的样本复杂度为 $\Theta(\sqrt{n}\varepsilon^{-2})$。但是，当输入分布既不均匀也不远离均匀时，已知算法可能具有高度不可复制的行为。因此，如果将这些算法应用于科学研究，它们可能会导致矛盾的结果，从而削弱公众对科学的信任。
在这项工作中，我们在算法可重复性框架下重新审视均匀性测试 [STOC &#39;22]，要求算法在任意分布下均可复制。虽然可复制性通常会在样本复杂度方面产生 $\rho^{-2}$ 因子开销，但我们仅使用 $\tilde{O}(\sqrt{n} \varepsilon^{-2} \rho^{-1})$ 个样本就获得了可复制的均匀性测试器。据我们所知，这是第一个对 $\rho$ 具有（几乎）线性依赖性的可复制学习算法。
最后，我们考虑一类“对称”算法 [FOCS &#39;00]，其输出在域 $[n]$ 的重新标记下不变，其中包括所有现有的均匀性测试器（包括我们的）。对于这种自然的算法类，我们证明了可复制均匀性测试的样本复杂度下限几乎匹配。]]></description>
      <guid>https://arxiv.org/abs/2410.10892</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>COME：通过保守最小化熵实现测试时间适应</title>
      <link>https://arxiv.org/abs/2410.10894</link>
      <description><![CDATA[arXiv:2410.10894v1 公告类型：新
摘要：机器学习模型必须不断地自我调整以适应开放世界中的新数据分布。作为主要原则，熵最小化 (EM) 已被证明是现有测试时间自适应 (TTA) 方法中简单而有效的基石。但不幸的是，它的致命限制（即过度自信）往往会导致模型崩溃。针对这个问题，我们提出了保守最小化熵 (COME)，它是传统 EM 的简单替代品，可以优雅地解决这一限制。本质上，COME 通过在 TTA 期间对模型预测表征狄利克雷先验分布来明确地模拟不确定性。通过这样做，COME 自然地规范化模型以支持对不可靠样本的保守信心。从理论上讲，我们提供了初步分析，揭示了 COME 通过在熵上引入数据自适应下限来增强优化稳定性的能力。经验上，我们的方法在常用基准上实现了最先进的性能，在标准、终身和开放世界 TTA 等各种设置下的分类准确度和不确定性估计方面都有显着改善，即准确度提高了 $34.5\%$，假阳性率提高了 $15.1\%$。]]></description>
      <guid>https://arxiv.org/abs/2410.10894</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于评估非结构化数据集上的神经互信息估计器的基准套件</title>
      <link>https://arxiv.org/abs/2410.10924</link>
      <description><![CDATA[arXiv:2410.10924v1 公告类型：新
摘要：互信息 (MI) 是量化两个随机变量之间依赖关系的基本指标。当我们只能访问样本而不能访问底层分布函数时，我们可以使用基于样本的估计量来评估 MI。然而，对此类 MI 估计量的评估几乎总是依赖于包括高斯多元变量在内的分析数据集。此类数据集允许对真实 MI 值进行分析计算，但它们的局限性在于它们不能反映真实世界数据集的复杂性。本研究介绍了一套全面的基准测试套件，用于评估非结构化数据集上的神经 MI 估计量，特别关注图像和文本。通过利用同类采样进行正配对并引入二元对称通道技巧，我们表明我们可以准确地操纵真实世界数据集的真实 MI 值。使用基准测试套件，我们研究了七个具有挑战性的场景，揭示了神经 MI 估计量对非结构化数据集的可靠性。]]></description>
      <guid>https://arxiv.org/abs/2410.10924</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>位置尺度家族中的变分推断：均值和相关矩阵的精确恢复</title>
      <link>https://arxiv.org/abs/2410.11067</link>
      <description><![CDATA[arXiv:2410.11067v1 公告类型：新
摘要：给定一个难处理的目标密度 $p$，变分推理 (VI) 试图从易处理的族 $Q$ 中找到最佳近似值 $q$。这通常通过最小化独有的 Kullback-Leibler 散度 $\text{KL}(q||p)$ 来实现。实际上，$Q$ 不足以包含 $p$，并且即使它是 $\text{KL}(q||p)$ 的唯一全局最小化器，近似值也会被错误指定。在本文中，我们分析了当 $p$ 表现出某些对称性并且 $Q$ 是共享这些对称性的位置尺度族时，VI 对这些错误指定的稳健性。我们不仅证明了在温和的规律性条件下，而且在严重的错误指定面前，也证明了 VI 的强保证。即，我们表明：(i) 当 $p$ 表现出 \textit{偶数} 对称性时，VI 会恢复 $p$ 的平均值；(ii) 当 ~$p$ 还表现出 \textit{椭圆} 对称性时，VI 会恢复 $p$ 的相关矩阵。即使 $q$ 被分解而 $p$ 没有被分解，这些保证对于平均值仍然成立；即使 ~$q$ 和 ~$p$ 在其尾部表现不同，这些保证对于相关矩阵仍然成立。我们分析了贝叶斯推理的各种机制，其中这些对称性是有用的理想化，我们还通过实验研究了 VI 在没有对称性的情况下的表现。]]></description>
      <guid>https://arxiv.org/abs/2410.11067</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大规模高维信号的两阶段联邦学习方法进行工业预测</title>
      <link>https://arxiv.org/abs/2410.11101</link>
      <description><![CDATA[arXiv:2410.11101v1 公告类型：新
摘要：工业预测旨在开发数据驱动的方法，利用来自资产的高维退化信号来预测其故障时间。这些模型的成功在很大程度上取决于大量历史数据的可用性。然而，在实践中，单个组织往往缺乏足够的数据来独立训练可靠的预测模型，而隐私问题阻碍了组织之间共享数据以进行协作模型训练。为了克服这些挑战，本文提出了一种基于统计学习的联合模型，使多个组织能够联合训练预测模型，同时保持其数据的本地化和安全。所提出的方法涉及两个关键阶段：联合降维和联合（对数）位置尺度回归。在第一阶段，我们开发了一种用于多元函数主成分分析的联合随机奇异值分解算法，该算法在保持数据隐私的同时有效地降低了退化信号的维数。第二阶段提出了一种用于（对数）位置尺度回归的联合参数估计算法，使各组织能够协作估计故障时间分布而无需共享原始数据。所提出的方法通过使用统计学习技术解决了现有联合预测方法的局限性，这些技术在较小的数据集上表现良好并提供全面的故障时间分布。使用模拟数据和来自 NASA 存储库的数据集验证了所提出模型的有效性和实用性。]]></description>
      <guid>https://arxiv.org/abs/2410.11101</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有依赖数据的深度神经网络的统计特性</title>
      <link>https://arxiv.org/abs/2410.11113</link>
      <description><![CDATA[arXiv:2410.11113v1 公告类型：新
摘要：本文建立了依赖数据下的深度神经网络 (DNN) 估计量的统计特性。给出了两个可直接应用于 DNN 估计量的非参数筛估计量的一般结果。第一个建立了非平稳数据下概率收敛的速度。第二个提供了平稳 $\beta$ 混合数据下 $\mathcal{L}^{2}$ 误差的非渐近概率界限。我将这些结果应用于回归和分类环境中的 DNN 估计器，仅施加标准 H_older 平滑度假设。然后使用这些结果来演示如何在第一阶段 DNN 估计无限维参数后对部分线性回归模型的有限维参数进行渐近推理。所考虑的 DNN 架构在应用中很常见，具有完全连接的前馈网络，具有任何连续分段线性激活函数、无界权重以及随样本大小增长的宽度和深度。所提供的框架还为研究其他 DNN 架构和时间序列应用提供了潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.11113</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>专家混合中的二次门控函数：统计洞察</title>
      <link>https://arxiv.org/abs/2410.11222</link>
      <description><![CDATA[arXiv:2410.11222v1 公告类型：新
摘要：混合专家 (MoE) 模型在扩展模型容量的同时保持计算效率方面非常有效，其中门控网络或路由器通过将输入引导到适当的专家而发挥着核心作用。在本文中，我们在 MoE 框架和注意力机制之间建立了一种新颖的联系，展示了二次门控如何成为一种更具表现力和效率的替代方案。受此见解的启发，我们探索了二次门控在 MoE 模型中的实现，确定了自注意力机制与二次门控之间的联系。我们对二次 softmax 门控 MoE 框架进行了全面的理论分析，显示专家和参数估计的样本效率有所提高。我们的分析为二次门控和专家函数的最佳设计提供了关键见解，进一步阐明了广泛使用的注意力机制背后的原理。通过广泛的评估，我们证明了二次门控 MoE 优于传统的线性门控 MoE。此外，我们的理论见解指导了一种新型注意力机制的开发，并通过大量实验对其进行了验证。结果表明，该机制在各种任务上的表现均优于传统模型。]]></description>
      <guid>https://arxiv.org/abs/2410.11222</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>非线性表示学习的保证：非同一协变量、相关数据、更少的样本</title>
      <link>https://arxiv.org/abs/2410.11227</link>
      <description><![CDATA[arXiv:2410.11227v1 公告类型：新
摘要：现代机器学习广泛适用性背后的驱动力是能够从许多来源中提取有意义的特征。然而，许多实际领域涉及的数据在来源之间分布不均匀，并且在来源内具有统计相关性，这违反了现有理论研究中的重要假设。为了解决这些问题，我们建立了统计保证，以从多个数据源学习一般的$\textit{nonlinear}$表示，这些数据源允许不同的输入分布和可能的依赖数据。具体来说，我们研究从函数类$\mathcal F \times \mathcal G$学习$T+1$函数$f_\star^{(t)} \circ g_\star$的样本复杂度，其中$f_\star^{(t)}$是任务特定的线性函数，$g_\star$是共享的非线性表示。使用来自每个 $T$ 个源任务的 $N$ 个样本来估计表示 $\hat g$，并使用通过 $\hat g$ 的目标任务的 $N&#39;$ 个样本来拟合微调函数 $\hat f^{(0)}$。我们证明，当 $N \gtrsim C_{\mathrm{dep}} (\mathrm{dim}(\mathcal F) + \mathrm{C}(\mathcal G)/T)$ 时，目标任务上的 $\hat f^{(0)} \circ \hat g$ 的超额风险随着 $\nu_{\mathrm{div}} \big(\frac{\mathrm{dim}(\mathcal F)}{N&#39;} + \frac{\mathrm{C}(\mathcal G)}{N T} \big)$ 而衰减，其中 $C_{\mathrm{dep}}$ 表示数据依赖性的影响，$\nu_{\mathrm{div}}$ 表示源任务和目标任务之间 $\textit{任务多样性}$ 的（可估计）度量，$\mathrm C(\mathcal G)$ 表示表示的复杂性类 $\mathcal G$。具体来说，我们的分析表明：随着任务数量 $T$ 的增加，样本要求和风险界限都会收敛到 $r$ 维回归的水平，就好像 $g_\star$ 已经给定一样，依赖性的影响只进入样本要求，而风险界限与 iid 设置相匹配。]]></description>
      <guid>https://arxiv.org/abs/2410.11227</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于 Transformer 的秩相关泛化误差界限</title>
      <link>https://arxiv.org/abs/2410.11500</link>
      <description><![CDATA[arXiv:2410.11500v1 公告类型：新
摘要：在本文中，我们引入了线性函数类的各种覆盖数界限，每个界限都受到输入和矩阵范数的不同约束。这些界限取决于每类矩阵的秩。然后，我们应用这些界限来推导单层变压器的泛化误差。我们的结果改进了文献中几个现有的泛化界限，并且与输入序列长度无关，突出了在变压器设计中使用低秩矩阵的优势。更具体地说，我们实现的泛化误差界限衰减为 $O(1/\sqrt{n})$，其中 $n$ 是样本长度，这改进了研究文献中现有的 $O((\log n)/(\sqrt{n}))$ 阶的结果。它还衰减为 $O(\log r_w)$，其中 $r_w$ 是查询和键矩阵组合的秩。]]></description>
      <guid>https://arxiv.org/abs/2410.11500</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应参数选择的随机对角线估计</title>
      <link>https://arxiv.org/abs/2410.11613</link>
      <description><![CDATA[arXiv:2410.11613v1 公告类型：新
摘要：本文研究大型或隐式矩阵的对角线估计，旨在开发一种结合自适应参数选择的新型高效随机算法。我们探讨了不同特征值分布对对角线估计的影响，并分析了在随机对角线估计器中引入投影方法和自适应参数优化的必要性。基于此分析，我们推导出满足给定概率误差界限所需的随机查询向量数量的下限，这构成了我们的自适应随机对角线估计算法的基础。最后，数值实验证明了所提出的估计器对各种矩阵类型的有效性，与其他现有随机对角线估计方法相比，它表现出了更高的效率和稳定性。]]></description>
      <guid>https://arxiv.org/abs/2410.11613</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型的基于零样本模型的强化学习</title>
      <link>https://arxiv.org/abs/2410.11711</link>
      <description><![CDATA[arXiv:2410.11711v1 公告类型：新
摘要：大型语言模型 (LLM) 的新兴零样本能力已使其应用范围远远超出自然语言处理任务。在强化学习中，虽然 LLM 已广泛应用于基于文本的环境，但它们与连续状态空间的集成仍未得到充分研究。在本文中，我们研究了如何利用预先训练的 LLM 来预测连续马尔可夫决策过程的动态。我们认为处理多变量数据和合并控制信号是限制 LLM 在此设置中部署潜力的关键挑战，并提出了解缠结上下文学习 (DICL) 来解决这些问题。我们在两种强化学习环境中提出了概念验证应用：基于模型的策略评估和数据增强的离策略强化学习，并得到了对所提出方法的理论分析的支持。我们的实验进一步表明，我们的方法可以产生经过良好校准的不确定性估计。我们在 https://github.com/abenechehab/dicl 上发布了代码。]]></description>
      <guid>https://arxiv.org/abs/2410.11711</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>局部分布的 l_inf 近似</title>
      <link>https://arxiv.org/abs/2410.11771</link>
      <description><![CDATA[arXiv:2410.11771v1 公告类型：新
摘要：空间模型中的分布通常表现出局部特征。直观地说，这种局部性意味着低固有维数，可以利用它来有效地近似和计算复杂的分布。然而，现有的近似理论主要考虑联合分布，这并不能保证边际误差很小。在这项工作中，我们为近似分布的边际建立了一个与维度无关的误差界限。这个$\ell_\infty$近似误差是使用 Stein 的方法获得的，我们提出了一个$\delta$局部性条件来量化分布的局部化程度。我们还展示了如何从表征分布局部性的不同条件中得出$\delta$局部性。我们的$\ell_\infty$界限促使现有近似方法的局部化尊重局部性。作为示例，我们展示了如何使用局部似然信息子空间方法和局部分数匹配，这不仅避免了近似误差中的维度依赖性，而且由于基于局部结构的局部和并行实现而显著降低了计算成本。]]></description>
      <guid>https://arxiv.org/abs/2410.11771</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对比扩散进行贝叶斯实验设计</title>
      <link>https://arxiv.org/abs/2410.11826</link>
      <description><![CDATA[arXiv:2410.11826v1 公告类型：新
摘要：贝叶斯最优实验设计 (BOED) 是一种强大的工具，可以降低运行一系列实验的成本。当基于预期信息增益 (EIG) 时，设计优化对应于先验和后验分布之间某些难以处理的预期 {\it 对比度} 的最大化。由于 BOED 固有的计算复杂性，将这种最大化扩展到高维和复杂设置一直是一个问题。在这项工作中，我们引入了具有成本效益采样特性的 {\it 预期后验} 分布，并通过新的 EIG 梯度表达式提供对 EIG 对比度最大化的可处理访问。基于扩散的采样器用于计算预期后验的动态，并利用双层优化的思想来推导有效的联合采样优化循环，而无需诉诸 EIG 的下限近似。由此产生的效率增益允许将 BOED 扩展到经过充分测试的扩散模型生成能力。通过将生成模型纳入 BOED 框架，我们扩大了其范围及其在以前不切实际的场景中的使用。数值实验和与最先进方法的比较显示了该方法的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.11826</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AlphaPruning：利用重尾自正则化理论改进大型语言模型的分层剪枝</title>
      <link>https://arxiv.org/abs/2410.10912</link>
      <description><![CDATA[arXiv:2410.10912v1 公告类型：交叉 
摘要：最近对大型语言模型 (LLM) 进行修剪的研究表明，人们可以在不影响性能的情况下消除大量参数，这使得修剪成为减少 LLM 模型大小的一种有前途的策略。现有的 LLM 修剪策略通常在各层之间分配统一的修剪率，从而限制了整体修剪能力；最近对 LLM 分层修剪的研究通常基于启发式方法，这很容易导致性能不佳。在本文中，我们利用重尾自正则化 (HT-SR) 理论，特别是权重矩阵的经验谱密度 (ESD) 的形状，为 LLM 设计改进的分层修剪率。我们的分析揭示了 LLM 不同层的训练程度以及可修剪程度存在很大差异。基于此，我们提出了 AlphaPruning，它使用形状度量以更符合理论原则的方式分配逐层稀疏度比率。AlphaPruning 可以与多种现有的 LLM 修剪方法结合使用。我们的实证结果表明，AlphaPruning 将 LLaMA-7B 的稀疏度修剪至 80%，同时保持合理的困惑度，这在 LLM 文献中尚属首次。我们已在 https://github.com/haiquanlu/AlphaPruning 上开源了我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2410.10912</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 意味着什么？来自 Hessian 理论分析的见解</title>
      <link>https://arxiv.org/abs/2410.10986</link>
      <description><![CDATA[arXiv:2410.10986v1 公告类型：交叉 
摘要：Transformer 架构无疑彻底改变了深度学习，超越了多层感知器 (MLP) 和卷积神经网络 (CNN) 等经典架构。从本质上讲，注意力模块在形式和功能上与深度学习中的大多数其他架构组件不同 - 与 MLP/CNN 相比，Transformer 通常伴随着自适应优化器、层规范化、学习率预热等。这些外在表现背后的根本原因以及控制它们的确切机制仍然知之甚少。在这项工作中，我们通过提供对 Transformer 与其他架构的区别的基本理解来弥合这一差距 - 基于对 (loss) Hessian 的理论比较。具体而言，对于单个自注意力层，(a) 我们首先完全推导 Transformer 的 Hessian 并将其表示为矩阵导数； (b) 然后我们根据数据、权重和注意力矩依赖性对其进行表征；(c) 在此过程中进一步强调了与经典网络的 Hessian 的重要结构差异。我们的结果表明，Transformer 中各种常见的架构和优化选择可以追溯到它们对数据和权重矩阵的高度非线性依赖，这些依赖在参数之间异质变化。最终，我们的研究结果提供了对 Transformer 独特的优化前景及其带来的挑战的更深入的理解。]]></description>
      <guid>https://arxiv.org/abs/2410.10986</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>