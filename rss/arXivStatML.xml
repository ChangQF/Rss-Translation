<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 27 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>重新洗牌重采样分割可以提高超参数优化的泛化能力</title>
      <link>https://arxiv.org/abs/2405.15393</link>
      <description><![CDATA[arXiv:2405.15393v1 公告类型：新 
摘要：超参数优化对于获得机器学习模型的最佳性能至关重要。标准协议使用泛化误差的重采样估计来评估各种超参数配置，以指导优化并选择最终的超参数配置。在没有太多证据的情况下，通常建议使用配对重采样分割，即固定的训练验证分割或固定的交叉验证方案。令人惊讶的是，我们表明，重新调整每个配置的分割通常可以提高最终模型对未见数据的泛化性能。我们的理论分析解释了重新洗牌如何影响验证损失表面的渐近行为，并提供了限制机制中预期后悔的界限。这种界限将重新洗牌的潜在好处与底层优化问题的信号和噪声特征联系起来。我们在受控模拟研究中证实了我们的理论结果，并在大规模、现实的超参数优化实验中证明了重组的实际用途。虽然重新洗牌带来的测试性能与使用固定分割相比具有竞争力，但它极大地改善了单个训练验证保留协议的结果，并且通常可以使保留与标准 CV 竞争，同时计算成本更低。]]></description>
      <guid>https://arxiv.org/abs/2405.15393</guid>
      <pubDate>Mon, 27 May 2024 06:20:23 GMT</pubDate>
    </item>
    <item>
      <title>有向无环图上的协调多邻域学习</title>
      <link>https://arxiv.org/abs/2405.15358</link>
      <description><![CDATA[arXiv:2405.15358v1 公告类型：新 
摘要：学习因果有向无环图（DAG）的结构在机器学习和人工智能的许多领域都很有用，具有广泛的应用。然而，在高维环境中，如果没有强有力且通常具有限制性的假设，要获得良好的经验和理论结果是具有挑战性的。此外，网络中声称包含的所有变量是否都是可观察的也是值得怀疑的。因此，将考虑范围限制在变量的子集上以获得相关且可靠的推论是有意义的。事实上，各个学科的研究人员通常可以选择网络中的一组目标节点来进行因果发现。本文开发了一种新的基于约束的方法，用于估计多个用户指定的目标节点周围的局部结构，从而实现邻域之间结构学习的协调。我们的方法无需学习整个 DAG 结构即可促进因果发现。我们为我们的算法建立了相对于真实图中目标节点的局部邻域结构的一致性结果。合成数据和真实世界数据的实验结果表明，我们的算法在学习邻域结构方面更加准确，并且比估计整个 DAG 的标准方法的计算成本要少得多。可以通过 https://github.com/stephenvsmith/CML 访问实现我们方法的 R 包。]]></description>
      <guid>https://arxiv.org/abs/2405.15358</guid>
      <pubDate>Mon, 27 May 2024 06:20:22 GMT</pubDate>
    </item>
    <item>
      <title>紧凑支撑上的对数凹采样：多功能近端框架</title>
      <link>https://arxiv.org/abs/2405.15379</link>
      <description><![CDATA[arXiv:2405.15379v1 公告类型：新 
摘要：在本文中，我们探讨了从凸支撑和紧支撑上定义的强对数凹分布中进行采样。我们提出了一个通用的近端框架，涉及投影到约束集上，该框架高度灵活并支持各种投影选项。具体来说，我们考虑欧几里得投影和规范投影的情况，后者具有使用成员资格预言机有效执行的优点。该框架可以与多种采样方法无缝集成。我们的分析重点是约束采样背景下的 Langevin 型采样算法。我们提供了 W1 和 W2 误差的非渐近上限，对这些方法在约束采样中的性能进行了详细比较。]]></description>
      <guid>https://arxiv.org/abs/2405.15379</guid>
      <pubDate>Mon, 27 May 2024 06:20:22 GMT</pubDate>
    </item>
    <item>
      <title>基于组的 SLOPE 模型的强大筛选规则</title>
      <link>https://arxiv.org/abs/2405.15357</link>
      <description><![CDATA[arXiv:2405.15357v1 公告类型：新 
摘要：调整惩罚回归模型中的正则化参数是一项昂贵的任务，需要沿着参数路径拟合多个模型。强大的筛选规则通过在拟合之前降低输入的维度来大大降低计算成本。我们为基于组的排序 L-One 惩罚估计 (SLOPE) 模型开发了强大的筛选规则：组 SLOPE 和稀疏组 SLOPE。制定的规则适用于更广泛的基于群体的 OWL 模型系列，包括 OSCAR。我们对合成数据和真实数据的实验表明，筛选规则显着加速了拟合过程。筛选规则使群 SLOPE 和稀疏群 SLOPE 可以应用于高维数据集，特别是遗传学中遇到的数据集。]]></description>
      <guid>https://arxiv.org/abs/2405.15357</guid>
      <pubDate>Mon, 27 May 2024 06:20:21 GMT</pubDate>
    </item>
    <item>
      <title>软修正下广义贝叶斯规则指导的半监督学习</title>
      <link>https://arxiv.org/abs/2405.15294</link>
      <description><![CDATA[arXiv:2405.15294v1 公告类型：新 
摘要：我们对带有软修正的 Gamma-Maximin 方法进行了理论和计算研究，该方法最近被提出作为半监督学习中伪标签选择（PLS）的鲁棒标准。与传统的 PLS 方法相反，我们使用先验的信任集（“广义贝叶斯”）来表示认知建模的不确定性。然后，通过 Gamma-Maximin 方法通过软修正来更新后者。我们最终选择伪标记数据，这些数据最有可能根据更新后的凭证集中最不利的分布。我们将寻找最佳伪标记数据的任务形式化。将软修正作为优化问题的 Gamma-Maximin 方法。然后，逻辑模型类的具体实现使我们能够将该方法的预测能力与竞争方法进行比较。据观察，采用软修正的 Gamma-Maximin 方法可以获得非常有希望的结果，特别是当标记数据的比例较低时。]]></description>
      <guid>https://arxiv.org/abs/2405.15294</guid>
      <pubDate>Mon, 27 May 2024 06:20:20 GMT</pubDate>
    </item>
    <item>
      <title>总变异距离的判别性估计：生成数据的保真度审核员</title>
      <link>https://arxiv.org/abs/2405.15337</link>
      <description><![CDATA[arXiv:2405.15337v1 公告类型：新 
摘要：随着生成人工智能的普及和生成数据（也称为合成数据）数量的增加，评估生成数据的保真度已成为一个关键问题。在本文中，我们提出了一种判别性方法来估计两个分布之间的总变异（TV）距离，作为生成数据保真度的有效度量。我们的方法定量地描述了对两个分布进行分类的贝叶斯风险与其电视距离之间的关系。因此，总变异距离的估计简化为贝叶斯风险的估计。特别是，本文建立了关于两个高斯分布之间电视距离估计误差收敛速度的理论结果。我们证明，通过在分类中选择特定的假设类别，可以实现估计电视距离的快速收敛速度。具体来说，电视距离的估计精度被证明本质上取决于两个高斯分布的分离度：当两个高斯分布距离较远时，估计误差较小。这种现象也通过大量模拟得到了实证验证。最后，我们应用这种判别性估计方法对使用 MNIST 数据集的合成图像数据的保真度进行排名。]]></description>
      <guid>https://arxiv.org/abs/2405.15337</guid>
      <pubDate>Mon, 27 May 2024 06:20:20 GMT</pubDate>
    </item>
    <item>
      <title>学习逆向因果执行预测中的分布图</title>
      <link>https://arxiv.org/abs/2405.15172</link>
      <description><![CDATA[arXiv:2405.15172v1 公告类型：新 
摘要：在众多预测场景中，预测模型会影响样本分布；例如，求职者通常会精心制作简历以通过筛选系统。这种分布的变化在社交计算领域尤其普遍，然而，从数据中学习这些变化的策略仍然非常有限。受微观经济模型的启发，该模型巧妙地描述了劳动力市场中代理人的行为，我们引入了一种新的方法来学习分配转移。我们的方法基于反向因果模型，其中预测模型仅通过一组有限的代理行为来引发分布变化。在此框架内，我们采用了代理行为的微观基础模型，并开发了一种统计上合理的方法来学习分布变化图，我们证明该方法可以有效地最小化执行预测风险。]]></description>
      <guid>https://arxiv.org/abs/2405.15172</guid>
      <pubDate>Mon, 27 May 2024 06:20:19 GMT</pubDate>
    </item>
    <item>
      <title>超参数化机制之外的神经网络的新颖内核模型和精确表示理论</title>
      <link>https://arxiv.org/abs/2405.15254</link>
      <description><![CDATA[arXiv:2405.15254v1 公告类型：新 
摘要：本文提出了两种神经网络模型及其训练，适用于任意宽度、深度和拓扑的神经网络，假设仅具有有限能量的神经激活；以及基于矩阵值核的神经网络的新颖表示理论。第一个模型是精确的（未近似的）和全局的，将神经网络转换为复制内核 Banach 空间 (RKBS) 中的元素；我们使用这个模型来提供 Rademacher 复杂性的严格界限。第二个模型是精确的和局部的，将由权重和偏差（即训练步骤）的有界变化引起的神经网络函数的变化铸造为局部本征神经核（LiNK）的再现核希尔伯特空间（RKHS） ）。该局部模型通过对网络适应的 Rademacher 复杂性的严格限制，提供了对模型适应的深入了解。我们还证明神经正切核（NTK）是 LiNK 核的一阶近似。最后，注意到 LiNK 由于技术原因没有提供表示理论，我们提出了一种精确新颖的表示理论，用于根据局部外在神经核 (LeNK) 进行非正则梯度下降的分层神经网络训练。该表示理论深入了解了高阶统计量在神经网络训练中的作用以及核进化在神经网络核模型中的影响。在整篇论文中，(a) 前馈 ReLU 网络和 (b) 残差网络 (ResNet) 均用作说明性示例。]]></description>
      <guid>https://arxiv.org/abs/2405.15254</guid>
      <pubDate>Mon, 27 May 2024 06:20:19 GMT</pubDate>
    </item>
    <item>
      <title>ProDAG：有向无环图的投影诱导变分推理</title>
      <link>https://arxiv.org/abs/2405.15167</link>
      <description><![CDATA[arXiv:2405.15167v1 公告类型：新 
摘要：有向无环图（DAG）学习是一个快速扩展的研究领域。尽管该领域在过去几年中取得了显着的进步，但从数据中学习单个（点估计）DAG 在统计和计算上仍然具有挑战性，更不用说提供不确定性量化了。我们的文章通过开发基于直接支持 DAG 空间的新颖分布的变分贝叶斯推理框架来解决量化图不确定性的艰巨任务。我们用来形成先验和变分后验的分布是通过投影运算得出的，其中任意连续分布被投影到稀疏加权非循环邻接矩阵（DAG 的矩阵表示）的空间上，概率质量精确为零。尽管该投影构成了一个组合优化问题，但它可以通过最近开发的将非循环性重新表述为连续约束的技术来大规模解决。我们凭经验证明，我们的方法 ProDAG 可以提供准确的推理，并且通常优于现有的最先进的替代方法。]]></description>
      <guid>https://arxiv.org/abs/2405.15167</guid>
      <pubDate>Mon, 27 May 2024 06:20:18 GMT</pubDate>
    </item>
    <item>
      <title>算法稳定性可测试吗？计算约束下的统一框架</title>
      <link>https://arxiv.org/abs/2405.15107</link>
      <description><![CDATA[arXiv:2405.15107v1 公告类型：新
摘要：算法稳定性是学习理论中的一个核心概念，它量化了算法对训练数据中微小变化的敏感度。如果学习算法满足某些稳定性属性，这将导致许多重要的下游影响，例如泛化、鲁棒性和可靠的预测推理。因此，验证特定算法的稳定性是一个重要且实际的问题。然而，最近的结果表明，在数据位于不可数无限空间（例如实值数据）的环境中，给定来自未知分布的有限数据，测试黑盒算法的稳定性是不可能的。在这项工作中，我们将这个问题扩展到检查更广泛的设置，其中数据可能位于任何空间 - 例如分类数据。我们开发了一个统一的框架来量化测试算法稳定性的难度，该框架确定在所有设置中，如果可用数据有限，那么穷举搜索基本上是证明算法稳定性的唯一普遍有效的机制。由于在实践中，任何稳定性测试自然都会受到计算限制，详尽的搜索是不可能的，所以这意味着我们测试黑盒算法稳定性属性的能力受到根本限制。]]></description>
      <guid>https://arxiv.org/abs/2405.15107</guid>
      <pubDate>Mon, 27 May 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>超越噪声：具有最佳邻域识别的内在维度估计</title>
      <link>https://arxiv.org/abs/2405.15132</link>
      <description><![CDATA[arXiv:2405.15132v1 公告类型：新
摘要：内在维度 (ID) 是无监督学习和特征选择中的一个关键概念，因为它是描述系统所需变量数量的下限。然而，在几乎任何现实世界数据集中，ID 都取决于分析数据的规模。通常在小规模下，ID 非常大，因为数据会受到测量误差的影响。在大规模下，由于包含数据的流形的曲率和拓扑，ID 也可能错误地大。在这项工作中，我们引入了一种自动协议来选择最佳点，即 ID 有意义且有用的正确尺度范围。该协议基于强制规定，对于小于正确尺度的距离，数据的密度是恒定的。由于要估计密度，必须知道 ID，因此该条件是自洽地施加的。我们通过对人工和真实世界数据集的基准测试证明了该程序的实用性和稳健性。]]></description>
      <guid>https://arxiv.org/abs/2405.15132</guid>
      <pubDate>Mon, 27 May 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>关键资源的认证库存控制</title>
      <link>https://arxiv.org/abs/2405.15105</link>
      <description><![CDATA[arXiv:2405.15105v1 公告类型：新 
摘要：库存控制受服务水平要求的影响，尽管需求未知，但仍必须保持足够的库存水平。我们提出了一种数据驱动的订单策略，可以在未知需求过程的最小假设下证明任何规定的服务水平。该政策利用任何在线学习方法和整体行动来实现这一目标。我们进一步提出了一种在有限样本中有效的推理方法。使用合成数据和真实数据说明了该方法的特性和理论保证。]]></description>
      <guid>https://arxiv.org/abs/2405.15105</guid>
      <pubDate>Mon, 27 May 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>自适应选择组的均衡覆盖的共形分类</title>
      <link>https://arxiv.org/abs/2405.15106</link>
      <description><![CDATA[arXiv:2405.15106v1 公告类型：新 
摘要：本文介绍了一种共形推理方法，通过根据自适应选择的特征生成具有有效覆盖范围的预测集来评估分类中的不确定性。这些特征经过精心选择，以反映潜在的模型限制或偏差。这有助于在效率（通过提供信息丰富的预测）和算法公平性（通过确保最敏感群体的均衡覆盖）之间找到实际的折衷方案。我们在模拟和真实数据集上证明了该方法的有效性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.15106</guid>
      <pubDate>Mon, 27 May 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>可证明有效的无限视野平均奖励线性 MDP 强化学习</title>
      <link>https://arxiv.org/abs/2405.15050</link>
      <description><![CDATA[arXiv:2405.15050v1 公告类型：新 
摘要：我们解决了为具有 $\widetilde{O}(\sqrt{T})$ 遗憾的无限范围平均奖励线性马尔可夫决策过程 (MDP) 设计计算高效算法的开放问题。之前使用 $\widetilde{O}(\sqrt{T})$ 的方法遗憾的是要么计算效率低下，要么需要对动力学进行强有力的假设，例如遍历性。在本文中，我们通过折扣设置来近似平均奖励设置，并表明运行基于乐观值迭代的算法来学习折扣设置可以在折扣时实现$\widetilde{O}(\sqrt{T})$后悔因子 $\gamma$ 已适当调整。近似方法的挑战是获得一个强烈依赖于有效范围 $1 / (1 - \gamma)$ 的遗憾界限。我们使用计算效率高的裁剪算子来限制乐观状态值函数估计的跨度，以在有效范围内实现尖锐的遗憾界限，这会导致 $\widetilde{O}(\sqrt{T})$ 遗憾。]]></description>
      <guid>https://arxiv.org/abs/2405.15050</guid>
      <pubDate>Mon, 27 May 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>计算最优神经缩放定律的 4+3 阶段</title>
      <link>https://arxiv.org/abs/2405.15074</link>
      <description><![CDATA[arXiv:2405.15074v1 公告类型：新 
摘要：我们考虑 Maloney、Roberts 和 Sully 提出的三参数可解神经标度模型。该模型具有三个参数：数据复杂性、目标复杂性和模型参数计数。我们使用这种神经缩放模型来导出有关计算有限、无限数据缩放定律制度的新预测。为了训练神经缩放模型，我们对均方损失运行一次随机梯度下降。我们得出了损失曲线的表示，该曲线适用于所有迭代计数，并随着模型参数计数的增加而提高准确性。然后，我们分析计算最优模型参数计数，并识别数据复杂性/目标复杂性相平面中的 4 个阶段（+3 个子阶段）。阶段边界由模型容量、优化器噪声和特征嵌入的相对重要性决定。我们还通过数学证明和广泛的数值证据推导了所有这些阶段的标度律指数，特别是计算作为浮点运算预算的函数的最佳模型参数计数。]]></description>
      <guid>https://arxiv.org/abs/2405.15074</guid>
      <pubDate>Mon, 27 May 2024 06:20:15 GMT</pubDate>
    </item>
    </channel>
</rss>