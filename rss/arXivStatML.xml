<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Tue, 05 Dec 2023 03:14:26 GMT</lastBuildDate>
    <item>
      <title>根据回归协方差矩阵的特征值分布，PLS 和 OLS 回归之间的关系。 （arXiv：2312.01379v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2312.01379</link>
      <description><![CDATA[偏最小二乘法 (PLS) 是一种降维技术
引入化学计量学领域并成功应用于许多其他领域
地区。 PLS 分量是通过最大化之间的协方差获得的
回归量和目标变量的线性组合。在这个
在工作中，我们重点关注其在标量回归问题中的应用。 PLS回归
在于找到最小二乘预测器，它是以下各项的线性组合
PLS 组件的子集。或者，可以制定 PLS 回归
作为限制于 Krylov 子空间的最小二乘问题。这相当于
采用公式来分析之间的距离
${\hat{\boldsymbol\beta}\;}_{\mathrm{PLS}}^{\scriptscriptstyle {(L)}}$，PLS
基于线性回归模型系数向量的估计器
$L$ PLS 分量，以及 $\hat{\boldsymbol \beta}_{\mathrm{OLS}}$，即
通过普通最小二乘法 (OLS) 获得，作为 $L$ 的函数。具体来说，
${\hat{\boldsymbol\beta}\;}_{\mathrm{PLS}}^{\scriptscriptstyle {(L)}}$ 是
上述 Krylov 子空间中最接近于的系数向量
$\hat{\boldsymbol \beta}_{\mathrm{OLS}}$ 以马氏距离表示
关于 OLS 估计的协方差矩阵。我们提供一个绑定
这个距离仅取决于特征值的分布
回归协方差矩阵。合成和现实世界的数值例子
数据用于说明之间的距离
${\hat{\boldsymbol\beta}\;}_{\mathrm{PLS}}^{\scriptscriptstyle {(L)}}$ 和
$\hat{\boldsymbol \beta}_{\mathrm{OLS}}$ 取决于簇的数量
其中回归协方差矩阵的特征值被分组。
]]></description>
      <guid>http://arxiv.org/abs/2312.01379</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>深度集成与分位数回归：时间序列的不确定性感知插补。 （arXiv：2312.01294v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.01294</link>
      <description><![CDATA[多元时间序列无处不在。尽管如此，现实世界的时间序列
数据经常表现出大量缺失值，这就是时间序列插补
任务。尽管之前的深度学习方法已被证明是有效的
对于时间序列插补，它们被证明会产生过度自信
估算，这可能是对可靠性的潜在被忽视的威胁
的情报系统。基于分数的扩散方法（即 CSDI）是
对于时间序列插补任务有效，但计算成本较高
生成扩散模型框架的本质。在本文中，我们
提出一种非生成时间序列插补方法，可以产生准确的
具有固有不确定性的插补，同时在计算上
高效的。具体来说，我们将深度集成纳入分位数回归
具有共享的模型主干和一系列分位数区分
该框架结合了精确不确定性估计的优点
深度集成和分位数回归，最重要的是共享模型
主干极大地减少了多个的大部分计算开销
合奏团。我们检查了所提出的方法在两个现实世界中的性能
数据集：空气质量和医疗保健数据集并进行广泛
实验表明我们的方法擅长做出确定性和
概率预测。与基于分数的扩散方法相比：
CSDI，我们可以获得可比较的预测结果，并且数据越多越好
不见了。此外，作为一种非生成模型，与 CSDI 相比，
所提出的方法消耗更小的计算开销，产生更多
更快的训练速度和更少的模型参数。
]]></description>
      <guid>http://arxiv.org/abs/2312.01294</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>用于横截面和纵向多视图数据集成的深度学习管道。 （arXiv：2312.01238v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.01238</link>
      <description><![CDATA[生物医学研究现在通常会整合来自不同领域的不同数据类型或观点
同一个人可以更好地了解复杂疾病的病理学，
但挑战在于如何有效地整合这些不同的观点。
现有方法通常需要来自所有视图的相同类型的数据
（仅横截面数据或仅纵向数据）或不考虑任何
集成方法中的类结果存在局限性。克服
这些限制，我们开发了一个管道，利用
统计和深度学习方法来整合横截面和
来自多个来源的纵向数据。此外，它还识别关键
有助于视图之间关联和分离的变量
类之间，提供更深入的生物学见解。该管道包括
使用线性和非线性方法、特征进行变量选择/排序
使用函数主成分分析和欧拉进行提取
特征，以​​及使用密集的联合集成和分类
前馈网络和循环神经网络。我们应用了这个管道
横截面和纵向多组学数据（宏基因组学，
炎症性肠病 (IBD) 的转录组学和代谢组学）
研究中我们确定了微生物途径、代谢物和基因
按 IBD 状态进行区分，提供有关 IBD 病因的信息。我们
进行了模拟来比较两种特征提取方法。这
建议的管道可从以下 GitHub 存储库获取：
https://github.com/lasandrall/DeepIDA-GRU。
]]></description>
      <guid>http://arxiv.org/abs/2312.01238</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>重新思考 PGD 攻击：符号功能有必要吗？ （arXiv：2312.01260v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.01260</link>
      <description><![CDATA[神经网络已经在各个领域取得了成功，但它们的
即使很小的输入扰动也会显着降低性能。
因此，这种扰动的构建，称为对抗性扰动
攻击，已引起广泛关注，其中许多属于
我们可以完全访问神经网络的“白盒”场景。现存的
攻击算法，例如投影梯度下降（PGD），通常采用
更新对抗性输入之前原始梯度上的符号函数，
从而忽略梯度幅度信息。在本文中，我们提出了一个
这种基于符号的更新算法如何影响的理论分析
逐步攻击性能及其警告。我们也解读一下原因
之前直接使用原始梯度的尝试失败了。基于此，我们
进一步提出了一种新的原始梯度下降（RGD）算法，该算法消除了
标志的使用。具体来说，我们将约束优化问题转化为
一种无约束的变量，通过引入一个新的非裁剪隐藏变量
可以超越约束的扰动。的有效性
提出的 RGD 算法已在实验中得到广泛证明，
在各种情况下均优于 PGD 和其他竞争对手，且不会产生任何费用
任何额外的计算开销。代码可在
https://github.com/JunjieYang97/RGD。
]]></description>
      <guid>http://arxiv.org/abs/2312.01260</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>用于无重放增量学习的高效扩展和基于梯度的任务推理。 （arXiv：2312.01188v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.01188</link>
      <description><![CDATA[本文提出了一种简单但高效的基于扩展的模型
不断学习。最近的特征转换、掩蔽和
基于分解的方法很有效，但它们只能使模型增长
全局或共享参数。因此，这些方法并不能完全
利用先前学到的信息，因为相同的任务特定
参数忘记了先前的知识。因此，这些方法显示出有限的
迁移学习能力。此外，大多数这些模型都具有恒定的
所有任务的参数增长，无论任务复杂程度如何。我们的工作
提出了一种基于简单滤波器和通道扩展的方法，该方法可以增长
模型基于先前的任务参数而不仅仅是全局参数。
因此，它充分利用了所有先前学到的信息，而无需
遗忘，这会导致更好的知识转移。我们的增长率
提出的模型是任务复杂性的函数；因此对于一个简单的任务，
该模型的参数增长较小，而对于复杂的任务，该模型
需要更多的参数来适应当前的任务。基于最近的扩展
模型显示了任务增量学习（TIL）的有希望的结果。然而，对于
类增量学习（CIL），任务id的预测是一个关键的挑战；
因此，随着任务数量的增加，他们的结果会迅速下降。在这个
工作中，我们提出了一种利用熵的鲁棒任务预测方法
使用伪标签的加权数据增强和模型梯度。我们
在 TIL、CIL 和 中的各种数据集和架构上评估我们的模型
生成性持续学习设置。所提出的方法表明
所有这些设置均达到最先进的结果。我们广泛的消融研究
显示所提出组件的功效。
]]></description>
      <guid>http://arxiv.org/abs/2312.01188</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>当准确的预测模型产生有害的自我实现的预言时。 （arXiv：2312.01210v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2312.01210</link>
      <description><![CDATA[预测模型在医学研究和实践中很受欢迎。通过预测
对于特定患者感兴趣的结果，这些模型可能有助于告知
艰难的治疗决定，经常被誉为“典型儿童”
个性化、数据驱动的医疗保健。

然而，我们表明，使用预测模型进行决策可能会导致
有害的决定，即使预测在之后表现出良好的辨别力
部署。这些模型是有害的自我实现预言：它们的
部署会伤害一组患者，但这些患者的结果更糟
不会使模型的预测能力失效。我们的主要结果是
一组此类预测模型的正式表征。接下来我们展示一下
在部署之前和之后经过良好校准的模型对于
决策，因为他们没有改变数据分布。这些结果
指出需要修改验证、部署和部署的标准实践
评估医疗决策中使用的预测模型。
]]></description>
      <guid>http://arxiv.org/abs/2312.01210</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>用于持续和广义零样本学习的元学习属性自交互网络。 （arXiv：2312.01167v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.01167</link>
      <description><![CDATA[零样本学习（ZSL）是将模型泛化为一种有前景的方法
通过利用类属性在训练期间看不见的类别，但是
挑战依然存在。最近，使用生成模型来对抗偏见的方法
培训期间看到的课程已经推动了最先进的技术，但是这些
生成模型的训练速度可能很慢，或者计算成本很高。还，
这些生成模型假设每个未见类的属性向量
可以在训练时先验地获得，但这并不总是实用的。此外，
虽然许多以前的 ZSL 方法假设一次性适应未见过的类，
事实上，世界总是在变化，需要不断调整
已部署模型的数量。没有准备好处理顺序数据流的模型是
很可能会经历灾难性的遗忘。我们提出了元学习
用于持续 ZSL 的属性自交互网络 (MAIN)。通过配对
使用元学习和逆向训练属性自交互
属性编码器的正则化，我们能够超越
无需利用看不见的类属性即可获得最先进的结果
还能够比昂贵的模型更快地训练我们的模型（&gt; 100 倍）
基于生成的方法。我们通过五个实验证明了这一点
广义中的标准 ZSL 数据集（CUB、aPY、AWA1、AWA2 和 SUN）
零样本学习和持续（固定/动态）零样本学习设置。
广泛的消融和分析证明了各种成分的功效
建议的。
]]></description>
      <guid>http://arxiv.org/abs/2312.01167</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:22 GMT</pubDate>
    </item>
    <item>
      <title>SASSL：通过神经风格迁移增强自我监督学习。 （arXiv：2312.01187v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.01187</link>
      <description><![CDATA[自监督学习在很大程度上依赖于数据增强来提取
来自未标记图像的有意义的表示。虽然现有
最先进的增强管道包含了广泛的原始
变换，这些通常忽略自然图像结构。因此，增强了
样本可以表现出退化的语义信息和低风格多样性，
影响自我监督表示的下游表现。到
为了克服这个问题，我们提出 SASSL：自我监督的风格增强
学习，一种基于神经风格迁移的新型增强技术。这
方法解耦图像中的语义和风格属性并应用
只对风格进行转换，同时保留内容，生成
不同的增强样本可以更好地保留其语义属性。
实验结果表明我们的技术实现了 top-1 分类
与 ImageNet 相比，性能提升超过 2%
完善的 MoCo v2。我们还衡量跨领域的迁移学习表现
五个不同的数据集，观察到高达 3.75% 的显着改进。我们的
实验表明，将风格与内容信息解耦
跨数据集转移风格以实现增强多样化可以显着
提高自我监督表示的下游绩效。
]]></description>
      <guid>http://arxiv.org/abs/2312.01187</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:22 GMT</pubDate>
    </item>
    <item>
      <title>分布极小极大问题的对称平均场 Langevin 动力学。 （arXiv：2312.01127v1 [数学.OC]）</title>
      <link>http://arxiv.org/abs/2312.01127</link>
      <description><![CDATA[在本文中，我们将平均场 Langevin 动力学扩展到极小极大优化
首次在对称且可证明的概率分布上
收敛更新。我们提出平均场朗之万平均梯度（MFL-AG），
实现梯度下降上升的单循环算法
具有新颖的加权平均的分布空间，并建立
平均迭代收敛于混合纳什均衡。我们也研究这两个
时间和粒子离散化机制并证明新的时间一致
混沌结果的传播，解释了粒子的依赖性
所有以前的发行版上的交互。此外，我们提出平均场
Langevin 锚定最佳响应 (MFL-ABR)，一种对称双环算法
基于具有线性最后迭代收敛的最佳响应动力学。最后，
我们研究零和马尔可夫博弈的应用并进行模拟
证明长期最优性。
]]></description>
      <guid>http://arxiv.org/abs/2312.01127</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>$t^3$-变分自动编码器：使用 Student t 和幂散度学习重尾数据。 （arXiv：2312.01133v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.01133</link>
      <description><![CDATA[变分自编码器 (VAE) 通常采用标准正态先验
作为概率潜在编码器的正则化器。然而，高斯
尾部通常衰减太快而无法有效容纳编码点，
未能保留隐藏在数据中的关键结构。在本文中，我们
探索使用重尾模型来对抗过度正则化。绘画
根据信息几何的见解，我们提出 $t^3$VAE，一种改进的 VAE
框架结合了先验、编码器的学生 t 分布，
和解码器。这导致了功率形式的联合模型分布，其中
我们认为可以更好地适应现实世界的数据集。我们得出一个新目标
将证据下界重新表述为 KL 散度的联合优化
两个统计流形之间并用 $\gamma$-幂散度替换，
权力家庭的自然选择。 $t^3$VAE 表现出优越性
在重尾合成数据上进行训练时生成低密度区域。
此外，我们表明 $t^3$VAE 在以下方面显着优于其他模型
CelebA 和不平衡的 CIFAR-100 数据集。
]]></description>
      <guid>http://arxiv.org/abs/2312.01133</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:21 GMT</pubDate>
    </item>
    <item>
      <title>二阶不确定性量化：基于距离的方法。 （arXiv：2312.00995v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.00995</link>
      <description><![CDATA[在过去的几年中，出现了各种表示和
量化机器学习中不同类型的预测不确定性，
特别是在分类设置方面，提出了以下基础：
二阶概率分布，即预测形式为
概率分布上的分布。完全确定的解决方案
然而，尚未发现，正如最近对常用的批评所表明的那样
与二阶分布相关的不确定性度量，识别
这些措施的理论特性不理想。鉴于这些
批评，我们提出了一套正式的标准来衡量有意义的不确定性
基于二阶分布的预测不确定性的测量应该
遵守。此外，我们还提供了一个发展不确定性的总体框架
考虑这些标准的措施，并提供基于
Wasserstein 距离，我们证明满足所有标准。
]]></description>
      <guid>http://arxiv.org/abs/2312.00995</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>用于异常检测的袋装正则 $k$ 距离。 （arXiv：2312.01046v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.01046</link>
      <description><![CDATA[我们考虑无监督异常检测的范例，其中涉及
在没有标记的情况下识别数据集中的异常
例子。尽管基于距离的方法对于无监督来说表现最好
异常检测，他们严重受到对选择的敏感性的影响
最近邻居的数量。在本文中，我们提出了一种新的
基于距离的算法，称为袋装正则化$k$-异常距离
检测（BRDAD）将无监督异常检测问题转化为
凸优化问题。我们的 BRDAD 算法通过以下方式选择权重
最小化替代风险，即经验的有限样本范围
用于密度估计的袋装加权 $k$ 距离 (BWDDE) 的风险。这
方法使我们能够成功应对敏感性挑战
基于距离的算法中的超参数选择。此外，在处理
对于大规模数据集，效率问题可以通过
将装袋技术纳入我们的 BRDAD 算法中。在理论方面，
我们建立了我们算法的 AUC 遗憾的快速收敛速度，并且
证明 bagging 技术显着减少了计算量
复杂。在实践方面，我们对异常进行数值实验
检测基准来说明参数选择的不敏感性
我们的算法与其他最先进的基于距离的方法进行了比较。
此外，应用装袋技术带来了有希望的改进
在我们对现实世界数据集的算法中。
]]></description>
      <guid>http://arxiv.org/abs/2312.01046</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>纳什从人类反馈中学习。 （arXiv：2312.00886v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.00886</link>
      <description><![CDATA[基于人类反馈的强化学习 (RLHF) 已成为主要的学习方法
将大型语言模型（LLM）与人类偏好保持一致的范例。
通常，RLHF 涉及从人类学习奖励模型的第一步
反馈，通常表示为文本生成对之间的偏好
由经过预先培训的法学硕士制作。随后，LLM的政策进行了微调
通过强化学习对其进行优化以最大化奖励模型
算法。然而，当前奖励模型的固有局限性是
无法充分代表人类偏好的丰富性及其
依赖于抽样分布。

在这项研究中，我们引入了一种替代管道来微调
使用成对人类反馈的法学硕士。我们的方法需要初步学习
偏好模型，它以给定提示的两个输入为条件，
其次是追求能够持续产生反应的政策
优于任何竞争政策产生的政策，从而定义了纳什
该偏好模型的均衡。我们将这种方法称为纳什学习
人类反馈（NLHF）。

在表格策略表示的背景下，我们提出了一种新颖的
算法解决方案 Nash-MD，建立在镜像下降原理的基础上。
该算法产生一系列策略，最后一次迭代
收敛到正则化纳什均衡。此外，我们还探索
策略的参数表示并引入梯度下降
深度学习架构的算法。为了证明其有效性
我们的方法，我们提出了涉及微调的实验结果
用于文本摘要任务的法学硕士。我们相信 NLHF 提供了一条引人注目的途径
用于偏好学习和政策优化，具有推进的潜力
使法学硕士与人类偏好保持一致的领域。
]]></description>
      <guid>http://arxiv.org/abs/2312.00886</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>无限维空间上的极小极大优化问题的收敛性，以实现对抗训练中的稳定性。 （arXiv：2312.00991v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.00991</link>
      <description><![CDATA[训练需要对抗性优化的神经网络，例如
生成对抗网络（GAN）和无监督域适应
（UDAs），遭受不稳定的困扰。这种不稳定问题来自于
极小极大优化的难度，并且有多种方法
GAN 和 UDA 来克服这个问题。在这项研究中，我们解决了这个问题
通过泛函分析从理论上解决问题。具体来说，我们展示了
极小极大问题的收敛性
连续函数和概率测度的无限维空间
在某些条件下。使用这个设置，我们可以讨论 GAN 和 UDA
全面、独立地研究过。此外，我们还展示
收敛性的必要条件被解释为
对抗训练的稳定技术，例如光谱
归一化和梯度惩罚。
]]></description>
      <guid>http://arxiv.org/abs/2312.00991</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:19 GMT</pubDate>
    </item>
    <item>
      <title>超越一阶 Tweedie：使用潜在扩散解决逆问题。 （arXiv：2312.00852v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.00852</link>
      <description><![CDATA[从后验分布中采样构成了主要的计算
使用潜在扩散模型解决逆问题的挑战。常见的
方法依赖于 Tweedie 的一阶矩，已知该矩会引起
质量限制偏差。现有的二阶近似是不切实际的
计算成本高昂，制定标准的反向扩散过程
后采样很难处理。本文介绍二阶Tweedie
来自 Surrogate Loss (STSL) 的采样器，一种提供效率的新颖采样器
与一阶 Tweedie 相当，具有易于处理的逆向过程，使用
二阶近似。我们的理论结果表明
二阶近似的下界受我们的代理损失的限制，仅
需要使用 Hessian 矩阵的迹和下界进行 $O(1)$ 计算
我们推导出一个新的漂移项，使逆过程易于处理。我们的方法
超越 SoTA 求解器 PSLD 和 P2L，实现神经网络计算量减少 4 倍和 8 倍
分别进行功能评估，同时显着提高采样质量
FFHQ、ImageNet 和 COCO 基准。此外，我们还表明 STSL 扩展到
文本引导的图像编辑并解决来自
领先的文本引导图像编辑方法中的图像损坏。竭尽全力
知识，这是第一个提供高效二阶的工作
使用潜在扩散和编辑解决逆问题的近似
具有损坏的真实世界图像。
]]></description>
      <guid>http://arxiv.org/abs/2312.00852</guid>
      <pubDate>Tue, 05 Dec 2023 03:14:18 GMT</pubDate>
    </item>
    </channel>
</rss>