<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 09 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>与模型无关的元学习器，用于估计随时间推移的异质治疗效果</title>
      <link>https://arxiv.org/abs/2407.05287</link>
      <description><![CDATA[arXiv:2407.05287v1 公告类型：交叉 
摘要：在个性化医疗等许多学科中，随时间估计异质治疗效果 (HTE) 至关重要。例如，电子健康记录通常在几个时间段内收集，然后用于个性化治疗决策。现有的这项任务主要集中在基于模型的学习者（即适应特定机器学习模型的学习者）。相比之下，与模型无关的学习者——所谓的元学习者——基本上尚未被探索。在我们的论文中，我们提出了几种与模型无关的元学习者，因此可以与任意机器学习模型（例如，Transformers）结合使用，以估计随时间推移的 HTE。在这里，我们的重点是可以通过加权伪结果回归获得的学习者，这允许通过直接针对治疗效果进行有效估计。然后，我们提供了全面的理论分析，该分析描述了不同的学习器，并使我们能够了解何时更适合使用特定的学习器。最后，我们通过数值实验证实了我们的理论见解。总之，虽然元学习器在静态设置中已经是最先进的，但我们是第一个提出一套全面的元学习器来估计时变设置中的 HTE 的人。]]></description>
      <guid>https://arxiv.org/abs/2407.05287</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:51 GMT</pubDate>
    </item>
    <item>
      <title>非凸复合损失上循环采样 DP-SGD 中最后一次迭代的隐私</title>
      <link>https://arxiv.org/abs/2407.05237</link>
      <description><![CDATA[arXiv:2407.05237v1 公告类型：交叉 
摘要：差分隐私随机梯度下降 (DP-SGD) 是指一类优化算法，通过 DP 核算技术提供保证级别的差分隐私 (DP)。然而，当前的核算技术做出的假设与实际的 DP-SGD 实现大相径庭。例如，他们可能假设损失函数是 Lipschitz 连续且凸的，随机抽取批次并替换，或者省略梯度裁剪步骤。
在这项工作中，我们分析了最常用的 DP-SGD 变体，其中我们循环抽取批次并替换，执行梯度裁剪，并且仅发布最后的 DP-SGD 迭代。更具体地说，在不假设损失函数具有凸性、平滑性或 Lipschitz 连续性的情况下，我们为最后一次 DP-SGD 迭代建立了新的 R\&#39;enyi 差分隐私 (RDP) 界限，前提是 (i) DP-SGD 步长相对于损失函数中的拓扑常数较小，并且 (ii) 损失函数是弱凸的。此外，我们表明，当目标函数的弱凸性参数趋近于零时，我们的界限会收敛到先前建立的凸界限。对于非 Lipschitz 平滑损失函数，我们提供了一个较弱的界限，该界限在 DP-SGD 迭代次数方面具有很好的可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2407.05237</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:50 GMT</pubDate>
    </item>
    <item>
      <title>规范测地凸规划</title>
      <link>https://arxiv.org/abs/2407.05261</link>
      <description><![CDATA[arXiv:2407.05261v1 公告类型：交叉 
摘要：凸规划在机器学习、数据科学和工程中起着基础性的作用。测试非线性规划中的凸性结构依赖于验证目标和约束的凸性。\citet{grant2006disciplined} 引入了一个框架，即 Disciplined Convex Programming (DCP)，用于自动执行各种凸函数的验证任务，这些凸函数可以使用凸性保持组合和变换（规则）分解为基本凸函数（原子）。然而，对欧几里得凸性概念的限制可能会限制该框架的适用性。例如，机器学习应用中许多值得注意的统计估计量和矩阵值（子）例程都是欧几里得非凸的，但通过更一般的黎曼透镜表现出测地线凸性。在这项工作中，我们通过引入规范测地凸规划 (DGCP) 将规范编程扩展到此设置。我们确定一般 Cartan-Hadamard 流形上的测地凸函数的凸性保持组合和变换，以及矩阵值优化中常见的设置——对称正定矩阵的特殊情况。对于后者，我们还定义了一组基本原子。我们的论文附有 Julia 包 SymbolicAnalysis.jl，它提供了测试和认证符合 DGCP 的表达式的功能。我们的库与流形优化软件接口，允许直接求解经过验证的测地凸程序。]]></description>
      <guid>https://arxiv.org/abs/2407.05261</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:50 GMT</pubDate>
    </item>
    <item>
      <title>通过组合数据驱动方法实现准势的稀疏识别</title>
      <link>https://arxiv.org/abs/2407.05050</link>
      <description><![CDATA[arXiv:2407.05050v1 公告类型：交叉 
摘要：准势函数允许理解和预测非线性动力系统中亚稳态的逃逸机制。该函数是非梯度系统势函数的自然延伸，它揭示了系统的最大似然转换路径、转换率和预期退出时间等重要特性。在这里，我们利用机器学习，通过结合两种数据驱动技术，即神经网络和稀疏回归算法，来获得准势函数的符号表达式。关键思想是首先使用神经网络确定控制底层动力学的矢量场的正交分解，然后符号化地解释分解的下坡和循环分量。这些函数通过添加数学约束同时回归。我们表明，我们的方法发现了一个简约的准势方程，适用于具有已知精确准势的原型模型和纳米机械谐振器的动力学。解析形式可以直接访问亚稳态的稳定性，并预测具有显著计算优势的罕见事件。我们的数据驱动方法对于评估波动动力学的广泛应用很有意义。]]></description>
      <guid>https://arxiv.org/abs/2407.05050</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:49 GMT</pubDate>
    </item>
    <item>
      <title>DMTG：一次性可区分多任务分组</title>
      <link>https://arxiv.org/abs/2407.05082</link>
      <description><![CDATA[arXiv:2407.05082v1 公告类型：交叉 
摘要：我们旨在通过多任务分组 (MTG) 解决具有大量任务的多任务学习 (MTL)。给定 N 个任务，我们建议同时从 2^N 个候选任务中识别最佳任务组，并一次性同时训练模型权重，充分利用高阶任务亲和性。这与按顺序识别组并训练模型权重的开创性方法不同，其中组识别通常依赖于启发式方法。因此，我们的方法不仅提高了训练效率，而且还减轻了顺序程序引入的客观偏差，这些偏差可能导致次优解决方案。具体而言，我们将 MTG 表述为由底层分类分布确定的自适应网络架构上的完全可微分剪枝问题。为了将 N 个任务分为 K 个组（由 K 个编码器分支表示），我们首先设置 KN 个任务头，其中每个分支连接到所有 N 个任务头以利用高阶任务亲和性。然后，我们通过学习宽松可微分类分布逐渐将 KN 个任务头修剪为 N 个，确保每个任务都被唯一地归类到一个分支中。在 CelebA 和 Taskonomy 数据集上进行的大量实验以及详细的消融表明了我们的方法具有良好的性能和效率。代码可在 https://github.com/ethanygao/DMTG 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.05082</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:49 GMT</pubDate>
    </item>
    <item>
      <title>深度、宽度和树大小在深度森林表达力中的作用</title>
      <link>https://arxiv.org/abs/2407.05108</link>
      <description><![CDATA[arXiv:2407.05108v1 公告类型：交叉 
摘要：随机森林是经典的集成算法，它构建多个随机决策树并使用朴素平均法聚合它们的预测。 \citet{zhou2019deep} 进一步提出了一种具有多层森林的深度森林算法，该算法在各种任务中均优于随机森林。深度森林的性能在实践中与三个超参数有关：深度、宽度和树大小，但对其理论解释知之甚少。这项工作首次提供了关于这三个超参数的深度森林近似复杂度的上限和下限。我们的结果证实了深度的独特作用，与宽度和树大小相比，深度可以成倍地增强深度森林的表现力。实验证实了理论结果。]]></description>
      <guid>https://arxiv.org/abs/2407.05108</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:49 GMT</pubDate>
    </item>
    <item>
      <title>随机过程的 Kullback-Leibler 重心</title>
      <link>https://arxiv.org/abs/2407.04860</link>
      <description><![CDATA[arXiv:2407.04860v1 公告类型：交叉
摘要：我们考虑代理旨在结合不同专家模型的观点和见解的问题。具体来说，每个专家都会在有限的时间范围内提出一个扩散过程。然后，代理通过最小化每个专家模型的加权 Kullback-Leibler 散度来组合专家的模型。我们展示了重心模型的存在性和唯一性，并证明了 Radon-Nikodym 导数相对于平均漂移模型的明确表示。我们进一步允许代理包含他们自己的约束，从而产生一个最佳模型，该模型可以看作是专家重心模型的扭曲，以结合代理的约束。
提出了两种深度学习算法来寻找组合模型的最佳漂移，从而实现高效的模拟。第一种算法旨在通过匹配测量变化来学习最佳漂移，而第二种算法利用可引发性的概念来直接估计值函数。本文最后提出了一个扩展应用，将根据不同数据集估计的隐含波动率微笑模型结合起来。]]></description>
      <guid>https://arxiv.org/abs/2407.04860</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:48 GMT</pubDate>
    </item>
    <item>
      <title>个性特征高斯过程心理评估</title>
      <link>https://arxiv.org/abs/2407.04970</link>
      <description><![CDATA[arXiv:2407.04970v1 公告类型：交叉 
摘要：我们开发了一种基于高斯过程共区域化模型的新型测量框架，以解决心理测量学中长期存在的争论：性格等心理特征是否在整个人群中具有共同的结构，因人而异，还是以某种方式组合。我们提出了个性高斯过程 (IPGP) 框架，这是一个中间模型，既可以适应整个人群的共享特征结构，也可以适应个人的“个性”偏差。IPGP 利用高斯过程共区域化模型来处理电池响应的分组性质，但调整为非高斯序数数据。我们进一步利用随机变分推理来进行大规模个性建模所需的有效潜在因子估计。使用合成和真实数据，我们表明 IPGP 相对于现有基准改进了对实际响应的预测和对个性化因子结构的估计。在第三项研究中，我们表明，IPGP 还能在现实世界数据中识别出独特的人格分类集群，在推进个性化心理诊断和治疗方法方面显示出巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.04970</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:48 GMT</pubDate>
    </item>
    <item>
      <title>基于可能性的空间一致性降尺度生成方法</title>
      <link>https://arxiv.org/abs/2407.04724</link>
      <description><![CDATA[arXiv:2407.04724v1 公告类型：交叉 
摘要：深度学习已成为降水降尺度分析的一种有前途的工具。然而，目前的模型依赖于基于似然的损失函数来正确模拟降水分布，导致采样时出现空间不一致的预测。这项工作通过融合生成模型中使用的基于似然和对抗损失的优势，探索了一种新方法。因此，我们提出了一种基于似然的降水生成方法，充分利用了这两种方法的优势。]]></description>
      <guid>https://arxiv.org/abs/2407.04724</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:47 GMT</pubDate>
    </item>
    <item>
      <title>RPN：面向统一 PGM、核 SVM、MLP 和 KAN 的协调多项式网络</title>
      <link>https://arxiv.org/abs/2407.04819</link>
      <description><![CDATA[arXiv:2407.04819v1 公告类型：交叉 
摘要：在本文中，我们将介绍一种用于深度函数学习的新型深度模型，即协调多项式网络 (RPN)。RPN 具有非常通用的架构，可用于构建具有各种复杂性、容量和完备程度的模型，这些都有助于这些模型的正确性。如副标题所示，RPN 还可以作为将不同基础模型统一为一个规范表示的主干。这包括非深度模型，如概率图模型 (PGM) - 例如贝叶斯网络和马尔可夫网络 - 和核支持向量机 (核 SVM)，以及深度模型，如经典的多层感知器 (MLP) 和最近的 Kolmogorov-Arnold 网络 (KAN)。
从技术上讲，RPN 建议将要推断的底层函数分解为数据扩展函数和参数协调函数的内积。 RPN 与余数函数一起，可以精确地近似控制数据分布的底层函数。RPN 中的数据扩展函数将数据向量从输入空间投影到定义中的扩展函数指定的高维中间空间。同时，RPN 还引入了参数协调函数，将少量参数合成一个高阶参数矩阵，以解决数据扩展导致的“维数灾难”问题。此外，余数函数为 RPN 提供了额外的补充信息，以减少潜在的近似误差。我们对多个模态的大量基准数据集进行了广泛的实证实验，包括连续函数数据集、离散视觉和语言数据集以及经典表格数据集，以研究 RPN 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.04819</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:47 GMT</pubDate>
    </item>
    <item>
      <title>关于最近邻分类器的高维修改</title>
      <link>https://arxiv.org/abs/2407.05145</link>
      <description><![CDATA[arXiv:2407.05145v1 公告类型：新
摘要：最近邻分类器可以说是文献中最简单、最流行的非参数分类器。然而，由于成对距离的集中和邻域结构的破坏，这种分类器在高维、低样本量 (HDLSS) 情况下经常受到影响，尤其是当竞争类别之间的尺度差异主导其位置差异时。文献中已经进行了几次尝试来解决这个问题。在本文中，我们讨论了一些现有的方法并提出了一些新方法。我们在这方面进行了一些理论研究，并分析了几个模拟和基准数据集，以将所提出的方法与一些现有方法的经验性能进行比较。]]></description>
      <guid>https://arxiv.org/abs/2407.05145</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>DNN 如何打破维数诅咒：组合性和对称学习</title>
      <link>https://arxiv.org/abs/2407.05664</link>
      <description><![CDATA[arXiv:2407.05664v1 公告类型：新
摘要：我们表明深度神经网络 (DNN) 可以有效地学习具有有界 $F_{1}$ 范数的任何函数组合，这使得 DNN 能够以浅层网络无法做到的方式打破维数灾难。更具体地说，我们推导出一个泛化界限，它结合了组合性的覆盖数参数和大宽度自适应性的 $F_{1}$ 范数（或相关的 Barron 范数）。我们表明，DNN 正则化损失的全局最小化器可以从少量观察中拟合两个函数 $f^{*}=h\circ g$ 的组合，假设 $g$ 是平滑/规则的并且降低了维数（例如，$g$ 可以是 $f^{*}$ 对称性的模映射），因此尽管 $h$ 的规则性较低，但仍然可以学习它。我们考虑的规则性度量是具有不同可微性水平的 Sobolev 范数，它与 $F_{1}$ 范数非常吻合。我们根据经验计算缩放定律，并根据 $g$ 或 $h$ 是否更难学习来观察相变，正如我们的理论所预测的那样。]]></description>
      <guid>https://arxiv.org/abs/2407.05664</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>基于模拟的基因扰动实验中的因果结构学习基准测试</title>
      <link>https://arxiv.org/abs/2407.06015</link>
      <description><![CDATA[arXiv:2407.06015v1 公告类型：新
摘要：因果结构学习 (CSL) 是指从数据中学习因果关系的任务。CSL 的进步现在允许学习不同应用领域的因果图，这有可能促进数据驱动的因果决策。现实世界中的 CSL 性能取决于许多特定于上下文的因素，包括特定于上下文的数据分布和非线性依赖关系，这些因素在实际用例中很重要。然而，我们对如何在特定环境中评估和选择 CSL 方法的理解仍然有限。为了解决这一差距，我们提出了一个乘性效应结构因果模型，允许生成结合特定于上下文的属性的观察和干预数据，重点是基因扰动实验的设置。使用现实世界的基因扰动数据，我们表明 CausalRegNet 可以生成准确的分布，并且扩展性远优于当前的模拟框架。我们说明了在生物学干预实验中如何使用 CausalRegNet 来评估 CSL 方法。]]></description>
      <guid>https://arxiv.org/abs/2407.06015</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>通过稳定列表解码实现不可知私有密度估计</title>
      <link>https://arxiv.org/abs/2407.04783</link>
      <description><![CDATA[arXiv:2407.04783v1 公告类型：新
摘要：我们引入了一种新的稳定性概念——我们称之为稳定列表解码——并证明了它在设计差异隐私密度估计器中的适用性。此定义比全局稳定性 [ABLMM22] 弱，并且与可复制性 [ILPS22] 和列表可复制性 [CMY23] 的概念有关。我们表明，如果一类分布是稳定列表可解码的，那么它可以在不可知环境中私下学习。作为我们框架的主要应用，我们证明了不可知环境中高斯混合模型隐私密度估计样本复杂度的第一个上限，扩展了 Afzali 等人的可实现结果。[AAL24]。]]></description>
      <guid>https://arxiv.org/abs/2407.04783</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>通过核分析实现神经网络分类双样本测试的训练保证</title>
      <link>https://arxiv.org/abs/2407.04806</link>
      <description><![CDATA[arXiv:2407.04806v1 公告类型：新
摘要：我们构建并分析了一个神经网络双样本测试，以确定两个数据集是否来自同一分布（零假设）或不是（备择假设）。我们对神经正切核 (NTK) 双样本测试进行时间分析。具体来说，我们推导出确保 NTK 双样本测试检测到数据集之间的偏差水平所需的理论最短训练时间。同样，我们推导出 NTK 双样本测试检测到偏差水平之前的理论最大训练时间。通过用 NTK 动态近似神经网络动态，我们将此时间分析扩展到由时变训练动态和有限训练样本生成的现实神经网络双样本测试。对由时变训练动态生成但针对总体进行训练的神经网络双样本测试进行了类似的扩展。为了提供统计保证，我们表明，当神经网络训练样本和测试评估样本趋于无穷大时，与神经网络双样本测试相关的统计功效趋于 1。此外，我们证明了在零假设和备选假设情景中检测相同偏差水平所需的训练时间是分开的。最后，我们运行了一些实验，展示了一个困难的双样本测试问题上的双层神经网络双样本测试，并绘制了双样本测试的统计功效与训练时间和网络复杂度之间的热图。]]></description>
      <guid>https://arxiv.org/abs/2407.04806</guid>
      <pubDate>Wed, 10 Jul 2024 03:16:45 GMT</pubDate>
    </item>
    </channel>
</rss>