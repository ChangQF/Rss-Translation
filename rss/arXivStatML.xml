<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Wed, 20 Dec 2023 03:11:34 GMT</lastBuildDate>
    <item>
      <title>Dip-test 指令集的扩展——用于聚类的高效且可微分的 p 值计算。 （arXiv：2312.12050v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12050</link>
      <description><![CDATA[在过去的十年中，单峰性的 Dip 测试取得了越来越多的成果
对数据挖掘社区的兴趣，因为它是一种无参数统计
可靠地评估一维样本中的模态的测试。它返回一个
所谓的 Dip 值和样本的相应概率
单峰性（Dip-p-值）。这两个值具有 S 型关系。
然而，具体的转换取决于样本大小。许多
基于 Dip 的聚类算法使用引导查找表进行翻译
特定有限数量样本量的 Dip- 到 Dip-p 值。我们提出一个
专门设计的 sigmoid 函数作为这些函数的替代品
最先进的查找表。这加速了计算并提供了
每个样本的 Dip 到 Dip p 值转换的近似值
尺寸。此外，它是可微分的，因此可以很容易地集成到
使用梯度下降的学习方案。我们通过利用我们的
在称为 Dip&#39;n&#39;Sub 的新型子空间聚类算法中发挥作用。我们
在广泛的实验中强调我们建议的各种好处。
]]></description>
      <guid>http://arxiv.org/abs/2312.12050</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:33 GMT</pubDate>
    </item>
    <item>
      <title>固定预算的最佳手臂识别：大偏差视角。 （arXiv：2312.12137v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12137</link>
      <description><![CDATA[我们考虑在随机多臂中识别最佳臂的问题
使用固定抽样预算的 Bandits (MAB)。表征最小化
该问题的特定于实例的错误概率构成了其中之一
人与生物圈计划中尚未解决的重要问题。当使用选择手臂时
静态采样策略，误差概率随
可以通过大偏差明确导出的速率的样本数量
技术。使用自适应采样分析算法的性能
然而，策略更具挑战性。在本文中，我们建立了一个
大偏差原理（LDP）之间的联系满足
手臂抽签的经验比例以及经验手臂满足的比例
奖励。此连接适用于任何自适应算法，并且可以利用 (i)
提高一些现有算法的错误概率上限，例如
著名的 \sr（连续拒绝）算法 \citep{audibert2010best}，以及
(ii) 设计和分析新算法。特别是，我们提出\sred
（连续拒绝），一种真正的自适应算法，可以拒绝{\it
任何}轮基于观察到的各种奖励之间的经验差距
武器。应用我们的大偏差结果，我们证明 \sred 享有更好的结果
性能保证优于现有算法，包括 \sr。广泛的
数值实验证实了这一观察结果。
]]></description>
      <guid>http://arxiv.org/abs/2312.12137</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:33 GMT</pubDate>
    </item>
    <item>
      <title>CUDC：一种好奇心驱动的无监督数据收集方法，具有自适应时间距离，用于离线强化学习。 （arXiv：2312.12191v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12191</link>
      <description><![CDATA[离线强化学习（RL）旨在从
预先收集的数据集。大多数现有的工作都是为了开发复杂的
学习算法，不太注重改进数据收集
过程。此外，扩展单任务设置和
收集与任务无关的数据集，允许代理执行多个任务
下游任务。在本文中，我们提出了一种好奇心驱动的无监督
使用自适应时间扩展特征空间的数据收集（CUDC）方法
与任务无关的数据收集的距离并最终改善学习
多任务离线强化学习的效率和功能。为了实现这一目标，CUDC
估计 k 步未来状态可从
当前状态，并调整动态向未来迈出多少步
模型应该预测。有了这种自适应可达性机制，
特征表示可以多样化，代理可以自行导航到
带着好奇心收集更高质量的数据。根据经验，CUDC 超越
现有的无监督方法在各种领域的效率和学习表现
DeepMind 控制套件的下游离线 RL 任务。
]]></description>
      <guid>http://arxiv.org/abs/2312.12191</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:33 GMT</pubDate>
    </item>
    <item>
      <title>使用低成本传感器对布琼布拉的细颗粒物动力学进行建模和表征。 （arXiv：2312.12003v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.12003</link>
      <description><![CDATA[空气污染是多种来源造成的，包括自然污染和
人类活动。诸如此类的城市快速城市化
布隆迪的经济首都布琼布拉，就是这些因素之一。第一个
布琼布拉 PM2.5 时空变化特征
本文采用以下方法对 PM2.5 浓度进行预测：
2022年8月至2023年8月一年内按低成本收集的数据
安装在布琼布拉市的传感器。对于每个公社，每小时、每天和
进行了季节性分析，结果表明：
三市PM2.5浓度各公社不同
其他。 PM2.5时均、年均浓度超世界
卫生组织标准。范围在 28.3 至 35.0 微克/立方米之间
。为了预测PM2.5浓度，研究RNN
已经进行了长短期记忆（LSTM）。
]]></description>
      <guid>http://arxiv.org/abs/2312.12003</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:32 GMT</pubDate>
    </item>
    <item>
      <title>LightGCNet：用于数据驱动软传感器的轻量级几何构造神经网络。 （arXiv：2312.12022v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.12022</link>
      <description><![CDATA[数据驱动的软传感器提供了潜在的成本效益和更多
准确的建模方法来测量难以测量的指标
工业过程与机械方法相比。人造的
深度学习等智能（AI）技术已成为流行的软件
机器学习和大数据领域的传感器建模方法。
然而，基于深度学习的软传感器模型可能会导致复杂的
模型结构和过多的训练时间。此外，工业过程
通常依赖于以资源为特征的分布式控制系统（DCS）
限制。在此，以空间几何为指导，提出了一种轻量级几何
提出了构造性神经网络，即 LightGCNet，它利用
紧凑的角度约束来分配动态的隐藏参数
间隔。同时提出了节点池策略和空间几何
关系用于可视化和优化分配过程
隐藏参数，增强可解释性。此外，通用
通过空间几何分析证明了LightGCNet的近似性质。
本文介绍了 LightGCNet 的两个版本的算法实现
文章。关于基准数据集和矿石的模拟结果
磨削过程显示了LightGCNet在小尺寸方面的显着优点
网络规模大，学习速度快，泛化能力强。
]]></description>
      <guid>http://arxiv.org/abs/2312.12022</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:32 GMT</pubDate>
    </item>
    <item>
      <title>悲观离线强化学习的神经网络逼近。 （arXiv：2312.11863v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.11863</link>
      <description><![CDATA[深度强化学习 (RL) 在特定领域取得了显着的成功
离线决策场景，但其理论保证仍然存在
正在开发中。现有的离线强化学习理论著作主要强调
一些琐碎的设置，例如线性 MDP 或一般函数逼近
强有力的假设和独立数据，缺乏实际使用的指导。
深度学习和贝尔曼残差的耦合使得这个问题
除了数据依赖的困难之外，还具有挑战性。在本文中，
我们使用以下方法建立悲观离线强化学习的非渐近估计误差
使用 $\mathcal{C}$ 混合数据的一般神经网络近似
网络结构、数据集维度和集中性
在温和假设下的数据覆盖范围。我们的结果表明估计
误差由两部分组成：第一部分以期望的速率收敛到零
浓度部分可控的样本量，第二个
如果残差约束很紧，则变得可以忽略不计。这个结果
展示了深度对抗性离线 RL 框架的显式效率。
我们利用经验处理工具来进行 $\mathcal{C}$-混合序列
H\&quot;{o}lder类的神经网络逼近理论来实现
这。我们还开发了限制贝尔曼估计误差的方法
具有经验贝尔曼约束扰动的函数逼近。
此外，我们提出了一个减少维数灾难的结果
使用具有低内在维度的数据和具有低内在维度的函数类
复杂。我们的估计为发展提供了宝贵的见解
深度离线强化学习和算法模型设计指导。
]]></description>
      <guid>http://arxiv.org/abs/2312.11863</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:31 GMT</pubDate>
    </item>
    <item>
      <title>大学习期望最大化。 （arXiv：2312.11926v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.11926</link>
      <description><![CDATA[混合模型是一种具有多种应用的基本工具。
然而，他们的训练技术，比如流行的期望最大化
(EM) 算法，对参数初始化非常敏感，并且经常
遭受糟糕的局部最优，可能比最优更差。
为了解决长期存在的不良局部最优挑战，我们汲取了灵感
从最近的突破性基础模型中汲取灵感，并建议利用它们
升级 EM 的基本大学习原则。具体来说，我们提出
Big Learning EM (BigLearn-EM)，一种同时执行的 EM 升级
数据之间的联合、边际和正交变换边际匹配
和模型分布。通过模拟实验，我们凭经验证明
BigLearn-EM 能够以高的速度提供最佳结果
可能性;基准聚类数据集的比较进一步证明
其相对于现有技术的有效性和优势。代码是
可以在
https://github.com/YulaiCong/Big-Learning-Expectation-Maximization。
]]></description>
      <guid>http://arxiv.org/abs/2312.11926</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:31 GMT</pubDate>
    </item>
    <item>
      <title>利用拓扑的 Urysohn 引理增强二元分类器。 （arXiv：2312.11948v1 [物理.data-an]）</title>
      <link>http://arxiv.org/abs/2312.11948</link>
      <description><![CDATA[在本文中，我们对 Urysohn 分类器进行了全面分析
在二元分类上下文中。它利用 Urysohn 拓扑引理
构建分离功能，提供严格且适应性强的解决方案。
数值实验展示了卓越的性能，得分范围
从 95% 到 100%。值得注意的是，Urysohn 的分类器优于 CatBoost，并且
各种场景下的KNN。尽管对 p 度量参数敏感，但它
事实证明，它具有鲁棒性和适应性。 Urysohn 分类器的数学严谨性和
适应性使其在二元分类方面具有广阔的应用前景
医疗诊断、欺诈检测和网络安全。未来的研究包括
参数优化以及将 Urysohn 分类器与其他分类器相结合
技术。它提供了一种优雅且有原则的分类方法，
确保完整性和有价值的数据洞察。
]]></description>
      <guid>http://arxiv.org/abs/2312.11948</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:31 GMT</pubDate>
    </item>
    <item>
      <title>噪声机制下异常值的根本原因解释。 （arXiv：2312.11818v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2312.11818</link>
      <description><![CDATA[识别因果过程中异常的根本原因对于各个领域都至关重要
学科。一旦确定，就可以隔离根本原因并实施
采取必要措施恢复正常运行。因果过程往往是
建模为图，实体为节点及其路径/互连
作为边缘。现有的工作仅考虑节点的贡献
生成过程，因此不能将离群值归因于
连接发生异常时的机制。在本文中，我们
在识别时考虑每个机制的单个边缘和节点
根本原因。我们引入一个噪声函数因果模型来解释这一点
目的。然后，我们采用贝叶斯学习和推理方法来推断
节点和边缘的噪声。然后我们表示一个函数形式
目标离群叶作为节点和边缘噪声的函数。最后，我们
提出一种有效的基于梯度的归因方法来计算异常
归因分数与节点和边的数量线性缩放。
对模拟数据集和两个真实场景数据集的实验表明
与现有方法相比，所提出的方法具有更好的异常归因性能
基线。我们的方法可以扩展到具有更多节点和边的更大图。
]]></description>
      <guid>http://arxiv.org/abs/2312.11818</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:30 GMT</pubDate>
    </item>
    <item>
      <title>Topo-MLP：没有消息传递的简单网络。 （arXiv：2312.11862v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.11862</link>
      <description><![CDATA[由于它们能够对集合之间有意义的高阶关系进行建模
实体的高阶网络模型最近出现作为一种强大的
基于图的网络模型的替代方案，只能建模
二元关系。消息传递范式仍然主要用于
甚至可以学习高阶网络模型的表示。强大的同时，
消息传递在推理过程中可能有缺点，特别是当
高阶连接信息丢失或损坏。为了克服这样的
由于局限性，我们提出了 Topo-MLP，一种纯粹基于 MLP 的单纯神经网络
学习单纯复形中元素表示的算法
无需明确依赖消息传递。我们的框架采用了一种新颖的
高阶邻域对比 (HONC) 损失隐含地包含
将简单结构引入表示学习。我们提出的模型
简单性使得推理过程更快。此外，我们表明我们的模型
当面临连接结构丢失或损坏时，它是鲁棒的。
]]></description>
      <guid>http://arxiv.org/abs/2312.11862</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:30 GMT</pubDate>
    </item>
    <item>
      <title>具有高斯权重的宽深度神经网络非常接近高斯过程。 （arXiv：2312.11737v1 [数学.ST]）</title>
      <link>http://arxiv.org/abs/2312.11737</link>
      <description><![CDATA[我们为随机深度神经网络的高斯近似建立了新的速率
具有高斯参数（权重和偏差）和 Lipschitz 激活的网络
函数，在宽范围内。我们的界限适用于 a 的联合输出
网络评估任何有限输入集，前提是具有一定的非简并性
无限宽度协方差的条件成立。我们证明了
网络输出与对应高斯分布之间的距离
近似值与网络宽度成反比，表现得更快
比中心极限定理所建议的朴素启发式收敛。我们
还应用我们的界限来获得精确的理论近似值
网络的贝叶斯后验分布，当似然为
在（有限）上评估的网络输出的有界 Lipschitz 函数
训练集。这包括流行的情况，例如高斯似然，即
指数减去均方误差。
]]></description>
      <guid>http://arxiv.org/abs/2312.11737</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:29 GMT</pubDate>
    </item>
    <item>
      <title>用于一般张量分解的 ADMM-MM 算法。 （arXiv：2312.11763v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.11763</link>
      <description><![CDATA[在本文中，我们提出了一种新的通用优化算法
张量分解被表述为低秩的反问题
一般线性观测模型中的张量。提出的算法
支持三种基本损失函数（$\ell_2$-loss、$\ell_1$-loss 和 KL
散度）和各种低秩张量分解模型（CP、Tucker、TT、
和 TR 分解）。我们基于以下推导出优化算法
乘法器交替方向法的分层组合
（ADMM）和最大化最小化（MM）。我们展示了广泛的应用
可以通过所提出的算法来解决，并且可以很容易地扩展到任何
以即插即用的方式建立张量分解模型。
]]></description>
      <guid>http://arxiv.org/abs/2312.11763</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:29 GMT</pubDate>
    </item>
    <item>
      <title>最优分离下有界协方差分布的聚类混合。 （arXiv：2312.11769v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.11769</link>
      <description><![CDATA[我们研究有界协方差混合的聚类问题
在细粒度分离假设下的分布。具体来说，给定
来自 $k$ 分量混合分布的样本 $D = \sum_{i =1}^k w_i P_i$，
其中每个 $w_i \ge \alpha$ 对应某个已知参数 $\alpha$，每个 $P_i$
对于某些未知的协方差 $\Sigma_i \preceq \sigma^2_i \cdot I_d$
$\sigma_i$，目标是假设成对均值对样本进行聚类
每个之间的间隔顺序为 $(\sigma_i+\sigma_j)/\sqrt{\alpha}$
一对组件 $P_i$ 和 $P_j$。我们的贡献如下：

对于几乎均匀混合物的特殊情况，我们给出第一个聚合时间
此聚类任务的算法。之前的工作要么需要分离
以最大聚类标准差进行缩放（即 $\max_i \sigma_i$）
[DKK+22b] 或需要额外的结构假设和均值
分离缩放作为 $1/\alpha$ [BKK22] 中的大次多项式。

对于一般权重混合物，我们指出准确的聚类是
在我们的细粒度平均分离下，信息理论上是不可能的
假设。我们引入聚类细化的概念——一系列
满足类似分离且可以合并的不太小的子集
成一个近似真实情况的聚类——并表明它是
可以有效地计算准确的聚类细化
样品。此外，在“无大子簇”条件的变体下
从之前的工作 [BKK22] 中，我们表明我们的算法输出准确的
聚类，不仅仅是一种改进，即使对于一般重量的混合物也是如此。作为一个
推论，我们获得了混合的有效聚类算法
条件良好的高维对数凹分布。

此外，我们的算法对于对抗性的 $\Omega(\alpha)$ 分数具有鲁棒性
异常值。
]]></description>
      <guid>http://arxiv.org/abs/2312.11769</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:29 GMT</pubDate>
    </item>
    <item>
      <title>改进的差分私有和惰性在线凸优化。 （arXiv：2312.11534v1 [cs.CR]）</title>
      <link>http://arxiv.org/abs/2312.11534</link>
      <description><![CDATA[我们研究$(\epsilon, \delta)$的任务——在线差分隐私
凸优化（OCO）。在在线设置中，每个不同的版本的发布
决策或迭代可能会导致隐私丢失。这
这个问题的研究历史悠久，始于 Jain 等人。 [2012] 和
给出了 {\epsilon} 非常小的状态的最著名结果
在阿加瓦尔等人。 [2023]。在本文中，我们改进了 Agarwal 的结果
等人。 [2023] 在尺寸因素方面以及删除
平滑度的要求。我们的结果现在是 DP-OCO 最著名的费率
在这个政权下。

我们的算法建立在 [Asi et al., 2023] 的工作基础上，该工作引入了
通过拒绝采样明确限制开关数量的想法。
我们算法的主要创新是使用强采样
对数凹密度使我们能够更好地权衡尺寸因素
从而改善结果。
]]></description>
      <guid>http://arxiv.org/abs/2312.11534</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:28 GMT</pubDate>
    </item>
    <item>
      <title>引出 Kemeny 排名。 （arXiv：2312.11663v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.11663</link>
      <description><![CDATA[我们提出了引出代理人偏好的问题，目标是
寻找 Kemeny 排名作为决斗强盗问题。这里是强盗的武器
对应需要排序的备选方案，反馈对应
通过随机抽样的代理对备选方案进行成对比较。我们
考虑有放回和无放回的抽样，即可能性
多次询问同一个代理人是否进行某些比较。

我们发现 Kemeny 排名的近似界限取决于置信度
武器的估计获胜概率的间隔。基于这些我们声明
找到可能近似正确（PAC）解决方案并详细阐述的算法
关于有或没有放回抽样的样本复杂性。
此外，如果所有代理的偏好都严格排序
替代方案，我们提供了修剪置信区间的方法，从而指导
更有效的诱导。我们制定了几种自适应采样方法
使用前瞻来估计置信区间的大小（从而
近似保证）可能会收紧。所有描述的方法都是
综合数据进行比较。
]]></description>
      <guid>http://arxiv.org/abs/2312.11663</guid>
      <pubDate>Wed, 20 Dec 2023 03:11:28 GMT</pubDate>
    </item>
    </channel>
</rss>