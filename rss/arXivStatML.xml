<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 20 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>非消极对比学习</title>
      <link>https://arxiv.org/abs/2403.12459</link>
      <description><![CDATA[arXiv:2403.12459v1 公告类型：交叉
摘要：当以黑盒方式转移到下游任务时，深度表示表现出了良好的性能。然而，它们固有的缺乏可解释性仍然是一个重大挑战，因为这些特征通常对人类理解来说是不透明的。在本文中，我们提出了非负对比学习（NCL），这是非负矩阵分解（NMF）的复兴​​，旨在导出可解释的特征。 NCL 的强大之处在于它对特征执行非负约束，这让人想起 NMF 提取与样本簇密切相关的特征的能力。 NCL 不仅在数学上与 NMF 目标很好地吻合，而且还保留了 NMF 的可解释性属性，从而与标准对比学习 (CL) 相比，产生更加稀疏和解缠结的表示。理论上，我们为 NCL 的可识别性和下游泛化建立了保证。根据经验，我们表明这些优势使 NCL 在特征解缠、特征选择以及下游分类任务上显着优于 CL。最后，我们证明 NCL 可以轻松扩展到其他学习场景，并且也有利于监督学习。代码可在 https://github.com/PKU-ML/non_neg 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.12459</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:42 GMT</pubDate>
    </item>
    <item>
      <title>精度矩阵的另一种图形套索算法</title>
      <link>https://arxiv.org/abs/2403.12357</link>
      <description><![CDATA[arXiv:2403.12357v1 公告类型：交叉
摘要：图形套索（GLasso）算法快速且广泛用于估计稀疏精度矩阵（Friedman et al., 2008）。它在高维协方差估计文献中的核心作用可与均值向量稀疏估计的 Lasso 回归相媲美。 Mazumder 和 Hastie (2011) 揭示、解决并提出了有关其优化目标、收敛性、正定性和性能的一些谜团，从而产生了新的/改进的（双原始）DP-GLasso。使用精度矩阵最后一列的新的且略有不同的重新参数化，我们表明正则化正态对数似然自然地解耦为两个易于最小化的凸函数之和，其中一个是套索回归问题。这种分解是开发透明、简单的迭代块坐标下降算法的关键，该算法用于计算 GLasso 更新，其性能与 DP-GLasso 相当。特别是，我们的算法从一开始就将精度矩阵作为其优化目标，并保留了 DP-GLasso 算法的所有有利属性。]]></description>
      <guid>https://arxiv.org/abs/2403.12357</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>通过奖励样本转移连续多臂强盗</title>
      <link>https://arxiv.org/abs/2403.12428</link>
      <description><![CDATA[arXiv:2403.12428v1 公告类型：交叉
摘要：我们考虑一个顺序随机多臂老虎机问题，其中代理在多个阶段与老虎机交互。手臂的奖励分配在整个情节中保持不变，但可以在不同的情节中发生变化。我们提出了一种基于 UCB 的算法来转移前几集的奖励样本，并提高所有集的累积遗憾性能。我们为我们的算法提供了后悔分析和实证结果，这表明它比没有转移的标准 UCB 算法有显着的改进。]]></description>
      <guid>https://arxiv.org/abs/2403.12428</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>生成的数据总是有助于对比学习吗？</title>
      <link>https://arxiv.org/abs/2403.12448</link>
      <description><![CDATA[arXiv:2403.12448v1 公告类型：交叉
摘要：对比学习（CL）已成为无监督视觉表示学习最成功的范例之一，但它通常依赖于密集的手动数据增强。随着生成模型，尤其是扩散模型的兴起，生成接近真实数据分布的真实图像的能力已经得到了广泛认可。这些生成的高质量图像已成功应用于增强对比表示学习，这是一种称为“数据膨胀”的技术。然而，我们发现生成的数据（即使来自像 DDPM 这样的良好扩散模型）有时甚至可能损害对比学习。我们从数据膨胀和数据增强的角度调查了这种失败背后的原因。我们首次揭示了更强的数据膨胀应该伴随着更弱的增强的互补作用，反之亦然。我们还通过推导数据膨胀下的泛化界限，为这些现象提供严格的理论解释。根据这些见解，我们提出了自适应通货膨胀（AdaInf），这是一种纯粹以数据为中心的策略，无需引入任何额外的计算成本。在基准数据集上，AdaInf 可以为各种对比学习方法带来显着的改进。值得注意的是，在不使用外部数据的情况下，AdaInf 使用 SimCLR 在 CIFAR-10 上获得了 94.70% 的线性精度，创下了超越许多复杂方法的新记录。代码可在 https://github.com/PKU-ML/adainf 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.12448</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>FedFisher：利用 Fisher 信息进行一次性联邦学习</title>
      <link>https://arxiv.org/abs/2403.12329</link>
      <description><![CDATA[arXiv:2403.12329v1 公告类型：交叉
摘要：标准联邦学习（FL）算法通常需要服务器和客户端之间进行多轮通信，这存在一些缺点，包括需要持续的网络连接、重复投资计算资源以及容易受到隐私攻击。 One-Shot FL 是一种新范例，旨在通过使服务器能够在单轮通信中训练全局模型来应对这一挑战。在这项工作中，我们提出了 FedFisher，一种用于单次 FL 的新颖算法，该算法利用在本地客户端模型上计算的 Fisher 信息矩阵，其灵感来自于 FL 的贝叶斯观点。首先，我们从理论上分析了两层超参数化 ReLU 神经网络的 FedFisher，并表明随着神经网络宽度和客户端本地训练量的增加，我们的一次性 FedFisher 全局模型的误差变得非常小。接下来，我们使用对角 Fisher 和 K-FAC 近似来提出完整 Fisher 的 FedFisher 实用变体，并强调它们的 FL 通信和计算效率。最后，我们对各种数据集进行了广泛的实验，结果表明 FedFisher 的这些变体相对于竞争基线持续改进。]]></description>
      <guid>https://arxiv.org/abs/2403.12329</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>规范空间中的随机 Halpern 迭代及其在强化学习中的应用</title>
      <link>https://arxiv.org/abs/2403.12338</link>
      <description><![CDATA[arXiv:2403.12338v1 公告类型：交叉
摘要：我们分析了具有方差缩减的随机 Halpern 迭代的预言复杂性，旨在逼近规范化有限维空间中非扩张和收缩算子的不动点。我们表明，如果底层随机预言具有均匀有界方差，我们的方法表现出 $\tilde{O}(\varepsilon^{-5})$ 的总体预言复杂度，从而提高了为随机 Krasnoselskii-Mann 迭代建立的近期速率。此外，我们建立了 $\Omega(\varepsilon^{-3})$ 的下限，它适用于各种算法，包括所有平均迭代，即使是小批量处理。通过对我们的方法进行适当的修改，我们在运算符是 $\gamma$- 的情况下得出 $O(\varepsilon^{-2}(1-\gamma)^{-3})$ 复杂度界限收缩。作为一个应用，我们提出了用于平均奖励和折扣奖励马尔可夫决策过程的新同步算法。特别是，对于平均奖励，我们的方法改进了最著名的样本复杂性。]]></description>
      <guid>https://arxiv.org/abs/2403.12338</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>通过平方和进行私人图元估计</title>
      <link>https://arxiv.org/abs/2403.12213</link>
      <description><![CDATA[arXiv:2403.12213v1 公告类型：交叉
摘要：我们开发了第一个纯节点差分隐私算法，用于学习随机块模型和对任何恒定数量的块进行多项式运行时间的图元估计。统计效用保证与之前解决这些问题的最佳信息论（指数时间）节点私有机制相匹配。该算法基于分数函数的指数机制，该分数函数是根据平方和松弛定义的，其级别取决于块的数量。我们结果的关键成分是（1）根据双随机矩阵多面体的二次优化来表征块图子之间的距离，（2）任意多项式优化的一般平方和收敛结果多胞体；(3) 作为平方和算法范式的一部分执行得分函数 Lipschitz 扩展的通用方法。]]></description>
      <guid>https://arxiv.org/abs/2403.12213</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>选择具有错误覆盖率控制的信息丰富的共形预测集</title>
      <link>https://arxiv.org/abs/2403.12295</link>
      <description><![CDATA[arXiv:2403.12295v1 公告类型：交叉
摘要：在监督学习中，包括回归和分类，共形方法为任何机器学习预测变量提供具有有限样本覆盖的结果/标签的预测集。我们在这里考虑这种预测集是在选择过程之后出现的情况。选择过程要求所选择的预测集在明确定义的意义上是“信息丰富的”。我们考虑分类和回归设置，其中分析师可能仅将预测标签集或预测间隔足够小的样本视为提供信息，排除空值或遵守其他适当的“单调”约束。虽然这涵盖了各种应用中可能感兴趣的许多设置，但我们开发了一个统一的框架来构建此类信息丰富的保形预测集，同时控制所选样本的错误覆盖率（FCR）。虽然选择后的共形预测集一直是该领域最近文献的焦点，但据我们所知，新引入的程序（称为 InfoSP 和 InfoSCOP）是第一个为信息预测集提供 FCR 控制的程序。我们展示了我们的结果程序在真实和模拟数据上的有用性。]]></description>
      <guid>https://arxiv.org/abs/2403.12295</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>用于学习神经网络等变表示的图神经网络</title>
      <link>https://arxiv.org/abs/2403.12143</link>
      <description><![CDATA[arXiv:2403.12143v1 公告类型：交叉
摘要：处理其他神经网络参数的神经网络在多种领域都有应用，例如分类隐式神经表示、生成神经网络权重和预测泛化误差。然而，现有的方法要么忽略了神经网络中固有的排列对称性，要么依赖复杂的权重共享模式来实现等变，而忽略了网络架构本身的影响。在这项工作中，我们建议将神经网络表示为参数的计算图，这使我们能够利用强大的图神经网络和保持排列对称性的变压器。因此，我们的方法使单个模型能够编码具有不同架构的神经计算图。我们展示了我们的方法在各种任务上的有效性，包括隐式神经表示的分类和编辑、预测泛化性能以及学习优化，同时始终优于最先进的方法。源代码在 https://github.com/mkofinas/neural-graphs 上开源。]]></description>
      <guid>https://arxiv.org/abs/2403.12143</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>少数人的力量：通过核心集选择加速和增强数据重新加权</title>
      <link>https://arxiv.org/abs/2403.12166</link>
      <description><![CDATA[arXiv:2403.12166v1 公告类型：交叉
摘要：随着机器学习任务的不断发展，收集更大的数据集并训练越来越大的模型已成为趋势。虽然这提高了准确性，但也将计算成本提升到了不可持续的水平。为了解决这个问题，我们的工作旨在在计算效率和模型准确性之间取得微妙的平衡，这是该领域持续存在的挑战。我们引入了一种新颖的方法，该方法采用核心子集选择来重新加权，有效地优化计算时间和模型性能。通过专注于战略选择的核心集，我们的方法提供了强大的代表性，因为它有效地最小化了异常值的影响。然后重新校准的权重被映射回并传播到整个数据集。我们的实验结果证实了这种方法的有效性，强调了其作为可扩展且精确的模型训练解决方案的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.12166</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>使用数据增强的神经网络中的后验不确定性量化</title>
      <link>https://arxiv.org/abs/2403.12729</link>
      <description><![CDATA[arXiv:2403.12729v1 公告类型：新
摘要：在本文中，我们通过预测框架来解决深度学习中的不确定性量化问题，该框架通过指定我们对未见的未来数据的预测分布的假设来捕获模型参数的不确定性。根据这种观点，我们表明深度集成（Lakshminarayanan 等人，2017）从根本上来说是一个错误指定的模型类，因为它假设未来的数据仅支持现有的观察结果——这种情况在实践中很少遇到。为了解决这个限制，我们提出了 MixupMP，一种使用流行的数据增强技术构建更现实的预测分布的方法。 MixupMP 可以作为深度集成的直接替代品，其中每个集成成员都接受来自该预测分布的随机模拟的训练。 MixupMP 基于最近提出的 Martingale 后验框架（Fong 等人，2023），从隐式定义的贝叶斯后验返回样本。我们的实证分析表明，与现有的贝叶斯和非贝叶斯方法相比，MixupMP 在各种图像分类数据集上实现了卓越的预测性能和不确定性量化。]]></description>
      <guid>https://arxiv.org/abs/2403.12729</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>顺序核回归的更严格的置信界限</title>
      <link>https://arxiv.org/abs/2403.12732</link>
      <description><![CDATA[arXiv:2403.12732v1 公告类型：新
摘要：置信界限是严格量化预测不确定性的重要工具。通过这种能力，它们可以告知探索与利用的权衡，并形成许多顺序学习和决策算法的核心组成部分。更严格的置信界限会产生具有更好的经验性能和更好的性能保证的算法。在这项工作中，我们使用鞅尾界和无限维凸程序的有限维重构来为顺序核回归建立新的置信界。我们证明，在这种情况下，我们的新置信区间总是比现有的置信区间更紧。我们将置信区间应用于内核强盗问题，其中未来的行动取决于之前的历史。当我们的置信边界取代现有的置信边界时，KernelUCB（GP-UCB）算法具有更好的经验性能、匹配的最坏情况性能保证和可比较的计算成本。我们的新置信界限可以用作通用工具来为其他核化学习和决策问题设计改进的算法。]]></description>
      <guid>https://arxiv.org/abs/2403.12732</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>通过神经网络逼近 RKHS 泛函</title>
      <link>https://arxiv.org/abs/2403.12187</link>
      <description><![CDATA[arXiv:2403.12187v1 公告类型：新
摘要：由于时间序列和图像等丰富的函数数据的推动，人们越来越有兴趣将这些数据集成到神经网络中，并学习从函数空间到 R（即泛函）的映射。在本文中，我们研究了使用神经网络再现核希尔伯特空间（RKHS）的泛函逼近。我们建立了 RKHS 泛函近似的普适性。具体来说，我们得出了由逆多重二次函数、高斯函数和 Sobolev 函数引起的显式误差界限。此外，我们将我们的发现应用于函数回归，证明神经网络可以准确地近似广义函数线性模型中的回归图。现有的函数学习工作需要使用一组预先指定的基函数进行集成型基函数扩展。通过利用 RKHS 中的插值正交投影，我们提出的网络要简单得多，因为我们使用点评估来代替基函数展开。]]></description>
      <guid>https://arxiv.org/abs/2403.12187</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>基于半监督评分的匹配算法评估公共卫生干预效果</title>
      <link>https://arxiv.org/abs/2403.12367</link>
      <description><![CDATA[arXiv:2403.12367v1 公告类型：新
摘要：多元匹配算法在观察研究中“配对”相似的研究单位，以消除由于缺乏随机化而导致的潜在偏差和混杂效应。在一对一的多元匹配算法中，大量要匹配的“对”可能意味着来自大样本和大量任务的信息，因此，为了最好地匹配这些对，这样的匹配算法由领域专家通过配对单元的“训练”集提供的效率和相对有限的辅助匹配知识实际上很有趣。
  我们提出了一种基于二次评分函数 $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$ 的新颖的一对一匹配算法。权重 $\beta$ 可以解释为变量重要性度量，旨在最小化配对训练单元之间的分数差异，同时最大化未配对训练单元之间的分数差异。此外，在训练集远小于未配对集的典型但复杂的情况下，我们提出了一个 \underline{s}emisupervised \underline{c}ompanion \underline{o}ne-\underline{t}o-\ underline{o}ne \underline{m}atching \underline{a}算法 (SCOTOMA) 充分利用了不成对的单元。当真值匹配标准确实是二次得分函数时，所提出的权重估计器被证明是一致的。当违反模型假设时，我们通过一系列模拟证明所提出的算法仍然优于一些流行的竞争匹配算法。我们将所提出的算法应用于现实世界的研究，以调查面对面教育对社区 Covid-19 传播率的影响，以用于政策制定的目的。]]></description>
      <guid>https://arxiv.org/abs/2403.12367</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>Dirichlet 混合模型中有效 KL 散度估计的变分方法</title>
      <link>https://arxiv.org/abs/2403.12158</link>
      <description><![CDATA[arXiv:2403.12158v1 公告类型：新
摘要：这项研究解决了狄利克雷混合模型 (DMM) 中 Kullback-Leibler (KL) 散度的有效估计，这对于组合数据的聚类至关重要。尽管数字万用表很重要，但获得 KL 散度的分析上易于处理的解决方案已被证明是难以捉摸的。过去的方法依赖于计算要求较高的蒙特卡罗方法，这促使我们引入一种新颖的变分方法。我们的方法提供了封闭式解决方案，显着提高了快速模型比较和稳健估计评估的计算效率。使用真实和模拟数据进行的验证展示了其优于传统蒙特卡罗方法的效率和准确性，为快速探索各种 DMM 模型和推进成分数据的统计分析开辟了新途径。]]></description>
      <guid>https://arxiv.org/abs/2403.12158</guid>
      <pubDate>Wed, 20 Mar 2024 06:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>