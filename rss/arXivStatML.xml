<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 09 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>连续模下投影朗之万算法的混合时间和隐私分析</title>
      <link>https://arxiv.org/abs/2501.04134</link>
      <description><![CDATA[arXiv:2501.04134v1 公告类型：新
摘要：我们研究了投影朗之万算法 (LA) 的混合时间和噪声随机梯度下降 (SGD) 的隐私曲线，超越了非扩张迭代。具体来说，我们推导出投影 LA 的新混合时间界限，在某些重要情况下，这些界限在精度上是无维度的和多对数的，与平滑凸情况下的现有结果非常接近。此外，我们为子采样噪声 SGD 算法的隐私曲线建立了新的上限。这些界限显示出对梯度规律性的关键依赖性，并且对于平滑情况以外的各种凸损失都很有用。我们的分析依赖于对隐私放大迭代 (PABI) 框架 (Feldman 等人，2018 年；Altschuler 和 Talwar，2022 年，2023 年) 的适当扩展，以适应梯度图不一定非膨胀的噪声迭代。此扩展是通过设计一个优化问题来实现的，该优化问题考虑了应用 PABI 获得的最佳 R\&#39;enyi 散度界限，其中问题的可处理性与相关梯度映射的连续性模量密切相关。我们表明，在几个有趣的情况下——包括非光滑凸、弱光滑和（强）耗散——这种优化问题可以精确而明确地求解。这产生了最严格的基于 PABI 的界限，其中我们的结果要么是新的，要么比以前的作品要清晰得多。]]></description>
      <guid>https://arxiv.org/abs/2501.04134</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从嘈杂示例中生成</title>
      <link>https://arxiv.org/abs/2501.04179</link>
      <description><![CDATA[arXiv:2501.04179v1 公告类型：新 
摘要：我们继续研究生成的学习理论基础，扩展了 Kleinberg 和 Mullainathan [2024] 和 Li 等人 [2024] 的结果，以解释嘈杂的示例流。在 Kleinberg 和 Mullainathan [2024] 和 Li 等人 [2024] 的无噪声设置中，对手从二元假设类中选择一个假设，并为生成器提供其正例序列。生成器的目标是最终输出新的、未见过的正例。在嘈杂的环境中，对手仍然会选择一个假设及其正例序列。但是，在将流呈现给生成器之前，对手会插入有限数量的反例。不知道哪些例子是有噪声的，生成器的目标仍然是最终输出新的、未见过的正例。在本文中，我们提供了二元假设类何时可以噪声生成所需的充分条件。我们针对在完美生成正例之前需要看到的不同示例数量的各种限制提供了此类条件。有趣的是，对于有限和可数类，我们表明生成性在很大程度上不受有限数量的噪声示例的影响。]]></description>
      <guid>https://arxiv.org/abs/2501.04179</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习基准测试中总体性能指标的统计不确定性量化</title>
      <link>https://arxiv.org/abs/2501.04234</link>
      <description><![CDATA[arXiv:2501.04234v1 公告类型：新
摘要：现代人工智能由机器学习模型（例如基础模型）支持，这些模型在海量数据语料库上进行预训练，然后进行调整以解决各种下游任务。为了总结多个任务的性能，评估指标通常被汇总为一个汇总指标，例如 10 个问答任务的平均准确率。在汇总评估指标时，将不确定性纳入汇总指标中很有用，以便更真实地了解模型性能。我们在这项工作中的目标是展示如何使用统计方法来量化已在多个任务中汇总的指标中的不确定性。我们强调的方法是引导、贝叶斯分层（即多级）建模和考虑标准误差的任务权重可视化。这些技术揭示了一些见解，例如尽管整体性能不佳，但特定模型在某些类型的任务中占主导地位。我们使用流行的 ML 基准——视觉任务适应基准 (VTAB)，来证明我们的方法的实用性。]]></description>
      <guid>https://arxiv.org/abs/2501.04234</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论神经网络中回归任务的权重和方差不确定性</title>
      <link>https://arxiv.org/abs/2501.04272</link>
      <description><![CDATA[arXiv:2501.04272v1 公告类型：新
摘要：我们考虑了 [Blundell 等人（2015 年）提出的权重不确定性问题。神经网络中的权重不确定性。在国际机器学习会议，1613-1622，PMLR。] 在专门用于回归任务的神经网络 {(NNs)} 中。{我们进一步} 研究了方差不确定性在 {他们的模型} 中的影响。我们表明，包括方差不确定性可以提高贝叶斯 {NN} 的预测性能。方差不确定性通过考虑方差参数的后验分布来增强模型的泛化。{我们使用函数近似} 示例检查所提模型的泛化能力，并用} 核黄素基因数据集 {进一步说明}。{我们分别探索具有} 高斯和尖峰和平板先验的完全连接的密集网络和 dropout NN，用于网络权重。]]></description>
      <guid>https://arxiv.org/abs/2501.04272</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>视觉自回归模型的电路复杂性界限</title>
      <link>https://arxiv.org/abs/2501.04299</link>
      <description><![CDATA[arXiv:2501.04299v1 公告类型：新 
摘要：了解特定模型的表达能力对于掌握其容量限制至关重要。最近，一些研究已经为 Transformer 架构建立了电路复杂度界限。此外，视觉自回归 (VAR) 模型已成为图像生成领域的一种重要方法，在生成高质量图像方面优于以前的技术，例如扩散变压器。我们在本研究中研究了 VAR 模型的电路复杂度并建立了一个界限。我们的主要结果表明，VAR 模型相当于具有隐藏维度 $d \leq O(n)$ 和 $\mathrm{poly}(n)$ 精度的均匀 $\mathsf{TC}^0$ 阈值电路的模拟。这是第一项严格强调 VAR 模型表达能力局限性的研究，尽管它们的性能令人印象深刻。我们相信我们的研究结果将为这些模型的固有约束提供宝贵的见解，并指导未来开发更高效、更具表现力的架构。]]></description>
      <guid>https://arxiv.org/abs/2501.04299</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DCIts——时间序列的深度卷积解释器</title>
      <link>https://arxiv.org/abs/2501.04339</link>
      <description><![CDATA[arXiv:2501.04339v1 公告类型：新
摘要：我们引入了一种可解释的深度学习模型，用于多变量时间序列预测，该模型优先考虑预测性能和可解释性 - 理解复杂物理现象的关键要求。我们的模型不仅匹配而且经常超越现有的可解释性方法，在不影响准确性的情况下实现这一目标。通过大量实验，我们证明了它能够识别最相关的时间序列和滞后，这些时间序列和滞后有助于预测未来值，为其预测提供直观和透明的解释。为了最大限度地减少对人工监督的需求，该模型的设计使得人们能够稳健地确定最佳窗口大小，以在尽可能短的时间内捕获所有必要的交互。此外，它有效地确定了最佳模型阶数，在合并高阶项时平衡了复杂性。这些进步对建模和理解动态系统具有重要意义，使该模型成为应用和计算物理学家的宝贵工具。]]></description>
      <guid>https://arxiv.org/abs/2501.04339</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多峰优化的自然变分退火</title>
      <link>https://arxiv.org/abs/2501.04667</link>
      <description><![CDATA[arXiv:2501.04667v1 公告类型：新
摘要：我们引入了一种称为自然变分退火 (NVA) 的新多模态优化方法，该方法结合了三个基础概念的优势，可以同时搜索黑盒非凸目标的多个全局和局部模式。首先，它通过使用变分后验（例如高斯混合）来实现同时搜索。其次，它应用退火来逐步权衡探索和利用。最后，它使用自然梯度学习来学习变分搜索分布，其中更新类似于众所周知且易于实现的算法。这三个概念在 NVA 中结合在一起，产生了新的算法，也使我们能够融入“适应度塑造”，这是进化算法的一个核心概念。我们评估模拟搜索的质量，并将它们与使用梯度下降和进化策略的方法进行比较。我们还为行星科学中的真实逆问题提供了一个应用。]]></description>
      <guid>https://arxiv.org/abs/2501.04667</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>算法偏差评估中统计功效的充分性：ABROCA 测试</title>
      <link>https://arxiv.org/abs/2501.04683</link>
      <description><![CDATA[arXiv:2501.04683v1 公告类型：新
摘要：算法偏差是教育数据挖掘 (EDM) 中的一个紧迫问题，因为它有可能加剧学习成果的不平等。ROC 曲线间面积 (ABROCA) 指标经常用于衡量不同人口群体之间模型性能的差异，以量化整体模型公平性。然而，它的偏斜分布——尤其是当存在类别或群体不平衡时——使得显着性检验具有挑战性。本研究调查了 ABROCA 的分布特性，并为其显着性检验提供了可靠的方法。具体来说，我们解决 (1) ABROCA 是否遵循任何已知分布，(2) 如何使用 ABROCA 可靠地测试算法偏差，以及 (3) 在典型的 EDM 样本规范下使用基于 ABROCA 的偏差评估可以实现的统计能力。模拟结果证实，ABROCA 与标准分布不匹配，包括那些适合适应偏斜的分布。我们提出了针对 ABROCA 的非参数随机化测试，并证明使用 ABROCA 可靠地检测偏差需要大量样本或显著的效应量，尤其是在不平衡的环境中。研究结果表明，基于 EDM 中常见的样本量的 ABROCA 偏差评估往往缺乏说服力，从而削弱了模型公平性结论的可靠性。通过提供开源代码来模拟说服力和统计测试 ABROCA，本文旨在促进 EDM 研究中更可靠的统计测试。它支持在教育建模中为实现可复制性和公平性而做出的更广泛努力。]]></description>
      <guid>https://arxiv.org/abs/2501.04683</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Multi-SpaCE：基于多目标子序列的多元时间序列分类稀疏反事实解释</title>
      <link>https://arxiv.org/abs/2501.04009</link>
      <description><![CDATA[arXiv:2501.04009v1 公告类型：交叉 
摘要：深度学习系统在复杂任务中表现出色，但往往缺乏透明度，限制了它们在关键应用中的使用。反事实解释是可解释人工智能 (XAI) 中的核心工具，它通过识别对输入的最小更改来改变其预测结果，从而提供对模型决策的洞察。然而，现有的时间序列数据方法受到单变量假设、对修改的严格约束或缺乏有效性保证的限制。本文介绍了一种用于多变量时间序列的多目标反事实解释方法 Multi-SpaCE。使用非支配排序遗传算法 II (NSGA-II)，Multi-SpaCE 平衡了接近度、稀疏性、合理性和连续性。与大多数方法不同，它确保完美的有效性，支持多变量数据并提供解决方案的帕累托前沿，从而能够灵活地满足不同的最终用户需求。在不同数据集中进行的全面实验证明了 Multi-SpaCE 能够始终如一地实现完美的有效性并提供优于现有方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.04009</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Frechet 度量中的近似率：Barron 空间、Paley-Wiener 空间和 Fourier 乘数</title>
      <link>https://arxiv.org/abs/2501.04023</link>
      <description><![CDATA[arXiv:2501.04023v1 公告类型：交叉 
摘要：算子学习是通过神经网络模拟偏微分方程 (PDE) 的最新进展。这种方法背后的想法是学习算子的行为，使得得到的神经网络是无限维空间中的 (近似) 映射，能够 (近似) 模拟由 PDE 控制的解算子。在我们的工作中，我们通过在傅里叶域中近似相应的符号来研究线性微分算子的一些一般近似能力。类似于 H\&quot;ormander-Symbols 类的结构，我们考虑由半范数序列引起的拓扑的近似。从这个意义上讲，我们用 Fr\&#39;echet 度量来测量近似误差，我们的主要结果确定了实现预定义近似误差的充分条件。其次，我们关注我们的主要定理的自然扩展，其中我们设法减少对半范数序列的假设。基于指数谱 Barron 空间的一些现有近似结果，我们随后给出了一个可以很好地近似的符号的具体示例，并且我们还展示了这种近似与信号处理中数字滤波器设计的类比。]]></description>
      <guid>https://arxiv.org/abs/2501.04023</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数据驱动低秩矩阵分解对 Vlasov 方程加速解的评估</title>
      <link>https://arxiv.org/abs/2501.04024</link>
      <description><![CDATA[arXiv:2501.04024v1 公告类型：交叉 
摘要：低秩方法已成功加速由 Vlasov 方程描述的无碰撞等离子体的模拟，但每个时间步骤仍然依赖于计算成本高昂的线性代数。我们提出了一种数据驱动的分解方法，使用人工神经网络，特别是卷积层架构，对现有的模拟数据进行训练。在推理时，模型输出带电粒子分布场的低秩分解，我们证明这一步比标准线性代数技术更快。数值实验表明，该方法有效地插入时间序列数据，并以不仅仅是记忆训练数据的方式推广到看不见的测试数据；分解中的模式也固有地遵循与代数方法（例如，截断奇异值分解）相同的数值趋势。然而，当对时间序列数据的前 70% 进行训练并对剩余的 30% 进行测试时，该方法无法进行有意义的推断。尽管存在这种限制性结果，但该技术可能对统计稳定状态的模拟或以其他方式显示时间稳定性有好处。]]></description>
      <guid>https://arxiv.org/abs/2501.04024</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不完整数据的神经参数估计</title>
      <link>https://arxiv.org/abs/2501.04330</link>
      <description><![CDATA[arXiv:2501.04330v1 公告类型：交叉
摘要：人工智能 (AI) 和深度学习的进步使得神经网络被用于生成复杂问题的闪电般快速答案，像莫奈一样绘画，或像普鲁斯特一样写作。利用其计算速度和灵活性，神经网络还被用于促进快速、无似然的统计推断。然而，对于由于各种原因而不完整的数据，使用神经网络并不简单，这阻碍了它们在许多应用中的使用。最近提出的一种解决此问题的方法是将适当填充的数据向量和编码缺失模式的向量输入到神经网络中。虽然计算效率高，但这种“掩蔽”方法可能导致统计效率低下的推断。在这里，我们提出了一种基于蒙特卡罗期望最大化 (EM) 算法的替代方法。我们的 EM 方法是无似然的，比传统的 EM 算法快得多，因为它不需要在每次迭代时进行数值优化，并且比掩蔽方法在统计上更有效。这项研究代表了一个原型问题，表明如何通过引入贝叶斯统计思维来改进人工智能。我们使用来自两个模型的模拟不完整数据比较了两种缺失值方法：空间高斯过程模型和空间 Potts 模型。该方法的实用性在北极海冰数据和加密货币数据上得到了证明。]]></description>
      <guid>https://arxiv.org/abs/2501.04330</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>加速超梯度型方法 - 第 2 部分：共次单调性下的泛化和亚线性收敛速度</title>
      <link>https://arxiv.org/abs/2501.04585</link>
      <description><![CDATA[arXiv:2501.04585v1 公告类型：交叉 
摘要：继我们项目的第一部分之后，本文全面研究了两种基于外梯度的方法：锚定外梯度和 Nesterov 加速外梯度，用于解决 [非] 线性包含（特别是方程），主要在 Lipschitz 连续性和同单调性假设下。我们将一类用于单调包含的锚定外梯度方法统一并推广到更广泛的方案，包括现有算法作为特殊情况。我们为这个通用框架建立了底层映射残差范数上的 $\mathcal{O}(1/k)$ 最后一次迭代收敛速度，然后对其进行专门化以获得特定实例的收敛保证，其中 $k$ 表示迭代计数器。我们将我们的方法扩展到一类锚定的 Tseng 前向-后向-前向分裂方法，以获得更广泛的用于解决同低单调包含的算法类。同样，我们分析了此通用方案的 $\mathcal{O}(1/k)$ 最后一次迭代收敛速度，并对其进行特殊化以获得现有和新变体的收敛结果。我们将 Nesterov 的加速外梯度方法推广并统一为一类新算法，该算法将现有方案作为特殊实例覆盖，同时生成新变体。对于这些方案，我们可以证明同低单调性下残差范数的 $\mathcal{O}(1/k)$ 最后一次迭代收敛速度，涵盖一类非单调问题。我们提出了另一类新的 Nesterov 加速外梯度方法来解决包含问题。有趣的是，这些算法实现了 $\mathcal{O}(1/k)$ 和 $o(1/k)$ 的最后迭代收敛速度，以及在同次单调性和 Lipschitz 连续性下的迭代序列收敛。最后，我们提供了一组涵盖不同场景的数值实验来验证我们的算法和理论保证。]]></description>
      <guid>https://arxiv.org/abs/2501.04585</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对比预训练和多模态生成人工智能的统计理论</title>
      <link>https://arxiv.org/abs/2501.04641</link>
      <description><![CDATA[arXiv:2501.04641v1 公告类型：交叉 
摘要：多模态生成 AI 系统（例如结合视觉和语言的系统）依赖于对比预训练来学习跨不同模态的表示。虽然它们的实际好处得到了广泛认可，但对对比预训练框架的严格理论理解仍然有限。本文开发了一个理论框架来解释对比预训练在下游任务（如零样本分类、条件扩散模型和视觉语言模型）中的成功。我们引入了近似充分统计的概念，这是经典充分统计的概括，并表明对比预训练损失的近似最小化器是近似充分的，使其能够适应各种下游任务。我们进一步提出了用于图像和文本联合分布的联合生成分层模型，表明 Transformer 可以通过信念传播有效地近似该模型中的相关函数。在此框架的基础上，我们推导出基于对比预训练表示的多模态学习的样本复杂性保证。数值模拟验证了这些理论发现，证明了对比预训练的 Transformer 在各种多模态任务中的强大泛化性能。]]></description>
      <guid>https://arxiv.org/abs/2501.04641</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索数值稳定性的边缘</title>
      <link>https://arxiv.org/abs/2501.04697</link>
      <description><![CDATA[arXiv:2501.04697v1 公告类型：交叉 
摘要：Grokking 是长时间过度拟合后突然出现的泛化现象，是一种令人惊讶的现象，挑战了我们对深度学习的理解。尽管在理解 grokking 方面取得了重大进展，但延迟泛化及其对正则化的依赖背后的原因仍不清楚。在这项工作中，我们认为，如果没有正则化，grokking 任务会将模型推向数值稳定性的边缘，从而在 Softmax 函数中引入浮点误差，我们将其称为 Softmax Collapse (SC)。我们证明 SC 可以防止 grokking，而减轻 SC 可以使 grokking 无需正则化。调查 SC 的根本原因，我们发现，除了过度拟合之外，梯度与我们所谓的朴素损失最小化 (NLM) 方向高度一致。梯度的这个部分不会改变模型的预测，但会通过缩放 logits（通常是沿其当前方向缩放权重）来减少损失。我们表明，这种对 logits 的缩放解释了 grokking 的泛化延迟特征，并最终导致 SC，从而停止进一步的学习。为了验证我们的假设，我们引入了两个解决 grokking 任务挑战的关键贡献：StableMax，一种新的激活函数，可防止 SC 并实现无需正则化的 grokking，以及 $\perp$Grad，一种通过完全防止 NLM 来促进 grokking 任务快速泛化的训练算法。这些贡献为 grokking 提供了新的见解，阐明了其延迟泛化、对正则化的依赖以及现有 grokking 诱导方法的有效性。本文的代码可在https://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability找到。]]></description>
      <guid>https://arxiv.org/abs/2501.04697</guid>
      <pubDate>Thu, 09 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>