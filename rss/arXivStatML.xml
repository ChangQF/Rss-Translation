<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 02 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>随机逆问题：稳定性、正则化和 Wasserstein 梯度流</title>
      <link>https://arxiv.org/abs/2410.00229</link>
      <description><![CDATA[arXiv:2410.00229v1 公告类型：新
摘要：物理或生物科学中的逆问题通常涉及恢复随机的未知参数。所寻求的数量是未知参数的概率分布，它产生与测量一致的数据。因此，这些问题自然被定义为随机逆问题。在本文中，我们探讨了这个问题的三个方面：直接反演、带正则化的变分公式和通过梯度流进行优化，与确定性逆问题相似。与确定性情况的一个关键区别是我们操作的空间。在这里，我们在概率空间而不是欧几里得或索博列夫空间内工作，使得测量传输理论中的工具对于研究必不可少。我们的研究结果表明，度量的选择——无论是在损失函数的设计中还是在优化过程中——都会显着影响优化器的稳定性和属性。]]></description>
      <guid>https://arxiv.org/abs/2410.00229</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分布式 PCA 的广义均值方法</title>
      <link>https://arxiv.org/abs/2410.00397</link>
      <description><![CDATA[arXiv:2410.00397v1 公告类型：新 
摘要：主成分分析 (PCA) 是一种广泛使用的降维技术。随着数据集的不断增长，分布式 PCA (DPCA) 已成为一个活跃的研究领域。由于计算开销，DPCA 的一个关键挑战在于有效地跨多台机器或计算节点聚合结果。Fan 等人 (2019) 介绍了一种开创性的 DPCA 方法来估计领先的秩-$r$ 特征空间，通过平均聚合局部秩-$r$ 投影矩阵。然而，他们的方法没有利用特征值信息。在本文中，我们提出了一种新颖的 DPCA 方法，该方法结合特征值信息通过矩阵 $\beta$-mean 聚合局部结果，我们称之为 $\beta$-DPCA。矩阵 $\beta$-mean 通过可调节的 $\beta$ 值选择提供了一种灵活而强大的聚合方法。值得注意的是，当 $\beta=1$ 时，它对应于算术平均值；当 $\beta=-1$ 时，它对应于调和平均值；当 $\beta \to 0$ 时，它对应于几何平均值。此外，矩阵 $\beta$-mean 与矩阵 $\beta$-divergence（Bregman 矩阵散度的一个子类）相关联，以支持 $\beta$-DPCA 的稳健性。我们还研究了 $\beta$-DPCA 在特征值扰动下特征向量排序的稳定性。通过数值研究评估了我们提案的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.00397</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可微分交互多模型粒子滤波</title>
      <link>https://arxiv.org/abs/2410.00620</link>
      <description><![CDATA[arXiv:2410.00620v1 公告类型：新
摘要：当研究模型表现出随机不连续的行为跳跃时，我们提出了一种用于参数学习的顺序蒙特卡罗算法。为了促进高维参数集（例如与神经网络相关的参数集）的学习，我们采用了新兴的可微分粒子过滤框架，其中参数通过梯度下降进行训练。我们设计了一种新的可微分交互多模型粒子过滤器，能够同时学习个体行为模式和控制跳跃的模型。与以前的方法相比，我们的算法允许控制每个模式分配的计算工作量，同时使用处于给定模式的概率来指导采样。此外，我们开发了一种新的梯度估计器，其方差低于既定方法并且计算速度很快，我们证明了其一致性。我们建立了所提出算法的新理论结果，并与以前最先进的算法相比展示了卓越的数值性能。]]></description>
      <guid>https://arxiv.org/abs/2410.00620</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有 Manly 变换成分的混合模型的 EM 梯度算法</title>
      <link>https://arxiv.org/abs/2410.00848</link>
      <description><![CDATA[arXiv:2410.00848v1 公告类型：新
摘要：Zhu 和 Melnykov (2018) 开发了一种模型，用于拟合混合模型，当组件来自 Manly 变换时。他们的 EM 算法在 M 步中使用 Nelder-Mead 优化来更新倾斜参数 $\boldsymbol{\lambda}_g$。当模型参数的初始估计值良好时，提出了一种替代的 EM 梯度算法，使用牛顿法的一步。]]></description>
      <guid>https://arxiv.org/abs/2410.00848</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>存在干扰的情况下优化治疗分配</title>
      <link>https://arxiv.org/abs/2410.00075</link>
      <description><![CDATA[arXiv:2410.00075v1 公告类型：交叉 
摘要：在影响力最大化 (IM) 中，目标是在给定预算的情况下，选择网络中最佳的实体集作为治疗目标，从而最大化总体效果。例如，在营销中，目标是针对最大化总体响应率的客户集，这既包括对目标客户的直接治疗效果，也包括针对这些客户产生的间接溢出效应。最近，提出了在存在网络干扰的情况下估计治疗效果的新方法。然而，如何利用这些模型做出更好的治疗分配决策的问题在很大程度上被忽视了。传统上，在提升建模 (UM) 中，根据估计的治疗效果对实体进行排名，并为排名靠前的实体分配治疗。由于在网络环境中，实体相互影响，因此 UM 排名方法将不是最优的。在网络环境中寻找最佳治疗分配的问题是组合性的，通常必须启发式地解决。为了填补 IM 和 UM 之间的空白，我们提出了 OTAPI：在存在干扰的情况下优化治疗分配，以使用治疗效果估计来寻找 IM 问题的解决方案。OTAPI 包含两个步骤。首先，训练因果估计器来预测网络环境中的治疗效果。其次，利用该估计器通过将其集成到经典 IM 算法中来确定最佳治疗分配。我们证明这种新方法在合成和半合成数据集上都优于经典 IM 和 UM 方法。]]></description>
      <guid>https://arxiv.org/abs/2410.00075</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过光谱匹配进行随机线性回归</title>
      <link>https://arxiv.org/abs/2410.00078</link>
      <description><![CDATA[arXiv:2410.00078v1 公告类型：交叉 
摘要：混洗线性回归 (SLR) 试图通过线性变换来估计潜在特征，而测量维度中的未知排列使这一过程变得复杂。该问题通过联合估计排列扩展了传统的最小二乘 (LS) 和最小绝对收缩和选择算子 (LASSO) 方法，从而产生了混洗 LS 和混洗 LASSO 公式。现有方法受排列恢复的组合复杂性限制，通常用于处理测量有限的小规模情况。相比之下，我们专注于大规模 SLR，特别适合测量样本丰富的环境。我们提出了一种光谱匹配方法，通过对齐测量和特征协方差的光谱分量来有效地解决排列问题。严格的理论分析表明，在给定足够数量的样本的情况下，我们的方法在混洗 LS 和混洗 LASSO 设置中都能实现准确的估计。此外，我们扩展了该方法以解决图像配准任务中同时进行姿态和对应性估计的问题。在合成数据集和真实世界图像配准场景上的实验表明，我们的方法在估计精度和配准性能方面均优于现有算法。]]></description>
      <guid>https://arxiv.org/abs/2410.00078</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>（几乎）顺利航行：通过条件数的可微分正则化实现神经网络的数值稳定性</title>
      <link>https://arxiv.org/abs/2410.00169</link>
      <description><![CDATA[arXiv:2410.00169v1 公告类型：交叉 
摘要：保持机器学习模型中的数值稳定性对于其可靠性和性能至关重要。保持网络层稳定性的一种方法是将权重矩阵的条件数作为正则项集成到优化算法中。然而，由于其不连续性和缺乏可微性，条件数不适用于梯度下降方法。本文介绍了一种新的正则化器，它几乎在任何地方都是可微的，并促进了具有低条件数的矩阵。特别是，我们推导出这个正则化器的梯度公式，它可以很容易地实现并集成到现有的优化算法中。我们展示了这种方法对 MNIST 图像的噪声分类和去噪的优势。]]></description>
      <guid>https://arxiv.org/abs/2410.00169</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>加速梯度下降优化和正则化的预处理</title>
      <link>https://arxiv.org/abs/2410.00232</link>
      <description><![CDATA[arXiv:2410.00232v1 公告类型：交叉 
摘要：加速训练算法，例如自适应学习率和各种规范化方法，被广泛使用但尚未完全理解。当引入正则化时，标准优化器（如自适应学习率）可能无法有效执行。这引发了对替代正则化方法的需求，以及如何正确结合正则化和预处理的问题。在本文中，我们使用预处理理论解决这些挑战，如下所示：（1）我们解释如何使用 AdaGrad、RMSProp 和 Adam 进行预处理来加速训练；（2）我们探索正则化和预处理之间的相互作用，概述选择正则化变量的不同选项，特别是我们讨论如何为梯度正则化实现这一点；（3）我们展示了正则化方法如何通过改进 Hessian 条件来加速训练，并讨论了这种观点如何导致新的预处理训练算法。我们的研究结果为理解各种加速技术和推导适当的正则化方案提供了一个统一的数学框架。]]></description>
      <guid>https://arxiv.org/abs/2410.00232</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机最优控制损失函数的分类</title>
      <link>https://arxiv.org/abs/2410.00345</link>
      <description><![CDATA[arXiv:2410.00345v1 公告类型：交叉 
摘要：随机最优控制 (SOC) 旨在指导噪声系统的行为，在科学、工程和人工智能领域有着广泛的应用。特别是，扩散和流匹配模型的奖励微调以及非规范化方法的采样可以重新定义为 SOC 问题。最近的一项研究引入了伴随匹配 (Domingo-Enrich 等人，2024)，这是一种用于 SOC 问题的损失函数，在奖励微调设置中远远优于现有的损失函数。这项工作的目标是阐明所有现有（和一些新的）SOC 损失函数之间的联系。也就是说，我们表明 SOC 损失函数可以分组为共享相同梯度期望的类，这意味着它们的优化景观是相同的；它们只是在梯度方差上有所不同。我们进行了简单的 SOC 实验来了解不同损失函数的优缺点。]]></description>
      <guid>https://arxiv.org/abs/2410.00345</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度 ReLU 与深度算子网络的神经尺度律：理论研究</title>
      <link>https://arxiv.org/abs/2410.00357</link>
      <description><![CDATA[arXiv:2410.00357v1 公告类型：交叉 
摘要：神经缩放定律在深度神经网络的性能中起着关键作用，并且已在广泛的任务中观察到。然而，理解这些缩放定律的完整理论框架仍然不完善。在本文中，我们探索深度算子网络的神经缩放定律，其中涉及学习函数空间之间的映射，重点关注 Chen 和 Chen 风格的架构。这些方法包括流行的深度算子网络 (DeepONet)，使用可学习基函数和依赖于输入函数的系数的线性组合来近似输出函数。我们通过分析其近似和泛化误差建立了一个量化神经缩放定律的理论框架。我们阐明了深度算子网络的近似和泛化误差与网络模型大小和训练数据大小等关键因素之间的关系。此外，我们解决了输入函数表现出低维结构的情况，使我们能够得出更严格的误差界限。这些结果也适用于深度 ReLU 网络和其他类似结构。我们的研究结果部分解释了算子学习中的神经缩放定律，并为其应用提供了理论基础。]]></description>
      <guid>https://arxiv.org/abs/2410.00357</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对多年空间变化的稳健交通预测</title>
      <link>https://arxiv.org/abs/2410.00373</link>
      <description><![CDATA[arXiv:2410.00373v1 公告类型：交叉 
摘要：时空图神经网络 (ST-GNN) 和 Transformer 的最新进展通过有效捕捉时间和空间相关性，展示了交通预测的巨大潜力。时空模型的泛化能力在最近的学术讨论中受到了相当大的关注。然而，还没有提出专门针对交通分布外 (OOD) 场景的实质性数据集。现有的 ST-OOD 方法要么局限于对现有数据进行测试，要么需要对数据集进行手动修改。因此，当前时空模型在 OOD 场景中的泛化能力仍未得到充分探索。在本文中，我们使用新提出的交通 OOD 基准研究了最先进的模型，令人惊讶的是，我们发现这些模型的性能显著下降。通过细致的分析，我们将这种下降归因于模型无法适应以前未观察到的空间关系。为了应对这一挑战，我们提出了一种新颖的混合专家 (MoE) 框架，该框架在训练期间学习一组图形生成器（即图元），并根据新的环境条件自适应地组合它们以生成新图形，以处理测试期间的空间分布变化。我们进一步将这一概念扩展到 Transformer 架构，实现了显着的改进。我们的方法既简约又有效，可以无缝集成到任何时空模型中，在解决空间动态方面的表现优于当前最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2410.00373</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>最佳因果表示和因果信息瓶颈</title>
      <link>https://arxiv.org/abs/2410.00535</link>
      <description><![CDATA[arXiv:2410.00535v1 公告类型：交叉 
摘要：为了有效地研究复杂的因果系统，通常需要构建表示来简化系统的各个部分，方法是丢弃不相关的细节，同时保留关键特征。信息瓶颈 (IB) 方法是表示学习中广泛使用的方法，它可以压缩随机变量，同时保留有关目标变量的信息。像 IB 这样的传统方法纯粹是统计性的，忽略了潜在的因果结构，因此不适合因果任务。我们提出了因果信息瓶颈 (CIB)，这是 IB 的因果扩展，它可以压缩一组选定的变量，同时保持对目标变量的因果控制。该方法产生的表示具有因果可解释性，可用于推理干预措施。我们展示的实验结果表明，学习到的表示能够准确地捕捉到预期的因果关系。]]></description>
      <guid>https://arxiv.org/abs/2410.00535</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稳定库马拉斯瓦米的分配</title>
      <link>https://arxiv.org/abs/2410.00660</link>
      <description><![CDATA[arXiv:2410.00660v1 公告类型：交叉 
摘要：大规模隐变量模型需要富有表现力的连续分布，以支持高效采样和低方差分化，这可以通过重新参数化技巧实现。Kumaraswamy (KS) 分布既富有表现力，又支持使用简单闭式逆 CDF 的重新参数化技巧。然而，它的采用仍然有限。我们识别并解决了逆 CDF 和 log-pdf 中的数值不稳定性，暴露了 PyTorch 和 TensorFlow 等库中的问题。然后，我们基于 KS 引入了简单且可扩展的隐变量模型，改善了上下文多臂老虎机中的探索-利用权衡，并增强了图神经网络链接预测的不确定性量化。我们的结果支持稳定的 KS 分布作为有界隐变量可扩展变分模型的核心组成部分。]]></description>
      <guid>https://arxiv.org/abs/2410.00660</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一致性编码器可能会反转时间维度</title>
      <link>https://arxiv.org/abs/2410.00680</link>
      <description><![CDATA[arXiv:2410.00680v1 公告类型：交叉 
摘要：我们有时会在基于 Conformer 的全局注意力编码器-解码器 (AED) 模型中观察到单调递减的交叉注意力权重。进一步的研究表明，Conformer 编码器在时间维度上内部反转了序列。我们分析了解码器交叉注意力机制的初始行为，发现它鼓励 Conformer 编码器自注意力在初始帧和所有其他信息帧之间建立连接。此外，我们表明，在训练的某个时刻，Conformer 的自注意力模块开始在前馈模块上主导输出，然后只允许反转的信息通过。我们提出了几种避免这种翻转的方法和想法。此外，我们研究了一种新方法，通过使用标签对数概率相对于编码器输入帧的梯度来获得标签帧位置对齐。]]></description>
      <guid>https://arxiv.org/abs/2410.00680</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>研究大型语言模型中模型复杂性的影响</title>
      <link>https://arxiv.org/abs/2410.00699</link>
      <description><![CDATA[arXiv:2410.00699v1 公告类型：交叉 
摘要：基于预训练微调范式的大型语言模型 (LLM) 已成为解决自然语言处理任务的关键，始终实现最先进的性能。然而，关于模型复杂性如何影响微调性能的理论理解仍然具有挑战性，尚未得到很好的探索。在本文中，我们专注于自回归 LLM，并建议采用隐马尔可夫模型 (HMM) 对其进行建模。基于 HMM 建模，我们研究了模型复杂性与下游任务中的泛化能力之间的关系。具体来说，我们考虑了一种流行的下游任务调整范式，即头部调整，其中所有预训练参数都被冻结，并且只有单个头部在预训练的 LLM 上进行训练。我们的理论分析表明，风险最初随着模型复杂性的增加而增加，然后降低，表现出“双下降”现象。在这种情况下，初始“下降”是退化的，这意味着当模型大小为零时，偏差和方差达到平衡的“最佳点”。获得本研究结论面临多项挑战，主要围绕有效建模自回归 LLM 和下游任务，以及对多元回归进行全面的风险分析。我们的研究通过对 HMM 生成的数据进行的实验得到证实，这些实验提供了实证支持并与我们的理论见解保持一致。]]></description>
      <guid>https://arxiv.org/abs/2410.00699</guid>
      <pubDate>Wed, 02 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>