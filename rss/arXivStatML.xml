<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 02 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于语言模型对齐的自我博弈偏好优化</title>
      <link>https://arxiv.org/abs/2405.00675</link>
      <description><![CDATA[arXiv:2405.00675v1 公告类型：交叉 
摘要：传统的强化学习人类反馈 (RLHF) 方法依赖于 Bradley-Terry 模型等参数模型，无法捕捉人类偏好中的不传递性和非理性。最近的进展表明，直接使用偏好概率可以更准确地反映人类偏好，从而实现更灵活、更准确的语言模型对齐。在本文中，我们提出了一种基于自我对弈的语言模型对齐方法，该方法将问题视为一个恒定和双人游戏，旨在识别纳什均衡策略。我们的方法称为 \textit{自我对弈偏好优化} (SPPO)，通过迭代策略更新近似纳什均衡，并享有理论上的收敛保证。我们的方法可以有效地增加所选响应的对数似然并降低被拒绝响应的对数似然，这是无法通过对称成对损失（例如直接偏好优化 (DPO) 和身份偏好优化 (IPO)）轻松实现的。在我们的实验中，仅使用 UltraFeedback 数据集中的 60k 条提示（无响应）且没有任何提示增强，通过利用仅具有 0.4B 个参数的预训练偏好模型 PairRM，SPPO 可以从微调 Mistral-7B-Instruct-v0.2 中获得一个模型，该模型在 AlpacaEval 2.0 上对 GPT-4-Turbo 实现了 28.53% 的最佳长度控制胜率。它还优于 MT-Bench 和 Open LLM 排行榜上的（迭代）DPO 和 IPO。值得注意的是，SPPO 的强劲表现是在没有 GPT-4 或其他更强大的语言模型的额外外部监督（例如响应、偏好等）的情况下实现的。]]></description>
      <guid>https://arxiv.org/abs/2405.00675</guid>
      <pubDate>Fri, 03 May 2024 01:02:08 GMT</pubDate>
    </item>
    <item>
      <title>高维岭回归中的最优偏差校正和有效推理：封闭式解决方案</title>
      <link>https://arxiv.org/abs/2405.00424</link>
      <description><![CDATA[arXiv:2405.00424v1 公告类型：交叉
摘要：岭回归是大数据计量经济学中不可或缺的工具，但存在影响统计效率和可扩展性的偏差问题。当维度$p$小于样本大小$n$时，我们引入迭代策略来有效地纠正偏差。对于 $p&gt;n$，我们的方法通过响应的线性变换最佳地将偏差降低到无法实现的水平。我们采用岭筛选（RS）方法来处理 $p&gt;n$ 时的剩余偏差，创建适合偏差校正的简化模型。在某些条件下，所选模型嵌套真实模型，使 RS 成为一种新颖的变量选择方法。我们为 $p&lt; n$ 和 $p&gt;n$ 建立了去偏岭估计器的渐近性质和有效推论，其中 $p$ 和 $n$ 可能随着迭代次数而向无穷大增长。我们的方法使用模拟和真实数据示例进行了验证，为岭回归推理中的偏差挑战提供了封闭式解决方案。]]></description>
      <guid>https://arxiv.org/abs/2405.00424</guid>
      <pubDate>Fri, 03 May 2024 01:02:07 GMT</pubDate>
    </item>
    <item>
      <title>通过 $f$-Divergence 和 $\alpha$-R\'enyi Divergence 进行鲁棒半监督学习</title>
      <link>https://arxiv.org/abs/2405.00454</link>
      <description><![CDATA[arXiv:2405.00454v1 公告类型：交叉
摘要：本文研究了一系列适用于半监督学习中自我训练方法的经验风险函数和正则化方法。这些方法从各种散度度量中汲取灵感，例如 $f$-散度和 $\alpha$-R\&#39;enyi 散度。受到源于散度的理论基础（即 $f$-散度和 $\alpha$-R\&#39;enyi 散度）的启发，我们还提供了宝贵的见解，以增强对我们的经验风险函数和正则化技术的理解。在伪标签和熵最小化技术作为有效半监督学习的自训练方法中，自训练过程在真实标签和伪标签（嘈杂的伪标签）之间存在一些固有的不匹配以及我们的一些经验风险对于嘈杂的伪标签，函数是鲁棒的。在某些条件下，与传统的自我训练方法相比，我们的经验风险函数表现出更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.00454</guid>
      <pubDate>Fri, 03 May 2024 01:02:07 GMT</pubDate>
    </item>
    <item>
      <title>重新审视 Skip-Gram 负采样：维度正则化以在图嵌入中更有效地保持相异性</title>
      <link>https://arxiv.org/abs/2405.00172</link>
      <description><![CDATA[arXiv:2405.00172v1 公告类型：交叉
摘要：各种图嵌入目标分解为两个部分：一个部分吸引被认为相似的节点的嵌入，另一个部分排斥被认为不相似的节点的嵌入。由于现实世界的图是稀疏的，并且不同对的数量随节点数量呈二次方增长，因此 Skip-Gram 负采样 (SGNS) 已成为一种流行且高效的排斥方法。 SGNS 排斥不同节点样本中的每个节点，而不是排斥所有不同节点。在这项工作中，我们表明节点级排斥总的来说是节点嵌入维度的近似重新居中。这种维度操作比节点操作更具可扩展性。尺寸方法除了更有效之外，还可以对排斥力产生更简单的几何解释。我们的结果将自监督学习文献的发现扩展到了skip-gram模型，在skip-gram节点对比和维度正则化之间建立了联系。我们证明，在大图的限制下，在温和的正则性条件下，原始节点排斥目标通过维度正则化收敛到优化。我们利用这一观察结果提出了一种算法增强框架，该框架可以使用 SGNS 加速任何现有算法（有监督或无监督）。该框架优先考虑节点吸引，并用维度正则化取代 SGNS。我们实例化了 LINE 和 node2vec 的通用框架，并表明增强算法保留了下游性能，同时显着提高了效率。]]></description>
      <guid>https://arxiv.org/abs/2405.00172</guid>
      <pubDate>Fri, 03 May 2024 01:02:06 GMT</pubDate>
    </item>
    <item>
      <title>利用活动子空间捕获分子设计深度生成模型中的认知模型不确定性</title>
      <link>https://arxiv.org/abs/2405.00202</link>
      <description><![CDATA[arXiv:2405.00202v1 公告类型：交叉
摘要：深度生成模型一直在加速材料和药物设计中的逆向设计过程。与典型分子设计框架中的对应属性预测器不同，生成分子设计模型在不确定性量化（UQ）方面的努力较少，因为其大量参数给贝叶斯推理带来了计算挑战。在这项工作中，我们重点关注连接树变分自动编码器（JT-VAE），这是一种流行的生成分子设计模型，并通过利用低维活动子空间来捕获模型参数的不确定性来解决这个问题。具体来说，我们近似活动子空间参数的后验分布，以估计极高维参数空间中的认知模型不确定性。所提出的 UQ 方案不需要更改模型架构，使其易于适用于任何预训练模型。我们的实验通过探索认知不确定性下的模型多样性，证明了基于 AS 的 UQ 的有效性及其对分子优化的潜在影响。]]></description>
      <guid>https://arxiv.org/abs/2405.00202</guid>
      <pubDate>Fri, 03 May 2024 01:02:06 GMT</pubDate>
    </item>
    <item>
      <title>有序分类的适形风险控制</title>
      <link>https://arxiv.org/abs/2405.00417</link>
      <description><![CDATA[arXiv:2405.00417v1 公告类型：交叉
摘要：作为标准保形预测方法的自然延伸，最近开发了几种保形风险控制方法并将其应用于各种学习问题。在这项工作中，我们寻求控制有序分类任务预期的共形风险，这对许多实际问题有广泛的应用。为此，我们首先在共形风险控制框架中制定了序数分类任务，并提供了风险控制方法的理论风险界限。然后，我们提出了两种专门为序数分类任务设计的损失函数，并开发了相应的算法来确定每种情况的预测集，以将其风险控制在所需的水平。我们证明了所提出方法的有效性，并分析了三个不同数据集（包括模拟数据集、UTKFace 数据集和糖尿病视网膜病变检测数据集）上两种类型风险之间的差异。]]></description>
      <guid>https://arxiv.org/abs/2405.00417</guid>
      <pubDate>Fri, 03 May 2024 01:02:06 GMT</pubDate>
    </item>
    <item>
      <title>不精确马尔可夫半群及其遍历性</title>
      <link>https://arxiv.org/abs/2405.00081</link>
      <description><![CDATA[arXiv:2405.00081v1 公告类型：交叉
摘要：我们引入了不精确马尔可夫半群的概念。它使我们能够将具有不精确转移概率的马尔可夫链和过程视为（扩散的集合）算子，从而解锁几何、泛函分析和（高维）概率的技术来研究它们的遍历行为。我们证明，如果不精确马尔可夫半群的初始分布已知且不变，在某些还涉及状态空间几何的条件下，最终转移概率的模糊性会消失。我们将这种属性称为不精确马尔可夫半群的遍历性，并将其与经典（伯克霍夫）的遍历性概念联系起来。当状态空间是欧几里得或黎曼流形时，以及当它是任意可测空间时，我们都证明了遍历性。还讨论了我们的发现对于机器学习和计算机视觉领域的重要性。]]></description>
      <guid>https://arxiv.org/abs/2405.00081</guid>
      <pubDate>Fri, 03 May 2024 01:02:05 GMT</pubDate>
    </item>
    <item>
      <title>对于密集网络或饱和动态的网络重建，复杂传染的效果优于简单传染</title>
      <link>https://arxiv.org/abs/2405.00129</link>
      <description><![CDATA[arXiv:2405.00129v1 公告类型：交叉
摘要：网络科学家经常使用复杂的动态过程来描述网络传染，但拟合传染模型的工具通常假设简单的动力学。在这里，我们通过开发一种非参数方法来重建网络和一系列节点状态的动态，并使用打破简单成对传染和复杂基于邻域传染之间二分法的模型来解决这一差距。然后，我们表明，当通过复杂传染的镜头观察时，如果网络密集或动态饱和，则更容易重建网络，否则简单的传染会更好。]]></description>
      <guid>https://arxiv.org/abs/2405.00129</guid>
      <pubDate>Fri, 03 May 2024 01:02:05 GMT</pubDate>
    </item>
    <item>
      <title>BayesBlend：在 Python 中使用伪贝叶斯模型平均、堆叠和分层堆叠进行轻松模型混合</title>
      <link>https://arxiv.org/abs/2405.00158</link>
      <description><![CDATA[arXiv:2405.00158v1 公告类型：交叉
摘要：如果模型经过最佳加权以最大化预测性能，则对多个竞争推理模型的平均预测通常优于任何单个模型的预测。在所谓的 $\mathcal{M}$-开放设置中尤其如此，其中真实模型不在候选模型集中，并且可能既不能在数学上具体化，也不能精确已知。这种模型平均的实践在统计学和机器学习中有着丰富的历史，目前有多种方法来估计构建模型平均预测分布的权重。尽管如此，很少有现有的软件包可以从各种可用的方法中估计模型权重，并且没有一个软件包可以根据估计的权重将模型预测混合成连贯的预测分布。在本文中，我们介绍了 BayesBlend Python 包，它提供了一个用户友好的编程接口来估计权重并混合多个（贝叶斯）模型的预测分布。 BayesBlend 实现伪贝叶斯模型平均、堆叠以及独特的分层贝叶斯堆叠来估计模型权重。我们通过保险损失建模示例演示了 BayesBlend 的用法。]]></description>
      <guid>https://arxiv.org/abs/2405.00158</guid>
      <pubDate>Fri, 03 May 2024 01:02:05 GMT</pubDate>
    </item>
    <item>
      <title>异步联邦学习的排队动态</title>
      <link>https://arxiv.org/abs/2405.00017</link>
      <description><![CDATA[arXiv:2405.00017v1 公告类型：交叉
摘要：我们研究具有潜在不同计算速度的节点的异步联邦学习机制。在这样的环境中，每个节点都可以处理具有潜在延迟的模型，并按照自己的节奏对中央服务器进行更新。此类算法的现有分析通常取决于最大节点延迟等棘手的量，并且不考虑系统的底层排队动态。在本文中，我们提出了一种中央服务器的非均匀采样方案，考虑到相关计算图的封闭杰克逊网络结构，该方案允许更低的延迟和更好的复杂性。我们的实验清楚地表明，在图像分类问题上，我们的方法相对于当前最先进的异步算法有显着改进。]]></description>
      <guid>https://arxiv.org/abs/2405.00017</guid>
      <pubDate>Fri, 03 May 2024 01:02:04 GMT</pubDate>
    </item>
    <item>
      <title>从线性到可线性化优化：一种应用于稳态和非稳态 DR 子模优化的新颖框架</title>
      <link>https://arxiv.org/abs/2405.00065</link>
      <description><![CDATA[arXiv:2405.00065v1 公告类型：交叉 
摘要：本文引入了上线性化/可二次化函数的概念，该类在各种设置中扩展了凹性和 DR 子模性，包括不同凸集上的单调和非单调情况。设计了一种通用元算法，将线性/二次最大化算法转换为优化上二次化函数的算法，为解决凹和 DR 子模优化问题提供了统一的方法。本文将这些结果扩展到多种反馈设置，促进了半匪/一阶反馈和匪/零阶反馈之间的转换，以及一阶/零阶反馈和半匪/匪反馈之间的转换。利用这个框架，使用跟随扰动领导者 (FTPL) 和其他算法作为线性/凸优化的基础算法，推导出新的无投影算法，在各种情况下改进了最先进的结果。 DR 子模最大化获得了动态和自适应遗憾保证，标志着首个在这些设置中实现此类保证的算法。值得注意的是，与现有的最先进结果相比，本文以更少的假设实现了这些进步，强调了其广泛的适用性和对非凸优化的理论贡献。]]></description>
      <guid>https://arxiv.org/abs/2405.00065</guid>
      <pubDate>Fri, 03 May 2024 01:02:04 GMT</pubDate>
    </item>
    <item>
      <title>对焦点损失的几何见解：降低曲率以增强模型校准</title>
      <link>https://arxiv.org/abs/2405.00442</link>
      <description><![CDATA[arXiv:2405.00442v1 公告类型：新
摘要：在决策情况下实施机器学习算法的关键因素不仅是模型的准确性，还有其置信度。为了方便起见，分类问题中模型的置信度通常由 softmax 函数的输出向量给出。然而，众所周知，这些值与实际预期的模型置信度存在很大偏差。这个问题称为模型校准，已经得到了广泛的研究。解决此任务的最简单技术之一是焦点损失，它是通过引入一个正参数对交叉熵的概括。尽管由于该思想和形式化的简单性而存在许多相关研究，但对其行为的理论分析仍然不足。在这项研究中，我们的目标是通过几何重新解释该函数来了解焦点损失的行为。我们的分析表明，焦点损失在训练模型时降低了损失表面的曲率。这表明曲率可能是实现模型校准的重要因素之一。我们设计数值实验来支持这一猜想，以揭示焦点损失的行为以及校准性能和曲率之间的关系。]]></description>
      <guid>https://arxiv.org/abs/2405.00442</guid>
      <pubDate>Fri, 03 May 2024 01:02:03 GMT</pubDate>
    </item>
    <item>
      <title>高维回归中的缩放和重整化</title>
      <link>https://arxiv.org/abs/2405.00592</link>
      <description><![CDATA[arXiv:2405.00592v1 公告类型：新
摘要：本文利用随机矩阵理论和自由概率的基本工具，对各种高维岭回归模型的训练和泛化性能进行了简洁的推导。我们针对这些主题的最新结果进行了介绍和回顾，面向具有物理学和深度学习背景的读者。训练和泛化误差的解析公式是直接从自由概率的 $S$ 变换的属性中通过几行代数获得的。这样可以直接识别模型性能中幂律缩放的来源。我们计算一大类随机特征模型的泛化误差。我们发现在所有模型中，$S$ 变换对应于训练测试泛化差距，并产生广义交叉验证估计器的类似物。使用这些技术，我们为具有结构化协变量的一类非常通用的随机特征模型导出细粒度的偏差-方差分解。这些新颖的结果使我们能够发现随机特征模型的缩放机制，其中特征引起的方差限制了过度参数化设置中的性能。我们还演示了随机特征模型中的各向异性权重结构如何限制性能并导致超参数化设置中有限宽度校正的非平凡指数。我们的结果扩展并为早期的神经标度定律模型提供了统一的视角。]]></description>
      <guid>https://arxiv.org/abs/2405.00592</guid>
      <pubDate>Fri, 03 May 2024 01:02:03 GMT</pubDate>
    </item>
    <item>
      <title>从经验观察到普遍性：基于高斯混合的输入的深度学习动力学</title>
      <link>https://arxiv.org/abs/2405.00642</link>
      <description><![CDATA[arXiv:2405.00642v1 公告类型：新
摘要：这项研究通过深入研究神经网络的动力学，其输入展示了高斯混合（GM）的结构特征，拓宽了深度学习的理论框架范围。我们分析了 GM 结构输入下神经网络的动力学如何偏离基于简单高斯结构的传统理论的预测。我们工作的一个启示是，即使使用标准化的 GM 输入，也观察到神经网络动力学与传统理论的收敛，突显出意想不到的普遍性。我们发现标准化，特别是与某些非线性函数结合使用，在这种现象中起着至关重要的作用。因此，尽管 GM 分布具有复杂性和多样性，我们证明神经网络表现出符合简单高斯框架下预测的渐近行为。]]></description>
      <guid>https://arxiv.org/abs/2405.00642</guid>
      <pubDate>Fri, 03 May 2024 01:02:03 GMT</pubDate>
    </item>
    <item>
      <title>树结构高斯混合断棍过程的变分贝叶斯方法</title>
      <link>https://arxiv.org/abs/2405.00385</link>
      <description><![CDATA[arXiv:2405.00385v1 公告类型：新
摘要：上下文树源贝叶斯编码算法是信息论中文本压缩中贝叶斯树估计的成功范例。该算法提供了后验树分布的有效参数表示及其参数的精确更新。我们将该算法应用于机器学习中的聚类任务。更具体地说，我们将其应用于树结构破棍过程（TS-SBP）混合模型的贝叶斯估计。对于TS-SBP混合模型，迄今为止仅提出了马尔可夫链蒙特卡罗方法，而尚未提出任何变分贝叶斯方法。在本文中，我们提出了一种变分贝叶斯方法，其子程序类似于上下文树源的贝叶斯编码算法。我们通过对玩具示例进行数值实验来确认其行为。]]></description>
      <guid>https://arxiv.org/abs/2405.00385</guid>
      <pubDate>Fri, 03 May 2024 01:02:02 GMT</pubDate>
    </item>
    </channel>
</rss>