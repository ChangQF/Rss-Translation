<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Mon, 17 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>低度猜想和相关随机图中应用的算法连续性</title>
      <link>https://arxiv.org/abs/2502.09832</link>
      <description><![CDATA[ARXIV：2502.09832V1公告类型：新 
摘要：在本文中，假设低度猜想的自然加强，我们为两个问题提供了计算硬度的证据：（1）稀疏相关的ERD \ H {O} S-R \&#39;中（部分）匹配的恢复问题（部分）匹配的恢复问题Enyi图$ \ MATHCAL g（n，q; \ rho）$当边缘密度$ q = n^{ -  1+o（1）} $和相关性$ \ rho &lt;\ sqrt {\ alpha} $ lies在水獭的阈值下方，在\ cite {ddl23+}中求解了剩余的问题; （2）相关的稀疏随机块模型$ \ MATHCAL S（n，\ tfrac {\ lambda} {n} {n}; k，\ epsilon; s）$与一对独立的随机块$ \ Mathcal S之间的检测问题（n，\ tfrac {\ lambda s} {n}; k，\ epsilon）$当$ \ epsilon^2 \ lambda s &lt;1 $时，位于Kesten-Stigum（ks）阈值和$ S &lt;\ sqrt {\ sqrt {\ sqrt {\ sqrt {\ sqrt { Alpha} $位于Otter的阈值以下，在\ cite {CDGL24+}中求解了剩余的问题。
  我们证明中的主要成分之一是根据基于其低度优势的界限，在两个概率度量之间得出某些形式的\ emph {算法连续性}。更确切地说，请考虑基于示例$ \ mathsf y $的两个概率度量之间的高维假设测试问题。我们表明，如果低度优势$ \ mathsf {adv} _ {\ leq d} \ big（\ frac {\ mathrm {d} \ mathbb {p}}} {\ mathrm {d} \ big）= o（1）$，然后（假设低度猜想）没有有效的算法$ \ mathcal a $，以至于$ \ mathbb {q}（\ mathcal a（\ mathcal a（\ mathsf y）= 0）= 0）= 0）= 0）= 0） 1-o（1）$和$ \ Mathbb {p}（\ Mathcal a（\ Mathsf y）= 1）= \ Omega（1）$。该框架为执行不同推理任务减少的有用工具提供了有用的工具。]]></description>
      <guid>https://arxiv.org/abs/2502.09832</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于保形回归中的体积最小化</title>
      <link>https://arxiv.org/abs/2502.09985</link>
      <description><![CDATA[ARXIV：2502.09985V1公告类型：新 
摘要：我们研究了拆分保形回归中的量最佳性问题，与覆盖范围的控制相比，这个话题仍然很差。利用校准步骤可以看作是经验体积最小化问题的事实，我们首先在经典拆分方法返回的间隔的多余体积损失上得出有限样本的上限。这个重要的数量衡量用拆分方法获得的间隔与最短的甲骨文预测间隔之间的长度差。然后，我们介绍了一种修改学习步骤的方法，以便选择基本预测函数，以最大程度地减少返回间隔的长度。特别是，我们对努力产生的预测集的过剩体积损失的理论分析揭示了学习和校准步骤之间的联系，尤其是基本预测指标函数类别选择的影响。我们还引入了Ad-forfort，这是先前方法的扩展，该方法会产生大小适应协变量值的间隔。最后，我们评估了方法论的经验绩效和鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2502.09985</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用经验损失对学习系数的估计</title>
      <link>https://arxiv.org/abs/2502.09998</link>
      <description><![CDATA[ARXIV：2502.09998V1公告类型：新 
摘要：学习系数在分析信息标准的性能中起着至关重要的作用，例如广泛适用的信息标准（WAIC）和广泛适用的贝叶斯信息标准（WBIC），Sumio Watanabe开发了用于评估模型泛化能力的Sumio Watanabe。在常规统计模型中，学习系数由D/2给出，其中D是参数空间的维度。更一般而言，它被定义为从kullback-leibler差异和先前的分布得出的Zeta函数的极点的绝对值。但是，除了诸如减少级别回归之类的特定情况外，学习系数不能以封闭形式得出。渡边提出了一种数值方法来估计学习系数，IMAI进一步完善了其收敛性。这些方法利用了WBIC的渐近行为，并且随着样本量的增长，在统计上是一致的。在本文中，我们提出了一种新颖的数值估计方法，该方法从根本上与以前的方法不同，并利用了渡边引入的新数量“经验损失”。通过数值实验，我们证明了我们所提出的方法与渡边和IMAI相比表现出较低的偏差和较低的方差。此外，我们提供了理论分析，阐明了为什么我们的方法优于现有技术并提供支持我们发现的经验证据。]]></description>
      <guid>https://arxiv.org/abs/2502.09998</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>改善了多项式逻辑匪徒的在线置信度范围</title>
      <link>https://arxiv.org/abs/2502.10020</link>
      <description><![CDATA[ARXIV：2502.10020V1公告类型：新 
摘要：在本文中，我们提出了一种改进的在线置信度，以限制多项式逻辑（MNL）模型，并将此结果应用于MNL匪徒，从而实现了差异依赖性的最佳遗憾。最近，Lee＆Oh（2024）建立了在MNL模型的在线信心，并在MNL强盗中获得了几乎最小的遗憾。但是，他们的结果仍然取决于未知参数$ b $的规范结合性以及可能结果的最大大小$ k $。为了解决这个问题，我们首先获得了$ o \ left（\ sqrt {d \ log t} + b \ right）$的在线置信度，这是$ o（b \ sqrt {D } \ log t \ log k）$（Lee＆Oh，2024）。这主要是通过建立MNL损失的更紧密的自信特性来实现的，并引入了一个新颖的中介术语来绑定估计误差。使用这种新的在线置信界，我们提出了一种恒定的算法，即ofu-mnl ++，该算法获得了$ o \ big的差异依赖性遗憾（d \ log t \ sqrt {\ smash [b smash [b] {\ sum_ {\ sum_ {t = 1}^t} \ sigma_t^2} \ big）$用于足够大的$ t $，其中$ \ sigma_t^2 $表示奖励的差异，$ t $ t $，$ d $是上下文的尺寸，$ t $是回合的总数。此外，我们引入了一个最大似然估计（MLE）基于任何时间的基于基于的算法，ofu-mn $^2 $ l，poly（$（b）$） -  $ o \ big的免费遗憾（d \ log（bt） ）\ sqrt {\ smash [b] {\ sum_ {t = 1}^t} \ sigma_t^2} \ big）$。]]></description>
      <guid>https://arxiv.org/abs/2502.10020</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>组合加强学习与偏好反馈</title>
      <link>https://arxiv.org/abs/2502.10158</link>
      <description><![CDATA[ARXIV：2502.10158V1公告类型：新 
摘要：在本文中，我们考虑组合加强学习与偏好反馈，其中学习代理依次提供了一项动作 - 各种各样的项目向 - 一个用户，他们的偏好反馈遵循多项式逻辑逻辑（MNL）模型。该框架使我们能够建模现实世界中的场景，尤其是涉及长期用户参与度的情况，例如在推荐系统和在线广告中。但是，该框架面临两个主要挑战：（1）与传统的MNL土匪不同，每项的未知价值仅针对单步优先反馈，以及（2）在组合行动中保持可拖动分类的同时确保乐观的难度具有未知值的空间。在本文中，我们假设一个上下文MNL偏好模型，其中平均实用程序是线性的，并且每个项目的值通过一般函数近似。我们提出了一种算法MNL-VQL，该算法解决了这些挑战，使其在计算和统计学上都有效率。作为一个特殊情况，对于线性MDP（带有MNL偏好反馈），我们在此框架中建立了第一个遗憾的下限，并表明MNL-VQL几乎实现了最小的遗憾。据我们所知，这是第一项提供统计保证与偏好反馈的统计保证的工作。]]></description>
      <guid>https://arxiv.org/abs/2502.10158</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>改编：调整单变量基础模型以预测概率的多元时间序列</title>
      <link>https://arxiv.org/abs/2502.10235</link>
      <description><![CDATA[ARXIV：2502.10235V1公告类型：新 
摘要：预训练的基础模型（FMS）在单变量时间序列预测任务中表现出了出色的性能。但是，一些实际的挑战仍然存在，包括管理特征之间复杂的依赖性以及量化预测中的不确定性。这项研究旨在通过引入适配器来应对这些关键局限性；功能空间转换有助于有效使用预训练的单变量时间序列FMS进行多元任务。适配器通过将多元输入投影到合适的潜在空间并独立应用于每个维度来运行。受到代表学习和部分随机贝叶斯神经网络的文献的启发，我们提出了一系列适配器和优化/推理策略。在合成和现实世界数据集上进行的实验证实了适配器的功效，与基线方法相比，预测准确性和不确定性量化方面有很大的增强。我们的框架，适应定位器作为在多元上下文中利用时间序列FMS的模块化，可扩展且有效的解决方案，从而促进了其在现实应用程序中的更广泛采用。我们在https://github.com/abenechehab/adapts上发布代码。]]></description>
      <guid>https://arxiv.org/abs/2502.10235</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>广义平行回火：通过流和扩散的灵活复制品交换</title>
      <link>https://arxiv.org/abs/2502.10328</link>
      <description><![CDATA[ARXIV：2502.10328V1公告类型：新 
摘要：平行回火（PT）是一种经典的MCMC算法，旨在利用并行计算通过退火从高维，多模式或其他复杂分布中有效地采样。 PT标准公式的一个局限性是通过有效的样本量或往返速率来衡量产生高质量样本所需的计算资源的增长，以衡量越来越具有挑战性的分布。为了解决这个问题，我们提出了一个框架：广义平行回火（GEPT），它允许在平行回火中纳入现代生成建模的最新进展，例如标准化流量和扩散模型，同时保持与MCMC-相同的理论保证基于方法。例如，我们表明，这使我们能够以并行的方式利用扩散模型，绕过大量步骤的通常计算成本来生成质量样本。此外，我们从经验上证明，GEPT可以提高样本质量并减少处理经典算法所需的计算资源的增长。]]></description>
      <guid>https://arxiv.org/abs/2502.10328</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>单步一致的扩散采样器</title>
      <link>https://arxiv.org/abs/2502.07579</link>
      <description><![CDATA[ARXIV：2502.07579V1公告类型：交叉 
摘要：来自非规范目标分布的抽样是机器学习和统计数据中的一项基本而又具有挑战性的任务。现有的采样算法通常需要许多迭代步骤来生产高质量的样本，从而导致高计算成本限制了它们在时间敏感或资源约束的设置中的实用性。在这项工作中，我们引入了一致的扩散采样器，这是​​一类新的采样器，旨在在一个步骤中生成高保真样本。我们首先开发了一种蒸馏算法，以训练从验证的扩散模型中训练一致的扩散采样器，而无需预购大型样品数据集。我们的算法直接从扩散过程中利用了不完整的采样轨迹和嘈杂的中间状态。我们进一步提出了一种方法来训练一致的扩散采样器从划痕训练一致的扩散采样器，从而通过训练单个模型来完全摊销探索，该模型既可以执行扩散采样，又可以使用自抗性损失跳过中间步骤。通过对各种非平均分布的广泛实验，我们表明我们的方法使用传统扩散采样器所需的不到1％的网络评估产生了高保真样本。]]></description>
      <guid>https://arxiv.org/abs/2502.07579</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图像超分辨率，并通过共形生成模型保证</title>
      <link>https://arxiv.org/abs/2502.09664</link>
      <description><![CDATA[ARXIV：2502.09664V1公告类型：交叉 
摘要：对于图像超分辨率的生成ML基础模型的使用日益增加，要求鲁棒和可解释的不确定性量化方法。我们通过提出一种基于共形预测技术的新方法来解决这种需求，以创建一个“信心面具”，能够可靠，直觉地传达可以信任生成的图像的地方。我们的方法适用于任何黑盒生成模型，包括锁定不透明API的模型，仅需要易于达到的校准数据，并且可以通过选择本地图像相似性度量来高度自定义。我们证明了跨越保真度误差控制的方法（根据我们的本地图像相似度度量），重建质量以及面对数据泄漏时的鲁棒性。最后，我们通过经验评估这些结果并确定方法的扎实性能。]]></description>
      <guid>https://arxiv.org/abs/2502.09664</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>k-llmmeans：总结为可解释和可扩展LLM的文本群集的质心</title>
      <link>https://arxiv.org/abs/2502.09667</link>
      <description><![CDATA[ARXIV：2502.09667V1公告类型：交叉 
摘要：我们介绍了K-llmmeans，这是对K-均值聚类算法的一种新颖的修改，该算法利用LLMS将文本摘要作为群集质心生成，从而捕获上下文和语义上的细微差别，通常依赖于纯粹的数值文档嵌入数字手段。这种修改保留了K-均值的属性，同时提供了更大的解释性：群集质心由LLM生成的摘要表示，其嵌入指南群集分配。我们还提出了一个迷你批次变体，可实现有效的在线聚类，用于流式文本数据并提供不断发展的群集质心的实时可解释性。通过大量的模拟，我们表明我们的方法的表现优于多个指标的香草k-均值，而仅产生不随数据集大小扩展的适度LLM使用情况。最后，我们提出了一项案例研究，展示了顺序文本流中不断发展的集群质心的解释性。作为评估的一部分，我们从Stackexchange编译了一个新数据集，为文本流集群提供了基准。]]></description>
      <guid>https://arxiv.org/abs/2502.09667</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高维审查的MIDAS逻辑回归用于公司生存预测</title>
      <link>https://arxiv.org/abs/2502.09740</link>
      <description><![CDATA[ARXIV：2502.09740V1公告类型：交叉 
摘要：本文解决了预测公司困扰的挑战，这是一个以三个关键统计障碍为标志的问题：（i）正确的审查，（ii）高维预测变量和（iii）混合频率数据。为了克服这些复杂性，我们引入了一种新型的高维MIDAS（混合数据采样）逻辑回归。我们的方法通过使用稀疏组的惩罚来处理众多混合频率预测因子，通过反概率加权来处理审查，并实现准确的估计。我们为估计误差，审查，MIDAS近似误差和沉重的尾巴建立有限样本界限。通过蒙特卡洛模拟证明了该方法的出色性能。最后，我们介绍了我们方法的广泛应用，以预测中国上市公司的财务困境。我们的新型程序是在“生存”软件包中实现的。]]></description>
      <guid>https://arxiv.org/abs/2502.09740</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过变压器解决经验贝叶斯</title>
      <link>https://arxiv.org/abs/2502.09844</link>
      <description><![CDATA[ARXIV：2502.09844V1公告类型：交叉 
摘要：这项工作将现代的AI工具（变形金刚）应用于解决最古老的统计问题之一：泊松在经验贝叶斯（Poisson-EB）设置下。在Poisson-eb中，根据$ x = \ mathrm {poisson}（\ theta）$，估算了估计从未知的先前$ \ pi $采样的高维平均值vector $ \ theta $（带有IID坐标）。变压器模型已在一组合成生成的对$（x，\ theta）$上进行了预训练，并通过适应未知$ \ pi $来学习在文本学习（ICL）中进行进行。从理论上讲，我们表明，足够宽的变压器可以在甲骨文估算器中获得消失的遗憾，而甲骨文估计器知道$ \ pi $随着尺寸增长到无穷大。实际上，我们发现已经很小的型号（100K参数）能够超过运行时和验证损失的最佳经典算法（非参数最大可能性，或NPMLE），我们将其计算为脱离分布的合成数据以及现实世界中的数据集（NHL曲棍球，MLB棒球，BookCorpusopen）。最后，通过使用线性探针，我们确认变压器的EB估计器似乎内部工作与NPMLE或Robbins的估计器不同。]]></description>
      <guid>https://arxiv.org/abs/2502.09844</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梯度GA：用于药物分子设计的梯度遗传算法</title>
      <link>https://arxiv.org/abs/2502.09860</link>
      <description><![CDATA[ARXIV：2502.09860V1公告类型：交叉 
摘要：分子发现为化学工业带来了巨大的好处。开发了各种分子设计技术来鉴定具有理想特性的分子。传统的优化方法，例如遗传算法，继续在多个分子设计基准中取得最新的结果。但是，这些技术仅依赖于随机步行探索，这既阻碍了最终解决方案的质量和收敛速度。为了解决这一局限性，我们提出了一种称为梯度遗传算法（梯度GA）的新方法，该方法将目标函数中的梯度信息纳入遗传算法中。每个提出的样品不是随机探索，而是通过遵循梯度方向进行最佳解决方案。我们通过设计通过神经网络参数化的可区分目标函数来实现这一目标，并利用离散的langevin提案来实现离散分子空间中的梯度指导。实验结果表明，我们的方法显着提高了收敛速度和溶液质量，表现优于尖端技术。例如，它比香草遗传算法的前十名得分提高了25％。该代码可在https://github.com/debadyuti23/gradientga上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2502.09860</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自我监督单词嵌入的可解决动力和类比推理的出现</title>
      <link>https://arxiv.org/abs/2502.09863</link>
      <description><![CDATA[arxiv：2502.09863v1公告类型：交叉 
摘要：大语言模型的显着成功依赖于他们从训练训练的语料库中隐式学习结构化潜在表示的能力。作为在语言建模中表示的更简单的代理，我们研究了一类可解决的对比度自我监视算法，我们将其称为二次嵌入模型。这些模型类似于Word2Vec算法，并在下游任务上类似地执行。我们的主要贡献是针对训练动力学（在某些超参数选择下）和最终单词嵌入的分析解决方案，仅根据语料库统计来给出。我们的解决方案表明，这些模型一次学习正交线性子空间一个，每个子空间都会增加嵌入的有效等级，直到模型容量饱和为止。在Wikitext上的培训，我们发现顶级子空间代表了可解释的概念。最后，我们使用动力学理论来预测模型如何以及何时获得完成类比的能力。]]></description>
      <guid>https://arxiv.org/abs/2502.09863</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在在线游戏体验中使用机器学习的可解释的早期警告</title>
      <link>https://arxiv.org/abs/2502.09880</link>
      <description><![CDATA[ARXIV：2502.09880V1公告类型：交叉 
摘要：源于物理学，后来应用于其他领域，例如生态学，临界过渡理论表明，某些政权转移之前是统计的预警信号。 Reddit的R/Place实验是一个大规模的社交游戏，为在进行关键过渡的数千个子系统中始终如一地测试这些信号提供了独特的机会。在r/place中，数以百万计的用户共同创建了构图或像素 - 图纸，其中一种构图迅速替换了另一个构图时发生过渡。我们开发了一种基于机器的预警系统，该系统通过梯度提高决策树和保留内存功能结合了多个系统特定时间序列的预测能力。我们的方法大大优于标准预警指标。我们的算法在2022 R/位置数据上进行了培训，检测到20分钟内发生的一半过渡，仅为3.7％的假正率。在2023 R/place事件上进行测试时，其性能仍然很强，证明了跨不同环境的普遍性。使用Shapley添加说明（SHAP）来解释预测，我们调查了潜在的警告驱动因素，这可能与其他复杂系统，尤其是在线社交系统有关。我们揭示了过渡之前的模式相互作用，例如临界减速或加速，缺乏创新或协调，动荡的历史以及缺乏图像复杂性。这些发现表明，在社会生态系统中，机器学习指标的潜力可以预测政权的转变和理解其动态。]]></description>
      <guid>https://arxiv.org/abs/2502.09880</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>