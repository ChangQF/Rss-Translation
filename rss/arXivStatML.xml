<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Fri, 05 Jan 2024 03:14:08 GMT</lastBuildDate>
    <item>
      <title>贝叶斯网络的熵和 Kullback-Leibler 散度：计算复杂性和高效实现。 （arXiv：2312.01520v2 [cs.AI] 已更新）</title>
      <link>http://arxiv.org/abs/2312.01520</link>
      <description><![CDATA[贝叶斯网络 (BN) 是机器学习和计算领域的基础模型
因果推断。它们的图形结构可以处理高维
问题，将它们分成较小的稀疏集合，是朱迪亚的基础
珍珠的因果关系，决定了它们的可解释性和可解释性。
尽管它们很受欢迎，但文献中几乎没有资源
如何计算香农熵和 Kullback-Leibler (KL) 散度
BN 在最常见的分布假设下。在本文中，我们
通过利用 BN 的能力，为两者提供计算高效的算法
图形结构，我们用一套完整的数值来说明它们
例子。在此过程中，我们表明可以减少计算量
高斯 BN 的 KL 复杂度从三次变为二次。
]]></description>
      <guid>http://arxiv.org/abs/2312.01520</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:08 GMT</pubDate>
    </item>
    <item>
      <title>利用内核 Stein 差异控制力矩。 （arXiv：2211.05408v2 [stat.ML] 已更新）</title>
      <link>http://arxiv.org/abs/2211.05408</link>
      <description><![CDATA[核斯坦因差异 (KSD) 衡量分布的质量
近似值，即使目标密度具有
棘手的归一化常数。值得注意的应用包括诊断
近似 MCMC 采样器和非标准化的拟合优度检验
统计模型。目前的工作分析了收敛控制
KSD 的属性。我们首先展示用于弱收敛的标准 KSD
控制无法控制矩收敛。为了解决这个限制，我们接下来
为替代扩散 KSD 的控制提供充分的条件
矩收敛和弱收敛。作为直接的结果，我们开发了，
每个 $q &gt; 0$，第一个已知准确表征 $q$ 的 KSD -Wasserstein
收敛。
]]></description>
      <guid>http://arxiv.org/abs/2211.05408</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>神经网络模型压缩：框架、算法和收敛保证。 （arXiv：2303.06815v2 [cs.LG] 已更新）</title>
      <link>http://arxiv.org/abs/2303.06815</link>
      <description><![CDATA[模型压缩是部署神经网络 (NN) 的关键部分，
特别是当计算设备的内存和存储在许多方面受到限制时
应用程序。本文重点讨论两种模型压缩技术：低秩
神经网络中的近似和权重剪枝非常流行
如今。然而，使用低秩近似和权重剪枝训练神经网络
总是遭受严重的精度损失和收敛问题。在本文中，
从新颖的角度提出了模型压缩的整体框架
通过设计适当的目标函数来实现非凸优化。然后，
我们引入 NN-BCD，一种块坐标下降（BCD）算法来解决
非凸优化。我们的算法的一个优点是，一种有效的
迭代方案可以用封闭形式导出，它是无梯度的。
因此，我们的算法不会受到梯度消失/爆炸的影响
问题。此外，利用我们的 Kurdyka-{\L}ojasiewicz (K{\L}) 属性
目标函数，我们证明我们的算法全局收敛到一个临界值
点的速率为 O(1/k)，其中 k 表示迭代次数。最后，
张量序列分解和权重剪枝的广泛实验
展示所提出框架的效率和卓越性能。
我们的代码实现位于 https://github.com/ChenyangLi-97/NN-BCD
]]></description>
      <guid>http://arxiv.org/abs/2303.06815</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>用于高维函数逼近的切片梯度增强克里金法。 （arXiv：2204.03562v3 [stat.ML] 已更新）</title>
      <link>http://arxiv.org/abs/2204.03562</link>
      <description><![CDATA[梯度增强克里金法 (GE-Kriging) 是一种成熟的替代方法
用于近似昂贵的计算模型的建模技术。然而，
由于大小的原因，它对于高维问题往往变得不切实际
固有相关矩阵和相关的高维
超参数调整问题。为了解决这些问题，出现了一种新方法，称为
本文开发了切片 GE-Kriging（SGE-Kriging），用于减少
相关矩阵的大小和超参数的数量。我们首先
将训练样本集分割成多个切片，并调用贝叶斯定理
通过切片似然函数来近似完全似然函数，
其中利用多个小相关矩阵来描述
样本集的相关性，而不是一个大的相关性。然后，我们替换
原始高维超参数调整问题与低维
通过学习超参数和
基于衍生品的全球敏感性指数。 SGE-Kriging 的性能为
最终通过几个基准的数值实验进行了验证
高维空气动力学建模问题。结果表明，
SGE-Kriging 模型的准确性和稳健性可与
标准一种，但培训成本要低得多。好处是最多的
对于具有数十个变量的高维问题来说很明显。
]]></description>
      <guid>http://arxiv.org/abs/2204.03562</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:06 GMT</pubDate>
    </item>
    <item>
      <title>用于场景分析的齐次伊辛模型的快速近似。 （arXiv：1712.02195v4 [stat.ME] 已更新）</title>
      <link>http://arxiv.org/abs/1712.02195</link>
      <description><![CDATA[伊辛模型在许多统计建模和推理中都很重要
应用程序，但是其归一化常数，活动顶点的平均数量
和平均自旋相互作用——推理所需的量——是
计算上难以处理。我们提供准确的近似值，使其
可以在均匀情况下对这些量进行数值计算。
模拟研究表明我们的近似公式具有良好的性能
具有可扩展性，并且不受网络规模（节点数量、图度）的影响
马尔可夫随机场。我们的近似公式的实际意义是
在功能磁共振中执行贝叶斯推理进行说明
成像激活检测实验，以及似然比测试
开心果树年增长空间模式的各向异性
产量。
]]></description>
      <guid>http://arxiv.org/abs/1712.02195</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>平滑损失函数的联合优化。 （arXiv：2201.01954v2 [cs.LG] 已更新）</title>
      <link>http://arxiv.org/abs/2201.01954</link>
      <description><![CDATA[在这项工作中，我们研究了联邦内的经验风险最小化（ERM）
学习框架，其中中央服务器最小化 ERM 目标函数
使用存储在 $m$ 个客户端上的训练数据。在此设置下，
联合平均 (FedAve) 算法是确定
$\epsilon$-ERM 问题的近似解。与标准类似
优化算法，FedAve的收敛分析仅依赖于
优化参数中损失函数的平滑度。然而，损失
训练数据中的函数通常也非常平滑。为了利用这个
额外的平滑度，我们提出联合低阶梯度下降
(FedLRGD) 算法。由于数据的平滑性会导致近似低等级
损失函数的结构，我们的方法首先执行几轮
服务器和客户端之间的通信，以了解服务器的权重
可用于近似客户的梯度。然后，我们的方法解决了 ERM
服务器上使用不精确梯度下降的问题。为了表明 FedLRGD 可以
具有优于FedAve的性能，我们提出了联邦预言机的概念
复杂性与规范预言机复杂性相对应。在一些
对损失函数的假设，例如参数的强凸性，
$\eta$-H\&quot;较旧的数据平滑度等，我们证明联邦预言机
FedLRGD 尺度的复杂性如 $\phi m(p/\epsilon)^{\Theta(d/\eta)}$ 和
FedAve 的尺度类似于 $\phi m(p/\epsilon)^{3/4}$ （忽略次主导
因子），其中 $\phi\gg 1$ 是“通信与计算的比率”，$p$ 是
参数维度，$d$ 是数据维度。然后，我们证明当
$d$很小并且损失函数在数据中足够平滑，FedLRGD
在联邦预言机复杂性方面击败了 FedAve。最后，在过程中
分析 FedLRGD，我们还建立了低秩近似的结果
潜变量模型。
]]></description>
      <guid>http://arxiv.org/abs/2201.01954</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化分析的调查。 （arXiv：2401.02349v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.02349</link>
      <description><![CDATA[强化学习研究取得重大成功并受到关注
利用深度神经网络解决高水平问题
维度状态或动作空间。虽然深度强化学习政策
目前被部署在医疗应用的许多不同领域
对于自动驾驶汽车，该领域仍然存在一些正在尝试的问题
解答深度强化学习的泛化能力
政策。在本文中，我们将概述深度学习的根本原因。
强化学习策略遇到过拟合问题，限制了其
鲁棒性和泛化能力。此外，我们将正式确定并
统一不同的解决方案以提高泛化性，并克服
状态-动作值函数的过度拟合。我们相信我们的研究可以提供
对当前深度学习进展进行紧凑系统的统一分析
强化学习，并帮助构建强大的深度神经策略
提高泛化能力。
]]></description>
      <guid>http://arxiv.org/abs/2401.02349</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:04 GMT</pubDate>
    </item>
    <item>
      <title>基于模拟的分位数回归推理。 （arXiv：2401.02413v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.02413</link>
      <description><![CDATA[我们提出了神经分位数估计（NQE），一种新颖的基于模拟的方法
基于条件分位数回归的推理（SBI）方法。网络质量工程师
自回归学习每个后验的单个一维分位数
维度，以数据和先前的后验维度为条件。后部
样本是通过使用单调插值预测分位数获得的
三次 Hermite 样条，对尾部行为进行特殊处理，
多模态分布。我们引入一个替代定义
使用局部累积密度函数 (CDF) 的贝叶斯可信区域，
提供比传统最高后验更快的评估速度
密度区域 (HPDR)。如果模拟预算有限和/或已知模型
如果指定错误，可以将后处理拓宽步骤集成到 NQE 中
确保后验估计的无偏性可以忽略不计
额外的计算成本。我们证明了所提出的 NQE 方法
在各种基准问题上实现了最先进的性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.02413</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:04 GMT</pubDate>
    </item>
    <item>
      <title>基于矩阵变量 $t$ 分布的稳健双线性因子分析。 （arXiv：2401.02203v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.02203</link>
      <description><![CDATA[基于多元 $t$ 分布 ($t$fa) 的因子分析是一种有用的方法
用于提取重尾或污染数据的共同因素的强大工具。
然而，$t$fa仅适用于矢量数据。当 $t$fa 应用于
对于矩阵数据，通常首先对矩阵观测值进行向量化。这
给 $t$fa 带来了两个挑战：(i) 固有的矩阵结构
数据被破坏，并且 (ii) 鲁棒性可能会丢失，如矢量化矩阵数据
通常会产生高数据维度，这很容易导致
$t$fa 的细目。要解决这些问题，从内在矩阵入手
矩阵数据结构，一种新颖的鲁棒因子分析模型，即双线性
基于矩阵变量 $t$ 分布 ($t$bfa) 的因子分析为
本文提出。新颖之处在于它能够同时
提取感兴趣的行和列变量的公共因子
重尾或受污染的矩阵数据。两种有效的最大化算法
开发了 $t$bfa 的似然估计。的闭合形式表达式
计算参数估计精度的 Fisher 信息矩阵为
衍生的。进行实证研究是为了了解拟议的 $t$bfa
模型并与相关竞争对手进行比较。结果表明
$t$bfa的优越性和实用性。重要的是，$t$bfa 展示了
击穿点明显高于 $t$fa，使其更适合
矩阵数据。
]]></description>
      <guid>http://arxiv.org/abs/2401.02203</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>分布式强化学习中具有可解释参数调整的鲁棒分位数 Huber 损失。 （arXiv：2401.02325v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.02325</link>
      <description><![CDATA[分布式强化学习 (RL) 估计回报分布
主要通过最小化分位数 Huber 损失来学习分位数值
函数，需要一个通常启发式或通过选择阈值参数
超参数搜索，这可能不能很好地概括并且可能不是最优的。
本文介绍了一种广义分位数 Huber 损失函数，由
高斯分布之间的 Wasserstein 距离（WD）计算，捕获
预测（当前）和目标（贝尔曼更新）分位数值中的噪声。
与经典的分位数 Huber 损失相比，这种创新的损失函数
增强针对异常值的鲁棒性。值得注意的是，经典的 Huber 损失
函数可以看作是我们提出的损失的近似值，使得
通过近似数据中的噪声量来调整参数
学习过程。对雅达利游戏（Atari 游戏的常见应用）的实证测试
分布式 RL，以及最近使用分布式 RL 的对冲策略，
验证我们提出的损失函数的有效性及其潜力
分布式强化学习中的参数调整。
]]></description>
      <guid>http://arxiv.org/abs/2401.02325</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>U-可信模型。决策的可靠性、能力和信心。 （arXiv：2401.02062v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.02062</link>
      <description><![CDATA[随着人们对预测模型中的偏见和歧视的担忧日益增加，
人工智能社区越来越关注评估人工智能系统
值得信赖。传统上，值得信赖的人工智能文献依赖于
概率框架和校准作为可信度的先决条件。
在这项工作中，我们通过提出一种新颖的信托来摆脱这一观点
框架的灵感来自于关于信任的哲学文献。我们提出了一个精确的
可信度的数学定义，称为
$\mathcal{U}$-可信度，专门针对任务子集而定制
旨在最大化效用函数。我们认为模型的
$\mathcal{U}$-可信度取决于其最大化贝叶斯的能力
该任务子集中的实用程序。我们的第一组结果挑战
概率框架通过展示其有利于较少的潜力
可信模型并引入误导可信度的风险
评估。在 $\mathcal{U}$-可信度的背景下，我们证明
正确排名的模型本质上是值得信赖的。此外，
我们主张采用 AUC 指标作为首选指标
值得信赖。通过提供理论保证和实验
验证，AUC 能够对可信度进行稳健评估，从而增强
模型选择和超参数调整以产生更值得信赖的结果。
]]></description>
      <guid>http://arxiv.org/abs/2401.02062</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>基于能量的扩散发生器，用于玻尔兹曼分布的高效采样。 （arXiv：2401.02080v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.02080</link>
      <description><![CDATA[我们引入了一种新颖的采样器，称为基于能量的扩散发生器
从任意目标分布生成样本。抽样模型
采用类似于变分自动编码器的结构，利用解码器
将潜在变量从简单分布转换为随机变量
逼近目标分布，我们设计了一个基于
扩散模型。利用扩散的强大建模能力
对于复杂分布的模型，我们可以获得准确的变分估计
生成的分布之间的 Kullback-Leibler 散度
样本和目标。此外，我们提出了一种基于广义的解码器
哈密​​顿动力学进一步增强采样性能。通过实证
评估，我们证明了我们的方法在各种不同的情况下的有效性
复杂的分布函数，与现有的相比显示出其优越性
方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.02080</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>通过持久性能量在 ${\Lambda}$CDM 宇宙学中进行层次聚类。 （arXiv：2401.01988v1 [astro-ph.CO]）</title>
      <link>http://arxiv.org/abs/2401.01988</link>
      <description><![CDATA[在这项研究中，我们研究了宇宙网的结构演化，
采用拓扑数据分析的先进方法。我们的方法
涉及利用 $Persistence$ $Signals$，这是最近的一种创新方法
有助于将持久性图嵌入向量的文献
通过将空间重新概念化为 $\mathbb R^2_+$ 中的信号。利用这个
方法论中，我们分析了三种典型的宇宙结构：星团、
细丝和空隙。一个核心发现是以下两者之间的相关性：
$Persistence$ $Energy$ 和红移值，将持久同源性与
宇宙演化并提供对宇宙结构动力学的见解。
]]></description>
      <guid>http://arxiv.org/abs/2401.01988</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:01 GMT</pubDate>
    </item>
    <item>
      <title>使用无约束 ReLU 特征模型进行跨熵类不平衡学习的神经崩溃。 （arXiv：2401.02058v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.02058</link>
      <description><![CDATA[当前训练深度神经网络进行分类的范例
任务包括最小化导致训练损失的经验风险
即使训练误差消失后，值也趋向于零。在这个
训练的最后阶段，观察到最后一层特征
塌陷到它们的类均值，并且这些类均值收敛到
单纯形等角紧框架 (ETF)。这种现象被称为神经
塌陷（NC）。为了从理论上理解这种现象，最近的作品采用了
一个简化的无约束特征模型来证明 NC 在全球范围内出现
训练问题的解决方案。然而，当训练数据集为
类别不平衡，一些 NC 属性将不再成立。例如，
当损失发生时，类均值几何将偏离单纯形 ETF
收敛。在本文中，我们将 NC 推广到不平衡状态
无约束ReLU特征模型下的交叉熵损失。我们证明，
虽然类内功能折叠属性在此设置中仍然有效，
类均值将收敛为由正交向量组成的结构
具有不同的长度。此外，我们发现分类器权重是
与缩放和居中的类对齐，缩放因子取决于
每个类别的训练样本数，概括了 NC
阶级平衡的设置。我们通过实验证明了我们的结果
实用的架构和数据集。
]]></description>
      <guid>http://arxiv.org/abs/2401.02058</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:01 GMT</pubDate>
    </item>
    <item>
      <title>超越遗憾：贝叶斯优化的几何度量。 （arXiv：2401.01981v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.01981</link>
      <description><![CDATA[贝叶斯优化是黑盒的原则性优化策略
目标函数。它在各种现实世界中显示了其有效性
应用，例如科学发现和实验设计。一般来说，
贝叶斯优化的性能通过基于遗憾的指标进行评估
例如瞬时的、简单的和累积的遗憾。这些指标仅依赖于
函数评估，因此他们不考虑几何关系
查询点和全局解决方案之间，或查询点本身之间。尤其，
他们无法区分是否成功找到了多个全球解决方案。
此外，他们没有评估贝叶斯优化利用和
探索给定的搜索空间。为了解决这些问题，我们提出了四个新的建议
几何度量，即精度、召回率、平均度和平均值
距离。这些指标使我们能够比较贝叶斯优化算法
考虑查询点和全局最优的几何形状，或查询
点。然而，它们伴随着一个额外的参数，需要
仔细确定。因此，我们设计了无参数形式
通过整合附加参数来获得各自的指标。最后，我们
凭经验验证我们提出的指标可以提供更有说服力的
贝叶斯优化算法的解释和理解
与传统指标相比，具有独特的视角。
]]></description>
      <guid>http://arxiv.org/abs/2401.01981</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:00 GMT</pubDate>
    </item>
    </channel>
</rss>