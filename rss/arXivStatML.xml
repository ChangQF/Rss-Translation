<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 16 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于加速刚性非线性系统模拟的恒速潜动力学方法</title>
      <link>https://arxiv.org/abs/2501.08423</link>
      <description><![CDATA[arXiv:2501.08423v1 公告类型：新
摘要：求解刚性常微分方程 (StODE) 需要复杂的数值求解器，这通常计算成本高昂。特别是，StODE 通常无法用传统的显式时间积分方案求解，必须采用昂贵的隐式方法来计算解。另一方面，基于机器学习 (ML) 的最先进的方法（如神经 ODE (NODE)）无法很好地处理 StODE 解的各个元素的时间尺度分离，并且需要昂贵的隐式求解器在推理时进行积分。在这项工作中，我们走上了一条不同的道路，涉及学习 StODE 的潜在动力学，其中完全避免了数值积分。为此，我们考虑一个恒速潜在动力系统，其解是一系列直线。给定 ODE 的初始条件和参数，编码器网络学习斜率（即恒定速度）和潜在动力学的初始条件。换句话说，原始动力学的解决方案被编码成一系列直线，这些直线可以在需要时解码回来以检索实际解决方案。我们方法的另一个关键思想是时间的非线性变换，它允许在潜在空间中“拉伸/压缩”时间，从而允许对解决方案中不同时间区域的不同程度的关注。此外，我们提供了一个简单的通用近似型证明，表明我们的方法可以在紧凑集上近似刚性非线性系统的解，精度为任何程度，{\epsilon}。我们表明，我们方法中潜在动力系统的维度与{\epsilon}无关。对原型 StODE 的数值研究表明，我们的方法优于处理 StODE 的最先进的机器学习方法。]]></description>
      <guid>https://arxiv.org/abs/2501.08423</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向一般概念类的乐观通用在线可学习性理论</title>
      <link>https://arxiv.org/abs/2501.08551</link>
      <description><![CDATA[arXiv:2501.08551v1 公告类型：新
摘要：我们对具有 $\{0, 1\}$ 标签的乐观通用在线学习的概念类进行了完整的描述。乐观通用在线学习的概念是在 [Hanneke, 2021] 中定义的，目的是理解在最小假设下的可学习性。在本文中，遵循这项工作背后的哲学，我们研究了两个问题，即针对每个概念类：(1) 允许在线学习的数据处理上的最小假设是什么？(2) 是否存在一种学习算法，它可以在满足最小假设的每个数据处理下取得成功？对于给定的概念类，这种算法被称为乐观通用的。我们为所有概念类解决了这两个问题，而且，作为解决方案的一部分，我们为每种情况设计了通用的学习算法。最后，我们将这些算法和结果扩展到不可知论的情况，证明了对于每个概念类，在不可知论和可实现情况下，对数据处理的可学习性的最小假设之间的等价性，以及乐观的通用可学习性的等价性。]]></description>
      <guid>https://arxiv.org/abs/2501.08551</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测因子的因果与非因果合并</title>
      <link>https://arxiv.org/abs/2501.08426</link>
      <description><![CDATA[arXiv:2501.08426v1 公告类型：交叉 
摘要：我们研究使用相同数据在因果和反因果方向上合并预测因子所产生的差异。特别是，我们研究在简单模型中出现的不对称性，在该模型中，我们使用一个二进制变量作为目标，两个连续变量作为预测因子来合并预测因子。我们使用因果最大熵 (CMAXENT) 作为归纳偏差来合并预测因子，但是，我们预计当我们使用其他考虑因果之间不对称的合并方法时，也会出现类似的差异。我们表明，如果我们观察所有双变量分布，CMAXENT 解决方案在因果方向上会简化为逻辑回归，在反因果方向上会简化为线性判别分析 (LDA)。此外，我们研究了当我们仅观察到一些双变量分布对变量外 (OOV) 泛化的影响时，这两个解决方案的决策边界如何不同。]]></description>
      <guid>https://arxiv.org/abs/2501.08426</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>量子储层计算和风险界限</title>
      <link>https://arxiv.org/abs/2501.08640</link>
      <description><![CDATA[arXiv:2501.08640v1 公告类型：交叉 
摘要：我们提出了一种使用 Rademacher 复杂度来限制几类量子库的泛化误差的方法。我们为两个特定的量子库类给出了特定的、依赖于参数的界限。我们分析了泛化界限如何随着量子比特数量的增加而扩展。将我们的结果应用于具有多项式读出函数的类，我们发现风险界限在训练样本的数量上收敛。我们的界限中对量子库和读出参数的明确依赖可以在一定程度上控制泛化误差。应该注意的是，界限随着量子比特数 $n$ 呈指数增长。Rademacher 复杂度的上限可以应用于满足量子动力学和读出函数的一些假设的其他库类。]]></description>
      <guid>https://arxiv.org/abs/2501.08640</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应特征模型中核希尔伯特空间重现中的对角线过参数化：泛化和自适应性</title>
      <link>https://arxiv.org/abs/2501.08679</link>
      <description><![CDATA[arXiv:2501.08679v1 公告类型：交叉 
摘要：本文介绍了一种对角自适应核模型，该模型在训练过程中同时动态学习核特征值和输出系数。与与神经正切核理论相关的固定核方法不同，对角自适应核模型适应真值函数的结构，显著提高了固定核方法的泛化能力，尤其是当初始核与目标不一致时。此外，我们表明自适应性来自于在训练过程中学习正确的特征值，表现出特征学习行为。通过扩展到更深的参数化，我们进一步展示了额外的深度如何增强适应性和泛化能力。这项研究结合了特征学习和隐式正则化的见解，为超越核机制的神经网络的自适应性和泛化潜力提供了新的视角。]]></description>
      <guid>https://arxiv.org/abs/2501.08679</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解缠交织变分编码</title>
      <link>https://arxiv.org/abs/2501.08710</link>
      <description><![CDATA[arXiv:2501.08710v1 公告类型：交叉 
摘要：目标冲突对交叉多任务学习提出了相当大的挑战，需要精心设计和平衡，以确保在所有任务中有效学习具有代表性的潜在数据空间，而不会相互产生负面影响。从概率论中边际和条件概率分布的概念中汲取灵感，我们设计了一种有原则、有理有据的方法，将原始输入解缠为变分自动编码器潜在空间中的边际和条件概率分布。我们提出的模型深度解缠交错变分编码 (DeepDIVE) 从原始输入中学习解缠特征以在嵌入空间中形成聚类，并通过融合阶段的交叉注意机制统一这些特征。我们从理论上证明了将重建和预测目标结合起来可以完全捕捉到下限，并使用朴素贝叶斯从数学上推导出用于解缠的损失函数。在先验是对数凹分布的混合假设下，我们还确定了先验和后验之间的 Kullback-Leibler 散度的上限由交叉熵损失的最小化器最小化的函数决定，这为我们采用径向基函数 (RBF) 和交叉熵进行 DeepDIVE 交错训练提供了合理的收敛基础。在两个公共数据集上进行的实验表明，DeepDIVE 解缠了原始输入，并且比原始 VAE 具有更好的预测精度，并且可与现有的最先进基线相媲美。]]></description>
      <guid>https://arxiv.org/abs/2501.08710</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对于有限度量空间，量级是否“一般连续”？</title>
      <link>https://arxiv.org/abs/2501.08745</link>
      <description><![CDATA[arXiv:2501.08745v1 公告类型：交叉 
摘要：量级是度量空间的实值不变量，在有限设置中，可以理解为随着度量尺度的变化，记录空间中的“有效点数”。受拓扑数据分析应用的启发，本文研究了量级的稳定性：其相对于 Gromov-Hausdorff 拓扑的连续性。我们表明，量级在有限度量空间的 Gromov-Hausdorff 空间上不连续。然而，我们发现证据表明它可能是“一般连续的”，因为一般的 Gromov-Hausdorff 极限由量级保留。我们认为，事实上，“一般稳定性”才是适用性的关键。]]></description>
      <guid>https://arxiv.org/abs/2501.08745</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过潜在空间遍历实现图反事实可解释人工智能</title>
      <link>https://arxiv.org/abs/2501.08850</link>
      <description><![CDATA[arXiv:2501.08850v1 公告类型：交叉 
摘要：解释深度神经网络的预测并非易事，但高质量的预测解释通常是从业者信任这些模型的先决条件。反事实解释旨在通过找到分布中“最近”的替代输入来解释预测，该输入的预测以预先指定的方式发生变化。然而，如何定义这个最近的替代输入仍然是一个悬而未决的问题，它的解决方案取决于领域（例如图像、图形、表格数据等）和所考虑的特定应用。对于图，这个问题很复杂：i）因为它们的离散性质，而不是最先进的图分类器的连续性质；ii）节点置换组作用于图。我们提出了一种利用特定案例的置换等变图变分自动编码器为任何可微黑盒图分类器生成反事实解释的方法。我们通过遍历自动编码器的潜在空间跨越分类器的分类边界，以连续的方式生成反事实解释，从而实现离散图结构和连续图属性的无缝集成。我们在三个图数据集上对该方法进行了实证验证，结果表明我们的模型始终表现出色，并且比基线更稳健。]]></description>
      <guid>https://arxiv.org/abs/2501.08850</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强不变流形学习</title>
      <link>https://arxiv.org/abs/2211.00460</link>
      <description><![CDATA[arXiv:2211.00460v3 公告类型：替换 
摘要：数据增强是一种广泛使用的技术，也是自监督表示学习最新进展的重要组成部分。通过保留增强数据之间的相似性，生成的数据表示可以改进各种下游分析并在许多应用中实现最先进的性能。尽管经验上有效，但大多数现有方法在一般非线性环境下缺乏理论理解。为了填补这一空白，我们在低维乘积流形上开发了一个统计框架来对数据增强变换进行建模。在这个框架下，我们引入了一种称为增强不变流形学习的新表示学习方法，并通过将其重新表述为随机优化问题来设计一种计算效率高的算法。与现有的自监督方法相比，新方法同时利用了流形的几何结构和增强数据的不变性，并具有明确的理论保证。我们的理论研究描述了数据增强在所提方法中的作用，并揭示了从增强数据中学习到的数据表示为何以及如何改进下游分析中的 $k$-最近邻分类器，表明更复杂的数据增强会带来下游分析的更多改进。最后，我们在模拟和真实数据集上进行了数值实验，以证明所提方法的优点。]]></description>
      <guid>https://arxiv.org/abs/2211.00460</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>线性赌博机的集成采样：小型集成就足够了</title>
      <link>https://arxiv.org/abs/2311.08376</link>
      <description><![CDATA[arXiv:2311.08376v4 公告类型：替换 
摘要：我们首次对随机线性老虎机设置的集合抽样进行了有用且严格的分析。具体而言，我们表明，在标准假设下，对于具有交互范围 $T$ 的 $d$ 维随机线性老虎机，大小为 $d \log T$ 阶集合的集合抽样最多会导致 $(d \log T)^{5/2} \sqrt{T}$ 阶的遗憾。我们的结果是在任何结构化设置中第一个不需要集合的大小与 $T$ 线性缩放的结果——这违背了集合抽样的目的——同时获得接近 $\smash{\sqrt{T}}$ 阶的遗憾。我们的结果也是第一个允许无限动作集的结果。]]></description>
      <guid>https://arxiv.org/abs/2311.08376</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RoME：一种用于优化移动医疗干预的稳健混合效应老虎机算法</title>
      <link>https://arxiv.org/abs/2312.06403</link>
      <description><![CDATA[arXiv:2312.06403v4 公告类型：替换 
摘要：移动健康利用通过老虎机和强化学习算法优化的个性化和情境化干预措施。然而，在实践中，参与者异质性、非平稳性和非线性关系等挑战阻碍了算法的性能。我们提出了一种稳健的混合效应情境老虎机算法 RoME，该算法通过 (1) 使用用户和时间特定的随机效应对差异奖励进行建模、(2) 网络凝聚力惩罚和 (3) 去偏机器学习来灵活估计基线奖励，同时解决了这些挑战。我们建立了一个高概率遗憾界限，该界限仅取决于差异奖励模型的维度，使我们能够在基线奖励非常复杂的情况下实现稳健的遗憾界限。我们在模拟和两项离线策略评估研究中展示了 RoME 算法的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2312.06403</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>改进情境动态定价算法</title>
      <link>https://arxiv.org/abs/2406.11316</link>
      <description><![CDATA[arXiv:2406.11316v2 公告类型：替换 
摘要：在情境动态定价中，卖方根据情境信息按顺序对商品进行定价。只有当价格低于其估值时，买方才会购买产品。卖方的目标是设计一种定价策略，以收集尽可能多的收入。我们关注两种不同的估值模型。第一个假设估值线性依赖于上下文，并进一步受到噪声的扭曲。在次要规律性假设下，我们的算法实现了最佳遗憾界限$\tilde{\mathcal{O}}(T^{2/3})$，从而改进了现有结果。第二个模型消除了线性假设，仅要求预期买方估值在上下文中为$\beta$-H\&quot;older。对于这个模型，我们的算法获得遗憾$\tilde{\mathcal{O}}(T^{d+2\beta/d+3\beta})$，其中$d$是上下文空间的维度。]]></description>
      <guid>https://arxiv.org/abs/2406.11316</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>粒子半隐式变分推理</title>
      <link>https://arxiv.org/abs/2407.00649</link>
      <description><![CDATA[arXiv:2407.00649v3 公告类型：替换 
摘要：半隐式变分推理 (SIVI) 通过利用核和混合分布来分层定义变分分布，丰富了变分族的表达能力。现有的 SIVI 方法使用隐式分布参数化混合分布，导致难以处理的变分密度。因此，直接最大化证据下限 (ELBO) 是不可能的，因此他们诉诸以下方法之一：优化 ELBO 的界限，采用昂贵的内环马尔可夫链蒙特卡罗运行，或解决极小最大目标。在本文中，我们提出了一种称为粒子变分推理 (PVI) 的 SIVI 新方法，该方法采用经验测量来近似最佳混合分布，其特征是自由能函数的最小化器。 PVI 自然而然地作为欧几里德-沃瑟斯坦梯度流的粒子近似而出现，与之前的研究不同，它直接优化了 ELBO，同时不对混合分布做出任何参数假设。我们的实证结果表明，与其他 SIVI 方法相比，PVI 在各种任务中的表现都很好。此外，我们还对相关自由能函数的梯度流行为进行了理论分析：确定解的存在性和唯一性以及混沌结果的传播。]]></description>
      <guid>https://arxiv.org/abs/2407.00649</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>广义线性模型的统一置信序列及其在老虎机中的应用</title>
      <link>https://arxiv.org/abs/2407.13977</link>
      <description><![CDATA[arXiv:2407.13977v3 公告类型：替换 
摘要：我们为任何（自协和）广义线性模型 (GLM) 提出了一个统一的基于似然比的置信序列 (CS)，该序列保证是凸的且数值严密。我们表明，这与各种 GLM 的已知 CS 相当或有所改进，包括高斯、伯努利和泊松。特别是，我们的伯努利 CS 首次具有 $\mathrm{poly}(S)$ 自由半径，其中 $S$ 是未知参数的范数。我们的第一个技术新颖性是它的推导，它利用了时间均匀的 PAC-Bayesian 边界和均匀的先验/后验，尽管后者是推导 CS 的一个相当不受欢迎的选择。作为我们新 CS 的直接应用，我们提出了一种简单而自然的乐观算法，称为 OFUGLB，适用于任何广义线性老虎机（GLB；Filippi 等人（2010））。我们的分析表明，这种著名的乐观方法同时实现了各种自协调（不一定有界）GLB 的最先进的遗憾，甚至实现了有界 GLB（包括逻辑老虎机）的 $\mathrm{poly}(S)$-free。遗憾分析是我们的第二项技术创新，它源于将我们的新 CS 与一种新的证明技术相结合，该技术完全避免了以前广泛使用的自协调控制引理（Faury 等人，2020，引理 9）。从数值上看，OFUGLB 的表现优于或与逻辑老虎机的先前算法相当。]]></description>
      <guid>https://arxiv.org/abs/2407.13977</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>窄神经网络的高维学习</title>
      <link>https://arxiv.org/abs/2409.13904</link>
      <description><![CDATA[arXiv:2409.13904v2 公告类型：替换 
摘要：近年来，机器学习应用的多样化和普及程度不断提高。然而，对于神经网络从高维数据中学习的惊人效率的坚定理论理解仍然在很大程度上难以实现。在这一努力中，受统计物理学启发的分析已被证明是有用的，它能够对高维神经网络的学习进行严格的渐近表征，适用于广泛的可解模型。本文回顾了这项工作的最新进展背后的工具和想法。我们引入了一个通用模型——序列多指标模型——它包含了许多以前研究过的模型作为特殊实例。这个统一的框架涵盖了一类具有有限数量隐藏单元的机器学习架构，包括多层感知器、自动编码器、注意力机制；以及在大数据维度和相当大量样本的限制下，包括（无）监督学习、去噪、对比学习在内的任务。我们详细阐述了序列多指标模型学习的分析，使用统计物理技术，例如复制方法和近似消息传递算法。因此，本文统一介绍了之前几篇论文中报告的分析，并详细概述了机器学习统计物理领域的核心技术。这篇评论对于对统计物理方法感兴趣的机器学习理论家来说应该是一本有用的入门书；对于有兴趣将这些想法转移到神经网络研究的统计物理学家来说，它也应该很有价值。]]></description>
      <guid>https://arxiv.org/abs/2409.13904</guid>
      <pubDate>Thu, 16 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>