<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于在线多组学习的分组预言机高效算法</title>
      <link>https://arxiv.org/abs/2406.05287</link>
      <description><![CDATA[arXiv:2406.05287v1 公告类型：交叉 
摘要：我们研究在线多组学习问题，这是一种学习模型，其中在线学习者必须同时对对应于一组组的大集合（可能重叠）子序列实现较小的预测遗憾。组是上下文空间的子集，在公平应用中，它们可能对应于由人口统计属性的表达函数定义的子种群。与之前关于这种学习模型的工作相比，我们考虑了组群太大而无法明确枚举的情况，因此我们寻求仅通过优化预言机访问组的算法。在本文中，我们在各种设置下设计了具有亚线性遗憾的这种预言机高效算法，包括：（i）i.i.d. 设置，（ii）具有平滑上下文分布的对抗设置，以及（iii）对抗传导设置。]]></description>
      <guid>https://arxiv.org/abs/2406.05287</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>选择加权度校正随机块模型的社区数量</title>
      <link>https://arxiv.org/abs/2406.05340</link>
      <description><![CDATA[arXiv:2406.05340v1 公告类型：交叉 
摘要：我们研究如何在没有完全似然建模的情况下选择加权网络的社区数量。首先，我们提出了一种新的加权度校正随机块模型 (DCSBM)，其中平均邻接矩阵的建模与标准 DC​​SBM 相同，而方差轮廓矩阵被假定为通过给定的方差函数与平均邻接矩阵相关。我们选择社区数量的方法基于顺序测试框架，在每个步骤中，加权 DCSBM 都通过某种谱聚类方法进行拟合。关键步骤是对估计的方差轮廓矩阵进行矩阵缩放。得到的缩放因子可用于规范化邻接矩阵，从中获得测试统计量。在加权 DCSBM 的温和条件下，我们提出的程序在估计真实社区数量方面被证明是一致的。对模拟和真实网络数据进行的数值实验也证明了我们方法的理想经验特性。]]></description>
      <guid>https://arxiv.org/abs/2406.05340</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络统计泛化的多方面视角</title>
      <link>https://arxiv.org/abs/2406.05225</link>
      <description><![CDATA[arXiv:2406.05225v1 公告类型：交叉 
摘要：卷积神经网络已成功扩展到图操作，从而产生了图神经网络 (GNN)。GNN 通过连续应用图卷积将来自相邻节点的信息组合起来。GNN 已成功应用于各种学习任务，而对其泛化能力的理论理解仍在进行中。在本文中，我们利用流形理论来分析在流形采样点上构建的图上运行的 GNN 的统计泛化差距。我们研究了 GNN 在节点级和图级任务上的泛化差距。我们表明，泛化差距随着训练图中节点数量的增加而减小，这保证了 GNN 可以泛化到流形上看不见的点。我们在多个真实世界数据集中验证了我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2406.05225</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>不评估目标的马尔可夫链蒙特卡罗：辅助变量方法</title>
      <link>https://arxiv.org/abs/2406.05242</link>
      <description><![CDATA[arXiv:2406.05242v1 公告类型：交叉 
摘要：在采样任务中，通常已知目标分布的归一化常数。然而，在许多情况下，评估非归一化分布的成本可能很高或不可行。这个问题出现在从高数据集的贝叶斯后验和“双重难处理”分布中采样等场景中。在本文中，我们首先观察到看似不同的马尔可夫链蒙特卡罗 (MCMC) 算法，例如交换算法、PoissonMH 和 TunaMH，可以在一个简单的通用程序下统一。然后，我们将此过程扩展为一个新框架，允许在提议和接受-拒绝步骤中使用辅助变量。我们开发了新框架的理论，将其应用于现有算法以简化和扩展其结果。从该框架中出现了几种新算法，在合成数据集和真实数据集上都表现出了更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.05242</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>通过最优传输实现多元随机优势及其在模型基准测试中的应用</title>
      <link>https://arxiv.org/abs/2406.06425</link>
      <description><![CDATA[arXiv:2406.06425v1 公告类型：新
摘要：随机优势是概率论、计量经济学和社会选择理论中的一个重要概念，用于稳健地建模代理在随机结果之间的偏好。虽然许多工作都致力于单变量情况，但在多变量场景中却做得很少，其中代理必须在不同的多变量结果之间做出决定。通过利用耦合方面的多元一阶随机优势的特征，我们引入了一个统计数据，该统计数据在最优传输框架下以平滑成本评估多元几乎随机优势。此外，我们引入了该统计数据的熵正则化，并建立了中心极限定理 (CLT) 和经验统计的引导程序的一致性。基于这个 CLT，我们提出了一个假设检验框架以及使用 Sinkhorn 算法的有效实现。我们展示了我们在比较和基准测试基于多个指标评估的大型语言模型方面的方法。我们的多元随机优势测试使我们能够捕捉指标之间的依赖关系，以便对模型的相对性能做出明智且具有统计意义的决策。]]></description>
      <guid>https://arxiv.org/abs/2406.06425</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:12 GMT</pubDate>
    </item>
    <item>
      <title>自然语言生成中的主观不确定性量化与校准</title>
      <link>https://arxiv.org/abs/2406.05213</link>
      <description><![CDATA[arXiv:2406.05213v1 公告类型：交叉 
摘要：大型语言模型的应用通常涉及自由形式响应的生成，在这种情况下，不确定性量化变得具有挑战性。这是因为需要识别特定于任务的不确定性（例如，关于语义），这在一般情况下似乎很难定义。这项工作从贝叶斯决策理论的角度解决了这些挑战，从我们的效用由相似性度量表征的假设开始，该相似性度量将生成的响应与假设的真实响应进行比较。我们讨论了这一假设如何实现对模型的主观不确定性及其校准的原则量化。我们进一步推导出一种认知不确定性的度量，基于缺失数据视角及其作为超额风险的特征。所提出的措施可应用于黑盒语言模型。我们在问答和机器翻译任务中展示了所提出的方法，它们从 GPT 和 Gemini 模型中提取了具有广泛意义的不确定性估计并量化了它们的校准。]]></description>
      <guid>https://arxiv.org/abs/2406.05213</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:12 GMT</pubDate>
    </item>
    <item>
      <title>Neural-g：用于混合密度估计的深度学习框架</title>
      <link>https://arxiv.org/abs/2406.05986</link>
      <description><![CDATA[arXiv:2406.05986v1 公告类型：新
摘要：混合（或先验）密度估计是机器学习和统计学中的一个重要问题，尤其是在经验贝叶斯 $g$ 建模中，准确估计先验对于做出良好的后验推断是必要的。在本文中，我们提出了神经-$g$，一种新的基于神经网络的 $g$ 建模估计器。神经-$g$ 使用 softmax 输出层来确保估计的先验是有效的概率密度。在默认超参数下，我们表明神经-$g$ 非常灵活，能够捕获许多未知密度，包括具有平坦区域、重尾和/或不连续性的密度。相比之下，现有方法难以捕捉所有这些先验形状。我们通过建立一个新的通用近似定理来证明神经-$g$ 的合理性，该定理涉及神经网络学习任意概率质量函数的能力。为了加速数值实现的收敛，我们利用加权平均梯度下降法来更新网络参数。最后，我们将神经-$g$ 扩展到多元先验密度估计。我们通过对真实数据集的模拟和分析来说明我们方法的有效性。实现神经-$g$ 的软件包可在 https://github.com/shijiew97/neuralG 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2406.05986</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:11 GMT</pubDate>
    </item>
    <item>
      <title>差异隐私最佳臂识别</title>
      <link>https://arxiv.org/abs/2406.06408</link>
      <description><![CDATA[arXiv:2406.06408v1 公告类型：新 
摘要：最佳臂识别 (BAI) 问题逐渐用于数据敏感型应用，例如设计自适应临床试验、调整超参数和开展用户研究。受这些应用程序引起的数据隐私问题的启发，我们研究了在局部和中心模型中具有固定置信度的 BAI 问题，即 $\epsilon$-local 和 $\epsilon$-global 差分隐私 (DP)。首先，为了量化隐私成本，我们推导出满足 $\epsilon$-global DP 或 $\epsilon$-local DP 的任何 $\delta$-correct BAI 算法的样本复杂度的下限。我们的下限表明存在两种隐私制度。在高隐私制度下，硬度取决于隐私和涉及总变差的新信息论量的耦合效应。在低隐私制度下，下限降低为非隐私下限。我们提出了 Top Two 算法的 $\epsilon$-local DP 和 $\epsilon$-global DP 变体，分别是 CTB-TT 和 AdaP-TT*。对于 $\epsilon$-local DP，CTB-TT 通过插入基于随机响应的均值私有估计器实现渐近最优。对于 $\epsilon$-global DP，我们的均值私有估计器在 arm-dependent 自适应事件中运行，并添加拉普拉斯噪声以确保良好的隐私效用权衡。通过调整运输成本，AdaP-TT* 的预期样本复杂度达到渐近下限，最高可达乘法常数。]]></description>
      <guid>https://arxiv.org/abs/2406.06408</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:11 GMT</pubDate>
    </item>
    <item>
      <title>组合顺序干预的结构化学习</title>
      <link>https://arxiv.org/abs/2406.05745</link>
      <description><![CDATA[arXiv:2406.05745v1 公告类型：新
摘要：我们考虑顺序治疗方案，其中每个单元随着时间的推移都会受到干预措施的组合。当干预措施用定性标签描述时，例如“由于疫情而关闭学校一个月”或“在本周向该用户推广此播客”，尚不清楚哪些适当的结构假设允许我们将行为预测推广到以前未见过的组合序列。将分类变量序列映射到输出的标准黑盒方法是适用的，但它们依赖于对如何获得可靠泛化的理解不足的假设，并且可能在稀疏序列、时间变化和大动作空间下表现不佳。为了解决这个问题，我们提出了一个明确的 \emph{composition} 模型，即如何将顺序干预的效果分离成模块，阐明哪些数据条件允许识别它们在不同单位和时间步骤的组合效果。我们展示了组合模型的识别属性，该模型的灵感来自因果矩阵分解方法的进步，但重点是针对新型干预组合的预测模型，而不是矩阵完成任务和因果效应估计。我们将我们的方法与灵活但通用的黑盒模型进行比较，以说明结构如何帮助预测稀疏数据条件下的结果。]]></description>
      <guid>https://arxiv.org/abs/2406.05745</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:10 GMT</pubDate>
    </item>
    <item>
      <title>分布式稳健安全样本筛选</title>
      <link>https://arxiv.org/abs/2406.05964</link>
      <description><![CDATA[arXiv:2406.05964v1 公告类型：新
摘要：在本研究中，我们提出了一种称为分布稳健安全样本筛选 (DRSSS) 的机器学习方法。DRSSS 旨在识别不必要的训练样本，即使训练样本的分布在未来发生变化。为了实现这一目标，我们有效地结合了分布稳健 (DR) 范式（旨在增强模型对数据分布变化的稳健性）和安全样本筛选 (SSS)（在模型训练之前识别不必要的训练样本）。由于我们需要考虑无数种与分布变化有关的情景，因此我们应用了 SSS，因为它不需要在分布改变后进行模型训练。在本文中，我们采用协变量转移框架来表示训练样本的分布，并将 DR 协变量转移问题重新表述为加权经验风险最小化问题，其中权重在预定范围内受不确定性影响。通过扩展现有的 SSS 技术来适应这种权重不确定性，DRSSS 方法能够在指定范围内的任何未来分布下可靠地识别不必要的样本。我们为 DRSSS 方法提供了理论保证，并通过在合成数据集和真实数据集上的数值实验验证了其性能。]]></description>
      <guid>https://arxiv.org/abs/2406.05964</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:10 GMT</pubDate>
    </item>
    <item>
      <title>面板数据中的异质性治疗效应</title>
      <link>https://arxiv.org/abs/2406.05633</link>
      <description><![CDATA[arXiv:2406.05633v1 公告类型：新
摘要：我们解决了因果推断中的一个核心问题：使用具有一般治疗模式的面板数据估计异质治疗效果。许多现有方法要么没有利用面板数据中潜在的底层结构，要么在允许的治疗模式方面存在限制。在这项工作中，我们提出并评估了一种新方法，该方法首先使用回归树将观测值划分为具有相似治疗效果的不相交簇，然后利用面板数据的（假设）低秩结构来估计每个簇的平均治疗效果。我们的理论结果确定了所得估计值与真实治疗效果的收敛性。使用半合成数据的计算实验表明，与使用不超过 40 个叶子的回归树的替代方法相比，我们的方法实现了更高的准确性。因此，我们的方法比其他方法提供了更准确和可解释的估计值。]]></description>
      <guid>https://arxiv.org/abs/2406.05633</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:09 GMT</pubDate>
    </item>
    <item>
      <title>语境连续体盗贼：静态遗憾与动态遗憾</title>
      <link>https://arxiv.org/abs/2406.05714</link>
      <description><![CDATA[arXiv:2406.05714v1 公告类型：新
摘要：我们研究上下文连续强盗问题，其中学习者依次接收一个边信息向量，并且必须在凸集中选择一个动作，以最小化与上下文相关的函数。目标是最小化接收到的上下文的所有底层函数，从而产生动态（上下文）遗憾概念，这比标准的静态遗憾更强。假设目标函数相对于上下文是 H\&quot;older，我们证明任何实现次线性静态遗憾的算法都可以扩展以实现次线性动态遗憾。我们进一步研究了强凸和平滑函数在观测值有噪声的情况下的情况。受内点法的启发，并采用自协调障碍，我们提出了一种实现次线性动态遗憾的算法。最后，我们提出了一个极小最大下界，这意味着两个关键事实。首先，没有算法可以在相对于上下文不连续的函数上实现次线性动态遗憾。其次，对于强凸和平滑函数，我们提出的算法实现了对查询数函数的极小最大动态遗憾率，最高可达对数因子。]]></description>
      <guid>https://arxiv.org/abs/2406.05714</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:09 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络在近似和估计中适应函数规律和数据分布</title>
      <link>https://arxiv.org/abs/2406.05320</link>
      <description><![CDATA[arXiv:2406.05320v1 公告类型：新
摘要：深度学习在不同领域都表现出了显著的成果。为了了解它的成功，大量的研究已经转向了它的理论基础。然而，这些研究中的大多数都研究了深度神经网络如何很好地对具有均匀规律性的函数进行建模。在本文中，我们探索了一个不同的角度：深度神经网络如何适应不同位置和尺度上的函数的不同规律以及不均匀的数据分布。更准确地说，我们关注由非线性树型近似定义的一类广泛的函数。这类函数包括一系列函数类型，例如具有均匀规律性的函数和不连续函数。我们使用深度 ReLU 网络为这类函数开发了非参数近似和估计理论。我们的结果表明，深度神经网络可以适应不同位置和尺度上的函数的不同规律和非均匀的数据分布。我们将结果应用于几个函数类，并推导出相应的近似和泛化误差。通过数值实验证明了结果的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.05320</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:08 GMT</pubDate>
    </item>
    <item>
      <title>弥合差距：稳健和标准泛化的 Rademacher 复杂性</title>
      <link>https://arxiv.org/abs/2406.05372</link>
      <description><![CDATA[arXiv:2406.05372v1 公告类型：新
摘要：使用对抗性示例训练深度神经网络 (DNN) 通常会导致对测试时间对抗性数据的泛化能力较差。本文通过 Rademacher 复杂性的视角研究了这个问题，即所谓的对抗性鲁棒泛化。在 Khim 和 Loh (2018)；Yin 等人 (2019) 的研究基础上，已经有大量研究致力于解决这一问题，但实现令人满意的界限仍然是一个难以实现的目标。现有的 DNN 研究要么适用于替代损失而不是鲁棒损失，要么产生与标准对应物相比明显更宽松的界限。在后一种情况下，边界对 DNN 的宽度 $m$ 或数据的维度 $d$ 有更高的依赖性，并且至少有一个额外的因子 $\mathcal{O}(\sqrt{m})$ 或 $\mathcal{O}(\sqrt{d})$。
本文提出了 DNN 对抗 Rademacher 复杂度的上限，该上限与 Bartlett 等人 (2017) 的工作中确定的标准设置中最著名的上限相匹配，对宽度和维度的依赖性为 $\mathcal{O}(\ln(dm))$。解决的核心挑战是计算对抗函数类的覆盖数。我们的目标是构建一个具有两个属性的新覆盖：1) 与对抗示例的兼容性，2) 与标准设置中使用的覆盖相当的精度。为此，我们引入了一种新的覆盖数变体，称为 \emph{均匀覆盖数}，专门设计和证明可以协调这两个属性。因此，我们的方法有效地弥合了稳健和标准泛化中 Rademacher 复杂度之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2406.05372</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:08 GMT</pubDate>
    </item>
    <item>
      <title>通过树流进行密度回归的生成建模</title>
      <link>https://arxiv.org/abs/2406.05260</link>
      <description><![CDATA[arXiv:2406.05260v1 公告类型：新
摘要：表格数据分析的一个常见目标是估计给定一组“协变量”的一组“结果”变量的条件分布（而不是仅产生预测），这有时被称为“密度回归”问题。除了对条件分布的估计之外，还需要从学习到的条件分布中抽取合成样本的生成能力，因为它进一步扩大了应用范围。我们提出了一种基于流的生成模型，专门用于表格数据的密度回归任务。我们的流程对初始均匀噪声应用一系列基于树的分段线性变换，最终从给定协变量的复杂条件密度（单变量或多变量）结果中生成样本，并允许对样本空间中任何点的拟合条件密度进行有效的分析评估。我们引入了一种训练算法，使用分而治之的策略来拟合基于树的变换，该策略将树流的最大似然训练转换为在交叉熵损失下训练一组二元分类器（每个树分裂一个）。我们在样本外似然评估下评估了我们的方法的性能，并将其与一系列模拟和真实基准表格数据集上的各种最先进的条件密度学习器进行了比较。我们的方法始终以极低的训练和采样预算实现可比或更优异的性能。最后，我们通过基于在公开可用的微生物组研究上训练我们的流程来生成合成纵向微生物组组成数据，证明了我们方法的生成能力的实用性。]]></description>
      <guid>https://arxiv.org/abs/2406.05260</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:07 GMT</pubDate>
    </item>
    </channel>
</rss>