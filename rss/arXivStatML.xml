<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>贝叶斯优化中熵搜索和预期改进的统一框架</title>
      <link>https://arxiv.org/abs/2501.18756</link>
      <description><![CDATA[arXiv:2501.18756v1 公告类型：新
摘要：贝叶斯优化是一种广泛用于优化昂贵黑盒函数的方法，其中预期改进是最常用的获取函数之一。相比之下，信息论获取函数旨在减少对函数最优值的不确定性，通常被认为与 EI 根本不同。在这项工作中，我们通过引入一个统一的理论框架变分熵搜索来挑战这种流行的观点，这表明 EI 和信息论获取函数的关系比以前认识到的更密切。我们证明 EI 可以解释为流行的信息论获取函数的变分推理近似，称为最大值熵搜索。基于这一见解，我们提出了 VES-Gamma，这是一种平衡 EI 和 MES 优势的新型获取函数。在低维和高维合成和真实世界基准上进行的大量实证评估表明，VES-Gamma 具有与最先进的采集功能相竞争力，并且在许多情况下优于 EI 和 MES。]]></description>
      <guid>https://arxiv.org/abs/2501.18756</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散生成模型中概率流微分方程的适应性和收敛性</title>
      <link>https://arxiv.org/abs/2501.18863</link>
      <description><![CDATA[arXiv:2501.18863v1 公告类型：新
摘要：基于分数的生成模型通过学习逆转扩散过程将噪声转化为数据，已成为现代生成式人工智能的基石。本文有助于为概率流 ODE 建立理论保证，概率流 ODE 是一种广泛使用的基于扩散的采样器，以其实际效率而闻名。虽然许多先前的工作都涉及其一般收敛理论，但概率流 ODE 采样器是否能够适应自然图像数据中常见的低维结构仍不清楚。我们证明，通过准确的分数函数估计，概率流 ODE 采样器在总变分距离（忽略对数因子）中实现了 $O(k/T)$ 的收敛速度，其中 $k$ 是目标分布的固有维度，$T$ 是迭代次数。这种无量纲收敛速度改进了现有的结果，这些结果可以随通常更大的环境维度而扩展，突出了概率流 ODE 采样器利用目标分布中固有的低维结构进行更快采样的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.18863</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成式 AI 模型的可信评估</title>
      <link>https://arxiv.org/abs/2501.18897</link>
      <description><![CDATA[arXiv:2501.18897v1 公告类型：新
摘要：生成式人工智能 (GenAI) 模型最近在各种应用中取得了显著的经验表现，然而，它们的评估仍然缺乏不确定性量化。在本文中，我们提出了一种基于相对性能差距的无偏估计量来比较两个生成模型的方法。从统计上讲，我们的估计量实现了参数收敛速度和渐近正态性，从而实现了有效的推理。从计算上讲，我们的方法是有效的，可以通过并行计算和利用预存储的中间结果来加速。在具有已知基本事实的模拟数据集上，我们展示了我们的方法有效地控制了 I 型错误并实现了与常用指标相当的功效。此外，我们展示了我们的方法在以统计置信度评估真实图像数据集上的扩散模型方面的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.18897</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过变化进行优化：时变贝叶斯优化算法的界限和建议</title>
      <link>https://arxiv.org/abs/2501.18963</link>
      <description><![CDATA[arXiv:2501.18963v1 公告类型：新
摘要：时变贝叶斯优化 (TVBO) 是优化时变、昂贵、嘈杂的黑盒函数的首选框架。然而，迄今为止提出的大多数解决方案要么依赖于对目标函数性质的不切实际的假设，要么不提供任何理论保证。我们提出了第一种分析方法，该分析仅在温和而现实的假设下渐近地限制了 TVBO 算法的累积遗憾。具体来说，我们提供了一个与算法无关的下限遗憾界和一个适用于大量 TVBO 算法的上限遗憾界。基于此分析，我们为 TVBO 算法制定了建议，并通过对合成问题和现实问题的实验展示了遵循这些建议的算法 (BOLT) 如何比最先进的 TVBO 表现更好。]]></description>
      <guid>https://arxiv.org/abs/2501.18963</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于最优传输的共形预测</title>
      <link>https://arxiv.org/abs/2501.18991</link>
      <description><![CDATA[arXiv:2501.18991v1 公告类型：新
摘要：共形预测 (CP) 是一种量化黑盒学习模型中不确定性的原则框架，通过构建具有有限样本覆盖保证的预测集。传统方法依赖于标量不一致分数，无法充分利用多变量输出的几何结构，例如在多输出回归或多类分类中。解决这一限制的最新方法为预测集施加了预定义的凸形状，可能与内在数据几何不一致。我们引入了一种新颖的 CP 程序，通过最优传输的视角处理多元分数函数。具体而言，我们利用 Monge-Kantorovich 向量秩和分位数来构建具有灵活、潜在非凸形状的预测区域，更适合多元学习任务中遇到的复杂不确定性模式。我们证明我们的方法确保有限样本、无分布的覆盖属性，类似于典型的 CP 方法。然后，我们将我们的方法调整为多输出回归和多类分类，并提出简单的调整以生成具有渐近条件覆盖保证的自适应预测区域。最后，我们在实际回归和分类问题上评估了我们的方法，说明了它在（条件）覆盖和效率方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2501.18991</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分层分类中的共形预测</title>
      <link>https://arxiv.org/abs/2501.19038</link>
      <description><![CDATA[arXiv:2501.19038v1 公告类型：新
摘要：共形预测已成为在分类和回归任务中构建有效预测集的广泛使用的框架。在这项工作中，我们将分割共形预测框架扩展到分层分类，其中预测集通常限制为预定义层次结构的内部节点，并提出了两种计算效率高的推理算法。第一个算法将内部节点作为预测集返回，而第二个算法使用表示复杂性的概念放宽了这一限制，产生了更一般和组合的推理问题，但集合大小更小。对几个基准数据集的实证评估证明了所提出的算法在实现名义覆盖方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.19038</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在按需汽车共享网络中重新定位的同时进行学习</title>
      <link>https://arxiv.org/abs/2501.19208</link>
      <description><![CDATA[arXiv:2501.19208v1 公告类型：新
摘要：我们考虑一个由单向、按需车辆共享服务驱动的网络库存问题。由于需求和回报的不确定性，以及 $n$ 个位置网络中固定数量的租赁单位，服务提供商必须定期重新定位车辆，以在空间上匹配供应和需求，同时最小化成本。在一般 $n$ 个位置网络下，如果不知道最优值函数，则无法解决最优重新定位策略。我们引入最佳基本库存重新定位策略作为经典库存控制策略到 $n$ 维的推广，并在一般网络结构下在两个不同的限制范围内建立其渐近最优性。我们提出了重新表述，以便在离线环境中使用预先收集的数据有效地计算这种最佳基本库存策略。
在线设置中，我们表明自然 Lipschitz-bandit 方法实现了 $\widetilde{O}(T^{\frac{n}{n+1}})$ 的遗憾保证，但存在对 $n$ 的指数依赖性。我们通过遗憾下限分析和展示替代算法方法的次优性，说明了在网络系统中使用删失数据进行学习的挑战。受这些挑战的启发，我们提出了一种仅依赖于删失需求的在线梯度重新定位算法。在温和的成本结构假设下，我们证明它实现了 $O(n^{2.5} \sqrt{T})$ 的最佳遗憾，这与 $T$ 中的遗憾下限相匹配，并且仅实现了对 $n$ 的多项式依赖。关键的算法创新涉及提出替代成本以解开跨期依赖关系，并利用对偶解来找到政策变化的梯度。数值实验证明了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.19208</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论多项式逻辑赌博机的帕累托最优性</title>
      <link>https://arxiv.org/abs/2501.19277</link>
      <description><![CDATA[arXiv:2501.19277v1 公告类型：新
摘要：我们提供了一种新的在线学习算法来解决多项 Logit Bandit (MNL-Bandit) 问题。尽管 MNL 模型的组合性质带来了挑战，但我们开发了一种基于上限置信度 (UCB) 的新方法，通过平衡遗憾最小化和分类收入和 MNL 参数的估计误差来实现帕累托最优。我们通过信息论界限开发了表征 MNL-Bandit 问题遗憾与估计误差之间权衡的理论保证，并提出了一种改进的 UCB 算法，该算法结合强制探索来提高参数估计精度，同时保持较低的遗憾。我们的分析为如何在动态分类优化中最佳地平衡收集的收入和治疗估计提供了重要的见解。]]></description>
      <guid>https://arxiv.org/abs/2501.19277</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>因果模型和表示的因果关系是什么？</title>
      <link>https://arxiv.org/abs/2501.19335</link>
      <description><![CDATA[arXiv:2501.19335v1 公告类型：新
摘要：因果贝叶斯网络是“因果”模型，因为它们对干预分布做出预测。为了将这种因果模型预测与现实世界的结果联系起来，我们必须确定世界上的哪些行为对应于模型中的哪些干预。例如，要将某个行为解释为对治疗变量的干预，该行为可能必须 a) 以与干预相对应的方式改变治疗的分布，以及 b) 不改变其他方面，例如结果如何取决于治疗；而某些变量的边际分布可能会因影响而改变。我们引入了一个正式框架，以使对作为干预的不同行为的解释的要求变得精确。我们证明，将行为解释为干预的看似自然的解释是循环的：在这种解释下，每个正确建模观察分布的因果贝叶斯网络在干预上也是有效的，并且没有任何行为会产生可能证伪这种模型的经验数据。我们证明了一个不可能的结果：不存在非循环的解释，同时满足一组自然要求。相反，我们研究可能违反某些要求的非循环解释，并展示这反过来如何导致因果模型的证伪。通过严格研究因果贝叶斯网络如何成为世界的“因果”模型而不仅仅是数学对象，我们的形式框架为因果表示学习、因果发现和因果抽象的概念基础做出了贡献，同时也强调了现有方法的一些局限性。]]></description>
      <guid>https://arxiv.org/abs/2501.19335</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越固定视野：自适应去噪扩散的理论框架</title>
      <link>https://arxiv.org/abs/2501.19373</link>
      <description><![CDATA[arXiv:2501.19373v1 公告类型：新
摘要：我们引入了一类新的生成扩散模型，与传统的去噪扩散模型不同，该模型实现了噪声和去噪过程的时间同质结构，允许步骤数根据噪声水平自适应调整。这是通过使用 Doob 的 $h$ 变换来调节正向过程来实现的，该变换在随机时间以合适的采样分布终止该过程。该模型特别适合生成具有较低固有维度的数据，因为终止标准简化为首次命中规则。该模型的一个关键特性是它对目标数据的适应性，使用预先训练的无条件生成模型实现各种下游任务。这些任务包括通过适当初始化去噪过程和对噪声数据进行分类来进行自然调节。]]></description>
      <guid>https://arxiv.org/abs/2501.19373</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随着批次大小的增加，黎曼随机梯度下降的收敛速度更快</title>
      <link>https://arxiv.org/abs/2501.18164</link>
      <description><![CDATA[arXiv:2501.18164v1 公告类型：交叉 
摘要：机器学习中使用的许多模型已经变得如此之大，以至于即使计算机计算损失函数的完整梯度也是不切实际的。这使得有必要使用有限的可用信息（例如批量大小和学习率）来有效地训练模型。我们从理论上分析了黎曼随机梯度下降 (RSGD) 的使用，发现使用增加的批量大小比使用恒定的批量大小（不仅在恒定的学习率下，而且在衰减的学习率下，例如余弦退火衰减和多项式衰减）可以更快地收敛 RSGD。具体而言，RSGD 的收敛速度 $O(\frac{1}{\sqrt{T}})$ 比现有的学习率 $O(\frac{\sqrt{\log T}}{\sqrt[4]{T}})$ 更好，其中 $T$ 是迭代次数。在主成分分析和低秩矩阵完成问题上的实验结果证实，除了 MovieLens 数据集和恒定学习率之外，使用多项式增长批量大小或指数增长批量大小比使用恒定批量大小可获得更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.18164</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过放宽边际约束的最优传输构建细胞类型分类</title>
      <link>https://arxiv.org/abs/2501.18650</link>
      <description><![CDATA[arXiv:2501.18650v1 公告类型：交叉 
摘要：单细胞数据的快速出现促进了在细胞水平上对许多不同生物状况的研究。聚类分析已广泛应用于识别细胞类型，以更简洁的形式捕获原始数据的基本模式。细胞聚类分析的一个挑战是匹配从不同来源或条件的数据集中提取的聚类。许多现有算法在建立从两个样本获得的聚类之间的对应关系时，无法识别仅存在于两个样本中的一个样本中的新细胞类型。此外，当有两个以上的样本时，同时对齐所有样本的聚类比执行成对对齐更有利。我们的方法旨在为所有样本的细胞簇构建一个分类法，以更好地注释这些簇并有效提取特征以进行下游分析。通过结合具有放松边际约束的最佳传输 (OT-RMC) 技术以及同时对齐多个样本的簇，开发了一种用于构建细胞类型分类法的新系统。 OT-RMC 使我们能够应对样本间簇比例差异很大或某些簇并非出现在所有样本中时出现的挑战。在 20 多个数据集上的实验表明，此新系统构建的分类法可以高度准确地注释细胞类型。此外，基于分类法提取的样本级特征可实现样本的准确分类。]]></description>
      <guid>https://arxiv.org/abs/2501.18650</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于增强小样本的合成数据生成</title>
      <link>https://arxiv.org/abs/2501.18741</link>
      <description><![CDATA[arXiv:2501.18741v1 公告类型：交叉 
摘要：小数据集在健康研究中很常见。然而，当训练数据集较小时，机器学习模型的泛化性能并不理想。为了解决这个问题，数据增强是一种解决方案。增强增加了样本量，被视为一种正则化形式，增加了小数据集的多样性，使它们在看不见的数据上表现更好。我们发现增强可以提高以下数据集的预后性能：观察次数较少、基线 AUC 较小、基数分类变量较高、结果变量更平衡。没有一个特定的生成模型始终优于其他模型。我们开发了一个决策支持模型，可用于告知分析师增强是否有用。对于七个小型应用数据集，增强现有数据可使 AUC 增加 4.31%（AUC 从 0.71 增加到 0.75）和 ​​43.23%（AUC 从 0.51 增加到 0.73），平均相对改善 15.55%，表明增强对小型数据集的影响非同小可（p=0.0078）。增强 AUC 高于仅重采样 AUC（p=0.016）。增强数据集的多样性高于重采样数据集的多样性（p=0.046）。]]></description>
      <guid>https://arxiv.org/abs/2501.18741</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用已知观测模型在平均奖励 POMDP 中实现 $\widetilde{\mathcal{O}}(\sqrt{T})$ 遗憾</title>
      <link>https://arxiv.org/abs/2501.18790</link>
      <description><![CDATA[arXiv:2501.18790v1 公告类型：交叉 
摘要：我们使用未知的转换模型但已知的观察模型来解决平均奖励无限期 POMDP，这种设置以前曾以两种限制性方式解决：（i）依赖于次优随机策略的频率方法，选择每个动作的概率最小，以及（ii）采用最优策略类但需要对所用估计量的一致性做出强假设的贝叶斯方法。我们的工作通过为转换模型提供方便的估计保证并引入一种利用确定性信念策略的最佳类的乐观算法来消除这些限制。我们对现有的估计技术进行了修改，为每个估计的动作转换矩阵分别提供理论保证。与无法使用来自不同策略的样本的现有估计方法不同，我们提出了一种新颖而简单的估计量来克服这一障碍。这种新的数据高效技术与所提出的 \emph{Action-wise OAS-UCRL} 算法和更严格的理论分析相结合，使得第一种方法与最优策略相比具有 $\mathcal{O}(\sqrt{T \,\log T})$ 阶的遗憾保证，从而优于最先进的技术。最后，通过数值模拟验证了理论结果，表明我们的方法相对于基线方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.18790</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>单调神经网络集成的偏好探索贝叶斯优化</title>
      <link>https://arxiv.org/abs/2501.18792</link>
      <description><![CDATA[arXiv:2501.18792v1 公告类型：交叉 
摘要：许多现实世界的黑盒优化问题都有多个相互冲突的目标。交互式偏好学习不是试图近似整个帕累托最优解集，而是允许将搜索重点放在最相关的子集上。然而，之前很少有研究利用效用函数通常是单调的这一事实。在本文中，我们解决了带偏好探索的贝叶斯优化 (BOPE) 问题，并提出使用神经网络集成作为效用替代模型。这种方法自然地整合了单调性并支持成对比较数据。我们的实验表明，所提出的方法优于最先进的方法，并且在效用评估中表现出对噪声的鲁棒性。一项消融研究强调了单调性在提高性能方面的关键作用。]]></description>
      <guid>https://arxiv.org/abs/2501.18792</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>