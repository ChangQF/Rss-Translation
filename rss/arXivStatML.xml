<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 15 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>强盗优化和控制的二阶方法</title>
      <link>https://arxiv.org/abs/2402.08929</link>
      <description><![CDATA[arXiv:2402.08929v1 公告类型：交叉
摘要：强盗凸优化（BCO）是不确定性下在线决策的通用框架。虽然已经建立了一般凸损失的严格后悔界限，但实现这些界限的现有算法对于高维数据来说计算成本过高。
  在本文中，我们受到在线牛顿步算法的启发，提出了一种简单实用的 BCO 算法。我们表明，我们的算法为一大类我们称为 $\kappa$-凸的凸函数实现了最佳（就视野而言）后悔边界。此类包含各种实际相关的损失函数，包括线性、二次和广义线性模型。除了最佳遗憾之外，该方法还是最有效的已知算法，适用于包括老虎机逻辑回归在内的多个经过充分研究的应用。
  此外，我们研究了二阶老虎机算法对内存在线凸优化的适应性。我们表明，对于具有一定仿射结构的损失函数，扩展算法获得了最佳遗憾。这导致了在完全对抗性噪声模型下对强盗 LQR/LQG 问题具有最佳遗憾的算法，从而解决了 \citep{gradu2020non} 和 \citep{sun2023optimal} 中提出的悬而未决的问题。
  最后，我们证明了（非仿射）内存的 BCO 更普遍的问题更难。即使在平滑和二次损失的假设下，我们也得出$\tilde{\Omega}(T^{2/3})$后悔下限。]]></description>
      <guid>https://arxiv.org/abs/2402.08929</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>通过近点法减少随机优化中的方差并降低样本复杂度</title>
      <link>https://arxiv.org/abs/2402.08992</link>
      <description><![CDATA[arXiv:2402.08992v1 公告类型：交叉
摘要：本文提出了一种随机近点法来解决随机凸复合优化问题。随机优化中的高概率结果通常取决于对随机梯度噪声的限制性假设，例如亚高斯分布。假设仅假设随机梯度有界方差等弱条件，本文建立低样本复杂度以获得所提方法收敛的高概率保证。此外，这项工作的一个值得注意的方面是开发了一个子程序来解决近端子问题，这也是一种减少方差的新技术。]]></description>
      <guid>https://arxiv.org/abs/2402.08992</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>立场文件：拓扑深度学习的挑战和机遇</title>
      <link>https://arxiv.org/abs/2402.08871</link>
      <description><![CDATA[arXiv:2402.08871v1 公告类型：交叉
摘要：拓扑深度学习（TDL）是一个快速发展的领域，它利用拓扑特征来理解和设计深度学习模型。本文认为，TDL 可以通过结合拓扑概念来补充图表示学习和几何深度学习，从而为各种机器学习设置提供自然的选择。为此，本文讨论了 TDL 中的开放问题，从实际效益到理论基础。对于每个问题，它概述了潜在的解决方案和未来的研究机会。同时，本文也邀请科学界积极参与TDL研究，以释放这一新兴领域的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.08871</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>镜像影响假说：通过利用前向传播进行有效的数据影响估计</title>
      <link>https://arxiv.org/abs/2402.08922</link>
      <description><![CDATA[arXiv:2402.08922v1 公告类型：交叉
摘要：大规模黑盒模型已经在众多应用中无处不在。了解各个训练数据源对这些模型做出的预测的影响对于提高其可信度至关重要。当前的影响估计技术涉及计算每个训练点的梯度或对不同子集进行重复训练。当扩展到大型数据集和模型时，这些方法面临着明显的计算挑战。
  在本文中，我们介绍并探索了镜像影响假说，强调了训练数据和测试数据之间影响的相互性质。具体来说，它建议评估训练数据对测试预测的影响可以重新表述为一个等效但相反的问题：评估如果模型在特定测试样本上进行训练，训练样本的预测将如何改变。通过实证和理论验证，我们证明了我们的假设的广泛适用性。受此启发，我们引入了一种估计训练数据影响的新方法，该方法需要计算特定测试样本的梯度，并与每个训练点的前向传播配对。该方法可以利用并发检查的测试样本数量远小于训练数据集规模的场景中常见的不对称性，从而比现有方法获得显着的效率提升。
  我们展示了我们的方法在一系列场景中的适用性，包括扩散模型中的数据归因、数据泄漏检测、记忆分析、错误标记的数据检测和语言模型中的跟踪行为。我们的代码将在 https://github.com/ruoxi-jia-group/Forward-INF 提供。]]></description>
      <guid>https://arxiv.org/abs/2402.08922</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>使用次要结果融合个体化治疗规则</title>
      <link>https://arxiv.org/abs/2402.08828</link>
      <description><![CDATA[arXiv:2402.08828v1 公告类型：交叉
摘要：个体化治疗规则（ITR）是一种根据患者个体特征变量推荐治疗的决策规则。在许多实践中，主要结果的理想 ITR 也有望对其他次要结果造成最小的损害。因此，我们的目标是学习一个不仅最大化主要结果的价值函数，而且尽可能接近次要结果的最优规则的 ITR。为了实现这一目标，我们引入了融合惩罚，以鼓励基于不同结果的 ITR 产生类似的建议。提出了两种使用替代损失函数来估计 ITR 的算法。我们证明，与不考虑次要结果的情况相比，主要结果的估计 ITR 与次要结果的最佳 ITR 之间的一致性率更快地收敛到真实一致性率。此外，我们推导了该方法的价值函数和误分类率的非渐近性质。最后，使用仿真研究和真实数据示例来证明所提出方法的有限样本性能。]]></description>
      <guid>https://arxiv.org/abs/2402.08828</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>关系函数和注意力机制的近似</title>
      <link>https://arxiv.org/abs/2402.08856</link>
      <description><![CDATA[arXiv:2402.08856v1 公告类型：交叉
摘要：神经网络特征图的内积作为输入之间关系建模的方法出现在各种机器学习框架中。这项工作研究神经网络内积的近似性质。结果表明，多层感知器与其自身的内积是对称正定关系函数的通用逼近器。在不对称关系函数的情况下，表明两个不同多层感知器的内积是通用逼近器。在这两种情况下，都获得了实现给定近似精度所需的神经元数量的界限。在对称情况下，函数类可以用再生核希尔伯特空间的核来识别，而在非对称情况下，函数类可以用再生核巴拿赫空间的核来识别。最后，将这些近似结果应用于分析 Transformer 底层的注意力机制，表明任何由抽象预序定义的检索机制都可以通过其内积关系由注意力来近似。这个结果利用了经济学中的德布鲁表示定理，用效用函数来表示偏好关系。]]></description>
      <guid>https://arxiv.org/abs/2402.08856</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>范数有界无限宽度神经网络中的深度分离</title>
      <link>https://arxiv.org/abs/2402.08808</link>
      <description><![CDATA[arXiv:2402.08808v1 公告类型：交叉
摘要：我们研究无限宽度神经网络中的深度分离，其中复杂性由权重的总体平方 $\ell_2$-范数（网络中所有权重的平方和）控制。虽然之前的深度分离结果集中在宽度方面的分离，但这样的结果并没有深入了解深度是否决定是否有可能学习一个即使在网络宽度无界时也能很好地泛化的网络。在这里，我们根据可学习性所需的样本复杂性来研究分离。具体来说，我们表明，有些函数可以通过范数控制的深度 3 ReLU 网络在输入维度上以样本复杂度多项式学习，但不能通过范数控制的深度 2 ReLU 网络以次指数样本复杂度学习（使用任何标准值）。我们还表明，相反方向上的类似陈述是不可能的：任何可以通过具有无限宽度的范数控制的深度 2 ReLU 网络以多项式样本复杂性学习的函数也可以通过范数控制的深度 3 以多项式样本复杂性学习ReLU 网络。]]></description>
      <guid>https://arxiv.org/abs/2402.08808</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>将算法公平性与官方统计和调查制作中机器学习的质量维度联系起来</title>
      <link>https://arxiv.org/abs/2402.09328</link>
      <description><![CDATA[arXiv:2402.09328v1 公告类型：新
摘要：国家统计组织 (NSO) 越来越多地利用机器学习 (ML) 来提高其产品的及时性和成本效益。在引入机器学习解决方案时，国家统计局必须确保在稳健性、可重复性和准确性方面保持高标准，例如统计算法质量框架（QF4SA；Yung 等人，2022）。与此同时，越来越多的研究关注公平性，将其作为安全部署机器学习的先决条件，以防止实践中出现不同的社会影响。然而，在国家统计局应用机器学习的背景下，公平性尚未被明确讨论为质量方面。我们聘请了 Yung 等人。 (2022) 的 QF4SA 质量框架，并提出其质量维度到算法公平性的映射。因此，我们以多种方式扩展了 QF4SA 框架：我们主张公平作为其自身的质量维度，我们研究公平与其他维度的相互作用，并且我们明确地处理数据本身及其与应用方法的相互作用。在提供实证插图的同时，我们展示了我们的映射如何为官方统计、算法公平性和值得信赖的机器学习领域的方法论做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2402.09328</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>具有时变约束的无投影在线凸优化</title>
      <link>https://arxiv.org/abs/2402.08799</link>
      <description><![CDATA[arXiv:2402.08799v1 公告类型：交叉
摘要：我们考虑设置具有对抗性时变约束的在线凸优化，其中动作必须是可行的。一个固定的约束集，并且平均还需要近似满足附加的时变约束。受固定可行集（硬约束）难以投影的场景的启发，我们考虑仅通过线性优化预言机（LOO）访问该集的无投影算法。我们提出了一种算法，在长度为 $T$ 的序列上并使用对 LOO 的总体 $T$ 调用，保证 $\tilde{O}(T^{3/4})$ 后悔。损失和 $O(T^{7/8})$ 约束违规（忽略除 $T$ 之外的所有数量）。特别是，这些界限成立。序列的任意间隔。我们还提出了一种更有效的算法，该算法仅需要一阶预言机访问软约束并实现类似的边界。整个序列。我们将后者扩展到强盗反馈的设置，并在期望中获得类似的界限（作为 $T$ 的函数）。]]></description>
      <guid>https://arxiv.org/abs/2402.08799</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>混合输出高斯过程潜变量模型</title>
      <link>https://arxiv.org/abs/2402.09122</link>
      <description><![CDATA[arXiv:2402.09122v1 公告类型：新
摘要：这项工作开发了一种贝叶斯非参数信号分离方法，其中信号可能根据潜在变量而变化。我们的主要贡献是增强高斯过程潜变量模型（GPLVM），以纳入每个数据点包含在多个输入位置观察到的已知数量纯分量信号的加权和的情况。我们的框架允许对每个观察的权重使用一系列先验。这种灵活性使我们能够表示用例，包括用于估计分数组成的和对一约束以及用于分类的二进制权重。我们的贡献与光谱学特别相关，其中条件的变化可能会导致潜在的纯成分信号因样品而异。为了证明光谱学和其他领域的适用性，我们考虑了几种应用：具有不同温度的近红外光谱数据集、用于识别通过管道的流动配置的模拟数据集，以及用于确定岩石类型的数据集。它的反射率。]]></description>
      <guid>https://arxiv.org/abs/2402.09122</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>迈向基于模型的鲁棒强化学习，对抗对抗性腐败</title>
      <link>https://arxiv.org/abs/2402.08991</link>
      <description><![CDATA[arXiv:2402.08991v1 公告类型：新
摘要：这项研究解决了基于模型的强化学习（RL）中对抗性腐败的挑战，其中过渡动态可能会被对手破坏。现有关于腐败鲁棒强化学习的研究主要集中在无模型强化学习的设置上，其中鲁棒最小二乘回归通常用于价值函数估计。然而，这些技术不能直接应用于基于模型的强化学习。在本文中，我们关注基于模型的强化学习，并采用最大似然估计（MLE）方法来学习转换模型。我们的工作包括线上和线下设置。在在线环境中，我们引入了一种称为腐败鲁棒乐观 MLE (CR-OMLE) 的算法，该算法利用基于总变异 (TV) 的信息比率作为 MLE 的不确定性权重。我们证明 CR-OMLE 实现了 $\tilde{\mathcal{O}}(\sqrt{T} + C)$ 的遗憾，其中 $C$ 表示 $T$ 事件后的累积损坏级别。我们还证明了一个下界，以表明对 $C$ 的加性依赖是最优的。我们将加权技术扩展到离线设置，并提出了一种名为腐败鲁棒悲观 MLE（CR-PMLE）的算法。在均匀覆盖条件下，CR-PMLE 的次优性恶化了 $\mathcal{O}(C/n)$，几乎与下限匹配。据我们所知，这是第一个基于抗腐败模型的强化学习算法的研究，具有可证明的保证。]]></description>
      <guid>https://arxiv.org/abs/2402.08991</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>神经算子满足基于能量的理论：哈密顿量和耗散偏微分方程的算子学习</title>
      <link>https://arxiv.org/abs/2402.09018</link>
      <description><![CDATA[arXiv:2402.09018v1 公告类型：新
摘要：算子学习近年来受到了广泛的关注，其目的是学习函数空间之间的映射。先前的工作已经提出了深度神经网络（DNN）来学习这种映射，从而能够学习偏微分方程（PDE）的解算子。然而，这些作品仍然难以学习遵守物理定律的动力学。本文提出了能量一致神经算子（ENO），这是一种学习偏微分方程解算子的通用框架，它遵循观察到的解轨迹的能量守恒定律或耗散定律。我们引入了一种新颖的惩罚函数，其灵感来自基于能量的物理训练理论，其中能量函数由另一个 DNN 建模，允许对基于 DNN 的解算子的输出进行偏置，以确保能量一致性，而无需显式偏微分方程。在多个物理系统上的实验表明，ENO 在根据数据预测解决方案方面优于现有的 DNN 模型，尤其是在超分辨率设置中。]]></description>
      <guid>https://arxiv.org/abs/2402.09018</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>基于梯度的优化中的走廊几何形状</title>
      <link>https://arxiv.org/abs/2402.08818</link>
      <description><![CDATA[arXiv:2402.08818v1 公告类型：新
摘要：当最速下降的连续曲线（梯度流的解）变成直线时，我们将损失表面的区域描述为走廊。我们表明，走廊为基于梯度的优化提供了见解，因为走廊正是梯度下降和梯度流遵循相同轨迹的区域，而损失线性下降。因此，走廊内部不存在因梯度下降和梯度流之间的漂移而出现的隐式正则化效应或训练不稳定性。利用走廊上的损失线性减少，我们设计了一种梯度下降的学习率自适应方案；我们将此方案称为走廊学习率（CLR）。 CLR 公式与在凸优化背景下发现的 Polyak 步长大小的特殊情况一致。最近，Polyak 步长被证明对神经网络也具有良好的收敛特性；我们通过 CIFAR-10 和 ImageNet 上的结果进一步证实了这一点。]]></description>
      <guid>https://arxiv.org/abs/2402.08818</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>时空桥-扩散</title>
      <link>https://arxiv.org/abs/2402.08847</link>
      <description><![CDATA[arXiv:2402.08847v1 公告类型：新
摘要：在这项研究中，我们介绍了一种新的方法，用于生成新的合成样本，这些样本与高维实值概率分布独立同分布（i.i.d.），由一组地面实况（GT）样本隐式定义。我们方法的核心是跨时间和空间维度的时空混合策略的整合。我们的方法以三个相互关联的随机过程为基础，旨在实现从易于处理的初始概率分布到由 GT 样本表示的目标分布的最佳传输：（a）结合时空混合的线性过程，产生高斯条件概率密度，（b） ）它们的桥扩散类似物以初始和最终状态向量为条件，以及（c）通过分数匹配技术细化的非线性随机过程。我们训练机制的关键在于微调非线性模型，以及潜在的线性模型 - 以与 GT 数据紧密结合。我们通过数值实验验证了时空扩散方法的有效性，为未来更广泛的理论和实验奠定了基础，以充分验证该方法，特别是提供更有效（可能无需模拟）的推理。]]></description>
      <guid>https://arxiv.org/abs/2402.08847</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>对“遍历随机微分方程数值近似分布的 Wasserstein 距离估计”的更正</title>
      <link>https://arxiv.org/abs/2402.08711</link>
      <description><![CDATA[arXiv:2402.08711v1 公告类型：新
摘要：Sanz-Serna 和 Zygalakis 在“遍历随机微分方程数值近似分布的 Wasserstein 距离估计”中提出了一种分析 Wasserstein-2 距离中遍历 SDE 数值离散化的非渐近保证的方法。 UBU 积分器是二阶强积分，每一步只需要一次梯度评估，从而产生理想的非渐近保证，特别是 $\mathcal{O}(d^{1/4}\epsilon^{-1/2} )$ 步骤达到 Wasserstein-2 中距目标分布距离 $\epsilon &gt; 0$ 的距离。然而，Sanz-Serna 和 Zygalakis (2021) 中的局部误差估计存在错误，特别是需要更强的假设来实现这些复杂性估计。本文将理论与许多感兴趣的应用中实践中观察到的尺寸依赖性相协调。]]></description>
      <guid>https://arxiv.org/abs/2402.08711</guid>
      <pubDate>Thu, 15 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    </channel>
</rss>