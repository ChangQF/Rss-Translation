<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 13 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用自我知识提炼来指导框架级 CTC 对齐</title>
      <link>https://arxiv.org/abs/2406.07909</link>
      <description><![CDATA[arXiv:2406.07909v1 公告类型：交叉 
摘要：具有联结时间分类 (CTC) 框架的 Transformer 编码器广泛用于自动语音识别 (ASR)。然而，ASR 的知识蒸馏 (KD) 显示出教师-学生模型在帧级对齐方面的不一致问题，这最终阻碍了它提高学生模型的性能。为了解决这个问题，本文介绍了一种自我知识蒸馏 (SKD) 方法，用于在训练期间指导帧级对齐。与使用单独的教师和学生模型的传统方法相比，本研究介绍了一种简单有效的方法，即共享编码器层并将子模型应用为学生模型。总的来说，我们的方法在提高资源效率和性能方面都很有效。我们还对尖峰时间进行了实验分析，以说明所提出的方法通过减少对齐分歧来提高性能。]]></description>
      <guid>https://arxiv.org/abs/2406.07909</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:51 GMT</pubDate>
    </item>
    <item>
      <title>分离潜在 MDP 中的近似最优学习和规划</title>
      <link>https://arxiv.org/abs/2406.07920</link>
      <description><![CDATA[arXiv:2406.07920v1 公告类型：交叉 
摘要：我们研究学习潜在马尔可夫决策过程 (LMDP) 的计算和统计方面。在这个模型中，学习者与在每个时期开始时从未知的 MDP 混合中抽取的 MDP 进行交互。为了避开已知的不可能结果，我们考虑了几种分离组成 MDP 的概念。本文的主要目的是为有效学习所需的视界长度建立一个近乎尖锐的 *统计阈值*。在计算方面，我们表明，在最优策略下较弱的可分离性假设下，存在一个准多项式算法，其时间复杂度根据统计阈值缩放。我们进一步展示了指数时间假设下近似匹配的时间复杂度下限。]]></description>
      <guid>https://arxiv.org/abs/2406.07920</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:51 GMT</pubDate>
    </item>
    <item>
      <title>诊断并解决分子设计贝叶斯优化中的常见问题</title>
      <link>https://arxiv.org/abs/2406.07709</link>
      <description><![CDATA[arXiv:2406.07709v1 公告类型：交叉 
摘要：贝叶斯优化 (BO) 是分子设计任务的一种原则性方法。在本文中，我们解释了 BO 的三个缺陷，这些缺陷可能导致经验性能不佳：先验宽度不正确、过度平滑和采集函数最大化不足。我们表明，解决这些问题后，即使是基本的 BO 设置也能够在分子设计的 PMO 基准上实现最高的整体性能（Gao 等人，2022 年）。这些结果表明，BO 可能会在分子机器学习社区中得到更多关注。]]></description>
      <guid>https://arxiv.org/abs/2406.07709</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:50 GMT</pubDate>
    </item>
    <item>
      <title>具有不变子空间的多层网络的偏差校正联合谱嵌入：逐项特征向量扰动和推理</title>
      <link>https://arxiv.org/abs/2406.07849</link>
      <description><![CDATA[arXiv:2406.07849v1 公告类型：交叉 
摘要：在本文中，我们提出使用一种新的偏差校正联合谱嵌入算法来估计跨异构多网络的不变子空间。所提出的算法利用闭式偏差公式递归地校准网络邻接矩阵平方和的对角线偏差，并使用最新估计的偏差迭代更新子空间估计器。相应地，我们为所提出的算法建立了完整的逐项子空间估计理论，包括尖锐的逐项子空间扰动界限和逐项特征向量中心极限定理。利用这些结果，我们解决了两个多网络推理问题：多层随机块模型中的精确社区检测和多层混合成员模型中成员配置文件相等性的假设检验。我们的证明依赖于精细的留一法和留二法分析，这些分析专门针对块对称随机矩阵，以及对逐项特征向量中心极限定理具有根本意义的鞅论证。]]></description>
      <guid>https://arxiv.org/abs/2406.07849</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:50 GMT</pubDate>
    </item>
    <item>
      <title>基于消融的反事实</title>
      <link>https://arxiv.org/abs/2406.07908</link>
      <description><![CDATA[arXiv:2406.07908v1 公告类型：交叉 
摘要：扩散模型是一类生成高质量样本的生成模型，但目前很难描述它们如何依赖于训练数据。这种困难引发了科学和监管问题，也是扩散模型及其采样过程复杂性的结果。为了分析这种依赖性，我们引入了基于消融的反事实 (ABC)，这是一种依赖于模型消融而不是模型再训练的反事实分析方法。在我们的方法中，我们在训练集的不同但重叠的分割上训练模型的独立组件。然后将这些组件组合成一个模型，通过消融模型组件的组合，可以从中消除任何训练样本的因果影响。我们演示了如何使用一组扩散模型构建这样的模型。然后，我们使用该模型通过枚举完整的反事实景观来研究训练数据归因的局限性，并表明单一来源归因性随着训练数据大小的增加而降低。最后，我们证明了不可归因样本的存在。]]></description>
      <guid>https://arxiv.org/abs/2406.07908</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:50 GMT</pubDate>
    </item>
    <item>
      <title>Nystr\"om 内核 Stein 差异</title>
      <link>https://arxiv.org/abs/2406.08401</link>
      <description><![CDATA[arXiv:2406.08401v1 公告类型：新
摘要：核方法是数据科学和统计学中许多最成功方法的基础，它们允许将概率测度表示为再生核希尔伯特空间的元素而不会丢失信息。最近，将 Stein 方法与核技术相结合的核 Stein 差异 (KSD) 引起了广泛关注。通过 Stein 运算符，KSD 允许构建强大的拟合优度检验，其中知道目标分布最多一个乘法常数就足够了。然而，典型的基于 U 和 V 统计量的 KSD 估计器存在二次运行时复杂性，这阻碍了它们在大规模环境中的应用。在本研究中，我们提出了一种基于 Nystr\&quot;om 的 KSD 加速方法，对于 $n$ 个样本和 $m\ll n$ 个 Nystr\&quot;om 点，运行时间为 $\mathcal O\!\left(mn+m^3\right)$，并在经典亚高斯假设下证明了其在零假设下的 $\sqrt{n}$-一致性，并证明了其在一系列基准测试中的拟合优度检验的适用性。]]></description>
      <guid>https://arxiv.org/abs/2406.08401</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:49 GMT</pubDate>
    </item>
    <item>
      <title>MambaLRP：解释选择性状态空间序列模型</title>
      <link>https://arxiv.org/abs/2406.07592</link>
      <description><![CDATA[arXiv:2406.07592v1 公告类型：交叉 
摘要：最近使用选择性状态空间序列模型（称为 Mamba 模型）的序列建模方法引起了人们的极大兴趣。这些模型可以在线性时间内高效处理长序列，并迅速被广泛采用，例如语言建模，表现出良好的性能。为了促进它们在现实世界场景中的可靠使用，增强它们的透明度至关重要。我们的工作通过将可解释性（尤其是分层相关性传播 (LRP)）引入 Mamba 架构来弥补这一关键差距。在相关性守恒公理的指导下，我们确定了 Mamba 架构中导致不忠实解释的特定组件。为了解决这个问题，我们提出了 MambaLRP，这是 LRP 框架内的一种新算法，可确保通过这些组件进行更稳定、更可靠的相关性传播。我们提出的方法在理论上是合理的，并且在各种模型和数据集中实现最先进的解释性能方面表现出色。此外，MambaLRP 有助于更深入地检查 Mamba 架构，发现各种偏差并评估其重要性。它还可以分析有关 Mamba 模型的长期能力的先前推测。]]></description>
      <guid>https://arxiv.org/abs/2406.07592</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:49 GMT</pubDate>
    </item>
    <item>
      <title>Treeffuser：通过梯度提升树的条件扩散进行概率预测</title>
      <link>https://arxiv.org/abs/2406.07658</link>
      <description><![CDATA[arXiv:2406.07658v1 公告类型：交叉 
摘要：概率预测旨在计算预测分布而不是单点预测。这些分布使从业者能够量化不确定性、计算风险和检测异常值。然而，大多数概率方法都假设参数响应，例如高斯或泊松分布。当这些假设失败时，这些模型会导致错误的预测和校准不佳的不确定性。在本文中，我们提出了 Treeffuser，这是一种易于使用的表格数据概率预测方法。这个想法是学习一个条件扩散模型，其中使用梯度提升树估计得分函数。条件扩散模型使 Treeffuser 灵活且非参数化，而梯度提升树使其稳健且易于在 CPU 上训练。Treeffuser 学习校准良好的预测分布，可以处理各种回归任务——包括具有多变量、多模态和倾斜响应的任务。 % ，以及分类预测因子和缺失数据我们在合成数据和真实数据上研究 Treeffuser，并表明它优于现有方法，提供更精确的概率预测。我们进一步展示了其多功能性，并使用沃尔玛的销售数据将其应用于不确定情况下的库存分配。我们在 \href{https://github.com/blei-lab/treeffuser}{https://github.com/blei-lab/treeffuser} 中实现了 Treeffuser。]]></description>
      <guid>https://arxiv.org/abs/2406.07658</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:49 GMT</pubDate>
    </item>
    <item>
      <title>从强混合观测中进行深度学习：稀疏惩罚正则化和极小极大最优</title>
      <link>https://arxiv.org/abs/2406.08321</link>
      <description><![CDATA[arXiv:2406.08321v1 公告类型：新
摘要：最近，来自独立数据的深度神经网络估计量的显式正则化和最优性取得了长足的进步。在相关数据上研究此类特性仍然是一个挑战。在本文中，我们从强混合观测中进行深度学习，并处理平方和一大类损失函数。我们考虑深度神经网络预测器的稀疏惩罚正则化。对于包括回归估计、分类、时间序列预测、$\cdots$ 的一般框架，建立了预期超额风险的 oracle 不等式，并提供了 H\&quot;older 平滑函数类的界限。对于来自强混合数据和亚指数误差的非参数回归，我们为 $L_2$ 误差提供了一个 oracle 不等式，并研究了该误差在一类 H\&quot;older 组合函数上的上限。对于具有高斯和拉普拉斯误差的非参数自回归的特定情况，建立了此 H\&quot;older 组合类的 $L_2$ 误差的下界。直到对数因子，此界限与其上限相匹配；因此，深度神经网络估计器达到极小极大最优速率。]]></description>
      <guid>https://arxiv.org/abs/2406.08321</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:48 GMT</pubDate>
    </item>
    <item>
      <title>可微分成本参数化 Monge 映射估计器</title>
      <link>https://arxiv.org/abs/2406.08399</link>
      <description><![CDATA[arXiv:2406.08399v1 公告类型：新
摘要：在最优传输 (OT) 领域，地面成本的选择对于确保传输图的最优性与实际应用中的实用性相对应至关重要。因此，最好使用已知信息来定制成本函数，从而学习适合当前问题的 OT 图。通过考虑一类具有已知形式的 Monge 图的神经地面成本，我们构建了一个可微分的 Monge 图估计器，该估计器可以优化为与有关 OT 图的已知信息一致。在此过程中，我们同时学习 OT 图估计器和相应的适应成本函数。通过适当选择损失函数，我们的方法提供了一种在学习适应的 OT 图和成本函数时结合有关 Monge 图本身的先验信息的通用方法。]]></description>
      <guid>https://arxiv.org/abs/2406.08399</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:48 GMT</pubDate>
    </item>
    <item>
      <title>最低通信成本统计学习</title>
      <link>https://arxiv.org/abs/2406.08193</link>
      <description><![CDATA[arXiv:2406.08193v1 公告类型：新
摘要：可以访问 $n$ 个训练数据样本的客户端设备需要获得统计假设或模型 $W$，然后将其发送到远程服务器。客户端和服务器设备共享一些共同的随机序列以及假设空间的先验。在这个问题中，合适的假设或模型 $W$ 应该同时满足两个不同的设计标准：(i) 推理阶段的风险（总体）较小，(ii) 将其传送到服务器的“复杂性”较小，通信成本最低。在本文中，我们提出了一种具有可证明的期望保证的联合训练和源编码方案，其中期望值在编码器的输出消息之上。具体来说，我们表明，通过对给定 $W$ 的压缩学习模型 $\widehat{W}$ 引起的条件分布与先验之间的适当 Kullback-Leibler 散度施加约束，可以同时保证较小的平均经验风险（又称训练损失）、较小的平均泛化误差和较小的平均通信成本。我们还考虑了一种一次性场景，其中为每个编码器的输出消息获得经验风险和泛化误差的保证。]]></description>
      <guid>https://arxiv.org/abs/2406.08193</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:47 GMT</pubDate>
    </item>
    <item>
      <title>Wasserstein 梯度流的前向欧拉时间离散化可能是错误的</title>
      <link>https://arxiv.org/abs/2406.08209</link>
      <description><![CDATA[arXiv:2406.08209v1 公告类型：新
摘要：在本说明中，我们研究了用于模拟 Wasserstein 梯度流的前向欧拉离散化。我们提供了两个反例，展示了这种离散化的失败，即使在能量函数被定义为针对某些结构良好的概率密度的 KL 散度的简单情况下也是如此。还讨论了这种失败的简单解释。]]></description>
      <guid>https://arxiv.org/abs/2406.08209</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:47 GMT</pubDate>
    </item>
    <item>
      <title>使用稳健非参数测试测量模型变异性</title>
      <link>https://arxiv.org/abs/2406.08307</link>
      <description><![CDATA[arXiv:2406.08307v1 公告类型：新
摘要：训练深度神经网络通常涉及随机优化，这意味着每次运行都会产生不同的模型。用于初始化优化过程随机元素的种子严重影响训练模型的质量，这可能与许多常见的汇总统计数据（如准确性）不符。然而，随机种子通常不包含在超参数优化中，可能是因为种子和模型质量之间的关系很难描述。这项工作试图描述用不同随机种子训练的深度网络模型与预期模型行为之间的关系。我们采用稳健的假设检验来提出一种新的网络相似性汇总统计数据，称为 $\alpha$ 修剪水平。我们使用 $\alpha$ 修剪水平来表明，随着集合中的模型数量增加，由具有不同随机种子的训练模型集合创建的集成模型的经验累积分布函数近似于这些函数的平均值。这一见解为应抽取多少个随机种子以确保这些经过训练的模型集合具有可靠的代表性提供了指导。我们还表明，$\alpha$ 修剪水平比单独使用不同的性能指标（如验证准确率、流失率或预期校准误差）更具表现力，并且可能以更有原则的方式帮助随机种子选择。我们在实际实验中证明了所提出的统计数据的价值，并通过迁移学习中的实验说明了微调相对于随机种子的优势。]]></description>
      <guid>https://arxiv.org/abs/2406.08307</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:47 GMT</pubDate>
    </item>
    <item>
      <title>保持速率减少以实现 Blackwell 可达性</title>
      <link>https://arxiv.org/abs/2406.07585</link>
      <description><![CDATA[arXiv:2406.07585v1 公告类型：新
摘要：Abernethy 等人 (2011) 表明 Blackwell 可接近性和无遗憾学习是等价的，即任何解决特定 Blackwell 可接近性实例的算法都可以转换为特定无遗憾学习实例的亚线性遗憾算法，反之亦然。在本文中，我们研究了这种简化的更细粒度形式，并询问何时问题之间的这种转换不仅保持了亚线性收敛速度，而且还保持了最佳收敛速度。也就是说，在哪些情况下，只需找到无遗憾学习实例的最佳遗憾界限，即可找到相应可接近性实例的最佳收敛速度？
我们表明 Abernethy 等人的简化。 (2011) 不保留速率：它们的减少可能会将具有最佳收敛速率 $R_1$ 的 $d$ 维可接近性实例 $I_1$ 减少为具有最佳每轮遗憾数 $R_2$ 的无遗憾学习实例 $I_2$，其中 $R_{2}/R_{1}$ 任意大（特别是，有可能 $R_1 = 0$ 和 $R_{2} &gt; 0$）。另一方面，我们表明可以将任何可接近性实例紧密减少为广义遗憾最小化形式的实例，我们称之为不当 $\phi$-遗憾最小化（Gordon 等人 (2008) 的 $\phi$-遗憾最小化的变体，其中变换函数可能会将动作映射到动作集之外）。
最后，我们描述了何时线性变换足以将不当 $\phi$-遗憾最小化问题以速率保持的方式减少为标准类的遗憾最小化问题。我们证明，一些不适当的 $\phi$-regret 最小化实例不能以这种方式简化为实例的任一子类，这表明可接近性可以捕捉一些无法用在线学习语言表达的问题。]]></description>
      <guid>https://arxiv.org/abs/2406.07585</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:46 GMT</pubDate>
    </item>
    <item>
      <title>线性二次系统控制的完全自适应遗憾保证算法</title>
      <link>https://arxiv.org/abs/2406.07746</link>
      <description><![CDATA[arXiv:2406.07746v1 公告类型：新 
摘要：Abbasi-Yadkori 和 Szepesv\&#39;ari (2011) 提出了第一个针对具有未知系统模型的线性二次 (LQ) 控制问题的算法，其遗憾值为 $\mathcal{O}(\sqrt{T})$。认识到该算法的计算复杂性，后续努力（参见 Cohen 等人 (2019)、Mania 等人 (2019)、Faradonbeh 等人 (2020a) 和 Kargin 等人 (2022)）致力于提出在保留这种遗憾顺序的同时易于计算的算法。尽管成功了，但文献中现有的研究缺乏完全自适应的探索-利用权衡调整，并且需要用户定义的值，这可能导致某些因素导致整体遗憾界限增长。在本研究中，我们注意到了这一差距，提出了第一个完全自适应算法，该算法可以控制策略更新次数（即调整探索-利用权衡）并自适应地优化遗憾的上限。我们提出的算法以 Cohen 等人（2019 年）基于 SDP 的方法为基础，并通过适当调整正则化参数和添加自适应输入扰动来放宽对视界相关预热阶段的需求。我们进一步表明，通过仔细调整探索-利用权衡，无需遵循广泛使用的强序贯稳定性概念，因为强序贯稳定性具有限制性，并且可能在初始化中引入复杂性。]]></description>
      <guid>https://arxiv.org/abs/2406.07746</guid>
      <pubDate>Fri, 14 Jun 2024 03:15:46 GMT</pubDate>
    </item>
    </channel>
</rss>