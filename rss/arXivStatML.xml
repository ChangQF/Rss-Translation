<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 21 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>利用自适应收集的数据进行离线策略估计：在线学习的力量</title>
      <link>https://arxiv.org/abs/2411.12786</link>
      <description><![CDATA[arXiv:2411.12786v1 公告类型：新
摘要：我们考虑使用自适应收集的数据估计治疗效果的线性函数。这项任务有多种应用，包括情境老虎机中的离策略评估（\textsf{OPE}）和因果推理中的平均治疗效果估计（\textsf{ATE}）。虽然某类增强逆倾向加权（\textsf{AIPW}）估计量具有理想的渐近性质，包括半参数效率，但人们对它们使用自适应收集数据的非渐近理论知之甚少。为了填补这一空白，我们首先建立 AIPW 估计量类均方误差的通用上限，这主要取决于治疗效果与其估计值之间的连续加权误差。受此启发，我们还提出了一种通用的简化方案，允许人们通过在线学习生成一系列治疗效果估计值，以最小化连续加权估计误差。为了说明这一点，我们提供了三个具体实例，分别是（\romannumeral 1）表格情况；（\romannumeral 2）线性函数逼近的情况；（\romannumeral 3）结果模型的一般函数逼近的情况。然后，我们提供了一个局部极小极大下限，以显示使用无遗憾在线学习算法的 \textsf{AIPW} 估计器的实例相关最优性。]]></description>
      <guid>https://arxiv.org/abs/2411.12786</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种新的输入凸神经网络及其在期权定价中的应用</title>
      <link>https://arxiv.org/abs/2411.12854</link>
      <description><![CDATA[arXiv:2411.12854v1 公告类型：新
摘要：我们引入了一类新的神经网络，这些神经网络被设计为其输入的凸函数，利用任何凸函数都可以表示为其主导的仿射函数的上确界这一原理。这些神经网络相对于其输入本质上是凸的，特别适合近似具有凸收益的期权的价格。我们详细介绍了它的架构，并建立了验证其近似能力的理论收敛界限。我们还引入了一个 \emph{scrambling} 阶段来改进这些网络的训练。最后，我们用数字证明了这些网络在估计三种具有凸收益的期权价格方面的有效性：篮子期权、百慕大期权和波动期权。]]></description>
      <guid>https://arxiv.org/abs/2411.12854</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>局部反集中类：贪婪线性上下文老虎机的对数遗憾</title>
      <link>https://arxiv.org/abs/2411.12878</link>
      <description><![CDATA[arXiv:2411.12878v1 公告类型：新
摘要：我们研究了针对线性上下文强盗问题的无探索贪婪算法的性能保证。我们引入了一种新条件，称为 \textit{局部反集中} (LAC) 条件，它使贪婪强盗算法能够实现可证明的效率。我们表明，LAC 条件由广泛的分布类满足，包括高斯、指数、均匀、柯西和学生~$t$ 分布，以及其他指数族分布及其截断变体。这大大扩展了贪婪算法可以有效执行的分布类。在我们提出的 LAC 条件下，我们证明线性上下文强盗问题的贪婪算法的累积预期遗憾由 $O(\operatorname{poly} \log T)$ 界定。我们的结果建立了迄今为止已知的最广泛的分布范围，允许贪婪算法的亚线性遗憾界限，进一步实现了尖锐的多对数遗憾。]]></description>
      <guid>https://arxiv.org/abs/2411.12878</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论双边最近邻的适应性和极小最大最优性</title>
      <link>https://arxiv.org/abs/2411.12965</link>
      <description><![CDATA[arXiv:2411.12965v1 公告类型：新
摘要：最近邻 (NN) 算法已广泛用于推荐系统和顺序决策系统中的缺失数据问题。当基础数据足够平滑且缺失概率有下限时，先前的理论分析已经为 NN 建立了有利的保证。在这里，我们分析了具有大量缺失的非平滑非线性函数的 NN。具体来说，我们考虑矩阵完成设置，其中基础矩阵的条目遵循潜在非线性因子模型，非线性属于比 Lipschitz 平滑度更低的 \Holder 函数类。我们的结果为合适的双面 NN 建立了以下有利属性：(1) NN 的均方误差 (MSE) 适应非线性的平滑度，(2) 在某些规律性条件下，NN 错误率与配备行和列潜在​​因子知识的 oracle 获得的速率相匹配，最后 (3) 即使矩阵中的几个条目可能确定性缺失，NN 的 MSE 在各种设置中都是不平凡的。我们通过大量数值模拟和来自移动健康研究 HeartSteps 的数据案例研究来支持我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2411.12965</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>消除基于梯度的模拟参数估计的比率偏差</title>
      <link>https://arxiv.org/abs/2411.12995</link>
      <description><![CDATA[arXiv:2411.12995v1 公告类型：新
摘要：本文解决了在似然函数无法解析的随机模型中参数校准的挑战。我们提出了一种基于梯度的模拟参数估计框架，利用多时间尺度算法解决了最大似然估计和后验密度估计问题中的比率偏差问题。此外，我们引入了一个嵌套模拟优化结构，为所提算法提供了包括强收敛性、渐近正态性、收敛速度和预算分配策略在内的理论分析。该框架进一步扩展到神经网络训练，为机器学习中的随机近似提供了一个新的视角。数值实验表明，我们的算法可以提高估计精度并节省计算成本。]]></description>
      <guid>https://arxiv.org/abs/2411.12995</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带有 Shapley 值估计的策略增强图混合模型的敏感性分析</title>
      <link>https://arxiv.org/abs/2411.13396</link>
      <description><![CDATA[arXiv:2411.13396v1 公告类型：新
摘要：受生物制造中高复杂性和高不确定性等关键挑战的推动，我们提出了一个全面且计算效率高的敏感性分析框架，用于一般非线性策略增强知识图 (pKG) 混合模型，该模型表征了基于风险和科学的对潜在随机决策过程机制的理解。通过将 Shapley 值 (SV) 敏感性分析应用于 pKG（称为 SV-pKG）来测量每个输入（即随机因素、策略参数和模型参数）的关键性，从而考虑过程因果相互依赖性。为了快速评估大量仪器生物过程的 SV，我们使用线性高斯 pKG 模型近似它们的动态，并利用线性高斯特性提高 SV 估计效率。此外，我们提出了一种有效的置换采样方法，该方法结合了 TFWW 变换和方差减少技术，即准蒙特卡洛和对偶采样方法，以进一步提高一般非线性和线性高斯 pKG 模型的采样效率和 SV 估计精度。我们提出的框架有利于高效解释并支持生物制造中稳定的最佳过程控制。]]></description>
      <guid>https://arxiv.org/abs/2411.13396</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分层数据的保形预测</title>
      <link>https://arxiv.org/abs/2411.13479</link>
      <description><![CDATA[arXiv:2411.13479v1 公告类型：新
摘要：协调已成为分层时间序列多变量点预测中的重要工具。然而，人们对概率预测协调技术的理论特性仍然缺乏了解。同时，保形预测是一个越来越受欢迎的通用框架，它为有限样本中的预测集提供了概率保证。在本文中，我们通过分析在分割保形预测 (SCP) 程序中包含协调步骤如何增强生成的预测集，提出了将保形预测和预测协调结合起来的第一步。特别是，我们表明 SCP 授予的有效性在提高预测集效率的同时仍然存在。我们还提倡对理论程序进行改进以供实际使用。最后，我们通过模拟说明了这些结果。]]></description>
      <guid>https://arxiv.org/abs/2411.13479</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分数阶微分特征和三重屏障标记的监督自动编码器可增强对噪声数据的预测</title>
      <link>https://arxiv.org/abs/2411.12753</link>
      <description><![CDATA[arXiv:2411.12753v1 公告类型：交叉 
摘要：本文研究了通过监督自动编码器 (SAE) 使用神经网络来增强金融时间序列预测，以提高投资策略绩效。使用夏普比率和信息比率，它特别检查了噪声增强和三重障碍标记对风险调整回报的影响。该研究重点关注比特币、莱特币和以太坊作为 2016 年 1 月 1 日至 2022 年 4 月 30 日的交易资产。研究结果表明，具有平衡噪声增强和瓶颈大小的监督自动编码器可显着提高策略有效性。然而，过多的噪音和较大的瓶颈尺寸会损害性能。]]></description>
      <guid>https://arxiv.org/abs/2411.12753</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有序反馈的奖励建模：群体智慧</title>
      <link>https://arxiv.org/abs/2411.12843</link>
      <description><![CDATA[arXiv:2411.12843v1 公告类型：交叉 
摘要：从人类偏好中学习奖励模型 (RM) 一直是对齐大型语言模型 (LLM) 的重要组成部分。从成对偏好数据中学习 RM 的规范设置植根于经典的 Bradley-Terry (BT) 模型，该模型接受二元反馈，即标签要么是响应 1 优于响应 2，要么是相反的。这样的设置不可避免地会丢弃潜在的有用样本（例如两个响应之间“并列”）并丢失更多细粒度信息（例如“略好”）。在本文中，我们提出了一个在序数反馈下学习 RM 的框架，该框架将二元偏好反馈的情况推广到任意粒度。具体而言，我们首先确定一个边际无偏条件，该条件将现有二元反馈设置中的 BT 模型假设推广。该条件通过群体智慧的社会学概念得到验证。在此条件下，我们为序数反馈下的成对偏好数据开发了一个自然概率模型并分析了其性质。与二元反馈的情况相比，我们证明了序数反馈在降低 Rademacher 复杂度方面的统计优势。提出的学习目标和理论还扩展到铰链损失和直接策略优化 (DPO)。特别是，当应用于看似不相关的知识提炼问题以解释其中的偏差-方差权衡时，理论分析可能具有独立的兴趣。该框架还为人类注释者的写作指导提供了启示。我们的数值实验验证了细粒度反馈可以为分布内和分布外设置带来更好的奖励学习。进一步的实验表明，结合一定比例的具有绑定偏好的样本可以促进 RM 学习。]]></description>
      <guid>https://arxiv.org/abs/2411.12843</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NPGPT：使用基于 GPT 的化学语言模型生成类似天然产物的化合物</title>
      <link>https://arxiv.org/abs/2411.12886</link>
      <description><![CDATA[arXiv:2411.12886v1 公告类型：交叉 
摘要：天然产物是自然界中生物产生的物质，往往具有生物活性和结构多样性。基于天然产物的药物开发多年来一直很普遍。然而，这些化合物的复杂结构在结构测定和合成方面带来了挑战，特别是与合成化合物的高通量筛选效率相比。近年来，基于深度学习的方法已被应用于分子的生成。在本研究中，我们在天然产物数据集上训练了化学语言模型并生成了类似天然产物的化合物。结果表明，生成的化合物的分布与天然产物相似。我们还评估了生成的化合物作为候选药物的有效性。我们的方法可用于探索广阔的化学空间，并减少天然产物药物发现的时间和成本。]]></description>
      <guid>https://arxiv.org/abs/2411.12886</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>损失对损失预测：所有数据集的缩放定律</title>
      <link>https://arxiv.org/abs/2411.12925</link>
      <description><![CDATA[arXiv:2411.12925v1 公告类型：交叉 
摘要：虽然缩放定律为预测单个数据分布的跨计算规模的训练损失提供了一种可靠的方法，但对于这些预测应该如何随着分布的变化而变化，人们知之甚少。在本文中，我们推导出一种从一种损失预测另一种损失的策略，并将其应用于跨不同的预训练数据集以及从预训练数据到下游任务数据的预测。即使在用于拟合曲线的最大 FLOP 预算的 20 倍下，我们的预测也能很好地推断出来。更准确地说，我们发现 (1) 当模型通过训练计算配对（训练到训练）时，在两个单独的数据集上训练的两个模型的训练损失、(2) 单个模型在任何下游分布上的训练损失和测试损失（训练到测试）以及 (3) 在两个单独的训练数据集上训练的两个模型的测试损失（测试到测试）之间存在简单的移位幂律关系。结果适用于差异很大的预训练数据集（有些完全是代码，有些则根本没有代码）和各种下游任务。最后，我们发现在某些情况下，这些移位幂律关系可以产生比推断单数据集缩放定律更准确的预测。]]></description>
      <guid>https://arxiv.org/abs/2411.12925</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于传导学习的图变换器可压缩性理论</title>
      <link>https://arxiv.org/abs/2411.13028</link>
      <description><![CDATA[arXiv:2411.13028v1 公告类型：交叉 
摘要：图上的传导任务与典型的监督机器学习任务有着根本的不同，因为独立同分布 (i.i.d.) 假设在样本之间不成立。相反，所有训练/测试/验证样本都在训练期间存在，这使得它们更类似于半监督任务。这些差异使得模型的分析与其他模型有很大不同。最近，Graph Transformers 通过克服长距离依赖问题显着改善了这些数据集的结果。然而，完整 Transformers 的二次复杂性促使社区探索更高效的变体，例如具有更稀疏注意力模式的变体。虽然注意力矩阵已经被广泛讨论，但网络的隐藏维度或宽度却受到较少的关注。在这项工作中，我们建立了一些理论界限，说明如何以及在什么条件下可以压缩这些网络的隐藏维度。我们的结果适用于 Graph Transformers 的稀疏和密集变体。]]></description>
      <guid>https://arxiv.org/abs/2411.13028</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大概是精确度和回忆率的学习</title>
      <link>https://arxiv.org/abs/2411.13029</link>
      <description><![CDATA[arXiv:2411.13029v1 公告类型：交叉
摘要：准确率和召回率是机器学习的基础指标，其中准确的预测和全面的覆盖都是必不可少的，例如在推荐系统和多标签学习中。在这些任务中，平衡准确率（预测项目占相关项目的比例）和召回率（成功预测的相关项目的比例）至关重要。一个关键的挑战是，单边反馈——在训练期间只观察到正面的例子——是许多实际问题所固有的。例如，在 YouTube 等推荐系统中，训练数据仅包含用户主动选择的视频，而未选择的项目仍未看到。尽管在训练中缺乏负面反馈，但在测试时避免不良推荐至关重要。
我们引入了一个 PAC 学习框架，其中每个假设都由一个图表示，边表示正向交互，例如用户和项目之间的交互。该框架涵盖了经典的二分类和多分类 PAC 学习模型以及具有部分反馈的多标签学习，其中每个示例仅观察到单个随机正确标签，而不是所有正确标签。
我们的工作揭示了丰富的统计和算法领域，对可以学习和不能学习的内容有细微的界限。值得注意的是，像经验风险最小化这样的经典方法在这种情况下会失败，即使对于只有两个假设的简单假设类也是如此。为了应对这些挑战，我们开发了全新的算法，这些算法专门从正数据中学习，有效地将精度和召回率损失降至最低。具体而言，在可实现的设置中，我们设计了实现最佳样本复杂度保证的算法。在不可知的情况下，我们表明不可能实现加性误差保证（这是 PAC 学习中的标准），而是获得有意义的乘性近似值。]]></description>
      <guid>https://arxiv.org/abs/2411.13029</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用多指标模型全面预测单指标模型</title>
      <link>https://arxiv.org/abs/2411.13083</link>
      <description><![CDATA[arXiv:2411.13083v1 公告类型：交叉 
摘要：最近关于监督学习的研究 [GKR+22] 定义了全能预测器的概念，即预测函数 $p$ 针对同时具有最小化损失函数系列 $\mathcal{L}$ 与比较器类 $\mathcal{C}$ 竞争力的特征。全能预测需要在损失最小化范式之外近似贝叶斯最优预测器，并引起了学习理论界的极大兴趣。然而，即使对于诸如不可知学习单指标模型 (SIM) 之类的基本设置，现有的全能预测器构造也需要不切实际的大样本复杂性和运行时间，并输出复杂、高度不恰当的假设。
我们的主要贡献是为 SIM 构建一种新的、简单的全能预测器。我们让学习者输出一个全能预测器，当比较器类是有界线性预测器时，该预测器对单调 Lipschitz 链接函数引起的任何匹配损失都具有 $\varepsilon$ 竞争力。我们的算法需要 $\approx \varepsilon^{-4}$ 个样本，运行时间接近线性，如果链接函数是双 Lipschitz，其样本复杂度将提高到 $\approx \varepsilon^{-2}$。这显著改进了之前已知的唯一构造，这是由于 [HJKRR18, GHK+23]，它使用了 $\gtrsim \varepsilon^{-10}$ 个样本。
我们通过对经典 Isotron 算法 [KS09, KKKS11] 在具有挑战性的不可知学习环境中进行新的、尖锐的分析来实现我们的构造，这可能具有独立意义。此前，Isotron 已在可实现的设置中正确学习 SIM，以及平方损失下的常数因子竞争假设 [ZWDD24]。由于它们基于 Isotron，我们的全能预测器是具有 $\approx \varepsilon^{-2}$ 预测头的多指标模型，使我们更接近对一般损失系列和比较器进行正确全能预测的诱人目标。]]></description>
      <guid>https://arxiv.org/abs/2411.13083</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有限加权平均的统一分析</title>
      <link>https://arxiv.org/abs/2411.13169</link>
      <description><![CDATA[arXiv:2411.13169v1 公告类型：交叉 
摘要：随机梯度下降 (SGD) 的平均迭代在训练深度学习模型方面取得了实证成功，例如随机权重平均 (SWA)、指数移动平均 (EMA) 和 LAtest 权重平均 (LAWA)。特别是，使用有限权重平均方法，LAWA 可以获得更快的收敛和更好的泛化。然而，它的理论解释仍然很少被探索，因为有限和无限设置之间存在根本区别。在这项工作中，我们首先将 SGD 和 LAWA 概括为有限权重平均 (FWA)，并从优化和泛化的角度解释它们与 SGD 相比的优势。一个关键的挑战是传统方法在分析 FWA 的收敛性时不适用于无限维设置的期望或最优值。其次，FWA 引入的累积梯度给泛化分析带来了额外的困惑，尤其使得在不同假设下讨论它们变得更加困难。将最终迭代收敛分析扩展到 FWA，本文在凸性假设下建立了收敛界限 $\mathcal{O}(\log\left(\frac{T}{k}\right)/\sqrt{T})$，其中 $k\in[1, T/2]$ 是一个常数，表示最后 $k$ 次迭代。与使用 $\mathcal{O}(\log(T)/\sqrt{T})$ 的 SGD 相比，我们从理论上证明了 FWA 具有更快的收敛速度，并解释了平均点数量的影响。在泛化分析中，我们找到了一种使用数学归纳法来限制累积梯度的递归表示。我们提供了常数和衰减学习率以及凸和非凸情况的界限，以展示 FWA 良好的泛化性能。最后，在多个基准上的实验结果验证了我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2411.13169</guid>
      <pubDate>Thu, 21 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>