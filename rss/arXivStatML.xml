<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 24 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>在专家混合中，Sigmoid 门控比 Softmax 门控的样本效率更高</title>
      <link>https://arxiv.org/abs/2405.13997</link>
      <description><![CDATA[arXiv:2405.13997v1 公告类型：新 
摘要：softmax 门函数可以说是专家混合建模中最流行的选择。尽管其在实践中得到广泛使用，但softmax门控可能会导致专家之间不必要的竞争，由于其固有的结构，可能会导致表示崩溃的不良现象。作为回应，最近提出了 sigmoid 门函数作为替代方案，并已根据经验证明可以实现卓越的性能。然而，当前文献中缺乏对 sigmoid 门函数的严格检查。在本文中，我们从理论上验证了在专家估计的统计任务中，Sigmoid 门控实际上比 Softmax 门控具有更高的样本效率。为了实现这一目标，我们考虑一个回归框架，其中未知回归函数被建模为专家的混合，并研究在过度指定的情况下最小二乘估计器的收敛速度，其中拟合的专家数量大于真正的价值。我们证明了两个门控机制自然出现，并且在每个门控机制中，我们为专家函数制定了可识别性条件并推导了相应的收敛率。在这两种情况下，我们发现，用常用激活（例如 $\mathrm{ReLU}$ 和 $\mathrm{GELU}$）制定为前馈网络的专家在 sigmoid 门控下比 Softmax 门控下具有更快的收敛速度。此外，在专家选择相同的情况下，我们证明了 sigmoid 门函数需要比其 softmax 更小的样本量才能获得相同的专家估计误差，因此样本效率更高。]]></description>
      <guid>https://arxiv.org/abs/2405.13997</guid>
      <pubDate>Fri, 24 May 2024 06:19:29 GMT</pubDate>
    </item>
    <item>
      <title>FLIPHAT：高维稀疏线性强盗的联合差分隐私</title>
      <link>https://arxiv.org/abs/2405.14038</link>
      <description><![CDATA[arXiv:2405.14038v1 公告类型：新 
摘要：高维稀疏线性老虎机是序列决策问题（例如个性化医疗）的有效模型，其中用户的高维特征（例如基因组数据）可用，但只有一小部分是相关的。出于这些应用程序中数据隐私问题的动机，我们研究了联合差分隐私高维稀疏线性老虎机，其中奖励和上下文都被视为私有数据。首先，为了量化隐私成本，我们得出了在这种情况下可实现的遗憾的下限。为了进一步解决这个问题，我们设计了一种计算效率高的老虎机算法， \textbf{F}orgetfu\textbf{L} \textbf{I}terative \textbf{P}rivate \textbf{HA}rd \textbf{T}hresholding (翻转）。随着情节和情节遗忘的加倍，FLIPHAT 部署了噪声迭代硬阈值 (N-IHT) 算法的变体作为稀疏线性回归预言机，以确保隐私和遗憾最优性。我们证明 FLIPHAT 在对数因子范围内实现了最佳后悔。我们通过对 N-IHT 估计误差进行新颖的精细分析来分析遗憾，这也是人们感兴趣的。]]></description>
      <guid>https://arxiv.org/abs/2405.14038</guid>
      <pubDate>Fri, 24 May 2024 06:19:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Wasserstein-proximal-regularized $\alpha$-divergences 学习重尾分布</title>
      <link>https://arxiv.org/abs/2405.13962</link>
      <description><![CDATA[arXiv:2405.13962v1 公告类型：新 
摘要：在本文中，我们提出 $\alpha$-散度的 Wasserstein 近端作为以稳定方式学习重尾分布的合适目标函数。首先，我们提供了数据维度、$\alpha$ 和数据分布的衰减率之间的充分关系（在某些情况下是必要的），以使 Wasserstein 近端正则化散度有限。然后在某些尾部条件下提供 Wasserstein-1 近端发散情况下估计的有限样本收敛率。数值实验证明，使用合适的生成模型（例如 GAN 和与我们提出的 Wasserstein-proximal-相关的基于流的模型），可以稳定地学习重尾分布（即使是那些没有第一或第二矩的分布），而无需对尾部行为有任何明确的了解。正则化$\alpha$-散度。启发式地，$\alpha$-散度处理重尾部，而 Wasserstein 近端允许分布之间的非绝对连续性，并在基于流的算法学习尾部深处的目标分布时控制它们的速度。]]></description>
      <guid>https://arxiv.org/abs/2405.13962</guid>
      <pubDate>Fri, 24 May 2024 06:19:28 GMT</pubDate>
    </item>
    <item>
      <title>具有隐藏对称性的对称线性老虎机</title>
      <link>https://arxiv.org/abs/2405.13899</link>
      <description><![CDATA[arXiv:2405.13899v1 公告类型：新 
摘要：具有低维结构的高维线性老虎机由于其实际意义而在最近的研究中受到了广泛的关注。文献中最常见的结构是稀疏性。然而，在实践中可能无法实现。对称性，即在臂集上的某些变换组下奖励不变，是高维情况下的另一个重要的归纳偏差，涵盖了许多标准结构，包括稀疏性。在这项工作中，我们研究高维对称线性老虎机，其中对称性对学习者隐藏，并且需要在在线设置中学习正确的对称性。我们检查隐藏对称性集合的结构，并提供一种基于低维子空间集合内的模型选择的方法。我们的算法实现了 $ O(d_0^{1/3} T^{2/3} \log(d))$ 的后悔界限，其中 $d$ 是可能非常大的环境维度，$d_0$是真正的低维子空间的维度，使得 $d_0 \ll d$。通过对分离良好模型的额外假设，我们可以进一步将遗憾提高到 $ O(d_0\sqrt{T\log(d)} )$。]]></description>
      <guid>https://arxiv.org/abs/2405.13899</guid>
      <pubDate>Fri, 24 May 2024 06:19:27 GMT</pubDate>
    </item>
    <item>
      <title>光纤采样问题的演员批评算法</title>
      <link>https://arxiv.org/abs/2405.13950</link>
      <description><![CDATA[arXiv:2405.13950v1 公告类型：新 
摘要：我们针对代数统计和离散优化中出现的一系列复杂问题提出了一种行动批评算法。核心任务是从高维多面体定义的非负整数格的有限子集中生成样本。我们将问题转化为马尔可夫决策过程，并设计了一个演员评论家强化学习（RL）算法来学习一组可用于采样的好动作。我们证明了 actor-critic 算法收敛到近似最优的采样策略。
  为了解决这些采样问题中通常出现的复杂性问题，并让强化学习能够大规模发挥作用，我们的解决策略采取三个步骤：分解样本的起点，对每个引发的子问题使用强化学习，以及重构以获得样本在原来的多面体中。在此设置中，收敛证明适用于分解中的每个子问题。
  我们在两种情况下测试该方法。在统计应用中，高维多胞形作为分类数据统计模型的模型/数据拟合测试中参考分布的支持集而出现。我们演示了如何使用 RL 来解决数据集的模型拟合测试问题，对于这些数据集，传统 MCMC 采样器由于问题大小和稀疏结构而收敛速度过慢。为了测试算法的鲁棒性并探索其泛化特性，我们将其应用于各种大小和稀疏级别的综合生成数据。]]></description>
      <guid>https://arxiv.org/abs/2405.13950</guid>
      <pubDate>Fri, 24 May 2024 06:19:27 GMT</pubDate>
    </item>
    <item>
      <title>通过显式前向-后向桥接调节扩散模型</title>
      <link>https://arxiv.org/abs/2405.13794</link>
      <description><![CDATA[arXiv:2405.13794v1 公告类型：新 
摘要：给定一个无条件扩散模型 $\pi(x, y)$，用它来执行条件模拟 $\pi(x \mid y)$ 在很大程度上仍然是一个悬而未决的问题，通常是通过学习去噪的条件漂移来实现的SDE 事后。在这项工作中，我们将条件模拟表示为对应于部分 SDE 桥的增广空间上的推理问题。这种观点使我们能够实现高效且有原则的粒子吉布斯和伪边际采样器，以边际目标条件分布 $\pi(x \mid y)$ 为目标。与现有方法相反，除了蒙特卡洛误差之外，我们的方法没有对无条件扩散模型引入任何额外的近似。我们通过一系列合成和真实数据示例展示了我们方法的优点和缺点。]]></description>
      <guid>https://arxiv.org/abs/2405.13794</guid>
      <pubDate>Fri, 24 May 2024 06:19:26 GMT</pubDate>
    </item>
    <item>
      <title>回归树了解微积分</title>
      <link>https://arxiv.org/abs/2405.13846</link>
      <description><![CDATA[arXiv:2405.13846v1 公告类型：新 
摘要：回归树因其处理非线性、交互效应和尖锐不连续性的能力而成为解决现实世界回归问题的卓越工具。在本文中，我们宁愿研究应用于行为良好的可微函数的回归树，并确定节点参数与被逼近函数的局部梯度之间的关系。我们找到了一种简单的梯度估计，可以使用流行的树学习库公开的数量来有效地计算该梯度。这使得在可微分算法（如神经网络和高斯过程）背景下开发的工具可以部署到基于树的模型中。为了证明这一点，我们研究了根据梯度积分定义的模型灵敏度的度量，并演示如何使用所提出的梯度估计来计算回归树的模型灵敏度。定量和定性数值实验揭示了回归树估计的梯度改善预测分析、解决不确定性量化任务以及提供模型行为解释的能力。]]></description>
      <guid>https://arxiv.org/abs/2405.13846</guid>
      <pubDate>Fri, 24 May 2024 06:19:26 GMT</pubDate>
    </item>
    <item>
      <title>粗糙信号驱动的随机尖峰神经网络的精确梯度</title>
      <link>https://arxiv.org/abs/2405.13587</link>
      <description><![CDATA[arXiv:2405.13587v1 公告类型：新 
摘要：我们引入了一个基于粗糙路径理论的严格数学框架，将随机尖峰神经网络（SSNN）建模为具有事件不连续性的随机微分方程（事件 SDE），并由 c\`adl\`ag 粗糙路径驱动。我们的形式主义足够通用，允许在解决方案轨迹和驱动噪声中都存在潜在的跳跃。然后，我们确定一组充分条件，确保解轨迹和事件时间相对于网络参数的路径梯度的存在，并展示这些梯度如何满足递归关系。此外，我们引入了一种通用损失函数，该函数通过在 c\`adl\`ag 粗糙路径上索引的一类新签名内核来定义，并使用它来训练 SSNN 作为生成模型。我们为事件 SDE 提供了一个端到端的可微分求解器，并将其实现作为 $\texttt{diffrax}$ 库的一部分提供。据我们所知，我们的框架是第一个支持基于梯度的 SSNN 训练，其中噪声会影响尖峰时序和网络动态。]]></description>
      <guid>https://arxiv.org/abs/2405.13587</guid>
      <pubDate>Fri, 24 May 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>控制、传输和采样：实现更好的损耗设计</title>
      <link>https://arxiv.org/abs/2405.13731</link>
      <description><![CDATA[arXiv:2405.13731v1 公告类型：新 
摘要：利用基于扩散的采样、最优传输和最优随机控制之间的联系（通过它们与 Schr\&quot;odinger 桥问题的共享链接），我们提出了新的目标函数，可用于将 $\nu$ 传输到 $\mu$ ，因此通过最优控制动态从目标 $\mu$ 中进行采样，我们强调了路径视角的重要性以及路径度量上的各种最优条件可以在有效训练损失的设计中发挥的作用，其中的仔细选择提供了。实际实施中的数字优势。]]></description>
      <guid>https://arxiv.org/abs/2405.13731</guid>
      <pubDate>Fri, 24 May 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>用于回归的深度线性网络隐式正则化为平坦最小值</title>
      <link>https://arxiv.org/abs/2405.13456</link>
      <description><![CDATA[arXiv:2405.13456v1 公告类型：新
摘要：神经网络的 Hessian 最大特征值或锐度是理解其优化动态的关键量。在本文中，我们研究了深度线性网络在超定单变量回归中的锐度。最小化器可以具有任意大的锐度，但不能具有任意小的锐度。实际上，我们展示了最小化器锐度的下限，该下限随深度线性增长。然后，我们研究了梯度流发现的最小化器的属性，这是学习率消失的梯度下降的极限。我们展示了一种朝向平坦最小值的隐式正则化：最小化器的锐度不超过常数乘以下限。常数取决于数据协方差矩阵的条件数，但不取决于宽度或深度。这个结果对于小规模初始化和残差初始化都得到了证明。两种情况下都显示了独立感兴趣的结果。对于小规模初始化，我们表明学习到的权重矩阵近似为一阶，并且它们的奇异向量对齐。对于残差初始化，证明了残差网络高斯初始化的梯度流的收敛性。数值实验说明了我们的结果，并将它们与具有非零学习率的梯度下降联系起来。]]></description>
      <guid>https://arxiv.org/abs/2405.13456</guid>
      <pubDate>Fri, 24 May 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>具有公共特征的局部私有估计</title>
      <link>https://arxiv.org/abs/2405.13481</link>
      <description><![CDATA[arXiv:2405.13481v1 公告类型：新 
摘要：我们发起了具有公共特征的局部差分私有（LDP）学习的研究。我们定义半特征LDP，其中一些特征是公开可用的，而其余特征以及标签需要在本地差分隐私下进行保护。在半特征 LDP 下，我们证明非参数回归的最小-最大收敛速度与经典 LDP 相比显着降低。然后我们提出了 HistOfTree，一个充分利用公共和私有特征中包含的信息的估计器。理论上，HistOfTree达到了极小极大最优收敛速度。根据经验，HistOfTree 在合成数据和真实数据上都实现了卓越的性能。我们还探索了用户可以灵活地手动选择保护功能的场景。在这种情况下，我们提出了估计器和数据驱动的参数调整策略，从而得出类似的理论和实证结果。]]></description>
      <guid>https://arxiv.org/abs/2405.13481</guid>
      <pubDate>Fri, 24 May 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>通过分层狄利克雷过程借用分布鲁棒优化的强度</title>
      <link>https://arxiv.org/abs/2405.13160</link>
      <description><![CDATA[arXiv:2405.13160v1 公告类型：新 
摘要：本文提出了一种新颖的优化框架，以解决现代机器学习应用带来的关键挑战：高维、分布不确定性和数据异构性。我们的方法将正则化估计、分布稳健优化 (DRO) 和分层贝叶斯建模统一在单个数据驱动标准中。通过采用分层狄利克雷过程（HDP）先验，该方法有效地处理多源数据，实现正则化、分布鲁棒性，并在不同但相关的数据生成过程中借用力量。我们通过建立理论性能保证和基于狄利克雷过程（DP）理论的易于处理的蒙特卡罗近似来证明该方法的优势。数值实验验证了该框架在提高和稳定预测和参数估计精度方面的功效，展示了其在复杂数据环境中的应用潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.13160</guid>
      <pubDate>Fri, 24 May 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>Olivier-Ricci 曲率下界的加速评估：桥接理论与计算</title>
      <link>https://arxiv.org/abs/2405.13302</link>
      <description><![CDATA[arXiv:2405.13302v1 公告类型：新 
摘要：曲率是一种有效的描述性不变量，其有效性在图论中得到了理论和实践的验证。我们采用奥利维尔提出的广义里奇曲率的定义，林和丘后来将其应用于图论，称为奥利维尔-里奇曲率（ORC）。 ORC 使用 Wasserstein 距离测量曲率，从而将几何概念与概率论和最优传输相结合。 Jost 和 Liu 之前通过显示 Wasserstein 距离的上限讨论了 ORC 的下限。我们将这些界限的适用性扩展到具有整数度量的离散空间，特别是超图。与 Coupette、Dalleiger 和 Rieck 之前在超图中进行的 ORC 工作（面临计算挑战）相比，我们的方法引入了一种具有线性计算复杂度的简化方法，使其特别适合分析大规模网络。通过对合成数据集和现实数据集的广泛模拟和应用，我们展示了我们的方法在评估 ORC 方面所提供的显着改进。]]></description>
      <guid>https://arxiv.org/abs/2405.13302</guid>
      <pubDate>Fri, 24 May 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>具有元变量的异构数据集的图结构距离</title>
      <link>https://arxiv.org/abs/2405.13073</link>
      <description><![CDATA[arXiv:2405.13073v1 公告类型：新
摘要：异构数据集出现在各种机器学习或优化应用中，这些应用具有不同的数据源、各种数据类型和变量之间的复杂关系。在实践中，异构数据集通常被划分为更小的、行为良好的数据集，以便于处理。然而，一些应用涉及生成成本高昂或大小有限的数据集，这激发了基于整个数据集的方法。这项工作的第一个主要贡献是一个建模图结构框架，它概括了最先进的分层、树结构或可变大小框架。该框架对涉及异构数据集的领域进行建模，其中变量可能是连续的、整数的或分类的，如果它们的值决定了包含/排除或影响其他所谓的规定变量的界限，则其中一些变量被标识为元变量。引入排除变量来管理根据给定点包含或排除的变量。第二个主要贡献是图结构距离，它将扩展点与包含和排除变量的任意组合进行比较：可以比较任何一对点，从而可以直接在具有元变量的异构数据集中工作。通过一些回归实验说明了这些贡献，其中多层感知器相对于其超参数的性能是用反距离加权和 $K$-最近邻模型建模的。]]></description>
      <guid>https://arxiv.org/abs/2405.13073</guid>
      <pubDate>Fri, 24 May 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>以非线性观测为条件的高斯测度：一致性、MAP 估计器和模拟</title>
      <link>https://arxiv.org/abs/2405.13149</link>
      <description><![CDATA[arXiv:2405.13149v1 公告类型：新 
摘要：本文系统地研究了在 $F \circ \phi(\xi)$ 形式的非线性观测值上调节高斯随机变量 $\xi$ 的问题，其中 $\phi: \mathcal{X} \to \mathbb{R}^N$ 是有界线性算子，$F$ 是非线性算子。此类问题出现在贝叶斯推理和最近受机器学习启发的 PDE 求解器的背景下。我们给出条件随机变量 $\xi \mid F\circ \phi(\xi)$ 的表示定理，指出它分解为无限维高斯（通过分析识别）以及有限维高斯的和。维非高斯测度。我们还通过考虑问题自然松弛的极限，引入了条件测度模式的新概念，我们可以应用后验测度的最大后验估计量的现有概念。最后，我们引入了拉普拉斯近似的一种变体，用于有效模拟上述条件高斯随机变量以实现不确定性量化。]]></description>
      <guid>https://arxiv.org/abs/2405.13149</guid>
      <pubDate>Fri, 24 May 2024 06:19:22 GMT</pubDate>
    </item>
    </channel>
</rss>