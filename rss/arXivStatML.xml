<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Thu, 14 Dec 2023 03:14:08 GMT</lastBuildDate>
    <item>
      <title>基于最优传输的去噪新视角。 （arXiv：2312.08135v1 [数学.ST]）</title>
      <link>http://arxiv.org/abs/2312.08135</link>
      <description><![CDATA[在去噪问题的标准表述中，给出了
与潜在变量 $\Theta \in \Omega \subset 相关的概率模型
\mathbb{R}^m \; (m\ge 1)$ 和观测值 $Z \in \mathbb{R}^d$ 根据：
$Z \mid \Theta \sim p(\cdot\mid \Theta)$ 和 $\Theta \sim G^*$，目标是
构建一个地图以从观察中恢复潜在变量。这
后验均值是从 $Z$ 估计 $\Theta$ 的自然候选者，达到
最小贝叶斯风险（在平方误差损失下），但代价是
过度缩小 $Z$，并且通常可能无法捕获几何
先验分布 $G^*$ 的特征（例如，低维度，
离散性、稀疏性等）。为了纠正这些缺陷，在本文中我们
受最优启发，对这个去噪问题采取新的视角
传输（OT）理论，并用它提出一种新的基于 OT 的降噪器
人口水平设定。我们严格证明，在一般假设下
在模型上，我们基于 OT 的降噪器定义明确且独特，并且与
连接到 Monge OT 问题的解决方案。然后我们证明，在
对模型进行适当的可识别性假设，我们基于 OT 的降噪器可以
只能从 $Z$ 的边际分布信息中恢复，并且
解决线性松弛问题后模型的后验平均值
合适的联轴器空间，让人想起标准的多边距
OT（MOT）问题。特别是，由于 Tweedie 公式，当
似然模型 $\{ p(\cdot \mid \theta) \}_{\theta \in \Omega}$ 是一个
指数分布族，基于 OT 的降噪器可以恢复
仅来自$Z$的边际分布。总的来说，我们的OT家族
松弛本身和去噪问题都很有趣
受到丰富文献的启发，提出了替代数值方法
计算型 OT。
]]></description>
      <guid>http://arxiv.org/abs/2312.08135</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>主动学习，对标签请求有偏见不回应。 （arXiv：2312.08150v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.08150</link>
      <description><![CDATA[主动学习可以通过以下方式提高训练预测模型的效率
确定要获取的信息最丰富的新标签。然而，没有回应
标记请求会影响现实世界中主动学习的有效性
上下文。我们通过考虑以下类型来概念化这种退化：
数据中存在无答复，表明有偏见的无答复是
对模型性能尤其不利。我们认为，这种
在标签过程中，特别有可能不答复的情况
本质上，依赖于用户交互。减轻偏见的影响
如果没有答复，我们提出了一种基于成本的抽样策略修正——
预期效用的置信上限 (UCB-EU)——这似乎可以，
适用于任何主动学习算法。通过实验，我们
证明我们的方法成功减少了标签带来的危害
在许多情况下没有反应。然而，我们也描述了设置
在 UCB-EU 下，注释中的无反应偏差仍然是有害的
特定的采样方法和数据生成过程。最后我们评价一下
我们的方法基于电子商务平台淘宝的真实数据集。我们展示
UCB-EU 为转换模型带来了显着的性能改进
根据点击次数进行训练。最一般而言，这项研究服务于
更好地概念化不回应类型和不回应类型之间的相互作用
通过主动学习改进模型，并提供实用、易于使用的方法
实施有助于减轻模型退化的修正。
]]></description>
      <guid>http://arxiv.org/abs/2312.08150</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:07 GMT</pubDate>
    </item>
    <item>
      <title>使用不确定数据训练神经网络，混合专家方法。 （arXiv：2312.08083v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.08083</link>
      <description><![CDATA[本文提出了“不确定性感知专家组合”（uMoE），
旨在解决训练中的任意不确定性的新方法
基于神经网络 (NN) 的预测模型。虽然现有方法
主要关注于管理推理过程中的不确定性，uMoE 集成
不确定性直接影响到训练过程。 uMoE 方法采用
“分而治之”范式将不确定的输入空间划分为更多
可管理的子空间。它由专家组件组成，每个组件都经过专门培训
与其子空间相对应的输入不确定性部分。在上面
专家，门控单位，以有关的附加信息为指导
不确定输入在这些子空间中的分布，学习加权
专家们尽量减少与事实真相的偏差。我们的结果突出显示
uMoE 在处理数据方面明显优于基线方法
不确定。此外，我们还进行了稳健性分析，说明了其
适应不同程度的不确定性并提出最佳建议的能力
阈值参数。这种创新方法具有广泛的适用性
不同的数据驱动领域，包括生物医学信号处理，
自动驾驶和生产质量控制。
]]></description>
      <guid>http://arxiv.org/abs/2312.08083</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:06 GMT</pubDate>
    </item>
    <item>
      <title>抽象的因果最优传输。 （arXiv：2312.08107v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.08107</link>
      <description><![CDATA[因果抽象（CA）理论建立了关联的正式标准
不同粒度级别的多个结构因果模型（SCM）
定义它们之间的映射。这些地图具有重要的相关性
现实世界的挑战，例如综合多个因素的因果证据
实验环境，学习因果一致的表示
不同的决议，并将跨多个 SCM 的干预措施联系起来。在这个
工作中，我们提出了 COTA，这是第一个学习抽象图的方法
观察和干预数据，无需假设完全了解
底层的 SCM。特别是，我们引入了多边际最优
强制执行微积分因果约束的传输 (OT) 公式，
以及依赖于干预信息的成本函数。我们
广泛评估 COTA 对合成问题和现实世界问题的影响，并展示
它相对于非因果、独立和聚合的 COTA 公式的优势。
最后，我们展示了我们的方法作为数据增强的效率
通过将其与最先进的 CA 学习框架进行比较，该工具
假设在现实世界的下游任务中完全指定了 SCM。
]]></description>
      <guid>http://arxiv.org/abs/2312.08107</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:06 GMT</pubDate>
    </item>
    <item>
      <title>使用深度内核校准高斯过程的元学习以进行回归不确定性估计。 （arXiv：2312.07952v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.07952</link>
      <description><![CDATA[虽然具有深核的高斯过程（GP）已经成功
用于回归任务中的元学习，其不确定性估计
性能可能很差。我们提出了一种用于校准深度的元学习方法
核 GP 用于提高回归不确定性估计性能
训练数据数量有限。所提出的方法元学习如何
通过最小化测试，使用来自各种任务的数据来校准不确定性
预期的校准误差，并将知识用于看不见的任务。我们设计
我们的模型使得每个任务的适应和校准可以是
无需迭代过程即可执行，从而实现有效的元学习。
特别是，特定于任务的未校准输出分布由
具有任务共享编码器网络的GP，并将其转换为经过校准的
一种使用特定于任务的高斯混合的累积密度函数
模型（GMM）。通过将 GP 和 GMM 集成到我们基于神经网络的模型中，
我们可以以端到端的方式元学习模型参数。我们的实验
证明所提出的方法改进了不确定性估计
性能，同时保持高回归性能与
在少数镜头设置中使用真实世界数据集的现有方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.07952</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>组合随机贪婪老虎机。 （arXiv：2312.08057v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.08057</link>
      <description><![CDATA[我们提出了一种新颖的组合随机贪婪老虎机（SGB）算法
当没有额外信息时的组合多臂老虎机问题
选定的 $n$ 个臂组在每个时间步 $t\in [T]$ 的联合奖励
被观察到。 SGB 采用优化的随机探索然后提交方法
专为具有大量基臂的场景而设计。
与探索整组未选择的基臂的现有方法不同
在每个选择步骤中，我们的 SGB 算法仅采样优化的
未选择的手臂的比例，并从此子集中选择动作。我们证明
我们的算法达到了 $(1-1/e)$-regret 界限
$\mathcal{O}(n^{\frac{1}{3}} k^{\frac{2}{3}} T^{\frac{2}{3}}
\log(T)^{\frac{2}{3}})$ 用于单调随机子模奖励，其中
在基数约束 $k$ 方面优于最先进的技术。
此外，我们根据经验评估了我们的算法的性能
在线限制社会影响力最大化的背景。我们的成果
证明我们提出的方法始终优于其他方法
算法，随着 $k$ 的增长而增加性能差距。
]]></description>
      <guid>http://arxiv.org/abs/2312.08057</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>术语模型：用于密度估计的张量环混合模型。 （arXiv：2312.08075v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.08075</link>
      <description><![CDATA[高效的概率密度估计是统计领域的核心挑战
机器学习。基于张量的概率图方法解决
神经网络中遇到的可解释性和稳定性问题
接近。然而，大量潜在的张量排列可以
导致具有相同结构但不同表达的张量网络
能力。在本文中，我们对密度采用张量环分解
估计器，显着减少排列候选的数量
与现有使用的相比，同时增强了表达能力
分解。此外，混合模型包含多个
进一步设计具有自适应权重的排列候选，从而得到
提高表达的灵活性和综合性。不同于
张量网络结构/排列搜索的流行方向，我们的
该方法提供了受集成学习启发的新观点。这种方法
承认次优排列可以提供独特的信息
除了最优排列之外。实验证明了该方法的优越性
提出的估计中等维度概率密度的方法
数据集和采样以捕获复杂的细节。
]]></description>
      <guid>http://arxiv.org/abs/2312.08075</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:05 GMT</pubDate>
    </item>
    <item>
      <title>具有无冲突信号的稀疏多参考对齐的最小最大最优估计。 （arXiv：2312.07839v1 [数学.ST]）</title>
      <link>http://arxiv.org/abs/2312.07839</link>
      <description><![CDATA[多参考对齐（MRA）问题旨在恢复
在一组潜在作用下重复观察的未知信号
循环等距，存在高强度的加性噪声
$\西格玛$。它是著名的冷冻电镜模型的更易于处理的版本。在
关键的高噪声区域，已知其样本复杂度为
$\sigma^6$。最近的调查表明，对于实际
稀疏信号显着设置，样本复杂度最大
似然估计量随着噪声水平渐近缩放为 $\sigma^4$。
在这项工作中，我们研究了信号估计的极小极大最优性
用于所谓无碰撞信号的 MRA 模型。特别是这个信号
类涵盖了稀稀疏性通用信号的设置（其中
支持大小 $s=O(L^{1/3})$，其中 $L$ 是环境尺寸。

我们证明了稀疏矩阵的极小极大最优估计率
此设置中的 MRA 问题为 $\sigma^2/\sqrt{n}$，其中 $n$ 是样本
尺寸。特别是，这广泛地概括了样本复杂性渐近
对于此设置中的受限 MLE，将其建立为统计
最优估计器。最后，我们证明了集中不等式
限制 MLE 与真实情况的偏差。
]]></description>
      <guid>http://arxiv.org/abs/2312.07839</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:04 GMT</pubDate>
    </item>
    <item>
      <title>迈向最佳统计水印。 （arXiv：2312.07930v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07930</link>
      <description><![CDATA[我们通过将其表述为假设检验来研究统计水印
问题，一个包含所有先前统计数据的总体框架
水印方法。我们公式的关键是输出标记的耦合
和拒绝区域，在实践中由伪随机生成器实现，
允许在 I 类错误和 II 类错误之间进行重要的权衡。
我们在这种情况下描述了统一最强（UMP）水印的特征。在
最常见的情况是输出是一系列 $n$ 令牌，我们
建立独立同分布数量的匹配上限和下限。代币
需要保证较小的 I 类和 II 类错误。我们的费率范围为
$\Theta(h^{-1} \log (1/h))$ 相对于每个令牌 $h$ 的平均熵
从而大大提高了之前作品中的$O(h^{-2})$率。为了
在检测器缺乏模型分布知识的情况下，我们
引入模型无关水印的概念并建立极小极大
II 型错误由此产生的增加的界限。此外，我们还制定了
鲁棒水印问题，其中允许用户执行一类
对生成的文本进行扰动，并表征最佳 II 类错误
通过线性规划问题进行稳健的 UMP 测试。尽我们最大的努力
知识，这是第一次系统的统计处理
i.i.d 中接近最优速率的水印问题设置，并且可能
对未来的作品感兴趣。
]]></description>
      <guid>http://arxiv.org/abs/2312.07930</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:04 GMT</pubDate>
    </item>
    <item>
      <title>特性电路。 （arXiv：2312.07790v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07790</link>
      <description><![CDATA[在许多现实场景中，能够可靠且可靠地
在不确定性下进行有效推理，同时捕获复杂关系
数据。概率电路 (PC)，一个重要的易处理电路家族
概率模型，通过编写简单的、
将易处理的分布转化为高维概率分布。
然而，在异构数据上学习 PC 具有挑战性，并且数据的密度
一些参数分布不以封闭形式提供，限制了它们的
潜在用途。我们介绍特征电路（CC），一个系列
易于处理的概率模型提供了统一的形式化
谱域中异构数据的分布。一对一的
特征函数和概率测度之间的关系使得
我们学习异构数据域上的高维分布
即使没有封闭形式密度，也有助于有效的概率推理
功能可用。我们证明CC的结构和参数可以是
从数据中有效学习并发现 CC 的性能优于最先进的技术
通用基准数据上异构数据域的密度估计器
套。
]]></description>
      <guid>http://arxiv.org/abs/2312.07790</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>高维嵌入向量的估计。 （arXiv：2312.07802v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07802</link>
      <description><![CDATA[嵌入是许多机器中基本的初始特征提取步骤
学习模型，特别是在自然语言处理方面。一个嵌入
尝试将数据标记映射到相似标记所在的低维空间
映射到嵌入中通过某种度量彼此接近的向量
空间。一个基本问题是这种嵌入的学习效果如何？学习
这个问题，我们考虑离散数据的简单概率模型，其中
有一些“真实”但未知的嵌入，其中随机的相关性
变量与嵌入的相似性有关。在这个模型下，
结果表明，嵌入可以通过低秩的变体来学习
近似消息传递（AMP）方法。 AMP 方法可实现精确
对某些高维估计精度的预测
限制。特别是，该方法提供了对关键关系的洞察
参数，例如每个值的样本数、术语的频率、
以及概率分布上嵌入相关性的强度。
我们的理论发现通过对两种合成数据的模拟得到验证
和真实的文本数据。
]]></description>
      <guid>http://arxiv.org/abs/2312.07802</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>综合数据：我们可以相信统计估算器吗？ （arXiv：2312.07837v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07837</link>
      <description><![CDATA[人们对数据共享的兴趣日益浓厚，使得合成数据极具吸引力。
然而，对综合数据的分析提出了一套独特的方法论
挑战。在这项工作中，我们强调了推理效用的重要性
并提供经验证据来反对从合成数据中进行天真的推断
（处理这些问题就好像它们确实被观察到一样）。我们认为，比率
假阳性结果（1 类错误）将高得令人无法接受，即使
估计是无偏见的。原因之一是低估了真实情况
标准误差，甚至可能随着样本量的增大而逐渐增加
由于收敛速度较慢。这对于深度生成来说尤其成问题
楷模。在发布合成数据之前，必须开发
此类数据的统计推断工具。
]]></description>
      <guid>http://arxiv.org/abs/2312.07837</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:03 GMT</pubDate>
    </item>
    <item>
      <title>用于共识预测的贝叶斯在线学习。 （arXiv：2312.07679v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07679</link>
      <description><![CDATA[给定一个预先训练的分类器和多名人类专家，我们调查了
在线分类任务，其中模型预测是免费提供的，但是
询问人类会产生成本。在这个实用但尚未探索的环境中，
甲骨文地面真相不可用。相反，预测目标被定义
作为所有专家的一致投票。鉴于查询完全共识可以
成本高昂，我们提出了在线贝叶斯共识的总体框架
估计，利用多元超几何的性质
分配。基于这个框架，我们提出了一系列方法
通过产生一个从部分反馈动态估计专家共识
专家和模型信念的后验。分析这个后验归纳出
查询成本和分类性能之间的可解释权衡。
我们根据各种基准证明了我们的框架的有效性
CIFAR-10H 和 ImageNet-16H，两个大规模众包数据集。
]]></description>
      <guid>http://arxiv.org/abs/2312.07679</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>GP+：通过高斯过程进行基于内核的学习的 Python 库。 （arXiv：2312.07694v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07694</link>
      <description><![CDATA[在本文中，我们介绍了 GP+，一个基于内核的开源库
通过高斯过程（GP）学习，这是强大的统计模型
完全由参数协方差和均值来表征
功能。 GP+ 基于 PyTorch 构建，提供了用户友好且
用于概率学习和推理的面向对象工具。和我们一样
通过大量示例证明，GP+ 比其他方法具有一些独特的优势
GP 建模库。我们主要通过整合来实现这些优势
具有 GP 协方差和均值函数的非线性流形学习技术。
作为介绍 GP+ 的一部分，在本文中我们还做了方法论
(1) 实现概率数据融合和逆参数的贡献
估计，以及（2）为 GP 配备简约的参数平均函数，
跨越具有分类和定量特征的混合特征空间
变量。我们展示了这些贡献在以下背景下的影响：
贝叶斯优化、多保真度建模、敏感性分析和
计算机模型的校准。
]]></description>
      <guid>http://arxiv.org/abs/2312.07694</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:02 GMT</pubDate>
    </item>
    <item>
      <title>超越端到端培训：通过上下文供应促进贪婪本地学习。 （arXiv：2312.07636v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07636</link>
      <description><![CDATA[深度网络的传统端到端（E2E）训练需要存储
反向传播的中间激活，产生大量内存
GPU 占用空间和有限的模型并行化。作为备选，
贪婪的局部学习将网络划分为梯度隔离的模块，
根据当地初步损失进行监督训练，从而提供
大幅减少内存的异步和并行训练方法
成本。然而，实证实验表明，随着分割的数量
梯度隔离模块的增加，局部的性能
学习方案大幅退化，严重限制了其可扩展性。到
为了避免这个问题，我们从理论上分析了贪婪局部学习
从信息论的角度提出了ContSup方案，
合并隔离模块之间的上下文供应以补偿
信息丢失。在基准数据集（即CIFAR、SVHN、STL-10）上进行实验
达到 SOTA 结果并表明我们提出的方法可以显着
用最少的内存提高贪婪局部学习的性能
计算开销，允许增加隔离的数量
模块。我们的代码可在 https://github.com/Tab-ct/ContSup 获取。
]]></description>
      <guid>http://arxiv.org/abs/2312.07636</guid>
      <pubDate>Thu, 14 Dec 2023 03:14:01 GMT</pubDate>
    </item>
    </channel>
</rss>