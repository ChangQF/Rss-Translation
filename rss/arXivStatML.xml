<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 30 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Multi-CATE：多精度条件平均治疗效果估计，对未知协变量偏移具有鲁棒性</title>
      <link>https://arxiv.org/abs/2405.18206</link>
      <description><![CDATA[arXiv:2405.18206v1 公告类型：交叉 
摘要：估计异质治疗效果对于为最有可能受益的个体量身定制治疗非常重要。然而，条件平均治疗效果预测因子通常可能在一个群体上进行训练，但可能部署在不同的、可能未知的群体上。我们使用学习多精度预测因子的方法来对 CATE T 学习器（差分回归）进行后处理，以在部署时对未知的协变量偏移具有鲁棒性。该方法通常适用于伪结果回归，例如 DR 学习器。我们展示了这种方法如何通过从观察数据集中学习混杂预测因子，并在随机对照试验中审核多精度，从而结合（大）混杂观察数据集和（小）随机数据集。我们在模拟中展示了偏差和均方误差的改善，其中协变量偏移越来越大，并且在平行的大型观察研究和较小的随机对照实验的半合成案例研究中也是如此。总的来说，我们建立了为多分布学习开发的方法之间的联系，并在因果推理和机器学习中实现了有吸引力的愿望（例如外部有效性）。]]></description>
      <guid>https://arxiv.org/abs/2405.18206</guid>
      <pubDate>Thu, 30 May 2024 06:19:18 GMT</pubDate>
    </item>
    <item>
      <title>矩阵流形神经网络++</title>
      <link>https://arxiv.org/abs/2405.19206</link>
      <description><![CDATA[arXiv:2405.19206v1 公告类型：新
摘要：黎曼流形上的深度神经网络 (DNN) 在各个应用领域引起了越来越多的关注。例如，球面和双曲流形上的 DNN 已被设计用于解决广泛的计算机视觉和自然语言处理任务。这些网络成功的关键因素之一是球面和双曲流形具有丰富的陀螺组和陀螺矢量空间代数结构。这使得最成功的 DNN 能够原则性地、有效地推广到这些流形。最近，一些研究表明，陀螺组和陀螺矢量空间理论中的许多概念也可以推广到矩阵流形，例如对称正定 (SPD) 和格拉斯曼流形。因此，可以以与球面和双曲模型完全类似的方式推导出 SPD 和 Grassmann 神经网络的一些构建块，例如等距模型和多项逻辑回归 (MLR)。基于这些工作，我们为 SPD 神经网络设计了全连接 (FC) 和卷积层。我们还在对称半正定 (SPSD) 流形上开发了 MLR，并提出了一种在投影仪视角下使用 Grassmann 对数图进行反向传播的方法。我们在人体动作识别和节点分类任务中证明了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.19206</guid>
      <pubDate>Thu, 30 May 2024 06:19:17 GMT</pubDate>
    </item>
    <item>
      <title>动态 GNN 的有效共形预测</title>
      <link>https://arxiv.org/abs/2405.19230</link>
      <description><![CDATA[arXiv:2405.19230v1 公告类型：新
摘要：图神经网络 (GNN) 是功能强大的黑盒模型，已显示出令人印象深刻的经验性能。然而，如果没有任何形式的不确定性量化，在高风险场景中很难信任此类模型。共形预测旨在解决这个问题，然而，其有效性需要可交换性的假设，这限制了其对静态图和传导机制的适用性。我们建议使用展开，它允许任何现有的静态 GNN 输出具有可交换性属性的动态图嵌入。利用这一点，我们将共形预测的有效性扩展到传导和半感应机制中的动态 GNN。我们在这些情况下提供了有效共形预测的理论保证，并证明了展开 GNN 相对于标准 GNN 架构在模拟和真实数据集上的经验有效性以及性能提升。]]></description>
      <guid>https://arxiv.org/abs/2405.19230</guid>
      <pubDate>Thu, 30 May 2024 06:19:17 GMT</pubDate>
    </item>
    <item>
      <title>状态空间模型在估计具有动态平滑度的函数方面与 Transformer 相当</title>
      <link>https://arxiv.org/abs/2405.19036</link>
      <description><![CDATA[arXiv:2405.19036v1 公告类型：新
摘要：基于状态空间模型 (SSM) 的深度神经网络在序列建模中引起了广泛关注，因为它们的计算成本明显小于 Transformers。虽然 SSM 的功能主要通过实验比较进行研究，但对 SSM 的理论理解仍然有限。特别是，缺乏对 SSM 是否可以取代 Transformers 的统计和定量评估。在本文中，我们从估计序列到序列函数的角度理论上探讨了在哪些任务中 SSM 可以替代 Transformers。我们考虑目标函数具有方向相关的平滑度的设置，并证明 SSM 可以以与 Transformers 相同的收敛速度估计此类函数。此外，我们证明 SSM 可以估计目标函数，即使平滑度根据输入序列而变化，也可以像 Transformers 一样。我们的结果表明，在估计实践中出现的某些类别的函数时，SSM 可以取代 Transformers。]]></description>
      <guid>https://arxiv.org/abs/2405.19036</guid>
      <pubDate>Thu, 30 May 2024 06:19:16 GMT</pubDate>
    </item>
    <item>
      <title>我敢打赌你不是这个意思：通过博彩测试语义重要性</title>
      <link>https://arxiv.org/abs/2405.19146</link>
      <description><![CDATA[arXiv:2405.19146v1 公告类型：新
摘要：最近的研究将特征重要性的概念扩展到 \emph{语义概念}，这些概念对于与黑盒预测模型交互的用户来说本质上是可解释的。然而，需要精确的统计保证，例如假阳性率控制，以透明地传达发现并避免在现实世界中出现意外后果。在本文中，我们通过条件独立性形式化了语义概念对不透明模型预测的全局（即在总体上）和局部（即对于样本）统计重要性，从而允许进行严格的测试。我们使用最近的顺序核化测试 (SKIT) 思想来归纳概念之间重要性的等级，并展示我们的框架在合成数据集以及使用视觉语言模型（如 CLIP）的图像分类任务上的有效性和灵活性。]]></description>
      <guid>https://arxiv.org/abs/2405.19146</guid>
      <pubDate>Thu, 30 May 2024 06:19:16 GMT</pubDate>
    </item>
    <item>
      <title>核半隐式变分推断</title>
      <link>https://arxiv.org/abs/2405.18997</link>
      <description><![CDATA[arXiv:2405.18997v1 公告类型：新
摘要：半隐式变分推理 (SIVI) 扩展了传统的变分族，其半隐式分布以分层方式定义。由于半隐式分布的密度难以处理，经典 SIVI 通常采用证据下限 (ELBO) 替代方法，这会引入训练偏差。SIVI 的最新进展，称为 SIVI-SM，利用了一种替代分数匹配目标，通过极小极大公式使其易于处理，尽管需要额外的低级优化。在本文中，我们提出了核 SIVI (KSIVI)，这是 SIVI-SM 的一种变体，它通过核技巧消除了对低级优化的需求。具体而言，我们表明，当在再生核希尔伯特空间 (RKHS) 上进行优化时，低级问题有一个明确的解决方案。这样，上层目标就变成了核斯坦差异 (KSD)，由于半隐式变分分布的层次结构，它很容易通过随机梯度下降计算出来。推导出 KSD 目标的蒙特卡洛梯度估计量的方差上限，这使我们能够建立 KSIVI 的新收敛保证。我们证明了 KSIVI 在合成分布和各种真实数据贝叶斯推理任务上的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2405.18997</guid>
      <pubDate>Thu, 30 May 2024 06:19:15 GMT</pubDate>
    </item>
    <item>
      <title>适用于多尺度参数化 PDE 的物理感知神经隐式求解器，可用于非均匀介质</title>
      <link>https://arxiv.org/abs/2405.19019</link>
      <description><![CDATA[arXiv:2405.19019v1 公告类型：新
摘要：我们提出了物理感知神经隐式求解器 (PANIS)，这是一种用于学习参数化偏微分方程 (PDE) 替代项的新型数据驱动框架。它由一个概率学习目标组成，其中加权残差用于探测 PDE 并提供 {\em 虚拟} 数据源，即实际 PDE 永远不需要求解。这与物理感知隐式求解器相结合，该求解器由原始 PDE 的更粗略的离散化版本组成，它为高维问题提供了必要的信息瓶颈，并能够在分布外设置（例如不同的边界条件）中进行泛化。我们在随机异质材料的背景下展示了它的能力，其中输入参数代表材料微观结构。我们将该框架扩展到多尺度问题，并表明可以学习有效（均质化）解决方案的替代项，而无需解决参考问题。我们进一步展示了所提出的框架如何适应和概括几个现有的学习目标和架构，同时产生可以量化预测不确定性的概率替代品。]]></description>
      <guid>https://arxiv.org/abs/2405.19019</guid>
      <pubDate>Thu, 30 May 2024 06:19:15 GMT</pubDate>
    </item>
    <item>
      <title>EntProp：高熵传播，提高准确性和鲁棒性</title>
      <link>https://arxiv.org/abs/2405.18931</link>
      <description><![CDATA[arXiv:2405.18931v1 公告类型：新
摘要：尽管深度神经网络 (DNN) 性能出色，但它们很难推广到与训练中的分布不同的分布外域。在实际应用中，DNN 必须具有高标准准确率和对分布外域的鲁棒性。一种可以同时实现这两种改进的技术是通过辅助批量规范化层 (ABN) 进行混合分布的解缠学习。该技术将干净样本和转换后的样本视为不同的域，从而使 DNN 能够从混合域中学习更好的特征。但是，如果我们根据熵区分样本的域，我们会发现一些转换后的样本与干净样本来自同一域，而这些样本并不是完全不同的域。为了生成从与干净样本完全不同的域中提取的样本，我们假设对干净的高熵样本进行转换以进一步增加熵会生成离分布内域更远的分布外样本。基于这一假设，我们提出了高熵传播（EntProp），将高熵样本输入到使用 ABN 的网络。我们引入了两种技术，即数据增强和自由对抗训练，它们可以增加熵并使样本远离分布域。这些技术不需要额外的训练成本。我们的实验结果表明，与基线方法相比，EntProp 以更低的训练成本实现了更高的标准准确率和稳健性。特别是，EntProp 在小型数据集上的训练非常有效。]]></description>
      <guid>https://arxiv.org/abs/2405.18931</guid>
      <pubDate>Thu, 30 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>使用随机森林实现的 Mallows 类异常检测标准</title>
      <link>https://arxiv.org/abs/2405.18932</link>
      <description><![CDATA[arXiv:2405.18932v1 公告类型：新
摘要：异常信号检测的有效性可能因依赖一个指定模型的固有不确定性而受到严重损害。在模型平均方法的框架下，本文提出了一种新的标准来选择多个模型聚合的权重，其中焦点损失函数解释了极度不平衡数据的分类。该策略通过取代传统的投票方法进一步融入随机森林算法。我们已经在包括网络入侵在内的各个领域的基准数据集上评估了所提出的方法。研究结果表明，我们提出的方法不仅超越了具有典型损失函数的模型平均，而且在准确性和鲁棒性方面也超过了常见的异常检测算法。]]></description>
      <guid>https://arxiv.org/abs/2405.18932</guid>
      <pubDate>Thu, 30 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>具有参考优势分解的联邦 Q 学习：几乎最优遗憾和对数通信成本</title>
      <link>https://arxiv.org/abs/2405.18795</link>
      <description><![CDATA[arXiv:2405.18795v1 公告类型：新
摘要：在本文中，我们考虑了用于表格情节马尔可夫决策过程的无模型联邦强化学习。在中央服务器的协调下，多个代理协作探索环境并学习最佳策略，而无需共享其原始数据。尽管联邦 Q 学习算法最近取得了进展，实现了近线性的遗憾加速和低通信成本，但现有算法仅达到与信息界限相比的次优遗憾。我们提出了一种新颖的无模型联邦 Q 学习算法，称为 FedQ-Advantage。我们的算法利用参考优势分解来减少方差，并在两种不同的机制下运行：代理和服务器之间的同步以及策略更新，两者均由事件触发。我们证明，我们的算法不仅需要较低的对数通信成本，而且还实现了几乎最佳的遗憾，当时间范围足够大时，与单代理算法相比，达到了对数因子的信息界限和近线性的遗憾加速。]]></description>
      <guid>https://arxiv.org/abs/2405.18795</guid>
      <pubDate>Thu, 30 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>受污染未标记数据的深度正未标记异常检测</title>
      <link>https://arxiv.org/abs/2405.18929</link>
      <description><![CDATA[arXiv:2405.18929v1 Announce Type: new 
摘要：半监督异常检测引起了人们的关注，其目的是通过在未标记数据之外使用少量异常数据来提高异常检测器的性能。现有的半监督方法假设未标记数据大多是正常的。它们训练异常检测器以最小化未标记数据的异常分数，并最大化异常数据的异常分数。然而，在实践中，未标记数据经常受到异常的污染。这削弱了最大化异常分数的效果，并阻止我们提高检测性能。为了解决这个问题，我们提出了正无标记自动编码器，它基于正无标记学习和异常检测器（如自动编码器）。通过我们的方法，我们可以使用未标记和异常数据来近似正常数据的异常分数。因此，在没有标记的正常数据的情况下，我们可以训练异常检测器以最小化正常数据的异常分数，并最大化异常数据的异常分数。此外，我们的方法适用于各种异常检测器，例如 DeepSVDD。在各种数据集上的实验表明，我们的方法比现有方法实现了更好的检测性能。]]></description>
      <guid>https://arxiv.org/abs/2405.18929</guid>
      <pubDate>Thu, 30 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>从共形预测到置信区域</title>
      <link>https://arxiv.org/abs/2405.18601</link>
      <description><![CDATA[arXiv:2405.18601v1 公告类型：新
摘要：共形预测方法显著推进了预测模型中不确定性的量化。然而，模型参数置信区域的构建提出了一个显著的挑战，通常需要对数据分布做出严格的假设或仅仅提供渐近保证。我们引入了一种称为 CCR 的新方法，该方法采用模型输出的共形预测区间组合来建立模型参数的置信区域。我们在噪声假设最小的情况下提出了覆盖保证，并且在有限样本范围内有效。我们的方法适用于分裂共形预测和黑盒方法，包括全共形或交叉共形方法。在线性模型的特定情况下，导出的置信区域表现为混合整数线性规划 (MILP) 的可行集，有助于推导单个参数的置信区间并实现稳健优化。我们通过实证将 CCR 与具有挑战性的设置（例如异方差和非高斯噪声）中的最新进展进行了比较。]]></description>
      <guid>https://arxiv.org/abs/2405.18601</guid>
      <pubDate>Thu, 30 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>通过学习密度比率进行拒绝</title>
      <link>https://arxiv.org/abs/2405.18686</link>
      <description><![CDATA[arXiv:2405.18686v1 公告类型：新
摘要：拒绝分类作为一种学习范式出现，它允许模型放弃做出预测。主要方法是通过增强典型的损失函数来改变监督学习管道，让模型拒绝产生的损失低于错误预测。相反，我们提出了一种不同的分布视角，我们试图找到一个理想的数据分布，以最大化预训练模型的性能。这可以通过使用 $ \phi$-divergence 正则化项优化损失风险来形式化。通过这个理想化的分布，可以利用该分布与数据分布之间的密度比来做出拒绝决定。我们专注于我们的 $ \phi $-divergence 由 $ \alpha $-divergence 系列指定的设置。我们的框架在干净和嘈杂的数据集上进行了实证测试。]]></description>
      <guid>https://arxiv.org/abs/2405.18686</guid>
      <pubDate>Thu, 30 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>分类的大边缘判别损失</title>
      <link>https://arxiv.org/abs/2405.18499</link>
      <description><![CDATA[arXiv:2405.18499v1 公告类型：新
摘要：在本文中，我们在深度学习的背景下引入了一种具有大边距的新型判别损失函数。这种损失提高了神经网络的判别能力，以类内紧凑性和类间可分性为代表。一方面，通过同一类样本之间的近距离来确保类紧凑性。另一方面，通过边距损失来提高类间可分性，该损失确保每个类与其最近边界的距离最小。我们损失中的所有项都有明确的含义，可以直接查看获得的特征空间。我们从数学上分析了紧凑性和边距项之间的关系，给出了超参数对学习特征的影响的指导方针。此外，我们还分析了损失梯度相对于神经网络参数的性质。在此基础上，我们设计了一种称为部分动量更新的策略，该策略在训练中同时具有稳定性和一致性。此外，我们还研究了泛化误差以获得更好的理论见解。在我们的实验中，与标准 softmax 损失相比，我们的损失函数系统地提高了模型的测试准确率。]]></description>
      <guid>https://arxiv.org/abs/2405.18499</guid>
      <pubDate>Thu, 30 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>通过几何复杂度实现基于边缘的多类泛化界限</title>
      <link>https://arxiv.org/abs/2405.18590</link>
      <description><![CDATA[arXiv:2405.18590v1 公告类型：新
摘要：人们付出了相当大的努力来更好地理解深度神经网络的泛化能力，这既是了解其成功的理论手段，也是提供进一步改进的方向。在本文中，我们研究了基于边际的神经网络多类泛化界限，该界限依赖于最近为神经网络开发的复杂性度量，即几何复杂性。我们推导出了泛化误差的新上限，该上限与网络的边际归一化几何复杂性成比例，并且适用于广泛的数据分布和模型类。我们对使用 SGD 在 CIFAR-10 和 CIFAR-100 数据集上训练的 ResNet-18 模型进行了实证研究，该数据集具有原始和随机标签。]]></description>
      <guid>https://arxiv.org/abs/2405.18590</guid>
      <pubDate>Thu, 30 May 2024 06:19:11 GMT</pubDate>
    </item>
    </channel>
</rss>