<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 29 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>高维重叠高斯混合数据分类：从最优分类器到神经网络</title>
      <link>https://arxiv.org/abs/2405.18427</link>
      <description><![CDATA[arXiv:2405.18427v1 公告类型：新
摘要：我们推导出高维重叠高斯混合模型 (GMM) 数据二元分类中贝叶斯最优决策边界的闭式表达式，并展示了它们如何依赖于类协方差的特征结构，尤其是对于有趣的结构化数据。我们通过对受真实数据启发的合成 GMM 进行实验，从经验上证明，经过分类训练的深度神经网络可以学习近似于派生的最优分类器的预测因子。我们进一步将研究扩展到对真实数据进行训练的网络，观察到决策阈值与协方差特征向量而不是特征值相关，这与我们的 GMM 分析相吻合。这为神经网络执行概率推理和从复杂分布中提取统计模式的能力提供了理论见解。]]></description>
      <guid>https://arxiv.org/abs/2405.18427</guid>
      <pubDate>Wed, 29 May 2024 18:17:56 GMT</pubDate>
    </item>
    <item>
      <title>Bagging 极大地提高了泛化能力</title>
      <link>https://arxiv.org/abs/2405.14741</link>
      <description><![CDATA[arXiv:2405.14741v1 公告类型：交叉 
摘要：Bagging 是一种流行的集成技术，可以提高机器学习模型的准确性。它取决于一个公认的原理，即通过反复对重采样数据进行再训练，聚合模型表现出较低的方差，从而具有较高的稳定性，尤其是对于不连续的基础学习者。在本文中，我们提供了一个关于 bagging 的新视角：通过在参数化而不是输出级别适当地聚合基础学习者，bagging 可以成倍地提高泛化性能，这种强度比方差减少要强大得多。更准确地说，我们表明，对于遭受缓慢（即多项式）衰减泛化误差的一般随机优化问题，bagging 可以有效地将这些误差降低到指数衰减。此外，bagging 的这种能力与解决方案无关，包括常见的经验风险最小化、分布稳健优化和各种正则化。我们展示了 bagging 如何能够在一系列涉及本质上速率较慢的重尾数据的示例中显著提高泛化性能。]]></description>
      <guid>https://arxiv.org/abs/2405.14741</guid>
      <pubDate>Wed, 29 May 2024 18:17:56 GMT</pubDate>
    </item>
    <item>
      <title>用于建模 SGD 的 Hessian 感知随机微分方程</title>
      <link>https://arxiv.org/abs/2405.18373</link>
      <description><![CDATA[arXiv:2405.18373v1 公告类型：新
摘要：随机梯度下降 (SGD) 的连续时间近似是研究其从驻点逃逸行为的重要工具。然而，现有的随机微分方程 (SDE) 模型无法完全捕捉这些行为，即使对于简单的二次目标也是如此。基于一种新颖的随机后向误差分析框架，我们推导出 Hessian 感知随机修正方程 (HA-SME)，这是一种将目标函数的 Hessian 信息纳入其漂移和扩散项的 SDE。我们的分析表明，HA-SME 符合文献中现有 SDE 模型中顺序最佳近似误差保证，同时显著降低了对目标平滑度参数的依赖性。此外，对于二次目标，在温和条件下，HA-SME 被证明是第一个在分布意义上准确恢复 SGD 动态的 SDE 模型。因此，当驻点附近的局部景观可以用二次函数近似时，HA-SME 有望准确预测 SGD 的局部逃逸行为。]]></description>
      <guid>https://arxiv.org/abs/2405.18373</guid>
      <pubDate>Wed, 29 May 2024 18:17:55 GMT</pubDate>
    </item>
    <item>
      <title>关于预测驱动的引导程序的注释</title>
      <link>https://arxiv.org/abs/2405.18379</link>
      <description><![CDATA[arXiv:2405.18379v1 公告类型：新
摘要：我们介绍了 PPBoot：一种基于引导的预测驱动推理方法。PPBoot 适用于任意估计问题，并且实现起来非常简单，基本上只需要一次引导应用。通过一系列示例，我们证明当后者适用时，PPBoot 的表现通常与早期基于渐近正态性的 PPI(++) 方法$\unicode{x2013}$几乎相同（有时甚至更好），而后者不需要任何渐近特征。鉴于其多功能性，PPBoot 可以简化和扩展预测驱动推理的应用范围，以解决中心极限定理难以证明的问题。]]></description>
      <guid>https://arxiv.org/abs/2405.18379</guid>
      <pubDate>Wed, 29 May 2024 18:17:55 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯网络分类器的上下文特定改进</title>
      <link>https://arxiv.org/abs/2405.18298</link>
      <description><![CDATA[arXiv:2405.18298v1 公告类型：新
摘要：监督分类是机器学习中最普遍的任务之一。基于贝叶斯网络的生成分类器因其可解释性和竞争性准确性而经常被使用。广泛使用的朴素分类器和 TAN 分类器是具有受限底层图的贝叶斯网络分类器的具体实例。本文介绍了扩展 TAN 和其他著名贝叶斯网络分类器类型的新型生成分类器。我们的方法基于分阶段树模型，该模型通过允许复杂的、特定于上下文的依赖模式来扩展贝叶斯网络。我们正式研究了我们的新型分类器与贝叶斯网络之间的关系。我们为我们的模型引入并实施数据驱动的学习例程，并在广泛的计算研究中调查它们的准确性。研究表明，嵌入不对称信息的模型可以提高分类准确性。]]></description>
      <guid>https://arxiv.org/abs/2405.18298</guid>
      <pubDate>Wed, 29 May 2024 18:17:54 GMT</pubDate>
    </item>
    <item>
      <title>从不完整数据中学习分阶段树</title>
      <link>https://arxiv.org/abs/2405.18306</link>
      <description><![CDATA[arXiv:2405.18306v1 公告类型：新
摘要：分阶段树是概率图形模型，能够通过对其顶点进行着色来表示任何非对称独立性类别。在频率论或贝叶斯范式下，已经定义并实施了几种结构学习例程来从数据中学习分阶段树。他们假设已经完全观察到数据集，并且在实践中，在学习模型之前，会删除或估算缺失条目的观察结果。在这里，我们介绍了第一个处理模型学习中缺失值的分阶段树算法。为此，我们描述了在存在缺失数据的情况下分阶段树模型的可能性，并讨论了近似它的伪可能性。还实施并评估了一种直接从全可能性估计模型的结构期望最大化算法。计算实验展示了新学习算法的性能，证明了在学习分阶段树时考虑不同的缺失模式是可行的。]]></description>
      <guid>https://arxiv.org/abs/2405.18306</guid>
      <pubDate>Wed, 29 May 2024 18:17:54 GMT</pubDate>
    </item>
    <item>
      <title>用于离散密度估计的非负张量混合学习</title>
      <link>https://arxiv.org/abs/2405.18220</link>
      <description><![CDATA[arXiv:2405.18220v1 公告类型：新
摘要：我们提出了一种基于期望最大化 (EM) 的非负张量分解统一框架，以优化 Kullback-Leibler 散度。为了避免每个 M 步中的迭代和学习率调整，我们在低秩分解和多体近似之间建立了一般关系。利用这种联系，我们利用多体近似的闭式解可用于在 M 步中同时更新所有参数。我们的框架不仅为各种低秩结构（包括 CP、Tucker 和 Train 分解）提供了统一的方法，而且还为它们的组合提供了形成张量混合以及稳健的自适应噪声建模。从经验上讲，我们证明与传统的基于张量的方法相比，我们的框架为离散密度估计提供了更好的泛化。]]></description>
      <guid>https://arxiv.org/abs/2405.18220</guid>
      <pubDate>Wed, 29 May 2024 18:17:53 GMT</pubDate>
    </item>
    <item>
      <title>具有流式数据的高维 GLM 中的自适应去偏 SGD</title>
      <link>https://arxiv.org/abs/2405.18284</link>
      <description><![CDATA[arXiv:2405.18284v1 公告类型：新
摘要：在线统计推断有助于实时分析连续收集的数据，这有别于依赖静态数据集的传统方法。本文介绍了一种在高维广义线性模型中进行在线推断的新方法，我们在每次新数据到达时更新回归系数估计及其标准误差。与需要完整数据集访问或大维汇总统计数据存储的现有方法相比，我们的方法以单次通过模式运行，大大降低了时间和空间复杂度。我们方法创新的核心在于针对动态目标函数量身定制的自适应随机梯度下降算法，以及新颖的在线去偏程序。这使我们能够保持低维汇总统计数据，同时有效控制动态变化的损失函数引入的优化误差。我们证明，我们的方法称为近似去偏套索 (ADL)，不仅减轻了对有界个体概率条件的需求，而且显著提高了数值性能。数值实验表明，所提出的 ADL 方法在各种协方差矩阵结构中始终表现出稳健的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.18284</guid>
      <pubDate>Wed, 29 May 2024 18:17:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习对于自然科学来说是好是坏？</title>
      <link>https://arxiv.org/abs/2405.18095</link>
      <description><![CDATA[arXiv:2405.18095v1 公告类型：新
摘要：机器学习 (ML) 方法对所有科学领域都产生了巨大影响。然而，ML 具有强大的本体论（其中只有数据存在）和强大的认识论（如果模型在保留的训练数据上表现良好，则该模型被认为是好的。这些理念与自然科学的标准实践和关键理念存在强烈冲突。在这里，我们确定了自然科学中本体论和认识论有价值的一些 ML 位置。例如，当在因果推理中使用表达性机器学习模型来表示混杂因素（例如前景、背景或仪器校准参数）的影响时，ML 的模型容量和宽松理念可以使结果更值得信赖。我们还表明，在某些情况下，引入 ML 会引入强烈的、不必要的统计偏差。首先，当使用 ML 模型来模拟物理（或第一性原理）模拟时，它们会引入强烈的确认偏差。另一方面，当使用表达性回归来标记数据集时，这些标签不能用于下游联合或集成分析，否则会产生不受控制的偏差。标题中的问题是针对所有自然科学的；也就是说，我们呼吁科学界退一步思考 ML 在其领域中的作用和价值；我们在此给出的（部分）答案来自物理学的特定视角。]]></description>
      <guid>https://arxiv.org/abs/2405.18095</guid>
      <pubDate>Wed, 29 May 2024 18:17:52 GMT</pubDate>
    </item>
    <item>
      <title>SEMF：用于预测区间的监督期望最大化框架</title>
      <link>https://arxiv.org/abs/2405.18176</link>
      <description><![CDATA[arXiv:2405.18176v1 公告类型：新
摘要：这项工作引入了监督期望最大化框架 (SEMF)，这是一个多功能且与模型无关的框架，可为具有完整或缺失数据的数据集生成预测区间。SEMF 将传统上用于无监督学习的期望最大化 (EM) 算法扩展到监督环境，使其能够提取用于不确定性估计的潜在表示。该框架通过对 11 个表格数据集的广泛实证评估证明了其稳健性，在某些情况下实现了比传统分位数回归方法更窄的归一化预测区间和更高的覆盖率。此外，SEMF 与现有的机器学习算法（如梯度提升树和神经网络）无缝集成，体现了其在实际应用中的实用性。实验结果凸显了 SEMF 在推动不确定性量化最先进技术方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.18176</guid>
      <pubDate>Wed, 29 May 2024 18:17:52 GMT</pubDate>
    </item>
    <item>
      <title>谱截断核：$C^*$ 代数核机中的非交换性</title>
      <link>https://arxiv.org/abs/2405.17823</link>
      <description><![CDATA[arXiv:2405.17823v1 公告类型：新
摘要：在本文中，我们提出了一类基于谱截断的新型正定核，该核已在非交换几何和 $C^*$-代数领域进行了讨论。我们专注于输入和输出为函数的核，并通过引入截断参数 $n$ 来概括现有核，例如多项式、乘积和可分离核，该参数描述核中出现的乘积的非交换性。当 $n$ 趋于无穷大时，所提出的核趋向于现有的交换核。如果 $n$ 是有限的，它们会表现出不同的行为，并且非交换性会沿数据函数域引起相互作用。我们表明截断参数 $n$ 是导致性能增强的决定因素：通过设置适当的 $n$，我们可以平衡表示能力和表示空间的复杂性。所提出的核类的灵活性使我们能够超越以前的交换核。]]></description>
      <guid>https://arxiv.org/abs/2405.17823</guid>
      <pubDate>Wed, 29 May 2024 18:17:51 GMT</pubDate>
    </item>
    <item>
      <title>利用间接数据进行有效的预先校准</title>
      <link>https://arxiv.org/abs/2405.17955</link>
      <description><![CDATA[arXiv:2405.17955v1 公告类型：新
摘要：贝叶斯反演对于量化科学和工程领域众多应用中出现的问题的不确定性至关重要。要制定该方法，需要四个要素：将未知参数映射到解空间元素的前向模型，通常是微分方程的解空间；将解空间元素映射到数据空间的观测算子；描述噪声如何污染观测的噪声模型；以及描述在获取数据之前有关未知参数的知识的先验模型。本文关注从数据中学习先验模型；特别是从通过嘈杂的观察过程获得的间接数据的多个实现中学习先验。使用生成模型将先验表示为潜在空间中高斯的前推；通过最小化适当的损失函数来学习前推图。使用在经验近似下定义明确的度量来定义前推图的损失函数，以制定可实施的方法。此外，提出了一种有效的基于残差的正向模型神经算子近似方法，并表明可以使用问题的双层优化公式与推进图同时学习该方法；这种神经算子近似方法的使用有可能使从间接数据中进行的先前学习在计算上更加高效，尤其是在观察过程昂贵、不平滑或未知的情况下。这些想法通过达西流逆问题得到说明，即从测压头测量中找出渗透率。]]></description>
      <guid>https://arxiv.org/abs/2405.17955</guid>
      <pubDate>Wed, 29 May 2024 18:17:51 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯神经网络中的结构化部分随机性</title>
      <link>https://arxiv.org/abs/2405.17666</link>
      <description><![CDATA[arXiv:2405.17666v1 公告类型：新
摘要：贝叶斯神经网络后验分布具有大量对应于相同网络函数的模式。此类模式的丰富性可能使近似推理方法难以发挥作用。最近的工作证明了部分随机性对贝叶斯神经网络近似推理的好处；推理成本可以更低，有时还可以提高性能。我提出了一种结构化的方法来选择确定性的权重子集，以消除神经元置换对称性，从而消除相应的冗余后验模式。通过大幅简化后验分布，发现现有近似推理方案的性能得到了极大改善。]]></description>
      <guid>https://arxiv.org/abs/2405.17666</guid>
      <pubDate>Wed, 29 May 2024 18:17:50 GMT</pubDate>
    </item>
    <item>
      <title>在较弱条件下驯服的朗之万采样</title>
      <link>https://arxiv.org/abs/2405.17693</link>
      <description><![CDATA[arXiv:2405.17693v1 公告类型：新
摘要：受深度学习应用的启发，这些应用通常无法满足标准的 Lipschitz 平滑度要求，我们研究了从非对数凹且仅具有弱耗散性的分布中采样的问题，其中对数梯度允许在无穷远处超线性增长。在结构方面，我们仅假设目标分布满足对数 Sobolev 或 Poincar\&#39;e 不等式和局部 Lipschitz 平滑度假设，其中模数可能在无穷远处多项式增长。这组假设大大超出了“原始”未调整的 Langevin 算法 (ULA) 的操作限制，使得从此类分布中采样成为一件非常复杂的事情。为了解决这个问题，我们引入了一种根据目标分布的增长和衰减特性量身定制的驯服方案，并根据 Kullback-Leibler (KL) 散度、总变分和与目标分布的 Wasserstein 距离为所提出的采样器提供明确的非渐近保证。]]></description>
      <guid>https://arxiv.org/abs/2405.17693</guid>
      <pubDate>Wed, 29 May 2024 18:17:50 GMT</pubDate>
    </item>
    <item>
      <title>特征学习的汉密尔顿力学：泄漏 ResNet 中的瓶颈结构</title>
      <link>https://arxiv.org/abs/2405.17573</link>
      <description><![CDATA[arXiv:2405.17573v1 公告类型：新
摘要：我们研究了 Leaky ResNets，它根据“有效深度”超参数 $\tilde{L}$ 在 ResNets ($\tilde{L}=0$) 和全连接网络 ($\tilde{L}\to\infty$) 之间进行插值。在无限深度极限下，我们研究“表示测地线”$A_{p}$：表示空间中的连续路径（类似于 NeuralODEs），从输入 $p=0$ 到输出 $p=1$，最小化网络的参数范数。我们给出了拉格朗日和哈密顿重新表述，强调了两个术语的重要性：有利于小层导数 $\partial_{p}A_{p}$ 的动能和有利于低维表示的势能，以“身份成本”衡量。这两种力量之间的平衡为 ResNets 中的特征学习提供了直观的理解。我们利用这种直觉来解释瓶颈结构的出现，正如之前的工作中所观察到的：对于较大的 $\tilde{L}$，势能占主导地位并导致时间尺度分离，其中表示从高维输入快速跳转到低维表示，在低维表示空间内缓慢移动，然后跳回到潜在的高维输出。受此现象的启发，我们使用自适应层步长进行训练以适应时间尺度分离。]]></description>
      <guid>https://arxiv.org/abs/2405.17573</guid>
      <pubDate>Wed, 29 May 2024 18:17:49 GMT</pubDate>
    </item>
    </channel>
</rss>