<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>可解释预测时间序列校正的替代模型</title>
      <link>https://arxiv.org/abs/2412.19897</link>
      <description><![CDATA[arXiv:2412.19897v1 公告类型：新
摘要：我们引入了一种可解释时间序列预测的局部替代方法。使用最初不可解释的预测模型来改进经典时间序列“基础模型”的预测。通过将基础模型再次拟合到已删除（减去）错误预测的数据，可以实现校正的“可解释性”，从而产生可解释的模型参数差异。我们提供示例来展示该方法发现和解释数据中潜在模式的潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.19897</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度广义薛定谔桥：从图像生成到解决平均场博弈</title>
      <link>https://arxiv.org/abs/2412.20279</link>
      <description><![CDATA[arXiv:2412.20279v1 公告类型：新
摘要：广义薛定谔桥（GSB）是一种基本数学框架，用于分析基于最小作用原理（包括动能和势能）的最可能粒子演化。除了它们在量子力学和最优传输理论领域的广泛存在之外，本文还重点关注算法视角，旨在增强实际应用。我们观察到，具有由 GSB 描述的最优结构的传输问题在各种科学领域中普遍存在，例如机器学习中的生成模型、随机控制中的平均场博弈等等。因此，探索 GSB 的数学建模与现代算法表征之间的内在联系是一条至关重要但尚未开发的途径。在本文中，我们将 GSB 重新解释为概率模型，并证明，借助一种称为非线性 Feynman-Kac 引理的精细数学工具，丰富的算法概念，诸如似然、变分间隙和时间差异等，自然而然地从 GSB 的最优结构中出现。由此产生的计算框架由深度学习和神经网络驱动，在完全连续的状态空间（即无网格）中运行并满足分布约束，使其有别于以前依赖空间离散化或约束松弛的数值求解器。我们展示了我们的方法在生成建模和平均场游戏中的有效性，强调了它在数学建模、随机过程、控制和机器学习交叉领域的变革性应用。]]></description>
      <guid>https://arxiv.org/abs/2412.20279</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用密集 ReLU 网络构建置信区间并估计条件方差</title>
      <link>https://arxiv.org/abs/2412.20355</link>
      <description><![CDATA[arXiv:2412.20355v1 公告类型：新
摘要：本文使用具有整流线性单元 (ReLU) 激活函数的密集网络解决了非参数回归中的条件方差估计和置信区间构建问题。我们提出了一个基于残差的条件方差估计框架，推导出异方差和同方差设置下方差估计的非渐近界限。我们放宽了亚高斯噪声假设，允许所提出的界限适应亚指数噪声及更高噪声。在此基础上，对于 ReLU 神经网络估计器，我们推导出其条件均值和方差估计的非渐近界限，这是使用 ReLU 网络进行方差估计的第一个结果。此外，我们开发了一个基于 ReLU 网络的稳健引导程序（Efron，1992），用于构建真实均值的置信区间，并对覆盖范围提供理论保证，为不确定性量化和深度学习环境中可靠置信区间的构建提供了重大进步。]]></description>
      <guid>https://arxiv.org/abs/2412.20355</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过连续概率空间中的迭代算法实现分布稳健优化</title>
      <link>https://arxiv.org/abs/2412.20556</link>
      <description><![CDATA[arXiv:2412.20556v1 公告类型：新 
摘要：我们考虑一个由分布稳健优化 (DRO) 驱动的极小极大问题，当最坏情况分布是连续时，由于优化问题的无限维性质，导致重大的计算挑战。最近的研究探索了使用基于神经网络的生成模型学习最坏情况分布以解决这些计算挑战，但缺乏算法收敛保证。本文通过提出一种迭代算法来解决这种极小极大问题，在温和的假设下实现全局收敛，并利用向量空间极小极大优化和连续概率密度空间中的凸分析技术工具，从而弥补了这一理论空白。特别是，利用 Brenier 定理，我们将最坏情况分布表示为应用于连续参考测度的传输图，并将基于正则化差异的 DRO 重新表述为 Wasserstein 空间中的极小极大问题。此外，我们证明了可以使用改进的 Jordan-Kinderlehrer-Otto (JKO) 方案高效地计算最坏情况分布，该方案具有足够大的正则化参数，用于常用的差异函数，并与模糊集的半径相关联。此外，我们推导出全局收敛速度，并量化获得近似驻点所需的次梯度和不精确改进的 JKO 迭代总数。这些结果可能适用于非凸和非平滑场景，与现代机器学习应用具有广泛的相关性。]]></description>
      <guid>https://arxiv.org/abs/2412.20556</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测试并提高认知模型摊销贝叶斯推理的稳健性</title>
      <link>https://arxiv.org/abs/2412.20586</link>
      <description><![CDATA[arXiv:2412.20586v1 公告类型：新
摘要：污染物观测值和异常值在估计认知模型的参数时经常会引起问题，认知模型是代表认知过程的统计模型。在本研究中，我们使用神经网络的摊销贝叶斯推理 (ABI) 测试并提高参数估计的稳健性。为此，我们对一个玩具示例进行系统分析，并使用流行的认知模型漂移扩散模型 (DDM) 分析合成数据和真实数据。首先，我们使用稳健统计工具研究 ABI 对污染物的敏感性：经验影响函数和崩溃点。接下来，我们提出了一种数据增强或噪声注入方法，该方法在训练期间将污染分布纳入数据生成过程。我们检查了几个候选分布，并评估了它们相对于标准估计量在准确性和效率损失方面的性能和成本。在训练过程中引入柯西分布中的污染物可显著提高神经密度估计器的稳健性，其衡量标准是影响函数有界且击穿点更高。总体而言，所提出的方法简单易行，在异常值检测或移除具有挑战性的领域具有广泛的适用性。]]></description>
      <guid>https://arxiv.org/abs/2412.20586</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机符号图的矩阵集中和符号随机块模型中的社区恢复</title>
      <link>https://arxiv.org/abs/2412.20620</link>
      <description><![CDATA[arXiv:2412.20620v1 公告类型：新
摘要：我们考虑图，其中边及其符号从所有节点对中随机独立添加。我们为从该系列随机图模型中获得的邻接和拉普拉斯矩阵建立强集中不等式。然后，我们将结果应用于研究从有符号随机块模型中采样的图。即，我们采用双社区设置，其中社区内的边具有正符号，社区之间的边具有负符号，并以概率 $0&lt; s &lt;1/2$ 应用随机符号扰动。在这种情况下，我们的发现包括：首先，相应有符号拉普拉斯矩阵的谱间隙以高概率集中在 $2s$ 附近；其次，拉普拉斯矩阵的第一个特征向量的符号为平衡社区检测问题或等效地，$\pm 1$ 同步问题定义了一个弱一致性估计量。我们利用从所考虑的模型中获得的实验数据来补充我们的理论贡献。]]></description>
      <guid>https://arxiv.org/abs/2412.20620</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用软钻石正则化器训练深度神经分类器</title>
      <link>https://arxiv.org/abs/2412.20724</link>
      <description><![CDATA[arXiv:2412.20724v1 公告类型：新
摘要：我们引入了新的 \emph{软钻石} 正则化器，既可以改善突触稀疏性，又可以保持深度神经网络的分类准确性。这些参数化正则化器的表现优于 Lasso 回归和分类的最先进的硬钻石拉普拉斯正则化器。它们使用厚尾对称 alpha 稳定 ($\mathcal{S \alpha S}$) 钟形曲线突触权重先验，这些先验不是高斯的，因此尾部较厚。菱形约束集的几何形状从圆形到星形不等，具体取决于先验概率密度函数的尾部厚度和离散度。直接使用这些先验进行训练需要大量计算，因为几乎所有 $\mathcal{S \alpha S}$ 概率密度都缺乏封闭形式。预先计算的查找表消除了这个计算瓶颈。我们在三个数据集 CIFAR-10、CIFAR-100 和 Caltech-256 上测试了新的软钻石正则化器和深度神经分类器。正则化器提高了分类器的准确性。改进包括 CIFAR-10 上的 $4.57\%$、CIFAR-100 上的 $4.27\%$ 和 Caltech-256 上的 $6.69\%$。它们在所有测试用例上的表现也优于 $L_2$ 正则化器。软钻石正则化器还优于 $L_1$ 套索或拉普拉斯正则化器，因为它们在提高分类准确性的同时更好地增加了稀疏性。软钻石先验与 dropout、批量或数据增强正则化相结合时，显著提高了 CIFAR-10 的准确性。]]></description>
      <guid>https://arxiv.org/abs/2412.20724</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>离散评分量表数据的稳健矩阵完成</title>
      <link>https://arxiv.org/abs/2412.20802</link>
      <description><![CDATA[arXiv:2412.20802v1 公告类型：新
摘要：近年来，矩阵补全引起了人们的极大兴趣。矩阵补全的目标是使用已知条目来预测部分观察到的矩阵的未知条目。尽管常见应用具有离散评级量表数据，例如推荐系统中的用户产品评级矩阵或社会和行为科学中的调查，但矩阵补全方法几乎总是针对连续数据设计和研究的。此外，尽管在实践中经常出现损坏的观察结果，但只有一小部分文献考虑了矩阵补全。例子包括对推荐系统的攻击（即恶意用户故意操纵评级以影响推荐系统以使其受益），或调查中的粗心受访者（即由于注意力不集中，受访者无论调查要求他们做什么都提供答案）。我们引入了一种矩阵补全算法，该算法针对评级量表数据的离散性质进行了定制，并且对损坏的观察结果具有鲁棒性。此外，我们研究了所提出的方法及其竞争对手在使用离散评级量表（而非连续）数据以及各种缺失数据机制和损坏观测类型时的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.20802</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用高斯过程进行不确定性感知的分布外检测</title>
      <link>https://arxiv.org/abs/2412.20918</link>
      <description><![CDATA[arXiv:2412.20918v1 公告类型：新
摘要：深度神经网络（DNN）通常是在封闭世界假设下构建的，这可能无法推广到分布外（OOD）数据。这导致 DNN 产生过于自信的错误预测，并可能在安全关键应用中造成灾难性的后果。现有的 OOD 检测方法主要依赖于整理一组 OOD 数据进行模型训练或超参数调整，以区分 OOD 数据和训练数据（也称为分布内数据或 InD 数据）。然而，在实际应用中，OOD 样本在训练阶段并不总是可用的，从而阻碍了 OOD 检测的准确性。为了克服这个限制，我们提出了一种基于高斯过程的 OOD 检测方法来仅基于 InD 数据建立决策边界。基本思想是通过多类高斯过程 (GP) 对 DNN 的无约束 softmax 分数进行不确定性量化，然后定义一个分数函数，根据 GP 后验预测分布的根本差异将 InD 和潜在 OOD 数据分开。对传统图像分类数据集和真实世界图像数据集进行了两个案例研究，以证明当在训练阶段未观察到 OOD 样本时，所提出的方法优于最先进的 OOD 检测方法。]]></description>
      <guid>https://arxiv.org/abs/2412.20918</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无监督发现多视图数据中的共享和私有几何</title>
      <link>https://arxiv.org/abs/2408.12091</link>
      <description><![CDATA[arXiv:2408.12091v2 公告类型：交叉 
摘要：现代应用通常利用研究对象的多种视角。在神经科学领域，人们对跨多个大脑区域的大规模同时记录的兴趣日益浓厚。了解视角之间的关系（例如，记录的每个区域的神经活动）可以揭示关于每个表示的特征和系统的基本原理。然而，现有的表征这种关系的方法要么缺乏捕捉复杂非线性所需的表达能力，要么只描述视角之间共享的方差来源，要么丢弃对解释数据至关重要的几何信息。在这里，我们开发了一种基于非线性神经网络的方法，给定高维视角的成对样本，解开这些视角背后的低维共享和私有潜在变量，同时保留内在的数据几何形状。在多个模拟和真实数据集中，我们证明了我们的方法优于竞争方法。使用模拟的外侧膝状体核 (LGN) 和 V1 神经元群，我们展示了我们的模型在不同噪声条件下发现可解释的共享和私有结构的能力。在未旋转和对应但随机旋转的 MNIST 数字数据集上，我们恢复旋转视图的私有潜在变量，无论数字类别如何，这些潜在变量都会编码旋转角度，并将角度表示放在一维流形上，而共享潜在变量编码数字类别但不编码旋转角度。将我们的方法应用于小鼠在线性轨道上奔跑时对海马体和前额叶皮层的同时 Neuropixels 记录，我们发现了一个编码动物位置的低维共享潜在空间。我们提出将我们的方法作为一种通用方法，用于根据解开的共享和私有潜在变量来查找配对数据集的简洁且可解释的描述。]]></description>
      <guid>https://arxiv.org/abs/2408.12091</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应裁剪的 DP-SGD 收敛性</title>
      <link>https://arxiv.org/abs/2412.19916</link>
      <description><![CDATA[arXiv:2412.19916v1 公告类型：交叉 
摘要：带梯度裁剪的随机梯度下降 (SGD) 是一种实现差分隐私优化的强大技术。尽管之前的研究广泛研究了具有恒定阈值的裁剪，但隐私训练仍然对阈值选择高度敏感，这可能代价高昂甚至无法调整。这种敏感性促使开发自适应方法，例如分位数裁剪，这些方法已证明在经验上取得了成功，但缺乏扎实的理论理解。本文首次全面分析了带分位数裁剪的 SGD (QC-SGD)。我们证明 QC-SGD 存在与恒定阈值裁剪 SGD 类似的偏差问题，但展示了如何通过精心设计的分位数和步长计划来缓解这一问题。我们的分析揭示了分位数选择、步长和收敛行为之间的关键关系，为参数选择提供了实用指南。我们将这些结果扩展到差分隐私优化，为 DP-QC-SGD 建立了第一个理论保证。我们的研究结果为广泛使用的自适应裁剪启发式算法提供了理论基础，并指明了未来研究的开放途径。]]></description>
      <guid>https://arxiv.org/abs/2412.19916</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>标准差启发式正则化可提高对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2412.19947</link>
      <description><![CDATA[arXiv:2412.19947v1 公告类型：交叉 
摘要：对抗训练 (AT) 已被证明可以提高深度神经网络 (DNN) 对抗攻击的鲁棒性。AT 是一种最小-最大优化程序，其中生成对抗性示例以训练更强大的 DNN。AT 的内部最大化步骤增加了输入相对于其实际类别的损失。外部最小化涉及最小化从内部最大化获得的对抗性示例的损失。这项工作提出了一个标准偏差启发 (SDI) 正则化项来提高对抗性鲁棒性和泛化能力。我们认为 AT 中的内部最大化类似于最小化模型输出概率的修改标准偏差。此外，我们建议最大化这个修改后的标准偏差可以补充 AT 框架的外部最小化。为了支持我们的论点，我们通过实验表明 SDI 度量可用于制作对抗性示例。此外，我们证明将 SDI 正则化项与现有的 AT 变体相结合可以增强 DNN 抵御更强大的攻击（例如 CW 和自动攻击）的鲁棒性，并提高泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2412.19947</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于统计推断和分布稳健性的去偏非参数回归</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description><![CDATA[arXiv:2412.20173v1 公告类型：交叉 
摘要：本研究提出了一种平滑非参数估计量的去偏方法。虽然随机森林和神经网络等机器学习技术已经表现出强大的预测性能，但它们的理论特性仍然相对未被充分探索。具体来说，许多现代算法缺乏逐点渐近正态性和均匀收敛的保证，这对于协变量偏移下的统计推断和稳健性至关重要，并且已经在 Nadaraya-Watson 回归等经典方法中得到充分证实。为了解决这个问题，我们引入了一种无模型去偏方法，该方法可以保证从任何非参数回归方法得出的平滑估计量的这些特性。通过添加一个校正项来估计原始估计量的条件预期残差，或者等效地估计其估计误差，我们得到了一个具有已证明的逐点渐近正态性、均匀收敛和高斯过程近似的去偏估计量。这些特性使得统计推断成为可能，并增强了对协变量转移的稳健性，使得该方法广泛应用于各种非参数回归问题。]]></description>
      <guid>https://arxiv.org/abs/2412.20173</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>潜在变量模型和正则化回归的精确核心集</title>
      <link>https://arxiv.org/abs/2412.20189</link>
      <description><![CDATA[arXiv:2412.20189v1 公告类型：交叉 
摘要：准确的核心集是原始数据集的加权子集，确保在准确的核心集上训练的模型保持与在完整数据集上训练的模型相同的准确度。这些核心集主要针对有限范围的机器学习模型进行研究。在本文中，我们介绍了一个用于构建准确核心集的统一框架。利用该框架，我们为一般问题提出了准确的核心集构建算法，包括各种潜在变量模型问题和$\ell_p$正则化的$\ell_p$回归。对于潜在变量模型，我们的核心集大小为$O\left(\mathrm{poly}(k)\right)$，其中$k$是潜在变量的数量。对于 $\ell_p$ 正则化的 $\ell_p$ 回归，我们的算法捕捉到了由于正则化而导致的模型复杂度的降低，从而产生了一个核心集，其大小始终小于正则化参数 $\lambda &gt; 0$ 的 $d^{p}$。这里，$d$ 是输入点的维度。这本质上提高了岭回归的精确核心集的大小。我们通过对真实数据集的大量实验评估证实了我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2412.20189</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无限精度 Transformer 的下限</title>
      <link>https://arxiv.org/abs/2412.20195</link>
      <description><![CDATA[arXiv:2412.20195v1 公告类型：交叉 
摘要：在本说明中，我们使用 VC 维度技术来证明针对具有无限精度的单层 softmax 变换器的第一个下界。我们针对两个任务执行此操作：Peng、Narayanan 和 Papadimitriou 考虑的函数组合，以及 Sanford、Hsu 和 Telgarsky 考虑的 SUM$_2$ 任务。]]></description>
      <guid>https://arxiv.org/abs/2412.20195</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>