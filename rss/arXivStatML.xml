<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 03 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过变分自动编码器的不确定性引导微调来增强生成分子设计</title>
      <link>https://arxiv.org/abs/2405.20573</link>
      <description><![CDATA[arXiv:2405.20573v1 公告类型：交叉 
摘要：近年来，深度生成模型已成功应用于各种分子设计任务，特别是在生命和材料科学领域。预训练生成分子设计 (GMD) 模型面临的一个关键挑战是对其进行微调，以更好地适应旨在优化特定分子特性的下游设计任务。然而，为每个新的设计任务从头开始重新设计和训练现有的有效生成模型是不切实际的。此外，典型的下游任务$\unicode{x2013}$（例如属性预测$\unicode{x2013}$）的黑箱性质使得以特定于任务的方式优化生成模型变得并非易事。在这项工作中，我们提出了一种新方法，通过主动学习环境中的性能反馈对基于预训练变分自动编码器 (VAE) 的 GMD 模型进行模型不确定性引导的微调。主要思想是量化生成模型中的模型不确定性，通过在高维 VAE 参数的低维活动子空间内工作来提高生成模型的效率，这些参数解释了模型输出中的大部分变化。模型不确定性的纳入通过解码器多样性扩展了可行分子的空间。然后，我们通过黑盒优化探索由此产生的模型不确定性类，而活动子空间的低维性使黑盒优化变得易于处理。这使我们能够识别和利用一组多样化的高性能模型来生成增强的分子。使用多个基于 VAE 的生成模型对六个目标分子特性的实证结果表明，我们的不确定性引导微调方法始终优于原始预训练模型。]]></description>
      <guid>https://arxiv.org/abs/2405.20573</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>委托代理多任务：最优契约的一致性及其通过工具回归的有效学习</title>
      <link>https://arxiv.org/abs/2405.20642</link>
      <description><![CDATA[arXiv:2405.20642v1 公告类型：交叉 
摘要：本研究研究多任务委托代理问题。我首先展示了一个“一致性”结果。具体来说，当任务是完全替代的，并且代理的成本函数在一定程度上是同质的，那么最优合同仅取决于每个任务的边际效用和同质程度。然后，我研究了一种设置，其中每个任务的边际效用都是未知的，因此必须使用观察数据来学习或估计最优合同。我将这个问题确定为具有测量误差的回归问题，并观察到这个问题可以转化为工具回归问题。当前的研究观察到合同和重复观察（当可用时）都可以作为有效的工具变量，并建议使用广义矩估计量从离线数据计算近似最优合同。我还研究了一个在线设置，并展示了如何使用这两个估计量以在线方式有效地学习最优合同。在这里，委托人面临着探索-利用权衡：她必须尝试新的合同并观察其结果，同时确保她的实验不会偏离最佳合同太多。这项工作表明，当有重复观察可用且代理足够“多样化”时，委托人可以实现非常低的$ \ widetilde {O}（d）$累积效用损失，即使使用“纯利用”算法也是如此。]]></description>
      <guid>https://arxiv.org/abs/2405.20642</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>非负矩阵分解与潜在狄利克雷分配之间的联系</title>
      <link>https://arxiv.org/abs/2405.20542</link>
      <description><![CDATA[arXiv:2405.20542v1 公告类型：交叉 
摘要：具有广义 Kullback-Leibler 散度的非负矩阵分解 (NMF) 和潜在狄利克雷分配 (LDA) 是两种流行的非负数据降维方法。在这里，我们表明，在分解的两个矩阵的列上具有 $\ell_1$ 正则化约束，并且在一个矩阵的列上具有狄利克雷先验的 NMF 等同于 LDA。为了证明这一点，我们证明，通过在优化问题中添加 $\ell_1$ 正则化约束来明确考虑 NMF 的缩放模糊性，允许在广泛使用的乘法更新 (MU) 算法中联合更新两个矩阵。当两个矩阵都被正则化时，联合 MU 算法会导致概率潜在语义分析 (PLSA)，它是没有狄利克雷先验的 LDA。我们推导 NMF 联合更新的方法还表明，一个矩阵上的 Lasso 惩罚与另一个矩阵上的 $\ell_1$ 正则化约束不足以引起任何稀疏性。]]></description>
      <guid>https://arxiv.org/abs/2405.20542</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>深度学习的不确定性量化</title>
      <link>https://arxiv.org/abs/2405.20550</link>
      <description><![CDATA[arXiv:2405.20550v1 公告类型：交叉 
摘要：提供了深度学习的完整且统计一致的不确定性量化，包括来自（1）新输入数据、（2）训练和测试数据（3）神经网络的权重向量和（4）神经网络的不确定性来源，因为它不是一个完美的预测器。使用贝叶斯定理和条件概率密度，我们展示了如何系统地量化每个不确定性来源。我们还首次介绍了一种快速实用的方法来整合和组合所有错误来源。为了说明，新方法用于量化云自动转换率中的误差，该误差由人工神经网络预测，该人工神经网络由亚速尔群岛的飞机云探测器测量和以二矩箱模型表示的随机收集方程训练而成。对于这个特定的例子，由训练和测试数据的不确定性引起的输出不确定性占主导地位，其次是输入数据中的不确定性、训练后的神经网络中的不确定性以及权重中的不确定性。我们讨论了该方法对于机器学习实践的实用性，以及如何通过在训练数据中包含不确定性，新方法对训练数据集之外的输入数据不太敏感。]]></description>
      <guid>https://arxiv.org/abs/2405.20550</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>利用环境之间的结构：系统发育正则化激励解开表征</title>
      <link>https://arxiv.org/abs/2405.20482</link>
      <description><![CDATA[arXiv:2405.20482v1 公告类型：交叉 
摘要：许多因果系统（例如细胞中的生物过程）只能通过测量（例如基因表达）间接观察到。因果表示学习——将低级观察正确映射到潜在因果变量的任务——可以通过推断潜在变量（例如通路激活）来促进科学理解。在本文中，我们开发了从多个相关数据集（环境）和任务推断潜在变量的方法。作为一个运行示例，我们考虑从基因表达预测表型的任务，其中我们经常从以已知方式相关的多种细胞类型或生物体中收集数据。关键见解是，从基因表达驱动的潜在变量到感兴趣的表型的映射在密切相关的环境中稀疏变化。为了对稀疏变化进行建模，我们引入了基于树的正则化（TBR），这是一个最小化预测误差并规范密切相关环境以学习相似预测因子的目标。我们证明，在假设稀疏变化程度的情况下，TBR 可以识别出真正的潜在变量，最多可进行一些简单的变换。我们通过模拟和真实基因表达数据对该理论进行了实证评估。我们发现，在这些设置中，即使在违反理论某些假设的设置下，TBR 也能比相关方法更好地恢复潜在因果变量。]]></description>
      <guid>https://arxiv.org/abs/2405.20482</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>完全不受约束的在线学习</title>
      <link>https://arxiv.org/abs/2405.20540</link>
      <description><![CDATA[arXiv:2405.20540v1 公告类型：交叉 
摘要：我们提供了一种在线学习算法，该算法在不知道 $G$ 或 $\|w_\star\|$ 的情况下，针对任何比较点 $w_\star$ 获得 $G$-Lipschitz 凸损失上的遗憾 $G\|w_\star\|\sqrt{T\log(\|w_\star\|G\sqrt{T})} + \|w_\star\|^2 + G^2$。重要的是，这与具有此类知识（最多对数因子）的最佳界限 $G\|w_\star\|\sqrt{T}$ 相匹配，除非 $\|w_\star\|$ 或 $G$ 太大，以至于即使 $G\|w_\star\|\sqrt{T}$ 在 $T$ 中也大致是线性的。因此，它与所有可以实现亚线性遗憾的情况的最佳界限相匹配，这可以说是最“有趣”的场景。]]></description>
      <guid>https://arxiv.org/abs/2405.20540</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>深度学习计算马尔可夫链的收敛速度</title>
      <link>https://arxiv.org/abs/2405.20435</link>
      <description><![CDATA[arXiv:2405.20435v1 公告类型：交叉 
摘要：一般状态空间马尔可夫链的收敛速度分析在马尔可夫链蒙特卡罗和算法分析（用于计算显式收敛界限）等领域具有根本重要性。然而，这个问题非常困难，因为传统的分析方法通常不会为现实的马尔可夫链生成实用的收敛界限。我们提出了深度收缩漂移计算器 (DCDC)，这是第一个基于样本的通用算法，用于将马尔可夫链的收敛限制在 Wasserstein 距离的平稳性范围内。DCDC 有两个组成部分。首先，受 (Qu et.al, 2023) 中新的收敛分析框架的启发，我们引入了收缩漂移方程 (CDE)，其解可得出显式收敛界限。其次，我们开发了一个高效的基于神经网络的 CDE 求解器。有了这两个组件，DCDC 可以解决 CDE 并将解决方案转换为收敛界限。我们分析了算法的样本复杂度，并通过为随机处理网络以及恒定步长随机优化中出现的实际马尔可夫链生成收敛界限，进一步证明了 DCDC 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.20435</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>使用信息度量理解机器学习中的编码器-解码器结构</title>
      <link>https://arxiv.org/abs/2405.20452</link>
      <description><![CDATA[arXiv:2405.20452v1 公告类型：交叉 
摘要：我们从信息论的角度提出了新的结果来建模和理解编码器-解码器设计在机器学习 (ML) 中的作用。我们使用两个主要的信息概念，信息充分性 (IS) 和相互信息丢失 (MIL)，来表示机器学习中的预测结构。我们的第一个主要结果提供了一个函数表达式，它描述了与 IS 编码器-解码器潜在预测结构一致的概率模型类。这个结果正式证明了许多现代 ML 架构采用的编码器-解码器前向阶段，以学习潜在 (压缩) 表示进行分类。为了说明 IS 是一个现实且相关的模型假设，我们重新审视了一些已知的 ML 概念并提出了一些有趣的新示例：不变、稳健、稀疏和数字模型。此外，我们的 IS 表征使我们能够使用交叉熵风险来解决基本问题，即当在学习环境中采用给定的编码器-解码器架构时，可能会损失多少性能（预测表达能力）。在这里，我们的第二个主要结果表明，互信息损失量化了由于选择（有偏差的）编码器-解码器 ML 设计而导致的表达能力不足。最后，我们通过编码器-解码器设计解决了通用交叉熵学习的问题，其中建立了必要和充分条件来满足这一要求。在所有这些结果中，香农的信息度量为表征学习提供了新的解释和说明。]]></description>
      <guid>https://arxiv.org/abs/2405.20452</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>李群动量优化器的定量收敛</title>
      <link>https://arxiv.org/abs/2405.20390</link>
      <description><![CDATA[arXiv:2405.20390v1 公告类型：交叉 
摘要：通过变分优化和动量平凡化，可以构造显式的、基于动量的动力学来优化在李群上定义的函数。结构保持时间离散化可以将这种动力学转化为优化算法。本文研究了两种类型的离散化，一种是已知的分裂方案 Lie Heavy-Ball，另一种是新提出的 Lie NAG-SC。它们的收敛速度在 $L$ 平滑度和局部强凸性假设下明确量化。Lie NAG-SC 在无动量情况下提供加速，即黎曼梯度下降，但 Lie Heavy-Ball 没有。与现有的一般流形加速优化器相比，由于利用了群结构，Lie Heavy-Ball 和 Lie NAG-SC 在计算上更便宜且更容易实现。仅需要梯度预言机和指数映射，而不需要计算成本高的对数映射或并行传输。]]></description>
      <guid>https://arxiv.org/abs/2405.20390</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>通过聚类网络信息准则 (NICc) 进行快速留一聚类交叉验证</title>
      <link>https://arxiv.org/abs/2405.20400</link>
      <description><![CDATA[arXiv:2405.20400v1 公告类型：交叉 
摘要：本文引入了网络信息准则（NICc）的聚类估计量来近似留一法交叉验证偏差，在对聚类数据进行建模时，可以将其用作基于聚类的交叉验证的替代方法。Stone 证明了，如果参数模型成立，则赤池信息准则（AIC）与留一法交叉验证渐近等价。Ripley 指出，当模型不成立时，Stone 证明中得出的网络信息准则（NIC）是留一法交叉验证的更好近似值。对于聚类数据，我们通过将 NIC 中的 Fisher 信息矩阵替换为针对聚类进行调整的估计量，得出了 NIC 的聚类估计量，称为 NICc。这种调整在对聚类数据进行建模时，对 NICc 施加的惩罚比 NIC 的非聚类估计量更大，从而更有效地防止过度拟合。在一项模拟研究和一个实证示例中，我们分别使用线性和逻辑回归对具有高斯或二项式响应的聚类数据进行建模。我们表明，NICc 比 AIC 和贝叶斯信息准则 (BIC) 更能近似留一聚类偏差，并能更有效地防止过度拟合。与 AIC 和 BIC 相比，NICc 可实现更准确的模型选择，这由基于聚类的交叉验证确定。]]></description>
      <guid>https://arxiv.org/abs/2405.20400</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>基于个人级别差分隐私的隐私均值估计</title>
      <link>https://arxiv.org/abs/2405.20405</link>
      <description><![CDATA[arXiv:2405.20405v1 公告类型：交叉 
摘要：我们研究每个人持有多个样本的情况下的差分隐私 (DP) 均值估计。通常称为“用户级”设置，这里的 DP 需要通常的分布稳定性概念，即当一个人的所有数据点都可以修改时。非正式地，如果 $n$ 个人各自有 $m$ 个样本，这些样本来自未知的 $d$ 维分布，且 $k$ 个矩有界，则我们表明
\[n = \tilde \Theta\left(\frac{d}{\alpha^2 m} + \frac{d }{ \alpha m^{1/2} \varepsilon} + \frac{d}{\alpha^{k/(k-1)} m \varepsilon} + \frac{d}{\varepsilon}\right)\]
个人对于在 $\varepsilon$-差分隐私（及其常见放宽）下估计 $\ell_2$-norm 中距离 $\alpha$ 的平均值是必要且充分的。在多变量设置中，我们给出了近似 DP（样本复杂度略有降低）下的计算效率高的算法和纯 DP 下的计算效率低下的算法，并且我们的几乎匹配的下限适用于最宽松的近似 DP 情况。我们的计算效率估计量基于众所周知的噪声截断均值方法，但我们设置的分析需要对独立的、向量值、有界矩随机变量和的尾部进行新的界限，以及对由截断引入的偏差进行界限的新论证。]]></description>
      <guid>https://arxiv.org/abs/2405.20405</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>Rough Transformers：具有路径签名的轻量级连续时间序列建模</title>
      <link>https://arxiv.org/abs/2405.20799</link>
      <description><![CDATA[arXiv:2405.20799v1 公告类型：新
摘要：现实世界中的时间序列数据通常表现出长距离依赖性，并且以非均匀间隔观察。在这些设置中，传统的基于序列的循环模型会遇到困难。为了克服这个问题，研究人员经常用基于神经 ODE 的模型替换循环架构来解释不规则采样的数据，并使用基于 Transformer 的架构来解释长距离依赖性。尽管这两种方法都取得了成功，但对于即使是中等长度的输入序列，这两种方法都会产生非常高的计算成本。为了应对这一挑战，我们引入了 Rough Transformer，这是 Transformer 模型的一种变体，它对输入序列的连续时间表示进行操作，并且计算成本显著降低。特别是，我们提出了 \textit{多视图签名注意}，它使用路径签名来增强 vanilla 注意力并捕获输入数据中的局部和全局（多尺度）依赖关系，同时保持对序列长度和采样频率变化的鲁棒性并产生改进的空间处理。我们发现，在各种与时间序列相关的任务中，Rough Transformers 的表现始终优于普通注意力模型，同时获得了基于神经 ODE 模型的表征优势，而所有这些都只花费了计算时间和内存资源的一小部分。]]></description>
      <guid>https://arxiv.org/abs/2405.20799</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>PUAL：三叉正无标记数据分类器</title>
      <link>https://arxiv.org/abs/2405.20970</link>
      <description><![CDATA[arXiv:2405.20970v1 公告类型：新
摘要：正无标记（PU）学习旨在使用仅包含标记正实例和未标记实例的数据来训练分类器。然而，现有的 PU 学习方法通​​常很难在三叉数据上取得令人满意的性能，其中正实例分布在负实例的两侧。为了解决这个问题，首先我们提出了一种具有不对称损失的 PU 分类器（PUAL），通过在全局和局部学习分类器的目标函数中引入对正实例的不对称损失结构。然后我们开发了一种基于核的算法，使 PUAL 能够获得非线性决策边界。我们表明，通过在模拟和真实数据集上的实验，PUAL 可以在三叉数据上实现令人满意的分类。]]></description>
      <guid>https://arxiv.org/abs/2405.20970</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>稳健满意的统计特性</title>
      <link>https://arxiv.org/abs/2405.20451</link>
      <description><![CDATA[arXiv:2405.20451v1 公告类型：新
摘要：稳健满意 (RS) 模型是一种新兴的稳健优化方法，可提供简化的程序和跨各种应用的稳健泛化。然而，RS 的统计理论在文献中仍未被探索。本文通过全面分析 RS 模型的理论特性填补了这一空白。值得注意的是，与开创性的分布稳健优化 (DRO) 相比，RS 结构提供了一条更直接的获取统计保证的途径，从而产生了更丰富的结果。特别是，我们为最佳损失建立了双侧置信区间，而无需明确解决极小极大优化问题。我们进一步为 RS 优化器提供了有限样本泛化误差界限。重要的是，我们的结果扩展到涉及分布偏移的场景，其中采样和目标分布之间存在差异。我们的数值实验表明，RS 模型在小样本制度和分布偏移下始终优于基线经验风险最小化。此外，与 DRO 模型相比，RS 模型对超参数调整的敏感度较低，凸显了其在鲁棒性考虑方面的实用性。]]></description>
      <guid>https://arxiv.org/abs/2405.20451</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>执行性政策学习中的算法公平性：摆脱群体公平的不可能</title>
      <link>https://arxiv.org/abs/2405.20447</link>
      <description><![CDATA[arXiv:2405.20447v1 公告类型：新
摘要：在许多预测问题中，预测模型会影响预测目标的分布。这种现象被称为表演性，通常是由对预测模型结果有既得利益的个人的行为引起的。尽管表演性通常是有问题的，因为它表现为分布变化，但我们开发了算法公平实践，利用表演性在社会分类问题中实现更强的群体公平保证（与非表演性环境中可实现的相比）。特别是，我们利用政策制定者引导民众的能力来长期纠正不平等。这种方法的一个关键好处是可以解决相互冲突的群体公平定义之间的不兼容性。]]></description>
      <guid>https://arxiv.org/abs/2405.20447</guid>
      <pubDate>Tue, 04 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    </channel>
</rss>