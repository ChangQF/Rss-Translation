<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 26 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>无自由修剪：初始化时修剪的信息论障碍</title>
      <link>https://arxiv.org/abs/2402.01089</link>
      <description><![CDATA[arXiv:2402.01089v2 公告类型：替换 
摘要：初始化时或初始化附近“彩票”的存在提出了一个诱人的问题：深度学习是否需要大型模型，或者是否可以快速识别和训练稀疏网络而无需训练包含它们的密集模型。然而，在不训练密集模型（“初始化时修剪”）的情况下寻找这些稀疏子网络的努力基本上没有成功 arXiv:2009.08576。我们对此提出了一个理论解释，基于模型的有效参数计数 $p_\text{eff}$，由最终网络中非零权重的数量和稀疏掩码与数据之间的互信息之和给出。我们证明了 arXiv:2105.12806 的稳健性定律扩展到稀疏网络，其中通常的参数数量被 $p_\text{eff}$ 取代，这意味着一个可以稳健地插入噪声数据的稀疏神经网络需要一个高度依赖数据的掩码。我们假设在训练期间和训练之后进行修剪会输出具有比初始化时修剪产生的掩码更高的互信息掩码。因此，两个网络可能具有相同的稀疏性，但根据它们的训练方式，有效参数数量会有所不同。这表明在初始化附近进行修剪可能是不可行的，并解释了为什么彩票存在，但无法快速找到（即没有训练整个网络）。神经网络上的实验证实，训练期间获得的信息确实会影响模型容量。]]></description>
      <guid>https://arxiv.org/abs/2402.01089</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:06 GMT</pubDate>
    </item>
    <item>
      <title>使用最小生成树进行聚类：它有多好？</title>
      <link>https://arxiv.org/abs/2303.05679</link>
      <description><![CDATA[arXiv:2303.05679v3 公告类型：替换 
摘要：最小生成树 (MST) 为众多模式识别活动提供了一种方便的数据集表示。此外，它们的计算速度相对较快。在本文中，我们量化了它们在低维分区数据聚类任务中的意义。通过从大量基准数据中确定最佳 (oracle) 算法与专家标签之间的一致性上限，我们发现 MST 方法非常具有竞争力。接下来，我们回顾、研究、扩展和概括一些现有的、最先进的基于 MST 的分区方案。这导致了一些值得注意的新方法。总体而言，Genie 和信息论方法通常优于非 MST 算法，例如 K 均值、高斯混合、谱聚类、Birch、基于密度和经典的分层凝聚程序。尽管如此，我们发现仍有一些改进的空间，因此鼓励开发新的算法。]]></description>
      <guid>https://arxiv.org/abs/2303.05679</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:05 GMT</pubDate>
    </item>
    <item>
      <title>基于 Voronoi 分区的 Wasserstein 近似方案</title>
      <link>https://arxiv.org/abs/2310.09149</link>
      <description><![CDATA[arXiv:2310.09149v2 公告类型：替换 
摘要：我们考虑在 Wasserstein 空间 $\mathrm{W}_p(\mathbb{R}^d)$ 中对 $p\in[1,\infty)$ 进行测度的结构化近似，使用在从 $\mathbb{R}^d$ 的缩放 Voronoi 分区派生的 Voronoi 区域上紧支持的一般测度近似值。我们证明，如果将满秩格 $\Lambda$ 按 $h\in(0,1]$ 的因子缩放，则基于 $h\Lambda$ 的 Voronoi 分区的测度近似值为 $O(h)$，与 $d$ 或 $p$ 无关。然后，我们使用覆盖论证来证明紧支撑测度的 $N$ 项近似值为 $O(N^{-\frac1d})$，在大多数情况下，这与已知的最佳量化器速率和经验测度近似相匹配。此外，我们将我们的构造推广到非均匀 Voronoi 分区，突出显示了我们的方法对于各种测度近似场景的灵活性和稳健性。最后，我们将这些结果扩展到具有足够衰减的非紧支撑测度。我们的发现与计算机视觉和机器学习中的应用有关，其中测度用于表示图像等结构化数据。]]></description>
      <guid>https://arxiv.org/abs/2310.09149</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:05 GMT</pubDate>
    </item>
    <item>
      <title>针对具有稀有潜在状态的隐马尔可夫模型的目标随机梯度马尔可夫链蒙特卡罗</title>
      <link>https://arxiv.org/abs/1810.13431</link>
      <description><![CDATA[arXiv:1810.13431v3 公告类型：替换 
摘要：隐马尔可夫模型的马尔可夫链蒙特卡罗 (MCMC) 算法通常依赖于前向-后向采样器。这使得它们随着时间序列长度的增加而计算速度变慢，从而推动了基于子采样的方法的发展。这些通过在随机梯度 MCMC 内的每个 MCMC 迭代中使用数据的小随机子序列来近似完整的后验。在存在由罕见潜在状态导致的不平衡数据的情况下，子序列通常会排除罕见潜在状态数据，从而导致对罕见事件的推断和预测/检测不准确。我们提出了一种有针对性的子采样 (TASS) 方法，该方法在计算与罕见潜在状态相关的参数的随机梯度时对与罕见潜在状态相对应的观测值进行过度采样。TASS 使用数据的初始聚类来构建子序列权重，以减少梯度估计中的方差。这可以提高采样效率，特别是在罕见潜在状态对应于极端观察的情况下。我们展示了真实和合成示例的预测和推理准确性的显著提高。]]></description>
      <guid>https://arxiv.org/abs/1810.13431</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:04 GMT</pubDate>
    </item>
    <item>
      <title>自然梯度混合变分推理及其在深度混合模型中的应用</title>
      <link>https://arxiv.org/abs/2302.13536</link>
      <description><![CDATA[arXiv:2302.13536v2 公告类型：替换 
摘要：具有全局参数和潜在变量的随机模型很常见，变分推理 (VI) 很受欢迎。然而，现有的方法在高维中通常要么速度慢要么不准确。我们建议针对这种情况采用一种快速而准确的 VI 方法，该方法采用明确定义的自然梯度变分优化，以全局参数和潜在变量的联合后验为目标。这是一种混合方法，其中在每个步骤中都使用自然梯度更新全局参数，并从它们的条件后验生成潜在变量。使用 Tikhonov 阻尼 Fisher 信息矩阵的快速计算表达式以及重新参数化技巧来提供稳定的自然梯度。我们将该方法应用于深度混合模型，这是一种新兴的贝叶斯神经网络，具有随机输出层系数以允许异质性。一系列模拟表明，使用自然梯度比使用普通梯度效率高得多，并且该方法比两种尖端的自然梯度 VI 方法更快、更准确。在金融应用中，我们表明，使用深度混合模型考虑行业层面的异质性可以提高资产定价模型的准确性。实现该方法的 MATLAB 代码可在以下位置找到：https://github.com/WeibenZhang07/NG-HVI。]]></description>
      <guid>https://arxiv.org/abs/2302.13536</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:04 GMT</pubDate>
    </item>
    <item>
      <title>使用生成神经网络进行双重稳健条件独立性测试</title>
      <link>https://arxiv.org/abs/2407.17694</link>
      <description><![CDATA[arXiv:2407.17694v1 公告类型：交叉 
摘要：本文解决了在给定第三个随机向量 $Z$ 的情况下测试两个通用随机向量 $X$ 和 $Y$ 的条件独立性的问题，这在统计和机器学习应用中起着重要作用。我们提出了一种新的非参数测试程序，该程序避免明确估计任何条件分布，而是需要从给定 $Z$ 的两个边际条件分布 $X$ 和给定 $Z$ 的 $Y$ 中进行抽样。我们进一步建议使用生成神经网络 (GNN) 框架从这些近似的边际条件分布中进行抽样，这往往会减轻维数灾难，因为它可以适应任何低维结构和数据底层的平滑度。从理论上讲，我们的检验统计量具有双重稳健性，可以抵抗 GNN 近似误差，这意味着只要两个近似误差的乘积衰减到零的速度快于参数速率，检验统计量就会保留使用真实边际条件分布的 oracle 检验统计量的所有理想属性。我们的统计量的渐近性质和 bootstrap 程序的一致性是在零和局部替代方案下推导出来的。大量的数值实验和真实数据分析证明了我们提出的测试的有效性和广泛的适用性。]]></description>
      <guid>https://arxiv.org/abs/2407.17694</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:03 GMT</pubDate>
    </item>
    <item>
      <title>单标签多类分类任务概率评估的卓越评分规则</title>
      <link>https://arxiv.org/abs/2407.17697</link>
      <description><![CDATA[arXiv:2407.17697v1 公告类型：交叉 
摘要：本研究引入了新的优越评分规则，称为惩罚性 Brier 分数 (PBS) 和惩罚性对数损失 (PLL)，以改进概率分类的模型评估。传统的评分规则，如 Brier 分数和对数损失，有时会为错误分类分配比正确分类更好的分数。这种与实际奖励正确分类的偏好不一致可能导致模型选择不理想。通过整合对错误分类的惩罚，PBS 和 PLL 修改了传统的正确评分规则，以始终如一地为正确的预测分配更好的分数。正式证明表明，PBS 和 PLL 满足严格正确的评分规则属性，同时也优先奖励准确的分类。实验展示了使用 PBS 和 PLL 进行模型选择、模型检查点和早期停止的好处。与训练期间的 Brier 分数相比，PBS 与 F1 分数表现出更高的负相关性。因此，PBS 可以更有效地识别最佳检查点和早期停止点，从而提高 F1 分数。比较分析验证了 PBS 和 PLL 选择的模型可实现更高的 F1 分数。因此，PBS 和 PLL 通过整合适当的评分原则和对真实分类的明确偏好来解决不确定性量化和准确度最大化之间的差距。所提出的指标可以增强模型评估和选择，以实现可靠的概率分类。]]></description>
      <guid>https://arxiv.org/abs/2407.17697</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:03 GMT</pubDate>
    </item>
    <item>
      <title>统计最优传输</title>
      <link>https://arxiv.org/abs/2407.18163</link>
      <description><![CDATA[arXiv:2407.18163v1 公告类型：交叉 
摘要：我们根据圣弗卢尔概率学院 XLIX 的讲座，介绍统计最优传输领域。]]></description>
      <guid>https://arxiv.org/abs/2407.18163</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:03 GMT</pubDate>
    </item>
    <item>
      <title>用于模拟美国陆军车辆故障的生成学习</title>
      <link>https://arxiv.org/abs/2407.17654</link>
      <description><![CDATA[arXiv:2407.17654v1 公告类型：交叉 
摘要：我们开发了一种新颖的生成模型，以模拟车辆健康状况并预测故障，并考虑到实际操作因素。该模型基于美国陆军预测物流计划的数据进行训练，旨在支持预测性维护。它提前预测故障，以便在故障发生之前执行维护干预。该模型结合了影响车辆健康的现实因素。它还使我们能够通过分析运行数据并将每辆车描述为离散状态来了解车辆的状况。重要的是，该模型可以高精度地预测首次故障时间。我们将其性能与其他模型进行了比较，并展示了其成功的训练。]]></description>
      <guid>https://arxiv.org/abs/2407.17654</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:02 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫数据上的 Transformer：恒定深度就足够了</title>
      <link>https://arxiv.org/abs/2407.17686</link>
      <description><![CDATA[arXiv:2407.17686v1 公告类型：交叉 
摘要：基于注意力机制的 Transformer 在跨各种领域和模式的生成过程建模方面取得了显著成功。在本文中，我们研究了 Transformer 对来自 \kth 马尔可夫过程的数据的行为，其中序列中下一个符号的条件分布取决于之前观察到的 $k$ 个符号。我们通过经验观察到一个令人惊讶的现象，这与之前的发现相矛盾：经过足够长时间的训练后，具有固定深度和每层 $1$ 个头的 Transformer 能够在来自 \kth 马尔可夫源的序列上实现较低的测试损失，即使 $k$ 增长也是如此。此外，这种低测试损失是通过 Transformer 表示和学习上下文条件经验分布的能力实现的。在理论方面，我们的主要结果是，具有单个头部和三层的 Transformer 可以表示 \kth 马尔可夫源的上下文条件经验分布，这与我们的经验观察一致。在此过程中，我们证明具有 $O(\log_2(k))$ 层的 \textit{attention-only} Transformer 可以通过组合感应头部来跟踪序列中的前 $k$ 个符号来表示上下文条件经验分布。通过了解 Transformer 在马尔可夫源上的行为，这些结果为我们目前对 Transformer 学习捕获上下文的机制的理解提供了更多见解。]]></description>
      <guid>https://arxiv.org/abs/2407.17686</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:02 GMT</pubDate>
    </item>
    <item>
      <title>将标记解锁为更大语言模型的泛化界限的数据点</title>
      <link>https://arxiv.org/abs/2407.18158</link>
      <description><![CDATA[arXiv:2407.18158v1 公告类型：新
摘要：具有数十亿个参数的大型语言模型 (LLM) 擅长预测序列中的下一个标记。最近的工作为 LLM 计算了基于非空压缩的泛化界限，但这些界限对于十亿参数规模的大型模型来说是空洞的。此外，这些界限是通过限制性压缩技术获得的，限制了生成低质量文本的压缩模型。此外，这些现有界限的紧密性取决于训练集中 IID 文档的数量，而不是大量的非 IID 组成标记，留下了未开发的更紧密界限的潜力。在这项工作中，我们使用马丁格尔的属性来推导受益于 LLM 训练集中大量标记的泛化界限。由于数据集包含的标记远多于文档，我们的泛化界限不仅可以容忍而且实际上受益于限制性远低于的压缩方案。利用君主矩阵、克罗内克分解和训练后量化，我们实现了 LLaMA2-70B 等规模的 LLM 的非空泛化界限。与之前的方法不同，我们的工作实现了在实践中部署并生成高质量文本的模型的第一个非空界限。]]></description>
      <guid>https://arxiv.org/abs/2407.18158</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的量子优势与快速再训练的火花</title>
      <link>https://arxiv.org/abs/2407.16020</link>
      <description><![CDATA[arXiv:2407.16020v2 公告类型：交叉 
摘要：量子计算的出现有可能通过比传统计算机更有效地解决复杂问题来彻底改变各个领域。尽管前景光明，但实际的量子优势受到当前硬件限制的阻碍，特别是量子比特数量少和噪声水平高。在本研究中，我们利用绝热量子计算机来优化 Kolmogorov-Arnold 网络，这是一种强大的神经网络架构，用于以最少的参数表示复杂函数。通过修改网络以使用贝塞尔曲线作为基函数并将优化问题公式化为二次无约束二元优化问题，我们创建了一个固定大小的解空间，与训练样本的数量无关。与 Adam、随机梯度下降、自适应梯度和模拟退火等经典优化器相比，我们的方法通过更快的训练时间展示了量子优势的火花。此外，我们引入了一种新颖的快速再训练功能，使网络能够使用新数据进行再训练，而无需重新处理旧样本，从而提高动态环境中的学习效率。分类和回归任务的初始训练实验结果验证了我们方法的有效性，展示了显著的加速和与传统方法相当的性能。再训练实验表明，与基于梯度下降的优化器相比，使用基于绝热量子计算的优化可以提高 60 倍的速度，而理论模型允许这种加速甚至更大！我们的研究结果表明，随着量子硬件和算法优化的进一步发展，量子优化的机器学习模型可以在各个领域得到广泛的应用，最初的重点是快速再训练。]]></description>
      <guid>https://arxiv.org/abs/2407.16020</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>基于行动阶段聚类的驾驶模式解释</title>
      <link>https://arxiv.org/abs/2407.17518</link>
      <description><![CDATA[arXiv:2407.17518v1 公告类型：交叉 
摘要：当前识别驾驶异质性的方法面临着从潜在驾驶行为机制的角度理解基本模式的挑战。我们之前的工作提出了行动阶段的概念，它捕捉了具有物理意义的驾驶特征的多样性。本研究提出了一个新颖的框架，通过无监督的方式对行动阶段进行分类，进一步解释驾驶模式。在这个框架中，首先应用重采样和下采样方法 (RDM) 来标准化行动阶段的长度。然后迭代应用聚类校准程序，包括“特征选择”、“聚类分析”、“差异/相似性评估”和“行动阶段重新提取”，直到所有聚类之间的差异和聚类内的相似性都达到预定标准。将该框架应用于真实世界数据集后，发现 I80 数据集中有六种驾驶模式，分别标记为“追赶”、“保持距离”和“保持距离”，同时具有“稳定”和“不稳定”状态。值得注意的是，不稳定模式比稳定模式更多。在稳定模式中，“保持距离”是最常见的。这些观察结果与驾驶的动态性质相符。US101 数据集中缺少两种模式“稳定保持距离”和“不稳定追赶”，这符合我们的预期，因为之前显示该数据集的异质性较低。这表明驾驶模式在描述驾驶异质性方面的潜力。所提出的框架有望在解决监督学习中的标签稀缺问题和增强驾驶行为建模和驾驶轨迹预测等任务方面发挥优势。]]></description>
      <guid>https://arxiv.org/abs/2407.17518</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:01 GMT</pubDate>
    </item>
    <item>
      <title>空间或时空干扰下的离策略评估的因果深度集</title>
      <link>https://arxiv.org/abs/2407.17910</link>
      <description><![CDATA[arXiv:2407.17910v1 公告类型：新
摘要：离策略评估 (OPE) 广泛应用于制药和电子商务等领域，用于评估离线数据集中新产品或政策的有效性。本文介绍了一个因果深度框架，该框架放宽了几个关键的结构假设，主要是平均场假设，这些假设在处理时空干扰的现有 OPE 方法中很普遍。这些传统假设在现实环境中经常被证明是不充分的，从而限制了当前 OPE 方法有效解决复杂干扰影响的能力。作为回应，我们提倡实施置换不变性 (PI) 假设。这种创新方法实现了数据驱动的、自适应的平均场函数学习，提供了一种比传统平均更灵活的估计方法。此外，我们提出了将 PI 假设纳入 OPE 的新算法，并彻底检查了它们的理论基础。我们的数值分析表明，这种新方法比现有的基线算法能产生更精确的估计，从而大大提高 OPE 方法的实际适用性和有效性。我们提出的方法的 Python 实现可在 https://github.com/BIG-S2/Causal-Deepsets 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.17910</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:00 GMT</pubDate>
    </item>
    <item>
      <title>对数 Sobolev 不等式下期望最大化算法的快速收敛</title>
      <link>https://arxiv.org/abs/2407.17949</link>
      <description><![CDATA[arXiv:2407.17949v1 公告类型：新
摘要：通过利用最近开发的在 Wasserstein 空间上构建梯度流的工具，我们将一种常用于理解欧几里得空间上交替最小化算法的分析技术扩展到期望最大化 (EM) 算法，通过将其表示为欧几里得空间和概率分布空间乘积上的坐标最小化，这是 Neal 和 Hinton (1998) 提出的。通过这样做，我们在对数 Sobolev 不等式的自然推广下获得了 EM 算法的有限样本误差界限和指数收敛。我们进一步证明，该分析技术足够灵活，也可以分析 EM 算法的几种变体。]]></description>
      <guid>https://arxiv.org/abs/2407.17949</guid>
      <pubDate>Fri, 26 Jul 2024 06:21:00 GMT</pubDate>
    </item>
    </channel>
</rss>