<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>具有非参数提示的大型语言模型的排名</title>
      <link>https://arxiv.org/abs/2412.05506</link>
      <description><![CDATA[arXiv:2412.05506v1 公告类型：新
摘要：我们考虑对大型语言模型 (LLM) 进行排名的推断。对齐是缓解使用 LLM 时出现的幻觉的一大挑战。排名 LLM 已被证明是一种基于最佳 $N$ 策略改进对齐的良好工具。在本文中，我们提出了一个新的推理框架，用于检验假设并构建语言模型排名的置信区间。我们考虑广泛采用的 Bradley-Terry-Luce (BTL) 模型，其中每个项目都被分配一个正偏好分数，该分数决定了其成对比较的结果。我们进一步将其扩展到上下文设置中，其中每个模型的分数随提示而变化。我们展示了我们的估计器的收敛速度。通过扩展当前的高斯乘数引导理论以适应非同分布经验过程的上确界，我们构建了排序的置信区间并提出了有效的测试程序。我们还引入了置信图作为全局排序属性。我们进行了数值实验来评估我们方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.05506</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型低秩适配器的免训练贝叶斯化</title>
      <link>https://arxiv.org/abs/2412.05723</link>
      <description><![CDATA[arXiv:2412.05723v1 公告类型：新 
摘要：估计大型语言模型（LLM）响应的不确定性仍然是一个关键挑战。虽然最近的贝叶斯方法已经证明了通过低秩权重更新量化不​​确定性的有效性，但它们通常需要复杂的微调或训练后程序。在本文中，我们提出了无需训练的贝叶斯化（TFB），这是一种新颖的框架，可将现有的现成的经过训练的 LoRA 适配器转换为贝叶斯适配器，而无需额外的训练。TFB 系统地搜索权重后验中最大可接受的方差水平，限制在低秩各向同性高斯分布族内。我们从理论上证明，在温和的条件下，这个搜索过程相当于权重的变分推断。通过全面的实验，我们表明，与现有方法相比，TFB 实现了卓越的不确定性估计和泛化，同时消除了对复杂训练程序的需要。代码将在https://github.com/Wang-ML-Lab/bayesian-peft 上提供。]]></description>
      <guid>https://arxiv.org/abs/2412.05723</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>非线性自适应套索的近端迭代</title>
      <link>https://arxiv.org/abs/2412.05726</link>
      <description><![CDATA[arXiv:2412.05726v1 公告类型：新
摘要：使用 $\ell_1$ 惩罚来增强平滑成本函数，使分析师能够在复杂模型中同时有效地进行估计和变量选择，并且可以使用近端梯度方法有效地实现。然而，$\ell_1$ 惩罚的一个缺点是偏差：非零参数的幅度被低估，这促使人们使用诸如自适应套索之类的技术，为每个参数赋予自己的惩罚系数。但目前尚不清楚在复杂模型中应如何设置这些参数特定的惩罚。在本文中，我们研究了将惩罚系数视为以 \textit{Maximum a Posteriori} 方式学习的额外决策变量的方法，开发了一种近端梯度方法来联合优化这些变量以及任何可微分成本函数的参数。除了减少估计中的偏差之外，此过程还可以通过惩罚系数的先验来鼓励任意稀疏结构。我们将我们的方法与合成和真实数据集上非高斯回归的特定稀疏结构实现进行了比较，发现我们更通用的方法在速度和准确性方面都具有竞争力。然后，我们考虑了两个案例研究的非线性模型：COVID-19 疫苗接种行为和国际难民流动，强调了这种方法对复杂问题和错综复杂的稀疏结构的适用性。]]></description>
      <guid>https://arxiv.org/abs/2412.05726</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用黑盒模型评估无条件分布中的特征重要性</title>
      <link>https://arxiv.org/abs/2412.05759</link>
      <description><![CDATA[arXiv:2412.05759v1 公告类型：新
摘要：了解解释特征的变化如何影响结果的无条件分布在许多应用中都很重要。然而，现有的黑盒预测模型并不适合分析这类问题。在这项工作中，我们开发了一种近似方法来计算与结果无条件分布相关的特征重要性曲线，同时利用预先训练的黑盒预测模型的强大功能。特征重要性曲线衡量在解释特征变化的外部影响下结果分布分位数的变化。通过大量的数值实验和真实数据示例，我们证明了我们的近似方法产生了稀疏而真实的结果，并且计算效率高。]]></description>
      <guid>https://arxiv.org/abs/2412.05759</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>离散时间线性二次控制问题的强化学习及其应用</title>
      <link>https://arxiv.org/abs/2412.05906</link>
      <description><![CDATA[arXiv:2412.05906v1 公告类型：新
摘要：我们研究使用强化学习 (RL) 的离散时间线性二次 (LQ) 控制模型。使用熵来衡量探索成本，我们证明该问题的最优反馈策略必须是高斯型。然后，我们将离散时间 LQ 模型的结果应用于解决离散时间均值方差资产负债管理问题，并证明我们的 RL 算法的策略改进和收敛。最后，一个数值示例阐明了使用模拟建立的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2412.05906</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成式人工智能能否解决你的情境学习问题？马丁格尔视角</title>
      <link>https://arxiv.org/abs/2412.06033</link>
      <description><![CDATA[arXiv:2412.06033v1 公告类型：新
摘要：这项工作是关于估计条件生成模型 (CGM) 何时可以解决上下文学习 (ICL) 问题。上下文学习 (ICL) 问题包括 CGM、数据集和预测任务。CGM 可以是多模态基础模型；数据集是患者历史、测试结果和记录诊断的集合；预测任务是将诊断传达给新患者。ICL 的贝叶斯解释假设 CGM 计算未知贝叶斯模型的后验预测分布，该模型定义了潜在解释和可观察数据的联合分布。从这个角度来看，贝叶斯模型批评是一种合理的方法来评估给定的 CGM 是否适合 ICL 问题。然而，此类方法（例如后验预测检验 (PPC)）通常假设我们可以从贝叶斯模型定义的似然和后验中抽样，而这在当代 CGM 中并未明确给出。为了解决这个问题，我们展示了从 CGM 的预测分布中进行祖先抽样何时等同于从假定的贝叶斯模型的后验预测中抽样数据集。然后，我们开发了生成预测 $p$ 值，它使 PPC 及其同类可用于当代 CGM。然后，生成预测 $p$ 值可用于统计决策过程，以确定该模型何时适合 ICL 问题。我们的方法只需要从 CGM 生成查询和响应并评估其响应日志概率。我们使用大型语言模型在合成表格、成像和自然语言 ICL 任务上对我们的方法进行了实证评估。]]></description>
      <guid>https://arxiv.org/abs/2412.06033</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PINN深度学习一般偏微分方程的神经正切核总是收敛的吗？</title>
      <link>https://arxiv.org/abs/2412.06158</link>
      <description><![CDATA[arXiv:2412.06158v1 Announce Type: new 
摘要：本文研究了基于物理信息神经网络（PINN）的一般偏微分方程（PDE）的神经正切核（NTK）。众所周知，人工神经网络的训练可以转化为NTK的演化。我们分析了NTK的初始化以及NTK在一般PDE训练过程中的收敛条件。理论结果表明，微分算子的齐次性对NTK的收敛起着至关重要的作用。此外，基于PINN，我们利用正弦-Gordon方程的初值问题和KdV方程的初边值问题验证了NTK的收敛条件。]]></description>
      <guid>https://arxiv.org/abs/2412.06158</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>矩阵补全的表征迁移学习</title>
      <link>https://arxiv.org/abs/2412.06233</link>
      <description><![CDATA[arXiv:2412.06233v1 公告类型：新
摘要：我们提出通过聚合奇异子空间信息将来自多个来源的表征知识转移到目标噪声矩阵完成任务。在我们的表征相似性框架下，我们首先通过基于适当去偏的矩阵值数据集解决双向主成分分析问题来集成线性表征信息。在从源获取更好的列和行表示估计量后，原始高维目标矩阵完成问题随后转化为低维线性回归，从而保证了统计效率。同时还讨论了各种扩展论证，包括转移后统计推断和对负转移的鲁棒性。最后，报告了大量模拟结果和大量真实数据案例来支持我们的主张。]]></description>
      <guid>https://arxiv.org/abs/2412.06233</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用贝叶斯模型比较来推断两个系统之间的依赖关系</title>
      <link>https://arxiv.org/abs/2412.06478</link>
      <description><![CDATA[arXiv:2412.06478v2 公告类型：新
摘要：我们建议基于两个模型的贝叶斯比较来量化数据集 $D$ 中两个系统 $X$ 和 $Y$ 之间的依赖关系：一个模型 $H_0$ 表示统计独立性，另一个模型 $H_1$ 表示依赖性。在此框架中，$D$ 中 $X$ 和 $Y$ 之间的依赖关系（表示为 $B(X,Y|D)$）被量化为 $P(H_1|D)$，即给定 $D$ 的依赖模型的后验概率，或其任何严格递增函数。因此，它是 $H_1$ 建模并在 $D$ 中观察到的 $X$ 和 $Y$ 之间依赖关系的证据度量。我们回顾了几个统计模型，并根据 $B(X,Y|D)$ 作为依赖性的度量重新考虑标准结果。通过模拟，我们重点关注两个具体问题：噪声的影响和当 $H_1$ 具有用于编码依赖强度的参数时 $B(X,Y|D)$ 的行为。然后，我们推导出 $B(X,Y|D)$ 的一些一般属性，表明它量化了 $D$ 中包含的信息，有利于 $H_1$ 而不是 $H_0$。虽然其中一些属性是有效依赖度量所期望的典型属性，但其他属性则是新颖的，并且自然而然地作为特定依赖度量的期望特征出现，我们称之为推理。最后，我们将这些结果放在透视图中；特别是，我们讨论了使用贝叶斯框架的后果以及 $B(X,Y|D)$ 和互信息之间的异同。]]></description>
      <guid>https://arxiv.org/abs/2412.06478</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于图像恢复的等变降噪器</title>
      <link>https://arxiv.org/abs/2412.05343</link>
      <description><![CDATA[arXiv:2412.05343v1 公告类型：交叉 
摘要：图像恢复的一个关键要素是在干净的图像上定义一个现实的先验，以完成观察中缺失的信息。最先进的恢复方法依赖于神经网络来编码这个先验。此外，典型的图像分布对于某些变换集（例如旋转或翻转）是不变的。然而，大多数深度架构并非设计用于表示不变的图像分布。最近的研究提出通过在即插即用范例中包含等方差属性来克服这一困难。在这项工作中，我们提出了一个基于等变去噪器和随机优化的统一框架，称为等变正则化去噪 (ERED)。我们分析了该算法的收敛性并讨论了其实际效益。]]></description>
      <guid>https://arxiv.org/abs/2412.05343</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用张量收缩层和变换器生成表格数据</title>
      <link>https://arxiv.org/abs/2412.05390</link>
      <description><![CDATA[arXiv:2412.05390v1 公告类型：交叉 
摘要：表格数据的生成模型最近在深度学习领域引起了广泛关注。其目标是估计数据的底层分布。然而，估计表格数据的底层分布有其独特的挑战。具体来说，这种数据模态由混合类型的特征组成，这使得模型学习它们之间的内部关系并非易事。解决混合问题的一种方法是通过标记化将每个特征嵌入到连续矩阵中，而捕获变量之间内部关系的解决方案是通过变压器架构。在这项工作中，我们实证研究了在表格数据生成中使用嵌入表示的潜力，利用张量收缩层和变压器在变分自动编码器中对表格数据的底层分布进行建模。具体来说，我们比较了四种架构方法：一个基线 VAE 模型、两个分别关注张量收缩层和变压器的变体，以及一个集成两种技术的混合模型。我们针对 OpenML CC18 套件中的多个数据集进行了实证研究，比较了模型的密度估计和机器学习效率指标。从我们的研究结果中得出的主要结论是，在张量收缩层的帮助下利用嵌入表示可以改善密度估计指标，同时在机器学习效率方面保持竞争力。]]></description>
      <guid>https://arxiv.org/abs/2412.05390</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机特征组合没有免费午餐</title>
      <link>https://arxiv.org/abs/2412.05418</link>
      <description><![CDATA[arXiv:2412.05418v1 公告类型：交叉 
摘要：给定总模型大小的预算，必须决定是训练单个大型神经网络还是组合许多较小网络的预测。我们研究了随机特征岭回归模型集合的这种权衡。我们证明，当固定数量的可训练参数在 $K$ 个独立训练的模型之间进行分配时，只要岭参数经过最佳调整，$K=1$ 即可实现最佳性能。然后，我们推导出缩放定律，描述回归模型集合的测试风险如何随其总大小而衰减。我们确定了内核和任务特征结构的条件，在这些条件下，集合可以实现近乎最优的缩放定律。通过在 CIFAR-10 上训练深度卷积神经网络集合并在 C4 上训练变换器架构，我们发现，只要将权重衰减和特征学习强度调整到最佳值，单个大型网络的表现就会优于任何具有相同总参数数的网络集合。]]></description>
      <guid>https://arxiv.org/abs/2412.05418</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KEDformer：知识提取季节性趋势分解用于长期序列预测</title>
      <link>https://arxiv.org/abs/2412.05421</link>
      <description><![CDATA[arXiv:2412.05421v1 公告类型：交叉 
摘要：时间序列预测是能源、金融和气象等领域的一项关键任务，其中准确的长期预测至关重要。虽然基于 Transformer 的模型在捕获时间依赖性方面表现出色，但它们在扩展序列中的应用受到计算效率低下和泛化有限的限制。在本研究中，我们提出了 KEDformer，这是一个知识提取驱动的框架，它集成了季节性趋势分解来应对这些挑战。KEDformer 利用知识提取方法，专注于自注意机制中最具信息量的权重，以减少计算开销。此外，提出的 KEDformer 框架将时间序列分解为季节性和趋势成分。这种分解增强了模型捕捉短期波动和长期模式的能力。在能源、交通和天气领域的五个公共数据集上进行的大量实验证明了 KEDformer 的有效性和竞争力，为长期时间序列预测提供了有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2412.05421</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>支持向量回归的统计力学</title>
      <link>https://arxiv.org/abs/2412.05439</link>
      <description><![CDATA[arXiv:2412.05439v1 公告类型：交叉 
摘要：深度学习和计算神经科学中的一个关键问题是将神经表征的几何特性与任务性能联系起来。在这里，我们考虑这个问题，用于连续解码任务，其中神经变异性可能会影响任务精度。使用统计力学的方法，我们研究了 $\varepsilon$ 不敏感的支持向量回归 ($\varepsilon$-SVR) 的平均情况学习曲线，并讨论了其作为线性可解码性度量的能力。我们的分析揭示了临界负载下训练误差的相变，捕捉到了容差参数 $\varepsilon$ 和神经变异性之间的相互作用。我们发现了泛化误差中的双下降现象，表明 $\varepsilon$ 充当正则化器，既抑制又移动这些峰值。理论预测在玩具模型和深度神经网络上都得到了验证，将支持向量机的理论扩展到具有固有神经变异性的连续任务。]]></description>
      <guid>https://arxiv.org/abs/2412.05439</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>代数电路组合图集</title>
      <link>https://arxiv.org/abs/2412.05481</link>
      <description><![CDATA[arXiv:2412.05481v1 公告类型：交叉 
摘要：基于和积结构的电路已成为一种普遍存在的表示，用于紧凑地编码知识，从布尔函数到概率分布。通过对此类电路的结构施加约束，某些推理查询变得易于处理，例如模型计数和最可能的配置。最近的研究探索了将概率和因果推理查询分析为基本运算符的组合以得出可处理性条件。在本文中，我们从代数的角度进行组合推理，并表明一大类查询 - 包括边际 MAP、概率答案集编程推理和因果后门调整 - 对应于半环上的基本运算符的组合：聚合、乘积和元素映射。利用这个框架，我们发现了这些运算符可处理组合的简单和一般的充分条件，包括电路属性（例如边际确定性、兼容性）和元素映射的条件。应用我们的分析，我们为许多此类组合查询推导出新的可处理性条件。我们的结果统一了电路上现有问题的可处理性条件，同时为分析新的组合推理查询提供了蓝图。]]></description>
      <guid>https://arxiv.org/abs/2412.05481</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>