<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 20 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>统一的二元和多类基于保证金的分类</title>
      <link>https://arxiv.org/abs/2311.17778</link>
      <description><![CDATA[arXiv:2311.17778v2 公告类型：替换 
摘要：边距损失的概念一直是二分类算法开发和分析的核心。然而，到目前为止，对于多类分类的边距损失的类似物仍未达成共识。在这项工作中，我们表明，包括许多流行的多类损失函数在内的各种多类损失函数都可以用相对边距形式表示，这是二元损失边距形式的泛化。正如我们之前的工作（Wang and Scott，2020 年、2021 年）所示，相对边距形式对于理解和分析多类损失非常有用。为了进一步证明这种表达多类损失的方式的实用性，我们使用它来扩展 Bartlett 等人（2006 年）关于二元边距损失分类校准到多类的开创性成果。然后，我们分析了 Fenchel-Young 损失的类别，并扩展了已知经过分类校准的这些损失集。]]></description>
      <guid>https://arxiv.org/abs/2311.17778</guid>
      <pubDate>Mon, 20 May 2024 06:19:15 GMT</pubDate>
    </item>
    <item>
      <title>虚假特征是如何记忆的：随机特征和 NTK 特征的精确分析</title>
      <link>https://arxiv.org/abs/2305.12100</link>
      <description><![CDATA[arXiv:2305.12100v3 公告类型：替换
摘要：众所周知，深度学习模型会过度拟合并记住训练数据集中的虚假特征。尽管许多实证研究旨在理解这种现象，但仍然缺乏量化它的严格理论框架。在本文中，我们考虑与学习任务不相关的虚假特征，并通过两个单独的术语提供它们如何记忆的精确特征：（i）模型相对于单个训练样本的稳定性，以及（ii） ）虚假特征和完整样本之间的特征对齐。虽然第一个术语在学习理论中已经很成熟，并且与经典著作中的泛化错误有关，但据我们所知，第二个术语是新颖的。我们的关键技术结果给出了随机特征（RF）和神经正切核（NTK）回归的两种典型设置的特征对齐的精确表征。我们证明，随着泛化能力的增强，对虚假特征的记忆会减弱，并且通过对特征对齐的分析，我们揭示了模型及其激活函数的作用。数值实验显示了我们的理论对标准数据集（MNIST、CIFAR-10）的预测能力。]]></description>
      <guid>https://arxiv.org/abs/2305.12100</guid>
      <pubDate>Mon, 20 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>通过 Wasserstein 空间中的近端梯度下降实现基于流的生成模型的收敛</title>
      <link>https://arxiv.org/abs/2310.17582</link>
      <description><![CDATA[arXiv:2310.17582v2 公告类型：替换
摘要：基于流的生成模型在计算数据生成和可能性方面具有一定的优势，并且最近显示出有竞争力的实证性能。与相关基于分数的扩散模型的不断积累的理论研究相比，基于流的模型的分析仍然稀疏，这些模型在前向（数据到噪声）和反向（噪声到数据）方向上都是确定性的。在本文中，我们提供了通过渐进流模型（即所谓的 JKO 流模型）生成数据分布的理论保证，该模型在归一化流网络中实现了 Jordan-Kinderleherer-Otto (JKO) 方案。利用 Wasserstein 空间中近端梯度下降 (GD) 的指数收敛，我们证明当使用 $N \ 时，JKO 流模型数据生成的 Kullback-Leibler (KL) 保证为 $O(\varepsilon^2)$ lesssim \log (1/\varepsilon)$ 许多 JKO 步骤（流中的 $N$ 剩余块），其中 $\varepsilon $ 是每步骤一阶条件中的误差。数据密度的假设仅仅是有限二阶矩，该理论扩展到没有密度的数据分布以及当逆过程中存在反演误差时，我们获得KL-$W_2$混合误差保证。 JKO 型 $W_2$-proximal GD 的非渐近收敛率被证明适用于一般类别的凸目标函数，其中包括 KL 散度作为特例，可以具有独立的意义。该分析框架可以扩展到应用于基于流的生成模型的其他一阶 Wasserstein 优化方案。]]></description>
      <guid>https://arxiv.org/abs/2310.17582</guid>
      <pubDate>Mon, 20 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>通过主成分分析进行模型正交化和贝叶斯预测混合</title>
      <link>https://arxiv.org/abs/2405.10839</link>
      <description><![CDATA[arXiv:2405.10839v1 公告类型：交叉
摘要：通过使用贝叶斯统计机器学习框架结合不完善的复杂计算模型的预测，可以提高未知领域的可预测性。然而，在许多情况下，混合过程中使用的模型是相似的。除了污染模型空间之外，多重建模过程中此类相似甚至冗余模型的存在还可能导致对结果的误解和预测性能的恶化。在这项工作中，我们描述了一种基于主成分分析的方法，该方法消除了模型冗余。我们表明，通过将模型正交化添加到所提出的贝叶斯模型组合框架中，可以获得更好的预测精度并达到出色的不确定性量化性能。]]></description>
      <guid>https://arxiv.org/abs/2405.10839</guid>
      <pubDate>Mon, 20 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>动态环境中具有共形预测保证的递归可行收缩视野 MPC</title>
      <link>https://arxiv.org/abs/2405.10875</link>
      <description><![CDATA[arXiv:2405.10875v1 公告类型：交叉
摘要：在本文中，我们重点研究不确定动态环境中的缩域模型预测控制（MPC）问题。我们考虑控制一个确定性自主系统，该系统在执行任务期间与不可控的随机代理交互。现有工作利用共形预测工具，为未知智能体轨迹导出高置信度预测区域，并将这些区域集成到 MPC 合适的安全约束设计中。尽管保证了闭环轨迹的概率安全，但这些约束并不能确保相应 MPC 方案在整个任务期间的可行性。我们提出了一种缩小范围的 MPC，随着新的预测区域在线可用，它通过逐渐放松安全约束来保证递归的可行性。这种放宽强制执行安全约束，以保留所有可用预测区域集合中限制性最小的预测区域。在与现有技术的比较案例研究中，我们凭经验证明我们的方法会产生更严格的预测区域，并验证我们的 MPC 方案的递归可行性。]]></description>
      <guid>https://arxiv.org/abs/2405.10875</guid>
      <pubDate>Mon, 20 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>观察尺度定律和语言模型性能的可预测性</title>
      <link>https://arxiv.org/abs/2405.10938</link>
      <description><![CDATA[arXiv:2405.10938v1 公告类型：交叉
摘要：了解语言模型性能如何随规模变化对于基准测试和算法开发至关重要。尺度法则是建立这种理解的一种方法，但跨许多不同尺度的训练模型的要求限制了它们的使用。我们提出了一种替代的观察方法，绕过模型训练，而是从约 80 个公开可用的模型中构建缩放法则。由于训练计算效率和能力差异很大，从多个模型系列构建单一的缩放法则具有挑战性。然而，我们表明这些变化与简单的广义缩放定律一致，其中语言模型性能是低维能力空间的函数，并且模型族仅在将训练计算转换为能力的效率上有所不同。使用这种方法，我们展示了复杂尺度现象令人惊讶的可预测性：我们展示了几种新兴现象遵循平滑的 S 形行为，并且可以通过小模型进行预测；我们证明，可以从更简单的非代理基准中精确预测 GPT-4 等模型的代理性能；我们还展示了随着语言模型能力的不断提高，如何预测思想链和自我一致性等训练后干预措施的影响。]]></description>
      <guid>https://arxiv.org/abs/2405.10938</guid>
      <pubDate>Mon, 20 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>$\ell_1$-正则化广义最小二乘法</title>
      <link>https://arxiv.org/abs/2405.10719</link>
      <description><![CDATA[arXiv:2405.10719v1 公告类型：交叉
摘要：在本文中，我们提出了一种 $\ell_1$ 正则化 GLS 估计器，用于具有潜在自相关误差的高维回归。我们在允许高度持久的自回归误差的框架中建立非渐近预言不等式以提高估计准确性。在实践中，实现 GLS 所需的白化矩阵是未知的，我们为该矩阵提出了一个可行的估计器，得出一致性结果，并最终展示了我们提出的可行 GLS 如何能够接近地恢复最佳性能（就好像误差是白噪声一样）套索的。仿真研究验证了所提出方法的性能，证明了惩罚（可行）GLS-LASSO 估计器在白噪声误差的情况下与 LASSO 的性能相当，同时在符号恢复和估计误差方面优于它。误差表现出显着的相关性。]]></description>
      <guid>https://arxiv.org/abs/2405.10719</guid>
      <pubDate>Mon, 20 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>非凸非光滑条件随机优化的函数模型方法</title>
      <link>https://arxiv.org/abs/2405.10815</link>
      <description><![CDATA[arXiv:2405.10815v1 公告类型：交叉
摘要：我们考虑随机优化问题，涉及基本随机向量的非线性函数的期望值以及取决于基本随机向量、相关随机向量和决策变量的另一个函数的条件期望。我们将此类问题称为条件随机优化问题。它们出现在许多应用中，例如提升建模、强化学习和上下文优化。我们提出了一种专门的单时间尺度随机方法，用于具有 Lipschitz 平滑外函数和广义可微内函数的非凸约束条件随机优化问题。在该方法中，我们使用丰富的参数模型来近似内部条件期望，该模型的均方误差满足 {\L}ojasiewicz 条件的随机版本。该模型由内部学习算法使用。我们的方法的主要特点是，可以通过每次迭代的联合分布的一个观察来生成该方法所使用的方向的无偏随机估计，这使得它适用于实时学习。然而，方向不是任何整体目标函数的梯度或次梯度。我们使用微分包含方法和专门设计的 Lyapunov 函数以概率 1 证明了该方法的收敛性，涉及 Bregman 距离的随机推广。最后，数字说明证明了我们方法的可行性。]]></description>
      <guid>https://arxiv.org/abs/2405.10815</guid>
      <pubDate>Mon, 20 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>基于模拟的个性化零售促销强化学习代理基准测试</title>
      <link>https://arxiv.org/abs/2405.10469</link>
      <description><![CDATA[arXiv:2405.10469v1 公告类型：交叉 
摘要：开放式基准测试平台的开发可以大大加速零售业采用人工智能代理。本文对客户购物行为进行了全面的模拟，目的是对优化优惠券定位的强化学习 (RL) 代理进行基准测试。这个学习问题的难度很大程度上是由客户购买事件的稀疏性造成的。我们使用包含汇总客户购买历史的离线批量数据来训练代理，以帮助减轻这种影响。我们的实验表明，不太容易过度拟合稀疏奖励分布的上下文强盗和深度 RL 方法明显优于静态策略。这项研究提供了一个实用的框架来模拟优化整个零售客户旅程的人工智能代理。它旨在激发零售人工智能系统模拟工具的进一步发展。]]></description>
      <guid>https://arxiv.org/abs/2405.10469</guid>
      <pubDate>Mon, 20 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>通过 ADMM 进行分布式基于事件的学习</title>
      <link>https://arxiv.org/abs/2405.10618</link>
      <description><![CDATA[arXiv:2405.10618v1 公告类型：交叉
摘要：我们考虑一个分布式学习问题，其中代理通过网络交换信息来最小化全局目标函数。我们的方法有两个明显的特征：（i）它仅在必要时触发通信，从而大大减少了通信，并且（ii）它与不同代理之间的数据分布无关。因此，即使代理的本地数据分布任意不同，我们也可以保证收敛。我们分析了算法的收敛速度，并推导出凸设置下的加速收敛速度。我们还描述了通信中断的影响，并证明我们的算法对于通信故障具有鲁棒性。本文最后介绍了分布式 LASSO 问题的数值结果以及 MNIST 和 CIFAR-10 数据集上的分布式学习任务。这些实验强调，由于基于事件的通信策略，通信节省了 50% 或更多，显示了对异构数据分布的弹性，并强调我们的方法优于 FedAvg、FedProx 和 FedADMM 等常见基线。]]></description>
      <guid>https://arxiv.org/abs/2405.10618</guid>
      <pubDate>Mon, 20 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>用于带有错误分类惩罚的假设检验的子模信息选择</title>
      <link>https://arxiv.org/abs/2405.10930</link>
      <description><![CDATA[arXiv:2405.10930v1 公告类型：新
摘要：我们考虑为假设检验/分类任务选择最佳信息源子集的问题，其目标是基于来自源的有限观察样本，从有限的假设集中识别世界的真实状态。为了表征学习性能，我们提出了一个误分类惩罚框架，它可以对不同的误分类错误进行非统一处理。在集中式贝叶斯学习环境中，我们研究子集选择问题的两种变体：（i）选择最小成本信息集以确保错误分类真实假设的最大惩罚保持有限；（ii）在有限的预算来最小化错误分类真实假设的最大惩罚。在温和的假设下，我们证明这些组合优化问题的目标（或约束）是弱（或近似）子模的，并为贪婪算法建立了高概率的性能保证。此外，我们提出了一种基于错误分类总惩罚的信息集选择替代指标。我们证明该度量是子模的，并为信息集选择问题的贪婪算法建立了近乎最优的保证。最后，我们提出数值模拟来验证我们在几个随机生成的实例上的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2405.10930</guid>
      <pubDate>Mon, 20 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>快速提交者机器：可解释的内核预测</title>
      <link>https://arxiv.org/abs/2405.10410</link>
      <description><![CDATA[arXiv:2405.10410v1 公告类型：交叉
摘要：在随机动力学研究中，提交者函数描述了从初始配置 $x$ 开始的进程在集合 $B$ 之前到达集合 $A$ 的概率。本文介绍了一种快速且可解释的近似提交者的方法，称为“快速提交者机器”（FCM）。 FCM 基于模拟轨迹数据，并使用该数据来训练内核模型。 FCM 识别最佳地描述 $A$ 到 $B$ 转换的低维子空间，并且在内核模型中强调了这些子空间。 FCM 使用随机数值线性代数来训练模型，其运行时间随数据点数量线性缩放。本文将 FCM 应用于包括丙氨酸二肽小蛋白在内的示例系统：在这些实验中，FCM 通常比具有相似参数数量的神经网络更准确且训练速度更快。]]></description>
      <guid>https://arxiv.org/abs/2405.10410</guid>
      <pubDate>Mon, 20 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>不安分的线性强盗</title>
      <link>https://arxiv.org/abs/2405.10817</link>
      <description><![CDATA[arXiv:2405.10817v1 公告类型：新
摘要：线性老虎机问题的更通用的表述被认为允许随时间的依赖性。具体来说，假设存在未知的 $\mathbb{R}^d$ 值平稳 $\varphi$ 参数混合序列 $(\theta_t,~t \in \mathbb{N})$ ，这会产生到回报。这个问题的实例可以被视为具有独立同分布噪声的经典线性老虎机和有限臂不安分老虎机的概括。鉴于众所周知的不安强盗贼最优策略的计算难度，提出了一种近似方法，其误差由连续 $\theta_t$ 之间的 $\varphi$ 依赖性控制。针对 $\theta_t$ 具有指数混合率的情况，提出了一种称为 LinMix-UCB 的乐观算法。所提出的算法对于总是播放多个$\mathbb{E}\theta_t$。这种情况下的主要挑战是确保探索-利用策略对于远程依赖具有鲁棒性。所提出的方法依赖于 Berbee 的耦合引理来仔细选择近乎独立的样本，并围绕 $\mathbb{E}\theta_t$ 的经验估计构建置信椭球体。]]></description>
      <guid>https://arxiv.org/abs/2405.10817</guid>
      <pubDate>Mon, 20 May 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>关于持续在线学习的说明</title>
      <link>https://arxiv.org/abs/2405.10399</link>
      <description><![CDATA[arXiv:2405.10399v1 公告类型：新
摘要：在在线学习中，数据按顺序提供，学习者的目标是做出在线决策以最小化总体遗憾。本文涉及几个在线学习问题的连续时间模型和算法：在线线性优化、对抗性强盗和对抗性线性强盗。对于每个问题，我们将离散时间算法扩展到连续时间设置，并提供最佳遗憾界限的简明证明。]]></description>
      <guid>https://arxiv.org/abs/2405.10399</guid>
      <pubDate>Mon, 20 May 2024 06:19:08 GMT</pubDate>
    </item>
    <item>
      <title>可解释和可解释人工智能的数据科学原理</title>
      <link>https://arxiv.org/abs/2405.10552</link>
      <description><![CDATA[arXiv:2405.10552v1 公告类型：新
摘要：社会通过算法解决问题的能力从未如此强大。由于强大的抽象、丰富的数据和可访问的软件，人工智能现在比以往任何时候都应用在更多的领域。随着能力的扩展，风​​险也随之增加，模型通常在没有充分了解其潜在影响的情况下进行部署。可解释和交互式机器学习旨在使复杂模型更加透明和可控，从而增强用户代理。这篇综述综合了该领域不断增长的文献中的关键原则。
  我们首先引入用于讨论可解释性的精确词汇，例如玻璃盒和可解释算法之间的区别。然后，我们探索与经典统计和设计原则的联系，例如简约和交互鸿沟。基本的可解释性技术——包括学习嵌入、集成梯度和概念瓶颈——通过一个简单的案例研究来说明。我们还审查了客观评估可解释性方法的标准。自始至终，我们都强调在设计交互式算法系统时考虑受众目标的重要性。最后，我们概述了开放的挑战，并讨论了数据科学在解决这些挑战方面的潜在作用。可以在 https://go.wisc.edu/3k1ewe 找到重现所有示例的代码。]]></description>
      <guid>https://arxiv.org/abs/2405.10552</guid>
      <pubDate>Mon, 20 May 2024 06:19:08 GMT</pubDate>
    </item>
    </channel>
</rss>