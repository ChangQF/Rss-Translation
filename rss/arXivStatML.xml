<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 13 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>统计学习中的调整 Wasserstein 分布稳健估计器</title>
      <link>https://arxiv.org/abs/2303.15579</link>
      <description><![CDATA[arXiv:2303.15579v3 公告类型：替换
摘要：我们提出了一种调整的 Wasserstein 分布鲁棒估计器——基于统计学习中 Wasserstein 分布鲁棒 (WDRO) 估计器的非线性变换。经典的 WDRO 估计器是渐近有偏的，而我们调整后的 WDRO 估计器是渐近无偏的，从而导致更小的渐近均方误差。此外，在某些条件下，我们提出的调整技术为消除渐近偏差估计量提供了一般原则。具体来说，我们将研究如何在广义线性模型中开发调整后的 WDRO 估计器，包括逻辑回归、线性回归和泊松回归。数值实验证明了调整后的估计器比经典估计器具有良好的实际性能。]]></description>
      <guid>https://arxiv.org/abs/2303.15579</guid>
      <pubDate>Mon, 13 May 2024 06:19:04 GMT</pubDate>
    </item>
    <item>
      <title>通过变分自动编码器进行灵活高效的空间极限仿真</title>
      <link>https://arxiv.org/abs/2307.08079</link>
      <description><![CDATA[arXiv:2307.08079v3 公告类型：替换
摘要：许多现实世界的过程具有复杂的尾部依赖结构，无法使用经典高斯过程来表征。更灵活的空间极值模型表现出吸引人的极值依赖性特性，但通常难以在高维度上进行拟合和模拟。在本文中，我们的目标是通过在称为 XVAE 的变分自动编码器的编码-解码结构中集成一种新的空间极值模型，该模型具有灵活和非平稳的依赖属性，从而突破高维空间极值的计算和建模的界限。 XVAE 可以模拟空间观测并产生与输入具有相同统计属性的输出，尤其是在尾部。我们的方法还提供了一种利用复杂的极值过程进行快速推理的新颖方法。通过广泛的模拟研究，我们表明我们的 XVAE 比传统的贝叶斯推理更加省时，同时优于许多具有固定依赖结构的空间极值模型。最后，我们分析了红海海面温度的高分辨率卫星数据集，其中包括 30 年 16703 个网格单元的每日测量数据。我们演示了如何使用 XVAE 来识别气候变化下易受海洋热浪影响的区域，并检查极值依赖结构的空间和时间变化。]]></description>
      <guid>https://arxiv.org/abs/2307.08079</guid>
      <pubDate>Mon, 13 May 2024 06:19:04 GMT</pubDate>
    </item>
    <item>
      <title>任何数据分布都存在共形有效性保证</title>
      <link>https://arxiv.org/abs/2405.06627</link>
      <description><![CDATA[arXiv:2405.06627v1 公告类型：交叉
摘要：随着机器学习（ML）得到广泛采用，从业者越来越多地寻求量化和控制这些系统所产生风险的方法。当机器学习系统能够自主收集自己的数据时，例如在黑盒优化和主动学习中，它们的行为会导致数据分布中的顺序反馈循环变化，这一挑战尤其突出。共形预测已成为不确定性和风险量化的一种有前途的方法，但现有的变体要么无法适应数据相关的变化序列，要么没有充分利用代理引起的变化在我们控制之下的事实。在这项工作中，我们证明共形预测理论上可以扩展到 \textit{any} 联合数据分布，而不仅仅是可交换或准可交换的数据分布，尽管在最一般的情况下计算是非常不切实际的。对于实际应用，我们概述了为任何数据分布导出特定保形算法的过程，并且我们使用该过程为一系列代理引起的协变量偏移导出易于处理的算法。我们在综合黑盒优化和主动学习任务上对所提出的算法进行了实证评估。]]></description>
      <guid>https://arxiv.org/abs/2405.06627</guid>
      <pubDate>Mon, 13 May 2024 06:19:03 GMT</pubDate>
    </item>
    <item>
      <title>机器学习模型参数的有效推理</title>
      <link>https://arxiv.org/abs/2302.10840</link>
      <description><![CDATA[arXiv:2302.10840v2 公告类型：替换
摘要：机器学习模型的参数通常是通过最小化一组训练数据的损失函数来学习的。然而，这可能会带来过度训练的风险；为了使模型能够很好地泛化，非常重要的是我们能够在整个群体上（而不仅仅是在给定的训练样本上）找到模型的最佳参数。在本文中，我们为机器学习模型的最佳参数构建了有效的置信集，该置信集可以仅使用训练数据生成，而无需了解总体。然后，我们表明，研究该置信集的分布使我们能够将置信度概念分配给参数空间的任意区域，并且我们证明可以使用自举技术很好地近似该分布。]]></description>
      <guid>https://arxiv.org/abs/2302.10840</guid>
      <pubDate>Mon, 13 May 2024 06:19:03 GMT</pubDate>
    </item>
    <item>
      <title>无悔还不够！通过自适应后悔最小化具有一般约束的强盗</title>
      <link>https://arxiv.org/abs/2405.06575</link>
      <description><![CDATA[arXiv:2405.06575v1 公告类型：交叉
摘要：在带有背包框架（BwK）的强盗中，学习者有 $m$ 资源消耗（包装）约束。我们专注于 BwK 的泛化，其中学习者具有一组一般的长期约束。学习者的目标是最大化他们的累积奖励，同时实现小的累积约束违规。在这种情况下，存在一些简单的例子，传统的 BwK 方法无法产生亚线性违反约束的结果。我们证明，可以通过要求原始算法和对偶算法具有弱自适应性来规避这个问题。事实上，即使没有任何关于表征问题的 Slater 参数 $\rho$ 的信息，弱自适应原始和对偶后悔最小化器之间的相互作用也会产生对偶变量的“自限制”属性。特别是，即使没有明确的投影步骤，它们的范数在整个时间范围内仍然保持适当的上限。通过利用这一特性，我们为随机输入和对抗输入提供了两全其美的保证。在第一种情况下，我们证明该算法保证了次线性后悔。在后一种情况下，我们建立严格的竞争比$\rho/(1+\rho)$。在这两种设置中，约束违反都保证在时间上是次线性的。最后，这个结果使我们能够获得具有线性约束的上下文强盗问题的新结果，为对抗性上下文提供第一个 no-$\alpha$-regret 保证。]]></description>
      <guid>https://arxiv.org/abs/2405.06575</guid>
      <pubDate>Mon, 13 May 2024 06:19:02 GMT</pubDate>
    </item>
    <item>
      <title>学习算法在集体行动中的作用</title>
      <link>https://arxiv.org/abs/2405.06582</link>
      <description><![CDATA[arXiv:2405.06582v1 公告类型：交叉
摘要：机器学习中的集体行动是对协调群体对机器学习算法的控制的研究。虽然以前的研究集中于评估集体对贝叶斯最优分类器的影响，但这种观点是有限的，因为实际上，分类器很少实现贝叶斯最优，并且受到学习算法的选择及其固有的归纳偏差的影响。在这项工作中，我们开始研究学习算法的选择如何在实际环境中集体的成功中发挥作用。具体来说，我们关注分布式鲁棒算法（DRO），它流行于改善最差的群体误差，以及流行的随机梯度下降（SGD），因为它对“更简单”函数的归纳偏差。我们的实证结果在理论基础的支持下表明，集体的有效规模和成功高度依赖于学习算法的属性。这凸显了在研究机器学习中集体行动的影响时考虑学习算法的必要性。]]></description>
      <guid>https://arxiv.org/abs/2405.06582</guid>
      <pubDate>Mon, 13 May 2024 06:19:02 GMT</pubDate>
    </item>
    <item>
      <title>知识图表示学习的 PAC-贝叶斯泛化界限</title>
      <link>https://arxiv.org/abs/2405.06418</link>
      <description><![CDATA[arXiv:2405.06418v1 公告类型：交叉
摘要：虽然在过去十年中提出了许多知识图表示学习（KGRL）方法，但对其进行的理论分析却很少。在本文中，我们提出了 KGRL 方法的第一个 PAC-贝叶斯泛化界限。为了分析一大类 KGRL 模型，我们提出了一个名为 ReED（关系感知编码器-解码器）的通用框架，它由关系感知消息传递编码器和三元组分类解码器组成。我们的 ReED 框架可以表达至少 15 种不同的现有 KGRL 模型，不仅包括基于图神经网络的模型（例如 R-GCN 和 CompGCN），还包括浅层架构模型（例如 RotatE 和 ANALOGY）。我们的 ReED 框架的泛化界限为 KGRL 中常用的技巧（例如参数共享和权重归一化方案）提供了理论基础，并指导实际 KGRL 方法的理想设计选择。我们凭经验表明，泛化边界中的关键因素可以解释三个现实世界知识图上的实际泛化错误。]]></description>
      <guid>https://arxiv.org/abs/2405.06418</guid>
      <pubDate>Mon, 13 May 2024 06:19:01 GMT</pubDate>
    </item>
    <item>
      <title>加权共形预测的信息量</title>
      <link>https://arxiv.org/abs/2405.06479</link>
      <description><![CDATA[arXiv:2405.06479v1 公告类型：交叉
摘要：加权共形预测（WCP）是最近提出的一种框架，它提供了不确定性量化，并且可以灵活地适应训练和测试数据之间的不同协变量分布。然而，本文指出WCP的有效性严重依赖于协变量分布之间的重叠；重叠不足可能会导致预测间隔信息不足。为了增强 WCP 的信息量，我们针对涉及具有不同协变量分布的多个源的场景提出了两种方法。我们为我们提出的方法建立了理论保证，并通过模拟证明了它们的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.06479</guid>
      <pubDate>Mon, 13 May 2024 06:19:01 GMT</pubDate>
    </item>
    <item>
      <title>基于深度学习的不确定故障模式资产的剩余使用寿命预测</title>
      <link>https://arxiv.org/abs/2405.06068</link>
      <description><![CDATA[arXiv:2405.06068v1 公告类型：交叉
摘要：工业预测的重点是利用退化信号来预测并持续更新复杂工程系统的剩余使用寿命。然而，具有多种故障模式的系统的现有预测模型在实际应用中面临着一些挑战，包括来自多个组件的重叠退化信号、未标记的历史数据的存在以及不同故障模式之间信号的相似性。为了解决这些问题，本研究引入了两种将混合（对数）位置尺度分布与深度学习相结合的预测模型。这种集成有助于对重叠退化信号进行建模，消除了明确的故障模式识别的需要，并利用深度学习来捕获退化信号和剩余使用寿命之间复杂的非线性关系。数值研究验证了这些提出的模型与现有方法相比的优越性能。]]></description>
      <guid>https://arxiv.org/abs/2405.06068</guid>
      <pubDate>Mon, 13 May 2024 06:19:00 GMT</pubDate>
    </item>
    <item>
      <title>变换 Bootstrap：使用变换器计算平面 N = 4 中的散射幅度 Super Yang-Mills 理论</title>
      <link>https://arxiv.org/abs/2405.06107</link>
      <description><![CDATA[arXiv:2405.06107v1 公告类型：交叉
摘要：我们追求使用深度学习方法来改进理论高能物理中最先进的计算。平面 N = 4 超级杨米尔斯理论与描述大型强子对撞机希格斯玻色子产生的理论非常接近；它的散射幅度是包含整数系数的大型数学表达式。在本文中，我们应用 Transformer 来预测这些系数。该问题可以用符合标准交叉熵训练目标的类似语言的表示形式来表述。我们设计了两个相关的实验，结果表明该模型在这两个任务上都实现了高精度（&gt; 98%）。我们的工作表明，Transformers 可以成功应用于需要精确解决方案的理论物理问题。]]></description>
      <guid>https://arxiv.org/abs/2405.06107</guid>
      <pubDate>Mon, 13 May 2024 06:19:00 GMT</pubDate>
    </item>
    <item>
      <title>随机矩阵理论改进对称正定矩阵的Fr\'echet均值</title>
      <link>https://arxiv.org/abs/2405.06558</link>
      <description><![CDATA[arXiv:2405.06558v1 公告类型：新
摘要：在这项研究中，我们考虑了机器学习中的协方差矩阵领域，特别关注计算对称正定矩阵流形上的 Fr&#39;echet 均值，通常称为 Karcher 或几何均值。许多机器学习任务都利用了这种方法。依靠先进的统计工具，我们引入了一种基于随机矩阵理论的方法来估计 Fr&#39;echet 均值，这在处理低样本支持和大量矩阵进行平均时特别有用。我们的实验评估涉及合成和现实世界的脑电图和高光谱数据集，表明我们在很大程度上优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2405.06558</guid>
      <pubDate>Mon, 13 May 2024 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>神经网络高斯过程的威尔逊重整化</title>
      <link>https://arxiv.org/abs/2405.06008</link>
      <description><![CDATA[arXiv:2405.06008v1 公告类型：交叉
摘要：分离相关和不相关信息是任何建模过程或科学探究的关键。理论物理学以重整化群（RG）的形式提供了实现这一目标的强大工具。在这里，我们演示了一种在高斯过程 (GP) 回归背景下执行威尔逊 RG 的实用方法。我们系统地整合了GP核中不可学习的模式，从而获得了高斯过程的RG流，其中数据扮演了能量尺度的角色。在简单的情况下，这会导致岭参数的通用流，在包含非高斯性的更丰富的场景中，该参数变得依赖于输入。除了易于分析处理之外，这种方法还超越了 RG 和神经网络之间的结构类比，提供了 RG 流和可学习模式与不可学习模式之间的自然联系。研究此类流可以提高我们对深度神经网络中特征学习的理解，并识别这些模型中潜在的普遍性类别。]]></description>
      <guid>https://arxiv.org/abs/2405.06008</guid>
      <pubDate>Mon, 13 May 2024 06:18:59 GMT</pubDate>
    </item>
    <item>
      <title>使用深度 ReLU 网络进行泛化分析，用于度量和相似性学习</title>
      <link>https://arxiv.org/abs/2405.06415</link>
      <description><![CDATA[arXiv:2405.06415v1 公告类型：新
摘要：虽然度量和相似性学习的研究已经取得了相当大的理论进展，但泛化之谜仍然缺失。在本文中，我们利用真实度量（目标函数）的特定结构来研究度量和相似性学习的泛化性能。具体来说，通过推导真实度量的显式形式，用于度量和具有铰链损失的相似性学习，我们构建了一个结构化的深度ReLU神经网络作为真实度量的近似，其近似能力依赖于网络复杂性。这里，网络复杂度对应于网络的深度、非零权重的数量和计算单元。考虑由结构化深度 ReLU 网络组成的假设空间，我们通过仔细估计近似误差和估计误差，为度量和相似性学习问题开发超额泛化误差界限。通过选择所构建的假设空间的适当容量来导出最优超额风险率。据我们所知，这是有史以来第一个为度量和相似性学习提供过量泛化误差的泛化分析。此外，我们还研究了具有一般损失的度量和相似性学习的真实度量的属性。]]></description>
      <guid>https://arxiv.org/abs/2405.06415</guid>
      <pubDate>Mon, 13 May 2024 06:18:58 GMT</pubDate>
    </item>
    <item>
      <title>在过度参数化的情况下对“重要性加权”估计量的分布外误差进行尖锐分析</title>
      <link>https://arxiv.org/abs/2405.06546</link>
      <description><![CDATA[arXiv:2405.06546v1 公告类型：新
摘要：据观察，实现零训练误差的过度参数化模型平均概括性良好，但当面对训练样本中代表性不足的数据时，性能会下降。在这项工作中，我们研究了一种充满虚假特征的超参数化高斯混合模型，并敏锐地分析了包含“重要性权重”的成本敏感插值解决方案的分布内和分布外测试误差。与王等人最近的工作相比。 （2021），Behnia 等人。 （2022），我们的分析在匹配上限和下限的情况下非常清晰，并且显着削弱了数据维度所需的假设。我们的误差表征也适用于重要性权重的任何选择，并揭示了最坏情况对分布变化的鲁棒性和作为重要性权重大小函数的平均准确度之间的新颖权衡。]]></description>
      <guid>https://arxiv.org/abs/2405.06546</guid>
      <pubDate>Mon, 13 May 2024 06:18:58 GMT</pubDate>
    </item>
    <item>
      <title>Softmax 模型和杠杆评分模型的二元假设检验</title>
      <link>https://arxiv.org/abs/2405.06003</link>
      <description><![CDATA[arXiv:2405.06003v1 公告类型：新
摘要：Softmax 分布广泛应用于机器学习中，包括大型语言模型（LLM），其中注意力单元使用 Softmax 分布。我们将注意力单元抽象为 softmax 模型，其中给定向量输入，该模型产生从 softmax 分布（取决于向量输入）中提取的输出。我们在 softmax 模型的设置中考虑二元假设检验的基本问题。也就是说，给定一个未知的 softmax 模型，已知该模型是两个给定的 softmax 模型之一，需要多少次查询才能确定哪一个是真实的？我们证明样本复杂度渐近$O(\epsilon^{-2})$，其中$\epsilon$是模型参数之间的一定距离。
  此外，我们将softmax模型与杠杆评分模型进行类比，杠杆评分模型是线性代数和图论中算法设计的重要工具。从高层次上看，杠杆评分模型是一种模型，在给定向量输入的情况下，根据依赖于输入的分布生成输出。对于杠杆评分模型的二元假设检验问题，我们获得了类似的结果。]]></description>
      <guid>https://arxiv.org/abs/2405.06003</guid>
      <pubDate>Mon, 13 May 2024 06:18:57 GMT</pubDate>
    </item>
    </channel>
</rss>