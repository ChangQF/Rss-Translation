<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 21 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>学习非齐次时间泊松过程的泛化和正则化</title>
      <link>https://arxiv.org/abs/2402.12808</link>
      <description><![CDATA[arXiv:2402.12808v1 公告类型：交叉
摘要：泊松过程，特别是非齐次泊松过程（NHPP），是一种本质上重要的计数过程，具有众多的实际应用。迄今为止，几乎所有文献中的工作都是使用非数据驱动的分箱方法来估计无限数据的 NHPP。在本文中，我们将根据有限数据估计 NHPP 的问题表述为学习泛化问题。我们从数学上证明，虽然分箱方法对于 NHPP 的估计至关重要，但当数据量有限时，它们会带来过度拟合的威胁。我们提出了一个用于 NHPP 正则化学习的框架，该框架具有两种新的自适应和数据驱动的分箱方法，有助于消除分箱参数的临时调整。我们的方法在合成数据集和真实数据集上进行了实验测试，结果表明了其有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.12808</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>PIP-Net：野外行人意图预测</title>
      <link>https://arxiv.org/abs/2402.12810</link>
      <description><![CDATA[arXiv:2402.12810v1 公告类型：交叉
摘要：自动驾驶车辆（AV）准确的行人意图预测（PIP）是该领域当前的研究挑战之一。在本文中，我们介绍了 PIP-Net，这是一种新颖的框架，旨在预测现实城市场景中自动驾驶汽车的行人过路意图。我们提供两种 PIP-Net 变体，专为不同的摄像机安装和设置而设计。利用驾驶场景的运动学数据和空间特征，所提出的模型采用了基于循环和时间注意力的解决方案，超越了最先进的性能。为了增强道路使用者的视觉表示及​​其与自我车辆的接近度，我们引入了分类深度特征图，结合局部运动流特征，提供了对场景动态的丰富洞察。此外，我们还探讨了扩大摄像头视野的影响，从围绕自我车辆的一个摄像头增加到三个摄像头，从而增强模型的情境感知。该模型根据交通场景和道路环境，能够提前4秒预测行人过街意图，这是当前行人意图预测研究的突破。最后，我们首次提出了 Urban-PIP 数据集，这是一个定制的行人意图预测数据集，在现实世界的自动驾驶场景中具有多摄像头注释。]]></description>
      <guid>https://arxiv.org/abs/2402.12810</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>利用可行集曲率实现在线凸优化的快速优化</title>
      <link>https://arxiv.org/abs/2402.12868</link>
      <description><![CDATA[arXiv:2402.12868v1 公告类型：交叉
摘要：在本文中，我们探索了在线凸优化（OCO）并引入了一种新的分析，该分析通过利用可行集的曲率来提供快速的速度。在在线线性优化中，众所周知，如果损失函数的平均梯度大于某个值，则可以通过跟随领导者（FTL）算法利用可行集的曲率来实现对数遗憾。本文揭示了适应损失函数曲率的算法也可以利用可行集的曲率。我们首先证明，如果最优决策位于可行集的边界上并且底层损失函数的梯度非零，则该算法在随机条件下实现 $O(\rho \log T)$ 的遗憾上限环境。这里，$\rho &gt; 0$ 是包含最优决策并包含可行集的最小球体的半径。与现有方法不同，我们的方法可以直接使用凸损失函数，同时利用损失函数的曲率，并且只能通过可行集的局部属性来实现对数遗憾。此外，即使在 FTL 遭受 $\Omega(T)$ 遗憾的对抗环境中，它也能实现 $O(\sqrt{T})$ 遗憾，并获得 $O(\rho \log T + \sqrt{C \ rho \log T})$ 在损坏级别为 $C$ 的损坏随机环境中感到遗憾。此外，通过扩展我们的分析，我们建立了后悔上限 $O\Big(T^{\frac{q-2}{2(q-1)}} (\log T)^{\frac{q} {2(q-1)}}\Big)$ 表示 $q$-均匀凸可行集，其中均匀凸集包括强凸集和 $\ell_p$-$p \in [1,\infty)$ 的球。该界限弥补了强凸集 ($q=2$) 的 $O(\log T)$ 遗憾界限与非弯曲集 ($q) 的 $O(\sqrt{T})$ 遗憾界限之间的差距\到\infty$）。]]></description>
      <guid>https://arxiv.org/abs/2402.12868</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>具有统一最后迭代保证的 Bandit 算法实现近乎最优的遗憾</title>
      <link>https://arxiv.org/abs/2402.12711</link>
      <description><![CDATA[arXiv:2402.12711v1 公告类型：交叉
摘要：老虎机算法的现有性能测量，例如遗憾、PAC 边界或统一 PAC（Dann 等人，2017），通常评估累积性能，同时允许在任何有限时间 t 上发挥任意坏臂的作用。这种行为在高风险应用中可能非常有害。本文引入了更强大的性能衡量标准，即统一最后迭代（ULI）保证，捕获强盗算法的累积和瞬时性能。具体来说，ULI 表征了瞬时性能，因为它确保所玩手臂的每轮遗憾受一个函数限制，该函数单调递减。 （大）轮t，当有足够的样本可用时，防止重新访问坏武器。我们证明，接近最优的 ULI 保证直接意味着上述性能指标的累积性能接近最优。为了检验 ULI 在有限臂设置中的可实现性，我们首先提供了两个积极的结果，即一些基于消除的算法和具有更强分析或附加设计的高概率对抗算法可以获得接近最优的 ULI 保证。然后，我们还提供了一个负结果，表明乐观算法无法实现接近最优的 ULI 保证。最后，我们提出了一种针对具有无限多臂的线性老虎机的有效算法，在访问优化预言机的情况下，该算法实现了 ULI 保证。]]></description>
      <guid>https://arxiv.org/abs/2402.12711</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>扩散后采样在计算上很难处理</title>
      <link>https://arxiv.org/abs/2402.12727</link>
      <description><![CDATA[arXiv:2402.12727v1 公告类型：交叉
摘要：扩散模型是一种从分布 $p(x)$ 学习和采样的非常有效的方法。在后验采样中，还给出了一个测量模型 $p(y \mid x)$ 和一个测量 $y$，并且希望从 $p(x \mid y)$ 中采样。后采样对于修复、超分辨率和 MRI 重建等任务很有用，因此最近的许多工作都给出了启发式近似它的算法；但没有一个已知可以在多项式时间内收敛到正确的分布。
  在本文中，我们证明后验采样是\emph{计算上难以处理的}：在密码学中最基本的假设下——单向函数存在——存在\emph{every}算法需要超多项式时间的情况，即使\emph{无条件}采样被证明是很快的。我们还表明，在存在需要指数时间反转的单向函数这一更合理的假设下，指数时间拒绝采样算法本质上是最优的。]]></description>
      <guid>https://arxiv.org/abs/2402.12727</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>深度复合高斯神经网络的泛化界限</title>
      <link>https://arxiv.org/abs/2402.13106</link>
      <description><![CDATA[arXiv:2402.13106v1 公告类型：新
摘要：算法展开或展开是从迭代算法构建深度神经网络（DNN）的技术。在信号估计任务中，展开的 DNN 通常比标准 DNN 提供更好的可解释性和卓越的经验性能。一个最近才受到关注的重要理论问题是展开 DNN 的泛化误差界限的发展。这些界限为 DNN 在经验数据集上的性能提供了理论和实践见解，这些数据集不同于生成 DNN 训练数据的概率密度，但又是从中采样的。在本文中，我们为一类由复合高斯先验告知的展开 DNN 开发了新颖的泛化误差界限。这些复合高斯网络已被证明在压缩传感和断层扫描成像问题中优于比较标准和展开的深度神经网络。泛化误差界是通过用 Dudley 积分限制复合高斯网络估计类的 Rademacher 复杂度来制定的。在现实条件下，我们表明，在最坏的情况下，泛化误差在信号维度上缩放 $\mathcal{O}(n\sqrt{\ln(n)})$ ，在信号维度上缩放 $\mathcal{O}(($Network Size) $)^{3/2})$ 网络规模。]]></description>
      <guid>https://arxiv.org/abs/2402.13106</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>在没有流形学习的情况下学习流形</title>
      <link>https://arxiv.org/abs/2402.12687</link>
      <description><![CDATA[arXiv:2402.12687v1 公告类型：交叉
摘要：基于从未知分布中随机抽取的数据的函数逼近是机器学习中的一个重要问题。与通过最小化损失函数来解决该问题的流行范例相反，我们给出了直接的一次性构造以及流形假设下的最佳误差界限；即，假设数据是从高维欧几里德空间的未知子流形中采样的。大量研究涉及获取有关该流形的信息，例如 Laplace-Beltrami 算子的特征分解或坐标图，并使用该信息进行函数逼近。除了函数近似中固有的误差之外，这种两步方法意味着近似中还存在一些源自数据基本量的额外误差。在 Neural Networks, 132:253268, 2020 中，我们提出了一种一次性直接方法来实现函数逼近，而不需要提取流形除维数之外的任何信息。然而，人们无法确定该论文中使用的近似值的类别。
  在本文中，我们将未知流形视为环境超球面的子流形，并研究使用基于超球面的球面多项式构造一次性近似的问题。我们的方法不需要对数据进行预处理来获取除流形维数之外的有关流形的信息。我们给出相对“粗糙”函数的最佳逼近率。]]></description>
      <guid>https://arxiv.org/abs/2402.12687</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>将主动学习与干扰的因果推理相结合：在线实验的一种新方法</title>
      <link>https://arxiv.org/abs/2402.12710</link>
      <description><![CDATA[arXiv:2402.12710v1 公告类型：交叉
摘要：在因果推理研究领域，流行的潜在结果框架，特别是鲁宾因果模型（RCM），往往忽视个体干扰并假设独立的治疗效果。然而，这种假设经常与现实场景中错综复杂的现实不一致，在现实场景中，干扰不仅是一种可能性，而且是一种常见现象。我们的研究致力于通过在两个假设下估计直接和溢出治疗效果来解决这种差异：（1）基于网络的干扰，其中对连接网络内邻居的治疗会影响一个人的结果，以及（2）非随机治疗分配受混杂因素影响。为了提高估计潜在复杂效应函数的效率，我们引入了一种新颖的主动学习方法：干扰因果推理（ACI）中的主动学习。该方法使用高斯过程灵活地对直接和溢出治疗效果进行建模，作为邻居治疗分配的连续测量的函数。 ACI 框架按顺序识别需要更多数据的实验设置。它利用遗传算法进一步优化网络干扰结构下的治疗分配，以实现高效的学习结果。通过将我们的方法应用于模拟数据和腾讯游戏数据集，我们证明了其在减少数据需求的情况下实现准确效果估计的可行性。这种 ACI 方法标志着因果推理数据效率领域的重大进步，为传统方法提供了强大而高效的替代方案，特别是在以复杂干扰模式为特征的场景中。]]></description>
      <guid>https://arxiv.org/abs/2402.12710</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>最大边际自由度的界限</title>
      <link>https://arxiv.org/abs/2402.12885</link>
      <description><![CDATA[arXiv:2402.12885v1 公告类型：新
摘要：常见的核岭回归在内存分配和计算时间上是昂贵的。本文解决了核岭回归的低秩近似和代理，从而克服了这些困难。该论文的基本贡献是低维近似的秩的下限，这是预测能力保持可靠所必需的。该界限将有效维度与最大统计杠杆分数相关联。我们通过涉及核的正则性来描述有效维度及其相对于正则化参数的增长行为。对于适当选择的内核，这种增长被证明是渐近对数的，证明了 Nystr\&quot;om 方法的低秩近似是合理的。]]></description>
      <guid>https://arxiv.org/abs/2402.12885</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>具有部分反馈的模式估计</title>
      <link>https://arxiv.org/abs/2402.13079</link>
      <description><![CDATA[arXiv:2402.13079v1 公告类型：新
摘要：轻监督预训练和在线微调的结合在最近的人工智能发展中发挥了关键作用。这些新的学习途径需要新的理论框架。在本文中，我们通过一个简单的问题形式化了弱监督和主动学习的核心方面：使用部分反馈估计分布模式。我们展示了熵编码如何允许从部分反馈中获取最佳信息，为模式识别开发足够粗略的统计数据，并使老虎机算法适应我们的新设置。最后，我们将这些贡献结合到一个统计和计算上有效的解决方案中来解决我们的问题。]]></description>
      <guid>https://arxiv.org/abs/2402.13079</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>随机化可以减少偏差和方差：随机森林的案例研究</title>
      <link>https://arxiv.org/abs/2402.12668</link>
      <description><![CDATA[arXiv:2402.12668v1 公告类型：新
摘要：我们研究了一个经常被忽视的现象，首先在 \cite{breiman2001random} 中指出，即与 bagging 相比，随机森林似乎减少了偏差。受 \cite{mentch2020randomization} 一篇有趣论文的启发，作者认为随机森林降低了有效自由度，并且仅在低信噪比 (SNR) 设置中优于装袋集成，我们探索随机森林如何发现模式在装袋丢失的数据中。我们凭经验证明，在存在这种模式的情况下，随机森林会减少偏差和方差，并且当信噪比较高时，随机森林的性能会逐渐优于装袋集成。我们的观察提供了对随机森林在各种 SNR 的现实世界成功的见解，并增强了我们对随机森林和 bagging 集成之间在注入每个分割的随机化方面的差异的理解。我们的研究还对随机森林中调整 $mtry$ 的重要性产生了实际的见解。]]></description>
      <guid>https://arxiv.org/abs/2402.12668</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>奇点下的学习：改进 WBIC 和 sBIC 的信息标准</title>
      <link>https://arxiv.org/abs/2402.12762</link>
      <description><![CDATA[arXiv:2402.12762v1 公告类型：新
摘要：我们引入了一种新颖的信息准则（IC），称为奇点学习（LS），旨在增强广泛适用的贝叶斯信息准则（WBIC）和奇异贝叶斯信息准则（sBIC）的功能。 LS 是有效的，没有规律性约束，并且表现出稳定性。 Watanabe 将统计模型或学习机定义为常规模型，如果从参数到概率分布的映射是一对一的，并且其 Fisher 信息矩阵是正定的。相反，不满足这些条件的模型被称为奇异模型。在过去的十年中，已经提出了几种针对奇异案例的信息标准，包括 WBIC 和 sBIC。 WBIC适用于非常规场景，但面临样本量大和已知学习系数冗余估计的挑战。相反，sBIC 由于依赖于最大似然估计，因此其更广泛的应用受到限制。 LS 通过增强 WBIC 和 sBIC 的实用性来解决这些限制。它结合了广泛适用信息准则 (WAIC) 的经验损失来表示统计模型的拟合优度，以及类似于 sBIC 的惩罚项。这种方法为模型选择提供了一种灵活而稳健的方法，不受规律性约束。]]></description>
      <guid>https://arxiv.org/abs/2402.12762</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>带裁剪的 SGD 正在秘密估计中值梯度</title>
      <link>https://arxiv.org/abs/2402.12828</link>
      <description><![CDATA[arXiv:2402.12828v1 公告类型：新
摘要：随机优化有多种应用，可以从稳健的梯度估计中受益。例如，节点损坏的分布式学习、训练数据中存在大异常值、隐私约束下的学习，甚至是由于算法本身的动态性而产生的重尾噪声等领域。在这里，我们使用基于中值估计的稳健梯度估计器来研究 SGD。我们首先考虑计算样本之间的中值梯度，并表明即使在重尾、状态相关的噪声下，所得方法也可以收敛。然后，我们推导基于随机近点法的迭代方法来计算几何中位数及其概括。最后，我们提出了一种估计迭代中的中值梯度的算法，并发现几种众所周知的方法 - 特别是不同形式的裁剪 - 是该框架的特殊情况。]]></description>
      <guid>https://arxiv.org/abs/2402.12828</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>多类时态逻辑神经网络</title>
      <link>https://arxiv.org/abs/2402.12397</link>
      <description><![CDATA[arXiv:2402.12397v1 公告类型：新
摘要：时间序列数据可以表示自主系统的行为，例如无人机和自动驾驶汽车。二元分类和多类分类问题在该领域受到了广泛关注。神经网络代表了一种流行的数据分类方法；然而，它们缺乏可解释性，这对从中提取有意义的信息构成了重大挑战。信号时序逻辑（STL）是一种描述定时行为属性的形式主义。我们提出了一种结合上述所有方法的方法：代表时间序列数据多类分类的 STL 规范的神经网络。我们提供了两个关键贡献：1）我们引入了多类分类的边际概念，2）我们引入了基于 STL 的属性的使用来增强结果的可解释性。我们在两个数据集上评估我们的方法，并与最先进的基线进行比较。]]></description>
      <guid>https://arxiv.org/abs/2402.12397</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>FAST：透明机器学习中快速加法分割的优化框架</title>
      <link>https://arxiv.org/abs/2402.12630</link>
      <description><![CDATA[arXiv:2402.12630v1 公告类型：新
摘要：我们提出了 FAST，一种用于快速加性分割的优化框架。 FAST 对数据集中的每个特征进行分段常量形状函数，以生成透明的附加模型。该框架利用一种新颖的优化程序来拟合这些模型，其速度比现有最先进的方法（例如可解释的增强机 \citep{nori2019interpretml}）快 $\sim$2 个数量级。我们还在 FAST 框架中开发了新的特征选择算法，以适应性能良好的简约模型。通过实验和案例研究，我们表明 FAST 提高了加性模型的计算效率和可解释性。]]></description>
      <guid>https://arxiv.org/abs/2402.12630</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    </channel>
</rss>