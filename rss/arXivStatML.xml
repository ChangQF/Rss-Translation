<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 04 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用 EM 和 AM 算法进行混合线性回归的不可知学习</title>
      <link>https://arxiv.org/abs/2406.01149</link>
      <description><![CDATA[arXiv:2406.01149v1 公告类型：新
摘要：混合线性回归是参数统计和机器学习中一个研究得很好的问题。给定一组样本、协变量和标签的元组，混合线性回归的任务是找到最适合样本的线性关系的小列表。通常假设标签是通过随机选择两个或多个线性函数中的一个，将所选函数应用于协变量，并可能将噪声引入结果而随机生成的。在这种情况下，目标是估计地面真实线性函数直到某个参数误差。流行的期望最大化 (EM) 和交替最小化 (AM) 算法之前已经为此进行了分析。
在本文中，我们考虑了更普遍的问题，即从样本中进行混合线性回归的不可知学习，而没有这样的生成模型。具体来说，我们表明，在可分离性和良好初始化的标准条件下，AM 和 EM 算法通过收敛到群体损失最小化器（对于适当定义的损失函数）来实现混合线性回归中的不可知学习。从某种意义上说，这显示了 AM 和 EM 算法的强度，即使在没有可实现的生成模型的情况下，它们也能收敛到“最优解”。]]></description>
      <guid>https://arxiv.org/abs/2406.01149</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:48 GMT</pubDate>
    </item>
    <item>
      <title>学习等变张量函数及其在稀疏向量恢复中的应用</title>
      <link>https://arxiv.org/abs/2406.01552</link>
      <description><![CDATA[arXiv:2406.01552v1 公告类型：新
摘要：这项工作描述了从张量输入元组到张量输出的等变多项式函数。受物理学的启发，我们专注于正交群对张量的对角作用的等变函数。我们展示了如何将这种表征扩展到其他线性代数群，包括洛伦兹群和辛群。
这些表征背后的目标是定义等变机器学习模型。特别是，我们专注于稀疏向量估计问题。这个问题在理论计算机科学文献中得到了广泛的研究，通过平方和技术得出的显式谱方法可以在某些假设下恢复稀疏向量。我们的数值结果表明，所提出的等变机器学习模型可以学习谱方法，这些方法在某些情况下优于理论上已知的最佳谱方法。实验还表明，学习谱方法可以解决尚未经过理论分析的环境中的问题。
这是一个有前途的方向的例子，理论可以为机器学习模型提供参考，机器学习模型也可以为理论提供参考。]]></description>
      <guid>https://arxiv.org/abs/2406.01552</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:48 GMT</pubDate>
    </item>
    <item>
      <title>利用双随机梯度揭秘 SGD</title>
      <link>https://arxiv.org/abs/2406.00920</link>
      <description><![CDATA[arXiv:2406.00920v1 公告类型：新
摘要：以难解期望和的形式呈现的优化目标的重要性日益提升（例如，扩散模型、变分自动编码器等等），这种设置也称为“无限数据的有限和”。对于这些问题，一种流行的策略是采用具有双随机梯度的 SGD（双 SGD）：使用每个组件的梯度估计量来估计期望，而通过对这些估计量进行子采样来估计和。尽管它很受欢迎，但人们对双 SGD 的收敛特性知之甚少，除非在有界方差等强假设下。在这项工作中，我们在一般条件下建立了具有独立小批量和随机重新洗牌的双 SGD 的收敛性，其中包括依赖组件梯度估计量。特别是，对于依赖估计量，我们的分析允许对效果相关性进行细粒度分析。因此，在每次迭代的计算预算为 $b \times m$（其中 $b$ 是小批量大小，$m$ 是蒙特卡洛样本数量）的情况下，我们的分析表明了应该将大部分预算投入到哪里。此外，我们证明了随机重组 (RR) 可以改善对子采样噪声的复杂性依赖性。]]></description>
      <guid>https://arxiv.org/abs/2406.00920</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:47 GMT</pubDate>
    </item>
    <item>
      <title>分布式细化网络：通过深度学习进行分布式预测</title>
      <link>https://arxiv.org/abs/2406.00998</link>
      <description><![CDATA[arXiv:2406.00998v1 公告类型：新
摘要：精算建模中的一项关键任务涉及对损失的分布特性进行建模。经典（分布）回归方法（如广义线性模型 (GLM；Nelder 和 Wedderburn，1972））通常被使用，但在开发能够（i）允许协变量灵活地影响条件分布的不同方面，（ii）整合机器学习和人工智能的发展以在考虑（i）的同时最大化预测能力，以及（iii）在模型中保持一定程度的可解释性以增强对模型及其输出的信任，这在追求（i）和（ii）的努力中往往会受到损害。我们通过提出一种分布细化网络 (DRN) 来解决这个问题，它将固有可解释的基线模型（例如 GLM）与灵活的神经网络相结合——一种改进的深度分布回归 (DDR；Li 等人，2019) 方法。受组合精算神经网络 (CANN；Schelldorfer 和 W{\&#39;&#39;u}thrich，2019) 的启发，我们的方法灵活地细化了整个基线分布。因此，DRN 可以捕捉所有分位数中特征的不同影响，从而提高预测性能，同时保持足够的可解释性。使用合成数据和真实数据，我们展示了 DRN 卓越的分布预测能力。DRN 有可能成为精算科学及其他领域中强大的分布回归模型。]]></description>
      <guid>https://arxiv.org/abs/2406.00998</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:47 GMT</pubDate>
    </item>
    <item>
      <title>最优臂上有兼容性条件的 Lasso Bandit</title>
      <link>https://arxiv.org/abs/2406.00823</link>
      <description><![CDATA[arXiv:2406.00823v1 公告类型：新
摘要：我们考虑一个随机稀疏线性老虎机问题，其中只有稀疏的上下文特征子集会影响预期的奖励函数，即未知的奖励参数具有稀疏结构。在现有的 Lasso 老虎机文献中，兼容性条件与上下文特征上的额外多样性条件一起被施加以实现仅对数依赖于环境维度 $d$ 的遗憾界限。在本文中，我们证明，即使没有额外的多样性假设，仅在最优臂上的兼容性条件也足以得出对数依赖于 $d$ 的遗憾界限，并且我们的假设严格弱于单参数设置下套索老虎机文献中使用的假设。我们提出了一种采用强制采样技术的算法，并证明所提出的算法在边际条件下实现了 $O(\text{poly}\log dT)$ 遗憾。据我们所知，在单一参数设置下，所提出的算法在 Lasso bandit 算法中所需的假设最弱，可实现 $O(\text{poly}\log dT)$ 遗憾。通过数值实验，我们证实了所提算法的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2406.00823</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:46 GMT</pubDate>
    </item>
    <item>
      <title>因果推理的双重稳健学习教程</title>
      <link>https://arxiv.org/abs/2406.00853</link>
      <description><![CDATA[arXiv:2406.00853v1 公告类型：新
摘要：双重稳健学习通过整合倾向得分和结果建模，为从观察数据进行因果推断提供了一个稳健的框架。尽管它在理论上很有吸引力，但由于感知到的复杂性和难以访问的软件，实际应用仍然有限。本教程旨在揭开双重稳健方法的神秘面纱，并使用 EconML 包展示它们的应用。我们介绍了因果推断，讨论了结果建模和倾向得分的原理，并通过模拟案例研究说明了双重稳健方法。通过简化方法并提供实际的编码示例，我们打算让数据科学和统计学的研究人员和从业者能够使用双重稳健学习。]]></description>
      <guid>https://arxiv.org/abs/2406.00853</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:46 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的上下文学习是贝叶斯的吗？马丁格尔的观点</title>
      <link>https://arxiv.org/abs/2406.00793</link>
      <description><![CDATA[arXiv:2406.00793v1 公告类型：新
摘要：上下文学习 (ICL) 已成为大型语言模型 (LLM) 的一个特别显着的特征：给定一个预先训练的 LLM 和一个观察到的数据集，LLM 可以对来自相同分布的新数据点进行预测而无需微调。许多作品都将 ICL 假设为近似贝叶斯推理，使其成为一个自然假设。在这项工作中，我们通过马丁格尔性质从新的角度分析了这一假设，马丁格尔性质是贝叶斯学习系统对可交换数据的基本要求。我们表明，马丁格尔性质是此类场景中明确预测的必要条件，并且使可信、安全关键系统中至关重要的原则性、分解的不确定性概念成为可能。我们得出了可操作的检查，并给出了相应的理论和测试统计数据，如果满足马丁格尔性质，这些检查必须成立。我们还检查了当观察到更多数据时，LLM 中的不确定性是否会如贝叶斯学习中预期的那样减少。在三项实验中，我们提供了违反鞅性质的证据，以及偏离贝叶斯不确定性缩放行为的证据，从而推翻了 ICL 是贝叶斯的假设。]]></description>
      <guid>https://arxiv.org/abs/2406.00793</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:45 GMT</pubDate>
    </item>
    <item>
      <title>用于扩散目标生成的协方差自适应序贯黑盒优化</title>
      <link>https://arxiv.org/abs/2406.00812</link>
      <description><![CDATA[arXiv:2406.00812v1 公告类型：新 
摘要：扩散模型在为图像、自然语言、蛋白质领域等生成高质量内容方面表现出巨大潜力。然而，如何通过仅使用用户的黑盒目标分数的扩散模型进行用户偏好的目标生成仍然具有挑战性。为了解决这个问题，我们首先将与预训练扩散模型相关的目标保留时间随机微分方程 (SDE) 的微调公式化为顺序黑盒优化问题。此外，我们提出了一种新的协方差自适应顺序优化算法来优化未知过渡动力学下的累积黑盒分数。从理论上讲，我们证明了累积凸函数的 $O(\frac{d^2}{\sqrt{T}})$ 收敛速度，而无需平滑和强凸假设。从经验上讲，在数值测试问题和目标引导的 3D 分子生成任务上的实验都表明我们的方法在实现更好的目标分数方面具有卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.00812</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:45 GMT</pubDate>
    </item>
    <item>
      <title>重新审视逻辑变分贝叶斯</title>
      <link>https://arxiv.org/abs/2406.00713</link>
      <description><![CDATA[arXiv:2406.00713v1 公告类型：新
摘要：变分逻辑回归是一种流行的近似贝叶斯推理方法，广泛应用于机器学习的许多领域，包括：贝叶斯优化、强化学习和多示例学习等等。然而，由于证据下限的难解性，作者转而使用蒙特卡洛、求积或边界进行推理，这些方法成本高昂或对真实后验的近似值较差。
在本文中，我们引入了 softplus 函数期望的新边界，随后展示了如何将其应用于变分逻辑回归和高斯过程分类。与其他边界不同，我们的提议不依赖于扩展变分族，也不引入额外的参数来确保边界的紧密性。事实上，我们表明这个界限比最先进的技术更严格，并且由此产生的变分后验达到了最先进的性能，同时计算速度比蒙特卡罗方法快得多。]]></description>
      <guid>https://arxiv.org/abs/2406.00713</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:44 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图学习的贝叶斯联合加性因子模型</title>
      <link>https://arxiv.org/abs/2406.00778</link>
      <description><![CDATA[arXiv:2406.00778v1 公告类型：新
摘要：在各种应用环境中，在同一组样本上收集多种不同类型的数据越来越普遍。我们在本文中特别关注的是研究这种多视图特征与响应之间的关系。在精准医疗的背景下出现了一个激励应用，其中收集多组学数据以与临床结果相关联。在结合多模态信息以改进结果预测的同时，推断视图内和视图之间的依赖关系是很有意义的。信噪比在不同视图之间可能会有很大差异，这促使人们使用比标准晚期和早期融合更细致的统计工具。这一挑战伴随着保持可解释性、选择特征和获得准确的不确定性量化的需求。我们提出了一种联合加性因子回归模型 (JAFAR)，该模型具有结构化的加性设计，考虑了共享和特定于视图的组件。我们通过一种新颖的依赖累积收缩过程 (D-CUSP) 先验确保可识别性。我们通过部分折叠的 Gibbs 采样器提供有效的实现，并扩展我们的方法以允许灵活的特征和结果分布。从免疫组、代谢组和蛋白质组数据预测分娩开始时间表明，与最先进的竞争对手相比，我们的性能有所提高。我们的开源软件（R 包）可在 https://github.com/niccoloanceschi/jafar 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.00778</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:44 GMT</pubDate>
    </item>
    <item>
      <title>一种不影响性能的批量顺序减半算法</title>
      <link>https://arxiv.org/abs/2406.00424</link>
      <description><![CDATA[arXiv:2406.00424v1 公告类型：新
摘要：在本文中，我们研究了多臂老虎机背景下的纯探索问题，特别关注以固定大小的批次拉动手臂的场景。批处理已被证明可以提高计算效率，但由于反馈延迟和适应性降低，它可能会导致与原始顺序算法相比性能下降。我们引入了一个简单的批处理版本的顺序减半 (SH) 算法 (Karnin 等人，2013)，并提供理论证据，证明批处理在实际条件下不会降低原始算法的性能。此外，我们通过实验验证了我们的说法，证明了 SH 算法在固定大小批次设置中的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2406.00424</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:43 GMT</pubDate>
    </item>
    <item>
      <title>时间点过程循环神经网络的非渐近理论</title>
      <link>https://arxiv.org/abs/2406.00630</link>
      <description><![CDATA[arXiv:2406.00630v1 公告类型：新
摘要：时间点过程 (TPP) 是建模和预测各个领域不规则时间事件的重要工具。最近，基于循环神经网络 (RNN) 的 TPP 已显示出优于传统参数 TPP 模型的实际优势。然而，在目前的文献中，从理论角度理解神经 TPP 仍处于起步阶段。在本文中，我们在许多众所周知的 TPP 设置下建立了 RNN-TPP 的超额风险界限。我们特别表明，不超过四层的 RNN-TPP 可以实现消失的泛化误差。我们的技术贡献包括多层 RNN 类的复杂性的表征、用于近似动态事件强度函数的 $\tanh$ 神经网络的构建以及用于缓解无界事件序列问题的截断技术。我们的结果弥合了 TPP 的应用与神经网络理论之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2406.00630</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:43 GMT</pubDate>
    </item>
    <item>
      <title>结合实验数据和历史数据进行政策评估</title>
      <link>https://arxiv.org/abs/2406.00317</link>
      <description><![CDATA[arXiv:2406.00317v1 公告类型：新
摘要：本文研究了使用多个数据源的策略评估，特别是在涉及一个具有两个分支的实验数据集以及由单个控制分支下生成的历史数据集补充的场景中。我们提出了新颖的数据集成方法，该方法线性集成基于实验和历史数据构建的基本策略值估计量，并优化权重以最小化所得组合估计量的均方误差 (MSE)。我们进一步应用悲观原则来获得更稳健的估计量，并将这些发展扩展到顺序决策。从理论上讲，我们为我们提出的估计量的 MSE 建立了非渐近误差界限，并在广泛的奖励转移场景中推导出它们的预言、效率和稳健性属性。来自一家拼车公司的数值实验和基于真实数据的分析证明了所提出的估计量的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2406.00317</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:42 GMT</pubDate>
    </item>
    <item>
      <title>隐马尔可夫过程混合的表示和去交织</title>
      <link>https://arxiv.org/abs/2406.00416</link>
      <description><![CDATA[arXiv:2406.00416v1 公告类型：新 
摘要：隐马尔可夫过程（HMP）混合的去交织通常取决于其表示模型。现有的表示模型考虑马尔可夫链混合而不是隐马尔可夫，导致对观测噪声或缺失观测等非理想情况缺乏鲁棒性。此外，去交织方法采用基于搜索的策略，这非常耗时。为了解决这些问题，本文提出了一种新的HMP混合表示模型和相应的去交织方法。首先，设计了一个用于表示HMP混合的生成模型。随后，将去交织过程公式化为生成模型的后验推理。其次，开发了一种精确推理方法来最大化完整数据的似然，并开发了两种近似推理方法，通过创建易处理的结构来最大化证据下限。然后，使用似然比检验得出理论误差概率下限，并表明算法合理地接近该下限。最后，模拟结果表明，所提出的方法对于非理想情况非常有效且稳健，在模拟和实际数据上的表现优于基准方法。]]></description>
      <guid>https://arxiv.org/abs/2406.00416</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:42 GMT</pubDate>
    </item>
    <item>
      <title>稳定边缘的训练是由逐层雅可比对齐引起的</title>
      <link>https://arxiv.org/abs/2406.00127</link>
      <description><![CDATA[arXiv:2406.00127v1 公告类型：新
摘要：在神经网络训练期间，训练损失的 Hessian 矩阵的锐度会不断上升，直到训练处于稳定边缘。因此，即使是非随机梯度下降也不能准确地模拟由训练损失的梯度流定义的底层动力系统。我们使用指数欧拉求解器来训练网络而不进入稳定边缘，以便我们准确地近似真实的梯度下降动态。我们通过实验证明，Hessian 矩阵锐度的增加是由于网络的分层雅可比矩阵变得对齐引起的，因此网络输入附近的网络预激活的微小变化会导致网络输出的巨大变化。我们进一步证明，对齐程度与数据集的大小呈幂律关系，判定系数在 0.74 到 0.98 之间。]]></description>
      <guid>https://arxiv.org/abs/2406.00127</guid>
      <pubDate>Wed, 05 Jun 2024 03:17:41 GMT</pubDate>
    </item>
    </channel>
</rss>