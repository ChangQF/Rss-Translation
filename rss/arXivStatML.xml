<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 09 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>贝尔曼保形推理：校准时间序列的预测区间</title>
      <link>https://arxiv.org/abs/2402.05203</link>
      <description><![CDATA[我们引入贝尔曼保形推理 (BCI)，这是一个包含任何时间序列预测模型并提供校准预测区间的框架。与现有方法不同，BCI 能够利用多步提前预测，并通过在每个时间步解决一维随机控制问题 (SCP) 来显式优化平均间隔长度。特别是，我们使用动态规划算法来寻找 SCP 的最优策略。我们证明，即使多步提前预测不佳，BCI 也能在任意分布变化和时间依赖性下实现长期覆盖。我们根据经验发现，与现有方法相比，BCI 避免了无限长度的无信息区间，并在波动性预测问题上产生了更短的预测区间。]]></description>
      <guid>https://arxiv.org/abs/2402.05203</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:17 GMT</pubDate>
    </item>
    <item>
      <title>结构化强盗中贝叶斯固定预算最佳臂识别的先验相关分配</title>
      <link>https://arxiv.org/abs/2402.05878</link>
      <description><![CDATA[我们研究结构化老虎机中的贝叶斯固定预算最佳臂识别（BAI）问题。我们提出了一种基于先验信息和环境结构使用固定分配的算法。我们提供了其在不同模型中的性能的理论界限，包括线性和分层 BAI 的第一个依赖于先验的上限。我们的主要贡献是引入新的证明方法，与现有方法相比，这些方法可以为多臂 BAI 提供更严格的界限。我们将我们的方法与其他固定预算 BAI 方法进行了广泛比较，证明了其在各种环境下的一致和稳健的性能。我们的工作提高了我们对结构化老虎机中的贝叶斯固定预算 BAI 的理解，并强调了我们的方法在实际场景中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.05878</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:16 GMT</pubDate>
    </item>
    <item>
      <title>生存分析的在线学习方法</title>
      <link>https://arxiv.org/abs/2402.05145</link>
      <description><![CDATA[我们引入了用于生存分析的在线数学框架，允许实时适应动态环境和审查数据。该框架能够通过最佳二阶在线凸优化算法——在线牛顿步（ONS）来估计事件时间分布。这种以前未被探索过的方法具有显着的优势，包括具有非渐近收敛保证的显式算法。此外，我们分析了 ONS 超参数的选择，它取决于 exp-concavity 属性，并对遗憾界限有显着影响。我们提出了一种随机方法，保证 ONS 的对数随机遗憾。此外，我们引入了一种自适应聚合方法，可确保超参数选择的鲁棒性，同时保持快速后悔边界。本文的研究结果可以扩展到生存分析领域之外，并且适用于任何以差的指数凹性和不稳定的 ONS 为特征的案例。最后，通过模拟实验来说明这些主张。]]></description>
      <guid>https://arxiv.org/abs/2402.05145</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:16 GMT</pubDate>
    </item>
    <item>
      <title>理解变压器中的感应偏置：无穷大的视角</title>
      <link>https://arxiv.org/abs/2402.05173</link>
      <description><![CDATA[我们在无限超参数化高斯过程极限中研究 Transformer 中的归纳偏差，并认为 Transformer 倾向于偏向于序列空间中更多的排列对称函数。我们证明，当数据集与标记之间的排列对称时，对称群的表示理论可用于给出定量分析预测。我们提出了一个简化的变压器块，并在极限下求解模型，包括对学习曲线和网络输出的准确预测。我们表明，在常见的设置中，我们可以以缩放定律的形式导出可学习性作为上下文长度的函数的严格界限。最后，我们认为维基文本数据集确实具有一定程度的排列对称性。]]></description>
      <guid>https://arxiv.org/abs/2402.05173</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:16 GMT</pubDate>
    </item>
    <item>
      <title>具有通用激活的 \emph{wide} 隐藏层树状神经网络的精确容量</title>
      <link>https://arxiv.org/abs/2402.05719</link>
      <description><![CDATA[\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23}中研究\emph{树状委员会机}（TCM）神经网络（NN）的最新进展表明，随机对偶理论（RDT）及其\emph{部分提升}（pl RDT） ）变体是强大的工具，可用于非常精确的网络容量分析。在这里，我们考虑 \emph{wide} 隐藏层网络，并发现 \cite{Stojnictcmspnncapdiffactrdt23} 中面临的数值困难的某些方面奇迹般地消失了。特别是，我们采用最近开发的 \emph{完全提升} (fl) RDT 来表征 \emph{wide} ($d\rightarrow \infty$) TCM 网络容量。我们获得了非常通用的隐藏层激活类别的显式、封闭形式的容量表征。虽然所使用的方法显着降低了所需数值评估的数量，但最终 fl RDT 的有用性和成功仍然需要剩余数值工作的大部分。为了获得具体的容量值，我们采用四个非常著名的激活示例：\emph{\textbf{ReLU}}、\textbf{\emph{quadratic}}、\textbf{\emph{erf}} 和 \textbf{\ emph{tanh}}。在成功地对它们进行所有剩余数值工作后，我们发现整个提升机制表现出非常快速的收敛，相对改进不超过 $\sim 0.1\%$ 已经在第三级提升上发生。作为一个方便的奖励，我们还发现，在第一级和第二级提升上获得的容量特征与通过 \cite{ZavPeh21} 中的通用和 \cite{BalMalZech19} 中的统计物理复制理论方法获得的容量特征精确匹配ReLU 激活。]]></description>
      <guid>https://arxiv.org/abs/2402.05719</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:15 GMT</pubDate>
    </item>
    <item>
      <title>Transformers 如何进行上下文自回归学习？</title>
      <link>https://arxiv.org/abs/2402.05787</link>
      <description><![CDATA[Transformers 在语言建模任务中取得了最先进的性能。然而，他们巨大成功背后的原因仍不清楚。在本文中，为了更好地理解，我们在简单的下一个标记预测任务上训练 Transformer 模型，其中序列是作为一阶自回归过程 $s_{t+1} = W s_t$ 生成的。我们展示了经过训练的 Transformer 如何通过首先在上下文中学习 $W$，然后应用预测映射来预测下一个标记。我们将由此产生的过程称为上下文自回归学习。更准确地说，关注交换正交矩阵 $W$，我们首先表明，在考虑增强标记时，训练有素的单层线性 Transformer 实现一步梯度下降，以最小化内部目标函数。当标记未增强时，我们表征单层对角线性多头 Transformer 的全局最小值。重要的是，我们展示了头之间的正交性，并表明位置编码捕获了数据中的三角关系。在实验方面，我们考虑非交换正交矩阵的一般情况并概括我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2402.05787</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:15 GMT</pubDate>
    </item>
    <item>
      <title>协作非参数双样本测试</title>
      <link>https://arxiv.org/abs/2402.05715</link>
      <description><![CDATA[本文解决了图结构设置中的多个两个样本测试问题，这是空间统计和神经科学等领域的常见场景。固定图中的每个节点 $v$ 处理两个特定于节点的概率密度函数 (pdf) $p_v$ 和 $q_v$ 之间的双样本测试问题。目标是在假设连接的节点会产生类似的测试结果的情况下，识别应拒绝原假设 $p_v = q_v$ 的节点。我们提出了非参数协作双样本测试（CTST）框架，该框架有效地利用图结构并最小化对 $p_v$ 和 $q_v$ 的假设。我们的方法整合了 f 散度估计、核方法和多任务学习的元素。我们使用综合实验和检测地震活动的真实传感器网络来证明 CTST 优于独立应用于每个节点的最先进的非参数统计测试，因此忽略了问题的几何形状。]]></description>
      <guid>https://arxiv.org/abs/2402.05715</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:14 GMT</pubDate>
    </item>
    <item>
      <title>REMEDI：改进神经熵估计的校正变换</title>
      <link>https://arxiv.org/abs/2402.05718</link>
      <description><![CDATA[信息论量在机器学习中发挥着核心作用。最近数据和模型的复杂性激增增加了对这些数量的准确估计的需求。然而，随着维度的增长，估计提出了重大挑战，现有方法已经在相对较低的维度中陷入困境。为了解决这个问题，在这项工作中，我们引入了 $\texttt{REMEDI}$ 来高效准确地估计微分熵（一种基本的信息理论量）。该方法结合了简单、自适应基础模型的交叉熵的最小化以及根据数据密度的相对熵来估计它们的偏差。我们的方法展示了广泛的估计任务的改进，包括对合成数据和自然数据的熵估计。此外，我们将重要的理论一致性结果扩展到我们的方法所需的更广义的设置。我们说明了该框架如何自然地扩展到信息理论监督学习模型，特别关注信息瓶颈方法。事实证明，与信息瓶颈中的现有方法相比，该方法具有更好的准确性。此外，我们还探索了 $\texttt{REMEDI}$ 与使用拒绝采样和 Langevin 动力学的生成建模之间的自然联系。]]></description>
      <guid>https://arxiv.org/abs/2402.05718</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:14 GMT</pubDate>
    </item>
    <item>
      <title>通过随机近似梯度的非参数工具变量回归</title>
      <link>https://arxiv.org/abs/2402.05639</link>
      <description><![CDATA[本文提出了 SAGD-IV，这是一种通过采用随机近似梯度进行非参数工具变量 (NPIV) 回归的新颖框架，以最大限度地减少预计的人口风险。工具变量（IV）在计量经济学中被广泛使用，以解决存在不可观察混杂因素的估计问题，并且机器学习社区投入了大量精力来改进现有方法并在 NPIV 环境中设计新方法，众所周知，NPIV 环境是一种病态的方法。提出的线性逆问题。我们为我们的算法提供理论支持，并通过实证实验进一步例证其竞争性能。此外，我们还解决了二元结果的情况，并取得了有希望的结果，该情况并没有像其持续的对应物那样受到社区的足够关注。]]></description>
      <guid>https://arxiv.org/abs/2402.05639</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:13 GMT</pubDate>
    </item>
    <item>
      <title>对抗训练的高维模型：几何和权衡</title>
      <link>https://arxiv.org/abs/2402.05674</link>
      <description><![CDATA[这项工作研究了高维体系中基于边缘的线性分类器背景下的对抗性训练，其中维度 $d$ 和数据点数量 $n$ 以固定比率 $\alpha = n / d$ 发散。我们引入了一种易于处理的数学模型，可以研究数据和对抗性攻击者几何形状之间的相互作用，同时捕获对抗性鲁棒性文献中观察到的核心现象。我们的主要理论贡献是在一般凸和不增加损失下对对抗性经验风险最小化器的充分统计的精确渐近描述。我们的结果使我们能够精确地表征数据中的哪些方向与更高的泛化/鲁棒性权衡相关，如鲁棒性和有用性指标所定义的。特别是，我们揭示了可以在不影响准确性的情况下捍卫方向的存在。最后，我们展示了在训练期间防御非鲁棒特征的优势，将统一的保护确定为本质上有效的防御机制。]]></description>
      <guid>https://arxiv.org/abs/2402.05674</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:13 GMT</pubDate>
    </item>
    <item>
      <title>固定宽度树状神经网络容量分析——通用激活</title>
      <link>https://arxiv.org/abs/2402.05696</link>
      <description><![CDATA[我们考虑\emph{树状委员会机器}（TCM）神经网络的容量。依靠随机对偶理论（RDT），\cite{Stojnictcmspnncaprdt23} 最近引入了一个用于容量分析的通用框架。然后在 \cite{Stojnictcmspnncapliftedrdt23} 中提出了基于所谓的 \emph{部分提升} RDT (pl RDT) 的升级。这两条线的工作都集中在具有最典型的 \emph{sign} 激活的网络上。另一方面，在这里，我们关注具有其他更通用的激活类型的网络，并表明 \cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} 的框架也足够强大，可以处理此类场景。除了标准的\emph{线性}激活之外，我们发现两种非常常用的激活，即\emph{二次}和\emph{修正线性单元（ReLU）}激活，可以获得特别方便的结果。更具体地说，对于每个激活，我们获得了给定（偶数）个隐藏层神经元 $d$ 的 \emph{any} 的基于 RDT 和 pl RDT 的记忆容量上限表征。在此过程中，我们还发现了以下两个相当引人注目的事实：1）与常识相反，两组结果都表明，当$d$（隐藏层的宽度）较大时，边界能力会减小，同时收敛到恒定值； 2）精确地具有 \textbf{\emph{two}} 隐藏层神经元的网络实现了最大边界容量！此外，观察到大的$d$收敛值与基于统计物理复制理论的预测非常一致。]]></description>
      <guid>https://arxiv.org/abs/2402.05696</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:13 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降导致深度非线性网络的权重与经验 NTK 之间的对齐</title>
      <link>https://arxiv.org/abs/2402.05271</link>
      <description><![CDATA[了解神经网络从输入标签对中提取统计数据的机制是监督学习中最重要的未解决问题之一。先前的工作已经确定，一般架构的经过训练的神经网络中的权重的克矩阵与模型的平均梯度外积成正比，在称为神经特征 Ansatz (NFA) 的声明中。然而，人们对这些数量在训练过程中变得相关的原因知之甚少。在这项工作中，我们解释了这种相关性的出现。我们发现 NFA 相当于权重矩阵的左奇异结构和与这些权重相关的经验神经正切核的重要组成部分之间的对齐。我们确定之前的工作中引入的 NFA 是由隔离这种对齐的中心 NFA 驱动的。我们表明，可以在早期训练时根据输入和标签的简单统计来分析预测 NFA 的开发速度。最后，我们引入了一种简单的干预措施来增加任意给定层的 NFA 相关性，从而显着提高学习特征的质量。]]></description>
      <guid>https://arxiv.org/abs/2402.05271</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:12 GMT</pubDate>
    </item>
    <item>
      <title>无似然推理中滋扰参数下的分类和广义标签偏移</title>
      <link>https://arxiv.org/abs/2402.05330</link>
      <description><![CDATA[一个开放的科学挑战是，当我们拥有数据生成过程的机械模型，但训练数据和目标数据之间的标签和潜在干扰参数的分布不同时，如何通过可靠的不确定性测量对事件进行分类。我们将这种类型的分布偏移称为广义标签偏移（GLS）。使用观测数据 $\mathbf{X}$ 作为协变量进行直接分类会导致预测有偏差，并且标签 $Y$ 的不确定性估计无效。我们通过提出一种鲁棒不确定性量化的新方法来克服这些偏差，该方法将分类作为有害参数下的假设检验问题。关键思想是估计分类器在整个干扰参数空间中的接收器操作特性 (ROC)，这使我们能够设计出在 GLS 下不变的截止值。我们的方法有效地赋予预训练的分类器域适应能力，并返回有效的预测集，同时保持高功率。我们利用来自现实机械模型的数据展示了其在生物学和天体粒子物理学中两个具有挑战性的科学问题上的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.05330</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:12 GMT</pubDate>
    </item>
    <item>
      <title>元学习策略镜像下降中的镜像映射</title>
      <link>https://arxiv.org/abs/2402.05187</link>
      <description><![CDATA[策略镜像下降（PMD）是强化学习中的一种流行框架，作为包含众多算法的统一视角。这些算法是通过选择镜像映射导出的，并享有有限时间收敛保证。尽管 PMD 很受欢迎，但对其全部潜力的探索是有限的，大多数研究都集中在特定的镜像映射（即负熵）上，这产生了著名的自然策略梯度（NPG）方法。现有的理论研究尚不确定镜像图的选择是否显着影响 PMD 的功效。在我们的工作中，我们进行了实证调查，以表明传统的镜像映射选择 (NPG) 在多个标准基准环境中通常会产生不太理想的结果。通过应用元学习方法，我们确定了更有效的镜像映射，可以提高性能，无论是平均性能还是沿着训练轨迹实现的最佳性能。我们分析这些学习到的镜像映射的特征，并揭示某些设置之间的共同特征。我们的结果表明，镜像贴图具有适应各种环境的潜力，从而提出了如何将镜像贴图与环境的结构和特征最佳匹配的问题。]]></description>
      <guid>https://arxiv.org/abs/2402.05187</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:11 GMT</pubDate>
    </item>
    <item>
      <title>专家偏差高斯混合中的参数估计</title>
      <link>https://arxiv.org/abs/2402.05220</link>
      <description><![CDATA[我们考虑专家偏差高斯混合中的参数估计问题，其中数据由 $(1 - \lambda^{\ast}) g_0(Y| X)+ \lambda^{\ast} \sum_{i 生成= 1}^{k_{\ast}} p_{i}^{\ast} f(Y|(a_{i}^{\ast})^{\top}X+b_i^{\ast},\ sigma_{i}^{\ast})$，其中$X、Y$分别是协变量向量和响应变量，$g_{0}(Y|X)$是已知函数，$\lambda^{\ ast} \in [0, 1]$ 为真，但混合比例未知，且 $(p_{i}^{\ast}, a_{i}^{\ast}, b_{i}^{\ast}, $1 \leq i \leq k^{\ast}$ 的 \sigma_{i}^{\ast})$ 是专家高斯混合的未知参数。当我们想要测试数据是从 $g_{0}(Y|X)$ 生成（零假设）还是从整个混合物生成（替代假设）时，这个问题是由拟合优度检验产生的。基于专家函数的代数结构以及 $g_0$ 和混合部分之间的可区分性，我们构造了新颖的基于 Voronoi 的损失函数来捕获模型的最大似然估计 (MLE) 的收敛速度。我们进一步证明，我们提出的损失函数比广义 Wasserstein 更准确地描述了参数估计的局部收敛率，广义 Wasserstein 是一种通常用于估计专家高斯混合中参数的损失函数。]]></description>
      <guid>https://arxiv.org/abs/2402.05220</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:11 GMT</pubDate>
    </item>
    </channel>
</rss>