<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 29 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过选择性推理进行自动特征工程的统计测试</title>
      <link>https://arxiv.org/abs/2410.19768</link>
      <description><![CDATA[arXiv:2410.19768v1 公告类型：新
摘要：自动特征工程 (AFE) 在开发实用的机器学习管道中起着至关重要的作用，它自动将原始数据转换为有意义的特征，从而提高模型性能。通过以数据驱动的方式生成特征，AFE 能够发现可能通过人类经验或直觉无法发现的重要特征。另一方面，由于 AFE 基于数据生成特征，因此存在这些特征可能过度适应数据的风险，因此必须适当评估其可靠性。不幸的是，由于大多数 AFE 问题都被表述为组合搜索问题并通过启发式算法解决，因此从理论上量化生成特征的可靠性一直具有挑战性。为了解决这个问题，我们提出了一种基于选择性推理框架的 AFE 算法生成特征的新统计测试。作为概念证明，我们考虑了一类简单的基于树搜索的启发式 AFE 算法，并考虑了在线性模型中使用生成的特征时测试它们的问题。所提出的测试可以以 $p$ 值的形式量化生成的特征的统计显著性，从而从理论上保证对错误发现的风险的控制。]]></description>
      <guid>https://arxiv.org/abs/2410.19768</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用对称小批量分割朗之万动力学从贝叶斯神经网络后验中采样</title>
      <link>https://arxiv.org/abs/2410.19780</link>
      <description><![CDATA[arXiv:2410.19780v1 公告类型：新
摘要：我们提出了一种可扩展的动力学朗之万动力学算法，用于大数据和人工智能应用的采样参数空间。我们的方案将小批量的对称前向/后向扫描与朗之万动力学的对称离散化相结合。对于特定的朗之万分裂方法 (UBU)，我们表明，尽管每次迭代只使用一个小批量，但得到的对称小批量分裂-UBU (SMS-UBU) 积分器在维度 $d&gt;0$ 和步长 $h&gt;0$ 上具有偏差 $O(h^2 d^{1/2})$，从而可以出色地控制采样偏差作为步长的函数。我们应用该算法探索贝叶斯神经网络 (BNN) 后验分布的局部模式，并评估具有卷积神经网络架构的神经网络在三个不同数据集（Fashion-MNIST、Celeb-A 和胸部 X 光）上分类问题的后验预测概率的校准性能。我们的结果表明，与标准训练方法和随机权重平均相比，使用 SMS-UBU 采样的 BNN 可以提供明显更好的校准性能。]]></description>
      <guid>https://arxiv.org/abs/2410.19780</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高维高斯混合数据分类中的统计推断</title>
      <link>https://arxiv.org/abs/2410.19950</link>
      <description><![CDATA[arXiv:2410.19950v1 公告类型：新
摘要：我们考虑具有一般协方差矩阵的两个高斯高维混合的分类问题。使用统计物理学中的复制方法，我们研究了高维极限中一类一般正则化凸分类器的渐近行为，其中样本大小 $n$ 和维度 $p$ 都趋近于无穷大，而它们的比率 $\alpha=n/p$ 保持不变。我们的重点是估计量的泛化误差和变量选择属性。具体而言，基于分类器的分布极限，我们构建了一个去偏估计量，通过适当的假设检验程序执行变量选择。以 $L_1$ 正则化逻辑回归为例，我们进行了大量的计算实验，以确认我们的分析结果与有限大小系统中的数值模拟一致。我们还探讨了协方差结构对去偏估计量性能的影响。]]></description>
      <guid>https://arxiv.org/abs/2410.19950</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Clipped SGD 进行近乎最优的流式重尾统计估计</title>
      <link>https://arxiv.org/abs/2410.20135</link>
      <description><![CDATA[arXiv:2410.20135v1 公告类型：新
摘要：我们考虑了流式设置中的高维重尾统计估计问题，由于内存限制，这比传统的批处理设置要困难得多。我们将这个问题视为具有重尾随机梯度的随机凸优化，并证明，只要随机梯度噪声的二阶矩有限，广泛使用的 Clipped-SGD 算法就能获得接近最优的亚高斯统计率。更准确地说，使用 $T$ 个样本，我们表明对于平滑和强凸目标，Clipped-SGD 的误差为 $\sqrt{\frac{\mathsf{Tr}(\Sigma)+\sqrt{\mathsf{Tr}(\Sigma)\|\Sigma\|_2}\log(\frac{\log(T)}{\delta})}{T}}$，概率为 $1-\delta$，其中 $\Sigma$ 是截断梯度的协方差。请注意，波动（取决于 $\frac{1}{\delta}$）的阶数低于项 $\mathsf{Tr}(\Sigma)$。这改进了 Clipped-SGD 的当前最佳速率 $\sqrt{\frac{\mathsf{Tr}(\Sigma)\log(\frac{1}{\delta})}{T}}$，该速率仅适用于平滑和强凸目标。我们的结果还扩展到平滑凸和 Lipschitz 凸目标。我们结果的关键是针对马丁格尔集中的新型迭代细化策略，改进了 Catoni 和 Giulini 的 PAC-Bayes 方法。]]></description>
      <guid>https://arxiv.org/abs/2410.20135</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大规模联合网络中的稳健模型评估</title>
      <link>https://arxiv.org/abs/2410.20250</link>
      <description><![CDATA[arXiv:2410.20250v1 公告类型：新
摘要：在本文中，我们解决了使用可用源网络的测量结果来认证机器学习模型在看不见的目标网络上的性能的挑战。我们专注于这样一种场景，其中异构数据集分布在客户端的源网络中，所有客户端都连接到中央服务器。具体来说，考虑一个由 $K$ 个客户端组成的源网络“A”，每个客户端都持有来自独特和异构分布的私有数据，这些数据被认为是来自更广泛的元分布 $\mu$ 的独立样本。我们的目标是为模型在另一个看不见的目标网络“B”上的性能提供认证保证，该网络由另一个元分布 $\mu&#39;$ 控制，假设 $\mu$ 和 $\mu&#39;$ 之间的偏差受 Wasserstein 距离或 $f$ 散度的限制。我们推导出模型经验平均损失的理论保证，并为风险 CDF 提供统一的界限，其中后者对应于 Glivenko-Cantelli 定理和 Dvoretzky-Kiefer-Wolfowitz (DKW) 不等式的新颖且具有对抗鲁棒性的版本。我们的界限可以在多项式时间内计算出来，对 $K$ 个客户端进行多项式数量的查询，通过仅查询模型对私人数据的（潜在对抗性）损失来保护客户端隐私。我们还建立了非渐近泛化界限，随着 $K$ 和最小客户端样本量的增长，该界限始终收敛到零。广泛的实证评估验证了我们的界限在实际任务中的鲁棒性和实用性。]]></description>
      <guid>https://arxiv.org/abs/2410.20250</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于贝叶斯加性回归树的高斯过程极限</title>
      <link>https://arxiv.org/abs/2410.20289</link>
      <description><![CDATA[arXiv:2410.20289v1 公告类型：新
摘要：贝叶斯加性回归树 (BART) 是一种名声鹊起的非参数贝叶斯回归技术。它是一个决策树总和模型，在某种意义上是提升的贝叶斯版本。在无限树的极限下，它等同于高斯过程 (GP) 回归。这个极限是已知的，但尚未导致任何有用的分析或应用。我第一次推导并计算了精确的 BART 先验协方差函数。用它将 BART 的无限树极限实现为 GP 回归。通过实证测试，我表明这个极限在固定配置下比标准 BART 更差，但同时以自然 GP 方式调整超参数会产生一种有竞争力的方法，尽管适当调整的 BART 仍然更胜一筹。使用 BART 的 GP 替代方法的优势在于分析可能性，这简化了模型构建并避开了复杂的 BART MCMC。更一般地说，这项研究开辟了理解和开发 BART 和 GP 回归的新方法。BART 作为 GP 的实现可在 Python 包 https://github.com/Gattocrucco/lsqfitgp 中找到。]]></description>
      <guid>https://arxiv.org/abs/2410.20289</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过斯蒂费尔流形上的测地线哈密顿蒙特卡罗实现低秩贝叶斯矩阵补全</title>
      <link>https://arxiv.org/abs/2410.20318</link>
      <description><![CDATA[arXiv:2410.20318v1 公告类型：新
摘要：我们提出了一种基于采样的新方法，用于高效计算低秩贝叶斯矩阵补全并量化相关的不确定性。首先，我们设计了一个基于低秩矩阵奇异值分解 (SVD) 参数化的新先验模型。我们的先验类似于非贝叶斯设置中使用的精髓核范数正则化，并通过将因子矩阵约束到 Stiefel 流形来强制因子矩阵中的正交性。然后，我们设计了一个测地线汉密尔顿蒙特卡罗（-within-Gibbs）算法来生成 SVD 因子矩阵的后验样本。我们证明我们的方法解决了标准 Gibbs 采样器在矩阵补全中使用的常见双矩阵分解中遇到的采样困难。更重要的是，测地线汉密尔顿采样器允许在具有比大多数现有贝叶斯矩阵完成文献中采用的典型高斯似然和高斯先验假设更普遍的可能性的情况下进行采样。我们展示了我们的方法在拟合小鼠蛋白质数据集的分类数据和 MovieLens 推荐问题中的应用。数值示例展示了卓越的采样性能，包括更好的混合和更快的收敛到平稳分布。此外，它们在我们考虑的两个现实世界基准问题上展示了更高的准确性。]]></description>
      <guid>https://arxiv.org/abs/2410.20318</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Fisher-Rao 梯度流的核近似</title>
      <link>https://arxiv.org/abs/2410.20622</link>
      <description><![CDATA[arXiv:2410.20622v1 公告类型：新
摘要：本文旨在回答核方法和 PDE 梯度流接口中的一些未解决的问题。受机器学习（特别是在生成建模和采样方面）的最新进展的启发，我们对 Fisher-Rao 和 Wasserstein 型梯度流的梯度结构、流方程及其核近似进行了严格的研究。具体来说，我们专注于 Fisher-Rao（也称为 Hellinger）几何及其各种基于核的近似，使用 PDE 梯度流和最优传输理论的工具开发了一个原则性的理论框架。我们还提供了最大均值差异 (MMD) 空间中梯度流的完整表征，并与现有的学习和推理算法建立了联系。我们的分析揭示了将 Fisher-Rao 流、Stein 流、核差异和非参数回归联系起来的精确理论见解。然后，我们严格证明核近似 Fisher-Rao 流的进化 $\Gamma$ 收敛，提供超越逐点收敛的理论保证。最后，我们使用亥姆霍兹-瑞利原理分析能量耗散，建立力学经典理论与现代机器学习实践之间的重要联系。我们的结果为通过严格的梯度流和变分方法视角理解和分析机器学习应用中梯度流的近似提供了统一的理论基础。]]></description>
      <guid>https://arxiv.org/abs/2410.20622</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>逻辑赌博机中的近似最优纯探索</title>
      <link>https://arxiv.org/abs/2410.20640</link>
      <description><![CDATA[arXiv:2410.20640v1 公告类型：新
摘要：由于 Bandit 算法在现实场景中的实际应用，它引起了广泛关注。然而，除了多臂或线性 Bandit 等简单设置之外，最佳算法仍然很少。值得注意的是，在广义线性模型 (GLM) Bandit 的背景下，纯探索问题不存在最优解。在本文中，我们缩小了这一差距，并开发了第一个用于逻辑 Bandit 下的一般纯探索问题的跟踪和停止算法，称为逻辑跟踪和停止 (Log-TS)。Log-TS 是一种有效的算法，它渐近地匹配对数因子的预期样本复杂度的实例特定下限的近似值。]]></description>
      <guid>https://arxiv.org/abs/2410.20640</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ReLU 门的注入能力</title>
      <link>https://arxiv.org/abs/2410.20646</link>
      <description><![CDATA[arXiv:2410.20646v1 公告类型：新
摘要：我们考虑 ReLU 网络层的注入性。确定 ReLU 注入容量（层输入和输出的数量之比）与确定所谓的 $\ell_0$ 球形感知器的容量同构。采用 \emph{完全提升随机对偶理论} (fl RDT)，开发了一个强大的程序，用于处理 $\ell_0$ 球形感知器和隐式 ReLU 层注入性。为了将整个 fl RDT 机制投入实际使用，还进行了大量的数值评估。观察到提升机制收敛速度非常快，估计量的相对校正在第三级提升中已经不超过 $\sim 0.1\%$。还揭示了关键提升参数之间的闭式显式分析关系。这些关系除了在处理所有必需的数值工作中具有极其重要的意义外，还为提升结构中优美的参数互连提供了新的见解。最后，所得到的结果也与 [40] 中的复制预测相当接近。]]></description>
      <guid>https://arxiv.org/abs/2410.20646</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对低维数据的深度联邦学习的统计分析</title>
      <link>https://arxiv.org/abs/2410.20659</link>
      <description><![CDATA[arXiv:2410.20659v1 公告类型：新
摘要：联邦学习 (FL) 已成为协作机器学习的突破性范例，强调分散模型训练以解决数据隐私问题。虽然在优化联邦学习方面取得了重大进展，但对泛化误差的探索，特别是在异构环境中，一直受到限制，主要集中在参数情况上。本文研究了两阶段采样模型中深度联邦回归的泛化特性。我们的研究结果强调，当使用适当的网络大小时，由熵维度定义的内在维度对于确定收敛速度至关重要。具体而言，如果响应和解释变量之间的真实关系由 $\beta$-H\&quot;older 函数表征，并且有来自 $m$ 个参与客户端的 $n$ 个独立且相同分布 (i.i.d.) 样本，则参与客户端的错误率最多为 $\tilde{O}\left((mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))}\right)$，而对于非参与客户端，它的扩展为 $\tilde{O}\left(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))}\right)$。这里， $\bar{d}_{2\beta}(\lambda)$ 表示 $\lambda$ 的 $2\beta$ 熵维度，即解释变量的边际分布，$\Delta$ 表示采样阶段之间的依赖关系。我们的结果明确考虑了客户端的“接近度”，表明深度联邦学习器的收敛速度取决于内在的高维性，而不是名义上的高维性。]]></description>
      <guid>https://arxiv.org/abs/2410.20659</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过高斯近似推理进行似然近似</title>
      <link>https://arxiv.org/abs/2410.20754</link>
      <description><![CDATA[arXiv:2410.20754v1 公告类型：新
摘要：非高斯似然对于建模复杂的现实世界观测至关重要，但在学习和推理方面带来了重大的计算挑战。即使有高斯先验，非高斯似然也常常会导致难以分析的后验，从而需要近似方法。为此，我们提出了一种有效的方案，通过基于变分推理和变换基中的矩匹配的高斯密度来近似非高斯似然的影响。这使得最初为具有高斯似然的模型设计的有效推理策略得以部署。我们的实证结果表明，所提出的匹配策略在大规模点估计和分布推理设置中对二分类和多类分类实现了良好的近似质量。在具有挑战性的流式问题中，所提出的方法优于精确模型中所有现有的似然近似和近似推理方法。作为副产品，我们表明，对于神经网络分类，所提出的近似对数似然是原始标签上最小二乘法的更好替代方案。]]></description>
      <guid>https://arxiv.org/abs/2410.20754</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有平滑总变差距离的核指数族的稳健估计</title>
      <link>https://arxiv.org/abs/2410.20760</link>
      <description><![CDATA[arXiv:2410.20760v1 公告类型：新
摘要：在统计推断中，我们通常假设样本与预先指定的统计模型中包含的概率分布是独立且同分布的。然而，这种假设在实践中经常被违反。即使是一个意想不到的极端样本，称为{\it outlier}，也会对经典估计量产生重大影响。稳健统计研究如何构建可靠的统计方法，即使理想假设被违反，这些方法也能有效地工作。最近，一些研究表明，诸如 Tukey 中位数之类的稳健估计量可以通过生成对抗网络 (GAN) 很好地近似，生成对抗网络是一种使用神经网络的复杂生成模型的流行学习方法。GAN 被认为是一种使用积分概率度量 (IPM) 的学习方法，它是概率分布的差异度量。然而，在大多数对 Tukey 中位数及其基于 GAN 的近似的理论分析中，高斯或椭圆分布被假设为统计模型。在本文中，我们探讨了类似 GAN 的估计量在一般统计模型中的应用。作为统计模型，我们考虑包含有限维和无限维模型的核指数族。为了构建稳健估计量，我们提出平滑总变分 (STV) 距离作为 IPM 类。然后，我们从理论上研究了基于 STV 的估计量的稳健性。我们的分析表明，基于 STV 的估计量对核指数族的分布污染具有稳健性。此外，我们分析了蒙特卡洛近似方法的预测精度，该方法避免了归一化常数的计算难度。]]></description>
      <guid>https://arxiv.org/abs/2410.20760</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于缩放的生成模型数据增强及其理论扩展</title>
      <link>https://arxiv.org/abs/2410.20780</link>
      <description><![CDATA[arXiv:2410.20780v1 公告类型：新
摘要：本文研究了生成模型的稳定学习方法，以实现高质量的数据生成。噪声注入通常用于稳定学习。然而，选择合适的噪声分布具有挑战性。最近开发的一种方法 Diffusion-GAN 通过使用具有时间步长相关鉴别器的扩散过程来解决此问题。我们研究了 Diffusion-GAN，并揭示了数据缩放是稳定学习和高质量数据生成的关键组成部分。基于我们的研究结果，我们提出了一种使用数据缩放和基于方差的正则化的学习算法 Scale-GAN。此外，我们从理论上证明数据缩放控制估计误差界限的偏差-方差权衡。作为理论扩展，我们考虑具有可逆数据增强的 GAN。在基准数据集上的比较评估证明了我们的方法在提高稳定性和准确性方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.20780</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对双重难解分布的 Stein 梯度下降法</title>
      <link>https://arxiv.org/abs/2410.21021</link>
      <description><![CDATA[arXiv:2410.21021v1 公告类型：新
摘要：双重难解分布的贝叶斯推断具有挑战性，因为它们包含难解项，这些难解项是感兴趣参数的函数。尽管已经为此类模型开发了几种替代方案，但由于重复的辅助变量模拟，它们的计算量很大。我们提出了一种新颖的蒙特卡洛斯坦变分梯度下降 (MC-SVGD) 方法用于双重难解分布的推断。通过有效的梯度近似，我们的 MC-SVGD 方法可以快速变换任意参考分布以近似感兴趣的后验分布，而无需为后验预定义任何变分分布类。这种传输图是通过最小化再生核希尔伯特空间 (RKHS) 中变换分布和后验分布之间的 Kullback-Leibler 散度获得的。我们还研究了所提方法的收敛速度。我们说明了该方法在具有挑战性的例子中的应用，包括 Potts 模型、指数随机图模型和 Conway-Maxwell-Poisson 回归模型。与现有算法相比，所提出的方法实现了显著的计算增益，同时为后验分布提供了可比的推理性能。]]></description>
      <guid>https://arxiv.org/abs/2410.21021</guid>
      <pubDate>Tue, 29 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>