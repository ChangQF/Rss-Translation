<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Thu, 20 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>行为学习数据的模型选择以及对上下文强盗的应用</title>
      <link>https://arxiv.org/abs/2502.13186</link>
      <description><![CDATA[ARXIV：2502.13186V1公告类型：新 
摘要：对动物或人类的学习是导致更好适合环境的行为的过程。这个过程在很大程度上取决于学习的个人，通常仅通过个人的行为才能观察到。本文提出了使用此单个行为数据来找到最能解释个人学习方式的模型的方法。我们提出了两种模型选择方法：一般性固定程序和AIC类型标准，都适用于非平稳依赖性数据。我们为这些方法提供了与标准I.I.D.的这些方法的理论误差界限。案件。为了比较这些方法，我们将它们应用于上下文强盗模型，并说明它们在人类分类任务中的合成和实验学习数据上的使用。]]></description>
      <guid>https://arxiv.org/abs/2502.13186</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>任务转移：从分类到过度参数线性模型的回归</title>
      <link>https://arxiv.org/abs/2502.13285</link>
      <description><![CDATA[ARXIV：2502.13285V1公告类型：新 
摘要：现代的机器学习方法最近证明了在任务转移下概括的显着能力，其中潜在知识被转移到相似的数据分布下的另一个更困难的任务。我们在过度参数化的线性回归设置中研究了这种现象，在该设置中，任务从训练期间的分类转变为评估过程中的回归。在零射击情况下，没有回归数据，我们证明，对于任何高斯协变量分布，稀疏信号和随机信号模型都是不可能的。在有限的回归数据可用的几种情况下，我们提出了一种简单的后处理算法，该算法渐近地恢复了地面真相预测因子。我们的分析利用了可能具有独立感兴趣的最小值插值引起的单个参数的细粒度表征。我们的结果表明，尽管用于分类的最小值插值剂无法将回归转移到先验的回归中，但它们经历了令人惊讶的结构化衰减，从而可以通过有限的附加数据成功进行任务转移。]]></description>
      <guid>https://arxiv.org/abs/2502.13285</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有效的基于置换的核两样本测试</title>
      <link>https://arxiv.org/abs/2502.13570</link>
      <description><![CDATA[ARXIV：2502.13570V1公告类型：新 
摘要：两样本假设检验决定是否从相同的分布中绘制了两组数据，这是统计和机器学习中的基本问题，并具有广泛的科学应用。在非参数测试的背景下，由于其灵活性和强大的理论基础，最大的平均差异（MMD）已成为测试统计数据。但是，它在大规模场景中的使用受到高计算成本的困扰。在这项工作中，我们使用nyStr \“ OM的MMD近似值来设计计算效率和实用的测试算法，同时保留统计保证。我们的主要结果是在提议的分布的幂上绑定了有限样本，这些样本是足够的分布的幂。相对于MMD分离。实验，强调现实的科​​学数据。]]></description>
      <guid>https://arxiv.org/abs/2502.13570</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过学习窄带光谱内核来推断图信号推断</title>
      <link>https://arxiv.org/abs/2502.13686</link>
      <description><![CDATA[ARXIV：2502.13686V1公告类型：新 
摘要：图形信号分析中的一个共同假设是信号的平滑度或频谱的带限性，但在许多情况下，实际图数据的光谱可能集中在光谱的多个区域，可能包括中高高频组件。在这项工作中，我们提出了一个新型的图形信号模型，其中信号频谱通过图频域中的窄带内核的组合表示。然后，我们提出了一种算法，该算法通过优化内核参数和来自图形信号集合的信号表示系数共同学习模型。我们的问题公式具有允许在不同图表上获得的信号纳入学习算法的灵活性。然后，我们从理论上研究了所提出方法的信号重建性能，还可以详细阐述何时在多个图上进行联合学习，而不是在每个图上学习单个模型。几个图数据集的实验结果表明，与文献中的各种参考方法相比，所提出的方法提供了令人满意的信号插值精度。]]></description>
      <guid>https://arxiv.org/abs/2502.13686</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>识别深层变量模型的度量结构</title>
      <link>https://arxiv.org/abs/2502.13757</link>
      <description><![CDATA[ARXIV：2502.13757V2公告类型：新 
摘要：深层可变模型学习了数据的凝结表示，希望能反映所研究现象的内部起作用。不幸的是，这些潜在表示在统计上是无法识别的，这意味着它们不能独特地确定。因此，域专家在解释这些专家时需要仔细踩踏。当前的解决方案通过对潜在变量模型的其他约束（例如通过需要标记的培训数据，或限制模型的表达性。我们改变了目标：我们没有识别潜在变量，而是确定它们之间的关系，例如有意义的距离，角度和卷。我们证明这在非常温和的模型条件下是可行的，没有其他标记的数据。我们从经验上证明，我们的理论会导致更可靠的潜在距离，从而在从深层变量模型中提取值得信赖的结论中提供了有原则的途径。]]></description>
      <guid>https://arxiv.org/abs/2502.13757</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫链的不确定性量化，并应用于时间差异学习</title>
      <link>https://arxiv.org/abs/2502.13822</link>
      <description><![CDATA[ARXIV：2502.13822V1公告类型：新 
摘要：马尔可夫链是统计机器学习的基础，基于马尔可夫链蒙特卡洛（MCMC）采样和时间差异（TD）学习（RL）。鉴于它们的广泛使用，至关重要的是建立严格的概率确保其融合，不确定性和稳定性。在这项工作中，我们为马尔可夫链的向量和矩阵值函数开发了新颖的高维浓度不平等和浆果 - 评估界限，从而解决了现有理论工具的关键局限性来处理依赖数据。我们利用这些结果来分析TD Learning算法，这是一种在RL中用于策略评估的方法。我们的分析产生了急剧的高概率一致性保证，该保证与对数因素与对数因素相匹配。此外，我们建立了一个$ O（t^{ -  \ frac {1} {4}} \ log t）$分布收敛速率，用于以凸距离测量的TD估计器的高斯近似值。这些发现为RL算法的统计推断提供了新的见解，弥合了经典随机近似理论与现代强化学习应用之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2502.13822</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度的计算优势：学习具有梯度下降的高维层次功能</title>
      <link>https://arxiv.org/abs/2502.13961</link>
      <description><![CDATA[ARXIV：2502.13961V1公告类型：新 
摘要：了解由梯度下降（GD）训练的深神经网络与浅层模型相比的优势仍然是一个公开的理论挑战。尽管对高维数据的多指数模型的研究为GD训练的神经网络的益处提供了分析性见解，而深度在改善GD培训网络中深度的作用在改善样本复杂性和泛化方面的作用仍然鲜为人知。在本文中，我们介绍了包含潜在子空间维度层次结构的一类目标函数（单个和多索引高斯层次目标）。该框架使我们能够分析与高维限制的浅网络相比，深层网络的学习动力和泛化性能。具体而言，我们的主要定理表明，具有GD的特征学习可以降低有效维度，从而将高维问题转化为一系列较低维度的问题。这使得与浅网络相比，以大幅度的样本学习目标功能。尽管结果在受控的培训环境中得到了证明，但我们还讨论了更常见的培训程序，并认为它们通过相同的机制学习。这些发现为进一步的定量研究开辟了道路，以了解深度在学习层次结构中的重要作用。]]></description>
      <guid>https://arxiv.org/abs/2502.13961</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过智能数据质量评估来增强机器学习性能：无监督的以数据为中心的框架</title>
      <link>https://arxiv.org/abs/2502.13198</link>
      <description><![CDATA[ARXIV：2502.13198V1公告类型：交叉 
摘要：数据质量差会限制机器学习（ML）的优势能力，并削弱了高性能的ML软件系统。如今，由于数量和复杂性的增加，数据更容易容易出现质量差的风险。因此，在ML管道中进一步移动之前，乏味且耗时的工作将用于数据制备和改进。为了应对这一挑战，我们提出了一个以数据为中心的评估框架，该框架可以识别高质量的数据并改善ML系统的性能。提出的框架结合了质量测量和无监督学习的策划，以区分高质量和低质量数据。该框架旨在集成灵活和通用方法，以便将其部署在各种域和应用中。为了验证设计框架的结果，我们在分析化学领域的现实用例中实现了它，在该案例中，在三个抗Sensens寡核苷酸的数据集中对其进行了测试。咨询域专家以确定相关质量测量并评估框架的结果。结果表明，以质量为中心的数据评估框架确定了指导有效的实验室实验进行的高质量数据的特征，从而提高了ML系统的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.13198</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>作为贝叶斯正交的保形预测</title>
      <link>https://arxiv.org/abs/2502.13228</link>
      <description><![CDATA[ARXIV：2502.13228V1公告类型：交叉 
摘要：由于基于机器学习的预测系统越来越多地用于高风险情况，因此了解这种预测模型在部署时将如何执行。无分配不确定性量化技术（例如保形预测）即使隐藏了模型的细节，也会为黑框模型提供保证。但是，此类方法基于常见的概率，这过度限制了其适用性。从贝叶斯的角度来看，我们重新审视了共形预测的主要方面，从而阐明了常见保证的缺点。我们提出了一种基于贝叶斯正交的实用替代方案，该替代方案提供了可解释的保证，并提供了更丰富的代表可能在测试时观察到的损失范围。]]></description>
      <guid>https://arxiv.org/abs/2502.13228</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>早期停止梯度下降的好处用于过度参数化的逻辑回归</title>
      <link>https://arxiv.org/abs/2502.13283</link>
      <description><![CDATA[arxiv：2502.13283v1公告类型：交叉 
摘要：在过度参数化的逻辑回归中，梯度下降（GD）迭代差异，同时朝向最大$ \ ell_2 $ -margin解决方案 - 一种被称为GD隐式偏置的现象。这项工作调查了在明确指定的高维逻辑回归中早期停止引起的其他正则化效应。我们首先证明，早期停滞的GD的过多逻辑风险消失了，但在融合时的GD迭代方面分歧为无穷大。这表明早期停滞的GD是校准良好的，而渐近GD在统计学上是不一致的。其次，我们表明，要达到一个小过多的零风险，在多项式上，许多样本足以容纳早期停滞的GD，而对于任何插值估计器（包括渐近GD）来说，许多样品都是必需的。这种分离强调了在过度参数化政权中早期停止的统计益处。最后，我们基于早期停滞的GD和$ \ ell_2 $调查的经验风险最小化的规范和角度差异建立了非串扰界限，从而将GD的隐式正规化与显式$ \ ell_2 $ regarlization连接起来。]]></description>
      <guid>https://arxiv.org/abs/2502.13283</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于流量的生成模型作为概率空间中的迭代算法</title>
      <link>https://arxiv.org/abs/2502.13394</link>
      <description><![CDATA[ARXIV：2502.13394V1公告类型：交叉 
摘要：生成AI（Genai）通过在各种应用程序中促进高维数据的合成，包括图像产生，语言建模，生物医学信号处理和异常检测，彻底改变了数据驱动的建模。基于流量的生成模型为捕获复杂的概率分布提供了一个强大的框架，提供了精确的似然估计，有效的采样和分布之间的确定性转换。这些模型利用了由普通微分方程（ODE）控制的可逆映射，从而实现了精确的密度估计和似然评估。该教程为基于流量的生成模型提供了一个直观的数学框架，将它们作为基于神经网络的连续概率密度表示。我们探讨了关键的理论原理，包括Wasserstein度量，梯度流和受ODES管辖的密度进化，以建立与理论见解的融合保证和桥梁经验进步。通过提供严格但可访问的治疗方法，我们旨在为研究人员和从业人员提供必要的工具，以有效地将基于流量的生成模型应用于信号处理和机器学习中。]]></description>
      <guid>https://arxiv.org/abs/2502.13394</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Ogboost：用于序列梯度提升的Python软件包</title>
      <link>https://arxiv.org/abs/2502.13456</link>
      <description><![CDATA[ARXIV：2502.13456V1公告类型：交叉 
摘要：本文介绍了Ogboost，这是一种使用梯度提升的Scikit-Learn兼容Python软件包，用于序数回归。序数变量（例如，评分量表，质量评估）位于名义数据和连续数据之间，需要采用反映其固有顺序的专门方法。 Ogboost建立在用于优化和用于序数回归的潜在可变量框架的基础上，Ogboost执行了潜在的连续回归函数（功能梯度下降）和将潜在连续值转换为离散类别概率（经典类别概率）的关节优化梯度下降）。除了Scikit-Learn分类器的Stanadard方法外，渐变bloostingordinal类实现了“ decision_function”，该“ deciess_function”返回每个观测值的潜在函数的（标量）值，可以用作比较类标签的高分辨率替代方案和排名观察。该类可以选择使用交叉验证来提早停止，而不是单个保留验证集，这是针对小型和/或不平衡数据集的更强大的方法。此外，用户可以选择具有不同基础算法和/或超级参数的基础学习者，以在整个促进迭代中使用，从而导致一种“异质”集合方法，可以用作超级参数调整的更有效替代方案（例如，通过网格搜索）。我们使用UCI呼吸道的葡萄酒质量数据集说明了Ogboost的功能。该软件包可在PYPI上获得，可以通过“ PIP Install Ogboost”安装。]]></description>
      <guid>https://arxiv.org/abs/2502.13456</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>特征图的构造的一些见解，以学习与图神经网络的成对特征相互作用</title>
      <link>https://arxiv.org/abs/2502.13471</link>
      <description><![CDATA[ARXIV：2502.13471V1公告类型：交叉 
摘要：特征交互在预测机学习模型中至关重要，因为它捕获了影响模型性能的特征之间的关系。在这项工作中，我们专注于成对相互作用，并研究它们在构造图形神经网络（GNN）特征图中的重要性。我们没有提出新方法，而是利用现有的GNN模型和工具来探索特征图结构与它们在建模相互作用中的有效性之间的关系。通过对合成数据集的实验，我们发现交互特征之间的边缘对于使GNN能够有效地建模特征相互作用很重要。我们还观察到，包括非相互作用的边缘可以充当噪声，降低模型性能。此外，我们使用最小描述长度（MDL）原理为稀疏特征图选择提供了理论支持。我们证明，仅保留必要交互边缘的特征图比完整的图表产生更有效和可解释的表示形式，与Occam的剃须刀对齐。
  我们的发现提供了设计特征图的理论见解和实用指南，以提高GNN模型的性能和解释性。]]></description>
      <guid>https://arxiv.org/abs/2502.13471</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>平滑的归一化以进行有效的分布式私人优化</title>
      <link>https://arxiv.org/abs/2502.13482</link>
      <description><![CDATA[ARXIV：2502.13482V1公告类型：交叉 
摘要：联合学习可以使培训机器学习模型在保留参与者的隐私时。令人惊讶的是，没有差异性私有分布式方法来平滑，非凸优化问题。原因是标准隐私技术需要对参与者的贡献进行界限，这通常是通过更新的$ \ textit {剪裁} $强制执行的。现有文献通常通过假设梯度规范的界限或分析剪辑的分布算法的界限来忽略剪辑的效果，但忽略了DP约束。在这项工作中，我们通过$ \ textit {平滑的归一化} $研究了一种替代方法，该方法是由于其在单节点设置中的有利性能而动机的。通过将平滑的归一化与错误反馈机制集成在一起，我们设计了一种新的分布式算法$ \ alpha $  -  $ \ sf normec $。我们证明我们的方法比以前的工作达到了优越的收敛率。通过将$ \ alpha $  -  $ \ sf normec $扩展到DP设置，我们获得了具有可证明的融合保证的第一个差异私有分布式优化算法。最后，我们来自神经网络培训的经验结果表明，在不同的参数设置中，$ \ alpha $  -  $ \ sf normec $的强大收敛。]]></description>
      <guid>https://arxiv.org/abs/2502.13482</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>倒置随机采样矩阵的基本偏见，并应用于亚采样牛顿</title>
      <link>https://arxiv.org/abs/2502.13583</link>
      <description><![CDATA[ARXIV：2502.13583V1公告类型：交叉 
摘要：机器学习（ML）和随机数值线性代数（Randnla）的大量工作已经利用了各种随机素描方法，包括随机采样和随机投影，并使用Johnson-Lindenstrauss和Uspaspace Embing进行了许多分析技术。最近的研究已经确定了反转偏见的问题 - 尽管草图本身是无偏见的，但随机草图的倒置并非公正。这种偏见提出了在各种ML管道中使用随机草图的挑战，例如快速随机优化，可扩展的统计估计器和分布式优化。
  在随机投影的背景下，可以轻松纠正反转偏置的密集高斯预测（但是，对于许多应用而言，这太昂贵了）。最近的工作表明，如何纠正稀疏的高斯预测的反转偏差。在本文中，我们展示了如何纠正反转偏置，以获取基于均匀和非均匀杠杆的随机抽样方法，以及结构化的随机投影，包括基于Hadamard变换的偏差。使用这些结果，我们为子采样的牛顿方法建立了与问题无关的局部收敛速率。]]></description>
      <guid>https://arxiv.org/abs/2502.13583</guid>
      <pubDate>Thu, 20 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>