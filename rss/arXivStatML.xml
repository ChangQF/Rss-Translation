<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于离线非平稳强化学习的深度迁移 $Q$-Learning</title>
      <link>https://arxiv.org/abs/2501.04870</link>
      <description><![CDATA[arXiv:2501.04870v1 公告类型：新 
摘要：在商业和医疗保健领域的动态决策场景中，利用来自不同人群的样本轨迹可以显着提高针对特定目标人群的强化学习 (RL) 性能，尤其是在样本量有限的情况下。虽然现有的迁移学习方法主要侧重于线性回归设置，但它们缺乏对强化学习算法的直接适用性。本文开创了对非平稳有限时域马尔可夫决策过程建模的动态决策场景的迁移学习的研究，利用神经网络作为强大的函数逼近器和后向归纳学习。我们证明，在回归设置中有效的简单样本池策略在马尔可夫决策过程中失败。为了应对这一挑战，我们引入了一种新颖的“重新加权目标程序”来构建“可迁移的 RL 样本”，并提出了“迁移深度 $Q^*$ 学习”，从而实现具有理论保证的神经网络近似。我们假设奖励函数是可转移的，并处理转移密度可转移或不可转移的情况。我们在神经网络近似和转移密度转移中的转移学习分析技术具有更广泛的含义，扩展到使用神经网络和领域转移场景的监督转移学习。在合成数据集和真实数据集上进行的经验实验证实了我们的方法的优势，展示了其通过在非平稳强化学习环境中战略性地构建可转移的 RL 样本来改善决策的潜力。]]></description>
      <guid>https://arxiv.org/abs/2501.04870</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RieszBoost：Riesz 回归的梯度提升</title>
      <link>https://arxiv.org/abs/2501.04871</link>
      <description><![CDATA[arXiv:2501.04871v1 公告类型：新
摘要：回答因果问题通常涉及估计条件期望的线性函数，例如平均治疗效果或纵向修改的治疗政策的效果。根据 Riesz 表示定理，这些函数可以表示为结果的条件期望与 Riesz 表示器的预期乘积，Riesz 表示器是双重稳健估计方法中的关键组成部分。传统上，Riesz 表示器是通过推导其显式解析形式、估计其成分并将这些估计值代入已知形式（例如，逆倾向得分）来间接估计的。然而，推导或估计解析形式可能具有挑战性，并且替换方法通常对实际的正性违规很敏感，从而导致更高的方差和更宽的置信区间。在本文中，我们提出了一种新颖的梯度提升算法来直接估计 Riesz 表示器，而不需要其显式解析形式。该方法特别适用于表格数据，为现有的 Riesz 回归方法提供了一种灵活、非参数且计算效率高的替代方案。通过模拟研究，我们证明了我们的算法在一系列函数中的表现与间接估计技术相当甚至更好，为估计因果量提供了一种用户友好且强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2501.04871</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>工具变量回归的深度神经特征的最优性和自适应性</title>
      <link>https://arxiv.org/abs/2501.04898</link>
      <description><![CDATA[arXiv:2501.04898v1 公告类型：新
摘要：我们提供了深度特征工具变量 (DFIV) 回归 (Xu et al., 2021) 的收敛性分析，这是一种使用深度神经网络分两个阶段学习的数据自适应特征进行 IV 回归的非参数方法。我们证明，当目标结构函数位于 Besov 空间中时，DFIV 算法实现了极小最大最优学习率。这是在标准非参数 IV 假设下显示的，并且在给定工具的情况下对协变量的条件分布的规律性进行了额外的平滑性假设，这控制了第 1 阶段的难度。我们进一步证明，作为一种数据自适应算法，DFIV 在两个方面优于固定特征（核或筛）IV 方法。首先，当目标函数具有较低的空间同质性（即它既有平滑区域，也有尖锐/不连续区域）时，DFIV 仍能达到最佳速率，而固定特征方法则被证明是严格次优的。其次，与基于核的两阶段回归估计器相比，DFIV 在第 1 阶段样本中可证明具有更高的数据效率。]]></description>
      <guid>https://arxiv.org/abs/2501.04898</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>理解决策树中的偏见</title>
      <link>https://arxiv.org/abs/2501.04903</link>
      <description><![CDATA[arXiv:2501.04903v1 公告类型：新
摘要：长期以来，人们普遍认为机器学习模型在从不平衡数据中学习时会偏向多数（或负）类，从而导致它们忽略或忽略少数（或正）类。在这项研究中，我们表明这种信念对于决策树来说不一定正确，它们的偏见实际上可能朝着相反的方向。受最近一项模拟研究的启发，该研究表明决策树可能偏向少数类，我们的论文旨在调和该研究与数十年来其他研究之间的冲突。首先，我们批判性地评估了过去关于这个问题的文献，发现没有考虑数据生成过程导致了关于决策树偏见的错误结论。然后我们证明，在与预测因子相关的特定条件下，适合纯度并在只有一个正例的数据集上训练的决策树偏向少数类。最后，我们证明当有多个正例时，决策树中的分裂也会有偏差。我们的发现对于流行的基于树的模型（例如随机森林）的使用具有重要意义。]]></description>
      <guid>https://arxiv.org/abs/2501.04903</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稀疏模型中惩罚最小二乘法性能的非渐近分析</title>
      <link>https://arxiv.org/abs/2501.04946</link>
      <description><![CDATA[arXiv:2501.04946v1 公告类型：新
摘要：最小二乘 (LTS) 估计量是经典最小二乘估计量的著名稳健替代方案，在定位、回归、机器学习和 AI 文献中很受欢迎。关于 LTS 的研究很多，包括其稳健性、计算算法、非线性情况的扩展、渐近性等。LTS 已应用于高维实数据稀疏模型设置的惩罚回归，其中维度 $p$（以千为单位）远大于样本大小 $n$（以十或百为单位）。在这种实际情况下，样本量 $n$ 通常是具有特殊属性的子群体的数量（例如，阿尔茨海默氏症、帕金森氏症、白血病或 ALS 等患者的数量），而 N 是固定的有限大小。假设 $n$ 趋向于无穷大的渐近分析在这种场景下在实践上并不令人信服和合法。非渐近或有限样本分析将更为可取和可行。
本文首次建立了一些有限样本（非渐近）误差界限，用于基于 LTS 的高概率估计和预测。]]></description>
      <guid>https://arxiv.org/abs/2501.04946</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有不断发展的任务和性能保证的监督学习</title>
      <link>https://arxiv.org/abs/2501.05089</link>
      <description><![CDATA[arXiv:2501.05089v1 公告类型：新
摘要：多种监督学习场景由一系列分类任务组成。例如，多任务学习和持续学习旨在学习一系列固定或随时间增长的任务。现有的学习序列任务的技术是针对特定场景量身定制的，缺乏对其他场景的适应性。此外，大多数现有技术都考虑了序列中任务顺序无关紧要的情况。然而，序列中的任务通常会不断发展，因为连续的任务通常具有更高的相似性。本文提出了一种适用于多种监督学习场景并适应不断发展的任务的学习方法。与现有技术不同，我们提供可计算的严格性能保证，并分析表征有效样本量的增加。在基准数据集上的实验表明，所提出的方法在多种场景中的性能得到了改善，并且所提出的性能保证是可靠的。]]></description>
      <guid>https://arxiv.org/abs/2501.05089</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稳健得分匹配</title>
      <link>https://arxiv.org/abs/2501.05105</link>
      <description><![CDATA[arXiv:2501.05105v1 公告类型：新
摘要：Hyv\&quot;arinen (2005) 提出了一种不需要计算分布正则化常数的参数估计程序。在这项工作中，我们利用均值的几何中位数来开发一种稳健的分数匹配程序，该程序在观测数据被污染的情况下产生一致的参数估计。所提出方法的一个特别吸引人的地方是它在指数族模型中保留了凸性。因此，这种新方法对于非高斯、指数族图形模型特别有吸引力，因为这些模型的正则化常数的评估是难以实现的。当存在污染时，为此类模型提供了支持恢复保证。此外，在数值实验和降水数据集上研究了支持恢复。我们证明，当不存在污染时，所提出的稳健分数匹配估计器的性能与标准分数匹配估计器相当，但在存在污染的环境中，其性能大大优于该估计器。]]></description>
      <guid>https://arxiv.org/abs/2501.05105</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于评估个性化治疗效果的因果机器学习方法——从两项大型试验中获得的有效性见解</title>
      <link>https://arxiv.org/abs/2501.04061</link>
      <description><![CDATA[arXiv:2501.04061v1 公告类型：交叉 
摘要：因果机器学习 (ML) 方法通过估计个性化治疗效果，有望推动精准医疗的发展。然而，它们的可靠性在实证环境中仍未得到很大程度的验证。在这项研究中，我们使用两项大型随机对照试验的数据评估了 17 种主流因果异质性 ML 方法（包括元学习器、基于树的方法和深度学习方法）的内部和外部有效性：国际卒中试验 (N=19,435) 和中国急性卒中试验 (N=21,106)。我们的研究结果表明，无论是内部还是外部，没有任何 ML 方法能够可靠地验证其性能，在提出的评估指标上，训练数据和测试数据之间存在显着差异。即使在没有分布变化的情况下，从训练数据估计的个性化治疗效果也无法推广到测试数据。这些结果引发了人们对因果机器学习模型在精准医疗中当前适用性的担忧，并强调需要更强大的验证技术来确保普遍性。]]></description>
      <guid>https://arxiv.org/abs/2501.04061</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络中确定性不确定性量化的概率跳过连接</title>
      <link>https://arxiv.org/abs/2501.04816</link>
      <description><![CDATA[arXiv:2501.04816v1 公告类型：交叉 
摘要：深度学习中的确定性不确定性量化 (UQ) 旨在通过利用网络特征提取器的输出，通过一次通过网络来估计不确定性。现有方法要求特征提取器既敏感又平滑，确保有意义的输入变化会在特征向量中产生有意义的变化。平滑度可以实现泛化，而灵敏度可以防止特征崩溃，其中不同的输入被映射到相同的特征向量。为了满足这些要求，当前的确定性方法通常使用谱归一化重新训练网络。我们建议使用神经崩溃度量来识别既敏感又平滑的现有中间层，而不是修改训练。然后，我们将概率模型拟合到该中间层的特征向量，我们称之为概率跳过连接 (PSC)。通过实证分析，我们探讨了谱归一化对神经崩溃的影响，并证明 PSC 可以有效地解开随机不确定性和认知不确定性。此外，我们表明 PSC 实现了不确定性量化和分布外 (OOD) 检测性能，可与需要训练修改的现有单次通过方法相媲美甚至超过它们。通过改造现有模型，PSC 无需重新训练即可实现高质量的 UQ 和 OOD 功能。]]></description>
      <guid>https://arxiv.org/abs/2501.04816</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>k-匿名条件下的定向在线广告覆盖率测量、优化和频次上限</title>
      <link>https://arxiv.org/abs/2501.04882</link>
      <description><![CDATA[arXiv:2501.04882v1 公告类型：交叉 
摘要：近年来，在线广告在提升品牌知名度方面的使用增长很大程度上归因于社交媒体的普及。促成在线品牌广告成功的一项关键技术是频次上限，这是一种使营销人员能够控制向特定用户展示广告的次数的机制。然而，随着行业倾向于优先考虑用户隐私的广告解决方案，这项技术的基础正在受到审查。本文深入探讨了在 $k$ 匿名性的背景下的覆盖率测量和优化问题，$k$ 匿名性是一种在主要在线广告平台上越来越受欢迎的隐私保护模型。我们概述了如何在这个新的隐私环境中报告覆盖率，并展示了如何使用概率折扣（传统频次上限的概率改编）来优化广告系列效果。进行实验以评估用户隐私与在线品牌广告效果之间的权衡。值得注意的是，只要引入隐私保护，我们就会发现性能会大幅下降，但对于广告平台而言，为用户提供更多隐私只需支付有限的额外成本。]]></description>
      <guid>https://arxiv.org/abs/2501.04882</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习紧阿贝尔群上的卷积算子</title>
      <link>https://arxiv.org/abs/2501.05279</link>
      <description><![CDATA[arXiv:2501.05279v1 公告类型：交叉 
摘要：我们考虑学习与紧致阿贝尔群相关的卷积算子的问题。我们研究一种基于正则化的方法并提供相应的学习保证，讨论卷积核的自然规律性条件。更准确地说，我们假设卷积核是平移不变希尔伯特空间中的函数，并分析自然岭回归 (RR) 估计量。基于现有的 RR 结果，我们用有限样本界限来描述估计量的准确性。有趣的是，在 RR 分析中经典的规律性假设在空间/频率局部化方面具有新颖而自然的解释。理论结果通过数值模拟得到说明。]]></description>
      <guid>https://arxiv.org/abs/2501.05279</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过推测抽样加速扩散模型</title>
      <link>https://arxiv.org/abs/2501.05370</link>
      <description><![CDATA[arXiv:2501.05370v1 公告类型：交叉 
摘要：推测采样是一种流行的技术，用于加速大型语言模型中的推理，通过使用快速草稿模型生成候选标记并根据目标模型的分布接受或拒绝它们。虽然推测采样以前仅限于离散序列，但我们将其扩展到扩散模型，该模型通过连续的向量值马尔可夫链生成样本。在这种情况下，目标模型是一种高质量但计算成本高的扩散模型。我们提出了各种起草策略，包括一种简单有效的方法，该方法不需要训练草稿模型并且可以开箱即用地应用于任何扩散模型。我们的实验表明，各种扩散模型的生成速度显着加快，将函数评估次数减半，同时从目标模型生成精确样本。]]></description>
      <guid>https://arxiv.org/abs/2501.05370</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高维纠缠均值估计</title>
      <link>https://arxiv.org/abs/2501.05425</link>
      <description><![CDATA[arXiv:2501.05425v1 公告类型：交叉 
摘要：我们研究信号子集模型中的高维纠缠均值估计任务。具体而言，给定 $\mathbb{R}^D$ 中的 $N$ 个独立随机点 $x_1,\ldots,x_N$ 和一个参数 $\alpha \in (0, 1)$，其中每个 $x_i$ 都取自均值为 $\mu$ 且协方差未知的高斯分布，并且未知的 $\alpha$ 部分点具有恒等有界协方差，目标是估计共同均值 $\mu$。过去几十年来，该任务的一维版本在理论计算机科学和统计学中受到了广泛关注。最近的研究 [LY20; CV24] 为一维设置给出了近乎最优的上限和下限。另一方面，我们对多变量设置的信息论方面的理解仍然有限。
在本研究中，我们设计了一种计算效率高的算法，实现了信息理论近似最优误差。具体来说，我们表明最优误差（最多为多对数因子）为 $f(\alpha,N) + \sqrt{D/(\alpha N)}$，其中 $f(\alpha,N)$ 项是一维问题的误差，第二项是亚高斯误差率。我们的算法方法采用迭代细化策略，通过此策略我们逐步学习更准确的近似值 $\hat \mu$ 到 $\mu$。这是通过一种新颖的拒绝采样程序实现的，该程序会删除明显偏离 $\hat \mu$ 的点，以尝试过滤掉异常嘈杂的样本。出现的一个复杂因素是拒绝采样会在剩余点的分布中引入偏差。为了解决这个问题，我们对偏差进行了仔细的分析，开发了一种迭代降维策略，并采用了一种受列表可解码学习启发的利用一维结果的新型子程序。]]></description>
      <guid>https://arxiv.org/abs/2501.05425</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有二元结果的提升建模的新转换方法</title>
      <link>https://arxiv.org/abs/2310.05549</link>
      <description><![CDATA[arXiv:2310.05549v2 公告类型：替换 
摘要：提升模型已在营销和客户保留等领域得到有效应用，以针对那些更有可能因活动或治疗而做出反应的客户。本质上，它是一种机器学习技术，可以预测执行某些操作相对于不执行某些操作的收益。一类流行的提升模型是转换方法，该方法使用原始治疗指标重新定义目标变量。这些转换方法只需要直接训练和预测结果的差异。这些方法的主要缺点是，一般来说，它不会使用治疗指标中的信息，而只是构建转换后的结果，而且通常效率不高。在本文中，我们为二元目标变量的情况设计了一种新颖的转换结果，并解锁了具有零结果的样本的全部价值。从实际角度来看，我们的新方法灵活且易于使用。在合成和真实世界数据集上的实验结果明显表明，我们的新方法优于传统方法。目前，我们的新方法已经应用于中国某全国性金融控股集团的精准营销。]]></description>
      <guid>https://arxiv.org/abs/2310.05549</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习模型的双尺度复杂度测量</title>
      <link>https://arxiv.org/abs/2401.09184</link>
      <description><![CDATA[arXiv:2401.09184v3 公告类型：替换 
摘要：我们基于有效维度为统计模型引入了一种新的容量度量 2sED。新数量可证明在对模型做出温和假设的情况下限制了泛化误差。此外，对标准数据集和流行模型架构的模拟表明 2sED 与训练误差有很好的相关性。对于马尔可夫模型，我们展示了如何通过分层迭代方法从下方有效地近似 2sED，这使我们能够处理具有大量参数的深度学习模型。模拟结果表明，该近似适用于不同的突出模型和数据集。]]></description>
      <guid>https://arxiv.org/abs/2401.09184</guid>
      <pubDate>Fri, 10 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>