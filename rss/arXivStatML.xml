<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用强化学习为扩散模型添加条件控制</title>
      <link>https://arxiv.org/abs/2406.12120</link>
      <description><![CDATA[arXiv:2406.12120v1 公告类型：交叉 
摘要：扩散模型是强大的生成模型，可以精确控制生成样本的特征。虽然这些在大型数据集上训练的扩散模型已经取得了成功，但通常需要在下游微调过程中引入额外的控制，将这些强大的模型视为预训练的扩散模型。这项工作提出了一种基于强化学习 (RL) 的新方法来添加额外的控制，利用由输入和相应标签组成的离线数据集。我们将这个任务表述为一个 RL 问题，从离线数据集中学习到的分类器和与预训练模型的 KL 散度作为奖励函数。我们介绍了我们的方法 $\textbf{CTRL}$（$\textbf{C}$ 条件化预 $\textbf{T}$ 训练的扩散模型，使用 $\textbf{R}$ 强化 $\textbf{L}$ 收益），它产生了最大化上述奖励函数的软最优策略。我们正式证明，我们的方法能够在推理过程中从以附加控制为条件的条件分布中进行采样。与现有方法相比，我们基于 RL 的方法具有多项优势。与常用的无分类器指导相比，我们的方法提高了采样效率，并且可以通过利用输入和附加控制之间的条件独立性大大简化离线数据集构建。此外，与分类器指导不同，我们避免了将分类器从中间状态训练到附加控制的需要。]]></description>
      <guid>https://arxiv.org/abs/2406.12120</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>分位数风险下的极小最大线性回归</title>
      <link>https://arxiv.org/abs/2406.12145</link>
      <description><![CDATA[arXiv:2406.12145v1 公告类型：交叉 
摘要：我们研究在分位数风险下设计线性回归中的极小最大程序的问题。我们首先考虑具有独立高斯噪声的可实现设置，其中对于任何给定的噪声水平和输入分布，我们获得丰富的误差函数族的精确极小最大分位数风险并建立 OLS 的极小最大性。这改进了平方误差特殊情况的已知下限，并为我们提供了更大分布集上的极小最大分位数风险的下限。在平方误差和输入分布的四阶矩假设下，我们表明这个下限在更大类的问题上是严格的。具体而言，我们证明了最近提出的最小-最大回归程序变体的最坏情况分位数风险的匹配上限，从而建立了它的极小最大性，直到绝对常数。我们通过将这个结果扩展到 $p \in (2, \infty)$ 的所有 $p$ 次方误差函数来说明我们方法的实用性。在此过程中，我们开发了一种与经典贝叶斯方法类似的通用方法，用于在处理分位数风险时降低极小最大风险，以及对样本协方差矩阵最小特征值的分位数进行严格表征。]]></description>
      <guid>https://arxiv.org/abs/2406.12145</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:04 GMT</pubDate>
    </item>
    <item>
      <title>使用部分网络数据的基于模型的干扰推理和实验设计</title>
      <link>https://arxiv.org/abs/2406.11940</link>
      <description><![CDATA[arXiv:2406.11940v1 公告类型：交叉 
摘要：稳定单位治疗价值假设指出，个人的结果不受他人治疗状态的影响，然而在许多现实世界的应用中，治疗会对直接治疗以外的许多其他人产生影响。干扰通常可以被认为是通过某种网络结构介导的。然而，在许多经验相关的情况下，完整的网络数据（需要调整这些溢出效应）太昂贵或在逻辑上不可行而无法收集。部分或间接观察到的网络数据（例如，子样本、聚合关系数据 (ARD)、自我中心抽样或受访者驱动抽样）减少了收集网络数据的后勤和财务负担，但这些设计策略的治疗效果调整的统计特性才刚刚开始被探索。在本文中，我们通过结构因果模型的视角，提出了一个使用部分网络数据估计和推断治疗效果调整的框架。我们还说明了仅使用部分网络数据分配治疗的过程，目的是最小化估计方差或最佳播种。我们推导出适用于底层图模型的各种选择的单网络渐近结果。我们使用在观察到的图上进行的模拟实验验证了我们的方法，并将其应用于印度和马拉维的信息传播。]]></description>
      <guid>https://arxiv.org/abs/2406.11940</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>通过强化学习构建祖先重组图</title>
      <link>https://arxiv.org/abs/2406.12022</link>
      <description><![CDATA[arXiv:2406.12022v1 公告类型：交叉 
摘要：多年来，人们提出了许多方法来构建祖先重组图 (ARG)，该图用于表示个体之间的遗传关系。在这些方法中，许多方法都依赖于这样的假设：最可能的图是最短的图之一。在本文中，我们提出了一种构建短 ARG 的新方法：强化学习 (RL)。我们利用了寻找一组基因序列与其最近的共同祖先之间的最短路径与寻找迷宫入口和出口之间的最短路径之间的相似性，这是一个经典的 RL 问题。在迷宫问题中，学习者（称为代理）必须学习要采取的方向才能尽快逃脱，而在我们的问题中，代理必须学习在合并、突变和重组之间采取的行动，以便尽快到达最近的共同祖先。我们的结果表明，RL 可用于构建与使用优化的启发式算法构建的 ARG 一样短的 ARG，有时甚至更短。此外，我们的方法允许为给定样本构建短 ARG 的分布，并且还可以将学习推广到学习过程中未使用的新样本。]]></description>
      <guid>https://arxiv.org/abs/2406.12022</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>拟贝叶斯遇见 Vines</title>
      <link>https://arxiv.org/abs/2406.12764</link>
      <description><![CDATA[arXiv:2406.12764v1 公告类型：新
摘要：最近提出的准贝叶斯 (QB) 方法通过递归直接构建贝叶斯预测分布，开创了贝叶斯计算的新时代，消除了对贝叶斯后验分布进行采样所涉及的昂贵计算。这已被证明对于单变量预测是数据高效的，但扩展到多维依赖于条件分解，该分解是由对 Dirichlet 过程混合模型的内核的预定义假设引起的，这是使用的隐式非参数模型。在这里，我们提出了一种不同的方法，通过使用 Sklar 定理将准贝叶斯预测扩展到高维，将预测分布分解为一维预测边际和高维 copula。因此，我们对一维边际使用高效的递归 QB 构造，并使用高度表达的藤蔓 copula 对依赖关系进行建模。此外，我们使用稳健散度（例如能量得分）调整超参数，并表明我们提出的准贝叶斯 Vine (QB-Vine) 是一个完全非参数密度估计器，在某些情况下具有 \emph{解析形式} 和与数据维度无关的收敛速度。我们的实验表明，QB-Vine 适用于高维分布（$\sim$64），只需很少的样本即可进行训练（$\sim$200），并且在密度估计和监督任务方面的表现远远优于具有解析形式的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2406.12764</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>图知识蒸馏至专家混合</title>
      <link>https://arxiv.org/abs/2406.11919</link>
      <description><![CDATA[arXiv:2406.11919v1 公告类型：交叉 
摘要：就准确性而言，图神经网络 (GNN) 是节点分类任务的最佳架构选择。它们在实际部署中的缺点是邻域处理操作产生的延迟。延迟问题的一个解决方案是从训练过的 GNN 到多层感知器 (MLP) 执行知识蒸馏，其中 MLP 仅处理被分类节点的特征（可能还有一些预先计算的结构信息）。然而，对于现有的知识蒸馏技术，此类 MLP 在传导和归纳设置中的性能仍然不一致。我们建议通过使用专门设计的学生模型而不是 MLP 来解决性能问题。我们的模型名为 Routing-by-Memory (RbM)，是一种混合专家 (MoE) 的形式，其设计强制专家专业化。通过鼓励每个专家专注于隐藏表示空间的某个区域，我们通过实验证明可以在多个数据集上获得更加一致的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.11919</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:02 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程回归中的共轭梯度和 Lanczos 近似后验的收缩率</title>
      <link>https://arxiv.org/abs/2406.12678</link>
      <description><![CDATA[arXiv:2406.12678v1 公告类型：新
摘要：由于其灵活性和理论可处理性，高斯过程 (GP) 回归模型已成为现代统计学和机器学习的中心主题。虽然这些模型中的真实后验是明确给出的，但数值评估取决于增强核矩阵 $ K + \sigma^2 I $ 的求逆，这需要最多 $ O(n^3) $ 次运算。对于现代应用中通常给出的大样本量 n，这在计算上是不可行的，并且需要使用后验的近似版本。虽然这种方法在实践中被广泛使用，但它们通常具有非常有限的理论基础。
在这种情况下，我们分析了概率数值领域最近提出的一类近似算法。它们可以用核矩阵的 Lanczos 近似特征向量或后验均值的共轭梯度近似来解释，这在真正大规模的应用中特别有利，因为它们从根本上来说仅基于适合现代软件框架的 GPU 加速的矩阵向量乘法。我们将数值分析文献的结果与核矩阵谱的最先进的浓度结果相结合，以获得极小极大收缩率。我们的理论发现通过数值实验得到说明。]]></description>
      <guid>https://arxiv.org/abs/2406.12678</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>可分离数据上的镜像流隐性偏差</title>
      <link>https://arxiv.org/abs/2406.12763</link>
      <description><![CDATA[arXiv:2406.12763v1 公告类型：新
摘要：我们研究镜像下降的连续时间对应物，即镜像流，用于线性可分的分类问题。此类问题在“无穷大”处最小化，并有许多可能的解决方案；我们研究算法根据镜像势首选哪种解决方案。对于指数尾部损失，在对势的温和假设下，我们表明迭代朝 $\phi_\infty$ 最大边际分类器的方向收敛。函数 $\phi_\infty$ 是镜像势的 $\textit{horizo​​n function}$，并表征其“无穷大”的形状。当势可分时，一个简单的公式就可以计算这个函数。我们分析了几个势的例子，并提供了突出我们结果的数值实验。]]></description>
      <guid>https://arxiv.org/abs/2406.12763</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:01 GMT</pubDate>
    </item>
    <item>
      <title>变分贝叶斯方法对高维线性回归中的低维参数进行去偏推断</title>
      <link>https://arxiv.org/abs/2406.12659</link>
      <description><![CDATA[arXiv:2406.12659v1 公告类型：新
摘要：我们提出了一种可扩展的变分贝叶斯方法，用于稀疏线性回归中高维参数坐标的单个或低维子集的统计推断。我们的方法依赖于为干扰坐标分配均值场近似，并仔细建模给定干扰的目标条件分布。这只需要一个预处理步骤，并保留了均值场变分贝叶斯的计算优势，同时确保对目标参数的准确可靠推断，包括不确定性量化。我们研究了我们的算法的数值性能，表明它与现有方法具有竞争力。我们进一步以伯恩斯坦-冯·米塞斯定理的形式为估计和不确定性量化建立了伴随的理论保证。]]></description>
      <guid>https://arxiv.org/abs/2406.12659</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:00 GMT</pubDate>
    </item>
    <item>
      <title>平移等变 Transformer 神经过程</title>
      <link>https://arxiv.org/abs/2406.12409</link>
      <description><![CDATA[arXiv:2406.12409v1 公告类型：新 
摘要：神经过程 (NPs) 在建模后验预测图（从数据到后验预测分布的映射）方面的有效性自诞生以来已得到显着提高。这种改进可以归因于两个主要因素：(1) 置换不变集函数架构的进步，这是所有 NPs 所固有的；(2) 利用真实后验预测图中存在的对称性，这些对称性取决于问题。Transformers 是置换不变集函数的一个显着发展，它们在 NPs 中的效用已通过我们称为 TNPs 的模型系列得到证明。尽管人们对 TNPs 非常感兴趣，但很少有人关注对称性的结合。值得注意的是，静止数据的后验预测图（时空建模中的常见假设）表现出平移等变性。在本文中，我们介绍了一种新的翻译等变 TNP 系列，它们结合了翻译等变性。通过对合成和现实世界时空数据进行大量实验，我们证明了 TE-TNP 相对于非翻译等变 TNP 和其他 NP 基线的有​​效性。]]></description>
      <guid>https://arxiv.org/abs/2406.12409</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:59 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯数据选择</title>
      <link>https://arxiv.org/abs/2406.12560</link>
      <description><![CDATA[arXiv:2406.12560v1 公告类型：新
摘要：各种机器学习算法都会迭代地将数据添加到训练样本中。示例包括半监督学习、主动学习、多臂老虎机和贝叶斯优化。我们将这种数据添加嵌入到决策理论中，将数据选择定义为决策问题。这为找到贝叶斯最优数据选择铺平了道路。对于半监督学习中自我训练的说明性案例，我们推导出相应的贝叶斯标准。我们进一步表明，通过对模拟和现实世界数据上的广义线性模型、半参数广义加性模型和贝叶斯神经网络的方法进行实证评估，部署此标准可以缓解确认偏差问题。]]></description>
      <guid>https://arxiv.org/abs/2406.12560</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:59 GMT</pubDate>
    </item>
    <item>
      <title>网络回归的最优传输方法</title>
      <link>https://arxiv.org/abs/2406.12204</link>
      <description><![CDATA[arXiv:2406.12204v1 公告类型：新
摘要：我们研究网络回归问题，其中人们感兴趣的是网络拓扑如何随着欧几里得协变量的变化而变化。我们基于 Fr\&#39;echet 均值的度量空间上广义回归模型的最新发展，并提出了一种使用 Wasserstein 度量的网络回归方法。我们表明，当将图表示为多元高斯分布时，网络回归问题需要计算黎曼质心（即 Fr\&#39;echet 均值）。具有非负权重的 Fr\&#39;echet 均值转化为重心问题，可以使用定点迭代有效地计算。虽然用于计算 Wasserstein 仿射平均值的定点迭代的收敛保证仍然是一个悬而未决的问题，但我们提供了大量合成和真实数据场景中收敛的证据。大量数值结果表明，所提出的方法通过在综合实验中准确考虑图形大小、拓扑和稀疏性来改进现有程序。此外，使用所提出方法进行的实际实验可获得更高的判定系数 ($R^{2}$) 值和更低的均方预测误差 (MSPE)，从而在实践中巩固了改进的预测能力。]]></description>
      <guid>https://arxiv.org/abs/2406.12204</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:58 GMT</pubDate>
    </item>
    <item>
      <title>自上而下的和积网络贝叶斯后验采样</title>
      <link>https://arxiv.org/abs/2406.12353</link>
      <description><![CDATA[arXiv:2406.12353v1 公告类型：新
摘要：和积网络 (SPN) 是一种概率模型，其特点是可以准确、快速地评估基本概率运算。其出色的计算易处理性已应用于许多领域，例如具有时间限制或准确性要求的机器学习和实时系统。然而，支持快速推理的 SPN 的结构约束导致学习时间复杂度增加，并可能成为构建高表达力 SPN 的障碍。本研究旨在开发一种可以在大规模 SPN 上有效实施的贝叶斯学习方法。我们通过边缘化多个随机变量来快速获得后验分布，从而推导出一种新的吉布斯采样的完全条件概率。复杂性分析表明，即使对于最大的 SPN，我们的采样算法也能有效工作。此外，我们提出了一种超参数调整方法，以平衡大规模 SPN 中先验分布的多样性和优化效率。我们的方法提高了学习时间复杂度，在 20 多个数据集的数值实验中表现出了几十到百倍以上的计算速度和优异的预测性能。]]></description>
      <guid>https://arxiv.org/abs/2406.12353</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:58 GMT</pubDate>
    </item>
    <item>
      <title>通过拼接迭代进行稀疏约束优化</title>
      <link>https://arxiv.org/abs/2406.12017</link>
      <description><![CDATA[arXiv:2406.12017v1 公告类型：新
摘要：稀疏约束优化在信号处理、统计和机器学习中具有广泛的适用性。现有的快速算法必须繁琐地调整参数，例如步长或精确停止标准的实现，这在实践中可能很难确定。为了解决这个问题，我们开发了一种名为通过拼接迭代进行稀疏约束优化 (SCOPE) 的算法，用于优化低维子空间中具有强凸性和平滑性的非线性微分目标函数。从算法上讲，SCOPE 算法无需调整参数即可有效收敛。从理论上讲，SCOPE 具有线性收敛速度，并且在正确指定稀疏性时收敛到恢复真实支持集的解。我们还开发了没有限制等距属性类型条件的并行理论结果。我们利用 SCOPE 的多功能性和强大功能来解决稀疏二次优化问题、学习稀疏分类器以及恢复二元变量的稀疏马尔可夫网络。这些特定任务的数值结果表明，SCOPE 完美地识别了真正的支持集，速度比标准精确求解器提高了 10-1000 倍，证实了 SCOPE 的算法和理论优势。我们基于 C++ 实现的开源 Python 包 skscope 在 GitHub 上公开可用，与 cvxpy 库实现的竞争凸松弛方法相比，速度提高了十倍。]]></description>
      <guid>https://arxiv.org/abs/2406.12017</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:57 GMT</pubDate>
    </item>
    <item>
      <title>熵回归 DMD (ERDMD) 发现信息稀疏和非均匀时间延迟模型</title>
      <link>https://arxiv.org/abs/2406.12062</link>
      <description><![CDATA[arXiv:2406.12062v1 公告类型：新
摘要：在这项工作中，我们提出了一种通过熵回归确定最佳多步动态模式分解 (DMD) 模型的方法，这是一种非线性信息流检测算法。受 \cite{clainche} 的高阶 DMD (HODMD) 方法和 \cite{bollt, bollt2} 中用于网络检测和模型构建的熵回归 (ER) 技术的启发，我们开发了一种称为 ERDMD 的方法，该方法可生成高保真时间延迟 DMD 模型，允许非均匀时间空间，并且时间间隔是通过考虑基于 ER 的大多数信息量来发现的。这些模型被证明是高效且稳健的。我们在由混沌吸引子生成的几个数据集上测试了我们的方法，并表明我们能够使用相对最小的模型构建出色的重建。我们同样能够通过我们的模型更好地识别多尺度特征，从而增强动态模式分解的效用。]]></description>
      <guid>https://arxiv.org/abs/2406.12062</guid>
      <pubDate>Wed, 19 Jun 2024 06:19:57 GMT</pubDate>
    </item>
    </channel>
</rss>