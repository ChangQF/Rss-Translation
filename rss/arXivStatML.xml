<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Mon, 27 Nov 2023 11:47:18 GMT</lastBuildDate>
    <item>
      <title>GATGPT：带有用于时空插补的图注意网络的预训练大型语言模型。 （arXiv：2311.14332v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.14332</link>
      <description><![CDATA[时空数据的分析越来越多地应用于不同领域
领域，包括交通、医疗保健和气象。在现实世界中
设置中，由于传感器等问题，此类数据通常包含丢失的元素
故障和数据传输错误。时空目标
插补是通过理解内在的值来估计这些缺失值
观察到的多元时间序列中的空间和时间关系。
传统上，时空插补依赖于特定的、复杂的
为此目的而设计的架构，其受到有限的影响
适用性强，计算复杂度高。相比之下，我们的方法
将预训练的大语言模型 (LLM) 集成到时空模型中
插补，引入了一个突破性的框架，GATGPT。这个框架
将图注意力机制与 LLM 合并。我们维持大部分的LLM
参数不变，以利用现有知识来学习时间
模式，同时微调适合各种应用的上层。
图注意力组件增强了法学硕士理解空间的能力
关系。通过对三个不同的现实世界数据集的测试，我们的
创新方法展示了与已建立的深度可比的结果
学习基准。
]]></description>
      <guid>http://arxiv.org/abs/2311.14332</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:18 GMT</pubDate>
    </item>
    <item>
      <title>注释敏感性：训练数据收集方法影响模型性能。 （arXiv：2311.14212v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2311.14212</link>
      <description><![CDATA[当从人类注释者那里收集训练数据时，
注释工具、给注释者的指示、
注释者的特征及其交互会影响训练
数据。这项研究表明，在创建一个
注释工具也会影响根据结果训练的模型
注释。

我们引入术语“注释敏感性”来指代注释敏感性的影响
注释本身和注释数据收集方法
下游模型性能和预测。

我们收集五种仇恨言论和攻击性语言的注释
注释仪器的实验条件，随机分配
条件注释器。然后，我们对五个模型中的每一个进行微调 BERT 模型
生成的数据集并评估每个数据集的保留部分的模型性能
健康）状况。我们发现 1) 的条件之间存在很大差异
仇恨言论/攻击性语言注释的比例，2) 模型性能，3)
模型预测，4) 模型学习曲线。

我们的结果强调了注释工具所发挥的关键作用
这在机器学习文献中很少受到关注。我们称之为
进一步研究仪器如何以及为何影响注释
为仪器设计最佳实践的发展提供信息。
]]></description>
      <guid>http://arxiv.org/abs/2311.14212</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:17 GMT</pubDate>
    </item>
    <item>
      <title>假设精益和数据自适应的预测后推理。 （arXiv：2311.14220v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2311.14220</link>
      <description><![CDATA[现代科学研究面临的一个主要挑战是有限的
黄金标准数据的可用性可能既昂贵又劳动密集型
来获得。随着机器学习（ML）的快速发展，科学家们已经
依靠机器学习算法轻松预测这些黄金标准结果
获得协变量。然而，这些预测结果通常直接使用
在随后的统计分析中，忽略不精确性和异质性
由预测过程引入。这可能会导致错误
积极的发现和无效的科学结论。在这项工作中，我们
引入假设精益和数据自适应的预测后推理
(POP-Inf) 过程允许基于以下内容进行有效且有力的推理
机器学习预测的结果。其“假设精益”的特性保证了可靠
不假设 ML 预测的统计推断，适用于广泛的
统计量的范围。其“数据自适应”功能保证了
与现有的预测后推理方法相比，效率有所提高，无论
ML 预测的准确性。我们展示了优越性和适用性
通过模拟和大规模基因组数据来验证我们的方法。
]]></description>
      <guid>http://arxiv.org/abs/2311.14220</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:17 GMT</pubDate>
    </item>
    <item>
      <title>超参数化线性回归的加速 SGD 的风险界限。 （arXiv：2311.14222v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.14222</link>
      <description><![CDATA[加速随机梯度下降（ASGD）是深度学习的主力
学习，并且通常能获得比 SGD 更好的泛化性能。
然而现有的优化理论只能解释更快的收敛
ASGD，但无法解释其更好的泛化性。在本文中，我们研究
ASGD 对超参数化线性回归的推广，即
可能是最简单的过度参数化学习设置。我们
在每个实例中为 ASGD 建立一个与实例相关的超额风险界限
数据协方差矩阵的特征子空间。我们的分析表明 (i) ASGD
在小特征值的子空间中优于 SGD，表现出更快的速度
偏差误差的指数衰减，而在大的子空间中
特征值，其偏差误差衰减比 SGD 慢； (ii) 方差误差
ASGD 始终大于 SGD。我们的结果表明 ASGD 可以
当初始化和真实值之间存在差异时，优于 SGD
权重向量主要局限于小特征值的子空间。
此外，当我们的分析专门针对线性回归时
强凸设置，它产生的偏差误差范围比
最著名的结果。
]]></description>
      <guid>http://arxiv.org/abs/2311.14222</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:17 GMT</pubDate>
    </item>
    <item>
      <title>用于组合优化的图变分退火。 （arXiv：2311.14156v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.14156</link>
      <description><![CDATA[最近的几种无监督学习方法使用概率方法来
基于以下假设解决组合优化 (CO) 问题
统计上独立的解变量。我们证明这
假设对性能施加限制，特别是在困难问题上
实例。我们的结果证实了自回归方法
捕获解决方案变量之间的统计依赖性，产生优越的结果
在许多常见的 CO 问题上的性能。我们在中引入子图标记化
其中一组解变量的配置由
单个令牌。这种标记化技术减轻了长的缺点
自回归方法固有的顺序抽样程序
而不牺牲表现力。重要的是，我们从理论上激励
退火熵正则化并根据经验表明它对于
高效稳定的学习。
]]></description>
      <guid>http://arxiv.org/abs/2311.14156</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:16 GMT</pubDate>
    </item>
    <item>
      <title>通过矩阵微分计算进行多罚岭回归的基于梯度的双层优化。 （arXiv：2311.14182v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.14182</link>
      <description><![CDATA[线性回归常用的正则化算法，例如LASSO和
岭回归，依靠正则化超参数来平衡
最小化拟合误差和学习模型的范数之间的权衡
系数。由于该超参数是标量，因此可以通过以下方式轻松选择
优化交叉验证标准的随机或网格搜索。然而，使用一个
标量超参数限制了算法的灵活性和潜力
更好的概括。在本文中，我们解决了线性问题
使用 l2 正则化进行回归，其中不同的正则化
超参数与每个输入变量相关联。我们优化这些
超参数使用基于梯度的方法，其中a的梯度
关于正则化超参数的交叉验证标准
通过矩阵微积分进行分析计算。此外，我们
引入两种针对稀疏模型学习问题的策略，旨在
降低验证数据过度拟合的风险。数值例子
证明我们的多超参数正则化方法优于
LASSO、岭回归和弹性网络回归。此外，解析计算
事实证明，梯度的计算时间更有效
与自动微分相比，尤其是在处理大量数据时
输入变量。在超参数化识别中的应用
还提出了线性参数变化模型。
]]></description>
      <guid>http://arxiv.org/abs/2311.14182</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:16 GMT</pubDate>
    </item>
    <item>
      <title>模型选择中交叉验证和变异验证的实证比较。 （arXiv：2311.14079v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.14079</link>
      <description><![CDATA[突变验证（MV）是最近提出的一种模型选择方法，
由于其独特的特征和潜力而引起人们的极大兴趣
与广泛使用的交叉验证（CV）方法相比的优点。在这个
研究中，我们使用基准和 $k$ 倍 CV 进行实证比较
真实世界的数据集。通过采用贝叶斯测试，我们比较了泛化能力
产生三个后验概率的估计：实际等效性、CV
优越性和MV优越性。我们还评估了差异
所选模型的容量和计算效率。我们发现
MV 和 CV 都选择具有实际上等效泛化能力的模型
各种机器学习算法和大多数机器学习算法的性能
基准数据集。 MV在选择更简单方面表现出优势
模型并降低计算成本。然而，在某些情况下，MV选择过度
过于简单化的模型会导致拟合不足并表现出不稳定
超参数选择。 MV 的这些局限性在
评估现实世界中预测出生性别的神经科学任务
使用大脑功能连接。
]]></description>
      <guid>http://arxiv.org/abs/2311.14079</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:15 GMT</pubDate>
    </item>
    <item>
      <title>使用主动学习进行领域自适应语义分割的类平衡动态采集。 （arXiv：2311.14146v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.14146</link>
      <description><![CDATA[领域自适应主动学习在标签效率方面处于领先地位
神经网络的训练。对于语义分割，最先进的模型
联合使用不确定性和多样性两个标准来选择训练
标签，结合逐像素采集策略。然而，我们表明
此类方法目前面临类别不平衡问题，从而降低了它们的性能
更大的主动学习预算的性能。然后我们引入类
平衡动态获取（CBDA），一种新颖的主动学习方法，
缓解了这个问题，特别是在高预算制度下。越平衡
标签提高了少数类的表现，这反过来又允许模型
对于 5% 的预算，比之前的基线高出 0.6、1.7 和 2.4 mIoU，
分别为 10% 和 20%。此外，对少数群体的关注导致
最低类别性能提高 0.5、2.9 和 4.6 IoU
分别。表现最好的模型甚至超过了完全监督的模型
基线，表明比整个地面事实更平衡的标签
是有益的。
]]></description>
      <guid>http://arxiv.org/abs/2311.14146</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:15 GMT</pubDate>
    </item>
    <item>
      <title>动态步长调度的局部最优下降。 （arXiv：2311.13877v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.13877</link>
      <description><![CDATA[我们引入了一种新颖的动态学习率调度方案，该方案基于
理论的目标是简化手动和耗时的调整
实践中的时间表。我们的方法基于估计局部最优
步长，保证随机方向的最大下降
当前步骤的梯度。我们首先建立理论收敛界限
对于我们在平滑非凸随机优化背景下的方法，
匹配最先进的边界，同时仅假设了解
平滑度参数。然后我们提出了我们的实际实施
算法并在不同的数据集上进行系统实验
优化算法，将我们的方案与现有最先进的方案进行比较
学习率调度程序。我们的研究结果表明我们的方法需要最少的
与现有方法相比进行调整，无需辅助
手动计划和预热阶段并实现与
大大减少了参数调整。
]]></description>
      <guid>http://arxiv.org/abs/2311.13877</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:14 GMT</pubDate>
    </item>
    <item>
      <title>使用张量 $U_1$ 范数进行高阶张量恢复。 （arXiv：2311.13958v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2311.13958</link>
      <description><![CDATA[最近，许多基于张量 SVD (t-SVD) 的张量恢复方法已经出现
出现了，在处理视觉数据方面显示出了前景。然而，这些方法
当遇到高阶问题时，常常会出现性能下降的情况
张量数据表现出非平滑变化，在现实世界中常见
但被传统的基于 t-SVD 的方法忽略。我们的目标是
这项研究旨在提供一种有效的张量恢复技术来处理
张量数据的非平滑变化并有效地探索之间的相关性
跨各个维度的高阶张量数据，无需引入
许多变量和权重。为此，我们引入一个新的张量
分解和一个新的张量范数，称为张量 $U_1$ 范数。我们利用
这些解决高阶张量完成问题的新技术
问题的解决，为准确恢复提供理论保证
生成的张量完成模型。提出了一种优化算法
通过组合迭代求解得到的张量完成模型
乘子交替方向法的近端算法。
理论分析表明该算法收敛于
优化问题的 Karush-Kuhn-Tucker (KKT) 点。数值
实验证明了该方法在高阶情况下的有效性
张量补全，特别是对于非平滑变化的张量数据。
]]></description>
      <guid>http://arxiv.org/abs/2311.13958</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:14 GMT</pubDate>
    </item>
    <item>
      <title>模型平均中的稳定性和 L2 惩罚。 （arXiv：2311.13827v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2311.13827</link>
      <description><![CDATA[模型平均在过去二十年中受到了广泛关注，
通过对潜在模型进行平均来整合可用信息。虽然
已经开发了各种模型平均方法，但文献很少
从以下角度探讨模型平均的理论性质
稳定性，并且这些方法中的大多数将模型权重限制为
单纯形。本文的目的是从统计中引入稳定性
将理论学习到模型平均中。因此，我们定义稳定性，渐近
经验风险最小化、泛化和模型平均的一致性
并研究它们之间的关系。我们的结果表明稳定性可以
确保模型平均具有良好的泛化性能和一致性
在合理的条件下，其中一致性意味着模型平均估计器
可以渐近最小化均方预测误差。我们还建议
一种不限制模型权重的L2惩罚模型平均方法并证明
它具有稳定性和一致性。为了减少调整的影响
参数选择，我们使用10折交叉验证来选择候选集
调整参数并执行模型估计量的加权平均值
基于估计误差的权重。蒙特卡罗模拟和
说明性应用证明了所提出方法的有用性。
]]></description>
      <guid>http://arxiv.org/abs/2311.13827</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:13 GMT</pubDate>
    </item>
    <item>
      <title>使用前推地图进行巡回采样。 （arXiv：2311.13845v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.13845</link>
      <description><![CDATA[采样方法的数量对于从业者来说可能是令人畏惧的
将强大的机器学习方法应用于特定问题。这张纸
采取理论立场来审查和组织许多抽样方法
“生成建模”设置，人们想要生成新的数据
与一些训练示例类似。通过揭示现有之间的联系
方法，它可能有助于克服当前的一些挑战
使用扩散模型进行采样，例如由于扩散而导致推理时间较长
模拟，或生成的样本缺乏多样性。
]]></description>
      <guid>http://arxiv.org/abs/2311.13845</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:13 GMT</pubDate>
    </item>
    <item>
      <title>样本有效的扩散训练。 （arXiv：2311.13745v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.13745</link>
      <description><![CDATA[基于分数的扩散模型已成为深度学习最流行的方法
图像的生成建模，很大程度上是由于它们的经验表现和
可靠性。最近，一些理论著作\citep{chen2022，
Chen2022ImprovedAO、Chenetal23flowode、benton2023线性} 表明
扩散模型可以有效地采样，假设 $L^2$-准确的分数
估计。分数匹配目标自然接近真实分数
在 $L^2$ 中，但现有边界的样本复杂度取决于
\emph{多项式} 上的数据半径和所需的 Wasserstein 精度。经过
相比之下，采样的时间复杂度只是这些中的对数
参数。我们证明，估计 $L^2$ \emph{需要} 这个
多项式依赖性，但是可以缩放的样本数量
Wasserstein 精度的多对数实际上足以满足
采样。我们证明，对于多对数数量的样本，ERM
分数匹配目标在除概率 $\delta$ 之外的所有方面均准确 $L^2$
真实分布的一小部分，并且这种较弱的保证就足够了
以实现高效采样。
]]></description>
      <guid>http://arxiv.org/abs/2311.13745</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:12 GMT</pubDate>
    </item>
    <item>
      <title>使用三层神经网络学习分层多项式。 （arXiv：2311.13774v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.13774</link>
      <description><![CDATA[我们研究在标准上学习分层多项式的问题
具有三层神经网络的高斯分布。我们特别
考虑形式为 $h = g \circ p$ 的目标函数，其中 $p : \mathbb{R}^d
\rightarrow \mathbb{R}$ 是 $k$ 次多项式，$g: \mathbb{R}
\rightarrow \mathbb{R}$ 是 $q$ 次多项式。这个函数类
概括了单索引模型，它对应于$k=1$，并且是
具有底层层次结构的自然函数类。我们的
主要结果表明，对于 $k$ 次多项式 $p$ 的大子类，
通过正方形上的分层梯度下降训练的三层神经网络
损失学习目标 $h$ 直到消失的测试误差
$\widetilde{\mathcal{O}}(d^k)$ 样本和多项式时间。这是一个严格的
改进核方法，需要 $\widetilde \Theta(d^{kq})$
样本，以及现有的两层网络保证，这需要
目标函数是低秩的。我们的结果还概括了之前的工作
三层神经网络，仅限于 $p$ 为
二次的。当 $p$ 确实是二次方时，我们得到
信息论最优样本复杂度
$\widetilde{\mathcal{O}}(d^2)$，比之前的改进
work~\citep{nichani2023provable} 需要样本大小
$\widetilde\Theta(d^4)$。我们的证明继续表明，在初始阶段
训练阶段网络进行特征学习以恢复特征
$p$ 和 $\widetilde{\mathcal{O}}(d^k)$ 样本。这项工作展示了
三层神经网络学习复杂特征的能力
结果，学习了一大类层次函数。
]]></description>
      <guid>http://arxiv.org/abs/2311.13774</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:12 GMT</pubDate>
    </item>
    <item>
      <title>BackboneLearn：用于扩展基于混合整数优化的机器学习的库。 （arXiv：2311.13695v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.13695</link>
      <description><![CDATA[我们推出 BackboneLearn：一个开源软件包和框架
将带有指标变量的混合整数优化 (MIO) 问题缩放为
高维问题。这种优化范式自然可以用于
提出可解释的监督学习中的基本问题（例如，
稀疏回归和决策树），在无监督学习中（例如，
聚类）等； BackboneLearn解决了上述问题
比精确方法更快，并且比常用方法具有更高的准确度
启发式。该包是用 Python 构建的，用户友好且易于使用
可扩展：用户可以直接为其 MIO 实现骨干算法
手头的问题。 BackboneLearn 的源代码可在 GitHub 上获取（链接：
https://github.com/chziakas/backbone_learn）。
]]></description>
      <guid>http://arxiv.org/abs/2311.13695</guid>
      <pubDate>Mon, 27 Nov 2023 11:47:11 GMT</pubDate>
    </item>
    </channel>
</rss>