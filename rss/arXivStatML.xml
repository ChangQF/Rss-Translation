<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 19 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于无家可归者街头外展和拾取可食用食物的资源受限随机调度算法</title>
      <link>https://arxiv.org/abs/2403.10638</link>
      <description><![CDATA[arXiv:2403.10638v1 公告类型：交叉
摘要：我们开发了一种通用算法解决方案，解决具有不同使命和业务的社会变革组织所遇到的资源受限的外展问题：Breaking Ground——一个帮助纽约无家可归的个人过渡到永久住房的组织；Leket——以色列国家食品银行，从农场和其他地方拯救食物来养活饥饿的人。具体来说，我们开发了一种针对 $k$ 步转换下部分观察到的情景不安强盗的估计和优化方法。结果表明，我们的汤普森采样与马尔可夫链恢复（通过斯坦变分梯度下降）算法显着优于两个组织的问题的基线。我们以前瞻性的方式开展这项工作，其明确目标是设计一种足够灵活但又足够有用的解决方案，帮助克服数据科学缺乏可持续影响的问题，造福社会。]]></description>
      <guid>https://arxiv.org/abs/2403.10638</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:31 GMT</pubDate>
    </item>
    <item>
      <title>与人类比较相一致的概率方法</title>
      <link>https://arxiv.org/abs/2403.10771</link>
      <description><![CDATA[arXiv:2403.10771v1 公告类型：交叉
摘要：日益增长的趋势涉及将人类知识整合到学习框架中，利用微妙的人类反馈来完善人工智能模型。尽管取得了这些进展，但尚未开发出描述人类比较改善传统监督微调过程的具体条件的综合理论框架。为了弥补这一差距，本文研究了如何有效利用人类比较来解决噪声数据和高维模型带来的局限性。我们提出了一个两阶段的“监督微调+人类比较”（SFT+HC）框架，通过概率二分方法将机器学习与人类反馈联系起来。两阶段框架首先通过 SFT 过程从噪声标记数据中学习低维表示，然后使用人类比较来改进模型对齐。为了检查比对阶段的有效性，我们引入了一个称为“标签噪声与比较精度”（LNCA）比率的新概念。本文从理论上确定了“SFT+HC”框架优于纯 SFT 方法的条件，利用该比率来强调结合人类评估者在降低样本复杂性方面的优势。我们通过 Amazon Mechanical Turk 实验进行的案例研究验证了所提出的 LNCA 比率条件是否得到满足。]]></description>
      <guid>https://arxiv.org/abs/2403.10771</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:31 GMT</pubDate>
    </item>
    <item>
      <title>非平稳随机老虎机的激励探索</title>
      <link>https://arxiv.org/abs/2403.10819</link>
      <description><![CDATA[arXiv:2403.10819v1 公告类型：交叉
摘要：我们研究了具有非平稳奖励分布的多臂老虎机（MAB）问题的激励探索，其中玩家会因探索除贪婪选择之外的武器而获得补偿，并且可能会提供有关奖励的有偏见的反馈。我们考虑两种不同的非平稳环境：突然变化和连续变化，并提出各自的激励探索算法。我们表明，所提出的算法随着时间的推移实现了亚线性遗憾和补偿，从而有效地激励了探索，尽管存在非平稳性和有偏差或漂移的反馈。]]></description>
      <guid>https://arxiv.org/abs/2403.10819</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:31 GMT</pubDate>
    </item>
    <item>
      <title>非光滑隐式微分：确定性和随机收敛率</title>
      <link>https://arxiv.org/abs/2403.11687</link>
      <description><![CDATA[arXiv:2403.11687v1 公告类型：新
摘要：我们研究了有效计算参数不可微收缩图不动点导数的问题。这个问题在机器学习中有着广泛的应用，包括超参数优化、元学习和数据中毒攻击。我们分析两种流行的方法：迭代微分（ITD）和近似隐式微分（AID）。非光滑设置背后的一个关键挑战是链式法则不再成立。以 Bolte 等人最近的工作为基础。 (2022)证明了不可微 ITD 的线性收敛，我们在确定性情况下为 ITD 和 AID 提供了改进的线性收敛率。我们进一步介绍 NSID，这是一种当不动点定义为外部映射和内部映射的组合时计算隐式导数的新方法，该内部映射只能通过随机无偏估计器访问。我们建立了 NSID 与真实导数的收敛速率，包括平滑设置中的最佳可用速率。我们提出了说明性实验来证实我们的分析。]]></description>
      <guid>https://arxiv.org/abs/2403.11687</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:30 GMT</pubDate>
    </item>
    <item>
      <title>针对混杂离线数据的带有中介的悲观因果强化学习</title>
      <link>https://arxiv.org/abs/2403.11841</link>
      <description><![CDATA[arXiv:2403.11841v1 公告类型：新
摘要：在现实场景中，由于时间和预算的限制，从随机实验中收集的数据集通常受到大小的限制。因此，利用大型观测数据集成为实现高质量政策学习的更有吸引力的选择。然而，大多数现有的离线强化学习（RL）方法依赖于两个关键假设——无混杂性和积极性——而这些假设在观察数据环境中通常不成立。认识到这些挑战，我们提出了一种新的策略学习算法：PESsimistic CAusal Learning (PESCAL)。我们利用基于前门准则的中介变量来消除混杂偏差；此外，我们采用悲观原则来解决候选政策引起的行动分布与生成观察数据的行为政策之间的分布转变。我们的主要观察结果是，通过合并调节动作对系统动力学影响的辅助变量，学习中介分布函数的下界（而不是 Q 函数）就足以部分缓解分布偏移问题。这种见解通过规避估计 Q 函数的顺序不确定性量化这一具有挑战性的任务，显着简化了我们的算法。此外，我们为我们提出的算法提供了理论保证，并通过模拟以及利用领先叫车平台的离线数据集进行的现实实验来证明其有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.11841</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:30 GMT</pubDate>
    </item>
    <item>
      <title>用于摊余变分推理中的包容性 KL 最小化的顺序蒙特卡罗</title>
      <link>https://arxiv.org/abs/2403.10610</link>
      <description><![CDATA[arXiv:2403.10610v1 公告类型：交叉
摘要：为了训练编码器网络执行摊销变分推理，Kullback-Leibler (KL) 从精确后验到其近似的散度（称为包容性或前向 KL）由于质量问题而成为变分目标越来越流行的选择。覆盖其最小化器的属性。然而，最小化这一目标具有挑战性。一种流行的现有方法，重新加权唤醒睡眠（RWS），受到严重偏差的梯度和导致高度集中的变异分布的循环病理的影响。作为替代方案，我们提出了 SMC-Wake，这是一种拟合摊销变分近似的过程，该过程使用似然调节顺序蒙特卡洛采样器来估计包含 KL 散度的梯度。我们提出了三个梯度估计器，所有这些估计器在迭代次数上都是渐近无偏的，并且其中两个是强一致的。我们的方法将随机梯度更新、SMC 采样器和归一化常数估计的迭代改进相结合，以减少自归一化带来的偏差。在模拟和真实数据集的实验中，SMC-Wake 拟合的变分分布比现有方法更准确地逼近后验。]]></description>
      <guid>https://arxiv.org/abs/2403.10610</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:30 GMT</pubDate>
    </item>
    <item>
      <title>使用函数逼近的后验采样强化学习的先验相关分析</title>
      <link>https://arxiv.org/abs/2403.11175</link>
      <description><![CDATA[arXiv:2403.11175v1 公告类型：新
摘要：这项工作通过线性混合 MDP 建模的函数逼近推进了强化学习 (RL) 的随机探索。我们用函数逼近建立了第一个依赖于先验的贝叶斯后悔界限的强化学习；并完善后采样强化学习（PSRL）的贝叶斯遗憾分析，提出 ${\mathcal{O}}(d\sqrt{H^3 T \log T})$ 的上限，其中 $d$ 表示过渡内核的维数，$H$ 为规划范围，$T$ 为交互总数。这意味着通过优化 $\mathcal{O}(\sqrt{\log T})$ 因子相对于之前指定给线性混合 MDP 的基准（Osband 和 Van Roy，2014），方法得到了增强。我们的方法利用以价值为目标的模型学习视角，引入了解耦论证和方差减少技术，超越了依赖于置信集和集中不等式的传统分析，更有效地形式化了贝叶斯遗憾界限。]]></description>
      <guid>https://arxiv.org/abs/2403.11175</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:29 GMT</pubDate>
    </item>
    <item>
      <title>分布外检测应使用保形预测（反之亦然？）</title>
      <link>https://arxiv.org/abs/2403.11532</link>
      <description><![CDATA[arXiv:2403.11532v1 公告类型：新
摘要：分布外（OOD）检测的研究主要集中于构建有效区分 OOD 数据与分布内（ID）数据的分数。另一方面，保形预测（CP）使用不符合分数来构建具有概率覆盖保证的预测集。在这项工作中，我们建议使用 CP 来更好地评估 OOD 评分的效率。具体来说，我们强调，在标准 OOD 基准设置中，由于测试数据集的样本量有限，评估指标可能过于乐观。基于（Bates 等人，2022）的工作，我们定义了新的共形​​ AUROC 和共形 FRP@TPR95 指标，它们是为这些指标的可变性提供概率保守性保证的修正。我们展示了这些修正对两个参考 OOD 和异常检测基准 OpenOOD（Yang 等人，2022）和 ADBench（Han 等人，2022）的影响。我们还表明，将 OOD 与 CP 一起使用的好处可以反过来使用，即使用 OOD 分数作为不合格分数，从而改进当前的 CP 方法。这些贡献的关键信息之一是，由于 OOD 涉及设计分数，而 CP 涉及解释这些分数，因此这两个领域可能本质上是交织在一起的。]]></description>
      <guid>https://arxiv.org/abs/2403.11532</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:29 GMT</pubDate>
    </item>
    <item>
      <title>用于顺序学习的神经网络的函数空间参数化</title>
      <link>https://arxiv.org/abs/2403.10929</link>
      <description><![CDATA[arXiv:2403.10929v1 公告类型：新
摘要：由于难以合并新数据和保留先验知识，顺序学习范式对基于梯度的深度学习提出了挑战。虽然高斯过程优雅地解决了这些问题，但它们在可扩展性和处理丰富的输入（例如图像）方面遇到了困难。为了解决这些问题，我们引入了一种通过双参数化将神经网络从权重空间转换为函数空间的技术。我们的参数化提供了：（i）一种通过稀疏化将函数空间方法扩展到大型数据集的方法，（ii）当对过去数据的访问受到限制时保留先验知识，以及（iii）一种无需重新训练即可合并新数据的机制。我们的实验表明，我们可以在持续学习中保留知识并有效地整合新数据。我们进一步展示了其在不确定性量化和指导基于模型的强化学习方面的优势。更多信息和代码可在项目网站上找到。]]></description>
      <guid>https://arxiv.org/abs/2403.10929</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:28 GMT</pubDate>
    </item>
    <item>
      <title>在顺序任务设置中最小化局部遗憾的谬误</title>
      <link>https://arxiv.org/abs/2403.10946</link>
      <description><![CDATA[arXiv:2403.10946v1 公告类型：新
摘要：在强化学习（RL）领域，在线强化学习通常被概念化为优化问题，其中算法与未知环境交互以最小化累积遗憾。在固定环境中，可以获得强有力的理论保证，例如次线性（$\sqrt{T}$）后悔界限，这通常意味着收敛到最优策略并停止探索。然而，这些理论设置往往过于简单化了现实世界中 RL 实现中遇到的复杂性，其中任务按顺序到达，任务之间存在重大变化，并且算法可能不允许在某些任务中进行自适应学习。我们研究结果分布之外的变化，包括奖励设计（从结果到奖励的映射）和允许的政策空间的变化。我们的结果揭示了短视地最小化每项任务中的后悔的谬误：在早期任务中获得最佳后悔率可能会导致后续任务中的后悔率更差，即使结果分布保持不变。为了实现所有任务中的最佳累积后悔界限，算法必须在早期任务中进行过度探索。这种理论见解具有实际意义，表明由于任务之间的意外变化（例如，快速的技术发展或人机交互的参与），算法需要比每个任务中通常的固定设置进行更多的探索。这种含义与在移动健康临床试验中使用修剪策略以及在机器人学习中保持$\epsilon$贪婪探索的固定速率的常见做法产生了共鸣。]]></description>
      <guid>https://arxiv.org/abs/2403.10946</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:28 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的高斯过程回归系统可靠性分析</title>
      <link>https://arxiv.org/abs/2403.11125</link>
      <description><![CDATA[arXiv:2403.11125v1 公告类型：新
摘要：基于机器学习的可靠性分析方法在计算效率和准确性方面取得了巨大进步。最近，人们提出了许多有效的学习策略来提高计算性能。然而，很少有人探索理论上的最优学习策略。在本文中，我们提出了几个促进这种探索的定理。具体来说，对考虑和忽略候选设计样本之间的相关性的情况进行了很好的阐述。此外，我们证明了在忽略克里金相关性的情况下，著名的 U 学习函数可以重新表述为最优学习函数。此外，还通过贝叶斯估计和相应的损失函数，在数学上探索了连续多个训练样本富集的理论最优学习策略。仿真结果表明，在减少性能函数评估次数方面，考虑克里金相关性的最优学习策略比忽略克里金相关性和文献中其他最先进的学习函数的效果更好。然而，实现需要研究非常大的计算资源。]]></description>
      <guid>https://arxiv.org/abs/2403.11125</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:28 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯深度学习中的无 Hessian 拉普拉斯</title>
      <link>https://arxiv.org/abs/2403.10671</link>
      <description><![CDATA[arXiv:2403.10671v1 公告类型：新
摘要：贝叶斯后验的拉普拉斯近似（LA）是一种以最大后验估计为中心的高斯分布。它在贝叶斯深度学习中的吸引力源于事后量化不确定性的能力（即在标准网络参数优化之后）、从近似后验中采样的简易性以及模型证据的分析形式。然而，LA的一个重要的计算瓶颈是计算和反转对数后验的Hessian矩阵的必要步骤。 Hessian 矩阵可以通过多种方式进行近似，其质量随网络、数据集和推理任务等多种因素的变化而变化。在本文中，我们提出了一种回避 Hessian 计算和反演的替代框架。无 Hessian 拉普拉斯 (HFL) 近似使用对数后验和网络预测的曲率来估计其方差。只需要两个点估计：标准最大后验参数和网络预测正则化损失下的最优参数。我们证明，在贝叶斯深度学习中 LA 的标准假设下，HFL 的目标与 LA 具有相同的方差，并且可以在预训练网络中有效地摊销。实验证明其性能与精确和近似 Hessian 相当，并且对中间不确定性具有出色的覆盖范围。]]></description>
      <guid>https://arxiv.org/abs/2403.10671</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:27 GMT</pubDate>
    </item>
    <item>
      <title>一种用于更快分布鲁棒优化的原始对偶算法</title>
      <link>https://arxiv.org/abs/2403.10763</link>
      <description><![CDATA[arXiv:2403.10763v1 公告类型：新
摘要：我们考虑具有封闭凸不确定性集的惩罚分布鲁棒优化 (DRO) 问题，该设置包含实践中使用的 $f$-DRO、Wasserstein-DRO 和谱/$L$-风险公式。我们提出了 Drago，一种随机原对偶算法，它在强凸-强凹 DRO 问题上实现了最先进的线性收敛速度。该方法将随机和循环成分与小批量相结合，有效地处理了 DRO 中原始问题和对偶问题的独特不对称性质。我们用分类和回归的数值基准来支持我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2403.10763</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:27 GMT</pubDate>
    </item>
    <item>
      <title>神经核条件均值嵌入</title>
      <link>https://arxiv.org/abs/2403.10859</link>
      <description><![CDATA[arXiv:2403.10859v1 公告类型：新
摘要：核条件均值嵌入（CME）提供了一个强大的框架来表示条件分布，但它们经常面临可扩展性和表达性的挑战。在这项工作中，我们提出了一种新方法，有效地将深度学习与 CME 的优势结合起来，以应对这些挑战。具体来说，我们的方法利用基于内核的目标的端到端神经网络（NN）优化框架。这种设计规避了当前 CME 方法所需的计算成本高昂的 Gram 矩阵求逆。为了进一步提高性能，我们提供了有效的策略来优化剩余的内核超参数。在条件密度估计任务中，我们的 NN-CME 混合实现了有竞争力的性能，并且经常超越现有的基于深度学习的方法。最后，我们通过将其无缝集成到强化学习（RL）环境中来展示其卓越的多功能性。在 Q-learning 的基础上，我们的方法自然会产生分布式强化学习方法的新变体，该方法在不同环境中表现出一致的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.10859</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:27 GMT</pubDate>
    </item>
    <item>
      <title>随机旋转浅水噪声的生成模型</title>
      <link>https://arxiv.org/abs/2403.10578</link>
      <description><![CDATA[arXiv:2403.10578v1 公告类型：新
摘要：在最近的工作中，作者开发了一种用于校准流体动力学随机偏微分方程中噪声的通用方法，其中引入随机性来参数化亚网格尺度过程。在估计天气和气候预测的不确定性时，需要对子网格尺度过程进行随机参数化，以表示子网格尺度波动引起的系统模型误差。先前的方法使用基于随机参数化的增量呈正态分布的假设的主成分分析（PCA）技术。
  在本文中，PCA技术被生成模型技术取代。这使我们能够避免对增量施加额外的限制。该方法在随机旋转浅水模型上进行测试，并将模型的高程变量用作输入数据。数值模拟表明噪声确实是非高斯的。生成建模技术给出了良好的 RMSE、CRPS 评分和预测排名直方图结果。]]></description>
      <guid>https://arxiv.org/abs/2403.10578</guid>
      <pubDate>Tue, 19 Mar 2024 06:16:26 GMT</pubDate>
    </item>
    </channel>
</rss>