<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 14 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Verifix：训练后校正，通过验证样本提高标签噪声鲁棒性</title>
      <link>https://arxiv.org/abs/2403.08618</link>
      <description><![CDATA[arXiv:2403.08618v1 公告类型：交叉
摘要：标签损坏（训练样本标签不正确）会显着降低机器学习模型的性能。这种腐败通常是由非专家标签或对抗性攻击引起的。获取大型、完美标记的数据集成本高昂，而当干净的数据集可用时从头开始重新训练大型模型的计算成本也很高。为了应对这一挑战，我们提出了训练后校正，这是一种新的范例，可以在初始训练后调整模型参数以减轻标签噪声，从而消除重新训练的需要。我们引入了 Verifix，这是一种基于奇异值分解 (SVD) 的新型算法，它利用经过验证的小型数据集，通过单次更新来校正模型权重。 Verifix 使用 SVD 来估计干净激活空间，然后将模型的权重投影到该空间上以抑制与损坏数据相对应的激活。我们展示了 Verifix 对合成和真实标签噪声的有效性。在具有 25% 合成损坏的 CIFAR 数据集上进行的实验显示，泛化能力平均提高了 7.36%。此外，我们观察到 WebVision1.0 和 Clothing1M 等自然损坏的数据集的泛化能力提高了高达 2.63%。]]></description>
      <guid>https://arxiv.org/abs/2403.08618</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:35 GMT</pubDate>
    </item>
    <item>
      <title>通过在线偏好优化对大型语言模型进行人类调整</title>
      <link>https://arxiv.org/abs/2403.08635</link>
      <description><![CDATA[arXiv:2403.08635v1 公告类型：交叉
摘要：确保语言模型的输出与人类偏好保持一致对于保证有用、安全和愉快的用户体验至关重要。因此，人类对齐最近得到了广泛的研究，并且出现了多种方法，例如人类反馈强化学习（RLHF）、直接策略优化（DPO）和序列似然校准（SLiC）。在本文中，我们的贡献有两个。首先，我们展示了两种最近的对齐方法之间的等价性，即身份策略优化（IPO）和纳什镜像下降（Nash-MD）。其次，我们引入了 IPO 的推广，称为 IPO-MD，它利用了 Nash-MD 提出的正则化抽样方法。
  这种等价乍一看似乎令人惊讶，因为 IPO 是一种离线方法，而 Nash-MD 是一种使用偏好模型的在线方法。然而，当我们考虑 IPO 的在线版本时，即当两代人都通过在线政策进行采样并通过经过训练的偏好模型进行注释时，这种等价性就可以得到证明。用这样的数据流来优化IPO损失就相当于通过自我博弈找到偏好模型的纳什均衡。基于这种等价性，我们引入了 IPO-MD 算法，该算法使用与一般 Nash-MD 算法类似的混合策略（在线策略和参考策略之间）生成数据。我们在汇总任务中将在线 IPO 和 IPO-MD 与偏好数据（例如 DPO 和 SLiC）的现有损失的不同在线版本进行比较。]]></description>
      <guid>https://arxiv.org/abs/2403.08635</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:35 GMT</pubDate>
    </item>
    <item>
      <title>利用本地 GPR 将搜索区域限制为较低维度的贝叶斯优化</title>
      <link>https://arxiv.org/abs/2403.08331</link>
      <description><![CDATA[arXiv:2403.08331v1 公告类型：交叉
摘要：许多领域都需要优化产品和系统特性，包括设计和控制。贝叶斯优化（BO）通常在观察成本较高时使用，因为 BO 理论上保证了后悔的上限。然而，计算成本随着待优化参数的数量呈指数级增长，从而降低了搜索效率。我们提出了一种将搜索区域限制为较低维度的 BO，并利用局部高斯过程回归（LGPR）将 BO 扩展到更高维度。 LGPR 将低维搜索区域视为“局部”，从而提高了那里的预测精度。 LGPR 模型是根据该地区特定的本地数据子集进行训练的。这提高了预测精度和搜索效率，并降低了高斯过程回归中矩阵求逆的时间复杂度。在使用20D Ackley和Rosenbrock函数的评估中，搜索效率等于或高于比较方法的搜索效率，比没有LGPR的情况提高了约69%和40%。我们将我们的方法应用于功率半导体器件的自动设计任务。我们成功地将特定导通电阻比传统方法降低了 25%，比没有 LGPR 的情况降低了 3.4%。]]></description>
      <guid>https://arxiv.org/abs/2403.08331</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>部分可观察因果表示学习的稀疏性原理</title>
      <link>https://arxiv.org/abs/2403.08335</link>
      <description><![CDATA[arXiv:2403.08335v1 公告类型：交叉
摘要：因果表示学习旨在从感知数据中识别高级因果变量。大多数方法假设所有潜在因果变量都在高维观察中捕获。相反，我们考虑部分观察的设置，其中每个测量仅提供有关潜在因果状态的子集的信息。之前的工作已经通过多个域或视图研究了这种设置，每个域或视图都依赖于固定的潜在子集。在这里，我们专注于从具有依赖于实例的部分可观察性模式的数据集中的不成对观察中学习。我们的主要贡献是为此设置建立两个可识别性结果：一个用于对底层因果模型没有参数假设的线性混合函数，另一个用于具有高斯潜在因果变量的分段线性混合函数。基于这些见解，我们提出了两种通过在推断表示中强制稀疏来估计潜在因果变量的方法。对不同模拟数据集和建立的基准进行的实验强调了我们的方法在恢复地面真实潜伏方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.08335</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>关于深贝叶斯神经网络后验的局部自适应和可扩展的基于扩散的采样方法的收敛</title>
      <link>https://arxiv.org/abs/2403.08609</link>
      <description><![CDATA[arXiv:2403.08609v1 公告类型：交叉
摘要：为深度神经网络实现鲁棒的不确定性量化是深度学习的许多实际应用中的一个重要要求，例如医学成像，其中需要评估神经网络预测的可靠性。贝叶斯神经网络是一种很有前景的深度神经网络不确定性建模方法。不幸的是，从神经网络的后验分布生成样本是一个重大挑战。这一方向的一个重大进步是将自适应步长（类似于现代神经网络优化器）纳入蒙特卡洛马尔可夫链采样算法，而不会显着增加计算需求。在过去的几年里，有几篇论文介绍了采样算法，并声称它们实现了这一特性。然而，它们确实收敛到正确的分布吗？在本文中，我们证明这些方法在采样的分布中可能存在很大的偏差，即使在消失步长和全批量大小的限制下也是如此。]]></description>
      <guid>https://arxiv.org/abs/2403.08609</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>混合潜在动力学的无监督学习：学习识别框架</title>
      <link>https://arxiv.org/abs/2403.08194</link>
      <description><![CDATA[arXiv:2403.08194v1 公告类型：交叉
摘要：现代应用越来越需要从高维时间序列中无监督地学习潜在动态。这对可识别性提出了重大挑战：许多抽象的潜在表征可以重建观察结果，但它们能否保证对主导动态的充分识别？本文从两个角度研究了这一挑战：使用特定于建模数据的物理归纳偏差，以及将预测目标与用于识别的数据分开的学习识别策略。我们将这两种策略结合在一个新颖的框架中，用于混合潜在动力学（Meta-HyLaD）的无监督元学习：1）一个潜在动态函数，该函数将先前物理学的已知数学表达式与描述其未知错误的神经函数混合在一起，2）一种元学习公式，用于学习单独识别混合动力学的两个组成部分。通过对五种物理系统和一种生物医学系统的广泛实验，我们提供了强有力的证据，证明 Meta-HyLaD 能够整合丰富的先验知识，同时确定其与观测数据的差距。]]></description>
      <guid>https://arxiv.org/abs/2403.08194</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:33 GMT</pubDate>
    </item>
    <item>
      <title>通过基于导数的神经算子实现非线性贝叶斯反演的高效几何马尔可夫链蒙特卡罗</title>
      <link>https://arxiv.org/abs/2403.08220</link>
      <description><![CDATA[arXiv:2403.08220v1 公告类型：交叉
摘要：我们提出了一种算子学习方法来加速几何马尔可夫链蒙特卡罗（MCMC），以解决无限维非线性贝叶斯逆问题。虽然几何MCMC采用适应后局部几何的高质量提案，但它需要计算局部梯度和对数似然的Hessian信息，当通过昂贵的模型模拟定义参数到可观测（PtO）映射时会产生很高的成本。我们考虑一种由 PtO 图的神经算子代理驱动的延迟接受几何 MCMC 方法，该方法旨在利用对数似然的快速代理近似，同时利用其梯度和 Hessian 矩阵。为了实现显着的加速，代理需要准确地预测可观测量及其参数导数（可观测量相对于参数的导数）。通过使用输入输出样本的传统​​算子学习来训练这样的代理通常需要大量的模型模拟。在这项工作中，我们提出了导数通知算子学习的扩展 [O&#39;Leary-Roseberry 等人，J. Comput。 Phys.，496（2024）]使用输入-输出-导数训练样本。这种学习方法产生导数通知神经算子（DINO）代理，它能够以比传统方法低得多的训练成本准确预测可观测值及其参数导数。提供了减少基础 DINO 替代品的成本和误差分析。对 PDE 约束贝叶斯反演的数值研究表明，DINO 驱动的 MCMC 生成有效后验样本的速度比几何 MCMC 快 3--9 倍，比先前基于几何的 MCMC 快 60--97 倍。此外，与几何 MCMC 相比，DINO 代理的训练成本在仅收集 10--25 个有效后验样本后就达到了收支平衡。]]></description>
      <guid>https://arxiv.org/abs/2403.08220</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:33 GMT</pubDate>
    </item>
    <item>
      <title>构建多保真代理模型时表征有害数据源</title>
      <link>https://arxiv.org/abs/2403.08118</link>
      <description><![CDATA[arXiv:2403.08118v1 公告类型：交叉
摘要：近年来，代理建模技术在应用于工业设计问题的建模和优化时受到越来越多的关注。当评估特定设计的性能需要高成本时，这些技术非常相关，因为可以通过构建要查询的模型来代替可用的高成本源来降低总体成本。这些模型的构建有时可以使用其他信息源，这些信息既便宜又不太准确。然而，这些来源的存在提出了构建模型时应使用哪些来源的问题。最近的研究试图描述有害数据源的特征，以指导从业者选择何时忽略某个数据源。这些研究是在综合环境中进行的，使用实践中无法获得的大量数据来表征来源。其中一些研究还被证明可能存在分析中使用的基准存在偏差的问题。在这项研究中，我们仅使用可用于训练替代模型的有限数据来描述有害低保真源的特征。我们采用最近开发的基准过滤技术来进行无偏差评估，为未来的研究提供客观的不同规模的基准套件。通过使用称为实例空间分析的技术分析这些基准套件之一，我们提供了何时应使用低保真源的直观可视化，并使用此分析来提供可在应用工业环境中使用的指南。]]></description>
      <guid>https://arxiv.org/abs/2403.08118</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:32 GMT</pubDate>
    </item>
    <item>
      <title>深度同质神经网络中小初始化的早期定向收敛</title>
      <link>https://arxiv.org/abs/2403.08121</link>
      <description><![CDATA[arXiv:2403.08121v1 公告类型：交叉
摘要：本文研究了训练深度同质神经网络时出现的梯度流动力学，从小初始化开始。目前的工作考虑假设具有局部 Lipschitz 梯度和严格大于 2 的同质性阶的神经网络。本文证明，对于足够小的初始化，在训练的早期阶段，神经网络的权重在范数中保持很小，并且沿着 [ 中引入的神经相关函数的 Karush-Kuhn-Tucker (KKT) 点的方向近似收敛。 1]。此外，对于平方损失并且在神经网络权重的可分离性假设下，在损失函数的某些鞍点附近显示了梯度流动力学的类似方向收敛。]]></description>
      <guid>https://arxiv.org/abs/2403.08121</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:32 GMT</pubDate>
    </item>
    <item>
      <title>通过热扩散进行高效组合优化</title>
      <link>https://arxiv.org/abs/2403.08757</link>
      <description><![CDATA[arXiv:2403.08757v1 公告类型：新
摘要：组合优化问题广泛存在，但由于其离散性质，本身就具有挑战性。现有方法的主要限制是它们在每次迭代时只能访问解空间的一小部分，导致搜索全局最优的效率有限。为了克服这一挑战，与扩大求解器搜索范围的传统努力不同，我们专注于使信息能够通过热扩散主动传播到求解器。通过变换目标函数同时保持其最优值，热扩散促进信息从遥远区域流向求解器，从而提供更高效的导航。利用热扩散，我们提出了一个解决一般组合优化问题的框架。所提出的方法在一系列最具挑战性和广泛遇到的组合优化中展示了卓越的性能。与利用热力学生成人工智能的最新进展相呼应，我们的研究进一步揭示了其在推进组合优化方面的巨大潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.08757</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:31 GMT</pubDate>
    </item>
    <item>
      <title>绘制热核：使用高斯过程嵌入数据</title>
      <link>https://arxiv.org/abs/2403.07929</link>
      <description><![CDATA[arXiv:2403.07929v1 公告类型：交叉
摘要：本文介绍了一种新颖的非确定性方法，用于在低维欧几里得空间中嵌入数据，该方法基于取决于数据几何形状的高斯过程的计算实现。这种类型的嵌入首次出现在（Adler et al, 2018）中，作为高维通用流形的理论模型。
  特别地，我们将高斯过程的协方差函数作为热核，计算嵌入相当于绘制一个表示热核的矩阵。 Karhunen-Lo`eve 展开揭示了嵌入中的直线距离在概率意义上近似于扩散距离，避免了尖锐截止的需要并保持了一些较小尺度的结构。
  我们的方法在对异常值的鲁棒性方面展示了进一步的优势。我们通过理论和实验证明了该方法的合理性。]]></description>
      <guid>https://arxiv.org/abs/2403.07929</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:31 GMT</pubDate>
    </item>
    <item>
      <title>利用稀缺数据进行科学机器学习的多重保真线性回归</title>
      <link>https://arxiv.org/abs/2403.08627</link>
      <description><![CDATA[arXiv:2403.08627v1 公告类型：新
摘要：机器学习（ML）方法适合给定参数化模型类的参数数据，作为学习传统仿真成本昂贵的复杂工程系统的替代模型的潜在方法，引起了人们的极大兴趣。然而，在许多科学和工程环境中，生成用于训练 ML 模型的高保真数据的成本很高，而且用于生成训练数据的可用预算也很有限。在所得的稀缺高保真数据上训练的 ML 模型具有高方差，并且对训练数据集的变化很敏感。我们提出了一种用于科学机器学习的新的多保真度训练方法，该方法利用了可获得不同保真度和成本数据的科学背景；例如，高保真度数据可能由昂贵的完全解析物理模拟生成，而低保真度数据可能来自基于简化假设的更便宜的模型。我们使用多保真度数据为线性回归模型的未知参数定义新的多保真度蒙特卡罗估计量，并提供理论分析来保证该方法的准确性并提高对小训练预算的鲁棒性。数值结果验证了理论分析，并表明，与仅在成本相当的高保真数据上训练的标准模型相比，在稀缺的高保真数据和额外的低保真数据上训练的多保真学习模型可以实现更低数量级的模型方差。这说明在稀缺数据情况下，我们的多保真度训练策略产生的模型的预期误差低于标准训练方法。]]></description>
      <guid>https://arxiv.org/abs/2403.08627</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:30 GMT</pubDate>
    </item>
    <item>
      <title>深度网络的神经再生核 Banach 空间和表示定理</title>
      <link>https://arxiv.org/abs/2403.08750</link>
      <description><![CDATA[arXiv:2403.08750v1 公告类型：新
摘要：研究神经网络定义的函数空间有助于理解相应的学习模型及其归纳偏差。虽然在某些限制下，神经网络对应于再现内核希尔伯特空间的函数空间，但这些机制并未捕获实践中使用的网络的属性。相比之下，在本文中，我们展示了深度神经网络定义了合适的再生内核巴拿赫空间。
  这些空间配备了强制稀疏形式的规范，使它们能够适应输入数据及其表示中的潜在潜在结构。特别是，利用再现内核 Banach 空间的理论，结合变分结果，我们推导出代表定理，证明应用中常用的有限体系结构是合理的。我们的研究扩展了浅层网络的类似结果，并且可以被视为朝着考虑更实际合理的神经架构迈出的一步。]]></description>
      <guid>https://arxiv.org/abs/2403.08750</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:30 GMT</pubDate>
    </item>
    <item>
      <title>超越线性缩放机制的随机特征回归的渐近</title>
      <link>https://arxiv.org/abs/2403.08160</link>
      <description><![CDATA[arXiv:2403.08160v1 公告类型：新
摘要：机器学习的最新进展是通过使用经过训练直到训练数据接近插值的过参数化模型来实现的。例如，通过双下降现象表明，参数数量不能很好地代表模型复杂性和泛化能力。这就留下了理解参数化对这些模型性能的影响的问题。模型复杂性和泛化性如何取决于参数 $p$ 的数量？我们应该如何选择相对于样本量$n$的$p$以达到最佳测试误差？
  在本文中，我们研究了随机特征岭回归（RFRR）的示例。该模型可以被视为核岭回归 (KRR) 的有限秩近似，也可以被视为在所谓的惰性机制中训练的神经网络的简化模型。我们考虑协变量均匀分布在 $d$ 维球体上，并计算高维多项式缩放中 RFRR 测试误差的锐渐近，其中 $p,n,d \to \infty$ 而 $p/ d^{\对于所有 $\kappa_1 、 \kappa_2 \in \mathbb{R}_{&gt;0}$，kappa_1}$ 和 $n / d^{\kappa_2}$ 保持不变。这些渐近精确地表征了随机特征的数量和正则化参数对测试性能的影响。特别是，RFRR 表现出近似能力和泛化能力之间的直观权衡。对于 $n = o(p)$，样本大小 $n$ 是瓶颈，RFRR 实现与 KRR 相同的性能（相当于取 $p = \infty$）。另一方面，如果 $p = o(n)$，则随机特征的数量 $p$ 是限制因素，并且 RFRR 测试误差与随机特征模型类的近似误差相匹配（类似于采用 $n = \infty $）。最后，在 $n= p$ 处出现双重下降，这种现象以前仅在线性缩放 $\kappa_1 = \kappa_2 = 1$ 中表征。]]></description>
      <guid>https://arxiv.org/abs/2403.08160</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:29 GMT</pubDate>
    </item>
    <item>
      <title>平均场微正则梯度下降</title>
      <link>https://arxiv.org/abs/2403.08362</link>
      <description><![CDATA[arXiv:2403.08362v1 公告类型：新
摘要：微正则梯度下降是基于能量的模型的采样过程，允许对高维分布进行有效采样。它的工作原理是使用梯度下降将样本从高熵分布（例如高斯白噪声）传输到低能量区域。我们将该模型置于标准化流的框架中，展示了它如何经常因在下降过程中丢失不必要的熵而过度拟合。作为一种补救措施，我们提出了一种平均场微正则梯度下降，它同时对几个弱耦合数据点进行采样，从而可以更好地控制熵损失，同时在似然拟合方面付出很少的代价。我们在金融时间序列的背景下研究这些模型，说明合成数据和真实数据的改进。]]></description>
      <guid>https://arxiv.org/abs/2403.08362</guid>
      <pubDate>Thu, 14 Mar 2024 06:16:29 GMT</pubDate>
    </item>
    </channel>
</rss>