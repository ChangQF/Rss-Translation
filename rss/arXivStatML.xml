<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Tue, 23 Jan 2024 06:18:06 GMT</lastBuildDate>
    <item>
      <title>通过方差减少草图进行非参数估计。 （arXiv：2401.11646v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.11646</link>
      <description><![CDATA[非参数模型在各种科学和领域中引起了极大的兴趣
工程学科。经典核方法，虽然在数值上稳健且
在低维环境中统计上合理，在
由于维度诅咒而导致更高维度的设置。在本文中，
我们引入了一个称为方差减少草图（VRS）的新框架，
专门设计用于估计密度函数和非参数函数
更高维度的回归函数，减少了诅咒
维度。我们的框架将多变量函数概念化为
无限大小的矩阵，并促进了一种新的草图技术
减少估计方差的数值线性代数文献
问题。我们通过
一系列的模拟实验和真实世界的数据应用。值得注意的是，VRS
与现有的神经网络估计器相比显示出显着的改进
众多密度估计和非参数中的经典核方法
回归模型。此外，我们还为 VRS 提供了理论依据
支持其提供非参数估计并减少灾难的能力
维度。
]]></description>
      <guid>http://arxiv.org/abs/2401.11646</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:06 GMT</pubDate>
    </item>
    <item>
      <title>噪声背景下随机强盗的汤普森采样：信息论遗憾分析。 （arXiv：2401.11565v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11565</link>
      <description><![CDATA[我们探索随机上下文线性强盗问题，其中代理
通过噪声通道观察真实上下文的噪声、损坏版本
具有未知的噪声参数。我们的目标是设计一项行动政策
可以近似”预言机，它可以访问奖励模型，
通道参数，以及真实上下文的预测分布
观察到的噪声环境。在贝叶斯框架中，我们引入 Thompson
具有高斯上下文噪声的高斯老虎机采样算法。领养
通过信息论分析，我们证明了我们的贝叶斯遗憾
有关预言机行动策略的算法。我们也将这个问题延伸到
代理在观察真实上下文后有一定延迟的场景
获得奖励并表明延迟的真实上下文导致贝叶斯较低
后悔。最后，我们凭经验证明了所提出的性能
针对基线的算法。
]]></description>
      <guid>http://arxiv.org/abs/2401.11565</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>了解后期学习率衰减的泛化优势。 （arXiv：2401.11600v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11600</link>
      <description><![CDATA[为什么神经网络要以大学习率训练更长时间
通常会导致更好的泛化？在本文中，我们深入探讨这个问题
通过检查神经网络中训练和测试损失之间的关系。
通过这些损失的可视化，我们注意到训练轨迹
以较大的学习率导航通过最小流形
训练损失，最终接近测试损失最小值的邻域。
受这些发现的启发，我们引入了一个非线性模型，其损失
景观反映了真实神经网络观察到的景观。经调查
在我们的模型上使用 SGD 的训练过程中，我们证明了一个扩展
具有较大学习率的阶段将我们的模型引向最低范数
训练损失的解决方案，可以实现接近最优的泛化，
从而证实了根据经验观察到的后期学习率的好处
衰变。
]]></description>
      <guid>http://arxiv.org/abs/2401.11600</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>有效的局部线性正则化可克服灾难性的过度拟合。 （arXiv：2401.11618v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11618</link>
      <description><![CDATA[单步对抗训练 (AT) 中的灾难性过度拟合 (CO)
导致对抗性测试准确度突然下降（甚至下降到 0%）。为了
使用多步 AT 训练的模型，观察到损失函数
相对于输入呈局部线性行为，但是这会丢失
单步AT。为了解决单步 AT 中的 CO 问题，已经提出了几种方法
提议通过正则化强制损失的局部线性。然而，
由于 Double，这些正则化项大大减慢了训练速度
反向传播。相反，在这项工作中，我们引入了正则化项，
称为 ELLE，在经典 AT 中有效且高效地减少 CO
评估，以及一些更困难的制度，例如大型对抗性制度
干扰和长期的训练计划。我们的正则化项可以是
理论上与损失函数的曲率相关，并且在计算上
通过避免双重反向传播，比以前的方法便宜。我们彻底的
实验验证表明我们的工作不会受到二氧化碳的影响，
即使是在以前的作品受到影响的具有挑战性的环境中。我们也
请注意，在训练期间调整我们的正则化参数（ELLE-A）
极大地提高了性能，特别是在大型 $\epsilon$ 设置中。我们的
实现可在 https://github.com/LIONS-EPFL/ELLE 中找到。
]]></description>
      <guid>http://arxiv.org/abs/2401.11618</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:05 GMT</pubDate>
    </item>
    <item>
      <title>MoMA：用于离线强化学习的基于模型的 Mirror Ascent。 （arXiv：2401.11380v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11380</link>
      <description><![CDATA[基于模型的离线强化学习方法（RL）已实现
由于其在许多决策问题上的最先进的性能
样本效率和普遍性。尽管取得了这些进步，现有的
基于模型的离线强化学习方法要么专注于理论研究，要么专注于理论研究，而不关注理论研究。
开发实用算法或依赖受限参数策略
空间，从而没有充分利用不受限制的政策空间的优势
基于模型的方法所固有的。为了解决这个限制，我们开发了 MoMA，
基于模型的镜像上升算法，具有一般函数逼近
部分覆盖离线数据。现代艺术博物馆 (MoMA) 与现有的
通过采用不受限制的政策类别来研究文学。在每次迭代中，MoMA
通过最小化过程保守估计价值函数
政策评估步骤中的一组转换模型的置信度，然后
使用一般函数近似更新策略，而不是
策略改进步骤中常用的参数策略类。在下面
一些温和的假设，我们通过证明来建立 MoMA 的理论保证
返回策略次优性的上限。我们还提供
该算法的实际可实现的近似版本。这
现代艺术博物馆的有效性通过数值研究得到证明。
]]></description>
      <guid>http://arxiv.org/abs/2401.11380</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>使用基于 Wasserstein 距离的重新称重增强选择性。 （arXiv：2401.11562v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.11562</link>
      <description><![CDATA[给定两个标记数据集 $\mathcal{S}$ 和 $\mathcal{T}$，我们设计一个
简单高效的贪心算法重新权衡损失函数，使得
神经网络权重的极限分布由
$\mathcal{S}$ 上的训练接近极限分布
通过 $\mathcal{T}$ 训练得到的结果。

在理论方面，我们证明当输入的度量熵
数据集是有界的，我们的贪心算法输出接近最优
重新加权，即网络权重的两个不变分布将是
可证明总变异距离接近。而且算法很简单
且可扩展，我们还证明了算法效率的界限。

我们的算法可以故意引入分布平移来执行（软）
多标准优化。作为一个激励应用程序，我们训练一个神经网络
识别 MNK2（一种 MAP 激酶，负责细胞
信号），它们不与 MNK1（一种高度相似的蛋白质）结合。我们调
算法的参数使得保留损失的总体变化可以忽略不计，
但选择性，即前 100 个 MNK2 结合物中 MNK1 的比例
由于我们重新称重，非粘合剂从 54% 增加到 95%。的
43 种不同的小分子预计对烯胺最具选择性
目录中，2个小分子经实验验证具有选择性，即
他们将 MNK2 的酶活性降低到 50\% 以下，但没有将 MNK1 的酶活性降低到 10$\mu$M
-- 5% 的成功率。
]]></description>
      <guid>http://arxiv.org/abs/2401.11562</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:04 GMT</pubDate>
    </item>
    <item>
      <title>AFS-BM：通过二进制掩蔽的自适应特征选择来增强模型性能。 （arXiv：2401.11250v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11250</link>
      <description><![CDATA[我们研究一般机器学习（ML）中的特征选择问题
背景，这是该领域最关键的主题之一。虽然，
存在许多特征选择方法，但是这些方法面临着
诸如可扩展性、管理高维数据、处理
相关特征，适应可变特征重要性，并集成
领域知识。为此，我们引入了“自适应特征选择”
二进制掩码”（AFS-BM）可以解决这些问题。AFS-BM 实现
这是通过同时进行特征选择和模型的联合优化来实现的
训练。特别是，我们进行联合优化和二进制掩蔽
在训练过程中不断调整特征集和模型参数
过程。这种方法可以显着提高模型的准确性和
减少计算需求。我们提供广泛的
我们将 AFS-BM 与已建立的特征选择进行比较的实验
使用来自现实生活比赛的众所周知的数据集的方法。我们的结果显示
AFS-BM 在准确性方面有了显着的提高，并且需要
计算复杂度显着降低。这是由于 AFS-BM 能够
动态调整以适应训练期间特征重要性的变化
过程，这对该领域做出了重要贡献。我们公开分享我们的代码
为了我们的结果的可复制性并促进进一步的研究。
]]></description>
      <guid>http://arxiv.org/abs/2401.11250</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>通过（正交）审查无偏见学习来估计生存结果的异质治疗效果。 （arXiv：2401.11263v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2401.11263</link>
      <description><![CDATA[估计异质治疗效果 (HTE) 的方法
观察数据主要集中于连续或二元结果，
对生存结果的关注较少，几乎不关注有
竞争风险。在这项工作中，我们开发了审查无偏变换
（CUT）用于有和没有竞争风险的生存结果。之后
使用这些 CUT 转换事件时间结果，直接应用 HTE
学习者对连续结果产生一致的异质估计
累积发生效应、总效应和可分离直接效应。我们的
CUT 能够应用更多最先进的 HTE 学习者
对于审查结果比以前可用的，特别是在
竞争风险设置。我们提供通用的无模型学习者专用预言机
限制有限样本超额风险的不平等。预言机效率
结果取决于预言机选择器和估计的所有干扰函数
转型中涉及的步骤。我们展示了实证表现
模拟研究中提出的方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.11263</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>量子机器学习：从 NISQ 到容错。 (arXiv:2401.11351v1 [quant-ph])</title>
      <link>http://arxiv.org/abs/2401.11351</link>
      <description><![CDATA[量子机器学习，涉及运行机器学习算法
量子器件方面的研究引起了学术界和学术界的广泛关注
商界。在本文中，我们提供了全面且公正的评论
量子机器领域出现的各种概念
学习。这包括噪声中尺度量子中使用的技术
（NISQ）兼容算法的技术和方法
容错量子计算硬件。我们的审查涵盖了基本
与量子相关的概念、算法和统计学习理论
机器学习。
]]></description>
      <guid>http://arxiv.org/abs/2401.11351</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:03 GMT</pubDate>
    </item>
    <item>
      <title>用于加权最近邻算法的高效数据 Shapley。 （arXiv：2401.11103v1 [cs.DS]）</title>
      <link>http://arxiv.org/abs/2401.11103</link>
      <description><![CDATA[这项工作旨在解决数据评估文献中的一个开放问题
关于加权 $K$ 最接近的 Data Shapley 的有效计算
邻居算法（WKNN-Shapley）。通过考虑硬标签的准确性
以离散权重作为效用函数的 KNN，我们重新构建了
将 WKNN-Shapley 计算为计数问题并引入
二次时间算法，较 $O(N^K)$ 有显着改进，
现有文献的最佳结果。我们开发确定性近似
算法进一步提高计算效率，同时保持
Shapley 值的关键公平属性。通过大量的实验，我们
展示WKNN-Shapley的计算效率及其优越性
与未加权的同行相比，在辨别数据质量方面的性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.11103</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>通过工具变量识别和估计条件平均部分因果效应。 （arXiv：2401.11130v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11130</link>
      <description><![CDATA[最近人们对估计异质性产生了很大的兴趣
因果效应。在本文中，我们引入了条件平均部分因果
效应（CAPCE）揭示连续因果效应的异质性
治疗。我们提供在工具中识别 CAPCE 的条件
变量设置。我们开发了三个 CAPCE 估计器系列：筛子、
基于参数化和再现核希尔伯特空间（RKHS），并进行分析
他们的统计特性。我们说明了拟议的 CAPCE 估计量
合成数据和真实世界数据。
]]></description>
      <guid>http://arxiv.org/abs/2401.11130</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>使用半定规划对群体聚类进行去偏和局部分析。 （arXiv：2401.10927v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.10927</link>
      <description><![CDATA[在本文中，我们考虑对小数据样本进行分区的问题
大小 $n$ 是从 $2$ 亚高斯分布的混合中得出的。尤其，
我们分析同一作者提出的计算高效算法，以
大约根据数据的总体将数据分为两组
给出了小样本的起源。这项工作的动机是应用
使用 $p$ 根据其原籍人口对个体进行聚类
当任意两个群体之间的差异很小时，可以使用标记。我们
建立在整数二次规划的半定松弛基础上
本质上公式化为寻找图上的最大割，其中边
切割中的权重表示两个节点之间的不相似性分数，基于
他们的 $p$ 功能。这里我们用 $\Delta^2 :=p \gamma$ 来表示 $\ell_2^2$
两个中心之间的距离（均值向量），即$\mu^{(1)}$、$\mu^{(2)}$
$\in$$\mathbb{R}^p$。目标是允许在之间进行全方位的权衡
$n, p, \gamma$ 是指部分恢复（成功率 $&lt; 100\%$）
一旦信噪比 $s^2 := \min\{np \gamma^2, \Delta^2\}$ 可行
下界为常数。重要的是，我们证明了
错误分类误差相对于 SNR $s^2$ 呈指数衰减。
这个结果是之前提出的，没有完整的证明。因此我们提出
当前工作中的完整证明。最后，对于平衡分区，我们
考虑 SDP1 的一个变体，并表明新的估计器具有极好的
去偏属性。据我们所知，这是新颖的。
]]></description>
      <guid>http://arxiv.org/abs/2401.10927</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>具有结构化变分族的可证明可扩展的黑盒变分推理。 （arXiv：2401.10989v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.10989</link>
      <description><![CDATA[具有满秩协方差近似的变分族尚不为人所知
在黑盒变分推理（BBVI）中表现良好，无论是经验上还是
理论上。事实上，BBVI 最近的计算复杂度结果
确定全等级变分家族与
问题的维度与例如平均田野家庭。这是
对于具有局部变量的分层贝叶斯模型尤其重要；
它们的维度随着数据集的大小而增加。因此，一
获取迭代复杂度，并显式依赖于 $\mathcal{O}(N^2)$
数据集大小 $N$。在本文中，我们探索了一个理论上的中间立场
平均场变分族和满秩族之间：结构化
变分族。我们严格证明某些尺度的矩阵结构
可以实现更好的迭代复杂度$\mathcal{O}(N)$，意味着更好
相对于 $N$ 的缩放。我们通过实证验证了我们的理论结果
大规模分层模型。
]]></description>
      <guid>http://arxiv.org/abs/2401.10989</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>从聚合响应中学习：实例级别与包级别损失函数。 （arXiv：2401.11081v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.11081</link>
      <description><![CDATA[由于隐私问题的兴起，在许多实际应用中
训练数据在与学习者共享之前先进行聚合，以便
保护用户敏感响应的隐私。在综合学习中
框架中，数据集被分组为样本袋，其中每个样本袋是
仅适用于汇总响应，提供个人的摘要
那个袋子里有回复。在本文中，我们研究了两个自然损失函数
从聚合响应中学习：包级损失和实例级损失。
在前者中，模型是通过最小化聚合之间的损失来学习的
响应和聚合模型预测，而后者的模型目标
使个人预测适合总体反应。在这项工作中，我们展示了
实例级损失可以被视为正则化形式
袋级损失。这一观察结果让我们可以比较这两种方法
尊重所得估计量的偏差和方差，并引入一种新颖的
结合了两种方法的插值估计器。对于线性
回归任务，我们提供了风险的精确表征
在渐近状态中插值估计器，其中训练的大小
集合与特征维度成比例增长。我们的分析使我们能够
从理论上理解不同因素（例如袋子尺寸）的影响
模型预测风险。此外，我们还提出了一种机制
从总体响应中进行差异化私人学习并得出最优
包大小在预测风险与隐私权衡方面的影响。我们还开展
彻底的实验来证实我们的理论并展示其功效
插值估计器。
]]></description>
      <guid>http://arxiv.org/abs/2401.11081</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>用于随机优化的 Hessian 逆矩阵的在线估计，并应用于通用随机牛顿算法。 （arXiv：2401.10923v1 [数学.OC]）</title>
      <link>http://arxiv.org/abs/2401.10923</link>
      <description><![CDATA[本文讨论了用于估计的二阶随机优化
写为期望的凸函数的最小化器。直接递归
使用 Robbins-Monro 的 Hessian 逆矩阵估计技术
介绍了程序。这种方法可以大大减少
计算复杂度。最重要的是，它允许开发通用随机模型
牛顿方法并研究所提出的渐近效率
方法。这项工作因此扩展了二阶算法的应用范围
在随机优化中。
]]></description>
      <guid>http://arxiv.org/abs/2401.10923</guid>
      <pubDate>Tue, 23 Jan 2024 06:18:00 GMT</pubDate>
    </item>
    </channel>
</rss>