<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Wed, 13 Dec 2023 03:14:33 GMT</lastBuildDate>
    <item>
      <title>平均嵌入上的分布式 Bellman 算子。 （arXiv：2312.07358v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.07358</link>
      <description><![CDATA[我们提出了一种新颖的分布式强化算法框架
学习，基于学习回报的有限维平均嵌入
分布。我们推导出几种新的动态规划算法
基于该框架的时差学习，提供渐近
收敛理论，并检查算法的实证性能
一套表格任务。此外，我们表明这种方法可以
直接与深度强化学习结合，得到新的
深度 RL 代理，改进了基线分布式方法
街机学习环境。
]]></description>
      <guid>http://arxiv.org/abs/2312.07358</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:33 GMT</pubDate>
    </item>
    <item>
      <title>使用核方法进行标签移位适应的类概率匹配。 （arXiv：2312.07282v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.07282</link>
      <description><![CDATA[在域适应中，协变量偏移和标签偏移问题是两个
不同且互补的任务。在协变量平移适应中，
数据分布的差异源于特征的变化
概率，现有的方法自然地解决这个问题基于
\textit{特征概率匹配} (\textit{FPM})。然而，对于标签转移
数据分布差异完全源于的适应
类概率的变化，当前的方法仍然使用 FPM
$d$维特征空间来估计类别概率比
一维标签空间。更自然地解决标签转移适应问题
并且有效地受到源域类的新表示的启发
概率，我们提出了一个新的框架，称为 \textit{类概率
匹配} (\textit{CPM}) 匹配两个类概率函数
一维标签空间来估计类别概率比，
与在 $d$ 维特征上运行的 FPM 根本不同
空间。此外，通过将核逻辑回归合并到
CPM框架来估计条件概率，我们提出了一个算法
称为 \textit{使用核方法进行类概率匹配}
(\textit{CPMKM}) 用于标签移位适应。从理论角度来看，
我们建立了 CPMKM 的最优收敛速率
多类标签移位适应的交叉熵损失。来自
实验角度，与真实数据集的比较表明 CPMKM
优于现有的基于 FPM 和基于最大似然的算法。
]]></description>
      <guid>http://arxiv.org/abs/2312.07282</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:32 GMT</pubDate>
    </item>
    <item>
      <title>Bandit 问题中的强制探索。 （arXiv：2312.07285v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07285</link>
      <description><![CDATA[多臂老虎机（MAB）是一个经典的顺序决策问题。最多
工作需要对奖励分配进行假设（例如，有界），而
从业者可能很难获得有关这些的信息
为他们的问题设计模型的分布，特别是在非平稳中
人与生物圈计划问题。本文旨在设计一种多臂老虎机算法
无需使用有关奖励分配的信息即可实现
仍然达到了相当大的遗憾上限。为此，我们提出一个
在贪婪规则和强制探索之间交替的新颖算法。我们的
方法可以应用于高斯、伯努利和其他亚高斯
发行版，其实现不需要额外的信息。
我们对不同的强制探索策略采用统一的分析方法
并为固定和提供与问题相关的遗憾上限
分段平稳设置。此外，我们将我们的算法与
不同奖励分布上的流行强盗算法。
]]></description>
      <guid>http://arxiv.org/abs/2312.07285</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:32 GMT</pubDate>
    </item>
    <item>
      <title>使用方差特征归因识别预测不确定性的驱动因素。 （arXiv：2312.07252v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07252</link>
      <description><![CDATA[可解释性和不确定性量化是可信的两大支柱
人工智能。然而，不确定性估计背后的原因是
通常无法解释。识别不确定性互补的驱动因素
在认识潜在模型局限性时对点预测的解释。
它有助于检测不确定性中的过度简化
估计过程。对不确定性的解释增强沟通和信任
在决策中。它们允许验证模型的主要驱动因素是否
不确定性是相关的，并且可能会影响模型的使用。到目前为止，主题为
解释不确定性的研究很少。少数例外在
现有文献是针对贝叶斯神经网络量身定制的，或者严重依赖于
技术上复杂的方法阻碍了其广泛采用。我们建议
方差特征归因，一个简单且可扩展的解决方案来解释
预测性的任意不确定性。首先，我们将不确定性估计为
通过为神经网络配备高斯输出来预测方差
通过添加方差输出神经元进行分布。因此，我们可以信赖
预训练的点预测模型并对它们进行微调以获得有意义的方差
估计。其次，我们对方差输出应用开箱即用的解释器
这些模型来解释不确定性估计。我们评估我们的方法
在数据生成过程已知的综合环境中。我们表明
我们的方法可以比传统方法更可靠、更快速地解释不确定性的影响
建立基线 CLUE。我们微调最先进的年龄回归模型
估计不确定性并获得归因。我们的解释强调
不确定性的潜在来源，例如笑线。方差特征
归因为不确定性估计提供了准确的解释
对模型架构的修改很少，计算开销也很低。
]]></description>
      <guid>http://arxiv.org/abs/2312.07252</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:31 GMT</pubDate>
    </item>
    <item>
      <title>分析标签噪声下分类器的鲁棒性。 （arXiv：2312.07271v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07271</link>
      <description><![CDATA[本研究探讨了标签噪声分类器的鲁棒性，旨在
增强模型针对复杂现实场景中的噪声数据的弹性。
监督学习中的标签噪声，其特征是错误或不精确
标签，显着损害模​​型性能。这项研究的重点是
标签噪声对实际应用的影响日益成为相关问题。
为了解决训练数据标签不准确的普遍挑战，我们
集成对抗性机器学习 (AML) 和重要性重新加权
技术。我们的方法涉及采用卷积神经网络 (CNN)
作为基础模型，重点是参数调整
单独的训练样本。该策略旨在提高模型的
重点关注对性能有关键影响的样本。
]]></description>
      <guid>http://arxiv.org/abs/2312.07271</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:31 GMT</pubDate>
    </item>
    <item>
      <title>安全多任务贝叶斯优化。 （arXiv：2312.07281v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07281</link>
      <description><![CDATA[贝叶斯优化已成为安全在线优化的强大工具
系统，由于其高采样效率和噪声鲁棒性。为了更进一步的
系统的加速简化物理模型可以合并到
优化以加速这一过程，因为模型能够提供
实际系统的近似值，并且从它们中采样是显着的
更便宜。模型与现实之间的相似性由附加的表示
超参数并在优化过程中学习。安全是一个
贝叶斯优化等在线优化方法的重要标准，
最近的文献已经解决了这个问题，提供了安全保证
在已知超参数的假设下。然而，实际上这并不是
适用的。因此，我们扩展了鲁棒高斯过程均匀误差
边界以满足多任务设置，其中涉及计算
利用超参数后验分布的置信区域
马尔可夫链蒙特卡罗方法。然后，使用稳健的安全界限，
应用贝叶斯优化来安全地优化系统，同时
结合模型的测量。模拟表明
与其他优化相比可以显着加速
取决于保真度的最先进的安全贝叶斯优化方法
的模型。
]]></description>
      <guid>http://arxiv.org/abs/2312.07281</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:31 GMT</pubDate>
    </item>
    <item>
      <title>向量值正则化最小二乘算法的最优 Sobolev 范数率。 （arXiv：2312.07186v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.07186</link>
      <description><![CDATA[我们提出了无限维向量值的第一个最优速率
在 $L_2$ 之间插值的连续范数上的岭回归
以及假设空间，我们将其视为向量值再现
核希尔伯特空间。这些比率允许处理错误指定的情况，其中
真实的回归函数不包含在假设空间中。我们
将假设空间容量的标准假设与
向量值插值空间的新颖张量积构造
来表征回归函数的平滑程度。我们的上层
边界不仅达到与实值核岭回归相同的速率，
但也消除了目标回归函数有界的假设。
对于下界，我们使用以下方法将问题简化为标量设置：
投影论证。我们证明这些比率在大多数情况下都是最佳的，并且
与输出空间的维度无关。我们展示了我们的结果
向量值 Sobolev 空间的特殊情况。
]]></description>
      <guid>http://arxiv.org/abs/2312.07186</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:30 GMT</pubDate>
    </item>
    <item>
      <title>MCFNet：用于实时语义分割的多尺度协方差特征融合网络。 （arXiv：2312.07207v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.07207</link>
      <description><![CDATA[低层空间细节信息和高层语义抽象
信息对于语义分割任务都是必不可少的。特点
通过深度网络提取可以获得丰富的语义信息，同时很多
的空间信息丢失。然而，如何恢复空间细节
信息有效地与高级语义融合尚未得到很好的解决
到目前为止已解决。在本文中，我们提出了一种基于
称为多尺度协方差特征的双边分割网络（BiseNet）
融合网络（MCFNet）。具体来说，该网络引入了一个新功能
细化模块和新的特征融合模块。此外，还有一个选通单元
提出名为L-Gate，过滤掉无效信息并融合多尺度
特征。我们在 Cityscapes、CamVid 数据集和
将其与最先进的方法进行比较。大量实验表明
我们的方法取得了竞争上的成功。在 Cityscapes 上，我们实现了 75.5% mIOU
速度为 151.3 FPS。
]]></description>
      <guid>http://arxiv.org/abs/2312.07207</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:30 GMT</pubDate>
    </item>
    <item>
      <title>高斯线性隐马尔可夫模型：Python 包。 （arXiv：2312.07151v1 [q-bio.NC]）</title>
      <link>http://arxiv.org/abs/2312.07151</link>
      <description><![CDATA[我们提出了高斯线性隐马尔可夫模型（GLHMM），这是一种概括
神经科学中常用的不同类型的 HMM。简而言之，GLHMM
是一个通用框架，其中线性回归用于灵活参数化
高斯状态分布，从而适应广泛的用途
-包括无监督、编码和解码模型。 GLHMM 的实现为
一个Python工具箱，重点是统计测试和样本外
预测 - 即旨在发现和表征大脑行为
协会。该工具箱使用随机变分推理方法，
使其能够在合理的计算时间内处理大型数据集。
总的来说，该方法可以应用于多种数据模式，包括
动物记录或非大脑数据，并广泛应用
实验范式。为了进行演示，我们展示了 fMRI 示例，
皮质电图、脑磁图和瞳孔测量。
]]></description>
      <guid>http://arxiv.org/abs/2312.07151</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>研究学习优化器的训练动态。 （arXiv：2312.07174v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07174</link>
      <description><![CDATA[优化是现代深度学习不可或缺的一部分。最近，
学习优化器的概念已经出现，作为加速这一过程的一种方式
通过替换传统的手工算法来优化过程
元学习函数。尽管这些方法最初取得了有希望的结果，
稳定性和通用性问题仍然存在，限制了其实际应用
使用。此外，他们在不同条件下的内部运作和行为是
尚未完全理解，因此很难提出改进。为了
因此，我们的工作从以下方面检查了它们的优化轨迹：
网络架构对称性和参数更新的视角
分布。此外，通过将学习到的优化器与其
手动设计的对应物，我们确定了几个关键见解
展示每种方法如何能够从另一种方法的优势中受益。
]]></description>
      <guid>http://arxiv.org/abs/2312.07174</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>非平滑随机镜像下降的一般尾界。 （arXiv：2312.07142v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07142</link>
      <description><![CDATA[在本文中，我们为优化误差提供了新颖的尾界
用于凸面和 Lipschitz 物镜的随机镜面下降。我们的分析
扩展了经典轻尾亚高斯分布的现有尾部边界
噪声情况到重尾噪声状态。我们研究了优化误差
最后一次迭代以及迭代的平均值。我们实例化我们的
导致两种重要情况：一类具有指数尾部的噪声和一类
具有多项式尾部。我们的结果的一个显着特点是它们没有
需要域直径的上限。最后，我们支持我们的
理论与说明性实验比较平均值的行为
重尾噪声区域中的迭代的迭代与最后迭代的迭代的迭代的迭代的迭代的迭代的迭代。
]]></description>
      <guid>http://arxiv.org/abs/2312.07142</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>具有在线神经回归的上下文强盗。 （arXiv：2312.07145v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07145</link>
      <description><![CDATA[最近的工作表明，从上下文强盗到在线强盗的减少
可实现性假设下的回归 [Foster 和 Rakhlin，2020，Foster
和克里希那穆蒂，2021]。在这项工作中，我们研究了神经网络的使用
用于此类在线回归和相关神经上下文强盗的网络
（NeuCB）。使用广泛网络的现有结果，人们可以轻松地展示
${\mathcal{O}}(\sqrt{T})$对带有平方损失的在线回归感到遗憾，其中
通过减少意味着 ${\mathcal{O}}(\sqrt{K} T^{3/4})$ 遗憾
NeuCB。与这种标准方法不同，我们首先展示
$\mathcal{O}(\log T)$对几乎凸损失的在线回归感到遗憾
满足 QG（二次增长）条件，PL 的推广
（Polyak-\L ojasiewicz）条件，并且具有独特的最小值。虽然不是
直接适用于广泛的网络，因为它们没有独特的最小值，我们
表明向网络添加合适的小随机扰动
令人惊讶的是，预测使损失满足 QG 的独特最小值。基于
对于这样一个令人不安的预测，我们表现出 ${\mathcal{O}}(\log T)$ 的遗憾
具有平方损失和 KL 损失的在线回归，然后进行转换
这些分别为 $\tilde{\mathcal{O}}(\sqrt{KT})$ 和
$\tilde{\mathcal{O}}(\sqrt{KL^*} + K)$ 对 NeuCB 感到遗憾，其中 $L^*$ 是
失去最佳政策。另外，我们还表明现有的遗憾界限
对于 NeuCB 是 $\Omega(T)$ 或假设 i.i.d.与这项工作不同的上下文。
最后，我们在各种数据集上的实验结果表明，我们的
算法，尤其是基于 KL 损失的算法，持续优于
现有的算法。
]]></description>
      <guid>http://arxiv.org/abs/2312.07145</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>变压器可以代表卡尔曼滤波器吗？ （arXiv：2312.06937v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.06937</link>
      <description><![CDATA[Transformers 是一类自回归深度学习架构，
最近在各种视觉方面都取得了最先进的表现，
语言和机器人任务。我们再回顾一下卡尔曼滤波的问题
线性动力系统并表明 Transformers 可以逼近卡尔曼
进行强烈的过滤。具体来说，对于任何可观测的 LTI 系统，我们
构造一个显式因果屏蔽 Transformer 来实现卡尔曼
过滤，直到一个小的加性误差，该误差在时间上均匀有界；我们
将我们的构造称为变压器滤波器。我们的建设是基于
两步减少。我们首先证明 softmax 自注意力块可以
精确地表示某个高斯核平滑估计器。然后我们展示
该估计器非常接近卡尔曼滤波器。我们也调查
如何使用变压器滤波器进行测量反馈控制以及
证明所得的非线性控制器非常接近
标准最优控制策略（例如 LQG 控制器）的性能。
]]></description>
      <guid>http://arxiv.org/abs/2312.06937</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>Ahpatron：一种新的预算在线内核学习机，具有更严格的错误约束。 （arXiv：2312.07032v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.07032</link>
      <description><![CDATA[在本文中，我们研究了在线内核学习的错误界
预算。我们提出了一种新的预算在线内核学习模型，称为
Ahpatron，显着改善了之前工作的错误界限
解决了 Dekel、Shalev-Shwartz 和 Singer (2005) 提出的开放问题。我们
首先提出一种感知器的激进变体，名为 AVP，这是一个没有
预算，使用主动更新规则。然后我们设计一个新的预算
维护机制，删除了一半的例子，并预测了
将示例删除到由剩余示例跨越的假设空间上。
Ahpatron采用上述机制来近似AVP。理论分析
证明 Ahpatron 具有更严格的错误界限，实验结果表明
Ahpatron 在相同或一个方面优于最先进的算法
较小的预算。
]]></description>
      <guid>http://arxiv.org/abs/2312.07032</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>重置已修复的损坏的 ELBO。 （arXiv：2312.06828v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.06828</link>
      <description><![CDATA[变分自动编码器（VAE）是一类生成概率
为基于已知数据进行推理而设计的潜变量模型。他们平衡
重建和正则项。变分近似产生
证据下限（ELBO）。将正则化项乘以 beta 可得到
beta-VAE/ELBO，改善潜在空间的解缠结。然而，任何
与 1 不同的 beta 值违反了条件概率定律。
为了提供类似的参数化 VAE，我们开发了 Renyi（相对于 Shannon）
熵 VAE 和变分近似 RELBO 引入了类似的
范围。 Renyi VAE 有一个额外的类似 Renyi 正则化项，其中
未学习到的条件分布。该术语的评估本质上是
使用奇异值分解方法进行分析。
]]></description>
      <guid>http://arxiv.org/abs/2312.06828</guid>
      <pubDate>Wed, 13 Dec 2023 03:14:26 GMT</pubDate>
    </item>
    </channel>
</rss>