<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 08 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>$L^1$ 估计：关于线性估计量的最优性</title>
      <link>https://arxiv.org/abs/2309.09129</link>
      <description><![CDATA[arXiv:2309.09129v4 公告类型：replace-cross 
摘要：考虑在 $L^1$ 保真度标准下，从噪声观测 $Y = X+ Z$ 中估计随机变量 $X$ 的问题，其中 $Z$ 是标准正态分布。众所周知，在这种情况下，最佳贝叶斯估计量是条件中位数。这项工作表明，唯一导致条件中位数线性的 $X$ 先验分布是高斯分布。
在此过程中，还提出了其他几个结果。特别是，证明了如果条件分布 $P_{X|Y=y}$ 对所有 $y$ 都是对称的，那么 $X$ 必须遵循高斯分布。此外，我们考虑了其他 $L^p$ 损失并观察到以下现象：对于 $p \in [1,2]$，高斯是唯一能产生线性最佳贝叶斯估计量的先验分布，而对于 $p \in (2,\infty)$，$X$ 上的无穷多个先验分布可以产生线性。最后，我们提供了扩展以涵盖导致某些指数族条件分布的噪声模型。]]></description>
      <guid>https://arxiv.org/abs/2309.09129</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>约束块黎曼优化的块主要化-最小化的收敛性和复杂性</title>
      <link>https://arxiv.org/abs/2312.10330</link>
      <description><![CDATA[arXiv:2312.10330v2 公告类型：replace-cross 
摘要：块主化-最小化 (BMM) 是一种简单的非凸优化迭代算法，它依次最小化每个块坐标中目标函数的主化代理，同时其他块坐标保持不变。我们考虑一组用于最小化平滑非凸目标的 BMM 算法，其中每个参数块都限制在黎曼流形的子集内。我们确定该算法渐近收敛到驻点集，并在 $\widetilde{O}(\epsilon^{-2})$ 次迭代内达到 $\epsilon$ 驻点。特别是，当底层流形是欧几里得或 Stiefel 流形的乘积时，我们对复杂性结果的假设完全是欧几里得的，尽管我们的分析明确使用了黎曼几何。我们的一般分析适用于具有黎曼约束的各种算法：黎曼 MM、块投影梯度下降、乐观似然估计、测地线约束子空间跟踪、稳健 PCA 和黎曼 CP 字典学习。我们通过实验验证了我们的算法比应用于黎曼设置的标准欧几里得算法收敛速度更快。]]></description>
      <guid>https://arxiv.org/abs/2312.10330</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络的格兰杰因果关系</title>
      <link>https://arxiv.org/abs/2208.03703</link>
      <description><![CDATA[arXiv:2208.03703v2 公告类型：替换 
摘要：网络中节点之间的依赖关系是一个重要概念，它渗透到许多领域，包括金融、政治、社会学、基因组学和脑科学。表征多元时间序列数据成分之间依赖关系的一种方法是通过 Granger 因果关系 (GC)。对 GC 估计/推理的标准传统方法通常假设线性动力学，然而这种简化并不适用于信号本质上是非线性的许多实际应用。在这种情况下，强加线性模型（例如向量自回归 (VAR) 模型）可能会导致对真正的 Granger 因果关系的错误表征。为了克服这一限制，Tank 等人（IEEE 模式分析与机器学习交易，2022 年）提出了一种使用具有稀疏正则化惩罚的神经网络的解决方案。正则化鼓励可学习的权重稀疏，从而实现对 GC 的推理。本文利用机器学习和深度学习的进步克服了当前方法的局限性，这些进步已被证明可以学习数据中的隐藏模式。我们提出了新型模型，可以以计算高效的方式处理底层非线性，同时提供 GC 和滞后阶数选择。首先，我们提出了学习核 VAR (LeKVAR) 模型，该模型学习由共享神经网络参数化的核，然后对可学习权重进行惩罚以发现 GC 结构。其次，我们表明可以通过解耦惩罚直接解耦滞后和单个时间序列重要性。这很重要，因为我们想在 GC 估计过程中选择滞后阶数。这种解耦充当过滤作用，可以扩展到任何 DL 模型，包括多层感知器 (MLP)、循环神经网络 (RNN)、长短期记忆网络 (LSTM)、Transformers 等，用于同时进行 GC 估计和滞后选择。]]></description>
      <guid>https://arxiv.org/abs/2208.03703</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的算法集体行动</title>
      <link>https://arxiv.org/abs/2302.04262</link>
      <description><![CDATA[arXiv:2302.04262v3 公告类型：replace-cross 
摘要：我们在部署机器学习算法的数字平台上发起了一项算法集体行动的原则性研究。我们提出了一个集体与公司学习算法互动的简单理论模型。集体汇集参与个人的数据，并通过指导参与者如何修改自己的数据来实现集体目标来执行算法策略。我们在三个基本的学习理论环境中研究了该模型的后果：非参数最优学习算法、参数风险最小化器和基于梯度的优化。在每种情况下，我们都会提出协调的算法策略，并将自然成功标准描述为集体规模的函数。作为我们理论的补充，我们对一项技能分类任务进行了系统实验，该任务涉及来自自由职业者零工平台的数万份简历。通过对类似 BERT 的语言模型进行两千多次模型训练，我们看到我们的经验观察与我们的理论做出的预测之间存在惊人的对应关系。总的来说，我们的理论和实验广泛支持这样的结论：极小分数规模的算法集体可以对平台的学习算法施加显著的控制。]]></description>
      <guid>https://arxiv.org/abs/2302.04262</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>使用隐式先验和扩散后验采样的灵活贝叶斯最后一层模型</title>
      <link>https://arxiv.org/abs/2408.03746</link>
      <description><![CDATA[arXiv:2408.03746v1 公告类型：交叉 
摘要：贝叶斯最后一层 (BLL) 模型仅关注神经网络输出层的不确定性，其性能可与更复杂的贝叶斯模型相媲美。然而，在贝叶斯最后一层 (BLL) 模型中，最后一层权重使用高斯先验限制了它们在面对非高斯、异常值丰富或高维数据集时的表达能力。为了解决这一不足，我们引入了一种新方法，该方法结合了扩散技术和隐式先验，用于贝叶斯最后一层权重的变分学习。该方法利用隐式分布来对 BLL 中的权重先验进行建模，并结合扩散采样器来近似真实的后验预测，从而建立了全面的贝叶斯先验和后验估计策略。通过提供明确且计算高效的变分下限，我们的方法旨在增强 BLL 模型的表达能力，提高模型准确性、校准和分布外检测能力。通过详细的探索和实验验证，我们展示了该方法在确保计算效率的同时提高预测准确性和不确定性量化的潜力。]]></description>
      <guid>https://arxiv.org/abs/2408.03746</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:03 GMT</pubDate>
    </item>
    <item>
      <title>Nadaraya-Watson 核平滑作为随机能量模型</title>
      <link>https://arxiv.org/abs/2408.03769</link>
      <description><![CDATA[arXiv:2408.03769v1 公告类型：交叉 
摘要：我们利用 Nadaraya-Watson 核平滑估计量与随机能量模型和密集联想记忆的关系，研究其在高维中的行为。]]></description>
      <guid>https://arxiv.org/abs/2408.03769</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:03 GMT</pubDate>
    </item>
    <item>
      <title>使用最佳预后元模型进行敏感性分析</title>
      <link>https://arxiv.org/abs/2408.03590</link>
      <description><![CDATA[arXiv:2408.03590v1 公告类型：交叉 
摘要：在虚拟原型制作过程中的实际应用中，并不总是能够降低物理模型的复杂性并获得可以快速解决的数值模型。通常，每个数值模拟都需要数小时甚至数天。尽管数值方法和高性能计算取得了进展，但在这种情况下，不可能探索各种模型配置，因此需要有效的替代模型。通常，可用的元模型技术根据所研究的问题显示出几个优点和缺点。在本文中，我们提出了一种自动方法，用于为实际问题选择最佳合适的元模型。结合使用高级过滤技术自动减少变量空间，还可以对高维问题进行有效近似。这种过滤技术可以将高维变量空间缩小到更小的子空间，在该子空间中进行基于元模型的灵敏度分析以评估重要变量的影响，并确定具有相应替代模型的最佳子空间，从而实现最准确的概率分析。为此，我们研究了基于方差和无矩灵敏度测量以及移动最小二乘法和克里金法等高级元模型。]]></description>
      <guid>https://arxiv.org/abs/2408.03590</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:02 GMT</pubDate>
    </item>
    <item>
      <title>关于随机特征图中不可训练内部权重的选择</title>
      <link>https://arxiv.org/abs/2408.03626</link>
      <description><![CDATA[arXiv:2408.03626v1 公告类型：交叉 
摘要：计算成本低廉的随机特征图机器学习架构可以看作是一个单层前馈网络，其中隐藏层的权重是随机但固定的，并且只有外部权重是通过线性回归学习的。内部权重通常从规定的分布中选择。内部权重的选择会显著影响随机特征图的准确性。我们在这里讨论如何最好地选择内部权重的任务。特别是，我们考虑预测问题，其中随机特征图用于学习动态系统的一步传播器图。我们提供了一种计算成本低廉的命中和运行算法来选择良好的内部权重，从而获得良好的预测技巧。我们表明，良好特征的数量是控制随机特征图预测技巧的主要因素，并充当有效的特征维度。最后，我们将随机特征图与单层前馈神经网络进行比较，在单层前馈神经网络中，现在使用梯度下降来学习内部权重。我们发现随机特征图具有卓越的预测能力，同时计算成本降低了几个数量级。]]></description>
      <guid>https://arxiv.org/abs/2408.03626</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:02 GMT</pubDate>
    </item>
    <item>
      <title>NeurAM：通过神经活性流形进行非线性降维，实现不确定性量化</title>
      <link>https://arxiv.org/abs/2408.03534</link>
      <description><![CDATA[arXiv:2408.03534v1 公告类型：交叉 
摘要：我们提出了一种非线性降维的新方法，专门用于计算成本高昂的数学模型。我们利用自动编码器来发现一个一维神经活性流形 (NeurAM)，捕捉模型输出的可变性，以及一个同时学习的代理模型，该模型的输入在此流形上。然后可以应用所提出的降维框架来执行外循环多查询任务，如敏感性分析和不确定性传播。特别是，我们在理想条件下从理论上证明了 NeurAM 如何通过在发现的低维和模型间共享流形上对模型进行采样来获得具有降低方差的多保真采样估计量。几个数值例子说明了所提出的降维策略的主要特点，并强调了它相对于文献中现有方法的优势。]]></description>
      <guid>https://arxiv.org/abs/2408.03534</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:01 GMT</pubDate>
    </item>
    <item>
      <title>In2Core：利用影响函数在大型语言模型的指令微调中进行核心集选择</title>
      <link>https://arxiv.org/abs/2408.03560</link>
      <description><![CDATA[arXiv:2408.03560v1 公告类型：交叉 
摘要：尽管取得了进展，但由于模型泛化所需的大量参数和大量数据，对大型语言模型 (LLM) 进行微调的成本仍然很高。计算资源的可访问性仍然是开源社区的障碍。为了应对这一挑战，我们提出了 In2Core 算法，该算法通过分析训练和评估样本与训练模型之间的相关性来选择核心集。值得注意的是，我们评估模型的内部梯度来估计这种关系，旨在对每个训练点的贡献进行排序。为了提高效率，我们提出了一种优化方法，以减少层数来计算影响函数，同时实现相似的精度。通过将我们的算法应用于 LLM 的指令微调数据，我们仅使用 50% 的训练数据就可以实现类似的性能。同时，使用影响函数分析模型对某些测试样本的覆盖率可以提供有关训练集对这些测试点的覆盖率的可靠且可解释的信号。]]></description>
      <guid>https://arxiv.org/abs/2408.03560</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:01 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归使小型 LLM 成为强大且可解释的“数十次”分类器</title>
      <link>https://arxiv.org/abs/2408.03414</link>
      <description><![CDATA[arXiv:2408.03414v1 公告类型：交叉 
摘要：对于简单的分类任务，我们表明用户可以从使用小型、本地、生成语言模型而不是大型商业模型的优势中受益，而无需牺牲性能或引入额外的标签成本。这些优势，包括隐私、可用性、成本和可解释性方面的优势，在商业应用和更广泛的人工智能民主化中都很重要。通过对 17 个句子分类任务（2-4 个类）的实验，我们表明，在“数十次”机制下，小型 LLM 嵌入的惩罚逻辑回归等于（并且通常更好）大型 LLM 的性能。这不需要比验证大型 LLM 性能所需的更多标记实例。最后，我们为分类决策提取稳定且合理的解释。]]></description>
      <guid>https://arxiv.org/abs/2408.03414</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:00 GMT</pubDate>
    </item>
    <item>
      <title>分类器的概率分数，校准是不够的</title>
      <link>https://arxiv.org/abs/2408.03421</link>
      <description><![CDATA[arXiv:2408.03421v1 公告类型：交叉 
摘要：在二元分类任务中，准确表示概率预测对于各种实际应用至关重要，例如预测付款违约或评估医疗风险。然后必须对模型进行良好的校准，以确保预测概率与实际结果一致。然而，当分数异质性偏离底层数据概率分布时，传统的校准指标就会失去可靠性，无法将分数分布与实际概率对齐。在本研究中，我们重点介绍了优先优化预测分数和真实概率分布之间的一致性的方法，而不是最小化传统的性能或校准指标。当使用基于树的模型（如随机森林和 XGBoost）时，我们的分析强调了这些模型在调整超参数以最小化预测和真实分布之间的 Kullback-Leibler (KL) 差异方面提供的灵活性。通过对 10 个 UCI 数据集和模拟进行广泛的实证分析，我们证明基于 KL 散度优化基于树的模型可以在不显著降低性能的情况下实现预测分数和实际概率之间的良好一致性。在实际场景中，参考概率是通过最大似然估计的 Beta 分布先验确定的。相反，最小化传统校准指标可能会导致次优结果，其特点是性能明显下降和 KL 值较低。我们的研究结果揭示了传统校准指标的局限性，这可能会削弱预测模型在关键决策中的可靠性。]]></description>
      <guid>https://arxiv.org/abs/2408.03421</guid>
      <pubDate>Fri, 09 Aug 2024 03:17:00 GMT</pubDate>
    </item>
    <item>
      <title>使用有理多项式混沌展开的贝叶斯优化对线性结构动力学模型进行最大后验估计</title>
      <link>https://arxiv.org/abs/2408.03569</link>
      <description><![CDATA[arXiv:2408.03569v1 公告类型：新
摘要：贝叶斯分析能够将先验知识与测量数据相结合以学习模型参数。通常，当仅对参数的点估计感兴趣时，人们会求助于计算最大后验 (MAP) 估计。我们将 MAP 估计应用于结构动态模型，其中系统响应可以用频率响应函数来描述。为了减轻重复昂贵的模型调用带来的高计算需求，我们利用有理多项式混沌展开 (RPCE) 代理模型，将系统频率响应表示为具有复系数的两个多项式的有理数。我们提出了一种基于拉普拉斯对分母系数后验分布的近似的现有 RPCE 稀疏贝叶斯学习方法的扩展。此外，我们引入了一种贝叶斯优化方法，该方法允许在 MAP 估计的整个优化过程中自适应地丰富实验设计。因此，我们利用预期改进获取函数作为一种手段来识别输入空间中可能与较大目标函数值相关的样本点。获取函数是通过蒙特卡罗抽样估计的，该抽样基于稀疏贝叶斯学习过程中确定的展开系数的后验分布。通过将稀疏诱导学习过程与顺序实验设计相结合，我们有效地减少了 MAP 估计问题中的模型评估次数。我们证明了所提出的方法在代数二自由度系统的参数更新问题和交叉层压木板的有限元模型上的适用性。]]></description>
      <guid>https://arxiv.org/abs/2408.03569</guid>
      <pubDate>Fri, 09 Aug 2024 03:16:59 GMT</pubDate>
    </item>
    <item>
      <title>来自二次方样本的广度神经网络的贝叶斯最优学习</title>
      <link>https://arxiv.org/abs/2408.03733</link>
      <description><![CDATA[arXiv:2408.03733v1 公告类型：新
摘要：我们考虑学习与单隐层神经网络相对应的目标函数的问题，该网络在第一层之后具有二次激活函数和随机权重。我们考虑输入维度和网络宽度成比例的渐近极限。最近的工作 [Cui &amp; al &#39;23] 确定，当可用样本的数量在维度上仅呈线性时，线性回归可提供贝叶斯最优测试误差来学习这样的函数。这项工作强调了在更有趣的情况下从理论上分析最优测试误差的开放挑战，其中样本数量在维度上是二次的。在本文中，我们解决了二次激活的这一挑战，并推导出贝叶斯最优测试误差的闭式表达式。我们还提供了一种称为 GAMP-RIE 的算法，它将近似消息传递与旋转不变矩阵去噪相结合，并渐近地实现了最佳性能。从技术上讲，我们的结果与最近关于广义秩矩阵最优去噪和椭圆拟合问题的研究建立了联系。我们进一步通过经验证明，在没有噪声的情况下，随机初始化的梯度下降似乎可以对权重空间进行采样，从而导致零训练损失，而对初始化进行平均会导致测试误差等于贝叶斯最优误差。]]></description>
      <guid>https://arxiv.org/abs/2408.03733</guid>
      <pubDate>Fri, 09 Aug 2024 03:16:59 GMT</pubDate>
    </item>
    <item>
      <title>平均网络何时能捕捉到网络样本的拓扑结构？</title>
      <link>https://arxiv.org/abs/2408.03461</link>
      <description><![CDATA[arXiv:2408.03461v1 公告类型：新
摘要：Fr\&#39;echet 均值（也称为“重心”）网络的概念是大多数机器学习算法的主力，这些算法需要估计“位置”参数来分析网络值数据。在这种情况下，网络重心继承训练数据集中网络的拓扑结构至关重要。度量 - 测量网络之间的接近度 - 控制重心的结构特性。这项工作意义重大，因为它首次为随机块模型提供了样本 Fr\&#39;echet 均值的分析估计，该模型处于随机网络严格概率分析的前沿。我们表明，使用汉明距离计算的均值网络无法捕捉训练样本中网络的拓扑结构，而使用有效电阻距离计算的均值网络可以恢复正确的分区和相关的边缘密度。从实用的角度来看，我们的工作为在使用样本 Fr\&#39;echet 均值网络来表征网络价值机器学习的网络拓扑结构的背景下的指标选择提供了参考]]></description>
      <guid>https://arxiv.org/abs/2408.03461</guid>
      <pubDate>Fri, 09 Aug 2024 03:16:58 GMT</pubDate>
    </item>
    </channel>
</rss>