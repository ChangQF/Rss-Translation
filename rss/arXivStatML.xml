<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Mon, 18 Dec 2023 03:14:35 GMT</lastBuildDate>
    <item>
      <title>学习具有自由形式流的流形分布。 （arXiv：2312.09852v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09852</link>
      <description><![CDATA[许多现实世界的数据，特别是自然科学和计算机领域的数据
视觉，位于已知的黎曼流形上，例如球面、环面或群
旋转矩阵。学习分布的主要方法
这样的流形需要求解微分方程才能从中采样
模型并评估密度。由此产生的采样时间变慢
通过大量的功能评估。在这项工作中，我们提出了一个
仅需要单个功能评估的替代方法
通过流形的投影。训练是通过适应
最近提出了黎曼流形的自由形式流框架。中央
想法是通过轨迹估计负对数似然的梯度
在切线空间中求值。我们在各种流形上评估我们的方法，
并发现与竞争性能相比，推理速度明显加快
之前的工作。我们在 https://github.com/vislearn/FFF 公开我们的代码。
]]></description>
      <guid>http://arxiv.org/abs/2312.09852</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:34 GMT</pubDate>
    </item>
    <item>
      <title>时间序列分类的深度无监督域适应：基准。 （arXiv：2312.09857v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09857</link>
      <description><![CDATA[无监督域适应（UDA）旨在利用标记的源数据
为未标记的目标数据训练模型。尽管在各个领域进行了广泛的研究
与计算机视觉和自然语言处理一样，UDA 仍未得到充分探索
对于时间序列数据，它在现实世界中具有广泛的应用，包括
医学和制造业到地球观测和人类活动识别。
我们的论文通过引入一个全面的基准来解决这一差距
评估用于时间序列分类的 UDA 技术，重点关注深度
学习方法。我们提供了七个新的基准数据集，涵盖各种
领域转移和时间动态，促进公平和标准化的 UDA
使用最先进的神经网络骨干进行方法评估（例如
Inception）用于时间序列数据。该基准提供了对
评估方法的优点和局限性，同时保留
领域适应的无监督性质，使其直接适用于
实际问题。我们的论文是研究人员和研究人员的重要资源
从业者，推进时间序列数据的领域适应解决方案和
促进这一关键领域的创新。这个的实现代码
基准测试可在 https://github.com/EricssonResearch/UDA-4-TSC 上获取。
]]></description>
      <guid>http://arxiv.org/abs/2312.09857</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:34 GMT</pubDate>
    </item>
    <item>
      <title>专家混合的分布式学习。 （arXiv：2312.09877v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09877</link>
      <description><![CDATA[在现代机器学习问题中，我们处理的数据集是
按自然分布或潜在较大分布
计算通常是一种标准的处理方式，因为集中式算法
总体上是无效的。我们提出了一种分布式学习方法
专家混合（MoE）模型与聚合策略来构建
并行拟合分布式局部估计器的减少估计器
数据的子集。聚合基于最优最小化
由当地组成的大型教育部之间的预期交通差异
估计器和未知的期望 MoE 模型。我们证明所提供的
一旦局部估计量被减少，减少估计量就保持一致
聚合是一致的，并且其构造是由提议的
计算有效的主化最小化 (MM) 算法。我们
研究拟议减少的统计和数值特性
实验上的估计器证明其性能与
从完整数据集中以集中方式构建的全局估计器。
对于某些情况，计算时间要快十倍以上，对于
可比较的性能。我们的源代码在 Github 上公开可用。
]]></description>
      <guid>http://arxiv.org/abs/2312.09877</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:34 GMT</pubDate>
    </item>
    <item>
      <title>包括稳定 RNN 在内的动态系统的 PAC-Bayes 泛化界限。 （arXiv：2312.09793v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09793</link>
      <description><![CDATA[在本文中，我们推导了泛化差距上的 PAC-Bayes 约束，其中
一类特殊离散时间非线性的监督时间序列设置
动力系统。此类包括稳定的循环神经网络（RNN），
这项工作的动机是将其应用于 RNN。为了
为了达到结果，我们施加一些稳定性约束，在允许的情况下
楷模。在这里，稳定性是从动力系统的意义上理解的。为了
RNN，这些稳定性条件可以用以下条件来表示：
重量。我们假设所涉及的过程本质上是有界的，并且损失
函数是利普希茨函数。泛化差距的拟议界限取决于
关于数据分布的混合系数和本质上界
的数据。此外，随着数据集大小的变化，界限收敛到零
增加。在本文中，我们 1）将学习问题形式化，2）推导
此类系统的 PAC-贝叶斯误差界限，3) 讨论
这个错误界限，以及 4) 显示了一个说明性示例，并讨论了
计算建议的界限。与其他可用边界不同，派生边界
对于非独立同分布成立数据（时间序列）并且它不会随着数据数量的增加而增长
RNN 的步骤。
]]></description>
      <guid>http://arxiv.org/abs/2312.09793</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:33 GMT</pubDate>
    </item>
    <item>
      <title>在预测空间中使用贝叶斯推理校准一轮联邦学习。 （arXiv：2312.09817v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09817</link>
      <description><![CDATA[联邦学习 (FL) 涉及在分布式数据集上训练模型
在客户端之间，每个客户端的数据集都是本地化的，并且
可能是异质的。在 FL 中，小而嘈杂的数据集很常见，
强调需要经过良好校准的模型来代表不确定性
的预测。最接近实现这些目标的 FL 技术是
贝叶斯 FL 方法从局部后验中收集参数样本，以及
将它们聚合起来以近似全局后验。为了提高可扩展性
对于较大的模型，一种常见的贝叶斯方法是近似全局
通过乘以局部预测后验来预测后验。在这项工作中，
我们证明这种方法可以系统地给出过度自信的预测，
我们通过提出 $\beta$-Predictive Bayes（贝叶斯 FL）来解决这个问题
在预测的混合和乘积之间进行插值的算法
后验，使用可调参数 $\beta$。该参数调整为
在将全局集合蒸馏为单个集合之前，改进其校准
模型。我们的方法在各种回归和分类上进行评估
数据集来证明其在校准方面优于其他基线，甚至
随着数据异构性的增加。代码可在
https://github.com/hasanmohsin/betaPredBayes_FL
]]></description>
      <guid>http://arxiv.org/abs/2312.09817</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:33 GMT</pubDate>
    </item>
    <item>
      <title>向量化字符串条目以进行表上的数据处理：什么时候更大的语言模型更好？ （arXiv：2312.09634v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.09634</link>
      <description><![CDATA[数据处理管道的效率越来越高
数字向量，例如大多数机器学习模型，或向量
用于快速相似性搜索的数据库。这些需要将数据转换为
数字。虽然这种转换对于简单的数值和分类来说很容易
条目，数据库与文本条目发生冲突，例如名称或描述。
在大语言模型时代，向量化的最佳策略是什么
表条目，请记住较大的模型需要更多的操作
复杂？我们研究了语言模型在 14 项分析任务中的优势
表同时改变训练大小，以及模糊连接基准。
我们介绍了一个列的简单特征，它揭示了两个设置：
1) 肮脏的类别设置，其中字符串之间有很多相似之处
条目，反之2) 多样化的条目设置。对于脏类别，
与更简单的语言模型相比，预训练的语言模型几乎没有带来任何好处
字符串模型。对于不同的条目，我们表明更大的语言模型可以改善
数据处理。对于这些，我们研究复杂性与性能的权衡
并表明它们反映了经典文本嵌入的特征：较大的模型往往
以获得更好的性能，但为了嵌入目的而对它们进行微调是有用的。
]]></description>
      <guid>http://arxiv.org/abs/2312.09634</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:32 GMT</pubDate>
    </item>
    <item>
      <title>复制对称破缺下的密集联想记忆的无监督和监督学习。 (arXiv:2312.09638v1 [cond-mat.dis-nn])</title>
      <link>http://arxiv.org/abs/2312.09638</link>
      <description><![CDATA[自旋玻璃的统计力学是实现这一目标的主线之一
理解神经网络和学习的信息处理
机器。解决这种方法，在相当标准的副本对称
描述级别，最近具有多节点的 Hebbian 吸引子网络
交互作用（通常称为密集联想记忆）已被证明可以
在许多任务中都优于经典的配对同行，从
他们抵御对抗性攻击的稳健性以及与其他人合作的能力
其超线性存储容量的信号极其微弱。聚焦
在本文中，我们更多地关注数学技术而不是计算方面
放宽副本对称假设，我们推导出一步
监督和无监督学习的副本对称性破缺图
这些密集联想记忆的协议：空间中的相图
控制参数是通过 Parisi 独立实现的
当时的复制品技巧以及格拉望远镜中的等级制度
在损坏的复制品插值内。进一步，明确的分析
提供调查以加深大数据和基态限制
这些网络以及复制对称性破缺不会发生的证明
改变学习阈值并稍微增加最大存储量
容量。最后是 De Almeida 和 Thouless 线，描绘了
复制品对称描述的不稳定性，也是通过分析得出的
强调如何跨越这个边界，损坏的副本描述应该
被优先考虑。
]]></description>
      <guid>http://arxiv.org/abs/2312.09638</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:32 GMT</pubDate>
    </item>
    <item>
      <title>强盗协作学习的最佳遗憾界限。 （arXiv：2312.09674v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09674</link>
      <description><![CDATA[我们考虑在一般协作多智能体中遗憾最小化
多臂老虎机模型，其中每个智能体面对一组有限的手臂，
可以通过中央控制器与其他代理进行通信。最佳手臂
对于该模型中的每个代理来说，都是具有最大预期混合奖励的手臂，
其中每个臂的混合奖励是其奖励的加权平均值
所有代理，因此代理之间的沟通至关重要。虽然接近最佳
最佳手臂识别的样本复杂性在此已知
协作模型中，最佳遗憾的问题仍然悬而未决。在这项工作中，
我们解决了这个问题并提出了第一个阶数最优的算法
在这种协作强盗模型下，遗憾是有限度的。此外，我们表明
仅需要少量恒定数量的预期通信轮次。
]]></description>
      <guid>http://arxiv.org/abs/2312.09674</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:32 GMT</pubDate>
    </item>
    <item>
      <title>一般状态空间模型的变分超额风险界限。 （arXiv：2312.09607v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2312.09607</link>
      <description><![CDATA[在本文中，我们考虑用于一般状态的变分自动编码器（VAE）
空间模型。我们考虑变分的向后因式分解
分布来分析与 VAE 相关的超额风险。这么落后
最近提出了分解来执行在线变分学习
并获得变分估计误差的上限。什么时候
在强烈混合下观察到序列的独立轨迹
对状态空间模型和变分分布的假设，我们
提供样本数量和显式的预言不等式
观察序列的长度。然后我们得出这个结果
理论结果。特别是，当数据分布由
状态空间模型，我们为 Kullback-Leibler 提供一个上限
数据分布与其估计量之间以及数据分布之间的分歧
变分后验和估计状态空间后验
分布。在经典假设下，我们证明我们的结果可以是
应用于用密集和循环神经网络构建的高斯后向内核
网络。
]]></description>
      <guid>http://arxiv.org/abs/2312.09607</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:31 GMT</pubDate>
    </item>
    <item>
      <title>重新思考图神经网络中的因果关系学习。 （arXiv：2312.09613v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09613</link>
      <description><![CDATA[图神经网络（GNN）通过有效地证明了它们的重要性
对图结构数据中复杂的相互关系进行建模。加强
GNN 的可信度和鲁棒性，变得异常重要
增强他们捕捉因果关系的能力。然而，尽管最近
确实通过因果学习增强了 GNN 的进步
针对因果关系进行深入分析
GNN 的建模能力仍然是一个未解决的问题。为了
从因果学习的角度全面分析各种GNN模型，
我们构建了一个人工合成的数据集，具有已知且可控的
数据和标签之间的因果关系。生成的合理性
通过理论基础进一步保证数据。从中汲取见解
使用我们的数据集进行分析，我们引入了一个轻量级且高度
旨在增强 GNN 因果学习能力的适应性 GNN 模块
跨越各种不同的任务。通过一系列的实验
我们根据经验验证了合成数据集和其他现实世界的数据集
所提议模块的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2312.09613</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:31 GMT</pubDate>
    </item>
    <item>
      <title>流行病传播建模和预测：高斯过程回归方法。 （arXiv：2312.09384v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.09384</link>
      <description><![CDATA[流行病传播的建模和预测对于协助
缓解政策的制定。因此，我们提出了一种基于
高斯过程回归来建模和预测流行病，并量化
通过方差和高概率误差范围预测置信度。
高斯过程回归擅长使用小型数据集并提供
不确定性界限，这两个属性在建模和
用有限的数据预测流行病的传播过程。但是，那
使用高斯时仍然缺乏形式不确定性界限的推导
流行病背景下的过程回归，限制了其在以下方面的用处：
指导缓解工作。因此，在这项工作中，我们开发了一种新颖的界限
关于量化流行病影响的预测方差
我们做出的预测的数据。此外，我们开发了一个高概率错误
受预测约束，我们量化流行病的传播方式、感染情况
数据和预测范围的长度都会影响这个误差范围。我们
还表明，根据长度，误差保持在某个阈值以下
预测范围。为了说明这个框架，我们利用高斯
使用真实世界感染进行过程回归建模和预测 COVID-19
数据来自英国。
]]></description>
      <guid>http://arxiv.org/abs/2312.09384</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:30 GMT</pubDate>
    </item>
    <item>
      <title>组合复合体：弥合细胞复合体和超图之间的差距。 （arXiv：2312.09504v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09504</link>
      <description><![CDATA[基于图的信号处理技术已成为处理问题的关键
非欧几里得空间中的数据。然而，人们越来越认识到这些
图模型可能需要扩展到“高阶”域
有效地表示高维数据中发现的复杂关系。
这种高阶域通常被建模为超图，或
单纯形、立方形或其他细胞复合体。在这种情况下，细胞复合物
通常被视为具有附加代数结构的超图的子类
例如，可以利用它来发展谱理论。在这篇文章中，我们
提倡另类观点。我们认为超图和细胞
复合体强调 \emph{不同} 类型的关系，这可能有
根据应用程序上下文不同的实用程序。而超图是
有效建模实体、单元之间的集合类型、多体关系
复合体提供了一种有效的方法来建模层次结构，
内部到边界类型的关系。我们讨论的相对优势
这两个选择并详细阐述了之前介绍的a的概念
实现集合类型和层次结构共存的组合复合体
关系。最后，我们提供了一个简短的数值实验来证明
这种建模灵活性对于学习任务来说是有利的。
]]></description>
      <guid>http://arxiv.org/abs/2312.09504</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:30 GMT</pubDate>
    </item>
    <item>
      <title>使用回归神经网络的可靠预测区间。 （arXiv：2312.09606v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09606</link>
      <description><![CDATA[本文提出了传统回归神经网络的扩展
（NN）用预测替换它们产生的点预测
满足所需置信水平的区间。我们的方法遵循
新颖的机器学习框架，称为保形预测（CP），用于
在不做任何假设的情况下为预测分配可靠的置信度度量
不仅如此，数据是独立且同分布（i.i.d.）的。
我们在四个基准数据集和问题上评估所提出的方法
预测总电子含量（TEC），这是一个重要的参数
跨电离层链路；对于后者，我们使用超过 60000 TEC 的数据集
11 年期间收集的测量结果。我们的实验结果表明
我们的方法产生的预测区间都经过良好校准
并且足够紧密以在实践中有用。
]]></description>
      <guid>http://arxiv.org/abs/2312.09606</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:30 GMT</pubDate>
    </item>
    <item>
      <title>具有大量标签的多标签文本分类的经过良好校准的置信度测量。 （arXiv：2312.09304v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09304</link>
      <description><![CDATA[我们将之前的归纳保形预测 (ICP) 工作扩展到
多标签文本分类并提出了一种解决该问题的新方法
Label Powerset (LP) ICP 的计算效率低下，出现在
处理大量独特的标签。我们展示实验结果
使用原始的和提议的高效 LP-ICP 来处理两个英语和一个
捷克语数据集。具体来说，我们将 LP-ICP 应用于三个深度
人工神经网络 (ANN) 分类器有两种类型：一种基于
上下文化（bert）和两个非上下文化（word2vec）词嵌入。
在 LP-ICP 设置中，我们将不合格分数分配给标签集，其中
确定相应的 p 值和预测集。我们的方法
通过消除来处理 LP 增加的计算负担
考虑大量肯定具有 p 值的标签集
低于指定的显着性水平。这大大减少了
该方法的计算复杂性，同时充分尊重标准 CP
保证。我们的实验结果表明，基于情境的
分类器超越了基于非上下文的分类器并获得
所有检查的数据集均具有最先进的性能。良好的表现
底层分类器的 ICP 对应项无需
任何显着的精度损失，但具有 ICP 的额外优势，即
封装在预测集中的置信信息。我们实验性地
证明所得到的预测集可以足够严格
尽管所有可能的标签集包含更多内容，但实际上很有用
比 $1e+16$ 组合。此外，经验错误率
获得的预测集证实我们的输出经过良好校准。
]]></description>
      <guid>http://arxiv.org/abs/2312.09304</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>上下文强盗的分层最近邻方法。 （arXiv：2312.09332v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.09332</link>
      <description><![CDATA[在本文中，我们考虑度量中的对抗性上下文强盗问题
空间。论文“Nearest Neighbor with Bandit Feedback”解决了这个问题
但是当比较器的决策边界附近有很多上下文时
政策使其遭受高度遗憾。在本文中，我们消除了这个问题，
设计一种算法，在该算法中我们可以在以下情况下保留任何上下文集：
计算我们的遗憾项。我们的算法建立在“最近邻居
具有强盗反馈”，因此继承了其极高的计算效率。
]]></description>
      <guid>http://arxiv.org/abs/2312.09332</guid>
      <pubDate>Mon, 18 Dec 2023 03:14:29 GMT</pubDate>
    </item>
    </channel>
</rss>