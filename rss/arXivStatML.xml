<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 06 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>定向 Rockafellar-Uryasev 回归</title>
      <link>https://arxiv.org/abs/2411.02557</link>
      <description><![CDATA[arXiv:2411.02557v1 公告类型：新
摘要：大多数 ost 大数据数据集都存在选择偏差。例如，X（Twitter）训练观察与测试离线观察有很大不同，因为 Twitter 上的个人通常受教育程度更高、更民主或更左倾。因此，可靠估计的一个主要障碍是训练数据和测试数据之间的差异。即使存在不可忽略的选择机制，研究人员如何利用这些数据？已经为这个问题开发了许多方法，例如分布稳健优化 (DRO) 或学习公平性。减少偏见影响的一种可能途径是元信息。研究人员作为领域专家，可能事先了解影响其数据集的选择偏差的形式和程度，以及选择可能导致估计值发生变化的方向，例如高估或低估。同时，没有直接的方法在学习中利用这些类型的信息。我提出了一个损失函数，它考虑了研究人员提供的两种元数据信息：训练集中偏差的数量和方向（采样不足或过度）。然后通过神经网络（即定向 Rockafellar-Uryasev (dRU) 回归模型）使用所提出的损失函数进行估计。我在有偏差的训练数据集（大数据在线绘制的选举民意调查）上测试了 dRU 模型。我使用与从以前的研究中获得的政治和采样信息一致的元数据信息来应用所提出的模型。结果表明，与不包含元信息的模型相比，包含元信息的模型可以改善选举结果预测。]]></description>
      <guid>https://arxiv.org/abs/2411.02557</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的分类器链网络</title>
      <link>https://arxiv.org/abs/2411.02638</link>
      <description><![CDATA[arXiv:2411.02638v1 公告类型：新
摘要：分类器链是一种广泛用于分析多标记数据集的方法。在本研究中，我们介绍了分类器链的泛化：分类器链网络。分类器链网络可以联合估计模型参数，并可以考虑早期标签预测对链中后续分类器的影响。通过模拟，我们根据多种基准方法评估了分类器链网络的性能，即使在偏离其建模假设的场景中也表现出有竞争力的结果。此外，我们提出了一种检测标签之间条件依赖关系的新方法，并使用经验数据集说明了分类器链网络的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.02638</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有事件时间不确定性的点过程</title>
      <link>https://arxiv.org/abs/2411.02694</link>
      <description><![CDATA[arXiv:2411.02694v1 公告类型：新
摘要：点过程是广泛使用的统计模型，用于揭示相关事件数据中的时间模式。在许多应用中，事件时间无法准确观察到，因此需要将时间不确定性纳入点过程数据的建模中。在这项工作中，我们引入了一个框架来对网络上的时间不确定的点过程进行建模。我们首先在应用场景驱动的几个假设下，在连续时间设置中推导公式。在施加时间网格后，我们得到一个离散时间模型，该模型便于推理，可以使用基于批处理的随机梯度下降 (SGD) 通过梯度下降或变分不等式 (VI) 等一阶优化方法进行计算。使用 $k$ SGD 步骤，证明了 VI 推理的参数恢复保证，收敛速度为 $O(1/k)$。我们的框架通过将推理内核建模为矩阵（或网络上的张量）来处理非平稳过程，并将平稳过程（例如经典霍克斯过程）作为特例。我们通过实验表明，所提出的方法在模拟和真实数据上的表现优于以前的一般线性模型 (GLM) 基线，并揭示了败血症相关错乱数据集上有意义的因果关系。]]></description>
      <guid>https://arxiv.org/abs/2411.02694</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>椭圆 Wishart 分布：信息几何、最大似然估计、性能分析和统计学习</title>
      <link>https://arxiv.org/abs/2411.02726</link>
      <description><![CDATA[arXiv:2411.02726v1 公告类型：新
摘要：本文在信号处理和机器学习的背景下讨论了椭圆 Wishart 分布（它是 Wishart 分布的推广）。提出了两种计算最大似然估计量 (MLE) 的算法：基于椭圆 Wishart 分布的导出信息几何的固定点算法和黎曼优化方法。描述了 MLE 的存在性和唯一性以及两种估计算法的收敛性。还研究了 MLE 的统计特性，例如一致性、渐近正态性和 Fisher 效率的内在版本。在统计学习方面，设计了新颖的分类和聚类方法。对于 $t$-Wishart 分布，在模拟和真实 EEG 和高光谱数据上评估了 MLE 和统计学习算法的性能，展示了我们提出的方法的趣味性。]]></description>
      <guid>https://arxiv.org/abs/2411.02726</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>循环神经网络的泛化和风险界限</title>
      <link>https://arxiv.org/abs/2411.02784</link>
      <description><![CDATA[arXiv:2411.02784v1 公告类型：新
摘要：循环神经网络（RNN）在序列数据预测方面取得了巨大成功。然而，由于其复杂的互连结构，它们的理论研究仍然落后。在本文中，我们为 vanilla RNN 建立了一个新的泛化误差界限，并提供了一个统一的框架来计算可应用于各种损失函数的 Rademacher 复杂度。当使用斜坡损失时，我们表明我们的界限比基于对权重矩阵的 Frobenius 和谱范数的相同假设以及一些温和条件的现有界限更紧。我们的数值结果表明，我们的新泛化界限是三个公共数据集中所有现有界限中最紧的。当使用 $\tanh$ 和 ReLU 激活函数时，我们的界限分别比第二紧的界限平均提高了 13.80% 和 3.01%。此外，当损失函数满足伯恩斯坦条件时，我们推导出在多类分类问题中通过经验风险最小化 (ERM) 获得的基于 RNN 的估计量的尖锐估计误差界限。]]></description>
      <guid>https://arxiv.org/abs/2411.02784</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梯度下降法发现过度参数化的神经网络，具有非参数回归的敏锐泛化能力：无分布分析</title>
      <link>https://arxiv.org/abs/2411.02904</link>
      <description><![CDATA[arXiv:2411.02904v1 公告类型：新
摘要：本文研究了通过梯度下降 (GD) 训练的过度参数化的两层神经网络的非参数回归。我们表明，如果神经网络由带早期停止的 GD 训练，则训练后的网络的非参数回归风险率会急剧上升至 $\cO(\eps_n^2)$，这与带早期停止的 GD 训练的经典核回归的风险率相同，其中 $\eps_n$ 是与网络相关的神经切线核 (NTK) 的临界人口率，$n$ 是训练数据的大小。值得注意的是，我们的结果不需要对训练数据进行分布假设，这与许多依赖特定分布（例如球形均匀数据分布或满足某些限制条件的分布）的现有结果形成了鲜明对比。已知速率 $\cO(\eps_n^2)$ 在特定情况下是极小极大最优的，例如 NTK 具有多项式特征值衰减率的情况，这种情况在某些分布假设下会发生。我们的结果正式填补了训练经典核回归模型和通过 GD 训练过度参数化但宽度有限的神经网络之间的空白，用于非参数回归，而无需分布假设。我们还对某些未解决的问题提供了肯定的答案，或解决了文献中关于通过 GD 训练过度参数化的神经网络并提前停止非参数回归的特定问题，包括停止时间的表征、网络宽度的下限以及 GD 中使用的恒定学习率。]]></description>
      <guid>https://arxiv.org/abs/2411.02904</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>原生关联变分自编码器实现多视图插补</title>
      <link>https://arxiv.org/abs/2411.03097</link>
      <description><![CDATA[arXiv:2411.03097v1 公告类型：新
摘要：来自同一来源的多视图数据通常表现出相关性。这反映在每个数据视图上训练的单独变分自动编码器 (VAE) 的潜在空间之间的相关性中。提出了一种多视图 VAE 方法，该方法结合了 VAE 潜在空间之间的联合先验和非零相关结构。通过强制执行这种相关结构，可以发现更强相关的潜在空间。使用条件分布在这些潜在空间之间移动，可以估算缺失的视图并用于下游分析。学习这种相关结构涉及保持先验分布的有效性，以及允许端到端学习的成功参数化。]]></description>
      <guid>https://arxiv.org/abs/2411.03097</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在线数据收集以实现高效半参数推断</title>
      <link>https://arxiv.org/abs/2411.03195</link>
      <description><![CDATA[arXiv:2411.03195v1 公告类型：新
摘要：虽然许多作品都研究了统计数据融合，但它们通常假设各种数据集是预先给出的。然而，在实践中，估计需要做出艰难的数据收集决策，例如确定可用的数据源、它们的成本以及从每个源收集多少样本。此外，这个过程通常是连续的，因为在给定时间收集的数据可以改善未来的收集决策。在我们的设置中，给定对多个数据源的访问和预算约束，代理必须按顺序决定查询哪个数据源以有效地估计目标参数。我们使用在线矩选择来形式化这个任务，这是一个半参数框架，适用于由一组矩条件确定的任何参数。有趣的是，最佳预算分配取决于（未知的）真实参数。我们提出了两种在线数据收集策略，探索然后提交和探索然后贪婪，它们使用给定时间的参数估计来最佳地分配未来步骤中的剩余预算。我们证明，相对于预言机策略，这两种策略都实现了零遗憾（通过渐近 MSE 评估）。我们在合成和现实世界的因果效应估计任务上对我们的方法进行了实证验证，表明在线数据收集策略的表现优于固定策略。]]></description>
      <guid>https://arxiv.org/abs/2411.03195</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>减轻法学硕士监督微调和偏好学习中的遗忘</title>
      <link>https://arxiv.org/abs/2410.15483</link>
      <description><![CDATA[arXiv:2410.15483v2 公告类型：交叉 
摘要：预训练 LLM 的后训练通常由监督微调 (SFT) 阶段和偏好学习 (RLHF 或 DPO) 阶段组成，对于有效和安全的 LLM 应用至关重要。在流行的开源 LLM 的后训练中，广泛采用的方法是按顺序执行 SFT 和 RLHF/DPO。然而，顺序训练在 SFT 和 RLHF/DPO 权衡方面并不是最优的：LLM 在进行第二阶段的训练时逐渐忘记第一阶段的训练。我们从理论上证明了顺序后训练的次优性。此外，我们提出了一个实用的联合后训练框架，具有理论收敛保证，并且在计算成本相似的情况下，在经验上优于顺序后训练框架。我们的代码可在 https://github.com/heshandevaka/XRIGHT 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.15483</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SLED：自 Logits 进化解码，用于提高大型语言模型中的事实性</title>
      <link>https://arxiv.org/abs/2411.02433</link>
      <description><![CDATA[arXiv:2411.02433v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 已经展示了卓越的能力，但它们的输出有时可能不可靠或事实不正确。为了解决这个问题，我们引入了自 Logits 进化解码 (SLED)，这是一种新颖的解码框架，它可以增强 LLM 的真实性，而无需依赖外部知识库或需要进一步微调。从优化的角度来看，我们的 SLED 框架通过将最后一层的输出 logit 与早期层的输出 logit 进行对比，利用嵌入在 LLM 中的潜在知识。然后，它利用近似梯度方法使潜在知识能够指导输出的自我改进，从而有效提高事实准确性。已经在各种模型系列（LLaMA 2、LLaMA 3、Gemma）和规模（从 2B 到 70B）的既定基准上进行了广泛的实验，包括更高级的架构配置，例如专家混合 (MoE)。我们的评估涵盖了多种任务，包括多项选择、开放生成和对思维链推理任务的适应性。结果表明，与现有解码方法相比，SLED 可以持续提高高达 20% 的事实准确性，同时保持自然语言流畅性和可忽略不计的延迟开销。此外，它可以灵活地与其他解码方法结合使用，以进一步提高其性能。]]></description>
      <guid>https://arxiv.org/abs/2411.02433</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MADOD：通过 G-Invariance 元学习将 OOD 检测推广到未知域</title>
      <link>https://arxiv.org/abs/2411.02444</link>
      <description><![CDATA[arXiv:2411.02444v1 公告类型：交叉 
摘要：现实世界的机器学习应用经常面临同时发生的协变量和语义转变，这对传统的领域泛化和分布外 (OOD) 检测方法提出了挑战。我们引入了跨域元学习分布外检测 (MADOD)，这是一个旨在同时解决这两种转变的新框架。MADOD 利用元学习和 G 不变性来增强模型的泛化能力和在看不见的领域中的 OOD 检测。我们的关键创新在于任务构建：我们在每个元学习任务中随机将分布内类别指定为伪 OOD，使用现有数据模拟 OOD 场景。这种方法与基于能量的正则化相结合，能够学习稳健的领域不变特征，同时校准决策边界以进行有效的 OOD 检测。MADOD 在与测试领域无关的环境中运行，消除了推理过程中进行调整的需要，使其适用于测试数据不可用的场景。在现实世界和合成数据集上进行的大量实验表明，MADOD 在跨看不见的域的语义 OOD 检测方面具有卓越的性能，实现了 8.48% 至 20.81% 的 AUPR 提升，同时保持了有竞争力的分布内分类准确率，代表了在处理协变量和语义转变方面取得的显着进步。]]></description>
      <guid>https://arxiv.org/abs/2411.02444</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>你脱离上下文了！</title>
      <link>https://arxiv.org/abs/2411.02464</link>
      <description><![CDATA[arXiv:2411.02464v1 公告类型：交叉 
摘要：本研究基于数据向量空间表示中的“变形”概念，提出了一种用于机器学习 (ML) 模型的新型漂移检测方法。认识到新数据可以充当拉伸、压缩或扭曲模型学习到的几何关系的力，我们探索了各种数学框架来量化这种变形。我们研究了诸如协方差矩阵的特征值分析等措施来捕捉全局形状变化、使用核密度估计 (KDE) 进行局部密度估计以及 Kullback-Leibler 散度来识别数据集中的细微变化。此外，我们从连续力学中汲取灵感，提出了一种“应变张量”类比来捕捉不同数据类型的多面变形。这需要仔细估计位移场，我们深入研究了从基于密度的方法到流形学习和神经网络方法等策略。通过持续监控这些变形指标并将它们与模型性能关联起来，我们旨在提供一个敏感、可解释且适应性强的漂移检测系统，该系统能够区分良性数据演变和真实漂移，从而实现及时干预并确保机器学习系统在动态环境中的可靠性。针对此方法的计算挑战，我们讨论了降维、近似算法和并行化等缓解策略，用于实时和大规模应用。该方法的有效性通过对真实文本数据的实验得到证明，重点是检测生成式人工智能中的上下文变化。我们的结果由公开可用的代码支持，突出了这种基于变形的方法在捕捉传统统计方法经常遗漏的细微漂移方面的优势。此外，我们还在医疗保健领域提供了一个详细的应用示例，展示了该方法在不同领域的潜力。未来的工作将侧重于进一步提高计算效率并探索不同 ML 领域的其他应用。]]></description>
      <guid>https://arxiv.org/abs/2411.02464</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>看到它，思考它，分类：大型多模态模型是小样本时间序列异常分析仪</title>
      <link>https://arxiv.org/abs/2411.02465</link>
      <description><![CDATA[arXiv:2411.02465v1 公告类型：交叉 
摘要：由于各个行业的时间序列数据快速增长，时间序列异常检测 (TSAD) 变得越来越重要。例如，Web 服务数据中的异常可能表示系统故障或服务器故障等关键事件，需要及时检测和响应。然而，大多数现有的 TSAD 方法严重依赖手动特征工程或需要大量标记的训练数据，同时提供有限的可解释性。为了应对这些挑战，我们引入了一个称为时间序列异常多模态分析器 (TAMA) 的开创性框架，它利用大型多模态模型 (LMM) 的强大功能来增强对时间序列数据中异常的检测和解释。通过将时间序列转换为 LMM 可以有效处理的视觉格式，TAMA 利用少量上下文学习功能来减少对大量标记数据集的依赖。我们的方法通过对多个真实数据集的严格实验得到验证，其中 TAMA 在 TSAD 任务中始终优于最先进的方法。此外，TAMA 提供丰富的基于自然语言的语义分析，为检测到的异常的性质提供更深入的见解。此外，我们贡献了首批开源数据集之一，其中包括异常检测标签、异常类型标签和上下文描述，促进了这一关键领域的更广泛探索和进步。最终，TAMA 不仅在异常检测方面表现出色，而且还提供了一种全面的方法来理解异常的根本原因，通过创新方法和见解推动 TSAD 向前发展。]]></description>
      <guid>https://arxiv.org/abs/2411.02465</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无论人口统计学如何，追求无害的罗尔斯公平</title>
      <link>https://arxiv.org/abs/2411.02467</link>
      <description><![CDATA[arXiv:2411.02467v1 公告类型：交叉 
摘要：出于隐私和安全方面的考虑，群体公平性方面的最新进展主张无论人口统计信息如何，都要进行模型训练。然而，大多数方法仍然需要事先了解人口统计数据。在这项研究中，我们探索了在没有向训练集提供先前人口统计数据的情况下实现公平而不损害其效用的潜力，即 \emph{无害的罗尔斯公平}。我们确定，这种不需要先前人口统计信息的公平要求会促使训练损失呈现狄拉克增量分布。为此，我们提出了一种简单但有效的方法，称为 VFair，以最小化最佳经验损失集内的训练损失方差。然后通过一种在损失和梯度维度上运行的定制动态更新方法优化这个问题，将模型引向相对公平的解决方案，同时保留其完整的效用。我们的实验结果表明，回归任务（文献中相对较少探索）可以通过 VFair 实现显著的公平性改进，而不管之前是否有任何改进，而分类任务通常由于其量化效用测量而无法实现。我们方法的实现可在 \url{https://github.com/wxqpxw/VFair} 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2411.02467</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预训练的 Transformer 可在上下文中高效学习低维目标函数</title>
      <link>https://arxiv.org/abs/2411.02544</link>
      <description><![CDATA[arXiv:2411.02544v1 公告类型：交叉 
摘要：Transformers 可以从示例演示中有效地进行上下文学习。大多数现有的理论分析研究了 Transformers 对线性函数类的上下文学习 (ICL) 能力，其中通常表明预训练损失的最小化器在最小二乘目标上实现了一个梯度下降步骤。然而，这种简化的线性设置可能并没有证明 ICL 的统计效率，因为预训练的 Transformer 在测试提示上的表现并不优于直接解决线性回归。在本文中，我们通过具有非线性 MLP 层的变换器研究了非线性函数类的 ICL：给定一类 \textit{单索引} 目标函数 $f_*(\boldsymbol{x}) = \sigma_*(\langle\boldsymbol{x},\boldsymbol{\beta}\rangle)$，其中索引特征 $\boldsymbol{\beta}\in\mathbb{R}^d$ 来自 $r$ 维子空间，我们表明，通过梯度下降优化的非线性变换器（其预训练样本复杂度取决于链接函数 $\sigma_*$ 的 \textit{信息指数}）在上下文中学习 $f_*$，其提示长度仅取决于目标函数 $r$ 分布的维数；相反，任何在测试提示上直接学习 $f_*$ 的算法都会产生与环境维度 $d$ 成比例的统计复杂度。我们的结果强调了预训练变换器对函数类低维结构的适应性，这使得样本高效的 ICL 优于只能访问上下文数据的估计器。]]></description>
      <guid>https://arxiv.org/abs/2411.02544</guid>
      <pubDate>Wed, 06 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>