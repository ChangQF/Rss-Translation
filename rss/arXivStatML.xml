<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 14 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>NNGP 核与 Matern 核的对应关系</title>
      <link>https://arxiv.org/abs/2410.08311</link>
      <description><![CDATA[arXiv:2410.08311v1 公告类型：新
摘要：表示神经网络架构极限情况的内核最近越来越受欢迎。然而，与现有选项（例如 Matern 内核）相比，这些新内核的应用和性能尚未得到很好的研究。我们采用一种实用的方法来探索神经网络高斯过程 (NNGP) 内核及其在高斯过程回归数据中的应用。我们首先证明了规范化对于生成有效 NNGP 内核的必要性，并探索相关的数值挑战。我们进一步证明，该模型的预测非常不灵活，因此在有效的超参数集上不会有太大变化。然后，我们展示了一个令人惊讶的结果，即在特定情况下，NNGP 内核给出的预测与 Matern 内核给出的预测非常接近，这表明过度参数化的深度神经网络与 Matern 内核之间存在深度相似性。最后，我们在三个基准数据案例上展示了 NNGP 核与 Matern 核的性能比较，并得出结论：由于其灵活性和实用性能，Matern 核在实际应用中比新型 NNGP 更受欢迎。]]></description>
      <guid>https://arxiv.org/abs/2410.08311</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>避免通过强化学习微调的扩散模型中的模式崩溃</title>
      <link>https://arxiv.org/abs/2410.08315</link>
      <description><![CDATA[arXiv:2410.08315v1 公告类型：新
摘要：通过强化学习 (RL) 对基础模型进行微调已被证明有望与下游目标保持一致。在扩散模型 (DM) 的情况下，虽然 RL 训练从早期时间步骤开始改善对齐，但出现了诸如训练不稳定性和模式崩溃等关键问题。我们通过利用 DM 的层次性来解决这些缺点：我们在每个时期使用量身定制的 RL 方法动态训练它们，从而可以持续评估和逐步改进模型性能（或对齐）。此外，我们发现并非每个去噪步骤都需要进行微调以使 DM 与下游任务保持一致。因此，除了剪辑之外，我们还通过滑动窗口方法在不同的学习阶段对模型参数进行正则化。我们的方法称为分层奖励微调（HRF），在去噪扩散策略优化方法上得到了验证，我们表明，使用 HRF 训练的模型在下游任务中更好地保​​持了多样性，从而增强了微调的稳健性和不影响平均奖励的稳定性。]]></description>
      <guid>https://arxiv.org/abs/2410.08315</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迭代函数系统轨道再生核希尔伯特空间的学习上界</title>
      <link>https://arxiv.org/abs/2410.08361</link>
      <description><![CDATA[arXiv:2410.08361v1 公告类型：新
摘要：学习理论中的一个关键问题是计算一个函数 $f$，该函数可以紧密近似某个输入 $x$ 和相应输出 $y$ 之间的关系，使得 $y\approx f(x)$。此近似基于样本点 $(x_t,y_t)_{t=1}^{m}$，其中函数 $f$ 可以使用各种学习算法在再生核希尔伯特空间内进行近似。在学习理论的背景下，通常习惯假设样本点是从未知的底层分布中独立且相同分布 (i.i.d.) 抽取的。但是，我们放宽了这个 i.i.d。假设将输入序列 $(x_t)_{t\in {\mathbb N}}$ 视为迭代函数系统生成的轨迹，该轨迹形成特定的马尔可夫链，其中 $(y_t)_{t\in {\mathbb N}}$ 对应于模型处于相应状态 $x_t$ 时的观察序列。对于这样的过程，我们使用马尔可夫链随机梯度算法近似函数 $f$，并通过在再生核希尔伯特空间中推导上界来估计误差。]]></description>
      <guid>https://arxiv.org/abs/2410.08361</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SOAK：相同/其他/所有 K 折交叉验证，用于估计数据子集中模式的相似性</title>
      <link>https://arxiv.org/abs/2410.08643</link>
      <description><![CDATA[arXiv:2410.08643v1 公告类型：新
摘要：在机器学习的许多实际应用中，我们感兴趣的是了解是否有可能对迄今为止收集的数据进行训练，并对在某些方面（时间段、地理区域等）存在定性差异的新测试数据子集进行准确预测。另一个问题是数据子集是否足够相似，以便在模型训练期间合并子集是有益的。我们提出了 SOAK，即相同/其他/所有 K 倍交叉验证，这是一种可用于回答这两个问题的新方法。SOAK 系统地比较在不同数据子集上训练的模型，然后用于对固定测试子集进行预测，以估计数据子集中可​​学习/可预测模式的相似性。我们展示了在六个新的真实数据集（具有地理/时间子集，以检查对新子集的预测是否准确）、3 个图像对数据集（子集是不同的图像类型，以检查我们在相似图像上获得较小的预测误差）和 11 个具有预定义训练/测试分割的基准数据集（以检查预定义分割的相似性）上使用 SOAK 的结果。]]></description>
      <guid>https://arxiv.org/abs/2410.08643</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>失去维度：生成扩散中的几何记忆</title>
      <link>https://arxiv.org/abs/2410.08727</link>
      <description><![CDATA[arXiv:2410.08727v1 公告类型：新
摘要：生成扩散过程是与统计物理学的基本概念密切相关的最先进的机器学习模型。根据数据集的大小和网络的容量，它们的行为已知会从联想记忆状态转变为泛化阶段，这种现象被描述为玻璃相变。在这里，使用统计物理技术，我们将生成扩散中的记忆理论扩展到流形支持的数据。我们的理论和实验结果表明，由于记忆效应，在不同的关键时间和数据集大小下，不同的切线子空间会丢失，这取决于数据沿其方向的局部方差。也许与直觉相反，我们发现，在某些情况下，由于记忆效应，方差较大的子空间会首先丢失。这会导致选择性维度损失，其中数据的某些突出特征被记住，而不会在任何单个训练点上完全崩溃。我们通过对图像数据集和线性流形上训练的网络进行全面的实验来验证我们的理论，结果与理论预测具有显著的定性一致性。]]></description>
      <guid>https://arxiv.org/abs/2410.08727</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>校准的计算感知高斯过程</title>
      <link>https://arxiv.org/abs/2410.08796</link>
      <description><![CDATA[arXiv:2410.08796v1 公告类型：新
摘要：高斯过程因训练集大小的三次方缩放而臭名昭著，无法应用于非常大的回归问题。计算感知高斯过程 (CAGP) 通过利用概率线性求解器来降低复杂性来解决此缩放问题，由于计算量减少而扩大后验并增加计算不确定性。然而，最常用的 CAGP 框架导致（有时显着）保守的不确定性量化，使后验在实践中不切实际。在这项工作中，我们证明，如果使用的概率线性求解器在严格的统计意义上经过校准，那么诱导的 CAGP 也是如此。因此，我们提出了一种新的 CAGP 框架 CAGP-GS，基于使用 Gauss-Seidel 迭代作为底层概率线性求解器。当测试集为低维且执行的迭代次数较少时，CAGP-GS 与现有方法相比表现良好。我们在一个综合问题上测试校准性，并将其性能与大规模全球温度回归问题的现有方法进行比较。]]></description>
      <guid>https://arxiv.org/abs/2410.08796</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FedProx 中个性化的影响：对统计准确性和沟通效率的细粒度分析</title>
      <link>https://arxiv.org/abs/2410.08934</link>
      <description><![CDATA[arXiv:2410.08934v1 公告类型：新
摘要：FedProx 是一种简单而有效的联邦学习方法，可通过正则化实现模型个性化。尽管在实践中取得了显著的成功，但尚未完全建立对这种正则化如何可证明地提高每个客户端本地模型的统计准确性的严格分析。启发式地设置正则化强度存在风险，因为不适当的选择甚至会降低准确性。这项工作通过分析正则化对统计准确性的影响填补了这一空白，从而为设置实现个性化的正则化强度提供了理论指导。我们证明，通过在不同的统计异质性下自适应地选择正则化强度，FedProx 可以始终优于纯本地训练并实现接近极小最大最优的统计速率。此外，为了阐明资源分配，我们设计了一种算法，可证明更强的个性化可以降低通信复杂性而不会增加计算成本开销。最后，我们的理论在合成和现实世界的数据集上得到验证，并在非凸设置中验证了其普遍性。]]></description>
      <guid>https://arxiv.org/abs/2410.08934</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提升判定系数：快速无模型预测区间和无似然模型比较</title>
      <link>https://arxiv.org/abs/2410.08958</link>
      <description><![CDATA[arXiv:2410.08958v1 公告类型：新
摘要：我们提出了 $\textit{提升线性模型}$，并得出无模型预测区间，随着预测和观察之间的相关性增加，这些区间会变得更紧密。这些区间激发了 $\textit{提升判定系数}$，这是基于预测的设置（例如回归、分类或计数）中任意损失函数的模型比较标准。我们将预测区间扩展到更一般的误差分布，并提出了一种快速的无模型回归异常值检测算法。最后，我们通过数值实验说明了该框架。]]></description>
      <guid>https://arxiv.org/abs/2410.08958</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图混合依赖下的在线到 PAC 泛化界限</title>
      <link>https://arxiv.org/abs/2410.08977</link>
      <description><![CDATA[arXiv:2410.08977v1 公告类型：新
摘要：统计学习中的传统泛化结果需要由独立绘制的示例组成的训练数据集。最近为放宽这种独立性假设所做的大多数努力都考虑了纯时间（混合）依赖性或图依赖性，其中不相邻的顶点对应于独立的随机变量。这两种方法都有各自的局限性，前者需要时间有序的结构，而后者缺乏量化相互依赖强度的方法。在这项工作中，我们通过提出一个依赖性随图距离衰减的框架来弥合这两条工作线。我们利用在线到 PAC 框架推导出泛化界限，通过推导出集中结果并引入包含图结构的在线学习框架。由此产生的高概率泛化保证取决于混合率和图的色数。]]></description>
      <guid>https://arxiv.org/abs/2410.08977</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>广义线性模型不平衡分类的最佳下采样</title>
      <link>https://arxiv.org/abs/2410.08994</link>
      <description><![CDATA[arXiv:2410.08994v1 公告类型：新
摘要：下采样或欠采样是一种在大型和高度不平衡的分类模型背景下使用的技术。我们使用广义线性模型 (GLM) 研究不平衡分类的最佳下采样。我们提出了一个伪最大似然估计量，并在相对于越来越大的样本量而越来越不平衡的群体背景下研究了它的渐近正态性。我们为引入的估计量提供了理论保证。此外，我们使用平衡统计准确性和计算效率的标准来计算最佳下采样率。我们对合成数据和经验数据进行的数值实验进一步验证了我们的理论结果，并证明了引入的估计量优于常见的替代方案。]]></description>
      <guid>https://arxiv.org/abs/2410.08994</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用幂律数据谱分析双层网络中的神经尺度律</title>
      <link>https://arxiv.org/abs/2410.09005</link>
      <description><![CDATA[arXiv:2410.09005v1 公告类型：新
摘要：神经缩放定律描述了深度神经网络的性能如何随训练数据大小、模型复杂度和训练时间等关键因素而缩放，通常遵循多个数量级的幂律行为。尽管这些缩放定律是经验观察的结果，但对这些缩放定律的理论理解仍然有限。在这项工作中，我们采用统计力学技术来分析学生-教师框架内的一次随机梯度下降，其中学生和老师都是两层神经网络。我们的研究主要关注泛化误差及其对表现出幂律谱的数据协方差矩阵的响应行为。对于线性激活函数，我们推导出泛化误差的解析表达式，探​​索不同的学习机制并确定幂律缩放出现的条件。此外，我们将分析扩展到特征学习机制中的非线性激活函数，研究数据协方差矩阵中的幂律谱如何影响学习动态。重要的是，我们发现对称平台的长度取决于数据协方差矩阵的不同特征值的数量和隐藏单元的数量，从而展示了这些平台在不同配置下的行为方式。此外，我们的结果揭示了当数据协方差矩阵具有幂律谱时，在特殊阶段从指数收敛转变为幂律收敛。这项工作有助于从理论上理解神经缩放定律，并为在涉及复杂数据结构的实际场景中优化学习性能提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2410.09005</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>流形假设下扩散模型的线性收敛</title>
      <link>https://arxiv.org/abs/2410.09046</link>
      <description><![CDATA[arXiv:2410.09046v1 公告类型：新
摘要：分数匹配生成模型已被证明能够成功从复杂的高维数据分布中进行采样。在许多应用中，人们认为这种分布集中在嵌入到 $D$ 维空间中的低得多的 $d$ 维流形上；这被称为流形假设。目前最著名的收敛保证要么是 $D$ 的线性收敛，要么是 $d$ 的多项式（超线性）。后者利用了一种用于后向 SDE 的新型积分方案。我们兼顾了两全其美，并表明扩散模型在 Kullback-Leibler~(KL) 散度中收敛所需的步数在固有维度 $d$ 上是线性的（最多为对数项）。此外，我们表明这种线性依赖性是尖锐的。]]></description>
      <guid>https://arxiv.org/abs/2410.09046</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EarthquakeNPP：利用神经点过程进行地震预测的基准数据集</title>
      <link>https://arxiv.org/abs/2410.08226</link>
      <description><![CDATA[arXiv:2410.08226v1 公告类型：交叉 
摘要：几十年来，经典点过程模型（例如流行型余震序列 (ETAS) 模型）已被广泛用于预测地震事件发生的时间和位置。最近的进展导致了神经点过程 (NPP) 的出现，它有望比经典模型具有更大的灵活性和改进。然而，目前使用的 NPP 基准数据集并不代表地震学界的最新挑战，因为它缺乏该地区的关键地震序列，并且不正确地分割了训练和测试数据。此外，初始地震预报基准缺乏与地震学界通常使用的最先进的地震预报模型的比较。为了解决这些差距，我们引入了 EarthquakeNPP：一组基准数据集，以方便在地震数据上测试 NPP，并伴随着可靠的 ETAS 模型实现。这些数据集涵盖了加利福尼亚州从 1971 年到 2021 年的一系列从小到大的目标区域，并包括不同的数据集生成方法。在基准测试实验中，我们将三个时空 NPP 与 ETAS 进行了比较，发现在空间或时间对数似然方面，没有一个能胜过 ETAS。这些结果表明，当前的 NPP 实现尚不适合实际的地震预报。然而，EarthquakeNPP 将成为地震学和机器学习社区之间合作的平台，旨在提高地震的可预测性。]]></description>
      <guid>https://arxiv.org/abs/2410.08226</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>循环变换器 (Looped Transformers) 能否学习实现多步梯度下降以进行上下文学习？</title>
      <link>https://arxiv.org/abs/2410.08292</link>
      <description><![CDATA[arXiv:2410.08292v1 公告类型：交叉 
摘要：Transformers 无需任何微调即可进行推理和少量学习，这种出色的能力被广泛推测源于它们能够在一次前向传递中用权重隐式模拟多步骤算法（例如梯度下降）。最近，从表达力的角度理解这一复杂现象取得了进展，证明了 Transformers 可以表达这种多步骤算法。然而，除了单层模型之外，我们对它的可学习性的更基本方面的了解非常有限。特别是，训练 Transformers 能否实现算法解决方案的收敛？在这项工作中，我们解决了上下文线性回归与线性循环 Transformers 的问题——这是一个具有权重共享的多层模型，据推测它具有归纳偏差来学习定点迭代算法。更具体地说，对于这种设置，我们表明种群训练损失的全局最小化器实现了多步预处理梯度下降，其中预处理器适应数据分布。此外，我们通过证明一种新的梯度优势条件，证明了梯度流在回归损失上的快速收敛，尽管地形不凸。据我们所知，这是这种设置下多层 Transformer 的首次理论分析。我们通过综合实验进一步验证了我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2410.08292</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>概念学习和组合概括的动态</title>
      <link>https://arxiv.org/abs/2410.08309</link>
      <description><![CDATA[arXiv:2410.08309v1 公告类型：交叉 
摘要：先前的研究表明，文本条件扩散模型可以学习识别和操纵组合数据生成过程背后的原始概念，从而能够推广到全新的、超出分布的组合。除了性能评估之外，这些研究还开发了丰富的学习动力学经验现象学，表明模型按顺序进行推广，尊重数据生成过程的组合层次结构。此外，数据中以概念为中心的结构显著影响模型学习操纵概念的能力的速度。在本文中，我们旨在从理论的角度更好地描述这些经验结果。具体而言，我们通过引入结构化身份映射 (SIM) 任务提出了先前工作的组合泛化问题的抽象，其中训练模型以学习具有结构组织质心的高斯混合上的身份映射。我们用数学方法分析了在这项 SIM 任务上训练的神经网络的学习动态，并表明尽管 SIM 很简单，但它的学习动态可以捕捉并帮助解释在先前工作中确定的扩散模型的组合泛化的关键经验观察。我们的理论还提供了几个新的见解——例如，我们发现了一种在训练早期阶段测试损失的非单调学习动态的新机制。我们通过训练文本条件扩散模型来验证我们的新预测，将我们的简化框架与复杂的生成模型连接起来。总的来说，这项工作将 SIM 任务确立为现代生成模型中概念学习动态的有意义的理论抽象。]]></description>
      <guid>https://arxiv.org/abs/2410.08309</guid>
      <pubDate>Mon, 14 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>