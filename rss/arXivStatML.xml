<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 14 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>具有凸低层问题的简单双层优化的加速梯度法</title>
      <link>https://arxiv.org/abs/2402.08097</link>
      <description><![CDATA[在本文中，我们关注简单的双层优化问题，其中我们最小化另一个凸平滑约束优化问题的最优解集上的凸平滑目标函数。我们提出了一种新颖的双层优化方法，该方法使用切割平面方法局部近似较低层问题的解集，并采用基于加速梯度的更新来减少近似解集上的上层目标函数。我们根据次优误差和不可行性误差来衡量我们方法的性能，并为这两个误差标准提供非渐近收敛保证。具体来说，当可行集是紧凑的时，我们表明我们的方法最多需要 $\mathcal{O}(\max\{1/\sqrt{\epsilon_{f}}, 1/\epsilon_g\})$ 次迭代找到一个$\epsilon_f$-次优且$\epsilon_g$-不可行的解决方案。此外，在较低层目标满足 $r$-th H\&quot;olderian 误差界限的附加假设下，我们表明我们的方法实现了 $\mathcal{O}(\max\{\epsilon_{ f}^{-\frac{2r-1}{2r}},\epsilon_{g}^{-\frac{2r-1}{2r}}\})$，符合单层最优复杂度当$r=1$时的凸约束优化。]]></description>
      <guid>https://arxiv.org/abs/2402.08097</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>学习具有拉普拉斯约束的笛卡尔积图</title>
      <link>https://arxiv.org/abs/2402.08105</link>
      <description><![CDATA[图拉普拉斯学习，也称为网络拓扑推理，是多个社区非常感兴趣的问题。在高斯图模型（GM）中，图学习相当于赋予拉普拉斯结构协方差选择。在图信号处理（GSP）中，从过滤系统的输出推断未观察到的图至关重要。在本文中，我们研究了拉普拉斯约束下学习笛卡尔积图的问题。笛卡尔图积是对高阶条件依赖进行建模的自然方法，也是将 GSP 推广到多路张量的关键。我们为笛卡尔乘积拉普拉斯算子的惩罚最大似然估计（MLE）建立了统计一致性，并提出了一种有效的算法来解决该问题。我们还扩展了我们的方法，以在存在结构缺失值的情况下进行有效的联合图学习和插补。对合成数据集和真实世界数据集的实验表明，我们的方法优于以前的 GSP 和 GM 方法。]]></description>
      <guid>https://arxiv.org/abs/2402.08105</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>CNN 需要哪些频率？特征学习中出现的瓶颈结构</title>
      <link>https://arxiv.org/abs/2402.08010</link>
      <description><![CDATA[我们描述了 CNN 中卷积瓶颈 (CBN) 结构的出现，其中网络使用其前几层将输入表示转换为仅沿少数频率和通道支持的表示，然后使用最后几层进行映射回到输出。我们定义了 CBN 等级，它描述了瓶颈内保留的频率的数量和类型，并部分证明了表示函数 $f$ 所需的参数范数随着深度乘以 CBN 等级 $f$ 而变化。我们还表明，参数范数在下一阶上取决于 $f$ 的规律性。我们证明，任何具有几乎最佳参数范数的网络都会在权重和激活中表现出 CBN 结构（假设网络在大学习率下稳定），这激发了下采样的常见实践；我们验证了 CBN 结果在下采样时仍然成立。最后，我们使用 CBN 结构来解释 CNN 在许多任务中学习到的函数。]]></description>
      <guid>https://arxiv.org/abs/2402.08010</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>扩散生成模型的最近邻分数估计器</title>
      <link>https://arxiv.org/abs/2402.08018</link>
      <description><![CDATA[评分函数估计是扩散生成模型训练和采样的基石。尽管如此，最常用的估计器要么是有偏差的神经网络近似值，要么是基于条件分数的高方差蒙特卡罗估计器。我们引入了一种新颖的最近邻得分函数估计器，它利用训练集中的多个样本来显着降低估计器方差。我们在两个引人注目的应用程序中利用了我们的低方差估计器。使用我们的估计器训练一致性模型，我们报告收敛速度和样本质量都有显着提高。在扩散模型中，我们表明我们的估计器可以取代学习网络进行概率流 ODE 集成，为未来研究开辟了有希望的新途径。]]></description>
      <guid>https://arxiv.org/abs/2402.08018</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>根据低质量数据对 COVID-19 ICU 需求进行年龄结构估计</title>
      <link>https://arxiv.org/abs/2006.06530</link>
      <description><![CDATA[我们按照确诊病例的年龄结构概率对加重病例进行抽样，并使用 ICU 占用数据来查找子通知因素。然后采用逻辑拟合来预测 COVID-19 流行病的进展，并从已达到此阶段的地点获取平稳情景。最后，通过子通知因子对找到的逻辑曲线进行校正，并进行采样以预测未来对 ICU 床位的需求。]]></description>
      <guid>https://arxiv.org/abs/2006.06530</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>泛化与平坦最小值之间的 PAC-贝叶斯联系</title>
      <link>https://arxiv.org/abs/2402.08508</link>
      <description><![CDATA[现代机器学习通常涉及过度参数化设置（训练参数的数量大于数据集大小）的预测器，它们的训练不仅在训练数据上产生良好的性能，而且具有良好的泛化能力。这种现象挑战了许多理论结果，并且仍然是一个悬而未决的问题。为了更好地理解，我们提供了涉及梯度项的新颖的泛化界限。为此，我们将 PAC-Bayes 工具箱与 Poincar&#39;e 和 Log-Sobolev 不等式相结合，避免对预测变量空间维度的显式依赖。我们的结果强调了\emph{平坦最小值}（邻域的最小值也几乎最小化了学习问题）对泛化性能的积极影响，直接涉及优化阶段的好处。]]></description>
      <guid>https://arxiv.org/abs/2402.08508</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>调整识别距离：因果结构学习的 gadjid</title>
      <link>https://arxiv.org/abs/2402.08616</link>
      <description><![CDATA[评估因果发现算法学习到的图是很困难的：两个图之间不同的边数并不能反映图在它们为因果效应建议的识别公式方面有何不同。我们引入了一个用于开发图之间因果距离的框架，其中包括作为特殊情况的有向无环图的结构干预距离。我们使用这个框架来开发改进的基于调整的距离以及对完整的部分有向无环图和因果顺序的扩展。我们开发多项式时间可达性算法来有效地计算距离。在我们的 gadjid 包（开源 https://github.com/CausalDisco/gadjid）中，我们提供了距离的实现；它们比结构干预距离快几个数量级，从而为因果发现提供了一个成功指标，可以扩展到以前令人望而却步的图形大小。]]></description>
      <guid>https://arxiv.org/abs/2402.08616</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>网络上的交互粒子系统：网络和交互内核的联合推理</title>
      <link>https://arxiv.org/abs/2402.08412</link>
      <description><![CDATA[对网络上的多代理系统进行建模是许多学科中的一项基本挑战。我们共同推断网络的权重矩阵和交互内核，它们分别确定哪些代理与其他代理交互以及从由多个轨迹组成的数据中交互的规则。我们提出的估计器自然会导致非凸优化问题，并且我们研究了两种解决方案的方法：一种是基于交替最小二乘（ALS）算法；另一种是基于交替最小二乘（ALS）算法。另一种基于一种名为交替最小二乘算子回归（ORALS）的新算法。这两种算法都可以扩展到大型数据轨迹集合。我们建立保证可识别性和适定性的强制条件。即使在小数据范围内，ALS 算法在统计上也显得高效且稳健，但缺乏性能和收敛保证。 ORALS 估计量在矫顽力条件下是一致且渐近正态的。我们进行了多项数值实验，范围从网络上的仓本粒子系统到领导者-追随者模型中的意见动态。]]></description>
      <guid>https://arxiv.org/abs/2402.08412</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>通过熵传输内核从批量不配对点传输算子</title>
      <link>https://arxiv.org/abs/2402.08425</link>
      <description><![CDATA[在本文中，我们关注的是估计随机变量 $X$ 和 $Y$ 的联合概率，给定 $N$ 个独立观察块 $(\boldsymbol{x}^i,\boldsymbol{y}^i)$， $i=1,\ldots,N$，每个 $M$ 个样本 $(\boldsymbol{x}^i,\boldsymbol{y}^i) = \bigl((x^i_j, y^i_{\sigma ^i(j)}) \bigr)_{j=1}^M$，其中 $\sigma^i$ 表示 i.i.d 的未知排列。采样对 $(x^i_j,y_j^i)$, $j=1,\ldots,M$。这意味着观察块内$M$样本的内部排序是未知的。我们推导了最大似然推理函数，提出了计算上易于处理的近似值并分析了它们的属性。特别是，我们证明了 $\Gamma$ 收敛结果，表明当块的数量 $N$ 趋于无穷大时，我们可以从经验近似中恢复真实密度。使用熵最优传输内核，我们对一类密度函数的假设空间进行建模，在该假设空间上可以最小化推理函数。该假设类特别适合从数据中近似推断传输算子。我们通过修改 EMML 算法来解决由此产生的离散最小化问题，以考虑额外的转移概率约束，并证明该算法的收敛性。概念验证示例证明了我们方法的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.08425</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>Noisy-SGD 中的隐式偏差：在差异化私人训练中的应用</title>
      <link>https://arxiv.org/abs/2402.08344</link>
      <description><![CDATA[与大批量训练相比，使用随机梯度下降 (SGD) 小批量训练深度神经网络 (DNN) 可产生卓越的测试性能。众所周知，SGD 固有的特定噪声结构是造成这种隐含偏差的原因。 DP-SGD 用于确保 DNN 训练中的差分隐私 (DP)，将高斯噪声添加到剪切的梯度中。令人惊讶的是，大批量训练仍然会导致性能显着下降，这构成了一个重要的挑战，因为强大的 DP 保证需要使用大批量。我们首先证明这种现象扩展到 Noisy-SGD（没有削波的 DP-SGD），这表明随机性（而不是削波）是这种隐式偏差的原因，即使有额外的各向同性高斯噪声。我们从理论上分析了线性最小二乘和对角线性网络设置的连续版本 Noisy-SGD 获得的解决方案，并揭示了隐式偏差确实被附加噪声放大了。因此，大批量 DP-SGD 训练的性能问题与 SGD 有着相同的基本原理，这为大批量训练策略的潜在改进带来了希望。]]></description>
      <guid>https://arxiv.org/abs/2402.08344</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>关于 Transformer 架构的局限性</title>
      <link>https://arxiv.org/abs/2402.08164</link>
      <description><![CDATA[大语言模型（LLM）产生幻觉的根本原因是什么？我们使用通信复杂性来证明，如果函数的域足够大，则 Transformer 层无法组合函数（例如，识别谱系中某个人的祖父母）；我们通过例子表明，当域非常小时，这种能力在经验上已经存在。我们还指出，对于法学硕士来说，所谓的组合任务的核心几个数学任务不太可能通过 Transformers 来解决，对于足够大的实例，并假设计算领域中某些广为接受的猜想复杂性是真实的。]]></description>
      <guid>https://arxiv.org/abs/2402.08164</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>弱分布重叠下马尔可夫决策过程的离策略评估</title>
      <link>https://arxiv.org/abs/2402.08201</link>
      <description><![CDATA[双鲁棒方法在顺序可忽略性下的马尔可夫决策过程 (MDP) 中的离策略评估具有相当大的前景：它们已被证明收敛为 $1/\sqrt{T}$ 与范围 $T$，在大范围内具有统计效率。样本，并允许模块化实现，其中可以使用标准强化学习技术执行初步估计任务。然而，现有的结果大量使用了强分布重叠假设，即目标策略和数据收集策略的平稳分布在彼此的有界因子内，并且这种假设通常仅在状态空间为MDP 是有界的。在本文中，我们在较弱的分布重叠概念下重新审视 MDP 中的离策略评估任务，并引入一类截断双鲁棒 (TDR) 估计器，我们发现它在这种情况下表现良好。当目标和数据收集策略的分配比率是平方可积（但不一定有界）时，我们的方法恢复了先前在强分布重叠下建立的大样本行为。当该比率不可平方积时，TDR 仍然一致，但速度慢于 $1/\sqrt{T}$；此外，对于仅使用混合条件定义的一类 MDP，该收敛速度是极小极大。我们在数值上验证了我们的方法，并发现，在我们的实验中，当强分布重叠不成立时，适当的截断在实现准确的离策略评估方面发挥着重要作用。]]></description>
      <guid>https://arxiv.org/abs/2402.08201</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>离散扩散模型的收敛性分析：通过均匀化精确实现</title>
      <link>https://arxiv.org/abs/2402.08095</link>
      <description><![CDATA[扩散模型在数据生成任务中取得了巨大的经验成功。最近，人们做出了一些努力，使扩散模型的框架适应离散状态空间，为建模本质上离散的数据（例如语言和图）提供了更自然的方法。这是通过将前向噪声过程和相应的反向过程表述为连续时间马尔可夫链（CTMC）来实现的。在本文中，我们研究了离散扩散模型的理论特性。具体来说，我们引入了一种利用连续马尔可夫链均匀化的算法，在随机时间点上实现转换。在学习离散得分函数的合理假设下，我们得出了从超立方体上的任何分布进行采样的总变异距离和 KL 散度保证。我们的结果与 $\mathbb{R}^d$ 中扩散模型的最先进成果一致，并进一步强调了离散扩散模型与 $\mathbb{R}^d$ 设置相比的优势。]]></description>
      <guid>https://arxiv.org/abs/2402.08095</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>微分同胚测度与生成建模的核匹配</title>
      <link>https://arxiv.org/abs/2402.08077</link>
      <description><![CDATA[本文提出了一个通用框架，用于使用常微分方程 (ODE) 和再生核希尔伯特空间 (RKHS) 进行最小散度生成建模和采样的概率测量传输，其灵感来自微分同胚匹配和图像配准的思想。对该方法进行了理论分析，给出了模型复杂性、训练集中样本数量和模型错误指定的先验误差范围。一系列广泛的数值实验进一步突出了该方法的属性、优点和缺点，并将其适用性扩展到其他任务，例如条件模拟和推理。]]></description>
      <guid>https://arxiv.org/abs/2402.08077</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:04 GMT</pubDate>
    </item>
    <item>
      <title>基于分数的生成模型打破了学习亚高斯概率分布族时的维数诅咒</title>
      <link>https://arxiv.org/abs/2402.08082</link>
      <description><![CDATA[虽然基于分数的生成模型（SGM）在巨大的图像生成任务中取得了显着的成功，但它们的数学基础仍然有限。在本文中，我们分析了 SGM 在学习亚高斯概率分布族中的近似和泛化。我们引入了概率分布的复杂性概念，即概率分布相对于标准高斯测度的相对密度。我们证明，如果对数相对密度可以通过参数可以适当限制的神经网络来局部近似，那么经验得分匹配生成的分布就可以以与维度无关的比率近似总变差中的目标分布。我们通过例子来说明我们的理论，其中包括某些高斯的混合。我们证明的一个重要组成部分是为与前向过程相关的真实得分函数导出无维深度神经网络逼近率，这本身就很有趣。]]></description>
      <guid>https://arxiv.org/abs/2402.08082</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:04 GMT</pubDate>
    </item>
    </channel>
</rss>