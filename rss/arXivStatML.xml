<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 04 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>FredNormer：非平稳时间序列预测的频域归一化</title>
      <link>https://arxiv.org/abs/2410.01860</link>
      <description><![CDATA[arXiv:2410.01860v1 公告类型：新
摘要：最近基于规范化的方法在解决分布偏移问题方面取得了巨大成功，促进了非平稳时间序列预测。由于这些方法在时间域中运行，它们可能无法完全捕捉到在频域中更明显的动态模式，从而导致结果不理想。本文首先从理论上分析了规范化方法如何影响频率成分。我们证明，当前在时间域中运行的规范化方法均匀地缩放非零频率，因此，它们很难确定有助于更稳健预测的成分。因此，我们提出了 FredNormer，它从频率的角度观察数据集并自适应地增加关键频率成分的权重。为此，FredNormer 由两个部分组成：一个基于频率稳定性对输入样本进行规范化的统计指标，以及一个可学习的加权层，用于调整稳定性并引入特定于样本的变化。值得注意的是，FredNormer 是一个即插即用的模块，与现有的标准化方法相比，它不会牺牲效率。大量实验表明，FredNormer 在 ETTm2 数据集上将骨干预测模型的平均 MSE 提高了 33.3% 和 55.3%。与基线标准化方法相比，FredNormer 在 28 种设置中实现了 18 个 top-1 结果和 6 个 top-2 结果。]]></description>
      <guid>https://arxiv.org/abs/2410.01860</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于生成先验的朗之万动力学后验采样</title>
      <link>https://arxiv.org/abs/2410.02078</link>
      <description><![CDATA[arXiv:2410.02078v1 公告类型：新
摘要：使用生成模型在高维空间中进行后验采样对各种应用具有重要前景，包括但不限于逆问题和引导生成任务。尽管最近取得了许多进展，但生成不同的后验样本仍然是一个挑战，因为现有方法需要为每个新样本重新启动整个生成过程，这使得该过程的计算成本很高。在这项工作中，我们通过在预训练生成模型的噪声空间中模拟朗之万动力学来提出有效的后验采样。通过利用蒸馏流或一致性模型可以提供的噪声和数据空间之间的映射，我们的方法能够无缝探索后验，而无需重新运行整个采样链，从而大大减少了计算开销。从理论上讲，我们证明了所提出的噪声空间朗之万动力学可以近似后验，假设生成模型充分近似先验分布。我们的框架已在涉及噪声线性和非线性前向算子的图像恢复任务上进行了实验验证，这些算子应用于 LSUN-Bedroom (256 x 256) 和 ImageNet (64 x 64) 数据集。结果表明，即使在有限数量的函数评估下，我们的方法也能生成具有增强语义多样性的高保真样本，与现有的基于扩散的后验采样技术相比，其效率和性能更出色。]]></description>
      <guid>https://arxiv.org/abs/2410.02078</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用集成路径稳定性选择进行误差控制的快速非参数特征选择</title>
      <link>https://arxiv.org/abs/2410.02208</link>
      <description><![CDATA[arXiv:2410.02208v1 公告类型：新
摘要：特征选择可以大大提高机器学习问题的性能和可解释性。然而，现有的非参数特征选择方法要么缺乏理论上的误差控制，要么在实践中无法准确控制误差。许多方法也很慢，尤其是在高维情况下。在本文中，我们介绍了一种通用特征选择方法，该方法将集成路径稳定性选择应用于阈值以控制假阳性和错误发现率。该方法还估计了 q 值，与 p 值相比，q 值更适合高维数据。我们重点关注基于梯度提升 (IPSSGB) 和随机森林 (IPSSRF) 的通用方法的两种特殊情况。使用 RNA 测序数据进行的大量模拟表明，IPSSGB 和 IPSSRF 具有更好的错误控制，检测到更多的真阳性，并且比现有方法更快。我们还使用这两种方法来检测与卵巢癌相关的 microRNA 和基因，发现它们比其他方法使用更少的特征做出更好的预测。]]></description>
      <guid>https://arxiv.org/abs/2410.02208</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于多臂老虎机中的 Lai 置信上限</title>
      <link>https://arxiv.org/abs/2410.02279</link>
      <description><![CDATA[arXiv:2410.02279v1 公告类型：新
摘要：在这篇纪念论文中，我们纪念 Tze Leung Lai 对多臂老虎机主题的开创性贡献，特别关注他在置信上限方面的开创性工作。我们为具有恒定高斯奖励探索水平的置信上限指数建立了尖锐的非渐近遗憾界限。此外，我们为 \cite{lai1987adaptive} 的置信上限指数建立了非渐近遗憾界限，该界限采用的探索函数随相应臂的样本大小而减小。遗憾界限具有与 Lai-Robbins 下限相匹配的领先常数。我们的研究结果突出了 Lai 开创性工作的一个方面，值得在机器学习文献中得到更多关注。]]></description>
      <guid>https://arxiv.org/abs/2410.02279</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用离散观察的功能数据进行分布式学习</title>
      <link>https://arxiv.org/abs/2410.02376</link>
      <description><![CDATA[arXiv:2410.02376v1 公告类型：新
摘要：通过选择不同的滤波函数，谱算法可以在样本学习框架内生成各种正则化方法来解决统计逆问题。本文将分布式谱算法与Sobolev核相结合，以解决函数线性回归问题。算法的设计和数学分析仅要求在离散样本点处观察到函数协变量。此外，算法的假设函数空间是Sobolev核生成的Sobolev空间，优化了逼近能力和灵活性。通过为目标函数和函数协变量建立正则性条件，我们推导出Sobolev范数中分布式谱算法收敛的匹配上下界。这表明所提出的正则性条件是合理的，并且在这些条件下的收敛分析是严格的，捕捉到了函数线性回归的基本特征。本文开发的分析技术和估计也增强了先前文献中的现有结果。]]></description>
      <guid>https://arxiv.org/abs/2410.02376</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过人类反馈进行强化学习的双重主动学习</title>
      <link>https://arxiv.org/abs/2410.02504</link>
      <description><![CDATA[arXiv:2410.02504v1 公告类型：新 
摘要：将大型语言模型 (LLM) 与人类偏好对齐对于生成人工智能的最新进展至关重要。从人类反馈中强化学习 (RLHF) 被广泛应用于实现这一目标。RLHF 中的一个关键步骤是从人类反馈中学习奖励函数。然而，人工反馈成本高昂且耗时，因此收集高质量的对话数据供人类教师标记至关重要。此外，不同的人类教师的专业水平也不同。因此，向最合适的老师征求他们的意见至关重要。在本文中，我们使用离线强化学习 (RL) 来制定对齐问题。受 $D$ 最优设计思想的启发，我们首先提出了一种双重主动奖励学习算法，用于同时选择对话和教师。接下来，我们应用悲观 RL 来解决对齐问题，基于学习到的奖励估计器。从理论上讲，我们证明了通过我们提出的自适应选择策略获得的奖励估计器渐近地实现了最小广义方差，并证明了我们的悲观策略的次优性在给定样本预算 $T$ 的情况下可达到 $O(1/\sqrt{T})$。通过对 LLM 的模拟和实验，我们证明了我们的算法的有效性及其优于最先进算法的优势。]]></description>
      <guid>https://arxiv.org/abs/2410.02504</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>局部流匹配生成模型</title>
      <link>https://arxiv.org/abs/2410.02548</link>
      <description><![CDATA[arXiv:2410.02548v1 公告类型：新
摘要：流匹配 (FM) 是一种无需模拟的方法，用于学习连续且可逆的流以在两个分布之间进行插值，特别是在生成建模中从噪声中生成数据。在本文中，我们介绍了局部流匹配 (LFM)，它学习一系列 FM 子模型，每个子模型匹配一个扩散过程，直到数据到噪声方向的步长时间。在每个步骤中，子模型要插值的两个分布比数据与噪声更接近，这使得可以使用更小的模型进行更快的训练。LFM 的逐步结构很容易被提炼，可以采用不同的提炼技术来加速生成。从理论上讲，我们根据生成的数据分布和真实数据分布之间的 $\chi^2$ 散度证明了所提出的流模型的生成保证。在实验中，我们证明了 LFM 在表格数据和图像数据集的无条件生成以及机器人操作策略的条件生成方面相比 FM 具有更高的训练效率和更具竞争力的生成性能。]]></description>
      <guid>https://arxiv.org/abs/2410.02548</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯在在线共形预测中的优势</title>
      <link>https://arxiv.org/abs/2410.02561</link>
      <description><![CDATA[arXiv:2410.02561v1 公告类型：新
摘要：基于共形预测 (CP) 框架，我们研究了给定黑盒机器学习模型的有效置信集的在线构建。通过将目标置信度转换为分位数水平，问题可以简化为预测顺序显示的数据序列的分位数（事后看来）。之前已经研究了两种截然不同的方法。 (i) 直接方法：假设数据序列是 iid 或可交换的，可以将观察到的数据的经验分布作为算法信念，并直接预测其分位数。 (ii) 间接方法：由于统计假设在实践中通常不成立，最近的趋势是考虑对抗性设置并将一阶在线优化应用于移动分位数损失（Gibbs &amp; Cand\`es，2021）。它需要事先知道目标分位数水平，并且由于相关的损失线性化，获得的置信集存在某些有效性问题。
本文提出了一种结合了两者优势的新型贝叶斯 CP 框架。在没有任何统计假设的情况下，它能够：(i) 在线回答多个任意置信度查询，并可证明遗憾程度较低；(ii) 克服一阶优化基线因“以数据为中心”而非“以迭代为中心”而遭受的有效性问题。
从技术角度来看，我们的关键思想是通过贝叶斯先验来规范上述直接方法的算法信念，通过在输出上模拟非线性的跟随正则化领导者 (FTRL) 算法来“增强”它。对于统计学家来说，这可以看作是贝叶斯推理的在线对抗性观点。重要的是，所提出的信念更新主干由针对不同置信度的预测主管共享，带来类似于 U 校准的实际好处（Kleinberg 等人，2023 年）。]]></description>
      <guid>https://arxiv.org/abs/2410.02561</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高度适应的脊</title>
      <link>https://arxiv.org/abs/2410.02680</link>
      <description><![CDATA[arXiv:2410.02680v1 公告类型：新
摘要：在本文中，我们提出了高度自适应岭 (HAR)：一种在具有平方可积截面导数的右连续函数类中实现 $n^{-1/3}$ 无维 L2 收敛速度的回归方法。这是一个大型非参数函数类，特别适用于表格数据。HAR 正是基于饱和零阶张量积样条基展开的具有特定数据自适应核的核岭回归。我们使用模拟和真实数据来验证我们的理论。我们展示了比小型数据集的最新算法更好的经验性能。]]></description>
      <guid>https://arxiv.org/abs/2410.02680</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型作为马尔可夫链</title>
      <link>https://arxiv.org/abs/2410.02724</link>
      <description><![CDATA[arXiv:2410.02724v1 公告类型：新
摘要：大型语言模型 (LLM) 已被证明非常高效，不仅在广泛的自然语言处理任务中如此，而且远远超出了这些任务的范围。然而，对其令人印象深刻的表现的起源的全面理论分析仍然难以捉摸。在本文中，我们通过绘制词汇量大小为 $T$、上下文窗口大小为 $K$ 的通用自回归语言模型与定义在大小为 $\mathcal{O}(T^K)$ 的有限状态空间上的马尔可夫链之间的等价关系来解决这一具有挑战性的任务。我们得出几个令人惊讶的发现，这些发现与马尔可夫链的平稳分布的存在有关，这些分布捕捉了 LLM 的推理能力、它们向它的收敛速度以及温度对后者的影响。然后，我们证明了预训练和上下文泛化界限，并展示了绘制的等价关系如何让我们丰富它们的解释。最后，我们通过最近几门 LLM 的实验说明了我们的理论保证，以强调它们如何捕捉实践中观察到的行为。]]></description>
      <guid>https://arxiv.org/abs/2410.02724</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Bayes-CATSI：一种用于医学时间序列数据插补的变分贝叶斯方法</title>
      <link>https://arxiv.org/abs/2410.01847</link>
      <description><![CDATA[arXiv:2410.01847v1 公告类型：交叉 
摘要：医学时间序列数据集具有缺失值，需要数据插补方法，然而，传统的机器学习模型由于预测中缺乏不确定性量化而无法达到预期效果。在这些模型中，CATSI（上下文感知时间序列插补）通过将上下文向量纳入插补过程，捕捉每个患者的全局依赖关系，以其有效性脱颖而出。在本文中，我们提出了一个贝叶斯上下文感知时间序列插补（Bayes-CATSI）框架，该框架利用变分推理提供的不确定性量化。我们考虑来自脑电图（EEG）、眼电图（EOG）、肌电图（EMG）、心电图（EKG）的时间序列。变分推理假设后验分布的形状，并通过最小化 Kullback-Leibler（KL）散度，找到最接近真实后验分布的变分密度。因此，我们将变分贝叶斯深度学习层集成到 CATSI 模型中。我们的结果表明，Bayes-CATSI 不仅提供了不确定性量化，而且与 CATSI 模型相比，还实现了更出色的插补性能。具体而言，Bayes-CATSI 的一个实例比 CATSI 的性能高出 9.57%。我们提供了一个开源代码实现，用于将 Bayes-CATSI 应用于其他医疗数据插补问题。]]></description>
      <guid>https://arxiv.org/abs/2410.01847</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>共形预测集可能造成不同的影响</title>
      <link>https://arxiv.org/abs/2410.01888</link>
      <description><![CDATA[arXiv:2410.01888v1 公告类型：交叉 
摘要：虽然共形预测是一种量化机器学习模型不确定性的有前途的方法，但它输出的预测集本身并不是可操作的。许多应用程序需要一个输出来采取行动，而不是几个。为了克服这个问题，可以将预测集提供给人类，然后人类做出明智的决定。在任何这样的系统中，确保受保护群体之间结果的公平性都至关重要，研究人员已经提出将均衡覆盖作为公平的标准。通过对人类参与者进行实验，我们证明提供预测集会增加他们决策的不公平性。令人不安的是，我们发现，与边际覆盖相比，提供满足均衡覆盖的集合实际上会增加不公平性。我们建议在各个群体之间均衡集合大小，而不是均衡覆盖，这在经验上会带来更公平的结果。]]></description>
      <guid>https://arxiv.org/abs/2410.01888</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于分数的拉回黎曼几何</title>
      <link>https://arxiv.org/abs/2410.01950</link>
      <description><![CDATA[arXiv:2410.01950v1 公告类型：交叉 
摘要：数据驱动的黎曼几何已成为可解释表示学习的强大工具，可提高下游任务的效率。展望未来，平衡廉价的流形映射和高效的训练算法至关重要。在这项工作中，我们整合了拉回黎曼几何和生成模型的概念，提出了一个在几何和学习上都可扩展的数据驱动黎曼几何框架：基于分数的拉回黎曼几何。首先关注单峰分布，我们提出了一种基于分数的黎曼结构，该结构具有通过数据概率密度的闭式测地线。利用这种结构，我们构建了一个具有误差界限的黎曼自动编码器 (RAE)，用于发现正确的数据流形维度。通过在训练期间采用等距正则化，该框架可以自然地与各向异性正则化流一起使用。通过对各种数据集进行数值实验，我们证明了我们的框架不仅可以通过数据支持生成高质量的测地线，而且还能可靠地估计数据流形的固有维度并提供流形的全局图表，即使在高维环境空间中也是如此。]]></description>
      <guid>https://arxiv.org/abs/2410.01950</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>哪些算法具有严格的泛化界限？</title>
      <link>https://arxiv.org/abs/2410.01969</link>
      <description><![CDATA[arXiv:2410.01969v1 公告类型：交叉 
摘要：我们研究哪些机器学习算法具有严格的泛化界限。首先，我们提出排除严格泛化界限存在的条件。具体来说，我们表明具有某些归纳偏差导致其不稳定的算法不允许严格的泛化界限。接下来，我们表明足够稳定的算法确实具有严格的泛化界限。我们最后得出一个简单的特征，将严格泛化界限的存在与算法损失的条件方差联系起来。]]></description>
      <guid>https://arxiv.org/abs/2410.01969</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自调节原始对偶混合梯度法和交替方向乘子法</title>
      <link>https://arxiv.org/abs/2410.01979</link>
      <description><![CDATA[arXiv:2410.01979v1 公告类型：交叉 
摘要：线搜索程序通常用于双线性鞍点问题的原始对偶方法中，尤其是当线性算子的范数很大或难以计算时。在本文中，我们通过引入一种新的原始对偶方法，即自条件原始对偶混合梯度 (AC-PDHG) 方法，证明了线搜索是不必要的，该方法实现了解决双线性鞍点问题的最佳复杂度。AC-PDHG 完全适应线性算子，仅使用过去的迭代来估计其范数。我们进一步定制 AC-PDHG 以解决线性约束问题，为最优间隙和约束违规提供收敛保证。此外，我们探索了一类重要的线性约束问题，其中目标和约束都分解为两部分。通过将 AC-PDHG 的设计原理融入预条件交替方向乘法器 (ADMM)，我们提出了自条件交替方向乘法器 (AC-ADMM)，该方法仅基于约束矩阵的一部分保证收敛并完全适应它，无需进行线搜索。最后，我们扩展了 AC-PDHG 和 AC-ADMM 以解决具有附加平滑项的双线性问题。通过将这些方法与新颖的加速方案相结合，我们在单预言机设置下实现了最佳迭代复杂度。]]></description>
      <guid>https://arxiv.org/abs/2410.01979</guid>
      <pubDate>Fri, 04 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>