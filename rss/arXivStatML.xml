<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>修改分类树的最终拆分，以在政策制定中进行微调亚群目标</title>
      <link>https://arxiv.org/abs/2502.15072</link>
      <description><![CDATA[ARXIV：2502.15072V1公告类型：新 
摘要：决策者经常使用分类和回归树（CART）根据二进制结果和目标亚群来划分种群，这些结果的二进制可能性超过了阈值。但是，经典的购物车和知识蒸馏方法的学生模型是购物车（称为KD-Cart）并不能最大程度地减少与分类这些二进制事件的潜在概率相关的错误分类风险。为了降低错误分类风险，我们提出了两种方法，惩罚最终拆分（PFS）并最大化距离最终拆分（MDFS）。 PFS将可调节的惩罚纳入标准推车拆分标准函数中。 MDFS最大化节点均值和阈值之间的加权总和。它可以指向在唯一相交的潜在概率假设下的最佳分裂。此外，我们为MDFS分裂规则估计而开发理论结果，该估计的渐近风险为零。通过广泛的仿真研究，我们证明了这些方法在错误分类误差方面主要优于经典推车和KD-Cart。此外，在我们的经验评估中，这些方法比两种基线方法提供了更深的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.15072</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨两分之产的产品的变性系统发育推断</title>
      <link>https://arxiv.org/abs/2502.15110</link>
      <description><![CDATA[ARXIV：2502.15110V1公告类型：新 
摘要：贝叶斯系统发育学需要在树上的后验分布进行准确有效的近似。在这项工作中，我们开发了一种用于超级系统发育树的差异贝叶斯方法。我们基于单个连接聚类的合并时间提出了一种新型的变分家族，并得出了在树木上分布的封闭形式密度。与现有的超级树的方法不同，我们的方法对所有树木空间进行了推理，它不需要任何马尔可夫链蒙特卡洛子例程，而且我们的各种家族都是可区分的。通过基准基因组数据集的实验以及对SARS-COV-2的应用，我们证明了我们的方法达到了竞争精度，同时需要比现有的最新技术要少得多的梯度评估。]]></description>
      <guid>https://arxiv.org/abs/2502.15110</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>功能方差分析模型的张量产品神经网络</title>
      <link>https://arxiv.org/abs/2502.15215</link>
      <description><![CDATA[ARXIV：2502.15215V1公告类型：新 
摘要：随着机器学习模型变得越来越复杂，机器学习模型的解释性变得越来越重要。功能性方差分析模型将高维函数分解为所谓的组件的较低维函数总和，是最流行的可解释AI的工具之一，最近，已经开发了各种神经网络模型，以估算每个组件中的每个组件功能方差分析模型。但是，当估计组件时，由于组件本身不是唯一的定义，因此这种神经网络非常不稳定。也就是说，给定函数有多个功能方差分析分解。在本文中，我们提出了一个新颖的可解释模型，该模型可以保证独特的功能方差分析分解，因此能够稳定地估计每个组件。我们称我们提出的模型ANOVA节点是对功能方差分析模型的神经遗漏决策（节点）的修改。从理论上讲，我们证明方差分析节点可以很好地近似光滑的函数。此外，我们在实验上表明，ANOVA节点对每个组件提供了更稳定的估计，因此当训练数据和模型参数的初始值与现有的神经网络模型相比，解释更加稳定。]]></description>
      <guid>https://arxiv.org/abs/2502.15215</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Fr \'echet累积协方差网络，用于深度非线性降低尺寸，随机对象</title>
      <link>https://arxiv.org/abs/2502.15374</link>
      <description><![CDATA[ARXIV：2502.15374V1公告类型：新 
摘要：非线性足够的尺寸降低\ citep {libing_generalsd}，它构建非线性低维表示以总结高维数据的基本特征，是表示学习的重要分支。但是，当响应变量是复杂的非欧盟随机对象时，大多数现有方法不适用，这在许多最近的统计应用中经常遇到。在本文中，我们引入了一种新的统计依赖度量，称为Fr \&#39;Echet累积协方差（FCCOV），并开发了一种基于FCCOV的新型非线性SDR框架。我们的方法不仅适用于复杂的非欧国人数据，而且还适用于对异常值的鲁棒性。我们进一步融合了前馈神经网络（FNN）和卷积神经网络（CNN），以估计样品水平的非线性方向。从理论上讲，我们证明我们的方法是平方的Frobenius Norm正规化在$ \ sigma $ field级别上实现无偏见。此外，我们基于FNN和Resnet型CNN为估计量建立了非反应收敛速率，它们与非参数回归的最小值速率与对数因素相匹配。密集的仿真研究验证了我们在欧几里得和非欧几里得环境中方法的性能。我们将我们的方法应用于面部表达识别数据集，结果强调了我们的建议更现实和更广泛的适用性。]]></description>
      <guid>https://arxiv.org/abs/2502.15374</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Jeffrey的更新规则是Kullback-Leibler Divergence的最小化规则</title>
      <link>https://arxiv.org/abs/2502.15504</link>
      <description><![CDATA[ARXIV：2502.15504V1公告类型：新 
摘要：在本文中，我们显示了由研究人员Bart Jacobs得出的原始定理的原始证明：在学习或更新内部状态的贝叶斯更新规则的背景下在应用Jeffrey的更新规则更新内部状态时，观察值和预测之间的熵会减少。]]></description>
      <guid>https://arxiv.org/abs/2502.15504</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过数据依赖性高斯混合先验的代表性学习的概括保证</title>
      <link>https://arxiv.org/abs/2502.15540</link>
      <description><![CDATA[ARXIV：2502.15540V1公告类型：新 
摘要：我们在表示类型算法的概括误差上建立了不观察和尾巴界限。界限是根据从训练中提取的表示和“测试”数据集提取的表示的分布与数据依赖性对称先验的相对熵的，即训练和训练的潜在变量的最小描述长度（MDL）测试数据集。然后，我们使用我们的意见约束，以设计合适的数据依赖性正常化程序；我们彻底研究了先验选择的重要问题。我们提出了一种系统的方法来同时学习与数据相关的高斯混合物，并将其用作正规器。有趣的是，我们表明，在此过程中，加权注意机制自然出现。我们的实验表明，我们的方法的表现优于现在流行的变分信息瓶颈（VIB）方法以及最近的依赖类别的VIB（CDVIB）。]]></description>
      <guid>https://arxiv.org/abs/2502.15540</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Laplacian内核及其概括的特征地图</title>
      <link>https://arxiv.org/abs/2502.15575</link>
      <description><![CDATA[ARXIV：2502.15575V1公告类型：新 
摘要：内核方法在机器学习中的最新应用对Laplacian内核产生了新的兴趣，这是由于其稳定性与高斯内核相比，其表达性与神经涉及的神经涉及的核的表达性相等深连接的网络。但是，与高斯内核不同，拉普拉斯内核不是可分离的。这给技术带来了近似IT的挑战，尤其是通过随机傅立叶特征（RFF）方法及其变体。在这项工作中，我们为Laplacian内核及其两个概括提供了随机功能：MAT \&#39;{e} rn内核和指数功率内核。我们提供有效实现的方案来采样重量矩阵，以便随机具有近似这些内核。这些重量矩阵具有弱耦合的重尾随机性。通过实际数据集上的数值实验，我们证明了这些随机特征图的功效。]]></description>
      <guid>https://arxiv.org/abs/2502.15575</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>奖励在测试时间扩散模型中的奖励引导的迭代改进，并应用于蛋白质和DNA设计</title>
      <link>https://arxiv.org/abs/2502.14944</link>
      <description><![CDATA[ARXIV：2502.14944V1公告类型：交叉 
摘要：为了充分利用扩散模型的功能，我们通常有兴趣在推理过程中优化下游奖励功能。尽管由于其重要性，最近提出了许多用于奖励引导生成的算法，但当前的方法主要集中在单发子发电上，从充分的噪声转变为deNo的状态。我们提出了一个新型的框架，以进化算法启发的扩散模型进行推理时间奖励优化。我们的方法采用了一个迭代的完善过程，该过程包括每个迭代中的两个步骤：noising和奖励引导的denoising。这种顺序完善允许逐步纠正奖励优化期间引入的错误。此外，我们为我们的框架提供了理论保证。最后，我们证明了其在蛋白质和细胞类型的调节DNA设计中的出色经验性能。该代码可在\ href {https://github.com/masa-ue/prodifevo-refinement} {https://github.com/masa-masa-ue/prodifevo-refinement}中获得。]]></description>
      <guid>https://arxiv.org/abs/2502.14944</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>没有对称因果解释的对称观察</title>
      <link>https://arxiv.org/abs/2502.14950</link>
      <description><![CDATA[ARXIV：2502.14950V1公告类型：交叉 
摘要：从观察到的相关性中推断出因果模型是一项具有挑战性的任务，对许多科学领域至关重要。为了减轻努力，重要的是要知道观测中的对称性是否与基础实现中的对称性相对应。通过一个明确的例子，我们以负面的方式回答这个问题。我们在二进制事件上使用三方概率分布，通过使用三个（不同的）独立的经典随机性来实现，这可以实现。我们证明，即使删除源分布经典物理描述的系统的条件，i）源的要求i）源分布相同的物理系统，ii）这些物理系统尊重相对论因果关系，iii）相关性是所观察到的，是一个相关性的，不相容。]]></description>
      <guid>https://arxiv.org/abs/2502.14950</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>部分观察到的功能数据的联合注册和共形预测</title>
      <link>https://arxiv.org/abs/2502.15000</link>
      <description><![CDATA[ARXIV：2502.15000V1公告类型：交叉 
摘要：预测部分观察到的函数中缺失段，这是由于无限二数，在观察值内外和噪声不规则的依赖性以及不规则的噪声而挑战。存在两个不同的功能数据变化来源，称为振幅（沿$ y $  - 轴的变化）和相位（沿$ x $ -Axis的变化），进一步加剧了这些挑战。虽然注册可以使它们从完整的功能数据中解散，但对于部分观察，该过程更加困难。因此，现有的功能数据预测方法通常会忽略相位变化。此外，它们依赖于强有力的参数假设，并且需要精确的模型规格或计算密集型技术（例如自举）来构建预测间隔。为了解决这个问题，我们提出了在保形预测框架下的部分观察功能的统一注册和预测方法，该框架分别着重于振幅和相位成分。通过利用分裂的保形方法，我们的方法可以通过精心构造的预测响应对来确保交换性。使用邻域平滑算法，该框架在弱假设下会产生具有有限样本边际覆盖的有限样本边缘覆盖范围的侧面预测带。该方法易于实现，计算上有效，适合并行化。数值研究和现实世界数据示例清楚地证明了该方法的有效性和实用性。]]></description>
      <guid>https://arxiv.org/abs/2502.15000</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>加速神经网络培训：对algoperf竞争的分析</title>
      <link>https://arxiv.org/abs/2502.15015</link>
      <description><![CDATA[ARXIV：2502.15015V1公告类型：交叉 
摘要：算法的目标：培训算法竞赛是评估仅通过改善基础培训算法实现的神经网络培训的实际加速度。在外部调整规则集中，提交必须提供工作负载不合时宜的超参数搜索空间，而在自我调整规则集中，它们必须完全不含超级参数。在这两个规则集中，在多个深度学习工作负载的时间表上进行了比较，对固定硬件进行了培训。本文介绍了首届Algoperf竞赛的成果，该竞赛的结果吸引了10个团队的18种不同的意见。我们的调查揭示了几个关键发现：（1）使用分布式洗发水在外部调整规则集中的获胜提交，即使在墙壁通行时间运行时进行了比较，也证明了非违规预处理对亚当等流行方法的有效性。 （2）根据时间表免费ADAMW算法，自我调整规则集中的获胜提交为完全无效的培训算法展示了新的有效性。 （3）最高得分的提交对工作负载的变化令人惊讶。我们还讨论了在确保不同培训算法之间进行公平比较时遇到的工程挑战。这些结果突出了迄今为止的重大进展，也凸显了进一步改进的余地。]]></description>
      <guid>https://arxiv.org/abs/2502.15015</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>估计有限输入和特征的神经表示对准</title>
      <link>https://arxiv.org/abs/2502.15104</link>
      <description><![CDATA[ARXIV：2502.15104V1公告类型：交叉 
摘要：在人工和生物系统中，中心内核比对（CKA）已成为一种广泛使用的工具，用于量化神经表示相似性。尽管当前的CKA估计器通常对有限刺激采样的影响纠正，但采样神经元的效果被忽略了，在标准实验场景中引入了显着的偏见。在这里，我们提供了一个理论分析，表明这种偏见如何受到表示几何形状的影响。然后，我们引入了一个新颖的估计器，该估计量均可纠正输入和特征采样。我们使用我们的方法来评估大脑对脑和模型对脑对齐的比对，并表明即使与非常稀少的采样神经元，它也可以提供可靠的比较。我们对来自视觉皮质区域V1，V4和IT数据的电生理数据的动物内和跨动物比较进行了比较，并将其用作基准来评估模型到脑的比对。我们还采用我们的方法来揭示生物和人工系统中对象表示如何逐渐跨越层次的对象表示。这些发现强调了CKA中纠正特征采样偏见的重要性，并证明了我们的偏见校正估计器提供了更忠实的表示对齐方式的衡量标准。改进的估计值增加了我们对神经活动如何在生物系统和人工系统中结构的理解。]]></description>
      <guid>https://arxiv.org/abs/2502.15104</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我们真的需要Rademacher的复杂性吗？</title>
      <link>https://arxiv.org/abs/2502.15118</link>
      <description><![CDATA[ARXIV：2502.15118V1公告类型：交叉 
摘要：我们研究了关于凸阶段平方损失的学习基本问题。在这种情况下，最新的样本复杂性估计取决于通常难以控制的Rademacher复杂性。我们证明，与普遍的信念和在最少的假设下相反，样本复杂性不受Rademacher复杂性的控制，而是受限制高斯过程的行为的控制。特别是，所有具有相同$ L_2 $结构的学习问题（即使是重型分布的人）都具有相同的样本复杂性。这构成了一般凸学习问题的第一个普遍性结果。
  该证明是基于一种新颖的学习程序，并且通过将实现的随机变量与Talagrand的通用链式方法相结合的最佳平均估计技术来研究其性能。]]></description>
      <guid>https://arxiv.org/abs/2502.15118</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高维二元分类中的最佳且可证明的校准：角度校准和PLATT缩放</title>
      <link>https://arxiv.org/abs/2502.15131</link>
      <description><![CDATA[Arxiv：2502.15131V1公告类型：交叉 
摘要：我们研究了$ \ sigma的线性二进制分类器的基本问题，$ \ hat {w} $是真线性权重$ w^\ star $的估计器。通过使用非信息\ textit {Chance Classifier} $插值，我们构造了一个拟定的预测指标，其插值重量取决于angle $ \ Angle（\ hat {w}，w_ \ star），估算器$ \ hat之间的$ w} $和真正的线性权重$ w_ \ star $。我们确定这种角度校准方法在高维状态下得到了很好的校准，在高维状态下，样品的数量和以相当的速度具有差异。可以始终估算角度$ \ angle（\ hat {w}，w_ \ star）$。此外，所得的预测变量是唯一的$ \ textit {bregman-optimal} $，将布雷格曼（Bregman）与合适的校准预测变量类别中的bregman差异最小化。我们的工作是第一个提供校准策略，该策略既满足校准和最佳属性，又可以证明在高维度中。此外，我们确定了经典的PLATT缩放预测器在Bregman-Pratimal校准溶液中收敛的条件。因此，Platt缩放还以高维度继承了这些理想的特性。]]></description>
      <guid>https://arxiv.org/abs/2502.15131</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过操纵错误方法，高维线性回归中无维度的界限</title>
      <link>https://arxiv.org/abs/2502.15437</link>
      <description><![CDATA[ARXIV：2502.15437V1公告类型：交叉 
摘要：我们考虑使用随机设计的高维线性回归问题。我们建议一种新的方法称为操作员错误，该方法不会直接估算设计协方差$ \ sigma $，而是将其纳入经验风险最小化中。我们提供了超额预测风险的扩展，并在领先期限和其余部分中得出了无质尺寸的界限。这有助于我们证明辅助变量不会增加问题的有效维度，前提是程序的参数得到了正确的调整。我们还讨论了方法的计算方面，并通过数值实验说明了其性能。]]></description>
      <guid>https://arxiv.org/abs/2502.15437</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>