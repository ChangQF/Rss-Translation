<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 04 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>前瞻性学习：为充满活力的未来而学习</title>
      <link>https://arxiv.org/abs/2411.00109</link>
      <description><![CDATA[arXiv:2411.00109v1 公告类型：新
摘要：在实际应用中，数据的分布和我们的目标会随着时间的推移而发展。研究机器学习的主流理论框架，即可能近似正确 (PAC) 学习，在很大程度上忽略了时间。因此，现有的解决数据和目标动态性质的策略在现实世界中表现不佳。本文开发了一种称为“前瞻性学习”的理论框架，该框架专门用于最佳假设随时间变化的情况。在 PAC 学习中，经验风险最小化 (ERM) 是一致的。我们开发了一个名为 Prospective ERM 的学习器，它返回一系列对未来数据进行预测的预测因子。我们证明，在对生成数据的随机过程的某些假设下，前瞻性 ERM 的风险会收敛到贝叶斯风险。粗略地说，前瞻性 ERM 除了数据外，还结合了时间作为输入。我们表明，在 PAC 学习中采用的标准 ERM 如果不考虑时间因素，则会导致在分布动态时无法学习。数值实验表明，前瞻性 ERM 可以学习由 MNIST 和 CIFAR-10 构建的合成和视觉识别问题。]]></description>
      <guid>https://arxiv.org/abs/2411.00109</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>理解生成模型中的记忆的几何框架</title>
      <link>https://arxiv.org/abs/2411.00113</link>
      <description><![CDATA[arXiv:2411.00113v1 公告类型：新
摘要：随着深度生成模型的进步，最近的研究表明，它们在部署时能够记忆和重现训练数据点。这些发现对生成模型的可用性提出了质疑，尤其是考虑到记忆带来的法律和隐私风险。为了更好地理解这一现象，我们提出了流形记忆假设 (MMH)，这是一个几何框架，它利用流形假设转化为一种清晰的语言来推理记忆。我们建议根据 $(i)$ 地面真实数据流形和 $(ii)$ 模型学习的流形的维度之间的关系来分析记忆。该框架为数据点的“记忆方式”提供了一个正式标准，并系统地将记忆数据分为两类：由过度拟合驱动的记忆和由底层数据分布驱动的记忆。通过分析 MMH 背景下的先前研究，我们解释并统一了文献中的各种观察结果。我们使用合成数据和图像数据集对 MMH 进行了实证验证，直至稳定扩散的规模，并开发了用于检测和防止在此过程中生成记忆样本的新工具。]]></description>
      <guid>https://arxiv.org/abs/2411.00113</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>流形上的残差深度高斯过程</title>
      <link>https://arxiv.org/abs/2411.00161</link>
      <description><![CDATA[arXiv:2411.00161v1 公告类型：新
摘要：我们提出了黎曼流形上的实用深度高斯过程模型，其精神类似于残差神经网络。通过流形到流形的隐藏层和任意最后一层，它们可以对流形和标量值函数以及矢量场进行建模。我们的目标是流形上固有支持的数据，这对于其上的浅层高斯过程来说太复杂了。例如，虽然后者在高海拔风数据上表现良好，但它们在低海拔地区处理更复杂、非平稳模式时却举步维艰。我们的模型显著提高了这些设置下的性能，提高了预测质量和不确定性校准，并且保持了对过度拟合的鲁棒性，当不需要额外的复杂性时会恢复到浅层模型。我们进一步展示了我们在流形上贝叶斯优化问题的模型，使用机器人技术启发的程式化示例，并在优化过程的后期阶段取得了实质性的改进。最后，我们证明了我们的模型具有加速非流形数据推理的潜力，前提是它可以很好地映射到代理流形。]]></description>
      <guid>https://arxiv.org/abs/2411.00161</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习未知因果干预的混合</title>
      <link>https://arxiv.org/abs/2411.00213</link>
      <description><![CDATA[arXiv:2411.00213v1 公告类型：新
摘要：进行干预的能力在学习变量之间的因果关系中起着关键作用，从而促进了基因组学、经济学和机器学习等不同科学学科的应用。然而，在这些应用的许多情况下，生成干预数据的过程容易受到噪声的影响：干预产生的数据不是直接从预期的干预分布中采样，而是经常从预期和非预期干预分布的混合中采样。
我们考虑了在不了解真实因果图的情况下，在具有高斯加性噪声的线性结构方程模型 (SEM) 中解开混合干预和观察数据的基本挑战。我们证明，无论是进行干预还是软干预，都会产生具有足够多样性和有利于有效恢复混合物中每个成分的特性的分布。此外，我们确定，解开混合数据所需的样本复杂性与干预对控制受影响变量值的方程式所引起的变化程度成反比。因此，可以识别因果图直至其干预马尔可夫等价类，类似于没有噪声影响干预数据生成的场景。我们通过进行模拟来进一步支持我们的理论发现，在模拟中我们从此类混合数据中进行因果发现。]]></description>
      <guid>https://arxiv.org/abs/2411.00213</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>包容性 KL 最小化：Wasserstein-Fisher-Rao 梯度流视角</title>
      <link>https://arxiv.org/abs/2411.00214</link>
      <description><![CDATA[arXiv:2411.00214v1 公告类型：新 
摘要：Otto (2001) 的排他性 KL 散度函数 Wasserstein 梯度流为分析学习和推理算法提供了强大且符合数学原理的视角。相比之下，对于包容性 KL 推理的算法，即最小化某个目标 $ \pi $ 的 $ \mathrm{KL}(\pi \| \mu) $ 相对于 $ \mu $，很少使用数学分析工具进行分析。本文表明，可以使用从 PDE 分析中得出的梯度流理论构建通用的近似包容性 KL 推理范式。我们发现，几种现有的学习算法可以看作是包容性 KL 推理范式的特定实现。例如，现有的采样算法，如 Arbel 等人 (2019) 和 Korba 等人（2021）可以统一视为具有近似梯度估计量的包容性 KL 推理。最后，我们为最小化包容性 KL 散度的 Wasserstein-Fisher-Rao 梯度流提供了理论基础。]]></description>
      <guid>https://arxiv.org/abs/2411.00214</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>亚高斯线性老虎机的最小经验散度</title>
      <link>https://arxiv.org/abs/2411.00229</link>
      <description><![CDATA[arXiv:2411.00229v1 公告类型：新
摘要：我们提出了一种称为 LinMED（线性最小经验散度）的新型线性老虎机算法，它是最初为多臂老虎机设计的 MED 算法的线性扩展。LinMED 是一种随机算法，它允许对臂采样概率进行闭式计算，这与流行的随机算法线性汤普森采样不同。这种特性对于离策略评估非常有用，因为无偏评估需要准确计算采样概率。我们证明 LinMED 享有接近最优的遗憾界限 $d\sqrt{n}$，最高可达对数因子，其中 $d$ 是维度，$n$ 是时间范围。我们进一步表明，LinMED 具有 $\frac{d^2}{\Delta}\left(\log^2(n)\right)\log\left(\log(n)\right)$ 问题相关遗憾，其中 $\Delta$ 是最小次优差距，低于标准算法 OFUL (Abbasi-yadkori et al., 2011) 的 $\frac{d^2}{\Delta}\log^3(n)$。我们的实证研究表明，LinMED 的性能与最先进的算法相媲美。]]></description>
      <guid>https://arxiv.org/abs/2411.00229</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我们需要多少个分类器？</title>
      <link>https://arxiv.org/abs/2411.00328</link>
      <description><![CDATA[arXiv:2411.00328v1 公告类型：新
摘要：随着通过扩展数据和/或模型大小获得的性能收益不断递减，转向集成变得越来越流行，即将多个模型的预测组合在一起以提高准确性。在本文中，我们详细分析了分类器之间的分歧和极化（我们在本文中引入和定义的概念）与通过聚合单个分类器实现的性能提升之间的关系，适用于分类任务中的多数投票策略。我们通过以下方式解决这些问题。（1）推导出极化的上限，并提出了所谓的神经极化定律：大多数插值神经网络模型都是 4/3 极化的。我们的实证结果不仅支持这一猜想，而且还表明，无论分类器的超参数或架构如何，数据集的极化几乎是恒定的。 (2) 在限制熵条件下考虑多数表决分类器的误差，我们提出了一个严格的上限，表明分歧与目标呈线性相关，并且斜率在极化中呈线性。 (3) 我们证明了分歧在分类器数量方面的渐近行为的结果，我们表明这有助于预测大量分类器的性能与较少数量分类器的性能。我们的理论和主张得到了使用各种类型的神经网络的几个图像分类任务的实证结果的支持。]]></description>
      <guid>https://arxiv.org/abs/2411.00328</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对总奖励、最大奖励等的老虎机问题上限置信界限策略的统一理论</title>
      <link>https://arxiv.org/abs/2411.00339</link>
      <description><![CDATA[arXiv:2411.00339v1 公告类型：新
摘要：上置信界 (UCB) 策略被认为是经典总奖励老虎机问题的顺序最优解决方案。虽然类似的基于 UCB 的方法已应用于最大老虎机问题，旨在最大化累积最大奖励，但它们的顺序最优性仍不清楚。在本研究中，我们阐明了 UCB 策略在总奖励和最大老虎机问题中实现顺序最优性的统一条件。我们理论的一个关键概念是预言量，它通过其最高值来识别最佳臂。这允许将 UCB 策略统一定义为拉动预言量中 UCB 最高的臂。此外，在这种设置下，可以通过用失败次数作为核心指标取代传统的遗憾来进行最优性分析。我们分析的一个结果是，随着试验的增加，预言量的置信区间必须适当缩小，以确保 UCB 策略的顺序最优性。从这一结果中，我们证明了先前提出的 MaxSearch 算法满足这一条件，并且是最大老虎机问题的阶数最优策略。我们还证明了，通过提供适当的预言机数量及其置信区间，可以系统地推导出新的老虎机问题及其阶数最优的 UCB 算法。在此基础上，我们提出了 PIUCB 算法，旨在拉动具有最高改进概率 (PI) 的臂。这些算法可以应用于实践中的最大老虎机问题，并且在小示例中的表现与 MaxSearch 算法相当或更好。这表明我们的理论有可能生成针对特定预言机数量的新策略。]]></description>
      <guid>https://arxiv.org/abs/2411.00339</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HAVER：最大均值估计的实例相关误差界限及其在 Q 学习中的应用</title>
      <link>https://arxiv.org/abs/2411.00405</link>
      <description><![CDATA[arXiv:2411.00405v1 公告类型：新
摘要：我们研究通过从 $K$ 个分布中抽取样本来估计其中最大均值的 \emph{值} 的问题（而不是估计 \emph{哪个} 分布具有最大均值），该问题来自各种机器学习任务，包括 Q 学习和蒙特卡洛树搜索。虽然已经提出了一些算法，但它们的性能分析仅限于它们的偏差，而不是精确的误差度量。在本文中，我们提出了一种称为 HAVER（Head AVERaging）的新算法，并分析了其均方误差。我们的分析表明，HAVER 在两个方面具有令人信服的性能。首先，HAVER 估计最大均值以及知道最佳分布身份并报告其样本均值的预言机。其次，也许令人惊讶的是，当最佳分布附近有许多分布时，HAVER 表现出比这个预言机更好的速率。这两项改进都是文献中同类改进的首例，我们还证明了报告最大经验平均值的简单算法未达到这些界限。最后，我们通过包括老虎机和 Q 学习场景的数值实验证实了我们的理论发现，其中 HAVER 优于基线方法。]]></description>
      <guid>https://arxiv.org/abs/2411.00405</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>采用原始对偶朗之万蒙特卡罗进行约束采样</title>
      <link>https://arxiv.org/abs/2411.00568</link>
      <description><![CDATA[arXiv:2411.00568v1 公告类型：新
摘要：这项工作考虑了从已知的概率分布中抽样的问题，该概率分布最多为一个归一化常数，同时满足由一般非线性函数的期望值指定的一组统计约束。该问题在贝叶斯推理等应用中得到应用，它可以约束矩以评估反事实场景或强制执行诸如预测公平性之类的要求。为处理支持约束而开发的方法（例如基于镜像映射、障碍和惩罚的方法）不适合这项任务。因此，这项工作依赖于 Wasserstein 空间中的梯度下降-上升动力学来提出一种离散时间原始对偶朗之万蒙特卡罗算法 (PD-LMC)，该算法同时约束目标分布并从中抽样。我们分析了在目标分布和约束的标准假设（即（强）凸性和对数 Sobolev 不等式）下 PD-LMC 的收敛性。为此，我们将鞍点算法的经典优化论证引入 Wasserstein 空间的几何中。我们说明了 PD-LMC 在多个应用中的相关性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.00568</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过负依赖的小核心集：DPP、线性统计和浓度</title>
      <link>https://arxiv.org/abs/2411.00611</link>
      <description><![CDATA[arXiv:2411.00611v1 公告类型：新
摘要：行列式点过程 (DPP) 是具有可调负依赖性的点的随机配置。由于采样易于处理，DPP 是子采样任务的自然候选者，例如小批量选择或核心集构建。\emph{核心集} 是 (大型) 训练集的子集，因此最小化核心集上的平均经验损失是对难以最小化的原始经验损失的可控替代。通常，控制采取保证核心集上的平均损失在整个参数空间中均匀近似于总损失的形式。最近的研究为使用 DPP 构建随机核心集提供了重要的经验支持，并结合了有趣的理论结果，这些结果具有启发性，但留下了一些关键问题未得到解答。具体来说，基于 DPP 的核心集的基数是否从根本上小于基于独立采样的核心集这一核心问题仍未得到解决。在本文中，我们对这个问题给出了肯定的回答，表明 \emph{DPP 可以证明其优于独立绘制的核心集}。在这方面，我们贡献了对核心集损失的概念理解，即（随机）核心集的 \emph{线性统计}。我们利用这种结构观察将核心集问题与 DPP 线性统计的更一般的集中现象问题联系起来，其中我们获得了 \emph{远远超出最先进水平的有效集中不等式}，涵盖了一般的非投影甚至非对称核。后者最近被证明在核心集以外的机器学习中很受关注，但其理论工具箱有限，我们的结果有助于扩展这一工具箱。最后，我们还能够解决向量值目标函数的核心集问题，这是核心集文献中的新颖之处。]]></description>
      <guid>https://arxiv.org/abs/2411.00611</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 RKHS 对 Hawkes 过程进行非参数估计</title>
      <link>https://arxiv.org/abs/2411.00621</link>
      <description><![CDATA[arXiv:2411.00621v1 公告类型：新
摘要：本文讨论了非线性多元霍克斯过程的非参数估计，其中假设交互函数位于再生核希尔伯特空间 (RKHS) 中。受神经科学应用的启发，该模型允许复杂的交互函数，以表达兴奋和抑制效应，也可以表达两者的结合（这对于模拟神经元的不应期特别有趣），并反过来认为条件强度由 ReLU 函数纠正。后一个特性带来了一些方法上的挑战，本文提出了一些解决方法。特别是，结果表明，对于对数似然和最小二乘标准的近似版本，可以获得表示定理。在此基础上，我们提出了一种估计方法，该方法依赖于两个简单的近似值（ReLU 函数和积分算子）。我们提供了一个近似值界限，证明了这些近似值的统计效应可以忽略不计。对合成数据的数值结果证实了这一事实以及所提出的估计量的良好渐近行为。它还表明，与相关的非参数估计技术相比，我们的方法实现了更好的性能，并且适合神经元应用。]]></description>
      <guid>https://arxiv.org/abs/2411.00621</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一些核矩阵的快速谱估计</title>
      <link>https://arxiv.org/abs/2411.00657</link>
      <description><![CDATA[arXiv:2411.00657v1 公告类型：新
摘要：在数据科学中，通常假设单个观察独立于底层概率空间。由大量此类观察形成的核矩阵经常出现，例如在分类任务期间。最好在不明确形成这些矩阵的情况下了解其特征值衰减特性，例如在确定低秩近似是否可行时。在这项工作中，我们为一些核矩阵引入了一种新的特征值分位数估计框架。该框架为核矩阵的所有特征值提供了有意义的界限，同时避免了构建完整矩阵的成本。所考虑的核矩阵来自一个核，该核从对角线快速衰减，应用于任意维度的欧几里得空间中均匀分布的点集。我们在给定核函数的某些界限的情况下证明了该框架的有效性，并为其准确性提供了经验证据。在此过程中，我们还证明了有限数集的一个非常通用的交错型定理。此外，我们还指出了此框架在数据本征维数研究中的应用，以及推广这项工作的其他几个方向。]]></description>
      <guid>https://arxiv.org/abs/2411.00657</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过非对称自我博弈实现对齐进化</title>
      <link>https://arxiv.org/abs/2411.00062</link>
      <description><![CDATA[arXiv:2411.00062v1 公告类型：交叉 
摘要：当前用于对齐大型语言模型 (LLM) 的 RLHF 框架通常假设固定的提示分布，这不是最优的，并且限制了对齐的可扩展性和模型的通用性。为了解决这个问题，我们引入了一个通用的开放式 RLHF 框架，将对齐视为两个玩家之间的不对称游戏：(i) 使用奖励模型生成越来越有用的提示分布的创建者，以及 (ii) 学习对创建者生成的提示产生更多偏好响应的求解器。通过非对称自我游戏 (eva) 进化对齐的这种框架产生了一种简单而有效的方法，可以利用任何现有的 RLHF 算法进行可扩展对齐。eva 在广泛使用的基准上优于最先进的方法，而无需任何额外的人工制作提示。具体来说，eva 将 Gemma-2-9B-it 在 Arena-Hard 上的胜率从 51.6% 提高到 60.1%（使用 DPO），从 55.7% 提高到 58.9%（使用 SPPO），从 52.3% 提高到 60.7%（使用 SimPO），从 54.8% 提高到 60.3%（使用 ORPO），超越了 27B 版本并与 claude-3-opus 相匹配。即使引入新的人工制作的提示，这种改进仍然持续存在。最后，我们展示了 eva 在各种消融设置下都是有效且稳健的。]]></description>
      <guid>https://arxiv.org/abs/2411.00062</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$\boldsymbol{\mu}\mathbf{P^2}$：有效的清晰度感知最小化需要逐层扰动缩放</title>
      <link>https://arxiv.org/abs/2411.00075</link>
      <description><![CDATA[arXiv:2411.00075v1 公告类型：交叉 
摘要：清晰度感知最小化 (SAM) 可增强各种神经架构和数据集的性能。随着模型不断扩大以提高性能，严格理解 SAM 的缩放行为至关重要。为此，我们使用张量程序框架研究使用 SAM 训练的神经网络的无限宽度极限。我们的研究结果表明，即使使用最佳超参数，标准 SAM 的动态也可以有效地减少到仅在宽神经网络的最后一层应用 SAM。相反，我们通过逐层扰动缩放确定了一种稳定的参数化，我们称之为 $\textit{最大更新和扰动参数化}$ ($\mu$P$^2$)，以确保所有层都是特征学习并在极限中有效扰动。通过对 MLP、ResNets 和 Vision Transformers 的实验，我们通过经验证明了 $\mu$P$^2$ 是实现跨模型尺度的学习率和扰动半径联合最优值的超参数迁移的第一个参数化。此外，我们提供了一个直观的条件来推导其他扰动规则（如自适应 SAM 和 SAM-ON）的 $\mu$P$^2$，同时确保所有层的扰动效果均衡。]]></description>
      <guid>https://arxiv.org/abs/2411.00075</guid>
      <pubDate>Mon, 04 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>