<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Thu, 10 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>深度时空点过程：进步和新方向</title>
      <link>https://arxiv.org/abs/2504.06364</link>
      <description><![CDATA[ARXIV：2504.06364V1公告类型：新 
摘要：时空点过程（STPP）模型分散在时空中分布的离散事件，在犯罪学，地震学，流行病学和社交网络等领域具有重要的应用。传统模型通常依赖于参数内核，从而限制了它们捕获异质，非组织动态的能力。最近的创新可以通过直接对条件强度函数进行建模或通过学习灵活的，数据驱动的影响内核来整合深度神经体系结构，从而实质上扩大了其表现力。本文回顾了深层影响内核方法的发展，该方法具有统计解释性，因为核心在捕获事件影响的时空传播及其对未来事件的影响的模型中仍然存在，同时也具有强大的表现力，从而从两者中受益。我们解释了开发深内核点过程的主要组成部分，利用功能基础分解和图形神经网络等工具来编码复杂的空间或网络结构，以及使用基于可能性的基于可能性和可能的​​无可能方法的方法进行估算，以及解决大规模数据的计算可扩展性。我们还讨论了内核可识别性的理论基础。模拟和真实数据示例强调了犯罪分析，地震余震预测和败血症预测建模的应用，我们通过讨论该领域的有希望的方向来得出结论。]]></description>
      <guid>https://arxiv.org/abs/2504.06364</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度公平学习：具有足够网络的微调表示的统一框架</title>
      <link>https://arxiv.org/abs/2504.06470</link>
      <description><![CDATA[ARXIV：2504.06470V1公告类型：新 
摘要：确保机器学习中的公平性是一项至关重要且具有挑战性的任务，因为偏见的数据表示通常会导致不公平的预测。为了解决这个问题，我们提出了深入的公平学习，该框架将非线性足够的尺寸缩小与深度学习结合起来，以构建公平和信息丰富的表示形式。通过在微调期间引入新的罚款项，我们的方法可以在敏感属性和学习的表示之间实现条件独立性，以解决其来源的偏见，同时保留预测性能。与先前的方法不同，它支持各种敏感属性，包括连续，离散，二进制或多组类型。对各种数据结构的实验表明，我们的方法在公平和效用之间取得了卓越的平衡，大大优于最先进的基线。]]></description>
      <guid>https://arxiv.org/abs/2504.06470</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稀疏学习的本地固定过程</title>
      <link>https://arxiv.org/abs/2504.06477</link>
      <description><![CDATA[ARXIV：2504.06477V1公告类型：新 
摘要：在许多机器学习任务中，稀疏学习无处不在。它旨在通过添加一个惩罚术语来正规化目标函数，以考虑对学习参数的约束。本文考虑了学习重尾LSP的问题。我们开发了一个灵活而坚固的稀疏学习框架，能够处理具有本地固定行为的重尾数据并提出集中度不平等。我们进一步为不同类型的稀疏性提供了非反应的甲骨文不平等现象，包括$ \ ell_1 $ norm和最小平方损失的总变化惩罚。]]></description>
      <guid>https://arxiv.org/abs/2504.06477</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stealthrank：LLM通过隐形及时优化进行排名</title>
      <link>https://arxiv.org/abs/2504.05804</link>
      <description><![CDATA[ARXIV：2504.05804V1公告类型：交叉 
摘要：大型语言模型（LLM）集成到信息检索系统中引入了新的攻击表面，尤其是对于对抗性排名操作。我们提出Stealthrank，这是一种新颖的对抗排名攻击，可以操纵LLM驱动的产品推荐系统，同时保持文本流利度和隐身。与经常引入可检测异常的现有方法不同，StealthRank采用了基于能量的优化框架与Langevin Dynamics结合使用，以生成嵌入了嵌入产品描述中却有有效却有效影响LLM排名机制的产品描述中的stealthrank提示（SRP） - 对话文本序列。我们评估了跨多个LLM的Stealthrank，这证明了其秘密提高目标产品排名的能力，同时避免了可以轻松检测到的明确操纵痕迹。我们的结果表明，StealthRank始终在有效性和隐身方面都优于最先进的对抗排名基线，从而突出了LLM驱动的推荐系统中的关键脆弱性。]]></description>
      <guid>https://arxiv.org/abs/2504.05804</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>由大都会调整的兰格文算法，用于对Jeffreys进行抽样</title>
      <link>https://arxiv.org/abs/2504.06372</link>
      <description><![CDATA[ARXIV：2504.06372V1公告类型：交叉 
摘要：推论和估计是统计，系统识别和机器学习的基本方面。对于大多数推理问题，可以在系统上可以进行建模知识，而贝叶斯分析是一种自然框架，以先验分布的形式强加此类信息。但是，在许多情况下，提出完全指定的先前分布并不容易，因为先前的知识可能太模糊了，因此，从业者则希望使用尽可能“无知的”或“无知”的先前分布，而在不施加主观信念的意义上，同时仍然支持可靠的统计分析。 Jeffreys Prior是一个吸引人的非信息先验，因为它提供了两个重要的好处：（i）在模型的任何重新参数下它是不变的，（ii）它通过Fisher Information Matrix编码参数空间的固有几何结构，这反过来又增强了参数示例的多样性。尽管有这些好处，但从Jeffreys Prior中绘制样本是一项具有挑战性的任务。在本文中，我们使用大都市调整后的langevin算法提出了一种一般抽样方案，该方案可以从Jeffreys先验中对参数值进行采样，并通过多个示例提供我们方法的数值插图。]]></description>
      <guid>https://arxiv.org/abs/2504.06372</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Wasserstein距离的界限局部固定功能时间序列</title>
      <link>https://arxiv.org/abs/2504.06453</link>
      <description><![CDATA[ARXIV：2504.06453V1公告类型：交叉 
摘要：功能时间序列（FTS）扩展了传统方法，以适应观察到的功能/曲线的数据。 FTS中的一个重大挑战包括准确捕获时间依赖性结构，尤其是在时间变化的协变量的情况下。在用时间变化的统计属性分析时间序列时，局部固定时间序列（LST）提供了一个强大的框架，可以随着时间的推移平均变化和方差平稳。这项工作调查了Nadaraya-Watson（NW）的估计程序，用于局部固定功能时间序列（LSFTS）的条件分布，该过程中的协变量驻留在半度质量的半度空间中。在小球概率和混合条件下，我们建立了与Wasserstein距离相对于LSFT的NW估计量的收敛速率。模型的有限样本性能和估计方法通过在功能模拟和真实数据上进行的广泛数值实验来说明。]]></description>
      <guid>https://arxiv.org/abs/2504.06453</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>免费的随机投影，用于内在强化学习</title>
      <link>https://arxiv.org/abs/2504.06983</link>
      <description><![CDATA[arxiv：2504.06983v1公告类型：交叉 
摘要：假设分层归纳偏见以促进强化学习中的可推广政策，如明确的双曲线潜在表示和体系结构所证明的那样。因此，一种更灵活的方法是使这些偏见从算法中自然出现。我们引入了自由的随机投影，这是一个基于自由概率理论的输入映射，该映射构建了层次结构的随机正交矩阵。免费的随机投影通过在输入空间内编码层次组织而无需明确的体系结构修改，将无缝集成到现有的内部文化增强学习框架中。多环境基准的经验结果表明，自由随机投影始终优于标准随机投影，从而改善了概括。此外，在线性解决的马尔可夫决策过程中进行的分析和对内核随机矩阵谱的研究揭示了自由随机投影的理论基础，即自由随机投影的增强性能增强了性能，突出了其在层次结构结构的状态空间中有效适应的能力。]]></description>
      <guid>https://arxiv.org/abs/2504.06983</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机数据集的不同批处理分解</title>
      <link>https://arxiv.org/abs/2504.06991</link>
      <description><![CDATA[ARXIV：2504.06991V1公告类型：交叉 
摘要：为了更好地学习，大型数据集通常分为小批次，并依次馈送到预测模型。在本文中，我们从概率的角度研究了此类分解。我们假设数据点（可能损坏）是独立于给定空间绘制的，并定义了两个数据点之间相似性的概念。然后，我们考虑限制每个批次内相似性量并获得最小尺寸的高概率边界的分解。我们证明了放宽相似性约束和整体规模之间的固有权衡，还使用Martingale方法来获得具有给定相似性的数据子集的最大尺寸的界限。]]></description>
      <guid>https://arxiv.org/abs/2504.06991</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>硬件噪声表征的无假设保真度范围</title>
      <link>https://arxiv.org/abs/2504.07010</link>
      <description><![CDATA[ARXIV：2504.07010V1公告类型：交叉 
摘要：在量子至上制度中，如果我们可以估计，减轻或纠正不可避免的硬件噪声，则量子计算机可以在多个任务上克服经典机器。估计误差需要经典的模拟，这在量子至高无上的制度中变得不可行。我们利用机器学习数据驱动的方法和共形预测，这是一种以其轻度假设和有限样本有效性而闻名的机器学习不确定性量化工具，以找到量子设备噪声和嘈杂输出之间的忠诚度的理论上有效的上限。在合理的外推假设下，所提出的方案适用于任何量子计算硬件，不需要对设备的噪声源进行建模，并且在不可用的经典模拟时可以使用，例如在量子至上制度中。]]></description>
      <guid>https://arxiv.org/abs/2504.07010</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>雕刻子空间：llms中的全面微调限制用于持续学习</title>
      <link>https://arxiv.org/abs/2504.07097</link>
      <description><![CDATA[ARXIV：2504.07097V1公告类型：交叉 
摘要：大语言模型（LLMS）的持续学习容易遭受灾难性的遗忘，在这些遗忘中，适应新任务会大大降低以前学到的任务的绩效。现有方法通常依赖于限制模型表现力并每个任务引入其他参数的低级参数效率更新，从而导致可扩展性问题。为了解决这些局限性，我们提出了一种新型的持续全面微调方法，利用适应性奇异价值分解（SVD）。我们的方法动态地识别特定任务的低级别参数子空间，并将更新与与先前任务相关的关键方向进行正交，从而有效地最大程度地减少干扰而没有其他参数开销或存储先前的任务梯度。我们使用编码器编码器（T5-Large）和仅解码器（Llama-2 7b）模型对标准持续学习基准进行了广泛评估我们的方法，涵盖了各种任务，包括分类，发电和推理。从经验上讲，我们的方法可实现最先进的结果，比最近的基线（如O-Lora）高出7％的平均准确性，并且在整个持续学习过程中，通过降低忘记近乎不可识别的水平，尤其是该模型的一般语言能力，指导遵循的准确性和安全性。我们的自适应SVD框架有效地平衡了模型的可塑性和知识保留，从而为大型语言模型中的连续学习场景提供了实用的，理论上的基础和计算可扩展的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2504.07097</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从连续词典中学习混合物的网格学习</title>
      <link>https://arxiv.org/abs/2207.00171</link>
      <description><![CDATA[Arxiv：2207.00171V2公告类型：替换 
摘要：我们考虑一个通用的非线性模型，其中信号是未知（可能增加的，可能增加的特征数量）的有限混合物，该特征是由由真实的非线性参数参数的连续字典发出的。在连续设置或离散设置中使用高斯（可能相关）噪声观察信号。我们提出了一种网格优化方法，即一种未在参数空间上使用任何离散化方案的方法来估算特征的非线性参数和混合物的线性参数。我们使用有关离网方法的几何形状的最新结果，在真实的潜在非线性参数上给出最小的分离，以便可以构建插值证书函数。还使用尾部界限用于高斯过程的上部，我们将预测误差限制在很高的概率上。假设可以构建证书函数，我们的预测错误限制为$ \ log $ factor，类似于线性回归模型中Lasso预测器所获得的速率。我们还建立了收敛速率，以高概率量化线性和非线性参数的估计质量。我们全面开发了两个应用程序的主要结果：高斯尖峰反卷积和缩放指数模型。]]></description>
      <guid>https://arxiv.org/abs/2207.00171</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>摊销的贝叶斯多级模型</title>
      <link>https://arxiv.org/abs/2408.13230</link>
      <description><![CDATA[ARXIV：2408.13230V2公告类型：替换 
摘要：多级模型（MLMS）是贝叶斯工作流程的中心构建块。它们能够跨层次级别的数据进行联合，可解释的数据建模，并提供不确定性的概率量化。尽管具有良好的认可的优势，但MLMS构成了重大的计算挑战，通常会在合理的时间限制内使其估计和评估可靠。基于模拟的推理的最新进展为使用深层生成网络解决复杂的概率模型提供了有希望的解决方案。但是，深度学习方法估计贝叶斯MLM的效用和可靠性在很大程度上尚未探索，尤其是与金标准的采样器相比。为此，我们探索了一个神经网络体系结构家族，该家族利用多级模型的概率分解，以促进有效的神经网络训练，并随后对未见数据集的近代后验推断。我们在几个现实世界中的案例研究中测试了我们的方法，并在可能的情况下与Stan的黄金标准采样器进行了全面的比较。最后，我们提供了我们的方法的开源实施，以刺激摊销贝叶斯推论的新生领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2408.13230</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的超参数优化</title>
      <link>https://arxiv.org/abs/2410.22854</link>
      <description><![CDATA[ARXIV：2410.22854V2公告类型：替换 
摘要：超参数是控制机器学习算法行为的配置变量。它们在机器学习和人工智能中无处不在，其价值的选择决定了基于这些技术的系统的有效性。手动超参数搜索通常不令人满意，当超参数数量较大时就变得不可行。自动搜索是迈向前进，精简和系统化机器学习的重要一步，使研究人员和从业人员都从试用和错误中找到一套好的超参数的负担。在这项调查中，我们提出了对超参数优化的统一处理，从而为读者提供了例子，对最新艺术的见解以及与进一步阅读的许多链接。我们涵盖了自动化超参数搜索的主要技术系列，通常称为超参数优化或调整，包括随机和准随机搜索，匪徒，模型，人群，基于梯度的方法。我们进一步讨论扩展，包括在线，约束和多目标配方，涉及与其他领域（例如元学习和神经体系结构搜索）的联系，并以开放的问题和未来的研究方向结束。]]></description>
      <guid>https://arxiv.org/abs/2410.22854</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过变化进行优化：随时间变化的贝叶斯优化算法的界限和建议</title>
      <link>https://arxiv.org/abs/2501.18963</link>
      <description><![CDATA[arxiv：2501.18963v2公告类型：替换 
摘要：随时间变化的贝叶斯优化（TVBO）是优化时变，昂贵，嘈杂的黑色盒子功能的首选框架。但是，迄今为止提出的大多数解决方案要么依赖于对目标函数性质的不切实际的假设，要么不提供任何理论保证。我们提出了第一个分析，即仅在轻度和现实的假设下渐近地限制tvbo算法的累积遗憾。特别是，我们提供了独立于算法的下遗憾束缚和较高的遗憾界限，可为大量的TVBO算法提供。基于此分析，我们为TVBO算法提出建议，并通过对合成和现实世界中的问题进行实验，展示其跟随它们的算法（螺栓）的表现如何比TVBO的最先进。]]></description>
      <guid>https://arxiv.org/abs/2501.18963</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>分销自动编码器知道分数</title>
      <link>https://arxiv.org/abs/2502.11583</link>
      <description><![CDATA[Arxiv：2502.11583V2公告类型：替换 
摘要：这项工作介绍了最近引入的一类自动编码器类别的新颖和理想的属性 - 分销主自动编码器（DPA） - 将分布在分布上正确的重建与主组件的类似组件的解释性结合在一起。首先，我们正式表明，编码器Orient的水平集与数据分布的分数有关。这两者都解释了该方法在解散数据变化因素方面经常出色的性能，并且可以在仅访问样品的同时恢复其分布的可能性。在分数本身具有物理含义的设置（例如数据遵守玻尔兹曼分布时），我们证明该方法可以恢复科学重要的数量，例如最小自由能路径。其次，我们证明，如果数据位于可以由编码器近似的歧管上，则最佳编码器的组件超出了歧管的范围，将绝对没有有关数据分布的其他信息。这有望确定数据相关维度数量的潜在新方法。因此，结果表明，DPA优雅地结合了两个经常无监督学习的目标：数据分布的学习和内在数据维度的学习。]]></description>
      <guid>https://arxiv.org/abs/2502.11583</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>