<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Tue, 25 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Sketch-GNN：具有亚线性训练复杂度的可扩展图神经网络</title>
      <link>https://arxiv.org/abs/2406.15575</link>
      <description><![CDATA[arXiv:2406.15575v1 公告类型：交叉 
摘要：图神经网络 (GNN) 广泛应用于图学习问题，例如节点分类。当将 GNN 的底层图扩展到更大的尺寸时，我们被迫要么在完整的图上进行训练，并将完整的图邻接和节点嵌入保存在内存中（这通常是不可行的），要么对图进行小批量采样（这会导致计算复杂度随 GNN 层数呈指数增长）。提出了各种基于采样和基于历史嵌入的方法来避免这种复杂性的指数增长。然而，这些解决方案都不能消除对图大小的线性依赖。本文提出了一种基于草图的算法，通过在几个紧凑的图邻接和节点嵌入草图上训练 GNN，其训练时间和内存随图大小呈亚线性增长。我们的框架基于多项式张量草图 (PTS) 理论，提供了一种新颖的协议，用于在 GNN 中绘制非线性激活和图卷积矩阵，而不是在神经网络中绘制线性权重或梯度的现有方法。此外，我们还开发了一种局部敏感哈希 (LSH) 技术，可以对其进行训练以提高草图的质量。在大型图基准测试上的实验证明了我们的 Sketch-GNN 与全尺寸 GNN 相比具有可扩展性和竞争性能。]]></description>
      <guid>https://arxiv.org/abs/2406.15575</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:59 GMT</pubDate>
    </item>
    <item>
      <title>$(f,\Gamma)$-GAN 的集中不等式</title>
      <link>https://arxiv.org/abs/2406.16834</link>
      <description><![CDATA[arXiv:2406.16834v1 公告类型：新
摘要：生成对抗网络 (GAN) 是一种无监督学习方法，用于训练生成器分布以生成近似于从目标分布中抽取的样本。许多此类方法可以表述为度量或散度的最小化。最近的研究已经证明了基于积分概率度量 (IPM) 的 GAN 的统计一致性，例如基于 1-Wasserstein 度量的 WGAN。IPM 是通过在鉴别器空间上优化线性函数（期望差异）来定义的。可以使用 $(f,\Gamma)$-散度构建更大的 GAN 类，允许使用非线性目标函数；它们在 IPM 和 $f$-散度（例如 KL 或 $\alpha$-散度）之间进行概括和插值。事实证明，$(f,\Gamma)$-GAN 实例在许多应用中表现出更好的性能。在这项工作中，我们研究了 $(f,\Gamma)$-GAN 对于一般 $f$ 和 $\Gamma$ 的统计一致性。具体来说，我们推导出有限样本浓度不等式。由于目标函数的非线性，这些推导需要新的参数。我们证明，我们的新结果在适当的极限下简化为 IPM-GAN 的已知结果，同时也显著扩展了该理论的适用范围。]]></description>
      <guid>https://arxiv.org/abs/2406.16834</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:58 GMT</pubDate>
    </item>
    <item>
      <title>统一无监督图级异常检测和分布外检测：基准</title>
      <link>https://arxiv.org/abs/2406.15523</link>
      <description><![CDATA[arXiv:2406.15523v1 公告类型：交叉 
摘要：为了构建安全可靠的图形机器学习系统，无监督图形级异常检测（GLAD）和无监督图形级分布外（OOD）检测（GLOD）近年来受到了广泛关注。虽然这两条研究路线确实有相同的目标，但由于不同的评估设置，它们在社区中被独立研究，从而造成了阻碍方法应用和评估的差距。为了弥合这一差距，在这项工作中，我们提出了一个无监督图级OOD和异常检测的统一基准（我们的方法），这是一个全面的评估框架，在广义图级OOD检测的概念下统一了GLAD和GLOD。我们的基准涵盖了4个实际异常和OOD检测场景的35个数据集，有助于比较16种代表性的GLAD / GLOD方法。我们进行多维度分析，探索现有方法的有效性、通用性、稳健性和效率，揭示其优势和局限性。此外，我们还提供了我们方法的开源代码库 (https://github.com/UB-GOLD/UB-GOLD)，以促进可重复的研究，并根据我们的见解概述未来研究的潜在方向。]]></description>
      <guid>https://arxiv.org/abs/2406.15523</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:58 GMT</pubDate>
    </item>
    <item>
      <title>SAIL：自我改进的大型语言模型高效在线对齐</title>
      <link>https://arxiv.org/abs/2406.15567</link>
      <description><![CDATA[arXiv:2406.15567v1 公告类型：交叉 
摘要：从人类反馈中强化学习 (RLHF) 是将大型语言模型 (LLM) 与人类偏好对齐的关键方法。然而，当前的离线对齐方法（如 DPO、IPO 和 SLiC）严重依赖固定偏好数据集，这可能导致性能不佳。另一方面，最近的文献专注于设计在线 RLHF 方法，但仍然缺乏统一的概念公式，并且存在分布偏移问题。为了解决这个问题，我们确定在线 LLM 对齐以双层优化为基础。通过将这个公式简化为有效的单级一阶方法（使用奖励策略等价性），我们的方法生成新样本并通过探索响应和调节偏好标签来迭代细化模型对齐。通过这样做，我们允许对齐方法以在线和自我改进的方式运行，并将先前的在线 RLHF 方法概括为特殊情况。与最先进的迭代 RLHF 方法相比，我们的方法以最小的计算开销显著提高了开源数据集上的对齐性能。]]></description>
      <guid>https://arxiv.org/abs/2406.15567</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:58 GMT</pubDate>
    </item>
    <item>
      <title>深度学习预测：超越平均水平</title>
      <link>https://arxiv.org/abs/2406.16590</link>
      <description><![CDATA[arXiv:2406.16590v1 公告类型：新
摘要：准确评估预测模型对于确保可靠的预测至关重要。当前评估和比较预测模型的做法侧重于使用 SMAPE 等指标将性能总结为单个分数。我们假设对所有样本的平均性能会稀释有关模型相对性能的相关信息。特别是，这种相对性能不同于整体准确性的情况。我们通过提出一种新颖的框架来解决这一限制，该框架用于从多个角度评估单变量时间序列预测模型，例如一步预测与多步预测。我们通过将最先进的深度学习方法与经典预测技术进行比较来展示该框架的优势。虽然经典方法（例如 ARIMA）是长期存在的预测方法，但深度神经网络（例如 NHITS）最近在基准数据集中表现出最先进的预测性能。我们进行了大量的实验，结果表明 NHITS 通常表现最佳，但其优越性会因预测条件而异。例如，就预测范围而言，NHITS 仅在多步预测方面优于传统方法。另一个相关的见解是，在处理异常时，NHITS 的表现不如 Theta 等方法。这些发现凸显了基于方面的模型评估的重要性。]]></description>
      <guid>https://arxiv.org/abs/2406.16590</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:57 GMT</pubDate>
    </item>
    <item>
      <title>具有组件可交换性的共形时间序列分解</title>
      <link>https://arxiv.org/abs/2406.16766</link>
      <description><![CDATA[arXiv:2406.16766v1 公告类型：新
摘要：共形预测为无分布不确定性量化提供了一个实用框架，在相对温和的数据可交换性假设下提供有限样本覆盖保证。然而，由于时间序列的时间相关性，这些假设不再适用于时间序列。在这项工作中，我们提出了一种新的共形预测用于时间序列预测的用途，该用途结合了时间序列分解。这种方法使我们能够单独建模不同的时间成分。通过将特定的共形算法应用于每个成分，然后合并获得的预测区间，我们可以定制我们的方法来考虑每个成分所依赖的不同可交换性机制。我们对基于分解的方法进行了深入讨论，并在合成数据和真实数据上进行了实证评估。我们发现该方法在结构良好的时间序列上提供了有希望的结果，但可能会受到诸如更复杂数据的分解步骤等因素的限制。]]></description>
      <guid>https://arxiv.org/abs/2406.16766</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:57 GMT</pubDate>
    </item>
    <item>
      <title>OAML：用于增强 OOD 检测的异常值感知度量学习</title>
      <link>https://arxiv.org/abs/2406.16525</link>
      <description><![CDATA[arXiv:2406.16525v1 公告类型：新
摘要：已经开发了分布外 (OOD) 检测方法来识别模型在训练期间未见过的对象。异常值暴露 (OE) 方法使用辅助数据集直接训练 OOD 检测器。然而，代表性 OOD 样本的收集和学习可能会带来挑战。为了解决这些问题，我们提出了异常值感知度量学习 (OAML) 框架。我们方法的主要思想是使用 k-NN 算法和稳定扩散模型在特征级别生成异常值进行训练，而不做任何分布假设。为了增加语义空间中的特征差异，我们开发了一种基于互信息的对比学习方法，可以有效地从 OOD 数据中学习。理论和实证结果都证实了这种对比学习技术的有效性。此外，我们将知识蒸馏纳入我们的学习框架，以防止分布内分类准确性的下降。对比学习和知识蒸馏算法的结合显着提高了 OOD 检测的性能。在各种数据集上的实验结果表明，我们的方法明显优于以前的 OE 方法。]]></description>
      <guid>https://arxiv.org/abs/2406.16525</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:56 GMT</pubDate>
    </item>
    <item>
      <title>条件贝叶斯求积法</title>
      <link>https://arxiv.org/abs/2406.16530</link>
      <description><![CDATA[arXiv:2406.16530v1 公告类型：新
摘要：我们提出了一种在获取样本或评估被积函数成本高昂的情况下估计条件或参数期望的新方法。通过概率数值方法（如贝叶斯求积法）的框架，我们的新方法可以结合被积函数的先验信息，尤其是被积函数和条件期望的先验平滑度知识。因此，我们的方法提供了一种量化不确定性的方法，并实现了快速的收敛速度，这在贝叶斯敏感性分析、计算金融和不确定性决策等具有挑战性的任务中得到了理论和经验的证实。]]></description>
      <guid>https://arxiv.org/abs/2406.16530</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:56 GMT</pubDate>
    </item>
    <item>
      <title>缺失值变化下的稳健预测</title>
      <link>https://arxiv.org/abs/2406.16484</link>
      <description><![CDATA[arXiv:2406.16484v1 公告类型：新
摘要：由于协变量缺失，预测变得更具挑战性。选择哪种方法来处理缺失会极大地影响模型的性能。在许多现实问题中，最佳预测性能是由能够利用缺失值的信息性质的模型实现的。然而，一旦模型在实践中部署，协变量缺失的原因就会发生变化。如果发生这种缺失偏移，则目标数据中值缺失的条件概率会有所不同。源数据中的预测性能可能不再是一个好的选择标准，不依赖于信息缺失的方法可能更可取。然而，我们表明，贝叶斯预测器不会因可忽略的偏移而发生变化，对于这些偏移，缺失的概率仅取决于观察到的数据。因此，贝叶斯预测器的任何一致估计量都可能在这些条件下产生稳健的预测，尽管我们通过经验表明，不同的方法对不同类型的偏移似乎具有稳健性。如果缺失值偏移不可忽略，贝叶斯预测器可能会因偏移而改变。虽然在这种情况下两种方法都无法恢复贝叶斯预测器，但我们从经验上发现，当缺失值信息量很大时，忽略缺失值是最有利的。]]></description>
      <guid>https://arxiv.org/abs/2406.16484</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:55 GMT</pubDate>
    </item>
    <item>
      <title>结合并征服：数据移位和分布外检测的元分析</title>
      <link>https://arxiv.org/abs/2406.16045</link>
      <description><![CDATA[arXiv:2406.16045v1 公告类型：新
摘要：本文介绍了一种无缝组合分布外 (OOD) 检测分数的通用方法。这些分数涵盖了广泛的技术，这些技术利用了深度学习模型的自信心和潜在空间中特征的异常行为。毫不奇怪，使用简单的统计数据组合如此多样化的人群是不够的。为了克服这一挑战，我们提出了分位数归一化来将这些分数映射到 p 值中，从而有效地将问题框架化为多变量假设检验。然后，我们使用已建立的元分析工具组合这些测试，从而产生具有合并决策边界的更有效的检测器。此外，我们通过将最终统计数据映射到具有已知参数的分布中来创建概率可解释标准。通过实证研究，我们探索了不同类型的转变，每种转变对数据的影响程度各不相同。我们的结果表明，我们的方法显着提高了不同 OOD 检测场景中的整体稳健性和性能。值得注意的是，我们的框架易于扩展，以适应未来检测分数的发展，并且是第一个在此背景下结合决策边界的框架。与这项工作相关的代码和工件是公开可用的\footnote{\url{https://github.com/edadaltocg/detectors}}。]]></description>
      <guid>https://arxiv.org/abs/2406.16045</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:54 GMT</pubDate>
    </item>
    <item>
      <title>VICatMix：离散生物医学数据的变分贝叶斯聚类和变量选择</title>
      <link>https://arxiv.org/abs/2406.16227</link>
      <description><![CDATA[arXiv:2406.16227v1 公告类型：新
摘要：生物医学数据的有效聚类对于精准医疗至关重要，可以对患者或样本进行准确的分层。然而，包括组学数据在内的高维分类数据可用性的增长需要计算效率高的聚类算法。我们提出了 VICatMix，这是一种专为分类数据聚类而设计的变分贝叶斯有限混合模型。在其训练中使用变分推理 (VI) 使该模型在效率方面胜过竞争对手，同时保持高精度。VICatMix 还执行变量选择，增强其在高维、噪声数据上的性能。所提出的模型结合了汇总和模型平均来缓解 VI 中的局部最优性较差，从而可以同时改进对真实聚类数量的估计和特征显着性。我们利用模拟数据和真实数据展示了 VICatMix 的性能，包括应用于 Cancer Genome Atlas (TCGA) 的数据集，展示了其在癌症亚型和驱动基因发现中的应用。我们展示了 VICatMix 在整合聚类分析中使用不同组学数据集的实用性，从而能够发现新的亚型。
\textbf{可用性：}VICatMix 可作为 R 包免费使用，结合 C++ 以加快计算速度，网址为 \url{https://github.com/j-ackierao/VICatMix}。]]></description>
      <guid>https://arxiv.org/abs/2406.16227</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:54 GMT</pubDate>
    </item>
    <item>
      <title>平坦后验对贝叶斯迁移学习很重要</title>
      <link>https://arxiv.org/abs/2406.15664</link>
      <description><![CDATA[arXiv:2406.15664v1 公告类型：新
摘要：大规模预训练神经网络在提升下游任务性能方面取得了显著成功。另一种有前途的泛化方法是贝叶斯神经网络 (BNN)，它将贝叶斯方法集成到神经网络架构中，提供贝叶斯模型平均 (BMA) 和不确定性量化等优势。尽管有这些好处，但 BNN 的迁移学习尚未得到广泛研究，并且改进有限。我们假设这个问题源于无法找到平坦最小值，这对于泛化性能至关重要。为了解决这个问题，我们在各种设置中评估了 BNN 的锐度，揭示了它们在寻找平坦最小值方面的不足以及平坦度对 BMA 性能的影响。因此，我们提出了锐度感知贝叶斯模型平均 (SA-BMA)，一种与贝叶斯迁移学习相结合的贝叶斯拟合平坦后验寻找优化器。 SA-BMA 计算参数空间中后验之间的差异，与 BNN 的性质一致，并作为现有锐度感知优化器的通用版本。我们验证了 SA-BMA 通过确保平坦度来提高小样本分类和分布偏移场景中的泛化性能。]]></description>
      <guid>https://arxiv.org/abs/2406.15664</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:53 GMT</pubDate>
    </item>
    <item>
      <title>随机学习率的影响：通过平稳分布对非凸优化中 SGD 动态的理论分析</title>
      <link>https://arxiv.org/abs/2406.16032</link>
      <description><![CDATA[arXiv:2406.16032v1 公告类型：新
摘要：我们考虑了具有随机学习率的随机梯度下降 (SGD) 的变体，并揭示了它的收敛特性。SGD 是机器学习尤其是深度学习中广泛使用的随机优化算法。大量研究揭示了 SGD 及其简化变体的收敛特性。其中，使用更新参数的平稳分布分析收敛提供了可推广的结果。然而，为了获得平稳分布，参数的更新方向不能退化，这限制了 SGD 的适用变体。在本研究中，我们考虑了一种新的 SGD 变体 Poisson SGD，它具有退化的参数更新方向，而是使用随机学习率。因此，我们证明，在对损失函数的弱假设下，由 Poisson SGD 更新的参数分布收敛到平稳分布。在此基础上，我们进一步证明了泊松随机梯度下降法 (Poisson SGD) 可以在非凸优化问题中找到全局最小值，并使用此方法评估泛化误差。作为一种证明技术，我们利用分段确定性马尔可夫过程 (PDMP) 的理论进步，用弹性粒子采样器 (BPS) 的分布来近似泊松随机梯度下降法 (Poisson SGD) 的分布，并得出其平稳分布。]]></description>
      <guid>https://arxiv.org/abs/2406.16032</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:53 GMT</pubDate>
    </item>
    <item>
      <title>随机森林看不见的隐藏变量</title>
      <link>https://arxiv.org/abs/2406.15500</link>
      <description><![CDATA[arXiv:2406.15500v1 公告类型：新
摘要：人们普遍认为随机森林能够很好地捕捉相互作用。然而，一些简单的例子表明，在存在某些纯相互作用的情况下，它们表现不佳，而传统的 CART 标准在树构建过程中很难捕捉到这些相互作用。我们认为，在树生长过程中使用的简单替代分区方案可以增强对这些相互作用的识别。在一项模拟研究中，我们将这些变体与传统的随机森林和极端随机树进行了比较。我们的结果验证了所考虑的修改在纯相互作用起关键作用的场景中增强了模型的拟合能力。]]></description>
      <guid>https://arxiv.org/abs/2406.15500</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:52 GMT</pubDate>
    </item>
    <item>
      <title>系统辨识的随机占用核方法</title>
      <link>https://arxiv.org/abs/2406.15661</link>
      <description><![CDATA[arXiv:2406.15661v1 公告类型：新
摘要：占用核方法已被用于以非参数方式从数据中学习常微分方程。我们提出了一种两步方法来学习随机微分方程的漂移和扩散，给定过程的快照。在第一步中，我们通过将占用核算法应用于过程的期望值来学习漂移。在第二步中，我们使用半定程序在给定漂移的情况下学习扩散。具体来说，我们将扩散平方学习为与核平方相关的 RKHS 中的非负函数。我们提供了示例和模拟。]]></description>
      <guid>https://arxiv.org/abs/2406.15661</guid>
      <pubDate>Tue, 25 Jun 2024 06:19:52 GMT</pubDate>
    </item>
    </channel>
</rss>