<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过删除补丁来增强掩蔽时间序列建模</title>
      <link>https://arxiv.org/abs/2412.15315</link>
      <description><![CDATA[arXiv:2412.15315v1 Announce Type: new 
摘要：本文探讨了如何通过随机丢弃时间序列的子序列级patch来增强现有的masked时间序列建模。在此基础上，提出了一种简单而有效的方法DropPatch，该方法具有两个显著的​​优势：1）通过平方级优势提高预训练效率；2）为域内、跨域、小样本学习和冷启动等场景的建模提供了额外的优势。本文进行了全面的实验来验证该方法的有效性并分析其内在机制。从经验上讲，DropPatch强化了注意力机制，减少了信息冗余，是一种有效的数据增强手段。从理论上讲，证明了DropPatch通过随机丢弃patch减缓了Transformer表示塌缩到秩1线性子空间的速度，从而优化了学习到的表示的质量]]></description>
      <guid>https://arxiv.org/abs/2412.15315</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 SPAR 模型深度学习海洋气象变量的联合极值</title>
      <link>https://arxiv.org/abs/2412.15808</link>
      <description><![CDATA[arXiv:2412.15808v1 公告类型：新
摘要：本文提出了一种基于半参数角径向 (SPAR) 模型的新型深度学习框架，用于估计海洋气象变量的多元联合极值。在极坐标中考虑时，多元极值建模问题转化为角密度建模问题，以及以角度为条件的单变量径向变量尾部建模问题。在 SPAR 方法中，径向变量的尾部使用广义帕累托 (GP) 分布建模，为单变量极值理论向多元设置的自然延伸提供了依据。在这项工作中，我们展示了该方法如何应用于更高维度，并使用五个海洋气象变量的案例研究：风速、风向、波高、波周期和波向。角度变量是经验建模的，而 GP 模型的参数则使用全连接深度神经网络进行近似。我们的数据驱动方法在可表示的依赖结构方面提供了极大的灵活性，同时还提供了用于训练模型的计算效率高的例程。此外，与现有方法相比，该方法的应用需要对底层分布做出更少的假设，并且需要一种渐近合理的方法来推断观测范围之外的情况。使用各种诊断图，我们表明拟合的模型可以很好地描述所考虑的海洋气象变量的联合极值。]]></description>
      <guid>https://arxiv.org/abs/2412.15808</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用矩阵乘积状态进行时间序列机器学习</title>
      <link>https://arxiv.org/abs/2412.15826</link>
      <description><![CDATA[arXiv:2412.15826v1 公告类型：新
摘要：矩阵积态 (MPS) 已被证明是一种用于建模量子多体物理的多功能假设。对于许多应用，特别是在一维应用中，它们可以捕获多体波函数中的相关量子相关性，同时仍然易于在经典计算机上存储和操作。这促使研究人员也将 MPS 假设应用于机器学习 (ML) 问题，其中捕获数据集中的复杂相关性也是一项关键要求。在这里，我们开发并应用了一种基于 MPS 的算法 MPSTime，用于学习观察到的时间序列数据集的联合概率分布，并展示如何使用它来解决重要的时间序列 ML 问题，包括分类和归纳。 MPSTime 可以直接从数据中高效地学习复杂的时间序列概率分布，只需要中等的最大 MPS 键维数 $\chi_{\rm max}$，我们的应用值介于 $\chi_{\rm max} = 20-150$ 之间，并且可以在单个对数损失函数下进行分类和归纳任务训练。使用合成的、公开可用的现实世界数据集，涵盖医学、能源和天文学领域的应用，我们展示了与最先进的 ML 方法相媲美的性能，但关键优势在于对从数据中学习到的完整联合概率分布进行编码。通过从联合概率分布中采样并计算其条件纠缠熵，我们展示了如何揭示和解释其底层结构。本手稿补充了一个公开可用的代码包 MPSTime，该包实现了我们的方法。基于 MPS 的假设从时间序列数据中学习复杂相关结构的效率可能会为科学、工业和医学领域具有挑战性的时间序列 ML 问题的可解释性进展奠定基础。]]></description>
      <guid>https://arxiv.org/abs/2412.15826</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于稳健的跨域对齐</title>
      <link>https://arxiv.org/abs/2412.15861</link>
      <description><![CDATA[arXiv:2412.15861v1 公告类型：新
摘要：Gromov-Wasserstein (GW) 距离是衡量不同环境空间上分布之间对齐的有效方法。它本质上计算了与等距的相互偏离，在域转换和网络分析中得到了广泛的应用。长期以来，人们已经证明它容易受到底层度量污染的影响。在 GW 中引入稳健性的所有努力都受到了最佳传输 (OT) 中类似技术的启发，这些技术主要主张部分质量传输或不平衡。相比之下，跨域对齐问题与 OT 根本不同，需要特定的解决方案来应对不同的应用和污染状况。从稳健统计数据中得出，我们讨论了三种上下文新颖的技术来增强 GW 及其变体的稳健性。对于每种方法，我们探索度量属性和稳健性保证以及它们与 GW 距离的相互依赖性和个体关系。为了全面了解，我们通过与最先进的方法相比，在真实的机器学习任务下，实证验证了它们对污染的卓越抵抗力。]]></description>
      <guid>https://arxiv.org/abs/2412.15861</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习线性逆问题的稀疏性促进正则化器</title>
      <link>https://arxiv.org/abs/2412.16031</link>
      <description><![CDATA[arXiv:2412.16031v1 公告类型：新
摘要：本文介绍了一种用于解决线性逆问题的学习稀疏性促进正则化器的新方法。我们开发了一个双层优化框架来选择一个最佳合成算子，表示为 $B$，该算子在促进解决方案的稀疏性的同时对逆问题进行正则化。该方法利用基础数据的统计特性，并通过选择 $B$ 来整合先验知识。我们建立了优化问题的适定性，为学习过程提供了理论保证，并提出了样本复杂度界限。该方法通过示例进行了演示，包括已知算子的紧凑扰动和学习母小波的问题，展示了其在将先验知识纳入正则化框架方面的灵活性。这项工作通过解决不可微规范并提出一种无限维度稀疏正则化的数据驱动方法，扩展了 Tikhonov 正则化方面的先前努力。]]></description>
      <guid>https://arxiv.org/abs/2412.16031</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>过度参数化模型中具有多个局部步骤的分布式梯度下降</title>
      <link>https://arxiv.org/abs/2412.07971</link>
      <description><![CDATA[arXiv:2412.07971v1 公告类型：交叉 
摘要：在机器学习模型的分布式训练中，具有局部迭代步骤的梯度下降是一种非常流行的方法，其变体通常称为 Local-SGD 或联邦平均 (FedAvg)。在该方法中，基于本地数据集的梯度步骤在分布式计算节点中独立采取以更新本地模型，然后间歇性地聚合这些模型。虽然现有的收敛分析表明，对于异构数据，随着局部步骤数量的增加，FedAvg 会遇到性能快速下降的问题，但它在实践中被证明非常有效，特别是在大型语言模型的分布式训练中。在这项工作中，我们试图从具有大量局部步骤的局部梯度下降 (Local-GD) 中的隐性偏差的角度来解释这种良好的性能。在过度参数化的状态下，每个计算节点的梯度下降都会将模型引导到局部的特定方向。我们描述了聚合全局模型的动态，并将其与使用所有数据在一个地方训练的集中式模型进行了比较。具体来说，我们分析了梯度下降对线性模型的隐性偏差，包括回归和分类任务。我们的分析表明，对于回归任务，聚合全局模型完全收敛到集中式模型，对于分类任务，聚合全局模型收敛到（在方向上）与集中式模型相同的可行集。我们进一步提出了一种具有精细聚合的改进型局部梯度下降，并从理论上表明它在线性分类的方向上收敛到集中式模型。我们在线性模型中通过经验验证了我们的理论发现，并对预训练神经网络的分布式微调进行了实验，以进一步应用我们的理论。]]></description>
      <guid>https://arxiv.org/abs/2412.07971</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Kolmogorov-Arnold 网络进行 Granger 因果关系检测</title>
      <link>https://arxiv.org/abs/2412.15373</link>
      <description><![CDATA[arXiv:2412.15373v1 公告类型：交叉 
摘要：发现时间序列数据中的因果关系是许多科学领域的核心，从经济学到气候科学。格兰杰因果关系是因果关系检测的有力工具。然而，它的原始公式受到其线性形式的限制，而且直到最近才引入了非线性机器学习泛化。本研究通过研究 Kolmogorov-Arnold 网络 (KAN) 在 Granger 因果关系检测中的应用并将其能力与多层感知器 (MLP) 进行比较，为神经 Granger 因果关系模型的定义做出了贡献。在这项工作中，我们开发了一个名为 Granger 因果关系 KAN (GC-KAN) 的框架以及专门为 Granger 因果关系检测设计的定制训练方法。我们在向量自回归 (VAR) 模型和混沌 Lorenz-96 系统上测试了该框架，分析了 KAN 通过识别 Granger 因果关系来稀疏化输入特征的能力，从而为 Granger 因果关系检测提供了简洁而准确的模型。我们的研究结果表明，KAN 在辨别可解释的 Granger 因果关系方面有潜力胜过 MLP，特别是在高维环境中识别稀疏 Granger 因果关系模式的能力方面，以及更广泛地说，AI 在物理系统中动态定律的因果关系发现方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.15373</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过节点分类了解图注意力机制何时以及为何起作用</title>
      <link>https://arxiv.org/abs/2412.15496</link>
      <description><![CDATA[arXiv:2412.15496v1 公告类型：交叉 
摘要：尽管图注意机制越来越受欢迎，但对其理论的理解仍然有限。本文旨在通过上下文随机块模型 (CSBM) 的视角探索这些机制在节点分类任务中有效的条件。我们的理论分析表明，结合图注意机制并非普遍有益。具体而言，通过在图中适当定义 \emph{结构噪声} 和 \emph{特征噪声}，我们表明当结构噪声超过特征噪声时，图注意机制可以提高分类性能。相反，当特征噪声占主导地位时，更简单的图卷积操作更有效。此外，我们研究了过度平滑现象，并表明在高信噪比 (SNR) 状态下，图卷积网络会出现过度平滑，而图注意机制可以有效地解决这个问题。基于这些见解，我们提出了一种新颖的多层图注意力网络 (GAT) 架构，该架构在实现 CSBM 中的 \emph{完美节点分类} 方面明显优于单层 GAT，将 SNR 要求从 $ \omega(\sqrt{\log n}) $ 放宽到 $ \omega(\sqrt{\log n} / \sqrt[3]{n}) $。据我们所知，这是第一项使用多层 GAT 描述完美节点分类条件的研究。我们的理论贡献得到了对合成和真实数据集的大量实验的证实，突出了我们的研究结果的实际意义。]]></description>
      <guid>https://arxiv.org/abs/2412.15496</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过图常微分方程进行架构感知学习曲线外推</title>
      <link>https://arxiv.org/abs/2412.15554</link>
      <description><![CDATA[arXiv:2412.15554v1 公告类型：交叉 
摘要：学习曲线外推法可从早期训练阶段预测神经网络性能，并已应用于加速 AutoML，促进超参数调整和神经架构搜索。然而，现有方法通常孤立地对学习曲线的演变进行建模，而忽略了影响损失状况和学习轨迹的神经网络 (NN) 架构的影响。在这项工作中，我们探索结合神经网络架构是否可以改善学习曲线建模以及如何有效地整合这些架构信息。受优化动态系统观点的启发，我们提出了一种新颖的架构感知神经微分方程模型来持续预测学习曲线。我们通过经验证明了它能够捕捉波动学习曲线的总体趋势，同时通过变分参数量化不确定性。对于基于 MLP 和 CNN 的学习曲线，我们的模型优于当前最先进的学习曲线外推方法和纯时间序列建模方法。此外，我们还探索了我们的方法在神经架构搜索场景中的适用性，例如训练配置排名。]]></description>
      <guid>https://arxiv.org/abs/2412.15554</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高维线性回归讲义</title>
      <link>https://arxiv.org/abs/2412.15633</link>
      <description><![CDATA[arXiv:2412.15633v1 公告类型：交叉 
摘要：这些讲​​义涵盖了线性回归的高级主题，深入探讨了此设置中最突出的估计量的存在性、唯一性、关系、计算和非渐近性质。涵盖的估计量包括最小二乘、无脊估计、脊估计和套索估计。内容遵循命题证明结构，适合寻求对机器学习方法背后的统计理论进行正式和严格理解的学生。]]></description>
      <guid>https://arxiv.org/abs/2412.15633</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Ricci 曲率的超图聚类：边缘传输视角</title>
      <link>https://arxiv.org/abs/2412.15695</link>
      <description><![CDATA[arXiv:2412.15695v1 公告类型：交叉 
摘要：在本文中，我们介绍了一种将 Ricci 流扩展到超图的新方法，即在边上定义概率度量并将其传输到线扩展上。这种方法在边上产生了一种新的权重，这对社区检测特别有效。我们将这种方法与在团扩展上定义的类似 Ricci 流概念进行了广泛的比较，证明了它对超图结构的增强敏感性，尤其是在存在大型超边的情况下。这两种方法是互补的，共同构成了一个强大且高度可解释的超图社区检测框架。]]></description>
      <guid>https://arxiv.org/abs/2412.15695</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>单变量多模态数据的统计建模</title>
      <link>https://arxiv.org/abs/2412.15894</link>
      <description><![CDATA[arXiv:2412.15894v1 公告类型：交叉 
摘要：单峰性构成了一个关键属性，指示数据围绕其密度的单一模式进行分组行为。我们提出了一种方法，通过围绕数据密度谷点进行递归分割，将单变量数据划分为单峰子集。对于谷点检测，我们引入了经验累积密度函数 (ecdf) 图凸包上临界点的属性，这些属性可以指示密度谷的存在。接下来，我们应用单峰数据建模方法，该方法以均匀混合模型 (UMM) 的形式为每个获得的单峰子集提供统计模型。因此，以 UMM 混合的形式获得初始数据集的分层统计模型，称为单峰混合模型 (UDMM)。所提出的方法是非参数的、无超参数的，可以自动估计单峰子集的数量，并提供准确的统计模型，这一点已在聚类和密度估计任务中的实验结果得到证实。]]></description>
      <guid>https://arxiv.org/abs/2412.15894</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>非欧几里得经验风险最小化的黑箱一致稳定性</title>
      <link>https://arxiv.org/abs/2412.15956</link>
      <description><![CDATA[arXiv:2412.15956v1 公告类型：交叉 
摘要：我们研究了对于经验风险最小化 (ERM) 问题一致稳定的一阶算法，该问题对于 $p$ 范数是凸的且平滑的，$p \geq 1$。我们提出了一种黑盒简化方法，通过利用一致凸正则化器的属性，将 H\&quot;older 平滑凸损失的优化算法转变为一致稳定的学习算法，对超额风险具有最佳统计风险界限，最高可达一个取决于 $p$ 的常数因子。实现一致稳定性的黑盒简化是一个悬而未决的问题 (Attia and Koren, 2022)，他们已经解决了欧几里得情况 $p=2$。我们探索利用非欧几里得几何解决二元分类问题的应用。]]></description>
      <guid>https://arxiv.org/abs/2412.15956</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>核双样本检验的变量选择</title>
      <link>https://arxiv.org/abs/2302.07415</link>
      <description><![CDATA[arXiv:2302.07415v4 公告类型：替换 
摘要：我们考虑双样本测试的变量选择问题，旨在选择最具信息量的变量来确定两个样本集合是否遵循相同的分布。为了解决这个问题，我们提出了一个基于核最大均值差异 (MMD) 的新框架。我们的方法寻找一个具有预先指定大小的变量子集，以最大化方差正则化的核 MMD 统计量。我们关注三种常用的核类型：线性、二次和高斯。从计算的角度来看，我们推导出混合整数规划公式，并提出了具有性能保证的精确和近似算法来解决这些公式。从统计的角度来看，我们在适当的条件下推导出我们框架的测试能力率。这些结果表明，三个核的样本量要求主要取决于所选变量的数量，而不是数据维度。在合成和真实数据集上的实验结果表明，与其他变量选择框架相比，我们的方法具有更优异的性能，尤其是在高维设置中。]]></description>
      <guid>https://arxiv.org/abs/2302.07415</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 流程：基于自然语言的数值预测分布</title>
      <link>https://arxiv.org/abs/2405.12856</link>
      <description><![CDATA[arXiv:2405.12856v5 公告类型：替换 
摘要：机器学习从业者在将他们的先验知识和信念正式整合到预测模型中时经常面临重大挑战，这限制了进行细微和情境感知分析的潜力。此外，将这些先验知识整合到概率建模中所需的专业知识通常会将这些模型的应用限制在专家身上。我们的目标是建立一个回归模型，该模型可以处理数值数据并在任意位置进行概率预测，由描述用户先验知识的自然语言文本指导。大型语言模型 (LLM) 为设计此类工具提供了一个有用的起点，因为它们 1) 提供了一个界面，用户可以在其中以自然语言的形式整合专家见解，2) 提供了一个机会来利用 LLM 中编码的潜在问题相关知识，而用户自己可能没有这些知识。我们首先探索从 LLM 中引出明确、连贯的数值预测分布的策略。我们在预测、多维回归、黑箱优化和图像建模等场景中研究了这些联合预测分布（我们称之为 LLM 过程），这些分布适用于任意多个数量。我们研究了提示以得出连贯预测分布的实际细节，并展示了它们在回归中的有效性。最后，我们展示了将文本有效地纳入数值预测的能力，从而提高了预测性能并给出了反映定性描述的定量结构。这让我们开始探索 LLM 隐式编码的丰富、扎实的假设空间。]]></description>
      <guid>https://arxiv.org/abs/2405.12856</guid>
      <pubDate>Mon, 23 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>