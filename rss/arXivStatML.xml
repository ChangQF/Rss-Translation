<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 18 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>理解深度学习中的双重下降现象</title>
      <link>https://arxiv.org/abs/2403.10459</link>
      <description><![CDATA[arXiv:2403.10459v1 公告类型：交叉
摘要：随着模型类容量变大，将经验风险最小化与容量控制相结合是机器学习中尝试控制泛化差距并避免过度拟合的经典策略。然而，在现代深度学习实践中，非常大的超参数化模型（例如神经网络）经过优化以完美适应训练数据，并且仍然获得出色的泛化性能。经过插值点，增加模型复杂性似乎实际上会降低测试误差。
  在本教程中，我们解释双重下降的概念及其机制。第一部分设定了经典的统计学习框架并介绍了双下降现象。通过查看大量示例，第 2 节介绍了归纳偏差，通过在多个插值解决方案中选择平滑的经验风险最小化器，归纳偏差似乎在双重下降中发挥着关键作用。最后，第 3 节探讨了两个线性模型的双下降，并给出了最近相关工作中的其他观点。]]></description>
      <guid>https://arxiv.org/abs/2403.10459</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>对抗性自适应采样：统一 PINN 和最优传输以逼近偏微分方程</title>
      <link>https://arxiv.org/abs/2305.18702</link>
      <description><![CDATA[arXiv:2305.18702v2 公告类型：替换
摘要：求解偏微分方程（PDE）是科学计算的核心任务。近年来，偏微分方程的神经网络逼近因其灵活的无网格离散化及其解决高维问题的潜力而受到越来越多的关注。一个基本的数值困难是训练集中的随机样本将统计误差引入损失函数的离散化中，这可能成为最终近似中的主要误差，从而掩盖了神经网络的建模能力。在这项工作中，我们提出了一种新的最小最大公式，以同时优化神经网络模型给出的近似解和深度生成模型提供的训练集中的随机样本。关键思想是使用深度生成模型来调整训练集中的随机样本，使得近似 PDE 解引起的残差在最小化时能够保持平滑的轮廓。这种想法是通过将残差引起的分布和均匀分布之间的 Wasserstein 距离隐式嵌入到损失中来实现的，然后将损失与残差一起最小化。几乎均匀的残差分布意味着对于任何归一化权重函数来说其方差都很小，使得损失函数的蒙特卡洛近似误差对于一定的样本量显着减小。这项工作中提出的对抗性自适应采样（AAS）方法是第一次尝试将两个基本组成部分（最小化残差和寻找最佳训练集）公式化为偏微分方程神经网络逼近的一个最小最大目标函数。]]></description>
      <guid>https://arxiv.org/abs/2305.18702</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>更便宜的神经网络集成的可靠不确定性：工业零件分类的案例研究</title>
      <link>https://arxiv.org/abs/2403.10182</link>
      <description><![CDATA[arXiv:2403.10182v1 公告类型：交叉
摘要：在运筹学（OR）中，预测模型经常遇到分布外（OOD）场景，其中数据分布与训练数据分布不同。近年来，神经网络 (NN) 因其在图像分类等领域的卓越性能而在 OR 领域受到关注。然而，神经网络在面对 OOD 数据时往往会做出自信但不正确的预测。不确定性估计为过度自信的模型提供了解决方案，可以在输出应该（不）可信时进行通信。因此，神经网络中可靠的不确定性量化在 OR 领域至关重要。由多个独立神经网络组成的深度集成已成为一种有前途的方法，不仅提供强大的预测准确性，而且提供可靠的不确定性估计。然而，由于大量的计算需求，它们的部署具有挑战性。最近的基础研究提出了更高效的神经网络集成，即快照集成、批量集成和多输入多输出集成。这项研究首次对单个神经网络、深度集成和三种高效神经网络集成进行了全面比较。此外，我们提出了一种多样性质量度量，以在一个度量中量化集成在分布内和 OOD 集上的性能。 OR 案例研究讨论了工业零件分类，以识别和管理备件，这对于工业工厂的及时维护非常重要。结果表明，批量集成是深度集成的一种经济有效且具有竞争力的替代方案。它在不确定性和准确性方面都优于深度集成，同时训练时间加速了 7 倍，测试时间加速了 8 倍，并且节省了 9 倍的内存。]]></description>
      <guid>https://arxiv.org/abs/2403.10182</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>Huber污染下具有最优误差的高斯稳健稀疏估计</title>
      <link>https://arxiv.org/abs/2403.10416</link>
      <description><![CDATA[arXiv:2403.10416v1 公告类型：交叉
摘要：我们研究了 Huber 污染模型中的高斯稀疏估计任务，重点是均值估计、PCA 和线性回归。对于每一个任务，我们都会在常数因子内给出第一个样本和计算高效的稳健估计器，并具有最佳误差保证。所有先前针对这些任务的有效算法都会产生定量次优误差。具体来说，对于 $\mathbb{R}^d$ 上的高斯鲁棒 $k$ 稀疏均值估计，腐败率 $\epsilon&gt;0$，我们的算法具有样本复杂度 $(k^2/\epsilon^2)\mathrm {polylog}(d/\epsilon)$，在样本多项式时间内运行，并在 $\ell_2$-error $O(\epsilon)$ 内逼近目标平均值。以前的高效算法本质上会产生错误$\Omega(\epsilon \sqrt{\log(1/\epsilon)})$。在技​​术层面，我们在稀疏机制中开发了一种新颖的多维滤波方法，该方法可能会找到其他应用。]]></description>
      <guid>https://arxiv.org/abs/2403.10416</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>综合表格数据的结构化评估</title>
      <link>https://arxiv.org/abs/2403.10424</link>
      <description><![CDATA[arXiv:2403.10424v1 公告类型：交叉
摘要：表格数据很常见，但通常不完整、数据量小，并且由于隐私问题而访问受到限制。合成数据生成提供了潜在的解决方案。有许多指标可用于评估合成表格数据的质量；然而，我们对许多指标缺乏客观、连贯的解释。为了解决这个问题，我们提出了一个具有单一数学目标的评估框架，该目标假设合成数据应该来自与观察数据相同的分布。通过目标的各种结构分解，该框架使我们能够首次推理任何一组指标的完整性，并统一现有指标，包括源于保真度考虑、下游应用程序和基于模型的方法的指标。此外，该框架还激发了无模型基线和一系列新的指标。我们评估结构知情的合成器和由深度学习驱动的合成器。使用我们的结构化框架，我们表明显式表示表格结构的合成数据生成器优于其他方法，尤其是在较小的数据集上。]]></description>
      <guid>https://arxiv.org/abs/2403.10424</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>通过基于区域的稳定对随机森林进行多元高斯逼近</title>
      <link>https://arxiv.org/abs/2403.09960</link>
      <description><![CDATA[arXiv:2403.09960v1 公告类型：交叉
摘要：在数据生成过程的相当温和的规律性假设下，我们基于泊松过程给出的一组训练点推导出随机森林预测的高斯近似界限。我们的方法基于随机森林预测满足称为基于区域稳定的特定几何属性的关键观察。在开发随机森林结果的过程中，我们还在基于区域稳定的泊松过程的一般泛函的多元高斯近似边界上建立了一个可能具有独立兴趣的概率结果。这个一般结果利用了 Malliavin-Stein 方法，并且可能适用于各种相关的统计问题。]]></description>
      <guid>https://arxiv.org/abs/2403.09960</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>不确定性的可解释性：利用神经网络做出值得信赖的决策</title>
      <link>https://arxiv.org/abs/2403.10168</link>
      <description><![CDATA[arXiv:2403.10168v1 公告类型：交叉
摘要：不确定性是任何机器学习模型的一个关键特征，在神经网络中尤其重要，因为神经网络往往过于自信。这种过度自信在分布变化下令人担忧，随着数据分布与训练数据分布的偏离，模型性能会悄然下降。不确定性估计为过度自信的模型提供了解决方案，可以在输出应该（不）可信时进行通信。尽管不确定性估计方法已经开发出来，但它们尚未与可解释人工智能（XAI）领域明确联系起来。此外，运筹学文献忽略了不确定性估计的可操作性部分，也没有考虑分布变化。这项工作提出了一个通用的不确定性框架，具有三重贡献：(i) ML 模型中的不确定性估计被定位为 XAI 技术，给出局部和特定于模型的解释； (ii) 拒绝分类用于通过让人类专家了解不确定的观察结果来减少错误分类； (iii) 该框架应用于教育数据挖掘中受分布变化影响的神经网络的案例研究。 XAI 的不确定性提高了模型在下游决策任务中的可信度，从而在运筹学中产生更具可操作性和鲁棒性的机器学习系统。]]></description>
      <guid>https://arxiv.org/abs/2403.10168</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>关于机器学习重要性加权的简短调查</title>
      <link>https://arxiv.org/abs/2403.10175</link>
      <description><![CDATA[arXiv:2403.10175v1 公告类型：交叉
摘要：重要性加权是统计学和机器学习中的一个基本过程，它在某种意义上根据实例的重要性对目标函数或概率分布进行加权。这个想法的简单性和实用性导致了重要性加权的许多应用。例如，众所周知，在训练和测试分布之间的差异（称为分布偏移）的假设下的监督学习可以通过密度比的重要性加权来保证统计上理想的属性。这项调查总结了重要性加权在机器学习和相关研究中的广泛应用。]]></description>
      <guid>https://arxiv.org/abs/2403.10175</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>概率稳健的可扩展机器学习分类的保形预测</title>
      <link>https://arxiv.org/abs/2403.10368</link>
      <description><![CDATA[arXiv:2403.10368v1 公告类型：新
摘要：保形预测使得定义可靠且鲁棒的学习算法成为可能。但它们本质上是一种评估算法是否足以在实践中使用的方法。为了从设计之初就定义一个可靠的分类学习框架，引入了可扩展分类器的概念，通过将经典分类器与统计顺序理论和概率学习理论联系起来来概括经典分类器的概念。在本文中，我们通过引入得分函数的新定义并定义一组特殊的输入变量（共形安全集）来分析可扩展分类器和共形预测之间的相似性，该集合可以识别输入空间中满足错误覆盖率的模式保证，即属于该集合的点观察到错误（可能不安全）标签的概率受到预定义的 $\varepsilon$ 错误级别的限制。我们通过网络安全中用于识别 DNS​​ 隧道攻击的应用来展示该框架的实际影响。我们的工作有助于开发概率上稳健且可靠的机器学习模型。]]></description>
      <guid>https://arxiv.org/abs/2403.10368</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>覆盖范围有限的混合强化学习在线算法的自然扩展</title>
      <link>https://arxiv.org/abs/2403.09701</link>
      <description><![CDATA[arXiv:2403.09701v1 公告类型：交叉
摘要：利用在线和离线数据的混合强化学习（RL）最近引起了人们的兴趣，但对其可证明的好处的研究仍然很少。此外，许多现有的混合强化学习算法（Song et al., 2023；Nakamoto et al., 2023；Amortila et al., 2024）对离线数据集施加了覆盖假设，但我们证明这是不必要的。一个精心设计的在线算法应该“填补离线数据集中的空白”，探索行为策略没有探索的状态和动作。与之前专注于估计离线数据分布以指导在线探索的方法不同（Li et al., 2023b），我们展示了标准乐观在线算法的自然扩展——通过将离线数据集包含在体验回放中来热启动它们buffer——即使离线数据集不具有单策略集中性，也能从混合数据中获得类似的可证明的收益。我们通过将状态-动作空间分为两部分来实现这一点，通过离线和在线复杂性度量来限制每个分区上的遗憾，并表明这种混合 RL 算法的遗憾可以通过最佳分区来表征——尽管该算法不知道分区本身。作为一个例子，我们提出了 DISC-GOLF，这是对现有乐观在线算法的修改，具有通用函数逼近，称为 GOLF，在 Jin 等人中使用。 （2021）；谢等人。 (2022a)，并表明它比仅在线和仅离线强化学习表现出可证明的收益，在专门用于表格、线性和块 MDP 情况时具有竞争界限。数值模拟进一步验证了我们的理论，即混合数据有助于更有效的探索，支持混合强化学习在各种场景中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.09701</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>一种用于学习哈密顿系统的结构保持核方法</title>
      <link>https://arxiv.org/abs/2403.10070</link>
      <description><![CDATA[arXiv:2403.10070v1 公告类型：新
摘要：提出了一种保留结构的核岭回归方法，该方法允许从由哈密顿向量场的噪声观测构成的数据集中恢复潜在的高维和非线性哈密顿函数。该方法提出了一种封闭式解决方案，可产生出色的数值性能，优于该设置中文献中提出的其他技术。从方法论的角度来看，本文将核回归方法扩展到需要涉及梯度线性函数的损失函数的问题，特别是在此背景下证明了差分再现特性和表示定理。分析了结构保持核估计器和高斯后验均值估计器之间的关系。进行完整的误差分析，使用固定和自适应正则化参数提供收敛率。通过各种数值实验说明了所提出的估计器的良好性能。]]></description>
      <guid>https://arxiv.org/abs/2403.10070</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>用于生存分析的可解释机器学习</title>
      <link>https://arxiv.org/abs/2403.10250</link>
      <description><![CDATA[arXiv:2403.10250v1 公告类型：新
摘要：随着黑盒机器学习模型的传播和快速发展，可解释机器学习（IML）或可解释人工智能（XAI）领域在过去十年中变得越来越重要。这对于生存分析尤其重要，其中 IML 技术的采用促进了敏感领域的透明度、问责制和公平性，例如临床决策过程、靶向治疗的开发、干预措施或其他医疗或保健相关环境。更具体地说，可解释性可以揭示生存模型的潜在偏差和局限性，并提供更数学上合理的方法来理解如何以及哪些特征对预测有影响或构成风险因素。然而，缺乏现成的 IML 方法可能会阻碍公共卫生领域的医生和决策者充分利用机器学习的潜力来预测事件发生时间数据。我们对一般 IML 分类法背景下用于生存分析的 IML 方法的现有有限工作量进行了全面回顾。此外，我们正式详细介绍了如何调整常用的 IML 方法，例如个体条件期望 (ICE)、部分依赖图 (PDP)、累积局部效应 (ALE)、不同的特征重要性度量或 Friedman 的 H 交互统计生存结果。将多种 IML 方法应用于人口统计和健康调查 (DHS) 计划中加纳 5 岁以下儿童死亡率数据的实际数据，为研究人员提供了教程或指南，帮助他们了解如何在实践中利用这些技术来促进理解模型决策或预测。]]></description>
      <guid>https://arxiv.org/abs/2403.10250</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>用于连续高效时间序列建模的粗糙变压器</title>
      <link>https://arxiv.org/abs/2403.10288</link>
      <description><![CDATA[arXiv:2403.10288v1 公告类型：新
摘要：现实世界医疗环境中的时间序列数据通常表现出长期依赖性，并且以不均匀的时间间隔进行观察。在这种情况下，传统的基于序列的循环模型陷入困境。为了克服这个问题，研究人员用基于神经常微分方程的模型替换循环架构来对不规则采样的数据进行建模，并使用基于 Transformer 的架构来解释远程依赖性。尽管这两种方法取得了成功，但对于中等长度或更长的输入序列来说，这两种方法都会产生非常高的计算成本。为了缓解这个问题，我们引入了 Rough Transformer，这是 Transformer 模型的一种变体，它对输入序列的连续时间表示进行操作，并显着降低计算成本，这对于解决医疗环境中常见的远程依赖关系至关重要。特别是，我们提出了多视图签名注意力，它使用路径签名来增强普通注意力并捕获输入数据中的局部和全局依赖性，同时对序列长度和采样频率的变化保持鲁棒性。我们发现，Rough Transformers 始终优于普通的注意力模型，同时在合成和现实世界的时间序列任务上使用一小部分计算时间和内存资源，获得基于神经常微分方程的模型的好处。]]></description>
      <guid>https://arxiv.org/abs/2403.10288</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>估计随机递归树的历史</title>
      <link>https://arxiv.org/abs/2403.09755</link>
      <description><![CDATA[arXiv:2403.09755v1 公告类型：新
摘要：本文研究估计随机递归树中顶点到达顺序的问题。具体来说，我们研究了两个基本模型：均匀依恋模型和线性优先依恋模型。我们提出了一个基于约旦中心性度量的订单估计器，并定义了一系列风险度量来量化排序过程的质量。此外，我们为此问题建立了一个极小极大下界，并证明所提出的估计器几乎是最优的。最后，我们通过数值证明所提出的估计器优于基于度的和谱排序程序。]]></description>
      <guid>https://arxiv.org/abs/2403.09755</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>注意差距：利用群体意识先验提高子群体变化的稳健性</title>
      <link>https://arxiv.org/abs/2403.09869</link>
      <description><![CDATA[arXiv:2403.09869v1 公告类型：新
摘要：机器学习模型在数据分布的子群体变化下通常表现不佳。开发允许机器学习模型更好地概括此类转变的方法对于在现实环境中安全部署至关重要。在本文中，我们开发了一系列神经网络参数的群体感知先验（GAP）分布，这些分布明确有利于在子群体变化下泛化良好的模型。我们设计了一个简单的群体感知先验，只需要访问带有群体信息的一小组数据，并证明使用此先验进行训练可以产生最先进的性能 - 即使仅重新训练先前训练的最后一层非鲁棒模型。群体感知先验在概念上很简单，是对属性伪标签和数据重新加权等现有方法的补充，并为利用贝叶斯推理实现对子群体变化的鲁棒性开辟了有前景的新途径。]]></description>
      <guid>https://arxiv.org/abs/2403.09869</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:25 GMT</pubDate>
    </item>
    </channel>
</rss>