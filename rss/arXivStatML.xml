<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 12 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过混合狄利克雷分布改进证据深度学习</title>
      <link>https://arxiv.org/abs/2402.06160</link>
      <description><![CDATA[本文探讨了一种称为证据深度学习 (EDL) 的现代预测不确定性估计方法，其中训练单个神经网络模型，通过最小化特定目标函数来学习预测分布的元分布。尽管 Bengs 等人最近的研究表现出色，但他们的实证表现却很出色。确定现有方法的一个基本缺陷：即使在无限样本限制下，习得的认知不确定性也可能不会消失。我们通过提供文献中广泛使用的一类目标的统一视图来证实观察结果。我们的分析表明，EDL 方法本质上是通过最小化分布与样本大小无关的目标分布之间的某些分歧度量来训练元分布，从而导致虚假的认知不确定性。基于理论原理，我们建议通过混合狄利克雷分布进行建模并通过变分推理进行学习来学习一致的目标分布。之后，最终的元分布模型从目标模型中提取学习到的不确定性。各种基于不确定性的下游任务的实验结果证明了我们提出的方法的优越性，并说明了习得的认知不确定性的一致性和不一致性所产生的实际影响。]]></description>
      <guid>https://arxiv.org/abs/2402.06160</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>通过潜在部分因果模型揭示多模态对比表示学习</title>
      <link>https://arxiv.org/abs/2402.06223</link>
      <description><![CDATA[多模态对比表示学习方法已被证明在一系列领域取得了成功，部分原因在于它们能够生成复杂现象的有意义的共享表示。为了增强对这些获得的表示的分析和理解的深度，我们引入了专门为多模态数据设计的统一因果模型。通过检查该模型，我们表明多模态对比表示学习擅长识别所提出的统一模型中的潜在耦合变量，直至由不同假设产生的线性或排列变换。我们的研究结果阐明了预训练多模态模型（例如 CLIP）在通过一种令人惊讶的简单但高效的工具：线性独立分量分析来学习解缠结表示方面的潜力。实验证明了我们的研究结果的稳健性，即使假设被违反，并验证了所提出的方法在学习解缠结表示方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.06223</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>参数到可观测映射的算子学习视角</title>
      <link>https://arxiv.org/abs/2402.06031</link>
      <description><![CDATA[参数化物理模型的计算高效替代物在科学和工程中发挥着至关重要的作用。算子学习提供了在功能空间之间映射的数据驱动的代理。然而，可用数据通常只是模型输入的有限维参数化或模型输出的有限可观测值，而不是全场测量。本文以傅里叶神经算子为基础，介绍了能够适应此类有限维输入和输出的傅里叶神经映射（FNM）框架。本文提出了该方法的通用逼近定理。此外，在许多应用中，底层参数到可观测量（PtO）映射是通过无限维算子隐式定义的，例如偏微分方程的解算子。一个自然的问题是，端到端学习 PtO 图还是先学习解算子，然后从全场解中计算可观测量是否更有效。对线性泛函的贝叶斯非参数回归的理论分析（这是一个独立的兴趣）表明，端到端方法实际上可能具有更差的样本复杂性。超越理论，三个非线性 PtO 图的 FNM 近似的数值结果证明了本文采用的算子学习视角的好处。]]></description>
      <guid>https://arxiv.org/abs/2402.06031</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>用于玻尔兹曼密度采样的迭代去噪能量匹配</title>
      <link>https://arxiv.org/abs/2402.06121</link>
      <description><![CDATA[从非标准化概率分布中有效地生成统计上独立的样本（例如多体系统的平衡样本）是科学中的一个基本问题。在本文中，我们提出了迭代去噪能量匹配（iDEM），这是一种迭代算法，它使用一种新颖的随机分数匹配目标，仅利用能量函数及其梯度（没有数据样本）来训练基于扩散的采样器。具体来说，iDEM 在 (I) 来自基于扩散的采样器的高模型密度采样区域和 (II) 在我们的随机匹配目标中使用这些样本以进一步改进采样器之间进行交替。 iDEM 作为内部匹配目标可扩展到高维度，无需模拟，并且不需要 MCMC 样本。此外，通过利用扩散的快速模式混合行为，iDEM 平滑了能量景观，从而实现了摊销采样器的高效探索和学习。我们在一系列任务上评估 iDEM，从标准合成能量函数到不变的 $n$ 体粒子系统。我们表明，所提出的方法在所有指标上都实现了最先进的性能，并且训练速度提高了 2-5 倍，这使其成为第一个在具有挑战性的 55 美元粒子 Lennard-Jones 上使用能量进行训练的方法系统。]]></description>
      <guid>https://arxiv.org/abs/2402.06121</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 PEAK 进行查看：多个数据流均值的顺序非参数复合假设检验</title>
      <link>https://arxiv.org/abs/2402.06122</link>
      <description><![CDATA[我们提出了一种新颖的非参数序贯检验，用于多个数据流均值的复合假设。我们提出的方法，\emph{基于期望的平均资本}（PEAK），建立在测试即投注框架的基础上，并在任何停止时间内提供非渐近 $\alpha$ 级别的测试。 PEAK 在计算上易于处理，并且可以有效地拒绝满足我们非参数假设的所有潜在分布中不正确的假设，从而能够对多个数据流进行联合复合假设检验。我们在强盗设置中的最佳臂识别和阈值识别下对我们的理论结果进行了数值验证，说明了我们的方法相对于最先进的测试方法的计算效率。]]></description>
      <guid>https://arxiv.org/abs/2402.06122</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>NPSVC++：非并行分类器遇到表示学习</title>
      <link>https://arxiv.org/abs/2402.06010</link>
      <description><![CDATA[本文重点介绍称为非并行支持向量分类器 (NPSVC) 的特定分类器系列。与典型的分类器不同，NPSVC 的训练涉及多个目标的最小化，从而导致特征次优和类依赖性的潜在问题。因此，还没有建立有效的学习方案来通过表示学习，特别是深度学习来提高 NPSVC 的性能。为了打破这个瓶颈，我们开发了基于多目标优化的NPSVC++，实现了NPSVC及其特征的端到端学习。 NPSVC++通过追求帕累托最优，理论上保证了跨类特征最优，从而有效克服了上述两个问题。提出了一种通过对偶优化的通用学习过程，在此基础上我们提供了两个适用的实例：K-NPSVC++ 和 D-NPSVC++。实验表明其相对于现有方法的优越性并验证了NPSVC++的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.06010</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>使用全局非凸优化软件检查充分分散的条件</title>
      <link>https://arxiv.org/abs/2402.06019</link>
      <description><![CDATA[充分分散条件（SSC）是研究各种矩阵分解问题可辨识性的关键条件，包括非负、最小体积、对称、单纯形结构和多面矩阵分解问题。 SSC 可以保证计算出的矩阵分解是唯一的/可识别的，甚至可以忽略不计的歧义。然而，这种情况一般来说是NP-难检查的。在本文中，我们表明，当分解秩不太大时，可以在现实场景中在合理的时间内对其进行检查。这是通过将问题表述为有界集上的非凸二次优化问题来实现的。我们使用全局非凸优化软件 Gurobi，并展示了该代码在合成数据集和真实世界高光谱图像上的有用性。]]></description>
      <guid>https://arxiv.org/abs/2402.06019</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>关于随机梯度下降（SGD）的收敛率及其在多武装老虎机的修改策略梯度中的应用</title>
      <link>https://arxiv.org/abs/2402.06388</link>
      <description><![CDATA[当学习率遵循反时间衰减时间表时，我们提出了随机梯度下降（SGD）收敛率的独立证明；接下来，我们将结果应用于具有 $L2$ 正则化的策略梯度 Multi-Armed Bandit (MAB) 的修改形式的收敛。]]></description>
      <guid>https://arxiv.org/abs/2402.06388</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>灵活的无限宽度图卷积网络和表示学习的重要性</title>
      <link>https://arxiv.org/abs/2402.06525</link>
      <description><![CDATA[理解神经网络的常见理论方法是采用无限宽度限制，此时输出变为高斯过程（GP）分布。这称为神经网络高斯过程（NNGP）。然而，NNGP 内核是固定的，只能通过少量的超参数进行调整，从而消除了表示学习的任何可能性。这与有限宽度神经网络形成鲜明对比，有限宽度神经网络通常被认为表现良好，因为它们能够学习表示。因此，在简化神经网络以使其在理论上易于处理的过程中，神经网络GP可能会准确地消除使它们发挥良好作用的因素（表示学习）。这促使我们了解表示学习在一系列图分类任务中是否必要。我们为这项任务开发了一个精确的工具，即图卷积深度内核机。这与 NNGP 非常相似，因为它具有无限的宽度限制并使用内核，但带有一个“旋钮”来控制表示学习的量。我们发现表示学习在图分类任务和异亲节点分类任务中是必要的（从某种意义上说，它提供了显着的性能改进），但在同亲节点分类任务中则不然。]]></description>
      <guid>https://arxiv.org/abs/2402.06525</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>内存高效的视觉变压器：激活感知的混合等级压缩策略</title>
      <link>https://arxiv.org/abs/2402.06004</link>
      <description><![CDATA[随着视觉变压器 (ViT) 越来越多地在计算机视觉领域树立新的基准，它们在推理引擎上的实际部署往往因其巨大的内存带宽和（片上）内存占用要求而受到阻碍。本文通过引入一种激活感知模型压缩方法来解决这一内存限制，该方法使用不同层的选择性低秩权重张量近似来减少 ViT 的参数数量。关键思想是将权重张量分解为两个参数有效张量的和，同时最小化输入激活与原始权重张量的乘积与输入激活与近似张量和的乘积之间的误差。通过采用有效的逐层误差补偿技术（使用层输出损失的梯度），可以进一步细化该近似值。这些技术的结合取得了优异的结果，同时避免了在优化过程的早期陷入浅局部最小值，并在模型压缩和输出精度之间取得了良好的平衡。值得注意的是，所提出的方法将 DeiT-B 的参数数量显着减少了 60%，并且 ImageNet 数据集上的准确率下降了不到 1%，克服了低秩近似中常见的准确率下降问题。除此之外，所提出的压缩技术可以压缩大型 DeiT/ViT 模型，使其具有与较小的 DeiT/ViT 变体大致相同的模型大小，同时产生高达 1.8% 的精度增益。这些结果凸显了我们方法的有效性，为在内存受限的环境中嵌入 ViT 提供了一种可行的解决方案，同时又不影响其性能。]]></description>
      <guid>https://arxiv.org/abs/2402.06004</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>SMC 就是您所需要的：并行强扩展</title>
      <link>https://arxiv.org/abs/2402.06173</link>
      <description><![CDATA[在贝叶斯推理的一般框架中，目标分布只能评估到比例常数。经典的一致贝叶斯方法，例如顺序蒙特卡罗（SMC）和马尔可夫链蒙特卡罗（MCMC）具有无限的时间复杂度要求。我们开发了一种完全并行的顺序蒙特卡罗（pSMC）方法，该方法被证明可以提供并行的强扩展，即如果允许异步进程的数量增长，时间复杂度（和每节点内存）仍然有限。更准确地说，pSMC 的理论收敛速度为 MSE$ = O(1/NR)$，其中 $N$ 表示每个处理器中通信样本的数量，$R$ 表示处理器的数量。特别是，对于适当大的问题相关 $N$，如 $R \rightarrow \infty$，该方法收敛到无限小精度 MSE$=O(\varepsilon^2)$ 并具有固定的有限时间复杂度 Cost$=O (1)$并且没有效率泄漏，即计算复杂度Cost$=O(\varepsilon^{-2})$。考虑了许多贝叶斯推理问题来比较 pSMC 和 MCMC 方法。]]></description>
      <guid>https://arxiv.org/abs/2402.06173</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>粒子去噪扩散采样器</title>
      <link>https://arxiv.org/abs/2402.06320</link>
      <description><![CDATA[去噪扩散模型在生成建模中已变得无处不在。核心思想是通过扩散将数据分布传输到高斯分布。然后，通过使用分数匹配思想估计这种扩散的时间反转，获得数据分布的近似样本。我们在这里遵循类似的策略从非标准化概率密度中采样并计算它们的标准化常数。然而，这里使用依赖于新颖的分数匹配损失的原始迭代粒子方案来模拟时间反转扩散。与标准去噪扩散模型相反，所得的粒子去噪扩散采样器 (PDDS) 在温和的假设下提供渐近一致的估计。我们在多模态和高维采样任务上演示了 PDDS。]]></description>
      <guid>https://arxiv.org/abs/2402.06320</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>用于改进决策树的基于提升的序列元树集成构建</title>
      <link>https://arxiv.org/abs/2402.06386</link>
      <description><![CDATA[决策树是机器学习领域最流行的方法之一。然而，它存在因树深度过深而导致的过拟合问题。然后，最近提出了元树。解决了树木过深导致的过拟合问题。此外，元树保证基于贝叶斯决策理论的统计最优性。因此，元树预计比决策树表现更好。与单个决策树相比，众所周知，决策树集合（通常由增强算法构建）在提高预测性能方面更有效。因此，预计元树的集合在提高预测性能方面比单个元树更有效，并且之前没有在 boosting 中构建多个元树的研究。因此，在本研究中，我们提出了一种使用提升方法构建多个元树的方法。通过对合成数据集和基准数据集的实验，我们对所提出的方法和使用决策树集成的传统方法进行了性能比较。此外，虽然决策树的集成可能会导致过度拟合以及单个决策树，但实验证实元树的集成可以防止由于树深度而导致的过度拟合。]]></description>
      <guid>https://arxiv.org/abs/2402.06386</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>POTEC：通过两阶段策略分解实现大型行动空间的离策略学习</title>
      <link>https://arxiv.org/abs/2402.06151</link>
      <description><![CDATA[我们研究大型离散行动空间中上下文强盗策略的离策略学习（OPL），其中现有方法（其中大多数主要依赖于奖励回归模型或重要性加权策略梯度）由于过度偏差或方差而失败。为了克服 OPL 中的这些问题，我们提出了一种新颖的两阶段算法，称为通过两阶段策略分解进行策略优化 (POTEC)。它利用行动空间中的聚类，并分别通过基于策略和基于回归的方法学习两种不同的策略。特别是，我们推导了一种新颖的低方差梯度估计器，它能够通过基于策略的方法有效地学习用于集群选择的第一阶段策略。为了在第一阶段策略采样的集群内选择特定操作，POTEC 使用从每个集群内基于回归的方法派生的第二阶段策略。我们证明了局部正确性条件，仅要求回归模型保留每个集群内动作的相对预期奖励差异，确保我们的策略梯度估计器是无偏的，并且第二阶段策略是最优的。我们还表明，POTEC 提供了基于策略和回归的方法及其相关假设的严格概括。综合实验表明，POTEC 显着提高了 OPL 有效性，特别是在大型结构化动作空间中。]]></description>
      <guid>https://arxiv.org/abs/2402.06151</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>Wasserstein 近端算子描述了基于分数的生成模型并解决了记忆问题</title>
      <link>https://arxiv.org/abs/2402.06162</link>
      <description><![CDATA[我们专注于基于分数的生成模型（SGM）的基本数学结构。我们首先根据 Wasserstein 近端算子 (WPO) 制定 SGM，并通过平均场博弈 (MFG) 证明，WPO 公式揭示了描述扩散和基于评分模型的归纳偏差的数学结构。特别是，MFG 以一对耦合偏微分方程的形式产生最优条件：前向控制的 Fokker-Planck (FP) 方程和后向的 Hamilton-Jacobi-Bellman (HJB) 方程。通过 Cole-Hopf 变换并利用交叉熵可以与密度的线性函数相关的事实，我们证明 HJB 方程是一个不受控的 FP 方程。其次，利用现有的数学结构，我们提出了一种可解释的基于内核的评分函数模型，该模型在训练样本和训练时间方面显着提高了 SGM 的性能。此外，显式构建了 WPO 通知的内核模型，以避免最近研究的基于分数的生成模型的记忆效应。新的基于核的模型的数学形式与 MFG 终止条件的使用相结合，揭示了对 SGM 的流形学习和泛化特性的新解释，并为其记忆效应提供了解决方案。最后，我们的数学知识、可解释的基于内核的模型为高维应用提出了新的可扩展定制神经网络架构。]]></description>
      <guid>https://arxiv.org/abs/2402.06162</guid>
      <pubDate>Mon, 12 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    </channel>
</rss>