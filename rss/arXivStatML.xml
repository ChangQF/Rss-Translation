<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Fri, 14 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>关于Mahalanobis距离的贝叶斯非参数视角，用于无法分配检测</title>
      <link>https://arxiv.org/abs/2502.08695</link>
      <description><![CDATA[ARXIV：2502.08695V1公告类型：新 
摘要：贝叶斯非参数方法自然适合于分布（OOD）检测问题。但是，这些技术在很大程度上被避免了基于预训练或学到的数据点嵌入之间的距离，而采用了更简单的方法。在这里，我们显示了贝叶斯非参数模型与相对Mahalanobis距离评分（RMDS）之间的形式关系，这是一种常用的OOD检测方法。在这方面，我们提出了贝叶斯非参数混合模型，并具有将RMDS推广的层次先验。我们在OpenOOD检测基准上评估了这些模型，并表明贝叶斯非参数方法可以改善现有的OOD方法，尤其是在训练类别的协方差结构以及每个类别相对较少的数据点的方案中。]]></description>
      <guid>https://arxiv.org/abs/2502.08695</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对缺少随机奖励的建议的非政策评估</title>
      <link>https://arxiv.org/abs/2502.08993</link>
      <description><![CDATA[ARXIV：2502.08993V1公告类型：新 
摘要：无偏见的推荐学习（URL）和非政策评估/学习（OPE/L）技术可有效解决由显示位置和记录策略引起的数据偏差，从而不断提高建议的性能。但是，当两个偏差都在记录的数据中退出时，这些估计器可能会遭受重大偏见。在这项研究中，我们首先分析当奖励丢失而不是随机时，OPE估计器的位置偏差。为了减轻这两种偏见，我们提出了一个新颖的估计值，该估计值利用了记录策略和奖励观察结果作为倾向得分的两个概率。我们的实验表明，与其他估计量相比，提出的估计器的性能较高，即使奖励观察的偏见水平增加。]]></description>
      <guid>https://arxiv.org/abs/2502.08993</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在协变量下的线性回归中的最佳算法：关于先决条件的重要性</title>
      <link>https://arxiv.org/abs/2502.09047</link>
      <description><![CDATA[ARXIV：2502.09047V1公告类型：新 
摘要：现代统计学习中的一种共同追求是从源数据分布（OOD）中获得令人满意的概括。从理论上讲，即使在线性模型的协变量转移的规范环境下，挑战仍然无法解决。本文研究了基础（高维）线性回归，其中地面真相变量局限于椭圆形约束，并在此制度中解决了两个基本问题：（i）给定目标协变量矩阵，最小值\是什么是什么是最小值 - 最小值\什么是什么。 emph {optimal}算法在协变量位置下？ （ii）对于哪种目标类别，通常使用的SGD型算法可实现最佳性？我们的分析始于通过贝叶斯cramer-rao不平等建立紧密的较低概括。对于（i），我们证明了最佳估计器可以只是对源分布最佳估计器的一定线性转换。给定源矩阵，我们表明可以通过凸面程序有效地计算转换。 SGD的最佳最佳分析利用了我们认识到应用算法的累积更新的想法，又是理想的转换为学习变量的前提。当SGD具有其加速度变体达到最佳性时，我们提供了足够的条件。]]></description>
      <guid>https://arxiv.org/abs/2502.09047</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散退火的兰格文蒙特卡洛的非反应分析用于生成建模</title>
      <link>https://arxiv.org/abs/2502.09306</link>
      <description><![CDATA[ARXIV：2502.09306V1公告类型：新 
摘要：我们研究了数据分布条件下，我们研究了一般扩散（插值）路径的理论特性及其Langevin Monte Carlo实施，称为扩散退火Langevin Monte Carlo（DALMC）。具体而言，我们为退火的Langevin动力学分析并提供了非反应误差界限，其中分布路径被定义为数据分布的高斯卷积，如扩散模型中。然后，我们将结果扩展到最近提出的重尾（Student&#39;s T）扩散路径，首次证明了其重型数据分布的理论特性。我们的分析为一类基于分数的生成模型提供了理论保证，这些模型在简单分布（高斯或学生的T）和数据分布之间插值有限时间。与基于标准分数的扩散方法相比，这种方法提供了更广泛的观点，该方法通常基于前向Ornstein-uhlenbeck（OU）Nunising过程。]]></description>
      <guid>https://arxiv.org/abs/2502.09306</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一个基于区分等级的目标，用于更好的功能学习</title>
      <link>https://arxiv.org/abs/2502.09445</link>
      <description><![CDATA[ARXIV：2502.09445V1公告类型：新 
摘要：在本文中，我们利用现有的统计方法来更好地了解数据。我们通过修改\ cite {azadkia2021simple}中引入的条件独立性（foci）的特征排序（foci）来修改特征排序（foci）来解决此问题。尽管FOCI基于条件依赖性的非参数系数，但我们引入了其参数，可区分的近似值。借助这种近似的相关系数，我们提出了一种称为Diffoci的新算法，该算法适用于其可区分的性质和可学习的参数，适用于更广泛的机器学习问题。我们在三个上下文中介绍了Diffoci：（1）作为一种可变选择方法，具有基线比较与焦点的比较，（2）作为具有神经网络参数的可训练模型，（3）作为一种通用的，广泛适用的神经网络正常化器，一种通过更好地管理虚假相关性，可以改善特征学习。我们评估了越来越复杂的问题，从玩具示例中的基本变量选择到卷积网络中的显着性图比较。然后，我们展示如何在不依赖敏感数据的情况下促进分类的情况下融合Diffoci。]]></description>
      <guid>https://arxiv.org/abs/2502.09445</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可扩展的离散扩散采样器：组合优化和统计物理</title>
      <link>https://arxiv.org/abs/2502.08696</link>
      <description><![CDATA[ARXIV：2502.08696V1公告类型：交叉 
摘要：学会从复杂的非平均分布中进行采样，而不是在统计物理学，变异推断和组合优化中应用的有前途的研究方向出现的离散域。最近的工作证明了在该域中扩散模型的潜力。但是，现有方法在内存缩放中面临局限性，因此可以通过整个生成过程进行反向传播，因此可达到的扩散步骤的数量。为了克服这些局限性，我们引入了两种新型的培训方法，用于离散扩散采样器，一种基于策略梯度定理，另一个基于一个利用自称的神经重要性采样（SN-nis）。这些方法产生了记忆有效的训练并实现最新的训练，从而实现了无监督的组合优化。许多科学应用还需要无偏抽样的能力。我们介绍了SN-NIS和神经马尔可夫链蒙特卡洛的适应性，这是首次将离散扩散模型应用于此问题。我们验证了关于ISING模型基准测试的方法，并发现它们表现优于流行的自动回归方法。我们的工作开辟了新的途径，用于将扩散模型应用于迄今仅限于精确可能模型的离散域中的广泛科学应用。]]></description>
      <guid>https://arxiv.org/abs/2502.08696</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稀疏变分高斯过程的新界限</title>
      <link>https://arxiv.org/abs/2502.08730</link>
      <description><![CDATA[ARXIV：2502.08730V1公告类型：交叉 
摘要：稀疏变分高斯过程（GPS）构造了与GP模型的可拖动后近似值。这些方法的核心是假设训练函数的真实后验分布值$ {\ bf f} $和诱导变量$ {\ bf u} $近似于与条件GP先前$ p（ {\ bf f} |。虽然此假设被认为是基本的，但我们表明，对于模型培训，我们可以通过使用更通用的变分发行$ q（{\ bf f} | {\ bf u}）$放松它，该$取决于$ n $ extra参数，其中$ n $是培训示例的数量。在GP回归中，我们可以分析优化在额外参数上的下限证据，并表达比以前的界限更紧密的易变折叠结合。新的界限也适合随机优化，其实现需要对现有的稀疏GP代码进行微小修改。此外，我们还描述了非高斯可能性的扩展。在几个数据集上，我们证明我们的方法可以在学习超帕帕照明器时减少偏差，并可以提高预测性能。]]></description>
      <guid>https://arxiv.org/abs/2502.08730</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在线域间高斯流程的经常记忆</title>
      <link>https://arxiv.org/abs/2502.08736</link>
      <description><![CDATA[ARXIV：2502.08736V1公告类型：交叉 
摘要：我们提出了一个新颖的在线高斯流程（GP）模型，该模型能够在在线回归环境中捕获连续数据中的长期记忆。我们的模型，在线河马稀疏变异高斯过程回归（OHSGPR），利用了河马（高阶多项式投影操作员）框架，该框架由于其远程存储器建模功能而在RNN域中普及。我们将河马时间变化的正交投影解释为具有时间依赖性正交多项式函数的诱导变量，这允许SGPR诱导点记住过程历史记录。我们表明，HIPPO框架自然符合域间GP框架，并证明了内核矩阵也可以基于河马的ODE演变以复发形式在线更新。我们在时间序列回归任务上评估我们的方法，表明它在预测性能和计算效率方面表现优于现有的在线GP方法]]></description>
      <guid>https://arxiv.org/abs/2502.08736</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>治疗反应作为潜在变量</title>
      <link>https://arxiv.org/abs/2502.08776</link>
      <description><![CDATA[ARXIV：2502.08776V1公告类型：交叉 
摘要：科学家通常需要在一项对治疗做出反应的研究中分析样本，以完善其假设并找到潜在的因果关系驱动因素。结局的自然变化使得将反应者与非反应者分开成为统计推断问题。为了处理潜在响应，我们介绍了因果两组（C2G）模型，这是经典两组模型的因果扩展。根据某些先前的概率，C2G模型认为处理样品可能会或可能不会产生效果。我们为因果关系两组模型提出了两个经验贝叶斯程序，一种在半参数条件下，另一个在完全非参数条件下。半参数模型具有加性治疗效果，可以从观察到的数据中识别。非参数模型无法识别，但我们表明它仍然可以用于测试每个处理过的样本中的响应。我们从经验和理论上表明，两种选择响应者的方法都以近乎最佳的力量控制目标水平的错误发现率。我们还提出了两个新颖的感兴趣估计，并提供了一种策略，以推导无法识别的非参数模型中的估计和间隔。在癌症免疫疗法数据集上，非参数C2G模型恢复了阳性和负结果的临床验证预测生物标志物。代码可从https://github.com/tanse-lab/causal2groups获得。]]></description>
      <guid>https://arxiv.org/abs/2502.08776</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型的一阶生成二元优化框架</title>
      <link>https://arxiv.org/abs/2502.08808</link>
      <description><![CDATA[ARXIV：2502.08808V1公告类型：交叉 
摘要：扩散模型，迭代地将数据样本构成高质量输出，已在整个领域取得了经验成功。但是，针对下游任务进行优化这些模型通常涉及嵌套的双层结构，例如在训练动力学中调整超参数以进行微调任务或噪声时间表，在这种动力学中，由于无限二维概率概率空间和过度的采样成本，传统的双光线方法失败了。我们将这一挑战正式化为生成性双重优化问题，并解决了两个关键方案：（1）通过仅推理的仅较低级别求解器进行微调预训练的模型，并与上层的样品有效梯度估计器配对，以及（ 2）通过重新聚集较低级别的问题并设计计算可拖动的梯度估计器，从头开始训练扩散模型，并优化噪声时间表。我们的一阶二线框架克服了传统的二线方法与扩散过程的不相容性，提供了理论基础和计算实用性。实验表明，我们的方法优于现有的微调和超参数搜索基准。]]></description>
      <guid>https://arxiv.org/abs/2502.08808</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散SDE的可逆求解器</title>
      <link>https://arxiv.org/abs/2502.08834</link>
      <description><![CDATA[ARXIV：2502.08834V1公告类型：交叉 
摘要：扩散模型已迅速成为许多不同数据模式的生成任务的最新技术。扩散模型的重要能力是能够从数据分布中编码样本回到先验分布中。这对于通过连续的伴随方程进行对真实数据样本的更改以及指导生成很有用。我们为扩散SDE提出了一个代数可逆的求解器，该求解器可以准确地将实际数据样本倒入先前的分布中。]]></description>
      <guid>https://arxiv.org/abs/2502.08834</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>何时及为什么随机探索起作用（在线性匪徒中）</title>
      <link>https://arxiv.org/abs/2502.08870</link>
      <description><![CDATA[ARXIV：2502.08870V1公告类型：交叉 
摘要：我们提供了一种分析随机探索算法（如汤普森采样）的方法，该算法不依赖于强迫乐观或后膨胀。这样，我们证明在$ d $维的线性匪徒设置中，当动作空间平稳且强烈凸出时，随机探索算法享受$ n $ step的遗憾，以$ o（d \ sqrt {n）{n } \ log（n））$。值得注意的是，这首先表明存在非平凡的线性匪徒设置，汤普森采样可以在遗憾中获得最佳的维度依赖性。]]></description>
      <guid>https://arxiv.org/abs/2502.08870</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Wendy非线性参数ODE</title>
      <link>https://arxiv.org/abs/2502.08881</link>
      <description><![CDATA[ARXIV：2502.08881V1公告类型：交叉 
摘要：非线性动力学（Wendy）算法的弱估计估计可容纳非线性参数（NIP）的普通微分方程系统。扩展基于可能性函数，其梯度和Hessian矩阵的派生分析表达式。 Wendy利用这些使用这些基于适用于非凸优化问题的优化例程来近似最大似然估计量。所得的参数估计算法具有更好的精度，它的收敛范围要大得多，并且通常比常规输出误差最小二乘法（基于前向求解器）快的数量级。
  Wendy.jl算法在朱莉娅有效实施。我们证明了该算法可以适应添加剂正常和乘数对数正常噪声的弱形式优化的能力，并在普通微分方程的基准系统的套件上提供了结果。为了证明我们方法的实际好处，我们就准确性，精度，偏见和覆盖范围的方法和输出误差方法进行了广泛的比较。]]></description>
      <guid>https://arxiv.org/abs/2502.08881</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>线性时间用户级DP-SCO通过强大的统计信息</title>
      <link>https://arxiv.org/abs/2502.08889</link>
      <description><![CDATA[ARXIV：2502.08889V1公告类型：交叉 
摘要：用户级差异化私有随机凸优化（DP-SCO）引起了极大的关注，因为在现代大型机器学习应用程序中保护用户隐私的重要性至关重要。当前的方法，例如基于差异私有随机梯度下降（DP-SGD）的方法，由于需要私有化每个中间迭代的私有化，因此通常在高噪声积累和次优效用方面遇到困难。在这项工作中，我们介绍了一种新型的线性时间算法，该算法利用了强大的统计数据，特别是中位数和修剪的平均值来克服这些挑战。我们的方法独特地界定了基于稳健的统计数据的所有中间迭代的SGD迭代率的敏感性，从而大大降低了隐私目的的梯度估计噪声并增强了隐私 - 实用性权衡。通过避免以前方法所需的重复私有化，我们的算法不仅实现了改进的理论隐私性 - 耐用性权衡，而且还保持了计算效率。我们通过信息理论下限对算法进行补充，这表明我们的上限是对数因素以及对$ \ epsilon $的依赖性的最佳选择。这项工作为机器学习中的更强大，更有效的隐私技术奠定了基础，对现场的未来研究和应用产生了影响。]]></description>
      <guid>https://arxiv.org/abs/2502.08889</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>要了解为什么数据扩展改善概括</title>
      <link>https://arxiv.org/abs/2502.08940</link>
      <description><![CDATA[ARXIV：2502.08940V1公告类型：交叉 
摘要：数据增强是深度学习中的基石技术，广泛用于改善模型概括。传统方法等传统方法以及诸如切割，混合和cutmix之类的先进技术等传统方法在各个领域都取得了显着的成功。但是，数据增强改善概括的机制仍然很少理解，现有的理论分析通常集中在没有统一解释的情况下。在这项工作中，我们提出了一个统一的理论框架，该框架阐明了数据增强如何通过两个关键效果增强概括：部分语义特征去除和特征混合。部分语义特征去除可以减少模型对单个功能的依赖，从而促进多样化的特征学习和更好的概括。通过缩小原始语义特征并引入噪音，功能混合可以提高训练的复杂性，并推动模型开发更强大的功能。诸如cutmix之类的高级方法综合了这两种效果，从而获得了互补的好处。实验结果进一步支持了我们的理论见解，从而验证了这种统一观点的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.08940</guid>
      <pubDate>Fri, 14 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>