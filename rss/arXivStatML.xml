<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>随机 LR 模糊区间的模拟</title>
      <link>https://arxiv.org/abs/2501.10482</link>
      <description><![CDATA[arXiv:2501.10482v1 公告类型：新
摘要：随机模糊变量结合了不精确性（由于其“模糊部分”）和随机性的建模。此类对象的统计样本被广泛使用，因此需要直接、数值有效地生成它们。通常，这些样本由三角或梯形模糊数组成。在本文中，我们描述了另一类模糊数——具有区间值核的 LR 模糊数的理论结果和模拟算法。从具有区间值核的分段线性 LR 模糊数的模拟角度开始，然后考虑它们的极限行为。这使我们得到了用于模拟由此类模糊值组成的样本的数值有效算法。]]></description>
      <guid>https://arxiv.org/abs/2501.10482</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有分数和混合激活函数的对称神经网络算子的扩展</title>
      <link>https://arxiv.org/abs/2501.10496</link>
      <description><![CDATA[arXiv:2501.10496v1 公告类型：新
摘要：我们通过结合分数和混合激活函数提出了一种对称神经网络算子的新扩展。这项研究解决了现有模型在近似高阶平滑函数方面的局限性，特别是在复杂和高维空间中。我们的框架在激活函数中引入了分数指数，允许自适应非线性近似并提高精度。我们基于 $q$ 变形和 $\theta$ 参数化逻辑模型定义新的密度函数，并推导出建立统一收敛速度的高级 Jackson 型不等式。此外，我们为所提出的算子提供了严格的数学基础，并通过数值验证证明了它们在处理振荡和分数分量方面的效率。结果将神经网络近似理论的适用性扩展到更广泛的函数空间，为解决偏微分方程和建模复杂系统的应用铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2501.10496</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多输出共形回归：与新共形评分的统一比较研究</title>
      <link>https://arxiv.org/abs/2501.10533</link>
      <description><![CDATA[arXiv:2501.10533v1 公告类型：新
摘要：量化多元回归中的不确定性在许多实际应用中都是必不可少的，但现有的构建预测区域的方法往往面临一些限制，例如无法捕获复杂的依赖关系、缺乏覆盖保证或计算成本高。共形预测提供了一个强大的框架，用于生成具有有限样本覆盖保证的无分布预测区域。在这项工作中，我们对多输出共形方法进行了统一的比较研究，探索了它们的性质和相互联系。根据我们的研究结果，我们引入了两类实现渐近条件覆盖的一致性分数：一类与任何生成模型兼容，另一类通过利用可逆生成模型提供低计算成本。最后，我们对 32 个表格数据集进行了全面的实证研究，以比较本研究中考虑的所有多输出共形方法。所有方法都在统一的代码库中实现，以确保公平一致的比较。]]></description>
      <guid>https://arxiv.org/abs/2501.10533</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DPERC：混合数据的直接参数估计</title>
      <link>https://arxiv.org/abs/2501.10540</link>
      <description><![CDATA[arXiv:2501.10540v1 公告类型：新
摘要：协方差矩阵是众多统计和机器学习应用的基础，例如主成分分析、相关热图等。然而，数据集中的缺失值对准确估计该矩阵造成了巨大的障碍。虽然插补方法提供了解决这一挑战的一种途径，但它们通常需要在计算效率和估计精度之间进行权衡。因此，考虑到直接参数估计的精确性和减少的计算负担，人们的注意力已转向直接参数估计。在本文中，我们提出了具有分类特征的随机缺失数据的直接参数估计 (DPERC)，这是一种针对包含连续特征中缺失值的混合数据的直接参数估计的有效方法。我们的方法旨在利用分类特征中的信息，这可以显着增强连续特征的协方差矩阵估计。我们的方法有效地利用了混合数据结构中嵌入的信息。通过对各种数据集的全面评估，我们证明了 DPERC 与各种当代技术相比具有竞争力。此外，我们还通过实验表明，DPERC 是可视化关联热图的有价值的工具。]]></description>
      <guid>https://arxiv.org/abs/2501.10540</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于解决非参数回归中概念转变的模型稳健和自适应最优迁移学习</title>
      <link>https://arxiv.org/abs/2501.10870</link>
      <description><![CDATA[arXiv:2501.10870v1 公告类型：新
摘要：当目标领域中存在概念转移和样本稀缺时，非参数回归学习者通常难以有效地概括。迁移学习技术通过利用来自相似源域的数据或预训练模型来解决这些问题。虽然现有的基于核的迁移学习的泛化分析通常依赖于正确指定的模型，但我们提出了一种迁移学习程序，该程序对模型错误指定具有鲁棒性，同时自适应地实现最优性。为了促进我们的分析并避免在经典错误指定结果中发现的饱和风险，我们在错误指定的单任务学习设置中建立了一个新结果，表明具有固定带宽高斯核的谱算法可以在真实函数位于 Sobolev 空间中的情况下实现极小极大收敛率，这可能是独立的兴趣。在此基础上，我们推导出在流行的假设迁移学习算法中指定高斯核的超额风险的自适应收敛率。我们的结果是针对对数因子的极小极大最优，并阐明了传输效率的关键决定因素。]]></description>
      <guid>https://arxiv.org/abs/2501.10870</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过拓扑表示证明稳健性</title>
      <link>https://arxiv.org/abs/2501.10876</link>
      <description><![CDATA[arXiv:2501.10876v1 公告类型：新
摘要：我们提出了一种神经网络架构，可以从持久性图中学习数据的判别几何表示，持久性图是拓扑数据分析的常用描述符。学习到的表示具有可控的 Lipschitz 常数，具有 Lipschitz 稳定性。在对抗性学习中，这种稳定性可用于证明数据集中样本的 $\epsilon$ 稳健性，我们在表示离散动力系统轨道的 ORBIT5K 数据集上进行了演示。]]></description>
      <guid>https://arxiv.org/abs/2501.10876</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的神经切线核方法存在的问题</title>
      <link>https://arxiv.org/abs/2501.10929</link>
      <description><![CDATA[arXiv:2501.10929v1 公告类型：新
摘要：神经切线核 (NTK) 已被提出用于从高斯过程的角度研究训练后的神经网络的行为。这项工作的一个重要结果是训练后的神经网络与具有相应 NTK 的核回归之间的等价定理。该定理允许将神经网络解释为核回归的特殊情况。但是，这个等价定理在实践中成立吗？
在本文中，我们严格重新审视 NTK 的推导，并进行数值实验来评估这个等价定理。我们观察到，在神经网络中添加一层和相应的更新 NTK 不会产生匹配的预测误差变化。此外，我们观察到文献中不​​考虑神经网络训练的高斯过程核的核回归产生的预测误差非常接近使用 NTK 的核回归。这些观察结果表明等价定理在实践中并不成立，并对神经正切核是否能充分解决神经网络的训练过程提出了质疑。]]></description>
      <guid>https://arxiv.org/abs/2501.10929</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Contextual-LSBM 社区检测：误分类率的理论限制和有效算法</title>
      <link>https://arxiv.org/abs/2501.11139</link>
      <description><![CDATA[arXiv:2501.11139v1 公告类型：新
摘要：网络信息和节点属性信息的整合最近在社区恢复问题的背景下引起了广泛关注。在这项工作中，我们解决了确定具有节点属性信息的 Label-SBM (LSBM) 模型的最佳分类率的任务。具体来说，我们推导出最佳下限，其特征是具有高斯节点属性的一般 LSBM 网络模型的 Chernoff-Hellinger 散度。此外，我们强调了我们模型中的散度 $D(\bs\alpha, \mb P, \bs\mu)$ 与 \cite{yun2016optimal} 和 \cite{lu2016statistical} 中引入的散度之间的联系。我们还提出了一种基于谱方法的一致算法，用于提出的聚合潜在因子模型。]]></description>
      <guid>https://arxiv.org/abs/2501.11139</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用对抗性随机森林进行生成建模的条件特征重要性</title>
      <link>https://arxiv.org/abs/2501.11178</link>
      <description><![CDATA[arXiv:2501.11178v1 公告类型：新
摘要：本文提出了一种通过生成模型测量条件特征重要性的方法。在可解释人工智能 (XAI) 中，条件特征重要性评估在给定其他特征信息的情况下，某个特征对预测模型性能的影响。与模型无关的事后方法通常会评估在流形特征值操作下预测性能的变化。此类过程需要创建尊重条件特征分布的特征值，这在实践中可能具有挑战性。生成模型的最新进展可以促进这一点。对于可能由分类特征和连续特征组成的表格数据，对抗性随机森林 (ARF) 是一种生成模型，它可以生成流形数据点而无需大量调整工作或计算资源，使其成为 XAI 方法中子程序的有希望的候选模型。本文提出了 cARFi（条件 ARF 特征重要性），这是一种通过从 ARF 估计的条件分布中采样的特征值来测量条件特征重要性的方法。cARFi 只需进行少量调整即可产生稳健的重要性分数，该分数可以灵活地适应特征重要性的条件或边际概念，包括对特征子集条件的直接扩展，并允许通过统计测试推断特征重要性的重要性。]]></description>
      <guid>https://arxiv.org/abs/2501.11178</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越 R 重心：Stiefel 和 Grassmann 流形上的有效平均方法</title>
      <link>https://arxiv.org/abs/2501.11555</link>
      <description><![CDATA[arXiv:2501.11555v1 公告类型：新
摘要：本文解决了流形上的平均数据问题。虽然黎曼几何产生的 Fr\&#39;echet 平均值看起来很理想，但不幸的是它并不总是可用，而且计算成本通常非常高。为了克服这个问题，R-barycenters 已被提出并成功应用于 Stiefel 和 Grassmann 流形。然而，R-barycenters 仍然存在严重的局限性，因为它们依赖于迭代算法和复杂的运算符。我们提出了更简单但有效的重心，我们称之为 RL-barycenters。我们表明，在与大多数应用相关的设置中，我们的框架产生了令人惊讶的简单重心：投影到流形上的算术平均值。我们将这种方法应用于 Stiefel 和 Grassmann 流形。在模拟数据上，我们的方法与现有的平均方法相比具有竞争力，同时计算成本更低。]]></description>
      <guid>https://arxiv.org/abs/2501.11555</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯神经网络可以做出可靠的预测吗？</title>
      <link>https://arxiv.org/abs/2501.11773</link>
      <description><![CDATA[arXiv:2501.11773v1 公告类型：新
摘要：贝叶斯推理有望为神经网络预测的原则性不确定性量化提供一个框架。采用的​​障碍包括难以完全表征网络参数的后验分布和后验预测分布的可解释性。我们证明，在内层权重的离散化先验下，我们可以将后验预测分布准确地表征为高斯混合。此设置允许我们定义产生相同可能性（训练误差）的网络参数值的等价类，并将这些类的元素与网络的缩放机制相关联——通过训练样本大小、每层的大小和最终层参数的数量的比率来定义。特别令人感兴趣的是映射到低训练误差但对应于后验预测分布中不同模式的不同参数实现。我们确定了表现出这种预测多模态性的设置，从而深入了解了单峰后验近似的准确性。我们还通过评估不同缩放方案中后验预测的收缩来描述模型“从数据中学习”的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.11773</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于混合模型、最大似然和熵最优传输之间关系的注记</title>
      <link>https://arxiv.org/abs/2501.12005</link>
      <description><![CDATA[arXiv:2501.12005v1 公告类型：新
摘要：本说明旨在证明对混合模型进行最大似然估计相当于最小化具有熵正则化的最佳传输问题的参数。目标是教学：我们试图以简洁且希望简单的方式呈现这个已知的结果。我们通过展示标准 EM 算法是最佳传输损失的特定块坐标下降来用高斯混合模型进行说明。]]></description>
      <guid>https://arxiv.org/abs/2501.12005</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>因子图中的对偶 NUP 表示和最小最大化</title>
      <link>https://arxiv.org/abs/2501.12113</link>
      <description><![CDATA[arXiv:2501.12113v1 公告类型：新
摘要：具有未知参数的法线 (NUP) 可用于将非平凡的基于模型的估计问题转换为线性最小二乘或高斯估计问题的迭代。在本文中，我们通过用凸对偶变量和相关 NUP 表示增强因子图来扩展这种方法。具体而言，在状态空间设置中，我们提出了一种新的迭代前向后向算法，该算法与最近提出的后向前向算法是对偶的。]]></description>
      <guid>https://arxiv.org/abs/2501.12113</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机迭代算法缩放极限的定量误差界限</title>
      <link>https://arxiv.org/abs/2501.12212</link>
      <description><![CDATA[arXiv:2501.12212v1 公告类型：新
摘要：随机迭代算法，包括随机梯度下降 (SGD) 和随机梯度朗之万动力学 (SGLD)，被广泛用于机器学习、统计和工程中大规模和高维问题的优化和采样。许多工作已经限制了这些近似中的参数误差并描述了这些近似的不确定性。一种常见的方法是使用缩放极限分析将算法样本路径的分布与连续时间随机过程近似联系起来，特别是在渐近设置中。本文着眼于单变量设置，在以前的工作的基础上，我们使用无限维版本的 Stein 可交换对方法，推导出算法样本路径和 Ornstein-Uhlenbeck 近似之间的非渐近函数近似误差界限。我们表明，在适度的附加假设下，该界限意味着弱收敛，并导致算法迭代平均值方差的误差有界限。此外，我们使用主要结果根据两个常用指标构建误差界限：L\&#39;{e}vy-Prokhorov 和有界 Wasserstein 距离。我们的结果为开发多变量设置和更复杂的随机近似算法的类似误差界限奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2501.12212</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有数百万至数十亿个参数的高斯混合模型的亚线性变分优化</title>
      <link>https://arxiv.org/abs/2501.12299</link>
      <description><![CDATA[arXiv:2501.12299v1 公告类型：新
摘要：高斯混合模型 (GMM) 是最常用的机器学习模型之一。然而，对于具有许多高维 $D$ 数据点 $N$ 的数据集，训练大型通用 GMM 在计算上会变得非常困难。对于具有任意协方差的 GMM，我们在此推导出一种高效的变分近似，它与因子分析器混合 (MFA) 相结合。对于具有 $C$ 个组件的 GMM，我们提出的算法显着降低了每次迭代的运行时间复杂度，从 $\mathcal{O}(NCD^2)$ 降低到与 $D$ 线性扩展的复杂度，并且相对于 $C$ 保持不变。然后对这种理论复杂性降低的数值验证显示以下内容：整个 GMM 优化过程所需的距离评估与 $NC$ 呈亚线性关系。在大规模基准测试中，与最先进的方法相比，这种次线性可使速度提高一个数量级。作为概念验证，我们在约 1 亿张图像上训练了具有超过 100 亿个参数的 GMM，并在单个最先进的 CPU 上观察到大约 9 小时的训练时间。]]></description>
      <guid>https://arxiv.org/abs/2501.12299</guid>
      <pubDate>Wed, 22 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>