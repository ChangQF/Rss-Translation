<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 04 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>位置尺度和形状的神经加性模型：超越均值的可解释神经回归框架</title>
      <link>https://arxiv.org/abs/2301.11862</link>
      <description><![CDATA[arXiv:2301.11862v2 公告类型：替换
摘要：深度神经网络（DNN）已被证明在各种任务中都非常有效，使其成为解决需要高水平预测能力的问题的首选方法。尽管取得了如此成功，但 DNN 的内部工作原理通常并不透明，这使得它们难以解释或理解。这种可解释性的缺乏导致近年来对本质上可解释的神经网络的研究不断增加。神经加性模型 (NAM) 等模型通过经典统计方法与 DNN 的结合实现视觉可解释性。然而，这些方法仅集中于平均响应预测，而忽略了基础数据响应分布的其他属性。我们提出了位置尺度和形状的神经加性模型（NAMLSS），这是一种建模框架，它将经典深度学习模型的预测能力与分布回归的固有优势相结合，同时保持加性模型的可解释性。该代码可从以下链接获取：https://github.com/AnFreTh/NAMpy]]></description>
      <guid>https://arxiv.org/abs/2301.11862</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:01 GMT</pubDate>
    </item>
    <item>
      <title>项目反应理论模型的可扩展学习</title>
      <link>https://arxiv.org/abs/2403.00680</link>
      <description><![CDATA[arXiv:2403.00680v1 公告类型：交叉
摘要：项目反应理论（IRT）模型旨在评估 $n$ 考生的潜在能力以及来自分类数据的 $m$ 测试项目的潜在难度特征，表明其相应答案的质量。传统的心理测量评估是基于相对较少数量的考生和项目，例如，一个由 200 美元的学生组成的班级正在解决包含 10 美元问题的考试。最近的全球大规模评估，例如 PISA 或互联网研究，可能会导致参与者数量显着增加。此外，在机器学习的背景下，算法扮演考生的角色，数据分析问题扮演项目的角色，$n$ 和 $m$ 都可能变得非常大，对计算的效率和可扩展性提出挑战。为了从大数据中学习 IRT 模型中的潜在变量，我们利用这些模型与逻辑回归的相似性，可以使用称为核心集的小加权子集来精确近似。我们开发了核心集，用于交替 IRT 训练算法，促进从大数据中进行可扩展的学习。]]></description>
      <guid>https://arxiv.org/abs/2403.00680</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>跟随正规化领导者的自适应学习率：竞争比分析和两全其美</title>
      <link>https://arxiv.org/abs/2403.00715</link>
      <description><![CDATA[arXiv:2403.00715v1 公告类型：交叉
摘要：跟随正则化领导者（FTRL）被认为是在线学习中一种有效且通用的方法，其中适当选择学习率对于减少遗憾至关重要。为此，我们将调整 FTRL 学习率的问题表述为顺序决策问题，并引入竞争分析框架。我们建立了竞争比的下限，并提出了学习率的更新规则，以在该下限的恒定因子内实现上限。具体来说，我们说明最佳竞争比的特征是惩罚项的组成部分的（近似）单调性，表明如果惩罚项的组成部分形成单调非递增序列，则可以实现恒定的竞争比，并得出当惩罚条件为 $\xi$ 时，竞争比率接近单调非递增。我们提出的更新规则，称为 \textit{稳定性惩罚匹配}，也有助于为随机和对抗环境构建两全其美（BOBW）算法。在这些环境中，我们的结果有助于实现更严格的后悔界限，并扩大算法对各种设置的适用性，例如多臂老虎机、图老虎机、线性老虎机和上下文老虎机。]]></description>
      <guid>https://arxiv.org/abs/2403.00715</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:00 GMT</pubDate>
    </item>
    <item>
      <title>间接参数化具体自动编码器</title>
      <link>https://arxiv.org/abs/2403.00563</link>
      <description><![CDATA[arXiv:2403.00563v1 公告类型：交叉
摘要：在数据高维或获取全套特征成本高昂的情况下，特征选择是一项至关重要的任务。基于神经网络的嵌入式特征选择的最新发展在广泛的应用中显示出了有希望的结果。具体自动编码器（CAE）被认为是嵌入式特征选择中最先进的，但可能难以实现稳定的联合优化，从而损害了它们的训练时间和泛化能力。在这项工作中，我们发现这种不稳定性与 CAE 学习重复选择相关。为了解决这个问题，我们提出了一种简单而有效的改进：间接参数化 CAE（IP-CAE）。 IP-CAE 学习嵌入以及从其到 Gumbel-Softmax 分布参数的映射。尽管实现起来很简单，但 IP-CAE 在多个用于重建和分类的数据集上的泛化和训练时间方面都比 CAE 表现出显着且一致的改进。与 CAE 不同，IP-CAE 有效地利用了非线性关系，并且不需要重新训练联合优化的解码器。此外，我们的方法原则上可以推广到特征选择之外的 Gumbel-Softmax 分布。]]></description>
      <guid>https://arxiv.org/abs/2403.00563</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:59 GMT</pubDate>
    </item>
    <item>
      <title>最大切片 Wasserstein 距离的锐界</title>
      <link>https://arxiv.org/abs/2403.00666</link>
      <description><![CDATA[arXiv:2403.00666v1 公告类型：交叉
摘要：我们获得了可分离希尔伯特空间上的概率测度与其来自 $n$ 个样本的经验分布之间的预期最大切片 1-Wasserstein 距离的尖锐上限和下限。还获得了巴纳赫空间上概率测量结果的一个版本。]]></description>
      <guid>https://arxiv.org/abs/2403.00666</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:59 GMT</pubDate>
    </item>
    <item>
      <title>差分隐私的平移插值</title>
      <link>https://arxiv.org/abs/2403.00278</link>
      <description><![CDATA[arXiv:2403.00278v1 公告类型：交叉
摘要：噪声梯度下降及其变体是差分隐私机器学习的主要算法。量化其隐私泄露是一个基本问题，但即使在凸损失的基本背景下，严格的表征仍然是开放的。本文通过在 $f$（差异隐私）的统一框架中建立（和完善）“迭代放大隐私”现象，改进了之前的分析，它紧密地捕捉了隐私损失的各个方面，并立即意味着其他领域更严格的隐私核算。差分隐私的概念，例如 $(\varepsilon,\delta)$-DP 和 Renyi DP。我们的关键技术见解是构建移位插值过程，该过程解开了流行的移位散度论点，使泛化超越了基于散度的 DP 松弛。值得注意的是，这导致了在强凸优化的基础设置中的第一个精确的隐私分析。我们的技术扩展到许多设置：凸/强凸、受约束/无约束、完整/循环/随机批次及其所有组合。作为直接的推论，我们恢复了 Gopi 等人中强凸优化指数机制的 $f$-DP 表征。 （2022），并将这一结果扩展到更一般的设置。]]></description>
      <guid>https://arxiv.org/abs/2403.00278</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:58 GMT</pubDate>
    </item>
    <item>
      <title>Epsilon-Greedy Thompson 采样到贝叶斯优化</title>
      <link>https://arxiv.org/abs/2403.00540</link>
      <description><![CDATA[arXiv:2403.00540v1 公告类型：交叉
摘要：汤普森采样（TS）是解决贝叶斯优化（BO）中利用探索困境的解决方案。虽然 TS 通过随机生成和最大化高斯过程 (GP) 后验的样本路径来优先考虑探索，但 TS 通过在每次探索执行后收集有关真实目标函数的信息来弱管理其利用。在本研究中，我们将强化学习中成熟的选择策略 epsilon-greedy ($\varepsilon$-greedy) 策略纳入 TS 中，以改进其利用。我们首先描述了应用于 BO 的 TS 的两个极端，即通用 TS 和样本平均 TS。前者和后者分别促进勘探和开采。然后我们使用$\varepsilon$-贪婪策略在两个极端之间随机切换。 $\varepsilon \in (0,1)$ 的较小值会优先考虑利用，反之亦然。我们凭经验表明，具有适当 $\varepsilon$ 的 $\varepsilon$ 贪婪 TS 优于其两个极端之一，并与另一个极端竞争。]]></description>
      <guid>https://arxiv.org/abs/2403.00540</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:58 GMT</pubDate>
    </item>
    <item>
      <title>通过恢复潜在变量进行替代调整</title>
      <link>https://arxiv.org/abs/2403.00202</link>
      <description><![CDATA[arXiv:2403.00202v1 公告类型：交叉
摘要：去混杂器被提出作为一种在具有多种原因和未观察到的混杂因素的情况下估计因果参数的方法。它基于从观察到的原因中恢复潜在变量。我们将因果解释与统计估计问题分开，并表明一般估计中的解混因素调整了回归目标参数。它通过针对恢复的潜在变量（称为替代品）进行调整的结果回归来实现这一点。我们将去除因果假设的通用算法称为替代调整。我们给出理论结果来支持当回归量在给定潜在变量的情况下条件独立时替代调整估计调整回归参数。我们还引入了替代调整算法的一种变体，该算法用最小的模型假设来估计假设倾斜的目标参数。然后，在潜在变量取有限集合中的值的情况下，我们给出有限样本界限和支持替代调整估计的渐近结果。模拟研究说明了替代调整的有限样本特性。我们的结果支持，当回归变量的潜变量模型成立时，替代调整是调整回归的可行方法。]]></description>
      <guid>https://arxiv.org/abs/2403.00202</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:57 GMT</pubDate>
    </item>
    <item>
      <title>关于循环 MCMC 采样</title>
      <link>https://arxiv.org/abs/2403.00230</link>
      <description><![CDATA[arXiv:2403.00230v1 公告类型：交叉
摘要：循环MCMC是Zhang等人最近提出的一种新颖的MCMC框架。 （2019）解决高维多模态后验分布（例如深度学习中出现的分布）带来的挑战。该算法的工作原理是生成一个非齐次马尔可夫链，该链随时间循环跟踪目标分布的调节版本。我们在这项工作中表明，在所使用的马尔可夫核是快速混合并且使用足够长的周期的设置中，循环 MCMC 收敛到所需的概率分布。然而，在更常见的慢速混合内核设置中，算法可能无法根据所需的分布生成样本。特别是，在方差不等的简单混合示例中，我们通过模拟表明循环 MCMC 无法收敛到所需的极限。最后，我们表明，即使我们没有收敛到目标，循环 MCMC 通常也能很好地估计每个模式周围目标分布的局部形状。]]></description>
      <guid>https://arxiv.org/abs/2403.00230</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:57 GMT</pubDate>
    </item>
    <item>
      <title>定义专业知识：在治疗效果估计中的应用</title>
      <link>https://arxiv.org/abs/2403.00694</link>
      <description><![CDATA[arXiv:2403.00694v1 公告类型：新
摘要：决策者通常是其领域的专家，并根据其领域知识采取行动。例如，医生可以通过预测每种可用治疗的可能结果来制定治疗方案。因此，专家的行为自然地编码了他们的部分领域知识，并且可以帮助在同一领域内做出推论：知道医生试图为患者开出最好的治疗方法，我们可以知道更频繁地开出的治疗方法可能会更有效。然而在机器学习中，大多数决策者都是专家这一事实经常被忽视，并且“专业知识”很少被用作归纳偏见。对于治疗效果估计的文献尤其如此，其中通常对行动做出的唯一假设是重叠。在本文中，我们认为专业知识 - 特别是某个领域的决策者可能拥有的专业知识类型 - 可以为设计和选择治疗效果估计方法提供信息。我们正式定义了两种类型的专业知识：预测性和预后性，并凭经验证明：（i）某个领域的突出专业知识类型显着影响不同方法在治疗效果估计中的表现，以及（ii）可以预测数据集中存在的专业知识类型，可以为模型选择提供定量基础。]]></description>
      <guid>https://arxiv.org/abs/2403.00694</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:56 GMT</pubDate>
    </item>
    <item>
      <title>统计中基于变压器的参数估计</title>
      <link>https://arxiv.org/abs/2403.00019</link>
      <description><![CDATA[arXiv:2403.00019v1 公告类型：交叉
摘要：参数估计是统计学中最重要的任务之一，是帮助人们理解观测样本背后的分布的关键。传统上，参数估计是通过闭式解（例如，高斯分布的最大似然估计）来完成的，或者当闭式解不存在时（例如，对于 Beta 分布），通过迭代数值方法（例如牛顿-拉夫森方法）来完成。
  在本文中，我们提出了一种基于变压器的参数估计方法。与现有的解决方案相比，我们的方法不需要封闭式解决方案或任何数学推导。它甚至不需要知道数值方法所需的概率密度函数。训练 Transformer 模型后，只需要一次推理即可根据观测样本估计基础分布的参数。在实证研究中，我们将我们的方法与常用分布（例如正态分布、指数分布和 beta 分布）的最大似然估计进行了比较。结果表明，我们的方法达到了与均方误差测量相似或更好的精度。]]></description>
      <guid>https://arxiv.org/abs/2403.00019</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:56 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络的“无损”压缩：一种高维神经切线核方法</title>
      <link>https://arxiv.org/abs/2403.00258</link>
      <description><![CDATA[arXiv:2403.00258v1 公告类型：新
摘要：现代深度神经网络（DNN）非常强大；然而，这是以增加深度和每层具有更多参数为代价的，这使得它们的训练和推理在计算上更具挑战性。为了解决这一关键限制，人们致力于对这些大规模机器学习模型进行压缩（例如稀疏化和/或量化），以便将它们部署在低功耗物联网设备上。在本文中，基于神经正切核（NTK）和随机矩阵理论（RMT）的最新进展，我们为宽且全连接的\emph{deep}神经网络提供了一种新颖的压缩方法。具体来说，我们证明，在数据点的数量 $n$ 及其维度 $p$ 都很大的高维状态下，并且在数据的高斯混合模型下，之间存在 \emph{渐近谱等价} DNN 模型大族的 NTK 矩阵。这一理论结果使得能够对给定的 DNN 执行“无损”压缩，即压缩网络渐近地产生与原始（密集且未量化）网络相同的 NTK，其权重和激活值取值 \emph{only}在 $\{ 0, \pm 1 \}$ 中进行缩放。对合成数据和真实数据进行了实验，以支持所提出的压缩方案的优点，代码可在 \url{https://github.com/Model-Compression/Lossless_Compression} 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.00258</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:55 GMT</pubDate>
    </item>
    <item>
      <title>使用模拟参考值验证 ML-UQ 校准统计数据：灵敏度分析</title>
      <link>https://arxiv.org/abs/2403.00423</link>
      <description><![CDATA[arXiv:2403.00423v1 公告类型：新
摘要：一些流行的机器学习不确定性量化（ML-UQ）校准统计量没有预定义的参考值，主要用于比较研究。因此，校准几乎从未得到验证，诊断结果留给读者自行判断。已经提出基于从实际不确定性导出的合成校准数据集的模拟参考值来缓解这个问题。由于合成误差模拟的生成概率分布通常不受约束，因此模拟参考值对生成分布选择的敏感性可能存在问题，从而对校准诊断产生疑问。这项研究探讨了这个问题的各个方面，并表明当生成分布未知时，一些统计数据对用于验证的生成分布的选择过于敏感。例如，绝对误差和不确定性 (CC) 之间的相关系数以及预期归一化校准误差 (ENCE) 就是这种情况。提出了一种处理模拟参考值的稳健验证工作流程。]]></description>
      <guid>https://arxiv.org/abs/2403.00423</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:55 GMT</pubDate>
    </item>
    <item>
      <title>高度非均匀采样下低秩矩阵完成的特定条目界限</title>
      <link>https://arxiv.org/abs/2403.00184</link>
      <description><![CDATA[arXiv:2403.00184v1 公告类型：新
摘要：低秩矩阵补全涉及使用一组稀疏的观察到的条目来估计矩阵中未观察到的条目的问题。我们考虑非均匀设置，其中观察到的条目以高度变化的概率进行采样，可能具有不同的渐近缩放。我们表明，在结构化采样概率下，在较小的子矩阵而不是整个矩阵上运行估计算法通常更好，有时甚至是最优的。特别是，我们证明了为每个条目定制的误差上限，在某些条件下与极小极大下限相匹配。我们的界限将估计每个条目的难度描述为局部采样概率的函数。我们提供数值实验来证实我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2403.00184</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:54 GMT</pubDate>
    </item>
    <item>
      <title>具有一般因果模型和干预措施的因果强盗</title>
      <link>https://arxiv.org/abs/2403.00233</link>
      <description><![CDATA[arXiv:2403.00233v1 公告类型：新
摘要：本文考虑了因果强盗（CB）用于因果系统干预措施的顺序设计。目标是通过最大限度地减少事后干预的最佳顺序的累积后悔措施来优化奖励函数。本文从三个方向推进了关于 CB 的研究结果。首先，假设结构因果模型（SCM）是未知的，并且是从 Lipschitz 连续函数的一般类 $\mathcal{F}$ 中任意抽取的。现有的结果通常集中在（广义）线性 SCM。其次，假设干预措施是广义软的，具有任何所需的粒度水平，从而导致无限数量的可能干预措施。相比之下，现有文献普遍采用原子和硬干预。第三，我们提供了后悔的一般上限和下限。上限包含（并改进）特殊情况的已知界限。下限迄今通常未知。这些边界被表征为 (i) 图参数、(ii) SCM 空间的逃避维数（用 $\operatorname{dim}(\mathcal{F})$ 表示）和 (iii) 的覆盖数的函数函数空间，用${\rm cn}(\mathcal{F})$表示。具体来说，在地平线 $T$ 上可累积的遗憾为 $\mathcal{O}(K d^{L-1}\sqrt{T\operatorname{dim}(\mathcal{F}) \log({\rm cn }(\mathcal{F}))})$，其中$K$与Lipschitz常数相关，$d$是图的最大入度，$L$是最长因果路径的长度。对于特殊类别的 SCM（神经网络、多项式和线性），上限进一步细化，并提供了相应的下限。]]></description>
      <guid>https://arxiv.org/abs/2403.00233</guid>
      <pubDate>Mon, 04 Mar 2024 07:02:54 GMT</pubDate>
    </item>
    </channel>
</rss>