<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Fri, 22 Dec 2023 06:17:14 GMT</lastBuildDate>
    <item>
      <title>KSD 聚合拟合优度检验。 （arXiv：2202.00824v6 [stat.ML] 已更新）</title>
      <link>http://arxiv.org/abs/2202.00824</link>
      <description><![CDATA[我们研究基于 Kernel Stein 的拟合优度检验的属性
差异（KSD）。我们引入了一种构建测试的策略，称为 KSDAgg，
它聚合了具有不同内核的多个测试。 KSDAgg 避免分裂
用于执行内核选择的数据（这会导致测试能力损失），以及
而是最大化对一组内核的测试能力。我们提供
对 KSDAgg 威力的非渐近保证：我们证明它实现了
集合的最小均匀分离率，最多为对数项。
对于具有有界模型得分函数的紧支持密度，我们得出
KSDAgg 相对于受限制的 Sobolev 球的比率；该比率对应于
相对于不受限制的 Sobolev 球的极小极大最优速率，直至迭代
对数项。 KSDAgg 在实践中可以精确计算，因为它依赖于
在参数引导程序或野生引导程序上估计
分位数和水平修正。尤其是对于关键的选择
固定内核的带宽，它避免诉诸任意启发法（例如
作为中值或标准差）或数据分割。我们在两者上都发现
KSDAgg 优于其他最先进的合成数据和真实数据
基于二次时间自适应 KSD 的拟合优度测试程序。
]]></description>
      <guid>http://arxiv.org/abs/2202.00824</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>路径套索的量子算法。 (arXiv:2312.14141v1 [quant-ph])</title>
      <link>http://arxiv.org/abs/2312.14141</link>
      <description><![CDATA[我们提出了一种新颖的量子高维线性回归算法
基于经典 LARS（最小角度回归）的 $\ell_1$-惩罚
路径算法。与可用的经典数值算法类似
Lasso，我们的量子算法提供了完整的正则化路径
惩罚项各不相同，但在特定条件下每次迭代速度更快
状况。特征/预测变量 $d$ 数量的二次加速是
通过使用 D\&quot;urr 和中的简单量子最小值查找子例程可以实现
Hoyer (arXiv&#39;96) 以获得每次迭代的加入时间。然后我们
改进这个简单的量子算法并获得二次加速
特征数量 $d$ 和观察数量 $n$ 通过使用
Chen 和 de Wolf 最近的近似量子最小寻找子程序
（ICAP&#39;23）。作为我们的主要贡献之一，我们构建了一个量子酉
基于量子振幅估计来近似计算连接
通过近似量子最小发现来搜索的时间。自从
不再精确计算连接时间，不再清楚
由此产生的近似量子算法得到了很好的解。作为我们的第二个
我们通过 KKT 条件的近似版本证明了主要贡献
和对偶间隙，即 LARS 算法（因此我们的量子
算法）对错误具有鲁棒性。这意味着它仍然输出一条路径
如果连接时间为，则将 Lasso 成本函数最小化到一个小误差
仅近似计算。最后，在观察结果为的模型中
由具有未知系数向量的基础线性模型生成，我们
证明未知系数向量与未知系数向量之间差异的界限
近似 Lasso 解，概括了有关收敛的已知结果
经典统计学习理论分析中的比率。
]]></description>
      <guid>http://arxiv.org/abs/2312.14141</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>学习逆问题的重构方法：样本误差估计。 （arXiv：2312.14078v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.14078</link>
      <description><![CDATA[基于学习和数据驱动的技术最近已成为一个主题
主要兴趣在于逆的重建和正则化领域
问题。除了开发新方法外，还取得了优异的成果
在一些应用中，他们的理论研究吸引了越来越多的人
兴趣，例如对可靠性、稳定性和可解释性等主题的兴趣。
在这项工作中，描述了一个总体框架，使我们能够解释许多
这些技术在统计学习的背景下。这不是
旨在提供对现有方法的完整调查，而是将
他们从工作的角度来看，这自然允许他们的理论
治疗。因此，本论文的主要目标是解决
学习到的重建方法的泛化特性，特别是
执行样本错误分析。这项任务，在
统计学习，在于估计所学知识的依赖性
操作员关于其培训所使用的数据。相当
提出了总体策略，其假设满足一大类
逆问题和学习方法，如通过精选示例所示。
]]></description>
      <guid>http://arxiv.org/abs/2312.14078</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>具有非凸支持的数据的快速内核半空间深度。 （arXiv：2312.14136v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.14136</link>
      <description><![CDATA[数据深度是一种统计函数，它将顺序和分位数概括为
多元设置及其他，应用程序跨越
描述性和可视化统计、异常检测、测试等。
著名的半空间深度通过优化程序利用数据几何
提供不变性、鲁棒性和非参数性的属性。
然而，它隐含地假设凸数据支持并要求
指数计算成本。为了解决分销的多模式问题，我们
扩展再生核希尔伯特空间 (RKHS) 中的半空间深度。我们
表明所获得的深度是直观的，并与
允许进行均匀性测试的可证明的浓度范围。拟议的
可以使用流形梯度计算深度，比半空间更快
深度提高了几个数量级。我们深度的表现是
通过数值模拟以及诸如
真实数据的异常检测和同质性测试。
]]></description>
      <guid>http://arxiv.org/abs/2312.14136</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>R\'enyi 河豚隐私：一般加性噪声机制和迭代隐私放大。 （arXiv：2312.13985v1 [cs.CR]）</title>
      <link>http://arxiv.org/abs/2312.13985</link>
      <description><![CDATA[河豚隐私是差分隐私的灵活概括，
允许对任意秘密和对手的先验知识进行建模
数据。不幸的是，设计通用且易于处理的河豚机制
不妥协实用性是具有挑战性的。此外，该框架不
提供在迭代机中直接使用所需的组合保证
学习算法。为了缓解这些问题，我们引入了 R\&#39;enyi
基于发散的河豚变体，并表明它允许我们扩展
河豚框架的适用性。我们首先概括 Wasserstein
机制来覆盖广泛的噪声分布并引入了几种
提高其效用的方法。我们还获得更有力的保障
分布外的对手。最后，作为合成的替代方案，我们
证明收缩噪声迭代的隐私放大结果以及
展示河豚在私人凸优化中的首次使用。普通的
我们的结果背后的因素是减少班次的使用和扩展
引理。
]]></description>
      <guid>http://arxiv.org/abs/2312.13985</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>AdamMCMC：将 Metropolis 调整的 Langevin 与基于动量的优化相结合。 （arXiv：2312.14027v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.14027</link>
      <description><![CDATA[不确定性估计是考虑应用时的一个关键问题
科学与工程中的深度神经网络方法。在这项工作中，我们
引入一种通过蒙特量化认知不确定性的新颖算法
从缓和后验分布中进行卡罗采样。它结合了井
建立了基于动量的大都会调整朗之万算法（MALA）
使用 Adam 进行优化并利用长长提案分布，
有效地从后部绘制。我们证明所构造的链承认
吉布斯后验作为不变分布并收敛到该吉布斯
总变异距离的后验。数值评估被推迟到
第一次修订。
]]></description>
      <guid>http://arxiv.org/abs/2312.14027</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>通过保留图生成模式来微调图神经网络。 （arXiv：2312.13583v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.13583</link>
      <description><![CDATA[最近预训练和微调图神经网络的范式
已被深入研究并应用于广泛的图挖掘任务。
它的成功通常归因于结构之间的一致性
预训练和下游数据集，然而，这在许多情况下并不成立
现实世界的场景。现有的工作表明结构分歧
预训练和下游图之间的差异极大地限制了
使用普通微调策略时的可转移性。这种分歧
导致模型在预训练图上过度拟合并导致困难
捕获下游图的结构属性。在本文中，我们
确定结构差异的根本原因是结构差异
预训练图和下游图之间的生成模式。
此外，我们建议 G-Tuning 来保留生成模式
下游图。给定一个下游图G，核心思想是调整
预训练的 GNN，以便它可以重建 G 的生成模式，即
graphon W。然而，已知 graphon 的精确重建是
计算成本昂贵。为了克服这一挑战，我们提供了一个理论
分析确定存在一组称为
任何给定图子的图子基础。通过利用这些的线性组合
基于图基，我们可以有效地近似 W。这个理论发现形成
我们提出的模型的基础，因为它能够有效地学习
图子基及其相关系数。与现有相比
算法方面，G-Tuning 的平均性能提升了 0.5% 和 2.6%
分别进行域内和域外迁移学习实验。
]]></description>
      <guid>http://arxiv.org/abs/2312.13583</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>批量多臂老虎机问题中的最佳臂识别。 （arXiv：2312.13875v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.13875</link>
      <description><![CDATA[最近，在许多现实生活场景中都出现了多臂老虎机问题
由于代理等待时间有限，必须分批对武器进行采样
反馈。此类应用包括生物实验和在线
营销。当手臂数量较多时，问题会更加复杂
而且批次数量少。我们考虑批量的纯粹探索
多臂老虎机问题。我们介绍一个通用的线性规划框架
可以将不同理论设置的目标合并到最佳臂中
鉴别。线性程序产生了一个两阶段算法，可以
取得良好的理论性质。我们通过数值研究证明
与某些 UCB 类型或算法相比，该算法也具有良好的性能
汤普森抽样方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.13875</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>夺旗：利用大型语言模型揭示数据洞察力。 （arXiv：2312.13876v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.13876</link>
      <description><![CDATA[从大量的信息中提取少量的相关见解
数据是数据驱动决策的重要组成部分。然而，
完成这项任务需要相当多的技术技能、领域
专业知识和人力。本研究探讨了使用大型
语言模型 (LLM) 可自动发现数据见解，
利用推理和代码生成技术的最新进展。我们
提出一种基于“夺旗”原则的新评估方法，
衡量此类模型识别有意义和相关的能力
数据集中的信息（标志）。我们进一步提出两个概念验证
具有不同内部运作方式的代理，并比较他们捕获的能力
现实世界销售数据集中的此类标志。虽然这里报告的工作是
初步而言，我们的结果足够有趣，可以指导未来
社区的探索。
]]></description>
      <guid>http://arxiv.org/abs/2312.13876</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>重新审视深度广义典型相关分析。 （arXiv：2312.13455v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.13455</link>
      <description><![CDATA[典型相关分析（CCA）是一种经典的统计方法
发现支撑两个或多个观察到的随机的潜在共变
向量。已经提出了 CCA 的几种扩展和变体，
增强了我们揭示常见随机因素的能力
多视图数据集。在这项工作中，我们首先回顾最新的
深度 CCA 的确定性扩展并突出其优点和
这些最先进方法的局限性。有些方法允许琐碎的
解决方案，而其他人可能会错过弱公因数。其他人超载
还试图揭示观点中不常见的内容，即，
完全重建每个视图所需的私有组件。这
后者往往会使问题及其计算和样本超载
复杂性。为了改善这些限制，我们设计了一种新颖且
有效的制定可以缓解当前的一些限制。这
主要思想是将私有组件建模为条件独立的给定
共同的，这使得所提出的紧凑的公式成为可能。此外，
我们还提供了识别常见随机数的充分条件
因素。使用合成数据集和真实数据集进行的明智实验展示了
我们的主张的有效性以及所提议方法的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2312.13455</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯迁移学习。 （arXiv：2312.13484v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.13484</link>
      <description><![CDATA[迁移学习是统计机器学习中的一个新兴概念
旨在提高某个领域的推理和/或预测准确性
通过利用相关领域的数据来激发兴趣。虽然“转让”一词
“学习”最近引起了人们的广泛兴趣，其基本原则
以各种形式存在多年。计算机先前文献综述
科学和电气工程试图将这些想法集中起来，
主要调查这些学科的一般方法和工作。
本文重点介绍了迁移学习的贝叶斯方法，该方法具有
尽管它们与生俱来的兼容性，但受到的关注相对有限
利用先验知识来指导新的学习任务的概念。我们的
调查涵盖了广泛的贝叶斯迁移学习框架
适用于各种实际设置。我们讨论这些方法如何
解决寻找最佳信息在之间传输的问题
域，这是迁移学习的核心问题。我们举例说明
通过模拟研究贝叶斯迁移学习方法的效用，我们
与常客竞争对手进行性能比较。
]]></description>
      <guid>http://arxiv.org/abs/2312.13484</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>遍历动力系统的一致长期预测。 （arXiv：2312.13426v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.13426</link>
      <description><![CDATA[我们研究遍历作用下分布的演化
动力系统，本质上可能是随机的。通过使用以下工具
库普曼和转移算子理论可以演化出任意初始分布
状态及时向前推进，我们研究这些算子的估计者如何
执行长期预测。受到标准的观察的启发
估计器可能会在这项任务上失败，我们引入了一种学习范式，可以巧妙地
结合了算子理论的特征值紧缩的经典技术和
以统计数据为中心的特征。此范例适用于任何操作员
基于经验风险最小化的估计器，使他们满足学习
在未来分布的整个轨迹上一致的界限，
并遵守每个预测分布的质量守恒。
数值实验说明了我们的方法在实践中的优势。
]]></description>
      <guid>http://arxiv.org/abs/2312.13426</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>独立机制分析和流形假设。 （arXiv：2312.13438v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.13438</link>
      <description><![CDATA[独立机制分析 (IMA) 旨在解决
非线性独立分量分析 (ICA)，假设雅可比行列式为
混合函数具有正交列。正如 ICA 中的典型情况一样，之前的工作
关注具有相同数量潜在成分的情况并观察
混合物。在这里，我们将 IMA 扩展到具有大量混合物的设置，
驻留在嵌入比潜在空间更高维度的流形上——
符合表示学习中的流形假设。为了这
设置，我们表明 IMA 仍然规避了几个不可识别性问题，
表明这对于高维也可能是一个有益的原则
当流形假设成立时的观察结果。进一步，我们证明IMA
原则大致满足高概率（随着
观察到的混合物的数量）当潜伏的方向
影响观测值的成分是随机独立选择的。这
为 IMA 提供了新的、严格的统计解释。
]]></description>
      <guid>http://arxiv.org/abs/2312.13438</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>深度近似空间的采样复杂性。 （arXiv：2312.13379v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.13379</link>
      <description><![CDATA[虽然众所周知，神经网络具有出色的逼近能力
能力，计算这样的近似值仍然是一个巨大的挑战
点样本。基于基于信息的复杂性的工具，最近的工作
Grohs 和 Voigtlaender [Journal of the FoCM (2023)] 制定了严格的
评估这种所谓的“理论与实践差距”的框架。更多的
准确地说，在那项工作中表明存在可以被
由具有任意 ReLU 激活函数的神经网络近似
率，同时需要指数增长（在输入维度）的数字
用于数值计算的样本。本研究扩展了这些
通过显示 ReQU 激活函数的类似结果来得出结论。
]]></description>
      <guid>http://arxiv.org/abs/2312.13379</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>通过多阶段采样技术 (MUST) 增强隐私、实用性和计算效率之间的权衡。 （arXiv：2312.13389v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.13389</link>
      <description><![CDATA[将随机算法应用于数据集的子集而不是数据集的子集
整个数据集是增强其隐私保证的常用方法
发布的信息。我们提出了一类名为的子采样方法
用于隐私放大 (PA) 的多级采样技术 (MUST)
差异隐私（DP）的上下文。我们对以下情况进行全面分析
PA 效果和多个 2 阶段 MUST 程序的实用程序，即 MUST.WO、
MUST.OW 和 MUST.WW 分别表示带 (W) 的采样，不带 (W) 的采样
(O)，在阶段 I 中用 (W) 替换原始数据集，然后
在阶段 II 中进行无 (O)、有 (W)、有 (W) 替换的抽样
在第一阶段绘制的子集。我们还提供了隐私成分分析
通过傅立叶会计算法重复应用 MUST。我们的
理论和实证结果表明 MUST.OW 和 MUST.WW
$\epsilon$ 中的 PA 比常见的一级抽样程序更强
包括泊松采样、无放回采样、带放回采样
替换，而 $\delta$ 的结果因情况而异。我们还证明
MUST.WO 相当于 PA 中的放回抽样。此外，
MUST 过程生成的最终子集是一个多重集，其中可能包含
由于放回抽样而导致相同数据点的多个副本
涉及，这提高了算法的计算效率
需要对不同数据点进行复杂的函数计算（例如梯度
血统）。我们的实用实验表明，必须提供类似或改进的
与一级相比，隐私保护输出的实用性和稳定性
类似隐私损失的二次采样方法。必须可以无缝集成
进入随机优化算法或涉及并行或
DP 时同时进行子采样（例如装袋和子采样自举）
保证是必要的。
]]></description>
      <guid>http://arxiv.org/abs/2312.13389</guid>
      <pubDate>Fri, 22 Dec 2023 06:17:07 GMT</pubDate>
    </item>
    </channel>
</rss>