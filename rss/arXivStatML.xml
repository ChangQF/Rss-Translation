<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 08 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>差分隐私下基于结构偏好的图嵌入生成</title>
      <link>https://arxiv.org/abs/2501.03451</link>
      <description><![CDATA[arXiv:2501.03451v1 公告类型：新
摘要：图嵌入生成技术旨在学习图中每个节点的低维向量，最近引起了越来越多的研究关注。发布低维节点向量可以实现各种图分析任务，例如结构等价性和链接预测。然而，不当发布会为恶意攻击者打开后门，他们可以从低维节点向量推断出个人的敏感信息。现有方法通过开发具有差分隐私 (DP) 的深度图学习模型来解决此问题。然而，它们经常受到大量噪声注入的影响，并且无法提供与挖掘目标一致的结构偏好。最近，基于 skip-gram 的图嵌入生成技术因其提取可定制结构的能力而被广泛使用。基于 skip-gram，我们提出了 SE-PrivGEmb，一种在 DP 下支持结构偏好的图嵌入生成。对于任意结构偏好，我们通过扰动非零向量设计了一种统一的噪声容忍机制。该机制减轻了高灵敏度导致的效用下降。通过精心设计 skip-gram 中的负采样概率，我们从理论上证明了 skip-gram 可以保留任意近似度，从而量化图中的结构特征。大量实验表明，我们的方法在结构等价性和链接预测任务下优于现有的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2501.03451</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>正则化回归中的类别平衡偏差</title>
      <link>https://arxiv.org/abs/2501.03821</link>
      <description><![CDATA[arXiv:2501.03821v1 公告类型：新
摘要：正则化模型通常对数据中特征的尺度很敏感，因此在拟合模型之前对特征进行归一化（中心化和缩放）已成为标准做法。但是，归一化特征的方法有很多种，选择可能会对生成的模型产生巨大影响。尽管如此，到目前为止还没有关于这个主题的研究。在本文中，我们开始通过研究套索、岭回归和弹性网络回归背景下的归一化来弥补这一知识空白。我们关注正态特征和二元特征，并表明二元特征的类平衡直接影响回归系数，并且这种影响取决于所使用的归一化和正则化方法的组合。我们证明，可以通过在套索的情况下用方差缩放二元特征，在岭回归的情况下用标准差缩放二元特征来减轻这种影响，但这是以增加方差为代价的。对于弹性网络，我们表明缩放惩罚权重（而不是特征）可以达到相同的效果。最后，我们还处理了二元特征和正常特征的混合以及交互，并提供了关于如何在这些情况下对特征进行规范化的一些初步结果。]]></description>
      <guid>https://arxiv.org/abs/2501.03821</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tree-Wasserstein 距离的耦合层次结构学习</title>
      <link>https://arxiv.org/abs/2501.03627</link>
      <description><![CDATA[arXiv:2501.03627v1 公告类型：交叉 
摘要：在许多应用中，数据样本和特征都具有底层层次结构。然而，现有的学习这些潜在结构的方法通常侧重于样本或特征，而忽略了它们之间可能存在的耦合。在本文中，我们介绍了一种使用树-Wasserstein 距离 (TWD) 的耦合层次结构学习方法。我们的方法联合计算样本和特征的 TWD，将它们的潜在层次结构表示为树。我们提出了一种基于扩散几何、双曲几何和小波滤波器的迭代、无监督程序来构建这些样本和特征树。我们表明，这种迭代过程可以收敛并从经验上提高构建树的质量。该方法在计算上也非常高效，并且在高维设置中具有很好的扩展性。我们的方法可以与双曲图卷积网络 (HGCN) 无缝集成。我们证明，在多个 word 文档和单细胞 RNA 测序数据集上，我们的方法在稀疏近似和无监督 Wasserstein 距离学习方面优于竞争方法。此外，将我们的方法集成到 HGCN 中可提高链接预测和节点分类任务的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.03627</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度网络正在复制核链</title>
      <link>https://arxiv.org/abs/2501.03697</link>
      <description><![CDATA[arXiv:2501.03697v1 公告类型：交叉 
摘要：为深度神经网络确定合适的函数空间仍然是一个关键的未决问题。虽然浅层神经网络自然与再生核 Banach 空间 (RKBS) 相关，但深度网络带来了独特的挑战。在这项工作中，我们将 RKBS 扩展为链式 RKBS (cRKBS)，这是一个由内核而不是函数组成的新框架，保留了 RKBS 的理想属性。我们证明任何深度神经网络函数都是神经 cRKBS 函数，反之，在有限数据集上定义的任何神经 cRKBS 函数都对应于深度神经网络。这种方法为经验风险最小化问题提供了稀疏解，每层需要的神经元不超过 $N$ 个，其中 $N$ 是数据点的数量。]]></description>
      <guid>https://arxiv.org/abs/2501.03697</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的对称性和泛化</title>
      <link>https://arxiv.org/abs/2501.03858</link>
      <description><![CDATA[arXiv:2501.03858v1 公告类型：交叉 
摘要：这项工作旨在了解不变性和等变性对监督学习中泛化的影响。我们使用平均算子提供的视角来表明，对于任何非等变的预测因子，在所有正确指定等变性的回归问题上，都有一个具有严格较低测试风险的等变预测因子。这构成了一个严格的证明，即以不变性或等变性形式出现的对称性是一种有用的归纳偏差。
我们将这些想法分别应用于随机设计最小二乘和核岭回归中的等变性和不变性。这使我们能够在更具体的设置中指定预期测试风险的降低，并用组、模型和数据的属性来表达它。
在此过程中，我们给出了示例和其他结果来证明平均算子方法在分析等变预测因子中的实用性。此外，我们采用了另一种视角，并将使用不变模型进行学习的普遍直觉形式化为轨道代表问题。这种形式主义自然延伸到等变模型的类似直觉。最后，我们将这两种观点联系起来，并为未来的工作提供一些想法。]]></description>
      <guid>https://arxiv.org/abs/2501.03858</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于序列线性模拟推理的宇宙学参数估计</title>
      <link>https://arxiv.org/abs/2501.03921</link>
      <description><![CDATA[arXiv:2501.03921v1 公告类型：交叉 
摘要：我们开发了基于线性模拟的推理 (LSBI) 框架，这是基于模拟的推理的一种应用，其中似然度由其参数的高斯线性函数近似。我们获得了线性似然度的超参数后验分布的解析表达式，这些解析表达式以从模拟器中抽取的样本表示，包括均匀和共轭先验。该方法依次应用于几个玩具模型，并在模拟数据集上对宇宙微波背景温度功率谱进行了测试。我们发现，经过四到五轮 $\mathcal{O}(10^4)$ 模拟后，就可以实现收敛，这与最先进的神经密度估计方法相媲美。因此，我们证明了可以获得显着的信息增益并生成与底层参数一致的后验，同时保持可解释性和智力监督。]]></description>
      <guid>https://arxiv.org/abs/2501.03921</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>投资组合的合成数据：掷骰子永远不会消除机会</title>
      <link>https://arxiv.org/abs/2501.03993</link>
      <description><![CDATA[arXiv:2501.03993v1 公告类型：交叉 
摘要：模拟方法一直在金融领域发挥着重要作用，而具有最小模型规范的数据驱动方法（通常称为生成模型）引起了越来越多的关注，尤其是在深度学习在广泛领域取得成功之后。然而，这些模型在金融应用中的采用并没有跟上日益增长的兴趣，这可能是由于金融市场独特的复杂性和挑战。本文旨在更深入地了解生成模型的局限性，特别是在投资组合和风险管理方面。为此，我们首先介绍关于初始样本量重要性的理论结果，并指出生成远超最初可用数据的潜在陷阱。然后，我们通过触及一个悖论来强调模型开发和所需用例不可分割的性质：通用生成模型本质上不太关心构建投资组合（尤其是多空投资组合）的重要内容。基于这些发现，我们提出了一种生成多元回报的流程，该流程既符合美国大量股票的传统评估标准，又符合资产回报中观察到的典型事实，并扭转了我们之前发现的陷阱。此外，我们坚持需要更精细的评估方法，并通过均值回归策略的示例提出了一种基于反刍训练的方法，旨在识别特定应用的不良模型，即使用模型本身生成的数据对其进行再训练，这在统计学中通常称为可识别性。]]></description>
      <guid>https://arxiv.org/abs/2501.03993</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WAPTS：一种针对高维稀疏实验设置的加权分配概率调整汤普森抽样算法</title>
      <link>https://arxiv.org/abs/2501.03999</link>
      <description><![CDATA[arXiv:2501.03999v1 公告类型：交叉 
摘要：为了实现更有效的实验设计，例如在视频内容广告中，不同的内容选项争夺用户参与度，这些场景可以建模为多臂老虎机问题。在由于外部因素（例如进行实验的成本）而导致交互有限的情况下，推荐者通常会因用户交互数量少而受到限制。此外，在选择最佳治疗方法和基于个人因素进行个性化和情境化的能力之间需要权衡。解决这一困境的一个流行方法是情境老虎机框架。它旨在最大化结果，同时结合个性化（情境）因素，根据个人偏好定制用户个人资料等治疗方法。尽管情境老虎机算法具有优势，但它们面临着测量偏差和“维数灾难”等挑战。这些问题使大量干预措施的管理变得复杂，并且经常通过参与者细分导致数据稀疏。为了解决这些问题，我们引入了加权分配概率调整汤普森抽样 (WAPTS) 算法。WAPTS 通过使用动态加权参数，在上下文汤普森抽样方法的基础上建立。这改进了干预措施的分配过程，并能够在数据稀疏的环境中实现快速优化。我们展示了我们的方法在不同数量的臂和效果大小上的表现。]]></description>
      <guid>https://arxiv.org/abs/2501.03999</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过惩罚最优传输网络实现高效生成建模</title>
      <link>https://arxiv.org/abs/2402.10456</link>
      <description><![CDATA[arXiv:2402.10456v2 公告类型：替换 
摘要：生成具有忠实模拟底层数据生成机制的分布的合成数据具有至关重要的意义。 Wasserstein 生成对抗网络 (WGAN) 已成为此任务的重要工具；然而，由于极小极大公式的微妙平衡和高维下 Wasserstein 距离的不稳定性，WGAN 经常表现出模式崩溃的病态现象。这导致生成的样本收敛到一组受限的输出，并且无法充分捕捉真实分布的尾部行为。这种限制可能导致严重的下游后果。为此，我们提出了惩罚最优传输网络 (POTNet)，这是一种基于边际惩罚 Wasserstein (MPW) 距离的多功能深度生成模型。通过 MPW 距离，POTNet 有效地利用低维边际信息来指导联合分布的整体对齐。此外，我们基于原始的框架可以直接评估 MPW 距离，从而无需使用批评者网络。这种公式避免了对抗方法固有的训练不稳定性，并避免了进行大量参数调整的需要。我们推导出 MPW 损失泛化误差的非渐近界限，并建立了 POTNet 学习的生成分布的收敛速度。我们的理论分析与广泛的实证评估证明了 POTNet 在准确捕获底层数据结构（包括其尾部行为和次要模态）方面的卓越性能。此外，与最先进的替代方案相比，我们的模型在采样阶段实现了数量级的加速，从而实现了计算效率高的大规模合成数据生成。]]></description>
      <guid>https://arxiv.org/abs/2402.10456</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫类别中的随机神经网络对称化</title>
      <link>https://arxiv.org/abs/2406.11814</link>
      <description><![CDATA[arXiv:2406.11814v4 公告类型：替换 
摘要：我们考虑沿群同态对称化神经网络的问题：给定同态 $\varphi : H \to G$，我们想要一个将 $H$-等变神经网络转换为 $G$-等变神经网络的过程。我们用马尔可夫类别来表述这一点，这使我们能够考虑输出可能是随机的但抽象出测度理论细节的神经网络。我们获得了一个灵活且组合的对称化框架，该框架依赖于对群结构和底层神经网络架构的最小假设。我们的方法恢复了现有的用于对称化确定性模型的规范化和平均技术，并扩展为对称化随机模型提供了一种新方法。除此之外，我们的研究结果还证明了马尔可夫类别在以概念清晰但数学精确的方式解决机器学习中的复杂问题方面的实用性。]]></description>
      <guid>https://arxiv.org/abs/2406.11814</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有非线性目标函数的 GAN 的统计误差界限</title>
      <link>https://arxiv.org/abs/2406.16834</link>
      <description><![CDATA[arXiv:2406.16834v2 公告类型：替换 
摘要：生成对抗网络 (GAN) 是一种无监督学习方法，用于训练生成器分布以生成近似于从目标分布中提取的样本。许多此类方法可以表述为最小化概率分布之间的度量或散度。最近的研究已经推导出基于积分概率度量 (IPM) 的 GAN 的统计误差界限，例如基于 1-Wasserstein 度量的 WGAN。一般来说，IPM 是通过在鉴别器空间上优化线性函数（期望差）来定义的。可以使用 $f$-散度（例如 Jensen-Shannon、KL 或 $\alpha$-散度）以及正则化鉴别器空间 $\Gamma$（例如 $1$-Lipschitz 函数）构建更大的 GAN 类，我们在此将其称为 $(f,\Gamma)$-GAN。这些 GAN 具有非线性目标函数，具体取决于 $f$ 的选择，并且已被证明在许多应用中表现出改进的性能。在这项工作中，我们以有限样本浓度不等式的形式推导出 $f$ 和 $\Gamma$ 一般类的 $(f,\Gamma)$-GAN 的统计误差界限。这些结果证明了 $(f,\Gamma)$-GAN 的统计一致性，并在适当的极限下简化为 IPM-GAN 的已知结果。最后，我们的结果还为具有无界支持的分布的 GAN 性能提供了新的见解。]]></description>
      <guid>https://arxiv.org/abs/2406.16834</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ALTBI：通过优化内部记忆效应构建改进的异常值检测模型</title>
      <link>https://arxiv.org/abs/2408.09791</link>
      <description><![CDATA[arXiv:2408.09791v2 公告类型：替换 
摘要：异常值检测 (OD) 是通过学习正常观测值 (或内点) 的独特模式，从给定或即将到来的数据中识别异常观测值 (或异常值) 的任务。最近，一项研究基于对深度生成模型的新观察，引入了一种强大的无监督 OD (UOD) 求解器，称为内点记忆 (IM) 效应，这表明生成模型在早期学习阶段会先记住内点，然后再记住异常值。在本研究中，我们旨在开发一种理论上可行的方法，通过最大限度地利用 IM 效应来解决 UOD 任务。我们首先观察到，当给定的训练数据包含较少的异常值时，IM 效应会更明显。这一发现表明，如果我们在设计损失函数时能够有效地从小批量中排除异常值，那么在 UOD 机制中增强 IM 效果的潜力。为此，我们引入了两种主要技术：1) 在模型训练过程中增加小批量大小；2) 使用自适应阈值计算截断损失函数。我们从理论上表明，这两种技术可以有效地从截断损失函数中滤除异常值，使我们能够充分利用 IM 效应。结合额外的集成策略，我们提出了我们的方法，并将其称为带批量增量的自适应损失截断 (ALTBI)。我们提供了大量实验结果来证明，与其他近期方法相比，ALTBI 在识别异常值方面实现了最先进的性能，即使计算成本明显降低。此外，我们表明，当与隐私保护算法相结合时，我们的方法可实现稳健的性能。]]></description>
      <guid>https://arxiv.org/abs/2408.09791</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>受限玻尔兹曼机上的无数据集权重初始化</title>
      <link>https://arxiv.org/abs/2409.07708</link>
      <description><![CDATA[arXiv:2409.07708v2 公告类型：替换 
摘要：在前馈神经网络中，已经开发了无数据集权重初始化方法，例如 LeCun、Xavier（或 Glorot）和 He 初始化。这些方法基于特定分布（例如高斯或均匀分布）随机确定权重参数的初始值，而无需使用训练数据集。据作者所知，尚未为受限玻尔兹曼机 (RBM) 开发这种无数据集权重初始化方法，受限玻尔兹曼机是由两层组成的概率神经网络。在本研究中，我们基于统计力学分析推导出一种用于伯努利-伯努利 RBM 的无数据集权重初始化方法。在提出的权重初始化方法中，权重参数来自具有零均值的高斯分布。我们假设，两层之间的层相关性 (LC) 越大，标准差越能提高学习效率，因此我们基于此假设对高斯分布的标准差进行了优化。LC 的表达式是基于统计力学分析得出的。标准差的最优值对应于 LC 的最大值点。在特定情况下（即当两层的大小相同、层的随机变量为 $\{-1,1\}$ 二进制，所有偏差参数均为零），所提出的权重初始化方法与 Xavier 初始化相同。使用玩具数据集和真实数据集的数值实验证明了所提出的权重初始化方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.07708</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>流形、随机矩阵和谱间隙：生成扩散的几何阶段</title>
      <link>https://arxiv.org/abs/2410.05898</link>
      <description><![CDATA[arXiv:2410.05898v5 公告类型：替换 
摘要：本文研究了流形假设下的生成扩散模型的潜在几何形状。为此，我们分析了得分函数雅可比矩阵的特征值（和奇异值）谱，其不连续性（间隙）揭示了不同子流形的存在和维数。使用统计物理方法，我们推导出几个分布假设下的谱分布和谱间隙公式，并将这些理论预测与从训练网络估计的谱进行比较。我们的分析揭示了生成过程中存在三个不同的定性阶段：一个平凡阶段；一个流形覆盖阶段，其中扩散过程适合流形内部的分布；一个合并阶段，其中得分与流形正交，所有粒子都投影在数据支持上。不同时间尺度之间的这种“分工”很好地解释了为什么生成扩散模型不受困扰基于可能性的模型的流形过度拟合现象的影响，因为内部分布和流形几何是在生成过程中的不同时间点产生的。]]></description>
      <guid>https://arxiv.org/abs/2410.05898</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>采用原始对偶朗之万蒙特卡罗进行约束采样</title>
      <link>https://arxiv.org/abs/2411.00568</link>
      <description><![CDATA[arXiv:2411.00568v2 公告类型：替换 
摘要：这项工作考虑了从已知的概率分布中抽样的问题，该概率分布最多为一个归一化常数，同时满足由一般非线性函数的期望值指定的一组统计约束。该问题在贝叶斯推理等应用中得到应用，它可以约束矩以评估反事实场景或强制执行诸如预测公平性之类的要求。为处理支持约束而开发的方法（例如基于镜像映射、障碍和惩罚的方法）不适合这项任务。因此，这项工作依赖于 Wasserstein 空间中的梯度下降-上升动力学来提出一种离散时间原始对偶朗之万蒙特卡罗算法 (PD-LMC)，该算法同时约束目标分布并从中抽样。我们分析了在目标分布和约束的标准假设下 PD-LMC 的收敛性，即（强）凸性和对数 Sobolev 不等式。为此，我们将鞍点算法的经典优化论证引入 Wasserstein 空间的几何中。我们说明了 PD-LMC 在多个应用中的相关性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.00568</guid>
      <pubDate>Wed, 08 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>