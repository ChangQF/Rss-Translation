<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 05 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>多项式神经网络的几何</title>
      <link>https://arxiv.org/abs/2402.00949</link>
      <description><![CDATA[我们研究具有单项激活函数的多项式神经网络（PNN）的表达能力和学习过程。网络的权重对神经流形进行参数化。在本文中，我们使用代数几何工具研究某些神经流形：我们将明确的描述作为半代数集并表征它们的 Zariski 闭包，称为神经变体。我们研究它们的维度，并将代数程度（学习程度）与神经多样性联系起来。维度作为网络表达能力的几何度量，学习度是训练网络复杂性的度量，并提供可学习函数数量的上限。这些理论结果伴随着实验。]]></description>
      <guid>https://arxiv.org/abs/2402.00949</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:47 GMT</pubDate>
    </item>
    <item>
      <title>以球谐函数作为控制变量的切片 Wasserstein 估计</title>
      <link>https://arxiv.org/abs/2402.01493</link>
      <description><![CDATA[概率度量之间的切片 Wasserstein (SW) 距离定义为相关一维投影产生的 Wasserstein 距离的平均值。因此，SW距离可以写成关于球体上均匀测量的积分，并且可以采用蒙特卡洛框架来计算SW距离。球谐函数是球面上的多项式，形成球面上可平方可积函数集的正交基。将这两个事实放在一起，提出了一种新的蒙特卡罗方法，称为球谐函数控制变量（SHCV），用于使用球谐函数作为控制变量来近似 SW 距离。由此产生的方法被证明具有良好的理论特性，例如，在变量之间某种形式的线性相关性下，高斯测度具有无误差特性。此外，与蒙特卡罗相比，针对一般措施建立了更高的收敛速度。收敛性分析依赖于与 SW 被积函数相关的 Lipschitz 属性。几个数值实验证明了 SHCV 相对于最先进的 SW 距离计算方法具有优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.01493</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么随机森林有效？将树集成理解为自调节自适应平滑器</title>
      <link>https://arxiv.org/abs/2402.01502</link>
      <description><![CDATA[尽管它们具有显着的有效性和广泛的应用，但树丛成功的驱动因素仍然没有被完全理解。在本文中，我们强调如何将树集成解释为自适应和自调节平滑器可以为该主题提供新的直觉和更深入的见解。我们用这个观点来表明，当作为平滑器进行研究时，随机树集成不仅做出比它们所组成的单个树的预测在量化上更平滑的预测，而且还根据差异在测试时进一步调节它们的平滑度测试和训练输入之间。首先，我们利用这一见解来重新审视、完善和调和最近对森林成功的两种解释，通过测量树群所暗示的有效平滑程度，提供一种客观量化树群推测行为的新方法。然后，我们超越了对树集合改进个体树的机制的现有解释，并挑战了流行的观点，即森林的优越性能应该被理解为单独减少方差的结果。我们认为，目前统计学中普遍存在的偏差和方差减少的高级二分法不足以理解树集成——因为偏差的流行定义没有捕捉到树木和森林形成的假设类别的表达性差异。相反，我们证明森林可以通过三种不同的机制来改善树木，这三种机制通常隐含地相互关联。特别是，我们证明了集成的平滑效果可以减少由于结果生成中的噪声而导致的预测方差，减少给定固定输入数据的学习函数质量的变异性，并通过丰富可用的假设空间来减少可学习函数的潜在偏差。]]></description>
      <guid>https://arxiv.org/abs/2402.01502</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:46 GMT</pubDate>
    </item>
    <item>
      <title>弱监督学习器通过可证明的性能保证来纠正人工智能错误</title>
      <link>https://arxiv.org/abs/2402.00899</link>
      <description><![CDATA[我们通过引入具有先验性能保证的弱监督人工智能纠错器，提出了一种处理人工智能错误的新方法。这些人工智能校正器是辅助地图，其作用是通过批准或拒绝某些先前构建的底层分类器的决策来调节其决策。拒绝决定可以用作建议放弃做出决定的信号。这项工作的一个关键技术重点是通过限制错误决策的概率为这些新的人工智能校正器提供性能保证。这些界限与分布无关，并且不依赖于数据维度的假设。我们的实证示例说明了如何应用该框架来提高图像分类器在训练数据稀缺的具有挑战性的现实任务中的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.00899</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 Noisy Oracle 进行高效查询的关联聚类</title>
      <link>https://arxiv.org/abs/2402.01400</link>
      <description><![CDATA[我们研究了一种通用的聚类设置，其中有 $n$ 个元素要聚类，我们的目标是对返回两个元素之间相似性的噪声样本的预言机执行尽可能少的查询。我们的设置涵盖许多应用领域，在这些领域中，相似性函数的计算成本很高，而且本身就有噪声。我们提出了两种基于组合多臂老虎机纯粹探索（PE-CMAB）范式的在线学习问题的新颖表述：固定置信度和固定预算设置。对于这两种设置，我们设计了将采样策略与用于相关聚类的经典近似算法相结合的算法，并研究了它们的理论保证。我们的结果是多项式时间算法的第一个例子，适用于 PE-CMAB 的情况，其中底层离线优化问题是 NP 困难的。]]></description>
      <guid>https://arxiv.org/abs/2402.01400</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:45 GMT</pubDate>
    </item>
    <item>
      <title>调节非线性和无限维扩散过程</title>
      <link>https://arxiv.org/abs/2402.01434</link>
      <description><![CDATA[科学和工程中的生成扩散模型和许多随机模型在离散化之前自然存在于无限维度中。为了将观察到的数据纳入统计和学习任务，需要以观察为条件。虽然最近的工作已经处理了无限维度中的调节线性过程，但尚未探索无限维度中的调节非线性过程。本文条件函数值随机过程无需事先离散。为此，我们使用吉尔萨诺夫定理的无限维版本来调节函数值随机过程，从而得出涉及分数的条件过程的随机微分方程 (SDE)。我们应用这种技术对进化生物学中的生物体形状进行时间序列分析，通过傅立叶基础进行离散，然后通过分数匹配方法学习分数函数的系数。]]></description>
      <guid>https://arxiv.org/abs/2402.01434</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:45 GMT</pubDate>
    </item>
    <item>
      <title>深度条件生成学习：模型和误差分析</title>
      <link>https://arxiv.org/abs/2402.01460</link>
      <description><![CDATA[我们引入了一种基于常微分方程 (ODE) 的深度生成方法来学习条件分布，称为条件 Follmer 流。从标准高斯分布开始，所提出的流可以在时间 1 时有效地将其转换为目标条件分布。为了有效实现，我们使用欧拉方法离散化流，其中我们使用深度神经网络非参数估计速度场。此外，我们推导了学习样本分布与目标分布之间的 Wasserstein 距离的非渐近收敛率，为通过 ODE 流进行条件分布学习提供了第一个全面的端到端误差分析。我们的数值实验展示了它在一系列场景中的有效性，从标准的非参数条件密度估计问题到涉及图像数据的更复杂的挑战，说明了它相对于各种现有条件密度估计方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2402.01460</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:45 GMT</pubDate>
    </item>
    <item>
      <title>具有衰减步长的在线共形预测</title>
      <link>https://arxiv.org/abs/2402.01139</link>
      <description><![CDATA[我们引入了一种具有衰减步长的在线共形预测方法。与以前的方法一样，我们的方法具有对任意序列覆盖的回顾性保证。然而，与以前的方法不同，我们可以同时估计总体分位数（如果存在）。我们的理论和实验表明实际性能得到了显着改善：特别是，当分布稳定时，每个时间点的覆盖范围都接近所需水平，而不仅仅是观察序列的平均值。]]></description>
      <guid>https://arxiv.org/abs/2402.01139</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:44 GMT</pubDate>
    </item>
    <item>
      <title>变形金刚在上下文中学习非线性特征：注意力景观上的非凸平均场动力学</title>
      <link>https://arxiv.org/abs/2402.01258</link>
      <description><![CDATA[基于 Transformer 架构的大型语言模型已经展示了令人印象深刻的上下文学习能力。然而，现有关于这种现象如何产生的理论研究仅限于在线性回归任务上训练的单层注意力的动态。在本文中，我们研究了 Transformer 的优化，该 Transformer 由全连接层和线性关注层组成。 MLP充当常见的非线性表示或特征图，极大地增强了上下文学习的能力。我们在平均场和两倍时间尺度极限下证明，参数分布的无限维损失景观虽然高度非凸，但变得相当良性。我们还分析了平均场动力学的二阶稳定性，并表明 Wasserstein 梯度流几乎总是避开鞍点。此外，我们建立了新的方法来获得远离和接近临界点的具体改进率。这代表了平均场动力学的第一个鞍点分析，并且这些技术具有独立的意义。]]></description>
      <guid>https://arxiv.org/abs/2402.01258</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:44 GMT</pubDate>
    </item>
    <item>
      <title>均质随机梯度下降中重尾的出现</title>
      <link>https://arxiv.org/abs/2402.01382</link>
      <description><![CDATA[人们反复观察到，随机梯度下降（SGD）的损失最小化会导致神经网络参数的重尾分布。在这里，我们分析了 SGD 的连续扩散近似（称为均质随机梯度下降），表明它具有渐近重尾的行为，并给出了其尾部指数的明确上限和下限。我们在数值实验中验证了这些界限，并表明它们通常非常接近 SGD 迭代的经验尾部指数。此外，它们的显式形式使我们能够量化优化参数和尾部指数之间的相互作用。这样做，我们为正在进行的关于重尾和神经网络泛化性能之间的联系以及 SGD 避免次优局部最小值的能力的讨论做出了贡献。]]></description>
      <guid>https://arxiv.org/abs/2402.01382</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:44 GMT</pubDate>
    </item>
    <item>
      <title>没有自由修剪：初始化时修剪的信息论障碍</title>
      <link>https://arxiv.org/abs/2402.01089</link>
      <description><![CDATA[“彩票”arXiv:1803.03635 在初始化或接近初始化时的存在引发了一个诱人的问题：深度学习中是否需要大型模型，或者是否可以快速识别和训练稀疏网络，而无需训练包含它们的密集模型。然而，在不训练密集模型（“初始化时修剪”）的情况下寻找这些稀疏子网络的努力基本上不成功 arXiv：2009.08576。我们对此提出了理论解释，基于模型的有效参数计数 $p_\text{eff}$，由最终网络中非零权重的数量与稀疏掩模之间的互信息之和给出和数据。我们展示了 arXiv:2105.12806 的鲁棒性定律扩展到稀疏网络，其中通常的参数计数被 $p_\text{eff}$ 替换，这意味着鲁棒地插入噪声数据的稀疏神经网络需要严重依赖于数据的掩码。我们假设训练期间和之后的修剪输出的掩码具有比初始化时修剪产生的互信息更高的互信息。因此，两个网络可能具有相同的稀疏性，但根据它们的训练方式，有效参数计数有所不同。这表明在初始化附近进行修剪可能是不可行的，并解释了为什么彩票存在，但无法快速找到（即不训练整个网络）。神经网络实验证实，训练过程中获得的信息确实可能会影响模型容量。]]></description>
      <guid>https://arxiv.org/abs/2402.01089</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:43 GMT</pubDate>
    </item>
    <item>
      <title>可扩展的高阶张量积样条模型</title>
      <link>https://arxiv.org/abs/2402.01090</link>
      <description><![CDATA[在当前海量数据和透明机器学习的时代，技术必须能够大规模运行，同时提供对该方法内部运作的清晰的数学理解。尽管已经存在用于考虑数据非线性的大规模应用的可解释的半参数回归方法，但模型的复杂性通常仍然受到限制。主要挑战之一是这些模型中缺乏交互作用，为了更好的可解释性而将其排除，但也是由于不切实际的计算成本。为了克服这一限制，我们提出了一种使用因式分解方法来导出高度可扩展的高阶张量积样条模型的新方法。我们的方法允许合并非线性特征效应的所有（高阶）交互，同时计算成本与没有交互的模型成正比。我们进一步开发了一种有意义的惩罚方案并检查了引发的优化问题。我们通过评估我们的方法的预测和估计性能来得出结论。]]></description>
      <guid>https://arxiv.org/abs/2402.01090</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:43 GMT</pubDate>
    </item>
    <item>
      <title>神经尺度规律的动态模型</title>
      <link>https://arxiv.org/abs/2402.01092</link>
      <description><![CDATA[在各种任务中，神经网络的性能可预测地随着训练时间、数据集大小和模型大小的提高而提高多个数量级。这种现象被称为神经标度定律。最重要的是计算最佳缩放法则，它在最佳选择模型大小时将性能报告为计算单位的函数。我们分析用梯度下降训练的随机特征模型作为网络训练和泛化的可解模型。这再现了许多关于神经标度定律的观察结果。首先，我们的模型预测了为什么性能随训练时间和模型大小的变化具有不同的幂律指数。因此，该理论预测了一种不对称的计算最优缩放规则，其中训练步骤数的增加速度快于模型参数的增加速度，这与最近的经验观察结果一致。其次，据观察，在训练早期，网络以 $1/\textit{width}$ 的速率收敛到无限宽度动态，但在后期表现出 $\textit{width}^{-c}$ 的速率，其中 $c$ 取决于架构和任务的结构。我们证明我们的模型表现出这种行为。最后，我们的理论表明，由于数据的重复重用，训练和测试损失之间的差距如何随着时间的推移逐渐扩大。]]></description>
      <guid>https://arxiv.org/abs/2402.01092</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:43 GMT</pubDate>
    </item>
    <item>
      <title>具有相关误差的多元概率时间序列预测</title>
      <link>https://arxiv.org/abs/2402.01000</link>
      <description><![CDATA[对误差之间的相关性进行建模与模型量化概率时间序列预测中的预测不确定性的准确程度密切相关。最近的多变量模型在解释误差之间的同期相关性方面取得了重大进展，而对这些误差的一个常见假设是，为了统计简单性，它们在时间上是独立的。然而，现实世界的观察常常偏离这一假​​设，因为由于各种因素（例如排除时间相关的协变量），误差通常表现出显着的自相关性。在这项工作中，我们提出了一种基于协方差矩阵的低秩加对角参数化的有效方法，它可以有效地表征误差的自相关性。所提出的方法具有几个理想的特性：复杂性不随时间序列的数量而变化，所得协方差可用于校准预测，并且可以与任何具有高斯分布误差的模型无缝集成。我们使用两种不同的神经预测模型（GPVar 和 Transformer）凭经验证明了这些属性。我们的实验结果证实了我们的方法在提高多个现实世界数据集的预测准确性和不确定性量化质量方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.01000</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:42 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯非参数潜在块模型的分布式 MCMC 推理</title>
      <link>https://arxiv.org/abs/2402.01050</link>
      <description><![CDATA[在本文中，我们介绍了一种新颖的分布式马尔可夫链蒙特卡罗（MCMC）推理方法，用于贝叶斯非参数潜在块模型（DisNPLBM），采用主/工人架构。我们的非参数共聚类算法使用潜在多元高斯块分布将观测值和特征划分为多个分区。行上的工作负载均匀分布在工作人员之间，这些工作人员只与主设备通信，而不是在它们之间进行通信。 DisNPLBM 通过实验结果展示了其对集群标记准确性和执行时间的影响。此外，我们提出了一个将我们的方法应用于共簇基因表达数据的真实用例。代码源可在 https://github.com/redakhoufache/Distributed-NPLBM 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2402.01050</guid>
      <pubDate>Tue, 06 Feb 2024 00:56:42 GMT</pubDate>
    </item>
    </channel>
</rss>