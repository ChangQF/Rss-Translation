<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Fri, 07 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>在上下文学习任务中，有两个具有复杂功能</title>
      <link>https://arxiv.org/abs/2502.03503</link>
      <description><![CDATA[ARXIV：2502.03503V1公告类型：新 
摘要：我们在上下文学习（ICL）任务中检查了两个在变压器模型的数学和测试设置中的数学功能。我们的研究通过表明小型变压器（甚至只有注意层的模型）可以近似任意多项式函数，因此在某些条件下连续函数来概括对线性函数的工作。我们的模型还可以近似以前看不见的多项式函数以及复杂函数的零。我们的模型在此任务上的表现要比GPT4等LLM好得多，并在提供合适的培训数据和方法时涉及复杂的推理。我们的模型也有重要的局限性；他们无法在培训分布之外概括，因此不要学习班级的功能。我们解释了为什么是这样。]]></description>
      <guid>https://arxiv.org/abs/2502.03503</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$ \ beta- $和$ \ phi- $混合序列的希尔伯特空间中的在线学习算法</title>
      <link>https://arxiv.org/abs/2502.03551</link>
      <description><![CDATA[ARXIV：2502.03551V1公告类型：新 
摘要：在本文中，我们研究了基于一类依赖过程的繁殖内核希尔伯特空间（RKHS）中的在线算法，称为混合过程。对于这样的过程，依赖性程度是通过各种混合系数来衡量的。作为一个代表性的例子，我们分析了一个严格的固定马尔可夫链，其中依赖性结构的特征是\（\ beta- \）和\（\ phi  -  \）混合系数。对于这些因样品，我们得出了几乎最佳的收敛速率。我们的发现扩展了I.I.D.的现有错误界限观察结果，表明I.I.D.案例是我们框架的特殊实例。此外，我们明确说明了马尔可夫链中的依赖性结构引入的附加因素。]]></description>
      <guid>https://arxiv.org/abs/2502.03551</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用最佳传输的多元共形预测</title>
      <link>https://arxiv.org/abs/2502.03609</link>
      <description><![CDATA[ARXIV：2502.03609V1公告类型：新 
摘要：共形预测（CP）通过构建合理的输出组来量化机器学习模型的不确定性。这些集合是通过利用所谓的合格分数，使用感兴趣的输入点，预测模型和过去观察结果计算的数量来构建的。然后，通过评估所有可能的输出的一致性得分，并根据其分数的等级选择它们来获得CP集。由于这一排名步骤，大多数CP方法都依赖于单变量的分数函数。将这些分数扩展到多元空间的挑战在于，没有规范的向量顺序。为了解决这个问题，我们利用基于最佳运输（OT）的多元分数排名的自然扩展。我们的方法OTCP提供了一个原则上的框架，用于在多维设置中构建共形预测集，从而通过有限的数据示例保证了无分布的覆盖范围。我们在多元回归问题的基准数据集中证明了切实的收益，并解决了通过ot映射估算合格分数时出现的计算\＆统计权衡。]]></description>
      <guid>https://arxiv.org/abs/2502.03609</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于规则的日期模糊系统的时间序列预测：基于2型模糊集措施方法的新观点</title>
      <link>https://arxiv.org/abs/2502.03650</link>
      <description><![CDATA[ARXIV：2502.03650V1公告类型：新 
摘要：现实世界数据包含不确定性和变化，这些变化可以与外部变量（称为随机性）相关。随机性的另一种原因是混乱，这可能是混乱时间序列的重要组成部分。处理此类数据的现有方法之一是使用不断发展的模糊系统（EFSS），由于其自​​主权处理数据和高度的自主权，因此已被证明是时间序列预测的强大模型实际应用中的复杂问题。但是，由于其工作结构，对于高度不确定的情况，类型2模糊集可以胜过类型1的模糊集。然后，我们提出了EPL-KRLS-FSM+，这是一种结合参与性学习（PL）的增强类别的模糊建模方法，内核递归最小二乘法（KRLS），类型2模糊逻辑和数据转换为模糊集（FSS）。这种改进允许创建和测量2型模糊集，以更好地处理数据中的不确定性，从而生成一个模型，该模型可以以提高精度预测混乱的数据。使用两个复杂的数据集评估该模型：混乱的时间序列Mackey -Glass延迟差分方程不同程度的混乱程度，以及台湾资本化加权股票指数-TAIEX的主要库存指数。将模型性能与相关的基于规则的EFS模型和经典方法进行比较，并根据错误指标，运行时和最终规则数量进行分析。预测结果表明，所提出的模型具有竞争力，并且与Type-1模型相比始终如一地表现，也通过显示最低的误差指标和最终规则数来优于其他预测方法。]]></description>
      <guid>https://arxiv.org/abs/2502.03650</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过梯度下降学习率约束指导两层神经网络Lipschitzness</title>
      <link>https://arxiv.org/abs/2502.03792</link>
      <description><![CDATA[ARXIV：2502.03792V1公告类型：新 
摘要：我们证明，将最终的衰减应用于经验风险最小化（ERM）中，使用标准梯度下降（GD）将均方体误差损失最小化，以训练具有两层神经网络，并Lipschitz的激活功能可确保所得网络表现出高度的Lipschitz规律性，即是小的Lipschitz常数。此外，我们表明，这种衰减并不会阻碍目前以Huber损失衡量的经验风险的收敛速率，直到非Convex经验风险的关键点。从这些发现中，我们得出了经过GD训练的两层神经网络的概括界限，并具有衰减的LR，其依赖于其可训练参数的数量，这表明这些网络的统计行为独立于过度参数化。我们通过一系列玩具数值实验来验证我们的理论结果，令人惊讶的是，我们观察到，经过恒定步骤尺寸的GD训练的网络具有与经过衰减LR训练的人相似的学习和规律性。这表明接受标准GD培训的神经网络可能已经是高度常规的学习者。]]></description>
      <guid>https://arxiv.org/abs/2502.03792</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于概率负载预测的多任务在线学习</title>
      <link>https://arxiv.org/abs/2502.04163</link>
      <description><![CDATA[ARXIV：2502.04163V1公告类型：新 
摘要：负载预测对于电力系统的高效，可靠和具有成本效益的管理至关重要。通过学习多个实体（例如区域，建筑物）之间的相似性，可以改善负载预测性能。基于多任务学习的技术通过利用多个实体及其关系的历史负载需求的消费模式来获得预测。但是，现有技术无法有效地评估负载需求中固有的不确定性，也无法说明消费模式的动态变化。本文提出了一种用于在线和概率负载预测的多任务学习技术。该技术通过利用其动态相似性来为多个实体的负载提供准确的概率预测。该方法的性能是使用注册多个实体的负载需求并包含多种多样和动态消耗模式的数据集评估的。实验结果表明，所提出的方法可以显着提高各种负载消耗方案中当前多任务学习方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.04163</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学生 - 过程作为后贝叶斯神经网络的无限宽度限制</title>
      <link>https://arxiv.org/abs/2502.04247</link>
      <description><![CDATA[ARXIV：2502.04247V1公告类型：新 
摘要：已经对贝叶斯神经网络（BNN）的渐近性质进行了广泛的研究，特别是关于它们在无限宽度限制中的高斯过程的近似值。我们通过表明后BNN可以通过Student-T过程近似后扩展这些结果，从而在建模不确定性时具有更大的灵活性。具体而言，我们表明，如果BNN的参数遵循高斯先验分布，以及最后一个隐藏层和高斯的可能性函数的方差遵循逆伽玛的先验分布，则结果后BNN收敛到学生 - 无限宽度限制的t过程。我们的证明利用了Wasserstein指标来建立对Student-T过程近似收敛速率的控制。]]></description>
      <guid>https://arxiv.org/abs/2502.04247</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>线性PDE中的反问题的高斯过程回归</title>
      <link>https://arxiv.org/abs/2502.04276</link>
      <description><![CDATA[ARXIV：2502.04276V1公告类型：新 
摘要：本文在系统理论中介绍了一种计算有效的算法，用于解决由线性部分微分方程（PDE）控制的反问题。我们使用高斯工艺对线性PDE的解决方案进行对基于先进的交换代数和代数分析定义的先验的过程。这些先验的实现是算法的，并使用MacAulay2计算机代数软件实现了。一个示例应用程序包括识别经典波动方程的嘈杂数据的波速，该数据广泛用于物理。该方法在提高计算效率的同时达到了高度的精度。]]></description>
      <guid>https://arxiv.org/abs/2502.04276</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预测驱动的电子价值</title>
      <link>https://arxiv.org/abs/2502.04294</link>
      <description><![CDATA[ARXIV：2502.04294V1公告类型：新 
摘要：质量统计推断需要足够数量的数据，这些数据可能会丢失或难以获得。为此，预测驱动的推论已成为一种有希望的方法，但是现有方法在很大程度上仅限于Z估计问题，例如手段和分位数的推理。在本文中，我们将预测推理的思想应用于电子价值。通过这样做，我们继承了电子价值的所有常规好处，例如任何时间效率，事后有效性和多功能顺序推断 - 以及以预测驱动的方式实现的一组推论集。特别是，我们表明，可以用电子价值来构建的每个推论过程都有一个预测驱动的对应物，由我们的方法给出。我们在广泛的推理任务中展示了框架的有效性，从简单的假设测试和置信区间到更改点检测和因果发现的更多涉及的程序，这些程序无法实现以前的技术。我们的方法是模块化的，易于整合到现有算法中，这是实用应用的引人注目的选择。]]></description>
      <guid>https://arxiv.org/abs/2502.04294</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无监督域适应的Stein差异</title>
      <link>https://arxiv.org/abs/2502.03587</link>
      <description><![CDATA[ARXIV：2502.03587V1公告类型：交叉 
摘要：无监督的域适应（UDA）利用标签源数据集的信息来提高相关但未标记的目标数据集的准确性。 UDA的一种常见方法是通过最大程度地减少其数据分布之间的距离来对齐表示形式。先前的方法采用了距离，例如瓦斯坦距离和最大平均差异。但是，当目标数据明显稀缺时，这些方法的有效性较小。 Stein差异是分布之间的不对称距离，仅通过其得分函数依赖一个分布。在本文中，我们提出了一种新颖的\ ac {uda}方法，该方法使用Stein差异来测量源和目标域之间的距离。我们使用非内分和内核的Stein差异来开发学习框架。从理论上讲，我们为概括误差提供了上限。数值实验表明，我们的方法在只有少量的目标数据可用时，使用其他域差异量度优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2502.03587</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>类型2 tobit样品选择模型与贝叶斯添加剂回归树</title>
      <link>https://arxiv.org/abs/2502.03600</link>
      <description><![CDATA[ARXIV：2502.03600V1公告类型：交叉 
摘要：本文介绍了2型Tobit贝叶斯添加剂回归树（Tobart-2）。巴特可以产生准确的个人特异性治疗效果估计。但是，实际上，估计通常会因样本选择而偏差。我们将2型TOBIT样本选择模型扩展到通过在选择方程和结果方程中包括树的总和来考虑非线性和模型不确定性。错误项的dirichlet过程混合物分布允许脱离双变量正态分布错误的假设。柔软的树木和差异概率的事先提前改善了光滑和稀疏数据生成过程的建模。我们包括一项仿真研究和兰德健康保险实验数据集的应用。]]></description>
      <guid>https://arxiv.org/abs/2502.03600</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>未实现的期望：比较AI方法与最大独立集的经典算法</title>
      <link>https://arxiv.org/abs/2502.03669</link>
      <description><![CDATA[ARXIV：2502.03669V1公告类型：交叉 
摘要：AI方法，例如生成模型和增强学习，最近已应用于组合优化（CO）问题，尤其是NP-HARD问题。本文将基于GPU的方法与基于CPU的经典方法（MIS）进行了比较。标准图系列的实验表明，基于AI的算法无法跑赢大盘，并且在许多情况下，可以匹配在单个CPU上运行的最先进的经典求解器Kamis的解决方案质量。一些基于GPU的方法甚至类似于最简单的启发式，基于学位的贪婪。即使采用了当地搜索等后处理技术，基于AI的方法仍然比基于CPU的求解器更糟糕。
  我们开发了一种新的分析模式，以揭示非折进AI方法，例如LTFT（基于Gflownets），最终与最简单的贪婪方法相似，因此比Kamis更糟。我们还发现，基于CPU的算法，尤其是KAMIS，在稀疏的随机图上具有很强的性能，这似乎反驳了众所周知的猜想上限，从而从Coja-Oghlan＆Efthymiou（2015）反驳了有效的算法。]]></description>
      <guid>https://arxiv.org/abs/2502.03669</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过离散自动回归偏置来控制的LLM解码</title>
      <link>https://arxiv.org/abs/2502.03685</link>
      <description><![CDATA[ARXIV：2502.03685V1公告类型：交叉 
摘要：受控文本生成允许对大型语言模型输出的用户定义的约束，这是越来越重要的领域，因为LLMS在日常生活中变得更加普遍。一种常见的方法使用基于能量的解码，该解码通过能量函数定义目标分布，将多个约束结合到加权平均值中。但是，即使对能量函数的系数进行了广泛的调整，这些方法通常也很难平衡流利性和限制满意度。在本文中，我们确定了这种次优的平衡是由在连续空间中进行采样而不是文本令牌的自然离散空间。为了解决这个问题，我们提出了离散自动回归偏置，这是一种受控的解码算法，在完全在离散文本域中运行时利用梯度。具体而言，我们通过在生成的序列和辅助偏置序列上定义联合分布来引入一种新的公式，以实现受控文本生成。为了有效地从该联合分布中采样，我们提出了使用基于梯度的离散MCMC的langevin-gibbs采样算法。我们的方法显着提高了限制满意度，同时保持可比或更好的流利度，所有这些都以较低的计算成本。我们证明了我们受控解码方法在情感控制，语言排毒和关键字引导生成方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.03685</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型中指导的变分控制</title>
      <link>https://arxiv.org/abs/2502.03686</link>
      <description><![CDATA[ARXIV：2502.03686V1公告类型：交叉 
摘要：扩散模型表现出极好的样本质量，但是现有的指导方法通常需要进行其他模型培训或仅限于特定任务。从变异推理和控制的角度来看，我们在扩散模型中重新访问了指导，引入了扩散轨迹匹配（DTM），从而使指导预处理的扩散轨迹能够满足末端成本。 DTM统一了广泛的指导方法，并实现了新颖的实例。我们在此框架内介绍了一种新方法，该方法在几个线性和（盲）非线性反问题上实现最先进的结果，而无需进行其他模型培训或修改。例如，在Imagenet非线性脱毛上，我们的模型获得了34.31的FID得分，比预审预周读的方法的基线显着提高（FID 78.07）。我们将在将来的更新中提供代码。]]></description>
      <guid>https://arxiv.org/abs/2502.03686</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>汇总和征服：通过在多层上结合非线性预测因子来检测和转向LLM概念</title>
      <link>https://arxiv.org/abs/2502.03708</link>
      <description><![CDATA[ARXIV：2502.03708V1公告类型：交叉 
摘要：训练有素的大语言模型（LLM）包含许多人类知识。但是，很难衡量这些知识的程度或准确性，因为LLM并不总是``知道他们知道什么&#39;&#39;，甚至可能会积极误导。在这项工作中，我们提供了一种通用方法，用于检测LLMS内部激活中的语义概念。此外，我们表明我们的方法可以很容易地适应转向llms朝着理想的产出。我们的创新如下：（1）我们使用非线性特征学习方法来识别重要的线性方向，以预测每一层的概念； （2）我们跨层汇总了特征，以构建强大的概念探测器和转向机制。我们通过在七个基准测试中获得幻觉，有害性，毒性和不真实的内容来展示方法的力量。我们通过将LLMS转向新概念来强调我们方法的普遍性，据我们所知，这些概念以前在文献中没有被考虑，包括：语义歧义，人类语言，编程语言，幻觉的回答，科学主题，诗意/莎士比亚英语，甚至是多个概念。此外，我们的方法可以用数值属性（例如产品评论）引导概念。我们在https://github.com/dmbeaglehole/neural_controllers上提供代码（包括我们方法的简单API）。]]></description>
      <guid>https://arxiv.org/abs/2502.03708</guid>
      <pubDate>Fri, 07 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>