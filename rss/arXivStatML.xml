<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 27 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>高斯随机场的第二个最大值和精确 (t-) 间距检验</title>
      <link>https://arxiv.org/abs/2406.18397</link>
      <description><![CDATA[arXiv:2406.18397v1 公告类型：交叉
摘要：在本文中，我们引入了黎曼子流形上高斯随机场的第二个最大值的新概念。这个第二个最大值是表征最大值分布的有力工具。通过利用一个临时的 Kac Rice 公式，我们推导出最大值分布的显式形式，该形式以第二个最大值和黎曼 Hessian 的一些回归分量为条件。这种方法基于对这些最大值之间间距的评估得出一个精确的测试，我们将其称为间距测试。
我们研究了该测试在高斯对称张量、连续稀疏反卷积和具有平滑整流器的双层神经网络中检测稀疏替代方案的适用性。我们的理论结果得到了数值实验的支持，这些实验说明了所提出的测试的校准性和功效。更一般地，该检验可应用于黎曼流形上的任何高斯随机场，并且我们为间距检验在连续稀疏核回归中的应用提供了一个通用框架。
此外，当高斯随机场的方差-协方差函数已知到某个比例因子时，我们推导出我们检验的精确学生化版本，称为 $t$ 间距检验。该检验在零假设下经过完美校准，并且具有检测稀疏替代方案的强大功效。]]></description>
      <guid>https://arxiv.org/abs/2406.18397</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:47 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士会梦见大象吗（当被告知不要梦见大象时）？Transformer 中的潜在概念联想和联想记忆</title>
      <link>https://arxiv.org/abs/2406.18400</link>
      <description><![CDATA[arXiv:2406.18400v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 具有存储和回忆事实的能力。通过对开源模型的实验，我们观察到这种检索事实的能力可以通过改变上下文轻松操纵，甚至无需改变其事实含义。这些发现强调 LLM 可能表现得像一个联想记忆模型，其中上下文中的某些标记可​​作为检索事实的线索。我们通过研究 LLM 的构建块转换器如何完成此类记忆任务来从数学上探索这一特性。我们研究了具有单层转换器的简单潜在概念关联问题，并从理论和经验上证明了转换器使用自注意力收集信息并使用值矩阵进行联想记忆。]]></description>
      <guid>https://arxiv.org/abs/2406.18400</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:47 GMT</pubDate>
    </item>
    <item>
      <title>使用分布压缩实现高效准确的解释估计</title>
      <link>https://arxiv.org/abs/2406.18334</link>
      <description><![CDATA[arXiv:2406.18334v1 公告类型：交叉 
摘要：各种机器学习解释的精确计算需要大量的模型评估，在极端情况下变得不切实际。随着数据和模型参数的不断增加，近似的计算成本也会增加。已经提出了许多启发式方法来有效地近似事后解释。本文表明，在广泛的解释估计算法中使用的标准 i.i.d. 采样会导致值得改进的近似误差。为此，我们引入了压缩然后解释 (CTE)，这是一种更高效、更准确的解释估计的新范式。CTE 通过核稀疏使用分布压缩来获得最接近边际分布的数据样本。我们表明，CTE 以可忽略不计的计算开销改进了基于移除的局部和全局解释的估计。它通常使用少 2-3 倍的样本（即需要少 2-3 倍的模型评估）来实现同等的解释近似误差。CTE 是一个简单但功能强大的插件，适用于任何现在依赖 i.i.d. 采样的解释方法。]]></description>
      <guid>https://arxiv.org/abs/2406.18334</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>几乎毫无遗憾地学习纯量子态</title>
      <link>https://arxiv.org/abs/2406.18370</link>
      <description><![CDATA[arXiv:2406.18370v1 公告类型：交叉 
摘要：我们启动了具有最小遗憾的量子态断层扫描研究。学习者可以连续访问未知的纯量子态，并在每一轮中选择一个纯探测状态。如果未知状态与该探测正交测量，则会产生遗憾，而学习者的目标是在 $T$ 轮中最小化预期的累积遗憾。挑战在于在最具信息量的测量和产生最小遗憾的测量之间找到平衡。我们使用一种基于均值最小二乘估计中值的新断层扫描算法表明，累积遗憾的规模为 $\Theta(\operatorname{polylog} T)$。该算法采用偏向未知状态的测量，并产生在观察到的样本数量上最优（最多对数项）的在线估计。]]></description>
      <guid>https://arxiv.org/abs/2406.18370</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>通过边界增强软 Q 学习</title>
      <link>https://arxiv.org/abs/2406.18033</link>
      <description><![CDATA[arXiv:2406.18033v1 公告类型：交叉 
摘要：代理利用过去经验的能力对于有效解决新任务至关重要。先前的工作重点是使用价值函数估计来获得新任务解决方案的零样本近似值。在软 Q 学习中，我们展示了如何使用任何价值函数估计来推导最佳价值函数的双边界限。导出的界限导致了提高训练性能的新方法，我们通过实验验证了这一点。值得注意的是，我们发现所提出的框架提出了一种更新 Q 函数的替代方法，从而提高了性能。]]></description>
      <guid>https://arxiv.org/abs/2406.18033</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络在过度参数化下的局部线性恢复保证</title>
      <link>https://arxiv.org/abs/2406.18035</link>
      <description><![CDATA[arXiv:2406.18035v1 公告类型：交叉 
摘要：确定深度神经网络 (DNN) 模型是否能够在过度参数化的情况下可靠地恢复目标函数是深度学习理论中一个关键而复杂的问题。为了加深对这一领域的理解，我们引入了一个概念，我们称之为“局部线性恢复”(LLR)，这是一种较弱的目标函数恢复形式，使问题更适合理论分析。在 LLR 的意义上，我们证明，用较窄的 DNN 表达的函数保证可以从比模型参数更少的样本中恢复。具体而言，我们为给定 DNN 空间中的函数建立了乐观样本大小的上限，定义为保证 LLR 所需的最小样本大小。此外，我们证明这些上限在两层 tanh 神经网络的情况下是可以实现的。我们的研究为未来研究 DNN 在过度参数化场景中的恢复能力奠定了坚实的基础。]]></description>
      <guid>https://arxiv.org/abs/2406.18035</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>学习具有稀疏激活的神经网络</title>
      <link>https://arxiv.org/abs/2406.17989</link>
      <description><![CDATA[arXiv:2406.17989v1 公告类型：交叉 
摘要：许多成功的神经网络架构中都存在一个核心组件，即两个完全连接层的 MLP 块，中间有一个非线性激活。包括在 Transformer 架构中，经验观察到的一个有趣现象是，经过训练后，此 MLP 块隐藏层中的激活在任何给定输入上都趋于极其稀疏。与传统形式的稀疏性（其中存在可以从网络中删除的神经元/权重）不同，这种形式的 {\em 动态} 激活稀疏性似乎更难利用来获得更高效的网络。受此启发，我们发起了一项关于表现出激活稀疏性的 MLP 层的 PAC 可学习性的正式研究。我们提出了各种结果，表明此类函数确实比非稀疏函数具有可证明的计算和统计优势。我们希望对稀疏激活网络有更好的理论理解，从而产生可以在实践中利用激活稀疏性的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.17989</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>核方法的可扩展双坐标下降法</title>
      <link>https://arxiv.org/abs/2406.18001</link>
      <description><![CDATA[arXiv:2406.18001v1 公告类型：交叉 
摘要：双坐标下降 (DCD) 和块双坐标下降 (BDCD) 是解决凸优化问题的重要迭代方法。在这项工作中，我们为核支持向量机 (K-SVM) 和核岭回归 (K-RR) 问题开发了可扩展的 DCD 和 BDCD 方法。在分布式内存并行机器上，这些方法的可扩展性受到每次迭代都需要通信的限制。在通信成本高出几个数量级的现代硬件上，DCD 和 BDCD 方法的运行时间主要由通信成本决定。我们通过分别推导 DCD 和 BDCD 的 $s$ 步变体来解决这个通信瓶颈，以解决 K-SVM 和 K-RR 问题。$s$ 步变体将通信频率降低了可调因子 $s$，但需要额外的带宽和计算。 $s$ 步变体计算的解与精确算法中现有的方法相同。我们进行了数值实验，以说明 $s$ 步变体在有限算法中也是数值稳定的，即使对于较大的 $s$ 值也是如此。我们进行了理论分析，以将新设计的变体的计算和通信成本限制在领先阶。最后，我们开发了用 C 和 MPI 编写的高性能实现，并在 Cray EX 集群上进行了扩展实验。新的 $s$ 步变体使用多达 $512$ 个核心，实现了比现有方法高达 $9.8\times$ 的强大扩展速度。]]></description>
      <guid>https://arxiv.org/abs/2406.18001</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>规划是什么类型的推理？</title>
      <link>https://arxiv.org/abs/2406.17863</link>
      <description><![CDATA[arXiv:2406.17863v1 公告类型：交叉
摘要：概率图形模型有多种类型的推理，例如边际、最大后验，甚至边际最大后验。研究人员在谈论“规划作为推理”时指的是哪一种？文献中没有一致性，使用了不同类型的推理，并且它们进行规划的能力与特定的近似值或附加约束进一步纠缠在一起。在这项工作中，我们使用变分框架来表明，所有常用的推理类型都对应于变分问题中熵项的不同权重，并且规划与一组不同的权重完全对应。这意味着变分推理的所有技巧都可以很容易地应用于规划。我们开发了一种循环信念传播的类似物，使我们能够在分解状态马尔可夫决策过程中执行近似规划，而不会因指数级大的状态空间而产生难解性。变分视角表明，之前的规划推理类型仅适用于随机性较低的环境，并允许我们根据每种类型的优点对其进行描述，从而将推理类型与其实际使用所需的额外近似值区分开来。我们在国际规划竞赛中提出的合成 MDP 和任务上对这些结果进行了实证验证。]]></description>
      <guid>https://arxiv.org/abs/2406.17863</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>通过可学习的后期交互实现高效的文档排序</title>
      <link>https://arxiv.org/abs/2406.17968</link>
      <description><![CDATA[arXiv:2406.17968v1 公告类型：交叉 
摘要：交叉编码器 (CE) 和双编码器 (DE) 模型是信息检索中查询文档相关性的两种基本方法。为了预测相关性，CE 模型使用联合查询文档嵌入，而 DE 模型维护分解的查询和文档嵌入；通常，前者具有更高的质量，而后者受益于较低的延迟。最近，已经提出了后期交互模型来实现更有利的延迟质量权衡，通过使用 DE 结构，然后使用基于查询和文档标记嵌入的轻量级评分器。然而，这些轻量级评分器通常是手工制作的，并且人们对它们的近似能力没有了解；此外，这样的评分器需要访问单个文档标记嵌入，这会增加延迟和存储负担。在本文中，我们提出了解决这些问题的新型可学习后期交互模型 (LITE)。从理论上讲，我们证明 LITE 是连续评分函数的通用近似器，即使对于相对较小的嵌入维度也是如此。从经验上讲，LITE 在域内和零样本重新排序任务上都优于之前的后期交互模型（例如 ColBERT）。例如，对 MS MARCO 段落重新排序的实验表明，与 ColBERT 相比，LITE 不仅产生了具有更好泛化的模型，而且延迟更低，并且所需的存储空间是 ColBERT 的 0.25 倍。]]></description>
      <guid>https://arxiv.org/abs/2406.17968</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>稀疏深度神经网络用于高维稀疏回归中的非参数估计</title>
      <link>https://arxiv.org/abs/2406.18137</link>
      <description><![CDATA[arXiv:2406.18137v1 公告类型：新
摘要：在高维环境下，稀疏深度神经网络的泛化理论已经建立。除了泛化之外，参数估计也很重要，因为它对于深度神经网络的变量选择和可解释性至关重要。当前关于参数估计的理论研究主要集中在两层神经网络上，这是因为参数估计的收敛性严重依赖于 Hessian 矩阵的正则性，而深度神经网络的 Hessian 矩阵具有高度奇异性。为了避免深度神经网络在参数估计中的不可识别性，我们提出对输入的偏导数进行非参数估计。我们首先证明，当参数的 $\ell_{1}$ 范数得到很好的约束时，样本复杂度仅随参数数量或输入维数的对数增长，从而保证稀疏深度神经网络的模型收敛性。然后通过限制偏导数的范数和散度，我们建立了偏导数非参数估计的收敛速度为$\mathcal{O}(n^{-1/4})$，该速度比模型收敛速度$\mathcal{O}(n^{-1/2})$慢。据我们所知，这项研究首次将非参数估计与参数稀疏深度神经网络结合起来。由于偏导数的非参数估计对于非线性变量选择具有重要意义，目前的结果为深度神经网络的可解释性带来了光明的未来。]]></description>
      <guid>https://arxiv.org/abs/2406.18137</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>受局部线性嵌入启发的边界检测算法</title>
      <link>https://arxiv.org/abs/2406.18456</link>
      <description><![CDATA[arXiv:2406.18456v1 公告类型：新
摘要：在高维数据的研究中，通常假设数据集具有底层低维结构。这种结构的实用模型是具有边界的嵌入紧流形。由于底层流形结构通常是未知的，因此从流形上分布的数据中识别边界点对于各种应用都至关重要。在这项工作中，我们提出了一种检测边界点的方法，该方法受到广泛使用的局部线性嵌入算法的启发。我们使用两种最近邻域搜索方案实现该方法：$\epsilon$-半径球方案和$K$-最近邻方案。该算法结合了数据结构的几何信息，特别是通过它与局部协方差矩阵的密切关系。我们通过探索两种邻域搜索方案中局部协方差矩阵的谱特性来讨论关键参数的选择并分析算法。此外，我们通过模拟示例展示了该算法的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.18456</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>对抗鲁棒性随机平滑中的统计估计问题处理</title>
      <link>https://arxiv.org/abs/2406.17830</link>
      <description><![CDATA[arXiv:2406.17830v1 公告类型：新
摘要：随机平滑是一种流行的经过认证的对抗攻击防御方法。本质上，我们需要解决一个统计估计问题，这通常非常耗时，因为我们需要对每个要认证的点执行大量（通常为 10^5）分类器前向传递。在本文中，我们回顾了随机平滑的统计估计问题，以确定计算负担是否必要。特别是，我们考虑对抗鲁棒性的（标准）任务，其中我们需要在保持统计保证的同时使用尽可能少的样本来决定一个点在某个半径内是否稳健。我们提出了使用置信序列的估计程序，这些置信序列享有与标准方法相同的统计保证，具有估计任务的最佳样本复杂度，并通过经验证明了它们的良好性能。此外，我们提供了 Clopper-Pearson 置信区间的随机版本，从而产生了严格更强大的证书。]]></description>
      <guid>https://arxiv.org/abs/2406.17830</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>动作擦除下的 Bandits 学习</title>
      <link>https://arxiv.org/abs/2406.18072</link>
      <description><![CDATA[arXiv:2406.18072v1 公告类型：新
摘要：我们考虑一种新颖的多臂老虎机 (MAB) 设置，其中学习者需要通过擦除通道将动作传达给分布式代理，而动作的奖励则通过外部传感器直接提供给学习者。在我们的模型中，虽然分布式代理知道某个动作是否被擦除，但中央学习者不知道（没有反馈），因此不知道观察到的奖励是否来自期望的动作。我们提出了一种可以在任何（现有或未来的）MAB 算法上运行的方案，并使其对动作擦除具有鲁棒性。我们的方案导致动作擦除通道的最坏情况遗憾最多是底层 MAB 算法的无擦除最坏情况遗憾的 $O(1/\sqrt{1-\epsilon})$ 倍，其中 $\epsilon$ 是擦除概率。我们还提出了一种对连续臂消除算法的修改，并证明其最坏情况遗憾值为 $\Tilde{O}(\sqrt{KT}+K/(1-\epsilon))$，我们通过提供匹配的下限证明其是最优的。]]></description>
      <guid>https://arxiv.org/abs/2406.18072</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>分布可学习性和稳健性</title>
      <link>https://arxiv.org/abs/2406.17814</link>
      <description><![CDATA[arXiv:2406.17814v1 公告类型：新
摘要：我们研究了分布学习问题的可学习性和稳健（或不可知）可学习性之间的关系。我们表明，与其他学习设置（例如函数类的 PAC 学习）相反，一类概率分布的可实现可学习性并不意味着其不可知可学习性。我们继续研究哪种类型的数据损坏会破坏分布类的可学习性，以及这种可学习性对什么具有稳健性。我们表明，一类分布的可实现可学习性意味着其仅对加性损坏具有稳健的可学习性，但对减性损坏则不具有稳健的可学习性。
我们还探讨了压缩方案和差分隐私可学习性背景下的相关含义。]]></description>
      <guid>https://arxiv.org/abs/2406.17814</guid>
      <pubDate>Fri, 28 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    </channel>
</rss>