<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 06 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>捕捉气候变化：利用深度学习进行随机降尺度分析</title>
      <link>https://arxiv.org/abs/2406.02587</link>
      <description><![CDATA[arXiv:2406.02587v1 公告类型：交叉 
摘要：适应不断变化的气候需要准确的当地气候信息，这是一个计算上具有挑战性的问题。最近的研究使用了生成对抗网络 (GAN)，一种深度学习，来学习复杂的分布并有效地缩小气候变量。在缩小尺度的同时捕捉变化对于估计不确定性和描述极端事件至关重要——这是气候适应的关键信息。由于缩小尺度是一个未确定的问题，许多细尺度状态与粗分辨率状态在物理上是一致的。为了量化这个不适定问题，缩小尺度技术应该是随机的，能够从以低分辨率输入为条件的高分辨率分布中采样实现。以前的随机缩小尺度尝试发现了严重的欠分散，模型无法代表完整的分布。我们提出了三种方法来改进 GAN 的随机校准：a) 在网络内部注入噪声，b) 调整训练过程以明确考虑随机性，c) 使用概率损失指标。我们首先在具有已知分布特性的合成数据集上测试我们的模型，然后在现实的降尺度场景中测试我们的模型，从低分辨率气候协变量预测高分辨率风分量。注入噪声本身大大提高了使用合成数据测试的条件分布和完整分布的质量，但在风场降尺度方面表现不佳，模型仍然分散不足。对于风降尺度，我们发现调整训练方法并包括概率损失可以改善校准。最好的模型，包括这三种变化，在捕捉高分辨率分布的全部变化方面表现出了很大的提高，从而在表征极端情况方面表现出了很大的提高。]]></description>
      <guid>https://arxiv.org/abs/2406.02587</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>边缘机器学习中的数据质量：最新调查</title>
      <link>https://arxiv.org/abs/2406.02600</link>
      <description><![CDATA[arXiv:2406.02600v1 公告类型：交叉 
摘要：使用机器学习 (ML) 训练的数据驱动人工智能 (AI) 系统正在塑造我们生活中日益增长的部分（在规模和重要性方面），包括但不限于推荐系统、自动驾驶技术、医疗诊断、金融服务和个性化营销。一方面，这些系统的巨大影响力要求高质量标准，特别是用于训练它们的数据。另一方面，由于边缘计算和物联网设备的激增，以及它们越来越多地用于训练和部署 ML 模型，建立和维护数据质量 (DQ) 标准变得更具挑战性。边缘环境的性质——以资源有限、数据存储和处理分散为特征——加剧了与数据相关的问题，使它们更频繁、更严重、更难以检测和缓解。从这些观察中可以看出，边缘 ML 的 DQ 研究是当前和未来 AI 系统的安全性和稳健实用性的关键和紧迫的探索方向。尽管如此，边缘 ML 的 DQ 研究仍处于起步阶段。关于该主题的文献仍然零散且分散在不同的研究社区中，迄今为止尚未进行全面的调查。因此，本文旨在通过提供来自多个学科的现有文献的全局视图来填补这一空白，这些文献可以归类为边缘 ML 的 DQ。具体来说，我们提出了边缘计算中数据质量的初步定义，并用它来建立一组 DQ 维度。我们详细探讨了每个维度，包括现有的缓解解决方案。]]></description>
      <guid>https://arxiv.org/abs/2406.02600</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>受约束还是不受约束？基于神经网络的数据方程发现</title>
      <link>https://arxiv.org/abs/2406.02581</link>
      <description><![CDATA[arXiv:2406.02581v1 公告类型：交叉 
摘要：在许多领域，从业者通常依靠微分方程来建模系统。然而，对于许多应用来说，这种方程的理论推导和/或它们的解的准确解析可能是难以解决的。相反，最近开发的方法，包括基于参数估计、算子子集选择和神经网络的方法，允许在可解释性范围内以数据驱动的方式发现常微分方程和偏微分方程 (PDE)。这些策略的成功通常取决于从状态变量的噪声观察中正确识别代表性方程，以及同样重要且与此交织在一起的用于执行这些方程的数学策略。具体而言，后者通常通过无约束优化策略来解决。将 PDE 表示为神经网络，我们建议通过解决约束优化问题并使用类似于物理信息神经网络 (PINN) 的中间状态表示来发现 PDE。该约束优化问题的目标函数促进数据匹配，而约束要求 PDE 在几个空间共置点处得到满足。我们提出了一种罚函数和一种广泛使用的信赖域障碍方法来解决这个约束优化问题，并在数值示例上比较了这些方法。我们对 Burgers 和 Korteweg-De Vreis 方程的结果表明，后一种约束方法优于罚函数，特别是对于较高的噪声水平或较少的共置点。对于这两种方法，我们都使用经典方法（例如有限差分法）来解决这些发现的神经网络 PDE，而不是依赖自动微分的 PINN 型方法。我们简要介绍了其他细小但至关重要的实施细节。]]></description>
      <guid>https://arxiv.org/abs/2406.02581</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>行星因果推断：对贫困地理的影响</title>
      <link>https://arxiv.org/abs/2406.02584</link>
      <description><![CDATA[arXiv:2406.02584v1 公告类型：交叉 
摘要：卫星图像等地球观测数据与机器学习相结合，可以通过预测生活条件对我们理解贫困地理产生深远影响，尤其是在政府得出的经济指标不可用或可能不可信的情况下。最近的研究在使用 EO 数据不仅预测空间经济结果，而且探索因果关系方面取得了进展，这种理解对于下游政策分析至关重要。在这篇评论中，我们首先记录了对因果空间中 EO-ML 分析的兴趣的增长。然后，我们追踪空间统计和 EO-ML 方法之间的关系，然后讨论 EO 数据在因果 ML 管道中的四种使用方式——（1.）下游因果分析的贫困结果归因，（2.）EO 图像去混杂，（3.）基于 EO 的治疗效果异质性，以及（4.）基于 EO 的可运输性分析。最后，我们提供了一个工作流程，说明研究人员如何将 EO 数据纳入因果 ML 分析中。]]></description>
      <guid>https://arxiv.org/abs/2406.02584</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>情境计数：Transformer 在定量任务中的机制研究</title>
      <link>https://arxiv.org/abs/2406.02585</link>
      <description><![CDATA[arXiv:2406.02585v1 公告类型：交叉 
摘要：Transformers 已经彻底改变了不同领域的机器学习，但了解它们的行为仍然至关重要，特别是在高风险应用中。本文介绍了上下文计数任务，这是一个新颖的玩具问题，旨在增强我们对定量和科学背景下的 Transformers 的理解。此任务需要在数据集内进行精确定位和计算，类似于对象检测或基于区域的科学分析。我们使用因果和非因果 Transformer 架构进行理论和实证分析，研究各种位置编码对性能和可解释性的影响。特别是，我们发现因果注意力更适合这项任务，并且没有位置嵌入可以带来最佳准确性，尽管旋转嵌入具有竞争力并且更容易训练。我们还表明，分布外性能与它使用哪些标记作为偏差项紧密相关。]]></description>
      <guid>https://arxiv.org/abs/2406.02585</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>具有多个输出和卷积层的有限宽度贝叶斯深度线性网络中的特征学习</title>
      <link>https://arxiv.org/abs/2406.03260</link>
      <description><![CDATA[arXiv:2406.03260v1 公告类型：新
摘要：深度线性网络已被广泛研究，因为它们提供了简化的深度学习模型。然而，对于具有多个输出和卷积层的有限宽度架构的情况知之甚少。在本文中，我们为上述网络类实现的函数统计提供了严格的结果，从而更接近贝叶斯设置中特征学习的完整表征。我们的结果包括：（i）以高斯混合形式给出的输出联合先验分布的精确和基本非渐近积分表示；（ii）平方误差损失函数（高斯似然）情况下后验分布的分析公式；（iii）使用大偏差理论对特征学习无限宽度机制进行定量描述。从物理角度来看，具有多个输出或卷积层的深度架构代表了核形状重整化的不同表现形式，我们的工作提供了一本词典，将这种物理直觉和术语转化为严格的贝叶斯统计数据。]]></description>
      <guid>https://arxiv.org/abs/2406.03260</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>具有单调性约束的安全贝叶斯优化的无遗憾算法</title>
      <link>https://arxiv.org/abs/2406.03264</link>
      <description><![CDATA[arXiv:2406.03264v1 公告类型：新
摘要：我们考虑在形式为 $(s,\mathbf{x})$ 的一组动作上顺序最大化未知函数 $f$ 的问题，其中所选动作必须满足关于未知安全函数 $g$ 的安全约束。我们将 $f$ 和 $g$ 建模为位于再生核希尔伯特空间 (RKHS) 中，这有利于使用高斯过程方法。虽然针对此设置的现有工作提供了保证识别近乎最优的安全动作的算法，但实现低累积遗憾的问题仍然很大程度上未被探索，关键挑战是扩大安全区域可能会产生高遗憾。为了应对这一挑战，我们表明，如果 $g$ 仅关于单个变量 $s$ 是单调的（对 $f$ 没有这样的约束），则使用我们提出的算法可以实现亚线性遗憾。此外，我们表明，我们算法的修改版本能够实现亚线性遗憾（对于适当定义的遗憾概念），以找到与每个 $\mathbf{x}$ 相对应的近似最优 $s$，而不是仅找到全局安全最优值。我们的发现得到了对各种目标和安全函数的实证评估的支持。]]></description>
      <guid>https://arxiv.org/abs/2406.03264</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>具有重尾权重的深度神经网络的后验和变分推断</title>
      <link>https://arxiv.org/abs/2406.03369</link>
      <description><![CDATA[arXiv:2406.03369v1 公告类型：新
摘要：我们在贝叶斯框架中考虑深度神经网络，其中先验分布随机采样网络权重。根据 Agapiou 和 Castillo (2023) 最近的想法，他们表明重尾先验分布可以自动适应平滑度，我们引入了一种基于重尾权重和 ReLU 激活的简单贝叶斯深度学习先验。我们表明，相应的后验分布在包括非参数回归、几何数据和 Besov 空间在内的各种情况下实现了接近最优的极小极大收缩率，同时适应底层函数的内在维度和平滑度。虽然到目前为止大多数工作都需要在先验分布中内置一种模型选择形式，但我们方法的一个关键方面是它不需要采样超参数来学习网络的架构。我们还提供了结果的变分贝叶斯对应物，表明均值场变分近似仍然受益于近乎最优的理论支持。]]></description>
      <guid>https://arxiv.org/abs/2406.03369</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>您只需接受一次样本：快速、自校正的随机变分推理</title>
      <link>https://arxiv.org/abs/2406.02838</link>
      <description><![CDATA[arXiv:2406.02838v1 公告类型：新
摘要：我们引入了 YOASOVI，这是一种在大型贝叶斯分层模型上执行变分推理 (VI) 的快速、自校正随机优化算法。为了实现这一点，我们利用每次迭代中用于随机 VI 的目标函数的可用信息，并用验收抽样代替常规蒙特卡罗抽样。我们不会花费计算资源对大样本进行绘制和评估以获得梯度，而是只绘制一个样本并以与目标的预期改进成比例的概率接受它。以下论文开发了该算法的两个版本：第一个版本基于天真的直觉，另一个版本将算法构建为 Metropolis 类型的方案。基于多元高斯混合模型的模拟和基准数据集的经验结果表明，YOASOVI 始终比正则化蒙特卡罗和准蒙特卡罗 VI 算法收敛得更快（以时钟时间计算），并且在更好的最佳邻域内。]]></description>
      <guid>https://arxiv.org/abs/2406.02838</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>预测驱动的因果推理泛化</title>
      <link>https://arxiv.org/abs/2406.02873</link>
      <description><![CDATA[arXiv:2406.02873v1 公告类型：新
摘要：随机对照试验 (RCT) 的因果推断可能不适用于某些效应修饰因子具有不同分布的目标人群。先前的研究将试验结果推广到没有结果但有协变量数据的目标人群。我们展示了试验规模有限如何使推广成为统计上不可行的任务，因为它需要估计复杂的干扰函数。我们开发了泛化算法，使用从额外观察性研究 (OS) 中学习到的预测模型补充试验数据，而不对 OS 做出任何假设。我们从理论和经验上表明，当 OS 质量高时，我们的方法有助于更好地推广，而当 OS 质量低时，例如具有未测量的混杂因素时，我们的方法仍然很稳健。]]></description>
      <guid>https://arxiv.org/abs/2406.02873</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>协变量移位下的高维核方法：数据相关的隐式正则化</title>
      <link>https://arxiv.org/abs/2406.03171</link>
      <description><![CDATA[arXiv:2406.03171v1 公告类型：新
摘要：本文研究了协变量偏移下高维核岭回归，并分析了重要性重新加权的作用。我们首先推导出协变量偏移下高维核的渐近展开。通过偏差方差分解，我们从理论上证明了重新加权策略可以降低方差。对于偏差，我们分析了任意或精心选择的尺度的正则化，表明偏差在不同的正则化尺度下表现会非常不同。在我们的分析中，偏差和方差可以通过数据相关正则化核的谱衰减来表征：原始核矩阵与附加重新加权矩阵相关联，因此重新加权策略可以被视为数据相关的正则化，以便更好地理解。此外，我们的分析提供了协变量偏移下核函数/向量的渐近展开，这有其自身的兴趣。]]></description>
      <guid>https://arxiv.org/abs/2406.03171</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>放松分位数回归：非对称噪声的预测区间</title>
      <link>https://arxiv.org/abs/2406.03258</link>
      <description><![CDATA[arXiv:2406.03258v1 公告类型：新
摘要：构建有效的预测区间而不是点估计是回归设置中量化不确定性的一种行之有效的方法。具有这种能力的模型会输出一个值区间，其中地面真实目标将以一些预先指定的概率落入该区间。这是许多现实世界应用中的基本要求，在这些应用中，简单的点预测无法传达误差的大小和频率，因此不足以用于高风险决策。分位数回归是一种通过对输出（非参数）分布中的分位数进行经验估计来获得此类区间的主要方法。这种方法简单、计算成本低、可解释、无假设且有效。但是，它确实要求先验地选择要学习的特定分位数。这导致 (a) 区间围绕中位数任意对称，这对于实际的偏斜分布而言不是最优的，或 (b) 学习过多的区间。在本研究中，我们提出了放松分位数回归 (RQR)，这是基于分位数回归的区间构造的直接替代方案，它消除了这种任意约束，同时保留了其优势。我们证明，这种增加的灵活性可使区间具有理想的质量（例如平均宽度），同时保留分位数回归的基本覆盖保证。]]></description>
      <guid>https://arxiv.org/abs/2406.03258</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>高维统计中的可重复性</title>
      <link>https://arxiv.org/abs/2406.02628</link>
      <description><![CDATA[arXiv:2406.02628v1 公告类型：新
摘要：可复制性危机是几乎所有实证科学领域的一个主要问题，呼吁对统计学中的可复制性进行正式研究。在此背景下，[Impagliazzo、Lei、Pitassi 和 Sorrell STOC 2022] 引入了可复制学习算法的概念，并给出了包括统计查询在内的 1 维任务的基本程序。在这项工作中，我们研究了几个基本的高维统计任务的可复制性的计算和统计成本，包括多假设检验和均值估计。
我们的主要贡献建立了最佳可复制算法和高维等周平铺之间的计算和统计等价性。因此，我们获得了匹配的样本复杂度上限和下限，用于具有有界协方差的分布的可复制均值估计，解决了 [Bun、Gaboardi、Hopkins、Impagliazzo、Lei、Pitassi、Sivakumar 和 Sorrell，STOC2023] 的未解决问题，以及 $N$-Coin 问题，解决了 [Karbasi、Velegkas、Yang 和 Zhou，NeurIPS2023] 的问题，最多可达对数因子。
虽然我们的等价性是计算性的，允许我们从最知名的高效算法中削减样本复杂度的对数因子，但高效的等周平铺尚不清楚。为了解决这个问题，我们引入了几个允许样本和计算效率高的算法的宽松范式，包括允许预处理、自适应性和近似可复制性。在这些情况下，我们给出了有效的算法来匹配或超越最佳已知样本复杂度的平均估计和硬币问题，包括一个通用程序，将可复制性的标准二次开销降低到期望的线性。]]></description>
      <guid>https://arxiv.org/abs/2406.02628</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>具有非对称数据的对称核：与数据无关的可学习性界限</title>
      <link>https://arxiv.org/abs/2406.02663</link>
      <description><![CDATA[arXiv:2406.02663v1 公告类型：新
摘要：核岭回归 (KRR) 和高斯过程 (GP) 是统计学和机器学习的基本工具，最近应用于高度过度参数化的深度神经网络。这些工具学习目标函数的能力与在输入数据上采样的核的特征值直接相关。对更高特征值有支持的目标更容易学习。虽然内核通常是高度对称的对象，但数据通常不是。因此，内核对称性似乎对上述特征值或可学习性几乎没有影响，使得对真实数据的谱分析具有挑战性。在这里，我们表明，与这种常见的诱惑相反，人们可以使用与高度理想化的数据度量相关的特征值和特征函数来限制现实数据的可学习性。作为演示，我们给出了与作用于自然语言的通用转换器相关的内核的复制头的样本复杂度的理论下限。]]></description>
      <guid>https://arxiv.org/abs/2406.02663</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>线性对角网络重加权最小二乘算法的精确渐近性</title>
      <link>https://arxiv.org/abs/2406.02769</link>
      <description><![CDATA[arXiv:2406.02769v1 公告类型：新
摘要：经典的迭代加权最小二乘 (IRLS) 算法旨在通过执行一系列加权最小二乘问题从线性测量中恢复未知信号，其中权重在每个步骤中递归更新。该算法的变体已被证明可以实现良好的经验性能和稀疏恢复和 $\ell_p$ 范数最小化的理论保证。最近，IRLS 与某些类型的非凸线性神经网络架构之间也建立了一些初步联系，这些架构被观察到利用高维线性模型中的低维结构。在这项工作中，我们为一系列算法提供了统一的渐近分析，这些算法包括 IRLS、最近提出的 lin-RFM 算法（其灵感来自神经网络中的特征学习）和线性对角神经网络上的交替最小化算法。我们的分析在具有 i.i.d 的“批处理”设置中进行。高斯协变量表明，通过适当选择重新加权策略，该算法可以在少数迭代中实现良好的性能。我们还将结果扩展到组稀疏恢复的情况，并表明与坐标重新加权相比，在重新加权方案中利用此结构可以明显改善测试误差。]]></description>
      <guid>https://arxiv.org/abs/2406.02769</guid>
      <pubDate>Fri, 07 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    </channel>
</rss>