<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>消除深度生成模型生成的合成数据的偏见</title>
      <link>https://arxiv.org/abs/2411.04216</link>
      <description><![CDATA[arXiv:2411.04216v1 公告类型：新
摘要：虽然合成数据在隐私保护方面前景广阔，但其统计分析也带来了重大挑战，需要创新的解决方案。众所周知，使用深度生成模型 (DGM) 生成合成数据会给合成数据分析带来相当大的偏差和不精确性，从而损害其与原始数据分析相比的推理效用。这种偏差和不确定性可能足以阻碍统计收敛速度，即使在平均值计算等看似简单的分析中也是如此。然后，此类估计量的标准误差随样本大小的收缩速度比典型的 1/根 $n$ 速率更慢。这使 p 值和置信区间等基本计算变得复杂，目前没有直接的补救措施。为了应对这些挑战，我们提出了一种新策略，该策略针对 DGM 为特定数据分析创建的合成数据。我们的方法借鉴了去偏和有针对性的机器学习的见解，可以考虑偏差、提高收敛速度，并有助于计算具有容易近似的大样本方差的估计量。我们通过对玩具数据的模拟研究和对现实世界数据的两个案例研究来举例说明我们的建议，强调了为有针对性的数据分析量身定制 DGM 的重要性。这种去偏策略有助于提高统计推断中合成数据的可靠性和适用性。]]></description>
      <guid>https://arxiv.org/abs/2411.04216</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ION-C：通过约束整合重叠网络</title>
      <link>https://arxiv.org/abs/2411.04243</link>
      <description><![CDATA[arXiv:2411.04243v1 公告类型：新
摘要：在许多因果学习问题中，感兴趣的变量通常并非全部在相同的观测值上测量，而是分布在具有重叠变量的多个数据集中。Tillman 等人（2008 年）提出了第一种利用局部独立关系枚举与所有输入图一致的地面实况 DAG 的最小等价类的算法，称为 ION。在本文中，这个问题被表述为一个计算效率更高的答案集编程 (ASP) 问题，我们称之为 ION-C，并使用 ASP 系统 clingo 解决。ION-C 算法在具有不同大小、密度和子图之间重叠程度的随机合成图上运行，其中重叠对运行时间、解决方案图数量和输出集内的一致性影响最大。为了在真实数据上验证 ION-C，我们在从欧洲社会调查 (ESS) 两次连续迭代的数据中学习到的重叠图上运行了该算法，并使用了联合独立性测试的程序来防止输入不一致。]]></description>
      <guid>https://arxiv.org/abs/2411.04243</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络和非通勤算子</title>
      <link>https://arxiv.org/abs/2411.04265</link>
      <description><![CDATA[arXiv:2411.04265v1 公告类型：新
摘要：图神经网络 (GNN) 在各种任务中提供最先进的结果，这些任务通常涉及预测图顶点的特征。它们由图卷积层构建，这些卷积层可作为描述顶点之间信息流的强大归纳偏差。通常，有多种数据模态可用。这项工作考虑了一种设置，其中多个图具有相同的顶点集和一个共同的顶点级学习任务。这将标准 GNN 模型推广到具有多个不交换的图运算符的 GNN。我们可以将这种模型称为图元组神经网络 (GtNN)。
在这项工作中，我们开发了数学理论，以使用非交换非扩展运算符的属性来解决 GtNN 的稳定性和可迁移性。我们开发了图元组神经网络的极限理论，并用它来证明一个通用的可转移性定理，该定理保证所有图元组神经网络在收敛的图元组序列上都是可转移的。特别是，在我们这里考虑的收敛下，没有不可转移的能量。我们的理论结果将众所周知的 GNN 可转移性定理扩展到多个同时图 (GtNN) 的情况，并对目前已知的 GNN 情况进行了严格的改进。
我们通过对合成数据和真实数据进行的简单实验来说明我们的理论结果。为此，我们推导出一个训练程序，可以证明增强结果模型的稳定性。]]></description>
      <guid>https://arxiv.org/abs/2411.04265</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>贪婪递归分割估计器的统计计算权衡</title>
      <link>https://arxiv.org/abs/2411.04394</link>
      <description><![CDATA[arXiv:2411.04394v1 公告类型：新
摘要：基于递归分区的模型（例如决策树及其集合）在高维回归中很受欢迎，因为它们可以潜在地避免维数灾难。由于经验风险最小化 (ERM) 在计算上不可行，因此这些模型通常使用贪婪算法进行训练。尽管这些算法在许多情况下都是有效的，但经验上观察到它们会陷入局部最优。我们在学习 $d$ 个二元特征上的稀疏回归函数的背景下探索了这种现象，结果表明，当真正的回归函数 $f^*$ 不满足所谓的合并阶梯属性 (MSP) 时，贪婪训练需要 $\exp(\Omega(d))$ 才能实现较低的估计误差。相反，当 $f^*$ 满足 MSP 时，贪婪训练仅使用 $O(\log d)$ 个样本就可以实现较小的估计误差。这一性能与在平均场范围内用随机梯度下降 (SGD) 训练的两层神经网络的性能相似，从而建立了 SGD 训练的神经网络和贪婪递归分区估计器之间的直接比较。此外，无论 $f^*$ 是否满足 MSP，ERM 训练的递归分区估计器都可以使用 $O(\log d)$ 个样本实现较低的估计误差，从而证明了贪婪训练的统计计算权衡。我们的证明基于对贪婪递归分区的新颖解释，使用随机过程理论和可能具有独立意义的耦合技术。]]></description>
      <guid>https://arxiv.org/abs/2411.04394</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于模糊真实值分类的共形信条区域</title>
      <link>https://arxiv.org/abs/2411.04852</link>
      <description><![CDATA[arXiv:2411.04852v1 公告类型：新
摘要：\emph{不精确概率机器学习} 中的一个未解决的问题是如何在没有任何先验知识或假设的情况下，从可用数据中凭经验得出一个信条区域（即输出空间上的封闭凸概率族）。在分类问题中，信条区域是一种工具，它能够通过表征标签分布的不确定性，在现实假设下提供可证明的保证。在之前工作的基础上，我们表明可以使用保形方法直接构建信条区域。这使我们能够为经典保形预测提供一种新颖的扩展，以解决具有模糊基本事实的问题，即当给定输入的确切标签不完全已知时。由此产生的构造具有理想的实际和理论特性：（i）共形覆盖保证，（ii）较小的预测集（与经典共形预测区域相比）和（iii）不确定性源的解缠（认知、随机）。我们在合成数据集和真实数据集上通过实证验证了我们的发现。]]></description>
      <guid>https://arxiv.org/abs/2411.04852</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用后验抽样进行帕累托集识别</title>
      <link>https://arxiv.org/abs/2411.04939</link>
      <description><![CDATA[arXiv:2411.04939v1 公告类型：新
摘要：在具有实值分布的项目集合中识别最佳答案的问题已广为人知。
尽管它对许多应用具有实际意义，但当有多个可能相互冲突的指标可用于评估项目质量时，很少有人研究它的扩展。
帕累托集识别 (PSI) 旨在识别平均值不均匀地比另一个更差的答案集。
本文研究了具有潜在相关目标的传导线性设置中的 PSI。
基于停止和采样规则中的后验采样，我们提出了 PSIPS 算法，该算法同时处理结构和相关性，而无需支付现有基于 oracle 的算法的计算成本。
从频率论和贝叶斯的角度来看，PSIPS 都是渐近最优的。
我们在现实世界和合成实例中展示了其良好的经验性能。]]></description>
      <guid>https://arxiv.org/abs/2411.04939</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>递归粘性分层狄利克雷过程隐马尔可夫模型</title>
      <link>https://arxiv.org/abs/2411.04278</link>
      <description><![CDATA[arXiv:2411.04278v1 公告类型：交叉 
摘要：分层狄利克雷过程隐马尔可夫模型 (HDP-HMM) 是经典隐马尔可夫模型的自然贝叶斯非参数扩展，用于从（空间）时间数据中学习。提出了一种粘性 HDP-HMM 来增强 HDP-HMM 中的自我持久性概率。然后，提出了解缠结粘性 HDP-HMM 来解缠自我持久性先验和转换先验的强度。然而，粘性 HDP-HMM 假设自我持久性概率是固定的，限制了它的表现力。在这里，我们以粘性 HDP-HMM 和解缠结粘性 HDP-HMM 的先前工作为基础，开发了一个更通用的模型：循环粘性 HDP-HMM (RS-HDP-HMM)。我们开发了一种新颖的吉布斯采样策略，以便在该模型中进行有效推理。我们表明，RS-HDP-HMM 在合成和真实数据分割中均优于解缠粘性 HDP-HMM、粘性 HDP-HMM 和 HDP-HMM。]]></description>
      <guid>https://arxiv.org/abs/2411.04278</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>循环显式持续时间切换线性动力系统中的贝叶斯推理</title>
      <link>https://arxiv.org/abs/2411.04280</link>
      <description><![CDATA[arXiv:2411.04280v1 公告类型：交叉 
摘要：在本文中，我们提出了一种名为循环显式持续时间切换线性动态系统 (REDSLDS) 的新模型，该模型将循环显式持续时间变量合并到 rSLDS 模型中。我们还提出了一种涉及使用 P\&#39;olya-gamma 增强的推理和学习方案。我们在三个基准数据集上展示了我们模型改进的分割能力，包括两个定量数据集和一个定性数据集。]]></description>
      <guid>https://arxiv.org/abs/2411.04280</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成合成电子健康记录 (EHR) 数据：基准审查</title>
      <link>https://arxiv.org/abs/2411.04281</link>
      <description><![CDATA[arXiv:2411.04281v1 公告类型：交叉 
摘要：我们对现有的合成 EHR 数据生成方法进行了范围界定审查，并使用拟议的开源软件对主要方法进行了基准测试，以向从业者提供建议。我们在三个学术数据库中搜索了范围界定审查。方法以开源 EHR 数据集 MIMIC-III/IV 为基准。实施并比较了七种涵盖主要类别的现有方法和两种基线方法。评估指标涉及数据保真度、下游效用、隐私保护和计算成本。确定了 42 项研究并将其分为五类。选择了涵盖所有类别的七种开源方法，在 MIMIC-III 上进行训练，并在 MIMIC-III 或 MIMIC-IV 上进行评估以考虑可移植性。其中，基于 GAN 的方法在 MIMIC-III 上的保真度和效用方面表现出色；基于规则的方法在隐私保护方面表现出色。在 MIMIC-IV 上也观察到了类似的发现，只是基于 GAN 的方法在保持保真度方面的表现进一步优于基线方法。我们提供了一个 Python 包“SynthEHRella”，用于整合各种方法选择和评估指标，从而能够更简化地探索和评估多种方法。我们发现方法选择取决于下游用例中评估指标的相对重要性。我们提供了一个决策树来指导在基准方法中进行选择。基于决策树，当训练和测试人群之间存在分布偏差时，基于 GAN 的方法会表现出色。否则，CorGAN 和 MedGAN 分别最适合关联建模和预测建模。未来的研究应优先考虑在控制隐私暴露的同时提高合成数据的保真度，以及对纵向或条件生成方法进行全面的基准测试。]]></description>
      <guid>https://arxiv.org/abs/2411.04281</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型是隐藏的推理者：通过自我奖励释放潜在推理能力</title>
      <link>https://arxiv.org/abs/2411.04282</link>
      <description><![CDATA[arXiv:2411.04282v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 已经展现出令人印象深刻的能力，但在处理需要多个步骤的复杂推理任务时仍然举步维艰。虽然基于提示的方法（如思维链 (CoT)）可以在推理时改进 LLM 推理，但在训练期间优化推理能力仍然具有挑战性。我们引入了潜在推理优化 (LaTRO)，这是一个原则性框架，它将推理公式化为从潜在分布中采样并通过变分方法对其进行优化。LaTRO 使 LLM 能够同时改进其推理过程和评估推理质量的能力，而无需外部反馈或奖励模型。我们通过使用多种模型架构在 GSM8K 和 ARC-Challenge 数据集上进行实验来验证 LaTRO。在 GSM8K 上，LaTRO 在 Phi-3.5-mini、Mistral-7B 和 Llama-3.1-8B 上将零样本准确率平均提高了 12.5%（比基础模型高），在监督微调上提高了 9.6%。我们的研究结果表明，预训练的 LLM 具有潜在推理能力，可以通过我们提出的优化方法以自我改进的方式解锁和增强。LaTRO 的代码可在 \url{https://github.com/SalesforceAIResearch/LaTRO} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2411.04282</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 IVON 进行变分低秩自适应</title>
      <link>https://arxiv.org/abs/2411.04421</link>
      <description><![CDATA[arXiv:2411.04421v1 公告类型：交叉 
摘要：我们表明变分学习可以显著提高低秩自适应 (LoRA) 的准确性和校准，而不会大幅增加成本。我们用改进的变分在线牛顿 (IVON) 算法代替 AdamW 来微调大型语言模型。对于具有 70 亿个参数的 Llama-2，IVON 的准确率比 AdamW 提高了 2.8%，预期校准误差提高了 4.6%。准确率也优于其他贝叶斯替代方案，但成本更低，实施更容易。我们的工作为 IVON 对大型语言模型的有效性提供了额外的证据。代码可在 https://github.com/team-approx-bayes/ivon-lora 获得。]]></description>
      <guid>https://arxiv.org/abs/2411.04421</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过针对多样性实现开放式模拟器中的自适应代理训练</title>
      <link>https://arxiv.org/abs/2411.04466</link>
      <description><![CDATA[arXiv:2411.04466v1 公告类型：交叉 
摘要：端到端学习方法在具体决策领域的更广泛应用仍然受到其对代表目标领域的大量训练数据的依赖的瓶颈。元强化学习 (meta-RL) 方法放弃了零样本泛化的目标（标准强化学习 (RL) 的目标），转而采用少量样本自适应，因此有望弥合更大的泛化差距。虽然学习这种元级自适应行为仍然需要大量数据，但接近现实世界复杂性的高效环境模拟器正在日益普及。即便如此，为这些复杂领域手工设计足够多样化和大量的模拟训练任务仍然需要大量劳动力。域随机化 (DR) 和程序生成 (PG) 是解决此问题的解决方案，它们要求模拟器拥有精心定义的参数，这些参数直接转化为有意义的任务多样性——这是一个同样令人望而却步的假设。在这项工作中，我们提出了 DIVA，这是一种在这种复杂的开放式模拟器中生成各种训练任务的进化方法。与无监督环境设计 (UED) 方法一样，DIVA 可以应用于任意参数化，但还可以结合实际可用的领域知识 - 从而继承 UED 的灵活性和通用性，以及 DR 和 PG 利用的精心设计的模拟器中嵌入的监督结构。我们的实证结果展示了 DIVA 克服复杂参数化和成功训练自适应代理行为的独特能力，远远超过先前文献中的竞争基线。这些发现突出了这种半监督环境设计 (SSED) 方法的潜力，其中 DIVA 是第一个不起眼的组成部分，它能够在现实的模拟领域中进行训练，并产生更强大、更强大的自适应代理。]]></description>
      <guid>https://arxiv.org/abs/2411.04466</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Transformers 进行测量间插值</title>
      <link>https://arxiv.org/abs/2411.04551</link>
      <description><![CDATA[arXiv:2411.04551v1 公告类型：交叉 
摘要：Transformers 是深度神经网络架构，是大型语言模型近期成功的基础。与可以视为点对点映射的更经典的架构不同，Transformer 充当度量到度量的映射，作为单位球面上的特定交互粒子系统实现：输入是提示中标记的经验度量，其演化由连续性方程控制。事实上，Transformers 不仅限于经验度量，原则上可以处理任何输入度量。由于 Transformers 处理的数据性质正在迅速扩展，因此研究它们作为从任意度量到另一个任意度量的映射的表达能力非常重要。为此，我们提供了一个明确的参数选择，允许单个 Transformer 将 $N$ 个任意输入度量与 $N$ 个任意目标度量相匹配，最低限度假设每对输入目标度量都可以通过某个传输图匹配。]]></description>
      <guid>https://arxiv.org/abs/2411.04551</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KL 正则化上下文老虎机和 RLHF 的精确分析</title>
      <link>https://arxiv.org/abs/2411.04625</link>
      <description><![CDATA[arXiv:2411.04625v1 公告类型：交叉 
摘要：逆 Kullback-Leibler (KL) 正则化已成为增强强化学习 (RL) 和人类反馈强化学习 (RLHF) 中的策略优化的主要技术，它迫使学习到的策略保持接近参考策略。虽然 KL 正则化的有效性和必要性已在各种实际场景中得到经验证明，但目前对 KL 正则化 RLHF 的理论分析仍然获得与没有 KL 正则化的问题相同的 $\mathcal{O}(1 / \epsilon^2)$ 样本复杂度。为了理解具有 KL 正则化的策略学习目标和不具有 KL 正则化的策略学习目标之间的根本区别，我们首次从理论上展示了 KL 正则化的强大功能，对 KL 正则化的上下文老虎机和 RLHF 进行了清晰的分析，揭示了当 $\epsilon$ 足够小时，样本复杂度为 $\mathcal{O}(1 / \epsilon)$。
我们进一步探讨了数据覆盖在上下文老虎机和 RLHF 中的作用。虽然覆盖假设通常用于离线 RLHF，以将参考策略中的样本链接到最优策略，通常以对覆盖系数的乘法依赖为代价，但它对在线 RLHF 样本复杂度的影响仍不清楚。先前对在线 RLHF 的理论分析通常需要明确的探索和对奖励函数类的额外结构假设。相比之下，我们表明，在参考策略具有足够的覆盖率的情况下，简单的两阶段混合采样策略可以实现仅对覆盖系数具有加法依赖性的样本复杂度。我们的结果全面介绍了 KL 正则化和数据覆盖在 RLHF 中的作用，为设计更高效的 RLHF 算法提供了启示。]]></description>
      <guid>https://arxiv.org/abs/2411.04625</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络的中心图移位算子</title>
      <link>https://arxiv.org/abs/2411.04655</link>
      <description><![CDATA[arXiv:2411.04655v1 公告类型：交叉 
摘要：图移位算子 (GSO)，例如邻接矩阵和图拉普拉斯矩阵，在图论和图表示学习中起着基础性作用。传统的 GSO 通常通过用度矩阵（局部中心性度量）对邻接矩阵进行规范化来构建。在这项工作中，我们提出并研究了中心性 GSO (CGSO)，它通过全局中心性指标（例如 PageRank、$k$ 核心或固定长度步行计数）对邻接矩阵进行规范化。我们研究 CGSO 的谱特性，从而了解它们对图信号的作用。我们通过在几个合成和真实世界的数据集上定义和运行基于不同 CGSO 的谱聚类算法来确认这一理解。此外，我们概述了我们的 CGSO 如何充当任何图神经网络中的消息传递运算符，特别是在几个真实基准数据集上使用我们的 CGSO 展示了图卷积网络和图注意力网络变体的强劲性能。]]></description>
      <guid>https://arxiv.org/abs/2411.04655</guid>
      <pubDate>Fri, 08 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>