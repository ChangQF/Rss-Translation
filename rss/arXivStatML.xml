<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>对未标记数据进行具有自洽损失的稳健摊销贝叶斯推理</title>
      <link>https://arxiv.org/abs/2501.13483</link>
      <description><![CDATA[arXiv:2501.13483v1 公告类型：新
摘要：神经摊销贝叶斯推理 (ABI) 可以比传统方法快几个数量级地解决概率逆问题。然而，神经 ABI 还不够强大，无法广泛且安全地应用。特别是，当对训练期间看到的模拟数据范围之外的观测值进行推理时，例如由于模型错误指定，后验近似可能会变得高度偏差。由于当前神经后验估计器在模拟之外的状态下具有不良的预渐近行为，因此无法通过仅模拟更多训练数据在可接受的时间内修复由此产生的估计偏差。在这篇概念验证论文中，我们提出了一种半监督方法，不仅可以对模型生成的（标记）模拟数据进行训练，还可以对来自任何来源（包括真实世界数据）的未标记数据进行训练。为了实现后者，我们利用贝叶斯自洽性，该特性可以转化为严格适当的损失，而无需了解真实参数值，即无需数据标签。我们初步实验的结果显示，ABI 对模拟外数据的稳健性有了显著改善。即使观察到的数据远离标记和未标记的训练数据，推理仍然非常准确。如果我们的发现也能推广到其他场景和模型类别，我们相信我们的新方法代表了神经 ABI 的重大突破。]]></description>
      <guid>https://arxiv.org/abs/2501.13483</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LITE：有效估计最大值的高斯概率</title>
      <link>https://arxiv.org/abs/2501.13535</link>
      <description><![CDATA[arXiv:2501.13535v1 公告类型：新
摘要：我们考虑计算高斯随机向量的最大值概率 (PoM) 的问题，即每个维度最大的概率。这是从贝叶斯优化到强化学习等应用中的关键挑战，其中 PoM 不仅有助于找到最佳动作，还可以对动作域进行细粒度分析，这对于药物发现等任务至关重要。现有技术成本高昂，计算和内存随向量大小呈多项式扩展。我们介绍了 LITE，这是第一种以近乎线性的时间和内存复杂度估计高斯 PoM 的方法。LITE 在许多任务上实现了 SOTA 精度，同时在实践中比基线快几个数量级。这也意味着在熵估计和强盗的最优控制等下游任务上具有更好的性能。理论上，我们将 LITE 视为熵正则化的 UCB，并将其与先前的 PoM 估计量连接起来。]]></description>
      <guid>https://arxiv.org/abs/2501.13535</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在作为和不作为事件异常值下的学习</title>
      <link>https://arxiv.org/abs/2501.13599</link>
      <description><![CDATA[arXiv:2501.13599v1 公告类型：新
摘要：事件流是现实生活中的重要数据格式。通常预期事件会随着时间的推移遵循一些规律。但是，这些模式可能会受到事件的意外缺失或发生的影响。在本文中，我们采用时间点过程框架来学习事件流，并提供一种简单但有效的方法来处理事件的失职和遗漏。特别是，我们引入了一种新颖的权重函数来动态调整每个观察事件的重要性，以便最终估计量可以提供多种统计优点。我们将所提出的方法与分类问题中的原始方法进行了比较，其中事件流可以聚类为不同的组。理论和数值结果都证实了我们新方法的有效性。据我们所知，我们的方法是第一个可以同时处理失职和遗漏异常值的方法。]]></description>
      <guid>https://arxiv.org/abs/2501.13599</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 Gromov-Wasserstein 距离的降维技术</title>
      <link>https://arxiv.org/abs/2501.13732</link>
      <description><![CDATA[arXiv:2501.13732v1 公告类型：新
摘要：分析对象之间的关系是数据科学中的关键问题。在此背景下，降维（DR）技术被用于生成更小、更易于管理的数据表示。本文提出了一种基于最优传输理论和 Gromov-Wasserstein 距离的降维新方法。我们提供了经典多维缩放（MDS）算法和非线性降维算法 Isomap（等距映射或等距特征映射）的新概率视图，该算法扩展了经典 MDS，其中我们使用高维数据的概率度量与其低维表示之间的 Gromov-Wasserstein 距离。通过梯度下降，我们的方法将高维数据嵌入到低维空间中，为分析复杂的高维数据集提供了一种稳健而有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2501.13732</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>线性逆问题中的贝叶斯模型参数学习及其在脑电图焦点源成像中的应用</title>
      <link>https://arxiv.org/abs/2501.13109</link>
      <description><![CDATA[arXiv:2501.13109v1 公告类型：交叉 
摘要：逆问题可以描述为有限数据问题，其中无法直接观察到感兴趣的信号。通常需要基于物理的正向模型将信号与观测值联系起来。不幸的是，未知的模型参数和不完善的正向模型会破坏信号恢复。尽管监督机器学习提供了有希望的方法来提高解决方案的鲁棒性，但当无法获得训练的基本事实时，我们必须依赖基于模型的学习。在这里，我们研究了一个包含未知非线性模型参数的线性逆问题，并利用了基于贝叶斯模型的学习方法，该方法允许信号恢复并随后估计模型参数。这种方法称为贝叶斯近似误差方法，它采用了问题物理的简化模型，并增加了一个近似误差项来补偿简化。借助近似误差协方差矩阵的特征向量，我们构建了一个误差子空间，这样除了原始信号外，还可以同时估计诱发误差。然后使用估计的误差和信号来确定未知的模型参数。对于模型参数估计，我们测试了不同的方法：条件高斯回归、迭代（基于模型）优化和借助物理信息学习建模的高斯过程。此外，还使用交替优化作为参考方法。作为一个示例应用，我们专注于在模型中不知道患者头骨电导率的情况下从 EEG 记录重建大脑活动的问题。我们的结果显示 EEG 源定位精度明显提高，并为未知的模型参数颅骨电导率提供了可行的估计值。]]></description>
      <guid>https://arxiv.org/abs/2501.13109</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索重要性抽样中的方差减少以实现高效的 DNN 训练</title>
      <link>https://arxiv.org/abs/2501.13296</link>
      <description><![CDATA[arXiv:2501.13296v1 公告类型：交叉 
摘要：重要性抽样被广泛用于通过减少梯度估计量的方差来提高深度神经网络 (DNN) 训练的效率。然而，由于计算开销，有效评估相对于均匀采样的方差减少仍然具有挑战性。本文提出了一种仅使用在重要性抽样下采样的小批量来估计 DNN 训练期间方差减少的方法。通过利用所提出的方法，本文还提出了一种有效的小批量大小以实现自动学习率调整。还介绍了一种量化重要性抽样效率的绝对指标以及一种基于移动梯度统计的重要性分数实时估计算法。理论分析和基准数据集上的实验表明，与当前的重要性抽样方法相比，所提出的算法在保持最小计算开销的同时，持续降低了方差、提高了训练效率并提高了模型准确性。]]></description>
      <guid>https://arxiv.org/abs/2501.13296</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>共同学习贝叶斯优化</title>
      <link>https://arxiv.org/abs/2501.13332</link>
      <description><![CDATA[arXiv:2501.13332v1 公告类型：交叉 
摘要：众所周知，贝叶斯优化（BO）在解决黑箱问题方面具有样本效率。然而，即使有大量样本，BO 算法有时也会陷入次优解。本质上，BO 的这种次优问题可以归因于训练的高斯过程（GP）的替代精度较差，特别是在最优解所在的区域。因此，我们建议建立多个 GP 模型而不是单个 GP 替代模型来相互补充，从而解决 BO 的次优问题。然而，根据偏差-方差权衡方程，当增加模型的多样性时，单个预测误差可能会增加，这可能导致整体替代精度更差。另一方面，基于 Rademacher 复杂度理论，已证明利用模型在未标记信息上的一致性有助于降低假设空间的复杂度，从而以更少的样本达到所需的替代准确率。模型一致性的这种价值已在协同训练风格的算法中得到广泛证明，以利用少量样本提高模型准确率。受此启发，我们提出了一种新的 BO 算法，称为协同学习 BO（CLBO），它利用模型多样性和未标记信息的一致性来提高有限样本下的整体替代准确率，从而实现更有效的全局优化。通过对五个数值玩具问题和三个工程基准的测试，很好地证明了所提出的 CLBO 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.13332</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习在非平稳重复第一价格拍卖中竞标</title>
      <link>https://arxiv.org/abs/2501.13358</link>
      <description><![CDATA[arXiv:2501.13358v1 公告类型：交叉 
摘要：一价拍卖最近在数字广告市场中获得了显著的关注，例如谷歌从二价拍卖向一价拍卖的转变。与二价拍卖不同，在二价拍卖中，出价自己的私人估值是一种主导策略，而确定一价拍卖中的最佳出价策略则更加复杂。从学习的角度来看，学习者（特定竞标者）可以按顺序与环境（其他竞标者）互动以推断他们的行为。现有研究通常假设特定的环境条件，并根据最佳固定策略（静态基准）对性能进行基准测试。虽然这种方法可以确保强大的学习保证，但在即使是轻微非平稳性的环境中，静态基准也会与最佳策略有显著偏差。为了解决这种情况，动态基准（表示每个时间步骤中最佳可能奖励的总和）提供了更合适的目标。然而，要实现对动态基准的无悔学习需要额外的约束。通过检查在线首价拍卖中的奖励函数，我们引入了两个指标来量化竞价序列的规律性，这两个指标可作为非平稳性的度量。当这两个指标中的任何一个在时间范围内为次线性时，我们提供了动态遗憾的极小极大最优表征。]]></description>
      <guid>https://arxiv.org/abs/2501.13358</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一般分布偏移下的 Wasserstein 正则化共形预测</title>
      <link>https://arxiv.org/abs/2501.13430</link>
      <description><![CDATA[arXiv:2501.13430v1 公告类型：交叉 
摘要：在独立同分布假设下，共形预测产生的预测集保证对真实目标的$1-\alpha$覆盖，但该假设可能不成立，导致$1-\alpha$与实际覆盖率之间存在差距。先前的研究使用总变分距离来限制差距，但无法识别给定$\alpha$时分布偏移下的差距变化。此外，现有方法大多局限于协变量偏移，而一般联合分布偏移在实践中更常见，但研究较少。为此，我们首先提出了基于Wasserstein距离的覆盖差距上限，并使用偏移联合数据和共形分数分布之间的概率测度前推来分析该上限，从而能够分离协变量和概念偏移对覆盖差距的影响。我们利用这种分离设计了一种基于重要性加权和正则化表示学习 (WR-CP) 的算法，以有限样本误差界限降低 Wasserstein 界限。WR-CP 在共形预测精度和效率之间实现了可控的平衡。在六个数据集上的实验证明，WR-CP 可以将不同置信水平的覆盖差距缩小到 $3.1\%$，并且输出的预测集平均比最坏情况方法小 38$\%$。]]></description>
      <guid>https://arxiv.org/abs/2501.13430</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有固定置信度的最优多目标最佳臂识别</title>
      <link>https://arxiv.org/abs/2501.13607</link>
      <description><![CDATA[arXiv:2501.13607v1 公告类型：交叉 
摘要：我们考虑一个具有有限多条臂的多臂老虎机设置，其中每条臂在选择时都会产生 $M$ 维向量奖励。我们假设每个维度（又称 {\em objective}）的奖励是独立于其他维度生成的。任何给定目标的最佳臂是具有与目标相对应的最大均值分量的臂。最终目标是在最短（预期）时间内确定 {\em every} 目标的最佳臂，但要受到错误概率上限（即固定置信度制度）的限制。我们在消失错误概率的极限下，建立了一个与问题相关的预期停止时间的极限增长率下限。我们表明，这个下限的特点是最大最小优化问题，在每个时间步骤中求解该问题都需要大量计算。我们提出了一种算法，该算法使用新颖的替代比例思想在每个时间步骤对臂进行采样，从而无需在每个步骤中解决最大最小优化问题。我们从理论上证明了我们的算法是渐近最优的。此外，我们还提供了广泛的实证研究来证实我们算法的效率。虽然现有的关于多目标多臂老虎机纯探索的研究主要集中在帕累托边界识别上，但我们的工作通过对多目标最佳臂识别问题进行正式调查填补了文献中的空白。]]></description>
      <guid>https://arxiv.org/abs/2501.13607</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>安全贝叶斯优化中的安全性及其对控制的影响</title>
      <link>https://arxiv.org/abs/2501.13697</link>
      <description><![CDATA[arXiv:2501.13697v1 公告类型：交叉 
摘要：控制工程中一个反复出现的重要任务是在约束条件下进行参数调整，从概念上讲，这相当于对只能通过噪声评估才能访问的黑盒函数进行优化。例如，在控制实践中，预先设计的控制器的参数通常通过反馈与工厂在线调整，并且只应尝试安全的参数值，以避免例如不稳定。最近，机器学习方法已被用于解决这个重要问题，特别是贝叶斯优化 (BO)。为了处理安全约束，已经使用了安全 BO 中的算法，尤其是 SafeOpt 类型的算法，这种算法在基于学习的控制、机器人技术和相邻领域中非常受欢迎。然而，我们发现了两个影响实际安全性的重大障碍。首先，SafeOpt 类型的算法依赖于定量不确定性界限，大多数实现都用理论上不受支持的启发式方法取代这些界限。其次，理论上有效的不确定性界限关键取决于一个数量——目标函数的再生核希尔伯特空间范数——目前无法使用既定的先验工程知识可靠地约束该数量。通过仔细的数值实验，我们表明这些问题确实会导致安全违规。为了克服这些问题，我们提出了仅 Lipschitz 安全贝叶斯优化 (LoSBO)，这是一种安全的 BO 算法，其安全性仅依赖于已知的 Lipschitz 界限。此外，我们提出了一种变体 (LoS-GP-UCB)，它避免了搜索空间的网格化，因此即使对于中等高维问题也适用。]]></description>
      <guid>https://arxiv.org/abs/2501.13697</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有状态空间表示的相互依赖客户端的联邦格兰杰因果关系学习</title>
      <link>https://arxiv.org/abs/2501.13890</link>
      <description><![CDATA[arXiv:2501.13890v1 公告类型：交叉 
摘要：先进的传感器和物联网设备改善了复杂工业企业的监控和控制。他们还在这些企业中创建了一个地理分布的流程操作（客户端）相互依赖的结构。格兰杰因果关系是一种有效的方法，它通过检查一个客户端的状态如何随时间影响其他客户端来检测和量化相互依赖关系。了解这些相互依赖关系可以捕捉到局部事件（例如故障和中断）如何在整个系统中传播，从而可能造成广泛的运营影响。然而，工业数据的庞大数量和复杂性给这些相互依赖关系的建模带来了挑战。本文开发了一种学习格兰杰因果关系的联合方法。我们利用线性状态空间系统框架，利用低维状态估计来分析相互依赖关系。这解决了通常与集中式数据处理相关的带宽限制和计算负担。我们建议使用服务器通过机器学习 (ML) 功能学习的格兰杰因果关系信息来增强客户端模型。我们研究了增强客户端和服务器模型之间的相互依赖性，并将框架重新表述为独立的 ML 算法，为其次线性和线性收敛速度提供条件。我们还研究了框架与集中式预言机模型的收敛性。此外，我们还包括差分隐私分析，以确保数据安全，同时保留因果洞察力。使用合成数据，我们进行了全面的实验，以证明我们的方法对因果关系扰动的稳健性、对通信规模、客户端数量和原始数据维度的可扩展性。我们还通过报告分散化节省的数据量来评估两个真实工业控制系统数据集的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.13890</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不确定：使用未知噪声水平和 Stein 无偏风险评估进行自我监督学习</title>
      <link>https://arxiv.org/abs/2409.01985</link>
      <description><![CDATA[arXiv:2409.01985v3 公告类型：替换 
摘要：最近，已经提出了许多用于图像重建的自监督学习方法，这些方法可以仅从噪声数据中学习，而无需使用真实数据参考。大多数现有方法集中在两类：i）Stein 的无偏风险估计 (SURE) 和类似方法，这些方法假设完全了解分布，ii）Noise2Self 和类似的交叉验证方法，这些方法只需要对噪声分布有非常温和的了解。第一类方法往往不切实际，因为噪声水平在实际应用中通常是未知的，而第二类方法与监督学习相比通常是次优的。在本文中，我们提供了一个描述这种表达力-鲁棒性权衡的理论框架，并提出了一种基于 SURE 的新方法，但与标准 SURE 不同，它不需要了解噪声水平。通过一系列实验，我们表明，所提出的估计器在各种成像逆问题上优于其他现有的自监督方法。]]></description>
      <guid>https://arxiv.org/abs/2409.01985</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多视图表示学习的通用且鲁棒的谱方法</title>
      <link>https://arxiv.org/abs/2411.02138</link>
      <description><![CDATA[arXiv:2411.02138v2 公告类型：替换 
摘要：近年来，多视图表示学习 (MvRL) 引起了广泛关注，这得益于对能够有效处理和分析来自多个来源的数据的应用程序的需求不断增长。在这种情况下，基于图拉普拉斯的 MvRL 方法在表示多视图数据方面表现出了显著的成功。然而，这些方法通常难以推广到新数据，并且面临可扩展性的挑战。此外，在许多实际场景中，多视图数据受到噪声或异常值的污染。在这种情况下，依赖于对齐或对比目标的现代基于深度学习的 MvRL 方法在下游任务中会降低性能，因为它们可能会在清晰和损坏的数据源之间施加不正确的一致性。我们引入了 $\textit{SpecRaGE}$，这是一种基于融合的新型框架，它将图拉普拉斯方法的优势与深度学习的力量相结合，以克服这些挑战。 SpecRage 使用神经网络来学习参数映射，该映射近似于图拉普拉斯算子的联合对角化。此解决方案绕过了对齐的需要，同时实现了对信息丰富且有意义的表示的通用和可扩展学习。此外，它还包含一个元学习融合模块，该模块可动态适应数据质量，确保对异常值和噪声视图的鲁棒性。我们进行了广泛的实验，结果表明 SpecRaGE 的表现优于最先进的方法，尤其是在数据污染的情况下，为更可靠、更高效的多视图学习铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2411.02138</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于混合模型、最大似然和熵最优传输之间关系的注记</title>
      <link>https://arxiv.org/abs/2501.12005</link>
      <description><![CDATA[arXiv:2501.12005v2 公告类型：替换 
摘要：本说明旨在证明对混合模型进行最大似然估计相当于最小化具有熵正则化的最佳传输问题的参数。目标是教学：我们试图以简洁且希望简单的方式呈现这个已知的结果。我们通过展示标准 EM 算法是最佳传输损失的特定块坐标下降来用高斯混合模型进行说明。]]></description>
      <guid>https://arxiv.org/abs/2501.12005</guid>
      <pubDate>Fri, 24 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>