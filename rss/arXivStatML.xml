<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>stat.ml arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>arxiv.org e-print存档上的stat.ml更新。</description>
    <lastBuildDate>Wed, 19 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>次优的沙普利价值解释</title>
      <link>https://arxiv.org/abs/2502.12209</link>
      <description><![CDATA[ARXIV：2502.12209V1公告类型：新 
摘要：深度神经网络（DNN）在支持各种应用方面表现出强大的能力。 Shapley价值已成为一种重要的工具，可以分析特征重要性，以帮助人们了解深神经模型的推理过程。计算Shapley值函数需要选择一个基线来表示特征的缺失。但是，现有的随机和有条件的基线可能会对解释产生负面影响。在本文中，通过分析不同基准的次优性，我们确定了有问题的基线，在该基线中，$ \ bm {x}&#39;_ i $之间的不对称相互作用（更换忠实有影响力的功能）和其他特征具有对模型的重要方向偏见输出，并得出结论，$ p（y | \ bm {x}&#39;_ i）= p（y）$可能最小化不对称涉及$ \ bm {x}&#39;_ i $的交互。我们进一步将$ \ bm {x}&#39;_ $的不大型性转换为标签$ l $，以避免估计$ p（y）$，并设计一种基于不确定的重新重新加油机制，以加速计算过程。我们对各种NLP任务进行实验，我们的定量分析证明了基于不确定性的重新加权机制的有效性。此外，通过衡量可解释方法和人类产生的解释的一致性，我们强调了模型推断与人类理解之间的差异。]]></description>
      <guid>https://arxiv.org/abs/2502.12209</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大多数投票范式转变：当受欢迎时会遇到最佳</title>
      <link>https://arxiv.org/abs/2502.12581</link>
      <description><![CDATA[ARXIV：2502.12581V2公告类型：新 
摘要：可靠地标记数据通常需要多个人类工人的注释。但是，人类远非完美。因此，从多个注释者聚集的标签是一种普遍的做法，以对真实标签进行更自信的估计。在许多汇总方法中，简单而著名的多数票（MV）选择了班级标签，以投票数量最高。然而，尽管其重要性，但MV标签聚合的最佳性尚未得到广泛的研究。我们通过表征MV在标签估计误差上实现理论上最佳下限的条件来解决这一差距。我们的结果捕获了对注释噪声的可耐受性限制，在该噪声下，MV可以最佳地恢复给定类别分布的标签。该最佳证书为标签聚合提供了一种更有原则的模型选择方法，可替代其他有时包括更高专家，金标签等的效率低下的实践，这些实践都受到了同样的人类不确定性和货币成本而损害的。合成和现实世界数据的实验证实了我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2502.12581</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过二重性差距诱导数据集蒸馏的广义内核诱导点</title>
      <link>https://arxiv.org/abs/2502.12607</link>
      <description><![CDATA[ARXIV：2502.12607V1公告类型：新 
摘要：我们提出了双重性差距KIP（DGKIP），这是数据集蒸馏的核诱导点（KIP）方法的扩展。尽管现有的数据集蒸馏方法通常依赖于双层优化，但DGKIP消除了通过在凸编程中利用二元性理论来进行优化的需求。已经引入了KIP方法，以避免双层优化。但是，它仅限于平方损耗，并且不支持其他更适合分类任务的其他损失功能（例如，跨透镜或铰链损失）。 DGKIP通过使用双重性差距在数据集蒸馏后利用上限对参数变化来解决此限制，从而使其应用程序能够应用于更广泛的损失函数范围。我们还通过在数据集蒸馏后的测试误差和预测一致性上提供上限来表征DGKIP的理论属性。对标准基准（例如MNIST和CIFAR-10）的实验结果表明，DGKIP保留了KIP的效率，同时提供更广泛的适用性和稳健的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.12607</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>联合的贝叶斯混合模型的变分推断</title>
      <link>https://arxiv.org/abs/2502.12684</link>
      <description><![CDATA[ARXIV：2502.12684V1公告类型：新 
摘要：我们提出了一种基于贝叶斯模型的大规模二进制和分类数据集的联合学习方法。我们使用与局部合并和删除数据批处理的变异推理一起引入了一个原则上的“鸿沟和征服”推理过程，然后在数据的批处理中进行删除移动，然后是跨批处理的“全局”合并移动，以找到全局聚类结构。我们表明，这些合并动作仅需要每个批处理中数据的摘要，从而使整个本地节点的联合学习无需共享完整的数据集。模拟和基准数据集的经验结果表明，与现有的聚类算法相比，我们的方法的性能很好。我们通过将其应用于大规模电子健康记录（EHR）数据来验证该方法的实际实用性。]]></description>
      <guid>https://arxiv.org/abs/2502.12684</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>绿色石灰：通过设计实验提高AI的解释性</title>
      <link>https://arxiv.org/abs/2502.12753</link>
      <description><![CDATA[ARXIV：2502.12753V1公告类型：新 
摘要：在人工智能（AI）中，许多模型和过程的复杂性通常超过了人类的解释性，因此了解为什么做出特定预测是一项挑战。在医疗保健等关键领域，这种缺乏透明度尤其有问题，因为对模型预测的信任至关重要。结果，机器学习（ML）和其他复杂模型的解释性已成为重点的关键领域。改善模型可解释性的努力通常涉及对AI系统进行实验，并通过更简单的机制近似其行为。但是，这些程序可能是资源密集的。实验的最佳设计旨在最大化从有限数量的观测值获得的信息，为提高这些解释性技术的效率提供了有希望的方法。
  为了证明这一潜力，我们探索了当地的可解释模型不足的解释（lime），这是Ribeiro，Singh和Guestrin引入的广泛使用的方法，2016年。Lime通过在兴趣实例附近生成新的数据点并通过它们传递给它们，从而提供了解释。模型。尽管有效，但此过程在计算上可能很昂贵，尤其是当预测成本高昂或需要许多样本时。石灰具有高度的用途，可以应用于各种型号和数据集。在这项工作中，我们专注于涉及表格数据，回归任务和线性模型作为可解释的本地近似值的模型。
  通过利用实验技术的最佳设计，我们减少了复杂模型的功能评估数量，从而将石灰的计算工作减少了大量量。我们认为这种修改后的石灰版本是节能或“绿色”。]]></description>
      <guid>https://arxiv.org/abs/2502.12753</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>蒸馏能扩散模型和顺序蒙特卡洛的组成和控制</title>
      <link>https://arxiv.org/abs/2502.12786</link>
      <description><![CDATA[ARXIV：2502.12786V1公告类型：新 
摘要：扩散模型可以作为基于能量的模型的时间指数序列进行配合，其中分数对应于能量函数的负梯度。与直接学习分数相反，能量参数化具有吸引力，因为能量本身可用于通过蒙特卡洛采样器来控制发电。与直接近似分数或denoiser相比，到目前为止，能量参数化模型中的建筑约束和训练不稳定性的性能较低。我们通过蒸馏预先训练的扩散模型，类似于得分向量场的helmholtz分解，从而解决了这些缺陷，以引入一种新的训练状态，以实现能量功能。我们通过将扩散采样程序施放为Feynman KAC模型，进一步展示了能量和得分之间的协同作用，其中使用来自学习的能量函数的电势来控制采样。 Feynman KAC模型形式主义可以通过顺序蒙特卡洛进行组成和低温采样。]]></description>
      <guid>https://arxiv.org/abs/2502.12786</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过质量排斥最佳运输的无监督异常检测</title>
      <link>https://arxiv.org/abs/2502.12793</link>
      <description><![CDATA[ARXIV：2502.12793V1公告类型：新 
摘要：在数据集中检测异常是机器学习中的长期问题。在这种情况下，将异常定义为显着偏离其余数据的样本。同时，最佳运输（OT）是与运输有关的数学领域，至少在两个概率指标之间，至少努力。在经典的OT中，措施本身的最佳运输策略是身份。在本文中，我们通过强迫样品取代其质量来解决异常检测，同时保持最少的努力目标。我们称这种新的运输问题大量排斥最佳运输（MROT）。自然，位于空间低密度区域的样品将被迫取代质量，从而产生更高的运输成本。我们使用这些概念来设计新的异常得分。通过现有基准测试和故障检测问题的一系列实验，我们表明我们的算法对现有方法有所改善。]]></description>
      <guid>https://arxiv.org/abs/2502.12793</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>带有正则对比度学习的时间序列归因地图</title>
      <link>https://arxiv.org/abs/2502.12977</link>
      <description><![CDATA[ARXIV：2502.12977V1公告类型：新 
摘要：基于梯度的归因方法旨在解释深度学习模型的决策，但到目前为止缺乏可识别性保证。在这里，我们提出了一种通过开发经时序列数据培训的正规对比度学习算法以及一种称为倒神经元梯度（统称为XCEBRA）的新归因方法，以生成具有可识别性保证的归因图。从理论上讲，我们表明XCEBRA具有识别数据生成过程的Jacobian矩阵的有利属性。从经验上讲，我们在合成数据集的地面真相归因图中证明了零与非零条目的稳健近似，以及基于特征消融，沙普利值和其他基于梯度的方法基于先前归因方法的显着改进。我们的工作构成了时间序列归因图的可识别推理的第一个示例，并为更好地理解时间序列数据（例如神经动力学和神经网络中的决策过程）提供了途径。]]></description>
      <guid>https://arxiv.org/abs/2502.12977</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>选择性推理具有统计意义的$ k $ nnad</title>
      <link>https://arxiv.org/abs/2502.12978</link>
      <description><![CDATA[ARXIV：2502.12978V1公告类型：新 
摘要：在本文中，我们使用k-nearthiend邻居方法研究了无监督的异常检测问题。 K-Neart最邻居异常检测（KNNAD）是一种简单但有效的方法，用于识别各个领域和田地的异常。包括KNNAD在内的异常检测中的一个关键挑战是适当量化检测到的异常的可靠性。为了解决这个问题，我们将KNNAD作为统计假设检验，并使用$ p $值量化错误检测的可能性。主要技术挑战在于对相同数据进行异常检测和统计测试，这阻碍了常规统计测试框架内的$ p $值计算。为了解决此问题，我们引入了一个称为选择性推理（SI）的统计假设检验框架，并提出了一种名为统计意义的NNAD（Stat-Knnad）的方法。通过利用SI，Stat-Knnad方法可确保检测到的异常具有统计学意义，并具有理论保证。所提出的Stat-Knnad方法适用于原始特征空间和来自深度学习模型的潜在特征空间的异常检测。通过对工业产品异常检测的合成数据和应用的数值实验，我们证明了Stat-Knnad方法的有效性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.12978</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机设计线性和内核回归模型的渐近乐观情绪</title>
      <link>https://arxiv.org/abs/2502.12999</link>
      <description><![CDATA[Arxiv：2502.129999V1公告类型：新 
摘要：我们在随机设计下得出了线性回归模型的封闭形式的渐近造型，并将其推广到内核脊回归。我们使用缩放的渐近乐观作为一种通用预测模型的复杂性度量，我们研究了线性回归模型，切线核（NTK）回归模型和三层完全连接的神经网络（NN）的基本不同行为。我们的贡献是两个方面：我们为使用缩放乐观作为模型预测复杂性度量提供了理论基础；我们从经验上表明，NN具有与此措施下的内核模型不同。通过重新采样技术，我们还可以计算具有真实数据的回归模型的乐观主义。]]></description>
      <guid>https://arxiv.org/abs/2502.12999</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>似然比正规分位数回归：适应综合预测到高维协变量移位</title>
      <link>https://arxiv.org/abs/2502.13030</link>
      <description><![CDATA[ARXIV：2502.13030V1公告类型：新 
摘要：我们考虑了协变量转移下的共形预测问题。给定来自源域的标记数据和来自协变量转移目标域的未标记数据，我们试图在目标域中构建具有有效边缘覆盖范围的预测集。大多数现有方法都需要估计未知的似然比函数，这对于高维数据（例如图像）可能会过高。为了应对这一挑战，我们介绍了可能性比率正则分位数回归（LR-QR）算法，该算法将弹球损失与新颖的正则化选择结合在一起，以构建阈值函数，而无需直接估计未知的可能性比率。我们表明，LR-QR方法在目标域中的所需级别具有覆盖范围，直到我们可以控制的小误差项。我们的证明借鉴了通过学习理论的稳定性界限对覆盖范围的新分析。我们的实验表明，LR-QR算法的表现优于高维预测任务的现有方法，包括社区和犯罪数据集的回归任务，以及Wilds存储库的图像分类任务。]]></description>
      <guid>https://arxiv.org/abs/2502.13030</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>共同信息的神经差异估计器</title>
      <link>https://arxiv.org/abs/2502.13085</link>
      <description><![CDATA[ARXIV：2502.13085V1公告类型：新 
摘要：估计共同信息（MI）是无需特定建模假设的随机数量依赖性的关键度量，是高维度的一个挑战性问题。我们提出了一个新型的共同信息估计器，基于使用归一化流程的参数性条件密度，这是一种近年来的深层生成模型。该估计器利用块自回旋结构来提高标准基准任务的偏差变化权衡。]]></description>
      <guid>https://arxiv.org/abs/2502.13085</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>混合分数训练：通过混合分布的分数估计，训练一步生成模型简单</title>
      <link>https://arxiv.org/abs/2502.09609</link>
      <description><![CDATA[ARXIV：2502.09609V2公告类型：交叉 
摘要：我们提出了分数训练（SMT），这是一个新颖的框架，用于训练一步生成模型，通过最大程度地减少称为$ \ alpha $ -skew Jensen-Shannon Divergence的分歧。 SMT以多种噪声水平估计了真实样品和假样品之间的混合分布的分数。与一致性模型相似，我们的方法使用验证的扩散模型支持从头开始的训练（SMT）和蒸馏，我们称之为混合分数蒸馏（SMD）。实施非常简单，需要最小的高参数调整并确保稳定的培训。 CIFAR-10和Imagenet 64x64上的实验表明，SMT/SMD具有竞争力，甚至可以超过现有方法。]]></description>
      <guid>https://arxiv.org/abs/2502.09609</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>幻觉是不可避免的，但统计学上可以忽略不计</title>
      <link>https://arxiv.org/abs/2502.12187</link>
      <description><![CDATA[ARXIV：2502.12187V1公告类型：交叉 
摘要：幻觉是一种语言模型（LM）产生非事实内容的现象，对LMS的实际部署构成了重大挑战。虽然已经提出了许多经验方法来减轻幻觉，但最近的一项研究确立了一个可计算性的理论结果，表明任何LM都将不可避免地会在一套无限的输入上产生幻觉，而不管培训数据的质量和数量如何模型架构，培训和推理算法。尽管可计算性理论结果似乎似乎是悲观的，但在实际观点中的重要性尚不清楚。相反，我们从概率的角度提出了积极的理论结果。具体而言，我们证明幻觉可以在统计上可以忽略不计，只要培训数据的质量和数量就足够。有趣的是，我们的积极结果与可计算性理论结果并存，这意味着，尽管无法完全消除一组无限投入的幻觉，但可以通过改善算法和培训数据来降低它们的概率。通过通过信息理论的角度评估两个看似矛盾的结果，我们认为我们的概率理论阳性结果更好地反映了实际考虑，而不是可计算性理论负面结果。]]></description>
      <guid>https://arxiv.org/abs/2502.12187</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用立方体化的对称排名一级牛顿方法，用于深度学习</title>
      <link>https://arxiv.org/abs/2502.12298</link>
      <description><![CDATA[ARXIV：2502.12298V1公告类型：交叉 
摘要：随机梯度下降和其他一阶变体，例如Adam和Adagrad，由于其计算效率和低存储记忆要求，通常用于深度学习领域。但是，这些方法不会利用曲率信息。因此，迭代可以收敛到鞍点或局部最小值。另一方面，准Newton方法计算了用可比的计算预算利用此信息的黑森近似值。准Newton方法重新使用以前计算的迭代和梯度来计算低级别的结构化更新。使用最广泛的准Newton更新是L-BFGS，它保证了正定的半明确Hessian近似值，使其适用于行搜索设置。但是，DNN中的损失函数是非凸的，其中Hessian可能是非阳性的。在本文中，我们建议使用有限的内存对称级别的准牛顿方法，该方法允许无限期的Hessian近似值，从而可以利用负曲率的方向。此外，我们使用一种修改的自适应正规化立方体方法，该方法生成了一系列具有适当正则化选择的封闭形式解决方案的立方子问题。我们研究了我们提出的方法在自动编码器和前馈神经网络模型上的性能，并比较了我们最先进的一阶自适应随机方法以及其他准Newton方法的方法。]]></description>
      <guid>https://arxiv.org/abs/2502.12298</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>