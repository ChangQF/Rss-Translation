<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 06 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>有限 VC 维网络：优缺点</title>
      <link>https://arxiv.org/abs/2502.02679</link>
      <description><![CDATA[arXiv:2502.02679v1 公告类型：新
摘要：从高维几何和统计学习理论的角度研究了神经网络对大数据集分类器的近似和学习。将网络输入输出函数集的 VC 维数对近似能力的影响与其对从数据样本中学习的一致性的影响进行了比较。结果表明，虽然有限的 VC 维数对于经验误差的均匀收敛是理想的，但对于从概率分布中得出的函数的近似，它可能不是理想的，该概率分布模拟了它们在给定类型的应用中出现的可能性。基于高维几何的测量集中特性，证明了在处理大数据集时，对于实现具有有限 VC 维数的输入输出函数集的网络，近似误差和经验误差的行为几乎是确定性的。讨论了通用近似性质的实际局限性、近似精度和从数据学习的一致性之间的权衡，以及具有 ReLU 单元的网络深度对其精度和一致性的影响。]]></description>
      <guid>https://arxiv.org/abs/2502.02679</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>当稳健风险仅被部分识别时，可实现分布稳健性</title>
      <link>https://arxiv.org/abs/2502.02710</link>
      <description><![CDATA[arXiv:2502.02710v1 公告类型：新
摘要：在安全关键应用中，机器学习模型应该在最坏情况分布偏移下具有良好的泛化能力，即具有较小的稳健风险。当训练分布足够异质以识别稳健风险时，基于不变性的算法可以证明利用偏移的结构假设。然而，在实践中，这种可识别性条件很少得到满足——这种情况在理论文献中迄今为止尚未得到充分探索。在本文中，我们旨在填补这一空白，并建议研究当稳健风险仅部分可识别时的更一般设置。特别是，我们引入了最坏情况稳健风险作为一种新的稳健性度量，无论可识别性如何，它始终具有明确的定义。它的最小值对应于与算法无关的（种群）极小最大值，该量度在部分可识别性下测量最佳可实现的稳健性。虽然这些概念可以更广泛地定义，但在本文中，我们为线性模型明确地引入并推导了这些概念，以便于具体展示。首先，我们表明现有的稳健性方法在部分可识别的情况下可证明是次优的。然后，我们在现实世界的基因表达数据上评估这些方法和（经验）最坏情况稳健风险的最小化器，并发现类似的趋势：随着来自看不见的环境的数据比例的增加，现有稳健性方法的测试误差变得越来越不理想，而考虑部分可识别性可以实现更好的泛化。]]></description>
      <guid>https://arxiv.org/abs/2502.02710</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>联邦 $Q$ 学习的间隙相关界限</title>
      <link>https://arxiv.org/abs/2502.02859</link>
      <description><![CDATA[arXiv:2502.02859v1 公告类型：新
摘要：我们首次对表格情景有限时域马尔可夫决策过程 (MDP) 中的在线策略联合 $Q$ 学习的遗憾和通信成本进行了缺口依赖分析。现有的 FRL 方法侧重于最坏情况，导致 $\sqrt{T}$ 型遗憾界限和通信成本界限，其中 $\log T$ 项随代理数量 $M$、状态 $S$ 和动作 $A$ 而缩放，其中 $T$ 是每个代理的平均总步数。相比之下，我们的新框架利用 MDP 的良性结构，例如严格正的次优性缺口，来实现 $\log T$ 型遗憾界限和精细的通信成本界限，从而解开探索和利用。我们的间隙相关遗憾界限揭示了一种独特的多智能体加速模式，而我们的间隙相关通信成本界限消除了对 $\log T$ 项中 $MSA$ 的依赖。值得注意的是，当 $M=1$ 时，我们的间隙相关通信成本界限也产生了更好的全局切换成本，从而从 $\log T$ 项中消除了 $SA$。]]></description>
      <guid>https://arxiv.org/abs/2502.02859</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有校准机器学习预测的算法</title>
      <link>https://arxiv.org/abs/2502.02861</link>
      <description><![CDATA[arXiv:2502.02861v1 公告类型：新
摘要：具有预测的算法领域将机器学习建议融入在线算法的设计中，以提高实际性能。虽然这个理论框架通常假设所有预测都具有统一的可靠性，但现代机器学习模型现在可以提供实例级的不确定性估计。在本文中，我们提出校准作为一种原则性和实用性的工具来弥合这一差距，并通过两个案例研究展示了校准建议的好处：滑雪租赁和在线作业调度问题。对于滑雪租赁，我们设计了一种实现最佳预测相关性能的算法，并证明在高方差设置下，校准建议比其他不确定性量化方法提供更有效的指导。对于作业调度，我们证明使用校准的预测器可以显着提高现有方法的性能。对现实世界数据的评估验证了我们的理论发现，突出了校准对具有预测的算法的实际影响。]]></description>
      <guid>https://arxiv.org/abs/2502.02861</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用经验神经切线核进行不确定性量化</title>
      <link>https://arxiv.org/abs/2502.02870</link>
      <description><![CDATA[arXiv:2502.02870v1 公告类型：新
摘要：虽然神经网络在各种任务中都表现出令人印象深刻的性能，但准确量化其预测中的不确定性对于确保其可信度并在关键系统中广泛采用至关重要。存在几种贝叶斯不确定性量化 (UQ) 方法，它们要么便宜要么可靠，但不能两者兼而有之。我们提出了一种事后基于采样的 UQ 方法，用于训练结束时的过度参数化网络。我们的方法通过在适当线性化的网络上采用（随机）梯度下降采样过程来构建高效且有意义的深度集成。我们证明我们的方法使用经验神经切线核有效地近似高斯过程的后验。通过一系列数值实验，我们表明我们的方法不仅在计算效率方面优于竞争方法（通常将成本降低多个因素），而且在回归和分类任务的各种 UQ 指标中保持了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.02870</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在回归、聚类和分类之间架起桥梁</title>
      <link>https://arxiv.org/abs/2502.02996</link>
      <description><![CDATA[arXiv:2502.02996v1 公告类型：新
摘要：回归，即根据某些特征 x 预测连续标量目标 y 的任务，是机器学习和统计学中最基本的任务之一。据观察和理论分析，经典方法均方误差最小化在训练神经网络时会导致次优结果。在这项工作中，我们提出了一种新方法来改进这些模型在具有连续标量目标的回归任务上的训练。我们的方法是基于以不同的方式执行此任务，使用目标编码器和预测解码器，灵感来自分类和聚类方法。我们展示了我们的方法在广泛的真实世界数据集上的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.02996</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有统计可靠性的频域时间序列异常检测</title>
      <link>https://arxiv.org/abs/2502.03062</link>
      <description><![CDATA[arXiv:2502.03062v1 公告类型：新
摘要：复杂系统中的有效异常检测需要在频域中识别变化点 (CP)，因为异常通常发生在多个频率上。本文将基于选择性推理 (SI) 的统计显著性 CP 检测的最新进展扩展到频域。所提出的 SI 方法使用 $p$ 值量化频域中检测到的 CP 的统计显著性，确保检测到的变化反映目标系统中真正的结构变化。我们解决了实现这一目标的两个主要技术挑战。首先，我们通过适当利用离散傅里叶变换 (DFT) 的属性将现有的 SI 框架扩展到频域。其次，我们开发了一种 SI 方法，为在多个频率上发生变化的 CP 提供有效的 $p$ 值。实验结果表明，所提出的方法能够可靠地识别具有强大统计保证的真正 CP，从而能够在复杂系统的频域中进行更准确的根本原因分析。]]></description>
      <guid>https://arxiv.org/abs/2502.03062</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Cox 比例风险模型与随机生存森林算法在预测临床试验数据中患者特定生存概率方面的比较</title>
      <link>https://arxiv.org/abs/2502.03119</link>
      <description><![CDATA[arXiv:2502.03119v1 公告类型：新
摘要：Cox 比例风险模型通常用于对具有事件发生时间结果的随机对照试验 (RCT) 数据进行模型开发。随机生存森林 (RSF) 是一种以高预测性能而闻名的机器学习算法。我们进行了一项全面的中立比较研究，以比较 Cox 回归和 RSF 在现实世界和模拟数据中的预测性能。根据预后预测模型比较的建议，使用多种性能指标比较性能。我们发现，虽然在使用 $C$ 指数时 RSF 通常优于 Cox 模型，但 Cox 模型预测可能经过更好的校准。就整体性能而言，Cox 模型在非比例风险设置中通常超过 RSF，而 RSF 通常表现更好，尤其是在样本量较小的情况下。RSF 的整体性能受更高审查率的影响更大，而 Cox 模型的整体性能受样本量较小的影响更大。]]></description>
      <guid>https://arxiv.org/abs/2502.03119</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CARROT：成本感知速率优化路由器</title>
      <link>https://arxiv.org/abs/2502.03261</link>
      <description><![CDATA[arXiv:2502.03261v1 公告类型：新
摘要：随着大型语言模型 (LLM) 数量的快速增长，人们最近对 LLM 路由产生了兴趣，即将查询定向到可以提供合适响应的最便宜的 LLM。根据这项工作，我们引入了 CARROT，这是一种成本感知速率优化路由器，可以根据性能和成本之间的任何期望权衡来选择模型。给定一个查询，CARROT 会根据模型成本和性能的估计选择一个模型。它的简单性使 CARROT 具有计算效率，而我们的理论分析证明了其路由性能的极小最大速率优化。除了 CARROT，我们还引入了智能价格感知路由 (SPROUT) 数据集，以便使用最新的最先进 LLM 在广泛的查询范围内进行路由。使用 SPROUT 和先前的基准测试（例如 Routerbench 和 open-LLM-leaderboard-v2），我们通过实证验证了 CARROT 相对于几种替代路由器的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.03261</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>上下文通用性足够吗？MLP 在上下文中也是通用的</title>
      <link>https://arxiv.org/abs/2502.03327</link>
      <description><![CDATA[arXiv:2502.03327v1 公告类型：新
摘要：Transformer 的成功通常与其执行上下文学习的能力有关。最近的研究表明，Transformer 在上下文中具有通用性，能够近似上下文的任何实值连续函数（对 $\mathcal{X}\subseteq \mathbb{R}^d$ 的概率度量）和查询 $x\in \mathcal{X}$。这引出了一个问题：上下文通用性是否解释了它们相对于经典模型的优势？我们通过证明具有可训练激活函数的 MLP 在上下文中也是通用的来回答这个问题。这表明 Transformer 的成功可能是由于其他因素，例如归纳偏差或训练稳定性。]]></description>
      <guid>https://arxiv.org/abs/2502.03327</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于混合的指导扩散模型的框架</title>
      <link>https://arxiv.org/abs/2502.03332</link>
      <description><![CDATA[arXiv:2502.03332v1 公告类型：新
摘要：去噪扩散模型推动了贝叶斯逆问题领域的重大进展。最近的方法使用预先训练的扩散模型作为先验来解决各种此类问题，仅利用推理时间计算，从而消除了在同一数据集上重新训练特定于任务的模型的需要。为了近似贝叶斯逆问题的后验，扩散模型从一系列中间后验分布中采样，每个分布都有一个难以处理的似然函数。这项工作提出了这些中间分布的新型混合近似。由于这些混合物的直接基于梯度的采样由于难以处理的术语而不切实际，我们提出了一种基于吉布斯采样的实用方法。我们通过对图像逆问题的大量实验验证了我们的方法，利用像素和潜在空间扩散先验，以及使用音频扩散模型进行源分离。代码可在 https://www.github.com/badr-moufad/mgdm 获得]]></description>
      <guid>https://arxiv.org/abs/2502.03332</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>概率图模型中的自适应变分推理：超越 Bethe、树重加权和凸自由能</title>
      <link>https://arxiv.org/abs/2502.03341</link>
      <description><![CDATA[arXiv:2502.03341v1 公告类型：新
摘要：概率图模型中的变分推断旨在近似基本量，例如边际分布和分区函数。流行的方法是 Bethe 近似、树重加权和其他类型的凸自由能。这些近似是有效的，但如果模型复杂且高度交互，则可能会失败。在这项工作中，我们分析了包括上述方法作为特殊情况的两类近似：首先，如果模型参数发生变化；其次，如果熵近似发生变化。我们讨论了这两种方法的优缺点，并从这种分析中推断出应该如何理想地构建自由能近似。根据我们的观察，我们提出了自动适应给定模型的近似值，并证明了它们对一系列难题的有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.03341</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多任务持续学习的最佳任务顺序</title>
      <link>https://arxiv.org/abs/2502.03350</link>
      <description><![CDATA[arXiv:2502.03350v1 公告类型：新
摘要：多任务的持续学习仍然是神经网络面临的主要挑战。在这里，我们研究任务顺序如何影响持续学习，并提出优化策略。利用具有潜在因素的线性师生模型，我们推导出将任务相似性和排序与学习表现联系起来的解析表达式。我们的分析揭示了在宽参数范围内成立的两个原则：（1）任务应从最不具代表性到最典型的顺序排列，（2）相邻任务应该不同。我们在合成数据和真实世界图像分类数据集（Fashion-MNIST、CIFAR-10、CIFAR-100）上验证了这些规则，证明了多层感知器和卷积神经网络的性能持续改进。因此，我们的工作为任务增量持续学习中的任务顺序优化提供了一个可推广的框架。]]></description>
      <guid>https://arxiv.org/abs/2502.03350</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈出重要一步：降噪分数匹配中的大学习率可防止记忆</title>
      <link>https://arxiv.org/abs/2502.03435</link>
      <description><![CDATA[arXiv:2502.03435v1 公告类型：新
摘要：去噪分数匹配在基于扩散的生成模型的性能中起着关键作用。然而，经验最优分数——去噪分数匹配的精确解——会导致记忆，其中生成的样本复制训练数据。然而，在实践中，即使没有明确的正则化，也只能观察到中等程度的记忆。在本文中，我们通过揭示由大学习率驱动的隐式正则化机制来研究这种现象。具体而言，我们表明，在小噪声状态下，经验最优分数表现出高度不规则性。然后我们证明，当通过具有足够大学习率的随机梯度下降训练时，神经网络不能稳定地收敛到具有任意小超额风险的局部最小值。因此，学习到的分数不能任意接近经验最优分数，从而减轻记忆。为了便于分析，我们考虑一维数据和两层神经网络。实验验证了学习率在防止记忆方面的关键作用，甚至超越了一维设置。]]></description>
      <guid>https://arxiv.org/abs/2502.03435</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>线性化最优传输 pyLOT 库：点云机器学习工具包</title>
      <link>https://arxiv.org/abs/2502.03439</link>
      <description><![CDATA[arXiv:2502.03439v1 公告类型：新
摘要：pyLOT 库提供了线性化最优传输 (LOT) 技术和方法的 Python 实现，可用于下游任务。管道通过固定参考分布的最优传输映射将概率分布嵌入到希尔伯特空间中，这种线性化允许使用现成的（线性）机器学习算法完成下游任务。我们提供了一个对狐猴牙齿的 3D 扫描执行 ML 的案例研究，其中原始的分类、聚类、降维和数据生成问题简化为对 LOT 嵌入表示执行的简单线性运算。]]></description>
      <guid>https://arxiv.org/abs/2502.03439</guid>
      <pubDate>Thu, 06 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>