<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 28 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过谱神经网络和非线性矩阵传感进行隐式正则化</title>
      <link>https://arxiv.org/abs/2402.17595</link>
      <description><![CDATA[arXiv:2402.17595v1 公告类型：交叉
摘要：近年来，隐式正则化现象作为神经网络卓越泛化能力的一个基本方面引起了人们的兴趣。简而言之，它意味着许多神经网络中的梯度下降动力学，即使损失函数中没有任何显式正则化器，也会收敛到正则化学习问题的解决方案。然而，试图从理论上解释这种现象的已知结果绝大多数集中在线性神经网络的设置上，而线性结构的简单性对于现有的论点尤其重要。在本文中，我们在具有一般类别非线性激活函数的更现实的神经网络的背景下探讨了这个问题，并在矩阵感知问题的设置中严格证明了此类网络的隐式正则化现象，以及严格的速率保证确保梯度下降以指数方式快速收敛。在这方面，我们贡献了一种称为谱神经网络（简称 SNN）的网络架构，它特别适合矩阵学习问题。从概念上讲，这需要通过奇异值和奇异向量来协调矩阵空间，而不是通过它们的条目来协调，这是矩阵学习的一个潜在富有成果的视角。我们证明，SNN 架构本质上比普通神经网络更适合理论分析，并通过数学保证和实证研究证实了其在矩阵感知背景下的有效性。我们相信 SNN 架构有潜力在广泛的矩阵学习场景中具有广泛的适用性。]]></description>
      <guid>https://arxiv.org/abs/2402.17595</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>通过抽象属性的优先建模增强贝叶斯优化</title>
      <link>https://arxiv.org/abs/2402.17343</link>
      <description><![CDATA[arXiv:2402.17343v1 公告类型：交叉
摘要：实验（设计）优化是设计和发现新产品和工艺的关键驱动力。贝叶斯优化 (BO) 是优化昂贵的黑盒实验设计流程的有效工具。虽然贝叶斯优化是一种原则性的数据驱动的实验优化方法，但它从头开始学习一切，并且可以从人类（领域）专家的专业知识中受益匪浅，这些专家经常使用不一定是直接的物理属性来推理不同抽象级别的系统。测量的（或可测量的）。在本文中，我们提出了一种人类与人工智能协作的贝叶斯框架，将专家对未测量的抽象属性的偏好纳入代理建模中，以进一步提高 BO 的性能。我们提供有效的策略，还可以处理优先判断中任何不正确/误导性的专家偏见。我们讨论了我们提出的框架的收敛行为。我们涉及合成函数和现实世界数据集的实验结果表明我们的方法相对于基线的优越性。]]></description>
      <guid>https://arxiv.org/abs/2402.17343</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>线性时间变换器的潜在注意力</title>
      <link>https://arxiv.org/abs/2402.17512</link>
      <description><![CDATA[arXiv:2402.17512v1 公告类型：交叉
摘要：变压器中标准注意力机制的时间复杂度与序列长度呈二次方关系。我们引入了一种方法，基于通过潜在向量定义注意力，将其减少为随时间线性缩放。该方法很容易用作标准注意力机制的直接替代品。我们的“Latte Transformer”模型可以用于双向和单向任务，因果版本允许循环实现，在语言生成任务的推理过程中节省内存和时间。虽然下一个标记预测与标准 Transformer 的序列长度成线性比例，但 Latte Transformer 需要恒定的时间来计算下一个标记。我们方法的经验性能与标准注意力相当，但允许缩放到比标准注意力中的实用更大的上下文窗口。]]></description>
      <guid>https://arxiv.org/abs/2402.17512</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>物理信号的对抗性扰动</title>
      <link>https://arxiv.org/abs/2402.17104</link>
      <description><![CDATA[arXiv:2402.17104v1 公告类型：交叉
摘要：我们研究了基于计算机视觉的信号分类器对其输入的对抗性扰动的脆弱性，其中信号和扰动受到物理约束。我们考虑这样一种场景，其中源和干扰源发出信号，这些信号以波的形式传播到检测器，检测器尝试通过使用预先训练的神经网络分析接收到的信号的频谱图来对源进行分类。通过解决偏微分方程约束的优化问题，我们构建了干扰信号，即使接收信号的频谱图的扰动几乎无法察觉，也会导致检测器对源进行错误分类。尽管此类问题可能有数百万个决策变量，但我们引入了有效解决它们的方法。我们的实验表明，人们可以在各种物理条件下计算各种机器学习模型的有效且物理上可实现的对抗性扰动。]]></description>
      <guid>https://arxiv.org/abs/2402.17104</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>用于预测复杂系统动态的生成学习</title>
      <link>https://arxiv.org/abs/2402.17157</link>
      <description><![CDATA[arXiv:2402.17157v1 公告类型：交叉
摘要：我们引入生成模型，通过学习和发展复杂系统的有效动态来加速复杂系统的模拟。在提出的有效动力学生成学习（G-LED）中，高维数据的实例被下采样到通过自回归注意机制演化的较低维流形。反过来，贝叶斯扩散模型将低维流形映射到其相应的高维空间，捕获系统动力学的统计数据。我们在几个基准系统的模拟中展示了 G-LED 的功能和缺点，包括 Kuramoto-Sivashinsky (KS) 方程、向后台阶上的二维高雷诺数流以及三维湍流通道流的模拟。结果表明，生成学习为以降低的计算成本准确预测复杂系统的统计特性提供了新领域。]]></description>
      <guid>https://arxiv.org/abs/2402.17157</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>无限维中的随机近似</title>
      <link>https://arxiv.org/abs/2402.17258</link>
      <description><![CDATA[arXiv:2402.17258v1 公告类型：交叉
摘要：随机逼近 (SA) 于 20 世纪 50 年代初提出，几十年来一直是一个活跃的研究领域。虽然最初的重点是统计问题，但它被认为可以应用于信号处理、凸优化。 %在过去的十年中，人们对 SA 的兴趣重新燃起，因为在后来的几年中，SA 在强化学习 (RL) 中得到了应用，并导致人们重新燃起了兴趣。
  虽然大部分文献都是针对来自有限维欧几里德空间的观测值的 SA 的情况，但人们一直有兴趣将其扩展到无限维。推广到希尔伯特空间相对容易一些，但当我们涉及巴纳赫空间时，情况并非如此——因为在巴纳赫空间的情况下，即使是大数定律一般也不成立。我们考虑一些在 Banach 空间中进行近似的情况。我们的框架包括 Banach 空间 $\Bb$ 为 $\Cb([0,1],\R^d)$ 以及 $\L^1([0,1],\R^d) 的情况$，这两个案例甚至没有 Radon-Nikodym 属性。]]></description>
      <guid>https://arxiv.org/abs/2402.17258</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>基于熵的生成模型新颖性的可解释评估</title>
      <link>https://arxiv.org/abs/2402.17287</link>
      <description><![CDATA[arXiv:2402.17287v1 公告类型：交叉
摘要：生成模型框架和架构的大规模发展需要有原则的方法来评估模型与参考数据集或基线生成模型相比的新颖性。虽然最近的文献广泛研究了生成模型的质量、多样性和泛化性的评估，但机器学习社区尚未充分研究模型相对于基线模型的新颖性评估。在这项工作中，我们关注多模态生成模型下的新颖性评估，并尝试回答以下问题：给定生成模型 $\mathcal{G}$ 的样本和参考数据集 $\mathcal{S}$，我们如何才能比 $\mathcal{S}$ 更频繁地发现和计算 $\mathcal{G}$ 表达的模式。我们为所描述的任务引入了谱方法，并提出了基于内核的熵新颖性（KEN）分数来量化分布 $P_\mathcal{G}$ 相对于分布 $P_\mathcal{S}$ 的基于模式的新颖性。我们分析解释了具有亚高斯分量的混合分布下 KEN 得分的行为。接下来，我们开发了一种基于 Cholesky 分解的方法来计算观察样本的 KEN 分数。我们通过展示合成和真实图像分布的几个数值结果来支持基于 KEN 的新颖性量化。我们的数值结果表明所提出的方法在检测新颖模式和比较最先进的生成模型方面取得了成功。]]></description>
      <guid>https://arxiv.org/abs/2402.17287</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>关于 ML 后门检测作为假设检验问题的（不）可行性</title>
      <link>https://arxiv.org/abs/2402.16926</link>
      <description><![CDATA[arXiv:2402.16926v1 公告类型：交叉
摘要：我们为机器学习系统中的后门检测问题引入了正式的统计定义，并用它来分析此类问题的可行性，为我们的定义的实用性和适用性提供了证据。这项工作的主要贡献是后门检测的不可能性结果和可实现性结果。我们展示了一个没有免费午餐的定理，证明普遍的（对手不知道的）后门检测是不可能的，除了非常小的字母大小。因此，我们认为后门检测方法需要明确或隐含地了解对手。然而，我们的工作并不意味着后门检测不能在特定场景下工作，科学文献中成功的后门检测方法就证明了这一点。此外，我们将我们的定义与分布外检测问题的可能近似正确（PAC）可学习性联系起来。]]></description>
      <guid>https://arxiv.org/abs/2402.16926</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    <item>
      <title>沿朗之万扩散的独立样本和未调整的朗之万算法</title>
      <link>https://arxiv.org/abs/2402.17067</link>
      <description><![CDATA[arXiv:2402.17067v1 公告类型：交叉
摘要：我们研究初始随机变量和当前随机变量沿着马尔可夫链变得独立的速率，重点关注连续时间中的朗之万扩散和离散时间中的未调整朗之万算法（ULA）。我们通过随机变量的互信息来测量随机变量之间的依赖性。对于朗之万扩散，我们表明，当目标是强对数凹时，互信息以指数速度快速收敛到 $0$；当目标是弱对数凹时，互信息以多项式速率收敛到 $0$。这些速率类似于类似假设下朗之万扩散的混合时间。对于 ULA，我们表明当目标是强对数凹且平滑时，互信息以指数速度快速收敛到 $0$。我们通过开发这些马尔可夫链的混合时间分析的相互版本来证明我们的结果。我们还提供了基于朗之万扩散和 ULA 的强大数据处理不等式的替代证明，并通过在互信息中显示这些过程的规律性结果。]]></description>
      <guid>https://arxiv.org/abs/2402.17067</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    <item>
      <title>通过二参数模型和梯度流学习高维目标</title>
      <link>https://arxiv.org/abs/2402.17089</link>
      <description><![CDATA[arXiv:2402.17089v1 公告类型：新
摘要：我们探讨了当 $W&lt;d$ 必然存在大量 GF 不可学习目标子集时，通过梯度流（GF）使用 $W$ 参数模型学习 $d$ 维目标的理论可能性。特别是，可学习目标集在 $\mathbb R^d$ 中并不密集，并且与 $W$ 维球体同胚的 $\mathbb R^d$ 的任何子集都包含不可学习目标。最后，我们观察到，关于几乎保证的双参数学习的主要定理中的模型是使用分层过程构建的，因此不能用单个初等函数来表达。我们表明，这种限制是至关重要的，因为对于一大类初等函数来说，这种可学习性可以被排除。]]></description>
      <guid>https://arxiv.org/abs/2402.17089</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>数据集公平性：通过实用性保证可实现数据公平性</title>
      <link>https://arxiv.org/abs/2402.17106</link>
      <description><![CDATA[arXiv:2402.17106v1 公告类型：新
摘要：在机器学习公平性中，最小化不同敏感群体之间差异的训练模型通常会导致准确性下降，这种现象称为公平性与准确性的权衡。这种权衡的严重性从根本上取决于数据集特征，例如数据集不平衡或偏差。因此，跨数据集使用统一的公平性要求仍然值得怀疑，并且通常会导致模型的效用大幅降低。为了解决这个问题，我们提出了一种计算有效的方法来近似针对单个数据集定制的公平性-准确性权衡曲线，并得到严格的统计保证的支持。通过利用 You-Only-Train-Once (YOTO) 框架，我们的方法减轻了在近似权衡曲线时必须训练多个模型的计算负担。此外，我们通过在该曲线周围引入置信区间来量化近似值的不确定性，从而为任何给定精度阈值的可接受的公平违规范围提供基于统计的观点。我们对表格、图像和语言数据集的实证评估强调，我们的方法为从业者提供了跨各种数据模式的数据集特定公平性决策的原则框架。]]></description>
      <guid>https://arxiv.org/abs/2402.17106</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>MIM-Reasoner：通过理论保证进行学习以实现多重影响力最大化</title>
      <link>https://arxiv.org/abs/2402.16898</link>
      <description><![CDATA[arXiv:2402.16898v1 公告类型：交叉
摘要：多重影响力最大化（MIM）要求我们识别一组种子用户，以便最大化多重网络中受影响用户的预期数量。 MIM 一直是中心研究课题之一，特别是在当今的社交网络领域，用户参与多个在线社交网络（OSN），并且他们的影响力可以同时在多个 OSN 之间传播。尽管存在多种 MIM 组合算法，但由于其对异构网络的泛化能力及其多样化的传播特性，基于学习的解决方案一直是人们所期望的。在本文中，我们引入了 MIM-Reasoner，将强化学习与概率图模型相结合，有效捕获给定多路网络层内和层间的复杂传播过程，从而解决 MIM 中最具挑战性的问题。我们为 MIM-Reasoner 建立了理论保证，并对合成数据集和现实数据集进行了广泛的分析，以验证 MIM-Reasoner 的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.16898</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型中的相变揭示了数据的层次性质</title>
      <link>https://arxiv.org/abs/2402.16991</link>
      <description><![CDATA[arXiv:2402.16991v1 公告类型：新
摘要：理解真实数据的结构对于推进现代深度学习方法至关重要。图像等自然数据被认为是由以分层和组合方式组织的特征组成，神经网络在学习过程中捕获这些特征。最近的进展表明，扩散模型可以生成高质量的图像，这表明它们有能力捕获这种底层结构。我们在数据的分层生成模型中研究这种现象。我们发现，在时间$t$之后发生的后向扩散过程受到某个阈值时间的相变的控制，其中重建高级特征（例如图像的类别）的概率突然下降。相反，低级特征（例如图像的特定细节）的重建在整个扩散过程中顺利进行。该结果意味着有时在过渡之后，类发生了变化，但生成的样本可能仍然由初始图像的低级元素组成。我们通过类无条件 ImageNet 扩散模型的数值实验验证了这些理论见解。我们的分析描述了扩散模型中时间和规模之间的关系，并提出生成模型作为对组合数据属性进行建模的强大工具。]]></description>
      <guid>https://arxiv.org/abs/2402.16991</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>用于非线性动力系统状态和参数估计的迭代 INLA</title>
      <link>https://arxiv.org/abs/2402.17036</link>
      <description><![CDATA[arXiv:2402.17036v1 公告类型：新
摘要：数据同化（DA）方法使用微分方程产生的先验来稳健地内插和外推数据。处理高维、非线性 PDE 先验的集成方法等流行技术主要关注状态估计，但很难准确地学习参数。另一方面，基于机器学习的方法可以自然地学习状态和参数，但它们的适用性可能有限，或者产生难以解释的不确定性。受空间统计中集成嵌套拉普拉斯逼近（INLA）方法的启发，我们提出了一种基于迭代线性化动态模型的 DA 替代方法。这会在每次迭代时产生一个高斯马尔可夫随机场，使人们能够使用 INLA 来推断状态和参数。我们的方法可用于任意非线性系统，同时保留可解释性，并且进一步证明在 DA 任务上优于现有方法。通过提供更细致的方法来处理非线性偏微分方程先验，我们的方法提高了预测的准确性和鲁棒性，特别是在数据稀疏普遍存在的情况下。]]></description>
      <guid>https://arxiv.org/abs/2402.17036</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>关于具有潜在根变量的贝叶斯网络的注释</title>
      <link>https://arxiv.org/abs/2402.17087</link>
      <description><![CDATA[arXiv:2402.17087v1 公告类型：新
摘要：我们描述了从以潜在变量为根节点的贝叶斯网络计算的似然函数。我们表明，剩余的明显变量的边际分布也可以分解为贝叶斯网络，我们称之为经验网络。明显变量的观察数据集使我们能够量化经验贝叶斯网络的参数。我们证明（i）来自原始贝叶斯网络的此类数据集的可能性由经验数据集的可能性的全局最大值主导； (ii) 当且仅当贝叶斯网络的参数与经验模型的参数一致时，才能达到这样的最大值。]]></description>
      <guid>https://arxiv.org/abs/2402.17087</guid>
      <pubDate>Wed, 28 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    </channel>
</rss>