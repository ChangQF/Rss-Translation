<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 14 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>学习连续空间上的条件分布</title>
      <link>https://arxiv.org/abs/2406.09375</link>
      <description><![CDATA[arXiv:2406.09375v1 公告类型：新
摘要：我们研究基于样本的多维单元盒条件分布学习，允许特征和目标空间具有不同的维度。我们的方法涉及在特征空间中不同查询点附近对数据进行聚类，以在目标空间中创建经验度量。我们采用两种不同的聚类方案：一种基于固定半径球，另一种基于最近邻居。我们为这两种方法的收敛速度建立了上限，并从这些界限中推导出半径和邻居数量的最佳配置。我们建议将最近邻居方法纳入神经网络训练中，因为我们的实证分析表明它在实践中具有更好的性能。为了提高效率，我们的训练过程利用近似最近邻居搜索和随机二进制空间分区。此外，我们采用了 Sinkhorn 算法和稀疏强制传输计划。我们的实证结果表明，通过适当设计的结构，神经网络能够在局部适应适当水平的 Lipschitz 连续性。为了可重复性，我们的代码可在 \url{https://github.com/zcheng-a/LCD_kNN} 获得。]]></description>
      <guid>https://arxiv.org/abs/2406.09375</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:30 GMT</pubDate>
    </item>
    <item>
      <title>压缩 Tucker 分解的无意识子空间嵌入</title>
      <link>https://arxiv.org/abs/2406.09387</link>
      <description><![CDATA[arXiv:2406.09387v1 公告类型：新
摘要：张量文献强调规范多元 (CP) 张量分解的随机嵌入（低失真降维工具），而更具表现力的 Tucker 分解的类似结果则相对缺乏。当沿每个模式应用无意识随机嵌入时，这项工作为 Tucker 分解的估计建立了一般的 Johnson-Lindenstrauss (JL) 类型保证。当这些嵌入来自 JL 最优族时，可以在与最近的 CP 结果一致的嵌入维度限制下在 $\varepsilon$ 相对误差内估计分解。我们实现了具有随机嵌入的高阶正交迭代 (HOOI) 分解算法，以展示这种方法的实际好处及其提高原本难以实现的张量分析的可访问性的潜力。在中等规模的面部图像和 fMRI 神经成像数据集上，实证结果表明，与传统的 HOOI 相比，可以大幅降低维度，同时重构误差的增加微乎其微（$\leq$5% 的误差较大，大型模型的计算时间缩短 50%-60%，每个模式的维度减少 50%）。特别是对于大张量，我们的方法优于传统的高阶奇异值分解 (HOSVD) 和最近提出的 TensorSketch 方法。]]></description>
      <guid>https://arxiv.org/abs/2406.09387</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:30 GMT</pubDate>
    </item>
    <item>
      <title>高度相关条件下的岭插值器的精确分析——随机对偶理论观点</title>
      <link>https://arxiv.org/abs/2406.09199</link>
      <description><![CDATA[arXiv:2406.09199v1 公告类型：新
摘要：我们考虑完全行/列相关的线性回归模型，并研究几个经典估计量（包括最小范数插值器 (GLS)、普通最小二乘 (LS) 和岭回归器）。我们表明，\emph{随机对偶理论} (RDT) 可用于获得所有估计量相关的优化量（包括 \emph{预测风险}（测试或泛化误差））的精确闭式表征。在定性层面上，随着特征数量/样本大小比率的增加，结果恢复了风险众所周知的非单调（所谓的双下降）行为。在定量层面上，我们的闭式结果显示了风险如何明确地依赖于所有关键模型参数，包括问题维度和协方差矩阵。此外，当不存在样本内（或时间序列）相关性时，我们得到的结果的一个特殊情况与 [6,16,17,24] 中通过谱方法得到的相应结果完全匹配。]]></description>
      <guid>https://arxiv.org/abs/2406.09199</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:29 GMT</pubDate>
    </item>
    <item>
      <title>用于结构化预测的深度草图输出核回归</title>
      <link>https://arxiv.org/abs/2406.09253</link>
      <description><![CDATA[arXiv:2406.09253v1 公告类型：新
摘要：通过利用输出空间中的核技巧，核诱导损失提供了一种原则性的方法来定义各种输出模式的结构化输出预测任务。特别是，它们已成功用于替代非参数回归的背景下，其中核技巧通常也在输入空间中得到利用。然而，当输入是图像或文本时，深度神经网络等更具表现力的模型似乎比非参数方法更合适。在这项工作中，我们解决了如何训练神经网络来解决结构化输出预测任务的问题，同时仍然受益于核诱导损失的多功能性和相关性。我们设计了一个新颖的深度神经架构系列，其最后一层在由核诱导损失衍生的无限维输出特征空间的数据相关有限维子空间中进行预测。该子空间被选为经验核协方差算子的随机近似版本的特征函数的跨度。有趣的是，这种方法解锁了梯度下降算法（以及任何神经架构）在结构化预测中的应用。对合成任务以及现实世界监督图预测问题的实验表明了我们方法的相关性。]]></description>
      <guid>https://arxiv.org/abs/2406.09253</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:29 GMT</pubDate>
    </item>
    <item>
      <title>相关因子回归模型中的岭插值器——精确风险分析</title>
      <link>https://arxiv.org/abs/2406.09183</link>
      <description><![CDATA[arXiv:2406.09183v1 公告类型：新
摘要：我们考虑相关的\emph{因子}回归模型（FRM）并分析经典岭插值器的性能。利用强大的\emph{随机对偶理论}（RDT）数学引擎，我们获得了底层优化问题和所有相关优化量的\emph{精确}闭式表征。特别是，我们提供\emph{过度预测风险}表征，清楚地显示了对所有关键模型参数、协方差矩阵、载荷和维度的依赖性。作为过度参数化比率的函数，广义最小二乘（GLS）风险也表现出众所周知的\emph{双下降}（非单调）行为。与经典线性回归模型（LRM）类似，我们证明这种 FRM 现象可以通过最佳调整的岭正则化来平滑。理论结果由数值模拟补充，并且观察到两者之间有很好的一致性。此外，我们注意到“脊平滑”对于超过 5 的过度参数化比率通常已经效果有限，而对于超过 10 的过度参数化比率几乎没有效果。这巩固了这样一种观点，即最近最流行的神经网络范式之一 \emph{零训练（插值）泛化良好} 具有更广泛的适用性，包括 FRM 估计/预测上下文中的适用性。]]></description>
      <guid>https://arxiv.org/abs/2406.09183</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>通过具有平滑归纳偏差的物理信息学习开始在固定维度中过度拟合</title>
      <link>https://arxiv.org/abs/2406.09194</link>
      <description><![CDATA[arXiv:2406.09194v1 公告类型：新
摘要：机器学习理论的最新进展表明，使用过度参数化的机器学习算法对噪声样本进行插值总是会导致不一致。然而，这项工作令人惊讶地发现，当使用物理信息学习进行由描述物理定律的偏微分方程 (PDE) 控制的监督任务时，插值机器学习可以表现出良性的过度拟合和一致性。分析为解决涉及椭圆 PDE 的线性逆问题的核岭（无）回归提供了渐近 Sobolev 范数学习曲线。结果表明，与标准回归设置相比，PDE 算子可以稳定方差并导致固定维问题的良性过度拟合。还研究了通过最小化不同的 Sobolev 范数作为隐式正则化而引入的各种归纳偏差的影响。值得注意的是，对于岭回归和无岭回归，收敛速度与特定（平滑）归纳偏差无关。对于正则化最小二乘估计量，当正则化参数选择正确时，所有（足够平滑的）归纳偏差都可以实现最佳收敛速度。平滑性要求恢复了先前在贝叶斯设置中发现的条件，并将结论扩展到最小范数插值估计量。]]></description>
      <guid>https://arxiv.org/abs/2406.09194</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>不确定性量化视角下的生成模型与判别模型</title>
      <link>https://arxiv.org/abs/2406.09172</link>
      <description><![CDATA[arXiv:2406.09172v1 公告类型：新
摘要：从给定数据集中学习参数模型确实能够通过参数条件概率分布捕获随机变量之间的内在依赖关系，进而根据观察变量预测标签变量的值。在本文中，我们对生成方法和判别方法进行了比较分析，这两种方法在构造和底层推理问题的结构上有所不同。我们的目标是比较两种方法通过后验预测分布在认知不确定性感知推理中利用来自各种来源的信息的能力。我们评估了先验分布的作用，在生成情况下是显性的，在判别情况下是隐性的，从而引发了关于判别模型遭受数据集不平衡的讨论。接下来，我们研究了生成案例中观察变量所起的双重作用，并讨论了这两种方法与半监督学习的兼容性。我们还提供了实用的见解，并研究了建模选择如何影响后验预测分布的采样。对此，我们提出了一种通用的采样方案，使两种方法都能进行监督学习，并且当与所考虑的建模方法兼容时，还能进行半监督学习。在本文中，我们使用仿射回归的例子来说明我们的论点和结论，并通过使用基于神经网络的模型进行分类模拟来验证我们的比较分析。]]></description>
      <guid>https://arxiv.org/abs/2406.09172</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>通过有效的邻接性测试实现可扩展且灵活的因果发现</title>
      <link>https://arxiv.org/abs/2406.09177</link>
      <description><![CDATA[arXiv:2406.09177v1 公告类型：新
摘要：为了在多变量系统中做出准确的预测、理解机制和设计干预措施，我们希望从大规模数据中学习因果图。不幸的是，所有可能的因果图的空间是巨大的，因此可扩展且准确地搜索最适合数据的因果图是一项挑战。原则上，我们可以通过测试变量的条件独立性来大幅减少搜索空间，或完全学习图表。但是，确定因果图中的两个变量是否相邻可能需要进行指数级的测试。在这里，我们构建了一种可扩展且灵活的方法来评估因果图中的两个变量是否相邻，即可微分邻接测试 (DAT)。DAT 用可证明等效的放松问题取代了指数级的测试。然后，它通过训练两个神经网络来解决这个问题。我们基于 DAT 构建了一种图学习方法，即 DAT-Graph，它也可以从带有干预的数据中学习。 DAT-Graph 可以以最先进的精度学习 1000 个变量的图表。使用 DAT-Graph 学习的图表，我们还构建了模型，可以更准确地预测干预措施对大规模 RNA 测序数据的影响。]]></description>
      <guid>https://arxiv.org/abs/2406.09177</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>使用多模态变分自动编码器更新贝叶斯结构模型</title>
      <link>https://arxiv.org/abs/2406.09051</link>
      <description><![CDATA[arXiv:2406.09051v1 公告类型：新
摘要：本文提出了一种用于贝叶斯结构模型更新的新框架，并提出了一种利用多模态变分自动编码器的替代单模态编码器的方法。该方法有助于对描述观测数据的似然性进行有效的非参数估计。它特别适用于适用于各种动态分析模型的高维相关同时观测。使用具有加速度和动态应变测量的单层框架建筑的数值模型对所提出的方法进行了基准测试。]]></description>
      <guid>https://arxiv.org/abs/2406.09051</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:26 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫扩散模型的操作员知情得分匹配</title>
      <link>https://arxiv.org/abs/2406.09084</link>
      <description><![CDATA[arXiv:2406.09084v1 公告类型：新
摘要：扩散模型通常使用分数匹配进行训练，但分数匹配与定义模型的特定前向过程无关。本文认为，马尔可夫扩散模型比其他类型的扩散模型更具优势，因为可以利用其相关运算符来改进训练过程。具体而言，(i) 存在一个明确的正式解决方案，即一系列时间相关的核均值嵌入作为前向过程；(ii) 分数匹配和相关估计量的推导可以简化。基于 (i)，我们提出了黎曼扩散核平滑，这至少在低维环境中改善了对神经分数近似的需求；基于 (ii)，我们提出了基于运算符的分数匹配，这是一种方差减少技术，在低维和高维扩散建模中都很容易实现，并且在经验概念验证中被证明可以改善分数匹配。]]></description>
      <guid>https://arxiv.org/abs/2406.09084</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:26 GMT</pubDate>
    </item>
    <item>
      <title>通用微分方程中不确定性量化的评估</title>
      <link>https://arxiv.org/abs/2406.08853</link>
      <description><![CDATA[arXiv:2406.08853v1 公告类型：新
摘要：科学机器学习是一种新方法，它将物理知识和机械模型与数据驱动技术相结合，以揭示复杂过程的控制方程。在可用的方法中，通用微分方程 (UDE) 用于将机械公式形式的先验知识与通用函数逼近器（如神经网络）相结合。UDE 的有效性不可或缺的是使用经验数据联合估计机械公式中的参数和通用函数逼近器。然而，结果模型的稳健性和适用性取决于与这些参数相关的不确定性的严格量化，以及整个模型或其组成部分的预测能力。通过这项工作，我们为 UDE 提供了不确定性量化 (UQ) 的形式化，并研究了重要的频率学派和贝叶斯方法。通过分析三个复杂程度不同的合成示例，我们评估了集成、变分推理和马尔可夫链蒙特卡罗采样作为 UDE 的认知 UQ 方法的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2406.08853</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:25 GMT</pubDate>
    </item>
    <item>
      <title>变分推理训练的贝叶斯神经网络的中心极限定理</title>
      <link>https://arxiv.org/abs/2406.09048</link>
      <description><![CDATA[arXiv:2406.09048v1 公告类型：新
摘要：在本文中，我们严格推导了无限宽度极限中贝叶斯两层神经网络的中心极限定理 (CLT)，并通过回归任务的变分推理进行训练。不同的网络通过正则化证据下限的不同最大化方案进行训练：(i) 理想情况，使用重新参数化技巧精确估计多个高斯积分，(ii) 使用蒙特卡洛采样的小批量方案，通常称为 Bayes-by-Backprop，以及 (iii) 计算成本更低的算法，称为 Minimal VI。后者是最近通过利用在平均场极限级别获得的信息而引入的。对于允许相同渐近极限的三种方案，大数定律已经得到严格证明。通过推导 CLT，这项工作表明理想化和 Bayes-by-Backprop 方案具有相似的波动行为，这与 Minimal VI 不同。数值实验表明，尽管方差较大，但最小 VI 方案仍然更有效，这得益于其在计算复杂性方面的显著提升。]]></description>
      <guid>https://arxiv.org/abs/2406.09048</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:25 GMT</pubDate>
    </item>
    <item>
      <title>剪枝是学习高维稀疏特征的最佳方法</title>
      <link>https://arxiv.org/abs/2406.08658</link>
      <description><![CDATA[arXiv:2406.08658v1 公告类型：新
摘要：虽然在实践中普遍观察到将网络修剪到一定程度的稀疏性可以提高特征的质量，但对这种现象的理论解释仍然难以捉摸。在这项工作中，我们通过证明可以使用梯度下降训练的修剪神经网络在高维中最佳地学习大量统计模型来研究这一点。
我们考虑学习形式为 $y = \sigma^*(\boldsymbol{V}^{\top} \boldsymbol{x}) + \epsilon$ 的单指标和多指标模型，其中 $\sigma^*$ 是度为 $p$ 的多项式，$\boldsymbol{V} \in \mathbbm{R}^{d \times r}$ 和 $r \ll d$ 是包含相关模型方向的矩阵。我们假设 $\boldsymbol{V}$ 满足矩阵的某种 $\ell_q$ 稀疏性条件，并表明与 $\boldsymbol{V}$ 的稀疏性水平成比例的神经网络修剪可以提高其样本复杂度，而未修剪的网络则没有。此外，我们在此设置下建立了相关统计查询 (CSQ) 下限，其中考虑了 $\boldsymbol{V}$ 的稀疏性水平。我们表明，如果 $\boldsymbol{V}$ 的稀疏性水平超过某个阈值，则使用梯度下降算法训练修剪网络可以实现 CSQ 下限建议的样本复杂度。然而，在相同情况下，我们的结果表明，基础独立方法（例如通过标准梯度下降训练的模型，这些模型使用旋转不变的随机权重初始化）只能实现次优样本复杂度。]]></description>
      <guid>https://arxiv.org/abs/2406.08658</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:24 GMT</pubDate>
    </item>
    <item>
      <title>$Q$ 函数差分的正交化估计</title>
      <link>https://arxiv.org/abs/2406.08697</link>
      <description><![CDATA[arXiv:2406.08697v1 公告类型：新
摘要：离线强化学习在许多具有可用观察数据但由于安全、成本和其他问题而无法在线部署新策略的环境中都很重要。因果推理和机器学习的许多最新进展都针对因果对比函数（例如 CATE）的估计，这足以优化决策并可以适应潜在的更平滑的结构。我们开发了 R 学习器的动态泛化（Nie and Wager 2021，Lewis and Syrgkanis 2021），用于估计和优化 $Q^\pi$ 函数的差异，$Q^\pi(s,1)-Q^\pi(s,0)$（可用于优化多值动作）。我们利用正交估计来提高较慢干扰估计率下的收敛速度，并证明在边际条件下策略优化的一致性。该方法可以利用 $Q$ 函数和行为策略的黑盒干扰估计器来针对更结构化的 $Q$ 函数对比进行估计。]]></description>
      <guid>https://arxiv.org/abs/2406.08697</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:24 GMT</pubDate>
    </item>
    <item>
      <title>非均匀两层网络的大步长梯度下降：边缘改进和快速优化</title>
      <link>https://arxiv.org/abs/2406.08654</link>
      <description><![CDATA[arXiv:2406.08654v1 公告类型：新
摘要：在逻辑损失下使用大步长梯度下降 (GD) 的典型神经网络训练通常涉及两个不同的阶段，其中经验风险在第一阶段振荡，但在第二阶段单调下降。我们在满足近同质性条件的两层网络中研究了这种现象。我们表明，一旦经验风险低于某个阈值（取决于步长），第二阶段就会开始。此外，我们表明，在第二阶段，归一化边际几乎单调增长，表明 GD 在训练非同质预测因子时存在隐性偏差。如果数据集是线性可分的，并且激活函数的导数有界于零，我们表明平均经验风险会降低，这意味着第一阶段必须以有限的步骤停止。最后，我们证明，通过选择适当大的步长，经历这种相变的 GD 比单调降低风险的 GD 更有效。我们的分析适用于任何宽度的网络，超出了众所周知的神经正切核和平均场范围。]]></description>
      <guid>https://arxiv.org/abs/2406.08654</guid>
      <pubDate>Fri, 14 Jun 2024 06:20:23 GMT</pubDate>
    </item>
    </channel>
</rss>