<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 28 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>高维图卷积回归模型下的迁移学习用于节点分类</title>
      <link>https://arxiv.org/abs/2405.16672</link>
      <description><![CDATA[arXiv:2405.16672v1 公告类型：新
摘要：节点分类是一项基本任务，但在许多现实场景中，获取节点分类标签可能具有挑战性且成本高昂。迁移学习已成为解决这一挑战的一种有前途的解决方案，它利用源域的知识来增强目标域的学习。现有的节点分类迁移学习方法主要侧重于将图卷积网络 (GCN) 与各种迁移学习技术相结合。虽然这些方法已经显示出有希望的结果，但它们往往缺乏理论保证、限制条件和对超参数选择的高度敏感性。为了克服这些限制，我们提出了一个图卷积多项逻辑回归 (GCR) 模型和一种基于 GCR 模型的迁移学习方法，称为 Trans-GCR。我们为在高维设置下在 GCR 模型下获得的估计提供了理论保证。此外，Trans-GCR 表现出卓越的经验性能，计算成本低，并且比现有方法需要更少的超参数。]]></description>
      <guid>https://arxiv.org/abs/2405.16672</guid>
      <pubDate>Tue, 28 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>常步长随机逼近中记忆与非线性的共谋</title>
      <link>https://arxiv.org/abs/2405.16732</link>
      <description><![CDATA[arXiv:2405.16732v1 公告类型：新 
摘要：在这项工作中，我们研究了马尔可夫数据的随机近似（SA）和恒定步长 $\alpha&gt;0$ 下的非线性更新。现有的工作主要集中在 i.i.d.数据或线性更新规则。我们采取新的视角，仔细检查数据的马尔可夫依赖性和非线性更新规则的同时存在，描述这两种结构之间的相互作用如何导致现有技术无法捕获的复杂性。通过利用 SA 更新的平滑性和递归特性，我们对 SA 迭代 $\theta_k$ 和马尔可夫数据 $x_k$ 之间的相关性进行细粒度分析。这使我们能够克服现有分析中的障碍，并首次建立联合过程$(x_k, \theta_k)_{k\geq0}$的弱收敛。此外，我们提出了 SA 迭代渐近偏差的精确表征，由 $\mathbb{E}[\theta_\infty]-\theta^\ast=\alpha(b_\text{m}+b_\text {n}+b_\text{c})+O(\alpha^{3/2})$。这里，$b_\text{m}$ 与马尔可夫噪声相关，$b_\text{n}$ 与非线性相关，值得注意的是，$b_\text{c}$ 表示马尔可夫噪声之间的乘法交互作用和非线性，这在以前的作品中是不存在的。作为我们分析的副产品，我们推导出更高矩 $\mathbb{E}[\|\theta_k-\theta^\ast\|^{2p}]$ 上的有限时间界限，并提出非渐近几何收敛迭代率以及中心极限定理。]]></description>
      <guid>https://arxiv.org/abs/2405.16732</guid>
      <pubDate>Tue, 28 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>深度弱非线性网络的贝叶斯推理</title>
      <link>https://arxiv.org/abs/2405.16630</link>
      <description><![CDATA[arXiv:2405.16630v1 公告类型：新 
摘要：我们在严格的物理层面上证明，具有完全连接的神经网络和 $\phi(t) = t + \psi t^3/L$ 形式的成形非线性的贝叶斯推理是（扰动）可解的其中训练数据点的数量 $P$ 、输入维度 $N_0$ 、网络层宽度 $N$ 和网络深度 $L$ 同时很大。我们的结果在数据假设较弱的情况下成立；主要约束是$P &lt; N_0$。我们提供了在任意温度下以 $1/N$ 计算模型证据和任意阶后验的技术。我们报告一阶计算的以下结果：
  1. 当宽度$N$远大于深度$L$和训练集大小$P$时，神经网络贝叶斯推理与使用核的贝叶斯推理一致。 $\psi$ 的值决定了训练数据隐式嵌入到特征图下的球体、双曲线或平面的曲率。
  2. 当 $LP/N$ 是一个小常数时，神经网络贝叶斯推理偏离内核机制。在零温度下，神经网络贝叶斯推理相当于使用数据依赖核的贝叶斯推理，$LP/N$作为控制特征学习程度的有效深度。
  3. 在深度线性网络（$\psi=0$）和噪声数据的限制情况下，我们展示了一个简单的数据模型，其证据和泛化误差在零温度下是最佳的。随着 $LP/N$ 的增加，证据和泛化能力都进一步提高，证明了深度在良性过度拟合中的好处。]]></description>
      <guid>https://arxiv.org/abs/2405.16630</guid>
      <pubDate>Tue, 28 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>Polyak-Ruppert 平均线性随机逼近的高斯逼近和乘子自举及其在 TD 学习中的应用</title>
      <link>https://arxiv.org/abs/2405.16644</link>
      <description><![CDATA[arXiv:2405.16644v1 公告类型：新 
摘要：在本文中，我们获得了步长递减的线性随机逼近 (LSA) 算法的 Polyak-Ruppert 平均迭代的多元正态逼近的 Berry-Esseen 界。我们的研究结果表明，当设置最激进的步长 $\alpha_{k} \asymp k^{-1/2}$ 时，可以实现最快的正态逼近率。此外，我们证明了基于乘子引导的 LSA 参数估计置信区间的非渐近有效性。该过程在后续观测值到达时更新 LSA 估计以及一组随机扰动的 LSA 估计。我们用线性函数近似来说明我们在时间差异学习的设置中的发现。]]></description>
      <guid>https://arxiv.org/abs/2405.16644</guid>
      <pubDate>Tue, 28 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>具有强盗反馈的上下文线性优化</title>
      <link>https://arxiv.org/abs/2405.16564</link>
      <description><![CDATA[arXiv:2405.16564v1 公告类型：新 
摘要：上下文线性优化（CLO​​）使用预测观察来减少随机成本系数的不确定性，从而提高平均成本绩效。一个例子是具有随机边缘成本（例如流量）和预测特征（例如滞后流量、天气）的随机最短路径。现有的 CLO 工作假设数据已经完全观察到成本系数向量，但在许多应用中，我们只能看到历史决策的已实现成本，即只是随机成本系数向量的一个投影，我们将其称为 bandit反馈。我们研究了一类带有强盗反馈的 CLO 算法，我们将其称为诱导经验风险最小化 (IERM)，其中我们拟合一个预测模型来直接优化其诱导的策略的下游性能。我们展示了 IERM 的快速后悔界限，允许错误指定的模型类和优化估计的灵活选择，并且我们开发了计算上可处理的替代损失。我们的独立利益理论的一个副产品是，在充分反馈和错误指定的政策类别的情况下，IERM 很快就会后悔。我们使用随机最短路径示例对不同建模选择的性能进行数值比较，并从实证结果中提供实用的见解。]]></description>
      <guid>https://arxiv.org/abs/2405.16564</guid>
      <pubDate>Tue, 28 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>反射流匹配</title>
      <link>https://arxiv.org/abs/2405.16577</link>
      <description><![CDATA[arXiv:2405.16577v1 公告类型：新
摘要：连续正则化流 (CNF) 学习常微分方程以将先前样本转换为数据。流匹配 (FM) 最近作为一种无需模拟的方法来训练 CNF，它将速度模型回归到条件速度场。然而，在受限域上，学习到的速度模型可能会导致不良流动，从而导致非常不自然的样本，例如过饱和图像，这是由于流匹配误差和模拟误差造成的。为了解决这个问题，我们在 CNF 中添加了一个边界约束项，这会导致反射 CNF 将轨迹保持在受限域内。我们提出反射流匹配 (RFM)，通过以无需模拟的方式匹配条件速度场来训练反射 CNF 中的速度模型，类似于 vanilla FM。此外，RFM 中条件速度场的分析形式避免了潜在的偏差近似，使其优于现有的基于分数的受限域生成模型。我们证明 RFM 在标准图像基准上取得了相当或更好的结果，并在高指导权重下产生了高质量的类条件样本。]]></description>
      <guid>https://arxiv.org/abs/2405.16577</guid>
      <pubDate>Tue, 28 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>协变量平移下的训练条件覆盖范围</title>
      <link>https://arxiv.org/abs/2405.16594</link>
      <description><![CDATA[arXiv:2405.16594v1 公告类型：新 
摘要：保形预测中的训练条件覆盖保证涉及误差分布的集中度，以训练数据为条件，低于某个标称水平。保形预测方法最近已推广到协变量偏移设置，即训练数据和测试数据之间的协变量分布变化。在本文中，我们通过针对分布变化定制的 Dvoretzky-Kiefer-Wolfowitz (DKW) 不等式的加权版本，研究了协变量偏移下一系列共形预测方法的训练条件覆盖特性。分割共形方法的结果几乎是无假设的，而完全共形和折刀+方法的结果依赖于强假设，包括训练算法的一致稳定性。]]></description>
      <guid>https://arxiv.org/abs/2405.16594</guid>
      <pubDate>Tue, 28 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>关于大型语言模型与 RLHF 对齐的算法偏差：偏好崩溃和匹配正则化</title>
      <link>https://arxiv.org/abs/2405.16455</link>
      <description><![CDATA[arXiv:2405.16455v1 公告类型：新 
摘要：准确地将大语言模型（LLM）与人类偏好保持一致对于提供公平、经济合理且统计上有效的决策过程至关重要。然而，我们认为，基于人类反馈的强化学习（RLHF）——通过奖励模型使法学硕士与人类偏好保持一致的主要方法——由于其基于 Kullback-Leibler 的优化正则化而受到固有的算法偏差的影响。在极端情况下，这种偏见可能会导致我们称之为偏好崩溃的现象，即少数人的偏好实际上被忽视。为了减轻这种算法偏差，我们引入了偏好匹配（PM）RLHF，这是一种新颖的方法，可以证明法学硕士与 Bradley-Terry-Luce/Plackett-Luce 模型下奖励模型的偏好分布相一致。我们方法的核心是 PM 正则化器，它采用 LLM 响应策略概率分布的负对数形式，这有助于 LLM 平衡响应多样化和奖励最大化。值得注意的是，我们通过求解 PM 属性所必需的常微分方程来获得该正则化器。为了实际实现，我们引入了 PM RLHF 的条件变体，该变体专为自然语言生成而定制。最后，我们通过 OPT-1.3B 和 Llama-2-7B 模型的实验实证验证了条件 PM RLHF 的有效性，结果表明，通过某种指标衡量，与人类偏好相比，与人类偏好的一致性提高了 29% 至 41%。标准 RLHF。]]></description>
      <guid>https://arxiv.org/abs/2405.16455</guid>
      <pubDate>Tue, 28 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>随机特征的方差减少耦合：最优传输的视角</title>
      <link>https://arxiv.org/abs/2405.16541</link>
      <description><![CDATA[arXiv:2405.16541v1 公告类型：新
摘要：随机特征 (RF) 是一种流行的扩展机器学习中核方法的技术，用随机蒙特卡罗估计代替精确的核评估。它们支持多种模型，从高效变换器（通过近似注意力）到稀疏谱高斯过程（通过近似协方差函数）。通过加快这些估计的收敛速度可以进一步提高效率：方差减少问题。我们通过最优传输的统一框架来解决这个问题，使用理论见解和数值算法为在欧几里得和离散输入空间上定义的核开发新的高性能 RF 耦合。它们享有具体的理论性能保证，有时提供强大的经验下游收益，包括对图的可扩展近似推理。我们对方差减少作为范式的好处和局限性得出了令人惊讶的结论。]]></description>
      <guid>https://arxiv.org/abs/2405.16541</guid>
      <pubDate>Tue, 28 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>粗体：布尔逻辑深度学习</title>
      <link>https://arxiv.org/abs/2405.16339</link>
      <description><![CDATA[arXiv:2405.16339v1 公告类型：新 
摘要：深度学习是计算密集型的，主要致力于降低算术复杂性，特别是在数据移动主导的能耗方面。虽然现有文献强调推理，但培训要耗费大量资源。本文通过引入布尔变分的概念提出了一种新颖的数学原理，使得由布尔权重和输入组成的神经元可以使用布尔逻辑而不是梯度下降和实际算术在布尔域中进行有效的训练（这是第一次）。我们探索其收敛性，进行广泛的实验基准测试，并通过考虑芯片架构、内存层次结构、数据流和算术精度来提供一致的复杂性评估。我们的方法在 ImageNet 分类中实现了基线全精度精度，并超越了语义分割中最先进的结果，在图像超分辨率和基于 Transformer 的模型的自然语言理解方面具有显着的性能。此外，它还显着降低了训练和推理过程中的能耗。]]></description>
      <guid>https://arxiv.org/abs/2405.16339</guid>
      <pubDate>Tue, 28 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>Wasserstein GAN 及其他领域的微分方程方法</title>
      <link>https://arxiv.org/abs/2405.16351</link>
      <description><![CDATA[arXiv:2405.16351v1 公告类型：新 
摘要：我们提出了一种新的理论视角来看待 Wasserstein 生成对抗网络（WGAN）。在我们的框架中，我们定义了受分布相关常微分方程 (ODE) 启发的离散化。我们证明了这种离散化是收敛的，并提出了一类可行的对抗性训练方法来实现这种离散化，我们将其称为 W1 前向欧拉（W1-FE）。特别是，ODE 框架允许我们实现持久训练，这是一种新颖的训练技术，如果没有 ODE 解释，则无法应用于典型的 WGAN 算法。值得注意的是，当我们不实施持续训练时，我们证明我们的算法可以简化为现有的 WGAN 算法；当我们适当提高持续训练的水平时，我们的算法在低维和高维示例中都优于现有的 WGAN 算法。]]></description>
      <guid>https://arxiv.org/abs/2405.16351</guid>
      <pubDate>Tue, 28 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>反向转换核：加速扩散推理的灵活框架</title>
      <link>https://arxiv.org/abs/2405.16387</link>
      <description><![CDATA[arXiv:2405.16387v1 公告类型：新 
摘要：为了从经过训练的扩散模型生成数据，大多数推理算法（例如 DDPM、DDIM 和其他变体）依赖于离散化反向 SDE 或其等效的 ODE。在本文中，我们将整个去噪扩散过程分解为几个部分，每个部分对应一个反向转移核（RTK）采样子问题。具体来说，DDPM 对 RTK 使用高斯近似，导致每个子问题的复杂度较低，但需要大量的分段（即子问题），这被认为是低效的。为了解决这个问题，我们开发了一个通用的 RTK 框架，该框架能够实现更平衡的子问题分解，从而产生 $\tilde O(1)$ 子问题，每个子问题都具有强对数凹目标。然后，我们建议利用两种快速采样算法，即 Metropolis-Adjusted Langevin Algorithm (MALA) 和 Underdamped Langevin Dynamics (ULD) 来解决这些强对数凹子问题。这就产生了用于扩散推理的 RTK-MALA 和 RTK-ULD 算法。理论上，我们进一步开发了 RTK-MALA 和 RTK-ULD 在总变差（TV）距离上的收敛保证：RTK-ULD 可以在 $\tilde{\mathcal O}(d^{1 /2}\epsilon^{-1})$ 在温和条件下，RTK-MALA 在稍微严格的条件下具有 $\mathcal{O}(d^{2}\log(d/\epsilon))$ 收敛速度。这些理论结果超过了扩散推理的最先进的收敛速度，并得到了数值实验的良好支持。]]></description>
      <guid>https://arxiv.org/abs/2405.16387</guid>
      <pubDate>Tue, 28 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>使用深度生成先验近似的不可分解模型的联合学习</title>
      <link>https://arxiv.org/abs/2405.16055</link>
      <description><![CDATA[arXiv:2405.16055v1 公告类型：新 
摘要：联邦学习（FL）允许跨分散的客户端进行协作模型训练，同时通过避免数据共享来保护隐私。然而，当前的 FL 方法假设客户端模型之间存在条件独立性，从而限制了捕获依赖性的先验的使用，例如高斯过程 (GP)。我们通过深度生成模型逼近 (SIGMA) 先验引入了结构化独立性，使 FL 能够跨客户端进行不可分解的模型，从而将 FL 的适用性扩展到空间统计、流行病学、环境科学以及建模依赖关系至关重要的其他领域。 SIGMA 先验是一种预训练的深度生成模型，它近似所需的先验，并在潜在变量中引入指定的条件独立结构，从而创建适合 FL 设置的近似模型。我们展示了 SIGMA 先验对合成数据的有效性，并在 FL 空间数据的现实示例中展示了其实用性，使用条件自回归先验对澳大利亚各地的空间依赖性进行建模。我们的工作在依赖数据建模对于准确预测和决策至关重要的领域中实现了新的 FL 应用。]]></description>
      <guid>https://arxiv.org/abs/2405.16055</guid>
      <pubDate>Tue, 28 May 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>弱到强泛化的统计框架</title>
      <link>https://arxiv.org/abs/2405.16236</link>
      <description><![CDATA[arXiv:2405.16236v1 公告类型：新 
摘要：现代大语言模型（LLM）对齐技术依赖于人类反馈，但尚不清楚这些技术是否从根本上限制了对齐 LLM 的能力。特别是，目前尚不清楚是否有可能将具有超人能力的（较强的）法学硕士与（较弱的）人类反馈结合起来，而不降低其能力。这是弱到强泛化问题的一个实例：使用较弱（能力较差）的反馈来训练更强（能力更强）的模型。我们证明，通过从预先训练的法学硕士中引出潜在知识，弱到强的泛化是可能的。特别是，我们将弱到强泛化问题视为迁移学习问题，其中我们希望将潜在概念从弱模型迁移到强预训练模型。我们证明，朴素的微调方法存在根本性的局限性，但问题结构提出的另一种基于细化的方法证明克服了微调的局限性。最后，我们通过三个 LLM 对齐任务展示了细化方法的实际适用性。]]></description>
      <guid>https://arxiv.org/abs/2405.16236</guid>
      <pubDate>Tue, 28 May 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>机器学习回归模型的系统偏差及其修正：基于成像的脑年龄预测的应用</title>
      <link>https://arxiv.org/abs/2405.15950</link>
      <description><![CDATA[arXiv:2405.15950v1 公告类型：新 
摘要：连续结果的机器学习模型通常会产生系统性有偏差的预测，特别是对于很大程度上偏离平均值的值。具体来说，对大值结果的预测往往存在负偏差，而对小值结果的预测则存在正偏差。我们将这种线性集中趋势扭曲偏差称为“机器学习回归的系统偏差”。在本文中，我们首先证明这个问题在各种机器学习模型中都存在，然后深入研究其理论基础。我们提出了一种通用的约束优化方法，旨在纠正这种偏差，并开发一种计算高效的算法来实现我们的方法。我们的模拟结果表明，我们的校正方法有效地消除了预测结果的偏差。我们将所提出的方法应用于使用神经影像数据预测大脑年龄。与竞争的机器学习模型相比，我们的方法有效解决了基于神经成像的大脑年龄计算中长期存在的“机器学习回归的系统偏差”问题，产生了大脑年龄的无偏差预测。]]></description>
      <guid>https://arxiv.org/abs/2405.15950</guid>
      <pubDate>Tue, 28 May 2024 06:19:08 GMT</pubDate>
    </item>
    </channel>
</rss>