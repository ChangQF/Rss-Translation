<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 05 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>经验风险最小化的通用比率</title>
      <link>https://arxiv.org/abs/2412.02810</link>
      <description><![CDATA[arXiv:2412.02810v1 公告类型：新
摘要：众所周知的经验风险最小化 (ERM) 原理是许多广泛使用的机器学习算法的基础，在经典 PAC 理论中起着至关重要的作用。学习算法性能的常见描述是其所谓的“学习曲线”，即预期误差随输入样本大小的衰减。由于 PAC 模型无法解释学习曲线的行为，最近的研究探索了一种替代的通用学习模型，并最终揭示了最佳通用学习率和统一学习率之间的区别 (Bousquet 等人，2021)。然而，对这种差异的基本理解，特别是对 ERM 原理的理解尚未发展。
在本文中，我们考虑了可实现情况下 ERM 的通用学习问题，并研究了可能的通用速率。我们的主要结果是一个基本的四分法：ERM 的通用学习率只有四种，也就是说，任何可通过 ERM 学习的概念类的学习曲线都会以 $e^{-n}$、$1/n$、$\log(n)/n$ 或任意慢的速率衰减。此外，我们通过新的复杂性结构，对哪些概念类属于这些类别提供了完整的描述。我们还开发了新的组合维度，只要有可能，这些维度就会为这些速率提供尖锐的渐近有效常数因子。]]></description>
      <guid>https://arxiv.org/abs/2412.02810</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>汤普森抽样逻辑赌博机的信息论分析</title>
      <link>https://arxiv.org/abs/2412.02861</link>
      <description><![CDATA[arXiv:2412.02861v1 公告类型：新
摘要：我们研究了汤普森抽样算法在逻辑强盗问题中的性能，其中代理以逻辑函数 $\exp(\beta \langle a, \theta \rangle)/(1+\exp(\beta \langle a, \theta \rangle))$ 确定的概率获得二元奖励。我们重点关注动作 $a$ 和参数 $\theta$ 位于 $d$ 维单位球内的设置，动作空间包含参数空间。采用 (Russo $\&amp;$ Van Roy, ​​2015) 引入的信息论框架，我们分析了信息比，它定义为最佳和实际奖励之间的预期平方差与最佳动作和奖励之间的互信息之比。在之前结果的基础上，我们进一步确定信息比率受 $\tfrac{9}{2}d$ 限制。值得注意的是，我们获得了 $O(d\sqrt{T \log(\beta T/d)})$ 中的遗憾界限，该界限仅对数地依赖于参数 $\beta$。]]></description>
      <guid>https://arxiv.org/abs/2412.02861</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于偏好的纯探索</title>
      <link>https://arxiv.org/abs/2412.02988</link>
      <description><![CDATA[arXiv:2412.02988v1 公告类型：新
摘要：我们研究了具有向量值奖励的基于偏好的纯探索问题。奖励使用（给定的）偏好锥 $\mathcal{C}$ 进行排序，我们的目标是确定一组帕累托最优臂。首先，为了量化偏好的影响，我们推导出一个新的样本复杂度下限，用于识别置信度为 $1-\delta$ 的最优选策略。我们的下限引出了偏好锥的几何形状所起的作用，并强调了与现有最​​佳臂识别问题变体相比的难度差异。当奖励遵循高斯分布时，我们进一步阐明了这种几何形状。然后，我们对下限进行了凸松弛。并利用它来设计基于偏好的跟踪和停止 (PreTS) 算法来识别最优选的策略。最后，我们通过推导向量值奖励的新集中不等式证明了 PreTS 的样本复杂度是渐近紧密的。]]></description>
      <guid>https://arxiv.org/abs/2412.02988</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有调整偏移噪声的广义扩散模型</title>
      <link>https://arxiv.org/abs/2412.03134</link>
      <description><![CDATA[arXiv:2412.03134v1 公告类型：新
摘要：扩散模型已成为机器学习中数据分布建模的基本工具，并应用于图像生成、药物发现和音频合成。尽管这些模型取得了成功，但它们在生成具有极端亮度值的数据时仍面临挑战，这一点从广泛使用的框架（如稳定扩散）的局限性可以看出。偏移噪声已被提出作为此问题的经验解决方案，但其理论基础仍未得到充分探索。在本文中，我们提出了一种广义扩散模型，该模型在严格的概率框架内自然地结合了额外的噪声。我们的方法修改了正向和反向扩散过程，使输入能够扩散到具有任意均值结构的高斯分布中。我们根据证据下限推导出一个损失函数，通过某些调整建立了其与偏移噪声的理论等价性，同时扩大了其适用性。在合成数据集上的实验表明，我们的模型有效地解决了与亮度相关的挑战，并且在高维场景中优于传统方法。]]></description>
      <guid>https://arxiv.org/abs/2412.03134</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用神经跳跃微分方程进行非参数过滤、估计和分类</title>
      <link>https://arxiv.org/abs/2412.03271</link>
      <description><![CDATA[arXiv:2412.03271v1 公告类型：新
摘要：神经跳跃 ODE 模拟神经 ODE 观测值与新观测值到达时的跳跃之间的条件期望。它们已证明在具有不规则和部分观测值的环境中，在弱规律性假设下，完全数据驱动的在线预测的有效性。这项工作将框架扩展到输入输出系统，可直接应用于在线过滤和分类。我们为这种方法建立了理论收敛保证，为 $L^2$ 最优过滤提供了稳健的解决方案。实证实验突出了该模型优于经典参数方法的性能，特别是在具有复杂底层分布的场景中。这些结果强调了该方法在金融和健康监测等时间敏感领域的潜力，其中实时准确性至关重要。]]></description>
      <guid>https://arxiv.org/abs/2412.03271</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于对比学习的严格 PAC-贝叶斯风险证书</title>
      <link>https://arxiv.org/abs/2412.03486</link>
      <description><![CDATA[arXiv:2412.03486v1 公告类型：新
摘要：对比表示学习是一种通过增强来学习未标记数据表示的现代范式——确切地说，对比模型学习将语义相似的样本对（正样本对）嵌入得比独立抽取的样本（负样本）更近。尽管对比学习的统计理论在经验上取得了成功并在基础模型中得到广泛应用，但其探索程度仍然较低。最近的研究已经为对比损失开发了泛化误差界限，但由此产生的风险证书要么是空洞的（基于 Rademacher 复杂性或 $f$ 散度的证书），要么需要对样本做出在实践中不合理的强假设。本文考虑到流行的 SimCLR 框架的实际考虑，为对比表示学习开发了非空洞的 PAC-Bayesian 风险证书。值得注意的是，我们考虑到 SimCLR 将增强数据的正对重用为其他数据的负样本，从而导致强依赖性并使经典的 PAC 或 PAC-Bayesian 边界不适用。我们通过结合 SimCLR 特定因素（包括数据增强和温度缩放）进一步完善下游分类损失的现有边界，并得出对比零一风险的风险证书。对比损失和下游预测的最终边界比以前的风险证书要严格得多，这一点已在 CIFAR-10 上的实验中得到证明。]]></description>
      <guid>https://arxiv.org/abs/2412.03486</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越算法超参数：机器学习应用中的预处理超参数和相关陷阱</title>
      <link>https://arxiv.org/abs/2412.03491</link>
      <description><![CDATA[arXiv:2412.03491v1 公告类型：新
摘要：基于监督机器学习 (ML) 充分生成和评估预测模型通常具有挑战性，尤其是对于应用研究领域经验不足的用户而言。在模型生成过程涉及超参数调整的环境中需要特别注意，即数据驱动的不同类型的超参数优化以提高结果模型的预测性能。关于调整的讨论通常集中在 ML 算法的超参数上（例如，基于树的算法的每个终端节点中的最小观察数）。在这种情况下，人们经常忽略在将数据提供给算法之前应用于数据的预处理步骤的超参数也存在（例如，如何处理数据中缺失的特征值）。因此，尝试使用不同预处理选项来提高模型性能的用户可能没有意识到这构成了一种超参数调整形式（尽管是非正式的和非系统的），因此可能无法报告或解释这种优化。为了阐明这个问题，本文回顾并实证说明了生成和评估预测模型的不同程序，明确解决了应用 ML 用户通常处理算法和预处理超参数的不同方式。通过强调潜在的陷阱，特别是那些可能导致夸大性能要求的陷阱，本评论旨在进一步提高 ML 应用中预测建模的质量。]]></description>
      <guid>https://arxiv.org/abs/2412.03491</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于学习弱形式算子和梯度流的自测试损失函数</title>
      <link>https://arxiv.org/abs/2412.03506</link>
      <description><![CDATA[arXiv:2412.03506v1 公告类型：新
摘要：损失函数的构建是涉及 PDE 和梯度流中弱形式算子的数据驱动建模的一大挑战，特别是由于需要适当地选择测试函数。我们通过引入自测试损失函数来解决这一挑战，该函数采用依赖于未知参数的测试函数，特别是对于算子线​​性依赖于未知数的情况。所提出的自测试损失函数为梯度流节省能量，并与随机微分方程的预期对数似然比相一致。重要的是，它是二次的，有助于对逆问题的可识别性和适定性进行理论分析，同时也导致有效的参数或非参数回归算法。它在计算上很简单，只需要低阶导数甚至完全不需要导数，数值实验证明了它对噪声和离散数据的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2412.03506</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于核的最优控制：无穷小生成器方法</title>
      <link>https://arxiv.org/abs/2412.01591</link>
      <description><![CDATA[arXiv:2412.01591v1 公告类型：交叉 
摘要：本文提出了一种新颖的方法，用于在无限维再生核希尔伯特空间中使用无穷小生成器学习对非线性随机系统进行最优控制。我们的学习框架利用系统动力学和阶段成本函数的数据样本，仅提供控制惩罚和约束。所提出的方法直接学习无限维假设空间中受控 Fokker-Planck-Kolmogorov 方程的扩散算子。该算子模拟了控制系统状态的概率测度的连续时间演变。我们证明这种方法与现代凸算子理论 Hamilton-Jacobi-Bellman 递归无缝集成，从而实现了数据驱动的最优控制问题解决方案。此外，我们的统计学习框架包括非受控前向无穷小生成器的非参数估计器作为特殊情况。从合成微分方程到模拟机器人系统的数值实验展示了我们的方法与现代数据驱动和经典非线性规划最优控制方法相比的优势。]]></description>
      <guid>https://arxiv.org/abs/2412.01591</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>路径引导的基于粒子的采样</title>
      <link>https://arxiv.org/abs/2412.03312</link>
      <description><![CDATA[arXiv:2412.03312v1 公告类型：交叉 
摘要：基于粒子的贝叶斯推理方法通过从无分区目标（后验）分布中进行采样，例如 Stein 变分梯度下降 (SVGD)，引起了广泛关注。我们提出了一种基于路径引导的基于粒子的采样~(PGPS) 方法，该方法基于一种新的对数加权收缩 (LwS) 密度路径，将初始分布链接到目标分布。我们建议利用神经网络来学习由设计密度路径的 Fokker-Planck 方程激发的矢量场。从初始分布开始的粒子根据矢量场定义的常微分方程演化。这些粒子的分布沿着从初始分布到目标分布的密度路径引导。所提出的 LwS 密度路径允许在规范方法失败时有效搜索目标分布的模式。我们从理论上分析了由于近似和离散化误差导致的 PGPS 生成样本分布与目标分布的 Wasserstein 距离。实践上，与 SVGD 和 Langevin 动力学等基线相比，所提出的 PGPS-LwS 方法在模拟和真实世界贝叶斯学习任务的实验中表现出更高的贝叶斯推理精度和更好的校准能力。]]></description>
      <guid>https://arxiv.org/abs/2412.03312</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多路数据分析的可扩展贝叶斯张量环分解</title>
      <link>https://arxiv.org/abs/2412.03321</link>
      <description><![CDATA[arXiv:2412.03321v1 公告类型：交叉 
摘要：张量分解在与多路数据分析相关的众多应用中起着至关重要的作用。通过采用具有稀疏性诱导先验的贝叶斯框架，贝叶斯张量环 (BTR) 分解提供了概率估计和一种在学习过程中自动调整张量环秩的有效方法。然而，以前的 BTR 方法采用了自动相关性确定 (ARD) 先验，这可能导致次优解决方案。此外，它仅关注连续数据，而许多应用涉及离散数据。更重要的是，它依赖于坐标上升变分推理 (CAVI) 算法，该算法不足以处理具有大量观测的大张量。这些限制极大地限制了它的应用规模和范围，使其仅适用于小规模问题，例如图像/视频完成。为了解决这些问题，我们提出了一种新颖的 BTR 模型，该模型结合了非参数乘法伽马过程 (MGP) 先验，该先验以识别潜在结构的卓越准确性而闻名。为了处理离散数据，我们引入了 P\&#39;olya-Gamma 增强来进行闭式更新。此外，我们开发了一种高效的 Gibbs 采样器来进行一致的后验模拟，这将之前的 VI 算法的计算复杂度降低了两个阶，并且开发了一种可扩展到极大张量的在线 EM 算法。为了展示我们模型的优势，我们对模拟数据和实际应用进行了广泛的实验。]]></description>
      <guid>https://arxiv.org/abs/2412.03321</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>改进中位数估计的经典阴影</title>
      <link>https://arxiv.org/abs/2412.03381</link>
      <description><![CDATA[arXiv:2412.03381v1 公告类型：交叉 
摘要：Huang 等人引入的经典阴影协议 [Nat. Phys. 16, 1050 (2020)] 利用中位数 (MoM) 估计量，仅使用 $\mathcal{O}(\log(M/\delta))$ 测量值即可有效估计具有失败概率 $\delta$ 的 $M$ 个可观测量的期望值。在他们的分析中，Huang 等人为了简单起见，在他们的渐近性能界限中使用了松散的常数。然而，这些常数的具体值会显著影响实际实现中使用的镜头数量。为了解决这个问题，我们研究了 Minsker 提出的改进的 MoM 估计量 [PMLR 195, 5925 (2023)]，它使用最佳常数并涉及数据集的 U 统计量。为了高效估计，我们实现了两种不完全 U 统计估计器，第一种基于随机采样，第二种基于循环置换采样。我们比较了原始估计器和修改后的估计器与经典阴影协议一起使用时的性能，其中单量子比特 Clifford 幺正体（Pauli 测量）用于 Ising 自旋链，全局 Clifford 幺正体（Clifford 测量）用于 Greenberger-Horne-Zeilinger (GHZ) 状态。虽然原始估计器在 Pauli 测量方面优于修改后的估计器，但修改后的估计器在 Clifford 测量方面表现出比原始估计器更好的性能。我们的研究结果强调了根据特定测量设置定制估计器以优化经典阴影协议在实际应用中的性能的重要性。]]></description>
      <guid>https://arxiv.org/abs/2412.03381</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>局部平稳过程的 Wasserstein 距离界限</title>
      <link>https://arxiv.org/abs/2412.03414</link>
      <description><![CDATA[arXiv:2412.03414v1 公告类型：交叉 
摘要：局部平稳过程 (LSP) 为建模时变现象提供了一个强大的框架，允许统计特性（例如均值和方差）随时间平滑变化。在本文中，我们使用 Nadaraya-Watson (NW) 型估计量来估计 LSP 的条件概率分布。NW 估计量通过核平滑技术在给定协变量的情况下近似目标变量的条件分布。我们在 Wasserstein 距离下建立了单变量设置中 LSP 的 NW 条件概率估计量的收敛速度，并使用切片 Wasserstein 距离将此分析扩展到多变量情况。理论结果得到了对合成和真实世界数据集的数值实验的支持，证明了所提出的估计量的实际用途。]]></description>
      <guid>https://arxiv.org/abs/2412.03414</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于估计最佳动态治疗方案的惩罚共享参数算法</title>
      <link>https://arxiv.org/abs/2107.07875</link>
      <description><![CDATA[arXiv:2107.07875v3 公告类型：替换 
摘要：动态治疗方案 (DTR) 是一组决策规则，用于根据个人的病史为其提供个性化治疗。基于 Q 学习的 Q 共享算法已用于开发涉及跨干预的多个阶段共享的决策规则的 DTR。我们表明，由于在 Q 学习设置中使用线性模型，现有的 Q 共享算法可能会出现不收敛的情况，并确定 Q 共享失败的条件。我们开发了一种惩罚 Q 共享算法，该算法不仅在违反条件的设置中收敛，而且即使在满足条件的情况下也能胜过原始 Q 共享算法。我们在实际应用和几个综合模拟中为所提出的方法提供了证据。]]></description>
      <guid>https://arxiv.org/abs/2107.07875</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>现代网络的路径规范工具包：后果、前景和挑战</title>
      <link>https://arxiv.org/abs/2310.01225</link>
      <description><![CDATA[arXiv:2310.01225v5 公告类型：替换 
摘要：这项工作介绍了第一个围绕路径规范的工具包，它完全涵盖了具有偏差、跳过连接和基于顺序统计提取的任何操作的一般 DAG ReLU 网络：最大池化、GroupSort 等。该工具包特别允许我们为现代神经网络建立泛化界限，这些界限不仅是最广泛适用的基于路径规范的网络，而且还可以恢复或超越这种类型的已知最清晰的界限。这些扩展的路径规范进一步享受了路径规范的通常好处：易于计算、在网络对称性下的不变性以及与运算符规范的乘积（另一种最常用的复杂性度量）相比，分层全连接网络上的清晰度有所提高。
该工具包的多功能性及其易于实现的特性使我们能够通过在 ImageNet 上对 ResNets 的最清晰已知界限进行数值评估来挑战基于路径规范的泛化界限的具体承诺。]]></description>
      <guid>https://arxiv.org/abs/2310.01225</guid>
      <pubDate>Thu, 05 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>