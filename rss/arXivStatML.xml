<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 18 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>数据污染下的分裂共形预测</title>
      <link>https://arxiv.org/abs/2407.07700</link>
      <description><![CDATA[arXiv:2407.07700v2 公告类型：替换 
摘要：共形预测是一种非参数技术，用于在假设数据可交换的情况下从任意预测模型构建预测区间或集合。它之所以受欢迎，是因为它对预测集的边际覆盖率具有理论保证，并且与模型训练相比，分割共形预测变体的计算成本非常低。我们研究了数据污染设置中分割共形预测的稳健性，其中我们假设一小部分校准分数来自与大部分不同的分布。我们量化了在“干净”测试点上评估时损坏数据对构建集的覆盖率和效率的影响，并通过数值实验验证了我们的结果。此外，我们提出了一种分类设置调整，我们称之为污染稳健共形预测，并使用合成和真实数据集验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.07700</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:32 GMT</pubDate>
    </item>
    <item>
      <title>使用观察和干预数据表示特定情境的因果模型</title>
      <link>https://arxiv.org/abs/2101.09271</link>
      <description><![CDATA[arXiv:2101.09271v4 公告类型：replace-cross 
摘要：我们通过引入一种新的特定于上下文的条件独立模型系列（称为 CStrees）来解决基于在一般（例如硬或软）干预下收集的观察和实验数据来表示特定于上下文的因果模型的问题。该系列通过一种新颖的分解标准定义，该标准允许对定义一般干预 DAG 模型的分解属性进行泛化。我们推导出观察 CStrees 模型等价性的图形表征，该表征扩展了 DAG 的 Verma 和 Pearl 标准。然后将此表征扩展到一般、特定于上下文的干预下的 CStree 模型。为了获得这些结果，我们形式化了一种特定于上下文的干预概念，可以将其纳入 CStree 模型的简洁图形表示中。我们将 CStrees 与其他特定于上下文的模型联系起来，表明 DAG、CStrees、标记 DAG 和分阶段树的系列形成了严格的包含链。最后，我们将介入性 CStree 模型应用于真实数据集，揭示了数据依赖结构和软介入性扰动的上下文特定性质。]]></description>
      <guid>https://arxiv.org/abs/2101.09271</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:32 GMT</pubDate>
    </item>
    <item>
      <title>论多模态和单模态机器学习之间更强的计算分离</title>
      <link>https://arxiv.org/abs/2404.02254</link>
      <description><![CDATA[arXiv:2404.02254v2 公告类型：替换 
摘要：最近，多模态机器学习取得了巨大的经验成功（例如 GPT-4）。为了为这一经验成功提供理论依据，Lu（NeurIPS &#39;23，ALT &#39;24）引入了多模态学习理论，并考虑了多模态和单模态学习理论模型之间可能存在的 \textit{分离}。特别是，Lu（ALT &#39;24）展示了一种计算分离，这与学习任务的 \textit{最坏情况} 实例有关。在本文中，我们给出了更强的 \textit{平均情况} 计算分离，其中对于学习任务的“典型”实例，单模态学习在计算上很难，但多模态学习很容易。然后我们质疑平均情况分离有多“自然”。在实践中会遇到吗？为此，我们证明，在基本条件下，平均情况下单模态和多模态学习任务之间的任何给定计算分离都意味着相应的加密密钥协商协议。我们建议将此解释为证据，表明多模态学习的非常强大的\textit{计算}优势可能在实践中\textit{不常见}，因为它们仅存在于固有加密分布的“病态”情况下。然而，这并不适用于可能的（超多项式）\textit{统计}优势。]]></description>
      <guid>https://arxiv.org/abs/2404.02254</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:31 GMT</pubDate>
    </item>
    <item>
      <title>保持速率减少以实现 Blackwell 可达性</title>
      <link>https://arxiv.org/abs/2406.07585</link>
      <description><![CDATA[arXiv:2406.07585v2 公告类型：替换 
摘要：Abernethy 等人 (2011) 表明 Blackwell 可接近性和无遗憾学习是等价的，即任何解决特定 Blackwell 可接近性实例的算法都可以转换为特定无遗憾学习实例的亚线性遗憾算法，反之亦然。在本文中，我们研究了这种简化的更细粒度形式，并询问何时问题之间的这种转换不仅保持了亚线性收敛速度，而且还保持了最佳收敛速度。也就是说，在哪些情况下，只需找到无遗憾学习实例的最佳遗憾界限，即可找到相应可接近性实例的最佳收敛速度？
我们表明 Abernethy 等人的简化。 (2011) 不保留速率：它们的减少可能会将具有最佳收敛速率 $R_1$ 的 $d$ 维可接近性实例 $I_1$ 减少为具有最佳每轮遗憾率 $R_2$ 的无遗憾学习实例 $I_2$，其中 $R_{2}/R_{1}$ 任意大（特别是，有可能 $R_1 = 0$ 和 $R_{2} &gt; 0$）。另一方面，我们表明可以将任何可接近性实例紧密减少为广义遗憾最小化的实例，我们称之为不当 $\phi$-遗憾最小化（Gordon 等人 (2008) 的 $\phi$-遗憾最小化的变体，其中转换函数可能会将动作映射到动作集之外）。
最后，我们描述了线性变换何时足以以速率保持的方式将不恰当的$\phi$-遗憾最小化问题简化为标准遗憾最小化问题类。我们证明，一些不恰当的$\phi$-遗憾最小化实例不能以这种方式简化为实例的任一子类，这表明可接近性可以捕捉一些无法用在线学习语言表达的问题。]]></description>
      <guid>https://arxiv.org/abs/2406.07585</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:31 GMT</pubDate>
    </item>
    <item>
      <title>用于函数学习的分布式梯度下降</title>
      <link>https://arxiv.org/abs/2305.07408</link>
      <description><![CDATA[arXiv:2305.07408v2 公告类型：替换 
摘要：近年来，不同类型的分布式和并行学习方案因其在处理大规模数据信息方面的强大优势而受到越来越多的关注。在信息时代，为了应对最近源于函数数据分析的大数据挑战，我们提出了一种新颖的分布式梯度下降函数学习（DGDFL）算法，用于在再生核希尔伯特空间的框架内处理跨众多本地机器（处理器）的函数数据。基于积分算子方法，我们在文献的许多不同方面首次提供了对 DGDFL 算法的理论理解。在理解 DGDFL 的道路上，首先，提出了一种与单机模型相关的基于数据的梯度下降函数学习（GDFL）算法并进行了全面研究。在温和的条件下，获得了基于置信度的 DGDFL 最佳学习率，而没有先前在函数回归中的工作中遇到的规律性指数的饱和边界。我们进一步提供了一种半监督 DGDFL 方法来削弱对最大本地机器数量的限制，以确保最佳速率。据我们所知，DGDFL 为基于本质上无限维随机函数（函数协变量）的数据样本的函数学习提供了第一种分而治之的迭代训练方法，并丰富了函数数据分析的方法。]]></description>
      <guid>https://arxiv.org/abs/2305.07408</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:30 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的非空泛化界限</title>
      <link>https://arxiv.org/abs/2312.17173</link>
      <description><![CDATA[arXiv:2312.17173v3 公告类型：替换 
摘要：现代语言模型可以包含数十亿个参数，这引发了一个问题：它们是否可以在训练数据之外进行推广，还是仅仅重复其训练语料库。我们为预训练的大型语言模型 (LLM) 提供了第一个非空泛化界限，表明语言模型能够发现推广到未见数据的规律。具体而言，我们使用预测平滑推导出对无界对数似然损失有效的压缩界限，并且我们扩展了界限以处理子采样，从而将海量数据集上的界限计算速度提高了几个数量级。为了达到非空界限所需的极端压缩水平，我们设计了 SubLoRA，这是一种简单的低维非线性参数化，可为具有近十亿个参数的模型提供非空泛化界限。最后，我们使用我们的界限来理解 LLM 泛化，并发现较大的模型具有更好的泛化界限并且比较小的模型更具压缩性。]]></description>
      <guid>https://arxiv.org/abs/2312.17173</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:30 GMT</pubDate>
    </item>
    <item>
      <title>使用状态空间模型进行测试时间自适应</title>
      <link>https://arxiv.org/abs/2407.12492</link>
      <description><![CDATA[arXiv:2407.12492v1 公告类型：交叉 
摘要：在部署模型的生命周期中，训练数据和测试数据之间的分布变化几乎是不可避免的，并会导致性能下降。调整模型有望减轻这种性能下降。然而，调整具有挑战性，因为它必须是无监督的：我们通常在测试时无法访问任何标记数据。在本文中，我们提出了一个概率状态空间模型，可以调整受到分布漂移影响的部署模型。我们的模型学习由最后一组隐藏特征上的分布变化引起的动态。在不需要标签的情况下，我们推断出随时间演变的类原型，作为动态分类头。此外，我们的方法是轻量级的，只修改模型的最后一层线性层。在对现实世界分布变化和合成损坏的实验中，我们证明我们的方法与需要反向传播和访问模型主干的方法相比具有竞争力。我们的模型在小测试批次的情况下尤其出色 - 这是最困难的设置。]]></description>
      <guid>https://arxiv.org/abs/2407.12492</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:29 GMT</pubDate>
    </item>
    <item>
      <title>PL 不等式下自适应梯度法线性收敛的建立方法</title>
      <link>https://arxiv.org/abs/2407.12629</link>
      <description><![CDATA[arXiv:2407.12629v1 公告类型：交叉 
摘要：自适应梯度下降优化器是训练神经网络模型的标准选择。尽管自适应优化器的收敛速度比梯度下降更快，并且在实践中表现优异，但它们并不像普通梯度下降那样被人们所理解。原因之一是，有助于这些方法更快收敛的学习率的动态更新也使它们的分析变得复杂。特别是，对于一类优化问题，简单的梯度下降法以线性速率收敛，而实际上更快的自适应梯度法缺乏这样的理论保证。Polyak-{\L}ojasiewicz (PL) 不等式是已知的最弱的一类，对于该类，梯度下降及其动量变体的线性收敛已被证明。因此，在本文中，我们证明了当成本函数平滑且满足 PL 不等式时，两种著名的自适应梯度方法 AdaGrad 和 Adam 是线性收敛的。我们的理论框架遵循简单统一的方法，适用于批量和随机梯度，可以用于分析 Adam 其他变体的线性收敛。]]></description>
      <guid>https://arxiv.org/abs/2407.12629</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:29 GMT</pubDate>
    </item>
    <item>
      <title>抛物型偏微分方程的基本模型</title>
      <link>https://arxiv.org/abs/2407.12234</link>
      <description><![CDATA[arXiv:2407.12234v1 公告类型：交叉 
摘要：抛物线偏微分方程 (PDE) 出现在许多学科中，用于模拟各种数学对象的演变，例如概率流、控制理论中的价值函数和金融中的衍生品价格。通常需要在对应于该 PDE 的不同参数的多个场景中计算参数 PDE 的解或解的函数。此过程通常需要从头开始解决 PDE，这非常耗时。为了更好地利用现有的 PDE 模拟，我们提出了一个框架，通过元学习底层基础分布来寻找不同场景中抛物线 PDE 的解。我们在此基础分布的基础上提出了一种在不同参数设置下计算参数 PDE 解的方法。最后，我们通过生成建模、随机控制和金融中的大量实验说明了所提出方法的应用。实证结果表明，所提出的方法提高了在新的参数制度下求解 PDE 的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2407.12234</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>你为什么要 Grok？Grokking 模块化加法的理论分析</title>
      <link>https://arxiv.org/abs/2407.12332</link>
      <description><![CDATA[arXiv:2407.12332v1 公告类型：交叉 
摘要：我们提出了“grokking”现象的理论解释，即模型在过度拟合后很长时间内仍能推广，这是最初研究的模加问题。首先，我们表明，在梯度下降的早期，当“核机制”大致成立时，除非看到所有可能数据点的至少一个常数部分，否则没有置换等变模型可以在模加法上实现较小的总体误差。然而，最终模型会摆脱核机制。我们表明，在有界 $\ell_{\infty}$ 范数下实现零训练损失的两层二次网络在训练点明显较少的情况下可以很好地推广，并进一步表明这种网络存在，并且可以通过具有较小 $\ell_{\infty}$ 正则化的梯度下降找到。我们进一步提供经验证据，表明这些网络以及简单的 Transformers 仅在最初过度拟合后才会离开核机制。综上所述，我们的研究结果有力地支持了 grokking 的观点，因为它是从核函数行为到深度网络上梯度下降的限制行为的转变。]]></description>
      <guid>https://arxiv.org/abs/2407.12332</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>从自发叙述转录中检测特定语言障碍 (SLI) 的管道</title>
      <link>https://arxiv.org/abs/2407.12012</link>
      <description><![CDATA[arXiv:2407.12012v1 公告类型：交叉 
摘要：特定语言障碍 (SLI) 是一种影响交流的障碍，会影响理解和表达。本研究重点是使用来自 1063 次访谈的自发叙述记录有效地检测儿童中的 SLI。提出了一个三阶段级联管道。在第一阶段，使用随机森林 (RF) 和 Spearman 相关方法对数据进行特征提取和降维。在第二阶段，使用逻辑回归估计第一阶段最具预测性的变量，在最后阶段使用逻辑回归从自发叙述记录中检测儿童中的 SLI，使用最近邻分类器。结果显示识别 SLI 的准确率为 97.13%，突出了诸如回答的长度、话语的质量和语言的复杂性等方面。这种以自然语言处理为框架的新方法通过避免复杂的主观变量并专注于与儿童表现直接相关的定量指标，为 SLI 检测领域带来了显著的优势。]]></description>
      <guid>https://arxiv.org/abs/2407.12012</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>使用正则化 Wasserstein 距离对模拟输出分布进行凝聚聚类</title>
      <link>https://arxiv.org/abs/2407.12100</link>
      <description><![CDATA[arXiv:2407.12100v1 公告类型：交叉 
摘要：我们研究了聚类方法在随机模拟器生成的数据中的应用，并将其应用于异常检测、预优化和在线监控。我们引入了一种使用正则化 Wasserstein 距离对多元经验分布进行聚类的凝聚聚类算法，并将所提出的方法应用于呼叫中心模型。]]></description>
      <guid>https://arxiv.org/abs/2407.12100</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>线性回归模型是白盒且可解释的吗？</title>
      <link>https://arxiv.org/abs/2407.12177</link>
      <description><![CDATA[arXiv:2407.12177v1 公告类型：交叉 
摘要：可解释人工智能 (XAI) 是一组应用或嵌入机器学习模型以理解和解释模型的工具和算法。它们特别适用于复杂或高级模型，包括深度神经网络，因为它们从人类的角度来看是不可解释的。另一方面，包括线性回归在内的简单模型易于实现，计算复杂度较低，易于可视化输出。文献中的普遍观点是，包括线性回归在内的简单模型被视为“白盒”，因为它们更易于解释和理解。这是基于线性回归模型具有几个有利结果的想法，包括模型中特征的影响以及它们对模型输出产生积极或消极影响。此外，可以使用置信区间来测量或估计模型的不确定性。然而，我们认为这种看法并不准确，考虑到常见的 XAI 指标和可能面临的挑战，线性回归模型既不容易解释也不容易理解。这包括线性、局部解释、多重共线性、协变量、正则化、不确定性、特征贡献和公平性。因此，我们建议在可解释性和可解释性方面，所谓的简单模型应该与复杂模型同等对待。]]></description>
      <guid>https://arxiv.org/abs/2407.12177</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>机器学习的信息论基础</title>
      <link>https://arxiv.org/abs/2407.12288</link>
      <description><![CDATA[arXiv:2407.12288v1 公告类型：新
摘要：过去十年，机器学习的惊人进步令人瞩目。回想起来，这些里程碑是在几乎没有或根本没有严格理论指导实验的情况下实现的，这既令人惊叹又令人不安。尽管如此，从业者已经能够通过以前大规模实证调查的观察来指导他们未来的实验。然而，引用柏拉图的洞穴寓言，形成该领域现实概念的观察很可能只是代表现实碎片的影子。在这项工作中，我们提出了一个理论框架，试图回答洞穴外存在的问题。对于理论家来说，我们提供了一个数学上严谨的框架，为未来的探索留下了许多有趣的想法。对于实践者来说，我们提供了一个框架，其结果非常直观、通用，并将有助于形成指导未来调查的原则。具体而言，我们提供了一个以贝叶斯统计和香农信息论为基础的理论框架，该框架足够通用，可以统一机器学习中许多现象的分析。我们的框架描述了最佳贝叶斯学习器的性能，该学习器考虑了信息的基本限制。在整个工作过程中，我们得出了非常普遍的理论结果，并将它们应用于从在未知分布下独立且相同分布的数据到连续的数据，再到表现出适合元学习的层次结构的数据等设置的特定见解。我们最后用一节专门来描述错误指定算法的性能。这些结果令人兴奋，并且特别相关，因为我们努力克服这个无限复杂的世界中日益困难的机器学习挑战。]]></description>
      <guid>https://arxiv.org/abs/2407.12288</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:26 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯学习的可扩展蒙特卡洛</title>
      <link>https://arxiv.org/abs/2407.12751</link>
      <description><![CDATA[arXiv:2407.12751v1 公告类型：新
摘要：本书旨在为研究生提供马尔可夫链蒙特卡罗 (MCMC) 算法高级主题的介绍，该算法广泛应用于贝叶斯计算环境。这些主题中的大多数（如果不是全部的话）（随机梯度 MCMC、不可逆 MCMC、连续时间 MCMC 和新的收敛评估技术）都是最近十年才出现的，并推动了该领域近期的重大实践和理论进步。特别关注的是可扩展的数据量或数据维度的方法，这些方法的动机是机器学习和人工智能中新兴的高优先级应用领域。]]></description>
      <guid>https://arxiv.org/abs/2407.12751</guid>
      <pubDate>Thu, 18 Jul 2024 06:20:26 GMT</pubDate>
    </item>
    </channel>
</rss>