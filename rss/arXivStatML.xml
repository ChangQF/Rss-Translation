<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 06 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过预处理、中处理和后处理实现具有线性视差约束的贝叶斯最优公平分类</title>
      <link>https://arxiv.org/abs/2402.02817</link>
      <description><![CDATA[机器学习算法可能对受保护群体产生不同的影响。为了解决这个问题，我们开发了贝叶斯最优公平分类方法，旨在最大限度地减少给定群体公平约束下的分类错误。我们引入\emph{线性视差度量}的概念，它是概率分类器的线性函数；和 \emph{双线性视差测量}，它们在分组回归函数中也是线性的。我们证明了几种流行的不平等衡量标准——人口平等、机会平等和预测平等的偏差——是双线性的。
  通过揭示与内曼-皮尔逊引理的联系，我们在单个线性差异度量下找到了贝叶斯最优公平分类器的形式。对于双线性差异度量，贝叶斯最优公平分类器成为分组阈值规则。我们的方法还可以处理多个公平性约束（例如均等赔率），以及在预测阶段无法使用受保护属性时的常见场景。
  利用我们的理论结果，我们设计了在双线性视差约束下学习公平贝叶斯最优分类器的方法。我们的方法涵盖了三种流行的公平感知分类方法，即通过预处理（公平上采样和下采样）、处理中（公平成本敏感分类）和后处理（公平插件规则）。我们的方法直接控制差异，同时实现近乎最优的公平性与准确性权衡。我们凭经验表明，我们的方法与现有算法相比具有优势。]]></description>
      <guid>https://arxiv.org/abs/2402.02817</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:59 GMT</pubDate>
    </item>
    <item>
      <title>有偏自适应随机逼近的非渐近分析</title>
      <link>https://arxiv.org/abs/2402.02857</link>
      <description><![CDATA[具有自适应步骤的随机梯度下降（SGD）现在广泛用于训练深度神经网络。大多数理论结果都假设可以使用无偏梯度估计器，但在最近的几个使用蒙特卡罗方法的深度学习和强化学习应用中，情况并非如此。本文提供了对凸和非凸平滑函数具有偏置梯度和自适应步骤的 SGD 的全面非渐近分析。我们的研究结合了时间相关偏差，并强调了控制梯度估计器的偏差和均方误差（MSE）的重要性。特别是，我们确定具有偏置梯度的 Adagrad 和 RMSProp 收敛到平滑非凸函数的临界点，其速率与文献中针对无偏置情况的现有结果相似。最后，我们使用变分自动编码器（VAE）提供实验结果，说明我们的收敛结果，并展示如何通过适当的超参数调整来减少偏差的影响。]]></description>
      <guid>https://arxiv.org/abs/2402.02857</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:59 GMT</pubDate>
    </item>
    <item>
      <title>图神经机：表格数据学习的新模型</title>
      <link>https://arxiv.org/abs/2402.02862</link>
      <description><![CDATA[近年来，人们对将不同领域的数据映射到图结构越来越感兴趣。其中，诸如多层感知器（MLP）之类的神经网络模型可以建模为图。事实上，MLP 可以表示为有向无环图。图神经网络（GNN）最近已成为在图上执行机器学习任务的标准工具。在这项工作中，我们证明 MLP 相当于一个异步消息传递 GNN 模型，它在 MLP 的图表示上运行。然后，我们提出了一种新的表格数据机器学习模型，即所谓的图神经机（GNM），它用近乎完整的图取代了 MLP 的有向无环图，并采用了同步消息传递方案。我们证明单个 GNM 模型可以模拟多个 MLP 模型。我们在几个分类和回归数据集中评估所提出的模型。在大多数情况下，GNM 模型的性能优于 MLP 架构。]]></description>
      <guid>https://arxiv.org/abs/2402.02862</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:59 GMT</pubDate>
    </item>
    <item>
      <title>连续张量松弛寻找组合优化问题的多样化解决方案</title>
      <link>https://arxiv.org/abs/2402.02190</link>
      <description><![CDATA[寻找最佳解决方案是组合优化 (CO) 问题中最常见的目标。然而，单一解决方案可能并不适合实际场景，因为目标函数和约束只是原始现实情况的近似值。为了解决这个问题，寻找（i）“异构解决方案”，具有不同特征的多样化解决方案，以及（ii）“惩罚多样化解决方案”，即约束严重性的变化，是自然的方向。该策略提供了在后处理过程中选择合适解决方案的灵活性。然而，发现这些不同的解决方案比确定单一解决方案更具挑战性。为了克服这一挑战，本研究为基于无监督学习的 CO 求解器引入了连续张量松弛退火 (CTRA)。 CTRA 通过扩展连续松弛方法来同时解决各种问题，该方法将离散决策变量转换为连续张量。该方法通过相互交互找到异构且惩罚多样化的解决方案，其中一种解决方案的选择会影响其他选择。数值实验表明，CTRA 使基于 UL 的求解器能够比现有基于 UL 的求解器更快地找到异构且惩罚多样化的解决方案。此外，这些实验表明CTRA增强了探索能力。]]></description>
      <guid>https://arxiv.org/abs/2402.02190</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:58 GMT</pubDate>
    </item>
    <item>
      <title>论最小微量因子分析——老歌新调</title>
      <link>https://arxiv.org/abs/2402.02459</link>
      <description><![CDATA[主成分分析 (PCA) 和因子分析等降维方法是数据科学中许多问题的核心。然而，为具有显着异方差噪声的数据找到稳健的低维近似值存在严重且众所周知的挑战。本文介绍了最小微量因子分析 (MTFA) 的宽松版本，这是一种凸优化方法，其根源可以追溯到 1940 年 Ledermann 的工作。这种宽松对于不过度拟合异方差扰动特别有效，并解决了常见引用的 Heywood 案例因子分析和最近发现的现有光谱方法的“病态诅咒”。我们对所得低秩子空间的准确性以及所提出的计算该矩阵的算法的收敛速度提供了理论保证。我们与现有方法（包括 HeteroPCA、Lasso 和 Soft-Impute）建立了许多有趣的联系，以填补有关低秩矩阵估计的大量文献中的一个重要空白。数值实验将我们的结果与最近几个处理异方差噪声的建议进行比较。]]></description>
      <guid>https://arxiv.org/abs/2402.02459</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:58 GMT</pubDate>
    </item>
    <item>
      <title>加速贝叶斯优化中的前瞻：多级蒙特卡罗就是您所需要的</title>
      <link>https://arxiv.org/abs/2402.02111</link>
      <description><![CDATA[我们利用多级蒙特卡罗 (MLMC) 来提高涉及嵌套期望和最大化的多步前瞻贝叶斯优化 (BO) 方法的性能。对于嵌套操作，朴素蒙特卡罗的复杂度会降低，而 MLMC 能够针对此类问题实现典型的蒙特卡罗收敛率，与维度无关，并且无需任何平滑度假设。我们的理论研究重点是一步和两步前瞻采集函数的近似改进，但是，正如我们所讨论的，该方法可以通过多种方式进行推广，包括超出 BO 的范围。研究结果经过数值验证，MLMC 对 BO 的好处通过几个基准示例进行了说明。代码可在此处获取：https://github.com/Shangda-Yang/MLMCBO。]]></description>
      <guid>https://arxiv.org/abs/2402.02111</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:57 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯聚类有效性指数</title>
      <link>https://arxiv.org/abs/2402.02162</link>
      <description><![CDATA[选择聚类数量是应用聚类算法的关键过程之一。为了完成这项任务，引入了各种聚类有效性指数（CVI）。大多数聚类有效性指数被定义为检测数据集中隐藏的最佳聚类数量。然而，用户有时并不期望获得最佳的组数，而是期望获得对其应用程序更合理的次要组数。这促使我们引入基于现有基础指数的贝叶斯聚类有效性指数（BCVI）。该指数是基于狄利克雷或广义狄利克雷先验定义的，它们会产生相同的后验分布。然后，我们基于 Wiroonsri 指数 (WI) 和 Wiroonsri-Preedasawakul 指数 (WP) 分别作为硬聚类和软聚类的基础指数来测试我们的 BCVI。我们将它们的结果与原始基础指数以及其他一些现有 CVI 进行比较，包括 Davies 和 Bouldin (DB)、Starczewski (STR)、Xie 和 Beni (XB) 以及 KWON2 指数。当体验很重要时，我们提出的 BCVI 显然有利于 CVI 的使用，用户可以指定其最终集群数量的预期范围。我们的实验分为三种不同的情况，强调了这一点。最后，我们展示了一些在现实世界数据集（包括 MRI 脑肿瘤图像）中的应用。我们的工具将添加到最近开发的 R 包“UniversalCVI”的新版本中。]]></description>
      <guid>https://arxiv.org/abs/2402.02162</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:57 GMT</pubDate>
    </item>
    <item>
      <title>通过优化抽象对 Slate Bandit 策略进行离策略评估</title>
      <link>https://arxiv.org/abs/2402.02171</link>
      <description><![CDATA[我们研究了板岩上下文强盗问题中的离策略评估（OPE），其中策略选择称为板岩的多维动作。这个问题在推荐系统、搜索引擎、营销和医疗应用中普遍存在，然而，典型的逆倾向评分 (IPS) 估计器由于较大的动作空间而遭受很大的方差，这使得有效的 OPE 成为一项重大挑战。引入伪逆 (PI) 估计器是为了通过假设奖励函数中的线性来减轻方差问题，但这可能会导致显着的偏差，因为这种假设很难从观察到的数据中验证，并且经常被严重违反。为了解决以前估计器的局限性，我们开发了一种新型的 slate bandits OPE 估计器，称为 Latent IPS (LIPS)，它定义了低维 slate 抽象空间中的重要性权重，在该空间中我们优化 slate 抽象以最小化 slate 抽象的偏差和方差。 LIPS 以数据驱动的方式。通过这样做，LIPS 可以大幅减少 IPS 的方差，而无需对奖励函数结构（如线性）施加限制性假设。通过实证评估，我们证明 LIPS 大大优于现有的估计器，特别是在具有非线性奖励和大型石板空间的场景中。]]></description>
      <guid>https://arxiv.org/abs/2402.02171</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:57 GMT</pubDate>
    </item>
    <item>
      <title>结合 T 学习和 DR 学习：因果对比的 Oracle 高效估计框架</title>
      <link>https://arxiv.org/abs/2402.01972</link>
      <description><![CDATA[我们引入了高效的插件（EP）学习，这是一种用于估计异质因果对比的新颖框架，例如条件平均治疗效果和条件相对风险。 EP-学习框架享有与内曼正交学习策略（例如 DR-学习和 R-学习）相同的预言机效率，同时解决了它们的一些主要缺点，包括（i）它们的实际适用性可能会受到损失函数的阻碍非凸性； (ii) 由于逆概率加权和违反界限的伪结果，它们可能会出现性能不佳和不稳定的情况。为了避免这些缺点，EP-learner为因果对比构建了一个高效的群体风险函数插件估计器，从而继承了T-learning等插件估计策略的稳定性和鲁棒性。在合理的条件下，基于经验风险最小化的 EP 学习器是预言机有效的，表现出与人口风险函数的预言机有效一步偏偏估计量的最小化的渐近等价。在模拟实验中，我们证明了 EP 学习器的条件平均治疗效果和条件相对风险优于最先进的竞争对手，包括 T 学习器、R 学习器和 DR 学习器。我们的 R 包 hte3 中提供了所提出方法的开源实现。]]></description>
      <guid>https://arxiv.org/abs/2402.01972</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:56 GMT</pubDate>
    </item>
    <item>
      <title>$\alpha$-神经密度比估计的散度损失函数</title>
      <link>https://arxiv.org/abs/2402.02041</link>
      <description><![CDATA[最近，神经网络已经为密度比估计（DRE）（机器学习的一项基本技术）产生了最先进的结果。然而，现有方法存在由 DRE 损失函数引起的优化问题：Kullback-Leibler (KL) 散度的大样本要求、训练损失梯度消失以及损失函数梯度偏差。因此，本文提出了一种提供简洁实现和稳定优化的$\alpha$-divergence损失函数（$\alpha$-Div）。此外，还提出了所提出的损失函数的技术理由。实证证明了所提出的损失函数的稳定性，并研究了 DRE 任务的估计精度。此外，本研究提出了 DRE 的样本要求，使用所提出的损失函数以 $L_1$ 误差的上限为单位，这将维数灾难作为高维 DRE 任务中的常见问题联系起来。]]></description>
      <guid>https://arxiv.org/abs/2402.02041</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:56 GMT</pubDate>
    </item>
    <item>
      <title>当 QK 特征谱集中时，自注意力网络进行定位</title>
      <link>https://arxiv.org/abs/2402.02098</link>
      <description><![CDATA[自注意力机制在现代机器学习中盛行。它具有一个有趣的功能，即通过调节注意力定位程度从输入序列中自适应地选择标记，许多研究人员推测这是强大模型性能的基础，但使学习动态的潜在机制变得复杂。近年来，主要有两个论点将注意力定位与模型性能联系起来。一是等级崩溃，即自注意力块嵌入的令牌在不同令牌之间变得非常相似，导致网络表达能力降低。另一个是熵崩溃，其中注意力概率接近不均匀并导致低熵，使得学习动态更有可能陷入停滞状态。这两种故障模式显然可能相互矛盾，因为等级和熵崩溃分别与均匀和非均匀注意力相关。为此，我们通过查询关键参数矩阵的特征谱来表征注意力局部化的概念，并揭示小的特征谱方差会导致注意力局部化。有趣的是，小的特征谱方差可以防止秩和熵崩溃，从而提高模型的表达能力和可训练性。]]></description>
      <guid>https://arxiv.org/abs/2402.02098</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:56 GMT</pubDate>
    </item>
    <item>
      <title>用于端到端神经数据同化方案不确定性量化的 SPDE 先验</title>
      <link>https://arxiv.org/abs/2402.01855</link>
      <description><![CDATA[大型地球物理数据集的时空插值历来是通过最优插值 (OI) 和更复杂的基于模型或数据驱动的 DA 技术来解决的。在过去的十年中，随机偏微分方程（SPDE）和高斯马尔可夫随机场（GMRF）之间建立的联系开辟了一种在最佳插值中处理大型数据集和物理诱导协方差矩阵的新方法。深度学习社区的最新进展也能够通过嵌入数据同化变分框架的神经架构来解决这个问题。重建任务被视为涉及变分内部成本和后者基于梯度的最小化的先验的联合学习问题：先验模型和求解器都被描述为具有自动微分的神经网络，可以通过最小化损失来训练函数，通常表示为某些基本事实与重建之间的均方误差。在这项工作中，我们利用基于 SPDE 的高斯过程来估计复杂的先验模型，该模型能够处理空间和时间上的非平稳协方差，并为可解释性和不确定性量化提供随机框架。我们的神经变分方案经过修改，嵌入了一个增强状态公式，同时具有状态和 SPDE 参数化来估计。我们使用随机偏微分方程作为数据同化窗口的代理模型，而不是神经先验。训练涉及重建任务和 SPDE 先验模型的损失函数，其中训练涉及给定真实状态的 SPDE 参数的可能性。因为先验是随机的，所以我们可以在调节之前轻松地在先验分布中抽取样本，以提供一种灵活的方法来估计基于数千个成员的后验分布。]]></description>
      <guid>https://arxiv.org/abs/2402.01855</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:55 GMT</pubDate>
    </item>
    <item>
      <title>关于 f 散度原理的域适应：改进的框架</title>
      <link>https://arxiv.org/abs/2402.01887</link>
      <description><![CDATA[无监督域适应（UDA）在解决机器学习中的分布变化方面发挥着至关重要的作用。在这项工作中，我们完善了 Acuna 等人提出的 UDA 的理论基础。 （2021）通过改进基于 f 散度的差异并另外引入一种新的度量，f 域差异（f-DD）。通过去除绝对值函数并结合缩放参数，f-DD 产生新的目标误差和样本复杂度界限，使我们能够恢复之前基于 KL 的结果，并弥合 Acuna 等人提出的算法和理论之间的差距。 （2021）。利用定位技术，我们还开发了快速泛化边界。实证结果表明，基于 f-DD 的领域学习算法在流行的 UDA 基准测试中比以前的工作具有优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.01887</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Bellman 残差最小化进行分布式离策略评估</title>
      <link>https://arxiv.org/abs/2402.01900</link>
      <description><![CDATA[我们考虑分布式离策略评估问题，它是许多分布式强化学习（DRL）算法的基础。与大多数现有工作（依赖于上界扩展统计距离，例如上界-Wasserstein 距离）相比，我们研究了用于量化分布贝尔曼残差的期望扩展统计距离，并表明它可以上限估计的预期误差回报分配。基于这一吸引人的特性，通过将贝尔曼残差最小化框架扩展到DRL，我们提出了一种称为能量贝尔曼残差最小化器（EBRM）的方法来估计回报分布。我们在可实现性假设下为 EBRM 估计器建立了有限样本误差界限。此外，我们引入了基于多步引导过程的方法的变体，以实现多步扩展。通过选择适当的步骤级别，在某些不可实现性设置下，与单步骤 EBRM 相比，我们获得了该 EBRM 变体的更好的误差范围。最后，我们通过模拟研究与几种现有方法进行比较，证明了我们的方法的优越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.01900</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:55 GMT</pubDate>
    </item>
    <item>
      <title>近确定性回归中的错误指定不确定性</title>
      <link>https://arxiv.org/abs/2402.01810</link>
      <description><![CDATA[预期损失是模型泛化误差的上限，它允许稳健的 PAC-Bayes 学习界限。然而，众所周知，损失最小化会忽略错误指定，即模型无法准确再现观察结果。这导致大数据中参数不确定性的显着低估，或参数化不足的限制。我们分析了近确定性、错误指定和参数化不足的替代模型的泛化误差，这是一种在科学和工程中具有广泛相关性的机制。我们证明后验分布必须覆盖每个训练点，以避免发散的泛化误差，并导出遵守此约束的集合 {ansatz}，这对于线性模型来说会产生最小的开销。在应用于原子机器学习中的高维数据集之前，在模型问题上演示了有效的方法。错误指定导致的参数不确定性存在于参数化不足的限度内，从而可以准确预测和限制测试误差。]]></description>
      <guid>https://arxiv.org/abs/2402.01810</guid>
      <pubDate>Tue, 06 Feb 2024 15:12:54 GMT</pubDate>
    </item>
    </channel>
</rss>