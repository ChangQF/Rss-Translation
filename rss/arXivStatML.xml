<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Thu, 21 Dec 2023 06:17:31 GMT</lastBuildDate>
    <item>
      <title>通过无联合通用深度比较机器学习算法。 （arXiv：2312.12839v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12839</link>
      <description><![CDATA[我们提出了一个用于描述性分析偏序集的框架
基于深度函数的概念。尽管在线性方面进行了深入的研究
和度量空间，关于深度函数的讨论很少
非标准数据类型，例如偏序。我们引入了一个改编
众所周知的所有偏序集合的单纯深度，
无联合通用 (ufg) 深度。此外，我们利用 ufg 深度
基于多维度性能的机器学习算法比较
措施。具体来说，我们提供了两个分类器比较的例子
标准基准数据集的样本。我们的结果表明有希望
基于 UFG 方法的各种不同分析方法。
此外，这些例子概述了我们的方法与
现有的基准测试方法，从而为生动的方法添加了新的视角
关于分类器比较的争论。
]]></description>
      <guid>http://arxiv.org/abs/2312.12839</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>年鉴：语言模型可解释性的可模拟性基准。 （arXiv：2312.12747v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12747</link>
      <description><![CDATA[我们如何衡量语言模型可解释性方法的有效性？
虽然已经开发了许多可解释性方法，但它们通常是
对定制任务进行评估，避免进行同类比较。帮助
为了填补这一空白，我们推出了 ALMANACS，一种语言模型可解释性基准。
年鉴对可解释性方法的可模拟性进行评分，即，
解释改进了对新输入的行为预测。年鉴场景
涵盖十二个与安全相关的主题，例如道德推理和高级人工智能
行为；他们有特殊的前提来调用特定于模型的行为；
他们进行了训练-测试分配转变以鼓励忠实的
解释。通过使用另一种语言模型来预测行为
解释一下，ALMANACS 是一个完全自动化的基准测试。我们使用年鉴来
评估反事实、合理化、注意力和综合梯度
解释。我们的结果发人深省：对所有主题进行平均后，没有
解释方法优于无解释控制。我们的结论是
尽管之前的工作取得了一定的成功，但开发了一种解释方法
帮助年鉴中的可模拟性仍然是一个开放的挑战。
]]></description>
      <guid>http://arxiv.org/abs/2312.12747</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>用于使用不同数据集构建高维广义线性模型的异构迁移学习。 （arXiv：2312.12786v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2312.12786</link>
      <description><![CDATA[综合预测模型的开发常常引起人们的极大兴趣
许多科学学科，但包含所有所需信息的数据集
特征通常具有较小的样本量。在这篇文章中，我们描述了一个
用于构建高维广义线性的迁移学习方法
模型使用来自主要研究的数据，该研究包含所有方面的详细信息
预测因素，以及一项或多项外部研究，这些研究已经确定了更多
有限的预测变量集。我们建议使用外部数据集来构建
简化模型，然后传输有关底层参数的信息
通过一组校准方程对主要研究进行分析，同时
考虑某些设计变量的研究特定影响。然后我们
使用带有参数惩罚的广义矩法 (GMM)
估计并开发高度可扩展的算法来拟合模型
利用流行的 glmnet 包的优势。我们进一步表明，使用
自适应套索惩罚导致底层参数的预言属性
估计，从而导致方便的选择后推理过程。我们
进行广泛的模拟研究以调查预测性能
以及所提出方法的选择后推理属性。最后，我们
说明及时应用所提出的方法来开发
使用英国生物银行研究的五种常见疾病的风险预测模型，
结合所有研究参与者 (500K) 和最近的基线信息
发布了一个子集 (50K) 的高通量蛋白质组数据 (# Protein = 1500)
参与者们。
]]></description>
      <guid>http://arxiv.org/abs/2312.12786</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>使用混杂和选择偏向的离线数据稳健改进强盗算法：因果方法。 （arXiv：2312.12731v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12731</link>
      <description><![CDATA[本文研究了代理可以访问离线数据的强盗问题
这可能会被用来潜在地改进每个手臂的估计
奖励分配。这种设置的一个主要障碍是存在
观察数据的复合偏差。忽视这些偏见并盲目地
用有偏差的数据拟合模型甚至可能会对在线产生负面影响
学习阶段。在这项工作中，我们从因果关系来阐述这个问题
看法。首先，我们将偏差分为混杂偏差和
基于它们暗示的因果结构的选择偏差。接下来，我们提取
每个臂的因果界限对于有偏见的复合偏见是稳健的
观察数据。导出的边界包含基本事实平均奖励和
可以有效引导强盗智能体学习近乎最优的决策
政策。我们还对上下文和非上下文进行遗憾分析
强盗设置并表明先前的因果界限可以持续提供帮助
减少渐近遗憾。
]]></description>
      <guid>http://arxiv.org/abs/2312.12731</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>方差未知的两臂高斯老虎机中的局部最优固定预算最佳臂识别。 （arXiv：2312.12741v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12741</link>
      <description><![CDATA[我们以固定预算解决最佳手臂识别 (BAI) 问题
对于双臂高斯老虎机。在 BAI 中，给定多个臂，我们的目标是找到
最佳手臂，通过自适应获得最高预期奖励的手臂
实验。考夫曼等人。 (2016) 提出了概率的下界
错误识别最佳手臂。他们还提出了一项策略，假设
奖励的方差是已知的，并且表明它是渐近最优的
其错误识别概率与下限相符的感觉
预算接近无穷大。然而，渐近最优策略是
当方差未知时，未知。对于这个悬而未决的问题，我们提出了一个
估计适应性实验期间的方差并拉动手臂的策略
与估计标准差的比率。我们将此策略称为
奈曼分配 (NA)-增强逆概率加权 (AIPW)
战略。然后我们通过以下方式证明该策略是渐近最优的
表明其错误识别概率与下限相符
预算接近无穷大，并且两个人的预期回报之间的差距
武器接近于零（小差距制度）。我们的结果表明，在
以小差距制度为特征的最坏情况，我们的策略，
采用估计方差，即使方差是渐近最优的
未知。
]]></description>
      <guid>http://arxiv.org/abs/2312.12741</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>fMRI 数据的因果发现：挑战、解决方案和案例研究。 （arXiv：2312.12678v1 [q-bio.QM]）</title>
      <link>http://arxiv.org/abs/2312.12678</link>
      <description><![CDATA[设计应用因果发现的研究需要导航许多
研究者的自由度。当研究变得更加复杂时
涉及功能磁共振成像数据。在本文中，我们 (i) 描述了发生的九个挑战
将因果发现应用于功能磁共振成像数据时，(ii) 讨论
需要做出的决定，（iii）回顾最近的案例研究如何做出这些决定
决定，(iv) 并确定可能通过以下方式解决的现有差距：
新方法的开发。总的来说，因果发现是一个有前途的
分析功能磁共振成像数据的方法，以及多个成功的应用
表明它优于传统的功能磁共振成像功能连接
方法，但目前功能磁共振成像的因果发现方法为
改进。
]]></description>
      <guid>http://arxiv.org/abs/2312.12678</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>通过可解释性保证最大化集成的学习性能。 （arXiv：2312.12715v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.12715</link>
      <description><![CDATA[在本文中，我们提出了一种观测值最优分配的方法
本质上可解释的玻璃盒模型和黑盒模型之间。一个
最优分配被定义为对于任何给定的可解释性
水平（即可解释模型的观测值比例
预测函数），最大化集成的性能
底层任务，并最大化可解释模型的性能
分配给它的观测值，取决于最大的整体性能
健康）状况。所提出的方法被证明可以产生这种可解释性最佳的结果
跨各种类型的表格数据集基准套件的分配
可解释的和黑盒模型类型。发现这些学习分配
始终将集成性能保持在非常高的可解释性水平
（平均解释 $74\%$ 的观察结果），在某些情况下甚至
优于组件可解释模型和黑盒模型，同时
提高可解释性。
]]></description>
      <guid>http://arxiv.org/abs/2312.12715</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>多保真贝叶斯优化的长期行为。 （arXiv：2312.12633v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12633</link>
      <description><![CDATA[多保真贝叶斯优化 (MFBO) 已被证明通常可以
收敛速度比单保真贝叶斯优化 (SFBO) 更快 (Poloczek et
等人。 （2017））。受最近基准论文的启发，我们正在调查
MFBO 的长期行为，基于文献中的观察
在某些情况下可能表现不佳（Mikkola et al. (2023), Eggensperger
等人。 （2021））。从长远来看，MBFO 表现不佳可能会
极大地破坏了它在许多研究任务中的应用，特别是当
我们无法确定表现不佳何时开始。我们创建一个
简单的基准研究，展示实证结果并讨论场景和
表现不佳的可能原因。
]]></description>
      <guid>http://arxiv.org/abs/2312.12633</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>通过距离配置文件进行匹配。 （arXiv：2312.12641v1 [stat.ME]）</title>
      <link>http://arxiv.org/abs/2312.12641</link>
      <description><![CDATA[本文介绍并研究了基于距离的匹配方法
配置文件。对于点云的匹配，所提出的方法很容易
可以通过求解线性程序来实现，绕过计算
二次匹配的障碍。此外，我们提出并分析了一种灵活的方法
使用距离配置文件执行位置到位置匹配。此外，我们
提供统计估计误差分析
使用经验过程理论进行位置到位置匹配。此外，我们
将我们的方法应用于某个模型并通过以下方式显示其噪声稳定性
描述匹配成功的噪声水平条件。
最后，我们展示了所提出方法的性能并进行了比较
与一些使用合​​成数据和真实数据的现有方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.12641</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的凸景观：通过套索模型表征全局最优值和驻点。 （arXiv：2312.12657v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12657</link>
      <description><![CDATA[由于训练深度神经网络 (DNN) 模型的非凸性质，
它们的有效性依赖于非凸优化启发式的使用。
训练 DNN 的传统方法通常需要昂贵的经验方法来
产生成功的模型并且没有明确的理论基础。在
在本研究中，我们研究了凸优化理论和稀疏理论的使用
恢复模型来完善神经网络的训练过程并提供
更好地解释他们的最佳权重。我们重点训练两层
具有分段线性激活的神经网络，并证明它们可以
被表述为有限维凸规划。这些计划包括
促进稀疏性的正则化项，它构成了
组套索。我们首先利用半无限规划理论来证明强
有限宽度神经网络的对偶性，然后我们表达这些
架构相当于高维凸稀疏恢复模型。
值得注意的是，求解凸规划的最坏情况复杂度是多项式
在样本数和神经元数时对数据矩阵进行排序
是有界的，这就是卷积网络中的情况。扩展我们的方法
对于任意等级的训练数据，我们开发了一种新颖的多项式时间
基于区域子采样的近似方案，具有保证
近似比。我们还证明了非凸的所有平稳性
训练目标可以被描述为子采样的全局最优值
凸程序。我们的凸模型可以使用标准凸求解器进行训练
无需借助启发式方法或广泛的超参数调整，这与
非凸方法。通过大量的数值实验，我们表明
凸模型可以优于传统的非凸方法，但不是
对优化器超参数敏感。
]]></description>
      <guid>http://arxiv.org/abs/2312.12657</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>信任，但验证：随机平滑技术调查。 （arXiv：2312.12608v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12608</link>
      <description><![CDATA[机器学习模型在不同领域都取得了显着的成功
域，但仍然容易受到对抗性攻击。经验防御
由于新的攻击不断出现，机制常常存在不足，导致
现有的防御措施已经过时。从经验防御到
作为回应，我们观察到了基于认证的防御。随机化
平滑已成为一项有前途的技术，取得了显着的进步。这
研究回顾了理论基础、实证有效性和
随机平滑在验证机器学习分类器中的应用。
我们深入探讨了背后的基本概念
随机平滑，强调其在证明方面的理论保证
对抗性扰动的鲁棒性。此外，我们还讨论了
现有方法的挑战并提供富有洞察力的观点
潜在的解决方案。这篇论文的新颖之处在于它试图系统化
随机平滑背景下的现有知识。
]]></description>
      <guid>http://arxiv.org/abs/2312.12608</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>在线变分序列蒙特卡罗。 （arXiv：2312.12616v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2312.12616</link>
      <description><![CDATA[最经典的串行数据生成模型，状态空间模型
（SSM）是人工智能和统计机器学习的基础。在SSM中，任何形式
参数学习或潜在状态推断通常涉及
复杂潜在状态后验的计算。在这项工作中，我们建立在
变分序贯蒙特卡罗 (VSMC) 方法，提供
计算高效且准确的模型参数估计和贝叶斯
结合粒子方法和变分推理的潜在状态推理。
标准VSMC在离线模式下运行时，通过反复重新处理
给定一批数据，我们分布梯度的近似值
VSMC 使用随机近似及时替代 ELBO，允许在线
在存在数据流的情况下学习。这产生了一个算法，
在线 VSMC，能够高效、完全即时执行，
参数估计和粒子提议自适应。此外，我们
提供描述算法收敛性的严格理论结果
数据数量趋于无穷大时的属性以及数值
其出色的收敛特性和实用性的插图
批处理设置。
]]></description>
      <guid>http://arxiv.org/abs/2312.12616</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>使用部分动力学知识进行高效强化学习的示例。 （arXiv：2312.12558v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12558</link>
      <description><![CDATA[在线强化学习的样本复杂度问题经常出现
在文献中研究而不考虑任何片面知识
关于可能加速学习的系统动力学
过程。在本文中，我们研究了在线 Q-learning 的样本复杂度
当有关动力学的一些先验知识可用或可以时的方法
高效学习。我们专注于根据添加剂进化的系统
扰动模型的形式为 $S_{h+1} = f(S_h, A_h) + W_h$，其中 $f$
代表底层系统动力学，$W_h$ 是未知扰动
独立于状态和行动。在有限情景马尔可夫的设置下
具有 $S$ 状态、$A$ 操作和情节长度 $H$ 的决策过程，我们
提出一种乐观的 Q 学习算法，可以实现
$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$在完全了解的情况下后悔
$f$，其中$T$是与系统交互的总数。这是在
与典型的$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$遗憾相反
对于现有的 Q 学习方法。此外，如果只有 $\hat{f}$ 的噪声估计
$f$ 可用，我们的方法可以在
独立于状态和动作基数的样本数量
空间。次优差距取决于近似误差$\hat{f}-f$，
以及相应最优值函数的 Lipschitz 常数。
我们的方法不需要对转移概率进行建模
具有与无模型方法相同的内存复杂性。
]]></description>
      <guid>http://arxiv.org/abs/2312.12558</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>通过转换和增强不完美的训练数据实现强大的机器学习。 （arXiv：2312.12597v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12597</link>
      <description><![CDATA[机器学习 (ML) 是一种表达框架，用于将数据转化为
电脑程序。跨越许多问题领域——无论是行业还是政策
设置——准确预测或所需的计算机程序类型
最优控制很难手写。另一方面，收集
期望的系统行为的实例可能相对更可行。这
使机器学习具有广泛的吸引力，但也引起了数据敏感性，这通常
表现为部署期间意外的故障模式。从这个意义上说，
可用的训练数据往往不适合手头的任务。本论文
探讨了现代机器学习的几种数据敏感性以及如何
解决他们的问题。我们首先讨论如何防止 ML 预先编码
在训练数据中衡量人类歧视，我们采取公平的态度
表征学习方法。然后我们讨论学习的问题
包含虚假特征的数据，在过程中提供预测保真度
训练但部署后不可靠。在这里我们观察到，只要
标准的训练方法倾向于学习这样的特征，这种倾向可以是
用于搜索公开此信息的训练数据分区
不一致，最终促进学习算法对虚假不变
特征。最后，我们将注意力转向数据强化学习
没有充分覆盖所有可能的状态和行动。为了解决
覆盖问题，我们讨论如何使用因果先验来建模
收集数据的设置的单步动态。这使得
新型数据增强，将观察到的轨迹缝合在一起
产生新的但合理的反事实轨迹。
]]></description>
      <guid>http://arxiv.org/abs/2312.12597</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>输入凸神经网络的原则性权重初始化。 （arXiv：2312.12474v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.12474</link>
      <description><![CDATA[输入凸神经网络（ICNN）是保证凸性的网络
他们的输入-输出映射。这些网络已成功申请
基于能量的建模、最优运输问题和学习不变性。
ICNN 的凸性是通过使用非递减凸激活来实现的
函数和非负权重。由于这些特点，以前
隐含地假设中心权重的初始化策略不是
对 ICNN 有效。通过研究信号在各层中的传播
非负权重，我们能够导出原则性的权重初始化
对于 ICNN。具体来说，我们通过删除
假设权重是从中心分布中采样的。在一组
实验中，我们证明了我们的原则性初始化有效
加速 ICNN 的学习并带来更好的泛化能力。此外，我们
发现，与普遍看法相反，ICNN 无需训练即可训练
正确初始化时跳过连接。最后，我们将 ICNN 应用于
现实世界的药物发现任务并表明它们可以更有效
分子潜在空间探索。
]]></description>
      <guid>http://arxiv.org/abs/2312.12474</guid>
      <pubDate>Thu, 21 Dec 2023 06:17:24 GMT</pubDate>
    </item>
    </channel>
</rss>