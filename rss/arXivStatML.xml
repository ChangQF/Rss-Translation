<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 10 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>近确定性回归中的错误指定不确定性</title>
      <link>https://arxiv.org/abs/2402.01810</link>
      <description><![CDATA[arXiv:2402.01810v2 公告类型：替换
摘要：预期损失是模型泛化误差的上限，它允许稳健的 PAC-Bayes 边界进行学习。然而，众所周知，损失最小化会忽略错误指定，即模型无法准确再现观察结果。这导致大数据中参数不确定性的显着低估，或参数化不足的限制。我们分析了近确定性、错误指定和参数化不足的替代模型的泛化误差，这是一种在科学和工程中具有广泛相关性的机制。我们证明后验分布必须覆盖每个训练点，以避免发散的泛化误差，并导出遵守此约束的集合 \textit{ansatz}，这对于线性模型来说会产生最小的开销。在应用于原子机器学习中的高维数据集之前，在模型问题上演示了有效的方法。错误指定导致的参数不确定性存在于参数化不足的限度内，从而可以准确预测和限制测试误差。]]></description>
      <guid>https://arxiv.org/abs/2402.01810</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>通过逆条件排列的灵活公平学习</title>
      <link>https://arxiv.org/abs/2404.05678</link>
      <description><![CDATA[arXiv:2404.05678v2 公告类型：替换
摘要：均等赔率作为算法公平性的流行概念，旨在确保种族和性别等敏感变量在以真实结果为条件时不会不公平地影响算法预测。尽管取得了快速进展，但当前的大多数研究都集中在由一种敏感属性引起的均等赔率的违反，而同时考虑多个属性的挑战却没有得到充分解决。我们通过引入一种公平学习方法来解决这一差距，该方法将对抗性学习与新颖的逆条件排列相结合。这种方法有效且灵活地处理多个敏感属性，可能是混合数据类型。我们的方法的有效性和灵活性通过模拟研究和对现实世界数据集的实证分析得到了证明。]]></description>
      <guid>https://arxiv.org/abs/2404.05678</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>持续学习中的超参数选择</title>
      <link>https://arxiv.org/abs/2404.06466</link>
      <description><![CDATA[arXiv:2404.06466v1 公告类型：交叉
摘要：在持续学习（CL）中——学习者在数据流上进行训练——无法应用标准超参数优化（HPO），因为学习者无法同时访问所有数据。这促使了特定于 CL 的 HPO 框架的开发。在 CL 中调整超参数最流行的方法是使用不同的超参数设置对整个数据流进行重复训练。然而，这种训练结束 HPO 是不现实的，因为在实践中学习者只能看到流一次。因此，有一个悬而未决的问题：实践者应该使用什么 HPO 框架来解决现实中的 CL 问题？本文通过评估几个现实的 HPO 框架来回答这个问题。我们发现所有考虑的 HPO 框架（包括训练结束 HPO）的表现都相似。因此，我们提倡使用现实且计算效率最高的方法：在第一个任务上拟合超参数，然后在整个训练过程中修复它们。]]></description>
      <guid>https://arxiv.org/abs/2404.06466</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>神经网络模排列的同时线性连接</title>
      <link>https://arxiv.org/abs/2404.06498</link>
      <description><![CDATA[arXiv:2404.06498v1 公告类型：交叉
摘要：神经网络通常表现出排列对称性，这导致网络损失景观的非凸性，因为在训练网络的两个排列版本之间进行线性插值往往会遇到高损失障碍。最近的工作认为，排列对称性是非凸性的唯一来源，这意味着如果经过适当的排列，经过训练的网络之间基本上不存在此类障碍。在这项工作中，我们将这些论点提炼为三个不同的增强强度的主张。我们表明，现有证据仅支持“弱线性连接”——对于属于一组 SGD 解决方案的每对网络，存在（多个）排列将其与其他网络线性连接。相比之下，“强线性连接”的主张——对于每个网络，都存在一种排列，同时将其与其他网络连接——在直观上和实践上都更可取。这种更强有力的主张意味着，在考虑排列后，损失情况是凸的，并且可以在三个或更多独立训练的模型之间进行线性插值，而不会增加损失。在这项工作中，我们引入了一个中间主张——对于某些网络序列，存在一种排列可以同时对齐来自这些序列的匹配网络对。具体来说，我们发现单个排列对齐了迭代训练和迭代修剪网络的序列，这意味着两个网络分别在其优化和稀疏化轨迹的每一步都表现出低损失障碍。最后，我们通过证明在三个网络之间进行插值时障碍随着网络宽度的增加而减少，提供了第一个证据，证明在某些条件下强线性连接是可能的。]]></description>
      <guid>https://arxiv.org/abs/2404.06498</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>ExIFFI 和 EIF+：扩展扩展隔离林的可解释性和增强的通用性</title>
      <link>https://arxiv.org/abs/2310.05468</link>
      <description><![CDATA[arXiv:2310.05468v2 公告类型：替换
摘要：异常检测涉及识别复杂数据集和系统中的异常行为。虽然机器学习算法和决策支持系统 (DSS) 为这项任务提供了有效的解决方案，但在实际应用中，简单地查明异常可能是不够的。用户需要深入了解这些预测背后的基本原理，以促进根本原因分析并培养对模型的信任。然而，AD 的无监督性质给开发可解释的工具带来了挑战。本文通过引入 ExIFFI 来解决这一挑战，ExIFFI 是一种新颖的可解释性方法，专门用于解释扩展隔离森林所做的预测。 ExIFFI 利用特征重要性在全球和本地层面提供解释。这项工作还引入了 EIF+，它是扩展隔离森林的增强变体，旨在通过不同的分裂超平面设计策略来提高其泛化能力。进行了全面的比较分析，利用合成数据集和真实数据集来评估各种无监督 AD 方法。该分析证明了 ExIFFI 在为 AD 预测提供解释方面的有效性。此外，本文还探讨了 ExIFFI 作为无监督环境中特征选择技术的实用性。最后，这项工作通过提供开源代码、促进进一步的调查和可重复性为研究社区做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2310.05468</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>对局部高斯过程近似的进一步理解：表征有限域中的收敛性</title>
      <link>https://arxiv.org/abs/2404.06200</link>
      <description><![CDATA[arXiv:2404.06200v1 公告类型：交叉
摘要：我们表明，随着数据集大小 $n$ 的增加，高精度和大规模可扩展的基于最近邻的 GP 回归模型（GPnn：\cite{GPnn}）核函数的常见选择表现出逐渐收敛到渐近行为。对于各向同性核，例如 Mat\&#39;{e}rn 和平方指数，预测 MSE 的上限可以通过 $O(n^{-\frac{p}{d}})$ 获得，输入维度为 $ d$、$p$ 由内核决定（并且 $d&gt;p$）和固定数量的最近邻 $m$，并对输入分布进行最小假设。类似的界限可以在模型错误指定下找到，并结合起来给出 MSE 和重要校准指标的总体收敛率。我们表明，$n$ 的下限可以用 $m$、$l$、$p$、$d$、容差 $\varepsilon$ 和概率 $\delta$ 给出。当 $m$ 选择为 $O(n^{\frac{p}{p+d}})$ 时，可获得最小最大最优收敛速度。最后，我们展示了经验性能，并表明在许多情况下收敛速度比此处给出的上限更快。]]></description>
      <guid>https://arxiv.org/abs/2404.06200</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>使用保结构扩散模型生成量子态</title>
      <link>https://arxiv.org/abs/2404.06336</link>
      <description><![CDATA[arXiv:2404.06336v1 公告类型：交叉
摘要：本文考虑了量子系统状态的生成建模，并提出了一种基于去噪扩散模型的方法。关键贡献是尊重量子态物理性质的算法创新。更准确地说，混合态常用的密度矩阵表示必须是复值埃尔米特矩阵、正半定矩阵和迹一矩阵。即使所有训练数据都满足这些结构约束，通用扩散模型或其他生成方法可能无法生成严格满足这些结构约束的数据。为了开发一种具有物理硬连线的机器学习算法，我们利用镜像扩散模型的最新发展并设计了一个以前未考虑的镜像贴图，以实现严格的结构保留生成。实验证明，无条件生成和通过无分类器引导的条件生成都是有效的，后者甚至可以在看不见的标签上生成时设计新的量子态。]]></description>
      <guid>https://arxiv.org/abs/2404.06336</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>探索神经网络景观：星形和测地线连接</title>
      <link>https://arxiv.org/abs/2404.06391</link>
      <description><![CDATA[arXiv:2404.06391v1 公告类型：交叉
摘要：神经网络景观结构中最有趣的发现之一是模式连接现象：对于两个典型的全局最小值，存在一条无障碍连接它们的路径。这种模式连接的概念在理解深度学习中的重要现象方面发挥了至关重要的作用。
  在本文中，我们对这种连接现象进行了细粒度的分析。首先，我们证明在超参数化的情况下，连接路径可以像两段线性路径一样简单，并且路径长度可以几乎等于欧几里德距离。这一发现表明，在某种意义上，景观应该接近凸面。其次，我们发现了令人惊讶的星形连通性：对于有限数量的典型最小值，最小值流形上存在一个中心，通过线性路径同时连接所有它们。这些结果对于师生设置下的线性网络和两层 ReLU 网络来说是有效的，并且得到了在 MNIST 和 CIFAR-10 上训练的模型的经验支持。]]></description>
      <guid>https://arxiv.org/abs/2404.06391</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫网络最优 k 树拓扑的多项式时间推导</title>
      <link>https://arxiv.org/abs/2404.05991</link>
      <description><![CDATA[arXiv:2404.05991v1 公告类型：交叉
摘要：大型随机变量网络的联合概率分布特征仍然是数据科学中的一项具有挑战性的任务。实际上已经采用了具有简单拓扑的概率图近似；通常，树形拓扑使联合概率计算更加简单，并且可以有效地对不足的数据进行统计推断。然而，为了表征多个变量紧密合作以影响其他变量的网络组件，需要超越树的模型拓扑，但遗憾的是这是不可行的。特别是，我们之前的工作将树宽 k &gt;=2 的马尔可夫网络的最优近似与寻找最大生成 k 树 (MSkT) 的图论问题密切相关，这是一项被证明是棘手的任务。
  本文研究了具有 k 树拓扑的马尔可夫网络的最优近似，该拓扑保留了一些指定的底层子图。这样的子图可以编码科学应用中出现的某些背景信息，例如，关于基因网络中已知的重要途径或生物分子3D结构的残基相互作用图中不可或缺的主干连接。特别是，证明了 β 保留 MSkT 问题，对于图的多个类 β，对于每个固定的 k&gt;= 1，承认 O(n^{k+1}) 时间算法。这些 β -保留MSkT算法在需要保留某些持久信息的情况下为具有k树拓扑的马尔可夫网络的近似提供了有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2404.05991</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>适合上下文决斗强盗的感觉良好的 Thompson 采样</title>
      <link>https://arxiv.org/abs/2404.06013</link>
      <description><![CDATA[arXiv:2404.06013v1 公告类型：交叉
摘要：上下文决斗老虎机，其中学习者根据上下文比较两个选项并接收指示哪个选项是首选的反馈，通过合并用于决策和偏好学习的上下文信息来扩展经典决斗老虎机。已经针对线性上下文决斗老虎机提出了几种基于置信上限（UCB）的算法。然而，尽管在传统的上下文强盗中观察到了经验上的成功，但在这种情况下还没有开发出基于后验采样的算法。在本文中，我们提出了一种汤普森采样算法，名为 FGTS.CDB，用于线性上下文决斗强盗。我们算法的核心是一个新的“感觉良好”探索术语，专门为决斗强盗量身定制。该项利用了两个选定组的独立性，从而避免了分析中的交叉项。我们证明我们的算法实现了近乎最小最大最优的遗憾，即 $\tilde{\mathcal{O}}(d\sqrt T)$，其中 $d$ 是模型维度，$T$ 是时间范围。最后，我们在合成数据上评估我们的算法，并观察到 ​​FGTS.CDB 大幅优于现有算法。]]></description>
      <guid>https://arxiv.org/abs/2404.06013</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>使用傅里叶神经算子简化海洋动力学建模：多目标超参数和架构优化方法</title>
      <link>https://arxiv.org/abs/2404.05768</link>
      <description><![CDATA[arXiv:2404.05768v1 公告类型：交叉
摘要：训练有效的深度学习模型来学习海洋过程需要仔细选择各种超参数。我们利用 DeepHyper 的高级搜索算法进行多目标优化，简化为海洋建模量身定制的神经网络的开发。重点是优化傅里叶神经算子（FNO），这是一种能够模拟复杂海洋行为的数据驱动模型。选择正确的模型和调整超参数是一项具有挑战性的任务，需要付出很大的努力才能确保模型的准确性。 DeepHyper 允许高效探索与数据预处理相关的超参数、FNO 架构相关的超参数以及各种模型训练策略。我们的目标是获得一组最佳超参数，从而获得性能最佳的模型。此外，除了模型训练常用的均方误差之外，我们建议采用负异常相关系数作为附加损失项，以提高模型性能并研究两项之间的潜在权衡。实验结果表明，最佳超参数集增强了单时间步长预测中的模型性能，并大大超过了长达 30 天的长期预测的自回归部署中的基线配置。利用 DeepHyper，我们展示了一种增强 FNO 在海洋动力学预报中使用的方法，提供了精度更高的可扩展解决方案。]]></description>
      <guid>https://arxiv.org/abs/2404.05768</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>负面偏好优化：从灾难性崩溃到有效遗忘</title>
      <link>https://arxiv.org/abs/2404.05868</link>
      <description><![CDATA[arXiv:2404.05868v1 公告类型：交叉
摘要：大型语言模型（LLM）通常在预训练期间记住敏感、私人或受版权保护的数据。 LLM 去学习旨在消除预训练模型中不需要的数据的影响，同时保留模型对其他任务的效用。最近提出了几种用于 LLM 去学习的实用方法，主要基于丢失不需要的数据的梯度上升（GA）。然而，在某些遗忘任务中，这些方法要么无法有效地遗忘目标数据，要么遭受灾难性崩溃——模型效用急剧下降。
  在本文中，我们提出了负偏好优化（NPO），这是一种简单的对齐启发方法，可以高效且有效地忘却目标数据集。我们从理论上表明，通过最小化 NPO 损失而导致灾难性崩溃的进程比 GA 慢得多。通过对合成数据和基准 TOFU 数据集的实验，我们证明基于 NPO 的方法在消除不需要的数据和维护模型的实用性之间实现了更好的平衡。我们还观察到，基于 NPO 的方法比基于 GA 的方法产生更合理的输出，后者的输出通常是无意义的。值得注意的是，在 TOFU 上，基于 NPO 的方法是第一个在忘记 50%（或更多）训练数据方面取得合理的忘却结果的方法，而现有方法已经很难忘记 10% 的训练数据。]]></description>
      <guid>https://arxiv.org/abs/2404.05868</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>使用深度强化学习计算罕见事件研究的过渡路径</title>
      <link>https://arxiv.org/abs/2404.05905</link>
      <description><![CDATA[arXiv:2404.05905v1 公告类型：交叉
摘要：理解复杂系统中亚稳态之间的转变事件是计算物理、化学和生物学领域的重要课题。转变途径在表征转变机制方面发挥着重要作用，例如在生物分子构象变化的研究中。事实上，对于复杂的高维系统来说，计算过渡路径是一项具有挑战性的任务。在这项工作中，我们将寻路任务表述为特定路径空间上的成本最小化问题。成本函数改编自 Freidlin-Wentzell 动作函数，因此能够处理粗糙的潜在景观。然后使用基于深度确定性策略梯度算法（DDPG）的行动者批评家方法来解决路径寻找问题。该方法将系统的潜在力量纳入生成事件的策略中，并将系统的物理特性与分子系统的学习过程相结合。强化学习的利用和探索性质使该方法能够有效地对转换事件进行采样并计算全局最优转换路径。我们使用三个基准系统（包括扩展的 Mueller 系统和七个粒子的 Lennard-Jones 系统）说明了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.05905</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>即兴发挥：马尔可夫序列中缺失质量的最优估计</title>
      <link>https://arxiv.org/abs/2404.05819</link>
      <description><![CDATA[arXiv:2404.05819v1 公告类型：新
摘要：我们研究估计静止质量（也称为一元质量）的问题，该质量是离散时间遍历马尔可夫链的单个轨迹中缺失的。这个问题有多种应用——例如，估计固定缺失质量对于准确平滑序列模型中的概率估计至关重要。虽然 20 世纪 50 年代的经典 Good--Turing 估计器对于 i.i.d. 具有吸引人的特性。数据，已知它在马尔可夫设置中存在偏差，并且其他启发式估计器没有配备保证。在状态空间的大小可能远大于轨迹长度 $n$ 的一般设置中，我们开发了一个称为 \emph{Windowed Good--Turing} (\textsc{WingIt} ）并表明其风险随着 $\widetilde{\mathcal{O}}(\mathsf{T_{mix}}/n)$ 衰减，其中 $\mathsf{T_{mix}}$ 表示链的混合时间总变异距离。值得注意的是，该速率与状态空间的大小无关，并且在 $n / \mathsf{T_{mix}}$ 中的对数因子下是最小最大最优。我们还提出了缺失质量随机变量方差的界限，这可能是独立的兴趣。我们扩展我们的估计器来近似放置在 $X^n$ 中以小频率出现的元素上的固定质量。最后，我们展示了我们的估计器在规范链和由流行自然语言语料库构建的序列的模拟中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.05819</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>常步长非光滑收缩SA的预极限耦合与稳态收敛</title>
      <link>https://arxiv.org/abs/2404.06023</link>
      <description><![CDATA[arXiv:2404.06023v1 公告类型：新
摘要：在 Q 学习的推动下，我们研究了具有恒定步长的非光滑收缩随机逼近（SA）。我们关注两类重要的动力学：1）具有加性噪声的非平滑收缩SA，2）同步和异步Q学习，其特征是加性和乘性噪声。对于这两种动力学，我们建立了迭代到 Wasserstein 距离的稳态极限分布的弱收敛。此外，我们提出了一种用于建立稳态收敛的预极限耦合技术，并描述了步长趋近于零时稳态分布的极限。利用这个结果，我们得出非平滑 SA 的渐近偏差与步长的平方根成正比，这与平滑 SA 形成鲜明对比。这种偏差表征允许使用 Richardson-Romberg 外推来减少非平滑 SA 中的偏差。]]></description>
      <guid>https://arxiv.org/abs/2404.06023</guid>
      <pubDate>Wed, 10 Apr 2024 06:17:22 GMT</pubDate>
    </item>
    </channel>
</rss>