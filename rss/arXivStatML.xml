<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 19 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过协作扩散恢复可能性学习基于能量的模型</title>
      <link>https://arxiv.org/abs/2309.05153</link>
      <description><![CDATA[arXiv:2309.05153v4 公告类型：替换
摘要：在高维数据上训练基于能量的模型（EBM）既具有挑战性又耗时，并且 EBM 与 GAN 和扩散模型等其他生成框架之间的样本质量存在明显差距。为了缩小这一差距，受到最近通过最大化扩散恢复可能性（DRL）来学习 EBM 的努力的启发，我们提出了合作扩散恢复可能性（CDRL），这是一种从在噪声越来越大的版本上定义的一系列 EBM 中轻松学习和采样的有效方法数据集，与每个 EBM 的初始值设定项模型配对。在每个噪声级别，这两个模型在协作训练框架内进行联合估计：来自初始化器的样本作为起点，通过来自 EBM 的一些 MCMC 采样步骤进行细化。然后通过最大化恢复可能性来优化 EBM，同时通过学习精炼样本和初始样本之间的差异来优化初始化模型。此外，我们对EBM训练做了一些实用的设计，以进一步提高样本质量。结合这些进步，与 CIFAR-10 和 ImageNet 数据集上的现有 EBM 方法相比，我们的方法显着提高了生成性能。我们还展示了我们的模型对于几个下游任务的有效性，包括无分类器引导生成、组合生成、图像修复和分布外检测。]]></description>
      <guid>https://arxiv.org/abs/2309.05153</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:41 GMT</pubDate>
    </item>
    <item>
      <title>广义薛定谔电桥匹配</title>
      <link>https://arxiv.org/abs/2310.02233</link>
      <description><![CDATA[arXiv:2310.02233v2 公告类型：替换
摘要：用于训练扩散或流动模型的现代分布匹配算法直接规定了两个边界分布之间的边缘分布的时间演化。在这项工作中，我们考虑广义分布匹配设置，其中这些边际仅隐式描述为某些特定于任务的目标函数的解决方案。该问题设置被称为广义 Schr\&quot;odinger 桥 (GSB)，普遍出现在机器学习内外的许多科学领域中。我们提出了广义 Schr\&quot;odinger 桥匹配 (GSBM)，这是一种受最近的启发的新匹配算法进步，将它们推广到动能最小化之外，并考虑特定任务的国家成本。我们表明，这种推广可以被视为求解条件随机最优控制，为此可以使用有效的变分近似，并借助路径积分理论进一步消除偏差。与解决 GSB 问题的现有方法相比，我们的 GSBM 算法在整个训练过程中更好地保​​留了边界分布之间的可行传输图，从而实现了稳定的收敛并显着提高了可扩展性。我们通过一系列广泛的实验设置实证验证了我们的主张，包括人群导航、意见去极化、激光雷达流形和图像域传输。我们的工作为训练扩散模型带来了新的算法机会，并通过特定任务的最优结构进行了增强。代码可在 https://github.com/facebookresearch/generalized-schrodinger-bridge-matching 获取]]></description>
      <guid>https://arxiv.org/abs/2310.02233</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:41 GMT</pubDate>
    </item>
    <item>
      <title>自信特征排名</title>
      <link>https://arxiv.org/abs/2307.15361</link>
      <description><![CDATA[arXiv:2307.15361v2 公告类型：替换
摘要：机器学习模型广泛应用于各个领域。利益相关者经常使用事后特征重要性方法来更好地理解输入特征对模型预测的贡献。这些方法提供的重要性值的解释通常基于特征的相对顺序（它们的排名）而不是重要性值本身。由于顺序可能不稳定，我们提出了一个量化全局重要性值的不确定性的框架。我们提出了一种基于特征重要性值的框架和成对比较的特征重要性值的事后解释的新方法。该方法为特征的排名生成同时置信区间，其中包括高概率的“真实”（无限样本）排名，并能够选择前 k 个重要特征的集合。]]></description>
      <guid>https://arxiv.org/abs/2307.15361</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>使用基于注意力平面归一化流的变分自动编码器进行物理集成生成建模</title>
      <link>https://arxiv.org/abs/2404.12267</link>
      <description><![CDATA[arXiv:2404.12267v1 公告类型：交叉
摘要：物理集成生成建模是一类混合或灰盒建模，其中我们用控制数据分布的物理知识来增强数据驱动模型。物理知识的使用使得生成模型能够以受控的方式产生输出，从而使输出通过构造符合物理定律。它提高了推断训练分布之外的泛化能力，并提高了可解释性，因为该模型部分基于坚实的领域知识。在这项工作中，我们的目标是提高物理集成生成模型中重建的保真度和对噪声的鲁棒性。为此，我们使用变分自动编码器作为生成模型。为了改善解码器的重建结果，我们建议使用平面归一化流来学习物理以及可训练数据驱动组件的潜在后验分布。基于归一化流的后验分布利用了数据分布的固有动态结构，因此学习的模型更接近真实的底层数据分布。为了提高生成模型针对模型中注入的噪声的鲁棒性，我们提出了对基于归一化流的 VAE 的编码器部分的修改。我们设计的编码器将基于缩放点积注意力的上下文信息合并到噪声潜在向量中，这将减轻潜在向量中噪声的不利影响并使模型更加鲁棒。我们在人体运动数据集上对我们的模型进行了实证评估[33]，结果验证了我们提出的模型在重建质量改进以及对模型中注入噪声的鲁棒性方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.12267</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>神经梯度下降-上升的平均场分析：在函数条件矩方程中的应用</title>
      <link>https://arxiv.org/abs/2404.12312</link>
      <description><![CDATA[arXiv:2404.12312v1 公告类型：交叉
摘要：我们研究无限维函数类上定义的极小极大优化问题。特别是，我们将函数限制为超参数化两层神经网络类别，并研究（i）梯度下降-上升算法的收敛性和（ii）神经网络的表示学习。作为第一步，我们考虑源自通过对抗性估计估计由条件期望定义的函数方程的极小极大优化问题，其中目标函数在函数空间中是二次的。对于这个问题，我们通过考虑优化动力学的连续时间和无限宽度限制，在平均场体系下建立收敛。在这种情况下，梯度下降-上升对应于在神经网络参数空间上定义的概率测量空间上的 Wasserstein 梯度流。我们证明 Wasserstein 梯度流以 $\mathcal{O}(T^{-1} + \alpha^{-1} ) $ 次线性速率全局收敛到极小极大目标的驻点，并另外找到了解当极小极大目标的正则化器是强凸的时，函数方程。这里$T$表示时间，$\alpha$是神经网络的缩放参数。在表示学习方面，我们的结果表明，神经网络产生的特征表示允许与初始特征表示偏离 $\mathcal{O}(\alpha^{-1})$ 的量级，用以下术语衡量Wasserstein 距离。最后，我们将一般结果应用到具体例子中，包括政策评估、非参数工具变量回归和资产定价。]]></description>
      <guid>https://arxiv.org/abs/2404.12312</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>使用随机梯度下降匹配 k 稀疏奇偶校验问题的统计查询下界</title>
      <link>https://arxiv.org/abs/2404.12376</link>
      <description><![CDATA[arXiv:2404.12376v1 公告类型：交叉
摘要：$k$奇偶校验问题是计算复杂性和算法理论中的经典问题，是理解计算类的关键基准。在本文中，我们在两层全连接神经网络上使用随机梯度下降（SGD）解决了 $k$ 奇偶校验问题。我们证明 SGD 可以有效地解决 $d$ 维超立方体上的 $k$ 稀疏奇偶校验问题 ($k\le O(\sqrt{d})$)，样本复杂度为 $\tilde{O}( d^{k-1})$ 使用 $2^{\Theta(k)}$ 神经元，从而匹配已建立的统计查询 (SQ) 模型的 $\Omega(d^{k})$ 下界。我们的理论分析首先构建一个能够正确解决 $k$ 奇偶校验问题的良好神经网络。然后，我们演示使用 SGD 训练的神经网络如何有效地逼近这个良好的网络，以较小的统计误差解决 $k$ 奇偶校验问题。我们的理论结果和发现得到了经验证据的支持，展示了我们方法的效率和功效。]]></description>
      <guid>https://arxiv.org/abs/2404.12376</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>高维贝叶斯优化的预期坐标改进</title>
      <link>https://arxiv.org/abs/2404.11917</link>
      <description><![CDATA[arXiv:2404.11917v1 公告类型：交叉
摘要：贝叶斯优化（BO）算法在解决低维昂贵优化问题方面非常流行。将贝叶斯优化扩展到高维度是一项有意义但具有挑战性的任务。主要挑战之一是很难找到好的填充解决方案，因为采集函数也是高维的。在这项工作中，我们提出了高维贝叶斯优化的预期坐标改进（ECI）标准。所提出的 ECI 标准衡量了通过沿一个坐标移动当前最佳解决方案可以获得的潜在改进。该方法在每次迭代中选择具有最高 ECI 值的坐标进行细化，并通过迭代坐标逐渐覆盖所有坐标。所提出的ECI-BO（基于贝叶斯优化的预期坐标改进）算法相对于标准BO算法的最大优点是所提出算法的填充选择问题始终是一维问题，因此可以很容易地解决。数值实验表明，所提出的算法可以取得明显优于标准BO算法的结果，并且与五种最先进的高维BO相比具有竞争性的结果。这项工作为高维贝叶斯优化提供了一种简单但有效的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.11917</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>走向可观测算子模型的近似理论</title>
      <link>https://arxiv.org/abs/2404.12070</link>
      <description><![CDATA[arXiv:2404.12070v1 公告类型：交叉
摘要：可观察算子模型（OOM）为随机过程建模提供了强大的框架，在通用性和效率上超越了传统的隐马尔可夫模型（HMM）。然而，使用 OOM 来模拟无限维过程提出了重大的理论挑战。本文探讨了一种严格的方法来开发无限维过程 OOM 的近似理论。基于未发表的教程 [Jae98] 中概述的基础工作，严格建立了未来分布空间的内积结构，并证明了可观察算子相对于相关 2-范数的连续性。本文证明的原始定理描述了将未来分布的无限维空间转变为希尔伯特空间的基本障碍。所提出的发现为未来研究近似无限维过程的可观察算子奠定了基础，同时提出了解决所遇到障碍的方法。]]></description>
      <guid>https://arxiv.org/abs/2404.12070</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>用适当的评分规则量化任意和认知的不确定性</title>
      <link>https://arxiv.org/abs/2404.12215</link>
      <description><![CDATA[arXiv:2404.12215v1 公告类型：交叉
摘要：不确定性表示和量化在机器学习中至关重要，是安全关键型应用的重要先决条件。在本文中，我们提出了基于适当的评分规则来量化任意和认知不确定性的新方法，这些规则是具有有意义属性的损失函数，可以激励学习者预测真实（条件）概率。我们假设（认知）不确定性的两种常见表示，即根据信用集（即一组概率分布）或二阶分布（即概率分布上的分布）来表示。我们的框架在这​​些表示之间建立了一座天然的桥梁。我们为我们的方法提供了正式的论证，并引入了新的认知和任意不确定性测量作为具体实例。]]></description>
      <guid>https://arxiv.org/abs/2404.12215</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>通过概率提升实现通用批量贝叶斯优化的求积方法</title>
      <link>https://arxiv.org/abs/2404.12219</link>
      <description><![CDATA[arXiv:2404.12219v1 公告类型：交叉
摘要：贝叶斯优化中的并行化是一种常见策略，但面临一些挑战：采集函数和内核选择的灵活性、同时处理离散和连续变量的灵活性、模型错误指定以及最后的快速大规模并行化。为了应对这些挑战，我们引入了一种通用的模块化框架，通过内核求积的概率提升来进行批量贝叶斯优化，称为 SOBER，我们将其作为基于 GPyTorch/BoTorch 的 Python 库提供。我们的框架具有以下独特优势：（1）统一方法下下游任务的多功能性。 （2）无梯度采样器，不需要采集函数的梯度，提供与域无关的采样（例如，离散和混合变量、非欧几里德空间）。 (3)域优先分配的灵活性。 (4) 自适应批量大小（自主确定最佳批量大小）。 (5) 针对错误指定的再生核希尔伯特空间的鲁棒性。 (6)自然停止准则。]]></description>
      <guid>https://arxiv.org/abs/2404.12219</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>floZ：使用归一化流对后验样本进行证据估计</title>
      <link>https://arxiv.org/abs/2404.12294</link>
      <description><![CDATA[arXiv:2404.12294v1 公告类型：新
摘要：我们提出了一种基于归一化流的新方法（floZ），用于从非归一化后验分布中抽取的一组样本中估计贝叶斯证据（及其数值不确定性）。我们在证据已知的分布（最多 15 个参数空间维度）上对其进行验证，并与两种用于估计证据的最先进技术进行比较：嵌套采样（计算证据作为其主要目标）和 k - 最近邻技术，从后验样本中产生证据估计。如果目标后验的代表性样本可用，我们的方法对于具有尖锐特征的后验分布更加稳健，尤其是在更高维度上。它具有广泛的适用性，例如，可以估计来自变分推理、马尔可夫链蒙特卡罗样本或任何其他从非归一化后验密度提供样本的方法的证据。]]></description>
      <guid>https://arxiv.org/abs/2404.12294</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>通过基于共形的图稀疏化提高 GNN 预测的可解释性</title>
      <link>https://arxiv.org/abs/2404.12356</link>
      <description><![CDATA[arXiv:2404.12356v1 公告类型：新
摘要：图神经网络（GNN）在解决图分类任务方面取得了最先进的性能。然而，大多数 GNN 架构都会聚合来自图中所有节点和边的信息，无论它们与当前任务的相关性如何，从而阻碍了其预测的可解释性。与之前的工作相比，在本文中，我们提出了一种 GNN \emph{training} 方法，该方法联合 i) 通过删除边缘和/或节点来找到最具预测性的子图 -- -\emph{无需对子图结构做出假设} - - 同时 ii) 优化图分类任务的性能。为此，我们依靠强化学习来解决由此产生的双层优化，并使用基于保角预测的奖励函数来解决分类器当前训练中的不确定性。我们对九个不同图分类数据集的实证结果表明，我们的方法在性能上与基线竞争，同时依赖于显着稀疏的子图，从而产生更可解释的基于 GNN 的预测。]]></description>
      <guid>https://arxiv.org/abs/2404.12356</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签分类的深度依赖网络和高级推理方案</title>
      <link>https://arxiv.org/abs/2404.11667</link>
      <description><![CDATA[arXiv:2404.11667v1 公告类型：交叉
摘要：我们提出了一个称为深度依赖网络（DDN）的统一框架，它将依赖网络和深度学习架构结合起来进行多标签分类，特别强调图像和视频数据。与马尔可夫网络等其他概率图模型相比，依赖网络的主要优点是易于训练。特别是，当与深度学习架构相结合时，它们为多标签分类提供了直观、易于使用的损失函数。与马尔可夫网络相比，DDN 的一个缺点是缺乏先进的推理方案，需要使用吉布斯采样。为了应对这一挑战，我们提出了基于局部搜索和整数线性规划的新颖推理方案，用于计算给定观察结果的最有可能的标签分配。我们在三个视频数据集（Charades、TACoS、Wetlab）和三个图像数据集（MS-COCO、PASCAL VOC、NUS-WIDE）上评估我们的新方法，将它们的性能与（a）基本神经架构和（b）组合神经架构进行比较马尔可夫网络配备了先进的推理和学习技术。我们的结果证明了我们的新 DDN 方法相对于两种竞争方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2404.11667</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>物理回归问题的多保真高斯过程代理建模</title>
      <link>https://arxiv.org/abs/2404.11965</link>
      <description><![CDATA[arXiv:2404.11965v1 公告类型：新
摘要：代理建模的主要挑战之一是由于与计算昂贵的模拟相关的资源限制，数据的可用性有限。多保真度方法通过在层次结构中链接模型来提供解决方案，提高保真度，与较低的错误相关，但会增加成本。在本文中，我们比较了构建回归高斯过程代理时采用的不同多保真度方法。现有文献中的非线性自回归方法主要局限于双保真度模型，我们将这些方法扩展到处理两个以上保真度级别。此外，我们建议通过引入结构化内核来增强合并延迟项的现有方法。我们在各种学术和现实场景中展示了这些方法的性能。我们的研究结果表明，与单保真方法相比，在相同的计算成本下，多保真方法通常具有更小的预测误差，尽管它们的有效性在不同场景中有所不同。]]></description>
      <guid>https://arxiv.org/abs/2404.11965</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:36 GMT</pubDate>
    </item>
    <item>
      <title>去偏分布压缩</title>
      <link>https://arxiv.org/abs/2404.12290</link>
      <description><![CDATA[arXiv:2404.12290v1 公告类型：新
摘要：现代压缩方法可以比 i.i.d 更简洁地概括目标分布 $\mathbb{P}$。采样，但需要访问低偏差输入序列，例如快速收敛到 $\mathbb{P}$ 的马尔可夫链。我们引入了一套新的压缩方法，适用于带有偏置输入序列的压缩。给定以错误分布和二次时间为目标的 $n$ 个点，Stein Kernel Thinning (SKT) 返回 $\sqrt{n}$ 等权重点，最大值为 $\widetilde{O}(n^{-1/2})$与 $\mathbb {P}$ 的平均差异 (MMD)。对于更大规模的压缩任务，低秩 SKT 使用可能具有独立兴趣的自适应低秩去偏过程在次二次时间内实现了相同的壮举。对于支持单纯形或恒定保持权重的下游任务，Stein Recombination 和 Stein Cholesky 实现了更大的简约性，将 SKT 的保证与少至 $\operatorname{poly-log}(n)$ 权重点相匹配。这些进步的基础是对单纯形加权核心集的质量、核矩阵的谱衰减以及斯坦因核希尔伯特空间的覆盖数量的新保证。在我们的实验中，我们的技术提供了简洁而准确的后验摘要，同时克服了由于老化、近似马尔可夫链蒙特卡罗和回火造成的偏差。]]></description>
      <guid>https://arxiv.org/abs/2404.12290</guid>
      <pubDate>Fri, 19 Apr 2024 06:17:36 GMT</pubDate>
    </item>
    </channel>
</rss>