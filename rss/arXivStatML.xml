<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>高维拓扑保持特征选择的IVFS算法复现</title>
      <link>https://arxiv.org/abs/2409.12195</link>
      <description><![CDATA[arXiv:2409.12195v1 公告类型：新
摘要：特征选择是处理高维数据的关键技术。在无监督场景中，许多流行的算法专注于保留原始数据结构。在本文中，我们重现了 AAAI 2020 中引入的 IVFS 算法，该算法受到随机子集方法的启发，通过保持拓扑结构来保留数据相似性。我们系统地组织了 IVFS 的数学基础，并通过与原始论文类似的数值实验验证了其有效性。结果表明，IVFS 在大多数数据集上的表现优于 SPEC 和 MCFS，尽管其收敛性和稳定性问题仍然存在。]]></description>
      <guid>https://arxiv.org/abs/2409.12195</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度高斯过程的摊销变分推断</title>
      <link>https://arxiv.org/abs/2409.12301</link>
      <description><![CDATA[arXiv:2409.12301v1 公告类型：新
摘要：高斯过程 (GP) 是具有原则性预测不确定性估计的函数逼近的贝叶斯非参数模型。深度高斯过程 (DGP) 是 GP 的多层泛化，可以表示复杂的边际密度以及复杂的映射。由于精确推理在 GP 及其扩展中要么计算困难，要么在分析上难以处理，因此一些现有方法采用变分推理 (VI) 技术进行可处理的近似。然而，传统近似 GP 模型的表达能力严重依赖于独立的诱导变量，而这些变量对于某些问题可能不够有用。在这项工作中，我们为 DGP 引入了摊销变分推理，它学习了一个将每个观察值映射到变分参数的推理函数。该方法具有更具表现力的先验条件，该条件依赖于更少的输入相关诱导变量，以及灵活的摊销边际后验，能够对更复杂的函数进行建模。我们通过理论推理和实验结果表明，我们的方法在计算成本更低的情况下，性能与以前的方法类似或更好。]]></description>
      <guid>https://arxiv.org/abs/2409.12301</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测试时间增强与变分贝叶斯相结合</title>
      <link>https://arxiv.org/abs/2409.12587</link>
      <description><![CDATA[arXiv:2409.12587v1 公告类型：新
摘要：众所周知，数据增强对机器学习模型的稳健性贡献巨大。在大多数情况下，数据增强是在训练阶段使用的。测试时间增强 (TTA) 是一种在测试阶段利用这些数据增强来实现稳健预测的技术。更准确地说，TTA 平均一个实例的多个数据增强的预测以产生最终预测。虽然 TTA 的有效性已经得到实证报告，但可以预期，所实现的预测性能将取决于测试期间使用的数据增强方法集。特别是，所应用的数据增强方法应该对性能做出不同的贡献。也就是说，预计用于 TTA 的数据增强方法集的贡献程度可能不同，这可能会对预测性能产生负面影响。在本研究中，我们考虑基于每个数据增强的贡献的 TTA 加权版本。一些 TTA 变体可以被视为考虑确定适当权重的问题。我们证明了这种加权 TTA 系数的确定可以在变分贝叶斯框架中形式化。我们还表明，优化权重以最大化边际对数似然可以抑制测试阶段不需要的数据增强候选。]]></description>
      <guid>https://arxiv.org/abs/2409.12587</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>损失函数在强化学习中的核心作用</title>
      <link>https://arxiv.org/abs/2409.12799</link>
      <description><![CDATA[arXiv:2409.12799v1 公告类型：新
摘要：本文阐述了损失函数在数据驱动决策中的核心作用，全面调查了它们在成本敏感分类 (CSC) 和强化学习 (RL) 中的影响。我们展示了不同的回归损失函数如何影响基于价值的决策算法的样本效率和自适应性。在多种设置中，我们证明使用二元交叉熵损失的算法实现了与最优策略成本的一阶边界缩放，并且比常用的平方损失效率高得多。此外，我们证明使用最大似然损失的分布式算法实现了与策略方差的二阶边界缩放，甚至比一阶边界更清晰。这特别证明了分布式 RL 的好处。我们希望本文可以作为分析具有不同损失函数的决策算法的指南，并可以激励读者寻找更好的损失函数来改进任何决策算法。]]></description>
      <guid>https://arxiv.org/abs/2409.12799</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用量子认知机器学习对数据集的内在维度进行稳健估计</title>
      <link>https://arxiv.org/abs/2409.12805</link>
      <description><![CDATA[arXiv:2409.12805v1 公告类型：新
摘要：我们提出了一种基于量子认知机器学习的新数据表示方法，并将其应用于流形学习，特别是用于估计数据集的内在维度。这个想法是学习每个数据点作为量子态的表示，编码点的局部属性及其与整个数据的关系。受量子几何思想的启发，我们从量子态构建了一个配备量子度量的点云。该度量表现出一个谱间隙，其位置对应于数据的内在维度。所提出的估计器基于对这个谱间隙的检测。在合成流形基准上测试时，我们的估计被证明对引入逐点高斯噪声具有鲁棒性。这与当前最先进的估计器形成鲜明对比，后者倾向于将人工“阴影维度”归因于噪声伪影，从而导致高估。在处理真实数据集时，这是一个显著的优势，因为真实数据集不可避免地会受到未知水平的噪声影响。我们通过在 ISOMAP 人脸数据库、MNIST 和威斯康星州乳腺癌数据集上进行测试，展示了我们的方法对真实数据的适用性和稳健性。]]></description>
      <guid>https://arxiv.org/abs/2409.12805</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有效的数据子集选择以推广跨模型训练：传导和归纳网络</title>
      <link>https://arxiv.org/abs/2409.12255</link>
      <description><![CDATA[arXiv:2409.12255v1 公告类型：交叉 
摘要：现有的高效学习子集选择方法主要采用离散组合和模型特定方法，缺乏通用性。对于看不见的架构，不能使用为不同模型选择的子集。为了解决这个问题，我们提出了一个可训练的子集选择框架 $\texttt{SubSelNet}$，它可以跨架构进行推广。在这里，我们首先介绍一个基于注意力的神经小工具，它利用架构的图形结构并充当训练有素的深度神经网络的替代品，以进行快速模型预测。然后，我们使用这些预测来构建子集采样器。这自然为我们提供了两种 $\texttt{SubSelNet}$ 变体。第一个变体是传导性的（称为传导性-$\texttt{SubSelNet}$），它通过解决一个小的优化问题为每个模型分别计算子集。由于用模型近似器代替了显式模型训练，这种优化仍然非常快。第二种变体是归纳的（称为 Inductive-$\texttt{SubSelNet}$），它使用经过训练的子集选择器计算子集，而无需任何优化。我们的实验表明，我们的模型在几个真实数据集上的表现优于几种方法]]></description>
      <guid>https://arxiv.org/abs/2409.12255</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多元时间序列分类的用户友好型基础模型适配器</title>
      <link>https://arxiv.org/abs/2409.12264</link>
      <description><![CDATA[arXiv:2409.12264v1 公告类型：交叉 
摘要：基础模型虽然非常有效，但通常资源密集，需要大量的推理时间和内存。本文通过探索降维技术来解决在有限的计算资源下使这些模型更易于访问的挑战。我们的目标是让用户能够在标准 GPU 上运行大型预训练基础模型，而不会牺牲性能。我们研究了经典方法（例如主成分分析）以及基于神经网络的适配器，旨在降低多元时间序列数据的维数，同时保留关键特征。我们的实验表明，与基线模型相比，速度提高了 10 倍，而性能没有下降，并且使单个 GPU 能够容纳多达 4.5 倍的数据集，为更用户友好和可扩展的基础模型铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2409.12264</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Transformer 进行线性系统和线性椭圆偏微分方程的可证明上下文学习</title>
      <link>https://arxiv.org/abs/2409.12293</link>
      <description><![CDATA[arXiv:2409.12293v1 公告类型：交叉 
摘要：由 Transformer 架构驱动的自然语言处理基础模型表现出卓越的上下文学习 (ICL) 能力，允许预训练模型使用少量提示适应下游任务而无需更新其权重。最近，基于 Transformer 的基础模型也已成为解决科学问题的多功能工具，特别是在偏微分方程 (PDE) 领域。然而，这些科学模型中 ICL 能力的理论基础仍然很大程度上未被探索。这项工作为基于 Transformer 的 ICL 开发了一种严格的误差分析，该 ICL 应用于与线性椭圆 PDE 系列相关的解算子。我们首先证明，由线性自注意层定义的线性 Transformer 可以证明在上下文中学习以反转由 PDE 的空间离散化产生的线性系统。这是通过推导所提出的线性变压器的预测风险的理论缩放定律来实现的，这些理论缩放定律包括空间离散化大小、训练任务数量以及训练和推理期间使用的提示长度。这些缩放定律还使我们能够为学习 PDE 解建立定量误差界限。此外，我们量化了预训练变压器对下游 PDE 任务的适应性，这些任务在任务（由 PDE 系数表示）和输入协变量（由源项表示）中都会经历分布变化。为了分析任务分布变化，我们引入了一个新概念任务多样性，并根据任务变化的幅度来表征变压器的预测误差，假设预训练任务具有足够的多样性。我们还建立了确保任务多样性的充分条件。最后，我们通过大量数值实验验证了变压器的 ICL 能力。]]></description>
      <guid>https://arxiv.org/abs/2409.12293</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SANE：多模态和不可微黑盒函数中多重最优发现的战略自主非平滑探索</title>
      <link>https://arxiv.org/abs/2409.12295</link>
      <description><![CDATA[arXiv:2409.12295v1 公告类型：交叉 
摘要：计算和实验材料发现都带来了探索多维和多模态参数空间的挑战，例如具有多个相互作用的汉密尔顿相图、组合库的组成空间、材料结构图像空间和分子嵌入空间。这些系统通常是黑盒的，评估起来很耗时，这引起了人们对贝叶斯优化 (BO) 等主动学习方法的浓厚兴趣。然而，这些系统通常很嘈杂，这使得黑盒函数严重多模态且不可微分，其中普通 BO 可能会过度集中在单个或伪最优值附近，偏离科学发现的更广泛目标。为了解决这些限制，我们在此开发了战略自主非平滑探索 (SANE)，以促进智能贝叶斯优化导航，并提出成本驱动的概率获取函数来找到多个全局和局部最优区域，避免陷入单一最优的趋势。为了区分由于实验测量噪声而产生的真最佳区域和假最佳区域，将人类（领域）知识驱动的动态代理门与 SANE 集成。我们将 SANE 门应用于预先获取的铁电组合库的压电响应光谱数据（特定区域的噪声水平较高）和压电响应力显微镜 (PFM) 高光谱数据中。SANE 表现出比传统 BO 更好的性能，有助于探索多个最佳区域，从而优先学习具有更高科学价值覆盖率的自主实验。我们的工作展示了该方法在现实世界实验中的潜在应用，其中这种结合战略和人为干预的方法对于解锁自主研究中的新发现至关重要。]]></description>
      <guid>https://arxiv.org/abs/2409.12295</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过最大规律的 ReLU MLP 的最佳近似弥合近似与学习之间的差距</title>
      <link>https://arxiv.org/abs/2409.12335</link>
      <description><![CDATA[arXiv:2409.12335v1 公告类型：交叉
摘要：深度学习的基础由近似或学习理论这两个看似对立的观点所支持。前者主张不需要泛化的大型/富有表现力的模型，而后者则考虑具有泛化能力但可能太小/受限而无法成为通用近似器的类。受现实世界中既具有表现力又具有统计可靠性的深度学习实现的启发，我们问：“是否存在一类神经网络，它既足够大以具有通用性，又足够结构化以具有泛化能力？”
本文通过识别一类高度结构化的 ReLU 多层感知器 (MLP)，建设性地为这个问题提供了肯定的答案，它们是最佳函数近似器，并且在统计上表现良好。我们表明，任何从 $[0,1]^d$ 到 $[-n,n]$ 的 $L$-Lipschitz 函数都可以近似为 $[0,1]^d$ 上的均匀 $Ld/(2n)$ 误差，其中稀疏连接的 $L$-Lipschitz ReLU MLP 宽度为 $\mathcal{O}(dn^d)$，深度为 $\mathcal{O}(\log(d))$，具有 $\mathcal{O}(dn^d)$ 个非零参数，其权重和偏差取值范围为 $\{0,\pm 1/2\}$，但第一层和最后一层除外，它们的幅度最多为 $n$。与之前已知的“大型”通用 ReLU MLP 类不同，即使深度和宽度变得任意大，我们类的经验 Rademacher 复杂度仍然有界。此外，当给定 $N$ 个 i.i.d. 正则化亚高斯训练样本时，我们的 MLP 类实现了接近最佳的样本复杂度 $\mathcal{O}(\log(N)/\sqrt{N})$。
我们通过避免构建最佳 ReLU 近似器的标准方法来实现这一点，这种方法通过依赖小尖峰而牺牲了规律性。相反，我们引入了一种新的构造，它使用 Kuhn 三角剖分将线性部分完美地组合在一起，并避免了这些小尖峰。]]></description>
      <guid>https://arxiv.org/abs/2409.12335</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通信高效的联邦低秩更新算法及其与隐式正则化的联系</title>
      <link>https://arxiv.org/abs/2409.12371</link>
      <description><![CDATA[arXiv:2409.12371v1 公告类型：交叉 
摘要：联邦学习 (FL) 面临着与通信效率和异构性相关的重大挑战。为了解决这些问题，我们探索了使用低秩更新的潜力。我们的理论分析表明，与服务器的损失相比，客户端的损失表现出更高的秩结构（梯度跨越 Hessian 的高秩子空间）。基于这一见解，我们假设将客户端优化限制在低秩子空间可以提供隐式正则化效果。因此，我们提出了 FedLoRU，一种用于联邦学习的通用低秩更新框架。我们的框架强制执行低秩客户端更新并累积这些更新以形成更高秩的模型。此外，FedLoRU 的变体可以通过采用多个或分层的低秩更新来适应具有统计和模型异构性的环境。实验结果表明，FedLoRU 的性能与全秩算法相当，并且对异构和大量客户端表现出鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2409.12371</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经网络在低复杂度数据上进行泛化</title>
      <link>https://arxiv.org/abs/2409.12446</link>
      <description><![CDATA[arXiv:2409.12446v1 公告类型：交叉 
摘要：我们表明，具有 ReLU 激活的前馈神经网络可以在适当定义的低复杂度数据上推广。给定由简单编程语言生成的 i.i.d. 数据，对数据进行插值的最小描述长度 (MDL) 前馈神经网络可以高概率地推广。我们定义了这种简单的编程语言，以及此类网络的描述长度概念。我们提供了几个关于基本计算任务的示例，例如检查自然数的素数等等。对于素数测试，我们的定理显示以下内容。假设我们从 $1$ 到 $N$ 均匀随机地抽取一个 i.i.d. 样本 $\Theta(N^{\delta}\ln N)$ 数字，其中 $\delta\in (0,1)$。对于每个数字 $x_i$，如果 $x_i$ 是素数，则让 $y_i = 1$，如果不是，则让 $y_i = 0$。然后，拟合此数据的 MDL 网络将以高概率准确回答新抽取的介于 $1$ 和 $N$ 之间的数字是否为素数，测试误差为 $\leq O(N^{-\delta})$。请注意，该网络并非设计用于检测素数；最小描述学习可发现可检测素数的网络。]]></description>
      <guid>https://arxiv.org/abs/2409.12446</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>相干性流入对于量子库计算至关重要</title>
      <link>https://arxiv.org/abs/2409.12693</link>
      <description><![CDATA[arXiv:2409.12693v1 公告类型：交叉 
摘要：回声状态特性 (ESP) 是一种基本特性，它允许输入驱动的动态系统执行信息处理任务。最近，已经提出了将 ESP 扩展到潜在的非平稳系统和子系统，即非平稳 ESP 和子集/子空间 ESP。在本文中，我们从理论和数值上分析了量子系统满足非平稳 ESP 和子集/子空间非平稳 ESP 的充分和必要条件。基于广泛使用 Pauli 传输矩阵 (PTM) 形式，我们发现 (1) 与量子相干环境的相互作用（称为 \textit{相干流入}）对于实现非平稳 ESP 是必不可少的，并且 (2) PTM 的谱半径可以表征量子库计算 (QRC) 的衰落记忆特性。我们的数值实验涉及一个具有自旋玻璃/多体局域化相的哈密顿量的系统，结果表明 PTM 的谱半径可以描述此类系统固有的动态相变。为了全面了解 QRC 在 ESP 下的机制，我们提出了一个简化模型，即乘法储层计算 (mRC)，这是一个具有一维乘法输入的储层计算 (RC) 系统。从理论和数值上看，我们表明 mRC 中与谱半径和相干流入相对应的参数与其线性存储容量 (MC) 直接相关。我们对 QRC 和 mRC 的发现将为 PTM 和 QRC 的输入乘法性提供理​​论方面。结果将有助于更好地理解开放量子系统中的 QRC 和信息处理。]]></description>
      <guid>https://arxiv.org/abs/2409.12693</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于微磁能量最小化的物理感知机器学习：最近的算法发展</title>
      <link>https://arxiv.org/abs/2409.12877</link>
      <description><![CDATA[arXiv:2409.12877v1 公告类型：交叉 
摘要：在这项工作中，我们探索了先进的机器学习技术，用于在完整的 3D 微磁模拟中最小化吉布斯自由能。基于布朗的静磁自能界限，我们重新审视了它们在标量和矢量势传输问题的变分公式中的应用。为了克服全空间积分带来的计算挑战，我们在有限域上重新表述了这些界限，使该方法更高效，并且可用于数值模拟。我们的方法利用交替优化方案来联合最小化布朗的能量界限和吉布斯自由能。凯莱变换用于严格执行单位范数约束，而 R 函数用于在静磁场的计算中施加基本边界条件。我们的研究结果凸显了无网格物理信息神经网络 (PINN) 和极限学习机 (ELM) 与硬约束相结合的潜力，可提供高精度近似值。与传统数值方法相比，这些方法表现出了极具竞争力的性能，在计算静磁场和能量最小化应用（例如磁滞曲线计算）方面显示出巨大的前景。这项工作为未来研究更复杂的几何形状（例如晶粒结构模型）以及应用于传统数值方法难以解决的大规模问题设置开辟了道路。]]></description>
      <guid>https://arxiv.org/abs/2409.12877</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有拓扑向量空间输入的神经网络的通用逼近定理</title>
      <link>https://arxiv.org/abs/2409.12913</link>
      <description><![CDATA[arXiv:2409.12913v1 公告类型：交叉 
摘要：我们研究具有来自拓扑向量空间 (TVS-FNN) 的输入的前馈神经网络。与传统的前馈神经网络不同，TVS-FNN 可以处理更广泛的输入，包括序列、矩阵、函数等。我们证明了 TVS-FNN 的通用近似定理，该定理证明了它们能够近似定义在此扩展输入空间上的任何连续函数。]]></description>
      <guid>https://arxiv.org/abs/2409.12913</guid>
      <pubDate>Fri, 20 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>