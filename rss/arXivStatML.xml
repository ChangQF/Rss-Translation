<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Mon, 18 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于时空模型的密集 ReLU 神经网络</title>
      <link>https://arxiv.org/abs/2411.09961</link>
      <description><![CDATA[arXiv:2411.09961v1 公告类型：新
摘要：在本文中，我们重点研究利用整流线性单元 (ReLU) 激活函数进行非参数估计的全连接深度神经网络。我们推导出导致收敛速度的非渐近界限，解决了观察到的测量中的时间和空间依赖性。通过考虑跨时间和空间的依赖性，我们的模型可以更好地反映现实世界数据的复杂性，从而提高预测性能和理论稳健性。我们还通过在流形上对数据进行建模来解决维数灾难，探索高维数据的内在维数。我们通过将时空分析的现有理论发现应用于更一般情况下的神经网络来拓宽它们，并证明我们的证明技术对于具有短程依赖性的模型是有效的。我们对各种合成响应函数进行的经验模拟强调了我们方法的卓越性能，优于现有文献中已建立的方法。这些发现为密集神经网络在广泛的功能类别中实现时空建模的强大能力提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2411.09961</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论扩展双曲深度卷积神经网络的普适统计一致性</title>
      <link>https://arxiv.org/abs/2411.10128</link>
      <description><![CDATA[arXiv:2411.10128v1 公告类型：新
摘要：深度卷积神经网络 (DCNN) 的出现已成为实现计算机视觉广泛应用的普遍工具。尽管它具有捕获数据内部复杂模式的潜力，但底层嵌入空间仍然是欧几里得空间，主要追求收缩卷积。几个实例可以作为 DCNN 性能恶化的先例。双曲空间中神经网络的最新进展获得了关注，激励了双曲空间中卷积深度神经网络的发展。在这项工作中，我们提出了基于庞加莱圆盘的双曲 DCNN。这项工作主要围绕在非欧几里得域的背景下分析扩展卷积的性质。我们进一步提供了有关双曲空间中扩展卷积的普遍一致性的广泛理论见解。不仅在合成数据集上进行了多次模拟，还在一些真实数据集上进行了多次模拟。实验结果表明，双曲卷积架构的表现远胜于欧几里得架构。]]></description>
      <guid>https://arxiv.org/abs/2411.10128</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BONE：非平稳环境下贝叶斯在线学习的统一框架</title>
      <link>https://arxiv.org/abs/2411.10153</link>
      <description><![CDATA[arXiv:2411.10153v1 公告类型：新
摘要：我们提出了一个统一的框架，用于在非平稳环境中执行贝叶斯在线学习的方法。我们称该框架为 BONE，代表（非）平稳（E）环境中的（贝叶斯（O）在线学习。BONE 提供了一种通用结构来解决各种问题，包括在线持续学习、前序预测和上下文强盗。该框架需要指定三个建模选择：（i）测量模型（例如神经网络），（ii）用于建模非平稳性的辅助过程（例如自上次变化点以来的时间），以及（iii）模型参数的条件先验（例如多元高斯）。该框架还需要两种算法选择，我们利用这两种算法选择在该框架下进行近似推理：(i) 一种算法，用于在给定辅助变量的情况下估计关于模型参数的信念（后验分布）；(ii) 一种算法，用于估计关于辅助变量的信念。我们展示了这种模块化如何让我们将许多不同的现有方法编写为 BONE 的实例；我们还使用该框架提出了一种新方法。然后，我们在多个数据集上对现有方法和我们提出的新方法进行了实验比较；我们提供了关于在哪些情况下一种方法比另一种方法更适合给定任务的见解。]]></description>
      <guid>https://arxiv.org/abs/2411.10153</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于多元因果发现的连续贝叶斯模型选择</title>
      <link>https://arxiv.org/abs/2411.10154</link>
      <description><![CDATA[arXiv:2411.10154v1 公告类型：新
摘要：当前的因果发现方法需要限制性模型假设或假设可以访问干预数据以确保结构可识别性。这些假设在实际应用中通常不成立，导致实践中保证的丧失和准确性较差。最近的研究表明，在双变量情况下，贝叶斯模型选择可以通过将限制性建模换成更灵活的假设来大大提高准确性，但代价是出现较小错误概率。我们将贝叶斯模型选择方法扩展到重要的多变量设置，通过连续松弛使大型离散选择问题可扩展。我们演示了对于我们选择的贝叶斯非参数模型，即因果高斯过程条件密度估计器 (CGP-CDE)，如何从模型超参数构建邻接矩阵。然后使用边际似然和非循环正则化器优化此邻接矩阵，输出最大后验因果图。我们展示了我们的方法在合成数据集和真实世界数据集上的竞争力，表明可以使用贝叶斯模型选择执行多变量因果发现而无需不可行的假设。]]></description>
      <guid>https://arxiv.org/abs/2411.10154</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过安全多样化模型策略搜索进行迭代批量强化学习</title>
      <link>https://arxiv.org/abs/2411.09722</link>
      <description><![CDATA[arXiv:2411.09722v1 公告类型：交叉 
摘要：批量强化学习使策略学习在训练期间无需与环境直接交互，而仅依赖于先前收集的交互集。因此，这种方法非常适合高风险和成本密集型应用，例如工业控制。学习到的策略通常被限制为以与批处理中观察到的类似的方式运行。在现实世界中，学习到的策略部署在工业系统中，不可避免地导致收集新数据，这些数据随后可以添加到现有记录中。因此，学习和部署的过程可以在系统的整个生命周期内多次发生。在这项工作中，我们建议利用应用离线强化学习的这种迭代性质来引导学习到的策略在部署过程中实现高效和信息丰富的数据收集，从而不断改进学习到的策略，同时仍在收集的数据的支持范围内。我们提出了一种基于集成模型的策略搜索的迭代批量强化学习算法，并增强了安全性，更重要的是，增强了多样性标准。]]></description>
      <guid>https://arxiv.org/abs/2411.09722</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SureMap：单任务和多任务分解评估的同步均值估计</title>
      <link>https://arxiv.org/abs/2411.09730</link>
      <description><![CDATA[arXiv:2411.09730v1 公告类型：交叉 
摘要：分解评估——评估机器学习模型在不同子群体上的表现——是评估人工智能系统的性能和群体公平性时的核心任务。一个关键的挑战是评估数据稀缺，而由属性（例如种族、性别、年龄）交叉而产生的子群体通常很小。如今，多个客户从模型开发人员那里采购相同的人工智能模型是很常见的，每个客户都单独面临分解评估的任务。这就产生了我们所说的多任务分解评估问题，其中多个客户试图在他们自己的数据设置（任务）中对给定模型进行分解评估。在这项工作中，我们开发了一种称为 SureMap 的分解评估方法，该方法对黑盒模型的多任务和单任务分解评估都具有很高的估计精度。 SureMap 的效率提升来自于 (1) 将问题转化为结构化同步高斯均值估计和 (2) 整合外部数据，例如来自 AI 系统创建者或其他客户的数据。我们的方法结合了使用精心选择的先验的最大后验 (MAP) 估计和通过 Stein 的无偏风险估计 (SURE) 进行的无交叉验证调整。我们在多个领域的分类评估任务上对 SureMap 进行了评估，发现其准确率比几个强大的竞争对手有显著提高。]]></description>
      <guid>https://arxiv.org/abs/2411.09730</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>引导还是推出？最佳自适应插值</title>
      <link>https://arxiv.org/abs/2411.09731</link>
      <description><![CDATA[arXiv:2411.09731v1 公告类型：交叉 
摘要：引导和推出是强化学习 (RL) 中价值函数估计的两个基本原则。我们引入了一类新的贝尔曼算子，称为子图贝尔曼算子，它在引导和推出方法之间进行插值。我们的估计器是通过求解经验子图贝尔曼算子的不动点得出的，结合了基于引导的时间差 (TD) 估计器和基于推出的蒙特卡洛 (MC) 方法的优势。具体而言，我们的估计器的误差上限接近 TD 实现的最佳方差，附加项取决于状态空间选定子集的退出概率。同时，估计器表现出 MC 的有限样本自适应性，样本复杂度仅取决于该子集的占用率度量。我们用信息论下限补充上限，表明在合理的样本量下，附加项是不可避免的。总之，这些结果确立了子图贝尔曼估计量作为在策略评估中协调 TD 和 MC 方法的最佳自适应框架。]]></description>
      <guid>https://arxiv.org/abs/2411.09731</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实现更公平的非负矩阵分解</title>
      <link>https://arxiv.org/abs/2411.09847</link>
      <description><![CDATA[arXiv:2411.09847v1 公告类型：交叉 
摘要：主题建模，或更广泛地说，降维技术，为揭示大型数据集中的模式提供了强大的工具，并广泛应用于各个领域。我们研究非负矩阵分解 (NMF) 如何在数据组的表示中引入偏差，例如由人口统计或受保护属性定义的数据组。我们提出了一种称为 Fairer-NMF 的方法，该方法旨在最小化不同组相对于其大小和内在复杂性的最大重建损失。此外，我们提出了两种解决这个问题的算法。第一个是交替最小化 (AM) 方案，第二个是乘法更新 (MU) 方案，与 AM 相比，它减少了计算时间，同时仍实现了相似的性能。最后，我们在合成和真实数据集上进行了数值实验，以评估 Fairer-NMF 的整体性能和权衡]]></description>
      <guid>https://arxiv.org/abs/2411.09847</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>策略空间压缩问题的统计分析</title>
      <link>https://arxiv.org/abs/2411.09900</link>
      <description><![CDATA[arXiv:2411.09900v1 公告类型：交叉 
摘要：策略搜索方法在强化学习中至关重要，它提供了一个解决连续状态动作和部分可观察问题的框架。然而，探索巨大策略空间的复杂性可能导致严重的低效率。通过策略压缩来减少策略空间是一种强大的、无奖励的方法来加速学习过程。这种技术将策略空间压缩为一个较小的、有代表性的集合，同时保持了大部分原始有效性。我们的研究重点是确定准确学习这个压缩集所需的样本量。我们使用 R\&#39;enyi 散度来衡量真实和估计策略分布之间的相似性，为良好的近似值建立误差界限。为了简化分析，我们采用 $l_1$ 范数，确定基于模型和无模型设置的样本量要求。最后，我们将 $l_1$ 范数的误差界限与 R\&#39;enyi 散度的误差界限关联起来，区分顶点附近的策略和策略空间中间的策略，以确定所需样本量下限和上限。]]></description>
      <guid>https://arxiv.org/abs/2411.09900</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用双重/去偏机器学习对脉冲响应函数进行半参数推断</title>
      <link>https://arxiv.org/abs/2411.10009</link>
      <description><![CDATA[arXiv:2411.10009v1 公告类型：交叉 
摘要：我们引入了双重/去偏机器学习 (DML) 估计量，用于脉冲响应函数 (IRF)，其中感兴趣的时间序列受到随时间分配的多个离散处理，这可能会对未来结果产生因果影响。所提出的估计量可以依赖于治疗和结果变量之间的完全非参数关系，从而开辟了使用灵活的机器学习方法来估计 IRF 的可能性。为此，我们将 DML 理论从 i.i.d. 扩展到时间序列设置，并表明所提出的 IRF DML 估计量在参数速率下是一致的和渐近正态分布的，允许在时间序列设置中对动态效应进行半参数推断。通过将估计量应用于在混杂因素和观察创新过程中都存在序列依赖的情况下学习 IRF，在有限样本中对估计量的属性进行了数值验证。我们还通过将该方法应用于估计宏观经济冲击的影响来实证说明该方法。]]></description>
      <guid>https://arxiv.org/abs/2411.10009</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>G 计算可提高具有个体随机化和二元响应的临床试验的性能</title>
      <link>https://arxiv.org/abs/2411.10089</link>
      <description><![CDATA[arXiv:2411.10089v1 公告类型：交叉 
摘要：在临床试验中，随机分配旨在平衡各组之间的预后因素，防止真正的混杂因素。然而，由于偶然因素造成的残差差异可能会引入近似混杂因素。因此建议调整预后因素，特别是因为功率的相关增加。在本文中，我们假设与机器学习相关的 G 计算可能是适合随机临床试验的方法，即使样本量较小。它允许灵活地估计结果模型，即使协变量与结果的关系很复杂。通过模拟，比较了惩罚回归（Lasso、Elasticnet）和基于算法的方法（神经网络、支持向量机、超级学习者）。惩罚回归减少了方差，但可能会略微增加偏差。相关的样本量减少范围为 17\% 至 54\%。相比之下，基于算法的方法虽然对更大、更复杂的数据结构有效，但低估了标准差，尤其是在样本量较小的情况下。总之，使用惩罚模型进行 G 计算，尤其是在适当的情况下使用带样条的 Elasticnet，代表了一种提高 RCT 功效和考虑潜在近似混杂因素的相关方法。]]></description>
      <guid>https://arxiv.org/abs/2411.10089</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MARS：释放方差减少的力量，训练大型模型</title>
      <link>https://arxiv.org/abs/2411.10438</link>
      <description><![CDATA[arXiv:2411.10438v1 公告类型：交叉 
摘要：训练深度神经网络（以及最近的大型模型）需要高效且可扩展的优化器。自适应梯度算法（如 Adam、AdamW 及其变体）一直是这项任务的核心。尽管过去十年中开发了许多方差减少算法，旨在加速凸和非凸设置中的随机优化，但方差减少在训练深度神经网络或大型语言模型方面并未取得广泛成功。因此，它在现代人工智能中仍然是一种不太受欢迎的方法。在本文中，为了释放方差减少的力量以高效训练大型模型，我们提出了一个统一的优化框架 MARS（使方差减少大放异彩），它通过缩放随机递归动量技术将预条件梯度方法与方差减少相协调。在我们的框架中，我们引入了三个 MARS 实例，它们分别利用基于 AdamW、Lion 和 Shampoo 的预条件梯度更新。我们还将我们的算法与现有的优化器联系起来。在训练 GPT-2 模型上的实验结果表明，MARS 的表现始终远胜于 AdamW。]]></description>
      <guid>https://arxiv.org/abs/2411.10438</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于图的半监督学习中的模型改变主动学习</title>
      <link>https://arxiv.org/abs/2110.07739</link>
      <description><![CDATA[arXiv:2110.07739v2 公告类型：替换 
摘要：半监督分类中的主动学习涉及为未标记数据引入附加标签，以提高底层分类器的准确性。挑战在于确定哪些点需要标记才能最好地提高性能，同时限制新标签的数量。“模型变化”主动学习通过引入附加标签来量化分类器中发生的变化。我们将这个想法与基于图的半监督学习方法结合起来，该方法使用图拉普拉斯矩阵的频谱，可以将其截断以避免过大的计算和存储成本。我们考虑一组凸损失函数，对于这些函数，可以使用后验分布的拉普拉斯近似有效地近似获取函数。我们展示了各种多类示例，这些示例说明了与之前最先进的技术相比性能有所提高。]]></description>
      <guid>https://arxiv.org/abs/2110.07739</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络并不总是过度平滑</title>
      <link>https://arxiv.org/abs/2406.02269</link>
      <description><![CDATA[arXiv:2406.02269v2 公告类型：替换 
摘要：图神经网络 (GNN) 已成为处理应用中关系数据的强大工具。然而，GNN 存在过度平滑的问题，即所有节点的特征在各层上呈指数级收敛到同一向量，从而阻碍了深度 GNN 的设计。在这项工作中，我们利用图卷积网络 (GCN) 中无限多个隐藏特征极限下的高斯过程 (GP) 等价性来研究过度平滑。通过从传统的深度神经网络 (DNN) 中推广方法，我们可以用 GP 来描述深度 GCN 输出层的特征分布：正如预期的那样，我们发现文献中的典型参数选择会导致过度平滑。然而，该理论使我们能够识别一个新的非过度平滑阶段：如果网络的初始权重具有足够大的方差，则 GCN 不会过度平滑，并且即使在较大深度下节点特征仍然具有信息量。我们通过在有限大小 GCN 的输出上训练线性分类器来证明此预测的有效性。此外，利用 GCN GP 的线性化，我们将信息传播深度的概念从 DNN 推广到 GCN。这种传播深度在过度平滑和非过度平滑阶段之间的过渡处发散。我们测试了我们方法的预测并发现与有限大小 GCN 有很好的一致性。在接近非过度平滑阶段的过渡处初始化 GCN，我们获得既深又富于表现力的网络。]]></description>
      <guid>https://arxiv.org/abs/2406.02269</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时间序列的循环神经拟合优度检验</title>
      <link>https://arxiv.org/abs/2410.13986</link>
      <description><![CDATA[arXiv:2410.13986v3 公告类型：替换 
摘要：时间序列数据在金融和医疗保健等不同领域都至关重要，在这些领域中，准确的预测和决策依赖于先进的建模技术。虽然生成模型在捕捉时间序列中固有的复杂动态方面表现出巨大的潜力，但评估其性能仍然是一项重大挑战。由于特征的时间依赖性和潜在的高维性，传统的评估指标不足。在本文中，我们提出了 REcurrent NeurAL (RENAL) 拟合优度检验，这是一种用于评估生成时间序列模型的新颖且统计严格的框架。通过利用循环神经网络，我们将时间序列转换为条件独立的数据对，从而能够将基于卡方的拟合优度检验应用于数据中的时间依赖性。这种方法为评估生成模型的质量提供了一种强大的、理论扎实的解决方案，特别是在时间序列有限的环境中。我们在合成数据集和真实数据集上证明了我们的方法的有效性，在可靠性和准确性方面优于现有方法。我们的方法填补了时间序列生成模型评估中的一个关键空白，提供了一种既实用又适用于高风险应用的工具。]]></description>
      <guid>https://arxiv.org/abs/2410.13986</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>