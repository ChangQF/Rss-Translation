<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的统计 — 机器学习 (stat.ML) 更新</description>
    <lastBuildDate>Wed, 24 Jan 2024 03:14:29 GMT</lastBuildDate>
    <item>
      <title>用于解决一些随机最优控制问题的深度多任务神经网络。 （arXiv：2401.12923v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.12923</link>
      <description><![CDATA[大多数现有的基于神经网络的解决随机最优的方法
使用相关的向后动态规划原理控制问题
依赖于模拟底层状态变量的能力。然而，在
有些问题，这种模拟是不可行的，导致离散化
状态变量空间以及需要为每个数据训练一个神经网络
观点。当处理以下问题时，这种方法在计算上变得低效
大的状态变量空间。在本文中，我们考虑这种类型的一类
随机最优控制问题并介绍有效的解决方案
采用多任务神经网络。为了训练我们的多任务神经网络，我们
引入一种新颖的方案，可以动态平衡任务之间的学习。
通过对现实世界衍生品定价问题的数值实验，我们
证明我们的方法优于最先进的方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.12923</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>用于选择性分类的深度神经网络基准。 （arXiv：2401.12708v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12708</link>
      <description><![CDATA[随着机器学习模型在许多领域的部署不断增加
社会敏感的任务，对可靠和可靠的需求不断增长
值得信赖的预测。实现这些要求的一种方法是允许
当做出预测的风险很高时，模型放弃做出预测
一个错误。这就需要给模型添加一个选择机制，
选择模型将提供预测的那些示例。这
选择性分类框架旨在设计一种平衡机制
被拒绝的预测的分数（即，被拒绝的预测的示例的比例）
模型没有做出预测）与预测的改进
所选预测的性能。多重选择性分类
框架是存在的，其中大多数依赖于深度神经网络架构。
然而，现有方法的实证评估仍然有限
方法和设置之间的部分比较，为从业者提供
对它们的相对优点了解甚少。我们通过对标 18 来填补这一空白
包含图像和表格的 44 个不同数据集的基线
数据。此外，还有二进制任务和多类任务的混合。我们评估
这些方法使用多个标准，包括选择性错误率，
经验覆盖率、被拒绝实例类别的分布，以及
分布外实例上的性能。结果表明，有
在调查的基线中没有一个明显的获胜者，并且最好的方法
取决于用户的目标。
]]></description>
      <guid>http://arxiv.org/abs/2401.12708</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>映射：消除图神经网络的偏差，以实现公平节点分类，敏感信息泄漏有限。 （arXiv：2401.12824v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12824</link>
      <description><![CDATA[尽管在各种基于网络的应用程序中取得了显着的成功，但图神经网络
网络（GNN）继承并进一步加剧了历史歧视和
社会刻板印象，严重阻碍了他们在高风险领域的部署
在线临床诊断、金融征信等领域。然而，
当前的公平性研究主要基于独立同分布数据，无法
简单地复制到非独立同分布具有拓扑依赖性的图结构
样本之间。现有的公平图学习通常倾向于成对
约束实现公平但未能摆脱维度限制
并将它们概括为多个敏感属性；此外，大多数研究
专注于执行和校准公平性的处理技术，
在预处理中构建模型不可知的去偏 GNN 框架
防止下游误操作和提高训练可靠性的阶段仍然存在
很大程度上尚未得到充分探索。此外，之前关于 GNN 的工作往往会增强
无论是公平还是隐私，但很少有人探究它们之间的相互作用。在
在本文中，我们提出了一种新颖的模型不可知去偏框架，名为 MAPPING
(\underline{M}询问 \underline{A}nd \underline{P} 运行并
Message-\underline{P}assing train\underline{ING}) 用于公平节点分类，
其中我们采用基于距离协方差（$dCov$）的公平约束来
同时减少任意维度的特征和拓扑偏差，以及
将它们与对抗性去偏相结合以限制属性的风险
推理攻击。使用不同 GNN 在真实数据集上进行实验
变体证明了 MAPPING 的有效性和灵活性。我们的成果
表明 MAPPING 可以在效用和公平之间实现更好的权衡，
降低敏感信息泄露的隐私风险。
]]></description>
      <guid>http://arxiv.org/abs/2401.12824</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>DDMI：用于合成高质量隐式神经表示的领域不可知的潜在扩散模型。 （arXiv：2401.12517v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12517</link>
      <description><![CDATA[最近的研究引入了一类新的生成模型
合成捕获任意的隐式神经表示（INR）
各个域中的连续信号。这些模型打开了大门
与领域无关的生成模型，但它们往往无法实现高质量
一代。我们观察到现有方法生成的权重为
神经网络参数化 INR 并使用固定值评估网络
位置嵌入（PE）。可以说，这种架构限制了表达能力
生成模型的力量并导致低质量的 INR 生成。到
为了解决这个限制，我们提出了与领域无关的潜在扩散模型
INR（DDMI）生成自适应位置嵌入而不是神经网络
网络的权重。具体来说，我们开发了一个离散到连续的空间
变分自动编码器 (D2C-VAE)，无缝连接离散数据和
连续信号在共享潜在空间中起作用。此外，我们
引入一种新的调节机制来评估 INR
分层分解PE，进一步增强表达能力。广泛的
跨四种模式的实验，例如 2D 图像、3D 形状、神经辐射
具有七个基准数据集的字段和视频展示了多功能性
DDMI 及其与现有 INR 生成相比的优越性能
楷模。
]]></description>
      <guid>http://arxiv.org/abs/2401.12517</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>解释等变表示。 （arXiv：2401.12588v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12588</link>
      <description><![CDATA[潜在表示广泛用于下游任务，例如
深度学习模型的可视化、插值或特征提取。
不变和等变神经网络功能强大且完善
强制归纳偏差的模型。在本文中，我们证明了
还必须考虑等变模型施加的归纳偏差
使用潜在表示时的帐户。我们展示了如何不考虑
归纳偏差会导致下游任务的性能下降，反之亦然
反之亦然，如何通过使用
潜在表征的不变投影。我们提出原则
如何选择这样的投影，并展示使用这些原则的影响
在两个常见的例子中：首先，我们研究排列等变变分
为分子图生成而训练的自动编码器；在这里我们展示了不变式
可以设计投影，使得结果不会丢失信息
不变的表示。接下来，我们研究旋转等变表示
用于图像分类。在这里，我们说明随机不变性如何
投影可用于获得具有高的不变表示
信息保留程度。在这两种情况下，对不变潜伏的分析
事实证明，其表示优于其等变对应物。最后，我们
说明此处记录的等变神经网络现象
在标准神经网络中有对应的内容，鼓励不变性
通过增强。因此，虽然经验丰富的人可能知道这些含糊之处
作为等变模型的开发者，我们既创造了知识，也创造了
为更广泛的社区提供处理歧义的有效工具。
]]></description>
      <guid>http://arxiv.org/abs/2401.12588</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>通过高维二元类不平衡基因表达数据的稳健加权评分进行特征选择。 （arXiv：2401.12667v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.12667</link>
      <description><![CDATA[在本文中，不平衡数据的鲁棒加权得分 (ROWSU) 为
提出为高维选择最具辨别力的特征
具有类不平衡问题的基因表达二元分类。方法
解决高度倾斜的阶级中最具挑战性的问题之一
基因表达数据集中的分布对性能产生不利影响
的分类算法。首先，训练数据集通过以下方式进行平衡
从少数类观察中综合生成数据点。第二，
使用贪婪搜索方法选择基因的最小子集。第三，一个
新颖的加权稳健分数，其中权重由支持向量计算，
被引入以获得一组精炼的基因。基于得分最高的基因
在这种方法上与由选择的基因的最小子集相结合
贪婪搜索方法形成最终的基因组。新颖的方法确保
选择最具辨别力的基因，即使存在倾斜
类分布，从而提高分类器的性能。这
所提出的 ROWSU 方法的性能根据 $6$ 基因表达进行评估
数据集。分类精度和灵敏度作为性能
将所提出的 ROWSU 算法与其他几种算法进行比较的指标
最先进的方法。还构建了箱线图和稳定性图
更好地理解结果。结果表明，所提出的
该方法优于现有的基于特征选择程序
k 最近邻 (kNN) 和随机森林的分类性能
（RF）分类器。
]]></description>
      <guid>http://arxiv.org/abs/2401.12667</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习和降阶建模对具有乘性噪声的不可分离哈密顿量进行贝叶斯识别。 （arXiv：2401.12476v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.12476</link>
      <description><![CDATA[本文提出了一种结构保持贝叶斯学习方法
使用随机动态模型的不可分离哈密顿系统允许
统计相关的向量值加法和乘法测量
噪音。该方法由三个主要方面组成。首先，我们推导出一个
高斯滤波器用于统计相关、向量值、加法和
乘法噪声模型，需要评估内的可能性
贝叶斯后验。其次，我们开发了一种具有成本效益的新颖算法
贝叶斯系统识别在高维系统中的应用。
第三，我们演示了如何将结构保持方法纳入
提出的框架，使用不可分离的哈密顿量作为说明
系统类。我们将贝叶斯方法与最先进的机器进行比较
规范不可分离哈密顿模型和混沌的学习方法
具有小而嘈杂的训练数据集的双摆模型。结果显示
使用贝叶斯后验作为训练目标可以产生以上
使用训练数据将哈密顿均方误差提高 724 倍
与标准训练目标相比，乘性噪声高达 10%。
最后，我们展示了新的参数算法的实用性
空间离散非线性 64 维模型的估计
Schr\&quot;odinger 方程的数据被高达 20% 的乘性噪声损坏。
]]></description>
      <guid>http://arxiv.org/abs/2401.12476</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>深度学习的非参数逻辑回归。 （arXiv：2401.12482v1 [数学.ST]）</title>
      <link>http://arxiv.org/abs/2401.12482</link>
      <description><![CDATA[考虑非参数逻辑回归问题。在物流方面
回归时，我们通常考虑最大似然估计，
超额风险是对 Kullback-Leibler (KL) 差异的预期
真实和估计的条件类别概率。然而，在
非参数逻辑回归，KL 散度很容易发散，并且
因此，超额风险的收敛很难证明或者不能证明
抓住。一些现有的研究表明 KL 散度在
强有力的假设。在大多数情况下，我们的目标是估计真实的条件
类概率。因此，它不是分析超额风险本身，而是
足以证明最大似然估计在某些情况下的一致性
合适的度量。本文使用一种简单的统一方法来分析
非参数最大似然估计（NPMLE），我们直接推导
温和条件下 NPMLE 在 Hellinger 距离上的收敛速度
假设。尽管我们的结果与一些现有的结果相似
研究中，我们为这些结果提供了简单且更直接的证明。作为一个
重要的应用，我们推导出 NPMLE 的收敛速度
神经网络并表明导出率几乎达到极小极大
最优率。
]]></description>
      <guid>http://arxiv.org/abs/2401.12482</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>绝热量子支持向量机。 （arXiv：2401.12485v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12485</link>
      <description><![CDATA[绝热量子计算机可以解决困难的优化问题（例如，
二次无约束二元优化问题），而且它们看起来很好
适合训练机器学习模型。在本文中，我们描述了一个
用于训练支持向量机的绝热量子方法。我们表明
我们的量子方法的时间复杂度好一个数量级
比经典方法。接下来我们比较一下我们量子的测试精度
与使用 Scikit-learn 库的经典方法相反的方法
Python 跨五个基准数据集（Iris、威斯康星乳腺癌 (WBC)、
酒、数字和兰贝克）。我们证明我们的量子方法获得了准确性
与经典方法相当。最后，我们进行了可扩展性研究
我们计算量子方法的总训练时间和
特征数量和数据点数量不断增加的经典方法
在训练数据集中。我们的可扩展性结果表明量子方法
与数据集上的经典方法相比，获得了 3.5--4.5 倍的加速
许多（数百万）功能。
]]></description>
      <guid>http://arxiv.org/abs/2401.12485</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>具有 Pfaffian 激活函数的图神经网络的 VC 维。 （arXiv：2401.12362v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.12362</link>
      <description><![CDATA[图神经网络（GNN）近年来已成为一种强大的工具
以数据驱动的方式学习跨广泛图形领域的任务；
基于消息传递机制的 GNN 越来越受欢迎
由于其直观的表述，与 Weisfeiler-Lehman 密切相关
（WL）图同构测试，它们已被证明是等价的。来自一个
从理论角度来看，GNN 已被证明是通用逼近器，
以及它们的泛化能力（即 Vapnik Chervonekis 的界限）
（VC）维度）最近被研究用于分段的 GNN
多项式激活函数。我们工作的目的是扩展这种分析
将 GNN 的 VC 维度与其他常用激活函数进行比较，例如
作为 sigmoid 和双曲正切，使用 Pfaffian 函数的框架
理论。界限是针对架构参数（深度、
神经元数量、输入大小）以及颜色数量
由应用于图域的 1-WL 测试得出。理论
分析得到初步实验研究的支持。
]]></description>
      <guid>http://arxiv.org/abs/2401.12362</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>改进深贝叶斯模型的变分推理。 （arXiv：2401.12418v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12418</link>
      <description><![CDATA[深度学习在过去十年中发生了革命性的变化，处于最前沿
在包括计算机视觉在内的广泛任务中取得了非凡的进步，
自然语言处理和强化学习等等。
然而，众所周知，深度模型是通过最大似然训练的
估计往往过于自信并给出校准不佳的预测。
贝叶斯深度学习试图通过在模型上放置先验来解决这个问题
参数，然后与执行后验的可能性相结合
推理。不幸的是，对于深度模型，真正的后验是棘手的，
迫使用户诉诸近似值。在本论文中，我们探讨了
使用变分推理（VI）作为近似值，因为它在
同时逼近后验并提供下界
边际可能性。如果足够紧，这个下界可以用来优化
超参数并促进模型选择。然而，这种能力已经
很少被充分利用于贝叶斯神经网络，可能
因为实践中通常使用的近似后验可能缺乏
有效限制边际可能性的灵活性。因此我们探索
深度模型贝叶斯学习的三个方面：1）我们问是否
有必要对尽可能多的参数进行推理，或者是否
将其中许多视为可优化的超参数是合理的； 2）我们
提出一个变分后验，提供统一的推理视图
贝叶斯神经网络和深度高斯过程； 3）我们演示如何VI
可以通过分析在某些深度高斯过程模型中得到改进
消除后验对称性，并对 Gram 进行推理
矩阵而不是特征。我们希望我们的贡献能够提供
未来充分实现 VI 承诺的垫脚石。
]]></description>
      <guid>http://arxiv.org/abs/2401.12418</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>非参数回归的迁移学习：非渐近极小极大分析和自适应过程。 （arXiv：2401.12272v1 [stat.ML]）</title>
      <link>http://arxiv.org/abs/2401.12272</link>
      <description><![CDATA[考虑了非参数回归的迁移学习。我们首先学习
该问题的非渐近极小极大风险并开发一种新的估计器
称为置信度阈值估计器，它被证明可以实现
最小最大最优风险达到对数因子。我们的结果表明两个
迁移学习中的独特现象：自动平滑和超加速，
这与传统环境中的非参数回归不同。
然后，我们提出了一种数据驱动算法，自适应地实现极小极大
在各种参数空间中风险高达对数因子。
进行模拟研究以评估数值性能
自适应迁移学习算法，并提供了一个现实世界的例子
证明所提出方法的好处。
]]></description>
      <guid>http://arxiv.org/abs/2401.12272</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>用于目标注释的对比学习和基于循环一致性的转导迁移学习。 （arXiv：2401.12340v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.12340</link>
      <description><![CDATA[注释自动目标识别（ATR）是一项极具挑战性的任务，
主要是由于目标域中标记数据不可用。
因此，有必要通过以下方式构建最佳目标域分类器：
利用源域图像的标记信息。转导式
迁移学习 (TTL) 方法，结合了基于 CycleGAN 的不配对
领域翻译网络之前已在文献中提出过
有效的ATR注释。尽管该方法显示出巨大的潜力
ATR，它严重受到注释性能较低、Fr\&#39;echet 较高的影响
起始距离 (FID) 分数，以及视觉伪影的存在
合成图像。为了解决这些问题，我们提出了一种混合对比
学习基础不配对域翻译（H-CUT）网络，实现
显着降低 FID 分数。它结合了注意力和熵
强调特定领域的区域，生成一个噪声特征混合模块
高变分合成负片和调制噪声对比
估计（MoNCE）损失使用最优重新加权所有负补丁
运输以获得更好的性能。我们提出的对比学习和
基于周期一致性的 TTL (C3TTL) 框架由两个 H-CUT 网络组成
和两个分类器。它同时优化循环一致性、MoNCE 和
身份损失。在 C3TTL 中，通过一个网络使用了两个 H-CUT 网络
双射映射将重建的源域图像输入
预训练分类器来指导最佳目标域分类器。广泛的
对三个 ATR 数据集进行的实验分析表明
提出的 C3TTL 方法在注释民用和军用方面是有效的
车辆以及船舶目标。
]]></description>
      <guid>http://arxiv.org/abs/2401.12340</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>良性过度拟合对对抗鲁棒性的惊人危害。 （arXiv：2401.12236v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.12236</link>
      <description><![CDATA[最近的实证和理论研究已经确立了这一概括
经过训练的大型机器学习模型的能力
（近似或精确）拟合噪声数据。在这项工作中，我们证明了一个令人惊讶的
结果是，即使基本事实本身对于对抗性例子来说是稳健的，
就“标准”而言，良性过度拟合模型是良性的
样本外风险目标，这种良性的过度拟合过程可能是有害的
当样本外数​​据受到对抗性操纵时。更多的
具体来说，我们的主要结果包含两部分：（i）最小范数估计量
过度参数化的线性模型总是会导致对抗性脆弱性
“良性过度拟合”设置； (ii) 我们验证渐近权衡结果
每个山脊的标准风险和“对抗性”风险之间
回归估计量，意味着在合适的条件下这两项
通过任何单一的山脊选择，不能同时变小
正则化参数。此外，在惰性训练制度下，我们
展示两层神经正切核 (NTK) 模型的并行结果，
与深度神经网络中的经验观察相一致。我们的发现
为观察到的令人费解的现象提供了理论见解
实践中，真正的目标函数（例如人类）对于
对抗性攻击，而开始过度拟合的神经网络导致模型
那些不健壮的。
]]></description>
      <guid>http://arxiv.org/abs/2401.12236</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:23 GMT</pubDate>
    </item>
    <item>
      <title>用稀疏牛顿迭代加速 Sinkhorn 算法。 （arXiv：2401.12253v1 [数学.OC]）</title>
      <link>http://arxiv.org/abs/2401.12253</link>
      <description><![CDATA[计算统计分布之间的最佳传输距离为
机器学习的一项基本任务。最近一项显着的进步是
熵正则化和 Sinkhorn 算法，仅利用矩阵
缩放并保证具有近线性运行时间的近似解决方案。
尽管Sinkhorn算法取得了成功，但其运行时间可能仍然很慢
由于收敛可能需要大量迭代。到
实现可能的超指数收敛，我们提出
Sinkhorn-Newton-Sparse (SNS)，Sinkhorn 算法的扩展，通过
引入提前停止矩阵缩放步骤和第二阶段
具有牛顿型子程序。采用变分观点认为
Sinkhorn 算法最大化了凹 Lyapunov 势，我们提供了见解
势函数的 Hessian 矩阵近似稀疏。
Hessian 矩阵的稀疏化可实现快速 $O(n^2)$ 每次迭代
复杂度，与Sinkhorn算法相同。从总迭代次数来看
计数，我们观察到 SNS 算法收敛速度快了几个数量级
涵盖广泛的实际案例，包括最佳交通
经验分布之间并计算 Wasserstein $W_1, W_2$
离散密度的距离。实证表现得到了证实
Hessian 矩阵近似稀疏性的严格界限。
]]></description>
      <guid>http://arxiv.org/abs/2401.12253</guid>
      <pubDate>Wed, 24 Jan 2024 03:14:23 GMT</pubDate>
    </item>
    </channel>
</rss>