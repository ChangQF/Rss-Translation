<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 31 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>解开并减轻任务相似性对持续学习的影响</title>
      <link>https://arxiv.org/abs/2405.20236</link>
      <description><![CDATA[arXiv:2405.20236v1 公告类型：新
摘要：部分相似任务的持续学习对人工神经网络提出了挑战，因为任务相似性既提供了知识转移的机会，也带来了干扰和灾难性遗忘的风险。然而，输入特征和读出模式中的任务相似性如何影响知识转移和遗忘，以及它们如何与持续学习的常见算法相互作用仍不清楚。在这里，我们开发了一个具有潜在结构的线性师生模型，并通过分析表明，高输入特征相似性加上低读出相似性对于知识转移和保留都是灾难性的。相反，相反的情况相对无害。我们的分析进一步表明，任务相关的活动门控以牺牲转移为代价来提高知识保留，而任务相关的可塑性门控在过度参数化的极限下不会影响保留或转移性能。相比之下，基于 Fisher 信息度量的权重正则化显着提高了保留率，无论任务相似性如何，都不会影响转移性能。然而，其在欧几里得空间中的对角线近似和正则化对任务相似性的鲁棒性要差得多。我们在具有潜在变量的置换 MNIST 任务中展示了一致的结果。总的来说，这项工作提供了有关持续学习何时困难以及如何缓解困难的见解。]]></description>
      <guid>https://arxiv.org/abs/2405.20236</guid>
      <pubDate>Fri, 31 May 2024 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>广义光滑条件下多目标优化的收敛性</title>
      <link>https://arxiv.org/abs/2405.19440</link>
      <description><![CDATA[arXiv:2405.19440v1 公告类型：交叉 
摘要：多目标优化（MOO）在多任务学习等各个领域受到越来越多的关注。最近的研究提供了一些有效的算法和理论分析，但它们受到标准$L$平滑或有界梯度假设的限制，这通常不适用于神经网络，例如循环神经网络（RNN）和变压器。在本文中，我们研究了一类更通用和更现实的$\ell$平滑损失函数，其中$\ell$是梯度范数的一般非减函数。我们为$\ell$平滑MOO问题开发了两种新颖的单循环算法，广义平滑多目标梯度下降（GSMGrad）及其随机变体随机广义平滑多目标梯度下降（SGSMGrad），它们近似于最大化目标间最小改进的冲突避免（CA）方向。我们对这两种算法进行了全面的收敛分析，并表明它们在所有迭代中收敛到一个 $\epsilon$ 精确的 Pareto 驻点，并保证 $\epsilon$ 级平均 CA 距离（即更新方向和 CA 方向之间的差距），其中确定性和随机性设置分别需要总共 $\mathcal{O}(\epsilon^{-2})$ 和 $\mathcal{O}(\epsilon^{-4})$ 个样本。我们的算法还可以使用更多样本在每次迭代中保证更紧密的 $\epsilon$ 级 CA 距离。此外，我们提出了一种 GSMGrad 的实用变体，名为 GSMGrad-FA，仅使用恒定级别的时间和空间，同时实现与 GSMGrad 相同的性能保证。我们的实验验证了我们的理论并证明了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.19440</guid>
      <pubDate>Fri, 31 May 2024 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>分布稳健协方差估计量的几何统一：通过扩大模糊集来缩小频谱</title>
      <link>https://arxiv.org/abs/2405.20124</link>
      <description><![CDATA[arXiv:2405.20124v1 公告类型：新
摘要：用于估计高维协方差矩阵的最先进的方法都将样本协方差矩阵的特征值缩小到数据不敏感的收缩目标。底层收缩变换要么是启发式选择的 - 没有令人信服的理论依据 - 要么是考虑到限制性分布假设而最佳选择的。在本文中，我们提出了一种原则性的方法来构建协方差估计量而不施加限制性假设。也就是说，我们研究分布稳健的协方差估计问题，该问题最小化所有接近标称分布的数据分布的最坏情况 Frobenius 误差，其中分布的接近度通过协方差矩阵空间上的散度来衡量。我们确定了这种散度的温和条件，在这些条件下，得到的最小化器代表收缩估计量。我们表明相应的收缩变换与底层散度的几何特性密切相关。我们还证明了我们的稳健估计量是高效可计算的、渐近一致的，并且它们享有有限样本性能保证。我们通过合成由 Kullback-Leibler、Fisher-Rao 和 Wasserstein 散度引起的显式估计量来举例说明我们的通用方法。基于合成数据和真实数据的数值实验表明，我们的稳健估计量与最先进的估计量相媲美。]]></description>
      <guid>https://arxiv.org/abs/2405.20124</guid>
      <pubDate>Fri, 31 May 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>使用多项逻辑函数逼近进行强化学习的随机探索</title>
      <link>https://arxiv.org/abs/2405.20165</link>
      <description><![CDATA[arXiv:2405.20165v1 公告类型：新
摘要：我们研究使用多项逻辑 (MNL) 函数近似的强化学习，其中马尔可夫决策过程 (MDP) 的底层转移概率核由具有状态和动作特征的未知转移核心参数化。对于具有非均匀状态转换的有限视界情景设置，我们提出了具有频率后悔保证的随机探索的可证明有效算法。对于我们的第一个算法 $\texttt{RRL-MNL}$，我们采用乐观抽样以确保估计值函数具有足够频率的乐观性，并确定 $\texttt{RRL-MNL}$ 在统计和计算上都是有效的，实现了 $\tilde{O}(\kappa^{-1} d^{\frac{3}{2}} H^{\frac{3}{2}} \sqrt{T})$ 频率后悔界限，每集计算成本为常数时间。这里，$d$ 是转换核心的维度，$H$ 是视界长度，$T$ 是总步数，$\kappa$ 是与问题相关的常数。尽管 $\texttt{RRL-MNL}$ 简单实用，但它的遗憾界限会随着 $\kappa^{-1}$ 而变化，这在最坏情况下可能会很大。为了改善对 $\kappa^{-1}$ 的依赖，我们提出了 $\texttt{ORRL-MNL}$，它使用 MNL 转换模型的局部梯度信息来估计值函数。我们表明它的频率论遗憾界限是 $\tilde{O}(d^{\frac{3}{2}} H^{\frac{3}{2}} \sqrt{T} + \kappa^{-1} d^2 H^2)$。据我们所知，这些是第一个实现计算和统计效率的 MNL 转换模型的随机 RL 算法。数值实验证明了所提算法的优异性能。]]></description>
      <guid>https://arxiv.org/abs/2405.20165</guid>
      <pubDate>Fri, 31 May 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>过度参数化的神经网络中的对称性：平均场视图</title>
      <link>https://arxiv.org/abs/2405.19995</link>
      <description><![CDATA[arXiv:2405.19995v1 公告类型：新
摘要：我们开发了一种均值场 (MF) 视图，用于研究在数据对称于一般紧凑群 $G$ 的作用下，过参数化人工神经网络 (NN) 的学习动态。我们考虑一类广义浅层 NN，由一组 $N$ 个多层单元给出，使用随机梯度下降 (SGD) 和可能的对称利用 (SL) 技术进行联合训练，例如数据增强 (DA)、特征平均 (FA) 或等变架构 (EA)。我们在每个单个单元的参数空间上引入了弱和强不变定律 (WI 和 SI) 的概念，分别对应于 $G$ 不变分布，以及由群作用固定的参数支持的分布（编码 EA）。这使我们能够定义与取 $N\to\infty$ 兼容的对称模型，并根据描述其 MF 极限的 Wasserstein 梯度流解释 DA、FA 和 EA 的渐近动力学。当激活尊重群作用时，我们表明，对于对称数据，DA、FA 和自由训练的模型遵循完全相同的 MF 动态，该动态保持在 WI 定律空间中并在其中最小化人口风险。我们还给出了一个反例，以说明 SI 定律的一般可实现性。尽管如此，非常值得注意的是，我们表明，即使在自由训练时，一组 SI 定律也由 MF 动态保留。这与有限 $N$ 设置形成鲜明对比，在这种设置中，EA 通常不会由无约束的 SGD 保留。我们说明了我们的研究结果的有效性，因为在师生实验环境中，$N$ 变得越来越大，通过各种 SL 方案训练学生 NN 从 WI、SI 或任意教师模型中学习。最后，我们推导出一个数据驱动的启发式方法来发现支持问题 SI 分布的最大参数子空间，该子空间可用于设计具有最小泛化误差的 EA。]]></description>
      <guid>https://arxiv.org/abs/2405.19995</guid>
      <pubDate>Fri, 31 May 2024 06:19:46 GMT</pubDate>
    </item>
    <item>
      <title>与任务无关的机器学习辅助推理</title>
      <link>https://arxiv.org/abs/2405.20039</link>
      <description><![CDATA[arXiv:2405.20039v1 公告类型：新
摘要：机器学习 (ML) 在科学研究中发挥着越来越重要的作用。与经典统计方法相结合，ML 辅助分析策略在加速研究成果方面显示出巨大的前景。这也开辟了一个全新的方法学研究领域，专注于利用 ML 和统计学来解决数据科学挑战的综合方法。一种迅速流行的研究类型使用 ML 来预测大量样本中未观察到的结果，然后在下游统计推断中使用预测结果。然而，现有的旨在确保这种后预测推断有效性的方法仅限于非常基本的任务，例如线性回归分析。这是因为将这些方法扩展到新的、更复杂的统计任务需要特定于任务的代数推导和软件实现，这忽略了已经为复杂推理任务开发的大量现有软件工具库，并严重限制了实际应用中后预测推断的范围。为了应对这一挑战，我们提出了一种新颖的统计框架，用于与任务无关的 ML 辅助推理。它提供了一种预测后推理解决方案，可以轻松插入到几乎任何已建立的数据分析程序中。它提供了有效且高效的推理，对任意 ML 模型的选择都具有鲁棒性，同时允许将几乎所有现有的分析框架纳入 ML 预测结果的分析中。通过大量实验，我们展示了我们的方法与现有方法相比的有效性、多功能性和优越性。]]></description>
      <guid>https://arxiv.org/abs/2405.20039</guid>
      <pubDate>Fri, 31 May 2024 06:19:46 GMT</pubDate>
    </item>
    <item>
      <title>具有两个潜在向量的统计模型的可识别性：维数关系的重要性及其在图嵌入中的应用</title>
      <link>https://arxiv.org/abs/2405.19760</link>
      <description><![CDATA[arXiv:2405.19760v1 公告类型：新
摘要：统计模型的可识别性是无监督表示学习中的一个关键概念。非线性独立分量分析（ICA）的最新工作采用了辅助数据并建立了可识别条件。本文提出了一种具有单个辅助数据的两个潜在向量的统计模型，推广了非线性ICA，并建立了各种可识别性条件。与以前的工作不同，所提出的模型中的两个潜在向量可以具有任意维度，这一特性使我们能够在可识别性条件下揭示两个潜在向量与辅助数据之间的深刻维数关系。此外，令人惊讶的是，我们证明了所提出的模型的不确定性在某些条件下与\emph{linear} ICA相同：潜在向量中的元素可以恢复到它们的排列和尺度。接下来，我们将可识别性理论应用于图形数据的统计模型。因此，可识别性条件之一包含一个吸引人的暗示：统计模型的可识别性可能取决于图数据中链接权重的最大值。然后，我们提出了一种可识别图嵌入的实用方法。最后，我们通过数值证明，所提出的方法可以很好地恢复潜在向量，并且模型可识别性显然取决于链接权重的最大值，这支持了我们理论结果的暗示]]></description>
      <guid>https://arxiv.org/abs/2405.19760</guid>
      <pubDate>Fri, 31 May 2024 06:19:45 GMT</pubDate>
    </item>
    <item>
      <title>数据损坏下​​的稳健核假设检验</title>
      <link>https://arxiv.org/abs/2405.19912</link>
      <description><![CDATA[arXiv:2405.19912v1 公告类型：新
摘要：我们提出了两种在数据损坏的情况下构建稳健置换测试的通用方法。所提出的测试有效地控制了数据损坏下​​的非渐近 I 型错误，并且我们在最小条件下证明了它们的功效一致性。这有助于在具有潜在对抗性攻击的实际应用中实际部署假设检验。我们的一种方法固有地确保了差异隐私，进一步扩大了其对私人数据分析的适用性。对于双样本和独立性设置，我们表明我们的内核稳健测试是极小最大最优的，即它们保证对内核 MMD 和 HSIC 度量中与零点均匀分离的替代方案具有非渐近效力，并且以某种最佳速率（紧密匹配下限）。最后，我们提供公开可用的实现并通过经验说明了我们提出的测试的实用性。]]></description>
      <guid>https://arxiv.org/abs/2405.19912</guid>
      <pubDate>Fri, 31 May 2024 06:19:45 GMT</pubDate>
    </item>
    <item>
      <title>通过 Hellinger 相关性增强充分降维</title>
      <link>https://arxiv.org/abs/2405.19704</link>
      <description><![CDATA[arXiv:2405.19704v1 公告类型：新
摘要：在这项工作中，我们开发了一种在单指标模型中进行充分降维（SDR）的新理论和方法，其中 SDR 是基于条件独立性的监督降维的一个子领域。我们的工作主要受到最近引入的 Hellinger 相关性作为依赖性度量的推动。利用这一措施，我们开发了一种能够有效检测降维子空间的方法，并提供了理论依据。通过大量的数值实验，我们证明了我们提出的方法显著增强并优于现有的 SDR 方法。这一改进主要归功于我们提出的方法对数据依赖性的更深入理解以及对现有 SDR 技术的改进。]]></description>
      <guid>https://arxiv.org/abs/2405.19704</guid>
      <pubDate>Fri, 31 May 2024 06:19:44 GMT</pubDate>
    </item>
    <item>
      <title>因子增强张量神经网络</title>
      <link>https://arxiv.org/abs/2405.19610</link>
      <description><![CDATA[arXiv:2405.19610v1 公告类型：新
摘要：本文研究张量对张量回归的预测任务，其中协变量和响应都是跨时间的多维数组（又名张量），具有任意张量顺序和数据维度。现有方法要么侧重于线性模型而不考虑协变量和响应之间可能存在的非线性关系，要么直接采用未能利用固有张量结构的黑盒深度学习算法。在这项工作中，我们提出了一种因子增强张量对张量神经网络 (FATTNN)，将张量因子模型集成到深度神经网络中。我们首先从复杂结构化的张量协变量中总结和提取有用的预测信息（由“因子张量”表示），然后使用估计的因子张量作为时间卷积神经网络的输入继续进行预测任务。所提出的方法有效地处理了复杂数据结构之间的非线性，并且在预测精度和计算成本方面都比传统统计模型和传统深度学习方法有所改进。通过利用张量因子模型，我们提出的方法利用底层的潜在因子结构来增强预测，同时大幅降低数据维数，从而加快计算速度。我们提出的方法的经验性能通过模拟研究和对三个公共数据集的实际应用得到证明。数值结果表明，与基准方法相比，我们提出的算法实现了预测精度的大幅提高和计算时间的大幅减少。]]></description>
      <guid>https://arxiv.org/abs/2405.19610</guid>
      <pubDate>Fri, 31 May 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯在线自然梯度 (BONG)</title>
      <link>https://arxiv.org/abs/2405.19681</link>
      <description><![CDATA[arXiv:2405.19681v1 公告类型：新
摘要：我们提出了一种基于变分贝叶斯的顺序贝叶斯推理的新方法。关键见解是，在在线设置中，我们不需要添加 KL 项来正则化先验（来自前一个时间步的后验）；相反，我们可以只优化预期的对数似然，从先验预测开始执行单步自然梯度下降。我们证明，如果模型是共轭的，该方法可以恢复精确的贝叶斯推理，并且在非共轭设置中经验上优于其他在线 VB 方法，例如神经网络的在线学习，尤其是在控制计算成本时。]]></description>
      <guid>https://arxiv.org/abs/2405.19681</guid>
      <pubDate>Fri, 31 May 2024 06:19:43 GMT</pubDate>
    </item>
    <item>
      <title>流数据工具变量回归的随机优化算法</title>
      <link>https://arxiv.org/abs/2405.19463</link>
      <description><![CDATA[arXiv:2405.19463v1 公告类型：新
摘要：我们通过将问题视为条件随机优化问题来开发和分析工具变量回归算法。在最小二乘工具变量回归的背景下，我们的算法既不需要矩阵求逆也不需要小批量，并且提供了一种使用流数据执行工具变量回归的完全在线方法。当真实模型是线性时，我们得出期望的收敛速度，对于任何 $\iota&gt;0$，在双样本和单样本预言机的可用性下，分别为 $\mathcal{O}(\log T/T)$ 和 $\mathcal{O}(1/T^{1-\iota})$ 阶，其中 $T$ 是迭代次数。重要的是，在双样本预言机可用的情况下，我们的程序避免了明确建模和估计混杂因素与工具变量之间的关系，证明了所提出的方法优于最近将问题重新表述为极小极大优化问题的研究。提供了数值实验来证实理论结果。]]></description>
      <guid>https://arxiv.org/abs/2405.19463</guid>
      <pubDate>Fri, 31 May 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>海量数据的在线非参数监督学习</title>
      <link>https://arxiv.org/abs/2405.19486</link>
      <description><![CDATA[arXiv:2405.19486v1 公告类型：新
摘要：尽管参数机器学习算法（例如线性判别分析、二次判别分析或逻辑回归）具有简单、计算成本低和数据要求低等优点，但它们也存在严重的缺点，包括线性、特征与通常施加的正态分布的拟合度差和维数高。基于批处理核的非参数分类器克服了特征约束的线性和正态性，是监督分类问题的一种有趣替代方案。然而，它受到“维数灾难”的困扰。大数据时代的样本量爆炸式增长可以缓解这一问题，但大规模数据量也给数据的存储和分类器的计算带来了一些挑战。这些挑战使得经典的批量非参数分类器不再适用。这促使我们开发一种快速算法，用于在海量数据和流数据框架中实时计算非参数分类器。该在线分类器包括两个步骤。首先，我们考虑在线主成分分析，以非常低的计算成本降低特征的维数。然后，部署随机近似算法来实时计算非参数分类器。对所提出的方法进行了评估，并与一些常用的胎儿健康实时监测机器学习算法进行了比较。研究表明，在准确性方面，离线（或批量）和在线分类器都是随机森林算法的良好竞争对手。此外，我们表明，与离线分类器相比，在线分类器提供了最佳的权衡准确度/计算成本。]]></description>
      <guid>https://arxiv.org/abs/2405.19486</guid>
      <pubDate>Fri, 31 May 2024 06:19:42 GMT</pubDate>
    </item>
    <item>
      <title>近似汤普森抽样用于学习具有 $O(\sqrt{T})$ 遗憾的线性二次调节器</title>
      <link>https://arxiv.org/abs/2405.19380</link>
      <description><![CDATA[arXiv:2405.19380v1 公告类型：新
摘要：我们提出了一种近似汤普森采样算法，该算法学习线性二次调节器 (LQR)，其改进的贝叶斯遗憾界限为 $O(\sqrt{T})$。我们的方法利用朗之万动力学，采用精心设计的预处理器以及简单的激励机制。我们表明，激励信号会导致预处理器的最小特征值随时间增长，从而加速近似后验采样过程。此外，我们确定了由我们的算法生成的近似后验的非平凡集中特性。这些属性使我们能够限制系统状态的矩并实现 $O(\sqrt{T})$ 遗憾界限，而无需文献中经常使用的参数集的不切实际的限制性假设。]]></description>
      <guid>https://arxiv.org/abs/2405.19380</guid>
      <pubDate>Fri, 31 May 2024 06:19:41 GMT</pubDate>
    </item>
    <item>
      <title>最优多类 U 型校准误差及其他</title>
      <link>https://arxiv.org/abs/2405.19374</link>
      <description><![CDATA[arXiv:2405.19374v1 公告类型：新
摘要：我们考虑在线多类 U 校准问题，其中预测者旨在对 $K$ 个类进行连续分布预测，同时具有较低的 U 校准误差，即对所有有界适当损失的遗憾值较低。Kleinberg 等人 (2023) 开发了一种算法，经过 $T$ 轮后 U 校准误差为 $O(K\sqrt{T})$，并提出了最佳界限是什么的悬而未决的问题。我们通过展示最优 U 校准误差为 $\Theta(\sqrt{KT})$ 来解决这个问题——我们从一个简单的观察开始，即 Daskalakis 和 Syrgkanis (2016) 的 Follow-the-Perturbed-Leader 算法实现了这个上限，然后用特定的适当损失构造了一个匹配的下限（作为一个附带结果，这也证明了 Daskalakis 和 Syrgkanis (2016) 算法在在线学习中对抗具有有限选择的对手的最优性）。我们还在损失函数的自然假设下加强了我们的结果，包括 Lipschitz 适当损失的 $\Theta(\log T)$ U 校准误差、某类可分解适当损失的 $O(\log T)$ U 校准误差、具有低覆盖数的适当损失的 U 校准误差界限等。]]></description>
      <guid>https://arxiv.org/abs/2405.19374</guid>
      <pubDate>Fri, 31 May 2024 06:19:40 GMT</pubDate>
    </item>
    </channel>
</rss>