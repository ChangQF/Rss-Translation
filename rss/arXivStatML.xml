<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 stat.ML 更新</title>
    <link>http://rss.arxiv.org/rss/stat.ML</link>
    <description>stat.ML 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Thu, 01 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>针对法学硕士 (LLM) 的经济高效的幻觉检测</title>
      <link>https://arxiv.org/abs/2407.21424</link>
      <description><![CDATA[arXiv:2407.21424v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 容易产生幻觉 - 产生不可靠的输出，这些输出不忠于其输入、外部事实或内部不一致。在这项工作中，我们解决了生产环境中事后幻觉检测的几个挑战。我们的幻觉检测流程包括：首先，产生一个置信度分数，表示生成的答案是幻觉的可能性；其次，根据输入和候选响应的属性校准分数；最后，通过对校准分数进行阈值化来执行检测。我们在不同的数据集上对各种最先进的评分方法进行了基准测试，包括问答、事实核查和总结任务。我们采用不同的 LLM 来确保对性能进行全面的评估。我们表明，校准单个评分方法对于确保风险意识的下游决策至关重要。基于没有一种单一评分在所有情况下都表现最佳的发现，我们提出了一个多重评分框架，该框架结合了不同的评分并在所有数据集上实现了最佳性能。我们进一步引入了具有成本效益的多重评分，它可以匹敌甚至超越更昂贵的检测方法，同时显著降低计算开销。]]></description>
      <guid>https://arxiv.org/abs/2407.21424</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>Wasserstein 空间中的流形学习</title>
      <link>https://arxiv.org/abs/2311.08549</link>
      <description><![CDATA[arXiv:2311.08549v2 公告类型：替换 
摘要：本文旨在为绝对连续概率测度空间中的流形学习算法建立理论基础，该空间是 $\mathbb{R}^d$ 的紧凸子集，用 Wasserstein-2 距离 $\mathrm{W}$ 度量化。我们首先介绍一个概率测度子流形 $\Lambda$ 的构造，该子流形配备度量 $\mathrm{W}_\Lambda$，即 $W$ 到 $\Lambda$ 的测地线限制。与其他构造相比，这些子流形不一定是平坦的，但仍允许以类似于 $\mathbb{R}^d$ 的黎曼子流形的方式进行局部线性化。然后，我们展示了如何从 $\Lambda$ 的样本 $\{\lambda_i\}_{i=1}^N$ 和成对的外部 Wasserstein 距离 $\mathrm{W}$ 中学习 $(\Lambda,\mathrm{W}_{\Lambda})$ 的潜在流形结构。具体而言，我们展示了度量空间 $(\Lambda,\mathrm{W}_{\Lambda})$ 可以从节点 $\{\lambda_i\}_{i=1}^N$ 和边权重 $W(\lambda_i,\lambda_j)$ 的图中以 Gromov-Wasserstein 意义渐近地恢复。此外，我们展示了如何通过对合适的“协方差算子”进行谱分析，使用从 $\lambda$ 到足够接近且不同的样本 $\{\lambda_i\}_{i=1}^N$ 的最优传输映射，渐近地恢复样本 $\lambda$ 处的切线空间。本文最后介绍了一些子流形 $\Lambda$ 的显式构造以及通过谱分析恢复切线空间的数值示例。]]></description>
      <guid>https://arxiv.org/abs/2311.08549</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>带有不确定性评估的无模型预测</title>
      <link>https://arxiv.org/abs/2405.12684</link>
      <description><![CDATA[arXiv:2405.12684v4 公告类型：替换 
摘要：深度非参数回归以利用深度神经网络学习目标函数为特征，近年来已成为研究关注的焦点。尽管在理解收敛速度方面取得了相当大的进展，但缺乏渐近性质阻碍了严格的统计推断。为了解决这一差距，我们提出了一个新颖的框架，利用条件扩散模型将深度估计范式转变为有利于条件均值估计的平台。从理论上讲，我们为条件扩散模型开发了端到端收敛速度，并建立了生成样本的渐近正态性。因此，我们能够构建置信区域，从而促进稳健的统计推断。此外，通过数值实验，我们实证验证了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.12684</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>非参数 IV 模型中的自适应速率最优假设检验</title>
      <link>https://arxiv.org/abs/2006.09587</link>
      <description><![CDATA[arXiv:2006.09587v5 公告类型：replace-cross 
摘要：我们提出了一种新的自适应假设检验方法，用于非参数工具变量 (NPIV) 模型中结构函数的不等式（例如单调性、凸性）和等式（例如参数、半参数）限​​制。我们的检验统计量基于限制和非限制筛分两阶段最小二乘估计量之间的二次距离的改进留一样本模拟。我们提供计算简单、数据驱动的筛分调整参数和 Bonferroni 调整的卡方临界值选择。我们的测试适应未知内生性程度和未知工具强度的情况下替代函数的未知平滑度。它在 $L^2$ 中达到自适应极小极大测试率。也就是说，对于未知规律的 NPIV 模型，任何其他检验都无法最小化复合零假设上的 I 类误差的上确界和非参数替代模型上的 II 类误差的上确界之和。通过反转自适应检验，可获得 $L^2$ 中的置信集。模拟证实，在不同强度的工具和样本量中，我们的自适应检验控制了规模，其有限样本功效大大超过了现有的非自适应 NPIV 模型单调性和参数限制检验。介绍了用于检验差异化产品需求和恩格尔曲线形状限制的实证应用。]]></description>
      <guid>https://arxiv.org/abs/2006.09587</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>利用广义值函数近似实现可处理且可证明有效的分布式强化学习</title>
      <link>https://arxiv.org/abs/2407.21260</link>
      <description><![CDATA[arXiv:2407.21260v1 公告类型：交叉 
摘要：分布式强化学习通过有效捕捉环境随机性来提高性能，但对其有效性的全面理论理解仍然难以捉摸。在本文中，我们提出了在有限情景马尔可夫决策过程设置中使用一般价值函数近似的分布式强化学习的遗憾分析。我们首先引入了一个关键概念，即贝尔曼无偏性，用于通过统计函数动态规划进行可处理且可精确学习的更新。我们的理论结果表明，用有限数量的矩函数近似无限维回报分布是无偏学习统计信息的唯一方法，包括非线性统计函数。其次，我们提出了一种可证明有效的算法 $\texttt{SF-LSVI}$，实现了 $\tilde{O}(d_E H^{\frac{3}{2}}\sqrt{K})$ 的遗憾界限，其中 $H$ 是范围，$K$ 是情节数量，$d_E$ 是函数类的逃避维度。]]></description>
      <guid>https://arxiv.org/abs/2407.21260</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:52 GMT</pubDate>
    </item>
    <item>
      <title>用于非线性同化的状态观测增强扩散模型</title>
      <link>https://arxiv.org/abs/2407.21314</link>
      <description><![CDATA[arXiv:2407.21314v1 公告类型：交叉 
摘要：数据同化已成为一种关键技术，旨在将物理模型与观测数据相结合以估计状态变量。传统的同化算法通常面临物理和观测模型带来的高非线性挑战。在这项工作中，我们提出了一种基于生成模型的新型数据驱动同化算法来解决这些问题。我们的状态观察增强扩散 (SOAD) 模型旨在更有效地处理非线性物理和观测模型。与 SOAD 相关的边际后验已被推导，然后在温和的假设下被证明与真实后验相匹配，这显示出比以前基于分数的同化工作的理论优势。实验结果还表明，我们的 SOAD 模型可能比现有的数据驱动方法提供更高的准确性。]]></description>
      <guid>https://arxiv.org/abs/2407.21314</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:52 GMT</pubDate>
    </item>
    <item>
      <title>两种完全无参数的交替梯度投影算法，用于非凸（强）凹极小极大问题</title>
      <link>https://arxiv.org/abs/2407.21372</link>
      <description><![CDATA[arXiv:2407.21372v1 公告类型：交叉 
摘要：由于其在各种新兴应用中的重要性，解决极小极大问题的有效算法最近受到越来越多的关注。然而，许多现有算法需要事先了解问题参数才能实现最佳迭代复杂度。在本文中，我们提出了一种完全无参数交替梯度投影 (PF-AGP) 算法，使用回溯策略解决光滑非凸（强）凹极小极大问题，这不需要事先了解 Lipschtiz 常数 $L$ 或强凹常数 $\mu$ 等参数。PF-AGP 算法利用无参数梯度投影步骤在每次迭代中交替更新外部和内部变量。我们证明了 PF-AGP 算法求得非凸强凹极小极大问题的 $\varepsilon$ 驻点的梯度调用总数的上限为 $\mathcal{O}\left( L\kappa^3\varepsilon^{-2} \right)$，其中 $\kappa$ 为条件数，而求得非凸凹极小极大问题的 $\varepsilon$ 驻点的梯度调用总数的上限为 $\mathcal{O}\left( L^4\varepsilon^{-4} \right)$。据我们所知，这是第一个完全无参数的非凸强凹极小极大问题的算法，也是在单循环方法中实现最佳迭代复杂度的非凸凹极小极大问题的完全无参数算法。数值结果验证了所提出的PF-AGP算法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.21372</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:52 GMT</pubDate>
    </item>
    <item>
      <title>对称空间上的 Whitney 扩展定理，一个例子</title>
      <link>https://arxiv.org/abs/2407.21420</link>
      <description><![CDATA[arXiv:2407.21420v1 公告类型：交叉 
摘要：H. Whitney 于 1934 年提出了将 $\mathbb{R}^n$ 中一组点上的函数扩展为环境空间上的解析函数的问题。在本文中，我们证明了某些齐次空间上数据的 Whitney 型扩展定理。我们在齐次空间上使用谐波分析和紧和非紧约化群的表示理论。]]></description>
      <guid>https://arxiv.org/abs/2407.21420</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:52 GMT</pubDate>
    </item>
    <item>
      <title>Adam 优化器的收敛速度</title>
      <link>https://arxiv.org/abs/2407.21078</link>
      <description><![CDATA[arXiv:2407.21078v1 公告类型：交叉 
摘要：随机梯度下降 (SGD) 优化方法如今已成为人工智能系统中深度神经网络 (DNN) 训练的首选方法。在实际相关的训练问题中，通常不会采用普通的标准 SGD 方法来优化，而是采用适当加速和自适应的 SGD 优化方法。截至今天，这种加速和自适应 SGD 优化方法最流行的变体可能是 Kingma &amp; Ba 于 2014 年提出的著名 Adam 优化器。尽管 Adam 优化器在实现中很受欢迎，但即使在目标函数（想要最小化的函数）强凸的简单二次随机优化问题的情况下，为 Adam 优化器提供收敛分析仍然是一个悬而未决的研究问题。在本研究中，我们通过为一大类随机优化问题（特别是简单的二次随机优化问题）建立 Adam 优化器的最优收敛率来解决这一问题。我们收敛分析的关键要素是一个新的矢量场函数，我们建议将其称为 Adam 矢量场。这个 Adam 矢量场准确地描述了 Adam 优化过程的宏观行为，但不同于所考虑的随机优化问题的目标函数（我们打算最小化的函数）的负梯度。具体而言，我们的收敛分析表明，Adam 优化器通常不会收敛到所考虑的优化问题的目标函数的临界点（目标函数梯度的零点），但会以这个 Adam 矢量场的零点速率收敛。]]></description>
      <guid>https://arxiv.org/abs/2407.21078</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:51 GMT</pubDate>
    </item>
    <item>
      <title>利用自监督早期退出加速大型语言模型推理</title>
      <link>https://arxiv.org/abs/2407.21082</link>
      <description><![CDATA[arXiv:2407.21082v1 公告类型：交叉 
摘要：本文提出了一种通过在推理过程中引入早期退出来加速大型预训练语言模型 (LLM) 推理的新技术。这些模型在广泛的应用中使用，其计算需求可能很大。通过利用 token 复杂性的固有可变性，我们的方法可以选择性地加速推理过程。具体来说，我们建议在现有的 Transformer 层之上集成早期退出的“头部”，这有助于基于置信度指标的条件终止。这些头部以自监督的方式进行训练，使用模型自己的预测作为训练数据，从而无需额外的注释数据。使用校准集建立的置信度指标可确保所需的准确度，同时在置信度超过预定阈值时实现提前终止。值得注意的是，我们的方法保留了原始准确度并减少了某些任务的计算时间，利用了预训练 LLM 的现有知识，而无需进行大量的再训练。这种轻量级、模块化的修改有可能极大地增强 LLM 的实际可用性，特别是在资源受限环境中的实时语言处理等应用中。]]></description>
      <guid>https://arxiv.org/abs/2407.21082</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:51 GMT</pubDate>
    </item>
    <item>
      <title>DKL-KAN：使用 Kolmogorov-Arnold 网络的可扩展深度核学习</title>
      <link>https://arxiv.org/abs/2407.21176</link>
      <description><![CDATA[arXiv:2407.21176v1 公告类型：交叉 
摘要：机器学习中对可扩展和富有表现力的模型的需求至关重要，特别是在需要结构深度和灵活性的应用中。传统的深度学习方法，例如多层感知器 (MLP)，提供了深度，但缺乏将深度学习架构的结构特征与核方法的非参数灵活性相结合的能力。为了解决这个问题，引入了深度核学习 (DKL)，其中使用深度学习架构转换基础核的输入。这些内核可以替代标准内核，同时具有表达能力和可扩展性。Kolmogorov-Arnold 网络 (KAN) 的出现引起了科学领域研究人员的广泛关注和讨论。在本文中，我们引入了一种使用 KAN (DKL-KAN) 的可扩展深度内核，作为使用 MLP (DKL-MLP) 的 DKL 的有效替代方案。我们的方法涉及在高斯过程框架内使用边际似然同时优化这些内核属性。为了与 DKL-MLP 进行公平比较，我们分析了 DKL-KAN 的两个变体：一个变体具有与 DKL-MLP 相同的神经元和层数，另一个变体具有大致相同的可训练参数数量。为了处理大型数据集，我们对低维输入使用可扩展结构化高斯过程 (KISS-GP) 的核插值，对高维输入使用具有乘积核的 KISS-GP。根据广泛应用中的计算训练时间和测试预测准确性来评估 DKL-KAN 的有效性。此外，还检查了 DKL-KAN 在建模不连续性和准确估计预测不确定性方面的有效性。结果表明，DKL-KAN 在观测数量较少的数据集上的表现优于 DKL-MLP。相反，DKL-MLP 在具有大量观测的数据集中表现出更好的可扩展性和更高的测试预测准确性。]]></description>
      <guid>https://arxiv.org/abs/2407.21176</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:51 GMT</pubDate>
    </item>
    <item>
      <title>使用时间序列预测和蒙特卡罗技术分析面向客户的供应商体验</title>
      <link>https://arxiv.org/abs/2407.21193</link>
      <description><![CDATA[arXiv:2407.21193v1 公告类型：新
摘要：eBay 与外部供应商合作，允许客户自由选择供应商来完成他们的 eBay 体验。但是，供应商中断可能会妨碍客户体验。因此，eBay 可以禁用有问题的供应商以防止客户流失。太晚禁用供应商可能会失去愿意转换到其他供应商的客户，而过早禁用供应商可能会失去那些不愿意转换的客户。在本文中，我们提出了一个数据驱动的解决方案来回答 eBay 是否应该禁用有问题的供应商以及何时禁用它。我们的解决方案涉及预测客户行为。首先，我们使用乘法季节性模型来表示如果所有供应商都完全正常运作时的行为。接下来，我们使用蒙特卡罗模拟来表示如果有问题的供应商保持启用时的行为。最后，我们使用线性模型来表示如果供应商被禁用时的行为。通过比较这些预测，我们确定了 eBay 禁用有问题的供应商的最佳时间。]]></description>
      <guid>https://arxiv.org/abs/2407.21193</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:50 GMT</pubDate>
    </item>
    <item>
      <title>用于流形概率学习的瞬态各向异性核</title>
      <link>https://arxiv.org/abs/2407.21435</link>
      <description><![CDATA[arXiv:2407.21435v1 公告类型：新
摘要：PLoM（流形上的概率学习）是一种于 2016 年推出的方法，用于处理小型训练数据集，通过从随机耗散汉密尔顿动力系统中投影 It\^o 方程来充当 MCMC 生成器，其中训练数据集的 KDE 估计概率测度是不变测度。PLoM 使用由时间无关的各向同性核构建的扩散图 (DMAPS) 基对与训练数据集相关的降阶向量基进行投影。在本文中，我们提出了一种由瞬态各向异性核构建的新型 ISDE 投影向量基，为 DMAPS 基提供了一种替代方案，以改进具有异质数据的随机流形的统计替代。该构造确保在接近初始时间的时间，DMAPS 基与瞬态基重合。对于较大的时间，两个基之间的差异由它们跨越的向量子空间的角度来表征。产生最佳瞬态基的最佳时刻是使用信息论中的互信息估计来确定的，该估计通过熵估计进行归一化，以考虑估计中使用的实现数量的影响。因此，这个新的向量基可以更好地表示任何维度的学习概率度量中的统计依赖性。三个具有不同统计复杂性和数据异质性的应用程序验证了所提出的理论，表明瞬态各向异性核可以改善学习概率度量。]]></description>
      <guid>https://arxiv.org/abs/2407.21435</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:50 GMT</pubDate>
    </item>
    <item>
      <title>扩展基准推断：迈向统计推断的自动化过程</title>
      <link>https://arxiv.org/abs/2407.21622</link>
      <description><![CDATA[arXiv:2407.21622v1 公告类型：新
摘要：虽然 R.A. Fisher 的基准推断被广泛认为是一个大错误，但他最初设定的目标——“根据观察推断模型参数的不确定性”——一直被许多统计学家所追求。为此，我们开发了一种称为扩展基准推断 (EFI) 的新统计推断方法。新方法通过利用先进的统计计算技术实现了基准推断的目标，同时保持了对大数据的可扩展性。EFI 涉及使用随机梯度马尔可夫链蒙特卡罗联合输入观测中实现的随机误差，并使用稀疏深度神经网络 (DNN) 估计逆函数。稀疏 DNN 估计器的一致性确保观测中嵌入的不确定性通过估计的逆函数正确传播到模型参数，从而验证下游统计推断。与频率学派和贝叶斯方法相比，EFI 在参数估计和假设检验方面具有显着优势。具体来说，EFI 在参数估计方面提供了更高的保真度，尤其是在观测值中存在异常值时；并且消除了假设检验中对理论参考分布的需求，从而使统计推断过程自动化。EFI 还为半监督学习提供了一个创新框架。]]></description>
      <guid>https://arxiv.org/abs/2407.21622</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:50 GMT</pubDate>
    </item>
    <item>
      <title>高频做市中的强化学习</title>
      <link>https://arxiv.org/abs/2407.21025</link>
      <description><![CDATA[arXiv:2407.21025v1 公告类型：交叉 
摘要：本文为强化学习（RL）在高频做市中的应用建立了一种新的、全面的理论分析。我们将现代 RL 理论与高频金融经济学中的连续时间统计模型联系起来。与大多数现有的关于开发各种 RL 方法用于做市问题的方法论研究文献不同，我们的工作是一项提供理论分析的试验。我们针对采样频率的影响，发现在调整时间增量 $\Delta$ $ 的值时，RL 算法的误差和复杂性之间存在有趣的权衡 - $\Delta$ 变小，误差会变小，但复杂性会变大。我们还研究了一般和博弈框架下的双人案例，并建立了纳什均衡向连续时间博弈均衡的收敛，即 $\Delta\rightarrow0$。采用纳什 Q 学习算法（一种在线多智能体 RL 方法）来解决均衡问题。我们的理论不仅有助于从业者选择采样频率，而且非常通用，适用于其他高频金融决策问题，例如最佳执行，只要采用连续时间马尔可夫决策过程的时间离散化即可。蒙特卡洛模拟证据支持我们所有的理论。]]></description>
      <guid>https://arxiv.org/abs/2407.21025</guid>
      <pubDate>Fri, 02 Aug 2024 03:17:50 GMT</pubDate>
    </item>
    </channel>
</rss>