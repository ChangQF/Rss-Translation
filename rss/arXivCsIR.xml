<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.IR 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 在 arXiv.org 电子印刷档案上进行更新。</description>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>带有嵌入的知识图谱可视化：关于最新趋势和方法的论文</title>
      <link>https://arxiv.org/abs/2412.05289</link>
      <description><![CDATA[arXiv:2412.05289v1 公告类型：新
摘要：在本文中，我们讨论了知识图谱的可视化分析和探索的最新趋势，特别是与知识图谱嵌入技术相结合。我们概述了知识图谱可视化技术和框架的现状，并讨论了四个已确定的挑战。可视化知识图谱的挑战包括对直观和模块化界面的需求、处理大数据的性能以及用户理解和使用查询语言的困难。我们发现框架通常可以满足直观的 UI、性能和查询支持要求，但很少有框架可以满足模块化要求。在知识图谱嵌入的背景下，我们将使用嵌入来促进知识图谱探索的方法与旨在解释嵌入本身的方法区分开来。我们发现这两种观点之间存在显著差异。最后，我们强调了未来工作的一些可能方向，包括未满足需求的传播、新视觉特征的实现、以及关系可视化作为知识图谱特殊元素的实验。]]></description>
      <guid>https://arxiv.org/abs/2412.05289</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PyTerrier-GenRank：用于使用大型语言模型进行重新排序的 PyTerrier 插件</title>
      <link>https://arxiv.org/abs/2412.05339</link>
      <description><![CDATA[arXiv:2412.05339v1 公告类型：新
摘要：使用 LLM 作为重新排序器需要试验各种超参数，例如提示格式、模型选择和重新表述策略。我们引入了 PyTerrier-GenRank，这是一个 PyTerrier 插件，用于促进使用 LLM 进行无缝重新排序实验，支持流行的排名策略，如逐点和逐列表提示。我们通过 HuggingFace 和 OpenAI 托管端点验证我们的插件。]]></description>
      <guid>https://arxiv.org/abs/2412.05339</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ULMRec：以用户为中心的顺序推荐大型语言模型</title>
      <link>https://arxiv.org/abs/2412.05543</link>
      <description><![CDATA[arXiv:2412.05543v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展利用其卓越的语言理解能力，在顺序推荐任务中表现出色。然而，现有的基于 LLM 的推荐方法主要侧重于对项目级共现模式进行建模，而未能充分捕捉用户级个性化偏好。这是有问题的，因为即使表现出相似行为模式（例如，点击或购买类似商品）的用户也可能有根本不同的潜在兴趣。为了缓解这个问题，在本文中，我们提出了 ULMRec，这是一个有效地将用户个性化偏好集成到 LLM 中进行顺序推荐的框架。考虑到项目 ID 和 LLM 之间存在语义差距，我们将项目 ID 替换为用户历史行为中的相应标题，使模型能够捕获项目语义。为了整合用户的个性化偏好，我们设计了两个关键组件：（1）用户索引：一种个性化的用户索引机制，利用用户评论和用户 ID 上的矢量量化来生成有意义且独特的用户表示；（2）对齐调整：基于对齐的调整阶段，采用全面的偏好对齐任务来增强模型捕获个性化信息的能力。通过这种设计，ULMRec 实现了语言语义与用户个性化偏好的深度融合，从而有助于有效地适应推荐。在两个公共数据集上进行的大量实验表明，ULMRec 明显优于现有方法，验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.05543</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KG-Retriever：针对检索增强大型语言模型的高效知识索引</title>
      <link>https://arxiv.org/abs/2412.05547</link>
      <description><![CDATA[arXiv:2412.05547v1 公告类型：新
摘要：具有检索增强生成的大型语言模型在复杂的检索任务中遇到了关键挑战，例如多跳问答，这需要模型跨多个文档导航并根据碎片化信息生成全面的响应。为了应对这一挑战，我们引入了一种基于知识图谱的新型 RAG 框架，该框架具有分层知识检索器，称为 KG-Retriever。KG-Retriever 中的检索索引构建在由知识图谱层和协作文档层组成的分层索引图上。充分利用图结构的关联性来加强文档内和文档间的连通性，从而从根本上缓解信息碎片化问题，同时提高 LLM 跨文档检索的检索效率。借助来自邻近文档的粗粒度协作信息和来自知识图谱的简洁信息，KG-Retriever 在五个公共 QA 数据集上取得了显著的改进，展示了我们提出的 RAG 框架的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2412.05547</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从缺乏上下文信息的产品目录中的结构化数据中自动提取和创建 FBS 设计推理知识图</title>
      <link>https://arxiv.org/abs/2412.05868</link>
      <description><![CDATA[arXiv:2412.05868v1 公告类型：新
摘要：基于本体的知识图谱 (KG) 适用于有效的知识管理和在各种决策场景（包括设计）中重用。除非开发用于知识提取和图形创建的自动化流程，否则基于特定本体模型创建和填充广泛的 KG 可能非常耗费人力和时间。大多数关于自动提取和创建 KG 的研究和开发都基于提供上下文信息的大量非结构化数据集。然而，一些关于公司产品和服务的最有用的信息传统上被记录为结构化数据。这种结构化数据集很少遵循标准本体，不捕获实体之间关系的明确映射，也不提供上下文信息。因此，本研究报告了一种为解决这一差距而开发的方法和数字工作流程。开发的方法和工作流程采用基于规则的技术从遗留结构化数据（尤其是规格表和产品目录）中提取和创建基于功能行为结构 (FBS) 本体的 KG。解决方案方法由两个主要部分组成：用于为 FBS 本体概念导出上下文和基于上下文的分类规则的过程，以及用于填充和检索基于 FBS 本体的 KG 的工作流。KG 和自然语言处理 (NLP) 用于自动提取、表示和检索知识。通过在工业环境中的试点实施证明了该工作流的有效性。报告了从试点研究中获得的有关挑战和机遇的见解，包括讨论 FBS 本体和概念。]]></description>
      <guid>https://arxiv.org/abs/2412.05868</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>学习近似最近邻搜索的聚类代表</title>
      <link>https://arxiv.org/abs/2412.05921</link>
      <description><![CDATA[arXiv:2412.05921v1 公告类型：新
摘要：开发越来越高效和准确的近似最近邻搜索算法是现代信息检索的首要目标。解决这个问题的主要方法是聚类，它涉及将数据集划分为不同的组，每个组由一个代表性数据点表征。通过这种方法，检索查询的前 k 个数据点需要根据它们的代表识别最相关的集群——路由步骤——然后仅在这些集群内进行最近邻搜索，从而大大减少搜索空间。
本论文的目的不仅是全面解释基于聚类的近似最近邻搜索，而且还介绍和深入研究我们新颖的先进方法的各个方面，该方法源自自然观察：路由函数解决了排名问题，使该函数适合学习排名。这种直觉的发展及其在最大内积搜索中的应用，使我们证明了使用简单的线性函数学习聚类代表可以显著提高基于聚类的近似最近邻搜索的准确性。]]></description>
      <guid>https://arxiv.org/abs/2412.05921</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统的模糊范数显式乘积量化</title>
      <link>https://arxiv.org/abs/2412.06069</link>
      <description><![CDATA[arXiv:2412.06069v1 公告类型：新
摘要：随着数据资源的增长，提供最符合需求的建议已成为商业和生活中克服信息过载问题的一项重要要求。然而，建立一个提出相关建议的系统一直是一个争论点。在以低复杂度产生相关建议方面，最具成本效益的技术之一是产品量化 (PQ)。近年来，PQ 方法不断发展。该系统的关键挑战是在不影响其复杂性的情况下提高产品量化在召回率方面的性能。这使得该算法适用于需要大量潜在相关项目而不忽略其他项目的问题，以高速和低成本跟上流量。这就是在线商店的情况，尽管客户可能会容易受到其他产品的影响，但出于此目的的建议很重要。本研究提出了一种模糊方法来执行基于规范的产品量化。 2 型模糊集 (T2FS) 定义码本，允许子向量 (T2FS) 与码本的多个元素相关联，然后通过积分解决其范数微积分。我们的方法完善了召回率测量，使算法适用于需要查询最多可能的相关项目而不忽略其他项目的问题。所提出的方法在 Netflix、Audio、Cifar60k 数据集中分别实现 94%、69%、59% 的召回率，优于所有 PQ 方法（如 NEQ、PQ 和 RQ），最高可达 +6%、+5% 和 +8%。此外，计算时间和复杂度几乎相当于最先进的现有 PQ 方法中计算效率最高的方法。]]></description>
      <guid>https://arxiv.org/abs/2412.06069</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Mixture-of-PageRanks：用实时、稀疏的 GraphRAG 取代长上下文</title>
      <link>https://arxiv.org/abs/2412.06078</link>
      <description><![CDATA[arXiv:2412.06078v1 公告类型：新
摘要：最近的进展极大地扩展了前沿 LLM 的上下文窗口，从几千个标记扩展到数百万个标记，使整本书和代码库能够适应上下文。然而，推断长上下文 LLM 的计算成本巨大，在实践中往往令人望而却步。RAG 提供了一种高效且有效的替代方案：仅检索和处理对当前任务最重要的上下文子集。虽然很有希望，但最近将 RAG 应用于长上下文任务的工作有两个核心限制：1) 很少关注如何提高 RAG 管道的计算效率，2) 此类工作仅在简单的 QA 任务上进行测试，它们在更具挑战性的任务上的表现尚不清楚。为了解决这个问题，我们开发了一种基于 PageRank 的算法，这是一种基于图的检索算法，我们称之为混合 PageRanks (MixPR)。 MixPR 使用基于 PageRank 的图检索算法，这些算法使用稀疏矩阵实现，可实现高效、廉价的检索，并可处理各种复杂任务。我们的 MixPR 检索器在各种长上下文基准任务中取得了最先进的结果，尽管计算效率更高，但其表现优于现有的 RAG 方法、专门的检索架构和长上下文 LLM。由于使用稀疏嵌入，我们的检索器具有极高的计算效率，能够在几秒钟内嵌入和检索数百万个标记，并且完全在 CPU 上运行。]]></description>
      <guid>https://arxiv.org/abs/2412.06078</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PRECISE：使用协作和语义信息对顺序推荐器进行预训练</title>
      <link>https://arxiv.org/abs/2412.06308</link>
      <description><![CDATA[arXiv:2412.06308v1 公告类型：新
摘要：现实世界的推荐系统通常为用户提供多样化的内容场景。考虑到工业平台中庞大的用户数量，使用单一统一的推荐模型来满足所有场景的需求是不可行的。通常，为每个不同的场景建立单独的推荐管道。这种做法导致全面掌握用户兴趣的挑战。最近的研究努力通过预训练模型来解决这个问题，以封装用户的整体兴趣。传统的预训练推荐模型主要通过利用协作信号来捕捉用户兴趣。然而，这些系统的一个普遍缺点是它们无法处理长尾项目和冷启动场景。随着大型语言模型的出现，利用 LLM 提取用户和项目的语义信息的研究工作显着增加。然而，基于文本的推荐高度依赖于复杂的特征工程，并且经常无法捕捉协作相似性。为了克服这些限制，我们提出了一种新颖的顺序推荐预训练框架，称为 PRECISE。该框架将协作信号与语义信息相结合。此外，PRECISE 采用一种学习框架，该框架首先对所有推荐场景中的用户综合兴趣进行建模，然后集中于目标场景行为的特定兴趣。我们证明 PRECISE 可以精确捕捉整个用户兴趣范围并有效地将其转移到目标兴趣。实证结果表明，PRECISE 框架在公共数据集和工业数据集上均表现出色。]]></description>
      <guid>https://arxiv.org/abs/2412.06308</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义搜索与推荐算法</title>
      <link>https://arxiv.org/abs/2412.06649</link>
      <description><![CDATA[arXiv:2412.06649v1 公告类型：新
摘要：本文介绍了一种新的语义搜索算法，该算法使用 Word2Vec 和 Annoy Index 来提高从大型数据集中检索信息的效率。所提出的方法通过提供更高的速度、准确性和可扩展性来解决传统搜索方法的局限性。在高达 100GB 的数据集上进行的测试证明了该方法在处理大量数据的同时保持高精度和高性能的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.06649</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更深层次：密集脑电图通道检索</title>
      <link>https://arxiv.org/abs/2412.06695</link>
      <description><![CDATA[arXiv:2412.06695v1 公告类型：新
摘要：信息检索系统历来依赖于明确的查询表述，要求用户将他们的信息需求转化为文本。这个过程在阅读任务中尤其具有破坏性，因为用户必须中断他们的自然流程来制定查询。我们提出了 DEEPER（密集脑电图段落检索），这是一个新颖的框架，可以在自然阅读过程中直接从用户的神经信号中检索相关段落，而无需中间文本翻译。DEEPER 以密集检索架构为基础，采用双编码器方法，具有专门的组件来处理神经数据，将 EEG 信号和文本段落映射到共享语义空间中。通过精心的架构设计和跨模态负采样策略，我们的模型学会将神经模式与其相应的文本内容对齐。ZuCo 数据集上的实验结果表明，直接的脑到段落检索明显优于当前的 EEG 到文本基线，在 Precision@1 上实现了 571% 的提高。我们的消融研究表明，该模型成功学习了 EEG 和文本模态之间的对齐表示（0.29 余弦相似度），而我们的硬负采样策略有助于提高整体性能。]]></description>
      <guid>https://arxiv.org/abs/2412.06695</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于图形的对话式人工智能驱动个人记忆捕获和检索方法在实际应用中的应用</title>
      <link>https://arxiv.org/abs/2412.05447</link>
      <description><![CDATA[arXiv:2412.05447v1 公告类型：交叉 
摘要：TOBU 是一款新颖的移动应用程序，它以用户参与的 AI 引导对话方式捕获和检索“个人记忆”（图片/视频以及围绕这些时刻的故事和背景）。我们的初始原型表明，现有的检索技术（例如检索增强生成 (RAG) 系统）由于在理解记忆关系方面的局限性而存在不足，导致回忆率低、幻觉和不令人满意的用户体验。我们设计了一种新颖的基于图的检索方法 TOBUGraph。在捕获过程中，TOBUGraph 利用大型语言模型 (LLM) 自动创建记忆的动态知识图，建立这些记忆的上下文和关系。在检索过程中，TOBUGraph 将 LLM 与记忆图相结合，通过图遍历实现全面回忆。我们使用真实用户数据进行的评估表明，TOBUGraph 在准确率和召回率方面均优于多种 RAG 实现，通过提高检索准确性和减少幻觉显著改善了用户体验。]]></description>
      <guid>https://arxiv.org/abs/2412.05447</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士作为法官：基于法学硕士的评估方法综合调查</title>
      <link>https://arxiv.org/abs/2412.05579</link>
      <description><![CDATA[arXiv:2412.05579v1 公告类型：交叉 
摘要：大型语言模型（LLM）的快速发展推动了它们在各个领域的应用不断扩大。最有前途的应用之一是它们作为基于自然语言响应的评估者的角色，称为“LLMs-as-judges”。由于其出色的有效性、跨任务泛化能力以及自然语言形式的可解释性，该框架引起了学术界和工业界越来越多的关注。本文从功能、方法、应用、元评估和局限性五个关键角度对 LLMs-as-judges 范式进行了全面的概述。我们首先对 LLMs-as-judges 进行系统定义并介绍其功能（为什么使用 LLM 法官？）。然后我们讨论使用 LLM 构建评估系统的方法（如何使用 LLM 法官？）。此外，我们研究了其应用的潜在领域（在哪里使用 LLM 法官？）并讨论了在各种情况下评估它们的方法（如何评估 LLM 法官？）。最后，我们详细分析了 LLM 法官的局限性并讨论了未来的潜在方向。通过结构化和全面的分析，我们旨在为 LLMs-as-judges 在研究和实践中的开发和应用提供见解。我们将继续在 https://github.com/CSHaitao/Awesome-LLMs-as-Judges 维护相关资源列表。]]></description>
      <guid>https://arxiv.org/abs/2412.05579</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于从英语维基百科到印地语维基百科的有效知识转移</title>
      <link>https://arxiv.org/abs/2412.05708</link>
      <description><![CDATA[arXiv:2412.05708v1 公告类型：交叉 
摘要：尽管维基百科是最大的多语言百科全书，但它本质上仍然是不完整的。高资源语言（HRL，例如英语）和低资源语言（LRL，例如印地语）之间的内容质量存在显著差异，许多 LRL 文章缺乏足够的信息。为了弥补这些内容差距，我们提出了一个轻量级框架来增强英语和印地语之间的知识公平性。如果英文维基百科页面不是最新的，我们的框架会从现成的外部资源（如英文书籍）中提取相关信息，并使用大型语言模型的上下文学习功能对其进行调整，以符合维基百科的独特风格，包括其 \textit{中立观点} (NPOV) 政策。然后将改编后的内容机器翻译成印地语，以集成到相应的维基百科文章中。另一方面，如果英文版本全面且最新，该框架会直接将知识从英文转移到印地语。我们的框架有效地为印地语维基百科各部分生成了新内容，根据自动和人工判断的评估，印地语维基百科文章分别增强了 65% 和 62%。]]></description>
      <guid>https://arxiv.org/abs/2412.05708</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PromptRefine：通过从相关示例库中选择示例来增强低资源印度语的少量样本性能</title>
      <link>https://arxiv.org/abs/2412.05710</link>
      <description><![CDATA[arXiv:2412.05710v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 最近通过上下文学习 (ICL) 展示了令人印象深刻的少样本学习能力。然而，ICL 性能高度依赖于少样本演示的选择，使得选择最优示例成为一项持续的研究挑战。这个问题在资源匮乏的印度语中进一步放大，其中真实数据的稀缺使选择过程变得复杂。在这项工作中，我们提出了 PromptRefine，一种新颖的交替最小化示例选择方法，可提高资源匮乏的印度语的 ICL 性能。PromptRefine 利用来自相关高资源印度语的辅助示例库，并采用多任务学习技术来对齐特定于语言的检索器，从而实现有效的跨语言检索。此外，我们在所选示例中加入了多样性，以增强泛化并减少偏差。通过使用最先进的 LLM（例如 LLAMA-3.1-8B、LLAMA-2-7B、Qwen-2-7B 和 Qwen-2.5-7B）对四个文本生成任务（跨语言问答、多语言问答、机器翻译和跨语言摘要）进行全面评估，我们证明了 PromptRefine 在检索示例方面的表现明显优于现有的框架。]]></description>
      <guid>https://arxiv.org/abs/2412.05710</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>