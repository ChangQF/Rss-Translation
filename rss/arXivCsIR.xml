<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.IR 更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 更新了 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 26 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>迈向实体解析的通用密集阻塞</title>
      <link>https://arxiv.org/abs/2404.14831</link>
      <description><![CDATA[arXiv:2404.14831v2 公告类型：replace-cross
摘要：分块是实体解析的关键步骤，基于神经网络的表示模型的出现导致了密集分块的发展，成为探索分块中深层语义的有前途的方法。然而，以前的先进自监督密集阻塞方法需要对目标域进行特定领域的训练，这限制了这些方法的好处和快速适应。为了解决这个问题，我们提出了 UniBlocker，这是一种密集拦截器，它使用自监督对比学习在独立于领域、易于获得的表格语料库上进行预训练。通过进行与领域无关的预训练，UniBlocker可以适应各种下游阻塞场景，而无需针对特定领域进行微调。为了评估我们的实体拦截器的通用性，我们还构建了一个新的基准，涵盖来自多个领域和场景的广泛拦截任务。我们的实验表明，所提出的 UniBlocker 在没有任何特定领域学习的情况下，显着优于以前的自监督和无监督密集块方法，并且与最先进的稀疏块方法具有可比性和补充性。]]></description>
      <guid>https://arxiv.org/abs/2404.14831</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:08 GMT</pubDate>
    </item>
    <item>
      <title>检索和提取：在线推荐系统的时态数据无移位范式</title>
      <link>https://arxiv.org/abs/2404.15678</link>
      <description><![CDATA[arXiv:2404.15678v2 公告类型：替换
摘要：当前的推荐系统受到严重的时间数据偏移问题的严重影响，即历史数据与在线数据分布的不一致。大多数现有模型专注于利用更新的数据，忽略了可以从移动数据中学习的可转移的、时间数据无变化的信息。我们提出关联时间不变性定理，该定理表明，给定固定的搜索空间，数据与搜索空间中的数据之间的关系随着时间的推移保持不变。利用这一原理，我们设计了一个基于检索的推荐系统框架，可以使用移位数据训练数据无移位的相关网络，显着增强推荐系统中原始模型的预测性能。然而，基于检索的推荐模型在在线部署时面临着巨大的推理时间成本。为了解决这个问题，我们进一步设计了一个提取框架，可以使用移位数据将相关网络中的信息提取到参数化模块中。提炼后的模型可以与原始模型一起在线部署，而推理时间仅增加极少。对多个真实数据集的大量实验表明，我们的框架通过利用移位数据显着提高了原始模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2404.15678</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:07 GMT</pubDate>
    </item>
    <item>
      <title>基于描述的文本相似度</title>
      <link>https://arxiv.org/abs/2305.12517</link>
      <description><![CDATA[arXiv:2305.12517v3 公告类型：replace-cross
摘要：识别具有给定语义的文本是许多信息搜索场景的核心。对向量嵌入的相似性搜索似乎是这种能力的核心，但当前文本嵌入中反映的相似性是语料库驱动的，并且对于许多用例来说是不一致且次优的。那么，对于有效的文本检索来说，相似度的一个好概念是什么？
  我们根据文本内容的抽象描述来确定搜索文本的需要，以及\emph{基于描述的相似性}的相应概念。我们证明了当前文本嵌入的不足，并提出了一种替代模型，该模型在标准最近邻搜索中使用时可以显着改进。该模型使用通过提示法学硕士获得的正负对进行训练，演示了如何使用法学硕士的数据来创建使用原始模型无法立即实现的新功能。]]></description>
      <guid>https://arxiv.org/abs/2305.12517</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:07 GMT</pubDate>
    </item>
    <item>
      <title>反垄断、亚马逊和算法审计</title>
      <link>https://arxiv.org/abs/2403.18623</link>
      <description><![CDATA[arXiv:2403.18623v2 公告类型：replace-cross
摘要：在数字市场中，尽管数字平台在当今每个人的生活中发挥着主导作用，但反垄断法和特殊法规的目的是确保市场保持竞争力。与传统市场不同，这些市场中的市场参与者行为很容易观察到。我们对亚马逊从事通常被描述为自我偏好的做法的程度进行了一系列实证调查。我们讨论如何在基于算法审计并需要大规模监管数字市场的监管环境中使用本文中使用的计算机科学工具。]]></description>
      <guid>https://arxiv.org/abs/2403.18623</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:07 GMT</pubDate>
    </item>
    <item>
      <title>MMGRec：使用 Transformer 模型的多模态生成推荐</title>
      <link>https://arxiv.org/abs/2404.16555</link>
      <description><![CDATA[arXiv:2404.16555v1 公告类型：新
摘要：多模态推荐旨在根据用户历史交互的项目和相关的多模态信息推荐用户偏好的候选人。以前的研究通常采用嵌入和检索范式：在同一嵌入空间中学习用户和项目表示，然后通过嵌入内积为用户检索相似的候选项目。然而，这种范式存在推理成本、交互建模和假阴性问题。为此，我们提出了一种新的 MMGRec 模型，将生成范式引入多模态推荐中。具体来说，我们首先设计了一种分层量化方法 Graph RQ-VAE，根据其多模态和 CF 信息为每个项目分配 Rec-ID。 Rec-ID 由语义上有意义的标记元组组成，充当每个项目的唯一标识符。然后，我们训练一个基于 Transformer 的推荐器，根据历史交互序列生成用户偏好项目的 Rec-ID。生成范式是合格的，因为该模型以自回归方式系统地预测标识推荐项目的标记元组。此外，为 Transformer 设计了一种关系感知的自注意力机制来处理非顺序交互序列，该机制探索了元素成对关系来替代绝对位置编码。与最先进的方法相比，大量实验评估了 MMGRec 的有效性。]]></description>
      <guid>https://arxiv.org/abs/2404.16555</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:06 GMT</pubDate>
    </item>
    <item>
      <title>从局部到全局：以查询为中心的摘要的图 RAG 方法</title>
      <link>https://arxiv.org/abs/2404.16130</link>
      <description><![CDATA[arXiv:2404.16130v1 公告类型：交叉
摘要：使用检索增强生成（RAG）从外部知识源检索相关信息使大型语言模型（LLM）能够回答有关私人和/或以前未见过的文档集合的问题。然而，RAG 在针对整个文本语料库的全局问题上失败了，例如“数据集中的主题是什么？”，因为这本质上是一个以查询为中心的摘要 (QFS) 任务，而不是一个显式检索任务。与此同时，先前的 QFS 方法无法扩展到典型 RAG 系统索引的文本数量。为了结合这些对比方法的优点，我们提出了一种在私有文本语料库上进行问答的 Graph RAG 方法，该方法可根据用户问题的普遍性和要索引的源文本的数量进行扩展。我们的方法使用 LLM 分两个阶段构建基于图的文本索引：首先从源文档导出实体知识图，然后为所有密切相关的实体组预先生成社区摘要。给定一个问题，每个社区摘要都用于生成部分响应，然后所有部分响应再次汇总为对用户的最终响应。对于 100 万个 token 范围内的数据集上的一类全局意义构建问题，我们表明 Graph RAG 在生成答案的全面性和多样性方面比原始 RAG 基线带来了实质性改进。基于全局和本地 Graph RAG 方法的实现即将在 https://aka.ms/graphrag 上发布。]]></description>
      <guid>https://arxiv.org/abs/2404.16130</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:06 GMT</pubDate>
    </item>
    <item>
      <title>通过减轻先令攻击来推进推荐系统</title>
      <link>https://arxiv.org/abs/2404.16177</link>
      <description><![CDATA[arXiv:2404.16177v1 公告类型：新
摘要：考虑到所提供的产品数量呈指数级增长，而用户在做出决定之前可以吸收的数据量相对较小，推荐系统有助于根据用户偏好对内容进行分类。协同过滤因其良好的性能而成为一种广泛使用的推荐计算方法。但是，这种方法使系统容易受到试图使建议产生偏差的攻击。这些攻击被称为“先令攻击”，其目的是在系统中推送某个项目或对某个项目进行核攻击。本文提出了一种算法来准确检测系统中的此类先令配置文件，并研究此类配置文件对推荐的影响。]]></description>
      <guid>https://arxiv.org/abs/2404.16177</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:05 GMT</pubDate>
    </item>
    <item>
      <title>OmniSearchSage：用于 Pinterest 搜索的多任务多实体嵌入</title>
      <link>https://arxiv.org/abs/2404.16260</link>
      <description><![CDATA[arXiv:2404.16260v1 公告类型：新
摘要：在本文中，我们介绍了 OmniSearchSage，这是一个多功能且可扩展的系统，用于理解 Pinterest 搜索的搜索查询、pin 和产品。我们共同学习统一的查询嵌入以及 pin 和产品嵌入，从而在 Pinterest 的生产搜索系统中提高了 $&gt;8\%$ 相关性、$&gt;7\%$ 参与度和 $&gt;5\%$ 广告点击率。这些成果的主要贡献者是内容理解的提高、更好的多任务学习和实时服务。我们使用从生成法学硕士、历史参与度和用户策划的董事会的图像标题衍生的各种文本来丰富我们的实体表示。我们的多任务学习设置在与引脚和产品嵌入相同的空间中生成单个搜索查询嵌入，并与预先存在的引脚和产品嵌入兼容。我们通过消融研究展示了每个特征的价值，并展示了统一模型与独立模型相比的有效性。最后，我们分享了如何在 Pinterest 搜索堆栈中部署这些嵌入，从检索到排名，扩展以低延迟每秒处理 30 万美元的请求。我们对这项工作的实现可以在 https://github.com/pinterest/atg-research/tree/main/omnisearchsage 上找到。]]></description>
      <guid>https://arxiv.org/abs/2404.16260</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:05 GMT</pubDate>
    </item>
    <item>
      <title>RE-RecSys：用于推荐房地产领域房产的端到端系统</title>
      <link>https://arxiv.org/abs/2404.16553</link>
      <description><![CDATA[arXiv:2404.16553v1 公告类型：新
摘要：我们提出了一种端到端的房地产推荐系统 RE-RecSys，该系统已在现实行业环境中投入生产。我们根据可用的历史数据将任何用户分为 4 类：i）冷启动用户； ii) 短期用户； iii) 长期用户； iv) 短期长期用户。对于冷启动用户，我们提出了一种新颖的基于规则的引擎，该引擎基于位置的流行度和用户偏好。对于短期用户，我们建议使用内容过滤模型，该模型根据用户最近的交互来推荐属性。对于长期和短期长期用户，我们提出了一种基于内容和协同过滤的新颖组合方法，该方法可以在现实场景中轻松生产。此外，基于转化率，我们针对用户在平台上的不同印象设计了一种新颖的权重方案，用于内容和协作模型的训练。最后，我们在从印度领先的房地产平台收集的真实世界房地产和点击流数据集上展示了所提出的管道 RE-RecSys 的效率。我们表明，所提出的管道可以在现实场景中部署，在 1000 rpm 的速度下平均延迟 &lt;40 毫秒。]]></description>
      <guid>https://arxiv.org/abs/2404.16553</guid>
      <pubDate>Fri, 26 Apr 2024 06:19:05 GMT</pubDate>
    </item>
    </channel>
</rss>