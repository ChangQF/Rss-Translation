<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.IR 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 在 arXiv.org 电子印刷档案上进行更新。</description>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>GAugLLM：使用大型语言模型改进文本属性图的图对比学习</title>
      <link>https://arxiv.org/abs/2406.11945</link>
      <description><![CDATA[arXiv:2406.11945v1 公告类型：交叉 
摘要：这项工作研究了文本属性图 (TAG) 的自监督图学习，其中节点由文本属性表示。与传统的图对比方法不同，这些方法会扰乱数值特征空间并改变图的拓扑结构，我们的目标是通过语言监督来改进视图生成。这是由实际应用中文本属性的普遍性所驱动的，它用丰富的语义信息补充了图结构。然而，这带来了挑战，主要有两个原因。首先，文本属性的长度和质量通常各不相同，因此很难在不改变原始文本描述原始语义含义的情况下对其进行扰乱。其次，虽然文本属性补充了图结构，但它们本身并不是很好的对齐。为了弥补这一差距，我们引入了 GAugLLM，这是一个用于增强 TAG 的新框架。它利用 Mistral 等先进的大型语言模型来增强自监督图学习。具体来说，我们引入了一种混合提示专家技术来生成增强节点特征。该方法自适应地将多个提示专家（每个专家使用提示工程修改原始文本属性）映射到数值特征空间中。此外，我们设计了一个协作边缘修改器来利用结构和文本共性，通过检查或建立节点之间的连接来增强边缘增强。跨越不同领域的五个基准数据集的实证结果强调了我们的框架作为插件工具增强领先对比方法性能的能力。值得注意的是，我们观察到增强的特征和图形结构还可以增强标准生成方法以及流行的图形神经网络的性能。我们的 GAugLLM 的开源实现可在 Github 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.11945</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:23 GMT</pubDate>
    </item>
    <item>
      <title>无国界新闻：多语言句子嵌入的领域自适应用于跨语言新闻推荐</title>
      <link>https://arxiv.org/abs/2406.12634</link>
      <description><![CDATA[arXiv:2406.12634v1 公告类型：新
摘要：多语言新闻消费者数量的快速增长对新闻推荐系统在提供定制推荐方面提出了越来越大的挑战。首先，现有的神经新闻推荐器即使由多语言语言模型 (LM) 提供支持，在零样本跨语言迁移 (ZS-XLT) 中也会遭受相当大的性能损失。其次，当前在特定任务数据上微调神经推荐器主干 LM 的范式在计算上是昂贵的，并且在数据稀缺或完全不可用的少样本推荐和冷启动设置中是不可行的。在这项工作中，我们提出了一种新闻自适应句子编码器 (NaSE)，它是从预训练的大规模多语言句子编码器 (SE) 领域专门化的。为此，我们构建并利用了 PolyNews 和 PolyNewsParallel，两个多语言新闻专用语料库。在新闻适配的多语言 SE 到位后，我们测试了监督微调对新闻推荐的有效性（即质疑其必要性），并提出了一个简单而强大的基线，该基线基于 (i) 冻结的 NaSE 嵌入和 (ii) 后期点击行为融合。我们表明，NaSE 在 ZS-XLT 中实现了真正的冷启动和少量新闻推荐方面的最先进性能。]]></description>
      <guid>https://arxiv.org/abs/2406.12634</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:22 GMT</pubDate>
    </item>
    <item>
      <title>用于高效顺序推荐的行为相关线性循环单元</title>
      <link>https://arxiv.org/abs/2406.12580</link>
      <description><![CDATA[arXiv:2406.12580v1 公告类型：新
摘要：顺序推荐系统旨在通过使用 RNN 和注意等各种运算符进行用户行为建模来预测用户的下一次交互。然而，现有的模型通常无法同时实现顺序推荐的三大黄金原则，即训练效率、低成本推理和强大的性能。为此，我们提出了一种基于行为相关线性循环单元的高效顺序推荐模型 RecBLR，以实现这三个原则的不可能三角。通过将门控机制和行为相关设计纳入线性循环单元，我们的模型显著提高了用户行为建模和推荐性能。此外，我们通过设计具有定制 CUDA 内核的硬件感知扫描加速算法，为我们的模型解锁了可并行化的训练和推理效率。在具有不同长度的用户行为序列的真实数据集上进行的大量实验证明了 RecBLR 在同时实现所有三个黄金原则（强推荐性能、训练效率和低成本推理）方面的卓越有效性，同时对具有长用户交互历史的数据集表现出出色的可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2406.12580</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:21 GMT</pubDate>
    </item>
    <item>
      <title>PromptDSI：基于提示的无排练实例增量学习文档检索</title>
      <link>https://arxiv.org/abs/2406.12593</link>
      <description><![CDATA[arXiv:2406.12593v1 公告类型：新
摘要：可微分搜索索引 (DSI) 利用预训练语言模型 (PLM) 进行高效的文档检索，而无需依赖外部索引。但是，DSI 需要完全重新训练才能处理动态语料库中的更新，从而导致严重的计算效率低下。我们引入了 PromptDSI，这是一种无需排练、基于提示的方法，用于文档检索中的实例式增量学习。PromptDSI 将提示附加到冻结的 PLM 的 DSI 编码器上，利用其强大的表示来有效地索引新语料库，同时保持稳定性和可塑性之间的平衡。我们消除了基于提示的持续学习方法的初始前向传递，这使训练和推理时间加倍。此外，我们提出了一个主题感知提示池，它使用神经主题嵌入作为固定键。该策略确保了提示的多样化和有效使用，解决了查询键匹配机制崩溃导致的参数利用不足的挑战。我们的实证评估表明，PromptDSI 在管理遗忘方面与 IncDSI 相当，同时在新语料库上显著提高了 4% 以上的回忆率。]]></description>
      <guid>https://arxiv.org/abs/2406.12593</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:21 GMT</pubDate>
    </item>
    <item>
      <title>LLM4MSR：多场景推荐的 LLM 增强范式</title>
      <link>https://arxiv.org/abs/2406.12529</link>
      <description><![CDATA[arXiv:2406.12529v1 公告类型：新
摘要：随着对个性化推荐的需求不断增长以及商业场景的急剧增长，多场景推荐（MSR）的研究引起了广泛关注，它使用来自所有场景的数据来同时提高其推荐性能。然而，现有的方法往往整合了不足的场景知识，而忽略了学习个性化的跨场景偏好，从而导致性能不佳和可解释性不足。同时，虽然大型语言模型（LLM）表现出强大的推理和捕获语义信息的能力，但高推理延迟和高计算成本的调整阻碍了它在工业推荐系统中的实现。为了填补这些空白，我们在本文中提出了一种有效、高效、可解释的 LLM 增强范式 LLM4MSR。具体来说，我们首先利用 LLM 从设计的场景和用户级提示中发现多层次知识，包括场景相关性和用户的跨场景兴趣，而无需对 LLM 进行微调，然后采用分层元网络生成多层次元层，以明确提高场景感知和个性化推荐能力。我们在 KuaiSAR-small、KuaiSAR 和 Amazon 数据集上的实验验证了 LLM4MSR 的两个显着优势：（i）与不同多场景骨干模型的有效性和兼容性（在三个数据集上实现了 1.5%、1% 和 40% 的 AUC 改进），（ii）在工业推荐系统中的高效性和可部署性，以及（iii）提高可解释性。实现的代码和数据可供轻松复制。]]></description>
      <guid>https://arxiv.org/abs/2406.12529</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:20 GMT</pubDate>
    </item>
    <item>
      <title>根据出版时间预测获奖研究论文</title>
      <link>https://arxiv.org/abs/2406.12535</link>
      <description><![CDATA[arXiv:2406.12535v1 公告类型：新
摘要：近年来，许多研究都致力于预测研究论文的科学影响力。这些预测大多基于引用计数或依赖于仅从已发表的论文中获得的特征。在本研究中，我们仅依靠出版时可用的信息来预测研究论文获奖的可能性。对于每篇论文，我们根据其参考书目构建引文子图。我们最初考虑该子图的一些特征，例如密度和全局聚类系数，以进行预测。然后，我们将这些信息与从摘要和标题中提取的文本特征混合，以获得更准确的最终预测。我们考虑了 ArnetMiner 引文图进行了实验，而获奖论文的基本事实是从 32 个计算机科学会议的最佳论文奖集合中获得的。在我们的实验中，我们获得了令人鼓舞的 F1 分数 0.694。值得注意的是，高召回率和低假阴性率表明该模型在识别不会获奖的论文方面表现非常出色。这种行为可以帮助研究人员在出版时对其工作进行初步评估。最后，我们对可解释性进行了一些初步实验。我们的结果突出了拓扑和文本特征中的一些有趣模式。]]></description>
      <guid>https://arxiv.org/abs/2406.12535</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:20 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统中的 LLM 增强重新排序</title>
      <link>https://arxiv.org/abs/2406.12433</link>
      <description><![CDATA[arXiv:2406.12433v1 公告类型：新
摘要：重新排序是推荐系统中的一个关键组件，在改进推荐算法的输出方面起着至关重要的作用。传统的重新排序模型主要关注准确性，但现代应用需要考虑多样性和公平性等其他标准。现有的重新排序方法通常无法在模型级别有​​效地协调这些不同的标准。此外，由于这些模型的复杂性以及不同重新排序标准在不同场景中的重要性不同，它们经常在可扩展性和个性化方面遇到挑战。为此，我们引入了一个由 LLM 增强的综合重新排序框架，旨在无缝集成各种重新排序标准，同时保持可扩展性并促进个性化推荐。该框架采用全连通图结构，允许 LLM 通过连贯的思路链 (CoT) 过程同时考虑准确性、多样性和公平性等多个方面。还集成了可定制的输入机制，可以调整语言模型的焦点以满足特定的重新排序需求。我们使用三个流行的公共数据集验证了我们的方法，我们的框架在平衡多个标准方面表现出优于现有最先进的重新排序模型的性能。此实现的代码已公开发布。]]></description>
      <guid>https://arxiv.org/abs/2406.12433</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:19 GMT</pubDate>
    </item>
    <item>
      <title>通过去噪和对齐多模态内容和用户反馈来改进多模态推荐系统</title>
      <link>https://arxiv.org/abs/2406.12501</link>
      <description><![CDATA[arXiv:2406.12501v1 公告类型：新
摘要：多模态推荐系统 (MRS) 在各种在线网络平台中都发挥着关键作用，近年来引起了广泛关注。然而，以前的研究忽视了 (1) 嘈杂的多模态内容、(2) 嘈杂的用户反馈和 (3) 将多模态内容与用户反馈对齐的挑战。为了应对这些挑战，我们提出了去噪和对齐多模态推荐系统 (DA-MRS)。为了减轻多模态噪声，DA-MRS 首先构建由跨模态的一致内容相似性确定的项目-项目图。为了对用户反馈进行去噪，DA-MRS 将观察到的反馈的概率与多模态内容相关联，并设计了去噪的 BPR 损失。此外，DA-MRS 实现了由用户偏好引导的对齐以增强特定于任务的项目表示，并实现了由分级项目关系引导的对齐以提供更细粒度的对齐。大量实验验证了 DA-MRS 是一个即插即用框架，并且在各种数据集、主干模型和噪声场景中实现了显著且一致的改进。]]></description>
      <guid>https://arxiv.org/abs/2406.12501</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:19 GMT</pubDate>
    </item>
    <item>
      <title>CherryRec：通过 LLM 驱动的框架提高新闻推荐质量</title>
      <link>https://arxiv.org/abs/2406.12243</link>
      <description><![CDATA[arXiv:2406.12243v1 公告类型：新
摘要：大型语言模型 (LLM) 在语言理解和生成方面取得了显著进展。利用文本特征的自定义 LLM 已应用于推荐系统，并在各种推荐场景中展示了改进。然而，大多数现有方法基于预先训练的知识（例如电影推荐）执行未经训练的推荐，而 LLM 的自回归生成导致推理速度慢，使其在实时推荐中效果不佳。为了解决这个问题，我们提出了一个使用 LLM 进行新闻推荐的框架，名为 \textit{CherryRec}，它在加速推荐过程的同时确保推荐的质量。具体来说，我们使用知识感知新闻快速选择器根据用户的交互历史检索候选选项。然后将历史记录和检索到的项目作为文本输入到经过微调的 LLM，即内容感知新闻 Llm 评估器中，旨在增强新闻推荐能力。最后，价值感知新闻评分器整合评分以计算 CherryRec 评分，作为最终推荐的基础。我们通过在基准数据集上将所提出的框架与最先进的基线方法进行比较来验证其有效性。我们的实验结果一致表明，CherryRec 在推荐性能和效率方面均优于基线。项目资源可在以下位置访问：\url{https://github.com/xxxxxx}]]></description>
      <guid>https://arxiv.org/abs/2406.12243</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:18 GMT</pubDate>
    </item>
    <item>
      <title>记忆约束下的稠密检索器的梯度累积方法</title>
      <link>https://arxiv.org/abs/2406.12356</link>
      <description><![CDATA[arXiv:2406.12356v1 公告类型：新
摘要：InfoNCE 损失通常用于在信息检索任务中训练密集检索器。众所周知，大批量对于 InfoNCE 损失的稳定有效训练至关重要，这需要大量的硬件资源。由于对大批量的依赖，密集检索器存在应用和研究的瓶颈。最近，内存减少方法已被广泛采用，通过分解正向和反向或使用存储库来解决硬件瓶颈。然而，当前的方法仍然存在训练速度慢和不稳定的问题。为了解决这些问题，我们提出了 \longmodelname\ (\modelname)，这是一种稳定有效的密集检索器训练内存减少方法，它使用双存储库结构来利用先前生成的查询和段落表示。在广泛使用的五个信息检索数据集上的实验表明 \modelname\ 不仅可以超越现有的内存减少方法，还可以超越高资源场景。此外，理论分析和实验结果证实 \modelname\ 比当前的存储库利用方法提供更稳定的双编码器训练。]]></description>
      <guid>https://arxiv.org/abs/2406.12356</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:18 GMT</pubDate>
    </item>
    <item>
      <title>中级提炼：从黑盒法学硕士 (LLM) 中进行数据高效提炼，用于信息检索</title>
      <link>https://arxiv.org/abs/2406.12169</link>
      <description><![CDATA[arXiv:2406.12169v1 公告类型：新
摘要：最近的研究探索了从大型语言模型 (LLM) 中提取知识以优化检索器模型，尤其是在检索增强生成 (RAG) 框架内。然而，大多数现有的训练方法依赖于从 LLM 的权重或其输出概率中提取监督信号，这不仅资源密集，而且与黑盒 LLM 不兼容。在本文中，我们引入了 \textit{中间蒸馏}，这是一种数据高效的知识蒸馏训练方案，它将 LLM 视为黑盒，并通过创新的 LLM-排名器-检索器管道提取其知识，仅使用 LLM 的排名生成作为监督信号。大量实验表明，我们提出的方法仅使用 1,000 个训练实例就可以显着提高检索器模型的性能。此外，我们提炼的检索器模型显著提高了 RAG 框架内问答任务的性能，展示了 LLM 经济有效地训练小型模型的潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.12169</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>平衡推荐的嵌入频谱</title>
      <link>https://arxiv.org/abs/2406.12032</link>
      <description><![CDATA[arXiv:2406.12032v1 公告类型：新
摘要：现代推荐系统严重依赖从高维稀疏数据中学习到的高质量表示。虽然人们投入了大量精力来设计强大的算法来提取用户偏好，但有助于获得良好表示的因素仍然相对未被探索。在这项工作中，我们阐明了现有成对学习范式（即嵌入崩溃问题）中的一个问题，即表示往往跨越整个嵌入空间的子空间，导致次优解决方案并降低模型容量。具体而言，对观察到的交互进行优化相当于一个低通滤波器，导致用户/项目具有相同的表示并导致完全崩溃。虽然负采样充当高通滤波器以通过平衡嵌入频谱来缓解崩溃，但其有效性仅限于某些损失，这仍然会导致不完全崩溃。为了解决这个问题，我们提出了一种名为 DirectSpec 的新方法，它充当可靠的全通滤波器，在训练期间平衡嵌入的频谱分布，确保用户/项目有效地跨越整个嵌入空间。此外，我们从去相关的角度对 DirectSpec 进行了彻底的分析，并提出了一种增强变体 DirectSpec+，它采用自定步调的梯度来更有效地优化不相关的样本。此外，我们在 DirectSpec+ 和均匀性之间建立了紧密的联系，表明对比学习 (CL) 可以通过间接平衡频谱来缓解崩溃问题。最后，我们在两种流行的推荐模型上实现了 DirectSpec 和 DirectSpec+：MF 和 LightGCN。我们的实验结果证明了它相对于竞争基线的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2406.12032</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>通过相互学习来微调点击率预测模型</title>
      <link>https://arxiv.org/abs/2406.12087</link>
      <description><![CDATA[arXiv:2406.12087v1 公告类型：新
摘要：点击率 (CTR) 预测已成为数字行业（例如数字广告或在线购物）的一项基本任务。许多基于深度学习的方法已经得到实施，并已成为该领域最先进的模型。为了进一步提高 CTR 模型的性能，基于知识蒸馏的方法已被广泛使用。然而，目前大多数 CTR 预测模型都没有太多复杂的架构，因此很难称其中一个为“繁琐”，另一个为“微小”。另一方面，复杂模型和简单模型之间的性能差距也不是很大。因此，将知识从一个模型提炼到另一个模型是不值得的。考虑到这些因素，相互学习可能是一种更好的方法，因为所有模型都可以相互改进。在本文中，我们展示了当相互学习算法在平等时有多么有用。在我们对Criteo和Avazu数据集的实验中，相互学习算法使模型的性能提高了高达0.66%的相对提升。]]></description>
      <guid>https://arxiv.org/abs/2406.12087</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>个性化联合知识图谱嵌入客户端关系图谱</title>
      <link>https://arxiv.org/abs/2406.11943</link>
      <description><![CDATA[arXiv:2406.11943v1 公告类型：新
摘要：联合知识图谱嵌入 (FKGE) 最近引起了广泛关注，因为它能够从分布式知识图谱中提取富有表现力的表示，同时保护各个客户端的隐私。现有的 FKGE 方法通常利用来自所有客户端的实体嵌入的算术平均值作为全局补充知识，并为每个客户端学习全局共识实体嵌入的副本。然而，这些方法通常忽略了不同客户端之间固有的语义差异。这种疏忽不仅导致全局共享的互补知识在针对特定客户端进行定制时被过多的噪音淹没，而且还会导致局部和全局优化目标之间的差异。因此，学习到的嵌入的质量受到影响。为了解决这个问题，我们提出了个性化联合知识图嵌入客户端关系图 (PFedEG)，这是一种新颖的方法，它使用客户端关系图通过辨别来自其他客户端的嵌入的语义相关性来学习个性化嵌入。具体来说，PFedEG 通过根据客户端关系图上的“亲和力”合并来自邻近客户端的实体嵌入来为每个客户端学习个性化的补充知识。然后，每个客户端根据其本地三元组和个性化补充知识进行个性化嵌入学习。我们在四个基准数据集上进行了广泛的实验，以根据最先进的模型评估我们的方法，结果证明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2406.11943</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>当标签感知推荐中的框与图神经网络相遇</title>
      <link>https://arxiv.org/abs/2406.12020</link>
      <description><![CDATA[arXiv:2406.12020v1 公告类型：新
摘要：去年见证了由 LLM 丰富标签支持的标签感知推荐系统的重新繁荣。不幸的是，尽管已经做出了巨大的努力，但当前的解决方案可能无法仅使用标签驱动的配置文件来描述用户偏好中固有的多样性和不确定性。最近，随着基于几何的技术（例如框嵌入）的发展，用户偏好的多样性现在可以完全建模为高维空间中框内的范围。然而，缺陷仍然存在，因为这些方法无法捕获高阶邻居信号，即用户-标签-项目三分图中语义丰富的多跳关系，这严重限制了用户建模的有效性。为了应对这一挑战，在本文中，我们提出了一种称为 BoxGNN 的新算法，通过组合逻辑操作来执行消息聚合，从而合并高阶信号。具体来说，我们首先将用户、项目和标签作为超框而不是表示空间中的简单点进行嵌入，并定义两个逻辑操作以方便后续处理。接下来，我们通过逻辑操作的组合执行消息聚合机制，以获得相应的高阶框表示。最后，我们采用基于体积的学习目标和 Gumbel 平滑技术来细化框的表示。在两个公开可用的数据集和一个 LLM 增强的电子商务数据集上进行的大量实验验证了 BoxGNN 与各种最先进的基线相比的优越性。代码已在线发布]]></description>
      <guid>https://arxiv.org/abs/2406.12020</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:15 GMT</pubDate>
    </item>
    </channel>
</rss>