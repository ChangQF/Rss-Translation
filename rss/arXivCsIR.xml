<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.IR 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 在 arXiv.org 电子印刷档案上进行更新。</description>
    <lastBuildDate>Mon, 06 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于顺序推荐的项目关联分解混合马尔可夫链</title>
      <link>https://arxiv.org/abs/2501.01429</link>
      <description><![CDATA[arXiv:2501.01429v1 公告类型：新
摘要：顺序推荐是指根据特定用户在特定时间内的历史行为序列，为其推荐下一个感兴趣的商品。虽然先前的研究已经广泛研究了基于马尔可夫链的顺序推荐模型，但这些研究大多侧重于用户的历史行为序列，而很少关注商品之间的整体相关性。本研究引入了一种称为商品关联分解混合马尔可夫链的顺序推荐算法，该算法使用商品关联图来整合商品之间的关联信息，并将其与用户行为序列信息相结合。我们从四个公开数据集进行的实验结果表明，新引入的算法在不显着增加参数数量的情况下显着增强了推荐排名结果。此外，对先前平衡参数调整的研究强调了在不同数据集之间整合商品关联信息的重要性。]]></description>
      <guid>https://arxiv.org/abs/2501.01429</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向大型语言模型 (LLM) 时代的冷启动建议：全面调查和路线图</title>
      <link>https://arxiv.org/abs/2501.01945</link>
      <description><![CDATA[arXiv:2501.01945v1 Announce Type: new 
摘要：冷启动问题是推荐系统中长期存在的挑战之一，重点是准确建模新的或交互有限的用户或项目以提供更好的推荐。由于互联网平台的多样化以及用户和项目的指数级增长，冷启动推荐（CSR）的重要性日益凸显。同时，大型语言模型（LLM）取得了巨大的成功，拥有强大的用户和项目信息建模能力，为冷启动推荐提供了新的潜力。然而，CSR 研究界仍然缺乏对该领域的全面回顾和反思。基于此，本文站在大型语言模型时代的背景下，对 CSR 的路线图、相关文献和未来方向进行了全面的回顾和讨论。具体来说，我们探索了现有 CSR 如何利用信息的发展路径，从内容特征、图关系、领域信息，到大型语言模型所拥有的世界知识，旨在为 CSR 的研究和工业界提供新的见解。社区在 https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation 中收集并持续更新了冷启动推荐的相关资源。]]></description>
      <guid>https://arxiv.org/abs/2501.01945</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种针对大规模推荐系统中多任务融合的离线强化学习算法</title>
      <link>https://arxiv.org/abs/2404.17589</link>
      <description><![CDATA[arXiv:2404.17589v4 Announce Type: replace 
摘要：多任务融合（MTF）作为推荐系统的最后一个关键阶段，负责将多任务学习（MTL）输出的多个分数组合成最终分数，以最大化用户满意度，从而决定最终的推荐结果。最近，为了在推荐会话中优化长期用户满意度，业界将强化学习（RL）用于 MTF。然而，目前用于 MTF 的离线 RL 算法存在以下严重问题：1）为了避免分布不均（OOD）问题，它们的约束过于严格，严重损害了它们的性能；2）它们不知道用于生成训练数据的探索策略，从不与真实环境交互，因此只能学习次优策略；3）传统的探索策略效率低下，损害了用户体验。为了解决上述问题，我们提出了一种针对大规模 RS 中的 MTF 定制的新方法，称为 IntegratedRL-MTF。 IntegratedRL-MTF 将离线强化学习模型与我们的在线探索策略相结合，以放宽过于严格和复杂的约束，从而显著提高其性能。我们还设计了一种极其高效的探索策略，该策略消除了低价值的探索空间，并专注于探索潜在的高价值状态-动作对。此外，我们采用渐进式训练模式，借助我们的探索策略进一步提升模型的性能。我们在腾讯新闻的短视频频道进行了广泛的离线和在线实验。结果表明，我们的模型明显优于其他模型。IntegratedRL-MTF 已在我们的 RS 和腾讯的其他大型 RS 中全面部署，并取得了显着的改进。]]></description>
      <guid>https://arxiv.org/abs/2404.17589</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 PLM 的蛋白质检索框架</title>
      <link>https://arxiv.org/abs/2407.11548</link>
      <description><![CDATA[arXiv:2407.11548v2 公告类型：替换 
摘要：蛋白质检索旨在解构序列、结构和功能之间的关系，推动了生物学的发展。基于序列相似性的算法基本局部比对搜索工具 (BLAST) 证明了该领域的有效性。尽管现有的蛋白质检索工具优先考虑序列相似性，并且可能忽略不同但具有同源性或功能的蛋白质。为了解决这个问题，我们提出了一种新的蛋白质检索框架，以减轻对序列相似性的偏见。我们的框架主动利用蛋白质语言模型 (PLM) 将蛋白质序列嵌入高维特征空间中，从而增强后续分析的表示能力。随后，构建了一个加速索引向量数据库，以促进密集向量的快速访问和检索。大量实验表明，我们的框架可以平等地检索相似和不相似的蛋白质。此外，此方法还能识别传统方法无法发现的蛋白质，该框架将有效协助蛋白质挖掘，助力生物学的发展。]]></description>
      <guid>https://arxiv.org/abs/2407.11548</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种用于大规模推荐系统中多任务融合的增强状态强化学习算法</title>
      <link>https://arxiv.org/abs/2409.11678</link>
      <description><![CDATA[arXiv:2409.11678v3 Announce Type: replace 
摘要：多任务融合（MTF）作为推荐系统（RS）的最后一个关键阶段，负责将多任务学习（MTL）预测的多个分数组合成最终分数以最大化用户满意度，从而决定最终的推荐结果。近年来，为了在推荐会话中最大化用户长期满意度，强化学习（RL）被广泛应用于大规模RS中的MTF。然而，受限于其建模模式，现有的所有RL-MTF方法都只能利用用户特征作为状态来为每个用户生成动作，而无法利用项目特征和其他有价值的特征，导致结果不理想。解决这个问题是一项挑战，需要突破当前RL-MTF的建模模式。为了解决这个问题，我们提出了一种称为增强状态RL的RS中MTF的新方法。与上述现有方法不同，我们的方法首先将用户特征、项目特征和其他有价值的特征统称为增强状态；然后提出一种新颖的参与者和评论家学习过程，利用增强状态为每个用户-项目对做出更好的操作。据我们所知，这种新颖的建模模式是首次在 RL-MTF 领域提出。我们在大型 RS 中进行了广泛的离线和在线实验。结果表明，我们的模型明显优于其他模型。增强状态 RL 已在我们的 RS 中全面部署了半年多，与基线相比，用户有效消费提高了 3.84%，用户持续时间提高了 0.58%。]]></description>
      <guid>https://arxiv.org/abs/2409.11678</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>预训练 Transformer 中的知识电路</title>
      <link>https://arxiv.org/abs/2405.17969</link>
      <description><![CDATA[arXiv:2405.17969v4 公告类型：replace-cross 
摘要：现代大型语言模型的卓越能力源于其参数中编码的大量知识库，使它们能够感知世界并进行推理。这些模型如何存储知识的内部工作原理长期以来一直是研究人员密切关注和研究的主题。到目前为止，大多数研究都集中在这些模型中的孤立组件上，例如多层感知器和注意力头。在本文中，我们深入研究语言模型的计算图，以揭示有助于表达特定知识的知识回路。使用 GPT2 和 TinyLLAMA 进行的实验使我们能够观察到某些信息头、关系头和多层感知器如何在模型中协作编码知识。此外，我们评估了当前知识编辑技术对这些知识回路的影响，从而更深入地了解了这些编辑方法的功能和约束。最后，我们利用知识回路来分析和解释语言模型行为，例如幻觉和情境学习。我们相信知识回路有潜力增进我们对 Transformer 的理解，并指导知识编辑的改进设计。代码和数据可在 https://github.com/zjunlp/KnowledgeCircuits 中找到。]]></description>
      <guid>https://arxiv.org/abs/2405.17969</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AdaCQR：通过稀疏和密集检索对齐增强对话搜索的查询重构</title>
      <link>https://arxiv.org/abs/2407.01965</link>
      <description><![CDATA[arXiv:2407.01965v3 公告类型：replace-cross 
摘要：对话式查询重构 (CQR) 在解决对话式搜索的挑战方面取得了显著进展，特别是那些源于潜在用户意图和历史背景需求的挑战。最近的研究旨在通过对齐来提高 CQR 的性能。然而，它们是为一个特定的检索系统设计的，这可能会导致次优泛化。为了克服这一限制，我们提出了一个新颖的框架 AdaCQR。通过将重构模型与基于术语和基于语义的检索系统对齐，AdaCQR 通过两阶段训练策略增强了信息搜索查询在不同检索环境中的通用性。此外，提出了两种有效的方法来获得更好的标签和多样化的输入候选，从而提高了框架的效率和鲁棒性。 TopiOCQA 和 QReCC 数据集上的实验结果表明，AdaCQR 在更高效的框架中优于现有方法，在对话查询重构方面提供了定量和定性的改进。]]></description>
      <guid>https://arxiv.org/abs/2407.01965</guid>
      <pubDate>Mon, 06 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>