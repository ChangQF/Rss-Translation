<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.IR 更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 更新了 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 22 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>突破极限：非循环声音自由选择工作流网络中的并发检测（$O(P^2 + T^2)$）</title>
      <link>https://arxiv.org/abs/2401.16097</link>
      <description><![CDATA[arXiv:2401.16097v2 公告类型：replace-cross
摘要：并发性是Petri网描述和模拟复杂系统行为的一个重要方面。了解哪些位置和转换可以并行执行有助于理解网络并启用分析技术和其他属性的计算，例如因果关系、排他性等。所有基于并发检测的技术都取决于该检测方法的效率。 Kovalyov 和 Esparza 开发了算法，用于计算实时网络和有界网络的 $O\big((P+T)TP^2\big)$ 中的所有并发位置（其中 $P$ 和 $T$ 是位置和转换的数量) 和 $O\big(P(P+T)^2\big)$ 中的实时和有界自由选择网络。尽管这些算法具有相当好的计算复杂度，但大量并发的节点对仍然可能导致较长的计算时间。本文用并发路径 (CP) 算法补充了并发检测算法的调色板，以实现健全的自由选择工作流网络。该算法允许并行化，并且最坏情况下的计算复杂度对于非循环网络为 $O(P^2 + T^2)$，对于循环网络为 $O(P^3 + PT^2)$。尽管循环网络的计算复杂度没有提高，但评估显示了CP的好处，特别是当网络包含许多处于并发关系的节点时。]]></description>
      <guid>https://arxiv.org/abs/2401.16097</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:47 GMT</pubDate>
    </item>
    <item>
      <title>EasyInstruct：易于使用的大型语言模型指令处理框架</title>
      <link>https://arxiv.org/abs/2402.03049</link>
      <description><![CDATA[arXiv:2402.03049v3 公告类型：replace-cross
摘要：近年来，指令调优受到越来越多的关注，并成为增强大型语言模型（LLM）能力的关键技术。为了构建高质量的指令数据集，人们提出了许多指令处理方法，旨在实现数据数量和数据质量之间的微妙平衡。然而，由于各种指令处理方法之间存在不一致，导致社区没有标准的开源指令处理实现框架，这阻碍了从业者的进一步发展和进步。为了促进指令处理研究和开发，我们推出了 EasyInstruct，这是一个易于使用的 LLM 指令处理框架，它将指令生成、选择和提示模块化，同时还考虑了它们的组合和交互。 EasyInstruct 在 https://github.com/zjunlp/EasyInstruct 上公开发布并积极维护，并提供在线演示应用程序和用于快速入门的演示视频，呼吁以指令数据和合成数据为中心进行更广泛的研究。]]></description>
      <guid>https://arxiv.org/abs/2402.03049</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:47 GMT</pubDate>
    </item>
    <item>
      <title>用于深度点击率预测的离散语义标记化</title>
      <link>https://arxiv.org/abs/2403.08206</link>
      <description><![CDATA[arXiv:2403.08206v2 公告类型：替换
摘要：将项目内容信息纳入点击率（CTR）预测模型仍然是一个挑战，特别是在工业场景的时间和空间限制下。内容编码范例将用户和项目编码器直接集成到点击率模型中，随着时间的推移优先考虑空间。相比之下，基于嵌入的范例将项目和用户语义转换为潜在嵌入，随后缓存它们以牺牲空间为代价来优化处理时间。在本文中，我们引入了一种新的语义标记范式，并提出了一种离散语义标记化方法，即 UIST，用于用户和项目表示。 UIST 有助于快速训练和推理，同时保持保守的内存占用。具体来说，UIST 将密集嵌入向量量化为长度较短的离散标记，并采用分层混合推理模块来权衡每个用户-项目标记对的贡献。我们在新闻推荐上的实验结果展示了 UIST 对于 CTR 预测的有效性和效率（大约 200 倍空间压缩）。]]></description>
      <guid>https://arxiv.org/abs/2403.08206</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>TensorBank：用于基础模型训练的 Tensor Lakehouse</title>
      <link>https://arxiv.org/abs/2309.02094</link>
      <description><![CDATA[arXiv:2309.02094v3 公告类型：replace-cross
摘要：随着自然语言之外的基础模型的兴起，存储和流式传输用于基础模型训练的高维数据成为一项关键要求。在本文中，我们介绍了 TensorBank，这是一个 PB 级张量 Lakehouse，能够基于复杂的关系查询以线速将张量从云对象存储 (COS) 流式传输到 GPU 内存。我们使用分层统计索引（HSI）来加速查询。我们的架构允许使用 HTTP 范围读取直接在块级别上寻址张量。一旦进入 GPU 内存，就可以使用 PyTorch 转换来转换数据。我们提供通用的 PyTorch 数据集类型和相应的数据集工厂，将关系查询和请求的转换作为实例进行转换。通过利用 HSI，可以跳过不相关的块而不读取它们，因为这些索引包含其在不同层次分辨率级别的内容的统计数据。这是一种由开放标准支持并大量使用开源技术的固执己见的架构。尽管使用地理空间-时间数据对生产用途进行了强化，但该架构可推广到其他用例，例如计算机视觉、计算神经科学、生物序列分析等。]]></description>
      <guid>https://arxiv.org/abs/2309.02094</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>FIT-RAG：具有事实信息和代币减少的黑盒 RAG</title>
      <link>https://arxiv.org/abs/2403.14374</link>
      <description><![CDATA[arXiv:2403.14374v1 公告类型：交叉
摘要：由于参数数量非常多，微调大型语言模型（LLM）以更新长尾或过时的知识在许多应用中是不切实际的。为了避免微调，我们也可以将 LLM 视为黑盒（即冻结 LLM 的参数），并使用检索增强生成（RAG）系统（即黑盒 RAG）对其进行增强。近年来，黑盒RAG在知识密集型任务中取得了成功，受到了广泛关注。现有的黑盒 RAG 方法通常会对检索器进行微调，以满足法学硕士的偏好，并将所有检索到的文档连接起来作为输入，这存在两个问题：（1）忽略事实信息。 LLM首选文件可能不包含给定问题的事实信息，这可能会误导检索者并损害黑盒RAG的有效性； (2) 代币浪费。简单地连接所有检索到的文档会给 LLM 带来大量不必要的标记，从而降低了黑盒 RAG 的效率。为了解决这些问题，本文提出了一种新颖的黑盒 RAG 框架，该框架利用检索中的事实信息并减少用于增强的标记数量，称为 FIT-RAG。 FIT-RAG 通过构建双标签文档评分器来利用事实信息。此外，它通过引入自知识识别器和子文档级令牌缩减器来减少令牌。 FIT-RAG 实现了卓越的有效性和效率，这一点通过三个开放域问答数据集（TriviaQA、NQ 和 PopQA）的大量实验得到了验证。 FIT-RAG 可以将 Llama2-13B-Chat 在 TriviaQA 上的回答准确率提高 14.3%，在 NQ 上提高 19.9%，在 PopQA 上提高 27.5%。此外，它在三个数据集中平均可以节省大约一半的令牌。]]></description>
      <guid>https://arxiv.org/abs/2403.14374</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:45 GMT</pubDate>
    </item>
    <item>
      <title>RecMind：大型语言模型驱动的推荐代理</title>
      <link>https://arxiv.org/abs/2308.14296</link>
      <description><![CDATA[arXiv:2308.14296v3 公告类型：替换
摘要：虽然推荐系统（RS）通过深度学习取得了显着进步，但当前的 RS 方法通常在特定于任务的数据集上训练和微调模型，从而限制了它们对新推荐任务的泛化能力以及由于模型规模而利用外部知识的能力和数据大小限制。因此，我们设计了一个由 LLM 驱动的自主推荐代理 RecMind，它能够利用外部知识，利用精心规划的工具来提供零样本个性化推荐。我们提出了一种自我激励算法来提高规划能力。在每个中间步骤中，法学硕士都会自我激励考虑所有先前探索过的状态来规划下一步。这种机制极大地提高了模型在规划推荐时理解和利用历史信息的能力。我们评估 RecMind 在各种推荐场景中的表现。我们的实验表明，RecMind 在各种任务中都优于现有的基于零/少样本 LLM 的推荐基线方法，并实现了与完全训练的推荐模型 P5 相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2308.14296</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:45 GMT</pubDate>
    </item>
    <item>
      <title>M3：用于开放域多跳密集句子检索的多任务混合目标学习框架</title>
      <link>https://arxiv.org/abs/2403.14074</link>
      <description><![CDATA[arXiv:2403.14074v1 公告类型：新
摘要：在最近的研究中，对比学习已被证明是一种高效的表示学习方法，并广泛用于密集检索。然而，我们发现仅仅依赖对比学习可能会导致检索性能不佳。另一方面，尽管许多检索数据集支持对比学习之外的各种学习目标，但在多任务学习场景中有效地将它们组合起来可能具有挑战性。在本文中，我们介绍了 M3，这是一种先进的递归多跳密集句子检索系统，它建立在一种用于密集文本表示学习的新颖的多任务混合目标方法之上，解决了上述挑战。我们的方法在大规模开放域事实验证基准数据集 FEVER 上产生了最先进的性能。代码和数据可访问：https://github.com/TonyBY/M3]]></description>
      <guid>https://arxiv.org/abs/2403.14074</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>了解稀疏用户反馈的推荐排名损失</title>
      <link>https://arxiv.org/abs/2403.14144</link>
      <description><![CDATA[arXiv:2403.14144v1 公告类型：新
摘要：点击率（CTR）预测在在线广告领域具有重要意义。虽然许多现有方法将其视为二元分类问题并利用二元交叉熵 (BCE) 作为优化目标，但最近的进展表明，将 BCE 损失与排名损失相结合可以带来显着的性能改进。然而，这种组合损失的全部功效仍不完全清楚。在本文中，我们发现了与稀疏正反馈场景（例如 CTR 预测）中的 BCE 损失相关的新挑战：负样本的梯度消失。随后，我们介绍了排名损失在 CTR 预测中的有效性的新视角，强调其在负样本上生成更大梯度的能力，从而减轻其优化问题并提高分类能力。我们的观点得到了对公开数据集进行的广泛理论分析和实证评估的支持。此外，我们成功将排名损失部署在腾讯网络广告系统中，实现两个主要场景的商品总价值（GMV）显着提升0.70%和1.26%。我们方法的代码可以在以下 GitHub 存储库中公开访问：https://github.com/SkylerLinn/Understanding-the-Ranking-Loss。]]></description>
      <guid>https://arxiv.org/abs/2403.14144</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>以用户为中心的子图网络的知识增强推荐</title>
      <link>https://arxiv.org/abs/2403.14377</link>
      <description><![CDATA[arXiv:2403.14377v1 公告类型：新
摘要：推荐系统目前在各种平台上广泛应用，它根据用户的喜好向用户推荐相关的项目。依赖于用户-项目交互矩阵的经典方法有局限性，特别是在缺乏新项目交互数据的情况下。基于知识图（KG）的推荐系统已成为一种有前景的解决方案。然而，大多数基于知识图谱的方法采用节点嵌入，不能为不同用户提供个性化推荐，也不能很好地推广到新项目。为了解决这些限制，我们提出了知识增强的以用户为中心的子图网络（KUCNet），这是一种使用图神经网络（GNN）进行有效推荐的子图学习方法。 KUCNet 为每个用户-项目对构建一个 U-I 子图，捕获用户-项目交互的历史信息和 KG 中提供的辅助信息。基于注意力的 GNN 旨在对 U-I 子图进行编码以进行推荐。考虑到效率，进一步引入了以用户为中心的剪枝计算图，使得可以同时计算多个U-I子图，并且可以通过个性化PageRank来剪枝大小。我们提出的方法实现了准确、高效和可解释的推荐，尤其是对于新项目。实验结果证明了 KUCNet 相对于最先进的基于 KG 和基于协同过滤 (CF) 的方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2403.14377</guid>
      <pubDate>Fri, 22 Mar 2024 06:16:44 GMT</pubDate>
    </item>
    </channel>
</rss>