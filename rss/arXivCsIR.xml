<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.IR 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 在 arXiv.org 电子印刷档案上进行更新。</description>
    <lastBuildDate>Mon, 30 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于推荐的位置感知图形转换器</title>
      <link>https://arxiv.org/abs/2412.18731</link>
      <description><![CDATA[arXiv:2412.18731v1 公告类型：新
摘要：协作推荐从根本上涉及从交互数据中学习高质量的用户和项目表示。最近，图卷积网络 (GCN) 通过利用交互图中的高阶连接模式推动了该领域的发展，PinSage 和 LightGCN 等最先进的方法就是明证。然而，现有解决方案尚未很好地解决一个关键限制：捕获远程协作过滤信号，这对于建模用户偏好至关重要。在这项工作中，我们提出了一个新的图变换器 (GT) 框架 - \textit{Position-aware Graph Transformer for Recommendation} (PGTR)，它将 Transformer 块的全局建模能力与 GCN 的局部邻域特征提取相结合。关键见解是通过几个专门设计的位置编码将用户项目交互图中的节点位置和结构信息明确地合并到 GT 架构中。然后将 Transformer 块的长距离协作信号与 GCN 主干中的局部邻域特征线性组合，以增强节点嵌入，从而获得最终建议。实证研究表明，在四个真实数据集上各种基于 GCN 的主干上实施所提出的 PGTR 方法时，该方法是有效的，并且对交互稀疏性和噪声具有鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2412.18731</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于社交推荐的自适应自监督学习</title>
      <link>https://arxiv.org/abs/2412.18735</link>
      <description><![CDATA[arXiv:2412.18735v1 公告类型：新
摘要：近年来，研究人员试图利用社会关系来提高推荐系统的性能。通常，大多数现有的社交推荐方法严重依赖于主要推荐任务的大量领域知识和专业知识来设计有用的辅助任务。同时，自监督学习（SSL）最近在推荐领域受到了广泛关注，因为它可以通过从没有人工注释标签的原始数据构建自监督辅助任务来提供自监督信号，以协助改进目标推荐系统。尽管取得了巨大的成功，但这些基于SSL的社交推荐不足以自适应地平衡各种自监督辅助任务，因为在各种辅助任务上分配相同的权重会导致次优推荐性能，其中不同的自监督辅助任务可能对不同数据集上的主要社交推荐的改进做出不同的贡献。为了解决这个问题，在这项工作中，我们提出了一种自适应自监督学习社交推荐（AdasRec），利用各种自监督辅助任务。具体来说，提出了一种自适应加权机制，为各种自监督辅助任务学习自适应权重，以平衡这些自监督辅助任务对增强社交推荐中的表征学习的贡献。自适应加权机制用于对辅助任务分配不同的权重，以实现对整个辅助任务的整体加权，并最终通过具有自适应加权网络的元学习优化问题来辅助主要推荐任务。在各种真实数据集上进行了全面的实验，以验证我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.18735</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论生成信息检索模型的鲁棒性</title>
      <link>https://arxiv.org/abs/2412.18768</link>
      <description><![CDATA[arXiv:2412.18768v1 公告类型：新
摘要：生成信息检索方法通过直接生成文档标识符来检索文档。人们投入了大量精力来开发有效的生成 IR 模型。但对这些模型的鲁棒性关注较少。评估生成 IR 模型的分布外 (OOD) 泛化至关重要，即，这些模型如何推广到新的分布？为了回答这个问题，我们从检索问题的四个角度关注 OOD 场景：(i) 查询变化；(ii) 看不见的查询类型；(iii) 看不见的任务；(iv) 语料库扩展。基于此分类法，我们进行实证研究，以分析代表性生成 IR 模型相对于密集检索模型的 OOD 鲁棒性。我们的实证结果表明，生成 IR 模型的 OOD 鲁棒性需要改进。通过检查生成式 IR 模型的 OOD 稳健性，我们旨在为开发更可靠的 IR 模型做出贡献。代码可在 \url{https://github.com/Davion-Liu/GR_OOD} 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.18768</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>链式攻击：引导大型语言模型以攻击黑盒神经排序模型</title>
      <link>https://arxiv.org/abs/2412.18770</link>
      <description><![CDATA[arXiv:2412.18770v1 公告类型：新
摘要：神经排名模型 (NRM) 在检索性能方面已被证明非常有效。不幸的是，它们也显示出比上一代模型更高的攻击敏感度。为了帮助揭示和解决这种缺乏鲁棒性的问题，我们引入了一个名为 Attack-in-the-Chain 的新型排名攻击框架，该框架基于思路链 (CoT) 提示跟踪大型语言模型 (LLM) 和 NRM 之间的交互，以在黑盒设置下生成对抗性示例。我们的方法首先将排名位置高于目标文档的锚文档识别为推理链中的节点。然后，我们动态地为每个节点分配扰动词的数量，并提示 LLM 执行攻击。最后，我们在每个推理步骤中验证所有节点的攻击性能，然后继续生成下一个推理步骤。两个网络搜索基准上的经验结果表明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.18770</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于搜索未来的思考：回归过去？</title>
      <link>https://arxiv.org/abs/2412.18956</link>
      <description><![CDATA[arXiv:2412.18956v1 公告类型：新
摘要：当您有问题时，获得问题答案的最有效方法是直接与该主题的专家联系并与他们交谈。在发明书写之前，这是唯一的方法。虽然有效，但这种解决方案表现出可扩展性的挑战。书写使知识得以具体化、保存和复制，使得几个世纪以来不同技术的发展能够将信息搜索者与相关信息联系起来。这一进程最终在最近出现生成式人工智能之前，形成了我们熟悉的十蓝链接网络搜索范式。然而，我们常常忘记，消费静态内容是一个不完美的解决方案。随着大型语言模型的出现，通过允许用户直接与专家互动，开发出卓越的体验已成为可能。这些互动当然可以满足信息需求，但专家模型可以做更多的事情。这个即将到来的未来需要重新构想搜索。]]></description>
      <guid>https://arxiv.org/abs/2412.18956</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不要迷失自我：通过减少图卷积网络中的节点邻居差异来增强多模态推荐</title>
      <link>https://arxiv.org/abs/2412.18962</link>
      <description><![CDATA[arXiv:2412.18962v1 公告类型：新
摘要：多媒体内容的快速扩展导致了多模态推荐系统的出现。它在推荐系统中引起了越来越多的关注，因为它充分利用了来自不同模态的数据，缓解了持续的数据稀疏问题。因此，多模态推荐模型可以在视觉和文本方面学习有关节点的个性化信息。为了进一步缓解数据稀疏问题，一些先前的研究为多模态推荐系统引入了图卷积网络 (GCN)，通过捕捉用户和项目之间的潜在关系来增强用户和项目的语义表示。然而，采用 GCN 不可避免地会引入过度平滑问题，这会使节点过于相似。不幸的是，合并多模态信息会加剧这一挑战，因为过于相似的节点将丢失通过多模态信息学习到的个性化信息。为了解决这个问题，我们提出了一个新模型，通过减少节点邻居差异 (RedN^nD) 在特征聚合过程中保留自我节点的个性化信息。在三个公共数据集上进行的大量实验表明，RedN^nD 在准确性和鲁棒性方面达到了最先进的性能，并且比现有的基于 GCN 的多模态框架有显著的改进。]]></description>
      <guid>https://arxiv.org/abs/2412.18962</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Jasper 和 Stella：SOTA 嵌入模型的提炼</title>
      <link>https://arxiv.org/abs/2412.19048</link>
      <description><![CDATA[arXiv:2412.19048v1 公告类型：新
摘要：许多深度学习应用（例如 FAQ 和 RAG）的关键组成部分是密集检索，其中嵌入模型用于将原始文本转换为数值向量，然后通过 MIPS（最大内积搜索）获取最相似的文本。已经建立了一些文本嵌入基准（例如 MTEB、BEIR 和 AIR-Bench）来准确评估嵌入模型。得益于这些基准，我们可以使用 SOTA 模型；然而，这些模型在工业中的部署和应用受到其大向量维度和众多参数的阻碍。为了缓解这个问题，1）我们提出了一种蒸馏技术，可以使较小的学生模型实现良好的性能。2）受 MRL 的启发，我们提出了一种基于其自身向量或其教师向量降低向量维度的训练方法。3）我们在图像和文本之间进行简单但有效的对齐训练，使我们的模型成为一个多模态编码器。我们利用上述技术训练了 Stella 和 Jasper 模型，并在 MTEB 排行榜上取得了高分。我们在 Hugging Face Hub（https://huggingface.co/infgrad/jasper_en_vision_language_v1）发布了模型和数据，训练日志位于 https://api.wandb.ai/links/dunnzhang0/z8jqoqpb。]]></description>
      <guid>https://arxiv.org/abs/2412.19048</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向流行度感知推荐：具有正交性约束的多行为增强框架</title>
      <link>https://arxiv.org/abs/2412.19172</link>
      <description><![CDATA[arXiv:2412.19172v1 公告类型：新
摘要：Top-$K$ 推荐涉及推断潜在用户偏好并据此生成个性化推荐，这在各种决策系统中无处不在。尽管如此，推荐系统通常存在严重的 \textit{流行度偏差}，导致过度推荐流行商品。这种偏差偏离了如实地反映用户偏好的核心目标，损害了客户满意度和零售商利润。尽管流行度偏差很普遍，但现有的解决流行度偏差的方法仍然存在局限性，因为存在相当大的准确度-去偏差权衡和对广泛参数选择的敏感性，而积极的用户-商品交互的极端稀疏性进一步加剧了这种局限性。
在本文中，我们提出了一种集成多行为 \textbf{S}ide \textbf{I}nformation (PopSI) 的 \textbf{Pop}ularity-aware top-$K$ 推荐算法，旨在同时提高推荐准确性和去偏性能。具体而言，通过利用反映相似用户偏好的多个用户反馈并将其表示为三维张量，PopSI 可以利用所有切片有效地捕获所需的用户偏好。随后，我们引入了一种新颖的正交性约束来细化估计的项目特征空间，强制它对项目流行度特征不变，从而解决我们模型对流行度偏差的敏感性。在现实世界的电子商务数据集上进行的全面实验表明，PopSI 比最先进的去偏方法有普遍的改进，具有边际准确性-去偏权衡和可扩展到实际应用。我们的算法和实验的源代码可在 \url{https://github.com/Eason-sys/PopSI} 获得。]]></description>
      <guid>https://arxiv.org/abs/2412.19172</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>优化多阶段语言模型以实现有效的文本检索</title>
      <link>https://arxiv.org/abs/2412.19265</link>
      <description><![CDATA[arXiv:2412.19265v1 公告类型：新
摘要：高效的文本检索对于法律文件分析等应用至关重要，特别是在日本法律体系等专业环境中。现有的检索方法在这种特定领域的场景中往往表现不佳，因此需要量身定制的方法。在本文中，我们介绍了一种针对日本法律数据集优化的新型两阶段文本检索流程。我们的方法利用先进的语言模型来实现最先进的性能，显着提高检索效率和准确性。为了进一步增强稳健性和适应性，我们采用了一个集成多种检索策略的集成模型，从而在不同任务中获得了卓越的结果。大量实验验证了我们方法的有效性，在日本法律数据集和 MS-MARCO 等广受认可的基准上都表现出色。我们的工作为特定领域和一般背景下的文本检索建立了新标准，为解决法律和多语言环境中的复杂查询提供了全面的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2412.19265</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RecLM：推荐指令调整</title>
      <link>https://arxiv.org/abs/2412.19302</link>
      <description><![CDATA[arXiv:2412.19302v1 公告类型：新
摘要：现代推荐系统旨在通过用户过去的互动深入了解用户的复杂偏好。虽然使用图神经网络 (GNN) 的深度协同过滤方法擅长捕获用户-项目关系，但它们在处理稀疏数据或零样本场景时有效性有限，这主要是由于基于 ID 的嵌入函数的限制。为了应对这些挑战，我们提出了一种与模型无关的推荐指令调整范式，将大型语言模型与协同过滤无缝集成。我们提出的推荐语言模型 (RecLM) 通过精心设计的强化学习奖励函数增强了对用户偏好多样性的捕获，从而促进了语言模型的自我增强。全面的评估证明了我们的方法在各种设置中都具有显着的优势，并且它与最先进的推荐系统的即插即用兼容性可显着提高性能。]]></description>
      <guid>https://arxiv.org/abs/2412.19302</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从兴趣到见解：使用自然语言查询进行课程推荐的法学硕士方法</title>
      <link>https://arxiv.org/abs/2412.19312</link>
      <description><![CDATA[arXiv:2412.19312v1 公告类型：新
摘要：美国的大多数大学都鼓励学生在宣布专业之前探索学术领域，并通过满足各种要求来获得学术广度。每个学期，学生必须从涉及数十个学科领域的数千种课程中选择少数几门课程。课程环境也是动态的，校园内沟通不畅和搜索功能不佳会限制学生发现感兴趣的新课程的能力。为了在这样的环境中支持学生及其顾问，我们探索了一种新颖的大型语言模型 (LLM) 课程推荐系统，该系统将检索增强生成 (RAG) 方法应用于课程描述语料库。系统首先根据用户的查询生成“理想”课程描述。使用嵌入将此描述转换为搜索向量，然后通过比较嵌入相似性来使用该向量查找具有相似内容的实际课程。我们描述了该方法并评估了一些示例提示的质量和公平性。讨论了在校园内部署试点系统的步骤。]]></description>
      <guid>https://arxiv.org/abs/2412.19312</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>结合协同过滤和大型语言模型的增强推荐</title>
      <link>https://arxiv.org/abs/2412.18713</link>
      <description><![CDATA[arXiv:2412.18713v1 Announce Type: cross 
摘要：随着信息爆炸时代的到来，推荐系统在各类应用中的重要性日益凸显。传统的协同过滤算法因其能有效捕捉用户行为模式而被广泛应用，但在处理冷启动问题和数据稀疏性时受到限制。大型语言模型（LLM）以其强大的自然语言理解和生成能力，为推荐系统提供了新的突破。本研究提出一种结合协同过滤和LLM的增强推荐方法，旨在发挥协同过滤在建模用户偏好方面的优势，同时通过LLM增强对用户和物品文本信息的理解，提高推荐的准确性和多样性。本文首先介绍了协同过滤和LLM的基本理论，然后设计了一个融合两者的推荐系统架构，并通过实验验证了系统的有效性。结果表明，基于协同过滤和LLM的混合模型显著提高了准确率、召回率和用户满意度，在复杂的推荐场景中展现出潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.18713</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中协同过滤算法的优化和可扩展性</title>
      <link>https://arxiv.org/abs/2412.18715</link>
      <description><![CDATA[arXiv:2412.18715v1 公告类型：交叉 
摘要：随着大型语言模型 (LLM) 的快速发展和对个性化内容的需求不断增长，推荐系统已成为提升用户体验和推动参与的关键。协同过滤算法是许多推荐系统的核心，因其效率和可解释性而备受关注。然而，传统的协同过滤方法在集成到基于 LLM 的大型系统中时面临许多挑战，包括高计算成本、严重的数据稀疏性、冷启动问题和缺乏可扩展性。本文研究了大型语言模型中协同过滤算法的优化和可扩展性，并通过高级优化策略解决了这些限制。首先，我们分析了协同过滤算法的基本原理及其在基于 LLM 的环境中应用的局限性。接下来，提出了几种优化技术，如矩阵分解、近似最近邻搜索和并行计算，以提高计算效率和模型准确性。此外，还探索了分布式架构和模型压缩等策略，以促进数据密集型环境中的动态更新和可扩展性。]]></description>
      <guid>https://arxiv.org/abs/2412.18715</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Zema 数据集：对 Yaredawi Zema 的全面研究，重点关注时钟颂歌</title>
      <link>https://arxiv.org/abs/2412.18784</link>
      <description><![CDATA[arXiv:2412.18784v1 公告类型：交叉 
摘要：计算音乐研究在推动全球各种音乐风格的音乐制作、发行和理解方面发挥着关键作用。尽管具有巨大的文化和宗教意义，但埃塞俄比亚东正教 Tewahedo 教堂 (EOTC) 的圣歌在计算音乐研究中相对代表性不足。本文通过引入一个专门用于分析 EOTC 圣歌的新数据集（也称为 Yaredawi Zema）为该领域做出了贡献。这项工作全面概述了一个 10 小时的数据集、369 个实例、创建和管理过程，包括严格的质量保证措施。我们的数据集具有详细的单词级时间边界和阅读音调注释以及相应的音频吟唱模式标签。此外，我们还通过对它们进行相应的注释，确定了与手稿中的多个吟唱符号相关的吟唱选项。我们向公众开放此数据集 1 的目的是鼓励对 EOTC 圣歌进行更多研究和学习，包括歌词转录、歌词与音频对齐以及音乐生成任务。此类研究工作将促进知识和努力，以保护这种独特的礼拜音乐，这是埃塞俄比亚人民的无价文化遗产。]]></description>
      <guid>https://arxiv.org/abs/2412.18784</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FOR：针对对象级开放词汇图像检索进行微调</title>
      <link>https://arxiv.org/abs/2412.18806</link>
      <description><![CDATA[arXiv:2412.18806v1 公告类型：交叉 
摘要：随着处理大型数据集成为标准，通过开放集文本查询准确检索包含感兴趣对象的图像的任务变得非常重要。当前领先的方法利用预先训练的 CLIP 模型，而无需对目标域进行任何调整，通过额外的后处理来平衡准确性和效率。在这项工作中，我们提出了 FOR：以对象为中心的开放词汇图像检索的微调，它允许使用闭集标签对目标数据集进行微调，同时保持对开放词汇检索至关重要的视觉语言关联。FOR 基于两个设计元素：针对预期任务定制的 CLIP 头的专用解码器变体，以及它在多目标训练框架内的耦合。这些设计选择共同显著提高了准确性，在三个数据集上展示了比 SoTA 高达 8 mAP@50 点的改进。此外，我们证明 FOR 在半监督环境中也有效，即使只有一小部分数据集被标记也能取得令人印象深刻的结果。]]></description>
      <guid>https://arxiv.org/abs/2412.18806</guid>
      <pubDate>Mon, 30 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>