<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.IR 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR 在 arXiv.org 电子印刷档案上进行更新。</description>
    <lastBuildDate>Mon, 24 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>信任与准确性的故事：RAG 系统中的基础法学硕士与指导法学硕士</title>
      <link>https://arxiv.org/abs/2406.14972</link>
      <description><![CDATA[arXiv:2406.14972v1 公告类型：交叉 
摘要：检索增强生成 (RAG) 代表了人工智能的一项重大进步，它将检索阶段与生成阶段相结合，后者通常由大型语言模型 (LLM) 提供支持。RAG 中当前的常见做法涉及使用“指导式”LLM，这些 LLM 通过监督训练进行微调，以增强其遵循指令的能力，并使用最先进的技术与人类偏好保持一致。与普遍看法相反，我们的研究表明，在我们的实验环境下，基础模型在 RAG 任务中的表现平均比指导式模型高出 20%。这一发现挑战了关于指导式 LLM 在 RAG 应用中的优越性的普遍假设。进一步的调查揭示了一个更微妙的情况，质疑了 RAG 的基本方面，并表明需要就该主题进行更广泛的讨论；或者，正如弗洛姆所说，“仅仅看一眼统计数据，很少能够理解这些数字的含义”。]]></description>
      <guid>https://arxiv.org/abs/2406.14972</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:29 GMT</pubDate>
    </item>
    <item>
      <title>检索增强生成中的“生成后接地”多跳问答</title>
      <link>https://arxiv.org/abs/2406.14891</link>
      <description><![CDATA[arXiv:2406.14891v1 公告类型：交叉 
摘要：多跳问答 (MHQA) 任务对大型语言模型 (LLM) 提出了重大挑战，因为需要大量知识。当前的解决方案，如检索增强生成，通常从外部语料库中检索潜在文档以读取答案。然而，这种检索后阅读范式的性能受到检索器和检索到的文档中不可避免的噪音的限制。为了缓解这些挑战，我们引入了一个新颖的生成后接地 (GenGround) 框架，协同 LLM 和外部文档的参数知识来解决多跳问题。GenGround 使 LLM 能够交替进行两个阶段，直到得出最终答案：(1) 制定一个更简单的单跳问题并直接生成答案；(2) 将问答对接地到检索到的文档中，修改答案中的任何错误预测。我们还提出了一种指导性的基础蒸馏方法，将我们的方法推广到更小的模型。在四个数据集上进行的大量实验说明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2406.14891</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:28 GMT</pubDate>
    </item>
    <item>
      <title>说到做到并不代表做到：论大型语言模型在词汇蕴涵识别中的局限性</title>
      <link>https://arxiv.org/abs/2406.14894</link>
      <description><![CDATA[arXiv:2406.14894v1 公告类型：交叉 
摘要：动词是语言的支柱，为句子提供结构和意义。然而，它们复杂的语义细微差别带来了长期的挑战。通过词汇蕴涵的概念理解动词关系对于理解句子意义和掌握动词动态至关重要。这项工作调查了八个大型语言模型通过不同设计的提示策略和零/少样本设置对两个词汇数据库（即 WordNet 和 HyperLex）中的动词对识别动词之间的词汇蕴涵关系的能力。我们的研究结果表明，这些模型可以以中等良好的性能处理词汇蕴涵识别任务，尽管有效性程度不同且条件不同。此外，利用少样本提示可以提高模型的性能。然而，完美地解决这个任务对于所有经过检查的 LLM 来说都是一个未解决的挑战，这为进一步研究这一主题提供了契机。]]></description>
      <guid>https://arxiv.org/abs/2406.14894</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:28 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT 作为研究科学家：探究 GPT 作为研究图书管理员、研究伦理学家、数据生成器和数据预测器的能力</title>
      <link>https://arxiv.org/abs/2406.14765</link>
      <description><![CDATA[arXiv:2406.14765v1 公告类型：交叉 
摘要：ChatGPT 作为一名研究科学家有多优秀？我们使用心理科学作为测试领域，系统地探究了 GPT-3.5 和 GPT-4 在科学过程的四个核心组成部分中的能力：作为研究图书管理员、研究伦理学家、数据生成器和新数据预测器。在研究 1（研究图书管理员）中，与人类研究人员不同，GPT-3.5 和 GPT-4 产生了幻觉，分别有 36.0% 和 5.4% 的时间权威地生成虚构的参考资料，尽管 GPT-4 表现出了不断进化的承认其虚构的能力。在研究 2（研究伦理学家）中，GPT-4（虽然不是 GPT-3.5）被证明能够检测到虚构研究协议中的 p-hacking 等违规行为，纠正了 88.6% 的明显问题和 72.6% 的微妙问题。在研究 3（数据生成器）中，两个模型一致地复制了先前在大型语言语料库中发现的文化偏见模式，这表明 ChatGPT 可以模拟已知结果，这是数据生成和假设生成等技能有用的前提条件。相比之下，在研究 4（新数据预测器）中，两个模型都无法成功预测训练数据中不存在的新结果，而且在预测更多或更少的新结果时，它们似乎都没有利用大量新信息。总之，这些结果表明，GPT 是一个有缺陷但正在迅速进步的图书管理员，已经是一个优秀的研究伦理学家，能够在具有已知特征的简单领域生成数据，但不擅长预测新的经验数据模式以帮助未来的实验。]]></description>
      <guid>https://arxiv.org/abs/2406.14765</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:27 GMT</pubDate>
    </item>
    <item>
      <title>利用段落嵌入对大型语言模型进行高效的列表重新排序</title>
      <link>https://arxiv.org/abs/2406.14848</link>
      <description><![CDATA[arXiv:2406.14848v1 公告类型：交叉 
摘要：最近的研究已经证明了在段落排名中使用大型语言模型 (LLM) 的有效性。列表方法（例如 RankGPT）已成为此任务的新前沿。然而，RankGPT 模型的效率受到最大上下文长度和 LLM 推理相对较高的延迟的限制。为了解决这些问题，在本文中，我们提出了 PE-Rank，利用单个段落嵌入作为良好的上下文压缩来实现高效的列表段落重新排名。通过将每个段落视为特殊标记，我们可以直接将段落嵌入输入 LLM，从而减少输入长度。此外，我们引入了一种推理方法，可以动态地将解码空间限制为这些特殊标记，从而加速解码过程。为了使模型适应重新排名，我们采用列表学习对排名损失进行训练。多个基准测试的评估结果表明，PE-Rank 显着提高了预填充和解码的效率，同时保持了具有竞争力的排名效果。 {代码可在 \url{https://github.com/liuqi6777/pe_rank} 获取。}]]></description>
      <guid>https://arxiv.org/abs/2406.14848</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:27 GMT</pubDate>
    </item>
    <item>
      <title>Bioptic——基于疗效且不受靶点影响的小分子搜索引擎</title>
      <link>https://arxiv.org/abs/2406.14572</link>
      <description><![CDATA[arXiv:2406.14572v1 公告类型：交叉 
摘要：虚拟筛选最近取得的成功得益于大型模型和广泛的化学库。然而，将这些元素结合起来具有挑战性：模型越大，运行成本就越高，因此超大型库不可行。为了解决这个问题，我们开发了一种与目标无关、基于功效的分子搜索模型，该模型使我们能够找到具有相似生物活性的结构不同的分子。我们使用最佳实践来设计基于处理器优化的 SIMD 指令的快速检索系统，使我们能够以 100% 的召回率筛选超大型 40B Enamine REAL 库。我们对我们的模型和几种最先进的模型进行了广泛的基准测试，以了解新分子的速度性能和检索质量。]]></description>
      <guid>https://arxiv.org/abs/2406.14572</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:26 GMT</pubDate>
    </item>
    <item>
      <title>TTQA-RS-一种具有推理和总结功能的多跳表格文本问答分解提示方法</title>
      <link>https://arxiv.org/abs/2406.14732</link>
      <description><![CDATA[arXiv:2406.14732v1 公告类型：交叉 
摘要：多年来，表格和文本上的问答 (QA) 越来越受欢迎。多跳表格文本 QA 需要在表格和文本之间进行多次跳跃，这使其成为一项具有挑战性的 QA 任务。尽管有几项工作试图解决表格文本 QA 任务，但大多数涉及训练模型并需要标记数据。在本文中，我们提出了一个模型 - TTQA-RS：一种用于具有推理和总结的多跳表格文本问答的分解提示方法。我们的模型使用增强知识，包括表格文本摘要和分解的子问题及答案，用于基于推理的表格文本 QA。使用开源语言模型，我们的模型在现有的表格文本 QA 数据集（如 HybridQA 和 OTT-QA 的开发集）上优于所有现有的表格文本 QA 任务提示方法。我们的结果与基于训练的最先进的模型相当，证明了使用开源 LLM 的基于提示的方法的潜力。此外，通过使用 GPT-4 和 LLaMA3-70B，我们的模型在多跳表格文本 QA 中实现了基于提示的方法的最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.14732</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:26 GMT</pubDate>
    </item>
    <item>
      <title>面向生成文本的细粒度引用评估：忠诚度指标的比较分析</title>
      <link>https://arxiv.org/abs/2406.15264</link>
      <description><![CDATA[arXiv:2406.15264v1 公告类型：新
摘要：大型语言模型 (LLM) 通常会产生不受支持或无法验证的信息，称为“幻觉”。为了缓解这种情况，检索增强型 LLM 结合了引文，将内容建立在可验证的来源中。尽管取得了这样的进展，但手动评估引文对相关陈述的支持程度仍然是一项重大挑战。以前的研究使用忠诚度指标来自动估计引文支持，但仅限于二元分类，忽略了实际场景中的细粒度引文支持。为了研究忠诚度指标在细粒度场景中的有效性，我们提出了一个比较评估框架，该框架评估了指标在区分三类支持级别（完全支持、部分支持和不支持）的引文方面的有效性。我们的框架采用相关性分析、分类评估和检索评估来全面衡量指标分数与人类判断之间的一致性。我们的结果表明，没有哪一项指标在所有评估中始终表现优异，这揭示了评估细粒度支持的复杂性。根据研究结果，我们为制定更有效的指标提供了实用建议。]]></description>
      <guid>https://arxiv.org/abs/2406.15264</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:25 GMT</pubDate>
    </item>
    <item>
      <title>STARD：包含非专业人士真实查询的中国法规检索数据集</title>
      <link>https://arxiv.org/abs/2406.15313</link>
      <description><![CDATA[arXiv:2406.15313v1 公告类型：新
摘要：法规检索旨在为特定查询找到相关的法规条款。该过程是法律咨询、自动化司法判决、法律文件起草等广泛法律应用的基础。现有的法规检索基准侧重于来自律师资格考试和法律案例文件等来源的正式和专业查询，从而忽略了来自公众的非专业查询，这些查询通常缺乏精确的法律术语和参考。为了解决这一差距，我们推出了法规检索数据集 (STARD)，这是一个中国数据集，包含从现实世界的法律咨询中收集的 1,543 个查询案例和 55,348 条候选法规条款。与主要关注专业法律查询的现有法规检索数据集不同，STARD 捕捉了来自公众的真实查询的复杂性和多样性。通过对各种检索基线的全面评估，我们发现现有的检索方法都无法满足非专业用户提出的真实查询。最好的方法仅实现了 0.907 的 Recall@100，这表明有必要进一步探索和研究这一领域。
所有代码和数据集均可从以下网址获取：https://github.com/oneal2000/STARD/tree/main]]></description>
      <guid>https://arxiv.org/abs/2406.15313</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:25 GMT</pubDate>
    </item>
    <item>
      <title>\'评估大语言模型（LLM）对历史学家问题的响应能力</title>
      <link>https://arxiv.org/abs/2406.15173</link>
      <description><![CDATA[arXiv:2406.15173v1 公告类型：新
摘要：大型语言模型 (LLM) （如 ChatGPT 或 Bard）彻底改变了信息检索，并以其在创纪录的时间内生成自定义响应的能力吸引了观众，无论主题如何。在本文中，我们评估了各种 LLM 在用法语生成可靠、全面且足够相关的历史事实响应方面的能力。为此，我们构建了一个测试平台，其中包含许多与历史相关的问题，这些问题类型、主题和难度级别各不相同。我们对十个选定的 LLM 的答复的评估揭示了实质和形式上的许多缺陷。除了总体准确率不足之外，我们还强调了对法语的不均衡处理，以及 LLM 提供的答复中的冗长和不一致问题。]]></description>
      <guid>https://arxiv.org/abs/2406.15173</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>检索增强零样本文本分类</title>
      <link>https://arxiv.org/abs/2406.15241</link>
      <description><![CDATA[arXiv:2406.15241v1 公告类型：新
摘要：零样本文本学习使文本分类器能够有效地处理看不见的类，从而减轻了对特定于任务的训练数据的需求。一种简单的方法通常依赖于将查询（文本）的嵌入与潜在类的嵌入进行比较。然而，简单查询的嵌入有时缺乏丰富的上下文信息，这会阻碍分类性能。传统上，这是通过昂贵的训练来改进嵌入模型来解决的。我们引入了 QZero，这是一种新颖的无需训练的知识增强方法，它通过从维基百科中检索支持类别来重新制定查询，以提高零样本文本分类性能。我们在六个不同数据集上进行的实验表明，QZero 无需重新训练即可提高最先进的静态和上下文嵌入模型的性能。值得注意的是，在新闻和医学主题分类任务中，QZero 分别将最大的 OpenAI 嵌入模型的性能提高了至少 5% 和 3%。 QZero 充当知识放大器，使小型词嵌入模型能够达到与大型上下文模型相当的性能水平，从而有可能节省大量计算资源。此外，QZero 还提供有意义的见解，阐明查询上下文并验证主题相关性，帮助理解模型预测。总体而言，QZero 改进了基于嵌入的零样本分类器，同时保持了其简单性。这使得它对于资源受限的环境和信息不断变化的领域特别有价值。]]></description>
      <guid>https://arxiv.org/abs/2406.15241</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 RAGElo 评估 RAG-Fusion：基于 Elo 的自动化框架</title>
      <link>https://arxiv.org/abs/2406.14783</link>
      <description><![CDATA[arXiv:2406.14783v1 公告类型：新 
摘要：检索增强生成 (RAG) 问答 (QA) 系统的自动评估面临的挑战包括领域特定知识中的幻觉问题以及公司内部任务缺乏黄金标准基准。这导致在英飞凌科技的产品 QA 任务中评估 RAG 变体（如 RAG-Fusion (RAGF)）变得困难。为了解决这些问题，我们提出了一个全面的评估框架，该框架利用大型语言模型 (LLM) 根据真实用户查询和域内文档生成大量合成查询数据集，使用 LLM 作为评判者对检索到的文档和答案进行评级，评估答案的质量，并使用 RAGElo 的基于 Elo 的自动竞争对检索增强生成 (RAG) 代理的不同变体进行排名。 LLM-as-a-judge 对随机样本合成查询的评分显示，在相关性、准确性、完整性和精确度方面，与领域专家评分呈中等正相关。虽然 RAGF 在 Elo 分数方面优于 RAG，但针对专家注释的显着性分析也表明，RAGF 在完整性方面明显优于 RAG，但在精确度方面表现不佳。此外，基于 MRR@5 分数，英飞凌的 RAGF 助手在文档相关性方面表现出略高的性能。我们发现 RAGElo 与人类注释者的偏好呈正相关，但仍需谨慎行事。最后，RAGF 的方法可以根据专家注释得出更完整的答案，并根据 RAGElo 的评估标准得出更好的答案。]]></description>
      <guid>https://arxiv.org/abs/2406.14783</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>解码问题：解决 LLM 推荐的放大偏差和同质性问题</title>
      <link>https://arxiv.org/abs/2406.14900</link>
      <description><![CDATA[arXiv:2406.14900v1 公告类型：新
摘要：鉴于生成项目和自然语言之间存在固有差异，调整大型语言模型 (LLM) 进行推荐需要仔细考虑解码过程。现有方法通常直接应用 LLM 的原始解码方法。然而，我们发现这些方法遇到了重大挑战：1) 放大偏差 - 标准长度规范化会使包含生成概率接近 1 的标记的项目的分数膨胀（称为幽灵标记），以及 2) 同质性问题 - 为用户生成多个相似或重复的项目。为了应对这些挑战，我们引入了一种名为去偏差多样化解码 (D3) 的新解码方法。D3 禁用幽灵标记的长度规范化以减轻放大偏差，并结合无文本辅助模型来鼓励 LLM 生成频率较低的标记以抵消推荐同质性。在真实数据集上进行的大量实验证明了该方法在提高准确性和多样性方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.14900</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>具有局部性的身份：基因序列搜索的理想哈希</title>
      <link>https://arxiv.org/abs/2406.14901</link>
      <description><![CDATA[arXiv:2406.14901v1 公告类型：新
摘要：基因序列搜索是计算基因组学中的一项基本操作。由于基因组档案的 PB 级规模，大多数基因搜索系统现在使用基于哈希的数据结构，例如布隆过滤器 (BF)。最先进的系统，如紧凑位切片签名索引 (COBS) 和重复和合并布隆过滤器 (RAMBO)，使用带有随机哈希 (RH) 函数的 BF 进行基因表示和识别。标准方法是将基因搜索问题转化为一系列成员问题，测试 Q 的每个后续基因子串（称为 kmer）是否存在于整个基因数据库 D 的 kmer 集合中。我们观察到，RH 函数对 BF 的内存和计算优势至关重要，但也会损害基因搜索系统的系统性能。虽然随后查询的 kmers 可能非常相似，但 RH 会忽略任何相似性，将 kmers 均匀分布到可能很大的 BF 的不同部分，从而触发过多的缓存未命中并导致系统变慢。我们提出了一种称为身份与局部性 (IDL) 哈希系列的新型哈希函数，它将键放在输入空间中不会引起冲突。这种方法既确保了缓存局部性，又确保了键的保存。IDL 函数可以作为 RH 函数的直接替代品，并有助于提高信息检索系统的性能。我们给出了一个简单但实​​用的 IDL 函数系列构造，并表明用 IDL 函数替换 RH 可将缓存未命中率降低 5 倍，从而将 COBS 和 RAMBO 等 SOTA 方法的查询和索引时间提高 2 倍，而不会影响其质量。我们还对 BF 与 IDL 函数的误报率进行了理论分析。我们的哈希函数是第一个将局部敏感哈希 (LSH) 和 RH 连接起来以获得缓存效率的研究。]]></description>
      <guid>https://arxiv.org/abs/2406.14901</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:23 GMT</pubDate>
    </item>
    <item>
      <title>RE-AdaptIR：通过逆向工程改进信息检索</title>
      <link>https://arxiv.org/abs/2406.14764</link>
      <description><![CDATA[arXiv:2406.14764v1 公告类型：新
摘要：针对文本检索进行微调的大型语言模型 (LLM) 已在多个信息检索 (IR) 基准中展示了最先进的结果。然而，改进这些模型的监督训练需要大量标记示例，这些示例通常不可用或获取成本高昂。在这项工作中，我们探索了将逆向工程适应扩展到信息检索环境 (RE-AdaptIR) 的有效性。我们使用 RE-AdaptIR 仅使用未标记数据来改进基于 LLM 的 IR 模型。我们在训练领域以及模型未见过查询的领域的零样本中都展示了改进的性能。我们分析了各种微调场景中的性能变化，并为从业者提供了可立即使用的结果。]]></description>
      <guid>https://arxiv.org/abs/2406.14764</guid>
      <pubDate>Tue, 25 Jun 2024 03:16:22 GMT</pubDate>
    </item>
    </channel>
</rss>