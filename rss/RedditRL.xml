<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 21 Jul 2024 18:18:27 GMT</lastBuildDate>
    <item>
      <title>使用自定义 Python 和 Unity 引擎完成的虚拟 AI 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</link>
      <description><![CDATA[       由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</guid>
      <pubDate>Sun, 21 Jul 2024 15:19:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MARL 解决 N vs N 追击-逃避游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</link>
      <description><![CDATA[有谁知道使用 MARL 解决 N vs N 追击规避游戏的 SOTA 工作吗？我研究过基于策略的方法（例如 MADDPG、MAPPO）和基于价值的方法（例如 QMIX、VDN、COMA 等）。大部分工作都是针对完全竞争场景（例如 GRF、SMAC、hanabi 挑战、使用启发式或随机移动的固定对手的 MPE）完成的。MADDPG 仅在混合动机场景（N vs 1）中解决了 MPE（多智能体粒子环境），其中合作智能体数量较少，对手为 1 个，由 MADDPG vs DDPG 捕获。我认为它在 N vs N 情况下无法扩展，因为 N 更高。如果有人能提出使用 MARL 进行混合动机和完全竞争设置的任何工作或想法，那将很有帮助。    提交人    /u/Meta_Sage_247   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</guid>
      <pubDate>Sun, 21 Jul 2024 13:36:51 GMT</pubDate>
    </item>
    <item>
      <title>“学习用语言模拟世界”，Lin 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8bbw1/learning_to_model_the_world_with_language_lin_et/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8bbw1/learning_to_model_the_world_with_language_lin_et/</guid>
      <pubDate>Sun, 21 Jul 2024 01:47:37 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理出现奇怪的周期性峰值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</guid>
      <pubDate>Sat, 20 Jul 2024 23:02:51 GMT</pubDate>
    </item>
    <item>
      <title>DQN 高估问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</link>
      <description><![CDATA[      我一直在尝试从头开始实现 DQN，并创建了一个测试环境来展示我遇到的问题。环境在每一步奖励 1，并在 100 步后终止。 DQN 不采取任何行动，仅尝试根据步骤号预测每一步的累积奖励。 问题是，DQN 为后续状态输出极高的 Q 值，这与实际值相反，并且需要几个时期才能开始输出半准确值。 https://preview.redd.it/n1fd4g054jdd1.png?width=915&amp;format=png&amp;auto=webp&amp;s=5d32fa33a3ee048528b39b61239d26b9dfaf0d05 代码（根据用户 dieplstks 的评论略作修改）： import torch from torch import nn, optim import matplotlib.pyplot 作为 plt 导入随机 导入 numpy 作为 np 比例 = 100 折扣 = .9 奖励 = [1 for i in range(scale-1)] + [0] net = nn.Sequential( nn.Linear(4,32), nn.Mish(), nn.Linear(32,32), nn.Mish(), nn.Linear(32,1)) opt = optim.Adam(net.parameters()，lr=1e-2) loss = nn.MSELoss() def step(c): global discount, rewards, net, opt opt.zero_grad() qvals = [] for i in range(scale): inp = torch.tensor([[i/20 for _ in range(4)]], dtype=torch.float32) q = net(inp) qvals.append(q) if i == scale-1: next_q = torch.zeros(1,1) else: next_inp = torch.tensor([[(i+1)/20 for _ in range(4)]], dtype=torch.float32) next_q = net(next_inp).detach() target = discount*next_q + rewards[i] q_loss = loss(q, target) #opt.zero_grad() q_loss.backward() nn.utils.clip_grad_norm_(net.parameters(), 1) opt.step() if c%1 == 0: plt.plot(list(range(scale)), actual_vals) plt.plot(list(range(scale)), torch.concat(qvals).detach().numpy()) plt.savefig(f&#39;zoo{c}.png&#39;) plt.clf() #plt.show() actual_vals = np.zeros((scale),dtype=np.float32) next_val = 0 for i in reversed(range(scale)): current_val = next_val * discount + rewards[i] actual_vals[i] = current_val next_val = current_val for i in range(100): step(i)     提交人    /u/AUser213   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</guid>
      <pubDate>Fri, 19 Jul 2024 19:51:56 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习用于推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</link>
      <description><![CDATA[嗨：我正在寻找合适的 Python（最好是基于 PyTorch）深度强化学习库来实现多会话对话推荐引擎。理想情况下，强化学习将无模型且脱离策略，并且必须与数据库系统交互以获取用户偏好和其他上下文数据。有什么建议吗？    提交人    /u/Extra_Reflection9056   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</guid>
      <pubDate>Fri, 19 Jul 2024 08:00:25 GMT</pubDate>
    </item>
    <item>
      <title>通过将实时屏幕截图作为输入并预测要模拟的 Windows 鼠标/键盘输入，训练 DQN 代理来玩自定义 Fortnite 地图。以下是可视化的卷积过滤器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</link>
      <description><![CDATA[        提交人    /u/voidupdate   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</guid>
      <pubDate>Fri, 19 Jul 2024 01:35:34 GMT</pubDate>
    </item>
    <item>
      <title>RL 教科书包含逆向强化学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</link>
      <description><![CDATA[寻找包含逆强化学习 (IRL) 部分的 RL 教科书。我只熟悉 Dixon 的《金融机器学习》一书，这本书很棒，但我还在寻找更多值得阅读的内容。  任何建议都值得赞赏！此外，如果您知道任何带有相应 github 的 IRL 论文，我也会喜欢的。    提交人    /u/Voltimeters   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</guid>
      <pubDate>Fri, 19 Jul 2024 00:28:38 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 openAI Gym 中自定义环境的平等约束问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</link>
      <description><![CDATA[您好， 我正在为优化主题创建自定义环境。以下是一些详细信息： 代理观察一个连续变量：a 代理采取两个操作：b 和 c 我希望我的代理学习采取操作 b、c，使得 b + c = a 且 c 最大。 我知道这很简单。在这种情况下，代理应该采取操作 c，使得 c=a，但这只是我整个环境的一个小组成部分。 * 如何以尊重此约束的方式对环境进行建模（应始终尊重约束） * 如何对奖励进行建模。我试图将奖励作为 c，但由于这是一个绝对值，因此代理不会改善行为。 * 知道我的观察和行动都是连续变量，哪种类型的算法最适合这类问题。 提前谢谢您。    提交人    /u/Effective_Farm_4844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</guid>
      <pubDate>Thu, 18 Jul 2024 18:11:33 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 更新了，有什么不同</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</link>
      <description><![CDATA[      DreamerV3 最近已更新。论文中有一些变化。不幸的是，我找不到与 2023 版相比发生了哪些变化的表格。我注意到了一些变化。例如，动态损失权重从 0.5 变为 1。评论家使用真实转换来计算重放损失。优化器已经改变，他们使用自动梯度标准剪辑。我想知道是否有人注意到其他重大变化。如果有人有更改表并愿意分享就太好了！ https://preview.redd.it/r94zls5159dd1.png?width=640&amp;format=png&amp;auto=webp&amp;s=babb912867b877dd94ed98f5de6b52ddb46a1f3a    提交人    /u/yulinzxc   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</guid>
      <pubDate>Thu, 18 Jul 2024 10:14:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</link>
      <description><![CDATA[您好， 我正在学习 Richard Sutton 的书，对强化学习有了一些了解，也将其应用于一些项目。 我想写一篇研究论文：希望申请研究型硕士学位。 你们有什么建议吗？我应该考虑哪些事情？你们如何进行研究？ 谢谢！    提交人    /u/Original_Phrase1902   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 18 Jul 2024 09:55:25 GMT</pubDate>
    </item>
    <item>
      <title>设计具有多维行动空间的 PPO AC 框架时遇到的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e65l1b/an_issue_in_designing_an_ac_framework_with_a/</link>
      <description><![CDATA[我正在使用 PPO 编写自己的环境，其中涉及多维动作空间。在适配 PyTorch 框架时，我在更新策略网络的批量采样训练过程中遇到了问题。由于我正在处理多维数组，pi 比率的样本大小与优势函数的大小不同，因此无法将它们相乘以计算总损失函数。该如何解决？ 如： for _ in range(self.K_epochs): for index in BatchSampler(SubsetRandomSampler(range(self.batch_size)), self.mini_batch_size, False): action_mean_now = self.actor(s[index]) dist_now = Categorical(probs=action_mean_now) dist_entropy = dist_now.entropy().view(-1, 1) a_logprob_now = dist_now.log_prob(a[index].squeeze()).view(-1, 1) a_logprob = a_logprob[index].view(-1, 1) ratios = torch.exp(a_logprob_now - a_logprob) surr1 = ratios * adv[index] surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * adv[index] actor_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy  发生这种情况： surr1 = ratios * adv[index] RuntimeError：张量 a 的大小（128）必须与非单例维度 0 处的张量 b 的大小（64）匹配  如何确保策略可以学习多维动作空间的特征，同时避免与优势函数发生大小冲突？    提交人    /u/VermicelliBrave1931   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e65l1b/an_issue_in_designing_an_ac_framework_with_a/</guid>
      <pubDate>Thu, 18 Jul 2024 07:32:48 GMT</pubDate>
    </item>
    <item>
      <title>有人在 RL 中实现过 OGD（正交梯度下降）吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e63o8r/has_anyone_implemented_ogd_orthogonal_gradient/</link>
      <description><![CDATA[我正在通过持续强化学习研究机器人手臂操作，我花了几周时间在现实生活中实现 OGD。但似乎 OGD 在现实生活中不起作用。我认为这是因为与监督学习任务相比，现实生活中的任务太复杂了。有人成功实现过 OGD 吗？ 对于那些还没有听说过 OGD 的人，这里有一个链接：https://arxiv.org/pdf/1910.07104    提交人    /u/ContestOk7604   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e63o8r/has_anyone_implemented_ogd_orthogonal_gradient/</guid>
      <pubDate>Thu, 18 Jul 2024 05:24:56 GMT</pubDate>
    </item>
    <item>
      <title>用于混合 MDP 和腿部运动任务的 PPO/SAC 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e5ivvt/pposac_implementation_for_hybrid_mdp_and_for/</link>
      <description><![CDATA[大家好， 我对强化学习领域还很陌生，我很乐意与该领域更有经验的人讨论我在强化学习任务中遇到的问题。 我在这里长话短说，但基本上，我试图以分层方式将模型预测控制与强化学习结合起来。在我看来，MPC 更多地充当参考跟踪等内容的本地控制器，而代理则负责更复杂的推理。具体来说，我的目标是运动任务，目前针对四足机器人。 MPC 允许跟踪扭曲参考并公开一些参数以在运行时选择接触阶段。 现在，正如您所想象的，我的动作空间自然是连续-离散混合，其中代理必须输出扭曲命令和 4 个离散（现在为布尔值）变量来选择是否要步进。 我现在处理这个混合空间的方式是天真地根本不处理它。我正在使用 PPO 和 SAC 的一些自定义实现，并对连续动作空间进行微小修改；然后简单地在环境级别，我根据步进（连续）动作变量的阈值选择是否要步进。 到目前为止，我在训练这种任务时遇到了困难。我开始使用四处走动的机器人获得一些勉强不错的结果，但我觉得这种近似离散动作的方式可能会大大减慢/阻碍收敛。此外，由于某种原因，SAC 比 PPO 困难得多（我已经使用 mujoco 环境测试了我的两个自定义实现，它们都按照 SoA 执行）。 您对此事有何看法？我的方法是否存在我忽略的固有问题？您是否知道我可以测试的混合 MDP 的某些 PPO/SAC/both 实现？    提交人    /u/Majestic-Product1179   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e5ivvt/pposac_implementation_for_hybrid_mdp_and_for/</guid>
      <pubDate>Wed, 17 Jul 2024 13:59:05 GMT</pubDate>
    </item>
    <item>
      <title>完成多智能体强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e4nsn5/completed_multiagent_reinforcement_learning/</link>
      <description><![CDATA[我潜伏在这个 subreddit 一段时间了，时不时地，我会看到一些想要开始 MARL 项目的人发帖。这些人中很多人都是这个领域的新手，尽管难度非常大，但他们（可以理解）还是想在这个最令人兴奋的子领域之一工作。话虽如此，但在最初阶段之后，我并没有看到太多关于它的讨论。 在我自己的工作中，我发现了几十个库，其中一些有自己的出版物，但在 Github 上查找它们，发现使用它们的（公共）存储库相对较少，尽管它们的星级很高。入门活动和已完成项目的数量之间似乎出现了惊人的下降，甚至比其他热门领域（如生成建模）的下降幅度更大。我知道这是一个有点不合常规的问题，但是，在尝试过 MARL 的人中，你们的情况怎么样？您是否有任何想要分享的项目，无论是作为存储库还是战争故事？    提交人    /u/Efficient_Star_1336   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e4nsn5/completed_multiagent_reinforcement_learning/</guid>
      <pubDate>Tue, 16 Jul 2024 12:48:36 GMT</pubDate>
    </item>
    </channel>
</rss>