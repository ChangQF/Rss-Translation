<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 24 Nov 2024 03:32:48 GMT</lastBuildDate>
    <item>
      <title>关于可靠性分析中非 Atari 基准的规范化方法的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gygc0w/question_on_normalization_methods_for_nonatari/</link>
      <description><![CDATA[大家好， 我目前正在使用 Rliablehttps://github.com/google-research/rliable 分析 DeepMind Control Suite (DMC) 和 PyBullet 等环境中的强化学习结果。与 Atari 基准测试不同，这些环境没有人工归一化的分数来标准化算法之间的比较。例如，我正在使用 SARC 等最新算法，缺乏这样的基线使得确保公平和一致的评估变得具有挑战性。 我正在考虑使用 Z 分数归一化和百分位数归一化作为比较不同 RL 算法的潜在解决方案，但我不确定这些方法是否理想或是否符合 Rliable 倡导的统计严谨性。 有人有这方面的经验或在这种情况下的最佳实践建议吗？如果您能提供其他适用于此情况的强大规范化方法的见解或建议，我将不胜感激。 感谢您的时间和想法！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gygc0w/question_on_normalization_methods_for_nonatari/</guid>
      <pubDate>Sun, 24 Nov 2024 02:14:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的经验设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gy8tdy/empirical_design_in_reinforcement_learning/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gy8tdy/empirical_design_in_reinforcement_learning/</guid>
      <pubDate>Sat, 23 Nov 2024 20:19:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 模拟引理的最佳紧密度界限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gy4g4a/r_an_optimal_tightness_bound_for_the_simulation/</link>
      <description><![CDATA[https://arxiv.org/abs/2406.16249（也在 RLC 上提出） 模拟引理是强化学习中广泛使用的基础结果，它限制了相对于模型错误指定的值估计误差。但正如许多人所注意到的，它提供的界限非常宽松，尤其是对于较大的错误指定或高折扣（见图 2）。直到现在！ 关键思想是，每次你对最终结果的判断错误时，你未来出错的概率就会降低。传统的模拟引理证明没有考虑到这一点，因此假设你可以永远在每个时间步中错误指定相同的 epsilon 概率质量（这就是为什么它对于长视野或较大的错误指定很宽松的原因）。利用这一观察，我们可以得到一个最佳的紧密界限。 我们的界限取决于与原始模拟引理相同的数量，因此应该能够插入当前使用原始引理的任何地方。希望大家喜欢！    提交人    /u/asdfwaevc   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gy4g4a/r_an_optimal_tightness_bound_for_the_simulation/</guid>
      <pubDate>Sat, 23 Nov 2024 17:09:35 GMT</pubDate>
    </item>
    <item>
      <title>我应该关注什么基于四轴飞行器的 RL 模拟环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gy3544/what_simulation_environment_should_i_be_looking/</link>
      <description><![CDATA[我将列出我考虑过的那些以及它们的局限性（据我所知）  Flightmare：似乎是总体上最好的选择，灵活的渲染和物理特性可以真正使用所有选项。但不幸的是，它似乎不再受支持，并且他们的 repo 充满了未解决的问题。 Isaac Sim/Pegasus：运行成本极高，因为它建立在 nvidia omniverse 之上。 Gazebo：缓慢且渲染设置有限 AirSim：不再受支持。 Mujoco：渲染极其有限，没有对传感器的原生支持，但速度非常快。  请让我知道您的想法，以及这个问题是否不适合该子版块。我也非常希望得到关于如何将 rl 算法集成到无人机的 ROS 包中的任何提示，因为我对机器人技术和模拟完全陌生。    提交人    /u/LowStatistician11   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gy3544/what_simulation_environment_should_i_be_looking/</guid>
      <pubDate>Sat, 23 Nov 2024 16:12:55 GMT</pubDate>
    </item>
    <item>
      <title>证明 v∗(s) = max(a∈A(s)) qπ∗(s,a)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxxwkw/proof_of_vs_maxaas_qπsa/</link>
      <description><![CDATA[      大家好，我正在研究 Sutton &amp; Barto 的书。在推导最佳状态值函数的贝尔曼方程时，作者从那里开始：  https://preview.redd.it/nm2284t13n2e1.png?width=369&amp;format=png&amp;auto=webp&amp;s=f73ef50c74d48b1d7032f068565acf7b2ffcc562 我以前没见过这样的东西。我们如何证明这个等式？     由    /u/demirbey05 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxxwkw/proof_of_vs_maxaas_qπsa/</guid>
      <pubDate>Sat, 23 Nov 2024 11:48:21 GMT</pubDate>
    </item>
    <item>
      <title>帮我创建一个关于如何选择强化学习算法的决策树</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxwwfd/help_me_create_a_decision_tree_about_how_to/</link>
      <description><![CDATA[      嘿！我是一名大学教授，我想在未来几年创建一个强化学习专业化课程。 我设法理解了各种经典算法，但我真的不知道在什么时候使用哪一种。我正在尝试在 chatgpt 的帮助下创建一个决策树。我可以得到您的一些评论和更正吗？    提交人    /u/Dougdaddyboy_off   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxwwfd/help_me_create_a_decision_tree_about_how_to/</guid>
      <pubDate>Sat, 23 Nov 2024 10:41:10 GMT</pubDate>
    </item>
    <item>
      <title>最近有任何关于基础 RL 改进的研究吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxudhp/any_research_regarding_the_fundamental_rl/</link>
      <description><![CDATA[我一直在 Google Scholar 上关注几位最负盛名的 RL 研究人员，我注意到他们中的许多人近年来已将重点转移到与 LLM 相关的研究上。 哪篇论文最引人注目，推动了 RL 的根本性改进？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxudhp/any_research_regarding_the_fundamental_rl/</guid>
      <pubDate>Sat, 23 Nov 2024 07:40:34 GMT</pubDate>
    </item>
    <item>
      <title>“对未知情况和环境的元认知 (MUSE)”，Valiente & Pilly 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxno5j/metacognition_for_unknown_situations_and/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxno5j/metacognition_for_unknown_situations_and/</guid>
      <pubDate>Sat, 23 Nov 2024 01:07:37 GMT</pubDate>
    </item>
    <item>
      <title>“Marco-o1：面向开放式解决方案的开放推理模型”，赵等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxngh4/marcoo1_towards_open_reasoning_models_for/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxngh4/marcoo1_towards_open_reasoning_models_for/</guid>
      <pubDate>Sat, 23 Nov 2024 00:57:31 GMT</pubDate>
    </item>
    <item>
      <title>关于 Wordle 性能不佳的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxdv1y/advice_regarding_poor_performance_on_wordle/</link>
      <description><![CDATA[大家好， 我正在寻找有关如何处理这个强化学习问题的建议。我正在尝试教一个编码器转换器模型玩 wordle。它是基于字符的，所以有 26 个标记 + 5 个特殊标记。输入是棋盘空间，因此它可以访问以前的猜测和反馈，以及显示猜测开始/结束位置等的特殊标记。 我目前使用的算法是 PPO，我已经将游戏简化为只需要猜测一个单词的极其简单的场景，我预计这会非常容易（但是由于我的 RL 知识有限，显然我搞砸了）。 我正在寻找有关在哪里寻找此问题根源的建议。该模型确实“最终”赢了一两次，但它似乎并没有停留在那里。此外，它似乎只能持续猜测两个或三个字母。 示例。目标单词是 Amble 该模型可以一致地猜测“aabak”，围绕 an 和 b 的逻辑结果是合理的，因为奖励结构会支持该猜测。我不知道为什么 k 会得到强化，或者为什么其他字母不那么普遍。 此外，我尝试了教师强制，即强制模型做出正确的猜测并获胜，但无济于事。有什么建议吗？ 编辑：此外，该游戏是“可赢的”，我创建了伪游戏并在这些游戏上训练了模型。不是真正的离线 RL，因为我使用了 CE 损失。但是，在模型已经训练过的单词上，它的表现足够好，即使是它没有见过的单词，它的表现也不错，足以展示对模式的“理解”。    提交人    /u/ur_a_glizzy_gobbler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxdv1y/advice_regarding_poor_performance_on_wordle/</guid>
      <pubDate>Fri, 22 Nov 2024 17:54:05 GMT</pubDate>
    </item>
    <item>
      <title>关于简单单过程囊实验的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxb5rw/question_regarding_simple_single_process_sac/</link>
      <description><![CDATA[即使我设置了正确的超参数并按照论文中的说法制定公式，仍然不足以相信代理会实现目标吗？ 奖励缩放是否必要？例如，对于半cheeath？    提交人    /u/Round_Apple2573   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxb5rw/question_regarding_simple_single_process_sac/</guid>
      <pubDate>Fri, 22 Nov 2024 16:01:30 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML-Agents 代理变得越来越笨，我的想法已经用完了。我需要帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</guid>
      <pubDate>Fri, 22 Nov 2024 10:42:24 GMT</pubDate>
    </item>
    <item>
      <title>灾害管理中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</link>
      <description><![CDATA[最近，我深入研究了 RL 的灾害管理，并阅读了几篇相关论文。许多论文都提到了与之相关的算法，但没有以某种方式对其进行模拟。是否有任何平台具有与 RL 相关的模拟来展示其应用？此外，如果您有关于此方面的任何其他优秀论文的信息，请提及。     提交人    /u/SuitSecret6497   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</guid>
      <pubDate>Fri, 22 Nov 2024 07:40:51 GMT</pubDate>
    </item>
    <item>
      <title>适用于 ML/AI/RL 的 Blue Sky 研究员入门包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</link>
      <description><![CDATA[大家好，许多研究人员正在加入 Blue Sky，而且似乎进展顺利，所以我想我会留下一些研究人员“入门包”供大家关注。欢迎随意发布您自己的内容 :)  RL：https://bsky.app/3WPHcHg AI：https://bsky.app/SipA7it NLP：https://bsky.app/SngwGeS ML 理论：https://bsky.app/21nFz12 AI 机器人：https://bsky.app/DfAoaJ1 贝叶斯 ML：https://bsky.app/2Bqtn6T 人工智能中的女性：https://bsky.app/LaGDpqg 人工智能和新闻：https://bsky.app/5sFqVNS 科学人工智能：https://bsky.app/JeFdryY  医学人工智能：https://bsky.app/N5UUARF     提交人    /u/secondterm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</guid>
      <pubDate>Thu, 21 Nov 2024 21:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何开始机器人操作器的强化学习研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</link>
      <description><![CDATA[你好， 我是一名研究生，对应用人工智能技术（特别是强化学习）来控制机器人操纵器（机械臂）感兴趣。 为了做到这一点，我不知道从哪里开始学习并决定研究主题。  有哪些基础论文和资源可以帮助我了解这个领域？ 有哪些最近的评论或调查论文可以帮助我了解该领域的现状？ 或者，为了研究人工智能机器人技术，我应该阅读哪些论文？   如有任何建议或意见，我们将不胜感激！ 谢谢！ 使用 DeepL.com （免费版） 进行翻译   提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 21 Nov 2024 18:59:33 GMT</pubDate>
    </item>
    </channel>
</rss>