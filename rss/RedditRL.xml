<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 08 Jan 2025 03:21:37 GMT</lastBuildDate>
    <item>
      <title>赛车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hw8tyf/auto_racing/</link>
      <description><![CDATA[我目前正在开展一个模仿强化学习项目，使用 DDPG 训练自动驾驶赛车代理。我使用 CarSim 进行车辆动力学模拟，因为我需要高保真物理和灵活的驾驶条件。我已经弄清楚了如何运行 CarSim 模拟并获得实时结果。 但是，我遇到了一些问题 - 当我尝试在 CarSim 中训练 DDPG 代理在我的自定义赛道上行驶时，它几乎立即失败并且似乎没有学到任何有意义的东西。我最初的猜测是任务太复杂，动作空间太大，代理无法找到好的学习方向。 为了解决这个问题，我收集了 5 组我自己的赛车数据（转向角、油门、刹车）并训练了一个神经网络来模仿我的驾驶行为。然后我尝试使用这个网络作为 DDPG 中的初始参与者模型进行进一步训练。但是，结果仍然是一样的——快速失败。 我想知道我的方法是否有缺陷。有没有人做过类似的项目，或者有更好的方法建议？非常感谢任何意见！    提交人    /u/Fun_Package_1786   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hw8tyf/auto_racing/</guid>
      <pubDate>Wed, 08 Jan 2025 02:12:30 GMT</pubDate>
    </item>
    <item>
      <title>GNN+DEEPRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvtu9k/gnndeeprl/</link>
      <description><![CDATA[大家好，我在使用端到端架构时遇到了一些麻烦：GNN（获取嵌入）然后是 Actor Critic 架构。 与使用原始特征相比，使用 gnn 嵌入的性能确实很差。我认为这是因为我得到的初始嵌入很差。 有什么想法可以改进吗？谢谢。    提交人    /u/TeamTop4542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvtu9k/gnndeeprl/</guid>
      <pubDate>Tue, 07 Jan 2025 15:25:56 GMT</pubDate>
    </item>
    <item>
      <title>我的 DQN 存在一些问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvqcdv/i_have_some_problems_with_my_dqn/</link>
      <description><![CDATA[      我尝试在类似国际象棋的环境中创建 DQN 代理（带有 lambda 目标），奖励总和为零。 我的params: optimizer=Adam lr=0.00005 loss=SmoothL1Loss rewards = [-1,0,+1] (loose, draw/max_game_length, win therefore) 我还使用衰减 epsilon 从 0.6 到 0.01 这是灾难性遗忘（或其他问题）的问题吗？如果是，我该如何解决？ reward_fn 或 decay_lr 能帮上忙吗？ 最近用这个参数测试： https://preview.redd.it/xcz7gkvkekbe1.png?width=1297&amp;format=png&amp;auto=webp&amp;s=95ed4c2dda38896507598b1dee4e785b34f5cdc7 平滑： https://preview.redd.it/zg7m9wurekbe1.png?width=1285&amp;format=png&amp;auto=webp&amp;s=de2def6873ac19c1017122008b2ce4e5a7ba3c1b    提交人    /u/No-Eggplant154   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvqcdv/i_have_some_problems_with_my_dqn/</guid>
      <pubDate>Tue, 07 Jan 2025 12:28:33 GMT</pubDate>
    </item>
    <item>
      <title>寻求衡量供应链管理强化学习模型效率和性能的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvolro/seeking_metrics_to_evaluate_efficiency_and/</link>
      <description><![CDATA[大家好， 我正在开发一种强化学习 (RL) 模型来帮助一家公司的自行车供应链。 RL 代理旨在通过制定战略决策来最大限度地减少生产延迟和管理相关风险，包括：  行动：  不采取任何行动：让生产自行进行，无需干预。 加快：加速组件的交付，以一定成本缩短其交付周期（例如 2 天）。 延迟生产：推迟特定自行车型号的生产以适应组件短缺或降低风险。  状态空间包括：  风险评分：基于特定于组件的风险的每个生产订单的汇总评分。 工厂产能（未来日期）：有关未来时期生产能力的信息。 采购订单：关键产品的预计到达日期组件。  奖励函数：  平衡过度延迟的惩罚与加快行动的成本，鼓励高效利用资源和及时生产。   我正在考虑使用 PPO 算法来训练代理，并且正在寻找有效的指标来衡量该 RL 模型的效率和整体性能。具体来说，我想评估代理在供应链模拟中管理延迟和降低风险的能力。 问题：  在这种情况下，您会推荐哪些指标来评估 RL 代理的效率？ 如何有效衡量代理在最小化延迟和管理风险方面的决策总体表现和成功率？ 在供应链 RL 应用中，是否有我应该考虑的最佳实践或标准评估方法？  如能提供任何建议、见解或相关文献参考，我们将不胜感激！ 提前感谢您的帮助！    提交人    /u/Euphoric-You-8437   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvolro/seeking_metrics_to_evaluate_efficiency_and/</guid>
      <pubDate>Tue, 07 Jan 2025 10:32:55 GMT</pubDate>
    </item>
    <item>
      <title>多人回合制 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvge53/multiplayer_turn_based_rl/</link>
      <description><![CDATA[我正在开发一个可以玩 Hansa Teutonica（3-5 人游戏）的 AI。 游戏逻辑很复杂，而且已经接近完成，但我无法理解为游戏结束时分配奖励。 在游戏中，游戏有 3 种结束方式，并且只能在一个人的回合中结束。 从理论上讲，游戏中的某些动作可能会导致僵局 - 类似于国际象棋中黑白棋的骑士来回移动（忽略 3 次重复）。 我现在的写法是，如果代理执行了好的操作，则分配一个次要的+奖励。而对于中立操作（或强制操作）则分配接近 0 的奖励。确定不良行为是未来的目标。 我真正感到困惑的是分配游戏结束时的奖励。 如果活跃玩家采取行动结束游戏，并获得第一名，那么直接奖励相当多的奖励就足够了。但是如果是 5 名中的第 2 名/第 3 名呢？ 我将如何奖励其他代理？代理的最后一个动作并没有直接导致他们的最终排名。 第三名玩家可以结束游戏，而第四名玩家可能很长时间没有采取行动。 我正在使用 PyTorch，并在执行操作后分配奖励。 如果不是活跃玩家的回合，为他们的最后一个动作分配奖励似乎不正确。 游戏中的另一个小问题是，当游戏接近尾声并且轮到你时，你可以 A) 结束游戏，获得第二名，或者 B) 放弃这一回合，也许让你的对手接管你的一些积分，将你推向更糟糕的排名。 我希望这足够有意义，因为我肯定很挣扎并且需要一些指导。    提交人    /u/Gozuk99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvge53/multiplayer_turn_based_rl/</guid>
      <pubDate>Tue, 07 Jan 2025 01:54:32 GMT</pubDate>
    </item>
    <item>
      <title>塞尔达传说 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1huzxec/the_legend_of_zelda_rl/</link>
      <description><![CDATA[我目前正在训练一个代理来“通关”《塞尔达传说：林克的觉醒》，但我面临一个问题：我无法想出一个可以让林克通过初始房间的奖励系统。 目前，我使用的唯一正向奖励是林克获得新物品时的 +1。我正在考虑对停留在同一地方太久的情况实施负向奖励（以阻止代理在同一个房间内转圈）。 你们觉得怎么样？关于如何改进奖励系统并解决这个问题有什么想法或建议吗？    提交人    /u/SlipFrosty2342   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1huzxec/the_legend_of_zelda_rl/</guid>
      <pubDate>Mon, 06 Jan 2025 14:13:15 GMT</pubDate>
    </item>
    <item>
      <title>Github 仓库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hue4bh/github_repo/</link>
      <description><![CDATA[抱歉，我对主题有疑问，但是有没有办法让 chatGPT 通过 github repo。     提交人    /u/Wide-Chef-7011   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hue4bh/github_repo/</guid>
      <pubDate>Sun, 05 Jan 2025 18:52:36 GMT</pubDate>
    </item>
    <item>
      <title>评论？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hue0ox/comments/</link>
      <description><![CDATA[        提交人    /u/ValueSeekerAgent   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hue0ox/comments/</guid>
      <pubDate>Sun, 05 Jan 2025 18:48:22 GMT</pubDate>
    </item>
    <item>
      <title>具有奖励（和价值）分布的分布式强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hudvfl/distributional_rl_with_reward_and_value/</link>
      <description><![CDATA[大多数分布式 RL 方法在训练值/q 值网络分布时使用标量即时奖励（尤其是：C51 和 QR 系列网络）。在这种情况下，奖励只是转移目标分布。 我很好奇是否有人遇到过任何学习即时奖励分布的工作（即随机奖励）。    提交人    /u/Losthero_12   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hudvfl/distributional_rl_with_reward_and_value/</guid>
      <pubDate>Sun, 05 Jan 2025 18:42:31 GMT</pubDate>
    </item>
    <item>
      <title>教 PPO“绘画”有困难</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hu4s6t/trouble_teaching_ppo_to_draw/</link>
      <description><![CDATA[我正尝试教神经网络在 colab 中“绘制”。这个想法是，给定一个输入画布和一个参考图像，网络需要输出两个 x 和 y 坐标以及一个 rgba 值，并在输入画布顶部绘制一个具有 rgba 颜色的矩形。画布上带有矩形，然后是新状态。然后重复该过程。 我正在使用 PPO 训练这个网络。据我所知，这是一种用于连续动作的良好 DRL 算法。 奖励是放置矩形之前和之后与参考图像相比的 mse 差异。此外，对于完全相同或非常接近的坐标，还会受到惩罚。通常，初始网络会吐出非常接近的坐标，导致绘制矩形时没有奖励。 一开始，损失似乎在下降，但过了一段时间就停滞了，我想找出我做错了什么。 我最后一次使用强化学习是在 2019 年，现在我有点生疏了。我已经订购了 Grokking DRL 书，10 天后到货。同时，我有几个问题： - PPO 是这个问题的正确算法选择吗？ - 我的 PPO 实现看起来正确吗？ - 您是否发现我的奖励函数有任何问题？ - 网络是否足够大以学习这个问题？（小得多的 CPPN 能够做得不错，但它们是符号网络） - 您认为我的网络也可以从参考图像作为输入中受益吗？即第二个 CNN 输入流，用于参考图像，我将其输出展平并将其连接到线性层的另一个输入流。    提交人    /u/matigekunst   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hu4s6t/trouble_teaching_ppo_to_draw/</guid>
      <pubDate>Sun, 05 Jan 2025 11:10:46 GMT</pubDate>
    </item>
    <item>
      <title>强化学习Flappy Bird代理失败！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htzwke/reinforcement_learning_flappy_bird_agent_failing/</link>
      <description><![CDATA[我尝试使用 DQN 为 Flappy Bird 创建强化学习代理，但代理根本没有学习。它一直与管道和地面发生碰撞，我不知道哪里出了问题。我不确定问题出在奖励系统、神经网络还是我实现的游戏机制上。有人能帮我吗？我会分享我的 GitHub 存储库链接以供参考。 GitHub 链接    提交人    /u/uddith   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htzwke/reinforcement_learning_flappy_bird_agent_failing/</guid>
      <pubDate>Sun, 05 Jan 2025 06:50:18 GMT</pubDate>
    </item>
    <item>
      <title>训练期间目标 Q 值告诉我什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htzpk6/what_does_the_target_qvalue_tell_me_during/</link>
      <description><![CDATA[大家好， 我正在训练一个 TD3 代理，想知道目标 Q 值能告诉我关于训练的什么信息。 我知道最基本的知识，即如果我们遵循某些最佳策略，它是预期的折扣奖励。那么如果它开始收敛到某个值，然后稍微减少，然后一遍又一遍地增加（有点像在 2 个点之间反弹），它是否学到了一些次优策略？还是训练还没有完成？对于奖励稀疏的环境来说，这尤其令人困惑，那么它是否可以成为一个有用的指标，表明在训练的哪个点它会达到最优策略？我之所以问这个问题，是因为在连续 5 个左右的场景中，环境会得到解决，然后出现不利的表现。这让我想到了以下问题： 如果动作中总是添加噪音，那么目标 Q 值是否有助于告诉我噪音是否妨碍了训练？至于具体细节，我确实将噪声衰减到了 0.1，这意味着添加的随机噪声是从标准差为 0.1 的正态分布中采样的。我觉得这可能会影响一些目标 Q 值？ 我觉得这是一个开放式的问题，所以我很乐意详细说明任何事情。 非常感谢！    提交人    /u/Sea_Farmer5942   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htzpk6/what_does_the_target_qvalue_tell_me_during/</guid>
      <pubDate>Sun, 05 Jan 2025 06:37:15 GMT</pubDate>
    </item>
    <item>
      <title>“无流程标签的免费流程奖励”，Yuan 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1httwti/free_process_rewards_without_process_labels_yuan/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1httwti/free_process_rewards_without_process_labels_yuan/</guid>
      <pubDate>Sun, 05 Jan 2025 01:17:53 GMT</pubDate>
    </item>
    <item>
      <title>“Aviary：训练语言代理完成具有挑战性的科学任务”，Narayanan 等人 2024 {Futurehouse}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htsw86/aviary_training_language_agents_on_challenging/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htsw86/aviary_training_language_agents_on_challenging/</guid>
      <pubDate>Sun, 05 Jan 2025 00:28:52 GMT</pubDate>
    </item>
    <item>
      <title>梦想家架构中不断变化的行动空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htoxs7/changing_action_spaces_in_dreamer_architecture/</link>
      <description><![CDATA[您好 r/reinforcementlearning， 因此，我正在设计一个模型来完成某种类型的复杂工作。 本质上，我设计环境的方式涉及在不同的动作空间上工作。 我认为，为了创建不同的动作空间，我只需更改 Agent 的动作空间即可；但是我检查了代码，似乎 。空间的数量非常有限（大约 30 个不同的动作空间），但它们是不同的 - 有时它只是一个从 1 到 3 的 uint，有时它是（3 个 float32 选择、一个 bool 选择、另一个但不同的 3 个 float32 选择）；或者有时它是一个包含 127 个布尔值的向量，模型应该选择真/假。 这绝对比使用单个 action 参数更复杂。 有人处理过这个问题吗？怎么做？ 干杯。 &gt; 我担心的一件事是不同的 dtypes。从技术上讲，我可以有 3 个输出，分别为布尔值、整数和浮点数，并惩罚不必要的操作，但是……我已经将所有环境编码为静态操作，此外，我很确定在这个环境中较少的循环是好的 - 我已经完成了数千个离散步骤才能实现它。    提交人    /u/JustZed32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htoxs7/changing_action_spaces_in_dreamer_architecture/</guid>
      <pubDate>Sat, 04 Jan 2025 21:29:13 GMT</pubDate>
    </item>
    </channel>
</rss>