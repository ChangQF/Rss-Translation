<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 16 Sep 2024 01:16:21 GMT</lastBuildDate>
    <item>
      <title>惩罚是否可以在任一方向（+ 和 -）改变权重？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhs3xa/is_it_feasible_for_punishments_to_alter_weights/</link>
      <description><![CDATA[我很难想出一个部署强化学习算法的策略，如果能得到任何见解，我将不胜感激。我正在构建一个模型，无论模型采取什么操作，该模型的状态都会有 96% 的时间失败。这会导致对权重的惩罚总是过早地降到最低点（这意味着它会降到 0，除非我将奖励提高到天文数字，否则永远不会回升）。我知道这是一个奇怪的问题，但如果惩罚不是严格地减去权重，这是否有意义？我的想法是，奖励会保持相同的权重，惩罚可以是加法或减法，如果这有意义的话。我只是担心它只会朝一个方向（向下）发展，而实际上，如果这有意义的话，我希望权重在两个方向（向上和向下）波动得更自由一些。    提交人    /u/Correct_Truth9920   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhs3xa/is_it_feasible_for_punishments_to_alter_weights/</guid>
      <pubDate>Mon, 16 Sep 2024 00:53:03 GMT</pubDate>
    </item>
    <item>
      <title>Dagger 手册专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhpy4q/manual_expert_for_dagger/</link>
      <description><![CDATA[大家好， 我正在研究一个结合运动规划的模仿学习问题。我有一个专家，他给出了 EEf 姿势，我用它来收集数据。行为克隆工作得还不错，符合预期。 我想继续使用 Dagger，但我将不得不花费大量时间来设置专家，以通过 Dagger 处理在线查询，而且每次迭代可能都很慢。 鉴于我的系统不是高频的，并且每集有 10 个转换，每个查询的手动输入是否可行？    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhpy4q/manual_expert_for_dagger/</guid>
      <pubDate>Sun, 15 Sep 2024 23:07:56 GMT</pubDate>
    </item>
    <item>
      <title>“扩散强制：下一个标记预测与全序列扩散相结合”，Chen 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</guid>
      <pubDate>Sun, 15 Sep 2024 16:44:39 GMT</pubDate>
    </item>
    <item>
      <title>批判情景环境中的重要性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fh8mlq/critic_importance_in_episodic_environments/</link>
      <description><![CDATA[大家好，在这篇文章中：https://ai.stackexchange.com/questions/25739/what-are-the-advantages-of-rl-with-actor-critic-methods-over-actor-only-methods 有以下段落：  一个实际的好处是，批评者可以使用 TD 学习进行引导，使他们能够在线学习每一步……像 REINFORCE 这样的纯演员算法……需要情节问题。它们可以学习的最小单位是整个情节。这是因为，如果没有评论家提供价值估计，那么估计回报的唯一方法就是从一集的结尾抽样实际回报。  我想进一步了解这一点。如果任何状态已经显示出一些奖励，为什么我需要评论家价值解释？ 我认为另一种提问方式是 - 假设对于每个状态，我预测每个奖励的概率，我可以使用这个分布的平均值作为该状态的评论家值吗？    提交人    /u/Potential_Hippo1724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fh8mlq/critic_importance_in_episodic_environments/</guid>
      <pubDate>Sun, 15 Sep 2024 09:27:00 GMT</pubDate>
    </item>
    <item>
      <title>需求变化对 RL 训练稳定性的影响</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fgt1ld/impact_of_varying_demand_on_rl_training_stability/</link>
      <description><![CDATA[      我正在使用不同的需求文件（代表汽车到达流程/时间表）来训练我的 RL 算法。每个文件包含不同数量的车辆，范围在 800 到 1200 之间。在我的问题中，如果车辆未与客户匹配，则它们会在一定时间后离开系统。累积奖励基于已服务的车辆和未服务的车辆。因此，如果我们在需求文件中拥有更多车辆，那么如果我们对未服务的车辆保持谨慎，我们就有可能积累更多奖励。 实际上，我有一个更复杂的问题，但我试图尽可能地简化它。 在训练过程中，我注意到累积奖励在各个情节中存在显着波动（我在每集结束时记录累积奖励）。 我的问题是：需求文件中车辆数量的不同是否会导致学习过程不稳定？如果是的话，我应该如何处理才能稳定训练并提高学习效果？ https://preview.redd.it/0iv7qojvmtod1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ec5d15963c9eb2d4c8f306c5ff9a06babf7eb11d    submitted by    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fgt1ld/impact_of_varying_demand_on_rl_training_stability/</guid>
      <pubDate>Sat, 14 Sep 2024 18:52:09 GMT</pubDate>
    </item>
    <item>
      <title>当思路链链接了太多的想法时。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fgc0wm/when_the_chainofthought_chains_too_many_thoughts/</link>
      <description><![CDATA[        提交人    /u/moschles   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fgc0wm/when_the_chainofthought_chains_too_many_thoughts/</guid>
      <pubDate>Sat, 14 Sep 2024 02:25:04 GMT</pubDate>
    </item>
    <item>
      <title>努力设置 unity mlagents</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg8ywi/struggling_to_setup_unity_mlagents/</link>
      <description><![CDATA[我几乎得到了我需要的所有东西，尽管我遇到了一个无论我怎么尝试都无法解决的问题。mlagents-envs 需要 numpy==1.21.2，但我认为 pytorch 所需的 scipy 有要求：2.3。&gt;=1.22.4。我到底该如何解决这个问题？我不想使用 tensorflow 代替 pytorch，当我尝试使用 tensorflow 时我遇到了更糟糕的问题，并且只会在万不得已的情况下才会这样做    提交人    /u/JMB4200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg8ywi/struggling_to_setup_unity_mlagents/</guid>
      <pubDate>Fri, 13 Sep 2024 23:44:58 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    <item>
      <title>关于 o1 的每篇最新帖子</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffxvj2/every_recent_post_about_o1/</link>
      <description><![CDATA[        提交人    /u/quiteconfused1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffxvj2/every_recent_post_about_o1/</guid>
      <pubDate>Fri, 13 Sep 2024 15:41:56 GMT</pubDate>
    </item>
    <item>
      <title>目前 rl 的实际应用有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffsx8f/what_are_the_actual_applications_of_rl_being_used/</link>
      <description><![CDATA[我知道 RL 在很多机器人和游戏开发领域理论上都有应用，甚至在自动驾驶和 sim2real 机器人领域也有实际应用。但这就是我的知识范围。 我见过很多 RL 被用于解决大型系统中的小问题的案例，比如在数据分析中，所以我想了解这个领域在现实生活中的实际用途。 我认为大多数 RL 将用于整体行为训练，有点保留了 RL 的精神，但事实并非如此吗？    提交人    /u/Kae1506   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffsx8f/what_are_the_actual_applications_of_rl_being_used/</guid>
      <pubDate>Fri, 13 Sep 2024 11:59:08 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 推出先思考后回答的 o1 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffpcfg/openai_introduces_o1_model_that_thinks_before/</link>
      <description><![CDATA[       由    /u/webbs3  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffpcfg/openai_introduces_o1_model_that_thinks_before/</guid>
      <pubDate>Fri, 13 Sep 2024 07:58:39 GMT</pubDate>
    </item>
    <item>
      <title>关于 CPI、TRPO 和 PPO 的一些问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffoutv/few_questions_surrounding_cpi_trpo_and_ppo/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffoutv/few_questions_surrounding_cpi_trpo_and_ppo/</guid>
      <pubDate>Fri, 13 Sep 2024 07:21:11 GMT</pubDate>
    </item>
    <item>
      <title>帮助我开始 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffnxxw/help_me_get_started_with_rl/</link>
      <description><![CDATA[大家好，我一直在学习 RL，但还没有实现。帮我开始写代码。我想从 MDP 开始。请分享一些笔记和教程，帮助我学习编写 RL 代码    提交人    /u/Sea-Application4821   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffnxxw/help_me_get_started_with_rl/</guid>
      <pubDate>Fri, 13 Sep 2024 06:15:17 GMT</pubDate>
    </item>
    <item>
      <title>RLC 录音</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffatc0/rlc_recordings/</link>
      <description><![CDATA[由于我无法参加，因此我想观看 RLC &#39;24 演讲的录音。在 RLC 的常见问题解答中，它声明了以下内容：“所有演讲都将被录制并在获得作者许可后公开。您无需注册即可查看演讲录音。” [https://rl-conference.cc/help.html ] 有人知道这些录音在哪里可以找到吗？我搜索过但没有找到任何东西。 此外，我发现与 RL 相关的会议录音通常很难找到，有人知道一些可以观看它们的地方吗？ 非常感谢花时间回复的任何人！ 编辑：我从 RL discord 上的某人那里听说他们仍在制作视频。上传后我会在这里发布链接！    提交人    /u/two_armed_bandit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffatc0/rlc_recordings/</guid>
      <pubDate>Thu, 12 Sep 2024 19:15:28 GMT</pubDate>
    </item>
    <item>
      <title>大家好，我是 RL 的一个应用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff9j09/an_application_of_rl_everyone/</link>
      <description><![CDATA[        提交人    /u/nimageran   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff9j09/an_application_of_rl_everyone/</guid>
      <pubDate>Thu, 12 Sep 2024 18:22:15 GMT</pubDate>
    </item>
    </channel>
</rss>