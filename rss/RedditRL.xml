<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 24 Jun 2024 21:13:57 GMT</lastBuildDate>
    <item>
      <title>åœ¨ ROS+Gazebo ä¸­å¼€å§‹ä½¿ç”¨ DAgger</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ ç”±äºæ™®é€šçš„å¼ºåŒ–å­¦ä¹ æœªèƒ½æ»¡è¶³æˆ‘çš„æœŸæœ›ï¼Œå› æ­¤æˆ‘å¸Œæœ›è½¬å‘æ¨¡ä»¿å­¦ä¹ ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäºæ“çºµå™¨çš„é¡¹ç›®ï¼Œå¸¦æœ‰çœ¼æ‰‹æ·±åº¦æ‘„åƒå¤´ã€‚æˆ‘æœ‰ä¸€ä¸ªå¯ä»¥æˆä¸ºä¸“å®¶çš„ç»å…¸ç®—æ³•ã€‚æ‰€æœ‰è¿™äº›éƒ½æ˜¯åœ¨ ROS+gazebo ä¸­å®Œæˆçš„ã€‚ æˆ‘è¯¥å¦‚ä½•ä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ä»»ä½•ç°æˆçš„åº“å—ï¼Ÿæˆ‘å¦‚ä½•åœ¨ Gazebo ä¸­ä¿å­˜æ¥è‡ªæˆ‘çš„ä¸“å®¶çš„æ¼”ç¤ºã€‚å¦‚æœæœ‰äººä»¥å‰è¿™æ ·åšè¿‡    æäº¤äºº    /u/Natural-Ad-6073   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</guid>
      <pubDate>Mon, 24 Jun 2024 21:03:55 GMT</pubDate>
    </item>
    <item>
      <title>è¿™éš¾é“ä¸æ˜¯ã€ŠIMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPOã€‹è¿™ç¯‡è®ºæ–‡ä¸­çš„é—®é¢˜å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨é˜…è¯»è¿™ç¯‡è®ºæ–‡ï¼šâ€œæ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸­çš„å®æ–½é—®é¢˜ï¼šPPO å’Œ TRPO æ¡ˆä¾‹ç ”ç©¶â€ [pdf é“¾æ¥]ã€‚ æˆ‘è®¤ä¸ºæˆ‘å¯¹è¿™ç¯‡è®ºæ–‡çš„ä¿¡æ¯æœ‰ç–‘é—®ã€‚çœ‹çœ‹è¿™ä¸ªè¡¨æ ¼ï¼š https://preview.redd.it/uaw20jf6fk8d1.png?width=1056&amp;format=png&amp;auto=webp&amp;s=e3c698529ec45dc4ad71b807f587572db2988dba æ ¹æ®è¿™ä¸ªè¡¨æ ¼ï¼Œä½œè€…è®¤ä¸º TRPO+ å³ TRPO åŠ ä¸Š PPO çš„ä»£ç çº§ä¼˜åŒ–ä¼˜äº PPOã€‚å› æ­¤ï¼Œè¿™è¡¨æ˜ä»£ç çº§ä¼˜åŒ–æ¯”ç®—æ³•æ›´é‡è¦ã€‚æˆ‘çš„é—®é¢˜æ˜¯ï¼Œä»–ä»¬è¯´ä»–ä»¬å¯¹ TRPO+ ä¸­æ‰“å¼€å’Œå…³é—­ä»£ç çº§ä¼˜åŒ–çš„æ‰€æœ‰å¯èƒ½ç»„åˆè¿›è¡Œç½‘æ ¼æœç´¢ï¼Œè€Œå¯¹äº PPOï¼Œåˆ™æ˜¯å°†æ‰€æœ‰ä¼˜åŒ–éƒ½æ‰“å¼€ã€‚  æˆ‘çš„é—®é¢˜æ˜¯ï¼Œé€šè¿‡è¿›è¡Œç½‘æ ¼æœç´¢ï¼Œä»–ä»¬ç»™äº† TRPO+ æ›´å¤šçš„æœºä¼šæ¥è·å¾—ä¸€æ¬¡è‰¯å¥½çš„è¿è¡Œã€‚æˆ‘çŸ¥é“ä»–ä»¬ä½¿ç”¨ç§å­ï¼Œä½†æœ‰ 10 ä¸ªç§å­ã€‚æ ¹æ® Henderson çš„è¯´æ³•ï¼Œè¿™è¿˜ä¸å¤Ÿï¼Œå› ä¸ºå³ä½¿æˆ‘ä»¬åš 10 ä¸ªéšæœºç§å­ï¼Œå°†å®ƒä»¬åˆ†ç»„ä¸ºä¸¤ä¸ª 5 ä¸ªç§å­å¹¶ç»˜åˆ¶å¥–åŠ±å’Œæ ‡å‡†å·®ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå¾—åˆ°å®Œå…¨åˆ†ç¦»çš„å›¾ï¼Œè¿™è¡¨æ˜æ–¹å·®å¤ªé«˜ï¼Œæ— æ³•è¢« 5 ä¸ªç§å­æˆ–æˆ‘çŒœç”šè‡³ 10 ä¸ªç§å­æ•è·ã€‚  å› æ­¤ï¼Œæˆ‘ä¸çŸ¥é“ä»–ä»¬çš„è®ºç‚¹åœ¨ä»–ä»¬æ­£åœ¨è¿›è¡Œçš„ç½‘æ ¼æœç´¢ä¸‹å¦‚ä½•æˆç«‹ã€‚è‡³å°‘ï¼Œä»–ä»¬ä¹Ÿåº”è¯¥å¯¹ PPO è¿›è¡Œç½‘æ ¼æœç´¢ã€‚  æˆ‘é—æ¼äº†ä»€ä¹ˆï¼Ÿ   ç”±    /u/miladink  æäº¤  [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</guid>
      <pubDate>Mon, 24 Jun 2024 18:53:12 GMT</pubDate>
    </item>
    <item>
      <title>â€œRho-1ï¼šå¹¶éæ‰€æœ‰ä»£å¸éƒ½æ˜¯ä½ æ‰€éœ€è¦çš„â€ï¼ŒLin ç­‰äººï¼Œ2024 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</guid>
      <pubDate>Mon, 24 Jun 2024 18:38:52 GMT</pubDate>
    </item>
    <item>
      <title>æ— æ¨¡å‹ Stewart å¹³å°</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</link>
      <description><![CDATA[        ç”±    /u/FriendlyStandard5985   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</guid>
      <pubDate>Mon, 24 Jun 2024 18:35:11 GMT</pubDate>
    </item>
    <item>
      <title>è¿™æ˜¯ RL çš„ä¸€ä¸ªå·§å¦™åº”ç”¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</link>
      <description><![CDATA[æ•°æ®é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ ï¼Œå®ç°æ´—è¡£æœºä¸­çš„æœ€ä½³ç”µæœºæ§åˆ¶    æäº¤äºº    /u/Obsesdian   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</guid>
      <pubDate>Mon, 24 Jun 2024 16:27:03 GMT</pubDate>
    </item>
    <item>
      <title>æœ‰äººå¯ä»¥åˆ›å»ºå®Œæ•´çš„ Pytorch -> Jax (ç¼–è¯‘å™¨) å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnedvc/can_someone_create_full_pytorch_jax_compiler/</link>
      <description><![CDATA[å—¨ï¼ æˆ‘è¯»åˆ° JAX é€‚ç”¨äºå¸¸é‡å¤§å°æ•°ç»„ã€‚å‡è®¾æˆ‘ä»¬æœ‰ torch æ•°ç»„ï¼š self.states = torch.zeros((self.capacity, state_dim), dtype=torch.float32).to(device) self.actions = torch.zeros((self.capacity, action_dim), dtype=torch.float32).to(device) self.rewards = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device) self.next_states = torch.zeros((self.capacity, state_dim), dtype=torch.float32).to(device) self.dones = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device)  ä½†å¯¹æˆ‘æ¥è¯´ï¼ŒJAX ä»£ç çœ‹èµ·æ¥åƒæ±‡ç¼–ç¨‹åºï¼Œè€Œä¸æ˜¯ç›´è§‚å‹å¥½çš„ Pytorchã€‚æˆ‘ç›¸ä¿¡æœ‰å¤§é‡ç”¨ Pytorchï¼ˆæˆ– Tensorflowï¼‰ç¼–å†™çš„åº“ã€‚æ˜¯å¦å¯ä»¥åˆ›å»ºå®Œæ•´çš„ Pytorch -&gt; JAX ç¼–è¯‘å™¨å¹¶è¿è¡Œä»£ç ï¼Œä¾‹å¦‚é€šè¿‡ä»¥ä¸‹æ–¹å¼ä»ç¼–è¯‘å™¨è®­ç»ƒï¼ˆåŠå…¶ä¾èµ–é¡¹ï¼‰ï¼š sys.argv è¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦åƒåœ¨ C è¯­è¨€ä¸­é‚£æ ·è½¬åˆ° JAX ç»†èŠ‚äº†ã€‚ PS: å½“æˆ‘ä»¬è¾¾åˆ° Replay Buffer æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ roll å‡½æ•°è¿›è¡Œå·¦ç§»ï¼š  def add(self, state, action, reward, next_state, done): if self.length&lt;self.capacity: self.length += 1 idx = self.length-1 self.states[idx,:] = torch.FloatTensor(state).to(self.device) self.actions[idx,:] = torch.FloatTensor(action).to(self.device) self.rewards[idx,:] = torch.FloatTensor([reward]).to(self.device) self.next_states[idx,:] = torch.FloatTensor(next_state).to(self.device) self.dones[idx,:] = torch.FloatTensor([done]).to(self.device) if self.length==self.capacity: self.states = torch.roll(self.states, shifts=-1, dims=0) self.actions = torch.roll(self.actions, shifts=-1, dims=0) self.rewards = torch.roll(self.rewards, shifts=-1, dims=0) self.next_states = torch.roll(self.next_states, shifts=-1, dims=0) self.dones = torch.roll(self.dones, shifts=-1, dims=0)     æäº¤äºº    /u/Timur_1988   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnedvc/can_someone_create_full_pytorch_jax_compiler/</guid>
      <pubDate>Mon, 24 Jun 2024 14:09:15 GMT</pubDate>
    </item>
    <item>
      <title>å­—å…¸æ ¼å¼ä¸æ•°ç»„æ ¼å¼ä»¥åŠå¯¹æ€§èƒ½çš„å½±å“ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dn6pqo/dict_format_vs_array_format_and_affect_on/</link>
      <description><![CDATA[æˆ‘æœ‰å››ä¸ªç‹¬ç«‹ä¸”ä¸å…·æœ‰ç©ºé—´ç›¸å…³æ€§çš„å› ç´ ã€‚æˆ‘æ­£åœ¨å°è¯•å†³å®šå¦‚ä½•æ„å»ºæˆ‘çš„è§‚å¯Ÿç»“æœä»¥è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è€ƒè™‘åˆ°å½“è§‚å¯Ÿç»“æœæ˜¯å­—å…¸æ—¶ï¼Œå®ƒä»¬æ˜¯æ‰å¹³çš„ï¼Œå¦‚æœæˆ‘å°†å®ƒä»¬å­˜å‚¨ä¸ºå­—å…¸è€Œä¸æ˜¯è¿æ¥æ•°ç»„ï¼Œåœ¨è®­ç»ƒæ–¹é¢ä¼šæœ‰æ‰€ä¸åŒå—ï¼Ÿ   DICTï¼šobs = {â€œfactor1â€ï¼š[0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0]ï¼Œâ€œfactor2â€ï¼š[1ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0]ï¼Œâ€œfactor3â€ï¼š[0ï¼Œ0ï¼Œ0ï¼Œ1ï¼Œ0]ï¼Œâ€œfactor4â€ï¼š[0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ1]  ARRAYï¼šobs = [0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ1]     ç”±    /u/TheExistentialSaudi  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dn6pqo/dict_format_vs_array_format_and_affect_on/</guid>
      <pubDate>Mon, 24 Jun 2024 06:15:43 GMT</pubDate>
    </item>
    <item>
      <title>é—®é¢˜ï¼šæœ´ç´  GA æ²¡æœ‰å­¦ä¹ ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dn3g8u/question_naive_ga_not_learning/</link>
      <description><![CDATA[      ä½ ä»¬çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘çš„è›‡æ²¡æœ‰å­¦ä¹ å—ï¼Ÿå®ƒå æ®äº†æ•´ä¸ª 8x8ï¼ˆ64ï¼‰æ¸¸æˆæ¿ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåŠ¨ä½œï¼ˆ4 ä¸ªç¥ç»å…ƒï¼‰ã€‚ æˆ‘æ­£åœ¨åšçš„æ˜¯ï¼š1. åˆå§‹åŒ–éšæœºåŸºç¡€ç½‘ç»œï¼ˆaï¼‰2. åˆå§‹åŒ–éšæœºçªå˜ç½‘ç»œï¼ˆbï¼‰3. å¯¹äºæ¯ä¸ªå¹¶å‘æ¨¡æ‹Ÿï¼Œé€šè¿‡ï¼ˆsimulation_index / total_simulationsï¼‰[å› å­] åˆå¹¶ a å’Œ b è¿™å¾ˆå¹¼ç¨šï¼Œå› ä¸ºæ²¡æœ‰äº¤å‰ã€‚ å®ƒåœ¨æŸç§ç¨‹åº¦ä¸Šæœ‰æ•ˆï¼Œä½†æ˜¯å½“è›‡é•¿å¤§æ—¶å®ƒå°±åœæ­¢å­¦ä¹ äº†ã€‚ æœ‰äººå¯ä»¥è§£é‡Šä¸€ä¸‹å—ï¼Ÿ    æäº¤äºº    /u/mguinhos   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dn3g8u/question_naive_ga_not_learning/</guid>
      <pubDate>Mon, 24 Jun 2024 02:59:37 GMT</pubDate>
    </item>
    <item>
      <title>ç¼–ç¨‹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</link>
      <description><![CDATA[        æäº¤äºº    /u/chagdubbish   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</guid>
      <pubDate>Sun, 23 Jun 2024 20:06:01 GMT</pubDate>
    </item>
    <item>
      <title>â€œå¯¹åœ¨ç¬¦å·å¤šæ­¥æ¨ç†ä»»åŠ¡ä¸Šè®­ç»ƒçš„ Transformer çš„æœºåˆ¶åˆ†æâ€ï¼ŒBrinkmann ç­‰äºº 2024 å¹´ï¼ˆTransformer å¯ä»¥åœ¨å‰å‘ä¼ é€’ä¸­è¿›è¡Œå†…éƒ¨è§„åˆ’ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmt524/a_mechanistic_analysis_of_a_transformer_trained/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmt524/a_mechanistic_analysis_of_a_transformer_trained/</guid>
      <pubDate>Sun, 23 Jun 2024 18:46:40 GMT</pubDate>
    </item>
    <item>
      <title>ä¸º Pong æ¸¸æˆåˆ¶ä½œäº† AIï¼Œä½†å®ƒä»ç„¶å¾ˆç¬¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dms403/made_ai_for_pong_game_but_it_is_still_dumb/</link>
      <description><![CDATA[æˆ‘æ˜¯å¦ç¼ºå°‘å‰§é›†æ•°é‡ï¼Ÿæˆ‘ä¸çŸ¥é“ï¼Œä½†æ˜¯å½“æˆ‘åœ¨æ¸¸æˆä¸­å°è¯•æ—¶ï¼Œå®ƒå®Œå…¨ç¼ºä¹å¦‚ä½•ç”¨å‡»çƒæ‰‹å‡»çƒçš„çŸ¥è¯†ï¼ˆå…¸å‹çš„ Pong æ¸¸æˆï¼‰ è¿™æ˜¯train.py -&gt; https://www.pythonmorsels.com/p/2y9vw/ è¿™æ˜¯env.py -&gt; https://www.pythonmorsels.com/p/2vdwb/    æäº¤äºº    /u/Cautious-Plan-9491   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dms403/made_ai_for_pong_game_but_it_is_still_dumb/</guid>
      <pubDate>Sun, 23 Jun 2024 18:00:49 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºdreamerv3çš„æ–¹ç¨‹æ¨å¯¼çš„é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmiv75/question_about_the_equation_derivation_of/</link>
      <description><![CDATA[      å—¨ï¼ æˆ‘å°è¯•å­¦ä¹  dreamerv3 åŠå…¶ä»£ç å®ç°ã€‚ä½†æ˜¯å¯¹äºç­‰å¼ 10ï¼Œ https://preview.redd.it/9xv44gx5oa8d1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8330ac94061559235a5b0448251c2561f64628d4 æˆ‘å¯¹ ln_p çš„è®¡ç®—æ„Ÿåˆ°å›°æƒ‘ï¼Œå®ƒå¯¹åº”äºä»¥ä¸‹ä»£ç ç‰‡æ®µ log_pred = self.logits - jax.scipy.special.logsumexp(self.logits, -1, keepdims=True) ä½ èƒ½å‘æˆ‘è§£é‡Šä¸€ä¸‹ä¸Šè¿°å®ç°å—ï¼Ÿä¸ºä»€ä¹ˆè¦ä»¥è¿™ç§æ–¹å¼æ¨å¯¼ log_predï¼Ÿ æå‰è‡´è°¢ã€‚    æäº¤äºº    /u/UpperSearch4172   [link] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmiv75/question_about_the_equation_derivation_of/</guid>
      <pubDate>Sun, 23 Jun 2024 10:03:25 GMT</pubDate>
    </item>
    <item>
      <title>è‚¡ç¥¨äº¤æ˜“çš„ç›‘ç£é¢„å­¦ä¹ ã€‚ä»¥å‰åšè¿‡å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmdn23/supervised_pre_learning_for_stock_trading_has/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä¸ºè®ºæ–‡æå‡ºæ–°é¢–çš„æƒ³æ³•ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºä½¿ç”¨ç›‘ç£å­¦ä¹ â€œçƒ­èº«â€ä»£ç†çš„æƒ³æ³•ã€‚ä¸€èˆ¬çš„æƒ³æ³•æ˜¯ï¼Œä¾‹å¦‚ï¼Œåœ¨ 2005-01-01 åˆ° 2010-12-31 ä¹‹é—´ï¼Œä»£ç†å°†æ¥å—â€œç†æƒ³â€æ“ä½œçš„è®­ç»ƒã€‚ä» 2011-01-01 åˆ° 2020-12-31ï¼Œä»£ç†å°†å†æ¬¡æ¥å—é‡æ–°è®­ç»ƒã€‚ä»¥å‰åšè¿‡å—ï¼Ÿæœ‰æ²¡æœ‰å…³äºè¿™ç§æ¦‚å¿µçš„è®ºæ–‡æˆ‘åº”è¯¥è¯»ä¸€è¯»ï¼Ÿ    æäº¤äºº    /u/newjeison   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmdn23/supervised_pre_learning_for_stock_trading_has/</guid>
      <pubDate>Sun, 23 Jun 2024 04:05:13 GMT</pubDate>
    </item>
    <item>
      <title>è‡ªé€‚åº”å››è½´é£è¡Œå™¨ç½‘ç»œ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</link>
      <description><![CDATA[ğŸ‘‹ å˜¿ï¼Œæˆ‘æ­£åœ¨å°è¯•æƒ³å‡ºä¸€ä¸ªå››è½´é£è¡Œå™¨æ— äººæœºè‡ªä¸»æ§åˆ¶çš„è§£å†³æ–¹æ¡ˆã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘è®¾æ³•è®©å®ƒä½¿ç”¨ç®€å•çš„ MLP + PPO è¿›è¡Œèˆªç‚¹å¯¼èˆªã€‚ è¿™å¾ˆæœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨åŸŸéšæœºåŒ–çš„æƒ…å†µä¸‹ï¼Œä½†å¦‚æœçªç„¶åˆ®èµ·ä¸€é˜µé£ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿå››è½´é£è¡Œå™¨çš„é‡é‡è¶…è¿‡äº†å®ƒåœ¨æ¨¡æ‹Ÿä¸­è®­ç»ƒçš„é‡é‡ï¼Ÿèºæ—‹æ¡¨æŸåï¼Œå…¶ä¸­ä¸€ä¸ªç”µæœºäº§ç”Ÿçš„æ¨åŠ›å‡å°ï¼Ÿ é€šè¿‡æ‰€æœ‰è¿™äº›ç¤ºä¾‹ï¼Œæˆ‘çš„é—®é¢˜æ˜¯æ— äººæœºæ— æ³•é€‚åº”å˜åŒ–ã€‚æˆ‘æƒ³åˆ›å»ºä¸€ä¸ªèƒ½å¤Ÿé€‚åº”ä»»ä½•æ— æ³•è§£é‡Šçš„å¤–éƒ¨å¹²æ‰°çš„ç½‘ç»œã€‚ æˆ‘æœ‰ä¸¤ä¸ªè®¡åˆ’ï¼Œä¸€ä¸ªæ˜¯å°† LSTM ä¸ PPO ç»“åˆä½¿ç”¨ï¼Œå› æ­¤ä»£ç†ä¼šä¿ç•™é£è¡Œæ•°æ®ã€‚ æˆ‘çš„å¦ä¸€ä¸ªæƒ³æ³•æ˜¯ä½¿ç”¨æˆ‘ç”¨äºè®­ç»ƒçš„æ¨¡æ‹Ÿå™¨æ¥è·å–æ— äººæœºåœ¨å½“å‰æ—¶é—´æ­¥é•¿çš„é¢„æœŸé€Ÿåº¦/å§¿æ€/ä½ç½®ï¼Œå¹¶å°†å…¶ä¸æˆ‘ä»ä¼ æ„Ÿå™¨è·å¾—çš„å®é™…å€¼ä¸€èµ·è¾“å…¥åˆ°ç­–ç•¥ä¸­ã€‚ æˆ‘å¸Œæœ›æ›´æœ‰ç»éªŒçš„äººå¯ä»¥å¯¹æˆ‘çš„æƒ³æ³•æä¾›åé¦ˆï¼Œè¯´å®è¯æˆ‘æœ‰ç‚¹è¿·èŒ«ï¼Œæˆ‘ä¸ç¡®å®šè¿™äº›æ˜¯å¦å¯è¡Œï¼Œä»»ä½•å¸®åŠ©éƒ½å€¼å¾—èµèµï¼    æäº¤äºº    /u/FutureComedian7749   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</guid>
      <pubDate>Sat, 22 Jun 2024 23:15:56 GMT</pubDate>
    </item>
    <item>
      <title>AgileRL - ç”¨äºæœ€å…ˆè¿›æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¿›åŒ–å‹ RLOps</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘ä¹‹å‰å‘å¸ƒè¿‡å…³äºæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ è¿›åŒ–è¶…å‚æ•°ä¼˜åŒ–å®ç° SOTA ç»“æœçš„å¸–å­ï¼Œä½†æˆ‘æƒ³åˆ†äº«çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å¼€æºæ¡†æ¶ç°åœ¨å·²ç»å‘å¸ƒäº† v1.0.0 ç‰ˆæœ¬ï¼ è¯·æŸ¥çœ‹ï¼https://github.com/AgileRL/AgileRL è¯¥åº“æœ€åˆä¸“æ³¨äºé€šè¿‡å¼€åˆ›å¼ºåŒ–å­¦ä¹ çš„è¿›åŒ– HPO æŠ€æœ¯æ¥å‡å°‘è®­ç»ƒæ¨¡å‹å’Œè¶…å‚æ•°ä¼˜åŒ–æ‰€éœ€çš„æ—¶é—´ã€‚è¿›åŒ– HPO å·²è¢«è¯æ˜å¯ä»¥é€šè¿‡è‡ªåŠ¨æ”¶æ•›åˆ°æœ€ä½³è¶…å‚æ•°æ¥å¤§å¹…å‡å°‘æ€»ä½“è®­ç»ƒæ—¶é—´ï¼Œè€Œæ— éœ€è¿›è¡Œå¤§é‡çš„è®­ç»ƒè¿è¡Œã€‚ æˆ‘ä»¬ä¸æ–­æ·»åŠ æ›´å¤šç®—æ³•å’ŒåŠŸèƒ½ã€‚ AgileRL å·²ç»åŒ…å«äº†æœ€å…ˆè¿›çš„å¯è¿›åŒ–çš„åœ¨çº¿ç­–ç•¥ã€ç¦»ç­–ç•¥ã€ç¦»çº¿ã€å¤šæ™ºèƒ½ä½“å’Œä¸Šä¸‹æ–‡å¤šè‡‚è€è™æœºå¼ºåŒ–å­¦ä¹ ç®—æ³•ä»¥åŠåˆ†å¸ƒå¼è®­ç»ƒã€‚ æˆ‘å¾ˆä¹æ„æ”¶åˆ°æ‚¨çš„åé¦ˆï¼    æäº¤äºº    /u/nicku_a   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</guid>
      <pubDate>Fri, 21 Jun 2024 17:50:05 GMT</pubDate>
    </item>
    </channel>
</rss>