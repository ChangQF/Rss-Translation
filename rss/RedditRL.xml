<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 22 Mar 2024 09:14:58 GMT</lastBuildDate>
    <item>
      <title>需要 DDQN 自动驾驶汽车项目的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</link>
      <description><![CDATA[      我最近开始学习强化学习，我用ddqn做了一个自动驾驶汽车项目，输入是这些光线的长度输出是向前、向后、向左、向右，什么都不做。我的问题是 rl Agent 需要多少时间来学习？即使已经播出了 40 集，它仍然没有达到奖励门槛。我还根据前进速度给予 0-1 奖励   由   提交/u/Invicto_50  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</guid>
      <pubDate>Fri, 22 Mar 2024 02:31:00 GMT</pubDate>
    </item>
    <item>
      <title>“RewardBench：评估语言建模的奖励模型”，Lambert 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkmtdy/rewardbench_evaluating_reward_models_for_language/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkmtdy/rewardbench_evaluating_reward_models_for_language/</guid>
      <pubDate>Fri, 22 Mar 2024 00:56:53 GMT</pubDate>
    </item>
    <item>
      <title>处理 Deep Q 网络中不同大小的状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkj8ve/dealing_with_states_of_varying_size_in_deep_q/</link>
      <description><![CDATA[问候， ​ 我是强化学习新手，我决定用 Python 制作一个简单的贪吃蛇游戏，这样我就可以训练 DQN 代理来玩它。在游戏的状态表示中，我传递给它的变量之一是一个包含所有 Snake 当前位置的列表（即，Snake 主体占据的每个位置都有一个元组 (x,y)）。在训练中，一旦 Snake 吃掉食物颗粒并长大，代理总是会崩溃，因为状态大小与初始值不同。 ​ 我在互联网上搜索了解决这个问题的方法。  一种解决方案是在状态上仅表示蛇的头，并添加四个变量来判断是否有上/下、左/右障碍物。这个解决方案似乎并没有捕获所有的基本信息，所以我怀疑代理即使训练了数千年也无法发挥最佳性能。  另一个解决方案是将蛇的身体表示为长度等于其最大可实现大小的列表，这确实捕获了所有基本信息，但如果我将地图大小增加到大值，可能会减慢该过程. ​ 我想知道，有没有办法处理深度 Q 网络中不同大小的状态？给代理的初始状态大小是否定义了所有后续状态的大小？   由   提交 /u/Clovergheister   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkj8ve/dealing_with_states_of_varying_size_in_deep_q/</guid>
      <pubDate>Thu, 21 Mar 2024 22:18:15 GMT</pubDate>
    </item>
    <item>
      <title>我应该继续训练他吗？因为显然他不想...</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkghvu/should_i_continue_training_him_cause_apparently/</link>
      <description><![CDATA[   /u/DisDoh  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkghvu/should_i_continue_training_him_cause_apparently/</guid>
      <pubDate>Thu, 21 Mar 2024 20:26:39 GMT</pubDate>
    </item>
    <item>
      <title>是否有基于 JAX 的 Arcade 学习环境实现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bka5nu/are_there_any_jaxbased_implementations_of_arcade/</link>
      <description><![CDATA[Arcade 学习环境有基于 JAX 的实现吗？我想使用 PureJaxRL 在 GPU 上加速 ALE 环境，例如 Gymnax/BRAX 环境？   由   提交/u/C7501  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bka5nu/are_there_any_jaxbased_implementations_of_arcade/</guid>
      <pubDate>Thu, 21 Mar 2024 16:07:42 GMT</pubDate>
    </item>
    <item>
      <title>SAC实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk944r/sac_implementation/</link>
      <description><![CDATA[嗨，我正在尝试在没有价值网络的情况下实现 Soft Actor Critic 算法，我想我已经接近并相信我的所有梯度都是是的，但我找不到问题。  我按照 spin-up 的解释阅读了原文。我的代码受到 youtube 频道“与 Phil 的机器学习”的启发 ​ 我尝试在 Pendulum-v1 环境上运行它，但它不收敛并且最终总是选择相同的操作。 有人可以帮助我处理我的代码吗？  def learn(self): if self.memory.mem_cntr &lt; p&gt;​ class ActorNetwork(keras.Model): def init(self, n_actions, Noise=1e-6): super(ActorNetwork, self).init() self. n_actions = n_actions self.noise = 噪声 self.fc1 = Dense(400, 激活=“relu”) self.fc2 = Dense(200, 激活=“relu”) self.mu = Dense(n_actions, 激活=无) self.sigma = Dense(n_actions,activation=None) def call(self, state): value = self.fc1(state) value = self.fc2(value) mu = self.mu(value) sigma = tf.clip_by_value( self.sigma(value), self.noise, 1) return mu, sigma def sample_normal(self, state): mu, sigma = self.call(state) # print(tf.reduce_mean(mu).numpy(), tf .reduce_mean(sigma).numpy()) action = tf.random.normal(mu.shape, mu, sigma, dtype=tf.float32) # log_prob = tf.math.log(tf.math.exp(-0.5 * tf.math.pow((action - mu) / sigma, 2)) / (sigma*tf.math.sqrt(2*NP_PI))) log_prob = -0.5 * tf.math.pow((action-mu)/ sigma, 2) - tf.math.log(sigma*tf.math.sqrt(2*NP_PI)) # 只是高斯分布的简化 log_prob -= tf.math.log(1-tf.math.square(tf.math) .tanh(action))) # 请参阅 Haarnoja2019“附录 C 执行操作边界”返回操作，log_prob    由   提交/u/antobom  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk944r/sac_implementation/</guid>
      <pubDate>Thu, 21 Mar 2024 15:23:11 GMT</pubDate>
    </item>
    <item>
      <title>斯瓦亚特机器人|印度 |极其动态复杂的交通动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk80o9/swaayatt_robots_india_extremely_dynamiccomplex/</link>
      <description><![CDATA[       由   提交/u/shani_786  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk80o9/swaayatt_robots_india_extremely_dynamiccomplex/</guid>
      <pubDate>Thu, 21 Mar 2024 14:36:58 GMT</pubDate>
    </item>
    <item>
      <title>DDPG算法伪代码latex</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk2jdg/ddpg_algorithm_psudocode_latex/</link>
      <description><![CDATA[谁能提供 DDPG 的 peusdocode 的 tex 代码吗？ 我知道它很懒，但我目前的工作量太大了，谢谢提前   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk2jdg/ddpg_algorithm_psudocode_latex/</guid>
      <pubDate>Thu, 21 Mar 2024 09:30:20 GMT</pubDate>
    </item>
    <item>
      <title>Rich Sutton 今天来参加我们的 Zoom 讲座。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjvzar/rich_sutton_came_to_our_zoom_lecture_today/</link>
      <description><![CDATA[       由   提交/u/chunchblooden  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjvzar/rich_sutton_came_to_our_zoom_lecture_today/</guid>
      <pubDate>Thu, 21 Mar 2024 02:35:28 GMT</pubDate>
    </item>
    <item>
      <title>RL 中的重置功能：初始状态还是当前状态？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/</link>
      <description><![CDATA[大家好， 我是强化学习新手。我目前正在致力于实现一个 RL 环境，以使用恒温器控制房间的温度。我正处于重置功能的十字路口，希望获得一些见解。 该环境代表一个带有恒温器的房间，其目标是将温度保持在所需的设定值，同时最大限度地减少能源消费。环境状态由当前温度和能耗表示，操作包括调整恒温器设置以增加或减少温度设定点。  我想到了两种方法：  重置为初始状态：此选项涉及在每个方法开始时将环境重置为其初始配置情节，其中温度设置为预定义的起始值。 （设置为最小状态）。 重置为当前状态：或者，重置功能可以将环境返回到房间的当前状态。 &lt; /ol&gt; 我特别喜欢重置当前状态的想法，因为它与我在代理决策过程中的方法一致。在每个步骤中，当代理选择一个操作时，我计划检查代理是否可以根据状态向量中表示的当前状态执行该操作，然后给出正奖励，否则给出负奖励。  问题：在这种环境下，您认为哪种方法更适合我的强化学习算法中的重置功能？我还需要考虑任何其他注意事项吗？ 提前感谢您的帮助！   由   提交/u/Few-Papaya101  /u/Few-Papaya101 reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/</guid>
      <pubDate>Wed, 20 Mar 2024 19:47:19 GMT</pubDate>
    </item>
    <item>
      <title>与决斗+c51作斗争，需要第二双眼睛。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjlmsa/struggling_with_duellingc51_need_a_second_pair_of/</link>
      <description><![CDATA[编辑： 继承时我确实犯了一个错误，我不小心覆盖了依赖注入。这导致优化器指向错误的网络。 原为： ```cs  public DQNPerNoisy(DQNAgentOptions opts, List&gt;; envs, IDQNNetProvider netProvider = null) : base(opts, envs, new DuelingNoisyNetworkProvider(opts.Width, opts.Depth, opts.NumAtoms))  { }  ``` ​ 应该是： ``` cs  public DQNPerNoisy(DQNAgentOptions opts, List&gt; envs, IDQNNetProvider netProvider = null) : base(opts, envs, netProvider ?? new DuelingNoisyNetworkProvider(opts.Width, opts.Depth, opts.NumAtoms)) { } ``` ​ 至少我在修复错误后立即开始工作，所以我的算法和张量形状都很好 😎  😎 😎但是复杂性dueling+c51 所做的事情是我到处寻找而不是错误所在。  亲爱的 RL 社区， 在尝试了很多个小时在我的框架中实现 c51+dueling 后，我终于准备放弃了。作为我最后的努力，我发布此内容是希望一些天才可以帮助我找到算法中的错误。 我正在尝试使用 TorchSharp 在 C# 中创建一个 DRL 库，并且我一直在尝试还提供彩虹。事实证明这真的很困难（或者我还没有准备好） 长话短说，我的损失并没有减少，我不确定我在实现算法时在哪里犯了错误，我尝试了很多事情并且无法弄清楚。 有人可以看一下并尝试发现我的错误吗？ 我的代码在这里：网络：https://github.com/asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/ DQN/NN/Variants/DuelingDNQ_C51_Noisy.cs 代理：https://github.com/asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/DQN/Variants/DQNRainbow.cs Python 翻译对于那些讨厌 C# 的人：https://github.com/ asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/DQN/Variants/PythonRainbowTranslation.txt 也欢迎道德支持。   由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjlmsa/struggling_with_duellingc51_need_a_second_pair_of/</guid>
      <pubDate>Wed, 20 Mar 2024 19:12:20 GMT</pubDate>
    </item>
    <item>
      <title>SB3 的 DQN 不起作用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bj8nh9/dqn_from_sb3_doesnt_work/</link>
      <description><![CDATA[我有一个自定义的 boid 植绒环境并使用 SB3 实现了 DQN。 但是，它继续运行并且没有输出任何内容。似乎无法找出错误。 代码：https://drive .google.com/drive/folders/1zoQSrLOVO13TBGtoJhkg5LwVoKEmM2gT?usp=sharing   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bj8nh9/dqn_from_sb3_doesnt_work/</guid>
      <pubDate>Wed, 20 Mar 2024 08:31:23 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 PyTorch 中实现 crossQ 不起作用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bj3rln/trying_to_implement_crossq_in_pytorch_does_not/</link>
      <description><![CDATA[你好，我正在尝试实现： https://openreview.net/pdf?id=PczQtTsTIX 而且似乎无法获得好的结果。我已经在 Ant-v4 中尝试过了 在更新贝尔曼方程中对我的 SAC 代码进行了以下更改： def update_critic(self, state_batch: torch.tensor, action_batch：torch.tensor，reward_batch：torch.tensor，next_state_batch：torch.tensor，mask_batch：torch.tensor）-&gt; torch.tensor: “””使用 Soft Bellman 方程更新批评家：param state_batch：从内存中提取的状态批次：param action_batch：从内存中提取的操作批次：paramreward_batch：从内存中提取的奖励批次：param next_state_batch：从内存中提取的下一个状态批次批次：param mask_batch：完成的掩码：返回：Q1 的浮动损失，Q2 的浮动损失“”“” next_state_action_batch, next_state_log_pi_batch, _ = self.policy.sample(next_state_batch, False, False) if self.config_agent[&#39;crossqstyle&#39;]: # (bsz x 2, nstate) cat_states = torch.cat([state_batch, next_state_batch], 0) # (bsz x 2, nact) cat_actions = torch.cat([action_batch, next_state_action_batch], 0) # (bsz x 2, 1) qfull1, qfull2 = self.critic(cat_states, cat_actions) # 分离 Q q1, q1next = torch .chunk(qfull1, chunks=2, dim=0) q2, q2next = torch.chunk(qfull2, chunks=2, dim=0) min_qnext = torch.min(q1next, q2next) - self.alpha * next_state_log_pi_batch next_qvalue = (奖励_batch + mask_batch * self.config_agent[&#39;gamma&#39;] * min_qnext).detach() 否则：使用 torch.no_grad(): q1next_target, q2next_target = self.critic_target(next_state_batch, next_state_action_batch) min_qnext = torch.min(q1next_target, q2next_target) - self.alpha * next_state_log_pi_batch next_qvalue =reward_batch + mask_batch * self.config_agent[&#39;gamma&#39;] * min_qnext q1, q2 = self.critic(state_batch, action_batch) q_loss, q1_loss, q2_loss = self.calculate_q_loss(q1, q2, next_qvalue) # 默认 self.critic_optim.zero_grad() q_loss.backward() self.critic_optim.step()  我还在批评者和演员中添加了批规范层。例如批评者的初始化： # Activations if activate == &quot;relu&quot;: self.activation = nn.ReLU() elifactivation == &quot;leaky_relu&quot;: self.activation = nn.LeakyReLU() elif 激活 == &quot;tanh&quot;: self.activation = nn.Tanh() else: 如果 bn_mode == &quot;bn&quot; 则引发 NotImplementedError: BN = nn.BatchNorm1d elif bn_mode == &quot;brn&quot; : BN = BatchRenorm1d else: raise NotImplementedError # Layers self.q1_list = nn.ModuleList() self.q2_list = nn.ModuleList() # BN层0 - 根据crossQ的代码 if use_batch_norm: self.q1_list.append(BN( state_dim + action_dim，动量=bn_momentum）） self.q2_list.append（BN（state_dim + action_dim，动量=bn_momentum）） self.q1_list.append（nn.Linear（int（（state_dim + action_dim）），hidden_​​dim）） self. q1_list.append(self.activation) self.q2_list.append(nn.Linear(int((state_dim + action_dim)),hidden_​​dim)) self.q2_list.append(self.activation) if use_batch_norm: self.q1_list.append(BN (hidden_​​dim,动量=bn_momentum)) self.q2_list.append(BN(hidden_​​dim,动量=bn_momentum)) for i in range(num_layers - 1): self.q1_list.append(nn.Linear(hidden_​​dim,hidden_​​dim)) self. q1_list.append(self.activation) self.q2_list.append(nn.Linear(hidden_​​dim,hidden_​​dim)) self.q2_list.append(self.activation) 如果use_batch_norm: self.q1_list.append(BN(hidden_​​dim)) self.q2_list .append(BN(hidden_​​dim)) self.q1_list.append(nn.Linear(hidden_​​dim, 1)) self.q2_list.append(nn.Linear(hidden_​​dim, 1))  是否有人有什么想法吗？ 我的 SAC 版本学习得很好。然而，在论文中，他们还报告了以批评者中的 tanh 激活函数作为基线的 SAC，在我的例子中，这也不起作用（默认激活是 ReLU）。我尝试使用论文中报告的超参数。 也许我忘记了一些技巧？   由   提交/u/LazyButAmbitious  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bj3rln/trying_to_implement_crossq_in_pytorch_does_not/</guid>
      <pubDate>Wed, 20 Mar 2024 03:14:41 GMT</pubDate>
    </item>
    <item>
      <title>“通过老虎机优化识别一般反应条件”，Wang 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biu16l/identifying_general_reaction_conditions_by_bandit/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biu16l/identifying_general_reaction_conditions_by_bandit/</guid>
      <pubDate>Tue, 19 Mar 2024 20:14:24 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>