<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 05 Feb 2024 18:18:44 GMT</lastBuildDate>
    <item>
      <title>[建议]OpenAI GYM/Stable Baselines：如何设计动作空间的依赖动作子集？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajikd2/advice_openai_gymstable_baselines_how_to_design/</link>
      <description><![CDATA[您好， 我正在开发自定义 OpenAI GYM/Stable Baseline 3 环境。假设我的环境中有总共 5 个操作 (0,1,2,3,4) 和 3 个状态 (A, B, Z)。在状态 A 中，我们只允许两个操作 (0,1)，状态 B 操作为 (2,3)  并且在状态 Z 中，所有 5 个都可供代理使用。 我一直在阅读各种文档/论坛（并且还实现了）允许所有动作在所有状态下都可用，但当在某个状态下执行无效动作时分配（大）负奖励。然而，在训练过程中，这会导致我出现奇怪的行为（特别是扰乱我的其他奖励/惩罚逻辑），这是我不喜欢的。 我想以编程方式清楚地消除每个动作中的无效动作状态，所以它们甚至不可用。使用动作组合的蒙版/向量对我来说也不可取。我还读到不建议动态改变操作空间（出于性能目的）？ TL;DR 我希望听到人们如何解决这个问题的最佳实践，我确信这对许多人来说都是常见情况。 编辑：我可能正在考虑的解决方案之一是返回 self.state 通过步骤循环中的info，然后实现一个自定义函数/lambda，它基于状态去除无效操作，但我认为这将是一个非常丑陋的黑客/干扰gym/sb的内部运作。 编辑2：再想一想，我认为上述想法真的很糟糕，因为它不允许模型学习训练阶段（循环阶段之前）的可用操作子集。因此，我认为这应该集成到环境的 Action Space 部分中。 编辑 3： 这个问题似乎也提到了 之前，但我没有使用 PPO 算法。   由   提交 /u/against_all_odds_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajikd2/advice_openai_gymstable_baselines_how_to_design/</guid>
      <pubDate>Mon, 05 Feb 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>寻求指导：选择低计算能力的 ML 研究主题进行会议提交</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajgagr/seeking_guidance_choosing_a_lowcomputational/</link>
      <description><![CDATA[机器学习科学家您好， 我正在寻找撰写机器学习领域的研究论文，并打算将其提交给明年将举行一次享有盛誉的会议。虽然我对机器学习和深度学习的基础知识有深入的了解，但我受到可用计算资源的限制；我将使用我的笔记本电脑进行研究。鉴于这一限制，您能否推荐一个无需大量计算能力即可探索的机器学习研究领域？ 谢谢  &amp; #32；由   提交 /u/Significant-Raise-61   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajgagr/seeking_guidance_choosing_a_lowcomputational/</guid>
      <pubDate>Mon, 05 Feb 2024 13:18:08 GMT</pubDate>
    </item>
    <item>
      <title>RL 的部分单调网络 [D]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajf7pa/partially_monotonic_networks_for_rl_d/</link>
      <description><![CDATA[大家好，正在寻找有关我正在做的项目的建议和评论。 我正在尝试做一个策略梯度强化学习问题其中某些输入/输出对之间存在某些递增/递减关系是可取的。 有一个基于理论 pde 的最优策略（具有所需的单调性）作为基线，并且无约束的简单 FNN 可以优于 pde 和尽管不存在单调性，但策略大多是一致的。 下一步，我想将部分矩阵权重限制为非负，以便获得部分单调的神经网络。该结构遵循 Trindade 2021，其中有两个 NN 块，一个受单调输入约束，另一个受正常输入约束，两个输出连接并馈入受约束的 NN 中以提供单个输出。 （我将 -1 乘以应随输出而减少的约束输入） 我在获得 pde 基线的目标值方面没有取得太大成功。对于激活，我尝试了 tanh，它最终给了我一堆线性神经网络。然后我使用了leakyrelu，其中一半是正常的，一半应用为-leakyrelu(-x)，以便该函数可以是单调的且具有非单调斜率（最佳策略可能有平坦部分）。我尝试了批量大小、学习率、神经网络维度等的整个网格，但没有成功。 任何对我的方法的评论或对下一步尝试的建议都表示赞赏。感谢您的阅读！   由   提交/u/CaptTeemo175  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajf7pa/partially_monotonic_networks_for_rl_d/</guid>
      <pubDate>Mon, 05 Feb 2024 12:21:23 GMT</pubDate>
    </item>
    <item>
      <title>华为：在现实世界中通过不确定性做出值得信赖的决策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj520x/hua_wei_trustworthy_decision_making_in_the_real/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj520x/hua_wei_trustworthy_decision_making_in_the_real/</guid>
      <pubDate>Mon, 05 Feb 2024 01:57:24 GMT</pubDate>
    </item>
    <item>
      <title>离线 DQN 的加权重要性采样</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj3l8f/weighted_importance_sampling_for_offline_dqn/</link>
      <description><![CDATA[      我的问题是“在离线环境中比较不同强化学习策略的最佳方式是什么？” 我自己做了一些研究，发现了加权重要性抽样是平衡偏差和方差的好方法，特别是当行为策略不是最优的（或接近我们想要评估的策略）时。然而，如果我们训练一个获取高维特征的 DQN 模型，并输出推荐操作的概率（假设通过 softmax 层），那么我在实现上会遇到问题。考虑到我们无法访问状态空间（我们具有以下特征），我们应该在 WIS 公式中将 π(at|st) （来自 RL 模型的最优策略）或 μ(at|st) （行为策略）设置为什么可以产生无限多个状态）？ ​ WIS 公式   由   提交/u/anagreement  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj3l8f/weighted_importance_sampling_for_offline_dqn/</guid>
      <pubDate>Mon, 05 Feb 2024 00:45:51 GMT</pubDate>
    </item>
    <item>
      <title>q 函数近似不收敛的 Actor Critic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/</link>
      <description><![CDATA[最近我一直在尝试实现论文。 但是，当使用车杆 v1 环境时，代理会学习一些行为，但随后就会崩溃。任何有关不正确实现或替代批评者功能的想法将不胜感激。 我也一直在使用超参数，但没有任何组合对我来说效果很好。 代码 ​ 编辑：我已经意识到在得分函数中，我不应该取输出的总和，而应该在 for 循环中单独计算每个动作的梯度。然而，该代理的性能仍然不佳。    由   提交/u/Tight_Apple_678   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/</guid>
      <pubDate>Mon, 05 Feb 2024 00:17:55 GMT</pubDate>
    </item>
    <item>
      <title>“从强化学习到代理：理解基础认知的框架”，Seifert 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ais80w/from_reinforcement_learning_to_agency_frameworks/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ais80w/from_reinforcement_learning_to_agency_frameworks/</guid>
      <pubDate>Sun, 04 Feb 2024 16:46:34 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶|斯瓦亚特机器人 |马恒达塔尔 |印度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aij99o/autonomous_driving_swaayatt_robots_mahindra_thar/</link>
      <description><![CDATA[       由   提交/u/shani_786  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aij99o/autonomous_driving_swaayatt_robots_mahindra_thar/</guid>
      <pubDate>Sun, 04 Feb 2024 08:31:55 GMT</pubDate>
    </item>
    <item>
      <title>如何同时训练代理和输入编码？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aibim9/how_to_train_agent_and_input_encoding/</link>
      <description><![CDATA[大家好。我正在研究一个 DRL 项目，其中状态是一个长度不同的序列。因为我使用的是 Gymnasium 和 Stable-Baselines3，需要固定的预定义观察空间形状，所以我正在考虑使用 RNN 将输入序列编码为 Gym 环境中的预定义形状，然后让 SB3 处理训练. 所以我的问题是，有没有办法同时训练 RNN 和 RL 代理？我没有太多经验，但我的理解是我需要连接 RNN 和策略/价值网络以使反向传播工作，但我不确定在使用 Gym 和 SB3 时如何实现这一点，因为 RNN是在 Gym 环境中创建的，而策略/价值网络是在 SB3 模型中的。   由   提交/u/McCree76   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aibim9/how_to_train_agent_and_input_encoding/</guid>
      <pubDate>Sun, 04 Feb 2024 01:06:50 GMT</pubDate>
    </item>
    <item>
      <title>DQN 不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</link>
      <description><![CDATA[嗨，我正在尝试做一个蛇游戏 DQN，但在前 1k 次迭代中没有看到太多结果（如果有的话）。该模型似乎正在回归。 我想知道我的更新循环是否正确 信息：吃食物的奖励+1，碰撞-1，其他奖励 - 欧氏距离/来自食物的 l2 范数 我确实有一个重播缓冲区 def train_v2(self, state_tensor, action,reward, new_state_tensor, did): # state -&gt; (3,32,24) # 模型 -&gt; conv net action_tensor = torch.tensor(action, dtype=torch.float)reward_tensor = torch.tensor(reward, dtype=torch.float) if len(state_tensor.shape) == 3: # 转换为批处理形式 if single example state_tensor = torch.unsqueeze（state_tensor，dim = 0）action_tensor = torch.unsqueeze（action_tensor，dim = 0）reward_tensor = torch.unsqueeze（reward_tensor，dim = 0）new_state_tensor = torch.unsqueeze（new_state_tensor，dim = 0）完成=（ done,) for i in range(len(done)): # for idx in batch q_pred = self.model.forward(state_tensor[i]) if did[i]: q_next = torch.zeros(1) # 没有下一个状态否则： q_next = self.model_target.forward(new_state_tensor[i]).max(dim=1)[0] q_target =reward_tensor[i] + self.gamma*q_next self.optimizer.zero_grad() loss = self.loss_function( q_target, q_pred) loss.backward() self.optimizer.step()    由   提交/u/throtaway85633  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</guid>
      <pubDate>Sat, 03 Feb 2024 05:39:48 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo tic-tac-toe 游戏中何时调用 reset() 函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</link>
      <description><![CDATA[我正在将 pettingzoo 环境用于 MARL 程序，并使用 tictactoe 环境（在此处找到）作为蓝图。现有环境似乎在每个单独的时期内多次调用重置函数，这对于我自己的目的来说是不可取的。在尝试查找重置调用的来源时，我将其追溯到“base.py”文件。文件位于 pettingzoo /utils/wrappers 目录中。我仍然无法准确确定何时调用重置。我想让它仅在每个纪元结束时调用重置，因为我积累了不想重置的值。 我复制了 tictactoe 测试代码来运行它。我在重置函数中放置了一个打印调用，以查看每个时期内调用重置的次数。我确认在井字棋游戏的每个时期都会多次调用重置。这样做的目的是什么？在我看来，你会想在每场比赛结束时调用重置。为什么要多次重置，如何更改重置被调用的次数？   由   提交/u/NobodySmart1617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</guid>
      <pubDate>Fri, 02 Feb 2024 22:36:23 GMT</pubDate>
    </item>
    <item>
      <title>Nuro 实现大规模强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahehz4/nuro_enabling_reinforcement_learning_at_scale/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/recklessdesuka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahehz4/nuro_enabling_reinforcement_learning_at_scale/</guid>
      <pubDate>Fri, 02 Feb 2024 21:16:17 GMT</pubDate>
    </item>
    <item>
      <title>DQN 探索策略的收敛速度比贪婪策略快得多</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ah70er/dqn_exploration_policy_converges_much_faster_than/</link>
      <description><![CDATA[      我在解释以下结果时遇到一些困难。橙色线是奖励训练曲线，蓝色线是评估曲线。 https://preview.redd.it/ashlhzdl07gc1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=0a409ee350a6b3d80c5784b62079f 37f8dfdb9f8 期间训练我使用 epsilon 贪婪策略，epsilon = 0.2。在评估过程中，我使用贪婪的 argmax 策略。这些结果表明，在我的环境中，贪婪策略需要大约 20 万步才能达到最优。然而，epsilon 贪婪策略使用与贪婪策略相同的模型，但以 20% 的概率采取随机行动，只需 50k 步就已经是最优的。 观察到这一点时，您的第一个想法是什么？    由   提交 /u/fedetask   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ah70er/dqn_exploration_policy_converges_much_faster_than/</guid>
      <pubDate>Fri, 02 Feb 2024 15:59:55 GMT</pubDate>
    </item>
    <item>
      <title>PPO算法动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1agx883/ppo_algorithm_actions/</link>
      <description><![CDATA[我知道 PPO 输出操作的平均值和标准差，但是我如何才能将我的操作限制在应用程序的安全范围内。或者还有其他我可以选择的算法来替代 PPO。   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1agx883/ppo_algorithm_actions/</guid>
      <pubDate>Fri, 02 Feb 2024 06:27:16 GMT</pubDate>
    </item>
    <item>
      <title>了解行为政策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1agdtah/understanding_behavior_policy/</link>
      <description><![CDATA[我目前正在尝试了解 on-policy 和 off-policy 之间的差异。 到目前为止，我了解到： - 行为策略：代理用于选择操作的策略 - 目标策略：代理优化的策略 - On-Policy：行为策略 = 目标策略 - Off-Policy：行为策略 ≠ 目标策略 我最大的困惑是了解行为策略在同策略方法中的作用。在on-policy中，比如SARSA，如果智能体从Q表中选择行动，他们不是总是处于剥削状态并且从不探索吗？ 如果情况不是这样，那么什么是在策略 epsilon-greedy 算法与非策略 epsilon-greedy 算法之间的区别？ 我读了两篇不同的文章： 1. https://builtin.com/machine-learning/sarsa 这篇文章说，使用 epsilon-greedy 动作选择是 on-policy，因为当我们利用时，我们会从目标策略中选择一个动作  https://www.baeldung.com/cs/epsilon-贪婪-q-学习这篇文章说，使用epsilon-greedy动作选择是离策略的，因为当我们探索时，我们随机选择一个动作  事实是，这两者文章定义了相同的动作选择函数。那么是哪一个呢？在策略还是离策略？   由   提交/u/bean_217  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1agdtah/understanding_behavior_policy/</guid>
      <pubDate>Thu, 01 Feb 2024 15:38:36 GMT</pubDate>
    </item>
    </channel>
</rss>