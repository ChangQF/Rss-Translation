<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 05 Aug 2024 01:08:52 GMT</lastBuildDate>
    <item>
      <title>比较在线学习/基于模型的算法与离线学习/无模型的算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ek1v0v/comparing_online_learningmodelbased_with_offline/</link>
      <description><![CDATA[嗨，我正在尝试比较基于模型的 RL（即 mcts）和无模型 RL（类似于 ppo）以解决 np-hard 问题。有人对在平等/公平的先决条件下对两种 RL 类型的性能评估/比较有什么建议吗？例如，我如何权衡 mcts 的卓越样本效率与离线学习导致的重新训练需求？应该可以推断出哪一个更适合现实世界的应用。有什么想法吗？此外，我该如何处理超参数？我只想基于性能进行比较，我需要确保超参数不包含在等式中。我将非常感谢任何建议    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ek1v0v/comparing_online_learningmodelbased_with_offline/</guid>
      <pubDate>Sun, 04 Aug 2024 18:16:15 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家连续动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ek1f1d/actor_critic_continuous_action_space/</link>
      <description><![CDATA[我正在尝试在 Keras 中实现 Actor Critic 算法，但无法找到有关如何设置演员网络的全面概述。我已经将 tanh 用于 mu，将 softplus 用于 log_std。有人可以建议这是否正确吗？ def create_actor(self): &quot;&quot;&quot; 创建连续演员神经网络 &quot;&quot;&quot; mu_output = layer.Dense(self.action_shape[0],activation=&#39;tanh&#39;)(self.shared_output) log_std_output = layer.Dense(self.action_shape[0],activation=&#39;softplus&#39;(self.shared_output) self.actor = Model(inputs=self.shared_input,outputs=[mu_output,log_std_output],name=&#39;actor&#39;)     提交人    /u/Electronic_Ad4530   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ek1f1d/actor_critic_continuous_action_space/</guid>
      <pubDate>Sun, 04 Aug 2024 17:57:59 GMT</pubDate>
    </item>
    <item>
      <title>通俗地说，“帕累托”是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ek0rsq/pareto_in_laymans_terms/</link>
      <description><![CDATA[在研究多目标多目标强化学习时，人们会多次遇到“帕累托”这个词。老实说，我甚至无法直观地理解这个意思。现在，我正在阅读一篇论文，它直接指出：  在这项工作中，我们表明，获得多目标机器人控制最佳性能权衡的有效表示是控制策略的帕累托集。我们通过经验表明，使用单个连续策略系列无法有效地表示帕累托集。  对于作者来说，这个意思是理所当然的。 也许你们中有人能帮我吗？ 谢谢    提交人    /u/WilhelmRedemption   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ek0rsq/pareto_in_laymans_terms/</guid>
      <pubDate>Sun, 04 Aug 2024 17:30:49 GMT</pubDate>
    </item>
    <item>
      <title>在峰值 ep_rew_mean 处保存模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejtu0v/save_model_at_peak_ep_rew_mean/</link>
      <description><![CDATA[      嗨， 我有这个简单的自定义环境，我用 SB3 DDPG 和 PPO 进行训练并对它们进行比较。当使用 DDPG 学习时，我观察到 ep_rew_mean 达到峰值水平，然后无法超越它，实际上它无法达到相同的水平并且训练结束。 https://preview.redd.it/nxk1ky332ngd1.png?width=802&amp;format=png&amp;auto=webp&amp;s=0d0a8c98637d8114291745bf95b6ef002e682d19 训练结束时我有了模型。我理解，保存的模型（因此推理性能）将反映次优策略。 我想我可以使用回调来观察最大 ep_rew_mean 达到峰值并仅在那时保存它，而不是在训练结束时保存它。 这是一种常见/好的做法吗？ 或者 ep_rew_mean 没有单调增加，表明我的 Env 或超参数存在问题？ 谢谢亲爱的伙伴们！    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejtu0v/save_model_at_peak_ep_rew_mean/</guid>
      <pubDate>Sun, 04 Aug 2024 12:14:34 GMT</pubDate>
    </item>
    <item>
      <title>这个机器学习库可以让你轻松地训练代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejsz6y/this_machine_learning_library_allows_you_to/</link>
      <description><![CDATA[https://github.com/NoteDance/Note    由   提交  /u/NoteDancing   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejsz6y/this_machine_learning_library_allows_you_to/</guid>
      <pubDate>Sun, 04 Aug 2024 11:23:49 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejkhnp/rl_model_survey/</link>
      <description><![CDATA[我想知道用于少数任务的模型的粗略大小。您在哪个项目中使用了什么模型大小？您的数据规模是多少？一些具有里程碑意义的 RL 里程碑的模型大小是多少？    提交人    /u/SandSnip3r   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejkhnp/rl_model_survey/</guid>
      <pubDate>Sun, 04 Aug 2024 02:21:33 GMT</pubDate>
    </item>
    <item>
      <title>当奖励和下一个状态部分随机时，实现 DQN 的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejghpd/best_way_to_implement_dqn_when_reward_and_next/</link>
      <description><![CDATA[我对机器学习还很陌生，我给自己设定了使用机器学习来解决宝石迷阵的任务，从阅读中可以看出强化学习是最好的方法，而形状为 (8, 8, 6) 的棋盘有 112 个动作，对于 q 表来说太大了。我想我需要使用 DQN 来近似 q 值 我想我已经掌握了基础知识，但我不确定如何定义宝石迷阵中的奖励和下一个状态，例如成功移动时。新的方块会随机添加到棋盘上，因此存在一系列可能的下一个状态。而且由于这些新方块也可以得分，因此也存在一系列可能的分数。 我是否应该假设模型能够在训练期间内部平均这些类似状态动作的不同奖励，或者我应该实施某种措施来解释随机性。也许就像对 10 种不同可能结果的奖励进行平均，但我不确定下一个状态该使用哪一种。  如有任何帮助或指点，不胜感激 此外，这对于模型来说看起来还可以吗  self.conv1 = nn.Conv2d(6, 32, kernel_size=5, padding=2) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) self.conv_v = nn.Conv2d(64, 64, kernel_size=(8, 1), padding=(0, 0)) self.fc1 = nn.Linear(64 * 8 * 8, 512) self.fc2 = nn.Linear(512, num_actions)  我的目标是一次匹配最多 5 个单元，因此最初使用 5x5 卷积。并且由于单元格向下移动，因此模型还需要垂直匹配模式，因此需要 (8,1) 卷积    提交人    /u/Lokipi   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejghpd/best_way_to_implement_dqn_when_reward_and_next/</guid>
      <pubDate>Sat, 03 Aug 2024 23:00:51 GMT</pubDate>
    </item>
    <item>
      <title>更大的 RL 模型总是更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejek2r/are_larger_rl_models_always_better/</link>
      <description><![CDATA[大家好，我目前正在我的自定义 RL 环境中尝试使用 stablebaselines 3 中不同大小的 PPO 模型。我假设较大的模型总是比较小的模型更好地最大化平均奖励。但我的环境/奖励函数似乎恰恰相反。这是正常的还是表示有错误？ 此外，训练/学习时间如何随模型大小而变化？是不是一个大得多的模型需要比小模型长 10 到 100 倍的训练时间，而只需更长的训练时间就可以解决我的问题？ 作为参考，该任务与本文中的情况非常相似 https://github.com/yininghase/multi-agent-control。当我谈论小模型时，我指的是 2 个 64 层的模型，而大模型则是 ~5 个 512 层的模型。 谢谢你的帮助 &lt;3    提交人    /u/Adorable-Spot-7197   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejek2r/are_larger_rl_models_always_better/</guid>
      <pubDate>Sat, 03 Aug 2024 21:32:48 GMT</pubDate>
    </item>
    <item>
      <title>具有视觉输入的多模态模型玩口袋妖怪（尝试世界纪录）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej8rz0/multimodal_model_with_vision_input_plays_pokemon/</link>
      <description><![CDATA[       由    /u/nicimunty  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej8rz0/multimodal_model_with_vision_input_plays_pokemon/</guid>
      <pubDate>Sat, 03 Aug 2024 17:21:37 GMT</pubDate>
    </item>
    <item>
      <title>“NAVIX：使用 JAX 扩展 MiniGrid 环境”，Pignatelli 等人，2024 年（将代理 + 环境打包到 GPU 上以获得 OOM 收益）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej3upp/navix_scaling_minigrid_environments_with_jax/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej3upp/navix_scaling_minigrid_environments_with_jax/</guid>
      <pubDate>Sat, 03 Aug 2024 13:52:09 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Zero V2 为何有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej1m0s/why_does_efficient_zero_v2_work/</link>
      <description><![CDATA[ 如果价值函数知道更好的动作，它不会已经按照这种方式训练策略了吗？ 如果它不知道更好的动作，它会不会错误地评价状态或动作，从而导致在蒙特卡洛树反向传播过程中进行错误评估？     提交人    /u/Automatic-Web8429   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej1m0s/why_does_efficient_zero_v2_work/</guid>
      <pubDate>Sat, 03 Aug 2024 11:57:25 GMT</pubDate>
    </item>
    <item>
      <title>一项新调查——离线策略学习的生成模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eipuq3/a_new_survey_generative_models_for_offline_policy/</link>
      <description><![CDATA[请查看我们新的 TMLR 工作：用于离线策略学习的深度生成模型。本文彻底回顾了深度生成模型在离线强化学习和模仿学习中的应用。我们涵盖的模型包括 VAE、GAN、规范化流、Transformers、扩散模型。    提交人    /u/Ashamed-Put-2344   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eipuq3/a_new_survey_generative_models_for_offline_policy/</guid>
      <pubDate>Sat, 03 Aug 2024 00:30:15 GMT</pubDate>
    </item>
    <item>
      <title>EWRL 的选择性如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</link>
      <description><![CDATA[大家好， 正如标题所说，你们知道 EWRL 研讨会在接受论文方面有多挑剔吗？总的来说，你觉得这个研讨会好吗？一些个人故事将不胜感激。    由   提交  /u/sel20   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</guid>
      <pubDate>Fri, 02 Aug 2024 20:41:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Decision Transformer 在 OfflineRL 顺序决策领域有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</link>
      <description><![CDATA[谢谢。    提交人    /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</guid>
      <pubDate>Fri, 02 Aug 2024 15:02:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么代理没有学会如何到达立方体的位置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</link>
      <description><![CDATA[        提交人    /u/CoolestSlave   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</guid>
      <pubDate>Fri, 02 Aug 2024 12:17:29 GMT</pubDate>
    </item>
    </channel>
</rss>