<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 11 Nov 2024 09:18:41 GMT</lastBuildDate>
    <item>
      <title>ä½¿ç”¨ RL ç© ARPG çš„é—®é¢˜å’Œæƒ³æ³•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</link>
      <description><![CDATA[æˆ‘æ˜¯ä¸€åè®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„æœ€åä¸€å¹´å­¦ç”Ÿï¼Œæˆ‘æƒ³åˆ›å»ºä¸€ä¸ªä»£ç†æ¥ç©æˆ‘è®¡åˆ’åœ¨ Unity ä¸­åˆ¶ä½œçš„ ARPGã€‚ç°åœ¨æˆ‘æƒ³çŸ¥é“è¿™æ˜¯å¦å¯è¡Œï¼Œå› ä¸ºæˆ‘éœ€è¦æ¦‚å¿µéªŒè¯ç›´åˆ° 1 æœˆã€‚ æˆ‘æƒ³æŒ‡å®šä»£ç†æ‰§è¡Œçš„æ“ä½œæ˜¯å¯¼èˆªå…³å¡å¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­æ€æ­»ä¸€äº›æ•Œäººï¼Œå…³å¡åº”è¯¥å…·æœ‰ä¸€å®šçš„å¤æ‚æ€§ï¼Œå®ƒä¸ä¼šæ˜¯ä¸€æ¡ç›´çº¿ï¼Œä½†ä¹Ÿä¸ä¼šæ˜¯ä¸€ä¸ªè¶…çº§å¤æ‚çš„è¿·å®«ã€‚æœ€å¥½å®ƒä¼šæœ‰ä¸€äº›å‘æ•£çš„è·¯å¾„ã€‚ ç°åœ¨ï¼Œå¦‚æœæˆ‘åœ¨ Unity ä¸­åˆ¶ä½œæ¸¸æˆï¼Œå¦‚æœæˆ‘å½“å‰çš„è®¾ç½®ç”± r5 5600 å’Œ rtx 4070 12GB vram ç»„æˆï¼Œæ˜¯å¦å¯ä»¥è®­ç»ƒä»£ç†ï¼Ÿå®ƒä¸ä¼šå¾ˆå¿«ï¼Œä½†æˆ‘å¯ä»¥è®©ç”µè„‘å…¨å¤©å€™è¿è¡Œã€‚ æå‰æ„Ÿè°¢æ‚¨çš„å›å¤ã€‚    æäº¤äºº    /u/Marmotacuparlacur   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</guid>
      <pubDate>Mon, 11 Nov 2024 08:42:01 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨ QMIX ä¸­ï¼Œæ¯ä¸ªä»£ç†åœ¨åƒ SMAC è¿™æ ·çš„ç¯å¢ƒä¸­æ˜¯å¦è¢«å¿½ç•¥ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1goal6i/in_qmix_is_peragent_done_ignored_in_an/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼æˆ‘æƒ³ç†è§£ä¸€ä¸ªç›¸å½“ç®€å•çš„é—®é¢˜ï¼Œæˆ‘æ­£åœ¨æŸ¥çœ‹ JaxMARL QMIX ä»£ç ï¼Œæˆ‘æ³¨æ„åˆ°å³ä½¿æˆ‘ä»¬ä½¿ç”¨æ¯ä¸ªä»£ç†çš„å•ç‹¬å®ŒæˆçŠ¶æ€æ¥é‡ç½®éšè—çŠ¶æ€ï¼Œè¿™äº›å®ŒæˆçŠ¶æ€ä¹Ÿä¸ä¼šåœ¨è®¡ç®— q å‡½æ•°ç›®æ ‡æ—¶ä½¿ç”¨ï¼Œè€Œåªæ˜¯æ•´ä½“ç¯å¢ƒå®Œæˆï¼šhttps://github.com/FLAIROx/JaxMARL/blob/main/baselines/QLearning/qmix_rnn.py#L477 æœ‰äººèƒ½è§£é‡Šä¸€ä¸‹è¿™æ˜¯ä¸ºä»€ä¹ˆå—ï¼Ÿæ˜¯ä¸æ˜¯å› ä¸ºæˆ‘ä»¬å·²ç»é€šè¿‡è€ƒè™‘å¯ç”¨æ“ä½œä¸ä¸å¯ç”¨æ“ä½œéšå¼åœ°å±è”½äº† q å€¼ï¼Œå½“ä»£ç†åœ¨æœ¬åœ°å®Œæˆä½†ç¯å¢ƒæœ¬èº«å°šæœªç»ˆæ­¢æ—¶ï¼Œè¿™äº›æ“ä½œå°†å‘ç”Ÿå˜åŒ–ï¼Ÿ    æäº¤äºº    /u/1cedrake   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1goal6i/in_qmix_is_peragent_done_ignored_in_an/</guid>
      <pubDate>Sun, 10 Nov 2024 20:52:07 GMT</pubDate>
    </item>
    <item>
      <title>Decisions & Dragonsï¼šä¸€ä¸ªå›ç­”å¸¸è§ RL é—®é¢˜çš„ç½‘ç«™</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</link>
      <description><![CDATA[å¤šå¹´æ¥ï¼Œæˆ‘åœ¨å„ç§ç¤¾äº¤åª’ä½“å¹³å°ä¸Šå›ç­”äº†å¾ˆå¤šå¼ºåŒ–å­¦ä¹ é—®é¢˜ã€‚æˆ‘å†³å®šæ˜¯æ—¶å€™åœ¨è‡ªå·±çš„ç½‘ç«™ä¸Šæ”¶é›†å’Œæ‰©å±•å®ƒä»¬äº†ï¼Œæˆ‘æŠŠè¿™ä¸ªç½‘ç«™å«åš Decisions and Dragonsã€‚ è™½ç„¶è¿™ä¸ªç½‘ç«™æ˜¯é¢å‘åˆå­¦è€…çš„ï¼Œä½†æˆ‘è®¤ä¸ºå®ƒå¯¹æ›´é«˜çº§çš„ä»ä¸šè€…æœ‰å¸®åŠ©ï¼Œå¯ä»¥ä½œä¸ºå¯¹æ ¸å¿ƒæ¦‚å¿µçš„å¤ä¹ ã€‚å®ƒæ¨å‡ºäº† 8 ä¸ªæ·±å…¥çš„ç­”æ¡ˆï¼Œæˆ‘å°†åœ¨æœªæ¥æ·»åŠ å®ƒã€‚ æˆ‘ä¸ç¡®å®šå®ƒä¼šæœ‰å¤šå—æ¬¢è¿ï¼Œä½†æˆ‘å¸Œæœ›å®ƒè‡³å°‘èƒ½å¸®åŠ©ä½ ä»¬ä¸­çš„ä¸€äº›äººï¼ https://www.decisionsanddragons.com/    æäº¤äºº    /u/Born_Preparation_308   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</guid>
      <pubDate>Sun, 10 Nov 2024 16:37:01 GMT</pubDate>
    </item>
    <item>
      <title>è‡ªå®šä¹‰å¡ç‰Œæ¸¸æˆç¯å¢ƒå»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go23os/environment_recommendations_for_custom_card_game/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ç”¨ Python å®ç°ä¸€ä¸ªçº¸ç‰Œæ¸¸æˆï¼ˆè‡ªå®šä¹‰æ¸¸æˆï¼Œè‡ªå®šä¹‰è§„åˆ™ï¼‰ã€‚å®ƒæ˜¯åŒäººã€éé›¶å’Œã€ä¸å®Œç¾ä¿¡æ¯ã€‚ç„¶åæˆ‘æƒ³åˆ›å»ºå‡ ä¸ªç®€å•çš„ä»£ç†ï¼Œç„¶åé€šè¿‡å¯¹æ›´ç®€å•çš„ä»£ç†è¿›è¡Œè®­ç»ƒæ¥è®­ç»ƒ RL ä»£ç†ã€‚ ä¸ºæ­¤ï¼Œæˆ‘éœ€è¦å…ˆç”¨ Python é‡æ–°åˆ›å»ºæ¸¸æˆã€‚æˆ‘åº”è¯¥ä»å¤´å¼€å§‹ç¼–å†™ä»£ç ï¼ˆè§„åˆ™ç›¸å½“ç®€å•ï¼‰è¿˜æ˜¯æœ‰åƒ Gymnasium è¿™æ ·çš„åº“å¯¹çº¸ç‰Œæ¸¸æˆæœ‰å¾ˆå¥½çš„æ”¯æŒï¼Ÿæ®æˆ‘æ‰€çŸ¥ï¼ŒGymnasium â€œä»…â€é€‚ç”¨äºè§†é¢‘æ¸¸æˆï¼ˆAtari ç­‰ï¼‰ï¼Œè€Œä¸å¤ªé€‚åˆçº¸ç‰Œæ¸¸æˆã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å·²ç»èƒ½å¤Ÿæ‰¾åˆ° Gymnasium å’Œ OpenSpielã€‚    æäº¤äºº    /u/n0stalghia   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go23os/environment_recommendations_for_custom_card_game/</guid>
      <pubDate>Sun, 10 Nov 2024 14:41:09 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ Q-Learning å¸®åŠ©æ— äººæœºè‡ªä¸»ç©¿è¶ŠæœªçŸ¥ç¯å¢ƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</link>
      <description><![CDATA[æˆ‘ä»¬çš„ä»»åŠ¡æ˜¯ä½¿ç”¨æ— äººæœºè¦†ç›–æœªçŸ¥åŒºåŸŸå¹¶åœ¨æœç´¢è¿‡ç¨‹ä¸­ç¡®å®šå…³é”®ç‚¹ã€‚æˆ‘ä»¬å‡è®¾äº†ä¸€ç§åœºæ™¯ï¼Œå³å¿…é¡»è¦†ç›–å—ç¾åœ°åŒºï¼Œå¹¶å¸Œæœ›ç¡®å®šå¹¸å­˜è€…ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å°†é—®é¢˜æŠ½è±¡ä¸ºä½¿ç”¨ 2D ç½‘æ ¼è¡¨ç¤ºæœç´¢åŒºåŸŸï¼Œç„¶åå¯è§†åŒ–æ— äººæœºåœ¨å…¶ä¸­ç§»åŠ¨çš„æƒ…å†µã€‚ æˆ‘ä»¬æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ–°æ‰‹ï¼Œä¸æ¸…æ¥šå¦‚ä½•åœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ q-learningã€‚å½“æ‚¨è¯•å›¾ä¸€æ¬¡æ€§è¦†ç›–ä¸€ä¸ªåŒºåŸŸå¹¶ä¸”æ‚¨ä¸çŸ¥é“ç¯å¢ƒæ˜¯ä»€ä¹ˆæ ·å­ï¼ŒåªçŸ¥é“è¦æœç´¢çš„åŒºåŸŸçš„è¾¹ç•Œæ—¶ï¼Œq-learning æ˜¯å¦æœ‰æ•ˆï¼Ÿå½“å¹¸å­˜è€…å¾ˆå¯èƒ½åªæ˜¯éšæœºåˆ†å¸ƒæ—¶ï¼Œå®ƒç”šè‡³å¯ä»¥å­¦ä¹ ä»€ä¹ˆæ ·çš„æ¨¡å¼ï¼Ÿä»»ä½•è§è§£/æŒ‡å¯¼éƒ½å°†ä¸èƒœæ„Ÿæ¿€ã€‚    æäº¤äºº    /u/naepalm7   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</guid>
      <pubDate>Sun, 10 Nov 2024 14:10:02 GMT</pubDate>
    </item>
    <item>
      <title>PPO åœ¨ AirSim ä¸­ä¸èµ·ä½œç”¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1f9v/ppo_doesnt_work_in_airsim/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨ç ”ç©¶æˆ‘çš„è®ºæ–‡é¡¹ç›®ï¼Œåœ¨ AirSim çš„é‚»é‡Œç¯å¢ƒä¸­å®æ–½ PPO ä»¥è¿›è¡Œæ±½è½¦é©¾é©¶ã€‚ä»£ç†éœ€è¦å­¦ä¼šå¾ˆå¥½åœ°é©¾é©¶ï¼Œæ‰€ä»¥æˆ‘æ­£åœ¨å¤„ç†è¿™ç§æƒ…å†µï¼šæˆ‘å¿…é¡»åœ¨è¿ç»­ç©ºé—´ä¸­ä¼°è®¡ 1 ä¸ªåŠ¨ä½œï¼ˆå›ºå®šé€Ÿåº¦ï¼‰ï¼ˆåŠ¨ä½œèŒƒå›´ä» -1 åˆ° 1ï¼Œæˆ‘ä½¿ç”¨æ­£æ€åˆ†å¸ƒæ¥æŠ½æ ·æœ‰æ•ˆåŠ¨ä½œï¼‰ã€‚æˆ‘å·²ç»æµ‹è¯•äº†å„ç§æ–¹æ³•å¥½å‡ å¤©äº†ï¼Œä½†æ˜¯ä»£ç†ä»ç„¶æ— æ³•æœ‰æ•ˆåœ°å­¦ä¹ ã€‚ å¯¹äº PPO ç®—æ³•ï¼Œæˆ‘ä»å‡ ä¸ª GitHub ä»£ç ä¸­è·å¾—äº†çµæ„Ÿï¼Œå¯¹äºç½‘ç»œï¼Œæˆ‘ä½¿ç”¨äº†ä»¥ä¸‹ç»“æ„ï¼š self.conv1 = Conv2D(32, (8, 8), strides=4, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.conv2 = Conv2D(64, (4, 4), strides=2, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.conv3 = Conv2D(64, (3, 3), strides=1, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.flatten = Flatten()  self.ad1 = Dense(512,activation=&#39;relu&#39;)  self.ad2_mean = Dense(1,activation=&#39;tanh&#39;) self.val = Dense(1)  ç›®å‰ï¼Œæˆ‘ä¿æŒ std å›ºå®šï¼Œå› ä¸ºå½“æˆ‘å°è¯•ä¼°è®¡å®ƒæ—¶ï¼Œå®ƒä¼šçˆ†ç‚¸å¼å¢é•¿åˆ°éå¸¸é«˜çš„å€¼ã€‚ æˆ‘æƒ³é—®ä¸€ä¸‹æ˜¯å¦æœ‰äººä»äº‹è¿‡ç±»ä¼¼çš„é¡¹ç›®å¹¶èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®æˆ–æŒ‡ç‚¹ï¼    æäº¤äºº    /u/MonfoTibetano   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1f9v/ppo_doesnt_work_in_airsim/</guid>
      <pubDate>Sun, 10 Nov 2024 14:08:13 GMT</pubDate>
    </item>
    <item>
      <title>RMSprop æ–¹æ³•åº”ç”¨äº Q å­¦ä¹ ï¼Œå®ç°è‡ªé€‚åº”åŠ¨æ€å­¦ä¹ ç‡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go0h35/rmsprop_approach_applied_to_qlearning_for/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å®æ–½å— RMSprop å¯å‘çš„åŠ¨æ€å­¦ä¹ ç‡ Q å­¦ä¹ ï¼Œéµå¾ªæˆ‘åœ¨ä¸€ç¯‡æ–‡ç« ä¸­æ‰¾åˆ°çš„æ–¹æ³•ã€‚ç›®æ ‡æ˜¯è®©å­¦ä¹ ç‡æ ¹æ®æ—¶é—´å·® (TD) è¯¯å·®çš„å¤§å°éšæ—¶é—´è¿›è¡Œè°ƒæ•´ã€‚ä½†æ˜¯ï¼Œæˆ‘é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼Œæ¢¯åº¦ä¼¼ä¹éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ï¼Œè€Œç†æƒ³æƒ…å†µä¸‹ï¼Œéšç€ä»£ç†å¯¹ç¯å¢ƒçš„äº†è§£è¶Šæ¥è¶Šå¤šï¼Œæ¢¯åº¦åº”è¯¥ä¼šå‡å°ã€‚ å…·ä½“æ¥è¯´ï¼š æˆ‘é¢„è®¡æ¢¯åº¦ï¼ˆTD è¯¯å·®ï¼‰ä¼šéšç€ Q å€¼çš„æ”¶æ•›è€Œé€æ¸å‡å°ï¼Œä½†ç›¸åï¼Œå®ƒä¼¼ä¹åœ¨å¢é•¿ã€‚å› æ­¤ï¼Œæˆ‘çš„å­¦ä¹ ç‡ï¼ˆä» 0.001 å¼€å§‹ï¼‰å¹¶æ²¡æœ‰åƒé¢„æœŸçš„é‚£æ ·éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ï¼Œè€Œæ˜¯ä½äºé¢„æœŸç”šè‡³åœ¨ä¸‹é™ã€‚ä¸‹é¢æ˜¯æˆ‘æ­£åœ¨ä½¿ç”¨çš„ Q å­¦ä¹ æ›´æ–°å‡½æ•°ï¼š**python å¤åˆ¶ä»£ç **def update_q_table(self, state, action, reward, next_state): &quot;&quot;&quot;ä½¿ç”¨ Q å­¦ä¹ æ›´æ–°è§„åˆ™æ›´æ–° Q è¡¨ã€‚&quot;&quot;&quot; def update_q_table(self, state, action, reward, next_state): best_next_action = np.argmax(self.q_table[next_state, :]) td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action] td_error = td_target - self.q_table[state, action] # æ›´æ–° RMSprop çš„å¹³æ–¹æ¢¯åº¦ç§»åŠ¨å¹³å‡å€¼ E[g^2] self.gradient_Q[state, action] = ( self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2) ) self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon) self.learning_rate_history.append(self.learning_rate) # ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡å’Œ TD è¯¯å·®æ›´æ–° Q å€¼ self.q_table[state, action] += self.learning_rate * td_error # å­˜å‚¨ E[g^2] å€¼ç”¨äºè·Ÿè¸ª self.gradient_history.append(self.gradient_Q[state, action])  æˆ‘æ­£åœ¨å®ç°å— RMSprop å¯å‘çš„åŠ¨æ€å­¦ä¹ ç‡ Q å­¦ä¹ ï¼Œéµå¾ªæˆ‘åœ¨ä¸€ç¯‡æ–‡ç« ä¸­æ‰¾åˆ°çš„æ–¹æ³•ã€‚ç›®æ ‡æ˜¯æ ¹æ®æ—¶é—´å·®å¼‚ (TD) è¯¯å·®çš„å¤§å°éšæ—¶é—´è°ƒæ•´å­¦ä¹ ç‡ã€‚ä½†æ˜¯ï¼Œæˆ‘é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ï¼Œæ¢¯åº¦ä¼¼ä¹éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ï¼Œè€Œç†æƒ³æƒ…å†µä¸‹ï¼Œéšç€ä»£ç†å¯¹ç¯å¢ƒçš„äº†è§£è¶Šæ¥è¶Šå¤šï¼Œæ¢¯åº¦åº”è¯¥ä¼šå‡å°‘ã€‚ å…·ä½“æ¥è¯´ï¼š æˆ‘é¢„è®¡æ¢¯åº¦ï¼ˆTD è¯¯å·®ï¼‰ä¼šéšç€ Q å€¼çš„æ”¶æ•›è€Œé€æ¸å‡å°ï¼Œä½†äº‹å®å¹¶éå¦‚æ­¤ï¼Œå®ƒä¼¼ä¹åœ¨å¢é•¿ã€‚å› æ­¤ï¼Œæˆ‘çš„å­¦ä¹ ç‡ï¼ˆä» 0.001 å¼€å§‹ï¼‰å¹¶æ²¡æœ‰åƒé¢„æœŸçš„é‚£æ ·éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ï¼Œè€Œæ˜¯ä½äºé¢„æœŸç”šè‡³ä¸‹é™ã€‚    æäº¤äºº    /u/Busy-Acadia5601   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go0h35/rmsprop_approach_applied_to_qlearning_for/</guid>
      <pubDate>Sun, 10 Nov 2024 13:18:31 GMT</pubDate>
    </item>
    <item>
      <title>PPO å’Œæœ€åè§‚å¯Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go00r6/ppo_and_last_observations/</link>
      <description><![CDATA[åœ¨å¸¸è§çš„ Python æ¼”å‘˜-è¯„è®ºå®¶ä»£ç†å®ç°ä¸­ï¼Œä¾‹å¦‚ stable_baselines3 åº“ä¸­çš„å®ç°ï¼ŒPPO æ˜¯å¦çœŸçš„ä½¿ç”¨å®ƒä»ç»ˆç«¯çŠ¶æ€æ”¶åˆ°çš„æœ€åä¸€ä¸ªè§‚å¯Ÿå€¼ï¼Ÿä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª PPO ä»£ç†ï¼Œæ— è®ºå½“å‰æ“ä½œå¦‚ä½•ï¼Œå®ƒéƒ½ä¼šåœ¨ n æ­¥åç»ˆæ­¢ MDP æˆ– POMDPï¼ˆè¿™æ„å‘³ç€ç»ˆç«¯çŠ¶æ€ä»…å–å†³äºæ­¥æ•°ï¼Œè€Œä¸å–å†³äºæ“ä½œé€‰æ‹©ï¼‰ï¼ŒPPO æ˜¯å¦ä»ä¼šåœ¨è®¡ç®—ä¸­ä½¿ç”¨æœ€åä¸€ä¸ªè§‚å¯Ÿå€¼ï¼Ÿ å¦‚æœ n=1ï¼ŒPPO æ˜¯å¦æœ¬è´¨ä¸Šåƒä¸€ä¸ªæƒ…å¢ƒè€è™æœºï¼Œå› ä¸ºå®ƒä»è§‚å¯Ÿå¼€å§‹ï¼Œå¹¶åœ¨å•æ­¥æƒ…èŠ‚ä¸­ç«‹å³ä»¥å¥–åŠ±ç»“æŸï¼Ÿ    æäº¤äºº    /u/Krnl_plt   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go00r6/ppo_and_last_observations/</guid>
      <pubDate>Sun, 10 Nov 2024 12:53:26 GMT</pubDate>
    </item>
    <item>
      <title>è¯­éŸ³åˆæˆä¸­çš„å¾®è°ƒä¸è¿ç§»å­¦ä¹  - INGOAMPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnz44s/finetuning_vs_transfer_learning_in_voice/</link>
      <description><![CDATA[        ç”±    /u/Potential_Arrival326  æäº¤  [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnz44s/finetuning_vs_transfer_learning_in_voice/</guid>
      <pubDate>Sun, 10 Nov 2024 11:59:21 GMT</pubDate>
    </item>
    <item>
      <title>è¯­éŸ³åˆæˆä¸­çš„å¾®è°ƒä¸è¿ç§»å­¦ä¹  - INGOAMPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnz3ej/finetuning_vs_transfer_learning_in_voice/</link>
      <description><![CDATA[        ç”±    /u/Potential_Arrival326  æäº¤  [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnz3ej/finetuning_vs_transfer_learning_in_voice/</guid>
      <pubDate>Sun, 10 Nov 2024 11:57:59 GMT</pubDate>
    </item>
    <item>
      <title>åˆšåˆšå‘ç°äº†å¾ˆæ£’çš„ Python å¼ºåŒ–å­¦ä¹ åº“ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gntun0/just_discovered_great_reinforcement_learning/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ æˆ‘æœ€è¿‘å¶ç„¶å‘ç°äº†ä¸€ä¸ªåä¸º DeepRL (deeprlearn) çš„å¼ºåŒ–å­¦ä¹ åº“ï¼Œæˆ‘è®¤ä¸ºå€¼å¾—ä¸€è¯•ã€‚å®ƒä¸“ä¸ºä¸­å°å‹ RL é¡¹ç›®è€Œè®¾è®¡ï¼Œå¹¶æä¾›ä¸€äº›ç®€æ´çš„åŠŸèƒ½ï¼Œä½¿å…¶æ—¢çµæ´»åˆå¯æ‰©å±•ã€‚å®ƒåŸºäº PyTorchï¼Œå› æ­¤å¦‚æœæ‚¨å·²ç»å°† PyTorch ç”¨äºå…¶ä»–é¡¹ç›®ï¼Œé‚£ä¹ˆè¿™åº”è¯¥ä¼šéå¸¸ç›´è§‚ã€‚ ä»¥ä¸‹æ˜¯å®ƒæä¾›çš„ä¸€äº›äº®ç‚¹ï¼š  åŠ¨æ€è§„åˆ’ç®—æ³•ï¼šå¼€ç®±å³ç”¨åœ°å®ç°ä»·å€¼è¿­ä»£å’Œç­–ç•¥è¿­ä»£ä»£ç†ã€‚ å¥–åŠ±å¡‘é€ ï¼šå¸¦æœ‰å†…ç½®ç­–ç•¥ï¼Œå¦‚åŸºäºæ½œåŠ›çš„å¡‘é€ ã€åŸºäºè·ç¦»çš„å¡‘é€ ç­‰ã€‚éå¸¸é€‚åˆåƒ FrozenLake æˆ– MountainCar è¿™æ ·å¥–åŠ±ç¨€ç–çš„ç¯å¢ƒã€‚ å‡½æ•°è¿‘ä¼¼ï¼šåŒ…æ‹¬ç”¨äº RBF æ ¸ã€å¤šé¡¹å¼ç‰¹å¾ç”šè‡³ç¥ç»ç½‘ç»œçš„å·¥å…·ï¼ˆå¦‚æœæ‚¨æƒ³èµ°è¿™æ¡è·¯ï¼‰ã€‚ Gymnasium é›†æˆï¼šä¸ Gym ç¯å¢ƒæ— ç¼åä½œã€‚ è¿›åº¦è·Ÿè¸ªï¼šä¸€ç§æ–¹ä¾¿çš„è¯¦ç»†æ¨¡å¼ï¼Œç”¨äºç›‘æ§è®­ç»ƒå¥–åŠ±ã€æ­¥éª¤å’Œæ¢ç´¢ç‡ã€‚ æ¨¡å‹ä¿å­˜/åŠ è½½ï¼šå¯ä»¥éå¸¸è½»æ¾åœ°ä¿å­˜å’Œé‡ç”¨ç»è¿‡è®­ç»ƒçš„ä»£ç†ã€‚  æˆ‘ä¸€ç›´åœ¨å°è¯•å®ƒï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å·²ç»åœ¨ FrozenLake-v1ï¼ˆé˜²æ»‘ï¼‰ä¸Šè®­ç»ƒäº†ä¸€ä¸ªä»·å€¼è¿­ä»£ä»£ç†ã€‚å®ƒå‡ºå¥‡çš„ç®€å•ï¼Œè¯¦ç»†æ¨¡å¼ä½¿ç›‘æ§è¿›åº¦å˜å¾—å®¹æ˜“ã€‚ è¯¥åº“ä»åœ¨ä¸æ–­å‘å±•ï¼Œä½†å®ƒå¯¹è´¡çŒ®æŒå¼€æ”¾æ€åº¦ã€‚å¦‚æœæ‚¨æ­£åœ¨å¯»æ‰¾ç”¨äº RL å®éªŒçš„è½»é‡çº§ä½†åŠŸèƒ½å¼ºå¤§çš„ä¸œè¥¿ï¼Œé‚£ä¹ˆè¿™å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼ GitHub Repoï¼šdeeprl å¦‚æœæ‚¨å°è¯•è¿‡æˆ–æœ‰ä»»ä½•åé¦ˆï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚çœ‹åˆ°æ›´å¤šäººå°è¯•è¿™ä¸ªä¼šå¾ˆæ£’ï¼    æäº¤äºº    /u/Traditional-Rate8550   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gntun0/just_discovered_great_reinforcement_learning/</guid>
      <pubDate>Sun, 10 Nov 2024 05:46:16 GMT</pubDate>
    </item>
    <item>
      <title>å®‰å…¨ã€æ— å¹»è§‰çš„ AI ç¼–ç ææ¡ˆ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnsq65/a_proposal_for_safe_and_hallucinationfree_coding/</link>
      <description><![CDATA[æˆ‘å†™äº†ä¸€ç¯‡è®ºæ–‡ã€Šå®‰å…¨ä¸”æ— å¹»è§‰çš„ AI ç¼–ç ææ¡ˆã€‹(https://gasstationmanager.github.io/ai/2024/11/04/a-proposal.html)ï¼Œå…¶ä¸­æˆ‘æè®®å°±ä¸€é¡¹ç ”ç©¶è®®ç¨‹å¼€å±•å¼€æºåˆä½œï¼Œæˆ‘ç›¸ä¿¡è¿™æœ€ç»ˆå°†å¯¼è‡´ç¼–ç å…·æœ‰è¶…äººçº§èƒ½åŠ›ã€æ— å¹»è§‰ä¸”å®‰å…¨çš„ AIã€‚ å¼ºåŒ–å­¦ä¹ ï¼Œå°¤å…¶æ˜¯ AlphaZeroï¼Œæ˜¯æˆ‘æå‡ºçš„è§£å†³æ–¹æ¡ˆçš„ä¸€éƒ¨åˆ†ã€‚ä½† AlphaZero é€šå¸¸åœ¨å®¹æ˜“è·å¾—åŸºæœ¬äº‹å®çš„é¢†åŸŸæ•ˆæœå¾ˆå¥½ï¼Œæ¯”å¦‚å›´æ£‹å’Œå›½é™…è±¡æ£‹â€¦â€¦æˆ‘æå‡ºäº†ä¸€ç§å°†ä»£ç ç”Ÿæˆé—®é¢˜è¡¨è¿°ä¸ºå¯ä»¥æ ¹æ®åŸºæœ¬äº‹å®éªŒè¯å€™é€‰è§£å†³æ–¹æ¡ˆçš„æ–¹æ³•ã€‚  æ¬¢è¿å‘è¡¨è¯„è®ºï¼å¦‚æœæ‚¨æœ‰å…´è¶£æ¢ç´¢å¼ºåŒ–å­¦ä¹ æˆ–è¯¥è®¡åˆ’çš„å…¶ä»–æ–¹é¢çš„æƒ³æ³•ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼     æäº¤äºº    /u/Admirable_Sorbet_544   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnsq65/a_proposal_for_safe_and_hallucinationfree_coding/</guid>
      <pubDate>Sun, 10 Nov 2024 04:37:28 GMT</pubDate>
    </item>
    <item>
      <title>ä¸€äº› RL ç®—æ³•çš„åˆ†ç±»</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnd86b/classification_of_some_rl_algorithms/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnd86b/classification_of_some_rl_algorithms/</guid>
      <pubDate>Sat, 09 Nov 2024 16:01:07 GMT</pubDate>
    </item>
    <item>
      <title>é€‚ç”¨äº 2 ä¸ªä»¥ä¸Šä»£ç†çš„ AI æ‰‘å…‹å¥èº«æˆ¿ç¯å¢ƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn9nux/ai_poker_gym_environment_for_more_than_2_agents/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä¸€åè®¡ç®—æœºç§‘å­¦ä¸“ä¸šçš„å­¦ç”Ÿï¼Œâ€‹â€‹æˆ‘æƒ³ä¸ºæˆ‘çš„ AI è¯¾ç¨‹æœŸæœ«é¡¹ç›®åšä¸€ä¸ª AI æ‰‘å…‹é”¦æ ‡èµ›ã€‚æˆ‘çš„æƒ³æ³•æ˜¯è®© 4/5 ä¸ªä¸åŒçš„ä»£ç†éƒ½ä½¿ç”¨ä¸åŒçš„ RL ç®—æ³•è¿›è¡Œè®­ç»ƒï¼Œè®©å®ƒä»¬äº’ç›¸ç©æ‰‘å…‹ï¼Œçœ‹çœ‹è°èµ¢ã€‚æˆ‘æ‰¾åˆ°äº†å‡ ä¸ªä¸åŒçš„æ‰‘å…‹ç¯å¢ƒï¼Œä½†å®ƒä»¬éƒ½æ˜¯é’ˆå¯¹ 2 ä¸ªä»£ç†çš„ã€‚æœ‰äººçŸ¥é“ä»»ä½•èƒ½å¤Ÿä¸ &gt;3 ä¸ªä»£ç†ä¸€èµ·å·¥ä½œçš„ç¯å¢ƒå—ï¼Ÿä»»ä½•å¸®åŠ©æˆ–å»ºè®®å¦‚ä½•ä½¿æˆ‘çš„é¡¹ç›®æ›´å¥½éƒ½å°†ä¸èƒœæ„Ÿæ¿€ã€‚    æäº¤äºº    /u/Livid-Ant3549   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn9nux/ai_poker_gym_environment_for_more_than_2_agents/</guid>
      <pubDate>Sat, 09 Nov 2024 13:03:55 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘æ˜¯å¦åº”è¯¥å…ˆå°†æˆ‘çš„ RL è®ºæ–‡æäº¤ç»™ arXiv ä»¥ä¿æŠ¤æ–°é¢–æ€§ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn6y6f/should_i_submit_my_rl_paper_to_arxiv_first_to/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ æˆ‘ä¸€ç›´åœ¨åŠªåŠ›æ”¹è¿›å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶ä¸”å–å¾—äº†ä¸€äº›ä¸é”™çš„æˆæœï¼Œå¾ˆé«˜å…´ä¸å¤§å®¶åˆ†äº«ã€‚åœ¨å‡†å¤‡æ’°å†™è®ºæ–‡æ—¶ï¼Œæˆ‘æƒ³çŸ¥é“åœ¨å°†å…¶å‘é€åˆ°æœºå™¨å­¦ä¹ æœŸåˆŠä¹‹å‰ï¼Œæ˜¯å¦æœ€å¥½å…ˆå°†å…¶æäº¤ç»™ arXivã€‚æˆ‘ä¸»è¦å…³å¿ƒçš„æ˜¯ç¡®ä¿æˆ‘çš„ç ”ç©¶çš„æ–°é¢–æ€§å¾—åˆ°ä¿æŠ¤ï¼Œå› ä¸ºæˆ‘å¬è¯´åœ¨ arXiv ä¸Šå‘è¡¨æ–‡ç« æœ‰åŠ©äºç¡®å®šè´¡çŒ®çš„æ—¶é—´æˆ³ã€‚ æ‰€ä»¥ï¼Œæˆ‘å¾ˆæƒ³çŸ¥é“ï¼š  åœ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸­ï¼Œå…ˆåœ¨ arXiv ä¸Šå‘è¡¨æ–‡ç« ï¼Œç„¶åå†æäº¤ç»™æœŸåˆŠï¼Œè¿™æ˜¯å¦æ˜¯ä¸€ç§å¸¸è§çš„æƒ¯ä¾‹ï¼Ÿ åœ¨ arXiv ä¸Šå‘è¡¨æ–‡ç« çœŸçš„æœ‰åŠ©äºä¿æŠ¤ç ”ç©¶çš„æ–°é¢–æ€§å—ï¼Ÿ  åœ¨å‘æœŸåˆŠæäº¤æ–‡ç« ä¹‹å‰ï¼Œæˆ‘æœ‰ä»€ä¹ˆç†ç”±é¿å…åœ¨ arXiv ä¸Šå‘è¡¨æ–‡ç« å—ï¼Ÿ  ä»»ä½•ç»å†è¿‡è¿™ä¸ªè¿‡ç¨‹æˆ–æœ‰ RL å‡ºç‰ˆç»éªŒçš„äººçš„å»ºè®®éƒ½ä¼šéå¸¸æœ‰å¸®åŠ©ï¼æå‰è°¢è°¢äº†ï¼ğŸ˜Š    æäº¤äºº    /u/Tonight223   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn6y6f/should_i_submit_my_rl_paper_to_arxiv_first_to/</guid>
      <pubDate>Sat, 09 Nov 2024 10:05:06 GMT</pubDate>
    </item>
    </channel>
</rss>