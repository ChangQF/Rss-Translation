<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 10 Sep 2024 09:18:05 GMT</lastBuildDate>
    <item>
      <title>如何在不违反马尔可夫特性的情况下处理强化学习中的延迟奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd9s9i/how_to_handle_delayed_rewards_in_rl_without/</link>
      <description><![CDATA[大家好，我正在研究一个强化学习问题，其中代理控制交通信号以最小化队列长度和碰撞风险。奖励函数有两个组成部分：  即时奖励：每个时间步骤通过交叉路口的车辆数量。 延迟奖励：只有在完成一个完整的信号周期（4 个阶段）后才能计算的碰撞风险分数。计算出这个碰撞风险分数后，我需要将其分配到前面的步骤中。  奖励=−(队列长度+碰撞风险) 挑战如下：  在每个步骤（操作：延长当前阶段或更改阶段）中，我可以根据通过的车辆数量立即计算奖励（例如，步骤 1：队列长度 = 4，步骤 2：队列长度 = 6，等等）。 但是，碰撞风险评分会延迟，并在整个信号周期之后计算。然后，我想将这个碰撞风险奖励分配到周期的前面几个步骤中（例如，步骤 1 获得一部分碰撞风险）。  示例：  步骤 1：队列长度 = 4，尚无碰撞风险 步骤 2：队列长度 = 6，尚无碰撞风险 步骤 3：队列长度 = 2，尚无碰撞风险 步骤 4：队列长度 = 5，碰撞风险 = 4（仅在此步骤之后已知） 信号周期结束后，我将碰撞风险分数向后分配到前面的步骤中（例如，步骤 1 奖励 = -(4+1)，步骤 2 奖励 = -(6+1)，等等）  问题：  我能否均匀地在不违反马尔可夫特性的情况下将碰撞风险向后分布到各个步骤中（因为奖励通常仅根据当前状态和动作来计算）？ 如果不是，如何在 RL 中正确处理这种延迟奖励，同时保留马尔可夫特性？是否有任何替代技术可以帮助，例如部分可观察的 MDP、N 步 TD 或分层 RL？     提交人    /u/muttahirulislam   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd9s9i/how_to_handle_delayed_rewards_in_rl_without/</guid>
      <pubDate>Tue, 10 Sep 2024 04:41:30 GMT</pubDate>
    </item>
    <item>
      <title>最佳强化学习和人工智能代理资源？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd9r00/best_reinforcement_learning_and_ai_agents/</link>
      <description><![CDATA[我在本科期间曾学习过机器学习和深度学习（监督学习）。 今年我已经毕业了，现在我对 RL 和 AI 代理产生了兴趣。我可以从哪些资源（最好是最新的）中学习？我还希望能够在学习后建立项目，因此如果资源还包含实践知识，那就最好了    提交人    /u/CS_UGRAD24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd9r00/best_reinforcement_learning_and_ai_agents/</guid>
      <pubDate>Tue, 10 Sep 2024 04:39:19 GMT</pubDate>
    </item>
    <item>
      <title>“Carpentopod：一个行走桌项目”（进化出更平滑的滚动腿）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd2h89/carpentopod_a_walking_table_project_evolving/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd2h89/carpentopod_a_walking_table_project_evolving/</guid>
      <pubDate>Mon, 09 Sep 2024 22:29:11 GMT</pubDate>
    </item>
    <item>
      <title>呼吁采取行动：对问题做出反应以帮助提供资金</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</link>
      <description><![CDATA[嗨，RL 的朋友们， 我和我的同事们一直在开发 Tianshou 库，并且即将将其变成真正造福社区的东西。这是我所知道的唯一一个具有广泛范围（各种算法、离线 RL、marl 等，最先进的性能和快速吞吐量）的库，旨在为研究人员和应用程序开发人员提供帮助。 现在我们公司发生了一些变化，导致新经理是一位非技术人员，不熟悉战略。如果我们不能以他们理解的方式展示社区的兴趣，该项目的资金可能会被取消。 我已经写了一个非常详细的计划，将 Tianshou 带到下一个主要版本，我相信结果对整个 RL 社区非常有用。我非常感谢您的支持，您只需对此问题留下点赞或评论即可。 当然，我也很高兴在那里就该计划和图书馆本身进行任何建设性的讨论！    提交人    /u/Left-Orange2267   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</guid>
      <pubDate>Mon, 09 Sep 2024 20:50:01 GMT</pubDate>
    </item>
    <item>
      <title>你们在哪里使用强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</link>
      <description><![CDATA[嗨，朋友们！ 我正在研究 RL，我想知道哪些公司正在应用 RL 来解决业务问题。当我搜索这个主题时，我只找到旧案例和来自大型科​​技公司的案例。 你们在学术界使用 RL 吗？你们在初创公司使用 RL 吗？只是想知道你们如何使用它并试图了解市场。 谢谢！    提交人    /u/embedding_turtle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</guid>
      <pubDate>Mon, 09 Sep 2024 20:48:33 GMT</pubDate>
    </item>
    <item>
      <title>深度Q迷宫</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fcjmzt/deep_q_maze/</link>
      <description><![CDATA[我正在使用深度 q 学习处理一个 8x8 的冰冻湖泊 - 它就像一个迷宫，但里面有洞，如果进入就会终止。我见过的许多其他实现直到迷宫至少完成一次才开始训练。我面临的问题是迷宫太大，无法偶然/不经过任何训练就解决（较小的迷宫效果很好）。当我允许它在不完成迷宫的情况下进行训练时，它会学会避开洞，而不是朝着终点前进。如果终止，奖励为 0，如果完成迷宫，奖励为 1 - 其他任何事情都使用奖励函数：0.9 * maxQ（下一个状态）。这个问题的解决方案是什么/我做错了什么？任何想法都将不胜感激。     提交人    /u/Magic__Mannn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fcjmzt/deep_q_maze/</guid>
      <pubDate>Mon, 09 Sep 2024 07:18:26 GMT</pubDate>
    </item>
    <item>
      <title>在价值迭代中，终端状态的值应该保持不变还是不保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</link>
      <description><![CDATA[我遵循了这个例子： https://courses.grainger.illinois.edu/cs440/fa2022/lectures/rl.html 这是一个网格世界，其中折扣 = 0.9，右上角的奖励为 +1，中右为 -1，而其他网格的奖励为零。 正如您所见，两个终端网格（右上和中右）的值在整个迭代过程中一直保持 +1 和 -1，这意味着它通过贝尔曼方程的迭代强制保持它们不变，但我在网上找到了多个其他数值例子，其中终端状态的值也根据贝尔曼方程进行更新。所以我的问题是：哪一种是正确的方法？非常感谢    提交人    /u/james_stevensson   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</guid>
      <pubDate>Sun, 08 Sep 2024 21:31:57 GMT</pubDate>
    </item>
    <item>
      <title>有沒有主动推理（自由能原理）的成功故事？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</link>
      <description><![CDATA[虽然自由能原理旨在通过最小化意外来开发一个统一的感知和控制框架，但据我所知，很少有经验结果可以证明其前景。有没有人听说过它在图像输入的连续控制问题或至少一些经典控制问题中的一些成功应用？    提交人    /u/OutOfCharm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</guid>
      <pubDate>Sun, 08 Sep 2024 09:48:59 GMT</pubDate>
    </item>
    <item>
      <title>比较 V-Trace 和 Retrace</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbizt8/comparing_vtrace_and_retrace/</link>
      <description><![CDATA[看起来 V-Trace 和 Retrace 非常相似，并且用途也非常相似，即纠正离策略数据的值估计。据我所知，唯一真正的区别是 V-Trace 增加了一个额外的裁剪参数（rho 参数），但我很难理解除此之外的区别。 到目前为止，我找不到两者的比较或任何提及它们相似性的文章（甚至 IMPALA 论文也没有提到 Retrace）。我还注意到，最近的作品提到了 Retrace，但没有提到 V-Trace（例如，Muesli 论文使用了 Retrace），尽管 V-Trace 的开发时间比 Retrace 晚两年。 所以我的问题是，我应该如何理解两者之间的关系？V-Trace 真的比 Retrace 好吗？但它在 IMPALA 之外从未真正流行过？或者它们用于完全不同的事情，而我完全误解了一些东西？    提交人    /u/vandelay_inds   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbizt8/comparing_vtrace_and_retrace/</guid>
      <pubDate>Sat, 07 Sep 2024 22:30:02 GMT</pubDate>
    </item>
    <item>
      <title>基于 pybullet 的健身房环境中的 PPO 四足动物行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/</link>
      <description><![CDATA[嗨，我正在尝试训练四足动物在基于 pybullet 的自定义健身环境中行走。我使用 Stablebaselines3 中的 PPO 作为具有默认超参数的算法。四足动物每条腿有 3 个自由度（总共 12 个）。观察空间是四足动物的基本线性位置、基本线性速度、基本角位置、基本角速度、12 个关节位置、12 个关节速度以及每只脚接触地面时的 4 个二进制值。动作空间和观察空间都已标准化。 奖励是沿 x 轴的正向移动（向前移动），惩罚是侧向漂移（沿 y 轴移动）。情节持续 5000 个时间步，但如果身体翻转也会结束。  通过运行 12 个并行环境进行训练，总共运行 100,000,000 个时间步（大约需要半天时间）。 到目前为止，最好的结果是，充其量，腿部的一些振动运动会产生向前的运动，但根本不自然。 有人有什么好的提示/资源关于如何获得更自然的结果吗？    提交人    /u/Fit_Passion6272   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/</guid>
      <pubDate>Sat, 07 Sep 2024 19:51:26 GMT</pubDate>
    </item>
    <item>
      <title>A2C 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fb2v7w/a2c_algorithm/</link>
      <description><![CDATA[      你好， 当我阅读《强化学习实践手册》（第二版，packt）时，我发现作者使用来自环境的奖励来计算回报 R，这更接近于蒙特卡洛更新，而不是引导从评论家估计 V(st+1)，您会在照片中找到更新。 我的问题是这会影响学习吗，这是 A2C 的另一个版本吗？ 提前谢谢您的回答！ A2C 算法，来源：动手强化学习书籍（第二版，packt）    提交人    /u/Capital_Win8377   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fb2v7w/a2c_algorithm/</guid>
      <pubDate>Sat, 07 Sep 2024 09:14:33 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的规范化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fb0fke/normalization_in_rl/</link>
      <description><![CDATA[我正在使用标准缩放和 MinMax 缩放，但它们在 RL 实现中效果不佳。我有 10 个特征，而每个特征都需要缩放到特定范围，如 [0, 1]。因为我的特征范围从 60,000 到低至 0.001。 如何才能以有效处理特征尺度上这些巨大差异的方式规范化此数据集以进行强化学习？ 有哪些高级规范化技术可以处理特征中的这种范围差异，同时仍能保持强化学习模型的稳定性？    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fb0fke/normalization_in_rl/</guid>
      <pubDate>Sat, 07 Sep 2024 06:13:13 GMT</pubDate>
    </item>
    <item>
      <title>强化学习奖励可以是当前状态和新状态的结合吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</link>
      <description><![CDATA[      我正在为我的 RL 代理构建奖励函数，并考虑采取行动后当前状态和新状态的组合。据我所知，基于 Sutton &amp;，这是可能的Barto，特别是公式 3.6，其中奖励是状态-动作对和结果状态的函数。我想确保我的方法符合 RL 理论。 有人可以确认这是否有效或分享见解吗？ https://preview.redd.it/7wndf4ny98nd1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=547a77aeecc75edcee9fbd41ec45dc95c772d3d1    提交人    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</guid>
      <pubDate>Fri, 06 Sep 2024 17:57:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习竞赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</link>
      <description><![CDATA[大家好，我通过强化学习开始训练，正在提升自己。我可以在网上参加和竞争这个领域的哪些比赛，我如何才能在这个领域有更好的发展？    提交人    /u/Weary-Ad-7225   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</guid>
      <pubDate>Fri, 06 Sep 2024 17:46:06 GMT</pubDate>
    </item>
    <item>
      <title>PPO 外汇交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</link>
      <description><![CDATA[      我正在使用 SB3 为外汇交易环境训练带有动作掩码的 PPO。该模型似乎学习了低点和高点枢轴点，但准确性非常可疑，而且它会反转动作，所以它在高点买入，在低点卖出！代码中哪里可能出错？  https://preview.redd.it/4dwdxlclj7nd1.png?width=986&amp;format=png&amp;auto=webp&amp;s=60ac1f2a3ce5cdd386d77d13c57c233a37be56a6    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</guid>
      <pubDate>Fri, 06 Sep 2024 15:31:34 GMT</pubDate>
    </item>
    </channel>
</rss>