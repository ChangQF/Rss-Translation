<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 16 May 2024 12:27:01 GMT</lastBuildDate>
    <item>
      <title>Dominion：人工智能研究的新领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct9r6u/dominion_a_new_frontier_for_ai_research/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.06846 摘要：  近年来，机器学习方法取得了巨大的进步，达到了在围棋、雅达利和扑克游戏中的超人表现。这些游戏以及之前的其他游戏不仅充当了测试平台，而且还有助于突破人工智能研究的界限。延续这一传统，我们研究了桌面游戏《Dominion》，并讨论了使其非常适合作为下一代强化学习 (RL) 算法基准的属性。我们还展示了 Dominion Online 数据集，该数据集包含经验丰富的玩家在 Dominion Online 网络服务器上玩过的超过 2,000,000 款 Dominion 游戏。最后，我们介绍了一个 RL 基线机器人，它使用现有技术来击败常见的基于启发式的机器人，并显示出与之前最强的机器人、Provincial 的竞争性能。  &lt;!-- SC_ON - -&gt;  由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct9r6u/dominion_a_new_frontier_for_ai_research/</guid>
      <pubDate>Thu, 16 May 2024 10:29:26 GMT</pubDate>
    </item>
    <item>
      <title>[D]不同操作空间或状态相关操作空间的当前 SOTA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct8wqi/dcurrent_sota_for_varying_action_spaces_or_state/</link>
      <description><![CDATA[你好。我的工作中一般不涉及强化学习。我主要使用 ML/DL 算法。但最近的问题陈述让我想知道是否可以使用 RL。目前的问题是，你当前处于一个状态，并且拥有所有过去状态的信息，并且知道从该状态可以采取哪些当前操作（这些操作随着每个状态的时间而不断变化，但它们是可数的操作），那么你会使用什么 RL 算法来完成这个任务。我再次在这方面是个菜鸟。    由   提交 /u/SmartEvening   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct8wqi/dcurrent_sota_for_varying_action_spaces_or_state/</guid>
      <pubDate>Thu, 16 May 2024 09:27:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习体育馆 ValueError</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct39ky/reinforcement_learning_gymnasium_valueerror/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct39ky/reinforcement_learning_gymnasium_valueerror/</guid>
      <pubDate>Thu, 16 May 2024 03:06:39 GMT</pubDate>
    </item>
    <item>
      <title>模组别生气</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct0ywo/mods_dont_be_mad/</link>
      <description><![CDATA[       由   提交 /u/glowingcanoodle   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct0ywo/mods_dont_be_mad/</guid>
      <pubDate>Thu, 16 May 2024 01:07:02 GMT</pubDate>
    </item>
    <item>
      <title>我需要帮助为我的论文找到问题陈述。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csul6i/i_need_help_finding_a_problem_statement_for_my/</link>
      <description><![CDATA[我目前在英国攻读硕士学位，专注于进化算法和强化学习的综合研究。我的目标是进行彻底的研究，旨在解决特定问题并在接下来的 3-4 个月内呈现切实的结果。我热衷于探索想法，并且非常感谢这方面的任何建议或见解。感谢您的支持。   由   提交/u/Fury2OP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csul6i/i_need_help_finding_a_problem_statement_for_my/</guid>
      <pubDate>Wed, 15 May 2024 20:14:38 GMT</pubDate>
    </item>
    <item>
      <title>ICYMI：现在，当给定论文或主题发布任何新代码时，您可以收到通知！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csswvw/icymi_you_can_now_get_notified_when_any_new_code/</link>
      <description><![CDATA[      ICYMI：现在，当给定论文或主题发布任何新代码时，您可以收到通知！只需安装代码查找器扩展程序（Chrome：https://chromewebstore.google.com/detail /ai-code-finder-for-papers/aikkeehnlfpamidigaffhfmgbkdeheil | Firefox： https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/ | 边缘：https://microsoftedge.microsoft.com/addons/detail/get-papers-with-code-ever/mflbgfojghoglejmalekheopgadjmlkm），单击任何铃声/警报浏览网页时遇到的图标，然后按照屏幕上的后续步骤操作 🙂 另外，通过提醒  将您感兴趣的领域的最新进展直接发送到您的收件箱。 作者最新作品：第一个知道作者何时发布新论文。  https://preview.redd.it/8knztvd22n0d1.png?width=2660&amp;format=png&amp;auto=webp&amp;s=6a426a414632 93d8dbd716f34b808574b3f78dc9&lt; /p&gt;   由   提交/u/fullerhouse570  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csswvw/icymi_you_can_now_get_notified_when_any_new_code/</guid>
      <pubDate>Wed, 15 May 2024 19:05:16 GMT</pubDate>
    </item>
    <item>
      <title>概率论书籍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cspq7r/books_on_probability_theory/</link>
      <description><![CDATA[我对概率论在强化学习中的应用有足够直观的理解，我能理解数学，但这些并不那么容易，而且我缺乏很多问题练习，这可能有助于我更好地理解概念，现在我可以理解数学，但我无法自己重新推导或证明这些界限或引理，所以如果你对有关书籍有任何建议概率论，将不胜感激您的反馈。 （另外，我懒得学习经典概率论〜纯数学，因为如果我想探索工程中应用概率的任何其他领域，它会派上用场或物理学或其他应用概率部分）所以任何可以为我提供该领域强大的基础和稳健多样性的书。谢谢！   由   提交/u/vyknot4wongs  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cspq7r/books_on_probability_theory/</guid>
      <pubDate>Wed, 15 May 2024 16:57:00 GMT</pubDate>
    </item>
    <item>
      <title>TorchRL 的概率参与者没有进行概率采样......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csjw4k/torchrls_probabilistic_actor_not_sampling/</link>
      <description><![CDATA[作为标题，概率参与者总是返回具有最大概率的动作... 代码： actor_net = nn.Sequential( nn.LazyConv2d(out_channels = 64, kernel_size = 5, stride=2, device=device), nn.Tanh() ， nn.LazyConv2d(out_channels = 32，kernel_size = 5，stride=2，device=device)， nn.Tanh()， nn .Flatten(start_dim=-3), nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear (env.action_spec.shape[-1], device=device), nn.Softmax(), ) policy_module = TensorDictModule( actor_net, in_keys=[“观察”], out_keys=[“概率”] ) policy_module = ProbabilisticActor(  module=policy_module， spec=env.action_spec， in_keys=[“probs”]， distribution_class=OneHotCategorical， return_log_prob=True, ) 我已经检查了概率是否具有足够高的熵（应该采取哪个操作几乎是随机的），当我我尝试通过代码跟踪它，但它包含在如此多的转换中，因此很难找到实际的采样器在哪里。我将进一步深入研究代码，但与此同时还有其他人遇到过这种情况吗？   由   提交/u/dagangsta2012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csjw4k/torchrls_probabilistic_actor_not_sampling/</guid>
      <pubDate>Wed, 15 May 2024 12:43:53 GMT</pubDate>
    </item>
    <item>
      <title>Lstm/gru 与过去的观察结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csjvwx/lstmgru_vs_past_observations/</link>
      <description><![CDATA[我有一个 pomdp 控制问题，我使用 TD3 算法和 gru 或 lstm 解决了这个问题（两者都有效，但 Lstm 稍好一些）。 是否可以不使用循环神经网络，而是再次向代理提供过去的观察结果，以获得系统动态的知识？  我搜索了使用过去的观察而不是 RNN 来解决 pomdp 控制问题的论文，但我找到的所有论文都使用了 RNN。 我希望我的问题是可以理解的。   由   提交/u/1qaym0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csjvwx/lstmgru_vs_past_observations/</guid>
      <pubDate>Wed, 15 May 2024 12:43:36 GMT</pubDate>
    </item>
    <item>
      <title>Q 和价值函数有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csdwxu/how_does_q_and_value_function_differ/</link>
      <description><![CDATA[      https://preview.redd.it/vq1vjyg27j0d1.png?width=1641&amp;format=png&amp;auto=webp&amp;s=c793e6352 ec192fed1f8b0351ec03f57c74c468f 我知道价值函数会在不考虑行动的情况下给出预期的奖励总和，但我真的不明白它是如何工作的  &amp;# 32；由   提交/u/Ordinary_Big_8726   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csdwxu/how_does_q_and_value_function_differ/</guid>
      <pubDate>Wed, 15 May 2024 06:07:17 GMT</pubDate>
    </item>
    <item>
      <title>机器人 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csdj5k/marl_for_robots/</link>
      <description><![CDATA[我正在寻找模拟移动机器人的竞争性 MARL。有相同的 3D 模拟器吗？   由   提交 /u/TopSimilar6673   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csdj5k/marl_for_robots/</guid>
      <pubDate>Wed, 15 May 2024 05:41:26 GMT</pubDate>
    </item>
    <item>
      <title>零样本强化学习 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs85oi/zero_shot_reinforcement_learning_r/</link>
      <description><![CDATA[有人熟悉零样本强化学习的概念吗？Ahmed Touati 最近进行的一项名为“零样本强化吗？”的调查学习存在吗？今年推出   由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs85oi/zero_shot_reinforcement_learning_r/</guid>
      <pubDate>Wed, 15 May 2024 00:48:36 GMT</pubDate>
    </item>
    <item>
      <title>零样本强化学习 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs84ou/zero_shot_reinforcement_learning_r/</link>
      <description><![CDATA[ 由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs84ou/zero_shot_reinforcement_learning_r/</guid>
      <pubDate>Wed, 15 May 2024 00:47:11 GMT</pubDate>
    </item>
    <item>
      <title>“鲁棒智能体学习因果世界模型”，Richens & Everitt 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs5xai/robust_agents_learn_causal_world_models_richens/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs5xai/robust_agents_learn_causal_world_models_richens/</guid>
      <pubDate>Tue, 14 May 2024 23:03:34 GMT</pubDate>
    </item>
    <item>
      <title>强化学习来识别最大化公式的特征组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crtcl0/reinforcement_learning_to_identify_combinations/</link>
      <description><![CDATA[我的特征空间非常大，我想知道如何使用强化学习来优化公式。我对机器学习这个领域非常陌生，因此感谢所有建议和帮助。谢谢   由   提交/u/Naive_Yoghurt4959   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crtcl0/reinforcement_learning_to_identify_combinations/</guid>
      <pubDate>Tue, 14 May 2024 14:20:16 GMT</pubDate>
    </item>
    </channel>
</rss>