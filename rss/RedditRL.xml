<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 26 Aug 2024 09:16:50 GMT</lastBuildDate>
    <item>
      <title>过山车大亨之旅中的列车特工</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</link>
      <description><![CDATA[嗨！ 我只是想分享我的项目，我尝试创建一个代理，在 Rollercoaster Tycoon 中构建自定义轨道，并使用游戏自己的乘坐评级作为适应度函数来训练代理构建最好的过山车。 目前，它使用 UI 单击按钮并读取游戏屏幕，这非常缓慢且脆弱，但如果它显示出任何实际学习某些东西的迹象，我将尝试为 OpenRCT2 创建一个插件，公开代理可以使用的真实 API 而不是 UI。然后应该有可能在无头模式下运行游戏，并且使用 API 和同时在多个 OpenRCT2 实例上更快地构建代理。 很高兴听到您的反馈，您可以在这里找到代码：https://github.com/ZerxXxes/openrct2_gym    提交人    /u/ZerxXxes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</guid>
      <pubDate>Mon, 26 Aug 2024 08:00:32 GMT</pubDate>
    </item>
    <item>
      <title>RL 竞赛/项目？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f18x61/rl_competitionsprojects/</link>
      <description><![CDATA[嗨，我打算攻读 RL 方向的博士学位，想知道什么样的经验会有所帮助。最近有什么推荐的比赛或项目吗？谢谢。     提交人    /u/Birdy6Liu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f18x61/rl_competitionsprojects/</guid>
      <pubDate>Sun, 25 Aug 2024 22:36:14 GMT</pubDate>
    </item>
    <item>
      <title>解决 2048 是不可能的</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f11gtg/solving_2048_is_impossible/</link>
      <description><![CDATA[所以我最近参加了一个强化学习课程，并决定通过解决 2048 游戏来测试我的知识。乍一看，这个游戏似乎很容易，但出于某种原因，对于代理来说相当困难。我尝试了不同的东西：改进后的 DQN，如 double-dqn、各种奖励和惩罚，现在是 PPO。但都不起作用。我能得到的最好结果是 512 个方块，这是我通过优化以下奖励获得的：任何合并 +1，无合并 0，无用移动 -1，游戏结束。我将棋盘编码为 (16,4,4) 独热张量，其中每个状态 [:, i, j] 代表 2 的幂。我尝试了各种架构：FC、CNN、Transformer 编码器。CNN 对我来说效果更好，但还远远不够好。 有人玩过这个游戏吗？也许有些提示？令我难以置信的是，用于非常复杂环境（如 dota 2、星际争霸等）的 RL 算法无法学会玩这个简单的游戏。    提交人    /u/Hopeful_Ad9591   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f11gtg/solving_2048_is_impossible/</guid>
      <pubDate>Sun, 25 Aug 2024 17:14:22 GMT</pubDate>
    </item>
    <item>
      <title>我的扫雷代理 (q-learning) 存在问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0xu49/problems_with_my_minesweeper_agent_qlearning/</link>
      <description><![CDATA[您好，我目前正在制作一个代理，用于解决扫雷游戏的问题。我是编程新手，在代理训练方面遇到了一些问题。我不知道为什么，但代理就是没有进步，所以也许我遗漏了代码的一些关键部分。它使用 q-learning 和奖励来改进。非常感谢您的帮助！我为此使用了 pycharm（不知道是否相关）。 https://we.tl/t-hbAmcIZjAn    提交人    /u/justindepro1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0xu49/problems_with_my_minesweeper_agent_qlearning/</guid>
      <pubDate>Sun, 25 Aug 2024 14:39:05 GMT</pubDate>
    </item>
    <item>
      <title>使用单个模拟服务器时运行矢量化环境的好方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0w5fe/what_is_a_good_way_to_run_vectorized_environments/</link>
      <description><![CDATA[我的自定义环境只是一个 Python 类。实际的环境模拟发生在 DLL 中。初始化需要一段时间，但随后 DLL 可以通过允许环境对 DLL 进行 C 函数调用、为该调用启动模拟，并且每个调用在内部被放置在单独的线程中，从而运行许多并行模拟。 我正尝试使用系统上的所有 CPU 来训练 SB3。 我理解 DummyVecEnv 将按顺序运行 n 个环境，因此这不会并行运行我的模拟。 SubprocVecEnv 可以并行运行环境，但据我所知，这将创建多个进程，而不是重新使用已经初始化的模拟世界。每个进程都会创建一个模拟世界的实例，我想避免这种情况，因为它相对较慢且占用大量内存。 我在这里有什么选择？    提交人    /u/RamenKomplex   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0w5fe/what_is_a_good_way_to_run_vectorized_environments/</guid>
      <pubDate>Sun, 25 Aug 2024 13:19:13 GMT</pubDate>
    </item>
    <item>
      <title>探索 Lode Runner 作为强化学习研究的基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0s960/exploring_lode_runner_as_a_benchmark_for/</link>
      <description><![CDATA[大家好， 我正在探索使用游戏 Lode Runner 作为强化学习 (RL) 的基准的想法。该游戏具有一些独特的机制，例如挖掘和攀爬，这可能会给 RL 算法带来有趣的挑战。 这是我正在考虑的：  单智能体 RL：测试智能体如何在受控环境中执行收集物品和避开敌人等任务。 多智能体 RL：实现多个智能体交互的场景，无论是一起工作还是相互竞争。一个例子是如果敌人也是基于 RL；也许可以尝试实验多个代理如何独立或合作地共同捕捉单个移动目标。 分层 RL：研究代理如何在游戏机制中学习高级策略和低级动作  我很想听听你的想法：  这是个好主意吗？Lode Runner 是否适合 RL 研究，或者有更好的选择？ 可能会出现哪些挑战？关于将经典游戏用于 RL 有什么建议吗？ 研究价值：你认为 Lode Runner 的独特之处会带来有趣的发现吗？ 与 星际争霸 的比较：使用 Lode Runner 与使用星际争霸 用于 RL 研究？Lode Runner 会相形见绌吗？还是它提供了独特的研究机会？  这是描述该游戏的维基百科页面的链接。 https://en.wikipedia.org/wiki/Lode_Runner 这是一个在线网站，您可以在其中尝试玩它。 https://www.retrogames.cz/play_079-NES.php 感谢您的任何见解。    提交人    /u/Abominable_Liar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0s960/exploring_lode_runner_as_a_benchmark_for/</guid>
      <pubDate>Sun, 25 Aug 2024 09:11:31 GMT</pubDate>
    </item>
    <item>
      <title>无法使用 DQN 来解决 CartPole-v1</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0lqkp/can_not_get_dqn_to_solve_cartpolev1/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0lqkp/can_not_get_dqn_to_solve_cartpolev1/</guid>
      <pubDate>Sun, 25 Aug 2024 02:07:55 GMT</pubDate>
    </item>
    <item>
      <title>我尝试制作深度强化学习股票交易机器人（失败了）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0lbng/my_failed_attempt_at_making_a_deep_reinforcement/</link>
      <description><![CDATA[        由    /u/NextgenAITrading 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0lbng/my_failed_attempt_at_making_a_deep_reinforcement/</guid>
      <pubDate>Sun, 25 Aug 2024 01:45:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 中的 RL 解决离散时间 LQR 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezvegd/struggling_to_solve_discretetime_lqr_with_rl_in/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezvegd/struggling_to_solve_discretetime_lqr_with_rl_in/</guid>
      <pubDate>Sat, 24 Aug 2024 03:03:42 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 对施米德胡贝的看法？人工智能之父，几乎不受关注</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezn907/discussion_thoughts_on_schmidhuber_father_of_ai/</link>
      <description><![CDATA[      我通过一些非常受人尊敬的技术人员发现了尤尔根·施米德胡贝，他们说他是最好的人工智能科学家。他的 LSTM 论文被引用超过 10 万次，声称为 transformers 奠定了基础，大量 RL 工作都是通过好奇心驱动的学习和建模世界进行的。他的论文非常棒，通俗易懂，信息量很大。他在自己的网站上炫耀二头肌和胡闹真的很有趣。 https://preview.redd.it/sp7160dn6hkd1.png?width=880&amp;format=png&amp;auto=webp&amp;s=9a82f2a764dca2a6ccc7c61c4875e2a8d43233d5 然后有些人告诉我不要读他的论文。其他一些顶级人工智能人物也对他不满。到目前为止，我发现的主要争议只是他有点讨厌地强迫别人正确引用他（我不知道谁是对的，但这听起来像是一个典型的问题，即将可以用其他方式充分解释的事情归咎于恶意。就像他们不引用他，因为他们真的没有使用他的工作，只是没有深入挖掘或类似的东西）。 我开始崇拜这个人，因为他在实验室所做的开创性工作和他退休前建立比自己更聪明的人工智能的人生使命。论文读起来很美。他说英语就像一个超级恶棍。只是一个整体有趣的角色。但这个领域的人怎么看他呢？我还没有和任何知道谣言和有内幕消息的人谈过这件事。 就目前而言，尽管有一些怪癖，但我会尽一切努力成为他的学徒，这是假设性的。似乎是这份工作的最佳人选之一。我会阅读他的所有论文，同时将这些想法付诸实践。此外，他的公司 NNAISENSE 似乎越来越不活跃了。我想知道发生了什么     提交人    /u/Inexperienced-Me   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezn907/discussion_thoughts_on_schmidhuber_father_of_ai/</guid>
      <pubDate>Fri, 23 Aug 2024 20:42:58 GMT</pubDate>
    </item>
    <item>
      <title>真实游戏的 Rl 框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezn25c/rl_framwork_for_real_games/</link>
      <description><![CDATA[向各位书呆子问好。我只是在寻找一个可以用于真实游戏的框架（或任何可以完成工作的东西）。例如卢克索游戏。我知道真正的控制台游戏是“gym retro”（我有 ps3 gens 及以上版本。没有那么好玩的游戏，更多的是炫耀图形和政治故事）。无论如何。有没有什么框架可以实现？或者有其他方法吗？ 我的一个朋友建议：构建一个应用程序来获取您的屏幕，然后使用 opencv，然后使用带有 kayboard 的整数......等等，所以不建议这样做。 非常感谢。    提交人    /u/matin1099   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezn25c/rl_framwork_for_real_games/</guid>
      <pubDate>Fri, 23 Aug 2024 20:35:01 GMT</pubDate>
    </item>
    <item>
      <title>Open AI Gym 从定义的空间中创建一个“空元素”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezjciq/open_ai_gym_create_an_empty_element_from_a/</link>
      <description><![CDATA[大家好， 关于如何使用 gymnasium.spaces，我有一个相对简单的问题。假设我定义了以下观察空间 observation_space = space.Dict( { &quot;local-obs&quot;: space.Box(low=-1, high=1, shape=(3,3,1)), &quot;global-obs&quot;: space.Box(low=-1, high=1, shape=(7,7,3)) } )  是否存在现有方法可以让我“初始化”满足上述观察空间结构的观察？即一个字典，其中 local-obs 是一个形状为 (3,3,1) 的全零数组，等等。 我可以不使用我定义的空间结构来单独创建这样的观察，但我觉得应该有更好的方法来强制执行该结构。 提前致谢！    提交人    /u/GammaYankee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezjciq/open_ai_gym_create_an_empty_element_from_a/</guid>
      <pubDate>Fri, 23 Aug 2024 17:59:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们没有为每款棋盘游戏配备超人 AI？是兴趣还是知识？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezjbkj/why_dont_we_have_superhuman_ai_for_every_board/</link>
      <description><![CDATA[我是 RL 的新手，所以如果这是一个愚蠢的问题，请别介意。 我想知道为什么大多数棋盘游戏中没有超人 AI，因为我们在国际象棋、围棋和扑克中都有超人 AI。 有很多流行的棋盘游戏，我真的很好奇超人 AI 会是什么样子，但却找不到任何类似的东西。例如，《卡坦岛》、《银河竞逐》、《辉煌》、《风险》等。 我们没有超人 AI 的原因是以下任何一种（或组合）吗？  现成的 RL 算法不能用于为每款棋盘游戏创建超人 AI，需要通过一些理论工作来适应这些游戏，这对非 RL 研究人员来说是一个很难解决的问题。 现成的 RL 算法可以由 RL 爱好者适应这些游戏，但人们对此兴趣不大（或者 RL 爱好者不够多） 还有什么原因吗？     提交人    /u/hellofloss   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezjbkj/why_dont_we_have_superhuman_ai_for_every_board/</guid>
      <pubDate>Fri, 23 Aug 2024 17:57:58 GMT</pubDate>
    </item>
    <item>
      <title>2024 年学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez8qrv/learning_rl_in_2024/</link>
      <description><![CDATA[您好，2024 年有哪些不错的免费在线资源（课程、笔记）可以学习 RL？ 谢谢！    提交人    /u/spacejunk99   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez8qrv/learning_rl_in_2024/</guid>
      <pubDate>Fri, 23 Aug 2024 09:51:18 GMT</pubDate>
    </item>
    <item>
      <title>强化算法之前的游戏中的对手？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez7g8b/opponents_in_games_before_reinforcement_algorithms/</link>
      <description><![CDATA[我目前正在学习（深度）强化学习。我的教授向我们展示了实施算法来解决雅达利游戏的经典示例。我们讨论的算法可以追溯到 1990 年代末到 2010 年代初。 这让我对以下事情产生了疑问：小时候我曾经玩过例如 Yu-Gi-Oh Power of Chaos 系列或 Fifa 98，在这些游戏中，你的对手相当聪明（至少在我小时候的记忆中是这样），据我所知，你甚至可以设定对手的难度。当时已经实施了这些算法吗？或者这些只是硬编码的规则集？    提交人    /u/Vast-Signature-8138   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez7g8b/opponents_in_games_before_reinforcement_algorithms/</guid>
      <pubDate>Fri, 23 Aug 2024 08:22:13 GMT</pubDate>
    </item>
    </channel>
</rss>