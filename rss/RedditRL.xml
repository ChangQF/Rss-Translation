<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Tue, 11 Jun 2024 18:20:16 GMT</lastBuildDate>
    <item>
      <title>NVidia Omniverse æ¥ç®¡äº†æˆ‘çš„è®¡ç®—æœº</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ddkw1g/nvidia_omniverse_took_over_my_computer/</link>
      <description><![CDATA[      https://preview.redd.it/6ecu4wdfgz5d1.png?width=967&amp;format=png&amp;auto=webp&amp;s=d56ddf933839a6a3f938d83b72aaed1b5fb9372d æˆ‘åªæ˜¯æƒ³ä½¿ç”¨ Nvidia ISAAC sim æ¥æµ‹è¯•ä¸€äº›å¼ºåŒ–å­¦ä¹ ã€‚ä½†å®ƒå®‰è£…äº†æ•´ä¸ªå¥—ä»¶ã€‚åœ¨æˆ‘è®¾æ³•åˆ é™¤ä¸€äº›ä¹‹å‰ï¼Œè¿˜æœ‰æ›´å¤šçš„æµç¨‹å’ŒæœåŠ¡ã€‚æˆ‘éœ€è¦æ‰€æœ‰è¿™äº›å—ï¼Ÿæˆ‘åªæƒ³èƒ½å¤Ÿç¼–å†™ä¸€äº›è„šæœ¬æ¥å­¦ä¹ å’Œå›æ”¾ Pythonã€‚è¿™å¯èƒ½å—ï¼Œè¿˜æ˜¯æˆ‘éœ€è¦æ‰€æœ‰è¿™äº›æœåŠ¡æ‰èƒ½ä½¿å…¶è¿è¡Œï¼Ÿ  è¿™æ¯”ä½¿ç”¨å¸¦æœ‰ MLAgents çš„ Unity æ›´å¥½å—ï¼Œå®ƒçœ‹èµ·æ¥å‡ ä¹æ˜¯åŒä¸€ä»¶äº‹ã€‚     æäº¤äºº    /u/No_Way_352   [link] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ddkw1g/nvidia_omniverse_took_over_my_computer/</guid>
      <pubDate>Tue, 11 Jun 2024 18:12:02 GMT</pubDate>
    </item>
    <item>
      <title>åŸºäºæ¨¡å‹ä¸æ— æ¨¡å‹çš„åŒºåˆ«</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcqdu5/distinction_of_modelbased_vs_modelfree/</link>
      <description><![CDATA[æ— è®ºæˆ‘çœ‹åˆ°å¤šå°‘æ•™ç§‘ä¹¦å®šä¹‰ï¼ŒæŸäº›æœºå™¨å­¦ä¹ å®šä¹‰éƒ½æ¯«æ— æ„ä¹‰ã€‚ åŸºäºæ¨¡å‹ä¸æ— æ¨¡å‹ï¼Œå‡è®¾æˆ‘ä»¬é‡‡ç”¨ç³»ç»Ÿè¯†åˆ«è§†è§’ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å·²ç»å®šä¹‰äº†åŠ¨æ€ç»“æ„ï¼Œå¹¶å¸Œæœ›æ‰¾åˆ°æ¨¡æ‹Ÿå‚æ•°ï¼Œä½¿äº‹æƒ…åƒåœ¨ç°å®ç”Ÿæ´»ä¸­ä¸€æ ·å±•å¼€ã€‚å¸Œæœ›ä»¥è¿­ä»£æ•°æ®é©±åŠ¨çš„æ–¹å¼æ‰§è¡Œæ­¤æ“ä½œå¯ä»¥èŠ‚çœå¤§é‡åŠ³åŠ¨ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡ç»“æ„å¯¹æ¨¡å‹æ–½åŠ çš„åå·®åº”è¯¥åŒæ—¶å¸®åŠ©å¯¹åŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶ä¿æŒè¶³å¤Ÿçš„çµæ´»æ€§ä»¥è¿›è¡Œè°ƒæ•´ã€‚è¿™ä¼¼ä¹ä»æ ¹æœ¬ä¸Šå­˜åœ¨ç¼ºé™·ï¼Œæ— æ³•å°†å…¶ä¸æ— æ¨¡å‹æ–¹æ³•åŒºåˆ†å¼€æ¥ã€‚æ— æ¨¡å‹æˆ‘ä»¬ä¸å¼ºåŠ ä»»ä½•åŠ¨æ€ç»“æ„ã€‚æˆ‘è®¤ä¸ºï¼Œæˆ‘ä»¬ä¸ä¼šé™åˆ¶åŠ¨æ€çš„å±•å¼€ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½åœ¨ä»æ ¹æœ¬ä¸Šç”¨æ•°æ®æˆ–ç»éªŒè°ƒæ•´å‚æ•°ã€‚ï¼ˆå°½ç®¡å‚æ•°å°‘å¾—å¤šï¼‰ã€‚æœ€ç»ˆï¼Œé—®é¢˜å–å†³äºç”±ï¼ˆåˆå§‹ï¼‰åè§é€‰æ‹©çš„æœªæ¥ç»éªŒï¼Œè€Œè¿™ç§åè§æ¨åŠ¨äº†â€œå¯»æ‰¾å‚æ•°â€çš„ç›®æ ‡ã€‚åè§è¦ä¹ˆæ¥è‡ªä»¥å‰çš„ç»éªŒï¼Œè¦ä¹ˆç”±æ¨¡å‹å¼•èµ·ï¼Œç­‰ç­‰ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œè¿™å½’ç»“ä¸ºæ¢ç´¢ä¸åˆ©ç”¨çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯æ–¹æ³•é—®é¢˜ã€‚ æ­£ç¡®çš„æ€è€ƒæ–¹å¼æ˜¯ä»€ä¹ˆï¼Ÿ    æäº¤äºº    /u/FriendlyStandard5985   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcqdu5/distinction_of_modelbased_vs_modelfree/</guid>
      <pubDate>Mon, 10 Jun 2024 16:55:43 GMT</pubDate>
    </item>
    <item>
      <title>ç‹¬å®¶ä¸“è®¿â€œUnitree G1 - äººå½¢ä»£ç† AI åŒ–èº«â€è½¯æœºå™¨äººæ’­å®¢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcni17/exclusive_interview_unitree_g1_humanoid_agent_ai/</link>
      <description><![CDATA[        æäº¤äºº    /u/meldiwin   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcni17/exclusive_interview_unitree_g1_humanoid_agent_ai/</guid>
      <pubDate>Mon, 10 Jun 2024 14:56:27 GMT</pubDate>
    </item>
    <item>
      <title>æ¼”å‘˜è¯„è®ºå®¶ç®—æ³•ä»£ç å¸®åŠ©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dckr5g/actor_critic_algorithm_code_help/</link>
      <description><![CDATA[å¯¹äºè¿ç»­çš„åŠ¨ä½œå’ŒçŠ¶æ€ï¼ŒA2C ç®—æ³•ä»£ç å¸®åŠ©çš„ä¼˜åŠ¿ åŠ¨ä½œæ²¡æœ‰æ”¹å˜ï¼Œå®ƒç»™å‡ºæ’å®šçš„å•ä¸€åŠ¨ä½œ    æäº¤äºº    /u/Past-News-1373   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dckr5g/actor_critic_algorithm_code_help/</guid>
      <pubDate>Mon, 10 Jun 2024 12:52:10 GMT</pubDate>
    </item>
    <item>
      <title>ğŸ™ Octoï¼šå¼€æºé€šæ‰æœºå™¨äººæ”¿ç­–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcit8c/octo_an_opensource_generalist_robot_policy/</link>
      <description><![CDATA[  ç”±    /u/HimitsuNoShougakusei  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcit8c/octo_an_opensource_generalist_robot_policy/</guid>
      <pubDate>Mon, 10 Jun 2024 11:02:59 GMT</pubDate>
    </item>
    <item>
      <title>æ¨¡æ‹Ÿé€€ç«ä¸å¼ºåŒ–å­¦ä¹ </title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</link>
      <description><![CDATA[å½“è€ƒè™‘å¯å‘å¼ç«äº‰ç¼–ç¨‹ä»»åŠ¡æ—¶ä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ä¾‹å­ï¼Œæ—…è¡Œå•†é—®é¢˜ï¼ˆæˆ–è€…æœ€è¿‘çš„è¿™ä¸ªç«èµ›ï¼Œå¾ˆå¤šäººéƒ½åœ¨è®¨è®º RL çš„å¯èƒ½æ€§ï¼Œä½†å¤§å¤šæ•°äººéƒ½ä¸æ˜¯ä¸“å®¶ï¼ˆåŒ…æ‹¬æˆ‘è‡ªå·±ï¼Œæœ€ç»ˆä¹Ÿä½¿ç”¨äº†æ¨¡æ‹Ÿé€€ç«ï¼Œä½†äº‹åå´å¾ˆç—›è‹¦ï¼Œå› ä¸ºæˆ‘æœ¬æ¥æƒ³åšç‚¹ä¸åŒçš„äº‹æƒ…ï¼‰ï¼‰ã€‚ å‡ ä¹æ‰€æœ‰è¿™äº›æ¯”èµ›éƒ½æ˜¯ä½¿ç”¨æ¨¡æ‹Ÿé€€ç«æˆ–å…¶ä»–å˜ä½“èµ¢å¾—çš„ã€‚å¯¹äºä¸ç†Ÿæ‚‰çš„äººæ¥è¯´ï¼Œæ‰€æœ‰è¿™äº›å˜ä½“éƒ½æ˜¯ä»æŸä¸ªè§£å†³æ–¹æ¡ˆå¼€å§‹ï¼Œç„¶åé€šè¿‡æŸç§å˜å¼‚è¿‡ç¨‹è¿­ä»£æ”¹è¿›å®ƒï¼Œä»¥æ‘†è„±å±€éƒ¨æœ€å°å€¼ã€‚å¯¹äºæ—…è¡Œå•†é—®é¢˜ï¼Œæ‚¨å¯ä»¥æå‡ºä¸€ä¸ªåˆå§‹çš„éšæœºåŸå¸‚åˆ—è¡¨ï¼Œç„¶åéšæœºäº¤æ¢ä¸€äº›åŸå¸‚ï¼Œç›´åˆ°å®ƒæ”¹è¿›äº†æ‚¨çš„è§£å†³æ–¹æ¡ˆï¼Œç„¶åå°†è¿™ä¸ªæ–°è§£å†³æ–¹æ¡ˆä½œä¸ºæœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œä¾æ­¤ç±»æ¨ã€‚å†åŠ ä¸Šä¸€äº›çªå˜ä»¥é€ƒé¿å±€éƒ¨æœ€å°å€¼ï¼ˆä¾‹å¦‚ï¼Œæ„å‘³ç€å¯¹åˆ—è¡¨çš„ä¸€å°éƒ¨åˆ†è¿›è¡Œæ”¹ç»„ - æˆ‘æ˜¾ç„¶æ˜¯åœ¨ç®€åŒ–ï¼‰ã€‚ ä»€ä¹ˆä¼šé˜»æ­¢äººä»¬åœ¨è¿™äº›é—®é¢˜ä¸Šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆå®é™…ä¸Šæ²¡æœ‰äººï¼Œè¿™ç¯‡æ–‡ç« ä¸­å·²ç»é’ˆå¯¹æ—…è¡Œå•†é—®é¢˜å®Œæˆäº†æ­¤æ“ä½œï¼šhttps://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/tje2.12303 - å¦‚æœæˆ‘æ²¡çœ‹é”™çš„è¯ï¼Œä½œè€…ç”šè‡³æåˆ°äº†æ¨¡æ‹Ÿé€€ç«ï¼Œä½†æ²¡æœ‰å°†ç»“æœä¸å®ƒè¿›è¡Œæ¯”è¾ƒï¼‰ã€‚å¥–åŠ±å‡½æ•°é€šå¸¸ä¸éš¾æƒ³å‡ºï¼ˆæˆ‘åœ¨ç«èµ›ä¸­æåˆ°çš„å¥–åŠ±å‡½æ•°ç”šè‡³æ¯” TSP æ›´å®¹æ˜“ï¼Œå› ä¸ºæ¯æ¬¡â€œæ€ªç‰©â€æ­»äº¡åä½ éƒ½ä¼šè·å¾—â€œé‡‘å¸â€ï¼Œä½ ä¼šå°è¯•æœ€å¤§åŒ–å®ƒï¼ˆç´¯è®¡é‡‘é¢ï¼‰ï¼‰ã€‚ æˆ‘å¯¹ä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„å‡è®¾æ˜¯ï¼š  å°½ç®¡å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œä½†è¿™äº›é—®é¢˜å®é™…ä¸Šå¾ˆå®¹æ˜“æ¨¡æ‹Ÿï¼Œå› æ­¤æ›´æ–°ç¥ç»ç½‘ç»œæˆ–ä»»ä½•å‡½æ•°è¿‘ä¼¼å™¨çš„å¼€é”€éƒ½å¤ªé«˜ã€‚åªæœ‰å½“è¿è¡Œä¸€é›†çš„æˆæœ¬éå¸¸é«˜æ—¶ï¼Œå¼ºåŒ–å­¦ä¹ æ‰ä¼šæœ‰è¶£ã€‚å¦åˆ™ï¼Œç”¨ C ç¼–å†™ç®€å•çš„é—ä¼ ç®—æ³•æ€»æ˜¯æ¯”ç”¨ Python ç¼–å†™çš„ RL æ›´æœ‰æ•ˆï¼ˆæ—¶é—´æ–¹é¢ï¼‰ã€‚ æ— éœ€æ¦‚æ‹¬ï¼Œè¿™äº›æ¯”èµ›çš„æµ‹è¯•ç”¨ä¾‹å·²ç»ç»™å‡ºï¼Œä½ åªéœ€è¦æƒ³å‡ºæœ€ä½³çš„è¡ŒåŠ¨åºåˆ—æ¥å½±å“ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œåœ¨æˆ‘çš„ç¬¬äºŒä¸ªä¾‹å­ä¸­è¦æ€æ­»å“ªäº›æ€ªç‰©ï¼‰å¹¶åœ¨è¿™äº›æµ‹è¯•ç”¨ä¾‹ä¸­è·å¾—æœ€é«˜å¥–åŠ±ã€‚å¦‚æœæ¯”èµ›å†…å®¹ç›¸åŒï¼Œä½†ä»–ä»¬åœ¨æ¯”èµ›ç»“æŸå‰ä¸‰ååˆ†é’Ÿå…¬å¸ƒæµ‹è¯•ç”¨ä¾‹ï¼Œé‚£ä¹ˆåœ¨ 8000 ä¸ªçº¿ç¨‹ä¸Šè¿è¡Œæ¨¡æ‹Ÿé€€ç«ä¸‰ååˆ†é’Ÿçš„æ•ˆç‡ä¸å¦‚ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„ä»£ç†ï¼Œè¯¥ä»£ç†åœ¨ GPU ä¸Šç»è¿‡å‡ å¤©çš„å¤§é‡ä¸åŒè™šæ„æµ‹è¯•ç”¨ä¾‹çš„è®­ç»ƒã€‚ RL åœ¨å¤šä»£ç†è®¾ç½®ï¼ˆé›¶å’Œæ¸¸æˆç­‰ï¼‰ä¸­çœŸæ­£æ˜¾ç¤ºäº†å…¶ä¸»å¯¼åœ°ä½ï¼Œå…¶ä¸­æ¨¡æ‹Ÿé€€ç«å’Œå˜ä½“ä¸æ˜“å®ç°ï¼ˆå°½ç®¡ MARL ä¼˜åŒ–çš„æ¯ä¸€æ­¥éƒ½åœ¨å°è¯•åˆ©ç”¨å½“å‰æœ€ä½³ç­–ç•¥ç»„åˆï¼Œè¿™å¯ä»¥é€šè¿‡é—ä¼ ç®—æ³•æ¥å®Œæˆ - ä½†æˆ‘è®¤ä¸ºè¿™è¢«ç§°ä¸º RLï¼Œå®ƒåªæ˜¯æ²¡æœ‰æ¢¯åº¦çš„ RLï¼‰ã€‚ ä½†åŒæ—¶ï¼ŒRL æ¯”å…¶ä»–æŠ€æœ¯æ›´å¤æ‚ï¼Œæ‰€ä»¥ä¹Ÿè®¸äººä»¬åªæ˜¯å› ä¸ºæ²¡æœ‰ä¸“ä¸šçŸ¥è¯†è€Œä¸å»é‚£é‡Œï¼Œè€Œ RL ä¸“å®¶å®é™…ä¸Šä¼šåœ¨å…¶ä¸­ä¸€äº›æ¯”èµ›ä¸­è¡¨ç°å‡ºè‰²ï¼Ÿ  æˆ‘é—æ¼äº†ä»€ä¹ˆå—ï¼Ÿæ‚¨ä»¬è¿™äº› RL ä¸“å®¶æ€ä¹ˆçœ‹ï¼ŸRich ä¼šæ€ä¹ˆè¯´ã€‚è¨é¡¿è¯´äº†ä»€ä¹ˆï¼Ÿ    æäº¤äºº    /u/Lindayz   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</guid>
      <pubDate>Mon, 10 Jun 2024 10:52:31 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºPPOä¸­å°æ‰¹é‡çš„é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcfqfd/question_about_minibatch_in_ppo/</link>
      <description><![CDATA[ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ–°æ‰‹ï¼Œå¯¹è®­ç»ƒ PPO æ—¶è®¾ç½®å°æ‰¹é‡æœ‰ç–‘é—®ã€‚ åœ¨åŸºäº DQN çš„ç®—æ³•ä¸­ï¼Œæˆ‘ç†è§£å°æ‰¹é‡å¯¹åº”äºä»é‡æ”¾ç¼“å†²åŒºä¸­è·å–çš„æ ·æœ¬æ•°é‡ã€‚ ä½†åœ¨åƒ PPO è¿™æ ·çš„åŸºäºç­–ç•¥çš„ç®—æ³•ä¸­ï¼Œéœ€è¦ä¸€é›†çš„å®Œæ•´è½¨è¿¹æ¥è®¡ç®—ç­–ç•¥æ¢¯åº¦ã€‚ è¿™æ˜¯å¦æ„å‘³ç€ PPO ä¸­çš„å°æ‰¹é‡å¯¹åº”äºé›†æ•°ï¼Ÿ è¿™éå¸¸ä»¤äººå›°æƒ‘ï¼Œå› ä¸ºåœ¨ DQN ä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ TD è¯¯å·®æ¥è®¡ç®—æ¢¯åº¦ï¼Œä½†åŸºäºç­–ç•¥çš„ç®—æ³•éœ€è¦å…¨é•¿æ¥è®¡ç®—æ¢¯åº¦ã€‚    æäº¤äºº    /u/MediocreAgency6070   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcfqfd/question_about_minibatch_in_ppo/</guid>
      <pubDate>Mon, 10 Jun 2024 07:22:25 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•æ„å»º Mujoco Envsã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dc44c7/how_to_build_mujoco_envs/</link>
      <description><![CDATA[æˆ‘æƒ³ä¸º Kinova çš„ jaco2 æ‰‹è‡‚æ„å»ºä¸€ä¸ª Mujoco ç¯å¢ƒï¼Œæˆ‘æœ‰ CADï¼Œå¹¶ä¸”å­˜åœ¨ ROS Gazebo ç¯å¢ƒï¼Œä½†æˆ‘ä¸€ç”Ÿéƒ½è®¨åŒ ROSï¼Œè€Œä¸”æˆ‘æ›´å–œæ¬¢ RL è€Œä¸æ˜¯æœºå™¨äººæŠ€æœ¯ã€‚æˆ‘çœŸçš„åœ¨å¯»æ‰¾é™¤ XML ä¹‹å¤–çš„å¦ä¸€ç§æ„å»ºç¯å¢ƒçš„æ–¹æ³•ã€‚è®©æˆ‘çŸ¥é“ã€‚     æäº¤äºº    /u/elonmusk-A12   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dc44c7/how_to_build_mujoco_envs/</guid>
      <pubDate>Sun, 09 Jun 2024 21:00:14 GMT</pubDate>
    </item>
    <item>
      <title>â€œå¥–åŠ±é»‘å®¢è¡Œä¸ºå¯ä»¥æ¨å¹¿åˆ°æ‰€æœ‰ä»»åŠ¡â€ï¼ŒNishimura-Gasparian ç­‰äººï¼Œ2024 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dc3obb/reward_hacking_behavior_can_generalize_across/</link>
      <description><![CDATA[        æäº¤äºº    /u/gwern   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dc3obb/reward_hacking_behavior_can_generalize_across/</guid>
      <pubDate>Sun, 09 Jun 2024 20:41:36 GMT</pubDate>
    </item>
    <item>
      <title>å°†å®éªŒç§»è‡³å¥èº«æˆ¿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbpceo/moving_experiment_to_gym/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æœ‰ç‚¹ç»æœ›ï¼Œå› ä¸ºæˆ‘æ­£åœ¨è¯•å›¾å¼„æ¸…æ¥šè¿™ä¸€ç‚¹ï¼Œè€Œä¸”æ²¡æœ‰å¤šå°‘æ—¶é—´äº†ã€‚ æˆ‘ç›®å‰æ­£å¤„äºè®ºæ–‡çš„åå­—è·¯å£ï¼Œè®ºæ–‡å°†åœ¨ä¸‰å‘¨å†…æˆªæ­¢ã€‚æ·±å…¥ç ”ç©¶å¹¶å°è¯•äº†ç¨³å®šåŸºçº¿å’Œç®€å•æ¨¡å‹åï¼Œæˆ‘æ„è¯†åˆ°ï¼Œè¦ä½¿æˆ‘çš„é¡¹ç›®æ­£å¸¸è¿è¡Œå¹¶ä½¿ç”¨ç¨³å®šåŸºçº¿æˆ– CleanRLï¼Œæˆ‘éœ€è¦åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ gym ç¯å¢ƒã€‚ åœ¨æˆ‘çš„é¡¹ç›®ä¸­ï¼Œæˆ‘å¿…é¡»ä½¿ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨ (NS3)ï¼Œå®ƒç”±å¤šè¾†è¯•å›¾åŒæ—¶ä¼ è¾“ä¿¡æ¯çš„æ±½è½¦ç»„æˆï¼Œå¹¶å°è¯•ä½¿ç”¨ RL æ¥äº†è§£å¦‚ä½•å‡å°‘ç½‘ç»œå»¶è¿Ÿã€‚ï¼ˆè§‚å¯Ÿç©ºé—´ã€äº¤é€šçŠ¶æ€å…ˆå‰çš„å»¶è¿Ÿï¼›æ¯è¾†è½¦ä¼ è¾“ä¿¡å·æ—¶çš„åŠ¨ä½œç©ºé—´ï¼‰ã€‚  æˆ‘å·²ç»æœ‰ä¸€ä¸ªä½¿ç”¨ DQN æ„å»ºçš„ä»£ç ï¼Œä½†æ˜¯æ€§èƒ½ä¸€ç›´å¾ˆå·®ï¼Œæˆ‘çš„â€œè®ºæ–‡â€æ˜¯å…³äºæ¢ç´¢æ–°ç®—æ³•çš„ã€‚ å› æ­¤ï¼Œæˆ‘æƒ³åˆ©ç”¨è¿™é‡Œçš„ reddit è¯»è€…çš„ç»éªŒå’Œæ™ºæ…§ - æˆ‘åº”è¯¥ (1) å°†æˆ‘çš„ç¯å¢ƒç§»è‡³ Gym å—ï¼Ÿ (2) åœ¨ GitHub ä¸Šå°è¯•ä¸€äº›åŸç”Ÿ PPO ç®—æ³•å—ï¼Ÿå¦‚æœæ‚¨èƒ½æä¾›ä»»ä½•å¯ä»¥æŸ¥çœ‹çš„åœ°æ–¹ï¼Œé‚£å°±å¤ªæ£’äº†ï¼ï¼ ä»»ä½•æƒ³æ³•éƒ½å°†ä¸èƒœæ„Ÿæ¿€ &lt;3ï¼éå¸¸æ„Ÿè°¢ï¼    æäº¤äºº    /u/No-Jelly-233   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbpceo/moving_experiment_to_gym/</guid>
      <pubDate>Sun, 09 Jun 2024 08:52:22 GMT</pubDate>
    </item>
    <item>
      <title>â€œå…‹åŠ³å¾·çš„æ€§æ ¼â€ï¼ŒAnthropicï¼ˆè®¾è®¡å…‹åŠ³å¾·-3åŠ©æ‰‹è§’è‰²ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbedkn/claudes_character_anthropic_designing_the_claude3/</link>
      <description><![CDATA[        æäº¤äºº    /u/gwern   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbedkn/claudes_character_anthropic_designing_the_claude3/</guid>
      <pubDate>Sat, 08 Jun 2024 22:20:01 GMT</pubDate>
    </item>
    <item>
      <title>DQN ä¸ Vanilla ç­–ç•¥æ¢¯åº¦</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbb6m7/dqn_vs_vanilla_policy_gradient/</link>
      <description><![CDATA[      å—¨ï¼Œ æˆ‘æ­£åœ¨å°è¯•æ¯”è¾ƒä¸¤ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼šVanilla Policy Gradient (REINFORCE) å’Œ DQNã€‚æˆ‘å¸Œæœ›æˆ‘çš„ç®—æ³•èƒ½å¤Ÿå­¦ä¼šåœ¨ä¸€ç»„æœºå™¨ä¸Šè°ƒåº¦ä½œä¸šï¼ŒåŒæ—¶å°½å¯èƒ½ç´§å¯†åœ°æ‰“åŒ…å®ƒä»¬ã€‚è§‚å¯Ÿç©ºé—´æ˜¯å°ºå¯¸ä¸º (20, 164) çš„å›¾åƒï¼Œä»£ç†å¿…é¡»ä» 10 å°æœºå™¨ä¸­é€‰æ‹© 1 å°ã€‚æˆ‘åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ 100 ä¸ªä¸åŒçš„ä½œä¸šåºåˆ—ï¼Œå¹¶ä½¿ç”¨ä¸€ç»„æ–°ä½œä¸šè¿›è¡Œæµ‹è¯•ã€‚æ¯é›†ä½¿ç”¨ä¸€ä¸ªä½œä¸šåºåˆ—ã€‚ ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘çš„ DQN ç®—æ³•çš„è¡¨ç°æ¯” Vanilla Policy Gradient (VPG) æ›´å·®ã€‚æˆ‘å°è¯•è°ƒæ•´ tauã€é‡æ”¾ç¼“å†²åŒºå¤§å°ã€æ‰¹é‡å¤§å°ã€æŠ˜æ‰£å› å­ã€å­¦ä¹ ç‡ç­‰ï¼Œä½†ç»“æœä¼¼ä¹æ²¡æœ‰ VPG é‚£ä¹ˆé«˜ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯å…·æœ‰ä»¥ä¸‹ DQN é…ç½®çš„ä¸¤è€…çš„æŠ˜æ‰£å¹³å‡å¥–åŠ±å›¾è¡¨  G = 0.95 é‡æ”¾å†…å­˜ = 100_000  æœ€å°é‡æ”¾å†…å­˜ = 10_000  æ‰¹é‡å¤§å° = 64  tau = 10  lr = 0.001 epochs = 600ï¼ˆåœ¨æ¯ä¸ª epoch è¿è¡Œ 20 episodes åè®­ç»ƒç½‘ç»œï¼‰ episodes = 20 éšè—èŠ‚ç‚¹ = 128 epochs æ•°é‡ = 600 æ¯ä¸ª epoch çš„ episodes æ•°é‡ = 20 å¦ã€‚æ¯é›†çš„å·¥ä½œé‡ = 200  .æ‚¨èƒ½å¸®åŠ©æˆ‘ç†è§£ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µå—ï¼Ÿæˆ‘åœ¨æ¯ä¸ªæ—¶æœŸåéƒ½ä¼šè®­ç»ƒä¸»ç½‘ç»œã€‚ https://preview.redd.it/3gg0dz3zje5d1.png?width=1924&amp;format=png&amp;auto=webp&amp;s=ac90a3c378d0817f00acc79600e87a34cb7c0fb3   ç”±    /u/hifzak  æäº¤  [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbb6m7/dqn_vs_vanilla_policy_gradient/</guid>
      <pubDate>Sat, 08 Jun 2024 19:49:39 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ äº§å“åŒ–çš„å¿…è¦ä¹‹æ¶</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbagrf/a_necessary_evil_for_productionizing_rl/</link>
      <description><![CDATA[        æäº¤äºº    /u/bin_und_zeit   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbagrf/a_necessary_evil_for_productionizing_rl/</guid>
      <pubDate>Sat, 08 Jun 2024 19:16:54 GMT</pubDate>
    </item>
    <item>
      <title>RLZoo è¶…å‚æ•°ä¸­çš„â€œnormalizeâ€æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1db61s7/what_does_normalize_mean_in_rlzoo_hyperparameters/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨æŸ¥çœ‹è¿™é‡Œç»™å‡ºçš„ RLZoo è¶…çº§å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹ä»£ç  - InvertedDoublePendulumBulletEnv-v0: normalize: true n_envs: 8 n_timesteps: !!float 2e6 policy: &#39;MlpPolicy&#39; n_steps: 2048 batch_size: 64 gae_lambda: 0.95 gamma: 0.99 n_epochs: 10 ent_coef: 0.0 learning_rate: 2.5e-4 clip_range: 0.2   è¿™é‡Œçš„ `normalize: true` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæˆ‘å‡è®¾è¿™å¯èƒ½æ˜¯è§‚å¯Ÿæ ‡å‡†åŒ–ï¼Œä½†æ˜¯ï¼Œè¿™é‡Œæœ‰è¿™æ ·çš„ä»£ç  -normalize: &quot;{&#39;norm_obs&#39;: False, &#39;norm_reward&#39;: True}&quot;å› æ­¤ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œ `normalize: true` æ—¶ï¼Œä»€ä¹ˆæ˜¯è¢«è§„èŒƒåŒ–çš„ï¼š rlzoo ä¸­æ˜¯å¦æœ‰ç”¨äºåŠ¨ä½œè§„èŒƒåŒ–çš„è¶…å‚æ•°ï¼Ÿæˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ï¼Œä½†ä¹Ÿè®¸å®ƒå¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Ÿ     æäº¤äºº    /u/Academic-Rent7800   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1db61s7/what_does_normalize_mean_in_rlzoo_hyperparameters/</guid>
      <pubDate>Sat, 08 Jun 2024 15:55:29 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘çš„åŠ¨ä½œç©ºé—´æœ‰ 90% è¢«æ©ç›–äº†ï¼Œæˆ‘å¸Œæœ›ä»ä¸­è·å¾—è®¡ç®—ä¸Šçš„å¥½å¤„</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</link>
      <description><![CDATA[æˆ‘çš„ q-learning ä»»åŠ¡ä¸­æœ‰ä¸€ä¸ªåŒ…å« 5000 ä¸ªåŠ¨ä½œçš„å¤§å‹åŠ¨ä½œç©ºé—´ï¼Œä½†åœ¨ä»»ä½•æ—¶å€™éƒ½åªæœ‰å‡ ç™¾ä¸ªåˆæ³•åŠ¨ä½œï¼Œå› æ­¤æˆ‘ä¼šå±è”½å®ƒä»¬ï¼Œä½†æ˜¯ï¼Œæˆ‘åªæ˜¯åœ¨æ¨¡å‹é¢„æµ‹æ­¥éª¤ä¹‹åè¿›è¡Œå±è”½ã€‚æœ‰æ²¡æœ‰åŠæ³•äº‹å…ˆå±è”½å®ƒå¹¶è·å¾—å¤§å¹…åŠ é€Ÿï¼Ÿ  def get_predictions(self, state, legal_mask): state = np.reshape(state, [1, self.state_size) act_values = [act_values[i] if legal_mask[i] == 1 else -np.inf for i in range(len(act_values))] return act_values     submitted by    /u/Breck_Emert   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</guid>
      <pubDate>Sat, 08 Jun 2024 07:03:17 GMT</pubDate>
    </item>
    </channel>
</rss>