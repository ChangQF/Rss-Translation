<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 15 Jan 2025 03:18:18 GMT</lastBuildDate>
    <item>
      <title>一款小型浏览器游戏，对手由经过 RL 训练的计算机控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i17ckc/a_little_browser_game_with_an_rltrained/</link>
      <description><![CDATA[我最近在开发一款小游戏时，遇到了一个使用强化学习训练过的计算机控制对手，这给了我不少乐趣，您可以直接在浏览器中玩这个游戏：https://adamheins.com/projects/shadows/web/ 这是一款 2D 小型捉迷藏游戏，当您不是“它”时，通过收集宝藏可以获得积分（而当您是“它”时，对手收集宝藏会扣分）。环境中有障碍物，而且由于您在障碍物后面的视线被阻挡，游戏难度加大。 计算机控制的代理使用两种不同的 SAC 模型：一种用于“它”，一种用于非“它”。目前，游戏并不完全“公平”因为计算机可以访问玩家的当前位置（即，它不必担心视线被遮挡，或者换句话说，它不必处理部分可观测性）。另一种方法是直接从像素训练模型，我尝试过，但 (1) 正如您所料，模型更难学习，并且 (2) 在浏览器实现中更难/更慢地进行图像观察。我使用 Python 版本的游戏进行实际训练，然后将模型导出到 ONNX 以在浏览器中运行。代码在这里：https://github.com/adamheins/shadows 享受！    提交人    /u/adamheins   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i17ckc/a_little_browser_game_with_an_rltrained/</guid>
      <pubDate>Tue, 14 Jan 2025 14:33:02 GMT</pubDate>
    </item>
    <item>
      <title>对 RLC 的看法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i15qi9/views_on_rlc/</link>
      <description><![CDATA[大家好，我是三年级的博士生，正在研究 Bandits 和 MDP。我想知道是否有人可以提供关于强化学习会议 (RLC) 的评论，作为提交的潜在场所。 我确实看到它的咨询委员会很好，但考虑到这是一个新会议，我想知道是否值得在那里提交    提交人    /u/Fantastic-Nerve-4056   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i15qi9/views_on_rlc/</guid>
      <pubDate>Tue, 14 Jan 2025 13:12:04 GMT</pubDate>
    </item>
    <item>
      <title>无真实数据的分割</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i14rdb/segmentation_without_groundtruth/</link>
      <description><![CDATA[大家好， 我对使用时间和奖励信息进行没有基本事实的分割很感兴趣。以下场景特别有趣：  前景检测：（示例）给定一段足球比赛的视频 - 分割球员和球 元素检测：（示例）给定游戏 Pong 的轨迹（特别是帧+奖励）- 分割球员和球  我想要的是能够区分视频/轨迹中的“重要”元素，而不依赖于给定分布的先验知识。依赖时间信息是可以的。即。在天空中一架飞机的视频中，通过飞机的运动来检测飞机是有意义的。 有没有关于这种情况的研究？ 我认为正在使用基础的segment-anything模型。    提交人    /u/Potential_Hippo1724   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i14rdb/segmentation_without_groundtruth/</guid>
      <pubDate>Tue, 14 Jan 2025 12:16:27 GMT</pubDate>
    </item>
    <item>
      <title>离策略确定性策略梯度的推导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i138r5/derivation_of_offpolicy_deterministic_policy/</link>
      <description><![CDATA[嗨！这是我在这个帖子上的第一个问题，所以如果缺少任何有助于您回答问题的内容，请告诉我。 我一直在研究确定性策略梯度论文（Silver 等人，2014 年），并试图理解方程 15。据我目前的理解，方程 14 表明我们可以使用从行为策略获得的状态分布来修改性能目标，因为我们正在尝试推导离策略确定性策略梯度。并且看起来，根据定理 1 的推导过程，对策略参数求导 14 将直接导致（离策略）性能目标的梯度。 所以我不明白为什么会有方程 15。作者提到他们已经删除了一个依赖于 Q 函数相对于的梯度的项。策略参数，但我不明白为什么应该删除它，因为当我们区分方程 14 时，该术语根本不存在。此外，我对方程 15 的第二行也很好奇，其中策略分布 $\mu_{\theta}(a|s)$ 变成了 $\mu_{\theta}$。 如果有人能回答我的问题，我将不胜感激。    提交人    /u/Fragrant-Leading8167   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i138r5/derivation_of_offpolicy_deterministic_policy/</guid>
      <pubDate>Tue, 14 Jan 2025 10:33:38 GMT</pubDate>
    </item>
    <item>
      <title>如果有人有兴趣参与我的棋盘游戏 Splendor 的 DDQN 最后部分，请告诉我。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i0v68g/if_anybody_is_interested_in_collaborating_for_the/</link>
      <description><![CDATA[除了最后一部分，也就是最有趣的部分，我不需要任何帮助。调整模型并使其工作。我在 TensorBoard 中记录了大量内容，以实现良好的可视化效果，您可以添加任何内容 - 我不期望任何编码帮助，因为它是一个相当大的代码库。但如果您想要，您完全可以。只是希望有人坐下来讨论如何让模型发挥性能。 https://github.com/BreckEmert/Splendor-AI 我正在研究的最大问题，我将从刚发给某人的消息中重新粘贴： 我很好奇你对我如何为棋盘游戏 Splendor. DQN 设置动作空间的初步想法。除了“获取宝石”之外，整个动作和状态空间都很容易处理。轮到您时，您可以从 5 种宝石类型池中获取 3 种不同的宝石，或者从同一种宝石中获取 2 种。因此，您会认为动作空间是 5 选择 3 + 5（选择 1）。但问题是您的库存中最多有 10 颗宝石，因此您还必须丢弃至 10 颗。因此，如果您有 10 颗宝石，则必须选择 3 颗并丢弃 3 颗，或者如果没有完整的 3 颗可用，则您只能选择 2 颗，等等。最后，我们至少要考虑（15 种获取宝石的方法）*（1800 种丢弃宝石的方法）。不知道这是否很混乱。 我决定在代理选择任何“获取宝石”动作时将其锁定在购买序列中。无论它选择 10 个选项中的哪个，它都会被迫连续进行最多 6 次移动（通过在 argmax 期间将其他选项设置为 -inf）。它最多可以得到三颗宝石，从 5 个动作空间中挑选。然后它会丢弃它需要丢弃的宝石，从另外 5 个动作空间中挑选。现在我的所有动作空间总共只有 15 个。我不知道这看起来是很棒还是很愚蠢，哈哈，但无论如何我的模型性能都很糟糕；它根本没有学习。    提交人    /u/Breck_Emert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i0v68g/if_anybody_is_interested_in_collaborating_for_the/</guid>
      <pubDate>Tue, 14 Jan 2025 02:00:27 GMT</pubDate>
    </item>
    <item>
      <title>人工智能项目需要废物数据集：塑料、纸张等</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i0r4hs/need_waste_dataset_for_ai_project_plastic_paper/</link>
      <description><![CDATA[你好，AI 爱好者！👋 我目前正在研究用于废物管理的图像分类模型，我正在寻找合适的数据集。具体来说，我正在寻找包含以下图像的数据集：  塑料废物 纸质废物 其他类型的废物  如果您知道任何可以提供帮助的公开数据集或资源，或者您正在从事类似的项目并希望合作，请告诉我！任何指导、链接或建议都将不胜感激。 提前谢谢您！🙏    提交人    /u/Grouchy-Door6480   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i0r4hs/need_waste_dataset_for_ai_project_plastic_paper/</guid>
      <pubDate>Mon, 13 Jan 2025 22:51:42 GMT</pubDate>
    </item>
    <item>
      <title>我即将参加一次面试，面试官将测试我是否能够运用强化学习解决一个问题（公司：Chewy）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i0q50g/i_have_an_interview_coming_up_where_i_will_be/</link>
      <description><![CDATA[大家好。我有 DRL 和制造业的背景。然而，我遇到了一次采访，主管将向我介绍他们的供应链补货问题，并看看我如何适应 DRL。他们希望我对实施情况进行非常高水平的概述。我从未做过高水平的概述，所以不知道我应该期待什么。  此外，如果有人有进行此类采访的经验，您的意见将非常有价值。     提交人    /u/wild_wolf19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i0q50g/i_have_an_interview_coming_up_where_i_will_be/</guid>
      <pubDate>Mon, 13 Jan 2025 22:09:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 6 自由度机器人进行拾取和投掷强化学习 - 寻求有关真实世界设置的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i0jksn/reinforcement_learning_with_pick_and_throw_using/</link>
      <description><![CDATA[大家好，我目前正在开展一个关于使用 6-DOF 机器人进行拾取和投掷强化学习 (RL)的项目。我发现了两篇与此主题相关的有趣论文，链接如下：  论文 1 – arxiv: 2405.19001 论文 2 – arxiv: 2406.13453  但是，我在在现实世界中设置系统时遇到了困难，我希望得到有关几个具体问题的建议：  验证投掷的准确性：我无法弄清楚这些论文如何处理对投掷是否落在正确位置的验证。在现实世界的设置中，我如何确认物体已被准确投掷？使用 RGB-D 相机 来估计箱子的位置，并使用另一台相机来验证物体是否被成功抛出，这是一种好方法吗？ 训练期间的域随机化：在论文中，域随机化用于在训练期间改变箱子的位置。当转移到现实世界时，我是否应该通过将箱子的位置直接包含在动作空间中并不断更新来简化事情，还是有更好的方法来处理这个问题？ 拾取和投掷的单独模型：我正在考虑两种不同的方法： 方法 1：将拾取和投掷任务组合成一个 RL 模型。 方法 2：将两个任务分成不同的模型 - 对拾取步骤使用固定坐标（因此机器人将夹持器移动到预定义位置），并仅对投掷步骤应用 RL 以优化投掷动作。这种分离是否会使问题在实践中变得更容易、更可行？   如果有人在现实世界的机器人系统中使用过 RL 或曾经处理过类似的问题，我将非常感谢您的任何见解或建议。 非常感谢您的阅读！    提交人    /u/Sunnnnny24   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i0jksn/reinforcement_learning_with_pick_and_throw_using/</guid>
      <pubDate>Mon, 13 Jan 2025 17:39:05 GMT</pubDate>
    </item>
    <item>
      <title>Furuta Pendulum：驱动臂的稳态误差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i09q3e/furuta_pendulum_steady_state_error_for_actuated/</link>
      <description><![CDATA[      https://preview.redd.it/k0ehn5qd4qce1.png?width=258&amp;format=png&amp;auto=webp&amp;s=549e3296edcc7c4b10a343a83ecdba3e3bf6b798 大家好！我训练了一个 furuta 摆锤摆动并保持平衡，但我无法将臂角度的稳态误差降至零，您是否知道为什么策略认为这是合适的，即使角度 theta 在奖励中反映如下：-factor * (theta)^2。  - k_1 (q_1 alpha^2+q_2 theta^2+q_3\dot\alpha^2+q_4\dot\theta^2+r_1 u_{k-1}^2+r_2(u_{k-2}-u_{k-1})^2) + Psi \\ Psi = k_2 \abs{\theta}&lt; \theta_{max} \wedge {\dot\theta}&lt;\dot\theta_{max} \\ 0 else    由    /u/Fit-Orange5911  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i09q3e/furuta_pendulum_steady_state_error_for_actuated/</guid>
      <pubDate>Mon, 13 Jan 2025 08:43:40 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 重播缓冲区容量问题：需要 229GB RAM 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i08xn2/dreamerv3_replay_buffer_capacity_issue_229gb_ram/</link>
      <description><![CDATA[大家好， 我正在尝试运行 DreamerV3 代码，但由于重放缓冲区的容量而遇到了 MemoryError。论文中将容量指定为 5,000,000，当我尝试复制它时，它需要 229GB 内存，这显然远远超出了我机器的 RAM（我有 31GB RAM，GPU：RTX3090）。 让我感到困惑的是：  其他人如何使用这种配置运行代码？ 我在优化方面是否遗漏了什么，或者人们通常会修改容量以适应他们的硬件？  如果您能提供任何关于如何在不遇到内存问题的情况下使其正常工作的见解或提示，我将不胜感激。提前致谢！😊   由    /u/DRLC_  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i08xn2/dreamerv3_replay_buffer_capacity_issue_229gb_ram/</guid>
      <pubDate>Mon, 13 Jan 2025 07:42:24 GMT</pubDate>
    </item>
    <item>
      <title># RL 实习生或教育机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i06yc4/rl_intern_or_educational_opportunity/</link>
      <description><![CDATA[过去 8 个月，我一直在从三个主要方向研究 RL：数学观点、计算机科学观点（算法 + 编码）和神经科学（或心理学）观点。凭借近 5 年的编程经验以及过去 8 个月的理解，我可以自信地说 RL 就是我想要终生追求的。最大的问题是我目前不在任何学习机构，也没有技术工作来获得任何实习或教育机会。我积极性很高，每天花大约 5-6 个小时学习 RL，但我觉得这一切都是在浪费时间。你们建议我做什么？我目前住在加拿大温哥华，我是一名寻求庇护者，但有工作许可，我有资格就读教育机构。    提交人    /u/Easy-Quail1384   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i06yc4/rl_intern_or_educational_opportunity/</guid>
      <pubDate>Mon, 13 Jan 2025 05:25:33 GMT</pubDate>
    </item>
    <item>
      <title>强化学习论文实现的最佳仓库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i04eb2/best_repo_for_rl_paper_implementations/</link>
      <description><![CDATA[我正在寻找一些最新的 RL 论文的实现。    提交人    /u/research-ml   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i04eb2/best_repo_for_rl_paper_implementations/</guid>
      <pubDate>Mon, 13 Jan 2025 03:00:47 GMT</pubDate>
    </item>
    <item>
      <title>Sutton Barto 的策略梯度定理证明步骤 4</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzpecq/sutton_bartos_policy_gradient_theorem_proof_step_4/</link>
      <description><![CDATA[      我正在检查 sutton 的书中策略梯度定理证明。我不明白 r 是如何从步骤 3 过渡到步骤 4 时消失的。r 不是依赖于动作而又依赖于参数吗？ https://preview.redd.it/i3lszgko2lce1.jpg?width=1910&amp;format=pjpg&amp;auto=webp&amp;s=5dd375d3a0241c01e4240e9116fc659f17ce695f   由    /u/demirbey05  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzpecq/sutton_bartos_policy_gradient_theorem_proof_step_4/</guid>
      <pubDate>Sun, 12 Jan 2025 15:44:07 GMT</pubDate>
    </item>
    <item>
      <title>博士毕业后的 RL 工程师职位</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzm18z/rl_engineer_jobs_after_phd/</link>
      <description><![CDATA[大家好， 我希望今年能获得博士学位。 我的博士最终目标是设计一个智能电网问题并用 RL 解决它。 我对 RL 的兴趣与日俱增，我想进一步提高我的技能。 你能指导我我在爱尔兰或其他国家有哪些求职选择吗？ 此外，在毕业之前我应该​​尝试涵盖 RL 的哪些主要领域？ 提前谢谢您。    提交人    /u/Dry-Image8120   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzm18z/rl_engineer_jobs_after_phd/</guid>
      <pubDate>Sun, 12 Jan 2025 12:52:51 GMT</pubDate>
    </item>
    <item>
      <title>最好的强化学习书籍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzkl2u/best_reinforcement_learning_book/</link>
      <description><![CDATA[我很好奇 RL 的最佳书籍是什么？我希望找到一些主要用 C 语言、其他 C 语言或 Python 编写的书籍。我查过亚马逊，但像往常一样，编程书籍只字未提该语言。 这是我尝试做的一件事的示例：https://www.youtube.com/watch?v=8oIQy6fxfCA    提交人    /u/luddens_desir   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzkl2u/best_reinforcement_learning_book/</guid>
      <pubDate>Sun, 12 Jan 2025 11:15:43 GMT</pubDate>
    </item>
    </channel>
</rss>