<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 02 Jun 2024 15:13:46 GMT</lastBuildDate>
    <item>
      <title>自我游戏、经验重放和稀疏奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d699fn/selfplay_experience_replay_and_sparse_rewards/</link>
      <description><![CDATA[我有几个问题。如果我们在 2 人游戏环境中使用 +1 表示赢，-1 表示输，该游戏会自我训练。我应该如何处理奖励为 0 的先前步骤？ 我应该将奖励为 0 的步骤放入经验重播中吗？    提交人    /u/erenpal01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d699fn/selfplay_experience_replay_and_sparse_rewards/</guid>
      <pubDate>Sun, 02 Jun 2024 09:11:01 GMT</pubDate>
    </item>
    <item>
      <title>需要有关快速消费品公司中 RL 在目标订单数量预测中的实施方式方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d62bqr/help_needed_regarding_way_of_implementation_of_rl/</link>
      <description><![CDATA[我最近被一家快速消费品公司选为实习生，他们要求我实施一种 RL 算法，该算法可帮助他们根据前 6 个月总订单量的销售数据分析来预测下个月的目标订单量 (TOQ)，同时测试当前月份的数据。  目前，他们正在实施一个 XGBRegressor 模型，他们说对于某些门店，该模型的预测还可以，对于某些门店，预测超出预期，而对于某些门店，预测低于预期。 除此之外，如果有人可以给我提供关于如何解决这个问题的粗略想法，以及是否有与此相关的论文或资源可以阅读，那就太好了。 提前致谢。    提交人    /u/wandering_soul_420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d62bqr/help_needed_regarding_way_of_implementation_of_rl/</guid>
      <pubDate>Sun, 02 Jun 2024 01:39:08 GMT</pubDate>
    </item>
    <item>
      <title>关于 DQN 的一些问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d60zdj/a_number_of_questions_re_dqn/</link>
      <description><![CDATA[因此，我正在尝试理解一些基本的 ML 概念 - 你知道，要具备一定的实用知识，以防在日常工作中遇到。 我正在尝试使用 TensorFlow 实现 DQN 来玩 Connect 4 - 只是一个玩具项目。我的代码目前位于 https://gist.github.com/lkingsford/a02cada950e2c911a02b7b1d76789ee5。这不是我的常用方法 - 但我使用 ChatGPT 部分来帮助处理 ML 位块（这自然需要进行相当多的更改才能真正发挥作用）。我会注意到代码已经可以运行了 - 它可以训练模型，并且相当快。 我知道我稍后会有更多的问题，因为我对这些概念有了更好的理解 - 但我有几个关于我可能做错的问题。 Zerothly - 我做的对吗？DQN 实际上适合这种用途吗？感觉无状态可能会限制我在编写游戏 AI 时的能力。 首先 - 我的 GPU 使用率真的很低。当 epsilon 很高，并且大多数操作都是随机的时，我在实例上获得了约 10% 的 GPU。当它开始预测更多动作时，它会下降到约 3%。我也没有使用特别强大的实例进行训练。我应该考虑线程吗？运行异步吗？我应该做些什么来更快地提供给 GPU？我看到了关于批处理的建议 - 但我不确定当有另一个需要响应的操作时它是如何工作的。 其次 - 我如何处理失败的奖励？如果游戏仍在进行，我的奖励函数返回 0，如果僵局，则返回 -2，如果获胜，则返回 10。但是，奖励函数仅在采取行动时奖励。如果 DQN 采取了获胜的行动，那么根据我的理解，导致其他玩家失败的一系列行动也应该有负奖励，因此预测可以更好地学习“如果我给其他玩家留下获胜的机会，我就会输”，而不仅仅是“如果我把石头放在这里，我就会赢”。 第三 - 我如何决定密集层的数量以及它们有多少个神经元？它是由随时可用的不同动作数量决定的吗？战略复杂性的数量，以及奖励需要多长时间？比如，如果我在制作《芝加哥快车》——它有一个大状态，以及更广泛的潜在行动，在你获得移动奖励之前有许多行动，在你发现自己是否真的赢之前有许多行动（至少与奖励很好地对应）——如果进展不顺利，我该如何确定是否应该增加神经元或层数？ 最后——我认为现在——我想我对正在发生的事情有一个非常基本的了解，但感觉有点黑箱。我从事软件工作已经很长时间了，以至于我不擅长魔法，特别是当我看不到内部结构的时候。你会去哪里学习第一性原理，而不是仅仅使用库来处理一堆我仍在尝试理解的东西？比如，我应该从头开始尝试一些基本的机器学习吗？还是找一本更概念化、理论化的书？ 感谢任何人提供的帮助。对这些特殊的黑匣子仍然很陌生，觉得有点费脑力。    提交人    /u/thelochok   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d60zdj/a_number_of_questions_re_dqn/</guid>
      <pubDate>Sun, 02 Jun 2024 00:26:04 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习理论和优化的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d5wy97/papers_regarding_rl_theory_and_optimization/</link>
      <description><![CDATA[嗨！我是一名一直在研究 RL 应用和实现的本科生。最近，我对多智能体和更实际的模拟越来越感兴趣，这些模拟的复杂程度越来越高。因此，我真的想尽可能地提高性能，以最大限度地缩短收敛时间和重新运行时间。 因此，我想联系一下，询问是否有人有最前沿的优化论文来源或链接，以及如何有效地训练 RL 模型。我浏览过许多旧资源，想看看当前算法或技术的最佳 SOTA 是什么。如果您有任何想法，请告诉我！    提交人    /u/anishfish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d5wy97/papers_regarding_rl_theory_and_optimization/</guid>
      <pubDate>Sat, 01 Jun 2024 21:10:19 GMT</pubDate>
    </item>
    <item>
      <title>“DeTikZify：使用 TikZ 合成用于科学图形和草图的图形程序”，Belouadi 等人 2024（用于编写 Latex 编译为所需图像的 MCTS）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d5vjru/detikzify_synthesizing_graphics_programs_for/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d5vjru/detikzify_synthesizing_graphics_programs_for/</guid>
      <pubDate>Sat, 01 Jun 2024 20:04:15 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习教程，教类人机器人在 mujoco 中接受和停止惩罚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d5h1y0/multi_agent_rl_tutorial_to_teach_humanoids_to/</link>
      <description><![CDATA[您好，刚刚在 github 上发布了一个新的教程。仓库 - https://github.com/goncalog/ai-robotics。教程 - https://github.com/goncalog/ai-robotics/blob/main/tutorials/penalties.ipynb 如果您想在点球大战中测试您的人形机器人与我的机器人，请告诉我    提交人    /u/goncalogordo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d5h1y0/multi_agent_rl_tutorial_to_teach_humanoids_to/</guid>
      <pubDate>Sat, 01 Jun 2024 06:51:45 GMT</pubDate>
    </item>
    <item>
      <title>谢尔盖·莱文 (Sergey Levine) 是 OP 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d5gsr7/is_sergey_levine_op/</link>
      <description><![CDATA[  由    /u/Sea-Collection-8844  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d5gsr7/is_sergey_levine_op/</guid>
      <pubDate>Sat, 01 Jun 2024 06:34:09 GMT</pubDate>
    </item>
    <item>
      <title>AI 学习伙伴小组</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d55xva/ai_study_buddies_group/</link>
      <description><![CDATA[嗨，我已经在类似的 AI 页面上发布过这篇文章，但对你们中的一些人可能有用。我为想要进入该领域或已经有 AI 经验的人创建了一个 AI 学习小组。如果想学习，欢迎所有人加入。这里有一些机器学习、神经网络、机器学习数学、深度学习、Pytorch 和路线图的资源。discord 服务器的链接在这里 - https://discord.gg/cz7jatjcEj    提交人    /u/MrMyagi007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d55xva/ai_study_buddies_group/</guid>
      <pubDate>Fri, 31 May 2024 20:54:21 GMT</pubDate>
    </item>
    <item>
      <title>截至 2024 年，SOTA 离线 RL 方法有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d555ws/what_are_the_sota_offline_rl_methods_as_of_2024/</link>
      <description><![CDATA[我的列表包括保守 Q 学习 (CQL)、隐式 Q 学习 (IQL) 和基于模型的离线强化学习 (MORel)。还有什么值得注意的？    提交人    /u/mziycfh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d555ws/what_are_the_sota_offline_rl_methods_as_of_2024/</guid>
      <pubDate>Fri, 31 May 2024 20:20:24 GMT</pubDate>
    </item>
    <item>
      <title>如何让代理商在信息完美的情况下避开障碍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d4y82h/how_to_get_an_agent_to_avoid_obstacles_with/</link>
      <description><![CDATA[我创建了一个简单的域，代理必须从点 A 移动到点 B，这些点是随机选择的。此外，还有几十个障碍物/红色圆圈，如果代理触碰它们，它们会得到很大的负奖励，并且情节结束。代理因达到目标而获得很大的正奖励，情节结束。 此外，在每个时间步骤中，代理都会收到 -0.001 * [代理完成的距离] 以鼓励快速解决方案。在没有障碍的场景中，代理可以使用 PPO 学会顺利到达目标。但是，如果有障碍，如果障碍惩罚太高，代理最终会学会保持静止。否则，即使经过几百万个训练步骤，代理有时也会撞到障碍物或表现出其他奇怪的行为（奇怪的回溯、圆圈）。 代理可以看到其坐标、目标坐标、到目标的距离以及到最近障碍物的距离。我该如何解决这些问题？谢谢！    由   提交  /u/spectraldecomp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d4y82h/how_to_get_an_agent_to_avoid_obstacles_with/</guid>
      <pubDate>Fri, 31 May 2024 15:23:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么 D4PG 的出色表现被放弃而优先考虑 SAC？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d4ug2m/why_d4pg_outstanding_performance_was_dropped/</link>
      <description><![CDATA[如果看一下 D4PG 的速度/稳定性，它会获得最高分数，并且像闪电一样稳定，但奇怪的是 SAC 更受欢迎，你认为为什么会这样？    提交人    /u/Timur_1988   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d4ug2m/why_d4pg_outstanding_performance_was_dropped/</guid>
      <pubDate>Fri, 31 May 2024 12:30:03 GMT</pubDate>
    </item>
    <item>
      <title>rl 对图像数字进行训练->真实生活表现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d4lqen/rl_training_on_images_digital_real_life/</link>
      <description><![CDATA[只是想知道是否有人知道像在游戏像素上训练的 RL 模型这样的东西会如何转化为他们在现实生活中看到的游戏图像/视频，即可能受到照明的影响，相机和屏幕之间的角度不完美，屏幕不能完美地填充框架，边缘有一些东西，因此物理像素不在完全相同的位置，也不是 RL 模型期望的完全相同的色调。 该模型仍然能够表现不错吗？还是只是疯了？    提交人    /u/disastorm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d4lqen/rl_training_on_images_digital_real_life/</guid>
      <pubDate>Fri, 31 May 2024 03:05:27 GMT</pubDate>
    </item>
    <item>
      <title>PPO 陷入局部最优</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d45d3f/ppo_stucks_in_local_optima/</link>
      <description><![CDATA[我为一款基于网格的俄罗斯方块类游戏编写了自己的自定义 Gymnasium 环境。它只有 10x10，非常小。有大约 35 种不同的方块（包括旋转方块）。方块可以放置在任何地方。清除一行/一列后，它们将被清空。可以放置 3 个随机块，完成后，将选择 3 个新块。 我的观察空间是：10x10 = 100 个网格 35 = 块可用性 1 = 乘数 1 = 足够的空间容纳巨大的块 1 = 可能通过插槽 1 中的块清除 1 = 可能通过插槽 2 中的块清除 1 = 可能通过插槽 3 中的块清除 10 = 网格的高度图（每行的总和）所以 150 个观察 动作空间大小为 300（3 个块 * 100 个单元格），编码为单个值 0-300。（0-100：将块放置在位置 XY 等） 放置一个方块会获得微小的奖励。清除一行/一列的得分要高得多（组合甚至更多）。 现在，我正在 GPU（3080）上同时使用 sb3 contrib 中的 MaskablePPO 和 16 个 Envs。 它运行良好，并且代理能够学习基础知识并随着时间的推移达到稳定的分数。 但是当我观看代理玩游戏时，似乎它不会学习一种策略来构建块以一次或一个接一个地清除多个行/列以获得更高的高分。 我甚至增加了将块放置在其他地方而不是清除的惩罚，那时游戏就会结束（没有可放置的块）。 我稍微更改了 PPO 超参数，例如 n_steps 64 batch_size 512 ent_coef 0.02 limit kl 0.02 gamma 0.95 学习率是从 6e-4 到 0 的线性时间表 我也在使用FrameStacking 总共有 9 个观测值。我也让它运行了 30 分钟，但大约 3 分钟后它已经达到了最佳状态。我也尝试过改变超参数，但它要么变得更糟，要么或多或少得到相同的结果。 还有什么我可以做的来帮助代理学习更深层次的策略吗？     提交人    /u/Maxxxel   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d45d3f/ppo_stucks_in_local_optima/</guid>
      <pubDate>Thu, 30 May 2024 14:27:59 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：梯度计算所需的变量之一已被就地操作修改</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d44u7k/runtimeerror_one_of_the_variables_needed_for/</link>
      <description><![CDATA[您好， 我真的需要您的帮助。 我有一个包含四个部分的 DQN 网络：一个共享网络和三个网络分支作为输出。我们的想法是分别更新每个部分的参数（shared_network 和网络分支）。我成功更新了前三个部分，但最后一个部分出现了问题，如下所示： “RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [64, 192]]，它是 AsStridedBackward0 的输出 0，处于版本 2；预期为版本 1。” 您有什么解决方案吗？我会全部尝试。    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d44u7k/runtimeerror_one_of_the_variables_needed_for/</guid>
      <pubDate>Thu, 30 May 2024 14:04:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么 T 是一个固定数字 作者：Sergey Levine</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3v9zv/why_is_t_a_fixed_number_by_sergey_levine/</link>
      <description><![CDATA[我是一名数学系学生，Sergey 的讲座让我感到困惑。在他的讲座中，他声称 T 是一个固定的常数，如果存在平稳分布，则 T 可以是无穷大。但是，我认为状态的值自然取决于时间步长。但他从未在值函数中写下标 t。他总是写 V(s_t)，我相信这意味着 V 不依赖于 t，因为 s_t 在评估时将被实际状态替换。为什么这有意义？ 在我读过的 RL 理论论文中，它几乎总是有限视界时间相关 MDP。事情非常清楚。 在 Sutton 的书中（我猜 Silver 的讲座也隐含地这样做了），T 被定义为依赖于实际推出的随机变量。诸如价值函数之类的东西可以通过无限和进行明确定义，如果我们想要有限视界 MDP，\gamma 可以是 1，我们可以假设一个终端状态。有了这个符号，我同意 V 不需要依赖于 t，因为它可以由相应的无限和来定义。    提交人    /u/mziycfh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3v9zv/why_is_t_a_fixed_number_by_sergey_levine/</guid>
      <pubDate>Thu, 30 May 2024 04:05:57 GMT</pubDate>
    </item>
    </channel>
</rss>