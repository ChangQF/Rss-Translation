<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 24 Dec 2024 06:24:08 GMT</lastBuildDate>
    <item>
      <title>“最大扩散强化学习”，Berrueta 等人 2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hl3au6/maximum_diffusion_reinforcement_learning_berrueta/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hl3au6/maximum_diffusion_reinforcement_learning_berrueta/</guid>
      <pubDate>Tue, 24 Dec 2024 02:05:50 GMT</pubDate>
    </item>
    <item>
      <title>评论家（在演员-评论家模型中）如何从动作和状态中学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hl2l9h/how_does_the_critic_in_an_actorcritic_model_learn/</link>
      <description><![CDATA[大家好， 我看到很多 Critic 的实现都涉及使用与 Actor 相同的架构。如何将动作和状态视为相同的信息？ 我也见过使用 CNN + RNN 的例子，将动作 + 前一层的输出作为输入，但没有关于它的论文。有没有人碰巧有一篇关于这种架构的论文可以让我读一读，或者只是讨论处理动作 + 状态输入？ 非常感谢    提交人    /u/Sea_Farmer5942   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hl2l9h/how_does_the_critic_in_an_actorcritic_model_learn/</guid>
      <pubDate>Tue, 24 Dec 2024 01:26:53 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习对 LLM 进行微调，以说服受害 LLM 选择错误的答案。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkvv2i/fine_tuning_an_llm_using_reinforcement_learning/</link>
      <description><![CDATA[我在这里写信是因为我需要帮助完成一个我不知道如何开始的大学项目。 我想这样做：  获取一个包含问题和多个答案的琐事数据集。需要知道正确答案。 对于每个问题，使用随机 LLM 生成一些中性上下文，提供有关该主题的一些信息，但不透露正确答案。 对于每个问题，选择一个错误的答案并指示本地 LLM 使用该上下文编写叙述，以说服受害者选择该答案。 将问题、上下文和叙述发送给受害者 LLM，并要求其仅根据我发送的内容选择一个选项。 如果受害者 LLM 选择了正确的选项，则不给予任何奖励。如果受害者选择了任何错误的选项，则向本地 LLM 提供一半的奖励。如果受害者选择了目标错误选项，则向本地 LLM 提供全额奖励  这应该会让我训练一个“欺骗者”LLM，试图说服其他 LLM 选择错误的答案。它可以撒谎并捏造事实和研究论文以说服受害者 LLM。 正如我所说，这是一个大学项目，但我从未做过任何与 LLM 或强化学习有关的事情。有人可以给我指明正确的方向并提供支持吗？我发现 huggingface 的 TRL 等库似乎很有用，但我以前从未使用过 pytorch 或类似的东西，所以我真的不知道如何开始。    提交人    /u/XLNBot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkvv2i/fine_tuning_an_llm_using_reinforcement_learning/</guid>
      <pubDate>Mon, 23 Dec 2024 19:53:16 GMT</pubDate>
    </item>
    <item>
      <title>“通过深度强化学习学习协作视觉对话代理”，Das 等人 2017 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hku11e/learning_cooperative_visual_dialog_agents_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hku11e/learning_cooperative_visual_dialog_agents_with/</guid>
      <pubDate>Mon, 23 Dec 2024 18:27:43 GMT</pubDate>
    </item>
    <item>
      <title>请推荐适合我的环境的模型选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hktor8/please_recommend_model_choices_for_my_environment/</link>
      <description><![CDATA[我正在为大学开展一个 RL 项目，我们应该训练一个在简单的 1v1 环境中打曲棍球的代理。观察是一个具有 18 个值的 1d 向量，而不是帧图像。动作空间由 4 个值组成，其中前 3 个是连续的（水平/垂直移动和旋转），最后一个也是连续的，但实际上只是一个阈值 0.5（持球或射门）。奖励是通过射门（游戏结束）给予的，但如果冰球正朝着你的一侧/球门移动，则在每一帧中靠近冰球也会给予奖励。我们已经实现了 sac，我想知道是否还有其他有前途的方法可以胜过它。我想实现一个梦想家类型的网络，但当观察空间如此之小的时候，这并不是很理想，对吧？    提交人    /u/dotaislife99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hktor8/please_recommend_model_choices_for_my_environment/</guid>
      <pubDate>Mon, 23 Dec 2024 18:11:40 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗控制（或一般基于 MC 的 RL 算法）的用例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkn2xn/use_cases_of_monte_carlo_control_or_mcbased_rl/</link>
      <description><![CDATA[问题在主题中。目前，基于 MC 的 RL 算法是否（仍然）有任何流行的用例，其表现优于基于 TD 学习的 SOTA RL 方法？该类别的研究进展如何？    提交人    /u/Intelligent-Put1607   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkn2xn/use_cases_of_monte_carlo_control_or_mcbased_rl/</guid>
      <pubDate>Mon, 23 Dec 2024 12:50:17 GMT</pubDate>
    </item>
    <item>
      <title>GPT 1 培训</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkm8ba/gpt_1_training/</link>
      <description><![CDATA[有人能告诉我训练 GPT 1 花了多长时间、使用了多少 GPU 以及进行了多少个 epoch 吗？    提交人    /u/notanhumanonlyai25   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkm8ba/gpt_1_training/</guid>
      <pubDate>Mon, 23 Dec 2024 11:56:42 GMT</pubDate>
    </item>
    <item>
      <title>通过仅玩游戏的最后步骤来伪造课程学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkm1y5/faking_curriculum_learning_by_in_self_play_by/</link>
      <description><![CDATA[我正在尝试教 ppo 玩各种游戏，当游戏很复杂、奖励稀少且完全是最终奖励时，建议进行课程学习。  如果游戏有固定的长度，比如说 5000 步，我可以通过在每个场景中随机播放前 4500 步来开始训练，然后让 PPO 播放最后 500 步，并且在训练进行的同时，我让 ppo 越来越早地开始玩，直到它自己玩整个游戏？ 当然，如果游戏的最后步骤在很大程度上取决于早期游戏，那么网络直到训练的很后期才会看到各种游戏状态，但对于具有重复/周期性结构的游戏来说，这应该没问题，比如说开始。 我的想法正确吗？这种技术有名字吗？   由    /u/drblallo  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkm1y5/faking_curriculum_learning_by_in_self_play_by/</guid>
      <pubDate>Mon, 23 Dec 2024 11:44:24 GMT</pubDate>
    </item>
    <item>
      <title>我通过强化学习和训练构建了一个玩《黑暗之魂》的人工智能。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkk837/i_built_a_ai_to_play_dark_souls_through/</link>
      <description><![CDATA[你好， 我构建了一个可直接与 Dark Souls 交互并玩游戏的 AI。Dark Souls 没有 API，因此这是一个持续的复杂过程，需要反复尝试。 到目前为止，该过程已取得良好结果，尤其是对于在非常大且复杂的环境中盲目运行且奖励稀少的代理而言。 为了促进 AI，我设计了一个非常大且量身定制的奖励塑造框架，专门针对 Dark Souls 环境，模拟类似 API 的奖励结构以进行指导和发展。正如人们所说，罗马不是一天建成的，但它带来了数次飞跃的进步和突发行为。 我还设计了两个新系统，试图帮助指导代理并促进学习和进步。  第一个方法称为 Vivid，这个过程允许代理直接从视频输入中学习，比如它所在精确区域的专业演练。这种方法跳过了传统的图片和数据文件帧提取过程，而是从直接视频帧中学习，提高了映射到动作和奖励结构的效率和准确性。  第二个方法称为 TGRL（文本引导强化学习），它允许代理直接从基于文本的演练中学习，该演练以基于脚本的步骤解析信息，通过关键词检测和动作映射进行上下文排序，并与代理跟随和学习的奖励结构相关联。  到目前为止，它已经在代理和进展中产生了一些有趣的结果和行为变化。  有一次，它甚至在游戏中执行了一个我从未遇到过或知道可能做到的动作，也没有在其他任何地方见过。  我当前的挑战是指导。虽然目前的奖励结构运行良好，但代理仍然处于反复试验的环境中，没有像 API 那样明确的游戏进程统一性方向。  如果有人对如何让代理在游戏中“定向移动”（应该是这样）以减少随机性有任何建议，我很乐意接受帮助。  当前进度包括：  挑选第一个牢房钥匙 打开第一个牢房门 杀死前三个被动空洞 成功爬上第一个梯子  下一个预期进度：  在第一堆篝火旁点燃并休息 进入并导航第一个 Boss 竞技场  可以执行游戏中的所有操作。菜单导航、设备导航和升级机制尚未设计或实施。     由    /u/UndyingDemon 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkk837/i_built_a_ai_to_play_dark_souls_through/</guid>
      <pubDate>Mon, 23 Dec 2024 09:28:52 GMT</pubDate>
    </item>
    <item>
      <title>具身沟通游戏：强化学习代理的任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkib91/the_embodied_communication_game_a_task_for/</link>
      <description><![CDATA[我一直在为我的数据科学硕士学位开展一个独立研究项目，我偶然发现了一个非常有趣的多智能体游戏，该游戏是在本文中设计和运行的（在人类身上）：https://thomscottphillips.wordpress.com/wp-content/uploads/2014/08/scott-phillips-et-al-2009-signalling-signalhood.pdf 这个游戏本质上是这样设计的：两个玩它的智能体必须协调他们各自对世界的观察所分配的信息。要想 100% 准确地做到这一点，唯一的办法就是根据玩家在游戏空间中的行为设计和使用通信系统。人类能够在这款游戏中进行交流，并实现 100% 的成功率（尽管并非每对人类都能做到这一点），而无需事先知道交流是否必要甚至可能。 我在 Gymnasium 用 PettingZoo 和 SuperSuit 的帮助设计了具身交流游戏 (ECG)，并最终在原始游戏和简化版本中训练了一些 Stable-Baselines3 RL 模型。我写了一篇论文，详细介绍了我所做的努力和取得的成果（毫不奇怪，对于模型之间紧急通信的结果持负面态度），虽然它还没有达到科学论文的水平，但我为撰写它而感到自豪，并且愿意接受批评和评论，所以我想我会把它发布在这里。 我也很好奇你们中是否有人将这个问题（如上面链接的论文中概述的具身通信游戏）作为多智能体 RL 问题来处理？这是一个非常有趣的问题，而且似乎很有可能用当前 RL 模型的某些版本来解决。 这是我关于 ECG 的论文的链接： https://evanmccormick37.github.io/independent-study-F24-learning-RL-with-gymnasium/    提交人    /u/EvanMcCormick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkib91/the_embodied_communication_game_a_task_for/</guid>
      <pubDate>Mon, 23 Dec 2024 07:00:22 GMT</pubDate>
    </item>
    <item>
      <title>使用行为策略时在 PPO/TRPO 中正确实现 Rollout</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hki1ob/correct_implementation_of_rollout_in_ppotrpo_when/</link>
      <description><![CDATA[      您好， 在 TRPO 论文的公式 14 中，作者建议可以使用不同的分布来对动作进行采样  https://preview.redd.it/h1hb15abnj8e1.png?width=259&amp;format=png&amp;auto=webp&amp;s=26e87c810443f4fd10692175f7d5cc631d30ca7a 如果我按照均匀分布对动作进行采样，其中 q 不是 pi_old，那么我该如何计算 Q_theta_old？这个 Q 不是来自 pi_old 吗？我应该将 Q 乘以重要性比率吗？我很困惑。本质上，我正在尝试使用均匀分布（基于本文后面提到的 Vine 方法）实现简短的推出，但优势函数或 Q 函数基于旧策略，而动作是从行为函数 q 中采样的，因此使用此动作计算的优势是否属于旧策略或行为函数 q ？    提交人    /u/gtm2122   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hki1ob/correct_implementation_of_rollout_in_ppotrpo_when/</guid>
      <pubDate>Mon, 23 Dec 2024 06:41:26 GMT</pubDate>
    </item>
    <item>
      <title>具有 O（log（T））遗憾界限的随机强盗算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk9kl7/stochastic_bandit_algorithms_with_ologt_regret/</link>
      <description><![CDATA[是否有任何随机老虎机算法具有比 O(sqrt(T)) 更好的遗憾界限？不是 O( \sqrt(T log T) )，而是 O( \log(T) ) 或至少 O( sqrt(log T) )？ 我知道具有 K 个臂的 UCB 具有 O( \sqrt(kT) ) 的遗憾界限。它是否表明 UCB 或其变体可以实现 O( log T ) 遗憾，或者至少 O( sqrt(log T) )？    提交人    /u/Anxious_Positive3998   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk9kl7/stochastic_bandit_algorithms_with_ologt_regret/</guid>
      <pubDate>Sun, 22 Dec 2024 22:34:42 GMT</pubDate>
    </item>
    <item>
      <title>SAC 中的 Pytorch 梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk8wlz/pytorch_gradients_in_sac/</link>
      <description><![CDATA[我在理解如何在 pytorch 中实现软演员评论家时计算梯度时遇到了麻烦。具体来说，多个神经网络的梯度是如何耦合的，以及何时计算梯度，何时不计算梯度。 我有一个基本的实现，与 Phil Tabor 的 YouTube 视频非常相似。我的实现是这里。我的问题是我不明白 `retain_graph=True` 字段是如何工作的。我在这个实现中也没有使用 .detach() 或 torch.no_grad。也许这是一个单独的问题，但是如果我只是要在调用 .backwards() 之前将梯度归零，那么执行 .detach() 有什么意义呢？ 我的目标是执行相同的实现，但不使用 `retain_graph=True` 参数，因为我不完全理解它的用途。我知道它在反向传播后会保持梯度，但我不明白在 SAC 中这样做的目的。我尝试在没有这个参数的情况下执行此操作，但是，我就是无法让它工作。该代码可以在此处看到。 任何帮助都将不胜感激！    提交人    /u/LostBandard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk8wlz/pytorch_gradients_in_sac/</guid>
      <pubDate>Sun, 22 Dec 2024 22:02:20 GMT</pubDate>
    </item>
    <item>
      <title>训练模型来学习在大规模离散动作空间中进行跟踪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk1pjv/training_a_model_to_learn_tracing_on_massive/</link>
      <description><![CDATA[大家好！我正在开展一个实验性的强化学习项目，用于 PCB 布线。该项目的理念是训练强化学习代理连接巨大的 2D 电路板上的输入和输出点（始终位于中间），电路板可能大到 20,000×20,000 个单元，并带有 3 像素宽的迹线、10 像素间距、不可通行点等限制。 目前，我正在尝试尽可能简化问题，减少限制，但仍打算在大型电路板上进行训练。现在，我只想让模型学习如何从随机输入/输出对中跟踪一条线，即使这样也相当具有挑战性。 它基本上感觉像是一种寻路或“贪吃蛇游戏”设置，但我不确定强化学习是否真的可以处理这种大小的输入。我还没有找到任何类似的项目可以比较，所以我怀疑 RL 是否是合适的工具。 我尝试过 DQN 方法（很快就被丢弃了，在 50x50 以上的棋盘上很吃力），基于 CNN 的方法（在大型棋盘上很吃力），现在我正在探索分层 RL。这感觉最有前途，但我仍然不确定它的扩展性如何。 我是这个领域的初学者，之前主要解决的是一些较小的问题。任何建议或参考都将不胜感激！ 谢谢！    提交人    /u/Deranged_Koala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk1pjv/training_a_model_to_learn_tracing_on_massive/</guid>
      <pubDate>Sun, 22 Dec 2024 16:19:03 GMT</pubDate>
    </item>
    <item>
      <title>如何学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk03kn/how_to_learn_reinforcement_learning/</link>
      <description><![CDATA[您好。我是一个编程经验超过 40 年的老家伙，想学习更多关于强化学习的知识，也许可以用强化学习编写一个简单的游戏，比如跳棋。 我想更好地理解强化学习的数学。我已经几十年没有接触过微积分了，但我相信只要努力，我就能学会。而且，我更喜欢亲自动手做一些编码的事情，以证明我确实理解了我正在学习的内容。 我在网上看过一些教程，它们似乎都使用了一些 RL 库，我假设这些库只是封装并隐藏了实际的数学知识，或者它们是对数学的高级讨论。 在哪里可以找到在线或书籍形式的关于理论和数学或机器学习的讨论，并在编程世界中进行应用练习？    提交人    /u/EricTheNerd2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk03kn/how_to_learn_reinforcement_learning/</guid>
      <pubDate>Sun, 22 Dec 2024 14:59:56 GMT</pubDate>
    </item>
    </channel>
</rss>