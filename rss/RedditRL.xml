<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 30 Jan 2025 06:23:09 GMT</lastBuildDate>
    <item>
      <title>与我们都在做的 RL 相比，为什么 LLM 上的 RL 微调如此容易和稳定？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</link>
      <description><![CDATA[我一直在观察不同的人尝试重现 Deepseek 训练配方，与我习惯的 RL 相比，我对这种配方的稳定性感到震惊。 经过大约 50 个训练步骤后，他们在数学问题上的准确率可靠地达到了 50%。他们尝试了几种不同的 RL 算法，并报告说它们的效果大致相同，而无需任何超参数调整。 如果我能在仅 50 个训练步骤中将平衡手推车的成功率提高 50%，我会认为自己很幸运。而且我可能必须为每个任务调整超参数。  （我的理论：由于无监督的预训练，这很容易。该模型已经学习了良好的表示和背景知识 - 即使它无法在 RL 之前完成任务 - 这使得问题变得容易得多。也许我们应该在 RL 中做更多这样的事。）    提交人    /u/currentscurrents   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</guid>
      <pubDate>Wed, 29 Jan 2025 19:31:43 GMT</pubDate>
    </item>
    <item>
      <title>关于连续 Cartpole 的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id1pu3/question_on_continuous_cartpole/</link>
      <description><![CDATA[我修改了 cartpole 环境，让动作空间连续，自然训练时间就长了很多。我用的算法是 A2C，每集更新一次。不知道有没有人用 DDPG 或其他处理连续动作空间的算法建过类似的模型，能加速训练吗？现在解决 cartpole 大概需要 20k 集。    submitted by    /u/Key-Entrance8005   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id1pu3/question_on_continuous_cartpole/</guid>
      <pubDate>Wed, 29 Jan 2025 19:06:29 GMT</pubDate>
    </item>
    <item>
      <title>关于离线强化学习的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icy4si/question_on_offline_rl/</link>
      <description><![CDATA[嗨，我对 RL 还比较陌生，我有一个问题，在离线 RL 中，关键点是我们在各处学习最佳策略。我的问题是，我们是否也在各处学习最佳价值函数和最佳 q 函数？ 具体来说，我想知道如何最好地从离线数据集中仅学习价值函数（不​​一定是策略），并且我想使用离线 RL 工具在各处学习最佳价值函数，但我对要研究什么以了解更多信息感到困惑。我想这样做来学习 V 作为状态的安全指标。 我希望我说得有道理。    提交人    /u/Limp-Ticket7808   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icy4si/question_on_offline_rl/</guid>
      <pubDate>Wed, 29 Jan 2025 16:42:46 GMT</pubDate>
    </item>
    <item>
      <title>我尝试构建一个 alphazero 来掌握井字游戏，但它找不到最佳动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icw7bw/i_tried_to_build_a_alphazero_to_master_tictactoe/</link>
      <description><![CDATA[github: https://github.com/asdeq20062/tictactoe_alphazero.git 这是我的井字游戏 alphazero，但经过这么多次训练，AI 总是在第一个回合移动到中心。最佳移动应该是角落。 有人能帮我检查哪里出了问题吗？谢谢。 main.py -&gt; 此文件是训练的起点     提交人    /u/Upstairs-Lead-2601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icw7bw/i_tried_to_build_a_alphazero_to_master_tictactoe/</guid>
      <pubDate>Wed, 29 Jan 2025 15:22:30 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能随着情节的增加而下降——动作重复和不稳定的奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icsv3j/dqn_performance_drops_with_more_episodes_action/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icsv3j/dqn_performance_drops_with_more_episodes_action/</guid>
      <pubDate>Wed, 29 Jan 2025 12:41:06 GMT</pubDate>
    </item>
    <item>
      <title>谁在奖励 DeepSeek R1？在 RL 中，您需要一些奖励功能或手动奖励，不是吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</link>
      <description><![CDATA[他们说他们没有在大数据上进行自我监督学习，那么奖励模型必须以某种方式在某些数据上进行训练，ChatGPT API 或 LLAMA 可以用作奖励工具，或者谁知道在没有奖励基线的情况下思想链是如何工作的？    提交人    /u/Timur_1988   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</guid>
      <pubDate>Wed, 29 Jan 2025 11:13:03 GMT</pubDate>
    </item>
    <item>
      <title>频繁奖励环境中 Q 值的收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icpjwh/problems_with_convergence_of_qvalues_in/</link>
      <description><![CDATA[大家好 :) 我有一个组合优化问题：这是一个设施位置问题，我需要选择一个可能的位置子集，在每个时间步骤中满足客户的需求。我将其建模为一个图形，其中边缘表示客户和设施之间的距离。在我的例子中，设施是服务器，距离是延迟，目标是在客户在白天移动时最小化延迟。 因此，奖励是（负）延迟。但是，此奖励在每个时间步骤都会立即给出。我已经使用 DQN 进行了离线策略训练，但 q 值似乎无限增长。所以，我根本看不到收敛行为。 我最初的猜测是奖励的频率可能是一个问题，因为每个 Q 值都需要表示所有未来奖励的折扣总和，而奖励频率较低时更容易实现（也许这不是真的，我只是有过这样的经历）。  所以我的问题是，您是否有过类似的经历，即决策总是会立即获得奖励反馈，而对于达到某种“获胜”状态或类似情况则没有“长期”奖励？ 在这种情况下，您的方法是什么？ 祝一切顺利，提前谢谢您 :)    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icpjwh/problems_with_convergence_of_qvalues_in/</guid>
      <pubDate>Wed, 29 Jan 2025 08:50:22 GMT</pubDate>
    </item>
    <item>
      <title>“Robopair：越狱 LLM 控制的机器人”，Robey 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icbl8v/robopair_jailbreaking_llmcontrolled_robots_robey/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icbl8v/robopair_jailbreaking_llmcontrolled_robots_robey/</guid>
      <pubDate>Tue, 28 Jan 2025 20:35:41 GMT</pubDate>
    </item>
    <item>
      <title>一种可能取代 Transformer 的结构 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ic12qo/a_structure_that_potentially_replaces_transformer/</link>
      <description><![CDATA[我有一个替代 Transformer 结构的想法，这里有一个简短的解释。 在 Transformer 架构中，它使用权重来选择值来生成新值，但如果我们这样做，新值就不够精确。  假设输入向量的长度为 N。在此方法中，它首先使用特殊的 RNN 单元遍历序列的所有输入，并生成长度为 M 的嵌入。然后，它使用此嵌入对形状为 (N X N) X M 的矩阵进行线性变换。 接下来，将得到的向量重塑为形状为 N x N 的矩阵。此矩阵是动态的，其值取决于输入，而前一个 (N X N) X M 矩阵是固定且经过训练的。 然后，将所有输入向量与矩阵相乘以输出长度为 N 的新向量。 以上所有步骤是结构的一层，可以重复多次。 经过几层之后，将所有层的输出连接起来。如果您有 Z 层，则新向量的长度将为 ZN。 最后，使用特殊的 RNN 单元处理整个序列以给出最终结果（添加几个 Dense 层之后）。 完整的细节在此代码中，包括 RNN 单元的工作原理以及如何添加位置编码： https://github.com/yanlong5/loong_style_model/blob/main/loong_style_model.ipynb   如果您对该算法感兴趣，请联系我，我的名字是 Yanlong，我的电子邮件是 [y35lyu@uwaterloo.ca](mailto:y35lyu@uwaterloo.ca)    由   提交  /u/Yanlong5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ic12qo/a_structure_that_potentially_replaces_transformer/</guid>
      <pubDate>Tue, 28 Jan 2025 13:04:20 GMT</pubDate>
    </item>
    <item>
      <title>昨天读了这篇文章，我想看看社区对这个 Google Deepmind MONA 的看法。你们觉得怎么样？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibzlpp/read_this_yesterday_and_i_wanted_to_see_what_the/</link>
      <description><![CDATA[        提交者    /u/GreyBamboo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibzlpp/read_this_yesterday_and_i_wanted_to_see_what_the/</guid>
      <pubDate>Tue, 28 Jan 2025 11:36:36 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习和无模型的强化学习有什么区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibyio9/whats_the_difference_between_modelbased_and/</link>
      <description><![CDATA[我试图理解基于模型和无模型强化学习之间的区别。据我所知：  无模型方法直接从真实经验中学习。它们观察当前状态，采取行动，然后以下一个状态和奖励的形式接收反馈。这些模型没有任何内部表示或对环境的理解；它们只是依靠反复试验来随着时间的推移改进其行动。 基于模型的方法则通过创建“模型”或环境模拟来学习。它们不仅仅是对状态和奖励做出反应，还试图模拟未来会发生什么。这些模型可以使用监督学习或学习函数（如 s′=F(s,a)s&#39; = F(s, a)s′=F(s,a) 和 R(s)R(s)R(s)）来预测未来状态和奖励。他们本质上建立了一个环境模型，并用它来规划行动。  因此，关键的区别在于基于模型的方法使用其学习到的模型来近似未来并提前规划，而无模型方法仅通过直接与环境交互来学习，而不尝试模拟它。 这样说对吗，还是我遗漏了什么？    提交人    /u/volvol7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibyio9/whats_the_difference_between_modelbased_and/</guid>
      <pubDate>Tue, 28 Jan 2025 10:21:25 GMT</pubDate>
    </item>
    <item>
      <title>自适应/在线 LQR 的研究合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibw7nq/research_collaboration_in_adaptativeonline_lqr/</link>
      <description><![CDATA[作为我博士研究的一部分，我已经从深度强化学习过渡到探索在线 LQR。具体来说，我一直在深入研究这篇论文中提出的想法。 我已经开发了一些我认为可能非常高效的算法思想。但是，我的背景主要是实践，我缺乏对这些方法进行严格理论分析的理论基础。 如果有人对这个主题感兴趣并希望在理论方面进行合作，我很乐意联系。:)    提交人    /u/riiswa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibw7nq/research_collaboration_in_adaptativeonline_lqr/</guid>
      <pubDate>Tue, 28 Jan 2025 07:21:03 GMT</pubDate>
    </item>
    <item>
      <title>需要项目规划方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibud58/need_help_for_planning_a_project/</link>
      <description><![CDATA[你好， 我需要为我的大学做一个项目。这是一个马尔可夫游戏，我应该对其进行建模然后解决它（使用不同的方法为其找到最优/近乎最优的策略。这是一个双人零和游戏。我可以使用哪些方法来解决它？您通常如何处理这种问题？从哪里开始？我知道如何在博弈论中对其进行建模，但我无法真正使用不同的算法来解决它，无法对它进行良好的可视化等等。    提交人    /u/Upset_Cauliflower320   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibud58/need_help_for_planning_a_project/</guid>
      <pubDate>Tue, 28 Jan 2025 05:16:50 GMT</pubDate>
    </item>
    <item>
      <title>需要基于项目的课程建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibnmmb/need_project_based_courses_recommendations/</link>
      <description><![CDATA[和标题一样多。我想一边做项目一边学习，所以如果有这方面的建议，请告诉我。如果你觉得这可能不是学习 RL 的最佳方法，请再次告诉我     提交人    /u/arrshsh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibnmmb/need_project_based_courses_recommendations/</guid>
      <pubDate>Mon, 27 Jan 2025 23:33:59 GMT</pubDate>
    </item>
    <item>
      <title>GRPO 可以用于多圈 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.03300 有些人可能已经看到了 PPO 的 RL 替代方案，即组相对策略优化 (GRPO)，其中不是训练价值模型，而是多次采样策略，获得平均奖励，并使用它来找出优势。 从审查实现来看，对话中只有一个回合，因为 LLM 要么正确解决数学问题，要么失败，所以在这种情况下奖励和价值是相同的，因为预期的未来奖励就是奖励。 GRPO 是否可以应用于多回合 RL 或更长远的项目，其中策略与环境多次交互？   提交者    /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</guid>
      <pubDate>Mon, 27 Jan 2025 16:05:02 GMT</pubDate>
    </item>
    </channel>
</rss>