<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 18 Jul 2024 01:05:32 GMT</lastBuildDate>
    <item>
      <title>用于混合 MDP 和腿部运动任务的 PPO/SAC 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e5ivvt/pposac_implementation_for_hybrid_mdp_and_for/</link>
      <description><![CDATA[大家好， 我对强化学习领域还很陌生，我很乐意与该领域更有经验的人讨论我在强化学习任务中遇到的问题。 我在这里长话短说，但基本上，我试图以分层方式将模型预测控制与强化学习结合起来。在我看来，MPC 更多地充当参考跟踪等内容的本地控制器，而代理则负责更复杂的推理。具体来说，我的目标是运动任务，目前针对四足机器人。 MPC 允许跟踪扭曲参考并公开一些参数以在运行时选择接触阶段。 现在，正如您所想象的，我的动作空间自然是连续-离散混合，其中代理必须输出扭曲命令和 4 个离散（现在为布尔值）变量来选择是否要步进。 我现在处理这个混合空间的方式是天真地根本不处理它。我正在使用 PPO 和 SAC 的一些自定义实现，并对连续动作空间进行微小修改；然后简单地在环境级别，我根据步进（连续）动作变量的阈值选择是否要步进。 到目前为止，我在训练这种任务时遇到了困难。我开始使用四处走动的机器人获得一些勉强不错的结果，但我觉得这种近似离散动作的方式可能会大大减慢/阻碍收敛。此外，由于某种原因，SAC 比 PPO 困难得多（我已经使用 mujoco 环境测试了我的两个自定义实现，它们都按照 SoA 执行）。 您对此事有何看法？我的方法是否存在我忽略的固有问题？您是否知道我可以测试的混合 MDP 的某些 PPO/SAC/both 实现？    提交人    /u/Majestic-Product1179   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e5ivvt/pposac_implementation_for_hybrid_mdp_and_for/</guid>
      <pubDate>Wed, 17 Jul 2024 13:59:05 GMT</pubDate>
    </item>
    <item>
      <title>观察重要吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e56cwq/does_observation_matter/</link>
      <description><![CDATA[在设计观测时，我认为提供用于奖励函数的状态的完整信息比提供部分信息更好。 但在阅读一些论文时，我发现情况并非总是如此。例如，机器人底座的高度没有作为输入输入到神经网络，但在这篇论文 [1] 中它被用于跟踪高度奖励函数。相反，机器人底座的旋转速度用于观测，但在另一篇论文 [2] 中从未在任何奖励函数中使用过。 那么，设计观测空间的正确方法是什么？或者有“好”的方法吗？ 参考文献： [1] Li, Zhongyu, et al. &quot;Robust and versatile bipedal jump control through reinforcement learning.&quot; arXiv preprint arXiv:2302.09450 (2023)。 [2] Siekmann, Jonah 等人。“通过周期性奖励组合实现所有常见双足步态的模拟到真实学习。”2021 年 IEEE 国际机器人与自动化会议 (ICRA)。IEEE，2021 年。    提交人    /u/Open-Safety-1585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e56cwq/does_observation_matter/</guid>
      <pubDate>Wed, 17 Jul 2024 01:56:50 GMT</pubDate>
    </item>
    <item>
      <title>我的神经进化代理陷入了循环！我错过了什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e4wm1y/my_neuroevolution_agent_is_stuck_in_a_loop_what/</link>
      <description><![CDATA[大家好， 我一直在学习神经进化，灵感来自 The Coding Train 的播放列表，并根据他们的玩具神经网络教程构建了一个 Python 库。为了测试我的理解，我创建了一个简单的 Pygame 环境，其中代理会导航基于网格的地图以到达随机放置的目标板。 目前，我的神经网络设置涉及 4 个输入和 4 个输出。输入是代理和目标板的标准化坐标（例如，[agent_x_norm、agent_y_norm、plate_x_norm、plate_y_norm]）。输入这些输入后，我得到了 [0.482、0.209、0.705、0.791] 之类的输出，它们对应于向上、向下、向左或向右移动等动作。 我尚未实现突变或其他进化机制。我首先测试了前馈机制是否按预期工作。但是，我注意到在单个游戏会话中，代理倾向于重复选择相同的动作。例如，如果输出建议向右移动，代理就会向右移动，但下一个状态只会发生轻微变化，因为只有代理的 x 坐标会发生变化。新输入可能看起来像 [new_agent_x_norm、agent_y_norm、plate_x_norm、plate_y_norm]，而 new_agent_x_norm 只会发生轻微变化。 由于只有一个输入值略有变化，神经网络的输出也会略有变化，从而产生一组非常相似的输出。因此，最高输出保持不变，导致代理不断重复向同一方向（例如向右）移动。 这种行为与我在 The Coding Train 视频中观察到的情况形成鲜明对比，在视频中，输出随时间的变化更大。我想知道我是否忽略了什么，或者我的方法是否犯了错误。 如果您能提供任何关于如何在开发的早期阶段为我的代理行为引入更多变化和探索性的见解或建议，我将不胜感激。提前感谢你的帮助！    提交人    /u/PricePretty4971   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e4wm1y/my_neuroevolution_agent_is_stuck_in_a_loop_what/</guid>
      <pubDate>Tue, 16 Jul 2024 18:52:32 GMT</pubDate>
    </item>
    <item>
      <title>完成多智能体强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e4nsn5/completed_multiagent_reinforcement_learning/</link>
      <description><![CDATA[我潜伏在这个 subreddit 一段时间了，时不时地，我会看到一些想要开始 MARL 项目的人发帖。这些人中很多人都是这个领域的新手，尽管难度非常大，但他们（可以理解）还是想在这个最令人兴奋的子领域之一工作。话虽如此，但在最初阶段之后，我并没有看到太多关于它的讨论。 在我自己的工作中，我发现了几十个库，其中一些有自己的出版物，但在 Github 上查找它们，发现使用它们的（公共）存储库相对较少，尽管它们的星级很高。入门活动和已完成项目的数量之间似乎出现了惊人的下降，甚至比其他热门领域（如生成建模）的下降幅度更大。我知道这是一个有点不合常规的问题，但是，在尝试过 MARL 的人中，你们的情况怎么样？您是否有任何想要分享的项目，无论是作为存储库还是战争故事？    提交人    /u/Efficient_Star_1336   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e4nsn5/completed_multiagent_reinforcement_learning/</guid>
      <pubDate>Tue, 16 Jul 2024 12:48:36 GMT</pubDate>
    </item>
    <item>
      <title>对 MARLLib 的看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e42o68/views_on_marllib/</link>
      <description><![CDATA[您好，我最近问了一个有关我正在开发的项目的问题：https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button 但我发现还有另一个名为 MARLLib 的库，它与 RLlib 类似，但专门用于 MARL。你试过了吗？如果试过，你会推荐使用它吗？谢谢 :)    提交人    /u/Miss_Bat   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e42o68/views_on_marllib/</guid>
      <pubDate>Mon, 15 Jul 2024 18:47:07 GMT</pubDate>
    </item>
    <item>
      <title>关于 MuZero 终端状态的价值目标的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3xes8/question_about_value_targets_for_terminal_states/</link>
      <description><![CDATA[我一直在阅读有关 MuZero 的文章，但对终端节点的处理方式有些不清楚。一般来说，我们使用 n 步回报作为我们的价值目标，但这是否也适用于终端节点？我没有看到任何关于强制将终端状态的价值目标设为 0 的提及，但我也不明白如果我们只使用通常的 n 步回报，学习是如何可能的，因为从终端状态开始的任何序列的 lambda 回报等于该状态的当前价值估计。似乎整个价值函数都会被终端状态初始化的任何值完全搞乱。强制将终端状态的价值目标设为 0 似乎是必要的，但我没有在 MuZero 或 Stochastic MuZero 论文中看到这一点。 需要说明的是，我不是在谈论搜索过程中使用的值 - 我知道 MuZero 在 MCTS 期间不会将终端节点视为特殊。我特别想问的是训练期间使用的价值目标。    提交人    /u/YellowishWhite   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3xes8/question_about_value_targets_for_terminal_states/</guid>
      <pubDate>Mon, 15 Jul 2024 15:19:47 GMT</pubDate>
    </item>
    <item>
      <title>元 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3th8a/meta_marl/</link>
      <description><![CDATA[嗨，我是一名博士新生，专注于 MARL。 当我深入研究最近的出版物时，有一件事让我印象深刻：关注元强化学习的论文几乎比元 MARL 案例多出数百倍。事实上，在过去的两年里，这个主题的论文不到 10 篇。 这真的让我很困惑，因为多智能体系统中的智能体共享策略并在某种程度上进行通信，乍一看，“学会学习”的框架应该带来一些好处吗？还是有一些奇怪和奇怪的小事情？    提交人    /u/No-Deer3657   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3th8a/meta_marl/</guid>
      <pubDate>Mon, 15 Jul 2024 12:24:22 GMT</pubDate>
    </item>
    <item>
      <title>DQN 中的损失函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3gkro/loss_function_in_dqn/</link>
      <description><![CDATA[大家好， 请问一下深度 Q 学习算法中损失函数的作用和功能/目的是什么     提交人    /u/Correct-Jaguar-339   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3gkro/loss_function_in_dqn/</guid>
      <pubDate>Sun, 14 Jul 2024 23:47:27 GMT</pubDate>
    </item>
    <item>
      <title>“使用强化学习解决《流放之路》物品制作问题”（价值迭代）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e37obz/solving_path_of_exile_item_crafting_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e37obz/solving_path_of_exile_item_crafting_with/</guid>
      <pubDate>Sun, 14 Jul 2024 17:19:11 GMT</pubDate>
    </item>
    <item>
      <title>我计划为我的大学做一个自动驾驶汽车项目。我想要与自动驾驶汽车中的 RL 相关的指导。我应该如何在开放的 AI 健身房中为汽车创建训练环境。有很多问题 TT。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e32t8f/i_am_planning_to_make_a_self_driving_car_project/</link>
      <description><![CDATA[我从谷歌中提取了一个 3D 地图，并计划进一步将其导入到 Gazebo 中进行模拟和物理之类的事情。现在我有一个问题，我应该如何在 Gazebo 和 ROS 中使用 RL 来实现自动驾驶汽车。出于训练目的，考虑使用谷歌的 3D 地图。有哪位专家可以指导我吗？    提交人    /u/manas_otaku   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e32t8f/i_am_planning_to_make_a_self_driving_car_project/</guid>
      <pubDate>Sun, 14 Jul 2024 13:51:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 适用于 ML 应用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2xfz6/rl_for_ml_applications/</link>
      <description><![CDATA[大家好！我即将攻读与人工智能和机器学习相关的计算机科学博士学位。我想研究一些包含或结合强化学习和机器学习问题的东西。我曾参与过与控制系统和机器人相关的强化学习项目，这些项目主要涉及训练代理执行任务。但是，我想在一些机器学习问题中使用强化学习。根据目前的趋势，有人能建议一些有趣的领域或应用吗？其中考虑的一些是用于 NLP 任务的强化学习或用于图像分类的强化学习。我将研究更多可能性，但任何方向的指导都将不胜感激。    提交人    /u/shazfu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2xfz6/rl_for_ml_applications/</guid>
      <pubDate>Sun, 14 Jul 2024 08:38:39 GMT</pubDate>
    </item>
    <item>
      <title>寻找从零开始到完善代理的 RL 内容。任何主题或媒体</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2o9en/looking_for_rl_content_from_scratch_to_polished/</link>
      <description><![CDATA[我正在寻找涵盖从头开始到完善代理的环境编码内容。 我想了解他们从训练简单模型中获得的所有超参数和见解，因为他们建立了更复杂的奖励、惩罚或其他任何使代理工作的东西。 例如 https://www.youtube.com/watch?v=SX08NT55YhA https://www.youtube.com/watch?v=DcYLT37ImBY&amp;t=1294s 书面/视频很好，如果也有训练日志那就最好了。建议在哪里看？    由   提交  /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2o9en/looking_for_rl_content_from_scratch_to_polished/</guid>
      <pubDate>Sat, 13 Jul 2024 23:43:06 GMT</pubDate>
    </item>
    <item>
      <title>弃用函数近似中的折扣奖励（Sutton 10.4）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</link>
      <description><![CDATA[Sutton 指出，在持续问题中使用函数近似时，使用折扣奖励不再有意义。 这对我来说真的没有意义，有人可以详细说明一下吗？    提交人    /u/federicom01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</guid>
      <pubDate>Sat, 13 Jul 2024 16:40:41 GMT</pubDate>
    </item>
    <item>
      <title>用大约 13 分钟解释我 2 年的 RL 研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</link>
      <description><![CDATA[        由    /u/ejmejm1 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</guid>
      <pubDate>Sat, 13 Jul 2024 15:11:20 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 算法中的大批量会起作用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</link>
      <description><![CDATA[通常我看到人们在实施 REINFORCE 算法（使用神经网络）时会这样做： for state, action, reward in episodes: update (batch size is 1)  如果游戏长度为 50 轮，我们也可以将所有状态、动作和奖励连接到批量大小为 50 的张量中并进行更新。我尝试过，并且取得了相当不错的成功，值得注意的是（并且不出所料）它大大加快了训练速度。 所以我在想，是什么会阻止我们进行更多连接。假设我们不是每 50 轮游戏更新一次，而是每 10 轮游戏更新一次。张量的维度足够小，这将显著提高计算速度，并可能导致更好的梯度估计。但是，我们最终进行的更新较少。这是我们在监督学习中看到的标准 batch_size 超参数权衡问题。 为什么没人尝试过？或者，也许我只是不擅长搜索是否有人尝试过。 在尝试之前想问一下，因为模拟一切有时需要几天时间。 在你来找我之前，是的，我知道有更好的算法，我只是喜欢先探索非常非常简单的算法。    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</guid>
      <pubDate>Sat, 13 Jul 2024 13:46:53 GMT</pubDate>
    </item>
    </channel>
</rss>