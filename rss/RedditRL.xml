<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 16 Mar 2024 06:16:00 GMT</lastBuildDate>
    <item>
      <title>健身房观察组</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfwfr1/gym_observation_set/</link>
      <description><![CDATA[       大家好！我想改变我的 env.reset 和 env.step 返回的观察空间，例如 hopper，我想让它返回 (11,)，但实际上它返回(12,0)，因为我使用的gym版本太低，无法通过参数“exclude_current_positions_from_observation”更改。你能帮我解决一下吗？万分感谢！  https://preview.redd .it/wpsryhp65moc1.png?width=1298&amp;format=png&amp;auto=webp&amp;s=6635141bf5a7b6223d3f8e55e0b9e8b6e0b32347   由   提交/u/alleZhou  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfwfr1/gym_observation_set/</guid>
      <pubDate>Sat, 16 Mar 2024 03:12:42 GMT</pubDate>
    </item>
    <item>
      <title>“持续预训练大型语言模型的简单且可扩展的策略”，Ibrahim 等人 2024（循环 LR 和重放或多样化数据）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfulix/simple_and_scalable_strategies_to_continually/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfulix/simple_and_scalable_strategies_to_continually/</guid>
      <pubDate>Sat, 16 Mar 2024 01:39:47 GMT</pubDate>
    </item>
    <item>
      <title>MuZero简单实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfr08n/muzero_simple_implementation/</link>
      <description><![CDATA[我想尝试对随机 MuZero 与经典/原始 MuZero 的比较进行基准测试。我发现的所有实现都经过了大量优化。我正在寻找一个简单易读的 MuZero 实现，我可以轻松理解或修改它。您有什么建议吗？   由   提交 /u/_Hardric   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfr08n/muzero_simple_implementation/</guid>
      <pubDate>Fri, 15 Mar 2024 22:54:07 GMT</pubDate>
    </item>
    <item>
      <title>PPO 在训练期间（有探索）学习并表现良好，但在评估期间（没有探索）表现不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfmuwd/ppo_learns_and_performs_perfectly_during_training/</link>
      <description><![CDATA[我尝试过调整学习率、折扣因子、lambda、剪辑值、熵正则化系数和 L2 正则化系数，但到目前为止还没有成功。有什么建议来解决这个问题吗？  提前致谢！   由   提交/u/Appressive_Bag1262   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfmuwd/ppo_learns_and_performs_perfectly_during_training/</guid>
      <pubDate>Fri, 15 Mar 2024 19:54:54 GMT</pubDate>
    </item>
    <item>
      <title>迈向通用情境学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfbb3d/towards_generalpurpose_incontext_learning_agents/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=eDZJTdUsfe 演讲和幻灯片：https://neurips.cc/virtual/2023/79880 博客文章：http://louiskirsch.com/glas 摘要：  强化学习（RL）算法通常是手工制作，由人类研究和工程驱动。另一种方法是通过元学习使该研究过程自动化。一个特别雄心勃勃的目标是从头开始自动发现新的强化学习算法，使用上下文学习来完全从数据中学习如何学习，同时推广到广泛的环境。这些强化学习算法完全在神经网络中实现，通过根据环境中的先前经验进行调节，在元测试时没有任何基于优化的显式例程。为了实现泛化，这需要在多样化和具有挑战性的环境中进行广泛的任务分配。我们基于 Transformer 的通用学习代理 (GLA) 是朝这个方向迈出的重要的第一步。我们的 GLA 使用监督学习技术在离线数据集上进行元训练，并利用 RL 环境中的经验，并通过随机投影进行增强，以生成任务多样性。在元测试期间，我们的代理对完全不同的机器人控制问题（例如 Reacher、Cartpole 或 HalfCheetah）执行上下文元强化学习，这些问题不在元训练分布中。  &lt; !-- SC_ON --&gt;  由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfbb3d/towards_generalpurpose_incontext_learning_agents/</guid>
      <pubDate>Fri, 15 Mar 2024 11:02:09 GMT</pubDate>
    </item>
    <item>
      <title>监督学习与离线强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</link>
      <description><![CDATA[我从 RL 开始，这些可能是非常琐碎的问题，但我想尽我所能地解决所有问题。如果您有任何资源可以为强化学习应用提供良好的直觉，也请在评论中提供：）谢谢。 问题：  我们在哪些场景中使用与离线强化学习相比，更喜欢监督学习？ 样本数量如何影响每个案例的训练？监督学习收敛得更快吗？ 有哪些例子可以同时使用和比较它们进行比较分析？  直觉：  监督学习可以很好地预测给定状态的奖励，但我们不能依赖它来最大化未来的奖励。由于它不使用推出来最大化奖励，并且不进行规划，因此我们不能期望在预期延迟奖励的情况下使用它。 此外，在非独立同分布的动态环境中，每个动作都会影响状态，然后影响进一步采取的动作。因此，对于连续设置，我们在大多数情况下考虑了 RL 的分布变化。 监督学习尝试为每个状态找到最佳动作，这在大多数情况下可能是正确的，但它是一个非常好的方法。针对不断变化的环境采取僵化而愚蠢的方法。强化学习可以自我学习，并且适应性更强。  对于答案，如果可能的话，提供单行，然后任何细节和答案来源也将不胜感激。我希望这篇文章能够为任何尝试应用强化学习的人提供一个很好的指南。我将编辑和更新下面回答的任何问题的答案，以汇总我获得的所有信息。如果您觉得我应该考虑任何其他重大问题和疑虑，也请提及。谢谢！   由   提交 /u/StwayneXG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</guid>
      <pubDate>Fri, 15 Mar 2024 05:17:56 GMT</pubDate>
    </item>
    <item>
      <title>对于较小的网络来说表示学习值得吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bey25b/is_representation_learning_worth_it_for_smaller/</link>
      <description><![CDATA[我阅读了很多关于表示学习作为实际 RL 任务预训练的文献。我目前正在处理顺序传感器数据作为输入。因此，很多数据都是冗余且嘈杂的。因此，代理需要首先从原始输入时间序列中学习语义特征。  由于强化学习中奖励的梯度信号与无监督学习过程相比非常弱，我认为对特征编码器（又名表示学习）进行无监督预训练是值得的。  现在几乎所有的文献都在处理巨大的神经网络的比较和巨大的数据集。我正在处理大约 200k-1M 个参数和大约 1M 个可用于预训练的样本。  我的问题是：当人工神经网络相对较小时，是否值得进行预训练？我的 RL 训练时间目前约为 60 小时，我希望通过预训练大幅缩短训练时间。    由   提交/u/flxh13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bey25b/is_representation_learning_worth_it_for_smaller/</guid>
      <pubDate>Thu, 14 Mar 2024 22:28:54 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习的成功例子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bept24/successful_examples_of_transfer_learning/</link>
      <description><![CDATA[有谁知道是否有任何论文讨论/成功地将某种迁移学习从一项强化学习任务应用到相关任务？例如，如果我有一个迷你网格世界，代理接受了导航培训，然后将其移动到类似的迷你网格，但现在将块推入目标位置。或者说这种事情大部分都是通过多任务学习来完成的？    由   提交 /u/Impallion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bept24/successful_examples_of_transfer_learning/</guid>
      <pubDate>Thu, 14 Mar 2024 16:50:11 GMT</pubDate>
    </item>
    <item>
      <title>“为什么效应定律不会消失”，Dennett 1974（基于模型的强化学习的演变）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bem6z4/why_the_law_of_effect_will_not_go_away_dennett/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bem6z4/why_the_law_of_effect_will_not_go_away_dennett/</guid>
      <pubDate>Thu, 14 Mar 2024 14:15:41 GMT</pubDate>
    </item>
    <item>
      <title>2024 年暑期学校</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1belslq/summer_schools_for_2024/</link>
      <description><![CDATA[您好，我正在寻找 2024 年一些好的 RL 暑期学校，但我发现它对不同的可能性有点不知所措。  这里有人有任何经验/建议吗？    由   提交 /u/IAmNotMarcus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1belslq/summer_schools_for_2024/</guid>
      <pubDate>Thu, 14 Mar 2024 13:58:04 GMT</pubDate>
    </item>
    <item>
      <title>任何人都想购买我的课程学术帐户。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bekp5h/any_body_wants_to_buy_my_coursea_academic_account/</link>
      <description><![CDATA[      您可以免费注册任何课程。我想以 9000 印度卢比的终身价格出售它    由   提交/u/Efficient_Ambition34  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bekp5h/any_body_wants_to_buy_my_coursea_academic_account/</guid>
      <pubDate>Thu, 14 Mar 2024 13:05:03 GMT</pubDate>
    </item>
    <item>
      <title>“如何生成和使用合成数据进行微调”，Eugene Yan</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be20hu/how_to_generate_and_use_synthetic_data_for/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be20hu/how_to_generate_and_use_synthetic_data_for/</guid>
      <pubDate>Wed, 13 Mar 2024 20:34:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 rllib 的感觉如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be12gr/how_it_feels_using_rllib/</link>
      <description><![CDATA[     &lt; td&gt; 由   提交 /u/rl_is_best_pony   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be12gr/how_it_feels_using_rllib/</guid>
      <pubDate>Wed, 13 Mar 2024 19:56:42 GMT</pubDate>
    </item>
    <item>
      <title>连续 Alphazero 算法的状态和有效性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdvt39/state_and_effectiveness_of_continuous_alphazero/</link>
      <description><![CDATA[您好， 我有兴趣在具有连续状态和动作空间的环境中使用基于 MCTS 的 RL 算法。 Moerland 等人在 2018 年针对此设置提出了 Alphazero 的变体。该论文仅获得 61 次引用（大部分来自评论） ，所以看起来这并没有被广泛采用。  所以我想知道是否有人尝试过这个算法，或者知道其他连续动作空间的方法。    由   提交/u/Playmad37  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdvt39/state_and_effectiveness_of_continuous_alphazero/</guid>
      <pubDate>Wed, 13 Mar 2024 16:30:26 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>