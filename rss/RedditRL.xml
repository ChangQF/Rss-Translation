<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 21 Dec 2024 06:21:31 GMT</lastBuildDate>
    <item>
      <title>“偷取免费午餐：揭露 Dyna 式强化学习的局限性”，Barkley 和 Fridovich-Keil</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</link>
      <description><![CDATA[      我认为这是适合讨论这个问题的地方，但无论如何，对于这种无耻的自我推销，我深表歉意…… 论文：https://arxiv.org/abs/2412.14312 关于 X 的相当不错的 TLDR：https://x.com/bebark99/status/1869941518435512712 最近，作为研究的一部分，我对基于模型的 RL 非常感兴趣，但很快遇到了一个大问题：该领域一些主要方法的原始 PyTorch 实现速度非常慢。除非您有数百个 GPU 小时可供使用，否则几乎不可能完成任何合理的工作。 因此，我决定在 JAX 中重新实现一个知名算法，即基于模型的策略优化 (MBPO)，这让事情运行得更快。然而，经过大量的故障排除和测试后，我遇到了另一个惊喜：只要您尝试在不同于他们论文中测试的基准（即 DMC 而不是 Gym）上从头开始训练它，它的表现就会比简单的离线策略算法差，后者需要的训练时间要少几个数量级。 这涉及 6 个 gym 环境和 15 个 DMC 环境，因此非常一致。 这让我很好奇，经过一番挖掘，我和我的导师最终写了一篇关于它和其他 Dyna 风格的基于模型的 RL 方法的论文。剧透：并非所有 dyna 风格的方法都无法在基准测试中发挥作用，但当 Dyna 失败时，这并不是一个孤立或简单的问题。 如果有人有兴趣尝试的话，JAX 实现应该会在明年推出。希望得到大家的反馈！ https://preview.redd.it/ei78d7hbz28e1.png?width=3600&amp;format=png&amp;auto=webp&amp;s=c981756b0f0803e12b2e0e8e6a7816eedb085cfe    提交人    /u/darthbark   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</guid>
      <pubDate>Fri, 20 Dec 2024 22:32:37 GMT</pubDate>
    </item>
    <item>
      <title>表格软 q 学习 停留在简单的网格世界</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</link>
      <description><![CDATA[您好，我正在为一个简单的 5x5 网格世界开发一个简单的表格软 q 学习代理。经过几次尝试后，它卡在了特定状态。我不知道这是实施错误还是超参数不好。我将在下面附上代码。还有其他建议吗？ 谢谢 import numpy as np import time import os class Env(): def __init__(self): self.height = 5 self.width = 5 self.posX = 0 self.posY = 0 self.endX = self.width-1 self.endY = self.height-1 self.actions = [0, 1, 2, 3] self.stateCount = self.height*self.width self.actionCount = len(self.actions) def reset(self): self.posX = 0 self.posY = 0 self.done = False return 0, 0, False # 采取行动 def step(self, action): if action==0: # 左 self.posX = self.posX-1 if self.posX&gt;0 else self.posX if action==1: # 右 self.posX = self.posX+1 if self.posX&lt;self.width-1 else self.posX if action==2: # 向上 self.posY = self.posY-1 if self.posY&gt;0 else self.posY if action==3: # 向下 self.posY = self.posY+1 if self.posY&lt;self.height-1 else self.posY done = self.posX==self.endX and self.posY==self.endY # 将 (x,y) 位置映射到 0 到 5x5-1=24 之间的数字 nextState = self.width*self.posY + self.posX reward = 1 if done else -0.1 return nextState, reward, done # 返回一个随机动作 def randomAction(self): return np.random.choice(self.actions) # 显示环境 def render(self): for i in range(self.height): for j in range(self.width): if self.posY==i and self.posX==j: print(&quot;O&quot;, end=&#39;&#39;) elif self.endY==i and self.endX==j: print(&quot;T&quot;, end=&#39;&#39;) else: print(&quot;.&quot;, end=&#39;&#39;) print(&quot;&quot;) def softmax(x): e_x = np.exp(x - np.max(x)) # 为了数值稳定性 return e_x / e_x.sum() class Agent: def __init__(self, stateCount, actionCount, env, max_steps = 100, epochs = 50, discount_factor = 0.99, lr = 0.1, temp = 1): # Q 表：包含每个 (state,action) 对的 Q 值 self.Q = np.zeros((stateCount, actionCount)) # 超参数 self.temp = temp self.lr = lr self.epochs = epochs self.discount_factor = discount_factor # 环境 self.env = env self.max_steps = max_steps def getV(self, q_value): return self.temp * np.log(np.sum(np.exp(q_value / self.temp))) def choose_action(self, state): # q = self.Q[state] # v = self.getV(q) # dist = np.exp((q - v) / self.temp) # action_probs = dist / np.sum(dist) # return np.random.choice(env.actions, p=action_probs) action_probs = softmax((self.Q[state] - self.getV(self.Q[state])) / self.temp) return np.random.choice(env.actions, p=action_probs) # 训练循环 def run(self): for i in range(self.epochs): state, reward, done = self.env.reset() steps = 0 while not done: os.system(&#39;cls&#39;) # print(self.Q) print(&quot;epoch #&quot;, i+1, &quot;/&quot;, self.epochs) self.env.render() time.sleep(0.01) # 计数完成游戏的步骤 steps += 1 # 选择软 q 学习动作 action = self.choose_action(state) # 采取行动 next_state, reward, done = self.env.step(action) # 使用贝尔曼方程更新 Q 表值 # target = reward + self.discount_factor * np.sum(action_probs * self.Q[next_state]) # target = reward + self.discount_factor * self.getV(self.Q[next_state]) target = reward + (1 - done) * self.discount_factor * self.getV(self.Q[next_state]) self.Q[state][action] += self.lr * (target - self.Q[state][action]) # 更新状态 state = next_state if steps &gt;= self.max_steps: break print(&quot;\nDone in&quot;, steps, &quot;steps&quot;.format(steps)) time.sleep(0.8) def print_q_table(self): for i in range(0,len(self.Q)): for j in range(0,len(self.Q[i])): print(self.Q[i][j], end=&quot; &quot;, flush=True) print(&quot;&quot;) if __name__ == &quot;__main__&quot;: # 创建 CartPole 类的实例 env = Env() resolver = Agent(env.stateCount, env.actionCount, env) resolver.run()    由    /u/Majestic-Tap1577  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</guid>
      <pubDate>Fri, 20 Dec 2024 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>微网格 16x16 动态障碍物解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisobe/minigrid_16x16_dynamic_obstacles_solution/</link>
      <description><![CDATA[我使用 minigrid 和 rl 启动文件。我正在努力解决 16x16（和 8x8）动态迷宫，我目前正在使用 python -m scripts.train --algo ppo --env MiniGrid-Dynamic-Obstacles-16x16-v0 --model DoorKey --save-interval 10 --recurrence 8。你知道我应该更改哪些其他超参数吗？只有 3x3 动态迷宫对我来说是可解的    提交人    /u/More_Peanut1312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisobe/minigrid_16x16_dynamic_obstacles_solution/</guid>
      <pubDate>Fri, 20 Dec 2024 20:16:05 GMT</pubDate>
    </item>
    <item>
      <title>[AHT] 对 SOMALI CAT 环境的查询</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</link>
      <description><![CDATA[我刚刚阅读了两篇关于临时团队合作的非常有趣的论文，重点关注沟通如何提高此类任务的效率，&quot;A Penny for Your Thoughts: The Value of Communication in Ad Hoc Teamwork&quot; 和 &quot; Expected Value of Communication for Planning in Ad Hoc Teamworks&quot;。作者曾在一个名为 SOMALI CAT 的环境中工作过，我很好奇：这里有没有人曾经在这个环境中工作过，或者您是否知道它是否可以在某些通用 Python RL 测试框架（例如 Gymnasium）上使用。 感谢您的帮助    提交人    /u/potatodafish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</guid>
      <pubDate>Fri, 20 Dec 2024 18:43:58 GMT</pubDate>
    </item>
    <item>
      <title>预测行动和奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</link>
      <description><![CDATA[大家好，我正在处理一个非专家级数据的数据集，并且正在使用决策转换器。现在我想比较性能，但我找不到可以同时预测动作和奖励的离线 rl 模型。有人有什么建议吗？    提交人    /u/Anonymusguy99   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</guid>
      <pubDate>Fri, 20 Dec 2024 13:45:40 GMT</pubDate>
    </item>
    <item>
      <title>救命！我的 RL Agent 没有学习。（OpenAI Gym env + Pytorch）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</link>
      <description><![CDATA[我试图实现一个简单的深度 Q 网络，以便在 OpenAI Gymnasium 提供的 Cart Pole 环境中训练代理。我尝试调整超参数，但似乎没有任何效果。事实上，随着 epoch 的增加，情况似乎变得更糟（虽然不确定）。我觉得我已经正确地实现了一切。我正在使用 pytorch 来构建神经网络。我对 RL 和深度学习还不熟悉，所以如果我遗漏了什么，我深表歉意。 我附上了我的代码，以便您可以运行它。该笔记本非常不言自明，除了 pytorch 之外，您只需要 gyanasium[classic-control] 和 pygame 包。 https://github.com/Utsab-2010/OpenAI-Gym-RL-Tests/blob/main/Cart_Pole_Deep_QN.ipynb 任何建议或帮助都将不胜感激。    提交人    /u/Otaku_boi1833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</guid>
      <pubDate>Fri, 20 Dec 2024 08:04:59 GMT</pubDate>
    </item>
    <item>
      <title>DDPG、A2C、PPO 和 TRPO 实验在 StableBaselines3 中何时收敛？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</link>
      <description><![CDATA[我需要运行所有基于策略梯度的 RL 实验，以展示它们在使用和不使用我计划提出的自定义训练方案的情况下收敛的速度。对于环境，我正在考虑 OpenAI Gym 环境（CartPole、Pendulum、LunarLander 和 2-3 Mujoco 环境）。但我首先需要确定它们是否与通常的训练方法收敛。即使它们不收敛，也应该有改善的迹象（就情景回报而言）。SB3 上是否有任何资源提供在这些环境中工作的超参数或策略网络？此外，是否有人对记录反向传播的数量或类似的东西（任何其他有用的指标）以评估计算负载等有任何建议？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</guid>
      <pubDate>Fri, 20 Dec 2024 06:39:36 GMT</pubDate>
    </item>
    <item>
      <title>决策频率：‘信息’视角</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</link>
      <description><![CDATA[小动作重复潜力：细粒度控制问题：信用分配 大动作重复潜力：更明智的决策问题：延迟 如果决策之间没有足够的时间，代理将根据较少的信息采取行动。 如果所述时间很长，则“适应变化”会被延迟。  推荐解决方案的示例：分层 RL：存在在快速行动的较低级别与以较慢速度行动的较高级别之间进行通信的问题。 决策转换器：离线方法，因此无法在工作中学习 根据我的经验，这个问题与计算或模型容量无关。无论学习能力有多强，智能体采取行动的频率（或缺乏的信息）都是有限的。 你对这个困境有什么看法？    提交人    /u/XecutionStyle   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</guid>
      <pubDate>Thu, 19 Dec 2024 19:48:50 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线 3 进行 SAC 训练可停止 TensorBoard 更新，并在自定义环境中 3,000 步后加速</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhv1ij/sac_training_with_stable_baselines3_halts/</link>
      <description><![CDATA[大家好， 我在自定义环境中使用Soft Actor-Critic (SAC)算法，其中代理每次迭代都会调整另一个优化器的超参数。最初，训练和学习顺利进行，最多可达3,000 个时间步。然而，在此之后，TensorBoard 停止更新，并且训练速度急剧增加，但没有取得任何有意义的进展。 有人遇到过类似的问题或可以提出潜在的原因和解决方案吗？ 谢谢！    提交人    /u/YasinRL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhv1ij/sac_training_with_stable_baselines3_halts/</guid>
      <pubDate>Thu, 19 Dec 2024 15:14:06 GMT</pubDate>
    </item>
    <item>
      <title>“MaxInfoRL：通过信息增益最大化促进强化学习中的探索”，Sukhija 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</guid>
      <pubDate>Thu, 19 Dec 2024 06:30:27 GMT</pubDate>
    </item>
    <item>
      <title>SAC 代理在我们的自定义视频游戏环境中没有学到任何东西 - 救命！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</guid>
      <pubDate>Wed, 18 Dec 2024 20:57:41 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 和线下强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</link>
      <description><![CDATA[由于 LLM 模型以某种方式进行训练，例如行为克隆，那么使用离线 RL 进行训练怎么样？ 我知道奖励设计将是一个重大挑战和可扩展性等。 你怎么看？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</guid>
      <pubDate>Wed, 18 Dec 2024 17:29:39 GMT</pubDate>
    </item>
    <item>
      <title>努力训练用于路线优化的 Dueling DQN 模型 – 需要有关学习和计算要求的建议 😢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</link>
      <description><![CDATA[我正在使用 Dueling DQN 在自定义道路网络环境中进行路线优化项目，该环境具有许多节点和不同的动作空间。但是，该模型无法正确学习 - 训练结果不一致，并且代理难以找到最佳路径。 有人有兴趣贡献吗     提交人    /u/ProfessionalType9800   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</guid>
      <pubDate>Wed, 18 Dec 2024 14:54:11 GMT</pubDate>
    </item>
    <item>
      <title>David Silver 示例考试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</link>
      <description><![CDATA[      大家好， 我正在查看 David Silver 网站上的练习考试，但似乎无法理解本页最后一个问题的解决方案。对于状态一的 lambda 返回，它不应该是 0.5**2 x 1 而不是 0.5 x 1。之后，我完全不知道状态 2 和 3 的返回值了。    提交人    /u/LostBandard   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</guid>
      <pubDate>Wed, 18 Dec 2024 02:29:26 GMT</pubDate>
    </item>
    <item>
      <title>强化学习工作原理的示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</link>
      <description><![CDATA[  由    /u/A-Sexy-Name  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</guid>
      <pubDate>Tue, 17 Dec 2024 21:11:18 GMT</pubDate>
    </item>
    </channel>
</rss>