<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 30 Dec 2024 21:15:13 GMT</lastBuildDate>
    <item>
      <title>“基于协同效应的机器人群体行为的自动设计”，Salman 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpvi1r/automatic_design_of_stigmergybased_behaviours_for/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpvi1r/automatic_design_of_stigmergybased_behaviours_for/</guid>
      <pubDate>Mon, 30 Dec 2024 19:39:53 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo 基线3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpt4tr/pettingzoo_baselines3/</link>
      <description><![CDATA[我有 2 个具有不同角色的代理，问题是如何让模型在预测中（在测试加载的模型时）理解每个代理具有哪个角色？我所做的是在 obs 中添加一个布尔值来区分角色，但我想知道我是否可以发出它并在测试时简单地使用 2 个不同的模型。  我目前正在测试（aec） model = PPO.load(latest_policy) # print(env.possible_agents) rewards = {agent: 0 for agent in env.possible_agents} # 注意：我们使用并行 API 进行训练，但使用 AEC API 进行评估 # SB3 模型是为单代理设置设计的，我们通过对每个代理使用相同的模型来解决这个问题 for i in range(num_games): env.reset(seed=i) for agent in env.agent_iter(): obs, reward, Termination, truncation, info = env.last() # print(obs) if reward &gt; 0：rewards[agent] += reward 如果终止或截断：act = None else：act = model.predict(obs, deterministic=True)[0] print(f&quot;\nAgent: {agent}, Observation: {obs}, Reward: {reward}, Action: {act}&quot;) env.step(act) env.close()  在训练（并行）中我有 model = PPO( MlpPolicy, env, verbose=3, learning_rate=1e-3, batch_size=256, tensorboard_log=log_dir ) while True：model.learn(total_timesteps=steps, reset_num_timesteps=False) save_path = os.path.join(model_dir, f&quot;{env.unwrapped.metadata.get(&#39;name&#39;)}_{time.strftime(&#39;%Y%m%d-%H%M%S&#39;)}&quot;) model.save(save_path)     提交人    /u/More_Peanut1312   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpt4tr/pettingzoo_baselines3/</guid>
      <pubDate>Mon, 30 Dec 2024 17:59:58 GMT</pubDate>
    </item>
    <item>
      <title>关于为动态定价 RL 任务创建合成数据的建议。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpsnl4/advice_on_creating_synthetic_data_for_dynamic/</link>
      <description><![CDATA[大家好！ 我正在使用强化学习开展电子商务动态定价项目。由于我没有真实数据，因此我尝试生成合成数据进行训练。我的计划是比较 DQN 和 PPO 来完成此任务，并在自定义环境中设置价格以最大化收入或利润。 到目前为止，我了解了：  线性模型：价格上涨 → 需求下降（价格弹性）。 Logit 模型：基于经济模型的建模。 季节性：由于时间/事件导致的需求波动。  我希望数据能够模拟真实世界的行为，例如价格敏感度、季节性变化和一些随机性。我见过很多论文使用 DQN 进行离线学习，但我很想尝试 PPO 并比较结果。  我很乐意获得有关如何构建此类模型或应该包含哪些内容以使数据更真实的任何建议。 这是我第一次尝试从头开始创建环境（我只调整过健身房环境），所以我很乐意听取您的建议。    提交人    /u/Professional_Ant_140   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpsnl4/advice_on_creating_synthetic_data_for_dynamic/</guid>
      <pubDate>Mon, 30 Dec 2024 17:39:19 GMT</pubDate>
    </item>
    <item>
      <title>当回报在 1e6 和 1e10 之间时，如何标准化奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpsned/how_would_you_normalize_the_rewards_when_the/</link>
      <description><![CDATA[嘿，我正在努力使用 FQI 以外的任何其他方法获得良好的性能，以适应基于 https://orbi.uliege.be/bitstream/2268/13367/1/CDC_2006.pdf 的环境，最大时间步长为 200。观察空间的形状为 (6,)，动作空间是离散的 (4) 我不确定如何在奖励上做到这一点，因为随机代理获得的回报约为 1e7，而最佳代理应该获得 5e10。到目前为止，我得到的最佳结果是使用带有以下包装器的 PPO：  log(max(obs, 0) + 1) 将最后一个操作附加到 obs TimeAwareObservation FrameStack(10) VecNormalize  到目前为止，我尝试使用各种奖励标准化的 PPO 和 DQN，但没有成功（使用 sb3）：  使用 sb3 中的 VecNormalize 无标准化 除以 1e10（仅在 dqn 上尝试） 除以回报的运行平均值（仅在 dqn 上尝试） 除以回报的运行最大值（仅在 dqn 上尝试）  现在我有点绝望，并尝试使用 python-neat 运行 NEAT（性能低下）。 您可以在此处找到我对 env 的实现：https://pastebin.com/7ybwavEW 任何关于如何使用现代技术处理这种环境的建议都会受到欢迎！    提交人    /u/Butanium_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpsned/how_would_you_normalize_the_rewards_when_the/</guid>
      <pubDate>Mon, 30 Dec 2024 17:39:06 GMT</pubDate>
    </item>
    <item>
      <title>输出概率与初始初始化时没有变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hps2gg/output_probabilities_are_not_changing_from/</link>
      <description><![CDATA[因此，我正在实施一种 RL 方法用于股票交易，因此我有两个代理，一个决定进入哪个方向，另一个管理交易，因此进入模型输出买入和卖出之间的概率，它们初始化在 49、50 左右，但问题在于开发（验证），如果模型初始化时有一个略微优势的动作，它总是选择那个动作。我正在检查梯度并监控 wandb 上的权重，甚至概率比，尽管它们很小，但一切似乎都在移动，但输出保持不变。我的经理代理也是如此。所以我已经运行了五集，但单个训练集有 300 笔交易，每个交易都有六个训练阶段，所以我认为足够的训练可以看到概率分布的一些变化，但没有看到任何变化，奖励是不稳定的，所以我不能肯定地使用该指标来判断，至于验证，它一直执行单一动作，不像在训练中它探索并获得可观的回报，这可能是问题所在    提交人    /u/sk3ptica1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hps2gg/output_probabilities_are_not_changing_from/</guid>
      <pubDate>Mon, 30 Dec 2024 17:14:28 GMT</pubDate>
    </item>
    <item>
      <title>接受论文摘要的会议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpr46m/conferences_for_accepting_abstract_papers/</link>
      <description><![CDATA[大家好， 有任何会议/研讨会接受摘要论文吗？我现在全职工作。我没有太多时间进行实验，但我有一些想要发表的想法，有什么建议吗？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpr46m/conferences_for_accepting_abstract_papers/</guid>
      <pubDate>Mon, 30 Dec 2024 16:33:51 GMT</pubDate>
    </item>
    <item>
      <title>这里有业余爱好者或者独立现实爱好者吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpqy24/anybody_hobbyist_or_indie_rl_enthusiast_here/</link>
      <description><![CDATA[我有一些典型计算机科学的背景和经验，但在人工智能方面没有专业知识。所以我称自己为业余爱好者。我对解决现实世界的问题不感兴趣；我满足于追随大师在国际象棋或五子棋等已经征服的领域的成就。无论如何，我想将 RL 应用于双人参与抽象策略游戏。这个 subreddit 上有没有人尝试过类似的东西？    提交人    /u/Gloomy-Status-9258   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpqy24/anybody_hobbyist_or_indie_rl_enthusiast_here/</guid>
      <pubDate>Mon, 30 Dec 2024 16:26:32 GMT</pubDate>
    </item>
    <item>
      <title>比较 RL 代理的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpqsur/metrics_for_comparing_rl_agents/</link>
      <description><![CDATA[大家好！👋 我正在一个小型大学项目上工作，该项目在太空侵略者的背景下探索强化学习。我想将传统的 Q-Learning 代理与 DQN 进行比较，并且正在考虑使用哪些指标进行分析。 到目前为止，我已决定绘制：  每集得分 每集平均奖励 每集平均游戏时间  我还在考虑绘制平均 Q 值。但是，我对这是否合适有些怀疑。具体来说，我不确定如何解释由于每集步骤数的差异，Q 值在各集之间可能会有显著差异这一事实。 附注：我完全清楚 Q-Learning 是一种表格方法，不太适合具有大状态空间的环境。这一限制将是我进行比较分析的关键部分。 提前致谢！    提交人    /u/Lonely-Eye-8313   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpqsur/metrics_for_comparing_rl_agents/</guid>
      <pubDate>Mon, 30 Dec 2024 16:20:07 GMT</pubDate>
    </item>
    <item>
      <title>触发环境重置的明确奖励[体育馆和稳定基线3]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpnubr/explicit_reward_for_triggering_env_reset/</link>
      <description><![CDATA[大家好， 提前感谢大家的帮助！ 当我的代理导致环境重置（低于阈值）时，我想应用特定惩罚。我不明白的是，我可以正确触发重置，但惩罚没有应用，奖励是按常规计算的。如果您能指出我在某处误解了结构的话，那就太好了：) step() 伪代码： #action 提取 #action 处理 #updating 值 #reward 计算 # penalty 检查 if value1 &lt;= Threshold: Terminated = True self.reward = -200 # Override reward with penalty observer = self._get_observation() return observer, self.reward, Terminated, Triuncated, {}     submitted by    /u/Much_Razzmatazz_6641   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpnubr/explicit_reward_for_triggering_env_reset/</guid>
      <pubDate>Mon, 30 Dec 2024 14:02:13 GMT</pubDate>
    </item>
    <item>
      <title>无限武装老虎机问题的遗憾下限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpalfe/lower_bound_on_regret_in_infinitelyarmed_bandit/</link>
      <description><![CDATA[只要臂空间是连续/无限的，或者我们不对拉动每个臂的效用做任何假设，遗憾的下限应该是 Omega(T)，对吗（假设奖励在 [0, 1] 之间）？ 众所周知，在 K 臂老虎机问题中，T 轮累积遗憾的下限是 Omega(sqrt(KT))。如果没有对效用做任何假设，只是给定一个臂，则该臂的奖励是独立且相同分布的。在无限多臂老虎机中，我们有 K 趋向于无穷大，因此 Omega(sqrt(KT)) 下限变为无界，并且我们知道对于老虎机问题，遗憾最多为 T，因此最坏情况遗憾的下限应该是 Omega(T)。  我没见过任何地方有这样的说法，但也许很明显没有这样说。不过，我明白这并不意味着在每个连续体或无限多的武装匪徒问题中遗憾都是 Omega(T)，因为我们可以对导致更严格遗憾界限的效用做出假设。    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpalfe/lower_bound_on_regret_in_infinitelyarmed_bandit/</guid>
      <pubDate>Mon, 30 Dec 2024 00:42:27 GMT</pubDate>
    </item>
    <item>
      <title>迷宫/最短路径环境的奖励结构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hp8ns5/reward_structure_for_maze_shortest_path/</link>
      <description><![CDATA[嗨， 我正在构建一个环境，玩家必须穿过一个随机生成的迷宫，激活迷宫随机部分的开关，然后导航到出口。玩家的移动是连续的（即不是基于网格的）。 我一直在研究一种信息丰富的、有形状的奖励结构，以鼓励学习最短路径。 目前，我的奖励结构如下： - 每帧减 0.01 - 根据玩家与目标（出口开关或出口门，如果开关被激活）之间距离与前一步的差异，每帧给予小幅（&lt;0.5）奖励或惩罚 - 到达出口给予大额奖励（10） 在训练之前，我将奖励标准化为 0 到 1 之间。但是，这里似乎可能存在一些冗余，我想问问大家的想法，以及是否有更好的方式来构建奖励。 作为参考，这是一个模拟游戏 N++ 的环境。 谢谢大家的帮助！    由    /u/Tetramputechture  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hp8ns5/reward_structure_for_maze_shortest_path/</guid>
      <pubDate>Sun, 29 Dec 2024 23:10:51 GMT</pubDate>
    </item>
    <item>
      <title>RL 书籍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hp4nb7/rl_books/</link>
      <description><![CDATA[我开始学习 RL。这个领域最好的书籍或文章是什么。    提交人    /u/fg-dev   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hp4nb7/rl_books/</guid>
      <pubDate>Sun, 29 Dec 2024 20:12:58 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 carla 进行 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</link>
      <description><![CDATA[我的毕业设计用carla完成强化学习，能推荐一些在线课程吗？    submitted by    /u/Clean_Tip3272   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</guid>
      <pubDate>Sun, 29 Dec 2024 10:36:18 GMT</pubDate>
    </item>
    <item>
      <title>遗憾值为 O( \sqrt(log K T ) ) 的 K 臂随机赌博机算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</link>
      <description><![CDATA[我想知道是否有任何 K 臂随机老虎机算法可以实现 $O(\sqrt(T))$ 遗憾，且因子为常数 $\sqrt{ log K }$。  我知道 exp3 可以实现 O(\sqrt(T)) 遗憾，因子为 sqrt(k log K )，而 UCB 可以实现 \tilde{O}( sqrt(T) ) 遗憾，因子为 sqrt(k)？  是否有一种算法，其臂数与 sqrt( log K ) 类似？或者，是否有更严格的 exp3 或 UCB 分析，可以在臂数方面实现更好的因子？  我正在研究一个问题，其中臂的数量为 K^{a}，其中 a 是某个参数，并且我想将我的因子归结为类似 a * poly(K) - （poly(K) 表示关于 K 的多项式）的东西。    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</guid>
      <pubDate>Sun, 29 Dec 2024 05:59:36 GMT</pubDate>
    </item>
    <item>
      <title>RL“包裹” 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</link>
      <description><![CDATA[我通常会在假期的最后几天努力赶上进度（事实证明这些天不可能）并回顾学术和工业发展方面的主要亮点。请在此处添加您今年的顶级 RL 作品     提交人    /u/blitzkreig3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</guid>
      <pubDate>Sat, 28 Dec 2024 21:08:08 GMT</pubDate>
    </item>
    </channel>
</rss>