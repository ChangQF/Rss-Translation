<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 22 Nov 2024 12:33:51 GMT</lastBuildDate>
    <item>
      <title>我的 ML-Agents 代理变得越来越笨，我的想法已经用完了。我需要帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</guid>
      <pubDate>Fri, 22 Nov 2024 10:42:24 GMT</pubDate>
    </item>
    <item>
      <title>灾害管理中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</link>
      <description><![CDATA[最近，我深入研究了 RL 的灾害管理，并阅读了几篇相关论文。许多论文都提到了与之相关的算法，但没有以某种方式对其进行模拟。是否有任何平台具有与 RL 相关的模拟来展示其应用？此外，如果您有关于此方面的任何其他优秀论文的信息，请提及。     提交人    /u/SuitSecret6497   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</guid>
      <pubDate>Fri, 22 Nov 2024 07:40:51 GMT</pubDate>
    </item>
    <item>
      <title>策略梯度公式检查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwujcm/policy_gradient_formulas_check/</link>
      <description><![CDATA[      你好， 我正在写关于 RL 中的策略梯度方法，我对这些方程式有疑问。我理解目标是最大化目标函数 J(θ) 的值，即给定策略 (πθ) 的轨迹 (τ) 的总回报。这给了我们表达式 J(θ) = E τ∼πθ [R(τ)]。 从那里并使用梯度，我们可以推断出表达式 ∇θ J(θ) = E τ∼πθ [∑t ∇θ log πθ(at|st) R(τ)]。 我的问题是这些算法的以下目标函数是否正确（第一个是 REINFORCE）： https://preview.redd.it/prqve7kfhc2e1.png?width=754&amp;format=png&amp;auto=webp&amp;s=76e48fb82eb0ac1710352705b1c84c3869453d39 如果您能提供任何关于改进或以其他方式表达这些功能的建议，我将不胜感激。    提交人    /u/Street-Vegetable-117   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwujcm/policy_gradient_formulas_check/</guid>
      <pubDate>Fri, 22 Nov 2024 00:11:18 GMT</pubDate>
    </item>
    <item>
      <title>适用于 ML/AI/RL 的 Blue Sky 研究员入门包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</link>
      <description><![CDATA[大家好，许多研究人员正在加入 Blue Sky，而且似乎进展顺利，所以我想我会留下一些研究人员“入门包”供大家关注。欢迎随意发布您自己的内容 :)  RL：https://bsky.app/3WPHcHg AI：https://bsky.app/SipA7it NLP：https://bsky.app/SngwGeS ML 理论：https://bsky.app/21nFz12 AI 机器人：https://bsky.app/DfAoaJ1 贝叶斯 ML：https://bsky.app/2Bqtn6T 人工智能中的女性：https://bsky.app/LaGDpqg 人工智能和新闻：https://bsky.app/5sFqVNS 科学人工智能：https://bsky.app/JeFdryY  医学人工智能：https://bsky.app/N5UUARF     提交人    /u/secondterm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</guid>
      <pubDate>Thu, 21 Nov 2024 21:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何开始机器人操作器的强化学习研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</link>
      <description><![CDATA[你好， 我是一名研究生，对应用人工智能技术（特别是强化学习）来控制机器人操纵器（机械臂）感兴趣。 为了做到这一点，我不知道从哪里开始学习并决定研究主题。  有哪些基础论文和资源可以帮助我了解这个领域？ 有哪些最近的评论或调查论文可以帮助我了解该领域的现状？ 或者，为了研究人工智能机器人技术，我应该阅读哪些论文？   如有任何建议或意见，我们将不胜感激！ 谢谢！ 使用 DeepL.com （免费版） 进行翻译   提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 21 Nov 2024 18:59:33 GMT</pubDate>
    </item>
    <item>
      <title>帮我用 Unity3D 制作这辆 DDPG 自动驾驶汽车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</link>
      <description><![CDATA[我被这个项目困住了，我不知道我哪里做错了，可能是在脚本中，也可能是在 Unity 中。请帮助我解决和调试问题。请直接给我发私信，获取脚本和更多信息。    提交人    /u/pendalkumar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</guid>
      <pubDate>Thu, 21 Nov 2024 13:41:59 GMT</pubDate>
    </item>
    <item>
      <title>另一个调试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</link>
      <description><![CDATA[大家好， 我正在解决声音领域中具有连续动作的问题。 该模型是一个表示声音的 CNN。表示与一些参数一起被输入到 MLP 以获得值和动作。 在研究损失函数（在我们的例子中是奖励）之后，它是参数和动作的凸函数。我的意思是，对于给定的参数 + 声音，作为动作函数的奖励信号是凸的。 不幸的是，我们偶然发现了一个可以实现收敛的网络参数的良好初始化。问题是模型几乎一直都不会收敛。 如何调试问题的根源？我是否只需要等待足够长的时间？我是否扩大了模型？ 谢谢 编辑：我意识到我没有指定我正在使用的算法。PPO，A2C，Reinforce，OptionCritic，PPOC。 所有这些算法的作用本质上都是相同的。    提交人    /u/sagivborn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</guid>
      <pubDate>Thu, 21 Nov 2024 09:43:42 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 epymarl 来运行我的模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwbkq1/how_can_i_use_epymarl_to_run_my_model/</link>
      <description><![CDATA[我尝试通过 README 做一些事情，但是无法成功。有人能帮我吗，如何通过 README 注册我自己的环境，谢谢。    提交人    /u/NationalBat6637   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwbkq1/how_can_i_use_epymarl_to_run_my_model/</guid>
      <pubDate>Thu, 21 Nov 2024 08:26:40 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 Agent 进行国际象棋之类的游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gw7itl/how_do_you_train_agent_for_something_like_chess/</link>
      <description><![CDATA[到目前为止我还没有做过任何 RL，我想开始使用 RL 研究类似国际象棋模型的东西，但不知道从哪里开始    提交人    /u/Ok_Orchid_7408   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gw7itl/how_do_you_train_agent_for_something_like_chess/</guid>
      <pubDate>Thu, 21 Nov 2024 04:07:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在深度强化学习中处理多通道输入</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gw1vit/how_to_handle_multi_channel_input_in_deep/</link>
      <description><![CDATA[大家好。我正在尝试制作一个代理，它将学习如何使用深度强化学习下棋。我使用的是 pettingzoo 的 chess_v6 环境 (https://pettingzoo.farama.org/environments/classic/chess/)，它使用棋盘的观察空间，其形状为 (8,8,111)。我的问题是如何将此观察空间输入到深度学习模型中，因为它是一个多通道输入，以及哪种架构最适合我的 DL 模型。请随时分享您可能拥有的任何提示或我可以阅读的有关该主题或我使用的环境的任何资源。    提交人    /u/Livid-Ant3549   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gw1vit/how_to_handle_multi_channel_input_in_deep/</guid>
      <pubDate>Wed, 20 Nov 2024 23:35:37 GMT</pubDate>
    </item>
    <item>
      <title>“物理智能：将人工智能带入物理世界的十亿美元初创公司内部情况”（pi）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvzupn/physical_intelligence_inside_the_billiondollar/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvzupn/physical_intelligence_inside_the_billiondollar/</guid>
      <pubDate>Wed, 20 Nov 2024 21:20:01 GMT</pubDate>
    </item>
    <item>
      <title>RL 是否存在什么重大的局限性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvx2yb/are_there_any_significant_limitations_to_rl/</link>
      <description><![CDATA[我是在 DeepSeek 的新 R1 模型之后问这个问题的。它与 OpenAI 的 o1 大致相当，很快就会开源。这个问题听起来可能很蹩脚，但我很好奇这方面是否有任何强有力的数学结果。例如，我隐约知道维数灾难。    提交人    /u/dhhdhkvjdhdg   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvx2yb/are_there_any_significant_limitations_to_rl/</guid>
      <pubDate>Wed, 20 Nov 2024 18:55:46 GMT</pubDate>
    </item>
    <item>
      <title>RLtools：最快的深度强化学习库（C++；仅标头；无依赖项）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</link>
      <description><![CDATA[        提交人    /u/jonas-eschmann   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</guid>
      <pubDate>Wed, 20 Nov 2024 17:00:36 GMT</pubDate>
    </item>
    <item>
      <title>尽管我有 64 GB 的 RAM 和 24 GB 的 GPU RAM，但 RL 训练一段时间后还是会冻结</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvnw8k/rl_training_freezing_after_a_while_even_though_i/</link>
      <description><![CDATA[嗨，我有 64 GB RAM 和 24 GB GPU RAM。我正在训练一个乒乓球游戏的 RL 代理。训练在大约 120 万帧后冻结，我不知道为什么，即使 RAM 没有达到最大值。重播缓冲区大小约为 1_000_000。 [代码链接](https://github.com/VachanVY/Reinforcement-Learning/blob/main/dqn.py) 可能是什么原因以及如何解决这个问题？请帮忙。谢谢。    提交人    /u/VVY_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvnw8k/rl_training_freezing_after_a_while_even_though_i/</guid>
      <pubDate>Wed, 20 Nov 2024 12:02:24 GMT</pubDate>
    </item>
    <item>
      <title>正在寻找南部各州的硕士课程，有什么推荐吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvhoyv/looking_for_masters_programs_in_the_southern/</link>
      <description><![CDATA[嗨，我一直在寻找好的以研究为导向的硕士课程，在那里我可以专注于 RL 理论！所以我主要寻找的是在这个领域有良好研究的大学，这些大学并不是明显的首选。例如，您对亚利桑那州立大学、德克萨斯大学达拉斯分校和德克萨斯 A&amp;M 大学的看法是什么？    提交人    /u/the_real_custart   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvhoyv/looking_for_masters_programs_in_the_southern/</guid>
      <pubDate>Wed, 20 Nov 2024 04:47:41 GMT</pubDate>
    </item>
    </channel>
</rss>