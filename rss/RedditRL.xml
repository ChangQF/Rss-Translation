<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 24 Feb 2025 01:19:04 GMT</lastBuildDate>
    <item>
      <title>基于模型的RL：开环控制是亚最佳选择，因为..？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在观看Sergei Levine的讲座。他是一个很好的资源；将学习理论的联系起来很多。使用数学测试的类比，通过开环控制通过开环控制是亚最佳的。我想象这个类比就像搜索树一样例如，但即使如此，它也有点清除。但是，要与抽象的例子保持在一起，为什么该模型不会基于以前与环境互动的经验产生可能性？ Sergei提到，如果我们选择测试，我们将得到正确的答案，但也意味着没有办法将这些信息传递给模型（在这种情况下，代理商的决策者）。感觉从现实中消除了，即如果可能的测试尺寸足够大，那么最佳的动作就是回家。如果您对参加测试的能力有任何信心（例如以前的推出经验），那么您的最佳策略会更改，但这是信息，您可以通过与以前的示例相同的分布来理解。  也许我缺少标记。为什么开放环控制次优？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iwhd9t/model_based_rl_rl_openloop_iscontrol_is_suboptimal/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1iwhd9t/model_based_rl_rl_openloop_control_is_is_suboptimal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</guid>
      <pubDate>Sun, 23 Feb 2025 18:52:18 GMT</pubDate>
    </item>
    <item>
      <title>我的代码未显示Dyna-Q和Dyna-Q+算法之间的差异。请帮助修复它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwdj4k/difference_between_dynaq_and_dynaq_algorithm_not/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  首先，我在此env www.github.com/VachanVY/Reinforcement-Learning/blob/main/images/shortcut_maze_before_Dyna-Q_with_25_planning_steps.gif到达目标的路线更长。 然后，我从这里乘坐Q阀来训练dyna-q + algo在修改后的env上，其中包含通往目标的较短途径证明当env发生变化时，dyna-q + 更好，但是在以下代码应用后，我认为应用Dyna-Q+ algo没有区别，应该采取较短的路径。  www.github.com/vachanvy/reinforeccation-learning/blob/main/images/shortcut_maze_after_dyna-q+_with_with_25_25_planning_steps.gifs.gif    我看不到它所采取的路线的任何变化，就像强化学习Sutton和Barto  的介绍时所说的那样，``````````````&#39; ，日志：bool = false，q_values = none，epsilon = epsilon）：plan = true如果 num_planning_steps&gt;0 else False if not plan: assert not dyna_q_plus q_values = init_q_vals(NUM_STATES, NUM_ACTIONS) if q_values is None else q_values env_model = init_env_model(NUM_STATES, NUM_ACTIONS) if plan else None last_visited_time_step = init_last_visited_times（num_states，num_actions）    sum_rewards_episodes = []; timeStep_episodes = [] total_step = 0 for Epistion（1，num_episodes+1）：状态，info = env.reset（）; sum_rewards = float（0）在计数（1）中的TSTEP：total_step += 1 action = sample_action（q_values [state]，epsilon）next_state，奖励，完成，截断，nofe = ency.step（action）; sum_rewards +=奖励q_values [state] [action] += alpha *（奖励 +gamma * max（q_values [next_state]）） -  q_values [state] [action] [action]）last_visited_time_step [state] ：env_model [state] [action] =（奖励，next_state）＃（奖励，next_state）如果完成或截断：break状态= next_state sum_rewards_episodes.append（sum_rewards）timestep_episodes.append.append（tstep）如果log：print：print（f&#39;epsisode：&#39;epsisode：{ponvision} ||奖励总和{sum_rewards} ＆quot”＃计划是否计划：用于Planning_step in range（num_planning_steps）：planning_state = rando_prev_observed_state（last_visited_time_step）＃随机观察到的状态planning_action_action_action_action_action_action_action_action_action_action_for_for_state（env_model [plance_state] env_model [planning_state] [planning_action]如果dyna_q_plus：＃为了鼓励测试＃长途动作的行为，则在涉及＃这些动作的模拟体验上给出了特殊的“奖励奖励”。特别是，如果对过渡的建模奖励为R，并且在τ时间步骤中尚未尝试过过渡＃，则**计划更新**就像该过渡＃产生了R +κ*的奖励一样（τ（τ） ）^0.5，对于一些小κ。这鼓励代理商继续测试所有可访问的状态过渡，甚至可以找到长长的操作序列以进行＃进行此类测试。 ＃当前步骤 - 上次访问plance_reward += kappa * nath.sqrt（total_visited_time_time_step [plance_state] [plance_actate] [plancy_action]）q_values [planning_state] [planning_state] [planning_action] += alpha * [planning_state] [planning_action]）打印（总步骤：＆quot; total_step）返回q_values，sum_rewards_episodes，timeStep_episodes    ``` 32;提交由＆＃32; /u/vvy_     [link]  &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/1iwdj4k/difference_between_dynaq_and_ynaq_and_dynaq_algorithm_not/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwdj4k/difference_between_dynaq_and_dynaq_algorithm_not/</guid>
      <pubDate>Sun, 23 Feb 2025 16:09:40 GMT</pubDate>
    </item>
    <item>
      <title>解决迷宫的RL代理：疑问</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw7g65/rl_agent_for_solving_mazes_doubts/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好。我将毕业于CS，并想创建一个关于在统一的沙盒环境中进行的关于迷宫式解决方案的强化学习的论文项目。我对AI和相关主题有基本知识，但是我对自己的首发想法有一些疑问。 我想在统一环境中进行强化学习的项目，重点关注代理商的发展能够解决迷宫。给定简单的迷宫，代理应该能够在其中导航并在最短的时间内到达出口。团结将作为代理商的测试环境。迷宫是由用户通过专用编辑器构建的。创建后，用户可以将代理放置在起点并定义奖励和惩罚权重，并根据这些参数训练AI。可以保存训练的模型，在新的迷宫上进行测试或通过不同的设置进行重新训练。  是否可以训练能够解决具有可变起点和出口的不同迷宫的好代理？也许程序中的变量不应该是这两个点，而是迷宫中的内容（例如障碍）或目标（而不是退出迷宫，而是要收集尽可能多的硬币） 您认为这个项目太雄心勃勃，无法在3个月内完成？ 与RL代理相比，A*算法是可以解决所有迷宫的算法。是真的吗？有什么区别？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/asiiaiapiazza     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw7g65/rl_agent_for_solving_mazes_doubts/</guid>
      <pubDate>Sun, 23 Feb 2025 10:41:23 GMT</pubDate>
    </item>
    <item>
      <title>学习政策以最大化满足B</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试学习一个控制策略，该策略在确保条件B时最大化变量。例如，机器人在将速度保持在给定范围内（b）的同时最大化能源效率（a）。 我的想法：将奖励定义为a *（b）。当B被满足B时，奖励将为= A，并且在违反B时为= 0。但是，这可能会在培训的早期引起稀疏的回报。我可能会使用模仿学习来初始化策略来帮助解决此问题。 是否有适合此类问题的现有框架或技术？我非常感谢任何方向或相关关键字！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iw3ijl/learning_policy_to_to_maximize_a_a_a_a_while_satisfying_b/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</guid>
      <pubDate>Sun, 23 Feb 2025 06:05:48 GMT</pubDate>
    </item>
    <item>
      <title>Gridworld RL培训：情节的奖励不会改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivx8mj/gridworld_rl_training_rewards_over_episodes/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivx8mj/gridworld_rl_training_rewards_over_episodes/</guid>
      <pubDate>Sun, 23 Feb 2025 00:20:14 GMT</pubDate>
    </item>
    <item>
      <title>博客：衡量政策梯度的理论观点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我在这里很新，很抱歉，如果它不符合规则（我找不到任何规则），但是我想与您分享我的博客，以衡量政策梯度的理论观点，我介绍了我们如何利用Raadon-Nikodym derivative不仅可以得出标准增强，还可以得出一些以后的版本，以及我们如何使用占用度量作为轨迹采样的倒入替代品。希望您能享受并给我一些反馈，因为我喜欢在RL  中分享直觉的重大解释，这是链接： https：//myxik.github.io/posts/measuretheoretic-view/    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforeverctionlearning/comments/1ivwzw9/blog_measure_theoretod_theoretic_theoretic_on_policy_gradients/”&gt; [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</guid>
      <pubDate>Sun, 23 Feb 2025 00:08:21 GMT</pubDate>
    </item>
    <item>
      <title>对于简单的MDP，此奖励值是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivu7uf/does_this_reward_values_makes_sense_for_a_simple/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！ 我正在尝试解决MDP，我为其定义了以下奖励，但是我有一个很难用价值迭代解决它。看来，国家价值功能不会收敛，经过一些迭代后，它将不再改善。所以，我在想也许问题是我的奖励结构？因为它有很大的变化。您认为这可能是一个原因吗？   r1 = {x1＆quord&#39;：500，&#39;x2＆quot” x2＆quot;：300，＆quort&#39;x3＆quot x3＆quort&#39;：100} r_2 = 1 r3 = -100 = -100 r4 = {x1＆quot;：-1000，x2＆quot;：-500，x3＆quort;：-200}    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/using_cauliflower320     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivu7uf/does_this_reward_values_makes_sense_for_a_simple/</guid>
      <pubDate>Sat, 22 Feb 2025 21:57:20 GMT</pubDate>
    </item>
    <item>
      <title>在美国顶级大学中录取的博士学位需要什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivr24m/what_is_required_for_a_phd_admit_in_a_top_tier_us/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有兴趣在强化学习中申请15个博士学位课程，并想了解一般的入学统计和期望。我目前是Virginia Tech的硕士学生，在RL撰写研究论文，是研究生水平深度RL课程的TA，并在计算机视觉方面具有先前的研究经验。如何使我的个人资料脱颖而出？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ivr24m/what_is_is_required_for_for_for_a_phd_admit_in_a_a_a_top_top_tier_us/”&gt; [links]      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/comments/1ivr24m/what_is_is_required_for_a_phd_phd_admit_in_a_a_a_a_a_a_top_top_top_tier_us/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivr24m/what_is_required_for_a_phd_admit_in_a_top_tier_us/</guid>
      <pubDate>Sat, 22 Feb 2025 19:37:07 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA CULE：“启用CUDA的ATARI 2600模拟器，该模拟器直接在GPU内存中呈现框架”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivog6c/nvidia_cule_a_cuda_enabled_atari_2600_emulator/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/masterscrat     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ivog6c/nvidia_cule_a_cuda_cuda_enabled_atari_atari_2600_emulator/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivog6c/nvidia_cule_a_cuda_enabled_atari_2600_emulator/</guid>
      <pubDate>Sat, 22 Feb 2025 17:46:51 GMT</pubDate>
    </item>
    <item>
      <title>强化学习是实现AGI的关键吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivns8i/is_reinforcement_learning_the_key_for_achieving/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是新RL。我已经看到了深刻的纸，他们非常强调RL。我知道GPT和其他LLM使用RL，但深刻的寻求使其成为主要。因此，我想学习RL，因为我想成为一名研究人员。是我的结论甚至正确，请验证它。如果是真的，请建议我来源。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tarnatraining822     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivns8i/is_reinforcement_learning_the_key_for_achieving/</guid>
      <pubDate>Sat, 22 Feb 2025 17:19:29 GMT</pubDate>
    </item>
    <item>
      <title>学习级研究项目思想</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivmj63/learninglevel_research_project_ideas/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在我收到任何仇恨评论我的问题之前，我想提一下，我知道它不是正确的心态，请选择“轻松问题” ，但是ID喜欢在3个月的时间范围内进行RL研究项目，以暴露于研究界，并深入研究我喜欢的RL。这是一种曝光，我想从事的一种破冰者的工作，大约一个月前开始学习的领域。 我想对社区的想法有一些乞egine的想法 - 我们可以冒险进入并涉足的友好RL研究领域。完成此操作后，我最终将进入RL的其他分支。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/extension-economy-78     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivmj63/learninglevel_research_project_ideas/</guid>
      <pubDate>Sat, 22 Feb 2025 16:27:23 GMT</pubDate>
    </item>
    <item>
      <title>基于物理的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivlnhu/physicsbased_environments/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，有机机器人， 我正在在物理模拟领域开发一个个人项目，并理解，通过流体动力学或热扩散。我一直在考虑应用程序不仅是为了设计目的，而且我目前对RL的兴趣，我一直在探索使用这些模拟在这些领域训练控制器的想法，例如在湍流下改进飞机控制或对数据的最佳控制中心冷却系统。  在此引言中，我想了解是否需要这些类型的环境来培训行业中的RL算法。  和裸露的牢记，我知道需要从模拟到权衡速度和准确性的不同水平的忠诚度 - 也许初步的训练低忠诚度，然后过渡到高忠诚度，将是无缝的一个加号。 我很想知道您对此的想法和/或知道行业对这类问题的需求。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/navier-gives-strokes     [link]   ＆＃32;   [comment]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivlnhu/physicsbased_environments/</guid>
      <pubDate>Sat, 22 Feb 2025 15:49:31 GMT</pubDate>
    </item>
    <item>
      <title>RL解决多个机器人问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用在共享环境中导航的多个移动机器人进行模拟。每个机器人都有一个预先加载的空间地图，并使用范围传感器（例如飞行传感器的时间）进行本地化。最初的全球路径计划是为每个机器人独立完成的，而无需考虑其他机器人。一旦开始移动，他们就可以检测到附近的机器人位置，速度和计划的途径以避免碰撞。 问题是，在紧密的空间中，他们经常被困在一种僵局中。在没有机器人可以移动的地方，他们都互相阻挡。一个人可以很容易地看到，如果说，1个机器人会向前移动一点，另一个机器人向前移动并转动一点，其余的都可以清除。但是在基于规则的系统中编码这种逻辑非常困难。 我正在考虑使用ML/ RL来解决此问题，但是我想知道这是否是一种实用方法。有没有人尝试解决RL的类似问题？您将如何处理？很想听听您的想法。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/</guid>
      <pubDate>Sat, 22 Feb 2025 11:41:21 GMT</pubDate>
    </item>
    <item>
      <title>我如何学习新手的模型预测控制。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivgjbg/how_can_i_learn_model_predictive_control_as_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是对控制方案的新手。我有一项在倒摆上实施的MPC任务。我需要学习。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/on_yesterday_2539      [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivgjbg/how_can_i_learn_model_predictive_control_as_a/</guid>
      <pubDate>Sat, 22 Feb 2025 11:14:47 GMT</pubDate>
    </item>
    <item>
      <title>GRPO与进化策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iv6ui7/grpo_vs_evolution_strategies/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   grpo看起来不像（或可以从在这里？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/majestic-tap1577     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iv6ui7/grpo_vs_evolution_strategies/</guid>
      <pubDate>Sat, 22 Feb 2025 01:12:41 GMT</pubDate>
    </item>
    </channel>
</rss>