<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 18 Feb 2024 18:15:05 GMT</lastBuildDate>
    <item>
      <title>算法收敛于单一行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1atqur7/algorithm_converge_to_single_behaviour/</link>
      <description><![CDATA[嘿 我有一艘飞船需要躲避导弹。 奖励是导弹经过的最短距离宇宙飞船。我随机化船舶偏航角度（方向），我使用 td3，但有些角度更容易解决，这会导致所有角度的单一机动，因为奖励传播到更具挑战性的角度。 （这个动作可能对这些角度来说是最佳的）这是一个连续的问题，我尝试使用常规噪声和奥恩斯坦噪声。 （我宁愿不使用SAC） TL;DR飞船躲避导弹类型，有些角度更容易学习，所有角度都收敛到更容易的角度学习的行为 &lt; !-- SC_ON --&gt;  由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1atqur7/algorithm_converge_to_single_behaviour/</guid>
      <pubDate>Sun, 18 Feb 2024 10:08:51 GMT</pubDate>
    </item>
    <item>
      <title>有人可以帮助我了解如何进行策略迭代吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1atafso/could_anyone_please_help_me_understand_how_to_do/</link>
      <description><![CDATA[      我观看了很多视频，但我很难理解政策的流程迭代。有人可以提供使用所附示例图像实现最佳策略的分步指南吗？ ​ https://preview.redd.it/zzloc9ey87jc1.png?width=1102&amp;format=png&amp;auto= webp&amp;s=3399d830ce0107b5ff48637b3949f1e35a17849a 在此场景中，转移概率如下：A = 0.61、B = 0.39、C = 0.47、D = 0.53、E = 0.84 和 F = 0.16。将连续迭代之间的最大误差 (ε) 视为 0.01，将折扣因子 (γ) 视为 0.2。利用策略迭代方法，“Standing”状态的值是多少？ 提前谢谢。   由   提交 /u/thesmudgelord   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1atafso/could_anyone_please_help_me_understand_how_to_do/</guid>
      <pubDate>Sat, 17 Feb 2024 19:45:12 GMT</pubDate>
    </item>
    <item>
      <title>从头开始在 Unity 中训练 FlappyBird：5 分钟内 10k 个管道！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1asl0ob/training_flappybird_in_unity_from_scratch_10k/</link>
      <description><![CDATA[   /u/imitagent  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1asl0ob/training_flappybird_in_unity_from_scratch_10k/</guid>
      <pubDate>Fri, 16 Feb 2024 22:04:19 GMT</pubDate>
    </item>
    <item>
      <title>专家的混合解锁深度强化学习的参数缩放</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ashob2/mixtures_of_experts_unlock_parameter_scaling_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.08609 摘要：  （自）监督学习模型最近的快速进展是很大一部分是通过经验缩放定律预测的：模型的性能与其大小成正比。然而，对于强化学习领域来说，类似的缩放定律仍然难以捉摸，增加模型的参数数量通常会损害其最终性能。在本文中，我们证明了将专家混合 (MoE) 模块，特别是软 MoE（Puigcerver 等人，2023）纳入基于价值的网络会产生更多参数可扩展的模型，性能的显着提高就证明了这一点跨越各种训练制度和模型大小。因此，这项工作为制定强化学习的缩放法则提供了强有力的经验证据。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ashob2/mixtures_of_experts_unlock_parameter_scaling_for/</guid>
      <pubDate>Fri, 16 Feb 2024 19:46:19 GMT</pubDate>
    </item>
    <item>
      <title>RL 目前有什么用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1arnyvq/what_is_rl_good_for_currently/</link>
      <description><![CDATA[ 由   提交 /u/BadMeditator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1arnyvq/what_is_rl_good_for_currently/</guid>
      <pubDate>Thu, 15 Feb 2024 19:34:51 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 PPO 导航问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/</link>
      <description><![CDATA[      我正在尝试使用 PPO 算法来解决一个简单的机器人导航问题。这是环境截图。 ​ https://preview.redd.it/ja7tq5v5uqic1.png?width=577&amp;format=png&amp;auto=webp&amp;s=e20b72e7f0c29b51c9e890b 3c33cec686d2327f1 &lt; p&gt;机器人（纯蓝色）必须导航到目标配置（空蓝色圆圈）。 演员网络设置为将单个灰度图像作为输入并输出下一个代理动作。  批评者网络将图像和当前时间步作为输入，并输出预期回报。 动作集为  wait&lt; /li&gt; 向前移动 向后移动 旋转 30 度 旋转 -30 度  每一步的奖励由以下公式给出： -0.1 + (dist_prev - dist_curr) + 100（如果达到目标）- 10（如果撞墙） 我使用的网络模型与 Atari DQN Nature 论文中使用的网络模型大致相同。 我面临的困难是，智能体在几千集之后似乎没有学到任何东西。 这些是我的 PPO 超参数：  GAMMA = 0.95 TRAJECTORIES_PER_LEARNING_STEP = 10 UPDATES_PER_LEARNING_STEP = 10 MAX_STEPS_PER_EPISODE = 100 ENTROPY_LOSS_COEF = 0 V_LOSS_CEOF = 0.5 CLIP = 0.2 LR = 3e-4  ​ 这是每集奖励的平滑图，它似乎只表现出随机行为。 ​ https://preview.redd.it/ 1yg4c1acwqic1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=cd55d2c2b4b55bf66dce593b0934a6bc60f24987 问题：  为什么不起作用？&lt; /li&gt; 它应该有效吗？ 您希望它需要多少集？  如果需要，我很乐意分享代码。预先感谢您的评论！ ​   由   提交/u/david-wb  /u/david-wb  reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/</guid>
      <pubDate>Thu, 15 Feb 2024 12:52:14 GMT</pubDate>
    </item>
    <item>
      <title>帮助自定义环境 Pettingzoo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</link>
      <description><![CDATA[我有一个自定义环境，我将代理设置为  self.agents = [&quot;EV_&quot; + str(r) for r in range(num_agents)]  当以以下形式测试环境时，我收到下一个错误： from EVenv import DepotEnv from pettingzoo.test import parallel_api_test from pettingzoo.test import api_test if __name__ == &quot;__main__&quot;: env = DepotEnv(num_agents=3) parallel_api_test(env, num_cycles=1000)  ，第 7 行，在  parallel_api_test(env, num_cycles=1000) 文件“C:\Users\” luisb\EVCHARGING\EVenv\lib\site-packages\pettingzoo\test\parallel_test.py”，第 122 行，parallel_api_test  assert ( AssertionError: [&#39;EV_0&#39;, &#39; EV_1&#39;, &#39;EV_2&#39;] != set()  我到处都找不到解决方案，我尝试设置代理，但它也不起作用，有什么想法吗？     提交者   /u/Barbajan22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</guid>
      <pubDate>Thu, 15 Feb 2024 01:11:18 GMT</pubDate>
    </item>
    <item>
      <title>自然语言强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqwxf0/natural_language_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2402.07157 OpenReview：https:// /openreview.net/forum?id=0VzU2H13qj 摘要：  强化学习（RL）在以下方面表现出了卓越的能力：学习决策任务的策略。然而，强化学习常常受到样本效率低、缺乏可解释性和监督信号稀疏等问题的阻碍。为了解决这些限制，我们从人类学习过程中汲取灵感，引入了自然语言强化学习 (NLRL)，它创新地将强化学习原理与自然语言表示相结合。具体来说，NLRL 重新定义了自然语言空间中的任务目标、策略、价值函数、贝尔曼方程和策略迭代等 RL 概念。我们介绍如何利用 GPT-4 等大型语言模型 (LLM) 的最新进展来实际实施 NLRL。对表格 MDP 的初步实验证明了 NLRL 框架的有效性、效率和可解释性。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqwxf0/natural_language_reinforcement_learning/</guid>
      <pubDate>Wed, 14 Feb 2024 20:28:52 GMT</pubDate>
    </item>
    <item>
      <title>为机器人技术贡献提出重要的强化学习建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqq0df/suggest_important_rl_for_robotics_contributions/</link>
      <description><![CDATA[我多年来一直在研究应用强化学习，并且很幸运能够获得博士学位。人形机器人强化学习候选者。很兴奋！ :) 你能提示我一些 RL + 机器人领域必读的文献吗？   由   提交 /u/seawee1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqq0df/suggest_important_rl_for_robotics_contributions/</guid>
      <pubDate>Wed, 14 Feb 2024 15:47:49 GMT</pubDate>
    </item>
    <item>
      <title>帮助确定为什么我的 PPO 的 JAX 实现比 PyTorch 实现慢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqeiyo/help_determining_why_my_jax_implementation_of_ppo/</link>
      <description><![CDATA[大家好！我正在学习 JAX，作为学习的一部分，我尝试重新创建一个简单的离散操作版本的 PPO（最初基于 cleanRL JAX PPO 和 cleanRL PyTorch PPO）。然而，我发现它比本质上非常相似的代码的 PyTorch 版本要慢得多。谁能告诉我我在 JAX 实现中可能做错了什么？我在这里故意避免使用 envpool，只是为了坚持更简单的 Gymnasium 设置。 这是我的 JAX 脚本（全部在一个文件中，并且可以在一个文件中运行，如果您有必要的话，只需复制并粘贴即可）包）：https://pastes.io/kronipluiy 这是等效的 PyTorch 脚本：https://pastes.io/u5oz948e27   由   提交 /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqeiyo/help_determining_why_my_jax_implementation_of_ppo/</guid>
      <pubDate>Wed, 14 Feb 2024 04:43:06 GMT</pubDate>
    </item>
    <item>
      <title>应用于 FrozeLakeV1（体育馆）的 QLearning 无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aq5dfj/qlearning_applied_to_frozelakev1_gymnasium_doesnt/</link>
      <description><![CDATA[      我的代码与 本教程。然而，与我的教程中的情况不同，代理根本不会学习。奖励始终为 0（因为代理永远无法达到目标。（下面是奖励（y 轴）和情节（x 轴）的图表。我的代码可以在 这里。 https://preview.redd.it/yeejhatv6fic1.png?width=567&amp;format=png&amp;auto=webp&amp;s=7021334596b72295082ca9bbde6303684ffe956a &lt;!-- SC_ON - -&gt;   提交者    /u/Miggus_amogus   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aq5dfj/qlearning_applied_to_frozelakev1_gymnasium_doesnt/</guid>
      <pubDate>Tue, 13 Feb 2024 21:39:42 GMT</pubDate>
    </item>
    <item>
      <title>有 Minigrid 的 RL 基准吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aq1i8o/any_rl_benchmarks_for_minigrid/</link>
      <description><![CDATA[嗨，有人知道在 minigrid env 中运行的代理的基准/排行榜/预训练权重吗？谢谢！   由   提交 /u/pengzhenghao   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aq1i8o/any_rl_benchmarks_for_minigrid/</guid>
      <pubDate>Tue, 13 Feb 2024 19:03:01 GMT</pubDate>
    </item>
    <item>
      <title>quilterai 筹集 1000 万美元，构建 RL 支持的硬件编译器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</link>
      <description><![CDATA[      强化学习最令人兴奋的行业应用之一即将规模化！    由   提交 /u/mccrearyd   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</guid>
      <pubDate>Tue, 13 Feb 2024 18:03:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在整个阅读过程中应用萨顿和巴托的概念</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</link>
      <description><![CDATA[目前正在通过阅读这本书自学强化学习萨顿巴托：从头到尾介绍强化学习。我已经读了 4 章半了，感觉被它所强加的所有理论淹没了，有没有关于如何应用这些概念的随附材料或指南，以便我可以放慢一点的速度，并真正内化这些概念我正在阅读的内容？理想情况下，这些将应用于编程环境。 如果有人有时间提供一些建议，我将非常感激！  &amp;# 32；由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</guid>
      <pubDate>Tue, 13 Feb 2024 18:02:05 GMT</pubDate>
    </item>
    <item>
      <title>开始 RL 是否需要 ML/DL 背景？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apgn0t/is_a_background_in_mldl_required_to_start_in_rl/</link>
      <description><![CDATA[我现在正在学习 ML，是为了深入研究 RL，我是在浪费时间吗？为了更深入地了解 RL，我可以学习什么作为先决条件？   由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apgn0t/is_a_background_in_mldl_required_to_start_in_rl/</guid>
      <pubDate>Tue, 13 Feb 2024 01:06:48 GMT</pubDate>
    </item>
    </channel>
</rss>