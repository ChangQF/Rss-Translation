<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 13 Oct 2024 06:22:37 GMT</lastBuildDate>
    <item>
      <title>面向初学者的奖励函数发现简单教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</guid>
      <pubDate>Sat, 12 Oct 2024 13:28:33 GMT</pubDate>
    </item>
    <item>
      <title>关于 ALE 论文和超参数调优的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1zrgr/question_regarding_ale_paper_and_hyper_parameter/</link>
      <description><![CDATA[我一直在阅读此链接上的论文“Arcade 学习环境：通用代理的评估平台”：https://arxiv.org/abs/1207.4708，我不确定他们如何进行超参数调整。  据我所知，他们在 5 个不同的环境中优化超参数，然后使用这些超参数对其余环境进行训练和评估。  那么我的问题是，这是如何工作的？我很难理解。如何同时在多个环境中优化超参数？  我假设所有环境都有相同的观察和动作空间，但如何同时在不同环境中进行训练和评估？   由    /u/IAmNotMarcus  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1zrgr/question_regarding_ale_paper_and_hyper_parameter/</guid>
      <pubDate>Sat, 12 Oct 2024 13:08:55 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium - 股票交易环境的终止状态与截断状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1yi0c/gymnasium_terminated_vs_truncated_state_for_stock/</link>
      <description><![CDATA[嗨， 所以我读了一些关于 gym.Env 中终止和截断之间的区别。根据我的理解： terminated = True -&gt; 表示达到 MDP 定义下的终止状态（因此取决于您如何定义底层 MDP） truncated = True -&gt; 由于 MDP 中未明确定义的条件，情节结束。例如（来自 Gym Docu），代理在物理上超出界限或达到时间限制。 虽然这对于机器人任务来说是有意义的，但当涉及到我正在处理的问题（用于交易/管理金融资产的代理）时，我缺少一些部分。我主要有两个问题：  代理以数据框的形式在给定长度 T 的一系列状态（每天一个）上进行训练。一旦到达情节的结尾，我会设置 done = True（在 gym 0.26 之前的版本下）。现在我必须设置 determinant = True 或 truncated = True。这里什么才有意义？请记住，代理的目标是最大化利润，因此没有“明确的目标条件”表明代理成功或失败了特定任务（就像在机器人技术中一样）。 假设我正在使用 StableBaselines 之类的框架。代理对 terminated = True 和 truncated = True 的解释是否不同？     提交人    /u/Intelligent-Put1607   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1yi0c/gymnasium_terminated_vs_truncated_state_for_stock/</guid>
      <pubDate>Sat, 12 Oct 2024 11:56:49 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关攻读 RL 和机器人学博士学位的建议，同时处理签证问题和职业变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1wgby/seeking_advice_on_pursuing_a_phd_in_rl_and/</link>
      <description><![CDATA[我的职业生涯正处于十字路口，需要一些建议。我目前在海外一家大型科技公司担任高级 SDE，但我感觉并不充实。我计划转向 RL 和机器人技术，目标是未来获得博士学位。↳ 我的情况如下：↳  我将在一所优秀大学的机器人实验室担任研究助理，一直工作到 2025 年 6 月。 为了这个机会，我愿意大幅减薪（从每月 1x k+ 到 2k）。 我今年将申请硕士和博士课程，但我不确定我是否能进入一个好的课程。 我有大约 12 万美元的存款，但我担心在过渡期间的财务问题。 作为中国公民，签证问题令人担忧。即使有工作机会，对于拥有学士学位的中国女性来说，H1B 抽签的成功率也只有 20%。 我也在考虑个人生活方面的问题，比如约会和结婚，这可能会对签证问题有所帮助。我有点漂亮，这对我来说约会容易一些，但保持美丽需要花费大量的时间和精力。  我的主要目标是专注于 RL 和机器人技术，发表优秀的论文，并在该领域产生影响。但是，我担心会因财务问题和签证问题而分心。↳ 有人遇到过类似的情况吗？或者对平衡职业目标和实际问题有什么建议吗？我如何在处理这些其他因素的同时专注于我的研究？↳ 任何见解或经验都将不胜感激！    提交人    /u/FaithlessnessFree554   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1wgby/seeking_advice_on_pursuing_a_phd_in_rl_and/</guid>
      <pubDate>Sat, 12 Oct 2024 09:31:02 GMT</pubDate>
    </item>
    <item>
      <title>“更大、更规则、更乐观：计算和样本高效连续控制的扩展”，Nauman 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1il8n/bigger_regularized_optimistic_scaling_for_compute/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2405.16158 摘要：  强化学习 (RL) 中的样本效率传统上由算法增强驱动。在这项工作中，我们证明扩展也可以带来显着的改进。我们对扩展模型容量和特定领域的 RL 增强之间的相互作用进行了彻底的研究。这些实证发现为我们提出的 BRO（更大，正则化，乐观）算法的设计选择提供了信息。BRO 背后的关键创新是强正则化允许有效扩展评论家网络，这与乐观探索相结合，可带来卓越的性能。 BRO 取得了最先进的成果，在 DeepMind Control、MetaWorld 和 MyoSuite 基准的 40 个复杂任务中，其表现显著优于领先的基于模型和无模型的算法。BRO 是第一个在极具挑战性的狗和人形任务中实现近乎最优策略的无模型算法。    [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1il8n/bigger_regularized_optimistic_scaling_for_compute/</guid>
      <pubDate>Fri, 11 Oct 2024 19:57:17 GMT</pubDate>
    </item>
    <item>
      <title>“奖励进步：扩展 LLM 推理的自动化流程验证器”，Setlur 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1igfb/rewarding_progress_scaling_automated_process/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2410.08146 摘要：  一种用于改进大型语言模型推理的有前途的方法是使用过程奖励模型 (PRM)。PRM 在多步推理跟踪的每个步骤中提供反馈，与仅在最后一步提供反馈的结果奖励模型 (ORM) 相比，可能改善信用分配。但是，收集密集的每步人工标签是不可扩展的，并且迄今为止，从自动标记的数据训练 PRM 带来的收益有限。为了通过针对 PRM 运行搜索或将其用作强化学习 (RL) 的密集奖励来改进基础策略，我们问：“我们应该如何设计过程奖励？”。我们的关键见解是，为了有效，步骤的过程奖励应该衡量进度：在采取该步骤之前和之后，未来产生正确响应的可能性的变化，与 RL 中的步骤级优势概念相对应。至关重要的是，应该在不同于基础策略的证明者策略下衡量这一进展。我们从理论上描述了一组好的证明者，我们的结果表明，优化这些证明者的过程奖励可以改善测试时搜索和在线 RL 期间的探索。事实上，我们的描述表明，弱证明者策略可以显著改善更强大的基础策略，我们也通过经验观察到了这一点。我们通过训练过程优势验证器 (PAV)来预测此类证明器下的进展，从而验证了我们的说法，并表明与 ORM 相比，针对 PAV 的测试时搜索准确率高出 8% 以上，计算效率高出 1.5-5 倍。通过 PAV 密集奖励的在线 RL 实现了 首批结果之一，与 ORM 相比，样本效率提高了 5-6 倍，准确率提高了 6% 以上。    [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1igfb/rewarding_progress_scaling_automated_process/</guid>
      <pubDate>Fri, 11 Oct 2024 19:51:19 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero MCTS 的搜索深度有多深？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1iavg/how_deep_does_alphazero_mcts_search/</link>
      <description><![CDATA[我在 A0 的论文中看到，他们在训练期间运行了 1600 次 MCTS 迭代，与单独使用策略网络相比，搜索结果可获得 1000+ elo 增益。但是，假设在围棋中，每个状态有 5 种合理的走法，那么经过 5 步深度后，5^5 = 3125 &gt; 1600。显然，这是一个粗略的估计，可能由于剪枝能力而被低估，但直观地看，1600 感觉很少。有没有发布 alphazero 搜索深度的信息，或者 alphago lee 在与李世石对弈时搜索的深度？    提交人    /u/DumplingLife7584   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1iavg/how_deep_does_alphazero_mcts_search/</guid>
      <pubDate>Fri, 11 Oct 2024 19:44:19 GMT</pubDate>
    </item>
    <item>
      <title>按顺序运行数据点或选择随机点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1df5n/running_though_the_datapoints_sequentially_or/</link>
      <description><![CDATA[我正在使用 sb3 训练交易环境，我的数据集由 250k 个数据点组成。我不确定我是否应该让环境始终从数据集的开头开始并按顺序运行直到 250k 结束以计算奖励，或者我应该使用固定的情节长度（例如 50k）并让它从每个情节的随机点开始。哪个可以让训练效果更好？    提交人    /u/Acceptable_Egg6552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1df5n/running_though_the_datapoints_sequentially_or/</guid>
      <pubDate>Fri, 11 Oct 2024 16:08:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 Mario 和 AI（深度强化学习）交易比特币</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g19oxu/trading_bitcoin_using_mario_ai_deep_reinforcement/</link>
      <description><![CDATA[大家好， 在过去的两个月里，我一直在做一个很酷的小项目，并决定制作一个视频，以鼓励自己做更多这样的项目。视频标题为： 使用 Mario 和 AI（深度强化学习）交易比特币 这是关于利用强化学习来交易比特币。这项工作非常酷，我玩得很开心。如果你有兴趣，请观看并告诉我你的想法！ https://www.youtube.com/watch?v=dACkVX5PkVc     提交人    /u/Kibo178   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g19oxu/trading_bitcoin_using_mario_ai_deep_reinforcement/</guid>
      <pubDate>Fri, 11 Oct 2024 13:23:08 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习和脉冲神经网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g15gw3/model_based_reinforcement_learning_and_spiking/</link>
      <description><![CDATA[有人知道是否有基于模型的强化学习和脉冲神经网络的相关论文吗？或者只是关于带有 snn 的模型的相关论文？    提交人    /u/Embri21   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g15gw3/model_based_reinforcement_learning_and_spiking/</guid>
      <pubDate>Fri, 11 Oct 2024 09:01:31 GMT</pubDate>
    </item>
    <item>
      <title>“评估生成模型中隐含的世界模型”，Vafa 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0tqvr/evaluating_the_world_model_implicit_in_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0tqvr/evaluating_the_world_model_implicit_in_a/</guid>
      <pubDate>Thu, 10 Oct 2024 21:28:59 GMT</pubDate>
    </item>
    <item>
      <title>这个问题可以用 RL 解决吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0kz70/can_this_problem_be_solved_with_rl/</link>
      <description><![CDATA[您好， 我是 RL 的新手，正在研究一个问题，电动汽车需要决定何时何地充电，以最大限度地减少等待时间和充电成本（价格随时间波动）。 我最初的想法是将每辆电动汽车视为一个代理，每个电动汽车都有自己的观察结果，例如电池状态、充电站位置、电价以及每个充电站的排队长度。 行动空间为： • 0：延迟充电（下一小时再决定） • 1：在充电站 1 充电 • 2：在充电站 2 充电 每个情节有 24 个时间段，代理只有在选择充电站后才会获得奖励。 我的问题是： 一旦电动汽车选择了一个充电站，它就会停止做出决策，因此轨迹会提前结束。例如，某些轨迹可能是 {0,0,0,1}（在 t=4 时转到 CS1），而其他轨迹可能是 {2}（在 t=0 时转到 CS2）。只有当 EV 选择充电站时，我才会获得奖励。 MARL 在这里仍然是一种好方法吗？ 我也不确定这个问题是否适合 MDP 框架，因为我见过的大多数论文都集中处理分配，当代理收到充电请求时，他们会立即决定充电站。 提前谢谢您！    提交人    /u/Full_Friendship8349   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0kz70/can_this_problem_be_solved_with_rl/</guid>
      <pubDate>Thu, 10 Oct 2024 15:07:19 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Q-Learning 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0ikbh/help_in_a_qlearning_project/</link>
      <description><![CDATA[      嘿，我是 RL 的新手，正在我的一位教授手下做一个项目。最初的任务之一是训练一个代理，使其能够在 5x5 网格中找到从随机初始化的起点到随机初始化的终点的最佳路径。 我学习了一些理论（主要来自 Medium 文章和 Chatgpt），并认为使用 Q 学习是一种很好的方法。然而我似乎陷入了困境，无论我如何更改参数或更改奖励结构都无济于事。训练结束时所采用的平均时间步长约为 16-17，对于这个简单的问题来说，这个数字确实很高，而且代理总体上表现不佳。 这是我的奖励结构+超参数+训练循环的片段 我尝试将奖励设为常数，减少（甚至消除）时间步长惩罚，并增加/减少几乎所有的超参数，但并没有取得太大的进步。 如果这是一个非常简单的问题，我很抱歉在这里发布，我可能犯了很多新手和基本错误。我将非常感激你们提供的所有帮助以及任何可以加深我理解的资源。谢谢！    提交人    /u/Hot_Program2634   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0ikbh/help_in_a_qlearning_project/</guid>
      <pubDate>Thu, 10 Oct 2024 13:15:59 GMT</pubDate>
    </item>
    <item>
      <title>强化学习提高化学反应性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0e0pj/reinforcement_learning_for_improving_chemical/</link>
      <description><![CDATA[我很高兴地告诉大家，我们孟买印度理工学院的研究小组（RBS 小组）最近在著名的《美国化学会志》（JACS）上发表了一篇论文，重点介绍了强化学习 (RL) 在提高化学反应性能方面的应用。 在复杂的化学世界中，优化反应条件可能是一项艰巨的任务，通常需要大量的反复试验。我们的论文提出了一种利用 RL 算法来预测和改善反应结果的新方法。通过将优化过程视为动态决策问题，我们能够显著提高反应产量和选择性。 我们希望我们的工作能够激发人工智能与化学交叉领域的进一步探索，促进该领域复杂问题的创新解决方案。 这里是链接https://pubs.acs.org/doi/full/10.1021/jacs.4c08866    提交人    /u/Kindly-Mortgage-2459   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0e0pj/reinforcement_learning_for_improving_chemical/</guid>
      <pubDate>Thu, 10 Oct 2024 08:20:47 GMT</pubDate>
    </item>
    <item>
      <title>梦想家与旧论文非常相似</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0d22d/dreamer_is_very_similar_to_an_older_paper/</link>
      <description><![CDATA[我随意浏览了 Yannic Kilcher 的旧视频，发现了这个视频，内容是关于 David Ha 和 Jürgen Schmidhuber 的论文“World Models”。我很惊讶地发现，尽管没有被引用或作者相同，但它提出了与 Dreamer（发表时间稍晚）非常相似的想法。 两者都涉及学习潜在动态，可以产生“梦想”环境，在这种环境中，RL 策略可以在不需要在真实环境中进行部署的情况下进行训练。即使是架构也基本相同，从观察自动编码器到处理实际前向演化的 RNN/LSTM 模型。 但是，尽管这些大体内容相同，但实际的论文结构却截然不同。 Dreamer 的论文有更好的实验和数值结果，以及不同想法的呈现方式。 我不确定这是否只是巧合，或者作者是否有一些共同的圈子。无论如何，我认为鉴于 Dreamer 的受欢迎程度，早期的论文应该得到更多的认可。    提交人    /u/irrelevant_sage   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0d22d/dreamer_is_very_similar_to_an_older_paper/</guid>
      <pubDate>Thu, 10 Oct 2024 07:04:38 GMT</pubDate>
    </item>
    </channel>
</rss>