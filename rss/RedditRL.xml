<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 09 Jan 2024 15:14:35 GMT</lastBuildDate>
    <item>
      <title>在可塑性之前推断神经活动作为超越反向传播的学习基础</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192aniz/inferring_neural_activity_before_plasticity_as_a/</link>
      <description><![CDATA[论文：https://www.nature.com/articles/s41593-023-01514-1 预印本版本：https://www.biorxiv.org/content/10.1101/2022.05.17.492325  代码：https://github.com/YuhangSong/Prospective-Configuration 摘要：  对于人类和机器来说，学习的本质是查明信息处理管道中的哪些组件对其输出中的错误负责，一个被称为“学分分配”的挑战。长期以来，人们一直认为学分分配最好通过反向传播来解决，这也是现代机器学习的基础。在这里，我们提出了一个根本不同的信用分配原则，称为“预期配置”。在前瞻性配置中，网络首先推断学习应产生的神经活动模式，然后修改突触权重以巩固神经活动的变化。我们证明，与反向传播相比，这种独特的机制（1）是在完善的皮质回路模型家族中进行学习的基础，（2）使得学习能够在生物有机体面临的许多环境中更加高效和有效，（3） ）再现了在不同的人类和大鼠学习实验中观察到的令人惊讶的神经活动和行为模式。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192aniz/inferring_neural_activity_before_plasticity_as_a/</guid>
      <pubDate>Tue, 09 Jan 2024 09:15:44 GMT</pubDate>
    </item>
    <item>
      <title>帮助使用 RL 实现背包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192a0wo/help_in_implementing_knapsack_using_rl/</link>
      <description><![CDATA[我想使用强化学习实现有界和无界的背包问题。如何开始并实施它。请任何人帮忙！   由   提交 /u/Formal-Champion4260    reddit.com/r/reinforcementlearning/comments/192a0wo/help_in_implementing_knapsack_using_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192a0wo/help_in_implementing_knapsack_using_rl/</guid>
      <pubDate>Tue, 09 Jan 2024 08:30:39 GMT</pubDate>
    </item>
    <item>
      <title>社区可能开展的活动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19283fk/possible_activities_from_a_community/</link>
      <description><![CDATA[我想知道社区可以组织哪些可能的活动来在其他个人中改进/传播这一领域。我想出了以下几个。如果您愿意，请随意批评它们/添加更多内容。 1. 每周论文阅读 2. 撰写论文摘要/疑难主题解释 3. 针对这些主题的视频制作比赛 4. 解释代码实现的视频   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19283fk/possible_activities_from_a_community/</guid>
      <pubDate>Tue, 09 Jan 2024 06:23:35 GMT</pubDate>
    </item>
    <item>
      <title>限制机器人的适配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19275hp/restricting_the_adaptation_of_robot/</link>
      <description><![CDATA[虽然我希望机器人比人类有所改进，但你看，人类对什么是对的，什么是错的，我们有一定的感觉。尽早定义我们的性格，我们是什么，一旦我们陷入新环境，我们就开始放松我们的性格，开始变得像新环境中的人一样，即使我们的性格与新环境非常相反，但我们开始适应事物这是我们不想要的。这就是为什么（根据我的直觉）逆强化学习并不是一个训练机器人的好主意，如果它们落入我们不希望的新环境中，它就会忘记它的原则，所以我们可以做什么如何使这些机器人在其原理上更加稳健？因为随着人类思维的发展或人类反馈的强化学习的发展，它将被鼓励/奖励去适应环境。如果它的这些原则太强，它就会被迫离开那个环境，因为如果没有任何东西符合它的原则，它就什么也做不了。所以我们希望机器人能够在环境中生存，但不要忘记它的原则。任何直观的答案都可以。   由   提交/u/vyknot4wongs  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19275hp/restricting_the_adaptation_of_robot/</guid>
      <pubDate>Tue, 09 Jan 2024 05:29:42 GMT</pubDate>
    </item>
    <item>
      <title>使用非 MARL 库进行 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1926w72/using_nonmarl_library_for_marl/</link>
      <description><![CDATA[Stable Baselines 3(SB3) 显然不支持 MARL。我正在使用带有 SB3 PPO 的自定义环境，以 CTDE 方法进行 MARL Boid 植绒。  我想知道我的设置是否已在代码中成功实现了 MARL，或者是否存在问题，我需要采用不同的方式来进行。 我的代码&lt; /strong&gt;: Boid 植绒   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1926w72/using_nonmarl_library_for_marl/</guid>
      <pubDate>Tue, 09 Jan 2024 05:14:56 GMT</pubDate>
    </item>
    <item>
      <title>关于使用 LLM 解决顺序控制问题的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1922g57/questions_about_using_llms_for_sequential_control/</link>
      <description><![CDATA[我对法学硕士/基础模型非常陌生。我正在尝试一些开源 LLM 模型，我发现通过直接提示将它们用于类似 RL 的问题非常耗时。 （LLM 的时间步长选定操作大约需要 10 秒）而对于深度强化学习模型，可能需要不到 0.001 秒（？）。 我还没有深入研究它，但我什至想知道如果我使用 API 调用。如果我使用最快、最先进的模型，它会达到与深度强化学习模型相同的速度吗？ （我知道法学硕士非常庞大，是否有可能加快他们的推理速度？） ​   由   提交/u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1922g57/questions_about_using_llms_for_sequential_control/</guid>
      <pubDate>Tue, 09 Jan 2024 01:37:23 GMT</pubDate>
    </item>
    <item>
      <title>Lunai 简介 - 无需任何编码的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1921w5e/introducing_lunai_reinforcement_learning_without/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/Feralzi  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1921w5e/introducing_lunai_reinforcement_learning_without/</guid>
      <pubDate>Tue, 09 Jan 2024 01:11:19 GMT</pubDate>
    </item>
    <item>
      <title>导入错误：libmujoco150.so：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/</link>
      <description><![CDATA[我正在尝试构建一个包含 mujoco 的 Docker 映像。此外，我希望它出现在我的自定义地址中。  ​ 这是我创建的 Dockerfile。我引用了此处使用的环境变量 -  FROM ubuntu:22.04 WORKDIR /app SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] RUN mkdir -p myhome/house ENV HOME=&quot;/myhome/house:${PATH}&quot; RUN 回显“Hello World！”运行 apt-get update &amp;&amp; apt-get install -y\libosmesa6-dev\sudo\wget\curl\unzip\gcc\g++\&amp;&amp; apt-get install \ libosmesa6-dev \ &amp;&amp; rm -rf /var/lib/apt/lists/* ENV DEBIAN_FRONTEND=非交互式 ENV PATH=&quot;/miniconda3/bin:${PATH}&quot; ARG PATH=“/miniconda3/bin:${PATH}”运行 cd / \ &amp;&amp; mkdir -p /miniconda3 \ &amp;&amp; wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /miniconda3/miniconda.sh \ &amp;&amp; bash /miniconda3/miniconda.sh -b -u -p /miniconda3 \ &amp;&amp; /miniconda3/bin/conda init bash \ &amp;&amp;源 ~/.bashrc \ &amp;&amp; conda init \ &amp;&amp; conda create -y -n myenv python=3.8 \ &amp;&amp; conda update -y conda WORKDIR /~ RUN wget https://roboti.us/download/mjpro150_linux.zip \ &amp;&amp;解压mjpro150_linux.zip \ &amp;&amp; mkdir ~/.mujoco \ &amp;&amp; mv mjpro150 ~/.mujoco \ &amp;&amp; wget https://roboti.us/file/mjkey.txt \ &amp;&amp; mv mjkey.txt ~/.mujoco \ &amp;&amp; rm mjpro150_linux.zip ENV MJLIB_PATH=“/myhome/house/.mujoco/mjpro150/bin/libmujoco150.so:${MJLIB_PATH}” ENV LD_LIBRARY_PATH=“/myhome/house/.mujoco/mjpro150/bin:${LD_LIBRARY_PATH}” ENV MUJOCO_PY_MUJOCO_PATH=“/myhome/house/.mujoco/mjpro150:${MUJOCO_PY_MUJOCO_PATH}” ENV MUJOCO_PY_MJKEY_PATH=“/myhome/house/.mujoco/mjkey.txt:${MUJOCO_PY_MJKEY_PATH}”运行 cd /miniconda3/envs/myenv/lib/ &amp;&amp; mv libstdc++.so.6 libstdc++.so.6.old &amp;&amp; ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6 SHELL [“conda”, “run”, “-n”, “myenv”, “/ bin/bash”，“-c”] EXPOSE 5003 RUN pip install --no-cache-dir “Cython&lt;3” RUN pip install mujoco-py==1.50.1.0  构建不断失败，错误显示在顶部。有人可以帮忙解决这个问题吗？   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/</guid>
      <pubDate>Mon, 08 Jan 2024 23:00:50 GMT</pubDate>
    </item>
    <item>
      <title>最佳强化学习研究框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191yu6y/best_rl_research_framework/</link>
      <description><![CDATA[我需要启动一个新的强化学习项目，并问自己哪个强化学习库或框架最适合学术研究。我假设我将使用gymnasium 来构建我需要构建的自定义环境，但我不确定策略（算法）的库。这个想法是能够在自定义环境中切换到几种不同的算法。我过去使用稳定的基线，然后从头开始编写 PPO 实现，并使用了一段时间。现在我想过渡到更灵活的东西，我不必从头开始实现不同的算法。稳定的基线仍然是最好的选择吗？    由   提交 /u/alebrini   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191yu6y/best_rl_research_framework/</guid>
      <pubDate>Mon, 08 Jan 2024 22:59:01 GMT</pubDate>
    </item>
    <item>
      <title>Rich Sutton 的 10 个人工智能口号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191waws/rich_suttons_10_ai_slogans/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191waws/rich_suttons_10_ai_slogans/</guid>
      <pubDate>Mon, 08 Jan 2024 21:16:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 采访里奇·萨顿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191rmsl/d_interview_with_rich_sutton/</link>
      <description><![CDATA[ 由   提交 /u/atgctg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191rmsl/d_interview_with_rich_sutton/</guid>
      <pubDate>Mon, 08 Jan 2024 18:10:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么奖励价值高于累积奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191gc9m/why_reward_to_go_values_over_cumulative_rewards/</link>
      <description><![CDATA[嗨，强化学习新手，目前正在研究基于序列建模或基于离线强化学习的方法。当他们使用像架构这样的 GPT 时，我发现他们经常似乎将奖励作为每个时间步骤的代币嵌入之一以及动作和状态，而不是在该时间步骤中累计获得的奖励的天真的奖励-step？ 如果我错了，请纠正我，谢谢！   由   提交 /u/alchemistensei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191gc9m/why_reward_to_go_values_over_cumulative_rewards/</guid>
      <pubDate>Mon, 08 Jan 2024 08:18:45 GMT</pubDate>
    </item>
    <item>
      <title>机器人课程项目调查！任何经验都有帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</link>
      <description><![CDATA[大家好， 我正在与卡耐基梅隆大学和宾夕法尼亚大学的一些机器人专业学生合作开展一个课堂项目，以研究疼痛学者和行业专业人士在从事机器人开发时面临的问题。如果您从事或认识从事机器人开发流程任何部分的人，并且有 10 分钟的空闲时间，我们将非常感谢您的意见。我们希望获得广泛经验水平的意见。因此，我们重视刚开始接触机器人技术的人们以及具有多年经验的人们的意见。回复是匿名的，绝不反映绩效，因此我们要求您诚实回答。我们计划在 1 月 14 日之前收集回复（但如果调查之后开放，请随时贡献您的想法！）。  https://forms.gle/Mx247TgeDbEydY426 谢谢，   由   提交/u/awkyu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</guid>
      <pubDate>Sun, 07 Jan 2024 23:56:05 GMT</pubDate>
    </item>
    <item>
      <title>演奏乐器的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</link>
      <description><![CDATA[寻找任何已知的演奏乐器的模拟环境。例如，一个灵巧的特工在弹吉他。   由   提交 /u/Ultra-Neural   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</guid>
      <pubDate>Sun, 07 Jan 2024 21:31:00 GMT</pubDate>
    </item>
    <item>
      <title>这是从模型停止的地方继续训练的正确方法吗？稳定基线3、Pytorch、Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</link>
      <description><![CDATA[嗨， 我正在训练一个模型，昨天我保存并关闭了，因为已经很晚了，我需要睡觉。现在，今天我想从上次停下来的地方继续训练，但谷歌的结果好坏参半，2018 年、19 年、20 年的答案等等。 这是我的代码，如果有人可以确认这是正确的序列，我会很感激。 log_dir = &quot;/path/where/I/want/logs/saved&quot; model_dir =“/path/to/saved/zip/file” env = MyENV() env.reset () model = PPO(&quot;MlpPolicy&quot;, env, verbose = 1,tensorboard_log=log_dir) model .set_parameters(model_path, True) TIMESTEPS = 10000 CONTINUE_BOOKMARK = 35 #最新保存的文件是340000，所以350,000 将是下一个邮政编码... for i in range(CONTINUE_BOOKMARK, 51): model.learn （total_timesteps=TIMESTEPS，reset_num_timesteps=False，tb_log_name=“log_name_here”） model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 我即将运行它，但我担心我可能做得不对，如果它确实有效，那只是巧合。 ​ 编辑： 我最终使用了类似的东西 model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 唯一的事情是，tensorboard 日志看起来没有从之前的日志继续... &lt;!-- SC_ON - -&gt;  由   提交 /u/phantomBlurrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</guid>
      <pubDate>Sun, 07 Jan 2024 19:44:19 GMT</pubDate>
    </item>
    </channel>
</rss>