<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 05 Sep 2024 12:31:11 GMT</lastBuildDate>
    <item>
      <title>我不知道如何开始我的第一个 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</link>
      <description><![CDATA[过去我一直是 Unity 游戏开发者，我想尝试做一些强化学习 - 两个 AI 玩家之间的标签游戏。我一直在使用 anaconda 和 jupyter 笔记本进行我的第一个机器学习项目，到目前为止我很喜欢它。现在我想知道我该如何实现我的想法？什么环境？我应该使用 pygame 吗？有人有这方面的教程/课程吗？ 我不介意浏览无尽的库文档。    提交人    /u/JMB4200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</guid>
      <pubDate>Thu, 05 Sep 2024 11:00:04 GMT</pubDate>
    </item>
    <item>
      <title>“RTX 4060 足以使用 Isaac Sim 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</link>
      <description><![CDATA[      我正在使用 Isaac Gym 进行运动任务训练，但我想在安装了 RTX 4060 的家用电脑上进行训练。它足以完成这项任务吗？需求文档列出的最低要求是 GeForce RTX 3070，这比 4060 略好一些。如果有人有使用 4060 的经验，请告诉我它是否足够。 https://preview.redd.it/vx4oohnhsymd1.png?width=1120&amp;format=png&amp;auto=webp&amp;s=ad49a07ed80612f57ad6b871ccc6180f85ffe03c    由    /u/Vegetable_Pirate_263  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</guid>
      <pubDate>Thu, 05 Sep 2024 10:04:32 GMT</pubDate>
    </item>
    <item>
      <title>随着损失的增加，调试拟合 Q 评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8lhrg/debug_fitted_qevaluation_with_increasing_loss/</link>
      <description><![CDATA[嗨，专家，我正在使用 FQE 进行离线离策略评估。但是，我发现在训练过程中我的 FQE 损失并没有减少。  我的环境是离散动作空间和连续状态/奖励空间。我尝试了几种修改来调试根本原因：  更改超参数：学习率、FQE 的时期数 更改/规范化奖励函数 确保数据解析正确  上述方法均不起作用。 以前我有一个类似的数据集，我很确定我的训练/评估流程是正确的并且运行良好。 您还会检查/实验什么以确保 FQE 正在学习？    提交人    /u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8lhrg/debug_fitted_qevaluation_with_increasing_loss/</guid>
      <pubDate>Wed, 04 Sep 2024 05:23:31 GMT</pubDate>
    </item>
    <item>
      <title>体育馆渲染问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8fq7g/gymnasium_rendering_question/</link>
      <description><![CDATA[我知道目前 gymnasium box2d 环境有 2 种渲染模式：human 和 rgb_array。有没有办法通过删除所有查看器渲染来加快程序速度？ 编辑：我应该指定这一点，因为使用 rgb_array 运行似乎花费的时间几乎是 human 模式的两倍    提交人    /u/Admirable-Length-465   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8fq7g/gymnasium_rendering_question/</guid>
      <pubDate>Wed, 04 Sep 2024 00:27:06 GMT</pubDate>
    </item>
    <item>
      <title>在矢量输入环境上重置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8fg21/resetting_on_vector_input_environments/</link>
      <description><![CDATA[大家好，有人尝试过在矢量观测环境中使用策略软重置吗？因为我的代理在软重置甚至 0.1 正常噪声后都无法恢复。 我根据 P. D&#39;Oro 2023 尝试过。    提交人    /u/Automatic-Web8429   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8fg21/resetting_on_vector_input_environments/</guid>
      <pubDate>Wed, 04 Sep 2024 00:14:25 GMT</pubDate>
    </item>
    <item>
      <title>数据质量对训练模仿学习模型的影响：使用 Aloha Kit 进行实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</link>
      <description><![CDATA[        由    /u/Trossen_Robotics  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</guid>
      <pubDate>Tue, 03 Sep 2024 20:45:06 GMT</pubDate>
    </item>
    <item>
      <title>在 cudnn 中使用 Pytorch 寻找计算二阶导数的更好方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</link>
      <description><![CDATA[嗨，我试图将安全算法：CPO 应用于我当前的 RL 代码。我使用 pytorch 和 GPU（cuda）运行我的代码。当计算 CPO 中的二阶导数时，cuda 会自动使用 cudnn 进行此类计算。问题是，由于 CuDNN API 的限制，CuDNN RNN 不支持 Double Backing。我也在 stackoverflow 中描述了这个问题，但没有得到答案。而且当我搜索时，这个错误， NotImplementedError：&#39;_cudnn_rnn_backward&#39; 的导数未实现。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs)NotImplementedError: 未实现“_cudnn_rnn_backward”的导数。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs) 在 RNN 和相关模型中的双重向后计算中更常见。如果你们知道解决方案，那将会很有帮助   由    /u/dAmiBouY539  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</guid>
      <pubDate>Tue, 03 Sep 2024 20:10:56 GMT</pubDate>
    </item>
    <item>
      <title>分享我的玩具项目“JAxtar”，用于解谜的纯 jax 和 jittable A* 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</link>
      <description><![CDATA[嗨，我想介绍一下我的玩具项目 JAxtar。 我将其发布到 r/reinforcementlearning 这里以及 r/JAX，因为它与典型的 RL 不同，但它是为使用神经启发式算法的 RL 编写的，例如 DeepCubeA，我计划朝这个方向改进它。 这不是很多人会觉得有用的代码，但我在编写它时用 Jax 完成了大部分杂技，我认为它可能会激励其他人使用 Jax 的人。 我的硕士论文是关于使用 RL 进行 A* 和神经启发式训练以解决 15 个难题，但当我反思它时，最大的头痛是 CPU 和 GPU 之间数据传输的频率高且长度长。几乎一半的执行时间都花在这些通信瓶颈上。这个问题的另一种解决方案是 DeepCubeA 提出的分批 A*，但我觉得这不是一个完整的解决方案。 有一天，我偶然发现了 mctx，这是 google deepmind 用纯 jax 编写的 mcts 库。 我对这种方法很着迷，并多次尝试用 Jax 编写 A*，但都没有成功。问题在于哈希表和优先级队列。 毕业后经过很长一段时间，研究了许多例子，绞尽脑汁，我终于设法编写了一些可用的代码。 我很自豪地说，此代码有几个特殊元素  用于将定义的状态转换为哈希键的 hash_func_builder 用于并行查找和插入的哈希表 可以批处理、推送和弹出的优先级队列 用于拼图的完全 jitted A* 算法。  我希望这个项目可以成为任何喜欢 Jax 和这种类型的 RL（带启发式的 A*）的人的鼓舞人心的例子    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</guid>
      <pubDate>Tue, 03 Sep 2024 14:50:19 GMT</pubDate>
    </item>
    <item>
      <title>作为输出的动作向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</link>
      <description><![CDATA[大家好，我计划在即将开展的项目中利用强化学习。在这种情况下，我的输入（或状态）将由两个矩阵组成：一个表示车辆的位置，另一个表示车辆的速度。输出或动作将是一个具有四个连续元素的向量，每个元素的范围在 0 到 1 之间。例如，一个步骤之后的一个可能动作可能是 [0.51, 0.76, 0.9, 0.12]。 有人能建议哪种强化学习算法最适合这种类型的问题吗？提前感谢您的帮助！    提交人    /u/muttahirulislam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</guid>
      <pubDate>Tue, 03 Sep 2024 13:38:02 GMT</pubDate>
    </item>
    <item>
      <title>随情节变化动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</link>
      <description><![CDATA[当动作空间本身随着情节而变化时，开关策略算法的预期行为是什么。这会导致非平稳性？ 动作空间是连续的。在 Mujoco Ant Cheetah 等中，典型情况是它代表扭矩。假设在一集中动作空间是 [1, -1] 下一集是 [1.2, -0.8] 下一集是 [1.4, -0.6] ... ... 未来的某一集是 [2, 0] .. 动作空间范围的变化由某些函数控制，并且在每个情节开始之前都会随着情节而变化。像 ppo trpo ddpg sac td3 这样的算法的预期行为应该是什么？他们能够处理吗？对于像 mappo maddpg matrpo matd3 等 marl 算法也有类似的问题。 这是由于动态变化而导致的非平稳性吗？是否存在任何无效的操作范围。我们可以将总体范围限制为某个高低值，但范围会随着情节而改变。    提交人    /u/Intrepid_Discount_67   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</guid>
      <pubDate>Tue, 03 Sep 2024 12:24:25 GMT</pubDate>
    </item>
    <item>
      <title>深度 CFR 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</link>
      <description><![CDATA[我从原始 Deep CFR 文章中获取了代码 链接：https://arxiv.org/pdf/1811.00164 我现在正在为我的游戏实现 Deep CFR 算法，当我编写完整算法时，我遇到了很多错误。所以我的问题是，本文末尾的代码是否完全正确，错误是否在我的代码中？如果有人已经为他们自己的任务实现了 Deep CFR 并且可以分享提示/代码，那就太好了 如果有人愿意提供帮助，我可以上传我的实现。它需要最终确定，而且我没有足够的经验来了解如何正确实现它。 这是我的代码： nn.py https://codeshare.io/KWByyK memory.py https://codeshare.io/obp99r deep_cfr.py https://codeshare.io/k0zAAA game_tree.py https://codeshare.io/mP6MMp utils.py https://codeshare.io/ldQppd 对于游戏，我从这个库中获取了 texasholdem：https://github.com/SirRender00/texasholdem。    提交人    /u/silenthnowakeup   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</guid>
      <pubDate>Tue, 03 Sep 2024 01:11:53 GMT</pubDate>
    </item>
    <item>
      <title>“运动物理学”及其对人类模仿学习的启示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</guid>
      <pubDate>Mon, 02 Sep 2024 21:09:17 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习项目征集及创新建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f78wnt/request_for_deep_reinforcement_learning_projects/</link>
      <description><![CDATA[基本上就是标题！ 您能推荐一些带有代码的深度强化学习项目吗？此外，如果您能让我知道是否有可能添加新颖性或更改代码，我将不胜感激。谢谢！    提交人    /u/Sweet_Speed9010   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f78wnt/request_for_deep_reinforcement_learning_projects/</guid>
      <pubDate>Mon, 02 Sep 2024 15:01:36 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Gymnasium 创建自定义环境来训练 CloudSimPlus 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</link>
      <description><![CDATA[基本上，我从基于 Java 的模拟器（CloudSimPlus）中提取指标，并使用 ProcessBuilder 将其发送到 py 脚本，该脚本反过来应该做出响应，决定如何更改云中的基础设施 我想要构建一个环境，其中状态表示为 5 个单独的离散值，并使用 3 种可能的操作训练代理 我已经按照这里的建议尝试了一个基本版本，但现在有点困惑，因为它没有按照我的意图工作    提交人    /u/Automatic_You_1939   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</guid>
      <pubDate>Mon, 02 Sep 2024 08:57:14 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的元学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</link>
      <description><![CDATA[您好，RL 中的大多数元学习似乎已应用于策略空间，而很少应用于 DQN 中的价值空间。我想知道为什么如此注重将策略适应新任务，而不是将价值网络适应新任务。Meta Q Learning 论文似乎是唯一一篇使用 Q 网络进行元学习的论文。这是真的吗？如果是，为什么？    提交人    /u/Sea-Collection-8844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</guid>
      <pubDate>Sun, 01 Sep 2024 07:22:46 GMT</pubDate>
    </item>
    </channel>
</rss>