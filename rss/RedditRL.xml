<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 20 Sep 2024 03:19:51 GMT</lastBuildDate>
    <item>
      <title>LeanRL：一个简单的 PyTorch RL 库，用于快速（>5 倍）训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</link>
      <description><![CDATA[我们很高兴地宣布，我们已经开源了LeanRL，这是一个轻量级的 PyTorch 强化学习库，它使用 torch.compile 和 CUDA 图表提供快速 RL 训练的方法。 通过利用这些工具，与原始 CleanRL 实现相比，我们实现了显着的加速 - 速度提高了 6 倍！ RL 训练的问题 强化学习是出了名的 CPU 受限，因为小型 CPU 操作（例如从模块中检索参数或在 Python 和 C++ 之间转换）的频率很高。幸运的是，PyTorch 强大的编译器可以帮助缓解这些问题。但是，输入编译后的代码也有其自身的成本，例如检查保护以确定是否需要重新编译。对于 RL 中使用的小型网络，这种开销可能会抵消编译的好处。 进入 LeanRL LeanRL 通过提供简单的方法来加速您的训练循环并更好地利用您的 GPU，从而解决了这一挑战。受到 gpt-fast 和 sam-fast 等项目的启发，我们证明了 CUDA 图可以与 torch.compile 结合使用，以实现前所未有的性能提升。我们的结果表明：  使用 PPO（Atari）可提高 6.8 倍速度 使用 SAC 可提高 5.7 倍速度 使用 TD3 可提高 3.4 倍速度 使用 PPO（连续动作）可提高 2.7 倍速度  此外，LeanRL 可以更有效地利用 GPU，让您可以同时训练多个网络而不会牺牲性能。 主要特点  具有最少依赖性的 RL 算法的单文件实现 所有技巧都在 README 中进行了说明 从流行的 CleanRL 分叉   在 https://github.com/pytorch-labs/leanrl 上查看 LeanRL    由    /u/AdCool8270  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</guid>
      <pubDate>Fri, 20 Sep 2024 00:22:12 GMT</pubDate>
    </item>
    <item>
      <title>E[G_(t+1) | S_t=s] = V(S_(t+1)) 总是正确的吗？如何证明？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkuvsy/is_it_always_true_that_eg_t1_s_ts_vs_t1_how_to/</link>
      <description><![CDATA[编辑：在第二个成员中，我的意思是 E[V(S(t+1)) | S_t=s] 而不仅仅是 V(S(t+1)) 也许我淹没在一杯什么中，但是你如何证明这个等式成立？我的目标是证明 E[G_t|S_t=s] = E[R_(t+1) + gamma* V(S_(t+1)) | S_t=s ] 就像 sutton 和 barto 的等式 4.3 中一样，老实说，我对为什么会发生这种情况有一个直观的想法，但我正在寻找一种更正式的方式来展示这个属性    提交人    /u/samas69420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkuvsy/is_it_always_true_that_eg_t1_s_ts_vs_t1_how_to/</guid>
      <pubDate>Thu, 19 Sep 2024 20:55:18 GMT</pubDate>
    </item>
    <item>
      <title>对于这个特定于图形的任务，多智能体还是分层 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkmed4/multiagent_or_hierarchical_rl_for_this/</link>
      <description><![CDATA[我正在研究一个图形应用问题，其中一个RL代理必须将图形编码视为状态，并选择涉及一对节点和它们之间的一种操作的操作。 我正在考虑将此问题分解为具有多个代理的子任务，如下所示：  代理 1：接收图形编码并选择源节点。 代理 2：接收图形编码和所选源节点，然后选择目标节点。 代理 3：接收图形编码和所选源节点和目标节点，然后在它们之间选择一个操作。  我想到两个解决方案：  分层 RL：虽然任务看起来是分层的，但这可能并不完全合适。对于每个主要操作，必须执行所有三个代理（选项），并且需要按固定顺序执行。他们的行动应该是一步行动。我不确定分层 RL 是否最适合，因为问题没有明确的层次结构，而是顺序合作。 多智能体 RL：这可以构建为具有共同团队奖励的合作多智能体设置，其中执行顺序是固定的，每个智能体都会看到图形编码和先前智能体的操作（根据顺序）。  哪种方法——分层 RL 或多智能体 RL——更适合这个问题？是否有与此类问题相符的现有公式或框架？    提交人    /u/fterranova   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkmed4/multiagent_or_hierarchical_rl_for_this/</guid>
      <pubDate>Thu, 19 Sep 2024 14:36:09 GMT</pubDate>
    </item>
    <item>
      <title>聘请 RL 研究人员——构建下一代专家系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</link>
      <description><![CDATA[嗨！我们是 Atman Labs，一家位于伦敦的 AI 初创公司，在软件中模拟人类专家。我们认为，业界需要超越法学硕士 (LLM)，构建能够解决复杂、知识密集型任务的系统，这些任务需要多步推理。我们的研究使用强化学习来探索知识图谱，以形成针对目标的语义基础策略，并代表了一条模拟专家推理的新颖、可靠的途径。 如果您对 RL 充满热情，并希望构建和商业化下一代智能系统，那么您可能非常适合我们的创始团队。让我们聊聊吧 :) https://atmanlabs.ai/team/rl-founding-engineer    提交人    /u/Tricky_Amphibian_836   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</guid>
      <pubDate>Thu, 19 Sep 2024 12:04:36 GMT</pubDate>
    </item>
    <item>
      <title>CleanRL 现已为 PPO + Transformer-XL 提供基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkeqa9/cleanrl_has_now_a_baseline_for_ppo_transformerxl/</link>
      <description><![CDATA[之前，我们的 PPO-Transformer-XL 基线已发布至 Github。 此实现最终已完善为单文件实现以加入 CleanRL！它在 Memory Gym 的新颖无尽环境中重现了原始结果。 文档：https://docs.cleanrl.dev/rl-algorithms/ppo-trxl/ 论文：https://arxiv.org/abs/2309.17207 视频：https://marcometer.github.io/ 我们希望这将进一步改善有效使用变压器的方式并在基于内存的深度强化学习中高效运行。当然，接下来需要解决一些限制：  加快推理速度：与 GRU 和 LSTM 相比，数据采样成本高昂 节省 GPU 内存：缓存 TrXL 的隐藏状态以进行优化成本高昂     提交人    /u/LilHairdy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkeqa9/cleanrl_has_now_a_baseline_for_ppo_transformerxl/</guid>
      <pubDate>Thu, 19 Sep 2024 06:52:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将离线数据的动作空间重新映射到原始动作空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkahjl/how_to_remap_the_action_space_of_offline_data_to/</link>
      <description><![CDATA[嗨！ 我想用一些预定义的原始动作来训练厨房任务。但是，原始动作空间是 9 自由度（即 7 个​​手臂关节和 2 个夹持关节）。我应该如何将原始 9 自由度动作重新映射到原始动作以计算参与者损失？ 提前感谢您的帮助！    提交人    /u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkahjl/how_to_remap_the_action_space_of_offline_data_to/</guid>
      <pubDate>Thu, 19 Sep 2024 02:31:17 GMT</pubDate>
    </item>
    <item>
      <title>我可以将 DPO（直接偏好优化）应用于仅具有（y_win，y_loss）一侧的训练数据吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjwqvl/can_i_apply_dpo_direct_preference_optimization_to/</link>
      <description><![CDATA[我有一堆 (x_i, y_i, win_or_lose) 的标记数据。RLHF 论文的大部分内容都使用成对损失函数，这需要 (x_i, y_i_win) 和 (x_i, y_i_lose)，而我没有。我还能将 DPO 用于单侧训练数据吗？ 将缺失侧的隐式奖励值设置为 0，然后仍然应用反向传播，这样可以吗？    提交人    /u/PuzzleheadedBasis951   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjwqvl/can_i_apply_dpo_direct_preference_optimization_to/</guid>
      <pubDate>Wed, 18 Sep 2024 16:14:35 GMT</pubDate>
    </item>
    <item>
      <title>我目前遇到一个问题。给定一组项目，我需要选择一个子集并将其传递给黑盒，之后我将获得该值。我的目标是最大化该值，项目集包含大约 200 个项目。在这种情况下，sota 模型是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjj8mk/i_am_currently_encountering_an_issue_given_a_set/</link>
      <description><![CDATA[  由    /u/Fast-Ad3508  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjj8mk/i_am_currently_encountering_an_issue_given_a_set/</guid>
      <pubDate>Wed, 18 Sep 2024 03:19:42 GMT</pubDate>
    </item>
    <item>
      <title>QR-DQN 爆炸值域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj6fvy/qrdqn_exploding_value_range/</link>
      <description><![CDATA[我正在研究分布式强化学习，目前正在尝试实现 QR-DQN。 Github 中有一个直观的解释，但对环境的简短解释是代理从 (0,0,0) 开始。向“左”或“右”是随机选择的，向左会导致最左边的 0 被 -1 替换，向右会将最左边的 0 替换为 +1。每个非终止步骤的奖励为 0。一旦代理到达终点，奖励将计算为  s=(-1,-1,-1) =&gt; r=0 s=(-1,-1,1) =&gt; r=1 . . . s=(1,1,1) =&gt; r=7 请注意，QR-DQN 不会采取任何行动，它只是试图预测奖励分布。这意味着在状态 s=(0,0,0) 时，分布应在 0 到 7 之间均匀分布，在状态 s=(1,0,0) 时，分布应在 4 到 7 之间均匀分布，等等。 但是，QR-DQN 输出的分布范围从 -20,000 到 +20,000，并且似乎永远不会收敛。我很确定这是一个引导问题，但我不知道如何解决它。 代码：https://github.com/Wung8/QR-DQN/blob/main/qr_dqn_demo.ipynb    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj6fvy/qrdqn_exploding_value_range/</guid>
      <pubDate>Tue, 17 Sep 2024 18:13:40 GMT</pubDate>
    </item>
    <item>
      <title>关于在情景强化学习设置中使用演员评论家架构的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</link>
      <description><![CDATA[大家好，RL 的朋友们， 我最近遇到了一个问题，我正在将多智能体 PPO 与演员-评论家相结合应用于一个问题，由于问题的性质，我首先实施了它的一个情节版本作为初始实施。 我理解拥有评论家的优势之一是可以使用情节中估计的值来更新演员，从而无需等到情节结束时才能用奖励来更新演员。但是，如果无论如何都在情节设置中，使用评论家而不是实际奖励有什么好处吗？    提交人    /u/Ingenuity39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</guid>
      <pubDate>Tue, 17 Sep 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>用于实现 RL 以优化数学函数的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</link>
      <description><![CDATA[有人可以推荐任何资源作为 RL 实现示例来优化数学函数/测试函数吗？因为我能找到的大多数东西基本上都在 gym 环境中。但我正在寻找一个带有代码的示例，它可以对数学函数进行优化（最好使用 actor critical，但其他方法也可以）。如果有人知道这样的资源，请提出建议。提前谢谢您。    提交人    /u/anikbis17   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</guid>
      <pubDate>Tue, 17 Sep 2024 15:46:46 GMT</pubDate>
    </item>
    <item>
      <title>预订建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</link>
      <description><![CDATA[我需要什么书来进行强化学习？ 我希望书既直观又具有数学性，我能理解艰难的数学，因为我有很强的数学背景。 向我推荐一些有很好解释并且包含很好数学内容的书。    提交人    /u/Evening-Passenger311   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</guid>
      <pubDate>Tue, 17 Sep 2024 14:25:13 GMT</pubDate>
    </item>
    <item>
      <title>如何优化奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</link>
      <description><![CDATA[我一直在用强化学习训练汽车，但一直遇到奖励函数问题。我希望汽车保持较高的恒定速度，并一直使用 speed 和最近的 progress 等参数来奖励它。但是，我注意到，当仅根据速度进行奖励时，汽车有时会加速，但会立即减速，而进度似乎根本没有影响。我还奖励了其他动作，例如 all_wheel_on_track，这很有帮助，因为每次汽车偏离赛道都会受到 5 秒的惩罚。 附注：这是 aws deep racer 比赛，如果您愿意，可以在此处查看参数。    提交人    /u/KatCelest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</guid>
      <pubDate>Tue, 17 Sep 2024 07:41:31 GMT</pubDate>
    </item>
    <item>
      <title>推荐阅读因果强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</link>
      <description><![CDATA[嗨， 我的经济学背景是因果推理（据我所知，我的背景是鲁宾学派，而不是 Pearls 学派），我想了解更多关于因果 RL 的知识。我已经观看了关于因果强化学习的本教程，但我仍然不太明白它在做什么。 有推荐阅读材料吗？这篇论文是一个好的开始吗？ 此外，我目前的理解是“传统”因果推理假设因果关系，而（一些）RL 则从数据中学习它们而不做假设？这是正确的吗？ 谢谢！    提交人    /u/WinnieXi   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</guid>
      <pubDate>Mon, 16 Sep 2024 15:36:16 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>