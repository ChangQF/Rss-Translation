<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 13 Jul 2024 21:14:02 GMT</lastBuildDate>
    <item>
      <title>[R] 理解强化学习中离散表示的不合理有效性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2gqcn/r_understanding_the_unreasonable_effectiveness_of/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2gqcn/r_understanding_the_unreasonable_effectiveness_of/</guid>
      <pubDate>Sat, 13 Jul 2024 18:05:58 GMT</pubDate>
    </item>
    <item>
      <title>弃用函数近似中的折扣奖励（Sutton 10.4）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</link>
      <description><![CDATA[Sutton 指出，在持续问题中使用函数近似时，使用折扣奖励不再有意义。 这对我来说真的没有意义，有人可以详细说明一下吗？    提交人    /u/federicom01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</guid>
      <pubDate>Sat, 13 Jul 2024 16:40:41 GMT</pubDate>
    </item>
    <item>
      <title>用大约 13 分钟解释我 2 年的 RL 研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</link>
      <description><![CDATA[        由    /u/ejmejm1 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</guid>
      <pubDate>Sat, 13 Jul 2024 15:11:20 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 算法中的大批量会起作用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</link>
      <description><![CDATA[通常我看到人们在实施 REINFORCE 算法（使用神经网络）时会这样做： for state, action, reward in episodes: update (batch size is 1)  如果游戏长度为 50 轮，我们也可以将所有状态、动作和奖励连接到批量大小为 50 的张量中并进行更新。我尝试过，并且取得了相当不错的成功，值得注意的是（并且不出所料）它大大加快了训练速度。 所以我在想，是什么会阻止我们进行更多连接。假设我们不是每 50 轮游戏更新一次，而是每 10 轮游戏更新一次。张量的维度足够小，这将显著提高计算速度，并可能导致更好的梯度估计。但是，我们最终进行的更新较少。这是我们在监督学习中看到的标准 batch_size 超参数权衡问题。 为什么没人尝试过？或者，也许我只是不擅长搜索是否有人尝试过。 在尝试之前想问一下，因为模拟一切有时需要几天时间。 在你来找我之前，是的，我知道有更好的算法，我只是喜欢先探索非常非常简单的算法。    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</guid>
      <pubDate>Sat, 13 Jul 2024 13:46:53 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：在 pokerenv 中 >>: 'list' 和 'int' 的操作数类型不受支持</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e23q7r/typeerror_unsupported_operand_types_for_list_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e23q7r/typeerror_unsupported_operand_types_for_list_and/</guid>
      <pubDate>Sat, 13 Jul 2024 06:32:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于连续组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e22p2t/rl_for_continuous_combinatorics/</link>
      <description><![CDATA[您好， 我在工作中遇到一种情况，我正尝试使用机器学习模型来解决连续组合问题。本质上，想象一系列变量 x1-xN，它们可以是任意数字（连续）。我的梯度非常陡峭，很难通过 SGD 等方法导航。我需要找到这些变量的组合，以便优化从这些变量派生的某些属性。有什么建议吗？    提交人    /u/elaraxelara   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e22p2t/rl_for_continuous_combinatorics/</guid>
      <pubDate>Sat, 13 Jul 2024 05:28:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DQN 和 DRL 有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</link>
      <description><![CDATA[我知道 NN 是函数近似器，但要近似某些东西，您必须知道真实值（监督学习）。而且您永远无法在 TD 方法（如 Q-Learning 和一般 RL）中看到真实值。它将样本估计作为客观值与我们已经拥有的东西（更多样本估计）进行比较。为什么它效果这么好？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</guid>
      <pubDate>Fri, 12 Jul 2024 22:57:05 GMT</pubDate>
    </item>
    <item>
      <title>平均奖励与奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1lyk2/mean_reward_vs_reward/</link>
      <description><![CDATA[为什么在每个 episode 的简单奖励上使用平均奖励而不是简单奖励？    提交人    /u/Glum_Flower_8682   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1lyk2/mean_reward_vs_reward/</guid>
      <pubDate>Fri, 12 Jul 2024 16:29:26 GMT</pubDate>
    </item>
    <item>
      <title>这个 Chatgpt 的 NEAT 实现会起作用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1jf8a/will_this_chatgpts_neat_implementation_work/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1jf8a/will_this_chatgpts_neat_implementation_work/</guid>
      <pubDate>Fri, 12 Jul 2024 14:44:41 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫性质被尊重吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1gi7j/is_markov_property_respected/</link>
      <description><![CDATA[我的系统由离散动作和有限状态组成 在状态 1 中执行动作后，动作将不再产生任何效果。我的意思是：&lt;状态 1，任何动作，奖励取决于动作，Same\_Next\_State2&gt;。无论代理尝试什么动作，它都会转到相同的 next_state 并根据动作获得奖励。    提交人    /u/Glum_Flower_8682   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1gi7j/is_markov_property_respected/</guid>
      <pubDate>Fri, 12 Jul 2024 12:31:59 GMT</pubDate>
    </item>
    <item>
      <title>交易中的造型奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1fhdj/shape_reward_in_trading/</link>
      <description><![CDATA[大家好， 我正在交易中实施 PPO 算法，对于具有稀疏奖励的买入、持有、卖出操作，只有在获利或亏损后才会给予奖励。我们如何为这个场景塑造奖励，有人有在交易中塑造奖励的经验吗？比如在持有和等待的场景中，奖励应该是什么？    提交人    /u/laxuu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1fhdj/shape_reward_in_trading/</guid>
      <pubDate>Fri, 12 Jul 2024 11:38:17 GMT</pubDate>
    </item>
    <item>
      <title>人形训练-v4 借助外力进行步行训练。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e18pjf/humanoid_training_v4_walk_training_with_external/</link>
      <description><![CDATA[您好，我正在使用 Stable-Baseline3 训练 mujoco 的人形机器人向前行走。我已经能够证明 SAC 可以很好地实现这一目标。我想证明代理可以承受外力并仍然实现相同的目标。有人可以提供如何使用 mujoco 环境实现这一点的指示吗？     提交人    /u/lulislomelo   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e18pjf/humanoid_training_v4_walk_training_with_external/</guid>
      <pubDate>Fri, 12 Jul 2024 04:27:45 GMT</pubDate>
    </item>
    <item>
      <title>“关于通过弱 LLM 评判强 LLM 的可扩展监督”，Kenton 等人 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e14e7r/on_scalable_oversight_with_weak_llms_judging/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e14e7r/on_scalable_oversight_with_weak_llms_judging/</guid>
      <pubDate>Fri, 12 Jul 2024 00:41:34 GMT</pubDate>
    </item>
    <item>
      <title>实验——招募参与者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e0pxpf/experiment_recruiting_participants/</link>
      <description><![CDATA[嘿！您能帮我完成我的大学调查吗？有点与强化学习有关（计划谬误）。 作为奖励 - 您将有机会赢得 100 英镑的亚马逊代金券，因为我只需要 100 名参与者，这是一个赢得 100 英镑的好机会。 https://uniofbath.questionpro.eu/t/AB3u1qYZB3vuhE 该研究包括一份问卷，其中包括您选择您希望第二天花时间进行的活动和一份 30 个问题的测试，该测试将衡量您的冲动程度。第二天，您将被要求记录您在所选活动上花费的时间，并回答一些后续问题。 如有任何问题 - 请告诉我！    提交人    /u/ImACashBadger   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e0pxpf/experiment_recruiting_participants/</guid>
      <pubDate>Thu, 11 Jul 2024 14:12:53 GMT</pubDate>
    </item>
    <item>
      <title>使用“stable_baselines3”时，如何为“gymnasium”环境重置播种？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e0comw/how_to_seed_gymnasium_environment_resets_when/</link>
      <description><![CDATA[我想为我的体育馆环境播种。从[官方文档][1]中，我这样做的方式是 -  ```py import gymnasium as gym env = gym.make(&quot;LunarLander-v2&quot;, render_mode=&quot;human&quot;) observer, info = env.reset(seed=42)  ``` 但是，stable_baselines3 似乎不需要从用户端进行重置，如下面的程序所示 -  ``` import gymnasium as gym from stable_baselines3 import PPO from stable_baselines3.common.env_util import make_vec_env # Parallel environment vec_env = make_vec_env(&quot;CartPole-v1&quot;, n_envs=4) model = PPO(&quot;MlpPolicy&quot;, vec_env, verbose=1) model.learn(total_timesteps=25000) model.save(&quot;ppo_cartpole&quot;) del model # 删除以演示保存和加载 model = PPO.load(&quot;ppo_cartpole&quot;)  ``` 如何使用 `stable_baselines3` 放置种子？我尝试放置 `np.random.seed(24)`，但没有成功。 [1]: https://gymnasium.farama.org/    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e0comw/how_to_seed_gymnasium_environment_resets_when/</guid>
      <pubDate>Thu, 11 Jul 2024 01:19:00 GMT</pubDate>
    </item>
    </channel>
</rss>