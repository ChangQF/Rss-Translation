<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 08 Jan 2024 01:01:57 GMT</lastBuildDate>
    <item>
      <title>机器人课程项目调查！任何经验都有帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</link>
      <description><![CDATA[大家好， 我正在与卡耐基梅隆大学和宾夕法尼亚大学的一些机器人专业学生合作开展一个课堂项目，以研究疼痛学者和行业专业人士在从事机器人开发时面临的问题。如果您从事或认识从事机器人开发流程任何部分的人，并且有 10 分钟的空闲时间，我们将非常感谢您的意见。我们希望获得广泛经验水平的意见。因此，我们重视刚开始接触机器人技术的人们以及具有多年经验的人们的意见。回复是匿名的，绝不反映绩效，因此我们要求您诚实回答。我们计划在 1 月 14 日之前收集回复（但如果调查之后开放，请随时贡献您的想法！）。  https://forms.gle/Mx247TgeDbEydY426 谢谢，   由   提交/u/awkyu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</guid>
      <pubDate>Sun, 07 Jan 2024 23:56:05 GMT</pubDate>
    </item>
    <item>
      <title>演奏乐器的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</link>
      <description><![CDATA[寻找任何已知的演奏乐器的模拟环境。例如，一个灵巧的特工在弹吉他。   由   提交 /u/Ultra-Neural   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</guid>
      <pubDate>Sun, 07 Jan 2024 21:31:00 GMT</pubDate>
    </item>
    <item>
      <title>这是从模型停止的地方继续训练的正确方法吗？稳定基线3、Pytorch、Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</link>
      <description><![CDATA[嗨， 我正在训练一个模型，昨天我保存并关闭了，因为已经很晚了，我需要睡觉。现在，今天我想从上次停下来的地方继续训练，但谷歌的结果好坏参半，2018 年、19 年、20 年的答案等等。 这是我的代码，如果有人可以确认这是正确的序列，我会很感激。 log_dir = &quot;/path/where/I/want/logs/saved&quot; model_dir =“/path/to/saved/zip/file” env = MyENV() env.reset () model = PPO(&quot;MlpPolicy&quot;, env, verbose = 1,tensorboard_log=log_dir) model .set_parameters(model_path, True) TIMESTEPS = 10000 CONTINUE_BOOKMARK = 35 #最新保存的文件是340000，所以350,000 将是下一个邮政编码... for i in range(CONTINUE_BOOKMARK, 51): model.learn （total_timesteps=TIMESTEPS，reset_num_timesteps=False，tb_log_name=“log_name_here”） model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 我即将运行它，但我担心我可能做得不对，如果它确实有效，那只是巧合。 ​ 编辑： 我最终使用了类似的东西 model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 唯一的事情是，tensorboard 日志看起来没有从之前的日志继续... &lt;!-- SC_ON - -&gt;  由   提交 /u/phantomBlurrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</guid>
      <pubDate>Sun, 07 Jan 2024 19:44:19 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化分析调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/190qhw5/a_survey_analyzing_generalization_in_deep/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.02349 存储库：https ://github.com/EzgiKorkmaz/generalization-reinforcement-learning 摘要：  强化学习研究取得重大成功和注意力利用深度神经网络来解决高维状态或动作空间中的问题。虽然深度强化学习策略目前被部署在从医疗应用到自动驾驶汽车的许多不同领域，但该领域仍然存在一些关于深度强化学习策略的泛化能力的问题。在本文中，我们将概述深度强化学习策略遇到限制其鲁棒性和泛化能力的过度拟合问题的根本原因。此外，我们将形式化和统一不同的解决方案以提高泛化性，并克服状态-动作价值函数的过度拟合。我们相信我们的研究可以为当前深度强化学习的进展提供紧凑、系统的统一分析，并有助于构建具有更高泛化能力的鲁棒深度神经策略。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/190qhw5/a_survey_analyzing_generalization_in_deep/</guid>
      <pubDate>Sun, 07 Jan 2024 11:49:27 GMT</pubDate>
    </item>
    <item>
      <title>如何获得 AI/ML 和强化学习方面的经验来担任研究职位？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/190l2rc/how_to_get_experience_in_aiml_and_reinforcement/</link>
      <description><![CDATA[您好，我是一名计算机科学专业的新生，对 AI/ML 研究非常感兴趣，尤其是强化学习。我想向教授寻求研究机会，但我没有太多经验可以展示。我已经完成了一些在线课程，阅读了教科书等，但除了我完成了一些编码作业作为其中的一部分之外，我没有什么可以展示的。您对我可以做些什么来获得强化学习经验有什么建议吗？我可以向教授展示这些经验，以证明我已经准备好在他们的实验室进行研究？我一直在考虑从头开始实现一些论文和/或做一些涉及机器学习的副项目。这是一个好的起点吗？   由   提交/u/meemaowie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/190l2rc/how_to_get_experience_in_aiml_and_reinforcement/</guid>
      <pubDate>Sun, 07 Jan 2024 05:46:30 GMT</pubDate>
    </item>
    <item>
      <title>没有的极限是什么？ PPO 中的观察结果是否有助于良好且快速的训练？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zym15/whats_the_limit_of_no_of_observations_in_ppo_for/</link>
      <description><![CDATA[我是 PPO 的新手，我有一个疑问，比如什么是一个好的数字（观察数量），它将通过 PPO 提供良好的训练结果算法？就像更多的观察意味着更多的信息和快速学习或者什么......   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zym15/whats_the_limit_of_no_of_observations_in_ppo_for/</guid>
      <pubDate>Sat, 06 Jan 2024 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>增强静态数据环境中 DRL 代理的泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zxyjh/enhancing_generalization_in_drl_agents_in_static/</link>
      <description><![CDATA[上下文： 我正在类似市场的环境中使用深度强化学习 (DRL) 代理，其行为不影响环境。环境使用特定日期之前的历史数据进行训练，并保留该日期之后的数据用于评估。训练阶段中的每个时间步“t”为代理提供数据集中的相应行。 问题：当训练超出“T”时间步时，代理开始看到重复相同的观察结果，这引起了人们对过度拟合及其泛化能力的担忧。尽管重播缓冲区通过随机采样观察结果来更新模型权重有所帮助，但我担心在长期训练中，代理可能会学习训练数据集中的特定转换，而不是开发通用的解决方案。  问题：如何增强 DRL 代理在这种静态、数据驱动的训练环境中的泛化能力？是否有特定的训练策略或调整可以鼓励代理制定可推广且有效的策略，而不仅仅是记住训练数据集？   由   提交 /u/Disastrous_Effort725   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zxyjh/enhancing_generalization_in_drl_agents_in_static/</guid>
      <pubDate>Sat, 06 Jan 2024 11:21:38 GMT</pubDate>
    </item>
    <item>
      <title>运行非常简单的代码</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zxam9/getting_very_simple_code_to_run/</link>
      <description><![CDATA[我正在尝试稳定的 baslines3 中最简单的代码，但我无法让它运行。它给了我： ​ 文件“/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3 /common/vec_env/dummy_vec_env.py”，第 77 行，重置中 obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self ._seeds[env_idx], **maybe_options) TypeError: CoinFlipEnv.reset() 得到意外的关键字参数“seed”  ​ 这是代码： ​ 将gymnasium导入为gym &lt; p&gt;将 numpy 导入为 np 从 stable_baselines3 导入 PPO 从 stable_baselines3.common.vec_env 导入 DummyVecEnv&lt; /code&gt; ​ class CoinFlipEnv(gym.Env): def __init__(self ，heads_probability=0.8）： super(CoinFlipEnv, self).__init__() self.action_space =gym. space.Discrete(2) # 0 为头部，1 为尾部 self.observation_space =gym.spaces.Discrete(2) # 0 为头部，1 为尾部代码&gt; self.heads_probability = Heads_probability self.flip_result = None ​ ; def reset(self): # 重置环境 self .flip_result = None 返回 self._get_observation() ​ def step(self, action): # 执行动作（0 为正面，1 为反面） self. Flip_result = int(np.random.rand() &lt; self.heads_probability) ​ # 计算奖励（预测正确为 1，预测错误为 -1） reward = 1 if self.flip_result == action else -1 ​ # 返回观察值、奖励、完成和信息 return self._get_observation(),reward, True, {} ​&lt; /p&gt; def _get_observation(self): # 返回当前抛硬币结果  return self.flip_result ​ # 创建正面概率为0.8的环境 &lt; code&gt;env = DummyVecEnv([lambda: CoinFlipEnv(heads_probability=0.8)]) ​ # 创建 PPO 模型 model = PPO(&quot;MlpPolicy&quot;, env, verbose=1) ​ # 训练模型 model.learn(total_timesteps=10000) ​ &lt; code&gt;# 保存模型 model.save(&quot; ;coin_flip_model&quot;) ​ # 评估模型 obs = env .reset() for _ in range(10): action, _states = model.predict(obs)  obs、奖励、完成、info = env.step(action) print(f&quot;Action: {action }，观察：{obs}，奖励：{rewards}”) ​ ​ 什么我做错了吗？ ​ ​   由   提交 /u/wiggyhat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zxam9/getting_very_simple_code_to_run/</guid>
      <pubDate>Sat, 06 Jan 2024 10:37:03 GMT</pubDate>
    </item>
    <item>
      <title>元强化学习任务的程序生成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zs3l1/procedural_generation_of_metareinforcement/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2302.05583 OpenReview：https:// /openreview.net/forum?id=16fkkkCeOC 代码：https://github.com/ThomasMiconi/Meta-Task-Generator 摘要：  开放性能够生成无限多样、具有挑战性的环境，将受益匪浅。一种特别有趣的挑战类型是元学习（“学会学习”），这是智能行为的标志。然而，文献中元学习环境的数量是有限的。在这里，我们描述了具有任意刺激的简单元强化学习（meta-RL）任务的参数化空间。参数化使我们能够随机生成任意数量的新颖的简单元学习任务。参数化的表达能力足以包含许多众所周知的元强化学习任务，例如强盗问题、Harlow 任务、T 迷宫、Daw 两步任务等。简单的扩展使其能够捕获基于二维拓扑空间的任务，例如完整迷宫或查找域。我们描述了许多随机生成的具有不同复杂度的元强化学习域，并讨论了随机生成产生的潜在问题。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zs3l1/procedural_generation_of_metareinforcement/</guid>
      <pubDate>Sat, 06 Jan 2024 05:05:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 新手：在剧集终止后可以继续观察吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zotah/newbie_to_rl_is_it_okay_to_keep_an_observation/</link>
      <description><![CDATA[使用 pytorch、开放式 AI 健身房、pygame。 我正在尝试训练代理玩贪吃蛇游戏，灵感来自于YouTube 上 Sentdex 的教程系列。我试图让代理停止做的事情之一是反复吃掉自己并终止。所以我想添加一个计数器，这样如果一个情节由于与自身碰撞（蛇吃自己）而终止，那么它会给计数器添加+1。如果情节不是由于与自身发生碰撞而终止，则该计数器会重置。所以效果是，如果智能体由于碰撞 w 本身而终止，那么它会因与自身碰撞而收到 -100。如果它再次执行，那么对于相同的终止条件，它将是 -200。如果智能体因不同原因幸存并终止，则计数器重置，并且下次智能体与自身碰撞时，奖励再次为-100。此外，我还给代理一个“与自我碰撞”的结果。标记为观察，如果由于其他原因终止，则为 0；如果由于与自身碰撞而终止，则为 1。 我的问题是，这是否允许？我使用的变量是在 env INIT 上初始化的，而不是在 env RESET 中初始化的。是否允许使用这样的变量作为观察？ 新来的，如果我的术语也混淆了，我很抱歉，我的理解是每一帧都是一个步骤，如果代理喜欢撞墙或吃东西本身或类似的东西，然后*情节*终止，并调用重置。所以我目前正在做的就是从技术上讲对各个情节进行观察，对吧？这是允许的吗？ 另外我也不知道这个问题可接受的风格是什么，lmk。   由   提交 /u/phantomBlurrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zotah/newbie_to_rl_is_it_okay_to_keep_an_observation/</guid>
      <pubDate>Sat, 06 Jan 2024 02:18:20 GMT</pubDate>
    </item>
    <item>
      <title>“动物的随机搜索可能有助于它们狩猎：觅食和掠食动物的神经系统可能会促使它们沿着一种称为 Lévy 行走的特殊随机路径移动，以便在没有线索的情况下有效地寻找食物”（Lévy 航班）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18znfh9/random_search_wired_into_animals_may_help_them/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18znfh9/random_search_wired_into_animals_may_help_them/</guid>
      <pubDate>Sat, 06 Jan 2024 01:14:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么在强化学习中需要包含随机元素 epsilon？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zmrfl/why_do_you_need_to_include_a_random_element/</link>
      <description><![CDATA[假设您正在尝试自动化吃豆人游戏。您拥有所有 pacman 状态，并获取每个可能操作的 q 值。为什么要有随机性的因素呢？随机性如何发挥作用来获取 q 值？   由   提交 /u/Throwawaybutlove   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zmrfl/why_do_you_need_to_include_a_random_element/</guid>
      <pubDate>Sat, 06 Jan 2024 00:43:32 GMT</pubDate>
    </item>
    <item>
      <title>支持 3D 和物理的机器人强化学习的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zgqid/best_library_for_reinforcement_learning_in/</link>
      <description><![CDATA[我目前正在使用 Unity ML 代理，它相当直观且运行良好。我确实发现它有限制，尤其是最近的 Unity 戏剧，我不确定它是否可以免费使用或长期支持。 我想切换到开源的东西，这会给我带来好处作为程序员，我有更多的控制权 我使用 OpenCV 制作了一个自定义的 2D 健身房，用于稳定基线，并且效果很好。  我需要将 3D 用于机器人技术，并最终与真实系统交互并使用传感器进行反馈。  我对 PyChrono 感到很兴奋，它似乎拥有所有正确的功能，但我就是无法让它工作。  查看教程，他们只有 1 个关于强化学习的教程。 https://api.projectchrono.org/tutorial_pychrono_demo_tensorflow.html 当尝试遵循它时，它要求安装tensorflow-gpu=1.14，它非常旧，并且不能与其他安装指令使用的Python=3.9正确安装 而且他们的主库大约4个月前停止获取更新，不确定是否停止开发 总体而言，PyChrono 对 ML 的支持很差，使用起来会很麻烦。 有哪些更好的替代方案可以继续获得支持？&lt; /p&gt; OpenAI Gym 是否配备 3D/物理/渲染引擎？这会得到多年的支持吗？ 谢谢 编辑。我找到了 PyBullet。似乎正是我正在寻找的。对此有何建议？   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zgqid/best_library_for_reinforcement_learning_in/</guid>
      <pubDate>Fri, 05 Jan 2024 20:30:35 GMT</pubDate>
    </item>
    <item>
      <title>[问题]强化学习算法资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zd6dc/question_resource_for_reinforcemnt_learning/</link>
      <description><![CDATA[是否有任何资源可以解释最近所有重要的深度强化学习算法？我看过博客和文章。我还找到了以下论文： 2209.14940.pdf (arxiv.org) 谢谢   由   提交 /u/Top_Badger9050   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zd6dc/question_resource_for_reinforcemnt_learning/</guid>
      <pubDate>Fri, 05 Jan 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法的分类</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18z9kt7/classification_of_rl_algorithms/</link>
      <description><![CDATA[大家好， 我想对 RL 算法进行分类。据我了解，分类有两个维度。第一个维度基于代理在学习过程中如何收集和利用数据：在策略学习和离策略学习。第二个维度基于一般策略：基于价值的方法、基于策略的方法、基于行动者批评家的方法。 ​ 现在我想根据这两个维度对以下算法进行分类： - Sarsa：在策略学习，基于价值的方法 - REINFORCE：在策略学习，基于策略的方法 - A2C：On-policy 学习，基于 actor-critic 的方法 - PPO：On-policy 学习，基于 actor-critic 的方法和基于策略的方法 - Q-Learning：Off-policy 学习， value-based method - DQN：Off-policylearning，基于value的方法 - TD3：Off-policylearning，基于actor-critic的方法 - DDPG：Off-policylearning，基于演员-评论家的方法 ​ 你对我的分类有何看法？这是对的吗？有时算法可能分为两类，例如 PPO，它是一种基于 actor-critic 的方法，也是一种基于策略的方法。    由   提交 /u/PBerit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18z9kt7/classification_of_rl_algorithms/</guid>
      <pubDate>Fri, 05 Jan 2024 15:32:51 GMT</pubDate>
    </item>
    </channel>
</rss>