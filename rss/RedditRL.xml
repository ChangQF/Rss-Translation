<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 02 Dec 2024 06:27:36 GMT</lastBuildDate>
    <item>
      <title>我从 DQN 获得的操作问题有时超出范围。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4m96d/problem_with_action_that_i_get_from_dqn_is/</link>
      <description><![CDATA[大家好， 我是强化学习的初学者。目前正在研究 DQN，并遇到了 DQN 预测动作的问题。我的问题如下：  Atari 游戏有 16 个一般动作空间，但每个游戏都有来自这个一般空间的自己的动作子集。 假设我正在探索 5 个不同的游戏，每个游戏都有不同数量的有效动作。 在进化过程中，一个游戏只有 3 个有效动作，但您的 DQN 返回的是这个有效空间之外的动作。  一种解决方案是用动作 0（NOOP）替换现在有效的动作，这意味着什么也不做。 还有其他方法可以有效地处理这种情况吗？ 提前谢谢您    提交人    /u/Grasmit_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4m96d/problem_with_action_that_i_get_from_dqn_is/</guid>
      <pubDate>Mon, 02 Dec 2024 04:00:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DreamerV3 的炒作程度不如 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</link>
      <description><![CDATA[据我所知，PPO 通常是强化学习任务的首选算法。为什么不改用 DreamerV3？它似乎更稳定，并且需要更少的超参数调整。    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</guid>
      <pubDate>Mon, 02 Dec 2024 01:22:39 GMT</pubDate>
    </item>
    <item>
      <title>图注意力对于旋转变换是否不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4h1lj/do_graph_attention_is_invariant_to_rotational/</link>
      <description><![CDATA[我真的不确定我是否错了，但就等变和不变图而言，神经网络可以有效地处理这个问题，但旋转等变除外。 当我听到这个词时，我仍然认为 GAT 是否应该添加更多步骤以成为旋转变换不变，使输出 3D 特征不变天气位置不同或不在 3D 图上的 PCQM4Mv2 回归预测任务中。  我的管道： 节点特征 = 从数据集中提取的特征 边缘特征 = 3D 欧几里得     提交人    /u/Ill_Strawberry8459   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4h1lj/do_graph_attention_is_invariant_to_rotational/</guid>
      <pubDate>Sun, 01 Dec 2024 23:39:17 GMT</pubDate>
    </item>
    <item>
      <title>寻找合作者 - 强化学习运筹学问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4fwym/looking_for_collaborators_rl_for_operations/</link>
      <description><![CDATA[大家好 :) 我目前正在攻读硕士学位，一直专注于强化学习。我现在已经在这个领域完成了几个项目，并且我目前还在与一些合作者合作撰写一篇关于在不确定的随机环境中的强化学习的出版物，这些环境可以建模为图形。我在做本科论文时注意到了运筹学领域，我将一个随机组合问题构建为强化学习问题，并使用图神经网络解决了它。在那段时间以及从那时起，我阅读了一些最近的出版物，这些出版物试图用现代强化学习方法解决传统的 OR 问题（例如 https://arxiv.org/abs/2312.15658）。 我认为这总体上仍未得到充分探索，但同时也非常有趣。更现代的神经网络架构（例如 GNN）似乎非常适合与 RL 结合解决许多 OR 问题。因此，我也想专注于图形机器学习方法（例如 GNN），但我也对任何其他建模方法持开放态度。此外，还有一个 OR gym 存储库（https://github.com/hubbs5/or-gym），我想探索并用它来做一些新方法的实验。 因此，我正在寻找一些愿意加入我并共同努力用更现代的基于 RL 的方法解决其中一些问题的人。我还没有想过每周要花多少时间在这些项目上，因为从逻辑上讲，我还有很多事情要做（大学、工作、出版）。因此，如果有兴趣，我们可以联系并找到适合我们所有人的良好设置:)  我个人在设计、构建和流水线大规模神经网络方面拥有丰富的经验，并且非常乐意与来自不同背景的人合作。 喜欢收到您的来信！    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4fwym/looking_for_collaborators_rl_for_operations/</guid>
      <pubDate>Sun, 01 Dec 2024 22:48:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的顺序动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</link>
      <description><![CDATA[我有一个代理，其操作必须遵循顺序：A、B、C。但是，环境不稳定或不可预测。代理根据当前情况采取行动，完成其序列（A、B、C）。完成序列后，代理会根据工作完成情况获得一些奖励。它等待环境并分析下一个情况，然后再决定并执行下一组操作。我们如何在这个场景中使用 RL？我们如何训练模型以具有适当的意识来采取行动。每个动作对于获得良好的动作都同样重要。  总之，环境是不可预测的，但我们必须找到一些隐藏的模式来采取这个动作序列。  提前谢谢您！    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</guid>
      <pubDate>Sun, 01 Dec 2024 19:47:34 GMT</pubDate>
    </item>
    <item>
      <title>“通过语言游戏实现无边界的苏格拉底式学习”，Schaul 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h48l24/boundless_socratic_learning_with_language_games/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h48l24/boundless_socratic_learning_with_language_games/</guid>
      <pubDate>Sun, 01 Dec 2024 17:36:34 GMT</pubDate>
    </item>
    <item>
      <title>在离策略 PPO 中训练 Ant 时超参数是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h45ovi/what_are_the_hyperparameters_when_training_ant_in/</link>
      <description><![CDATA[我是强化学习的新手。使用 torchrl 作为库，我已按照以下教程创建了 PPO 代码，并确认 InvertedDoublePendulum 可以进行离线策略训练。 接下来我想尝试使用 Ant-v4 进行学习，因此我将环境名称更改为 Ant-v4 并开始学习。它似乎学得不太好，所以我将 `frame_skip` 设置为 5，`frames_per_batch` 设置为 `50_000 // frame_skip`，`total_frames` 设置为 `60_000_000 // frame_skip`，`sub_batch_size` 设置为 2500 来增加训练量（其他超参数与上一个教程中的相同）。 然而，Ant 的奖励来来去去，水平很低，并没有达到预期效果，如下面的视频所示。鉴于增加学习量不起作用，我认为这是超参数设置的问题，但我没有从谷歌获得任何好的见解。 我的代码中缺少什么？ https://reddit.com/link/1h45ovi/video/ewie7pmk994e1/player    提交人    /u/Novel-Resolve-1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h45ovi/what_are_the_hyperparameters_when_training_ant_in/</guid>
      <pubDate>Sun, 01 Dec 2024 15:28:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Q_Learning 算法不能正常学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/</link>
      <description><![CDATA[嗨，我目前正在编写一个 AI，该 AI 应该使用 Q-Learning 学习井字游戏。我的问题是，该模型在开始时学习了一点，但随后变得越来越糟，并没有变得更好。我正在使用  old_qvalue + self.alpha * (reward + self.gamma * max_qvalue_nextstate - old_qvalue) 更新 QValues，其中 alpha 为 0.3，gamma 为 0.9。我还使用 Epsilon Greedy 策略和衰减的 Epsilon，从 0.9 开始，每回合减少 0.0005，在 0.1 时停止减少。对手是一个 Minimax 算法。我没有发现代码中的任何缺陷，Chat GPT 也没有，我想知道我做错了什么。如果有人有任何提示，我将不胜感激。不幸的是，代码是德文的，我现在没有设置 Github 帐户。    提交人    /u/_waterstar_   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/</guid>
      <pubDate>Sat, 30 Nov 2024 15:19:12 GMT</pubDate>
    </item>
    <item>
      <title>SAC（Soft Actor Critc）无法解决某些任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h3bey8/sac_soft_actor_critc_cannot_solve_some_tasks/</link>
      <description><![CDATA[我编写了一个软演员评论家算法，我想稍后将其用于 carla 模拟器的自动驾驶。我的代码管理器可以解决简单的任务，但是当我在 carla 上尝试它时，即使我的奖励基于硕士论文，我也会得到糟糕的表现。硕士论文获得了更好的表现。如果有人可以检查我的代码是否存在数学或编程错误，那就太好了。你可以在 github 上找到我的代码：https://github.com/b-gtr/Soft-Actor-Critic    提交人    /u/Fair_Device_4961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h3bey8/sac_soft_actor_critc_cannot_solve_some_tasks/</guid>
      <pubDate>Sat, 30 Nov 2024 12:19:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我训练 PPO 时，标准差会增加？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h39upr/why_is_std_increasing_when_i_train_ppo/</link>
      <description><![CDATA[我正在演员-评论家设置中，对具有连续动作（高斯分布）的简单任务测试 PPO。演员网络正在快速学习平均值的最优值，但标准差不断增加（最优解是确定性的，标准差越高，回报越差）。发生这种情况的原因可能是什么？我没有使用任何奖励进行探索。标准差与状态无关。    提交人    /u/Ok_Amoeba_9527   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h39upr/why_is_std_increasing_when_i_train_ppo/</guid>
      <pubDate>Sat, 30 Nov 2024 10:29:01 GMT</pubDate>
    </item>
    <item>
      <title>我无法让这个 dqn 在网格世界中收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h374em/i_cannot_get_this_dqn_to_converge_on_grid_world/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h374em/i_cannot_get_this_dqn_to_converge_on_grid_world/</guid>
      <pubDate>Sat, 30 Nov 2024 07:09:35 GMT</pubDate>
    </item>
    <item>
      <title>“机器人学习方式的革命：未来一代的机器人不会被编程来完成特定任务。相反，它们将使用人工智能进行自我教育”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2wveu/a_revolution_in_how_robots_learn_a_future/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2wveu/a_revolution_in_how_robots_learn_a_future/</guid>
      <pubDate>Fri, 29 Nov 2024 21:53:05 GMT</pubDate>
    </item>
    <item>
      <title>确定性策略的预期收益公式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ud32/expected_return_formula_of_deterministic_policy/</link>
      <description><![CDATA[      我有一个关于确定性策略的预期回报如何写出的问题。我发现在某些情况下使用 Q 函数，如表达式 5 所示。但是，我不完全理解与随机策略相反地获得它的步骤。获得表达式 5 的步骤或理由是什么？ https://preview.redd.it/q6ykgkjzbw3e1.png?width=711&amp;format=png&amp;auto=webp&amp;s=b4ad5f9bbd75430a83d6b240395f52431fed3486    提交人    /u/Street-Vegetable-117   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ud32/expected_return_formula_of_deterministic_policy/</guid>
      <pubDate>Fri, 29 Nov 2024 19:58:19 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 动作模仿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2t726/mujoco_motion_imitation/</link>
      <description><![CDATA[编写一个程序，获取 bvh 数据并尝试用 mujoco 人形机器人模仿它，这有多可行。我认为代理必须在大量数据上进行训练，我已经将 CMU 数据集确定为常用数据集。有人能指出实现这一点的项目吗，或者描述它将如何执行？谢谢。    提交人    /u/snotrio   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2t726/mujoco_motion_imitation/</guid>
      <pubDate>Fri, 29 Nov 2024 19:06:21 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 SAC 方法是否过度拟合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</link>
      <description><![CDATA[      https://preview.redd.it/r1tizdsxbt3e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=0ba0880704571a2868c12a02d3780bb3244d34aa 我是强化学习的初学者，正在使用软演员-评论家（SAC）方法对电动汽车的智能充电进行优化。目标是优化多个电动汽车在离散时间段内的充电计划，以最大限度地降低成本，同时满足电池和电网约束。我已经实现了一个带有优先采样的重放缓冲区，并添加了优先级衰减和动态采样等技术来增强训练稳定性并解决潜在的过度拟合问题。但是，我不确定是否发生了过度拟合，以及如何根据训练和评估奖励之间的差距确定合适的停止标准。我希望得到有关改进模型学习和确保更好的泛化的指导。    提交人    /u/Ok_Efficiency_1318   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</guid>
      <pubDate>Fri, 29 Nov 2024 10:02:52 GMT</pubDate>
    </item>
    </channel>
</rss>