<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 15 May 2024 06:19:40 GMT</lastBuildDate>
    <item>
      <title>Q 和价值函数有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csdwxu/how_does_q_and_value_function_differ/</link>
      <description><![CDATA[      https://preview.redd.it/vq1vjyg27j0d1.png?width=1641&amp;format=png&amp;auto=webp&amp;s=c793e6352 ec192fed1f8b0351ec03f57c74c468f 我知道价值函数会在不考虑行动的情况下给出预期的奖励总和，但我真的不明白它是如何工作的  &amp;# 32；由   提交/u/Ordinary_Big_8726   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csdwxu/how_does_q_and_value_function_differ/</guid>
      <pubDate>Wed, 15 May 2024 06:07:17 GMT</pubDate>
    </item>
    <item>
      <title>机器人 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csdj5k/marl_for_robots/</link>
      <description><![CDATA[我正在寻找模拟移动机器人的竞争性 MARL。有相同的 3D 模拟器吗？   由   提交 /u/TopSimilar6673   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csdj5k/marl_for_robots/</guid>
      <pubDate>Wed, 15 May 2024 05:41:26 GMT</pubDate>
    </item>
    <item>
      <title>零样本强化学习 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs85oi/zero_shot_reinforcement_learning_r/</link>
      <description><![CDATA[有人熟悉零样本强化学习的概念吗？Ahmed Touati 最近进行的一项名为“零样本强化吗？”的调查学习存在吗？今年推出   由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs85oi/zero_shot_reinforcement_learning_r/</guid>
      <pubDate>Wed, 15 May 2024 00:48:36 GMT</pubDate>
    </item>
    <item>
      <title>零样本强化学习 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs84ou/zero_shot_reinforcement_learning_r/</link>
      <description><![CDATA[ 由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs84ou/zero_shot_reinforcement_learning_r/</guid>
      <pubDate>Wed, 15 May 2024 00:47:11 GMT</pubDate>
    </item>
    <item>
      <title>“鲁棒智能体学习因果世界模型”，Richens & Everitt 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs5xai/robust_agents_learn_causal_world_models_richens/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs5xai/robust_agents_learn_causal_world_models_richens/</guid>
      <pubDate>Tue, 14 May 2024 23:03:34 GMT</pubDate>
    </item>
    <item>
      <title>强化学习来识别最大化公式的特征组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crtcl0/reinforcement_learning_to_identify_combinations/</link>
      <description><![CDATA[我的特征空间非常大，我想知道如何使用强化学习来优化公式。我对机器学习这个领域非常陌生，因此感谢所有建议和帮助。谢谢   由   提交/u/Naive_Yoghurt4959   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crtcl0/reinforcement_learning_to_identify_combinations/</guid>
      <pubDate>Tue, 14 May 2024 14:20:16 GMT</pubDate>
    </item>
    <item>
      <title>异构GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crfg0g/heterogeneous_gnn/</link>
      <description><![CDATA[社区您好， 我目前正在将异构 GNN 应用于涉及作业和机器且作业和机器之间有连接的场景。然而，我观察到，即使工作的特征值不同，工作嵌入也包含所有工作的相同张量。聊天 GPT 告诉我，问题是因为作业节点连接到同一台机器。但特征值不同，所以我不知道如何解决这个问题，以便模型可以学习为免费机器选择作业。 你能帮我吗？    由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crfg0g/heterogeneous_gnn/</guid>
      <pubDate>Tue, 14 May 2024 00:49:28 GMT</pubDate>
    </item>
    <item>
      <title>一个时间步内执行多个操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crd71b/multiple_actions_in_one_timestep/</link>
      <description><![CDATA[我正在处理一个问题，其中代理操作是选择打开或关闭多个二进制开关（您可以想象输出是一个二进制数像 01011...）。我见过的大多数文献都涉及每个时间步一个动作，是否有一种简单的方法来实现我所缺少的这个动作空间？将每个可能的组合作为神经网络中的终端节点会很快失控，为 N 个开关提供 2N 个输出节点。 我目前的想法是使用像 A2C 这样的值方法，演员有 N 个输出节点，每个节点对应于打开开关的采样概率，但是，我不确定这会如何改变损失函数。 是否有一个简单的实现我失踪了？任何指示将不胜感激。   由   提交/u/Chewden_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crd71b/multiple_actions_in_one_timestep/</guid>
      <pubDate>Mon, 13 May 2024 23:02:18 GMT</pubDate>
    </item>
    <item>
      <title>CleanRL PPO 不学习简单的双积分器环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cr1jze/cleanrl_ppo_not_learning_a_simple_double/</link>
      <description><![CDATA[我有一个代表双积分器的自定义环境。一开始将环境位置和速度都设置为0，然后选择一个目标值，目标是尽快减小位置与目标之间的差异。代理观察错误和速度。 我尝试使用 CleanRL 的 PPO 实现，但算法似乎无法学习如何解决环境问题，每集的平均回报随机从 -1k 跳到更大价值观。对我来说，这看起来是一个相当简单的环境，但我不知道为什么它不起作用，有人有任何解释吗？ class DoubleIntegrator(gym.Env): def __init__(self , render_mode=None): super(DoubleIntegrator, self).__init__() self.pos = 0 self.vel = 0 self.target = 0 self.curr_step = 0 self.max_steps = 300 self.terminate = False self.truncated =假 self.action_space =gym.spaces.Box(low=-1, high=1, shape=(1,)) self.observation_space =gym.spaces.Box(low=-5, high=5, shape=(2 ,)) def 步骤(self, 动作): 奖励 = -10 * (self.pos - self.target) vel = self.vel + 0.1 * 动作 pos = self.pos + 0.1 * self.vel self.vel = vel self.pos = pos self.curr_step += 1 如果 self.curr_step &gt; self.max_steps: self.terminate = True self.truncated = True 返回 self._get_obs(),reward,self.termerated,self.truncated,self._get_info() def reset(self,seed=None,options=None): self.pos = 0 self.vel = 0 self.target = np.random.uniform() * 10 - 5 self.curr_step = 0 self.terminate = False self.truncated = False 返回 self._get_obs(), self._get_info () def _get_obs(self): return np.array([self.pos - self.target, self.vel], dtype=np.float32) def _get_info(self): return {&#39;target&#39;: self.target, &#39; pos&#39;: self.pos}    由   提交/u/sauro97  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cr1jze/cleanrl_ppo_not_learning_a_simple_double/</guid>
      <pubDate>Mon, 13 May 2024 15:05:17 GMT</pubDate>
    </item>
    <item>
      <title>QL 如何解决而 DQN 无法解决？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqvb9d/how_ql_solves_but_dqn_cannot/</link>
      <description><![CDATA[我有一个可以通过 QL 成功解决的自定义环境，但无法使用 DQN 解决相同的环境（具有离散状态 - 离散操作）。  QL 可以解决问题，但 DQN 根本无法解决问题，这可能是什么原因？ DQN 是 QL 的神经网络改编版本，不是可以处理连续空间吗？当我尝试在相同环境下使用连续状态空间时，DQN 再次陷入困境。  这怎么可能？ 我的操作是 0 和 1，而代理始终只选择 1。这就像坚持同一个动作，这是完全糟糕的。   由   提交/u/OpenToAdvices96   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqvb9d/how_ql_solves_but_dqn_cannot/</guid>
      <pubDate>Mon, 13 May 2024 09:43:38 GMT</pubDate>
    </item>
    <item>
      <title>确定国际象棋代理的 DQN 架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqm9kd/determining_dqn_architecture_for_a_chess_agent/</link>
      <description><![CDATA[嗨，我正在训练 RL 代理下棋 - 我正在使用 open_spiel 库，所以它更具连接性预制棋子（即国际象棋环境和 RL 模型已创建）。但是，我想知道如何设置模型超参数，特别是关于隐藏层的数量和每个隐藏层的节点数量。我应该如何解决这个问题？ TIA   由   提交/u/shadowknife392   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqm9kd/determining_dqn_architecture_for_a_chess_agent/</guid>
      <pubDate>Mon, 13 May 2024 00:26:06 GMT</pubDate>
    </item>
    <item>
      <title>Stockfish 和 Lc0，在不同的部署次数下进行测试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqemmm/stockfish_and_lc0_tested_at_different_number_of/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqemmm/stockfish_and_lc0_tested_at_different_number_of/</guid>
      <pubDate>Sun, 12 May 2024 18:36:56 GMT</pubDate>
    </item>
    <item>
      <title>“SOPHON: Non-Fine-Tunable Learning to Retrain Task Transferability For Pre-trained Models”，Deng et al 2024（MAML 在微调时会导致目标任务的灾难性遗忘）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</guid>
      <pubDate>Sun, 12 May 2024 18:05:11 GMT</pubDate>
    </item>
    <item>
      <title>尝试寻找有关 q 学习的学习率和伽玛设置的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cq7dkk/trying_to_find_papers_on_learningrate_and_gamma/</link>
      <description><![CDATA[大家好。 我正在写关于学校的关于 q-learning 的期末论文。简而言之，我的项目基于 Netlogo 中的 99x99 环境，该环境具有包含七种类型地面类型（人行道、草地等）的网格。每种地面类型都有不同的奖励。该特工被设置为跨越 16 个不同的地点，并在连续 10 集稳定时收敛。我对 q-learning 参数的设置是learning-rate = 0,9 和 gamma = 1.0。 qlearning 收敛在 6500-8000 集左右。情节的定义是到达目标位置或击中建筑物/障碍物并开始新的情节。当代理收敛并找到最佳路线时，它会更新我尝试过这些值（0-30）的该路径的奖励。因此，当下一个代理启动时，一些补丁已从上一个代理更新。我对 100 个代理运行此程序以找到最佳路径。当所有 100 个智能体都找到最佳路径时，我会为路径着色，并与现实生活中的环境足迹观察结果进行比较来评估路径。环境基于真实位置，项目基于收集这些足迹值的先前工作。 如果我没记错的话，当我和老师交谈时，这些高参数设置的原因是因为算法搜索的空间很大。 但是我需要一个关于为什么我选择这些设置的来源，你们有任何论文或可以推荐这些设置的东西吗？ 感谢您的帮助   由   提交 /u/TheAmitySloth   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cq7dkk/trying_to_find_papers_on_learningrate_and_gamma/</guid>
      <pubDate>Sun, 12 May 2024 13:02:14 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗方法解决赌徒问题没有给出最优解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpwtxe/monte_carlo_method_for_gamblers_problem_not/</link>
      <description><![CDATA[     &lt; /td&gt; 大家好，我最近开始学习强化学习，并尝试用赌徒问题实现一个简单的蒙特卡罗学习算法。我用 python 编写了一些代码，但经过 100000000 次迭代后得出的解决方案是这样的： 从 0 到 99 的各个资本级别的最佳下注大小 我的代码是这样的： M = 100000000 alpha = 0.1 # 假设 H = 1, T = 0 def Flip_coin() -&gt; int: return random.choices([0, 1], Weights=[0.6, 0.4], k=1)[0] def policy(Q, epsilon, Capital : int) -&gt; int: &#39;&#39;&#39; :param Capital: 玩家目前拥有的资本等级 :return: 赌注大小 &#39;&#39;&#39; gredy_action = Q[ Capital, : ].argmax() do_greedy = random.choices([0, 1], Weights= [epsilon, 1 - epsilon], k=1)[0] if do_greedy: 返回greedy_action else: return random.randint(0,capital) def play_game(Q, epsilon): &#39;&#39;&#39; :return: 包含状态的列表列表、行动（投注）和最终奖励（100 或 0） &#39;&#39;&#39; 历史 = [] 资本 = 50 而资本 != 0 且资本 &lt; 100：断言资本&lt; 100 断言资本 != 0 赌注 = 策略(Q, epsilon, 资本) coin = Flip_coin() History.append([资本,赌注]) if coin == 1: Capital += bet elif coin == 0: Capital -=下注 如果大写 == 0: 返回 [历史记录, 大写] 如果大写 &gt;= 100: 返回 [历史记录, 100] def mc_control(track_history=True): &#39;&#39;&#39; :param track_history: :return: Q, Q_hist &#39;&#39;&#39; Q = np.zeros((100,100)) # 索引 = 资本，列 = 下注大小 Q_hist = [] t = 0 for m in tqdm(range(M + 1)): t += 1 epsilon = max(0.1, 1 - 1e-5 * t) # 线性衰减 epsilon 历史，reward = play_game(Q,epsilon) if track_history: Q_hist.append(Q) for state_action in History: Capital = state_action[0] bet = state_action[1] Q[capital , 赌注] = Q[资本, 赌注] + alpha * (奖励 - Q[资本, 赌注]) 返回 Q, Q_hist     ;由   提交 /u/yuriIsLifeFuckYou   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpwtxe/monte_carlo_method_for_gamblers_problem_not/</guid>
      <pubDate>Sun, 12 May 2024 01:46:38 GMT</pubDate>
    </item>
    </channel>
</rss>