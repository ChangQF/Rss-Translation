<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 24 Jan 2024 12:27:03 GMT</lastBuildDate>
    <item>
      <title>需要对 DRL 中的 RNN 进行一些健全性检查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ecxyc/need_some_sanity_check_on_rnns_in_drl/</link>
      <description><![CDATA[嘿。  您通常如何使用单模型多代理 RNN DRL 处理隐藏状态？ 我正在考虑： -将隐藏状态从网络中拉出并保留在周围每当我的策略想要执行另一个步骤时进行植入。 -保留之前观察的历史记录，并为未来的每个步骤重新运行这些虚拟体验，以使隐藏状态达到应有的位置。  我认为拉动隐藏状态并缓存它是更好的方法，因为我不必进行前向传递来恢复它。  对于反向传播，这适用于 PPO，因为我默认对整个剧集进行采样，但不适用于 DQN。我想我应该修改它以采样整个剧集？  然后我还必须注意批处理和重置隐藏状态。  天啊，人们已经开始觉得 RNN 不应该属于 DRL。    由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ecxyc/need_some_sanity_check_on_rnns_in_drl/</guid>
      <pubDate>Wed, 24 Jan 2024 09:06:26 GMT</pubDate>
    </item>
    <item>
      <title>在 PPO 中梯度是否流过熵项？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ecedk/in_ppo_do_gradients_flow_through_the_entropy_term/</link>
      <description><![CDATA[根据标题，添加熵损失时是否通过策略参数进行反向传播？   由   提交/u/Conscious_Heron_9133   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ecedk/in_ppo_do_gradients_flow_through_the_entropy_term/</guid>
      <pubDate>Wed, 24 Jan 2024 08:25:57 GMT</pubDate>
    </item>
    <item>
      <title>对尝试在 Google Colab 中使用自定义健身房环境感到困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19eaq8o/confused_on_trying_to_use_a_custom_gym/</link>
      <description><![CDATA[       由   提交/u/kwasi3114  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19eaq8o/confused_on_trying_to_use_a_custom_gym/</guid>
      <pubDate>Wed, 24 Jan 2024 06:32:09 GMT</pubDate>
    </item>
    <item>
      <title>有哪些可以在 google collab 上运行的基本 3D 游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19e5xmu/any_basic_3d_games_that_work_on_google_collab/</link>
      <description><![CDATA[通过使用 opencv 制作的一些非常简单的游戏，我已经能够获得健身房和稳定的基线。  我想看看是否有一个 3d 引擎可以与 google collab 和gymnasium 配合使用来制作一些基本的 3d 动画以与稳定的基线一起使用。  如果有一个支持此功能的良好 Python 库或教程，请在此处链接。谢谢   由   提交 /u/ResponsibilityNew423   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19e5xmu/any_basic_3d_games_that_work_on_google_collab/</guid>
      <pubDate>Wed, 24 Jan 2024 02:13:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习能否有效地生成具有很多约束的布局？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dwhxk/is_reinforcement_learning_efficient_to_generate/</link>
      <description><![CDATA[      你好， 对于一个学校项目，我想尝试使用强化学习生成平面图，并将其与用于此问题的现有方法（例如进化算法和监督机器学习）进行比较。我希望有一些 RL 经验的人对该项目进行一些评论。 输入：房间列表、房间邻接矩阵、计划占地面积、每个房间的一些空间限制，例如最小/最大面积或比率（宽度/长度）。迭代从随机设置房间的原始布局开始（也许我会启动具有不同开始布局的多个强化学习系统）。 操作：交换 2 个房间、推动房间墙壁、划分房间墙壁（没有矩形形状），合并房间墙 奖励：尊重房间邻接矩阵，尊重空间约束，可以访问所有房间。 使用进化算法，我发现的最相似问题的文章： https://www.researchgate.net/publication/312263676_Evolutionary_approach_for_spatial_architecture_layout _design_enhanced_by_an_agent-based_topology_finding_system 使用强化学习，我发现了与问题最相似的论文：“用于快速芯片设计的图形放置方法” https://www.nature.com/文章/s41586-021-03544-w.epdf?sharing_token=tYaxh2mR5EozfsSL0WHZLdRgN0jAjWel9jnR3ZoTv0PW0K0NmVrRsFPaMa9Y5We9O4Hqf_liatg-lvhiVcYpHL_YQpqkurA31sxqtmA-E1yNUWVM MVSBxWSp7ZFFIWawYQYnEXoBE4esRDSWqubhDFWUPyI5wK_5B_YIO-D_kS8%3D 目标是强化学习过程学习“如何设计住宅平面图”能够适应像这样的新足迹： https://preview.redd.it/clnijy4to8ec1.png?width=1460&amp;format=png&amp;auto=webp&amp;s=8d50de4c4348237b29218c39c963dd7ddf6eaad7   由   提交/u/Geralt2477  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dwhxk/is_reinforcement_learning_efficient_to_generate/</guid>
      <pubDate>Tue, 23 Jan 2024 19:22:28 GMT</pubDate>
    </item>
    <item>
      <title>第一个项目：蛇</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</link>
      <description><![CDATA[     &lt; td&gt; 算法是某种类型的强化（虽然不确定，我只是从课程中获取了 nn 更新部分），我有一个神经网络69m 参数。网络的输入是 3 个网格：苹果位置、蛇位置和地图外区域。我还根据蛇的旋转来旋转输入，因此它始终朝上   由   提交/u/thebrownfrog  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:18 GMT</pubDate>
    </item>
    <item>
      <title>头脑风暴：多智能体的强化学习系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dr25j/brainstorming_rl_system_for_multiple_agents/</link>
      <description><![CDATA[我正在寻求有关如何构建由多个智能体追逐目标的强化学习系统的建议。目标是让所有智能体接近目标，但又不能太近。同时，我希望代理均匀分布在目标周围。 在 2D 中，想象理想的解决方案是代理沿着目标周围的圆均匀分布。  (1) 我能否期望使用 PPO 训练每个代理实例会产生良好的组性能？或者我是否需要研究像 POCA 这样的多代理方法？ (2) 关于如何创建平衡这些同时目标的奖励函数有什么建议吗？    由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dr25j/brainstorming_rl_system_for_multiple_agents/</guid>
      <pubDate>Tue, 23 Jan 2024 15:34:05 GMT</pubDate>
    </item>
    <item>
      <title>仅考虑剧集奖励的 PPO 应用程序</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dmyvg/ppo_applications_which_consider_only_episode/</link>
      <description><![CDATA[这里有人遇到过 PPO 文献或应用程序，其中我们正在训练代理，然后只考虑最佳训练集（具有最大奖励的集）生成策略？ 主要问题是我可以在我的应用程序中执行此操作，因为无论我尝试什么，我的算法都会收敛到局部次最优解，所以我在想是否可以选出最好的执行情节来构建我的最终策略？   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dmyvg/ppo_applications_which_consider_only_episode/</guid>
      <pubDate>Tue, 23 Jan 2024 12:07:14 GMT</pubDate>
    </item>
    <item>
      <title>一些 PPO 超参数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dk2ep/some_of_ppo_hyperparams/</link>
      <description><![CDATA[仅设置并行环境数量 = 物理核心数量和每次更新的总时间步数 = 内存中适合的内容是标准程序吗？我之前有过不好的经历，但我不确定我是否只是运气不好，如果我不这样做，我觉得我只是在浪费机器的潜力。 其他超参数会当然也取决于这些，所以我想如果我在以前研究过的环境中工作，那么找到新的学习率、剪辑等是另一个问题，我可以从其他人发现工作正常的任何内容开始   由   提交 /u/victorsevero   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dk2ep/some_of_ppo_hyperparams/</guid>
      <pubDate>Tue, 23 Jan 2024 08:50:10 GMT</pubDate>
    </item>
    <item>
      <title>迷宫游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d9a63/maze_game/</link>
      <description><![CDATA[Q-learning 项目，智能体自行学习找到迷宫内的出口。该项目是作为基于关卡的游戏实现的。 ​ https ://github.com/F-a-b-r-i-z-i-o/maze-game   由   提交/u/Stunning_Ad_1539   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d9a63/maze_game/</guid>
      <pubDate>Mon, 22 Jan 2024 23:04:55 GMT</pubDate>
    </item>
    <item>
      <title>有人知道斯坦福强化学习 XCS234 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d7mey/does_anyone_know_about_stanford_reinforcement/</link>
      <description><![CDATA[大家好， 我正在考虑这个在线课程。不过，我有一份全职工作。我的工作日程非常灵活，但这并不意味着我可以忽略所有的会议。  我看到描述说它不是学生节奏而是教师节奏。那么这是否意味着一旦我错过了课程，我就错过了？那么完成硬件并拿到证书会不会很麻烦？  这里有人以前上过课吗？有评论吗？谢谢   由   提交/u/sunson29  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d7mey/does_anyone_know_about_stanford_reinforcement/</guid>
      <pubDate>Mon, 22 Jan 2024 21:56:45 GMT</pubDate>
    </item>
    <item>
      <title>Cogment Lab 简介 - 用于人机循环 RL 的开发人员工具包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d6owp/introducing_cogment_lab_a_developers_toolkit_for/</link>
      <description><![CDATA[      你好你好，我&#39;我很高兴终于能分享我过去几个月在 AI-R 致力于的开源项目：Cogment Lab！ ​ tl ;博士，如果您想在有人参与的情况下运行 Gymnasium 或 PettingZoo 环境，现在就可以了。 ​ 您可以执行的操作的非详尽列表使用 Cogment Lab 轻松完成： 在 Gymnasium/PZ 中收集人类演示以进行模仿学习 观察学习代理并覆盖其行为 运行实验PettingZoo 环境中的混合人类与人工智能团队（与您的 RL 代理合作，或在竞争性游戏中击败它） 根据人类干预设置奖励 训练基于奖励的 RL 交织在一起实时行为克隆 ​ 该库仍在开发中，但应该完全可用。绝对欢迎任何建议、错误报告和贡献。 ​ Repo 链接：https://github.com/cogment/cogment-lab 教程：https://github.com/cogment/cogment-lab/tree/develop/ ​ ​ &gt; https://preview.redd.it/ 8t8ec4b162ec1.png?width=1081&amp;format=png&amp;auto=webp&amp;s=7ec357da5ac1e318d18ec4c7bde566be39c9c03b ​ PS 我很确定我的老板还在没有注意到这个标志，但它一直保持这种状态，直到有人强迫我让它变得更专业，并与公司在垂直业务或其他方面的协同作用保持一致  &amp;# 32；由   提交 /u/RedTachyon   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d6owp/introducing_cogment_lab_a_developers_toolkit_for/</guid>
      <pubDate>Mon, 22 Jan 2024 21:19:37 GMT</pubDate>
    </item>
    <item>
      <title>帮助表示赞赏！尝试让代理在 Unity ML Agents 中投篮</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d2fm0/help_appreciated_trying_to_get_an_agent_to_shoot/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d2fm0/help_appreciated_trying_to_get_an_agent_to_shoot/</guid>
      <pubDate>Mon, 22 Jan 2024 18:24:19 GMT</pubDate>
    </item>
    <item>
      <title>我用 3D 动画教这个机器人自己行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</link>
      <description><![CDATA[       由   提交/u/djessimb   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</guid>
      <pubDate>Mon, 22 Jan 2024 07:08:18 GMT</pubDate>
    </item>
    <item>
      <title>编程…</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</link>
      <description><![CDATA[       由   提交/u/Throwawaybutlove  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</guid>
      <pubDate>Mon, 22 Jan 2024 01:27:44 GMT</pubDate>
    </item>
    </channel>
</rss>