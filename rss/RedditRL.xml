<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 29 Dec 2024 18:20:46 GMT</lastBuildDate>
    <item>
      <title>小型 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1howtr2/small_rl_project/</link>
      <description><![CDATA[您好，我有一个故障单元传感器数据样本，还有一个健康单元的样本。我想尝试下面的代码，但代理没有学习任何东西。我的代码有什么问题或如何改进？ import numpy as np import matplotlib.pyplot as plt from stable_baselines3 import SAC import gym from gym import space 定义 FaultyToHealthyEnv（使用您之前的实现） class FaultyToHealthyEnv(gym.Env): def init(self, healthywindows,aulty_windows, max_steps=5000): super(FaultyToHealthyEnv, self).init_()  self.healthy_windows = healthy_windows self.faulty_windows =aulty_windows self.current_index = 0 self.current_faulty = None self.steps_taken = 0 self.max_steps = max_steps self.previous_mse = 0 # 观察：当前故障信号 self.observation_space = space.Box( low=-np.inf, high=np.inf, shape=(faulty_windows.shape[1],), dtype=np.float32 ) # 动作：与故障信号大小匹配的校正向量 self.action_space = space.Box( low=-0.1, high=0.1, shape=(faulty_windows.shape[1],), dtype=np.float32 ) def reset(self): self.current_index = 0 # 始终选择第一个样本 self.current_faulty = self.faulty_windows[self.current_index].copy() self.steps_taken = 0 return self.current_faulty def step(self, action): self.current_faulty += action healthy_signal = self.healthy_windows[self.current_index] # 计算奖励 mse = np.mean((self.current_faulty - healthy_signal) ** 2) Improvement = self.previous_mse - mse smoothness_penalty = 0.1 * np.sum(np.abs(action)) target_bonus = 100.0 if mse &lt; 0 else 0.0 reward = Improvement - smoothness_penalty - 0.01 * self.steps_taken + target_bonus self.previous_mse = mse # 更新下一步 # 检查情节是否完成 self.steps_taken += 1 done = mse &lt; 0.01 或 self.steps_taken &gt;= self.max_steps return self.current_faulty, reward, done, {} def render(self, mode=&quot;human&quot;): pass  加载健康和故障信号数据（用实际数据替换） healthy_windows = np.array(dataset[dataset[&#39;label&#39;] == 1][&#39;signal&#39;].tolist())aulty_windows = np.array(dataset[dataset[&#39;label&#39;] == 0][&#39;signal&#39;].tolist()) 使用 FaultyToHealthyEnv 进行 SAC 的训练函数 def train_sac_faulty_to_healthy(healthy_windows,aulty_windows, episodes=100, timesteps_per_episode=200): env = FaultyToHealthyEnv(healthy_windows，faulty_windows) model = SAC( &quot;MlpPolicy&quot;, env, verbose=1, learning_rate=1e-4, buffer_size=500000, batch_size=128, ent_coef=0.1 ) episode_rewards = [] for episode in range(episodes): state = env.reset() total_reward = 0 done = False step_count = 0 transitions = [] while not done and step_count &lt; timesteps_per_episode：action，_ = model.predict（state，deterministic=False）next_state，reward，done，_ = env.step（action）transitions.append（（state，action，reward，next_state，done））state = next_statetotal_reward + = reward step_count + = 1 episode_rewards.append（total_reward）print（f＆quot;Episode {episode + 1}，Total Reward：{total_reward}＆quot;）# 在积累经验后训练模型model.learn（total_timesteps = episodes * timesteps_per_episode）env.close（）return episode_rewards，model，transitions，state 在 FaultyToHealthyEnv 上训练 SAC 代理 rewards，trained_model，transitions，corrected = train_sac_faulty_to_healthy（ healthy_windows=healthy_windows,aulty_windows=faulty_windows,episodes=100,timesteps_per_episode=50) 绘制情节奖励 plt.figure(figsize=(10, 6)) plt.plot(rewards,label=&quot;情节奖励&quot;) plt.xlabel(&quot;情节&quot;) plt.ylabel(&quot;总奖励&quot;) plt.title(&quot;SAC 在错误信号校正上的训练表现&quot;) plt.legend() plt.show() plt.figure(figsize=(12, 6)) plt.plot(corrected,label=&quot;校正信号&quot;,alpha=0.7) plt.plot(healthy_windows[0], label=&quot;健康信号&quot;, linestyle=&quot;--&quot;) plt.plot(faulty_windows[0], label=&quot;故障信号&quot;, linestyle=&quot;:&quot;) plt.legend() plt.title(&quot;已校正与健康信号&quot;)    提交人    /u/ezgiorhan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1howtr2/small_rl_project/</guid>
      <pubDate>Sun, 29 Dec 2024 14:14:45 GMT</pubDate>
    </item>
    <item>
      <title>我的 DQN 代理怎么会这么弱智？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hova0i/how_my_dqn_agent_can_be_so_rtarded/</link>
      <description><![CDATA[很抱歉出现这个标题，但我真的很沮丧。我真的请求一些帮助，并弄清楚我遗漏了什么...... 我正在尝试教我的 DQN 代理学习最简单的控制器问题，遵循所需的值。 我正在模拟一个只有 1 个状态和 3 个动作的淋浴环境。  目标 = 达到所需的温度范围。 状态 = 当前温度 动作 = 增加（+1），Noop（0），减少（-1） 如果温度为 [36, 38]，则奖励 = +1，否则 -1 重置 = 20 + random.randint(-5, 5)  我的 DQN 代理实际上无法学习世界上最简单的问题。 这怎么可能？ Q-Learning 可以学习这个。DQN 算法有什么不同？ DQN 不是试图近似最佳 Q 函数吗？换句话说，试图模仿正确的 Q 表，但使用函数而不是查找表？ 我的干净代码在这里。我想了解到底发生了什么，以及为什么我的代理无法学到任何东西！ 谢谢！ 代码： from stable_baselines3.common.callbacks import BaseCallback from stable_baselines3 import DQN import numpy as np import gym import random from gym import space from gym.spaces import Box class ShowerEnv(gym.Env): def __init__(self): super(ShowerEnv, self).__init__() # 动作空间：减少、保持、增加 self.action_space = space.Discrete(3) # 观察空间：温度 self.observation_space = Box(low=np.array([0], dtype=np.float32), high=np.array([100.0], dtype=np.float32)) # 设置起始温度 self.state = 20 + random.randint(-5, 5) # 设置淋浴长度 self.shower_length = 100 def step(self, action): # 应用操作 ---&gt; [-1, 0, 1] self.state += action - 1 # 将淋浴长度减少 1 秒 self.shower_length -= 1 # 保护边界状态条件 if self.state &lt; 0: self.state = 0 reward = -1 # 保护边界状态条件 elif self.state &gt; 100：self.state = 100 reward = -1 # 如果状态在边界状态条件内 else：# 温度条件的期望范围 if 36 &lt;= self.state &lt;= 38：reward = 1 # 温度条件的非期望范围 else：reward = -1 # 检查情节是否结束 if self.shower_length &lt;= 0：done = True else：done = False info = {} return np.array([self.state]), reward, done, {} def render(self, action=None): pass def reset(self): self.state = 20 + random.randint(-50, 50) self.shower_length = 100 return np.array([self.state]) class SaveOnEpisodeEndCallback(BaseCallback): def __init__(self, save_freq_episodes, save_path, verbose=1):超级（SaveOnEpisodeEndCallback，self）。__init__（verbose）self.save_freq_episodes = save_freq_episodes self.save_path = save_path self.episode_count = 0 def _on_step（self）-&gt; bool：如果self.locals [&#39;dones&#39;] [0]：self.episode_count + = 1如果self.episode_count％self.save_freq_episodes == 0：save_path_full = f＆quot; {self.save_path} _ep_ {self.episode_count}＆quot; self.model.save（save_path_full）如果self.verbose＆gt; 0：print（f&quot;模型在剧集 {self.episode_count}&quot;）如果 __name__ ==&quot;__main__&quot;则返回 True：env = ShowerEnv() save_callback = SaveOnEpisodeEndCallback(save_freq_episodes=25，save_path=&#39;./models_00/dqn_model&#39;) logdir =&quot;logs&quot;模型 = DQN(policy=&#39;MlpPolicy&#39;, env=env, batch_size=32, buffer_size=10000, exploration_final_eps=0.005, exploration_fraction=0.01, gamma=0.99, gradient_steps=32, learning_rate=0.001, learning_starts=200, policy_kwargs=dict(net_arch=[16, 16]), target_update_interval=20, train_freq=64, verbose=1, tensorboard_log=logdir) 模型.学习(total_timesteps=int(1000000.0), reset_num_timesteps=False, callback=save_callback, tb_log_name=&quot;DQN&quot;)     提交人    /u/OpenToAdvices96   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hova0i/how_my_dqn_agent_can_be_so_rtarded/</guid>
      <pubDate>Sun, 29 Dec 2024 12:42:39 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle 和 Colab 上可用的 GPU 是否足以学习 Deep RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1houfkt/will_gpu_available_on_kaggle_and_colab_be_enough/</link>
      <description><![CDATA[大家好， 我正在考虑深入研究深度强化学习。我无法在本地使用强大的 GPU。 所以我有这个问题，Kaggle 和 Colab 上可用的 GPU 是否有助于学习和探索所有不同的算法。深度强化学习尚未实现样本效率。 我见过人们训练 2M+ 或更多步骤才能获得结果。 谢谢。    提交人    /u/mono1110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1houfkt/will_gpu_available_on_kaggle_and_colab_be_enough/</guid>
      <pubDate>Sun, 29 Dec 2024 11:45:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 carla 进行 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</link>
      <description><![CDATA[我的毕业设计用carla完成强化学习，能推荐一些在线课程吗？    submitted by    /u/Clean_Tip3272   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</guid>
      <pubDate>Sun, 29 Dec 2024 10:36:18 GMT</pubDate>
    </item>
    <item>
      <title>遗憾值为 O( \sqrt(log K T ) ) 的 K 臂随机赌博机算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</link>
      <description><![CDATA[我想知道是否有任何 K 臂随机老虎机算法可以实现 $O(\sqrt(T))$ 遗憾，且因子为常数 $\sqrt{ log K }$。  我知道 exp3 可以实现 O(\sqrt(T)) 遗憾，因子为 sqrt(k log K )，而 UCB 可以实现 \tilde{O}( sqrt(T) ) 遗憾，因子为 sqrt(k)？  是否有一种算法，其臂数与 sqrt( log K ) 类似？或者，是否有更严格的 exp3 或 UCB 分析，可以在臂数方面实现更好的因子？  我正在研究一个问题，其中臂的数量为 K^{a}，其中 a 是某个参数，并且我想将我的因子归结为类似 a * poly(K) - （poly(K) 表示关于 K 的多项式）的东西。    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</guid>
      <pubDate>Sun, 29 Dec 2024 05:59:36 GMT</pubDate>
    </item>
    <item>
      <title>似乎无法理解如何使用 NEAT-Python 结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hokvmb/cant_seem_to_understand_how_to_work_with/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hokvmb/cant_seem_to_understand_how_to_work_with/</guid>
      <pubDate>Sun, 29 Dec 2024 01:33:15 GMT</pubDate>
    </item>
    <item>
      <title>RL“包裹” 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</link>
      <description><![CDATA[我通常会在假期的最后几天努力赶上进度（事实证明这些天不可能）并回顾学术和工业发展方面的主要亮点。请在此处添加您今年的顶级 RL 作品     提交人    /u/blitzkreig3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</guid>
      <pubDate>Sat, 28 Dec 2024 21:08:08 GMT</pubDate>
    </item>
    <item>
      <title>山地车项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hobshj/mountain_car_project/</link>
      <description><![CDATA[      我正在尝试使用 Q 学习、DQN 和 Soft Actor Critic 解决山地车问题。 我设法在离散空间中使用 Q 学习解决了该问题，但在调整 DQN 时，我发现训练图并不像 Q 学习那样收敛。相反，它相当不稳定。但是，当我使用情节长度和回报来评估策略时，我发现大多数种子情节都很短并且奖励更高。这是否意味着我解决了它？ 参数是： {&#39;env&#39;: &lt;gymnax.environments.classic_control.mountain_car.MountainCar at 0x7b368faf7ee0&gt;, &#39;env_params&#39;: {&#39;max_steps_in_episode&#39;: 200, &#39;min_position&#39;: -1.2, &#39;max_position&#39;: 0.6, &#39;max_speed&#39;: 0.07, &#39;goal_position&#39;: 0.5, &#39;goal_velocity&#39;: 0.0, &#39;force&#39;: 0.001, &#39;gravity&#39;: 0.0025}, &#39;eval_callback&#39;: &lt;function RLinJAX.algos.algorithm.Algorithm.create。&lt;locals&gt;.eval_callback(algo, ts, rng)&gt;，&#39;eval_freq&#39;：5000，&#39;skip_initial_evaluation&#39;：False，&#39;total_timesteps&#39;：1000000，&#39;learning_rate&#39;：0.0003，&#39;gamma&#39;：0.99，&#39;max_grad_norm&#39;：inf，&#39;normalize_observations&#39;：False，&#39;target_update_freq&#39;：800，&#39;polyak&#39;：0.98，&#39;num_envs&#39;：10，&#39;buffer_size&#39;：250000，&#39;fill_buffer&#39;：1000，&#39;batch_size&#39;：256， &#39;eps_start&#39;：1，&#39;eps_end&#39;：0.05，&#39;exploration_fraction&#39;：0.6，&#39;agent&#39;：{&#39;hidden_​​layer_sizes&#39;：（64,64），&#39;activation&#39;：&lt;PjitFunction&gt;，&#39;action_dim&#39;：3，&#39;parent&#39;：None，&#39;name&#39;：None}，&#39;num_epochs&#39;：5，&#39;ddqn&#39;：True}  对学习到的政策的评估 编辑：我打印了短剧集百分比和高奖励剧集百分比： 短剧集百分比 99.718 高奖励百分比 99.718    提交人    /u/Ordinary_Reveal8842   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hobshj/mountain_car_project/</guid>
      <pubDate>Sat, 28 Dec 2024 18:26:30 GMT</pubDate>
    </item>
    <item>
      <title>你喜欢观看 RL 内容视频吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ho23ty/do_you_love_watching_rl_content_videos/</link>
      <description><![CDATA[基本上就是标题，但更具体地说： 我喜欢观看机器人学习某些东西（在模拟中）的内容，这比教育意义更有趣。 尤其是来自“AI Warehouse”这样的频道，其中创建者对 AI（立方体/假人）提出挑战，并描述所玩的“游戏”，并附加有趣的评论。 由于这个 subreddit 更倾向于教育而不是娱乐，我想知道你们中有多少人像我（男，27 岁）一样喜欢这些？ （+ 年龄/性别）？    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ho23ty/do_you_love_watching_rl_content_videos/</guid>
      <pubDate>Sat, 28 Dec 2024 09:15:53 GMT</pubDate>
    </item>
    <item>
      <title>RL 是否用于训练游戏《黎明杀机》中的机器人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnsh5d/was_rl_used_to_train_the_bots_in_the_game_dead_by/</link>
      <description><![CDATA[在这个帖子和这个中有很多讨论，但似乎没人知道 - 开发人员对此没有发表任何评论。我也在游戏的 discord 服务器中询问过，但也没人知道。 他们可以使用强化学习来训练它们吗？我对强化学习的了解非常基础，我现在正在尝试研究它（我刚刚开始了解深度 Q 学习）。这似乎是可能的，因为我知道 RL 已经用于很多游戏（尽管我见过的例子都是老式游戏）。    提交人    /u/gitgud_x   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnsh5d/was_rl_used_to_train_the_bots_in_the_game_dead_by/</guid>
      <pubDate>Fri, 27 Dec 2024 23:46:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的第一步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnqx9l/first_step_in_rl/</link>
      <description><![CDATA[如何开始学习/做 RL？ - 什么方法可以学习？ - 了解哪些 hello world 项目？ - 学习 RL 的步骤是什么？ - 如果我想从零开始成为 RL 的英雄，我该怎么做？    提交人    /u/Soft_Awareness6826   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnqx9l/first_step_in_rl/</guid>
      <pubDate>Fri, 27 Dec 2024 22:34:09 GMT</pubDate>
    </item>
    <item>
      <title>AAAI 2025 教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</link>
      <description><![CDATA[AI 安全：从强化学习到基础模型 链接：https://aaai.org/conference/aaai/aaai-25/tutorial-and-lab-list/#TQ10     提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</guid>
      <pubDate>Fri, 27 Dec 2024 13:07:15 GMT</pubDate>
    </item>
    <item>
      <title>TD3 算法第二次更新时策略严重偏向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hndzen/heavily_biased_policy_on_the_second_update_for/</link>
      <description><![CDATA[大家好， 我为演员和评论家的架构实现了一个带有 CNN 和线性层的 TD3 算法。它被用于解决 Open AI Gym 的 CarRacing 环境。动作空间是连续的，其中最大动作为 [1, 1, 1]，最小动作为 [-1, 0, 0]，分别用于转向、加油和刹车。策略更新是在情节结束时完成的，即环境中的步骤处于完成状态时。环境中发生的时间步数用于在情节结束后多次更新策略。重放缓冲区也使用 1000 个时间步的随机动作初始化。 为什么在第一集之后，演员会选择立即开始向左大力转向（-1）的动作？转向几乎始终为 1 且高于 0.9，并且只发生在第二集中。第一集中选择的动作与随机采样动作本质上相同。 非常感谢    提交人    /u/Sea_Farmer5942   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hndzen/heavily_biased_policy_on_the_second_update_for/</guid>
      <pubDate>Fri, 27 Dec 2024 12:37:05 GMT</pubDate>
    </item>
    <item>
      <title>O(sqrt(T)) 遗憾比 O(sqrt(T \log T)) 遗憾更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hn2me2/is_osqrtt_regret_better_than_osqrtt_log_t_regret/</link>
      <description><![CDATA[从数学上讲，sqrt(T) 优于 sqrt(T \log T)，但如果我提交论文，sqrt(T) 遗憾算法会被认为比 sqrt(T \log T) 遗憾算法更好吗？我正在阅读一篇论文，作者声称他们的算法是 \tildeO( \sqrt(T) )；尽管在论文正文中报告遗憾值为 O( \sqrt(T log T) )。我有点困惑，因为我认为 \tilde 应该是“忽略常量/模型参数”，但 log T 不是 T 方面的常数。他们还提到了一个特殊情况，其中遗憾是 O( \sqrt(T) )。我还查看了高概率遗憾与预期遗憾，他们似乎在说预期遗憾的上限为 O( sqrt (T log T ) )。 O( \sqrt(T) ) 是否被认为比 O( \sqrt(T \log T) ) 更好，或者差异可以忽略不计？    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hn2me2/is_osqrtt_regret_better_than_osqrtt_log_t_regret/</guid>
      <pubDate>Fri, 27 Dec 2024 00:41:34 GMT</pubDate>
    </item>
    <item>
      <title>DQN 中的训练图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmxh3f/training_plot_in_dqn/</link>
      <description><![CDATA[      大家好， 圣诞快乐，节日快乐！ 我在阅读 DQN 代理的训练图时遇到了麻烦，因为它似乎没有太大的改进，但如果我将它与随机代理进行比较，它的结果会更好。 此外，它还有很多噪音，我认为这不是一个好的事情。 我见过一些人在验证剧集中监控奖励图 对于剧集 = 2000： （在 4096 步上进行训练，然后在其中一集上进行验证并使用其奖励进行绘图） 剧集++ 另外，我已经阅读了有关奖励标准化的内容，我应该尝试一下吗？ returns =（returns - returns.mean()）/（returns.std() + eps） 期待任何见解和训练情节已附上。 提前致谢 https://preview.redd.it/wu6j4o9h9n9e1.png?width=884&amp;format=png&amp;auto=webp&amp;s=435c1d881b29412020c355308543eb911b35db94    提交人    /u/Dry-Image8120   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmxh3f/training_plot_in_dqn/</guid>
      <pubDate>Thu, 26 Dec 2024 20:40:51 GMT</pubDate>
    </item>
    </channel>
</rss>