<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 17 Nov 2024 09:17:11 GMT</lastBuildDate>
    <item>
      <title>Isaac Lab 中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</link>
      <description><![CDATA[您好，我是在模拟中训练机器人的新手。我刚刚设置了我的 isaac 实验室，但我不知道如何在其中训练我自己的模型。关于它的文档也不多（我知道 NVidia 文档，但就是这样）。有人能为我提供更多关于如何入门的信息吗？另外，没有关于它的教程/视频/文档，因为它是新的还是不好的？它什么时候向公众开放的？谢谢！    提交人    /u/North_Set8162   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</guid>
      <pubDate>Sat, 16 Nov 2024 23:06:43 GMT</pubDate>
    </item>
    <item>
      <title>银行业有趣的研究课题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</link>
      <description><![CDATA[我目前是一名计算机科学 (ML 专业) 的兼职硕士生，在银行业担任数据工程师，我计划通过在银行业务场景中应用 RL 代理来撰写研究报告。我能想到一些事情，比如贷款决策或欺诈检测，但对我来说没有什么真正有趣的。有什么建议可以告诉我可以研究什么吗？我理想情况下想要一些我们有一些开源数据的东西。    提交人    /u/SatwikGu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</guid>
      <pubDate>Sat, 16 Nov 2024 22:33:22 GMT</pubDate>
    </item>
    <item>
      <title>“可解释的对比蒙特卡洛树搜索推理”，Gao 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</guid>
      <pubDate>Sat, 16 Nov 2024 21:40:25 GMT</pubDate>
    </item>
    <item>
      <title>人的手臂</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</link>
      <description><![CDATA[你好。我想制作一个人体手臂模型，并使用强化学习使其达到目标。 我知道这很难实现（如果可能的话，大量的 DOF、较长的训练时间），所以我尝试使用简单的模型和完全增加的模型来构建它。 如果需要，我很乐意制作自己的 urdf 模型，但也很乐意使用已经存在的东西。 你会推荐从哪里开始？最好的算法是什么（可能是 PPO、SAC、DDPG）？最好的平台是什么（可能是 pybullet、MuJoCo、ROS 和 Gazebo）？ 任何帮助表示感谢。    提交人    /u/tedthemouse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</guid>
      <pubDate>Sat, 16 Nov 2024 20:20:38 GMT</pubDate>
    </item>
    <item>
      <title>为研究论文编写方程式并组织人员</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</link>
      <description><![CDATA[大家好，我目前是强化学习和迁移学习领域的博士生。我正在准备写我的第一篇论文，写方程式及其证明、推导等感觉很不舒服。我想知道经验丰富的研究人员是如何做到的？他们使用什么样的工具？在整个项目中，他们如何不断写下所有这些数学符号和方程式，他们如何呈现它们、跟踪它们，并同时维护多个项目。对于工具，你们使用 iPad 之类的工具吗？我理解 overleaf 的用途，但我觉得亲手写它们更有成就感。你们能分享一下你们是如何用数学和代码等开发系统的吗？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</guid>
      <pubDate>Sat, 16 Nov 2024 17:01:50 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么关于训练 ppo/dqn 解决迷宫的技巧？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gslsxc/any_tips_for_training_ppodqn_on_solving_mazes/</link>
      <description><![CDATA[创建了自己的 gym 环境，其中观察结果由一个形状为 4 的 numpy 数组 (agent_x,agent_y,target_x,target_y) 组成。代理获得的基本奖励为 (distancebefore - distanceafter)（使用 astar），每一步为 -1 或 0 或 1，到达目标时获得奖励 = 100，与墙壁碰撞时获得奖励 = -1（如果我使用 distancebefore - distanceafter，则为 0）。 我正在尝试训练 ppo 或 dqn 代理（尝试了两者）来解决带有墙壁的 10x10 迷宫 你们有什么技巧可以让我尝试，以便我的代理可以在我的环境中学习？ 欢迎提供任何帮助和提示，我之前从未在迷宫上训练过代理，我想知道是否有什么特别需要考虑的。如果有其他模型更好，请告诉你 如果我的代理始终从左上角开始，而目标始终在右下角，则 dqn 可以解决它而 ppo 不能，然而，在我的用例中我想解决的是一个迷宫，每次调用 reset() 时代理都从随机位置开始。这个迷宫能解决吗？ （ppo 似乎也试图穿过障碍物，就像它由于某种原因无法检测到它们一样） 我理解，每次固定代理和目标位置时，dqn 都需要学习一条路径，但是如果代理位置每次重置时都会发生变化，则需要学习许多正确的路径。 墙壁总是固定的。 我对模型使用 baselines3 （我也尝试了 sb3_contrib qrdqn 和循环 ppo） https://imgur.com/a/SWfGCPy    提交人    /u/More_Peanut1312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gslsxc/any_tips_for_training_ppodqn_on_solving_mazes/</guid>
      <pubDate>Sat, 16 Nov 2024 11:59:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助模拟人形站立任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsl3xh/help_with_simulated_humanoid_standing_task/</link>
      <description><![CDATA[  由    /u/Budget_Bad_4135  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsl3xh/help_with_simulated_humanoid_standing_task/</guid>
      <pubDate>Sat, 16 Nov 2024 11:09:18 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习 DEEPRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsjv66/transfer_learning_deeprl/</link>
      <description><![CDATA[你好， DeepRl 中的迁移学习/领域适应的最新进展是什么？ 谢谢！☺️    提交人    /u/TeamTop4542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsjv66/transfer_learning_deeprl/</guid>
      <pubDate>Sat, 16 Nov 2024 09:35:00 GMT</pubDate>
    </item>
    <item>
      <title>找到达到目标的最少移动次数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsii5c/finding_the_minimum_number_of_moves_to_a_goal/</link>
      <description><![CDATA[我是强化学习的新手。我想使用 RL 作为练习来解决 15 个难题 https://en.m.wikipedia.org/wiki/15_puzzle。第一个问题是随机移动会非常缓慢地到达解决状态。所以我想我可以从解决状态开始，进行少量移动，训练代理来解决这个问题，然后慢慢地从解决状态进行越来越多的移动。 我打算使用稳定的基线 3。我不确定我的想法是否可以使用该库进行编码，因为它必须以某种方式记住训练过的代理，并且每次我增加从解决状态开始的移动次数时，都从该点继续训练。 这个想法看起来合理吗？   由    /u/MrMrsPotts  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsii5c/finding_the_minimum_number_of_moves_to_a_goal/</guid>
      <pubDate>Sat, 16 Nov 2024 07:51:55 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：强化学习以在多边形中分布点（Stable-Baselines3）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gse59f/help_needed_reinforcement_learning_for/</link>
      <description><![CDATA[大家好， 我是强化学习的新手，之前没有使用过 Python 或 Stable-Baselines3 库。尽管如此，我还是想解决一个项目，让代理学习在多边形内均匀分布点。 问题陈述：  代理应尽可能均匀地分布点。 此外，这些点必须与多边形的边缘保持最小距离。 多边形可以具有任意形状（不仅仅是简单的矩形等）。  我正在努力弄清楚如何：  为这个问题定义环境。 创建一个有意义的奖励函数来鼓励点的均匀分布。 使用 Stable-Baselines3 设置和配置学习过程。  如果有人有类似问题的经验或可以指导我完成初始步骤，我将不胜感激！我也欢迎有关教程、示例或可以帮助我入门的一般技巧的建议。 提前感谢您的帮助！    提交人    /u/Initial-Crew2533   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gse59f/help_needed_reinforcement_learning_for/</guid>
      <pubDate>Sat, 16 Nov 2024 03:13:20 GMT</pubDate>
    </item>
    <item>
      <title>是否有人知道人工智能可以接受超过三个空间维度感知的训练？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gs7stm/does_anyone_know_of_ai_being_trained_with_more/</link>
      <description><![CDATA[我刚刚注意到，虽然人类的视觉能力有限，但人工智能却不必如此。我们知道制作使用四个或更多空间维度的游戏所需的所有数学知识。虽然这种为人类设计的游戏被投射到 3D 世界，然后经常投射到 2D 屏幕上，但如果游戏仅供人工智能使用，则无需这样做。 我们可以训练人工智能在更高维度上执行任务，看看我们能否从中学到任何东西。 也许可以创建程序化的 4D 环境，就像 Deepmind 在 XLand 中所做的那样（请参阅 https://arxiv.org/pdf/2107.12808）用于 3D。 是否有人知道以前尝试过类似方法的例子？ 我特别要求超过三个空间维度。当然，我们确实经常使用具有许多独立特征的高维数据。    提交人    /u/SimulatedScience   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gs7stm/does_anyone_know_of_ai_being_trained_with_more/</guid>
      <pubDate>Fri, 15 Nov 2024 21:53:41 GMT</pubDate>
    </item>
    <item>
      <title>一款用于多智能体模仿学习和强化学习的开源 2D 版《反恐精英》，全部采用 Python 编写</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gs71k5/an_opensource_2d_version_of_counterstrike_for/</link>
      <description><![CDATA[      SiDeGame（简化的拆弹游戏）是我 3 年前的一个项目，我最终想与大家分享一下，但一直推迟，因为我心里还有一些更新的想法。现在我必须承认我手头上有太多新工作，所以就在这里： 游戏 GIF 该项目的最初目的是为我的硕士论文创建一个 AI 基准环境。从人工智能的角度看，我对 CS 感兴趣的原因有以下几个：  共享经济（玩家可以为他人购买和丢弃物品）， 不确定的角色（每个人在游戏开始时都具有相同的能力和可用物品）， 不完善的盟友信息（第一人称视角限制了对队友信息的访问）， 双峰感知（声音是重要的信息来源，特别是在没有视觉效果的情况下）， 标准化（游戏规则很少改变）， 直观的界面（易于保持一致以进行人机对比）。  起初，我考虑与 CSGO 甚至 CS1.6 的实际游戏进行交互，但后来决定从头开始制作我自己的版本，这样我就可以了解所有的细节，然后根据需要进行更改。我只有一年的时间来做这件事，所以我选择用 Python 来做所有事情——这是我以及 AI 社区中的许多人最熟悉的语言，我认为以后可以提高效率。 有几种方法可以训练 AI 玩 SiDeGame：  模仿学习：让人类玩一些在线游戏。网络历史记录将被记录下来，并可用于重新模拟会话、提取输入输出标签、统计数据等。代理通过监督学习进行训练，以克隆玩家的行为。 本地 RL：使用游戏的同步版本手动踏入并行环境。代理通过反复试验进行强化学习训练。 远程 RL：将参与者客户端连接到远程服务器并让代理实时自我游戏。  作为 AI 基准，我仍然认为它不完整。我不得不匆忙进行模仿学习，直到最近我才重写了强化学习示例以使用我经过测试的实现。现在我可能不会再独自进行任何重大工作了，但我认为作为一个开源在线多人伪 FPS 学习环境，它对 AI 社区来说仍然很有趣。 以下是链接：  代码：https://github.com/jernejpuc/sidegame-py 简短会议论文：https://plus.cobiss.net/cobiss/si/en/bib/86401795（4 页英文，联合 PDF 的一部分，80 MB） 完整论文：https://repozitorij.uni-lj.si/IzpisGradiva.php?lang=eng&amp;id=129594（90 页斯洛文尼亚语，PDF 8 MB）     由   提交  /u/yerney   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gs71k5/an_opensource_2d_version_of_counterstrike_for/</guid>
      <pubDate>Fri, 15 Nov 2024 21:20:43 GMT</pubDate>
    </item>
    <item>
      <title>就状态值函数而言的动作值函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grv082/actionvalue_function_in_terms_of_state_value/</link>
      <description><![CDATA[      https://preview.redd.it/bs4ena7c421e1.png?width=637&amp;format=png&amp;auto=webp&amp;s=b7625cb7538e6ead3183e8f14cbd0acd613581f5 我正在阅读 Sutton&amp;Barto 的书。我在练习 3.13 处卡住了。问题是用 vπ 和 p(s′,r∣s,a) 写出 qπ。我追踪了上述步骤。我怎样才能从那里继续，或者我的逻辑是正确的？     提交人    /u/demirbey05   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grv082/actionvalue_function_in_terms_of_state_value/</guid>
      <pubDate>Fri, 15 Nov 2024 12:14:08 GMT</pubDate>
    </item>
    <item>
      <title>RL 角色的面试流程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grrdo5/interview_process_for_rl_roles/</link>
      <description><![CDATA[大家好！ 目前我是一名大四本科生，我也在一个机器人实验室工作，我们主要在那里使用 RL。我最初是因控制理论/朴素 ML 而被聘用的，后来被调到了 RL 团队。 从明年开始，我将在 RL 领域寻找工作或攻读博士学位的机会，我想知道这种类型的职位需要经历什么样的面试过程。它是否类似于 ML/SWE 职位，需要进行几轮技术面试和分配任务，还是完全不同？ 此外，我目前在中等影响力的会议和期刊上发表了几篇论文。那么对于攻读博士学位的机会，我应该尝试至少在高影响力的期刊上发表文章还是谨慎行事？ 谢谢你的帮助🙏    提交人    /u/oz_zey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grrdo5/interview_process_for_rl_roles/</guid>
      <pubDate>Fri, 15 Nov 2024 07:52:09 GMT</pubDate>
    </item>
    <item>
      <title>Yann LeCun 仍然不认为 RL 对 AI 系统至关重要。他认为只有无监督/监督学习/SSL 算法才能处理 RL 所用到的问题类型，例如顺序决策，或者它们将如何处理诸如探索之类的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</link>
      <description><![CDATA[    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</guid>
      <pubDate>Fri, 15 Nov 2024 00:44:11 GMT</pubDate>
    </item>
    </channel>
</rss>