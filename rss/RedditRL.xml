<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 05 Oct 2024 21:15:56 GMT</lastBuildDate>
    <item>
      <title>适合初学者的超级简单教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fwpn0q/super_simple_tutorial_for_beginners/</link>
      <description><![CDATA[        提交人    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fwpn0q/super_simple_tutorial_for_beginners/</guid>
      <pubDate>Sat, 05 Oct 2024 12:49:03 GMT</pubDate>
    </item>
    <item>
      <title>在哪里训练 RL 代理（计算资源）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fwmtcg/where_to_train_rl_agents_computing_resources/</link>
      <description><![CDATA[嗨， 我对训练（较大的）RL 应用程序还比较陌生。我需要训练 12-15 个代理，以比较它们在 POMDP 问题（在金融领域 -&gt; 纯表格数据）上的表现，并在状态空间中对特定特征进行不同的表示。  我还没有开始训练，想知道在例如本地云架构上进行训练是否有意义。替代方案是配备 NVIDIA GeForce RTX 3060、4GB 的笔记本电脑。  我尝试提供尽可能多的有关潜在计算成本的信息：  状态空间由每个 t 的 10N+1 维组成，其中 N 是资产数量（如果这可以大致了解状态中的维度，我将主要使用 5-9 个资产）-&gt;所有维度都是连续的。一个时期由~1250个观测值组成 动作空间由 2N 个维度组成 -&gt; N 个维度的范围是 [-1,1]，其他 N 个维度的范围是 [0,1]。 我可能会使用某种 TD3 算法  我不知道这些信息是否足以得出计算出的意见，但是由于我对将 RL 应用于“更大”问题和管理计算约束还很陌生，因此非常感谢每一个提示/想法/讨论。    提交人    /u/Intelligent-Put1607   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fwmtcg/where_to_train_rl_agents_computing_resources/</guid>
      <pubDate>Sat, 05 Oct 2024 09:46:14 GMT</pubDate>
    </item>
    <item>
      <title>稳定的 Baselines3 回调函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fwmnh1/stable_baselines3_callback_function/</link>
      <description><![CDATA[嗨，我正在努力解决 Stable Baselines3 和评估过程的问题。代码不是我的，评估的回调是一个自定义函数，它将数据推送到权重和偏差 (WandB)。 evaluate_policy(model, env, n_eval_episodes=eval_episodes, callback=eval_callback) ... def eval_callback(result_local, result_global):  我的问题是：result_local 和 result_global 是什么？我尝试打印数据，但我只得到情节奖励或情节长度等总体指标。我如何访问所有奖励的列表来计算我自己的指标？ 感谢您的帮助。 干杯    提交人    /u/BitRa1n   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fwmnh1/stable_baselines3_callback_function/</guid>
      <pubDate>Sat, 05 Oct 2024 09:33:55 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 有没有什么有前景的研究可以利用 RL 从人类反馈中改进计算机视觉任务？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fw49i6/discussion_are_there_any_promising_work_on_using/</link>
      <description><![CDATA[  由    /u/Appropriate_Bear_894  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fw49i6/discussion_are_there_any_promising_work_on_using/</guid>
      <pubDate>Fri, 04 Oct 2024 17:06:16 GMT</pubDate>
    </item>
    <item>
      <title>（重复）没有自我注意力的前馈可以预测未来的标记吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/</link>
      <description><![CDATA[       由    /u/Timur_1988  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/</guid>
      <pubDate>Thu, 03 Oct 2024 21:43:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TD-MPC2 中没有循环模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fvckc1/why_no_recurrent_model_in_tdmpc2/</link>
      <description><![CDATA[我正在阅读 TD-MPC2 论文，我对整个想法非常了解。我唯一不太理解的是为什么潜在动力学模型是一个简单的 MLP，而不是像许多其他基于模型的论文中那样的循环模型。 主要问题是：潜在动力学模型如何一步一步地维持一个潜在表示 z，该表示结合了来自前一个时间步骤的信息，而没有任何隐藏状态。我猜他们测试的许多环境都需要这种能力，而且该算法似乎表现得非常好。 我的理解是，通过反向传播整个序列，潜在状态 z 仍然会从以下步骤接收梯度，因此潜在动力学模型可以隐式学习如何产生下一个潜在状态，该状态保留所有先前状态的信息。 但是，这不是效率低下吗？我很确定作者没有使用任何类型的序列模型（LSTM 等）是有原因的，但我似乎找不到令人满意的答案。你有吗？  论文链接     提交人    /u/fedetask   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fvckc1/why_no_recurrent_model_in_tdmpc2/</guid>
      <pubDate>Thu, 03 Oct 2024 17:01:38 GMT</pubDate>
    </item>
    <item>
      <title>Esquilax：大型多智能体 RL JAX 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fva0qn/esquilax_a_largescale_multiagent_rl_jax_library/</link>
      <description><![CDATA[我已经发布了Esquilax，一个多智能体模拟和 ML/RL 库。  它专为大规模多智能体系统（想想群体、羊群社交网络）的建模及其用作 RL 和其他 ML 方法的训练环境而设计。  它实现了常见的模拟和多智能体训练功能，减少了实现复杂模型和实验所需的时间和代码量。它还旨在与现有的 JAX ML 工具一起使用，例如 Flax 和 Evosax。 代码和完整文档可在以下位置找到： https://github.com/zombie-einstein/esquilax https://zombie-einstein.github.io/esquilax/ 您还可以在此处看到一个使用 Esquilax 将 boids 实现为 RL 环境的更大的项目    由   提交  /u/Familiar-Watercress2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fva0qn/esquilax_a_largescale_multiagent_rl_jax_library/</guid>
      <pubDate>Thu, 03 Oct 2024 15:09:22 GMT</pubDate>
    </item>
    <item>
      <title>您如何看待 Ben Recht 对强化学习极端主义者的这种批评？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fv5iy3/what_do_you_think_of_this_kind_of_critique_of/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fv5iy3/what_do_you_think_of_this_kind_of_critique_of/</guid>
      <pubDate>Thu, 03 Oct 2024 11:33:22 GMT</pubDate>
    </item>
    <item>
      <title>价值模型与流程奖励模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fut7r6/value_model_vs_process_reward_model/</link>
      <description><![CDATA[嗨，在 LLM 和 RLHF 的背景下，这两者有什么区别？ 据我了解，价值模型估计状态（或部分生成）的优劣，而 PRM 过程估计给定状态下动作的优劣？这使得 PRM 看起来有点像 Q 函数。 还有其他细微的差别吗？    提交人    /u/RingKitchen8808   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fut7r6/value_model_vs_process_reward_model/</guid>
      <pubDate>Wed, 02 Oct 2024 22:45:41 GMT</pubDate>
    </item>
    <item>
      <title>Pybullet 与 Google Brex 与 Mujoco</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fuhw1a/pybullet_vs_google_brex_vs_mujoco/</link>
      <description><![CDATA[我正在寻找 Pybullet、Google Brex、Mujoco 等优秀的物理模拟软件。它可用于强化学习任务。 这些是考虑的要点：  功能丰富 快速 支持 Ubuntu 支持 Jupiter Notebook - 意味着 RL 模型可以在笔记本中训练并渲染动作。 GUI 可用性  查看投票    提交人    /u/iNdramal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fuhw1a/pybullet_vs_google_brex_vs_mujoco/</guid>
      <pubDate>Wed, 02 Oct 2024 14:44:29 GMT</pubDate>
    </item>
    <item>
      <title>对表格 Q 学习实现的疑问</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fufqdw/doubt_about_implementation_of_tabular_qlearning/</link>
      <description><![CDATA[      我一直在复习 Q-learning 的知识。我正在检查以下实现： https://github.com/dennybritz/reinforcement-learning/blob/master/TD/Q-Learning%20Solution.ipynb 这是 Sutton 书中的伪代码： https://preview.redd.it/3v3o2e8cccsd1.png?width=1271&amp;format=png&amp;auto=webp&amp;s=86ec7a83efe6fbf563ea5835e7794035f49596d2 我不确定该实现中的策略。似乎即使 Q 函数在每个步骤后都会更新，策略也始终是固定的（因为它不在循环中）。它不应该在每次更新后（或至少在每集之后）更新吗？     由    /u/NavirAur 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fufqdw/doubt_about_implementation_of_tabular_qlearning/</guid>
      <pubDate>Wed, 02 Oct 2024 13:06:08 GMT</pubDate>
    </item>
    <item>
      <title>致力于可扩展多智能体强化学习——需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fuezif/working_on_scalable_multiagent_reinforcement/</link>
      <description><![CDATA[      您好， 我写这封信是为了寻求您的帮助。 我目前正在将强化学习应用于名为 CARLA 的自动驾驶模拟。 问题如下： https://preview.redd.it/v9k6ix2q7csd1.png?width=1923&amp;format=png&amp;auto=webp&amp;s=057074710bf54ce02c123734ff28a02a56ec8200  车辆在红色（主干道）和蓝色（合并道路）标记的区域中随机生成。 （仅使用主干道上的最后一条车道进行车辆生成。） 此时，有人驾驶的车辆（2 到 4 辆车）和强化学习代理控制的车辆（3 到 5 辆车）混合存在。 每集生成的车辆数量是随机的，并且在上面括号中指定的范围内。 生成位置也是随机的；它可能在主干道或合并道路上。 代理的操作如下： 油门：0 到 1 之间的值。 观察包括代理周围车辆（最多 4 辆车）的 x、y、vx 和 vy，按距离排序。 奖励结构简单：碰撞会导致 -200，0 到 80 公里/小时之间的速度值会产生 0 到 1 之间的奖励（80 公里/小时为 1，0 公里/小时为 0）。 如果任何代理发生碰撞或所有代理都到达目标（合并点后 100 米的点），则情节结束。  总之，任务是让代理安全地通过合并区域而不发生碰撞，即使代理的数量随机变化也是如此。 有什么资源可以参考吗？ 请提供我有一些建议。请帮帮我😢 我会很感激你的建议。 谢谢。    由   提交  /u/audi_etron   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fuezif/working_on_scalable_multiagent_reinforcement/</guid>
      <pubDate>Wed, 02 Oct 2024 12:28:52 GMT</pubDate>
    </item>
    <item>
      <title>TD3 在智能列车优化中的应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ftwjrn/td3_in_smart_train_optimization/</link>
      <description><![CDATA[我有一个模拟环境，火车可以在其中启动、加速和在车站停靠。但是，当使用 TD3 代理进行 1,000 集时，它很难掌握场景。我尝试调整超参数、奖励和神经网络层，但代理在测试期间仍采取类似的操作值。 在我的设置中，操作控制火车的加速度，具有距离、速度、到达车站的时间和模拟动作等特征。奖励函数采用各种指标设计，在开始时应用较大的惩罚，并在火车接近目标时减少惩罚以激励前进。 我将原始数据传递给策略而没有进行规范化。这个问题可能与奖励结构、模型本身有关，还是我应该考虑添加其他功能？    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ftwjrn/td3_in_smart_train_optimization/</guid>
      <pubDate>Tue, 01 Oct 2024 19:20:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 构建算法交易代理的教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fthite/tutorial_on_using_rl_to_build_algo_trading_agent/</link>
      <description><![CDATA[https://www.aion-research.com/post/building-a-reinforcement-learning-agent-for-algorithmic-trading 这是一个简化的示例，因此请勿将其用于您的实际交易。我还没有能够将 RL 应用于我的真实量化金融工作，所以如果有人之前成功过，请告诉我！    提交人    /u/MarketMood   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fthite/tutorial_on_using_rl_to_build_algo_trading_agent/</guid>
      <pubDate>Tue, 01 Oct 2024 06:24:26 GMT</pubDate>
    </item>
    <item>
      <title>强化学习在线讲座</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ft5e8u/online_lectures_on_reinforcement_learning/</link>
      <description><![CDATA[大家好，我想与大家分享我在 YouTube 上关于强化学习的讲座：   https://www.youtube.com/playlist?list=PLW4eqbV8qk8YUmaN0vIyGxUNOVqFzC2pd   每周三和周日早上都会发布新的视频。您可以订阅我的YouTube频道（https://www.youtube.com/tyucelen）并开启通知以随时关注！如果您能将这些讲座转发给您的同事/学生，我将不胜感激。   以下是要涵盖的主题：    强化学习简介（已发布） 马尔可夫决策过程（已发布） 动态规划（已发布） Q 函数迭代 Q 学习 带有 Matlab 代码的 Q 学习示例 SARSA 带有 Matlab 代码的 SARSA 示例 神经网络 连续空间中的强化学习 神经 Q 学习 带有 Matlab 代码的神经 Q 学习示例 神经 SARSA 带有 Matlab 代码的神经 SARSA 示例 经验重播 运行时保证 带有 Matlab 代码的 Gridworld 示例  祝一切顺利， Tansel Tansel Yucelen，博士 自主、控制、信息和系统实验室主任（LACIS) 机械工程系副教授 南佛罗里达大学，美国佛罗里达州坦帕市 33620 X，领英, YouTube, 770-331-8496 (手机)    提交人    /u/Original-Promise-312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ft5e8u/online_lectures_on_reinforcement_learning/</guid>
      <pubDate>Mon, 30 Sep 2024 20:16:21 GMT</pubDate>
    </item>
    </channel>
</rss>