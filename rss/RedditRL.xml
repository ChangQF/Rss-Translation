<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 29 Jan 2024 21:11:52 GMT</lastBuildDate>
    <item>
      <title>长期内奖励稀疏的困难</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ae4ps2/difficulty_with_sparse_reward_over_long_time/</link>
      <description><![CDATA[我有一个非常简单的道路网络设置。有一个信号可以向左或向右发送汽车。因此，动作空间只是大小为 2 的离散空间。观察空间约为 4000，其形式为：spaces.Box(low=-1, high=1, shape=(num_cells,), dtype=np.float32 ) 代表道路网络中某个单元格的占用情况（这是一个元胞自动机类型的东西，汽车只是向前移动） 奖励就是在路径的正确末端退出的汽车，它固定在右边。所以它所要做的就是把车送到右边并收集它的甜蜜奖励。但只有在 300 个时间步之后，当它到达路的尽头时！ 我不能让它工作，ARRRRRG 我正在使用 rllib。我尝试过 APPO、IMPALA、PPO、DQN，运行了不同超参数的负载，学习率从 0.001 到 0.00001，伽马值提升至0.999。  它不会学习，帮帮我，我已经在这上面花了一个星期了，快被淹死了。 我已经训练了 15M 时间步，但没有任何乐趣。 Am我做了一些根本错误的事情吗？这应该有效吗？或者奖励距离行动太晚了？我什至尝试将 rollout_fragment_length 提高到奖励的时间范围，因为我不清楚短于奖励周期的碎片是否是这些并行算法的问题？ 请帮忙......&lt; /p&gt;   由   提交 /u/memebox2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ae4ps2/difficulty_with_sparse_reward_over_long_time/</guid>
      <pubDate>Mon, 29 Jan 2024 20:04:33 GMT</pubDate>
    </item>
    <item>
      <title>多机器人强化学习模拟器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ae3e91/simulators_for_multirobot_reinforcement_learning/</link>
      <description><![CDATA[哪种模拟器最适合具有模拟到真实可迁移性的多机器人强化学习？  &lt; /div&gt;  由   提交 /u/anointedninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ae3e91/simulators_for_multirobot_reinforcement_learning/</guid>
      <pubDate>Mon, 29 Jan 2024 19:10:56 GMT</pubDate>
    </item>
    <item>
      <title>将知识从评论家网络转移/共享到演员网络背后的直觉是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ae0iox/what_is_the_intuition_behind_transferringsharing/</link>
      <description><![CDATA[标准 PPO 算法对于参与者和批评者都有一个单一的网络，分别有两个输出头用于策略和值。  在Cobbe 等人中。 (2021) [阶段性政策梯度] 和 Aitchison, Sweetser (2022) [PPO-DNA] 作者认为拥有联合网络不利于具有基线的策略梯度算法的性能。相反，他们建议必须将经过不同时期数（以及偏差/方差程度）独立训练的网络分开。然而，他们的行动者网络仍然有一个额外的价值头，分别在辅助或蒸馏阶段进行优化（在约束下）。 他们指出，有知识可以从价值函数转移到价值函数政策（他们表明这实际上提高了算法的性能）。  我想知道该声明背后的直觉。一个对特定状态下的预期（折扣）回报进行估计的函数如何为该状态下要采取的最佳行动提供信息？ 为了更容易理解，让我们想象一个小例子：我的经纪人沿着赛道驾驶一辆汽车。她的评论家网络提供了有关该赛道中的位置相对于她未来预期回报的估计优度的信息。参与者网络规定了方向盘的角度和加速度。在辅助/蒸馏阶段，有利于改善政策的头寸的预期回报信息如何？机制或思路是什么？   由   提交 /u/Tortoise_vs_Hare   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ae0iox/what_is_the_intuition_behind_transferringsharing/</guid>
      <pubDate>Mon, 29 Jan 2024 17:15:38 GMT</pubDate>
    </item>
    <item>
      <title>标准化值函数输出</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1adsbjm/normalizing_value_function_output/</link>
      <description><![CDATA[我正在对价值函数错误的折扣回报进行标准化。我无法从神经网络输出大值。我没有在互联网上找到任何有关此问题的论文或视频，令我惊讶的是没有那么多人遇到与我相同的问题。这仅适用于价值神经网络。我听说过采用标准差等等，但我应该将其应用于每个奖励吗？那岂不是意味着每项奖励都基本等价？而且不同的时间步长在未来会有不同的奖励，因为获得奖励的时间步长较少。他们的问题太多了，我不知道该怎么办，我想回顾一下如何获取值函数的错误。  &amp;# 32；由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1adsbjm/normalizing_value_function_output/</guid>
      <pubDate>Mon, 29 Jan 2024 10:34:30 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡洛树搜索：奖励函数和启发函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1adadk5/monte_carlo_tree_search_reward_function_and/</link>
      <description><![CDATA[在每次模拟中的 MCTS 中，您都会遍历搜索树，直到选择一个导致节点（表示状态）的操作，而该节点不存在于搜索树。然后，您添加该新节点（=新状态），并可以将启发式函数应用于新状态，该新状态利用领域知识并访问状态的“好程度”。该值被反向传播到根节点。在反向传播期间，沿路径的动作的奖励在计算状态值时发挥作用。如果反向传播同时使用奖励和启发式函数计算的值，那么这是否会对我认为难以管理的这两个函数施加设计约束？下面是一个简单（夸张）的例子来解释我所说的“设计约束”的含义：想象一下奖励函数计算 0 到 1 之间的值，启发函数计算 100,000 到 1,000,000 之间的值。在这种情况下，启发式函数可以完全主导状态值的计算。当然，这个例子是这两个函数的愚蠢设计。但我想，在实际应用中，要表达“适量的好坏”可能并不容易。通过启发式函数，与奖励函数的设计方式很好地平衡！？   由   提交 /u/m_jochim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1adadk5/monte_carlo_tree_search_reward_function_and/</guid>
      <pubDate>Sun, 28 Jan 2024 19:12:17 GMT</pubDate>
    </item>
    <item>
      <title>RSL 最新出版物“DTC：深度跟踪控制”中的实验幕后视频</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad7tyi/behindthescenes_videos_of_experiments_from_rsls/</link>
      <description><![CDATA[   /u/leggedrobotics   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad7tyi/behindthescenes_videos_of_experiments_from_rsls/</guid>
      <pubDate>Sun, 28 Jan 2024 17:27:08 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad3v3s/i_need_some_advice/</link>
      <description><![CDATA[嗨，我是 RL 新手，我正在尝试在自定义环境上训练代理。我正在使用 SB3，对于环境我使用 PyBullet。代理是一辆有四个轮子的汽车，应该接触一个立方体。 观察空间看起来像这样Box(low=0, high=255, shape=(4, 64, 64), dtype=np.uint8)&lt; /strong&gt; （这只是从汽车上的摄像头捕获的图像）和 动作空间 像这样的 Box(low=-10, high=10,形状=（4，），dtype=np.float32）。我尝试了多种算法但没有成功。我可以尝试模仿学习，但我不知道如何将我的输入保存为专家数据。有人可以给我提示吗？ 编辑：这是代码 &lt; /div&gt;  由   提交 /u/SebyR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad3v3s/i_need_some_advice/</guid>
      <pubDate>Sun, 28 Jan 2024 14:30:23 GMT</pubDate>
    </item>
    <item>
      <title>我如何在强化学习领域建立人际网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad36a2/how_do_i_network_in_the_reinforcement_learning/</link>
      <description><![CDATA[我在计算机视觉方面拥有丰富的经验，例如：  图像处理 &lt; li&gt;分割 分类  ​ 我决定使用强化学习，而且我很幸运我获得了很好的 CS教授作为我的顾问，我希望能够与业界人士建立联系。  我该怎么办？我觉得社交就是交流，但由于我是一个完全的初学者，我觉得我没有太多贡献，因此，我没有太多社交的机会。  我的网络目标是学习行业技巧，可能找到行业导师，或者只是可以为我提供任何类型的个人或研究反馈的人。  P.S.：我已经毕业很多年了，强化学习在我国的研究前景中并不是一个强项。  谢谢！如果你们中的一些人愿意与我分享一些你们在网络方面的经验，我将非常感激！   由   提交/u/pandaswontlie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad36a2/how_do_i_network_in_the_reinforcement_learning/</guid>
      <pubDate>Sun, 28 Jan 2024 13:55:43 GMT</pubDate>
    </item>
    <item>
      <title>PPO 与 SAC 中的动作剪辑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad2fzp/action_clipping_in_ppo_vs_sac/</link>
      <description><![CDATA[我注意到，在 SAC 中，您通常对高斯分布应用 tanh 变换，其中您的 Applied_actions = tanh( a ~ N(net(states), std（状态）））。在流行的 PPO 实现中，我经常遇到未绑定的网络输出和应用操作上的标准剪辑，即 Applied_actions = Clip( a ~ N(net(states),std) min, max )。更新 actor 时，梯度通过 tanh 激活流回。 奇怪的是，在 PPO 的情况下，网络不知道裁剪（通常它应用于环境中，因此它确实会影响奖励）但不是存储的操作），但它似乎仍然工作得很好。有谁知道为什么为 PPO 选择这种设计，以及为什么它看起来仍然运行良好？   由   提交 /u/IgneousPutorius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad2fzp/action_clipping_in_ppo_vs_sac/</guid>
      <pubDate>Sun, 28 Jan 2024 13:16:22 GMT</pubDate>
    </item>
    <item>
      <title>我有一个关于强化学习中时间相关性问题的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad2e5a/i_have_a_question_about_the_issue_of_temporal/</link>
      <description><![CDATA[我一直在学习强化学习，并试图了解样本之间时间相关性的影响。我知道这会使学习不稳定，但我不清楚为什么。是不是因为只针对某些情况计算梯度，导致学习存在偏差和不稳定？ 我的另一个问题是，在我正在阅读的 RL 书的 PG 部分中，它说使用return（REINFORCE）的策略梯度方法，样本之间的相关性不是问题，因为更新是使用return完成的，这是总奖励，所以没有必要使用重播缓冲区，对吗？ 我知道A2C算法是一种在线学习方法，它使用Q函数而不是返回更新每一步，但这会导致样本之间的相关性问题吗？如果是的话，那么使用 return 的 REINFORCE 是否具有离线 RL 的特征？   由   提交/u/DRLC_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad2e5a/i_have_a_question_about_the_issue_of_temporal/</guid>
      <pubDate>Sun, 28 Jan 2024 13:13:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 DQN 模型无法学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad0pgt/why_does_my_dqn_model_not_learn/</link>
      <description><![CDATA[大家好！ 我是强化学习新手，关注了 这个教程非常仔细。 但是，我的代理似乎根本没有学习......！事实上，80后，它只是绕着同一个地方旋转。我想知道为什么在我的案例中会发生这种情况，而在教程中，代理获得了非常高的分数，远远高于 1。 这是我的代码。 如果有任何帮助，我们将不胜感激：D 非常感谢！ 编辑： 日志 情节   由   提交 /u/Rainbowusher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad0pgt/why_does_my_dqn_model_not_learn/</guid>
      <pubDate>Sun, 28 Jan 2024 11:30:24 GMT</pubDate>
    </item>
    <item>
      <title>沙发火炬加上基线</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ad07gb/couching_torch_plus_baselines/</link>
      <description><![CDATA[嗨。我热衷于股票预测，并且是一名全职开发人员。我最近进入了稳定、变压器等要求较高的领域。这个领域充满了陷阱，我同时接触了很多新事物，如果有人能给我提供有偿咨询，我们会尝试调试我的项目，帮助我找到更快发现问题的方法，那就太好了，总的来说加快了我的经验收集速度。请PM我。   由   提交/u/doker0  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ad07gb/couching_torch_plus_baselines/</guid>
      <pubDate>Sun, 28 Jan 2024 10:56:48 GMT</pubDate>
    </item>
    <item>
      <title>AI足球世界杯刚刚开始！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1acopua/the_ai_soccer_world_cup_has_just_started/</link>
      <description><![CDATA[    /u/whoami_ai   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1acopua/the_ai_soccer_world_cup_has_just_started/</guid>
      <pubDate>Sat, 27 Jan 2024 23:51:49 GMT</pubDate>
    </item>
    <item>
      <title>涉及人类反馈的强化学习研究领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1acoirx/research_area_in_rl_that_involve_human_feedback/</link>
      <description><![CDATA[大家好，我正在作为一名本科生进行研究，目的是完成论文（在两个学期内）。与我一起工作的教授和我正在讨论 RL 中将用于 AV 的方法。我们讨论了该模型可以使用模仿学习、轨迹明智和人类反馈作为在不一定有可用奖励政策的情况下制定功能奖励的方法。他提到前两种方法已经被研究了很多，但 HF 很热门并且即将到来。  我很高兴接受利用 RLHF 的研究/论文主题和想法。我可以研究什么想法吗？谢谢:)   由   提交 /u/--indubitously   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1acoirx/research_area_in_rl_that_involve_human_feedback/</guid>
      <pubDate>Sat, 27 Jan 2024 23:42:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么随机网络蒸馏选择鼓励状态探索而不是状态动作探索？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1abzszi/why_does_random_network_distillation_choose_to/</link>
      <description><![CDATA[我猜他们确实尝试过，发现情况更糟，尽管他们没有写任何相关内容。只是好奇是否有人对此有任何见解。我计划很快实现 RND，这听起来像是一个简单的修改。   由   提交 /u/JustTaxLandLol   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1abzszi/why_does_random_network_distillation_choose_to/</guid>
      <pubDate>Sat, 27 Jan 2024 02:14:23 GMT</pubDate>
    </item>
    </channel>
</rss>