<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 09 Jul 2024 12:29:31 GMT</lastBuildDate>
    <item>
      <title>为什么与离策略算法相比，状态表示学习方法（通过辅助损失）较少应用于 PPO 等在线策略 RL 算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</link>
      <description><![CDATA[我已经看到了不同的状态表征学习方法（通过辅助损失，无论是自我预测还是基于结构化探索），它们已经与离线策略方法（如 DQN、Rainbow、SAC 等）一起应用。例如，SPR（自我预测表征）已与 Rainbow 一起使用，CURL（强化学习的对比无监督表征）已与 DQN、Rainbow 和 SAC 一起使用，以及RA-LapRep（通过拉普拉斯表征进行表征学习）已与 DDPG 和 DQN 一起使用。我很好奇为什么这些方法没有像 PPO（近端策略优化）这样的在线策略算法得到广泛应用。将这些表示学习技术与在线策略算法学习相结合是否存在理论问题？    提交人    /u/C7501   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</guid>
      <pubDate>Tue, 09 Jul 2024 10:56:37 GMT</pubDate>
    </item>
    <item>
      <title>有没有任何具有随机性的离线 RL 基准？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</link>
      <description><![CDATA[我正在研究风险敏感的离线强化学习，发现许多众所周知的基准并不合适，因为它们缺乏随机性。Mujoco 环境几乎是确定性的；它们的内部代码不包含任何随机性，除了在“重置”期间阶段。 [参见：https://arxiv.org/abs/2205.15967\]  我发现 NeoRL 也是如此 [https://arxiv.org/abs/2102.00714.\] 您可以轻松验证运行 TD3PlusBC 时没有差异。 那么，有针对具有随机性的离线 RL 的基准吗？    提交人    /u/korsyoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</guid>
      <pubDate>Tue, 09 Jul 2024 10:09:56 GMT</pubDate>
    </item>
    <item>
      <title>强化学习代理没有采取现实行动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyxh40/reinforcement_learning_agent_not_taking_realistic/</link>
      <description><![CDATA[我在 Simulink 环境中使用 PPO 代理，但代理产生的操作似乎是离散的。具体来说，代理仅输出上限或下限。您知道为什么会发生这种情况吗？我使用 RL Toolbox 进行训练。以下是有关我的设置的一些详细信息：  我使用带有 ode23t 求解器的可变步长时间 Simulink 模型。 我的 Simulink 模型使用 Simscape 热流体库并模拟简化的区域供热网络。DHN 有 2 个分支：北 (NORD) 和南 (SUD)。 我尝试使用 RL 代理来优化控制，最初专注于通过更改分支中的质量流量来最大限度地降低能源成本。   关于代理的超参数，我使用的是具有以下参数的 RL 工具箱：   采样时间 = 3600  折扣因子 = 0.99  GPU  批次大小 = 512  学习率 = 1e-3（对于演员和评论家）   我怀疑我的模型或代理可能有问题。 我将附上 Simulink 模型（应事先加载属性表）。 我希望问题清楚，并且有人可以提供帮助！ 提前谢谢您！    提交人    /u/Resident_Wish9453   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyxh40/reinforcement_learning_agent_not_taking_realistic/</guid>
      <pubDate>Tue, 09 Jul 2024 08:42:43 GMT</pubDate>
    </item>
    <item>
      <title>如何处理3D体素观察？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</link>
      <description><![CDATA[我目前在使用 3D 体素状态训练 PPO 时遇到了困难。 3D 体素的形状为 [64, 128, 128]，区域信息很重要。 只使用 3D CNN 编码器可以吗？ 我是强化学习的新手，我还没有看到任何使用 3D 编码器的论文，而且大多数 RL 教程都使用 2d CNN 编码器或仅使用 MLP     提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</guid>
      <pubDate>Tue, 09 Jul 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>RLHub：强化学习环境的统一平台 - 寻求反馈！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</link>
      <description><![CDATA[嗨，我的 RL 伙伴们！ 我是加州大学伯克利分校的博士后，和几个朋友一起在开展一个名为 RLHub 的新项目，我很想听听你们的想法。我们的想法是创建一个标准化的强化学习环境平台，类似于 Hugging Face 为 NLP 模型所做的工作。 主要功能：1. 适用于各种 RL 环境（mujoco、unity、gym 等）的统一 API 2. 轻松上传和共享自定义环境 3. 自动依赖项管理 4. 本地和云执行选项 5. 标准化元数据和文档 可能的附加功能：- 标准化主要算法（PPO、DDPG、TD3……）UI，用于在云端训练代理 目标是简化查找、使用和共享 RL 环境的过程。研究人员可以轻松地在多种环境中尝试他们的算法，环境创建者可以接触到更广泛的受众。 我有一些问题：1. 这对您的工作有用吗？2. 您会优先考虑哪些功能？3. 对标准化有什么顾虑？4. 关于在 MVP 中包含云执行的想法？ 我特别想听听 RL 研究人员和从业人员的意见。您对当前 RL 环境管理有哪些痛点，可以解决这些痛点吗？ 提前感谢您的任何反馈！    提交人    /u/elonmusk-A12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</guid>
      <pubDate>Tue, 09 Jul 2024 03:03:51 GMT</pubDate>
    </item>
    <item>
      <title>神经网络调试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyjkv6/neural_network_debugging/</link>
      <description><![CDATA[大家好， 我知道神经网络调试的基础知识。但我想知道是否有人可以分享在训练、测试和生产阶段进行调试的技巧。我相信这在这里会非常有帮助。    提交人    /u/MuscleML   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyjkv6/neural_network_debugging/</guid>
      <pubDate>Mon, 08 Jul 2024 20:43:56 GMT</pubDate>
    </item>
    <item>
      <title>Rnd 与 rnn 网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy8g0z/rnd_with_rnn_networks/</link>
      <description><![CDATA[我有带有 rnn 网络的 Ppo 用于策略和参与者。我想添加像 rnd 这样的好奇心机制，我想知道目标和预测网络是否也应该包括 rnn……有人有这方面的经验吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy8g0z/rnd_with_rnn_networks/</guid>
      <pubDate>Mon, 08 Jul 2024 13:05:17 GMT</pubDate>
    </item>
    <item>
      <title>我到底该怎么做？当操作无法影响即将到来的状态时会出现问题……</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy6y4d/what_exactly_should_i_do_problem_when_actions/</link>
      <description><![CDATA[您好， 考虑一个非常简单的玩具问题。您有一辆汽车，它沿 x 轴移动。汽车以一定的起始速度在 1 维中移动。您的汽车有 2 个刹车，一旦打开刹车，就无法收回。这意味着：如果您打开第一个刹车，就无法再次关闭它，您已经失去了机会。如果您在错误的时间打开第一个刹车而犯了一个错误，那么您必须小心，在最佳时间打开第二个刹车。 环境：  状态：&lt;位置、速度、刹车状态 (0：无、1：刹车\_1 打开、2：刹车\_2 打开)&gt; 操作：0 或 1 (当前刹车状态 += 操作 --&gt; 根据当前状态添加操作) 奖励 = - (最后位置 - 目标位置) ^ 2 起始速度 = 10，起始位置 = 0，目标位置 = 55 如果打开，第一个刹车接合：速度 -= 1 如果打开，第二个刹车接合：速度-= 2 情节结束 -&gt; （如果位置 &gt; 65 或速度 &lt; 0）  -&gt; 解决方案是：在第 3 步打开第 1 个制动器，在第 4 步打开第 2 个制动器 10 + 10 + 10（在此处打​​开 Brake_1）+ 9（在此处打​​开 Brake_2）+ 7 + 5 + 3 + 1 = 55 我的问题： 打开 Brake_2 后，操作将不再产生任何效果。我的意思是：&lt;State, Action=0, Same\_Reward, Same\_Next\_State&gt;，&lt;State, Action=1, Same\_Reward, Same\_Next\_State&gt;。无论代理尝试采取什么行动，它都会进入相同的 next_state 并获得相同的奖励。基本上，代理已经失去了改变即将到来的状态的能力。 如果我添加一个由关闭中断组成的机制，代理将继续具有塑造即将到来的状态的能力，这意味着塑造汽车的最后位置直到情节结束。这已经奏效，代理能够找到正确的动作组合。动作基本上是可逆的。 如果我创建如上所述的环境，由于 Break_2 之后，保持对汽车速度的控制的能力将消失，代理开始努力解决问题。动作基本上是不可逆的。 总而言之，如果在某些事件之后动作对下一个状态和奖励实际上没有影响，我该怎么办？忽略中间的 &lt;状态、动作、奖励、下一个状态、完成&gt;，等到情节结束，并将最后一个状态和奖励与 Break_2 打开的 previous_state 相结合？    提交人    /u/OpenToAdvices96   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy6y4d/what_exactly_should_i_do_problem_when_actions/</guid>
      <pubDate>Mon, 08 Jul 2024 11:51:32 GMT</pubDate>
    </item>
    <item>
      <title>创建街头霸王 II：世界战士 AI 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy5fjt/creating_a_street_fighter_ii_the_world_warrior_ai/</link>
      <description><![CDATA[是否可以用 Python 在 GymRetro 或 StableRetro 中玩游戏？如果可以，我是否可以上传自己的游戏方式（按下按钮）以用于训练我自己的 AI 模型。非常感谢！    提交人    /u/More-Background-1626   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy5fjt/creating_a_street_fighter_ii_the_world_warrior_ai/</guid>
      <pubDate>Mon, 08 Jul 2024 10:21:46 GMT</pubDate>
    </item>
    <item>
      <title>不同种子的 SAC 性能存在差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy0t9m/variability_in_performance_of_sac_from_seed_to/</link>
      <description><![CDATA[您好！我目前正在尝试为离散动作空间实现 SAC，以便在 OpenAI LunarLander 环境中使用。然而，在训练过程中，我遇到了代理在一个种子上表现良好，但在另一个种子上（具有相同的超参数）表现较差的问题。我该如何解决这个问题？任何帮助都将不胜感激，因为我已经为此奋斗了几个月。 代码 Alpha/Actor Loss Alpha 值和情景奖励 Q 函数损失 每个彩色图表代表一次训练运行，它们之间的唯一区别是不同的种子。    提交人    /u/Tight_Apple_678   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy0t9m/variability_in_performance_of_sac_from_seed_to/</guid>
      <pubDate>Mon, 08 Jul 2024 05:13:36 GMT</pubDate>
    </item>
    <item>
      <title>纯探索中的顺序减半算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dxqs6v/sequential_halving_algorithm_in_pure_exploration/</link>
      <description><![CDATA[      在 Tor Lattimore 和 Csaba Szepsvari 的书的第 33 章中 https://tor-lattimore.com/downloads/book/book.pdf#page=412 他们展示了顺序减半算法，如下图所示。我的问题是为什么在第 6 行我们必须忘记来自其他迭代 $l$ 的所有样本？我试图实现这个算法，记住上次运行中采样的样本，效果很好，但我不明白算法中提到的忘记过去迭代中生成的所有样本的原因。 https://preview.redd.it/ufmxz837u5bd1.png?width=1275&amp;format=png&amp;auto=webp&amp;s=87a37f7eadb3fc9faf70d1423b5998289765cb34    由    /u/VanBloot  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dxqs6v/sequential_halving_algorithm_in_pure_exploration/</guid>
      <pubDate>Sun, 07 Jul 2024 20:59:16 GMT</pubDate>
    </item>
    <item>
      <title>将 LLM 环境转换为基于文本的环境；还有更多示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dxo8nd/converting_environments_to_textbased_for_llms_any/</link>
      <description><![CDATA[https://github.com/histmeisah/Large-Language-Models-play-StarCraftII 可能只需将我的环境转换为基于文本即可尝试类似操作。还有其他类似的例子吗？我依稀记得几年前的决策转换器，但从那以后就再也没有见过它们。    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dxo8nd/converting_environments_to_textbased_for_llms_any/</guid>
      <pubDate>Sun, 07 Jul 2024 19:10:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 MuZero 训练现代桌面游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dx15bv/using_muzero_to_train_modern_tabletop_games/</link>
      <description><![CDATA[嗨。我在一家棋盘游戏公司（捷克游戏版）工作，我对 AI 玩现代棋盘游戏非常感兴趣。我想看看 MuZero 是否是一个值得尝试的好选择。我读过很多文章和教程，探索过 MuZero 的几种实现，包括 C++ 变体和 OpenSpiel。我只找到了相对简单的游戏。我试图回答几个问题： 1) 我可以用现有的计算能力训练一款普通的现代棋盘游戏吗？即使使用普通 GPU 几天/几周，我也能得到结果吗？ 2) 如何从棋盘、几个令牌库以及几个玩家和全局单数指针创建观察空间？ 3) 如果我想将特定数字而不是随机数放入推理中以测试 AI 与现场玩家之间的对抗，那么它应该是一个新的虚拟玩家（“游戏管理员”）吗？ 4) 如果游戏板发生变化，但在一个游戏会话中保持不变，我可以将其“形状”添加到观察空间，以便 AI 能够玩其他类似的棋盘吗？ 到目前为止，对于我们的游戏，我们一直非常成功地使用简单的 MCTS，仅用于确定单个玩家移动中的动作（我们不模拟对手的动作）。我们没有使用神经网络的经验。我觉得我们可以创造一个更强大的 AI 对手，所以我正在学习和探索。    提交人    /u/damucz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dx15bv/using_muzero_to_train_modern_tabletop_games/</guid>
      <pubDate>Sat, 06 Jul 2024 22:16:03 GMT</pubDate>
    </item>
    <item>
      <title>新的（更具数学性的）强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</link>
      <description><![CDATA[嗨  我已经参加了强化学习课程，我认为我对这些概念有很好的掌握，但正在寻找强化学习领域算法开发的当前前沿 有什么算法 / 主题的想法可以让我开始吗？    提交人    /u/Total-Ad-4461   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 17:09:36 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 制作一款回合制策略游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</link>
      <description><![CDATA[        由    /u/Novel_Can_6870  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</guid>
      <pubDate>Sat, 06 Jul 2024 02:23:55 GMT</pubDate>
    </item>
    </channel>
</rss>