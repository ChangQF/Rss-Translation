<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Thu, 27 Feb 2025 03:27:42 GMT</lastBuildDate>
    <item>
      <title>跨领域的凉爽自我校正机制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  从控制理论的反馈循环和卡尔曼过滤到自然选择，DNA修复，多数投票和引导 - 无数的方式系统自我校正错误，尤其是当地面真理未知时！生物学，人工智能，统计甚至进化似乎都融合了机制以遏制误差传播。想知道您遇到的有趣的自我纠正机制是什么，无论是自然界，哲学，工程还是以后？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neat_comparison_2726      [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</guid>
      <pubDate>Wed, 26 Feb 2025 19:48:15 GMT</pubDate>
    </item>
    <item>
      <title>现在，您可以使用GRPO（5GB VRAM最小值）训练自己的推理模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，很棒的人！第一篇文章在这里！今天，我很高兴地宣布，您现在可以使用grpo +使用Grpo +我们的开放式项目unsploth使用5GB VRAM训练自己的推理模型： https：&gt; https：&gt; https：&gt;是DeepSeek-R1背后的算法以及如何受过训练。它比PPO更有效，我们设法将VRAM使用降低了90％。您需要一个大约500行，答案对和奖励功能的数据集，然后可以启动整个过程！ 这允许将任何开放的LLM（如Llame，Misstral，Phi等）等开放，可以将其转换为具有链链过程的推理模型。关于GRPO的最好的部分是，与更大的型号相比，与更大的训练时间相比，与较大的训练时间相比，与较大的型号相比，训练小型型号与较大的型号无关紧要，因此最终结果将非常相似！您也可以在执行其他操作的同时，在PC的背景下进行GRPO培训！  由于我们新添加的有效的GRPO算法，这使得 10x更长的上下文长度长度  90％使用 90％的vram  vram/strong&gt; vraM/strong&gt; lora/qlora/qula li&gt; li afteraive  li&gt; field afteraiment &lt;0&gt; 标准GRPO设置，Llama 3.1（8b）20K上下文长度的培训需要510.8GB的VRAM。但是，Unsploth的90％VRAM减少的要求使同一设置中的需求仅为54.3GB。 我们利用我们的渐变”检查 algorithm，我们发布了一个aLgorithm。它可以巧妙地将中间激活卸载到系统RAM异步，同时仅慢1％。此剃须372GB VRAM ，因为我们需要num \ _ generations = 8。我们可以通过中间梯度累积进一步减少此内存使用。 使用Google的免费上下文使用我们的GRPO Notebook，使用Google的免费gpus： href =“ https://colab.research.google.com/github/unslothai/notebooks/blob/blob/main/nb/llama3.1_(8B”&gt; llama 3.1（8b）on colab  -grpo.ipynb）以及更多： align =“ left”&gt; metric   unsploth   trl + fa2           training Moregre Cost（GB） align =“左”&gt; 414GB      grpo内存成本（gb）   9.8gb    78.3gb  78.3gb  78.3gb    0gb   16gb      推理20K上下文（GB）   2.5GB  2.5gb  总内存使用   54.3GB（少90％）       510.8GB              我们在所有方面都花了很多时间（pboty&gt;  ：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/yoracale     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</guid>
      <pubDate>Wed, 26 Feb 2025 19:41:48 GMT</pubDate>
    </item>
    <item>
      <title>策划可塑性损失的论文清单</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨， 我已经创建了一个存储库，其中包含有关可塑性损失的论文列表。重点是深度RL，但是那里也有一些持续的学习。  https：//github.com/github.com/github.com/probabilistic--interactive-mlaw---interactive-mlaw yourplastive yourplastive plapery plapery plapery  我们还在撰写有关该主题的调查，但仍处于早期阶段：很多牵引力，我希望这可以帮助人们加快速度：）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/timo_kk     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</guid>
      <pubDate>Wed, 26 Feb 2025 16:40:39 GMT</pubDate>
    </item>
    <item>
      <title>以非常折扣价的困惑pro</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  任何有兴趣以50％折扣价获得困惑Pro的人，请与我联系  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/beast_of_iit    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iykcbt/perplexity_pro_at_a_very_very_very_very_very_very_very_discounted_price/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</guid>
      <pubDate>Wed, 26 Feb 2025 10:15:47 GMT</pubDate>
    </item>
    <item>
      <title>RL代理当前在不激励特定行为的情况下最佳执行的最复杂环境是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我很想知道sota在环境复杂性方面，在不需要任何中级奖励的情况下，RL代理执行的性能 - 只是+1 +1 for“ win”和-1为“损失”   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]     32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 07:34:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么某些环境（例如Minecraft）太困难了，而另一些环境（例如Openai's Hide N See Seek）是可行的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   tldr：是什么让hide n寻求可解决的环境，但是我很难解决的我的minecraft或简化的Minecraft环境？ 我没有遇到任何RL代理在Minecraft中成功生存的任何RL代理。理想情况下，如果根据代理商的活力来给予奖励，它至少应该为食物建立庇护所和农场。 ， ，Openai的hide n of from 5年前从5年前开始寻求视频，从划痕中，在那个环境中学到了很多东西，甚至没有激励任何行为。为什么不适用于Minecraft？有一个更容易的环境称为手工艺者，但即使是这样的奖励似乎是这样设计的，以至于最佳行为被激励，而不仅仅是基于生存的奖励，而最佳绩效（Dreamer）仍然没有与人类的绩效相比。 是什么让hide n寻求可解决的环境，但可以解决，但可以解决，但是是如此，但是如此难以解决的或简化的Minecraft环境，以求解Minecraft solve？提交由＆＃32; /u/aliaslight     [link]   [注释] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</guid>
      <pubDate>Wed, 26 Feb 2025 05:29:58 GMT</pubDate>
    </item>
    <item>
      <title>使用深RL的自我标记汽车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iya5jf/selfparking_car_using_deep_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想训练一个PPO型号以并行停车，可成功地将车停在汽车上。你们知道我可以为此目的使用的任何模拟环境吗？另外，训练这样的模型会很长吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iya5jf/selfparking_car_used_usis_deep_rl/​​”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iya5jf/selfparking_car_using_deep_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 00:12:17 GMT</pubDate>
    </item>
    <item>
      <title>事后经验重播（她）表现的主要贡献者是什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</link>
      <description><![CDATA[Hello, I have been studying Hindsight Experience Replay (HER) recently, and I’ve been examining the mechanism by which HER significantly improves performance in sparse reward environments. In my view, HER enhances performance in two aspects:  Enhanced Exploration:  In sparse reward环境，如果代理未能达到最初的目标，它几乎无法获得任何奖励，导致缺乏学习信号并迫使代理继续随机进行随机探索。 她通过使用最终状态作为目标来重新定义目标，使代理人可以通过实际上可以通过该过程来实现各个国家的启用。     策略概括：  她将目标与国家一起融入了网络的输入中，使政策能够有条件地学习，以实现州和规定的目标。  因此，通过捕获各种目标之间的关系并没有直接实现的目标，从某种程度上来实现。       在这些要点上，我很好奇，我在哪些因素上 - 探索或政策普遍性地如果状态空间为r  2 &lt; /sup&gt;，目标是（2,2），但是代理人恰好仅在第二个象限内探索，则最终状态将被限制在该地区。在这种情况下，该政策可能很难将其推广到探索区域之外的（2,2）之类的目标。这样的限制会如何影响她的表现？提交由＆＃32; /u/drlc_     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</guid>
      <pubDate>Tue, 25 Feb 2025 18:20:38 GMT</pubDate>
    </item>
    <item>
      <title>Q学习，折扣系数为0。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在研究一个项目，以实现Q-学习的代理。我只是意识到对环境，状态和动作进行了配置，因此当前的行动不会影响未来的状态或奖励。我认为在这种情况下，折现因子应该等于零，但是我不知道Q学习代理是否有意义解决此类问题。在我看来，它比MDP更像是上下文的强盗问题。 Q学习算法的名称为0，或等效算法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ixzkgs/qlearning_with_a_a_discount_factor_of_0/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</guid>
      <pubDate>Tue, 25 Feb 2025 16:50:04 GMT</pubDate>
    </item>
    <item>
      <title>精确的仿真模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixtjk0/precise_simulationmodel/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 我目前正在使用Bipedal机器人从事大学项目。我想实现一个基于RL的控制器进行行走。据我所知，有必要拥有一个精确的学习模型，以便成功地跳入SIM2REAL差距。我们在NX中有一个CAD型号，我听说有一个选择将CAD转换为Isaac Sim中的UDF。 ，但是哪种工业“黄金标准”方法是为模拟的良好模型吗？   &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/u/theoneandonly_ncb      [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixtjk0/precise_simulationmodel/</guid>
      <pubDate>Tue, 25 Feb 2025 12:12:12 GMT</pubDate>
    </item>
    <item>
      <title>现在，增强型套图支持PPO！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  嘿，大家，  renforceui-studio现在包括接近策略优化（ppo）！🚀 href=&quot;https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/&quot;&gt;here), I introduced ReinforceUI-Studio as a tool to make training RL models easier. I received many requests for PPO, and it&#39;s finally here!如果您有兴趣，请检查一下，让我知道您的想法。另外，保持算法请求的到来 - 您的反馈有助于使工具变得更好！  文档： https://docs.reinforceui-studio.com/algorithms/algorithm_list/algorithm_list  href =“ https://github.com/dvalenciar/reinforceui-studio”&gt; https://github.com/dvalenciar/reinforceui-studio       &lt;！提交由＆＃32; /u/u/dvr_dvr     [link]        [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</guid>
      <pubDate>Tue, 25 Feb 2025 08:18:11 GMT</pubDate>
    </item>
    <item>
      <title>DDPG问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</guid>
      <pubDate>Tue, 25 Feb 2025 00:02:36 GMT</pubDate>
    </item>
    <item>
      <title>与RL一起使用的最佳机器人模拟器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在尝试模拟我的机器人必须与连接到末端效应器上的传感器设备进行交互的环境，并使用RL进行读数。我希望然后在实际的硬件上使用这个训练有素的代理。您会推荐什么模拟器？我看过Pybullet和Guazebo。但是我不确定哪个似乎是最简单，最佳的方法，因为我在模拟方面几乎没有经验。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bananaoramama   href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ix8eux/best_robotic_simulator_to_to_use_with_with_rl/”&gt; [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</guid>
      <pubDate>Mon, 24 Feb 2025 18:01:00 GMT</pubDate>
    </item>
    <item>
      <title>奖励成型想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我对奖励成型形式有一个想法，想知道大家都在考虑这一点。 想象您拥有超级稀疏的奖励功能，例如+1赢得胜利和-1的损失，情节很长。这个奖励功能正是我们想要的。  当然，我们都知道稀疏的奖励功能很难学习。因此，引入密集的奖励功能似乎很有用。一个函数，表明我们的代理商正朝正确或错误的方向行驶。 It is often really tricky to define such a reward function that exactly matches our true reward function, so I think it only makes sense to temporarily use this reward function to initially get our agent in roughly the right area in policy space. As a disclaimer, I must say that I&#39;ve not read any research on reward shaping, so forgive me if my ideas are silly. One thing I&#39;ve done in the past with a DQN-like algorithm is在培训过程中，逐渐从一个奖励功能转移到另一个奖励功能。一开始，我使用了100％的致密奖励功能和稀疏的0％。一段时间后，我开始逐渐“退火”。这个比率直到我只使用真正的稀疏奖励功能。我看得很好。 我这样做的原因是“退火”。是因为我认为Q学习算法很难适应完全不同的奖励功能。但是我确实想知道退火率浪费了多少时间。我也不喜欢退火率是另一个超参数。 我的想法是将奖励函数的硬转换应用于演员批评算法。想象一下，我们将模型训练在密集的奖励功能上。我们假设我们得出了一项体面的政策，也是评论家的体面价值估计。现在，我们要做的就是冻结演员，硬击奖励功能，并重新审查评论家。我认为我们可以消除高参数，因为现在我们可以训练，直到评论家的错误达到一定的门槛为止。我想这是一个新的超参数。无论如何，我们会解开演员并恢复正常的培训。 我认为这在实践中应该很好。我还没有机会尝试。你们都对这个想法有何看法？有什么理由期望它行不通吗？我不是演员 - 批评算法的专家，所以这个想法甚至没有意义。 让我知道！谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [comment]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</guid>
      <pubDate>Mon, 24 Feb 2025 15:12:25 GMT</pubDate>
    </item>
    <item>
      <title>Simbav2：可扩展深度增强学习的超透明标准化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</link>
      <description><![CDATA[    介绍 simbav2！   📄项目页面： https://arxiv.org/abs/2502.15280   🔗代码： https://github.com/dojeon-ai/simbav2      simbav2是一种简单，可扩展的RL体系结构，可稳定 simberal formant br br br br br br br brs 。演员评论家在57个连续的控制任务（Mujoco，dmcontrol，Myosuite，humyoid-Bench）中实现了最先进的表现（SOTA）。  它与体育馆1.0.0 api   - 尝试一下！ ，如果您有任何问题，请随时与之伸出援手：）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/joonleesky     [link]     [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</guid>
      <pubDate>Mon, 24 Feb 2025 11:43:23 GMT</pubDate>
    </item>
    </channel>
</rss>