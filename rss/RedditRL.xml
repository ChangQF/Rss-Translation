<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 13 Feb 2025 18:23:13 GMT</lastBuildDate>
    <item>
      <title>当您已经拥有体育馆环境时，sb3 矢量化环境如何工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</link>
      <description><![CDATA[我不太明白。您只是使用他们的 VecEnv 包装它吗？还是我必须重写它？    提交人    /u/blearx   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</guid>
      <pubDate>Thu, 13 Feb 2025 17:33:39 GMT</pubDate>
    </item>
    <item>
      <title>RLLib 使用多个 Runner 不会增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</link>
      <description><![CDATA[抱歉这里没有发布任何图片。  所以，我的问题是在 RLLib 上使用 24 个带有 SAC 的 env 运行器，导致根本没有学习。但是使用 2 个 env 运行器确实学到了（一点点）。  详细信息： Env - 是简单的 2d 移动到目标位置，当达到目标状态时，每个时间步骤的稀疏奖励为 -0.01，具有 500 帧限制，Box(shape=(10,)) 观察和 Box(-1,1) 动作空间。我尝试了一堆超参数，但似乎都没有用。 对 RLlib 非常陌生。我曾经制作过自己的 rl 库，但这次我想尝试 rllib。 有人知道问题是什么吗？如果您需要更多信息，请问我！！谢谢    由   提交  /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</guid>
      <pubDate>Thu, 13 Feb 2025 14:53:55 GMT</pubDate>
    </item>
    <item>
      <title>参考丢失：带有 RL 算法分类法/本体的电子表格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</link>
      <description><![CDATA[我在这里的某个地方看到过它，现在我找不到它了。我知道有几篇论文调查了 RL 算法，但我正在尝试找到一个“电子表格”，一位成员在评论中发布了这一信息。我相信这是一个指向谷歌文档的链接。 每一行都有一些更高级别的分组，每个组中都有算法和注释。它根据算法的属性（例如连续动作空间等）将它们分开。 有人知道该资源或我可以在哪里找到它吗？ 编辑：找到了！https://rl-picker.github.io/    提交人    /u/ParamedicFabulous345   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</guid>
      <pubDate>Thu, 13 Feb 2025 11:41:11 GMT</pubDate>
    </item>
    <item>
      <title>没有机器学习的强化学习，可以做到吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioezsw/reinforcement_learning_without_machine_learning/</link>
      <description><![CDATA[嗨，我对[回归+分类+聚类+关联规则]有了解。我理解数学方法和算法，但不了解代码（我有 现在，我想了解计算机视觉和强化学习。 所以有人可以告诉我是否可以在不编写 ML 代码的情况下学习强化学习吗？    提交人    /u/InternationalWill912   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioezsw/reinforcement_learning_without_machine_learning/</guid>
      <pubDate>Thu, 13 Feb 2025 08:59:41 GMT</pubDate>
    </item>
    <item>
      <title>什么是良好的文本到 Avatar 语音模型/管道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</link>
      <description><![CDATA[基本上就是这样。你们推荐哪个管道来生成一个可以读取文本的头像 - 所有报告的固定头像？（理想情况下是开源的，因为我可以访问 gpu 集群并且不想为第三方服务付费 - 因为我将提供合理的信息）。     提交人    /u/Gvascons   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</guid>
      <pubDate>Thu, 13 Feb 2025 05:25:12 GMT</pubDate>
    </item>
    <item>
      <title>Sergey Levine 强化学习 [在哪里可以找到这个]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</link>
      <description><![CDATA[嗨  作为初学者，我希望很好地掌握 RL 背后的数学。## 你能告诉我在哪里可以找到这门课程吗？拜托。 ## [Sutton Barto] 强化学习 = https://www.amazon.in/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249?dplnkId=c3df8b9c-8d63-4f9b-8a4e-bc601029852c 还有哪些其他资源值得关注？您能列出使用过的资源吗？请 另外  我开始学习 ML，想问问这里的有经验的人，关于理解每个算法（如 K-NN/SVM）背后的数学证明的要求 了解算法背后的数学真的很重要吗？或者只需观看视频，了解关键点，然后开始编码 学习 ML 的适当方法是什么？## ML 工程师是否深入研究了这么多编码，还是他们只是通过可视化和开始编码来低估关键点？ 请让我知道。（我在这个领域无望）    提交人    /u/InternationalWill912   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</guid>
      <pubDate>Thu, 13 Feb 2025 03:02:00 GMT</pubDate>
    </item>
    <item>
      <title>有人有 Julia 中 PPO RL 的工作示例吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</link>
      <description><![CDATA[似乎我发现的所有代码库都已过时且无法使用。     提交人    /u/D3MZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</guid>
      <pubDate>Wed, 12 Feb 2025 16:57:19 GMT</pubDate>
    </item>
    <item>
      <title>目前，击败超级马里奥第一关的最佳 RL 方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</link>
      <description><![CDATA[我见过 PPO、DQN 和 NEAT。SethBling 在 2015 年使用 NEAT 编写了一个 RL 代理，看起来它的表现是所有代理中最好的。在 4 年后，我重新回到了 RL 领域，并希望用 Python 实现它作为个人项目。我应该实现哪一个？有新方法吗？    提交人    /u/marblesandcookies   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</guid>
      <pubDate>Wed, 12 Feb 2025 16:10:25 GMT</pubDate>
    </item>
    <item>
      <title>体育馆环境中的代理动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</link>
      <description><![CDATA[你好，有人知道我如何访问安全健身房、openai gym 中的代理动态吗？ 通常 .step() 直接模拟动态，但我的应用程序中需要动态，因为我需要对这些动态进行区分。更具体地说，我需要计算 f(x) 的梯度和 g(x) 的梯度，其中 x_dot=f(x)+g(x)u。x 是状态，u 是输入（动作） 我总是可以将其视为黑匣子并学习它们，但我更喜欢直接从地面真实动态中得出梯度。 请告诉我！    提交人    /u/Limp-Ticket7808   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</guid>
      <pubDate>Wed, 12 Feb 2025 15:57:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和机器人技术领域的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</link>
      <description><![CDATA[大家好，我最近获得了 RL（技术上是逆向 RL）的博士学位，该学位应用于人机协作。我使用过 4 种不同的机器人操纵器、4 种不同的夹持器和 4 种不同的 RGB-D 相机。我的专长在于使用感知反馈学习智能行为，以实现安全高效的操作。 我建立了端到端管道，用于在传送带上对产品进行分类、在未受精的卵子进入孵化器之前对其进行无损识别和移除、使用机器人对医疗器械进行智能无菌处理，以及其他一些项目。我曾在三菱电机研究实验室实习，目前已在顶级会议上发表了 6 篇以上的论文。 我使用过许多物体检测平台，例如 YOLO、Faster-RCNN、Detectron2、MediaPipe 等，并且拥有丰富的注释和训练经验。我擅长使用 Pytorch、ROS/ROS2、Python、Scikit-Learn、OpenCV、Mujoco、Gazebo、Pybullet，并且对 WandB 和 Tensorboard 有一些经验。由于我最初不是来自计算机科学背景，所以我不是一名专业的软件开发人员，但我编写的代码稳定、干净、易于扩展。 我一直在寻找与此相关的工作，但目前我在就业市场上很难找到工作。如果您能提供任何帮助、建议、推荐等，我将不胜感激。作为一名持学生签证的人，我时间紧迫，需要尽快找工作。提前谢谢您。    提交人    /u/prasuchit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:05 GMT</pubDate>
    </item>
    <item>
      <title>将本地环境连接到 HPC（高性能计算）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ins11i/connecting_local_environment_to_hpc_high/</link>
      <description><![CDATA[我有一个环境，由于权限问题，无法安装在 HPC 中。但我已经将它安装在我的电脑上。我的想法是将具有 GPU 的 HPC 连接到具有强化学习数据的本地，但我无法使用 gRPC 实现，因为它变得复杂了。 有什么想法我应该从哪里开始我的研究？    提交人    /u/Gullible_Ad_6713   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ins11i/connecting_local_environment_to_hpc_high/</guid>
      <pubDate>Wed, 12 Feb 2025 14:27:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么 deepseek 不使用 mcts</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inqdsr/why_deepseek_didnt_use_mcts/</link>
      <description><![CDATA[mtcs 有问题吗    提交人    /u/Alarming-Power-813   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inqdsr/why_deepseek_didnt_use_mcts/</guid>
      <pubDate>Wed, 12 Feb 2025 13:08:22 GMT</pubDate>
    </item>
    <item>
      <title>“Satori：通过行动思维链强化学习通过自回归搜索增强 LLM 推理”，Shen 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inl7uk/satori_reinforcement_learning_with/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inl7uk/satori_reinforcement_learning_with/</guid>
      <pubDate>Wed, 12 Feb 2025 07:00:43 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个寻找 RLHF 工作的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</link>
      <description><![CDATA[我们在 AI 的多个学科都有工作机会。我们也有专门的 RLHF 工作页面。在过去 30 天内，我们有 48 个涉及 RLHF 的工作机会。 您可以在此处找到所有 RLHF 工作： https://www.moaijobs.com/rlhf-jobs 请告诉我您的想法。谢谢。    提交人    /u/WordyBug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</guid>
      <pubDate>Wed, 12 Feb 2025 02:20:52 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1incdey/ppo_implementation/</link>
      <description><![CDATA[大家好。我正在做一个项目，我必须使用 PPO 来训练一个代理下棋，但我很难实现该算法。有人可以告诉我一个已经实现了这个的库，或者给我一个可以查看以获得灵感的 repo 链接吗？我正在使用 pettingzoo 和 tensorflow 的国际象棋实现。谢谢    提交人    /u/Livid-Ant3549   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1incdey/ppo_implementation/</guid>
      <pubDate>Tue, 11 Feb 2025 23:10:09 GMT</pubDate>
    </item>
    </channel>
</rss>