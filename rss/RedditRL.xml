<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 16 Jan 2024 03:16:09 GMT</lastBuildDate>
    <item>
      <title>SB3 的随机启动状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</link>
      <description><![CDATA[我正在使用 SB3 的 DDPG，但在学习时无法加载具有不同启动状态的文件。我每次都尝试在重置方法中更改它。我的猜测是训练黑鬼只发生在一个情节中，因为没有调用重置方法，所以没有变化。也用 PPO 尝试过。另外，我如何控制训练次数和时间步长？ 在网上搜索的绳索结束🙂   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</guid>
      <pubDate>Mon, 15 Jan 2024 23:44:37 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习：综合调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.10256 摘要：  多代理应用程序的流行遍及我们的各种互连系统日常生活。尽管它们无处不在，但在共享环境中集成和开发智能决策代理对其有效实施提出了挑战。这项调查深入研究了多智能体系统 (MAS) 领域，特别强调阐明 MAS 框架内学习最优控制的复杂性，通常称为多智能体强化学习 (MARL)。本次调查的目的是提供对 MAS 各个方面的全面见解，揭示无数机会，同时强调多代理应用程序所面临的固有挑战。我们希望不仅有助于更深入地了解 MAS 景观，而且还为研究人员和从业者提供有价值的观点。通过这样做，我们的目标是在 MAS 的动态领域内促进知情探索并促进发展，认识到在解决 MARL 中出现的复杂性方面需要适应性策略和持续发展。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</guid>
      <pubDate>Mon, 15 Jan 2024 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您对强化学习的真实体验是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</link>
      <description><![CDATA[ 由   提交 /u/Smallpaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</guid>
      <pubDate>Mon, 15 Jan 2024 21:31:30 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助 - 检查输入时出错：预期 flatten_input 有 3 个维度，但得到形状为 (4, 1) 的数组</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197f70n/need_help_error_when_checking_input_expected/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197f70n/need_help_error_when_checking_input_expected/</guid>
      <pubDate>Mon, 15 Jan 2024 18:02:27 GMT</pubDate>
    </item>
    <item>
      <title>我的 AIRL 无法正常工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1978gsl/my_airl_is_not_working/</link>
      <description><![CDATA[专家轨迹的概率在增加，而策略生成的轨迹在减少，但策略无法从推断的奖励函数中学习。   由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1978gsl/my_airl_is_not_working/</guid>
      <pubDate>Mon, 15 Jan 2024 13:15:28 GMT</pubDate>
    </item>
    <item>
      <title>累积奖励曲线平滑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196svan/cumulative_reward_curve_smooth/</link>
      <description><![CDATA[嗨， 我正在多维离散动作和观察空间上运行 A2C。在训练期间，我正在计算运行均值和方差以标准化我的奖励。我的奖励计算是随机的，因为有需求被实现。在评估我学到的政策时，我发现无论给出什么观察，都是相同的行动。我绘制了累积奖励，它看起来非常平滑的线性。我想知道这是否是预期的行为？我打印出了逐步奖励，它确实并不总是 1，在 [-1,1] 之间波动（大部分）。谢谢！   由   提交 /u/polymerase2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196svan/cumulative_reward_curve_smooth/</guid>
      <pubDate>Sun, 14 Jan 2024 22:54:41 GMT</pubDate>
    </item>
    <item>
      <title>奖励稀疏，剧集长度长。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196rt1n/sparse_reward_with_long_episode_length/</link>
      <description><![CDATA[嗨！我正在尝试使用 PPO 算法找到一个好的策略来优化本地搜索启发式中的参数。挑战在于我只能在每集结束时评估策略的性能，其中提供 [0,1] 范围内的稀疏奖励。剧集长度固定为 1000 步。在这种情况下是否有机会学习成功的政策？到目前为止，即使采用非常简单的观察结构，我也没有取得任何积极的成果。也许我可以尝试一些技巧。预先感谢您的帮助！   由   提交 /u/OpportunityHot7289   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196rt1n/sparse_reward_with_long_episode_length/</guid>
      <pubDate>Sun, 14 Jan 2024 22:10:28 GMT</pubDate>
    </item>
    <item>
      <title>强化学习优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196idl8/reinforcement_learning_for_optimization/</link>
      <description><![CDATA[有没有人尝试使用 RL 来解决优化问题，例如旅行商问题或类似问题，我检查了几篇他们使用 DQN 的论文，但在实际实现后我还没有即使对于简单的问题，例如将盒子从迷宫的一端移到另一端，也没有得到任何实际的结果。我还担心基于 DQN 的解决方案能否在未见过的数据上表现良好。欢迎提出任何建议。   由   提交 /u/HSaurabh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196idl8/reinforcement_learning_for_optimization/</guid>
      <pubDate>Sun, 14 Jan 2024 15:29:51 GMT</pubDate>
    </item>
    <item>
      <title>[需要建议/反馈] 训练时 DQN 波动很大。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196eref/need_advicefeedback_dqn_strongly_fluctuates_when/</link>
      <description><![CDATA[      大家好，我是 RL 新手，想用 DDQN 做点什么。我从这个 PyTorch 网站 找到了这篇关于如何使用 DQN 玩 CartPole 的文章。我尝试采用此代码，但将游戏更改为 BreakOut（更具体地说，BreakOutv5 {frame_skip = 4，repeat_action = 0.25））。  我对原始代码所做的更改是通过 GreyScale、Cropping、Resize 和 FrameStack 对环境进行预处理。 def Observation_preproc(frame):cropped_frame = frame[ 35:195, 7:153]/255 returncropped_frame STACK_NUM= 4 RESIZE_HEIGHT= 84 RESIZE_WIDTH= 84 # 制作游戏环境 env=gym.make(&quot;ALE/Breakout-v5&quot;, render_mode=&#39;rgb_array&#39;) env=gym. wrappers.GrayScaleObservation（env）env=gym.wrappers.TransformObservation（env，observation_preproc）env=gym.wrappers.ResizeObservation（env，（RESIZE_HEIGHT，RESIZE_WIDTH））env=gym.wrappers.FrameStack（env，STACK_NUM）  我还用卷积层重新构建了 DDQN 模型 class DQN(nn.Module): def __init__(self, n_stack, n_actions): super(DQN, self).__init__() self.conv_layer1= nn.Sequential( nn.Conv2d(in_channels= n_stack, out_channels= 32, kernel_size= 8, stride= 4), nn.ReLU(inplace= True) ) self.conv_layer2= nn.顺序( nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= 4, stride= 2), nn.ReLU(inplace= True) ) self.conv_layer3= nn.Sequential( nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= 3, stride= 1), nn.ReLU(inplace= True) ) self.relu= nn.ReLU(inplace= True) self.flatten= nn.Flatten() self.layer1= nn.Linear(7 *7*64, 512) self.layer2= nn.Linear(512, n_actions) defforward(self, x): x= self.conv_layer1(x) x= self.conv_layer2(x) x= self.conv_layer3(x ) x= self.flatten(x) x= self.relu(self.layer1(x)) x= self.layer2(x)  我计划训练它一百万集但在大约3000集时，我注意到：  训练集奖励和持续时间图。 学习过程面临一些强烈的波动。 ​ 我的问题是：这种现象叫什么？你认为我怎样才能阻止它？我知道 DQN 已经过时了，但是我正在一步步学习（另外，在这个 DeepMind论文，他们的DQN得分比我高:)))) 如果你想阅读我的整个笔记本，这里有直接的Github 链接，请看一下:)  谢谢大家 &lt; /div&gt;  由   提交/u/Q_H_Chu  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196eref/need_advicefeedback_dqn_strongly_fluctuates_when/</guid>
      <pubDate>Sun, 14 Jan 2024 12:24:28 GMT</pubDate>
    </item>
    <item>
      <title>减少迭代次数或其他方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196dxse/reduce_number_of_iterations_or_other_methods/</link>
      <description><![CDATA[嘿， ​ 我目前正在写我的硕士论文，涉及强化学习在代码生成中的应用。我的重点是新开发的领域特定语言 (DSL)，它的可用示例有限，因为还没有用这种语言编写的功能程序的广泛数据库。 我的目标是训练能够在这个新的 DSL 中编写代码的模型。对于环境，我有能力执行代码以确定它是否产生预期的输出。目前，我的方法包括随机选择 1 到 200 个操作来验证每次迭代中生成的代码是否正确。然而，事实证明这种方法非常耗时。 您能否建议我一种减少迭代次数的方法？任何见解或建议将不胜感激。 ​ 谢谢！   由   提交/u/mim549276  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196dxse/reduce_number_of_iterations_or_other_methods/</guid>
      <pubDate>Sun, 14 Jan 2024 11:32:34 GMT</pubDate>
    </item>
    <item>
      <title>奇怪的行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196boz1/strange_behaviour/</link>
      <description><![CDATA[我正在使用 Q 学习开发剪刀石头布代理 - https://github.com/revyu/RPS .在玩mrugesh时没有任何问题，它的胜率相当稳定，但在kris上它玩得很糟糕。它可以玩 {&#39;player&#39;: 400, &#39;opponent&#39;: 201, &#39;tie&#39;: 399}, winrate=0.400000 或 {&#39;player&#39;: 0, &#39;opponent&#39;: 1000, &#39;tie&#39;: 0}, winrate=0.000000 ，没有中间结果。我对机器学习和强化学习还很陌生，无法理解发生了什么。最让我惊讶的不是算法表现不佳，而是它的结果正好位于彼此相距很远的两个点上。   由   提交 /u/revyakin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196boz1/strange_behaviour/</guid>
      <pubDate>Sun, 14 Jan 2024 08:59:46 GMT</pubDate>
    </item>
    <item>
      <title>“潜伏特工：通过安全培训持续存在的训练欺骗性法学硕士”，Hubinger 等人 2024 {Anthropic}（RLHF 和对抗性训练未能消除法学硕士中的后门）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195x2tw/sleeper_agents_training_deceptive_llms_that/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195x2tw/sleeper_agents_training_deceptive_llms_that/</guid>
      <pubDate>Sat, 13 Jan 2024 20:17:21 GMT</pubDate>
    </item>
    <item>
      <title>“语言模型可以解决计算机任务”，Kim 等人 2023（MiniWoB++ 的内心独白）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195v9nt/language_models_can_solve_computer_tasks_kim_et/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195v9nt/language_models_can_solve_computer_tasks_kim_et/</guid>
      <pubDate>Sat, 13 Jan 2024 19:00:04 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自学</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195l81f/reinforcement_learning_self_taught/</link>
      <description><![CDATA[大家好， 我想进入强化学习，但不知道从哪里开始，因此我想问一下如果有人对从哪里开始有任何建议，并且可能有一些资源来这样做。我是一名 STEM 专业的大学生，拥有 Python 经验，想开始深入研究强化学习，因为它看起来非常有趣且具有挑战性。我很想听听您是如何学习的，以及关于我如何学习的任何建议。 提前致谢   由   提交 /u/Simozzzo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195l81f/reinforcement_learning_self_taught/</guid>
      <pubDate>Sat, 13 Jan 2024 10:35:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 学习梯度下降步长</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</link>
      <description><![CDATA[       问题陈述： 我一直在研究一个使用强化学习加速梯度下降收敛的项目。我想学习一种策略，可以将梯度下降的当前状态映射到最佳动作，即本例中的步长。提醒一下：梯度下降迭代由 x_k+1 = x_k - gamma*grad(f) 给出，其中 gamma 为步长。目前，我只考虑 f(x) = x&#39;Qx 形式的凸二次函数。我想在函数分布上训练策略，以便在预测时它可以泛化到该分布中的所有函数，以及在训练期间未见过的函数。通过在每次迭代中预测最佳步长的策略，目标是梯度下降在该分布内的所有函数的迭代次数较少的情况下收敛。 ​ 当前方法： 目前，我一直在使用无模型的强化学习算法，如 Soft Actor-Critic (SAC) 和 Twin Delayed Deep Definitive Policy Gradient (TD3) 来训练策略，但我发现即使对于某个特定函数过度拟合的简单情况，所需的内存和计算量也非常高。此外，当您过度拟合（对同一函数进行训练和评估）时，您会期望奖励收敛到某个值。如图所示，奖励确实增加了，但在某些时候代理完全忘记了它所学到的东西。我使用稀疏奖励：每次迭代中收敛时为 0，未收敛时为 -1。也许最好有一个奖励，说明每次迭代中残差（=梯度范数）的减少，这样代理不仅会在回合结束时接收信息。对于状态，我尝试了不同的方法，但仅包含当前梯度似乎或多或少有效。我使用的算法是SAC，它似乎比TD3更快。演员和评论家均由神经网络参数化，每个神经网络有 3 个隐藏层和 128 个节点。我使用了 Stable-Baselines 3 的实现。 ​ 我的问题： - 是无模型的RL 解决这个问题的正确方法是什么？它的计算成本非常高。是否有更好的方法，例如基于模型的强化学习或某种策略搜索？ - 在图中，为什么奖励突然减少？它与重放缓冲区的大小有关系吗？目前我可以分配 120Gb 的内存，这已经是相当多了。 - RL 理论通常基于马尔可夫过程。因此它假设马尔可夫性质，即当前状态完全独立于先前的状态。但是，最好添加一些有关先前梯度的信息以增加动量（例如 Nesterov 加速）。在这个框架中这可能吗？ ​ https://preview.redd.it/lnn1k9s333cc1.jpg?width=937&amp;format=pjpg&amp;auto=webp&amp;s=4254e662c840e4b4ca719b1f 70a488041376fad2   由   提交 /u/Lennitar   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</guid>
      <pubDate>Fri, 12 Jan 2024 22:15:47 GMT</pubDate>
    </item>
    </channel>
</rss>