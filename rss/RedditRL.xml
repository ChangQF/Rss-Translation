<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 11 Jan 2024 06:19:10 GMT</lastBuildDate>
    <item>
      <title>二维动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193ud0k/twodimensional_action_space/</link>
      <description><![CDATA[如何处理二维动作空间，我说的不是二维的向量，而是二维的张量。 例如，一个 lstm actor，其输入是（timesteps，features=256），输出另一个特征序列（timesteps，features=4096）。有了如此大的特征数量，是否可以将输出展平以将其视为一维动作空间？ 为了避免混淆，上面的时间步长不是环境的时间步长，而是环境的时间步长输入数据（状态）。   由   提交 /u/FancyUsual7476   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193ud0k/twodimensional_action_space/</guid>
      <pubDate>Thu, 11 Jan 2024 05:47:15 GMT</pubDate>
    </item>
    <item>
      <title>“图式学习和重新绑定作为上下文学习和涌现的机制”，Swaminathan 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193m3bx/schemalearning_and_rebinding_as_mechanisms_of/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193m3bx/schemalearning_and_rebinding_as_mechanisms_of/</guid>
      <pubDate>Wed, 10 Jan 2024 23:04:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO 代理无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193kw1i/ppo_agent_fails_to_learn/</link>
      <description><![CDATA[我正在做一个基于PPO算法的路径规划项目。在我的实验中，有一个16*16的网格地图，其中有几个障碍物区域作为环境，目的是训练代理直到找到到达目标的路径。这是我的模型的详细主要信息：  我部署了一个 A2C 机制，其中 Actor 和 Critic 网络都是四层结构，其中 Actor 具有 2*512*512*8 和评论家 2*512*512*1。二维输入是当前位置作为状态，八维向量输出是八个动作朝相应方向移动的概率。  超参数设置如下： 学习率：actor：3e-04，cirtic：4e-04 两个网络的最大等级：1.5&lt; /li&gt; 策略剪辑：epsilon = 0.3 折扣：gamma = 0.99 GAE 参数：lambda = 0.95 熵正则化参数：0.005&lt; /li&gt; Batchsize = 512 奖励函数设置规则如下：  当当前节点到目标的曼哈顿距离为时，获得正奖励（+1.0）较小，否则为负值（-1.0）。  当智能体移动到障碍物时，奖励为-10.0。 如果智能体从与障碍物相邻的节点离开，则奖励为+10.0。  &gt;奖励重塑：如果智能体不断朝着正确的方向前进，奖励将会额外增加。    实验结果：根据我对两个网络损失值的追踪结果，我可以非常确定该模型是迭代期间收敛。然而，收敛效果并不如我预期。 MSE 的批评者血统损失从一开始的 100, 000.00 以上开始，最终停留在 4,000.00 左右，然后开始波动。另一方面，演员损失可以从超过 100.00 开始减少，但在 60.00 左右停止减少。简而言之，培训有效果，但效果不佳。  此外，经过 5000 次训练后，回报率仍然没有明显改善。在相当大的概率下，智能体仍然会选择远离目标或移动到障碍物的动作。每集的平均回报率一直保持在 -2.0 左右，而且从未有任何改善。   ​ 在这里，我真的希望有人能帮助我解决这个问题。我将非常感谢您的帮助和建议。非常感谢！  ​ ​   由   提交/u/Tight_Boysenberry692   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193kw1i/ppo_agent_fails_to_learn/</guid>
      <pubDate>Wed, 10 Jan 2024 22:15:32 GMT</pubDate>
    </item>
    <item>
      <title>软演员评论家：Huber 与 MSE 损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1939z6c/soft_actorcritic_huber_vs_mse_loss/</link>
      <description><![CDATA[Huber（或平滑 L1）损失似乎常用于 DQN 算法中以提高稳定性，这对我来说很有意义，因为目标 Q 网络是最初可能是垃圾。然而，我见过的大多数 SAC 实现都使用 MSE 作为批评者，而且我还无法仅通过谷歌搜索找到理由。 是否有任何直觉为什么 MSE 可能比 Huber 工作得更好专门针对SAC的损失？它可能与问题相关吗？人们是否因为 MSE 效果足够好而懒得尝试 Huber 损失？   由   提交 /u/DoNotAbsquatulate   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1939z6c/soft_actorcritic_huber_vs_mse_loss/</guid>
      <pubDate>Wed, 10 Jan 2024 14:49:28 GMT</pubDate>
    </item>
    <item>
      <title>通过体操游戏训练 CNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1939cpb/train_cnn_with_gymnasium_games/</link>
      <description><![CDATA[大家好， 我是大学讲师，想向我的学生展示 CNN 和 Deep Q 的结合-学习。他们应该接受一项任务，让代理解决一个简单的游戏（简单是因为他们应该能够使用“普通”笔记本来解决它）。我刚刚看了gymnasium的文档，但没有找到可以将图像作为状态传递的游戏。图书馆里没有这样的东西吗？ 提前感谢大家的帮助:)   由   提交/u/MarcoX0395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1939cpb/train_cnn_with_gymnasium_games/</guid>
      <pubDate>Wed, 10 Jan 2024 14:20:53 GMT</pubDate>
    </item>
    <item>
      <title>低级政策法学硕士</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1934hug/llms_for_low_level_policies/</link>
      <description><![CDATA[最近有一些关于使用 LLM 生成高级任务计划并使用低级技能执行这些任务计划的工作。 我的问题是，LLM可以直接用于低级技能培训吗？语言预训练对导航技能有何帮助？   由   提交 /u/Ultra-Neural   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1934hug/llms_for_low_level_policies/</guid>
      <pubDate>Wed, 10 Jan 2024 09:35:50 GMT</pubDate>
    </item>
    <item>
      <title>和谐世界模型：提高基于模型的强化学习的样本效率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1931dhs/harmony_world_models_boosting_sample_efficiency/</link>
      <description><![CDATA[OpenReview: https: //openreview.net/forum?id=RN7RzMxwjC arXiv：https ://arxiv.org/abs/2310.00344 摘要：  基于模型的强化学习（MBRL）持有通过利用世界模型来实现样本高效学习，该模型对环境如何工作进行建模，通常包含两个任务的组件：观察建模和奖励建模。在本文中，通过专门的实证研究，我们更深入地了解了每个任务在世界模型中所扮演的角色，并通过协调观察和奖励模型之间的干扰，揭示了更高效的 MBRL 被忽视的潜力。我们的主要见解是，虽然显式 MBRL 的流行方法试图通过观测模型来恢复环境的丰富细节，但由于环境的复杂性和有限的模型容量，这是很困难的。另一方面，奖励模型虽然在隐式 MBRL 中占主导地位并且擅长学习以任务为中心的动态，但在没有更丰富的学习信号的情况下不足以进行样本有效的学习。利用这些见解和发现，我们提出了一种简单而有效的方法，Harmony World Models (HarmonyWM)，它引入了一个轻量级协调器来维持两个任务之间的动态平衡在世界模型学习中。我们对三个视觉控制域的实验表明，配备 HarmonyWM 的基本 MBRL 方法获得了 10%-55% 的绝对性能提升。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1931dhs/harmony_world_models_boosting_sample_efficiency/</guid>
      <pubDate>Wed, 10 Jan 2024 06:06:47 GMT</pubDate>
    </item>
    <item>
      <title>对 TorchRL 的看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192wfmj/opinions_on_torchrl/</link>
      <description><![CDATA[ 由   提交/u/marques576  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192wfmj/opinions_on_torchrl/</guid>
      <pubDate>Wed, 10 Jan 2024 01:53:08 GMT</pubDate>
    </item>
    <item>
      <title>“思想克隆：通过模仿人类思维在行动中学习思考”，Hu & Clune 2023（网格世界智能体的内心独白知识蒸馏）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192pexa/thought_cloning_learning_to_think_while_acting_by/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192pexa/thought_cloning_learning_to_think_while_acting_by/</guid>
      <pubDate>Tue, 09 Jan 2024 20:58:20 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡洛ES算法难以理解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192o7sw/difficult_in_understanding_the_monte_carlo_es/</link>
      <description><![CDATA[        由   提交 /u/VanBloot   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192o7sw/difficult_in_understanding_the_monte_carlo_es/</guid>
      <pubDate>Tue, 09 Jan 2024 20:10:18 GMT</pubDate>
    </item>
    <item>
      <title>“打造通用机器人大脑的全球项目”：RT-X 和缩放机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192mss4/the_global_project_to_make_a_general_robotic/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192mss4/the_global_project_to_make_a_general_robotic/</guid>
      <pubDate>Tue, 09 Jan 2024 19:13:57 GMT</pubDate>
    </item>
    <item>
      <title>“音乐推荐中熟悉度、相似度和发现的算法平衡”，Mehrotra 2021 {Spotify}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192kmgn/algorithmic_balancing_of_familiarity_similarity/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192kmgn/algorithmic_balancing_of_familiarity_similarity/</guid>
      <pubDate>Tue, 09 Jan 2024 17:46:40 GMT</pubDate>
    </item>
    <item>
      <title>AI摧毁NHL94（1对1模式）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192jk7k/ai_destroys_nhl94_1_vs_1_mode/</link>
      <description><![CDATA[       由   提交/u/matpoliquin  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192jk7k/ai_destroys_nhl94_1_vs_1_mode/</guid>
      <pubDate>Tue, 09 Jan 2024 17:03:56 GMT</pubDate>
    </item>
    <item>
      <title>具有可重置环境的强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192hc7d/reinforcement_learning_with_resettable/</link>
      <description><![CDATA[我正在研究中探索不同类型的学习问题。我注意到一种有趣的问题类型，可以有效地建模为 RL 问题，其中环境提供将环境重置为早期状态的操作。这允许代理进行实验并确保代理永远不会陷入困境。 但是，我很难找到有关此概念的任何论文。我能找到的唯一论文是关于检测游戏是否可重置的。我感兴趣的是一款将可重置性作为假设特征的游戏，并了解基于该假设可以在 RL 代理中构建什么样的优化。 有谁知道这个方向的研究吗？ ？也许用不同的名字？   由   提交 /u/Smart-Emu5581   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192hc7d/reinforcement_learning_with_resettable/</guid>
      <pubDate>Tue, 09 Jan 2024 15:32:25 GMT</pubDate>
    </item>
    <item>
      <title>在可塑性之前推断神经活动作为超越反向传播的学习基础</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/192aniz/inferring_neural_activity_before_plasticity_as_a/</link>
      <description><![CDATA[论文：https://www.nature.com/articles/s41593-023-01514-1 预印本版本：https://www.biorxiv.org/content/10.1101/2022.05.17.492325  代码：https://github.com/YuhangSong/Prospective-Configuration 摘要：  对于人类和机器来说，学习的本质是查明信息处理管道中的哪些组件对其输出中的错误负责，一个被称为“学分分配”的挑战。长期以来，人们一直认为学分分配最好通过反向传播来解决，这也是现代机器学习的基础。在这里，我们提出了一个根本不同的信用分配原则，称为“预期配置”。在前瞻性配置中，网络首先推断学习应产生的神经活动模式，然后修改突触权重以巩固神经活动的变化。我们证明，与反向传播相比，这种独特的机制（1）是在完善的皮质回路模型家族中进行学习的基础，（2）使得学习能够在生物有机体面临的许多环境中更加高效和有效，（3） ）再现了在不同的人类和大鼠学习实验中观察到的令人惊讶的神经活动和行为模式。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/192aniz/inferring_neural_activity_before_plasticity_as_a/</guid>
      <pubDate>Tue, 09 Jan 2024 09:15:44 GMT</pubDate>
    </item>
    </channel>
</rss>