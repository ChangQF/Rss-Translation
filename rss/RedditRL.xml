<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 10 May 2024 12:25:51 GMT</lastBuildDate>
    <item>
      <title>使用 RL Baselines3 Zoo 进行分布式优化的启动试验有多少次</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</link>
      <description><![CDATA[您好，我正在 RL Baselines3 Zoo 中的 6 个进程中分配优化（github），它使用 Optuna 。 我关心的参数代码位于 rl_zoo3/train.py 中：  parser.add_argument( &quot;--n-trials&quot;, help=&quot;优化超参数的试验次数。&quot; &quot;这适用于每个优化运行器，而不是整个优化过程。&quot;, type=int, default= 500, ) parser.add_argument(&quot;--n-startup-trials&quot;, help=&quot;使用 optuna 采样器之前的试验次数&quot;,  我知道我是否使用每个6 个进程 --n-trials 100 那么我将获得 600 次试验。 但是 --n-startup-Trials 10 会怎么样呢？是 10 次还是 60 次启动试验？    提交ufoludek3000&quot;&gt; /u/ufoludek3000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</guid>
      <pubDate>Fri, 10 May 2024 10:25:03 GMT</pubDate>
    </item>
    <item>
      <title>生成式人工智能已经达到顶峰了吗？ - 电脑爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co4gsj/has_generative_ai_already_peaked_computerphile/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/FedeRivade  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co4gsj/has_generative_ai_already_peaked_computerphile/</guid>
      <pubDate>Thu, 09 May 2024 18:38:09 GMT</pubDate>
    </item>
    <item>
      <title>“通过强化学习出现类似信念的表征”，Hennig 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</guid>
      <pubDate>Thu, 09 May 2024 16:06:37 GMT</pubDate>
    </item>
    <item>
      <title>AlphaMath 几乎为零：流程无流程监督</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnuaab/alphamath_almost_zero_process_supervision_without/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2405.03553 代码：https://github.com/MARIO-Math-Reasoning/Super_MARIO 模型：https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B 摘要：  大型语言模型的最新进展（LLM）的数学推理能力得到了显著增强。然而，这些模型仍然难以解决需要多个推理步骤的复杂问题，经常导致逻辑或数字错误。虽然数字错误在很大程度上可以通过集成代码解释器来解决，但识别中间步骤中的逻辑错误更具挑战性。此外，手动注释这些步骤以进行训练不仅成本高昂，而且需要专业知识。在本研究中，我们引入了一种创新方法，通过利用蒙特卡洛树搜索（MCTS）框架自动生成过程监督和评估信号，消除了手动注释的需要。本质上，当LLM经过良好的预训练时，只需要数学问题及其最终答案即可生成我们的训练数据，而无需解决方案。我们继续训练一个步骤级价值模型，旨在改进LLM在数学领域的推理过程。我们的实验表明，使用增强了MCTS的LLM自动生成的解决方案可显着提高模型处理复杂数学推理任务的能力。    由    /u/EternalBlueFriday  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnuaab/alphamath_almost_zero_process_supervision_without/</guid>
      <pubDate>Thu, 09 May 2024 10:49:53 GMT</pubDate>
    </item>
    <item>
      <title>用于在网格地图上寻找路径的 RL - SLAM。成功率问题（SAC + HER）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnr2w8/rl_for_path_finding_on_the_grid_map_slam_success/</link>
      <description><![CDATA[      大家好！我一直在努力解决涉及车辆在黑白地图上移动的自定义环境。您可以将地图视为二进制网格。车辆的目标是到达白色区域内随机选择的目的地。当车辆到达目的地、撞到地图上的黑色区域或撞上边界墙时，游戏结束。 我使用 SB3 中的 SAC + HER，但我一直在使用一个多月以来，一直在努力实现 0.6 以上的成功率。这是我尝试过的方法：  使用熵系数进行实验 添加模拟激光雷达光束和到最近黑色像素的距离等观测结果 使用不同的超参数集进行实验（超过 500 次运行） 调整缓冲区大小  大多数情况下，代理很难到达起始点正前方黑墙之外的区域点。 我的奖励计算如下： rew = -np.power(np.dot(np.abs(achieved_goal[:1] -desired_goal), Weights_array ), 0.5)  观察空间定义为： self.observation_space =spaces.Dict({ &#39;observation&#39;:spaces.Box(low= 0，高= 1，形状=（6 + self.num_lidar_beams，），dtype = np.float32），&#39;实现的目标&#39;：spaces.Box（低= 0，高= 1，形状=（6，），dtype = np .float32), &#39;desired_goal&#39;:spaces.Box(low=0, high=1, shape=(6,), dtype=np.float32), })  动作空间：  self.action_space = space.Box(low=np.array([-1, 0]), high=np.array([1, 1]), dtype=np.float32 )  这些动作控制车辆的转向角度和速度。 状态是： self.state[0 ] = self.normalize_state(self.x,self.screen_width,0) self.state[1] = self.normalize_state(self.y,self.screen_height,0) self.state[2] = hdg_norm self.state[3 ] = v self.state[4] = self.normalize_state(np.cos(np.deg2rad(hdg)), 1, 0) self.state[5] = self.normalize_state(np.sin(np.deg2rad(hdg) )), 1, 0) self.state[-self.num_lidar_beams:] = litar_distances # 更新该州的激光雷达数据  我附上了四张图片：  环境的外观 环境的另一个视图 按区域说明成功率的地图 显示随时间变化的成功率的图  https:// /preview.redd.it/6dmfb58qnczc1.png?width=3364&amp;format=png&amp;auto=webp&amp;s=6b13c219c427f66e1035cf16bf78b4baefed7fc2 自从我工作以来，我非常感谢任何建议对此已经有一段时间了。有没有人从事过类似的项目并愿意分享代码？或者你知道 GitHub 上类似 2D 问题的示例吗（不是停车环境，因为我已经广泛研究过这些问题）？ 如果没有地图（所有像素都是白色），成功率会达到 1.0&lt; /p&gt;   由   提交/u/Sharp-Record1600  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnr2w8/rl_for_path_finding_on_the_grid_map_slam_success/</guid>
      <pubDate>Thu, 09 May 2024 07:06:19 GMT</pubDate>
    </item>
    <item>
      <title>将 tanh 应用于正常样本后，SAC 估计概率密度函数的视觉表示。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnnhye/visual_representation_for_sac_estimated/</link>
      <description><![CDATA[Desmos PDF 图表 我创建这个是因为我不明白如何从策略输出中跨越我的 std（标准差）。我听说将值限制在区间 (0, 1] 中很好，但是简单地限制时会有一些梯度损失，该策略可以开始输出低于 0 并且永远不会恢复。所以我尝试使用 sigmoid 作为 std 的激活，但过了一会儿，它使熵停留在 1，从而破坏了策略。 查看 PDF 后，我想我会尝试使用 sigmoid * 0.5，但也许我应该将 alpha 更改为αH(a(s)) 是动态的，但我不知道，如果你在熵正则化方面也遇到了麻烦，并且动态改变 alpha 对你有帮助，请告诉我。无论如何，我在互联网上的其他地方没有发现这个看起来有帮助。 p&gt;    提交者 /u/O_CLIPE   [链接]   ; [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnnhye/visual_representation_for_sac_estimated/</guid>
      <pubDate>Thu, 09 May 2024 03:22:53 GMT</pubDate>
    </item>
    <item>
      <title>努力从头开始实施 PPO。 （健身房）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnhcoa/struggling_with_ppo_from_scratch_implementation/</link>
      <description><![CDATA[过去 5 个月，我一直在研究从头开始的 PPO 实现。除了数值计算库（例如 numpy）之外，我的大部分工作都是从头开始的。从监督学习网络开始，到现在。我似乎就是无法理解。我读过的每篇论文都是 A. 过时/不正确 B. 不完整。没有一篇论文完整描述他们做什么以及他们使用什么超参数。我尝试阅读 SB3 代码，但它与我的实现差别太大，我只是不明白发生了什么，因为文件太多，我找不到小细节。所以我只是要发布我的后向方法，如果有人愿意阅读它并告诉我一些错误/建议。那就太好了！旁注：我制作了使用标准梯度下降的优化器，而批评器只采用状态。我没有使用 GAE，因为我试图将潜在的故障点降到最低。所有超参数都是标准值。 def behind(self): T = len(self.trajectory[&#39;actions&#39;]) for i in range(T): G = 0 for j in range(i, T): current = self.trajectory[&#39;rewards&#39;][j] G += current * pow(self.gamma, j - i) # G = np.clip(G, 0, 15) # CRITIC STUFF if np.isnan(G): break state_t = self.trajectory[&#39;states&#39;][i] action_t = self.trajectory[&#39;actions&#39;][i] # 计算 state_t 的批评值 critical_value = self.critic(state_t) # print(f&quot;Critic: {critic_value}&quot;) # print(f&quot;G: {G}&quot;) # 计算状态-动作对的优势 advantages = G - critical_value #打印（f“”“”返回：{G} # 预期回报：{critic}“”“）# 旧参数内容 new_policy = self.forward（state_t，1000）# PPO 内容 ratio = new_policy / action_t clipped_ratio = np.clip（ratio，1.0 - self.clip，1.0 + self.clip） surrogate_loss = -np.minimum（ratio * advantages，clipped_ratio * advantages）# entropy_loss = -np.mean（np.sum（action_t * np.log（action_t），axis = 1））# 参数向量 weights_w = self.hidden.weights.flatten（） weights_x = self.hidden.bias.flatten（） weights_y = self.output.weights.flatten（） weights_z = self.output.bias.flatten（） weights_w = np.concatenate((weights_w, weights_x)) weights_w = np.concatenate((weights_w, weights_y)) param_vec = np.concatenate((weights_w, weights_z)) param_vec.flatten() loss = np.mean(surrogate_loss) # + self.l2_regularization(param_vec) # print(f&quot;loss: {loss}&quot;) # 反向传播 next_weights = self.output.weights self.hidden.layer_loss(next_weights, loss, tanh_derivative) self.hidden.zero_grad() self.output.zero_grad() self.hidden.backward() self.output.backward(loss) self.hidden.update_weights() self.output.update_weights() self.critic_backward(G)     由    /u/meh_coder 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnhcoa/struggling_with_ppo_from_scratch_implementation/</guid>
      <pubDate>Wed, 08 May 2024 22:26:26 GMT</pubDate>
    </item>
    <item>
      <title>MARL 零和博弈中自我对战 vs 双神谕</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnei6i/self_play_vs_double_oracle_in_marl_zerosum_games/</link>
      <description><![CDATA[据我了解： 在自我对战中，智能体直接与自身版本（历史版本或当前版本）进行比赛复制）以改进其策略。基本上计算出“到目前为止的一切”的最佳响应。另一方面，双预言算法涉及为两个代理维护显式策略集，并逐步扩展。但扩展是类似的。两者都根据其他代理模型和过去模型的混合策略计算最佳响应，并将这个新模型添加到自己的模型列表中。 我理解对吗？坦率地说，我见过的谈论这些概念的论文对我来说太复杂了，但似乎基本原理并不那么深刻。也许我在理解这些算法时遗漏了一些重要的东西？ 在我们使用函数逼近器（例如神经网络）的情况下，我们什么时候停止在这两种情况下寻找最佳响应？  p&gt; 一个比另一个更好吗？什么是权衡？也许更有可能收敛到纳什均衡？   由   提交 /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnei6i/self_play_vs_double_oracle_in_marl_zerosum_games/</guid>
      <pubDate>Wed, 08 May 2024 20:27:14 GMT</pubDate>
    </item>
    <item>
      <title>具有重放缓冲区的 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cndw4j/rl_algorithms_with_replay_buffers/</link>
      <description><![CDATA[什么是具有重播缓冲区但适用于离散动作空间（除了 DQN）的 RL 算法？   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cndw4j/rl_algorithms_with_replay_buffers/</guid>
      <pubDate>Wed, 08 May 2024 20:01:35 GMT</pubDate>
    </item>
    <item>
      <title>DDPG - 输出问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnbp86/ddpg_issues_with_the_output/</link>
      <description><![CDATA[大家好， 因此，我正在训练我的 DDPG 算法进行 AP 选择，其中模型或操作的输出是基于移动代理的某些因素的 AP ID 列表，即基本上是一个向量。最初，该模型在探索总共 4 个 AP 的所有 AP 组合方面表现相当不错，如下所示： 输出：[2, 2, 3, 2, 1, 4] 但是在 120 集左右之后，输出总是在 1 和 2 的选择之间，即。 输出：[2, 1, 1, 1, 1, 2] 它甚至没有根据其他参数选择这些 AP，因为这些 AP 显然不是最佳选择。我尝试将 OUNoise max_sigma 增加到 4.0（我知道这太高了），但它在情节图中的平均奖励中产生了峰值。 OUNoise(action_space=self.action_size, max_sigma=10.0, min_sigma=0.01, decay_period=1000000)  我也尝试过将学习率降低到 = 0.01。我甚至为输出中的冗余设置了负奖励，但它不起作用。如您所见，这是我第一次实施 DDPG 或 DRL，我很慌张！有人可以告诉我我做错了什么吗？    提交人    /u/NecessaryThat2571   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnbp86/ddpg_issues_with_the_output/</guid>
      <pubDate>Wed, 08 May 2024 18:29:07 GMT</pubDate>
    </item>
    <item>
      <title>对高斯策略的策略梯度的质疑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cna10j/doubt_about_policy_gradient_with_gaussian_policy/</link>
      <description><![CDATA[      嗨， 我正在观看 YouTube 上的 Sergey Levine 课程。  我现在使用策略梯度，到目前为止零问题，但一些数学表达式让我感到困惑。 在此视频中 https://youtu.be/VSPYKXm_hMA?si=WIdrh41TX8RHXYu3&amp;t=238 at 3:58 他使用了以下等式： https://preview.redd.it /dl9qxk3vj8zc1.png?width=426&amp;format=png&amp;auto=webp&amp;s=b6f322d291b88dd3627cd16812bb0532fa684fcb 我不明白为什么对数概率等于这个距离以及如何计算梯度这个的。  谢谢。   由   提交 /u/RikoteMasterrrr   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cna10j/doubt_about_policy_gradient_with_gaussian_policy/</guid>
      <pubDate>Wed, 08 May 2024 17:18:37 GMT</pubDate>
    </item>
    <item>
      <title>问题：DQN 纸牌游戏 让代理了解他可以玩哪些纸牌有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cn19em/question_dqn_card_game_does_it_make_sense_to_have/</link>
      <description><![CDATA[我正在尝试训练 AI 使用 DQN 玩纸牌游戏。 相关规则的简短摘要概述纸牌游戏： ~40 张纸牌，4 名玩家。每个玩家获得 10 张随机牌。进行 10 轮，每个玩家打出 1 张牌（连续）。现在，您并不总是被允许打出手中的每张牌，因为这很大程度上取决于该轮中已经打出的牌。 我目前正在做的是在我的神经网络中输出 40 个动作（40 张牌），然后从所有 40 张中取出价值最高的一张。如果这张牌不在特工手中，或者在给定情况下玩不合法，我就会取消游戏，给它一个不好的奖励，然后继续下一个。  现在我想知道这是否有意义，或者与在给定情况下从所有法律行动中选择具有最高价值的行动相比是否没有任何好处，这样我会显然，不必学习有关哪些牌可以合法玩以及经纪人手中有哪些牌的所有规则。 （我现在使用的状态是：handcards（40），cardsonboard（40）。One Hot Encoded） 我真的没有看到我当前的做法有任何优势，我只是不确定是否从所有操作的不断变化的子集中选择代理操作时，在训练过程中可能会出现任何问题吗？   由   提交 /u/TratanusII   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cn19em/question_dqn_card_game_does_it_make_sense_to_have/</guid>
      <pubDate>Wed, 08 May 2024 10:29:44 GMT</pubDate>
    </item>
    <item>
      <title>PID 类型 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cmvqqk/pid_type_rl/</link>
      <description><![CDATA[大家好， 我正在尝试用 stable_baselines3 的 RL 替换 PID（对于初学者来说似乎是最简单的） ）。一切都工作正常，但无论我使用什么算法或策略，我的操作总是非常嘈杂，并且不适用于我的真实系统。我的系统非常慢（大约 200 步才能得到响应）。 有人对算法、策略或如何配置超参数以实现缓慢且连续的操作有一些建议吗？我尝试配置奖励来平息行动，但即使我试图鼓励最后行动与实际行动之间的差异较小，它总是以饱和到最小值或最大值结束。 非常感谢您救命……我快要疯了！   由   提交/u/dos145  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cmvqqk/pid_type_rl/</guid>
      <pubDate>Wed, 08 May 2024 04:15:59 GMT</pubDate>
    </item>
    <item>
      <title>有街道行人行为模拟器吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cmpv69/is_there_a_simulator_for_street_pedestrian/</link>
      <description><![CDATA[我有一辆带摄像头的遥控汽车，我想知道是否可以针对街上行人的行为训练强化学习策略在模拟器中，然后在现实世界中尝试。   由   提交/u/mymooh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cmpv69/is_there_a_simulator_for_street_pedestrian/</guid>
      <pubDate>Tue, 07 May 2024 23:18:58 GMT</pubDate>
    </item>
    <item>
      <title>你是如何将强化学习变成你的职业的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cm37ba/how_did_you_make_rl_into_a_career/</link>
      <description><![CDATA[我是一名刚毕业的学生，​​对 RL 有一定的基础。（我的最后一年项目就是在这个基础上完成的）。但是，当我寻找 RL 工作时，我只找到硕士或博士职位。我很想听听人们如何在 RL 中找到工作的故事？关于探索的行业或我的下一步应该做什么有什么建议吗？    提交人    /u/Guilty-Cheesecake660   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cm37ba/how_did_you_make_rl_into_a_career/</guid>
      <pubDate>Tue, 07 May 2024 04:18:41 GMT</pubDate>
    </item>
    </channel>
</rss>