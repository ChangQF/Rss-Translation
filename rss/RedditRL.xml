<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 21 Feb 2025 09:18:06 GMT</lastBuildDate>
    <item>
      <title>RL不和谐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iukm6y/rl_discord/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我看到人们说他们想要一个rl学习组，但没有一个不和谐，所以我决定做一个，如果你想要你想要&lt; a href =“ https://discord.gg/xu36gsht”&gt; https://discord.gg/xu36gsht     &lt;！ - sc_on-&gt;＆＃32;提交由＆＃32; /u/u/damrstick     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iukm6y/rl_discord/</guid>
      <pubDate>Fri, 21 Feb 2025 06:37:53 GMT</pubDate>
    </item>
    <item>
      <title>非LLM RL博士学位的就业市场</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu8jos/job_market_for_nonllm_rl_phd_grads/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  传统RL PHD毕业生的当前市场如何（DEEP RL，RL理论）？任何人都想分享求职经验？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hmi2015    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iu8jos/job_market_for_nonllm_rl_rl_phd_grads/”&gt; [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu8jos/job_market_for_nonllm_rl_phd_grads/</guid>
      <pubDate>Thu, 20 Feb 2025 20:31:30 GMT</pubDate>
    </item>
    <item>
      <title>分布参与者批评</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu7z8i/distributional_actorcritic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我真的很喜欢分布强化学习的想法。我已经阅读了C51和QR-DQN论文。 IQN是我的列表中的下一个。 一些演员 - 批判算法将Q值学习为评论家吗？我认为，这是SAC，TD3和DDPG的算法，对吗？这是一个有希望的方向吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu7z8i/distributional_actorcritic/</guid>
      <pubDate>Thu, 20 Feb 2025 20:08:03 GMT</pubDate>
    </item>
    <item>
      <title>人形步态训练Isaacgym＆Motion Mimitation</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu7u8z/humanoid_gait_training_isaacgym_motion_imitation/</link>
      <description><![CDATA[在：//smpl.is.tue.mpg.de/“&gt; https://smpl.is.tue.mpg.de/ ）走路并一直在某些问题中运行。我选择实施PPO来训练以人形生物状态（联合DOF，脚力传感器等）读取的政策，并以任何位置（ISAACGYM PD控制器接管）或基于扭矩的驱动作用。然后，我设计了奖励功能，包括：（1）正向速度（2）直立姿势（3）脚触点交替（4）对称运动 （5）Hyprextension约束（6）骨盆身高稳定性（7）脚滑罚&lt; /p&gt; 使用这种方法，我尝试了多次训练运行，每个训练效果不同，结果不同， IE。我没有看到任何甚至遥不可及的向前运动的事物的实际融合，更不用说是自然步态了。我在以前的RL段之上构建了它。 mocap步行数据（Amass DataSet  https://amass.is.tue.mpg.de/ ）。当我在ISAACGYM培训约1000个环境时，我会在每个环境中加载唯一的设置序列长度情节，并包括他们的“性能”。 使用这种方法模仿动作集作为奖励的一部分。 &lt; /p&gt; 以下是我注意到的有关培训的现象：（1）训练很快收敛。我正在运行1000个环境，每个时期的300步序列长度，每个时期5个网络更新，并且在第一个时期内观察收敛（融合到性能差）。（2）我的价值损失非常高，例如12个订单由于损失政策，我目前正在研究这一点。 有人在这种培训方面有任何经验还是对解决方案有任何建议？ 谢谢多！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricesLearning/comments/1iu7u8z/humanoid_gait_gait_training_isaacgym_motion_imitation/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu7u8z/humanoid_gait_training_isaacgym_motion_imitation/</guid>
      <pubDate>Thu, 20 Feb 2025 20:02:31 GMT</pubDate>
    </item>
    <item>
      <title>将PPO调整到AEC Env中</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu33gw/adapt_ppo_to_aec_env/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在从事RL项目，必须为Pettingzoo AEC环境实现PPO。我想使用稳定基线的实现，但它与AEC Envs不起作用。有什么方法可以使其适应AEC，还是可以使用另一个库？如果有帮助  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/livid-ant3549     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu33gw/adapt_ppo_to_aec_env/</guid>
      <pubDate>Thu, 20 Feb 2025 16:51:55 GMT</pubDate>
    </item>
    <item>
      <title>RL博士学位的主题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu2x0x/themes_for_phd_in_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿！ 简介。我在2024年获得了硕士学位。我的研究生工作认为学习机器人是为了避免使用熊猫和pybullet模拟障碍。目前，我在金融领域担任ML工程师，从事经典ML，主要是推荐系统。 最近，我在同一所获得BS和MS的大学开始了我的博士学位课程。自2024年秋天以来，我一直在这样做。我很好奇RL算法及其应用程序，特别是机器人技术。到目前在模拟中创建的副本。我计划进行一些实验，以控制它，以解决一些基本任务，例如到达对象，将其放入盒子中。我想写第一篇论文。后来，我计划更深入地进入该领域，并进行更多实验。此外，我将对RL中的当前状态进行一些分析，并可能也撰写有关它的出版物。 我决定去学习博士学位，主要是因为我想从一边有额外的动力学习RL（因为很难不放弃），写一些论文（因为在ML Sphere中有一些有用的论文），然后进行一些实验。将来，如果我有这样的机会，我想使用RL，机器人或自动驾驶汽车。因此，我在这里不是要做很多学术工作，而是为我的个人教育，未来的职业和行业业务做更多的事情。  但是，我的主要研究人员更多地是工程的东西，而且还很老。这意味着她可以就如何正确进行研究给我很多建议，但是她对RL和AI领域的了解并不深刻。我几乎一个人做。 所以我想知道是否有人可以就考虑RL和Robotics的研究主题提出一些建议？有没有社区可以与他人分享利益？如果有人对合作感兴趣，我很想进行对话，并且可以分享联系人  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/alex_werben     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu2x0x/themes_for_phd_in_rl/</guid>
      <pubDate>Thu, 20 Feb 2025 16:44:48 GMT</pubDate>
    </item>
    <item>
      <title>用于食品和饮料推荐系统的RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu0qre/rl_for_food_and_beverage_recommendation_system/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  ，因此我目前正在研究如何利用RL，以便为餐馆和主题公园的食品和饮料提供更好的推荐引擎。目前，我的眼睛抓住了珍珠，这似乎非常有前途，因为它具有太多的模块，可以调整它可以向用户提出建议的方式。但是，我还可以研究其他RL模型吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iu0qre/rl_for_food_food_and_and_and_beverage_recommendation_system/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu0qre/rl_for_food_and_beverage_recommendation_system/</guid>
      <pubDate>Thu, 20 Feb 2025 15:13:33 GMT</pubDate>
    </item>
    <item>
      <title>增强学习的书籍[代码+理论]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itxbjj/books_for_reinforcement_learning_code_theory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ 该代码似乎有些复杂，因为很难编程我在RL中介绍的初始理论。  关于加固学习，哪些书可以阅读以了解代码以及代码部分。  另外，阅读RL理论和概念的时间是多少，一个人可以开始编码rl。 请告诉我！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/internationalwill912     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itxbjj/books_for_reinforcement_learning_code_theory/</guid>
      <pubDate>Thu, 20 Feb 2025 12:26:38 GMT</pubDate>
    </item>
    <item>
      <title>代理不学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itwfgc/agent_not_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://reddit.com /link/1Itwfgc/video/ggfrxkxf4ake1/player   大家好，我目前正在进行自动化车辆模拟。我已经进行了汽车和当前的训练，以使其绕着赛道。但是，尽管训练了超过100k的步骤，但代理似乎没有学到任何东西。这里可能有什么问题？奖励/惩罚点是否未正确给出？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lonely_joke944     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itwfgc/agent_not_learning/</guid>
      <pubDate>Thu, 20 Feb 2025 11:32:22 GMT</pubDate>
    </item>
    <item>
      <title>稳定生物素的子保护</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itw58z/subprocvecenv_from_stablebaselines/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试使用start_method =＆quot =&#39;fork subprocvecenv中的稳定baselines2中的多量量，但它无效，无法找到＆quort的上下文; fork&#39;&#39;。我使用的是稳定的 - 冰淇淋3 2.6.0a1，打印了所有可用的方法，我唯一可以使用的方法是“ Spawn”我不知道为什么。有人知道我该怎么做才能修复它？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/clightable-button264     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itw58z/subprocvecenv_from_stablebaselines/</guid>
      <pubDate>Thu, 20 Feb 2025 11:13:37 GMT</pubDate>
    </item>
    <item>
      <title>最佳RL存储库具有简单的SOTA算法实现，这些算法易于编辑？ （最好在JAX中）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ituera/best_rl_repo_with_simple_implementations_of_sota/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/godireallyhateyoutim     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ituera/best_rl_repo_with_simple_implementations_of_sota/</guid>
      <pubDate>Thu, 20 Feb 2025 09:11:51 GMT</pubDate>
    </item>
    <item>
      <title>对于那些通过模拟进行加固学习（RL）的人，我已经在NVIDIA ISAAC实验室上播放了10个视频</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itu6na/for_those_looking_into_reinforcement_learning_rl/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32;态href =“ https://www.youtube.com/watch?v=sl1wcfp9tru＆amp；   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itu6na/for_those_looking_into_reinforcement_learning_rl/</guid>
      <pubDate>Thu, 20 Feb 2025 08:55:36 GMT</pubDate>
    </item>
    <item>
      <title>好奇你们用作DRL算法的库。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itpjje/curious_on_what_you_guys_use_as_a_library_for_drl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我已经在练习强化学习（RL）已经有一段时间了。最初，我曾经根据研究论文编码算法，但是如今，我使用体育馆图书馆和具有稳定的基本线3（SB3）的RL代理来开发环境，并在必要时创建自定义策略。  i&#39;&#39;&#39;很想知道您在从事的工作以及用于环境和算法的哪些库。此外，如果行业中有任何专业人员，我很想听听您是否使用任何特定库或使用代码库。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/wild_wolf19    href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1itpjje/curious_on_what_you_guys_as_a_a_a_a_library_for_for_for_for_drl/”&gt; [links]       &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1itpjje/curious_on__ what_you_guys_ause_ause_a_a_a_a_library_for_for_for_drl/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itpjje/curious_on_what_you_guys_use_as_a_library_for_drl/</guid>
      <pubDate>Thu, 20 Feb 2025 03:59:12 GMT</pubDate>
    </item>
    <item>
      <title>从字面上重新创建了数学推理和DeepSeek的AHA时刻，不到10美元，通过最终简单的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1it2zhv/literally_recreated_mathematical_reasoning_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://medium.com/@rjusnba/overnight-end-end-eend-to-end-raind-raining-a-3b-model-on-a-a grade-school-school-math-math-dataset-leads-leads-to-to-to-to-to-rounconing-df61410c04c6   我感到惊讶！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/unightent-life9355     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1it2zhv/literally_recreated_mathematical_reasoning_and/</guid>
      <pubDate>Wed, 19 Feb 2025 11:06:27 GMT</pubDate>
    </item>
    <item>
      <title>RL研究组？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isz4eh/study_group_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有RL研究组？美国时区 更新： 您会添加 时区或位置 当前ML背景  P&gt;对RL的重点或兴趣，即传统的RL，Deep RL，理论和论文，Pytorch等/p&gt;  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/best_fish_2941     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isz4eh/study_group_for_rl/</guid>
      <pubDate>Wed, 19 Feb 2025 06:36:15 GMT</pubDate>
    </item>
    </channel>
</rss>