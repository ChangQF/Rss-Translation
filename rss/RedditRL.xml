<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 23 Aug 2024 09:15:47 GMT</lastBuildDate>
    <item>
      <title>强化算法之前的游戏中的对手？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez7g8b/opponents_in_games_before_reinforcement_algorithms/</link>
      <description><![CDATA[我目前正在学习（深度）强化学习。我的教授向我们展示了实施算法来解决雅达利游戏的经典示例。我们讨论的算法可以追溯到 1990 年代末到 2010 年代初。 这让我对以下事情产生了疑问：小时候我曾经玩过例如 Yu-Gi-Oh Power of Chaos 系列或 Fifa 98，在这些游戏中，你的对手相当聪明（至少在我小时候的记忆中是这样），据我所知，你甚至可以设定对手的难度。当时已经实施了这些算法吗？或者这些只是硬编码的规则集？    提交人    /u/Vast-Signature-8138   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez7g8b/opponents_in_games_before_reinforcement_algorithms/</guid>
      <pubDate>Fri, 23 Aug 2024 08:22:13 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道我的 RL 股票交易模型是否表现优异，因为它真的很好，或者是因为代码有故障？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez7896/how_can_i_know_whether_my_rl_stock_trading_model/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez7896/how_can_i_know_whether_my_rl_stock_trading_model/</guid>
      <pubDate>Fri, 23 Aug 2024 08:05:54 GMT</pubDate>
    </item>
    <item>
      <title>TRPO 论文中的随机变量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez3vih/random_variable_in_trpo_paper/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez3vih/random_variable_in_trpo_paper/</guid>
      <pubDate>Fri, 23 Aug 2024 04:26:43 GMT</pubDate>
    </item>
    <item>
      <title>彻底改变机器人行为：Transformer 如何通过动作分块提升模仿学习能力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyvevk/revolutionizing_robot_behavior_how_transformers/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyvevk/revolutionizing_robot_behavior_how_transformers/</guid>
      <pubDate>Thu, 22 Aug 2024 21:41:34 GMT</pubDate>
    </item>
    <item>
      <title>扑克解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyoa2i/poker_solution/</link>
      <description><![CDATA[我有一个类似于扑克的任务，即有全押、弃牌、过牌、跟注、加注等操作。但本质是比较牌的等级，等级较高的获胜。我决定实施其中一种 CFR 算法。但我不知道是哪一种。 所以我有几个问题：  实施的最佳算法是什么（Vanilla CFR、CFR+、MCCFR、Deep CFR 或其他（例如 PPO））。 处理加注选择的最佳方法是什么？我应该将其离散化（例如检查、跟注、弃牌、加注 0.5bb、1bb 等）还是有其他方法？ 是否有可以找到这些 CFR 算法的现成实现的来源？     提交人    /u/silenthnowakeup   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyoa2i/poker_solution/</guid>
      <pubDate>Thu, 22 Aug 2024 16:48:58 GMT</pubDate>
    </item>
    <item>
      <title>pybullet 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyhdme/pybullet_question/</link>
      <description><![CDATA[我正在做一个四足动物的项目，我想得到布尔值，看看它的脚是否接触地面，有人知道怎么得到它吗？    提交人    /u/youssef_naderr   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyhdme/pybullet_question/</guid>
      <pubDate>Thu, 22 Aug 2024 11:54:34 GMT</pubDate>
    </item>
    <item>
      <title>MARL 的框架/库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyd8rw/framework_library_for_marl/</link>
      <description><![CDATA[嗨， 我在为 MARL 寻找类似于 CleanRL/SB3 的东西。 有人能推荐一下吗？我看到了 BenchMARL，但添加自己的环境看起来有点奇怪。我还看到了 epymarl 和 mava，但不确定哪个最好。理想情况下，我更喜欢 torch 中的某些东西。 期待您的推荐！ 谢谢 !    提交人    /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyd8rw/framework_library_for_marl/</guid>
      <pubDate>Thu, 22 Aug 2024 07:25:55 GMT</pubDate>
    </item>
    <item>
      <title>神经进化 + 强化学习问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ey13vs/neuroevolution_rl_question/</link>
      <description><![CDATA[我最近完成了（大概）从头开始编写 PPO。我遇到的最大问题是，很难将其扩大规模并给它更多的时间进行训练，因为它最终会忘记一切，或者也可能会陷入局部最大值。我对神经进化有点兴趣，据我所知，神经进化的主要问题是它不能很好地扩展到更大的网络，并且需要大量的计算。  所以我的问题是，为什么没有很多神经进化 + RL 研究，或者没有普遍使用的实现？（如果有，请留下链接或名称）据我所知，独立训练一群 RL 代理并选择/交叉最好的代理应该可以解决神经进化和 RL 单独存在的缺点。    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ey13vs/neuroevolution_rl_question/</guid>
      <pubDate>Wed, 21 Aug 2024 21:09:26 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的条件动作问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exvagq/conditional_action_problem_in_reinforcement/</link>
      <description><![CDATA[      大家好， 希望你们都过得好。我需要你的帮助来为在连续两室环境中运行的机器人设计一个决策过程。场景如下：  机器人从房间 A 开始，然后移动到房间 B，并继续此顺序 - 首先是房间 A，然后是房间 B，依此类推。 在每个房间中，机器人有四个动作选项：静止不动、向前移动、左转或右转。 如果它静止不动，它会保留所有这些选项以用于下一步。 如果它向前移动，它可以在下一步中静止不动或移动到下一个房间（房间 B 或返回房间 A，具体取决于它所在的位置）。 如果它左转，它可以在下一步中静止不动或右转，然后在下一步中，它可以再次静止不动或移动到下一个房间。 同样，如果它右转，它可以静止不动或左转在下一步中，然后站着不动或移动到下一个房间。 它最多可以连续站立三步。之后，它必须选择另一个动作。  我正在尝试弄清楚如何最好地制定这些动作。有人能建议最合适的算法或方法来处理这些条件和顺序动作吗？我知道这个场景可能有点复杂，如果它令人困惑，我深表歉意。任何指导或建议都将不胜感激！ https://preview.redd.it/ll6daz5qv1kd1.png?width=757&amp;format=png&amp;auto=webp&amp;s=8985d702bd80e892a7d1519f58e6c7367f7237ea    提交人    /u/muttahirulislam   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exvagq/conditional_action_problem_in_reinforcement/</guid>
      <pubDate>Wed, 21 Aug 2024 17:15:41 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Gym Custom 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exu802/help_with_gym_custom_environment/</link>
      <description><![CDATA[      嘿！需要一些 Gymnasium 自定义环境方面的帮助。我有这种布局，其中我需要对象仅沿多边形移动而不会发生碰撞。此类对象的数量不受限制。我已在此处附上布局以供您参考。 我几乎已为该环境编写了 pygame 代码，该代码处理该过程的 gui 和实时坐标跟踪。 我现在如何将其描述为自定义健身房环境？我知道需要制作的各种方法，例如 init、step、reset 等，但我不确定在我的特定情况下这些方法的内容是什么。  如能得到任何帮助，我们将不胜感激！ https://preview.redd.it/z3nvb8ntn1kd1.png?width=3895&amp;format=png&amp;auto=webp&amp;s=6836e2f9a79623c5da364180934baf96bcc7709a    提交人    /u/Strange-Durian3382   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exu802/help_with_gym_custom_environment/</guid>
      <pubDate>Wed, 21 Aug 2024 16:33:43 GMT</pubDate>
    </item>
    <item>
      <title>解决位置控制延迟问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exq77k/solution_for_delay_for_positional_control/</link>
      <description><![CDATA[我目前正在使用 pybullet 教虚拟四足机器人行走。我正在使用位置控制，这意味着模型输出关节的所需位置，然后内置的 pid 控制器开始将关节的实际位置移动到所需位置。这会造成延迟，可能会损害学习过程，有没有针对这种延迟的实际解决方案？     提交人    /u/youssef_naderr   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exq77k/solution_for_delay_for_positional_control/</guid>
      <pubDate>Wed, 21 Aug 2024 13:52:33 GMT</pubDate>
    </item>
    <item>
      <title>多大的行动空间才算太大？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exo9dz/how_large_of_an_action_space_is_too_large/</link>
      <description><![CDATA[我是强化学习的新手，所以我不确定这是否是一个合理的担忧。目前，我正在从事一个关于 THz 波段通信的研究项目。我正在编写一个深度 Q 学习算法来选择传输数据的最佳频带。我有 1217 个频带可供选择。我正在使用 OpenAI 的 gymnasium 框架。因此，我的动作空间如下所示： self.action_space = space.MultiBinary(1217)  我为代理选择使用的通道分配 1，为它选择传输数据的通道分配 0。这个动作空间是否太大？我是否应该增加波段的大小以减少可供选择的波段数量？或者是否有另一种方法允许代理从大列表中选择多个项目？应该允许代理选择它想要选择的任意数量。    提交人    /u/Hailwel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exo9dz/how_large_of_an_action_space_is_too_large/</guid>
      <pubDate>Wed, 21 Aug 2024 12:25:04 GMT</pubDate>
    </item>
    <item>
      <title>开局 & (m,n,k)=(5,5,4)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exn49t/open_spiel_mnk554/</link>
      <description><![CDATA[我在游戏 m,n,k=5,5,4 中使用 open_spiel。我在为这个游戏创建 rl 机器人时遇到了问题。我尝试使用 alpha_zero 和 mcts 进行 100k 次模拟，并且总是第一个玩家获胜。对于 m、n、k 或井字游戏等游戏，最好的机器人是什么？    提交人    /u/Present_Formal2674   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exn49t/open_spiel_mnk554/</guid>
      <pubDate>Wed, 21 Aug 2024 11:26:15 GMT</pubDate>
    </item>
    <item>
      <title>他们经过一些训练后回来了</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ex4yli/theyre_back_after_some_training/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ex4yli/theyre_back_after_some_training/</guid>
      <pubDate>Tue, 20 Aug 2024 19:43:23 GMT</pubDate>
    </item>
    <item>
      <title>FrozenLake-v1 中的 MCTS 算法性能不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ex1szp/poor_performance_with_mcts_algorithm_in/</link>
      <description><![CDATA[大家好， 我目前正在 Gymnasium 的 FrozenLake-v1 环境中实施蒙特卡洛树搜索 (MCTS) 算法。虽然实施运行时没有错误，但我遇到了性能非常差的问题。成功率一直很低，并且代理很难学习有效的策略。 以下是我的设置的一些细节：  我使用树的上限置信区间 (UCT) 来平衡探索和利用。 状态动作值 (Q) 和访问计数 (N) 在每次模拟后都会初始化和更新。 我尝试调整探索权重，但并没有带来显着的改进。 环境是随机的，我怀疑这可能是导致问题的原因。  我正在寻找有关如何在此特定环境中提高 MCTS 性能的建议。是否有其他人在随机环境（如 FrozenLake）中遇到过类似的 MCTS 问题？任何关于如何调整算法的建议或任何处理此类环境的策略都将不胜感激。 提前致谢！    提交人    /u/EAG2705   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ex1szp/poor_performance_with_mcts_algorithm_in/</guid>
      <pubDate>Tue, 20 Aug 2024 17:36:38 GMT</pubDate>
    </item>
    </channel>
</rss>