<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 30 Dec 2024 21:15:13 GMT</lastBuildDate>
    <item>
      <title>â€œåŸºäºååŒæ•ˆåº”çš„æœºå™¨äººç¾¤ä½“è¡Œä¸ºçš„è‡ªåŠ¨è®¾è®¡â€ï¼ŒSalman ç­‰äºº 2024 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpvi1r/automatic_design_of_stigmergybased_behaviours_for/</link>
      <description><![CDATA[        æäº¤äºº    /u/gwern   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpvi1r/automatic_design_of_stigmergybased_behaviours_for/</guid>
      <pubDate>Mon, 30 Dec 2024 19:39:53 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo åŸºçº¿3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpt4tr/pettingzoo_baselines3/</link>
      <description><![CDATA[æˆ‘æœ‰ 2 ä¸ªå…·æœ‰ä¸åŒè§’è‰²çš„ä»£ç†ï¼Œé—®é¢˜æ˜¯å¦‚ä½•è®©æ¨¡å‹åœ¨é¢„æµ‹ä¸­ï¼ˆåœ¨æµ‹è¯•åŠ è½½çš„æ¨¡å‹æ—¶ï¼‰ç†è§£æ¯ä¸ªä»£ç†å…·æœ‰å“ªä¸ªè§’è‰²ï¼Ÿæˆ‘æ‰€åšçš„æ˜¯åœ¨ obs ä¸­æ·»åŠ ä¸€ä¸ªå¸ƒå°”å€¼æ¥åŒºåˆ†è§’è‰²ï¼Œä½†æˆ‘æƒ³çŸ¥é“æˆ‘æ˜¯å¦å¯ä»¥å‘å‡ºå®ƒå¹¶åœ¨æµ‹è¯•æ—¶ç®€å•åœ°ä½¿ç”¨ 2 ä¸ªä¸åŒçš„æ¨¡å‹ã€‚  æˆ‘ç›®å‰æ­£åœ¨æµ‹è¯•ï¼ˆaecï¼‰ model = PPO.load(latest_policy) # print(env.possible_agents) rewards = {agent: 0 for agent in env.possible_agents} # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨å¹¶è¡Œ API è¿›è¡Œè®­ç»ƒï¼Œä½†ä½¿ç”¨ AEC API è¿›è¡Œè¯„ä¼° # SB3 æ¨¡å‹æ˜¯ä¸ºå•ä»£ç†è®¾ç½®è®¾è®¡çš„ï¼Œæˆ‘ä»¬é€šè¿‡å¯¹æ¯ä¸ªä»£ç†ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ for i in range(num_games): env.reset(seed=i) for agent in env.agent_iter(): obs, reward, Termination, truncation, info = env.last() # print(obs) if reward &gt; 0ï¼šrewards[agent] += reward å¦‚æœç»ˆæ­¢æˆ–æˆªæ–­ï¼šact = None elseï¼šact = model.predict(obs, deterministic=True)[0] print(f&quot;\nAgent: {agent}, Observation: {obs}, Reward: {reward}, Action: {act}&quot;) env.step(act) env.close()  åœ¨è®­ç»ƒï¼ˆå¹¶è¡Œï¼‰ä¸­æˆ‘æœ‰ model = PPO( MlpPolicy, env, verbose=3, learning_rate=1e-3, batch_size=256, tensorboard_log=log_dir ) while Trueï¼šmodel.learn(total_timesteps=steps, reset_num_timesteps=False) save_path = os.path.join(model_dir, f&quot;{env.unwrapped.metadata.get(&#39;name&#39;)}_{time.strftime(&#39;%Y%m%d-%H%M%S&#39;)}&quot;) model.save(save_path)     æäº¤äºº    /u/More_Peanut1312   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpt4tr/pettingzoo_baselines3/</guid>
      <pubDate>Mon, 30 Dec 2024 17:59:58 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºä¸ºåŠ¨æ€å®šä»· RL ä»»åŠ¡åˆ›å»ºåˆæˆæ•°æ®çš„å»ºè®®ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpsnl4/advice_on_creating_synthetic_data_for_dynamic/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ æˆ‘æ­£åœ¨ä½¿ç”¨å¼ºåŒ–å­¦ä¹ å¼€å±•ç”µå­å•†åŠ¡åŠ¨æ€å®šä»·é¡¹ç›®ã€‚ç”±äºæˆ‘æ²¡æœ‰çœŸå®æ•°æ®ï¼Œå› æ­¤æˆ‘å°è¯•ç”Ÿæˆåˆæˆæ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘çš„è®¡åˆ’æ˜¯æ¯”è¾ƒ DQN å’Œ PPO æ¥å®Œæˆæ­¤ä»»åŠ¡ï¼Œå¹¶åœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­è®¾ç½®ä»·æ ¼ä»¥æœ€å¤§åŒ–æ”¶å…¥æˆ–åˆ©æ¶¦ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘äº†è§£äº†ï¼š  çº¿æ€§æ¨¡å‹ï¼šä»·æ ¼ä¸Šæ¶¨ â†’ éœ€æ±‚ä¸‹é™ï¼ˆä»·æ ¼å¼¹æ€§ï¼‰ã€‚ Logit æ¨¡å‹ï¼šåŸºäºç»æµæ¨¡å‹çš„å»ºæ¨¡ã€‚ å­£èŠ‚æ€§ï¼šç”±äºæ—¶é—´/äº‹ä»¶å¯¼è‡´çš„éœ€æ±‚æ³¢åŠ¨ã€‚  æˆ‘å¸Œæœ›æ•°æ®èƒ½å¤Ÿæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„è¡Œä¸ºï¼Œä¾‹å¦‚ä»·æ ¼æ•æ„Ÿåº¦ã€å­£èŠ‚æ€§å˜åŒ–å’Œä¸€äº›éšæœºæ€§ã€‚æˆ‘è§è¿‡å¾ˆå¤šè®ºæ–‡ä½¿ç”¨ DQN è¿›è¡Œç¦»çº¿å­¦ä¹ ï¼Œä½†æˆ‘å¾ˆæƒ³å°è¯• PPO å¹¶æ¯”è¾ƒç»“æœã€‚  æˆ‘å¾ˆä¹æ„è·å¾—æœ‰å…³å¦‚ä½•æ„å»ºæ­¤ç±»æ¨¡å‹æˆ–åº”è¯¥åŒ…å«å“ªäº›å†…å®¹ä»¥ä½¿æ•°æ®æ›´çœŸå®çš„ä»»ä½•å»ºè®®ã€‚ è¿™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡å°è¯•ä»å¤´å¼€å§‹åˆ›å»ºç¯å¢ƒï¼ˆæˆ‘åªè°ƒæ•´è¿‡å¥èº«æˆ¿ç¯å¢ƒï¼‰ï¼Œæ‰€ä»¥æˆ‘å¾ˆä¹æ„å¬å–æ‚¨çš„å»ºè®®ã€‚    æäº¤äºº    /u/Professional_Ant_140   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpsnl4/advice_on_creating_synthetic_data_for_dynamic/</guid>
      <pubDate>Mon, 30 Dec 2024 17:39:19 GMT</pubDate>
    </item>
    <item>
      <title>å½“å›æŠ¥åœ¨ 1e6 å’Œ 1e10 ä¹‹é—´æ—¶ï¼Œå¦‚ä½•æ ‡å‡†åŒ–å¥–åŠ±</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpsned/how_would_you_normalize_the_rewards_when_the/</link>
      <description><![CDATA[å˜¿ï¼Œæˆ‘æ­£åœ¨åŠªåŠ›ä½¿ç”¨ FQI ä»¥å¤–çš„ä»»ä½•å…¶ä»–æ–¹æ³•è·å¾—è‰¯å¥½çš„æ€§èƒ½ï¼Œä»¥é€‚åº”åŸºäº https://orbi.uliege.be/bitstream/2268/13367/1/CDC_2006.pdf çš„ç¯å¢ƒï¼Œæœ€å¤§æ—¶é—´æ­¥é•¿ä¸º 200ã€‚è§‚å¯Ÿç©ºé—´çš„å½¢çŠ¶ä¸º (6,)ï¼ŒåŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£çš„ (4) æˆ‘ä¸ç¡®å®šå¦‚ä½•åœ¨å¥–åŠ±ä¸Šåšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºéšæœºä»£ç†è·å¾—çš„å›æŠ¥çº¦ä¸º 1e7ï¼Œè€Œæœ€ä½³ä»£ç†åº”è¯¥è·å¾— 5e10ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å¾—åˆ°çš„æœ€ä½³ç»“æœæ˜¯ä½¿ç”¨å¸¦æœ‰ä»¥ä¸‹åŒ…è£…å™¨çš„ PPOï¼š  log(max(obs, 0) + 1) å°†æœ€åä¸€ä¸ªæ“ä½œé™„åŠ åˆ° obs TimeAwareObservation FrameStack(10) VecNormalize  åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å°è¯•ä½¿ç”¨å„ç§å¥–åŠ±æ ‡å‡†åŒ–çš„ PPO å’Œ DQNï¼Œä½†æ²¡æœ‰æˆåŠŸï¼ˆä½¿ç”¨ sb3ï¼‰ï¼š  ä½¿ç”¨ sb3 ä¸­çš„ VecNormalize æ— æ ‡å‡†åŒ– é™¤ä»¥ 1e10ï¼ˆä»…åœ¨ dqn ä¸Šå°è¯•ï¼‰ é™¤ä»¥å›æŠ¥çš„è¿è¡Œå¹³å‡å€¼ï¼ˆä»…åœ¨ dqn ä¸Šå°è¯•ï¼‰ é™¤ä»¥å›æŠ¥çš„è¿è¡Œæœ€å¤§å€¼ï¼ˆä»…åœ¨ dqn ä¸Šå°è¯•ï¼‰  ç°åœ¨æˆ‘æœ‰ç‚¹ç»æœ›ï¼Œå¹¶å°è¯•ä½¿ç”¨ python-neat è¿è¡Œ NEATï¼ˆæ€§èƒ½ä½ä¸‹ï¼‰ã€‚ æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°æˆ‘å¯¹ env çš„å®ç°ï¼šhttps://pastebin.com/7ybwavEW ä»»ä½•å…³äºå¦‚ä½•ä½¿ç”¨ç°ä»£æŠ€æœ¯å¤„ç†è¿™ç§ç¯å¢ƒçš„å»ºè®®éƒ½ä¼šå—åˆ°æ¬¢è¿ï¼    æäº¤äºº    /u/Butanium_   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpsned/how_would_you_normalize_the_rewards_when_the/</guid>
      <pubDate>Mon, 30 Dec 2024 17:39:06 GMT</pubDate>
    </item>
    <item>
      <title>è¾“å‡ºæ¦‚ç‡ä¸åˆå§‹åˆå§‹åŒ–æ—¶æ²¡æœ‰å˜åŒ–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hps2gg/output_probabilities_are_not_changing_from/</link>
      <description><![CDATA[å› æ­¤ï¼Œæˆ‘æ­£åœ¨å®æ–½ä¸€ç§ RL æ–¹æ³•ç”¨äºè‚¡ç¥¨äº¤æ˜“ï¼Œå› æ­¤æˆ‘æœ‰ä¸¤ä¸ªä»£ç†ï¼Œä¸€ä¸ªå†³å®šè¿›å…¥å“ªä¸ªæ–¹å‘ï¼Œå¦ä¸€ä¸ªç®¡ç†äº¤æ˜“ï¼Œå› æ­¤è¿›å…¥æ¨¡å‹è¾“å‡ºä¹°å…¥å’Œå–å‡ºä¹‹é—´çš„æ¦‚ç‡ï¼Œå®ƒä»¬åˆå§‹åŒ–åœ¨ 49ã€50 å·¦å³ï¼Œä½†é—®é¢˜åœ¨äºå¼€å‘ï¼ˆéªŒè¯ï¼‰ï¼Œå¦‚æœæ¨¡å‹åˆå§‹åŒ–æ—¶æœ‰ä¸€ä¸ªç•¥å¾®ä¼˜åŠ¿çš„åŠ¨ä½œï¼Œå®ƒæ€»æ˜¯é€‰æ‹©é‚£ä¸ªåŠ¨ä½œã€‚æˆ‘æ­£åœ¨æ£€æŸ¥æ¢¯åº¦å¹¶ç›‘æ§ wandb ä¸Šçš„æƒé‡ï¼Œç”šè‡³æ¦‚ç‡æ¯”ï¼Œå°½ç®¡å®ƒä»¬å¾ˆå°ï¼Œä½†ä¸€åˆ‡ä¼¼ä¹éƒ½åœ¨ç§»åŠ¨ï¼Œä½†è¾“å‡ºä¿æŒä¸å˜ã€‚æˆ‘çš„ç»ç†ä»£ç†ä¹Ÿæ˜¯å¦‚æ­¤ã€‚æ‰€ä»¥æˆ‘å·²ç»è¿è¡Œäº†äº”é›†ï¼Œä½†å•ä¸ªè®­ç»ƒé›†æœ‰ 300 ç¬”äº¤æ˜“ï¼Œæ¯ä¸ªäº¤æ˜“éƒ½æœ‰å…­ä¸ªè®­ç»ƒé˜¶æ®µï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¶³å¤Ÿçš„è®­ç»ƒå¯ä»¥çœ‹åˆ°æ¦‚ç‡åˆ†å¸ƒçš„ä¸€äº›å˜åŒ–ï¼Œä½†æ²¡æœ‰çœ‹åˆ°ä»»ä½•å˜åŒ–ï¼Œå¥–åŠ±æ˜¯ä¸ç¨³å®šçš„ï¼Œæ‰€ä»¥æˆ‘ä¸èƒ½è‚¯å®šåœ°ä½¿ç”¨è¯¥æŒ‡æ ‡æ¥åˆ¤æ–­ï¼Œè‡³äºéªŒè¯ï¼Œå®ƒä¸€ç›´æ‰§è¡Œå•ä¸€åŠ¨ä½œï¼Œä¸åƒåœ¨è®­ç»ƒä¸­å®ƒæ¢ç´¢å¹¶è·å¾—å¯è§‚çš„å›æŠ¥ï¼Œè¿™å¯èƒ½æ˜¯é—®é¢˜æ‰€åœ¨    æäº¤äºº    /u/sk3ptica1   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hps2gg/output_probabilities_are_not_changing_from/</guid>
      <pubDate>Mon, 30 Dec 2024 17:14:28 GMT</pubDate>
    </item>
    <item>
      <title>æ¥å—è®ºæ–‡æ‘˜è¦çš„ä¼šè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpr46m/conferences_for_accepting_abstract_papers/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æœ‰ä»»ä½•ä¼šè®®/ç ”è®¨ä¼šæ¥å—æ‘˜è¦è®ºæ–‡å—ï¼Ÿæˆ‘ç°åœ¨å…¨èŒå·¥ä½œã€‚æˆ‘æ²¡æœ‰å¤ªå¤šæ—¶é—´è¿›è¡Œå®éªŒï¼Œä½†æˆ‘æœ‰ä¸€äº›æƒ³è¦å‘è¡¨çš„æƒ³æ³•ï¼Œæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ    æäº¤äºº    /u/Blasphemer666   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpr46m/conferences_for_accepting_abstract_papers/</guid>
      <pubDate>Mon, 30 Dec 2024 16:33:51 GMT</pubDate>
    </item>
    <item>
      <title>è¿™é‡Œæœ‰ä¸šä½™çˆ±å¥½è€…æˆ–è€…ç‹¬ç«‹ç°å®çˆ±å¥½è€…å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpqy24/anybody_hobbyist_or_indie_rl_enthusiast_here/</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€äº›å…¸å‹è®¡ç®—æœºç§‘å­¦çš„èƒŒæ™¯å’Œç»éªŒï¼Œä½†åœ¨äººå·¥æ™ºèƒ½æ–¹é¢æ²¡æœ‰ä¸“ä¸šçŸ¥è¯†ã€‚æ‰€ä»¥æˆ‘ç§°è‡ªå·±ä¸ºä¸šä½™çˆ±å¥½è€…ã€‚æˆ‘å¯¹è§£å†³ç°å®ä¸–ç•Œçš„é—®é¢˜ä¸æ„Ÿå…´è¶£ï¼›æˆ‘æ»¡è¶³äºè¿½éšå¤§å¸ˆåœ¨å›½é™…è±¡æ£‹æˆ–äº”å­æ£‹ç­‰å·²ç»å¾æœçš„é¢†åŸŸçš„æˆå°±ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘æƒ³å°† RL åº”ç”¨äºåŒäººå‚ä¸æŠ½è±¡ç­–ç•¥æ¸¸æˆã€‚è¿™ä¸ª subreddit ä¸Šæœ‰æ²¡æœ‰äººå°è¯•è¿‡ç±»ä¼¼çš„ä¸œè¥¿ï¼Ÿ    æäº¤äºº    /u/Gloomy-Status-9258   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpqy24/anybody_hobbyist_or_indie_rl_enthusiast_here/</guid>
      <pubDate>Mon, 30 Dec 2024 16:26:32 GMT</pubDate>
    </item>
    <item>
      <title>æ¯”è¾ƒ RL ä»£ç†çš„æŒ‡æ ‡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpqsur/metrics_for_comparing_rl_agents/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ğŸ‘‹ æˆ‘æ­£åœ¨ä¸€ä¸ªå°å‹å¤§å­¦é¡¹ç›®ä¸Šå·¥ä½œï¼Œè¯¥é¡¹ç›®åœ¨å¤ªç©ºä¾µç•¥è€…çš„èƒŒæ™¯ä¸‹æ¢ç´¢å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘æƒ³å°†ä¼ ç»Ÿçš„ Q-Learning ä»£ç†ä¸ DQN è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶ä¸”æ­£åœ¨è€ƒè™‘ä½¿ç”¨å“ªäº›æŒ‡æ ‡è¿›è¡Œåˆ†æã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å·²å†³å®šç»˜åˆ¶ï¼š  æ¯é›†å¾—åˆ† æ¯é›†å¹³å‡å¥–åŠ± æ¯é›†å¹³å‡æ¸¸æˆæ—¶é—´  æˆ‘è¿˜åœ¨è€ƒè™‘ç»˜åˆ¶å¹³å‡ Q å€¼ã€‚ä½†æ˜¯ï¼Œæˆ‘å¯¹è¿™æ˜¯å¦åˆé€‚æœ‰äº›æ€€ç–‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä¸ç¡®å®šå¦‚ä½•è§£é‡Šç”±äºæ¯é›†æ­¥éª¤æ•°çš„å·®å¼‚ï¼ŒQ å€¼åœ¨å„é›†ä¹‹é—´å¯èƒ½ä¼šæœ‰æ˜¾è‘—å·®å¼‚è¿™ä¸€äº‹å®ã€‚ é™„æ³¨ï¼šæˆ‘å®Œå…¨æ¸…æ¥š Q-Learning æ˜¯ä¸€ç§è¡¨æ ¼æ–¹æ³•ï¼Œä¸å¤ªé€‚åˆå…·æœ‰å¤§çŠ¶æ€ç©ºé—´çš„ç¯å¢ƒã€‚è¿™ä¸€é™åˆ¶å°†æ˜¯æˆ‘è¿›è¡Œæ¯”è¾ƒåˆ†æçš„å…³é”®éƒ¨åˆ†ã€‚ æå‰è‡´è°¢ï¼    æäº¤äºº    /u/Lonely-Eye-8313   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpqsur/metrics_for_comparing_rl_agents/</guid>
      <pubDate>Mon, 30 Dec 2024 16:20:07 GMT</pubDate>
    </item>
    <item>
      <title>è§¦å‘ç¯å¢ƒé‡ç½®çš„æ˜ç¡®å¥–åŠ±[ä½“è‚²é¦†å’Œç¨³å®šåŸºçº¿3]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpnubr/explicit_reward_for_triggering_env_reset/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æå‰æ„Ÿè°¢å¤§å®¶çš„å¸®åŠ©ï¼ å½“æˆ‘çš„ä»£ç†å¯¼è‡´ç¯å¢ƒé‡ç½®ï¼ˆä½äºé˜ˆå€¼ï¼‰æ—¶ï¼Œæˆ‘æƒ³åº”ç”¨ç‰¹å®šæƒ©ç½šã€‚æˆ‘ä¸æ˜ç™½çš„æ˜¯ï¼Œæˆ‘å¯ä»¥æ­£ç¡®è§¦å‘é‡ç½®ï¼Œä½†æƒ©ç½šæ²¡æœ‰åº”ç”¨ï¼Œå¥–åŠ±æ˜¯æŒ‰å¸¸è§„è®¡ç®—çš„ã€‚å¦‚æœæ‚¨èƒ½æŒ‡å‡ºæˆ‘åœ¨æŸå¤„è¯¯è§£äº†ç»“æ„çš„è¯ï¼Œé‚£å°±å¤ªå¥½äº†ï¼š) step() ä¼ªä»£ç ï¼š #action æå– #action å¤„ç† #updating å€¼ #reward è®¡ç®— # penalty æ£€æŸ¥ if value1 &lt;= Threshold: Terminated = True self.reward = -200 # Override reward with penalty observer = self._get_observation() return observer, self.reward, Terminated, Triuncated, {}     submitted by    /u/Much_Razzmatazz_6641   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpnubr/explicit_reward_for_triggering_env_reset/</guid>
      <pubDate>Mon, 30 Dec 2024 14:02:13 GMT</pubDate>
    </item>
    <item>
      <title>æ— é™æ­¦è£…è€è™æœºé—®é¢˜çš„é—æ†¾ä¸‹é™</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpalfe/lower_bound_on_regret_in_infinitelyarmed_bandit/</link>
      <description><![CDATA[åªè¦è‡‚ç©ºé—´æ˜¯è¿ç»­/æ— é™çš„ï¼Œæˆ–è€…æˆ‘ä»¬ä¸å¯¹æ‹‰åŠ¨æ¯ä¸ªè‡‚çš„æ•ˆç”¨åšä»»ä½•å‡è®¾ï¼Œé—æ†¾çš„ä¸‹é™åº”è¯¥æ˜¯ Omega(T)ï¼Œå¯¹å—ï¼ˆå‡è®¾å¥–åŠ±åœ¨ [0, 1] ä¹‹é—´ï¼‰ï¼Ÿ ä¼—æ‰€å‘¨çŸ¥ï¼Œåœ¨ K è‡‚è€è™æœºé—®é¢˜ä¸­ï¼ŒT è½®ç´¯ç§¯é—æ†¾çš„ä¸‹é™æ˜¯ Omega(sqrt(KT))ã€‚å¦‚æœæ²¡æœ‰å¯¹æ•ˆç”¨åšä»»ä½•å‡è®¾ï¼Œåªæ˜¯ç»™å®šä¸€ä¸ªè‡‚ï¼Œåˆ™è¯¥è‡‚çš„å¥–åŠ±æ˜¯ç‹¬ç«‹ä¸”ç›¸åŒåˆ†å¸ƒçš„ã€‚åœ¨æ— é™å¤šè‡‚è€è™æœºä¸­ï¼Œæˆ‘ä»¬æœ‰ K è¶‹å‘äºæ— ç©·å¤§ï¼Œå› æ­¤ Omega(sqrt(KT)) ä¸‹é™å˜ä¸ºæ— ç•Œï¼Œå¹¶ä¸”æˆ‘ä»¬çŸ¥é“å¯¹äºè€è™æœºé—®é¢˜ï¼Œé—æ†¾æœ€å¤šä¸º Tï¼Œå› æ­¤æœ€åæƒ…å†µé—æ†¾çš„ä¸‹é™åº”è¯¥æ˜¯ Omega(T)ã€‚  æˆ‘æ²¡è§è¿‡ä»»ä½•åœ°æ–¹æœ‰è¿™æ ·çš„è¯´æ³•ï¼Œä½†ä¹Ÿè®¸å¾ˆæ˜æ˜¾æ²¡æœ‰è¿™æ ·è¯´ã€‚ä¸è¿‡ï¼Œæˆ‘æ˜ç™½è¿™å¹¶ä¸æ„å‘³ç€åœ¨æ¯ä¸ªè¿ç»­ä½“æˆ–æ— é™å¤šçš„æ­¦è£…åŒªå¾’é—®é¢˜ä¸­é—æ†¾éƒ½æ˜¯ Omega(T)ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥å¯¹å¯¼è‡´æ›´ä¸¥æ ¼é—æ†¾ç•Œé™çš„æ•ˆç”¨åšå‡ºå‡è®¾ã€‚    æäº¤äºº    /u/Anxious_Positive3998   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpalfe/lower_bound_on_regret_in_infinitelyarmed_bandit/</guid>
      <pubDate>Mon, 30 Dec 2024 00:42:27 GMT</pubDate>
    </item>
    <item>
      <title>è¿·å®«/æœ€çŸ­è·¯å¾„ç¯å¢ƒçš„å¥–åŠ±ç»“æ„</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hp8ns5/reward_structure_for_maze_shortest_path/</link>
      <description><![CDATA[å—¨ï¼Œ æˆ‘æ­£åœ¨æ„å»ºä¸€ä¸ªç¯å¢ƒï¼Œç©å®¶å¿…é¡»ç©¿è¿‡ä¸€ä¸ªéšæœºç”Ÿæˆçš„è¿·å®«ï¼Œæ¿€æ´»è¿·å®«éšæœºéƒ¨åˆ†çš„å¼€å…³ï¼Œç„¶åå¯¼èˆªåˆ°å‡ºå£ã€‚ç©å®¶çš„ç§»åŠ¨æ˜¯è¿ç»­çš„ï¼ˆå³ä¸æ˜¯åŸºäºç½‘æ ¼çš„ï¼‰ã€‚ æˆ‘ä¸€ç›´åœ¨ç ”ç©¶ä¸€ç§ä¿¡æ¯ä¸°å¯Œçš„ã€æœ‰å½¢çŠ¶çš„å¥–åŠ±ç»“æ„ï¼Œä»¥é¼“åŠ±å­¦ä¹ æœ€çŸ­è·¯å¾„ã€‚ ç›®å‰ï¼Œæˆ‘çš„å¥–åŠ±ç»“æ„å¦‚ä¸‹ï¼š - æ¯å¸§å‡ 0.01 - æ ¹æ®ç©å®¶ä¸ç›®æ ‡ï¼ˆå‡ºå£å¼€å…³æˆ–å‡ºå£é—¨ï¼Œå¦‚æœå¼€å…³è¢«æ¿€æ´»ï¼‰ä¹‹é—´è·ç¦»ä¸å‰ä¸€æ­¥çš„å·®å¼‚ï¼Œæ¯å¸§ç»™äºˆå°å¹…ï¼ˆ&lt;0.5ï¼‰å¥–åŠ±æˆ–æƒ©ç½š - åˆ°è¾¾å‡ºå£ç»™äºˆå¤§é¢å¥–åŠ±ï¼ˆ10ï¼‰ åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘å°†å¥–åŠ±æ ‡å‡†åŒ–ä¸º 0 åˆ° 1 ä¹‹é—´ã€‚ä½†æ˜¯ï¼Œè¿™é‡Œä¼¼ä¹å¯èƒ½å­˜åœ¨ä¸€äº›å†—ä½™ï¼Œæˆ‘æƒ³é—®é—®å¤§å®¶çš„æƒ³æ³•ï¼Œä»¥åŠæ˜¯å¦æœ‰æ›´å¥½çš„æ–¹å¼æ¥æ„å»ºå¥–åŠ±ã€‚ ä½œä¸ºå‚è€ƒï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¸¸æˆ N++ çš„ç¯å¢ƒã€‚ è°¢è°¢å¤§å®¶çš„å¸®åŠ©ï¼    ç”±    /u/Tetramputechture  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hp8ns5/reward_structure_for_maze_shortest_path/</guid>
      <pubDate>Sun, 29 Dec 2024 23:10:51 GMT</pubDate>
    </item>
    <item>
      <title>RL ä¹¦ç±</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hp4nb7/rl_books/</link>
      <description><![CDATA[æˆ‘å¼€å§‹å­¦ä¹  RLã€‚è¿™ä¸ªé¢†åŸŸæœ€å¥½çš„ä¹¦ç±æˆ–æ–‡ç« æ˜¯ä»€ä¹ˆã€‚    æäº¤äºº    /u/fg-dev   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hp4nb7/rl_books/</guid>
      <pubDate>Sun, 29 Dec 2024 20:12:58 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘å¦‚ä½•ä½¿ç”¨ carla è¿›è¡Œ RLï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</link>
      <description><![CDATA[æˆ‘çš„æ¯•ä¸šè®¾è®¡ç”¨carlaå®Œæˆå¼ºåŒ–å­¦ä¹ ï¼Œèƒ½æ¨èä¸€äº›åœ¨çº¿è¯¾ç¨‹å—ï¼Ÿ    submitted by    /u/Clean_Tip3272   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</guid>
      <pubDate>Sun, 29 Dec 2024 10:36:18 GMT</pubDate>
    </item>
    <item>
      <title>é—æ†¾å€¼ä¸º O( \sqrt(log K T ) ) çš„ K è‡‚éšæœºèµŒåšæœºç®—æ³•ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</link>
      <description><![CDATA[æˆ‘æƒ³çŸ¥é“æ˜¯å¦æœ‰ä»»ä½• K è‡‚éšæœºè€è™æœºç®—æ³•å¯ä»¥å®ç° $O(\sqrt(T))$ é—æ†¾ï¼Œä¸”å› å­ä¸ºå¸¸æ•° $\sqrt{ log K }$ã€‚  æˆ‘çŸ¥é“ exp3 å¯ä»¥å®ç° O(\sqrt(T)) é—æ†¾ï¼Œå› å­ä¸º sqrt(k log K )ï¼Œè€Œ UCB å¯ä»¥å®ç° \tilde{O}( sqrt(T) ) é—æ†¾ï¼Œå› å­ä¸º sqrt(k)ï¼Ÿ  æ˜¯å¦æœ‰ä¸€ç§ç®—æ³•ï¼Œå…¶è‡‚æ•°ä¸ sqrt( log K ) ç±»ä¼¼ï¼Ÿæˆ–è€…ï¼Œæ˜¯å¦æœ‰æ›´ä¸¥æ ¼çš„ exp3 æˆ– UCB åˆ†æï¼Œå¯ä»¥åœ¨è‡‚æ•°æ–¹é¢å®ç°æ›´å¥½çš„å› å­ï¼Ÿ  æˆ‘æ­£åœ¨ç ”ç©¶ä¸€ä¸ªé—®é¢˜ï¼Œå…¶ä¸­è‡‚çš„æ•°é‡ä¸º K^{a}ï¼Œå…¶ä¸­ a æ˜¯æŸä¸ªå‚æ•°ï¼Œå¹¶ä¸”æˆ‘æƒ³å°†æˆ‘çš„å› å­å½’ç»“ä¸ºç±»ä¼¼ a * poly(K) - ï¼ˆpoly(K) è¡¨ç¤ºå…³äº K çš„å¤šé¡¹å¼ï¼‰çš„ä¸œè¥¿ã€‚    æäº¤äºº    /u/Anxious_Positive3998   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</guid>
      <pubDate>Sun, 29 Dec 2024 05:59:36 GMT</pubDate>
    </item>
    <item>
      <title>RLâ€œåŒ…è£¹â€ 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</link>
      <description><![CDATA[æˆ‘é€šå¸¸ä¼šåœ¨å‡æœŸçš„æœ€åå‡ å¤©åŠªåŠ›èµ¶ä¸Šè¿›åº¦ï¼ˆäº‹å®è¯æ˜è¿™äº›å¤©ä¸å¯èƒ½ï¼‰å¹¶å›é¡¾å­¦æœ¯å’Œå·¥ä¸šå‘å±•æ–¹é¢çš„ä¸»è¦äº®ç‚¹ã€‚è¯·åœ¨æ­¤å¤„æ·»åŠ æ‚¨ä»Šå¹´çš„é¡¶çº§ RL ä½œå“     æäº¤äºº    /u/blitzkreig3   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</guid>
      <pubDate>Sat, 28 Dec 2024 21:08:08 GMT</pubDate>
    </item>
    </channel>
</rss>