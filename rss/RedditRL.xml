<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 15 Nov 2024 01:19:54 GMT</lastBuildDate>
    <item>
      <title>Yann LeCun 仍然不认为 RL 对 AI 系统至关重要。他认为只有无监督/监督学习/SSL 算法才能处理 RL 所用到的问题类型，例如顺序决策，或者它们将如何处理诸如探索之类的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</link>
      <description><![CDATA[    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</guid>
      <pubDate>Fri, 15 Nov 2024 00:44:11 GMT</pubDate>
    </item>
    <item>
      <title>Catan AI 所需的规格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gra5pz/specs_needed_for_catan_ai/</link>
      <description><![CDATA[嗨，我正在训练一个人工智能来玩 Catan 棋盘游戏，我需要一些建议，关于进行训练的计算机需要什么规格。我可能会使用 pytorch 或 tenserflow 进行训练。有什么想法吗？ 我考虑租用虚拟机进行训练，你推荐吗？    提交人    /u/FunMetJoel   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gra5pz/specs_needed_for_catan_ai/</guid>
      <pubDate>Thu, 14 Nov 2024 17:29:53 GMT</pubDate>
    </item>
    <item>
      <title>有人有 DreamerV3 实现吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gr5ixz/anybody_has_a_dreamerv3_implementation/</link>
      <description><![CDATA[大家好 r/reinforcementlearning， 我正在尝试使用 DreamerV3 模型，这是迄今为止性能最高的 RL 模型。 问题是，它的代码是一个自我实现的半 Jax 半 numpy 半 python 操作；有自定义线程管理（使用 Jax 时），以及大多数开箱即用的 ML 库支持的许多其他代码。它很难使用。 有人有 jittable jax 实现吗？我有一个用 Jax 编写的环境，因此对其进行研究完全有意义，其他许多研究人员也是如此。 也许有人可以分享/开源他们的实现？ 干杯。    提交人    /u/JustZed32   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gr5ixz/anybody_has_a_dreamerv3_implementation/</guid>
      <pubDate>Thu, 14 Nov 2024 14:06:52 GMT</pubDate>
    </item>
    <item>
      <title>上帝说，我们需要意识到，不要把领导者放在我们心中的神坛上，但当他做好事时，支持他是正确的，在我看来</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gr5ic3/god_said_that_we_need_to_be_aware_not_to_put/</link>
      <description><![CDATA[当人们的自尊心受到触动时，他们往往会走向极端。 左走向极左 右走向极右。 在某些时候，我们不会互相倾听，开始传播仇恨。 也害怕彼此。    提交人    /u/Timur_1988   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gr5ic3/god_said_that_we_need_to_be_aware_not_to_put/</guid>
      <pubDate>Thu, 14 Nov 2024 14:06:02 GMT</pubDate>
    </item>
    <item>
      <title>基于脉冲神经网络的强化学习模型性能提升建议 [P] [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gr2yyl/advice_for_improving_the_performance_of_my/</link>
      <description><![CDATA[大家好！我正在开展一个项目，专注于使用脉冲神经网络 (SNN) 训练强化学习代理。我的目标是提高模型的性能，尤其是通过“做梦”体验（离线训练）有效学习的能力。 简要项目背景（基于模型的强化学习）： 代理与环境（游戏 Pong）交互，在主动训练阶段（“清醒”）和离线学习的“做梦”阶段之间交替。 我面临的挑战： 学习速度很慢，而且有些不稳定。我尝试了一些优化，但仍然没有达到理想的性能。具体来说，我注意到增加网络（代理和模型）中的神经元数量并没有提高性能；在某些情况下，甚至会恶化。我降低了模型的学习率，但没有看到任何改进。我还通过在清醒阶段禁用学习来测试模型，以仅查看其在做梦阶段的行为。我发现模型在 1-2 个梦中有所改进，但当达到 3 个梦时性能会下降。 问题：  您是否知道任何可以在 SNN 环境中提高模型稳定性和收敛性的技术？ 您有什么建议或意见吗？ 使用重放缓冲区会有所帮助吗？     提交人    /u/Embri21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gr2yyl/advice_for_improving_the_performance_of_my/</guid>
      <pubDate>Thu, 14 Nov 2024 11:45:48 GMT</pubDate>
    </item>
    <item>
      <title>美国有哪些教授和实验室在机器人强化学习方面进行了出色的研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqzdjf/any_professors_labs_doing_good_research_in/</link>
      <description><![CDATA[我正在美国申请硕士学位，我不知道哪位教授或学院最适合机器人强化学习。    提交人    /u/Different_Prune_9756   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqzdjf/any_professors_labs_doing_good_research_in/</guid>
      <pubDate>Thu, 14 Nov 2024 07:14:53 GMT</pubDate>
    </item>
    <item>
      <title>有人成功实现了 DeepMind、INRIA 等论文《无模型强化学习中的反事实信用分配》吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqyuqs/anyone_found_success_implementing_the_paper/</link>
      <description><![CDATA[我一直在尝试实现这篇论文的连续动作版本 https://arxiv.org/pdf/2011.09464 。任何成功实现这项工作并愿意分享如何实现它的见解的人。我已经实现了这项工作的连续动作版本，但得到了好坏参半的结果，现在不确定我是否正确地实现了它。    提交人    /u/AvisekEECS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqyuqs/anyone_found_success_implementing_the_paper/</guid>
      <pubDate>Thu, 14 Nov 2024 06:37:06 GMT</pubDate>
    </item>
    <item>
      <title>真正从事 RL 研究员工作的人们，你们是如何获得这份工作的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gquq8o/people_who_really_work_as_an_rl_researcher_how/</link>
      <description><![CDATA[真正从事 RL 研究员工作的人（主要从事 RL 项目）。   您在哪里工作？ 您什么时候得到这份工作的？ 您的背景？  我的博士研究主要涉及 RL，但我现在以 MLE 的身份从事各种 ML/DL/RL 项目。我提交了几份申请，通过了最后的面试，但作为行业中的 RL 研究员，我还是不够格。 LinkedIn 上真正纯粹与 RL 相关的工作总是少于 30 个。 我想知道人们如何获得纯粹的 RL 研究员工作？    提交人    /u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gquq8o/people_who_really_work_as_an_rl_researcher_how/</guid>
      <pubDate>Thu, 14 Nov 2024 02:37:18 GMT</pubDate>
    </item>
    <item>
      <title>PPO 之后是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqr1k3/whats_after_ppo/</link>
      <description><![CDATA[我最近完成了从 PyTorch 实现 PPO 以及任何看似相关的实现细节（vec envs、GAE lambda）。我还做了少量的行为克隆 (DAgger) 和多智能体强化学习 (IPPO)。 我想知道是否有人对下一步该怎么做有指示或建议？也许你正在研究某些东西，对 PPO 的改进是我完全错过的，或者只是一篇有趣的读物。到目前为止，我的兴趣只是游戏 AI。    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqr1k3/whats_after_ppo/</guid>
      <pubDate>Wed, 13 Nov 2024 23:37:38 GMT</pubDate>
    </item>
    <item>
      <title>“当你的人工智能欺骗你时：从人类反馈进行强化学习的部分可观察性挑战”，Lang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqibc6/when_your_ais_deceive_you_challenges_of_partial/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqibc6/when_your_ais_deceive_you_challenges_of_partial/</guid>
      <pubDate>Wed, 13 Nov 2024 17:27:54 GMT</pubDate>
    </item>
    <item>
      <title>帮助在 SARC 中实现 Q 误差跟踪（RL 研究）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqa0y9/help_with_implementing_qerror_tracking_in_sarc_rl/</link>
      <description><![CDATA[大家好， 我一直在研究 SARC，这是我对强化学习研究的一部分，并且专注于实现回顾性 Q 误差跟踪，类似于 SARC 论文第 6.3 节中概述的内容。 在我的实验中，我注意到 SARC 结果文件夹中有三个文件： - `q_losses_wrt_retroQvals_retroStates.pkl` - `q_losses_wrt_td3Qvals_td3States.pkl` - `demos.pkl` 我查看了训练代码和支持文件（例如 `core.py`、`retro_loss.py`、`logx.py`），但找不到这些特定文件的创建位置。我想复制此设置来跟踪和验证我工作中的 Q 误差收敛。  通过定期跟踪 Q 值来自己实现这一点是否有意义，或者我可能错过了哪些特定代码？此外，如果这里有人有这方面的经验，我的方法对于未来的期刊审稿人来说是否足够可靠，或者他们可能会提出担忧？  这是 [SARC GitHub 链接](https://github.com/sukritiverma1996/SARC) 和 [SARC 论文的 PDF](https://arxiv.org/abs/2306.16503)。 提前感谢您提供的任何见解！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqa0y9/help_with_implementing_qerror_tracking_in_sarc_rl/</guid>
      <pubDate>Wed, 13 Nov 2024 10:36:05 GMT</pubDate>
    </item>
    <item>
      <title>DPG 算法是基于策略的还是演员评论家的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gpje0r/is_dpg_algorithm_policybased_or_actorcritic/</link>
      <description><![CDATA[我有一个问题，即确定性策略梯度算法的基本形式是基于策略的还是演员评论家。我一直在寻找答案，在某些情况下，它说它是基于策略的，而在其他情况下，它并没有明确地说它是一个演员评论家，但它使用演员评论家框架来优化策略，因此我怀疑什么是策略改进方法。 我知道演员评论家方法本质上是基于策略的方法，并增加了评论家来提高学习效率和稳定性。    提交人    /u/Street-Vegetable-117   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gpje0r/is_dpg_algorithm_policybased_or_actorcritic/</guid>
      <pubDate>Tue, 12 Nov 2024 12:17:32 GMT</pubDate>
    </item>
    <item>
      <title>通过 RL 实现训练语言模型进行自我纠正——寻找测试人员和反馈！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gphaav/implementation_of_training_language_models_to/</link>
      <description><![CDATA[嗨， 我最近创建了论文通过强化学习训练语言模型进行自我纠正的最小 PyTorch 实现。 但是，我是将 RL 应用于语言模型的新手，不确定它是否正确实现。我希望社区能够帮助测试和改进它！  我需要帮助的内容：  测试：我的设置有限，因此如果有更多计算能力的人可以运行实验并提供反馈，我将不胜感激。 调试：这仍然是新鲜的，因此可能存在我尚未发现的错误。 优化速度：如果您有加快速度的想法，我很乐意听取！  如果能使此实现尽可能高效和有效，那就太好了，任何帮助都将不胜感激！ 查看：GitHub  提前致谢！    提交人    /u/sedidrl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gphaav/implementation_of_training_language_models_to/</guid>
      <pubDate>Tue, 12 Nov 2024 09:54:22 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个 RL 代理来在月球表面软着陆:)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gpd2uw/i_created_a_rl_agent_to_soft_land_in_lunar_surface/</link>
      <description><![CDATA[        由    /u/Few_Tooth_2474  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gpd2uw/i_created_a_rl_agent_to_soft_land_in_lunar_surface/</guid>
      <pubDate>Tue, 12 Nov 2024 04:55:24 GMT</pubDate>
    </item>
    <item>
      <title>轻松在 SMAC 和 MAMuJoCo 上记录离线数据，然后进行离线训练（离线 MARL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gotmkd/easily_record_offline_data_on_smac_and_mamujoco/</link>
      <description><![CDATA[大家好，我是南非的一名博士生，研究离线多智能体强化学习。我正在维护一个名为 Off-the-Grid MARL (og-marl) 的 GitHub 项目，该项目为离线 MARL 提供数据集和基线算法。我希望它可以帮助其他人开始该领域。我最近制作了一个快速的 Google Colab 笔记本来演示 og-marl 中的一些功能。我想这个社区中的一些人可能有兴趣查看它。在笔记本中，我演示了如何在 SMAC 或 MAMuJoCo 上在线训练 MARL 算法、记录数据、分析数据并在其上训练离线 MARL 算法。 https://colab.research.google.com/drive/1bfc7-tMLYmbKwh7HiqPzXU3f62tOuTY7?usp=sharing 如果您有兴趣进入离线 MARL，请随时通过 GitHub 与我们联系。我很乐意提供帮助。    提交人    /u/OfflineMARL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gotmkd/easily_record_offline_data_on_smac_and_mamujoco/</guid>
      <pubDate>Mon, 11 Nov 2024 14:37:18 GMT</pubDate>
    </item>
    </channel>
</rss>