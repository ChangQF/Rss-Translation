<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 04 Aug 2024 03:16:33 GMT</lastBuildDate>
    <item>
      <title>RL 模型调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejkhnp/rl_model_survey/</link>
      <description><![CDATA[我想知道用于少数任务的模型的粗略大小。您在哪个项目中使用了什么模型大小？您的数据规模是多少？一些具有里程碑意义的 RL 里程碑的模型大小是多少？    提交人    /u/SandSnip3r   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejkhnp/rl_model_survey/</guid>
      <pubDate>Sun, 04 Aug 2024 02:21:33 GMT</pubDate>
    </item>
    <item>
      <title>当奖励和下一个状态部分随机时，实现 DQN 的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejghpd/best_way_to_implement_dqn_when_reward_and_next/</link>
      <description><![CDATA[我对机器学习还很陌生，我给自己设定了使用机器学习来解决宝石迷阵的任务，从阅读中可以看出强化学习是最好的方法，而形状为 (8, 8, 6) 的棋盘有 112 个动作，对于 q 表来说太大了。我想我需要使用 DQN 来近似 q 值 我想我已经掌握了基础知识，但我不确定如何定义宝石迷阵中的奖励和下一个状态，例如成功移动时。新的方块会随机添加到棋盘上，因此存在一系列可能的下一个状态。而且由于这些新方块也可以得分，因此也存在一系列可能的分数。 我是否应该假设模型能够在训练期间内部平均这些类似状态的不同奖励，或者我应该实施某种措施来解释随机性。也许就像对 10 种不同可能结果的奖励进行平均，但我不确定下一个状态该使用哪一种。  如有任何帮助或指点，不胜感激 此外，这对于模型来说看起来还可以吗  self.conv1 = nn.Conv2d(6, 32, kernel_size=5, padding=2) self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) self.conv_v = nn.Conv2d(64, 64, kernel_size=(8, 1), padding=(0, 0)) self.fc1 = nn.Linear(64 * 8 * 8, 512) self.fc2 = nn.Linear(512, num_actions)  我的目标是一次匹配最多 5 个单元，因此最初使用 5x5 卷积。并且由于单元格向下移动，因此模型还需要垂直匹配模式，因此需要 (8,1) 卷积    提交人    /u/Lokipi   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejghpd/best_way_to_implement_dqn_when_reward_and_next/</guid>
      <pubDate>Sat, 03 Aug 2024 23:00:51 GMT</pubDate>
    </item>
    <item>
      <title>更大的 RL 模型总是更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejek2r/are_larger_rl_models_always_better/</link>
      <description><![CDATA[大家好，我目前正在我的自定义 RL 环境中尝试使用 stablebaselines 3 中不同大小的 PPO 模型。我假设较大的模型总是比较小的模型更好地最大化平均奖励。但我的环境/奖励函数似乎恰恰相反。这是正常的还是表示有错误？ 此外，训练/学习时间如何随模型大小而变化？是不是一个大得多的模型需要比小模型长 10 到 100 倍的训练时间，而只需更长的训练时间就可以解决我的问题？ 作为参考，该任务与本文中的情况非常相似 https://github.com/yininghase/multi-agent-control。当我谈论小模型时，我指的是 2 个 64 层的模型，而大模型则是 ~5 个 512 层的模型。 谢谢你的帮助 &lt;3    提交人    /u/Adorable-Spot-7197   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejek2r/are_larger_rl_models_always_better/</guid>
      <pubDate>Sat, 03 Aug 2024 21:32:48 GMT</pubDate>
    </item>
    <item>
      <title>具有视觉输入的多模态模型玩口袋妖怪（尝试世界纪录）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej8rz0/multimodal_model_with_vision_input_plays_pokemon/</link>
      <description><![CDATA[       由    /u/nicimunty  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej8rz0/multimodal_model_with_vision_input_plays_pokemon/</guid>
      <pubDate>Sat, 03 Aug 2024 17:21:37 GMT</pubDate>
    </item>
    <item>
      <title>“NAVIX：使用 JAX 扩展 MiniGrid 环境”，Pignatelli 等人，2024 年（将代理 + 环境打包到 GPU 上以获得 OOM 收益）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej3upp/navix_scaling_minigrid_environments_with_jax/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej3upp/navix_scaling_minigrid_environments_with_jax/</guid>
      <pubDate>Sat, 03 Aug 2024 13:52:09 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Zero V2 为何有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej1m0s/why_does_efficient_zero_v2_work/</link>
      <description><![CDATA[ 如果价值函数知道更好的动作，它不会已经按照这种方式训练策略了吗？ 如果它不知道更好的动作，它会不会错误地评价状态或动作，从而导致在蒙特卡洛树反向传播过程中进行错误评估？     提交人    /u/Automatic-Web8429   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej1m0s/why_does_efficient_zero_v2_work/</guid>
      <pubDate>Sat, 03 Aug 2024 11:57:25 GMT</pubDate>
    </item>
    <item>
      <title>一项新调查——离线策略学习的生成模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eipuq3/a_new_survey_generative_models_for_offline_policy/</link>
      <description><![CDATA[请查看我们新的 TMLR 工作：用于离线策略学习的深度生成模型。本文彻底回顾了深度生成模型在离线强化学习和模仿学习中的应用。我们涵盖的模型包括 VAE、GAN、规范化流、Transformers、扩散模型。    提交人    /u/Ashamed-Put-2344   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eipuq3/a_new_survey_generative_models_for_offline_policy/</guid>
      <pubDate>Sat, 03 Aug 2024 00:30:15 GMT</pubDate>
    </item>
    <item>
      <title>EWRL 的选择性如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</link>
      <description><![CDATA[大家好， 正如标题所说，你们知道 EWRL 研讨会在接受论文方面有多挑剔吗？总的来说，你觉得这个研讨会好吗？一些个人故事将不胜感激。    由   提交  /u/sel20   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</guid>
      <pubDate>Fri, 02 Aug 2024 20:41:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Decision Transformer 在 OfflineRL 顺序决策领域有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</link>
      <description><![CDATA[谢谢。    提交人    /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</guid>
      <pubDate>Fri, 02 Aug 2024 15:02:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在 EPyMARL 中加载和测试训练模型以进行 MAPPO 算法评估？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eib4td/how_to_load_and_test_a_trained_model_in_epymarl/</link>
      <description><![CDATA[      大家好， 我是强化学习 (RL) 的新手，最近开始使用 EPyMARL 框架进行多智能体强化学习 (MARL)。我已经在我的健身房的自定义环境中使用 MAPPO 和其他类似算法成功训练了一个模型。现在，我想加载这个经过训练的模型并在我使用的案例中评估其性能，但我不确定如何做到这一点。 这是我用于训练的命令： python3 src/main.py --config=mappo --env-config=gymma with env_args.key=&quot;LoRaEnv-v0&quot;  下面是我尝试用于评估的命令： python3 src/main.py --config=mappo --env-config=gymma with env_args.key=&quot;LoRaEnv-v0&quot; checkpoint_path=&quot;my_save_model_path&quot; assess=True  我不确定这是否正确，或者我是否需要采取其他步骤，因为我尝试运行脚本但遇到了一些奇怪的错误。有人可以指导我如何正确加载和测试我训练过的模型以使用 EPyMARL 进行评估吗？ 以下是包含我保存的模型的目录的屏幕截图： 如能提供任何帮助或指点，我们将不胜感激！提前致谢！ 训练模型目录    提交人    /u/Interesting_Dingo983   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eib4td/how_to_load_and_test_a_trained_model_in_epymarl/</guid>
      <pubDate>Fri, 02 Aug 2024 14:14:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么代理没有学会如何到达立方体的位置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</link>
      <description><![CDATA[        提交人    /u/CoolestSlave   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</guid>
      <pubDate>Fri, 02 Aug 2024 12:17:29 GMT</pubDate>
    </item>
    <item>
      <title>CORL 和 D4RL 分数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</link>
      <description><![CDATA[我提前为一个可能微不足道的问题道歉： 在评估离线强化学习算法时，我们应该关心哪些分数？它们是 D4RL 标准化分数吗？还有其他需要考虑的吗？ 提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</guid>
      <pubDate>Thu, 01 Aug 2024 21:26:26 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型内部的 RL 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</link>
      <description><![CDATA[大家好。 我正在为纸牌游戏制作强化学习算法。我可以采取的可能动作取决于我手中的牌。如果我打出一张牌，我可以执行特定的操作。您可以假设这是一场 1v1 游戏。如果我的对手打出一张特定的牌，那么我必须决定给对手一张牌。 本质上，我希望有一个 RL 模型来决定打出哪张牌，然后另一个 RL 模型用于我必须给对手一张牌的特定情况。第一个 RL 模型将在每个回合激活，而第二个 RL 模型仅在我的对手打出迫使我给他一张自己的牌的特定牌时激活。 我想通过先与随机模型对战，然后再与自己对战来训练模型。 这可以在体育馆内完成吗？如果是这样，该怎么办？    提交人    /u/Practical-Resort7278   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</guid>
      <pubDate>Thu, 01 Aug 2024 12:30:12 GMT</pubDate>
    </item>
    <item>
      <title>既然离线 RL 与环境无关，为什么很多论文实现仍然基于 gym？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</link>
      <description><![CDATA[谢谢。    由   提交  /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:33 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为不同的游戏选择不同的 RL 算法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</link>
      <description><![CDATA[我是荷兰 6 VWO 的一名 16 岁学生，目前正在参与一个关于强化学习 (RL) 在各种电脑游戏中的应用的学校研究项目。我的主要研究问题是：电脑游戏的具体特征如何影响不同 RL 算法的有效性？ 子问题：  不同类型的电脑游戏有哪些具体特征？ 有哪些 RL 算法可用，它们的特点是什么？ 游戏特征如何影响 RL 算法的性能？  实践部分：我计划将不同的 RL 算法应用于各种游戏。我正在考虑的游戏是：  超级马里奥兄弟 贪吃蛇 国际象棋 赛车  算法标准：  具有显著差异的算法。 最好是新算法。  反馈问题：  考虑到这些游戏的独特特点，您能否推荐适合这些游戏的特定 RL 算法？ 您认为我选择的游戏适合研究不同 RL 算法的有效性吗？如果不适合，您会建议什么游戏？     提交人    /u/matmoet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</guid>
      <pubDate>Wed, 31 Jul 2024 23:23:28 GMT</pubDate>
    </item>
    </channel>
</rss>