<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 14 Jun 2024 21:15:06 GMT</lastBuildDate>
    <item>
      <title>“蚂蚁”机器人有货吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dfzfqe/ant_robot_availability/</link>
      <description><![CDATA[你们中的一些人可能知道经典的 Ant 环境，它在 RL 中提供了一个相对容易学习的 3D 物理问题。有人知道“蚂蚁”机器人的实际物理实现是否存在吗？我试着四处寻找，但找不到任何东西（当然，搜索起来有点困难）。我愿意自己建造它，特别是如果有一种相对简单的方法来 3D 打印它的组件，但我真的希望能够执行迁移学习，将经过 MuJoCo 模拟训练的模型放在物理机器人中运行，这需要模型/机器人非常相似。 如果没有，有人推荐类似的机器人吗？我认为找到一个可用的机器人并基于它创建一个 MuJoCo 模型比反过来更容易，但理想情况下，已经存在为这种工作设置的东西。我特别想专注于这种运动，所以机器人手/手臂或车辆之类的东西对我来说不起作用。    提交人    /u/Nater5000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dfzfqe/ant_robot_availability/</guid>
      <pubDate>Fri, 14 Jun 2024 19:44:37 GMT</pubDate>
    </item>
    <item>
      <title>基于SAC CrossQ的高效连续控制代理示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dfqfsc/a_sample_efficient_continuous_control_agent_based/</link>
      <description><![CDATA[大家好，我想我的（业余）RL 研究取得了一些进展。最近有一些很棒的论文，受这些论文的启发*我创建了一个代理，它似乎比 CrossQ 学习得更快，而且可能更一致，在 WalkerHardcore 等环境中（~500k 步解决）。但我不确定——使用 3060Ti 需要时间来运行所需的各种实验和消融，更不用说超参数优化了。  如果其他人对这个问题感兴趣，代理是 sac_crossq_bro.py（WIP！）：https://github.com/modelbased/minirllab 并让我知道你是如何找到它的。 初步测试表明它对高重放率稳定，并且受益于更大的 Q 模型尺寸。我怀疑 CrossQ 引入的批量重正则化是正则化和限制可塑性损失所需要的全部，但同样，目前还不确定。 如果这是正确的，这个代理将是对 CrossQ 的简单修改，部分灵感来自 BRO、SR-SAC、BBF 等，但保持了 CrossQ 相对计算高效的方法。 也非常有兴趣听取改进建议。 * 该代理的有影响力的论文，强烈推荐，无特定顺序： CrossQ：深度强化学习中的批量归一化，以提高样本效率和简单性 https://arxiv.org/abs/1902.05605 通过再生正则化在持续学习中保持可塑性 https://arxiv.org/abs/2308.11958 深度持续学习中的可塑性损失https://arxiv.org/abs/2306.13812 更大、更好、更快：具有人类水平效率的人类水平 Atari https://arxiv.org/abs/2305.19452 BRO：更大、正则化、乐观：计算扩展和样本高效的连续控制 https://arxiv.org/abs/2405.16158 SR-SAC：通过打破重放比率障碍实现样本高效强化学习 https://openreview.net/forum?id=OpC-9aBBVJe 过度估计、过度拟合和可塑性 https://arxiv.org/abs/2403.00514     提交人    /u/thiagoazevedo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dfqfsc/a_sample_efficient_continuous_control_agent_based/</guid>
      <pubDate>Fri, 14 Jun 2024 13:08:58 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 特工大师 Super Hang-On（世嘉 Genesis）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dfo1rz/dreamerv3_agent_masters_super_hangon_sega_genesis/</link>
      <description><![CDATA[我之前发布过我让模型玩过的最后一款游戏，大家似乎都很喜欢，所以我想我会发布这个。 我训练了一个 DreamerV3 模型，该模型能够完成 Sega Genesis 的 Super Hang-On 的所有 4 门课程。 DreamerV3 使用 SheepRL 的实现，在 64x64 像素的 RGB 游戏图像上进行训练，具有 4 帧跳过和无帧堆叠。使用了 4 个并行的 gym 环境。 在 AI 评论部分，Gym 环境基本上会返回一些关于游戏状态的数据，然后我将其形成为文本提示，然后将其输入到开源 LLM 中，这样它就可以对游戏玩法做出一些简单的评论，然后将其转换为 TTS，同时让 Whisper 模型将我的 SpeechToText 转换为这样我就可以与角色交谈（当我说出角色的名字时触发）。所有这些都连接到我制作的包含虚拟角色和环境的 UE5 应用程序中。 这是我的视频链接：https://www.youtube.com/watch?v=IxrNMrVxxCs    提交人    /u/disastorm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dfo1rz/dreamerv3_agent_masters_super_hangon_sega_genesis/</guid>
      <pubDate>Fri, 14 Jun 2024 10:54:45 GMT</pubDate>
    </item>
    <item>
      <title>MARL 中合作任务的奖励设置</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dflcpj/reward_setting_in_cooperative_tasks_in_marl/</link>
      <description><![CDATA[大家好，我有一个关于合作任务中的奖励设置的问题。例如，考虑一个任务，两个机器人必须使用它们的传感器到达复杂场景中的目标位置（同一个）。我在论文中看到有两种奖励设置：1- 每个机器人都会收到自己的单独奖励函数。2- 所有机器人都会收到相同的奖励，即所有单独奖励的总和。这些配置之间有什么区别，可能的结果是什么？提前谢谢大家，大家继续努力！    提交人    /u/Many_Reception_4921   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dflcpj/reward_setting_in_cooperative_tasks_in_marl/</guid>
      <pubDate>Fri, 14 Jun 2024 07:42:07 GMT</pubDate>
    </item>
    <item>
      <title>解决概率井字游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dfhcoh/solving_probabilistic_tictactoe/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dfhcoh/solving_probabilistic_tictactoe/</guid>
      <pubDate>Fri, 14 Jun 2024 03:24:41 GMT</pubDate>
    </item>
    <item>
      <title>奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1df8dub/reward_function/</link>
      <description><![CDATA[我正在为四阶传递函数设计一个自适应一阶控制器，并为此使用 RL。我可以访问系统在给定时间内的响应以及我正在随机化的设定点，到目前为止，我尝试使用积分平方误差和稀疏奖励作为奖励函数，但模型没有收敛，我正在使用 DDPG 算法。有关奖励函数建模或我应该选择提供什么作为观察结果的任何提示都将非常有帮助。谢谢     提交人    /u/abhishank1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1df8dub/reward_function/</guid>
      <pubDate>Thu, 13 Jun 2024 20:08:45 GMT</pubDate>
    </item>
    <item>
      <title>如何使用基于强化学习的模型（例如 Q 学习）构建等待时间优化模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1df2g2m/how_can_i_build_waiting_time_optimizing_model/</link>
      <description><![CDATA[我正在尝试建立电影院客户等待时间模拟模型，以下是客户可能采取的各种流程  到达剧院 排队买票 买票 排队等待检票 检票 决定是否购买优惠商品 购买优惠商品或直接就座  如何使用强化学习建立更切合实际的模拟模型？  我使用 SIMPY 离散事件模拟模型构建了它，如本 笔记本 中所述，它更符合现实生活场景，但当我尝试将基于 q 学习的模型与 openai gym 结合使用时，我得到了不切实际的值  如能提供任何见解、建议或示例，我们将不胜感激。谢谢！    提交人    /u/aiiguy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1df2g2m/how_can_i_build_waiting_time_optimizing_model/</guid>
      <pubDate>Thu, 13 Jun 2024 15:58:55 GMT</pubDate>
    </item>
    <item>
      <title>理解 MC 每次访问算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1df1o2w/understanding_mc_every_visit_algorithm/</link>
      <description><![CDATA[您好，我刚刚开始学习 RL 算法。我试图理解蒙特卡洛每次访问，但仍然有几个问题。 在蒙特卡洛每次访问中，要计算 V(s)，您是除以状态出现的情节数，还是除以所有情节中状态的访问次数。 谢谢！:)    提交人    /u/imraybot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1df1o2w/understanding_mc_every_visit_algorithm/</guid>
      <pubDate>Thu, 13 Jun 2024 15:25:27 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI Gym 步骤函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dezvei/openai_gym_step_function/</link>
      <description><![CDATA[大家好， 我有一个关于 OpenAI Gym 自定义环境实现中的步进函数的问题。 我在大学里做一个深度强化学习的小项目，想检查一下我的方法。 我正在尝试优化 Jetbot 的连续控制，使其尽可能靠近车道中心。对动作进行采样并计算该动作的奖励是没有意义的，因为系统需要一些时间来平衡变化（P-调节器需要一些时间才能达到目标值）。因此，我每 1 秒计算一次奖励。我还想让代理在采样新动作之前先驾驶一会儿。这就是为什么我实现了这个 obs/帧计数器，它会每 10 帧更改动作/对动作进行采样。这意味着我为 10 帧设置了相同的动作。这有效吗？我还没有找到步进函数在后台做什么。我想确保我没有覆盖在后台采样的任何操作。 def step(self, action): current_time = time.time() self.elapsed_time = current_time - self.start_time self.action_count += 1 self.current_timestep += 1 if self.frame_counter % 10 == 0 or self.current_action is None: self.current_action = action min_value = 0.8 max_value = 1.2 self.current_kp = min_value +(self.current_action[0] +1) * (max_value - min_value) / 2 print(f&quot;current kp :{self.current_kp:.3f}&quot;) action = self.current_action kp = self.current_kp xc1_norm = (self.current_vector[0] + self.current_vector[2] ) /2 xc2_norm = (self.current_vector[1] + self.current_vector[3] ) /2 penalty_norm = abs(xc1_norm ) +abs(xc2_norm) if self.lane_centers: x_center = self.lane_centers[-1] rotation_speed = 0.7 * kp * x_center * (-1) else: rotation_speed = 0.0 self.publish_speed(self.current_rot) rclpy.spin_once(self,timeout_sec=0.1) 因此，作为 obs，我使用 Laneprediction 模型的输出，即我标准化的 4 个 x 坐标。我的奖励是“奖励 = -penalty”(norm)，因此通过最大化奖励，我可以最小化“惩罚”。x_center 是根据 4 个 obs 计算出的变量 简而言之：我为每一帧设置旋转速度，但只更改我的“kp”每 10 帧计算一次，而每 1 秒计算一次我的奖励。将从动作空间中采样的动作（对应于 kp 值）设置为连续 10 帧有效吗？不会覆盖步骤函数在 API 后台执行的任何操作？我将不胜感激一些帮助/澄清     提交人    /u/Puzzleheaded_Map5480   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dezvei/openai_gym_step_function/</guid>
      <pubDate>Thu, 13 Jun 2024 14:07:32 GMT</pubDate>
    </item>
    <item>
      <title>有 DQN 算法专家吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dey8gh/any_expert_in_dqn_algorithm/</link>
      <description><![CDATA[  由    /u/Shyamala_K  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dey8gh/any_expert_in_dqn_algorithm/</guid>
      <pubDate>Thu, 13 Jun 2024 12:51:41 GMT</pubDate>
    </item>
    <item>
      <title>涉及强化学习的数学金融研究领域（股票、债务、加密货币、保险等任何资产类别）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1deh31o/area_of_research_in_mathematical_finance_any/</link>
      <description><![CDATA[我目前正在攻读硕士学位，论文是关于在量化金融中实施强化学习框架。目前有很多工作要做，但更多的是侧重于在（大多数）基本金融环境中实施和改进不同的强化学习算法（例如投资组合权重优化、基于奖励/（简单）风险衡量的交易机器人，或引入波动性、流动性和其他基本变量作为学习环境的一部分）。我正在寻找一个金融领域，它更加专注（即不是通用设置），并且仍然可以使用强化学习框架进行探索（基本上是量化金融中的数学/优化问题）。我对所有资产类别和所有类型的问题持开放态度。     提交人    /u/Rogue260   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1deh31o/area_of_research_in_mathematical_finance_any/</guid>
      <pubDate>Wed, 12 Jun 2024 20:45:52 GMT</pubDate>
    </item>
    <item>
      <title>我们可以将强化学习用于未标记数据吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1degb6s/can_we_use_reinforcement_learning_for_unlabeled/</link>
      <description><![CDATA[我正在尝试根据一些表格分数、体重等制定个性化的锻炼计划。我确实有一个从易到难分类的锻炼列表。我没有任何标记数据，但我可以加入人工反馈。从头开始训练 RL 代理是否有意义？ 关于如何解决这个问题的任何意见都会有所帮助。    提交人    /u/Hot_Direction6179   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1degb6s/can_we_use_reinforcement_learning_for_unlabeled/</guid>
      <pubDate>Wed, 12 Jun 2024 20:13:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Dreamer-v3 如何在稀疏奖励探索任务上表现如此出色？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1defbq8/d_how_does_dreamerv3_do_so_well_on_sparsereward/</link>
      <description><![CDATA[在阅读了 Dreamer-v3 论文 后，我有点困惑它为什么在艰难的探索任务上表现如此出色。例如 MineCraft 领域。要获得一颗钻石，您总共需要采取 12 个奖励步骤，例如获取木材、制作桌子、获取镐、使用熔炉、制作钻石。 我明白模型及其设计选择如何非常有利于泛化和更好的价值评估。因此，一旦您做了一些能带来奖励的事情（例如收集木材），您就可以比其他竞争方法更可靠地回到它。但是假设您已经获得过一次木材，并且您每次都很快学会了获得它。从那时起，您不是在犹豫不决吗？唯一的探索是对动作进行软最大化，如果您不知道任何未来的奖励，那么这基本上应该是随机动作。是什么促使你制作工作台的？ 是不是因为犹豫不决就足以让你达到目标了？是否存在某种旨在改进模型的隐式定向探索？基于模型的泛化是否比我所认为的要强大得多？我只是非常惊讶他们可以在没有探索奖励之类的东西的情况下解决如此长期的任务。希望大家的想法！    提交人    /u/asdfwaevc   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1defbq8/d_how_does_dreamerv3_do_so_well_on_sparsereward/</guid>
      <pubDate>Wed, 12 Jun 2024 19:32:24 GMT</pubDate>
    </item>
    <item>
      <title>这仍然是真的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1deaain/is_it_still_true/</link>
      <description><![CDATA[      https://preview.redd.it/09kujbm8z56d1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=211213d6c841804ec6446b9e0d246213460c2ada    提交人    /u/Logical_Jaguar_3487   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1deaain/is_it_still_true/</guid>
      <pubDate>Wed, 12 Jun 2024 16:04:17 GMT</pubDate>
    </item>
    <item>
      <title>这个人工智能可以创造游戏阶段！使用 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1de8z2r/this_artificial_intelligence_can_create_game/</link>
      <description><![CDATA[        提交人    /u/Flimsy_Roll_5666   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1de8z2r/this_artificial_intelligence_can_create_game/</guid>
      <pubDate>Wed, 12 Jun 2024 15:08:33 GMT</pubDate>
    </item>
    </channel>
</rss>