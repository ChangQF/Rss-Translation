<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 14 Feb 2025 01:15:30 GMT</lastBuildDate>
    <item>
      <title>强化学习并没有改进基础监督模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</link>
      <description><![CDATA[      我有一个基础模型 (RNN)，它在序列预测方面做得还不错。 然后我创建了一个 PPO RL 模型来调整预训练 RNN 模型的输出。 问题：RL实际上降低了 MSE 指标。 我有点惊讶 RL 实际上会造成这么大的危害。  没有 RL 调整的 MSE：0.000047 有 RL 调整的 MSE：0.002053  验证 MSE 与迭代    提交人    /u/MaaDoTaa   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</guid>
      <pubDate>Fri, 14 Feb 2025 00:00:09 GMT</pubDate>
    </item>
    <item>
      <title>“具有大型推理模型的竞争性编程 [o3]”，El-Kishky 等人 2025 {OA}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</guid>
      <pubDate>Thu, 13 Feb 2025 22:56:18 GMT</pubDate>
    </item>
    <item>
      <title>Langevin Soft Actor-Critic：通过不确定性驱动的评论学习进行有效探索，Ishfaq 等人 2025 年。ICLR 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</link>
      <description><![CDATA[      现有的演员-评论家算法在连续控制强化学习 (RL) 任务中很受欢迎，但由于缺乏原则性的探索机制，导致样本效率低下。受汤普森采样在 RL 中有效探索的成功启发，我们提出了一种新颖的无模型 RL 算法 \emph{Langevin Soft Actor Critic} (LSAC)，该算法优先通过不确定性估计而不是策略优化来增强评论家学习。LSAC 采用了三项关键创新：通过基于分布朗之万蒙特卡罗 (LMC) 的更新进行近似汤普森采样、并行调节以探索函数后验的多种模式，以及用动作梯度正则化的扩散合成状态动作样本。我们进行了大量的实验，结果表明 LSAC 在连续控制任务中的表现优于或匹敌主流无模型 RL 算法。值得注意的是，LSAC 标志着基于 LMC 的 Thompson 采样首次成功应用于具有连续动作空间的连续控制任务。    submitted by    /u/hmi2015   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</guid>
      <pubDate>Thu, 13 Feb 2025 18:50:08 GMT</pubDate>
    </item>
    <item>
      <title>我的个人项目 - AlphaYINSHZero (Blitz)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</link>
      <description><![CDATA[      我使用 AlphaZero 在 YINSH 的 Blitz 版本上训练了一个 AI 模型，它能够在 BoardSpace 上击败 SmartBot。 请注意，Blitz 版本是您尝试获得5 连胜一次。 这是迭代 174 与自己对战的情况。 https://preview.redd.it/1ohn81p23yie1.png?width=866&amp;format=png&amp;auto=webp&amp;s=3d7261cdc446f7349eae31fe5bca4b66b8bcbfed 在训练过程中，有强有力的证据表明 Blitz 版本具有先手优势，因为先手玩家的胜率逐渐攀升至 80%。结束。 我是强化学习的新手，在策略分配方面，我 - 也许是天真地 - 想出了一种特殊的方法，所以请随时告诉我这是否是一种有效的方法，或者它是否会对 AI 训练造成问题。 我将 YINSH 表示为一个 11 x 11 数组，因此动作空间为 121 + 1（传球回合）。 我想避免使用较大的策略分布，例如 121（起始）* 121（目的地）= 14641 因此，我将游戏分为几个阶段：环放置（放置 10 个环）、环选择（挑选要移动的环）和标记放置（放置标记并移动选定的环）。 因此单个玩家的回合如下： 第 1 回合 - 选择您要移动的环。 第 2 回合- 对手传球。 第三回合 - 选择您想要移动圆环的位置。 通过将其分解为几个阶段，我可以使用 121 + 1 的行动空间。这种方法对我来说“感觉”更干净。 当然，我有一个堆叠的观察，可以编码游戏状态处于哪个阶段。 这是一种有效的方法吗？它似乎有效。 ... 我曾尝试训练 YINSH 的完整游戏，但它不完整。到目前为止，我对它的策略非常不满意。 不满意，我的意思是它只是沿着边缘形成了一个密集的标记场，它们不想相互交互。我真的希望 AI 能够战斗并造成混乱，但它们太和平了 - 只关心自己的事。通过沿边缘形成密集的标记，标记变得不可翻转。 AI 的（天真的？）方法只是：“让我像农民一样在边缘形成一片标记场，这样我就可以在同一区域收获多个 5 连成一排的标记。”他们就像板两端的两个农民，和平地制作自己的标记场。 Blitz 版本更加令人兴奋，AI 互相争斗 :D    提交人    /u/Kiwigami   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</guid>
      <pubDate>Thu, 13 Feb 2025 18:29:17 GMT</pubDate>
    </item>
    <item>
      <title>当您已经拥有体育馆环境时，sb3 矢量化环境如何工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</link>
      <description><![CDATA[我不太明白。您只是使用他们的 VecEnv 包装它吗？还是我必须重写它？    提交人    /u/blearx   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</guid>
      <pubDate>Thu, 13 Feb 2025 17:33:39 GMT</pubDate>
    </item>
    <item>
      <title>RLLib 使用多个 Runner 不会增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</link>
      <description><![CDATA[抱歉这里没有发布任何图片。  所以，我的问题是在 RLLib 上使用 24 个带有 SAC 的 env 运行器，导致根本没有学习。但是使用 2 个 env 运行器确实学到了（一点点）。  详细信息： Env - 是简单的 2d 移动到目标位置，当达到目标状态时，每个时间步骤的稀疏奖励为 -0.01，具有 500 帧限制，Box(shape=(10,)) 观察和 Box(-1,1) 动作空间。我尝试了一堆超参数，但似乎都没有用。 对 RLlib 非常陌生。我曾经制作过自己的 rl 库，但这次我想尝试 rllib。 有人知道问题是什么吗？如果您需要更多信息，请问我！！谢谢    由   提交  /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</guid>
      <pubDate>Thu, 13 Feb 2025 14:53:55 GMT</pubDate>
    </item>
    <item>
      <title>参考丢失：带有 RL 算法分类法/本体的电子表格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</link>
      <description><![CDATA[我在这里的某个地方看到过它，现在我找不到它了。我知道有几篇论文调查了 RL 算法，但我正在尝试找到一个“电子表格”，一位成员在评论中发布了这一信息。我相信这是一个指向谷歌文档的链接。 每一行都有一些更高级别的分组，每个组中都有算法和注释。它根据算法的属性（例如连续动作空间等）将它们分开。 有人知道该资源或我可以在哪里找到它吗？ 编辑：找到了！https://rl-picker.github.io/    提交人    /u/ParamedicFabulous345   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</guid>
      <pubDate>Thu, 13 Feb 2025 11:41:11 GMT</pubDate>
    </item>
    <item>
      <title>没有机器学习的强化学习，可以做到吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioezsw/reinforcement_learning_without_machine_learning/</link>
      <description><![CDATA[嗨，我对[回归+分类+聚类+关联规则]有了解。我理解数学方法和算法，但不了解代码（我有 现在，我想了解计算机视觉和强化学习。 所以有人可以告诉我是否可以在不编写 ML 代码的情况下学习强化学习吗？    提交人    /u/InternationalWill912   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioezsw/reinforcement_learning_without_machine_learning/</guid>
      <pubDate>Thu, 13 Feb 2025 08:59:41 GMT</pubDate>
    </item>
    <item>
      <title>什么是良好的文本到 Avatar 语音模型/管道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</link>
      <description><![CDATA[基本上就是这样。你们推荐哪个管道来生成一个可以读取文本的头像 - 所有报告的固定头像？（理想情况下是开源的，因为我可以访问 gpu 集群并且不想为第三方服务付费 - 因为我将提供合理的信息）。     提交人    /u/Gvascons   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</guid>
      <pubDate>Thu, 13 Feb 2025 05:25:12 GMT</pubDate>
    </item>
    <item>
      <title>Sergey Levine 强化学习 [在哪里可以找到这个]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</link>
      <description><![CDATA[嗨  作为初学者，我希望很好地掌握 RL 背后的数学。## 你能告诉我在哪里可以找到这门课程吗？拜托。 ## [Sutton Barto] 强化学习 = https://www.amazon.in/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249?dplnkId=c3df8b9c-8d63-4f9b-8a4e-bc601029852c 还有哪些其他资源值得关注？您能列出使用过的资源吗？请 另外  我开始学习 ML，想问问这里的有经验的人，关于理解每个算法（如 K-NN/SVM）背后的数学证明的要求 了解算法背后的数学真的很重要吗？或者只需观看视频，了解关键点，然后开始编码 学习 ML 的适当方法是什么？## ML 工程师是否深入研究了这么多编码，还是他们只是通过可视化和开始编码来低估关键点？ 请让我知道。（我在这个领域无望）    提交人    /u/InternationalWill912   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</guid>
      <pubDate>Thu, 13 Feb 2025 03:02:00 GMT</pubDate>
    </item>
    <item>
      <title>有人有 Julia 中 PPO RL 的工作示例吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</link>
      <description><![CDATA[似乎我发现的所有代码库都已过时且无法使用。     提交人    /u/D3MZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</guid>
      <pubDate>Wed, 12 Feb 2025 16:57:19 GMT</pubDate>
    </item>
    <item>
      <title>目前，击败超级马里奥第一关的最佳 RL 方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</link>
      <description><![CDATA[我见过 PPO、DQN 和 NEAT。SethBling 在 2015 年使用 NEAT 编写了一个 RL 代理，看起来它的表现是所有代理中最好的。在 4 年后，我重新回到了 RL 领域，并希望用 Python 实现它作为个人项目。我应该实现哪一个？有新方法吗？    提交人    /u/marblesandcookies   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</guid>
      <pubDate>Wed, 12 Feb 2025 16:10:25 GMT</pubDate>
    </item>
    <item>
      <title>体育馆环境中的代理动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</link>
      <description><![CDATA[你好，有人知道我如何访问安全健身房、openai gym 中的代理动态吗？ 通常 .step() 直接模拟动态，但我的应用程序中需要动态，因为我需要对这些动态进行区分。更具体地说，我需要计算 f(x) 的梯度和 g(x) 的梯度，其中 x_dot=f(x)+g(x)u。x 是状态，u 是输入（动作） 我总是可以将其视为黑匣子并学习它们，但我更喜欢直接从地面真实动态中得出梯度。 请告诉我！    提交人    /u/Limp-Ticket7808   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</guid>
      <pubDate>Wed, 12 Feb 2025 15:57:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和机器人技术领域的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</link>
      <description><![CDATA[大家好，我最近获得了 RL（技术上是逆向 RL）的博士学位，该学位应用于人机协作。我使用过 4 种不同的机器人操纵器、4 种不同的夹持器和 4 种不同的 RGB-D 相机。我的专长在于使用感知反馈学习智能行为，以实现安全高效的操作。 我建立了端到端管道，用于在传送带上对产品进行分类、在未受精的卵子进入孵化器之前对其进行无损识别和移除、使用机器人对医疗器械进行智能无菌处理，以及其他一些项目。我曾在三菱电机研究实验室实习，目前已在顶级会议上发表了 6 篇以上的论文。 我使用过许多物体检测平台，例如 YOLO、Faster-RCNN、Detectron2、MediaPipe 等，并且拥有丰富的注释和训练经验。我擅长使用 Pytorch、ROS/ROS2、Python、Scikit-Learn、OpenCV、Mujoco、Gazebo、Pybullet，并且对 WandB 和 Tensorboard 有一些经验。由于我最初不是来自计算机科学背景，所以我不是一名专业的软件开发人员，但我编写的代码稳定、干净、易于扩展。 我一直在寻找与此相关的工作，但目前我在就业市场上很难找到工作。如果您能提供任何帮助、建议、推荐等，我将不胜感激。作为一名持学生签证的人，我时间紧迫，需要尽快找工作。提前谢谢您。    提交人    /u/prasuchit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:05 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个寻找 RLHF 工作的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</link>
      <description><![CDATA[我们在 AI 的多个学科都有工作机会。我们也有专门的 RLHF 工作页面。在过去 30 天内，我们有 48 个涉及 RLHF 的工作机会。 您可以在此处找到所有 RLHF 工作： https://www.moaijobs.com/rlhf-jobs 请告诉我您的想法。谢谢。    提交人    /u/WordyBug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</guid>
      <pubDate>Wed, 12 Feb 2025 02:20:52 GMT</pubDate>
    </item>
    </channel>
</rss>