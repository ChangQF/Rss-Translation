<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 09 Sep 2024 06:25:48 GMT</lastBuildDate>
    <item>
      <title>在价值迭代中，终端状态的值应该保持不变还是不保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</link>
      <description><![CDATA[我遵循了这个例子： https://courses.grainger.illinois.edu/cs440/fa2022/lectures/rl.html 这是一个网格世界，其中折扣 = 0.9，右上角的奖励为 +1，中右为 -1，而其他网格的奖励为零。 正如您所见，两个终端网格（右上和中右）的值在整个迭代过程中一直保持 +1 和 -1，这意味着它通过贝尔曼方程的迭代强制保持它们不变，但我在网上找到了多个其他数值例子，其中终端状态的值也根据贝尔曼方程进行更新。所以我的问题是：哪一种是正确的方法？非常感谢    提交人    /u/james_stevensson   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</guid>
      <pubDate>Sun, 08 Sep 2024 21:31:57 GMT</pubDate>
    </item>
    <item>
      <title>有沒有主动推理（自由能原理）的成功故事？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</link>
      <description><![CDATA[虽然自由能原理旨在通过最小化意外来开发一个统一的感知和控制框架，但据我所知，很少有经验结果可以证明其前景。有没有人听说过它在图像输入的连续控制问题或至少一些经典控制问题中的一些成功应用？    提交人    /u/OutOfCharm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</guid>
      <pubDate>Sun, 08 Sep 2024 09:48:59 GMT</pubDate>
    </item>
    <item>
      <title>比较 V-Trace 和 Retrace</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbizt8/comparing_vtrace_and_retrace/</link>
      <description><![CDATA[看起来 V-Trace 和 Retrace 非常相似，并且用途也非常相似，即纠正离策略数据的值估计。据我所知，唯一真正的区别是 V-Trace 增加了一个额外的裁剪参数（rho 参数），但我很难理解除此之外的区别。 到目前为止，我找不到两者的比较或任何提及它们相似性的文章（甚至 IMPALA 论文也没有提到 Retrace）。我还注意到，最近的作品提到了 Retrace，但没有提到 V-Trace（例如，Muesli 论文使用了 Retrace），尽管 V-Trace 的开发时间比 Retrace 晚两年。 所以我的问题是，我应该如何理解两者之间的关系？V-Trace 真的比 Retrace 好吗？但它在 IMPALA 之外从未真正流行过？或者它们用于完全不同的事情，而我完全误解了一些东西？    提交人    /u/vandelay_inds   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbizt8/comparing_vtrace_and_retrace/</guid>
      <pubDate>Sat, 07 Sep 2024 22:30:02 GMT</pubDate>
    </item>
    <item>
      <title>基于 pybullet 的健身房环境中的 PPO 四足动物行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/</link>
      <description><![CDATA[嗨，我正在尝试训练四足动物在基于 pybullet 的自定义健身环境中行走。我使用 Stablebaselines3 中的 PPO 作为具有默认超参数的算法。四足动物每条腿有 3 个自由度（总共 12 个）。观察空间是四足动物的基本线性位置、基本线性速度、基本角位置、基本角速度、12 个关节位置、12 个关节速度以及每只脚接触地面时的 4 个二进制值。动作空间和观察空间都已标准化。 奖励是沿 x 轴的正向移动（向前移动），惩罚是侧向漂移（沿 y 轴移动）。情节持续 5000 个时间步，但如果身体翻转也会结束。  通过运行 12 个并行环境进行训练，总共运行 100,000,000 个时间步（大约需要半天时间）。 到目前为止，最好的结果是，充其量，腿部的一些振动运动会产生向前的运动，但根本不自然。 有人有什么好的提示/资源关于如何获得更自然的结果吗？    提交人    /u/Fit_Passion6272   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/</guid>
      <pubDate>Sat, 07 Sep 2024 19:51:26 GMT</pubDate>
    </item>
    <item>
      <title>A2C 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fb2v7w/a2c_algorithm/</link>
      <description><![CDATA[      你好， 当我阅读《强化学习实践手册》（第二版，packt）时，我发现作者使用来自环境的奖励来计算回报 R，这更接近于蒙特卡洛更新，而不是引导从评论家估计 V(st+1)，您会在照片中找到更新。 我的问题是这会影响学习吗，这是 A2C 的另一个版本吗？ 提前谢谢您的回答！ A2C 算法，来源：动手强化学习书籍（第二版，packt）    提交人    /u/Capital_Win8377   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fb2v7w/a2c_algorithm/</guid>
      <pubDate>Sat, 07 Sep 2024 09:14:33 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的规范化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fb0fke/normalization_in_rl/</link>
      <description><![CDATA[我正在使用标准缩放和 MinMax 缩放，但它们在 RL 实现中效果不佳。我有 10 个特征，而每个特征都需要缩放到特定范围，如 [0, 1]。因为我的特征范围从 60,000 到低至 0.001。 如何才能以有效处理特征尺度上这些巨大差异的方式规范化此数据集以进行强化学习？ 有哪些高级规范化技术可以处理特征中的这种范围差异，同时仍能保持强化学习模型的稳定性？    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fb0fke/normalization_in_rl/</guid>
      <pubDate>Sat, 07 Sep 2024 06:13:13 GMT</pubDate>
    </item>
    <item>
      <title>任何好的‘Rl Agent’设计资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faqpef/any_good_rl_agent_design_resources/</link>
      <description><![CDATA[我目前对编程 RL 代理没有太多经验。我完成了 hugging face Deep RL 课程，也参加了 David Sliver 的课程（真的很棒，顺便说一下，它是免费的），所以我对一些 RL 中常用的机器学习算法背后的一些概念有一定的了解，但我从未做过“项目”或任何事情 所以我开始实施不同的 RL 算法来与 Open AI Gym 环境进行交互。但我设计的实际课程总是到处都是。  例如，我经常在代理类中存储健身房环境的第二个表示，然后从内部函数训练代理，这对我来说似乎不正确。 我在编程方面也不是很擅长（就项目和课程设计而言）——我已经做了一年的软件工程师，所以我可以很好地编写代码，只是整个设计方面对我来说很难哈哈 在构建 RL 项目时，从结构到类设计，有没有什么好的资源/例子来介绍一些好的编程实践。    提交人    /u/Amazing_Track4881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faqpef/any_good_rl_agent_design_resources/</guid>
      <pubDate>Fri, 06 Sep 2024 21:35:21 GMT</pubDate>
    </item>
    <item>
      <title>强化学习奖励可以是当前状态和新状态的结合吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</link>
      <description><![CDATA[      我正在为我的 RL 代理构建奖励函数，并考虑采取行动后当前状态和新状态的组合。据我所知，基于 Sutton &amp;，这是可能的Barto，特别是公式 3.6，其中奖励是状态-动作对和结果状态的函数。我想确保我的方法符合 RL 理论。 有人可以确认这是否有效或分享见解吗？ https://preview.redd.it/7wndf4ny98nd1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=547a77aeecc75edcee9fbd41ec45dc95c772d3d1    提交人    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</guid>
      <pubDate>Fri, 06 Sep 2024 17:57:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习竞赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</link>
      <description><![CDATA[大家好，我通过强化学习开始训练，正在提升自己。我可以在网上参加和竞争这个领域的哪些比赛，我如何才能在这个领域有更好的发展？    提交人    /u/Weary-Ad-7225   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</guid>
      <pubDate>Fri, 06 Sep 2024 17:46:06 GMT</pubDate>
    </item>
    <item>
      <title>PPO 外汇交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</link>
      <description><![CDATA[      我正在使用 SB3 为外汇交易环境训练带有动作掩码的 PPO。该模型似乎学习了低点和高点枢轴点，但准确性非常可疑，而且它会反转动作，所以它在高点买入，在低点卖出！代码中哪里可能出错？  https://preview.redd.it/4dwdxlclj7nd1.png?width=986&amp;format=png&amp;auto=webp&amp;s=60ac1f2a3ce5cdd386d77d13c57c233a37be56a6    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</guid>
      <pubDate>Fri, 06 Sep 2024 15:31:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 进行黑盒目标函数优化 - 需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</link>
      <description><![CDATA[大家好， 我是强化学习领域的新手，目前正在研究一个让我有点困惑的问题。我正在尝试使用强化学习来优化黑盒目标函数，但我不确定我是否走在正确的轨道上。我非常感谢这个 subreddit 中经验丰富的人提供的任何指导或见解！ 所以，事情是这样的：我有一个目标函数，它接受两个输入（x，y）并输出标量值 z。目标是找到这个函数的全局最大值。棘手的部分是我无法访问该函数的内部工作原理。我所能做的就是从一个随机位置 (x, y) 开始，然后采取小步骤来探索函数的格局。 目标函数可能看起来像这样： def objective_function(x, y): return -(x**2 - 10 * np.cos(2 * np.pi * x) + y**2 - 10 * np.cos(2 * np.pi * y) + 20)  这个想法是使用 Q 值估计和神经网络来学习最大化目标函数的最佳策略。 以下是我一直在尝试的粗略概述：  使用 PyTorch 设置 DQN 模型，该模型以当前状态 (x, y) 作为输入并输出每个动作的 Q 值。 创建重放缓冲区来存储转换（状态、动作、奖励、 在训练期间，使用 epsilon-greedy 探索根据当前状态选择一个动作。 采取行动并观察下一个状态和奖励。 将转换存储在重放缓冲区中。 如果重放缓冲区有足够的样本，则对一批转换进行采样并执行梯度下降以更新 DQN 模型。  训练后，从随机状态开始并根据学习到的 Q 值贪婪地选择动作，找到最佳解决方案。  现在，我有点不确定，需要一些帮助：  由于这个问题没有明确的终点或终端状态，所以我不确定如何调整 Q 值估计。我应该将 Q 值估计为固定步数的累积奖励，而不是直到达到终止状态？我可能离题太远了，所以如果我误解了什么，请纠正我！ 如果没有终止状态，由于起点随机且步数有限，Q 值每次都会不同。这是预料之中的，还是我以错误的方式处理这个问题？  我将非常感谢这个 subreddit 中出色的人提供的任何指导、建议或替代方法。如果您有类似的优化问题经验，或者对如何改进此场景的 DQN 方法有想法，请分享您的智慧！ 非常感谢您的帮助。我真的很高兴向大家学习，并希望在这个具有挑战性的问题上取得一些进展！    提交人    /u/Technical-Vehicle888   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</guid>
      <pubDate>Fri, 06 Sep 2024 12:50:41 GMT</pubDate>
    </item>
    <item>
      <title>元强化学习框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</link>
      <description><![CDATA[大家好， 我目前正在探索元 RL 的主题，但在寻找合适的框架/库时遇到了问题。  虽然我已经找到了一些关于元学习主题的一般内容（例如 learn2learn、更高版本），但 RL 方面往往是次要关注点。  您是否使用过某些框架？可以推荐一些吗？    提交人    /u/RandomAgentIml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</guid>
      <pubDate>Fri, 06 Sep 2024 11:32:18 GMT</pubDate>
    </item>
    <item>
      <title>“深度贝叶斯老虎机对决：汤普森抽样的贝叶斯深度网络实证比较”，Riquelme 等人 2018 年 {G}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa2qfb/deep_bayesian_bandits_showdown_an_empirical/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa2qfb/deep_bayesian_bandits_showdown_an_empirical/</guid>
      <pubDate>Fri, 06 Sep 2024 01:03:03 GMT</pubDate>
    </item>
    <item>
      <title>“探索的长期价值：测量、发现和算法”，Su 等人 2023 {G}（推荐者）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa28ph/longterm_value_of_exploration_measurements/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa28ph/longterm_value_of_exploration_measurements/</guid>
      <pubDate>Fri, 06 Sep 2024 00:38:27 GMT</pubDate>
    </item>
    <item>
      <title>主要的 RL 实验室都有盈利吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9ykt6/are_any_of_the_major_rl_labs_profitable/</link>
      <description><![CDATA[      我很好奇为什么 DeepMind 放弃了他们的历史项目，并停止了 AlphaZero 作为国际象棋引擎的开发。但后来我看了他们的利润，一切都说得通了。 即使是最近几年，主要客户也是他们的所有者，因此这主要是创造性会计，他们可以随心所欲地支付内部项目的费用。那么，RL 对任何更大规模的人来说都是有利可图的吗？或者这只是纯粹的研究资金浪费？ 我相信你可以让 DeepMind 真正盈利，你只需要一个新的领导层，与人民更紧密地联系在一起。 https://preview.redd.it/how5yh9a92nd1.png?width=774&amp;format=png&amp;auto=webp&amp;s=d93fb31fa812bff823ca899e3d0166697c39f4e9   由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9ykt6/are_any_of_the_major_rl_labs_profitable/</guid>
      <pubDate>Thu, 05 Sep 2024 21:51:50 GMT</pubDate>
    </item>
    </channel>
</rss>