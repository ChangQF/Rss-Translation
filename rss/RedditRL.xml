<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 13 Apr 2024 15:10:24 GMT</lastBuildDate>
    <item>
      <title>自打输球让ppo“恐惧”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2ym5s/losing_in_self_play_makes_ppo_fearfull/</link>
      <description><![CDATA[我已经成功地让 ppo 代理在对手随机走棋时玩相当复杂的游戏。游戏是零和游戏，但它足够复杂，我需要在真实分数之上提供一些启发式方法，以便代理能够弄清楚他们需要做什么才能获胜。 现在我已经启用了通过每隔 X 步保存网络状态来进行自我对弈，有时让代理与那些旧版本的网络进行对战。  所发生的情况是，网队会很好地学习，直到弄清楚如何获胜，这会为参与的参与者提供大量奖励/负奖励，以表明赢得比赛比简单地跟随价值较低的比赛更重要启发式。  然后会发生的情况是，当其中一个玩家获胜时，另一个玩家会变得保守并放弃其积累的积分以确保对方不会获胜，并且它会开始学习更慢，这是预期的。出乎意料的是，每次这个过程发生时，失败的网络都会变得越来越保守，直到它不再尝试增加自己的平均奖励，但它只会达到对手不会获胜的已知点。  我尝试在赢得比赛时减少奖金奖励，但这没有帮助。  接下来我要尝试的 * 是奖励标准化 * 只给胜利者奖励，而不给失败者， * 以提高探索率。 * 有时让代理与随机代理对战。  有什么技巧可以解决这个问题吗？   由   提交/u/drblallo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2ym5s/losing_in_self_play_makes_ppo_fearfull/</guid>
      <pubDate>Sat, 13 Apr 2024 10:15:45 GMT</pubDate>
    </item>
    <item>
      <title>需要一些关于 JAX 中 Actor-Critic 的实现的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2t8ym/need_some_help_regarding_the_implementation_of/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2t8ym/need_some_help_regarding_the_implementation_of/</guid>
      <pubDate>Sat, 13 Apr 2024 04:18:30 GMT</pubDate>
    </item>
    <item>
      <title>关于A2C算法的两种不同实现方式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2avvb/about_two_different_ways_for_implementing_a2c/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2avvb/about_two_different_ways_for_implementing_a2c/</guid>
      <pubDate>Fri, 12 Apr 2024 14:41:56 GMT</pubDate>
    </item>
    <item>
      <title>模型行为的随机性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1xekr/randomness_in_model_behavior/</link>
      <description><![CDATA[     &lt; td&gt; 我有一个自定义多代理Boid&lt; /a&gt; 使用来自 SB3 的 PPO 进行植绒环境。 代码 我正在做什么： 我已经用 100 万个时间步训练了环境，每个 boid 从不同/随机的初始位置开始（文件）。尽管在整个训练过程中位置保持不变。 ​ 终端条件： 1) 任何 Boid 碰撞，代理距离小于安全半径 2) 任何 Boid 远离群体，代理距离大于邻域半径。 ​ &lt; strong&gt;奖励函数： def奖励(self, agent, neighbor_velocities, neighbor_positions): multiplier=1 Total_reward=0 outofflock=False if(len(neighbor_positions) &gt; 0) ：对于neighbor_positions中的neighbor_position：距离= np.linalg.norm（agent.position - neighbour_position）如果距离&lt; SimulationVariables[“SafetyRadius”]: # 较大的惩罚，以阻止代理过于接近total_reward -= 10 multiplier = 1 elif SimulationVariables[“SafetyRadius”] &lt;距离&lt; SimulationVariables[“NeighborhoodRadius”]: multiplier=10 # 奖励的衰减指数函数 alpha = 0.1 # 根据需要调整此参数total_reward += np.exp(-alpha * distance) if (len(neighbor_velocities) &gt; 0):average_velocity = np.mean(neighbor_velocities, axis=0)desired_orientation =average_velocity -agent.velocityorientation_diff = np.arctan2(desired_orientation[1],desired_orientation[0])-np.arctan2(agent.velocity[1],agent.velocity[ 0]) 如果orientation_diff &gt; np.pi：orientation_diff -= 2 * np.pi eliforientation_diff &lt; 0:orientation_diff += 2 * np.pialignment = 1 - np.abs(orientation_diff) if (alignment &lt; 0.5):total_reward -= 50 * (alignment) else:total_reward += 50 * (alignment) else: # If没有邻居，鼓励智能体寻找羊群total_reward -=50 outofflock=True 返回total_reward，outofflock  测试输出： ​ ; 奖励 10 个不同的剧集 -  问题： 在我第一次尝试测试时，奖励在 600000 范围内的 boids 没有正确移动，而 1 Mil 范围内的奖励则正确。现在它们都可以正确移动，但奖励值不同，即在 600000 范围内以及 1 Mil 范围内。我不知道模型是否正确学习，也不知道为什么会出现这种差异。 ​ 输出（当前行为）： ​ Boid 运动有 4 个不同的初始位置（每个持续时间10 秒） ​ 我的改进想法： 更多用于训练的位置文件防止过度拟合。 ​ 欢迎任何建议。   由   提交/u/Sadboi1010   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1xekr/randomness_in_model_behavior/</guid>
      <pubDate>Fri, 12 Apr 2024 02:08:02 GMT</pubDate>
    </item>
    <item>
      <title>需要指导！！！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1ryie/needing_guidance/</link>
      <description><![CDATA[我正在实习，我正在尝试构建一个 RL 来优化和管理任何工业环境中的能源消耗。强化学习算法有很多，我不知道哪种最适合这种情况？我应该选择连续空间还是离散空间？    由   提交/u/geegeebabebabe  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1ryie/needing_guidance/</guid>
      <pubDate>Thu, 11 Apr 2024 22:10:59 GMT</pubDate>
    </item>
    <item>
      <title>有没有没有优势函数的演员批评算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1nqmt/is_there_any_actor_critic_algorithm_without_the/</link>
      <description><![CDATA[优势函数的使用可以在演员评论家算法中看到，其中使用 TD(0) 方法来近似优势，但是是否存在任何方式都可以创造优势，因为它是真实的方程 A(s,a) = Q(s,a) - V(s)，其中 V(s) 可以定义为 \sum\pi(a|s)Q(s ,a)（ps 只需遵循方程）并且能够使用此过程收敛！我们是否也可以在不使用优势函数的情况下创建一个演员批评算法？   由   提交 /u/Professional_Pound63   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1nqmt/is_there_any_actor_critic_algorithm_without_the/</guid>
      <pubDate>Thu, 11 Apr 2024 19:24:22 GMT</pubDate>
    </item>
    <item>
      <title>Anterion – 开源人工智能软件工程师（SWE-agent 和 OpenDevin）- RL Baseline</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1gyif/anterion_opensource_ai_software_engineer_sweagent/</link>
      <description><![CDATA[       由   提交 /u/Ok-Alps-7918   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1gyif/anterion_opensource_ai_software_engineer_sweagent/</guid>
      <pubDate>Thu, 11 Apr 2024 14:48:16 GMT</pubDate>
    </item>
    <item>
      <title>关于A2C实现中的损失和优化器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c18roz/about_the_loss_and_optimizer_in_a2c_implementation/</link>
      <description><![CDATA[我目前正在实现 A2C 算法，在代码实现中遇到了关于损失和优化器的困惑。 当我第一次使用时学习了A2C，我提到的代码为actor和critic分配单独的优化器，并分别优化，就像：```Python def learn(self,transition_dict): states = torch.tensor(np.array(transition_dict[&#39;states) &#39;]), dtype=torch.float).view(-1, self.state_dim).to(self.device) # 将 ndarray 列表转换为 ndarray，因为将 ndarray 列表转换为张量非常慢 actions = torch.tensor (transition_dict[&#39;actions&#39;], dtype=torch.long).view(-1, 1).to(self.device)  返回，优点 = self.compute_returns_and_advantages(transition_dict, self .lamda) log_probs = torch.log(self.actor(states).gather(1, actions)) # actions中的元素必须是下标才能使用gather() actor_loss = torch.mean(-log_probs *优点.detach()) # detach()防止临时加载，共享数据内存，值相同 # td_error 与 actor 参数无关，可以看作常量 Critical_loss = F.mse_loss(self.critic(states), returns.detach()) # 准备update self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() # 计算梯度 actor_loss.backward()ritic_loss.backward() # 更新权重 self.actor_optimizer.step() self.critic_optimizer.step() &lt; /pre&gt;  但当我回顾流行的开源 RL 库中的代码时，我发现它们都使用了针对 actor 和 critic 的通用优化器，并对 actor、值和熵的损失求和来进行优化。 [stable-baselines3](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/a2c/a2c.py#L132) 的实现如下： Python def train(自我）-&gt;无：“”使用当前收集的推出缓冲区更新策略（整个数据上的一个梯度步骤）。 ”“” # 切换到训练模式（这会影响批量标准化/dropout） self.policy.set_training_mode(True)  # 更新优化器学习率 self._update_learning_rate(self.policy.optimizer) # 这只会循环一次（一次性获取所有数据） for rollout_data in self.rollout_buffer.get(batch_size=None): actions = rollout_data.actions if isinstance(self.action_space,spaces.Discrete): # 将离散动作从浮点动作转换为长动作= actions.long().flatten()values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)values =values.flatten() # 标准化优势（原始实现中不存在）优点 = rollout_data.优点 if self.normalize_advantage: 优点 = (优点 - 优点.mean()) / (advantages.std() + 1e-8) # 策略梯度损失 policy_loss = -(advantages * log_prob).mean() # 使用TD(gae_lambda) target value_loss = F.mse_loss(rollout_data.returns, value) # 熵损失有利于探索 if entropy is None: # 无分析形式时的近似熵 entropy_loss = -th.mean(-log_prob) else: entropy_loss = -th .mean(entropy) loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss # 优化步骤 self.policy.optimizer.zero_grad() loss.backward() # 剪辑梯度范数 th.nn.utils.clip_grad_norm_(self .policy.parameters(), self.max_grad_norm) self.policy.optimizer.step()  ``` 我的问题：为什么要实现 Actor-Critic这样？是为了方便吗？还是为了性能？   由   提交 /u/Awkward_Swimmer_5649   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c18roz/about_the_loss_and_optimizer_in_a2c_implementation/</guid>
      <pubDate>Thu, 11 Apr 2024 07:07:36 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 RL 录取的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c17bwu/need_advice_on_an_rl_admit/</link>
      <description><![CDATA[大家好，我在巴塞罗那 UPF 申请了智能信息系统硕士学位，我认为我已被接受，因为申请门户状态显示已接受，但是尚未收到任何正式的邮件沟通。 我对强化学习非常感兴趣，Reddit 上的一篇帖子向我推荐了这所大学，而且这里的教授似乎也非常好。 我想知道我在这里是否有良好的学习范围和机会。我的最终目标是成为一名教授。 我唯一怀疑的原因是这所大学相当年轻~ 30 年。而欧洲其他大学已有一百多年的历史。 这是我唯一可能承认的，我曾向其他 3 个地方申请直接博士学位，但都被拒绝了。 大家觉得怎么样？如果我获得一门课程或申请 2025 年 9 月开始的硕士课程，我应该接受录取吗？ 如有任何帮助，我们将不胜感激，谢谢！   由   提交 /u/FlyTrain1011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c17bwu/need_advice_on_an_rl_admit/</guid>
      <pubDate>Thu, 11 Apr 2024 05:38:25 GMT</pubDate>
    </item>
    <item>
      <title>NEAT + Q 学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c16gry/neat_q_learning/</link>
      <description><![CDATA[我一直在寻找将 NEAT 与 DQN 结合使用的论文，但没有找到。我想知道这对于 OpenAI Gym 中的 2D 自动驾驶汽车来说是否是个好主意。   由   提交/u/fa_anony__mous   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c16gry/neat_q_learning/</guid>
      <pubDate>Thu, 11 Apr 2024 04:47:41 GMT</pubDate>
    </item>
    <item>
      <title>无模型强化学习的奖励。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c147yn/reward_in_modelfree_reinforcement_learning/</link>
      <description><![CDATA[嗨，我看到有些论文把 Reward 写成 R -&gt; S X A X S&#39; 和其他一些使用 R-&gt; 的论文S X A 哪一个对于无模型强化学习有效？   由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c147yn/reward_in_modelfree_reinforcement_learning/</guid>
      <pubDate>Thu, 11 Apr 2024 02:46:20 GMT</pubDate>
    </item>
    <item>
      <title>有人让 mujoco-py 在 Linux 上运行吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0zytk/has_anyone_gotten_mujocopy_working_on_linux/</link>
      <description><![CDATA[我的一位同事给了我一些使用“pen- human-v1”的代码数据集，它是需要 Mujoco（特别是 Mujoco-py）的 3d Gym 环境之一。 Mujoco-py 已有 7 年历史，似乎已经过时了 - 我无法让它在 Windows 或 Linux 上运行（我认为我的同事在 Mac 上）。我相信 Gymnasium 现在使用最新的 Mujoco 绑定，不需要一堆额外的安装步骤，但尚不清楚如何使 d4rl 使用 Gymnasium 而不是 Gym。所以我不确定人们如何让 d4rl 在 Linux 上工作（使用不同的发行版会更好？）   由   提交/u/hearthstoneplayer100  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0zytk/has_anyone_gotten_mujocopy_working_on_linux/</guid>
      <pubDate>Wed, 10 Apr 2024 23:26:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在训练期间将变化的伽玛传递给 DQN 或 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0q5o6/how_to_pass_a_varying_gamma_to_dqn_or_ppo_during/</link>
      <description><![CDATA[强化学习和 SB3 实现应用典型的常量 gamma 在学习时对未来值进行折扣。这对于离散时间环境来说很好，其中对于每个操作，未来值都会被折扣为每个步骤的常数。 我有一个自定义的健身房环境，其中我的环境在离散决策时期中步进，但每个操作都需要一个不同的时间量。因此，以恒定比率贴现未来价值是不正确的。我需要做的是用 gamma 来折扣未来值，gamma 是在环境中执行操作所需时间的函数。 无论如何，是否可以将 gamma 作为函数或张量传递在学习过程中映射到每个 (s, a, s&#39;, r) 元组？也许可以使用现有功能或回调？如果可能的话，我希望避免分叉存储库。 任何意见都将不胜感激，因为我已经在这方面坚持了一段时间了。提前致谢！   由   提交 /u/Real_Zesty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0q5o6/how_to_pass_a_varying_gamma_to_dqn_or_ppo_during/</guid>
      <pubDate>Wed, 10 Apr 2024 16:44:13 GMT</pubDate>
    </item>
    <item>
      <title>如何将强化学习与 GYM 库应用于 Flappy Bird 等 2D 视频游戏。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0nx8l/how_to_apply_reinforcement_learning_with_gym/</link>
      <description><![CDATA[我使用算法创建了一个 flappy Bird 机器人。它工作得很好并且可以无限运行。现在我想尝试使用强化学习来做到这一点，看看它能提高多少。我在 Python 中发现了一个名为 Gym 的库，它似乎可以简化事情。我尝试在网上寻找类似的示例，但找不到任何适合初学者创建自己的环境的内容。所以我希望有人能引导我做这件事。 请注意，这是我制作的离线游戏，不违反任何条款。 我已经可以在坐标中检测到屏幕上的鸟和障碍物（参见下面的代码）。所以我确实有数据，我只需要围绕它编写机器学习部分。 # 截取游戏窗口的屏幕截图 game_frame = pyautogui.screenshot(region=(200, 190, 500 , 860)) deflocate_flappy_bird(game_frame): # 找到 flappy 鸟。 Returns X,Y flappy_bird_location = pyautogui.locate(“800px-1080pxBirdEye2.jpg”, game_frame,confidence=0.85) return flappy_bird_location deflocate_obstacles(game_frame): # 定位屏幕上的所有障碍物。返回 pyautogui.locateAll(“Obstacle_Ball.jpg”, game_frame,confidence=0.90) 中障碍物的 X,Y 距离 = pow(10, 2) 障碍物位置 = [] 列表: if all(abs(obstacle.top - y[ 0]) &gt; y 在障碍物位置中的距离): 障碍物位置.append((int(obstacle.top), int(obstacle.left))) 返回障碍物位置 current_location_flappy =locate_flappy_bird(game_frame) 障碍物位置 =locate_obstacles(game_frame) # flappy 位置( x, y) = [(240, 20)] # 障碍物位置(x ,y) = [(218, 115), (361, 115), (566, 334), (727, 334)]     由   提交/u/pubGGWP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0nx8l/how_to_apply_reinforcement_learning_with_gym/</guid>
      <pubDate>Wed, 10 Apr 2024 15:11:00 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>