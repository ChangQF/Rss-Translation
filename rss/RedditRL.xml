<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Thu, 10 Apr 2025 03:34:05 GMT</lastBuildDate>
    <item>
      <title>“对开放式LLM的防篡改保障措施”，Tamirisa等人2024年（诸如sophon之类的元学习un-fineTune-the-fignune-lights）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jvk7b7/tamperresistant_safeguards_for_openweight_llms/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/gwern       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jvk7b7/tamperresistant_safeguards_for_openweight_llms/</guid>
      <pubDate>Wed, 09 Apr 2025 23:40:43 GMT</pubDate>
    </item>
    <item>
      <title>无法实现稀疏性-PPO单步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jvckub/failing_to_implement_sparsity_ppo_singlestep/</link>
      <description><![CDATA[Hi everyone, I&#39;m trying to induce sparsity on the choices of a custom PPO RL agent (implemented using stable_baseline3), solving a single-episodic problem (basically a contextual bandit) which operates in a continuous action space implemented using gymnasium.spaces.Box(low= -1, high= +1, dtype= np.float64）。 代理必须通过选择“ n＆quot的参数向量”来优化问题框对象中的元素在选择最小的非零值条目（模块小于给定收费：1E-3）仍然充分解决问题。问题在于，无论我如何鼓励这种稀疏性，代理人根本不会选择接近0个值，看来代理甚至无法探索小值，这显然是因为他们考虑到-1至1的完整连续空间。 。我什至将成本推高，以至于唯一的奖励信号来自稀疏性。我尝试了许多不同的正则化功能，例如，参数矢量的每个非零入口和各种熵正规化（例如tsallis）的1s总和。 很明显，显然，代理人甚至无法探索小价值，甚至可以获得高昂的成本，无论选择什么，因此无论是正则化的成本是否都不在那里，都无法获得高昂的成本。我该怎么办？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/krnl_plt     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jvckub/failing_to_implement_sparsity_ppo_singlestep/</guid>
      <pubDate>Wed, 09 Apr 2025 18:11:20 GMT</pubDate>
    </item>
    <item>
      <title>多个任务的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jv6edg/approaches_for_multiple_tasks/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ 考虑一个玩具示例，机器人必须执行一系列任务A，B和C。假设：没有数据集或可用轨迹记录。我可以选择使用RL完成此操作？我是否错过了任何方法？    a，b和c的单独政策都受到了独立训练。并在满足适当条件时使用计划算法这样的决策树。    结束2结束，并具有精心设计的奖励功能。如果要添加更多任务，会发生什么？ 我是一个问这个问题，以在我的研究中获得指导。 Google在架构解决方案方面确实不太合作。谢谢您的宝贵时间。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/s_vaichu     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jv6edg/approaches_for_multiple_tasks/</guid>
      <pubDate>Wed, 09 Apr 2025 13:56:24 GMT</pubDate>
    </item>
    <item>
      <title>A2C与DL4J的连续动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jucjct/a2c_continous_action_space_with_dl4j/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在寻找帮助在DL4J中实现持续动作空间的A2C算法。 I&#39;ve implemented it for discrete action space while looking into the deprecated RL4J project but now i&#39;m stuck because i don&#39;t understand how i need to change my A2C logic to have a continous action space which returns a vector of real numbers as action. Here are my networks: private DenseModel buildActorModel() { return DenseModel.builder() 。 } private densemodel buildCriticModel（）{return densemodel.builder（）.inputSize（inputsize）.outputsize（1）.learningrate（crialityLearningRate）.l2（crialitedl2）.hiddenlayers .hiddenlayers（critichidendlayers）（critichidendlayers）.weightinit（critisInitInit（crialtInitInit）。 }    这是我的培训方法：     private void learningfromemory（）{memoryBatch memoryBatch = this.memory .allBatch（）; indarray状态= memoryBatch.states（）; Indarray ActionIndices = MemoryBatch.actions（）; indarray奖励= memoryBatch.rewards（）; indarray terminals = memoryBatch.dones（）; indarray critterOutput = model .predict（state，true）[0] .dup（）; int batchsize = menmor.size（）; indarray返回= nd4j。 double rvalue = 0.0; for（int i = batchSize -1; i＆gt; = 0; i-）{double r = rewards.getDouble（i）; boolean完成=终端.getDouble（i）＆gt; 0.0;如果（完成|| i == batchsize -1）{rvalue = r; } else {rvalue = r + gamma * critteroutput.getFloat（i + 1）; } returns.putscalar（i，rvalue）; } indarray groupages = returns .sub（critterOutput）; int numactions = getActionspace（）。size（）; indarray actorlabels = nd4j .eros（batchsize，numactions）; for（int i = 0; i＆lt; batchsize; i ++）{int actionIndex =（int）actionIndices.getDouble（i）; double Advantage = AffAtages.GetDouble（i）; ActorLabels.putscalar（new Int [] {i，ActionIndex}，Advantage）; } model.train（状态，new Indarray [] {actorLabels，return}）; }    这是我的Actor网络损失函数：    公共最终类procorcriticloss bloctorcriticloss bloct iLossfunctions iLossFunction {公共静态最终double double Default_beta = 0.01;私人最终双重测试版； public actorcriticloss（）{this（default_beta）; } public actorcriticloss（double beta）{this.beta = beta; } @Override public String name（）{return toString（）; } @Override publy double Computescore（Indarray标签，Indarray PreOutput，Iactivation ActivationFN，Indarray Mask，Boolean平均值）{返回0; } @Override public Indarray ComputesCoreArearray（Indarray标签，Indarray PreOutput，Iactivation ActivationFN，Indarray Mask）{return null; } @Override public Indarray ComputeGradient（Indarray标签，Indarray PreOutput，Iactivation ActivationFN，Indarray Mask）{indarray output = activationFn .getActivation（preOutput.dup.dup（），true）.addi（1e-8）; indarray logoutput = transform.log（输出，true）; indarray entropydev = logoutput .addi（1）; indarray dlda =输出.rdivi（labels）.subi（entropydev.muli（beta））.negi（）; indarray grad = activationfn .backprop（preOutput，dlda）.getFirst（）; if（mask！= null）{lossutil.applymask（grad，bask）; }返回毕业； } @Override公共对＆lt; double，indarray＆gt; ComputeGradientAndScore（Indarray标签，Indarray Preotput，Iactivation ActivationFN，Indarray Mask，Boolean平均值）{return null; } @Override public String toString（）{return＆quot&#39;protorcriticloss（）＆quot;; }}    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1jucjct/a2c_continous_action_space_space_with_with_dl4j/&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jucjct/a2c_continous_action_space_with_dl4j/</guid>
      <pubDate>Tue, 08 Apr 2025 12:38:28 GMT</pubDate>
    </item>
    <item>
      <title>LLM培训的好额RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ju8m9e/good_toturial_rl_for_llm_training/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi Guys  我目前正在从事纸质想法，要求我熟悉LLM培训中的RL系统。在这种情况下，我对RL很新，想知道RL是否有很好的简介。 我对基础知识很熟悉，因此任何博客都受到欢迎。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/impressive_chip_435     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ju8m9e/good_toturial_rl_for_llm_training/</guid>
      <pubDate>Tue, 08 Apr 2025 08:25:51 GMT</pubDate>
    </item>
    <item>
      <title>实时动态增强学习可能吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ju76oa/realtime_dynamic_reinforcement_learning_possible/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否可以在实时和动态环境中使用强化学习？如果可能的话，我想在这样的环境中训练它。问题在于，当我的经纪人执行一项行动或仍在训练时，环境会发生变化。在培训过程中，可以在模拟器中冻结环境。但是我该怎么办观察空间问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/faire_device_4961      [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ju76oa/realtime_dynamic_reinforcement_learning_possible/</guid>
      <pubDate>Tue, 08 Apr 2025 06:37:12 GMT</pubDate>
    </item>
    <item>
      <title>这是学习！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jtne3e/its_learning/</link>
      <description><![CDATA[        &lt;！ -  sc_off-&gt;  只是想分享，因为我很高兴！  几周前，我重新创建了 konane 的变体。刀片II：Bannerlord，在Python。 （只有几个不同的规则，例如启动播放器和第一回合） 首先尝试了qlearning，并且自我播放最终与ppo一起使用了AI播放，因为黑色碎片与白色的碎片进行随机移动。自我播放让我担心（我通过每一步切换白色和黑色碎片改变了POV）  konane对稀疏奖励（仅赢）（仅赢）和对随机移动的训练都很友好，因为每一步都是捕获。在6x6网格上，这意味着每个游戏总是在8到18个动作之间。捕获不应获得较小的奖励，因为这就像奖励国际象棋中的任何举动一样，而且双重捕获不一定比单个捕获更好，因为游戏的目标是将董事会定位，以便您的对手在进行之前就耗尽了动作。我考虑了减少对手球员的举动的较小奖励，但决定反对并将其删除，因为我希望它会学习漫长的比赛，而又一次，最终定位是最重要的，而不是让您的对手在游戏中中途进行1或2个可能的动作。 y&#39;all！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ubister      [注释]         ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jtne3e/its_learning/</guid>
      <pubDate>Mon, 07 Apr 2025 15:02:06 GMT</pubDate>
    </item>
    <item>
      <title>强化学习会议审查和提交</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jtig7z/reinforcement_learning_conference_reviews_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有人向加固学习会议（RLC）提交了论文吗？他们与作者的讨论期今天开始，他们说这不是作者的回应期，但他们可以提出澄清和问题。  因此，作者不会暗示审稿人如何看待他们的论文，对吗？澄清问题将发送给所有人，同时或仅发送给几篇论文？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/facefited_relief179     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jtig7z/reinforcement_learning_conference_reviews_and/</guid>
      <pubDate>Mon, 07 Apr 2025 10:56:58 GMT</pubDate>
    </item>
    <item>
      <title>WordQuant University MSC金融工程信誉</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jtg7we/wordquant_university_msc_in_financial_engineering/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在加入WorldQuant University的硕士金融工程计划，但我不确定其认证状态。无论是宝贵的机会还是浪费时间，我都感到困惑。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/laxuu     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jtg7we/wordquant_university_msc_in_financial_engineering/</guid>
      <pubDate>Mon, 07 Apr 2025 08:15:45 GMT</pubDate>
    </item>
    <item>
      <title>关于RL的分类是否正确？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jtdqli/is_this_classification_about_rl_correct/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我在网站上看到了此分类表： https://comfyai.app/article/alticle/llm-posttraining/reinforcement-Lectrescement-Learning 。但是我对“一半在线，半离线”有些困惑。 DQN的一部分。一半半吗？          &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/great-reception447     [link]       [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jtdqli/is_this_classification_about_rl_correct/</guid>
      <pubDate>Mon, 07 Apr 2025 05:16:46 GMT</pubDate>
    </item>
    <item>
      <title>在Mac（M2芯片）上下载Metaworld和DMC健身房</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jszoha/download_metaworld_and_dmc_gym_on_mac_m2_chip/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在开始一个项目，但我无法在笔记本电脑上下载metaworld和dmc。有人遇到同样的问题并可以帮助我吗？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ismail_el_minawi6      [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1jszoha/1jszoha/download_metaworld_and_dmc_gym_gym_on_mac_mac_mac_m2_chip/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jszoha/download_metaworld_and_dmc_gym_on_mac_m2_chip/</guid>
      <pubDate>Sun, 06 Apr 2025 17:37:32 GMT</pubDate>
    </item>
    <item>
      <title>哪些深入的RL主题具有有希望的实际影响？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jspgca/what_are_some_deep_rl_topics_with_promising/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试识别（可能（可能）（可能）具有实用性影响的深度RL研究主题，但会感到迷失。 一方面，一方面，POLICY RL算法（如PPO）在某些域中似乎在某些域中在某些域中可以很好地工作，例如，robot locomotion locomotion and llm post and and and and and llm。但是，多年来的核心算法并没有发生太大变化，并且在改进算法方面似乎很少（据我所知 - 例如[1]，[2]，从引用数量的数量中，几乎没有引起关注）。只是在算法侧没有什么可做的？但是，非政策RL似乎并未在实际应用中广泛使用，只有几个（例如，现实世界的机器人RL [3]）。 那么，有一些新颖的范式，例如离线RL，meta-rl，Meta-rl，理论上是富裕而有趣的，但它们的真实世界似乎很遥远。 和在近学期或中期显示现实世界中使用的承诺？  [1] Singla，J。，Agarwal，A。，＆＆＆＆＆＆amp; D. Pathak（2024）。 SAPG：拆分和汇总政策梯度。  arxiv，abs/abs/2407.20230 。  [2]王Pathak，D。（2025）。进化政策优化。  [3] Luo，J.，Hu，Z.，Xu，C.，Tan，Y.L.，Berg，J.，Sharma，A.，Schaal，S.，Finn，C.，Gupta，Gupta，A。 Levine，S。（2024）。 SERL：用于样品有效机器人增强学习的软件套件。  2024 IEEE国际机器人和自动化会议（ICRA），16961-16969。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/xyllong     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jspgca/what_are_some_deep_rl_topics_with_promising/</guid>
      <pubDate>Sun, 06 Apr 2025 08:26:04 GMT</pubDate>
    </item>
    <item>
      <title>GPU关于机器人技术和增强学习的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jsmhwf/gpu_recommendation_for_robotics_and_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您好，我打算购买一台PC，以测试使用（几乎）逼真的水物理和力量的简单游泳机器人鱼的强化学习。然后将应用于真实的硬件版本。到目前为止，我看到的是需要一些CFD。我当前的PC没有GPU，几乎不能以5 fps的速度运行简单的Mujoco示例。我打算运行软件库Mujoco，Webots，Guazebo，ROS，基于CFD的库，Unity Engine，Unreal Engine，基本上需要什么。   什么NVIDIA GPU足以满足这些任务？我正在考虑获得5070ti。 较便宜的选项，例如4060、4060TI，3060等？ 我愿意花费高达5070TI级别。但是，如果是过度杀伤，我将获得一张较旧的Gen Lower Tier卡。我的学院有4090和A6000 GPU的工作站计算机，但是他们总是需要许可安装任何减慢我的Wokflow的东西，因此我想为自己购买一张卡片来为自己尝试一些东西，然后将工作转移到更大的计算机上。  （我选择NVIDIA，因为大多数可用的项目代码都使用CUDA，并且我不确定与ROCM的AMD卡现在是否现在可以提供任何好处/支持）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/poop_studio     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jsmhwf/gpu_recommendation_for_robotics_and_reinforcement/</guid>
      <pubDate>Sun, 06 Apr 2025 04:59:46 GMT</pubDate>
    </item>
    <item>
      <title>将LLM申请视为POMDP，而不是代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jsalgg/think_of_llm_applications_as_pomdps_not_agents/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/bianconi     ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jsalgg/think_of_llm_applications_as_pomdps_not_agents/</guid>
      <pubDate>Sat, 05 Apr 2025 18:50:37 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>