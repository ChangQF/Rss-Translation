<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 31 Oct 2024 03:25:59 GMT</lastBuildDate>
    <item>
      <title>“CodeIt：具有优先后视重放功能的自我改进语言模型”，Butt 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gg3zbi/codeit_selfimproving_language_models_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gg3zbi/codeit_selfimproving_language_models_with/</guid>
      <pubDate>Thu, 31 Oct 2024 02:13:03 GMT</pubDate>
    </item>
    <item>
      <title>我正在为想要创建和协作创新项目的人工智能人士构建一个在线平台！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfruhy/im_building_an_online_platform_for_people_in_ai/</link>
      <description><![CDATA[大家好 :) 我有一些很酷的东西想和你们分享，在过去的几个月里，我一直在努力寻找实现梦想的方法 我正在为那些关心技术创新并通过建设和参与项目产生积极影响的人工智能人士创建一个在线中心 这个中心将是一个寻找志同道合的人联系并一起从事激情项目的地方。 目前，我们正在编写一个平台，以便每个人都能找到彼此并相互了解 在我们获得一些初始用户后，我们将从短期建设者计划开始，个人和团队可以参加在线竞赛，最突出的项目可以获得一些奖品 :) 我们的目标是通过帮助他人做同样的事情来让世界变得更美好 如果您喜欢我们的倡议，请在下面的网站上注册！ https://www.yournewway-ai.com/  几周后，一旦我们准备就绪，我们将向您发送加入我们平台的邀请 :)    提交人    /u/unknownstudentoflife   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfruhy/im_building_an_online_platform_for_people_in_ai/</guid>
      <pubDate>Wed, 30 Oct 2024 17:16:40 GMT</pubDate>
    </item>
    <item>
      <title>使用此 YouTube 聊天工具学习 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfrasj/learn_rl_using_this_youtube_chat_tool/</link>
      <description><![CDATA[      对于 RL 的自学者，我知道有大量超过一小时的 RL YouTube 视频。所以我创建了这个 YouTube 聊天，这样我就可以与视频聊天并进行总结。欢迎尝试！  www.arcanx-search.com https://preview.redd.it/8sefolufbxxd1.png?width=2988&amp;format=png&amp;auto=webp&amp;s=5a7e4ec3b2b1c5b92f75ea9a677c9102efcf58e9    提交人    /u/Limp-Run-9075   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfrasj/learn_rl_using_this_youtube_chat_tool/</guid>
      <pubDate>Wed, 30 Oct 2024 16:54:08 GMT</pubDate>
    </item>
    <item>
      <title>有没有论文研究过RL算法和优化器之间的关系？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfm7a3/are_there_any_papers_that_have_studied_the/</link>
      <description><![CDATA[我个人正在尝试很多简单的算法，我已经考虑这个问题一段时间了：adam 的动量可以帮助模型快速收敛，但当目标值发生变化时，这会有害吗，就像 RL 一样？如果大部分动量超出预测，并且目标网络复制了这种超出，则模型可能会发散。有人对此做过研究吗？    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfm7a3/are_there_any_papers_that_have_studied_the/</guid>
      <pubDate>Wed, 30 Oct 2024 13:13:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 实现的 PPO，学习动作分布的平均值和标准差 σ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfgwwq/ppo_implementation_in_python_with_learned_mean/</link>
      <description><![CDATA[我正在寻找有关 Python 软件包的推荐，这些软件包允许训练 PPO 代理，其中动作分布的平均值 (μ) 和标准差 (σ) 都是学习到的。 具体来说，我需要模型在给定当前观察的情况下在推理过程中输出动作和相应的标准差。这将帮助我评估代理对其预测的信心。 我见过关于 skrl 和 AgileRL 软件包的讨论，它们据称是模块化的。有人用过这些吗？他们也可以提供学习到的 σ 吗？ 任何建议或见解都将不胜感激。 谢谢。    提交人    /u/Disastrous_Effort725   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfgwwq/ppo_implementation_in_python_with_learned_mean/</guid>
      <pubDate>Wed, 30 Oct 2024 07:23:26 GMT</pubDate>
    </item>
    <item>
      <title>Meta FAIR CodeGen 团队的新 RL 实习机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfam2x/new_rl_internship_at_meta_fair_codegen_team/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfam2x/new_rl_internship_at_meta_fair_codegen_team/</guid>
      <pubDate>Wed, 30 Oct 2024 00:58:26 GMT</pubDate>
    </item>
    <item>
      <title>“Centaur：人类认知的基础模型”，Binz 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gf6nsf/centaur_a_foundation_model_of_human_cognition/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gf6nsf/centaur_a_foundation_model_of_human_cognition/</guid>
      <pubDate>Tue, 29 Oct 2024 21:55:42 GMT</pubDate>
    </item>
    <item>
      <title>开源火箭联盟环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gf3q6d/open_source_rocket_league_environment/</link>
      <description><![CDATA[        提交人    /u/compressor0101   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gf3q6d/open_source_rocket_league_environment/</guid>
      <pubDate>Tue, 29 Oct 2024 19:51:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 Memoroids 进行循环强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gekts9/recurrent_reinforcement_learning_with_memoroids/</link>
      <description><![CDATA[  由    /u/smorad  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gekts9/recurrent_reinforcement_learning_with_memoroids/</guid>
      <pubDate>Tue, 29 Oct 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家模型对所有输入采取相同的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gek86t/actor_critic_model_taking_same_action_for_all/</link>
      <description><![CDATA[大家好， 我正在研究 Actor Critic 模型，以便根据时间序列数据做出决策。我尝试了不同的模型架构，如 LSTM、Transformer 甚至 Mamba，以检查任何不同的结果，但没有任何变化。基本上，我将时间序列数据传递给我的模型，以便在每个时间步骤中从 6 个动作中选择一个动作。 对于输入数据，我尝试了 2 种不同的方法；全上下文长度和输入形状如 (seq_len, hidden_​​dim)，在这种情况下，模型将采取所有时间步骤并为每个时间步骤提供动作输出。我还尝试了固定上下文长度和类似 (batch_size, seq_len, hidden_​​dim) 的输入形状，在这种情况下，模型为每个批次而不是时间步长创建一个动作输出。 我还实现了 Epsilon Greedy 进行探索，并在每个 epoch 结束时打印每个动作的选定动作百分比作为输出，以检查模型输出。 我的问题从这一点开始，我正在使用 epsilon 退火进行 epoch，在训练时它正在减少 epsilon 数以减少探索。当我检查我的 Critic Loss 值随时间的变化时，它正在显着减少（Critic 的 MSE Loss 从 0.9 左右开始，对于 +1 和 -1 之间的奖励值下降到 0.002）所以我认为批评家正在学习。 在每个 epoch 结束时，我还在运行评估以查看状态，在评估时，我还在检查每个动作的选定动作百分比。问题是，无论输入是什么，我的模型都会选择相同的动作。在训练中，我使用贪婪的 epsilon 分类抽样，因此它选择了不同的动作，但是当我在评估步骤使用 argmax 时，它选择了相同的动作。  我尝试过的；具有各种参数大小的不同模型架构。不同的学习率，从太高到太低。不同的初始 epsilon 值。不同的输入类型（固定长度和完整上下文）  它们都没有任何区别。我从这个例子中实现了我的演员评论家模型，并仔细检查了是否有任何错误； https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f 经过这一切，我找不到任何解决方案。有人有什么想法吗？    由    /u/BagComprehensive79  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gek86t/actor_critic_model_taking_same_action_for_all/</guid>
      <pubDate>Tue, 29 Oct 2024 02:26:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习不仅能解决难题，还能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1geez5d/rl_solves_hard_problems_but_also/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1geez5d/rl_solves_hard_problems_but_also/</guid>
      <pubDate>Mon, 28 Oct 2024 22:21:13 GMT</pubDate>
    </item>
    <item>
      <title>多头PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gee2o7/multihead_ppo/</link>
      <description><![CDATA[大家好， 我有一个有四个头的网络。一个头（有三个动作）决定是否执行第二个、第三个或第四个头的动作。第二个头的动作有助于情节结束，而第三个和第四个头的动作对于实现主要目标很重要。我正在使用 PPO，但有时网络会卡在第三个或第四个头，因此情节永远不会结束。有人知道是什么导致了这种行为吗？    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gee2o7/multihead_ppo/</guid>
      <pubDate>Mon, 28 Oct 2024 21:43:11 GMT</pubDate>
    </item>
    <item>
      <title>机器人深度强化学习：现实世界的成功调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ge2hn5/deep_reinforcement_learning_for_robotics_a_survey/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ge2hn5/deep_reinforcement_learning_for_robotics_a_survey/</guid>
      <pubDate>Mon, 28 Oct 2024 13:48:41 GMT</pubDate>
    </item>
    <item>
      <title>期待明年开始攻读 RL 博士学位——寻求建议！🤞🏼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdyx82/looking_forward_to_start_phd_in_rl_next_year/</link>
      <description><![CDATA[大家好！我准备明年开始攻读强化学习博士学位，但需要一些关于积累研究经验和与教授联系的建议。  背景：我做过很多实际的强化学习项目（MARL、PettingZoo、策略梯度工作），但我缺乏正式的研究经验。我也有一年的 ML/LLM 实习经历，但寻找强化学习的研究实习机会一直很困难。 挑战：我担心接触 LOR，因为我没有直接与强化学习的教授合作过。任何关于有助于展示我的能力的项目建议或接触技巧都将非常有帮助。另外，如果有人有兴趣合作，请直接发信息给我！ &lt;3  任何见解都值得赞赏 - 谢谢大家！    提交人    /u/cheenchann   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdyx82/looking_forward_to_start_phd_in_rl_next_year/</guid>
      <pubDate>Mon, 28 Oct 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>哪些 RL 算法适用于计算心理学？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdytbm/which_rl_algorithms_for_computational_psychology/</link>
      <description><![CDATA[我是一名数据科学家，希望通过多个代理模拟人类社交互动。我只是想知道是否有人可以指点一下要探索哪些算法。例如，如果我想鼓励反馈循环和突发行为以更真实地描述人类行为，我应该使用无模型算法还是基于模型的算法？ 从我最初的研究中，我听说了关于 Decision Transformer 和 DreamerV3 的好评。 感谢您的时间！    提交人    /u/culturedindividual   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdytbm/which_rl_algorithms_for_computational_psychology/</guid>
      <pubDate>Mon, 28 Oct 2024 10:33:59 GMT</pubDate>
    </item>
    </channel>
</rss>