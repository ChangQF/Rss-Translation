<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 07 Feb 2025 15:18:32 GMT</lastBuildDate>
    <item>
      <title>Schnell等人“时间差异学习：为什么会变得快速，如何更快”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijxavb/temporal_difference_learning_why_it_can_be_fast/</link>
      <description><![CDATA[    [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijxavb/temporal_difference_learning_why_it_can_be_fast/</guid>
      <pubDate>Fri, 07 Feb 2025 15:17:07 GMT</pubDate>
    </item>
    <item>
      <title>“用字母分析求解奥林匹克几何形状的金医师表现”，Chervonyi等2025 {dm}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijx584/goldmedalist_performance_in_solving_olympiad/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijx584/goldmedalist_performance_in_solving_olympiad/</guid>
      <pubDate>Fri, 07 Feb 2025 15:10:04 GMT</pubDate>
    </item>
    <item>
      <title>困惑Pro 7.99 $ / yr</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijvk6b/perplexity_pro_799_yr/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好！我以7.99 $/yr（每月的0.66 $！）的价格出售困惑Pro。    Pro访问可以直接在您的电子邮件中激活！您可以通过PayPal，Wise，USDT，ETH，UPI，PAYTM或其他方法轻松付款。  •不要错过这笔负担得起的交易！这是通过困惑Pro合作计划100％合法的。   dm me或在下面发表评论，如果有兴趣！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/panelable_smm     [link]  ＆＃32;   [注释]    /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijvk6b/perplexity_pro_799_yr/</guid>
      <pubDate>Fri, 07 Feb 2025 13:57:48 GMT</pubDate>
    </item>
    <item>
      <title>TMLR或UAI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijva4i/tmlr_or_uai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，伙计，博士学位ML学生这个方面。实际上，我对我的工作的潜在场所感到困惑。因此，如您所知，UAI截止日期是2月10日，此后，我看到的著名会议（以ML为核心）是神经，该神经在5月的截止日期。  所以我想知道TMLR是否比UAI更好，而我知道ICML，ICLR和Neurips游戏完全不同，我只是想知道我是否应该继续前进或更喜欢提交向TMLR工作。   ps：工作在在线学习的空间中，主要是为强盗文献做出贡献（高度理论），而LLM SPSCE   pps汲取了动机：不确定是否重要，但是我更倾向于在我的博士学位  &lt;！ -  sc_on-&gt;＆＃32;之后更倾向于行业角色。提交由＆＃32; /u/u/fanding-nerve-4056      fink]  ＆＃32;   [comment]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijva4i/tmlr_or_uai/</guid>
      <pubDate>Fri, 07 Feb 2025 13:43:49 GMT</pubDate>
    </item>
    <item>
      <title>您将如何使用很少的数据来为编程语言进行RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijujzz/how_would_you_go_about_doing_rl_for_a_programming/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  如果说我可以编译代码以使用错误作为奖励的一部分，那么训练LLM的最佳方法是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/new_description8537      [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ijujzz/how_would_you_go_go_about_doing_doing_rl_for_for_a_a_a_programming/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijujzz/how_would_you_go_about_doing_rl_for_a_programming/</guid>
      <pubDate>Fri, 07 Feb 2025 13:06:49 GMT</pubDate>
    </item>
    <item>
      <title>我们的RL框架将用于快速，进化HPO的任何网络/算法转换。我们应该使LLM可用于进化RL推理培训吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijr36h/our_rl_framework_converts_any_networkalgorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家，我们刚刚发布了agilerl v2.0！ 查看最新更新： https://github.com/agilerl/agilerl     agilerl是一个RL培训库，可实现任何网络的进化超参数优化。我们的基准测试比rllib的训练更快10倍。 这是我们添加的一些很酷的功能：  广义突变 - 一个完全模块化的，灵活的网络和RL的柔性突变框架超参数。  Evolvablenetwork API  - 在可演化的设置中使用任何网络架构，包括预验证的网络。  evolvableAlgorithm层次结构 - 简化的进化RL算法的实现。 EvolvableModule层次结构 - 跟踪复杂网络中突变的更聪明的方法。 支持复杂空间 - 使用EvolvableMultiinput无缝处理多输入空间。  知道是：我们应该将其完全扩展到LLM吗？当前大型型号的HPO并不是真正的，因为它们是如此难/昂贵。但是我们的框架可以使其更加有效。我已经意识到人们比较了用于在DeepSeek R0娱乐活动中获得更好结果的超参数的人，这意味着这可能很有用。我想知道您对进化HPO是否对培训大型推理模型有用的想法？而且，如果有人幻想有助于为这项工作做出贡献，我们会很喜欢您的帮助！谢谢   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nicku_a     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijr36h/our_rl_framework_converts_any_networkalgorithm/</guid>
      <pubDate>Fri, 07 Feb 2025 09:21:43 GMT</pubDate>
    </item>
    <item>
      <title>谁能帮助我（自定义env + SB3）？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijektz/can_anyone_help_me_custom_env_sb3/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我创建了一个自定义的健身房环境，该环境与Java中的模拟器对话。基本上，它从光网络收集Infos。 OBS空间是拓扑，动作空间是一个路由和初始插槽，可以将流动分配。要处理的流是事件中断的流。每个事件都有一堆中断的流。我正在尝试训练一个代理商，为每条路线和插槽分配流程的每条流程做出明智的决策。一旦分配流动，拓扑就会发生变化，否则什么都不会改变。我正在使用SB3（DQN，MLPPOLICY），并将时间步骤设置为每个事件的流量数（这是必须这样做的方式，因为它与模拟器进行了对话）。问题是，当事件具有x流数时，Model.learn（）执行2或3个步骤，而不是流量数。它会引起混乱，因为模拟器试图处理新事件的新流程，但是它会从模型中重复流过。关于如何解决这个问题的想法？我可以共享代码和联系人，我真的需要解决此问题。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/leilaff89     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijektz/can_anyone_help_me_custom_env_sb3/</guid>
      <pubDate>Thu, 06 Feb 2025 21:52:27 GMT</pubDate>
    </item>
    <item>
      <title>“多基因填充：与多种推理链的自我改进”，Subramaniam等2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij81ye/multiagent_finetuning_self_improvement_with/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij81ye/multiagent_finetuning_self_improvement_with/</guid>
      <pubDate>Thu, 06 Feb 2025 17:27:45 GMT</pubDate>
    </item>
    <item>
      <title>Chen等人“对长马相互作用LLM代理的增强学习”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij2nfz/reinforcement_learning_for_longhorizon/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij2nfz/reinforcement_learning_for_longhorizon/</guid>
      <pubDate>Thu, 06 Feb 2025 13:34:36 GMT</pubDate>
    </item>
    <item>
      <title>用于定制演员和评论家网络的RL库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我希望测试Pytorch和基准指标中的自定义神经网络（即收敛速率）与Actor-Cricit-Critic RL算法中的标准MLP。我已经围绕着subreddit查看，并且已经建议使用以下库来实施此类网络：   rllib   rlpyt   skrl &lt;&gt; /li&gt;  torchrl   对这些意见有什么意见或良好的经验？我已经看到了对rllib的爱与恨，但在过去三个中没有太多的爱。我正在尝试避免SB3，因为我认为我的神经网络不属于他们拥有的任何自定义策略类别，除非我非常误解了他们的自定义策略类别的工作方式。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/voltimeters     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</guid>
      <pubDate>Thu, 06 Feb 2025 12:55:17 GMT</pubDate>
    </item>
    <item>
      <title>有关Mappo实施的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij0m2l/question_about_mappo_implementation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij0m2l/question_about_mappo_implementation/</guid>
      <pubDate>Thu, 06 Feb 2025 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>积极的在线运动计划和决策|印度| Swaayatt机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiyt2j/aggressive_online_motion_planning_and_decision/</link>
      <description><![CDATA[    该系统即时执行动态轨迹计算，对24米内的障碍物做出反应半径。演示展示了曲折和左车道避免模式，尽管车身挑战很高，但车辆的速度仍高于45 kmph。 。 &gt; youtube_link   该框架在单线程i7处理器上以800+ Hz运行，并将轨迹跟踪系统集成了纯粹的追击系统。未来的计划包括使用端到端的深度强化学习扩展框架 原始作者LinkedIn： sanjeev_sharma_linkedin a&gt; 原始链接帖子： pose_link    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/shani_786     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiyt2j/aggressive_online_motion_planning_and_decision/</guid>
      <pubDate>Thu, 06 Feb 2025 09:36:49 GMT</pubDate>
    </item>
    <item>
      <title>需要有关高级RL资源的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我现在已经深入了强化学习，但是我撞墙了。我发现的几乎每个课程或资源都涵盖了相同的内容 -  PPO，SAC，DDPG等。它们非常适合理解基础知识，但我感到卡住了。就像我只是在相同的算法周围盘旋而没有真正前进。 我试图弄清楚如何摆脱它并进入更高级或更新的RL方法。诸如遗憾最小化，基于模型的RL，甚至是多代理系统＆amp＆amp; HRL听起来令人兴奋，但我不确定从哪里开始。 其他人有这种感觉吗？如果您设法推动了这个高原，您是如何做到的？任何课程，论文甚至个人提示都将非常有帮助。 事先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/helpful-number1288     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</guid>
      <pubDate>Thu, 06 Feb 2025 08:15:09 GMT</pubDate>
    </item>
    <item>
      <title>RL不适用于运动控制和学习！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想知道有人知道使用RL用于运动学习的研究吗？我听说它从来没有能够在现实世界中进行建模或控制运动的工作。这是真的吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/berkeleyears     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</guid>
      <pubDate>Thu, 06 Feb 2025 04:55:56 GMT</pubDate>
    </item>
    <item>
      <title>RL对类人形生物的控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我有兴趣从事基于RL的类人体控制器。如果您可以将一些重要的资源列为起点，我将非常感谢。谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/rua0ra1     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</guid>
      <pubDate>Thu, 06 Feb 2025 04:42:30 GMT</pubDate>
    </item>
    </channel>
</rss>