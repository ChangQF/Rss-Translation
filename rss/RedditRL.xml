<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 18 Dec 2024 01:19:16 GMT</lastBuildDate>
    <item>
      <title>强化学习代理趋向于不做任何事/负奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgq3z8/rl_agent_converging_on_doing_nothing_negative/</link>
      <description><![CDATA[大家好 - 我正在使用 gymnasium、stable baselines 3 和 pyboy 创建一个代理来玩 NES/GBC 游戏 1942。我在训练中遇到了一个问题，我的代理不断收敛到暂停游戏并坐在那里什么也不做的策略。我尝试过放大正奖励、使负奖励极端化、使用帧缓冲区分配负奖励、生存奖励、负生存信号，但我似乎无法理解是什么导致了这种行为。以前有人见过这样的事情吗？  我的代码在这里：https://github.com/lukerenchik/NineteenFourtyTwoRL 行为可视化在这里：https://www.youtube.com/watch?v=Aaisc4rbD5A    提交人    /u/LukeRenchik   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgq3z8/rl_agent_converging_on_doing_nothing_negative/</guid>
      <pubDate>Wed, 18 Dec 2024 01:12:36 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity AI Pro 1 年优惠券 - 仅需 25 美元（23 欧元）| 订阅后付款！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hglmhb/perplexity_ai_pro_1year_coupon_only_25_23/</link>
      <description><![CDATA[仅需 25 美元即可获得 1 年 Perplexity Pro 促销代码（节省 175 美元！） 以合理的价格使用顶级模型和工具增强您的 AI 体验： 高级 AI 模型：访问 GPT-4o、o1 和Llama 3.1 还利用了 Claude 3.5 Sonnet、Claude 3.5 Haiku 和 Grok-2。 图像生成：探索 Flux.1、DALL-E 3 和 Playground v3 Stable Diffusion XL 可供无有效 Pro 订阅的用户使用，可在全球范围内访问。 简单的购买流程： 加入我们的社区： Discord 拥有 500 多名成员。 安全付款：使用 PayPal 保障您的安全和买家保护。 即时访问：通过简单的促销链接接收您的代码。 为什么选择我们？ 我们的业绩记录不言而喻。 查看我们的已验证买家 + VIP 买家和客户反馈 2、反馈 3、反馈 4、反馈 5    提交人    /u/AICentralZA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hglmhb/perplexity_ai_pro_1year_coupon_only_25_23/</guid>
      <pubDate>Tue, 17 Dec 2024 21:41:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习工作原理的示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</link>
      <description><![CDATA[  由    /u/A-Sexy-Name  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</guid>
      <pubDate>Tue, 17 Dec 2024 21:11:18 GMT</pubDate>
    </item>
    <item>
      <title>对 Gt 和 Rt 的条件期望的使用感到困惑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hggz4f/confused_over_usage_of_conditional_expectation/</link>
      <description><![CDATA[      来自&quot;强化学习：简介&quot;我明白了 https://preview.redd.it/0nbi4o798g7e1.png?width=549&amp;format=png&amp;auto=webp&amp;s=ca68ef2e18e1c301c942b1031170c57380a60d9d 我理解，根据多重条件期望的公式，上述内容是正确的。 但是，当我像下面这样对 St-1、At-1 和 St 取 Gt 的期望时，两个项都是相等。 E[Gt | St-1=s, At-1=a, St=s`] = E[Gt | St = s`]。因为我可以利用马尔可夫特性，所以 Gt 取决于 St 而不是之前的状态。这个技巧是推导状态值函数的贝尔曼方程所必需的。 我的问题为什么 Gt 取决于当前状态而不是 Rt？ 谢谢    提交人    /u/mono1110   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hggz4f/confused_over_usage_of_conditional_expectation/</guid>
      <pubDate>Tue, 17 Dec 2024 18:17:25 GMT</pubDate>
    </item>
    <item>
      <title>p(s`, r | s, a) 与 p(s` | s, a) 相同吗？？？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgeuly/is_ps_r_s_a_same_as_ps_s_a/</link>
      <description><![CDATA[   目前正在阅读&quot;强化学习：简介&quot;作者：Barto 和 Sutton。 给定一个状态和动作，下一个状态的概率和与下一个状态相关的奖励应该相同。这就是我的理解。 我的理解是两者应该相同，但这本书似乎对此有不同的处理。例如在下面的等式中（第 49 页） https://preview.redd.it/36v3pegmtf7e1.png?width=549&amp;format=png&amp;auto=webp&amp;s=440ffff0dc5594d52d2c6cda67757b15b8e5173e 根据条件概率规则，上述等式是正确的。我的疑问是这两个概率是如何不同的。 我在这里遗漏了什么？ 谢谢    提交人    /u/mono1110   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgeuly/is_ps_r_s_a_same_as_ps_s_a/</guid>
      <pubDate>Tue, 17 Dec 2024 16:47:15 GMT</pubDate>
    </item>
    <item>
      <title>学习代理 | Unreal Fest 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgdja2/learning_agents_unreal_fest_2024/</link>
      <description><![CDATA[       由    /u/Deathcalibur  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgdja2/learning_agents_unreal_fest_2024/</guid>
      <pubDate>Tue, 17 Dec 2024 15:49:26 GMT</pubDate>
    </item>
    <item>
      <title>尚未探索的救援方法是否具有人工智能增强的潜力？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgct66/unexplored_rescue_methods_with_potential_for/</link>
      <description><![CDATA[我目前正在考虑高中毕业设计要做什么，我想做一些涉及强化控制无人机（与环境交互的人工智能）的事情。然而，我一直在努力寻找任何可以轻松实现人工智能无人机的应用。我正在寻找可以从自动无人机中获益的救援行动，比如消防，但一直遇到问题，比如火灾中无人机的热损伤。人工智能无人机在危险的救援行动中可能优于人类，或者在大面积或无人机飞行员有限的地方，如日本地震区或人类的辐射限制，优于人类的远程控制。它也应该是一些尚未探索的东西，比如无人机稳定地使用水管，而不是更常见的事情，比如使用计算机视觉进行监控或救援搜索。我正在尝试寻找一些物理上可行的无人机，但尚未被探索过。 你们对我可以在物理模拟中做的实现有什么想法吗？在这种模拟中，可以训练 AI 无人机在生命攸关的情况下完成对人类来说太危险或太忙碌的任务？ 我将不胜感激任何答案，希望找到一些可以在我的强化学习项目的训练环境中实现的东西。    提交人    /u/Specific_Bad8641   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgct66/unexplored_rescue_methods_with_potential_for/</guid>
      <pubDate>Tue, 17 Dec 2024 15:16:26 GMT</pubDate>
    </item>
    <item>
      <title>辩论统计评估（样本效率曲线）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hg9062/debating_statistical_evaluation_sample_efficiency/</link>
      <description><![CDATA[      嗨各位， 我提交的一篇论文已经进入期刊接受的后期阶段。但是，关于评估协议仍然存在持续的冲突。我很乐意听听大家对统计指标和汇总的意见。 假设我在 5 个随机种子（重复）上训练了一个算法，并在给定不同时间步长的几个情节中对其进行了评估。包含情节回报的 numpy 数组可能如下所示： (5, 101, 50) Dim 0：运行次数 Dim 1：时间步长 Dim 2：评估情节次数 您是先对运行次数求平均值，然后计算平均值和标准差，还是将运行次数和情节维度合并为 (101, 250)，然后取平均值和标准差？ 我认为这在研究论文中通常不清楚。在我的特定情况下，首先进行聚合会导致非常严格的标准差和置信区间。因此，我更喜欢对所有原始事件回报取平均值和标准差。 通常，我遵循 Rliable 协议。对于样本效率曲线，建议使用四分位均值和分层引导置信区间。在目前的审核过程中，Rliable 被认为不适合仅运行 5 次。 如果能听到一些意见就太好了！ 运行次数与集数    提交人    /u/LilHairdy   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hg9062/debating_statistical_evaluation_sample_efficiency/</guid>
      <pubDate>Tue, 17 Dec 2024 11:53:59 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 的奖励设计注意事项</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hg8m8j/reward_design_considerations_for_reinforce/</link>
      <description><![CDATA[我刚刚为推车杆环境（离散动作）开发了一个有效的 REINFORCE 代理，作为一项学习练习，现在我正试图将其转换为自定义玩具环境。 该环境是一个简单的骰子游戏，其中通过采取动作 (0) 来掷出两个六面骰子，它们的总和添加到每次掷骰子时累积的分数中。如果分数落在 10 的倍数上（“陷阱”），则整个分数都会丢失。可以采取行动 (1) 自愿结束情节，并保留累积分数。最终，网络应该学会平衡失去所有分数的风险和增加分数的奖励。 直观地说，由于两个骰子的预期总和是 7，因此任何低于陷阱 7 的值都应被标识为更高风险状态（即 3、13、23......），并且这个数字越高，就越应该停止情节并接受当前奖励。 以下是状态和动作的摘要。 动作： [roll，end_episode] 状态： [score，distance_to_next_trap，multiple_traps_in_range]（所有整数值，后一个变量跟踪一次掷骰子是否可以达到多个陷阱，特殊情况是当前分数低于陷阱 2） 到目前为止，我已经考虑了两种不同的结构奖励函数：  稀疏奖励结构，其中仅在采取行动 1 时才给出奖励 = 分数， 使用中间奖励，其中每次成功滚动未落在陷阱上时都会给出 +1，如果落在陷阱上，则会给出奖励 = -分数。  在这两种情况下，我还没有取得好成绩。我正在运行 10000 集，并且知道 REINFORCE 收敛速度很慢，所以我认为这可能太低了。我目前也将我的时间步骤限制为 50。 希望我已经表达清楚了。如果有人有任何有用的见解或进一步的问题，我们将非常欢迎。我目前计划的下一步如下：  在插入策略网络之前对状态进行规范化。 在计算折扣回报之前对奖励进行规范化。  [编辑 1] 我发现我的对数概率正在变得非常小。我现在正在阅读有关熵正则化的内容。    提交人    /u/Melzy_the_First   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hg8m8j/reward_design_considerations_for_reinforce/</guid>
      <pubDate>Tue, 17 Dec 2024 11:27:27 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习控制交通信号灯的最新进展如何？硕士论文的思路是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hg8m4m/whats_the_state_of_the_art_in_traffic_light/</link>
      <description><![CDATA[大家好， 我目前正在计划我的硕士论文，我对RL在交通灯控制系统中的应用很感兴趣。 我遇到过使用不同算法的研究。但是，我想知道：  该领域目前的最新技术水平是什么？是否有任何值得注意的论文、基准或现实世界的实现？ 存在哪些挑战或差距需要解决？例如，是否存在可扩展性、实时适应性或多智能体合作问题？ 创新想法：  是否有尚未应用于该领域的有前途的 RL 算法？ 我可以探索混合方法（例如，将 RL 与启发式方法相结合）吗？ 是否可以合并新类型的数据，例如实时行人或骑自行车的人的行为？   我非常感谢任何见解、资源链接或关于我可以采取什么方向为该领域做出有意义贡献的一般建议。 提前感谢您的帮助！    提交人    /u/__Baki__Hanma__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hg8m4m/whats_the_state_of_the_art_in_traffic_light/</guid>
      <pubDate>Tue, 17 Dec 2024 11:27:15 GMT</pubDate>
    </item>
    <item>
      <title>“模仿、探索和自我提升：慢思维推理系统的复现报告”，Min 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hfr26r/imitate_explore_and_selfimprove_a_reproduction/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hfr26r/imitate_explore_and_selfimprove_a_reproduction/</guid>
      <pubDate>Mon, 16 Dec 2024 19:20:34 GMT</pubDate>
    </item>
    <item>
      <title>奖励函数思路</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hfmcbf/reward_function_ideas/</link>
      <description><![CDATA[我有一个机器人在人群中走动。我希望机器人接近每个人并给他们拍照。 只有当机器人距离目标足够近并看着目标时，它才能拍照。多次拍摄同一张脸部照片是没有意义的。 对于这种用例，你会如何设计奖励函数？🙏    提交人    /u/CuriousDolphin1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hfmcbf/reward_function_ideas/</guid>
      <pubDate>Mon, 16 Dec 2024 16:01:42 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI Gym 环境表不起作用。替代品在哪里？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hfjxbb/openai_gym_table_of_environments_not_working/</link>
      <description><![CDATA[我是 RL 的完全初学者，如果这是常识的话，我很抱歉。我刚刚开始学习该主题的课程。 这是 OpenAI 的 github 链接，他们在那里保存环境表：https://github.com/openai/gym/wiki/Table-of-environments 单击此表中的任何链接（例如 CartPole-v0）都会将您重定向到 gym.openai.com 的某个页面，据我从这篇 reddit 帖子 中理解。它已被 https://www.gymlibrary.dev/ 取代。 现在我可以在哪里找到这些环境的链接？    提交人    /u/iamconfusion1996   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hfjxbb/openai_gym_table_of_environments_not_working/</guid>
      <pubDate>Mon, 16 Dec 2024 14:10:56 GMT</pubDate>
    </item>
    <item>
      <title>仅参与者的 REINFORCE 算法的性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hfjklc/performance_of_actoronly_reinforce_algorithm/</link>
      <description><![CDATA[嗨， 这个问题似乎毫无意义，但我感兴趣的是，具有以下属性的算法的性能可能如何：  仅限演员 强化优化（使用完整情节生成梯度并计算累积奖励） 参数集较少。例如：2 层 CNN + 2 线性层（假设 LL 上有 200 个隐藏参数） 除了使帧更小（例如 64x64）外，不对帧进行预处理 1e-6 学习率  在长情节环境中。例如 atari pong 可能需要 3000 帧才能获得 -21 奖励，可能需要 10k 帧甚至更多。 这样的算法可以在足够多的（数千场游戏？数百万场？）迭代之后掌握游戏吗？ 在实践中，我试图了解改进该算法的最有效方法是什么，因为我不想增加参数数量（但可以将模型本身从 cnn 更改为其他东西）    提交人    /u/Potential_Hippo1724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hfjklc/performance_of_actoronly_reinforce_algorithm/</guid>
      <pubDate>Mon, 16 Dec 2024 13:53:46 GMT</pubDate>
    </item>
    <item>
      <title>AI 学习使用虚幻引擎来平衡球！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hffktt/ai_learns_to_balance_a_ball_using_unreal_engine/</link>
      <description><![CDATA[        提交人    /u/Cyber​​Eng   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hffktt/ai_learns_to_balance_a_ball_using_unreal_engine/</guid>
      <pubDate>Mon, 16 Dec 2024 09:39:01 GMT</pubDate>
    </item>
    </channel>
</rss>