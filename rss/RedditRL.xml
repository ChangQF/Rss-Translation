<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 27 Aug 2024 12:29:32 GMT</lastBuildDate>
    <item>
      <title>如何将我自定义的凉亭世界导入健身房进行强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2clog/how_do_i_import_my_custom_gazebo_world_to_gym_for/</link>
      <description><![CDATA[我是强化学习的新手，想尝试一下我的自定义世界。但是我找不到将我的世界导入健身房的方法。我该如何将我的 Gazebo 环境导入健身房？我的 Gazebo 世界包含 urdf、world 和 launch 文件，这是带有 Gazebo 环境的 git 存储库。我使用的是带有 Gazebo 11 和 ros1 的 ubuntu 20.04。    提交人    /u/brian22lee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2clog/how_do_i_import_my_custom_gazebo_world_to_gym_for/</guid>
      <pubDate>Tue, 27 Aug 2024 09:15:17 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的反作弊系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</link>
      <description><![CDATA[我对创建基于 ML 的国际象棋反作弊系统很感兴趣。您知道任何关于使用 ML 方法进行反作弊的论文吗？我正在寻找概念。但到目前为止找不到任何类似的东西。有很多困难：例如，数据集中作弊者的例子很少，但公平玩家数据很多。在公平玩家数据上使用离线 RL 算法来预测异常行为是否有意义？    提交人    /u/HimitsuNoShougakusei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</guid>
      <pubDate>Tue, 27 Aug 2024 08:49:53 GMT</pubDate>
    </item>
    <item>
      <title>未获得官方支持的多代理 PPO 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f25yg9/multiagent_ppo_environment_without_official/</link>
      <description><![CDATA[问题陈述： 我想使用 gym 和 sb3 创建一个多智能体环境。目前，我有一个自定义的 Boid 群集环境，其中有一个神经网络，它采取所有动作并产生输出，即来自 sb3 的 PPO。 我想要实现的目标：我希望每个 boid 都有自己的演员评论家，以分散的方式执行，但 gym 默认不支持多智能体。我该如何实现呢？ 我正在使用 PPO，并希望有效地完成 MAPPO 所做的事情。集中训练，分散执行 (CTDE)。    提交人    /u/Ok-Teacher208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f25yg9/multiagent_ppo_environment_without_official/</guid>
      <pubDate>Tue, 27 Aug 2024 02:13:01 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f257ww/rl_for_manipulation/</link>
      <description><![CDATA[大家好！我正在深入研究强化/模仿学习和远程操作。我主修机械工程，在运动强化学习研究方面有经验，并且了解现代控制理论/ML/DL（我的理论理解还有待提高，但我都参加了研究生课程）。期待建议！我希望得到建议的论文、资源或开源项目，以便我可以提高我的整体理解。谢谢    提交人    /u/Sea-Hovercraft4777   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f257ww/rl_for_manipulation/</guid>
      <pubDate>Tue, 27 Aug 2024 01:36:44 GMT</pubDate>
    </item>
    <item>
      <title>“利用精选数据的自消费生成模型可证明优化人类偏好”，Ferbach 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f204co/selfconsuming_generative_models_with_curated_data/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f204co/selfconsuming_generative_models_with_curated_data/</guid>
      <pubDate>Mon, 26 Aug 2024 21:42:35 GMT</pubDate>
    </item>
    <item>
      <title>如何处理各种输出类型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1zsbd/how_to_deal_with_various_output_types/</link>
      <description><![CDATA[Sup r/Reinforcementlearning, 考虑一下深度强化学习问题：  您有一个上下文 您面前有一个经典的 10 臂老虎机 老虎机不做任何事情（0 奖励），但是当代理拉动它们时，每个老虎机都会输出一个带有浮点数、布尔输入字段（有时还有更多老虎机）的“表单”。每种形式都不同，包含 1-4 个输入。  因此，有不同的输出 - 首先是老虎机问题，然后是连续的数据输入，最好用单个步骤填充。 神经网络如何在不同的步骤中输出不同的输出格式？可以吗？ 干杯。（也许这是学位课程中教授的内容，但我是自学的。）    提交人    /u/JustZed32   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1zsbd/how_to_deal_with_various_output_types/</guid>
      <pubDate>Mon, 26 Aug 2024 21:28:14 GMT</pubDate>
    </item>
    <item>
      <title>是否有一个备份图，其中状态动作节点之后有多个分支通向相同状态但不同的奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1wt93/is_it_legit_to_have_a_backup_diagram_where_after/</link>
      <description><![CDATA[我正在阅读 sutton 和 barto 的 RL 书，当引入备份图的概念时，有一个隐含的假设，即在我们在状态“s”中执行动作“a”之后，离开 a-s 节点的所有分支都指向每个可能的下一个状态（每个状态都有一个奖励 r），但是当你可以从 s 转到 ŝ，动作为 a1，但有多个可能的奖励（假设为 r1 和 r2）时会发生什么？在这种情况下，备份图会是什么样子？ 我猜是这样的，但我不确定  s |_a1 | |_r1_ŝ | |_r2_ŝ | ... | |_r__s&#39; | |_a2     提交人    /u/samas69420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1wt93/is_it_legit_to_have_a_backup_diagram_where_after/</guid>
      <pubDate>Mon, 26 Aug 2024 19:22:26 GMT</pubDate>
    </item>
    <item>
      <title>迷失于现实</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1klw6/lost_in_rl/</link>
      <description><![CDATA[大家好， 我一直在深入研究强化学习，但最近我感觉有点不知所措，不确定自己的方向。这个领域很广阔，很难看到一条清晰的前进道路。 我开始怀疑自己的选择，担心自己的努力可能徒劳无功。我对强化学习充满热情，但不确定性正在造成影响。 还有其他人有这种感觉吗？你克服过类似的挑战吗？如果是这样，你用什么策略来保持动力和专注力？    提交人    /u/Eng-Epsilon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1klw6/lost_in_rl/</guid>
      <pubDate>Mon, 26 Aug 2024 10:11:08 GMT</pubDate>
    </item>
    <item>
      <title>过山车大亨之旅中的列车特工</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</link>
      <description><![CDATA[嗨！ 我只是想分享我的项目，我尝试创建一个代理，在 Rollercoaster Tycoon 中构建自定义轨道，并使用游戏自己的乘坐评级作为适应度函数来训练代理构建最好的过山车。 目前，它使用 UI 单击按钮并读取游戏屏幕，这非常缓慢且脆弱，但如果它显示出任何实际学习某些东西的迹象，我将尝试为 OpenRCT2 创建一个插件，公开代理可以使用的真实 API 而不是 UI。然后应该有可能在无头模式下运行游戏，并且使用 API 和同时在多个 OpenRCT2 实例上更快地构建代理。 很高兴听到您的反馈，您可以在这里找到代码：https://github.com/ZerxXxes/openrct2_gym    提交人    /u/ZerxXxes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</guid>
      <pubDate>Mon, 26 Aug 2024 08:00:32 GMT</pubDate>
    </item>
    <item>
      <title>RL 竞赛/项目？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f18x61/rl_competitionsprojects/</link>
      <description><![CDATA[嗨，我打算攻读 RL 方向的博士学位，想知道什么样的经验会有所帮助。最近有什么推荐的比赛或项目吗？谢谢。     提交人    /u/Birdy6Liu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f18x61/rl_competitionsprojects/</guid>
      <pubDate>Sun, 25 Aug 2024 22:36:14 GMT</pubDate>
    </item>
    <item>
      <title>解决 2048 是不可能的</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f11gtg/solving_2048_is_impossible/</link>
      <description><![CDATA[所以我最近参加了一个强化学习课程，并决定通过解决 2048 游戏来测试我的知识。乍一看，这个游戏似乎很容易，但出于某种原因，对于代理来说相当困难。我尝试了不同的东西：改进后的 DQN，如 double-dqn、各种奖励和惩罚，现在是 PPO。但都不起作用。我能得到的最好结果是 512 个方块，这是我通过优化以下奖励获得的：任何合并 +1，无合并 0，无用移动 -1，游戏结束。我将棋盘编码为 (16,4,4) 独热张量，其中每个状态 [:, i, j] 代表 2 的幂。我尝试了各种架构：FC、CNN、Transformer 编码器。CNN 对我来说效果更好，但还远远不够好。 有人玩过这个游戏吗？也许有些提示？令我震惊的是，用于非常复杂环境（如 dota 2、星际争霸等）的 RL 算法无法学会玩这个简单的游戏。    提交人    /u/Hopeful_Ad9591   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f11gtg/solving_2048_is_impossible/</guid>
      <pubDate>Sun, 25 Aug 2024 17:14:22 GMT</pubDate>
    </item>
    <item>
      <title>我的扫雷代理 (q-learning) 存在问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0xu49/problems_with_my_minesweeper_agent_qlearning/</link>
      <description><![CDATA[您好，我目前正在制作一个代理，用于解决扫雷游戏的问题。我是编程新手，在代理训练方面遇到了一些问题。我不知道为什么，但代理就是没有进步，所以也许我遗漏了代码的一些关键部分。它使用 q-learning 和奖励来改进。非常感谢您的帮助！我为此使用了 pycharm（不知道是否相关）。 https://we.tl/t-hbAmcIZjAn    提交人    /u/justindepro1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0xu49/problems_with_my_minesweeper_agent_qlearning/</guid>
      <pubDate>Sun, 25 Aug 2024 14:39:05 GMT</pubDate>
    </item>
    <item>
      <title>使用单个模拟服务器时运行矢量化环境的好方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0w5fe/what_is_a_good_way_to_run_vectorized_environments/</link>
      <description><![CDATA[我的自定义环境只是一个 Python 类。实际的环境模拟发生在 DLL 中。初始化需要一段时间，但随后 DLL 可以通过允许环境对 DLL 进行 C 函数调用、为该调用启动模拟，并且每个调用在内部被放置在单独的线程中，从而运行许多并行模拟。 我正尝试使用系统上的所有 CPU 来训练 SB3。 我理解 DummyVecEnv 将按顺序运行 n 个环境，因此这不会并行运行我的模拟。 SubprocVecEnv 可以并行运行环境，但据我所知，这将创建多个进程，而不是重新使用已经初始化的模拟世界。每个进程都会创建一个模拟世界的实例，我想避免这种情况，因为它相对较慢且占用大量内存。 我在这里有什么选择？    提交人    /u/RamenKomplex   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0w5fe/what_is_a_good_way_to_run_vectorized_environments/</guid>
      <pubDate>Sun, 25 Aug 2024 13:19:13 GMT</pubDate>
    </item>
    <item>
      <title>探索 Lode Runner 作为强化学习研究的基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0s960/exploring_lode_runner_as_a_benchmark_for/</link>
      <description><![CDATA[大家好， 我正在探索使用游戏 Lode Runner 作为强化学习 (RL) 的基准的想法。该游戏具有一些独特的机制，例如挖掘和攀爬，这可能会给 RL 算法带来有趣的挑战。 这是我正在考虑的：  单智能体 RL：测试智能体如何在受控环境中执行收集物品和避开敌人等任务。 多智能体 RL：实现多个智能体交互的场景，无论是一起工作还是相互竞争。一个例子是如果敌人也是基于 RL；也许可以尝试实验多个代理如何独立或合作地共同捕捉单个移动目标。 分层 RL：研究代理如何在游戏机制中学习高级策略和低级动作  我很想听听你的想法：  这是个好主意吗？Lode Runner 是否适合 RL 研究，或者有更好的选择？ 可能会出现哪些挑战？关于将经典游戏用于 RL 有什么建议吗？ 研究价值：你认为 Lode Runner 的独特之处会带来有趣的发现吗？ 与 星际争霸 的比较：使用 Lode Runner 与使用星际争霸 用于 RL 研究？Lode Runner 会相形见绌吗？还是它提供了独特的研究机会？  这是描述该游戏的维基百科页面的链接。 https://en.wikipedia.org/wiki/Lode_Runner 这是一个在线网站，您可以在其中尝试玩它。 https://www.retrogames.cz/play_079-NES.php 感谢您的任何见解。    提交人    /u/Abominable_Liar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0s960/exploring_lode_runner_as_a_benchmark_for/</guid>
      <pubDate>Sun, 25 Aug 2024 09:11:31 GMT</pubDate>
    </item>
    <item>
      <title>我尝试制作深度强化学习股票交易机器人（失败了）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f0lbng/my_failed_attempt_at_making_a_deep_reinforcement/</link>
      <description><![CDATA[        由    /u/NextgenAITrading 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f0lbng/my_failed_attempt_at_making_a_deep_reinforcement/</guid>
      <pubDate>Sun, 25 Aug 2024 01:45:33 GMT</pubDate>
    </item>
    </channel>
</rss>