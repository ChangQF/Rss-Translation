<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 28 May 2024 15:14:17 GMT</lastBuildDate>
    <item>
      <title>用于 RL 和 Pytorch 的库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2hhqn/library_to_use_for_rl_and_pytorch/</link>
      <description><![CDATA[嘿大家，我过去做了很多理论强化学习，并从头到尾实现了我的大部分算法（PPO、DQl 等）理解它们并稍微调整它们。然而，现在我需要 RL 来解决一个实际问题，我不想花太多时间来实现每个算法。您建议使用哪些库来与 Pytorch 实现良好的协同作用？ 就我个人而言，我一直对所有 Pytorch 库和子库都有丰富的经验，这就是为什么我认为 TorchRL 在这里可能有用。然而，由于它还处于起步阶段，资源有点稀缺。所以我不太确定其他拥有更大社区的库是否更适合这里。   由   提交/u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2hhqn/library_to_use_for_rl_and_pytorch/</guid>
      <pubDate>Tue, 28 May 2024 11:46:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 学习图的色数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2fuzp/learning_chromatic_number_of_graphs_with_ppo/</link>
      <description><![CDATA[我正在使用 PPO+GNN 解决色数问题，其中代理必须学习用最少的颜色为图形着色。作为输入给出的颜色数量始终大于色数。假设图形的色数为 3，我们为代理提供了 10 种颜色，并希望它用 3 种颜色为图形着色。这就是正在发生的事情，它在很短的几步内学会了用 5 种颜色为图形着色，但它并没有从那里收敛到最佳解决方案。我该怎么做才能让它收敛到最佳解决方案？附注：最初参与者损失很高，一旦它收敛到次优解（5），它就会变得非常少。此外，该算法为 MIS 问题提供了最佳解决方案和良好的收敛性。    提交人    /u/Low-Advertising-1892   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2fuzp/learning_chromatic_number_of_graphs_with_ppo/</guid>
      <pubDate>Tue, 28 May 2024 10:04:14 GMT</pubDate>
    </item>
    <item>
      <title>帮助 SB3 包装器和日志记录</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2dzm1/help_with_sb3_wrappers_and_logging/</link>
      <description><![CDATA[嗨， 我收到此警告： UserWarning: WARN: env.Instance_attribute to get来自其他包装器的变量已被弃用，并将在 v1.0 中删除，要获取此变量，您可以对环境变量执行 env.unwrapped.Instance_attribute 或执行 env.get_wrapper_attr(&#39;Instance_attribute&#39;) 来搜索提醒包装器。  class CustomCallback(BaseCallback): def __init__(self, verbose: int = 0): super().__init__(verbose) def _on_step(self) -&gt; bool: Instance_attribute_1 = sum(self.training_env.get_attr(&quot;Instance_attribute&quot;)[0]) self.logger.record(&quot;Instance_attribute 1&quot;, Instance_attribute_1) return True  环境正在被 `Monitor` 包装器和 DummyVecEnv 包装。 有人知道如何摆脱这个警告吗？   由   提交/u/k0ldsoul  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2dzm1/help_with_sb3_wrappers_and_logging/</guid>
      <pubDate>Tue, 28 May 2024 07:47:15 GMT</pubDate>
    </item>
    <item>
      <title>通过克罗内克积证明价值函数的梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2brn9/proof_of_gradient_of_value_function_via_kronecker/</link>
      <description><![CDATA[您好，我对赵世宇强化学习数学基础中发现的证明有疑问。  我将其发布在 stackexchange  因为我认为格式化会更容易。   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2brn9/proof_of_gradient_of_value_function_via_kronecker/</guid>
      <pubDate>Tue, 28 May 2024 05:13:50 GMT</pubDate>
    </item>
    <item>
      <title>训练 DQN 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2bn3k/training_dqn_agent/</link>
      <description><![CDATA[您好，我正在定制环境中工作，为此我没有得到训练 DQN 代理的内容。 任何教程或线索拜托？ 谢谢   由   提交/u/ComplianceOil566   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2bn3k/training_dqn_agent/</guid>
      <pubDate>Tue, 28 May 2024 05:05:57 GMT</pubDate>
    </item>
    <item>
      <title>v2 和 v4 MuJoCO 环境 D4RL 之间的差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d278ah/difference_between_v2_and_v4_mujoco_environments/</link>
      <description><![CDATA[我想知道 D4RL 基准测试中提供的离线 RL MuJoCo-v2 数据集是否与 v3 和 v4 版本兼容。如果我最终想用这些数据集 (D4RL mujoco v2) 训练模型，然后使用相应的 v3 或 v4 版本在线测试模型，那么各个版本的奖励和环境动态是否仍然相同？谢谢    提交人    /u/Particular_Rip_6148   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d278ah/difference_between_v2_and_v4_mujoco_environments/</guid>
      <pubDate>Tue, 28 May 2024 00:58:42 GMT</pubDate>
    </item>
    <item>
      <title>将 GNN 集成到 RL 模型中</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d24iht/integrating_gnns_in_rl_models/</link>
      <description><![CDATA[大家好， 我目前正在撰写关于优化动态变化的服务器网络的学士论文。为了对网络结构进行建模，我使用了一个图表，这使得 GNN 的使用在这里非常诱人。虽然我在 GNN 和 RL 方面拥有扎实的背景（主要是理论，大多数算法都是从头开始编写的），但我从未将两者结合起来。所以我想知道您是否有将 GNN 与稳定基线等常见 RL 库结合使用进行 RL 的经验？   由   提交/u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d24iht/integrating_gnns_in_rl_models/</guid>
      <pubDate>Mon, 27 May 2024 22:44:55 GMT</pubDate>
    </item>
    <item>
      <title>我的深度 Q 学习正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d23cek/am_i_doing_deep_qlearning_right/</link>
      <description><![CDATA[我正在尝试通过深度 q-learning 解决 OpenAI Gym cartpole 问题，我只是从头开始写了一些东西，但我不知道如果它做正确的事。有更了解 RL 的人来帮忙吗？这就是我正在编写的脚本，它是一个“双重深度 q-learning”：  https://github.com/rlucasfm/Q-learning-tf/blob/master/double_dqn_tf.py。 同一存储库上有一个用于简单深度 Q 学习的脚本。&lt; /p&gt;   由   提交 /u/WitnessedWrath   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d23cek/am_i_doing_deep_qlearning_right/</guid>
      <pubDate>Mon, 27 May 2024 21:51:30 GMT</pubDate>
    </item>
    <item>
      <title>可变大小的状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1w9rn/variablesized_states/</link>
      <description><![CDATA[在我的项目中，我试图了解应用程序 UI 的状态。在执行任何给定操作后，应用程序都会向我提供有关 UI 中发生的一个或多个更改的数据（组件更改、添加/删除了哪些新组件等），我从中构建状态。 问题在于 - 对话框和组件的数量和身份在会话之间可能有很大差异，并且不知道可能会添加哪些组件。 有什么好办法可以解决这个问题？零填充并不实用，因为组件数量可能非常大，而且我们已经为每个组件提取了大量数据。此外，这意味着对应用程序的任何新更改都需要从头开始重新训练。    提交人    /u/CJIsABusta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1w9rn/variablesized_states/</guid>
      <pubDate>Mon, 27 May 2024 16:59:58 GMT</pubDate>
    </item>
    <item>
      <title>我可以用 4 A30 进行对齐吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1njta/can_i_do_alignment_with_4_a30/</link>
      <description><![CDATA[实验室的主要焦点是 RL，但我发现 rlhf 对我更有吸引力。 好吧，我手上有 4xA30 24G，是可以对 6B-8B LLMalignmnet 进行研究吗？ 任何回复都会有帮助。   由   提交/u/siyuan01  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1njta/can_i_do_alignment_with_4_a30/</guid>
      <pubDate>Mon, 27 May 2024 09:21:49 GMT</pubDate>
    </item>
    <item>
      <title>如何确定重播缓冲区的最小大小</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</link>
      <description><![CDATA[我正在统一训练一个 dqn，它使用 [偏航、横向向量和前向向量] 作为我的动作空间来控制游戏对象。我读到人们在开始训练之前获得了重放缓冲区的最低经验数（在某些帖子中，读到该最小值高达 10000）。我发现我的模型基本上对每个推理都给出了相同的动作，因为它现在完全未经训练。在我看来，在开始训练之前收集大量此类数据基本上是多余的，因为它并没有真正尝试各种操作。如果我的批量大小为 64，如果我想使用包含（假设为 256）个经验的重播缓冲区开始训练，有什么需要注意的吗？我假设这将帮助我在早期增加训练数据的多样性。 我是一个菜鸟，所以我非常愿意参考可能与此相关的论文，提前谢谢。  p&gt;   由   提交 /u/ProtectionFrosty5393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</guid>
      <pubDate>Mon, 27 May 2024 04:55:29 GMT</pubDate>
    </item>
    <item>
      <title>我可以在电子商务网站中使用强化学习进行产品推荐吗？有没有关于这个主题的资源？或者有哪些其他最佳选择。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</link>
      <description><![CDATA[嗨，我是一家餐饮/电子商务服务提供商的实习生，我被指派为 The F&amp;B/E-commerce 服务提供商创建推荐引擎。 B和电子商务网站，建议我一些关于基于RL的推荐系统的好的指导材料或任何其他效果好的方法，注意：数据有限哪个可能是最好的？   由   提交/u/Educational-Town-710   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</guid>
      <pubDate>Mon, 27 May 2024 02:53:59 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关通过电池和电网交互构建能源管理 RL 环境的建议和见解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</link>
      <description><![CDATA[大家好！ 我正在参与一个项目，重点是使用强化学习来模拟和优化能源管理系统。目标是管理电网和可再生能源的能源存储和消耗，并密切关注实时定价和能源需求。 这种设置代表了我在给定情况下提出的最佳方法。数据集和项目的紧急情况。（我愿意接受任何想法） 系统概述： 状态空间包括：  两个电池的充电状态 (SoC)。 当前电网电价。 能源使用的历史和预测数据。 &lt; li&gt;未来 6 小时的可再生能源发电预测。 一天中的时间和一周中的日期指示器。  行动空间涉及：  确定每个电池充电的电量。 （2 个连续值） 安排充电操作的开始时间（选项范围从立即到延迟 6 小时）。  数据和迭代： 我已经转换了我的数据集，以表示每小时的所有相关信息，例如负载、价格和可再生能源预测。环境中的每个模拟步骤对应一小时的实时时间，其中模型根据当前状态预测开始充电的最佳时间以及充电量。 主要挑战：&lt; /strong&gt; 延迟操作： 例如，如果 2024 年 1 月 1 日 00:00:00 决定在 5 小时后开始充电，我应该如何：  考虑操作对系统的延迟影响来计算奖励？ 更新模型的预测： 针对几小时后影响系统的操作，确定最佳方案模型重新评估和做出新预测的时间令人困惑。 学习的数据迭代： 假设环境的每个步骤处理来自转换后的数据集的一小时的数据：&lt; /p&gt; 如何确保 RL 模型有效地迭代数据集以实现最大程度的学习？ 建议采用哪些策略来处理这种每小时数据的连续流，特别是在集成操作的延迟效果时模型的学习过程？ 我正在寻找有关在强化学习环境中管理延迟操作和有效数据迭代的见解、建议或资源。 感谢您的指导和时间!   由   提交 /u/Nnarruqt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</guid>
      <pubDate>Sun, 26 May 2024 13:20:52 GMT</pubDate>
    </item>
    <item>
      <title>学士论文游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</link>
      <description><![CDATA[嘿，我想为我的计算机科学学士学位论文训练一个人工智能来玩强化学习游戏。 我还没有强化学习的经验。 我可以选择哪些当时可行的游戏？   由   提交/u/TMG_Indi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</guid>
      <pubDate>Sun, 26 May 2024 08:40:15 GMT</pubDate>
    </item>
    <item>
      <title>环境复杂性与最优策略收敛的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</link>
      <description><![CDATA[嘿伙计们，有没有关于环境复杂性与学习到的最优策略本身之间关系的文献？例如，如果一个环境是由“世界模型”中的 VAE 生成的，那么环境复杂性和策略之间的关系是什么？    提交人    /u/Main_Pressure271   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</guid>
      <pubDate>Sun, 26 May 2024 06:58:00 GMT</pubDate>
    </item>
    </channel>
</rss>