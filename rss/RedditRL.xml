<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 01 May 2024 15:13:43 GMT</lastBuildDate>
    <item>
      <title>模型外部的不确定性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chobrx/uncertainty_outside_of_model/</link>
      <description><![CDATA[对于无法进行完整建模的环境，假设动作 a_k 导致状态 S_k，而该状态超出了我的模型（这也意味着马尔可夫属性不再成立）。 我应该怎么做才能使 a_k+1 和 a_k+2 继续在控制范围内？ 递归贝叶斯估计可以用于“更新”策略吗？    提交人    /u/Alive-Opportunity-23   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chobrx/uncertainty_outside_of_model/</guid>
      <pubDate>Wed, 01 May 2024 15:01:13 GMT</pubDate>
    </item>
    <item>
      <title>无需从头开始训练，如何应对新的动作维度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cho457/how_to_deal_with_new_action_dimensions_without/</link>
      <description><![CDATA[例如，RL 代理首先基于具有 N 维的动作空间进行训练。现在，有 2 个新的动作维度，使动作空间变为 N+2 维。是否有任何可能的方法来在 N+2 维动作空间上训练预先训练过的代理，而无需从头开始重新训练代理？    提交人    /u/YuDerrickZ   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cho457/how_to_deal_with_new_action_dimensions_without/</guid>
      <pubDate>Wed, 01 May 2024 14:52:19 GMT</pubDate>
    </item>
    <item>
      <title>tf.js 用于强化学习的稳定性如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chjfal/how_stable_is_tfjs_for_doing_reinforcement/</link>
      <description><![CDATA[嘿，我想在浏览器中做一些事情，而不是在 python 中。 python 的主要挑战是我必须设置一些东西来演示输出或视频记录它，如果我可以在 js 中做到这一点，那么我可以进行现场网络演示...这就是我在 js 中做到这一点的目的，我也擅长三个 js 和其他可视化 js 库，如果你有另一种方法在 python 中做到这一点，请告诉我。 来到主要问题，我看到在 tf.js 中我们可以做backpropgration using  // 执行梯度下降 optimizer.minimize(() =&gt; loss);我们可以吗定义一个神经网络并在 js 中进行反向传播等操作，现在够好了吗？ ​   由   提交/u/yellowsprinklee  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chjfal/how_stable_is_tfjs_for_doing_reinforcement/</guid>
      <pubDate>Wed, 01 May 2024 11:06:14 GMT</pubDate>
    </item>
    <item>
      <title>dm_control 的替代方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chhx6w/alternatives_to_dm_control/</link>
      <description><![CDATA[嗨 我知道 dm_control 在很多研究工作中使用，我也想使用它。事实证明，它没有很好的记录，很难导航，而且最糟糕的是，维护者没有正确回答问题，有时甚至完全忽略这些问题。这让我很愤怒，但我无能为力，我不会因此责怪开发人员，他们可能将时间投入到其他一些作品上，并且在任何情况下都没有义务回答我们。  ​ 话虽这么说，我真的很希望看到该领域开发出一些替代方案，以便减少闯入该领域的人并做出更多贡献。  ​ 您是否知道一些正在朝这个方向发展的作品？  &amp; #32；由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chhx6w/alternatives_to_dm_control/</guid>
      <pubDate>Wed, 01 May 2024 09:32:07 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习的四轴飞行器控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ch4h1d/quadcopter_control_with_reinforcement_learning/</link>
      <description><![CDATA[大家好，我是一名学生，正在研究一个使用深度强化学习的四轴飞行器控制器项目。不幸的是，我的项目演示在下周，我的电脑缺乏从头开始构建它的能力。你知道我可以参考的任何现有来源或项目吗？    提交人    /u/taha_sbh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ch4h1d/quadcopter_control_with_reinforcement_learning/</guid>
      <pubDate>Tue, 30 Apr 2024 21:32:30 GMT</pubDate>
    </item>
    <item>
      <title>具有不确定性的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgx7uz/environments_with_uncertainty/</link>
      <description><![CDATA[有人知道有哪些环境可能表现出一些不确定性吗？例如，让我们想象环境中的一个部分，如果代理进入其中，不确定性为： ​  采取所需操作的概率会降低并且更加随机 奖励在该区域中变得随机 还是其他东西？  ​ 我想要它，这样我就可以研究一些预先存在的不确定性强化学习技术。最好环境与健身房兼容，我不介意离散或连续，对两者都很满意:) ​ 提前致谢！ &lt; /div&gt;  由   提交/u/Glum_Significance140   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgx7uz/environments_with_uncertainty/</guid>
      <pubDate>Tue, 30 Apr 2024 16:33:08 GMT</pubDate>
    </item>
    <item>
      <title>DQN - 调度状态表示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgvx6k/dqn_scheduling_state_representation/</link>
      <description><![CDATA[大家好， 我对 DQN 和强化学习比较陌生，我想针对调度问题实现 DQN 模型在Python中使用keras。在尝试自定义环境之前，我做了一些教程，例如健身车杆，并观看了一些教程，但我似乎在努力解决 DQN 的状态表示（我认为）。 问题陈述： 假设我们有 X 名工人和 Y 份工作。目标是根据以下因素将工作分配给最合适的工人：  工作所需的技能以及工人获得的技能 工作的持续时间工作和工人的剩余可用时间。  其中可用性和持续时间是连续整数，技能是二进制值（如果不存在技能则为 0，如果存在则为 1）。 如果如果不满足这些条件，那么理想情况下，不应将工人分配到该工作 这是我设计状态的方式 - 我从所有工人和只有一项工作开始（我们只是假设工作依次分配）： [[AvailabilityWorker1, Skill1Worker1, Skill2Worker1, Skill3Worker1], [AvailabilityWorker2, Skill1Worker2, Skill2Worker2, Skill3Worker2],  ... , [JobDuration, Skill1Job, Skill2Job, Skill3Job]] 操作是选择其中一个工作人员，或者根本不选择工作人员（因此 no_workers + 1） 每执行一步后都会更新状态，以将当前工作替换为下一个工作已安排。此外，所选工作人员的可用性也会更新。 我似乎在与状态代表作斗争，或者与其他阻止我的代理人理解问题的事情作斗争。我尝试了许多不同的状态表示和模型参数，但训练后的模型始终不是一个好的解决方案。 我很高兴收到任何建议！ 提前致谢！   由   提交/u/-Aryiox-  /u/-Aryiox-  reddit.com/r/reinforcementlearning/comments/1cgvx6k/dqn_scheduling_state_representation/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgvx6k/dqn_scheduling_state_representation/</guid>
      <pubDate>Tue, 30 Apr 2024 15:39:29 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何资源可以研究如何对样本进行加权（例如，按情节）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgutqp/is_there_any_resources_which_look_at_how_to/</link>
      <description><![CDATA[我无法将支持和反对各种样本加权策略的论据形式化。首先我想说，一个情节有多少个样本与该情节对于我的特定问题的重要性无关，也就是说，只有奖励才能影响错误对损失的影响。话虽这么说，我面临着一个两难的境地，我希望有人能帮助我思考。可能值得注意的是，这也是针对我的问题的，单个情节==单个奖励，并且我不想包括时间范围。例如，一个情节中的每个样本都将具有相同的奖励，因为我想确保优化考虑一个情节中的所有样本对奖励的同等贡献。 一方面，我觉得按情节对样本进行加权，确保我们平等地观看每一集。如果一个剧集没有很多样本但奖励较高，而相比于一个剧集有很多样本但奖励较小，我们仍然会关注更重要的剧集。 如果我们对样本进行统一加权，我们会考虑每个样本独立。这样做的好处是，对于大的episode，我们仍然关心个体样本的预测。然而，通过按情节加权，对于较大的情节，我们不会太关心单个样本的误差。我认为这在某些方面是有道理的，但在其他方面可能没有道理。 我已经尽可能考虑使用 1 / 剧集长度的平方根作为样本权重，以本质上找到两者之间的中间立场两者（因为统一权重是 1/所有样本，剧集权重是 1/len(episode)）。虽然无法将论点形式化，但很难知道这个决定是否有意义。 有什么想法吗？    ;由   提交/u/Yogi_DMT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgutqp/is_there_any_resources_which_look_at_how_to/</guid>
      <pubDate>Tue, 30 Apr 2024 14:54:00 GMT</pubDate>
    </item>
    <item>
      <title>ML-Agent 和 Unity</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgs5xa/mlagent_and_unity/</link>
      <description><![CDATA[这个问题对于这个 subreddit 来说可能非常基本，但我在其他地方找不到答案。我刚刚开始学习如何在 Unity 中使用 ML-Agents 训练神经网络。我从一个相当基本的问题开始，即我有一个智能体和一个目标，我正在训练智能体实现目标，并在它与目标发生冲突时给予奖励。很简单，我知道。然而，我想对此进行扩展并训练神经网络尽可能快地做到这一点。但是，我不确定如何确定奖励模型。显然，简单的答案是“完成目标所需的时间越短，奖励就越高”，但我似乎不知道如何将其映射到我的 C# 脚本。有什么建议，无论是针对 ML-Agents 和 Unity 还是一般性的？   由   提交/u/Tasty_Personality537   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgs5xa/mlagent_and_unity/</guid>
      <pubDate>Tue, 30 Apr 2024 12:55:53 GMT</pubDate>
    </item>
    <item>
      <title>策略梯度方法：学习随机策略的最优方差？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgrz9g/policy_gradient_methods_learning_optimal_variance/</link>
      <description><![CDATA[我有一个具有连续行动空间的任务，其中最优策略是随机的（由于信息不完美）。我将使用典型的策略梯度算法（例如 PPO），我的策略将是一个输出动作分布均值和方差的神经网络。  通常的做法是从高方差开始，并且在训练过程中方差往往会减小到接近零。但是，如果最优策略的方差是一个有限的、相当大的值怎么办？这种方法仍然有效吗？如果方差太低，方差会回升吗？或者还有另一种优化方差值的方法吗？   由   提交 /u/redditDRL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgrz9g/policy_gradient_methods_learning_optimal_variance/</guid>
      <pubDate>Tue, 30 Apr 2024 12:46:27 GMT</pubDate>
    </item>
    <item>
      <title>用于库存优化的多代理强化学习的可扩展性如何？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgqtjs/how_scalable_is_multi_agent_rl_for_inventory/</link>
      <description><![CDATA[我是一名数据科学家/机器学习工程师，在强化学习方面还是个新手。我最近才开始研究它——还没有动手。  无论如何，我的团队提出了以仓库库存管理为中心的用例，其中包括 1000 种独特的产品。我想我们也许可以为此应用 MARL（例如 https://github.com/microsoft/maro）尽量减少超龄/未成年。但首先我想知道 MARL 的可扩展性如何？ 如果我理解正确的话...代理的操作是分散的（因此可并行），但是策略的调整是集中的，因此不可并行(?)  是否可以在合理的时间内训练 10k 个代理（每个独特产品 1 个），或者会花费太长时间吗？ / 成本太高？   由   提交 /u/WhyDo TheyAlwaysWin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgqtjs/how_scalable_is_multi_agent_rl_for_inventory/</guid>
      <pubDate>Tue, 30 Apr 2024 11:46:44 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习目前的最新技术是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgobyl/what_is_the_current_state_of_the_art_in_multi/</link>
      <description><![CDATA[自 2019 年以来我就没怎么关注过 MARL，我很好奇从那以后发生了什么。论文链接将不胜感激！    由   提交/u/geargi_steed  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgobyl/what_is_the_current_state_of_the_art_in_multi/</guid>
      <pubDate>Tue, 30 Apr 2024 09:07:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么DPO的损失函数中有“reference free”选项？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cglgra/why_is_there_a_reference_free_option_in_dpos_loss/</link>
      <description><![CDATA[有一个 trl 的 DPO 损失函数实现中的 #L762&quot;&gt;reference_free 参数，而 原始DPO论文没有提到“reference free”的概念。 在trl的实现中： &lt; code&gt; pi_logratios =policy_chosen_logps -policy_rejected_logps 如果reference_free：ref_logratios = 0 else：ref_logratios =reference_chosen_logps -reference_rejected_logps logits = pi_logratios - ref_logratios  当reference_free为True时，损失计算为 policy_chosen_logps 和 policy_rejected_logps 之间的交叉熵，而 reference_chosen_logps 和 reference_rejected_logps 为未使用。 “无参考”的优点和缺点是什么？ DPO？   由   提交/u/yang_bo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cglgra/why_is_there_a_reference_free_option_in_dpos_loss/</guid>
      <pubDate>Tue, 30 Apr 2024 05:50:41 GMT</pubDate>
    </item>
    <item>
      <title>写我的第一篇 RL 相关论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgilu1/writing_my_first_rl_related_paper/</link>
      <description><![CDATA[我目前即将在一家私营公司完成实习，我一直在该公司进行专门利用 RL 的优化研究项目（就上下文而言，我目前正在在我本科学位的最后一年，学习计算机科学/物理学）。我的经理曾是澳大利亚大学的数学学者，他鼓励我发表一篇关于我的发现的论文，因为他和其他一些同事相信我的结果是值得论文发表的。有人对撰写一般论文有任何建议或我可以参考的资源吗？我在攻读学位期间写了大量报告（物理学和计算机科学），并阅读了许多论文，但从未撰写过正式出版物，尤其是利用强化学习的出版物。仅供参考，我使用的 RL 实际上一点也不复杂（只不过是一个非常简单的 DQN 实现）。任何建议将不胜感激！ :)    由   提交 /u/AlternativePool6088   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgilu1/writing_my_first_rl_related_paper/</guid>
      <pubDate>Tue, 30 Apr 2024 03:10:10 GMT</pubDate>
    </item>
    <item>
      <title>“对齐算法的机械理解：DPO 和毒性案例研究”，Lee 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgeqkv/a_mechanistic_understanding_of_alignment/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgeqkv/a_mechanistic_understanding_of_alignment/</guid>
      <pubDate>Tue, 30 Apr 2024 00:05:01 GMT</pubDate>
    </item>
    </channel>
</rss>