<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 02 May 2024 06:19:14 GMT</lastBuildDate>
    <item>
      <title>如何将健身裹布与 SB3 结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/</link>
      <description><![CDATA[我正在尝试结合健身房包装器“TransformReward&quot;与SB3。这是我的代码 -  from stable_baselines3 import DQN from stable_baselines3.common.env_util import make_vec_env fromgym.wrappers import TransformReward defreward_transform(reward): # 奖励转换函数示例 returnreward * 0 # Scale将奖励加0以检查函数是否正常工作 # 创建向量化环境 env = make_vec_env(&quot;CartPole-v1&quot;, n_envs=1,wrapper_class=TransformReward,wrapper_kwargs={&quot;f&quot;:reward_transform}) # 创建并训练DQN 模型 model = DQN(&quot;MlpPolicy&quot;, env, verbose=1) model.learn(total_timesteps=100000, log_interval=4) model.save(&quot;dqn_cartpole&quot;)  As为了确保我的代码正常工作，我尝试将奖励缩放为 0。 但是，我看到代理正在学习并获得奖励 -  ``` &lt;前&gt;&lt;代码&gt;-------------------------------- |推出/ | | | ep_len_mean | 83.6 | 83.6 | ep_rew_mean | 83.6 | 83.6 |探索率 | 0.05 | 0.05 |时间/| | |剧集 | 3548 | 3548 |帧率 | 3401 | 3401 |已用时间 | 29 | 29 |总时间步数 | 99751 | 99751 |火车/ | | |学习率 | 0.0001 | |损失| 2.29e-06 | | n_更新 | 12437 | 12437 ----------------------------------  ``` 有人可以告诉我的代码有什么问题吗？谢谢！   由   提交/u/Academic-Rent7800  /u/Academic-Rent7800  reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/</guid>
      <pubDate>Wed, 01 May 2024 21:34:05 GMT</pubDate>
    </item>
    <item>
      <title>在我的电脑中设置airSim项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chvlkj/set_up_airsim_project_in_my_computer/</link>
      <description><![CDATA[大家好，我需要一些帮助。我一直在尝试在我的计算机上设置这个项目（它是一个准备使用的项目）并遵循提到的所有步骤，但我似乎无法管理它。当我到达“运行first.py以控制无人机”的步骤时，我找不到名为“first.py”的文件。我不确定这里缺少什么。如果已经尝试过的人可以帮助我，那将非常有帮助。 https://github。 com/lap98/RL-Drone-Stabilization   由   提交/u/gintooki_23  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chvlkj/set_up_airsim_project_in_my_computer/</guid>
      <pubDate>Wed, 01 May 2024 19:57:59 GMT</pubDate>
    </item>
    <item>
      <title>如何运用和运用模仿学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chr4lq/how_to_apply_and_use_imitation_learning/</link>
      <description><![CDATA[嗨 我想实现一个人形机器人执行某些动作的模拟。我计划使用 cmu mocap 数据集用于训练目的，并使用 dm_control 给出的 cmu_ humanoid 。 我应该使用什么以及我应该如何处理已经写了一些论文，它们似乎遵循编码器解码器计划来实现这一目标。然而，他们不提供代码实现，我对如何实现这一点感到非常困惑。 这是我引用的论文用于人形控制的神经概率运动原语。 也许我应该参考他们之前使用编码器-解码器来使用此类东西的代码（不需要跟踪 cmu使用人形机器人进行运动），但我似乎找不到太多代码。 您能否向我介绍一些代码实现。 谢谢    由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chr4lq/how_to_apply_and_use_imitation_learning/</guid>
      <pubDate>Wed, 01 May 2024 16:57:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么这么多论文重新解释强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chq0w2/why_is_rl_reexplained_in_so_many_papers/</link>
      <description><![CDATA[我遇到的许多论文都花了第二章的整个开头重新解释 RL 的基础知识（状态/动作值函数、贝尔曼方程、q -学习）就好像这是最近的一些发现，没有人听说过，同时又如此快速地深入到如此多的细节，如果你不熟悉这些概念，你最好在其他地方学习它。 其他领域不会这样做，你不会看到每一篇物理论文都重新解释广义相对论是如何工作的。假设您正在阅读该论文，那么您对该领域有些熟悉。 我很好奇，强化学习中是否存在这种情况的原因？我唯一能想到的是，如果每个人对强化学习的理解略有不同，他们会希望读者在同一页面上，但每次总是几乎相同。   由   提交/u/giorgiocav123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chq0w2/why_is_rl_reexplained_in_so_many_papers/</guid>
      <pubDate>Wed, 01 May 2024 16:11:44 GMT</pubDate>
    </item>
    <item>
      <title>模型外部的不确定性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chobrx/uncertainty_outside_of_model/</link>
      <description><![CDATA[对于不可能进行完整建模的环境，假设操作 a_k 导致状态 S_k 位于我的模型之外（这也意味着马尔可夫属性不再成立）。 我应该怎么做才能使 a_k+1 和 a_k+2 继续在控制范围内？ 递归贝叶斯估计可以用于“更新”策略吗？    由   提交 /u/Alive-Opportunity-23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chobrx/uncertainty_outside_of_model/</guid>
      <pubDate>Wed, 01 May 2024 15:01:13 GMT</pubDate>
    </item>
    <item>
      <title>无需从头开始训练，如何应对新的动作维度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cho457/how_to_deal_with_new_action_dimensions_without/</link>
      <description><![CDATA[例如，RL 代理首先基于 N 维的动作空间进行训练。现在，有 2 个新的动作维度，使动作空间成为 N+2 维。是否有任何可能的方法可以在 N+2 维动作空间上训练预训练的智能体，而无需从头开始重新训练智能体？  （假设无法知道是否会有新的动作维度/在发生之前会有多少个新的动作维度。因此不可能将动作空间设置为 N+2 维）   由   提交 /u/YuDerrickZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cho457/how_to_deal_with_new_action_dimensions_without/</guid>
      <pubDate>Wed, 01 May 2024 14:52:19 GMT</pubDate>
    </item>
    <item>
      <title>tf.js 用于强化学习的稳定性如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chjfal/how_stable_is_tfjs_for_doing_reinforcement/</link>
      <description><![CDATA[嘿，我想在浏览器中做一些事情，而不是在 python 中。 python 的主要挑战是我必须设置一些东西来演示输出或视频记录它，如果我可以在 js 中做到这一点，那么我可以进行现场网络演示...这就是我在 js 中做到这一点的目的，我也擅长三个 js 和其他可视化 js 库，如果你有另一种方法在 python 中做到这一点，请告诉我。 来到主要问题，我看到在 tf.js 中我们可以做backpropgration using  // 执行梯度下降 optimizer.minimize(() =&gt; loss);我们可以吗定义一个神经网络并在 js 中进行反向传播等操作，现在够好了吗？ ​   由   提交/u/yellowsprinklee  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chjfal/how_stable_is_tfjs_for_doing_reinforcement/</guid>
      <pubDate>Wed, 01 May 2024 11:06:14 GMT</pubDate>
    </item>
    <item>
      <title>dm_control 的替代方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chhx6w/alternatives_to_dm_control/</link>
      <description><![CDATA[嗨 我知道 dm_control 在很多研究工作中使用，我也想使用它。事实证明，它没有很好的记录，很难导航，而且最糟糕的是，维护者没有正确回答问题，有时甚至完全忽略这些问题。这让我很愤怒，但我无能为力，我不会因此责怪开发人员，他们可能将时间投入到其他一些作品上，并且在任何情况下都没有义务回答我们。  ​ 话虽这么说，我真的很希望看到该领域开发出一些替代方案，以便减少闯入该领域的人并做出更多贡献。  ​ 您是否知道一些正在朝这个方向发展的作品？  &amp; #32；由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chhx6w/alternatives_to_dm_control/</guid>
      <pubDate>Wed, 01 May 2024 09:32:07 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习的四轴飞行器控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ch4h1d/quadcopter_control_with_reinforcement_learning/</link>
      <description><![CDATA[您好，我是一名学生，正在研究一个有关使用深度强化学习的四轴飞行器控制器的项目。不幸的是，我的项目演示将于下周进行，而我的电脑缺乏从头开始构建它的能力。您知道我可以参考的任何现有资源或项目吗？    由   提交/u/taha_sbh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ch4h1d/quadcopter_control_with_reinforcement_learning/</guid>
      <pubDate>Tue, 30 Apr 2024 21:32:30 GMT</pubDate>
    </item>
    <item>
      <title>具有不确定性的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgx7uz/environments_with_uncertainty/</link>
      <description><![CDATA[有人知道有哪些环境可能表现出一些不确定性吗？例如，让我们想象环境中的一个部分，如果代理进入其中，不确定性为： ​  采取所需操作的概率会降低并且更加随机 奖励在该区域中变得随机 还是其他东西？  ​ 我想要它，这样我就可以研究一些预先存在的不确定性强化学习技术。最好环境与健身房兼容，我不介意离散或连续，对两者都很满意:) ​ 提前致谢！ &lt; /div&gt;  由   提交/u/Glum_Significance140   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgx7uz/environments_with_uncertainty/</guid>
      <pubDate>Tue, 30 Apr 2024 16:33:08 GMT</pubDate>
    </item>
    <item>
      <title>DQN - 调度状态表示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgvx6k/dqn_scheduling_state_representation/</link>
      <description><![CDATA[大家好， 我对 DQN 和强化学习比较陌生，我想针对调度问题实现 DQN 模型在Python中使用keras。在尝试自定义环境之前，我做了一些教程，例如健身车杆，并观看了一些教程，但我似乎在努力解决 DQN 的状态表示（我认为）。 问题陈述： 假设我们有 X 名工人和 Y 份工作。目标是根据以下因素将工作分配给最合适的工人：  工作所需的技能以及工人获得的技能 工作的持续时间工作和工人的剩余可用时间。  其中可用性和持续时间是连续整数，技能是二进制值（如果不存在技能则为 0，如果存在则为 1）。 如果如果不满足这些条件，那么理想情况下，不应将工人分配到该工作 这是我设计状态的方式 - 我从所有工人和只有一项工作开始（我们只是假设工作依次分配）： [[AvailabilityWorker1, Skill1Worker1, Skill2Worker1, Skill3Worker1], [AvailabilityWorker2, Skill1Worker2, Skill2Worker2, Skill3Worker2],  ... , [JobDuration, Skill1Job, Skill2Job, Skill3Job]] 操作是选择其中一个工作人员，或者根本不选择工作人员（因此 no_workers + 1） 每执行一步后都会更新状态，以将当前工作替换为下一个工作已安排。此外，所选工作人员的可用性也会更新。 我似乎在与状态代表作斗争，或者与其他阻止我的代理人理解问题的事情作斗争。我尝试了许多不同的状态表示和模型参数，但训练后的模型始终不是一个好的解决方案。 我很高兴收到任何建议！ 提前致谢！   由   提交/u/-Aryiox-  /u/-Aryiox-  reddit.com/r/reinforcementlearning/comments/1cgvx6k/dqn_scheduling_state_representation/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgvx6k/dqn_scheduling_state_representation/</guid>
      <pubDate>Tue, 30 Apr 2024 15:39:29 GMT</pubDate>
    </item>
    <item>
      <title>是否有任何资源可以研究如何对样本进行加权（例如，按情节）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgutqp/is_there_any_resources_which_look_at_how_to/</link>
      <description><![CDATA[我无法将支持和反对各种样本加权策略的论据形式化。首先我想说，一个情节有多少个样本与该情节对于我的特定问题的重要性无关，也就是说，只有奖励才能影响错误对损失的影响。话虽这么说，我面临着一个两难的境地，我希望有人能帮助我思考。可能值得注意的是，这也是针对我的问题的，单个情节==单个奖励，并且我不想包括时间范围。例如，一个情节中的每个样本都将具有相同的奖励，因为我想确保优化考虑一个情节中的所有样本对奖励的同等贡献。 一方面，我觉得按情节对样本进行加权，确保我们平等地观看每一集。如果一个剧集没有很多样本但奖励较高，而相比于一个剧集有很多样本但奖励较小，我们仍然会关注更重要的剧集。 如果我们对样本进行统一加权，我们会考虑每个样本独立。这样做的好处是，对于大的episode，我们仍然关心个体样本的预测。然而，通过按情节加权，对于较大的情节，我们不会太关心单个样本的误差。我认为这在某些方面是有道理的，但在其他方面可能没有道理。 我已经尽可能考虑使用 1 / 剧集长度的平方根作为样本权重，以本质上找到两者之间的中间立场两者（因为统一权重是 1/所有样本，剧集权重是 1/len(episode)）。虽然无法将论点形式化，但很难知道这个决定是否有意义。 有什么想法吗？    ;由   提交/u/Yogi_DMT   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgutqp/is_there_any_resources_which_look_at_how_to/</guid>
      <pubDate>Tue, 30 Apr 2024 14:54:00 GMT</pubDate>
    </item>
    <item>
      <title>多智能体 RL 对于库存优化的可扩展性如何？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgqtjs/how_scalable_is_multi_agent_rl_for_inventory/</link>
      <description><![CDATA[我是一名数据科学家/机器学习工程师，在强化学习方面还是个新手。我最近才开始研究它——还没有动手。  无论如何，我的团队提出了以仓库库存管理为中心的用例，其中包括 1000 种独特的产品。我想我们也许可以为此应用 MARL（例如 https://github.com/microsoft/maro）尽量减少超龄/未成年。但首先我想知道 MARL 的可扩展性如何？ 如果我理解正确的话...代理的操作是分散的（因此可并行），但是策略的调整是集中的，因此不可并行(?)  是否可以在合理的时间内训练 10k 个代理（每个独特产品 1 个），或者会花费太长时间吗？ / 成本太高？   由   提交 /u/WhyDo TheyAlwaysWin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgqtjs/how_scalable_is_multi_agent_rl_for_inventory/</guid>
      <pubDate>Tue, 30 Apr 2024 11:46:44 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习目前的最新技术是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgobyl/what_is_the_current_state_of_the_art_in_multi/</link>
      <description><![CDATA[自 2019 年以来我就没怎么关注过 MARL，我很好奇从那以后发生了什么。论文链接将不胜感激！    由   提交/u/geargi_steed  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgobyl/what_is_the_current_state_of_the_art_in_multi/</guid>
      <pubDate>Tue, 30 Apr 2024 09:07:35 GMT</pubDate>
    </item>
    <item>
      <title>为什么DPO的损失函数中有“reference free”选项？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cglgra/why_is_there_a_reference_free_option_in_dpos_loss/</link>
      <description><![CDATA[在reference_free参数中，trl对DPO的损失函数实现中，有一个reference_free参数，而DPO原论文中没有提到&quot;reference free&quot;这个概念。 在trl的实现中：  pi_logratios = policy_chosen_logps - policy_rejected_logps if reference_free: ref_logratios = 0 else: ref_logratios = reference_chosen_logps - reference_rejected_logps logits = pi_logratios - ref_logratios  当 reference_free 为 True 时，损失计算为 policy_chosen_logps 和 policy_rejected_logps 之间的交叉熵，而 reference_chosen_logps 和 reference_rejected_logps 未被使用。 “无参考”DPO 的优缺点是什么？    提交人    /u/yang_bo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cglgra/why_is_there_a_reference_free_option_in_dpos_loss/</guid>
      <pubDate>Tue, 30 Apr 2024 05:50:41 GMT</pubDate>
    </item>
    </channel>
</rss>