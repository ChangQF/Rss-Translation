<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 09 Jan 2024 01:01:59 GMT</lastBuildDate>
    <item>
      <title>导入错误：libmujoco150.so：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/</link>
      <description><![CDATA[我正在尝试构建一个包含 mujoco 的 Docker 映像。此外，我希望它出现在我的自定义地址中。  ​ 这是我创建的 Dockerfile。我引用了此处使用的环境变量 -  FROM ubuntu:22.04 WORKDIR /app SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] RUN mkdir -p myhome/house ENV HOME=&quot;/myhome/house:${PATH}&quot; RUN 回显“Hello World！”运行 apt-get update &amp;&amp; apt-get install -y\libosmesa6-dev\sudo\wget\curl\unzip\gcc\g++\&amp;&amp; apt-get install \ libosmesa6-dev \ &amp;&amp; rm -rf /var/lib/apt/lists/* ENV DEBIAN_FRONTEND=非交互式 ENV PATH=&quot;/miniconda3/bin:${PATH}&quot; ARG PATH=“/miniconda3/bin:${PATH}”运行 cd / \ &amp;&amp; mkdir -p /miniconda3 \ &amp;&amp; wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /miniconda3/miniconda.sh \ &amp;&amp; bash /miniconda3/miniconda.sh -b -u -p /miniconda3 \ &amp;&amp; /miniconda3/bin/conda init bash \ &amp;&amp;源 ~/.bashrc \ &amp;&amp; conda init \ &amp;&amp; conda create -y -n myenv python=3.8 \ &amp;&amp; conda update -y conda WORKDIR /~ RUN wget https://roboti.us/download/mjpro150_linux.zip \ &amp;&amp;解压mjpro150_linux.zip \ &amp;&amp; mkdir ~/.mujoco \ &amp;&amp; mv mjpro150 ~/.mujoco \ &amp;&amp; wget https://roboti.us/file/mjkey.txt \ &amp;&amp; mv mjkey.txt ~/.mujoco \ &amp;&amp; rm mjpro150_linux.zip ENV MJLIB_PATH=“/myhome/house/.mujoco/mjpro150/bin/libmujoco150.so:${MJLIB_PATH}” ENV LD_LIBRARY_PATH=“/myhome/house/.mujoco/mjpro150/bin:${LD_LIBRARY_PATH}” ENV MUJOCO_PY_MUJOCO_PATH=“/myhome/house/.mujoco/mjpro150:${MUJOCO_PY_MUJOCO_PATH}” ENV MUJOCO_PY_MJKEY_PATH=“/myhome/house/.mujoco/mjkey.txt:${MUJOCO_PY_MJKEY_PATH}”运行 cd /miniconda3/envs/myenv/lib/ &amp;&amp; mv libstdc++.so.6 libstdc++.so.6.old &amp;&amp; ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6 SHELL [“conda”, “run”, “-n”, “myenv”, “/ bin/bash”，“-c”] EXPOSE 5003 RUN pip install --no-cache-dir “Cython&lt;3” RUN pip install mujoco-py==1.50.1.0  构建不断失败，错误显示在顶部。有人可以帮忙解决这个问题吗？   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/</guid>
      <pubDate>Mon, 08 Jan 2024 23:00:50 GMT</pubDate>
    </item>
    <item>
      <title>最佳强化学习研究框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191yu6y/best_rl_research_framework/</link>
      <description><![CDATA[我需要启动一个新的强化学习项目，并问自己哪个强化学习库或框架最适合学术研究。我假设我将使用gymnasium 来构建我需要构建的自定义环境，但我不确定策略（算法）的库。这个想法是能够在自定义环境中切换到几种不同的算法。我过去使用稳定的基线，然后从头开始编写 PPO 实现，并使用了一段时间。现在我想过渡到更灵活的东西，我不必从头开始实现不同的算法。稳定的基线仍然是最好的选择吗？    由   提交 /u/alebrini   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191yu6y/best_rl_research_framework/</guid>
      <pubDate>Mon, 08 Jan 2024 22:59:01 GMT</pubDate>
    </item>
    <item>
      <title>Rich Sutton 的 10 个人工智能口号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191waws/rich_suttons_10_ai_slogans/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191waws/rich_suttons_10_ai_slogans/</guid>
      <pubDate>Mon, 08 Jan 2024 21:16:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 采访里奇·萨顿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191rmsl/d_interview_with_rich_sutton/</link>
      <description><![CDATA[ 由   提交 /u/atgctg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191rmsl/d_interview_with_rich_sutton/</guid>
      <pubDate>Mon, 08 Jan 2024 18:10:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么奖励价值高于累积奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191gc9m/why_reward_to_go_values_over_cumulative_rewards/</link>
      <description><![CDATA[嗨，强化学习新手，目前正在研究基于序列建模或基于离线强化学习的方法。当他们使用像架构这样的 GPT 时，我发现他们经常似乎将奖励作为每个时间步骤的代币嵌入之一以及动作和状态，而不是在该时间步骤中累计获得的奖励的天真的奖励-step？ 如果我错了，请纠正我，谢谢！   由   提交 /u/alchemistensei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191gc9m/why_reward_to_go_values_over_cumulative_rewards/</guid>
      <pubDate>Mon, 08 Jan 2024 08:18:45 GMT</pubDate>
    </item>
    <item>
      <title>机器人课程项目调查！任何经验都有帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</link>
      <description><![CDATA[大家好， 我正在与卡耐基梅隆大学和宾夕法尼亚大学的一些机器人专业学生合作开展一个课堂项目，以研究疼痛学者和行业专业人士在从事机器人开发时面临的问题。如果您从事或认识从事机器人开发流程任何部分的人，并且有 10 分钟的空闲时间，我们将非常感谢您的意见。我们希望获得广泛经验水平的意见。因此，我们重视刚开始接触机器人技术的人们以及具有多年经验的人们的意见。回复是匿名的，绝不反映绩效，因此我们要求您诚实回答。我们计划在 1 月 14 日之前收集回复（但如果调查之后开放，请随时贡献您的想法！）。  https://forms.gle/Mx247TgeDbEydY426 谢谢，   由   提交/u/awkyu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</guid>
      <pubDate>Sun, 07 Jan 2024 23:56:05 GMT</pubDate>
    </item>
    <item>
      <title>演奏乐器的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</link>
      <description><![CDATA[寻找任何已知的演奏乐器的模拟环境。例如，一个灵巧的特工在弹吉他。   由   提交 /u/Ultra-Neural   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</guid>
      <pubDate>Sun, 07 Jan 2024 21:31:00 GMT</pubDate>
    </item>
    <item>
      <title>这是从模型停止的地方继续训练的正确方法吗？稳定基线3、Pytorch、Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</link>
      <description><![CDATA[嗨， 我正在训练一个模型，昨天我保存并关闭了，因为已经很晚了，我需要睡觉。现在，今天我想从上次停下来的地方继续训练，但谷歌的结果好坏参半，2018 年、19 年、20 年的答案等等。 这是我的代码，如果有人可以确认这是正确的序列，我会很感激。 log_dir = &quot;/path/where/I/want/logs/saved&quot; model_dir =“/path/to/saved/zip/file” env = MyENV() env.reset () model = PPO(&quot;MlpPolicy&quot;, env, verbose = 1,tensorboard_log=log_dir) model .set_parameters(model_path, True) TIMESTEPS = 10000 CONTINUE_BOOKMARK = 35 #最新保存的文件是340000，所以350,000 将是下一个邮政编码... for i in range(CONTINUE_BOOKMARK, 51): model.learn （total_timesteps=TIMESTEPS，reset_num_timesteps=False，tb_log_name=“log_name_here”） model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 我即将运行它，但我担心我可能做得不对，如果它确实有效，那只是巧合。 ​ 编辑： 我最终使用了类似的东西 model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 唯一的事情是，tensorboard 日志看起来没有从之前的日志继续... &lt;!-- SC_ON - -&gt;  由   提交 /u/phantomBlurrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</guid>
      <pubDate>Sun, 07 Jan 2024 19:44:19 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化分析调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/190qhw5/a_survey_analyzing_generalization_in_deep/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.02349 存储库：https ://github.com/EzgiKorkmaz/generalization-reinforcement-learning 摘要：  强化学习研究取得重大成功和注意力利用深度神经网络来解决高维状态或动作空间中的问题。虽然深度强化学习策略目前被部署在从医疗应用到自动驾驶汽车的许多不同领域，但该领域仍然存在一些关于深度强化学习策略的泛化能力的问题。在本文中，我们将概述深度强化学习策略遇到限制其鲁棒性和泛化能力的过拟合问题的根本原因。此外，我们将形式化和统一不同的解决方案以提高泛化性，并克服状态-动作价值函数的过度拟合。我们相信我们的研究可以为当前深度强化学习的进展提供紧凑、系统的统一分析，并有助于构建具有更高泛化能力的鲁棒深度神经策略。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/190qhw5/a_survey_analyzing_generalization_in_deep/</guid>
      <pubDate>Sun, 07 Jan 2024 11:49:27 GMT</pubDate>
    </item>
    <item>
      <title>如何获得 AI/ML 和强化学习方面的经验来担任研究职位？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/190l2rc/how_to_get_experience_in_aiml_and_reinforcement/</link>
      <description><![CDATA[您好，我是一名计算机科学专业的新生，对 AI/ML 研究非常感兴趣，尤其是强化学习。我想向教授寻求研究机会，但我没有太多经验可以展示。我已经完成了一些在线课程，阅读了教科书等，但除了我完成了一些编码作业作为其中的一部分之外，我没有什么可以展示的。您对我可以做些什么来获得强化学习经验有什么建议吗？我可以向教授展示这些经验，以证明我已经准备好在他们的实验室进行研究？我一直在考虑从头开始实现一些论文和/或做一些涉及机器学习的副项目。这是一个好的起点吗？   由   提交/u/meemaowie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/190l2rc/how_to_get_experience_in_aiml_and_reinforcement/</guid>
      <pubDate>Sun, 07 Jan 2024 05:46:30 GMT</pubDate>
    </item>
    <item>
      <title>没有的极限是什么？ PPO 中的观察结果是否有助于良好且快速的训练？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zym15/whats_the_limit_of_no_of_observations_in_ppo_for/</link>
      <description><![CDATA[我是 PPO 的新手，我有一个疑问，比如什么是一个好的数字（观察数量），它将通过 PPO 提供良好的训练结果算法？就像更多的观察意味着更多的信息和快速学习或者什么......   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zym15/whats_the_limit_of_no_of_observations_in_ppo_for/</guid>
      <pubDate>Sat, 06 Jan 2024 12:04:19 GMT</pubDate>
    </item>
    <item>
      <title>增强静态数据环境中 DRL 代理的泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zxyjh/enhancing_generalization_in_drl_agents_in_static/</link>
      <description><![CDATA[上下文： 我正在类似市场的环境中使用深度强化学习 (DRL) 代理，其行为不影响环境。环境使用特定日期之前的历史数据进行训练，并保留该日期之后的数据用于评估。训练阶段中的每个时间步“t”为代理提供数据集中的相应行。 问题：当训练超出“T”时间步时，代理开始看到重复相同的观察结果，这引起了人们对过度拟合及其泛化能力的担忧。尽管重放缓冲区通过随机采样观察结果来更新模型权重有所帮助，但我担心在长期训练中，代理可能会学习训练数据集中的特定转换，而不是开发可通用的解决方案。  问题：如何增强 DRL 代理在这种静态、数据驱动的训练环境中的泛化能力？是否有特定的训练策略或调整可以鼓励代理制定可推广且有效的策略，而不仅仅是记住训练数据集？   由   提交 /u/Disastrous_Effort725   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zxyjh/enhancing_generalization_in_drl_agents_in_static/</guid>
      <pubDate>Sat, 06 Jan 2024 11:21:38 GMT</pubDate>
    </item>
    <item>
      <title>运行非常简单的代码</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zxam9/getting_very_simple_code_to_run/</link>
      <description><![CDATA[我正在尝试稳定的 baslines3 中最简单的代码，但我无法让它运行。它给了我： ​ 文件“/home/user/python/mypython3.10/lib/python3.10/site-packages/stable_baselines3 /common/vec_env/dummy_vec_env.py”，第 77 行，重置中 obs, self.reset_infos[env_idx] = self.envs[env_idx].reset(seed=self ._seeds[env_idx], **maybe_options) TypeError: CoinFlipEnv.reset() 得到意外的关键字参数“seed”  ​ 这是代码： ​ 将gymnasium导入为gym &lt; p&gt;将 numpy 导入为 np 从 stable_baselines3 导入 PPO 从 stable_baselines3.common.vec_env 导入 DummyVecEnv&lt; /code&gt; ​ class CoinFlipEnv(gym.Env): def __init__(self ，heads_probability=0.8）： super(CoinFlipEnv, self).__init__() self.action_space =gym. space.Discrete(2) # 0 为头部，1 为尾部 self.observation_space =gym.spaces.Discrete(2) # 0 为头部，1 为尾部代码&gt; self.heads_probability = Heads_probability self.flip_result = None ​ ; def reset(self): # 重置环境 self .flip_result = None 返回 self._get_observation() ​ def step(self, action): # 执行动作（0 为正面，1 为反面） self. Flip_result = int(np.random.rand() &lt; self.heads_probability) ​ # 计算奖励（预测正确为 1，预测错误为 -1） reward = 1 if self.flip_result == action else -1 ​ # 返回观察值、奖励、完成和信息 return self._get_observation(),reward, True, {} ​&lt; /p&gt; def _get_observation(self): # 返回当前抛硬币结果  return self.flip_result ​ # 创建正面概率为0.8的环境 &lt; code&gt;env = DummyVecEnv([lambda: CoinFlipEnv(heads_probability=0.8)]) ​ # 创建 PPO 模型 model = PPO(&quot;MlpPolicy&quot;, env, verbose=1) ​ # 训练模型 model.learn(total_timesteps=10000) ​ &lt; code&gt;# 保存模型 model.save(&quot; ;coin_flip_model&quot;) ​ # 评估模型 obs = env .reset() for _ in range(10): action, _states = model.predict(obs)  obs、奖励、完成、info = env.step(action) print(f&quot;Action: {action }，观察：{obs}，奖励：{rewards}”) ​ ​ 什么我做错了吗？ ​ ​   由   提交 /u/wiggyhat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zxam9/getting_very_simple_code_to_run/</guid>
      <pubDate>Sat, 06 Jan 2024 10:37:03 GMT</pubDate>
    </item>
    <item>
      <title>元强化学习任务的程序生成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zs3l1/procedural_generation_of_metareinforcement/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2302.05583 OpenReview：https:// /openreview.net/forum?id=16fkkkCeOC 代码：https://github.com/ThomasMiconi/Meta-Task-Generator 摘要：  开放性能够生成无限多样、具有挑战性的环境，将受益匪浅。一种特别有趣的挑战类型是元学习（“学会学习”），这是智能行为的标志。然而，文献中元学习环境的数量是有限的。在这里，我们描述了具有任意刺激的简单元强化学习（meta-RL）任务的参数化空间。参数化使我们能够随机生成任意数量的新颖的简单元学习任务。参数化的表达能力足以包含许多众所周知的元强化学习任务，例如强盗问题、Harlow 任务、T 迷宫、Daw 两步任务等。简单的扩展使其能够捕获基于二维拓扑空间的任务，例如完整迷宫或查找域。我们描述了许多随机生成的具有不同复杂度的元强化学习域，并讨论了随机生成产生的潜在问题。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zs3l1/procedural_generation_of_metareinforcement/</guid>
      <pubDate>Sat, 06 Jan 2024 05:05:58 GMT</pubDate>
    </item>
    <item>
      <title>“动物的随机搜索可能有助于它们狩猎：觅食和掠食动物的神经系统可能会促使它们沿着一种称为 Lévy 行走的特殊随机路径移动，以便在没有线索的情况下有效地寻找食物”（Lévy 航班）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18znfh9/random_search_wired_into_animals_may_help_them/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18znfh9/random_search_wired_into_animals_may_help_them/</guid>
      <pubDate>Sat, 06 Jan 2024 01:14:02 GMT</pubDate>
    </item>
    </channel>
</rss>