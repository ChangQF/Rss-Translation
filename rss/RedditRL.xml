<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 06 Jun 2024 15:16:26 GMT</lastBuildDate>
    <item>
      <title>反过来想，在 RL 中“使用采样命令进行探索”有什么意义呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</link>
      <description><![CDATA[在行为函数更新后，RL upside down 选择最佳的前 k 个近期情节并汇总来自它们的信息以提出目标命令，这些命令是这些精英情节的奖励和范围之和的平均值。然后这些命令用于生成更多情节以添加到重放缓冲区中。但我注意到在生成情节时，初始状态是重置状态，这意味着前进的轨迹可能与这些命令完全无关，因为那些新看到的状态可能不会导致那些提出的命令，从而导致随机采样动作。在这方面，RL upside down 是否依靠神经网络的泛化来获得更高的奖励轨迹？否则我不明白它是如何工作的。    提交人    /u/OutOfCharm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</guid>
      <pubDate>Thu, 06 Jun 2024 15:06:44 GMT</pubDate>
    </item>
    <item>
      <title>多模式 Mamba/mamba+Transformers 可以使用文本进行在线 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d96n3c/can_multimodal_mambamambatransformers_do_online/</link>
      <description><![CDATA[你好 r/ReinforcementLearning 所以我正在解决一个比文本/图片/机器人更多（更多）的问题，并且基本上没有解决方案数据集可以训练，除了书籍和博客。  动作空间是一组离散、图形和多二进制动作，观察空间是动作空间+在其上执行的一些计算。 是否可以将大量文本输入模型，给它推理（实际推理），并期望模型在初步反复试验后使用文本知识来回答离散的非文本问题？ 此外，是否可以使用类似 Mamba+Transformers 架构的东西来进行这种类型的在线无模型 RL？  在这里做我的第一个模型......谢谢大家！    由    /u/JustZed32  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d96n3c/can_multimodal_mambamambatransformers_do_online/</guid>
      <pubDate>Thu, 06 Jun 2024 01:35:02 GMT</pubDate>
    </item>
    <item>
      <title>从这往哪儿走？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</link>
      <description><![CDATA[我有一个需要 RL 的项目，我学习了 Sutton 撰写的 RL 简介的前 200 页，掌握了基础知识和所有基本的理论信息。你们有什么建议可以开始真正用 RL 实现我的项目想法，比如从 OpenAI Gym 中的基本想法开始，或者我不知道自己是新手，你们能给我一些建议，告诉我如何在实践方面做得更好吗？    提交人    /u/Signal-Ad3628   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</guid>
      <pubDate>Thu, 06 Jun 2024 01:32:05 GMT</pubDate>
    </item>
    <item>
      <title>如何才能正确地实时地将正确的操作添加到内存缓冲区？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d95s2y/how_can_i_correctly_add_the_right_action_to_the/</link>
      <description><![CDATA[我正在尝试训练 PPO 代理来玩 Geometry Dash，我认为尝试使用 OpenCV 并使用视频源作为输入会很有趣，而不是创建游戏副本并直接使用游戏内存/状态。事实证明这比我想象的要困难得多。 实施的高级概述 我训练了 YoloV4 来检测代理是活着还是死了。我认为这将是一种确定代理是否已死亡的好方法。当代理活着时，它将根据给定的 4 帧堆栈作为输入来选择操作。如果代理存活，则每帧都会收到 +1 奖励，如果代理死亡，则收到 0。 问题 我注意到的一个问题是，存储在代理内存缓冲区中的操作与游戏中发生的操作并不完全匹配（即，代理因跳入尖刺而死亡，但内存缓冲区会存储它没有跳跃）。 我认为问题的一部分是玩家无法在空中做任何事情，但代理不知道这一点，并试图选择操作，这会覆盖它实际上能够成功执行的最后一个操作，然后将其存储在其内存中。 我尝试过的一件事是引入跳跃的冷却时间。但是，这是一个问题，因为关卡的某些部分需要连续多次跳跃而不停止。 我非常感谢您对如何改进项目的反馈！    提交人    /u/EducationalChicken_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d95s2y/how_can_i_correctly_add_the_right_action_to_the/</guid>
      <pubDate>Thu, 06 Jun 2024 00:50:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 的不完美信息游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d92kvj/imperfect_information_games_using_rl/</link>
      <description><![CDATA[如何使用 RL 制定多人不完美/不完整信息游戏？MARL 是唯一的方法还是还有其他方法/算法？    提交人    /u/Meta_Sage_247   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d92kvj/imperfect_information_games_using_rl/</guid>
      <pubDate>Wed, 05 Jun 2024 22:18:32 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型中出现了欺骗能力”，Hagendorff 2024（法学硕士设定目标，内心独白越来越容易被操纵）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8rxjn/deception_abilities_emerged_in_large_language/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8rxjn/deception_abilities_emerged_in_large_language/</guid>
      <pubDate>Wed, 05 Jun 2024 14:54:56 GMT</pubDate>
    </item>
    <item>
      <title>“国际象棋神经网络中学习前瞻的证据”，Erik Jenner 2024（Leela Chess Zero 在前向传递过程中至少会前瞻两次）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8ruai/evidence_of_learned_lookahead_in_a_chessplaying/</link>
      <description><![CDATA[        提交人    /u/gwern   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8ruai/evidence_of_learned_lookahead_in_a_chessplaying/</guid>
      <pubDate>Wed, 05 Jun 2024 14:50:58 GMT</pubDate>
    </item>
    <item>
      <title>我的 TD3 继续自杀，即使奖励更糟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8r7kz/my_td3_keep_suiciding_even_if_the_reward_is_worse/</link>
      <description><![CDATA[        提交人    /u/antoine12e9   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8r7kz/my_td3_keep_suiciding_even_if_the_reward_is_worse/</guid>
      <pubDate>Wed, 05 Jun 2024 14:24:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] Python 中 NEAT 的一个实际工作、简化实现（sNEAT）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8nsjl/p_an_actually_working_simplified_implementation/</link>
      <description><![CDATA[目前可用的神经进化论拓扑 (Kenneth O. Stanley, 2002) 的开源实现要么被极度弃用，要么根本无法提供预期的结果。 我用 Python 重写了它，并在 GPLv3 下提供它，因为我认为我不是唯一一个对这种非常有趣的进化方法的状态感到失望的人。 您可以在Github上找到它。    提交人    /u/Sthatic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8nsjl/p_an_actually_working_simplified_implementation/</guid>
      <pubDate>Wed, 05 Jun 2024 11:39:18 GMT</pubDate>
    </item>
    <item>
      <title>项目想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8mf15/project_ideas/</link>
      <description><![CDATA[嗨！一个项目要求我在连续动作空间环境中实现策略梯度算法  然后我应该提出某种研究问题并进行有意义的实验。  我没有太多时间，所以我计划从外部源实现 PPO，并在像月球着陆器连续这样的简单环境中工作，但我真的很难找到可以做的有意义的实验  你有什么建议吗？  谢谢！    提交人    /u/An4rcyst   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8mf15/project_ideas/</guid>
      <pubDate>Wed, 05 Jun 2024 10:15:11 GMT</pubDate>
    </item>
    <item>
      <title>基于价值的代理中的动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8ldq3/action_masking_in_valuebased_agents/</link>
      <description><![CDATA[策略梯度算法的动作掩蔽已有相当详尽的文档 (https://arxiv.org/pdf/2006.14171)。但是，当贪婪地选择最佳动作时，掩蔽 q 值（比如说一个简单的深度 q 网络）有什么意义吗？显然在“推理”时间中，它只能是正的（因为我们阻止代理选择无用/不可能的动作，尽管如果代理这样做，则可能意味着训练尚未收敛到最优值），但在训练期间呢？有资源谈论它吗？也许比较两者？    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8ldq3/action_masking_in_valuebased_agents/</guid>
      <pubDate>Wed, 05 Jun 2024 09:03:35 GMT</pubDate>
    </item>
    <item>
      <title>需要研究项目的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8ktnu/need_help_in_research_project/</link>
      <description><![CDATA[我是 RL 新手，我的导师给了我一个研究项目，我必须在任何具有形状值的环境中实现 dqn。我确实了解它们两者的工作原理，但我没有太多的编码经验来实现它。目前，我正在学习 huggingface 的 RL 课程。请建议我如何解决这个问题？    提交人    /u/MarionberryVisual911   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8ktnu/need_help_in_research_project/</guid>
      <pubDate>Wed, 05 Jun 2024 08:23:48 GMT</pubDate>
    </item>
    <item>
      <title>确实是更好的短语</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8fvug/truly_the_better_phrase/</link>
      <description><![CDATA[        提交人    /u/scruffy0014   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8fvug/truly_the_better_phrase/</guid>
      <pubDate>Wed, 05 Jun 2024 03:06:40 GMT</pubDate>
    </item>
    <item>
      <title>无法将状态和动作张量附加到列表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d891f1/unable_to_append_state_and_action_tensors_to_a/</link>
      <description><![CDATA[我正在开发一个基于 rl 算法的简单模型，它将状态和动作作为输入并输出下一个状态 请在这里找到我的代码。  我的问题与以下代码块有关 -  ```  preds_array.append(torch.max(preds, 1)[1].unsqueeze(0)) next_state_array.append(next_state_tensor) # 计算损失和准确度 loss = loss_fn(preds, next_state_tensor) if episode%50==0 and episode!=0: print(&quot;next_state_array = &quot;, next_state_array) print(&quot;Length = &quot;, len(preds_array))  ``` 由于某种原因，我的 `preds_array` 和 `next_state_array` 仅包含一个元素。我花了几个小时进行调试，也向 ChatGPT 寻求帮助，但就是搞不清楚问题出在哪里。 如果需要任何说明，请告诉我。    提交人    /u/Academic-Rent7800   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d891f1/unable_to_append_state_and_action_tensors_to_a/</guid>
      <pubDate>Tue, 04 Jun 2024 21:41:33 GMT</pubDate>
    </item>
    <item>
      <title>很难理解 PPO 损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d7wqqy/hard_time_understanding_ppo_loss/</link>
      <description><![CDATA[      我正在实施 PPO 方法，到目前为止，它被证明是成功的。我成功地在健身房的月球着陆器上训练了它。但最终的损失图对我来说没有意义。据我所知，我们正在尝试将其最小化，因此损失越低，模型越好。但是看看损失和平均奖励图表： https://preview.redd.it/jbqf5bxczj4d1.png?width=996&amp;format=png&amp;auto=webp&amp;s=892e907eb860242d3a4d38309e9f0ce231056371 大约 25-50 步时，损失大幅减少，这应该意味着模型变得更好。但平均奖励也大幅下降。大约 100 步损失增加，因此平均奖励也增加。看起来更高的损失意味着更好的模型，但这对我来说没有意义    提交人    /u/Aydiagam   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d7wqqy/hard_time_understanding_ppo_loss/</guid>
      <pubDate>Tue, 04 Jun 2024 13:09:01 GMT</pubDate>
    </item>
    </channel>
</rss>