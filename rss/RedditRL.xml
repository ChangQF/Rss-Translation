<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 28 Jun 2024 21:15:35 GMT</lastBuildDate>
    <item>
      <title>“法学硕士驱动的自主代理”，Lilian Weng</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqrm46/llm_powered_autonomous_agents_lilian_weng/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqrm46/llm_powered_autonomous_agents_lilian_weng/</guid>
      <pubDate>Fri, 28 Jun 2024 19:10:22 GMT</pubDate>
    </item>
    <item>
      <title>“监督预训练可以学习上下文强化学习”，Lee 等人，2023 年（决策变压器是进行后验采样的贝叶斯元学习器）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqrie9/supervised_pretraining_can_learn_incontext/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqrie9/supervised_pretraining_can_learn_incontext/</guid>
      <pubDate>Fri, 28 Jun 2024 19:05:39 GMT</pubDate>
    </item>
    <item>
      <title>“InterCode：通过执行反馈对交互式编码进行标准化和基准测试”，Yang 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqrffr/intercode_standardizing_and_benchmarking/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqrffr/intercode_standardizing_and_benchmarking/</guid>
      <pubDate>Fri, 28 Jun 2024 19:02:13 GMT</pubDate>
    </item>
    <item>
      <title>具有时空 GNN 和强化学习经验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqox4m/experience_with_spatiotemporal_gnns_and/</link>
      <description><![CDATA[大家好 :) 我想知道你们中是否有人有使用时空 GNN 作为 RL 模型的经验？我的论文需要它，我想优化网络中服务器的分配，以最小化总体延迟。 提前谢谢您！    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqox4m/experience_with_spatiotemporal_gnns_and/</guid>
      <pubDate>Fri, 28 Jun 2024 17:15:25 GMT</pubDate>
    </item>
    <item>
      <title>“利用梯度对抗不确定性：通过扩散分数匹配进行离线强化学习”，Suh 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqmwxd/fighting_uncertainty_with_gradients_offline/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqmwxd/fighting_uncertainty_with_gradients_offline/</guid>
      <pubDate>Fri, 28 Jun 2024 15:51:32 GMT</pubDate>
    </item>
    <item>
      <title>对于 PPO 算法来说，在具有多个离散动作（~50）的环境中，有哪些选项可以包含内在奖励来改善代理探索？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqilng/what_are_the_options_for_including_intrinsic/</link>
      <description><![CDATA[嗨， 我有一个具有许多离散状态的环境和一个具有 50 多个离散动作的代理。在这样的环境中，我的 PPO 代理很难学习，因为我的环境具有非常稀疏的奖励；代理在执行 10 多个正确的离散动作之前不会获得奖励。 我正在研究提供内在奖励的不同方法，这些方法可能会增强代理的探索和学习率。我尝试了本文中提出的内在好奇心奖励：https://pathak22.github.io/noreward-rl/。不幸的是，我的代理的表现并没有真正提高。 我想知道是否有任何其他更好的内在奖励选项，特别是针对我的自定义环境。 提前谢谢您！   由    /u/unknown_unknown_001  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqilng/what_are_the_options_for_including_intrinsic/</guid>
      <pubDate>Fri, 28 Jun 2024 12:35:44 GMT</pubDate>
    </item>
    <item>
      <title>尝试制作一个可以在强化学习中使用的无人机模拟视频。使用 isaac lab 和 isaac sim。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqdbg8/tried_to_make_a_drone_simualtion_video_that_i_can/</link>
      <description><![CDATA[      好吧，我尝试使用最新的强化学习软件制作一个无人机模拟的快速视频，结果很糟糕。我想我会把它发布在这里接受批评。批评吧！！ :&#39;D  https://youtu.be/U6uxMIEJ9Dc?feature=shared  https://preview.redd.it/6o67ldqde99d1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=a8d5a1613e9bd9fde24da21fd8433de09c9743a2    提交人    /u/Next-Crab3966   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqdbg8/tried_to_make_a_drone_simualtion_video_that_i_can/</guid>
      <pubDate>Fri, 28 Jun 2024 06:46:33 GMT</pubDate>
    </item>
    <item>
      <title>“智能 Go-Explore：站在巨人基础模型的肩膀上”，Lu 等人 2024 年（用于标记 Go-Explore 状态的 GPT-4）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dq6hq9/intelligent_goexplore_standing_on_the_shoulders/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dq6hq9/intelligent_goexplore_standing_on_the_shoulders/</guid>
      <pubDate>Fri, 28 Jun 2024 00:23:17 GMT</pubDate>
    </item>
    <item>
      <title>“程序合成的语法树上的扩散”，Kapur 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dq3hn6/diffusion_on_syntax_trees_for_program_synthesis/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dq3hn6/diffusion_on_syntax_trees_for_program_synthesis/</guid>
      <pubDate>Thu, 27 Jun 2024 22:02:44 GMT</pubDate>
    </item>
    <item>
      <title>通过近似抽样实现强化学习的更有效随机探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpz2ci/more_efficient_randomized_exploration_for/</link>
      <description><![CDATA[https://arxiv.org/abs/2406.12241 摘要：汤普森采样（TS）是强化学习（RL）中最流行的探索技术之一。然而，大多数具有理论保证的 TS 算法难以实现，并且不能推广到深度 RL。虽然新兴的基于近似采样的探索方案很有前景，但大多数现有算法特定于具有次优遗憾界限的线性马尔可夫决策过程（MDP），或者仅使用最基本的采样器，例如朗之万蒙特卡洛。在这项工作中，我们提出了一个算法框架，将不同的近似采样方法与最近提出的 Feel-Good Thompson 采样 (FGTS) 方法 (Zhang，2022；Dann 等人，2021) 结合起来，该方法以前被认为在计算上通常是难以解决的。当应用于线性 MDP 时，我们的遗憾分析产生了遗憾对维数的最佳依赖性，超越了现有的随机算法。此外，我们为每个使用的采样器提供了明确的采样复杂度。从经验上讲，我们表明，在需要深度探索的任务中，我们提出的结合 FGTS 和近似采样的算法与其他强基线相比表现明显更好。在 Atari 57 套件中的几款具有挑战性的游戏中，我们的算法实现的性能要么优于要么与深度 RL 文献中的其他强基线相当。    提交人    /u/hmi2015   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpz2ci/more_efficient_randomized_exploration_for/</guid>
      <pubDate>Thu, 27 Jun 2024 18:56:28 GMT</pubDate>
    </item>
    <item>
      <title>帮助使用最新的 Ray 导入 RLlib SACTrainer</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dptiyt/help_with_importing_rllib_sactrainer_with_latest/</link>
      <description><![CDATA[TL;DR 我正在修改一个旧的 RL 脚本 (Ray = 1.1.0)，当使用 Ray=2.31.0 时，该脚本无法识别 SACTrainer() 和 ModelCatalog.register_custom_preprocessor()。我需要更改哪些内容才能让 VS Code 识别它们？ 嗨！我在收到同事发来的旧脚本后尝试设置 RLlib。我的同事使用的是 Ray = 1.1.0，而我有 Ray 2.31.0。我已导入 ray.rllib.algorithms.sac 以使用 Soft-Actor Critic 和函数 SACTrainer。ray.rllib.agents.sac 无法被 VSC 识别，代理模块似乎也不可用，因此我使用的是 rllib.algorithms。  但是，我的 VS Code 无法识别可用的 DQNTrainer() 或 SACTrainer() 函数。SACTrainer() 与我的自定义注册环境一起使用，并具有用于训练的所有配置。 另一个无法识别的函数是 ModelCatalog.register_custom_preprocessor()。它还存在吗？ 我不确定我的导入中缺少什么，或者我是否需要稍微更改代码。我知道 RLlib 中的某些函数可能已被删除或彻底更改，但我无法借助在线资源确定 SACTrainer 是否仍然可用。我是否导入了错误的内容，我应该降级 Ray 吗？或者有没有什么办法可以修复它？ 我在下面添加了部分代码以便为您提供上下文 import ray from ray.rllib.env.external_env import ExternalEnv from ray.tune.registry import register_env from ray.tune.logger import pretty_print from ray.rllib.models import ModelV2, ModelCatalog from ray.rllib.models.preprocessors import Preprocessor, get_preprocessor import ray.rllib.algorithms.dqn as dqn import ray.rllib.algorithms.ppo as ppo import ray.rllib.algorithms.sac as sac import gym import numpy as np from ray.rllib.utils.framework import try_import_tf try: tf=try_import_tf() except Exception as e: tf = None print(f&quot;导入 TensorFlow 时出错：{e}&quot;) sys.exit(1) import tf_slim as slim class aEnvClass(ExternalEnv): &quot;&quot;&quot; 设置观察空间等的服务器后端 &quot;&quot;&quot; class CustomModel(ModelV2): &quot;&quot;&quot; 创建自定义 NN 模型。 &quot;&quot;&quot; # 一些用于函数 _build_layers 和修复状态的代码 class TupleFlatteningPreprocessor(Preprocessor): &quot;&quot;&quot; 将 double 类型的观察列表转换为可接受的离散和框特征格式。然后，它将嵌套的观察列表转换为平面列表。 &quot;&quot;&quot; # 函数 _init_shape() 和 transform() 的一些代码 if __name__ == &quot;__main__&quot;: ray.init(...) register_env(&quot;myEnv&quot;, lambda _: aEnvClass()) ModelCatalog.register_custom_model(&quot;custom_model&quot;, CustomModel) ModelCatalog.register_custom_preprocessor(&quot;my_prep&quot;, TupleFlatteningPreprocessor) my_trainer = sac.SACTrainer( env=&quot;myEnv&quot;, config={ &quot;model&quot;:{&quot;custom_model_config&quot;: &quot;custom_model&quot;, &quot;custom_preprocessor&quot;: &quot;my_prep&quot;,}, &quot;gamma&quot;: 0.1, ... }) while True: results = my_trainer.train() # 更多代码    由   提交  /u/BjunbjonDrinkingChai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dptiyt/help_with_importing_rllib_sactrainer_with_latest/</guid>
      <pubDate>Thu, 27 Jun 2024 15:06:42 GMT</pubDate>
    </item>
    <item>
      <title>建议我改进强化学习模型训练的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpszab/suggest_me_ways_to_improve_training_in_rl_model/</link>
      <description><![CDATA[当前代码是 -&gt; buffer.py -&gt; https://www.pythonmorsels.com/p/26w4b/ train.py -&gt; https://www.pythonmorsels.com/p/2t3ec/ env.py -&gt; https://www.pythonmorsels.com/p/2z9y2/    由   提交  /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpszab/suggest_me_ways_to_improve_training_in_rl_model/</guid>
      <pubDate>Thu, 27 Jun 2024 14:43:52 GMT</pubDate>
    </item>
    <item>
      <title>对于 REINFORCE 算法来说，使用目标网络是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpqokh/would_using_a_target_network_make_sense_for_the/</link>
      <description><![CDATA[由于 REINFORCE 算法中没有像深度 Q 学习那样进行“引导”（我们使用深度神经网络输出概率），是否有人尝试过使用目标网络？这样会产生更稳定的训练吗？我认为不会，但很好奇是否进行过任何实验。    提交人    /u/Lindayz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpqokh/would_using_a_target_network_make_sense_for_the/</guid>
      <pubDate>Thu, 27 Jun 2024 12:59:10 GMT</pubDate>
    </item>
    <item>
      <title>切换到多代理库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpnf11/switching_to_multiagent_library/</link>
      <description><![CDATA[我在 Open AI Gym 中使用 SB3 中的 PPO 和自定义环境。我想切换到具有多智能体算法的库，但对我的环境代码的更改太多。你们有什么建议？    提交人    /u/OccupyFood101   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpnf11/switching_to_multiagent_library/</guid>
      <pubDate>Thu, 27 Jun 2024 09:49:05 GMT</pubDate>
    </item>
    <item>
      <title>关于 ddpg 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpk45g/question_about_ddpg/</link>
      <description><![CDATA[我正在模拟训练一个带有移动底座和 9 个关节的机械臂来打乒乓球，每一步，状态还包含球拍中心的坐标和精确计算出的击球点的坐标，这样 AI 所需要做的就是将球拍移动到该点。我正在使用 DDPG，想知道在 1.2m 步之后仍然没有学会将球拍移动到精确的位置是否正常。    提交人    /u/MaxiHP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpk45g/question_about_ddpg/</guid>
      <pubDate>Thu, 27 Jun 2024 05:57:00 GMT</pubDate>
    </item>
    </channel>
</rss>