<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 30 May 2024 15:15:57 GMT</lastBuildDate>
    <item>
      <title>PPO 陷入局部最优</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d45d3f/ppo_stucks_in_local_optima/</link>
      <description><![CDATA[我为一款基于网格的俄罗斯方块类游戏编写了自己的自定义 Gymnasium 环境。它只有 10x10，非常小。有大约 35 种不同的方块（包括旋转方块）。方块可以放置在任何地方。清除一行/一列后，它们将被清空。可以放置 3 个随机块，完成后，将选择 3 个新块。 我的观察空间是：10x10 = 100 个网格 35 = 块可用性 1 = 乘数 1 = 足够的空间容纳巨大的块 1 = 可能通过插槽 1 中的块清除 1 = 可能通过插槽 2 中的块清除 1 = 可能通过插槽 3 中的块清除 10 = 网格的高度图（每行的总和）所以 150 个观察 动作空间大小为 300（3 个块 * 100 个单元格），编码为单个值 0-300。（0-100：将块放置在位置 XY 等） 放置一个方块会获得微小的奖励。清除一行/一列的得分要高得多（组合甚至更多）。 现在，我正在 GPU（3080）上同时使用 sb3 contrib 中的 MaskablePPO 和 16 个 Envs。 它运行良好，并且代理能够学习基础知识并随着时间的推移达到稳定的分数。 但是当我观看代理玩游戏时，似乎它不会学习一种策略来构建块以一次或一个接一个地清除多个行/列以获得更高的高分。 我甚至增加了将块放置在其他地方而不是清除的惩罚，那时游戏就会结束（没有可放置的块）。 我稍微更改了 PPO 超参数，例如 n_steps 64 batch_size 512 ent_coef 0.02 limit kl 0.02 gamma 0.95 学习率是从 6e-4 到 0 的线性时间表 我也在使用FrameStacking 总共有 9 个观测值。我也让它运行了 30 分钟，但大约 3 分钟后它已经达到了最佳状态。我也尝试过改变超参数，但它要么变得更糟，要么或多或少得到相同的结果。 还有什么我可以做的来帮助代理学习更深层次的策略吗？     提交人    /u/Maxxxel   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d45d3f/ppo_stucks_in_local_optima/</guid>
      <pubDate>Thu, 30 May 2024 14:27:59 GMT</pubDate>
    </item>
    <item>
      <title>RuntimeError：梯度计算所需的变量之一已被就地操作修改</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d44u7k/runtimeerror_one_of_the_variables_needed_for/</link>
      <description><![CDATA[您好， 我真的需要您的帮助。 我有一个包含四个部分的 DQN 网络：一个共享网络和三个网络分支作为输出。我们的想法是分别更新每个部分的参数（shared_network 和网络分支）。我成功更新了前三个部分，但最后一个部分出现了问题，如下所示： “RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [64, 192]]，它是 AsStridedBackward0 的输出 0，处于版本 2；预期为版本 1。” 您有什么解决方案吗？我会全部尝试。    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d44u7k/runtimeerror_one_of_the_variables_needed_for/</guid>
      <pubDate>Thu, 30 May 2024 14:04:49 GMT</pubDate>
    </item>
    <item>
      <title>棋盘游戏的奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3y08a/reward_function_for_a_board_game/</link>
      <description><![CDATA[大家好，我正在为 2 人棋盘游戏编写一个自玩训练的 deepQ。游戏环境是 7x7，有一些简单的规则。每个回合玩家都应该在 8 个方向之一移动 1 个方块并关闭一个未关闭的方块。目标是让敌人无法移动。 我将两个动作合并为 1 个动作。所以我的动作空间变成了 8*49 = 392，但我很难定义适当的奖励函数，这样代理就不会很糟糕。 你会如何定义这个游戏的奖励函数？    提交人    /u/erenpal01   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3y08a/reward_function_for_a_board_game/</guid>
      <pubDate>Thu, 30 May 2024 07:03:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么 T 是一个固定数字 作者：Sergey Levine</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3v9zv/why_is_t_a_fixed_number_by_sergey_levine/</link>
      <description><![CDATA[我是一名数学系学生，Sergey 的讲座让我感到困惑。在他的讲座中，他声称 T 是一个固定的常数，如果存在平稳分布，则 T 可以是无穷大。但是，我认为状态的值自然取决于时间步长。但他从未在值函数中写下标 t。他总是写 V(s_t)，我相信这意味着 V 不依赖于 t，因为 s_t 在评估时将被实际状态替换。为什么这有意义？ 在我读过的 RL 理论论文中，它几乎总是有限时域时间相关 MDP。事情非常清楚。 在 Sutton 的书中（我猜 Silver 的讲座也隐含地提到了这一点），T 被定义为依赖于实际推出的随机变量。诸如价值函数之类的东西可以通过无限和进行明确定义，如果我们想要有限视界 MDP，\gamma 可以是 1，我们可以假设一个终端状态。有了这个符号，我同意 V 不需要依赖于 t，因为它可以由相应的无限和来定义。    提交人    /u/mziycfh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3v9zv/why_is_t_a_fixed_number_by_sergey_levine/</guid>
      <pubDate>Thu, 30 May 2024 04:05:57 GMT</pubDate>
    </item>
    <item>
      <title>“MLP 在上下文中学习”，Tong & Pehlevan 2024（& 分布式元学习中的 MLP 相变）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3jsqe/mlps_learn_incontext_tong_pehlevan_2024_mlp_phase/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3jsqe/mlps_learn_incontext_tong_pehlevan_2024_mlp_phase/</guid>
      <pubDate>Wed, 29 May 2024 18:53:51 GMT</pubDate>
    </item>
    <item>
      <title>测试我的包裹递送 RL 代理的最佳环境是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3gsxq/what_is_the_best_environment_for_testing_my/</link>
      <description><![CDATA[  由    /u/amirdol7  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3gsxq/what_is_the_best_environment_for_testing_my/</guid>
      <pubDate>Wed, 29 May 2024 16:47:52 GMT</pubDate>
    </item>
    <item>
      <title>帮助设计 Risk DQN Bot</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3fous/help_for_designing_a_risk_dqn_bot/</link>
      <description><![CDATA[嗨， 我是强化学习的新手，熟悉监督学习技术，我只掌握强化学习的基本理论。 我正在构建一个 RISK：全球统治机器人。 目标是使用 DQN 和体育馆框架。 到目前为止，我已经设法在另一个游戏的现有简单环境中创建了一个 DQN 机器人，并使用 tensorflow 对其进行了编码，因为这是我将要使用的框架。 下一步是为 Risk 游戏定义我自己的自定义环境，以及一些有关神经网络的设计决策。  我发现很难找到任何关于如何创建自己的环境的好教程，你们有吗？ 此外，Risk 游戏包含多个游戏阶段，这些阶段没有相同的行动状态：部队部署、攻击和增援。因此，我的网络的输出层形状存在问题。到目前为止，我想出了两个解决方案：  使用 3 个不同的网络 仅使用 1 个形状更大的网络，并使用掩码根据游戏状态删除禁止操作  同样，我没有找到太多资源来更深入地了解该怎么做。 如果你们有什么好的建议那就太好了。谢谢    提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3fous/help_for_designing_a_risk_dqn_bot/</guid>
      <pubDate>Wed, 29 May 2024 16:00:22 GMT</pubDate>
    </item>
    <item>
      <title>MMORPG 特工 - 1v1 PVP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3dvkj/mmorpg_agent_1v1_pvp/</link>
      <description><![CDATA[我一直在学习强化学习，目的是将其融入我的 MMORPG 机器人中。我遇到了一些我没看到太多涉及的问题。假设我想训练一个代理进行 1v1 PVP（玩家对玩家）战斗。正如 MMORPG 中的典型情况一样，不同的角色“构建”是可能的，例如战士或巫师。如果我希望我的机器人能够以任何构建对抗任何构建进行 PVP，我应该怎么做？从理论上讲，我会认为我会用来自各种 PVP 的大量数据来训练代理，这些 PVP 具有不同的角色构建配对。然而，这听起来可能是一项太难的训练任务。或者，我可以想象我会将单个代理“训练为”每种不同的构建类型，与各种不同类型的对手构建进行战斗。然后，根据机器人控制的角色构建，我将为当前角色构建实例化该特定代理策略。我可以想象更进一步，将代理训练为特定构建对抗另一个特定构建。这里的平衡是什么？直观地说，我猜训练一个通用代理会更加昂贵。除了实验之外，还有什么方法可以感受到这种权衡？    提交人    /u/SandSnip3r   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3dvkj/mmorpg_agent_1v1_pvp/</guid>
      <pubDate>Wed, 29 May 2024 14:42:43 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 创建“智能 VWAP”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d3boj6/using_rl_to_create_smart_vwap/</link>
      <description><![CDATA[      我正在撰写关于使用 ML 和 RL 实现最佳交易执行的硕士学位论文。我已经有一些具有基本结果的简单 ML 模型。现在是使用 RL 的时候了。我有 1 只特定股票的 5 分钟日内历史数据。我试图跟随 stocks-v0 中的一些 gym（或添加了自定义指标以供学习的 customenv），但通常结果要么很糟糕（利润 = 0.1），要么我的代理会进行一笔交易，然后整个训练过程最终会停留很长时间。这可能是奖励函数中的问题，但我不知道如何正确设置它。我使用了 PPO、A2C 等。我不指望会有惊人的结果，但如果我获得 0.8-0.9 的总利润，我会没事的。 现在我的主管提出了第二个想法，不要试图制造一个完美的代理 - 尝试创建一个智能 TWAP 或 VWAP。基本上他向我解释的是，我们从每个箱子中获取开盘价，并将其与 TWAP 的平均箱子价格进行比较，在 VWAP 中我们计算平均交易量 - 这意味着每个箱子中的交易量越大，它的“权重”就越大。 所以我想创建本质上是“智能 VWAP”的 RL它将考虑这些权重（或要买入/卖出的股票数量）或添加其他功能。它可能包括交易量或价格，例如价格越差，交易速度就越慢，如果价格好，就应该买入/卖出更多。 我不期望惊人的结果，只是比仅仅检查 VWAP/TWAP 和一些自定义指标并在星星对齐时买入/卖出更好的东西。  如果有人可以指导我完成这个过程，我将不胜感激。 stocks-v0 带有自定义指标 - 1 笔交易后保持多头    提交人    /u/bjbmw3   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d3boj6/using_rl_to_create_smart_vwap/</guid>
      <pubDate>Wed, 29 May 2024 13:03:00 GMT</pubDate>
    </item>
    <item>
      <title>我可以为 A2C-RL 算法创建用于轨迹跟踪的数据驱动模型健身环境吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d367se/can_i_create_a_data_driven_model_gym_environment/</link>
      <description><![CDATA[我在 csv 文件中拥有实时系统的输入输出数据，我可以创建深度神经网络模型健身环境吗？    提交人    /u/Past-News-1373   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d367se/can_i_create_a_data_driven_model_gym_environment/</guid>
      <pubDate>Wed, 29 May 2024 07:15:29 GMT</pubDate>
    </item>
    <item>
      <title>简单的 tensorflow.js 僵尸 ppo 模型帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2ygza/naive_tensorflowjs_zombie_ppo_model_help/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2ygza/naive_tensorflowjs_zombie_ppo_model_help/</guid>
      <pubDate>Tue, 28 May 2024 23:54:50 GMT</pubDate>
    </item>
    <item>
      <title>反事实遗憾最小化（CFR）中的部分修剪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2p41o/partial_pruning_in_counterfactual_regret/</link>
      <description><![CDATA[我最近一直在重新实现一些 CFR 变体，并开始研究修剪。在 MCCFR 论文（Lanctot，2009）中提到 &quot;对于 vanilla CFR，我们使用了一种称为修剪的实现技巧来显着减少每次迭代的工作量。在更新一个玩家的遗憾时，如果另一个玩家没有达到当前历史的概率，则可以为当前迭代修剪该历史的整个子树，而不会对结果计算产生影响。&quot; （这种形式的修剪后来在 Noam Brown 的作品中称为&quot;部分修剪&quot;。） 但是，由于 CFR 在平均迭代中收敛到平衡，我想知道为什么这在我们不交替更新哪个玩家的情况下有效（因为我们仍然需要记录我们玩的策略）。在 pyspiel MCCFR 实现中，除非两个玩家的到达概率均为 0，否则似乎无法完成此操作（第 86 行，https://github.com/google-deepmind/open\_spiel/blob/7d3a355b4927acf4d21fd76e2ea799d9f5c0bb7a/open\_spiel/python/algorithms/external\_sampling\_mccfr.py) 谢谢！    由    /u/dieplstks  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2p41o/partial_pruning_in_counterfactual_regret/</guid>
      <pubDate>Tue, 28 May 2024 17:29:35 GMT</pubDate>
    </item>
    <item>
      <title>LSTM-SAC 代码帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2o539/lstmsac_code_help/</link>
      <description><![CDATA[我一直在尝试使用 lstm 层对 SAC 进行编程，这样在 POMDP 中效果会更好。我见过在 BipedalWalker gym env 上测试的类似内容，所以我目前在那里测试它，因为我在该环境中有一个有效的 SAC 实现。我以为我做得正确，因为完整代码的第一次测试比我预期的要好，但在运行几次之后，似乎这纯属巧合，而且大多数时候它都没有学到任何东西。这是我尝试实现 LSTM 层和序列的方式的问题吗？还是我的超参数的问题？管理隐藏状态重要吗？ 我的超参数与我在正常 SAC 中使用的超参数相同。在执行 LSTM 时，我唯一改变的是确保我的维度是 [batch_size、sequnce_length、任何正常维度]，并且重放缓冲区可以处理序列。最初我的代码是所有代码的 [batch_size,normal dim]。例如，第一个动作的 batch_size 为 1，但对演员/评论家的所有其他调用的 batch_size 都与重播缓冲区中给定的 batch_size 匹配。 我希望最终在硬核双足步行者上使用它，但目前它在正常情况下无法持续工作。并不是说我的正常 sac 曾经学习过硬核，但我认为这是一个超参数问题，因为它们没有经过有目的地调整。任何建议或帮助都将不胜感激。 LSTM 代码：https://github.com/jhunter533/Machine-Learning/blob/main/RSACBiped.py 正常 SAC：https://github.com/jhunter533/Machine-Learning/blob/main/SACBiped.py    提交人    /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2o539/lstmsac_code_help/</guid>
      <pubDate>Tue, 28 May 2024 16:49:04 GMT</pubDate>
    </item>
    <item>
      <title>用于 RL 和 Pytorch 的库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2hhqn/library_to_use_for_rl_and_pytorch/</link>
      <description><![CDATA[大家好，我过去做过很多理论强化学习，从头开始实现了我的大多数算法（PPO、DQl 等），以理解它们并对其进行一些调整。但是，现在我需要强化学习来解决一个实际问题，我不想花太多时间实现每个算法。你会推荐哪些库与 Pytorch 实现良好的协同作用？ 就我个人而言，我对所有 Pytorch 库和子库都有很好的体验，这就是为什么我认为 TorchRL 在这里很有用。但是，由于它还处于起步阶段，资源有点稀缺。所以我不太确定是否有其他拥有更大社区的库在这里更受欢迎。    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2hhqn/library_to_use_for_rl_and_pytorch/</guid>
      <pubDate>Tue, 28 May 2024 11:46:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 学习图的色数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2fuzp/learning_chromatic_number_of_graphs_with_ppo/</link>
      <description><![CDATA[我正在使用 PPO+GNN 解决色数问题，其中代理必须学习用最少的颜色为图形着色。作为输入给出的颜色数量始终大于色数。假设图形的色数为 3，我们为代理提供了 10 种颜色，并希望它用 3 种颜色为图形着色。这就是正在发生的事情，它在很短的几步内学会了用 5 种颜色为图形着色，但它并没有从那里收敛到最佳解决方案。我该怎么做才能让它收敛到最佳解决方案？附注：最初参与者损失很高，一旦它收敛到次优解（5），它就会变得非常少。此外，该算法为 MIS 问题提供了最佳解决方案和良好的收敛性。    提交人    /u/Low-Advertising-1892   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2fuzp/learning_chromatic_number_of_graphs_with_ppo/</guid>
      <pubDate>Tue, 28 May 2024 10:04:14 GMT</pubDate>
    </item>
    </channel>
</rss>