<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 21 Jan 2025 09:17:35 GMT</lastBuildDate>
    <item>
      <title>深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6ehaj/deep_reinforcement_learning/</link>
      <description><![CDATA[我有两本书  Richard S. Sutton 和 Andrew G. Barto 的《强化学习》  Miguel Morales 的《深度强化学习》 我发现两本书的内容表格都差不多。我即将自学 DQN、Actor Critic 和 PPO，但很难确定书中的重要主题。第一本书看起来更侧重于表格方法（？），对吗？  第二本书有几个章节和子章节，但我需要有人帮助指出里面的重要主题。我是一名普通软件工程师，在业余时间很难逐一消化所有概念。  有人可以帮忙指出哪个子主题很重要吗？如果我的想法正确，第一本书更侧重于表格方法？    由    /u/Best_Fish_2941  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6ehaj/deep_reinforcement_learning/</guid>
      <pubDate>Tue, 21 Jan 2025 08:56:48 GMT</pubDate>
    </item>
    <item>
      <title>“推理者的问题：祈求迁移学习”，艾丹·麦克劳克林（更多 RL 会修复 o1 风格的 LLM 吗？）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i65y2f/the_problem_with_reasoners_praying_for_transfer/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i65y2f/the_problem_with_reasoners_praying_for_transfer/</guid>
      <pubDate>Tue, 21 Jan 2025 00:35:02 GMT</pubDate>
    </item>
    <item>
      <title>高维连续动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i606pp/high_dimensional_continous_action_spaces/</link>
      <description><![CDATA[考虑实施 DDPG，但我可能需要多达 96 个动作输出，因此动作空间为 R ^ 96。我正在尝试优化 8 个形式为 I(t) 的函数，I: R -&gt; R，以达到某些基准。我考虑这样做的方法是将输入空间离散化为块，因此如果每个输入有 12 个块，我需要有 12 * 8 = 96 个实数输出。这是否合理可行？    提交人    /u/MilkyJuggernuts   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i606pp/high_dimensional_continous_action_spaces/</guid>
      <pubDate>Mon, 20 Jan 2025 20:30:00 GMT</pubDate>
    </item>
    <item>
      <title>面对工业工厂中任务分配优化的 DQN 性能不佳的问题，有什么帮助吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5rcvu/facing_poor_performance_with_dqn_for_mission/</link>
      <description><![CDATA[      嗨！我是 RL 的初学者，我一直在学习 dqn 并致力于使用它来优化工业工厂中的任务分配。 我们有几个机器人（AGV）和任务。每个任务都有一系列要遵循的步骤。例如，任务 1 的步骤 1 可能需要从标签 1 移动到标签 2，这意味着我们需要阻止其他机器人访问这两个标签以避免碰撞。机器人必须访问的步骤顺序是预定义的。我将状态构建为一个列表，其中包括： - 空闲机器人， - 当前正在执行任务的机器人， - 停止服务的机器人， - 正在充电的机器人， - 未请求的任务， - 已请求的任务， - 正在进行的任务， - 标签可用性， - 机器人位置， - 每个机器人的任务步骤（默认为 1）， - 所有机器人的电池电量。 例如，如果有 4 个机器人和 4 个任务，状态可能如下所示： [[0, 1, 1, 1], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [1, 0, 0, 0], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2], [1, 1, 1, 1], [0.71, 0.34, 0.6, 0.4]] 动作以对的形式表示，如 (&#39;1&#39;, &#39;4&#39;)，表示“将任务 4 分配给机器人1&quot; 如果某个动作被认为不可行（例如，机器人已经很忙或任务正在进行中），它​​会触发当前情节的终止条件。步骤如下：  惩罚应用：分配 -80 的惩罚以阻止不可行的操作。 状态处理：下一个状态与当前状态保持相同，因为未执行任何有效操作。 经验存储：元组（当前状态、所选操作的索引、惩罚、下一个状态）被添加到重放缓冲区，允许代理从错误中吸取教训。 情节终止：当前情节的循环结束，系统继续下一个情节。  奖励函数：  电池电量（10%）：奖励当前执行任务的机器人和选定的机器人更高的平均电池电量。 接近任务（20%）：奖励执行任务的机器人和选定的机器人更短的距离，以减少行进时间。 任务持续时间（70%）：优先考虑较短的完成时间以提高效率。 最终状态奖励：如果完成所有任务，则增加最小化完工时间的奖励。  然后将元组（状态、所选动作的索引、奖励、下一个状态）添加到缓冲区。 尽管测试了不同的激活函数和参数，但模型表现不佳。结果是“随机的”或者预测的动作是重复的（对我测试的每个随机状态都得到相同的预测） https://preview.redd.it/e260toyssaee1.png?width=1197&amp;format=png&amp;auto=webp&amp;s=4357c60792c99cba432da5ca57ddab2e7d1df413 我不确定是什么原因导致的这种情况或如何改进它，有什么想法吗:&#39;)？。如果对我的实施有任何不清楚的地方，请告诉我！    提交人    /u/Dazzling-Prize3371   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5rcvu/facing_poor_performance_with_dqn_for_mission/</guid>
      <pubDate>Mon, 20 Jan 2025 14:29:26 GMT</pubDate>
    </item>
    <item>
      <title>Pong 的策略梯度代理没有学习（帮助）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5q8kp/policy_gradient_agent_for_pong_is_not_learning/</link>
      <description><![CDATA[嗨，我是强化学习的新手，正在尝试使用策略梯度法训练我的代理玩 Pong。我参考了深度强化学习：从像素开始玩 Pong。和使用 Cartpole 和 PyTorch 的策略梯度。因为我想学习 Pytorch，所以我决定使用它，但我的实现似乎缺少一些东西。我尝试了很多东西，但它所做的只是学习一次反弹然后停止（之后它什么也不做）。我认为问题出在我的损失计算上，所以我尝试改进它，但它仍然重复相同的过程。 这是 git：使用 pytorch 的 Pong 的 RL    提交人    /u/nightsy-owl   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5q8kp/policy_gradient_agent_for_pong_is_not_learning/</guid>
      <pubDate>Mon, 20 Jan 2025 13:33:42 GMT</pubDate>
    </item>
    <item>
      <title>从哪里开始</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5bnne/where_to_begin/</link>
      <description><![CDATA[我最近有个想法，教一个学习模型玩一款名为蜜蜂群模拟器的游戏，这只是一个副项目。 我对 Python 了解甚少，但我对如何做这样的事情一无所知。我希望能够因为做正确的事情而获得奖励，但除此之外，我不知道我需要什么模型、什么脚本或任何东西。 如果你知道或见过类似的东西，请分享，否则如果你能告诉我从哪里开始学习，那就太好了，谢谢。    提交人    /u/Decreasify   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5bnne/where_to_begin/</guid>
      <pubDate>Sun, 19 Jan 2025 22:56:22 GMT</pubDate>
    </item>
    <item>
      <title>分类 DQN 对于确定性完全观察环境是否有用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5a0n5/is_categorical_dqn_useful_for_deterministic_fully/</link>
      <description><![CDATA[...喜欢 Cartpole？这个 Rainbow DQN 教程 使用了 Cartpole 示例，但我想知道“彩虹”的分类部分在这里是否有点过头了，因为 Q 值应该是一个明确定义的值，而不是统计分布，既缺乏随机性又缺乏部分可观测性。    提交人    /u/exploring_stuff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5a0n5/is_categorical_dqn_useful_for_deterministic_fully/</guid>
      <pubDate>Sun, 19 Jan 2025 21:45:33 GMT</pubDate>
    </item>
    <item>
      <title>关于策略迭代的紧急问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i58mt0/urgent_question_about_policy_iteration/</link>
      <description><![CDATA[你好，我明天有期末考试，我对策略迭代有结果。它有两个步骤：评估和改进。例如，我处于状态 s0，我的操作是 a。奖励 s0=0，奖励 s1=-0.04。采取行动后，我的状态将为 s1。在评估步骤中，我必须在计算 v(s0) 时计算 r(s0)，在改进步骤中，奖励是 -0.04，或者我对两个步骤都使用 -0.04。    提交人    /u/dodohasmala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i58mt0/urgent_question_about_policy_iteration/</guid>
      <pubDate>Sun, 19 Jan 2025 20:49:01 GMT</pubDate>
    </item>
    <item>
      <title>受限 3D 环境中使用 RL 进行多无人机目标分配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i588u3/multidrone_goal_allocation_with_rl_in_a/</link>
      <description><![CDATA[我正在研究一个具有挑战性的问题，涉及3D 环境中无人机的多智能体协调。具体来说：  场景： 有 20 架无人机必须共同访问 3D 地图上的所有目标点。 无人机从任意目标点出发（不一定是同一个目标点）。 目标是最小化访问所有目标点所需的总时间。  过程： 该过程分为&quot;轮&quot;： 在每一轮中，无人机选择要移动到的新目标点。 无人机前往其选定的目标点。一旦所有无人机到达目的地，它们就会同时进行测量（不提前开始）。 测量完成后，下一轮开始。   限制： 每架无人机的电池容量有限。 共有五个充电站，可放置在任何目标点。每个站点可以同时为无限数量的无人机提供服务，但充电需要时间。  目标： 尽量减少所有无人机集体访问所有目标点所需的总时间。   问题框架和挑战 我相信这是每轮最小-最大多旅行者推销员问题 (mTSP)的变体，带有电池限制和充电等额外限制。虽然用于成对距离的 Floyd-Warshall 和混合整数规划 (MIP) 等传统方法可以解决这个问题，但我想探索强化学习 (RL) 作为解决方案。但是，我正在努力应对几个挑战：  初始状态多变性：与许多无人机从单个仓库出发的 mTSP 方案不同，我的无人机从任意初始目标点出发。这引入了多种初始状态。  RL 如何处理这种多变性？ 即使我考虑从所有可能的初始状态的均匀概率开始，任何单一状态的概率都非常小，这可能会使学习效率低下。  动作空间大小：在每一轮中，每架无人机必须从所有剩余未访问点中选择一个目标点，从而产生巨大的动作空间（剩余点选择 20​​）。这种高维动作空间使 RL 难以有效地探索或学习最佳策略。  在此类问题中，是否存在有效的动作空间减少或分层 RL技术？  多智能体协调：由于这是一个多智能体设置，因此可能需要多智能体强化学习 (MARL)。但是，我对 MARL 框架或协作动态问题的最佳实践不太熟悉。  征求建议 我正在寻找以下方面的见解或指导：  多智能体强化学习 (MARL) 是否是解决此问题的正确方法？  如果是，是否有适合我的问题的规模和约束的特定框架、算法或策略（例如 QMIX、MADDPG 或其他）？  我如何有效地处理： 无人机的不同初始状态？ 每一轮的大动作空间？  是否有参考文献、研究论文或案例研究涉及多智能体 RL 进行动态目标分配或无人机协调问题？     提交人    /u/Frankie114514   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i588u3/multidrone_goal_allocation_with_rl_in_a/</guid>
      <pubDate>Sun, 19 Jan 2025 20:32:57 GMT</pubDate>
    </item>
    <item>
      <title>偏差和方差：萨顿痛苦教训的重演</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i56xkh/bias_and_variance_a_redux_of_suttons_bitter_lesson/</link>
      <description><![CDATA[原始形式 20 世纪 90 年代，计算机开始在国际象棋比赛中击败人类大师。许多人研究了这些国际象棋代理所使用的技术，并谴责道：“这只是机械地死记硬背所有动作。这不是真正的智能！”  旨在模仿人类认知某些方面的手工算法将始终赋予 AI 系统更高的性能。而且这种性能提升将是暂时的。随着更强大的计算能力的涌入，从长远来看，依赖“无意识”深度搜索或大量数据（CONV 网络）的算法将胜过它们。  Richard Sutton 将此描述为一个惨痛教训，因为他声称，过去 70 年的 AI 研究就是对此的证明。  统计形式 2022 年夏天，牛津大学和伦敦大学学院的研究人员发表了一篇足以包含章节的论文。这是一项关于因果机器学习的调查。第 7 章涵盖了因果强化学习的主题。在那里，Jean Kaddour 和其他人提到了 Sutton 的苦涩教训，但它以新的眼光出现——通过统计和概率的观点进行反映和过滤。   我们将两个社区关注点不同的原因归因于各自处理的应用类型。绝大多数关于现代 RL 的文献评估了能够生成大量数据的合成数据模拟器上的方法。例如，流行的 AlphaZero 算法假设可以访问棋盘游戏模拟，允许代理在不受数据量限制的情况下玩许多游戏。它的一项重要创新是 tabula rasa 算法，它具有更少的手工制作知识和特定领域的数据增强。有些人可能会认为 AlphaZero 证明了 Sutton 的惨痛教训。从统计学的角度来看，它大致表明，在给定更多计算和训练数据的情况下，具有低偏差和高方差的通用算法优于具有高偏差和低方差的方法。  你会说这反映在你自己的研究吗？在实践中，具有低偏差和高方差的算法是否优于高偏差低方差算法？ 你的想法？    http://www.incompleteideas.net/IncIdeas/BitterLesson.html  https://arxiv.org/abs/2206.15475     提交人    /u/moschles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i56xkh/bias_and_variance_a_redux_of_suttons_bitter_lesson/</guid>
      <pubDate>Sun, 19 Jan 2025 19:38:55 GMT</pubDate>
    </item>
    <item>
      <title>Carla自动驾驶收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4ydt0/carla_autonomous_driving_convergence_problem/</link>
      <description><![CDATA[我遇到了一个问题，我的自动驾驶代理无法收敛，我无法确定确切的原因。我想问问是否有人有时间和兴趣帮助我分析可能导致问题的原因。由于我使用的是 Stable-Baselines3，因此不太可能是 RL 算法本身的问题，因此它可能与超参数或奖励有关。 如果有人感兴趣，请随时对这篇文章发表评论，我会分享我的 Discord 以进一步讨论。    提交人    /u/Fair_Device_4961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4ydt0/carla_autonomous_driving_convergence_problem/</guid>
      <pubDate>Sun, 19 Jan 2025 13:26:17 GMT</pubDate>
    </item>
    <item>
      <title>教 AI 汽车驾驶：Unity ML-Agents 模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4vsq3/teaching_ai_cars_to_drive_a_unity_mlagents/</link>
      <description><![CDATA[https://reddit.com/link/1i4vsq3/video/ckatil9dgxde1/player 大家好！我一直在 Unity 中研究 AI 模拟，其中使用 ML-Agents 和强化学习训练汽车在红灯时停止、在绿灯时行驶以及在路口导航。 完整视频链接 - https://www.youtube.com/watch?v=rkrcTk5bTJA 在过去的 8-10 天里，我付出了很多努力来训练这些汽车，虽然结果还不完美，但看到它们的进步还是很令人兴奋的！ 我计划探索更复杂的场景，例如汽车处理多车道交通、在环形交叉路口导航以及对动态障碍物做出反应。我还打算与其他对 AI 模拟感兴趣的人合作，并最终在 GitHub 上分享这些实验的代码。 我在 YouTube 上发布了这个模拟的视频，我很乐意听到您的反馈或建议。如果您有兴趣看到更多此类项目，请考虑通过订阅频道来支持！ 谢谢    提交人    /u/FiredNeuron97   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4vsq3/teaching_ai_cars_to_drive_a_unity_mlagents/</guid>
      <pubDate>Sun, 19 Jan 2025 10:40:46 GMT</pubDate>
    </item>
    <item>
      <title>RL 中的优化？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4s3rt/optimization_within_rl/</link>
      <description><![CDATA[我想通过调整边界约束将 RL 应用于受约束的线性规划。LP 的形式为：最大 c&#39;v，受 Ax=0 约束，x &lt; xub。因此，我希望我的代理对 xub（连续）的元素采取行动。我将使用 x 的一些预测值通过欧拉前向方法更新环境。奖励将是每个时间步的函数值，以及该情节的一些折扣值。这可能吗？我可以为每个时间步求解 LP 吗？SAC 方法在这里有效吗？非常感谢您的指导！    提交人    /u/Tako_Poke   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4s3rt/optimization_within_rl/</guid>
      <pubDate>Sun, 19 Jan 2025 06:15:06 GMT</pubDate>
    </item>
    <item>
      <title>不清楚何时使用 RL 还是数学优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4q30v/confused_about_when_to_use_rl_vs_mathematical/</link>
      <description><![CDATA[你好 我是 RL 的新手。 我们优化的问题是库存管理和车间作业调度。 我理解 RL 可以考虑更多动态方面，并且可以在未来进行调整。但我无法将其转化为实际术语 MO 技术何时会失效？ 建模时，如何在 MO 技术和 RL 之间做出选择？ 谢谢。    提交人    /u/20231027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4q30v/confused_about_when_to_use_rl_vs_mathematical/</guid>
      <pubDate>Sun, 19 Jan 2025 04:18:45 GMT</pubDate>
    </item>
    <item>
      <title>图式网络：利用直觉物理的生成因果模型进行零样本迁移</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4m0xh/schema_networks_zeroshot_transfer_with_a/</link>
      <description><![CDATA[  由    /u/moschles  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4m0xh/schema_networks_zeroshot_transfer_with_a/</guid>
      <pubDate>Sun, 19 Jan 2025 00:46:48 GMT</pubDate>
    </item>
    </channel>
</rss>