<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 09 Apr 2024 12:25:10 GMT</lastBuildDate>
    <item>
      <title>IMPALA 实施与不断上升的批评损失：需要洞察！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzqheu/impala_implementation_with_rising_critic_loss/</link>
      <description><![CDATA[      我刚刚完成了一个 IMPALA 实施项目，在该项目中我通过套接字通信训练了多个参与者。这是分布式强化学习的一段令人着迷的旅程，但我遇到了一个障碍，我希望你能帮忙解决。 尽管演员表现出有希望的性能改进，但我看到了持续且令人不安的问题趋势：批评者损失不仅不稳定；它实际上随着时间的推移而增加。这很令人费解，因为随着训练的进行，你会期望更好的演员表现来稳定甚至减少批评者损失，对吗？ 另外，请继续关注😊我计划进一步探索分布式强化学习算法，并将与大家分享我的旅程。您今天的见解可能是该探索的重要组成部分！ 提前感谢您的想法和建议！ ​ https://github.com/seolhokim/SimpleDistributedRL ​ &lt; a href=&quot;https://preview.redd.it/9f5nlewf2gtc1.png?width=814&amp;format=png&amp;auto=webp&amp;s=81de8398d0b4175f6d6bdccf4b2fe1b8c95749e6&quot;&gt;https://preview.redd.it/9f5nlewf2gtc1.png?width =814&amp;format=png&amp;auto=webp&amp;s=81de8398d0b4175f6d6bdccf4b2fe1b8c95749e6 ​   由   提交 /u/Spiritual_Fig3632   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzqheu/impala_implementation_with_rising_critic_loss/</guid>
      <pubDate>Tue, 09 Apr 2024 12:06:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q-learning 为健身房中的 MountainCar 提供奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzm0qp/reward_function_for_mountaincar_in_gym_using/</link>
      <description><![CDATA[大家好，我一直在尝试使用 Qlearning 训练代理来解决健身房中的 MountainCar 问题，但无法让我的代理到达旗帜。当我使用返回的默认奖励时，它永远不会到达标志（每一步为 -1，到达标志时为 0），我让它运行 200,000 集，但无法到达那里。所以，我尝试编写自己的奖励函数，我尝试了一堆 - 距离旗帜越近，奖励呈指数级越高，旗帜处的奖励越大，奖励ABS（加速度）和顶部的奖励越大，等等。但我只是无法让我的代理一路到达顶部 - 其中一个功能非常接近，就像非常接近，但随后决定全力深入潜水（可能是因为我奖励加速，但我设置了一个标志仅在第一次向左移动时奖励加速，但我的代理仍然决定向下潜）。我不明白，有人可以建议我应该如何解决它吗？ 我不知道我做错了什么，因为我在网上看到了教程，而且代理也在那里仅使用默认奖励就非常快（&lt;4000 集），我不知道为什么即使使用相同的参数也无法复制它。我非常感谢任何帮助和建议。 这是 github 链接 如果有人想看一下代码。 《Q-learning-山车》是应该工作的代码，与发布的 OpenAI 示例非常相似，但经过修改以在gym 0.26上工作； copy 和 new 是我一直在尝试奖励功能的地方。 非常感谢任何意见、指导或建议。提前致谢。 编辑：在评论中解决。如果有人来自未来并且面临与我相同的问题，解决的代码将上传到上面链接的 github 存储库。   由   提交/u/guccicupcake69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzm0qp/reward_function_for_mountaincar_in_gym_using/</guid>
      <pubDate>Tue, 09 Apr 2024 07:14:53 GMT</pubDate>
    </item>
    <item>
      <title>调试 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byzydz/debugging_a_ppo/</link>
      <description><![CDATA[大家好， 我认为我已经完成了模拟和 ppo 代理中的所有部分的组合。我现在的问题是代理不学习。我尝试对其进行大量训练，但仍然没有任何改进（主要是查看演员和评论家的损失，然后测试模型）。 您对如何调试有什么建议吗？它？我应该注意什么来检查一切是否正常工作以及如何检查？ 任何提示都会有所帮助，谢谢:) 编辑： 这是我的 ppo 政策 https://pastebin.com/zY3AgeSF （我不像大多数论文那样使用 gaes，因为我正在执行 on-policy 并且还没有找到正确的方法，无需输入整个序列）。我的主要代码真的很混乱，因为我尝试了很多不同的东西，而且我不相信它写得很好。我更喜欢一些关于如何在模型似乎没有学习时进行调试的一般提示，如何检查它是否是超参数或代码中的其他内容不正确。还不够，如果你们认为有帮助，我将发布我的所有代码。   由   提交/u/razton  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byzydz/debugging_a_ppo/</guid>
      <pubDate>Mon, 08 Apr 2024 14:56:26 GMT</pubDate>
    </item>
    <item>
      <title>使用离线 rl 进行 PPO 预训练？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byzfha/ppo_pre_training_using_offline_rl/</link>
      <description><![CDATA[有没有离线训练ppo然后在线finetune的文章？或者也许可以离线学习 Actor Critic，然后在线使用 Ppo 进行调优？ 谢谢   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byzfha/ppo_pre_training_using_offline_rl/</guid>
      <pubDate>Mon, 08 Apr 2024 14:34:49 GMT</pubDate>
    </item>
    <item>
      <title>如何处理 Q-learning 中的连续（非情景）任务？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byvrfn/how_to_handle_continuing_nonepisodic_tasks_in/</link>
      <description><![CDATA[我想实现作业车间调度优化模拟（将多个制造任务分配给有限数量的可用机器）。这是一个永无止境的持续过程。 我使用 Q-learning 以及重播缓冲区和 epsilon-greedy 来进行探索/利用。通常我会训练 N 集，但由于没有集，我不知道如何继续。  我的想法： 引入辅助终止状态，使其再次转变为情景任务。我认为这对我的情况有用。例如，我可以说该情节在处理完 N 个任务后结束。这有意义吗？或者有更好的方法来处理这个问题吗？   由   提交/u/NMO13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byvrfn/how_to_handle_continuing_nonepisodic_tasks_in/</guid>
      <pubDate>Mon, 08 Apr 2024 11:43:59 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么进化算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byunu0/what_evolutionary_algorithm_should_i_use/</link>
      <description><![CDATA[我正在制作一个专注于编程的 Youtube 频道，用我自己的语言 Spiral，我正在实现一个 机器学习库。我直接在 GPU 上进行奇特的函数式编程，我的目标是将扑克游戏与 ML 库融合，并使用它来训练超人代理，所有这些都在单个 GPU 上进行，并且无需接触 CPU。这项工作的目的与我今天刚刚发现的基于 JAX 的库非常相似，希望它存在 3 年前。当时 Spiral v2 刚刚发布，我使用 PyTorch 将其用于很多 RL。当时该语言有 Cython 后端，我做出了使用它的错误选择。当时我做了一个扑克游戏，并尝试使用各种 RL 方法来训练 RL 智能体，其中许多方法都是我自己编造的，我的经验非常糟糕，所以我停止了编程两年，转而做了 3d 艺术。 现在我回来了，想要做正确的事情。 我已经脱离 ML 有一段时间了，我想知道无梯度方法的最新技术水平。我从来没有真正认真对待过 evo 算法，因为我认为我可以通过深度强化学习找到一些东西，但现在我将采取不同的方法并专注于基础知识。您建议我做什么？ 我决定不使用 OpenAI ES，因为它对奖励缩放很敏感，但除此之外没有特别的偏好。   由   提交 /u/abstractcontrol   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byunu0/what_evolutionary_algorithm_should_i_use/</guid>
      <pubDate>Mon, 08 Apr 2024 10:39:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 DDQN 解决旅行商问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byohkp/using_a_ddqn_for_the_travelling_salesman_problem/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byohkp/using_a_ddqn_for_the_travelling_salesman_problem/</guid>
      <pubDate>Mon, 08 Apr 2024 04:03:01 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗树搜索不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byifna/monte_carlo_tree_search_not_converging/</link>
      <description><![CDATA[我正在尝试实现 MCTS，但在测试中它无法收敛到 minmax，并且有一些我不明白的内容： &lt; p&gt;假设我的根节点有两个可能的动作，第一个分支分为 10 个其他动作，其中一个值为 0.6（获胜机会），其他 9 个值为 0，第二个分支只是一个值为 0.5 的单个动作 因此，在 minmax 中，第一个分支更好，因为在下一步中您可以采取 0.6 操作。但在 MCTS 中，第一个分支的值是好路径获胜的平均值，也是所有其他坏路径的平均值 0.6+0+0+0+0+0+0+0+0+0 / 10 父节点的值不应该是最佳子节点的值吗？ 但是在反向传播过程中，该值会被所有具有 0 的路径稀释 这就是我在模拟游戏中得到的结果。在此示例中，应该更多地播放 0.6 路径，以最终使低值路径变得无关紧要，但也许由于探索术语，不良路径永远不会减少到足以使 0.6 胜过 0.5 我错过了什么吗？ 谢谢。   由   提交 /u/SSCharles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byifna/monte_carlo_tree_search_not_converging/</guid>
      <pubDate>Sun, 07 Apr 2024 23:11:52 GMT</pubDate>
    </item>
    <item>
      <title>训练 DQN 来解决玩具 MARL 问题有多困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1by4xtx/how_difficult_is_it_to_train_dqns_for_toy_marl/</link>
      <description><![CDATA[我一直在尝试训练井字棋 DQN，但到目前为止还无法让它们学习最佳策略。 我正在使用 pettingzoo env（因此没有图像或 CNN），并并行训练两个代理，彼此独立，这样每个代理都有自己的重播缓冲区，一个始终作为第一个播放，另一个作为第一个播放第二。 我尝试训练它们数十万步，并且通常会到达它们（似乎？）收敛到纳什均衡的点，游戏以平局结束。除了当我尝试让他们中的任何一个与随机对手对抗时，他们仍然会输掉大约 10% 的时间，这意味着他们还没有学会最佳策略。 我认为发生这种情况是因为他们没有&#39;我无法充分探索游戏空间，我不确定为什么情况并非如此。我使用 softmax 采样从高温开始并在训练过程中逐渐减少，所以他们肯定应该做一些探索。我已经尝试过学习率和网络架构，只做出了最小的改进。 我想我可以更深入地研究超参数优化并训练更长时间，但这对于这样一个简单的玩具问题来说听起来有点矫枉过正。如果我想训练他们玩一些更复杂的游戏，我是否需要更多的资源？或者，例如，选择 PPO 是否更明智？ 无论如何，咆哮已经足够了，我想问一下训练 DQN 是否真的那么困难马尔。如果您可以分享任何适用于 Tic Tac Toe 的一组超参数的实验，出于好奇，我们将非常欢迎您。   由   提交 /u/OperaRotas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1by4xtx/how_difficult_is_it_to_train_dqns_for_toy_marl/</guid>
      <pubDate>Sun, 07 Apr 2024 13:49:06 GMT</pubDate>
    </item>
    <item>
      <title>A3C 评论家估计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxxhvv/a3c_critic_estimate/</link>
      <description><![CDATA[      我正在使用异步优势演员评论家模型，该模型似乎运行得相当好，但我注意到，随着时间的推移，我的评论家“估计”变得越来越差。和“实际值”随着时间的推移会爆炸成负面的。本质上，我认为这是一个引导问题，其中批评者损失如下： loss = torch.nn.MSELoss()(reward + gamma * value_next , value) 环境非常嘈杂，但偏向于负面奖励 - 所以我推测正在发生的事情是批评家会定期猜测得太积极，学会变得更加消极，所发生的既是初始状态的价值预测，也是随着时间的推移，下一个状态（都来自批评者）往往会变得越来越消极。但由于随着时间的推移，两者都会以相同的速度变得更加负面，批评者本身的损失计算并不会变得非常扭曲。  然而，“真实”的情况是这样的。该指标的值实际上应该落在 [+5,-5] 之间，因此随着时间的推移，我担心这有可能开始向参与者发出奇怪的信号以进行其优势更新。  我在这里做错了什么教科书吗？您可以推荐一些好的修复方法吗？  ​ ​ （颜色代表不同的评论代理） ​ ​ ​ ​ ​ ​    由   提交 /u/Rhyno_Time   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxxhvv/a3c_critic_estimate/</guid>
      <pubDate>Sun, 07 Apr 2024 06:22:52 GMT</pubDate>
    </item>
    <item>
      <title>需要一些关于在医学成像重建背景下使用强化学习的想法的反馈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxw4ci/need_some_feedback_on_an_idea_for_using/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxw4ci/need_some_feedback_on_an_idea_for_using/</guid>
      <pubDate>Sun, 07 Apr 2024 05:00:24 GMT</pubDate>
    </item>
    <item>
      <title>深Q网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxtca2/deep_q_network/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxtca2/deep_q_network/</guid>
      <pubDate>Sun, 07 Apr 2024 02:31:43 GMT</pubDate>
    </item>
    <item>
      <title>谁首先通过将指示函数写成马尔可夫噪声来证明异步 Q 学习的收敛性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxokh3/who_first_proved_the_convergence_of_asynchronous/</link>
      <description><![CDATA[大家好！ 到目前为止，我们已经有了几个依赖随机逼近理论的异步 Q-learning 收敛证明并进行了更新：  $Q_k+1(s,a) &lt;- Q_k(s,a) + I[s_k=s, a_k=a] \alpha (Y_k)$ I我正在考虑证明，特别注意到 I[s_k=s, a_k=a] 是一个马尔可夫过程（因为过程 (s,a) 是），其过渡函数依赖于 $Q$ 的当前值，并得出收敛结果来自带有马尔可夫噪声的随机过程。 有谁知道谁第一个引入了这种方法？ 非常感谢！   由   提交/u/ttlizon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxokh3/who_first_proved_the_convergence_of_asynchronous/</guid>
      <pubDate>Sat, 06 Apr 2024 22:46:39 GMT</pubDate>
    </item>
    <item>
      <title>使用同一代理并行训练具有不同数据的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxh59o/training_environments_with_different_data_in/</link>
      <description><![CDATA[我正在考虑训练几个自定义环境，但每个环境都有一批数据，大约是 2k 个 JSON 文件，每个数据代表一个剧集和环境从数据库获取这些文件。我的想法是使用包含在 SubprocVecEnv 中的 150 个环境实例来训练 SB3 DQN 模型，该模型似乎有效，但我面临两个问题：  速度训练似乎相当低，我无法弄清楚瓶颈是什么（在 4 个 Nvidia A100 GPU 上训练）我尝试增加环境数量，从而减少每个环境的文件数量，但这并没有&#39; t 改变速度  我不太确定单个代理是否会从所有 150 个实例的所有经验中学习，我想保证每个实例在训练完成之前完成所有文件  希望得到任何帮助或想法，谢谢！   由   提交 /u/CandidateIcy4911   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxh59o/training_environments_with_different_data_in/</guid>
      <pubDate>Sat, 06 Apr 2024 17:31:03 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>