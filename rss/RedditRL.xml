<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 12 Apr 2025 09:18:59 GMT</lastBuildDate>
    <item>
      <title>是否有像Pytorch Lightning这样的框架？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jxbp6l/are_there_frameworks_like_pytorch_lightning_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我认为Pytorch Lightning是改善灵活性，生殖能力和可读性的好框架，在处理更多复杂的监督学习项目时。我看到了代码示例   ，它表明可以使用drl，但它感觉很有可能，但它有点像材料，因为我发现了一个非常亮的范围。而不是“面向环境交往”。使用诸如体育馆之类的图书馆或使用闪电的正确方法。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/capelettin     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jxbp6l/are_there_frameworks_like_pytorch_lightning_for/</guid>
      <pubDate>Sat, 12 Apr 2025 07:18:09 GMT</pubDate>
    </item>
    <item>
      <title>[MBRL]为什么即使在Dreamerv3中的世界模型融合之后，政策绩效也会波动？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jxa7tn/mbrl_why_does_policy_performance_fluctuate_even/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 我目前正在与Dreamerv3合作，包括多个控制任务，包括DeepMind Control Suite的Walker_walk。 I&#39;ve noticed something interesting that I&#39;m hoping the community might have insights on. **Issue**: Even after both my world model and policy seem to have converged (based on their respective training losses), I still see fluctuations in the episode scores during policy learning. I understand that DreamerV3 follows the DYNA scheme (from Sutton&#39;s DYNA paper), where the world model and policy are trained in parallel.我的期望是，一旦世界模型融合到环境的准确表示，政策绩效就应该稳定。 是否有人使用Dreamerv3或其他MBRL算法经历了这一点？我很好奇这是否是：   MBRL系统中的预期行为？   我实施的迹象表明我的实施出了问题？                    dyna-style方法的基本限制？减少这种差异或解释为什么发生的任何提示将不胜感激！提交由＆＃32; /u/u/udanciprativecar545     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jxa7tn/mbrl_why_does_policy_performance_fluctuate_even/</guid>
      <pubDate>Sat, 12 Apr 2025 05:36:01 GMT</pubDate>
    </item>
    <item>
      <title>K-Subset选择的政策梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jx3czf/policy_gradient_for_ksubset_selection/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  假设我有一组n个项目，以及将每个k子设置映射到真实数字的奖励功能。 这些项目在每个“状态/上下文”中都会更改（这实际上是强盗问题）。目标是以状态为条件的政策，最大程度地提高了对其选择的子集的奖励，平均在所有状态下。 我很乐意为算法提出建议，但这是深度学习管道中的一个子问题，因此需要成为一些可不同的东西（没有启发式/进化算法）。专门加强。然后，问题变成了如何将k-Subset选择的策略参数化。任何子集都很容易，每个项目的概率都有bernoulli。是否有人遇到概括将Bernoulli样本限制为大小为K的子集？重要的是，我必须获得选择的动作/子集的准确概率 - 并且它不会太复杂（Gumbel Top -K不在列表中）。 编辑：为了清楚起见，问题本质上是策略输出的内容。我们如何进行采样并学习最佳的k-subset来选择！ 谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1jx3czf/policy_gradient_for_ksubset_selection/”&gt; [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jx3czf/policy_gradient_for_ksubset_selection/</guid>
      <pubDate>Fri, 11 Apr 2025 23:11:00 GMT</pubDate>
    </item>
    <item>
      <title>RL当前知道超人性能的唯一方法吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jx3by6/is_rl_the_currently_know_only_way_to_have/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否还有其他ML方法，我们可以通过这些方法获得第100个百分点的非平凡任务？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/wherpossible1414      [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jx3by6/is_rl_the_currently_know_only_way_to_have/</guid>
      <pubDate>Fri, 11 Apr 2025 23:09:37 GMT</pubDate>
    </item>
    <item>
      <title>对机器人技术的增强学习非常酷！ （对博士机器人学生的采访）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jx2rcd/reinforcement_learning_for_robotics_is_super_cool/</link>
      <description><![CDATA[    我当然学到了很多关于RL的机器人能力的知识，并被这次对话启发了。 href =“ https://youtu.be/39nb43ylas0?si=_dfxyq-tvztbsu9r”&gt; https://youtu.be/39nb43ylas0?si=_si = _si = _dfxyq-dfxyq-tvztbsu9r提交由＆＃32; /u/okthought8642      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/comments/1jx2rcd/reinforecction_learning_for_robotics_is_super_cool/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jx2rcd/reinforcement_learning_for_robotics_is_super_cool/</guid>
      <pubDate>Fri, 11 Apr 2025 22:42:14 GMT</pubDate>
    </item>
    <item>
      <title>企业量子AI通用情报完整的开源版本 - 具有自适应LR修复和量子同步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jx23is/corporate_quantum_ai_general_intelligence_full/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jx23is/corporate_quantum_ai_general_intelligence_full/</guid>
      <pubDate>Fri, 11 Apr 2025 22:11:45 GMT</pubDate>
    </item>
    <item>
      <title>RL会有未来吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jwv8tf/will_rl_have_a_future/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  显然有点单击诱饵，但要认真地问。我再次进入RL，因为这是最接近我的AI。仍然有许多未解决的问题，例如奖励功能设计，代理商不做您想要的事情，培训永远用于某些问题等。 你们都在想什么？是否值得进入RL并在不久的将来成为职业？另外，您的项目将在5  -  10年内发生什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/imstifler     [link]   ＆＃32;   [commist imprion]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jwv8tf/will_rl_have_a_future/</guid>
      <pubDate>Fri, 11 Apr 2025 17:17:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习 - 书籍收集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jwh2ar/reinforcement_learning_collection_of_books/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  关于强化学习的几本书：    href =“ https://macro.com/app/pdf/59bcf2222222222-e791-441f-bcaf-4b33d334e6b3”&gt; href =“ https://macro.com/app/pdf/a9903757-9b9b-4f8e-9d38-9d38-3bed437b5ad5”&gt;深增强的增强式学习动作-Maxim lapan  提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1jwh2ar/reinforecement_learning_collection_collection_of_of_books/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jwh2ar/reinforcement_learning_collection_of_books/</guid>
      <pubDate>Fri, 11 Apr 2025 04:00:20 GMT</pubDate>
    </item>
    <item>
      <title>达到截断限制或发作结束时，体育馆是否不会重置环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jwd7dm/does_gymnasium_not_reset_the_environment_when/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我只是重新阅读了文档，每当env and/tunceed the nest/thunceed nest时，它就会拨打env.Reset（）。但是，每当我将渲染模式设置为“人类”时，当情节被截断或终止时，环境似乎会自动重置。请参阅上面的视频，在某些时间步骤之后，Env截断了位置。我想念什么吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/dead_as_duck      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1jwd7dm/does_gymnasium_not_not_reset_reset_the_environment_when/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jwd7dm/does_gymnasium_not_reset_the_environment_when/</guid>
      <pubDate>Fri, 11 Apr 2025 00:32:01 GMT</pubDate>
    </item>
    <item>
      <title>如何让代理站立？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jw4jvc/how_to_get_an_agent_to_stand_still/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在使用RL方法导航到目标。要学会放慢脚步并保持目标，代理应该在目标周围保持5秒钟。代理商非常成功地找到了目标，但很难站稳脚跟。它通常在该区域内摇摆，直到情节结束。我已经对行动，动作的改变和终点区域的速度实施了惩罚。我尝试了一些随机搜索这些惩罚量表，但没有真正的成功。它要么四处摆动，要么没有达到目标。这是RL中的一个已知问题，以使代理在接近一件事情之后停留，还是我的奖励和规模的问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/paradoge     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jw4jvc/how_to_get_an_agent_to_stand_still/</guid>
      <pubDate>Thu, 10 Apr 2025 18:08:55 GMT</pubDate>
    </item>
    <item>
      <title>不断学习的代理与静态LLM：建筑差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jw1sph/continuously_learning_agents_vs_static_llms_an/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/asyncvibes     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jw1sph/continuously_learning_agents_vs_static_llms_an/</guid>
      <pubDate>Thu, 10 Apr 2025 16:15:22 GMT</pubDate>
    </item>
    <item>
      <title>“对开放式LLM的防篡改保障措施”，Tamirisa等人2024年（诸如sophon之类的元学习un-fineTune-the-fignune-lights）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jvk7b7/tamperresistant_safeguards_for_openweight_llms/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/gwern       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jvk7b7/tamperresistant_safeguards_for_openweight_llms/</guid>
      <pubDate>Wed, 09 Apr 2025 23:40:43 GMT</pubDate>
    </item>
    <item>
      <title>无法实现稀疏性-PPO单步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jvckub/failing_to_implement_sparsity_ppo_singlestep/</link>
      <description><![CDATA[Hi everyone, I&#39;m trying to induce sparsity on the choices of a custom PPO RL agent (implemented using stable_baseline3), solving a single-episodic problem (basically a contextual bandit) which operates in a continuous action space implemented using gymnasium.spaces.Box(low= -1, high= +1, dtype= np.float64）。 代理必须通过选择“ n＆quot的参数向量”来优化问题框对象中的元素在选择最小的非零值条目（模块小于给定收费：1E-3）仍然充分解决问题。问题在于，无论我如何鼓励这种稀疏性，代理人根本不会选择接近0个值，看来代理甚至无法探索小值，这显然是因为他们考虑到-1至1的完整连续空间。 。我什至将成本推高，以至于唯一的奖励信号来自稀疏性。我尝试了许多不同的正则化功能，例如，参数矢量的每个非零入口和各种熵正规化（例如tsallis）的1s总和。 很明显，显然，代理人甚至无法探索小价值，甚至可以获得高昂的成本，无论选择什么，因此无论是正则化的成本是否都不在那里，都无法获得高昂的成本。我该怎么办？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/krnl_plt     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jvckub/failing_to_implement_sparsity_ppo_singlestep/</guid>
      <pubDate>Wed, 09 Apr 2025 18:11:20 GMT</pubDate>
    </item>
    <item>
      <title>多个任务的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jv6edg/approaches_for_multiple_tasks/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ 考虑一个玩具示例，机器人必须执行一系列任务A，B和C。假设：没有数据集或可用轨迹记录。我可以选择使用RL完成此操作？我是否错过了任何方法？    a，b和c的单独政策都受到了独立训练。并在满足适当条件时使用计划算法这样的决策树。    结束2结束，并具有精心设计的奖励功能。如果要添加更多任务，会发生什么？ 我是一个问这个问题，以在我的研究中获得指导。 Google在架构解决方案方面确实不太合作。谢谢您的宝贵时间。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/s_vaichu     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jv6edg/approaches_for_multiple_tasks/</guid>
      <pubDate>Wed, 09 Apr 2025 13:56:24 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>