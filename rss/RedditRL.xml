<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 25 Mar 2024 21:12:40 GMT</lastBuildDate>
    <item>
      <title>MADDPG Pytorch RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [64, 2]]，它是 AsStridedBackward0 的输出 0，版本为 3；预期是版本 2。提示：上面的回溯显示了 f 的操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnkv7q/maddpg_pytorch_runtimeerror_one_of_the_variables/</link>
      <description><![CDATA[嗨，我一直在尝试实现 MADDPG，但是当我的第二个代理向后传递时，程序崩溃，出现标题中的错误，我到处查看，然后仍然找不到解决方案。如果我只训练一个智能体，那么代码就可以工作，但当问题出现时，就会进入多个智能体。 def learn(self, memory, Episode): &lt; code&gt;[indent]if not memory.ready(): [indent][indent]return [indent]actor_states，状态，动作、奖励、actor_new_states、states_、terminations = memory.sample_buffer() [indent]device = self.agents[0].actor.device [indent]states = T.tensor(states, dtype=T.float32).to(device) [indent]actions_np = np.array(actions, dtype= np.float32)  [indent]actions = T.tensor(actions_np, dtype=T.float32).to(device) [indent] ]rewards = T.tensor(rewards, dtype = T.float32).to(device) [indent]states_ = T.tensor(states_, dtype = T.float32).to （设备） [indent]terminations = T.tensor(terminations).to(device) [indent]all_agents_new_actions = [ ] [indent]all_agents_new_mu_actions = [] [indent]old_agents_actions = [] [indent]for agent_idx, agent in enumerate(self.agents): #根据参与者网络估计下一个状态的动作值  [indent][indent]new_states = T.tensor(actor_new_states[agent_idx], dtype = T.float32).to(device) [indent][indent]new_charge_rate, new_charge_decision = agent .target_actor.forward(new_states) [indent][indent]all_agents_new_actions.append((new_charge_rate)) [indent][indent]all_agents_new_actions .append((new_charge_decision)) [indent][indent]#Action for current state from actor network [indent][ indent]mu_states = T.tensor(actor_states[agent_idx], dtype = T.float32).to(device) [indent][indent]charge_rate, charge_decision = agent. actor.forward(mu_states) [缩进][缩进]all_agents_new_mu_actions.append((charge_rate)) [缩进][缩进]all_agents_new_mu_actions.append((charge_decision)) [缩进][缩进]old_agents_actions.append(actions[agent_idx])  &lt; code&gt;[indent]new_actions = T.cat([all_agents_new_actions 中的行为的行为], dim = 1) [indent]mu = T.cat([为行为的行为in all_agents_new_mu_actions], dim = 1) [indent]old_actions =T.cat([acts for actions in old_agents_actions], dim = 1) [indent]for agent_idx, agent in enumerate(self.agents): #获取目标批评者网络的状态和新动作并将其展平。&lt; /code&gt; #critic 值与目标批评家 #One-step Lookahead TD-error: [缩进] [indent]critic_value_ = agent.target_critic.forward(states_, new_actions).flatten() [indent][indent]critic_value = agent.critic.forward(states, old_actions ).flatten() [indent][indent]target =rewards[:,agent_idx] + agent.gamma*critic_value_  #计算当前critic值的损失 [indent][indent]critic_loss = F.mse_loss(target, Critical_value) [indent] [缩进]self.writer.add_scalar（f“EV_{agent_idx}/Loss/Critic”，critic_loss，情节） [缩进][缩进]agent.critic.optimizer .zero_grad() [indent][indent]critic_loss.backward(retain_graph = True) [indent][indent]agent.critic。 Optimizer.step() [indent][indent]actor_loss = agent.critic.forward(states, mu).flatten() &lt; code&gt;[indent][indent]actor_loss = -T.mean(actor_loss) [indent][indent]self.writer.add_scalar(f&quot;EV_{agent_idx}/Loss/Actor&quot; ;、actor_loss、剧集） [缩进][缩进]agent.actor.optimizer.zero_grad() [缩进][ indent]actor_loss.backward(retain_graph = True) [indent][indent]agent.actor.optimizer.step() [indent] [indent]agent.update_network_parameters() 关于如何修改它以使代理学习的任何想法？ ***对缩进感到抱歉，我尝试在这里获得一个好的格式****   由   提交 /u/Barbajan22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnkv7q/maddpg_pytorch_runtimeerror_one_of_the_variables/</guid>
      <pubDate>Mon, 25 Mar 2024 18:24:55 GMT</pubDate>
    </item>
    <item>
      <title>连续状态和动作空间的近似策略迭代</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnkc06/approximate_policy_iteration_for_continuous_state/</link>
      <description><![CDATA[我遇到的大多数理论分析都处理有限状态或动作空间，或一些其他算法，如近似拟合迭代等。 当状态和动作空间连续时，有关于\epsilon近似策略迭代收敛的理论结果吗？ 我记得一篇单独的论文处理近似策略迭代，其中假设近似误差为随着时间的推移趋于零，但是如果误差是恒定的怎么办？ 此外，是否存在“正统”的误差？这种算法的实际版本与理论算法相匹配吗？   由   提交/u/_An_Other_Account_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnkc06/approximate_policy_iteration_for_continuous_state/</guid>
      <pubDate>Mon, 25 Mar 2024 18:03:26 GMT</pubDate>
    </item>
    <item>
      <title>ICLR 2024：可证明且实用：通过 Langevin Monte Carlo 对强化学习进行有效探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnfvrd/iclr_2024_provable_and_practical_efficient/</link>
      <description><![CDATA[ 由   提交/u/hmi2015  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnfvrd/iclr_2024_provable_and_practical_efficient/</guid>
      <pubDate>Mon, 25 Mar 2024 15:03:44 GMT</pubDate>
    </item>
    <item>
      <title>单代理或多代理设置</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bneekj/single_agent_or_multiagent_setting/</link>
      <description><![CDATA[社区您好， 我目前正在研究代理可以具有不同输出结构的情况。此外，我利用图神经网络来描述状态，其中包含许多类别，这些类别的数量在代理之间可能有所不同。考虑到这种情况，什么配置最合适？在多代理设置中，在我们的例子中，代理通常不会同时请求操作。在任何给定的决策点，我们可能有一个或多个代理寻求行动，而其他代理则很忙。在这种情况下，批评者网络应该接收什么输入？ 提前谢谢您   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bneekj/single_agent_or_multiagent_setting/</guid>
      <pubDate>Mon, 25 Mar 2024 14:01:10 GMT</pubDate>
    </item>
    <item>
      <title>【预测】关于预测的强化学习是否可以利用强化学习来预测河流水质预警系统。哪种方式最适合强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bndqxe/prediction_reinforcement_learning_about/</link>
      <description><![CDATA[ 由   提交/u/Abcsunny95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bndqxe/prediction_reinforcement_learning_about/</guid>
      <pubDate>Mon, 25 Mar 2024 13:32:09 GMT</pubDate>
    </item>
    <item>
      <title>【预测】关于预测的强化学习是否可以利用强化学习来预测河流水质预警系统。哪种方式最适合强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bndpwv/prediction_reinforcement_learning_about/</link>
      <description><![CDATA[ 由   提交/u/Abcsunny95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bndpwv/prediction_reinforcement_learning_about/</guid>
      <pubDate>Mon, 25 Mar 2024 13:30:54 GMT</pubDate>
    </item>
    <item>
      <title>帮助理解 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn7ui2/help_in_understanding_ppo/</link>
      <description><![CDATA[大家好！我在从学术论文和我在网上找到的一些代码暗示中理解有关 PPO 的事情时遇到一些问题。在论文中，我了解到旧模型和新模型的输出之间存在近似。这是如何运作的？如何更新模型，然后计算更新量？我是否需要始终保存 i-1 模型以便进行计算？现在是暗示。我正在使用 IsaacGym 并一次运行 n 个模拟。所有暗示都会根据一系列动作更新模型，直到游戏完成。我希望它从我的 n 个环境中的单个操作中随机批量运行，但我很难理解我需要保存和更改的内容。保存每次迭代需要哪些参数？我想到了：放弃、行动、奖励、价值（V 净输出）、对数概率。我是否遗漏了需要保存的东西？抱歉，如果这篇文章有点长，每一个帮助都会很棒。    由   提交/u/razton  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn7ui2/help_in_understanding_ppo/</guid>
      <pubDate>Mon, 25 Mar 2024 07:33:31 GMT</pubDate>
    </item>
    <item>
      <title>机器人强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn6nns/rl_for_robotics/</link>
      <description><![CDATA[大家好，我整理了一些学习强化学习的学习材料和资源： 1) 深度强化学习，作者：加州大学伯克利分校的 Sergey Levine 2) David Silver 讲座笔记 3) Google Deepmind 讲座视频 4) NPTEL IITM 强化学习  我也更喜欢学习材料具有足够的数学严谨性，能够深入解释算法。  同时引用一堆资源也令人生畏。有人可以为像我这样的初学者推荐上面列出的材料中的笔记和讲座视频吗？如果您还有其他资源，请在评论部分提及。    由   提交 /u/Quirky_Assignment707   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn6nns/rl_for_robotics/</guid>
      <pubDate>Mon, 25 Mar 2024 06:09:33 GMT</pubDate>
    </item>
    <item>
      <title>车子仍然无法在拐角处转弯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn5bss/the_car_still_isnt_able_to_turn_at_the_corner/</link>
      <description><![CDATA[https://github.com/arthiondaena/Car -game 这是我一直在做的项目。在第 60 集，汽车能够到达拐角，但没有转弯，而是与赛道边界相撞。  我以为几集后会有所改善，但即使在 500 集之后，它仍然在拐角处崩溃。花了12个小时才达到500集。我没有看到任何改进。 如果有人能指出代码中的问题，我将不胜感激。 谢谢    由   提交/u/Invicto_50  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn5bss/the_car_still_isnt_able_to_turn_at_the_corner/</guid>
      <pubDate>Mon, 25 Mar 2024 04:43:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在健身房环境中显示视频指标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn4ft0/how_to_show_metrics_for_videos_in_gymnasium/</link>
      <description><![CDATA[嗨！我知道如何使用 RecordVideo 包装器录制视频。但该视频不包含任何任意指标，例如奖励/集数或任何自定义指标。有什么方法可以轻松做到这一点，还是我需要更深入地研究 moviepy？ 提前致谢。   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn4ft0/how_to_show_metrics_for_videos_in_gymnasium/</guid>
      <pubDate>Mon, 25 Mar 2024 03:54:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么具有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmx1pf/why_is_my_stable_baselines3_model_with_cartpole/</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加，而剧集平均奖励保持不变为 -1，这就是sutton_barto_reward系统工作正常。 从cartpole导入gymnasium 从stable_baselines3导入CartPoleEnv 从stable_baselines3.ppo.policies导入PPO 导入MlpPolicy env = CartPoleEnv(sutton_barto_reward=True) model = PPO(&quot;MlpPolicy&quot;, env, gamma=1, verbose=1) model.learn(total_timesteps=30000) &lt; /p&gt; 但是，我不明白为什么会这样，因为折扣率已设置为 1。剧集长度平均值是否应该没有任何改善，因为剧集的累积奖励始终是一样吗？ ​ ​ ​   由   提交/u/uglyboi34  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmx1pf/why_is_my_stable_baselines3_model_with_cartpole/</guid>
      <pubDate>Sun, 24 Mar 2024 22:10:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] Aleksa Godric 关于在 DeepMind 找到工作的帖子在今天仍然具有现实意义吗？ [是的]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bml5tw/d_is_aleksa_godrics_post_on_landing_a_job_at/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bml5tw/d_is_aleksa_godrics_post_on_landing_a_job_at/</guid>
      <pubDate>Sun, 24 Mar 2024 13:47:38 GMT</pubDate>
    </item>
    <item>
      <title>我究竟做错了什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmgifh/what_am_i_doing_wrong/</link>
      <description><![CDATA[我正在尝试训练 cartpole 代理。但他似乎并没有学到任何东西。我也尝试调试和更改超参数，但它仍然没有学到任何东西。 请帮助这里可能出了什么问题？ gridworld/dqn.py 位于 main · bherwanisuraj/gridworld (github.com)  P.S.感谢先生们抽出宝贵的时间来帮助我。  问题现已解决。显然，目标模型从未得到更新，因为我是在每个 update_target % epoch 而不是 epoch % update_target 更新它。 我知道这是一个愚蠢的错误。我会不断学习，让自己变得更好。再次感谢大家。   由   提交/u/purna_lingham  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmgifh/what_am_i_doing_wrong/</guid>
      <pubDate>Sun, 24 Mar 2024 09:10:22 GMT</pubDate>
    </item>
    <item>
      <title>PPO 和 DreamerV3 代理完成了《愤怒之铁拳》。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm9sjk/ppo_and_dreamerv3_agent_completes_streets_of_rage/</link>
      <description><![CDATA[不太确定我们是否可以自我推销，但我看到有人发布了他们的经纪人完成《街头霸王 3》的视频，所以我希望它被允许。&lt; /p&gt; 我一直在训练特工玩《怒之铁拳》的第一个阶段，现在终于可以完成游戏了，我的视频更多是为了娱乐，所以没有太多技术，但我将在下面解释一些内容。无论如何，这里是视频的链接： https://www.youtube.com/watch? v=gpRdGwSonoo ​ 这总共由 8 个模型完成，每个阶段 1 个模型。前 4 个模型是使用 SB3 训练的 PPO 模型，后 4 个模型是使用 SheepRL 训练的 DreamerV3 模型。两者都在相同的稳定复古健身房环境中使用我的奖励函数进行训练。 DreamerV3 在游戏的 64x64 像素 RGB 图像上进行训练，具有 4 个跳帧且无帧堆叠。 PPO 在游戏的 160x112 像素单色图像上进行训练，具有 4 个跳帧和 4 个帧堆叠。 每个连续阶段的模型都是在最后一个阶段的基础上构建的，除了切换到 DreamerV3 时，因为我再次从头开始，而且除了第8阶段游戏由向右移动改为向左移动外，我决定再次从头开始。 至于“娱乐”，我决定重新开始。在视频方面，Gym 环境基本上返回一些有关游戏状态的数据，然后我将其形成文本提示，并将其输入到开源 LLM 中，以便它可以对转换为 TTS 的游戏玩法做出一些简单的评论，同时让 Whisper 模型将我的 SpeechToText 转换，以便我也可以与角色交谈（当我说出角色的名字时触发）。这一切都连接到我制作的 UE5 应用程序，其中包含虚拟角色和环境。 我断断续续地训练了模型大约 5 或 6 个月的时间（不是直接的），所以我不真的知道我总共训练了他们多少小时。我认为第 8 阶段模型的训练时间大约为 15-30 小时。 DreamerV3 模型在 4 个平行健身房环境中进行训练，而 PPO 模型在 8 个平行健身房环境中进行训练。不管怎样，我希望它很有趣。   由   提交/u/disastorm  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm9sjk/ppo_and_dreamerv3_agent_completes_streets_of_rage/</guid>
      <pubDate>Sun, 24 Mar 2024 02:15:55 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>