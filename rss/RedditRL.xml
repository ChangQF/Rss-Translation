<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 13 Jun 2024 09:16:29 GMT</lastBuildDate>
    <item>
      <title>DDPG 使用变压器。动作饱和？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1den3nm/ddpg_using_a_transformer_action_saturation/</link>
      <description><![CDATA[我正在尝试使用 gpt transformers DDPG 样式实现 Actor/Critic 网络，但在使其收敛方面遇到了麻烦。我正在健身房的 pendulum-v1 环境中对其进行测试，该环境似乎可以使用简单的 MLP 设置。为了进行调试，我尝试仅使用块大小（上下文长度）为 1 的 GPT。从技术上讲，它的性能不应该和 MLP 一样好吗？我尝试添加熵进行探索，但效果从未好转。在进一步调试后，我注意到操作最终会迅速饱和到我的动作空间的边界（-2,2）或动作空间内的一小段范围（例如 0.5 到 0.7）。我将不胜感激任何见解或帮助。谢谢！    提交人    /u/Acrobatic-Subject-41   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1den3nm/ddpg_using_a_transformer_action_saturation/</guid>
      <pubDate>Thu, 13 Jun 2024 01:24:27 GMT</pubDate>
    </item>
    <item>
      <title>涉及强化学习的数学金融研究领域（股票、债务、加密货币、保险等任何资产类别）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1deh31o/area_of_research_in_mathematical_finance_any/</link>
      <description><![CDATA[我目前正在攻读硕士学位，论文是关于在量化金融中实施强化学习框架。目前有很多工作要做，但更多的是侧重于在（大多数）基本金融环境中实施和改进不同的强化学习算法（例如投资组合权重优化、基于奖励/（简单）风险衡量的交易机器人，或引入波动性、流动性和其他基本变量作为学习环境的一部分）。我正在寻找一个金融领域，它更加专注（即不是通用设置），并且仍然可以使用强化学习框架进行探索（基本上是量化金融中的数学/优化问题）。我对所有资产类别和所有类型的问题持开放态度。     提交人    /u/Rogue260   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1deh31o/area_of_research_in_mathematical_finance_any/</guid>
      <pubDate>Wed, 12 Jun 2024 20:45:52 GMT</pubDate>
    </item>
    <item>
      <title>我们可以将强化学习用于未标记数据吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1degb6s/can_we_use_reinforcement_learning_for_unlabeled/</link>
      <description><![CDATA[我正在尝试根据一些表格分数、体重等制定个性化的锻炼计划。我确实有一个从易到难分类的锻炼列表。我没有任何标记数据，但我可以加入人工反馈。从头开始训练 RL 代理是否有意义？ 关于如何解决这个问题的任何意见都会有所帮助。    提交人    /u/Hot_Direction6179   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1degb6s/can_we_use_reinforcement_learning_for_unlabeled/</guid>
      <pubDate>Wed, 12 Jun 2024 20:13:24 GMT</pubDate>
    </item>
    <item>
      <title>[D] Dreamer-v3 如何在稀疏奖励探索任务上表现如此出色？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1defbq8/d_how_does_dreamerv3_do_so_well_on_sparsereward/</link>
      <description><![CDATA[在阅读了 Dreamer-v3 论文 后，我有点困惑它为什么在艰难的探索任务上表现如此出色。例如 MineCraft 领域。要获得一颗钻石，您总共需要采取 12 个奖励步骤，例如获取木材、制作桌子、获取镐、使用熔炉、制作钻石。 我明白模型及其设计选择如何非常有利于泛化和更好的价值评估。因此，一旦您做了一些能带来奖励的事情（例如收集木材），您就可以比其他竞争方法更可靠地回到它。但是假设您已经获得过一次木材，并且您每次都很快学会了获得它。从那时起，您不是在犹豫不决吗？唯一的探索是对动作进行软最大化，如果您不知道任何未来的奖励，那么这基本上应该是随机动作。是什么促使你制作工作台的？ 是不是因为犹豫不决就足以让你达到目标了？是否存在某种旨在改进模型的隐式定向探索？基于模型的泛化是否比我所认为的要强大得多？我只是非常惊讶他们可以在没有探索奖励之类的东西的情况下解决如此长期的任务。希望大家的想法！    提交人    /u/asdfwaevc   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1defbq8/d_how_does_dreamerv3_do_so_well_on_sparsereward/</guid>
      <pubDate>Wed, 12 Jun 2024 19:32:24 GMT</pubDate>
    </item>
    <item>
      <title>动作拆分时，如何表示状态？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1deeqlr/how_to_represent_state_when_the_action_is_splitup/</link>
      <description><![CDATA[我有一个“购买卡”的动作，为了避免组合动作空间，它首先被分成购买动作，然后是模型预测在付款之前要花费哪些代币。因此，在发放卡片时，没有特定的顺序来给予奖励或改变状态。 我的想法非常容易完全修改，它们是：  在“买卡”动作上进行训练，保持 current_state 和 next_state 相同，并给予模型购买卡片的奖励。 对每个“花费代币”动作都执行奖励 = 0 并更新每个代币的状态。 仅在最后的“花费代币”动作上将卡片交给玩家。  但是，我不知道这在数学上是否正确。    提交人    /u/Breck_Emert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1deeqlr/how_to_represent_state_when_the_action_is_splitup/</guid>
      <pubDate>Wed, 12 Jun 2024 19:07:53 GMT</pubDate>
    </item>
    <item>
      <title>上下文推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dee1rw/contextual_recommender_system/</link>
      <description><![CDATA[大家好，我正在研究基于强化学习的上下文推荐系统，我有一个名为 assistments 的数据集，它是一个关于教育的数据集，包含问题、用户、正确和其他特征。我想要您关于如何根据提供的特征构建上下文向量的专业知识，这是我在上下文 rs 上的第一个项目，其他数据集似乎很容易包含上下文，但这个有点不同，我想先推荐问题，然后再推荐提示。 提前谢谢您 列名： import pandas as pd def create_user_context(data): data.fillna(-1, inplace=True) # 按用户和问题分组 grouped_data = data.groupby([&quot;user_id&quot;, &quot;problem_id&quot;]) # 提取用户特征 user_context = grouped_data[[&quot;user_id&quot;, &quot;student_class_id&quot;]].first() def split_skills(skill_name_list): if pd.isna(skill_name_list).empty: return [] if &quot;,&quot;在（skill_name_list）中： 打印（skill_name_list） 返回skill_name_list.split（＆quot;，＆quot;） else： 返回[skill_name_list] # 单个技能作为列表 user_context[＆quot;all_skills＆quot;] = grouped_data[＆quot;skill_name＆quot;].apply（split_skills）.explode（） # 性能特征 user_context[＆quot;attempt_count＆quot;] = grouped_data[＆quot;attempt_count＆quot;].first（） user_context[＆quot;ms_first_response＆quot;] = grouped_data[＆quot;ms_first_response＆quot;].first（） user_context[＆quot;previous_correct＆quot;] = data.groupby（＆quot;user_id＆quot;）[＆quot;correct＆quot;].transform（pd.Series.cumsum） - data[＆quot;correct＆quot;] data[&quot;cumulative_hint_usage&quot;] = data.groupby(&quot;user_id&quot;)[&quot;hint_count&quot;].transform(pd.Series.cumsum) user_context[&quot;past_hint_usage&quot;] = grouped_data[&quot;cumulative_hint_usage&quot;].first() return user_context user_context = create_user_context(data.copy()) print(user_context) [&#39;order_id&#39;, &#39;assignment_id&#39;, &#39;user_id&#39;, &#39;assistment_id&#39;, &#39;problem_id&#39;, &#39;original&#39;, &#39;correct&#39;, &#39;attempt_count&#39;, &#39;ms_first_response&#39;, &#39;tutor_mode&#39;, &#39;answer_type&#39;, &#39;sequence_id&#39;, &#39;student_class_id&#39;, &#39;position&#39;, &#39;type&#39;, &#39;base_sequence_id&#39;, &#39;skill_id&#39;, &#39;skill_name&#39;, &#39;teacher_id&#39;, &#39;school_id&#39;, &#39;hint_count&#39;, &#39;hint_total&#39;, &#39;overlap_time&#39;, &#39;template_id&#39;, &#39;answer_id&#39;, &#39;answer_text&#39;, &#39;first_action&#39;, &#39;bottom_hint&#39;, &#39;opportunity&#39;, &#39;opportunity_original&#39;]     提交人    /u/Fredybec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dee1rw/contextual_recommender_system/</guid>
      <pubDate>Wed, 12 Jun 2024 18:40:02 GMT</pubDate>
    </item>
    <item>
      <title>这仍然是真的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1deaain/is_it_still_true/</link>
      <description><![CDATA[      https://preview.redd.it/09kujbm8z56d1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=211213d6c841804ec6446b9e0d246213460c2ada    提交人    /u/Logical_Jaguar_3487   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1deaain/is_it_still_true/</guid>
      <pubDate>Wed, 12 Jun 2024 16:04:17 GMT</pubDate>
    </item>
    <item>
      <title>这个人工智能可以创造游戏阶段！使用 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1de8z2r/this_artificial_intelligence_can_create_game/</link>
      <description><![CDATA[        提交人    /u/Flimsy_Roll_5666   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1de8z2r/this_artificial_intelligence_can_create_game/</guid>
      <pubDate>Wed, 12 Jun 2024 15:08:33 GMT</pubDate>
    </item>
    <item>
      <title>如何处理稀疏终止条件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1de46b7/how_to_handle_sparse_termination_conditions/</link>
      <description><![CDATA[考虑以下与位翻转环境松散相关的玩具问题。每次重置时的状态是随机设置的 50 位。您有两种可能的操作。操作 1：查看位 i 并查看它是否为 1。操作 2：重置并随机设置另外 50 位。每次查看一位时，其成本都比前一次高。因此，在第 x 步，查看一位的操作成本为 x^2。您永远不会获得退款，因此如果您重置然后查看更多位，成本会不断增加 如果您发现一组位中有 35 位设置为 1，游戏将终止。 如果您只是使用 RL 编写代码并将奖励设置为第 x 个“查看步骤”的 -x^2，您将永远不会进入终止状态，因此训练将不会取得任何进展。我查看了 HER，但我不确定它是否适合这种情况。 处理它的正确方法是什么？ 编辑：我应该说额外的成本是 x^2，其中 x 是您查看该特定集合中某个位的次数。因此，在重置后立即查看某个位比稍后在同一组位中查看要便宜得多。这也意味着最小化在单个集合中发现 35 个 1 位的总累积成本与最小化步骤数不同。 让我试着举个例子。我们从随机设置的 50 个位开始。作为第一个动作，代理可能会查看位 3 和 5。这需要花费 1^2+2^2 = 5。然后它可能会放弃该组位并重置。然后它可能会查看位 4、3 和 10。这需要额外的成本 1^2 + 2^2 + 3^2= 14，因此到目前为止的总成本为 5 + 14 = 19。    提交人    /u/MrMrsPotts   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1de46b7/how_to_handle_sparse_termination_conditions/</guid>
      <pubDate>Wed, 12 Jun 2024 11:17:45 GMT</pubDate>
    </item>
    <item>
      <title>阿尔伯塔大学 RL 课程的作业是免费的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1de3yjx/are_the_assignments_for_uoas_rl_course_available/</link>
      <description><![CDATA[我尝试访问它们，但似乎我需要获取证书。有没有办法绕过这个问题，或者我只有付费才能访问它们？    提交人    /u/Rit2Strong   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1de3yjx/are_the_assignments_for_uoas_rl_course_available/</guid>
      <pubDate>Wed, 12 Jun 2024 11:05:05 GMT</pubDate>
    </item>
    <item>
      <title>嗨，我是新手，我正在尝试使用 openai gym 和稳定基线 3 为 cartpole 建立一个 rl 模型，我每次训练模型 12，大约有 30,000 个时间步长，当我在环境中实施它时，它能够保持平衡大约 1-2 小时，得分为 121303.0</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ddxuwj/hi_i_am_a_rookie_and_i_was_trying_to_a_rl_model/</link>
      <description><![CDATA[我向 Google gemini 询问了平均分数，也告诉了它我的分数，但 gemini 说平均分数是 200，环境可能有问题     提交人    /u/Several_Outcome_8331   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ddxuwj/hi_i_am_a_rookie_and_i_was_trying_to_a_rl_model/</guid>
      <pubDate>Wed, 12 Jun 2024 04:14:20 GMT</pubDate>
    </item>
    <item>
      <title>探索作为学习策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ddrdq3/exploration_as_learned_strategy/</link>
      <description><![CDATA[大家好 :) 我目前正在研究一种使用 GNN 的 RL 算法，以优化具有动态变化客户端位置的数据中心网络。但是，需要注意的是，代理在开始时对网络的信息非常少（只有数据中心初始配置之间的延迟）。他可以重新定位被动节点，这不需要花费太多成本来检索潜在其他位置的信息。这对由主动数据中心决定的总体延迟没有影响。他还可以重新定位主动节点，但是，这很昂贵。  因此，代理必须学习一种策略，在开始时他总是进行探索（一开始，这甚至可能是随机的），并且随着他收集有关网络的更多信息，他可以开始重新定位活动节点。 现在的问题是，如果您知道任何包含类似策略的论文，其中代理应该学习一种探索策略，然后该策略也用于实时系统的推理，而不仅仅是用于训练（其中探索当然非常重要并且发生在大多数训练算法中）。或者如果您有任何经验，我很乐意听取您对该主题的看法。 致以最诚挚的问候和感谢！    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ddrdq3/exploration_as_learned_strategy/</guid>
      <pubDate>Tue, 11 Jun 2024 22:42:01 GMT</pubDate>
    </item>
    <item>
      <title>NVidia Omniverse 接管了我的计算机</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ddkw1g/nvidia_omniverse_took_over_my_computer/</link>
      <description><![CDATA[      https://preview.redd.it/6ecu4wdfgz5d1.png?width=967&amp;format=png&amp;auto=webp&amp;s=d56ddf933839a6a3f938d83b72aaed1b5fb9372d 我只是想使用 Nvidia ISAAC sim 来测试一些强化学习。但它安装了整个套件。在我设法删除一些之前，还有更多的流程和服务。我需要所有这些吗？我只想能够编写一些脚本来学习和回放 Python。这可能吗，还是我需要所有这些服务才能使其运行？  这比使用带有 MLAgents 的 Unity 更好吗，它看起来几乎是同一件事。     提交人    /u/No_Way_352   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ddkw1g/nvidia_omniverse_took_over_my_computer/</guid>
      <pubDate>Tue, 11 Jun 2024 18:12:02 GMT</pubDate>
    </item>
    <item>
      <title>基于模型与无模型的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcqdu5/distinction_of_modelbased_vs_modelfree/</link>
      <description><![CDATA[无论我看到多少教科书定义，某些机器学习定义都毫无意义。 基于模型与无模型，假设我们采用系统识别视角。在这种情况下，我们已经定义了动态结构，并希望找到模拟参数，使事情像在现实生活中一样展开。希望以迭代数据驱动的方式执行此操作可以节省大量劳动。但是，我们通过结构对模型施加的偏差应该同时帮助对动态进行建模，同时保持足够的灵活性以进行调整。这似乎从根本上存在缺陷，无法将其与无模型方法区分开来。无模型我们不强加任何动态结构。我认为，我们不会限制动态的展开。在这两种情况下，我们都在从根本上用数据或经验调整参数。（尽管参数少得多）。最终，问题取决于由（初始）偏见选择的未来经验，而这种偏见推动了“寻找参数”的目标。偏见要么来自以前的经验，要么由模型引起，等等。对我来说，这归结为探索与利用的问题，而不是方法问题。 正确的思考方式是什么？    提交人    /u/FriendlyStandard5985   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcqdu5/distinction_of_modelbased_vs_modelfree/</guid>
      <pubDate>Mon, 10 Jun 2024 16:55:43 GMT</pubDate>
    </item>
    <item>
      <title>模拟退火与强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</link>
      <description><![CDATA[当考虑启发式竞争编程任务时会出现这个问题。让我们考虑一个非常基本的例子，旅行商问题（或者最近的这个竞赛，很多人都在讨论 RL 的可能性，但大多数人都不是专家（包括我自己，最终也使用了模拟退火，但事后却很痛苦，因为我本来想做点不同的事情））。 几乎所有这些比赛都是使用模拟退火或其他变体赢得的。对于不熟悉的人来说，所有这些变体都是从某个解决方案开始，然后通过某种变异过程迭代改进它，以摆脱局部最小值。对于旅行商问题，您可以提出一个初始的随机城市列表，然后随机交换一些城市，直到它改进了您的解决方案，然后将这个新解决方案作为最佳解决方案，依此类推。再加上一些突变以逃避局部最小值（例如，意味着对列表的一小部分进行改组 - 我显然是在简化）。 什么会阻止人们在这些问题上使用强化学习（实际上没有人，这篇文章中已经针对旅行商问题完成了此操作：https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/tje2.12303 - 如果我没看错的话，作者甚至提到了模拟退火，但没有将结果与它进行比较）。奖励函数通常不难想出（我在竞赛中提到的奖励函数甚至比 TSP 更容易，因为每次“怪物”死亡后你都会获得“金币”，你会尝试最大化它（累计金额））。 我对不使用强化学习的假设是：  尽管强化学习的样本效率更高，但这些问题实际上很容易模拟，因此更新神经网络或任何函数近似器的开销都太高。只有当运行一集的成本非常高时，强化学习才会有趣。否则，用 C 编写简单的遗传算法总是比用 Python 编写的 RL 更有效（时间方面）。 无需概括，这些比赛的测试用例已经给出，你只需要想出最佳的行动序列来影响环境（例如，在我的第二个例子中要杀死哪些怪物）并在这些测试用例中获得最高奖励。如果比赛内容相同，但他们在比赛结束前三十分钟公布测试用例，那么在 8000 个线程上运行模拟退火三十分钟的效率不如使用预先训练好的代理，该代理在 GPU 上经过几天的大量不同虚构测试用例的训练。 RL 在多代理设置（零和游戏等）中真正显示了其主导地位，其中模拟退火和变体不易实现（尽管 MARL 优化的每一步都在尝试利用当前最佳策略组合，这可以通过遗传算法来完成 - 但我认为这被称为 RL，它只是没有梯度的 RL）。 但同时，RL 比其他技术更复杂，所以也许人们只是因为没有专业知识而不去那里，而 RL 专家实际上会在其中一些比赛中表现出色？  我遗漏了什么吗？您们这些 RL 专家怎么看？Rich 会怎么说。萨顿说了什么？    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</guid>
      <pubDate>Mon, 10 Jun 2024 10:52:31 GMT</pubDate>
    </item>
    </channel>
</rss>