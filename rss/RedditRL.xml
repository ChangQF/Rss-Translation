<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 05 Sep 2024 18:20:18 GMT</lastBuildDate>
    <item>
      <title>在多任务/迁移学习中使用 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9pweb/using_rl_in_multitasktransfer_learning/</link>
      <description><![CDATA[我感兴趣的是看看神经网络如何高效地编码魔方，同时还能执行多项不同的任务。如果有人有多任务或迁移学习的经验，我想知道强化学习是否是一项适合纳入网络编码器部分训练中的好任务。    提交人    /u/thebrilliot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9pweb/using_rl_in_multitasktransfer_learning/</guid>
      <pubDate>Thu, 05 Sep 2024 15:56:19 GMT</pubDate>
    </item>
    <item>
      <title>创建代理来玩 Atomas 的指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9ml9e/guidance_in_creating_an_agent_to_play_atomas/</link>
      <description><![CDATA[我用 python 重新创建了一个我经常玩的游戏，叫做 atomas，主要目标是组合相似的原子并创建尽可能大的原子。它与 2048 非常相似，但不是在固定范围内生成新标题，而是中心原子范围每 40 步扩展一次。 原子可以放置在板中的任意两个原子之间，因此我决定用长度为 18 的列表来表示板（游戏结束前原子的最大数量）我用原子数量填充它，因为这是唯一重要的方面，其余的都留零。 我不确定这是否是表示板的最佳方式，但我无法想象更好的方式，中心原子随后进行编码，我将板中的原子数量以及移动次数包括在内。  我尝试过将值标准化为 0,1，将特殊原子编码为负值或仅高于可能的最大原子值。将所有内容标准化为 0,1 -1, 1。我尝试过 PPO，DQN 使用了掩码，因为动作空间是 19 0,17 是放置原子的索引，18 用于将中心原子转换为正值（有时由于特殊原子而可能）。 奖励函数变得非常复杂，但仍然没有提供良好的结果。由于大多数动作都不是特别好或特别坏，因此很难确定哪个是最佳动作。 到了我稍微编辑了奖励函数并将其转变为确定下一步动作的规则的地步，它的表现比任何算法都要好得多。我认为问题不在于训练时间，因为训练了 10k 集的算法表现与训练了 1M 集的算法相同或更差，并且它们都被硬编码规则所超越。  我知道有些问题并不是用 RL 来解决的，但我很确定 DRL 可能会培养出一个还不错的球员。  我愿意接受任何关于我如何改进以尝试获得可用代理的意见或指导。     提交人    /u/woimbouttamakeaname   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9ml9e/guidance_in_creating_an_agent_to_play_atomas/</guid>
      <pubDate>Thu, 05 Sep 2024 13:32:26 GMT</pubDate>
    </item>
    <item>
      <title>我不知道如何开始我的第一个 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</link>
      <description><![CDATA[我过去一直是一名 Unity 游戏开发者，我想尝试做一些强化学习 - 两个 AI 玩家之间的标签游戏。我一直在使用 anaconda 和 jupyter 笔记本进行我的第一个机器学习项目，到目前为止我很喜欢它。现在我想知道我该如何实现我的想法？什么环境？我应该使用 pygame 吗？有人有这方面的教程/课程吗？ 我不介意浏览无尽的库文档。    提交人    /u/JMB4200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</guid>
      <pubDate>Thu, 05 Sep 2024 11:00:04 GMT</pubDate>
    </item>
    <item>
      <title>“RTX 4060 足以使用 Isaac Sim 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</link>
      <description><![CDATA[      我正在使用 Isaac Gym 进行运动任务训练，但我想在安装了 RTX 4060 的家用电脑上进行训练。它足以完成这项任务吗？需求文档列出的最低要求是 GeForce RTX 3070，这比 4060 略好一些。如果有人有使用 4060 的经验，请告诉我它是否足够。 https://preview.redd.it/vx4oohnhsymd1.png?width=1120&amp;format=png&amp;auto=webp&amp;s=ad49a07ed80612f57ad6b871ccc6180f85ffe03c    由    /u/Vegetable_Pirate_263  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</guid>
      <pubDate>Thu, 05 Sep 2024 10:04:32 GMT</pubDate>
    </item>
    <item>
      <title>随着损失的增加，调试拟合 Q 评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8lhrg/debug_fitted_qevaluation_with_increasing_loss/</link>
      <description><![CDATA[嗨，专家，我正在使用 FQE 进行离线离策略评估。但是，我发现在训练过程中我的 FQE 损失并没有减少。  我的环境是离散动作空间和连续状态/奖励空间。我尝试了几种修改来调试根本原因：  更改超参数：学习率、FQE 的时期数 更改/规范化奖励函数 确保数据解析正确  上述方法均不起作用。 以前我有一个类似的数据集，我很确定我的训练/评估流程是正确的并且运行良好。 您还会检查/实验什么以确保 FQE 正在学习？    提交人    /u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8lhrg/debug_fitted_qevaluation_with_increasing_loss/</guid>
      <pubDate>Wed, 04 Sep 2024 05:23:31 GMT</pubDate>
    </item>
    <item>
      <title>体育馆渲染问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8fq7g/gymnasium_rendering_question/</link>
      <description><![CDATA[我知道目前 gymnasium box2d 环境有 2 种渲染模式：human 和 rgb_array。有没有办法通过删除所有查看器渲染来加快程序速度？ 编辑：我应该指定这一点，因为使用 rgb_array 运行似乎花费的时间几乎是 human 模式的两倍    提交人    /u/Admirable-Length-465   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8fq7g/gymnasium_rendering_question/</guid>
      <pubDate>Wed, 04 Sep 2024 00:27:06 GMT</pubDate>
    </item>
    <item>
      <title>在矢量输入环境上重置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8fg21/resetting_on_vector_input_environments/</link>
      <description><![CDATA[大家好，有人尝试过在矢量观测环境中使用策略软重置吗？因为我的代理在软重置甚至 0.1 正常噪声后都无法恢复。 我根据 P. D&#39;Oro 2023 尝试过。    提交人    /u/Automatic-Web8429   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8fg21/resetting_on_vector_input_environments/</guid>
      <pubDate>Wed, 04 Sep 2024 00:14:25 GMT</pubDate>
    </item>
    <item>
      <title>数据质量对训练模仿学习模型的影响：使用 Aloha Kit 进行实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</link>
      <description><![CDATA[        由    /u/Trossen_Robotics  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</guid>
      <pubDate>Tue, 03 Sep 2024 20:45:06 GMT</pubDate>
    </item>
    <item>
      <title>在 cudnn 中使用 Pytorch 寻找计算二阶导数的更好方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</link>
      <description><![CDATA[嗨，我试图将安全算法：CPO 应用于我当前的 RL 代码。我使用 pytorch 和 GPU（cuda）运行我的代码。当计算 CPO 中的二阶导数时，cuda 会自动使用 cudnn 进行此类计算。问题是，由于 CuDNN API 的限制，CuDNN RNN 不支持 Double Backing。我也在 stackoverflow 中描述了这个问题，但没有得到答案。而且当我搜索时，这个错误， NotImplementedError：&#39;_cudnn_rnn_backward&#39; 的导数未实现。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs)NotImplementedError: 未实现“_cudnn_rnn_backward”的导数。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs) 在 RNN 和相关模型中的双重向后计算中更常见。如果你们知道解决方案，那将会很有帮助   由    /u/dAmiBouY539  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</guid>
      <pubDate>Tue, 03 Sep 2024 20:10:56 GMT</pubDate>
    </item>
    <item>
      <title>分享我的玩具项目“JAxtar”，用于解谜的纯 jax 和 jittable A* 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</link>
      <description><![CDATA[嗨，我想介绍一下我的玩具项目 JAxtar。 我将其发布到 r/reinforcementlearning 这里以及 r/JAX，因为它与典型的 RL 不同，但它是为使用神经启发式算法的 RL 编写的，例如 DeepCubeA，我计划朝这个方向改进它。 这不是很多人会觉得有用的代码，但我在编写它时用 Jax 完成了大部分杂技，我认为它可能会激励其他人使用 Jax 的人。 我的硕士论文是关于使用 RL 进行 A* 和神经启发式训练以解决 15 个难题，但当我反思它时，最大的头痛是 CPU 和 GPU 之间数据传输的频率高且长度长。几乎一半的执行时间都花在这些通信瓶颈上。这个问题的另一种解决方案是 DeepCubeA 提出的分批 A*，但我觉得这不是一个完整的解决方案。 有一天，我偶然发现了 mctx，这是 google deepmind 用纯 jax 编写的 mcts 库。 我对这种方法很着迷，并多次尝试用 Jax 编写 A*，但都没有成功。问题在于哈希表和优先级队列。 毕业后经过很长一段时间，研究了许多例子，绞尽脑汁，我终于设法编写了一些可用的代码。 我很自豪地说，此代码有几个特殊元素  用于将定义的状态转换为哈希键的 hash_func_builder 用于并行查找和插入的哈希表 可以批处理、推送和弹出的优先级队列 用于拼图的完全 jitted A* 算法。  我希望这个项目可以成为任何喜欢 Jax 和这种类型的 RL（带启发式的 A*）的人的鼓舞人心的例子    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</guid>
      <pubDate>Tue, 03 Sep 2024 14:50:19 GMT</pubDate>
    </item>
    <item>
      <title>作为输出的动作向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</link>
      <description><![CDATA[大家好，我计划在即将开展的项目中利用强化学习。在这种情况下，我的输入（或状态）将由两个矩阵组成：一个表示车辆的位置，另一个表示车辆的速度。输出或动作将是一个具有四个连续元素的向量，每个元素的范围在 0 到 1 之间。例如，一个步骤之后的一个可能动作可能是 [0.51, 0.76, 0.9, 0.12]。 有人能建议哪种强化学习算法最适合这种类型的问题吗？提前感谢您的帮助！    提交人    /u/muttahirulislam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</guid>
      <pubDate>Tue, 03 Sep 2024 13:38:02 GMT</pubDate>
    </item>
    <item>
      <title>随情节变化动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</link>
      <description><![CDATA[当动作空间本身随着情节而变化时，开关策略算法的预期行为是什么。这会导致非平稳性？ 动作空间是连续的。在 Mujoco Ant Cheetah 等中，典型情况是它代表扭矩。假设在一集中动作空间是 [1, -1] 下一集是 [1.2, -0.8] 下一集是 [1.4, -0.6] ... ... 未来的某一集是 [2, 0] .. 动作空间范围的变化由某些函数控制，并且在每个情节开始之前都会随着情节而变化。像 ppo trpo ddpg sac td3 这样的算法的预期行为应该是什么？他们能够处理吗？对于像 mappo maddpg matrpo matd3 等 marl 算法也有类似的问题。 这是由于动态变化而导致的非平稳性吗？是否存在无效的操作范围。我们可以将总体范围限制为某个高低值，但范围会随着情节而改变。    提交人    /u/Intrepid_Discount_67   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</guid>
      <pubDate>Tue, 03 Sep 2024 12:24:25 GMT</pubDate>
    </item>
    <item>
      <title>深度 CFR 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</link>
      <description><![CDATA[我从原始 Deep CFR 文章中获取了代码 链接：https://arxiv.org/pdf/1811.00164 我现在正在为我的游戏实现 Deep CFR 算法，当我编写完整算法时，我遇到了很多错误。所以我的问题是，本文末尾的代码是否完全正确，错误是否在我的代码中？如果有人已经为他们自己的任务实现了 Deep CFR 并且可以分享提示/代码，那就太好了 如果有人愿意提供帮助，我可以上传我的实现。它需要最终确定，而且我没有足够的经验来了解如何正确实现它。 这是我的代码： nn.py https://codeshare.io/KWByyK memory.py https://codeshare.io/obp99r deep_cfr.py https://codeshare.io/k0zAAA game_tree.py https://codeshare.io/mP6MMp utils.py https://codeshare.io/ldQppd 对于游戏，我从这个库中获取了 texasholdem：https://github.com/SirRender00/texasholdem。    提交人    /u/silenthnowakeup   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</guid>
      <pubDate>Tue, 03 Sep 2024 01:11:53 GMT</pubDate>
    </item>
    <item>
      <title>“运动物理学”及其对人类模仿学习的启示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</guid>
      <pubDate>Mon, 02 Sep 2024 21:09:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Gymnasium 创建自定义环境来训练 CloudSimPlus 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</link>
      <description><![CDATA[基本上，我从基于 Java 的模拟器（CloudSimPlus）中提取指标，并使用 ProcessBuilder 将其发送到 py 脚本，该脚本反过来应该做出响应，决定如何更改云中的基础设施 我想要构建一个环境，其中状态表示为 5 个单独的离散值，并使用 3 种可能的操作训练代理 我已经按照这里的建议尝试了一个基本版本，但现在有点困惑，因为它没有按照我的意图工作    提交人    /u/Automatic_You_1939   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</guid>
      <pubDate>Mon, 02 Sep 2024 08:57:14 GMT</pubDate>
    </item>
    </channel>
</rss>