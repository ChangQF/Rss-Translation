<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 19 Aug 2024 09:17:15 GMT</lastBuildDate>
    <item>
      <title>“大脑在快速眼动睡眠期间模拟动作及其后果”，Senzai & Scanziani 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1evox75/the_brain_simulates_actions_and_their/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1evox75/the_brain_simulates_actions_and_their/</guid>
      <pubDate>Mon, 19 Aug 2024 00:50:43 GMT</pubDate>
    </item>
    <item>
      <title>“内隐学习过程中的走神与周期性脑电图活动增加和隐藏概率模式提取能力提高有关”，Simor 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1evotzq/mind_wandering_during_implicit_learning_is/</link>
      <description><![CDATA[        提交人    /u/gwern   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1evotzq/mind_wandering_during_implicit_learning_is/</guid>
      <pubDate>Mon, 19 Aug 2024 00:46:09 GMT</pubDate>
    </item>
    <item>
      <title>在 9x9 网格上收集奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ev6qhq/rewards_collecting_on_a_9x9_grid/</link>
      <description><![CDATA[嗨，我是深度学习新手。经过一番学习并完成了一些示例后，我尝试使用 pytorch 启动一个小项目，但很快就感到困惑。有人能告诉我这个项目是否适合用 DQN 解决吗？谢谢！ 详细信息：目标：在 9x9 网格（9 个 3x3 网格）上收集尽可能多的奖励游戏环境：  在每个游戏开始时，仅检查左上角的 3x3 网格并获得随机生成的奖励（每个网格有 9 分之 2 的机会获得奖励）。网格其他部分的奖励保持隐藏。 每一步，一个覆盖 3x3 网格内 4 个或 6 个网格的块都可以放置在任何已检查但未被占用的 3x3 网格上。玩家可以在 4/6 之间选择，但一旦选择，其形状将随机生成。放置一个方块后，将检查所有相邻的 3x3 网格，对角线相邻的网格除外。 当与方块重叠时，奖励被视为已收集，并且该方块连接到起始方块。只有 4 个网格的方块奖励将计为双倍。 9x9 网格上可以放置 9 个方块，放置 9 个方块后，玩家将获得 3 个额外的较小 1x1 方块，可放置在任何尚未占用的网格上。这 3 个方块可用于收集更多奖励或建立连接  放置 3 个小方块后游戏结束。  第一个问题是这个小游戏中的奖励是随机生成的，我做的所有例子都是固定奖励位置的，这是个问题吗？ 第二个问题是，最后的 3 个小方块的放置逻辑与前 9 个方块有很大不同，但对 9 个大方块的放置策略确实有一些影响。每场游戏大约有 1000~3000 种放置 3 个小方块的组合，虽然解决方案通常很明显，但对网络来说却并非如此。我在想这可能会给训练过程带来麻烦，我应该把它与 9 个方块的放置分开吗？ 如果有任何想法请分享，不胜感激！！！    提交人    /u/CancelDry9551   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ev6qhq/rewards_collecting_on_a_9x9_grid/</guid>
      <pubDate>Sun, 18 Aug 2024 11:10:23 GMT</pubDate>
    </item>
    <item>
      <title>有谁能帮忙解决这个问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ev5hhp/is_there_anyone_can_help_this_problem/</link>
      <description><![CDATA[      嗨，我是一名对机器人操控研究感兴趣的本科生。在阅读了一些调查后，似乎 DRL 最近正成为操控主题越来越重要的基础。所以我决定通过 Youtube 课程学习 RL 的基础知识。我目前正在观看 David Silver 的 RL 课程，虽然它有点旧了。 实际上，我想问几个我在学习过程中出现的问题：  为什么使用 TD 时 V(A) = six-eight？ 为什么 TD 需要 MDP 模型？这似乎是矛盾的，因为 TD 方法在线更新。但如果您可以构建 MDP 模型，则意味着您已经观察到了大量事件。  我不确定在这里问这类问题是否可以…… 感谢您的帮助！ https://preview.redd.it/9g9z0gsi9ejd1.png?width=1638&amp;format=png&amp;auto=webp&amp;s=55af9e82eeb1bc95eadb786faca9825e295e7b8f    由   提交  /u/Latter-Tomorrow-6850   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ev5hhp/is_there_anyone_can_help_this_problem/</guid>
      <pubDate>Sun, 18 Aug 2024 09:46:24 GMT</pubDate>
    </item>
    <item>
      <title>你好！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ev5a61/hi/</link>
      <description><![CDATA[我是菜鸟。很高兴认识你们！    提交者    /u/Latter-Tomorrow-6850   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ev5a61/hi/</guid>
      <pubDate>Sun, 18 Aug 2024 09:31:47 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero 和随机/不确定环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ev4i3o/alphazero_and_stochasticuncertain_environments/</link>
      <description><![CDATA[大家好， 我目前正在研究以随机 MDP 建模的随机环境中的 DRL。 我想分析像 AlphaZero 这样的基于模型的方法是否可以胜过使用 PPO 等随机策略的无模型方法。 假设我们面对一家工厂，其中必须以“良好”的顺序在机器上处理多个操作。该算法将尝试找到最佳序列以优化某个目标。这些机器可能会以一定的概率发生故障。 我们可以根据历史真实故障数据建立一个模型（即，在更换工具后发生故障的概率为 1%，并且这个百分比随着时间的推移增加到 20%，直到下一次更换工具）。 PPO 可能不会费力学习这一点，但与基于模型的方法相比，它可能需要更长的时间才能收敛（如果我错了，请纠正我）。 由于我们无法承受这种用例中的次优解决方案，因此我们需要在实际部署之前离线训练模型以建立基线。AlphaZero 可能可以利用如上所述的历史数据模型进行自我对弈。PPO 可能可以通过使用此模型进行模拟进行离线训练。 这两种算法都必须面对的最大问题是，环境动态可能会在部署后的某个时间发生变化，因此最初训练的算法需要通过在线学习进行适应。我们可以随时根据工厂的实时数据更新 alphazero 使用的模型。PPO 需要完全通过反复试验进行学习。 这里更合适的方法是什么？为什么？ 如果环境动态发生很大变化而不是逐渐变化，我们是否需要重新训练算法？ AlphaZero 能否利用其迭代学习，用一个在动态变化下表现更好的网络来替换先前的网络？ 此外，根据我对原始 AlphaZero 论文的理解，它无法开箱即用地处理随机环境。尽管如此，它已应用于 2048 等随机/随机的游戏。mcts 相对于 PPO 的优势是否会因环境动态变化而减弱？ 将问题建模为确定性 MDP，使用相当于秒/分钟的时间步长，并且仅考虑当前未发生故障的机器，也许会更好？    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ev4i3o/alphazero_and_stochasticuncertain_environments/</guid>
      <pubDate>Sun, 18 Aug 2024 08:35:05 GMT</pubDate>
    </item>
    <item>
      <title>新学习者 - 入门资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1euzw6t/new_learner_resources_to_get_started/</link>
      <description><![CDATA[您好， 我对学习强化学习和编写算法感兴趣。您认为哪些语言好，哪些库也适合初学者学习。 谢谢    提交人    /u/AlternativeExpress29   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1euzw6t/new_learner_resources_to_get_started/</guid>
      <pubDate>Sun, 18 Aug 2024 03:36:25 GMT</pubDate>
    </item>
    <item>
      <title>模仿是终极的奉承</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1euy90g/imitation_is_ultimate_flattery/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1euy90g/imitation_is_ultimate_flattery/</guid>
      <pubDate>Sun, 18 Aug 2024 02:05:15 GMT</pubDate>
    </item>
    <item>
      <title>自定义 PettingZoo MARL 环境的 MAPPO 不稳定</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1euqura/unstable_mappo_with_custom_pettingzoo_marl/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1euqura/unstable_mappo_with_custom_pettingzoo_marl/</guid>
      <pubDate>Sat, 17 Aug 2024 20:13:57 GMT</pubDate>
    </item>
    <item>
      <title>强化学习格斗游戏的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eup446/idea_for_reinforcement_learning_fighting_game/</link>
      <description><![CDATA[首先，我对强化学习还很陌生，但我读过一些论文，也上过一门普通的人工智能课程。 所以，我有一个关于格斗游戏的想法，其中敌方机器人是强化学习代理，而敌方力量是控制角色的真人。理想情况下，会有一个队列供人们玩，每个游戏都是一个情节。我想先让它尽可能简单，然后看看游戏会变得多复杂。 你们有没有关于从哪里开始这个项目的参考资料，或者对它听起来有多可信的反馈？谢谢你的建议！    提交人    /u/More-Potato-6718   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eup446/idea_for_reinforcement_learning_fighting_game/</guid>
      <pubDate>Sat, 17 Aug 2024 18:56:12 GMT</pubDate>
    </item>
    <item>
      <title>呼吁中级 RL 人员 - 您希望存在的视频/教程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1euj7ww/call_to_intermediate_rl_people_videostutorials/</link>
      <description><![CDATA[我正在考虑写一些博客文章/教程，可能也会以视频形式。我是一名 RL 研究人员/开发人员，所以这是我瞄准的主要主题。 我知道有很多 RL 教程。不幸的是，它们经常一遍又一遍地涵盖相同的主题。 问题是针对所有中级（甚至可能更低）RL 从业者 - 是否有任何特定主题您希望有更多关于它们的资源？ 我有很多自己的想法，特别是在我的特定领域，但我也想了解观众认为什么可能有用。因此，请放弃您希望存在但遗憾的是没有的任何教程主题！    提交人    /u/SmolLM   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1euj7ww/call_to_intermediate_rl_people_videostutorials/</guid>
      <pubDate>Sat, 17 Aug 2024 14:37:16 GMT</pubDate>
    </item>
    <item>
      <title>META 学习中的 RL - 是通往 AGI 的道路。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1euerpg/rl_in_meta_learning_is_the_way_to_agi/</link>
      <description><![CDATA[您如何看待 Google Deep Mind 的人形机器人学会走路？ 这是因为它的主要奖励是将前进速度增加到某个阈值。 现在想象一个机器人坐在/站在一个封闭的房间里。它附近有一个球，它有一个吸尘器、笔和纸。  情况 A。奖励由训练员预先调整，移动球从 0-100，清洁 100-1000，书写 - 1000-10k。并且机器人有元奖励来选择更好地增加奖励的活动。 情况 B。训练员没有预先调整任何特定奖励。元奖励是探索世界：找到世界的物理原理以及原因和结果。无论他用球、吸尘器、笔/纸做什么，如果它有助于探索世界，都会添加到奖励中。如果它发现了新的东西，奖励就会更强，如果更接近旧的经验，奖励就会更弱。  PS：最好从对世界有初步想法的 GPT 模型开始。    提交人    /u/Timur_1988   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1euerpg/rl_in_meta_learning_is_the_way_to_agi/</guid>
      <pubDate>Sat, 17 Aug 2024 10:42:26 GMT</pubDate>
    </item>
    <item>
      <title>魔方机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eu5i0j/rubiks_cube_bots/</link>
      <description><![CDATA[大家好！我只是好奇这个子版块上是否有很多人喜欢魔方，以及训练深度学习代理解决魔方是否是一种流行的练习。这感觉像是一个自然的强化学习问题，而且设置起来（足够）简单。或者可能比我想象的要难？    由   提交  /u/thebrilliot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eu5i0j/rubiks_cube_bots/</guid>
      <pubDate>Sat, 17 Aug 2024 01:12:06 GMT</pubDate>
    </item>
    <item>
      <title>关于将不可解析的参数传递给 gym 环境的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1etud8m/question_about_passing_unpickleable_argument_to/</link>
      <description><![CDATA[您好， 我有一个关于将参数传递给使用 make_vec_env 包装的自定义体育馆环境的问题。 如果我在 env_kwargs 中传递一些字符串参数，则以下代码有效。 venv0 = make_vec_env(WindFarmEnv_v1p3,vec_env_cls=SubprocVecEnv, n_envs=n_envs, seed=0, env_kwargs=env_kwargs,start_index = FFAddress) 但是，如果我尝试传递基于 cython 的类的实例（准确地说是 zeromq 服务器对象），我会收到错误，因为 stable-baselines3 似乎尝试 pickle env_kwargs 和基于 cython 的对象不可 pickle。完整的回溯包含在底部。 有没有办法绕过 env_kwargs 的 pickle 要求？如果没有，有没有办法以其他方式传递此类参数？ 谢谢    提交人    /u/dead_phoenix_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1etud8m/question_about_passing_unpickleable_argument_to/</guid>
      <pubDate>Fri, 16 Aug 2024 17:12:39 GMT</pubDate>
    </item>
    <item>
      <title>寻找适用于 MetaWorld 环境的 TD3/DDPG 的 MAML RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1etjf42/looking_for_maml_rl_implementation_with_td3ddpg/</link>
      <description><![CDATA[大家好， 我一直在寻找强化学习中模型无关元学习 (MAML) 的实现，该实现专门使用 TD3 或 DDPG 作为基础算法。到目前为止，我找到的唯一代码是 TRPO（像这个 one），但它不符合我的需求，因为我希望将其应用于 MetaWorld 环境或后来对其进行修改以与 HER 的目标条件环境一起使用。 是否有人知道任何现有的实现或有关如何修改现有 MAML 代码以与 TD3 或 DDPG 一起使用的提示？ 提前感谢任何帮助或指点！    提交人    /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1etjf42/looking_for_maml_rl_implementation_with_td3ddpg/</guid>
      <pubDate>Fri, 16 Aug 2024 08:21:55 GMT</pubDate>
    </item>
    </channel>
</rss>