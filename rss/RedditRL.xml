<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 05 Feb 2025 06:23:43 GMT</lastBuildDate>
    <item>
      <title>思考数据：我想知道我的想法是否可行。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</link>
      <description><![CDATA[你好。我很快就要开始学习计算机科学了（今年秋天或明年秋天，取决于我的大学何时允许我选择并专注于一个专业），但我想在人工智能最迷人的部分之一：强化学习方面取得突破。 我的计划：制作多个可以学习玩游戏的人工智能，然后将它们连接在一起，这样感觉就像一个人工智能。但这还不是全部。首先，它会从一个游戏开始，然后我将内存复制并粘贴（并最有可能对其进行一些修改）到另一个文件中，它将在其中玩另一个游戏，这样它就可以在已经知道基本控制的情况下快速启动。一段时间后，我会让它玩更高级的游戏，希望它知道大多数游戏都有类似的控制结构。 最终目标：拥有一个可以玩多个游戏的多用途人工智能，了解游戏无障碍指南，然后在一个文件中拆分无障碍审查。哦，是的，也许可以使用语言模型与我聊天。 在理想世界中，我会使用现有的 RL 代理（当然是在开发人员的许可下）来帮助加快该过程，并使用 LLM 与其聊天并获取仅玩游戏的 AI 无法提供的信息。 不幸的是，我有一台 MSI GF75 Thin，配备 Intel i5-10300h、NVIDIA GTX 1650（配备 4gh VRAM）和 32gb Ram。我认为很多都很好，除了显卡（我觉得即使没有尝试制作 AI 也缺乏显卡），所以我无法用我当前的设置做很多事情。但这是我想长期考虑的事情，因为有一天将我的想法付诸实践真的很酷。    提交人    /u/Octo_Chara   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</guid>
      <pubDate>Wed, 05 Feb 2025 01:11:40 GMT</pubDate>
    </item>
    <item>
      <title>研究实习生 - 欧洲</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihsdnf/research_intern_europe/</link>
      <description><![CDATA[不确定这是否是一个正确的子主题，但我想知道如何才能在欧洲找到 RL 的研究实习生职位。最好是德国。我不确定如何才能找到这样的职位，因为他们主要宣传为博士职位（如果有的话）。我的背景并不完全匹配，所以我宁愿先做实习生，然后再转为博士。但我应该在哪里寻找？我必须给实验室发冷邮件吗？我很少看到任何公开宣布的职位。我很感激任何建议。     提交人    /u/ProfileSad6040   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihsdnf/research_intern_europe/</guid>
      <pubDate>Tue, 04 Feb 2025 21:01:13 GMT</pubDate>
    </item>
    <item>
      <title>建立迷你法学硕士</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihlj8d/building_a_mini_llm/</link>
      <description><![CDATA[我正在考虑从头开始构建一个迷你 LLM。如何创建一个环境，让代理提供文本信息，并希望它使用阅读、总结和回答问题等 3 个动作进行学习     提交人    /u/iInventor_0134   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihlj8d/building_a_mini_llm/</guid>
      <pubDate>Tue, 04 Feb 2025 16:23:55 GMT</pubDate>
    </item>
    <item>
      <title>在 RL、决策智能应用方面有没有博士学位机会？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</link>
      <description><![CDATA[我是一名大四本科生，想申请 RL 或决策智能应用领域的直接攻读博士学位机会。  虽然我已经申请了一些大学，但我觉得我的机会很小。我已经后悔很久了，因为我去年没有跟踪申请或看清机会。如果你们当中有谁对 2025 年仍开放的直接攻读博士学位课程有所了解，请在这个 subreddit 中告诉我🙏    提交人    /u/Miserable_Ad2265   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</guid>
      <pubDate>Tue, 04 Feb 2025 14:59:07 GMT</pubDate>
    </item>
    <item>
      <title>体育馆 ClipAction 包装器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihhcbx/gymnasium_clipaction_wrapper/</link>
      <description><![CDATA[按照文档，有人能帮助我理解为什么使用包装器后 action_space 变成 Box(-inf, inf, (3,), float32) 吗？    提交人    /u/glitchyfingers3187   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihhcbx/gymnasium_clipaction_wrapper/</guid>
      <pubDate>Tue, 04 Feb 2025 13:13:27 GMT</pubDate>
    </item>
    <item>
      <title>PPO 陷入局部最优</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgtec/ppo_stuck_in_local_optima/</link>
      <description><![CDATA[大家好， 我正在做一个微电网问题，我之前用 DQN 完成了这个问题，结果还不错。 现在我正在用 PPO 解决相同的环境，但结果比 DQN 问题差（基线模型是 MILP）。 PPO 代理正在学习，但还不够好，我正在分享训练的图片。 https://imgur.com/a/GHHYmow MG 问题是在主电网价格低时对电池充电，在价格低时放电。 动作空间是 4 个电池的充电/放电（我稍后在电池中将其作为标准化形式，我将乘以 2.5，即最大 ch/disch）或者我应该初始化 -2.5 到 2.5 如果有帮助？ self.action_space = space.Box(low=-1, high=1, dtype=np.float32, shape=(4,))  为了将其保持在 -1 和 1 之间，我正在限制 NN 的平均值，然后在 -1 到 1 之间对操作进行采样，以确保电池充电/放电不会超出它，使用下面分享的这种方式。 mean = torch.tanh(mean) action = dist.sample()  action = torch.clip(action, -1, 1) 还有一件事，我对下面分享的 M 正态分布使用固定协方差，所有操作的协方差为 0.5。 dist = MultivariateNormal(mean, self.cov_mat) 请分享您的建议，我们将非常感谢并考虑。 如果您需要更多背景信息，请询问。    由   提交  /u/Dry-Image8120   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgtec/ppo_stuck_in_local_optima/</guid>
      <pubDate>Tue, 04 Feb 2025 12:45:10 GMT</pubDate>
    </item>
    <item>
      <title>关于 TRPO 论文的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</link>
      <description><![CDATA[      我正在研究 TRPO 论文，我有一个关于如何在以下优化问题中计算新策略的问题： https://preview.redd.it/l8fndz5ra4he1.png?width=940&amp;format=png&amp;auto=webp&amp;s=f49f53bedb23a9a6d04f6fbeaf79a643bde0052b 这个方程用于更新和寻找新策略，但我想知道如何计算π_θ(a|s)，因为它属于我们试图优化的策略——就像先有鸡还是先有蛋的问题。 论文提到，样本用于计算这个表达式：  1. 使用单路径或藤蔓程序收集一组状态-动作对以及它们的 Q 值的蒙特卡洛估计。 2. 通过对样本取平均值，构建公式 (14) 中的估计目标和约束。 3. 近似解决这个约束优化问题以更新策略的参数向量。我们使用共轭梯度算法，然后进行线搜索，这只比计算梯度本身稍微贵一点。详情请参阅附录 C。     提交人    /u/audi_etron   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</guid>
      <pubDate>Tue, 04 Feb 2025 12:38:38 GMT</pubDate>
    </item>
    <item>
      <title>在单台机器上运行 Ray Tune 的并行实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</link>
      <description><![CDATA[大家好，我是 Ray 的新手，这是一个流行的分布式计算框架，尤其适用于 ML，我一直致力于充分利用我有限的个人计算资源。这可能是我想要了解 Ray 及其库的主要原因之一。嗯，我相信许多学生和个人研究人员都有相同的动机。在使用 Ray Tune（全部基于 Python）进行一些实验后，我开始感到疑惑并想寻求帮助。任何帮助都将不胜感激！ 🙏🙏🙏：  Ray 在单台机器上仍然有效且高效吗？ 是否可以使用 Ray 在单台机器上运行并行实验（在我的情况下是 Tune）？ 我的脚本是否为此目的正确设置？ 我遗漏了什么吗？  故事：* 我的计算资源非常有限：一台配备 12 核 CPU 和 RTX 3080 Ti GPU 以及 12GB 内存的机器。 * 我的玩具实验没有充分利用可用资源：单次执行耗费 11% 的 GPU Util 和 300MiB /11019MiB。 * 从理论上讲，应该可以在这样的机器上同时进行 8-9 个这样的玩具实验一台机器。 * 自然而然，我求助于 Ray，希望它能帮助管理和运行具有不同超参数组的并行实验。 * 但是，根据下面的脚本，我没有看到任何并行执行，即使我在tune.run()中设置了max_concurrent_trials。根据我的观察，所有实验似乎一个接一个运行。到目前为止，我不知道如何修复我的代码以实现适当的并行性。 😭😭😭：* 以下是我的 ray tune 脚本（ray_experiment.py） ```python import os import ray from ray import tune from ray.tune import CLIReporter from ray.tune.schedulers import ASHAScheduler from Simulation import run_simulations # Ray Tune 中的可训练对象 from utils.trial_name_generator import trial_name_generator if name == &#39;ma​​in&#39;: ray.init() # 调试模式：ray.init(local_mode=True) # ray.init(num_cpus=12, num_gpus=1) print(ray.available_resources()) current_dir = os.path.abspath(os.getcwd()) # 当前目录的绝对路径params_groups = { &#39;exp_name&#39;: &#39;Ray_Tune&#39;, # 搜索空间 &#39;lr&#39;: tune.choice([1e-7, 1e-4]), &#39;simLength&#39;: tune.choice([400, 800]), } reporter = CLIReporter( metric_columns=[&quot;exp_progress&quot;, &quot;eval_episodes&quot;, &quot;best_r&quot;, &quot;current_r&quot;], print_intermediate_tables=True, ) analysis = tune.run( run_simulations, name=params_groups[&#39;exp_name&#39;], mode=&quot;max&quot;, config=params_groups, resources_per_trial={&quot;gpu&quot;: 0.25}, max_concurrent_trials=8, # scheduler=scheduler, storage_path=f&#39;{current_dir}/logs/&#39;, # 保存日志的目录 trial_dirname_creator=trial_name_generator, trial_name_creator=trial_name_generator, # resume=&quot;AUTO&quot; ) print(&quot;Best config:&quot;, analysis.get_best_config(metric=&quot;best_r&quot;, mode=&quot;max&quot;)) ray.shutdown()  ```    submitted by    /u/yxwmm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</guid>
      <pubDate>Tue, 04 Feb 2025 11:35:04 GMT</pubDate>
    </item>
    <item>
      <title>托盘装载问题 PPO 模型实际上不起作用 - 需要帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihcbpo/pallet_loading_problem_ppo_model_is_not_really/</link>
      <description><![CDATA[      因此，我正在研究一种 PPO 强化学习模型，该模型应该能够以最佳方式将箱子装载到托盘上。存在稳定性（可能悬垂 20%）和破碎（每个箱子都有一个破碎参数 - 您可以将箱子堆叠在具有更大破碎值的箱子顶部）约束。 我正在使用离散观察和行动空间。我为代理创建了一个可能位置列表，该列表传递了所有约束，然后代理有 5 种可能的操作 - 在位置列表中前进或后退、旋转箱子（仅在一个轴上）、放下一个箱子并跳过一个箱子并转到下一个。箱子先按破碎排序，然后按高度排序。 观察空间如下：托盘的高度图 - 您可以想象它就像从顶部看托盘 - 如果值为 0，则表示它是地面，1 - 托盘已填满。我曾尝试使用卷积神经网络，但没有任何改变。然后我有代理坐标（x，y，z）、箱子参数（长度、宽度、高度、重量、破碎）、接下来的 5 个箱子的参数、下一个位置、可能的位置数量、位置列表中的索引、剩余的箱子数以及箱子列表的索引。 我尝试了各种奖励函数，但都没有成功。目前我有这样的功能：无论如何在导航位置列表时 -0.1，对于与另一个箱子高度相等的箱子的每一条边 +0.5，如果在改变位置后这些边的数量更大，则对于接触另一个箱子的每一条边 +0.5。旋转时奖励相同，只比较最低位置和位置计数。选择下一个箱子时相同，但比较最低高度。最后，当放下一个盒子时，每个接触边+1或形成相等的高度和+3固定奖励。 我的神经网络由一个额外的层组成，用于观察不是高度图的观察（输出 - 256个神经元），然后是2个隐藏层，分别有1024和512个神经元，最后是演员评论家头。我对高度图和每个坐标进行了标准化。 我使用的超参数： learningRate = 3e-4 betas = [0.9, 0.99] gamma = 0.995 epsClip = 0.2 epochs = 10 updateTimeStep = 500 entropyCoefficient = 0.01 gaeLambda = 0.98 解决问题 - 我的模型无法收敛（从绘制统计数据可以看出，它似乎在采取随机动作。我已经调试了代码很长时间，似乎动作概率正在发生变化，损失计算正在正确完成，只是其他地方出了问题。可能是由于观察空间不好吗？神经网络架构？你会推荐使用 CNN 吗与卷积后的其他观察结果相结合？ 我附上了模型和统计数据的可视化。提前感谢您的帮助 https://preview.redd.it/kb9u2besp2he1.png?width=901&amp;format=png&amp;auto=webp&amp;s=b218e8573fd811d97cefcdd734a69590cbfd1dcd    提交人    /u/bimbum12   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihcbpo/pallet_loading_problem_ppo_model_is_not_really/</guid>
      <pubDate>Tue, 04 Feb 2025 07:25:36 GMT</pubDate>
    </item>
    <item>
      <title>“通过隐性奖励强化过程”，Cui 等人 2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</guid>
      <pubDate>Tue, 04 Feb 2025 06:21:10 GMT</pubDate>
    </item>
    <item>
      <title>第 7 版 Isaac Lab 教程发布！下一步我应该讲什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</link>
      <description><![CDATA[大家好！只是想顺便说一句，感谢大家对我的 Isaac Lab 教程的支持和鼓励。反馈非常棒，很高兴看到它们对你有多有用，老实说，我在制作它们的同时自己也学到了很多东西！ 我刚刚在不到 2 个月的时间内发布了我的第 7 个教程，我想保持这种势头。我现在将继续制作官方文档，但接下来你希望看到什么？ “从零到英雄”系列会很有趣吗？类似于： - 设计和在 Isaac Sim 中模拟机器人 - 在 Isaac Lab 中从头开始使用 RL 对其进行训练 - （最终）将其部署在真正的机器人上......一旦我买得起一个😅 让我知道您觉得最令人兴奋或最有帮助的内容！随时欢迎建议。 我在 YouTube 上上传了这些： Isaac Lab 教程 - LycheeAI    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</guid>
      <pubDate>Mon, 03 Feb 2025 22:16:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 进行布局生成（例如：道路和房屋）的最佳方法。当前模型未进行学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</link>
      <description><![CDATA[      我正在尝试使用 RL 生成简单郊区的布局：道路、障碍物和房屋。这更像是一个实验，但我最好奇的是想知道我是否有任何变化可以使用 RL 为此类问题提出合理的设计。 tensorboard 目前我已经解决了这个问题（使用 gymnasium 和 stable_baselines3）。我有一个简单的设置，其中有一个名为 env 的环境，我将我的世界表示为网格：  我从一个空网格开始，除了道路元素（入口点）和一些无法使用的单元格（障碍物，例如小湖） 模型采取的操作是，在每一步，放置一个道路或房屋的瓷砖。所以基本上（tile_position，tile_type）  至于我的奖励，它与整体设计相关（而不仅仅是对最后一步的奖励，因为早期的选择可能会对以后产生影响。并且最大化设计的整体质量，而不是局部的），基本上有 3 个加权项：  道路网络应该有意义：连接到入口，每个瓷砖应该连接到至少 1 个其他道路瓷砖。并且没有 2x2 的道路瓷砖集。-&gt; 整个设计（所有道路瓷砖）的总和（每个好瓷砖的奖励增加，每个坏瓷砖的奖励减少）。还尝试了所有瓷砖的 min() 分数。 房屋应始终连接到至少 1 条道路。-&gt; 整个设计（所有房屋瓷砖）的总和（每个好瓷砖的奖励增加，每个坏瓷砖的奖励减少）。还尝试了所有瓷砖的 min() 分数。 最大化房屋瓷砖的数量（瓷砖越多，奖励就越高）  每当我尝试运行它并让它学习时，我都会从较低的 entropy_loss（-5，在 100k 步后慢慢爬升至 0）和基本上为 0 的 explained_variance 开始。我的理解是：模型永远无法正确预测它采取的给定动作的奖励是什么。并且它采取的行动并不比随机好。 我对 RL 还很陌生，我的背景更“传统” ML、NLP，并且非常熟悉进化算法。 我认为这可能只是一个冷启动问题，或者课程学习可能会有所帮助。但即使如此，我也从简单的设计开始。例如 6x6 网格。我觉得这更多的是我的奖励函数设计方式的问题。或者也许与我如何构建问题有关。 ------ 问题：在这种情况下，您通常如何处理这样的问题？有了它，有哪些标准方法可以“调试”此类问题？例如，看看问题是否更多地与我选择的操作类型有关，或者与我的奖励设计方式等有关    提交人    /u/LostInGradients   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</guid>
      <pubDate>Mon, 03 Feb 2025 21:00:15 GMT</pubDate>
    </item>
    <item>
      <title>结果的可重复性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</link>
      <description><![CDATA[您好！我正在尝试查找本文中提到的基于模型的 PPO 的实现：基于模型的探索的策略优化，以便重现结果并可能在我的论文中使用该架构。但似乎任何地方都没有官方实现。我已经给作者发了电子邮件，但也没有收到任何回复。 在像 AAAI 这样的大型会议上发表的论文没有任何可重现的实现，这正常吗？    提交人    /u/GamingOzz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</guid>
      <pubDate>Mon, 03 Feb 2025 13:18:10 GMT</pubDate>
    </item>
    <item>
      <title>第一届 Tinker AI 大赛优胜作品！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</link>
      <description><![CDATA[        提交人    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</guid>
      <pubDate>Mon, 03 Feb 2025 10:00:02 GMT</pubDate>
    </item>
    <item>
      <title>尝试复制普通的 k-bandits 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</link>
      <description><![CDATA[大家好， 我正在尝试实现 Barto Sutton 书中的第一个 k-Bandits 测试平台。Python 代码可在 Git 上找到，但我正尝试从头开始独立完成。 截至目前，我正在尝试生成图 2.2 中的平均奖励图。我的代码可以工作，但平均奖励图过早稳定下来，并且保持稳定，而不是像书中/git 中那样增加。我无法弄清楚我哪里做错了。 如果有人能看一下并分享一些技巧，那将非常有帮助。如果有人想运行/测试它，代码应该按原样工作。  非常感谢！ ``` 该程序实现了 k-bandit 问题的 n 次运行 import numpy as np import matplotlib.pyplot as plt bandit_reward_dist_mean = 0 bandit_reward_dist_sigma = 1 k_bandits = 10 bandit_sigma = 1 samples_per_bandit = 1000 epsilon = 0.01 def select_action(): r = np.random.randn() if r &lt; epsilon：action = np.random.randint（0，k_bandits）else：action = np.argmax（q_estimates） 返回操作 def update_action_count（A_t）：# 到目前为止已采取每个操作的次数n_action [A_t] + = 1 def update_action_reward_total（A_t，R_t）：# 到目前为止每个操作的总奖励action_rewards [A_t] + = R_t def generate_reward（mean，sigma）：# 从正态分布中为这个特定的bandit提取奖励#r = np.random.normal（mean，sigma）r = np.random.randn（）+mean# 类似于在Git repo中所做的return r def update_q（A_t，R_t）： q_estimates[A_t] += 0.1 * (R_t - q_estimates[A_t]) n_steps = 1000 n_trials = 2000 #每次试验使用一批新的老虎机运行 n_steps 所有试验中每一步的奖励矩阵 - 从零开始 rewards_episodes_trials = np.zeros((n_trials, n_steps)) for j in range(0, n_trials): #q_true = np.random.normal(bandit_reward_dist_mean, bandit_reward_dist_sigma, k_bandits) q_true = np.random.randn(k_bandits) # 尝试复制 book/git 结果 # 每个动作（老虎机）的 Q 值 - 从random q_estimates = np.random.randn(k_bandits) # 每个动作（bandit）的总奖励 - 从零开始 action_rewards = np.zeros(k_bandits) # 到目前为止每个动作已采取的次数 - 从零开始 n_action = np.zeros(k_bandits) # 每一步的奖励 - 从 0 开始 rewards_episodes = np.zeros(n_steps) for i in range(0, n_steps): A_t = select_action() R_t = generate_reward(q_true[A_t], bandit_sigma) rewards_episodes[i] = R_t  update_action_reward_total(A_t, R_t) update_action_count(A_t) update_q(A_t, R_t) rewards_episodes_trials[j,:] = rewards_episodes  所有运行中每步的平均奖励 average_reward_per_step = np.zeros(n_steps) for i in range(0, n_steps): average_reward_per_step[i] = np.mean(rewards_episodes_trials[:,i]) plt.plot(average_reward_per_step) plt.show() ```    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</guid>
      <pubDate>Mon, 03 Feb 2025 09:35:25 GMT</pubDate>
    </item>
    </channel>
</rss>