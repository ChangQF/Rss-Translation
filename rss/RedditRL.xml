<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 22 Feb 2025 15:15:57 GMT</lastBuildDate>
    <item>
      <title>RL解决多个机器人问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用在共享环境中导航的多个移动机器人进行模拟。每个机器人都有一个预先加载的空间地图，并使用范围传感器（例如飞行传感器的时间）进行本地化。最初的全球路径计划是为每个机器人独立完成的，而无需考虑其他机器人。一旦开始移动，他们就可以检测到附近的机器人位置，速度和计划的途径以避免碰撞。 问题是，在紧密的空间中，他们经常被困在一种僵局中。在没有机器人可以移动的地方，他们都互相阻挡。一个人可以很容易地看到，如果说，1个机器人会向前移动一点，另一个机器人向前移动并转动一点，其余的都可以清除。但是在基于规则的系统中编码这种逻辑非常困难。 我正在考虑使用ML/ RL来解决此问题，但是我想知道这是否是一种实用方法。有没有人尝试解决RL的类似问题？您将如何处理？很想听听您的想法。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/</guid>
      <pubDate>Sat, 22 Feb 2025 11:41:21 GMT</pubDate>
    </item>
    <item>
      <title>我如何学习新手的模型预测控制。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivgjbg/how_can_i_learn_model_predictive_control_as_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是对控制方案的新手。我有一项在倒摆上实施的MPC任务。我需要学习。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/on_yesterday_2539      [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivgjbg/how_can_i_learn_model_predictive_control_as_a/</guid>
      <pubDate>Sat, 22 Feb 2025 11:14:47 GMT</pubDate>
    </item>
    <item>
      <title>GRPO与进化策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iv6ui7/grpo_vs_evolution_strategies/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   grpo看起来不像（或可以从在这里？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/majestic-tap1577     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iv6ui7/grpo_vs_evolution_strategies/</guid>
      <pubDate>Sat, 22 Feb 2025 01:12:41 GMT</pubDate>
    </item>
    <item>
      <title>上下文强盗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iv4v8m/contextual_bandit/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们有一个用例，我们想在其中个性化向用户显示的消息。也有一层资格，就像用户有资格获取消息一样。我们没有历史数据，也将经常添加新消息。上下文匪徒似乎是对这种用例的一个很好的答案。  当前系统正在使用消息的手动排名，因此数据很大程度上偏向于使用它。  我们遇到的问题是：   上下文强盗是适当的解决方案吗？    我们如何脱机和在线评估？   我想在mab     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/distuctionatorAdept118     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iv4v8m/contextual_bandit/</guid>
      <pubDate>Fri, 21 Feb 2025 23:21:36 GMT</pubDate>
    </item>
    <item>
      <title>多机构学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iv2jbn/multiagent_learning/</link>
      <description><![CDATA[在，游戏理论（决策理论），信息理论和动力学＆amp;控制。但是，我正在努力在该领域绘制一个清晰的研究路线图。感觉仍然像是一个相对较新的领域，当我遇到麻省理工学院的课程 多基金会学习中的主题 Gabriele Farina （看起来很棒！），我不确定绝对必要的领域我需要首先加强。 有点关于我：   背景：动态系统＆amp;控制  当前重点：学习深入强化学习  其他兴趣：认知科学（尤其是学习＆amp;决策）; 以前的竞争乒乓球运动员  当前状态：机器人技术中的PhD学生，但对我当前的项目感到非常无聊并渴望探索多构成系统并在其中建立职业。   如果您冒险进入多重RL，您是如何构建学习路径的？您会说哪些领域对于该领域的研究最关键？如果您有类似的兴趣，我很想听听您的想法！ 谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neat_comparison_2726      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iv2jbn/multiagent_learning/</guid>
      <pubDate>Fri, 21 Feb 2025 21:40:49 GMT</pubDate>
    </item>
    <item>
      <title>更改Pettingzoo奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iustf3/change_pettingzoo_reward_function/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在使用rllib的pettingzoo国际象棋env和PPO，但想适应我的问题。我想完全更改奖励功能。这可能在Pettingzoo或rllib之一中，如果是，我该怎么做？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/livid-ant3549     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iustf3/change_pettingzoo_reward_function/</guid>
      <pubDate>Fri, 21 Feb 2025 14:58:32 GMT</pubDate>
    </item>
    <item>
      <title>RL在监督学习中？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iuqwr7/rl_in_supervised_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！ 我对DRL有一个问题。我已经看到了有关在“入侵检测”，“异常检测”，“欺诈检测”等任务中使用DRL的几篇论文标题和新闻典型的监督学习，尽管根据我所读过的“ DRL是一种很好的技术，对这种任务都有良好的结果”。检查例如 https://www.cyberdb.co/top-5-deep-learning-techniques-for-enhancing-cyber-theat-detection/#: text = deep%20Revermention%20Rections%20Learningmentimpearmentilem ％20学习％20 from％20 their％20环境  问题是，在这些情况下，更具体地说，国家及其进化是如何建立DRL问题？代理的操作是明确的（例如，将数据标记为异常，什么都不做或将其标记为普通数据），但是由于我们处理数据集或数据集的集合，因此这些数据是不变的，不是吗？在这些情况下，它如何可能或如何完成，以使DRL系统的状态随代理的行为而变化？这很重要，因为它是马尔可夫决策过程的关键属性，因此是DRL系统的关键属性，不是吗？ 非常感谢您  &lt;！ - -SC_ON-&gt;＆＃32;提交由＆＃32; /u/u/carpoforo     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iuqwr7/rl_in_supervised_learning/</guid>
      <pubDate>Fri, 21 Feb 2025 13:29:36 GMT</pubDate>
    </item>
    <item>
      <title>RL不和谐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iukm6y/rl_discord/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我看到人们说他们想要一个rl学习组，但没有一个不和谐，所以我决定做一个，如果你想要你想要&lt; a href =“ https://discord.gg/xu36gsht”&gt; https://discord.gg/xu36gsht     &lt;！ - sc_on-&gt;＆＃32;提交由＆＃32; /u/u/damrstick     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iukm6y/rl_discord/</guid>
      <pubDate>Fri, 21 Feb 2025 06:37:53 GMT</pubDate>
    </item>
    <item>
      <title>非LLM RL博士学位的就业市场</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu8jos/job_market_for_nonllm_rl_phd_grads/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  传统RL PHD毕业生的当前市场如何（DEEP RL，RL理论）？任何人都想分享求职经验？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hmi2015    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iu8jos/job_market_for_nonllm_rl_rl_phd_grads/”&gt; [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu8jos/job_market_for_nonllm_rl_phd_grads/</guid>
      <pubDate>Thu, 20 Feb 2025 20:31:30 GMT</pubDate>
    </item>
    <item>
      <title>分布参与者批评</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu7z8i/distributional_actorcritic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我真的很喜欢分布强化学习的想法。我已经阅读了C51和QR-DQN论文。 IQN是我的列表中的下一个。 一些演员 - 批判算法将Q值学习为评论家吗？我认为，这是SAC，TD3和DDPG的算法，对吗？这是一个有希望的方向吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu7z8i/distributional_actorcritic/</guid>
      <pubDate>Thu, 20 Feb 2025 20:08:03 GMT</pubDate>
    </item>
    <item>
      <title>人形步态训练Isaacgym＆Motion Mimitation</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu7u8z/humanoid_gait_training_isaacgym_motion_imitation/</link>
      <description><![CDATA[在：//smpl.is.tue.mpg.de/“&gt; https://smpl.is.tue.mpg.de/ ）走路并一直在某些问题中运行。我选择实施PPO来训练以人形生物状态（联合DOF，脚力传感器等）读取的政策，并以任何位置（ISAACGYM PD控制器接管）或基于扭矩的驱动作用。然后，我设计了奖励功能，包括：（1）正向速度（2）直立姿势（3）脚触点交替（4）对称运动 （5）Hyprextension约束（6）骨盆身高稳定性（7）脚滑罚&lt; /p&gt; 使用这种方法，我尝试了多次训练运行，每个训练效果不同，结果不同， IE。我没有看到任何甚至遥不可及的向前运动的事物的实际融合，更不用说是自然步态了。我在以前的RL段之上构建了它。 mocap步行数据（Amass DataSet  https://amass.is.tue.mpg.de/ ）。当我在ISAACGYM培训约1000个环境时，我会在每个环境中加载唯一的设置序列长度情节，并包括他们的“性能”。 使用这种方法模仿动作集作为奖励的一部分。 &lt; /p&gt; 以下是我注意到的有关培训的现象：（1）训练很快收敛。我正在运行1000个环境，每个时期的300步序列长度，每个时期5个网络更新，并且在第一个时期内观察收敛（融合到性能差）。（2）我的价值损失非常高，例如12个订单由于损失政策，我目前正在研究这一点。 有人在这种培训方面有任何经验还是对解决方案有任何建议？ 谢谢多！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricesLearning/comments/1iu7u8z/humanoid_gait_gait_training_isaacgym_motion_imitation/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu7u8z/humanoid_gait_training_isaacgym_motion_imitation/</guid>
      <pubDate>Thu, 20 Feb 2025 20:02:31 GMT</pubDate>
    </item>
    <item>
      <title>RL博士学位的主题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iu2x0x/themes_for_phd_in_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿！ 简介。我在2024年获得了硕士学位。我的研究生工作认为学习机器人是为了避免使用熊猫和pybullet模拟障碍。目前，我在金融领域担任ML工程师，从事经典ML，主要是推荐系统。 最近，我在同一所获得BS和MS的大学开始了我的博士学位课程。自2024年秋天以来，我一直在这样做。我很好奇RL算法及其应用程序，特别是机器人技术。到目前在模拟中创建的副本。我计划进行一些实验，以控制它，以解决一些基本任务，例如到达对象，将其放入盒子中。我想写第一篇论文。后来，我计划更深入地进入该领域，并进行更多实验。此外，我将对RL中的当前状态进行一些分析，并可能也撰写有关它的出版物。 我决定去学习博士学位，主要是因为我想从一边有额外的动力学习RL（因为很难不放弃），写一些论文（因为在ML Sphere中有一些有用的论文），然后进行一些实验。将来，如果我有这样的机会，我想使用RL，机器人或自动驾驶汽车。因此，我在这里不是要做很多学术工作，而是为我的个人教育，未来的职业和行业业务做更多的事情。  但是，我的主要研究人员更多地是工程的东西，而且还很老。这意味着她可以就如何正确进行研究给我很多建议，但是她对RL和AI领域的了解并不深刻。我几乎一个人做。 所以我想知道是否有人可以就考虑RL和Robotics的研究主题提出一些建议？有没有社区可以与他人分享利益？如果有人对合作感兴趣，我很想进行对话，并且可以分享联系人  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/alex_werben     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iu2x0x/themes_for_phd_in_rl/</guid>
      <pubDate>Thu, 20 Feb 2025 16:44:48 GMT</pubDate>
    </item>
    <item>
      <title>增强学习的书籍[代码+理论]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itxbjj/books_for_reinforcement_learning_code_theory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ 该代码似乎有些复杂，因为很难编程我在RL中介绍的初始理论。  关于加固学习，哪些书可以阅读以了解代码以及代码部分。  另外，阅读RL理论和概念的时间是多少，一个人可以开始编码rl。 请告诉我！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/internationalwill912     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itxbjj/books_for_reinforcement_learning_code_theory/</guid>
      <pubDate>Thu, 20 Feb 2025 12:26:38 GMT</pubDate>
    </item>
    <item>
      <title>最佳RL存储库具有简单的SOTA算法实现，这些算法易于编辑？ （最好在JAX中）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ituera/best_rl_repo_with_simple_implementations_of_sota/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/godireallyhateyoutim     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ituera/best_rl_repo_with_simple_implementations_of_sota/</guid>
      <pubDate>Thu, 20 Feb 2025 09:11:51 GMT</pubDate>
    </item>
    <item>
      <title>对于那些通过模拟进行加固学习（RL）的人，我已经在NVIDIA ISAAC实验室上播放了10个视频</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itu6na/for_those_looking_into_reinforcement_learning_rl/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32;态href =“ https://www.youtube.com/watch?v=sl1wcfp9tru＆amp；   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itu6na/for_those_looking_into_reinforcement_learning_rl/</guid>
      <pubDate>Thu, 20 Feb 2025 08:55:36 GMT</pubDate>
    </item>
    </channel>
</rss>