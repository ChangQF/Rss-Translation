<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯æ—¨åœ¨æ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå’Œå­¦ä¹ å¦‚ä½•æœ€ä½³è·å¾—å¥–åŠ±çš„AI/ç»Ÿè®¡æ•°æ®çš„å­åœºã€‚ä¾‹å¦‚Alphagoï¼Œä¸´åºŠè¯•éªŒå’ŒA/Bæµ‹è¯•ä»¥åŠAtariæ¸¸æˆã€‚</description>
    <lastBuildDate>Tue, 11 Mar 2025 09:19:31 GMT</lastBuildDate>
    <item>
      <title>Renforceui Studioç°åœ¨æ”¯æŒDQNå’Œç¦»æ•£çš„åŠ¨ä½œç©ºé—´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j8lndf/reinforceui_studio_now_supports_dqn_discrete/</link>
      <description><![CDATA[       &lt;ï¼&lt;ï¼ -  sc_off-&gt;      Readforceui Studioç°åœ¨æ”¯æŒDQNï¼†amp;ç¦»æ•£çš„åŠ¨ä½œç©ºé—´ï¼ ğŸ‰  å¤§å®¶å¥½ï¼Œ æ­£å¦‚æˆ‘åœ¨å¼€æº GUIæ—¨åœ¨ç®€åŒ–RLåŸ¹è®­ï¼Œé…ç½®å’Œç›‘è§†ï¼Œè€Œæ²¡æœ‰æ›´å¤šçš„å‘½ä»¤è¡Œæ–—äº‰ï¼æœ€åˆï¼Œæˆ‘ä»¬ä¸“æ³¨äºè¿ç»­çš„åŠ¨ä½œç©ºé—´ï¼Œä½†æ˜¯ä½ ä»¬ä¸­çš„è®¸å¤šäººéƒ½è¦æ±‚æ”¯æŒ dqnå’Œç¦»æ•£çš„åŠ¨ä½œç©ºé—´ç®—æ³•  - æ‰€ä»¥è¿™å°±æ˜¯ï¼ ğŸ•¹ï¸ âœ¨æœ‰ä»€ä¹ˆæ–°é²œäº‹ç‰©ï¼Ÿ   dqnï¼†amp;ç¦»æ•£çš„åŠ¨ä½œç©ºé—´æ”¯æŒ  - è®­ç»ƒå’Œå¯è§†åŒ–ç¦»æ•£çš„RLæ¨¡å‹ã€‚ æ›´å¤šçš„ç¯å¢ƒå…¼å®¹æ€§  - æ‰©å±•ä¸ä»…ä»…æ˜¯è¿ç»­çš„åŠ¨ä½œç¯å¢ƒã€‚   å°è¯•ä¸€ä¸‹ï¼  githubï¼š https://github.com/github.com/github.com/dvalenciar/dvalenciar/reinforceui-reinforceui-studio-studio--reinforceui-studio-studio  https://docs.reinforceui-studio.com/welcome     è®©æˆ‘çŸ¥é“å…¶ä»–RLç®—æ³•æ‚¨å¯ä»¥çœ‹åˆ°ä»€ä¹ˆï¼æ‚¨çš„åé¦ˆæœ‰åŠ©äºå¡‘é€ å¢å¼ºå·¥ä½œå®¤ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¢å¼ºå¼å·¥ä½œå®¤æ”¯æŒä»¥ä¸‹ç®—æ³•ï¼š        algorithM   ctd4  è¿ç»­çš„åˆ†å¸ƒå‚ä¸è€… - æ‰¹åˆ¤å‰‚ä¸å¤šä¸ªè¯„è®ºå®¶çš„kalmanèåˆ      ddpg  ddpg  align =â€œ leftâ€&gt; dqn  æ·±q-network      ppo   ofximal politization&gt;è¿‘ç«¯ç­–ç•¥plocization                actor-Critic     td3  åŒå»¶è¿Ÿçš„æ·±å±‚ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦        tqc  tqc è¯„è®ºå®¶     &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/dvr_dvr     [link]  &lt;a href =â€œ https://www.reddit.com/r/reinforecriceslearning/comments/1j8lndf/reinforceui_studio_now_now_supports_dqn_dqn_discrete/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j8lndf/reinforceui_studio_now_supports_dqn_discrete/</guid>
      <pubDate>Tue, 11 Mar 2025 07:56:26 GMT</pubDate>
    </item>
    <item>
      <title>é€šè¿‡å¢å¼ºå­¦ä¹ å­¦ä¹ ä¿„ç½—æ–¯æ–¹å—</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j8ek56/learning_tetris_through_reinforcement_learning/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  åˆšå®Œæˆäº†æˆ‘çš„ç¬¬ä¸€ä¸ªRLé¡¹ç›®ã€‚é‚£äº›AIå­¦ä¹ å¦‚ä½•ç©æ¸¸æˆçš„è§†é¢‘æ€»æ˜¯çœ‹èµ·æ¥å¾ˆæœ‰è¶£ï¼Œæ‰€ä»¥æˆ‘æƒ³è¯•ä¸€è¯•ã€‚æˆ‘çš„githubä¸Šæœ‰ä¸€ä¸ªæ¼”ç¤ºè§†é¢‘ã€‚æˆ‘æœ‰GPTå¸®åŠ©åœ¨READMEä¸­ç»„ç»‡æˆ‘çš„æ€è€ƒè¿‡ç¨‹ã€‚å¦‚æœä»äº‹ç±»ä¼¼é¡¹ç›®çš„å·¥ä½œï¼Œä¹Ÿè®¸å…¶ä»–äººå¯ä»¥å‘ç°æœ‰ç”¨çš„ä¸œè¥¿ã€‚æˆ‘å¯¹è¿™ä¸ªè¯é¢˜éå¸¸æ–°é²œï¼Œå› æ­¤æ¬¢è¿ä»»ä½•åé¦ˆã€‚  httpsï¼š//github.com/truong.com/truonging/truonging/tetris-a.i  æäº¤ç”±ï¼†ï¼ƒ32; /u/truonging     [link]        [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j8ek56/learning_tetris_through_reinforcement_learning/</guid>
      <pubDate>Tue, 11 Mar 2025 00:52:02 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨GSM8Kæ•°æ®é›†å°†GRPOåº”ç”¨äºQWEN-0.5Bæ•™å­¦ï¼Œæœ€ç»ˆå°†è¾“å‡ºä½è¡¨ç°çš„æŒ‡ä»¤æ¨¡å‹ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j8cwzx/applying_grpo_to_qwen05binstruct_using_gsm8k/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¯¹äºä¸Šä¸‹æ–‡ï¼šä¸Šå‘¨æˆ‘åˆšåˆšé˜…è¯»å¹¶äº†è§£äº†GRPOã€‚æœ¬å‘¨ï¼Œæˆ‘å†³å®šé€šè¿‡åœ¨GSM8Kæ•°æ®é›†ä¸­åŸ¹è®­QWEN-0.5Bæ•™å­¦æ¥åº”ç”¨è¿™ç§æ–¹æ³•ã€‚ä½¿ç”¨TRLçš„grpotrainerï¼Œæˆ‘è®¾ç½®äº†2ä¸ªè®­ç»ƒæ—¶æœŸå’Œå‚è€ƒæ¨¡å‹æ¯25ä¸ªæ­¥éª¤åŒæ­¥ã€‚ I only used two reward functions: strict formatting (i.e., must follow &lt;reasoning&gt;...&lt;/reasoning&gt;&lt;answer&gt;...&lt;/answer&gt; format) and accuracy (i.e., must output the correct answer). However when I tried to ask it a simple question after training phase was done, it wasn&#39;t able to answer it.å®ƒåªæ˜¯å›ç­”\ nï¼ˆnewlineï¼‰å­—ç¬¦ã€‚æˆ‘æ£€æŸ¥äº†å¥–åŠ±åŠŸèƒ½çš„å›¾ï¼Œå®ƒä»¬æ˜¯â€œç¨³å®šçš„â€ã€‚åœ¨åŸ¹è®­ç»“æŸæ—¶1.0ã€‚ æˆ‘é”™è¿‡äº†ä»€ä¹ˆå—ï¼Ÿæƒ³å¬å¬ä½ çš„æƒ³æ³•ã€‚è°¢è°¢ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1j8cwzx/applying_grpo_grpo_to_qwen05binstruct_using_gsm8k/â€&gt; [link]  &lt;a href =â€œ https://www.reddit.com/r/reinforevectionlearning/comments/1j8cwzx/applying_grpo_grpo_to_qwen05binstruct_using_gsmusit_gsm8k/â€]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j8cwzx/applying_grpo_to_qwen05binstruct_using_gsm8k/</guid>
      <pubDate>Mon, 10 Mar 2025 23:33:59 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨RLæ¢ç´¢ç”µåŠ›å¸‚åœºç«æ ‡ä¸­çš„NASHå¹³è¡¡ - å¯»æ±‚åé¦ˆ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j88v0y/exploring_nash_equilibria_in_electricity_market/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨ç ”ç©¶ä¸€ä¸ªç ”ç©¶é¡¹ç›®ï¼Œæˆ‘ä»¬æ—¨åœ¨åœ¨ä½¿ç”¨åŠ å¼ºå­¦ä¹ çš„ç”µåŠ›å¸‚åœºç«æ ‡ä¸­æ¢ç´¢ nash equilibria ã€‚æ ¸å¿ƒé—®é¢˜æ˜¯ï¼š  â€œåœ¨ç«äº‰æ€§çš„ç”µåŠ›å¸‚åœºä¸­ï¼Œä»£ç†äººè‡ªç„¶ä¼šåƒå¤å…¸ç»æµç†è®ºé‚£æ ·ç«æ ‡å…¶ç”Ÿäº§æˆæœ¬ï¼Ÿ Or does strategic behavior emerge, leading to a different market equilibrium?&quot; Approach  Baseline Model (Perfect Competition &amp; Social Welfare Maximization):  We first model the electricity market using Pyomo, solving an optimization problem where all agents (generators and consumers) bid their  trueæˆæœ¬ã€‚ è¿™ä¼šå¯¼è‡´æœ€ä½³è°ƒåº¦ï¼Œæœ€å¤§åŒ–ç¤¾ä¼šç¦åˆ©å¹¶ç”¨ä½œåŸºå‡†ã€‚           rlibï¼‰å…è®¸ä»£ç†äººå­¦ä¹ ä»–ä»¬çš„æœ€ä½³æ‹›æ ‡ç­–ç•¥ã€‚ æ¯ä¸ªä»£ç†å•†éƒ½ä¼šæäº¤å‡ºä»·ï¼Œé€šè¿‡ pyomo è¿›è¡Œç«æ ‡ï¼Œå¹¶æ ¹æ®åˆ©æ¶¦åˆ†é…å¥–åŠ±ã€‚å¹³è¡¡æ²¡æœ‰ä»£ç†å¯ä»¥å•æ–¹é¢æ”¹è¿›ã€‚     æ¯”è¾ƒï¼†amp;è§è§£ï¼š  æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäº rlçš„NASHå¹³è¡¡ä¸å®Œç¾ç«äº‰åŸºå‡†  è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¯„ä¼°æˆ˜ç•¥æ€§ç«æ ‡å¸‚åœºæ˜¯å¸‚åœºæ“çºµæˆ–æ•ˆç‡           å°†æ¨¡å‹æ‰©å±•åˆ°å¤šå¤„æ‹å–ï¼Œä»£ç†å•†ä¼šéšç€æ—¶é—´çš„æµé€å­¦ä¹ æœ€ä½³ç­–ç•¥ã€‚ æ¢ç´¢æ··åˆç«äº‰æ€§åˆä»¶è®¾ç½®ï¼Œå½“åœ°ç¤¾åŒºä¸­çš„ä»£ç†å•†åœ¨å½“åœ°ç¤¾åŒºä¸­çš„ä»£ç†å•†è¿›è¡Œäº†åˆä½œï¼Œä½†ä¸å…¶ä»–ç¤¾åŒºç«äº‰ã€‚ç«æ ‡ã€‚  å¯»æ‰¾åé¦ˆï¼  æ‚¨æ˜¯å¦åœ¨å¸‚åœºæ¨¡æ‹Ÿä¹‹å‰å¤šä»£ç†RL ï¼Ÿ   åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹è¿›è¡Œå»ºæ¨¡åœ¨è¿™ç§æƒ…å†µä¸‹è¿›è¡Œå»ºæ¨¡çš„å»ºè®®ï¼Ÿ   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/iminderent-milk5530     [link]     [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j88v0y/exploring_nash_equilibria_in_electricity_market/</guid>
      <pubDate>Mon, 10 Mar 2025 20:42:57 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆåœ¨SARSAç®—æ³•çš„éæ”¿ç­–n-Stepç‰ˆæœ¬ä¸­ï¼Œé‡è¦æ€§é‡‡æ ·ç‡ä¸ä»…ä¼šä½¿æ•´ä¸ªé”™è¯¯ä¹˜ä»¥æ•´ä¸ªé”™è¯¯ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j85tzj/why_in_the_offpolicy_nstep_version_of_sarsa/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¯¹æˆ‘çš„ç†è§£ï¼Œæˆ‘ä»¬ä½¿ç”¨é‡è¦çš„é‡‡æ ·æ¯”&#39;rho;åŠ é‡åœ¨éµå¾ªè¡Œä¸ºæ”¿ç­–æ—¶è§‚å¯Ÿåˆ°çš„å›æŠ¥ã€‚æ ¹æ®ç›®æ ‡ç­–ç•¥â€œ piâ€è§‚å¯Ÿç›¸åŒè½¨è¿¹çš„æ¦‚ç‡ã€‚ç„¶åï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘å¯¹è®¸å¤šå›æŠ¥çš„æœŸæœ›ä»¥åŠè¡Œä¸ºæ”¿ç­–ç»™å‡ºçš„æ¦‚ç‡çš„æœŸæœ›ï¼Œæˆ‘ä»¬å°†è·å¾—ç›¸åŒçš„ä»·å€¼ï¼Œå°±å¥½åƒæˆ‘ä»¬æ¥å—äº†ç›¸åŒçš„å›æŠ¥çš„æœŸæœ›ï¼Œä½†ä½¿ç”¨ç›®æ ‡ç­–ç•¥çš„æ¦‚ç‡ï¼Œç›´è§‚åœ°ï¼Œæˆ‘è®¤ä¸ºè¿™å°±åƒè€ƒè™‘åŠ æƒæ”¶ç›Šrhoâ€¢gä¸€æ ·ï¼Œå°±åƒè€ƒè™‘åˆ°ç›®æ ‡ç­–ç•¥çš„ç›®æ ‡ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ›´æ–°è§„åˆ™æ˜¯Qï¼†lt; q qï¼†lt; qï¼†lt;  - å†™ä¸ºQï¼†lt;  -  q + alphaâ€¢rhoâ€¢ï¼ˆg-qï¼‰æˆ‘ä»¬å¦‚ä½•è·å¾—è¯¥è¡¨æ ¼ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/samas69420     [link]   [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j85tzj/why_in_the_offpolicy_nstep_version_of_sarsa/</guid>
      <pubDate>Mon, 10 Mar 2025 18:36:10 GMT</pubDate>
    </item>
    <item>
      <title>LLMå¯ä»¥å­¦ä¼šçœ‹å—ï¼Ÿå¾®è°ƒQWEN 0.5Bç”¨äºSFT + GRPOçš„è§†è§‰ä»»åŠ¡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j815kg/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/</link>
      <description><![CDATA[     &lt;ï¼ -  sc_off-&gt;  å˜¿å¤§å®¶ï¼ æˆ‘åˆšåˆšå‘å¸ƒäº†ä¸€ä¸ªåšå®¢ï¼Œåˆ†è§£äº†ä¸€ä¸ªåšå®¢ï¼Œåˆ†è§£äº†æ•°å­¦ ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–    grpoï¼Œdepsseek r1 r1 æ­¥éª¤ï¼   æœ‰è¶£çš„å®éªŒåŒ…æ‹¬ï¼š iå¾®è°ƒ qwen 2.5 0.5b ï¼Œa  folly&gt;è¯­è¨€ -  æ¨¡å‹ï¼Œæ²¡æœ‰äº‹å…ˆè§†è§‰è®­ç»ƒï¼Œä½¿ç”¨ sft + sft + grpo    &lt;å¼º&gt; href =â€œ https://preview.redd.it/ujxzkscccrvne1.png?width = 927ï¼†amp; format = pngï¼†amp; amp; amp; amp; amp; am https://preview.redd.it/ujxzksccrvne1.png?width=927ï¼†amp; format = pngï¼†amp; amp; amp; amp; auto = webppumppumppï¼›  å®Œæ•´åšå®¢&gt; href =â€œ https://github.com/jacksoncakes/vision-r1â€&gt; github    &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforeverctionlearning/comments/1j815kg/can_an_an_llm_llm_learn_learn_to_to_see_fine_fine_tuning_qwen_05b_for//â€&gt; [link]      &lt;a href =â€œ https://www.reddit.com/r/reinforevectionlearning/comments/1j815kg/can_an_an_llm_llm_llm_learn_to_to_to_fine_fine_tuning_qwen_05b_05b_for/â€]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j815kg/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/</guid>
      <pubDate>Mon, 10 Mar 2025 15:24:11 GMT</pubDate>
    </item>
    <item>
      <title>è®©SACåœ¨å¤§å‹å¹¶è¡Œæ¨¡æ‹Ÿå™¨ä¸Šå·¥ä½œï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</link>
      <description><![CDATA[&quot;As researchers, we tend to publish only positive results, but I think a lot of valuable insights are lost in our unpublished failures.&quot; This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim withå¹¶è¡Œæ¨¡æ‹Ÿäº†æ•°åƒä¸ªæœºå™¨äººã€‚å¦‚æœæ‚¨éµå¾ªæ—…ç¨‹ï¼Œæ‚¨å°†äº†è§£å¯èƒ½å¯¹æ€§èƒ½äº§ç”Ÿé‡å¤§å½±å“çš„ä»»åŠ¡è®¾è®¡å’Œç®—æ³•å®ç°çš„ç»†èŠ‚ã€‚ æ‰°æµæ¿è­¦æŠ¥ï¼šé“¾æ¥ï¼š https://araffin.github.io/post/post/sac-massive-sim/   æäº¤ç”±ï¼†ï¼ƒ32; /u/u/araffin2     [link]       [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</guid>
      <pubDate>Mon, 10 Mar 2025 08:27:24 GMT</pubDate>
    </item>
    <item>
      <title>å¤åˆ¶DeepSeek-R1 RLæ‰€éœ€çš„å»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;   hi rlç¤¾åŒºï¼Œæˆ‘æƒ³ç»§ç»­å¤åˆ¶DeepSeek R1çš„RLè®­ç»ƒç®¡é“ä¸­çš„å°æ•°æ®é›†ã€‚æˆ‘å¯¹åŸ¹è®­è¯­è¨€æ¨¡å‹æ„Ÿåˆ°æ»¡æ„ï¼Œä½†å¯¹åŸ¹è®­RLä»£ç†ä¸æ»¡æ„ã€‚æˆ‘å¯¹æ·±åº¦RLçš„ç»å…¸RLå’Œä¸­ç­‰ç†è®ºç†è§£æœ‰ä½“é¢çš„ç†è®ºç†è§£ã€‚  æˆ‘è®¤ä¸ºæˆ‘éœ€è¦é€æ­¥åŠ å¼ºå›°éš¾ï¼Œä»¥è®­ç»ƒæ¨ç†è¯­è¨€æ¨¡å‹ã€‚å› æ­¤ï¼Œæœ€è¿‘ï¼Œæˆ‘å¼€å§‹åŸ¹è®­PPOå®æ–½æ–¹æ³•æ¥è§£å†³ä¸€äº›æ›´è½»æ¾çš„å¥èº«ç¯å¢ƒï¼Œè¿™ç¡®å®åœ¨åŠªåŠ›... 1å‘¨ï¼Œæˆ‘ä»ç„¶æ— æ³•å†ç°ä½ä¿çœŸæ€§ï¼Œå°½ç®¡åŸºæœ¬ä¸ŠæŠ¬èµ·äº†ç¨³å®šçš„ - è´èµ›3ã€‚&gt; æˆ‘æƒ³äº†è§£æˆ‘çš„æœ€ç»ˆç›®æ ‡æ˜¯å¦æ­£ç¡®ã€‚ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä¸èƒ½RLè®­ç»ƒç®€å•çš„ä»£ç†å•†ï¼Œæˆ‘å°†å¦‚ä½•ä½¿ç”¨RLè®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä¸æˆ‘çš„æœ‹å‹è¿›è¡Œäº†é™åˆ¶RLç»éªŒçš„äº¤è°ˆï¼Œä»–æåˆ°ï¼Œç”±äºRLåŸ¹è®­è¯­è¨€æ¨¡å‹çš„ä»£ç å·²ç»åœ¨é‚£é‡Œï¼Œè€Œä¸”æŒ‘æˆ˜æ˜¯æ­£ç¡®çš„...    &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/u/u/complect-media-8074      [link]  &lt;a href =â€œ https://www.reddit.com/r/reinforecctionlearning/comments/1j7t5j4/advice_needed_needed_reproducing_deepseekr1_rl/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</guid>
      <pubDate>Mon, 10 Mar 2025 07:25:05 GMT</pubDate>
    </item>
    <item>
      <title>VINTIXï¼šé€šè¿‡æ–‡åŒ–å¼ºåŒ–å­¦ä¹ çš„åŠ¨ä½œæ¨¡å‹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼Œ æˆ‘ä»¬åˆšåˆšå‘å¸ƒäº†æˆ‘ä»¬åœ¨ç¦»çº¿èŒƒå›´å†…çš„åˆæ­¥åŠªåŠ›ï¼ˆè¯¸å¦‚Laskinç­‰äººï¼Œ2022å¹´çš„ç®—æ³•è’¸é¦ï¼‰æ—¶ï¼Œå°†å…¶æ”¾åˆ°ç¦»çº¿èŒƒå›´å†…çš„èŒƒå›´ã€‚è™½ç„¶æˆ‘ä»¬æ­£åœ¨ä»ç»å…¸çš„å…ƒå…ƒç´ æ„ä¹‰ä¸Šå¯»æ±‚æ¦‚æ‹¬ï¼Œä½†åˆæ­¥ç»“æœä»¤äººé¼“èˆï¼Œè¡¨ç°å‡ºå¯¹å‚æ•°å˜åŒ–çš„é€‚åº¦æ¦‚æ‹¬ï¼Œè€Œä»…åœ¨æ€»å…±87ä¸ªä»»åŠ¡ä¸‹æ¥å—äº†87ä¸ªä»»åŠ¡ã€‚ æˆ‘ä»¬çš„å…³é”®è¦ç‚¹åœ¨å®ƒä¸Šå·¥ä½œï¼šï¼ˆ1ï¼‰æ•°æ®ç­–åˆ’ICLRçš„æ•°æ®curation for Iclr hard tweakingæ˜¯ä¸€å®šçš„ã€‚å¸Œæœ›æ‰€è¿°çš„æ•°æ®æ”¶é›†æ–¹æ³•å°†æœ‰æ‰€å¸®åŠ©ã€‚è€Œä¸”æˆ‘ä»¬è¿˜å‘å¸ƒäº†æ•°æ®é›†ï¼ˆçº¦200mlnå…ƒç»„ï¼‰ã€‚ ï¼ˆ2ï¼‰å³ä½¿åœ¨ä¸åŒçš„æ•°æ®é›†ä¸‹ï¼Œä¹Ÿå¯èƒ½å¯¹é€‚åº¦å‚æ•°å˜åŒ–çš„æ¦‚æ‹¬ã€‚è¿™æ˜¯ä»¤äººé¼“èˆçš„ã€‚ä½†æ˜¯ï¼Œå³ä½¿åœ¨ç±»ä¼¼jatçš„å»ºç­‘ä¸­ï¼Œä¹Ÿä¸æ˜¯é‚£ä¹ˆå¯æ€•ï¼ˆä½†å¾ˆè¿‘ï¼‰ã€‚   nbï¼šéšç€æˆ‘ä»¬è¿›ä¸€æ­¥åŠªåŠ›æ‰©å±•å¹¶ä½¿çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ä¸å˜ - ä¹Ÿè®¸æ‚¨æœ‰ä¸€äº›æœ‰è¶£çš„ç¯å¢ƒ/åŸŸå/å…ƒå…ƒå­¦ä¹ åŸºå‡†ï¼Œæ‚¨å¸Œæœ›åœ¨å³å°†åˆ°æ¥çš„å·¥ä½œä¸­çœ‹åˆ°å—ï¼Ÿ href =â€œ https://github.com/dunnolab/vintixâ€&gt; https://github.com/dunnolab/vintix    å¦‚æœæ‚¨ä¼ æ’­è¯ï¼šhttps://x.com/vladkurenkov/status/1898823752995033299   æäº¤ç”±ï¼†ï¼ƒ32; /u/u/vkurenkov     [link]    [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</guid>
      <pubDate>Sun, 09 Mar 2025 21:03:08 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºå¤šç›®æ ‡å¢å¼ºå­¦ä¹ çš„ç¯å¢ƒçš„æ¦‚æ‹¬</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</link>
      <description><![CDATA[      ç°å®ä¸–ç•Œä¸­çš„é¡ºåºå†³ç­–ä»»åŠ¡é€šå¸¸æ¶‰åŠå¹³è¡¡ç›¸äº’çŸ›ç›¾çš„ç›®æ ‡ä¹‹é—´çš„æƒè¡¡å¹¶åœ¨å„ç§ç¯å¢ƒä¸­æ¦‚æ‹¬ã€‚å°½ç®¡å®ƒå¾ˆé‡è¦ï¼Œä½†å°šæœªæœ‰ä¸€é¡¹å·¥ä½œåœ¨æœ¬æ–‡ä¸­ç ”ç©¶å¤šç›®æ ‡ç¯å¢ƒä¸­çš„ç¯å¢ƒæ¦‚æ‹¬ï¼ æˆ‘ä»¬åœ¨å¤šç›®æ ‡å¢å¼ºå­¦ä¹ ï¼ˆMORLï¼‰ä¸­æ­£å¼åŒ–æ¦‚æ‹¬ä»¥åŠå¦‚ä½•è¯„ä¼°ã€‚æˆ‘ä»¬è¿˜ä»‹ç»äº† Morl Generalization åŸºå‡†æµ‹è¯•ï¼Œå…·æœ‰å„ç§å¤šæ ·æ€§åŸŸå…·æœ‰å¸¦æœ‰å‚æ•°åŒ–çš„ç¯å¢ƒé…ç½®ï¼Œä»¥ä¿ƒè¿›è¯¥é¢†åŸŸçš„ç ”ç©¶ã€‚ æˆ‘ä»¬å¯¹å½“å‰æœ€æ–°çš„æœ€æ–°è«å°”å°”ç®—æ³•çš„åŸºçº¿è¯„ä¼°2å…³é”®æ´å¯ŸåŠ›ï¼š  li li a li a li a li a li a li a li a li a li algorith ä¸å•ç›®æ ‡å¢å¼ºå­¦ä¹ ç›¸æ¯”ï¼Œè«å°”ï¼ˆMorlï¼‰è¡¨ç°å‡ºæ›´å¤§çš„å­¦ä¹ é€‚åº”æ€§è¡Œä¸ºçš„æ½œåŠ›ã€‚äº‹åçœ‹æ¥ï¼Œè¿™æ˜¯å¯ä»¥é¢„æœŸçš„ï¼Œå› ä¸ºå¤šç›®æ ‡å¥–åŠ±ç»“æ„æ›´å…·è¡¨ç°åŠ›ï¼Œå¹¶å…è®¸å­¦ä¹ æ›´å¤šçš„è¡Œä¸ºï¼ ğŸ˜²  æˆ‘ä»¬åšä¿¡ï¼Œåœ¨æœªæ¥å‡ å¹´ä¸­ï¼Œå¼€å‘èƒ½å¤Ÿè·¨å¤šç§ç¯å¢ƒå’Œç›®æ ‡æ¦‚æ‹¬çš„ä»£ç†å°†æˆä¸ºä¸€ä¸ªè‡³å…³é‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚æœ‰è®¸å¤šæœ‰å¸Œæœ›çš„é€”å¾„ç”¨äºè¿›ä¸€æ­¥çš„æ¢ç´¢å’Œç ”ç©¶ï¼Œå°¤å…¶æ˜¯åœ¨å•ä¸€ç›®æ ‡RLæ¦‚æ‹¬ç ”ç©¶ä¸­çš„é€‚åº”æŠ€æœ¯å’Œè§è§£ä¸­ï¼Œä»¥è§£å†³è¿™ä¸ªæ›´ä¸¥é‡çš„é—®é¢˜è®¾ç½®ï¼æˆ‘æœŸå¾…ç€ä¸æœ‰å…´è¶£æ¨è¿›è¿™ä¸€æ–°çš„ç ”ç©¶é¢†åŸŸçš„ä»»ä½•äººäº¤å¾€ï¼ ğŸ”—çº¸ï¼š https://arxiv.org/arxiv.org/abs/2503.00799999999999999       https://github.com/jaydenteoh         &lt;ï¼ -  SC_ON-&gt;ï¼†ï¼ƒ32;ä¸åŒç¯å¢ƒæ¦‚æ‹¬      &lt;ï¼æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1j7bdr3/on_generalization_carers_environments_in/â€&gt; [link]   ï¼†ï¼ƒ32;   [æ³¨é‡Š]    ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</guid>
      <pubDate>Sun, 09 Mar 2025 16:31:56 GMT</pubDate>
    </item>
    <item>
      <title>Pythonå’ŒUnityçš„RLç¯å¢ƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å—¨ï¼Œæˆ‘æƒ³è®­ç»ƒAIä½¿ç”¨Pythonç©æ¸¸æˆï¼Œå¹¶ä»¥Unityï¼ˆCï¼ƒï¼‰å½¢è±¡å¯è§†åŒ–æ¸¸æˆã€‚ç›®å‰ï¼Œæˆ‘éœ€è¦åœ¨Pythonä¸­åˆ›å»ºç¯å¢ƒï¼Œä»¥å­¦ä¹ å®é™…æ¸¸æˆç©æ³•ã€‚æœ‰æ²¡æœ‰åŠæ³•åˆ›å»ºæˆ‘å¯ä»¥åœ¨Pythonå’ŒUnityä¸­ä½¿ç”¨çš„ç¯å¢ƒï¼Ÿ   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/tot-chance9372     [link]        [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</guid>
      <pubDate>Sun, 09 Mar 2025 14:37:32 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆæˆ‘çš„æ¨¡å‹ä¸èƒ½å­¦ä¼šåœ¨è¿ç»­çš„ç½‘æ ¼ä¸–ç•Œä¸­ç©æ¸¸æˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ã€‚ç”±äºæˆ‘æ­£åœ¨ç ”ç©¶æ·±åº¦Qå­¦ä¹ ç®—æ³•ï¼Œå› æ­¤æˆ‘æ­£åœ¨å°è¯•ä»å¤´å¼€å§‹å®æ–½å®ƒã€‚æˆ‘åˆ›å»ºäº†ä¸€ä¸ªåœ¨ç½‘æ ¼ä¸–ç•Œä¸­ç©çš„ç®€å•æ¸¸æˆï¼Œæˆ‘çš„ç›®æ ‡æ˜¯å¼€å‘ä¸€ä¸ªç©æ­¤æ¸¸æˆçš„ä»£ç†å•†ã€‚åœ¨æˆ‘çš„æ¸¸æˆä¸­ï¼ŒçŠ¶æ€ç©ºé—´æ˜¯è¿ç»­çš„ï¼Œä½†æ˜¯åŠ¨ä½œç©ºé—´æ˜¯ç¦»æ•£çš„ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘è®¤ä¸ºDQNç®—æ³•åº”è¯¥èµ·ä½œç”¨çš„åŸå› ã€‚æˆ‘çš„æ¸¸æˆå…·æœ‰3ç§ä¸åŒçš„è§’è‰²ç±»å‹ï¼šä¸»è§’ï¼ˆä»£ç†ï¼‰ï¼Œç›®æ ‡å’Œçƒã€‚ç›®æ ‡æ˜¯åœ¨ä¸ä¸çƒç›¸æ’çš„æƒ…å†µä¸‹åˆ°è¾¾ç›®æ ‡ï¼Œè€Œçƒçº¿æ€§ç§»åŠ¨ã€‚æˆ‘çš„åŠ¨ä½œå€¼å‰©ä¸‹ï¼Œå³ä¸Šï¼Œå‘ä¸‹å’Œä»€ä¹ˆéƒ½ä¸æ˜¯ï¼Œæ€»å…±è¿›è¡Œäº†5ä¸ªç¦»æ•£æ“ä½œã€‚ æˆ‘ä½¿ç”¨Pygame Rectåœ¨Pythonä¸­ç¼–ç äº†æ¸¸æˆï¼Œç”¨äºç›®æ ‡ï¼Œè§’è‰²å’Œçƒã€‚æˆ‘å¥–åŠ±ä»£ç†å¦‚ä¸‹ï¼š   +5ï¼Œä¸è§’è‰²ç›¸æ’  -5ï¼Œä¸çƒç›¸æ’ï¼Œä»¥  +0.7   +0.7ï¼Œä»¥é è¿‘ç›®æ ‡ï¼ˆä½¿ç”¨Manhattanè·ç¦»ï¼‰   -1è¿œç¦»ç›®æ ‡ï¼ˆä½¿ç”¨Manhattanè·ç¦»ç§»åŠ¨ï¼‰ï¼ˆä½¿ç”¨Manhattanè·ç¦»è¿œå¤„ï¼‰ã€‚æˆ‘å°è¯•äº†ä¸åŒçš„çŠ¶æ€è¡¨ç¤ºå½¢å¼ï¼Œä½†æ˜¯åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæˆ‘çš„ç»çºªäººåªå­¦ä¼šé¿å…çƒå¹¶è¾¾åˆ°ç›®æ ‡ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä»£ç†æ ¹æœ¬ä¸ä¼šé¿å…çƒï¼Œæˆ–è€…æœ‰æ—¶å®ƒä¼šè¿ç»­å‘å·¦å’Œå‘å³è¿›å…¥æ‘‡æ‘†çš„è¿åŠ¨ï¼Œè€Œä¸æ˜¯è¾¾åˆ°ç›®æ ‡ã€‚&lt; /p&gt; æˆ‘ç»™å‡ºäº†çŠ¶æ€è¡¨ç¤ºï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š&lt; /p&gt;  agent.rect.rect.lect.lect.lect.lect.lect.rect.rect.rect.rect.rect.right.rect.rightï¼Œ agent.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.lect--rect-rect- egentï¼Œ target.rect.bottomï¼Œ agent.rect.bottom--  Agent.Rect.bottom-  ball.Rect.top ï¼Œ ball_direction_in_xï¼Œball_dircection_in_in_in_in_in_iin_y_y_y  partiveï¼ˆpartï¼‰è¿™æè¿°äº†å¯¹ç»çºªäººçš„æ¯”èµ›çŠ¶æ€ï¼Œæä¾›äº†çƒå’Œç›®æ ‡çš„ç›¸å¯¹ä½ç½®ä»¥åŠçƒçš„æ–¹å‘ã€‚ä½†æ˜¯ï¼Œæˆ‘çš„æ¨¡å‹çš„è¡¨ç°ä»¤äººæƒŠè®¶åœ°å·®ã€‚ç›¸åï¼Œæˆ‘å°†çŠ¶æ€åˆ†ç±»å¦‚ä¸‹ï¼š  å¦‚æœç›®æ ‡åœ¨å·¦è¾¹ï¼Œåˆ™ä¸º-1ã€‚ å¦‚æœç›®æ ‡åœ¨å³è¾¹ä¸º+1ã€‚æ¸¸æˆä¸­å¾ˆå°‘æˆ–æ²¡æœ‰çƒï¼‰ï¼Œæ¨¡å‹çš„æ€§èƒ½å¤§å¤§æé«˜ã€‚å½“æˆ‘ä»æ¸¸æˆä¸­åˆ é™¤çƒæ—¶ï¼Œåˆ†ç±»çš„çŠ¶æ€ä»£è¡¨å­¦å¾—å¾ˆå¥½ã€‚ä½†æ˜¯ï¼Œå½“å‡ºç°çƒæ—¶ï¼Œå³ä½¿è¡¨ç°å½¢å¼è¿ç»­ï¼Œè¯¥æ¨¡å‹ä¹Ÿéå¸¸æ…¢ï¼Œæœ€ç»ˆå®ƒè¿‡åº¦æ‹Ÿåˆã€‚ æˆ‘ä¸æƒ³å±å¹•æˆªå›¾æ¸¸æˆå±å¹•å¹¶å°†å…¶é¦ˆå…¥CNNã€‚æˆ‘æƒ³ä½¿ç”¨å¯†é›†çš„å±‚å°†æ¸¸æˆçš„ä¿¡æ¯ç›´æ¥æä¾›ç»™æ¨¡å‹ï¼Œç„¶åå­¦ä¹ ã€‚ä¸ºä»€ä¹ˆæˆ‘çš„æ¨¡å‹ä¸å­¦ä¹ ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1j786hr/1j786hr/why_cant_my_my_my_model_model_lealed_to_to_play_in_continuul/â€&gt; [link]   [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</guid>
      <pubDate>Sun, 09 Mar 2025 13:58:09 GMT</pubDate>
    </item>
    <item>
      <title>æœºå™¨äººæŠ€æœ¯å®šåˆ¶ä½“è‚²é¦†ç¯å¢ƒè®¾è®¡ã€‚åŒ…è£…çº¸è¿˜æ˜¯é˜¶çº§ç»§æ‰¿ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘æ­£åœ¨ä¸ºæ°´ä¸‹æœºå™¨äººæ„å»ºè‡ªå®šä¹‰ç¯å¢ƒã€‚æˆ‘å·²ç»å°è¯•ä½¿ç”¨ä¸€ä¸ªå¿«é€Ÿä¸”è„çš„æ•´ä½“ç¯å¢ƒï¼Œä½†æ˜¯å¦‚æœæˆ‘å°è¯•ä¿®æ”¹ç¯å¢ƒä»¥æ·»åŠ æ›´å¤šä¼ æ„Ÿå™¨ï¼Œè½¬æ¢è¾“å‡ºï¼Œé‡ç”¨ä»£ç ï¼Œä»¥å®Œæˆå¦ä¸€ä¸ªä»»åŠ¡ç­‰ï¼Œæˆ‘ç°åœ¨ä¼šé‡åˆ°é—®é¢˜ã€‚ ï¼Œæˆ‘æƒ³é‡æ„ä»£ç å¹¶å¿…é¡»åšå‡ºä¸€äº›è®¾è®¡é€‰æ‹©ï¼šæˆ‘åº”è¯¥ä½¿ç”¨ä¸€ä¸ªä¸é€‚ç”¨çš„spassç±»å¹¶åœ¨æ¯ä¸ªä»»åŠ¡ä¸­ä½¿ç”¨è®­ç»ƒå’Œè®­ç»ƒçš„èŒƒå›´ï¼Œæˆ–è€…æˆ‘è¦è®­ç»ƒåŠ³åŠ¨èŒƒå›´ï¼Œæˆ–è€…æˆ‘è¦è®­ç»ƒåŠ³åŠ¨èŒƒå›´ï¼Œæˆ–è€…æˆ‘è¦æ±‚èŒã€‚æˆ‘åªæœ‰ä¸€ä¸ªåŸºç±»ï¼Œå¹¶å°†å…¶ä»–æ‰€æœ‰å†…å®¹æ·»åŠ ä¸ºåŒ…è£…å™¨ï¼ˆåŒ…æ‹¬ä¼ æ„Ÿå™¨é…ç½®ï¼Œä»»åŠ¡å¥–åŠ± +é€»è¾‘ç­‰ï¼‰ï¼Ÿ å¦‚æœæ‚¨çŸ¥é“ç¯å¢ƒåˆ›å»ºçš„è‰¯å¥½èµ„æºï¼Œè¿™å°†ä¸èƒœæ„Ÿæ¿€ï¼‰  &lt;ï¼ -  sc_on- sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/equiald-diver     [link]   [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</guid>
      <pubDate>Sun, 09 Mar 2025 09:18:42 GMT</pubDate>
    </item>
    <item>
      <title>Hanç­‰äººâ€œä¸€èˆ¬æ¨ç†éœ€è¦å­¦ä¹ ä»ä¸€å¼€å§‹æ¨ç†â€ã€‚ 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</link>
      <description><![CDATA[   [link]  ï¼†ï¼ƒ32;   [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</guid>
      <pubDate>Sun, 09 Mar 2025 08:05:53 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Bartoå’ŒRichard S. Suttonè¢«ä»»å‘½ä¸º2024 ACM A.M.å›¾çµå¥–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =â€œ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/â€]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>