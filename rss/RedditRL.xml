<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Tue, 15 Apr 2025 06:30:48 GMT</lastBuildDate>
    <item>
      <title>在机器人方面的增强学习工程师的实习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jzkccn/interning_for_reinforcement_learning_engineer_in/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我最近完成了一个12个月的机器学习节目，旨在帮助Web开发人员在职业生涯中过渡到机器学习。我有兴趣从事专门用于强化机器人技术的职业。由于我对机器学习的新接触以及缺乏经验，我的履历显然缺乏相关的经验，除了Capstone项目，在该项目中，我与GPT-4这样的对象检测工作。  由于我缺乏真正的工作经验，我正在考虑实习，以最终可以找到RL -Robotics位置。  是否有人建议我在哪里找到实习？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/beautch_award_6626      [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jzkccn/interning_for_reinforcement_learning_engineer_in/</guid>
      <pubDate>Tue, 15 Apr 2025 05:43:52 GMT</pubDate>
    </item>
    <item>
      <title>学习POMDP代码</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jzi9bb/learning_pomdp_code/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在考虑学习POMDP编码，并想知道你们是否对从哪里开始有任何建议。我的教授给了我一篇名为“独裁者：在线POMDP规划的论文”。我已经阅读了论文，目前我专注于给定的代码。我不知道下一步该怎么办。我必须学习一些有关RL的课程吗？我可以做什么来撰写有关该项目的研究论文？我真诚地寻找建议。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/IntelligentStick0116     [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jzi9bb/learning_pomdp_code/</guid>
      <pubDate>Tue, 15 Apr 2025 03:39:25 GMT</pubDate>
    </item>
    <item>
      <title>在加强学习中的博士学位，对是否这样做感到困惑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jz0ah3/phd_in_reinforcement_learning_confused_on_whether/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨， ，我非常抱歉，鉴于这是一个很好的古老问题，我觉得很多人可能会/在问/在问我自己。 有点关于我自己的问题：我想在2026年春季毕业，我知道我是否在诸如研究的公司中毕业。目前，我特别想从事城市规划应用程序的深入强化学习（和图形神经网络）＆amp;上述模型/解决方案的解释性，例如公共交通计划，交通信号管理，道路布局的生成等。现在，我正在从事一个类似的项目，作为我的硕士项目的一部分。就像每个在我舞台上的人一样，我对下一步应该感到困惑。我应该攻读博士学位，还是应该在业界工作几年，更好地评估自己，获得更多的经验（到目前为止，我一直是数据科学家/ML工程师已有2年的时间），然后再启动我的大师。该领域内外的许多人都告诉我，尽管有硕士毕业生的研究职位，但他们越来越远，大多数角色都需要博士学位或同等学位的经验。  我可以在完成硕士学位后在业内工作，但是鉴于当前的经济，找到AI工作，更不用说RL工作，在这里感到非常困难，而RL工作在我的祖国几乎不存在。因此，我正在尝试评估直接获得博士学位的可行计划。鉴于RL具有很大的研究范围，而且我知道我想从事的事情。我当前项目的顾问告诉我，博士学位是对该项目和我的主人的良好而自然的进步，但我现在对此感到警惕。 我真的很感谢您对此的见解和意见。很抱歉，如果这不是发布此信息的正确位置。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/lowkeysuicidal14     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jz0ah3/phd_in_reinforcement_learning_confused_on_whether/</guid>
      <pubDate>Mon, 14 Apr 2025 14:31:13 GMT</pubDate>
    </item>
    <item>
      <title>博士学位论文的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jyqxhh/marl_ideas_for_phd_thesis/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我是一个具有控制系统背景和RL背景的博士生。我想研究我的论文多代理RL。目前，我的想法是我了解了MARL的一些领域和开放问题，并稍微阅读了一些方面。然后，根据我喜欢从他们的名单中列出的东西，并在清单上进行文献综述。现在，如果您建议您在MARL中的某些领域有趣或有助于我列出我的初始列表，我会很高兴。非常感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hive-emotion6291     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jyqxhh/marl_ideas_for_phd_thesis/</guid>
      <pubDate>Mon, 14 Apr 2025 04:56:40 GMT</pubDate>
    </item>
    <item>
      <title>增强学习死了吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jym1c2/is_reinforcement_learning_dead/</link>
      <description><![CDATA[在提交由＆＃32; /u/u/u/u/u/bellman_     ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jym1c2/is_reinforcement_learning_dead/</guid>
      <pubDate>Mon, 14 Apr 2025 00:19:24 GMT</pubDate>
    </item>
    <item>
      <title>AI学会玩Virtua Fighter 32X深钢筋学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jykm1v/ai_learns_to_play_virtua_fighter_32x_deep/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/ageofempimes4aoe4aoe4     [link]   ＆＃32;   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jykm1v/ai_learns_to_play_virtua_fighter_32x_deep/</guid>
      <pubDate>Sun, 13 Apr 2025 23:06:17 GMT</pubDate>
    </item>
    <item>
      <title>从模拟到现实：使用以撒实验室建造轮式机器人（增强学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jyf8qu/from_simulation_to_reality_building_wheeled/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/loveyouchee      [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jyf8qu/from_simulation_to_reality_building_wheeled/</guid>
      <pubDate>Sun, 13 Apr 2025 19:03:32 GMT</pubDate>
    </item>
    <item>
      <title>从头开始实现DeepSeek R1的GRPO算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jyf1i7/implementing_deepseek_r1s_grpo_algorithm_from/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/xcodevn      &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1jyf1i7/implementing_deepseek_r1s_r1s_grpo_algorithm_algorithm_from/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jyf1i7/implementing_deepseek_r1s_grpo_algorithm_from/</guid>
      <pubDate>Sun, 13 Apr 2025 18:55:02 GMT</pubDate>
    </item>
    <item>
      <title>寻找计算有效的泥浆环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jxkx4g/looking_for_computeefficient_marl_environments/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是一个学士学位的学生，计划在合作策略游戏中撰写有关多代理增强学习（MARL）的论文。最初，由于其丰富的动态，我被吸引使用外交（无压力版），但事实证明，培训MARL代理人的外交是非常密集的。 With a budget of only around $500 in cloud compute and my local device&#39;s RTX3060 Mobile, I need an alternative that’s both insightful and resource-efficient. I&#39;m on the lookout for MARL environments that capture the essence of cooperative strategy gameplay without demanding heavy compute resources , so far in my search i have found Hanabi , MPE and pettingZoo but unfortunately i feel like they don&#39;t capture the essence of games like外交或风险。你们有什么建议吗？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/skydiver4312     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jxkx4g/looking_for_computeefficient_marl_environments/</guid>
      <pubDate>Sat, 12 Apr 2025 16:15:56 GMT</pubDate>
    </item>
    <item>
      <title>是否有像Pytorch Lightning这样的框架？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jxbp6l/are_there_frameworks_like_pytorch_lightning_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我认为Pytorch Lightning是改善灵活性，生殖能力和可读性的好框架，在处理更多复杂的监督学习项目时。我看到了代码示例   ，它表明可以使用drl，但它感觉很有可能，但它有点像材料，因为我发现了一个非常亮的范围。而不是“面向环境交往”。使用诸如体育馆之类的图书馆或使用闪电的正确方法。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/capelettin     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jxbp6l/are_there_frameworks_like_pytorch_lightning_for/</guid>
      <pubDate>Sat, 12 Apr 2025 07:18:09 GMT</pubDate>
    </item>
    <item>
      <title>[MBRL]为什么即使在Dreamerv3中的世界模型融合之后，政策绩效也会波动？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jxa7tn/mbrl_why_does_policy_performance_fluctuate_even/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 我目前正在与Dreamerv3合作，包括多个控制任务，包括DeepMind Control Suite的Walker_walk。 I&#39;ve noticed something interesting that I&#39;m hoping the community might have insights on. **Issue**: Even after both my world model and policy seem to have converged (based on their respective training losses), I still see fluctuations in the episode scores during policy learning. I understand that DreamerV3 follows the DYNA scheme (from Sutton&#39;s DYNA paper), where the world model and policy are trained in parallel.我的期望是，一旦世界模型融合到环境的准确表示，政策绩效就应该稳定。 是否有人使用Dreamerv3或其他MBRL算法经历了这一点？我很好奇这是否是：   MBRL系统中的预期行为？   我实施的迹象表明我的实施出了问题？                    dyna-style方法的基本限制？减少这种差异或解释为什么发生的任何提示将不胜感激！提交由＆＃32; /u/u/udanciprativecar545     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jxa7tn/mbrl_why_does_policy_performance_fluctuate_even/</guid>
      <pubDate>Sat, 12 Apr 2025 05:36:01 GMT</pubDate>
    </item>
    <item>
      <title>K-Subset选择的政策梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jx3czf/policy_gradient_for_ksubset_selection/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  假设我有一组n个项目，以及将每个k子设置映射到真实数字的奖励功能。 这些项目在每个“状态/上下文”中都会更改（这实际上是强盗问题）。目标是以状态为条件的政策，最大程度地提高了对其选择的子集的奖励，平均在所有状态下。 我很乐意为算法提出建议，但这是深度学习管道中的一个子问题，因此需要成为一些可不同的东西（没有启发式/进化算法）。专门加强。然后，问题变成了如何将k-Subset选择的策略参数化。任何子集都很容易，每个项目的概率都有bernoulli。是否有人遇到概括将Bernoulli样本限制为大小为K的子集？重要的是，我必须获得选择的动作/子集的准确概率 - 并且它不会太复杂（Gumbel Top -K不在列表中）。 编辑：为了清楚起见，问题本质上是策略输出的内容。我们如何进行采样并学习最佳的k-subset来选择！ 谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1jx3czf/policy_gradient_for_ksubset_selection/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jx3czf/policy_gradient_for_ksubset_selection/</guid>
      <pubDate>Fri, 11 Apr 2025 23:11:00 GMT</pubDate>
    </item>
    <item>
      <title>对机器人技术的增强学习非常酷！ （对博士机器人学生的采访）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jx2rcd/reinforcement_learning_for_robotics_is_super_cool/</link>
      <description><![CDATA[    我当然学到了很多关于RL的机器人能力的知识，并被这次对话启发了。 href =“ https://youtu.be/39nb43ylas0?si=_dfxyq-tvztbsu9r”&gt; https://youtu.be/39nb43ylas0?si=_si = _si = _dfxyq-dfxyq-tvztbsu9r提交由＆＃32; /u/okthought8642      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/comments/1jx2rcd/reinforecction_learning_for_robotics_is_super_cool/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jx2rcd/reinforcement_learning_for_robotics_is_super_cool/</guid>
      <pubDate>Fri, 11 Apr 2025 22:42:14 GMT</pubDate>
    </item>
    <item>
      <title>RL会有未来吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jwv8tf/will_rl_have_a_future/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  显然有点单击诱饵，但要认真地问。我再次进入RL，因为这是最接近我的AI。仍然有许多未解决的问题，例如奖励功能设计，代理商不做您想要的事情，培训永远用于某些问题等。 你们都在想什么？是否值得进入RL并在不久的将来成为职业？另外，您的项目将在5  -  10年内发生什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/imstifler     [link]   ＆＃32;   [commist imprion]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jwv8tf/will_rl_have_a_future/</guid>
      <pubDate>Fri, 11 Apr 2025 17:17:54 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>