<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 05 Feb 2024 21:12:51 GMT</lastBuildDate>
    <item>
      <title>任何人都有处理gymnasium.spaces.Dict 的经验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajprnh/anyone_has_any_experience_with_handling/</link>
      <description><![CDATA[解决观察空间是一个  gymnasium.spaces.Dict 并包含 2 个 MultiDiscrete 组件的问题 &gt; observation_dict = { &quot;height_map&quot;: MultiDiscrete(height_map_repr), &quot;visible_box_sizes&quot;: MultiDiscrete(box_repr), }  明显的解决方案是我可以将其展平并提供给我的神经网络，但我还有其他方法可以处理吗？我正在使用 pytorch。   由   提交 /u/schrodingershit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajprnh/anyone_has_any_experience_with_handling/</guid>
      <pubDate>Mon, 05 Feb 2024 19:53:33 GMT</pubDate>
    </item>
    <item>
      <title>Scikit Learn 的计算机视觉 - Scikit Learn 的 Gael Varoquaux 创始人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajobdg/computer_vision_with_scikit_learn_gael_varoquaux/</link>
      <description><![CDATA[    /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajobdg/computer_vision_with_scikit_learn_gael_varoquaux/</guid>
      <pubDate>Mon, 05 Feb 2024 18:55:42 GMT</pubDate>
    </item>
    <item>
      <title>在处理大型、多离散动作空间时，PPO 收敛到“始终相同的动作”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajo4ps/ppo_converges_to_same_action_always_when_dealing/</link>
      <description><![CDATA[您好， 我在尝试使用大型、多离散操作解决自定义环境时遇到了问题空间。环境状态表示为一个包含 100 个元素的数组，每个元素代表实际环境的一个 10x10 块。数组中的值表示相应图块上存在的当前库存（例如货币单位或其他）。对于每个图块，代理可以分配三个动作分量之一：“1”和“1”。 (实现现有库存)，“2” （投资新股）或“0” （没做什么）。环境通过移除已实现的库存(1)、初始化新库存(2)、增长未触及的库存以及随机“杀死”现有库存来计算下一个状态。目前库存（库存越高，可能性越大）。还有一些随机效应可以证明空间方法的合理性，以防万一您可能想知道......奖励信号的计算方式为已实现库存的总和减去已实现或初始化的每个图块的固定成本。环境一开始是空的，并在 100 个时间步后被截断；不存在这样的最终状态。 理论上，最佳策略应该类似于“初始化空的图块，对低于某个阈值的图块不执行任何操作，并实现高于某个阈值的图块。” ;虽然细节可能有所不同，但初始化已经库存的瓷砖或收获空瓷砖肯定没有价值，因为这只会导致固定成本而没有相应的回报。 训练过程开始时相当平均情景回报迅速增长，远高于“每个动作组件的同等概率”基准。代理了解到，在大多数情况下，对大多数图块不执行任何操作是有用的，从而减少了实现和初始化的总体丰富度。然而，在某个（显然远离最佳点）点，情景回报突然停止增加，学习再也不会起飞。 我应该注意到它并没有急剧崩溃，也不会恢复；它只是不再改善了。到目前为止，我的所有运行中都出现了这种观察结果，并且更改一些超参数（学习率、批量大小和到目前为止的 n_epochs）只会改变整体学习速度 - 有些设置更早达到该点，有些设置更晚，但所有设置都在类似的范围内趋于平稳间歇性回报。  我调查了“终端”政策并意识到它们都是“始终采取相同的行动” （无论环境状态如何）和准确定性，这意味着它们总是在完全相同的图块上做完全相同的事情，完全忽略当前的观察。在我未经训练的眼睛看来，该算法似乎已经达到了某些局部最优值，并且无法摆脱它。 我正在寻求有关下一步该何去何从的建议。目前，我正在考虑改变我的奖励信号，以惩罚“愚蠢”的人。行动（实现空的瓷砖等）更加困难。然而，我更愿意在某种程度上保持奖励信号“人类可读”。我还想避免首先向代理提供太多的外部专家知识（并且只是通过改变奖励信号），但这主要是出于美观的原因。我还缺少哪些其他选择？进一步的超参数调整可能有任何价值吗？或者我应该看看我的代理的神经网络架构？目前，我正在使用开箱即用的 StableBaselines3 PPO...   由   提交 /u/ionatura   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajo4ps/ppo_converges_to_same_action_always_when_dealing/</guid>
      <pubDate>Mon, 05 Feb 2024 18:48:17 GMT</pubDate>
    </item>
    <item>
      <title>尽管奖励增加，PPO 模型仍无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajnub8/ppo_model_not_learning_despite_increasing_rewards/</link>
      <description><![CDATA[        由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajnub8/ppo_model_not_learning_despite_increasing_rewards/</guid>
      <pubDate>Mon, 05 Feb 2024 18:36:27 GMT</pubDate>
    </item>
    <item>
      <title>[建议]OpenAI GYM/Stable Baselines：如何设计动作空间的依赖动作子集？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajikd2/advice_openai_gymstable_baselines_how_to_design/</link>
      <description><![CDATA[您好， 我正在开发自定义 OpenAI GYM/Stable Baseline 3 环境。假设我的环境中有总共 5 个操作 (0,1,2,3,4) 和 3 个状态 (A, B, Z)。在状态 A 中，我们只允许两个操作 (0,1)，状态 B 操作为 (2,3)  并且在状态 Z 中，所有 5 个都可供代理使用。 我一直在阅读各种文档/论坛（并且还实现了）允许所有动作在所有状态下都可用，但当在某个状态下执行无效动作时分配（大）负奖励。然而，在训练过程中，这会导致我出现奇怪的行为（特别是扰乱我的其他奖励/惩罚逻辑），这是我不喜欢的。 我想以编程方式清楚地消除每个动作中的无效动作状态，所以它们甚至不可用。使用动作组合的蒙版/向量对我来说也不可取。我还读到不建议动态改变操作空间（出于性能目的）？ TL;DR 我希望听到人们如何解决这个问题的最佳实践，我确信这对许多人来说都是常见情况。 编辑：我可能正在考虑的解决方案之一是返回 self.state 通过步骤循环中的info，然后实现一个自定义函数/lambda，它基于状态去除无效操作，但我认为这将是一个非常丑陋的黑客/干扰gym/sb的内部运作。 编辑2：再想一想，我认为上述想法真的很糟糕，因为它不允许模型学习训练阶段（循环阶段之前）的可用操作子集。因此，我认为这应该集成到环境的 Action Space 部分中。 编辑 3： 这个问题似乎也提到了 之前，但我没有使用 PPO 算法。   由   提交 /u/against_all_odds_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajikd2/advice_openai_gymstable_baselines_how_to_design/</guid>
      <pubDate>Mon, 05 Feb 2024 15:02:50 GMT</pubDate>
    </item>
    <item>
      <title>寻求指导：选择低计算能力的 ML 研究主题进行会议提交</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajgagr/seeking_guidance_choosing_a_lowcomputational/</link>
      <description><![CDATA[机器学习科学家您好， 我正在寻找撰写机器学习领域的研究论文，并打算将其提交给明年将举行一次享有盛誉的会议。虽然我对机器学习和深度学习的基础知识有深入的了解，但我受到可用计算资源的限制；我将使用我的笔记本电脑进行研究。鉴于这一限制，您能否推荐一个无需大量计算能力即可探索的机器学习研究领域？ 谢谢  &amp; #32；由   提交 /u/Significant-Raise-61   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajgagr/seeking_guidance_choosing_a_lowcomputational/</guid>
      <pubDate>Mon, 05 Feb 2024 13:18:08 GMT</pubDate>
    </item>
    <item>
      <title>RL 的部分单调网络 [D]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajf7pa/partially_monotonic_networks_for_rl_d/</link>
      <description><![CDATA[大家好，正在寻找有关我正在做的项目的建议和评论。 我正在尝试做一个策略梯度强化学习问题其中某些输入/输出对之间存在某些递增/递减关系是可取的。 有一个基于理论 pde 的最优策略（具有所需的单调性）作为基线，并且无约束的简单 FNN 可以优于 pde 和尽管不存在单调性，但策略大多是一致的。 下一步，我想将部分矩阵权重限制为非负，以便获得部分单调的神经网络。该结构遵循 Trindade 2021，其中有两个 NN 块，一个受单调输入约束，另一个受正常输入约束，两个输出连接并馈入受约束的 NN 中以提供单个输出。 （我将 -1 乘以应随输出而减少的约束输入） 我在获得 pde 基线的目标值方面没有取得太大成功。对于激活，我尝试了 tanh，它最终给了我一堆线性神经网络。然后我使用了leakyrelu，其中一半是正常的，一半应用为-leakyrelu(-x)，以便该函数可以是单调的且具有非单调斜率（最佳策略可能有平坦部分）。我尝试了批量大小、学习率、神经网络维度等的整个网格，但没有成功。 任何对我的方法的评论或对下一步尝试的建议都表示赞赏。感谢您的阅读！   由   提交/u/CaptTeemo175  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajf7pa/partially_monotonic_networks_for_rl_d/</guid>
      <pubDate>Mon, 05 Feb 2024 12:21:23 GMT</pubDate>
    </item>
    <item>
      <title>华为：在现实世界中通过不确定性做出值得信赖的决策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj520x/hua_wei_trustworthy_decision_making_in_the_real/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj520x/hua_wei_trustworthy_decision_making_in_the_real/</guid>
      <pubDate>Mon, 05 Feb 2024 01:57:24 GMT</pubDate>
    </item>
    <item>
      <title>离线 DQN 的加权重要性采样</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj3l8f/weighted_importance_sampling_for_offline_dqn/</link>
      <description><![CDATA[      我的问题是“在离线环境中比较不同强化学习策略的最佳方式是什么？” 我自己做了一些研究，发现了加权重要性抽样是平衡偏差和方差的好方法，特别是当行为策略不是最优的（或接近我们想要评估的策略）时。然而，如果我们训练一个获取高维特征的 DQN 模型，并输出推荐操作的概率（假设通过 softmax 层），那么我在实现上会遇到问题。考虑到我们无法访问状态空间（我们具有以下特征），我们应该在 WIS 公式中将 π(at|st) （来自 RL 模型的最优策略）或 μ(at|st) （行为策略）设置为什么可以产生无限多个状态）？ ​ WIS 公式   由   提交/u/anagreement  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj3l8f/weighted_importance_sampling_for_offline_dqn/</guid>
      <pubDate>Mon, 05 Feb 2024 00:45:51 GMT</pubDate>
    </item>
    <item>
      <title>q 函数近似不收敛的 Actor Critic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/</link>
      <description><![CDATA[最近我一直在尝试实现论文。 但是，当使用车杆 v1 环境时，代理会学习一些行为，但随后就会崩溃。任何有关不正确实现或替代批评者功能的想法将不胜感激。 我也一直在使用超参数，但没有任何组合对我来说效果很好。 代码 ​ 编辑：我已经意识到在得分函数中，我不应该取输出的总和，而应该在 for 循环中单独计算每个动作的梯度。然而，该代理的性能仍然不佳。    由   提交/u/Tight_Apple_678   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/</guid>
      <pubDate>Mon, 05 Feb 2024 00:17:55 GMT</pubDate>
    </item>
    <item>
      <title>“从强化学习到代理：理解基础认知的框架”，Seifert 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ais80w/from_reinforcement_learning_to_agency_frameworks/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ais80w/from_reinforcement_learning_to_agency_frameworks/</guid>
      <pubDate>Sun, 04 Feb 2024 16:46:34 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶|斯瓦亚特机器人 |马恒达塔尔 |印度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aij99o/autonomous_driving_swaayatt_robots_mahindra_thar/</link>
      <description><![CDATA[       由   提交/u/shani_786  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aij99o/autonomous_driving_swaayatt_robots_mahindra_thar/</guid>
      <pubDate>Sun, 04 Feb 2024 08:31:55 GMT</pubDate>
    </item>
    <item>
      <title>如何同时训练代理和输入编码？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aibim9/how_to_train_agent_and_input_encoding/</link>
      <description><![CDATA[大家好。我正在研究一个 DRL 项目，其中状态是一个长度不同的序列。因为我使用的是 Gymnasium 和 Stable-Baselines3，需要固定的预定义观察空间形状，所以我正在考虑使用 RNN 将输入序列编码为 Gym 环境中的预定义形状，然后让 SB3 处理训练. 所以我的问题是，有没有办法同时训练 RNN 和 RL 代理？我没有太多经验，但我的理解是我需要连接 RNN 和策略/价值网络以使反向传播工作，但我不确定在使用 Gym 和 SB3 时如何实现这一点，因为 RNN是在 Gym 环境中创建的，而策略/价值网络是在 SB3 模型中的。   由   提交/u/McCree76   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aibim9/how_to_train_agent_and_input_encoding/</guid>
      <pubDate>Sun, 04 Feb 2024 01:06:50 GMT</pubDate>
    </item>
    <item>
      <title>DQN 不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</link>
      <description><![CDATA[嗨，我正在尝试做一个蛇游戏 DQN，但在前 1k 次迭代中没有看到太多结果（如果有的话）。该模型似乎正在回归。 我想知道我的更新循环是否正确 信息：吃食物的奖励+1，碰撞-1，其他奖励 - 欧氏距离/来自食物的 l2 范数 我确实有一个重播缓冲区 def train_v2(self, state_tensor, action,reward, new_state_tensor, did): # state -&gt; (3,32,24) # 模型 -&gt; conv net action_tensor = torch.tensor(action, dtype=torch.float)reward_tensor = torch.tensor(reward, dtype=torch.float) if len(state_tensor.shape) == 3: # 转换为批处理形式 if single example state_tensor = torch.unsqueeze（state_tensor，dim = 0）action_tensor = torch.unsqueeze（action_tensor，dim = 0）reward_tensor = torch.unsqueeze（reward_tensor，dim = 0）new_state_tensor = torch.unsqueeze（new_state_tensor，dim = 0）完成=（ done,) for i in range(len(done)): # for idx in batch q_pred = self.model.forward(state_tensor[i]) if did[i]: q_next = torch.zeros(1) # 没有下一个状态否则： q_next = self.model_target.forward(new_state_tensor[i]).max(dim=1)[0] q_target =reward_tensor[i] + self.gamma*q_next self.optimizer.zero_grad() loss = self.loss_function( q_target, q_pred) loss.backward() self.optimizer.step()    由   提交/u/throtaway85633  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</guid>
      <pubDate>Sat, 03 Feb 2024 05:39:48 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo tic-tac-toe 游戏中何时调用 reset() 函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</link>
      <description><![CDATA[我正在将 pettingzoo 环境用于 MARL 程序，并使用 tictactoe 环境（在此处找到）作为蓝图。现有环境似乎在每个单独的时期内多次调用重置函数，这对于我自己的目的来说是不可取的。在尝试查找重置调用的来源时，我将其追溯到“base.py”文件。文件位于 pettingzoo /utils/wrappers 目录中。我仍然无法准确确定何时调用重置。我想让它仅在每个纪元结束时调用重置，因为我积累了不想重置的值。 我复制了 tictactoe 测试代码来运行它。我在重置函数中放置了一个打印调用，以查看每个时期内调用重置的次数。我确认在井字棋游戏的每个时期都会多次调用重置。这样做的目的是什么？在我看来，你会想在每场比赛结束时调用重置。为什么要多次重置，如何更改重置被调用的次数？   由   提交/u/NobodySmart1617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</guid>
      <pubDate>Fri, 02 Feb 2024 22:36:23 GMT</pubDate>
    </item>
    </channel>
</rss>