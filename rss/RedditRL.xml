<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 02 Sep 2024 06:23:42 GMT</lastBuildDate>
    <item>
      <title>政策绩效分解证明问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f6hgtt/problem_with_proof_of_decomposition_of_policy/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f6hgtt/problem_with_proof_of_decomposition_of_policy/</guid>
      <pubDate>Sun, 01 Sep 2024 15:42:04 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的元学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</link>
      <description><![CDATA[您好，RL 中的大多数元学习似乎已应用于策略空间，而很少应用于 DQN 中的价值空间。我想知道为什么如此注重将策略适应新任务，而不是将价值网络适应新任务。Meta Q Learning 论文似乎是唯一一篇使用 Q 网络进行元学习的论文。这是真的吗？如果是，为什么？    提交人    /u/Sea-Collection-8844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</guid>
      <pubDate>Sun, 01 Sep 2024 07:22:46 GMT</pubDate>
    </item>
    <item>
      <title>寻找一个可供人类和代理合作完成任务的环境，其中存在多种可能的策略/子任务。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f64qvt/looking_for_an_environment_for_a_human_and_agent/</link>
      <description><![CDATA[大家好。我正在计划一个硕士研究项目，重点关注人类和 RL 代理如何协调共同完成任务。我正在寻找一个相对简单（理想情况下是 2D 和离散的）但仍允许团队采用不同高级策略的游戏式环境。这很重要，因为我的大多数潜在研究主题都集中在人类代理团队如何协调选择然后执行该高级策略。 到目前为止，Overcooked 环境 是我见过的最有前途的。在这种情况下，不同的高级策略可能是 (1) 拾取食材、(2) 烹饪食材、(3) 交付订单、(4) 丢弃垃圾。但所有这些策略都非常简单，所以我希望有更多选择。例如，在游戏中，代理可以决定是否收集资源、攻击敌人、治愈、探索地图等。任何建议都绝对值得赞赏。    提交人    /u/chowder138   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f64qvt/looking_for_an_environment_for_a_human_and_agent/</guid>
      <pubDate>Sun, 01 Sep 2024 03:09:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TD(0) 和 TD(lambda) 具有相同的计算成本？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5qy84/why_td0_and_tdlambda_have_same_computational_cost/</link>
      <description><![CDATA[我在 YouTube 上学习了 TD 值估计，它说 TD(0) 和 TD(lambda) 具有相同的计算成本，但这对我来说没有意义。如果你做 TD(lambda)，你需要通过状态存储每个奖励来估计起始状态的值，不是吗？甚至你需要计算它们与 lambda 相乘并求和。谢谢你的友好回答。    提交人    /u/Latter-Tomorrow-6850   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5qy84/why_td0_and_tdlambda_have_same_computational_cost/</guid>
      <pubDate>Sat, 31 Aug 2024 16:14:07 GMT</pubDate>
    </item>
    <item>
      <title>偏差不是总是比方差更重要吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5q575/isnt_it_always_that_bias_is_more_important_than/</link>
      <description><![CDATA[我正在研究偏差和方差之间的权衡关系，比较蒙特卡洛值估计和时间差异值估计。  但如果只能选择一个，低偏差似乎总是比低方差好得多。因为如果你反复采样，最终你会得到真实值。那么，准确的偏差不是更重要吗？我的意思是，如果偏差很大，即使方差很小，为什么还需要它呢？ 即使在这种情况下，方差是什么？ 如果这是一个愚蠢的问题，请原谅。    提交人    /u/Latter-Tomorrow-6850   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5q575/isnt_it_always_that_bias_is_more_important_than/</guid>
      <pubDate>Sat, 31 Aug 2024 15:38:53 GMT</pubDate>
    </item>
    <item>
      <title>MARL 与分层 RL 与经典 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5l9zl/marl_vs_hierarchical_vs_classic_rl/</link>
      <description><![CDATA[大家好， 我的论文已经进行了近 2 年，并且很长时间以来一直存在一个问题。 在我们的问题中，环境由多种工具组成，可以对其进行操纵以实现所需的解决方案。每个工具都被分成多个参数。奖励函数是与系统期望输出的距离。 可以通过添加不同工具（相同或不同类型的现有工具）来更改环境。您可以拥有同一类型工具的多个实例。 我们尝试的第一个实现是使用 REINFORCE 和 A2C 同时操作所有参数的单个代理。这些实现产生了良好的结果。 然后，我们尝试使用 OptionCritic 的分层方法，其中每个选项都是一个代理，这是一种封建模型。这种方法优于之前的方法， 我们继续尝试添加不同类型的工具，并且能够解决每个类型都有 1 个不同工具的问题。我们使用 OptionCritic 算法解决了这个问题。 后来我们改变了环境，让其拥有 2 个相同类型的工具，然后一切都停止了工作。 这让我们尝试回到经典算法，但这感觉不对。我试图向我的导师解释我的疑惑，但他拒绝听我说话。 所有这些背景现在都引出了我的问题。  您什么时候将不同的动作视为不同的代理，或者视为同一个代理同时做出 1 个聚合动作？ 您什么时候尝试用 MARL / Hierarchical / Classic RL 解决问题？  谢谢大家， 一个非常疲惫的硕士生    提交人    /u/sagivborn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5l9zl/marl_vs_hierarchical_vs_classic_rl/</guid>
      <pubDate>Sat, 31 Aug 2024 11:39:41 GMT</pubDate>
    </item>
    <item>
      <title>在类似青蛙游戏中训练 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5gvpj/training_a_dqn_in_a_froggerlike_game/</link>
      <description><![CDATA[      嘿！我目前正在开发一款类似青蛙过河的游戏，以便更多地了解 DQN，但我偶然发现了几个我不知道如何解决的问题。 因此，本质上，玩家在网格中移动。每个动作（上、下、左、右）都会使其移动一个网格单元。还有不执行任何操作的停留动作。世界上的其他一切都不是网格 - 汽车和火车以恒定的速度移动。目标是让玩家尽快到达终点。 因此，我面临的第一个问题是如何训练网络。我读过/看过的每个教程都让代理在每一帧中都采取一个动作。就我而言，让代理每帧都采取一个动作可能不是一个好主意（我认为是这样）。这是因为它会移动得太快，当我让训练过的模型以正常速度播放时，它的知识可能不适合较低的速度。因此，我现在正在做的是每 200 毫秒执行一次操作（游戏以 60 fps 运行，但没有任何依赖此操作）。但是，我觉得这对训练有重大影响。例如，如果玩家采取的行动导致它在 50 毫秒后死亡，它应该得到负面奖励，但这种奖励不会与任何行动相关联。有没有常见的方法来处理它？我读过关于跳帧的文章，但并不完全确定这是这里的解决方案。 第二个问题与实际训练有关。现在，当我在代理采取行动时（或当它死亡时，我在小批量上训练它）训练代理时，它通常要么学会尽可能快地跑起来，而不会避开任何汽车，要么学会不惜一切代价避开汽车，几乎根本不上去。我尝试过使用超参数和奖励，但老实说，我觉得我并不完全了解结果以及它们发生的原因。我尝试增加学习率和探索率，尝试使用更大更复杂的网络，尝试改变折扣因子，尝试了很多很多奖励组合，但似乎都不起作用。 我表示状态的方式如下： - 玩家 X 和 Y - 火车活跃 - 火车 Y - X 和 Y 轴上最近的汽车距离 - 最近的汽车方向和速度 - X 和 Y 轴上第二最近的汽车距离 - 第二最近的汽车方向和速度 - X 和 Y 轴上第三最近的汽车距离 - 第三最近的汽车方向和速度 - 玩家是否处于危险区域 - 玩家是否离汽车太近 至于奖励，我尝试对上升给予大额奖励，尤其是达到新的最高限制。我尝试过对倒下和留在原地进行惩罚，对获胜给予大量奖励，对死亡给予大量惩罚，但无论如何，它都无法正确学习。 如果你们有任何建议，我将不胜感激！谢谢 :)    提交人    /u/LielAmar   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5gvpj/training_a_dqn_in_a_froggerlike_game/</guid>
      <pubDate>Sat, 31 Aug 2024 06:29:23 GMT</pubDate>
    </item>
    <item>
      <title>Flappy Bird DQN 的损失曲线增加，但模型正在学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f59ayo/loss_curve_for_flappy_bird_dqn_increases_but_the/</link>
      <description><![CDATA[      我使用深度 Q 网络训练了一个 Flappy Bird 代理。第一个图中的得分大约等于小鸟穿过的管道数量（我使用的是第三方 openai gym，Flappy Bird Gymnasium），因此模型肯定学会了玩游戏，但损失一直在增加。我对强化学习还很陌生，但在我之前的机器学习经验中从未见过这种行为。根据您的经验，这意味着什么？此外，我意识到我应该训练更长时间，以便在训练期间充分利用模型。这是我的第一次运行，我计划继续运行保存的 DQN。 https://preview.redd.it/av03fnkbxvld1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=0b3684c3dd56dee7c56a8af44e8deeed22fde027    提交人    /u/Goober329   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f59ayo/loss_curve_for_flappy_bird_dqn_increases_but_the/</guid>
      <pubDate>Fri, 30 Aug 2024 23:25:34 GMT</pubDate>
    </item>
    <item>
      <title>对于多智能体纸牌游戏我应该尝试什么算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f53rnj/what_algorithm_should_i_try_for_a_multiagent_card/</link>
      <description><![CDATA[我目前正在学习 RL 和 AI，并在 PyTorch 中尝试了 DQN 和 Double DQN。 我想尝试实现一个多智能体纸牌游戏，我想听听你们的意见，了解最好的入门方法。由于这是一个不完全信息游戏，我认为更简单的方法不适用于此。 我为此建立了一个概念证明，其中 4 个 Double DQN 智能体相互对抗，每个模型都会从环境中分配一个玩家，然后进行游戏。作为状态，我给了它表中所有类型卡片的数量（13 个值）、手中所有类型卡片的数量（13 个值）以及有关游戏的更多信息。 不幸的是，正如预期的那样，它无法学到太多东西，所以我正在寻找其他选择。 这是游戏：https://bicyclecards.com/how-to-play/presidents 此外，我遇到的一个问题是模型在选择有效的动作时遇到了问题（它通常没有足够的所选类型的卡片，或者它与桌面上的卡片不匹配）。    提交人    /u/Neither_Butterfly_51   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f53rnj/what_algorithm_should_i_try_for_a_multiagent_card/</guid>
      <pubDate>Fri, 30 Aug 2024 19:25:06 GMT</pubDate>
    </item>
    <item>
      <title>强化学习调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4si41/reinforcement_learning_survey/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz/generalization-reinforcement-learning    由   提交  /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4si41/reinforcement_learning_survey/</guid>
      <pubDate>Fri, 30 Aug 2024 11:06:15 GMT</pubDate>
    </item>
    <item>
      <title>从自定义环境构建环境张量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4rxvb/building_environment_tensor_from_custom_env/</link>
      <description><![CDATA[我正在构建一个自定义扑克环境来训练 deepQLearning 代理进行游戏。我目前使用的基因环境张量如下所示： 大小：1,14 [玩家卡牌、玩家卡牌、桌子卡牌、桌子卡牌、桌子卡牌、桌子卡牌、桌子卡牌，] 其中每张牌由 2 个变量 hand_value 和 suit 表示。 但是当前的代理很垃圾，因为我没有将足够的信息从环境传递给算法。就像他们目前只将玩家牌映射到最终手牌值的奖励输出（如果这有意义的话） 但是我在扑克环境中存储了太多信息，我想将这些信息传递到深度 Q 学习算法中： 其他玩家的数量每个玩家的资金每个玩家之前的操作： 但我不确定应该如何构建环境张量，以便它保存有关所有这些提到的变量的信息，同时还优先考虑“players_cards”（因为实际上这些将决定玩家是否应该加注/弃牌/可以采取的任何其他操作） 是否有人知道任何资源或技巧来构建这个环境张量以传递到前馈网络来执行深度 Q 学习    提交人    /u/Amazing_Track4881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4rxvb/building_environment_tensor_from_custom_env/</guid>
      <pubDate>Fri, 30 Aug 2024 10:31:24 GMT</pubDate>
    </item>
    <item>
      <title>如何训练模型在基于网格的环境中导航到固定目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4q982/how_do_i_train_a_model_to_navigate_to_a_fixed/</link>
      <description><![CDATA[      我怎么也想不通，这个问题困扰了我好几个月。我最初以为让环境基于网格会简化训练，但我仍然在努力获得我想要的结果。我迫不及待地想完成这个项目，然后更直接地使用 PyTorch 或 Keras 等框架，而不必过多依赖 Gymnasium 或 Stable Baselines。 这是我的代码：https://codeshare.io/Q8A4VW。 当前的奖励函数很简单：  达到目标：+100（游戏结束） 撞到障碍物：-100（游戏结束） 每次移动：-2（以鼓励最佳寻路）  我已经尝试调整熵系数并修改奖励函数，但似乎没有任何效果。任何建议都将不胜感激！ PS：抱歉，如果这不是提问的正确地方，请告诉我。    提交人    /u/Z-A-F-A-R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4q982/how_do_i_train_a_model_to_navigate_to_a_fixed/</guid>
      <pubDate>Fri, 30 Aug 2024 08:36:05 GMT</pubDate>
    </item>
    <item>
      <title>如何训练模型在基于网格的环境中导航到固定目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6z4/how_do_i_train_a_model_to_navigate_to_a_fixed/</link>
      <description><![CDATA[      我怎么也想不通，这个问题困扰了我好几个月。我最初以为让环境基于网格会简化训练，但我仍然难以获得我想要的结果。我渴望完成这个项目，并更直接地使用 PyTorch 或 Keras 等框架，而不必过多依赖 Gymnasium 或 Stable Baselines。  这是我的代码：https://codeshare.io/Q8A4VW。 当前的奖励函数很简单：  达到目标：+100（游戏结束） 撞到障碍物：-100（游戏结束） 每次移动：-2（以鼓励最佳寻路）  我已经尝试调整熵系数并修改奖励函数，但似乎没有任何效果。任何建议都将不胜感激！ PS：抱歉，如果这不是提问的正确地方，请告诉我。    提交人    /u/Z-A-F-A-R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6z4/how_do_i_train_a_model_to_navigate_to_a_fixed/</guid>
      <pubDate>Fri, 30 Aug 2024 08:31:32 GMT</pubDate>
    </item>
    <item>
      <title>如何训练模型在基于网格的环境中导航到固定目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6jg/how_do_i_train_a_model_to_navigate_to_a_fixed/</link>
      <description><![CDATA[      我怎么也想不通，这个问题困扰了我好几个月。我最初以为让环境基于网格会简化训练，但我仍然在努力获得我想要的结果。我迫不及待地想完成这个项目，然后更直接地使用 PyTorch 或 Keras 等框架，而不必过多依赖 Gymnasium 或 Stable Baselines。 这是我的代码：https://codeshare.io/Q8A4VW。 当前的奖励函数很简单：  达到目标：+100（游戏结束） 撞到障碍物：-100（游戏结束） 每次移动：-2（以鼓励最佳寻路）  我已经尝试调整熵系数并修改奖励函数，但似乎没有任何效果。任何建议都将不胜感激！ PS：抱歉，如果这不是提问的正确地方，请告诉我。    提交人    /u/Z-A-F-A-R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6jg/how_do_i_train_a_model_to_navigate_to_a_fixed/</guid>
      <pubDate>Fri, 30 Aug 2024 08:30:41 GMT</pubDate>
    </item>
    <item>
      <title>关于 RL 中的符号的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4p34u/questions_about_notation_in_rl/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4p34u/questions_about_notation_in_rl/</guid>
      <pubDate>Fri, 30 Aug 2024 07:11:02 GMT</pubDate>
    </item>
    </channel>
</rss>