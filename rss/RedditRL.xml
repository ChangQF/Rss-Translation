<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 17 Jun 2024 21:14:28 GMT</lastBuildDate>
    <item>
      <title>模型似乎没有学到任何东西</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di8da4/model_doesnt_seem_to_be_learning_anything/</link>
      <description><![CDATA[我目前正在创建一个期权交易环境。由于在任何给定时间有多少期权合约，我只关注那些距离执行价格和到期日在一定距离内的合约。例如，如果 SPY 的当前价格为 300，它将包含 295-304 之间以及距离到期日 5 天之间的每个执行价格数据。因此，操作空间为 2（看涨/看跌）x 股票代码数量（仅查看 spy）x strike_size x expiration_size。我目前正在处理的数据集是 2 x 1 x 10 x 5。我尝试过不同的模型，如 PPO 和 SAC，但回报似乎没有增加。是因为我的状态和操作空间太大吗？可能是因为我定义环境的方式？我实施了一个系统，其中所拥有的合约根据日期是否发生变化或当前价格是否移动到新的执行范围而转移到新的位置。    提交人    /u/newjeison   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di8da4/model_doesnt_seem_to_be_learning_anything/</guid>
      <pubDate>Mon, 17 Jun 2024 20:39:52 GMT</pubDate>
    </item>
    <item>
      <title>Pong 游戏的自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di6q6v/custom_environment_for_pong_game/</link>
      <description><![CDATA[因此，我为 Pong 游戏创建了一个自定义环境，但遇到了一些问题（reddit 新手），因为我已经在这个问题上卡了好几个小时了 因此，基本机制是，有一个击球手和一个球 如果球击中顶部、底部和右侧墙壁，它应该被反射，如果它击中左侧墙壁，这意味着击球手无法接住它，那么游戏就结束了，但是，如果击球手和球相撞会怎样？我被困在这个逻辑中，我该如何实现这一切呢    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di6q6v/custom_environment_for_pong_game/</guid>
      <pubDate>Mon, 17 Jun 2024 19:31:48 GMT</pubDate>
    </item>
    <item>
      <title>完全隐藏对手的游戏 AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di1sr3/game_ai_with_completely_hidden_opponent/</link>
      <description><![CDATA[我有一个场景，我正在尝试构建一个对抗性 RL 解决方案。有一个代理（代理 X）从点 A 开始，需要遍历到点 B，然后返回点 A。有一个对手（代理 Y），其工作是使用上方的摄像头搜索代理 X。代理 Y 可以在成功识别代理 X 10 步后发起攻击。代理 X 永远不会知道代理 Y 是否真正看到了他们，直到他们被袭击杀死。  我的问题是......如果代理 X 实际上不知道他们是否被代理 Y 观察，那么塑造代理 X 的奖励函数以帮助他们隐秘行动的好方法是什么。或者他们会学会“随机”行动，以至于不会被追踪，仅仅因为被杀的巨大负面奖励？    提交人    /u/Cheap_Leather_6432   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di1sr3/game_ai_with_completely_hidden_opponent/</guid>
      <pubDate>Mon, 17 Jun 2024 16:07:11 GMT</pubDate>
    </item>
    <item>
      <title>梯度尺寸差异 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di13c6/gradient_size_difference_ppo/</link>
      <description><![CDATA[大家好，在我的 PPO 无法学习并继续打印网络中梯度的范数后，我关注了这个帖子。我发现价值函数范数比策略的范数大得多，甚至大 1000 倍。这导致我的表示模型的范数很大。 尽管随着训练的继续，优势确实会变小，但范数仍然很大。 有人知道可能是什么问题吗？ 附言：我正在运行具有多个级别的分层代理。    提交人    /u/sagivborn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di13c6/gradient_size_difference_ppo/</guid>
      <pubDate>Mon, 17 Jun 2024 15:38:10 GMT</pubDate>
    </item>
    <item>
      <title>集成模型的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhrtf2/reinforcement_learning_for_ensemble_models/</link>
      <description><![CDATA[大家好，据我所知，RLHF 一直与 Gen AI 任务相关联。原因是，由于 gen AI 是随机的并且可以生成多个响应（基于提示、温度等参数的细微变化），因此它是对齐的良好候选者。这是通过给它足够的成对样本（首选，不太首选）来实现的，然后它最小化由训练不足的策略生成的分布与首选选项之间的距离。这是通过批次完成的，并在一个时期内取平均值。到目前为止一切都好吗？ 我有一个集成模型（比如说 BERT 和 LayoutLM），其工作是从文档中提取键值对。由于这两个模型都是端到端联合训练的，因此它能够为相同的底层输入生成不同的选项（因为输入是通过 OCR 生成的文本，并且 OCR 本身可以根据文档的质量产生细微的差异）。我可以使用人工代理（通过 UX 生成的输出进行校正）生成的反馈来训练策略吗？我对这种方法感到困惑的原因是我无法说服自己为什么需要奖励函数 + 策略训练，因为我可以非常简单地使用人工操作员所做的校正来微调模型。如果只向模型传递正确答案，而不给它提供了解正确答案和错误答案之间区别的选项，我会错过什么吗？     提交人    /u/immortanslow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhrtf2/reinforcement_learning_for_ensemble_models/</guid>
      <pubDate>Mon, 17 Jun 2024 06:51:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么相关样本在基于策略的方法或更新 Actor 中不会出现问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhra6a/why_correlated_samples_are_not_problematic_in/</link>
      <description><![CDATA[在 DQN 中，我们说 s、s&#39; 是相关的，我们使用重放缓冲区来打破这种关系。  在 PPO 中，当我们更新价值网络时，相同的相关性问题不会影响更新吗？    提交人    /u/seatedrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhra6a/why_correlated_samples_are_not_problematic_in/</guid>
      <pubDate>Mon, 17 Jun 2024 06:13:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么不使用 Actor-Critic 和 PPO 中的目标网络来更新价值网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhr94u/why_not_use_a_target_network_in_actorcritic_ppo/</link>
      <description><![CDATA[在 DQN 中，我们使用目标网络来阻止目标移动。但是我们不对 PPO 或 Actor-Critic 方法使用相同的技巧。为什么移动目标对于这些方法的价值网络来说不是问题？在这两种方法中，目标 = r(s,a) + V(s&#39;)。因此更新 V(s) 将改变 V(s&#39;)，这不会影响稳定性吗？    提交人    /u/seatedrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhr94u/why_not_use_a_target_network_in_actorcritic_ppo/</guid>
      <pubDate>Mon, 17 Jun 2024 06:11:26 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Gym 什么时候被弃用了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhqx8y/when_did_isaac_gym_get_deprecated/</link>
      <description><![CDATA[我刚刚在下载网站上看到它被标记为“现已弃用”，并且将不再受支持，而应该考虑使用 Isaac Lab。 这到底是什么时候发生的？有人知道吗？ 我仍然需要研究 Isaac Lab，但有点担心是否会有很多新东西需要学习。我还是这个行业的新手。    提交人    /u/stop_stalking_me_plz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhqx8y/when_did_isaac_gym_get_deprecated/</guid>
      <pubDate>Mon, 17 Jun 2024 05:48:37 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习（DQN）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhopi4/deep_reinforcement_learning_dqn/</link>
      <description><![CDATA[我正在尝试为棋盘游戏开发深度强化学习模型，但训练效果并不理想。我需要与更有经验的人交流，帮助我改进模型。我来自中国，通过 GPT 找到了这个论坛。由于中国的互联网并未全球连接，我对 AI 相关知识的获取有限。我愿意付费服务，希望得到您的帮助。最低支付额为 700 美元，如果帮助有效则没有上限。请联系我 [xk520zyq@126.com]() 了解更多详情。    提交人    /u/Routine-Shift-2072   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhopi4/deep_reinforcement_learning_dqn/</guid>
      <pubDate>Mon, 17 Jun 2024 03:29:03 GMT</pubDate>
    </item>
    <item>
      <title>“创造力已远离聊天：消除语言模型偏见的代价”，Mohammedi 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhkn9o/creativity_has_left_the_chat_the_price_of/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhkn9o/creativity_has_left_the_chat_the_price_of/</guid>
      <pubDate>Sun, 16 Jun 2024 23:45:38 GMT</pubDate>
    </item>
    <item>
      <title>自定义体育馆环境：观察空间问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhilgl/custom_gymnasium_env_observation_space_problem/</link>
      <description><![CDATA[您好，我正在 pygame 中构建一个类似于 PvZ 的游戏，但它没有玩家，而是有一个应该学习如何玩游戏的代理。为此，我使用了 gymnasium，但我对这个模块还很陌生。 我目前正在尝试实现自定义 gym 环境，但在观察空间中遇到困难。 错误： 回溯（最近一次调用最后一次）： 文件 &quot;...\PVZ-RL\pvz_env.py&quot;，第 260 行，在 &lt;module&gt; check_env(env.unwrapped) 文件 &quot;...\Python\Python310\site-packages\gymnasium\utils\env_checker.py&quot;，第 301 行，在 check_env 中 check_reset_return_type(env) 文件&quot;...\Python\Python310\site-packages\gymnasium\utils\env_checker.py&quot;，第 214 行，在 env.observation_space 中的 check_reset_return_type obs 中 AssertionError: `env.reset()` 返回的第一个元素不在观察空间内。  代理在 9x5 的网格中移动，可以放置植物或收集太阳，就像在实际游戏中一样。 如视频所示，代理有 7 个动作： class AgentAction(Enum): UP=0 DOWN=1 LEFT=2 RIGHT=3 PLACE_PEASHOOTER= 4 PLACE_SUNFLOWER= 5 COLLECT_SUN = 6  我不太了解 observer_space 的高参数，但我选择了可​​能变量的最大数量作为值，例如例如： 最大植物数量 = 9*5（网格大小） 收集到的最大太阳数量 = 2000 代理的最大位置数量 = 9*5（网格大小） 杀死的最大僵尸数量 = 1000 我的自定义环境如下： class GameEnv(gym.Env): metadata = {&quot;render_modes&quot;: [&quot;human&quot;], &#39;render_fps&#39;: 30} def __init__(self, grid_rows=9, grid_cols=5, render_mode=None): self.zombies_killed_counter = 0 self.grid_rows = grid_rows self.grid_cols = grid_cols self.render_mode = render_mode self.pvz_game = Game(fps=self.metadata[&#39;render_fps&#39;]) self.action_space = space.Discrete(len(AgentAction)) self.observation_space = space.Box( low=0, high=np.array([9*5,2000,9*5,1000]), shape=(4,), dtype=np.int32 )  我的重置函数如下： def reset(self, seed=None, options=None): super().reset(seed=seed) self.pvz_game.reset(seed=seed) plants_owned = self.pvz_game.agent.get_plants_owned() suns = self.pvz_game.agent.get_suns() pos = self.pvz_game.agent.get_pos() zombies_killed = self.pvz_game.agent.get_zombies_killed() plants_owned = np.array(self.encode_plants(self.pvz_game.agent.get_plants_owned()), dtype=np.int32) suns = np.array([self.pvz_game.agent.get_suns()], dtype=np.int32) pos = np.array(self.pvz_game.agent.get_pos(), dtype=np.int32) zombies_killed = np.array([len(self.pvz_game.agent.get_zombies_killed())], dtype=np.int32) obs = np.concatenate((plants_owned, suns, pos, zombies_killed)) info = {} if self.render_mode == &#39;human&#39;: self.render()返回 obs, info  我不太明白这里的问题。    提交人    /u/Pyjam4a   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhilgl/custom_gymnasium_env_observation_space_problem/</guid>
      <pubDate>Sun, 16 Jun 2024 22:04:44 GMT</pubDate>
    </item>
    <item>
      <title>寻求使用 MuJoCo（或类似模拟器）进行机器人和人工智能项目的资源和建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhaixr/seeking_resources_and_advice_for_using_mujoco_or/</link>
      <description><![CDATA[大家好， 我是人工智能和深度学习领域的研究人员。虽然我在这些领域拥有丰富的知识，但我在图形和模拟器方面的经验有限。在学习计算机科学期间，我选修了一门与 Unity 相关的课程，但这就是我对模拟器的熟悉程度。 目前，我正在开展一个涉及认知架构的项目，并希望使用模拟器与我们使用的机器人一起工作。我正在寻找一个快速的模拟器，让我能够加快模拟速度，因为我有一个强大的 GPU (RTX)。 我听说我的一些同事使用 ROS，但我听说 Gazebo 可能不是快速模拟的最佳选择。出于这个原因，我正在考虑 MuJoCo，但我也愿意接受其他可能更适合我的项目的选择（我听说过 PyBullet 和 Isaac）。 您能否推荐一些资源，例如课程或教程，用于学习如何使用 MuJoCo 或其他快速高效的模拟器？如果能提供任何关于哪种模拟器最适合我的需求的建议，我将不胜感激 提前感谢您的帮助！    提交人    /u/SympathyOutside   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhaixr/seeking_resources_and_advice_for_using_mujoco_or/</guid>
      <pubDate>Sun, 16 Jun 2024 15:51:43 GMT</pubDate>
    </item>
    <item>
      <title>“使用大型语言模型发现偏好优化算法”，Lu 等人 2024（使用 LLM 编写新的 Python 损失函数发现 DPO 的小幅改进）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh9ts7/discovering_preference_optimization_algorithms/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh9ts7/discovering_preference_optimization_algorithms/</guid>
      <pubDate>Sun, 16 Jun 2024 15:19:22 GMT</pubDate>
    </item>
    <item>
      <title>在开始 RL 部分之前，如何设计自定义环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh5xnb/how_do_i_design_a_custom_environment_before/</link>
      <description><![CDATA[嘿，我只是想知道如何创建自定义 RL 环境。Youtube 视频通常只选择现成的自定义环境（例如 sentdex 在他的教程中选择了蛇游戏）。在开始担心 RL 的其他方面（如观察和动作空间、奖励函数等）之前，我想了解如何创建自己的环境。任何帮助都将不胜感激。 我需要数据集或任何东西来创建我的自定义环境吗？或者说如果我有布局的坐标，我可以使用它吗？    提交人    /u/Strange-Durian3382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh5xnb/how_do_i_design_a_custom_environment_before/</guid>
      <pubDate>Sun, 16 Jun 2024 11:52:34 GMT</pubDate>
    </item>
    <item>
      <title>哪个 RL 库最好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgys73/which_rl_library_is_best/</link>
      <description><![CDATA[我们正在寻求为一个项目实现一个自定义的 RL 环境。该环境相当复杂，涉及机场滑行布局，以分析飞机的滑行路线。对此有几个问题 -   哪个框架最适合这个？我们已经尝试使用 OpenAI Gym 的 stable-baselines3，但感觉非常受限。还看到了一些其他 RL 库，如 Acme、Ray (Rllibs) 等。 上述所有库是否都支持自定义环境，以及它对用户的友好程度如何？     提交人    /u/Strange-Durian3382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgys73/which_rl_library_is_best/</guid>
      <pubDate>Sun, 16 Jun 2024 03:32:55 GMT</pubDate>
    </item>
    </channel>
</rss>