<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 25 Mar 2024 03:16:24 GMT</lastBuildDate>
    <item>
      <title>为什么具有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmx1pf/why_is_my_stable_baselines3_model_with_cartpole/</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加，而剧集平均奖励保持不变为 -1，这就是sutton_barto_reward系统工作正常。 从cartpole导入gymnasium 从stable_baselines3导入CartPoleEnv 从stable_baselines3.ppo.policies导入PPO 导入MlpPolicy env = CartPoleEnv(sutton_barto_reward=True) model = PPO(&quot;MlpPolicy&quot;, env, gamma=1, verbose=1) model.learn(total_timesteps=30000) &lt; /p&gt; 但是，我不明白为什么会这样，因为折扣率已设置为 1。剧集长度平均值是否应该没有任何改善，因为剧集的累积奖励始终是一样吗？ ​ ​ ​   由   提交/u/uglyboi34  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmx1pf/why_is_my_stable_baselines3_model_with_cartpole/</guid>
      <pubDate>Sun, 24 Mar 2024 22:10:57 GMT</pubDate>
    </item>
    <item>
      <title>颠倒的累积奖励图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmq4l7/upside_down_cumulative_reward_graph/</link>
      <description><![CDATA[      我正在 OpenAI 健身房中使用 Q 学习模型开发车杆程序。  我不知道为什么，但我的累积奖励图完全颠倒了。从一开始就急剧下降，并在较低值处持平。如果完全相反，那将是正确的图表。 这非常令人沮丧，因为我觉得我已经很接近了，但在几个小时后我还无法调试它： ​  https://preview.redd.it/im0hvjingbqc1.png?width=767&amp;format=png&amp;auto=webp&amp;s=057240031d60ca6b607a7e08145f538c0c9dfd0a 如果我可以提供更多信息，请告诉我。谢谢！   由   提交 /u/deliberatedice   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmq4l7/upside_down_cumulative_reward_graph/</guid>
      <pubDate>Sun, 24 Mar 2024 17:24:47 GMT</pubDate>
    </item>
    <item>
      <title>[D] Aleksa Godric 关于在 DeepMind 找到工作的帖子在今天仍然具有现实意义吗？ [是的]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bml5tw/d_is_aleksa_godrics_post_on_landing_a_job_at/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bml5tw/d_is_aleksa_godrics_post_on_landing_a_job_at/</guid>
      <pubDate>Sun, 24 Mar 2024 13:47:38 GMT</pubDate>
    </item>
    <item>
      <title>我究竟做错了什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmgifh/what_am_i_doing_wrong/</link>
      <description><![CDATA[我正在尝试训练 cartpole 代理。但他似乎并没有学到任何东西。我也尝试调试和更改超参数，但它仍然没有学到任何东西。 请帮助这里可能出了什么问题？ gridworld/dqn.py 位于 main · bherwanisuraj/gridworld (github.com)  P.S.感谢先生们抽出宝贵的时间来帮助我。  问题现已解决。显然，目标模型从未得到更新，因为我是在每个 update_target % epoch 而不是 epoch % update_target 更新它。 我知道这是一个愚蠢的错误。我会不断学习，让自己变得更好。再次感谢大家。   由   提交/u/purna_lingham  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmgifh/what_am_i_doing_wrong/</guid>
      <pubDate>Sun, 24 Mar 2024 09:10:22 GMT</pubDate>
    </item>
    <item>
      <title>PPO 和 DreamerV3 代理完成了《愤怒之铁拳》。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm9sjk/ppo_and_dreamerv3_agent_completes_streets_of_rage/</link>
      <description><![CDATA[不太确定我们是否可以自我推销，但我看到有人发布了他们的经纪人完成《街头霸王 3》的视频，所以我希望它被允许。&lt; /p&gt; 我一直在训练特工玩《怒之铁拳》的第一个阶段，现在终于可以完成游戏了，我的视频更多是为了娱乐，所以没有太多技术，但我将在下面解释一些内容。无论如何，这里是视频的链接： https://www.youtube.com/watch? v=gpRdGwSonoo ​ 这总共由 8 个模型完成，每个阶段 1 个模型。前 4 个模型是使用 SB3 训练的 PPO 模型，后 4 个模型是使用 SheepRL 训练的 DreamerV3 模型。两者都在相同的稳定复古健身房环境中使用我的奖励函数进行训练。 DreamerV3 在游戏的 64x64 像素 RGB 图像上进行训练，具有 4 个跳帧且无帧堆叠。 PPO 在游戏的 160x112 像素单色图像上进行训练，具有 4 个跳帧和 4 个帧堆叠。 每个连续阶段的模型都是在最后一个阶段的基础上构建的，除了切换到 DreamerV3 时，因为我再次从头开始，而且除了第8阶段游戏由向右移动改为向左移动外，我决定再次从头开始。 至于“娱乐”，我决定重新开始。在视频方面，Gym 环境基本上返回一些有关游戏状态的数据，然后我将其形成文本提示，并将其输入到开源 LLM 中，以便它可以对转换为 TTS 的游戏玩法做出一些简单的评论，同时让 Whisper 模型将我的 SpeechToText 转换，以便我也可以与角色交谈（当我说出角色的名字时触发）。这一切都连接到我制作的 UE5 应用程序，其中包含虚拟角色和环境。 我断断续续地训练了模型大约 5 或 6 个月的时间（不是直接的），所以我不真的知道我总共训练了他们多少小时。我认为第 8 阶段模型的训练时间大约为 15-30 小时。 DreamerV3 模型在 4 个平行健身房环境中进行训练，而 PPO 模型在 8 个平行健身房环境中进行训练。不管怎样，我希望它很有趣。   由   提交/u/disastorm  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm9sjk/ppo_and_dreamerv3_agent_completes_streets_of_rage/</guid>
      <pubDate>Sun, 24 Mar 2024 02:15:55 GMT</pubDate>
    </item>
    <item>
      <title>为运动队制定时间表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm8lc5/building_schedules_for_sports_teams/</link>
      <description><![CDATA[免责声明：我是这个实践和这个子项目的新手，我正在尽可能快地学习，但仍然感觉我几乎一无所知 我正在尝试建立一个模型来创建一个时间表，其中 N 个团队在 (N-1)*2 周内互相比赛（例如 4 支团队，6 周）。 我认为 RL提供了一条成功之路。 我的具体问题涉及特工将采取的行动和状态，以及如何推进每个情节。我如何构建它以使代理/策略采取行动？一次安排一个团队还是尝试两人一组进行？我是在整个过程中提供奖励还是仅在最后为“有效”的奖励提供奖励？时间表（例如，没有球队自己比赛，对手的分布尽可能平等）。 我有网络编程背景，但这里的数学/代数很难掌握。 感谢任何帮助。 TIA   由   提交/u/brianmorton  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm8lc5/building_schedules_for_sports_teams/</guid>
      <pubDate>Sun, 24 Mar 2024 01:15:58 GMT</pubDate>
    </item>
    <item>
      <title>我从哪说起呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm20ga/where_do_i_start/</link>
      <description><![CDATA[我的任务需要输入 10 个数字集，每个数字的范围从 1 到 50，产生大约 28,291,700,360 种可能的组合。 &lt;我的目标是识别大约 5 组不同的输入数字。我需要使用能够猜测这些数字组的算法。 问题是目前需要 1 个 CPU 核心和大约 2 分钟来评估单个组合。我正在考虑租用 500 个 CPU 核心或过渡到 GPU。 我应该从哪里开始？我应该研究什么方法？   由   提交 /u/oTHeReX   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm20ga/where_do_i_start/</guid>
      <pubDate>Sat, 23 Mar 2024 20:27:13 GMT</pubDate>
    </item>
    <item>
      <title>MuJoCo XLA - 创建具有稳定基线的多环境3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bluvnm/mujoco_xla_make_multienvironment_with/</link>
      <description><![CDATA[我正在尝试使用 MJX 创建一个类似健身房的环境。我已成功使其在 1 个环境中与 stable-baselines3 配合使用。但是，MJX 不适合单一环境训练，训练大约 2 次/秒，这是正常的。 但是，我真的不知道如何使用 vmap 对我的环境进行矢量化。我不明白怎么做... 实际上，我不知道是否可能...有人尝试过 MuJoCo XLA ？ 你有一些我可以实现的吗看看？你知道我应该检查哪个 RL 库来训练 MuJoCO XLA 吗？ 我是 RL 社区的新手。这是为了工作。我刚刚被一家机器人公司录用，在此之前我对强化学习一无所知。所以我现在无法真正制定自己的政策。我有截止日期，所以我现在应该保留框架/库。我的目的是测试所有的非策略算法 + HER，但我的每次运行都需要 3 天才能完成，而我没有这个时间。因此，我尝试通过 GPU 模拟来提高训练速度，以显示一些结果...    由   提交/u/Aedelon0707   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bluvnm/mujoco_xla_make_multienvironment_with/</guid>
      <pubDate>Sat, 23 Mar 2024 15:29:38 GMT</pubDate>
    </item>
    <item>
      <title>PPO 价值损失立即收敛，而保单损失则陷入困境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1blqs2g/ppo_value_loss_converges_immediately_while_the/</link>
      <description><![CDATA[      我正在使用 SB3 训练 PPO 自定义环境。从张量板上可以看出，价值损失很快收敛到几乎为 0，而保单损失似乎随着时间的推移而恶化。此外，熵损失似乎被卡住了。  ​ https://preview.redd.it/qgcxv85ks2qc1.png?width=1648&amp;format=png&amp;auto=webp&amp;s=0d1609d756f42a0f2372b4006836d3eb4d4 e3744 但是，同时，奖励不断增加，但测试成绩却越来越差！  https://preview.redd .it/4vv051myr2qc1.png?width=556&amp;format=png&amp;auto=webp&amp;s=a1fe9ca87349d14c918bcb8e68766db6b02a3425 您能帮我了解问题出在哪里以及如何解决吗？  当前超参数： initial_learning_rate = 0.000005 model = MaskablePPO(MaskableActorCriticPolicy, env,tensorboard_log=&quot;./tensorboard&quot; ,n_steps=1024,learning_rate=initial_learning_rate,ent_coef=0.005)    由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1blqs2g/ppo_value_loss_converges_immediately_while_the/</guid>
      <pubDate>Sat, 23 Mar 2024 12:15:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试稳定的复古</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bl8ol6/experimenting_with_stable_retro/</link>
      <description><![CDATA[我最近一直在尝试复古稳定，它很复杂但很酷。它使用体育馆作为环境的基础和代理的稳定基线3，并在此基础上提供一个界面，以便人们可以使用 8/16 位时代的视频游戏。 首先整个包装系统看起来很复杂，因为你可以结合gymnasium和sbl3课程。更糟糕的是，网上有关于这些库的旧版本的旧文档，例如gym、gym-retro 和 stablebaselines。但是一旦掌握了它，潜力是巨大的。 例如，我一直在用各种游戏训练不同的模型，即使使用 DQN 也有可能取得良好的结果。这并不总是容易的，但这是可能的。一种方法是跳帧，大约 10 帧左右。这可以加快训练速度并在比赛中产生良好的效果。特别是与堆叠几帧相结合，3 个就足够了。 还有一些未记录的功能可以在场景的 json 文件中配置，例如输入图像的裁剪或有效动​​作，但后者似乎无法正常工作，因此我发现创建一个限制操作并将其转换为代理自己的空间的包装器更有效。   由   提交/u/deney-kedeip    reddit.com/r/reinforcementlearning/comments/1bl8ol6/experimenting_with_stable_retro/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bl8ol6/experimenting_with_stable_retro/</guid>
      <pubDate>Fri, 22 Mar 2024 20:04:49 GMT</pubDate>
    </item>
    <item>
      <title>RL 热启动与 IL（SAC、PPO）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bl4r3x/rl_hot_start_with_il_sac_ppo/</link>
      <description><![CDATA[您好，我目前正在尝试创建一个社交导航环境，其中机器人试图达到目标，并且必须避免与障碍物和墙壁碰撞。  问题变得越来越复杂，尽管我尝试调整超参数，但我的 PPO 收敛得不太好。  Si我决定使用模仿学习来“热启动”。强化学习算法。我已经使用 MPC 生成了轨迹，并找到了一个用于 SAC 离线训练的库（ https://github.com/takuseno/d3rlpy ）工作正常，但我真的不知道如何将权重转移到 SAC 策略的网络，因为一切都已完成 我的问题是：解决我的初始问题的最佳方法是什么，如果 IL +强化学习是一个很好的方法，你们有这样做的资源或代码吗？对于连续空格 谢谢   由   提交/u/Ybrik410  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bl4r3x/rl_hot_start_with_il_sac_ppo/</guid>
      <pubDate>Fri, 22 Mar 2024 17:23:08 GMT</pubDate>
    </item>
    <item>
      <title>强化学习理论图景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bky3ej/landscape_of_theoretical_reinforcement_learning/</link>
      <description><![CDATA[嗨，我即将开始撰写强化学习数学硕士论文。我只是想知道 RL 的理论前景是什么样的。我们仍试图回答哪些重大问题？是否有一些特别有前景的特定领域？是否有可能与统计力学或最佳传输或其他看起来有前途的数学领域有联系？ 谢谢！    ;由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bky3ej/landscape_of_theoretical_reinforcement_learning/</guid>
      <pubDate>Fri, 22 Mar 2024 12:30:44 GMT</pubDate>
    </item>
    <item>
      <title>DeepRL 代理与 Ken 一起完成《街头霸王 III》！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkxoho/deeprl_agent_completing_street_fighter_iii_with/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkxoho/deeprl_agent_completing_street_fighter_iii_with/</guid>
      <pubDate>Fri, 22 Mar 2024 12:07:43 GMT</pubDate>
    </item>
    <item>
      <title>需要 DDQN 自动驾驶汽车项目的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</link>
      <description><![CDATA[      我最近开始学习强化学习，我使用ddqn做了一个自动驾驶汽车项目，输入是这些光线的长度输出是向前、向后、向左、向右，什么也不做。我的问题是 rl Agent 需要多少时间来学习？即使已经播出了 40 集，它仍然没有达到奖励门槛。我还根据前进速度给予 0-1 奖励   由   提交/u/Invicto_50  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</guid>
      <pubDate>Fri, 22 Mar 2024 02:31:00 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>