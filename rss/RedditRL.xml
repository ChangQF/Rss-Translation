<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 01 Feb 2025 09:16:13 GMT</lastBuildDate>
    <item>
      <title>DDQN 未能在基于像素的四个房间上进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1if0xth/ddqn_failed_to_train_on_pixel_based_four_rooms/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1if0xth/ddqn_failed_to_train_on_pixel_based_four_rooms/</guid>
      <pubDate>Sat, 01 Feb 2025 07:10:41 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型思考太快，无法有效探索”，Pan 等人 2025 年（探索不佳 - GPT-4 o1 除外）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iexplv/large_language_models_think_too_fast_to_explore/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iexplv/large_language_models_think_too_fast_to_explore/</guid>
      <pubDate>Sat, 01 Feb 2025 03:47:37 GMT</pubDate>
    </item>
    <item>
      <title>我的 RL 项目缺少什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iexhxm/what_am_i_missing_with_my_rl_project/</link>
      <description><![CDATA[      我正在训练一个代理，让它擅长玩我制作的游戏。它在小行星在 2D 空间中向下坠落的环境中操作一艘航天器。到达底部后，小行星会在顶部以随机速度以随机位置重生。（太随机了？） 普通 DQN 和 Double DQN 不起作用。 我切换到 DuelingDQN 并添加了重播缓冲区。 随着训练的继续，损失终于减少了，但学习到的策略仍然导致高度可变的性能，平均而言没有实际改善。 我的奖励结构有问题吗？ 目前对每一步幸存使用 +1，并对小行星碰撞使用 -50 惩罚。 非常感谢您提供的任何帮助。我是新手，已经挣扎了好几天了。    提交人    /u/GimmeTheCubes   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iexhxm/what_am_i_missing_with_my_rl_project/</guid>
      <pubDate>Sat, 01 Feb 2025 03:36:00 GMT</pubDate>
    </item>
    <item>
      <title>“赋权有助于创意视频游戏中的探索行为”，Brändle 等人 2023 年（无先验的人类探索效率低下）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iexccf/empowerment_contributes_to_exploration_behaviour/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iexccf/empowerment_contributes_to_exploration_behaviour/</guid>
      <pubDate>Sat, 01 Feb 2025 03:27:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么 RL 比进化启发方法更受欢迎？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iex9km/why_is_rl_more_preferred_than_evolutioninspired/</link>
      <description><![CDATA[免责声明。我试着不带偏见。但趋势似乎是朝着深度强化学习发展。本文无意“争论”任何事情。我既不愿意也没有知识去主张什么。 进化算法实际上在 Sutton&amp;Barto 的著名书籍的开头就提到过，但我太笨了，无法理解上下文（我只是一个普通读者和业余爱好者）。 那里没有提到但我想到的另一个原因是并行化。我们都知道，机器学习热潮已经导致 GPU、TPU 和 NPU 制造商和设计师的股价飙升。我不太了解数学和技术细节，但我相信通过反向传播调整深度网络的能力归功于线性代数和 GPGPU，而进化算法不太可能从它们的帮助中受益。 再次重申，我远离 ML 知识，所以如果我错了，请告诉我。    提交人    /u/Gloomy-Status-9258   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iex9km/why_is_rl_more_preferred_than_evolutioninspired/</guid>
      <pubDate>Sat, 01 Feb 2025 03:23:04 GMT</pubDate>
    </item>
    <item>
      <title>近端策略优化算法（类似于用于训练 o1 的算法）与使用策略优化的一般强化学习 DeepSeek 背后的损失函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ieku4r/proximal_policy_optimization_algorithm_similar_to/</link>
      <description><![CDATA[        由    /u/AsideConsistent1056   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ieku4r/proximal_policy_optimization_algorithm_similar_to/</guid>
      <pubDate>Fri, 31 Jan 2025 17:57:44 GMT</pubDate>
    </item>
    <item>
      <title>“RL + Transformer = 通用问题解决器”，Rentschler & Roberts 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ieefn3/rl_transformer_a_generalpurpose_problem_solver/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ieefn3/rl_transformer_a_generalpurpose_problem_solver/</guid>
      <pubDate>Fri, 31 Jan 2025 13:10:50 GMT</pubDate>
    </item>
    <item>
      <title>搞砸了 DQN 编码面试。感觉很尴尬！！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ie7glt/messed_up_dqn_coding_interview_feel_embarrassing/</link>
      <description><![CDATA[我被一位 RL 科学家面试过。我很好地回答了所有理论问题，但是我搞砸了 DQN 的损失函数编码。我愣住了，写不出来。一个字也写不出来。所以我只写了关于代码逻辑的注释。我有 5 分钟的时间来写，但只有 4 行。做不到。面试结束后，我花了 10 分钟才写出来。我把代码发给他们，但我认为他们不会接受。我觉得我不会被选中进入下一轮。    提交人    /u/Remote_Marzipan_749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ie7glt/messed_up_dqn_coding_interview_feel_embarrassing/</guid>
      <pubDate>Fri, 31 Jan 2025 05:08:26 GMT</pubDate>
    </item>
    <item>
      <title>RL 的发展方向是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ie2l1a/where_is_rl_headed/</link>
      <description><![CDATA[大家好，我是一名在 RL 领域工作的博士生。尽管我从事这一领域的工作，但我对它的发展方向没有很强的认识，特别是在现实世界应用的可用性方面。除了 RL 的 Deepseek/GPT 用途（有些人认为这实际上不是 RL）之外，我经常感到沮丧，因为这个领域没有发展方向，我花在摆弄挑剔算法上的所有时间都浪费了。 我想听听你的想法。您预见到未来几年 RL 的趋势是什么？您预见到 RL 在不久的将来会在哪些行业应用领域有用？    提交人    /u/Sudden-Eagle-9302   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ie2l1a/where_is_rl_headed/</guid>
      <pubDate>Fri, 31 Jan 2025 00:51:15 GMT</pubDate>
    </item>
    <item>
      <title>土匪研究新手</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idq2go/newbie_in_bandit_research/</link>
      <description><![CDATA[我最近开始研究老虎机问题，并注意到许多相关文献都是高度理论化的，通常侧重于证明遗憾界限和类似的结果。我一直在阅读 Tor Lattimore 和 Csaba Szepesvári 的老虎机算法，除了测度理论方面，我可以轻松地理解大约 90% 的内容，并且对数学充满信心。此外，我还阅读了许多与我的具体问题相关的论文，并且可以理解数学论证。 然而，我的主要挑战是理解解决这些问题的整体方法。在老虎机研究中，目标通常是推导出遗憾界限，但我很难对新问题或算法进行分析。这似乎没有一个明确的标准框架，论文中的许多技术感觉像是凭空而来的。例如，一些论文引入了看似不相关的引理，然后以一种不太明显的方式将它们与主要分析联系起来。 如果有人能分享他们研究理论 RL/bandit 的经验或见解，我将不胜感激！    提交人    /u/Careless-Breath2913   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idq2go/newbie_in_bandit_research/</guid>
      <pubDate>Thu, 30 Jan 2025 15:54:58 GMT</pubDate>
    </item>
    <item>
      <title>极快的优先采样</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idl178/blazingly_fast_prioritized_sampling/</link>
      <description><![CDATA[  由    /u/JacksOngoingPresence  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idl178/blazingly_fast_prioritized_sampling/</guid>
      <pubDate>Thu, 30 Jan 2025 11:36:35 GMT</pubDate>
    </item>
    <item>
      <title>有关测量误差的论文？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idkp6u/papers_on_measurement_error/</link>
      <description><![CDATA[你们知道关于测量误差和模型在决策过程中导致不准确估计的任何书面文章吗？    提交人    /u/mowmail   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idkp6u/papers_on_measurement_error/</guid>
      <pubDate>Thu, 30 Jan 2025 11:13:44 GMT</pubDate>
    </item>
    <item>
      <title>对多智能体环境的质疑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idk7cn/doubt_regarding_multi_agent_environments/</link>
      <description><![CDATA[大家好！我有使用 DRL 的经验，但环境只有一个代理。现在我正在研究 GitHub 上 CybORG repo 的多代理“Scenario1b”，并尝试使用 Stable-Baselines3 训练一些代理。我已经用 PettingZoo 制作了一个包装器，我有几个疑问： 1- 在那种环境中，通常有一个很大的动作空间，但实际上可以执行的动作较少（即可以执行的动作已被过滤并从那些不能执行的动作中筛选出来）。这通常被称为“动作掩蔽”。我的疑问是，这是否可以包含在步骤方法本身中，还是必须像本例 (https://pettingzoo.farama.org/tutorials/sb3/connect_four/) 一样单独实现？ 2- 据说 SB3 不支持“dict”动作空间，但是在这个使用 SB3 的多代理环境的示例 (https://pettingzoo.farama.org/tutorials/sb3/kaz/) 中，它确实有一个 Dict 动作空间。这怎么理解呢？ 提前谢谢！！    提交人    /u/Carpoforo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idk7cn/doubt_regarding_multi_agent_environments/</guid>
      <pubDate>Thu, 30 Jan 2025 10:38:13 GMT</pubDate>
    </item>
    <item>
      <title>与我们都在做的 RL 相比，为什么 LLM 上的 RL 微调如此容易和稳定？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</link>
      <description><![CDATA[我一直在观察不同的人尝试重现 Deepseek 训练配方，与我习惯的 RL 相比，我对这种配方的稳定性感到震惊。 经过大约 50 个训练步骤后，他们在数学问题上的准确率可靠地达到了 50%。他们尝试了几种不同的 RL 算法，并报告说它们的效果大致相同，而无需任何超参数调整。 如果我能在仅 50 个训练步骤中将平衡手推车的成功率提高 50%，我会认为自己很幸运。而且我可能必须为每个任务调整超参数。  （我的理论：由于无监督的预训练，这很容易。该模型已经学习了良好的表示和背景知识 - 即使它无法在 RL 之前完成任务 - 这使得问题变得容易得多。也许我们应该在 RL 中做更多这样的事。）    提交人    /u/currentscurrents   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</guid>
      <pubDate>Wed, 29 Jan 2025 19:31:43 GMT</pubDate>
    </item>
    <item>
      <title>谁在奖励 DeepSeek R1？在 RL 中，您需要一些奖励功能或手动奖励，不是吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</link>
      <description><![CDATA[他们说他们没有在大数据上进行自监督学习，那么奖励模型必须以某种方式在某些数据上进行训练，ChatGPT API 或 LLAMA 可以用作奖励工具，或者谁知道在没有奖励基线的情况下思想链是如何工作的？ PS：据我了解，他们根据版本进度使用 LLAMA 作为基线 LLM 模型，并且它是公开可用的    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</guid>
      <pubDate>Wed, 29 Jan 2025 11:13:03 GMT</pubDate>
    </item>
    </channel>
</rss>