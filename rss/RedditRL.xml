<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 19 Jan 2024 15:14:57 GMT</lastBuildDate>
    <item>
      <title>本文如何将两项政策结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</link>
      <description><![CDATA[      我正在阅读这篇论文，我对算法 1 中的一些细节有点迷失 -  ​ ​ https://preview.redd.it/ f0hhmaye8edc1.png?width=845&amp;format=png&amp;auto=webp&amp;s=a99793f4a5bc099c1a165145aeb44e4db441d95c 在第 3 行中，它们似乎通过 $h 组合了 $h$ 和 $f$ (s) = \pi(s) + f(s)$。我不明白这是怎么发生的。  他们在本节中称 $h$ 为混合策略，但我不明白 -  ​ https://preview.redd.it/4flfgn1x8edc1.png?width=851&amp;format =png&amp;auto=webp&amp;s=a2108806ba92d46e9afcd7668dc9c78ee58120e9 ​ 如果需要任何说明，请告诉我。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</guid>
      <pubDate>Fri, 19 Jan 2024 12:52:43 GMT</pubDate>
    </item>
    <item>
      <title>[需要建议/反馈]第2部分：决斗双倍DQN奖励大多在0和2之间波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</guid>
      <pubDate>Fri, 19 Jan 2024 09:19:09 GMT</pubDate>
    </item>
    <item>
      <title>RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a9nli/rl/</link>
      <description><![CDATA[大家好 我想你会发现这篇文章很愚蠢。但我没有NN专业背景。我了解反向传播的基础知识以及有关神经网络的其他简单知识，我编写简单的神经网络进行函数预测。我在基本级别上也了解 RL 应该如何工作。为了好玩，我尝试使用 chatGPT 与 RL 代理创建基本游戏，该游戏使用输入作为整个屏幕。 我的游戏是两个 2d 玩家（圆圈）可以旋转、前进并发射射弹。寻找良好的奖励函数有点困难。因为如果我写下仅通过命中和惩罚来获得奖励，当射弹击中我时，我会错过特工所做的很多动作，而且我不知道如何奖励它。我想我错过了一些重要的事情。也许我应该记录几个动作并将它们奖励为 1 个动作。当射弹击中我时的惩罚是过去几次行动的结果，所以仅仅惩罚最后一个行动看起来是不正确的。  我的项目中有3个文件，如果你愿意的话可以查看ofc。但一般建议也会非常有帮助。  main.py（主游戏循环逻辑） https://pastebin.com/cLm8DC5g&lt; /p&gt; ​ DRLAgent.py。给我写信的逻辑是ChatGPT。我理解代码的概念，但我不明白什么对我的任务有好处，什么对我的任务没有好处。 https://pastebin.com /ST3Pfkk8 ​ DQNCNN.py。也是由 chatGPT 编写的。我完全不明白这段代码。我只知道这是我的屏幕输入的调用层，但我不知道转换层是如何工作的。 https://pastebin.com/ RXfB0meN ​ 最后一个愚蠢的问题。我有两个由同一个代理控制的玩家，我给他们屏幕框架和两个额外的功能，具体取决于哪个玩家 RL 尝试预测更好的动作 [1.0, 0.0] 或 [0.0, 1.0]。我希望他能找到这 2 个功能和屏幕上的颜色之间的依赖关系（因为 2 个玩家有不同的颜色），但我也认为这是一个糟糕的解决方案。 我应该给 RL 额外的信息，比如屏幕上的玩家位置和玩家回转？  非常感谢您的关注。 ​ ​ ​ ​   由   提交/u/Specialist_Soup_4994   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a9nli/rl/</guid>
      <pubDate>Fri, 19 Jan 2024 03:42:26 GMT</pubDate>
    </item>
    <item>
      <title>专门在密室中培训 MiniGrid 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a7mfw/training_minigrid_agents_specifically_the/</link>
      <description><![CDATA[大家好， 我目前正在尝试专门解决 MiniGrid 环境： https://minigrid.farama.org/environments/minigrid/LockedRoomEnv/  这个环境非常稀疏，我一直在尝试用 PPO 解决这个问题，尝试了不同的网络和超参数调整，但没有成功。 是否有人已经解决了这个问题或知道如何解决它？ p&gt; 另外我想知道是否有学习实用深度强化学习的资源（理论资源很多，但实用性并不好）。 谢谢！   由   提交/u/Key_Lie_7975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a7mfw/training_minigrid_agents_specifically_the/</guid>
      <pubDate>Fri, 19 Jan 2024 02:01:49 GMT</pubDate>
    </item>
    <item>
      <title>MARL 逐帧持续学习（格斗游戏研究）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a19ck/frame_by_frame_continuous_learning_for_marl/</link>
      <description><![CDATA[你好！ ​ 我和我的朋友正在研究如何使用 MARL格斗游戏的上下文，其中演员/代理同时提交输入，然后由格斗游戏物理引擎解析。有许多论文在格斗游戏的背景下讨论 DL / RL / 一些 MARL，但值得注意的是，它们不包含源代码或实际上谈论其方法，而是谈论普遍的发现/见解。 ​ 现在我们正在考虑使用 Pytorch（在 CUDA 上运行以提高训练速度）和 Petting Zoo（MARL 体育馆的扩展），特别是使用 AgileRL 库进行超参数优化。我们很清楚，超参数如此之多，当我们试图改进问题时，知道要改变什么是很棘手的。我们设想，我们有 8 个左右的研究游戏引擎实例（我有 10 个核心 CPU）连接到 10 个宠物动物园（可能是敏捷 RL 修改版）训练环境的实例，其中输入/输出在引擎和训练环境，来回。 ​ 我想我是在寻求一些关于我们正在使用的工具的一般建议/提示和反馈。如果您知道解决类似问题的特定教科书、GitHub 存储库的研究论文，那可能会非常有帮助。我们有一些关于超参数优化的资源以及一些关于如何摆弄设置的想法，但是仅仅为了进行人工智能学习而设计的项目初始结构/启动代码有点棘手。我们确实有一个 MARL 工作的 Connect 4 训练示例，由 AgileRL 提供。但我们正在寻求将其从轮流输入提交调整为同时输入提交（这当然是可能的，MARL 用于 MOBA 等实时游戏中）。 ​ ​ p&gt; 您可以提供给我们的任何信息都是一种祝福并且很有帮助。非常感谢您抽出时间。    由   提交/u/stardoge42  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a19ck/frame_by_frame_continuous_learning_for_marl/</guid>
      <pubDate>Thu, 18 Jan 2024 21:22:30 GMT</pubDate>
    </item>
    <item>
      <title>TMRL 和 vgamepad 现在可以在 Windows 和 Linux 上运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199ujyt/tmrl_and_vgamepad_now_work_on_both_windows_and/</link>
      <description><![CDATA[亲爱的社区，您好， 你们中的一些人要求我使这些库与 Linux 兼容，并在我们的帮助下我们刚刚做了伟大的贡献者。 对于那些不熟悉的人，tmrl 是一个开源项目面向机器人专家的强化学习框架，因为它支持对数据管道的实时控制和细粒度控制，在自动驾驶社区中因其在 TrackMania2020 视频游戏中基于视觉的管道而闻名。另一方面，vgamepad 是支持此应用程序中游戏手柄模拟的开源库，它可以模拟 Xbox 360 和 PS4 Python 中的游戏手柄适合您的应用程序。 Linux 支持刚刚推出，我真的很想找到测试人员和新的贡献者来改进它，特别是对于“vgamepad”，它并不支持 Windows 版本的所有功能在 Linux 中还没有。如果您有兴趣贡献...请加入:)   由   提交 /u/yannbouteiller   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199ujyt/tmrl_and_vgamepad_now_work_on_both_windows_and/</guid>
      <pubDate>Thu, 18 Jan 2024 16:48:54 GMT</pubDate>
    </item>
    <item>
      <title>如果你是 DL 研究人员，自学深度 RL 会不会很困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199ik9w/is_it_hard_to_selfstudy_deep_rl_if_youre_a/</link>
      <description><![CDATA[作为一名法学硕士研究员，自学 Deep-RL 很难吗？我希望能够理解来自 DeepMind 的 RL 论文，但它似乎与常规 ML 非常不同，以至于我在自学这方面遇到困难。   由   提交/u/DoubleAd9650   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199ik9w/is_it_hard_to_selfstudy_deep_rl_if_youre_a/</guid>
      <pubDate>Thu, 18 Jan 2024 05:25:32 GMT</pubDate>
    </item>
    <item>
      <title>“通过离散扩散学习自动驾驶的无监督世界模型”，Zhang 等人 2023（MAE 规划）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</guid>
      <pubDate>Wed, 17 Jan 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>在决斗深度 Q 网络 (DQN) 训练中更改张量维度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</link>
      <description><![CDATA[我目前正在使用 PyTorch 实现决斗深度 Q 网络 (DQN)，以在 Gym 的 Ms. Pacman 环境中训练代理。训练似乎开始顺利，但经过几集（特别是大约 100 集之后），我意识到模型的输入张量的尺寸开始发生变化，导致训练不稳定。 I&#39; m 使用包含 ObservationBuffer、ExperienceBuffer、FrameSkippingAgent、DuelingDQN 和 Agent 等类的代码结构。该模型是使用 Double DQN 方法进行训练的，我很难在张量维度变化中识别此问题的根源。 一些重要的观察结果：我正在使用 GPU (CUDA) 来加速训练。 Dueling DQN 模型的输入观察维度一开始是正确的，但在超过 200 个回合后开始发生变化 问题：训练期间张量维度发生这些变化的原因是什么？ 预先感谢您提供任何可能有助于解决此问题的指导或建议。如果需要，我很乐意提供更多详细信息。 完整代码：https://stackoverflow.com/questions/77830358/changing-tensor-dimensions-in-dueling-deep-q-network-dqn-training 第 481 集，总奖励：300.0 第 482 集，总奖励：770.0 第 483 集，总奖励：210.0 第 484 集，总奖励：200.0 第 485 集，总奖励：280.0 运行时错误：给定组=1，大小权重 [ 64, 4, 8, 8]，预期输入[1, 84, 84, 1]有4个通道，但得到了84个通道   由   提交 /u/sigma_ks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</guid>
      <pubDate>Wed, 17 Jan 2024 21:03:19 GMT</pubDate>
    </item>
    <item>
      <title>关于动作分支论文的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</link>
      <description><![CDATA[大家好， 我正在尝试改编 这篇论文适合我的应用，但在实现过程中，论文的方法有些不清楚。 ​ 在第 4 页上，方程 5 和 6 讨论了如何将 d 目标值（每个分支 1 个目标值）减少到一个目标 y。  ​ 但是，方程 7 将损失描述为 Q 值与目标之间的平方差之和，分别针对所有分支！  ​ 那么，他们是否将目标聚合为一个目标值，从而形成 y - Q(s, a) 形式的损失方程，或者他们是否不聚合？我试图深入研究代码，但这并没有让我变得更明智。  ​ 如果您理解这一点，请告诉我，这将会有巨大的帮助！  &amp; #32；由   提交 /u/Abilitytofart   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</guid>
      <pubDate>Wed, 17 Jan 2024 17:16:34 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习中的softmax导数（问题）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198znmv/about_softmax_derivatives_in_reinforcement/</link>
      <description><![CDATA[选择“类别”时从雅可比矩阵中我选择哪一个，因为我不知道哪个是“正确的”？这通常用于强化学习。   由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198znmv/about_softmax_derivatives_in_reinforcement/</guid>
      <pubDate>Wed, 17 Jan 2024 15:48:58 GMT</pubDate>
    </item>
    <item>
      <title>分析强化学习泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /泛化强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</guid>
      <pubDate>Wed, 17 Jan 2024 09:54:16 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议以加快稳定基线下的 PPO 模型训练3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</link>
      <description><![CDATA[嘿各位 Reddit 用户！ 我目前正在使用 Stable Baselines3 训练金融日交易模型，并且我&#39;我面临着训练速度的挑战。每天（每集）涉及大约 250 万个数据点，当采取随机操作时，我的模拟器可以在大约 60-70 秒内迭代它们。 训练我的 PPO 模型时会出现问题，因为它需要每集长达 40-45 分钟。我只在剧集结束时执行一次更新，没有有限的水平线截断。当模拟器可以在一分钟左右完成训练时，为什么要花这么长时间来训练一集？有什么提示或技巧可以加速这个训练过程吗？接受建议和见解！   由   提交 /u/Bunny_lad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</guid>
      <pubDate>Tue, 16 Jan 2024 14:00:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO 特工与随机玩家进行神奇宝贝对决。你知道为什么平均奖励如此不稳定吗？ lr=1e-3，8 个并行环境在训练前进行 60 次移动，num_epochs=3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</link>
      <description><![CDATA[       由   提交 /u/moisturemeister   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</guid>
      <pubDate>Tue, 16 Jan 2024 09:57:47 GMT</pubDate>
    </item>
    <item>
      <title>具有狄利克雷作用分布的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</link>
      <description><![CDATA[嗨！我正在通过 PPO 培训政策。该模型输出的 logits 成为狄利克雷分布的参数。这些操作的总和应为 1，并且在 [0, 1]（单纯形）范围内。问题是，随着动作大小（维度）的增加，动作的对数概率也会增加。这反过来最终会放大 ppo 使用的替代损失中的 logp 比率。 我的单纯形操作空间是长度为 400 的一维向量。对数概率通常在 2200 - 3000 的范围内。 e^(logp_1 - logp_2) 的 logp 比率会有很大的变化，从而破坏 pytorch 的梯度计算。导致看起来有效但梯度包含 NaN 值的损失。 有人知道如何在保持理论基础健全的同时抵消这个问题吗？或者也许我在某个地方的推理中犯了错误？ 提前致谢！   由   提交 /u/JMvanWestendorp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</guid>
      <pubDate>Tue, 16 Jan 2024 09:16:42 GMT</pubDate>
    </item>
    </channel>
</rss>