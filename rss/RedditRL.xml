<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 13 Nov 2024 01:15:01 GMT</lastBuildDate>
    <item>
      <title>DPG 算法是基于策略的还是演员评论家的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gpje0r/is_dpg_algorithm_policybased_or_actorcritic/</link>
      <description><![CDATA[我有一个问题，即确定性策略梯度算法的基本形式是基于策略的还是演员评论家。我一直在寻找答案，在某些情况下，它说它是基于策略的，而在其他情况下，它并没有明确地说它是一个演员评论家，但它使用演员评论家框架来优化策略，因此我怀疑什么是策略改进方法。 我知道演员评论家方法本质上是基于策略的方法，并增加了评论家来提高学习效率和稳定性。    提交人    /u/Street-Vegetable-117   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gpje0r/is_dpg_algorithm_policybased_or_actorcritic/</guid>
      <pubDate>Tue, 12 Nov 2024 12:17:32 GMT</pubDate>
    </item>
    <item>
      <title>通过 RL 实现训练语言模型进行自我纠正——寻找测试人员和反馈！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gphaav/implementation_of_training_language_models_to/</link>
      <description><![CDATA[嗨， 我最近创建了论文通过强化学习训练语言模型进行自我纠正的最小 PyTorch 实现。 但是，我是将 RL 应用于语言模型的新手，不确定它是否正确实现。我希望社区能够帮助测试和改进它！  我需要帮助的内容：  测试：我的设置有限，因此如果有更多计算能力的人可以运行实验并提供反馈，我将不胜感激。 调试：这仍然是新鲜的，因此可能存在我尚未发现的错误。 优化速度：如果您有加快速度的想法，我很乐意听取！  如果能使此实现尽可能高效和有效，那就太好了，任何帮助都将不胜感激！ 查看：GitHub  提前致谢！    提交人    /u/sedidrl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gphaav/implementation_of_training_language_models_to/</guid>
      <pubDate>Tue, 12 Nov 2024 09:54:22 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个 RL 代理来在月球表面软着陆:)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gpd2uw/i_created_a_rl_agent_to_soft_land_in_lunar_surface/</link>
      <description><![CDATA[        由    /u/Few_Tooth_2474  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gpd2uw/i_created_a_rl_agent_to_soft_land_in_lunar_surface/</guid>
      <pubDate>Tue, 12 Nov 2024 04:55:24 GMT</pubDate>
    </item>
    <item>
      <title>帮助查找在 SARC 算法 (RL) 中生成特定输出文件的代码</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gp6l61/help_finding_code_that_generates_specific_output/</link>
      <description><![CDATA[大家好， 我正在使用 SARC（Soft Actor Retrospective Critic）算法，它是 Soft Actor-Critic（SAC）算法的扩展。在 SARC 实验的结果文件夹中（github：https://github.com/sukritiverma1996/SARC/tree/main/spinningup-data-deepmind-control-suite/data/sac\_dmCheetahRun\_256\_retroloss\_2/sac\_dmCheetahRun\_256\_retroloss\_2\_s123），我发现了三个似乎只有 SARC 才能生成的特定文件： - `q_losses_wrt_retroQvals_retroStates.pkl` - `q_losses_wrt_td3Qvals_td3States.pkl` - `demos.pkl` 我已经查看了主要训练脚本以及支持文件，如 `core.py`、`retro_loss.py` 和 `logx.py`，但我仍然没有找到创建这些文件的行。我想知道这里是否有人使用过 SARC 或类似算法，可以指出这些文件可能在代码库中的哪个位置生成，或者我下一步可能会将搜索重点放在哪里。 任何见解或指示都将不胜感激 - 提前致谢！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gp6l61/help_finding_code_that_generates_specific_output/</guid>
      <pubDate>Mon, 11 Nov 2024 23:28:40 GMT</pubDate>
    </item>
    <item>
      <title>离线学习的现状如何？您对离线学习有何看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gove1w/what_is_the_state_of_the_art_in_offline_learning/</link>
      <description><![CDATA[特斯拉等公司似乎成功地利用了从其汽车收集的数据进行离线学习。考虑到模拟和现实环境之间的众多差异，离线学习在未来会变得更加重要吗？    提交人    /u/Better_Working5900   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gove1w/what_is_the_state_of_the_art_in_offline_learning/</guid>
      <pubDate>Mon, 11 Nov 2024 15:53:15 GMT</pubDate>
    </item>
    <item>
      <title>轻松在 SMAC 和 MAMuJoCo 上记录离线数据，然后进行离线训练（离线 MARL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gotmkd/easily_record_offline_data_on_smac_and_mamujoco/</link>
      <description><![CDATA[大家好，我是南非的一名博士生，研究离线多智能体强化学习。我正在维护一个名为 Off-the-Grid MARL (og-marl) 的 GitHub 项目，该项目为离线 MARL 提供数据集和基线算法。我希望它可以帮助其他人开始该领域。我最近制作了一个快速的 Google Colab 笔记本来演示 og-marl 中的一些功能。我想这个社区中的一些人可能有兴趣查看它。在笔记本中，我演示了如何在 SMAC 或 MAMuJoCo 上在线训练 MARL 算法、记录数据、分析数据并在其上训练离线 MARL 算法。 https://colab.research.google.com/drive/1bfc7-tMLYmbKwh7HiqPzXU3f62tOuTY7?usp=sharing 如果您有兴趣进入离线 MARL，请随时通过 GitHub 与我们联系。我很乐意提供帮助。    提交人    /u/OfflineMARL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gotmkd/easily_record_offline_data_on_smac_and_mamujoco/</guid>
      <pubDate>Mon, 11 Nov 2024 14:37:18 GMT</pubDate>
    </item>
    <item>
      <title>协助我的 DQN 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gotiu2/assistance_with_my_dqn_project/</link>
      <description><![CDATA[嗨，Redditor， 我是强化学习的新手，目前正在从事深度 Q 网络 (DQN) 项目，该项目涉及一个机器人，该机器人试图在途中遇到障碍物的情况下到达目标。我遇到了一些挑战，确实需要那些在 RL 方面更有经验的人的指导。 我已在此处在 GitHub 上分享了我的代码：https://github.com/ROUMANI-Hassan/Reinforcement_Learning.git 如果有人在 DQN 或强化学习方面有专业知识，我将不胜感激任何建议。您的见解非常宝贵，因为我正在尝试独自学习 RL！ 非常感谢您提供的任何支持。    提交人    /u/Appropriate_Try8844   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gotiu2/assistance_with_my_dqn_project/</guid>
      <pubDate>Mon, 11 Nov 2024 14:32:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的标准库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1goqd15/standard_library_for_rl/</link>
      <description><![CDATA[目前，所有实现都与我们训练 DRL 模型的环境紧密耦合，我想知道我们是否可以有一个稳定的 API（例如 Pandas），将数据 / 属性序列化为步骤、观察、信息、奖励，而无需将其耦合到任何环境？    提交人    /u/iconic_sentine_001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1goqd15/standard_library_for_rl/</guid>
      <pubDate>Mon, 11 Nov 2024 11:46:59 GMT</pubDate>
    </item>
    <item>
      <title>领导者-追随者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gop0nq/leaderfollower/</link>
      <description><![CDATA[有人知道 MARL 中使用的标准领导者-追随者环境吗？我想使用 RL 创建一个领导者和追随者代理，并在标准环境中运行它们，最好是类似网格世界的环境。我找到了一些用于多智能体学习的标准环境(https://agents.inf.ed.ac.uk/blog/multiagent-learning-environments/)，其中有同时执行的操作，但找不到专门用于顺序(领导者-追随者)操作的环境。有人有什么相关建议吗？    提交人    /u/No_Addition5961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gop0nq/leaderfollower/</guid>
      <pubDate>Mon, 11 Nov 2024 10:14:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 玩 ARPG 的问题和想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</link>
      <description><![CDATA[我是一名计算机科学专业的最后一年学生，我想创建一个代理来玩我计划在 Unity 中制作的 ARPG。现在我想知道这是否可行，因为我需要概念验证直到 1 月。 我想指定代理执行的操作是导航关卡并在此过程中杀死一些敌人，关卡应该具有一定的复杂性，它不会是一条直线，但也不会是一个超级复杂的迷宫。最好它会有一些发散的路径。 现在，如果我在 Unity 中制作游戏，如果我当前的设置由 r5 5600 和 rtx 4070 12GB vram 组成，是否可以训练代理？它不会很快，但我可以让电脑全天候运行。 提前感谢您的回复。    提交人    /u/Marmotacuparlacur   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</guid>
      <pubDate>Mon, 11 Nov 2024 08:42:01 GMT</pubDate>
    </item>
    <item>
      <title>在 QMIX 中，每个代理在像 SMAC 这样的环境中是否被忽略？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1goal6i/in_qmix_is_peragent_done_ignored_in_an/</link>
      <description><![CDATA[大家好！我想理解一个相当简单的问题，我正在查看 JaxMARL QMIX 代码，我注意到即使我们使用每个代理的单独完成状态来重置隐藏状态，这些完成状态也不会在计算 q 函数目标时使用，而只是整体环境完成：https://github.com/FLAIROx/JaxMARL/blob/main/baselines/QLearning/qmix_rnn.py#L477 有人能解释一下这是为什么吗？是不是因为我们已经通过考虑可用操作与不可用操作隐式地屏蔽了 q 值，当代理在本地完成但环境本身尚未终止时，这些操作将发生变化？    提交人    /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1goal6i/in_qmix_is_peragent_done_ignored_in_an/</guid>
      <pubDate>Sun, 10 Nov 2024 20:52:07 GMT</pubDate>
    </item>
    <item>
      <title>Decisions & Dragons：一个回答常见 RL 问题的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</link>
      <description><![CDATA[多年来，我在各种社交媒体平台上回答了很多强化学习问题。我决定是时候在自己的网站上收集和扩展它们了，我把这个网站叫做 Decisions and Dragons。 虽然这个网站是面向初学者的，但我认为它对更高级的从业者有帮助，可以作为对核心概念的复习。它推出了 8 个深入的答案，我将在未来添加它。 我不确定它会有多受欢迎，但我希望它至少能帮助你们中的一些人！ https://www.decisionsanddragons.com/    提交人    /u/Born_Preparation_308   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</guid>
      <pubDate>Sun, 10 Nov 2024 16:37:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q-Learning 帮助无人机自主穿越未知环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</link>
      <description><![CDATA[我们的任务是使用无人机覆盖未知区域并在搜索过程中确定关键点。我们假设了一种场景，即必须覆盖受灾地区，并希望确定幸存者。目前，我们将问题抽象为使用 2D 网格表示搜索区域，然后可视化无人机在其中移动的情况。 我们是强化学习的新手，不清楚如何在这种情况下使用 q-learning。当您试图一次性覆盖一个区域并且您不知道环境是什么样子，只知道要搜索的区域的边界时，q-learning 是否有效？当幸存者很可能只是随机分布时，它甚至可以学习什么样的模式？任何见解/指导都将不胜感激。    提交人    /u/naepalm7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</guid>
      <pubDate>Sun, 10 Nov 2024 14:10:02 GMT</pubDate>
    </item>
    <item>
      <title>PPO 在 AirSim 中不起作用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1f9v/ppo_doesnt_work_in_airsim/</link>
      <description><![CDATA[大家好， 我正在研究我的论文项目，在 AirSim 的邻里环境中实施 PPO 以进行汽车驾驶。代理需要学会很好地驾驶，所以我正在处理这种情况：我必须在连续空间中估计 1 个动作（固定速度）（动作范围从 -1 到 1，我使用正态分布来抽样有效动作）。我已经测试了各种方法好几天了，但是代理仍然无法有效地学习。 对于 PPO 算法，我从几个 GitHub 代码中获得了灵感，对于网络，我使用了以下结构： self.conv1 = Conv2D(32, (8, 8), strides=4, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.conv2 = Conv2D(64, (4, 4), strides=2, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.conv3 = Conv2D(64, (3, 3), strides=1, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.flatten = Flatten()  self.ad1 = Dense(512,activation=&#39;relu&#39;)  self.ad2_mean = Dense(1,activation=&#39;tanh&#39;) self.val = Dense(1)  目前，我保持 std 固定，因为当我尝试估计它时，它会爆炸式增长到非常高的值。 我想问一下是否有人从事过类似的项目并能给我一些建议或指点！    提交人    /u/MonfoTibetano   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1f9v/ppo_doesnt_work_in_airsim/</guid>
      <pubDate>Sun, 10 Nov 2024 14:08:13 GMT</pubDate>
    </item>
    <item>
      <title>RMSprop 方法应用于 Q 学习，实现自适应动态学习率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go0h35/rmsprop_approach_applied_to_qlearning_for/</link>
      <description><![CDATA[我正在实施受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。 具体来说： 我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率（从 0.001 开始）并没有像预期的那样随着时间的推移而增加，而是低于预期甚至在下降。下面是我正在使用的 Q 学习更新函数：**python 复制代码**def update_q_table(self, state, action, reward, next_state): &quot;&quot;&quot;使用 Q 学习更新规则更新 Q 表。&quot;&quot;&quot; def update_q_table(self, state, action, reward, next_state): best_next_action = np.argmax(self.q_table[next_state, :]) td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action] td_error = td_target - self.q_table[state, action] # 更新 RMSprop 的平方梯度移动平均值 E[g^2] self.gradient_Q[state, action] = ( self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2) ) self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon) self.learning_rate_history.append(self.learning_rate) # 使用固定学习率和 TD 误差更新 Q 值 self.q_table[state, action] += self.learning_rate * td_error # 存储 E[g^2] 值用于跟踪 self.gradient_history.append(self.gradient_Q[state, action])  我正在实现受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是根据时间差异 (TD) 误差的大小随时间调整学习率。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减少。 具体来说： 我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但事实并非如此，它似乎在增长。因此，我的学习率（从 0.001 开始）并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。    提交人    /u/Busy-Acadia5601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go0h35/rmsprop_approach_applied_to_qlearning_for/</guid>
      <pubDate>Sun, 10 Nov 2024 13:18:31 GMT</pubDate>
    </item>
    </channel>
</rss>