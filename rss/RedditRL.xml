<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 11 Jul 2024 01:06:40 GMT</lastBuildDate>
    <item>
      <title>具有自定义环境的 A3C 俄罗斯方块中的次优解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzordj/suboptimal_solutions_in_a3c_tetris_with_custom/</link>
      <description><![CDATA[最近，我选择 A3C 作为我的最终项目的首选算法。我使用 https://github.com/dgriff777/rl_a3c_pytorch 作为参考，但没有使用 LSTM。我使用的环境是定制的：观察空间基本上是具有 3 个通道图像的两块俄罗斯方块（第一个通道显示只有堆叠的四格骨牌的棋盘图像，第二个通道显示当前下落的四格骨牌，第三个通道显示下一个四格骨牌）。在运行了数千步之后，我的代理似乎陷入了次优解决方案，它倾向于将块堆叠在棋盘的左侧。我该如何解决这个问题？ 注意：我正在使用 dellacherie 评估函数来奖励我的代理    提交人    /u/handshakers01   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzordj/suboptimal_solutions_in_a3c_tetris_with_custom/</guid>
      <pubDate>Wed, 10 Jul 2024 06:22:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO算法的训练速度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzoolu/the_training_speed_of_ppo_algorithm/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzoolu/the_training_speed_of_ppo_algorithm/</guid>
      <pubDate>Wed, 10 Jul 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 AlphaZero 学习 Tichu（纸牌游戏）——寻求有关状态 + 网络设计的建议/推测</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzo93h/learning_tichu_card_game_with_alphazero_seeking/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzo93h/learning_tichu_card_game_with_alphazero_seeking/</guid>
      <pubDate>Wed, 10 Jul 2024 05:50:06 GMT</pubDate>
    </item>
    <item>
      <title>如何建立强化学习的方程式？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dznob5/how_to_develop_the_equations_for_reinforcement/</link>
      <description><![CDATA[最近我开始学习强化学习，我看到的所有视频或博客都只是提供一个模拟环境并描述强化学习的过程，有没有你们知道的博客或教程，他们教如何开发状态模型，如何形成策略中使用的所有方程式。那将非常有帮助，任何帮助都将不胜感激     提交人    /u/Away_Elk_6826   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dznob5/how_to_develop_the_equations_for_reinforcement/</guid>
      <pubDate>Wed, 10 Jul 2024 05:13:34 GMT</pubDate>
    </item>
    <item>
      <title>Craftium：用于创建强化学习环境的可扩展框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz9bp5/craftium_an_extensible_framework_for_creating/</link>
      <description><![CDATA[嗨！我一直在开发 Craftium：一个易于使用的 RL 环境创建框架。Craftium 将开源体素游戏引擎 Minetest 包装在一个实现 Gymnasium API 的 Python 库中。虽然 Craftium 旨在用于环境创建，但我们也包含了一些现成的环境和任务（请在此处 查看）。目前，Craftium 处于早期开发阶段，但已准备好使用和测试。希望您觉得它很有趣！欢迎提供反馈！！    由    /u/mikelma 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz9bp5/craftium_an_extensible_framework_for_creating/</guid>
      <pubDate>Tue, 09 Jul 2024 18:12:24 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Stable_Baselines3 中重置后记录观察结果？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz885r/how_do_i_log_observations_after_reset_in_stable/</link>
      <description><![CDATA[我想在使用 SB3 时，记录训练期间`重置`后获得的每个`观察`。 根据 [this][1] 问题消息，我​​决定使用`Monitor`包装器而不是回调。 但是，监视器包装器给了我以下错误 -  这是我的代码 - ```py import gym from stable_baselines3 import PPO from stable_baselines3.common.callbacks import BaseCallback from stable_baselines3.common.monitor import Monitor class CustomMonitor(Monitor): def __init__(self, env, filename=None, allow_early_resets=True, reset_keywords=(), info_keywords=()): super(CustomMonitor, self).__init__(env) self.reset_observations = [] def reset(self, **kwargs): observer = super(CustomMonitor, self).reset(**kwargs) self.reset_observations.append(observation) return observer env = gym.make(&#39;LunarLander-v2&#39;) env = CustomMonitor(env) model = PPO(&#39;MlpPolicy&#39;, env, verbose=1) model.learn(total_timesteps=1000000) model.save(&quot;ppo_lunarlander_mutant&quot;)  ``` 但是，运行它之后，我收到以下错误 - ``` Traceback (most recent call last): File &quot;minimal_example.py&quot;, line 21, in &lt;module&gt;模型 = PPO（&#39;MlpPolicy&#39;，env，verbose = 1）文件“/home/thoma/anaconda3/envs/wp/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.py”，第 109 行，在 __init__ super（）。__init__（文件“/home/thoma/anaconda3/envs/wp/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py”，第 85 行，在 __init__ super（）。__init__（文件“/home/thoma/anaconda3/envs/wp/lib/python3.8/site-packages/stable_baselines3/common/base_class.py”，第 180 行，在 __init__ 断言 isinstance（self.action_space， supports_action_spaces), ( AssertionError: 该算法仅支持 (&lt;class &#39;gymnasium.spaces.box.Box&#39;&gt;, &lt;class &#39;gymnasium.spaces.discrete.Discrete&#39;&gt;, &lt;class &#39;gymnasium.spaces.multi_discrete.MultiDiscrete&#39;&gt;, &lt;class &#39;gymnasium.spaces.multi_binary.MultiBinary&#39;&gt;) 作为动作空间，但提供了 Discrete(4)  ``` [1]: https://github.com/DLR-RM/stable-baselines3/issues/137#issuecomment-669862467    由   提交  /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz885r/how_do_i_log_observations_after_reset_in_stable/</guid>
      <pubDate>Tue, 09 Jul 2024 17:27:41 GMT</pubDate>
    </item>
    <item>
      <title>“认知校准和寻找真相空间”，Linus Lee（偏好调整图像生成器模型中的模式崩溃 - DALL-E 3 与 2 的无聊之处）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz5sbs/epistemic_calibration_and_searching_the_space_of/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz5sbs/epistemic_calibration_and_searching_the_space_of/</guid>
      <pubDate>Tue, 09 Jul 2024 15:49:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 生成路径</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz4mcu/path_generation_using_rl/</link>
      <description><![CDATA[我一直在尝试训练一个 RL 模型，这样给定一个 3D 空间中的目标和一个初始起始位置，就会采取小步骤来达到目标​​。所以基本上就像从初始位置到最终位置的路径生成。我一直在使用稳定的基线 3 和健身房。动作空间是连续的，在 x、y 和 z 方向上从 -1 到 1。观察空间也是连续的，在 x、y 和 z 方向上从 -10 到 10。我给出的奖励要么是密集奖励，这取决于从当前位置到目标的距离，要么是稀疏奖励（如果到目标的距离大于某个阈值，则为 -1，否则奖励为 0）。 我尝试了各种情节长度和步长。我遗漏了什么或做错了什么吗？    提交人    /u/Necessary-Cabinet-43   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz4mcu/path_generation_using_rl/</guid>
      <pubDate>Tue, 09 Jul 2024 15:02:16 GMT</pubDate>
    </item>
    <item>
      <title>如何训练相同的 RL 代理（具有单独的策略）以在每个时间步骤中传递信息？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz33aa/how_to_train_identical_rl_agents_with_individual/</link>
      <description><![CDATA[你好 我正在 StabeBaseline3 中使用 SAC 进行强化学习，它对于单个代理来说运行良好。现在，我想训练多个相同的代理（使用它们自己的单独策略），以便每个代理的操作都用于为所有代理生成新的观察结果。 你能建议我一些可以帮助我完成此操作的库吗？一个小型的工作示例将是最好的选择。我看过 RLIB (ray)，但仍然不知道如何在其中使用 stablebaseline3。 谢谢    提交人    /u/Previous-Advance-921   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz33aa/how_to_train_identical_rl_agents_with_individual/</guid>
      <pubDate>Tue, 09 Jul 2024 13:58:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么不使用 Sigmoid？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz1xbn/why_isnt_sigmoid_used/</link>
      <description><![CDATA[大家好，我正在使用 Unity 在 C# 中从头开始制作一个简单的策略梯度学习算法，没有库，我想知道为什么没有人使用强化学习中的 sigmoid 函数作为输出 一切都可以在网上找到，每个人都使用 softmax 函数来输出代理可以采取的动作的概率分布，然后他们随机（偏向更高的动作）选择一个动作，但这种方法只允许代理在每个状态下执行一个动作，例如。它既可以向前移动，也可以射击，但我不能同时做这两件事，我知道有方法可以解决这个问题，即为代理可以采取的每组动作制作多个输出层，但我想知道你是否也可以有一个映射到动作的 S 形输出层  比如如果我让一个代理学习走路和射击敌人，使用软最大值，你会有一个用于走路的输出层和一个用于射击的输出层，但使用 S 形，你只需要一个输出层，其中 5 个神经元映射到 4 个方向的移动和射击，这取决于神经元输出的值是否大于 0.5 TLDR：而不是使用软最大值函数层或多层，你可以使用一个大的层，其中 S 形函数映射到基于值是否大于 0.5 的动作    提交人    /u/DaMrStick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz1xbn/why_isnt_sigmoid_used/</guid>
      <pubDate>Tue, 09 Jul 2024 13:05:37 GMT</pubDate>
    </item>
    <item>
      <title>来自深度视频馈送的端到端控制网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz1hs0/end_to_end_control_network_from_depth_video_feed/</link>
      <description><![CDATA[我正在尝试创建一个网络，该网络使用黑白图像（深度图）以及基本惯性测量（速度、姿态）来自主控制四轴飞行器并具有避障能力。拟议的网络将输出身体速率命令，这些命令将被馈送到低级 PID 控制器。 这有多可行？有没有这种端到端解决方案在现实世界中成功的例子？（不限于无人机） 通常，我们首先做一些像 SLAM 和 VIO 这样的操作来进行定位和状态估计，然后我们必须制定一个规划算法，然后我们才能有一个真正发出身体速率命令的神经网络控制器，这听起来像是一堆不必要的抽象 + 工作。 我非常好奇我是否发现了什么，我应该实现它😅   由    /u/FutureComedian7749  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz1hs0/end_to_end_control_network_from_depth_video_feed/</guid>
      <pubDate>Tue, 09 Jul 2024 12:45:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么与离策略算法相比，状态表示学习方法（通过辅助损失）较少应用于 PPO 等在线策略 RL 算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</link>
      <description><![CDATA[我已经看到了不同的状态表征学习方法（通过辅助损失，无论是自我预测还是基于结构化探索），它们已经与离线策略方法（如 DQN、Rainbow、SAC 等）一起应用。例如，SPR（自我预测表征）已与 Rainbow 一起使用，CURL（强化学习的对比无监督表征）已与 DQN、Rainbow 和 SAC 一起使用，RA-LapRep（通过图拉普拉斯进行表征学习）已与 DDPG 和 DQN 一起使用。我很好奇为什么这些方法没有像 PPO（近端策略优化）这样的在线策略算法得到广泛应用。将这些表示学习技术与在线策略算法学习相结合是否存在理论问题？    提交人    /u/C7501   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</guid>
      <pubDate>Tue, 09 Jul 2024 10:56:37 GMT</pubDate>
    </item>
    <item>
      <title>有没有任何具有随机性的离线 RL 基准？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</link>
      <description><![CDATA[我正在研究风险敏感的离线强化学习，发现许多众所周知的基准并不合适，因为它们缺乏随机性。Mujoco 环境几乎是确定性的；它们的内部代码不包含任何随机性，除了在“重置”期间阶段。 [参见：https://arxiv.org/abs/2205.15967\]  我发现 NeoRL 也是如此 [https://arxiv.org/abs/2102.00714.\] 您可以轻松验证运行 TD3PlusBC 时没有差异。 那么，有针对具有随机性的离线 RL 的基准吗？    提交人    /u/korsyoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</guid>
      <pubDate>Tue, 09 Jul 2024 10:09:56 GMT</pubDate>
    </item>
    <item>
      <title>如何处理3D体素观察？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</link>
      <description><![CDATA[我目前在使用 3D 体素状态训练 PPO 时遇到了困难。 3D 体素的形状为 [64, 128, 128]，区域信息很重要。 只使用 3D CNN 编码器可以吗？ 我是强化学习的新手，我还没有看到任何使用 3D 编码器的论文，而且大多数 RL 教程都使用 2d CNN 编码器或仅使用 MLP     提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</guid>
      <pubDate>Tue, 09 Jul 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>RLHub：强化学习环境的统一平台 - 寻求反馈！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</link>
      <description><![CDATA[嗨，我的 RL 伙伴们！ 我是加州大学伯克利分校的博士后，和几个朋友一起在开展一个名为 RLHub 的新项目，我很想听听你们的想法。我们的想法是创建一个标准化的强化学习环境平台，类似于 Hugging Face 为 NLP 模型所做的工作。 主要功能：1. 适用于各种 RL 环境（mujoco、unity、gym 等）的统一 API 2. 轻松上传和共享自定义环境 3. 自动依赖项管理 4. 本地和云执行选项 5. 标准化元数据和文档 可能的附加功能：- 标准化主要算法（PPO、DDPG、TD3……）UI，用于在云端训练代理 目标是简化查找、使用和共享 RL 环境的过程。研究人员可以轻松地在多种环境中尝试他们的算法，环境创建者可以接触到更广泛的受众。 我有一些问题：1. 这对您的工作有用吗？2. 您会优先考虑哪些功能？3. 对标准化有什么顾虑？4. 关于在 MVP 中包含云执行的想法？ 我特别想听听 RL 研究人员和从业人员的意见。您对当前 RL 环境管理有哪些痛点，可以解决这些痛点吗？ 提前感谢您的任何反馈！    提交人    /u/elonmusk-A12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</guid>
      <pubDate>Tue, 09 Jul 2024 03:03:51 GMT</pubDate>
    </item>
    </channel>
</rss>