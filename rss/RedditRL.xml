<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 31 Jan 2024 18:13:54 GMT</lastBuildDate>
    <item>
      <title>Flappy Bird 1.6 小时 2100 管，你觉得学习速度如何？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afmw1h/flappy_bird_2100_pipes_in_16_hours_how_do_you/</link>
      <description><![CDATA[https://reddit.com /link/1afmw1h/video/vsymq3l66tfc1/player ​ 我们在 ~1.6 中使用 Unity 中的 DQN 算法（这不是 mlAgents）训练了 FlappyBird小时。 由于一切都是从头开始编写的（以及神经网络），因此可以更改许多参数。划分环境也有助于加快这一过程。 100个特工同时训练，数量逐渐减少。 ​ 我想拍一个视频或者详细写一下，所以在我想知道你之前意见：与其他方法或现有插件相比，它是快还是慢，其他人会感兴趣吗？   由   提交/u/Fazoway  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afmw1h/flappy_bird_2100_pipes_in_16_hours_how_do_you/</guid>
      <pubDate>Wed, 31 Jan 2024 17:12:31 GMT</pubDate>
    </item>
    <item>
      <title>引擎上的强化学习：学习恒定轨迹而不是实际轨迹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afkx4i/rl_on_engine_learns_a_constant_trajectory_instead/</link>
      <description><![CDATA[      社区您好， 我对我的问题有一个概念性问题。我正在尝试使用 DDPG 代理学习引擎控制模型，而我的引擎有一个 LSTM 模型作为植物。我模拟给定随机轨迹的引擎，并使用引擎输出以及引擎状态（LSTM 状态）和负载轨迹作为我的代理的观察模型。 我正在尝试训练 DDPG 代理要求其遵循如下参考负载轨迹（左上图中的虚线）。我观察到，尽管尝试了各种网络架构/噪声选项&amp;学习率时，学习模型代理选择只提供 6 左右的恒定负载（左上图中的橙色线），而不是遵循给定的参考轨迹。输出似乎有合理的变化（此处为蓝色），但学习仍然不可接受。 我正在调整每一集的轨迹以帮助学习，因为这样它就可以看到各种负载配置文件。 &lt; p&gt;您能否建议一下这里可能发生的情况？ 其他信息：如果我要求控制器匹配恒定负载轨迹（每集 constnat，然后更改为另一个随机常量，则会发生相同的效果）下一集 ）。  提前致谢:) https://preview.redd.it/6qrpihpfrsfc1.png?width=2540&amp;format=png&amp;auto=webp&amp;s=f2f19cad1f71d411b6a6c2615274227d018e6 d57   由   提交/u/Doctor-Featherheart  /u/Doctor-Featherheart reddit.com/r/reinforcementlearning/comments/1afkx4i/rl_on_engine_learns_a_constant_trajectory_instead/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afkx4i/rl_on_engine_learns_a_constant_trajectory_instead/</guid>
      <pubDate>Wed, 31 Jan 2024 15:50:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能使用基于流程的并行性有效地并行化我的强化学习程序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afik9g/why_cant_i_effectively_parallelize_my/</link>
      <description><![CDATA[我的目标是使用 Stable_Baselines3 库同时运行多个强化学习程序。我注意到，随着程序数量的增加，程序的迭代速度逐渐降低，这是相当令人惊讶的，因为每个程序应该运行在不同的进程（核心）上。  ​ 这是我的程序： ​ ```py &lt; p&gt;from joblib import 并行，延迟 ​ 导入gym # from sbx import SAC 导入torch&lt; /p&gt; ​ 从 stable_baselines3 导入 SAC def train(): ​ ​ env =gym.make(“Humanoid-v4”) ​ model = SAC(“MlpPolicy”) ;, env, verbose=1) model.learn(total_timesteps=7e5,progress_bar=True) ​ def train_model():  ​ train() ​ ​  ​ if __name__ == &#39;__main__&#39;: num_of_programs = 1 并行(n_jobs=10)(延迟(train)() for i in range(num_of_programs)) ``` ​ `num_of_programs` 用于控制我尝试的程序数量并行运行。  这里有一些统计数据 -  ​ 程序数量迭代速度 1 1 ~102 it/s&lt; /p&gt; 2 3 ~60 it/s 3 10 ~ 20 it/s ​ 我确定要请求足够的资源，这样就不会出现资源限制。这就是我使用 slurm 请求资源的方式 - `srun --time=10:00:00 --nodes=1 --cpus-per-task=16 --mem=32G --partition=gpu --gres=gpu :a100-pcie:1 --pty /usr/bin/bash` ​ 因此我有 16 个 cpu、32G 内存和 40 GB GPU。 p&gt; ​ 当我从 `stable_baselines3` 迁移到 `sbx` 时，我注意到了同样的问题。 `stable_baselines3` 使用 `torch` 作为深度学习库，而后者使用 `JAX`。 ​ ​   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1afik9g/why_cant_i_efficiently_parallelize_my/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afik9g/why_cant_i_effectively_parallelize_my/</guid>
      <pubDate>Wed, 31 Jan 2024 14:03:44 GMT</pubDate>
    </item>
    <item>
      <title>需要 MountainCarContinously 帮助 - 用于连续操作的 REINFORCE 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1affkro/need_help_with_mountaincarcontinuous_reinforce/</link>
      <description><![CDATA[      大家好，最近我一直在研究 REINFORCE 算法持续采取行动，但成效有限。最初，我想从简单的事情开始，所以我尝试为标准健身房环境开发一种算法。我相信我涵盖了所有必要的点，但正如你所看到的，我的代理正在上山，但它应该向前和向后移动，这很奇怪。有什么想法吗？  这是我的 colab 的链接。如果有人能抽出时间来帮助我，那就太好了。 https://colab.research.google.com/drive/1MrqEhww3rqZoZkKY1Jnwd4oPQHAN4xWH?hl=pl# scrollTo=sydH0wO1OFpJ https://reddit.com/link/1affkro/ video/1d1uomiserfc1/player   由   提交/u/Sharp-Record1600  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1affkro/need_help_with_mountaincarcontinuous_reinforce/</guid>
      <pubDate>Wed, 31 Jan 2024 11:16:56 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习和基于模型的强化学习在学习模型和控制方面有何区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afebbj/difference_between_offline_and_modelbased_rl_in/</link>
      <description><![CDATA[我看到通常诸如“如何在 rl 中使用预先收集的数据集”之类的问题的答案，答案与离线 RL，建议首先通过监督学习来学习模型。但是基于模型的学习还假设模型是根据经验数据学习的。正在从批量数据中学习基于模型的模型 + 使用典型的 MBRL 方法比如计划/想象不正确？我必须在与真实环境交互的同时学习模型？   由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afebbj/difference_between_offline_and_modelbased_rl_in/</guid>
      <pubDate>Wed, 31 Jan 2024 09:49:48 GMT</pubDate>
    </item>
    <item>
      <title>根据部分状态计算多个动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1af009r/computing_multiple_actions_based_on_part_of_the/</link>
      <description><![CDATA[假设我有一个状态，它由三个长度为 n 的向量组成，这些向量连接在长度为 3n S = [a, b, c] 的数组中。现在我可以使用它并在采取步骤时计算 n 维动作数组。  然而，我真正想要的是代理基于三维数组计算n个动作：动作1是使用状态S1 = [a1, b1, c1]计算的，动作2是使用状态计算的S2 = [a2, b2, c2]...所以在计算完这n个动作之后，我想采取一步，然后计算奖励。  可以通过哪些方式实现这一目标？我知道存在诸如多代理环境之类的东西，但是我正在使用稳定基线 3，目前不支持此功能。还有其他可能性吗？ 编辑：我不能使用串联状态的原因是我想要解决的不同问题的维度可能会有所不同。因此，每个问题的参数 n 都会不同，这给出了不同长度的观察空间。  ​   由   提交 /u/Lennitar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1af009r/computing_multiple_actions_based_on_part_of_the/</guid>
      <pubDate>Tue, 30 Jan 2024 21:32:27 GMT</pubDate>
    </item>
    <item>
      <title>RL 介绍途径</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aepy9e/rl_intro_pathway/</link>
      <description><![CDATA[所以我今天在这里发帖，支持和答案都很好，但我确实感到有点不知所措，有没有任何指南/路径/我可以遵循决策树来对此有一个小小的了解？例如：你想要 rl 或 dnn + rl，然后选择这个，然后你想要多代理或单代理，然后这样做，然后你想要模型或没有模型等 我想我正在问一组一般性的问题，这些问题将帮助我选择什么是“最好的”。对于我的项目/我应该研究什么   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aepy9e/rl_intro_pathway/</guid>
      <pubDate>Tue, 30 Jan 2024 14:41:49 GMT</pubDate>
    </item>
    <item>
      <title>代理不在 MARL 中学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aepm2o/agents_dont_learn_in_marl/</link>
      <description><![CDATA[大家好！背景 - 我正在帮助使用 MARL 框架和 NVIDIA Isaac Sim 的项目。基本上我们在一个 ENV 中有一个目标和 2 个代理。运行训练后，我进行推理，观察到一名智能体达到了目标，而第二名智能体只是在徘徊。我想也许第二个没有足够的时间来探索，所以我从 is_done 方法中删除了达到目标时的情节终止。这让两人都好奇了起来。谁能推荐我一种方法来获得所需的 AMR 行为，同时两者都达到相同的目标。谢谢！   由   提交 /u/No_Artichoke3603   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aepm2o/agents_dont_learn_in_marl/</guid>
      <pubDate>Tue, 30 Jan 2024 14:25:44 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试让我的 ppo 模型与自定义环境一起使用，以预测哪些通知最适合哪个用户，但到目前为止还没有得到令人信服的结果。我是否应该将它用于我的用例？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aema74/im_trying_to_get_my_ppo_model_to_work_with_a/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aema74/im_trying_to_get_my_ppo_model_to_work_with_a/</guid>
      <pubDate>Tue, 30 Jan 2024 11:29:33 GMT</pubDate>
    </item>
    <item>
      <title>python 中是否有有效的 WQMIX 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aem1lt/is_there_a_working_wqmix_implementation_in_python/</link>
      <description><![CDATA[我正在寻找 WQMIX 的实现，希望在 python 中，官方版本给我带来了真正的麻烦，有人可以帮忙吗？ &lt; /div&gt;  由   提交/u/InvestigatorLiving93  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aem1lt/is_there_a_working_wqmix_implementation_in_python/</guid>
      <pubDate>Tue, 30 Jan 2024 11:14:09 GMT</pubDate>
    </item>
    <item>
      <title>将重放缓冲区中的真实数据与 stablebaseline3 混合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aelyib/mixing_realworld_data_in_replay_buffer_with/</link>
      <description><![CDATA[我目前正在使用 stablebaseline3 在模拟器中训练虚拟机器人的策略。我还可以接触到真正的机器人。对于off-policy RL，我是否可以将一些现实世界的数据（state、action、state_next、reward...）放入缓冲区中以改进训练？   由   提交/u/Proof_Structure7071   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aelyib/mixing_realworld_data_in_replay_buffer_with/</guid>
      <pubDate>Tue, 30 Jan 2024 11:08:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习推荐的游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aek7uj/recommended_games_for_reinforcement_learning/</link>
      <description><![CDATA[我在大学有一门课程，叫强化学习，我对它很感兴趣，整个年级都是一个项目，我是考虑制作一个能够解决某些游戏的人工智能，正如我所看到的，它在强化学习中非常流行。现在的问题是，有哪些游戏既可以合理实现，又可以解决，并且可以提供不错的/有趣的/富有洞察力的结果。所以我会远离蛇，只是因为我经常看到它这样做，并且正在考虑瘟疫公司，但它似乎很难交互。   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aek7uj/recommended_games_for_reinforcement_learning/</guid>
      <pubDate>Tue, 30 Jan 2024 09:07:01 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的遗憾界限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aeiexo/regret_bounds_in_reinforcement_learning/</link>
      <description><![CDATA[我已经有几年没有阅读强化学习理论论文了，并且很好奇此后该领域的进展如何。上次我查的时候，有一篇论文声称他们封闭了 MDP 中后悔的上限和下限……在证明中发现了一个错误。从那时起发生了什么？ 编辑：我认为是这个（https ://proceedings.neurips.cc/paper/2017/hash/3621f1454cacf995530ea53652ddf8fb-Abstract.html）如果有人可以指出后续论文，我将非常感激！    由   提交 /u/HideFalls   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aeiexo/regret_bounds_in_reinforcement_learning/</guid>
      <pubDate>Tue, 30 Jan 2024 07:01:25 GMT</pubDate>
    </item>
    <item>
      <title>训练强化学习代理时奖励如何发挥作用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ae9t90/how_does_reward_work_while_training_a/</link>
      <description><![CDATA[我们是否应该在步骤函数开始时将奖励重置为其初始值？前任。奖励= 0。 另外，如果我们在step()开始时不将其设置为0，那么这是计算奖励的正确方法吗？奖励=奖励+计算 它是如何工作的？像 PPO 这样的算法如何使用每一步返回的奖励（）？  ​   由   提交/u/Fr4gg3r_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ae9t90/how_does_reward_work_while_training_a/</guid>
      <pubDate>Mon, 29 Jan 2024 23:37:19 GMT</pubDate>
    </item>
    <item>
      <title>将知识从评论家网络转移/共享到演员网络背后的直觉是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ae0iox/what_is_the_intuition_behind_transferringsharing/</link>
      <description><![CDATA[标准 PPO 算法对于参与者和批评者都有一个单一的网络，分别有两个输出头用于策略和值。  在Cobbe 等人中。 (2021) [阶段性政策梯度] 和 Aitchison, Sweetser (2022) [PPO-DNA] 作者认为拥有联合网络不利于具有基线的策略梯度算法的性能。相反，他们建议必须将经过不同时期数（以及偏差/方差程度）独立训练的网络分开。然而，他们的行动者网络仍然有一个额外的价值头，分别在辅助或蒸馏阶段进行优化（在约束下）。 他们指出，有知识可以从价值函数转移到价值函数政策（他们表明这实际上提高了算法的性能）。  我想知道该声明背后的直觉。一个对特定状态下的预期（折扣）回报进行估计的函数如何为该状态下要采取的最佳行动提供信息？ 为了更容易理解，让我们想象一个小例子：我的经纪人沿着赛道驾驶一辆汽车。她的评论家网络提供了有关该赛道中的位置相对于她未来预期回报的估计优度的信息。参与者网络规定了方向盘的角度和加速度。在辅助/蒸馏阶段，有利于改善政策的头寸的预期回报信息如何？机制或思路是什么？   由   提交 /u/Tortoise_vs_Hare   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ae0iox/what_is_the_intuition_behind_transferringsharing/</guid>
      <pubDate>Mon, 29 Jan 2024 17:15:38 GMT</pubDate>
    </item>
    </channel>
</rss>