<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 03 Nov 2024 06:24:07 GMT</lastBuildDate>
    <item>
      <title>需要帮助为策略游戏 Polytopia 设计 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gicg22/need_help_engineering_rl_algorithm_for_strategy/</link>
      <description><![CDATA[      大家好，我是 RL 新手，需要一些帮助来为策略游戏 Polytopia 设计算法。 我正在尝试为基于图块的策略游戏 Polytopia 制作 RL 代理。使用 OpenAI Gym，我制作了该游戏的原始版本。观察空间由 121 个图块组成，每个图块都有数据：（地形、资源、改进、气候、边界、改进所有者、单位所有者、单位类型、单位健康、改进进度、已攻击、已移动）以及玩家的星数。下面是游戏的示例（这是全局视图，所以没有雾，但各个代理看不到雾外面） https://preview.redd.it/aqpsk4c0elyd1.png?width=706&amp;format=png&amp;auto=webp&amp;s=9a2973083cc8512467aa2c6dcfbda9181946cb97 目前，我已将行动过程分为三个步骤。首先，代理从 1 到 121 中挑选一个方块（121 个动作）。其次，代理挑选要在该方块上执行的动作类型（8 种动作），例如：移动/攻击、收获资源、训练单位等。第三步仅当动作涉及目标方块时才会发生，例如：移动/攻击，代理从 1 到 121 中挑选一个代表目标方块的方块。示例动作序列为：59、1、49；这将选择方块 59，选择移动/攻击单位动作类型，并选择目标方块 49，这将导致骑手攻击战士。这是我的图表的链接：https://docs.google.com/presentation/d/1DPhYymGDfQIfVKAYlzK8lBkkiPoGlqbxRJ5JycDQI_U/edit?usp=sharing 我应该使用什么算法？处理这种多阶段操作的最佳方法是什么？我应该输入哪些参数？我的神经网络应该是模块化的还是分层的？PyTorch 是这类事情的好选择吗？任何关于如何开始学习过程的建议或链接都​​将不胜感激！    提交人    /u/Kingofath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gicg22/need_help_engineering_rl_algorithm_for_strategy/</guid>
      <pubDate>Sun, 03 Nov 2024 01:59:38 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助模拟人体运动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gi7t14/need_help_in_simulation_of_human_motion/</link>
      <description><![CDATA[基本上，我已经使用 HumanML3D 数据集生成了人体运动，现在想要使用 IssacGym/Mujoco 使用 RL（PPO）使其具有物理感知能力，有人可以提供一些资源来帮助我吗？非常感谢所有帮助。    提交人    /u/Character-Aioli-4356   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gi7t14/need_help_in_simulation_of_human_motion/</guid>
      <pubDate>Sat, 02 Nov 2024 22:09:50 GMT</pubDate>
    </item>
    <item>
      <title>对强化学习感兴趣可以申请哪些行业、什么职位？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gi12cc/which_industries_and_what_positions_can_we_apply/</link>
      <description><![CDATA[您好，我是密歇根大学马里兰分校的研究生，刚刚进入强化学习领域，到目前为止，我很喜欢它，这让我很好奇哪些行业可以申请实习，以及如何在此领域发展我的职业生涯。但就目前而言，我对强化学习的实际应用比研究更感兴趣。 我希望在这方面得到一些指导。 谢谢    提交人    /u/Odd-Pangolin4370   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gi12cc/which_industries_and_what_positions_can_we_apply/</guid>
      <pubDate>Sat, 02 Nov 2024 17:03:00 GMT</pubDate>
    </item>
    <item>
      <title>呼叫所有 ML 开发人员！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ghh9yz/calling_all_ml_developers/</link>
      <description><![CDATA[我正在开展一个研究项目，这将有助于我的博士论文。 这是一项用户研究，其中 ML 开发人员回答调查以了解 ML 开发人员构建隐私保护模型的问题、挑战和需求。  如果您从事 ML 产品或服务工作，或者您是从事 ML 工作的团队的一员，请帮助我回答以下问卷：https://pitt.co1.qualtrics.com/jfe/form/SV_6myrE7Xf8W35Dv0。 用于分享研究： LinkedIn：https://www.linkedin.com/feed/update/urn:li:activity:7245786458442133505?utm_source=share&amp;utm_medium=member_desktop 请随时与其他开发人员分享此调查。 感谢您的时间和支持！    提交人    /u/MaryAD_24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ghh9yz/calling_all_ml_developers/</guid>
      <pubDate>Fri, 01 Nov 2024 22:07:22 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习尚未奏效。发布于 2018 年。六年后，您认为情况发生了多大变化，哪些保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ghf83z/deep_reinforcement_learning_doesnt_work_yet/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ghf83z/deep_reinforcement_learning_doesnt_work_yet/</guid>
      <pubDate>Fri, 01 Nov 2024 20:35:14 GMT</pubDate>
    </item>
    <item>
      <title>变压器ppo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gh4qcv/transformer_ppo/</link>
      <description><![CDATA[我知道 cleanrl 已经发布了精益版本。有没有人有经验，可以告诉 transformer ppo 是否能取得更好的结果？更强大？比 gru 好吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gh4qcv/transformer_ppo/</guid>
      <pubDate>Fri, 01 Nov 2024 12:57:32 GMT</pubDate>
    </item>
    <item>
      <title>“π~0~：用于通用机器人控制的视觉-语言-动作流模型”，Black 等人 2024 年{物理智能}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggv2o3/π0_a_visionlanguageaction_flow_model_for_general/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggv2o3/π0_a_visionlanguageaction_flow_model_for_general/</guid>
      <pubDate>Fri, 01 Nov 2024 02:03:51 GMT</pubDate>
    </item>
    <item>
      <title>自然语言强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggpkub/rl_with_natural_language/</link>
      <description><![CDATA[我一直在研究一些将语言纳入强化学习框架的新论文，比如微软雷德蒙德的这篇论文(https://arxiv.org/pdf/1511.04636)、Reader(https://aclanthology.org/2023.emnlp-main.1032/)、Ready to Fight Monsters，以及最近的Learning to Model the World with Language(https://arxiv.org/abs/2308.01399)，我想知道这里是否有人可以指点一下强化学习领域其他有趣的作品。此字段。    提交人    /u/potatodafish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggpkub/rl_with_natural_language/</guid>
      <pubDate>Thu, 31 Oct 2024 21:28:34 GMT</pubDate>
    </item>
    <item>
      <title>[项目] PyMAB：一个用于多臂老虎机的探索性 Python 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gglajl/project_pymab_an_exploratory_python_library_for/</link>
      <description><![CDATA[大家好！我很高兴与大家分享 PyMAB，这是我为多臂老虎机 (MAB) 算法开发的 Python 库。它被设计为研究人员和强化学习爱好者尝试比较多种 MAB 算法和配置的实验工具。 📦 安装 pip install pymab 或者访问我们的 github 页面：https://github.com/danielaLopes/pymab 🎯 主要特点 多种 MAB 算法：  贪婪和 ε-greedy 汤普森采样（高斯和伯努利） 上限置信区间 (UCB) 贝叶斯 UCB 上下文 Bandits  多种环境：  平稳 非平稳  渐进式 突变 随机交换手臂   内置可视化：  奖励曲线 遗憾分析 动作分布 策略比较  📊 快速示例 以下是如何使用 PyMAB 的简单示例： from pymab.policies import ThompsonSamplingPolicy from pymab.game import Game # 初始化 Thompson Sampling policy = ThompsonSamplingPolicy(n_bandits=5) # 创建并运行模拟游戏 = Game(n_episodes=1000, n_steps=1000, strategies=[policy], n_bandits=5) game.game_loop() # 可视化结果 game.plot_average_reward_by_step() 该 repo 包含多个 jupyter-notebooks 示例。  如果您有任何问题或建议，请告诉我！我正在积极监控这个帖子，并很高兴收到您的反馈。 这是一个正在进行的项目，我们一直在寻找建议和贡献。如果您有任何想法或想提供帮助，请联系我们！ 标签：#MultiArmedBandits #ReinforcementLearning    提交人    /u/danielalopes97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gglajl/project_pymab_an_exploratory_python_library_for/</guid>
      <pubDate>Thu, 31 Oct 2024 18:21:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我们针对 AI 评估器尝试了不同的训练目标，结果如下：</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggjisd/r_our_results_experimenting_with_different/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggjisd/r_our_results_experimenting_with_different/</guid>
      <pubDate>Thu, 31 Oct 2024 17:05:51 GMT</pubDate>
    </item>
    <item>
      <title>MBRL 文本建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gghi1c/mbrl_text_recommendations/</link>
      <description><![CDATA[大家好！ 我有兴趣进一步了解基于连续动作模型的强化学习算法（来自动态和控制背景）。我读过 Farsi 和 Liu 的书，但如果有人有好的推荐，我正在寻找更多好书/论文！    提交人    /u/Voltimeters   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gghi1c/mbrl_text_recommendations/</guid>
      <pubDate>Thu, 31 Oct 2024 15:39:05 GMT</pubDate>
    </item>
    <item>
      <title>现已在 YouTube 上提供 - 观看 Emma Brunskill 主持的斯坦福 CS234 强化学习的所有讲座</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gghbth/now_available_on_youtube_stream_all_the_lectures/</link>
      <description><![CDATA[  由    /u/Stanford_Online  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gghbth/now_available_on_youtube_stream_all_the_lectures/</guid>
      <pubDate>Thu, 31 Oct 2024 15:31:42 GMT</pubDate>
    </item>
    <item>
      <title>关于DQN训练的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggejj6/question_about_dqn_training/</link>
      <description><![CDATA[在每一集之后进行训练而不是逐步进行训练可以吗？任何答案都会有所帮助。谢谢    提交人    /u/Sea-Collection-8844   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggejj6/question_about_dqn_training/</guid>
      <pubDate>Thu, 31 Oct 2024 13:29:17 GMT</pubDate>
    </item>
    <item>
      <title>用于知识蒸馏的决策转换器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggbau7/decision_transformer_for_knowledge_distillation/</link>
      <description><![CDATA[我正在研究一个模仿学习问题，我希望产生一个动作，让代理根据当前状态观察和先前的动作重现参考状态。我目前的想法是开发一个 MoE 或 MCP 策略，该策略可以查询一组预训练的 MLP，以查找代理可能遇到的不同“问题”。然后，我想将其提炼为一个可以独立运行的单一策略。 我正在研究各种选项，对于这个应用程序来说，使用转换器似乎很合理，因为从我的理解来看，我的问题的时间顺序特征可以从转换器中受益，我希望它可以提高模仿看不见的参考状态的策略的通用性。 但是，我对几件事不确定。理想情况下，可以使用 PPO 在线提炼/训练，但在线决策变压器似乎在更广泛的文献中未经测试（除非我不擅长找到它），并且奖励的适应性对我来说不是很清楚。 我也见过有人在决策变压器中放弃奖励，但仍然选择离线训练和在线调整。 或者，我可以使用另一个网络（例如 VAE）来提炼信息并进行完全在线训练，但我目前有兴趣探索除此之外的东西，除非它真的是最好的选择。  我很感激对此的一些意见，因为我是这些更先进/新颖的 RL 技术的新手，并且确切地知道应该何时应用它们。     提交人    /u/nalliable   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggbau7/decision_transformer_for_knowledge_distillation/</guid>
      <pubDate>Thu, 31 Oct 2024 10:30:59 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggawov/deep_reinforcement_learning_survey/</link>
      <description><![CDATA[深度强化学习中泛化的分析调查 链接：https://arxiv.org/pdf/2401.02349v2    提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggawov/deep_reinforcement_learning_survey/</guid>
      <pubDate>Thu, 31 Oct 2024 10:03:20 GMT</pubDate>
    </item>
    </channel>
</rss>