<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 01 Aug 2024 06:22:00 GMT</lastBuildDate>
    <item>
      <title>真人秀 PS1 游戏 - 铁拳 3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</link>
      <description><![CDATA[我是 RL 领域的新手。我尝试过使用 atari 游戏的复古模拟作为 ENV 的 openai gym，但我想下一步。 我想在 PS1 上为格斗游戏 Tekken 3 实现 RL 模型。 据我所知，我必须使用 PS1 模拟器实现自己的交互层才能正确获取状态，例如健康、位置等。 我该如何正确处理？  尝试对 PS1 游戏进行逆向工程，并找出具有必要值的内存地址（通过 python 获取）。 使用 OpenCV 进行相同的检测，但从帧中  当涉及到创建自定义环境时，您将如何处理？    提交人    /u/Comprehensive_Cod331   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</guid>
      <pubDate>Thu, 01 Aug 2024 04:58:31 GMT</pubDate>
    </item>
    <item>
      <title>既然离线 RL 与环境无关，为什么很多论文实现仍然基于 gym？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</link>
      <description><![CDATA[谢谢。    由   提交  /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:33 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为不同的游戏选择不同的 RL 算法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</link>
      <description><![CDATA[我是荷兰 6 VWO 的一名 16 岁学生，目前正在参与一个关于强化学习 (RL) 在各种电脑游戏中的应用的学校研究项目。我的主要研究问题是：电脑游戏的具体特征如何影响不同 RL 算法的有效性？ 子问题：  不同类型的电脑游戏有哪些具体特征？ 有哪些 RL 算法可用，它们的特点是什么？ 游戏特征如何影响 RL 算法的性能？  实践部分：我计划将不同的 RL 算法应用于各种游戏。我正在考虑的游戏是：  超级马里奥兄弟 贪吃蛇 国际象棋 赛车  算法标准：  具有显著差异的算法。 最好是新算法。  反馈问题：  考虑到这些游戏的独特特点，您能否推荐适合这些游戏的特定 RL 算法？ 您认为我选择的游戏适合研究不同 RL 算法的有效性吗？如果不适合，您会建议什么游戏？     提交人    /u/matmoet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</guid>
      <pubDate>Wed, 31 Jul 2024 23:23:28 GMT</pubDate>
    </item>
    <item>
      <title>“彩虹团队：开放式生成多样化对抗提示”，Samvelyan 等人 2024 {FB}（用于质量多样性搜索的 MAP-Elites）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</guid>
      <pubDate>Wed, 31 Jul 2024 01:48:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 epsilon-greedy 算法的概率部分中包含贪婪动作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</link>
      <description><![CDATA[      新手在这里...我正在阅读 Sutton 和 Barto 关于强化学习的常年书籍。在他们对 epsilon-greedy 策略的描述中，他们认为在给定状态下选择一个动作应该以较小的 epsilon 概率发生；其余时间则选择贪婪动作。我理解这一点，因为你想鼓励探索，这反过来又允许人们满足先决条件，即在无限的时间轴下，最终将为给定状态选择所有动作，以收敛到接近最优的策略。 但是，我不明白即使在随机操作时也允许代理选择贪婪动作的原因。书中给出的这个公式就是一个例子，其中选择贪婪的概率是 1 减去选择任何动作（甚至是贪婪的动作）的概率 epsilon 加上随机选择贪婪动作的概率： https://preview.redd.it/vz26lao3vqfd1.png?width=142&amp;format=png&amp;auto=webp&amp;s=e3c7c0f6d48f1f349f9260f4a4e8897f438e2b42 当然，代理选择贪婪动作的次数会比选择其他非贪婪动作的次数多得多。因此，当整个重点是防止代理以小概率 epsilon 利用此操作时，为什么代理会在随机阶段选择最佳操作？ 谢谢， 一位充满激情的 RL 学习者。    提交人    /u/Soft-Establishment96   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</guid>
      <pubDate>Wed, 31 Jul 2024 00:02:51 GMT</pubDate>
    </item>
    <item>
      <title>“Auto Evol-Instruct：大型语言模型的自动指令进化”，Zeng 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efy7aj/auto_evolinstruct_automatic_instruction_evolving/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efy7aj/auto_evolinstruct_automatic_instruction_evolving/</guid>
      <pubDate>Tue, 30 Jul 2024 16:45:14 GMT</pubDate>
    </item>
    <item>
      <title>你在PPO算法中遇到过这个问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</link>
      <description><![CDATA[我正在创建一个用于任务路由的深度强化学习环境，使用基于策略的 DRL 方法 PPO。在我的环境中，想法是每次任务到达一个节点时，都会生成一个概率，最终找到目的地并结束游戏。因此，每次任务到达一个节点时，都会生成相邻节点的概率，形成到达终点的连续路径。但是，存在一个问题：为每个状态生成的概率正在收敛。理论上，它们应该不同。例如，在选择一个节点后，应该生成像 s1[0.2,0.5,0.3] 和 s2[0.4,0.1,0.5] 这样的概率，但目前，每个状态都有相同的概率，例如 s1[0.2,0.5,0.3] 和 s2[0.2,0.5,0.3]。 我怀疑我的奖励设置可能有问题。我给每一步都设置了与时间参数相关的负奖励，完成的奖励为10，遇到路由循环就得到-10的负奖励，难道我的设计有问题？这让我很纳闷。    submitted by    /u/VermicelliBrave1931   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</guid>
      <pubDate>Tue, 30 Jul 2024 07:11:58 GMT</pubDate>
    </item>
    <item>
      <title>“对受试者进行反馈的序贯实验分析”，Diaconis & Graham 1981</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</guid>
      <pubDate>Mon, 29 Jul 2024 20:30:26 GMT</pubDate>
    </item>
    <item>
      <title>平均情节奖励差异，为什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</link>
      <description><![CDATA[      嗨， 我有一个简单的环境，我使用 SB3 中的各种算法进行训练作为练习。这是 DDPG 和 SAC 的 tensorboard 情节平均奖励（针对完全相同的环境）。在学习完成并保存模型时，报告的奖励约为 6。但是，当我在保存的模型上使用 SB3 assesse_policy (with n_eval_episodes=10) 时，我看到以下内容： 对于 SAC：平均奖励：1.5123082560196053 +/- 0.0008870645563937467 对于 DDPG：平均奖励：0.6197831666923037 +/- 0.000696591922367452 我预计平均奖励约为 6。这种预期是错误的吗？ https://preview.redd.it/xxag55fswgfd1.png?width=765&amp;format=png&amp;auto=webp&amp;s=e6903abf29863ad230d84451e83b162e265eb101    submitted by    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</guid>
      <pubDate>Mon, 29 Jul 2024 14:32:31 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线-3 工人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</link>
      <description><![CDATA[我正在研究 stable-baselines3 库 的一些算法的实现。 具体来说，我看到一些算法（A2C、PPO）的文档提到该实现使用了多个工作器。  对于其他算法，它没有提及任何内容，所以我的问题是：  我可以假设其他算法只使用一个学习者吗？ 为了进一步解答我的疑问，在所有算法的摘要表中显示，每个算法都支持“多处理”。但是，如果我理解正确的话，那仅指使用矢量化环境，尽管我不会打赌它。 最后说明：我尝试通过查看A2C（其中提到使用工作者）和SAC（其中没有提到工作者）的代码来回答我的问题，但这没有帮助，所以我在这里   由    /u/Frank-the-hank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</guid>
      <pubDate>Mon, 29 Jul 2024 07:27:37 GMT</pubDate>
    </item>
    <item>
      <title>创建博弈论论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</link>
      <description><![CDATA[在我国，对于原创博弈，制定博弈论与策略优化实施大纲的传统方式是什么？    提交人    /u/Former_Ad_4221   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</guid>
      <pubDate>Mon, 29 Jul 2024 06:24:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PPO 算法没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</guid>
      <pubDate>Mon, 29 Jul 2024 05:00:03 GMT</pubDate>
    </item>
    <item>
      <title>用于构建 RL 项目的简单可视化工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</link>
      <description><![CDATA[      我计划制作这个用于 RL 开发的简单工具。这个想法是快速构建和训练 RL 代理，无需代码。这对于快速开始新项目或轻松进行实验以调试 RL 代理非常有用。 目前设计中有 3 个选项卡：环境、网络和代理。我计划添加第四个选项卡，称为“实验”，用户可以在其中定义超参数实验并直观地查看每个实验的结果，以便调整代理。这个设计是一个非常早期的原型，可能会随着时间的推移而改变。 你们觉得怎么样？ https://preview.redd.it/sb5awqjys8fd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d1046c3b7e195dba0b7779ee55f11c9330ec3d12    提交人    /u/Charming-Quiet-2617   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</guid>
      <pubDate>Sun, 28 Jul 2024 11:12:20 GMT</pubDate>
    </item>
    <item>
      <title>关于 Stable Baselines3 和 AgileRL 的其他问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</link>
      <description><![CDATA[我尝试使用 SB3 实现我的 RL 算法，但是我对该框架中使用的术语感到很困惑。 我所有的旧 RL 算法都有一个这样的结构： untill episode &lt; max_episode_numbers: for step in max_steps_number: s&#39;, r, d, info = env.step 因此，基本上，我定义了最大 episode 数和最大 step 数，然后让算法运行。 现在在 SB3 中（以 PPO 算法 为例），有许多不同的术语，例如： n_epochs (int) – 优化替代损失时的 epoch 数 total_timesteps (int) – 要训练的总样本数（env 步骤） 据我所知，n_epochs 并不等同于我的 max_episode_numbers上面的示例中，但它是调用代理损失的优化的内部属性。 但如何定义要执行的最大情节数？即使他们论坛上有趣的线程也没有太大帮助。 我个人想出了类似以下的东西，但我不确定它是否有意义（我的想法来自这里）： for episode &lt; max_number_episode： model.learn(total_timesteps = 10_000) model.save(path_model) 更糟糕的是，我试图将该框架与 AgileRL 框架进行比较，后者看起来非常有前途。 在 AgileRL 中，明确定义了最大剧集数量的参数： &quot;EPISODES&quot;: 1000, # 要训练的剧集数量EPISODES&quot;: 1000, # 要训练的剧集数量 甚至是一集中的 max_number 步数： &quot;MAX_STEPS&quot;: 500, # 代理在环境中采取的最大步数 但是对于外部循环，它不采用情节数，而是采用步数。 # 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []# 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []  现在我完全对术语、它们的含义和功能感到困惑    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</guid>
      <pubDate>Sun, 28 Jul 2024 10:24:11 GMT</pubDate>
    </item>
    <item>
      <title>使用多臂老虎机的推荐系统参考资料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</link>
      <description><![CDATA[我正在尝试学习如何将多臂老虎机应用到推荐系统中，但我不知道如何将原始的老虎机问题转换为有数据的场景。例如，这个库 https://github.com/fidelity/mab2rec 声称他们使用多臂老虎机，但他们没有解释如何使用。我试着阅读源代码，但我不太明白。 有人可以推荐一些资源来学习这种特定的应用程序吗？    提交人    /u/VanBloot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</guid>
      <pubDate>Sat, 27 Jul 2024 22:44:09 GMT</pubDate>
    </item>
    </channel>
</rss>