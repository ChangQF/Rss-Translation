<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Tue, 25 Feb 2025 18:24:17 GMT</lastBuildDate>
    <item>
      <title>事后经验重播（她）表现的主要贡献者是什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，我最近一直在研究后视经验重播（她），我一直在研究她的机制在稀疏奖励环境中的性能。 在我看来，她在两个方面增强了性能：   增强探索：  在稀疏的奖励中环境，如果代理未能达到最初的目标，它几乎不会获得任何奖励，从而导致缺乏学习信号并迫使代理继续随机探索。 她通过使用最终状态重新定义了目标作为允许代理商获得实际可达到的状态的奖励的目标。 通过此过程，代理商通过随机行动从达到的各种最终状态中学习，使其能够更好地了解环境    策略概括：  她将目标与国家一起提供给网络的输入，允许有条件学习的政策 - 考虑国家和指定目标。 这使网络能够学习“采取哪种行动和特定目标”，从而提高了其在不同不同的不同的能力目标而不是仅限于一个目标。 ul&gt;   鉴于这些要点，我很好奇哪个因素（增强的探索或政策概括）在她的成功解决稀疏奖励问题方面的成功中扮演着更关键的作用。 另外，我还有一个问题：如果状态空间为r  2 &lt; /sup&gt;，目标是（2,2），但是代理人恰好仅在第二个象限内探索，那么最终状态将局限于该地区。在这种情况下，该政策可能很难将其推广到探索区域之外的（2,2）之类的目标。这样的限制将如何影响她的表现？ &gt; 感谢您的见解和您可以共享的任何相关实验结果。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/drlc_     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</guid>
      <pubDate>Tue, 25 Feb 2025 18:20:38 GMT</pubDate>
    </item>
    <item>
      <title>Q学习，折扣系数为0。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在研究一个项目，以实现Q-学习的代理。我只是意识到对环境，状态和动作进行了配置，因此当前的行动不会影响未来的状态或奖励。我认为在这种情况下，折现因子应该等于零，但是我不知道Q学习代理是否有意义解决此类问题。在我看来，它比MDP更像是上下文的强盗问题。 Q学习算法的名称为0，或等效算法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ixzkgs/qlearning_with_a_a_discount_factor_of_0/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</guid>
      <pubDate>Tue, 25 Feb 2025 16:50:04 GMT</pubDate>
    </item>
    <item>
      <title>精确的仿真模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixtjk0/precise_simulationmodel/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 我目前正在使用Bipedal机器人从事大学项目。我想实现一个基于RL的控制器进行行走。据我所知，有必要拥有一个精确的学习模型，以便成功地跳入SIM2REAL差距。我们在NX中有一个CAD模型，我听说有一个选择将CAD转换为Isaac Sim中的UDF。 ，但是哪些工业“黄金标准”方法是为模拟而获得良好模型的方法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/theoneandonly_ncb      [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixtjk0/precise_simulationmodel/</guid>
      <pubDate>Tue, 25 Feb 2025 12:12:12 GMT</pubDate>
    </item>
    <item>
      <title>现在，增强型套图支持PPO！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  嘿，  racefceui-studio现在包括近端策略优化（PPO）！🚀您在我以前的帖子中可能看到的那样（在这里” &gt; 我收到了许多PPO请求，终于在这里！如果您有兴趣，请检查一下，让我知道您的想法。另外，保持算法请求的到来 - 您的反馈有助于使工具变得更好！  文档： https://docs.reinforceui-studio.com/algorithm-algorithms/algorithm_lists/algorithm_list  github代码： https://github.com/dvalenciar/dvalenciar/reinforceui-studio  &gt;＆＃32;提交由＆＃32; /u/u/dvr_dvr     [link]        [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</guid>
      <pubDate>Tue, 25 Feb 2025 08:18:11 GMT</pubDate>
    </item>
    <item>
      <title>DDPG问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</guid>
      <pubDate>Tue, 25 Feb 2025 00:02:36 GMT</pubDate>
    </item>
    <item>
      <title>地平线很长的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix9yx3/environments_with_extremely_long_horizons/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi ash all  我正在尝试查找具有剧集的环境，这些剧集要完成数以万计的步骤完成。 Starcraft 2（数千），DOTA 2（20K）和Minecraft（24K）属于这一类别。有人知道相关环境吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lilhairdy     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix9yx3/environments_with_extremely_long_horizons/</guid>
      <pubDate>Mon, 24 Feb 2025 19:04:06 GMT</pubDate>
    </item>
    <item>
      <title>与RL一起使用的最佳机器人模拟器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在尝试模拟我的机器人必须与连接到末端效应器的传感器设备进行交互的环境，并使用RL进行读数。我希望然后在实际的硬件上使用这个训练有素的代理。您会推荐什么模拟器？我看过Pybullet和Guazebo。但是我不确定哪个似乎是最简单，最佳的方法，因为我在模拟方面几乎没有经验。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bananaoramama   href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ix8eux/best_robotic_simulator_to_to_use_with_with_rl/”&gt; [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</guid>
      <pubDate>Mon, 24 Feb 2025 18:01:00 GMT</pubDate>
    </item>
    <item>
      <title>奖励成型想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有一个奖励塑造形式的想法，想知道大家都在考虑它。 想象你有一个超级稀疏的奖励功能，例如+1赢得胜利和-1的损失，情节很长。这个奖励功能正是我们想要的。  当然，我们都知道稀疏的奖励功能很难学习。因此，引入密集的奖励功能似乎很有用。一个函数，表明我们的代理商正朝正确或错误的方向行驶。定义这样的奖励函数通常非常棘手，以与我们的真实奖励功能完全匹配，因此我认为暂时使用此奖励功能以最初使我们的代理在政策空间中大致适当的领域是有意义的。 作为免责声明，我必须说我没有阅读任何有关奖励成型的研究，所以如果我的想法很愚蠢，请原谅我。 我过去用DQN做的一件事 - 像算法一样在培训过程中，逐渐从一个奖励功能转移到另一个奖励功能。一开始，我使用了100％的致密奖励功能和稀疏的0％。一段时间后，我开始逐渐“退火”。这个比率直到我只使用真正的稀疏奖励功能。我看得很好。 我这样做的原因是“退火”。是因为我认为Q学习算法很难适应完全不同的奖励功能。但是我确实想知道退火率浪费了多少时间。我也不喜欢退火率是另一个超参数。 我的想法是将奖励函数的硬转换应用于演员批评算法。想象一下，我们将模型训练在密集的奖励功能上。我们假设我们得出了一项体面的政策，也是评论家的体面价值估计。现在，我们要做的就是冻结演员，硬击奖励功能，并重新审查评论家。我认为我们可以消除高参数，因为现在我们可以训练，直到评论家的错误达到一定的门槛为止。我想这是一个新的超参数。无论如何，我们会解开演员并恢复正常的培训。 我认为这在实践中应该很好。我还没有机会尝试。你们都对这个想法有何看法？有什么理由期望它行不通吗？我不是演员 - 批评算法的专家，所以这个想法甚至没有意义。 让我知道！谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</guid>
      <pubDate>Mon, 24 Feb 2025 15:12:25 GMT</pubDate>
    </item>
    <item>
      <title>200 for LLM FINETUNTINING的200个组合身份和定理数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix488a/200_combinatorial_identities_and_theorems_dataset/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/databaebee     [link]  ＆＃32;   [注释]    /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix488a/200_combinatorial_identities_and_theorems_dataset/</guid>
      <pubDate>Mon, 24 Feb 2025 15:09:58 GMT</pubDate>
    </item>
    <item>
      <title>Simbav2：可扩展深度增强学习的超透明标准化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</link>
      <description><![CDATA[     引入 simbav2！   📄项目页面： https://dojeon-ai.github.io/simbav2/  📄纸： https://arxiv.org/abs/2502.15280   🔗代码： https://github.com/dojeon-ai/simbav2     simbav2是一种简单，可扩展的RL体系结构通过简单地用Simbav2替换MLP，&lt; /strong&gt;。&lt; /strong&gt;。演员评论家在57个连续的控制任务（Mujoco，dmcontrol，Myosuite，humyoid-Bench）中实现了最先进的表现（SOTA）。  它与体育馆1.0.0 API   - 尝试一下！ ，如果您有任何疑问，请随时与之伸出援手：） &lt; /div&gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/joonleesky     [link]     [注释]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</guid>
      <pubDate>Mon, 24 Feb 2025 11:43:23 GMT</pubDate>
    </item>
    <item>
      <title>写了我关于生锈的强化学习的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwyveb/wrote_my_thesis_on_reinforcement_learning_in_rust/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/dashdeckers     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwyveb/wrote_my_thesis_on_reinforcement_learning_in_rust/</guid>
      <pubDate>Mon, 24 Feb 2025 10:18:10 GMT</pubDate>
    </item>
    <item>
      <title>我的张板的主要问题！请帮助我</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwyefh/major_issue_with_my_tensorboard_pls_help_me/</link>
      <description><![CDATA[      i我正在训练RL算法，并在张板中记录结果。我是Tensorboard的新手。当我仅记录数据时，我不知道时，我不知道时情节回报和长度是故障或错误。问题在于，日志以0个步骤开始，图表可以在100万步中可以使用，之后的奖励却以100万个IE的差距移动。最后一个数据仅具有20m至21m的图形。  我不知道我是什么错误的事情，你们可以指导我吗？  导入登录从torch.utils.utils.tensorboard import import import import os导入imports imports imports import import inct logger：def __init __（self，run_name，args）：self.log_name = f&#39;logs/{run_name}&#39;self.start_time = time.time（）self.n_eps = 0 os.makedirs（&#39;logs&#39;，stef_ok = true）os.makedirs（&#39;models&#39;，equent_ok = true）self.writer = summaryWriter（self.log_name）logging.basicconfig（level = = logging.debug，格式=&#39;％（asctime）s％（消息）s&#39;，handlers = [ logging.streamhandler（），logging.filehandler（f&#39;{self.log_name} .log&#39;，＆quot&#39;a＆quot;＆quot;＆quot;＆quort;]，]，datefmt =&#39;％y/％m/％m/％d％i：％m：％m：％s％s％p p p &#39;）logging.info（args）def log_scalars（self，scalar_dict，step）：对于键，val in scalar_dict.items（）：self.writer.add_scalar（key，val，step）def log_episode（self，info，step）：rewards = info = info [＆quort returns/epoindic_reward＆quorts;] length = infor = info = info = info; ＃使用长度而不是奖励完成的track情节=长度=长度＆gt; 0对于i在范围内（len（rewards）））：如果完成了_episodes [i]：self.n_eps += 1 powers_data = {＆querts; returns/ependodic_reward＆quot＆quot＆quot＆quot＆quot; } self.log_scalars（emotive_data，step）time_expired =（time.time（） - self.start_time） / 60 /60 logging.info（f＆quot; gt; ep = {self.n_eps} |总步骤= {step}＆quet; f＆quets; f＆quest; | reward = {rewards [rewards [i]} |长度}＆quot;我用来这样做的代码。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/decter_prune_9756      [link]   /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwyefh/major_issue_with_my_tensorboard_pls_help_me/</guid>
      <pubDate>Mon, 24 Feb 2025 09:46:11 GMT</pubDate>
    </item>
    <item>
      <title>如何掌握强化学习的可能性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwwpmo/how_to_master_probability_for_reinforcement/</link>
      <description><![CDATA[在我的概率技能不是他们需要的地方。我在本科期间参加了一个概率课程，但是我忘记了大部分内容。 我不仅想刷新我的记忆，我想变得擅长于概率，直到我可以直观地将其应用于RL和机器学习的其他领域。 对于那些掌握概率的人来说，最适合您的人？有什么书籍，课程，问题集或每日习惯会带来很大的不同吗？ 很想听听您的建议！  &lt;！ -  sc_on-&gt;＆＃32 ;提交由＆＃32; /u/u/hudhuddz     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwwpmo/how_to_master_probability_for_reinforcement/</guid>
      <pubDate>Mon, 24 Feb 2025 07:43:38 GMT</pubDate>
    </item>
    <item>
      <title>RL对于AGI，重点应该放在什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  那些认为RL是通往AGI的可行途径的人，当前需要专注于在RL中求解的当前局限性是什么？人们可以选择为此做出哪些研究问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</guid>
      <pubDate>Mon, 24 Feb 2025 02:33:15 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的RL：开环控制是亚最佳选择，因为..？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在观看Sergei Levine的讲座。他是一个很好的资源；将学习理论的联系起来很多。使用数学测试的类比，通过开环控制通过开环控制是亚最佳的。我想象这个类比就像搜索树一样例如，但即使如此，它也有点清除。但是，要与抽象的例子保持在一起，为什么该模型不会基于以前与环境互动的经验产生可能性？ Sergei提到，如果我们选择测试，我们将得到正确的答案，但也意味着没有办法将这些信息传递给模型（在这种情况下，代理商的决策者）。感觉从现实中消除了，即如果可能的测试尺寸足够大，那么最佳的动作就是回家。如果您对参加测试的能力有任何信心（例如以前的推出经验），那么您的最佳策略会更改，但这是信息，您可以通过与以前的示例相同的分布来理解。  也许我缺少标记。为什么开放环控制次优？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iwhd9t/model_based_rl_rl_openloop_iscontrol_is_suboptimal/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1iwhd9t/model_based_rl_rl_openloop_control_is_is_suboptimal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</guid>
      <pubDate>Sun, 23 Feb 2025 18:52:18 GMT</pubDate>
    </item>
    </channel>
</rss>