<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 11 Sep 2024 12:31:36 GMT</lastBuildDate>
    <item>
      <title>我想将强化学习应用到机械臂上。寻求建议！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fe73oa/i_want_to_apply_reinforcement_learning_to_a/</link>
      <description><![CDATA[您好， 我是一名正在学习强化学习的大学生。我正尝试首次将强化学习应用于机械手，感觉有点不知所措，因此，如果您能提供任何建议，我将不胜感激。  模拟器推荐：我不确定哪种模拟器最适合将强化学习应用于机械手。我听说过 PyBullet、MuJoCo、Gazebo 和其他几种模拟器。哪种模拟器使用最广泛，推荐度最高？ 论文推荐：如果您知道任何关于将强化学习应用于机械手的重要论文或评论文章，我将非常感激您的推荐。尤其是作为初学者，我想知道我应该从哪些论文开始。 推荐的学习资源：如果有任何网站或资源提供该领域的组织良好的学习材料，我将不胜感激。或者，如果您有将 RL 应用于操纵器的建议课程或学习路径，那将非常有帮助。  任何针对初学者的资源或建议都将不胜感激！提前谢谢大家。    提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fe73oa/i_want_to_apply_reinforcement_learning_to_a/</guid>
      <pubDate>Wed, 11 Sep 2024 10:46:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的自学研究方法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdz4jl/selflearning_research_methods_for_rl/</link>
      <description><![CDATA[你好！ 我想攻读 RL 博士学位。但是，我的潜在导师回复说我没有研究经验。 我在 PhD SubReddit 上发帖询问如何自学研究。我注意到似乎有更适合 CS 的研究方法。 我想我可以向这里的集体智慧寻求更具体的建议。我在哪里可以开始自学如何为 RL 空间进行研究和学术写作？ 有人知道好的资源吗？ 天方夜谭：如果这里有博士生导师并且愿意直接留言，我很想知道向你这样的人推销申请的最佳知识和经验基础是什么。 附注：我住在澳大利亚。   由    /u/jeroku  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdz4jl/selflearning_research_methods_for_rl/</guid>
      <pubDate>Wed, 11 Sep 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>学习一个价值函数，然后通过最小化相应的 Q 函数来学习一个策略，最后使用该策略来热启动最优控制求解器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdybbo/learning_a_value_function_then_learning_a_policy/</link>
      <description><![CDATA[任何在优化和最优控制、强化学习编程方面有专业知识的人可以自由联系我 [tesfayedmu@gmail.com](mailto:tesfayedmu@gmail.com)    提交人    /u/AsleepCreme5489   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdybbo/learning_a_value_function_then_learning_a_policy/</guid>
      <pubDate>Wed, 11 Sep 2024 01:19:49 GMT</pubDate>
    </item>
    <item>
      <title>没有任何经验并且只有一个强化学习项目的学生可以在 RL 领域找到工作吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdxgry/can_students_with_no_experience_and_one_project/</link>
      <description><![CDATA[没有发表过论文但有一个 RL 大项目的硕士生能在 RL 领域找到工作吗？    提交人    /u/optimum_point   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdxgry/can_students_with_no_experience_and_one_project/</guid>
      <pubDate>Wed, 11 Sep 2024 00:35:55 GMT</pubDate>
    </item>
    <item>
      <title>分享我的副项目 raice - 现实生活中的特工在 F1 赛道上的赛车比赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdtpn8/sharing_my_side_project_raice_a_racing/</link>
      <description><![CDATA[      嗨！  让我向您展示 https://github.com/Fer14/raice，这是使用不同算法训练的 RL 代理之间的赛车比赛。 不确定如何发布此内容，但自从学习 RL 以来，我认为让所有这些不同的算法以某种方式相互竞争会很有趣。因此，我有了这个想法，我从 YouTube 视频中获取了一个简洁的代理在自定义赛道上训练并实施更多算法（可能还会有更多算法）以查看谁在 F1 赛道上表现最佳。我不是 F1 的狂热爱好者，但我认为添加一条真正的赛道并举办一整场 F1 比赛会很有趣，所以这就是我现在正在做的事情，我认为分享会很有趣。 我不指望它能完美运行，一旦一切完成，我想做一些调整，但现在我认为它很酷！    提交人    /u/Fer14x   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdtpn8/sharing_my_side_project_raice_a_racing/</guid>
      <pubDate>Tue, 10 Sep 2024 21:40:20 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 无法学习简单的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdr8zu/ddpg_fails_to_learn_simple_environment/</link>
      <description><![CDATA[我在 https://github.com/JijaProGamer/Car-Racer-AI &gt; src/GPU/model.js 上有一个非常简单的 DDPG 代码 并且大部分代码都是从 https://keras.io/examples/rl/ddpg_pendulum/ 复制而来，只是用 TFJS（以及扩展的 JS）重写。 这里有一些内容：我使用与 keras 示例相同的超参数，我相信我得到了噪声类工作，并且没有错误，只是一些静默问题。 DQN 也有效（对于离散动作空间），所以我知道这不是环境或内存问题，而是 DDPG 特定代码的问题。 我不确定哪里出了问题，因为一切都看起来像 keras 中的 99%，并且演员损失不断上升，而批评家不断降低（趋于负值）。    提交人    /u/ZazaGaza213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdr8zu/ddpg_fails_to_learn_simple_environment/</guid>
      <pubDate>Tue, 10 Sep 2024 19:56:54 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习中的泛化和安全性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdgne3/generalization_and_safety_in_deep_reinforcement/</link>
      <description><![CDATA[https://blogs.ucl.ac.uk/steapp/2023/12/20/adversarial-attacks-robustness-and-generalization-in-deep-reinforcement-learning/    提交人    /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdgne3/generalization_and_safety_in_deep_reinforcement/</guid>
      <pubDate>Tue, 10 Sep 2024 12:29:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在不违反马尔可夫特性的情况下处理强化学习中的延迟奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd9s9i/how_to_handle_delayed_rewards_in_rl_without/</link>
      <description><![CDATA[大家好，我正在研究一个强化学习问题，其中代理控制交通信号以最小化队列长度和碰撞风险。奖励函数有两个组成部分：  即时奖励：每个时间步骤通过交叉路口的车辆数量。 延迟奖励：只有在完成一个完整的信号周期（4 个阶段）后才能计算的碰撞风险分数。计算出这个碰撞风险分数后，我需要将其分配到前面的步骤中。  奖励=−(队列长度+碰撞风险) 挑战如下：  在每个步骤（操作：延长当前阶段或更改阶段）中，我可以根据通过的车辆数量立即计算奖励（例如，步骤 1：队列长度 = 4，步骤 2：队列长度 = 6，等等）。 但是，碰撞风险评分会延迟，并在整个信号周期之后计算。然后，我想将这个碰撞风险奖励分配到周期的前面几个步骤中（例如，步骤 1 获得一部分碰撞风险）。  示例：  步骤 1：队列长度 = 4，尚无碰撞风险 步骤 2：队列长度 = 6，尚无碰撞风险 步骤 3：队列长度 = 2，尚无碰撞风险 步骤 4：队列长度 = 5，碰撞风险 = 4（仅在此步骤之后已知） 信号周期结束后，我将碰撞风险分数向后分配到前面的步骤中（例如，步骤 1 奖励 = -(4+1)，步骤 2 奖励 = -(6+1)，等等）  问题：  我能否均匀地在不违反马尔可夫特性的情况下将碰撞风险向后分布到各个步骤中（因为奖励通常仅根据当前状态和动作来计算）？ 如果不是，如何在 RL 中正确处理这种延迟奖励，同时保留马尔可夫特性？是否有任何替代技术可以帮助，例如部分可观察的 MDP、N 步 TD 或分层 RL？     提交人    /u/muttahirulislam   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd9s9i/how_to_handle_delayed_rewards_in_rl_without/</guid>
      <pubDate>Tue, 10 Sep 2024 04:41:30 GMT</pubDate>
    </item>
    <item>
      <title>最佳强化学习和人工智能代理资源？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd9r00/best_reinforcement_learning_and_ai_agents/</link>
      <description><![CDATA[我在本科期间曾学习过机器学习和深度学习（监督学习）。 今年我已经毕业了，现在我对 RL 和 AI 代理产生了兴趣。我可以从哪些资源（最好是最新的）中学习？我还希望能够在学习后建立项目，因此如果资源还包含实践知识，那就最好了    提交人    /u/CS_UGRAD24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd9r00/best_reinforcement_learning_and_ai_agents/</guid>
      <pubDate>Tue, 10 Sep 2024 04:39:19 GMT</pubDate>
    </item>
    <item>
      <title>“Carpentopod：一个行走桌项目”（进化出更平滑的滚动腿）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd2h89/carpentopod_a_walking_table_project_evolving/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd2h89/carpentopod_a_walking_table_project_evolving/</guid>
      <pubDate>Mon, 09 Sep 2024 22:29:11 GMT</pubDate>
    </item>
    <item>
      <title>呼吁采取行动：对问题做出反应以帮助提供资金</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</link>
      <description><![CDATA[嗨，RL 的朋友们， 我和我的同事们一直在开发 Tianshou 库，并且即将将其变成真正造福社区的东西。这是我所知道的唯一一个具有广泛范围（各种算法、离线 RL、marl 等，最先进的性能和快速吞吐量）的库，旨在为研究人员和应用程序开发人员提供帮助。 现在我们公司发生了一些变化，导致新经理是一位非技术人员，不熟悉战略。如果我们不能以他们理解的方式展示社区的兴趣，该项目的资金可能会被取消。 我已经写了一个非常详细的计划，将 Tianshou 带到下一个主要版本，我相信结果对整个 RL 社区非常有用。我非常感谢您的支持，您只需对此问题留下点赞或评论即可。 当然，我也很高兴在那里就该计划和图书馆本身进行任何建设性的讨论！    提交人    /u/Left-Orange2267   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</guid>
      <pubDate>Mon, 09 Sep 2024 20:50:01 GMT</pubDate>
    </item>
    <item>
      <title>你们在哪里使用强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</link>
      <description><![CDATA[嗨，朋友们！ 我正在研究 RL，我想知道哪些公司正在应用 RL 来解决业务问题。当我搜索这个主题时，我只找到旧案例和来自大型科​​技公司的案例。 你们在学术界使用 RL 吗？你们在初创公司使用 RL 吗？只是想知道你们如何使用它并试图了解市场。 谢谢！    提交人    /u/embedding_turtle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</guid>
      <pubDate>Mon, 09 Sep 2024 20:48:33 GMT</pubDate>
    </item>
    <item>
      <title>深度Q迷宫</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fcjmzt/deep_q_maze/</link>
      <description><![CDATA[我正在使用深度 q 学习处理一个 8x8 的冰冻湖泊 - 它就像一个迷宫，但里面有洞，如果进入就会终止。我见过的许多其他实现直到迷宫至少完成一次才开始训练。我面临的问题是迷宫太大，无法偶然/不经过任何训练就解决（较小的迷宫效果很好）。当我允许它在不完成迷宫的情况下进行训练时，它会学会避开洞，而不是朝着终点前进。如果终止，奖励为 0，如果完成迷宫，奖励为 1 - 其他任何事情都使用奖励函数：0.9 * maxQ（下一个状态）。这个问题的解决方案是什么/我做错了什么？任何想法都将不胜感激。     提交人    /u/Magic__Mannn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fcjmzt/deep_q_maze/</guid>
      <pubDate>Mon, 09 Sep 2024 07:18:26 GMT</pubDate>
    </item>
    <item>
      <title>在价值迭代中，终端状态的值应该保持不变还是不保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</link>
      <description><![CDATA[我遵循了这个例子： https://courses.grainger.illinois.edu/cs440/fa2022/lectures/rl.html 这是一个网格世界，其中折扣 = 0.9，右上角的奖励为 +1，中右为 -1，而其他网格的奖励为零。 正如您所见，两个终端网格（右上和中右）的值在整个迭代过程中一直保持 +1 和 -1，这意味着它通过贝尔曼方程的迭代强制保持它们不变，但我在网上找到了多个其他数值例子，其中终端状态的值也根据贝尔曼方程进行更新。所以我的问题是：哪一种是正确的方法？非常感谢    提交人    /u/james_stevensson   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</guid>
      <pubDate>Sun, 08 Sep 2024 21:31:57 GMT</pubDate>
    </item>
    <item>
      <title>有沒有主动推理（自由能原理）的成功故事？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</link>
      <description><![CDATA[虽然自由能原理旨在通过最小化意外来开发一个统一的感知和控制框架，但据我所知，几乎没有经验结果可以证明其前景。有没有人听说过它在图像输入的连续控制问题或至少一些经典控制问题中的一些成功应用？    提交人    /u/OutOfCharm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</guid>
      <pubDate>Sun, 08 Sep 2024 09:48:59 GMT</pubDate>
    </item>
    </channel>
</rss>