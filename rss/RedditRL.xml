<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 28 Nov 2023 05:50:03 GMT</lastBuildDate>
    <item>
      <title>离策略演员批评家目标函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</link>
      <description><![CDATA[      我正在阅读 Silver 的 DPG 论文。在这里，如下所示，目标函数已使用行为策略 beta 进行了修改。我很好奇，如果使用梯度最大化下面的目标，目标策略的目标函数（通常的策略目标）是否会最大化？ ​ &lt; a href=&quot;https://preview.redd.it/1779aomlyz2c1.png?width=737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e&quot;&gt;https://preview.redd.it/1779aomlyz2c1.png?width =737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e   由   提交 /u/RealJuney   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</guid>
      <pubDate>Tue, 28 Nov 2023 02:12:27 GMT</pubDate>
    </item>
    <item>
      <title>寻找职业建议。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</link>
      <description><![CDATA[大家好，过去 3 年我一直对机器学习感兴趣，我的大部分注意力都集中在监督学习上，但是在过去 3 个月里 RL引起了我的注意，我相信人工智能的下一个重大事件将来自该领域。我有兴趣通过学术界，因为我只有计算机科学学士学位，并且不会找到工作，因为我在津巴布韦，而我们在技术方面还没有达到这个水平。我申请在美国攻读博士学位，但拒绝的次数越来越多，所以我很可能最终会去中国获得奖学金。我想要一些建议，因为最终我想在西方的大公司从事研发工作。如果可以的话，请告诉我在中国攻读硕士学位期间我可以做些什么，以便在 2026/27 年毕业后让我更接近这个目标。 PS：我也在中国获得了学士学位。   由   提交/u/congo43  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</guid>
      <pubDate>Mon, 27 Nov 2023 19:53:09 GMT</pubDate>
    </item>
    <item>
      <title>多头DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</link>
      <description><![CDATA[大家好，我正在应用 DQN 每次选择一组元素（一次一个或多个）。如何避免动作 [0, 0, 0,…]，即如何强制代理选择至少一个元素？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</guid>
      <pubDate>Mon, 27 Nov 2023 19:01:00 GMT</pubDate>
    </item>
    <item>
      <title>确定性参数总是输出相同的动作（PPO）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1859bj1/deterministic_parameter_always_output_the_same/</link>
      <description><![CDATA[      我使用 SB3 中的 PPO 和动作掩码来训练具有以下超参数的环境。在训练中，模型似乎学会了采取不同的行动。然而，在测试阶段，模型似乎做同样的事情并且表现正常，除了当我使用“确定性 = True”时，模型始终只选择一个操作。 我的代码： action，_states = model.predict（obs，action_masks=env.valid_action_mask()，确定性=True）  initial_learning_rate = 0.00005  model = MaskablePPO(MaskableActorCriticPolicy, env, tensorboard_log=&quot;./tensorboard&quot; ,n_steps=2048 ,learning_rate=initial_learning_rate) # 创建根据奖励调整学习率的回调 callback = RewardBasedLearningRateSchedule() for i in range (1,202): model.learn(total_timesteps=TIMESTEPS , tb_log_name = &#39;PPO2&#39; , reset_num_timesteps=False,callback=callback) &lt;代码&gt;model.save(f&quot;{models_dir}/{TIMESTEPS*i}&quot;) ​ tensororad的输出： https://preview.redd.it/cpdvwhxnkx2c1.png?width=1652&amp;format=png&amp;auto=webp&amp;s=a6d3bdef2f5040ce935a6534b66de2d934029af6   由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1859bj1/deterministic_parameter_always_output_the_same/</guid>
      <pubDate>Mon, 27 Nov 2023 18:07:01 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 3.0 对阵 Isaac Gym</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</link>
      <description><![CDATA[您好， 对于那些尝试过并且熟悉 Mujoco 3.0 和 Isaac Gym 的人，建议他们使用哪一个学习以及为什么？   由   提交 /u/anointedninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</guid>
      <pubDate>Mon, 27 Nov 2023 16:59:33 GMT</pubDate>
    </item>
    <item>
      <title>用于图像分类的 DQN（阿尔茨海默病）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1856abz/dqn_for_image_classification_alzheimer/</link>
      <description><![CDATA[您好，我正在使用 2D MRI 扫描进行研究。有4个班。我想创建一个可以执行分类任务的 DQN。有人能帮我吗？   由   提交/u/armaghanbz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1856abz/dqn_for_image_classification_alzheimer/</guid>
      <pubDate>Mon, 27 Nov 2023 16:01:44 GMT</pubDate>
    </item>
    <item>
      <title>“Open AI Gym”如何跟踪 CartPole 环境中超过 500 的步数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1853fmr/how_did_open_ai_gym_keep_track_of_steps_exceeding/</link>
      <description><![CDATA[我正在查看 这里，我没有看到 `step` 函数（或任何其他函数）如何确保代理不会跨越 500 步 - &lt; /p&gt; ``` def step(self, action): err_msg = f&quot;{action!r} ({type(action)}) 无效&quot; &gt; assert self.action_space.contains(action), err_msg assert self.state is not None, &quot;在使用step方法之前调用reset。&quot; x, x_dot, theta, theta_dot = self.state force = self.force_mag if action == 1 else -self.force_mag costheta = math.cos(theta) sintheta = math.sin(theta) ​ # 对于感兴趣的读者： # https://coneural.org/florian/papers/05_cart_pole.pdf temp = ( force + self.polemass_length * theta_dot**2 * sintheta ) / self.total_mass thetaacc = (self.gravity * sintheta - costheta * temp) / ( self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass) ) xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass ​ 如果 self.kinematics_integrator == &quot;euler&quot;: x = x + self.tau * x_dot x_dot = x_dot + self.tau * xacc theta = theta + self.tau * theta_dot theta_dot = theta_dot + self.tau * thetaacc&lt; /p&gt; else: # 半隐式欧拉 x_dot = x_dot + self.tau * xacc x = x + self.tau * x_dot &lt; p&gt;theta_dot = theta_dot + self.tau * thetaacc theta = theta + self.tau * theta_dot ​ self.state = ( x, x_dot, theta, theta_dot) ​ 终止 = bool( x &lt;; -self.x_threshold 或 x &gt; self.x_threshold 或 theta &lt; -self.theta_threshold_radians 或 theta &gt; self.theta_threshold_radians ) ​ 如果没有终止： 奖励 = 1.0  elif self.steps_beyond_termminate is None: # 杆子刚刚掉下来！ self.steps_beyond_termminate = 0 reward = 1.0 else:  if self.steps_beyond_terminate == 0: logger.warn( “您正在调用 &#39;step()&#39;，即使这样” “环境已返回终止 = True。您” “一旦收到“终止 =”，就应始终调用“reset()”  “正确”——任何进一步的步骤都是未定义的行为。” ) self.steps_beyond_termerated += 1 reward = 0.0 ​ 如果 self.render_mode == “人类”: self.render() return np.array(self .state, dtype=np.float32), 奖励, 终止, False, {} ```  但 Farama Gymnasium 的情况并非如此。 步骤函数具有以下代码来确保它 -  ``` 截断 = self.steps &gt;= self.max_episode_steps ​ ```  不幸的是，我应该运行“gym”环境。我目前面临的问题是，即使代理跨越 500 步，也不会停止。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1853fmr/how_did_open_ai_gym_keep_track_of_steps_exceeding/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1853fmr/how_did_open_ai_gym_keep_track_of_steps_exceeding/</guid>
      <pubDate>Mon, 27 Nov 2023 13:52:19 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线推出/ep_rew_mean</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1850jcz/stable_baselines_rolloutep_rew_mean/</link>
      <description><![CDATA[我正在尝试做一些需要我能够获取每 4 集打印到命令提示符中的 ep_rew_mean 值的事情。但我只是无法弄清楚这些值存储在哪里，以便我可以在回调函数中访问它们。我正在从“ep_info_buffer”访问最后一集奖励，但这与我需要的 ep_rew_mean 有很大不同。我为此查看了记录器、OffPolicyAlgorithm 和 SAC 文件，但仍然找不到它。这些信息在打印之前存储在哪里？我如何访问它？   由   提交 /u/aliaslight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1850jcz/stable_baselines_rolloutep_rew_mean/</guid>
      <pubDate>Mon, 27 Nov 2023 11:08:48 GMT</pubDate>
    </item>
    <item>
      <title>帮助调试 TD3（4 条悬挂电缆）电缆机器人目标到达任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/184ypl4/help_in_debugging_td3_for_4_suspended_cables/</link>
      <description><![CDATA[大家好， 我正在为电缆机器人开发 RL 控制器，以学习目标到达任务。 我使用 obi 绳索包对带有柔性电缆的电缆机器人进行了统一模拟。 我一直在努力与代理学习正确的策略。 我会非常有兴趣建立合作来解决问题。 任何成功的结果都将联合发布。 任何有兴趣并且愿意的人，请通过 &lt; a href=&quot;mailto:rohit.dhakate@aau.at&quot;&gt;rohit.dhakate@aau.at 以获取有关该项目和当前状态的更多详细信息。 亲切的问候。   由   提交/u/fx619  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/184ypl4/help_in_debugging_td3_for_4_suspended_cables/</guid>
      <pubDate>Mon, 27 Nov 2023 09:02:29 GMT</pubDate>
    </item>
    <item>
      <title>没有批评家模型的策略梯度算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/184wmg5/policy_gradient_algorithms_without_critic_model/</link>
      <description><![CDATA[是否可以在没有 actor-critic 的情况下实现像 NPG、PPO 这样的策略梯度算法，或者它会破坏这些算法中当前更快的收敛性吗？  如果我想这样做，我可以从哪里开始呢？    由   提交/u/eles0range  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/184wmg5/policy_gradient_algorithms_without_critic_model/</guid>
      <pubDate>Mon, 27 Nov 2023 06:37:39 GMT</pubDate>
    </item>
    <item>
      <title>Q*：多步推理过程监督的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/184qj06/q_reinforcement_learning_for_process_supervision/</link>
      <description><![CDATA[那么我们普通人是否可以尝试模仿 Q*(Q-star) 所采取的方法，据称该方法使用强化学习来进行多进程的过程监督-步骤推理（又名“思想链”）？ https ://openai.com/research/improving-mathematical-reasoning-with-process-supervision https ://arxiv.org/abs/2305.20050 是否有可能采用上述强化学习并将其作为迁移学习应用到像 Orca2 这样的小语言模型上？如果是这样，那么最好的方法是什么？我可以查找哪些代码示例对我有帮助？   由   提交/u/san__man   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/184qj06/q_reinforcement_learning_for_process_supervision/</guid>
      <pubDate>Mon, 27 Nov 2023 01:08:37 GMT</pubDate>
    </item>
    <item>
      <title>当前离策略深度强化学习的 SOTA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/184ibu2/current_sota_for_offpolicy_deep_rl/</link>
      <description><![CDATA[我很想在这里查询社区。我有兴趣确定下一个要添加到 ML-Agents 的离策略算法。目前的候选者包括 TQC、REDQ 和 DroQ。参考：https://github.com/Unity-Technologies/ml-agents/issues/6012   由   提交 /u/drmajr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/184ibu2/current_sota_for_offpolicy_deep_rl/</guid>
      <pubDate>Sun, 26 Nov 2023 19:19:09 GMT</pubDate>
    </item>
    <item>
      <title>大脑中的多时间尺度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18440cc/multitimescale_reinforcement_learning_in_the_brain/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2023.11.12.566754v1 代码：https://github.com/pablotano8/multi_timescale_RL 摘要： 蓬勃发展复杂的环境、动物和人工智能体必须学会自适应地行动，以最大限度地提高适应度和回报。这种适应性行为可以通过强化学习来学习，强化学习是一类在训练人工智能体和表征中脑多巴胺神经元放电方面取得成功的算法。在经典的强化学习中，代理根据由折扣因子控制的单个时间尺度以指数方式折扣未来奖励。在这里，我们探索生物强化学习中多个时间尺度的存在。我们首先证明强化代理在多个时间尺度上学习具有明显的计算优势。接下来，我们报告执行两种行为任务的小鼠中的多巴胺神经元用多种折扣时间常数编码奖励预测误差。我们的模型解释了提示诱发的瞬态反应和称为多巴胺斜坡的较慢时间尺度波动的时间贴现的异质性。至关重要的是，测量到的单个神经元的折扣因子在这两项任务中是相关的，这表明它是细胞特异性的属性。总之，我们的结果提供了一个新的范例来理解多巴胺神经元的功能异质性，为人类和动物在许多情况下使用非指数折扣的经验观察提供了机制基础，并为设计更有效的强化开辟了新途径学习算法。   由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18440cc/multitimescale_reinforcement_learning_in_the_brain/</guid>
      <pubDate>Sun, 26 Nov 2023 05:57:59 GMT</pubDate>
    </item>
    <item>
      <title>新手问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/183gccb/newbie_questions/</link>
      <description><![CDATA[请尽可能多地回答 我是 RL 方面的新手和业余爱好者。我的首选设置是 OpenAI Gym with Stable Baselines3。  我有一些问题想问，但 Google 或 ChatGPT 尚未回答，堆栈溢出可能会将其否决为深渊。🥸  是由于我们没有基本事实，因此强化学习中未使用纪元这个术语？如果是，您能指出如何实现吗？ 熵正则化可以使用 SB3 PPO 完成还是需要使用 PyTorch 或任何其他方式实现？它还具有我可以访问的体验重播缓冲区吗？ 我的 TensorBoard 仪表板没有按照 verbose=1 的文档中所述输出应有的所有图表，随机输出一次，但是只有一次。有什么想法吗？ 在 OpenAI Gym 中测试时，状态 = 观察是步骤的输出吗？ 当我们使用SB3 中的 Predict() 方法，我们是否向 NN 发送我的观察结果，或者与 NN 的交互是如何发生的？   非常感谢您的回答。   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/183gccb/newbie_questions/</guid>
      <pubDate>Sat, 25 Nov 2023 09:46:02 GMT</pubDate>
    </item>
    <item>
      <title>推广 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/183bgwl/generalise_ppo/</link>
      <description><![CDATA[嗨，我正在研究多代理环境，该环境适用于许多任务。奖励给出了每个级别中特定任务的进展。该任务特定于个体代理，而不是协作的。我对所有代理使用集中式 PPO 算法，我应该从哪里开始使代理更加通用和具有探索性。    由   提交/u/MachinePolaSD   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/183bgwl/generalise_ppo/</guid>
      <pubDate>Sat, 25 Nov 2023 04:27:50 GMT</pubDate>
    </item>
    </channel>
</rss>