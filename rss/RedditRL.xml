<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 24 Jun 2024 21:13:57 GMT</lastBuildDate>
    <item>
      <title>在 ROS+Gazebo 中开始使用 DAgger</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</link>
      <description><![CDATA[大家好， 由于普通的强化学习未能满足我的期望，因此我希望转向模仿学习。这是一个基于操纵器的项目，带有眼手深度摄像头。我有一个可以成为专家的经典算法。所有这些都是在 ROS+gazebo 中完成的。 我该如何使用模仿学习来做到这一点。具体来说，我可以使用任何现成的库吗？我如何在 Gazebo 中保存来自我的专家的演示。如果有人以前这样做过    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</guid>
      <pubDate>Mon, 24 Jun 2024 21:03:55 GMT</pubDate>
    </item>
    <item>
      <title>这难道不是《IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO》这篇论文中的问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</link>
      <description><![CDATA[      我正在阅读这篇论文：“深度强化学习中的实施问题：PPO 和 TRPO 案例研究” [pdf 链接]。 我认为我对这篇论文的信息有疑问。看看这个表格： https://preview.redd.it/uaw20jf6fk8d1.png?width=1056&amp;format=png&amp;auto=webp&amp;s=e3c698529ec45dc4ad71b807f587572db2988dba 根据这个表格，作者认为 TRPO+ 即 TRPO 加上 PPO 的代码级优化优于 PPO。因此，这表明代码级优化比算法更重要。我的问题是，他们说他们对 TRPO+ 中打开和关闭代码级优化的所有可能组合进行网格搜索，而对于 PPO，则是将所有优化都打开。  我的问题是，通过进行网格搜索，他们给了 TRPO+ 更多的机会来获得一次良好的运行。我知道他们使用种子，但有 10 个种子。根据 Henderson 的说法，这还不够，因为即使我们做 10 个随机种子，将它们分组为两个 5 个种子并绘制奖励和标准差，我们也会得到完全分离的图，这表明方差太高，无法被 5 个种子或我猜甚至 10 个种子捕获。  因此，我不知道他们的论点在他们正在进行的网格搜索下如何成立。至少，他们也应该对 PPO 进行网格搜索。  我遗漏了什么？   由    /u/miladink  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</guid>
      <pubDate>Mon, 24 Jun 2024 18:53:12 GMT</pubDate>
    </item>
    <item>
      <title>“Rho-1：并非所有代币都是你所需要的”，Lin 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</guid>
      <pubDate>Mon, 24 Jun 2024 18:38:52 GMT</pubDate>
    </item>
    <item>
      <title>无模型 Stewart 平台</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</link>
      <description><![CDATA[        由    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</guid>
      <pubDate>Mon, 24 Jun 2024 18:35:11 GMT</pubDate>
    </item>
    <item>
      <title>这是 RL 的一个巧妙应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</link>
      <description><![CDATA[数据驱动的强化学习，实现洗衣机中的最佳电机控制    提交人    /u/Obsesdian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</guid>
      <pubDate>Mon, 24 Jun 2024 16:27:03 GMT</pubDate>
    </item>
    <item>
      <title>有人可以创建完整的 Pytorch -> Jax (编译器) 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnedvc/can_someone_create_full_pytorch_jax_compiler/</link>
      <description><![CDATA[嗨！ 我读到 JAX 适用于常量大小数组。假设我们有 torch 数组： self.states = torch.zeros((self.capacity, state_dim), dtype=torch.float32).to(device) self.actions = torch.zeros((self.capacity, action_dim), dtype=torch.float32).to(device) self.rewards = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device) self.next_states = torch.zeros((self.capacity, state_dim), dtype=torch.float32).to(device) self.dones = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device)  但对我来说，JAX 代码看起来像汇编程序，而不是直观友好的 Pytorch。我相信有大量用 Pytorch（或 Tensorflow）编写的库。是否可以创建完整的 Pytorch -&gt; JAX 编译器并运行代码，例如通过以下方式从编译器训练（及其依赖项）： sys.argv 这样我们就不需要像在 C 语言中那样转到 JAX 细节了。 PS: 当我们达到 Replay Buffer 最大值时，我们可以通过 roll 函数进行左移：  def add(self, state, action, reward, next_state, done): if self.length&lt;self.capacity: self.length += 1 idx = self.length-1 self.states[idx,:] = torch.FloatTensor(state).to(self.device) self.actions[idx,:] = torch.FloatTensor(action).to(self.device) self.rewards[idx,:] = torch.FloatTensor([reward]).to(self.device) self.next_states[idx,:] = torch.FloatTensor(next_state).to(self.device) self.dones[idx,:] = torch.FloatTensor([done]).to(self.device) if self.length==self.capacity: self.states = torch.roll(self.states, shifts=-1, dims=0) self.actions = torch.roll(self.actions, shifts=-1, dims=0) self.rewards = torch.roll(self.rewards, shifts=-1, dims=0) self.next_states = torch.roll(self.next_states, shifts=-1, dims=0) self.dones = torch.roll(self.dones, shifts=-1, dims=0)     提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnedvc/can_someone_create_full_pytorch_jax_compiler/</guid>
      <pubDate>Mon, 24 Jun 2024 14:09:15 GMT</pubDate>
    </item>
    <item>
      <title>字典格式与数组格式以及对性能的影响？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dn6pqo/dict_format_vs_array_format_and_affect_on/</link>
      <description><![CDATA[我有四个独立且不具有空间相关性的因素。我正在尝试决定如何构建我的观察结果以训练机器学习模型。考虑到当观察结果是字典时，它们是扁平的，如果我将它们存储为字典而不是连接数组，在训练方面会有所不同吗？   DICT：obs = {“factor1”：[0，0，1，0，0]，“factor2”：[1，0，0，0，0]，“factor3”：[0，0，0，1，0]，“factor4”：[0，0，0，0，1]  ARRAY：obs = [0，0，1，0，0，1，0，0，0，0，0，0，0，0，1，0，0，0，0，1]     由    /u/TheExistentialSaudi  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dn6pqo/dict_format_vs_array_format_and_affect_on/</guid>
      <pubDate>Mon, 24 Jun 2024 06:15:43 GMT</pubDate>
    </item>
    <item>
      <title>问题：朴素 GA 没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dn3g8u/question_naive_ga_not_learning/</link>
      <description><![CDATA[      你们知道为什么我的蛇没有学习吗？它占据了整个 8x8（64）游戏板，并输出一个动作（4 个神经元）。 我正在做的是：1. 初始化随机基础网络（a）2. 初始化随机突变网络（b）3. 对于每个并发模拟，通过（simulation_index / total_simulations）[因子] 合并 a 和 b 这很幼稚，因为没有交叉。 它在某种程度上有效，但是当蛇长大时它就停止学习了。 有人可以解释一下吗？    提交人    /u/mguinhos   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dn3g8u/question_naive_ga_not_learning/</guid>
      <pubDate>Mon, 24 Jun 2024 02:59:37 GMT</pubDate>
    </item>
    <item>
      <title>编程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</link>
      <description><![CDATA[        提交人    /u/chagdubbish   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</guid>
      <pubDate>Sun, 23 Jun 2024 20:06:01 GMT</pubDate>
    </item>
    <item>
      <title>“对在符号多步推理任务上训练的 Transformer 的机制分析”，Brinkmann 等人 2024 年（Transformer 可以在前向传递中进行内部规划）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmt524/a_mechanistic_analysis_of_a_transformer_trained/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmt524/a_mechanistic_analysis_of_a_transformer_trained/</guid>
      <pubDate>Sun, 23 Jun 2024 18:46:40 GMT</pubDate>
    </item>
    <item>
      <title>为 Pong 游戏制作了 AI，但它仍然很笨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dms403/made_ai_for_pong_game_but_it_is_still_dumb/</link>
      <description><![CDATA[我是否缺少剧集数量？我不知道，但是当我在游戏中尝试时，它完全缺乏如何用击球手击球的知识（典型的 Pong 游戏） 这是train.py -&gt; https://www.pythonmorsels.com/p/2y9vw/ 这是env.py -&gt; https://www.pythonmorsels.com/p/2vdwb/    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dms403/made_ai_for_pong_game_but_it_is_still_dumb/</guid>
      <pubDate>Sun, 23 Jun 2024 18:00:49 GMT</pubDate>
    </item>
    <item>
      <title>关于dreamerv3的方程推导的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmiv75/question_about_the_equation_derivation_of/</link>
      <description><![CDATA[      嗨！ 我尝试学习 dreamerv3 及其代码实现。但是对于等式 10， https://preview.redd.it/9xv44gx5oa8d1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8330ac94061559235a5b0448251c2561f64628d4 我对 ln_p 的计算感到困惑，它对应于以下代码片段 log_pred = self.logits - jax.scipy.special.logsumexp(self.logits, -1, keepdims=True) 你能向我解释一下上述实现吗？为什么要以这种方式推导 log_pred？ 提前致谢。    提交人    /u/UpperSearch4172   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmiv75/question_about_the_equation_derivation_of/</guid>
      <pubDate>Sun, 23 Jun 2024 10:03:25 GMT</pubDate>
    </item>
    <item>
      <title>股票交易的监督预学习。以前做过吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmdn23/supervised_pre_learning_for_stock_trading_has/</link>
      <description><![CDATA[我正在尝试为论文提出新颖的想法，我有一个关于使用监督学习“热身”代理的想法。一般的想法是，例如，在 2005-01-01 到 2010-12-31 之间，代理将接受“理想”操作的训练。从 2011-01-01 到 2020-12-31，代理将再次接受重新训练。以前做过吗？有没有关于这种概念的论文我应该读一读？    提交人    /u/newjeison   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmdn23/supervised_pre_learning_for_stock_trading_has/</guid>
      <pubDate>Sun, 23 Jun 2024 04:05:13 GMT</pubDate>
    </item>
    <item>
      <title>自适应四轴飞行器网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</link>
      <description><![CDATA[👋 嘿，我正在尝试想出一个四轴飞行器无人机自主控制的解决方案。 到目前为止，我设法让它使用简单的 MLP + PPO 进行航点导航。 这很有效，特别是在域随机化的情况下，但如果突然刮起一阵风会发生什么？四轴飞行器的重量超过了它在模拟中训练的重量？螺旋桨损坏，其中一个电机产生的推力减小？ 通过所有这些示例，我的问题是无人机无法适应变化。我想创建一个能够适应任何无法解释的外部干扰的网络。 我有两个计划，一个是将 LSTM 与 PPO 结合使用，因此代理会保留飞行数据。 我的另一个想法是使用我用于训练的模拟器来获取无人机在当前时间步长的预期速度/姿态/位置，并将其与我从传感器获得的实际值一起输入到策略中。 我希望更有经验的人可以对我的想法提供反馈，说实话我有点迷茫，我不确定这些是否可行，任何帮助都值得赞赏！    提交人    /u/FutureComedian7749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</guid>
      <pubDate>Sat, 22 Jun 2024 23:15:56 GMT</pubDate>
    </item>
    <item>
      <title>AgileRL - 用于最先进深度强化学习的进化型 RLOps</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</link>
      <description><![CDATA[嗨，我之前发布过关于我们的强化学习进化超参数优化实现 SOTA 结果的帖子，但我想分享的是，我们的开源框架现在已经发布了 v1.0.0 版本！ 请查看！https://github.com/AgileRL/AgileRL 该库最初专注于通过开创强化学习的进化 HPO 技术来减少训练模型和超参数优化所需的时间。进化 HPO 已被证明可以通过自动收敛到最佳超参数来大幅减少总体训练时间，而无需进行大量的训练运行。 我们不断添加更多算法和功能。 AgileRL 已经包含了最先进的可进化的在线策略、离策略、离线、多智能体和上下文多臂老虎机强化学习算法以及分布式训练。 我很乐意收到您的反馈！    提交人    /u/nicku_a   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</guid>
      <pubDate>Fri, 21 Jun 2024 17:50:05 GMT</pubDate>
    </item>
    </channel>
</rss>