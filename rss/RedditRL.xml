<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 06 Jan 2024 00:59:51 GMT</lastBuildDate>
    <item>
      <title>为什么在强化学习中需要包含随机元素 epsilon？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zmrfl/why_do_you_need_to_include_a_random_element/</link>
      <description><![CDATA[假设您正在尝试自动化吃豆人游戏。您拥有所有 pacman 状态，并获取每个可能操作的 q 值。为什么要有随机性的因素呢？随机性如何发挥作用来获取 q 值？   由   提交 /u/Throwawaybutlove   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zmrfl/why_do_you_need_to_include_a_random_element/</guid>
      <pubDate>Sat, 06 Jan 2024 00:43:32 GMT</pubDate>
    </item>
    <item>
      <title>支持 3D 和物理的机器人强化学习的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zgqid/best_library_for_reinforcement_learning_in/</link>
      <description><![CDATA[我目前正在使用 Unity ML 代理，它相当直观且运行良好。我确实发现它有限制，尤其是最近的 Unity 戏剧，我不确定它是否可以免费使用或长期支持。 我想切换到开源的东西，这会给我带来好处作为程序员，我有更多的控制权 我使用 OpenCV 制作了一个自定义的 2D 健身房，用于稳定基线，并且效果很好。  我需要将 3D 用于机器人技术，并最终与真实系统交互并使用传感器进行反馈。  我对 PyChrono 感到很兴奋，它似乎拥有所有正确的功能，但我就是无法让它工作。  查看教程，他们只有 1 个关于强化学习的教程。 https://api.projectchrono.org/tutorial_pychrono_demo_tensorflow.html 当尝试遵循它时，它要求安装tensorflow-gpu=1.14，它非常旧，并且不能与其他安装指令使用的Python=3.9正确安装 而且他们的主库大约4个月前停止获取更新，不确定是否停止开发 总体而言，PyChrono 对 ML 的支持很差，使用起来会很麻烦。 有哪些更好的替代方案可以继续获得支持？&lt; /p&gt; OpenAI Gym 是否配备 3D/物理/渲染引擎？这会得到多年的支持吗？ 谢谢 编辑。我找到了 PyBullet。似乎正是我正在寻找的。对此有何建议？   由   提交/u/Sharp-Cat2319  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zgqid/best_library_for_reinforcement_learning_in/</guid>
      <pubDate>Fri, 05 Jan 2024 20:30:35 GMT</pubDate>
    </item>
    <item>
      <title>[问题]强化学习算法资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zd6dc/question_resource_for_reinforcemnt_learning/</link>
      <description><![CDATA[是否有任何资源可以解释所有最新的重要深度强化学习算法？我看过博客和文章。我还找到了以下论文： 2209.14940.pdf (arxiv.org) 谢谢   由   提交 /u/Top_Badger9050   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zd6dc/question_resource_for_reinforcemnt_learning/</guid>
      <pubDate>Fri, 05 Jan 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习进行非线性最优控制的最优差距。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zbznr/optimality_gap_in_using_reinforcement_learning/</link>
      <description><![CDATA[您好，我一直在研究文献，包括许多 ML 会议，以寻找使用 RL 解决非线性最优控制问题的论文。我看到使用 Lyapunov 函数和其他安全强化学习应用程序对安全性有很多保证，但与经典方法相比，我找不到任何关于强化学习优化此类问题的能力的理论研究。就像最优差距一样。例如，对于具有非线性系统的 L2 目标 xTQx+uTRu，我希望找到一些论文，说明如果您使用此代理、网络结构，您将获得更好的性能等，但我没有找到任何将强化学习或神经网络的设计与控制器的最优性差距联系起来的研究   由   提交 /u/Specialist_Welder553   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zbznr/optimality_gap_in_using_reinforcement_learning/</guid>
      <pubDate>Fri, 05 Jan 2024 17:14:05 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法的分类</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18z9kt7/classification_of_rl_algorithms/</link>
      <description><![CDATA[大家好， 我想对 RL 算法进行分类。据我了解，分类有两个维度。第一个维度基于代理在学习过程中如何收集和利用数据：在策略学习和离策略学习。第二个维度基于一般策略：基于价值的方法、基于策略的方法、基于行动者批评家的方法。 ​ 现在我想根据这两个维度对以下算法进行分类： - Sarsa：在策略学习，基于价值的方法 - REINFORCE：在策略学习，基于策略的方法 - A2C：On-policy学习，基于actor-critic的方法 - PPO：On-policy学习，基于actor-critic的方法和基于策略的方法 - Q-Learning：Off-policy学习， value-based method - DQN：Off-policylearning，基于value的方法 - TD3：Off-policylearning，基于actor-critic的方法 - DDPG：Off-policylearning，基于演员-评论家的方法 ​ 你对我的分类有何看法？这是对的吗？有时算法可能分为两类，例如 PPO，它是一种基于 actor-critic 的方法，也是一种基于策略的方法。    由   提交 /u/PBerit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18z9kt7/classification_of_rl_algorithms/</guid>
      <pubDate>Fri, 05 Jan 2024 15:32:51 GMT</pubDate>
    </item>
    <item>
      <title>我应该期望代理学习多快？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18z0hlj/how_fast_should_i_expect_the_agent_to_learn/</link>
      <description><![CDATA[我是 RL 新手，刚开始玩扫雷游戏。一开始，我希望模型学习的唯一一件事是避免点击已经打开的方块。 我知道这可以通过动作屏蔽来完成，但我很好奇，想看看学习这个简单的行为需要多长时间。 点击打开的方块的奖励是-10000，而做其他任何事情的奖励是10。令我惊讶的是，训练持续了很长时间几个小时了，特工仍然没有学会避开已经打开的方块。目前，我看到大约 10% 的移动是点击一个打开的方块。 我只是想知道这是否太长。 以下是有关我的设置的更多信息：  p&gt;  我用的是gymnasium和stable-baselines3，型号是PPO 扫雷游戏是9*9的，有10个地雷。每个打开的方块都用 0-8 之间的数字表示，表示地雷的数量。未打开的方块用 -1 表示。 我使用配备 RTX 3060（120 瓦）的笔记本电脑进行训练。   &amp; #32；由   提交/u/yzhjonathan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18z0hlj/how_fast_should_i_expect_the_agent_to_learn/</guid>
      <pubDate>Fri, 05 Jan 2024 06:48:09 GMT</pubDate>
    </item>
    <item>
      <title>使用世界模型进行基于梯度的规划</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18yy57s/gradientbased_planning_with_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.17227 摘要：  人工智能领域持久的挑战是控制系统以实现所需的行为。而对于由简单动力学方程控制的系统，线性二次调节（LQR）等方法在历史上已被证明是非常有效的，大多数现实世界的任务都需要通用的问题求解器，需要具有无法通过简单方程轻松描述的动力学的世界模型。因此，这些模型必须使用神经网络从数据中学习。大多数为视觉世界模型设计的模型预测控制（MPC）算法传统上都探索基于群体的无梯度优化方法，例如用于规划的交叉熵和模型预测路径积分（MPPI）。然而，我们提出了一种基于梯度的替代方案的探索，该替代方案充分利用了世界模型的可微性。在我们的研究中，我们对我们的方法和其他基于 MPC 的替代方案以及基于策略的算法进行了比较分析。在样本有效的设置中，与大多数任务中的替代方法相比，我们的方法取得了同等或更好的性能。此外，我们引入了一种结合了策略网络和基于梯度的 MPC 的混合模型，该模型的性能优于纯基于策略的方法，从而有望在复杂的现实世界任务中使用世界模型进行基于梯度的规划。  &lt; /div&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18yy57s/gradientbased_planning_with_world_models/</guid>
      <pubDate>Fri, 05 Jan 2024 04:38:57 GMT</pubDate>
    </item>
    <item>
      <title>优先重播缓冲区 - 真的有用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/</link>
      <description><![CDATA[您好，我有一个问题要问你们所有有实施和评估优先重播缓冲区经验的人。 我自己做了一个问题优先重放缓冲区的实现，并将其与双 dqn 实现进行比较。比较是在 pythongym 库的 Lunar Lander 环境中进行的。对于优先重放缓冲区的 alpha 和 beta 值，我使用 0.7（固定）和 0.4 作为初始 beta 值，该值在整个数字中线性变化到 1.0集数（即 4000）。 在我的比较中，double dqn 在大约 2700 集时完成训练（当它在最后 100 集中达到平均 230 累积奖励时），而使用优先级的训练重播缓冲区在大约 2800 到 2900 集时完成训练（同样，当它在最后 100 集达到平均 230 累积奖励时）。我尝试从 0.3、0.4 和其他值开始将 alpha 值线性移动到 1.0 ，但每次，它的性能最多与双 dqn 一样好。我期望在使用优先重播缓冲区进行训练时能够更快地达到接受标准（230 奖励的平均值），因为它应该提供对代理来说更有意义的体验样本（与以随机统一方式采样的正常重放缓冲区相反）。 因此，根据您的经验，您是否发现使用优先重放缓冲区与使用优先重放缓冲区相比有好处？正常重播缓冲区（并使用双 dqn）？您认为优先重播缓冲区在月球着陆器等环境中没有明显好的结果吗？ 任何提示、建议、意见（基于您的经验） ），想法或分享的经验非常有价值，欢迎并感谢。 干杯！   由   提交/u/kxy-yumkimil  /u/kxy-yumkimil reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/</guid>
      <pubDate>Thu, 04 Jan 2024 21:49:39 GMT</pubDate>
    </item>
    <item>
      <title>有关 PPO 奖励的快速信息</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ydc5s/quick_info_on_ppo_reward/</link>
      <description><![CDATA[我的奖励函数有 0.01 和 1 这样的常量 如果我将其设置为 0.1 和 10，会有什么不同吗？  我问这个问题是因为当我通过正缩放器（添加常数项）改变奖励函数时，它产生了影响。  &amp;# 32；由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ydc5s/quick_info_on_ppo_reward/</guid>
      <pubDate>Thu, 04 Jan 2024 13:36:07 GMT</pubDate>
    </item>
    <item>
      <title>“桥接离散和反向传播：直通和超越”，Liu 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18y0fee/bridging_discrete_and_backpropagation/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18y0fee/bridging_discrete_and_backpropagation/</guid>
      <pubDate>Thu, 04 Jan 2024 01:28:10 GMT</pubDate>
    </item>
    <item>
      <title>在 7 多万英里的纯骑手驾驶中，Waymo 的表现显着优于可比较的人类基准（Kusano 等人，2023 年）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xzuvh/waymo_significantly_outperforms_comparable_human/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xzuvh/waymo_significantly_outperforms_comparable_human/</guid>
      <pubDate>Thu, 04 Jan 2024 01:02:58 GMT</pubDate>
    </item>
    <item>
      <title>“PASTA：预训练的动作状态转换器代理”，Boige 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xymab/pasta_pretrained_actionstate_transformer_agents/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xymab/pasta_pretrained_actionstate_transformer_agents/</guid>
      <pubDate>Thu, 04 Jan 2024 00:09:46 GMT</pubDate>
    </item>
    <item>
      <title>持续强化学习的定义</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2307.11046 OpenReview：https:// /openreview.net/forum?id=ZZS9WEWYbD 摘要：  在强化学习问题的标准视图中，代理的目标是有效地确定最大化长期回报的策略。然而，这种观点是基于一种有限的观点，即学习是寻找解决方案，而不是将学习视为无休止的适应。相反，持续强化学习是指最好的智能体永远不会停止学习的环境。尽管持续强化学习很重要，但社区缺乏一个简单的问题定义来强调其承诺并使其主要概念准确清晰。为此，本文致力于仔细定义持续强化学习问题。我们将“永不停止学习”的代理概念正式化。通过一种新的数学语言来分析和编目代理。使用这种新语言，我们将持续学习代理定义为可以无限期地执行隐式搜索过程的代理，并将持续强化学习定义为最佳代理都是持续学习代理的设置。我们提供了两个激励性的例子，说明多任务强化学习和持续监督学习的传统观点是我们定义的特例。总的来说，这些定义和观点形式化了学习核心的许多直观概念，并开辟了围绕持续学习代理的新研究途径。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</guid>
      <pubDate>Wed, 03 Jan 2024 21:09:29 GMT</pubDate>
    </item>
    <item>
      <title>暑期实习在 RL/Embodied AI 领域处于领先地位。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xos1z/summer_internship_leads_in_rlembodied_ai_space/</link>
      <description><![CDATA[我正在寻找 2024 年暑期实习的一些线索，以寻找潜在的（如果可能的话研究）实习机会。 我不是非常了解在这个领域工作的所有初创公司/组织，因为我自己对它非常陌生。我没有太多经验，但希望在这些实习中获得一些经验。如果有任何帮助，我们将不胜感激！   由   提交 /u/gchhablani   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xos1z/summer_internship_leads_in_rlembodied_ai_space/</guid>
      <pubDate>Wed, 03 Jan 2024 17:33:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何获得研究想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xlwli/how_do_i_get_ideas_for_research/</link>
      <description><![CDATA[事情的简短版本：我是一名 CS 硕士生。我已经完成了一年半的尝试并努力进入 EAI 领域。我还有大约一年的时间，我想充分利用这一年。我有一些 NLP 专业背景，也有一些研究经验，但不是很深入。我了解监督学习，并且擅长 PyTorch 之类的东西。我目前面临两个主要问题：  我想发表一些第一作者的作品，但无法获得强大的创造力在我体内流淌。我该如何开发它？ 我想从语言理解方面攻击 RL 的各个方面，但各个方面的研究都在疯狂增长（LLM、VLM） ，RL，变形金刚），我很难理解什么该读，什么不该读。  来自该领域有经验的人的任何想法或提示类似问题或只是有足够的经验？ 如果这不是合适的论坛，请指出我可以讨论此问题的地方。 &lt;!-- SC_ON - -&gt;  由   提交 /u/gchhablani   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xlwli/how_do_i_get_ideas_for_research/</guid>
      <pubDate>Wed, 03 Jan 2024 15:24:28 GMT</pubDate>
    </item>
    </channel>
</rss>