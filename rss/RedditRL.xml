<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 20 May 2024 18:18:51 GMT</lastBuildDate>
    <item>
      <title>如何使用人类演示来预训练 RL 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</link>
      <description><![CDATA[我想要一个代码示例来了解如何执行此操作。有人可以帮助我吗？   由   提交 /u/SebyR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</guid>
      <pubDate>Mon, 20 May 2024 18:16:01 GMT</pubDate>
    </item>
    <item>
      <title>“移动 ALOHA：通过低成本全身远程操作学习双手移动操作”，Fu 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</guid>
      <pubDate>Mon, 20 May 2024 17:35:15 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的对抗性攻击和对抗性训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /对抗性强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</guid>
      <pubDate>Mon, 20 May 2024 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：解决强化学习 PyTorch 模型中的“mat1 和 mat2 形状无法相乘”错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwj968/help_needed_solving_mat1_and_mat2_shapes_cannot/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwj968/help_needed_solving_mat1_and_mat2_shapes_cannot/</guid>
      <pubDate>Mon, 20 May 2024 16:08:17 GMT</pubDate>
    </item>
    <item>
      <title>q 学习和井字游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</link>
      <description><![CDATA[tttrl - Pastebin.com KDT85/Tic-Tac-Toe-RL：使用强化学习的 Tic-Tac-Toe 游戏 (github.com)&lt; /a&gt; 上面是我使用强化学习的井字棋游戏的代码，你们觉得怎么样。  它似乎可以工作，但 q 表没有填满，这正常吗？其训练次数为 20000000 集。  GitHub 中包含 q 表作为 Excel 文件。  谢谢！    由   提交 /u/chinfuk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</guid>
      <pubDate>Mon, 20 May 2024 15:12:36 GMT</pubDate>
    </item>
    <item>
      <title>致力于为 Azul Board Game 开发成功的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</link>
      <description><![CDATA[大家好， 我目前正在做一个项目，旨在训练一个模型精通桌游 Azul 。我用 Python 编写了自己的游戏引擎，并且使用 SB3 PPO 实现。我的观察是一个由大约 700 位组成的二进制向量（这是整个游戏状态的二进制表示，如工厂、玩家板等），我的动作空间是 300 大小的向量，每个位对应于某个动作（在 azul 游戏中，每回合你必须做出 3 个决定：选择哪个工厂、什么颜色以及将其放置在哪里，并且你有 10 个工厂（我的模型正在玩 4 人游戏）、5 种颜色和 6 条图案线，因此你有 300 种可能我已经多次运行它，使用不同的激活函数、层大小、层数、学习率、伽马等，我尝试使用分数作为奖励，也尝试使用一些任意奖励，例如进行有效的移动等。 ，但我从未达到令人满意的结果，该模型从平均 -90 分开始，逐渐达到 -30/-20，但这仍然是非常糟糕的结果，因为 azul 中的负分意味着它做了很多废话平均球员每场得分超过 40/50 分，所以我与我的模型还相去甚远。  azul 环境的巨大问题是，在 300 个动作中，通常只有 40 个有效动作，因此这意味着只有大约 10-15% 的动作是有效的。首先，我的模型将以某种方式工作，它采取具有最高神经元值的操作，如果有效，则执行此操作，如果建议的操作无效（例如，模型想要从工厂 2 获取蓝色瓷砖，并将其放在模式行3中，但该工厂没有蓝色瓷砖），它将采取第二高神经元值动作并检查它是否有效等等，直到达到有效动作。  我最近的想法是对模型输出应用一些屏蔽，这样当模型观察棋盘并返回动作向量时，它会过滤所有无效动作，将它们的值设置为 0，然后然后我们只有有效动作的动作向量，第二个变化是我不选择最高值，而是将 softmax 应用于这些值并使用计算的概率随机选择它们。  不幸的是，我没有得到任何结果，该模型根本没有学习。你们对我可以在哪里改进我的算法、为什么我的模型无法学习等有什么建议吗？我将非常感谢所有的帮助，我也愿意为那些感兴趣的人分享代码，所以请告诉我，  请帮助我（：    提交者    /u/Different-Leave8202   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</guid>
      <pubDate>Mon, 20 May 2024 12:34:04 GMT</pubDate>
    </item>
    <item>
      <title>这里有导师吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwbtia/any_mentors_here/</link>
      <description><![CDATA[目前正在使用强化学习进行最终项目..是​​否有任何导师愿意帮助我？我不需要你坐在那儿教我，更需要你指导。我正在使用 q 学习算法开发自动驾驶模型   由   提交/u/amulli21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwbtia/any_mentors_here/</guid>
      <pubDate>Mon, 20 May 2024 09:59:40 GMT</pubDate>
    </item>
    <item>
      <title>有人真的部署了一个模型来用于推理吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</link>
      <description><![CDATA[我只是好奇有人实际上使用 RL 训练的策略来帮助解决哪些应用程序或控制问题，您用什么 Algo 进行训练？在此过程中遇到的主要挑战是什么？    由   提交 /u/Aggressive-Reach1657    reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</guid>
      <pubDate>Mon, 20 May 2024 07:01:20 GMT</pubDate>
    </item>
    <item>
      <title>“认识一下 Shakey：第一个电子人——拥有自己思想的机器的迷人而可怕的现实”，Darrach 1970</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</guid>
      <pubDate>Mon, 20 May 2024 01:39:13 GMT</pubDate>
    </item>
    <item>
      <title>有没有你非常喜欢的 RL 应用的近期例子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw1yk3/any_recent_examples_of_rl_applications_you_really/</link>
      <description><![CDATA[ 由   提交/u/paswut  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw1yk3/any_recent_examples_of_rl_applications_you_really/</guid>
      <pubDate>Sun, 19 May 2024 23:50:34 GMT</pubDate>
    </item>
    <item>
      <title>输入/输出关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw0ycm/inputoutput_relationships/</link>
      <description><![CDATA[社区您好， 假设我们有 N 个元素，每个元素都有一组特征（Xi1，Xi2）和任务DQN 的目的是选择其中一个元素（因此我们有 3 个输出）。假设输入向量为[X11, X21, X12, X22]，DQN如何将元素的每个输入特征与其对应的输出联系起来，例如即使存在遥远的距离，它如何理解X11和X12是元素1的特征? 我希望描述清楚   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw0ycm/inputoutput_relationships/</guid>
      <pubDate>Sun, 19 May 2024 23:02:22 GMT</pubDate>
    </item>
    <item>
      <title>RL 导师/专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</link>
      <description><![CDATA[我是一名本科生，目前正在完成论文。我接手了一个项目，该项目使用 RL 进行连续控制，通过 6d 位姿估计器来控制机器人。我看得很远，但强化学习机器人技术在我们国家可能还不够饱和。我试图寻找结构化的方法来学习这一点，就像使用 OpenAI 旋转强化学习以及 Sutton &amp; 的理论背景一样。巴托的书。我真的很渴望在明年之前完成这个项目，但我没有导师。甚至我们大学的教授也很快就会采用强化学习机器人技术。我从以前的帖子里看到，在这里请教导师是可以的，所以请原谅。如果我无法正确地提出问题，我深表歉意。 我想要实现这些目标： - 充分掌握 RL 基础知识，特别是在连续动作空间控制方面。 - 熟悉艾萨克·西姆。 - 了解如何为强化学习建模物理系统 - 将经过训练的模型部署到物理机器人 - 通过项目慢慢积累知识，最终引导我完成项目 - 寻找指导我完成整个工作流程的导师 &lt; p&gt;我所知道的： - 深度学习的背景 - RL 的基本原理（直到 MDP 和 TD） - RL 算法的背景 - DQN、DDPG、TD3 如何在高级抽象中工作 - 在高级抽象中体验重播缓冲区和 HER - ROS 2 基础知识 我想知道什么： - 我需要学习所有数学知识吗？或者我可以只参考现有的实现吗？ - 考虑到我的资源限制，我只能实现一个算法（我在第三世界国家），我应该使用它来实现完成项目的最大可能性。目前，我正在关注TD3。 - 一个本科生团队有可能完成这样的项目吗？ - 鉴于资源限制，我们应该使用哪个 Jetson 板来运行策略？ - 我们的目标是针对易碎处理进行优化，我们如何限制研究？ 我的努力我目前正在研究更多并建立关于算法和强化学习的直觉。最近，我迁移到 Ubuntu 并设置了模拟所需的所有软件和环境 (Isaac Sim)。 挫败感 在没有人交谈的情况下继续这个项目非常具有挑战性，因为每个人都几乎不感兴趣与RL。每个资源都有一个非常陡峭的学习曲线，当我认为我知道某些资源时，某些资源指向了我不知道的其他东西。我必须在明年之前完成这个工作，尽管我正在尽我所能地学习，但仍有很多我不知道的事情。    由   提交 /u/echialas22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</guid>
      <pubDate>Sun, 19 May 2024 21:04:03 GMT</pubDate>
    </item>
    <item>
      <title>再现“SOLVING THE OSHI-ZUMO GAME M. Buro 2004”的结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvy09r/reproducing_results_of_solving_the_oshizumo_game/</link>
      <description><![CDATA[嗨， 我想在 Oshi-Zumo 游戏上进行 Self-Play 或 Double Oracle（我省略了一些细节，但，这是一个两人的零和游戏，每个玩家都有一定数量的硬币，他们同时下注一定数量的硬币，当硬币“落在”敌人的一侧时。任何玩家，该玩家都输了）。 首先，我想我不妨通过使用线性规划而不是使用函数近似来实际解决游戏来获得纳什均衡策略。我遵循了 M. Buro 2004 年的论文 SOLVING THE OSHI-ZUMO GAME 并我使用了 open spiel python 库。 我成功了（经过长时间的调试）来解决 [50,3,1]-Oshi-Zumo 博弈，并在位置 (50,50,0) 处找到与纳什均衡策略完全相同的结果（论文第 5 段开头）。同样，我对位置 (6, 3, -3) 有完全相同的政策和预期回报（就在同一段落的下面）。但我对位置 (20, 32, 3) 没有相同的策略（他们的动作 17 18 等 20 的概率在我的策略中被“分组”，该策略选择只玩 20 - 其余的对于涉及较少的动作是相同的硬币）。但预期回报是相同的。我想，我可能刚刚达到了另一个纳什均衡，因为那个游戏中可能存在几个混合纳什均衡（实际上很有可能）。 然后，我做了我的策略反对随机代理和硬编码代理，如果他们有超过 2 个硬币，则玩 2，否则玩 1（受到该论文图 3 的启发）。我在对抗随机代理时并没有赢那么多，但在对抗硬编码代理时我赢了很多（他们实际上输了一点）。我还模拟了 200,000 场比赛。 我的问题是：我“刚刚”玩过吗？达到另一个纳什均衡，因此我不应该担心我无法重现他们所做的事情？ （他们提供的用于重现结果的链接不再可用）或者我是否在某个地方搞砸了？如果是这样，我该怎么做才能确保我确实搞砸了。如果我一直输给某种策略，那么我当然没有达到纳什均衡，但到目前为止情况并非如此。 附带的额外问题：他们说“在笔记本电脑上一个 1-GHzPentium-IIICPU，解决标准 [50,3,1] 游戏只需 12 秒”。我在我的机器上花了 2 分钟多。我没有最好的机器，但我认为我的 CPU 可能比他们的更好。这种差距可以用我使用 Python 而他们使用 C++ 来解释吗？虽然我使用 pulp 来解决我的线性问题，所以我想这已经通过调用编译代码进行了优化。我唯一合理的解释是，我的线性问题的解决方案很可能更精确，因为程序花了很长时间才能获得大量小数，对吗？ 我已附上我的所有内容colab 笔记本中的代码。 即使是也应该非常容易理解由不熟悉 open_spiel 库的人编写。   由   提交 /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvy09r/reproducing_results_of_solving_the_oshizumo_game/</guid>
      <pubDate>Sun, 19 May 2024 20:49:27 GMT</pubDate>
    </item>
    <item>
      <title>需要帮忙！！强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvr5pt/need_help_rl_project/</link>
      <description><![CDATA[我目前正在使用 Q 学习算法为大学做一个期末项目。这里有没有人非常精通并且可以帮助我   由   提交/u/amulli21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvr5pt/need_help_rl_project/</guid>
      <pubDate>Sun, 19 May 2024 15:42:15 GMT</pubDate>
    </item>
    <item>
      <title>开+RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</link>
      <description><![CDATA[KAN 擅长持续学习，有可能使 on-policy 变得鲁棒吗？    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</guid>
      <pubDate>Sun, 19 May 2024 13:15:45 GMT</pubDate>
    </item>
    </channel>
</rss>