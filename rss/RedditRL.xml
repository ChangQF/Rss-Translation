<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 25 Jun 2024 06:21:14 GMT</lastBuildDate>
    <item>
      <title>“探索大型语言模型中上下文学习的决策边界”，Zhao 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnud65/probing_the_decision_boundaries_of_incontext/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnud65/probing_the_decision_boundaries_of_incontext/</guid>
      <pubDate>Tue, 25 Jun 2024 01:41:05 GMT</pubDate>
    </item>
    <item>
      <title>“主题：人工智能反馈中的内在动机”，Klissarov 等人 2023 {FB}（Nethack 法学硕士的标签表明这是一种学习奖励）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntush/motif_intrinsic_motivation_from_artificial/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntush/motif_intrinsic_motivation_from_artificial/</guid>
      <pubDate>Tue, 25 Jun 2024 01:14:09 GMT</pubDate>
    </item>
    <item>
      <title>“神经语言代理的差异历史”，Piterbarg 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntmy1/diff_history_for_neural_language_agents_piterbarg/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntmy1/diff_history_for_neural_language_agents_piterbarg/</guid>
      <pubDate>Tue, 25 Jun 2024 01:02:47 GMT</pubDate>
    </item>
    <item>
      <title>“使用 LLM 玩 NetHack：作为零次元特工的潜力与局限性”，Jeurissen 等人 2024 (gpt-4-turbo)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntgl1/playing_nethack_with_llms_potential_limitations/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntgl1/playing_nethack_with_llms_potential_limitations/</guid>
      <pubDate>Tue, 25 Jun 2024 00:54:08 GMT</pubDate>
    </item>
    <item>
      <title>强化学习库在超参数搜索和优化方面的现状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnromj/the_current_state_of_rl_libraries_in_terms_of/</link>
      <description><![CDATA[目前有许多库和框架为 RL 提供基线和即插即用算法。  由于众所周知 RL 对超参数非常敏感，我想问 RL 社区，在您使用的库和框架中，它们在系统超参数搜索和优化方面的经验排名如何？哪一个为其提供了更好的界面和/或效率更高？或者即使您使用过任何第三方库。     提交人    /u/Human_Professional94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnromj/the_current_state_of_rl_libraries_in_terms_of/</guid>
      <pubDate>Mon, 24 Jun 2024 23:28:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 制作动态视频游戏 NPC [博客文章]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnr4lz/making_dynamic_video_game_npcs_with_ppo_blog_post/</link>
      <description><![CDATA[查看我的强化学习博客文章 https://macjgames.net/Blog_Posts/RL_Shooter_NPC.html 我下一步应该做什么样的项目？ ![video]( &quot;战斗学校！&quot;)    提交人    /u/theLanguageSprite   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnr4lz/making_dynamic_video_game_npcs_with_ppo_blog_post/</guid>
      <pubDate>Mon, 24 Jun 2024 23:02:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习课程项目帮助/建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnpj7g/machine_learning_course_project_helpadvice/</link>
      <description><![CDATA[大家好，我在大学的机器学习课程中需要做一个项目，在这个项目中，我将训练一个人工智能在教授给我们的 pybullet 模拟器中玩乒乓球，通过在我的代码中添加客户端，我可以调用 get_state() 从模拟器中获取 37 个状态值，通过 send_joints() 我可以发回构成玩游戏的机器人的 11 个关节部位的运动。在最终的比赛中，AI 必须能够在 uni pc 上运行，因此我无法导入已编码的模型库，这就是为什么我让 OpenAI 的 ddpg 实现启动起来（我尝试使用 ppo，但找不到不需要修改多个文件即可工作的模型，并且对于连续空间，我们在课堂上只讨论了 ddpg），我使用 gymnasium 通过在里面插入客户端来创建自定义环境（可能可以用它来训练 ai，但不能用它来运行 AI），我实现环境和 ai 的方式非常简单，获取状态，计算移动球拍以击中球的确切点，将点和我从 37 个状态变量中提取的值作为输入（我认为其中一些是 ai 不需要的）让他吐出关节并将它们发送到环境中。 我的奖励函数也很简单，对移动过多的 ai 进行惩罚，惩罚等于预测点和桨当前位置之间的距离，最后给予非常大的得分奖励。 最不确定的是奖励函数和超参数的选择。如果您想要查看我当前的代码，请访问我的 github https://github.com/MaxiMoraru/ddpg，我知道它很混乱，但重要的文件是 core.py（取自 openAI spinning up） ddpg.py（spinning up 实现，做了一些小改动，例如删除了记录器） custom_gym_env.py（我的环境使用 gym 制作，并解释了状态变量） start.bat（仅使用 1 个命令启动服务器和客户端）。 欢迎提出任何批评/建议，提前谢谢大家！ 编辑：忘记补充说，服务器每秒发送 50 次状态并期望关节返回，我得到大约 800集数/小时我应该期待多少小时的训练？    提交人    /u/Expensive_Internet17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnpj7g/machine_learning_course_project_helpadvice/</guid>
      <pubDate>Mon, 24 Jun 2024 21:53:31 GMT</pubDate>
    </item>
    <item>
      <title>在 ROS+Gazebo 中开始使用 DAgger</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</link>
      <description><![CDATA[大家好， 由于普通的强化学习未能满足我的期望，因此我希望转向模仿学习。这是一个基于操纵器的项目，带有眼手深度摄像头。我有一个可以成为专家的经典算法。所有这些都是在 ROS+gazebo 中完成的。 我该如何使用模仿学习来做到这一点。具体来说，我可以使用任何现成的库吗？我如何在 Gazebo 中保存来自我的专家的演示。如果有人以前这样做过    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</guid>
      <pubDate>Mon, 24 Jun 2024 21:03:55 GMT</pubDate>
    </item>
    <item>
      <title>这难道不是《IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO》这篇论文中的问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</link>
      <description><![CDATA[      我正在阅读这篇论文：“深度强化学习中的实施问题：PPO 和 TRPO 案例研究” [pdf 链接]。 我认为我对这篇论文的信息有疑问。看看这个表格： https://preview.redd.it/uaw20jf6fk8d1.png?width=1056&amp;format=png&amp;auto=webp&amp;s=e3c698529ec45dc4ad71b807f587572db2988dba 根据这个表格，作者认为 TRPO+ 即 TRPO 加上 PPO 的代码级优化优于 PPO。因此，这表明代码级优化比算法更重要。我的问题是，他们说他们对 TRPO+ 中打开和关闭代码级优化的所有可能组合进行网格搜索，而对于 PPO，则是将所有优化都打开。  我的问题是，通过进行网格搜索，他们给了 TRPO+ 更多的机会来获得一次良好的运行。我知道他们使用种子，但有 10 个种子。根据 Henderson 的说法，这还不够，因为即使我们做 10 个随机种子，将它们分组为两个 5 个种子并绘制奖励和标准差，我们也会得到完全分离的图，这表明方差太高，无法被 5 个种子或我猜甚至 10 个种子捕获。  因此，我不知道他们的论点在他们正在进行的网格搜索下如何成立。至少，他们也应该对 PPO 进行网格搜索。  我遗漏了什么？   由    /u/miladink  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</guid>
      <pubDate>Mon, 24 Jun 2024 18:53:12 GMT</pubDate>
    </item>
    <item>
      <title>“Rho-1：并非所有代币都是你所需要的”，Lin 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</guid>
      <pubDate>Mon, 24 Jun 2024 18:38:52 GMT</pubDate>
    </item>
    <item>
      <title>无模型 Stewart 平台</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</link>
      <description><![CDATA[        由    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</guid>
      <pubDate>Mon, 24 Jun 2024 18:35:11 GMT</pubDate>
    </item>
    <item>
      <title>这是 RL 的一个巧妙应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</link>
      <description><![CDATA[数据驱动的强化学习，实现洗衣机中的最佳电机控制    提交人    /u/Obsesdian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</guid>
      <pubDate>Mon, 24 Jun 2024 16:27:03 GMT</pubDate>
    </item>
    <item>
      <title>有人可以创建完整的 Pytorch -> Jax (编译器) 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnedvc/can_someone_create_full_pytorch_jax_compiler/</link>
      <description><![CDATA[嗨！ 我读到 JAX 适用于常量大小数组。假设我们有 torch 数组： self.states = torch.zeros((self.capacity, state_dim), dtype=torch.float32).to(device) self.actions = torch.zeros((self.capacity, action_dim), dtype=torch.float32).to(device) self.rewards = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device) self.next_states = torch.zeros((self.capacity, state_dim), dtype=torch.float32).to(device) self.dones = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device)  但对我来说，JAX 代码看起来像汇编程序，而不是直观友好的 Pytorch。我相信有大量用 Pytorch（或 Tensorflow）编写的库。是否可以创建完整的 Pytorch -&gt; JAX 编译器并运行代码，例如通过以下方式从编译器训练（及其依赖项）： sys.argv 这样我们就不需要像在 C 语言中那样转到 JAX 细节了。 PS: 当我们达到 Replay Buffer 最大值时，我们可以通过 roll 函数进行左移：  def add(self, state, action, reward, next_state, done): if self.length&lt;self.capacity: self.length += 1 idx = self.length-1 self.states[idx,:] = torch.FloatTensor(state).to(self.device) self.actions[idx,:] = torch.FloatTensor(action).to(self.device) self.rewards[idx,:] = torch.FloatTensor([reward]).to(self.device) self.next_states[idx,:] = torch.FloatTensor(next_state).to(self.device) self.dones[idx,:] = torch.FloatTensor([done]).to(self.device) if self.length==self.capacity: self.states = torch.roll(self.states, shifts=-1, dims=0) self.actions = torch.roll(self.actions, shifts=-1, dims=0) self.rewards = torch.roll(self.rewards, shifts=-1, dims=0) self.next_states = torch.roll(self.next_states, shifts=-1, dims=0) self.dones = torch.roll(self.dones, shifts=-1, dims=0)     提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnedvc/can_someone_create_full_pytorch_jax_compiler/</guid>
      <pubDate>Mon, 24 Jun 2024 14:09:15 GMT</pubDate>
    </item>
    <item>
      <title>问题：朴素 GA 没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dn3g8u/question_naive_ga_not_learning/</link>
      <description><![CDATA[      你们知道为什么我的蛇没有学习吗？它占据了整个 8x8（64）游戏板，并输出一个动作（4 个神经元）。 我正在做的是：1. 初始化随机基础网络（a）2. 初始化随机突变网络（b）3. 对于每个并发模拟，通过（simulation_index / total_simulations）[因子] 合并 a 和 b 这很幼稚，因为没有交叉。 它在某种程度上有效，但是当蛇长大时它就停止学习了。 有人可以解释一下吗？    提交人    /u/mguinhos   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dn3g8u/question_naive_ga_not_learning/</guid>
      <pubDate>Mon, 24 Jun 2024 02:59:37 GMT</pubDate>
    </item>
    <item>
      <title>编程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</link>
      <description><![CDATA[        提交人    /u/chagdubbish   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</guid>
      <pubDate>Sun, 23 Jun 2024 20:06:01 GMT</pubDate>
    </item>
    </channel>
</rss>