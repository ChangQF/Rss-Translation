<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 27 Jan 2025 21:15:58 GMT</lastBuildDate>
    <item>
      <title>“在大型地下采矿环境中部署空中多智能体系统以实现自动任务执行”，Dhalquist 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibebim/deployment_of_an_aerial_multiagent_system_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibebim/deployment_of_an_aerial_multiagent_system_for/</guid>
      <pubDate>Mon, 27 Jan 2025 17:14:35 GMT</pubDate>
    </item>
    <item>
      <title>GRPO 可以用于多圈 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.03300 有些人可能已经看到了 PPO 的 RL 替代方案，即组相对策略优化 (GRPO)，其中不是训练价值模型，而是多次采样策略，获得平均奖励，并使用它来找出优势。 从审查实现来看，对话中只有一个回合，因为 LLM 要么正确解决数学问题，要么失败，所以在这种情况下奖励和价值是相同的，因为预期的未来奖励就是奖励。 GRPO 是否可以应用于多回合 RL 或更长远的项目，其中策略与环境多次交互？   提交者    /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</guid>
      <pubDate>Mon, 27 Jan 2025 16:05:02 GMT</pubDate>
    </item>
    <item>
      <title>钟摆政策不会学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ib4iem/pendulum_policy_doesnt_learn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ib4iem/pendulum_policy_doesnt_learn/</guid>
      <pubDate>Mon, 27 Jan 2025 09:35:57 GMT</pubDate>
    </item>
    <item>
      <title>我的系统是否真的需要 RL 模型，或者检测模型是否就足够了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ib04za/do_i_really_need_an_rl_model_for_my_system_or/</link>
      <description><![CDATA[大家好，希望你们一切顺利 我正在做一个项目，目标是确定何时在无线传感器网络中执行密钥刷新。一般的想法是识别节点中的异常行为（如受损或故障节点），然后决定是否需要密钥刷新。 密钥刷新需要大量资源，因此过于频繁地执行密钥刷新是一种浪费，但如果您不及时执行密钥刷新，您的网络将变得岌岌可危。 现在，我决定使用 RL 模型来做出这个决定，但我一直在质疑 RL 是否真的有必要，或者更简单的检测模型是否足够（然而检测传感器节点受损攻击非常困难）？特别是在这个子版块的一篇文章中，有人指出，确实可以使用简单的监督轻量级模型而不是 RL 模型来解决许多问题。 提前感谢您的建议！我很乐意回答任何问题。 PS：我只是一名计算机科学专业的学生，​​所以我对 RL 的了解有限，我发现它是最难理解的 ML 模型    提交人    /u/Sufficient-Lie-1632   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ib04za/do_i_really_need_an_rl_model_for_my_system_or/</guid>
      <pubDate>Mon, 27 Jan 2025 04:48:19 GMT</pubDate>
    </item>
    <item>
      <title>旧的 RL 课程仍然有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</link>
      <description><![CDATA[大家好。我想知道我应该从哪门课程开始学习 RL。我想从 2024 年的斯坦福 234 课程开始，但我不知道它是否教授基础知识。我还听说 David Silver 的课程很棒，但它是近 10 年前的，我不知道我应该从哪门课程开始。 TL;DR 开始学习 RL 的最佳课程是什么？    提交人    /u/madcraft256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</guid>
      <pubDate>Mon, 27 Jan 2025 04:31:32 GMT</pubDate>
    </item>
    <item>
      <title>4-7 年前的 PyTorch 代码能运行吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iahkrx/will_pytorch_code_from_47_years_ago_run/</link>
      <description><![CDATA[我发现很多 RL repos 上次更新是 4 到 7 年前，比如这个： https://github.com/Coac/never-give-up PyTorch 在过去几年里有过很多重大变化吗？修复旧代码以再次运行有多困难？    提交人    /u/exploring_stuff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iahkrx/will_pytorch_code_from_47_years_ago_run/</guid>
      <pubDate>Sun, 26 Jan 2025 15:38:11 GMT</pubDate>
    </item>
    <item>
      <title>Ray 2.40 上的 PBT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</link>
      <description><![CDATA[有人熟悉在 Ray 2.4 上做 PBT 吗？ 如果有人知道如何解决这个问题，我们将不胜感激： https://discuss.ray.io/t/metric-for-pbt-in-ray-2-40/21619 摘要：我想基于评估情节奖励均值指标使用 PBT 对 PPO 执行超参数优化，但我似乎无法使用该指标或任何有用的指标进行训练。    提交人    /u/nukelius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</guid>
      <pubDate>Sun, 26 Jan 2025 10:48:27 GMT</pubDate>
    </item>
    <item>
      <title>寻求与 RL 研究人员的合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</link>
      <description><![CDATA[大家好， 我是 UIUC 的计算机科学博士生，具有理论算法背景（在 SODA/ICALP/ESA 发表过文章；主要是近似算法、图问题的可扩展算法、在线算法等）。最近，我将重点转向 使用强化学习 (RL) 解决 NP 难图问题，我正在寻找有相同兴趣的合作者。 关于我的工作：  在理论会议（SODA、ESA）和 ML 会议（NeurIPS）上发表过文章。 最近开发了一种基于 RL 的 NP 难图问题方法，包括从头开始在 PyTorch 中编写自定义 GNN 框架。论文已提交给 ICML。 坚实的理论基础 + 良好的编码能力，旨在将理论与实践相结合。  寻找： 对将 RL 与图算法/组合优化问题相结合感兴趣的研究人员，尤其是以下人员：  研究 NP 难图问题（例如，TSP、顶点覆盖、图分区）。 关心为什么学习策略有效（例如，理论保证、泛化分析）。 想要构建既有原则又有实践效率的方法。  如果这与您的工作或兴趣重叠，请随时给我发私信！我很乐意分享我的论文草稿，讨论想法或探索合作。 （使用一次性帐户进行匿名，但可以通过电子邮件/ LinkedIn 进行验证。）    提交人    /u/ForAllEpsilonExists   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</guid>
      <pubDate>Sun, 26 Jan 2025 07:00:09 GMT</pubDate>
    </item>
    <item>
      <title>构建定制的机械臂环境并训练 AI 代理来控制它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</link>
      <description><![CDATA[        由    /u/Fabulous-Extension76  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</guid>
      <pubDate>Sun, 26 Jan 2025 03:59:44 GMT</pubDate>
    </item>
    <item>
      <title>特征选择/状态抽象方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</link>
      <description><![CDATA[大家好，有谁知道有什么论文/作品，其中代理具有非常高维的状态空间，并且可以通过某种方式减小尺寸？有没有什么常用的方法可以为代理选择最佳特征？    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</guid>
      <pubDate>Sat, 25 Jan 2025 23:25:36 GMT</pubDate>
    </item>
    <item>
      <title>“DeepSeek-R1：通过强化学习激励法学硕士中的推理能力”，Guo 等人 2025 {DeepSeek}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</guid>
      <pubDate>Sat, 25 Jan 2025 22:52:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Collab 中安装 MARLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</link>
      <description><![CDATA[我按照说明在 Collab 中安装 MARLib： https://marllib.readthedocs.io/en/latest/ conda create -n marllib python=3.8 conda activate marllib git clone cd MARLlib pip install --upgrade pip pip install -r requirements.txt # 我们推荐 gym 版本在 0.20.0~0.22.0 之间。 pip install gym&gt;=0.20.0,&lt;0.22.0 # 将补丁文件添加到 MARLlib python patch/add_patch.py​​ -yhttps://github.com/Replicable-MARL/MARLlib.git  要求安装到 ray 1.8.0，找不到该版本（我也尝试过 1.13 但找不到）。 删除版本会导致更多错误和更多不兼容性。总是显示相同的消息： 错误：subprocess-exited-with-error 当安装没有特定版本的所有内容时，调用 marl.algos.mappo 时，它会抛出： ModuleNotFoundError：没有名为“ray.rllib.agents”的模块 有人可以为我提供安装 MARLlib 的更新说明并且没有不兼容性吗？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</guid>
      <pubDate>Sat, 25 Jan 2025 22:33:04 GMT</pubDate>
    </item>
    <item>
      <title>文本推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</link>
      <description><![CDATA[大家好，我想知道你们有没有推荐的教科书，无论是在线的还是数字的，这些教科书可以从高层次深入到强化学习领域。就背景而言，我拥有电气硕士学位，并且做过相当多的机器学习工作，但我在强化学习方面做得最先进的是 cuda 中的批量 Q 学习。我甚至从未实现过自己的深度 Q 学习算法。希望能找到一些数学密集型的问题。主要关注机器人技术和寻路，但愿意看任何东西。    提交人    /u/Tassadon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</guid>
      <pubDate>Sat, 25 Jan 2025 01:57:22 GMT</pubDate>
    </item>
    <item>
      <title>对 5090 / GTC 2025 的思考</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</link>
      <description><![CDATA[有人对培训代理的 5090 感到兴奋吗？有什么特别的理由吗？ 此外，如果有人要去，廉价的边疆航班将让我今年第二次参加 GTC。很想喝点东西。去年我玩得很开心，将在周日参加其中一次培训，然后周二离开。    提交人    /u/ParamedicFabulous345   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</guid>
      <pubDate>Fri, 24 Jan 2025 21:36:59 GMT</pubDate>
    </item>
    <item>
      <title>仍然不太漂亮，但奖励函数略好一些</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</link>
      <description><![CDATA[       由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</guid>
      <pubDate>Fri, 24 Jan 2025 15:15:20 GMT</pubDate>
    </item>
    </channel>
</rss>