<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 03 Sep 2024 21:14:47 GMT</lastBuildDate>
    <item>
      <title>数据质量对训练模仿学习模型的影响：使用 Aloha Kit 进行实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</link>
      <description><![CDATA[        由    /u/Trossen_Robotics  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</guid>
      <pubDate>Tue, 03 Sep 2024 20:45:06 GMT</pubDate>
    </item>
    <item>
      <title>在 cudnn 中使用 Pytorch 寻找计算二阶导数的更好方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</link>
      <description><![CDATA[嗨，我试图将安全算法：CPO 应用于我当前的 RL 代码。我使用 pytorch 和 GPU（cuda）运行我的代码。当计算 CPO 中的二阶导数时，cuda 会自动使用 cudnn 进行此类计算。问题是，由于 CuDNN API 的限制，CuDNN RNN 不支持 Double Backing。我也在 stackoverflow 中描述了这个问题，但没有得到答案。而且当我搜索时，这个错误， NotImplementedError：&#39;_cudnn_rnn_backward&#39; 的导数未实现。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs)NotImplementedError: 未实现“_cudnn_rnn_backward”的导数。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs) 在 RNN 和相关模型中的双重向后计算中更常见。如果你们知道解决方案，那将会很有帮助   由    /u/dAmiBouY539  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</guid>
      <pubDate>Tue, 03 Sep 2024 20:10:56 GMT</pubDate>
    </item>
    <item>
      <title>分享我的玩具项目“JAxtar”，用于解谜的纯 jax 和 jittable A* 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</link>
      <description><![CDATA[嗨，我想介绍一下我的玩具项目 JAxtar。 我将其发布到 r/reinforcementlearning 这里以及 r/JAX，因为它与典型的 RL 不同，但它是为使用神经启发式算法的 RL 编写的，例如 DeepCubeA，我计划朝这个方向改进它。 这不是很多人会觉得有用的代码，但我在编写它时用 Jax 完成了大部分杂技，我认为它可能会激励其他人使用 Jax 的人。 我的硕士论文是关于使用 RL 进行 A* 和神经启发式训练以解决 15 个难题，但当我反思它时，最大的头痛是 CPU 和 GPU 之间数据传输的频率高且长度长。几乎一半的执行时间都花在这些通信瓶颈上。这个问题的另一种解决方案是 DeepCubeA 提出的分批 A*，但我觉得这不是一个完整的解决方案。 有一天，我偶然发现了 mctx，这是 google deepmind 用纯 jax 编写的 mcts 库。 我对这种方法很着迷，并多次尝试用 Jax 编写 A*，但都没有成功。问题在于哈希表和优先级队列。 毕业后经过很长一段时间，研究了许多例子，绞尽脑汁，我终于设法编写了一些可用的代码。 我很自豪地说，此代码有几个特殊元素  用于将定义的状态转换为哈希键的 hash_func_builder 用于并行查找和插入的哈希表 可以批处理、推送和弹出的优先级队列 用于拼图的完全 jitted A* 算法。  我希望这个项目可以成为任何喜欢 Jax 和这种类型的 RL（带启发式的 A*）的人的鼓舞人心的例子    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</guid>
      <pubDate>Tue, 03 Sep 2024 14:50:19 GMT</pubDate>
    </item>
    <item>
      <title>作为输出的动作向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</link>
      <description><![CDATA[大家好，我计划在即将开展的项目中利用强化学习。在这种情况下，我的输入（或状态）将由两个矩阵组成：一个表示车辆的位置，另一个表示车辆的速度。输出或动作将是一个具有四个连续元素的向量，每个元素的范围在 0 到 1 之间。例如，一个步骤之后的一个可能动作可能是 [0.51, 0.76, 0.9, 0.12]。 有人能建议哪种强化学习算法最适合这种类型的问题吗？提前感谢您的帮助！    提交人    /u/muttahirulislam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</guid>
      <pubDate>Tue, 03 Sep 2024 13:38:02 GMT</pubDate>
    </item>
    <item>
      <title>随情节变化动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</link>
      <description><![CDATA[当动作空间本身随着情节而变化时，开关策略算法的预期行为是什么。这会导致非平稳性？ 动作空间是连续的。在 Mujoco Ant Cheetah 等中，典型情况是它代表扭矩。假设在一集中动作空间是 [1, -1] 下一集是 [1.2, -0.8] 下一集是 [1.4, -0.6] ... ... 未来的某一集是 [2, 0] .. 动作空间范围的变化由某些函数控制，并且在每个情节开始之前都会随着情节而变化。像 ppo trpo ddpg sac td3 这样的算法的预期行为应该是什么？他们能够处理吗？对于像 mappo maddpg matrpo matd3 等 marl 算法也有类似的问题。 这是由于动态变化而导致的非平稳性吗？是否存在任何无效的操作范围。我们可以将总体范围限制为某个高低值，但范围会随着情节而改变。    提交人    /u/Intrepid_Discount_67   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</guid>
      <pubDate>Tue, 03 Sep 2024 12:24:25 GMT</pubDate>
    </item>
    <item>
      <title>深度 CFR 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</link>
      <description><![CDATA[我从原始 Deep CFR 文章中获取了代码 链接：https://arxiv.org/pdf/1811.00164 我现在正在为我的游戏实现 Deep CFR 算法，当我编写完整算法时，我遇到了很多错误。所以我的问题是，本文末尾的代码是否完全正确，错误是否在我的代码中？如果有人已经为他们自己的任务实现了 Deep CFR 并且可以分享提示/代码，那就太好了 如果有人愿意提供帮助，我可以上传我的实现。它需要最终确定，而且我没有足够的经验来了解如何正确实现它。 这是我的代码： nn.py https://codeshare.io/KWByyK memory.py https://codeshare.io/obp99r deep_cfr.py https://codeshare.io/k0zAAA game_tree.py https://codeshare.io/mP6MMp utils.py https://codeshare.io/ldQppd 对于游戏，我从这个库中获取了 texasholdem：https://github.com/SirRender00/texasholdem。    提交人    /u/silenthnowakeup   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</guid>
      <pubDate>Tue, 03 Sep 2024 01:11:53 GMT</pubDate>
    </item>
    <item>
      <title>“运动物理学”及其对人类模仿学习的启示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</guid>
      <pubDate>Mon, 02 Sep 2024 21:09:17 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习项目征集及创新建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f78wnt/request_for_deep_reinforcement_learning_projects/</link>
      <description><![CDATA[基本上就是标题！ 您能推荐一些带有代码的深度强化学习项目吗？此外，如果您能让我知道是否有可能添加新颖性或更改代码，我将不胜感激。谢谢！    提交人    /u/Sweet_Speed9010   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f78wnt/request_for_deep_reinforcement_learning_projects/</guid>
      <pubDate>Mon, 02 Sep 2024 15:01:36 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Gymnasium 创建自定义环境来训练 CloudSimPlus 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</link>
      <description><![CDATA[基本上，我从基于 Java 的模拟器（CloudSimPlus）中提取指标，并使用 ProcessBuilder 将其发送到 py 脚本，该脚本反过来应该做出响应，决定如何更改云中的基础设施 我想要构建一个环境，其中状态表示为 5 个单独的离散值，并使用 3 种可能的操作训练代理 我已经按照这里的建议尝试了一个基本版本，但现在有点困惑，因为它没有按照我的意图工作    提交人    /u/Automatic_You_1939   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</guid>
      <pubDate>Mon, 02 Sep 2024 08:57:14 GMT</pubDate>
    </item>
    <item>
      <title>政策绩效分解证明问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f6hgtt/problem_with_proof_of_decomposition_of_policy/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f6hgtt/problem_with_proof_of_decomposition_of_policy/</guid>
      <pubDate>Sun, 01 Sep 2024 15:42:04 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的元学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</link>
      <description><![CDATA[您好，RL 中的大多数元学习似乎已应用于策略空间，而很少应用于 DQN 中的价值空间。我想知道为什么如此注重将策略适应新任务，而不是将价值网络适应新任务。Meta Q Learning 论文似乎是唯一一篇使用 Q 网络进行元学习的论文。这是真的吗？如果是，为什么？    提交人    /u/Sea-Collection-8844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</guid>
      <pubDate>Sun, 01 Sep 2024 07:22:46 GMT</pubDate>
    </item>
    <item>
      <title>寻找一个可供人类和代理合作完成任务的环境，其中存在多种可能的策略/子任务。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f64qvt/looking_for_an_environment_for_a_human_and_agent/</link>
      <description><![CDATA[大家好。我正在计划一个硕士研究项目，重点关注人类和 RL 代理如何协调共同完成任务。我正在寻找一个相对简单（理想情况下是 2D 和离散的）但仍允许团队采用不同高级策略的游戏式环境。这很重要，因为我的大多数潜在研究主题都集中在人类代理团队如何协调选择然后执行该高级策略。 到目前为止，Overcooked 环境 是我见过的最有前途的。在这种情况下，不同的高级策略可能是 (1) 拾取食材、(2) 烹饪食材、(3) 交付订单、(4) 丢弃垃圾。但所有这些策略都非常简单，所以我希望有更多选择。例如，在游戏中，代理可以决定是否收集资源、攻击敌人、治愈、探索地图等。任何建议都绝对值得赞赏。    提交人    /u/chowder138   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f64qvt/looking_for_an_environment_for_a_human_and_agent/</guid>
      <pubDate>Sun, 01 Sep 2024 03:09:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TD(0) 和 TD(lambda) 具有相同的计算成本？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5qy84/why_td0_and_tdlambda_have_same_computational_cost/</link>
      <description><![CDATA[我在 YouTube 上学习了 TD 值估计，它说 TD(0) 和 TD(lambda) 具有相同的计算成本，但这对我来说没有意义。如果你做 TD(lambda)，你需要通过状态存储每个奖励来估计起始状态的值，不是吗？甚至你需要计算它们与 lambda 相乘并求和。谢谢你的友好回答。    提交人    /u/Latter-Tomorrow-6850   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5qy84/why_td0_and_tdlambda_have_same_computational_cost/</guid>
      <pubDate>Sat, 31 Aug 2024 16:14:07 GMT</pubDate>
    </item>
    <item>
      <title>MARL 与分层 RL 与经典 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5l9zl/marl_vs_hierarchical_vs_classic_rl/</link>
      <description><![CDATA[大家好， 我的论文已经进行了近 2 年，并且很长时间以来一直存在一个问题。 在我们的问题中，环境由多种工具组成，可以对其进行操纵以实现所需的解决方案。每个工具都被分成多个参数。奖励函数是与系统期望输出的距离。 可以通过添加不同工具（相同或不同类型的现有工具）来更改环境。您可以拥有同一类型工具的多个实例。 我们尝试的第一个实现是使用 REINFORCE 和 A2C 同时操作所有参数的单个代理。这些实现产生了良好的结果。 然后，我们尝试使用 OptionCritic 的分层方法，其中每个选项都是一个代理，这是一种封建模型。这种方法优于之前的方法， 我们继续尝试添加不同类型的工具，并且能够解决每个类型都有 1 个不同工具的问题。我们使用 OptionCritic 算法解决了这个问题。 后来我们改变了环境，让其拥有 2 个相同类型的工具，然后一切都停止了工作。 这让我们尝试回到经典算法，但这感觉不对。我试图向我的导师解释我的疑惑，但他拒绝听我说话。 所有这些背景现在都引出了我的问题。  您什么时候将不同的动作视为不同的代理，或者视为同一个代理同时做出 1 个聚合动作？ 您什么时候尝试用 MARL / Hierarchical / Classic RL 解决问题？  谢谢大家， 一个非常疲惫的硕士生    提交人    /u/sagivborn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5l9zl/marl_vs_hierarchical_vs_classic_rl/</guid>
      <pubDate>Sat, 31 Aug 2024 11:39:41 GMT</pubDate>
    </item>
    <item>
      <title>在类似青蛙游戏中训练 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f5gvpj/training_a_dqn_in_a_froggerlike_game/</link>
      <description><![CDATA[      嘿！我目前正在开发一款类似青蛙过河的游戏，以便更多地了解 DQN，但我偶然发现了几个我不知道如何解决的问题。 因此，本质上，玩家在网格中移动。每个动作（上、下、左、右）都会使其移动一个网格单元。还有不执行任何操作的停留动作。世界上的其他一切都不是网格 - 汽车和火车以恒定的速度移动。目标是让玩家尽快到达终点。 因此，我面临的第一个问题是如何训练网络。我读过/看过的每个教程都让代理在每一帧中都采取一个动作。就我而言，让代理每帧都采取一个动作可能不是一个好主意（我认为是这样）。这是因为它会移动得太快，当我让训练过的模型以正常速度播放时，它的知识可能不适合较低的速度。因此，我现在正在做的是每 200 毫秒执行一次操作（游戏以 60 fps 运行，但没有任何依赖此操作）。但是，我觉得这对训练有重大影响。例如，如果玩家采取的行动导致它在 50 毫秒后死亡，它应该得到负面奖励，但这种奖励不会与任何行动相关联。有没有常见的方法来处理它？我读过关于跳帧的文章，但并不完全确定这是这里的解决方案。 第二个问题与实际训练有关。现在，当我在代理采取行动时（或当它死亡时，我在小批量上训练它）训练代理时，它通常要么学会尽可能快地跑起来，而不会避开任何汽车，要么学会不惜一切代价避开汽车，几乎根本不上去。我尝试过使用超参数和奖励，但老实说，我觉得我并不完全了解结果以及它们发生的原因。我尝试增加学习率和探索率，尝试使用更大更复杂的网络，尝试改变折扣因子，尝试了很多很多奖励组合，但似乎都不起作用。 我表示状态的方式如下： - 玩家 X 和 Y - 火车活跃 - 火车 Y - X 和 Y 轴上最近的汽车距离 - 最近的汽车方向和速度 - X 和 Y 轴上第二最近的汽车距离 - 第二最近的汽车方向和速度 - X 和 Y 轴上第三最近的汽车距离 - 第三最近的汽车方向和速度 - 玩家是否处于危险区域 - 玩家是否离汽车太近 至于奖励，我尝试对上升给予大额奖励，尤其是达到新的最高限制。我尝试过对倒下和留在原地进行惩罚，对获胜给予大量奖励，对死亡给予大量惩罚，但无论如何，它都无法正确学习。 如果你们有任何建议，我将不胜感激！谢谢 :)    提交人    /u/LielAmar   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f5gvpj/training_a_dqn_in_a_froggerlike_game/</guid>
      <pubDate>Sat, 31 Aug 2024 06:29:23 GMT</pubDate>
    </item>
    </channel>
</rss>