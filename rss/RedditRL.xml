<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 25 Jan 2024 15:15:13 GMT</lastBuildDate>
    <item>
      <title>使用 RL 进行自主四轴飞行器仿真 - 需要路线图建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19fbeld/autonomous_quadcopter_simulation_with_rl_need/</link>
      <description><![CDATA[大家好， 完成了 Andrew Ng 的机器学习课程，并对强化学习感到兴奋。发现了 Airsim 飞行模拟器，并想使用 RL 构建我自己的自主四轴飞行器。谁能分享一个简单的路线图来帮助我实现这一目标？ 非常感谢！   由   提交/u/Double_Inspection_88   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19fbeld/autonomous_quadcopter_simulation_with_rl_need/</guid>
      <pubDate>Thu, 25 Jan 2024 15:06:12 GMT</pubDate>
    </item>
    <item>
      <title>涉及概率论的强化学习研究领域。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19fadt1/research_areas_in_rl_that_involves_probability/</link>
      <description><![CDATA[嗨。我正在攻读统计学硕士学位，我论文的最初想法是在随机环境中进行随机游走。但在开始对这个领域进行更多研究后，我最终认为我不太喜欢它，所以我开始寻找其他领域。自从 12 月开始我的 RL 之旅以来，我学习了 DeepMind 课程和 Sutton 书中的大部分章节。现在我非常渴望将我的论文改为涉及强化学习的内容，我最感兴趣的主题是MultiAgent-RL。我和我的顾问谈过，他对这个变化非常怀疑，他担心现在的强化学习主要围绕深度学习，这是一个他没有太多经验的主题，而且因为我才刚刚开始学习，他认为我将无法找到一个特定的主题来工作。考虑到这一点，我想知道是否有人可以参考 RL 中本质上涉及概率论的文章或特定主题。    由   提交 /u/VanBloot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19fadt1/research_areas_in_rl_that_involves_probability/</guid>
      <pubDate>Thu, 25 Jan 2024 14:19:25 GMT</pubDate>
    </item>
    <item>
      <title>现在使用软 Q 学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f8c03/is_soft_qlearning_used_today/</link>
      <description><![CDATA[你好， 我是强化学习学科的新手，目前正在研究不同的 RL 算法。我发现软 Q 学习算法对于具有连续动作空间的代理很有吸引力，因为与大多数其他 RL 算法相比，代理的策略不是由单峰高斯参数化的。  多模式功能使其能够同时探索多种解决方案。我认为其他算法可以收敛到局部最小值。我认为这个想法有可能更多地探索解决方案空间，从而找到更好的（全球？）解决方案。  现在我有一种感觉，与 SAC 或 PPO 等其他算法相比，软 Q 学习现在并不真正流行。这是正确的观察吗？为什么是这样？这与不稳定的训练有关系吗？我无法找到有关此主题的大量信息。 ​ 谢谢！   由   提交/u/DependentSecurity987   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f8c03/is_soft_qlearning_used_today/</guid>
      <pubDate>Thu, 25 Jan 2024 12:35:20 GMT</pubDate>
    </item>
    <item>
      <title>构建数据科学应用程序 - Scikit Learn 的 Gael Varoquaux 创始人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f7523/building_data_science_applications_gael_varoquaux/</link>
      <description><![CDATA[    /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f7523/building_data_science_applications_gael_varoquaux/</guid>
      <pubDate>Thu, 25 Jan 2024 11:22:22 GMT</pubDate>
    </item>
    <item>
      <title>学习在线课程，掌握职场技能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f3m9g/learning_mcts/</link>
      <description><![CDATA[您好，我对 MCTS 强化学习领域的工作非常感兴趣。我知道有些算法使用某种神经指导来解决 alphazero 和 muzero 等问题。我对此有几个问题。 了解 mcts 及其变体的最佳方法是什么？哪些算法首先出现，哪些算法较之前有所改进？ MCTS 在最近的过去有多重要，未来会有更多发展吗？   由   提交/u/anonymous1084  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f3m9g/learning_mcts/</guid>
      <pubDate>Thu, 25 Jan 2024 07:10:52 GMT</pubDate>
    </item>
    <item>
      <title>DQN 论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f2z5q/dqn_papers/</link>
      <description><![CDATA[我目前正在做最后一年的研究项目，标题为“使用 DRL 进行股票交易”。这是讲师提出的标题。我对 DRL 完全陌生，但我计划使用 DQN，因为它显然是最容易实现的。问题是，我对 DQN 也很困惑，我不知道如何解释这些理论和概念。有谁知道有什么期刊论文可以很好地解释 DQN 并且易于理解吗？   由   提交/u/cookiesandcream30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f2z5q/dqn_papers/</guid>
      <pubDate>Thu, 25 Jan 2024 06:28:50 GMT</pubDate>
    </item>
    <item>
      <title>使用基于模型的轨迹优化解决稀疏奖励强化学习问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19erddl/solving_sparsereward_rl_problems_with_modelbased/</link>
      <description><![CDATA[​ DTC：深度跟踪控制 你好。我们是机器人系统实验室（RSL），我们研究控制腿式机器人的新策略。在我们最近的工作中，我们将轨迹优化与强化学习相结合，以合成准确且稳健的运动行为。 您可以在此处找到 ArXiv 打印：https://arxiv.org/abs/2309.15462 此方法进一步描述视频。 我们还在这个 视频。   由   提交 /u/leggedrobotics   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19erddl/solving_sparsereward_rl_problems_with_modelbased/</guid>
      <pubDate>Wed, 24 Jan 2024 21:09:05 GMT</pubDate>
    </item>
    <item>
      <title>需要对 DRL 中的 RNN 进行一些健全性检查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ecxyc/need_some_sanity_check_on_rnns_in_drl/</link>
      <description><![CDATA[嘿。  您通常如何使用单模型多代理 RNN DRL 处理隐藏状态？我正在考虑： -将隐藏状态从网络中拉出并保留在周围每次我的策略想要执行另一个步骤时进行植入。 -保留之前观察的历史记录，并为未来的每个步骤重新运行这些虚拟体验，以使隐藏状态达到应有的位置。  我认为拉动隐藏状态并缓存它是更好的方法，因为我不必进行前向传递来恢复它。  对于反向传播，这适用于 PPO，因为我默认对整个剧集进行采样，但不适用于 DQN。我想我应该修改它以采样整个剧集？  然后我还必须注意批处理和重置隐藏状态。  天啊，人们已经开始觉得 RNN 不应该属于 DRL。    由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ecxyc/need_some_sanity_check_on_rnns_in_drl/</guid>
      <pubDate>Wed, 24 Jan 2024 09:06:26 GMT</pubDate>
    </item>
    <item>
      <title>在 PPO 中梯度是否流过熵项？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ecedk/in_ppo_do_gradients_flow_through_the_entropy_term/</link>
      <description><![CDATA[根据标题，添加熵损失时是否通过策略参数进行反向传播？   由   提交/u/Conscious_Heron_9133   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ecedk/in_ppo_do_gradients_flow_through_the_entropy_term/</guid>
      <pubDate>Wed, 24 Jan 2024 08:25:57 GMT</pubDate>
    </item>
    <item>
      <title>对尝试在 Google Colab 中使用自定义健身房环境感到困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19eaq8o/confused_on_trying_to_use_a_custom_gym/</link>
      <description><![CDATA[       由   提交/u/kwasi3114  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19eaq8o/confused_on_trying_to_use_a_custom_gym/</guid>
      <pubDate>Wed, 24 Jan 2024 06:32:09 GMT</pubDate>
    </item>
    <item>
      <title>有哪些可以在 google collab 上运行的基本 3D 游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19e5xmu/any_basic_3d_games_that_work_on_google_collab/</link>
      <description><![CDATA[通过使用 opencv 制作的一些非常简单的游戏，我已经能够获得健身房和稳定的基线。  我想看看是否有一个 3d 引擎可以与 google collab 和gymnasium 配合使用来制作一些基本的 3d 动画以与稳定的基线一起使用。  如果有一个支持此功能的良好 Python 库或教程，请在此处链接。谢谢   由   提交 /u/ResponsibilityNew423   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19e5xmu/any_basic_3d_games_that_work_on_google_collab/</guid>
      <pubDate>Wed, 24 Jan 2024 02:13:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习能否有效地生成具有很多约束的布局？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dwhxk/is_reinforcement_learning_efficient_to_generate/</link>
      <description><![CDATA[      你好， 对于一个学校项目，我想尝试使用强化学习生成平面图，并将其与用于此问题的现有方法（例如进化算法和监督机器学习）进行比较。我希望有一些 RL 经验的人对该项目进行一些评论。 输入：房间列表、房间邻接矩阵、计划占地面积、每个房间的一些空间限制，例如最小/最大面积或比率（宽度/长度）。迭代从随机设置房间的原始布局开始（也许我会启动具有不同开始布局的多个强化学习系统）。 操作：交换 2 个房间、推动房间墙壁、划分房间墙壁（没有矩形形状），合并房间墙 奖励：尊重房间邻接矩阵，尊重空间约束，可以访问所有房间。 使用进化算法，我发现的最相似问题的文章： https://www.researchgate.net/publication/312263676_Evolutionary_approach_for_spatial_architecture_layout _design_enhanced_by_an_agent-based_topology_finding_system 使用强化学习，我发现了与问题最相似的论文：“用于快速芯片设计的图形放置方法” https://www.nature.com/文章/s41586-021-03544-w.epdf?sharing_token=tYaxh2mR5EozfsSL0WHZLdRgN0jAjWel9jnR3ZoTv0PW0K0NmVrRsFPaMa9Y5We9O4Hqf_liatg-lvhiVcYpHL_YQpqkurA31sxqtmA-E1yNUWVM MVSBxWSp7ZFFIWawYQYnEXoBE4esRDSWqubhDFWUPyI5wK_5B_YIO-D_kS8%3D 目标是强化学习过程学习“如何设计住宅平面图”能够适应像这样的新足迹： https://preview.redd.it/clnijy4to8ec1.png?width=1460&amp;format=png&amp;auto=webp&amp;s=8d50de4c4348237b29218c39c963dd7ddf6eaad7   由   提交/u/Geralt2477  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dwhxk/is_reinforcement_learning_efficient_to_generate/</guid>
      <pubDate>Tue, 23 Jan 2024 19:22:28 GMT</pubDate>
    </item>
    <item>
      <title>第一个项目：蛇</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</link>
      <description><![CDATA[     &lt; td&gt; 算法是某种类型的强化（虽然不确定，我只是从课程中获取了 nn 更新部分），我有一个神经网络69m 参数。网络的输入是 3 个网格：苹果位置、蛇位置和地图外区域。我还根据蛇的旋转来旋转输入，因此它始终朝上   由   提交/u/thebrownfrog  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:18 GMT</pubDate>
    </item>
    <item>
      <title>头脑风暴：多智能体的强化学习系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dr25j/brainstorming_rl_system_for_multiple_agents/</link>
      <description><![CDATA[我正在寻求有关如何构建多个智能体追逐目标的强化学习系统的建议。目标是让所有智能体接近目标，但不要太接近。同时，我希望代理均匀分布在目标周围。 在 2D 中，想象理想的解决方案是代理沿着目标周围的圆均匀分布。  (1) 我能否期望使用 PPO 训练每个代理实例会产生良好的组性能？或者我是否需要研究像 POCA 这样的多代理方法？ (2) 关于如何创建平衡这些同时目标的奖励函数有什么建议吗？    由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dr25j/brainstorming_rl_system_for_multiple_agents/</guid>
      <pubDate>Tue, 23 Jan 2024 15:34:05 GMT</pubDate>
    </item>
    <item>
      <title>一些 PPO 超参数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dk2ep/some_of_ppo_hyperparams/</link>
      <description><![CDATA[仅设置并行环境数量 = 物理核心数量和每次更新的总时间步数 = 内存中适合的内容是标准程序吗？我之前有过不好的经历，但我不确定我是否只是运气不好，如果我不这样做，我觉得我只是在浪费机器的潜力。 其他超参数会当然也取决于这些，所以我想如果我在以前研究过的环境中工作，那么找到新的学习率、剪辑等是另一个问题，我可以从其他人发现工作正常的任何内容开始   由   提交 /u/victorsevero   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dk2ep/some_of_ppo_hyperparams/</guid>
      <pubDate>Tue, 23 Jan 2024 08:50:10 GMT</pubDate>
    </item>
    </channel>
</rss>