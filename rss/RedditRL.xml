<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 15 Feb 2025 06:22:21 GMT</lastBuildDate>
    <item>
      <title>RL之后的模仿学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipn26x/imitation_learning_after_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我知道您可以在模仿学习后执行RL，但是在RL之后您可以执行模仿学习。  &lt;！ -  sc_on - &gt;＆＃32;提交由＆＃32; /u/u/u/robotdodgeball     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipn26x/imitation_learning_after_rl/</guid>
      <pubDate>Fri, 14 Feb 2025 22:38:12 GMT</pubDate>
    </item>
    <item>
      <title>寻找具有语言定义目标的培训RL代理的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipiuwo/looking_for_work_on_training_rl_agents_with/</link>
      <description><![CDATA[在自然语言。具体来说，我正在寻找探索的工作：  使用语言作为灵活的奖励信号  培训政策以为条件文本中的描述   通过LLMS    层次结构rl与语言引导的子搜索   我很想阅读任何论文，存储库或博客文章探索此主题的内容。如果您从事类似的事情，我也很乐意讨论想法或协作！ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃ 32;提交由＆＃32; /u/u/foricas-ad2641     link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipiuwo/looking_for_work_on_training_rl_agents_with/</guid>
      <pubDate>Fri, 14 Feb 2025 19:33:38 GMT</pubDate>
    </item>
    <item>
      <title>需要RL学习伙伴</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipa8fy/need_study_partner_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在担任2.5 Yoe的数据科学家，主要是在经典ML和NLP上工作，但想探索RL，因为我可能有使用情况我的工作是从YT上观看David Silver演讲开始的，但是它的数学太重了（目前在第二LEC上），如果我能够完成PR，我会失去信心彼此之间明确的怀疑。请随意dm me !!   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/shirish0500     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipa8fy/need_study_partner_for_rl/</guid>
      <pubDate>Fri, 14 Feb 2025 13:10:53 GMT</pubDate>
    </item>
    <item>
      <title>实验室可以在欧洲的RL上获得博士学位</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip9lur/labs_to_do_a_phd_in_rl_in_europe/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在寻找2026年的博士带有RL等的型号。我不是在研究纯MDP或土匪之类的东西。我想要更适用的东西，例如可塑性研究，终身学习，甚至更好的RL架构，或多代理或分层RL，RL + LLM，RL +扩散等。更多的ML喜欢更好的变压器体系结构，状态空间模型等。我在EPFL，ETH和DARMSTADT上看到了一些实验室。但是真的很感激。 ＆＃32;提交由＆＃32; /u/u/no_carpenter7252      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip9lur/labs_to_do_a_phd_in_rl_in_europe/</guid>
      <pubDate>Fri, 14 Feb 2025 12:35:32 GMT</pubDate>
    </item>
    <item>
      <title>RL教程的业余爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://github.com /google-deepmind/mujoco/descordions/2404    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/goncalogordo     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</guid>
      <pubDate>Fri, 14 Feb 2025 09:06:19 GMT</pubDate>
    </item>
    <item>
      <title>熵重</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6d4b/entropy_weight/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在使用软演员评论家进行多代理强化学习。折扣奖励约为1000-1300。熵重的正确值是多少？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fuzzy-plantain2402      link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6d4b/entropy_weight/</guid>
      <pubDate>Fri, 14 Feb 2025 08:49:24 GMT</pubDate>
    </item>
    <item>
      <title>回顾我成为利基领域的RL研究人员的计划（AG/遗传学）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip51q9/review_my_plan_for_becoming_an_rl_researcher_in_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi。我从事遗传学和农业工作。我是平庸的软件工程师/定量遗传学家，数学背景相对较差。去年，我制作了一个简单的问题。在Pytorch中制作了一个环境，并使用SB3成功地训练代理，并成功地训练了一个非常简单的用例。我还审查了我的数学量，一旦我遇到泰勒系列（我从未在学校正式研究过）。 我已经完成了机器学习问题（计算机视觉）。在成功的行业中，对基金会的研究足以在可可/重新确定年龄的情况下进行监督/无监督的学习问题。  ，所以我的计划是做以下  1）继续研究DSA（阅读数据结构和算法的常识指南，第I卷，也许是第II卷）  2）阅读Grokking DL算法书籍  3）然后通过此内容（祈祷最近的LLM可以推动我解决此问题） https://github.com/mathfoundationrll /读书基础学习学习    all    4）在Jax中精心选择的功能重建我的环境然后再次重新进行我过去的SB3实验。 *虽然对我的实验非常公开  5）扩展实验以具有真实的分布式组件（在上下文中有意义）  我的目标是做这些工具在我的行业中的咨询...因此，一旦我的基础知识降低了，也许我可以开始独立发表论文。否则，我将不得不考虑申请其中一家大公司或攻读博士学位 我有6个月的合理跑道来实现这一目标，这使我希望我的基础很务实。我没有任何社区或指导可以与之讨论。如果我能在我放下头之前对此获得任何反馈，我会感谢它。另外，我不会拒绝任何经验丰富的人保持联系。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/foodisaweapon     [link]   ＆＃32;   [注释]     ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip51q9/review_my_plan_for_becoming_an_rl_researcher_in_a/</guid>
      <pubDate>Fri, 14 Feb 2025 07:10:51 GMT</pubDate>
    </item>
    <item>
      <title>人类会做RL，监督学习还是完全不同的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip2cl2/do_humans_do_rl_supervised_learning_or_something/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我已经在加强学习已经几个月了，当我不得不汗水以定义该问题时，这个问题总是在我的脑海中正确的奖励。 我得到了这种感觉，我们能够根据真正的奖励创造中间奖励。就像为了在X Company找到工作一样，我必须以前磨削这些n步骤，并且每次执行此步骤时都会很高兴。 在RL中RL模型，如果您可以正确调整损失功能？ 我的问题似乎尚不清楚，并且非常开放。我只是觉得人类在RL和有监督的学习之间有一个中间的，我无法真正掌握我的头。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/delicious_wall3597      [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1ip2cl2/do_humans_do_do_rl_supervise_learning_learning_or_something/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip2cl2/do_humans_do_rl_supervised_learning_or_something/</guid>
      <pubDate>Fri, 14 Feb 2025 04:19:33 GMT</pubDate>
    </item>
    <item>
      <title>epymarl- mappo rware总是给予0奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip1fj6/epymarl_mappo_rware_always_gives_0_reward/</link>
      <description><![CDATA[     你好， ，所以我正在使用epymarl  https：/ /Github.com/uoe-agents/epymarl 使用Mappo算法训练Rware。但是问题是即使我跑40m时间步骤，奖励总是0。   https：https：// pr = CFC968525607543A888330F7A01554C86A25944E7B   我对Marl有点陌生。如果有人已经使用了Rware，您可以告诉我我缺少的内容。 我没有更改EpyMarl repo   &lt;！&lt;！ -  sc_on-&gt;＆&gt;＆ ＃32;提交由＆＃32; /u/u/ajxbnu     [link]   ＆＃32;   [注释]   /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip1fj6/epymarl_mappo_rware_always_gives_0_reward/</guid>
      <pubDate>Fri, 14 Feb 2025 03:28:01 GMT</pubDate>
    </item>
    <item>
      <title>RL无法改善基本监督模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</link>
      <description><![CDATA[      i具有基于序列预测的合理工作的基于基于的模型（RNN）。然后，我创建了一个PPO RL模型来调整PRE的输出 - 训练的RNN模型。问题：RL实际上降低了MSE度量。我有些惊讶RL实际上可能会受到如此巨大的伤害。&lt; /p&gt;   MSE，没有RL调整：0.000047  MSE带有RL调整：0.002053    验证MSE vs Iteration     &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/u/maadotaa     [link]   ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</guid>
      <pubDate>Fri, 14 Feb 2025 00:00:09 GMT</pubDate>
    </item>
    <item>
      <title>“使用大型推理模型[O3]的竞争性编程”，El-Kishky等人2025 {oa}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</guid>
      <pubDate>Thu, 13 Feb 2025 22:56:18 GMT</pubDate>
    </item>
    <item>
      <title>Langevin Soft Actor-Critic：通过不确定性驱动的批评者学习，Ishfaq等人2025。ICLR 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</link>
      <description><![CDATA[        &lt;！ -  sc__off- sc_off-&gt;  现有的Actor-Critic算法在连续控制加强学习（RL）任务中很受欢迎，由于其中缺乏原则性的探索机制，其样本效率不佳。由于汤普森采样成功在RL中有效探索的动机，我们提出了一种新颖的无模型RL算法，\ emph {langevin soft Actor评论家}（LSAC）（LSAC）优先考虑通过对策略优化的不确定性估计来增强评论家的学习。 LSAC采用了三个关键创新：通过基于分布的LangeMonte Carlo（LMC）更新，近似汤普森采样，平行回火，用于探索该功能后部多种模式，以及与动作梯度正常化的综合状态行动样品。我们的广泛实验表明，LSAC的表现优于或匹配无连续控制任务主流模型RL算法的性能。值得注意的是，LSAC标志着基于LMC的Thompson采样在具有连续动作空间的连续控制任务中的首次成功应用  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hmi2015     [link]  ＆＃32;   [注释]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</guid>
      <pubDate>Thu, 13 Feb 2025 18:50:08 GMT</pubDate>
    </item>
    <item>
      <title>我的个人项目-Alphayinshzero（Blitz）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</link>
      <description><![CDATA[&lt; src =“ https://b.thumbs.redditmedia.com/qwtnkjppq5js2x​​jru2xjru2xv2ulsrfp8lpgjynuhfkuwg7i.jpg” title =“我的个人项目-Alphayinshzero（Blitz） - &gt;  我使用alphazero训练了闪电兹版本的AI模型，并且它能够在板空间上击败Smartbot。 请注意，Blitz版本是您尝试连续获得5个的地方。 这是迭代174对抗自己。  https://preview.redd.it/1ohn81p23yie1.png?width=866&amp;format=png&amp;auto=webp&amp;s=3d7261cdc446f7349eae31fe5bca4b66b8bcbfed&lt; /a&gt;  在训练期间，有充分的证据表明，闪电战版本具有第一球员优势，因为第一个球员逐渐攀升至最后的胜利率。 我是强化学习的新手，当涉及政策分配时，我可能会提出一种特殊的方法，因此请随时告诉我这是否是一种有效的方法，或者在AI培训中是否有问题。 我将yinsh表示为11 x 11数组，因此动作空间为121 + 1（通过转弯）。 我想避免使用大型政策分布，例如121（statter） * 121（目的地）= 14641  所以，我将游戏分为阶段：戒指放置（放置10个戒指），戒指选择（选择要移动的戒指）和标记放置（放置一个（放置一个）标记和移动选定的戒指）。 这样，单人的回合像这样可以工作： 转1-选择要移动的戒指。转弯2-对手通过。转3-选择要移动戒指的位置。&lt; /p&gt; 通过将其分为阶段，我可以使用121 + 1的动作空间。这种方法“感觉”对我而言。它似乎有效。  ...  我试图训练Yinsh的完整游戏，但这是不完整的。到目前为止，我对它的策略感到非常满意。 不满意，我的意思是它只是沿着边缘形成了一个密集的标记领域，他们不想互相互动。我真的希望AI战斗和造成混乱，但他们太宁静了 - 只是在关注自己的生意。通过沿着边缘形成致密的标记，标记变得难以置信。  AI的（天真？）方法只是：“让我像农民一样在边缘形成标记领域来自同一地区的5英寸排。”他们就像董事会对面的两个农民，宁静地制作了自己的标记领域。 闪电战版更令人兴奋，而人工智能互相战斗：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kiwigami     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</guid>
      <pubDate>Thu, 13 Feb 2025 18:29:17 GMT</pubDate>
    </item>
    <item>
      <title>当您已经拥有体育馆环境时，SB3矢量化环境如何工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我无法完全理解。您只是使用他们的Vecenv包装它吗？还是我必须重写它？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/blearx     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</guid>
      <pubDate>Thu, 13 Feb 2025 17:33:39 GMT</pubDate>
    </item>
    <item>
      <title>参考丢失：带有RL算法分类法/本体的电子表格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在这里的某个地方看到了它，现在找不到它。我知道有一些调查RL算法的论文，但我正在尝试找到“散布表”，其中一位成员在评论中发布。我相信这是Google文档的链接。 每行都有一些更高级别的分组，每个组和注释中都有算法。它通过其属性（例如连续的动作空间等）将算法分开。 有人知道该资源还是我可以找到的地方？ 编辑：找到它！  https://rl-picker.github.io/     &lt;！ -  sc_on- &gt;＆＃32;提交由＆＃32; /u/u/u/paramedicfabulou345     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</guid>
      <pubDate>Thu, 13 Feb 2025 11:41:11 GMT</pubDate>
    </item>
    </channel>
</rss>