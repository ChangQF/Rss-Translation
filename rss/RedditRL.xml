<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 29 Dec 2023 06:19:49 GMT</lastBuildDate>
    <item>
      <title>Boid环境下未实现植绒</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18t82zx/not_achieveing_flocking_in_boid_environment/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18t82zx/not_achieveing_flocking_in_boid_environment/</guid>
      <pubDate>Thu, 28 Dec 2023 23:35:14 GMT</pubDate>
    </item>
    <item>
      <title>具有策略切换功能的指挥官风格强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18t6vkf/commander_style_rl_with_policy_switching/</link>
      <description><![CDATA[现在我面前有一个特别有趣的问题，我有一个想法。但我不确定这个想法以前是否已经实现过... 我的环境基本上有一名经理和 4 名工人。可以为工人分配 3 种不同任务中的一种（比如做饭、上菜和洗碗）。每个任务都有自己的训练策略。这里有一个问题 - 经理可能会在剧集中间指示工作人员切换任务（例如：洗碗机可能需要在高峰期间帮助厨师，或者服务器可能需要在打烊时帮助洗碗） ）。这里本质上有两个主要的学习层次：每项任务的原始动作（烹饪的最佳方式、服务的最佳方式和洗碗的最佳方式）和资源分配任务（最好地利用 4 名工人来完成任务）。经营一家餐厅的最终目标）。 我对如何做到这一点有一些想法（但我不完全确定什么是可能的或合乎逻辑的）：  &lt; li&gt;将此视为某种分层学习问题 - 确实如此。经理每隔多个时间步设置任务分配，而工作人员则在每个时间步执行操作来完成这些任务。这是相当可行的，但我不确定如何完成策略切换。根据我的经验，每个多代理设置都有自己的训练策略，因此尝试创建 3 个通用策略并动态分配它们进行训练似乎......很棘手。 创建奖励函数而不是策略切换根据工人的分配奖励不同的结果。该分配可以是其观察空间的一部分（例如：当观察到工人是洗碗机时，奖励函数会奖励干净的盘子，但如果观察到工人是服务员，则奖励满水，等等）。将创建一项所有员工都可以共享的单一政策，并且他们的行为可能会根据他们观察到的任务而改变。 这似乎是最不合逻辑的，但也许我错过了一些东西......我可以分别对这 3 个策略中的每一个进行预训练，并将它们设为静态策略，供经理尝试将任务分配给工作人员。但是A）我认为这消除了策略之间的一些上下文依赖性（例如：在晚餐高峰期间，如果我只将厨师训练为单个实体，然后分配了两名厨师，则该行为可能不会接近最佳，因为以前从未见过）B）我仍然存在实际执行策略切换的问题。  ​ 所以我的问题由此可知，有两个方面。  一次事件期间的动态策略切换实际上可能吗？谁能给我举一些例子吗？ 其中哪一个实际上最有意义？  ​ TIA 提供任何建议并帮助！   由   提交/u/Cheap_Leather_6432   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18t6vkf/commander_style_rl_with_policy_switching/</guid>
      <pubDate>Thu, 28 Dec 2023 22:43:35 GMT</pubDate>
    </item>
    <item>
      <title>[R]可解释强化学习研究综合概述</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18szxuu/r_comprehensive_overview_of_explainable/</link>
      <description><![CDATA[ 由   提交/u/peppercat-2c4t9  /u/peppercat-2c4t9 reddit.com/r/MachineLearning/comments/18suecx/r_compressive_overview_of_explainable/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18szxuu/r_comprehensive_overview_of_explainable/</guid>
      <pubDate>Thu, 28 Dec 2023 17:51:48 GMT</pubDate>
    </item>
    <item>
      <title>“绘制行为结构的细胞基础”，El-Gaby 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18semw7/a_cellular_basis_for_mapping_behavioral_structure/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18semw7/a_cellular_basis_for_mapping_behavioral_structure/</guid>
      <pubDate>Wed, 27 Dec 2023 23:31:06 GMT</pubDate>
    </item>
    <item>
      <title>PASTA：预训练的动作状态转换器代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18se47h/pasta_pretrained_actionstate_transformer_agents/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2307.10936 OpenReview (1):  https://openreview.net/forum?id=pxK9MWuFF8 OpenReview (2): https://openreview.net/forum?id=ciBFYxzpBT 摘要：  自我-监督学习给各个计算领域带来了革命性的范式转变，包括 NLP、视觉和生物学。最近的方法涉及对大量未标记数据进行预训练 Transformer 模型，作为有效解决下游任务的起点。在强化学习中，研究人员最近采用了这些方法，开发了根据专家轨迹进行预训练的模型。这一进步使模型能够处理从机器人到推荐系统的广泛任务。然而，现有方法主要依赖于针对特定下游应用量身定制的复杂预训练目标。本文对模型进行了全面的研究，称为预训练动作状态转换代理（PASTA）。我们的研究涵盖了统一的方法论，并涵盖了广泛的一般下游任务，包括行为克隆、离线强化学习、传感器故障鲁棒性和动态变化适应。我们的目标是系统地比较各种设计选择，并提供有价值的见解，帮助从业者开发强大的模型。我们研究的主要亮点包括动作和状态的组件级别的标记化、基本预训练目标的使用（例如下一个标记预测或掩码语言建模）、跨多个领域的模型同步训练以及各种微调的应用策略。在这项研究中，开发的模型包含不到 700 万个参数，允许广泛的社区使用这些模型并重现我们的实验。我们希望这项研究能够鼓励进一步研究使用具有第一原理设计选择的变压器来表示 RL 轨迹，并为稳健的策略学习做出贡献。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18se47h/pasta_pretrained_actionstate_transformer_agents/</guid>
      <pubDate>Wed, 27 Dec 2023 23:09:02 GMT</pubDate>
    </item>
    <item>
      <title>RL IRL：2015-2019 年 Google 搜索中排名和偏好学习的使用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18scjqg/rl_irl_on_google_search_use_of_ranking/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18scjqg/rl_irl_on_google_search_use_of_ranking/</guid>
      <pubDate>Wed, 27 Dec 2023 22:03:10 GMT</pubDate>
    </item>
    <item>
      <title>艾未未无需任何编码就能学会玩《地铁跑酷》。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rttda/ai_learns_to_play_subway_surfers_without_any/</link>
      <description><![CDATA[    /u/Worldly-Daikon5001   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rttda/ai_learns_to_play_subway_surfers_without_any/</guid>
      <pubDate>Wed, 27 Dec 2023 06:19:29 GMT</pubDate>
    </item>
    <item>
      <title>GridWorld 的 Q-Learning 有时无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rtkso/qlearning_for_gridworld_occasionally_failing_to/</link>
      <description><![CDATA[我有一个 10x10 网格世界环境，我正在其中尝试实现 Q-Learning。所有单元格的奖励为 0.1，而终端单元格的奖励为 10。折扣因子为 0.9。有时，Q-Learning 无法收敛（例如每 10 次收敛一次），并且代理会卡在远离终端单元的位置。我尝试了衰减 epsilon，但这只会让训练变慢。请在此处找到该作品的链接。谢谢。    由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rtkso/qlearning_for_gridworld_occasionally_failing_to/</guid>
      <pubDate>Wed, 27 Dec 2023 06:05:30 GMT</pubDate>
    </item>
    <item>
      <title>我为我的 NeurIPS 2023 论文制作了一个 7 分钟的讲解视频。我希望你喜欢它 ：）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rti1y/i_made_a_7minute_explanation_video_of_my_neurips/</link>
      <description><![CDATA[       由   提交 /u/delayed_reward   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rti1y/i_made_a_7minute_explanation_video_of_my_neurips/</guid>
      <pubDate>Wed, 27 Dec 2023 06:01:13 GMT</pubDate>
    </item>
    <item>
      <title>“拒绝的理由？将语言模型与判断相结合”，Xu et al 2023 {腾讯}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rnxw1/reasons_to_reject_aligning_language_models_with/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rnxw1/reasons_to_reject_aligning_language_models_with/</guid>
      <pubDate>Wed, 27 Dec 2023 01:19:14 GMT</pubDate>
    </item>
    <item>
      <title>“ER-MRL：元强化学习的进化储存库”，Léger 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rmu8o/ermrl_evolving_reservoirs_for_meta_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rmu8o/ermrl_evolving_reservoirs_for_meta_reinforcement/</guid>
      <pubDate>Wed, 27 Dec 2023 00:27:03 GMT</pubDate>
    </item>
    <item>
      <title>我可以直接改变基于策略的方法中的动作概率吗？ 【安全勘探相关】</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rlzly/can_i_directly_alter_the_action_probability_in/</link>
      <description><![CDATA[假设我想在基于网格的环境中使用强化学习来执行一些规划任务，我希望代理在训练中偶尔避开某些单元。  在像 Q 学习这样的基于值的简单方法中，我可以减少与该操作相关的值，从而降低采取该操作的概率（假设我使用 softmax）。基于策略的方法或其他基于价值的方法是否有类似的东西？  这背后的直觉是，我想告诉智能体：“如果你可能因行动 X 而陷入危险状态，请降低在此状态下采取行动 X 的概率”。我不希望代理完全停止进入该状态，因为我仍然希望它能够探索需要进入该状态的轨迹。我总是不希望代理仅通过试错来学习这个概率，我想给代理一些先验知识。  我是否在考虑直接改变动作概率？还有其他方法可以像这样预先注入吗？  我希望这是有道理的！  谢谢！    由   提交 /u/AlloyEnt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rlzly/can_i_directly_alter_the_action_probability_in/</guid>
      <pubDate>Tue, 26 Dec 2023 23:49:04 GMT</pubDate>
    </item>
    <item>
      <title>“自我预测通用人工智能”（Self-AIXI）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r792j/selfpredictive_universal_ai_selfaixi/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=psXVkKO9No 摘要：  强化学习（RL）算法通常利用学习和/或制定有效政策的规划技术。事实证明，两种方法的集成在解决复杂的顺序决策挑战方面非常成功，AlphaZero 和 MuZero 等算法就证明了这一点，这些算法将规划过程整合到参数搜索策略中。 AIXI 是理论上最有效的通用智能体，它通过综合搜索进行规划作为寻找最优策略的主要手段。在这里，我们定义了一个替代的通用代理，我们称之为Self-AIXI，与A​​IXI相反，它最大限度地利用学习来获得良好的策略。它通过自我预测自己的动作数据流来实现这一点，与其他 TD(0) 代理类似，该数据流是通过对当前在策略（通用混合策略）Q 值估计采取动作最大化步骤来生成的。我们证明了Self-AIXI收敛于AIXI，并继承了最大Legg-Hutter智能和自优化特性等一系列特性。   &amp; #32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r792j/selfpredictive_universal_ai_selfaixi/</guid>
      <pubDate>Tue, 26 Dec 2023 12:36:43 GMT</pubDate>
    </item>
    <item>
      <title>GAE 来估计优势还是回报？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r3p15/gae_to_estimate_advantage_or_also_returns/</link>
      <description><![CDATA[嗨，在旋转 Ppo 时，他们使用 GAE 计算优势，并且仅使用奖励计算回报（蒙特卡罗估计）但是，其他植入使用 GAE 来计算近似回报和优势。有什么意见或想法吗？   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r3p15/gae_to_estimate_advantage_or_also_returns/</guid>
      <pubDate>Tue, 26 Dec 2023 08:36:09 GMT</pubDate>
    </item>
    <item>
      <title>PPO 与状态相关标准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r3l8z/ppo_with_state_dependent_std/</link>
      <description><![CDATA[嗨，有谁知道 Ppo 实现与可学习的 Logstd 取决于状态，而不仅仅是一个参数（例如 cleanrl）我尝试实现，但是它非常不稳定，我可以使用类似 sac 实现的东西，但试图找到 ppo 稳定的东西 谢谢   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r3l8z/ppo_with_state_dependent_std/</guid>
      <pubDate>Tue, 26 Dec 2023 08:28:56 GMT</pubDate>
    </item>
    </channel>
</rss>