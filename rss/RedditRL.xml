<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 17 Nov 2024 18:21:04 GMT</lastBuildDate>
    <item>
      <title>DDQN 无法收敛，可能出现灾难性遗忘</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtihzc/ddqn_not_converging_with_possible_catastrophic/</link>
      <description><![CDATA[      我正在训练 DDQN 代理进行股票交易，从下面的损失可以看出，在前 30k 步中，损失正在减少，然后直到 450k 步，模型似乎不再收敛 https://preview.redd.it/yc6o9wynwh1e1.png?width=592&amp;format=png&amp;auto=webp&amp;s=aef476c7eb177f82fe112d0d1fd5a95ef90c6917 此外，从投资组合价值的进展情况可以看出，模型似乎忘记了每集所学的内容。 这些是我的超参数，请注意，我使用的是固定的情节长度= 50k 步，并且每个情节都从一个随机点开始  learning_rate=0.00001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update=1000, buffer_capacity=20000, batch_size=128,  可能是什么问题以及如何解决它？    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtihzc/ddqn_not_converging_with_possible_catastrophic/</guid>
      <pubDate>Sun, 17 Nov 2024 17:19:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助需求：如何在不造成训练推理差距的情况下为 RLHF 中的每个标记分配奖励分数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gth0yi/helped_needs_how_to_assign_reward_scores_to_each/</link>
      <description><![CDATA[在 RLHF 中，我一直在努力解决如何有效地将奖励分数分配给各个标记的问题。 奖励模型通常使用成对比较进行训练，输出一个评估句子整体质量的单个标量。然而，在 RLHF 期间，为了训练价值函数（用于 PPO 等技术），我们需要计算累积奖励： $$Rt = \sum{t’=t&gt;T r(s{t’}, a{t’})$$ 这是我的主要问题： - 我们如何将这个句子级别的奖励分解为 token 级别的奖励？ 我正在考虑的一种简单方法是： - 直接将训练有素的线性层应用于每个 token 的隐藏状态以预测其奖励分数。 但是，我担心这可能会引发两个主要问题：1. 训练推理差距：奖励模型经过训练以评估整个句子，但这种逐个 token 的分解可能会与 RM 的原始训练设置不同。 2. 性能下降：推理过程中的奖励分布可能与真正的奖励信号不一致，从而可能损害策略优化。 我正在寻找社区的建议或见解： - 有没有更好的方法将句子级奖励分解为 token 级分数？ - 我们如何验证 token 奖励分解的有效性？ 我非常感谢任何想法或建议。谢谢！    提交人    /u/ForJadeForest   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gth0yi/helped_needs_how_to_assign_reward_scores_to_each/</guid>
      <pubDate>Sun, 17 Nov 2024 16:16:41 GMT</pubDate>
    </item>
    <item>
      <title>常规 RL 和 LORA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtb77g/regular_rl_and_lora/</link>
      <description><![CDATA[是否有任何 GitHub 示例用于微调常规 ppo，例如使用 lora 解决简单的 rl 问题？就像从一个 Atari 游戏到另一个  编辑用例：假设您有一个问题，其中有很多初始条件，如速度、方向等……95% 的初始条件已得到解决，5% 无法解决（尽管它们是可解的）但是您很少遇到它因为它只有 5% 的“样本”所以现在你想更多地训练这 5%，并且在训练期间增加它的数量..并且你不想“忘记”或破坏以前的成功。（这主要针对开启策略而不是具有高级回复缓冲区的关闭策略）...    提交人    /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtb77g/regular_rl_and_lora/</guid>
      <pubDate>Sun, 17 Nov 2024 11:00:11 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Lab 中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</link>
      <description><![CDATA[您好，我是在模拟中训练机器人的新手。我刚刚设置了我的 isaac 实验室，但我不知道如何在其中训练我自己的模型。关于它的文档也不多（我知道 NVidia 文档，但就是这样）。有人能为我提供更多关于如何入门的信息吗？另外，没有关于它的教程/视频/文档，因为它是新的还是不好的？它什么时候向公众开放的？谢谢！    提交人    /u/North_Set8162   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</guid>
      <pubDate>Sat, 16 Nov 2024 23:06:43 GMT</pubDate>
    </item>
    <item>
      <title>银行业有趣的研究课题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</link>
      <description><![CDATA[我目前是一名计算机科学 (ML 专业) 的兼职硕士生，在银行业担任数据工程师，我计划通过在银行业务场景中应用 RL 代理来撰写研究报告。我能想到一些事情，比如贷款决策或欺诈检测，但对我来说没有什么真正有趣的。有什么建议可以告诉我可以研究什么吗？我理想情况下想要一些我们有一些开源数据的东西。    提交人    /u/SatwikGu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</guid>
      <pubDate>Sat, 16 Nov 2024 22:33:22 GMT</pubDate>
    </item>
    <item>
      <title>“可解释的对比蒙特卡洛树搜索推理”，Gao 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</guid>
      <pubDate>Sat, 16 Nov 2024 21:40:25 GMT</pubDate>
    </item>
    <item>
      <title>人的手臂</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</link>
      <description><![CDATA[你好。我想制作一个人体手臂模型，并使用强化学习使其达到目标。 我知道这很难实现（如果可能的话，大量的 DOF、较长的训练时间），所以我尝试使用简单的模型和完全增加的模型来构建它。 如果需要，我很乐意制作自己的 urdf 模型，但也很乐意使用已经存在的东西。 你会推荐从哪里开始？最好的算法是什么（可能是 PPO、SAC、DDPG）？最好的平台是什么（可能是 pybullet、MuJoCo、ROS 和 Gazebo）？ 任何帮助表示感谢。    提交人    /u/tedthemouse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</guid>
      <pubDate>Sat, 16 Nov 2024 20:20:38 GMT</pubDate>
    </item>
    <item>
      <title>为研究论文编写方程式并组织人员</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</link>
      <description><![CDATA[大家好，我目前是强化学习和迁移学习领域的博士生。我正在准备写我的第一篇论文，写方程式及其证明、推导等感觉很不舒服。我想知道经验丰富的研究人员是如何做到的？他们使用什么样的工具？在整个项目中，他们如何不断写下所有这些数学符号和方程式，他们如何呈现它们、跟踪它们，并同时维护多个项目。对于工具，你们使用 iPad 之类的工具吗？我理解 overleaf 的用途，但我觉得亲手写它们更有成就感。你们能分享一下你们是如何用数学和代码等开发系统的吗？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</guid>
      <pubDate>Sat, 16 Nov 2024 17:01:50 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么关于训练 ppo/dqn 解决迷宫的技巧？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gslsxc/any_tips_for_training_ppodqn_on_solving_mazes/</link>
      <description><![CDATA[创建了自己的 gym 环境，其中观察结果由一个形状为 4 的 numpy 数组 (agent_x,agent_y,target_x,target_y) 组成。代理获得的基本奖励为 (distancebefore - distanceafter)（使用 astar），每一步为 -1 或 0 或 1，到达目标时获得奖励 = 100，与墙壁碰撞时获得奖励 = -1（如果我使用 distancebefore - distanceafter，则为 0）。 我正在尝试训练 ppo 或 dqn 代理（尝试了两者）来解决带有墙壁的 10x10 迷宫 你们有什么技巧可以让我尝试，以便我的代理可以在我的环境中学习？ 欢迎提供任何帮助和提示，我之前从未在迷宫上训练过代理，我想知道是否有什么特别需要考虑的。如果有其他模型更好，请告诉你 如果我的代理始终从左上角开始，而目标始终在右下角，则 dqn 可以解决它而 ppo 不能，然而，在我的用例中我想解决的是一个迷宫，每次调用 reset() 时代理都从随机位置开始。这个迷宫能解决吗？ （ppo 似乎也试图穿过障碍物，就像它由于某种原因无法检测到它们一样） 我理解，每次固定代理和目标位置时，dqn 都需要学习一条路径，但是如果代理位置每次重置时都会发生变化，则需要学习许多正确的路径。 墙壁总是固定的。 我对模型使用 baselines3 （我也尝试了 sb3_contrib qrdqn 和循环 ppo） https://imgur.com/a/SWfGCPy    提交人    /u/More_Peanut1312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gslsxc/any_tips_for_training_ppodqn_on_solving_mazes/</guid>
      <pubDate>Sat, 16 Nov 2024 11:59:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助模拟人形站立任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsl3xh/help_with_simulated_humanoid_standing_task/</link>
      <description><![CDATA[  由    /u/Budget_Bad_4135  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsl3xh/help_with_simulated_humanoid_standing_task/</guid>
      <pubDate>Sat, 16 Nov 2024 11:09:18 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习 DEEPRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsjv66/transfer_learning_deeprl/</link>
      <description><![CDATA[你好， DeepRl 中的迁移学习/领域适应的最新进展是什么？ 谢谢！☺️    提交人    /u/TeamTop4542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsjv66/transfer_learning_deeprl/</guid>
      <pubDate>Sat, 16 Nov 2024 09:35:00 GMT</pubDate>
    </item>
    <item>
      <title>找到达到目标的最少移动次数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsii5c/finding_the_minimum_number_of_moves_to_a_goal/</link>
      <description><![CDATA[我是强化学习的新手。我想使用 RL 作为练习来解决 15 个难题 https://en.m.wikipedia.org/wiki/15_puzzle。第一个问题是随机移动会非常缓慢地到达解决状态。所以我想我可以从解决状态开始，进行少量移动，训练代理来解决这个问题，然后慢慢地从解决状态进行越来越多的移动。 我打算使用稳定的基线 3。我不确定我的想法是否可以使用该库进行编码，因为它必须以某种方式记住训练过的代理，并且每次我增加从解决状态开始的移动次数时，都从该点继续训练。 这个想法看起来合理吗？   由    /u/MrMrsPotts  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsii5c/finding_the_minimum_number_of_moves_to_a_goal/</guid>
      <pubDate>Sat, 16 Nov 2024 07:51:55 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：强化学习以在多边形中分布点（Stable-Baselines3）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gse59f/help_needed_reinforcement_learning_for/</link>
      <description><![CDATA[大家好， 我是强化学习的新手，之前没有使用过 Python 或 Stable-Baselines3 库。尽管如此，我还是想解决一个项目，让代理学习在多边形内均匀分布点。 问题陈述：  代理应尽可能均匀地分布点。 此外，这些点必须与多边形的边缘保持最小距离。 多边形可以具有任意形状（不仅仅是简单的矩形等）。  我正在努力弄清楚如何：  为这个问题定义环境。 创建一个有意义的奖励函数来鼓励点的均匀分布。 使用 Stable-Baselines3 设置和配置学习过程。  如果有人有类似问题的经验或可以指导我完成初始步骤，我将不胜感激！我也欢迎有关教程、示例或可以帮助我入门的一般技巧的建议。 提前感谢您的帮助！    提交人    /u/Initial-Crew2533   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gse59f/help_needed_reinforcement_learning_for/</guid>
      <pubDate>Sat, 16 Nov 2024 03:13:20 GMT</pubDate>
    </item>
    <item>
      <title>一款用于多智能体模仿学习和强化学习的开源 2D 版《反恐精英》，全部采用 Python 编写</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gs71k5/an_opensource_2d_version_of_counterstrike_for/</link>
      <description><![CDATA[      SiDeGame（简化的拆弹游戏）是我 3 年前的一个项目，我最终想与大家分享一下，但一直推迟，因为我心里还有一些更新的想法。现在我必须承认我手头上有太多新工作，所以就在这里： 游戏 GIF 该项目的最初目的是为我的硕士论文创建一个 AI 基准环境。从人工智能的角度看，我对 CS 感兴趣的原因有以下几个：  共享经济（玩家可以为他人购买和丢弃物品）， 不确定的角色（每个人在游戏开始时都具有相同的能力和可用物品）， 不完善的盟友信息（第一人称视角限制了对队友信息的访问）， 双峰感知（声音是重要的信息来源，特别是在没有视觉效果的情况下）， 标准化（游戏规则很少改变）， 直观的界面（易于保持一致以进行人机对比）。  起初，我考虑与 CSGO 甚至 CS1.6 的实际游戏进行交互，但后来决定从头开始制作我自己的版本，这样我就可以了解所有的细节，然后根据需要进行更改。我只有一年的时间来做这件事，所以我选择用 Python 来做所有事情——这是我以及 AI 社区中的许多人最熟悉的语言，我认为以后可以提高效率。 有几种方法可以训练 AI 玩 SiDeGame：  模仿学习：让人类玩一些在线游戏。网络历史记录将被记录下来，并可用于重新模拟会话、提取输入输出标签、统计数据等。代理通过监督学习进行训练，以克隆玩家的行为。 本地 RL：使用游戏的同步版本手动踏入并行环境。代理通过反复试验进行强化学习训练。 远程 RL：将参与者客户端连接到远程服务器并让代理实时自我游戏。  作为 AI 基准，我仍然认为它不完整。我不得不匆忙进行模仿学习，直到最近我才重写了强化学习示例以使用我经过测试的实现。现在我可能不会再独自进行任何重大工作了，但我认为作为一个开源在线多人伪 FPS 学习环境，它对 AI 社区来说仍然很有趣。 以下是链接：  代码：https://github.com/jernejpuc/sidegame-py 简短会议论文：https://plus.cobiss.net/cobiss/si/en/bib/86401795（4 页英文，联合 PDF 的一部分，80 MB） 完整论文：https://repozitorij.uni-lj.si/IzpisGradiva.php?lang=eng&amp;id=129594（90 页斯洛文尼亚语，PDF 8 MB）     由   提交  /u/yerney   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gs71k5/an_opensource_2d_version_of_counterstrike_for/</guid>
      <pubDate>Fri, 15 Nov 2024 21:20:43 GMT</pubDate>
    </item>
    <item>
      <title>Yann LeCun 仍然不认为 RL 对 AI 系统至关重要。他认为只有无监督/监督学习/SSL 算法才能处理 RL 所用到的问题类型，例如顺序决策，或者它们将如何处理诸如探索之类的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</link>
      <description><![CDATA[    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</guid>
      <pubDate>Fri, 15 Nov 2024 00:44:11 GMT</pubDate>
    </item>
    </channel>
</rss>