<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 27 Jun 2024 21:13:40 GMT</lastBuildDate>
    <item>
      <title>通过近似抽样实现强化学习的更有效随机探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpz2ci/more_efficient_randomized_exploration_for/</link>
      <description><![CDATA[https://arxiv.org/abs/2406.12241 摘要：汤普森采样（TS）是强化学习（RL）中最流行的探索技术之一。然而，大多数具有理论保证的 TS 算法难以实现，并且不能推广到深度 RL。虽然新兴的基于近似采样的探索方案很有前景，但大多数现有算法特定于具有次优遗憾界限的线性马尔可夫决策过程（MDP），或者仅使用最基本的采样器，例如朗之万蒙特卡洛。在这项工作中，我们提出了一个算法框架，将不同的近似采样方法与最近提出的 Feel-Good Thompson 采样 (FGTS) 方法 (Zhang，2022；Dann 等人，2021) 结合起来，该方法以前被认为在计算上通常是难以解决的。当应用于线性 MDP 时，我们的遗憾分析产生了遗憾对维数的最佳依赖性，超越了现有的随机算法。此外，我们为每个使用的采样器提供了明确的采样复杂度。从经验上讲，我们表明，在需要深度探索的任务中，我们提出的结合 FGTS 和近似采样的算法与其他强基线相比表现明显更好。在 Atari 57 套件中的几款具有挑战性的游戏中，我们的算法实现的性能要么优于要么与深度 RL 文献中的其他强基线相当。    提交人    /u/hmi2015   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpz2ci/more_efficient_randomized_exploration_for/</guid>
      <pubDate>Thu, 27 Jun 2024 18:56:28 GMT</pubDate>
    </item>
    <item>
      <title>帮助使用最新的 Ray 导入 RLlib SACTrainer</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dptiyt/help_with_importing_rllib_sactrainer_with_latest/</link>
      <description><![CDATA[TL;DR 我正在修改一个旧的 RL 脚本 (Ray = 1.1.0)，当使用 Ray=2.31.0 时，该脚本无法识别 SACTrainer() 和 ModelCatalog.register_custom_preprocessor()。我需要更改哪些内容才能让 VS Code 识别它们？ 嗨！我在收到同事发来的旧脚本后尝试设置 RLlib。我的同事使用的是 Ray = 1.1.0，而我有 Ray 2.31.0。我已导入 ray.rllib.algorithms.sac 以使用 Soft-Actor Critic 和函数 SACTrainer。ray.rllib.agents.sac 无法被 VSC 识别，代理模块似乎也不可用，因此我使用的是 rllib.algorithms。  但是，我的 VS Code 无法识别可用的 DQNTrainer() 或 SACTrainer() 函数。SACTrainer() 与我的自定义注册环境一起使用，并具有用于训练的所有配置。 另一个无法识别的函数是 ModelCatalog.register_custom_preprocessor()。它还存在吗？ 我不确定我的导入中缺少什么，或者我是否需要稍微更改代码。我知道 RLlib 中的某些函数可能已被删除或彻底更改，但我无法借助在线资源确定 SACTrainer 是否仍然可用。我是否导入了错误的内容，我应该降级 Ray 吗？或者有没有什么办法可以修复它？ 我在下面添加了部分代码以便为您提供上下文 import ray from ray.rllib.env.external_env import ExternalEnv from ray.tune.registry import register_env from ray.tune.logger import pretty_print from ray.rllib.models import ModelV2, ModelCatalog from ray.rllib.models.preprocessors import Preprocessor, get_preprocessor import ray.rllib.algorithms.dqn as dqn import ray.rllib.algorithms.ppo as ppo import ray.rllib.algorithms.sac as sac import gym import numpy as np from ray.rllib.utils.framework import try_import_tf try: tf=try_import_tf() except Exception as e: tf = None print(f&quot;导入 TensorFlow 时出错：{e}&quot;) sys.exit(1) import tf_slim as slim class aEnvClass(ExternalEnv): &quot;&quot;&quot; 设置观察空间等的服务器后端 &quot;&quot;&quot; class CustomModel(ModelV2): &quot;&quot;&quot; 创建自定义 NN 模型。 &quot;&quot;&quot; # 一些用于函数 _build_layers 和修复状态的代码 class TupleFlatteningPreprocessor(Preprocessor): &quot;&quot;&quot; 将 double 类型的观察列表转换为可接受的离散和框特征格式。然后，它将嵌套的观察列表转换为平面列表。 &quot;&quot;&quot; # 函数 _init_shape() 和 transform() 的一些代码 if __name__ == &quot;__main__&quot;: ray.init(...) register_env(&quot;myEnv&quot;, lambda _: aEnvClass()) ModelCatalog.register_custom_model(&quot;custom_model&quot;, CustomModel) ModelCatalog.register_custom_preprocessor(&quot;my_prep&quot;, TupleFlatteningPreprocessor) my_trainer = sac.SACTrainer( env=&quot;myEnv&quot;, config={ &quot;model&quot;:{&quot;custom_model_config&quot;: &quot;custom_model&quot;, &quot;custom_preprocessor&quot;: &quot;my_prep&quot;,}, &quot;gamma&quot;: 0.1, ... }) while True: results = my_trainer.train() # 更多代码    由    /u/BjunbjonDrinkingChai 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dptiyt/help_with_importing_rllib_sactrainer_with_latest/</guid>
      <pubDate>Thu, 27 Jun 2024 15:06:42 GMT</pubDate>
    </item>
    <item>
      <title>建议我改进强化学习模型训练的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpszab/suggest_me_ways_to_improve_training_in_rl_model/</link>
      <description><![CDATA[当前代码是 -&gt; buffer.py -&gt; https://www.pythonmorsels.com/p/26w4b/ train.py -&gt; https://www.pythonmorsels.com/p/2t3ec/ env.py -&gt; https://www.pythonmorsels.com/p/2z9y2/    由   提交  /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpszab/suggest_me_ways_to_improve_training_in_rl_model/</guid>
      <pubDate>Thu, 27 Jun 2024 14:43:52 GMT</pubDate>
    </item>
    <item>
      <title>对于 REINFORCE 算法来说，使用目标网络是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpqokh/would_using_a_target_network_make_sense_for_the/</link>
      <description><![CDATA[由于 REINFORCE 算法中没有像深度 Q 学习那样进行“引导”（我们使用深度神经网络输出概率），是否有人尝试过使用目标网络？这样会产生更稳定的训练吗？我认为不会，但很好奇是否进行过任何实验。    提交人    /u/Lindayz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpqokh/would_using_a_target_network_make_sense_for_the/</guid>
      <pubDate>Thu, 27 Jun 2024 12:59:10 GMT</pubDate>
    </item>
    <item>
      <title>切换到多代理库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpnf11/switching_to_multiagent_library/</link>
      <description><![CDATA[我在 Open AI Gym 中使用 SB3 中的 PPO 和自定义环境。我想切换到具有多智能体算法的库，但对我的环境代码的更改太多。你们有什么建议？    提交人    /u/OccupyFood101   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpnf11/switching_to_multiagent_library/</guid>
      <pubDate>Thu, 27 Jun 2024 09:49:05 GMT</pubDate>
    </item>
    <item>
      <title>关于 ddpg 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpk45g/question_about_ddpg/</link>
      <description><![CDATA[我正在模拟训练一个带有移动底座和 9 个关节的机械臂来打乒乓球，每一步，状态还包含球拍中心的坐标和精确计算出的击球点的坐标，这样 AI 所需要做的就是将球拍移动到该点。我正在使用 DDPG，想知道在 1.2m 步之后仍然没有学会将球拍移动到精确的位置是否正常。    提交人    /u/MaxiHP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpk45g/question_about_ddpg/</guid>
      <pubDate>Thu, 27 Jun 2024 05:57:00 GMT</pubDate>
    </item>
    <item>
      <title>TSP 的决斗 QN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpjobo/dueling_qns_for_tsp/</link>
      <description><![CDATA[您好， 我在弄清楚如何为类似 TSP 的问题实现决斗 DQN 时遇到了问题。 我有 N 个地方可以分配资源，并且在一个给定的地方要分配 4 种类型的资源。我的环境会根据分配的资源和当时选择的地方给我奖励。该地方只能选择一次。我正在尝试找出最佳顺序。 所以我使用了决斗 DQN。现在我有一个近似 Q (s,a) = V(s) + A(s,a) 的 CNN。由于 A(s,a) 为您提供了状态和动作的好坏值。然后我认为我可以使用 argmax(A(s,a)) 来选择下一个分配资源的最佳位置，然后到那个地方使用我的 Q(s,a) 来执行此操作。例如，如果 argmax(A(s,a)) 告诉我这是位置 2，那么我将获得 Q(2,a) 的 argmax。  但是，我正在查看文献，我发现没有人使用它来解决这类问题。我发现更接近的是策略优势迭代。这也使用了使用优势函数的 argmax 的概念，但它使用了策略网络。虽然我知道我可以改用这种方法，但为什么不遵循我的初始程序？我哪里错了？  谢谢！     提交人    /u/GreenAppleRL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpjobo/dueling_qns_for_tsp/</guid>
      <pubDate>Thu, 27 Jun 2024 05:28:33 GMT</pubDate>
    </item>
    <item>
      <title>如何正确理解范畴批评的实施过程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpfjp5/how_to_understand_the_process_of_implementing/</link>
      <description><![CDATA[嗨！ 我尝试将标量批评家值转换为分类分布，如 C51。但是，我对分布式 RL 还不熟悉，无法理解这个过程。例如，以下代码片段完成了分类过程。  假设原子分布在二维 x 轴上，value 是否应该分布在 y 轴上？ limit 是什么意思？ 为什么 lower 由 value.floor().long() + limit 得出？可以用 value.floor().long() 替换吗？  ​ def to_categorical(value, limit=300): value = value.float() # 避免任何 fp16 恶作剧 value = value.clamp(-limit, limit) distribution = torch.zeros(value.shape[0], (limit*2+1), device=value.device) lower = value.floor().long() + limit upper = value.ceil().long() + limit upper_weight = value % 1 lower_weight = 1 - upper_weight distribution.scatter_add_(-1, lower.unsqueeze(-1), lower_weight.unsqueeze(-1)) distribution.scatter_add_(-1, upper.unsqueeze(-1), upper_weight.unsqueeze(-1)) return distribution  谢谢提前为您的关注和帮助！    提交人    /u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpfjp5/how_to_understand_the_process_of_implementing/</guid>
      <pubDate>Thu, 27 Jun 2024 01:42:18 GMT</pubDate>
    </item>
    <item>
      <title>“通过联合示例选择进行数据管理进一步加速了多模态学习”，Evans 等人 2024 {DM} (CLIP)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpeba1/data_curation_via_joint_example_selection_further/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpeba1/data_curation_via_joint_example_selection_further/</guid>
      <pubDate>Thu, 27 Jun 2024 00:39:40 GMT</pubDate>
    </item>
    <item>
      <title>关于 SB3 的 PPO 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dp768e/questions_about_ppo_from_sb3/</link>
      <description><![CDATA[1) SB3 的 PPO 是否使用经验缓冲区重放？ 2) 它是多智能体吗？如果不是，我该如何实现（任何指针）？想要转换 PPO 环境，以便所有智能体都有自己的决策网络。    提交人    /u/OccupyFood101   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dp768e/questions_about_ppo_from_sb3/</guid>
      <pubDate>Wed, 26 Jun 2024 19:26:15 GMT</pubDate>
    </item>
    <item>
      <title>q-learning：如何处理模型被锁定在动作序列中？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dp6rgb/qlearning_how_to_handle_the_model_being_locked/</link>
      <description><![CDATA[有很多场景会发生这种情况 - 最常见的可能是 QTE（快速时间事件），您会触发过场动画并必须按下按钮来响应。我遇到的问题是，一旦我的模型决定购买某样东西，它就会被锁定在交易步骤的序列中。我担心一些事情  它是否只有在整个序列之后才能获得购买的物品（在状态空间中）？因为当您决定进去购买它时获得它更直观（迫使您在那时购买它） 它是否只有在整个序列之后才能获得奖励？  我倾向于的方法 - 没有来源，也不是我胡乱想的 - 是它应该立即在其状态空间中获得卡，至少，可能还有奖励。然后，当它在交易中花钱时，它只会稍微更新状态 - 只是稍微改变玩家的资源。但这样一来，之后的举动就完全没用了——它们是有保证的，不会对任何事情产生影响，所以我担心这会完全破坏 q 函数。    提交人    /u/Breck_Emert   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dp6rgb/qlearning_how_to_handle_the_model_being_locked/</guid>
      <pubDate>Wed, 26 Jun 2024 19:09:36 GMT</pubDate>
    </item>
    <item>
      <title>寻求强化学习和 GAN 的课程推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dovbp2/seeking_course_recommendations_for_reinforcement/</link>
      <description><![CDATA[大家好， 我目前正在做我的毕业设计，我想深入研究强化学习 (RL) 和生成对抗网络 (GAN)。我对机器学习和深度学习概念有基本的了解，但对这些特定领域还比较陌生。 您能否推荐一些适合 RL 和 GAN 初学者的好课程或资源？我对免费和付费选项都持开放态度。任何关于书籍或在线教程的建议也欢迎。 提前感谢您的帮助！    提交人    /u/Gemyy48812   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dovbp2/seeking_course_recommendations_for_reinforcement/</guid>
      <pubDate>Wed, 26 Jun 2024 10:29:31 GMT</pubDate>
    </item>
    <item>
      <title>如何解决运行 MuZero 常规时的错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dom7kr/how_to_resolve_errors_when_running_muzero_general/</link>
      <description><![CDATA[我是日本人，所以我使用翻译器。 当我尝试运行 MuZero general 时，出现以下错误消息 我想知道如何解决这个问题。 (base) C:\Users\kooou\study\muzero-general-master&gt;python muzero.py 欢迎使用 MuZero！以下是游戏列表： 0. atari 1. breakout 2. cartpole 3. connect4 4. gomoku 5. gridworld 6. lunarlander 7. simple_grid 8. spiel 9. tictactoe 10. twentyone 输入一个数字来选择游戏：9 2024-06-26 09:24:16,548 INFO worker.py:1770 -- 启动了本地 Ray 实例。回溯（最近一次调用最后一次）：文件“C:\Users\kooou\study\muzero-general-master\muzero.py”，第 650 行，在&lt;module&gt; muzero = MuZero(game_name) ^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\muzero.py&quot;, 第 122 行, 在 __init__ self.checkpoint[&quot;weights&quot;], self.summary = copy.deepcopy(ray.get(cpu_weights)) ^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\auto_init_hook.py&quot;, 第 21 行, 在 auto_init_wrapper 中 return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\client_mode_hook.py&quot;, 第 103 行, 在包装器中返回 func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\worker.py&quot;, 第 2630 行, 在获取值中, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\worker.py&quot;, 第863，在 get_objects 中引发 value.as_instanceof_cause() ray.exceptions.RayTaskError(IndexError): ray::CPUActor.get_initial_weights() (pid=24336, ip=127.0.0.1, actor_id=93a6ec0dc4612ea8190775ec01000000, repr=&lt;muzero.CPUActor object at 0x0000021E07EC1FD0&gt;) 文件 &quot;python\ray\_raylet.pyx&quot;，第 1893 行，在 ray._raylet.execute_task 文件 &quot;python\ray\_raylet.pyx&quot;，第 1834 行，在 ray._raylet.execute_task.function_executor 文件中&quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\function_manager.py&quot;，第 691 行，在 actor_method_executor 中返回方法（__ray_actor，*args，**kwargs） ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\util\tracing\tracing_helper.py&quot;，第 467 行，在 _resume_span 中返回方法（self，*_args，**_kwargs） ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\study\muzero-general-master\muzero.py&quot;, 第 489 行, 在 get_initial_weights model = models.MuZeroNetwork(config) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\models.py&quot;, 第 23 行, 在 __new__ return MuZeroResidualNetwork( ^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\models.py&quot;, 第 487 行, 在 __init__ self.representation_network = torch.nn.DataParallel( ^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\Anaconda\Lib\site-packages\torch\nn\parallel\data_parallel.py&quot;，第 150 行，在 __init__ output_device = device_ids[0] ~~~~~~~~~~^^^ IndexError: 列表索引超出范围  谢谢。    提交人    /u/Sufficient-Fly-4040   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dom7kr/how_to_resolve_errors_when_running_muzero_general/</guid>
      <pubDate>Wed, 26 Jun 2024 01:10:24 GMT</pubDate>
    </item>
    <item>
      <title>muzero 如何构建他们的 MCTS？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dogoda/how_does_muzero_build_their_mcts/</link>
      <description><![CDATA[在 Muzero 中，他们同时在各种不同的游戏环境（围棋、atari 等）上训练他们的网络。  在训练期间，MuZero 网络展开 K 个假设步骤，并与从 MCTS 参与者生成的轨迹中采样的序列对齐。通过从重放缓冲区中的任何游戏中采样一个状态来选择序列，然后从该状态展开 K 个步骤。  我无法理解 MCTS 树是如何构建的。每个游戏环境都有一棵树吗？ 是否假设每个环境的初始状态都是恒定的？（不知道这是否适用于所有 atari 游戏）    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dogoda/how_does_muzero_build_their_mcts/</guid>
      <pubDate>Tue, 25 Jun 2024 21:00:03 GMT</pubDate>
    </item>
    <item>
      <title>理解和诊断深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</link>
      <description><![CDATA[发表于 ICML 2024 https://openreview.net/pdf?id=s9RKqT7jVM    由   提交  /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</guid>
      <pubDate>Tue, 25 Jun 2024 09:31:31 GMT</pubDate>
    </item>
    </channel>
</rss>