<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 03 Dec 2023 01:03:59 GMT</lastBuildDate>
    <item>
      <title>演员评论家：π(A|S,θ) 是什么形状？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189drnd/actor_critic_what_is_the_shape_of_πasθ/</link>
      <description><![CDATA[π(A|S,θ) 在 softmax 情况下为 exp(θ⊤Ax(S))/ Σexp(θ⊤Ax(S) )). 形状应该是什么？ 给定动作数量 = k 和特征数量 = d ​   由   提交 /u/Fashism   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189drnd/actor_critic_what_is_the_shape_of_πasθ/</guid>
      <pubDate>Sat, 02 Dec 2023 21:35:56 GMT</pubDate>
    </item>
    <item>
      <title>具有快速且健忘记忆的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1896eu4/reinforcement_learning_with_fast_and_forgetful/</link>
      <description><![CDATA[   /u/smorad  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1896eu4/reinforcement_learning_with_fast_and_forgetful/</guid>
      <pubDate>Sat, 02 Dec 2023 15:45:19 GMT</pubDate>
    </item>
    <item>
      <title>简单的 Q 学习与跨智能体共享值表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18902nv/simple_qlearning_with_value_table_sharing_across/</link>
      <description><![CDATA[围绕测试轨道进行朴素 q 学习的简单示例，使用 C++ 中的 raylib 实现 - 输入：5 个光线投射每个传感器分为 3 个区域：危险近、中、远。 （提供3^5=243个状态） - 动作：以匀速v直行，以v/2向左转向，以v/2向右转向 结束时每个episode，都会计算所有智能体之间的q表平均值并将其设置回智能体，以便在每个episode之后共享知识。当 epsilon 降至零时，我们只看到 1 次均匀移动，而不是 30 次，因为所有智能体始终选择贪婪动作。 https://reddit.com/link/18902nv/video/hnje6htdou3c1/player   由   提交 /u/goksankobe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18902nv/simple_qlearning_with_value_table_sharing_across/</guid>
      <pubDate>Sat, 02 Dec 2023 09:26:38 GMT</pubDate>
    </item>
    <item>
      <title>我能知道绑定 k 是如何绑定的吗？来源：强化学习：理论与算法，作者：Alekh Agarwal、Nan Jiang、Sham M. Kakade 和 Wen Sun https://rltheorybook.github.io/rltheorybook_AJKS.pdf</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188xrhh/can_i_know_how_the_bound_k_was_bound_origin/</link>
      <description><![CDATA[       由   提交/u/Professional_Card176   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188xrhh/can_i_know_how_the_bound_k_was_bound_origin/</guid>
      <pubDate>Sat, 02 Dec 2023 06:41:09 GMT</pubDate>
    </item>
    <item>
      <title>多代理强化学习基线</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188ox6r/mutli_agent_reinforcement_learning_baselines/</link>
      <description><![CDATA[我正在寻找稳定基线 3 的等效版本，但顾名思义，它适用于 MARL，以便我可以在自定义 MARL 环境上对算法进行基准测试。提到的大多数方法都利用与不同库（例如 Tianshou、CleanRL 和 Stable Baselines 3 本身）共享参数。不同的 MARL 算法还有其他常用的标准化基线吗？   由   提交/u/blitzkreig3  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188ox6r/mutli_agent_reinforcement_learning_baselines/</guid>
      <pubDate>Fri, 01 Dec 2023 22:55:03 GMT</pubDate>
    </item>
    <item>
      <title>当元学习遇到在线和持续学习时：一项调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188o8s1/when_metalearning_meets_online_and_continual/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.05241 摘要： &lt; blockquote&gt; 在过去的十年中，深度神经网络在使用涉及广泛数据集的小批量随机梯度下降的训练方案方面取得了巨大的成功。在这一成就的基础上，探索神经网络在其他学习场景中应用的研究激增。元学习是一个引起广泛关注的著名框架。通常被描述为“学会学习”，元学习是一种数据驱动的方法来优化学习算法。其他感兴趣的分支是持续学习和在线学习，两者都涉及使用流数据增量更新模型。虽然这些框架最初是独立开发的，但最近的工作已经开始研究它们的组合，提出新颖的问题设置和学习算法。然而，由于复杂性增加且缺乏统一术语，即使对于经验丰富的研究人员来说，辨别学习框架之间的差异也可能具有挑战性。为了促进清晰的理解，本文提供了一项全面的调查，使用一致的术语和正式的描述来组织各种问题设置。通过概述这些学习范式，我们的工作旨在促进这一有前途的研究领域的进一步进步。  https://preview.redd.it/ag7xaviaer3c1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=c4c0aa093ac 20ae7f6f567c9427f9a9e76e3a7ee&lt; /a&gt;   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188o8s1/when_metalearning_meets_online_and_continual/</guid>
      <pubDate>Fri, 01 Dec 2023 22:24:42 GMT</pubDate>
    </item>
    <item>
      <title>估计具有固定步数的环境的状态值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188jh9d/estimating_state_value_for_environments_with/</link>
      <description><![CDATA[我正在自定义环境中训练 RL 代理。我的环境在执行一定数量的操作后终止。我的直觉是，当涉及到通过价值网络估计状态价值时，剩余的步骤数应该是价值网络输入的一部分。 （目前我没有将此作为输入的一部分。）这是正确的吗？我很想听听您对此的想法，如果您能给我指出任何现有的实现或完成此操作的论文，那就太好了？谢谢！   由   提交 /u/morakorvai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188jh9d/estimating_state_value_for_environments_with/</guid>
      <pubDate>Fri, 01 Dec 2023 18:56:14 GMT</pubDate>
    </item>
    <item>
      <title>强化学习如何处理多个动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188iiew/how_does_rl_work_with_multiple_motions/</link>
      <description><![CDATA[例如，这是使用Unity创建的，还是它自己的gym？ https://www.youtube.com/watch?v=SsJ_AusntiU&amp;pp=ygUncmVpbmZvcmNlbWVudCBsZ WFybmluZyB0ZWFjaCBob3cgdG8gYm94&lt; /p&gt; 从视频中很难看出。  令人困惑的是它们是如何链接在一起的，以及机器学习代理是否可以实现这一点。我了解如何训练模型站立，但如何同时站立、起身、走动和装箱？   这一切都是在数十亿次重复中通过相同的观察和相同的奖励完成的吗？  或者是否有 1 个模型学会了如何站立，1 个模型学会了如何站起来。 1 个学习如何移动的简化模型。等等。   然后这些模型使用布娃娃物理原理相互绑定。本质上，正在学习的模型上的关节与了解每个单独运动的模型绑定在一起。  我问的原因是我发现 Unity ML 代理有点有限。就像动作太随机一样，我的平均奖励几乎没有增长。这些动作是否应该在学习过程中得到改善，或者它们只是在整个学习过程中随机进行？即使是像移动到目标这样简单的事情，我也会发现代理不断地抖动并随机地来回移动   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188iiew/how_does_rl_work_with_multiple_motions/</guid>
      <pubDate>Fri, 01 Dec 2023 18:14:51 GMT</pubDate>
    </item>
    <item>
      <title>Unity 之外允许导入模型等的任何物理引擎健身房。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188iazm/any_physics_engine_gyms_outside_of_unity_that/</link>
      <description><![CDATA[我发现用于统一的 ML Agent 有点过时且有局限性，并且比机器人更适合游戏开发，我想创建自己的健身房对于像 2D 游戏这样的简单东西，它非常简单，但我想做一些 3D 机器人模型并使用稳定的基线来驱动它们，为此我需要一个具有碰撞和物理以及模型/的 3D 引擎纹理导入、一些着色，当然还有渲染。最好是也可以渲染到 webgl 环境的东西。 我假设稳定基线可以与任何 3D 开源引擎一起使用。是否有一些非常基本的东西可以与 OpenAI Gym 一起使用来导入模型、添加刚体和碰撞参数等。或者 OpenAI Gym 是否已经具备所有这些功能？ ​ ​   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188iazm/any_physics_engine_gyms_outside_of_unity_that/</guid>
      <pubDate>Fri, 01 Dec 2023 18:05:56 GMT</pubDate>
    </item>
    <item>
      <title>🚀 DIAMBRA x OROBIX：发布 SheepRL 强化学习库集成！ 🤖🎮</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188eslz/diambra_x_orobix_releasing_sheeprl_reinforcement/</link>
      <description><![CDATA[      我们很高兴宣布与高级 RL 库 SheepRL 的创建者 OROBIX 进行动态合作！ 🤝💡 🌟 为什么选择 SheepRL？ SheepRL 无缝集成到 DIAMBRA 中，为现代 RL 算法提供了一条清晰的路径，重点关注可扩展应用程序的分布式训练。 📚 入门：在我们的官方 DIAMBRA 文档中探索 SheepRL 的强大功能。 DIAMBRA Agents 存储库中包含源代码示例。 🚀 RL 探索的 Swift Launchpad： 只需几行 Python 代码，即可在我们所有的游戏上启动 RL 训练，让 AI 探索变得轻而易举！ /&gt; 👉 通过下面评论中的链接发现更多信息！ 加入我们，拥抱强化学习研究的未来。 🚀🤖✨ DIAMBRA x SheepRL&lt; /a&gt;   由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188eslz/diambra_x_orobix_releasing_sheeprl_reinforcement/</guid>
      <pubDate>Fri, 01 Dec 2023 15:36:10 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18880wg/learning_rl_research/</link>
      <description><![CDATA[大家好，我现在正在研究强化学习，我正在寻找加快学习速度的方法。我的计划是能够在该领域做好研究。现在我正在研究离线强化学习。  现在有帮助的是我们每周与我的博士同学进行一次会议。我们展示我们读过的新研究论文。轮到我的时候我会分享有关 RL 论文的内容。  但有一个问题是我们的主题非常多样化。一项研究计算机视觉，一项研究图神经网络，一项研究时间序列预测。我是唯一一个从事强化学习的人。 如果我能与更多具有相同兴趣的人交谈，那就太好了。 为了补充背景信息，我来自菲律宾在这里很难获得支持或具有专业知识的团体来指导我。当我感觉自己在学习过程中几乎是孤身一人时，就很难保持动力。虽然我在荷兰有一位教授给了我很多帮助。 您对在线小组等有什么建议吗？这可以帮助我更快地学习，包括潜在的指导、资源共享等？ 谢谢！   由   提交/u/111user222  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18880wg/learning_rl_research/</guid>
      <pubDate>Fri, 01 Dec 2023 09:44:48 GMT</pubDate>
    </item>
    <item>
      <title>“使用直接偏好优化 (DPO) 的扩散模型对齐”，Wallace 等人 2023 {Salesforce}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187wknu/diffusion_model_alignment_using_direct_preference/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187wknu/diffusion_model_alignment_using_direct_preference/</guid>
      <pubDate>Thu, 30 Nov 2023 23:27:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一周后我将采访 Rich Sutton，我应该问他什么问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187t5ir/d_im_interviewing_rich_sutton_in_a_week_what/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187t5ir/d_im_interviewing_rich_sutton_in_a_week_what/</guid>
      <pubDate>Thu, 30 Nov 2023 21:06:43 GMT</pubDate>
    </item>
    <item>
      <title>创建跨多个环境运行的单个代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187mzev/creating_a_single_agent_that_acts_across_multiple/</link>
      <description><![CDATA[大家好，先感谢您的帮助！我正在开展一个 q-learning 项目，其中单个代理正在努力优化许多（数千台）独立机器的性能。所有机器都是相同的，并在定期更新的数据库上生成相同的指标。几年前，我读过这个社区的一个帖子，听起来好像可以在多个环境中应用单个代理。鉴于这些都是相同的环境并由唯一的 ID 表示，我想知道在构建模型时是否可以应用一个简单的函数分组来在更广泛的数据数组上进行训练，而不是之前在单个数据集上进行训练尝试将其部署到所有不同的环境中。再次感谢大家的帮助！   由   提交/u/Shikaze33_3   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187mzev/creating_a_single_agent_that_acts_across_multiple/</guid>
      <pubDate>Thu, 30 Nov 2023 16:45:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么我应该使用 RLLib 而不是 stable-baselines3？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187fahg/why_should_i_use_rllib_over_stablebaselines3/</link>
      <description><![CDATA[所以我已经使用 sb3 大约 3 年了，我发现设置实验、向 RL 添加功能修改/自定义网络架构非常简单算法，使用Optuna优化参数，并在训练后进行推理。除了了解其工作原理的基本动态之外，所有这些都不需要付出什么努力。  所以在我的新公司中，他们使用 RLLib，至少可以说，它是一个怪物，可以完成我在 sb3 中轻松完成的上述任何事情。 所以争论是通常认为使用 RLLib 部署模型很容易。但这真的那么容易吗？考虑到我们可以通过 FAST api 和 docker 组合轻松甚至更快地部署使用 sb3 训练的模型，仅仅为了它提供的额外算法就值得经历 RLLib 的困难吗？对于业内使用强化学习的人来说，您的体验如何？   由   提交/u/sharafath28  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187fahg/why_should_i_use_rllib_over_stablebaselines3/</guid>
      <pubDate>Thu, 30 Nov 2023 10:14:33 GMT</pubDate>
    </item>
    </channel>
</rss>