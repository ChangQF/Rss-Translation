<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Tue, 04 Mar 2025 18:24:29 GMT</lastBuildDate>
    <item>
      <title>加强学习的注释团队？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j3d5so/annotation_team_for_reinforced_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在努力培训具有稀疏奖励的RL模型，定义正确的奖励信号很痛苦。该模型通常会陷入次优的行为，因为收到有意义的反馈花费太长。 合成奖励感觉太骇人听闻，而且不太概括。人体标记的反馈 - 有用，但超级耗时且缩放时不一致。因此，在这一点上，我正在考虑外包注释 - 但不知道该选择谁！因此，我宁愿与我们社区表现良好的人一起工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/arthurnduhiu   href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j3d5so/annotation_team_for_reinforced_learning/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j3d5so/annotation_team_for_reinforced_learning/</guid>
      <pubDate>Tue, 04 Mar 2025 15:22:00 GMT</pubDate>
    </item>
    <item>
      <title>需要：如何在RL中从头开始并创建自己的更高研究研究建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j39fmw/help_needed_how_to_start_from_scratch_in_rl_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我是最新的机器人和自动化毕业生，我计划通过基于增强的决策来攻读硕士学位，专注于增强型学习（RL）的硕士学位（RL）。作为我的应用程序过程的一部分，我需要创建一项强大的研究建议，但是我正在努力从哪里开始。 我对AI和深度学习有一个基本的了解，但是我觉得我需要一种结构化的方法来学习RL，从基础上到能够定义自己的研究问题。我的主要关注点是：   学习路径：在RL？   数学背景中，最好的资源（书籍，课程，研究论文）是什么最佳资源（书籍，课程，研究论文）：我应该专注于真正了解RL的数学主题？ （我知道一些线性代数，概率和统计数据，但可能需要改进。）  代码语言：哪种语言对RL很重要？ （我知道Python和一些C ++，目前正在学习TensorFlow框架等）  实际实现：我应该如何开始编码RL算法？是否有初学者友好的项目可以获得动手经验？提前！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/any-cry-9264      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j39fmw/help_needed_how_how_start_start_from_scratch_scratch_in_rll_rl_and/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j39fmw/help_needed_how_to_start_from_scratch_in_rl_and/</guid>
      <pubDate>Tue, 04 Mar 2025 12:15:25 GMT</pubDate>
    </item>
    <item>
      <title>RNN和重播缓冲区</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j322cf/rnns_replay_buffer/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在我看来，与MLP相比，使用带有RNN的重播缓冲区（使用RNN）这样的算法（使用RNN）的算法要复杂得多。是正确的吗？ 用MLP＆amp;一个重播缓冲液，我们可以简单地对随机S，A，R，S&#39;元组进行采样，然后对其进行训练。这使我们能够遵守IID。但这似乎是_依行的简单_更改我们的神经网络，将其转化为RNN，这使我们的训练循环非常复杂。 我想我们仍然可以从重播缓冲区中采样随机的元素，但是我们还需要具有数据，连接，＆amp;＆amp;基础架构可以运行整个步骤通过我们的RNN，以便获得我们要训练的样本？这感觉有点腥，尤其是随着政策的变化，通过我们过去经历的同一状态序列运行RNN的意义较小。 在这里通常做了什么？我的主意对吗？我们做的事情完全不同吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j322cf/rnns_replay_buffer/</guid>
      <pubDate>Tue, 04 Mar 2025 03:57:03 GMT</pubDate>
    </item>
    <item>
      <title>单集RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2zlrc/single_episode_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这可能是一个非常幼稚的问题。通常，RL涉及多个情节的学习。但是，人们是否研究了学习（大概是长长的）情节的政策的情况？例如，学习仅在一个情节上学习半cheetah sprint的政策是否有意义？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1j2zlrc/single_episode_rl/”&gt; [link]   ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2zlrc/single_episode_rl/</guid>
      <pubDate>Tue, 04 Mar 2025 01:48:09 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的演员评论家模型在每个时间步中使用分配平均值作为评估模式的动作（试图利用）时会产生相同的输出？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2wb6e/why_is_my_actor_critic_model_giving_same_output/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我为投资组合优化的问题说明实现了Advantage Actor-Critic（A2C）算法。对于训练期间的探索，我将标准偏差用作学习参数，并从分类分布中选择动作。 模型训练很好，但是在评估模式下，当我尝试测试数据时，动作在时间上没有变化，因此我的投资组合分配是恒定的。  谁能告诉为什么会发生这种情况？以及解决此问题的任何解决方案或参考。是否有任何方法可以可视化RL？ 数据：数据：5年数据的5年数据状态空间：CLAINS PRISE，MACD，RSI，HOLDINGS和POTTFOLIO VALUE。提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1j2wb6e/1j2wb6e/why_is_my_my_my_critic_model_model_giving_giving_same_output/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2wb6e/1j2wb6e/why_is_my_my_critor_critic_model_model_giving_same_same_output/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2wb6e/why_is_my_actor_critic_model_giving_same_output/</guid>
      <pubDate>Mon, 03 Mar 2025 23:09:17 GMT</pubDate>
    </item>
    <item>
      <title>RL风险般的游戏建模？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2tw29/risklike_game_modeling_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在考虑解决一些新问题。想到的是游戏风险。有趣的原因是如何为RL学习者建模游戏的问题。观察/州空间非常简单 - 国家/地区的所有权/陆军数量以及每个玩家手中的卡片。我认为的挑战是如何对动作空间进行建模，因为它可能会变得非常巨大并且几乎棘手。这是安置军队和攻击相邻国家的结合。 如果有人从事这个问题或类似问题，很想看看您如何处理动作空间。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/scprotz     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2tw29/risklike_game_modeling_for_rl/</guid>
      <pubDate>Mon, 03 Mar 2025 21:24:46 GMT</pubDate>
    </item>
    <item>
      <title>有什么办法可以处理RL动作替代吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2ra6v/is_there_any_way_to_deal_with_rl_action_overrides/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 想象我正在用RL构建一种自动驾驶汽车算法。在现实世界中，驾驶员可以覆盖自动驾驶模式。如果我的经纪人经过训练以最大程度地减少旅行时间，则代理商可能会优先考虑速度而不是舒适度 - 考虑到突然的加速度，锋利的转弯或硬制动。自然，驾驶员不会很高兴，并且可能会介入控制。 现在，如果我的环境有（i）汽车和（ii）可以干预的驾驶员，我的经纪人可能会因为所有这些替代而难以完全探索动作空间。 i 假设它最终会学会与驾驶员进行互动并为奖励进行优化，但是……可以将 forever 。  。 以前有人解决过这种问题吗？当外部干预措施不断切断探索时，如何处理RL培训的任何想法？很想听听您的想法！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/open_question4921     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2ra6v/is_there_there_any_way_way_to_to_deal_with_with_rl_rl_action_overrides/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2ra6v/is_there_any_way_to_deal_with_rl_action_overrides/</guid>
      <pubDate>Mon, 03 Mar 2025 19:36:31 GMT</pubDate>
    </item>
    <item>
      <title>Q-LEARNING在凉亭SIM中无法正确融合 - 需要帮助调试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2q6c9/qlearning_in_gazebo_sim_not_converging_properly/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2q6c9/qlearning_in_gazebo_sim_not_converging_properly/</guid>
      <pubDate>Mon, 03 Mar 2025 18:51:57 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4.5在消除游戏基准中排名第一，该游戏测试了社会推理（形成联盟，欺骗，显得无威胁和说服陪审团）。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2q61k/gpt45_takes_first_place_in_the_elimination_game/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/gwern      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2q61k/gpt45_takes_first_first_inplace_in_elimination_elimination_game/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2q61k/gpt45_takes_first_place_in_the_elimination_game/</guid>
      <pubDate>Mon, 03 Mar 2025 18:51:38 GMT</pubDate>
    </item>
    <item>
      <title>RL生物技术？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2pmsl/rl_in_biotech/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有人知道正在研究/实施RL算法的任何生物技术公司吗？与药物发现，癌症研究甚至机器人技术有关的东西  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used-eagle-9302     [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2pmsl/rl_in_biotech/</guid>
      <pubDate>Mon, 03 Mar 2025 18:30:38 GMT</pubDate>
    </item>
    <item>
      <title>对于RL项目中控制策略的输入，我应该包括重要但固定的信息吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2p4f9/for_the_observation_vector_as_input_to_the/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我正在尝试使用PPO算法来训练新型的机器人操纵器，以达到其工作空间中的目标位置。我应该将观察向量作为控制策略的输入的观察向量？当然，我应该在观察矢量中包括相关状态，例如当前的操纵器形状（关节角度）。 ，但我担心以下两个状态/信息在观察矢量中的包含：1）：最终效应子的位置，可以根据关节角度易于计算出最终效应子的位置。这是令人困惑的，因为最终效应子的位置是重要的状态/信息。它将用于计算最终效应器与目标位置之间的距离，以确定奖励，以终止情节。但是，我可以将末端效应子的位置排除在观察矢量之外，因为它可以从关节角度轻松确定。关节角度和关节角依赖性最终效应子是否形成冗余？  2）：障碍物的位置。障碍物的位置也是重要的状态/信息。它将用于计算/检测操纵器和障碍物之间的碰撞，以在检测到的碰撞时施加惩罚，如果检测到碰撞，则终止发作。但是，由于障碍物在整个学习过程中保持固定，我可以将障碍物的位置排除在观察矢量之外吗？我根本不会改变障碍物的位置。是否需要将障碍物包含在观察矢量中？ href =“ https://preview.redd.it/iblf1bo6mime1.png？ https://preview.redd.it/iblf1bo6mime1.png?width=626＆amp; format = png＆amp; auto = webpp＆s = 2b9c71e0e71e7cdaea637bad2bad2b74949e1dc08cf2cf2cf2005d88cf2005d8  发布了一个非常相似的问题 https://ai.stackexchange.com/questions/46173/the-observation-pace-space-of-a-robot-arm-should-include-include-the-t-arget-t-arget-target-position-or-inly 但没有答案。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tilly_shift7974     [link]        [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2p4f9/for_the_observation_vector_as_input_to_the/</guid>
      <pubDate>Mon, 03 Mar 2025 18:09:50 GMT</pubDate>
    </item>
    <item>
      <title>寻找帮助培训在2D电路上学习AI的强化AI（Pygame + Gym + StableBaselines3）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2n47n/looking_for_help_training_a_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家， 我正在研究一个项目，在该项目中，我需要训练AI使用加固学习来导航2D电路。代理将收到以下输入：  5传感器（射线）：向前，左，向前，向前，右，右，右右→他们返回AI和障碍物之间的距离。 作为动作的加速度值。   我已经在Pygame中具有适用于Pygame的工作环境，并且我已经对其进行了适应的工作，并且可以兼顾它。但是，当我尝试使用stablebaselines3的模型3时，我会得到一个黑屏（根据Chatgpt，这可能是由于使用DummyveCenv进行了转换所致）。 因此，如果您知道简单而快速的方法可以有效地训练AI，或者有效地训练AI，或者我可以使用预先训练的型号，我可以使用它，我可以使用它！ sc_on-&gt;＆＃32;提交由＆＃32; /u/u/pt_quill     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2n47n/looking_for_help_training_a_reinforcement/</guid>
      <pubDate>Mon, 03 Mar 2025 16:48:56 GMT</pubDate>
    </item>
    <item>
      <title>多discrete offlicy</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2myun/multidiscrete_offpolicy/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有使用Multi-Discrete（带有Gumbel）（带gumbel）的算法（例如TD3/7 DDPG）的实现，或者我注定要使用PPO，如果我想使用多discrete Actions Actions Space（和不flatten）（并且不flattent It It）/u/what_did_it_it_cost_e_t      [link]   ＆＃32;   [comment]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2myun/multidiscrete_offpolicy/</guid>
      <pubDate>Mon, 03 Mar 2025 16:42:38 GMT</pubDate>
    </item>
    <item>
      <title>与（PSO）颗粒群优化杂交多代理增强学习（MARL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2kh0m/hybridizing_multi_agent_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是我的学士学位论文的计算机科学专业学生，我的主题是“基于智能算法的决策，用于搜索和救援中的群体机器人”。我对此一无所知，因此经过一些文献综述，我想我喜欢制作PSO+MARL混合算法的想法，以使群体机器人技术更快，更适应于搜索和救援环境。但是我仍然有0个背景，我不知道这是否是个好主意，我不知道它是否可行，所以我想知道是否有人知道如何开始或我应该改变我的方法？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/amrhesham2424    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j2kh0m/hhybridizing_multi_agent_reinforection_learche_learning/”&gt; [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2kh0m/hybridizing_multi_agent_reinforcement_learning/</guid>
      <pubDate>Mon, 03 Mar 2025 14:57:06 GMT</pubDate>
    </item>
    <item>
      <title>[D]没有赢家和未知最佳分数的游戏的加强学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2e1w5/d_reinforcement_learning_for_games_with_no_winner/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在即将到来的项目中，我需要在笼子内部包装盒子和密集。但是，这些盒子将一次到达一个，并带有随机尺寸和形状。目的是尽可能地填充笼子（理想情况下是100％，但在大多数情况下这是无法达到的）。 这个问题在传统上是一个离散的优化问题，但是由于我们不知道这些包裹在到达前的包装上，所以我怀疑它们确实是正确的，而且我真的很认为，这实际上是我的一点点，如果没有我的一点点我，这实际上是一定的。以前的强化学习，但总是适用于有赢家和宽松的游戏。但是，在这种情况下，我们没有。因此，当我在游戏结束时唯一的数字是0-1之间的数字，而1是完美的，但在大多数游戏中也可能无法实现。 我认为我多次重复每个游戏。因此，您可以获得完全相同的软件包配置，因此可以与该配置上的以前的游戏进行比较，并根据模型比以前做得更好或更糟糕的是奖励该模型，但是我不确定这是否效果很好。 有人是否有这样的经验？提交由＆＃32; /u/u/alyflex     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2e1w5/d_reinforcement_learning_for_games_with_no_winner/</guid>
      <pubDate>Mon, 03 Mar 2025 08:24:06 GMT</pubDate>
    </item>
    </channel>
</rss>