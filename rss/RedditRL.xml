<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 18 Nov 2024 03:31:42 GMT</lastBuildDate>
    <item>
      <title>学习 RL 的资源？？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtt7pi/resources_for_learning_rl/</link>
      <description><![CDATA[您好，我想从头开始学习 RL。了解主要用于计算机视觉领域的深度神经网络。需要深入了解理论。我是硕士一年级学生。 如果可能，请列出理论资源，甚至从简单到复杂的模型编码。 感谢任何帮助。    提交人    /u/iInventor_0134   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtt7pi/resources_for_learning_rl/</guid>
      <pubDate>Mon, 18 Nov 2024 01:24:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么奖励规范化中的奖励在 RND 中以“相反方向”（向后）折扣？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtplc6/why_are_the_rewards_in_reward_normalisation/</link>
      <description><![CDATA[在随机网络蒸馏中，由于存在内在和外在奖励，奖励被标准化。然而，在 CleanRL 实现中，用于计算标准偏差的奖励（其本身用于标准化奖励）不会像往常一样打折。据我所知，打折的方向与通常的做法相反，我们希望远期奖励的折扣比现在奖励的折扣更大。就上下文而言，gymnasium 提供了一个 NormalizeReward 包装器，其中奖励也在“相反方向”折扣。 下面您可以看到，在 RND 的 CleanRL 实现中，奖励以正常顺序传递（即，不是从时间的最后一步到时间的第一步）。 curiosity_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in wondering_rewards.cpu().data.numpy().T]) mean, std, count = (np.mean(curiosity_reward_per_env), np.std(curiosity_reward_per_env), len(curiosity_reward_per_env),) reward_rms.update_from_moments(mean, std**2, count) wondering_rewards /= np.sqrt(reward_rms.var)  下面您可以看到负责计算折扣奖励的类，然后使用这些奖励计算 CleanRL 中奖励正则化的标准差。 class RewardForwardFilter: def __init__(self, gamma): self.rewems = None self.gamma = gamma def update(self, rews): if self.rewems is None: self.rewems = rews else: self.rewems = self.rewems * self.gamma + rews return self.rewems  在GitHub 上，RND 论文的一位作者指出 &quot;需要注意的是，为了方便起见，我们沿时间向后而不是向前进行折现（这样很方便，因为在任何时候，过去都是完全可用的，而未来尚未到来）。&quot; 我的问题是，为什么我们可以使用在&quot;相反方向&quot; 折现的奖励的标准差？ （向后）使已经（或将要）折现的奖励正常化（即，我们希望未来的相同奖励价值低于现在的相同奖励）。 另请参阅：https://ai.stackexchange.com/questions/47243/rl-why-are-the-rewards-in-reward-normalisation-discounted-in-the-opposite-dire    提交人    /u/Glass_Artist7835   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtplc6/why_are_the_rewards_in_reward_normalisation/</guid>
      <pubDate>Sun, 17 Nov 2024 22:30:54 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 任务培训建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtlmgu/advice_for_training_on_mujoco_tasks/</link>
      <description><![CDATA[您好，我正在研究一种新的非策略深度强化学习优先级方案。 我从可靠的存储库中获得了 SAC 和 TD3 的 torch 实现。我使用原始 ER、PER 和我的方法在 Hopper-v5 和 Ant-v5 上进行实验。我在 3 个种子上运行实验。我训练 250k 或 500k 步以查看训练进展如何。我通过运行代理 10 集并平均每 2.5k 步奖励一次来进行评估。我使用与 SAC 和 TD3 的论文和官方实现相同的超参数。 我注意到评估分数中存在非常不规则的模式。这些曲线看起来不稳定，非常好的评估分数在某些步骤后突然下降。它多次上升和下降。这种不稳定的行为也存在于原始 ER 版本中。我从他们的官方存储库中获取了 TD3 和 SAC，因此我对这些评估分数感到困惑。这是正常的吗？在论文中，评估分数的行为更加单调。我应该为每个 Mujoco 任务搜索超参数吗？    提交人    /u/TheMefe   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtlmgu/advice_for_training_on_mujoco_tasks/</guid>
      <pubDate>Sun, 17 Nov 2024 19:36:01 GMT</pubDate>
    </item>
    <item>
      <title>DDQN 无法收敛，可能出现灾难性遗忘</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtihzc/ddqn_not_converging_with_possible_catastrophic/</link>
      <description><![CDATA[      我正在训练 DDQN 代理进行股票交易，从下面的损失可以看出，在前 30k 步中，损失正在减少，然后直到 450k 步，模型似乎不再收敛 https://preview.redd.it/yc6o9wynwh1e1.png?width=592&amp;format=png&amp;auto=webp&amp;s=aef476c7eb177f82fe112d0d1fd5a95ef90c6917 此外，从投资组合价值的进展情况可以看出，模型似乎忘记了每集所学的内容。 这些是我的超参数，请注意，我使用的是固定的情节长度= 50k 步，并且每个情节都从一个随机点开始  learning_rate=0.00001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update=1000, buffer_capacity=20000, batch_size=128,  可能是什么问题以及如何解决它？    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtihzc/ddqn_not_converging_with_possible_catastrophic/</guid>
      <pubDate>Sun, 17 Nov 2024 17:19:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助需求：如何在不造成训练推理差距的情况下为 RLHF 中的每个标记分配奖励分数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gth0yi/helped_needs_how_to_assign_reward_scores_to_each/</link>
      <description><![CDATA[在 RLHF 中，我一直在努力解决如何有效地将奖励分数分配给各个标记的问题。 奖励模型通常使用成对比较进行训练，输出一个评估句子整体质量的单个标量。然而，在 RLHF 期间，为了训练价值函数（用于 PPO 等技术），我们需要计算累积奖励： $$Rt = \sum{t’=t&gt;T r(s{t’}, a{t’})$$ 这是我的主要问题： - 我们如何将这个句子级别的奖励分解为 token 级别的奖励？ 我正在考虑的一种简单方法是： - 直接将训练有素的线性层应用于每个 token 的隐藏状态以预测其奖励分数。 但是，我担心这可能会引发两个主要问题：1. 训练推理差距：奖励模型经过训练以评估整个句子，但这种逐个 token 的分解可能会与 RM 的原始训练设置不同。 2. 性能下降：推理过程中的奖励分布可能与真正的奖励信号不一致，从而可能损害策略优化。 我正在寻找社区的建议或见解： - 有没有更好的方法将句子级奖励分解为 token 级分数？ - 我们如何验证 token 奖励分解的有效性？ 我非常感谢任何想法或建议。谢谢！    提交人    /u/ForJadeForest   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gth0yi/helped_needs_how_to_assign_reward_scores_to_each/</guid>
      <pubDate>Sun, 17 Nov 2024 16:16:41 GMT</pubDate>
    </item>
    <item>
      <title>常规 RL 和 LORA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtb77g/regular_rl_and_lora/</link>
      <description><![CDATA[是否有任何 GitHub 示例用于微调常规 ppo，例如使用 lora 解决简单的 rl 问题？就像从一个 Atari 游戏到另一个  编辑用例：假设您有一个问题，其中有很多初始条件，如速度、方向等……95% 的初始条件已得到解决，5% 无法解决（尽管它们是可解的）但是您很少遇到它因为它只有 5% 的“样本”所以现在你想更多地训练这 5%，并且在训练期间增加它的数量..并且你不想“忘记”或破坏以前的成功。（这主要针对开启策略而不是具有高级回复缓冲区的关闭策略）...    提交人    /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtb77g/regular_rl_and_lora/</guid>
      <pubDate>Sun, 17 Nov 2024 11:00:11 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Lab 中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</link>
      <description><![CDATA[您好，我是在模拟中训练机器人的新手。我刚刚设置了我的 isaac 实验室，但我不知道如何在其中训练我自己的模型。关于它的文档也不多（我知道 NVidia 文档，但就是这样）。有人能为我提供更多关于如何入门的信息吗？另外，没有关于它的教程/视频/文档，因为它是新的还是不好的？它什么时候向公众开放的？谢谢！    提交人    /u/North_Set8162   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</guid>
      <pubDate>Sat, 16 Nov 2024 23:06:43 GMT</pubDate>
    </item>
    <item>
      <title>银行业有趣的研究课题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</link>
      <description><![CDATA[我目前是一名兼职的计算机科学硕士 (ML 专业) 学生，在银行业担任数据工程师，我计划通过在银行业务场景中应用 RL 代理来撰写研究报告。我能想到一些事情，比如贷款决策或欺诈检测，但对我来说没有什么真正有趣的。有什么建议可以告诉我可以研究什么吗？我理想情况下想要一些我们有一些开源数据的东西。    提交人    /u/SatwikGu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</guid>
      <pubDate>Sat, 16 Nov 2024 22:33:22 GMT</pubDate>
    </item>
    <item>
      <title>“可解释的对比蒙特卡洛树搜索推理”，Gao 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</guid>
      <pubDate>Sat, 16 Nov 2024 21:40:25 GMT</pubDate>
    </item>
    <item>
      <title>人的手臂</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</link>
      <description><![CDATA[你好。我想制作一个人体手臂模型，并使用强化学习使其达到目标。 我知道这很难实现（如果可能的话，大量的 DOF、较长的训练时间），所以我尝试使用简单的模型和完全增加的模型来构建它。 如果需要，我很乐意制作自己的 urdf 模型，但也很乐意使用已经存在的东西。 你会推荐从哪里开始？最好的算法是什么（可能是 PPO、SAC、DDPG）？最好的平台是什么（可能是 pybullet、MuJoCo、ROS 和 Gazebo）？ 任何帮助表示感谢。    提交人    /u/tedthemouse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</guid>
      <pubDate>Sat, 16 Nov 2024 20:20:38 GMT</pubDate>
    </item>
    <item>
      <title>为研究论文编写方程式并组织人员</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</link>
      <description><![CDATA[大家好，我目前是强化学习和迁移学习领域的博士生。我正在准备写我的第一篇论文，写方程式及其证明、推导等感觉很不舒服。我想知道经验丰富的研究人员是如何做到的？他们使用什么样的工具？在整个项目中，他们如何不断写下所有这些数学符号和方程式，他们如何呈现它们、跟踪它们，并同时维护多个项目。对于工具，你们使用 iPad 之类的工具吗？我理解 overleaf 的用途，但我觉得亲手写它们更有成就感。你们能分享一下你们是如何用数学和代码等开发系统的吗？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</guid>
      <pubDate>Sat, 16 Nov 2024 17:01:50 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么关于训练 ppo/dqn 解决迷宫的技巧？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gslsxc/any_tips_for_training_ppodqn_on_solving_mazes/</link>
      <description><![CDATA[创建了自己的 gym 环境，其中观察结果由一个形状为 4 的 numpy 数组 (agent_x,agent_y,target_x,target_y) 组成。代理获得的基本奖励为 (distancebefore - distanceafter)（使用 astar），每一步为 -1 或 0 或 1，到达目标时获得奖励 = 100，与墙壁碰撞时获得奖励 = -1（如果我使用 distancebefore - distanceafter，则为 0）。 我正在尝试训练 ppo 或 dqn 代理（尝试了两者）来解决带有墙壁的 10x10 迷宫 你们有什么技巧可以让我尝试，以便我的代理可以在我的环境中学习？ 欢迎提供任何帮助和提示，我之前从未在迷宫上训练过代理，我想知道是否有什么特别需要考虑的。如果有其他模型更好，请告诉你 如果我的代理始终从左上角开始，而目标始终在右下角，则 dqn 可以解决它而 ppo 不能，然而，在我的用例中我想解决的是一个迷宫，每次调用 reset() 时代理都从随机位置开始。这个迷宫能解决吗？ （ppo 似乎也试图穿过障碍物，就像它由于某种原因无法检测到它们一样） 我理解，每次固定代理和目标位置时，dqn 都需要学习一条路径，但是如果代理位置每次重置时都会发生变化，则需要学习许多正确的路径。 墙壁总是固定的。 我对模型使用 baselines3 （我也尝试了 sb3_contrib qrdqn 和循环 ppo） https://imgur.com/a/SWfGCPy    提交人    /u/More_Peanut1312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gslsxc/any_tips_for_training_ppodqn_on_solving_mazes/</guid>
      <pubDate>Sat, 16 Nov 2024 11:59:30 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习 DEEPRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsjv66/transfer_learning_deeprl/</link>
      <description><![CDATA[你好， DeepRl 中的迁移学习/领域适应的最新进展是什么？ 谢谢！☺️    提交人    /u/TeamTop4542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsjv66/transfer_learning_deeprl/</guid>
      <pubDate>Sat, 16 Nov 2024 09:35:00 GMT</pubDate>
    </item>
    <item>
      <title>找到达到目标的最少移动次数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsii5c/finding_the_minimum_number_of_moves_to_a_goal/</link>
      <description><![CDATA[我是强化学习的新手。我想使用 RL 作为练习来解决 15 个难题 https://en.m.wikipedia.org/wiki/15_puzzle。第一个问题是随机移动会非常缓慢地到达解决状态。所以我想我可以从解决状态开始，进行少量移动，训练代理来解决这个问题，然后慢慢地从解决状态进行越来越多的移动。 我打算使用稳定的基线 3。我不确定我的想法是否可以使用该库进行编码，因为它必须以某种方式记住训练过的代理，并且每次我增加从解决状态开始的移动次数时，都从该点继续训练。 这个想法看起来合理吗？   由    /u/MrMrsPotts  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsii5c/finding_the_minimum_number_of_moves_to_a_goal/</guid>
      <pubDate>Sat, 16 Nov 2024 07:51:55 GMT</pubDate>
    </item>
    <item>
      <title>一款用于多智能体模仿学习和强化学习的开源 2D 版《反恐精英》，全部采用 Python 编写</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gs71k5/an_opensource_2d_version_of_counterstrike_for/</link>
      <description><![CDATA[      SiDeGame（简化的拆弹游戏）是我 3 年前的一个项目，我最终想与大家分享一下，但一直推迟，因为我心里还有一些更新的想法。现在我必须承认我手头上有太多新工作，所以就在这里： 游戏 GIF 该项目的最初目的是为我的硕士论文创建一个 AI 基准环境。从人工智能的角度看，我对 CS 感兴趣的原因有以下几个：  共享经济（玩家可以为他人购买和丢弃物品）， 不确定的角色（每个人在游戏开始时都具有相同的能力和可用物品）， 不完善的盟友信息（第一人称视角限制了对队友信息的访问）， 双峰感知（声音是重要的信息来源，特别是在没有视觉效果的情况下）， 标准化（游戏规则很少改变）， 直观的界面（易于保持一致以进行人机对比）。  起初，我考虑与 CSGO 甚至 CS1.6 的实际游戏进行交互，但后来决定从头开始制作我自己的版本，这样我就可以了解所有的细节，然后根据需要进行更改。我只有一年的时间来做这件事，所以我选择用 Python 来做所有事情——这是我以及 AI 社区中的许多人最熟悉的语言，我认为以后可以提高效率。 有几种方法可以训练 AI 玩 SiDeGame：  模仿学习：让人类玩一些在线游戏。网络历史记录将被记录下来，并可用于重新模拟会话、提取输入输出标签、统计数据等。代理通过监督学习进行训练，以克隆玩家的行为。 本地 RL：使用游戏的同步版本手动踏入并行环境。代理通过反复试验进行强化学习训练。 远程 RL：将参与者客户端连接到远程服务器并让代理实时自我游戏。  作为 AI 基准，我仍然认为它不完整。我不得不匆忙进行模仿学习，直到最近我才重写了强化学习示例以使用我经过测试的实现。现在我可能不会再独自进行任何重大工作了，但我认为作为一个开源在线多人伪 FPS 学习环境，它对 AI 社区来说仍然很有趣。 以下是链接：  代码：https://github.com/jernejpuc/sidegame-py 简短会议论文：https://plus.cobiss.net/cobiss/si/en/bib/86401795（4 页英文，联合 PDF 的一部分，80 MB） 完整论文：https://repozitorij.uni-lj.si/IzpisGradiva.php?lang=eng&amp;id=129594（90 页斯洛文尼亚语，PDF 8 MB）     由   提交  /u/yerney   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gs71k5/an_opensource_2d_version_of_counterstrike_for/</guid>
      <pubDate>Fri, 15 Nov 2024 21:20:43 GMT</pubDate>
    </item>
    </channel>
</rss>