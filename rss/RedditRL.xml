<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 26 Dec 2024 12:32:25 GMT</lastBuildDate>
    <item>
      <title>强化问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmk9pm/reinforcement_problem/</link>
      <description><![CDATA[我忍不住将我 8 个月大的宝宝当作强化学习问题来对待。设计适当的环境和奖励。只需要研究一种算法……    由    /u/tedthemouse  提交  [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmk9pm/reinforcement_problem/</guid>
      <pubDate>Thu, 26 Dec 2024 08:33:09 GMT</pubDate>
    </item>
    <item>
      <title>GAE 和 Actor Critic 方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmepx8/gae_and_actor_critic_methods/</link>
      <description><![CDATA[      我使用单独的参与者和评论家网络实现了相当经典的 GAE 方法。在 CartPole 任务上测试，使用的批处理大小为 8。看起来只有 GAE(lambda=1) 或接近 1 的某个 lambda 才能使参与者模型起作用。这相当于使用经验奖励来计算 td 误差（我对此进行了单独的实现，结果看起来几乎相同）。  任何较小的 lambda 值基本上都不起作用。预期的情节长度（达到步骤的批次平均值）要么永远不会大于 40；要么显示非常坎坷的曲线（在达到相当大的步骤数后迅速变得更糟）；或者只是收敛到一个非常小的值，例如低于 10。  我试图了解这是否是“预期的”。我理解我们不希望策略损失保持/收敛到 0（无论其质量如何都成为确定性策略）。这实际上发生在较小的 lambda 值上。  这纯粹是由于偏差-方差权衡吗？对于较大的（或 1.0）lambda 值，我们期望偏差较低但方差较高。从 Sergey Levine 的课程来看，我们似乎希望总体上避免这种情况？然而，这种“经验蒙特卡罗”方法似乎是唯一适合我的情况的方法。 此外，我们应该监控策略梯度方法的哪些指标？从我目前的观察来看，策略网络的损失或评论模型损失几乎毫无用处……唯一重要的事情似乎是预期的总回报？  分享一些我的 tensorboard 的截图： https://preview.redd.it/x7bcpud9s39e1.png?width=1572&amp;format=png&amp;auto=webp&amp;s=8dec61d8a3f0f0f1a4798a2da7fd15c5d0e7a23a    提交人    /u/encoreway2020   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmepx8/gae_and_actor_critic_methods/</guid>
      <pubDate>Thu, 26 Dec 2024 02:22:35 GMT</pubDate>
    </item>
    <item>
      <title>超大的观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hm5cnh/extremely_large_observation_space/</link>
      <description><![CDATA[根据标题，我一直在解决一个观察空间为 5 元组的问题，对于元组内的所有元素，低-高都是 int 0-100。动作空间只有离散的 3。 以前有人处理过这么大的空间吗？你认为哪种神经网络模型/管道能产生最好的结果？    提交人    /u/Neither_Canary_7726   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hm5cnh/extremely_large_observation_space/</guid>
      <pubDate>Wed, 25 Dec 2024 17:52:00 GMT</pubDate>
    </item>
    <item>
      <title>以 25 美元的价格购买 1 年 Perplexity Pro（正常价格：200 美元）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlzekm/get_perplexity_pro_1_year_for_25_normal_price_200/</link>
      <description><![CDATA[嗨， 我的服务提供商提供一项优惠，让我可以以 25 美元的价格使用 Perplexity Pro 一年 - 通常价格为 200 美元/年（约 75% 折扣） 我有大约 27 个促销代码，应在 12 月 31 日前兑换。 加入拥有 600 多名成员的 Discord，我将发送一个促销代码，您可以兑换。 我接受 PayPal 来保护买家，并接受加密货币来保护隐私。 我还有 LinkedIn Career Premium、Spotify Premium 和Xbox GamePass Ultimate。 再次感谢！    由    /u/minemateinnovation 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlzekm/get_perplexity_pro_1_year_for_25_normal_price_200/</guid>
      <pubDate>Wed, 25 Dec 2024 11:52:18 GMT</pubDate>
    </item>
    <item>
      <title>在世界模型中想象国家推出有什么好处？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlz6p3/what_is_the_benefit_of_imagined_state_rollouts_in/</link>
      <description><![CDATA[      大家好 :) 我有一个关于例如 https://arxiv.org/pdf/1803.10122 或 https://arxiv.org/pdf/1811.04551 中想象状态轨迹背后动机的问题。对我来说，这一切都说得通，我们如何做到这一点以及背后的原因对我来说也很清楚。但我仍然无法弄清楚为什么使用“模拟”的模型会更好未来的轨迹（在潜在空间还是在像素空间，无关紧要），当我们有机会以相同的成本与环境交互时，甚至更便宜（环境查询与通过 LSTM 等顺序模型的前向传递）。我们只会尝试重建已经存在的事物？ 我的意思是，这在交互成本很高的环境中是有意义的，但本文使用的示例大多是 OpenAI-Gym 环境，运行起来非常便宜。  Schmidhuber 和 Ha 在 World Model 论文中使用的算法 https://preview.redd.it/ahns2m2ddz8e1.png?width=1040&amp;format=png&amp;auto=webp&amp;s=6d5a1c7d579c98701cd7bd2afad8f27fd4b5f871 也在环境中执行该步骤。我看不出在这里使用顺序生成模型有什么好处？我们也可以使用一个非常强大的状态编码器来捕获过去的 k 个观测值。  也许，RNN 的顺序性质为我们提供了 h 中的更多信息，但是，我们也可以使用一个编码器来做到这一点，该编码器将过去的 k 个观测值映射到潜在空间，而无需任何世界模型。 那么，为什么我们要构建一个尝试重建可用数据的世界模型？    提交人    /u/No_Individual_7831   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlz6p3/what_is_the_benefit_of_imagined_state_rollouts_in/</guid>
      <pubDate>Wed, 25 Dec 2024 11:34:53 GMT</pubDate>
    </item>
    <item>
      <title>任何结合 RL 和 LLM 的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hly9nh/any_work_present_combining_rl_llms/</link>
      <description><![CDATA[有人知道结合 RL 和 LLM 的一些工作吗？我已经看到了一些可以使用但到目前为止还没有实际应用的提议方法。     提交人    /u/Wide-Chef-7011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hly9nh/any_work_present_combining_rl_llms/</guid>
      <pubDate>Wed, 25 Dec 2024 10:21:53 GMT</pubDate>
    </item>
    <item>
      <title>寻求指导：将 RL 应用于控制器设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hly2e9/looking_for_guidance_applying_rl_for_controller/</link>
      <description><![CDATA[大家好， 首先，祝整个 RL 社区圣诞快乐！🎄 我是一名控制理论家，在数学建模、经典控制、最优控制和刚体动力学方面有 7 年的经验。最近，我对探索如何应用强化学习 (RL) 算法来设计在不确定环境中表现出色的控制器产生了浓厚的兴趣。 我已经迈出了这一旅程的第一步，但我对将我的控制理论背景与 RL 领域联系起来的最佳方式感到有点迷茫。我希望找到一个结构化的路线图或实用建议来指导我走这条路。 如果您走过类似的道路，或者对课程、研究论文或其他可能有帮助的资源有任何建议，我将不胜感激。听到您关于浏览这个空间的经历或技巧对我来说也意义重大。 提前谢谢您！    提交人    /u/ValueSeekerAgent   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hly2e9/looking_for_guidance_applying_rl_for_controller/</guid>
      <pubDate>Wed, 25 Dec 2024 10:05:14 GMT</pubDate>
    </item>
    <item>
      <title>PPO 算法中总损失是如何使用的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hllhps/how_is_total_loss_used_in_ppo_algorithm/</link>
      <description><![CDATA[在 PPO 中，有两种损失：策略损失和价值损失。价值损失用于优化价值函数，而策略损失用于优化策略函数。但策略和价值损失（带有系数参数）结合在总损失函数中。 总损失函数有什么作用？我理解每个网络都使用自己的损失进行优化。那么总损失优化了什么？ 或者我理解错了，两个网络都使用相同的总损失进行优化，而不是使用各自的损失？    提交人    /u/BitShifter1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hllhps/how_is_total_loss_used_in_ppo_algorithm/</guid>
      <pubDate>Tue, 24 Dec 2024 20:10:43 GMT</pubDate>
    </item>
    <item>
      <title>具有离线 RL 的 GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlh6v7/gnn_with_offline_rl/</link>
      <description><![CDATA[我想使用离线 RL，即不与环境交互，仅使用过去的数据，这些数据可以组织为经验 (s、a、s&#39;、r)。代理 - 使用 Pytorch Geometric 的 GNN。状态 - 我使用 Pytorch Geometric 的 HeteroData 类型，这是一个异构图。算法 - CQN（保守 Q 学习）。动作空间 - 离散。奖励 - 仅在每集结束时。 有谁知道哪个 RL 框架可以最轻松地进行定制，而不必深入研究？ 到目前为止，我知道有 rllib、torchRL、d3RL、cleanRL、stable baselines、tianshou 几年前我只使用过稳定基线，做我需要的定制需要付出很多努力。我希望这次能避免这种情况。也许最好从头开始写？     由    /u/Aggravating_Rip_1882  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlh6v7/gnn_with_offline_rl/</guid>
      <pubDate>Tue, 24 Dec 2024 16:37:40 GMT</pubDate>
    </item>
    <item>
      <title>预测图块地图的缺失部分。（我需要使用 RL 吗？）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlgx3v/predict_missing_parts_of_tile_map_do_i_need_to/</link>
      <description><![CDATA[      大家好，我正在尝试制作一个可以预测 11x11 图块地图缺失部分的代理。我有无数张这样的地图需要训练。每张地图都已完全完成，包含所有数据（无雾），但我希望它能够预测我移除的图块。每张图块都有地形、资源和改进。我是否需要对部分地图进行雾化并使用 RL，或者我可以使用某种不需要从环境中学习的数据预测模型？这是地图的图像。 https://preview.redd.it/lpj59bxhot8e1.png?width=706&amp;format=png&amp;auto=webp&amp;s=fec19e3e9f5ab70c1e5d8ef11206d99c4d06e2b5    提交人    /u/Kingofath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlgx3v/predict_missing_parts_of_tile_map_do_i_need_to/</guid>
      <pubDate>Tue, 24 Dec 2024 16:24:02 GMT</pubDate>
    </item>
    <item>
      <title>如何实现“多智能体强化学习独立与合作智能体”？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlgpom/how_to_implement_multiagent_reinforcement/</link>
      <description><![CDATA[我具备强化学习和 Python 的基础知识。一位教授让我实现这篇论文。关于如何实现这一点以及从头开始学习哪些资源，您有什么建议吗？    提交人    /u/t_sia   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlgpom/how_to_implement_multiagent_reinforcement/</guid>
      <pubDate>Tue, 24 Dec 2024 16:13:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在基于 VAPI 的语音 AI 系统中创建/添加 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlg2ue/how_to_createadd_rl_in_vapibased_voice_ai_system/</link>
      <description><![CDATA[我有一个基于 VAPI 的 AI 语音助手，用于我的咨询业务，目前已与 Twilio（电话）、Deepgram（语音识别）、GPT-4（语言理解）和 ElevenLabs（文本转语音）集成。此外，此语音助手集成到 GoHighLevel CRM 系统中以存储客户信息。我想通过两个关键功能增强 AI 语音助手：1. 强化学习 (RL)，从用户交互中学习并不断改进响应。  检索增强生成 (RAG) 以确保从知识库（例如常见问题解答、政策文档或矢量数据库）获得事实和有根据的答案。  您能否：•提供将 RL 添加到我现有的 VAPI AI 工作流程中的分步说明示例？ •推荐一个向量数据库（FAISS、Pinecone 等）并概述如何为 RAG 构建检索管道？ •分享处理来自 Twilio/Deepgram 的语音数据、收集用户反馈以及定期更新模型的最佳实践？ 我的目标是让语音助手在每次通话中都更加准确、更具适应性。谢谢！    提交人    /u/IntelligentOil2047   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlg2ue/how_to_createadd_rl_in_vapibased_voice_ai_system/</guid>
      <pubDate>Tue, 24 Dec 2024 15:42:01 GMT</pubDate>
    </item>
    <item>
      <title>玩转人工智能：探索 Pistonball 中集体行为的魔力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hle8rw/fun_with_ai_discovering_the_magic_of_collective/</link>
      <description><![CDATA[      在本文中，我将分享我在 Pistonball 游戏中进行的一项小实验的惊人发现。一个简单的调整导致了 AI 代理之间意想不到的合作，揭示了多代理系统中集体行为的迷人见解。想知道发生了什么吗？快来看看吧！    提交人    /u/RyanlovesAI   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hle8rw/fun_with_ai_discovering_the_magic_of_collective/</guid>
      <pubDate>Tue, 24 Dec 2024 14:07:09 GMT</pubDate>
    </item>
    <item>
      <title>“搜索和学习的扩展：从强化学习角度重现 o1 的路线图”，Zeng 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlchn7/scaling_of_search_and_learning_a_roadmap_to/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlchn7/scaling_of_search_and_learning_a_roadmap_to/</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:44 GMT</pubDate>
    </item>
    <item>
      <title>我训练了一个强化学习代理来玩索尼克。希望得到一些反馈。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hl7pjf/i_trained_a_reinforcement_learning_agent_to_play/</link>
      <description><![CDATA[      我最近训练了一个 AI 来玩刺猬索尼克游戏。我写了一篇关于它的 LinkedIn 文章。但我最近一直在观看索尼克游戏，发现索尼克不仅仅是一款速度跑酷游戏。关卡中有许多很酷的隐藏角落和路径，如果玩家像玩马里奥游戏一样玩这些游戏，可能会错过。我很想收集一些关于你如何玩索尼克的反馈，以及你认为 AI 代理应该如何玩它。我现在只关注第一款游戏。你会针对不同的区域（绿山、大理石等）使用不同的策略吗？    提交人    /u/throwaway-bib   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hl7pjf/i_trained_a_reinforcement_learning_agent_to_play/</guid>
      <pubDate>Tue, 24 Dec 2024 06:34:32 GMT</pubDate>
    </item>
    </channel>
</rss>