<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 13 Dec 2024 09:18:55 GMT</lastBuildDate>
    <item>
      <title>寻求强化学习（RL）个人项目的想法和指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd5ef7/looking_for_ideas_and_guidance_for_personal/</link>
      <description><![CDATA[大家好！ 我刚刚完成硕士课程的第一年，并获得了计算机科学学士学位，主修人工智能。在过去的几年里，我通过工作、实习和研究获得了丰富的经验，特别是在我真正喜欢的领域，比如应用于车辆和无人机系统的强化学习 (RL)。 现在，我希望深入研究 RL 中的个人项目，以探索新的想法并加深我的知识。您对有趣的基于 RL 的个人项目有什么建议吗？我特别喜欢涉及机器人、无人机或自主系统的项目，但我也愿意接受任何有创意的建议。 此外，我很想听听关于如何开始个人 RL 项目的建议——您会向处于我这个位置的人推荐哪些工具、框架或资源？我认为自己非常精通 Python 及其相关知识。 提前感谢您的想法和提示！    提交人    /u/CJPeso   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd5ef7/looking_for_ideas_and_guidance_for_personal/</guid>
      <pubDate>Fri, 13 Dec 2024 05:30:06 GMT</pubDate>
    </item>
    <item>
      <title>学术背景调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcw8aa/academic_background_poll/</link>
      <description><![CDATA[大家好， 出于好奇，我想看看这里社区的背景分布是怎样的。 ​ 查看投票    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcw8aa/academic_background_poll/</guid>
      <pubDate>Thu, 12 Dec 2024 21:42:12 GMT</pubDate>
    </item>
    <item>
      <title>寻找远程实习：2025 年冬季</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcqe00/looking_for_remote_internship_winter_2025/</link>
      <description><![CDATA[大家好！ 我是来自印度的机器学习专业三年级博士生，专攻强化学习。我还是 Google DeepMind 的学生研究员，将于 2025 年加入 Adob​​e 进行暑期实习。 我正在寻找 2025 年冬季远程学生研究员职位，研究与 多臂老虎机 (MAB) 和 马尔可夫决策过程 (MDP) 相关的问题。 我的研究重点是开发用于老虎机优化和强化学习的有效算法，并在成本敏感型决策和策略优化中具有实际应用。我还可能通过涉及在 LLM 背景下应用老虎机的项目亲身体验 LLM 的实践。 如果您的组织正在研究类似的问题并有合作机会，我很乐意做出贡献。请随时给我发私信或分享相关线索。 感谢您的时间和考虑！    提交人    /u/Fantastic-Nerve-4056   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcqe00/looking_for_remote_internship_winter_2025/</guid>
      <pubDate>Thu, 12 Dec 2024 17:32:10 GMT</pubDate>
    </item>
    <item>
      <title>在整个轨迹中改变观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</link>
      <description><![CDATA[嗨， 有人知道关于在轨迹过程中代理的观察空间的场景的任何先前工作吗？ 例如，如果具有多个传感器的机器人决定在轨迹期间转动其中一个传感器（可能出于能量考虑）。  据我所知，最常用的算法没有考虑到轨迹过程中观察空间的变化。 很想听听大家的想法    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</guid>
      <pubDate>Thu, 12 Dec 2024 14:20:01 GMT</pubDate>
    </item>
    <item>
      <title>可视化环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcknsm/visualizing_environments/</link>
      <description><![CDATA[如何将体育馆环境可视化？有哪些好的做法？    提交人    /u/Main-Bit-6404   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcknsm/visualizing_environments/</guid>
      <pubDate>Thu, 12 Dec 2024 13:04:59 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 MATD3 和 MADDPG 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</link>
      <description><![CDATA[问候， 我需要在某些环境中运行这两个算法（无所谓）来证明多智能体学习确实有效！（是的，这非常简单，但很难！） 问题就在这里。找不到一个单一的框架来在环境中植入算法（现在基本上是宠物动物园 mpe）， 我做了一些研究：  Marllib 没有很好的文档记录。最后我还是搞不懂。 agileRL 很棒但是有一个 bug 我无法解决，（如果您能解决这个 bug 请告诉我）。 Thianshou ，我必须植入算法！！ CleanRL，嗯...我没搞懂。我的意思是我应该在我的主脚本中使用这些算法 .py 文件吗？  好吧请帮忙.......... 带着爱    提交人    /u/matin1099   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</guid>
      <pubDate>Thu, 12 Dec 2024 11:50:04 GMT</pubDate>
    </item>
    <item>
      <title>针对机械臂 RL 项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</link>
      <description><![CDATA[嗨，我正在寻找一些论文或想法，我可以在硬件机器人操纵器上尝试，以获得 RL 控制和规划方面的经验。     提交人    /u/Stunning-Stable-8553   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</guid>
      <pubDate>Thu, 12 Dec 2024 02:57:44 GMT</pubDate>
    </item>
    <item>
      <title>体育馆/mujoco 教程需要四足机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</link>
      <description><![CDATA[大家好，我正在做一个关于四足机器狗的项目。我正在尝试使用 gymnasium 和 MuJoCo，但在 gymnasium 上设置自定义环境真的很令人困惑。我正在寻找一个教程，这样我就可以学习如何设置它，或者是否有人建议我应该切换我正在使用的工具。    提交人    /u/PrincipleDistinct425   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</guid>
      <pubDate>Wed, 11 Dec 2024 21:48:47 GMT</pubDate>
    </item>
    <item>
      <title>Rl 用于自主导航</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</link>
      <description><![CDATA[大家好，我打算在我的最后一年项目中使用 ubuntu 22.04 和 ros2 hum，该项目包括使用 DRL 技术在 Gazebo 中为 turtlebot3 机器人进行自主导航，但我真的很担心我的笔记本电脑的容量，我真的没有合适的笔记本电脑，我有一台第 8 代英特尔 i5，8 核，我真的会发现学习问题吗？    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</guid>
      <pubDate>Wed, 11 Dec 2024 19:10:28 GMT</pubDate>
    </item>
    <item>
      <title>彼得·墨菲 (Peter Murphy) 的最新强化学习书籍有多好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</link>
      <description><![CDATA[编辑：应该是 Kevin Murphy。 我的一位同事推荐了 https://arxiv.org/pdf/2412.05265。 我发现它有点像洗衣清单，就像其他强化学习调查的情况一样。不同的想法感觉就像反复试验。我自己过去也曾在 TensorFlow 中编写过强化学习。但很难真正感受到它的威力。我有数学背景，我只是不确定是否值得花时间阅读这么厚的书，因为我知道我可能记不住很多东西，除非所有不同的概念形成连贯的意识流。换句话说，我发现这个主题没有足够的基础来轻松消化第一原理。 我对其他人对这个问题的看法很好奇，特别是从第一原理的角度来看。    提交人    /u/Crazy_Suspect_9512   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</guid>
      <pubDate>Wed, 11 Dec 2024 18:43:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Stable Baselines 3 中的训练期间动态修改超参数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/</link>
      <description><![CDATA[我正在使用 Stable Baselines 3，并尝试实施一个训练过程，在该过程中我在训练的不同阶段动态地改变超参数。具体来说，我正在使用 PPO 并想要更改 gamma 参数。 这是我正在尝试执行的操作的简化版本： ```py from stable_baselines3 import PPO # 初始训练 model = PPO(&quot;MlpPolicy&quot;, &quot;CartPole-v1&quot;, gamma=0.99) model.learn(total_timesteps=10000) print(f&quot;初始 gamma：{model.gamma}&quot;) print(f&quot;初始推出缓冲区 gamma：{model.rollout_buffer.gamma}&quot;) # 尝试更改 gamma model.gamma = 0.95 model.learn(total_timesteps=10000) print(f&quot;更改后 - 模型 gamma：{model.gamma}&quot;) print(f&quot;更改后 -推出缓冲区 gamma：{model.rollout_buffer.gamma}&quot;)  ``` 输出： ```py 初始 gamma：0.99 初始推出缓冲区 gamma：0.99 更改后 - 模型 gamma：0.95 更改后 - 推出缓冲区 gamma：0.99  ``` 我们可以看到，更改 model.gamma 不会更新所有必要的内部状态。 model.rollout_buffer.gamma 保持不变，这可能导致不一致的行为。 我考虑过使用新参数保存并重新加载模型： ```py model.save(&quot;temp_model&quot;) model = PPO.load(&quot;temp_model&quot;, gamma=0.95) model.learn(total_timesteps=10000) print(f&quot;重新加载后 - 模型 gamma：{model.gamma}&quot;) print(f&quot;重新加载后 - 滚动缓冲区 gamma：{model.rollout_buffer.gamma}&quot;)  ``` 输出： ```py 重新加载后 - 模型 gamma：0.95 重新加载后 - 滚动缓冲区 gamma：0.95  ``` 这种方法有效但似乎效率低下，特别是如果我想在训练期间频繁更改参数时。 在 Stable Baselines 3 中，是否有一种适当的方法可以在训练期间动态更新像 gamma 这样的超参数？理想情况下，我想要一个解决方案，确保所有相关的内部状态都得到一致更新，而无需保存和重新加载模型。 对于这种情况的任何见解或最佳实践都将不胜感激。    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/</guid>
      <pubDate>Wed, 11 Dec 2024 13:13:23 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 在我的用例中存在问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</link>
      <description><![CDATA[大家好， 这是我第一次从事 RL 项目，我正在构建一个可以与特定 DLT 一起使用的模型。具体来说，我希望它选择最佳数量的块来通过特定的 DLT 发送消息。我尝试了不同的算法，但由于它必须自主选择动作并且不受限制，所以我选择了 DDPG 方法。 然而，让我很困惑的是，对于我构建的特定奖励系统，对于单次训练运行（不更新模型），模型有时会学习，有时不会。这意味着对于大多数运行，模型不会探索选项，它将坚持发送消息所需的最少块数。而在较少的情况下，它似乎在学习，但仅此而已。下次我运行代码时，它可能会返回到选择最小数量的块。 不确定这是否与奖励系统、Actor - Critic 网络的架构或算法本身有关。但我很感激一些指导。非常感谢！    提交人    /u/LionTheAlpha   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</guid>
      <pubDate>Wed, 11 Dec 2024 08:57:42 GMT</pubDate>
    </item>
    <item>
      <title>协助定期 PPO 代理优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbdgqq/assistance_with_recurrent_ppo_agent_optimization/</link>
      <description><![CDATA[我正在训练我的循环 PPO 代理执行优化任务，代理的基于 token 的操作会输入到单独的数值优化器中。然而，在初始训练步骤之后，代理始终卡在其连续动作空间的上限和下限，并且奖励保持不变。您能否提供一些解决此问题的指导？     提交人    /u/YasinRL   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbdgqq/assistance_with_recurrent_ppo_agent_optimization/</guid>
      <pubDate>Tue, 10 Dec 2024 21:56:03 GMT</pubDate>
    </item>
    <item>
      <title>GameMaker 8.0 游戏的 Gym 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb7hvd/gym_environment_for_a_gamemaker_80_game/</link>
      <description><![CDATA[我最近决定尝试强化学习，以此来构建一个很酷的投资组合项目。我决定为 Spelunky Classic 制作一个深度 Q 学习算法，因为我在 YouTube 视频 中找到了七年前有人做过同样的事情，而且我是 Spelunky 的狂热粉丝，这让它更具吸引力。 我开始在我的计算机上设置所有适当的 Python 内容，经过几个小时的工作，这个 PyTorch 教程 开始运行。然而，此时我遇到了一个障碍——我了解 DQN 的工作原理，并且我有 Spelunky Classic 的源代码，所以我绝对可以为 DQN 修改它，但我对如何实现它感到困惑。我意识到下一步可能是为 Spelunky Classic 创建一个健身房环境，但我不知道如何将奖励返回给 Python 程序，因为游戏是用 GameMaker 8.0 编写的，这是一个弃用的引擎，在 OpenAI 热潮期间没有相关支持。 所有这些都是为了问，我现在应该如何让它工作？我是否必须制作某种自定义包装器才能将游戏变成健身房环境？有什么简单的方法可以将游戏转换为健身房环境？是否存在我尚未意识到的困难？ 任何帮助或建议都值得感激，我只是有点不知道下一步该怎么做。    提交人    /u/GarrettBotProgrammer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb7hvd/gym_environment_for_a_gamemaker_80_game/</guid>
      <pubDate>Tue, 10 Dec 2024 17:45:18 GMT</pubDate>
    </item>
    <item>
      <title>2 个 AI 代理玩捉迷藏。经过 150 万次模拟，代理学会了偷看、搜索和切换方向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hawyj5/2_ai_agents_playing_hide_and_seek_after_15/</link>
      <description><![CDATA[    /u/stokaty   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hawyj5/2_ai_agents_playing_hide_and_seek_after_15/</guid>
      <pubDate>Tue, 10 Dec 2024 08:03:36 GMT</pubDate>
    </item>
    </channel>
</rss>