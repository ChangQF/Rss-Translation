<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 14 Nov 2024 06:24:47 GMT</lastBuildDate>
    <item>
      <title>真正从事 RL 研究员工作的人们，你们是如何获得这份工作的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gquq8o/people_who_really_work_as_an_rl_researcher_how/</link>
      <description><![CDATA[真正从事 RL 研究员工作的人（主要从事 RL 项目）。   您在哪里工作？ 您什么时候得到这份工作的？ 您的背景？  我的博士研究主要涉及 RL，但我现在以 MLE 的身份从事各种 ML/DL/RL 项目。我提交了几份申请，通过了最后的面试，但作为行业中的 RL 研究员，我还是不够格。 LinkedIn 上真正纯粹与 RL 相关的工作总是少于 30 个。 我想知道人们如何获得纯粹的 RL 研究员工作？    提交人    /u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gquq8o/people_who_really_work_as_an_rl_researcher_how/</guid>
      <pubDate>Thu, 14 Nov 2024 02:37:18 GMT</pubDate>
    </item>
    <item>
      <title>PPO 之后是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqr1k3/whats_after_ppo/</link>
      <description><![CDATA[我最近完成了从 PyTorch 实现 PPO 以及任何看似相关的实现细节（vec envs、GAE lambda）。我还做了少量的行为克隆 (DAgger) 和多智能体强化学习 (IPPO)。 我想知道是否有人对下一步该怎么做有指示或建议？也许你正在研究某些东西，对 PPO 的改进是我完全错过的，或者只是一篇有趣的读物。到目前为止，我的兴趣只是游戏 AI。    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqr1k3/whats_after_ppo/</guid>
      <pubDate>Wed, 13 Nov 2024 23:37:38 GMT</pubDate>
    </item>
    <item>
      <title>“当你的人工智能欺骗你时：从人类反馈进行强化学习的部分可观察性挑战”，Lang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqibc6/when_your_ais_deceive_you_challenges_of_partial/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqibc6/when_your_ais_deceive_you_challenges_of_partial/</guid>
      <pubDate>Wed, 13 Nov 2024 17:27:54 GMT</pubDate>
    </item>
    <item>
      <title>帮助在 SARC 中实现 Q 误差跟踪（RL 研究）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqa0y9/help_with_implementing_qerror_tracking_in_sarc_rl/</link>
      <description><![CDATA[大家好， 我一直在研究 SARC，这是我对强化学习研究的一部分，并且专注于实现回顾性 Q 误差跟踪，类似于 SARC 论文第 6.3 节中概述的内容。 在我的实验中，我注意到 SARC 结果文件夹中有三个文件： - `q_losses_wrt_retroQvals_retroStates.pkl` - `q_losses_wrt_td3Qvals_td3States.pkl` - `demos.pkl` 我查看了训练代码和支持文件（例如 `core.py`、`retro_loss.py`、`logx.py`），但找不到这些特定文件的创建位置。我想复制此设置来跟踪和验证我工作中的 Q 误差收敛。  通过定期跟踪 Q 值来自己实现这一点是否有意义，或者我可能错过了哪些特定代码？此外，如果这里有人有这方面的经验，我的方法对于未来的期刊审稿人来说是否足够可靠，或者他们可能会提出担忧？  这是 [SARC GitHub 链接](https://github.com/sukritiverma1996/SARC) 和 [SARC 论文的 PDF](https://arxiv.org/abs/2306.16503)。 提前感谢您提供的任何见解！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqa0y9/help_with_implementing_qerror_tracking_in_sarc_rl/</guid>
      <pubDate>Wed, 13 Nov 2024 10:36:05 GMT</pubDate>
    </item>
    <item>
      <title>DPG 算法是基于策略的还是演员评论家的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gpje0r/is_dpg_algorithm_policybased_or_actorcritic/</link>
      <description><![CDATA[我有一个问题，即确定性策略梯度算法的基本形式是基于策略的还是演员评论家。我一直在寻找答案，在某些情况下，它说它是基于策略的，而在其他情况下，它并没有明确地说它是一个演员评论家，但它使用演员评论家框架来优化策略，因此我怀疑什么是策略改进方法。 我知道演员评论家方法本质上是基于策略的方法，并增加了评论家来提高学习效率和稳定性。    提交人    /u/Street-Vegetable-117   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gpje0r/is_dpg_algorithm_policybased_or_actorcritic/</guid>
      <pubDate>Tue, 12 Nov 2024 12:17:32 GMT</pubDate>
    </item>
    <item>
      <title>通过 RL 实现训练语言模型进行自我纠正——寻找测试人员和反馈！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gphaav/implementation_of_training_language_models_to/</link>
      <description><![CDATA[嗨， 我最近创建了论文通过强化学习训练语言模型进行自我纠正的最小 PyTorch 实现。 但是，我是将 RL 应用于语言模型的新手，不确定它是否正确实现。我希望社区能够帮助测试和改进它！  我需要帮助的内容：  测试：我的设置有限，因此如果有更多计算能力的人可以运行实验并提供反馈，我将不胜感激。 调试：这仍然是新鲜的，因此可能存在我尚未发现的错误。 优化速度：如果您有加快速度的想法，我很乐意听取！  如果能使此实现尽可能高效和有效，那就太好了，任何帮助都将不胜感激！ 查看：GitHub  提前致谢！    提交人    /u/sedidrl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gphaav/implementation_of_training_language_models_to/</guid>
      <pubDate>Tue, 12 Nov 2024 09:54:22 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个 RL 代理来在月球表面软着陆:)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gpd2uw/i_created_a_rl_agent_to_soft_land_in_lunar_surface/</link>
      <description><![CDATA[        由    /u/Few_Tooth_2474  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gpd2uw/i_created_a_rl_agent_to_soft_land_in_lunar_surface/</guid>
      <pubDate>Tue, 12 Nov 2024 04:55:24 GMT</pubDate>
    </item>
    <item>
      <title>离线学习的现状如何？您对离线学习有何看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gove1w/what_is_the_state_of_the_art_in_offline_learning/</link>
      <description><![CDATA[特斯拉等公司似乎成功地利用了从其汽车收集的数据进行离线学习。考虑到模拟和现实环境之间的众多差异，离线学习在未来会变得更加重要吗？    提交人    /u/Better_Working5900   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gove1w/what_is_the_state_of_the_art_in_offline_learning/</guid>
      <pubDate>Mon, 11 Nov 2024 15:53:15 GMT</pubDate>
    </item>
    <item>
      <title>轻松在 SMAC 和 MAMuJoCo 上记录离线数据，然后进行离线训练（离线 MARL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gotmkd/easily_record_offline_data_on_smac_and_mamujoco/</link>
      <description><![CDATA[大家好，我是南非的一名博士生，研究离线多智能体强化学习。我正在维护一个名为 Off-the-Grid MARL (og-marl) 的 GitHub 项目，该项目为离线 MARL 提供数据集和基线算法。我希望它可以帮助其他人开始该领域。我最近制作了一个快速的 Google Colab 笔记本来演示 og-marl 中的一些功能。我想这个社区中的一些人可能有兴趣查看它。在笔记本中，我演示了如何在 SMAC 或 MAMuJoCo 上在线训练 MARL 算法、记录数据、分析数据并在其上训练离线 MARL 算法。 https://colab.research.google.com/drive/1bfc7-tMLYmbKwh7HiqPzXU3f62tOuTY7?usp=sharing 如果您有兴趣进入离线 MARL，请随时通过 GitHub 与我们联系。我很乐意提供帮助。    提交人    /u/OfflineMARL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gotmkd/easily_record_offline_data_on_smac_and_mamujoco/</guid>
      <pubDate>Mon, 11 Nov 2024 14:37:18 GMT</pubDate>
    </item>
    <item>
      <title>协助我的 DQN 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gotiu2/assistance_with_my_dqn_project/</link>
      <description><![CDATA[嗨，Redditor， 我是强化学习的新手，目前正在从事深度 Q 网络 (DQN) 项目，该项目涉及一个机器人，该机器人试图在途中遇到障碍物的情况下到达目标。我遇到了一些挑战，确实需要那些在 RL 方面更有经验的人的指导。 我已在此处在 GitHub 上分享了我的代码：https://github.com/ROUMANI-Hassan/Reinforcement_Learning.git 如果有人在 DQN 或强化学习方面有专业知识，我将不胜感激任何建议。您的见解非常宝贵，因为我正在尝试独自学习 RL！ 非常感谢您提供的任何支持。    提交人    /u/Appropriate_Try8844   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gotiu2/assistance_with_my_dqn_project/</guid>
      <pubDate>Mon, 11 Nov 2024 14:32:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的标准库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1goqd15/standard_library_for_rl/</link>
      <description><![CDATA[目前，所有实现都与我们训练 DRL 模型的环境紧密耦合，我想知道我们是否可以有一个稳定的 API（例如 Pandas），将数据 / 属性序列化为步骤、观察、信息、奖励，而无需将其耦合到任何环境？    提交人    /u/iconic_sentine_001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1goqd15/standard_library_for_rl/</guid>
      <pubDate>Mon, 11 Nov 2024 11:46:59 GMT</pubDate>
    </item>
    <item>
      <title>领导者-追随者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gop0nq/leaderfollower/</link>
      <description><![CDATA[有人知道 MARL 中使用的标准领导者-追随者环境吗？我想使用 RL 创建一个领导者和追随者代理，并在标准环境中运行它们，最好是类似网格世界的环境。我找到了一些用于多智能体学习的标准环境(https://agents.inf.ed.ac.uk/blog/multiagent-learning-environments/)，其中有同时执行的操作，但找不到专门用于顺序(领导者-追随者)操作的环境。有人有什么相关建议吗？    提交人    /u/No_Addition5961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gop0nq/leaderfollower/</guid>
      <pubDate>Mon, 11 Nov 2024 10:14:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 玩 ARPG 的问题和想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</link>
      <description><![CDATA[我是一名计算机科学专业的最后一年学生，我想创建一个代理来玩我计划在 Unity 中制作的 ARPG。现在我想知道这是否可行，因为我需要概念验证直到 1 月。 我想指定代理执行的操作是导航关卡并在此过程中杀死一些敌人，关卡应该具有一定的复杂性，它不会是一条直线，但也不会是一个超级复杂的迷宫。最好它会有一些发散的路径。 现在，如果我在 Unity 中制作游戏，如果我当前的设置由 r5 5600 和 rtx 4070 12GB vram 组成，是否可以训练代理？它不会很快，但我可以让电脑全天候运行。 提前感谢您的回复。    提交人    /u/Marmotacuparlacur   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</guid>
      <pubDate>Mon, 11 Nov 2024 08:42:01 GMT</pubDate>
    </item>
    <item>
      <title>Decisions & Dragons：一个回答常见 RL 问题的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</link>
      <description><![CDATA[多年来，我在各种社交媒体平台上回答了很多强化学习问题。我决定是时候在自己的网站上收集和扩展它们了，我把这个网站叫做 Decisions and Dragons。 虽然这个网站是面向初学者的，但我认为它对更高级的从业者有帮助，可以作为对核心概念的复习。它推出了 8 个深入的答案，我将在未来添加它。 我不确定它会有多受欢迎，但我希望它至少能帮助你们中的一些人！ https://www.decisionsanddragons.com/    提交人    /u/Born_Preparation_308   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</guid>
      <pubDate>Sun, 10 Nov 2024 16:37:01 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q-Learning 帮助无人机自主穿越未知环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</link>
      <description><![CDATA[我们的任务是使用无人机覆盖未知区域并在搜索过程中确定关键点。我们假设了一种场景，即必须覆盖受灾地区，并希望确定幸存者。目前，我们将问题抽象为使用 2D 网格表示搜索区域，然后可视化无人机在其中移动的情况。 我们是强化学习的新手，不清楚如何在这种情况下使用 q-learning。当您试图一次性覆盖一个区域并且您不知道环境是什么样子，只知道要搜索的区域的边界时，q-learning 是否有效？当幸存者很可能只是随机分布时，它甚至可以学习什么样的模式？任何见解/指导都将不胜感激。    提交人    /u/naepalm7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</guid>
      <pubDate>Sun, 10 Nov 2024 14:10:02 GMT</pubDate>
    </item>
    </channel>
</rss>