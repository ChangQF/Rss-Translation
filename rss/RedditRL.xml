<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 08 Mar 2025 15:14:39 GMT</lastBuildDate>
    <item>
      <title>输入/输出建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6itiu/inputoutput_recommendation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是强化学习的新手，我真的不知道我的输入和输出应该如何优化学习。  它们是否应该在0到1或-1和1之间，我应该尝试最小化其数字并更多地依靠0和1之间的实际值等等...  您是否有任何资源（YouTube视频，文书工作）可以帮助我找到我正在寻找的东西？提交由＆＃32; /u/poppyshit     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6itiu/inputoutput_recommendation/</guid>
      <pubDate>Sat, 08 Mar 2025 14:54:55 GMT</pubDate>
    </item>
    <item>
      <title>兼容的RL算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6euod/compatible_rl_algorythims/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在启动我在计算机科学方面的硕士论文。我的目标是训练ISAAC实验室中的四倍机器人，并比较不同算法如何学习和对环境变化的反应。我计划使用SKRL库，其中具有以下算法可用：   对抗运动priors （ amp  ）      cross-entropy方法（ cem  ）     深层确定性策略梯度（ ddpg ）    &lt;&gt;  double q-network （ ddqn ） href =“ https://skrl.readthedocs.io/en/latest/api/api/agents/dqn.html”&gt; deep q-network （ dqn ） href =“ https://skrl.readthedocs.io/en/latest/api/apen/agents/ppo.html”&gt;近端策略优化（ ppo  ）      q-learning （ q-learning ）））   强大的策略优化（ rpo  ）      soft actor-critic （ sac   ）     国家行动奖励国家行动（ sarsa  ）      twin-delayed ddpg （ td3 ） href =“ https://skrl.readthedocs.io/en/latest/api/apent/trpo.html”&gt;信任区域策略优化（ trpo ））                &#39;我是否可以在iSAAC中实现所有效果。我还试图找到哪种算法更有趣，因为我无法使用所有算法。我认为3-4是最佳选择。任何帮助都将不胜感激，我在这个领域很新。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/solodres123     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6euod/compatible_rl_algorythims/</guid>
      <pubDate>Sat, 08 Mar 2025 11:01:46 GMT</pubDate>
    </item>
    <item>
      <title>GRPO在体育馆</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6eezp/grpo_in_gymnasium/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在适应 grpo 算法（最初是为LLM提出的），以在体育馆中使用Mujoco使用Mujoco进行连续的加固学习问题。  g 相应的奖励以计算相对优势。 对于连续行动任务，我的解释是，在每个时间步长，我需要：  示例 g 与策略分布相比。  使用这些奖励来计算相对优势，因此，在各自的环境中的行动。在连续行动环境中更有效？任何提高效率的建议或建议将不胜感激！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/totokk55     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6eezp/grpo_in_gymnasium/</guid>
      <pubDate>Sat, 08 Mar 2025 10:29:17 GMT</pubDate>
    </item>
    <item>
      <title>初学者项目：使用机器学习和图像处理检测假图像的A-Agent！”🚀</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6e6ve/beginner_project_aiagent_that_detects_fake_images/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/indows-phase-9280     [link]   ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6e6ve/beginner_project_aiagent_that_detects_fake_images/</guid>
      <pubDate>Sat, 08 Mar 2025 10:12:22 GMT</pubDate>
    </item>
    <item>
      <title>初学者QS关于范围和自动启动应用程序爱好项目的可行性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6cd1a/beginner_qs_about_scope_and_feasibility_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  感谢您阅读我的问题。首先让我说这是一个爱好项目，我知道它的局限性并非微不足道 - 尤其是，无法在尚未记录的市场数据上执行回测，这些市场数据与指标信号状态在应用程序运行的原始时间生成的指标状态。  话虽如此，这是项目：  p.1）记录指标信号和相关目标价格水平的状态，这些状态通过与信号状态词典进行比较来验证。 （完成）  p.2）构建“贸易设置”作为信号状态的汇合或组合。设置的定义是用布尔语表达解析器定义的，该纸张将表达式转换为机器的表达式（例如，“ RSI”过多买了状态，而Moving_average_50已上面;每个设置都由（1）条目，（2）输入目标（例如市场价格与输出以特定指标状态输出的价格）的定义（布尔表达式）组成，（3）出口；（4）出口目标。 （完成）  p.3）执行虚拟交易（部分完成），该虚拟交易模拟位置大小复合和从条目和退出（完成的）  p.4）  p.4）可以在每个蜡烛的时间邮票上记录所有指标状态，以及烛台的价格值。因此，例如，您可以在录制100个指标的数据时运行该应用程序两个星期或一个月。  从ML角度来看，问题的问题是：使用上述四个功能与机器学习库使用ML算法发现在该记录期间产生最佳P/L的设置定义吗？对我来说，这似乎是一个伸展的部分是，以信号状态定义设置的布尔表达式的构建似乎包含无限的可能性：受支持的操作员是（a）通过括号嵌套，（b）和（c）或（c）或（d）不嵌套。即使您对每个定义中可以使用多少操作数设定了一定的限制，当您考虑到“贸易设置”有四个子定义时，即使您可以使用多少操作数。 （进入，进入目标，退出，退出目标 - 尽管要明确的是目标定义，但如果信号x具有状态y，则非常简单，然后以“ else x”状态限制订单以其输出的价格值限制订单，而是市场订单的价格＆quot＆quot;），它似乎太复杂了（对于ML Fitness Onculation而言，可能性太高了，似乎太确定了）。但是我真的不知道。  我作为该领域的新手的问题是：  1）游戏笔记本电脑可以执行此类ML任务以找到在数据期间生成最佳P/L的最佳设置定义，如果是这样，则需要进行多少小时/天/天以进行分析？   2）是否有有充分记录的开源C＃库，最适合此？  3）如果您认为这是值得尝试的，那么您想在开始这样的鞋子之前，如果您在我的鞋子中脱颖而出，或者如果您认为这是不合时宜的，那么您是否会在我的鞋子中进行任何可能的p1 practial，否则在我的鞋子上又有其他任何可能的p1 p1 pry of u1 p1 p1 p1，您会想知道任何技巧或建议吗？  我很高兴能在这里获得知识渊博的观点的好处。 我在考虑它时，我相信第一阶段是训练它以使其做出有效的布尔表达方式。我已经有一个表达式解析器，它将接受任何输入，然后返回对输入是有效还是无效的判断（作为交易设置定义）。因此，如果有一种方法可以在预处理阶段中合并该解析器，在该阶段中，ML算法首先必须学习如何制作有效的定义，那么一旦它学会了如何制定有效的定义，我认为它可能有可能学习达到的最佳p/L p/L c/L c/l compeces。提交由＆＃32; /u/u/lmk99     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6cd1a/beginner_qs_about_scope_and_feasibility_for/</guid>
      <pubDate>Sat, 08 Mar 2025 07:55:08 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助我的研究的DRL实施！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6bc5v/need_help_for_my_researchs_drl_implementation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对所有人的问候，我想对那些愿意帮助我为研究的事情表示感谢。我目前被困在DRL实施中，这是我要做的事情：  1）我正在研究类似网格的，基于转弯的战术RPG。我选择了PPO作为DRL框架的骨干。我在策略网络中使用多模式设计用于状态表示：第一分支=空间数据，例如地形，定位等，第二个分支=字符状态。这两个分支都将通过处理层进行处理层，例如卷积层，嵌入，FC和最后连接到单个矢量中，并再次通过FC层。  2）我计划为策略网络使用共享的网络体系结构。   3）我想乘坐多i-discrete Action Space，E。tup cotive vector vector vector vector vector a tup cotife vector a tup vector a tup vector，e.g.g.g.g.，tup v。瓷砖，动作选择1，使用项目1（只是非常快的示例以说明）。 In other words, for every turn, the enemy AI model will yield these three decisions as a tuple at once. 4) I want to implement the hierarchical DRL for the decision-making, whereby the macro strategy decides whether the NPC should play aggressively, carefully, or neutral, while the micro strategy decides the movement, action choice, and item (which aligns to the output).我想动态训练决策。&lt; / p&gt;  5）我的问题 /混乱是，我应该在哪里实施层次结构设计？是在多模式体系结构的FC层之后的一层吗？还是在政策网络之外？还是在策略更新中？同样，当向量通过FC层（以防万一）通过FC层（完全连接的层）时，将将向量转换为非开采格式，而只是处理过的信息。那我该如何连接到我之前提到的层次设计？ 我不确定我是否正确地设计了此设计，或者是否有更好的方法来执行此操作。但是，我必须保留的实现是PPO，多模式设计和输出格式。如果我提供的上下文还不够清楚，请感谢您的帮助。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hengyewken96    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j6bc5v/need_help_for_my_my_researchs_drl_implementation/”&gt; [link]    [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6bc5v/need_help_for_my_researchs_drl_implementation/</guid>
      <pubDate>Sat, 08 Mar 2025 06:43:18 GMT</pubDate>
    </item>
    <item>
      <title>狭窄分布的交叉问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j67jxk/crossq_on_narrow_distributions/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！我想知道是否有人有与Crossq进行狭窄分布的经验？即STD很小。我的CrossQ实现在摆上效果很好，但在我的自定义环境上效果不佳。它非常不稳定，返回平均线将大大下降，然后爬回去。但是，当我使用SAC在自定义环境上学习时，这并没有发生。我知道，这里可能会有多个级别的问题来源，但是我很好奇地处理以下情况：STD非常小，并且随着代理商的了解，即使是较小的分配变化也会导致巨大的价值变化，因为批次批量的“归一部分”。运行的std很小 - ＆gt;非常稀有或新见的状态 - ＆gt; OOD，如果STD很小，则将新值标准化为巨大的值 - ＆GT;降低性能 - ＆gt;随着统计信息适应新值，性能再次成长 - ＆gt;重复重复或只是无法恢复。通常我的十字架确实恢复了，但这是次优的。  那么，有人知道如何处理这种情况吗？  另外，您如何监视batchnormalization的性病值？我不知道一个直截了当的方式，因为每个维度都会跟踪统计信息。也许是Max STD和Min STD？由于我的问题将出现在最小的STD小时。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/automatic-web8429     [link]    32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j67jxk/crossq_on_narrow_distributions/</guid>
      <pubDate>Sat, 08 Mar 2025 02:55:37 GMT</pubDate>
    </item>
    <item>
      <title>我想创建一个AI代理，控制吸血鬼幸存者游戏中的角色</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j64o5r/i_want_to_create_an_ai_agent_to_control_the/</link>
      <description><![CDATA[＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/vampiresurvivors/comments/comments/1j5zdve/i_want_want_to_create_ai_ai_ai_aigent_to_control_to_control_the/”&gt; [links]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j64o5r/i_want_want_to_create_create_an_ai_ai_ai_ai_ai_aigent_to_control_to_control_to/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j64o5r/i_want_to_create_an_ai_agent_to_control_the/</guid>
      <pubDate>Sat, 08 Mar 2025 00:25:49 GMT</pubDate>
    </item>
    <item>
      <title>量化礁石框架的计算效率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5riux/quantifying_the_computational_efficiency_of_the/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/quantifying-the-computation--computation-oficy-of-the-reef-framework-dramework-0e2b30d79746”&gt; [link]        [注释]            ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5riux/quantifying_the_computational_efficiency_of_the/</guid>
      <pubDate>Fri, 07 Mar 2025 15:50:07 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助实施RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5o3uj/need_help_implementing_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为我的公司建立AI代理，从本质上讲，我们有一些客户使用仪表板来构建动态UI，供用户保留和转换其移动或Web应用程序。  我们想建立一个可以通过用户的行为为客户选择最佳的UI变体的AI代理。  我在基本层面上开始建立代理的方法应该是什么？ 技术堆栈应该是什么？  我应该知道的链接或资源是否可以帮助我建立代理？  谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/poperbudget348     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5o3uj/need_help_implementing_rl/</guid>
      <pubDate>Fri, 07 Mar 2025 13:44:42 GMT</pubDate>
    </item>
    <item>
      <title>是时候训练DQN为Ale Pong V5</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5icxh/time_to_train_dqn_for_ale_pong_v5/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我使用的是带有3个Conv层（32、64、64滤波器）和完全连接的层（512个单位）的CNN。我的设置包括RTX 4070 Ti Super，但每集需要6-7秒。这比我使用CPU的每集50秒要快得多，但是GPU的使用仅为20-30％，而CPU的使用量低于20％ 这是典型的性能，还是我可以优化的东西来加快速度？任何建议将不胜感激！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/soliseeker     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5icxh/time_to_train_dqn_for_ale_pong_v5/</guid>
      <pubDate>Fri, 07 Mar 2025 07:51:47 GMT</pubDate>
    </item>
    <item>
      <title>在线学习的逻辑帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j57qnn/logic_help_for_online_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在研究一个自动的高速缓存内存管理项目，我旨在创建一个自动化的策略，以便在发生缓存失误时提高性能。目的是根据设定级别和传入的填充详细信息选择一个缓存块进行驱逐。 对于我的模型，我已经实施了一种离线学习方法，该方法是使用专家策略培训的，并根据专家决策进行了立即的奖励。现在，我想使用在线增强学习来完善这种脱机培训的模型，在这种模型中，与基准相比，根据IPC的改进来计算奖励（例如，像MockingJay这样的最先进的策略）。 我已经为这种方法写了一种在线学习算法（我为此提供了这种方法），但是我会为您提供了努力，因为我可以从中努力进行编码。我的方法有意义吗？您会完善什么？ 以下情况您可能应该知道：  1）没有下一个状态（s&#39;）的建模，因此我不模拟向下一个状态过渡到下一个状态（s&#39;）（s&#39;），因为缓存驱逐是单步决策问题，因为驱逐的效果仅在下一步的情况下，而不是在执行中，因此我在执行情况下，我不得不将驱逐出境，因此，我是在执行的情况下，而我却不是在执行中，而我却不是在执行中，而我却不是在执行中，而我却不是在执行范围。仅在模拟结束时观察。  2）在线学习微调离线学习网络  离线学习阶段使用对专家决策的监督学习 在线学习阶段在线学习阶段初始化政策 在线学习阶段可以根据IPC的限制          3 Simulation which is slightly different than textbook examples of RL so,  The reward is based on IPC improvement compared to a baseline policy The same reward is assigned to all eviction actions taken during that simulation  4) The bellman equation is simplified so no traditional Q-Learning bootstrapping (Q(s&#39;)) because I dont have my next state建模。然后将方程变为q（s，a）←q（s，a）+α（r -q（s，a））（我认为） 您可以在此处找到我为此问题写的算法： https://drive.google.com/file/d/100imnq2eeu_huvvztk6youwkeni13kve/view?usp = sharing   很抱歉长篇文章，但我确实在此处确实可以在此处提供您的帮助和反馈：)   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]     32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j57qnn/logic_help_for_online_learning/</guid>
      <pubDate>Thu, 06 Mar 2025 22:14:44 GMT</pubDate>
    </item>
    <item>
      <title>哪种机器人模拟器更适合增强学习？ Mujoco，Sapien或Isaaclab？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4wa9g/which_robotics_simulator_is_better_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图选择最合适的模拟器来加强我的研究机器人操纵任务。 Based on my knowledge, MuJoCo, SAPIEN, and IsaacLab seem to be the most suitable options, but each has its own pros and cons:  MuJoCo:  pros: good API and documentation, accurate simulation, large user base large. cons:并行性不那么好（需要 jax 才能平行执行）。        sapien：  优点：良好的API，良好的API，良好的平行性。   并行性，丰富的特征，NVIDIA生态系统。  cons：资源密集型，学习曲线太陡峭，仍在进行重大更新，据报道容易出现。         &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/xyllong     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4wa9g/which_robotics_simulator_is_better_for/</guid>
      <pubDate>Thu, 06 Mar 2025 14:10:29 GMT</pubDate>
    </item>
    <item>
      <title>分步教程：使用Llama 3.1（8b） + Google Colab + Grpo培训自己的推理模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4g234/stepbystep_tutorial_train_your_own_reasoning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4g234/stepbystep_tutorial_train_your_own_reasoning/</guid>
      <pubDate>Wed, 05 Mar 2025 22:29:52 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>