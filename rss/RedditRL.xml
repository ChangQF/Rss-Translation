<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 29 Feb 2024 18:17:15 GMT</lastBuildDate>
    <item>
      <title>努力在 STOCKS_GOOGL 数据上训练 A2C 模型 - 熵和价值损失收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b358ua/struggling_to_train_a2c_model_on_stocks_googl/</link>
      <description><![CDATA[我一直在从事一个项目，尝试使用 stable_baselines3 中的 A2C（Advantage Actor-Critic）算法来训练模型STOCKS_GOOGL数据，旨在预测股价走势。但是，我在训练过程中遇到了 entropy_loss 和 value_loss 收敛的持续问题。 详细信息： 数据集： 我正在使用 STOCKS_GOOGL 数据集，其中包含 Google (GOOGL) 的历史股票价格数据。 模型架构：我已使用默认 MlpPolicyand 设置了 A2C 模型确保输入维度与数据集匹配。 训练过程：我已初始化 A2C 模型并尝试使用提供的数据集对其进行训练。然而，我面临着 entropy_loss 和 value_loss 收敛的挑战。 观察： entropy_loss 和 value_loss 指标没有收敛的迹象，这使得模型很难有效地学习和预测股票价格走势。 尝试的解决方案：我尝试过不同的超参数，例如调整学习率和网络架构（隐藏层和神经元的数量） 。但是，问题仍然存在。 ​ 任何有关如何改进 entropy_loss 和 value_loss 收敛以获得更好训练结果的指导将不胜感激。 &lt; p&gt;​ 代码片段： from stable_baselines3 import A2C from stable_baselines3.common.vec_env import DummyVecEnv import torch.nn as nn importgym_anytrading.envs as envs fromgym_anytrading.datasets import STOCKS_GOOGL import stock_utils StockEnvClass = envs.StocksEnv # StockEnvClass = stock_utils.MyStocksEnv # 训练 env window_size = 6 env_factory = lambda: StockEnvClass(STOCKS_GOOGL, window_size=window_size, frame_bound=(window_size, len) (STOCKS_GOOGL))) env = DummyVecEnv([env_factory]) # 测试环境 test_data = STOCKS_GOOGL.tail(150) test_env = StockEnvClass(test_data, window_size=window_size, frame_bound=(window_size, len(test_data))) # tain 模型 test_callback = stock_utils.TestModelCallback（eval_env = test_env，eval_freq = 5000）policy =“MlpPolicy” model = A2C(policy, env, verbose=1,learning_rate=0.0001) model.learn(total_timesteps=300_000,callback=test_callback, log_interval=5000) # 评估 print(“评估模型：”) stock_utils.evaluate_model2(model, test_env, True) input(&quot;完成！按 Enter 退出...&quot;)  Repo: https://github.com/myxdream2020/stocks_drl.git    由   提交 /u/bigsml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b358ua/struggling_to_train_a2c_model_on_stocks_googl/</guid>
      <pubDate>Thu, 29 Feb 2024 17:00:47 GMT</pubDate>
    </item>
    <item>
      <title>普通重要性抽样和加权重要性抽样之间的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b33ijg/difference_between_ordinary_and_weighted/</link>
      <description><![CDATA[我正在学习 Sutton 和 Barto 的 RL 教科书，目前正在学习离策略评估，其中我们有两个策略，一个是目标策略 (确定性），另一个是行为策略（更具探索性），我无法理解加权重要性采样（我们用“rho”对每个回报进行加权，以获得“rhos”总和的回报）是如何实现的即使我们采取大量步骤，也会收敛到目标政策的真实值。  我可以理解，普通重要性采样将始终收敛于目标策略价值函数，因为它们乘以“rho”，但我不明白如何对加权重要性采样说同样的话，因为加权重要性采样的期望将是行为策略的期望   由   提交 /u/Unlikely_Spread_2618   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b33ijg/difference_between_ordinary_and_weighted/</guid>
      <pubDate>Thu, 29 Feb 2024 15:49:54 GMT</pubDate>
    </item>
    <item>
      <title>动态观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b32tjr/dynamic_observation_space/</link>
      <description><![CDATA[当观察空间大小动态变化时，最佳实践是什么？ 一些选项： 1. 定义最大大小并填充零缺失观测值 2. 定义较小的最大大小，以便始终抛出一些观测值（您的观测值总是多于允许的大小） 3. 使用 DNN 等将动态大小向量压缩为固定长度。 4. 还有什么吗？ 优点/缺点？   由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b32tjr/dynamic_observation_space/</guid>
      <pubDate>Thu, 29 Feb 2024 15:20:37 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的模因创作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b30nym/aidriven_meme_creation/</link>
      <description><![CDATA[我们为 Qbeast 的数据工程爱好者发布了一款人工智能驱动的模因生成器。这个有趣的项目帮助我们学习如何微调人工智能模型并定制幽默数据集。我们很高兴与其他对融合技术和创造力感兴趣的爱好者分享我们的经验。查看故事 https://qbeast.io/qbeasts-adventure-in-ai-驱动模因创作/。   由   提交/u/QbeastIO   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b30nym/aidriven_meme_creation/</guid>
      <pubDate>Thu, 29 Feb 2024 13:45:58 GMT</pubDate>
    </item>
    <item>
      <title>加权每决策重要性采样的一致性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2ydht/consistency_of_weighted_perdecision_importance/</link>
      <description><![CDATA[在第 5.9 节 Sutton 和 Barto 中，作者声称不存在一致的加权每决策 IS 估计器，然而，在 Precup 的原始论文中(https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&amp; ;context=cs_faculty_pubs），提出以下主张：“这个加权每决策重要性采样估计器是一致的，但有偏差，就像加权重要性采样估计器一样”。 诚然，没有原始论文中提供了加权 PD 估计器的一致性证明。  有谁知道之前的陈述是否经过了异议证明/是否有任何资源证明原始论文中建议的加权 PD 估计量不一致？  干杯！   由   提交/u/bean_the_great   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2ydht/consistency_of_weighted_perdecision_importance/</guid>
      <pubDate>Thu, 29 Feb 2024 11:42:21 GMT</pubDate>
    </item>
    <item>
      <title>您会使用什么 RL 算法作为推荐算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2s64f/what_rl_algorithm_would_you_use_for_a/</link>
      <description><![CDATA[您使用什么算法/奖励函数来提高 Tinder 或 Pinterest 等应用的留存率或参与度。 &lt; !-- SC_ON --&gt;  由   提交/u/simoo42  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2s64f/what_rl_algorithm_would_you_use_for_a/</guid>
      <pubDate>Thu, 29 Feb 2024 05:04:36 GMT</pubDate>
    </item>
    <item>
      <title>帮助复制生命游戏风格棋盘游戏的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2q5wi/help_with_reinforcement_learning_model_for/</link>
      <description><![CDATA[大家好， 我正在努力为一款逼真的游戏实现强化学习模型，其中两个玩家从 64 x 64 板上的一块开始。在每一步中，玩家可以选择攻击一个未占据的方格，该方格会复制他们的棋子。例如，从一个棋子开始，攻击一个方格会产生两个棋子，这两个棋子可以攻击空方格并产生 4 个棋子，依此类推。每个玩家选择自己的动作，并同时执行。多个棋子可以攻击同一个方格，获胜者由攻击该方格的棋子最多决定，否则如果均分则随机决定。 我目前面临设计强化学习算法的挑战对于这个游戏。我正在为国家代表和奖励机制而苦苦挣扎。具体来说，我不确定如何以捕获棋子复制动态的方式表示游戏状态以及如何定义适当的奖励。 以下是我当前方法的简要概述：  智能体倍增：我不确定如何对不同状态的智能体总数进行建模，在 Dota 等游戏中，使用 OpenAI 5，他们有 5 个可以单独训练的智能体，我&#39;我不确定如何让合作智能体在每个阶段的数字都发生变化的情况下集中工作。 奖励机制：我不确定应该分配什么奖励来鼓励模型战略性地复制片段，同时平衡其他目标例如控制棋盘或吃掉对手的棋子。  我读过各种论文，也读过有关使用 SAC 和 PPO 进行多智能体的内容，但我不确定到底我应该如何表示动作空间，以及我是否应该在每个回合或游戏结束时给予奖励，1表示获胜，-1表示失败，等等。 如果有人有经验通过强化学习或类似的棋盘游戏，我将非常感谢任何关于如何应对这些挑战的见解或建议。此外，如果有特定的资源或教程可以帮助我更好地理解这些概念，我将不胜感激。 提前感谢您的帮助！ &lt; !-- SC_ON --&gt;  由   提交/u/dickpham  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2q5wi/help_with_reinforcement_learning_model_for/</guid>
      <pubDate>Thu, 29 Feb 2024 03:20:21 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶汽车：需要帮助制定奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2g7mx/selfdriving_cars_need_help_for_reward_shaping/</link>
      <description><![CDATA[大家好， 我尝试训练在自定义模拟环境中控制不可玩汽车的 PPO 代理。我尝试为这项任务提供奖励。这些汽车没有目的地或目标状态。他们应该以速度限制的速度行驶并且永远（不与墙壁或其他车辆相撞）。动作和观察空间是连续的。操作包括油门（-1 表示完全制动，1 表示全油门）和转向（-pi/3 表示全左，pi/3 表示全右）。以下是一些有用的观察结果：  汽车速度 速度限制 &lt; li&gt;汽车前进方向与道路方向之间的夹角 偏离车道中心 上一页动作（为了平滑控制？）  模拟环境完全属于我，所以如果你认为其他一些观察可能有用，你也可以建议它们，我可以编码他们。 感谢您的回答。   由   提交 /u/Few-Pen-9807   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2g7mx/selfdriving_cars_need_help_for_reward_shaping/</guid>
      <pubDate>Wed, 28 Feb 2024 20:18:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyReason 进行推理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2aufn/reasoning_with_pyreason/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2aufn/reasoning_with_pyreason/</guid>
      <pubDate>Wed, 28 Feb 2024 16:52:46 GMT</pubDate>
    </item>
    <item>
      <title>强化学习与预测</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b28ko5/rl_prediction/</link>
      <description><![CDATA[大家好，我对强化学习还很陌生，有一个问题一直困扰着我，也许你可以帮助我： 规定性问题：优化问题，例如在自行车共享环境中重新分配自行车的车辆路线（例如，我有 k 个可以租用自行车的站点，还有很多其他信息，例如有多少辆卡车、成本等.) --&gt;目标是确保人们拥有足够的自行车（最大化利润或简单地使用），同时最小化重新分配的成本（例如，如果没有必要，我不需要将卡车发送到某些车站等） 问题：现在我有了来自预测模型的上下文信息，例如，对于上面的情况，我对下一个时间间隔 t （假设下一小时）的每个站点进行了需求预测 - 我可以使用上下文优化（例如，预测然后优化）来使用这个信息，但是我在徘徊，我应该把这个信息放在 RL 中的哪里（我是否在状态中对其进行编码？它与前瞻有关吗？）。 有任何帮助吗？或文献建议当然值得赞赏！提前谢谢！   由   提交 /u/mexodus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b28ko5/rl_prediction/</guid>
      <pubDate>Wed, 28 Feb 2024 15:22:45 GMT</pubDate>
    </item>
    <item>
      <title>如何训练机器狗在崎岖的地形上行走？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b22mn0/how_can_i_teach_a_robot_dog_to_walk_on_rough/</link>
      <description><![CDATA[我正在为一个班级制作一个课程项目，其中我有一只机器狗经过训练可以在模拟器上的崎岖地形上行走。你推荐我应该使用什么 RL 算法、环境或其他东西。我应该如何解决这个问题？政策的输出是什么？是关节上的扭矩吗？或者是其他东西？ （我在 ML 方面有很好的经验，并且在过去几个月才开始进入深度 RL）   由   提交/u/elonmusk-A12   /u/elonmusk-A12 reddit.com/r/reinforcementlearning/comments/1b22mn0/how_can_i_teach_a_robot_dog_to_walk_on_rough/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b22mn0/how_can_i_teach_a_robot_dog_to_walk_on_rough/</guid>
      <pubDate>Wed, 28 Feb 2024 10:10:26 GMT</pubDate>
    </item>
    <item>
      <title>请帮助仔细检查我是否正确传递 JAX PRNG 密钥以实现 RL 算法的再现性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1x61c/help_double_checking_whether_im_passing_jax_prng/</link>
      <description><![CDATA[   大家好！我有一个 JAX SAC 和 JAX PPO 的实现，它们可以正常运行并且可以正确训练，但不能完全重现。  对于 SAC 实现，同一种子的多次运行中的情景回报是相同的，但损失图略有不同： 多次运行的 SAC 图，回报相同，但损失略有不同 &lt;对于 PPO 运行，有时运行似乎相同，但有时则不然。例如，所有这些都是使用相同种子重复运行： ​ PPO 运行，全部具有相同的种子，两对，每对中的运行相同，但对与对不同 我的 SAC 实现在这里，所有内容都包含在一个脚本中：https://pastebin.com/bPA2eqqB 我的 PPO 实现在这里，也包含在一个脚本中：https://pastebin.com/XDMdBHRx &lt; p&gt;如果我以某种方式弄乱了我的 RNG 密钥处理，有人可以再次检查我吗？我自己看过这个，但我显然错过了一些东西。谢谢！   由   提交 /u/1cedrake   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1x61c/help_double_checking_whether_im_passing_jax_prng/</guid>
      <pubDate>Wed, 28 Feb 2024 04:32:24 GMT</pubDate>
    </item>
    <item>
      <title>帮助我理解：为什么使用政策网而不仅仅是价值网？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1te73/help_me_understand_why_use_a_policy_net_instead/</link>
      <description><![CDATA[上下文：比如说 AlphaZero。一种确定性游戏，你采取一个行动，很容易计算该行动后的后续状态。 在深入研究人们如何在此类场景中使用强化学习的示例时，我看到了很多训练两者的示例一个用于评估行动的政策网络和一个用于评估状态的价值网络。 我困惑的是，为什么两者都是？为什么不只用一个网络来评估状态，然后根据它将导致的状态来选择操作？我不知道这是一个困难的、依赖于上下文的问题，还是一个我很快就会听到的有明确答案的简单问题，或者是那种答案只是因为某种原因它效果更好的问题。我的直觉是，状态评估会比政策评估效果更好，而且只需要训练一个网络就会是一种优势。 我能想到的一些可能性：  &lt; li&gt;在一个状态上运行网络以获得 N 个动作的输出比在 N 个状态上运行网络要便宜得多（这感觉可能是答案的一部分） 不计算后续状态比在 N 个状态上运行网络便宜得多计算它们 它泛化到下一个状态未知的情况 当动作的表示比状态小得多时它效果更好 有一些拥有两个独立网络的额外好处 将政策网络训练到一定水平比将价值网训练到相同的准确性水平更容易，因为它关心的事情更少 &lt; /ul&gt;   由   提交/u/seventythird  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1te73/help_me_understand_why_use_a_policy_net_instead/</guid>
      <pubDate>Wed, 28 Feb 2024 01:31:31 GMT</pubDate>
    </item>
    <item>
      <title>没有顶级 ML 论文的人，你在哪里工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1suv1/people_with_no_toptier_ml_papers_where_are_you/</link>
      <description><![CDATA[我即将毕业，我的博士学位。研究的是强化学习算法及其应用。但是，我没能在顶级 ML 会议（NeurIPS、ICLR、ICML）上发表论文。但是我的领域有好几篇论文，如何才能获得我面试过一些移动和电子商务 (RecSys) 公司，但都失败了。  我不想做博士后，我对与学术界相关的任何事情都不感兴趣。  如果有初创公司的机会，或者我还没有探索过的其他职位，请告诉我。   由   提交/u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1suv1/people_with_no_toptier_ml_papers_where_are_you/</guid>
      <pubDate>Wed, 28 Feb 2024 01:06:45 GMT</pubDate>
    </item>
    <item>
      <title>PPO 无法学习简单任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1l6h5/ppo_failing_to_learn_simple_task/</link>
      <description><![CDATA[我创建了一个简单的环境： - 动作空间是离散的，长度为 10 -状态空间是长度为10的二进制，初始化为全0。 -智能体选择一个动作，状态规范中相应的索引更新为1，并给予0.1的奖励。  &gt;- 如果代理选择了一个已经选择的动作，则奖励为 0，并且剧集结束。 ​ 我将其构建为工作的简单基础做一些更复杂的事情，但我很惊讶我在这个环境中无法学到任何东西......我正在尝试使用 RLLib 进行 PPO 和 DQN 。我希望这能够很容易地学会。但随着训练的进行，奖励不断下降，随着探索的减少，智能体似乎每次都学会只选择相同的动作。 想知道是否有人对此有任何见解？我已经尝试调整超参数了很多，但没有任何东西能够学习来解决环境问题。这是环境代码： importgymnasiumasgymfromgymnasiumimportspacesimportnumpyasnpclassSimpleSim(gym.Env): &quot;&quot;&quot;&quot;遵循健身房界面的自定义环境。 ”“”元数据 = {&#39;render.modes&#39;: [&#39;人类&#39;]} def __init__(self): super(SimpleSim, self).__init__() self.num_actions = 10 self.action_space = space.Discrete(self.num_actions) self.观察空间 = 空间.MultiBinary(self.num_actions) self.selection_reward = 0.1 self.repeated_selection_penalty = 0 self.all_selected_reward = 0 self.episode_end_penalty = 0 def 重置(self, *, seed=None, options=None): self.state = np.zeros(self.num_actions, dtype=int) # 重置操作 return self.state, {} def step(self, action): did = False 奖励 = 0 if self.state[action] == 0: # 操作有之前没有被选择 # 更新状态并给予奖励 self.state[action] = 1reward = self.selection_reward else: # 之前选择了操作，结束剧集奖励 = self.repeated_selection_penalty did = True if np.all(self.state = = 1): # 所有操作均已选择完成 = True return self.state,reward,done,False,{}  我尝试过的其他内容： - 经常搞乱奖励函数，试图增加对过早停止的惩罚 - 不终止剧集，而是应用负奖励 我觉得我一定错过了一些东西，因为这应该很容易任务。   由   提交 /u/SkittlesUSA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1l6h5/ppo_failing_to_learn_simple_task/</guid>
      <pubDate>Tue, 27 Feb 2024 19:52:40 GMT</pubDate>
    </item>
    </channel>
</rss>