<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 18 Mar 2024 21:13:28 GMT</lastBuildDate>
    <item>
      <title>收敛到单一动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhvxi8/convergence_to_a_single_action/</link>
      <description><![CDATA[有点宽泛的问题，但在实现 RL 时，我的代理似乎很常见地会收敛到一个始终只选择 1 个操作的解决方案。它发生在 Pong 的深度 Q 学习代理和我正在攻读博士学位的多物理模型中的深度交叉熵代理中。任何人都知道可能发生这种情况的任何常见原因吗？    由   提交/u/Chewden_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhvxi8/convergence_to_a_single_action/</guid>
      <pubDate>Mon, 18 Mar 2024 17:07:39 GMT</pubDate>
    </item>
    <item>
      <title>手术配额MARL调度程序开发</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/</link>
      <description><![CDATA[嗨，我来自一个大学生强化学习团队，我们将创建一个新的多智能体合作对抗环境来学习最优策略传入请求的短期调度（在我们的例子中是外科手术）。请看一下当前的环境描述。缺什么？也许有什么多余的东西？您与我们分享的任何经验将不胜感激！  这是 GitHub 存储库：https://github.com/artemisak/Surgery-配额调度程序/树/主   由   提交 /u/Pythonic-af   /u/Pythonic-af  reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/</guid>
      <pubDate>Mon, 18 Mar 2024 15:38:55 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家无法过度拟合？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhta0e/actorcritic_unable_to_overfit/</link>
      <description><![CDATA[我目前正在开展一个项目，我正在尝试优化演员评论家模型，以根据某些输入数据最大化高分。然而，我面临着一个意想不到的挑战 - 尽管我打算出于实验目的这样做，但我似乎无法让我的模型过度拟合。 以下是我的模型设置的简要概述：  p&gt;  我的演员损失是策略梯度损失 -log(概率)*其决策的优势。 优势 = (前一帧的 Δscore) - (评论家估计的 Δscore评论家根据（来自前一帧的Δ分数）和（评论家根据前一帧估计的Δ分数）之间的MSE更新评论家  当我将批评者从第一帧开始的估计 Δscore 与实际 Δscore 进行比较时，我观察到零相关性，这令人费解 我正在一个非常小的数据集上训练模型 - 只有 1 或 2 集每个大约有 30 个事件。我的期望是模型能够轻松记住与这些事件相关的操作，并完全适应数据。  我检查了更新步骤之前和之后的梯度，所有内容看起来都已填充​​，所以我认为这就是“学习”。我似乎看不到任何问题，但对为什么我不能过度适应感到有点困惑。有没有从业者看到这个问题/知道明显的解决办法或检查我在这里遗漏的东西？  我尝试对 Actor/Critic 使用 LSTM 和 Transformer 模型，其中有 2 层，每层大小约为 200，丢失率为 0.25，学习率为 1e-5。    由   提交 /u/Rhyno_Time   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhta0e/actorcritic_unable_to_overfit/</guid>
      <pubDate>Mon, 18 Mar 2024 15:19:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助解释 NEAT 输出</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhrnaj/help_to_explain_neat_output/</link>
      <description><![CDATA[有人可以解释一下如何从 NEAT 的基本教程中导出生成的输出吗？ 如果我的输入是 (1.0, 1.0），我应该使用哪些权重和偏差来重现 0.06994251237198218？我熟悉 sigmoid 函数，但无法重现任何输出数字。 ​ 最佳基因组：密钥：6295 健身： 3.923495216838884 节点：0 DefaultNodeGene(key=0，bias=-0.3608400593343819，response=1.0，activation=sigmoid，aggregation=sum) 53 DefaultNodeGene(key=53，bias=-0.5517336026220021，response=1.0，activation=sigmoid，aggregation=sum) ） 838 DefaultNodeGene（密钥= 838，偏差= -0.5789866669875736，响应= 1.0，激活= sigmoid，聚合=和） 931 DefaultNodeGene（密钥= 931，偏差= -0.8513574951787184，响应= 1.0，激活= sigmoid，聚合=和） 1219 DefaultNodeGene(key=1219，bias=-0.5135172011791014，response=1.0，activation=sigmoid，aggregation=sum) 连接：DefaultConnectionGene(key=(-2, 0)，weight=0.6428472619552054，enabled=True) DefaultConnectionGene(key=(- 2, 53), 权重=-1.6935071067952197, 启用=False) DefaultConnectionGene(key=(-2, 1219), 权重=0.8991060064585895, 启用=True) DefaultConnectionGene(key=(-1, 0), 权重=-0.8006221066141777, 启用=真）DefaultConnectionGene（键=（-1，53），权重= 1.2070540852248426，启用=真）DefaultConnectionGene（键=（53，0），权重= 2.3774714023438124，启用=真）DefaultConnectionGene（键=（1219，53），权重=-2.5095137599707673，启用=True）输出：输入（0.0，0.0），预期输出（0.0，），得到[0.18185023040647358]输入（0.0，1.0），预期输出（1.0，），得到[0.8037737011422771]输入（1.0） , 0.0), 预期输出 (1.0,), 得到 [0.9937902124091639] 输入 (1.0, 1.0), 预期输出 (0.0,), 得到 [0.06994251237198218]  ​   由   提交/u/williego  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhrnaj/help_to_explain_neat_output/</guid>
      <pubDate>Mon, 18 Mar 2024 14:09:59 GMT</pubDate>
    </item>
    <item>
      <title>ExploRLLM：利用大型语言模型指导强化学习探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhqieo/explorllm_guiding_exploration_in_reinforcement/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.09583 网站：https://explorllm。 github.io/ 摘要：在具有较大观察和动作空间的基于图像的机器人操作任务中，强化学习面临样本效率低、训练速度慢、和不确定的收敛性。作为替代方案，大型预训练基础模型在机器人操作方面显示出了前景，特别是在零样本和少样本应用中。然而，由于推理能力有限以及理解物理和空间环境方面的挑战，直接使用这些模型是不可靠的。本文介绍了 ExploRLLM，这是一种利用基础模型（例如大型语言模型）的归纳偏差来指导强化学习探索的新颖方法。我们还利用这些基础模型来重新制定行动和观察空间，以提高强化学习的训练效率。我们的实验表明，引导探索比没有引导探索的训练能够更快地收敛。此外，我们还验证了 ExploRLLM 的性能优于普通基础模型基线，并且在模拟中训练的策略可以应用于现实环境，而无需额外训练。   由   提交 /u/Tbd_Sparks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhqieo/explorllm_guiding_exploration_in_reinforcement/</guid>
      <pubDate>Mon, 18 Mar 2024 13:18:05 GMT</pubDate>
    </item>
    <item>
      <title>DQN 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhoyx9/dqn_implementation/</link>
      <description><![CDATA[我想要一个在连续观察空间环境中实现 DQN 的示例，最好是在 SB3 中。   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhoyx9/dqn_implementation/</guid>
      <pubDate>Mon, 18 Mar 2024 11:59:22 GMT</pubDate>
    </item>
    <item>
      <title>剖析高更新率的深度强化学习：对抗价值高估和发散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhntvr/dissecting_deep_rl_with_high_update_ratios/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.05996 摘要：  我们证明深度强化学习可以保持其学习能力，而无需在梯度更新数量大大超过环境样本数量的设置中重置网络参数。在如此大的更新数据比率下，Nikishin 等人最近的一项研究。 （2022）提出了一种首要偏见，即代理过度适应早期的互动并淡化后来的经验，从而损害了他们的学习能力。在这项工作中，我们剖析了首要偏见背后的现象。我们检查了训练的早期阶段可能导致学习失败的原因，发现一个根本的挑战是一个长期存在的认识：价值高估。过度夸大的 Q 值不仅出现在分布外的数据上，而且还出现在分布内的数据上，并且可以追溯到由优化器动量推动的看不见的动作预测。我们采用简单的单位球归一化，可以在大更新率下进行学习，在广泛使用的 dm_control 套件上展示其功效，并在具有挑战性的狗任务上获得强大的性能，与基于模型的方法相竞争。我们的结果部分地质疑了由于早期数据过度拟合而导致的次优学习的先前解释。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhntvr/dissecting_deep_rl_with_high_update_ratios/</guid>
      <pubDate>Mon, 18 Mar 2024 10:52:00 GMT</pubDate>
    </item>
    <item>
      <title>与 CleanRL 类似的目标条件强化学习实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhm9bu/goalconditioned_rl_implementations_similar_to/</link>
      <description><![CDATA[大家好！ 我目前正在开发一个目标条件机器人项目（带有自定义环境）并面临一些挑战。我之前使用过 Stable Baselines3 (SB3)，我发现它相当全面，但对于我当前的需求来说有点复杂。  我希望创建一个自定义策略（模块化）并将迁移学习应用于不同的任务，这对于 SB3 来说似乎要复杂得多。因此，我对类似于 CleanRL 的实现特别感兴趣，因为它简单明了，但支持目标条件环境。  您能否推荐任何可能符合这些要求的资源、存储库或框架？ 以下是我正在寻找的内容的简要概述：  像 CleanRL 一样简单透明的框架或库。 支持目标条件环境（与 OpenAI Gym 的 GoalEnv 兼容）。 灵活地自定义策略网络，包括为网络的不同部分设置不同的学习率。   谢谢   由   提交 /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhm9bu/goalconditioned_rl_implementations_similar_to/</guid>
      <pubDate>Mon, 18 Mar 2024 09:05:03 GMT</pubDate>
    </item>
    <item>
      <title>使用开放人工智能健身房对超级马里奥兄弟实施 NEAT 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhlgq7/implementation_of_neat_algorithm_using_open_ai/</link>
      <description><![CDATA[我正在开发一个项目来实现 NEAT 算法，以便它学习玩超级马里奥兄弟（在开放 AI 健身房中）我想知道是否有是否有任何资源或人员已经完成此操作，以便我更好地了解如何执行此操作，感谢任何帮助 谢谢!! &lt;!-- SC_ON - -&gt;  由   提交 /u/MinuteNo5493   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhlgq7/implementation_of_neat_algorithm_using_open_ai/</guid>
      <pubDate>Mon, 18 Mar 2024 08:04:28 GMT</pubDate>
    </item>
    <item>
      <title>MuZero 应用程序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh7x8z/muzero_applications/</link>
      <description><![CDATA[嘿伙计们！ 我最近建立了自己的库来训练 MuZero 和 AlphaZero 模型，我意识到我从来没有看到了该算法的许多应用（除了来自 DeepMind 的）。  所以我想问一下你是否曾经使用 MuZero 做过什么？如果是这样，您的应用程序是什么？   由   提交/u/Skirlaxx  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh7x8z/muzero_applications/</guid>
      <pubDate>Sun, 17 Mar 2024 20:37:34 GMT</pubDate>
    </item>
    <item>
      <title>我应该选择哪个 Q 值作为深度 Q 网络输出的操作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh4jbw/which_qvalue_do_i_select_as_the_action_from_the/</link>
      <description><![CDATA[我正在开发一个项目，该项目涉及使用 Deep Q-Learning Agent 来学习选择 1000 节点 NetworkX 中某些节点的适当方法图形。我的 observation_space 是一个 (1000, 3) 数组，每行代表节点标签、度数和变量/属性（0、1 或 2）。 action_space 具有 (1000, 1) 形状，每个元素对应于在指定节点上采取的操作。  这是我的 Deep Q 网络的代码： ``` def nnmodel(observation_space, action_space): model = tf.keras .models.Sequential（） model.add（tf.keras.layers.Dense（128，input_shape =（无，observation_space.shape [0]，observation_space.shape [1]），activation =&#39;relu&#39;））model.add (tf.keras.layers.Dense(256，激活=&#39;relu&#39;)) model.add(tf.keras.layers.Dense(256，激活=&#39;relu&#39;)) model.add(tf.keras.layers.Dense (len(action_space),activation=&#39;线性&#39;)) model.compile(optimizer=Adam(),loss=&#39;mse&#39;,metrics = [&#39;accuracy&#39;]) 返回模型   ``` 从理论上讲，据我所知，应该给我来自   q_values = model.predict(observation_space) 的 q 值  但是，我的 q_values 的形状为 (1000, 1000)，我不确定哪个“最高 q 值”是我应该考虑对应于代理应该执行操作的节点。它是最高 q 值条目，其行/列对应于代理应选择的节点吗？或者它是最大的行/列总和？或者完全是另外一回事？我在网上查看的示例通常使用 np.argmax(q_values[0])，我觉得这不适用于我的情况。  此外，我的 input_shape 看起来对于我所描述的问题是否正确？  任何帮助表示赞赏！  max_q = np.max(q_values) position = np.where(q_values == max_q) print(position)  这返回最大 q 值的索引。我不确定这是否意味着我应该为第 i 行或第 j 列选择第 i/j 个节点。    由   提交/u/No_Type_2250   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh4jbw/which_qvalue_do_i_select_as_the_action_from_the/</guid>
      <pubDate>Sun, 17 Mar 2024 18:23:43 GMT</pubDate>
    </item>
    <item>
      <title>近端策略优化有帮助吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh3j6h/proximal_policy_optimization_help/</link>
      <description><![CDATA[      我想知道是否有人了解近端策略优化？我一直很难让它发挥作用，我花了几个星期。我正在 LUA 上创建一个编译版本。我认为这就是发生此错误的地方。  https://preview.redd .it/4aua8cchlxoc1.png?width=756&amp;format=png&amp;auto=webp&amp;s=a229c9dff65f4ba0dd64d7df41de63420ec7a03d   由   提交/u/Diligent_Marzipan65   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh3j6h/proximal_policy_optimization_help/</guid>
      <pubDate>Sun, 17 Mar 2024 17:43:01 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习 - PettingZoo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh2llu/multiagent_reinforcement_learning_pettingzoo/</link>
      <description><![CDATA[我有一款竞争性的团队射击游戏，我已将其转换为 PettingZoo 环境。然而，我现在面临一些问题。   是否有任何好的教程或库可以引导我使用 PettingZoo 环境来训练 MARL 策略？ 是否有任何简单的方法来实现自我对弈？ （只要它以某种能力存在，它就可以是非常基本的） 有什么好的方法来检查我的 PettingZoo 环境是否合规？每次我使用不同的库（即到目前为止我尝试过的 TianShou 和 TorchRL）时，它都会针对我的代码的错误给出不同的错误，并且每个库都要求 env 的格式完全不同。 &lt; /li&gt;  到目前为止，我已经尝试遵循 https://pytorch.org/rl/tutorials /multiagent_ppo.html，TorchRL 和 PettingZooWrapper 中都有 EnvBase，但两者都不起作用。除此之外，我尝试过 https://tianshou.org/en/master/01_tutorials/04_tictactoe.html  但修改它以适应我的环境。  通过“不工作”，我的意思是它给了我一些模糊的错误，在我理解它想要所有内容的格式之前我无法真正修复它，但我找不到关于什么的良好文档每个图书馆实际上都想要。 我绝对没有把我的工作留到最后一刻。 我真的很感激任何有关这方面的帮助，甚至是指向一个稍微有一些图书馆的指针。所有这一切的更清晰的文档。谢谢！   由   提交/u/SinglePhrase7  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh2llu/multiagent_reinforcement_learning_pettingzoo/</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:58 GMT</pubDate>
    </item>
    <item>
      <title>Cognition AI 推出 Devin：“金牌编码员构建了一个可以为他们完成工作的人工智能”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bggkqu/devin_launched_by_cognition_ai_goldmedalist/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bggkqu/devin_launched_by_cognition_ai_goldmedalist/</guid>
      <pubDate>Sat, 16 Mar 2024 21:26:39 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>