<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 04 Apr 2024 03:16:57 GMT</lastBuildDate>
    <item>
      <title>DQN 更新</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bv7iwk/update_of_dqn/</link>
      <description><![CDATA[       社区您好，我需要您的帮助 我正在使用自定义的 DQN 网络，该网络由四个独立的输出层 A、B、C 和 D 组成，每个输出层输出一个操作。并根据A（有3个离散输出）的输出，选择B、C或D中所选择的动作（仅执行一组动作）。在这种情况下，我不知道更新函数是否正确： https://preview.redd.it/sy29zf4xicsc1.png?width=1729&amp;format=png&amp;auto=webp&amp;s=8bfe4547eeb750aee4117b0902308 cb9a10463f5 https://preview.redd.it/ug0rws02jcsc1.png ?width=1750&amp;format=png&amp;auto=webp&amp;s=37a8628663ae4db1b971dd49a770b01307f66a21   由   提交/u/GuavaAgreeable208  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bv7iwk/update_of_dqn/</guid>
      <pubDate>Wed, 03 Apr 2024 23:14:15 GMT</pubDate>
    </item>
    <item>
      <title>A2C反复学习和死亡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bv2mbh/a2c_learns_and_dies_repeatedly/</link>
      <description><![CDATA[      我目前正在针对倒立摆问题实施 A2C。 （奖励 = 0 * 高度{0-1}**2，剧集长度 = 1000，下降率= 0.98） 由于某种原因，它在学习过程中不断死亡，并且对 750 多个游戏没有任何作用。这很奇怪，因为我在将优势输入到参与者网络之前将其标准化，将 π(s|a) 上限设置为 0.05（这样 A/π(s|a) 就不会爆炸），学习率为 0.001演员为 0.005，评论家为 0.005，并且网络不应过度拟合（演员：3-16-16-3，评论家：4-32-16-1）。此外，如果它获得如此低的分数，则意味着它在 90% 的情况下选择不执行 3 个操作。我唯一能想到的可能是评论家网络会感到困惑，因为在扣除未来奖励后，状态的值范围可以从 -20 到 60。 我认为这不是问题在环境、奖励或参与者网络中，因为我的 REINFORCE 实现具有平滑的学习曲线，并且获得了相当好的性能。 A2C的学习曲线，\“scores\”=累积奖励，crit_v=批评家的累积价值评估，bp_advantage=累积梯度大小传播到演员   由   提交/u/AUser213  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bv2mbh/a2c_learns_and_dies_repeatedly/</guid>
      <pubDate>Wed, 03 Apr 2024 20:01:15 GMT</pubDate>
    </item>
    <item>
      <title>适用于表格 RL 环境的软件包？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1buy67d/packages_for_tabular_rl_environments/</link>
      <description><![CDATA[是否有一个库，其中包含健身房/体育馆格式的流行 RL 环境的表格版本？理想情况下，状态空间是一个单热向量，它同时具有确定性和随机环境，以及稀疏和密集的奖励。似乎这是一个常见的需求，但除了简单的 gridworld 包之外我找不到任何东西。   由   提交 /u/asdfwaevc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1buy67d/packages_for_tabular_rl_environments/</guid>
      <pubDate>Wed, 03 Apr 2024 17:12:45 GMT</pubDate>
    </item>
    <item>
      <title>还有其他 RLHF/数据注释/标签公司吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bupqu8/any_other_rlhfdata_annotationlabeling_company/</link>
      <description><![CDATA[尝试比较和编写所有 RLHF 和数据注释/标签公司的工作。这是我的清单，你知道我错过了什么吗？谢谢！ Scale Labelbox Argilla Toloka SuperAnnotate HumanSignal Kili Watchfull Datasaur.ai Refuel iMerit Anote M47 Snorkel Ango AI AIMMO Alegion Sama CloudFactory    ;由   提交 /u/MeowCatalog   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bupqu8/any_other_rlhfdata_annotationlabeling_company/</guid>
      <pubDate>Wed, 03 Apr 2024 11:01:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在训练循环中陷入这个永无休止的输出？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bul9f2/why_am_i_stuck_on_this_never_ending_output_for/</link>
      <description><![CDATA[   /u/One-Character-1724   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bul9f2/why_am_i_stuck_on_this_never_ending_output_for/</guid>
      <pubDate>Wed, 03 Apr 2024 06:02:00 GMT</pubDate>
    </item>
    <item>
      <title>《人工智能奥林匹克数学竞赛-进步一等奖》（截止日期：2024-06-27，3个月）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bug017/ai_mathematical_olympiad_progress_prize_1/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bug017/ai_mathematical_olympiad_progress_prize_1/</guid>
      <pubDate>Wed, 03 Apr 2024 01:24:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用反向传播奖励正确计算多个情节之间的损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1buefgg/how_to_properly_calculate_loss_among_multiple/</link>
      <description><![CDATA[我正在研究一种算法，其中一个情节可以是 X 个时间步长，直到达到奖励。当达到奖励时，我们将该值反向传播到本集中之前的所有时间步。我知道诸如合并衰减之类的方法，但在这种情况下，我希望将一个情节的所有动作视为对产生奖励具有同等影响力。 话虽这么说，我正在对所有情节进行训练一次，模型的损失将从所有剧集的样本中计算出来。我面临的困境是试图了解在每集的基础上对样本进行加权是否更有意义。 一方面，如果我们在每集的基础上对样本进行加权（平均误差）对于剧集中的所有样本），我们将所有剧集视为相同。例如，即使一个情节只有 10 个样本，我们也会关心预测这些样本，就像假设一个情节有 1000 个样本一样。 但是，我也看到了想要只处理的观点单独的样本，因为从技术上讲，正确预测样本 A 与正确预测样本 B 一样重要。 不确定此类问题是否有先例。任何指导将不胜感激。   由   提交/u/Yogi_DMT  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1buefgg/how_to_properly_calculate_loss_among_multiple/</guid>
      <pubDate>Wed, 03 Apr 2024 00:11:23 GMT</pubDate>
    </item>
    <item>
      <title>欧洲 RL 博士</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bu2p2n/rl_phd_in_europe/</link>
      <description><![CDATA[您知道我可以在欧洲申请博士学位的实验室吗？    由   提交 /u/Peight_een   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bu2p2n/rl_phd_in_europe/</guid>
      <pubDate>Tue, 02 Apr 2024 16:13:10 GMT</pubDate>
    </item>
    <item>
      <title>“通过强化学习将预先训练的语言模型与多模式提示融合”，Yu 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bu1lqw/fusing_pretrained_language_models_with_multimodal/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bu1lqw/fusing_pretrained_language_models_with_multimodal/</guid>
      <pubDate>Tue, 02 Apr 2024 15:29:20 GMT</pubDate>
    </item>
    <item>
      <title>DROID：大规模野外机器人操作数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bts0hu/droid_a_largescale_inthewild_robot_manipulation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.12945 项目页面：https:// /droid-dataset.github.io/ 硬件代码： https://github.com/droid-dataset/droid 策略学习代码：https://github.com/droid-dataset/droid_policy_learning 数据集 Colab：https://github.com/droid-dataset/droid_policy_learning Research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing&quot;&gt;https://colab.research.google.com/drive/1b4PPH4XGht4Jve2xPKMCh-AXXAQziNQa?usp=sharing &lt;强&gt;摘要：  创建大型、多样化、高质量的机器人操作数据集是迈向更强大、更强大的机器人操作政策的重要基石。然而，创建此类数据集具有挑战性：在不同环境中收集机器人操作数据会带来后勤和安全挑战，并且需要在硬件和人力方面进行大量投资。因此，即使是当今最通用的机器人操纵策略，也大多是根据场景和任务多样性有限的少数环境中收集的数据进行训练的。在这项工作中，我们引入了DROID（分布式机器人交互数据集），这是一个多样化的机器人操作数据集，具有 76k 演示轨迹或 350 小时的交互数据，收集了 564 个场景和 84 个场景。北美、亚洲和欧洲的 50 名数据收集者在 12 个月内完成的任务。我们证明，使用 DROID 进行训练可以产生具有更高性能和更高泛化能力的策略。我们开源完整的数​​据集、策略学习代码以及用于重现机器人硬件设置的详细指南。    &lt; a href=&quot;https://www.reddit.com/r/reinforcementlearning/comments/1bts0hu/droid_a_largescale_inthewild_robot_manipulation/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bts0hu/droid_a_largescale_inthewild_robot_manipulation/</guid>
      <pubDate>Tue, 02 Apr 2024 06:43:26 GMT</pubDate>
    </item>
    <item>
      <title>“Deep de Finetti：从大型语言模型中恢复主题分布”，Zhang 等人，2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1btihaf/deep_de_finetti_recovering_topic_distributions/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1btihaf/deep_de_finetti_recovering_topic_distributions/</guid>
      <pubDate>Mon, 01 Apr 2024 22:53:04 GMT</pubDate>
    </item>
    <item>
      <title>政策梯度——未来回报</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1btd7im/policy_gradients_future_looking_returns/</link>
      <description><![CDATA[        由   提交/u/Both_Ebb_327   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1btd7im/policy_gradients_future_looking_returns/</guid>
      <pubDate>Mon, 01 Apr 2024 19:33:58 GMT</pubDate>
    </item>
    <item>
      <title>openAi 健身房赛车出现问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bt80t6/trouble_with_car_racing_from_openais_gym/</link>
      <description><![CDATA[我是强化学习的初学者，正如标题所说，我在让我的代理在 openAi 的赛车环境中学习时遇到了一些麻烦 我正在使用带经验回放的双 dqn，似乎无论我让它训练多长时间，我将批次或内存设置多大，它似乎都无法学习，为什么？我会如果有人有兴趣查看，请添加 github 链接 https://github.com/OogaBooga21/Car_Racing。 git    由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bt80t6/trouble_with_car_racing_from_openais_gym/</guid>
      <pubDate>Mon, 01 Apr 2024 16:19:22 GMT</pubDate>
    </item>
    <item>
      <title>Noob Post - 2024 超级马里奥兄弟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bspvoy/noob_post_2024_super_mario_bros/</link>
      <description><![CDATA[我只是想发布此内容，因为我花了太长时间试图让马里奥兄弟使用 PPO 设置，而我只是一直拥有模块由于健身房更新而出现问题。 这是最终使它对我有用的视频：https:/ /youtu.be/_gmQZToTMac?si=t2C8uY1eJrKf32NJ 通常我只是关注 Nicholas Renotte 的视频 (https ://youtu.be/2eeYqJ0uBKE?si=z70WzwUyJjcpwjUy）但是对gym的更新，其中done现在终止并截断了我的命。我弃用了这些模块，我觉得我尝试了新学习者能做的一切，但一直失败。 我知道我在 Reddit、Stack Overflow 和 YouTube 上搜索了解决方案，但没有成功。我将其发布给其他学习者，以便他们能够跳过我的痛苦。   由   提交 /u/AcrobaticAmoeba8158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bspvoy/noob_post_2024_super_mario_bros/</guid>
      <pubDate>Mon, 01 Apr 2024 00:34:14 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>