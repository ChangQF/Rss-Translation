<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 26 Jan 2025 12:28:30 GMT</lastBuildDate>
    <item>
      <title>Ray 2.40 上的 PBT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</link>
      <description><![CDATA[有人熟悉在 Ray 2.4 上做 PBT 吗？ 如果有人知道如何解决这个问题，我们将不胜感激： https://discuss.ray.io/t/metric-for-pbt-in-ray-2-40/21619 摘要：我想基于评估情节奖励均值指标使用 PBT 对 PPO 执行超参数优化，但我似乎无法使用该指标或任何有用的指标进行训练。    提交人    /u/nukelius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</guid>
      <pubDate>Sun, 26 Jan 2025 10:48:27 GMT</pubDate>
    </item>
    <item>
      <title>寻求与 RL 研究人员的合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</link>
      <description><![CDATA[大家好， 我是 UIUC 的计算机科学博士生，具有理论算法背景（在 SODA/ICALP/ESA 发表过文章；主要是近似算法、图问题的可扩展算法、在线算法等）。最近，我将重点转向 使用强化学习 (RL) 解决 NP 难图问题，我正在寻找有相同兴趣的合作者。 关于我的工作：  在理论会议（SODA、ESA）和 ML 会议（NeurIPS）上发表过文章。 最近开发了一种基于 RL 的 NP 难图问题方法，包括从头开始在 PyTorch 中编写自定义 GNN 框架。论文已提交给 ICML。 坚实的理论基础 + 良好的编码能力，旨在将理论与实践相结合。  寻找： 对将 RL 与图算法/组合优化问题相结合感兴趣的研究人员，尤其是以下人员：  研究 NP 难图问题（例如，TSP、顶点覆盖、图分区）。 关心为什么学习策略有效（例如，理论保证、泛化分析）。 想要构建既有原则又有实践效率的方法。  如果这与您的工作或兴趣重叠，请随时给我发私信！我很乐意分享我的论文草稿，讨论想法或探索合作。 （使用一次性帐户进行匿名，但可以通过电子邮件/ LinkedIn 进行验证。）    提交人    /u/ForAllEpsilonExists   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</guid>
      <pubDate>Sun, 26 Jan 2025 07:00:09 GMT</pubDate>
    </item>
    <item>
      <title>构建定制的机械臂环境并训练 AI 代理来控制它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</link>
      <description><![CDATA[        由    /u/Fabulous-Extension76  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</guid>
      <pubDate>Sun, 26 Jan 2025 03:59:44 GMT</pubDate>
    </item>
    <item>
      <title>特征选择/状态抽象方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</link>
      <description><![CDATA[大家好，有谁知道有什么论文/作品，其中代理具有非常高维的状态空间，并且可以通过某种方式减小尺寸？有没有什么常用的方法可以为代理选择最佳特征？    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</guid>
      <pubDate>Sat, 25 Jan 2025 23:25:36 GMT</pubDate>
    </item>
    <item>
      <title>“DeepSeek-R1：通过强化学习激励法学硕士中的推理能力”，Guo 等人 2025 {DeepSeek}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</guid>
      <pubDate>Sat, 25 Jan 2025 22:52:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Collab 中安装 MARLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</link>
      <description><![CDATA[我按照说明在 Collab 中安装 MARLib： https://marllib.readthedocs.io/en/latest/ conda create -n marllib python=3.8 conda activate marllib git clone cd MARLlib pip install --upgrade pip pip install -r requirements.txt # 我们推荐 gym 版本在 0.20.0~0.22.0 之间。 pip install gym&gt;=0.20.0,&lt;0.22.0 # 将补丁文件添加到 MARLlib python patch/add_patch.py​​ -yhttps://github.com/Replicable-MARL/MARLlib.git  要求安装到 ray 1.8.0，找不到该版本（我也尝试过 1.13 但找不到）。 删除版本会导致更多错误和更多不兼容性。总是显示相同的消息： 错误：subprocess-exited-with-error 当安装没有特定版本的所有内容时，调用 marl.algos.mappo 时，它会抛出： ModuleNotFoundError：没有名为“ray.rllib.agents”的模块 有人可以为我提供安装 MARLlib 的更新说明并且没有不兼容性吗？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</guid>
      <pubDate>Sat, 25 Jan 2025 22:33:04 GMT</pubDate>
    </item>
    <item>
      <title>文本推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</link>
      <description><![CDATA[大家好，我想知道你们有没有推荐的教科书，无论是在线的还是数字的，这些教科书可以从高层次深入到强化学习领域。就背景而言，我拥有电气硕士学位，并且做过相当多的机器学习工作，但我在强化学习方面做得最先进的是 cuda 中的批量 Q 学习。我甚至从未实现过自己的深度 Q 学习算法。希望能找到一些数学密集型的问题。主要关注机器人技术和寻路，但愿意看任何东西。    提交人    /u/Tassadon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</guid>
      <pubDate>Sat, 25 Jan 2025 01:57:22 GMT</pubDate>
    </item>
    <item>
      <title>对 5090 / GTC 2025 的思考</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</link>
      <description><![CDATA[有人对培训代理的 5090 感到兴奋吗？有什么特别的理由吗？ 此外，如果有人要去，廉价的边疆航班将让我今年第二次参加 GTC。很想喝点东西。去年我玩得很开心，将在周日参加其中一次培训，然后周二离开。    提交人    /u/ParamedicFabulous345   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</guid>
      <pubDate>Fri, 24 Jan 2025 21:36:59 GMT</pubDate>
    </item>
    <item>
      <title>如何确定扑克锦标赛中的最佳经纪人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</link>
      <description><![CDATA[我目前正在开展一个项目，该项目旨在确定哪种深度强化学习算法最适合复杂环境，例如无限注德州扑克。我正在使用 Tianshou 制作代理和 PettingZoo 环境。我已经完成了项目的这一部分，现在我必须确定哪个代理是最好的。我让每个代理相互对战了 30,000 场，并收集了大量数据。 起初，我认为赢得最多筹码的玩家应该是赢家，但这并不公平，因为一名玩家在与最弱的玩家之一的比赛中赢得了很多筹码，并输给了所有其他玩家，但这仍然使他成为赢得最多筹码的赢家。然后我考虑了 ELO 评级，但这也行不通，因为如果玩家赢得的钱很少，那么获胜就不重要了。 在其他游戏中最常用的 2 种情况的组合是 chip_won_by_A / (chips_won_by_A + chip_won_by_B)，但这种组合也行不通，因为这是一个零和游戏环境，chips_won_by_A = -chips_won_by_B，结果除以零。你对这类问题还有其他解决方案吗？我认为使用他们本可以赢得的筹码数量中赢得的筹码百分比可能是个好主意？欢迎提供任何帮助！    提交人    /u/komensalizam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</guid>
      <pubDate>Fri, 24 Jan 2025 17:42:05 GMT</pubDate>
    </item>
    <item>
      <title>策略迭代中的策略评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</link>
      <description><![CDATA[      在 Sutton 的书中，策略评估 (4.5) 是 pi(s,a) * q(s,a) 的总和。但是，当我们在策略迭代过程中使用策略评估时（图 4.3），为什么我们不需要对所有动作进行求和，而只需要对 pi(s) 进行评估呢？ https://preview.redd.it/5vo75evilyee1.png?width=1030&amp;format=png&amp;auto=webp&amp;s=77af1304d549008b8c2e24c9cd8dff034519acae    submitted by    /u/lalalagay   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</guid>
      <pubDate>Fri, 24 Jan 2025 15:22:14 GMT</pubDate>
    </item>
    <item>
      <title>仍然不太漂亮，但奖励函数略好一些</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</link>
      <description><![CDATA[       由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</guid>
      <pubDate>Fri, 24 Jan 2025 15:15:20 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Shadow Dextrous 的手在 pybullet 中抓取 3D 杯子模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</link>
      <description><![CDATA[你好。我正在尝试使用 PyBullet 来模拟假手抓握。我使用影子手 urdf 作为我的手，一个杯子的 3d 模型。我正在努力实现影子手抓取杯子。 我希望最终使用强化学习来优化对不同尺寸杯子的抓取，但我需要先运行没有任何 AI 的 Python 脚本，这样我才能有一个基线来与 RL 模型进行比较。有人知道可以帮助我的资源吗？提前谢谢了。    提交人    /u/Flamesword200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</guid>
      <pubDate>Fri, 24 Jan 2025 12:50:18 GMT</pubDate>
    </item>
    <item>
      <title>关于强盗贪婪策略的新手问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8n57l/noob_question_about_greedy_strategy_on_bandits/</link>
      <description><![CDATA[考虑 10 臂老虎机问题，从每个动作的初始奖励估计 0 开始。假设代理尝试的第一个动作的奖励为正。该动作的平均奖励的真实值也为正。还假设此特定动作的奖励的“正态分布”几乎完全为正（因此，从此动作获得负奖励的可能性非常低）。 贪婪策略是否会探索任何其他动作？    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8n57l/noob_question_about_greedy_strategy_on_bandits/</guid>
      <pubDate>Fri, 24 Jan 2025 04:20:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么要对卷出缓冲区数据进行随机排序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</link>
      <description><![CDATA[在 SB3 的循环缓冲区文件 (https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/recurrent/buffers.py) 中，第 182 行表示要在保留序列的同时对数据进行混洗，代码会在随机点拆分数据，交换每个拆分，然后将其重新连接在一起。  我的问题是，为什么这对于混洗来说已经足够好了，但我们为什么要首先对推出的数据进行混洗呢？   由    /u/AUser213  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</guid>
      <pubDate>Thu, 23 Jan 2025 23:31:30 GMT</pubDate>
    </item>
    <item>
      <title>IsaacSim 人形机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</link>
      <description><![CDATA[我需要一些帮助在 IsaacSim 中构建人形机器人演示，但除了开箱即用的人形机器人 (H1) 之外没有其他可用的东西，有人对 Neo、Sanctuary 等机器人的人形机器人政策有任何线索吗    提交人    /u/sohaib_01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</guid>
      <pubDate>Thu, 23 Jan 2025 21:18:29 GMT</pubDate>
    </item>
    </channel>
</rss>