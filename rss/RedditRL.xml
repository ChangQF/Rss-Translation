<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 19 Mar 2024 09:13:20 GMT</lastBuildDate>
    <item>
      <title>了解用于自定义 mujoco env 的gymnasium.make()</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bifl5f/understanding_gymnasiummake_for_custom_mujoco_envs/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bifl5f/understanding_gymnasiummake_for_custom_mujoco_envs/</guid>
      <pubDate>Tue, 19 Mar 2024 08:45:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习库 PyTorch</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bifiho/reinforcement_learning_library_pytorch/</link>
      <description><![CDATA[嗨， 我是强化学习领域的新手，目前正在寻找合适的库来实施我的深度强化学习研究项目。它应该是一个 Python 库，并使用 PyTorch 进行深度学习集成。我想在（多个）GPU 上并行运行多个环境。由于我的环境将包含神经网络，因此它应该能够直接在 GPU 上运行并使用张量。 有关合适库的任何经验。到目前为止，我发现了两个有趣的库  TorchRL TorchRL - torchrl 主要文档（pytorch.org）  自主学习库自主学习库 —autonomous-learning-library 0.9.1 文档  我想听听您的意见和经历。谢谢  &amp;# 32；由   提交/u/Opposite_Youth_442   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bifiho/reinforcement_learning_library_pytorch/</guid>
      <pubDate>Tue, 19 Mar 2024 08:39:56 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线中的自定义训练循环3？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bieqsf/custom_training_loop_in_stable_baselines3/</link>
      <description><![CDATA[我正在使用稳定的基线3，并且想知道如何实现以下情况。我想使用 PPO 代理： 我的环境中有 x 个可能的位置来执行操作。这里数字 x 是一个变量，取决于环境状态。在每个位置，都有一组可能的操作。 例如与我的环境类似的是跳棋游戏，棋子的数量会发生变化，但每个棋子的规则都是固定的。 我的环境具有周期性边界条件，这意味着棋盘上的某些东西会留在棋盘上左侧进入右侧，顶部留下的东西进入底部。  我想实现一个训练循环，在其中提供不同的视角，即对每个位置 x 进行一次观察，其中 x 居中。然后，我希望代理对操作概率最高的位置采取操作。有没有办法可以改变训练循环以获得这些概率，然后选择一个动作？因此，我必须在获取观察结果和选择操作之间调整代码。在某种程度上，我想对应该选择哪些操作/代理应该在哪个位置执行操作施加规则。对我来说特别困难的是位置 x 的数量各不相同 如果您对如何解决这个学习问题有其他想法，请告诉我！ :)   由   提交/u/ilse1301  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bieqsf/custom_training_loop_in_stable_baselines3/</guid>
      <pubDate>Tue, 19 Mar 2024 07:41:27 GMT</pubDate>
    </item>
    <item>
      <title>给对强化学习感兴趣的澳大利亚人的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bi9v8q/advice_for_an_australian_with_an_interest_in_rl/</link>
      <description><![CDATA[您好，我是澳大利亚人，目前在墨尔本 RMIT 攻读计算机科学学士学位。我对深度学习，更具体地说是强化学习非常感兴趣，因此我打算明年开始攻读人工智能硕士学位，然后再攻读深度学习博士学位。 我真的很想进入研究领域在一家大公司工作并尽可能地帮助挑战极限，但我不确定我在澳大利亚的工作/研究前景。此外，我不确定墨尔本大学莫纳什大学的博士学位是否会给我足够好的证书来实现我想要的目标。 我真的只是想知道这是否最适合我学习在这里并在这里找到研究，或者是否去海外更好，如果是的话在什么阶段（在博士学位之前或之后等）。任何建议将不胜感激。   由   提交 /u/TrueExcaliburGaming   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bi9v8q/advice_for_an_australian_with_an_interest_in_rl/</guid>
      <pubDate>Tue, 19 Mar 2024 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>RL 流量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bi5730/rl_in_trafic/</link>
      <description><![CDATA[嗨，我想在交通灯中添加 RL，所以我已经尝试过整洁，但在 2600 gen 之后我对结果不满意，还有其他吗我可以使用算法来创建该代理，或者任何人尝试将 RL 添加到交通灯，以及任何想法添加什么来创建多个代理环境，谢谢  &amp; #32；由   提交/u/Rude_Personne  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bi5730/rl_in_trafic/</guid>
      <pubDate>Mon, 18 Mar 2024 23:16:23 GMT</pubDate>
    </item>
    <item>
      <title>收敛到单一动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhvxi8/convergence_to_a_single_action/</link>
      <description><![CDATA[有点宽泛的问题，但在实现 RL 时，我的代理似乎很常见地会收敛到一个始终只选择 1 个操作的解决方案。它发生在 Pong 的深度 Q 学习代理和我正在攻读博士学位的多物理模型中的深度交叉熵代理中。任何人都知道可能发生这种情况的任何常见原因吗？    由   提交/u/Chewden_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhvxi8/convergence_to_a_single_action/</guid>
      <pubDate>Mon, 18 Mar 2024 17:07:39 GMT</pubDate>
    </item>
    <item>
      <title>手术配额MARL调度程序开发</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/</link>
      <description><![CDATA[嗨，我来自一个大学生强化学习团队，我们将创建一个新的多智能体合作对抗环境来学习最优策略传入请求的短期调度（在我们的例子中是外科手术）。请看一下当前的环境描述。缺什么？也许有什么多余的东西？您与我们分享的任何经验将不胜感激！  这是 GitHub 存储库：https://github.com/artemisak/Surgery-配额调度程序/树/主   由   提交 /u/Pythonic-af   /u/Pythonic-af  reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/</guid>
      <pubDate>Mon, 18 Mar 2024 15:38:55 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家无法过度拟合？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhta0e/actorcritic_unable_to_overfit/</link>
      <description><![CDATA[我目前正在开展一个项目，我正在尝试优化演员评论家模型，以根据某些输入数据最大化高分。然而，我面临着一个意想不到的挑战 - 尽管我打算出于实验目的这样做，但我似乎无法让我的模型过度拟合。 以下是我的模型设置的简要概述：  p&gt;  我的演员损失是策略梯度损失 -log(概率)*其决策的优势。 优势 = (前一帧的 Δscore) - (评论家估计的 Δscore评论家根据（来自前一帧的Δ分数）和（评论家根据前一帧估计的Δ分数）之间的MSE更新评论家  当我将批评者从第一帧开始的估计 Δscore 与实际 Δscore 进行比较时，我观察到零相关性，这令人费解 我正在一个非常小的数据集上训练模型 - 只有 1 或 2 集每个大约有 30 个事件。我的期望是模型能够轻松记住与这些事件相关的操作，并完全适应数据。  我检查了更新步骤之前和之后的梯度，所有内容看起来都已填充​​，所以我认为这就是“学习”。我似乎看不到任何问题，但对为什么我不能过度适应感到有点困惑。有没有从业者看到这个问题/知道明显的解决办法或检查我在这里遗漏的东西？  我尝试对 Actor/Critic 使用 LSTM 和 Transformer 模型，其中有 2 层，每层大小约为 200，丢失率为 0.25，学习率为 1e-5。    由   提交 /u/Rhyno_Time   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhta0e/actorcritic_unable_to_overfit/</guid>
      <pubDate>Mon, 18 Mar 2024 15:19:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助解释 NEAT 输出</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhrnaj/help_to_explain_neat_output/</link>
      <description><![CDATA[有人可以解释一下如何从 NEAT 的基本教程中导出生成的输出吗？ 如果我的输入是 (1.0, 1.0），我应该使用哪些权重和偏差来重现 0.06994251237198218？我熟悉 sigmoid 函数，但无法重现任何输出数字。 ​ 最佳基因组：密钥：6295 健身： 3.923495216838884 节点：0 DefaultNodeGene(key=0，bias=-0.3608400593343819，response=1.0，activation=sigmoid，aggregation=sum) 53 DefaultNodeGene(key=53，bias=-0.5517336026220021，response=1.0，activation=sigmoid，aggregation=sum) ） 838 DefaultNodeGene（密钥= 838，偏差= -0.5789866669875736，响应= 1.0，激活= sigmoid，聚合=和） 931 DefaultNodeGene（密钥= 931，偏差= -0.8513574951787184，响应= 1.0，激活= sigmoid，聚合=和） 1219 DefaultNodeGene(key=1219，bias=-0.5135172011791014，response=1.0，activation=sigmoid，aggregation=sum) 连接：DefaultConnectionGene(key=(-2, 0)，weight=0.6428472619552054，enabled=True) DefaultConnectionGene(key=(- 2, 53), 权重=-1.6935071067952197, 启用=False) DefaultConnectionGene(key=(-2, 1219), 权重=0.8991060064585895, 启用=True) DefaultConnectionGene(key=(-1, 0), 权重=-0.8006221066141777, 启用=真）DefaultConnectionGene（键=（-1，53），权重= 1.2070540852248426，启用=真）DefaultConnectionGene（键=（53，0），权重= 2.3774714023438124，启用=真）DefaultConnectionGene（键=（1219，53），权重=-2.5095137599707673，启用=True）输出：输入（0.0，0.0），预期输出（0.0，），得到[0.18185023040647358]输入（0.0，1.0），预期输出（1.0，），得到[0.8037737011422771]输入（1.0） , 0.0), 预期输出 (1.0,), 得到 [0.9937902124091639] 输入 (1.0, 1.0), 预期输出 (0.0,), 得到 [0.06994251237198218]  ​   由   提交/u/williego  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhrnaj/help_to_explain_neat_output/</guid>
      <pubDate>Mon, 18 Mar 2024 14:09:59 GMT</pubDate>
    </item>
    <item>
      <title>ExploRLLM：利用大型语言模型指导强化学习探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhqieo/explorllm_guiding_exploration_in_reinforcement/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.09583 网站：https://explorllm。 github.io/ 摘要：在具有较大观察和动作空间的基于图像的机器人操作任务中，强化学习面临样本效率低、训练速度慢、和不确定的收敛性。作为替代方案，大型预训练基础模型在机器人操作方面显示出了前景，特别是在零样本和少样本应用中。然而，由于推理能力有限以及理解物理和空间环境方面的挑战，直接使用这些模型是不可靠的。本文介绍了 ExploRLLM，这是一种利用基础模型（例如大型语言模型）的归纳偏差来指导强化学习探索的新颖方法。我们还利用这些基础模型来重新制定行动和观察空间，以提高强化学习的训练效率。我们的实验表明，引导探索比没有引导探索的训练能够更快地收敛。此外，我们还验证了 ExploRLLM 的性能优于普通基础模型基线，并且在模拟中训练的策略可以应用于现实环境，而无需额外训练。   由   提交 /u/Tbd_Sparks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhqieo/explorllm_guiding_exploration_in_reinforcement/</guid>
      <pubDate>Mon, 18 Mar 2024 13:18:05 GMT</pubDate>
    </item>
    <item>
      <title>DQN 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhoyx9/dqn_implementation/</link>
      <description><![CDATA[我想要一个在连续观察空间环境中实现 DQN 的示例，最好是在 SB3 中。   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhoyx9/dqn_implementation/</guid>
      <pubDate>Mon, 18 Mar 2024 11:59:22 GMT</pubDate>
    </item>
    <item>
      <title>剖析高更新率的深度强化学习：对抗价值高估和发散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhntvr/dissecting_deep_rl_with_high_update_ratios/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.05996 摘要：  我们证明深度强化学习可以保持其学习能力，而无需在梯度更新数量大大超过环境样本数量的设置中重置网络参数。在如此大的更新数据比率下，Nikishin 等人最近的一项研究。 （2022）提出了一种首要偏见，即代理过度适应早期的互动并淡化后来的经验，从而损害了他们的学习能力。在这项工作中，我们剖析了首要偏见背后的现象。我们检查了训练的早期阶段可能导致学习失败的原因，发现一个根本的挑战是一个长期存在的认识：价值高估。过度夸大的 Q 值不仅出现在分布外的数据上，而且还出现在分布内的数据上，并且可以追溯到由优化器动量推动的看不见的动作预测。我们采用简单的单位球归一化，可以在大更新率下进行学习，在广泛使用的 dm_control 套件上展示其功效，并在具有挑战性的狗任务上获得强大的性能，与基于模型的方法相竞争。我们的结果部分地质疑了由于早期数据过度拟合而导致的次优学习的先前解释。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhntvr/dissecting_deep_rl_with_high_update_ratios/</guid>
      <pubDate>Mon, 18 Mar 2024 10:52:00 GMT</pubDate>
    </item>
    <item>
      <title>与 CleanRL 类似的目标条件强化学习实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhm9bu/goalconditioned_rl_implementations_similar_to/</link>
      <description><![CDATA[大家好！ 我目前正在开发一个目标条件机器人项目（带有自定义环境）并面临一些挑战。我之前使用过 Stable Baselines3 (SB3)，我发现它相当全面，但对于我当前的需求来说有点复杂。  我希望创建一个自定义策略（模块化）并将迁移学习应用于不同的任务，这对于 SB3 来说似乎要复杂得多。因此，我对类似于 CleanRL 的实现特别感兴趣，因为它简单明了，但支持目标条件环境。  您能否推荐任何可能符合这些要求的资源、存储库或框架？ 以下是我正在寻找的内容的简要概述：  像 CleanRL 一样简单透明的框架或库。 支持目标条件环境（与 OpenAI Gym 的 GoalEnv 兼容）。 灵活地自定义策略网络，包括为网络的不同部分设置不同的学习率。   谢谢   由   提交 /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhm9bu/goalconditioned_rl_implementations_similar_to/</guid>
      <pubDate>Mon, 18 Mar 2024 09:05:03 GMT</pubDate>
    </item>
    <item>
      <title>使用开放人工智能健身房对超级马里奥兄弟实施 NEAT 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhlgq7/implementation_of_neat_algorithm_using_open_ai/</link>
      <description><![CDATA[我正在开发一个项目来实现 NEAT 算法，以便它学习玩超级马里奥兄弟（在开放 AI 健身房中）我想知道是否有是否有任何资源或人员已经完成此操作，以便我更好地了解如何执行此操作，感谢任何帮助 谢谢!! &lt;!-- SC_ON - -&gt;  由   提交 /u/MinuteNo5493   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhlgq7/implementation_of_neat_algorithm_using_open_ai/</guid>
      <pubDate>Mon, 18 Mar 2024 08:04:28 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>