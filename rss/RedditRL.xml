<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 23 Dec 2024 06:25:30 GMT</lastBuildDate>
    <item>
      <title>具有 O（log（T））遗憾界限的随机强盗算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk9kl7/stochastic_bandit_algorithms_with_ologt_regret/</link>
      <description><![CDATA[是否有任何随机老虎机算法具有比 O(sqrt(T)) 更好的遗憾界限？不是 O( \sqrt(T log T) )，而是 O( \log(T) ) 或至少 O( sqrt(log T) )？ 我知道具有 K 个臂的 UCB 具有 O( \sqrt(kT) ) 的遗憾界限。它是否表明 UCB 或其变体可以实现 O( log T ) 遗憾，或者至少 O( sqrt(log T) )？    提交人    /u/Anxious_Positive3998   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk9kl7/stochastic_bandit_algorithms_with_ologt_regret/</guid>
      <pubDate>Sun, 22 Dec 2024 22:34:42 GMT</pubDate>
    </item>
    <item>
      <title>SAC 中的 Pytorch 梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk8wlz/pytorch_gradients_in_sac/</link>
      <description><![CDATA[我在理解如何在 pytorch 中实现软演员评论家时计算梯度时遇到了麻烦。具体来说，多个神经网络的梯度是如何耦合的，以及何时计算梯度，何时不计算梯度。 我有一个基本的实现，与 Phil Tabor 的 YouTube 视频非常相似。我的实现是这里。我的问题是我不明白 `retain_graph=True` 字段是如何工作的。我在这个实现中也没有使用 .detach() 或 torch.no_grad。也许这是一个单独的问题，但是如果我只是要在调用 .backwards() 之前将梯度归零，那么执行 .detach() 有什么意义呢？ 我的目标是执行相同的实现，但不使用 `retain_graph=True` 参数，因为我不完全理解它的用途。我知道它在反向传播后会保持梯度，但我不明白在 SAC 中这样做的目的。我尝试在没有这个参数的情况下执行此操作，但是，我就是无法让它工作。该代码可以在此处看到。 任何帮助都将不胜感激！    提交人    /u/LostBandard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk8wlz/pytorch_gradients_in_sac/</guid>
      <pubDate>Sun, 22 Dec 2024 22:02:20 GMT</pubDate>
    </item>
    <item>
      <title>训练模型来学习在大规模离散动作空间中进行跟踪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk1pjv/training_a_model_to_learn_tracing_on_massive/</link>
      <description><![CDATA[大家好！我正在开展一个实验性的强化学习项目，用于 PCB 布线。该项目的理念是训练强化学习代理连接巨大的 2D 电路板上的输入和输出点（始终位于中间），电路板可能大到 20,000×20,000 个单元，并带有 3 像素宽的迹线、10 像素间距、不可通行点等限制。 目前，我正在尝试尽可能简化问题，减少限制，但仍打算在大型电路板上进行训练。现在，我只想让模型学习如何从随机输入/输出对中跟踪一条线，即使这样也相当具有挑战性。 它基本上感觉像是一种寻路或“贪吃蛇游戏”设置，但我不确定强化学习是否真的可以处理这种大小的输入。我还没有找到任何类似的项目可以比较，所以我怀疑 RL 是否是合适的工具。 我尝试过 DQN 方法（很快就被丢弃了，在 50x50 以上的棋盘上很吃力），基于 CNN 的方法（在大型棋盘上很吃力），现在我正在探索分层 RL。这感觉最有前途，但我仍然不确定它的扩展性如何。 我是这个领域的初学者，之前主要解决的是一些较小的问题。任何建议或参考都将不胜感激！ 谢谢！    提交人    /u/Deranged_Koala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk1pjv/training_a_model_to_learn_tracing_on_massive/</guid>
      <pubDate>Sun, 22 Dec 2024 16:19:03 GMT</pubDate>
    </item>
    <item>
      <title>如何学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk03kn/how_to_learn_reinforcement_learning/</link>
      <description><![CDATA[您好。我是一个编程经验超过 40 年的老家伙，想学习更多关于强化学习的知识，也许可以用强化学习编写一个简单的游戏，比如跳棋。 我想更好地理解强化学习的数学。我已经几十年没有接触过微积分了，但我相信只要努力，我就能学会。而且，我更喜欢亲自动手做一些编码的事情，以证明我确实理解了我正在学习的内容。 我在网上看过一些教程，它们似乎都使用了一些 RL 库，我假设这些库只是封装并隐藏了实际的数学知识，或者它们是对数学的高级讨论。 在哪里可以找到在线或书籍形式的关于理论和数学或机器学习的讨论，并在编程世界中进行应用练习？    提交人    /u/EricTheNerd2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk03kn/how_to_learn_reinforcement_learning/</guid>
      <pubDate>Sun, 22 Dec 2024 14:59:56 GMT</pubDate>
    </item>
    <item>
      <title>Mac mini m4 适合现实生活吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hjtjos/mac_mini_m4_for_rl/</link>
      <description><![CDATA[对于现实生活来说，这是否是个不错的选择？我正在考虑在这两者之间做出选择，或者自己组装一台电脑，但在我住的地方，一台像样的电脑可能要花两倍的钱。  我工作的环境并不复杂。    提交人    /u/Hulksulk666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hjtjos/mac_mini_m4_for_rl/</guid>
      <pubDate>Sun, 22 Dec 2024 07:13:00 GMT</pubDate>
    </item>
    <item>
      <title>1 年 Perplexity Pro 促销代码仅需 25 美元</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hj7of6/1year_perplexity_pro_promo_code_for_25/</link>
      <description><![CDATA[仅需 25 美元即可获得 1 年 Perplexity Pro 促销代码（节省 175 美元！） 以合理的价格使用顶级模型和工具增强您的 AI 体验： 高级 AI 模型：访问 GPT-4o、o1 和Llama 3.1 还利用了 Claude 3.5 Sonnet、Claude 3.5 Haiku 和 Grok-2。 图像生成：探索 Flux.1、DALL-E 3 和 Playground v3 Stable Diffusion XL 可供无有效 Pro 订阅的用户使用，可在全球范围内访问。 简单的购买流程： 加入我们的社区： Discord 拥有 550 多名成员。 安全付款：使用 PayPal 保障您的安全和买家保护。 即时访问：通过简单的促销链接接收您的代码。 为什么选择我们？ 我们的业绩记录不言而喻。 查看我们的已验证买家 + VIP 买家 其他可用产品：LinkedIn Premium、IPTV（19000 个频道）    由   提交  /u/minemateinnovation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hj7of6/1year_perplexity_pro_promo_code_for_25/</guid>
      <pubDate>Sat, 21 Dec 2024 11:03:12 GMT</pubDate>
    </item>
    <item>
      <title>实施 DQN 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hj6s2c/probem_implementing_dqn/</link>
      <description><![CDATA[您好，我是一名计算机工程师，正在攻读人工智能和机器人学硕士学位。我碰巧必须实现深度学习论文，总的来说，我没有遇到任何问题。我正在学习 RL，我试图通过阅读论文从头开始编写 DQN 的实现。然而，尽管架构很简单，但我在实现它时遇到了问题。 他们特别说：  第一个隐藏层将 16 个 8 × 8 滤波器与输入图像进行卷积，步长为 4，并应用整流器非线性 [10, 18]。第二个隐藏层将 32 个 4 × 4 滤波器与步长为 2 进行卷积，然后再次进行整流器非线性。最后的隐藏层是完全连接的，由 256 个整流单元组成。  让我认为有两个卷积层后面跟着一个完全连接层。我在 Hugging Face 上找到的示意图证实了这一点  ![示意图](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg) 然而在 PyTorch RL 教程 中，他们使用了这个网络： ```python class DQN(nn.Module): def init(self, nobservations, n_actions): super(DQN, self).init_() self.layer1 = nn.Linear(n_observations, 128) self.layer2 = nn.Linear(128, 128) self.layer3 = nn.Linear(128, n_actions) # 在优化期间使用一个元素调用以确定下一个操作或批次 #。返回 tensor([[left0exp,right0exp]...])。def forward(self, x): x = F.relu(self.layer1(x)) x = F.relu(self.layer2(x)) return self.layer3(x)  ``` 我不完全确定 128 来自哪里。 原始实现证实了这是预期的做法（我不是 LUA 专家，但看起来非常相似） Lua function nql:createNetwork() local n_hid = 128 local mlp = nn.Sequential() mlp:add(nn.Reshape(self.hist_len*self.ncols*self.state_dim)) mlp:add(nn.Linear(self.hist_len*self.ncols*self.state_dim, n_hid)) mlp:add(nn.Rectifier()) mlp:add(nn.Linear(n_hid, n_hid)) mlp:add(nn.Rectifier()) mlp:add(nn.Linear(n_hid, self.n_actions)) return mlp end  我在网上找到了各种实现，都使用了相同的架构。我显然遗漏了一些东西，但有人知道问题可能是什么吗？     提交人    /u/Puddino   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hj6s2c/probem_implementing_dqn/</guid>
      <pubDate>Sat, 21 Dec 2024 09:56:00 GMT</pubDate>
    </item>
    <item>
      <title>“偷取免费午餐：揭露 Dyna 式强化学习的局限性”，Barkley 和 Fridovich-Keil</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</link>
      <description><![CDATA[      我认为这是适合讨论这个问题的地方，但无论如何，对于这种无耻的自我推销，我深表歉意…… 论文：https://arxiv.org/abs/2412.14312 关于 X 的相当不错的 TLDR：https://x.com/bebark99/status/1869941518435512712 最近，作为研究的一部分，我对基于模型的 RL 非常感兴趣，但很快遇到了一个大问题：该领域一些主要方法的原始 PyTorch 实现速度非常慢。除非您有数百个 GPU 小时可供使用，否则几乎不可能完成任何合理的工作。 因此，我决定在 JAX 中重新实现一个知名算法，即基于模型的策略优化 (MBPO)，这让事情运行得更快。然而，经过大量的故障排除和测试后，我遇到了另一个惊喜：只要您尝试在不同于他们论文中测试的基准（即 DMC 而不是 Gym）上从头开始训练它，它的表现就会比简单的离线策略算法差，后者需要的训练时间要少几个数量级。 这涉及 6 个 gym 环境和 15 个 DMC 环境，因此非常一致。 这让我很好奇，经过一番挖掘，我和我的导师最终写了一篇关于它和其他 Dyna 风格的基于模型的 RL 方法的论文。剧透：并非所有 dyna 风格的方法都无法在基准测试中发挥作用，但当 Dyna 失败时，这并不是一个孤立或简单的问题。 如果有人有兴趣尝试的话，JAX 实现应该会在明年推出。希望得到大家的反馈！ https://preview.redd.it/ei78d7hbz28e1.png?width=3600&amp;format=png&amp;auto=webp&amp;s=c981756b0f0803e12b2e0e8e6a7816eedb085cfe    提交人    /u/darthbark   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</guid>
      <pubDate>Fri, 20 Dec 2024 22:32:37 GMT</pubDate>
    </item>
    <item>
      <title>表格软 q 学习 停留在简单的网格世界</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</link>
      <description><![CDATA[您好，我正在为一个简单的 5x5 网格世界开发一个简单的表格软 q 学习代理。经过几次尝试后，它卡在了特定状态。我不知道这是实施错误还是超参数不好。我将在下面附上代码。还有其他建议吗？ 谢谢 import numpy as np import time import os class Env(): def __init__(self): self.height = 5 self.width = 5 self.posX = 0 self.posY = 0 self.endX = self.width-1 self.endY = self.height-1 self.actions = [0, 1, 2, 3] self.stateCount = self.height*self.width self.actionCount = len(self.actions) def reset(self): self.posX = 0 self.posY = 0 self.done = False return 0, 0, False # 采取行动 def step(self, action): if action==0: # 左 self.posX = self.posX-1 if self.posX&gt;0 else self.posX if action==1: # 右 self.posX = self.posX+1 if self.posX&lt;self.width-1 else self.posX if action==2: # 向上 self.posY = self.posY-1 if self.posY&gt;0 else self.posY if action==3: # 向下 self.posY = self.posY+1 if self.posY&lt;self.height-1 else self.posY done = self.posX==self.endX and self.posY==self.endY # 将 (x,y) 位置映射到 0 到 5x5-1=24 之间的数字 nextState = self.width*self.posY + self.posX reward = 1 if done else -0.1 return nextState, reward, done # 返回一个随机动作 def randomAction(self): return np.random.choice(self.actions) # 显示环境 def render(self): for i in range(self.height): for j in range(self.width): if self.posY==i and self.posX==j: print(&quot;O&quot;, end=&#39;&#39;) elif self.endY==i and self.endX==j: print(&quot;T&quot;, end=&#39;&#39;) else: print(&quot;.&quot;, end=&#39;&#39;) print(&quot;&quot;) def softmax(x): e_x = np.exp(x - np.max(x)) # 为了数值稳定性 return e_x / e_x.sum() class Agent: def __init__(self, stateCount, actionCount, env, max_steps = 100, epochs = 50, discount_factor = 0.99, lr = 0.1, temp = 1): # Q 表：包含每个 (state,action) 对的 Q 值 self.Q = np.zeros((stateCount, actionCount)) # 超参数 self.temp = temp self.lr = lr self.epochs = epochs self.discount_factor = discount_factor # 环境 self.env = env self.max_steps = max_steps def getV(self, q_value): return self.temp * np.log(np.sum(np.exp(q_value / self.temp))) def choose_action(self, state): # q = self.Q[state] # v = self.getV(q) # dist = np.exp((q - v) / self.temp) # action_probs = dist / np.sum(dist) # return np.random.choice(env.actions, p=action_probs) action_probs = softmax((self.Q[state] - self.getV(self.Q[state])) / self.temp) return np.random.choice(env.actions, p=action_probs) # 训练循环 def run(self): for i in range(self.epochs): state, reward, done = self.env.reset() steps = 0 while not done: os.system(&#39;cls&#39;) # print(self.Q) print(&quot;epoch #&quot;, i+1, &quot;/&quot;, self.epochs) self.env.render() time.sleep(0.01) # 计数完成游戏的步骤 steps += 1 # 选择软 q 学习动作 action = self.choose_action(state) # 采取行动 next_state, reward, done = self.env.step(action) # 使用贝尔曼方程更新 Q 表值 # target = reward + self.discount_factor * np.sum(action_probs * self.Q[next_state]) # target = reward + self.discount_factor * self.getV(self.Q[next_state]) target = reward + (1 - done) * self.discount_factor * self.getV(self.Q[next_state]) self.Q[state][action] += self.lr * (target - self.Q[state][action]) # 更新状态 state = next_state if steps &gt;= self.max_steps: break print(&quot;\nDone in&quot;, steps, &quot;steps&quot;.format(steps)) time.sleep(0.8) def print_q_table(self): for i in range(0,len(self.Q)): for j in range(0,len(self.Q[i])): print(self.Q[i][j], end=&quot; &quot;, flush=True) print(&quot;&quot;) if __name__ == &quot;__main__&quot;: # 创建 CartPole 类的实例 env = Env() resolver = Agent(env.stateCount, env.actionCount, env) resolver.run()    由    /u/Majestic-Tap1577  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</guid>
      <pubDate>Fri, 20 Dec 2024 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>微网格 16x16 动态障碍物解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisobe/minigrid_16x16_dynamic_obstacles_solution/</link>
      <description><![CDATA[我使用 minigrid 和 rl 启动文件。我正在努力解决 16x16（和 8x8）动态迷宫，我目前正在使用 python -m scripts.train --algo ppo --env MiniGrid-Dynamic-Obstacles-16x16-v0 --model DoorKey --save-interval 10 --recurrence 8。你知道我应该更改哪些其他超参数吗？只有 3x3 动态迷宫对我来说是可解的    提交人    /u/More_Peanut1312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisobe/minigrid_16x16_dynamic_obstacles_solution/</guid>
      <pubDate>Fri, 20 Dec 2024 20:16:05 GMT</pubDate>
    </item>
    <item>
      <title>[AHT] 对 SOMALI CAT 环境的查询</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</link>
      <description><![CDATA[我刚刚阅读了两篇关于临时团队合作的非常有趣的论文，重点关注沟通如何提高此类任务的效率，&quot;A Penny for Your Thoughts: The Value of Communication in Ad Hoc Teamwork&quot; 和 &quot; Expected Value of Communication for Planning in Ad Hoc Teamworks&quot;。作者曾在一个名为 SOMALI CAT 的环境中工作过，我想知道：这里有没有人曾经在这个环境中工作过，或者你是否知道它是否可以在某些通用 Python RL 测试框架（例如 Gymnasium）上使用。 感谢您的帮助    提交人    /u/potatodafish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</guid>
      <pubDate>Fri, 20 Dec 2024 18:43:58 GMT</pubDate>
    </item>
    <item>
      <title>预测行动和奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</link>
      <description><![CDATA[大家好，我正在处理一个非专家级数据的数据集，并且正在使用决策转换器。现在我想比较性能，但我找不到可以同时预测动作和奖励的离线 rl 模型。有人有什么建议吗？    提交人    /u/Anonymusguy99   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</guid>
      <pubDate>Fri, 20 Dec 2024 13:45:40 GMT</pubDate>
    </item>
    <item>
      <title>救命！我的 RL 代理没有学习。（OpenAI Gym env + Pytorch）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</link>
      <description><![CDATA[我试图实现一个简单的深度 Q 网络，以便在 OpenAI Gymnasium 提供的 Cart Pole 环境中训练代理。我尝试调整超参数，但似乎没有任何效果。事实上，随着 epoch 的增加，情况似乎变得更糟（虽然不确定）。我觉得我已经正确地实现了一切。我正在使用 pytorch 来构建神经网络。我对 RL 和深度学习还不熟悉，所以如果我遗漏了什么，我深表歉意。 我附上了我的代码，以便您可以运行它。该笔记本非常不言自明，除了 pytorch 之外，您只需要 gyanasium[classic-control] 和 pygame 包。 https://github.com/Utsab-2010/OpenAI-Gym-RL-Tests/blob/main/Cart_Pole_Deep_QN.ipynb 任何建议或帮助都将不胜感激。    提交人    /u/Otaku_boi1833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</guid>
      <pubDate>Fri, 20 Dec 2024 08:04:59 GMT</pubDate>
    </item>
    <item>
      <title>DDPG、A2C、PPO 和 TRPO 实验在 StableBaselines3 中何时收敛？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</link>
      <description><![CDATA[我需要运行所有基于策略梯度的 RL 实验，以展示它们在使用和不使用我计划提出的自定义训练方案的情况下收敛的速度。对于环境，我正在考虑 OpenAI Gym 环境（CartPole、Pendulum、LunarLander 和 2-3 Mujoco 环境）。但我首先需要确定它们是否与通常的训练方法收敛。即使它们不收敛，也应该有改善的迹象（就情景回报而言）。SB3 上是否有任何资源提供在这些环境中工作的超参数或策略网络？此外，是否有人对记录反向传播的数量或类似的东西（任何其他有用的指标）以评估计算负载等有任何建议？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</guid>
      <pubDate>Fri, 20 Dec 2024 06:39:36 GMT</pubDate>
    </item>
    <item>
      <title>决策频率：‘信息’视角</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</link>
      <description><![CDATA[小动作重复潜力：细粒度控制问题：信用分配 大动作重复潜力：更明智的决策问题：延迟 如果决策之间没有足够的时间，代理将根据较少的信息采取行动。 如果所述时间很长，则“适应变化”会被延迟。  推荐解决方案的示例：分层 RL：存在在快速行动的较低级别与以较慢速度行动的较高级别之间进行通信的问题。 决策转换器：离线方法，因此无法在工作中学习 根据我的经验，这个问题与计算或模型容量无关。无论学习能力有多强，智能体采取行动的频率（或缺乏的信息）都是有限的。 你对这个困境有什么看法？    提交人    /u/XecutionStyle   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</guid>
      <pubDate>Thu, 19 Dec 2024 19:48:50 GMT</pubDate>
    </item>
    </channel>
</rss>