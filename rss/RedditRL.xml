<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 12 Aug 2024 09:17:23 GMT</lastBuildDate>
    <item>
      <title>DeepMimic 中的提前终止实际上是如何运作的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eq8fak/how_early_termination_in_deepmimic_actually_works/</link>
      <description><![CDATA[我已经在我的项目中实现了 DeepMimic 的提前终止技术，并且它有效。但是，我最近意识到提前终止实际上并没有像我最初想的那样终止情节。 起初，我假设提前终止会结束情节并停止收集状态-动作-奖励轨迹。但相反，似乎轨迹继续收集直到固定情节长度（这对于运动等无限任务很常见），在满足终止条件后将奖励设置为 0。因此，在这种情况下，“提前终止”只是指在满足某些条件后将所有奖励设置为 0，而不是真正结束情节。 我感到困惑，因为在大多数情况下，如果满足提前终止条件，即使没有明确调整提前终止，奖励也会自然为 0。这让我怀疑使用提前终止和不以这种方式使用提前终止之间是否真的有什么区别。看起来这个方法没有问题，但也没有显著的进展。 据我所知，提前终止的目的是防止模型在完全崩溃的情况下进行训练，使其专注于实现预期目标。但是，如果提前终止仅在某些条件下将奖励更改为 0，这真的会有所不同吗？ 如果我们在满足提前终止条件时实际上缩短情节，而不是仅仅将奖励更改为 0，会怎么样？我目前正在试验这种方法，看看会发生什么。你怎么看？我误解了提前终止的概念吗？    提交人    /u/Any_Way2779   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eq8fak/how_early_termination_in_deepmimic_actually_works/</guid>
      <pubDate>Mon, 12 Aug 2024 08:31:23 GMT</pubDate>
    </item>
    <item>
      <title>维拉·鲁宾巡天望远镜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epuchy/the_vera_rubin_survey_telescope/</link>
      <description><![CDATA[该望远镜每 3 天将拍摄一次整个夜空的照片，并将这些数据汇编成大约 60 PB 的信息。这是亚马逊当前包含的数据量的 15 倍。 他们要求我们帮助训练他们将用来识别星系的人工智能，然后让它自行完成过去 5 年的工作。 我觉得这太神奇了，你和我一样对此感到好奇吗？    提交人    /u/Sotomexw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epuchy/the_vera_rubin_survey_telescope/</guid>
      <pubDate>Sun, 11 Aug 2024 20:17:25 GMT</pubDate>
    </item>
    <item>
      <title>内在奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epsdkz/intrinsic_rewards/</link>
      <description><![CDATA[嘿，与原始 RND 论文相反，我发现大多数实现只是添加了外在和内在奖励......为什么？有人证明它足够好吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epsdkz/intrinsic_rewards/</guid>
      <pubDate>Sun, 11 Aug 2024 18:53:20 GMT</pubDate>
    </item>
    <item>
      <title>有人可以用 AlphaZero 和 DQN 作为例子解释一下基于模型和无模型的 RL 之间的区别吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eprsxu/can_someone_explain_the_difference_between/</link>
      <description><![CDATA[大家好， 我理解基于模型的方法（如 MCTS）需要模型来实际执行而不是仅仅估计未来状态和奖励，从而模拟未来状态和奖励。我还理解无模型方法（如 Q-Learning）不需要模型，而是只需执行操作并估计未来状态和奖励。 我不明白为什么 AlphaZero 被认为是基于模型的，而 DQN 被认为是无模型的。 AlphaZero 构建搜索树，使用 PUCT 平衡探索和利用。mcts 的推出部分被 ResNet 架构取代，用于估计策略和价值。这里如何涉及模型？ DQN 使用神经网络估计 q 函数值。与策略和价值估计的区别在哪里？如果 Alpha Zero 是基于模型的，为什么它是无模型的？    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eprsxu/can_someone_explain_the_difference_between/</guid>
      <pubDate>Sun, 11 Aug 2024 18:29:09 GMT</pubDate>
    </item>
    <item>
      <title>RL 是纯粹随机的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eppwzx/is_rl_purely_random/</link>
      <description><![CDATA[大家好，我想在这里复习一下我的 RL 基础知识，有个问题想问。 RL 中的训练，即寻找最优策略，它总是纯粹随机的吗？我们能否保证，如果我们的代理发现自己处于某种状态，它们是否会 100% 找到与找到不同起点的另一个代理相同的最优策略？还是它仅仅取决于环境？    提交人    /u/TittyMcSwag619   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eppwzx/is_rl_purely_random/</guid>
      <pubDate>Sun, 11 Aug 2024 17:09:33 GMT</pubDate>
    </item>
    <item>
      <title>NPG 的参数化不变性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epo03a/parametrization_invariance_of_npg/</link>
      <description><![CDATA[嗨，我正在研究自然策略梯度，但我不太明白 NPG 所谓的参数化不变性。显然，网络的参数化并不重要，梯度在策略空间中指向同一个方向。 我想我感到困惑的是，什么才算是一个网络到另一个网络的重新参数化？我假设两个网络必须具有相同数量的参数。为什么这种不变性会因原始策略梯度而失效？    提交人    /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epo03a/parametrization_invariance_of_npg/</guid>
      <pubDate>Sun, 11 Aug 2024 15:49:33 GMT</pubDate>
    </item>
    <item>
      <title>[MORL] 尝试掌握一种开发代码的直观方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epkgq5/morl_trying_to_grasp_an_intuitive_approach_for/</link>
      <description><![CDATA[几周以来，我一直试图通过阅读大量论文来了解 MORL。但问题是，即使阅读了这些论文，我也无法直观地掌握这种算法的工作原理。 假设我有一个 PPO 算法，它可以在许多环境中工作。在新的环境中，我想在多目标设置中使用它（当然具有不同的超参数）。我期望的是，让不同的代理学习不同的策略（我不考虑使用标量化函数的情况）。 所以现在我遇到了一个问题，我需要“权衡”不同的代理，以达到一个大目标（例如，使用末端执行器到达 3D 空间中的某个位置）。在这篇论文中，我发现了一个可能的概念：他们的代码返回一个 V 函数向量，每个代理一个。但他们使用 DQN 而不是 PPO。我的系统是连续的（观察、行动和奖励空间是连续的），我需要一个强大的算法来在我的环境中训练我的代理。 我查看了 GPI（我完全不理解）和 PGMORL（理论上更容易，但在实施中仍然存在问题）。最近，我想出了以下论文，描述了 MO-MPO。 但我正在寻找的是 MORL 实现的直观图景，以便能够理解这篇论文。遗憾的是，似乎关于这个主题的每篇论文都朝着不同的方向发展，因此很难得到一个具有统一理论的清晰图景（至少对我来说）。 是否有可能描述一种伪算法，使 MORL 实现在理论上如何工作更清晰？ 顺便说一句，我甚至找到了框架 AgileRL，它混合了进化技术的元素。遗憾的是，它非常接近 MORL，但并不完全相同。    提交人    /u/WilhelmRedemption   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epkgq5/morl_trying_to_grasp_an_intuitive_approach_for/</guid>
      <pubDate>Sun, 11 Aug 2024 13:11:21 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型中的状态变量是否需要直接更新方程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epek38/do_state_variables_in_rl_models_need_direct/</link>
      <description><![CDATA[我正在开发一个模拟模型，其中我正在使用 RL 优化目标函数。我的模拟提供了对许多状态变量的访问，但我的 RL 代理只需要这些状态的一个子集。 我不确定 RL 模型中的每个状态变量是否都需要使用我的状态空间中的可用状态进行直接更新方程。例如，我有一个“到达时间”变量 (T_{arrival})，它是根据车辆的当前位置和目的地计算得出的。如果我不在状态空间中包含位置和目的地，我是否仍可以使用 T_{arrival}，因为它是由模拟环境直接提供的？ 同样，对于充电基础设施，我有一个“充电站最早可用时间”的状态变量 (T_{charge}^{avail})。我是否需要在状态空间中包含每辆车的充电开始（T_{charge}^{start}）和结束（T_{charge}^{end}）时间来更新这一点，或者模拟可以通过内部计算状态来处理这个问题吗？ 是否有必要为每个状态变量提供明确的更新函数，或者我可以依靠模拟直接提供某些状态？    提交人    /u/Furious-Scientist   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epek38/do_state_variables_in_rl_models_need_direct/</guid>
      <pubDate>Sun, 11 Aug 2024 06:57:36 GMT</pubDate>
    </item>
    <item>
      <title>寻求强化学习算法改进研究的实验设置建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epayf7/seeking_advice_on_experimental_setup_for_rl/</link>
      <description><![CDATA[大家好， 我目前正在研究改进强化学习（RL）算法，非常希望能得到一些关于我的实验设置的建议。我计划在不同的任务中进行一系列 11 个实验：  **DeepMind Control Suite 任务：**  Cheetah-Run Finger-Spin Walker-Walk Walker-Stand Reacher-Easy Reacher-Hard  **PyBullet 环境：**  AntBulletEnv-v0 HalfCheetahBulletEnv-v0 HopperBulletEnv-v0 ReacherBulletEnv-v0 Walker2DBulletEnv-v0   我有访问权限到多台 Windows 和 Mac 计算机，我的计划是在不同的机器上运行每个实验/任务（使用多个种子和比较算法）。我的理由是算法在不同的硬件上的表现应该类似，并且由于将按实验/任务分析性能，所以这种方法应该没问题。 为了确保一致性，我在每台设备上创建了具有相同版本 Python 和其他依赖项（例如 PyTorch、TensorFlow）的 conda 环境。 **我的问题是：**  在多台设备上的这个实验设置正确吗？ 我选择的实验/任务是否适合评估和改进 RL 算法？  任何反馈或建议都将不胜感激！ 提前致谢！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epayf7/seeking_advice_on_experimental_setup_for_rl/</guid>
      <pubDate>Sun, 11 Aug 2024 03:21:45 GMT</pubDate>
    </item>
    <item>
      <title>绘制注意力图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epasze/plot_attention_map/</link>
      <description><![CDATA[      嗨！ 我最近读了一篇论文“仔细观察：弥合自我中心和我最近在“使用 Transformers 实现机器人操作的第三人称视角”上发表了一篇文章，对如何在没有 cls 标记的情况下绘制注意力图（如下图所示）感到好奇。 原始论文中的图 5 据我所知，绘制注意力图的大多数方法都是使用 cls 标记来绘制叠加图像。但是，本文中实现的视觉变换器不包含 cls 标记。 提前感谢您的时间和帮助！  这是官方实现的链接： https://github.com/jangirrishabh/look-closer.git    提交人    /u/UpperSearch4172   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epasze/plot_attention_map/</guid>
      <pubDate>Sun, 11 Aug 2024 03:13:41 GMT</pubDate>
    </item>
    <item>
      <title>利用建设性模拟进行强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eowptk/harnessing_constructive_simulations_for/</link>
      <description><![CDATA[        提交人    /u/chunkyks   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eowptk/harnessing_constructive_simulations_for/</guid>
      <pubDate>Sat, 10 Aug 2024 16:11:27 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型可以训练使用像搅拌机这样的复杂应用程序吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eourrs/can_a_rl_model_be_trained_to_use_complex_apps/</link>
      <description><![CDATA[我不是 ML 大师，我仍在学习，我对 RL 有一些兴趣。 RL 模型是否可以训练为使用复杂环境（如 blender）进行 3D 建模和动画以及其他复杂环境，还是不行？    提交人    /u/BEE_LLO   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eourrs/can_a_rl_model_be_trained_to_use_complex_apps/</guid>
      <pubDate>Sat, 10 Aug 2024 14:47:18 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eopzn2/ppo_implementation/</link>
      <description><![CDATA[嗨，我是一名大学生，正在做我的期末项目，使用强化学习进行无人机轨迹优化。我以“H. Bayerlein、P. De Kerret 和 D. Gesbert 的《通过强化学习实现自主飞行基站轨迹优化》IEEE 第 19 届无线通信信号处理进展国际研讨会 (SPAWC)，2018 年，第 1-5 页”这篇论文为参考，并使用 DQN 和 PPO 实现了相同的功能。我需要证明 PPO&gt; DQN&gt; Q 学习是我的成果。但 PPO 实现的学习速度并不比 DQN 快。您能检查一下并建议一些修改，让 PPO 实现更好吗？这是我的 github 链接 https://github.com/Divakar070/UAV/tree/main/PPO 或者建议一种前进的方式以获得所需的输出。    提交人    /u/Adventurous_Emu_5287   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eopzn2/ppo_implementation/</guid>
      <pubDate>Sat, 10 Aug 2024 10:38:08 GMT</pubDate>
    </item>
    <item>
      <title>利用脉冲神经网络进行强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eonxac/reinforcement_learning_with_spiking_neural/</link>
      <description><![CDATA[大家好，我刚刚开始我的硕士论文，我的项目是实现两个脉冲神经网络模块。更准确地说，我有一个“清醒”阶段，在这个阶段，代理和模型与环境（开放式人工智能健身房）交互并相互交互。另一个阶段是“做梦”，在这个阶段，环境处于离线状态，代理从模型中学习。你对如何实现这两个 SNN（代理和模型）有什么想法吗？从某种意义上说，在实现之前，我如何想象我的网络？    提交人    /u/Embri21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eonxac/reinforcement_learning_with_spiking_neural/</guid>
      <pubDate>Sat, 10 Aug 2024 08:14:38 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 的学习速度真的很慢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eo8927/damn_reinforce_is_really_slow_at_learning/</link>
      <description><![CDATA[      https://preview.redd.it/q8993hr5tohd1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=68a054614d29fd1a2b647af9f2854651814891d0 这是一场 1v1v1 的游戏 - 我正在使用 Self-Play 训练强化算法，在最佳反应迭代 20 次之后，它需要 10000*200=200 万场游戏模拟，一场比赛需要 100 轮（1 天的计算），才能接近 33% 的获胜概率，对抗之前最佳反应的混合策略。 只是想分享，没有别的。RIP 我“只尝试我完全理解的简单算法”的方法，将不得不尝试使用 Actor Critic。我已经在使用基线，但它并没有多大帮助。    提交人    /u/Lindayz   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eo8927/damn_reinforce_is_really_slow_at_learning/</guid>
      <pubDate>Fri, 09 Aug 2024 19:12:35 GMT</pubDate>
    </item>
    </channel>
</rss>