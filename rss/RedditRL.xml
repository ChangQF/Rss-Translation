<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 21 Sep 2024 09:15:36 GMT</lastBuildDate>
    <item>
      <title>日常生活中的 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fli9u5/rl_in_your_day_to_day/</link>
      <description><![CDATA[嗨，RL 社区， 我有 6 年的电子通讯/技术 DS 经验，但主要专注于实验和建模。我正在寻找下一个机会，希望更多地转向 RL。 我很想听听社区的意见，他们实际上是在为他们的日常角色构建 RL 系统。更具体地说，您正在解决什么类型的问题，您正在构建哪种类型的算法，等等。我针对角色领域/问题类型进行了民意调查，但也欢迎您发表评论，详细说明您使用 RL 的目的。谢谢！ 查看民意调查    提交人    /u/Djekob   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fli9u5/rl_in_your_day_to_day/</guid>
      <pubDate>Fri, 20 Sep 2024 17:58:23 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习与策略梯度在网络规模方面的比较</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fld2f3/deep_qlearning_vs_policy_gradient_in_terms_of/</link>
      <description><![CDATA[我一直在使用策略梯度和深度 Q 网络算法处理 CartPole 任务。我观察到，策略梯度算法在较小的网络（一个 16 个神经元的隐藏层）中的表现优于深度 Q 网络，后者需要更大的网络（两个分别有 1024 个和 512 个神经元的隐藏层）。学术界是否就这两种算法实现可比性能所需的网络规模达成共识？    提交人    /u/Atreya95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fld2f3/deep_qlearning_vs_policy_gradient_in_terms_of/</guid>
      <pubDate>Fri, 20 Sep 2024 14:15:30 GMT</pubDate>
    </item>
    <item>
      <title>证明遗憾的界限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1flcctx/proving_regret_bounds/</link>
      <description><![CDATA[我是一名本科生，我的研究是尝试证明在线学习问题的遗憾界限。 有没有人有资源可以帮助我从头开始熟悉遗憾分析？这些资源可以假设本科生概率的舒适度。 更新：感谢大家的建议！我最终阅读了一些论文和资源，查看了示例，这给了我一个证明的想法。我最终完成了一个遗憾界限证明！    提交人    /u/Mysterious-Ad-3855   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1flcctx/proving_regret_bounds/</guid>
      <pubDate>Fri, 20 Sep 2024 13:43:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于解决类似 VRP 的优化问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1flbj32/rl_for_vrplike_optimization_problems/</link>
      <description><![CDATA[大家好。我想就此话题征求一下您的意见： 假设我有一个组合问题，比如 TSP 或更具体地说是具有松散约束的 VRP（这涉及公共交通优化）。 我的想法是，GNN 架构可以学习有用的特征来产生良好的启发式方法，最终旨在安排良好的路线，其目标函数在某种程度上取决于用户体验（比如总旅行时间）和预算约束（例如优化冗余路线等）。 我想知道强化学习是否是正确的框架，因为最终目标最终取决于从零或预先存在的时间表开始的路线选择轨迹。 您觉得如何？你们当中有谁做过类似的事情，或者能给我推荐一些有趣的论文吗？ 另外再补充一点：我刚刚获得物理学和数据科学硕士学位，我的论文就是针对这个问题的。将 RL 融入其中的想法来自我，我很想在这个主题上深入研究，也许可以攻读博士学位来实现它。如果有人认识在 RL 方面投入精力并可能对这类问题感兴趣的教授或大学，那就太好了。谢谢大家，祝你有美好的一天！    提交人    /u/vaginedtable   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1flbj32/rl_for_vrplike_optimization_problems/</guid>
      <pubDate>Fri, 20 Sep 2024 13:05:13 GMT</pubDate>
    </item>
    <item>
      <title>在哪里以及为什么使用折扣累积奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl9g6r/where_and_why_is_discounted_cumulative_reward_used/</link>
      <description><![CDATA[嗨，我是强化学习的新手，我现在正在学习一些基本术语。我遇到了“折扣累积奖励”这个术语，我理解即时奖励比未来奖励更有价值，但我不明白折扣累积奖励何时会用到。我在谷歌上搜索了它，但我找到的都是“折扣累积奖励”是什么，但没有具体的例子说明它可能在哪里使用。它是否仅用于估计累积奖励，其中后期奖励被折扣，因为它们不太可预测？有没有具体的真实例子说明它可能在哪里使用？    提交人    /u/AdBitter9336   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl9g6r/where_and_why_is_discounted_cumulative_reward_used/</guid>
      <pubDate>Fri, 20 Sep 2024 11:13:26 GMT</pubDate>
    </item>
    <item>
      <title>二维装箱问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl6bxp/2d_bin_packing_problem/</link>
      <description><![CDATA[嗨！我正在研究 2D BPP 问题，需要一些指导。有一个定义的托盘和 3 种定义的箱子。我们希望用箱子填满托盘，每次一个。每个箱子都有定义的到达概率  允许箱子旋转 我们希望最好填满托盘的周长 我们避免挤压箱子（在其他箱子之间），因为这个问题是机器人问题，并且存在不确定性 我们必须在箱子到达时放置它们，不能跳过它们。一旦没有空间，我们就会终止  我使用启发式方法解决了这个问题，比较剩余的空间并选择最佳放置坐标。我还对周长使用了不同的搜索：通过沿着托盘周长的较大边优先填充边缘。我不知道如何将其变成学习问题并接受建议！    提交人    /u/Sea-Hovercraft4777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl6bxp/2d_bin_packing_problem/</guid>
      <pubDate>Fri, 20 Sep 2024 07:23:21 GMT</pubDate>
    </item>
    <item>
      <title>推荐涵盖最新算法的调查/学习材料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</link>
      <description><![CDATA[您好，有人可以推荐一些涵盖较新算法/技术（td-mpc2、dreamerv3、diffusion policy）的调查/学习材料吗？其格式类似于 openai 的 spinningup/lilianweng 的博客，现在有点过时了？谢谢    提交人    /u/saintshing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</guid>
      <pubDate>Fri, 20 Sep 2024 07:14:21 GMT</pubDate>
    </item>
    <item>
      <title>帮助对 LLM 进行微调</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl4w1p/help_in_alignment_fine_tuning_llm/</link>
      <description><![CDATA[有人能帮帮我吗，我有一些用于生成 llama 3.1 的二进制反馈数据，是否有一种方法或任何其他算法可以用来使用二进制反馈数据对 llm 进行微调。 数据格式： 用户查询 - 文本 LLM 输出 - 文本标签 - 布尔值    提交人    /u/TuringComplete-Model   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl4w1p/help_in_alignment_fine_tuning_llm/</guid>
      <pubDate>Fri, 20 Sep 2024 05:37:00 GMT</pubDate>
    </item>
    <item>
      <title>LeanRL：一个简单的 PyTorch RL 库，用于快速（>5 倍）训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</link>
      <description><![CDATA[我们很高兴地宣布，我们已经开源了LeanRL，这是一个轻量级的 PyTorch 强化学习库，它使用 torch.compile 和 CUDA 图表提供快速 RL 训练的方法。 通过利用这些工具，与原始 CleanRL 实现相比，我们实现了显着的加速 - 速度提高了 6 倍！ RL 训练的问题 强化学习是出了名的 CPU 受限，因为小型 CPU 操作（例如从模块中检索参数或在 Python 和 C++ 之间转换）的频率很高。幸运的是，PyTorch 强大的编译器可以帮助缓解这些问题。但是，输入编译后的代码也有其自身的成本，例如检查保护以确定是否需要重新编译。对于 RL 中使用的小型网络，这种开销可能会抵消编译的好处。 进入 LeanRL LeanRL 通过提供简单的方法来加速您的训练循环并更好地利用您的 GPU，从而解决了这一挑战。受到 gpt-fast 和 sam-fast 等项目的启发，我们证明了 CUDA 图可以与 torch.compile 结合使用，以实现前所未有的性能提升。我们的结果表明：  使用 PPO（Atari）可提高 6.8 倍速度 使用 SAC 可提高 5.7 倍速度 使用 TD3 可提高 3.4 倍速度 使用 PPO（连续动作）可提高 2.7 倍速度  此外，LeanRL 可以更有效地利用 GPU，让您可以同时训练多个网络而不会牺牲性能。 主要特点  具有最少依赖性的 RL 算法的单文件实现 所有技巧都在 README 中进行了说明 从流行的 CleanRL 分叉   在 https://github.com/pytorch-labs/leanrl 上查看 LeanRL    由    /u/AdCool8270  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</guid>
      <pubDate>Fri, 20 Sep 2024 00:22:12 GMT</pubDate>
    </item>
    <item>
      <title>E[G_(t+1) | S_t=s] = V(S_(t+1)) 总是正确的吗？如何证明？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkuvsy/is_it_always_true_that_eg_t1_s_ts_vs_t1_how_to/</link>
      <description><![CDATA[编辑：在第二个成员中，我的意思是 E[V(S(t+1)) | S_t=s] 而不仅仅是 V(S(t+1)) 也许我淹没在一杯什么中，但是你如何证明这个等式成立？我的目标是证明 E[G_t|S_t=s] = E[R_(t+1) + gamma* V(S_(t+1)) | S_t=s ] 就像 sutton 和 barto 的等式 4.3 中一样，老实说，我对为什么会发生这种情况有一个直观的想法，但我正在寻找一种更正式的方式来展示这个属性    提交人    /u/samas69420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkuvsy/is_it_always_true_that_eg_t1_s_ts_vs_t1_how_to/</guid>
      <pubDate>Thu, 19 Sep 2024 20:55:18 GMT</pubDate>
    </item>
    <item>
      <title>对于这个特定于图形的任务，多智能体还是分层 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkmed4/multiagent_or_hierarchical_rl_for_this/</link>
      <description><![CDATA[我正在研究一个图形应用问题，其中一个RL代理必须将图形编码视为状态，并选择涉及一对节点和它们之间的一种操作的操作。 我正在考虑将此问题分解为具有多个代理的子任务，如下所示：  代理 1：接收图形编码并选择源节点。 代理 2：接收图形编码和所选源节点，然后选择目标节点。 代理 3：接收图形编码和所选源节点和目标节点，然后在它们之间选择一个操作。  我想到两个解决方案：  分层 RL：虽然任务看起来是分层的，但这可能并不完全合适。对于每个主要操作，必须执行所有三个代理（选项），并且需要按固定顺序执行。他们的行动应该是一步行动。我不确定分层 RL 是否最适合，因为问题没有明确的层次结构，而是顺序合作。 多智能体 RL：这可以构建为具有共同团队奖励的合作多智能体设置，其中执行顺序是固定的，每个智能体都会看到图形编码和先前智能体的操作（根据顺序）。  哪种方法——分层 RL 或多智能体 RL——更适合这个问题？是否有与此类问题相符的现有公式或框架？    提交人    /u/fterranova   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkmed4/multiagent_or_hierarchical_rl_for_this/</guid>
      <pubDate>Thu, 19 Sep 2024 14:36:09 GMT</pubDate>
    </item>
    <item>
      <title>聘请 RL 研究人员——构建下一代专家系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</link>
      <description><![CDATA[嗨！我们是 Atman Labs，一家位于伦敦的 AI 初创公司，在软件中模拟人类专家。我们认为，业界需要超越法学硕士 (LLM)，构建能够解决复杂、知识密集型任务的系统，这些任务需要多步推理。我们的研究使用强化学习来探索知识图谱，以形成针对目标的语义基础策略，并代表了一条模拟专家推理的新颖、可靠的途径。 如果您对 RL 充满热情，并希望构建和商业化下一代智能系统，那么您可能非常适合我们的创始团队。让我们聊聊吧 :) https://atmanlabs.ai/team/rl-founding-engineer    提交人    /u/Tricky_Amphibian_836   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</guid>
      <pubDate>Thu, 19 Sep 2024 12:04:36 GMT</pubDate>
    </item>
    <item>
      <title>CleanRL 现已为 PPO + Transformer-XL 提供基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkeqa9/cleanrl_has_now_a_baseline_for_ppo_transformerxl/</link>
      <description><![CDATA[之前，我们的 PPO-Transformer-XL 基线已发布至 Github。 此实现最终已完善为单文件实现以加入 CleanRL！它在 Memory Gym 的新颖无尽环境中重现了原始结果。 文档：https://docs.cleanrl.dev/rl-algorithms/ppo-trxl/ 论文：https://arxiv.org/abs/2309.17207 视频：https://marcometer.github.io/ 我们希望这将进一步改善有效使用变压器的方式并在基于内存的深度强化学习中高效运行。当然，接下来需要解决一些限制：  加快推理速度：与 GRU 和 LSTM 相比，数据采样成本高昂 节省 GPU 内存：缓存 TrXL 的隐藏状态以进行优化成本高昂     提交人    /u/LilHairdy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkeqa9/cleanrl_has_now_a_baseline_for_ppo_transformerxl/</guid>
      <pubDate>Thu, 19 Sep 2024 06:52:11 GMT</pubDate>
    </item>
    <item>
      <title>我可以将 DPO（直接偏好优化）应用于仅具有（y_win，y_loss）一侧的训练数据吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjwqvl/can_i_apply_dpo_direct_preference_optimization_to/</link>
      <description><![CDATA[我有一堆 (x_i, y_i, win_or_lose) 的标记数据。RLHF 论文的大部分内容都使用成对损失函数，这需要 (x_i, y_i_win) 和 (x_i, y_i_lose)，而我没有。我还能将 DPO 用于单侧训练数据吗？ 将缺失侧的隐式奖励值设置为 0，然后仍然应用反向传播，这样可以吗？    提交人    /u/PuzzleheadedBasis951   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjwqvl/can_i_apply_dpo_direct_preference_optimization_to/</guid>
      <pubDate>Wed, 18 Sep 2024 16:14:35 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>