<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 08 Apr 2024 00:59:33 GMT</lastBuildDate>
    <item>
      <title>蒙特卡罗树搜索不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1byifna/monte_carlo_tree_search_not_converging/</link>
      <description><![CDATA[我正在尝试实现 MCTS，但在测试中它无法收敛到 minmax，并且有一些我不明白的内容： &lt; p&gt;假设我的根节点有两个可能的动作，第一个分支分为 10 个其他动作，其中一个值为 0.6（获胜机会），其他 9 个值为 0，第二个分支只是一个值为 0.5 的单个动作 因此，在 minmax 中，第一个分支更好，因为在下一步中您可以采取 0.6 操作。但在 MCTS 中，第一个分支的值是好路径获胜的平均值，也是所有其他坏路径的平均值 0.6+0+0+0+0+0+0+0+0+0 / 10 父节点的值不应该是最佳子节点的值吗？ 但是在反向传播过程中，该值会被所有具有 0 的路径稀释 这就是我在模拟游戏中得到的结果。在此示例中，应该更多地播放 0.6 路径，以最终使低值路径变得无关紧要，但也许由于探索术语，不良路径永远不会减少到足以使 0.6 胜过 0.5 我错过了什么吗？ 谢谢。   由   提交 /u/SSCharles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1byifna/monte_carlo_tree_search_not_converging/</guid>
      <pubDate>Sun, 07 Apr 2024 23:11:52 GMT</pubDate>
    </item>
    <item>
      <title>训练 DQN 来解决玩具 MARL 问题有多困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1by4xtx/how_difficult_is_it_to_train_dqns_for_toy_marl/</link>
      <description><![CDATA[我一直在尝试训练井字棋 DQN，但到目前为止还无法让它们学习最佳策略。 我正在使用 pettingzoo env（因此没有图像或 CNN），并并行训练两个代理，彼此独立，这样每个代理都有自己的重播缓冲区，一个始终作为第一个播放，另一个作为第一个播放第二。 我尝试训练它们数十万步，并且通常会到达它们（似乎？）收敛到纳什均衡的点，游戏以平局结束。除了当我尝试让他们中的任何一个与随机对手对抗时，他们仍然会输掉大约 10% 的时间，这意味着他们还没有学会最佳策略。 我认为发生这种情况是因为他们没有&#39;我无法充分探索游戏空间，我不确定为什么情况并非如此。我使用 softmax 采样从高温开始并在训练过程中逐渐减少，所以他们肯定应该做一些探索。我已经尝试过学习率和网络架构，只做出了最小的改进。 我想我可以更深入地研究超参数优化并训练更长时间，但这对于这样一个简单的玩具问题来说听起来有点矫枉过正。如果我想训练他们玩一些更复杂的游戏，我是否需要更多的资源？或者，例如，选择 PPO 是否更明智？ 无论如何，咆哮已经足够了，我想问一下训练 DQN 是否真的那么困难马尔。如果您可以分享任何适用于 Tic Tac Toe 的一组超参数的实验，出于好奇，我们将非常欢迎您。   由   提交 /u/OperaRotas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1by4xtx/how_difficult_is_it_to_train_dqns_for_toy_marl/</guid>
      <pubDate>Sun, 07 Apr 2024 13:49:06 GMT</pubDate>
    </item>
    <item>
      <title>A3C 评论家估计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxxhvv/a3c_critic_estimate/</link>
      <description><![CDATA[      我正在使用异步优势演员评论家模型，该模型似乎运行得相当好，但我注意到，随着时间的推移，我的评论家“估计”变得越来越差。和“实际值”随着时间的推移会爆炸成负面的。本质上，我认为这是一个引导问题，其中批评者损失如下： loss = torch.nn.MSELoss()(reward + gamma * value_next , value) 环境非常嘈杂，但对负面奖励有偏见 - 所以我理论上正在发生的事情是批评家会定期猜测得太积极，学会变得更加消极，所发生的既是初始状态的价值预测，也是随着时间的推移，下一个状态（都来自批评者）往往会变得越来越消极。但由于随着时间的推移，两者都会以相同的速度变得更加负面，批评者本身的损失计算并不会变得非常扭曲。  然而，“真实的”是该指标的值实际上应该落在 [+5,-5] 之间，因此随着时间的推移，我担心这有可能开始向参与者发出奇怪的信号以进行其优势更新。  我在这里做错了什么教科书吗？您可以推荐一些好的修复方法吗？  ​ ​ （颜色代表不同的评论代理） ​ ​ ​ ​ ​ ​    由   提交 /u/Rhyno_Time   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxxhvv/a3c_critic_estimate/</guid>
      <pubDate>Sun, 07 Apr 2024 06:22:52 GMT</pubDate>
    </item>
    <item>
      <title>需要一些关于在医学成像重建背景下使用强化学习的想法的反馈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxw4ci/need_some_feedback_on_an_idea_for_using/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxw4ci/need_some_feedback_on_an_idea_for_using/</guid>
      <pubDate>Sun, 07 Apr 2024 05:00:24 GMT</pubDate>
    </item>
    <item>
      <title>深Q网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxtca2/deep_q_network/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxtca2/deep_q_network/</guid>
      <pubDate>Sun, 07 Apr 2024 02:31:43 GMT</pubDate>
    </item>
    <item>
      <title>谁首先通过将指示函数写成马尔可夫噪声来证明异步 Q 学习的收敛性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxokh3/who_first_proved_the_convergence_of_asynchronous/</link>
      <description><![CDATA[大家好！ 到目前为止，我们已经有了几个依赖随机逼近理论的异步 Q-learning 收敛证明并进行了更新：  $Q_k+1(s,a) &lt;- Q_k(s,a) + I[s_k=s, a_k=a] \alpha (Y_k)$ I我正在考虑证明，特别注意到 I[s_k=s, a_k=a] 是一个马尔可夫过程（因为过程 (s,a) 是），其过渡函数依赖于 $Q$ 的当前值，并得出收敛结果来自带有马尔可夫噪声的随机过程。 有谁知道谁第一个引入了这种方法？ 非常感谢！   由   提交/u/ttlizon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxokh3/who_first_proved_the_convergence_of_asynchronous/</guid>
      <pubDate>Sat, 06 Apr 2024 22:46:39 GMT</pubDate>
    </item>
    <item>
      <title>使用同一代理并行训练具有不同数据的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bxh59o/training_environments_with_different_data_in/</link>
      <description><![CDATA[我正在考虑训练几个自定义环境，但每个环境都有一批数据，大约是 2k 个 JSON 文件，每个数据代表一个剧集和环境从数据库获取这些文件。我的想法是使用包含在 SubprocVecEnv 中的 150 个环境实例来训练 SB3 DQN 模型，该模型似乎有效，但我面临两个问题：  速度训练似乎相当低，我无法弄清楚瓶颈是什么（在 4 个 Nvidia A100 GPU 上训练）我尝试增加环境数量，从而减少每个环境的文件数量，但这并没有&#39; t 改变速度  我不太确定单个代理是否会从所有 150 个实例的所有经验中学习，我想保证每个实例在训练完成之前完成所有文件  希望得到任何帮助或想法，谢谢！   由   提交 /u/CandidateIcy4911   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bxh59o/training_environments_with_different_data_in/</guid>
      <pubDate>Sat, 06 Apr 2024 17:31:03 GMT</pubDate>
    </item>
    <item>
      <title>最近关于人工智能和黑客的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bwv7ru/recent_papers_on_ai_and_hacking/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bwv7ru/recent_papers_on_ai_and_hacking/</guid>
      <pubDate>Fri, 05 Apr 2024 22:30:06 GMT</pubDate>
    </item>
    <item>
      <title>如何让我的 REINFORCE 代理学习“FrozenLake”环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bwmwt3/how_do_i_make_my_reinforce_agent_learn_the/</link>
      <description><![CDATA[我在之前的发布，同时实施强化。我从高星级 GitHub 存储库中获取了以下实现。该程序是为 CartPole-v1 环境编写的，并且运行良好。但是，它不适用于 FrozenLake 环境。有人可以告诉为什么会这样吗？ 我的代码可以在这里 &lt;找到p&gt;这是最近 100 集的平均奖励。我期望得到 1.0 的分数，因为神经网络只需将 16 个状态映射到最佳操作。 剧集数：99880，平均分数：0.1 剧集数：99900 ，平均得分：0.15 剧集数量：99920，平均得分：0.2 剧集数量：99940，平均得分：0.25 剧集数量：99960，平均得分：0.25 剧集数量：99980，平均得分：0.3 &lt; /pre&gt; ​   由   提交 /u/Academic-Rent7800    reddit.com/r/reinforcementlearning/comments/1bwmwt3/how_do_i_make_my_reinforce_agent_learn_the/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bwmwt3/how_do_i_make_my_reinforce_agent_learn_the/</guid>
      <pubDate>Fri, 05 Apr 2024 16:50:24 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的 SAC 实现无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bw9maj/sac_implementation_in_pytorch_failing_to_learn/</link>
      <description><![CDATA[我一直在尝试使用奖励缩放来实现基于神经网络的 SAC，如 论文。然而，正如标题所说，我的实现无法学习，因为在整个训练过程中所有网络的损失都在增长。对于修复我的代码的任何建议将不胜感激！   由   提交/u/Tight_Apple_678   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bw9maj/sac_implementation_in_pytorch_failing_to_learn/</guid>
      <pubDate>Fri, 05 Apr 2024 04:56:32 GMT</pubDate>
    </item>
    <item>
      <title>普通策略梯度在简单的网格世界任务上失败</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bw4gxj/vanilla_policy_gradient_failing_on_a_simple/</link>
      <description><![CDATA[      我编写了一个普通的策略梯度，并尝试在网格世界（冰冻湖）任务上对其进行训练。当代理正在学习时，它非常嘈杂，这令人惊讶，因为任务很微不足道。 这是我的代码&lt; /a&gt; 这是结果 - https://preview.redd.it/hf615ri06ksc1.png?width=599&amp;format=png&amp;auto=webp&amp;s=05c3d63454b5be6136c387ac6adcb9ce22eedcca 这是环境 - https://preview.redd.it/gljggve46ksc1.png?width=309&amp;format=png&amp;auto=webp&amp;s =7fef7793eb2fcc64d71853a93cbe98a066a8e2eb 有人可以帮我理解问题是什么吗？我想保持简单，如果可能的话不添加任何优势计算。   由   提交 /u/Academic-Rent7800    reddit.com/r/reinforcementlearning/comments/1bw4gxj/vanilla_policy_gradient_failing_on_a_simple/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bw4gxj/vanilla_policy_gradient_failing_on_a_simple/</guid>
      <pubDate>Fri, 05 Apr 2024 00:49:39 GMT</pubDate>
    </item>
    <item>
      <title>“使用前瞻树搜索的序列到序列神经网络系统”，Leblond 等人 2022 {DM}（美国专利申请#US20240104353A1）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bvxjxi/sequenceto_sequence_neural_network_systems_using/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bvxjxi/sequenceto_sequence_neural_network_systems_using/</guid>
      <pubDate>Thu, 04 Apr 2024 20:15:14 GMT</pubDate>
    </item>
    <item>
      <title>艾萨克健身房要求帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bvrl5d/isaac_gym_requirements_help/</link>
      <description><![CDATA[嗨，我是一名大学生，我是这个世界的新手。我被选中参加一个需要 isaac 健身房的项目。由于我的电脑有点旧，我想知道你们是否可以帮助我满足我需要的系统要求（以便为此目的组装最好的电脑）。  现在我知道我可以在互联网上搜索它，但我只找到了 isaacSim 的要求，我真的不知道它们是否与 isaacgym 完全相同。 请原谅我的无知，但我对这一切都很陌生。谢谢！   由   提交/u/fartinsim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bvrl5d/isaac_gym_requirements_help/</guid>
      <pubDate>Thu, 04 Apr 2024 16:27:49 GMT</pubDate>
    </item>
    <item>
      <title>斯坦福 CS 25 变形金刚课程（向所有人开放 | 明天开始）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bve6ua/stanford_cs_25_transformers_course_open_to/</link>
      <description><![CDATA[Tl;dr：斯坦福大学最热门的研讨会课程之一。我们通过 Zoom 向公众开放课程。讲座将于明天（星期四）下午 4:30-5:50（太平洋夏令时间）开始，地点为 Zoom 链接。课程网站： https://web.stanford.edu/class/cs25/&lt; /a&gt; 对风靡全球的深度学习模型《变形金刚》感兴趣吗？想与研究人员进行亲密讨论吗？如果是这样，那么本课程适合您！您并不是每天都能亲自听到您所读论文的作者的来信并与他们聊天！ 每周，我们都会邀请处于 Transformers 研究前沿的人们讨论法学硕士架构的最新突破从 GPT 和 Gemini 到生成艺术（例如 DALL-E 和 Sora）、生物学和神经科学应用、机器人技术等方面的创意用例！ CS25 已成为斯坦福大学最热门、最令人兴奋的研讨会课程之一。我们邀请了最酷的演讲者，如 Andrej Karpathy、Geoffrey Hinton、Jim Fan、Ashish Vaswani 以及来自 OpenAI、Google、NVIDIA 等的人员。我们的课程在斯坦福大学内外广受欢迎，&lt;&lt;的总观看次数约为 100 万次。 a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&quot;&gt;YouTube。我们的 Andrej Karpathy 课程是 2023 年斯坦福大学上传的第二受欢迎的 YouTube 视频，观看次数超过 50 万意见！ 我们在 2024 年春季进行了重大改进，包括大型演讲厅、专业录音和直播（向公众）、社交活动和潜在的一对一网络！学生唯一的作业是每周参加讲座/讲座。此外，所有人都可以进行直播和审核。欢迎亲自审核或加入 Zoom 直播。 我们还有一个用于 Transformers 讨论的 Discord 服务器（超过 1500 名成员）。我们将其作为一个“变形金刚社区”向公众开放。欢迎加入并与数百名其他人讨论变形金刚！ P.S.是的，谈话将被记录！它们可能会在 YouTube 上上传并提供。每次讲座后 2 周。   由   提交/u/MLPhDStudent  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bve6ua/stanford_cs_25_transformers_course_open_to/</guid>
      <pubDate>Thu, 04 Apr 2024 04:23:13 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>