<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 28 Nov 2024 03:32:33 GMT</lastBuildDate>
    <item>
      <title>测试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1l2cq/test/</link>
      <description><![CDATA[        提交人    /u/AiLearnerXyf1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1l2cq/test/</guid>
      <pubDate>Thu, 28 Nov 2024 01:55:49 GMT</pubDate>
    </item>
    <item>
      <title>测试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1l093/test/</link>
      <description><![CDATA[        提交人    /u/AiLearnerXyf1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1l093/test/</guid>
      <pubDate>Thu, 28 Nov 2024 01:52:37 GMT</pubDate>
    </item>
    <item>
      <title>政策绩效是否受情境赌博的约束？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1izwx/policy_performance_bound_for_contextual_bandits/</link>
      <description><![CDATA[嗨，我正在研究上下文老虎机，以在线方式更新策略，类似于 PPO。本质上，我在缓冲区中收集一批样本，并使用 PPO 损失进行更新，不同之处在于我为所选操作获得单一奖励，即老虎机反馈。鉴于这种设置，RL 文献中的策略改进界限（参考：约束策略优化，推论 1，等式 5）是否适用于上下文老虎机？    提交人    /u/noob_simp_phd   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1izwx/policy_performance_bound_for_contextual_bandits/</guid>
      <pubDate>Thu, 28 Nov 2024 00:09:45 GMT</pubDate>
    </item>
    <item>
      <title>多智能体 RL 的近端策略优化 (PettingZoo)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1cdfo/proximal_policy_optimization_for_multiagent_rl/</link>
      <description><![CDATA[嗨！我是初学者。 据我所知，从多智能体的角度来看，标准 PPO（不是 MAPPO）是一种去中心化技术（智能体假设其他智能体的策略随时间变化是环境本身的一部分）。如果遵循这种逻辑，那么可以安全地假设，当在环境（合作设置）中使用多个 PPO 智能体时，收敛的概率可能不高，或者至少肯定不能保证。 在基于多智能体的 RL 框架中，特别是 PettingZoo，存在标准 PPO 的现有实现示例。 PettingZoo 环境中的 PPO 实现。 我的问题是，为什么在这样的框架中使用 PPO，但文档中没有提到非平稳性？多个 PPO 代理等方法是否有可能收敛？我主要只是想知道如果代理都使用 PPO，它们合作的现实性如何。另外，我没有考虑其他处理非平稳性的技术，如 IPPO，只是想了解 PPO。 我不知道我是否以足够精确的方式表达了我所想的内容，使其有意义 + 英语不是我的母语，所以我希望它被理解。 提前谢谢！:)    提交人    /u/Classic_Mongoose3182   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1cdfo/proximal_policy_optimization_for_multiagent_rl/</guid>
      <pubDate>Wed, 27 Nov 2024 19:16:44 GMT</pubDate>
    </item>
    <item>
      <title>需要有关迷宫求解强化学习任务的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h15gq5/need_help_regarding_maze_solving_reinforcement/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h15gq5/need_help_regarding_maze_solving_reinforcement/</guid>
      <pubDate>Wed, 27 Nov 2024 14:23:59 GMT</pubDate>
    </item>
    <item>
      <title>安全离线 MARL 数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h14a11/safe_offline_marl_datasets/</link>
      <description><![CDATA[大家好， 有没有带安全约束的 MARL 环境的开源数据集？    提交人    /u/ImaginaryEducation55   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h14a11/safe_offline_marl_datasets/</guid>
      <pubDate>Wed, 27 Nov 2024 13:27:33 GMT</pubDate>
    </item>
    <item>
      <title>如何学习 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0xs0i/how_to_learn_rl/</link>
      <description><![CDATA[我是新手，请给我一些建议    提交人    /u/Clean_Tip3272   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0xs0i/how_to_learn_rl/</guid>
      <pubDate>Wed, 27 Nov 2024 06:12:01 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 旋转教程中的链接缺失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0uocf/the_link_in_this_openai_spinning_up_tutorial_is/</link>
      <description><![CDATA[你好， 我正在阅读 OpenAI 旋转教程策略梯度简介 此声明的（可选）证明可以在 `此处`_找到，它最终取决于 EGLP 引理。 在上面的文字中，“此处”链接根本不起作用（它只是将我引导到同一个网页）。您知道这个证明的链接吗？ 非常感谢！    提交人    /u/Slight_Shift7974   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0uocf/the_link_in_this_openai_spinning_up_tutorial_is/</guid>
      <pubDate>Wed, 27 Nov 2024 03:14:33 GMT</pubDate>
    </item>
    <item>
      <title>RL 准备好取代传统 AI 了吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0g14b/is_rl_ready_to_replace_traditional_ai/</link>
      <description><![CDATA[大家好，我是一名大学生，目前正在研究游戏开发中的强化学习。我目前有一个想法。首先，强化学习代理是否准备好取代传统的人工智能代理？在进行研究后，我个人并不这么认为。例如，我读到代理会找到最有价值的情况，而不是最优解决方案或游戏开发者想要的解决方案。我在一项研究中确实读到，语义分段的帧可以用作代理的输入，它可以在比没有这些帧作为输入的代理更短的训练时间内击败超级马里奥兄弟关卡。你怎么看？强化学习是否准备好取代传统人工智能？ 查看投票    提交人    /u/linkpeahen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0g14b/is_rl_ready_to_replace_traditional_ai/</guid>
      <pubDate>Tue, 26 Nov 2024 16:32:43 GMT</pubDate>
    </item>
    <item>
      <title>有人能够使用支持 GPU 的 PyTorch 和 Carla 模拟器训练 RL 模型吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0aldv/has_someone_been_able_to_use_gpuenabled_pytorch/</link>
      <description><![CDATA[Carla 支持的最新 Python 版本是 3.8.0，对于 PyTorch GPU 加速来说太旧了。我曾尝试将我的 Carla 代码打包到服务器中，但速度太慢了。有什么建议吗？    提交人    /u/AdhesivenessSmall333   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0aldv/has_someone_been_able_to_use_gpuenabled_pytorch/</guid>
      <pubDate>Tue, 26 Nov 2024 12:22:12 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 演员在评估期间总是采取相同的动作。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h07zyb/ddpg_actor_always_taking_same_action_during/</link>
      <description><![CDATA[我正在使用自定义环境。其中状态表示为 (x1, x2)，动作为 (delta_x1, delta_x2)，下一个状态为 (x1+delta_x1, x2+ delta_x2)。有奖励。在训练期间，参与者也会多次到达状态空间的边界。我知道很多人都遇到过同样的问题，比如在 DDPG 中，参与者总是采取相同的动作。您的实现中存在什么问题，您是如何解决的？此外，任何其他帮助也非常感谢。提前致谢。     提交人    /u/Adventurous_Fly_5564   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h07zyb/ddpg_actor_always_taking_same_action_during/</guid>
      <pubDate>Tue, 26 Nov 2024 09:30:42 GMT</pubDate>
    </item>
    <item>
      <title>星际争霸母巢之战</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzvetr/starcraft_broodwar/</link>
      <description><![CDATA[Hello RL World! 我是星际争霸 Broodwar（来自韩国）的超级粉丝，自从它在 90 年代末首次推出以来，当时我还是个孩子。快进 24 年，在获得计算机科学学士学位后，我在不同的公司从事后端领域的工作，主要从事分布式系统/数据库工作 10 年。而现在，我仍然在观看 Broodwar 职业联赛。 9 年前我在韩国偶然发现了 AlphaGo（时间过得真快），当时我对人工智能产生了兴趣，但围棋不是我感兴趣的东西，所以兴趣逐渐消退，直到 AlphaStar 出现征服了星际争霸 II。然而现在我发现，在 Broodwar 中，我并没有看到太多在 APM 方面与人类相似的人工智能系统，这些人工智能系统经过训练可以挑战 Broodwar 传奇人物（比如 Flash、Bisu、Stork 等），所以我至少想了解一下为什么它还没有浮出水面挑战这些传奇人物。是训练模型的成本吗？还是 Broodwar API 上的挑战？ 我做后端工程师已经 10 年了，但我目前对 RL 还是新手，所以我刚从亚马逊买了本书《Grokking the Deep Reinforcement Learning (Morales)》并开始阅读（这是一个好的开始吗）？    提交人    /u/fsw0422   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzvetr/starcraft_broodwar/</guid>
      <pubDate>Mon, 25 Nov 2024 22:06:41 GMT</pubDate>
    </item>
    <item>
      <title>单步演员评论算法（RL 书）在 Cartpole 环境中无法按预期运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzmrry/onestep_actorcritic_algorithm_rl_book_not_working/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzmrry/onestep_actorcritic_algorithm_rl_book_not_working/</guid>
      <pubDate>Mon, 25 Nov 2024 16:24:25 GMT</pubDate>
    </item>
    <item>
      <title>Unity MLAgents 在一个简单的益智游戏上艰难地进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzffaj/unity_mlagents_struggle_to_train_on_a_simple/</link>
      <description><![CDATA[      https://preview.redd.it/uva3kh8zp03e1.png?width=677&amp;format=png&amp;auto=webp&amp;s=9f838885a8d433c50e324c68c681a006855448ad 我正在尝试在我的 Unity 益智游戏项目上训练一个代理，游戏的工作原理如下； 您需要发送与当前总线匹配的颜色。您只能扮演路径未被阻挡的角色。您有 5 个位置可以为后面的角色或错误的玩法腾出空间。 到目前为止我尝试过的； 我已经研究了大约一个月，但到目前为止没有成功。 我从矢量观察开始，并输入了瓷砖颜色、状态、当前总线颜色等。但是没有用。这太复杂了。每次失败时，我都会简化观察状态和设置。有一次，我只给了代理 1 和 0，这是它应该学会玩的部分，只有 1 值可以玩，因为我正在检查可玩状态以及颜色是否匹配。我也使用动作蒙版。我无法在这种简单的设置上训练它，这是一种战斗和挫败感。我甚至简化到了当它犯错时结束剧集的地步，我会给它负面奖励并结束剧集。我希望它选择正确的棋子，而不关心玩关卡和制定策略。但它在训练有素的关卡上玩得很好，但它过度拟合，记住了它们。在测试级别，即使是简单的也无法正确执行。  我已经开始深入研究应该如何处理它，并查看 Unity MLAgents 示例中的 match-3 示例。我了解到对于类似网格的结构，我需要使用 CNN，并且我已经创建了自定义传感器，现在进行视觉观察，例如在 20x20 的网格上放置 40 层信息。11 个颜色层 + 11 个总线颜色层 + 可以移动层 + 不能移动层等。我尝试了简单的视觉编码和 match3，但仍然无法对其进行一些训练。  我的问题是；在 RL 上训练这种益智游戏难吗？因为在 Unity 示例中有很多复杂的游戏玩法，即使给代理的帮助较少，它也能快速学习。还是我在核心方法中做错了什么？ 这是我目前使用的配置，但我已经尝试了很多东西，我已经改变并尝试了几乎每一种方法； ``` behaviors：AIAgentBehavior：trainer_type：ppo hyperparameters：batch_size：256 buffer_size：2560 #buffer_size = batch_size * 8 learning_rate：0.0003 beta：0.005 epsilon：0.2 lambd：0.95 num_epoch：3 shared_critic：False learning_rate_schedule：linear beta_schedule：linear epsilon_schedule：linear network_settings：normalize：True hidden_​​units：256 num_layers：3 vis_encode_type：match3 #conv_layers：#-filters： 32 # kernel_size: 3 # 步幅：1 # - 过滤器：64 # kernel_size: 3 # 步幅：1 # - 过滤器：128 # kernel_size: 3 # 步幅：1 确定性：False reward_signals： 外部： gamma：0.99 强度：1.0 # network_settings： # normalize：True # hidden_​​units：256 # num_layers：3 # # memory：None # deterministic：False # init_path：None keep_checkpoints：5 checkpoint_interval：50000 max_steps：200000 time_horizo​​n：32 summary_freq：1000 threaded：False  ```    提交人    /u/menelaus35   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzffaj/unity_mlagents_struggle_to_train_on_a_simple/</guid>
      <pubDate>Mon, 25 Nov 2024 09:57:37 GMT</pubDate>
    </item>
    <item>
      <title>“无需经验回放、目标网络或批量更新的深度强化学习”，Elsayed 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gz67aa/deep_reinforcement_learning_without_experience/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gz67aa/deep_reinforcement_learning_without_experience/</guid>
      <pubDate>Mon, 25 Nov 2024 00:36:29 GMT</pubDate>
    </item>
    </channel>
</rss>