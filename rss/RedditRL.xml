<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 15 Jan 2024 01:03:45 GMT</lastBuildDate>
    <item>
      <title>功能太多</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196v0cm/too_many_features/</link>
      <description><![CDATA[      &amp;# 32；由   提交/u/Throwawaybutlove  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196v0cm/too_many_features/</guid>
      <pubDate>Mon, 15 Jan 2024 00:27:17 GMT</pubDate>
    </item>
    <item>
      <title>累积奖励曲线平滑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196svan/cumulative_reward_curve_smooth/</link>
      <description><![CDATA[嗨， 我正在多维离散动作和观察空间上运行 A2C。在训练期间，我正在计算运行均值和方差以标准化我的奖励。我的奖励计算是随机的，因为有需求被实现。在评估我学到的政策时，我发现无论给出什么观察，都是相同的行动。我绘制了累积奖励，它看起来非常平滑的线性。我想知道这是否是预期的行为？我打印出了逐步奖励，它确实并不总是 1，在 [-1,1] 之间波动（大部分）。谢谢！   由   提交 /u/polymerase2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196svan/cumulative_reward_curve_smooth/</guid>
      <pubDate>Sun, 14 Jan 2024 22:54:41 GMT</pubDate>
    </item>
    <item>
      <title>定制您的内容 在几秒钟内实现您的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196ryk9/customize_your_content_materialize_your_idea_in/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交 /u/Agreeable-Feefda   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196ryk9/customize_your_content_materialize_your_idea_in/</guid>
      <pubDate>Sun, 14 Jan 2024 22:16:40 GMT</pubDate>
    </item>
    <item>
      <title>奖励稀疏，剧集长度长。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196rt1n/sparse_reward_with_long_episode_length/</link>
      <description><![CDATA[嗨！我正在尝试使用 PPO 算法找到一个好的策略来优化本地搜索启发式中的参数。挑战在于我只能在每集结束时评估策略的性能，其中提供 [0,1] 范围内的稀疏奖励。剧集长度固定为 1000 步。在这种情况下是否有机会学习成功的政策？到目前为止，即使采用非常简单的观察结构，我也没有取得任何积极的成果。也许我可以尝试一些技巧。预先感谢您的帮助！   由   提交 /u/OpportunityHot7289   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196rt1n/sparse_reward_with_long_episode_length/</guid>
      <pubDate>Sun, 14 Jan 2024 22:10:28 GMT</pubDate>
    </item>
    <item>
      <title>强化学习优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196idl8/reinforcement_learning_for_optimization/</link>
      <description><![CDATA[有没有人尝试使用 RL 来解决优化问题，例如旅行商问题或类似问题，我检查了几篇他们使用 DQN 的论文，但在实际实现后我还没有即使对于简单的问题，例如将盒子从迷宫的一端移到另一端，也没有得到任何实际的结果。我还担心基于 DQN 的解决方案能否在未见过的数据上表现良好。欢迎提出任何建议。   由   提交 /u/HSaurabh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196idl8/reinforcement_learning_for_optimization/</guid>
      <pubDate>Sun, 14 Jan 2024 15:29:51 GMT</pubDate>
    </item>
    <item>
      <title>[需要建议/反馈] 训练时 DQN 波动很大。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196eref/need_advicefeedback_dqn_strongly_fluctuates_when/</link>
      <description><![CDATA[      大家好，我是 RL 新手，想用 DDQN 做点什么。我从这个 PyTorch 网站 找到了这篇关于如何使用 DQN 玩 CartPole 的文章。我尝试采用此代码，但将游戏更改为 BreakOut（更具体地说，BreakOutv5 {frame_skip = 4，repeat_action = 0.25））。  我对原始代码所做的更改是通过 GreyScale、Cropping、Resize 和 FrameStack 对环境进行预处理。 def Observation_preproc(frame):cropped_frame = frame[ 35:195, 7:153]/255 returncropped_frame STACK_NUM= 4 RESIZE_HEIGHT= 84 RESIZE_WIDTH= 84 # 制作游戏环境 env=gym.make(&quot;ALE/Breakout-v5&quot;, render_mode=&#39;rgb_array&#39;) env=gym. wrappers.GrayScaleObservation（env）env=gym.wrappers.TransformObservation（env，observation_preproc）env=gym.wrappers.ResizeObservation（env，（RESIZE_HEIGHT，RESIZE_WIDTH））env=gym.wrappers.FrameStack（env，STACK_NUM）  我还用卷积层重新构建了 DDQN 模型 class DQN(nn.Module): def __init__(self, n_stack, n_actions): super(DQN, self).__init__() self.conv_layer1= nn.Sequential( nn.Conv2d(in_channels= n_stack, out_channels= 32, kernel_size= 8, stride= 4), nn.ReLU(inplace= True) ) self.conv_layer2= nn.顺序( nn.Conv2d(in_channels= 32, out_channels= 64, kernel_size= 4, stride= 2), nn.ReLU(inplace= True) ) self.conv_layer3= nn.Sequential( nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= 3, stride= 1), nn.ReLU(inplace= True) ) self.relu= nn.ReLU(inplace= True) self.flatten= nn.Flatten() self.layer1= nn.Linear(7 *7*64, 512) self.layer2= nn.Linear(512, n_actions) defforward(self, x): x= self.conv_layer1(x) x= self.conv_layer2(x) x= self.conv_layer3(x ) x= self.flatten(x) x= self.relu(self.layer1(x)) x= self.layer2(x)  我计划训练它一百万集但在大约3000集时，我注意到：  训练集奖励和持续时间图。 学习过程面临一些强烈的波动。 ​ 我的问题是：这种现象叫什么？你认为我怎样才能阻止它？我知道 DQN 已经过时了，但是我正在一步步学习（另外，在这个 DeepMind论文，他们的DQN得分比我高:)))) 如果你想阅读我的整个笔记本，这里有直接的Github 链接，请看一下:)  谢谢大家 &lt; /div&gt;  由   提交/u/Q_H_Chu  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196eref/need_advicefeedback_dqn_strongly_fluctuates_when/</guid>
      <pubDate>Sun, 14 Jan 2024 12:24:28 GMT</pubDate>
    </item>
    <item>
      <title>减少迭代次数或其他方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196dxse/reduce_number_of_iterations_or_other_methods/</link>
      <description><![CDATA[嘿， ​ 我目前正在写我的硕士论文，涉及强化学习在代码生成中的应用。我的重点是一种新开发的领域特定语言 (DSL)，它的可用示例有限，因为还没有用这种语言编写的功能程序的广泛数据库。 我的目标是训练能够在这个新的 DSL 中编写代码的模型。对于环境，我有能力执行代码以确定它是否产生预期的输出。目前，我的方法包括随机选择 1 到 200 个操作来验证每次迭代中生成的代码是否正确。然而，事实证明这种方法非常耗时。 您能否建议我一种减少迭代次数的方法？任何见解或建议将不胜感激。 ​ 谢谢！   由   提交/u/mim549276  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196dxse/reduce_number_of_iterations_or_other_methods/</guid>
      <pubDate>Sun, 14 Jan 2024 11:32:34 GMT</pubDate>
    </item>
    <item>
      <title>奇怪的行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196boz1/strange_behaviour/</link>
      <description><![CDATA[我正在使用 Q 学习开发剪刀石头布代理 - https://github.com/revyu/RPS .在玩mrugesh时没有任何问题，它的胜率相当稳定，但在kris上它玩得很糟糕。它可以玩 {&#39;player&#39;: 400, &#39;opponent&#39;: 201, &#39;tie&#39;: 399}, winrate=0.400000 或 {&#39;player&#39;: 0, &#39;opponent&#39;: 1000, &#39;tie&#39;: 0}, winrate=0.000000 ，没有中间结果。我对机器学习和强化学习还很陌生，无法理解发生了什么。最让我惊讶的不是算法表现不佳，而是它的结果正好位于彼此相距很远的两个点上。   由   提交 /u/revyakin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196boz1/strange_behaviour/</guid>
      <pubDate>Sun, 14 Jan 2024 08:59:46 GMT</pubDate>
    </item>
    <item>
      <title>“潜伏特工：通过安全培训持续存在的训练欺骗性法学硕士”，Hubinger 等人 2024 {Anthropic}（RLHF 和对抗性训练未能消除法学硕士中的后门）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195x2tw/sleeper_agents_training_deceptive_llms_that/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195x2tw/sleeper_agents_training_deceptive_llms_that/</guid>
      <pubDate>Sat, 13 Jan 2024 20:17:21 GMT</pubDate>
    </item>
    <item>
      <title>“语言模型可以解决计算机任务”，Kim 等人 2023（MiniWoB++ 的内心独白）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195v9nt/language_models_can_solve_computer_tasks_kim_et/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195v9nt/language_models_can_solve_computer_tasks_kim_et/</guid>
      <pubDate>Sat, 13 Jan 2024 19:00:04 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自学</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195l81f/reinforcement_learning_self_taught/</link>
      <description><![CDATA[大家好， 我想进入强化学习，但不知道从哪里开始，因此我想问一下如果有人对从哪里开始有任何建议，并且可能有一些资源来这样做。我是一名 STEM 专业的学生，​​拥有 Python 经验，想开始深入研究强化学习，因为它看起来非常有趣且具有挑战性。我很想听听您是如何学习的，以及关于我如何学习的任何建议。 提前致谢   由   提交 /u/Simozzzo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195l81f/reinforcement_learning_self_taught/</guid>
      <pubDate>Sat, 13 Jan 2024 10:35:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 学习梯度下降步长</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</link>
      <description><![CDATA[       问题陈述： 我一直在研究一个使用强化学习加速梯度下降收敛的项目。我想学习一种策略，可以将梯度下降的当前状态映射到最佳动作，即本例中的步长。提醒一下：梯度下降迭代由 x_k+1 = x_k - gamma*grad(f) 给出，其中 gamma 为步长。目前，我只考虑 f(x) = x&#39;Qx 形式的凸二次函数。我想在函数分布上训练策略，以便在预测时它可以泛化到该分布中的所有函数，以及在训练期间未见过的函数。通过在每次迭代中预测最佳步长的策略，目标是梯度下降在该分布内的所有函数的迭代次数较少的情况下收敛。 ​ 当前方法： 目前，我一直在使用无模型的强化学习算法，如 Soft Actor-Critic (SAC) 和 Twin Delayed Deep Definitive Policy Gradient (TD3) 来训练策略，但我发现即使对于某个特定函数过度拟合的简单情况，所需的内存和计算量也非常高。此外，当您过度拟合（对同一函数进行训练和评估）时，您会期望奖励收敛到某个值。如图所示，奖励确实增加了，但在某些时候代理完全忘记了它所学到的东西。我使用稀疏奖励：每次迭代中收敛时为 0，未收敛时为 -1。也许最好有一个奖励，说明每次迭代中残差（=梯度范数）的减少，这样代理不仅会在回合结束时接收信息。对于状态，我尝试了不同的方法，但仅包含当前梯度似乎或多或少有效。我使用的算法是SAC，它似乎比TD3更快。演员和评论家均由神经网络参数化，每个神经网络有 3 个隐藏层和 128 个节点。我使用了 Stable-Baselines 3 的实现。 ​ 我的问题： - 是无模型的RL 解决这个问题的正确方法是什么？它的计算成本非常高。是否有更好的方法，例如基于模型的强化学习或某种策略搜索？ - 在图中，为什么奖励突然减少？它与重放缓冲区的大小有关系吗？目前我可以分配 120Gb 的内存，这已经是相当多了。 - RL 理论通常基于马尔可夫过程。因此它假设马尔可夫性质，即当前状态完全独立于先前的状态。但是，最好添加一些有关先前梯度的信息以增加动量（例如 Nesterov 加速）。在这个框架中这可能吗？ ​ https://preview.redd.it/lnn1k9s333cc1.jpg?width=937&amp;format=pjpg&amp;auto=webp&amp;s=4254e662c840e4b4ca719b1f 70a488041376fad2   由   提交 /u/Lennitar   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</guid>
      <pubDate>Fri, 12 Jan 2024 22:15:47 GMT</pubDate>
    </item>
    <item>
      <title>解释方差增加，但随后稳定在低值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1956qfb/explained_variance_increases_but_then_stabilizes/</link>
      <description><![CDATA[      您好， 我正在使用 Stable Baselines 3 在我的自定义环境中训练 A2C 模型。我的自定义环境是离散的、多维的 (4 * 21)，用于操作和观察。我一直在尝试调整超参数，但似乎对于所有超参数集，都存在一个常见问题，即解释的方差首先增加，但随后保持在 &lt;&lt; 30%。 此外，当我评估我的策略时，似乎 model.predict(obs) 结果（动作预测）始终是单个动作，不依赖于观察。这是因为低解释方差表明仅使用“平均动作”并没有更好吗？谢谢！ ​ https://preview.redd.it/sr4twq7ux2cc1.png?width=1191&amp;format=png&amp;auto=webp&amp;s=0a3653b2c228fdb97306d8f8093ec5ec0926f3 b1 https://preview.redd.it/vux1wq7ux2cc1.png?width=1178&amp;格式=png&amp;auto=webp&amp;s=5da34c134fde44f7340fb30e9e678e86fe74e5bf   由   提交 /u/polymerase2   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1956qfb/explained_variance_increases_but_then_stabilizes/</guid>
      <pubDate>Fri, 12 Jan 2024 21:45:11 GMT</pubDate>
    </item>
    <item>
      <title>如何裁剪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</link>
      <description><![CDATA[我熟悉 RL 一个月了，想在一些 Atari 游戏上练习我的知识。我阅读了 DeepMind 的文章，了解他们如何使用 DQN 执行 Atari 任务 (https://arxiv.org/abs/1312.5602)。他们在论文中表示： “直接处理原始 Atari 帧（这些帧是具有 128 个调色板的 210 × 160 像素图像）的计算要求可能很高，因此我们应用了一个基本的预处理步骤，旨在减少输入维度。对原始帧进行预处理，首先将其 RGB 表示转换为灰度并将其下采样为 110×84 图像。最终的输入表示是通过裁剪图像的 84 × 84 区域来获得的，该区域大致捕获了游戏区域。” 这对我来说似乎很合理，但我想知道他们是如何以编程方式做到这一点的？我阅读了gymnasium（不是gym）文档（https://gymnasium.farama.org/api/wrappers/）但是，尽管他们有 FrameStack 和 GreyScale 的包装器，但下采样和裁剪包装器似乎不可用。 有人知道如何做到这一点吗？  非常感谢你们。   由   提交/u/Q_H_Chu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</guid>
      <pubDate>Fri, 12 Jan 2024 13:07:01 GMT</pubDate>
    </item>
    <item>
      <title>太空战争 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</link>
      <description><![CDATA[   /u/_Linux_AI_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</guid>
      <pubDate>Fri, 12 Jan 2024 03:50:56 GMT</pubDate>
    </item>
    </channel>
</rss>