<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 15 Feb 2025 12:30:14 GMT</lastBuildDate>
    <item>
      <title>“重新评估不完美信息游戏的策略梯度方法”，Rudolph等。 2025年（PPO竞争不完美的INFO游戏的定制算法）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzsqe/reevaluating_policy_gradient_methods_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  摘要：“在过去的十年中，受到对抗性不完美信息游戏中幼稚的自我扮演深度强化学习（DRL）的假定失败的动机，研究人员已经根据虚拟游戏（FP），双重甲骨文（DO）和反事实遗憾最小化（CFR）开发了许多DRL算法。鉴于磁性镜下降算法的最新结果，我们假设PPO（例如PPO）具有更简单的通用策略梯度方法具有竞争力或优于这些FP，DO和基于CFR的DRL方法。为了促进该假设的解决，我们针对四个大型游戏实施并发布了第一个可访问的确切可剥削性计算。使用这些游戏，我们对不完美的信息游戏进行了DRL算法的有史以来最大的可利用性比较。超过5600次培训运行，FP，DO和基于CFR的方法无法超越通用策略梯度方法。”   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mothmatic     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzsqe/reevaluating_policy_gradient_methods_for/</guid>
      <pubDate>Sat, 15 Feb 2025 11:53:25 GMT</pubDate>
    </item>
    <item>
      <title>UnrealMlagents 1.0.0：开源深钢筋学习框架！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzl5v/unrealmlagents_100_opensource_deep_reinforcement/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/cybereng     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzl5v/unrealmlagents_100_opensource_deep_reinforcement/</guid>
      <pubDate>Sat, 15 Feb 2025 11:39:00 GMT</pubDate>
    </item>
    <item>
      <title>多目标PPO指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzdjn/guidance_on_multiobjective_ppo/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试在动态环境中实现用于PPO（作为新手）的多目标算法（作为新手）。这里有两个主要奖励指标，我可以根据环境的当前状态成功计算出来：1）预期碰撞时间和2）当前速度和所需速度之间的差异（朝着目标方向的速度朝着目标的方向汽车的最大速度）。大多数研究论文的线性函数作为奖励功能，在该功能中，系数是手工调整的。到目前为止，我已经理解的是（遇到困难和混乱）是，我们不会立即标明奖励，而是我们计算每个奖励目标的政策，然后最终将其汇总。无论出于何种原因，我都找不到特定于多目标PPO的研究论文。你有建议吗？您甚至认为这是正确的方法吗？感谢您的时间（请帮助我，我迷路了）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/med-grade-8440     [link]  ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzdjn/guidance_on_multiobjective_ppo/</guid>
      <pubDate>Sat, 15 Feb 2025 11:23:20 GMT</pubDate>
    </item>
    <item>
      <title>关于我的HRL接下来我应该尝试什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipyfqv/suggestion_on_what_should_i_try_next_for_my_hrl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图通过使用称为rware的预先固定程序在网格世界中实现仓库任务分配。我正在HRL（继承人的增强学习）中使用封建网络。如果将架子带入世界上的目标障碍，则奖励Rware的提供仅为+1。奖励稀疏还是可以拥有这样的奖励系统？我只是有一个代理。我不能让代理商去做。屁股hrl很好。我该怎么做才能实现学习？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/decter_prune_9756       [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipyfqv/suggestion_on_what_should_i_try_next_for_my_hrl/</guid>
      <pubDate>Sat, 15 Feb 2025 10:13:33 GMT</pubDate>
    </item>
    <item>
      <title>[r]在增强学习方面的标签经验，以进行有效检索。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipxgd1/r_labelling_experiences_in_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好 r/reaceforceleconlearning ， p&gt; 我正在研究加强学习问题，因为我是一名创业者，所以我没有时间写论文，所以我想我应该在这里分享。 因此，我们目前在经验重播中使用随机样本。为1K样品提供缓冲区，然后获取随机物品。有人在“好奇的重播”上撰写了一篇论文，这使该模型为重播分配了“好奇心得分”，并更频繁地获取它们。和使用世界模型的火车，这实际上是SOTA可以重播的SOTA，但是我认为我们可以更深入地进行。 好奇心重播是不错的，但是这样考虑：当您（代理人）越过时，街，您重播了关于越过街道的回忆。人类不考虑烹饪或机器学习，当他们过马路时，我们会想到越过街道，，因为这很危险。 所以我们如何标记体验有类似VAE的编码器结构，它可以为缓冲区中的项目分配“标签空间”概率？然后，使用相同的体验编码器，编码当前状态（或世界模型）（编码为上述标签空间），并将其与所有缓冲体验进行比较。在任何匹配的地方，都会更有可能显示这种缓冲体验。 比较可以通过深网或简单的日志损失（二进制跨透明拷贝）进行比较。我认为，这种修改在SOTA世界模型中特别有用，在SOTA世界模型中，我们需要预测50个步骤，并且拥有更多相关的输入数据将是100％有用的 在最坏的情况下，我们会牺牲一点性能和随机样品，充其量，我们获得了非常扎实的经验重播。  watchu认为伙计们？ 我想到了这一点，因为我正在工作解决最难的RL AGI之后的问题，我需要这种边缘才能使我的模型更具性能。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/stzed32     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipxgd1/r_labelling_experiences_in_reinforcement_learning/</guid>
      <pubDate>Sat, 15 Feb 2025 08:57:51 GMT</pubDate>
    </item>
    <item>
      <title>困惑AI Pro 1年订阅</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipwsil/perplexity_ai_pro_1_year_subscription/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  出售困惑AI Pro 1年订阅以999€ 32;提交由＆＃32; &lt; //www.reddit.com/r/reinforecricelearning/comments/1ipwsil/perplexity_ai_pro_1_year_year_subscription/&quot;&gt; [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipwsil/perplexity_ai_pro_1_year_subscription/</guid>
      <pubDate>Sat, 15 Feb 2025 08:06:18 GMT</pubDate>
    </item>
    <item>
      <title>RL之后的模仿学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipn26x/imitation_learning_after_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我知道您可以在模仿学习后执行RL，但是在RL之后您可以执行模仿学习。  &lt;！ -  sc_on - &gt;＆＃32;提交由＆＃32; /u/u/u/robotdodgeball     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipn26x/imitation_learning_after_rl/</guid>
      <pubDate>Fri, 14 Feb 2025 22:38:12 GMT</pubDate>
    </item>
    <item>
      <title>寻找具有语言定义目标的培训RL代理的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipiuwo/looking_for_work_on_training_rl_agents_with/</link>
      <description><![CDATA[在自然语言。具体来说，我正在寻找探索的工作：  使用语言作为灵活的奖励信号  培训政策以为条件文本中的描述   通过LLMS    层次结构rl与语言引导的子搜索   我很想阅读任何论文，存储库或博客文章探索此主题的内容。如果您从事类似的事情，我也很乐意讨论想法或协作！ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃ 32;提交由＆＃32; /u/u/foricas-ad2641     link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipiuwo/looking_for_work_on_training_rl_agents_with/</guid>
      <pubDate>Fri, 14 Feb 2025 19:33:38 GMT</pubDate>
    </item>
    <item>
      <title>需要RL学习伙伴</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipa8fy/need_study_partner_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在担任2.5 Yoe的数据科学家，主要是在经典ML和NLP上工作，但想探索RL，因为我可能有使用情况我的工作是从YT上观看David Silver演讲开始的，但是它的数学太重了（目前在第二LEC上），如果我能够完成PR，我会失去信心彼此之间明确的怀疑。请随意dm me !!   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/shirish0500     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipa8fy/need_study_partner_for_rl/</guid>
      <pubDate>Fri, 14 Feb 2025 13:10:53 GMT</pubDate>
    </item>
    <item>
      <title>实验室可以在欧洲的RL上获得博士学位</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip9lur/labs_to_do_a_phd_in_rl_in_europe/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在寻找2026年的博士带有RL等的型号。我不是在研究纯MDP或土匪之类的东西。我想要更适用的东西，例如可塑性研究，终身学习，甚至更好的RL架构，或多代理或分层RL，RL + LLM，RL +扩散等。更多的ML喜欢更好的变压器体系结构，状态空间模型等。我在EPFL，ETH和DARMSTADT上看到了一些实验室。但是真的很感激。 ＆＃32;提交由＆＃32; /u/u/no_carpenter7252      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip9lur/labs_to_do_a_phd_in_rl_in_europe/</guid>
      <pubDate>Fri, 14 Feb 2025 12:35:32 GMT</pubDate>
    </item>
    <item>
      <title>RL教程的业余爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://github.com /google-deepmind/mujoco/descordions/2404    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/goncalogordo     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</guid>
      <pubDate>Fri, 14 Feb 2025 09:06:19 GMT</pubDate>
    </item>
    <item>
      <title>熵重</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6d4b/entropy_weight/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在使用软演员评论家进行多代理强化学习。折扣奖励约为1000-1300。熵重的正确值是多少？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fuzzy-plantain2402      link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6d4b/entropy_weight/</guid>
      <pubDate>Fri, 14 Feb 2025 08:49:24 GMT</pubDate>
    </item>
    <item>
      <title>回顾我成为利基领域的RL研究人员的计划（AG/遗传学）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip51q9/review_my_plan_for_becoming_an_rl_researcher_in_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi。我从事遗传学和农业工作。我是平庸的软件工程师/定量遗传学家，数学背景相对较差。去年，我制作了一个简单的问题。在Pytorch中制作了一个环境，并使用SB3成功地训练代理，并成功地训练了一个非常简单的用例。我还审查了我的数学量，一旦我遇到泰勒系列（我从未在学校正式研究过）。 我已经完成了机器学习问题（计算机视觉）。在成功的行业中，对基金会的研究足以在可可/重新确定年龄的情况下进行监督/无监督的学习问题。  ，所以我的计划是做以下  1）继续研究DSA（阅读数据结构和算法的常识指南，第I卷，也许是第II卷）  2）阅读Grokking DL算法书籍  3）然后通过此内容（祈祷最近的LLM可以推动我解决此问题） https://github.com/mathfoundationrll /读书基础学习学习    all    4）在Jax中精心选择的功能重建我的环境然后再次重新进行我过去的SB3实验。 *虽然对我的实验非常公开  5）扩展实验以具有真实的分布式组件（在上下文中有意义）  我的目标是做这些工具在我的行业中的咨询...因此，一旦我的基础知识降低了，也许我可以开始独立发表论文。否则，我将不得不考虑申请其中一家大公司或攻读博士学位 我有6个月的合理跑道来实现这一目标，这使我希望我的基础很务实。我没有任何社区或指导可以与之讨论。如果我能在我放下头之前对此获得任何反馈，我会感谢它。另外，我不会拒绝任何经验丰富的人保持联系。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/foodisaweapon     [link]   ＆＃32;   [注释]     ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip51q9/review_my_plan_for_becoming_an_rl_researcher_in_a/</guid>
      <pubDate>Fri, 14 Feb 2025 07:10:51 GMT</pubDate>
    </item>
    <item>
      <title>人类会做RL，监督学习还是完全不同的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip2cl2/do_humans_do_rl_supervised_learning_or_something/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我已经在加强学习已经几个月了，当我不得不汗水以定义该问题时，这个问题总是在我的脑海中正确的奖励。 我得到了这种感觉，我们能够根据真正的奖励创造中间奖励。就像为了在X Company找到工作一样，我必须以前磨削这些n步骤，并且每次执行此步骤时都会很高兴。 在RL中RL模型，如果您可以正确调整损失功能？ 我的问题似乎尚不清楚，并且非常开放。我只是觉得人类在RL和有监督的学习之间有一个中间的，我无法真正掌握我的头。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/delicious_wall3597      [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1ip2cl2/do_humans_do_do_rl_supervise_learning_learning_or_something/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip2cl2/do_humans_do_rl_supervised_learning_or_something/</guid>
      <pubDate>Fri, 14 Feb 2025 04:19:33 GMT</pubDate>
    </item>
    <item>
      <title>Langevin Soft Actor-Critic：通过不确定性驱动的批评者学习，Ishfaq等人2025。ICLR 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</link>
      <description><![CDATA[        &lt;！ -  sc__off- sc_off-&gt;  现有的Actor-Critic算法在连续控制加强学习（RL）任务中很受欢迎，由于其中缺乏原则性的探索机制，其样本效率不佳。由于汤普森采样成功在RL中有效探索的动机，我们提出了一种新颖的无模型RL算法，\ emph {langevin soft Actor评论家}（LSAC）（LSAC）优先考虑通过对策略优化的不确定性估计来增强评论家的学习。 LSAC采用了三个关键创新：通过基于分布的LangeMonte Carlo（LMC）更新，近似汤普森采样，平行回火，用于探索该功能后部多种模式，以及与动作梯度正常化的综合状态行动样品。我们的广泛实验表明，LSAC的表现优于或匹配无连续控制任务的无主流RL算法的性能。值得注意的是，LSAC标志着基于LMC的Thompson采样在具有连续动作空间的连续控制任务中的首次成功应用  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hmi2015     [link]  ＆＃32;   [注释]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</guid>
      <pubDate>Thu, 13 Feb 2025 18:50:08 GMT</pubDate>
    </item>
    </channel>
</rss>