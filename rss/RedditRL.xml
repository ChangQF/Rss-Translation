<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 10 Jun 2024 12:28:27 GMT</lastBuildDate>
    <item>
      <title>ğŸ™ Octoï¼šå¼€æºé€šæ‰æœºå™¨äººæ”¿ç­–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcit8c/octo_an_opensource_generalist_robot_policy/</link>
      <description><![CDATA[  ç”±    /u/HimitsuNoShougakusei  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcit8c/octo_an_opensource_generalist_robot_policy/</guid>
      <pubDate>Mon, 10 Jun 2024 11:02:59 GMT</pubDate>
    </item>
    <item>
      <title>æ¨¡æ‹Ÿé€€ç«ä¸å¼ºåŒ–å­¦ä¹ </title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</link>
      <description><![CDATA[å½“è€ƒè™‘å¯å‘å¼ç«äº‰ç¼–ç¨‹ä»»åŠ¡æ—¶ä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ã€‚è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ä¾‹å­ï¼Œæ—…è¡Œå•†é—®é¢˜ï¼ˆæˆ–è€…æœ€è¿‘çš„è¿™ä¸ªç«èµ›ï¼Œå¾ˆå¤šäººéƒ½åœ¨è®¨è®º RL çš„å¯èƒ½æ€§ï¼Œä½†å¤§å¤šæ•°äººéƒ½ä¸æ˜¯ä¸“å®¶ï¼ˆåŒ…æ‹¬æˆ‘è‡ªå·±ï¼Œæœ€ç»ˆä¹Ÿä½¿ç”¨äº†æ¨¡æ‹Ÿé€€ç«ï¼Œä½†äº‹åå´å¾ˆç—›è‹¦ï¼Œå› ä¸ºæˆ‘æœ¬æ¥æƒ³åšç‚¹ä¸åŒçš„äº‹æƒ…ï¼‰ï¼‰ã€‚ å‡ ä¹æ‰€æœ‰è¿™äº›æ¯”èµ›éƒ½æ˜¯ä½¿ç”¨æ¨¡æ‹Ÿé€€ç«æˆ–å…¶ä»–å˜ä½“èµ¢å¾—çš„ã€‚å¯¹äºä¸ç†Ÿæ‚‰çš„äººæ¥è¯´ï¼Œæ‰€æœ‰è¿™äº›å˜ä½“éƒ½æ˜¯ä»æŸä¸ªè§£å†³æ–¹æ¡ˆå¼€å§‹ï¼Œç„¶åé€šè¿‡æŸç§å˜å¼‚è¿‡ç¨‹è¿­ä»£æ”¹è¿›å®ƒï¼Œä»¥æ‘†è„±å±€éƒ¨æœ€å°å€¼ã€‚å¯¹äºæ—…è¡Œå•†é—®é¢˜ï¼Œæ‚¨å¯ä»¥æå‡ºä¸€ä¸ªåˆå§‹çš„éšæœºåŸå¸‚åˆ—è¡¨ï¼Œç„¶åéšæœºäº¤æ¢ä¸€äº›åŸå¸‚ï¼Œç›´åˆ°å®ƒæ”¹è¿›äº†æ‚¨çš„è§£å†³æ–¹æ¡ˆï¼Œç„¶åå°†è¿™ä¸ªæ–°è§£å†³æ–¹æ¡ˆä½œä¸ºæœ€ä½³è§£å†³æ–¹æ¡ˆï¼Œä¾æ­¤ç±»æ¨ã€‚å†åŠ ä¸Šä¸€äº›çªå˜ä»¥é€ƒé¿å±€éƒ¨æœ€å°å€¼ï¼ˆä¾‹å¦‚ï¼Œæ„å‘³ç€å¯¹åˆ—è¡¨çš„ä¸€å°éƒ¨åˆ†è¿›è¡Œæ”¹ç»„ - æˆ‘æ˜¾ç„¶æ˜¯åœ¨ç®€åŒ–ï¼‰ã€‚ ä»€ä¹ˆä¼šé˜»æ­¢äººä»¬åœ¨è¿™äº›é—®é¢˜ä¸Šä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆå®é™…ä¸Šæ²¡æœ‰äººï¼Œè¿™ç¯‡æ–‡ç« ä¸­å·²ç»é’ˆå¯¹æ—…è¡Œå•†é—®é¢˜å®Œæˆäº†æ­¤æ“ä½œï¼šhttps://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/tje2.12303 - å¦‚æœæˆ‘æ²¡çœ‹é”™çš„è¯ï¼Œä½œè€…ç”šè‡³æåˆ°äº†æ¨¡æ‹Ÿé€€ç«ï¼Œä½†æ²¡æœ‰å°†ç»“æœä¸å®ƒè¿›è¡Œæ¯”è¾ƒï¼‰ã€‚å¥–åŠ±å‡½æ•°é€šå¸¸ä¸éš¾æƒ³å‡ºï¼ˆæˆ‘åœ¨ç«èµ›ä¸­æåˆ°çš„å¥–åŠ±å‡½æ•°ç”šè‡³æ¯” TSP æ›´å®¹æ˜“ï¼Œå› ä¸ºæ¯æ¬¡â€œæ€ªç‰©â€æ­»äº¡åä½ éƒ½ä¼šè·å¾—â€œé‡‘å¸â€ï¼Œä½ ä¼šå°è¯•æœ€å¤§åŒ–å®ƒï¼ˆç´¯è®¡é‡‘é¢ï¼‰ï¼‰ã€‚ æˆ‘å¯¹ä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ çš„å‡è®¾æ˜¯ï¼š  å°½ç®¡å¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬æ•ˆç‡æ›´é«˜ï¼Œä½†è¿™äº›é—®é¢˜å®é™…ä¸Šå¾ˆå®¹æ˜“æ¨¡æ‹Ÿï¼Œå› æ­¤æ›´æ–°ç¥ç»ç½‘ç»œæˆ–ä»»ä½•å‡½æ•°è¿‘ä¼¼å™¨çš„å¼€é”€éƒ½å¤ªé«˜ã€‚åªæœ‰å½“è¿è¡Œä¸€é›†çš„æˆæœ¬éå¸¸é«˜æ—¶ï¼Œå¼ºåŒ–å­¦ä¹ æ‰ä¼šæœ‰è¶£ã€‚å¦åˆ™ï¼Œç”¨ C ç¼–å†™ç®€å•çš„é—ä¼ ç®—æ³•æ€»æ˜¯æ¯”ç”¨ Python ç¼–å†™çš„ RL æ›´æœ‰æ•ˆï¼ˆæ—¶é—´æ–¹é¢ï¼‰ã€‚ æ— éœ€æ¦‚æ‹¬ï¼Œè¿™äº›æ¯”èµ›çš„æµ‹è¯•ç”¨ä¾‹å·²ç»ç»™å‡ºï¼Œä½ åªéœ€è¦æƒ³å‡ºæœ€ä½³çš„è¡ŒåŠ¨åºåˆ—æ¥å½±å“ç¯å¢ƒï¼ˆä¾‹å¦‚ï¼Œåœ¨æˆ‘çš„ç¬¬äºŒä¸ªä¾‹å­ä¸­è¦æ€æ­»å“ªäº›æ€ªç‰©ï¼‰å¹¶åœ¨è¿™äº›æµ‹è¯•ç”¨ä¾‹ä¸­è·å¾—æœ€é«˜å¥–åŠ±ã€‚å¦‚æœæ¯”èµ›å†…å®¹ç›¸åŒï¼Œä½†ä»–ä»¬åœ¨æ¯”èµ›ç»“æŸå‰ä¸‰ååˆ†é’Ÿå…¬å¸ƒæµ‹è¯•ç”¨ä¾‹ï¼Œé‚£ä¹ˆåœ¨ 8000 ä¸ªçº¿ç¨‹ä¸Šè¿è¡Œæ¨¡æ‹Ÿé€€ç«ä¸‰ååˆ†é’Ÿçš„æ•ˆç‡ä¸å¦‚ä½¿ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„ä»£ç†ï¼Œè¯¥ä»£ç†åœ¨ GPU ä¸Šç»è¿‡å‡ å¤©çš„å¤§é‡ä¸åŒè™šæ„æµ‹è¯•ç”¨ä¾‹çš„è®­ç»ƒã€‚ RL åœ¨å¤šä»£ç†è®¾ç½®ï¼ˆé›¶å’Œæ¸¸æˆç­‰ï¼‰ä¸­çœŸæ­£æ˜¾ç¤ºäº†å…¶ä¸»å¯¼åœ°ä½ï¼Œå…¶ä¸­æ¨¡æ‹Ÿé€€ç«å’Œå˜ä½“ä¸æ˜“å®ç°ï¼ˆå°½ç®¡ MARL ä¼˜åŒ–çš„æ¯ä¸€æ­¥éƒ½åœ¨å°è¯•åˆ©ç”¨å½“å‰æœ€ä½³ç­–ç•¥ç»„åˆï¼Œè¿™å¯ä»¥é€šè¿‡é—ä¼ ç®—æ³•æ¥å®Œæˆ - ä½†æˆ‘è®¤ä¸ºè¿™è¢«ç§°ä¸º RLï¼Œå®ƒåªæ˜¯æ²¡æœ‰æ¢¯åº¦çš„ RLï¼‰ã€‚ ä½†åŒæ—¶ï¼ŒRL æ¯”å…¶ä»–æŠ€æœ¯æ›´å¤æ‚ï¼Œæ‰€ä»¥ä¹Ÿè®¸äººä»¬åªæ˜¯å› ä¸ºæ²¡æœ‰ä¸“ä¸šçŸ¥è¯†è€Œä¸å»é‚£é‡Œï¼Œè€Œ RL ä¸“å®¶å®é™…ä¸Šä¼šåœ¨å…¶ä¸­ä¸€äº›æ¯”èµ›ä¸­è¡¨ç°å‡ºè‰²ï¼Ÿ  æˆ‘é—æ¼äº†ä»€ä¹ˆå—ï¼Ÿæ‚¨ä»¬è¿™äº› RL ä¸“å®¶æ€ä¹ˆçœ‹ï¼ŸRich ä¼šæ€ä¹ˆè¯´ã€‚è¨é¡¿è¯´äº†ä»€ä¹ˆï¼Ÿ    æäº¤äºº    /u/Lindayz   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</guid>
      <pubDate>Mon, 10 Jun 2024 10:52:31 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºPPOä¸­å°æ‰¹é‡çš„é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcfqfd/question_about_minibatch_in_ppo/</link>
      <description><![CDATA[ä½ å¥½ï¼Œæˆ‘æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ–°æ‰‹ï¼Œå¯¹è®­ç»ƒ PPO æ—¶è®¾ç½®å°æ‰¹é‡æœ‰ç–‘é—®ã€‚ åœ¨åŸºäº DQN çš„ç®—æ³•ä¸­ï¼Œæˆ‘ç†è§£å°æ‰¹é‡å¯¹åº”äºä»é‡æ”¾ç¼“å†²åŒºä¸­è·å–çš„æ ·æœ¬æ•°é‡ã€‚ ä½†åœ¨åƒ PPO è¿™æ ·çš„åŸºäºç­–ç•¥çš„ç®—æ³•ä¸­ï¼Œéœ€è¦ä¸€é›†çš„å®Œæ•´è½¨è¿¹æ¥è®¡ç®—ç­–ç•¥æ¢¯åº¦ã€‚ è¿™æ˜¯å¦æ„å‘³ç€ PPO ä¸­çš„å°æ‰¹é‡å¯¹åº”äºé›†æ•°ï¼Ÿ è¿™éå¸¸ä»¤äººå›°æƒ‘ï¼Œå› ä¸ºåœ¨ DQN ä¸­æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ TD è¯¯å·®æ¥è®¡ç®—æ¢¯åº¦ï¼Œä½†åŸºäºç­–ç•¥çš„ç®—æ³•éœ€è¦å…¨é•¿æ¥è®¡ç®—æ¢¯åº¦ã€‚    æäº¤äºº    /u/MediocreAgency6070   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcfqfd/question_about_minibatch_in_ppo/</guid>
      <pubDate>Mon, 10 Jun 2024 07:22:25 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•æ„å»º Mujoco Envsã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dc44c7/how_to_build_mujoco_envs/</link>
      <description><![CDATA[æˆ‘æƒ³ä¸º Kinova çš„ jaco2 æ‰‹è‡‚æ„å»ºä¸€ä¸ª Mujoco ç¯å¢ƒï¼Œæˆ‘æœ‰ CADï¼Œå¹¶ä¸”å­˜åœ¨ ROS Gazebo ç¯å¢ƒï¼Œä½†æˆ‘ä¸€ç”Ÿéƒ½è®¨åŒ ROSï¼Œè€Œä¸”æˆ‘æ›´å–œæ¬¢ RL è€Œä¸æ˜¯æœºå™¨äººæŠ€æœ¯ã€‚æˆ‘çœŸçš„åœ¨å¯»æ‰¾é™¤ XML ä¹‹å¤–çš„å¦ä¸€ç§æ„å»ºç¯å¢ƒçš„æ–¹æ³•ã€‚è®©æˆ‘çŸ¥é“ã€‚     æäº¤äºº    /u/elonmusk-A12   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dc44c7/how_to_build_mujoco_envs/</guid>
      <pubDate>Sun, 09 Jun 2024 21:00:14 GMT</pubDate>
    </item>
    <item>
      <title>â€œå¥–åŠ±é»‘å®¢è¡Œä¸ºå¯ä»¥æ¨å¹¿åˆ°æ‰€æœ‰ä»»åŠ¡â€ï¼ŒNishimura-Gasparian ç­‰äººï¼Œ2024 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dc3obb/reward_hacking_behavior_can_generalize_across/</link>
      <description><![CDATA[        æäº¤äºº    /u/gwern   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dc3obb/reward_hacking_behavior_can_generalize_across/</guid>
      <pubDate>Sun, 09 Jun 2024 20:41:36 GMT</pubDate>
    </item>
    <item>
      <title>å°†å®éªŒç§»è‡³å¥èº«æˆ¿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbpceo/moving_experiment_to_gym/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æœ‰ç‚¹ç»æœ›ï¼Œå› ä¸ºæˆ‘æ­£åœ¨è¯•å›¾å¼„æ¸…æ¥šè¿™ä¸€ç‚¹ï¼Œè€Œä¸”æ²¡æœ‰å¤šå°‘æ—¶é—´äº†ã€‚ æˆ‘ç›®å‰æ­£å¤„äºè®ºæ–‡çš„åå­—è·¯å£ï¼Œè®ºæ–‡å°†åœ¨ä¸‰å‘¨å†…æˆªæ­¢ã€‚æ·±å…¥ç ”ç©¶å¹¶å°è¯•äº†ç¨³å®šåŸºçº¿å’Œç®€å•æ¨¡å‹åï¼Œæˆ‘æ„è¯†åˆ°ï¼Œè¦ä½¿æˆ‘çš„é¡¹ç›®æ­£å¸¸è¿è¡Œå¹¶ä½¿ç”¨ç¨³å®šåŸºçº¿æˆ– CleanRLï¼Œæˆ‘éœ€è¦åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ gym ç¯å¢ƒã€‚ åœ¨æˆ‘çš„é¡¹ç›®ä¸­ï¼Œæˆ‘å¿…é¡»ä½¿ç”¨ç‰©ç†æ¨¡æ‹Ÿå™¨ (NS3)ï¼Œå®ƒç”±å¤šè¾†è¯•å›¾åŒæ—¶ä¼ è¾“ä¿¡æ¯çš„æ±½è½¦ç»„æˆï¼Œå¹¶å°è¯•ä½¿ç”¨ RL æ¥äº†è§£å¦‚ä½•å‡å°‘ç½‘ç»œå»¶è¿Ÿã€‚ï¼ˆè§‚å¯Ÿç©ºé—´ã€äº¤é€šçŠ¶æ€å…ˆå‰çš„å»¶è¿Ÿï¼›æ¯è¾†è½¦ä¼ è¾“ä¿¡å·æ—¶çš„åŠ¨ä½œç©ºé—´ï¼‰ã€‚  æˆ‘å·²ç»æœ‰ä¸€ä¸ªä½¿ç”¨ DQN æ„å»ºçš„ä»£ç ï¼Œä½†æ˜¯æ€§èƒ½ä¸€ç›´å¾ˆå·®ï¼Œæˆ‘çš„â€œè®ºæ–‡â€æ˜¯å…³äºæ¢ç´¢æ–°ç®—æ³•çš„ã€‚ å› æ­¤ï¼Œæˆ‘æƒ³åˆ©ç”¨è¿™é‡Œçš„ reddit è¯»è€…çš„ç»éªŒå’Œæ™ºæ…§ - æˆ‘åº”è¯¥ (1) å°†æˆ‘çš„ç¯å¢ƒç§»è‡³ Gym å—ï¼Ÿ (2) åœ¨ GitHub ä¸Šå°è¯•ä¸€äº›åŸç”Ÿ PPO ç®—æ³•å—ï¼Ÿå¦‚æœæ‚¨èƒ½æä¾›ä»»ä½•å¯ä»¥æŸ¥çœ‹çš„åœ°æ–¹ï¼Œé‚£å°±å¤ªæ£’äº†ï¼ï¼ ä»»ä½•æƒ³æ³•éƒ½å°†ä¸èƒœæ„Ÿæ¿€ &lt;3ï¼éå¸¸æ„Ÿè°¢ï¼    æäº¤äºº    /u/No-Jelly-233   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbpceo/moving_experiment_to_gym/</guid>
      <pubDate>Sun, 09 Jun 2024 08:52:22 GMT</pubDate>
    </item>
    <item>
      <title>â€œå…‹åŠ³å¾·çš„æ€§æ ¼â€ï¼ŒAnthropicï¼ˆè®¾è®¡å…‹åŠ³å¾·-3åŠ©æ‰‹è§’è‰²ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbedkn/claudes_character_anthropic_designing_the_claude3/</link>
      <description><![CDATA[        æäº¤äºº    /u/gwern   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbedkn/claudes_character_anthropic_designing_the_claude3/</guid>
      <pubDate>Sat, 08 Jun 2024 22:20:01 GMT</pubDate>
    </item>
    <item>
      <title>DQN ä¸ Vanilla ç­–ç•¥æ¢¯åº¦</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbb6m7/dqn_vs_vanilla_policy_gradient/</link>
      <description><![CDATA[      å—¨ï¼Œ æˆ‘æ­£åœ¨å°è¯•æ¯”è¾ƒä¸¤ç§æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼šVanilla Policy Gradient (REINFORCE) å’Œ DQNã€‚æˆ‘å¸Œæœ›æˆ‘çš„ç®—æ³•èƒ½å¤Ÿå­¦ä¼šåœ¨ä¸€ç»„æœºå™¨ä¸Šè°ƒåº¦ä½œä¸šï¼ŒåŒæ—¶å°½å¯èƒ½ç´§å¯†åœ°æ‰“åŒ…å®ƒä»¬ã€‚è§‚å¯Ÿç©ºé—´æ˜¯å°ºå¯¸ä¸º (20, 164) çš„å›¾åƒï¼Œä»£ç†å¿…é¡»ä» 10 å°æœºå™¨ä¸­é€‰æ‹© 1 å°ã€‚æˆ‘åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ 100 ä¸ªä¸åŒçš„ä½œä¸šåºåˆ—ï¼Œå¹¶ä½¿ç”¨ä¸€ç»„æ–°ä½œä¸šè¿›è¡Œæµ‹è¯•ã€‚æ¯é›†ä½¿ç”¨ä¸€ä¸ªä½œä¸šåºåˆ—ã€‚ ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘çš„ DQN ç®—æ³•çš„è¡¨ç°æ¯” Vanilla Policy Gradient (VPG) æ›´å·®ã€‚æˆ‘å°è¯•è°ƒæ•´ tauã€é‡æ”¾ç¼“å†²åŒºå¤§å°ã€æ‰¹é‡å¤§å°ã€æŠ˜æ‰£å› å­ã€å­¦ä¹ ç‡ç­‰ï¼Œä½†ç»“æœä¼¼ä¹æ²¡æœ‰ VPG é‚£ä¹ˆé«˜ã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œæ˜¯å…·æœ‰ä»¥ä¸‹ DQN é…ç½®çš„ä¸¤è€…çš„æŠ˜æ‰£å¹³å‡å¥–åŠ±å›¾è¡¨  G = 0.95 é‡æ”¾å†…å­˜ = 100_000  æœ€å°é‡æ”¾å†…å­˜ = 10_000  æ‰¹é‡å¤§å° = 64  tau = 10  lr = 0.001 epochs = 600ï¼ˆåœ¨æ¯ä¸ª epoch è¿è¡Œ 20 episodes åè®­ç»ƒç½‘ç»œï¼‰ episodes = 20 éšè—èŠ‚ç‚¹ = 128 epochs æ•°é‡ = 600 æ¯ä¸ª epoch çš„ episodes æ•°é‡ = 20 å¦ã€‚æ¯é›†çš„å·¥ä½œé‡ = 200  .æ‚¨èƒ½å¸®åŠ©æˆ‘ç†è§£ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µå—ï¼Ÿæˆ‘åœ¨æ¯ä¸ªæ—¶æœŸåéƒ½ä¼šè®­ç»ƒä¸»ç½‘ç»œã€‚ https://preview.redd.it/3gg0dz3zje5d1.png?width=1924&amp;format=png&amp;auto=webp&amp;s=ac90a3c378d0817f00acc79600e87a34cb7c0fb3   ç”±    /u/hifzak  æäº¤  [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbb6m7/dqn_vs_vanilla_policy_gradient/</guid>
      <pubDate>Sat, 08 Jun 2024 19:49:39 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ äº§å“åŒ–çš„å¿…è¦ä¹‹æ¶</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbagrf/a_necessary_evil_for_productionizing_rl/</link>
      <description><![CDATA[        æäº¤äºº    /u/bin_und_zeit   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbagrf/a_necessary_evil_for_productionizing_rl/</guid>
      <pubDate>Sat, 08 Jun 2024 19:16:54 GMT</pubDate>
    </item>
    <item>
      <title>RLZoo è¶…å‚æ•°ä¸­çš„â€œnormalizeâ€æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1db61s7/what_does_normalize_mean_in_rlzoo_hyperparameters/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨æŸ¥çœ‹è¿™é‡Œç»™å‡ºçš„ RLZoo è¶…çº§å‚æ•°ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹ä»£ç  - InvertedDoublePendulumBulletEnv-v0: normalize: true n_envs: 8 n_timesteps: !!float 2e6 policy: &#39;MlpPolicy&#39; n_steps: 2048 batch_size: 64 gae_lambda: 0.95 gamma: 0.99 n_epochs: 10 ent_coef: 0.0 learning_rate: 2.5e-4 clip_range: 0.2   è¿™é‡Œçš„ `normalize: true` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæˆ‘å‡è®¾è¿™å¯èƒ½æ˜¯è§‚å¯Ÿæ ‡å‡†åŒ–ï¼Œä½†æ˜¯ï¼Œè¿™é‡Œæœ‰è¿™æ ·çš„ä»£ç  -normalize: &quot;{&#39;norm_obs&#39;: False, &#39;norm_reward&#39;: True}&quot;å› æ­¤ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œ `normalize: true` æ—¶ï¼Œä»€ä¹ˆæ˜¯è¢«è§„èŒƒåŒ–çš„ï¼Ÿ rlzoo ä¸­æ˜¯å¦æœ‰ç”¨äºåŠ¨ä½œè§„èŒƒåŒ–çš„è¶…å‚æ•°ï¼Ÿæˆ‘æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ï¼Œä½†ä¹Ÿè®¸å®ƒå¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Ÿ     æäº¤äºº    /u/Academic-Rent7800   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1db61s7/what_does_normalize_mean_in_rlzoo_hyperparameters/</guid>
      <pubDate>Sat, 08 Jun 2024 15:55:29 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘çš„åŠ¨ä½œç©ºé—´æœ‰ 90% è¢«æ©ç›–äº†ï¼Œæˆ‘å¸Œæœ›ä»ä¸­è·å¾—è®¡ç®—ä¸Šçš„å¥½å¤„</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</link>
      <description><![CDATA[æˆ‘çš„ q-learning ä»»åŠ¡ä¸­æœ‰ä¸€ä¸ªåŒ…å« 5000 ä¸ªåŠ¨ä½œçš„å¤§å‹åŠ¨ä½œç©ºé—´ï¼Œä½†åœ¨ä»»ä½•æ—¶å€™éƒ½åªæœ‰å‡ ç™¾ä¸ªåˆæ³•åŠ¨ä½œï¼Œå› æ­¤æˆ‘ä¼šå±è”½å®ƒä»¬ï¼Œä½†æ˜¯ï¼Œæˆ‘åªæ˜¯åœ¨æ¨¡å‹é¢„æµ‹æ­¥éª¤ä¹‹åè¿›è¡Œå±è”½ã€‚æœ‰æ²¡æœ‰åŠæ³•äº‹å…ˆå±è”½å®ƒå¹¶è·å¾—å¤§å¹…åŠ é€Ÿï¼Ÿ  def get_predictions(self, state, legal_mask): state = np.reshape(state, [1, self.state_size) act_values = [act_values[i] if legal_mask[i] == 1 else -np.inf for i in range(len(act_values))] return act_values     submitted by    /u/Breck_Emert   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</guid>
      <pubDate>Sat, 08 Jun 2024 07:03:17 GMT</pubDate>
    </item>
    <item>
      <title>[CfP] ä¸ RealAIGym åˆä½œä¸¾åŠç¬¬äºŒå±Šäººå·¥æ™ºèƒ½å¥¥è¿ä¼šï¼šIROS 2024 æœºå™¨äººç«èµ›â€”â€”ç«‹å³åŠ å…¥ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dahea9/cfp_2nd_ai_olympics_with_realaigym_robotics/</link>
      <description><![CDATA[        æäº¤äºº    /u/Dense-Positive6651   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dahea9/cfp_2nd_ai_olympics_with_realaigym_robotics/</guid>
      <pubDate>Fri, 07 Jun 2024 18:04:52 GMT</pubDate>
    </item>
    <item>
      <title>è®¡ç®—ä¸¤ä¸ª Q å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dacrpc/calculating_kldivergence_between_two_qlearning/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æƒ³è®¡ç®—ä½¿ç”¨ Q-learning è®­ç»ƒçš„ä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦ã€‚ç”±äº Q-learning æ ¹æ®æœ€é«˜ Q å€¼é€‰æ‹©åŠ¨ä½œï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™äº›ç­–ç•¥æ˜¯å¦åº”è¯¥è¡¨ç¤ºä¸ºç‹¬çƒ­å‘é‡ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œè€ƒè™‘åˆ°ç‹¬çƒ­å‘é‡ä¸­æ¦‚ç‡ä¸ºé›¶çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•è®¡ç®— KL æ•£åº¦ï¼Ÿ    æäº¤äºº    /u/Sea-Collection-8844   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dacrpc/calculating_kldivergence_between_two_qlearning/</guid>
      <pubDate>Fri, 07 Jun 2024 14:55:45 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ ä¸­å¯ä»¥ä»äº‹å“ªäº›å·¥ä½œ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dacmz0/which_jobs_in_reinforcement_learning/</link>
      <description><![CDATA[å˜¿ï¼Œ ç”±äºæˆ‘å¯¹å¼ºåŒ–å­¦ä¹ è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œæˆ‘è¯•å›¾äº†è§£å¦‚ä½•å¯»æ‰¾ä½¿ç”¨å®ƒçš„å·¥ä½œã€‚ å¤§å¤šæ•°æ•°æ®ç§‘å­¦å®¶ã€mlopsã€æ•°æ®å·¥ç¨‹å¸ˆèŒä½ä¼¼ä¹éƒ½åœ¨ä½¿ç”¨ç›‘ç£æœºå™¨å­¦ä¹ ï¼Œä¹Ÿè®¸åœ¨è¥é”€ç­‰æ–¹é¢ä½¿ç”¨æ— ç›‘ç£å­¦ä¹ ã€‚ é™¤äº†æœºå™¨äººå…¬å¸ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•ä½¿ç”¨ RL çš„èŒä½æè¿°ã€‚ ä½ å¦‚ä½•å¯»æ‰¾å¯ä»¥åº”ç”¨ RL çš„å·¥ä½œï¼Ÿ æ­¤å¤–ï¼ŒRL åœ¨æ¸¸æˆå’Œæœºå™¨äººèƒŒåçš„åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ æˆ‘å°†åˆ—å‡ºå‡ ä¸ªæˆ‘æ„Ÿå…´è¶£çš„ä¸»é¢˜ï¼Œä½†æˆ‘ä¸ç¡®å®šéœ€è¦åº”ç”¨å“ªç§ç±»å‹çš„æœºå™¨å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ï¼š  äº¤é€šä¼˜åŒ–ï¼Œ æ”¿åºœé¢„ç®—ä¼˜åŒ– æ‹¥æœ‰æ›´â€œå®Œæ•´â€è§†é¢‘æ¸¸æˆä½“éªŒ - å³ä¸ NPC åˆä½œï¼Œå¦‚ä½•ä¸ä»–ä»¬äº’åŠ¨ç­‰ã€‚  æ¨¡æ‹Ÿç¯å¢ƒï¼ˆä¸çŸ¥é“æ˜¯å¦å­˜åœ¨ï¼Œä½†å‡è®¾æ‚¨å°è¯•æ¨¡æ‹ŸæŸé¡¹æ–°æ³•å¾‹å°†å¯¹äººå£äº§ç”Ÿä»€ä¹ˆå½±å“ï¼Ÿï¼‰ æœºå™¨äººæŠ€æœ¯     æäº¤äºº    /u/BoxingBytes   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dacmz0/which_jobs_in_reinforcement_learning/</guid>
      <pubDate>Fri, 07 Jun 2024 14:50:11 GMT</pubDate>
    </item>
    <item>
      <title>Unity ML-Agents ä¸ Unreal å­¦ä¹ ä»£ç†</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da7j72/unity_mlagents_vs_unreal_learning_agents/</link>
      <description><![CDATA[å¯¹äºæˆ‘å³å°†åˆ°æ¥çš„å­¦å£«é¡¹ç›®ï¼Œæˆ‘å’Œæˆ‘çš„å›¢é˜Ÿå†³å®šå¼€å‘ä¸€æ¬¾å¸¦æœ‰ RL ä»£ç†çš„æ¸¸æˆã€‚æ¸¸æˆæ¦‚å¿µå°šæœª 100% ç¡®å®šã€‚æˆ‘ä»¬ç°åœ¨çš„ä¸»è¦é—®é¢˜æ˜¯å†³å®šæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å“ªç§æ¸¸æˆå¼•æ“ã€‚ä½ ä»¬å½“ä¸­æœ‰è°åŒæ—¶ä½¿ç”¨è¿‡ Unity ML-Agents å’Œ Unreal çš„å­¦ä¹ ä»£ç†ï¼Œå¯ä»¥å¯¹è¿™ä¸¤ä¸ªç¯å¢ƒè¿›è¡Œç®€å•çš„æ¯”è¾ƒå—ï¼Ÿ å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ›´å–œæ¬¢ Unity çš„æ¡†æ¶ï¼Œå› ä¸ºæˆ‘æ‹…å¿ƒ Unreal çš„å­¦ä¹ ä»£ç†è¿˜â€œä¸å¤Ÿå¥½â€ã€‚    æäº¤äºº    /u/Cuuuubee   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da7j72/unity_mlagents_vs_unreal_learning_agents/</guid>
      <pubDate>Fri, 07 Jun 2024 10:38:50 GMT</pubDate>
    </item>
    </channel>
</rss>