<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 14 Oct 2024 12:33:26 GMT</lastBuildDate>
    <item>
      <title>结果监督奖励模型和过程监督奖励模型有什么区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3drqr/whats_the_difference_between_outcomesupervised/</link>
      <description><![CDATA[在阅读 PPO 等 RL 算法的实现时，我发现优势和价值实际上是在 token 级别计算的。以下是如何在 OpenRLHF 中获得优势的实现（类似于 TRL） &quot;&quot;&quot;从奖励和价值计算优势和回报的函数。按照原始 PPO 论文中的计算方式计算：请注意，奖励可能包括 KL 散度损失项。优势如下所示： Adv1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... - V1 + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 返回如下所示： Ret1 = R1 + γ * λ * R2 + γ^2 * λ^2 * R3 + ... + γ * (1 - λ) V2 + γ^2 * λ * (1 - λ) V3 + ... 输入： - 值：形状为 (batch_size, response_size) 的张量 - 奖励：形状为 (batch_size, response_size) 的张量 输出： - 优势：形状为 (batch_size, response_size) 的张量 - 返回：形状为 (batch_size, response_size) 的张量 &quot;&quot;&quot;https://arxiv.org/abs/1707.06347  因此，尽管人们使用结果监督奖励模型 (ORM)，但他们实际上用它来获取 token 级反馈。从这个角度来看，PRM 和 ORM 似乎是相同的（因为它们都需要执行 token 级反馈）。我说得对吗？ 这是我的第一个问题。这是第二个问题：人们使用在结果反馈上训练的奖励模型来执行 token 级反馈是一种常见的做法吗？奖励/奖励模型似乎不准确/被误用。    提交人    /u/zetiansss   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3drqr/whats_the_difference_between_outcomesupervised/</guid>
      <pubDate>Mon, 14 Oct 2024 11:46:55 GMT</pubDate>
    </item>
    <item>
      <title>TorchRL 中针对 MARL 的动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3cjnw/action_masking_in_torchrl_for_marl/</link>
      <description><![CDATA[您好！我目前正在使用 TorchRL 解决我的 MARL 问题。我使用的是自定义 pettingzoo 环境和 pettingzoo 包装器。我的自定义环境的观察结果中包含一个动作掩码。在 TorchRL 中处理它的最简单方法是什么？因为我觉得 MultiAgentMLP 和 ProbabilisticActor 不能与动作掩码一起使用，对吗？ 谢谢！    提交人    /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3cjnw/action_masking_in_torchrl_for_marl/</guid>
      <pubDate>Mon, 14 Oct 2024 10:22:58 GMT</pubDate>
    </item>
    <item>
      <title>适合我的强化学习项目的 ubuntu/ROS2/Gazebo 版本</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3blyl/suitable_ubunturos2gazebo_versions_for_my/</link>
      <description><![CDATA[大家好，我将在 Gazebo 模拟器中对 epuck 模型机器人进行强化学习（我有一个来自 Gazebo Classic 的 urdf 模型，我必须适应新版本），我对 ros2 和 Gazebo 有基本的先验知识，但我想知道适合我的项目的版本，它是关于使用 RL 技术进行自主导航，我将非常感谢您的帮助。    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3blyl/suitable_ubunturos2gazebo_versions_for_my/</guid>
      <pubDate>Mon, 14 Oct 2024 09:12:31 GMT</pubDate>
    </item>
    <item>
      <title>RL 代理无法学习简单的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3aw2u/rl_agent_not_able_to_learn_for_a_simple_problem/</link>
      <description><![CDATA[大家好。 我对 RL 还很陌生，想为我创建的一款非常简单的游戏实现深度 Q 学习 RL 算法： 代理从 1 到 10 之间的某个随机整数坐标开始。目标是到达位置 5。代理可以在两个操作之间进行选择：向左移动 0.5 或向右移动 0.5。当代理到达位置 5 时，我会给他们 1 的奖励。如果游戏结束时他们没有到达 5，他们会得到 -1。在其他所有情况下，他们会得到 0。 我有一个简单的 DNN Q 函数逼近器。它有一个带有一个特征（当前位置）的输入层，后面紧接着是带有两个值的输出层，这两个值对应于每个动作的预期值。只有一层，所以它实际上是一个线性函数逼近器。除非我错过了什么，否则这应该足以解决这个问题（因为它实际上只是在学习当前位置是否 &gt; 5）。 问题是模型一直在从总是选择向左转到总是选择向右。它似乎并没有意识到低于 5 的位置应该与高于 5 的位置区别对待。此外，每个状态的动作（预测）的预期值彼此非常接近。它正在学习一些东西，但无法区分类别。 你认为游戏和我设置奖励 fn 或 DNN 的方式存在问题吗，或者我的代码可能有问题？    提交人    /u/IDidItMyWay_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3aw2u/rl_agent_not_able_to_learn_for_a_simple_problem/</guid>
      <pubDate>Mon, 14 Oct 2024 08:12:46 GMT</pubDate>
    </item>
    <item>
      <title>关于演员-评论家和在线学习中的策略梯度的困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g378u7/confusion_about_policy_gradient_in_actorcritic/</link>
      <description><![CDATA[      你好！我试图了解策略梯度是如何得出的，并在Actor-Critic 方法中使用，以及它如何与在线学习保持一致。 下图显示了 REINFORCE 和 Actor-Critic 的不同策略梯度公式： https://preview.redd.it/9i2nkp936nud1.png?width=1432&amp;format=png&amp;auto=webp&amp;s=8759231eeacc0d9a655807f87890345b800c02f4  在图片中的 Actor-Critic 方程中，期望中的和在最后一行消失了。然而，在其他材料中，我看到和仍然在期望中。如果我们从第一行推导出梯度（就像在 REINFORCE 中一样），那么总和似乎应该留在里面。我说得对吗？ 如果总和应该保留，那么 Actor-Critic 方法如何在在线学习中处理这个问题？我认为 Actor-Critic 方法可以在每一步（在线）更新策略，但我不确定在这些在线更新过程中如何处理所有步骤的总和。  如能对此作出任何澄清，我们将不胜感激！提前谢谢您。    提交人    /u/DRLC_   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g378u7/confusion_about_policy_gradient_in_actorcritic/</guid>
      <pubDate>Mon, 14 Oct 2024 03:35:36 GMT</pubDate>
    </item>
    <item>
      <title>不同的 RL 算法真的有很大影响吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</link>
      <description><![CDATA[我现在正在进行 RL 项目来解决组合优化问题，由于复杂的约束，这个问题很难用数学来表达。我正在使用 A2C 训练我的代理，这是最简单的入门方法。 我只是想知道其他算法（如 TRPO、PPO）在实践中是否真的效果更好，而不是像在基准测试中那样。 有没有人尝试过 SOTA 算法（论文中声称）并真的看到了差异？ 我觉得设计奖励比算法本身重要得多。    提交人    /u/Electronic_Estate854   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</guid>
      <pubDate>Mon, 14 Oct 2024 01:53:08 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND：世界建模的扩散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</link>
      <description><![CDATA[DIAMOND 💎 世界建模的扩散：Atari 中的视觉细节很重要 项目网页：https://diamond-wm.github.io/ 代码、代理和可玩世界模型：https://github.com/eloialonso/diamond 论文：https://arxiv.org/pdf/2405.12399 摘要  RL 代理是由 REINFORCE 训练的演员-评论家。  演员和评论家网络除最后一层外共享权重。这些共享层由一个卷积“主干”和一个 LSTM 单元组成。卷积主干有四个带有 2x2 最大池化的残差块。 每次训练运行需要 500 万帧，在一台 Nvidia RTX 4090 上持续 12 天。  世界模型是一个带有 U-Net 2D 的 2D 扩散模型。它不是潜在扩散模型。它直接从视频游戏中生成帧。 该模型将最后 4 帧和动作以及扩散噪声水平作为条件。 在 RTX 3090 上以 ~10 FPS 运行。 他们使用 EDM 采样器从扩散模型中采样，即使每帧只有 1 个扩散步骤，它仍然可以很好地训练 RL 代理。     由    /u/furrypony2718  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</guid>
      <pubDate>Mon, 14 Oct 2024 01:13:23 GMT</pubDate>
    </item>
    <item>
      <title>资源推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g30ici/resource_recommendation/</link>
      <description><![CDATA[嗨！我对 RL 还很陌生，对于我的课程项目，我希望在多智能体系统中做一些事情来监视和跟踪目标。假设已知环境，我想最大化群体覆盖的区域。 我真的想为此做一个好的可视化。我希望在任何类型的模拟器上运行它。 有人可以推荐任何类似的项目/资源来参考吗？    提交人    /u/whatsinthaname   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g30ici/resource_recommendation/</guid>
      <pubDate>Sun, 13 Oct 2024 21:37:05 GMT</pubDate>
    </item>
    <item>
      <title>为什么尽管使用了 RLZoo3 的最佳超参数，我的 SB3 DQN 代理仍无法学习 CartPole-v1？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2xpq9/why_is_my_sb3_dqn_agent_unable_to_learn/</link>
      <description><![CDATA[我从 [RLZoo3][1] 获得了用于训练 CartPole-v1 的最佳超参数。我创建了一个最小示例来展示我的 CartPole 代理的性能。根据官方 [文档][2]，代理应获得 500 分才能成功完成一集。不幸的是，分数没有超过 300。 这是我的代码 - https://pastecode.io/s/3j1c1xb0 这是最终结果 -  [![在此处输入图像描述][3]][3] 如果需要更多信息，请告诉我。非常感谢。 [1]: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml [2]: https://gymnasium.farama.org/environments/classic_control/cart_pole/ [3]: https://i.sstatic.net/wc00LfY8.png   由    /u/Academic-Rent7800  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2xpq9/why_is_my_sb3_dqn_agent_unable_to_learn/</guid>
      <pubDate>Sun, 13 Oct 2024 19:30:26 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家、探索问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2vcbm/actor_critic_exploration_issue/</link>
      <description><![CDATA[我正在训练一个简单的环境，其中输入是 144 个 1D 扁平数据（4x12 网格 x 3 个特征）。  我有两个问题：1. 如果我想要最优动作和替代动作，哪种探索更好？使用 E-greedy，状态的最优动作具有最高动作概率，而其他所有动作都接近 0。  如果我在 softmax 之前使用 Boltzman 温度进行平滑，则探索性不够。   128-64-32（三个隐藏层）的 nn 设置有效，但为什么 256-128-64 不起作用？或者只是一层的 128 个单位不起作用？或者任何其他配置。      提交人    /u/doctor_fhk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2vcbm/actor_critic_exploration_issue/</guid>
      <pubDate>Sun, 13 Oct 2024 17:47:09 GMT</pubDate>
    </item>
    <item>
      <title>Q 学习（及变体）评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2qqcg/q_learning_and_variations_evaluation/</link>
      <description><![CDATA[大家好， 我有一个非常基本/快速的问题，关于 RL 框架（特别是 Q 学习）的评估/测试。假设我们使用简单/经典的 Q 学习算法。我们让它训练一些情节来学习环境并构建 Q 表。那又怎么样？准备好 Q 表意味着训练后我们将遵循以下状态： action = np.argmax(q_table[state])  ？ 我的问题是：准备好 Q 表后，我们只需遵循它吗？我们构建了 Q 表然后我们遵循它，对吗？ 提前谢谢大家！！    提交人    /u/Interesting_Pea_4605   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2qqcg/q_learning_and_variations_evaluation/</guid>
      <pubDate>Sun, 13 Oct 2024 14:21:42 GMT</pubDate>
    </item>
    <item>
      <title>之前谁想过简单的 Actor-Critic Transformers 设置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2op1r/who_thought_about_simple_actorcritic_transformers/</link>
      <description><![CDATA[在上一篇文章中，我描述了这种廉价的前馈变压器（没有自我注意块）：https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/ 如果我们应用状态序列 S1、S2、S3 ... SN，并输出动作序列 A1、A2、A3 ... AN，称其为 Actor 将它们联合起来并应用于 [S1,A1]、[S2,A2]、[S3,A3] ... [SN,AN] 以创建 Q1、Q2 序列， Q3...QN。称其为评论家 我们可以通过它进行反向传播并改进动作序列吗？ 例如 N 步时间差异。 当然，这种设置将输给 Full Transformer，或者输给学习状态、动作和奖励（彼此平行）之间互连的决策变压器 但是这种设置可以被视为 2 个可以首先从推出中学习的长距离 LSTM。（如果我们有一些来自远程控制机器人的数据） 如果奖励函数得到适当调整，那么它可以在虚拟环境中得到潜在改进。相比之下，决策变压器的 TD 改进可能性较小。    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2op1r/who_thought_about_simple_actorcritic_transformers/</guid>
      <pubDate>Sun, 13 Oct 2024 12:37:09 GMT</pubDate>
    </item>
    <item>
      <title>如何通过控制和学习算法解决电动汽车充电问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2n5c2/how_to_solve_ev_charging_problem_by_control_and/</link>
      <description><![CDATA[下午好， 我计划实施文章中指定的 EV 充电算法：https://www.researchgate.net/publication/353031955_Learning-Based_Predictive_Control_via_Real-Time_Aggregate_Flexibility **问题描述** 我正在尝试思考如何实施这种基于控制和学习的算法。该算法解决了 EV 充电问题，确保 EV 充电成本最小化，同时满足基础设施约束（容量）和 EV 约束（满足所需的能源需求）。为了解决这个问题，我们需要实时协调聚合器和系统运营商。在每个时间步，系统操作员都会向聚合器提供可用电力。聚合器接收此电力并使用简单的调度算法（如 LLF）为电动汽车充电。聚合器向系统操作员发送（通过 RL 算法）学习到的最大熵反馈/灵活性（=电动汽车约束摘要），系统操作员据此选择下一个时间步的可用电力。这个循环重复到最后一个时间步（=直到一天结束）。 **RL 环境描述** 基本上，我们在时间步 t 的状态空间包含有关在时间步 t 连接到 EVSE 的每个电动汽车的信息（=剩余充电时间、剩余充电能量）。状态空间将是一个维度为 EVSE*2 + 1 的向量（也许包括时间步也是值得的）。 动作空间将是大小为 U 的概率向量（=灵活性）（其中 U 是不同的功率级）。根据这个概率向量，我们选择每个时间步的电动汽车充电功率水平（=基础设施容量）。 RL 算法将在每个充电日后终止。 **问题：**  学习是离线的究竟意味着什么？RL 代理是否具有有关系统未来成本和约束的信息？如果是，如何在离线学习期间为 RL 代理提供有关未来的信息，而无需扩大状态空间和动作空间（具有与文章中相似/相同的动作空间）？ 每个时间步的奖励函数包含所有时间步的充电决策（奖励函数中的第 3 项），但充电决策取决于给定动作生成的信号。基本上，奖励会考虑代理的未来行动，那么如何获得它们呢？如何设计用于在线测试的奖励函数？ 我们也可以在此问题中进行离线测试或在线训练/学习吗？  如何在我们的环境中为这个问题设计重置函数？我是否应该从给定的训练/测试数据集中随机选择不同的充电日并保持其他超参数不变？     提交人    /u/Superb-Carry6469   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2n5c2/how_to_solve_ev_charging_problem_by_control_and/</guid>
      <pubDate>Sun, 13 Oct 2024 11:00:46 GMT</pubDate>
    </item>
    <item>
      <title>解释 SB3 日志</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2jz2l/interpreting_sb3_logs/</link>
      <description><![CDATA[      我知道有几种来源试图解释 SB-3s PPO 的日志输出。但是，我只是没有完全掌握它。 从正常的 ML 中，我了解到损失是正的，应该近似于 0。但是，在我当前的 SB3 PPO 设置中，我的负损失接近于 0 这是正确的行为吗（即代理是否学习）？如果是，有没有人有好的资料可以解释这一点？ https://preview.redd.it/t32ed10g2hud1.png?width=467&amp;format=png&amp;auto=webp&amp;s=1e0c5ca156a173546aeb039456250247edd7657f    提交人    /u/luigi1603   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2jz2l/interpreting_sb3_logs/</guid>
      <pubDate>Sun, 13 Oct 2024 07:02:05 GMT</pubDate>
    </item>
    <item>
      <title>面向初学者的奖励函数发现简单教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</guid>
      <pubDate>Sat, 12 Oct 2024 13:28:33 GMT</pubDate>
    </item>
    </channel>
</rss>