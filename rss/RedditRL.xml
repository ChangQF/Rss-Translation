<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 19 Mar 2024 18:16:31 GMT</lastBuildDate>
    <item>
      <title>为多个代理寻找简单、对抗性、可扩展的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bip33c/looking_for_simple_adversary_scalable_environment/</link>
      <description><![CDATA[嗨， 我正在寻找允许以下操作的环境： 1. 控制多个代理的策略（是否不必同时支持多个代理训练）  相对简单 - 比国际象棋、围棋等简单 观察形状是相对较小 - 比 atari 游戏小 可以改变观察形状，最好是代理数量。   ​ 这种环境的一个例子是类似 petting-zoo connect-four/tic-tac-toe 但会允许控制代理数量或数字板大小（例如，使 tic tac toe 板 10x10） 我知道这可以通过编写自定义环境来完成，但我更愿意花更多时间来测试不同的算法以及代理数量和观察规模对其的影响。  这种环境的一个例子是类似 petting-zoo connect-four/tic-tac-toe 的东西，但允许控制代理的数量或数字板的大小（例如，制作 tic tac-toe 板） 10x10) 感谢您的所有建议！   由   提交/u/MrCogito_hs   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bip33c/looking_for_simple_adversary_scalable_environment/</guid>
      <pubDate>Tue, 19 Mar 2024 16:54:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DreamerV3 使用 actor-critic 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biky9x/why_dreamerv3_uses_the_actorcritic_models/</link>
      <description><![CDATA[       &amp;# 32；由   提交/u/yulinzxc   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biky9x/why_dreamerv3_uses_the_actorcritic_models/</guid>
      <pubDate>Tue, 19 Mar 2024 14:00:28 GMT</pubDate>
    </item>
    <item>
      <title>重新审视记忆幺半群的循环强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bii3f1/revisiting_recurrent_reinforcement_learning_with/</link>
      <description><![CDATA[ 由   提交/u/smorad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bii3f1/revisiting_recurrent_reinforcement_learning_with/</guid>
      <pubDate>Tue, 19 Mar 2024 11:35:17 GMT</pubDate>
    </item>
    <item>
      <title>“卡尔曼 -> 基于模型的强化学习”的文献建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biggav/literature_advice_for_kalman_modelbased_rl/</link>
      <description><![CDATA[你们中有人知道一本好书吗？它概括了从卡尔曼滤波器的信号处理、系统识别到基于模型的强化学习中的世界模型？    由   提交 /u/carlowilhelm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biggav/literature_advice_for_kalman_modelbased_rl/</guid>
      <pubDate>Tue, 19 Mar 2024 09:49:51 GMT</pubDate>
    </item>
    <item>
      <title>了解自定义 mujoco env 的gymnasium.make()</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bifl5f/understanding_gymnasiummake_for_custom_mujoco_envs/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bifl5f/understanding_gymnasiummake_for_custom_mujoco_envs/</guid>
      <pubDate>Tue, 19 Mar 2024 08:45:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习库 PyTorch</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bifiho/reinforcement_learning_library_pytorch/</link>
      <description><![CDATA[嗨， 我是强化学习领域的新手，目前正在寻找合适的库来实施我的深度强化学习研究项目。它应该是一个 Python 库，并使用 PyTorch 进行深度学习集成。我想在（多个）GPU 上并行运行多个环境。由于我的环境将包含神经网络，因此它应该能够直接在 GPU 上运行并使用张量。 有关合适库的任何经验。到目前为止，我发现了两个有趣的库  TorchRL TorchRL - torchrl 主要文档（pytorch.org）  自主学习库自主学习库 —autonomous-learning-library 0.9.1 文档  我想听听您的意见和经历。谢谢  &amp;# 32；由   提交/u/Opposite_Youth_442   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bifiho/reinforcement_learning_library_pytorch/</guid>
      <pubDate>Tue, 19 Mar 2024 08:39:56 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线中的自定义训练循环3？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bieqsf/custom_training_loop_in_stable_baselines3/</link>
      <description><![CDATA[我正在使用稳定的基线3，并且想知道如何实现以下情况。我想使用 PPO 代理： 我的环境中有 x 个可能的位置来执行操作。这里数字 x 是一个变量，取决于环境状态。在每个位置，都有一组可能的操作。 例如与我的环境类似的是跳棋游戏，棋子的数量会发生变化，但每个棋子的规则都是固定的。 我的环境具有周期性边界条件，这意味着棋盘上的某些东西会留在棋盘上左侧进入右侧，顶部留下的东西进入底部。  我想实现一个训练循环，在其中提供不同的视角，即对每个位置 x 进行一次观察，其中 x 居中。然后，我希望代理对操作概率最高的位置采取操作。有没有办法可以改变训练循环以获得这些概率，然后选择一个动作？因此，我必须在获取观察结果和选择操作之间调整代码。在某种程度上，我想对应该选择哪些操作/代理应该在哪个位置执行操作施加规则。对我来说特别困难的是位置 x 的数量各不相同 如果您对如何解决这个学习问题有其他想法，请告诉我！ :)   由   提交/u/ilse1301  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bieqsf/custom_training_loop_in_stable_baselines3/</guid>
      <pubDate>Tue, 19 Mar 2024 07:41:27 GMT</pubDate>
    </item>
    <item>
      <title>给对强化学习感兴趣的澳大利亚人的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bi9v8q/advice_for_an_australian_with_an_interest_in_rl/</link>
      <description><![CDATA[您好，我是澳大利亚人，目前在墨尔本 RMIT 攻读计算机科学学士学位。我对深度学习，更具体地说是强化学习非常感兴趣，因此我打算明年开始攻读人工智能硕士学位，然后再攻读深度学习博士学位。 我真的很想进入研究领域在一家大公司工作并尽可能地帮助挑战极限，但我不确定我在澳大利亚的工作/研究前景。此外，我不确定墨尔本大学莫纳什大学的博士学位是否会给我足够好的证书来实现我想要的目标。 我真的只是想知道这是否最适合我学习在这里并在这里找到研究，或者是否去海外更好，如果是的话在什么阶段（在博士学位之前或之后等）。任何建议将不胜感激。   由   提交 /u/TrueExcaliburGaming   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bi9v8q/advice_for_an_australian_with_an_interest_in_rl/</guid>
      <pubDate>Tue, 19 Mar 2024 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>RL 流量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bi5730/rl_in_trafic/</link>
      <description><![CDATA[嗨，我想在交通灯中添加 RL，所以我已经尝试过整洁，但在 2600 gen 之后我对结果不满意，还有其他吗我可以使用算法来创建该代理，或者任何人尝试将 RL 添加到交通灯，以及任何想法添加什么来创建多个代理环境，谢谢  &amp; #32；由   提交/u/Rude_Personne  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bi5730/rl_in_trafic/</guid>
      <pubDate>Mon, 18 Mar 2024 23:16:23 GMT</pubDate>
    </item>
    <item>
      <title>手术配额MARL调度程序开发</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/</link>
      <description><![CDATA[嗨，我来自一个大学生强化学习团队，我们将创建一个新的多智能体合作对抗环境来学习最优策略传入请求的短期调度（在我们的例子中是外科手术）。请看一下当前的环境描述。缺什么？也许有什么多余的东西？您与我们分享的任何经验将不胜感激！  这是 GitHub 存储库：https://github.com/artemisak/Surgery-配额调度程序/树/主   由   提交/u/Pythonic-af  /u/Pythonic-af reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhtqd2/surgery_quota_marl_scheduler_development/</guid>
      <pubDate>Mon, 18 Mar 2024 15:38:55 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家无法过度拟合？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhta0e/actorcritic_unable_to_overfit/</link>
      <description><![CDATA[我目前正在开展一个项目，我正在尝试优化演员评论家模型，以根据某些输入数据最大化高分。然而，我面临着一个意想不到的挑战 - 尽管我打算出于实验目的这样做，但我似乎无法让我的模型过度拟合。 以下是我的模型设置的简要概述：  p&gt;  我的演员损失是策略梯度损失 -log(概率)*其决策的优势。 优势 = (前一帧的 Δscore) - (评论家估计的 Δscore评论家根据（来自前一帧的Δscore）和（评论家根据前一帧估计的Δscore）之间的MSE更新评论家  当我将批评者从第一帧开始的估计 Δscore 与实际 Δscore 进行比较时，我观察到零相关性，这令人费解 我正在一个非常小的数据集上训练模型 - 只有 1 或 2 集每个大约有 30 个事件。我的期望是模型能够轻松记住与这些事件相关的操作，并完全适应数据。  我检查了更新步骤之前和之后的梯度，所有内容看起来都已填充​​，所以我认为这就是“学习”。我似乎看不到任何问题，但对为什么我不能过度适应感到有点困惑。有没有从业者看到这个问题/知道明显的解决办法或检查我在这里遗漏的东西？  我尝试对 Actor/Critic 使用 LSTM 和 Transformer 模型，其中有 2 层，每层大小约为 200，丢失率为 0.25，学习率为 1e-5。    由   提交 /u/Rhyno_Time   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhta0e/actorcritic_unable_to_overfit/</guid>
      <pubDate>Mon, 18 Mar 2024 15:19:30 GMT</pubDate>
    </item>
    <item>
      <title>DQN 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhoyx9/dqn_implementation/</link>
      <description><![CDATA[我想要一个在连续观察空间环境中实现 DQN 的示例，最好是在 SB3 中。   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhoyx9/dqn_implementation/</guid>
      <pubDate>Mon, 18 Mar 2024 11:59:22 GMT</pubDate>
    </item>
    <item>
      <title>剖析高更新率的深度强化学习：对抗价值高估和发散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhntvr/dissecting_deep_rl_with_high_update_ratios/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.05996 摘要：  我们证明深度强化学习可以保持其学习能力，而无需在梯度更新数量大大超过环境样本数量的设置中重置网络参数。在如此大的更新数据比率下，Nikishin 等人最近的一项研究。 （2022）提出了一种首要偏见，即代理过度适应早期的互动并淡化后来的经验，从而损害了他们的学习能力。在这项工作中，我们剖析了首要偏见背后的现象。我们检查了训练的早期阶段可能导致学习失败的原因，发现一个根本的挑战是一个长期存在的认识：价值高估。过度夸大的 Q 值不仅出现在分布外的数据上，而且还出现在分布内的数据上，并且可以追溯到由优化器动量推动的看不见的动作预测。我们采用简单的单位球归一化，可以在大更新率下进行学习，在广泛使用的 dm_control 套件上展示其功效，并在具有挑战性的狗任务上获得强大的性能，与基于模型的方法相竞争。我们的结果部分地质疑了由于早期数据过度拟合而导致的次优学习的先前解释。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhntvr/dissecting_deep_rl_with_high_update_ratios/</guid>
      <pubDate>Mon, 18 Mar 2024 10:52:00 GMT</pubDate>
    </item>
    <item>
      <title>与 CleanRL 类似的目标条件强化学习实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bhm9bu/goalconditioned_rl_implementations_similar_to/</link>
      <description><![CDATA[大家好！ 我目前正在开发一个目标条件机器人项目（带有自定义环境）并面临一些挑战。我之前使用过 Stable Baselines3 (SB3)，我发现它相当全面，但对于我当前的需求来说有点复杂。  我希望创建一个自定义策略（模块化）并将迁移学习应用于不同的任务，这对于 SB3 来说似乎要复杂得多。因此，我对类似于 CleanRL 的实现特别感兴趣，因为它简单明了，但支持目标条件环境。  您能否推荐任何可能符合这些要求的资源、存储库或框架？ 以下是我正在寻找的内容的简要概述：  像 CleanRL 一样简单透明的框架或库。 支持目标条件环境（与 OpenAI Gym 的 GoalEnv 兼容）。 灵活地自定义策略网络，包括为网络的不同部分设置不同的学习率。   谢谢   由   提交 /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bhm9bu/goalconditioned_rl_implementations_similar_to/</guid>
      <pubDate>Mon, 18 Mar 2024 09:05:03 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>