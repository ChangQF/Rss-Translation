<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 09 Feb 2024 09:14:05 GMT</lastBuildDate>
    <item>
      <title>RL新手，但非常感兴趣</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amazbb/rl_newbie_but_extremely_interested/</link>
      <description><![CDATA[背景：我对人工智能着迷已经有一段时间了，自从我完成了我的batcelor之后，我就取得了一个拥有相当多人工智能的硕士学位选项，经过监督学习，非常喜欢它，我达到了强化学习课程的地步，我非常喜欢它，我发现自己在提前学习，并在网上寻找越来越多的问题我已经经历过：qlearning、dqn、double dqn、dueling dqn、rainbow dqn、DGP、DDGP、PPO、A2C、A3C，在我的列表中，我有分层强化对于他们中的大多数人，我了解这一切是如何运作的，虽然我不得不承认我对诸如决斗 DQN 之类的事情仍然有点模糊（我明白什么是不同的，以及为什么这样做，但对我来说感觉不自然）这是否给了我足够的基础来做一个对于我的课程来说，这个项目足够好？（老师提到，如果它足够好，我们可以把它变成一篇研究论文或其他东西，我有点计划在 RL 上完成我的硕士论文） 我想我所有的问题都变成了to：现在这些够了吗？我还应该探索哪些其他领域，以及什么是“足够好”？   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amazbb/rl_newbie_but_extremely_interested/</guid>
      <pubDate>Fri, 09 Feb 2024 00:33:12 GMT</pubDate>
    </item>
    <item>
      <title>PC组件的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amasij/pc_component_thoughts/</link>
      <description><![CDATA[所以我想为 ML 构建一台电脑，而且我真的对 RL 更感兴趣 我得到了一个 7900gre 所以 gpu 是尽管我知道 NVidia 在这个市场上表现出色，但我有点希望 AMD 能够加强 ROCm 或他们拥有的其他一些开放魔法，这样这款 GPU 至少是不错的（如果我可以与任何 NVidia GPU 进行比较，它会太棒了，这样我就知道我对它的立场了，比如它比 3060 更好吗？考虑到所有因素） 接下来是 ram 和 CPU，我想知道 cpu 数量、线程数量、cpu 是多少时钟，缓存大小对 RL 很重要，我正在研究 7600x（如果它真的不重要）/ 7700（如果它有点重要），或者 7900x（如果它真的很重要），也正在研究 32 GB 内存或 64 GB，如果这真的很重要（速度重要吗？） 我知道这篇文章到处都是，但我不知道如何构建它，我有很多问题，总体来说 cpu/ram 影响有多大ML/RL 应用？   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amasij/pc_component_thoughts/</guid>
      <pubDate>Fri, 09 Feb 2024 00:24:23 GMT</pubDate>
    </item>
    <item>
      <title>神经常微分方程的替代方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alyyqq/alternate_of_neural_ode/</link>
      <description><![CDATA[https://arxiv.org/abs/2401.01836 我看到这篇论文，其中 NODEC 是为了实现未知动力系统的最优控制而实现的，我想知道我们还可以使用哪些其他方法来解决类似的问题。我知道强化学习，但我正在寻找更有效的数据。表征学习或模仿学习可能吗？或者有其他方法可以改善结果？   由   提交/u/Striking-Cricket788  /u/Striking-Cricket788  reddit.com/r/reinforcementlearning/comments/1alyyqq/alternate_of_neural_ode/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alyyqq/alternate_of_neural_ode/</guid>
      <pubDate>Thu, 08 Feb 2024 15:59:57 GMT</pubDate>
    </item>
    <item>
      <title>有哪些必须知道的算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alsnyj/what_are_the_mustknow_algorithms/</link>
      <description><![CDATA[我 3 个月前开始攻读 RL/IL 博士学位，我想知道哪些是必须知道的算法，以免因为我缺乏知识而受到限制知识。当然，我可以全部学习，但是太多了，我不确定这是否有用。  目前我已经了解了Q学习、SARSA、DQN和PPO的机制和方程。接下来您有什么建议？   由   提交/u/Ybrik410  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alsnyj/what_are_the_mustknow_algorithms/</guid>
      <pubDate>Thu, 08 Feb 2024 10:24:21 GMT</pubDate>
    </item>
    <item>
      <title>我创建了很棒的持续强化学习存储库！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alrprr/i_created_awesome_continual_rl_repository/</link>
      <description><![CDATA[您好，我是一名研究生，对连续强化学习领域感兴趣。 我想获取有关各种信息关于这个领域的论文，所以我创建了一个很棒的存储库。 请随时给我任何建议！！ ​ https://github.com/windust7/awesome-continual-reinforcment-learning &lt; !-- SC_ON --&gt;  由   提交 /u/Mission-Lawyer1787    reddit.com/r/reinforcementlearning/comments/1alrprr/i_created_awesome_continual_rl_repository/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alrprr/i_created_awesome_continual_rl_repository/</guid>
      <pubDate>Thu, 08 Feb 2024 09:15:47 GMT</pubDate>
    </item>
    <item>
      <title>[DQN] 我在这个问题上完全迷失了方向，正在寻求可能出现问题的指导。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alklc4/dqn_i_am_hopelessly_lost_on_this_issue_seeking/</link>
      <description><![CDATA[我一直在尝试创建一个 DQN 来解决井字游戏。我取得了成功，因此我尝试通过添加优先体验重播来改进它。 在获得实际 PER 之前，我只想开始自己进行随机采样。之前，我让 TensorFlow 通过以下代码进行采样： history = self.brain.fit(state_arr, qValueEstimates, batch_size=self.trainingBatchSize, verbose=0)&lt; /p&gt; 为了清楚起见：  state_arr 是 MxS，其中 S 是一维形状数组的大小，M 是体验回放的大小。 qValueEstimates 是 MxA，其中 A 是 1D 操作数组的大小。  ​ 当我将代码更改为： population = range(M)  sampleIndices = random.sample(population, self.trainingBatchSize)  &lt; code&gt;history = self.brain.fit(state_arr[sampleIndices], qValueEstimates[sampleIndices], batch_size=self.trainingBatchSize, verbose=0) 此命令运行，但算法不再学习。 python库不够随机吗？我不知道这里可能存在什么问题。   由   提交 /u/Garjiglio   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alklc4/dqn_i_am_hopelessly_lost_on_this_issue_seeking/</guid>
      <pubDate>Thu, 08 Feb 2024 02:12:05 GMT</pubDate>
    </item>
    <item>
      <title>高效的零动态网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aljipl/efficient_zero_dynamics_network/</link>
      <description><![CDATA[我正在研究高效的零实现，我注意到他们的动态网络通过转换层将输入压缩一个通道 然而，他们然后在残差/身份操作中添加原始 x，但没有最后一个通道 我想知道这是为什么？ 编辑：进一步挖掘，似乎他们将操作附加到状态，因此他们删除了 cnn 之前的操作以获得身份状态值   由   提交/u/proturtle46  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aljipl/efficient_zero_dynamics_network/</guid>
      <pubDate>Thu, 08 Feb 2024 01:20:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 A2C 训练的纸牌游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1al84n0/cards_game_trained_using_a2c/</link>
      <description><![CDATA[查看我如何使用 Tensorflow 和 Openai Gym 使用 A2C 算法训练纸牌游戏代理。 https://youtu.be/Odaa9T6PxkQ?si=qned3bP2n60eBGma   由   提交 /u/mehulgupta7991   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1al84n0/cards_game_trained_using_a2c/</guid>
      <pubDate>Wed, 07 Feb 2024 17:17:48 GMT</pubDate>
    </item>
    <item>
      <title>MA 环境中的独立 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1al6i7x/independent_dqn_in_ma_environments/</link>
      <description><![CDATA[假设我们使用 3x3 棋盘玩经典的井字游戏。简单地使用 DQN 输入大小 18（每个代理的串联 one-hot 编码向量）和输出大小 9（动作空间）是否就足够了？  问题 1：状态是否应该仅在给定代理的回合中观察到：X 是否应该每隔一轮才添加到其重播缓冲区？ 问题 2：重播缓冲区应该如何设置结构化的？基于 DQN 架构，重播缓冲区似乎应包含 (s、a、s&#39;、r、terminal) 形式的元素，其中 s 是长度为 18 的向量。这是正确的方法吗？还是应该重播buffer 包含历史记录：(h_t, a_t, h_t+1, r_t+1,terminal)，其中 h_t 是状态序列：(s0, s1, ..., st) [此处也与 Q1 相关]。如果重播缓冲区应该是 (h_t, a_t, h_t+1, r_t+1,terminal)，那么应该如何构造 DQN 来接受像 h_t 这样可变大小的输入？   由   提交/u/Top_Method_4623  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1al6i7x/independent_dqn_in_ma_environments/</guid>
      <pubDate>Wed, 07 Feb 2024 16:11:32 GMT</pubDate>
    </item>
    <item>
      <title>用于离散、未知动态设置的控制技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akz6eq/control_techniques_for_discrete_unknowndynamics/</link>
      <description><![CDATA[我正在研究交通信号控制问题，我想尝试除常见的 RL 算法之外的其他类型的控制算法。 这个问题很困难，因为我们没有系统模型：  进入车辆的数量是随机的，它取决于一天中的时间和特定场景 出站车辆的数量也是随机的：当我们为某个通道提供绿灯时，通过的车辆数量取决于有多少车辆已经在等待（可测量）其他车辆的当前位置和速度。另一方面，状态空间和动作空间很小且离散，因此可以通过广泛的模型系列来处理它们/  除了常见的强化学习算法之外，您能否建议一些其他方法来解决这个问题？我对任何非强化学习解决方案持开放态度，但也对混合解决方案持开放态度。例如，我想尝试学习系统的监督模型并应用 MPC 或用它进行规划。我还可以尝试其他有趣的事情吗？   由   提交 /u/fedetask   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akz6eq/control_techniques_for_discrete_unknowndynamics/</guid>
      <pubDate>Wed, 07 Feb 2024 09:32:10 GMT</pubDate>
    </item>
    <item>
      <title>Deep Q网络的计算复杂度分析</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akfg53/computational_complexity_analysis_of_deep_q/</link>
      <description><![CDATA[大家好， 是否有任何库可以计算训练 Deep Q 网络的计算复杂度？ &lt; p&gt;1）虽然使用品质因数（例如所花费的时间）来生成情节（例如情节与所花费的时间）是可以的。还可以使用哪些其他品质因数？ 2）DQN 的计算复杂度可以与标准前馈网络的计算复杂度相比较吗？  目前我的实现是基于 PyTorch 的。我期待任何想法。  谢谢。   由   提交/u/Putrid_Drummer_2870   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akfg53/computational_complexity_analysis_of_deep_q/</guid>
      <pubDate>Tue, 06 Feb 2024 17:37:31 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习训练 AI 相扑机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akeohu/training_an_ai_sumo_robot_with_reinforcement/</link>
      <description><![CDATA[   /u/ungluffy  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akeohu/training_an_ai_sumo_robot_with_reinforcement/</guid>
      <pubDate>Tue, 06 Feb 2024 17:05:20 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 用于非视觉控制任务？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akcyjs/dreamerv3_for_nonvisual_control_tasks/</link>
      <description><![CDATA[tl;dr：Dreamer 是否是非视觉控制任务的适当选择？如果是，世界模型的估计会如何变化？  目前，我正在开展一项比较研究，将多种 RL 算法应用于非视觉问题（例如 DSGE 模型、基于代理的模型……）。 所以到目前为止，我只应用了无模型算法，并且我想在我的分析中至少包含一种基于模型的算法。 Dreamer 似乎是迄今为止最先进的算法之一。 我的问题有两个：  如果我有基于矢量的观察（所以没有像素，但有几个连续变量），我的序列模型的输入是什么/我的 z 是什么？只是矢量格式的观察结果？  dreamer 是否是适合此类任务的算法，或者是否有更好的拟合算法？在这种情况下，梦想家就像是用大锤敲碎坚果吗？    由   提交 /u/Tortoise_vs_Hare   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akcyjs/dreamerv3_for_nonvisual_control_tasks/</guid>
      <pubDate>Tue, 06 Feb 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>[DQN] 我听说过 DQN 中的灾难性遗忘。可能是这样，还是由于超参数或体验重放缓冲区造成的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ak99fw/dqn_i_have_heard_about_catastrophic_forgetting_in/</link>
      <description><![CDATA[   嗨！ 我的 DQN 算法不断输出类似的结果，其中平均奖励随着情节的推移缓慢增加，但在某些时候它开始波动很大。似乎价值从高值（可能是最大奖励）跳到接近于零，然后回到最大奖励。这几乎就像忘记了然后重新拾起并获得高额回报。这可能是灾难性的遗忘，还是与经验重放等有关？ DQN 用于股票交易，其中操作被映射到买入/卖出（操作空间=2）。夏普比率表示代理的盈利能力，在收敛之前会很好地增加。奖励功能是每天的利润。数据集由 1759 个训练期数据集和 504 个测试期数据集组成。正如我所描述的，平均奖励似乎只是波动。其他超参数有： # DQN 算法相关的默认参数gamma = 0.8learningRate = 0.0001targetNetworkUpdate = 1000learningUpdatePeriod = 1# Experience Replay 机制相关的默认参数capacity = 10000batchSize = 64experiencesRequired = 1000#深度神经网络numberOfNeurons_h1 = 32numberOfNeurons_h2 = 64numberOfNeurons_h3 = 128dropout = 0.2# 与Epsilon-Greedy探索技术相关的默认参数epsilonStart = 1.0epsilonEnd = 0.01epsilonDecay = 10000# 关于粘性动作的默认参数RL泛化技术alpha = 0.1# 与预处理filterOr相关的默认参数德尔 = 5 # 与梯度和RL裁剪相关的默认参数rewardsgradientClipping = 1rewardClipping = 1# 与L2正则化相关的默认参数L2Factor = 0.000001 ​ https://preview.redd.it/9fh06f7qqygc1.png?width=640&amp; ;format=png&amp;auto=webp&amp;s=34e0c90642f1db8124169b97f032a2a131377a93 https://preview.redd.it/nz0x7e7qqygc1.png?width=640&amp;format=png&amp;auto=webp&amp;s=f4f67b621d1b42a8ae24c245fc8b7c 1f3ef05e7c 这个是剧集长度较短的结果，可能具有稍微不同的超参数，但会导致平均奖励的类似波动。 ​ https://preview.redd.it/6rpqberzqygc1.png?width=640&amp;format=png&amp;auto= webp&amp;s=e178a8772f284094990e8babc8bf97c3cd223538 有谁知道这个问题是否可以避免，如果可以，如何避免？非常感谢！   由   提交/u/sakkee99  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ak99fw/dqn_i_have_heard_about_catastrophic_forgetting_in/</guid>
      <pubDate>Tue, 06 Feb 2024 13:03:40 GMT</pubDate>
    </item>
    <item>
      <title>您对稳定基线 3 有何看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ak893e/what_is_your_opinion_regarding_stable_baselines_3/</link>
      <description><![CDATA[Stable Baselines 3 是 PyTorch 中强化学习算法的一组可靠实现。  一开始我直接使用 pytorch/tensorflow 并尝试实现不同的模型，但这导致了大量的超参数调整。我发现稳定基线是创建完成任务的代理的一种更快的方法，但我没有看到这个子经常提到它。  使用 SB3 之类的东西有缺点吗？有更好的工具/框架吗？针对不同问题创建代理的一般方法是什么？   由   提交 /u/IntroDucktory_Clause   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ak893e/what_is_your_opinion_regarding_stable_baselines_3/</guid>
      <pubDate>Tue, 06 Feb 2024 12:07:31 GMT</pubDate>
    </item>
    </channel>
</rss>