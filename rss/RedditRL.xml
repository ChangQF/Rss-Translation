<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 17 Oct 2024 21:17:13 GMT</lastBuildDate>
    <item>
      <title>“MLE-bench：在机器学习工程中评估机器学习代理”，Chan 等人 2024 {OA}（Kaggle 扩展）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5yr0p/mlebench_evaluating_machine_learning_agents_on/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5yr0p/mlebench_evaluating_machine_learning_agents_on/</guid>
      <pubDate>Thu, 17 Oct 2024 19:09:33 GMT</pubDate>
    </item>
    <item>
      <title>RL 用于系统的最优控制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5wecg/rl_for_optimal_control_of_systems/</link>
      <description><![CDATA[我最近看到了这篇 IEEE 论文，标题为“基于强化学习的使用 Carleman 线性化的非线性系统近似最优控制”。看起来他们在非线性系统的近似上使用某种形式的强化控制，并且与线性 RL 相比表现出良好的性能。 有人对这种 Carleman 近似方法有什么见解吗？    提交人    /u/Playful_Passage_2985   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5wecg/rl_for_optimal_control_of_systems/</guid>
      <pubDate>Thu, 17 Oct 2024 17:29:13 GMT</pubDate>
    </item>
    <item>
      <title>ADAS 的 RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5r81b/rl_implementation_for_adas/</link>
      <description><![CDATA[嘿。我想探索使用 RL 模型（本质上是一种基于奖励的模型）开发 ADAS 功能（如 FCW 或 ACC）的可能性，在这些功能中，将发出警告，并根据车辆采取的行动给予奖励。我希望有人能指导我如何做到这一点？我想使用 CARLA 来构建我的环境。     提交人    /u/SignificanceMotor285   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5r81b/rl_implementation_for_adas/</guid>
      <pubDate>Thu, 17 Oct 2024 13:44:54 GMT</pubDate>
    </item>
    <item>
      <title>从 DQN 到 Double DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5p8mi/from_dqn_to_double_dqn/</link>
      <description><![CDATA[我已经有一个 DQN 的实现。要将其更改为双 DQN，似乎只需要进行一点小改动：在 Q 值更新中，下一个状态（最佳）动作选择和该动作的评估均由 DQN 中的目标网络完成。而在双 DQN 中，下一个状态（最佳）动作选择由主网络完成，但该动作的评估由目标网络完成。 这似乎相当简单。我还遗漏了什么吗？    提交人    /u/No_Addition5961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5p8mi/from_dqn_to_double_dqn/</guid>
      <pubDate>Thu, 17 Oct 2024 12:02:35 GMT</pubDate>
    </item>
    <item>
      <title>何时使用强化学习，何时不使用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5mge9/when_to_use_reinforcement_learning_and_when_to/</link>
      <description><![CDATA[何时使用强化学习，何时不使用。我的意思是何时使用普通数据集来训练模型，何时使用强化学习     提交人    /u/Alarming-Power-813   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5mge9/when_to_use_reinforcement_learning_and_when_to/</guid>
      <pubDate>Thu, 17 Oct 2024 08:54:47 GMT</pubDate>
    </item>
    <item>
      <title>Chi Jin 的普林斯顿 RL 课程好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5leg1/is_chi_jins_princeton_rl_course_good/</link>
      <description><![CDATA[普林斯顿大学 ECE524 强化学习基础课程，2024 年春季。  本课程为研究生课程，重点介绍强化学习的理论基础。它涵盖马尔可夫决策过程 (MDP) 的基础知识、基于动态规划的算法、规划、探索、信息理论下界以及如何利用离线数据。还讨论了各种高级主题，包括策略优化、函数逼近、多机构和部分可观测性。本课程特别强调算法及其理论分析。需要具备线性代数、概率和统计学方面的先验知识。    提交人    /u/sahoosubramanyam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5leg1/is_chi_jins_princeton_rl_course_good/</guid>
      <pubDate>Thu, 17 Oct 2024 07:30:33 GMT</pubDate>
    </item>
    <item>
      <title>使用多智能体 RL 代理优化分布式系统中的工作平衡/通信</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5avgf/using_multiagent_rl_agents_for_optimizing_work/</link>
      <description><![CDATA[我偶然发现了这篇名为 &quot;负载平衡并行粒子追踪的强化学习&quot; 的论文，它让我绞尽脑汁。他们使用多智能体 RL 在分布式系统中实现负载平衡，但我不确定这是否可行。 以下是本文的要点：  他们使用多智能体 RL 来平衡工作负载并优化并行粒子追踪中的通信 每个进程（最多 16,384 个！）都有自己的 RL 代理（用于其策略网络的单层感知器） 代理的操作是在进程之间移动工作块以平衡事物  我听说多智能体 RL 很难正常工作？有了这么多进程，由于每个代理都可能决定将工作转移到数千个其他进程中的任何一个，因此操作空间不是非常巨大吗？ 所以，我的问题是：这真的可行吗？或者，动作空间太大，无法在实践中发挥作用？我很想听听任何有 RL 或并行计算经验的人的意见。我是否遗漏了什么，或者这听起来很疯狂？ 谢谢！P.S. 如果有人真的尝试过这样的事情，我会非常感兴趣听听它进展如何！    提交人    /u/ypsoh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5avgf/using_multiagent_rl_agents_for_optimizing_work/</guid>
      <pubDate>Wed, 16 Oct 2024 21:39:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么我无法使用`sbx`来为我的`DQN`程序提供种子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g599hv/why_am_i_unable_to_seed_my_dqn_program_usingsbx/</link>
      <description><![CDATA[我尝试在使用 `sbx` 时为我的 DQN 程序播种，但由于某种原因，我不断得到不同的结果。 这里尝试创建一个最小的可重现示例 - https://pastecode.io/s/nab6n3ib 结果相当令人惊讶。在多次运行此程序时，我得到了各种各样的结果。 这是我的结果 - 尝试 1： ``` 运行 = 0 使用种子：1 运行 = 1 使用种子：1 运行 = 2 使用种子：1 mean_rewards = [120.52, 120.52, 120.52] ``` 尝试 2： ``` 运行 = 0 使用种子：1 运行 = 1 使用种子：1 运行 = 2 使用种子：1 mean_rewards = [116.64, 116.64, 116.64] ``` 令人惊讶的是，在尝试中，我得到了相同的结果。但是当我再次运行该程序时，我得到了不同的结果。 我查看了从[此处][1]播种环境的文档，还阅读了以下内容 - “*无法保证在 PyTorch 版本或不同平台上完全可重现的结果。此外，即使使用相同的种子，结果也不必在 CPU 和 GPU 执行之间重现。”。但是，我想确保我这边没有错误。另外，我使用的是“sbx”而不是“stable-baselines3”。也许这是一个 `JAX` 问题？ 我还在 此处创建了一个 S.O 帖子 [1]: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html#reproducibility    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g599hv/why_am_i_unable_to_seed_my_dqn_program_usingsbx/</guid>
      <pubDate>Wed, 16 Oct 2024 20:30:52 GMT</pubDate>
    </item>
    <item>
      <title>帮助找到培训 Pool9 代理的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g553g6/help_to_find_a_way_to_train_pool9_agent/</link>
      <description><![CDATA[      嗨！ 我正在开发一个玩 Pool9 的 Agent 做出决定：在击球前，当所有球都处于静止位置时，会做出击球方向和力量的决定 观察： 1. 我首先放置球和球袋的标准化坐标 + 哪个球是目标的标志 2. 然后我开启使用方向和标准化到球的距离 3. 然后我添加了课程，它经过多次改进，最后的计划是 课程 0：学习触摸目标球 3 个球 随机目标 球的随机初始放置 触摸目标的奖励 课程 1：学习在触摸目标球后接住任何球 6 个球 随机目标 球的随机初始放置 触摸目标的奖励 + 接住任何球 非法击球的惩罚（目标球没有被触碰） 课程 2：游戏 9 个球 静态初始位置 目标编号 -有序 训练器：ppo 2-4 层 128-512 结果几乎相同，训练速度的差异， 但似乎代理无法预测轨迹：（ 有什么想法或建议吗？我将不胜感激 第 1 课从未达到 https://preview.redd.it/yu2wzujpl5vd1.png?width=3246&amp;format=png&amp;auto=webp&amp;s=5a783f500c84a05f2a80eba6286f3922db6e3fc6 https://reddit.com/link/1g553g6/video/vmkiuz9zl5vd1/player    由   提交  /u/Ecstatic-Ring3057   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g553g6/help_to_find_a_way_to_train_pool9_agent/</guid>
      <pubDate>Wed, 16 Oct 2024 17:33:00 GMT</pubDate>
    </item>
    <item>
      <title>Unity ML 代理和贪吃蛇等游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4xhqd/unity_ml_agents_and_games_like_snake/</link>
      <description><![CDATA[大家好， 我一直在尝试理解神经网络和游戏 AI 的训练。但我目前在努力玩 Snake。我想“好吧，让我们给它一些射线传感器、一个摄像头传感器，吃食物时给予奖励，与自身或墙壁碰撞时给予负面奖励”。 我想说它学得很好，但并不完美！在 10x10 的游戏场中，它的最高分约为 50，但到目前为止它从未掌握游戏。 有人能给我一些建议或线索，如何更好地处理使用 PPO 进行蛇 AI 训练吗？ 射线传感器可以检测墙壁、蛇本身和食物（3 个不同的传感器，每个传感器有 16 条射线） 摄像头传感器的分辨率为 50x50，也可以看到墙壁、蛇头以及蛇周围的蛇尾。它是一个尺寸为 8 的正交相机，因此它可以看到整个运动场。 首先，我只使用射线传感器进行测试，然后我添加了相机传感器，我可以说的是，它通过相机视觉观察学习得更快，但最后它的最高分大约相同。 我正在并行训练 10 个代理。 网络设置为： 50x50x1 视觉观察输入 大约 100 个射线观察输入 512 个隐藏神经元 2 个隐藏层 4 个离散输出动作 我目前正在尝试使用 buffer_size 为 25000 和 batch_size 为 2500。学习率为 0.0003，Num Epoch 为 3。时间范围设置为 250。 是否有人使用过 Unity 的 ML Agents Toolkit 并能帮助我一点？ 我做错了什么吗？ 我将感谢你们给予我的每一次帮助！ 这里有一个小视频，你可以在其中看到大约第 150 万步的培训： https://streamable.com/tecde6    提交人    /u/Seismoforg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4xhqd/unity_ml_agents_and_games_like_snake/</guid>
      <pubDate>Wed, 16 Oct 2024 11:51:50 GMT</pubDate>
    </item>
    <item>
      <title>修改策略迭代？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4ukf4/modified_policy_iteration/</link>
      <description><![CDATA[我是强化学习的新手，仍在学习中。我现在正在学习策略迭代和值迭代。 因此，据我所知，在策略迭代中，我们首先通过获取所有状态的状态值函数来评估当前策略，然后使用它们进行贪婪操作更新策略，然后通过再次获取所有状态的状态值函数来评估更新后的策略，并对其进行迭代，直到获得最佳策略。 我阅读了关于修改后的策略迭代的文章，对此我的看法不一。我现在可以看到两种方法：  修改后的策略迭代就是策略迭代，只不过我们只进行 k 次迭代？ 我们只评估部分状态？  我之所以问这个问题，是因为从我读到的内容来看，第一种方法似乎是正确的，但是我在使用的书中看到的图表和其他人的解释（他现在也是第一次学习 RL）表明它是第二种方法。    提交人    /u/AdBitter9336   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4ukf4/modified_policy_iteration/</guid>
      <pubDate>Wed, 16 Oct 2024 08:31:37 GMT</pubDate>
    </item>
    <item>
      <title>如何应对SAC的灾难性遗忘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4tklf/how_to_deal_with_the_catastrophic_forgetting_of/</link>
      <description><![CDATA[      嗨！ 我建立了一个使用SAC进行训练的自定义任务。成功率曲线在稳步上升后逐渐下降。在查阅了一些相关讨论后，我发现这种现象可能是灾难性的遗忘。 https://preview.redd.it/i5bxwet9j2vd1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=81ea533917317b57ebd924668f24fdd59e275c43 我尝试规范奖励并自动调整 alpha 的值来控制探索和利用之间的平衡。其次，我还降低了 actor 和 critic 的学习率，但这只会减慢学习过程并降低整体成功率。 我想得到一些关于如何进一步稳定这个训练过程的建议。 提前感谢您的时间和帮助！    提交人    /u/UpperSearch4172   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4tklf/how_to_deal_with_the_catastrophic_forgetting_of/</guid>
      <pubDate>Wed, 16 Oct 2024 07:13:25 GMT</pubDate>
    </item>
    <item>
      <title>什么可能导致我的 Q-Loss 值出现分歧（SAC + Godot <-> Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4t57g/what_could_be_causing_my_qloss_values_to_diverge/</link>
      <description><![CDATA[TLDR; 我正在开发一个使用 SAC 的 PyTorch 项目，类似于我的一个旧 Tensorflow 项目：https://www.youtube.com/watch?v=Jg7\_PM-q\_Bk。我无法让它与 PyTorch 一起工作，因为我的 Q-Loses 和 Policy 损失要么增长，要么收敛到 0 太快。你知道为什么会这样吗？  我已经在 Godot 中创建了一个游戏，通过套接字与 SAC 的 PyTorch 实现进行通信：https://github.com/philipjball/SAC_PyTorch 游戏是： 代理需要靠近目标，但它没有自己的位置或目标位置作为输入，而是有 6 个输入，表示目标与代理在特定角度的距离。始终有且只有 1 个输入的值不为 1。 代理输出 2 个值：移动的方向和沿该方向移动的幅度。 输入在 [0,1] 范围内（由最大距离标准化），2 个输出在 [-1,1] 范围内。 奖励为： score = -distance if score &gt;= -300: score = (300 - abs(score )) * 3 score = (score / 650.0) * 2 # 650 是最大距离，100 是每步的最大范围 return score * abs(score )  问题是： 两个评论家和策略的 Q-Loss 都在随着时间的推移缓慢增长。我尝试了几种不同的网络拓扑，但层数或每层中的节点数似乎对 Q-Loss 没有影响 我能做的最好的就是让奖励非常小，但这会导致 Q-Loss 和 Policy loss 收敛到 0，即使代理没有学到任何东西。 如果您做到了这一点，并且有兴趣提供帮助，我很乐意向您支付导师的费用，通过屏幕共享电话审查我的方法，并帮助我更好地了解如何让 SAC 代理工作。 提前谢谢您！！    提交人    /u/stokaty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4t57g/what_could_be_causing_my_qloss_values_to_diverge/</guid>
      <pubDate>Wed, 16 Oct 2024 06:41:22 GMT</pubDate>
    </item>
    <item>
      <title>我使用深度强化学习（使用 Unity ML Agents）制作了一个消防员 AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4iy4q/i_made_a_firefighter_ai_using_deep_rl_using_unity/</link>
      <description><![CDATA[视频链接：https://www.youtube.com/watch?v=REYx9UznOG4 我之前做过这个视频，花了好几个小时才制作出来，却没有人关注，这让我很沮丧，所以我现在在攻读人工智能博士学位，而不是成为一名 YouTuber，哈哈。 我想如果人们觉得它很有趣，现在为它做广告也不错。我确保添加了一些旁白和有趣的部分，这样它就不会无聊了。我希望这里的一些人能觉得它和我做这个项目一样有趣。 我对这个主题很感兴趣，所以如果有人有问题，我会在有时间的时候回答他们:D   由    /u/usernumero  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4iy4q/i_made_a_firefighter_ai_using_deep_rl_using_unity/</guid>
      <pubDate>Tue, 15 Oct 2024 21:28:04 GMT</pubDate>
    </item>
    <item>
      <title>Simba：深度强化学习中扩大参数的简单性偏差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g460jl/simba_simplicity_bias_for_scaling_up_parameters/</link>
      <description><![CDATA[      想要更快、更智能的强化学习？查看 SimBa – 我们可疯狂扩展的新架构！ 📄 项目页面：https://sonyresearch.github.io/simba 📄 arXiv：https://arxiv.org/abs/2410.09754 🔗 代码：https://github.com/SonyResearch/simba 🚀 厌倦了深度 RL 中缓慢的训练时间和不尽人意的结果？ 使用 SimBa，您可以毫不费力地扩展参数并达到最先进的性能 - 而无需更改核心 RL 算法。 💡 它是如何工作的？ 只需将您的 MLP 网络换成 SimBa，然后观看奇迹发生！在单个 Nvidia RTX 3090 上，只需 1-3 小时，您就可以训练出在 DMC、MyoSuite 和 HumanoidBench 等基准测试中表现最佳的代理。 🦾 ⚙️ 为什么它很棒： 即插即用，支持 SAC、DDPG、TD-MPC2、PPO 和 METRA 等 RL 算法。 无需调整您最喜欢的算法 - 只需切换到 SimBa 并让扩展能力接管即可。 训练更快、更智能、更好 - 非常适合研究人员、开发人员和任何探索深度 RL 的人！ 🎯 立即尝试并观察您的 RL 模型演变！ https://i.redd.it/olxmmgyauwud1.gif    提交人    /u/joonleesky   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g460jl/simba_simplicity_bias_for_scaling_up_parameters/</guid>
      <pubDate>Tue, 15 Oct 2024 12:03:38 GMT</pubDate>
    </item>
    </channel>
</rss>