<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 13 Jan 2025 06:25:42 GMT</lastBuildDate>
    <item>
      <title># RL 实习生或教育机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i06yc4/rl_intern_or_educational_opportunity/</link>
      <description><![CDATA[过去 8 个月，我一直在从三个主要方向研究 RL：数学观点、计算机科学观点（算法 + 编码）和神经科学（或心理学）观点。凭借近 5 年的编程经验以及过去 8 个月的理解，我可以自信地说 RL 就是我想要终生追求的。最大的问题是我目前不在任何学习机构，也没有技术工作来获得任何实习或教育机会。我积极性很高，每天花大约 5-6 个小时学习 RL，但我觉得这一切都是在浪费时间。你们建议我做什么？我目前住在加拿大温哥华，我是一名寻求庇护者，但有工作许可，我有资格就读教育机构。    提交人    /u/Easy-Quail1384   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i06yc4/rl_intern_or_educational_opportunity/</guid>
      <pubDate>Mon, 13 Jan 2025 05:25:33 GMT</pubDate>
    </item>
    <item>
      <title>强化学习论文实现的最佳仓库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i04eb2/best_repo_for_rl_paper_implementations/</link>
      <description><![CDATA[我正在寻找一些最新的 RL 论文的实现。    提交人    /u/research-ml   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i04eb2/best_repo_for_rl_paper_implementations/</guid>
      <pubDate>Mon, 13 Jan 2025 03:00:47 GMT</pubDate>
    </item>
    <item>
      <title>我的 GTrXL 变压器不适用于 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzsxqy/my_gtrxl_transformer_doesnt_work_with_ppo/</link>
      <description><![CDATA[      我实现了一个 GTrXL 变压器，带有稳定基线特征库提取器及其 PPO 算法，以训练具有部分可观测性的无人机代理（无需查看两个先前的状态并随机删除环境中的对象），但它似乎没有学习。 https://preview.redd.it/4kryli3ptlce1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=24593e69b0b7ddadd62b3ea2ade460f4ee2742b5 我从 GitHub 实现中获得了 GTrXL 的代码，并对其进行了调整，使其与 PPO 一起用作特征提取器。 我的代理在完整的可观察性配置中使用简单的 PPO 学习得很好。 有人知道为什么吗它不起作用？    提交人    /u/BitShifter1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzsxqy/my_gtrxl_transformer_doesnt_work_with_ppo/</guid>
      <pubDate>Sun, 12 Jan 2025 18:15:44 GMT</pubDate>
    </item>
    <item>
      <title>Sutton Barto 的策略梯度定理证明步骤 4</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzpecq/sutton_bartos_policy_gradient_theorem_proof_step_4/</link>
      <description><![CDATA[      我正在检查 sutton 的书中策略梯度定理证明。我不明白 r 是如何从步骤 3 过渡到步骤 4 时消失的。r 不是依赖于动作而又依赖于参数吗？ https://preview.redd.it/i3lszgko2lce1.jpg?width=1910&amp;format=pjpg&amp;auto=webp&amp;s=5dd375d3a0241c01e4240e9116fc659f17ce695f   由    /u/demirbey05  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzpecq/sutton_bartos_policy_gradient_theorem_proof_step_4/</guid>
      <pubDate>Sun, 12 Jan 2025 15:44:07 GMT</pubDate>
    </item>
    <item>
      <title>基于 RL 的本科课程简单项目的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzot12/idea_for_a_simple_project_based_on_rl_for_my/</link>
      <description><![CDATA[正如标题所示，我在 AI 本科期间选修了一门 RL 课程，并且必须就该课程完成一个小型项目，该项目几乎占整个课程成绩的四分之一。请就该课程建议一个简单且可实施的小型项目。谢谢！    提交人    /u/Orthodox_Shady   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzot12/idea_for_a_simple_project_based_on_rl_for_my/</guid>
      <pubDate>Sun, 12 Jan 2025 15:16:20 GMT</pubDate>
    </item>
    <item>
      <title>对强化学习新手的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzod9s/suggestions_for_a_newbie_in_reinforcement_learning/</link>
      <description><![CDATA[大家好！ 我是强化学习 (RL) 领域的新手，希望对其进行更深入的研究。我的背景是计算机科学，在机器学习和编程方面有一些经验，但我没有专门在 RL 上工作过。 我正在寻找某种可以遵循的路线图。    提交人    /u/research-ml   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzod9s/suggestions_for_a_newbie_in_reinforcement_learning/</guid>
      <pubDate>Sun, 12 Jan 2025 14:55:34 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 中的 RLHF 与 Gumbel Softmax</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hznraf/rlhf_vs_gumbel_softmax_in_llm/</link>
      <description><![CDATA[我的问题很简单。RLHF 用于微调 LLM，因为采样的 token 不可区分。为什么我们不使用 Gumbel softmax 采样来实现可区分采样并直接优化 LLM？ 整个 RLHF 感觉开销很大，我不明白为什么有必要    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hznraf/rlhf_vs_gumbel_softmax_in_llm/</guid>
      <pubDate>Sun, 12 Jan 2025 14:25:37 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助/建议来建立模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzm37d/need_helpsuggestions_for_building_a_model/</link>
      <description><![CDATA[大家好， 我目前正在进行一个路线优化项目，涉及使用 NetworkX 库加载的本地道路网络。以下是设置的简要概述：  环境：使用 NetworkX 以图形表示的本地道路网络文件 (.graphml)。 模型架构： GAT（图形注意网络）：它将状态和特征作为输入，并输出由图形中的节点总数形成的张量。下一个节点由此张量中的最高值标识。 Dueling DQN：GAT 模型的张量输出被传递给 Dueling DQN 模型，该模型还应返回相同形状的张量来决定操作（下一个节点）。   挑战：模型的输出与预期结果不一致。具体来说，路由决策似乎不是最佳的，我正在努力调整 GAT 和 Dueling DQN 之间的集成。 要求： 有关优化 GAT + Dueling DQN 管道的提示。 有关预处理图形特征以实现更好学习的建议。 在这种设置中调整超参数的最佳实践。 任何类似的实现或资源都可以提供帮助。 平均训练所需的时间 我很感激您提供的任何建议或见解！    提交人    /u/ProfessionalType9800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzm37d/need_helpsuggestions_for_building_a_model/</guid>
      <pubDate>Sun, 12 Jan 2025 12:56:23 GMT</pubDate>
    </item>
    <item>
      <title>博士毕业后的 RL 工程师职位</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzm18z/rl_engineer_jobs_after_phd/</link>
      <description><![CDATA[大家好， 我希望今年能获得博士学位。 我的博士最终目标是设计一个智能电网问题并用 RL 解决它。 我对 RL 的兴趣与日俱增，我想进一步提高我的技能。 你能指导我我在爱尔兰或其他国家有哪些求职选择吗？ 此外，在毕业之前我应该​​尝试涵盖 RL 的哪些主要领域？ 提前谢谢您。    提交人    /u/Dry-Image8120   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzm18z/rl_engineer_jobs_after_phd/</guid>
      <pubDate>Sun, 12 Jan 2025 12:52:51 GMT</pubDate>
    </item>
    <item>
      <title>需要有关自动驾驶遥控车的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzkwvn/need_help_regarding_autonomous_rc_car/</link>
      <description><![CDATA[我已经在 Unity 中训练了一个机器学习模型，该模型执行以下操作： 该模型通过强化学习使用神经网络自主驾驶汽车 我计划在硬件遥控车上使用这个模型，但我面临的问题是我对硬件部件知之甚少。 我已经在 Unity 中训练了一个机器学习模型，该模型执行以下操作： 该模型通过强化学习使用神经网络自主驾驶汽车 我计划在硬件遥控车上使用这个模型，但我面临的问题是我对硬件部件知之甚少。 有人可以帮帮我吗 我也有关于如何创建它的计划，但我对硬件的了解阻碍了我 https://reddit.com/link/1hzkwvn/video/auid31zvujce1/player    由   提交  /u/kungfuaryan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzkwvn/need_help_regarding_autonomous_rc_car/</guid>
      <pubDate>Sun, 12 Jan 2025 11:38:59 GMT</pubDate>
    </item>
    <item>
      <title>最好的强化学习书籍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzkl2u/best_reinforcement_learning_book/</link>
      <description><![CDATA[我很好奇 RL 的最佳书籍是什么？我希望找到一些主要用 C 语言、其他 C 语言或 Python 编写的书籍。我查过亚马逊，但像往常一样，编程书籍只字未提该语言。 这是我尝试做的一件事的示例：https://www.youtube.com/watch?v=8oIQy6fxfCA    提交人    /u/luddens_desir   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzkl2u/best_reinforcement_learning_book/</guid>
      <pubDate>Sun, 12 Jan 2025 11:15:43 GMT</pubDate>
    </item>
    <item>
      <title>混合动作空间的 SAC</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hzg1mv/sac_for_hybrid_action_space/</link>
      <description><![CDATA[我和我的团队正在开展一个项目，旨在打造一个能够使用强化学习学习弹奏简单钢琴曲的机器人。我们基于之前的模拟环境（论文网站：https://kzakka.com/robopianist/）进行构建，并用我们自己的定制设计替换他们的机器人手。本文作者使用 DroQ（SAC 的正则化变体）和纯连续动作空间，并进行典型的熵温度调整，如 https://arxiv.org/pdf/1812.05905 所示。它们的完整实现可在此处找到：https://github.com/kevinzakka/robopianist-rl。 在我们的手设计中，每个手指只能从左向右旋转（伺服 -&gt; 连续动作）并上下移动（螺线管 -&gt; 二进制/离散动作）。它非常类似于此设计：https://youtu.be/rgLIEpbM2Tw?si=Q8Opm1kQNmjp92fp。因此，我目前遇到的问题是如何最好地处理这个多维混合（连续-离散）动作空间。我看过这篇论文：https://arxiv.org/pdf/1912.11077，matlab 似乎也为其混合 SAC 实现了该论文，但我很好奇是否有人有进一步的建议或意见，尤其是关于实现多维度的离散/二进制动作（即针对每个手指）。我还看到了一些使用 Gumbel-softmax 方法的其他实现（例如 https://arxiv.org/pdf/2109.08512）。 我提前为我的无知道歉，我是一名本科生，对这些东西还不太熟悉。任何建议和/或指导都将不胜感激。谢谢！    由   提交  /u/potenza1702   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hzg1mv/sac_for_hybrid_action_space/</guid>
      <pubDate>Sun, 12 Jan 2025 05:46:54 GMT</pubDate>
    </item>
    <item>
      <title>人形机器人竞赛 - 寻找首批参与者/测试者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy8f7m/humanoid_race_competition_looking_for_first/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy8f7m/humanoid_race_competition_looking_for_first/</guid>
      <pubDate>Fri, 10 Jan 2025 16:33:41 GMT</pubDate>
    </item>
    <item>
      <title>我的 RL 学习方法（Isaac Lab 实践）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy4zcy/my_rl_learning_approach_handson_with_isaac_lab/</link>
      <description><![CDATA[大家好， 我只是想分享我学习 RL/机器人技术的历程。 我的 TL;DR 背景： 机械工程专业应届毕业生 一年前学会了 Python 编码 关于 NVIDIA 的 Omniverse Isaac Sim Replicator（AI/计算机视觉）的论文 开始攻读硕士学位，3 个月后退出硕士学位（不如预期） 大约 1 个月前，我开始对 RL/机器人技术产生巨大的动力。为了跟上所有 RL 术语和算法，我开始观看免费的 YT 教育视频。尽管那里有很多内容，但大多非常理论化，不适合初学者。作为一个喜欢通过动手方式学习更多知识的人，我很挣扎。  但是，由于我已经熟悉 NVIDIA 的 Isaac Sim，我开始探索 Isaac Lab，并立即着迷。我开始阅读教程和文档，并加入 Omniverse Discord Server 上的学习小组，学习 RL 感觉容易多了。至少对我来说，首先采取实用方法（构建机器人、场景等）并同时学习理论感觉更直观。 我并不是说 Isaac Lab 是学习 RL 的窍门，学习 API 肯定需要时间和精力，但实际上自己创建环境并观察机器人学习会让它变得非常有趣。我强烈建议您尝试一下！ 如果您想加入我，一起踏上 Isaac Lab RL 之旅，我开始在 YouTube 上创建 Isaac Lab 教程，以帮助每个人更轻松地完成任务（同时也可以跟踪我自己的进度）： https://www.youtube.com/playlist?list=PLQQ577DOyRN_hY6OAoxBh8K5mKsgyJi-r    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy4zcy/my_rl_learning_approach_handson_with_isaac_lab/</guid>
      <pubDate>Fri, 10 Jan 2025 13:59:12 GMT</pubDate>
    </item>
    <item>
      <title>大多数 RL 工作都需要博士学位吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxwgz3/do_most_rl_jobs_need_a_phd/</link>
      <description><![CDATA[我是机器人学硕士生，我的论文是将强化学习应用于操控。我可能无法想出一些新的算法，但我擅长理解和应用。 我有兴趣将机器人学习作为职业，但似乎我看到的每份工作都需要博士学位。这是常态吗？我如何准备简历上的项目，以便仅凭硕士学位就能找到一份从事操控/人形机器人的工作？任何建议和意见都是有帮助的。 鉴于机器人技术就业市场的状况，我有点担心..    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxwgz3/do_most_rl_jobs_need_a_phd/</guid>
      <pubDate>Fri, 10 Jan 2025 04:35:46 GMT</pubDate>
    </item>
    </channel>
</rss>