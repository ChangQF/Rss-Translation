<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 28 Mar 2025 12:35:23 GMT</lastBuildDate>
    <item>
      <title>“ Video-R1：在MLLM中加强视频推理”，Feng等。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jltyp2/videor1_reinforcing_video_reasoning_in_mllms_feng/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jltyp2/videor1_reinforcing_video_reasoning_in_mllms_feng/</guid>
      <pubDate>Fri, 28 Mar 2025 12:31:52 GMT</pubDate>
    </item>
    <item>
      <title>最佳课程还是RL学习材料？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jlqd9g/best_course_or_learning_material_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  学习RL和DRL的最佳方法是什么？我看着大卫·西尔弗（David Silver）的YT课程，但它已经快10岁了。我知道基本知识是相同的，但我想了解更多的RL和DRL的实现以及其背后的基础知识，任何人都可以共享一些资源吗？我有大约一个星期的时间为即将与主管进行大学项目工作的项目会议做准备，而我有点新手，我知道我可以通过它来学习，但它是基于截止日期的项目，因此我想处理理论和一些实用的事情。  还有我应该跟进RL中最新的最新发展的研究人员吗？还是DL？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/commistigand-way227     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jlqd9g/best_course_or_learning_material_for_rl/</guid>
      <pubDate>Fri, 28 Mar 2025 08:29:12 GMT</pubDate>
    </item>
    <item>
      <title>Manus AI帐户可用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jln3c6/manus_ai_accounts_available/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   lmk如果你们想要一个☝️  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/www-reseller     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jln3c6/manus_ai_accounts_available/</guid>
      <pubDate>Fri, 28 Mar 2025 04:35:09 GMT</pubDate>
    </item>
    <item>
      <title>寻找一些潜在的RL论文主题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jleqle/looking_for_some_potential_rl_thesis_topics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我目前正在追求我的数据科学科学硕士学位，并发现了对强化学习的热情。我正在弄清楚我想为我的硕士论文做什么，并正在寻找RL和深层RL中的某些潜在领域，这些领域可能会扩展。欢迎任何想法，我迫不及待想看看人们的建议。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/enseral-sink-2298     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jleqle/looking_for_some_potential_rl_thesis_topics/</guid>
      <pubDate>Thu, 27 Mar 2025 21:47:19 GMT</pubDate>
    </item>
    <item>
      <title>网格导航曲折</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jlemj0/grid_navigation_with_a_twist/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我是强化学习场景和一般编码场景的新手，但我决定跳入并开始玩耍。我想创建一个可以导航网格但有所扭曲的PPO模型。基本上，该模型具有不同大小的网格，并列出了起点和终点。代理在特定起点开始，然后移至终点，非常简单。然后，我想教导该模型以一定数量的步骤进行操作，这并不总是可能的最小步骤，因此我在观察空间中添加了预期的步骤数值为百分比。最后，我想教导该模型一遍又一遍地做到这一点，直到它可以用尽可能多的重叠路径填充电网为止。我遇到的一件事是，该模型在训练方面的表现不佳，并且似乎是犯错的错误。我将其归因于两件事之一 - 用户错误（我是新手，所以我很容易将其搞砸了），错误的模型（也许PPO并不是最好的方法），或者最后这只是机器学习应用程序。如果有人可以帮助我或给我一些很棒的指导！请随意进行DM或评论以获取其他问题。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/u/hungry-tough-3836     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jlemj0/grid_navigation_with_a_twist/</guid>
      <pubDate>Thu, 27 Mar 2025 21:42:41 GMT</pubDate>
    </item>
    <item>
      <title>只是将其传递给：</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jlb5g8/this_just_in_pass_it_on/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/特征性      [注释]    ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jlb5g8/this_just_in_pass_it_on/</guid>
      <pubDate>Thu, 27 Mar 2025 18:37:34 GMT</pubDate>
    </item>
    <item>
      <title>现在，您可以使用Google的新Gemma 3型号和GRPO来训练自己的推理LLM。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jl7oxh/you_can_now_use_googles_new_gemma_3_model_grpo_to/</link>
      <description><![CDATA[   GRPO tutorial here.这是我们的COLAB笔记本：   grpo：gemma 3（1b）笔记本： https：//cola B.Research.google.com/github/unslothai/notebooks/blob/main/nb/gemma3_(1B)-grpo.ipynb  -grpo.ipynb） 正常的Sft： gemma 3（4b）笔记本 .ipynb） .ipynb）.ipynb）.ipynb）.ipynb）.ipynb）    ：）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/yoracale     ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jl7oxh/you_can_now_use_googles_new_gemma_3_model_grpo_to/</guid>
      <pubDate>Thu, 27 Mar 2025 16:14:51 GMT</pubDate>
    </item>
    <item>
      <title>Isaaclab入门错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jl60ot/getting_started_errors_with_isaaclab/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有人让Isaac实验室工作吗？该文档非常疯狂。 我有isaacsim 4.2.0，我遵循了安装以isaaclab的文档，但是当我运行任何示例时，例如：    ./ isaaclab.sh.sh./isaaclab.sh./isaaclab.sh-p scripts/tutorials/tutorials/tutorials/tutorials/crect.ppy。脚本/tutorials/00_sim/create_empty.py   我得到错误：    modulenotfounderror：no Module名为&#39;omni.kit.kit.kit.usd&#39;  提交由＆＃32; /u/u/jcreed77     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jl60ot/getting_started_errors_with_isaaclab/</guid>
      <pubDate>Thu, 27 Mar 2025 15:03:54 GMT</pubDate>
    </item>
    <item>
      <title>企业学习：</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jkyg5z/enterprise_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  企业学习是关于评估和共享经验，而不是从书籍中学习或被教导知识。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/alj1974aus     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jkyg5z/enterprise_learning/</guid>
      <pubDate>Thu, 27 Mar 2025 07:28:01 GMT</pubDate>
    </item>
    <item>
      <title>高原 +训练中的下降趋势，有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jkd8s9/plateau_downtrend_in_training_any_advice/</link>
      <description><![CDATA[    src =“ https://preview.itd.it/5dgccdp1p1 re1.png？ 建议？” /&gt;    &lt;！ -  sc_off-&gt;  这是我的mujoco环境和张板日志。使用PPO与以下超参数使用PPO：   indition_lr = 0.00005 final_lr = 0.000001 initial_clip = 0.3 final_clip = 0.01 ppo_hyperparams = {&#39;learning_rate&#39;：line_rate&#39;：linear_schedule_schedule_schedule（initial_lr，firins_lr，firins_lr，firine_lr） final_clip), &#39;target_kl&#39;: 0.015, &#39;n_epochs&#39;: 4, &#39;ent_coef&#39;: 0.004, &#39;vf_coef&#39;: 0.7, &#39;gamma&#39;: 0.99, &#39;gae_lambda&#39;: 0.95, &#39;batch_size&#39;: 8192, &#39;n_steps&#39;: 2048, &#39;policy_kwargs&#39;: dict( net_arch = dict（pi = [256，128，64]，vf = [256，128，64]），activation_fn = torch.nn.elu，ortho_init = true，），&#39;formoralize_Advantage&#39;：true，true，true，&#39;max_grad_nomor - &gt;＆＃32;提交由＆＃32; /u/u/snotrio       [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jkd8s9/plateau_downtrend_in_training_any_advice/</guid>
      <pubDate>Wed, 26 Mar 2025 14:35:39 GMT</pubDate>
    </item>
    <item>
      <title>深度学习管道和大型数据集的开源RAG框架 - 更快的检索，较低的潜伏期，更智能的集成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jkckz6/opensource_rag_framework_for_deep_learning/</link>
      <description><![CDATA[     &lt;！ -  sc_off-&gt;  最近正在探索如何优化检索 最近的生成（rag），很明显，在动态环境中构建强大的                                为了突破这些边界，更快地处理检索任务，有效地扩展并与生态系统中的关键工具集成。 我们仍处于早期开发中，但是初始基准测试已经显示出一些有希望的结果。在某些情况下，它匹配甚至超过了 langchain 和 llamaindex  的匹配。    它与 tensorrt ， faiss ， vllm 等工具无缝集成。我们的路线图充满了进一步的优化和更新。无论是通过想法，代码还是简单地共享反馈，欢迎贡献。如果您发现它有用，将星星放在github上将意味着很多！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/gbalke     [link]        [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jkckz6/opensource_rag_framework_for_deep_learning/</guid>
      <pubDate>Wed, 26 Mar 2025 14:06:22 GMT</pubDate>
    </item>
    <item>
      <title>实施A3C用于Carracing-V3连续诉讼案例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jk0cit/implementing_a3c_for_carracingv3_continuous/</link>
      <description><![CDATA[    我现在面临的问题是从Sutton＆amp;关于Advantage Actor评论家A3C的BARTO我阅读了在这里。据我了解：   我的问题：   对于演员而言，我们最大化 j （θ），但我已经看到人们使用 l = -e [log]我假设我们在为∇j（θ） ∇中取出 （请参见上图中的（参见（3）），而不是最大化所获得的术语，我们将其负数最小化。我在正确的轨道上吗？ 因为演员和评论家使用了两个不同的损失功能，所以我认为我们将不得不为两个损失功能设置不同的优化器。但是我看到的是，人们俱乐部将损失纳入a 单个损失功能。为什么是这样？我的演员应该输出6个值（3个平均值，每个动作的3个差异）？这些值不相关吗？如果是这样，我不需要从多变量高斯的协方差矩阵和样品？ 是否接受过类似于 https：//arxiv.org =”  atari dqn 又是批评的，而不是在训练中？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/dead_as_duck    href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1jk0cit/implementing_a3c_for_carracingv3_continuul/”&gt; [link]    [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jk0cit/implementing_a3c_for_carracingv3_continuous/</guid>
      <pubDate>Wed, 26 Mar 2025 01:31:47 GMT</pubDate>
    </item>
    <item>
      <title>AI学会玩Starfox（SNES）（深度加强学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jjwtu0/ai_learns_to_play_starfox_snes_deep_reinforcement/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/ageofempimes4aoe4aoe4     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jjwtu0/ai_learns_to_play_starfox_snes_deep_reinforcement/</guid>
      <pubDate>Tue, 25 Mar 2025 22:49:45 GMT</pubDate>
    </item>
    <item>
      <title>2D平台环境中的ML代理商问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jjtiqc/mlagents_agent_problem_in_2d_platformer/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jjtiqc/mlagents_agent_problem_in_2d_platformer/</guid>
      <pubDate>Tue, 25 Mar 2025 20:32:12 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>