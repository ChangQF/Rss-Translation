<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 22 Dec 2023 15:13:32 GMT</lastBuildDate>
    <item>
      <title>ReCoRe：世界模型的正则化对比表示学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.09056 摘要：  虽然最近的无模型强化学习（RL）方法已经证明尽管在游戏环境中的人类水平的有效性，他们在视觉导航等日常任务中的成功受到限制，特别是在显着的外观变化下。这种限制源于 (i) 样本效率差和 (ii) 过度拟合训练场景。为了应对这些挑战，我们提出了一个世界模型，该模型使用（i）对比无监督学习和（ii）干预不变正则化器来学习不变特征。学习世界动态的显式表示（即世界模型）可以提高样本效率，而对比学习隐式地强制学习不变特征，从而提高泛化能力。然而，由于缺乏视觉编码器的监督信号，对比损失与世界模型的简单集成失败了，因为基于世界模型的强化学习方法独立地优化了表示学习和代理策略。为了克服这个问题，我们提出了一种以辅助任务（例如深度预测、图像去噪等）形式存在的干预不变正则化器，它明确地强制风格干预的不变性。我们的方法优于当前最先进的基于模型和无模型的 RL 方法，并且在 iGibson 基准评估的分布外点导航任务上表现显着。我们进一步证明，我们的方法仅通过视觉观察，优于最近的语言引导的点导航基础模型，这对于在计算能力有限的机器人上部署至关重要。最后，我们证明我们提出的模型在 Gibson 基准上的感知模块的模拟到真实转换方面表现出色。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</guid>
      <pubDate>Fri, 22 Dec 2023 13:36:49 GMT</pubDate>
    </item>
    <item>
      <title>是否有研究在 RL 期间使用 LoRA 和 QLoRA 等参数高效训练来进行预训练模型？我想在一些大型模型上运行强化学习，并且希望避免购买无数的 GPU。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oe4t2/has_there_been_any_research_into_using/</link>
      <description><![CDATA[我有一些相当大的预训练模型，我想在它们上运行强化学习，并且更愿意从成本较低的训练选项开始。有人知道是否有任何论文或文章详细介绍了使用 LoRA 等技术进行强化学习？   由   提交/u/30299578815310  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oe4t2/has_there_been_any_research_into_using/</guid>
      <pubDate>Fri, 22 Dec 2023 12:46:22 GMT</pubDate>
    </item>
    <item>
      <title>“强化学习迁移的基础：知识模态分类”，Wulfmeier 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</guid>
      <pubDate>Fri, 22 Dec 2023 01:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 amass 数据集转换为 mujoco 格式？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18nrdhb/how_to_convert_the_amass_dataset_to_mujoco_format/</link>
      <description><![CDATA[嗨， 我想将 amass 数据集转换为 mujoco 格式，以便我能够在 mujoco 中使用运动数据知道如何做到这一点吗？ 我对 amass 和 mujoco 都很陌生，所以如果这似乎是一个愚蠢的问题，我深表歉意。 &lt;!-- SC_ON - -&gt;  由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18nrdhb/how_to_convert_the_amass_dataset_to_mujoco_format/</guid>
      <pubDate>Thu, 21 Dec 2023 16:48:28 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线3收集部署时出错</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18nqifb/error_in_collecting_rollouts_using/</link>
      <description><![CDATA[        由   提交/u/Ecstatic-Rain-2460   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18nqifb/error_in_collecting_rollouts_using/</guid>
      <pubDate>Thu, 21 Dec 2023 16:09:55 GMT</pubDate>
    </item>
    <item>
      <title>“评估现实自主任务上的语言模型代理”，Kinniment 等人 2023 {ARC}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18np4kd/evaluating_languagemodel_agents_on_realistic/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18np4kd/evaluating_languagemodel_agents_on_realistic/</guid>
      <pubDate>Thu, 21 Dec 2023 15:07:37 GMT</pubDate>
    </item>
    <item>
      <title>“利用大型语言模型进行自主化学研究”，Boiko 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18np0m3/autonomous_chemical_research_with_large_language/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18np0m3/autonomous_chemical_research_with_large_language/</guid>
      <pubDate>Thu, 21 Dec 2023 15:02:45 GMT</pubDate>
    </item>
    <item>
      <title>你们如何处理强化学习中的梯度爆炸？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ninvh/how_you_guys_handle_gradient_exploding_in_rl/</link>
      <description><![CDATA[ 正确的权重初始化 梯度裁剪 lr调度程序我还能做什么？&lt; /li&gt;    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ninvh/how_you_guys_handle_gradient_exploding_in_rl/</guid>
      <pubDate>Thu, 21 Dec 2023 08:57:35 GMT</pubDate>
    </item>
    <item>
      <title>利用离散表示进行持续强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ni6k8/harnessing_discrete_representations_for_continual/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.01203 OpenReview：https:// /openreview.net/forum?id=o4AydSd3Lp 摘要：  强化学习（RL）智能体什么都不用做决策但来自环境的观察结果在很大程度上依赖于这些观察结果的表征。尽管最近的一些突破使用了基于向量的观察分类表示（通常称为离散表示），但很少有工作明确评估这种选择的重要性。在这项工作中，我们对强化学习背景下将观察结果表示为分类值向量的优势进行了彻底的实证研究。我们对世界模型学习、无模型强化学习以及最终的连续强化学习问题进行评估，其中的好处最能满足问题设置的需求。我们发现，与传统的连续表示相比，通过离散表示学习的世界模型可以用更少的容量准确地模拟更多的世界，并且用离散表示训练的智能体用更少的数据学习更好的策略。在持续强化学习的背景下，这些好处转化为更快的适应代理。此外，我们的分析表明，观察到的性能改进可归因于潜在向量中包含的信息以及潜在的离散表示本身的编码。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ni6k8/harnessing_discrete_representations_for_continual/</guid>
      <pubDate>Thu, 21 Dec 2023 08:23:29 GMT</pubDate>
    </item>
    <item>
      <title>《人类衰老的递减状态空间理论》，Eppinger 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18n2u46/diminished_state_space_theory_of_human_aging/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18n2u46/diminished_state_space_theory_of_human_aging/</guid>
      <pubDate>Wed, 20 Dec 2023 19:22:47 GMT</pubDate>
    </item>
    <item>
      <title>“ReST 与 ReAct 的结合：多步推理 LLM 代理的自我改进”，Aksitov 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18n1woy/rest_meets_react_selfimprovement_for_multistep/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18n1woy/rest_meets_react_selfimprovement_for_multistep/</guid>
      <pubDate>Wed, 20 Dec 2023 18:44:17 GMT</pubDate>
    </item>
    <item>
      <title>在您想要的任何环境中轻松训练类似 AlphaZero 的智能体！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18myr1m/easily_train_alphazerolike_agents_on_any/</link>
      <description><![CDATA[大家好， 我为那些想要训练自己的 AlphaZero 的人创建了一个简单的起点！&lt; /p&gt; 您所需要的只是一个训练代理的环境，其他一切都已设置完毕。将其视为 AlphaZero 代理的 Huggingface 变形金刚。 我想添加更多环境，因此需要帮助。请随意克隆存储库并提交 PR！ 让我知道您的想法，链接如下：https ://github.com/s-casci/tinyzero   由   提交/u/ayan0k0ji  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18myr1m/easily_train_alphazerolike_agents_on_any/</guid>
      <pubDate>Wed, 20 Dec 2023 16:34:09 GMT</pubDate>
    </item>
    <item>
      <title>AMAGO：自适应代理的可扩展上下文强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mszov/amago_scalable_incontext_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.09971 OpenReview：https:// /openreview.net/forum?id=M6XWoEdmwf 代码：https://github.com/UT-Austin-RPL/amago 项目页面：https://ut-austin-rpl.github.io/amago/ 摘要：  我们介绍 AMAGO，这是一种上下文强化学习 (RL) 代理，它使用序列模型来应对泛化、长期记忆和元学习的挑战。最近的研究表明，离策略学习可以使具有循环策略的上下文强化学习变得可行。尽管如此，这些方法需要大量的调整，并通过在代理的内存容量、规划范围和模型大小方面产生关键瓶颈来限制可扩展性。 AMAGO 重新审视并重新设计了脱离策略的上下文方法，以便在整个部署过程中与端到端 RL 并行地成功训练长序列 Transformer。我们的代理具有独特的可扩展性，适用于广泛的问题。我们根据经验证明了它在元强化学习和长期记忆领域的强大性能。 AMAGO 对稀疏奖励和离策略数据的关注也允许上下文学习扩展到具有挑战性探索的目标条件问题。当与新颖的事后重新标记方案相结合时，AMAGO 可以解决以前困难的开放世界领域类别，其中代理在程序生成的环境中完成许多可能的指令。我们在三个目标条件域上评估我们的代理，并研究其单独的改进如何连接以创建通用策略。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mszov/amago_scalable_incontext_reinforcement_learning/</guid>
      <pubDate>Wed, 20 Dec 2023 12:00:31 GMT</pubDate>
    </item>
    <item>
      <title>使用屏幕作为观察</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mq29t/use_the_screen_as_observations/</link>
      <description><![CDATA[大家好，需要一些建议。 我正在健身房环境中尝试视觉强化学习（lunarlander-v2） ）。我只是使用我之前表现良好的ppo程序，然后在actor和critic之前添加一个两层CNN网络，CNN接收屏幕截图作为输入，然后输出一个3k维张量作为观察。 我训练了5千次。但不幸的是它的性能非常糟糕。我什至没有看到损失收敛的趋势。显然这并不像我想象的那么容易。 我可以想一些方法来提高性能，比如使用预先训练的图像编码器。但我不知道主要原因在哪里，还是我有更大的误会。由于每次训练都需要很长时间，所以我不想在没有方向的情况下进行实验。有没有适合我的指南或者论文，非常感谢。 ​ ​ 最后，我这样做的原因是一场讨论。我认为对原始图像进行特征提取会对视觉强化学习有所帮助，但有些人认为这没有用。   由   提交 /u/Ruine_fff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mq29t/use_the_screen_as_observations/</guid>
      <pubDate>Wed, 20 Dec 2023 08:47:21 GMT</pubDate>
    </item>
    <item>
      <title>今天，DQN arXiv 十周年了！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mmsud/dqn_arxiv_turns_a_decade_old_today/</link>
      <description><![CDATA[ 由   提交/u/DeepQZero  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mmsud/dqn_arxiv_turns_a_decade_old_today/</guid>
      <pubDate>Wed, 20 Dec 2023 05:20:00 GMT</pubDate>
    </item>
    </channel>
</rss>