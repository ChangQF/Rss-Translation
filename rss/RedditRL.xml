<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 08 Oct 2024 06:24:20 GMT</lastBuildDate>
    <item>
      <title>“语言模型学会通过 RLHF 误导人类”，Wen 等人 2024 年（自然出现对不完美评估者的操纵，以最大化奖励，但不最大化质量）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fynve7/language_models_learn_to_mislead_humans_via_rlhf/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fynve7/language_models_learn_to_mislead_humans_via_rlhf/</guid>
      <pubDate>Tue, 08 Oct 2024 01:05:40 GMT</pubDate>
    </item>
    <item>
      <title>表示状态的临界性或稳定性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fycxzb/representation_of_criticality_or_stability_of_a/</link>
      <description><![CDATA[有人知道在假设一项政策的情况下，从状态计算或了解一般强化学习问题的不稳定程度或失败概率的方法吗？我的目标是：从一组应用程序中，找到一个表示，使我最需要适当的控制。 在控制理论中，存在计算这个的方法，但从我所见（不是专家）来看，它需要很多假设，大部分是线性的，因为非线性相当复杂，需要控制器矩阵和动力学。我想知道是否有类似的东西可以通过强化学习框架来学习？ 对于强化学习问题，为简单起见，我们假设一个不稳定的问题，具有像 cartpole 这样的故障条件。如何仅从转换来估计系统失败或稳定性的概率？显然你可以从角度和位置来做到这一点，但是对于未知的动态，有没有方法可以学习这一点？ 我认为优势是一个可以使用的功能，但它并不完全相同。    提交人    /u/Enryu77   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fycxzb/representation_of_criticality_or_stability_of_a/</guid>
      <pubDate>Mon, 07 Oct 2024 17:11:58 GMT</pubDate>
    </item>
    <item>
      <title>这是一个有效的 RL 问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fya6z5/is_it_a_valid_rl_problem/</link>
      <description><![CDATA[给定一组 html 页面，其中每个 html 页面都是文本段落序列，并且每个段落都被标记为 0 或 1。我可以使用强化学习来学习为 html 页面中的段落序列分配 0 或 1 的最佳策略吗？给出上面标记的数据集。  我认为每个 html 页面都是一个情节，其中可以从每个段落文本中得出状态，并且采取的行动是 0 或 1。 这是一个有效的 RL 问题吗？有人可以指出使用 RL 尝试解决此类问题的论文或链接吗？    提交人    /u/HotCauliflower2360   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fya6z5/is_it_a_valid_rl_problem/</guid>
      <pubDate>Mon, 07 Oct 2024 15:18:56 GMT</pubDate>
    </item>
    <item>
      <title>强化学习在游戏中有什么应用吗？（不是玩游戏，而是在游戏中使用）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fy1yhe/are_there_any_applications_of_rl_in_games_not/</link>
      <description><![CDATA[我对 RL 还很陌生，对我来说，它一直与游戏密切相关。然而，经过一段时间的接触，我注意到，就游戏而言，RL 仅用于“解决”问题。我从未见过有人试图将其用于游戏内的 AI 或其他系统    提交人    /u/Vefery   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fy1yhe/are_there_any_applications_of_rl_in_games_not/</guid>
      <pubDate>Mon, 07 Oct 2024 07:22:38 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 RL 游戏项目的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fxocnu/need_ideas_for_a_rl_in_games_project/</link>
      <description><![CDATA[这学期我在大学被分配做一个项目。我对游戏中的 RL 很感兴趣（或类似的东西），所以我选择它作为主题。而且由于这只是一项小研究，我需要得到一些有意义的结果。比如训练一个模型并观察它在不同场景和不同条件下的表现。但老实说，我完全没有主意了 我有使用 Unity 的经验，所以构建自定义环境不是问题。而且这个项目不需要非常复杂或取得突破。实际上我需要能够在 3-4 个月内完成它    提交人    /u/Aydiagam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fxocnu/need_ideas_for_a_rl_in_games_project/</guid>
      <pubDate>Sun, 06 Oct 2024 19:16:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么与配备 RTX 4070 的 Windows 机器相比，MacBook Pro（M2 Max）上的 ML-Agents 训练速度快 3-5 倍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fx4aqe/why_is_mlagents_training_35x_faster_on_macbook/</link>
      <description><![CDATA[我正在 Unity 中开发一个场景并使用 ML-Agents 进行训练。我注意到我拥有的两台机器在训练时间上存在显著差异，我试图理解为什么 MacBook Pro 的速度要快得多。以下是两台机器的硬件规格： MacBook Pro（Apple M2 Max）规格： • 型号名称：MacBook Pro • 芯片：Apple M2 Max • 12 核（8 核性能、4 核效率） • 内存：96 GB LPDDR5 • GPU：38 核 Apple M2 Max • Metal 支持：Metal 3  Windows 机器规格： • 处理器：Intel64，8 核 @ 3000 MHz • GPU：NVIDIA GeForce RTX 4070 • 内存：65 GB DDR4 • 总虚拟内存：75,180 MB  尽管 RTX 4070 是一款功能强大的 GPU，但 MacBook Pro 上的训练速度要快 3 到 5 倍。有人知道为什么 MacBook 在 ML-Agents 训练中的表现会远远超过 Windows 机器吗？ 此外，您是否认为在这种类型的工作负载下，4090 或未来的 5090 与 M2 Max 相比仍会存在性能差距？ 提前感谢您的任何见解！    提交人    /u/bbzzo   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fx4aqe/why_is_mlagents_training_35x_faster_on_macbook/</guid>
      <pubDate>Sun, 06 Oct 2024 00:22:46 GMT</pubDate>
    </item>
    <item>
      <title>机械工程到 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fx1a16/mechanical_engineering_to_rl/</link>
      <description><![CDATA[大家好，我是机械工程专业的应届毕业生，我想问一些关于如何转向强化学习行业的建议。  我的学位是机电一体化专业，我曾希望这能让我掌握广泛的技能，但机电一体化的大部分知识来自控制理论，而不是机器人技术，也几乎没有软件。 （不过，我从实习和个人项目中获得了一些经验） 在获得学位和机器人课程后，我意识到这才是我真正感兴趣的，但与机器人的实际机械设计相比，我更多地关注 RL 和 IL。  我的平均成绩 (GPA) 还不错，（大部分都是 A），但在软件方面经验不多，特别是人工智能。  我一直在考虑几条途径：  只要成为在线资源（Coursera、Sutton and Barto、Hugging Face 等）的 Rockstar 并建立强大的简历 尝试从研究生院转向 RL 部门，例如但不限于：2a。西北大学机器人学硕士 2b。UBC 数据科学硕士 2c。OMSCS  因为我是国际学生，所以也在考虑 NA 以外的地方，但 NA 似乎对 RL 来说是最好的。 任何帮助都将不胜感激！！！！    提交人    /u/HooChooHan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fx1a16/mechanical_engineering_to_rl/</guid>
      <pubDate>Sat, 05 Oct 2024 21:50:02 GMT</pubDate>
    </item>
    <item>
      <title>无法建立强化学习模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fx0wu5/fail_to_build_a_reinforcement_learning_model/</link>
      <description><![CDATA[        提交人    /u/Electronic-Still-1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fx0wu5/fail_to_build_a_reinforcement_learning_model/</guid>
      <pubDate>Sat, 05 Oct 2024 21:32:20 GMT</pubDate>
    </item>
    <item>
      <title>适合初学者的超级简单教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fwpn0q/super_simple_tutorial_for_beginners/</link>
      <description><![CDATA[        提交人    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fwpn0q/super_simple_tutorial_for_beginners/</guid>
      <pubDate>Sat, 05 Oct 2024 12:49:03 GMT</pubDate>
    </item>
    <item>
      <title>在哪里训练 RL 代理（计算资源）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fwmtcg/where_to_train_rl_agents_computing_resources/</link>
      <description><![CDATA[嗨， 我对训练（较大的）RL 应用程序还比较陌生。我需要训练 12-15 个代理，以比较它们在 POMDP 问题（在金融领域 -&gt; 纯表格数据）上的表现，并在状态空间中对特定特征进行不同的表示。  我还没有开始训练，想知道在例如本地云架构上进行训练是否有意义。替代方案是配备 NVIDIA GeForce RTX 3060、4GB 的笔记本电脑。  我尝试提供尽可能多的有关潜在计算成本的信息：  状态空间由每个 t 的 10N+1 维组成，其中 N 是资产数量（如果这可以大致了解状态中的维度，我将主要使用 5-9 个资产）-&gt;所有维度都是连续的。一个时期由~1250个观测值组成 动作空间由 2N 个维度组成 -&gt; N 个维度的范围是 [-1,1]，其他 N 个维度的范围是 [0,1]。 我可能会使用某种 TD3 算法  我不知道这些信息是否足以得出计算出的意见，但是由于我对将 RL 应用于“更大”问题和管理计算约束还很陌生，因此非常感谢每一个提示/想法/讨论。    提交人    /u/Intelligent-Put1607   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fwmtcg/where_to_train_rl_agents_computing_resources/</guid>
      <pubDate>Sat, 05 Oct 2024 09:46:14 GMT</pubDate>
    </item>
    <item>
      <title>稳定的 Baselines3 回调函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fwmnh1/stable_baselines3_callback_function/</link>
      <description><![CDATA[嗨，我正在努力解决 Stable Baselines3 和评估过程的问题。代码不是我的，评估的回调是一个自定义函数，它将数据推送到权重和偏差 (WandB)。 evaluate_policy(model, env, n_eval_episodes=eval_episodes, callback=eval_callback) ... def eval_callback(result_local, result_global):  我的问题是：result_local 和 result_global 是什么？我尝试打印数据，但我只得到情节奖励或情节长度等总体指标。我如何访问所有奖励的列表来计算我自己的指标？ 感谢您的帮助。 干杯    提交人    /u/BitRa1n   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fwmnh1/stable_baselines3_callback_function/</guid>
      <pubDate>Sat, 05 Oct 2024 09:33:55 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 有没有什么有前景的研究可以利用 RL 从人类反馈中改进计算机视觉任务？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fw49i6/discussion_are_there_any_promising_work_on_using/</link>
      <description><![CDATA[  由    /u/Appropriate_Bear_894  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fw49i6/discussion_are_there_any_promising_work_on_using/</guid>
      <pubDate>Fri, 04 Oct 2024 17:06:16 GMT</pubDate>
    </item>
    <item>
      <title>（重复）没有自我注意力的前馈可以预测未来的标记吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/</link>
      <description><![CDATA[       由    /u/Timur_1988  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/</guid>
      <pubDate>Thu, 03 Oct 2024 21:43:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么 TD-MPC2 中没有循环模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fvckc1/why_no_recurrent_model_in_tdmpc2/</link>
      <description><![CDATA[我正在阅读 TD-MPC2 论文，我对整个想法非常了解。我唯一不太理解的是为什么潜在动力学模型是一个简单的 MLP，而不是像许多其他基于模型的论文中那样的循环模型。 主要问题是：潜在动力学模型如何一步一步地维持一个潜在表示 z，该表示结合了来自前一个时间步骤的信息，而没有任何隐藏状态。我猜他们测试的许多环境都需要这种能力，而且该算法似乎表现得非常好。 我的理解是，通过反向传播整个序列，潜在状态 z 仍然会从以下步骤接收梯度，因此潜在动力学模型可以隐式学习如何产生下一个潜在状态，该状态保留所有先前状态的信息。 但是，这不是效率低下吗？我很确定作者没有使用任何类型的序列模型（LSTM 等）是有原因的，但我似乎找不到令人满意的答案。你有吗？  论文链接     提交人    /u/fedetask   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fvckc1/why_no_recurrent_model_in_tdmpc2/</guid>
      <pubDate>Thu, 03 Oct 2024 17:01:38 GMT</pubDate>
    </item>
    <item>
      <title>Esquilax：大型多智能体 RL JAX 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fva0qn/esquilax_a_largescale_multiagent_rl_jax_library/</link>
      <description><![CDATA[我已经发布了Esquilax，一个多智能体模拟和 ML/RL 库。  它专为大规模多智能体系统（想想群体、羊群社交网络）的建模及其用作 RL 和其他 ML 方法的训练环境而设计。  它实现了常见的模拟和多智能体训练功能，减少了实现复杂模型和实验所需的时间和代码量。它还旨在与现有的 JAX ML 工具一起使用，例如 Flax 和 Evosax。 代码和完整文档可在以下位置找到： https://github.com/zombie-einstein/esquilax https://zombie-einstein.github.io/esquilax/ 您还可以在此处看到一个使用 Esquilax 将 boids 实现为 RL 环境的更大的项目    由   提交  /u/Familiar-Watercress2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fva0qn/esquilax_a_largescale_multiagent_rl_jax_library/</guid>
      <pubDate>Thu, 03 Oct 2024 15:09:22 GMT</pubDate>
    </item>
    </channel>
</rss>