<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 20 Oct 2024 01:21:25 GMT</lastBuildDate>
    <item>
      <title>DeepMind 2023 年财务报告：预算 15 亿英镑（+5 亿英镑）[约 19 亿美元，+6 亿美元]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7lh05/deepmind_2023_financial_filings_15_billion_budget/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7lh05/deepmind_2023_financial_filings_15_billion_budget/</guid>
      <pubDate>Sat, 19 Oct 2024 23:22:32 GMT</pubDate>
    </item>
    <item>
      <title>[付费] 需要有人写一篇关于线性二次 (LQ) 最优控制的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7ivyn/paid_need_someone_to_do_a_paper_on/</link>
      <description><![CDATA[您好，我想找人帮我写一篇关于线性二次 (LQ) 最优控制强化学习机制的论文。我有更多细节可以在 DM 中分享。愿意为这项任务支付 100 美元。 相信我，我从来没有这样做过，但说实话，我应该在 3 年前完成这项作业，而现在我只想提交论文来获得班级成绩并获得学位。我实际上在班级考试中表现很好，只需要写这篇论文来完成手续。谢谢    提交人    /u/Used_Chapter007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7ivyn/paid_need_someone_to_do_a_paper_on/</guid>
      <pubDate>Sat, 19 Oct 2024 21:14:58 GMT</pubDate>
    </item>
    <item>
      <title>帮助-TD3 仅返回极值（即动作空间的边界值）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g77fs3/help_td3_only_returns_extreme_values_ie_bounding/</link>
      <description><![CDATA[嗨， 我对连续控制问题还不熟悉，由于我的背景，我更了解理论而不是实践方面。所以我正在训练一个基于 TD3 的代理来解决连续控制问题（在观察空间中交易带有情绪分数的多种资产）。  连续动作空间（如下所示）如下所示： Box([-1. -1. -1. -1. -1. -1. -1. -1. -1. 0. 0. 0. 0. 0. 0. 0. 0.], 1.0, (16,), float32) 解释一下：我在环境中交易 8 种资产，动作空间的前 8 个条目（范围从 -1 到 1）表示仓位（卖出、持有、买入 —— 在环境中从连续决策转化为离散决策），而后 8 个条目（范围从 0 到 1）表示动作的百分比金额（卖出仓位的百分比或用于买入动作的现金的百分比）。  我的模型目前在 100 集上进行训练（一集大约有 1250 个交易日/观察，大小为 81，这里只是简要介绍一下这个项目）。目前，代理只会无一例外地返回进入极端位置的动作（使用来自动作空间的边界值）。示例： [ 1. -1. 1. -1. 1. -1. -1. -1. 0. 0. 0. 1. 1. 0. 0. 0.] 我现在的问题是，在训练的早期阶段，这是否正常，或者这是否表明模型、环境或其他方面存在问题？由于在这样的环境中训练需要大量计算（= 成本高），我只想在训练（并可能支付）大量时间的训练之前澄清这是否可能是代码/算法本身的问题。    提交人    /u/Intelligent-Put1607   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g77fs3/help_td3_only_returns_extreme_values_ie_bounding/</guid>
      <pubDate>Sat, 19 Oct 2024 12:07:16 GMT</pubDate>
    </item>
    <item>
      <title>与我一起学习/合作从零开始学习 DRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g713de/study_collab_with_me_learning_drl_from_almost/</link>
      <description><![CDATA[大家好 👋 我几乎从零开始学习 DRL。对 NN、反向传播、LSTM 有一些了解，并使用我在互联网上能找到的任何东西（非常简单的东西）制作了一些模型。没有 SOTA。现在从“grokking DRL”这本书中学习。我有一种不同的方法来设计交易引擎，我正在用 golang（为了提高效率和扩展性）和 python（用于 ML 部分）构建它，还有很多东西需要解开。我认为我有一些关于交易的有趣想法可以在 DRL、LSTM 和 NEAT 中测试，但至少需要 6-8 个月才能产生任何有成效的东西。我正在寻找好奇的人一起工作。如果您愿意研究一些新的假设，只需按下 DM。我希望得到一些关于 DRL 的指导，理解已完成工作背后的所有理论非常耗时。 PS：如果您很了解这些内容并希望提供帮助，我可以在任何程度上帮助您处理数据结构、Web 开发、系统设计，如果您愿意的话。只是说说而已。    提交人    /u/WarBroWar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g713de/study_collab_with_me_learning_drl_from_almost/</guid>
      <pubDate>Sat, 19 Oct 2024 04:26:32 GMT</pubDate>
    </item>
    <item>
      <title>有人可以帮忙吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6pi8d/can_anyone_help/</link>
      <description><![CDATA[https://www.reddit.com/r/MachineLearning/s/8Vp8b3bGqI    由   提交  /u/Alarming-Power-813   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6pi8d/can_anyone_help/</guid>
      <pubDate>Fri, 18 Oct 2024 18:44:29 GMT</pubDate>
    </item>
    <item>
      <title>演员 评论家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6mlv2/actor_critic/</link>
      <description><![CDATA[https://arxiv.org/abs/1704.03732 是否有任何演员-评论家类似物可以将专家演示整合到演员-评论家学习中，就像 DQN 一样？    提交人    /u/Key-Faithlessness113   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6mlv2/actor_critic/</guid>
      <pubDate>Fri, 18 Oct 2024 16:40:32 GMT</pubDate>
    </item>
    <item>
      <title>我是 RL/DRL 的初学者。我有兴趣了解如何使用 DRL 解决非凸甚至凸优化问题（受约束或不受约束）。如果可能的话，有人可以分享使用 DRL 解决的代码吗...</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6he0y/i_am_a_beginner_to_rldrl_i_am_interested_to_know/</link>
      <description><![CDATA[我是 RL/DRL 的初学者。我有兴趣了解如何使用 DRL 解决非凸甚至凸优化问题（受约束或不受约束）。如果可能的话，有人可以分享使用 DRL 解决的代码吗？问题如下 最小化 (x + y-2)^2 受 xy &lt; 10 约束 并且 xy &gt; 1 x 和 y 是一些标量 以上是一个示例问题。也可以建议任何其他示例。但请保持建议和代码简单、可读且易懂。 -------------------- 更新 ------------------------------- * CVX / CVXPY 可以有效地解决它。 * 我对 SCA/SDP/AO 解决优化问题有非常基本的了解 * 我对 DRL / RL / 监督学习解决这个问题的方法很好奇......纯粹的好奇心而不是效率 * 我的思维方式是朝着例如多播波束成形..... minimize_{w} || w ||_2^2 &lt;-- 最小化功率 s.t. SINR(w) &gt;= 1（例如） 或其 QCQP 形式 min ||w||_2^2 s.t. w^T H_k w &gt;= 1 其中 H_k = h_k h_k^H， h_k = 从多天线基站到单天线用户的信道（从任何论文中获取任何信道函数） w \in C^{Nx1}​​ 用于 N 天线基站的波束成形向量.... 这个问题可以通过 SDP/SDR 方法轻松解决....但我正在寻找 ML 替代方案....在 pytorch 中任何进一步的帮助（编码）...都会很棒 ***** 我感谢已经做出贡献和正在做出贡献的成员 ************* @Human_Professional94 @Reasonable-Bee-7041 @Md_zouzou @BAKA_04    由    /u/gudduarnav  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6he0y/i_am_a_beginner_to_rldrl_i_am_interested_to_know/</guid>
      <pubDate>Fri, 18 Oct 2024 12:55:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 学生课程主题材料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6d5g9/material_on_topics_of_rl_for_student_course/</link>
      <description><![CDATA[我正在开设 RL 入门课程，希望学生熟悉给定的主题，然后将其介绍给剩余的课程。 为此，我正在寻找好的论文/文章/资源，理想情况下这些论文/文章/资源易于理解并提供有关该主题的良好概述。请分享任何适合这些主题的资源：  稀疏奖励 Sim2Real 可解释和可解释的 RL     提交人    /u/EmbarrassedCause3881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6d5g9/material_on_topics_of_rl_for_student_course/</guid>
      <pubDate>Fri, 18 Oct 2024 08:20:25 GMT</pubDate>
    </item>
    <item>
      <title>“MLE-bench：在机器学习工程中评估机器学习代理”，Chan 等人 2024 {OA}（Kaggle 扩展）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5yr0p/mlebench_evaluating_machine_learning_agents_on/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5yr0p/mlebench_evaluating_machine_learning_agents_on/</guid>
      <pubDate>Thu, 17 Oct 2024 19:09:33 GMT</pubDate>
    </item>
    <item>
      <title>RL 用于系统的最优控制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5wecg/rl_for_optimal_control_of_systems/</link>
      <description><![CDATA[我最近看到了这篇 IEEE 论文，标题为“基于强化学习的使用 Carleman 线性化的非线性系统近似最优控制”。看起来他们在非线性系统的近似上使用某种形式的强化控制，并且与线性 RL 相比表现出良好的性能。 有人对这种 Carleman 近似方法有什么见解吗？    提交人    /u/Playful_Passage_2985   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5wecg/rl_for_optimal_control_of_systems/</guid>
      <pubDate>Thu, 17 Oct 2024 17:29:13 GMT</pubDate>
    </item>
    <item>
      <title>ADAS 的 RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5r81b/rl_implementation_for_adas/</link>
      <description><![CDATA[嘿。我想探索使用 RL 模型（本质上是一种基于奖励的模型）开发 ADAS 功能（如 FCW 或 ACC）的可能性，在这些功能中，将发出警告，并根据车辆采取的行动给予奖励。我希望有人能指导我如何做到这一点？我想使用 CARLA 来构建我的环境。     提交人    /u/SignificanceMotor285   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5r81b/rl_implementation_for_adas/</guid>
      <pubDate>Thu, 17 Oct 2024 13:44:54 GMT</pubDate>
    </item>
    <item>
      <title>从 DQN 到 Double DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5p8mi/from_dqn_to_double_dqn/</link>
      <description><![CDATA[我已经有一个 DQN 的实现。要将其更改为双 DQN，似乎只需要进行一点小改动：在 Q 值更新中，下一个状态（最佳）动作选择和该动作的评估均由 DQN 中的目标网络完成。而在双 DQN 中，下一个状态（最佳）动作选择由主网络完成，但该动作的评估由目标网络完成。 这似乎相当简单。我还遗漏了什么吗？    提交人    /u/No_Addition5961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5p8mi/from_dqn_to_double_dqn/</guid>
      <pubDate>Thu, 17 Oct 2024 12:02:35 GMT</pubDate>
    </item>
    <item>
      <title>何时使用强化学习，何时不使用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5mge9/when_to_use_reinforcement_learning_and_when_to/</link>
      <description><![CDATA[何时使用强化学习，何时不使用。我的意思是何时使用普通数据集来训练模型，何时使用强化学习     提交人    /u/Alarming-Power-813   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5mge9/when_to_use_reinforcement_learning_and_when_to/</guid>
      <pubDate>Thu, 17 Oct 2024 08:54:47 GMT</pubDate>
    </item>
    <item>
      <title>Chi Jin 的普林斯顿 RL 课程好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5leg1/is_chi_jins_princeton_rl_course_good/</link>
      <description><![CDATA[普林斯顿大学 ECE524 强化学习基础课程，2024 年春季。  本课程为研究生课程，重点介绍强化学习的理论基础。它涵盖马尔可夫决策过程 (MDP) 的基础知识、基于动态规划的算法、规划、探索、信息理论下界以及如何利用离线数据。还讨论了各种高级主题，包括策略优化、函数逼近、多机构和部分可观测性。本课程特别强调算法及其理论分析。需要具备线性代数、概率和统计学方面的先验知识。    提交人    /u/sahoosubramanyam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5leg1/is_chi_jins_princeton_rl_course_good/</guid>
      <pubDate>Thu, 17 Oct 2024 07:30:33 GMT</pubDate>
    </item>
    <item>
      <title>使用多智能体 RL 代理优化分布式系统中的工作平衡/通信</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5avgf/using_multiagent_rl_agents_for_optimizing_work/</link>
      <description><![CDATA[我偶然发现了这篇名为 &quot;负载平衡并行粒子追踪的强化学习&quot; 的论文，它让我绞尽脑汁。他们使用多智能体 RL 在分布式系统中实现负载平衡，但我不确定这是否可行。 以下是本文的要点：  他们使用多智能体 RL 来平衡工作负载并优化并行粒子追踪中的通信 每个进程（最多 16,384 个！）都有自己的 RL 代理（用于其策略网络的单层感知器） 代理的操作是在进程之间移动工作块以平衡事物  我听说多智能体 RL 很难正常工作？有了这么多进程，由于每个代理都可能决定将工作转移到数千个其他进程中的任何一个，因此操作空间不是非常巨大吗？ 所以，我的问题是：这真的可行吗？或者，动作空间太大，无法在实践中发挥作用？我很想听听任何有 RL 或并行计算经验的人的意见。我是否遗漏了什么，或者这听起来很疯狂？ 谢谢！P.S. 如果有人真的尝试过这样的事情，我会非常感兴趣听听它进展如何！    提交人    /u/ypsoh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5avgf/using_multiagent_rl_agents_for_optimizing_work/</guid>
      <pubDate>Wed, 16 Oct 2024 21:39:23 GMT</pubDate>
    </item>
    </channel>
</rss>