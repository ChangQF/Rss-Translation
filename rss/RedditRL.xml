<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 13 Dec 2024 18:24:34 GMT</lastBuildDate>
    <item>
      <title>Sb3 中的 DummyVecEnv 导致 API 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hdcgdi/dummyvecenv_from_sb3_causes_api_problems/</link>
      <description><![CDATA[嗨 :) 我按照 gym 界面构建了一个自定义环境。step、reset 和 action_mask 方法调用我的棋盘游戏在 java 中提供的 Rest-Endpoint。sb3 中的 check_env 方法运行没有问题，但是当我尝试在该环境上训练代理时，我收到 HTTP 500 Server Errros。我认为这是因为 sb3 从我的 CustomEnv 创建了一个 DummyVecEnv，并且 API 仅支持一次运行一个游戏。有没有办法不使用 DummyVecEnv？我知道训练会变慢，但现在我只想让它正常工作 xD 如果有帮助，我可以分享错误日志，但我不想在这里发送太多文本... 提前谢谢 :)    提交人    /u/ItchyRoyal212   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hdcgdi/dummyvecenv_from_sb3_causes_api_problems/</guid>
      <pubDate>Fri, 13 Dec 2024 13:42:23 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tomb Engine API 帮助《古墓丽影》进行强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hdaz8y/help_with_reinforcement_learning_in_tomb_raider/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hdaz8y/help_with_reinforcement_learning_in_tomb_raider/</guid>
      <pubDate>Fri, 13 Dec 2024 12:18:17 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Lab 太疯狂了（Nvidia Omniverse）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd9byq/isaac_lab_is_insane_nvidia_omniverse/</link>
      <description><![CDATA[大家好，我最近对 ​​Nvidia Omniverse 及其 Isaac Lab（基于 Isaac Sim 构建）非常感兴趣。它对于强化学习非常强大，你一定要看看。 我甚至有动力制作一个视频来展示它的用例（我不知道是否可以在这里上传）。 https://www.youtube.com/watch?v=NfNC03rZssU    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd9byq/isaac_lab_is_insane_nvidia_omniverse/</guid>
      <pubDate>Fri, 13 Dec 2024 10:23:46 GMT</pubDate>
    </item>
    <item>
      <title>按 NeurIPS 2024 论文数量计算，强化学习是第三受欢迎的领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd9bac/rl_is_the_third_most_popular_area_by_number_of/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd9bac/rl_is_the_third_most_popular_area_by_number_of/</guid>
      <pubDate>Fri, 13 Dec 2024 10:22:17 GMT</pubDate>
    </item>
    <item>
      <title>寻求强化学习（RL）个人项目的想法和指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd5ef7/looking_for_ideas_and_guidance_for_personal/</link>
      <description><![CDATA[大家好！ 我刚刚完成硕士课程的第一年，并获得了计算机科学学士学位，主修人工智能。在过去的几年里，我通过工作、实习和研究获得了丰富的经验，特别是在我真正喜欢的领域，比如应用于车辆和无人机系统的强化学习 (RL)。 现在，我希望深入研究 RL 中的个人项目，以探索新的想法并加深我的知识。您对有趣的基于 RL 的个人项目有什么建议吗？我特别喜欢涉及机器人、无人机或自主系统的项目，但我也愿意接受任何有创意的建议。 此外，我很想听听关于如何开始个人 RL 项目的建议——您会向处于我这个位置的人推荐哪些工具、框架或资源？我认为自己非常精通 Python 及其相关知识。 提前感谢您的想法和提示！    提交人    /u/CJPeso   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd5ef7/looking_for_ideas_and_guidance_for_personal/</guid>
      <pubDate>Fri, 13 Dec 2024 05:30:06 GMT</pubDate>
    </item>
    <item>
      <title>学术背景调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcw8aa/academic_background_poll/</link>
      <description><![CDATA[大家好， 出于好奇，我想看看这里社区的背景分布是怎样的。 ​ 查看投票    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcw8aa/academic_background_poll/</guid>
      <pubDate>Thu, 12 Dec 2024 21:42:12 GMT</pubDate>
    </item>
    <item>
      <title>寻找远程实习：2025 年冬季</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcqe00/looking_for_remote_internship_winter_2025/</link>
      <description><![CDATA[大家好！ 我是来自印度的机器学习专业三年级博士生，专攻强化学习。我还是 Google DeepMind 的学生研究员，将于 2025 年加入 Adob​​e 进行暑期实习。 我正在寻找 2025 年冬季远程学生研究员职位，研究与 多臂老虎机 (MAB) 和 马尔可夫决策过程 (MDP) 相关的问题。 我的研究重点是开发用于老虎机优化和强化学习的有效算法，并在成本敏感型决策和策略优化中具有实际应用。我还可能通过涉及在 LLM 背景下应用老虎机的项目亲身体验 LLM 的实践。 如果您的组织正在研究类似的问题并有合作机会，我很乐意做出贡献。请随时给我发私信或分享相关线索。 感谢您的时间和考虑！    提交人    /u/Fantastic-Nerve-4056   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcqe00/looking_for_remote_internship_winter_2025/</guid>
      <pubDate>Thu, 12 Dec 2024 17:32:10 GMT</pubDate>
    </item>
    <item>
      <title>在整个轨迹中改变观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</link>
      <description><![CDATA[嗨， 有人知道关于在轨迹过程中代理的观察空间的场景的任何先前工作吗？ 例如，如果具有多个传感器的机器人决定在轨迹期间转动其中一个传感器（可能出于能量考虑）。  据我所知，最常用的算法没有考虑到轨迹过程中观察空间的变化。 很想听听大家的想法    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</guid>
      <pubDate>Thu, 12 Dec 2024 14:20:01 GMT</pubDate>
    </item>
    <item>
      <title>可视化环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcknsm/visualizing_environments/</link>
      <description><![CDATA[如何将体育馆环境可视化？有哪些好的做法？    提交人    /u/Main-Bit-6404   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcknsm/visualizing_environments/</guid>
      <pubDate>Thu, 12 Dec 2024 13:04:59 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 MATD3 和 MADDPG 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</link>
      <description><![CDATA[问候， 我需要在某些环境中运行这两个算法（无所谓）来证明多智能体学习确实有效！（是的，这非常简单，但很难！） 问题就在这里。找不到一个单一的框架来在环境中植入算法（现在基本上是宠物动物园 mpe）， 我做了一些研究：  Marllib 没有很好的文档记录。最后我还是搞不懂。 agileRL 很棒但是有一个 bug 我无法解决，（如果您能解决这个 bug 请告诉我）。 Thianshou ，我必须植入算法！！ CleanRL，嗯...我没搞懂。我的意思是我应该在我的主脚本中使用这些算法 .py 文件吗？  好吧请帮忙.......... 带着爱    提交人    /u/matin1099   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</guid>
      <pubDate>Thu, 12 Dec 2024 11:50:04 GMT</pubDate>
    </item>
    <item>
      <title>针对机械臂 RL 项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</link>
      <description><![CDATA[嗨，我正在寻找一些论文或想法，我可以在硬件机器人操纵器上尝试，以获得 RL 控制和规划方面的经验。     提交人    /u/Stunning-Stable-8553   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</guid>
      <pubDate>Thu, 12 Dec 2024 02:57:44 GMT</pubDate>
    </item>
    <item>
      <title>体育馆/mujoco 教程需要四足机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</link>
      <description><![CDATA[大家好，我正在做一个关于四足机器狗的项目。我正在尝试使用 gymnasium 和 MuJoCo，但在 gymnasium 上设置自定义环境真的很令人困惑。我正在寻找一个教程，这样我就可以学习如何设置它，或者是否有人建议我应该切换我正在使用的工具。    提交人    /u/PrincipleDistinct425   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</guid>
      <pubDate>Wed, 11 Dec 2024 21:48:47 GMT</pubDate>
    </item>
    <item>
      <title>Rl 用于自主导航</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</link>
      <description><![CDATA[大家好，我打算在我的最后一年项目中使用 ubuntu 22.04 和 ros2 hum，该项目包括使用 DRL 技术在 Gazebo 中为 turtlebot3 机器人进行自主导航，但我真的很担心我的笔记本电脑的容量，我真的没有合适的笔记本电脑，我有一台第 8 代英特尔 i5，8 核，我真的会发现学习问题吗？    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</guid>
      <pubDate>Wed, 11 Dec 2024 19:10:28 GMT</pubDate>
    </item>
    <item>
      <title>彼得·墨菲 (Peter Murphy) 的最新强化学习书籍有多好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</link>
      <description><![CDATA[编辑：应该是 Kevin Murphy。 我的一位同事推荐了 https://arxiv.org/pdf/2412.05265。 我发现它有点像洗衣清单，就像其他强化学习调查的情况一样。不同的想法感觉就像反复试验。我自己过去也曾在 TensorFlow 中编写过强化学习。但很难真正感受到它的威力。我有数学背景，我只是不确定是否值得花时间阅读这么厚的书，因为我知道我可能记不住很多东西，除非所有不同的概念形成连贯的意识流。换句话说，我发现这个主题没有足够的基础来轻松消化第一原理。 我对其他人对这个问题的看法很好奇，特别是从第一原理的角度来看。    提交人    /u/Crazy_Suspect_9512   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</guid>
      <pubDate>Wed, 11 Dec 2024 18:43:29 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 在我的用例中存在问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</link>
      <description><![CDATA[大家好， 这是我第一次从事 RL 项目，我正在构建一个可以与特定 DLT 一起使用的模型。具体来说，我希望它选择最佳数量的块来通过特定的 DLT 发送消息。我尝试了不同的算法，但由于它必须自主选择动作并且不受限制，所以我选择了 DDPG 方法。 然而，让我很困惑的是，对于我构建的特定奖励系统，对于单次训练运行（不更新模型），模型有时会学习，有时不会。这意味着对于大多数运行，模型不会探索选项，它将坚持发送消息所需的最少块数。而在较少的情况下，它似乎在学习，但仅此而已。下次我运行代码时，它可能会返回到选择最小数量的块。 不确定这是否与奖励系统、Actor - Critic 网络的架构或算法本身有关。但我很感激一些指导。非常感谢！    提交人    /u/LionTheAlpha   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</guid>
      <pubDate>Wed, 11 Dec 2024 08:57:42 GMT</pubDate>
    </item>
    </channel>
</rss>