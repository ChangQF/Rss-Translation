<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 11 Apr 2024 09:14:17 GMT</lastBuildDate>
    <item>
      <title>关于A2C实现中的损失和优化器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c18roz/about_the_loss_and_optimizer_in_a2c_implementation/</link>
      <description><![CDATA[我目前正在实现 A2C 算法，在代码实现中遇到了关于损失和优化器的困惑。 当我第一次使用时学习了A2C，我提到的代码为actor和critic分配单独的优化器，并分别优化，就像：```Python def learn(self,transition_dict): states = torch.tensor(np.array(transition_dict[&#39;states) &#39;]), dtype=torch.float).view(-1, self.state_dim).to(self.device) # 将 ndarray 列表转换为 ndarray，因为将 ndarray 列表转换为张量非常慢 actions = torch.tensor (transition_dict[&#39;actions&#39;], dtype=torch.long).view(-1, 1).to(self.device)  返回，优点 = self.compute_returns_and_advantages(transition_dict, self .lamda) log_probs = torch.log(self.actor(states).gather(1, actions)) # actions中的元素必须是下标才能使用gather() actor_loss = torch.mean(-log_probs *优点.detach()) # detach()防止临时加载，共享数据内存，值相同 # td_error 与 actor 参数无关，可以看作常量 Critical_loss = F.mse_loss(self.critic(states), returns.detach()) # 准备update self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() # 计算梯度 actor_loss.backward()ritic_loss.backward() # 更新权重 self.actor_optimizer.step() self.critic_optimizer.step() &lt; /pre&gt;  但当我回顾流行的开源 RL 库中的代码时，我发现它们都使用了针对 actor 和 critic 的通用优化器，并对 actor、值和熵的损失求和来进行优化。 [stable-baselines3](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/a2c/a2c.py#L132) 的实现如下： Python def train(自我）-&gt;无：“”使用当前收集的推出缓冲区更新策略（整个数据上的一个梯度步骤）。 ”“” # 切换到训练模式（这会影响批量标准化/dropout） self.policy.set_training_mode(True)  # 更新优化器学习率 self._update_learning_rate(self.policy.optimizer) # 这只会循环一次（一次性获取所有数据） for rollout_data in self.rollout_buffer.get(batch_size=None): actions = rollout_data.actions if isinstance(self.action_space,spaces.Discrete): # 将离散动作从浮点动作转换为长动作= actions.long().flatten()values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)values =values.flatten() # 标准化优势（原始实现中不存在）优点 = rollout_data.优点 if self.normalize_advantage: 优点 = (优点 - 优点.mean()) / (advantages.std() + 1e-8) # 策略梯度损失 policy_loss = -(advantages * log_prob).mean() # 使用TD(gae_lambda) target value_loss = F.mse_loss(rollout_data.returns, value) # 熵损失有利于探索 if entropy is None: # 无分析形式时的近似熵 entropy_loss = -th.mean(-log_prob) else: entropy_loss = -th .mean(entropy) loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss # 优化步骤 self.policy.optimizer.zero_grad() loss.backward() # 剪辑梯度范数 th.nn.utils.clip_grad_norm_(self .policy.parameters(), self.max_grad_norm) self.policy.optimizer.step()  ``` 我的问题：为什么要实现 Actor-Critic这样？是为了方便吗？还是为了性能？   由   提交 /u/Awkward_Swimmer_5649   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c18roz/about_the_loss_and_optimizer_in_a2c_implementation/</guid>
      <pubDate>Thu, 11 Apr 2024 07:07:36 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 RL 录取的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c17bwu/need_advice_on_an_rl_admit/</link>
      <description><![CDATA[大家好，我在巴塞罗那 UPF 申请了智能信息系统硕士学位，我认为我已被接受，因为申请门户状态显示已接受，但是尚未收到任何正式的邮件沟通。 我对强化学习非常感兴趣，Reddit 上的一篇帖子向我推荐了这所大学，而且这里的教授似乎也非常好。 我想知道我在这里是否有良好的学习范围和机会。我的最终目标是成为一名教授。 我唯一怀疑的原因是这所大学相当年轻~ 30 年。而欧洲其他大学已有一百多年的历史。 这是我唯一可能承认的，我曾向其他 3 个地方申请直接博士学位，但都被拒绝了。 大家觉得怎么样？如果我获得一门课程或申请 2025 年 9 月开始的硕士课程，我应该接受录取吗？ 如有任何帮助，我们将不胜感激，谢谢！   由   提交 /u/FlyTrain1011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c17bwu/need_advice_on_an_rl_admit/</guid>
      <pubDate>Thu, 11 Apr 2024 05:38:25 GMT</pubDate>
    </item>
    <item>
      <title>NEAT + Q 学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c16gry/neat_q_learning/</link>
      <description><![CDATA[我一直在寻找将 NEAT 与 DQN 结合使用的论文，但没有找到。我想知道这对于 OpenAI Gym 中的 2D 自动驾驶汽车来说是否是个好主意。   由   提交/u/fa_anony__mous   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c16gry/neat_q_learning/</guid>
      <pubDate>Thu, 11 Apr 2024 04:47:41 GMT</pubDate>
    </item>
    <item>
      <title>无模型强化学习的奖励。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c147yn/reward_in_modelfree_reinforcement_learning/</link>
      <description><![CDATA[嗨，我看到有些论文把 Reward 写成 R --&gt; S X A X S&#39; 和其他一些使用 R-&gt; 的论文S X A 哪一个对于无模型强化学习有效？   由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c147yn/reward_in_modelfree_reinforcement_learning/</guid>
      <pubDate>Thu, 11 Apr 2024 02:46:20 GMT</pubDate>
    </item>
    <item>
      <title>有人让 mujoco-py 在 Linux 上运行吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0zytk/has_anyone_gotten_mujocopy_working_on_linux/</link>
      <description><![CDATA[我的一位同事给了我一些使用“pen- human-v1”的代码数据集，它是需要 Mujoco（特别是 Mujoco-py）的 3d Gym 环境之一。 Mujoco-py 已经有 7 年历史了，似乎已经过时了 - 我无法让它在 Windows 或 Linux 上运行（我认为我的同事在 Mac 上）。我相信 Gymnasium 现在使用最新的 Mujoco 绑定，不需要一堆额外的安装步骤，但尚不清楚如何使 d4rl 使用 Gymnasium 而不是 Gym。所以我不确定人们如何让 d4rl 在 Linux 上工作（使用不同的发行版会更好吗？）   由   提交/u/hearthstoneplayer100  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0zytk/has_anyone_gotten_mujocopy_working_on_linux/</guid>
      <pubDate>Wed, 10 Apr 2024 23:26:48 GMT</pubDate>
    </item>
    <item>
      <title>如何在训练期间将变化的伽玛传递给 DQN 或 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0q5o6/how_to_pass_a_varying_gamma_to_dqn_or_ppo_during/</link>
      <description><![CDATA[强化学习和 SB3 实现应用典型的常量 gamma 在学习时对未来值进行折扣。这对于离散时间环境来说很好，其中对于每个操作，未来值都会被折扣为每个步骤的常数。 我有一个自定义的健身房环境，其中我的环境在离散决策时期中步进，但每个操作都需要一个不同的时间量。因此，以恒定比率贴现未来价值是不正确的。我需要做的是用 gamma 来折扣未来值，gamma 是在环境中执行操作所需时间的函数。 无论如何，是否可以将 gamma 作为函数或张量传递在学习过程中映射到每个 (s, a, s&#39;, r) 元组？也许可以使用现有功能或回调？如果可能的话，我希望避免分叉存储库。 任何意见都将不胜感激，因为我已经在这方面坚持了一段时间了。提前致谢！   由   提交 /u/Real_Zesty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0q5o6/how_to_pass_a_varying_gamma_to_dqn_or_ppo_during/</guid>
      <pubDate>Wed, 10 Apr 2024 16:44:13 GMT</pubDate>
    </item>
    <item>
      <title>如何将强化学习与 GYM 库应用于 Flappy Bird 等 2D 视频游戏。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0nx8l/how_to_apply_reinforcement_learning_with_gym/</link>
      <description><![CDATA[我使用算法创建了一个 flappy Bird 机器人。它工作得很好并且可以无限运行。现在我想尝试使用强化学习来做到这一点，看看它能提高多少。我在 Python 中发现了一个名为 Gym 的库，它似乎可以简化事情。我尝试在网上寻找类似的示例，但找不到任何适合初学者创建自己的环境的内容。所以我希望有人能引导我朝着正确的方向去做这件事。 请注意，这是我制作的离线游戏，不违反任何条款。 我已经可以在坐标中检测到屏幕上的鸟和障碍物（参见下面的代码）。所以我确实有数据，我只需要围绕它编写机器学习部分。 # 游戏窗口截图 game_frame = pyautogui.screenshot(region=(200, 190, 500 , 860)) deflocate_flappy_bird(game_frame): # 找到 flappy 鸟。 Returns X,Y flappy_bird_location = pyautogui.locate(“800px-1080pxBirdEye2.jpg”, game_frame,confidence=0.85) return flappy_bird_location deflocate_obstacles(game_frame): # 定位屏幕上的所有障碍物。返回 pyautogui.locateAll(“Obstacle_Ball.jpg”, game_frame,confidence=0.90) 中障碍物的 X,Y 距离 = pow(10, 2) 障碍物位置 = [] 列表: if all(abs(obstacle.top - y[ 0]) &gt; y 在障碍物位置中的距离): 障碍物位置.append((int(obstacle.top), int(obstacle.left))) 返回障碍物位置 current_location_flappy =locate_flappy_bird(game_frame) 障碍物位置 =locate_obstacles(game_frame) # flappy 位置( x, y) = [(240, 20)] # 障碍物位置(x ,y) = [(218, 115), (361, 115), (566, 334), (727, 334)]     由   提交/u/pubGGWP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0nx8l/how_to_apply_reinforcement_learning_with_gym/</guid>
      <pubDate>Wed, 10 Apr 2024 15:11:00 GMT</pubDate>
    </item>
    <item>
      <title>训练无法进行到下一集，并且在到达第一集结束后 matlab 崩溃</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0h0hc/training_is_not_proceeding_to_the_next_episode/</link>
      <description><![CDATA[   大家好， 我正在使用用于训练热泵 Simulink 模型的 Matlab RL 工具箱。我已正确附加所有连接，因为我能够看到正在生成的奖励以及从 simulink 模型内的 FMU 中提取的观察结果。但我面临的主要问题是在结束一个剧集后，我的整个软件崩溃并关闭，因此我只能看到第一集正在运行。这是我收到的崩溃报告和我的模型的一些屏幕截图。  如果有人可以分享他们的观点，我可能做错了什么，那将是巨大的帮助。感谢您宝贵的时间。 ​ https://preview.redd.it/11yg0c4o9mtc1.png?width=2486&amp;format=png&amp;auto=webp&amp;s=a0ef73e0e56c0ed4bc328fa83b322a a73d3d667c  https://preview.redd.it/hefmuz6i9mtc1.png？ width=3426&amp;format=png&amp;auto=webp&amp;s=15eddef2f12fdffc8490753d1079237c3e2067eb ​ https://preview.redd.it/27m4mrvl9mtc1.png?width=2021&amp;format=png&amp;auto=web普和斯=4061a4bba776260c04a39e37a06c6e92b924729b https ://preview.redd.it/gcpj8svl9mtc1.png?width=2014&amp;format=png&amp;auto=webp&amp;s=cbb0b1c7e2868321250bfec935d274a252120497 https://preview.redd.it/1vz3xsvl9mtc1.png?width=2024&amp;format=png&amp; ;自动=webp&amp; ;s=c604061c27535460f5498636270aaf9968ecf025 https://preview.redd.it/p34c5uvl9mtc1.png?width=2033&amp;format=png&amp;auto=webp&amp;s=eefac9ab0919f9d5113bd5ec27f3a8d982b8969e   由   提交/u/slimshadyy18   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0h0hc/training_is_not_proceeding_to_the_next_episode/</guid>
      <pubDate>Wed, 10 Apr 2024 08:57:16 GMT</pubDate>
    </item>
    <item>
      <title>图着色问题中的奖励设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0c8jq/reward_design_in_graph_coloring_problem/</link>
      <description><![CDATA[        由   提交 /u/FangYu_   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0c8jq/reward_design_in_graph_coloring_problem/</guid>
      <pubDate>Wed, 10 Apr 2024 03:50:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的代码可以解决 Fetch&Reach，但无法解决 PointMaze-OpenDense？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c09xc5/why_my_code_can_solve_fetchreach_but_cant_solve/</link>
      <description><![CDATA[我上次也问过类似的问题，但我还是不明白为什么同样的代码不能解决这么简单的环境。 &lt; p&gt;该代码在 Fetch&amp;Reach 的稀疏奖励环境中完美运行，但它无法解决环境 PointMaze-Open，即使对于密集奖励的情况也是如此。我想他们的难度应该是差不多的。我想到的唯一可能的解释是动量。 PointMaze-Open 是因为动量而变得非常困难的环境吗？还是体育馆环境版本问题？ ​ 迷宫环境文档：https://robotics.farama.org/envs/maze/point_maze/ 我正在使用 https://github.com/TianhongDai/hindsight-experience-replay ​ ​ ​   由   提交 /u/Chips-HalfPrice   /u/Chips-HalfPrice reddit.com/r/reinforcementlearning/comments/1c09xc5/why_my_code_can_solve_fetchreach_but_cant_solve/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c09xc5/why_my_code_can_solve_fetchreach_but_cant_solve/</guid>
      <pubDate>Wed, 10 Apr 2024 01:56:11 GMT</pubDate>
    </item>
    <item>
      <title>PPO - 价值损失快速收敛且解释的方差始终为负</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzzbu0/ppo_value_loss_converges_quickly_explained/</link>
      <description><![CDATA[      我正在使用 sb3 训练交易 PPO 自定义环境，并进行 3 个离散操作（买入、卖出、持有），如下所示，价值损失从一开始就几乎收敛，而解释的方差始终为负，最终收敛到零。  就奖励而言，一开始它会以一个很好的速率增加，然后它会以非常小的速率持续增加（例如在 200k 步后从 0.12 增加到 0.14） https://preview.redd.it/9lo9e26sxhtc1.png ?width=1658&amp;format=png&amp;auto=webp&amp;s=0b8c9e282c4b50e6bb56be538a1be836af935078 我已经尝试了很多超参数调整：  学习率 熵系数 gamma gae_lambda  我有以下问题：  模型是否已经找到最优策略？ 我可以进行什么样的超参数调整来解决这个问题？     由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzzbu0/ppo_value_loss_converges_quickly_explained/</guid>
      <pubDate>Tue, 09 Apr 2024 18:26:40 GMT</pubDate>
    </item>
    <item>
      <title>Gazebo + ros2 RL环境（不含gym）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzw3ys/gazebo_ros2_rl_environment_without_gym/</link>
      <description><![CDATA[我实现了一个 RL 环境，允许您与 Gazebo 中的机器人交互。 与现有环境的区别： p&gt;  环境使用地图文件（一个带有二维整数数组的 Python 文件来实例化，该数组定义墙壁 (1)、起始位置 (2) 和可到达空间 (0)。 环境使用此地图来生成 Gazebo 使用的 .world 文件，并且还可以了解墙壁的位置，以便在可到达的区域中均匀地对目标进行采样。 环境的 __init__ 函数在单独的进程中启动 Gazebo，并自动检测 ros2 主题和服务，因此您只需安装/源 ros 并实例化环境即可。 环境与真实环境兼容-time rl（在gazebo模拟中没有暂停）或经典rl（在每一步环境都会播放x秒，然后在将新状态发送给代理之前再次暂停）。这可以通过__init__参数设置。  目前只有“irobot”机器人可用（一种扁平的海龟机器人），但添加新机器人应该不是什么大问题。 来源： https://github.com/hbonnavaud/RLFramework/tree/main/environments/robotic_environment   由   提交 /u/hbonnavaud   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzw3ys/gazebo_ros2_rl_environment_without_gym/</guid>
      <pubDate>Tue, 09 Apr 2024 16:14:34 GMT</pubDate>
    </item>
    <item>
      <title>IMPALA 实施与不断上升的批评损失：需要洞察！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzqheu/impala_implementation_with_rising_critic_loss/</link>
      <description><![CDATA[   我刚刚完成了一个 IMPALA 实施项目，在该项目中我通过套接字通信训练了多个参与者。这是分布式强化学习的一段令人着迷的旅程，但我遇到了一个障碍，我希望你能帮忙解决。 尽管演员表现出有希望的性能改进，但我看到了持续且令人不安的问题趋势：批评者损失不仅不稳定；它实际上随着时间的推移而增加。这很令人费解，因为随着训练的进行，你会期望更好的演员表现来稳定甚至减少批评者损失，对吧？ 另外，请继续关注😊我计划进一步探索分布式强化学习算法，并将与大家分享我的旅程。您今天的见解可能是该探索的重要组成部分！ 提前感谢您的想法和建议！ https://github.com/seolhokim/SimpleDistributedRL https://preview.redd.it/tv46xs5p5ktc1.png?width=580&amp;format=png&amp;auto=webp&amp;s=4dd916a4482b7d2c03a4d4 1b3d64dd46b73b972b   由   提交 /u/Spiritual_Fig3632   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzqheu/impala_implementation_with_rising_critic_loss/</guid>
      <pubDate>Tue, 09 Apr 2024 12:06:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q-learning 为健身房中的 MountainCar 提供奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzm0qp/reward_function_for_mountaincar_in_gym_using/</link>
      <description><![CDATA[大家好，我一直在尝试使用 Qlearning 训练代理来解决健身房中的 MountainCar 问题，但无法让我的代理到达旗帜。当我使用返回的默认奖励时，它永远不会到达标志（每一步为 -1，到达标志时为 0），我让它运行 200,000 集，但无法到达那里。所以，我尝试编写自己的奖励函数，我尝试了一堆 - 距离旗帜越近，奖励呈指数级越高，旗帜处的奖励越大，奖励ABS（加速度）和顶部的奖励越大，等等。但我只是无法让我的代理一路到达顶部 - 其中一个功能非常接近，就像非常接近，但随后决定全力深入潜水（可能是因为我奖励加速，但我设置了一个标志仅在第一次向左移动时奖励加速，但我的代理仍然决定向下潜）。我不明白，有人可以建议我应该如何解决它吗？ 我不知道我做错了什么，因为我在网上看到了教程，而且代理也在那里仅使用默认奖励就非常快（&lt;4000 集），我不知道为什么即使使用相同的参数也无法复制它。我非常感谢任何帮助和建议。 这是 github 链接 如果有人想看一下代码。 《Q-learning-山车》是应该工作的代码，与发布的 OpenAI 示例非常相似，但经过修改以在gym 0.26上工作； copy 和 new 是我一直在尝试奖励功能的地方。 非常感谢任何意见、指导或建议。提前致谢。 编辑：在评论中解决。如果有人来自未来并且面临与我相同的问题，解决的代码将上传到上面链接的 github 存储库。   由   提交/u/guccicupcake69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzm0qp/reward_function_for_mountaincar_in_gym_using/</guid>
      <pubDate>Tue, 09 Apr 2024 07:14:53 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>