<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 05 Dec 2023 12:26:27 GMT</lastBuildDate>
    <item>
      <title>深度强化学习中时间信用分配的调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bai3f/a_survey_of_temporal_credit_assignment_in_deep/</link>
      <description><![CDATA[https://arxiv.org/abs/2312.01072   由   提交/u/Conscious_Heron_9133   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bai3f/a_survey_of_temporal_credit_assignment_in_deep/</guid>
      <pubDate>Tue, 05 Dec 2023 12:02:23 GMT</pubDate>
    </item>
    <item>
      <title>持续学习：应用和前进之路</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18b8ial/continual_learning_applications_and_the_road/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2311.11908 OpenReview：https:// /openreview.net/forum?id=axBIMcGZn9 Dagstuhl 研讨会 23122 总结：https://drops.dagstuhl.de/entities/document/10.4230/DagRep.13.3.74 摘要&lt; /strong&gt;:  持续学习是机器学习的一个子领域，其目的是让机器学习模型能够不断地学习新数据，通过积累知识而不忘记过去学到的东西。在这项工作中，我们退一步问：“为什么人们首先应该关心持续学习？”。我们通过调查最近在三个主要机器学习会议上发表的持续学习论文来奠定基础，并表明内存受限的设置在该领域占据主导地位。然后，我们讨论了机器学习中的五个开放问题，尽管乍一看它们似乎与持续学习无关，但我们表明持续学习将不可避免地成为其解决方案的一部分。这些问题包括模型编辑、个性化、设备上学习、更快（重新）训练和强化学习。最后，通过比较这些未解决问题的需求和当前持续学习的假设，我们强调并讨论了持续学习研究的四个未来方向。我们希望这项工作为持续学习的未来提供一个有趣的视角，同时展示其潜在价值以及我们为了使其成功而必须追求的路径。这项工作是作者在 2023 年 3 月的 Dagstuhl 深度持续学习研讨会上进行多次讨论的结果。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18b8ial/continual_learning_applications_and_the_road/</guid>
      <pubDate>Tue, 05 Dec 2023 09:44:59 GMT</pubDate>
    </item>
    <item>
      <title>拼图解算器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18b4p6c/nonogram_solver/</link>
      <description><![CDATA[您好！我在 ML 和神经网络方面有一些经验，但这是我第一次涉足 RL。我希望能够解决非图（数字难题），并且我已经找到了一种将约束数据预处理为 10x10 浮点网格（0-1（含）之间）的方法。我希望输出是二进制值或布尔值的 10x10 网格，但是我不太确定如何构造它。我觉得我遇到的问题是如何维持状态。使用预处理网格作为“游戏板”并让代理在其顶部放置 0 和 1（在使浮动范围不包含之后）是一个好主意吗，即使这会删除该网格中的先前信息正方形？有没有一种方法可以为约束信息设置一个单独的网格，并为游戏板设置一个单独的网格来保持空间数据的相关性？欢迎任何想法，谢谢！   由   提交/u/FissioN47   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18b4p6c/nonogram_solver/</guid>
      <pubDate>Tue, 05 Dec 2023 05:22:30 GMT</pubDate>
    </item>
    <item>
      <title>“强化学习（不是）用于自然语言处理：自然语言策略优化的基准、基线和构建模块”，Ramamurthy 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18az4ey/is_reinforcement_learning_not_for_natural/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18az4ey/is_reinforcement_learning_not_for_natural/</guid>
      <pubDate>Tue, 05 Dec 2023 00:36:09 GMT</pubDate>
    </item>
    <item>
      <title>即使损失下降，PPO 中的代理也不会学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ax96z/agent_in_ppo_not_learning_even_when_loss_is/</link>
      <description><![CDATA[嗨，我正在尝试直接从原始论文实现 PPO，所以我不想复制别人的代码。不幸的是，我遇到了一个问题，即演员和评论家的损失正在下降，但剧集分数却没有改善。我正在 pendulum-v1 上测试 PPO，平均剧集分数约为 -1200。如果有人能告诉我我做错了什么，我将不胜感激。 这是我正在运行的代码：https://github.com/stanislawraczk/PPO/tree/main 提前感谢您的帮助:)   由   提交 /u/stachursky   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ax96z/agent_in_ppo_not_learning_even_when_loss_is/</guid>
      <pubDate>Mon, 04 Dec 2023 23:12:22 GMT</pubDate>
    </item>
    <item>
      <title>解释 RLlib 中 PPO 的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18a930l/explaining_the_metrics_for_ppo_in_rllib/</link>
      <description><![CDATA[       您能否解释一下 Ray Rllib 图中的policy_loss、vf_loss 和total_loss 的预期趋势？目前，我的保单损失为负。它先下降然后增加，我认为这是由于高学习率造成的。 Total_loss 和 vf_loss 均为正且递减。  除了损失和平均奖励之外，我还应该关注哪些值来确定我的策略是否经过训练并表现良好？ ​  https://preview.redd.it/pcrssmq0m64c1.png？ width=354&amp;format=png&amp;auto=webp&amp;s=6f5b8150ad5fa7872168e6a2a2eef13647a4fdfd   由   提交 /u/Beautiful_Basis8441   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18a930l/explaining_the_metrics_for_ppo_in_rllib/</guid>
      <pubDate>Mon, 04 Dec 2023 01:34:53 GMT</pubDate>
    </item>
    <item>
      <title>帮我解决依赖问题！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependencies_issue/</link>
      <description><![CDATA[尝试在 google colab 上运行 DQN pong。  我使用了这行代码作为依赖项： !pip installgym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate ==3.1.* ​ 我收到此错误消息： 收集gym[box2d]==0.17.* 使用缓存的gym -0.17.3-py3-none-any.whl 已满足要求：/usr/local/lib/python3.10/dist-packages 中的 pyvirtualdisplay==0.2.* (0.2.5) 已满足要求：PyOpenGL==3.1 .* 在 /usr/local/lib/python3.10/dist-packages (3.1.7) 已满足要求： PyOpenGL-accelerate==3.1.* 在 /usr/local/lib/python3.10/dist-packages ( 3.1.7）已满足要求：/usr/local/lib/python3.10/dist-packages中的scipy（来自gym[box2d]==0.17.*）（1.11.4）已满足要求：numpy&gt;=1.10。 4 /usr/local/lib/python3.10/dist-packages (来自gym[box2d]==0.17.*) (1.23.5) 收集 pyglet&lt;=1.5.0,&gt;=1.4.0 (来自gym [box2d]==0.17.*) 使用缓存的 pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB) 收集 cloudpickle&lt;1.7.0,&gt;=1.2.0 (来自gym[box2d]= =0.17.*) 使用缓存的cloudpickle-1.6.0-py3-none-any.whl (23 kB) 收集box2d-py~=2.3.5 (来自gym[box2d]==0.17.*) 使用缓存的box2d-py -2.3.8.tar.gz (374 kB) 正在准备元数据 (setup.py) ... 已完成 要求已满足： /usr/local/lib/python3.10/dist-packages 中的 EasyProcess （来自 pyvirtualdisplay==0.2。 *）（1.1）已满足要求：未来在 /usr/local/lib/python3.10/dist-packages 中（来自 pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym[box2d]==0.17 .*) (0.18.3) 为收集的包构建轮子：box2d-py 错误：subprocess-exited-with-error × python setup.py bdist_wheel 未成功运行。 │ 退出码：1 ╰─&gt;请参阅上面的输出。  注意：此错误源自子进程，并且可能不是 pip 的问题。为 box2d-py 构建轮子（setup.py）...错误错误：为 box2d-py 构建轮子失败为 box2d-py 运行 setup.py clean 无法构建 box2d-py 错误：无法为 box2d-py 构建轮子，这是安装基于 pyproject.toml 的项目所必需的 ​ 任何帮助将不胜感激！   由   提交 /u/Opening-Ocelot1748    reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependency_issue/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependencies_issue/</guid>
      <pubDate>Sun, 03 Dec 2023 23:34:08 GMT</pubDate>
    </item>
    <item>
      <title>在规划中结合空间和时间抽象以实现更好的泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189r06i/combining_spatial_and_temporal_abstraction_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.00229 OpenReview：https:// /openreview.net/forum?id=eo9dHwtTFt 代码： https://github.com/mila-iqia/skipper 博客文章：http://mingde.world/combining-spatial-and-temporal-abstraction-in-planning/ 摘要:  受人类意识规划的启发，我们提出了Skipper，这是一种基于模型的强化学习代理，它利用空间和时间抽象来概括所学技能新颖的情况。它自动将手头的任务分解为更小规模、更易于管理的子任务，从而实现稀疏决策并将其计算集中在环境的相关部分。这依赖于表示为有向图的高级代理问题的定义，其中使用事后知识端到端地学习顶点和边。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法预计会有所帮助的地方。与现有最先进的分层规划方法相比，以泛化为中心的实验验证了 Skipper 在零样本泛化方面的显着优势。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189r06i/combining_spatial_and_temporal_abstraction_in/</guid>
      <pubDate>Sun, 03 Dec 2023 10:41:44 GMT</pubDate>
    </item>
    <item>
      <title>如何让SARSA模型正确预测？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189obpu/how_to_make_a_sarsa_model_predict_correctly/</link>
      <description><![CDATA[ 由   提交 /u/camelCase_Dev   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189obpu/how_to_make_a_sarsa_model_predict_correctly/</guid>
      <pubDate>Sun, 03 Dec 2023 07:25:19 GMT</pubDate>
    </item>
    <item>
      <title>寻求电池存储系统强化学习项目的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189llo9/seeking_help_with_reinforcement_learning_project/</link>
      <description><![CDATA[我目前正在开展一个项目，该项目涉及使用强化学习 (RL) 管理电池存储系统。尽管问题很简单，但我遇到了困难 - 似乎没有标准的 RL 方法（PPO、A2C、DQN）可以有效地学习。 我已经分享了所有项目 文件，其中包括详细的问题描述（这是一个简单的问题）、CSV 格式的输入数据、环境文件和代理初始化文件。&lt; /p&gt; 我非常感谢您提供的任何见解、建议或指导！如果您熟悉 RL 方法并且可以抽出时间看一下，您的帮助将非常有价值。   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189llo9/seeking_help_with_reinforcement_learning_project/</guid>
      <pubDate>Sun, 03 Dec 2023 04:30:18 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家：π(A|S,θ) 是什么形状？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189drnd/actor_critic_what_is_the_shape_of_πasθ/</link>
      <description><![CDATA[π(A|S,θ) 在 softmax 情况下为 exp(θ⊤Ax(S))/ Σexp(θ⊤Ax(S) )). 形状应该是什么？ 给定动作数量 = k 和特征数量 = d ​   由   提交 /u/Fashism   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189drnd/actor_critic_what_is_the_shape_of_πasθ/</guid>
      <pubDate>Sat, 02 Dec 2023 21:35:56 GMT</pubDate>
    </item>
    <item>
      <title>具有快速且健忘记忆的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1896eu4/reinforcement_learning_with_fast_and_forgetful/</link>
      <description><![CDATA[   /u/smorad  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1896eu4/reinforcement_learning_with_fast_and_forgetful/</guid>
      <pubDate>Sat, 02 Dec 2023 15:45:19 GMT</pubDate>
    </item>
    <item>
      <title>简单的 Q 学习与跨智能体共享值表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18902nv/simple_qlearning_with_value_table_sharing_across/</link>
      <description><![CDATA[围绕测试轨道进行朴素 q 学习的简单示例，使用 C++ 中的 raylib 实现 - 输入：5 个光线投射每个传感器分为 3 个区域：危险近、中、远。 （提供3^5=243个状态） - 动作：以匀速v直行，以v/2向左转向，以v/2向右转向 结束时每个episode，都会计算所有智能体之间的q表平均值并将其设置回智能体，以便在每个episode之后共享知识。当 epsilon 降至零时，我们只看到 1 次均匀移动，而不是 30 次，因为所有智能体始终选择贪婪动作。 https://reddit.com/link/18902nv/video/hnje6htdou3c1/player   由   提交 /u/goksankobe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18902nv/simple_qlearning_with_value_table_sharing_across/</guid>
      <pubDate>Sat, 02 Dec 2023 09:26:38 GMT</pubDate>
    </item>
    <item>
      <title>我能知道绑定 k 是如何绑定的吗？来源：强化学习：理论与算法，作者：Alekh Agarwal、Nan Jiang、Sham M. Kakade 和 Wen Sun https://rltheorybook.github.io/rltheorybook_AJKS.pdf</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188xrhh/can_i_know_how_the_bound_k_was_bound_origin/</link>
      <description><![CDATA[       由   提交/u/Professional_Card176   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188xrhh/can_i_know_how_the_bound_k_was_bound_origin/</guid>
      <pubDate>Sat, 02 Dec 2023 06:41:09 GMT</pubDate>
    </item>
    <item>
      <title>多代理强化学习基线</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188ox6r/mutli_agent_reinforcement_learning_baselines/</link>
      <description><![CDATA[我正在寻找稳定基线 3 的等效版本，但顾名思义，它适用于 MARL，以便我可以在自定义 MARL 环境上对算法进行基准测试。提到的大多数方法都利用与不同库（例如 Tianshou、CleanRL 和 Stable Baselines 3 本身）共享参数。不同的 MARL 算法还有其他常用的标准化基线吗？   由   提交/u/blitzkreig3  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188ox6r/mutli_agent_reinforcement_learning_baselines/</guid>
      <pubDate>Fri, 01 Dec 2023 22:55:03 GMT</pubDate>
    </item>
    </channel>
</rss>