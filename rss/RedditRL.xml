<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 17 Apr 2024 09:14:15 GMT</lastBuildDate>
    <item>
      <title>寻求建议：RL Agent 没有学会在特定场景中输出零</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5nbf2/seeking_advice_rl_agent_not_learning_to_output/</link>
      <description><![CDATA[大家好， 我对强化学习领域还比较陌生，目前正在开展一个项目，我在该项目中我们为电动汽车充电系统开发了一个自定义环境，其中代理的目标是在两个电池存储之间优化分配能量，并决定何时从电网获取电力。 环境详细信息：   状态空间：状态S由9个值组成： 当前步骤所需的总能量 当前网格能源价格 两个电池（主电池和副电池）的充电状态 (SoC) 以及其他功能  动作空间：动作输出是三个值的元组： 主电池放电（在0和1之间连续） 辅助电池放电（在0之间连续）和 1) 电网能量消耗（在 0 和 1 之间连续）   奖励功能：&lt; br /&gt; 该奖励旨在惩罚不必要的电网使用，特别是当电网价格为正时，并奖励不依赖电网而满足需求的能源分配。如果电网价格为负，该函数将根据使用的电量按比例奖励使用电网电力。 使用的算法： Soft Actor Critic (SAC) 的 Stablebaselines3 实现）与 MLP 策略。 挑战： 第三个操作（电网能源消耗）出现了具体问题。在最好将此操作设置为零的情况下（即电池中有足够的能量和正的电网价格），代理不会学习这样做。这些值向零收敛（低至 0.1 或 0.04），但从未达到零。这种次优行为会显着影响性能。 我正在考虑将值手动修剪为零作为最后的手段，但我更希望代理自然地学习这种行为。  问题：  在训练智能体在特定条件下选择动作空间中的零值时，是否有人遇到过类似的挑战？ 有吗？对 SAC 算法或奖励函数进行具体调整，以鼓励更精确的行为？ 奖励逻辑中任何额外惩罚或修改的实施是否可以帮助智能体学会更可靠地选择零？ &gt;  任何见解、技巧或类似场景的分享经验将不胜感激！ 提前感谢您的帮助！   由   提交 /u/Nnarruqt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5nbf2/seeking_advice_rl_agent_not_learning_to_output/</guid>
      <pubDate>Tue, 16 Apr 2024 18:23:01 GMT</pubDate>
    </item>
    <item>
      <title>强化学习文学</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5kysz/rl_literature/</link>
      <description><![CDATA[哪些被认为是当前 RL 上最好的文本？我已经阅读了萨顿的第二版，但不确定下一步该去哪里。 有什么建议吗？   由   提交 /u/NSADataBot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5kysz/rl_literature/</guid>
      <pubDate>Tue, 16 Apr 2024 16:49:59 GMT</pubDate>
    </item>
    <item>
      <title>更新观察空间形状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5ff9t/update_the_observation_space_shape/</link>
      <description><![CDATA[我正在使用gymnasium进行RL对象检测，我想问是否有人知道如何在重置中加载新图像时更新我的​​观察空间形状方法。我使用的数据集具有相同形状的图像，但我在具有各种形状的 visdrone2019 数据集中进行了更改。我在 init 函数中初始化了观察空间的形状，但我可以找到一种稍后更改其形状的方法 self.observation_shape = self.current_image.shape self.observation_space =gym。 space.Box( low=0, high=255, shape=self.observation_shape, dtype=np.uint8 )  ​   由   提交 /u/Confident-Crab-5686   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5ff9t/update_the_observation_space_shape/</guid>
      <pubDate>Tue, 16 Apr 2024 12:56:33 GMT</pubDate>
    </item>
    <item>
      <title>Taxi-v3 和 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5ci7f/taxiv3_and_dqn/</link>
      <description><![CDATA[大家好！ 我目前正在尝试使用 DQN 解决出租车问题，我可以在日志中清楚地看到代理正在学习。然而，一个奇怪的现象发生了：虽然代理在训练期间（常数 epsilon = 0.3）获得不同的分数（在 -250 和 +9 之间），但在验证期间只有好或坏（当然 epsilon = 0）。我得到的分数要么是 -200，要么是正值。我使用一个有 3 层、lr 为 0.001 的简单网络。状态通过独热编码传递给代理。除此之外，它是具有经验重放（100,000 大小）和 128 的批处理大小的标准 DQN。 以下是日志的摘录（这是 1000 次训练后的最后一次评估）： ----------------评估---------------- 第 1 集 | 分数：12 |步骤：9 | 损失：0 | 持续时间：0.002709 | Epsilon：0 第 2 集 | 得分：-200 | 步骤：200 | 损失：0 | 持续时间：0.031263 | Epsilon：0 第 3 集 | 得分：-200 | 步骤：200 | 损失：0 | 持续时间：0.019805 | Epsilon：0 第 4 集 | 得分：-200 | 步骤：200 | 损失：0 | 持续时间：0.015337 | Epsilon：0 第 5 集 | 得分：9 | 步骤：12 | 损失：0 | 持续时间：0.000748 | Epsilon：0 第 6 集 | 得分：-200 | 步骤：200 | 损失：0 |时长：0.014757 | Epsilon：0 第 7 集 | 得分：8 | 步数：13 | 损失：0 | 时长：0.001071 | Epsilon：0 第 8 集 | 得分：-200 | 步数：200 | 损失：0 | 时长：0.029834 | Epsilon：0 第 9 集 | 得分：-200 | 步数：200 | 损失：0 | 时长：0.049129 | Epsilon：0 第 10 集 | 得分：-200 | 步数：200 | 损失：0 | 时长：0.016023 | Epsilon：0 第 11 集 | 得分：11 | 步数：10 | 损失：0 | 时长：0.000647 Epsilon：0 第 12 集 | 得分：-200 | 步数：200 | 损失：0 | 持续时间：0.01529 | Epsilon：0 第 13 集 | 得分：-200 | 步数：200 | 损失：0 | 持续时间：0.019418 | Epsilon：0 第 14 集 | 得分：6 | 步数：15 | 损失：0 | 持续时间：0.002647 | Epsilon：0 第 15 集 | 得分：6 | 步数：15 | 损失：0 | 持续时间：0.001612 | Epsilon：0 第 16 集 | 得分：9 | 步数：12 | 损失：0 | 持续时间：0.001429 | Epsilon：0 第 17 集 | 得分：5 | 步数：16损失：0 | 持续时间：0.00137 | Epsilon：0 第 18 集 | 得分：-200 | 步骤：200 | 损失：0 | 持续时间：0.022115 | Epsilon：0 第 19 集 | 得分：8 | 步骤：13 | 损失：0 | 持续时间：0.001074 | Epsilon：0 第 20 集 | 得分：9 | 步骤：12 | 损失：0 | 持续时间：0.001218 | Epsilon：0 平均剧集 (eval) 得分：-95.85  有人知道原因吗？或者我该如何修复它？    提交人    /u/MarcoX0395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5ci7f/taxiv3_and_dqn/</guid>
      <pubDate>Tue, 16 Apr 2024 10:13:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 PettingZoo 进行自定义并行环境训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5acxd/custom_parallel_environment_training_with/</link>
      <description><![CDATA[大家好， 我了解了所有关于环境创建的教程，并且我制作了自己的并行环境，它通过并行测试API成功。但是，我没有找到任何有关如何在环境中训练代理的资源。我的意思是，PettingZoo 文档中没有明确的说明来解释如何训练自定义并行环境。到目前为止我所尝试的一切都没有奏效。如果有人帮助我，我将非常感激，我已经花了整整一周的时间来培训我的特工。提前谢谢您。   由   提交 /u/PutridBlood2123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5acxd/custom_parallel_environment_training_with/</guid>
      <pubDate>Tue, 16 Apr 2024 07:44:19 GMT</pubDate>
    </item>
    <item>
      <title>“DRPO：RLHF 的数据集重置策略优化”，Chang 等人 2024（离线 RL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c4ozd9/drpo_dataset_reset_policy_optimization_for_rlhf/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c4ozd9/drpo_dataset_reset_policy_optimization_for_rlhf/</guid>
      <pubDate>Mon, 15 Apr 2024 15:25:37 GMT</pubDate>
    </item>
    <item>
      <title>需要一些探索想法请🙏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c4fe7n/need_some_exploration_idea_please/</link>
      <description><![CDATA[像这样思考一个非常简单的问题。您处于一维空间中，您的汽车正在这条线上移动。有一个目的地点，您需要在该点停车。您只能激活一次休息，一旦激活休息，就无法再次将其停用。当您激活刹车时，您的汽车只能多移动 10 步。  状态空间-&gt;汽车的位置：[0, 1, 2, …, 250] 动作空间 -&gt;激活或不激活中断：[1, 0]  奖励-&gt; -|DestinationPoint - FinishedPoint| 示例：汽车始终从 0 开始并以相同的速度移动。假设我的目的地是 140，所以我需要在 130 处激活休息。如果我在 100 处激活休息，我会在 110 处完成比赛并结束。我得到的奖励是 -30。 看起来这个问题很简单，但是作为探索的 epsilon 贪婪在这里失败了。为什么？我可以以 50% 的概率采取 0 或 1 的随机操作。这意味着代理很可能会在 10 个步骤中选择操作 1。 （直到第10步才采取行动1的百分比是（1/2）10） 让我们看看直到130才启动刹车的概率。-&gt; (1/2)130 这是非常低的。这意味着当我使用 epsilon 贪婪时，代理将无法有效地探索，并且它将无法进一步探索） 我研究了其他技术，例如基于计数的探索、预测-基于探索，但我不确定它们是否有效。 我的问题很简单，因为它在状态空间只有 1 个特征，但当我在状态空间有 100 个特征时，会想到同样的问题。蛮力效率不高。 我想了解一下你对这类问题的探索思路。 （你不能收回你的行动，你需要等待采取行动的最佳时机，并且你有一次采取行动的机会）   由   提交/u/OpenToAdvices96   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c4fe7n/need_some_exploration_idea_please/</guid>
      <pubDate>Mon, 15 Apr 2024 06:31:51 GMT</pubDate>
    </item>
    <item>
      <title>如何衡量固定策略的学习价值函数的准确性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c4erop/how_to_measure_accuracy_of_learned_value_function/</link>
      <description><![CDATA[你好， 假设我们有一个给定的策略，其价值函数需要评估。获取价值函数的一种方法是使用预期的 SARSA，如此堆栈交换答案中所示。然而，我的 MDP 的状态空间很大，因此我使用 DQN 的修改版本，我称之为深度预期 SARSA。 DQN 的唯一变化是目标策略从“贪婪 wrt”更改为“贪婪 wrt”。现在，在使用深度预期 SARSA 训练价值函数时，我看到的损失曲线并没有显示出下降的趋势。我还在网上读到，DQN 损失曲线不必显示下降趋势，可以增加，这没关系。在这种情况下，如果损失曲线不一定会呈现下降趋势，那么如何衡量学习值函数的准确性？我唯一的想法是将（s，a）处学习值函数的输出与从（s，a）开始并遵循给定策略的许多部署的平均回报估计的预期回报进行比较。 我已经此时有两个问题  是否有比深度预期 SARSA 更好的方法来学习价值函数？在文献中找不到任何这样做的内容。 有更好的方法来衡量学习价值函数的准确性吗？  非常感谢您的宝贵时间!   由   提交/u/Longjumping_March368   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c4erop/how_to_measure_accuracy_of_learned_value_function/</guid>
      <pubDate>Mon, 15 Apr 2024 05:51:04 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero PUCT 公式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c46igr/alphazero_puct_formula/</link>
      <description><![CDATA[我了解 UCB（武装强盗）和 UCT (MCTS) 选择中置信区间的统计特性。 AlphaZero 和 MuZero 中使用的 PUCT 公式背后是否有任何属性？这看起来很随意，而且他们没有在论文中解释它。   由   提交 /u/_Hardric   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c46igr/alphazero_puct_formula/</guid>
      <pubDate>Sun, 14 Apr 2024 22:43:06 GMT</pubDate>
    </item>
    <item>
      <title>最后一年的项目想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3yf8y/final_year_project_ideas/</link>
      <description><![CDATA[我正在攻读数据科学学士学位，最后一年即将到来。我们必须在 2-3 名成员组成的小组中进行一个具有前端的研究和/或行业范围项目。我仍然对该项目的范围感到困惑（一个学士学生实际上应该走多远），但我知道一个“好的”人工智能/机器学习（强化学习！！！）项目通常位于医学领域与计算机视觉一起，或者与法学硕士一起创建语音到文本的聊天机器人。 这里有一些我已经做过的项目（无前端），只是为了表明我的目标是做比这些更大的事情对于我的最终项目：  不同染色的显微细胞图像中的有丝分裂检测 使用网络抓取（selenium + bs4）的艺术风格检测器 年龄使用自定义 CNN 进行性别/等识别 使用 VGG16/19 进行内窥镜分类 多语言文本的情感分析 时间序列分析 股票市场预测 基于 RNN 的实验室任务  我的目标是通过出色的项目获得良好的硕士学位。我对法学硕士和强化学习很好奇，但希望提供更具体的帮助！   由   提交/u/kafkaskewers  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3yf8y/final_year_project_ideas/</guid>
      <pubDate>Sun, 14 Apr 2024 17:06:15 GMT</pubDate>
    </item>
    <item>
      <title>在不同时间尺度做出多个决策的强化学习算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3tgwk/rl_algorithm_for_making_multiple_decisions_at/</link>
      <description><![CDATA[是否有特定的强化学习算法可以在不同的时间尺度（来自多个动作空间）做出多个决策？例如，假设游戏中有两种类型的决策，每n＞1步做出战略决策，而每一步做出操作决策。 RL算法如何解决这个问题？    由   提交/u/Intelligent_Bee_114   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3tgwk/rl_algorithm_for_making_multiple_decisions_at/</guid>
      <pubDate>Sun, 14 Apr 2024 13:22:11 GMT</pubDate>
    </item>
    <item>
      <title>NEAT-python 与 Kivy 在完成每一代的训练后生成默认训练报告？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3p12z/neatpython_with_kivy_for_generating_default/</link>
      <description><![CDATA[各位先生， 由于 Kivy 是单线程且事件驱动的，我修改了 population.run(run_simulation, 2) 到下面的代码。该程序似乎运行良好。但run_simulation自动返回后无法打印默认的训练报告。检查了文档和互联网，仍然找不到合适的解决方法来生成默认的培训报告。有什么推荐吗？  如果 self.simu_count &lt; 2: # 运行人口规模为 1 的模拟 self.population.run (self.run_simulation, 1) 提前致谢， Jefio   由   提交 /u/jefiochen   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3p12z/neatpython_with_kivy_for_generating_default/</guid>
      <pubDate>Sun, 14 Apr 2024 08:51:06 GMT</pubDate>
    </item>
    <item>
      <title>凌乱的图表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3ersj/messy_graph/</link>
      <description><![CDATA[       我正在为我的代理在每个时间步绘制累积奖励。该图表结果不好。  有办法解决这个问题吗？ ​ 奖励文件  图表   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3ersj/messy_graph/</guid>
      <pubDate>Sat, 13 Apr 2024 23:01:56 GMT</pubDate>
    </item>
    <item>
      <title>JK 显然 RL 比蛮力更有效……或者真的是这样吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c39u00/jk_obviously_rl_is_way_more_efficient_than_brute/</link>
      <description><![CDATA[   /u/tottombemon  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c39u00/jk_obviously_rl_is_way_more_efficient_than_brute/</guid>
      <pubDate>Sat, 13 Apr 2024 19:22:47 GMT</pubDate>
    </item>
    <item>
      <title>自打输球让ppo“恐惧”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2ym5s/losing_in_self_play_makes_ppo_fearfull/</link>
      <description><![CDATA[我已经成功地让 ppo 代理在对手随机走棋时玩相当复杂的游戏。游戏是零和游戏，但它足够复杂，我需要在真实分数之上提供一些启发式方法，以便代理能够弄清楚他们需要做什么才能获胜。 现在我已经启用了通过每隔 X 步保存网络状态来进行自我对弈，有时让代理与那些旧版本的网络进行对战。  所发生的情况是，网队会很好地学习，直到弄清楚如何获胜，这会为参与的参与者提供大量奖励/负奖励，以表明赢得比赛比简单地跟随价值较低的比赛更重要启发式。  然后会发生的情况是，当其中一个玩家获胜时，另一个玩家会变得保守并放弃其积累的积分以确保对方不会获胜，并且它会开始学习更慢，这是预期的。出乎意料的是，每次这个过程发生时，失败的网络都会变得越来越保守，直到它不再尝试增加自己的平均奖励，但它只会达到对手不会获胜的已知点。  我尝试在赢得比赛时减少奖金奖励，但这没有帮助。  接下来我要尝试的 * 是奖励标准化 * 只给胜利者奖励，而不给失败者， * 以提高探索率。 * 有时让代理与随机代理对战。  有什么技巧可以解决这个问题吗？   由   提交/u/drblallo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2ym5s/losing_in_self_play_makes_ppo_fearfull/</guid>
      <pubDate>Sat, 13 Apr 2024 10:15:45 GMT</pubDate>
    </item>
    </channel>
</rss>