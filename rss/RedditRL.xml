<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 03 Mar 2025 21:18:56 GMT</lastBuildDate>
    <item>
      <title>有什么办法可以处理RL动作替代吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2ra6v/is_there_any_way_to_deal_with_rl_action_overrides/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 想象我正在用RL构建一种自动驾驶汽车算法。在现实世界中，驾驶员可以覆盖自动驾驶模式。如果我的经纪人经过训练以最大程度地减少旅行时间，则代理商可能会优先考虑速度而不是舒适度 - 考虑到突然的加速度，锋利的转弯或硬制动。自然，驾驶员不会很高兴，并且可能会介入控制。 现在，如果我的环境有（i）汽车和（ii）可以干预的驾驶员，我的经纪人可能会因为所有这些替代而难以完全探索动作空间。 i 假设它最终会学会与驾驶员进行互动并为奖励进行优化，但是……可以将 forever 。  。 以前有人解决过这种问题吗？当外部干预措施不断切断探索时，如何处理RL培训的任何想法？很想听听您的想法！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/open_question4921     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2ra6v/is_there_there_any_way_way_to_to_deal_with_with_rl_rl_action_overrides/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2ra6v/is_there_any_way_to_deal_with_rl_action_overrides/</guid>
      <pubDate>Mon, 03 Mar 2025 19:36:31 GMT</pubDate>
    </item>
    <item>
      <title>Q-LEARNING在凉亭SIM中无法正确融合 - 需要帮助调试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2q6c9/qlearning_in_gazebo_sim_not_converging_properly/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2q6c9/qlearning_in_gazebo_sim_not_converging_properly/</guid>
      <pubDate>Mon, 03 Mar 2025 18:51:57 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4.5在消除游戏基准中排名第一，该游戏测试了社会推理（形成联盟，欺骗，显得无威胁和说服陪审团）。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2q61k/gpt45_takes_first_place_in_the_elimination_game/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/gwern      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2q61k/gpt45_takes_first_first_inplace_in_elimination_elimination_game/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2q61k/gpt45_takes_first_place_in_the_elimination_game/</guid>
      <pubDate>Mon, 03 Mar 2025 18:51:38 GMT</pubDate>
    </item>
    <item>
      <title>RL生物技术？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2pmsl/rl_in_biotech/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有人知道正在研究/实施RL算法的任何生物技术公司吗？与药物发现，癌症研究甚至机器人技术有关的东西  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used-eagle-9302     [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2pmsl/rl_in_biotech/</guid>
      <pubDate>Mon, 03 Mar 2025 18:30:38 GMT</pubDate>
    </item>
    <item>
      <title>对于RL项目中控制策略的输入，我应该包括重要但固定的信息吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2p4f9/for_the_observation_vector_as_input_to_the/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我正在尝试使用PPO算法来训练新型的机器人操纵器，以达到其工作空间中的目标位置。我应该将观察向量作为控制策略的输入的观察向量？当然，我应该在观察矢量中包括相关状态，例如当前的操纵器形状（关节角度）。 ，但我担心以下两个状态/信息在观察矢量中的包含：1）：最终效应子的位置，可以根据关节角度易于计算出最终效应子的位置。这是令人困惑的，因为最终效应子的位置是重要的状态/信息。它将用于计算最终效应器与目标位置之间的距离，以确定奖励，以终止情节。但是，我可以将末端效应子的位置排除在观察矢量之外，因为它可以从关节角度轻松确定。关节角度和关节角依赖性最终效应子是否形成冗余？  2）：障碍物的位置。障碍物的位置也是重要的状态/信息。它将用于计算/检测操纵器和障碍物之间的碰撞，以在检测到的碰撞时施加惩罚，如果检测到碰撞，则终止发作。但是，由于障碍物在整个学习过程中保持固定，我可以将障碍物的位置排除在观察矢量之外吗？我根本不会改变障碍物的位置。是否需要将障碍物包含在观察矢量中？ href =“ https://preview.redd.it/iblf1bo6mime1.png？ https://preview.redd.it/iblf1bo6mime1.png?width=626＆amp; format = png＆amp; auto = webpp＆s = 2b9c71e0e71e7cdaea637bad2bad2b74949e1dc08cf2cf2cf2005d88cf2005d8  发布了一个非常相似的问题 https://ai.stackexchange.com/questions/46173/the-observation-pace-space-of-a-robot-arm-should-should-include-include-the-tharget-target-target-position-or-inly 但没有答案。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tilly_shift7974     [link]        [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2p4f9/for_the_observation_vector_as_input_to_the/</guid>
      <pubDate>Mon, 03 Mar 2025 18:09:50 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习中的当前障碍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2p3u8/current_roadblocks_in_model_based_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   title   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2p3u8/current_roadblocks_in_model_based_reinforcement/</guid>
      <pubDate>Mon, 03 Mar 2025 18:09:11 GMT</pubDate>
    </item>
    <item>
      <title>寻找帮助培训在2D电路上学习AI的强化AI（Pygame + Gym + StableBaselines3）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2n47n/looking_for_help_training_a_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家， 我正在研究一个项目，在该项目中，我需要训练AI使用加固学习来导航2D电路。该代理会收到以下输入：  5传感器（射线）：向前，左，向前，向前，右，右，右右→他们返回AI和障碍物之间的距离。 作为动作的加速度值。 我已经在Pygame中具有适用于Pygame的工作环境，并且我已经适合使用Pygame，并且可以兼顾它。但是，当我尝试使用stablebaselines3的模型3时，我会得到一个黑屏（根据Chatgpt，这可能是由于使用DummyveCenv进行了转换所致）。 因此，如果您知道简单而快速的方法可以有效地训练AI，或者有效地训练AI，或者我可以使用预先训练的型号，我可以使用它，我可以使用它！ sc_on-&gt;＆＃32;提交由＆＃32; /u/u/pt_quill     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2n47n/looking_for_help_training_a_reinforcement/</guid>
      <pubDate>Mon, 03 Mar 2025 16:48:56 GMT</pubDate>
    </item>
    <item>
      <title>多discrete offlicy</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2myun/multidiscrete_offpolicy/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有使用Multi-Discrete（带有Gumbel）（带gumbel）的算法（例如TD3/7 DDPG）的实现，或者我注定要使用PPO，如果我想使用多discrete Actions Actions Space（和不flatten）（并且不flattent It It）/u/what_did_it_it_cost_e_t      [link]   ＆＃32;   [comment]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2myun/multidiscrete_offpolicy/</guid>
      <pubDate>Mon, 03 Mar 2025 16:42:38 GMT</pubDate>
    </item>
    <item>
      <title>与（PSO）颗粒群优化杂交多代理增强学习（MARL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2kh0m/hybridizing_multi_agent_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是我的学士学位论文的计算机科学专业学生，我的主题是“基于智能算法的决策，用于搜索和救援中的群体机器人”。我对此一无所知，因此经过一些文献综述，我想我喜欢制作PSO+MARL混合算法的想法，以使群体机器人技术更快，更适应于搜索和救援环境。但是我仍然有0个背景，我不知道这是否是个好主意，我不知道它是否可行，所以我想知道是否有人知道如何开始或我应该改变我的方法？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/amrhesham2424    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j2kh0m/hhybridizing_multi_agent_reinforection_learche_learning/”&gt; [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2kh0m/hybridizing_multi_agent_reinforcement_learning/</guid>
      <pubDate>Mon, 03 Mar 2025 14:57:06 GMT</pubDate>
    </item>
    <item>
      <title>[D]没有赢家和未知最佳分数的游戏的加强学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2e1w5/d_reinforcement_learning_for_games_with_no_winner/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在即将到来的项目中，我需要在笼子内部包装盒子和密集。但是，这些盒子将一次到达一个，并带有随机尺寸和形状。目的是尽可能地填充笼子（理想情况下是100％，但在大多数情况下这是无法达到的）。 这个问题在传统上是一个离散的优化问题，但是由于我们不知道这些包裹在到达前的包装上，所以我怀疑它们确实是正确的，而且我真的很认为，这实际上是我的一点点，如果没有我的一点点我，这实际上是一定的。以前的强化学习，但总是适用于有赢家和宽松的游戏。但是，在这种情况下，我们没有。因此，当我在游戏结束时唯一拥有的数字是0-1之间的数字，而1个完美，但在大多数游戏中也可能无法实现。 我认为我曾经多次重复每个游戏。因此，您可以获得完全相同的软件包配置，因此可以与该配置上的以前的游戏进行比较，并根据模型比以前做得更好或更糟糕的是奖励该模型，但是我不确定这是否效果很好。 有人是否有这样的经验？提交由＆＃32; /u/u/alyflex     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2e1w5/d_reinforcement_learning_for_games_with_no_winner/</guid>
      <pubDate>Mon, 03 Mar 2025 08:24:06 GMT</pubDate>
    </item>
    <item>
      <title>欧洲可以做什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1yynu/what_can_an_europoor_do/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我是欧盟公民。我在这里问，因为我不知道该如何处理我的RL激情。 我在应用数学方面有广泛的背景，并且在数据科学方面做了硕士学位。过去2年过去了，我一直在医疗保健行业担任AI工程师。自从我从事机器人技术的研究实习以来，我就爱上了RL。问题在于，我在欧盟看到了0个作业，我可以申请的少数博士学位（他们不会在其他地方赞助我）。   ，但是，我觉得非学生没有博士学位的机会（没有网络），而且我没有选择。我正在考虑在Uni中使用一个良好的RL/Robotics Lab做另一个大师，即使这可能是浪费时间。关于去哪里或从这里遵循什么途径的任何建议？我一直想进行研究，但是它看起来显得黯淡。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;  /u/Exkur   [link] ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1yynu/what_can_an_europoor_do/</guid>
      <pubDate>Sun, 02 Mar 2025 19:21:53 GMT</pubDate>
    </item>
    <item>
      <title>我们如何在离线学习中使用重播缓冲区？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1vuxo/how_do_we_use_the_replay_buffer_in_offline/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 如果您收集了一个巨大的数据集用于我的离线学习。有数百万个例子。我在线阅读，通常您会将整个数据集上传到重播缓冲区中。但是，对于数据集很大的情况，这将是一个巨大的内存开销。您将如何解决此问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1vuxo/how_do_we_use_the_replay_buffer_in_offline/</guid>
      <pubDate>Sun, 02 Mar 2025 17:15:02 GMT</pubDate>
    </item>
    <item>
      <title>关于DQN的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1r1pj/a_problem_about_dqn/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   dqn算法的输出只能是一个操作？  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/clean_tip3272     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1r1pj/a_problem_about_dqn/</guid>
      <pubDate>Sun, 02 Mar 2025 13:39:08 GMT</pubDate>
    </item>
    <item>
      <title>最佳提交Tinker AI的第二次比赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1qekd/best_submission_of_tinker_ais_second_competition/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/goncalogordo       [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1qekd/best_submission_of_tinker_ais_second_competition/</guid>
      <pubDate>Sun, 02 Mar 2025 13:05:10 GMT</pubDate>
    </item>
    <item>
      <title>使用DQN帮助解决山车问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好， 在开始之前，我想道歉，问问这个问题，因为我猜想这个问题可能已经被问到很多次了。我试图教自己增强学习，并且正在研究这款MountrainCar迷你项目。  我的模型似乎根本没有融合。我使用情节持续时间与情节编号的情节来检查/分析性能。我注意到的是，有时，对于我尝试过的所有架构，情节持续时间都会有所减少，然后再次增加。  I have tried doing the following things:  Changing the architecture of the Fully Connected Neural network. Changing the learning rate Changing the epsilon value, and the epsilon decay values.  For neither of these changes, I got a model that seems to converge during training.我平均培训了1500个持续时间。这就是每个模型通常看起来的图：   有没有适合此特定问题的技巧，特定的DQN体系结构和超参数范围？还有一组指南，应该牢记并用来创建这些DQN模型？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/lowkeysuicidal14     [link]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</guid>
      <pubDate>Sun, 02 Mar 2025 04:17:05 GMT</pubDate>
    </item>
    </channel>
</rss>