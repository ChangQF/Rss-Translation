<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 10 Jan 2025 01:17:38 GMT</lastBuildDate>
    <item>
      <title>isaac 健身房 vs isaac 模拟 vs isaac 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxr7gk/isaac_gym_vs_isaac_sim_vs_isaac_lab/</link>
      <description><![CDATA[大家好， 有人能帮我理解一下这里的一些基本分类法吗？isaac gym、isaac sim 和 isaac lab 有什么区别？ 谢谢，干杯！    提交人    /u/iawdib_da   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxr7gk/isaac_gym_vs_isaac_sim_vs_isaac_lab/</guid>
      <pubDate>Fri, 10 Jan 2025 00:01:08 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助解决涉及 2D 网格的扫雷 RL 训练问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxr48a/need_help_with_a_minesweeper_rl_training_issue/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxr48a/need_help_with_a_minesweeper_rl_training_issue/</guid>
      <pubDate>Thu, 09 Jan 2025 23:57:00 GMT</pubDate>
    </item>
    <item>
      <title>DQN 实施过程中的损失增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxl6o1/loss_increasing_for_dqn_implementation/</link>
      <description><![CDATA[      我正在使用 DQN 实现来最大限度地减少四轴飞行器控制器的损失。目标是让我的 RL 程序更改控制器的一些参数，然后接收从每个参数更改计算出的损失，算法的奖励是损失的负数。我运行了我的程序两次，随着时间的推移，损失都趋于增加（奖励减少），我不确定会发生什么。任何建议都将不胜感激，如果需要，我可以分享代码示例。  第一张图 以上是第一张图的结果。我再次训练了它，做了一些改变：增加批量大小、内存缓冲区大小、降低学习率、增加探索概率衰减，虽然奖励值更接近应有的值，但它们仍然像上面一样呈下降趋势。任何建议都将不胜感激。     提交人    /u/kwasi3114   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxl6o1/loss_increasing_for_dqn_implementation/</guid>
      <pubDate>Thu, 09 Jan 2025 19:38:19 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA ACE</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxju3b/nvidia_ace/</link>
      <description><![CDATA[有人有关于 NVIDIA ACE AI 的更多信息吗？我还没有深入研究这个话题（由于时间限制），但我的理解是，它将根据“NPC/AI 所犯的错误”调整 NPC 的决策。有人知道任何技术细节或相应论文的链接吗？    提交人    /u/Intelligent-Put1607   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxju3b/nvidia_ace/</guid>
      <pubDate>Thu, 09 Jan 2025 18:41:29 GMT</pubDate>
    </item>
    <item>
      <title>实现多智能体算法的参考资料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxaqv8/reference_materials_for_implementing_multiagent/</link>
      <description><![CDATA[您好， 我目前正在研究多智能体系统。  最近，我一直在阅读 多智能体 PPO 论文并致力于其实现。  是否有任何简单的参考资料，例如 minimalRL，我可以参考？    提交人    /u/audi_etron   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxaqv8/reference_materials_for_implementing_multiagent/</guid>
      <pubDate>Thu, 09 Jan 2025 11:26:45 GMT</pubDate>
    </item>
    <item>
      <title>选择硕士论文主题：拦截无人机的强化学习。好主意吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hx8j29/choosing_master_thesis_topic_reinforcement/</link>
      <description><![CDATA[对于我的航空航天工程硕士论文（为期 9 个月），我正在探索使用强化学习 (RL) 来训练能够动态响应威胁的拦截无人机的想法。转折点是引入对抗网络来模拟猎物无人机的行为。 我想研究一个既相关又有影响力的论文主题。鉴于廉价无人机目前带来的威胁，我发现反无人机措施特别有趣。但是，我对 RL 是否是拦截无人机的轨迹规划和控制输入的正确方法有些怀疑。 你觉得这个想法怎么样？它有潜力和相关性吗？如果您有任何其他建议，我很乐意听取！    提交人    /u/Marco_878a   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hx8j29/choosing_master_thesis_topic_reinforcement/</guid>
      <pubDate>Thu, 09 Jan 2025 08:41:22 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像扩散模型的密集奖励 + RLHF</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hx5vxl/dense_reward_rlhf_for_texttoimage_diffusion_models/</link>
      <description><![CDATA[分享我们的 ICML&#39;24 论文“基于偏好对齐文本到图像扩散的密集奖励视图”！（不，它还没有过时！） 在本文中，我们采用了密集奖励视角并开发了一种新颖的对齐目标，该目标打破了 DPO 式对齐损失中的时间对称性。我们的方法特别适合文本到图像扩散模型的生成层次（例如稳定扩散），通过强调扩散逆链/过程的初始步骤 --- 开始是艰难的！ 实验上，我们的密集奖励目标在将文本到图像扩散模型与人类/人工智能偏好相结合的有效性和效率方面显着优于经典的DPO损失（源自稀疏奖励）。  论文：https://arxiv.org/abs/2402.08265 海报：https://icml.cc/media/PosterPDFs/ICML%202024/32707.png?t=1717872664.0844204 代码： https://github.com/Shentao-YANG/Dense_Reward_T2I     提交人    /u/Leading-Contract7979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hx5vxl/dense_reward_rlhf_for_texttoimage_diffusion_models/</guid>
      <pubDate>Thu, 09 Jan 2025 05:34:55 GMT</pubDate>
    </item>
    <item>
      <title>当 epsilon 达到最小值时，CleanRL 中的损失停止减少。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwpmog/loss_stops_decreasing_in_cleanrl_when_epsilon/</link>
      <description><![CDATA[嗨， 我正在使用 CleanRL 的 DQN。我对所看到的内容有点困惑，并且不太了解其中的奥秘。 附件是我运行 10M 步的损失图表。随着 epsilon 在 5M 步达到最小值 (0.05)，损失停止减少并趋于平稳。 损失图 我发现有趣的是，这在任何数量的步骤（50k、100k、1M、5M、10M）中都是持久的。 我知道 epsilon 何时达到最小探索停止点。那么，损失是否严格来说只是因为代理不再真正探索，而是在 95% 的时间内执行最佳操作而趋于平稳？ 任何阅读或建议都将不胜感激。    提交人    /u/chysallis   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwpmog/loss_stops_decreasing_in_cleanrl_when_epsilon/</guid>
      <pubDate>Wed, 08 Jan 2025 17:26:51 GMT</pubDate>
    </item>
    <item>
      <title>ROCm (amd) 上的 pytorch？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwkp00/pytorch_on_rocm_amd/</link>
      <description><![CDATA[我使用的是 Linux，Nvidia 很麻烦。我正在考虑换回 AMD GPU，而且我已经看到了 ROCm。由于我只将 Pytorch 的东西（例如 Unity 中的 ml-agents）用作业余爱好，因此性能差异可能并不明显？ 有什么经验可以分享吗？    提交人    /u/sendbootypics_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwkp00/pytorch_on_rocm_amd/</guid>
      <pubDate>Wed, 08 Jan 2025 13:51:38 GMT</pubDate>
    </item>
    <item>
      <title>关于如何克服自我游戏 RL 中的推理速度瓶颈，有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwj10e/any_advice_on_how_to_overcome_the_inferencespeed/</link>
      <description><![CDATA[大家好！ 我一直在为棋盘游戏开发一个 MCTS 风格的 RL 项目，这是一个业余项目。没有什么特别的，类似于 alpha zero。使用网络进行树搜索，该网络将获取当前状态并输出价值判断和下一个可能动作的先验分布。 我的问题是，考虑到连续运行推理步骤的成本，我不明白如何在自我游戏中生成足够多的游戏。具体来说，假设我想每一步查看大约 1000 个位置。相当适中……但对于玩游戏的单个代理来说，这仍然是连续 1000 个推理步骤。使用合理大小的模型，比如像样的 resnet 大小，以及良好的 GPU，我估计我每秒可以获得大约 200 个状态评估。所以单个动作需要 1000/200 = 5 秒？？然后假设我的游戏平均持续 50 步。我们称其为自玩游戏的整整 5 分钟。真扫兴。 如果我想要游戏多样性，并且每个训练周期的重播缓冲区长度合理，比如 5000 场游戏，并且我可以并行运行代理，那么我可以同时运行 100 个代理，并批量处理到 GPU（这是乐观的 - 我在这方面很糟糕），这可以连续进行 50 场游戏，所以 250 分钟 = 4 小时，对于单代游戏。我需要其中几代才能让我的网络学到任何东西…… 我是否遗漏了什么，或者这个问题的解决方案仅仅是“更多资源，一切并行”，以便从自游戏中生成足够的样本？我是否在上述近似值中犯了一些严重错误？任何帮助或建议都非常感谢！   提交者    /u/Fd46692   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwj10e/any_advice_on_how_to_overcome_the_inferencespeed/</guid>
      <pubDate>Wed, 08 Jan 2025 12:24:58 GMT</pubDate>
    </item>
    <item>
      <title>建立强化学习直觉的最佳统计和概率书籍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwdbsr/best_statistics_and_probability_books_for/</link>
      <description><![CDATA[我是数学专业的。所以数学不是问题。Python 也不错。我只是需要对统计数据更加直观，并且如果需要任何高级概念，则需要特别关注 RL 的概率。请推荐一些好书。    提交人    /u/brahmawadi   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwdbsr/best_statistics_and_probability_books_for/</guid>
      <pubDate>Wed, 08 Jan 2025 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q 学习制作无与伦比的井字游戏 AI 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwcnrk/problem_with_making_unbeatable_tictactoe_ai_using/</link>
      <description><![CDATA[我正在尝试使用 q-learning 制作井字游戏 ai。但是，它根本不是不可战胜的。我试图在阻挡时给它更多的奖励，但它仍然没有阻挡对手。我真的不知道我哪里写错了代码。 下面的链接是我在 Google Colab 中的项目的链接。你可能会注意到我使用了 ChatGPT 的一些帮助，但我认为我真的清楚地理解了它们 Google Colab 链接 非常感谢。    提交人    /u/FrostFireThunderGlow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwcnrk/problem_with_making_unbeatable_tictactoe_ai_using/</guid>
      <pubDate>Wed, 08 Jan 2025 05:41:19 GMT</pubDate>
    </item>
    <item>
      <title>从课程到实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwbyjq/from_courses_to_implementation/</link>
      <description><![CDATA[我是 RL 新手，希望将我的职业转向 RL，一直在学习理解数学和建立直觉的东西，但无法转向实际模拟。还想知道是否有一个真正好的课程可以学习深度 RL 方法和基于 mujoco 的基本机器人实现，我可以在学习主题后继续学习。到目前为止，我已经了解了大部分基础知识，直到 q 学习。  任何帮助都将不胜感激。    提交人    /u/Awkward_Bid7858   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwbyjq/from_courses_to_implementation/</guid>
      <pubDate>Wed, 08 Jan 2025 04:59:59 GMT</pubDate>
    </item>
    <item>
      <title>使用连续 PPO 进行重新缩放操作时，裁剪与压缩 tanh 有何区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwau8q/clipping_vs_squashed_tanh_for_rescaling_actions/</link>
      <description><![CDATA[当我们有连续 PPO 时，它通常会从具有无界均值和标准差的高斯中采样动作。我已经看到 tanh 激活通常用于网络的中间激活，以便这些均值等不会太失控。 但是，当我实际从这个高斯中采样动作时，它们不在我的环境限制内（0 到 1）。确保从高斯中采样的动作最终在我的环境限制内的最佳方法是什么？在初始化我的高斯分布之前，最好在均值上添加一个 tanh 层，然后重新调整从该分布中采样的动作？还是直接剪辑高斯的原始输出使其介于 0 和 1 之间更好？    提交人    /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwau8q/clipping_vs_squashed_tanh_for_rescaling_actions/</guid>
      <pubDate>Wed, 08 Jan 2025 03:57:05 GMT</pubDate>
    </item>
    <item>
      <title>RLHF PPO 训练的更密集奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwadqv/denser_reward_for_rlhf_ppo_training/</link>
      <description><![CDATA[我很高兴与大家分享我们最近的成果“分割文本并学习其奖励以改进语言模型中的 RLHF”！ 在本文中，我们研究了 RLHF PPO 训练中动作空间的粒度，仅假设二元偏好标签。我们的建议是为每个语义完整的文本段分配奖励，而不是每个标记（可能过于精细）或强盗奖励（稀疏）。我们进一步设计了技术来确保在更密集的{segment, token}级奖励下 RLHF PPO 训练的有效性和稳定性。 我们的 Segment 级 RLHF PPO 及其 Token 级 PPO 变体在各种主干 LLM 下的 AlpacaEval 2、Arena-Hard 和 MT-Bench 基准测试中优于 bandit PPO。  论文：https://arxiv.org/pdf/2501.02790 代码：https://github.com/yinyueqin/DenseRewardRLHF-PPO RLHF 的 token 级奖励模型的前期工作：https://arxiv.org/abs/2306.00398     由   提交  /u/Leading-Contract7979   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwadqv/denser_reward_for_rlhf_ppo_training/</guid>
      <pubDate>Wed, 08 Jan 2025 03:32:33 GMT</pubDate>
    </item>
    </channel>
</rss>