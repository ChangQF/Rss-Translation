<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 10 Oct 2024 21:16:54 GMT</lastBuildDate>
    <item>
      <title>这个问题可以用 RL 解决吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0kz70/can_this_problem_be_solved_with_rl/</link>
      <description><![CDATA[您好， 我是 RL 的新手，正在研究一个问题，电动汽车需要决定何时何地充电，以最大限度地减少等待时间和充电成本（价格随时间波动）。 我最初的想法是将每辆电动汽车视为一个代理，每个电动汽车都有自己的观察结果，例如电池状态、充电站位置、电价以及每个充电站的排队长度。 行动空间为： • 0：延迟充电（下一小时再决定） • 1：在充电站 1 充电 • 2：在充电站 2 充电 每个情节有 24 个时间段，代理只有在选择充电站后才会获得奖励。 我的问题是： 一旦电动汽车选择了一个充电站，它就会停止做出决策，因此轨迹会提前结束。例如，某些轨迹可能是 {0,0,0,1}（在 t=4 时转到 CS1），而其他轨迹可能是 {2}（在 t=0 时转到 CS2）。只有当 EV 选择充电站时，我才会获得奖励。 MARL 在这里仍然是一种好方法吗？ 我也不确定这个问题是否适合 MDP 框架，因为我见过的大多数论文都集中处理分配，当代理收到充电请求时，他们会立即决定充电站。 提前谢谢您！    提交人    /u/Full_Friendship8349   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0kz70/can_this_problem_be_solved_with_rl/</guid>
      <pubDate>Thu, 10 Oct 2024 15:07:19 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Q-Learning 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0ikbh/help_in_a_qlearning_project/</link>
      <description><![CDATA[      嘿，我是 RL 的新手，正在我的一位教授手下做一个项目。最初的任务之一是训练一个代理，使其能够在 5x5 网格中找到从随机初始化的起点到随机初始化的终点的最佳路径。 我学习了一些理论（主要来自 Medium 文章和 Chatgpt），并认为使用 Q 学习是一种很好的方法。然而我似乎陷入了困境，无论我如何更改参数或更改奖励结构都无济于事。训练结束时所采用的平均时间步长约为 16-17，对于这个简单的问题来说，这个数字确实很高，而且代理总体上表现不佳。 这是我的奖励结构+超参数+训练循环的片段 我尝试将奖励设为常数，减少（甚至消除）时间步长惩罚，并增加/减少几乎所有的超参数，但并没有取得太大的进步。 如果这是一个非常简单的问题，我很抱歉在这里发布，我可能犯了很多新手和基本错误。我将非常感激你们提供的所有帮助和任何资源，以便我加深理解。谢谢！    提交人    /u/Hot_Program2634   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0ikbh/help_in_a_qlearning_project/</guid>
      <pubDate>Thu, 10 Oct 2024 13:15:59 GMT</pubDate>
    </item>
    <item>
      <title>强化学习提高化学反应性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0e0pj/reinforcement_learning_for_improving_chemical/</link>
      <description><![CDATA[我很高兴地告诉大家，我们孟买印度理工学院的研究小组（RBS 小组）最近在著名的《美国化学会志》（JACS）上发表了一篇论文，重点介绍了强化学习 (RL) 在提高化学反应性能方面的应用。 在复杂的化学世界中，优化反应条件可能是一项艰巨的任务，通常需要大量的反复试验。我们的论文提出了一种利用 RL 算法来预测和改善反应结果的新方法。通过将优化过程视为动态决策问题，我们能够显著提高反应产量和选择性。 我们希望我们的工作能够激发人工智能与化学交叉领域的进一步探索，促进该领域复杂问题的创新解决方案。 这里是链接https://pubs.acs.org/doi/full/10.1021/jacs.4c08866    提交人    /u/Kindly-Mortgage-2459   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0e0pj/reinforcement_learning_for_improving_chemical/</guid>
      <pubDate>Thu, 10 Oct 2024 08:20:47 GMT</pubDate>
    </item>
    <item>
      <title>梦想家与旧论文非常相似</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0d22d/dreamer_is_very_similar_to_an_older_paper/</link>
      <description><![CDATA[我随意浏览了 Yannic Kilcher 的旧视频，发现了这个视频，内容是关于 David Ha 和 Jürgen Schmidhuber 的论文“World Models”。我很惊讶地发现，尽管没有被引用或作者相同，但它提出了与 Dreamer（发表时间稍晚）非常相似的想法。 两者都涉及学习潜在动态，可以产生“梦想”环境，在这种环境中，RL 策略可以在不需要在真实环境中进行部署的情况下进行训练。即使是架构也基本相同，从观察自动编码器到处理实际前向演化的 RNN/LSTM 模型。 但是，尽管这些大体内容相同，但实际的论文结构却截然不同。 Dreamer 的论文有更好的实验和数值结果，以及不同想法的呈现方式。 我不确定这是否只是巧合，或者作者是否有一些共同的圈子。无论如何，我认为鉴于 Dreamer 的受欢迎程度，早期的论文应该得到更多的认可。    提交人    /u/irrelevant_sage   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0d22d/dreamer_is_very_similar_to_an_older_paper/</guid>
      <pubDate>Thu, 10 Oct 2024 07:04:38 GMT</pubDate>
    </item>
    <item>
      <title>连续动作空间优化问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0bgi4/trouble_in_continuous_action_space_optimization/</link>
      <description><![CDATA[事情是这样的。我正在尝试优化具有连续动作空间的环境。每次我采取动作后，环境都会进入一个新状态，我会获得该状态的奖励。我必须找到最佳状态（这对我来说应该是未知的）。目前，环境很简单，我知道最佳状态，但稍后将使用 RL 来处理类似类型但更复杂的环境。但问题是我的 RL 算法无法找到最佳状态。我正在使用 Advantage Actor Critic (A2C) 算法来完成这项任务。状态是二维的 - (x, y)。我使用 delta_x、delta_y 作为动作。所以我的神经网络会预测每个动作的 mu 和 sigma。（两个变量的两个动作）。然后我进行更新。所以我正在做的是 --- 我正在运行每个情节 30 步。获取奖励，将它们按河流顺序添加并带有折扣因子。在每个情节（即 30 步）之后，我都会更新梯度。在每个情节中，都会随机选择有效状态空间的初始状态。但是该算法无法找到最优点 l。这是一个非常简单的问题。因为我还为整个状态空间绘制了奖励函数。但即使这样，我也没有得到解决方案。后来我不知道如何为复杂的环境找到 RL 算法。我注意到的一件重要的事情是，当输出从 Actor 网络中取出时，两个动作空间的 mu 值都会相关。它大致相当于 delta_x = -0.95 * delta_y，但这不应该发生。我尝试在 theactor 网络的输出之前对 mu 和 sigma 使用单独的分支，但问题仍然存在。有人可以帮我吗？请不要建议除 RL 相关算法之外的任何其他算法。（比如我不能使用贝叶斯或其他东西，因为我被明确告知要使用 RL）提前谢谢。等待大家的支持和帮助。     提交人    /u/Adventurous_Fly_5564   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0bgi4/trouble_in_continuous_action_space_optimization/</guid>
      <pubDate>Thu, 10 Oct 2024 05:08:13 GMT</pubDate>
    </item>
    <item>
      <title>如何生成这样的图表？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fzz4b4/how_to_generate_such_diagram/</link>
      <description><![CDATA[      大家好， 阅读论文时，我看到许多像这样的图表： https://preview.redd.it/2q6a0gn02std1.png?width=811&amp;format=png&amp;auto=webp&amp;s=b00db0b0980dc291c269128beae1a6d134fd9661 取自 Online Decision Transformer 论文（source）。 查看左图，您可以看到一条蓝色实线和一条红色实线。它们周围有一个阴影，颜色相同但几乎透明。 我猜实线是平均值，阴影是标准差，对吗？ 我真的很好奇：您知道如何制作这样的图表吗？有 Python 库或类似的东西吗？ 谢谢    提交人    /u/WilhelmRedemption   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fzz4b4/how_to_generate_such_diagram/</guid>
      <pubDate>Wed, 09 Oct 2024 18:57:21 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium v​​1.0 发布（核心 API 现已稳定）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fzbbnd/gymnasium_v10_release_core_api_now_stable/</link>
      <description><![CDATA[我们很高兴地宣布 Gymnasium v​​1.0 的发布，它是 OpenAI Gym 的一个维护分支，用于定义强化学习环境。阅读发布说明，了解我们所做的所有更改。这是我们出色的志愿者在过去 3 年的共同努力。在此期间，我们稳步改进了库 - 修复错误、添加新功能和我们认为必要的 API 更改。使用 v1.0，这将是 Gymnasium 的第一个稳定版本，没有计划更改核心 API（Env、Space 或 VectorEnv），这意味着如果您正在等待更新项目，现在是时候了，请参阅我们的迁移指南了解更多信息。    提交人    /u/jkterry1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fzbbnd/gymnasium_v10_release_core_api_now_stable/</guid>
      <pubDate>Tue, 08 Oct 2024 21:36:21 GMT</pubDate>
    </item>
    <item>
      <title>持续动态的策略迭代</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fz7c8k/policy_iteration_for_continuous_dynamics/</link>
      <description><![CDATA[我正在开展一个项目，旨在构建应用于连续动态环境的策略迭代 (PI)实现。值函数 (VF)使用离散状态空间的每个单纯形内的线性插值来近似。插值系数的作用类似于随机过程中的概率，这有助于使用离散马尔可夫决策过程 (MDP)来近似连续动态。该算法由 Gymnasium 提供的 Cartpole 和 Mountain car 环境进行了测试。 Github 链接：DynamicProgramming    提交人    /u/Grouchy_Ad_4112   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fz7c8k/policy_iteration_for_continuous_dynamics/</guid>
      <pubDate>Tue, 08 Oct 2024 18:47:32 GMT</pubDate>
    </item>
    <item>
      <title>杜拉克的人工智能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fz3swl/ai_for_durak/</link>
      <description><![CDATA[我正在开展一个项目，为 Durak 构建 AI，Durak 是一款流行的俄罗斯纸牌游戏，具有不完全信息和多个代理。挑战类似于扑克，但也有一些不同。例如，与扑克中的 52 选择 2 不同，Durak 在发牌时的初始状态为 36 选择 7，这比扑克的状态多 6,000 倍，并且每场游戏中的决策数量要多得多，所以我不确定相同的方法是否可以很好地扩展。玩家拥有不完全信息，但可以根据对手的行为做出推断（例如，如果某人没有防御一张牌，他们可能没有那套花色）。 我正在寻找有关我应该为这种类型的游戏使用哪种 AI 技术或技术组合的建议。我一直在研究的一些事情：  蒙特卡洛树搜索 (MCTS) 和推出 来处理不确定性 强化学习 贝叶斯推理或某种形式的对手建模，根据对手的动作估计隐藏信息 基于规则的启发式，用于捕捉 Durak 独有的特定类人策略  编辑：我假设这个游戏中可能存在纳什均衡，但我主要关心的是考虑到复杂性，计算是否可行。 Durak 的扩展速度非常快，特别是当你增加玩家数量或从 36 张牌的牌组切换到 52 张牌的牌组时。每个玩家开始时有 6 张牌，因此可能的游戏状态数量很快就会变得比扑克还要多得多。 无论是牌组合还是玩家互动，可能性的激增让我担心 MCTS 和 RL 等方法是否能在合理的时间范围内处理游戏的复杂性。    提交人    /u/Iezgin   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fz3swl/ai_for_durak/</guid>
      <pubDate>Tue, 08 Oct 2024 16:20:04 GMT</pubDate>
    </item>
    <item>
      <title>RL 的范围</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fyu4fg/scope_of_rl/</link>
      <description><![CDATA[我是 RL 的新手。我正在学习 RL，基本上我已经看过 YouTube 上的 DRL 和 David silver 视频。1) 我想知道我是否真的应该把时间投入到 RL 中 2) 具体来说，在 RL 中我是否能够找到工作。3) 以及您是如何在这个领域找到工作的。4) 大约需要多少时间学习才能真正在这个领域工作。如果我问这个问题的语气不对或急于找工作，请原谅我，但这就是目的    提交人    /u/Historical-Bid-2029   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fyu4fg/scope_of_rl/</guid>
      <pubDate>Tue, 08 Oct 2024 07:23:49 GMT</pubDate>
    </item>
    <item>
      <title>“语言模型学会通过 RLHF 误导人类”，Wen 等人 2024 年（自然出现对不完美评估者的操纵，以最大化奖励，但不最大化质量）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fynve7/language_models_learn_to_mislead_humans_via_rlhf/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fynve7/language_models_learn_to_mislead_humans_via_rlhf/</guid>
      <pubDate>Tue, 08 Oct 2024 01:05:40 GMT</pubDate>
    </item>
    <item>
      <title>表示状态的临界性或稳定性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fycxzb/representation_of_criticality_or_stability_of_a/</link>
      <description><![CDATA[有人知道在假设一项政策的情况下，从状态计算或了解一般强化学习问题的不稳定程度或失败概率的方法吗？我的目标是：从一组应用程序中，找到一个表示，使我最需要适当的控制。 在控制理论中，存在计算这个的方法，但从我所见（不是专家）来看，它需要很多假设，大部分是线性的，因为非线性相当复杂，需要控制器矩阵和动力学。我想知道是否有类似的东西可以通过强化学习框架来学习？ 对于强化学习问题，为简单起见，我们假设一个不稳定的问题，具有像 cartpole 这样的故障条件。如何仅从转换来估计系统失败或稳定性的概率？显然你可以从角度和位置来做到这一点，但是对于未知的动态，有没有方法可以学习这一点？ 我认为优势是一个可以使用的功能，但它并不完全相同。    提交人    /u/Enryu77   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fycxzb/representation_of_criticality_or_stability_of_a/</guid>
      <pubDate>Mon, 07 Oct 2024 17:11:58 GMT</pubDate>
    </item>
    <item>
      <title>这是一个有效的 RL 问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fya6z5/is_it_a_valid_rl_problem/</link>
      <description><![CDATA[给定一组 html 页面，其中每个 html 页面都是文本段落序列，并且每个段落都被标记为 0 或 1。我可以使用强化学习来学习为 html 页面中的段落序列分配 0 或 1 的最佳策略吗？给出上面标记的数据集。  我认为每个 html 页面都是一个情节，其中可以从每个段落文本中得出状态，并且采取的行动是 0 或 1。 这是一个有效的 RL 问题吗？有人可以指出使用 RL 尝试解决此类问题的论文或链接吗？    提交人    /u/HotCauliflower2360   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fya6z5/is_it_a_valid_rl_problem/</guid>
      <pubDate>Mon, 07 Oct 2024 15:18:56 GMT</pubDate>
    </item>
    <item>
      <title>强化学习在游戏中有什么应用吗？（不是玩游戏，而是在游戏中使用）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fy1yhe/are_there_any_applications_of_rl_in_games_not/</link>
      <description><![CDATA[我对 RL 还很陌生，对我来说，它一直与游戏密切相关。然而，经过一段时间的接触，我注意到，就游戏而言，RL 仅用于“解决”问题。我从未见过有人试图将其用于游戏内的 AI 或其他系统    提交人    /u/Vefery   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fy1yhe/are_there_any_applications_of_rl_in_games_not/</guid>
      <pubDate>Mon, 07 Oct 2024 07:22:38 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 RL 游戏项目的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fxocnu/need_ideas_for_a_rl_in_games_project/</link>
      <description><![CDATA[这学期我在大学被分配做一个项目。我对游戏中的 RL 很感兴趣（或类似的东西），所以我选择它作为主题。而且由于这只是一项小研究，我需要得到一些有意义的结果。比如训练一个模型并观察它在不同场景和不同条件下的表现。但老实说，我完全没有主意了 我有使用 Unity 的经验，所以构建自定义环境不是问题。而且这个项目不需要非常复杂或取得突破。实际上我需要能够在 3-4 个月内完成它    提交人    /u/Aydiagam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fxocnu/need_ideas_for_a_rl_in_games_project/</guid>
      <pubDate>Sun, 06 Oct 2024 19:16:00 GMT</pubDate>
    </item>
    </channel>
</rss>