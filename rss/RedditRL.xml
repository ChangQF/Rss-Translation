<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 14 Mar 2024 03:14:26 GMT</lastBuildDate>
    <item>
      <title>“如何生成和使用合成数据进行微调”，Eugene Yan</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be20hu/how_to_generate_and_use_synthetic_data_for/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be20hu/how_to_generate_and_use_synthetic_data_for/</guid>
      <pubDate>Wed, 13 Mar 2024 20:34:15 GMT</pubDate>
    </item>
    <item>
      <title>Boid植绒环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be1j8p/boid_flocking_environment/</link>
      <description><![CDATA[我有一个自定义的 Boid 植绒环境在 OpenAI Gym 中使用来自 StableBaselines3 的 PPO。我希望它能够实现聚集。 问题： 我试图让 boids 聚集，但尽管有合理的距离，但它们不会一起移动，并且我的损失范围是 10e^4 - 10e^5。  我的方法： 我有两个奖励函数，一个检查凝聚力和分离，另一个检查对齐，如果群体远离邻居并给予奖励，它们就会受到惩罚-100 奖励，否则奖励本质上是标量。 训练中的情节在发生碰撞等时重置。 视频： 错误行为 奖励功能：  def Reward1(self, agent, neighbor_velocities): out_of_flock=False Total_reward=0 multiplier=(len(neighbor_velocities)) if (len(neighbor_velocities) &gt; 0):average_velocity = np.mean(neighbor_velocities) , axis=0)desired_orientation=average_velocity-agent.velocityorientation_diff=np.arctan2(desired_orientation[1],desired_orientation[0])-np.arctan2(agent.velocity[1],agent.velocity[0])#距离如果方向_差异&gt; np.pi：orientation_diff -= 2 * np.pi eliforientation_diff &lt; 0:orientation_diff += 2*np.pitotal_reward=100*乘数*(1-np.abs(orientation_diff))elif(len(neighbor_velocities)==0):out_of_flock=Truetotal_reward-=100returntotal_reward,out_of_flockdefReward2 (self,agent,neighbor_positions):total_reward=0 multiplier=(len(neighbor_positions)) if (len(neighbor_positions)&gt;0):对于neighbor_positions中的neighbor_position:distance = np.linalg.norm(agent.position - neighbor_position) if (distance  完整代码：代码 如有任何建议，我们将不胜感激。我应该如何重新制定我的奖励函数等？ ​   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be1j8p/boid_flocking_environment/</guid>
      <pubDate>Wed, 13 Mar 2024 20:15:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 rllib 的感觉如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be12gr/how_it_feels_using_rllib/</link>
      <description><![CDATA[     &lt; td&gt; 由   提交 /u/rl_is_best_pony   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be12gr/how_it_feels_using_rllib/</guid>
      <pubDate>Wed, 13 Mar 2024 19:56:42 GMT</pubDate>
    </item>
    <item>
      <title>批量强化学习帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdzp2z/batch_reinforcement_learning_help/</link>
      <description><![CDATA[ 由   提交/u/pokes41  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdzp2z/batch_reinforcement_learning_help/</guid>
      <pubDate>Wed, 13 Mar 2024 19:01:53 GMT</pubDate>
    </item>
    <item>
      <title>连续 Alphazero 算法的状态和有效性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdvt39/state_and_effectiveness_of_continuous_alphazero/</link>
      <description><![CDATA[您好， 我有兴趣在具有连续状态和动作空间的环境中使用基于 MCTS 的 RL 算法。 Moerland 等人在 2018 年针对此设置提出了 Alphazero 的变体。该论文仅获得 61 次引用（大部分来自评论） ，所以看起来这并没有被广泛采用。  所以我想知道是否有人尝试过这个算法，或者知道其他连续动作空间的方法。    由   提交/u/Playmad37  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdvt39/state_and_effectiveness_of_continuous_alphazero/</guid>
      <pubDate>Wed, 13 Mar 2024 16:30:26 GMT</pubDate>
    </item>
    <item>
      <title>从像素中学习。你预训练 CNN 了吗？您知道好的 3x3 过滤器吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdsoxm/learning_from_pixels_do_you_pretrain_cnn_do_you/</link>
      <description><![CDATA[亲爱的社区， 最近我开始使用 RGB 相机的灰度像素 (32x32) 来增强状态。为什么我不愿意这样做-&gt;因为 State 可能是冗余的，并且可能需要训练 CNN 网络，这会使 State 出错且不稳定。 如果我训练 CNN 网络，那么 State 一开始就明显有噪声。你训练它吗？ 如果我决定不训练它：那么我需要过滤器。我知道 2 边缘 3x3 过滤器值。但我需要更多， 如果深度是(1,4,8)那么我需要4+8=12个过滤器。 我清楚地理解为什么CNN这么好，因为它掌握像素之间的局部关系。 除了这些过滤器之外，您还有好的过滤器吗： [[1, 0, -1], [0 , 0, 0], [-1, 0, 1]] [[0, 1, 0], [1, -4 , 1], [0, 1, 0]] ​   由   提交/u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdsoxm/learning_from_pixels_do_you_pretrain_cnn_do_you/</guid>
      <pubDate>Wed, 13 Mar 2024 14:22:25 GMT</pubDate>
    </item>
    <item>
      <title>内存学习：大型语言模型的声明式学习框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdoco3/inmemory_learning_a_declarative_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.02757 摘要：  探索智能体是否可以在不依赖环境的情况下与环境保持一致人类标记的数据提出了一个有趣的研究主题。从智能生物体中观察到的对齐过程中汲取灵感，其中陈述性记忆在总结过去的经验中发挥着关键作用，我们提出了一种新颖的学习框架。代理熟练地从过去的经验中提取见解，完善和更新现有笔记，以提高他们在环境中的表现。整个过程发生在记忆组件内，并通过自然语言实现，因此我们将这个框架描述为内存学习。我们还深入研究了旨在评估自我改进过程的基准的主要特征。通过系统的实验，我们证明了我们的框架的有效性，并提供了对该问题的见解。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdoco3/inmemory_learning_a_declarative_learning/</guid>
      <pubDate>Wed, 13 Mar 2024 10:40:29 GMT</pubDate>
    </item>
    <item>
      <title>如何解释 Epoch 0 上的平均剧集奖励差异？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdhc2n/how_to_interpret_mean_episode_reward_differences/</link>
      <description><![CDATA[      我正在关注此 PyTorch 教程使用广义优势估计/PPO 在 RL 环境中训练不同的网络架构。我想比较不同风格网络的性能，因此我目前单独训练每个网络并保存每个训练时期的平均情节奖励值；我的想法是，随着时间的推移，一些网络会学习得更快，并在平均情节奖励中表现出急剧的跳跃，而其他网络可能会有更平坦的曲线。但是，我的结果如下所示： ​ X 轴：训练纪元#，Y 轴：每集平均奖励 似乎一个网络的表现已经比之前好近 4 倍另一个是从 Epoch 0 开始训练。我对 GAE loss / PPO 工作原理的理解是，首先收集每批的所有帧，然后使用梯度下降来更新所有参数。更新后的参数用于下一轮数据收集。这让我对如何解释蓝色网络在 Epoch 0 上的相对成功感到困惑（如果参数的更新直到下一轮数据收集才生效）？这是否仅仅反映了网络之间的某种初始化差异，或者它们是否有可能“学习”了网络之间的初始化差异？在计算第一个平均奖励时会发生什么？训练循环代码基本上来自链接的教程，如下： episode_reward_mean_list = [] for tensordict_data in Collector: with torch.no_grad(): GAE(tensordict_data, params=loss_module.critic_network_params, target_params =loss_module.target_critic_network_params, ) # 获得优势 data_view = tensordict_data.reshape(-1) replay_buffer.extend(data_view) # 重新填充缓冲区 for ep in range(num_epochs): for _ in range(frames_per_batch // minibatch_size): subdata = replay_buffer .sample() loss_vals = loss_module(子数据) loss_value = ( loss_vals[&quot;loss_objective&quot;] + loss_vals[&quot;loss_critic&quot;] + loss_vals[&quot;loss_entropy&quot;] ) loss_value.backward() optim.step() optim.zero_grad （）collector.update_policy_weights_（）完成=tensordict_data.get（（“下一个”，“完成”））episode_reward_mean =（tensordict_data.get（（“下一个”，“episode_reward”））.mean（）.item () ) Episode_reward_mean_list.append(episode_reward_mean)    由   提交/u/brantacanadensis906   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdhc2n/how_to_interpret_mean_episode_reward_differences/</guid>
      <pubDate>Wed, 13 Mar 2024 03:22:21 GMT</pubDate>
    </item>
    <item>
      <title>用代码分享我的强化学习教程系列。从基础知识开始，使用 PyTorch 中的 PPO 扩展到深度强化学习。让我知道你的想法！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdfxz2/sharing_my_tutorial_series_on_reinforcement/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交 /u/MrSirLRD   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdfxz2/sharing_my_tutorial_series_on_reinforcement/</guid>
      <pubDate>Wed, 13 Mar 2024 02:17:57 GMT</pubDate>
    </item>
    <item>
      <title>需要 NEAT-python 帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd9bga/need_help_with_neatpython/</link>
      <description><![CDATA[我正在尝试使用简洁的 python 来实现像 BipedalWalker 这样的简单示例，但训练从未开始，相反我只得到： ****** Running第 0 代 ****** 这是代码： i导入操作系统 导入pickle  导入整洁 将numpy导入为np #import cart_pole &lt; code&gt;将gymnasium导入为gym runs_per_net = 5 #使用NN网络表型和离散执行器力函数。 def eval_genome(genome, config): net=neat.nn.FeedForwardNetwork.create(genome, config) &lt; p&gt;fitnesses = [] 对于范围内的跑步(runs_per_net)： env =gym.make(&quot; BipedalWalker-v3&quot;) 观察，_ = env.reset() fitness = 0.0 done = False 未完成时： action = net.activate(observation) 观察、奖励、完成、_,info = env.step(action) 健身+=奖励 #env.render () fitnesses.append(fitness) return np.mean(fitnesses) def eval_genomes(genomes, config): 对于genome_id，基因组中的基因组： genome.fitness = eval_genome(genome, config) def run():&lt; br /&gt; # 加载配置文件，假定该文件与此脚本位于同一目录中。 local_dir = os.path.dirname(__file__)  config_path = os.path.join(local_dir, &#39;config.txt&#39;) config =整齐.Config(neat.DefaultGenome,整齐.DefaultReProduction,代码&gt;neat.DefaultSpeciesSet，neat.DefaultStagnation，config_path)pop =neat.Population( config) stats = freeze.StatisticsReporter() pop.add_reporter(stats) pop. add_reporter(neat.StdOutReporter(True)) # 运行 NEAT 算法 winner = pop.run(eval_genomes)  # 保存获胜者。 with open(&#39;winner&#39;, &#39;wb&#39;) as f: pickle.dump(winner, f) print(winner) run() 我使用了这个教程。配置文件只有 pop 大小从 250 更改为 20，但我不明白为什么它不训练   由   提交 /u/SebyR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd9bga/need_help_with_neatpython/</guid>
      <pubDate>Tue, 12 Mar 2024 21:34:35 GMT</pubDate>
    </item>
    <item>
      <title>Deepmind：停止回归：通过可扩展深度强化学习的分类训练价值函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd7frj/deepmind_stop_regressing_training_value_functions/</link>
      <description><![CDATA[ 由   提交/u/LushousLightfoot  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd7frj/deepmind_stop_regressing_training_value_functions/</guid>
      <pubDate>Tue, 12 Mar 2024 20:20:42 GMT</pubDate>
    </item>
    <item>
      <title>大多数 RL 库中的控制方向似乎是相反的</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd5b2f/the_direction_of_control_in_most_rl_libraries/</link>
      <description><![CDATA[看看像 Stable Baseline 3 这样的库，在我看来，控制的方向与应有的方向相反。 据我所知，我看到的几乎所有 RL 示例都假设有一个明确定义的“环境”。风格对象能够评估动作并作为单个统一函数产生奖励（例如 SB3 的 env 规范的 step） 似乎还有一个关于训练的假设，然后部署（以“赢得竞争”的方式）。 现在，这并没有从根本上限制人们可以使用 SB3 之类的东西做什么，但它使“现实世界”成为现实。用例更加困难。  假设例如控制机器人，其中机器人的“动作”是由机器人执行的。可能有许多利益相关者（远程命令、紧急停止、取代 RL 操作的硬编码规则、修改操作的硬编码约束...等） 在这种环境（任何现实世界的应用程序）中，它将 RL 环境作为一种“服务”是有意义的。样式实体，而不是最高级别的协调器 - 即公开诸如 recommend_action(inputs) （或 predict(inputs)）和 reward(奖励，[prev_inputs，prev_outputs]）（或step或train ...名称并不相关） 我是95% 确信“我不明白”出于某种原因，现在常见的接口要么更好，要么必要，但如果有人可以帮助减轻我对这些设计选择的困惑，我可能会更多地了解 RL 库生态系统及其约束条件。进化了。   由   提交/u/elcric_krej   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd5b2f/the_direction_of_control_in_most_rl_libraries/</guid>
      <pubDate>Tue, 12 Mar 2024 18:57:01 GMT</pubDate>
    </item>
    <item>
      <title>我制作这个并行模拟管理器很有趣，所以我想分享它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd1xyb/i_had_fun_making_this_parallel_simulation_manager/</link>
      <description><![CDATA[   /u/hbonnavaud  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd1xyb/i_had_fun_making_this_parallel_simulation_manager/</guid>
      <pubDate>Tue, 12 Mar 2024 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>PPO 多离散输出</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd1wo4/ppo_multidiscrete_output/</link>
      <description><![CDATA[大家好， 我很好奇是否有人对如何改进 PPO 算法以输出多离散有任何提示。我目前正在尝试做的是创建一个“经理”指导其他代理完成哪个任务的代理。为了做到这一点，我需要动作 0、1 或 2 的 4 个不同输出。但我不太确定如何做到这一点，因为我不能只做一个 softmax 层。我尝试过建立一个多头网络，在 4 个不同的头上执行 softmax 并返回每个头的 argmax，但随后我担心 4 个不同头的损失函数。 任何的意见都将会有帮助！谢谢！   由   提交/u/Cheap_Leather_6432   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd1wo4/ppo_multidiscrete_output/</guid>
      <pubDate>Tue, 12 Mar 2024 16:44:53 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>