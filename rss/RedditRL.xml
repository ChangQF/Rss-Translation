<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 14 Feb 2025 03:22:22 GMT</lastBuildDate>
    <item>
      <title>RL无法改善基本监督模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</link>
      <description><![CDATA[      i具有基于序列预测的合理工作的基于基于的模型（RNN）。然后，我创建了一个PPO RL模型来调整PRE的输出 - 训练的RNN模型。问题：RL实际上降低了MSE度量。我有些惊讶RL实际上可能会受到如此巨大的伤害。&lt; /p&gt;   MSE，没有RL调整：0.000047  MSE带有RL调整：0.002053    验证MSE vs Iteration     &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/u/maadotaa     [link]   ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</guid>
      <pubDate>Fri, 14 Feb 2025 00:00:09 GMT</pubDate>
    </item>
    <item>
      <title>“使用大型推理模型[O3]的竞争性编程”，El-Kishky等人2025 {oa}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</guid>
      <pubDate>Thu, 13 Feb 2025 22:56:18 GMT</pubDate>
    </item>
    <item>
      <title>Langevin Soft Actor-Critic：通过不确定性驱动的批评者学习，Ishfaq等人2025。ICLR 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</link>
      <description><![CDATA[        &lt;！ -  sc__off- sc_off-&gt;  现有的Actor-Critic算法在连续控制加强学习（RL）任务中很受欢迎，由于其中缺乏原则性的探索机制，其样本效率不佳。由于汤普森采样成功在RL中有效探索的动机，我们提出了一种新颖的无模型RL算法，\ emph {langevin soft Actor评论家}（LSAC）（LSAC）优先考虑通过对策略优化的不确定性估计来增强评论家的学习。 LSAC采用了三个关键创新：通过基于分布的LangeMonte Carlo（LMC）更新，近似汤普森采样，平行回火，用于探索该功能后部多种模式，以及与动作梯度正常化的综合状态行动样品。我们的广泛实验表明，LSAC的表现优于或匹配无连续控制任务主流模型RL算法的性能。值得注意的是，LSAC标志着基于LMC的Thompson采样在具有连续动作空间的连续控制任务中的首次成功应用  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hmi2015     [link]  ＆＃32;   [注释]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</guid>
      <pubDate>Thu, 13 Feb 2025 18:50:08 GMT</pubDate>
    </item>
    <item>
      <title>我的个人项目-Alphayinshzero（Blitz）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</link>
      <description><![CDATA[&lt; src =“ https://b.thumbs.redditmedia.com/qwtnkjppq5js2x​​jru2xjru2xv2ulsrfp8lpgjynuhfkuwg7i.jpg” title =“我的个人项目-Alphayinshzero（Blitz） - &gt;  我使用alphazero训练了闪电兹版本的AI模型，并且它能够在板空间上击败Smartbot。 请注意，Blitz版本是您尝试连续获得5个的地方。 这是迭代174对抗自己。  https://preview.redd.it/1ohn81p23yie1.png?width=866&amp;format=png&amp;auto=webp&amp;s=3d7261cdc446f7349eae31fe5bca4b66b8bcbfed&lt; /a&gt;  在训练期间，有充分的证据表明，闪电战版本具有第一球员优势，因为第一个球员逐渐攀升至最后的胜利率。 我是强化学习的新手，当涉及政策分配时，我可能会提出一种特殊的方法，因此请随时告诉我这是否是一种有效的方法，或者在AI培训中是否有问题。 我将yinsh表示为11 x 11数组，因此动作空间为121 + 1（通过转弯）。 我想避免使用大型政策分布，例如121（statter） * 121（目的地）= 14641  所以，我将游戏分为阶段：戒指放置（放置10个戒指），戒指选择（选择要移动的戒指）和标记放置（放置一个（放置一个）标记和移动选定的戒指）。 这样，单人的回合像这样可以工作： 转1-选择要移动的戒指。转弯2-对手通过。转3-选择要移动戒指的位置。&lt; /p&gt; 通过将其分为阶段，我可以使用121 + 1的动作空间。这种方法“感觉”对我而言。它似乎有效。  ...  我试图训练Yinsh的完整游戏，但这是不完整的。到目前为止，我对它的策略感到非常满意。 不满意，我的意思是它只是沿着边缘形成了一个密集的标记领域，他们不想互相互动。我真的希望AI战斗和造成混乱，但他们太宁静了 - 只是在关注自己的生意。通过沿着边缘形成致密的标记，标记变得难以置信。  AI的（天真？）方法只是：“让我像农民一样在边缘形成标记领域来自同一地区的5英寸排。”他们就像董事会对面的两个农民，宁静地制作了自己的标记领域。 闪电战版更令人兴奋，而人工智能互相战斗：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kiwigami     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</guid>
      <pubDate>Thu, 13 Feb 2025 18:29:17 GMT</pubDate>
    </item>
    <item>
      <title>当您已经拥有体育馆环境时，SB3矢量化环境如何工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我无法完全理解。您只是使用他们的Vecenv包装它吗？还是我必须重写它？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/blearx     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</guid>
      <pubDate>Thu, 13 Feb 2025 17:33:39 GMT</pubDate>
    </item>
    <item>
      <title>使用多个跑步者的rllib不会增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对不起，在这里绝对没有图片。  因此，我的问题是在RLLIB上使用SAC的24个ENV跑步者，根本没有学习。但是，使用2个Env跑步者确实学习了（有点）。 &lt; /p&gt; 详细信息： env-是简单的2D移动到目标位置，当目标状态达到-0.01时，稀疏奖励，每次步骤达到-0.01，带有500个框架限制，带有框（Shape =（10，））观察和框（-1,1）动作空间。我尝试了一堆超参数，但似乎没有用。对Rllib非常新。我曾经制作自己的RL库，但我想这次尝试rllib。 有人知道问题是什么吗？如果您需要更多信息，请问我！谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/automatic-web8429     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</guid>
      <pubDate>Thu, 13 Feb 2025 14:53:55 GMT</pubDate>
    </item>
    <item>
      <title>参考丢失：带有RL算法分类法/本体的电子表格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在这里的某个地方看到了它，现在找不到它。我知道有一些调查RL算法的论文，但我正在尝试找到“散布表”，其中一位成员在评论中发布。我相信这是Google文档的链接。 每行都有一些更高级别的分组，每个组和注释中都有算法。它通过其属性（例如连续的动作空间等）将算法分开。 有人知道该资源还是我可以找到的地方？ 编辑：找到它！  https://rl-picker.github.io/     &lt;！ -  sc_on- &gt;＆＃32;提交由＆＃32; /u/u/u/paramedicfabulou345     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</guid>
      <pubDate>Thu, 13 Feb 2025 11:41:11 GMT</pubDate>
    </item>
    <item>
      <title>没有机器学习的强化学习，这可以做到吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioezsw/reinforcement_learning_without_machine_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我对[回归 +分类 +群集 +关联规则]有知识。我了解数学方法和算法，但不了解代码（我有一个 现在，我想了解计算机视觉和增强学习。 所以任何人都可以让我知道如果我可以在不编码ML的情况下学习加固？ /user/internationalwill912“&gt;/u/u/internationalwill912     ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioezsw/reinforcement_learning_without_machine_learning/</guid>
      <pubDate>Thu, 13 Feb 2025 08:59:41 GMT</pubDate>
    </item>
    <item>
      <title>阿凡达语音模型/管道的好文字是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  主要是这样。你们建议哪种管道生成一个化身 - 修复了所有报告的化身 - 可以阅读文本？ （理想情况下，开源，因为我可以访问GPU群集，并且不想为第三方服务付费 - 因为我将提供明智的信息）。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/gvascons     [link]   ＆＃32;   [commist]        ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</guid>
      <pubDate>Thu, 13 Feb 2025 05:25:12 GMT</pubDate>
    </item>
    <item>
      <title>谢尔盖·莱文（Sergey Levine）的增强学习[我在哪里可以找到这个]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi     作为初学者，我希望对RL背后的数学背后的数学掌握。 ##您能让我知道在哪里可以找到这门课程？请。 ##     [sutton barto]增强学习= https://www.amazon.in/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249?dplnkId=c3df8b9c-8d63-4f9b-8a4e-bc601029852c     遵循的其他资源是什么？您能吸引他们使用的它们吗？请   我也   我开始学习ML，并想问这里有经验的人有关理解数学的要求像k-nn/svm  一样，在每种算法后面证明了在算法后面的数学或可以观看视频，了解关键，然后开始编码 研究ML的合适方法是什么？ ## ML工程师会涉足很多编码，还是通过可视化和开始编码来底层crux？ 请让我知道。 （我在这个域中没有希望）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/unternationalwill912     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</guid>
      <pubDate>Thu, 13 Feb 2025 03:02:00 GMT</pubDate>
    </item>
    <item>
      <title>有人在朱莉娅（Julia）中有PPO RL的示例吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  似乎我发现的所有代码库都是过时且非功能的。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/d3mz     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</guid>
      <pubDate>Wed, 12 Feb 2025 16:57:19 GMT</pubDate>
    </item>
    <item>
      <title>击败当前超级马里奥的第一级的最佳RL方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我看过PPO，DQN和Neat。 Sethbling在2015年写了一位使用Neat的RL代理，看起来它表现出了最好的表现。休息了4年后，我正在回到RL空间，并希望在Python中实施个人项目。我应该实施哪一个？有新方法吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/marblesandcookies     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</guid>
      <pubDate>Wed, 12 Feb 2025 16:10:25 GMT</pubDate>
    </item>
    <item>
      <title>体育馆环境的代理动力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，有人知道我如何访问Openai体育馆安全体育馆的代理商的动态吗？  通常。Step（）直接模拟动力学，但是我需要应用程序中的动力学，因为我需要相对于这些动力学进行区分。为了更具体，我需要计算f（x）的梯度和g（x）的梯度，其中x_dot = f（x）+g（x）u。 x是状态，u是输入（动作） 我总是可以将其视为黑匣子并学习它们，但我更喜欢直接从地面真相动态中得出梯度。 请让我知道！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/limp-ticket7808     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</guid>
      <pubDate>Wed, 12 Feb 2025 15:57:36 GMT</pubDate>
    </item>
    <item>
      <title>RL和机器人技术的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我最近在RL（技术上是反向RL）的博士学位毕业于人类机器人协作。我已经使用了4种不同的机器人操纵器，4个不同的抓钉和4台不同的RGB-D摄像头。我的专业知识在于使用感知反馈来学习智能行为，以进行安全有效的操纵。  我已经建造了端到端管道，用于在传送带上制作产品，在到达孵化器之前，非破坏性识别和去除不育卵，使用机器人对医疗器械进行智能无菌处理，还有一些其他项目。我已经在三菱电气研究实验室实习，到目前为止在顶级会议上发表了6篇论文。 我已经使用了许多对象检测平台，例如Yolo，例如Yolo，更快RCNN，detectron2，Mediapipe，Mediapipe，Mediapipe，等等，并具有丰富的注释和培训经验。我对Pytorch，ROS/ROS2，Python，Scikit-Learn，Opencv，Mujoco，Gazebo，Pybullet，并且在Wandb和Tensorboard方面有一定的经验。由于我不是来自CS背景，所以我不是专家软件开发人员，但是我写了稳定，干净，下降的代码，很容易扩展。 我一直在寻找与此相关的工作，但是我很难在就业市场上导航。我真的很感谢您提供的任何帮助，建议，建议等。作为一个学生签证的人，我正在时钟，需要尽快找到工作。预先感谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/prasuchit     [链接]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:05 GMT</pubDate>
    </item>
    <item>
      <title>我建立了一个网站来找到RLHF工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我们在AI中有多个学科的作业。我们也有专门用于RLHF作业的页面。在过去的30天中，我们有48个涉及RLHF的工作机会。 您可以在此处找到所有RLHF工作：   https://www.moaijobs.com/rlhf-jobs   请让我知道您的想法。谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/wordybug     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</guid>
      <pubDate>Wed, 12 Feb 2025 02:20:52 GMT</pubDate>
    </item>
    </channel>
</rss>