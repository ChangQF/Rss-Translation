<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 26 Sep 2024 09:18:02 GMT</lastBuildDate>
    <item>
      <title>... 天网？集中式还是分散式聊天GPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpf2of/skynet_centralized_or_decentralized_chatgpt/</link>
      <description><![CDATA[矿工试图解密区块链哈希中的随机数。 为什么不建立一个可以推翻科技巨头的庞大模型来赚点钱呢。 并行计算不是一个新领域。需要虚拟化任务，使其 100% 独立于硬件。 如果我们采用最强大的模型，例如决策变压器。为每个任务建立一个机器池，优先考虑最强大的机器，这样延迟就会很低，如果一台机器坏了，其他机器可以完成这项工作。这是 N 个并行作业。它甚至可以是三维的 - 池、并行任务、同时给出的并行系列并行任务。 人们可以考虑去中心化。如果我们添加具有一致哈希的区块链技术。但是与集中式...天网相比，并发性和能源效率较低。 普通人，我可以梦想利用我的旧台式电脑赚点钱吗....    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpf2of/skynet_centralized_or_decentralized_chatgpt/</guid>
      <pubDate>Wed, 25 Sep 2024 20:55:21 GMT</pubDate>
    </item>
    <item>
      <title>乐高遇见人工智能：BricksRL 被 NeurIPS 2024 接受！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/</link>
      <description><![CDATA[      我们很高兴地告诉大家，我们关于 BricksRL 的论文已被 NeurIPS 2024 接受为焦点论文，BricksRL 是一个 RL 算法库，可以在价格实惠的定制 LEGO 机器人上进行训练和部署！ 随着人工智能和机器学习继续掀起波澜，我们认为向社区提供可靠且价格合理的教育工具至关重要。并非每个人都可以使用数百个 GPU，而且了解 ML 在实践中的工作原理可能具有挑战性。 这就是我们一直在研究 BricksRL 的原因，这是庞培法布拉大学和 PyTorch 之间的合作项目。我们的目标是为人们提供一种有趣且引人入胜的方式来学习 AI、ML、机器人技术和 PyTorch，同时保持高标准的正确性和稳健性。 BricksRL 基于 Pybricks，可以部署在许多不同的 LEGO 集线器上。我们希望它能让世界各地的实验室以可承受的价格制作原型，而无需昂贵的机器人。 查看我们的网站：https://bricksrl.github.io/ProjectPage/ 该库在 GitHub 上根据 MIT 许可开源：https://github.com/BricksRL/bricksrl/ 阅读我们的论文：https://arxiv.org/abs/2406.17490 观看机器人的实际操作：https://www.youtube.com/watch?v=k_Vb30ZSatk&amp;t=10s 我们正在开展一些令人兴奋的后续项目，敬请期待！ 温哥华见 https://preview.redd.it/1ghfs9t9l0rd1.jpg?width=2006&amp;format=pjpg&amp;auto=webp&amp;s=868867adcd52825bd4ee719513a454527d017307    提交人    /u/AdCool8270   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/</guid>
      <pubDate>Wed, 25 Sep 2024 20:23:37 GMT</pubDate>
    </item>
    <item>
      <title>了解机器学习从业者在构建隐私保护模型方面面临的挑战和需求</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpaxuh/understanding_machine_learning_practitioners/</link>
      <description><![CDATA[您好 我们是匹兹堡大学的一支研究团队。我们正在研究 ML 开发人员在构建隐私保护模型时遇到的问题、挑战和需求。如果您从事 ML 产品或服务工作，请帮助我们回答以下问卷：https://pitt.co1.qualtrics.com/jfe/form/SV_6myrE7Xf8W35Dv0 谢谢！    提交人    /u/MaryAD_24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpaxuh/understanding_machine_learning_practitioners/</guid>
      <pubDate>Wed, 25 Sep 2024 18:00:45 GMT</pubDate>
    </item>
    <item>
      <title>我上一篇关于最佳资源受到喜爱的文章。在这里，我分享了一条详细的路径，一步一步引导你顺利进入 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fp665f/my_last_post_on_best_resources_are_loved_here_i/</link>
      <description><![CDATA[    /u/Fair_Detective_6568   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fp665f/my_last_post_on_best_resources_are_loved_here_i/</guid>
      <pubDate>Wed, 25 Sep 2024 14:44:22 GMT</pubDate>
    </item>
    <item>
      <title>交易的策略梯度，窦性例子</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fp2dlb/policy_gradient_for_trading_toy_example_on_sinus/</link>
      <description><![CDATA[        提交人    /u/Grouchy_Purpose8206   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fp2dlb/policy_gradient_for_trading_toy_example_on_sinus/</guid>
      <pubDate>Wed, 25 Sep 2024 11:44:37 GMT</pubDate>
    </item>
    <item>
      <title>MARL 在代理之间共享训练示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fow21c/marl_with_sharing_of_training_examples_between/</link>
      <description><![CDATA[您好， 我是一名学生，刚刚开始对 RL 和 MARL 进行一些初步研究，我正在尝试适应不同的子领域。我所想象的场景具有以下特点：  训练是分散的；环境仅部分可观察；并且代理具有不相同的奖励 代理在训练期间相互通信 代理间通信包括（选择性）共享训练示例  这样的场景的一个例子可能是正在学习个性化推荐系统的移动应用程序网络，但在隐私敏感区域中，因此数据只能根据用户的隐私偏好进行共享，并且只能以用户可审核的方式共享（因此联合学习，直接共享模型参数或发明的语言不会这样做）。 如果这个问题有点模糊或格式不正确，请原谅。我真的只是在寻找一些可以帮助我进行研究的关键字或调查论文链接。 编辑： 我发现https://arxiv.org/pdf/2311.00865听起来就像我在说什么。    提交人    /u/ConceptOk2393   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fow21c/marl_with_sharing_of_training_examples_between/</guid>
      <pubDate>Wed, 25 Sep 2024 04:19:08 GMT</pubDate>
    </item>
    <item>
      <title>具有 LSTM、CNN、FC 层和图形注意力网络的多智能体强化学习 A2C</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fofr68/multi_agent_reinforcement_learning_a2c_with_lstm/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fofr68/multi_agent_reinforcement_learning_a2c_with_lstm/</guid>
      <pubDate>Tue, 24 Sep 2024 15:48:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 SB3 DQN 上的 LunarLander 性能不佳？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1foe4p8/why_does_my_lunarlander_on_sb3_dqn_not_perform/</link>
      <description><![CDATA[我从这里获得了最佳超参数。因此，我期望算法能够达到最佳性能，即在训练结束时频繁获得 200 的情景奖励。但这并没有发生。  我已在此处附加我的代码 - https://pastecode.io/s/evo1c0ku 有人可以帮忙吗？    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1foe4p8/why_does_my_lunarlander_on_sb3_dqn_not_perform/</guid>
      <pubDate>Tue, 24 Sep 2024 14:40:50 GMT</pubDate>
    </item>
    <item>
      <title>我正在学习 RL，并且取得了很大的进步。我总结了我认为非常有用的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fobu5v/im_learning_rl_and_making_good_progress_i/</link>
      <description><![CDATA[        由    /u/Fair_Detective_6568   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fobu5v/im_learning_rl_and_making_good_progress_i/</guid>
      <pubDate>Tue, 24 Sep 2024 12:57:34 GMT</pubDate>
    </item>
    <item>
      <title>用于一般和博弈（即合作）的 MuZero 风格算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fo7css/muzero_style_algorithms_for_generalsum_games_ie/</link>
      <description><![CDATA[大家好， 我对将 MuZero 应用于合作纸牌游戏很感兴趣。阅读论文 https://arxiv.org/pdf/1911.08265 时，我注意到附录 B 中提到“... 一种渐近收敛到零和游戏中的极小最大值函数的规划方法”。由于我正在处理一般和博弈，因此我对最大-最大方案更感兴趣。 这里有没有地方知道这样做的作品/项目/论文？ 谢谢！    提交人    /u/Arconer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fo7css/muzero_style_algorithms_for_generalsum_games_ie/</guid>
      <pubDate>Tue, 24 Sep 2024 08:08:07 GMT</pubDate>
    </item>
    <item>
      <title>有行为分析师吗？……你们在招人吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnv8vh/any_behavior_analysts_out_there_are_you_hiring/</link>
      <description><![CDATA[有没有公司了解 RL 中行为分析的价值？RL 来自行为分析，但这两个领域似乎没有太多交流。我正试图进入 RL 行业，但不确定如何传达我十多年的专业知识。    提交人    /u/1fission   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnv8vh/any_behavior_analysts_out_there_are_you_hiring/</guid>
      <pubDate>Mon, 23 Sep 2024 20:58:25 GMT</pubDate>
    </item>
    <item>
      <title>“AI 研究所” 到底是什么？似乎与波士顿动力公司有着密切的联系。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnszu8/what_is_the_ai_institute_all_about_seems_to_have/</link>
      <description><![CDATA[“AI 研究所”到底是什么？似乎与波士顿动力公司有着密切的联系。 但我听说他们是由现代资助的？他们的研究重点是什么？产品？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnszu8/what_is_the_ai_institute_all_about_seems_to_have/</guid>
      <pubDate>Mon, 23 Sep 2024 19:24:43 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习解决高度随机的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnptsy/solving_highly_stochastic_environments_using/</link>
      <description><![CDATA[我一直在研究一个高度随机环境中的强化学习 (RL) 问题，其中噪声的影响远远超过代理操作的影响。为了说明这一点，请考虑以下示例： $ s&#39; = s + a + \epsilon $ 其中：  $ \epsilon \sim \mathcal{N}(0, 0.3)$ 是均值为 0、标准差为 0.3 的高斯噪声。 $ a \in {-0.01, 0, 0.01}$ 是代理可以采取的操作。  在此设置中，噪声 $\epsilon $ 主导动态，相比之下，代理操作的影响可以忽略不计。因此，使用标准 Q 学习进行学习被证明是低效的，因为噪声会淹没学习信号。 问题：在随机性（或噪声）比代理操作的影响大得多的环境中，我如何才能有效地学习？是否有更适合处理这种情况的替代 RL 算法或方法？ PS：向状态添加额外信息是一种选择，但可能并不有利，因为它会增加我现在试图避免的状态空间。 任何关于如何解决这个问题的建议或对类似工作的引用都将不胜感激！有人遇到过类似的问题吗？你是怎么解决的？提前谢谢您！    提交人    /u/Hey--Macarena   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnptsy/solving_highly_stochastic_environments_using/</guid>
      <pubDate>Mon, 23 Sep 2024 17:15:24 GMT</pubDate>
    </item>
    <item>
      <title>PPO 学得很好，但随后奖励不断减少</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnldyh/ppo_learns_quite_well_but_then_reward_keeps/</link>
      <description><![CDATA[      嘿，我正在使用 SB3 中的 PPO（在自己的自定义环境中），使用以下设置： policy_kwargs = dict( net_arch=dict(pi=[64,64], vf=[64,64])) log_path = &quot;..&quot; # model = PPO.load(&quot;./models/model_step_1740000.zip&quot;, env=env) model = PPO(&quot;MlpPolicy&quot;, env, verbose=1, tensorboard_log=log_path, policy_kwargs=policy_kwargs, seed=42, n_steps=512, batch_size=32) model.set_logger(new_logger) model = model.learn(total_timesteps=1000000, callback=save_model_callback, progress_bar=True, )  该模型学习得很好，但似乎很快就“忘记”了它学到的东西。例如，请参见以下曲线，其中步骤 25k-50k 的高奖励区域是完美的，但随后奖励明显下降。你能看出其中的原因吗？ https://preview.redd.it/ve40nmtogkqd1.png?width=682&amp;format=png&amp;auto=webp&amp;s=ffd6d0b5f3cea8d89ad78e42798bbb2b759182d1   由    /u/luigi1603  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnldyh/ppo_learns_quite_well_but_then_reward_keeps/</guid>
      <pubDate>Mon, 23 Sep 2024 14:10:47 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>