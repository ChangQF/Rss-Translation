<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 02 Feb 2025 15:16:52 GMT</lastBuildDate>
    <item>
      <title>“迈向通用无模型强化学习”，Fujimoto 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</guid>
      <pubDate>Sun, 02 Feb 2025 12:02:03 GMT</pubDate>
    </item>
    <item>
      <title>我对学习 RL 的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</link>
      <description><![CDATA[我读过 Sutton 和 Barto 的书，有时我发现很难理解其中的一些概念。然后，我开始探索这个资源。现在，我真正理解了价值迭代和其他基本概念背后的含义。我认为这本书应该在 Sutton 和 Barto 的书之前或同时阅读。这真是一本很棒的书！    提交人    /u/demirbey05   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</guid>
      <pubDate>Sun, 02 Feb 2025 07:59:42 GMT</pubDate>
    </item>
    <item>
      <title>信息抽象扑克解决方案中阻塞效应的解释</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifp5fk/accounting_for_blocker_effects_in_an_information/</link>
      <description><![CDATA[我目前正在学习扑克解算器的机制，并尝试使用 MCCFR 实现一个。 到目前为止，我已经成功解决了 Leduc Hold&#39;em 扑克问题，并正在尝试转向 NL Hold&#39;em。由于我正在创建一个解算器而不是扑克机器人，因此默认情况下，我将把行动空间抽象为两个玩家的几个可能的典型行动（例如 33%、50 美元、75%、底池、150% 和全押）。 目前，我也不会尝试解决翻牌前的问题，而只会使用已经存在的范围图表，这些图表将到达翻牌的某个位置（或让用户手动创建它们）。翻牌前的动作将被抽象到这个范围后面。翻牌将根据解决方案给出，所以我需要考虑的只是每个玩家的底牌、转牌和河牌。  我的问题是，在这些限制下，是否有可能在合理的时间内使用 MCCFR 解决它，而无需任何信息/聚类抽象，或者我是否仍然需要使用信息空间抽象，例如将相似的信息集分成一个策略。如果我这样做，我不太确定如何根据它们独特的阻断效果在一个桶中为不同的手单独创建策略。例如，众所周知，在完成转牌或河牌的同花上拥有花色的 A 是虚张声势的好时机，但是无论如何，它会被放在与任何其他 A 高手相同的桶中，尤其是在河牌上。  我读到我可以通过根据当前手牌强度和未来街道移动到不同桶的概率选择桶来部分缓和翻牌和转牌上的这种情况，但这仍然不能真正解决桶何时想要采取混合策略的问题，以及决定哪些手应该做哪些动作。    提交者    /u/lddzz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifp5fk/accounting_for_blocker_effects_in_an_information/</guid>
      <pubDate>Sun, 02 Feb 2025 04:25:39 GMT</pubDate>
    </item>
    <item>
      <title>阅读 DeepSeek 研究论文让我学到了关于人类智能的知识</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iflz8p/what_reading_the_deepseek_research_paper_taught/</link>
      <description><![CDATA[deepseek R1 论文展示了 LLM 如何通过在客观领域（数学、编码等）中的反复试验来教会自己推理。在阅读它时，我不断看到与人类学习结构化思维的方式相似之处。 我考虑了这 5 点  客观领域（数学、编码、物理，甚至商业）是学习如何思考的健身房 数学和编码是独立的现实，通过它们你可以学习如何思考 + 了解更多关于生活的知识 模型提炼 =&gt; 为什么人类向专家学习比从原始的反复试验中学习效果更好 自学成才的人工智能有可能开启新的视角/思维方式，让我们能够看待世界 语言作为一种认知工具，以及为什么 chatgpt 一直用中文思考  我在我的博客中详细介绍了其中的每一个。 https://syedfarrukhsaif.com/blog/what-deepseek-taught-about-human-intelligence – 简短、易读。    提交人    /u/Fabulous-Extension76   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iflz8p/what_reading_the_deepseek_research_paper_taught/</guid>
      <pubDate>Sun, 02 Feb 2025 01:31:27 GMT</pubDate>
    </item>
    <item>
      <title>“DivPO：多样化偏好优化”，Lanchantin 等人 2025（通过设置最小新颖性阈值来对抗 RLHF 模式崩溃）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifl1bp/divpo_diverse_preference_optimization_lanchantin/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifl1bp/divpo_diverse_preference_optimization_lanchantin/</guid>
      <pubDate>Sun, 02 Feb 2025 00:43:05 GMT</pubDate>
    </item>
    <item>
      <title>对于非新手项目，从哪里开始使用 GPU？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifjcbn/where_to_start_with_gpus_for_notsonovice_projects/</link>
      <description><![CDATA[经验丰富的软件工程师，希望涉足一些硬件 - 我想探索一些 AI/模拟支线任务。我完全知道 GPU 和（如果是 NVIDIA，那么就是 CUDA）对于这次旅程必不可少。但是，我不知道从哪里开始。 我是一个典型的 Mac 用户，所以组装一台 PC 或将多个 GPU 联网在一起的想法不是我做过的事情（但我可以学会）。我真的不知道要搜索什么或从哪里开始寻找。 对于如何开始熟悉构建和编程用于自托管目的的 GPU 集群，有什么建议吗？我熟悉一般的网络和相关的分布式编程（需要 VPC、Proxmox、Kubernetes 等），但不熟悉 GPU 方面。 我完全清楚我不知道我还不知道什么，我在寻求方向感。每个人都是从某个地方开始的。 如果有帮助的话，我感兴趣的两个项目是在集群中运行一些本地 Llama 模型，以及为一些机器人项目（Isaac / gym / 等）运行一些大规模并行深度强化学习过程。 如果有 A) 更适合“开发套件之后的步骤”的实用选项，和 B) 可以让我更全面地进入硬件生态系统并真正“理解”发生了什么的选项，我不会在 Jetson 开发套件上花钱。 有什么建议可以帮助迷失的灵魂吗？硬件、课程、YouTube 频道、博客 - 任何能帮助我直观地了解超越 devkit 级别交互的东西。    提交人    /u/NewEnergy21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifjcbn/where_to_start_with_gpus_for_notsonovice_projects/</guid>
      <pubDate>Sat, 01 Feb 2025 23:20:48 GMT</pubDate>
    </item>
    <item>
      <title>有给 RL 新手的模拟器推荐吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifg2k5/simulator_recommendation_for_rl_newbie/</link>
      <description><![CDATA[  由   提交  /u/Neat_Comparison_2726   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifg2k5/simulator_recommendation_for_rl_newbie/</guid>
      <pubDate>Sat, 01 Feb 2025 20:52:20 GMT</pubDate>
    </item>
    <item>
      <title>最好的强化学习课程或书籍？结构化路径</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifa7er/best_reinforcement_learning_course_or_books/</link>
      <description><![CDATA[我刚刚完成 ml 和深度学习，我想进入 RL。所以有没有什么资源你可以推荐给我，请分享，请以有序的路径分享，这样我最容易遵循。请分享你对它们的见解和经验。    提交人    /u/Unlikely_Slip327   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifa7er/best_reinforcement_learning_course_or_books/</guid>
      <pubDate>Sat, 01 Feb 2025 16:37:41 GMT</pubDate>
    </item>
    <item>
      <title>RL 中有哪些类型的职业？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1if3x11/what_type_of_careers_are_available_in_rl/</link>
      <description><![CDATA[我一直认为，我不可能进入一个完整的机器学习职业（只是机会或经验不足，或者我不够聪明），但最近我被伯克利的 Sergey Levine 实验室录取为本科生。现在，我正在权衡如何利用我将在他的实验室获得的 3.5 年 RL 研究经验（我现在只是一名大一新生）。 一方面，我可以攻读博士学位；我真的不喜欢额外的 5 年和它需要的所有承诺（也包括看到我所有的朋友毕业并开始赚钱），但这可能是在 RAIL 进行研究后进入机器学习职业的最可靠方法。我也觉得这是从做这么多本科研究中获得最大价值的选择（虽然可能是沉没成本谬论哈哈）。但我担心，等我毕业时，人工智能的炒作会冷却下来，或者强化学习可能不是一个适合攻读博士学位的领域。（要清楚的是，我想从事行业研究，而不是学术界） 另一方面，我可以从事某种标准的机器学习工程师职位。我担心的是，我更喜欢研发类的工作，而不是工程类的工作。我也觉得，我的研究经验对于招聘这些工作毫无用处（一些随机的招聘人员真的会关心研究吗？），所以这有点浪费了。但我进入职场的时间要早​​得多，而且不必忍受攻读博士学位的痛苦。 我觉得我想要介于这两个选项之间的某个职位，但不确定这个职位到底是什么。 除了任何与上述问题相关的建议外，我还有两个主要问题：  工程和研发之间的工作范围到底是什么？我听说过一些工作，比如研究工程师，它们介于两者之间，但这些工作似乎相当不常见。此外，没有博士学位的情况下，获得 ML 研发工作有多普遍（假设你已经在本科阶段拥有丰富的研究经验）？ RL 行业总体情况如何？我看到对 CV 和 NLP 专家的需求很大，但除了在 LLM 中的使用之外，我从未听说过太多关于 RL 的信息。RL 专业化是行业真正需要的吗？  谢谢！ - 一名困惑的学生    提交人    /u/dmann1945   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1if3x11/what_type_of_careers_are_available_in_rl/</guid>
      <pubDate>Sat, 01 Feb 2025 10:55:17 GMT</pubDate>
    </item>
    <item>
      <title>“SFT 记忆，RL 泛化：基础模型后训练的比较研究”，Chu 等人 2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1if2u13/sft_memorizes_rl_generalizes_a_comparative_study/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1if2u13/sft_memorizes_rl_generalizes_a_comparative_study/</guid>
      <pubDate>Sat, 01 Feb 2025 09:32:27 GMT</pubDate>
    </item>
    <item>
      <title>DDQN 未能在基于像素的四个房间上进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1if0xth/ddqn_failed_to_train_on_pixel_based_four_rooms/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1if0xth/ddqn_failed_to_train_on_pixel_based_four_rooms/</guid>
      <pubDate>Sat, 01 Feb 2025 07:10:41 GMT</pubDate>
    </item>
    <item>
      <title>我的 RL 项目缺少什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iexhxm/what_am_i_missing_with_my_rl_project/</link>
      <description><![CDATA[      我正在训练一个代理，让它擅长玩我制作的游戏。它在小行星在 2D 空间中向下坠落的环境中操作一艘航天器。到达底部后，小行星会在顶部以随机速度以随机位置重生。（太随机了？） 普通 DQN 和 Double DQN 不起作用。 我切换到 DuelingDQN 并添加了重播缓冲区。 随着训练的继续，损失终于减少了，但学习到的策略仍然导致高度可变的性能，平均而言没有实际改善。 我的奖励结构有问题吗？ 目前对每一步幸存使用 +1，并对小行星碰撞使用 -50 惩罚。 非常感谢您提供的任何帮助。我是新手，已经挣扎了好几天了。    提交人    /u/GimmeTheCubes   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iexhxm/what_am_i_missing_with_my_rl_project/</guid>
      <pubDate>Sat, 01 Feb 2025 03:36:00 GMT</pubDate>
    </item>
    <item>
      <title>“赋权有助于创意视频游戏中的探索行为”，Brändle 等人 2023 年（无先验的人类探索效率低下）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iexccf/empowerment_contributes_to_exploration_behaviour/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iexccf/empowerment_contributes_to_exploration_behaviour/</guid>
      <pubDate>Sat, 01 Feb 2025 03:27:27 GMT</pubDate>
    </item>
    <item>
      <title>为什么 RL 比进化启发方法更受欢迎？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iex9km/why_is_rl_more_preferred_than_evolutioninspired/</link>
      <description><![CDATA[免责声明。我试着不带偏见。但趋势似乎是朝着深度强化学习发展。本文无意“争论”任何事情。我既不愿意也没有知识去主张什么。 进化算法实际上在 Sutton&amp;Barto 的著名书籍的开头就提到过，但我太笨了，无法理解上下文（我只是一个普通读者和业余爱好者）。 那里没有提到但我想到的另一个原因是并行化。我们都知道，机器学习热潮已经导致 GPU、TPU 和 NPU 制造商和设计师的股价飙升。我不太了解数学和技术细节，但我相信通过反向传播调整深度网络的能力归功于线性代数和 GPGPU，而进化算法不太可能从它们的帮助中受益。 再次重申，我远离 ML 知识，所以如果我错了，请告诉我。    提交人    /u/Gloomy-Status-9258   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iex9km/why_is_rl_more_preferred_than_evolutioninspired/</guid>
      <pubDate>Sat, 01 Feb 2025 03:23:04 GMT</pubDate>
    </item>
    <item>
      <title>近端策略优化算法（类似于用于训练 o1 的算法）与使用策略优化的一般强化学习 DeepSeek 背后的损失函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ieku4r/proximal_policy_optimization_algorithm_similar_to/</link>
      <description><![CDATA[        由    /u/AsideConsistent1056   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ieku4r/proximal_policy_optimization_algorithm_similar_to/</guid>
      <pubDate>Fri, 31 Jan 2025 17:57:44 GMT</pubDate>
    </item>
    </channel>
</rss>