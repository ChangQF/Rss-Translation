<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 05 Feb 2024 06:18:04 GMT</lastBuildDate>
    <item>
      <title>华为：在现实世界中通过不确定性做出值得信赖的决策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj520x/hua_wei_trustworthy_decision_making_in_the_real/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj520x/hua_wei_trustworthy_decision_making_in_the_real/</guid>
      <pubDate>Mon, 05 Feb 2024 01:57:24 GMT</pubDate>
    </item>
    <item>
      <title>离线 DQN 的加权重要性采样</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj3l8f/weighted_importance_sampling_for_offline_dqn/</link>
      <description><![CDATA[      我的问题是“在离线环境中比较不同强化学习策略的最佳方式是什么？” 我自己做了一些研究，发现了加权重要性抽样是平衡偏差和方差的好方法，特别是当行为策略不是最优的（或接近我们想要评估的策略）时。然而，如果我们训练一个获取高维特征的 DQN 模型，并输出推荐操作的概率（假设通过 softmax 层），那么我在实现上会遇到问题。考虑到我们无法访问状态空间（我们具有以下特征），我们应该在 WIS 公式中将 π(at|st) （来自 RL 模型的最优策略）或 μ(at|st) （行为策略）设置为什么可以产生无限多个状态）？ ​ WIS 公式   由   提交/u/anagreement  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj3l8f/weighted_importance_sampling_for_offline_dqn/</guid>
      <pubDate>Mon, 05 Feb 2024 00:45:51 GMT</pubDate>
    </item>
    <item>
      <title>q 函数近似不收敛的 Actor Critic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/</link>
      <description><![CDATA[最近我一直在尝试实现论文。 但是，当使用车杆 v1 环境时，代理会学习一些行为，但随后就会崩溃。任何有关不正确实现或替代批评者功能的想法将不胜感激。 我也一直在使用超参数，但没有任何组合对我来说效果很好。 代码   由   提交/u/Tight_Apple_678   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aj2zey/actor_critic_with_qfunction_approximation_not/</guid>
      <pubDate>Mon, 05 Feb 2024 00:17:55 GMT</pubDate>
    </item>
    <item>
      <title>“从强化学习到代理：理解基础认知的框架”，Seifert 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ais80w/from_reinforcement_learning_to_agency_frameworks/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ais80w/from_reinforcement_learning_to_agency_frameworks/</guid>
      <pubDate>Sun, 04 Feb 2024 16:46:34 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶|斯瓦亚特机器人 |马恒达塔尔 |印度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aij99o/autonomous_driving_swaayatt_robots_mahindra_thar/</link>
      <description><![CDATA[       由   提交/u/shani_786  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aij99o/autonomous_driving_swaayatt_robots_mahindra_thar/</guid>
      <pubDate>Sun, 04 Feb 2024 08:31:55 GMT</pubDate>
    </item>
    <item>
      <title>如何同时训练代理和输入编码？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aibim9/how_to_train_agent_and_input_encoding/</link>
      <description><![CDATA[大家好。我正在研究一个 DRL 项目，其中状态是一个长度不同的序列。因为我使用的是 Gymnasium 和 Stable-Baselines3，需要固定的预定义观察空间形状，所以我正在考虑使用 RNN 将输入序列编码为 Gym 环境中的预定义形状，然后让 SB3 处理训练. 所以我的问题是，有没有办法同时训练 RNN 和 RL 代理？我没有太多经验，但我的理解是我需要连接 RNN 和策略/价值网络以使反向传播工作，但我不确定在使用 Gym 和 SB3 时如何实现这一点，因为 RNN是在 Gym 环境中创建的，而策略/价值网络是在 SB3 模型中的。   由   提交/u/McCree76   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aibim9/how_to_train_agent_and_input_encoding/</guid>
      <pubDate>Sun, 04 Feb 2024 01:06:50 GMT</pubDate>
    </item>
    <item>
      <title>DQN 不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</link>
      <description><![CDATA[嗨，我正在尝试做一个蛇游戏 DQN，但在前 1k 次迭代中没有看到太多结果（如果有的话）。该模型似乎正在回归。 我想知道我的更新循环是否正确 信息：吃食物的奖励+1，碰撞-1，其他奖励 - 欧氏距离/来自食物的 l2 范数 我确实有一个重播缓冲区 def train_v2(self, state_tensor, action,reward, new_state_tensor, did): # state -&gt; (3,32,24) # 模型 -&gt; conv net action_tensor = torch.tensor(action, dtype=torch.float)reward_tensor = torch.tensor(reward, dtype=torch.float) if len(state_tensor.shape) == 3: # 转换为批处理形式 if single example state_tensor = torch.unsqueeze（state_tensor，dim = 0）action_tensor = torch.unsqueeze（action_tensor，dim = 0）reward_tensor = torch.unsqueeze（reward_tensor，dim = 0）new_state_tensor = torch.unsqueeze（new_state_tensor，dim = 0）完成=（ done,) for i in range(len(done)): # for idx in batch q_pred = self.model.forward(state_tensor[i]) if did[i]: q_next = torch.zeros(1) # 没有下一个状态否则： q_next = self.model_target.forward(new_state_tensor[i]).max(dim=1)[0] q_target =reward_tensor[i] + self.gamma*q_next self.optimizer.zero_grad() loss = self.loss_function( q_target, q_pred) loss.backward() self.optimizer.step()    由   提交/u/throtaway85633  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</guid>
      <pubDate>Sat, 03 Feb 2024 05:39:48 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo tic-tac-toe 游戏中何时调用 reset() 函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</link>
      <description><![CDATA[我正在将 pettingzoo 环境用于 MARL 程序，并使用 tictactoe 环境（在此处找到）作为蓝图。现有环境似乎在每个单独的时期内多次调用重置函数，这对于我自己的目的来说是不可取的。在尝试查找重置调用的来源时，我将其追溯到“base.py”文件。文件位于 pettingzoo /utils/wrappers 目录中。我仍然无法准确确定何时调用重置。我想让它仅在每个纪元结束时调用重置，因为我积累了不想重置的值。 我复制了 tictactoe 测试代码来运行它。我在重置函数中放置了一个打印调用，以查看每个时期内调用重置的次数。我确认在井字棋游戏的每个时期都会多次调用重置。这样做的目的是什么？在我看来，你会想在每场比赛结束时调用重置。为什么要多次重置，如何更改重置被调用的次数？   由   提交/u/NobodySmart1617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</guid>
      <pubDate>Fri, 02 Feb 2024 22:36:23 GMT</pubDate>
    </item>
    <item>
      <title>Nuro 实现大规模强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahehz4/nuro_enabling_reinforcement_learning_at_scale/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/recklessdesuka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahehz4/nuro_enabling_reinforcement_learning_at_scale/</guid>
      <pubDate>Fri, 02 Feb 2024 21:16:17 GMT</pubDate>
    </item>
    <item>
      <title>DQN 探索策略的收敛速度比贪婪策略快得多</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ah70er/dqn_exploration_policy_converges_much_faster_than/</link>
      <description><![CDATA[      我在解释以下结果时遇到一些困难。橙色线是奖励训练曲线，蓝色线是评估曲线。 https://preview.redd.it/ashlhzdl07gc1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=0a409ee350a6b3d80c5784b62079f 37f8dfdb9f8 期间训练我使用 epsilon 贪婪策略，epsilon = 0.2。在评估过程中，我使用贪婪的 argmax 策略。这些结果表明，在我的环境中，贪婪策略需要大约 20 万步才能达到最优。然而，epsilon 贪婪策略使用与贪婪策略相同的模型，但以 20% 的概率采取随机行动，只需 50k 步就已经是最优的。 观察到这一点时，您的第一个想法是什么？    由   提交 /u/fedetask   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ah70er/dqn_exploration_policy_converges_much_faster_than/</guid>
      <pubDate>Fri, 02 Feb 2024 15:59:55 GMT</pubDate>
    </item>
    <item>
      <title>PPO算法动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1agx883/ppo_algorithm_actions/</link>
      <description><![CDATA[我知道 PPO 输出操作的平均值和标准差，但是我如何才能将我的操作限制在应用程序的安全范围内。或者还有其他我可以选择的算法而不是 PPO。   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1agx883/ppo_algorithm_actions/</guid>
      <pubDate>Fri, 02 Feb 2024 06:27:16 GMT</pubDate>
    </item>
    <item>
      <title>了解行为政策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1agdtah/understanding_behavior_policy/</link>
      <description><![CDATA[我目前正在尝试了解 on-policy 和 off-policy 之间的差异。 到目前为止，我了解到： - 行为策略：代理用于选择操作的策略 - 目标策略：代理优化的策略 - On-Policy：行为策略 = 目标策略 - Off-Policy：行为策略 ≠ 目标策略 我最大的困惑是了解行为策略在同策略方法中的作用。在on-policy中，比如SARSA，如果智能体从Q表中选择行动，他们不是总是处于剥削状态并且从不探索吗？ 如果情况不是这样，那么什么是在策略 epsilon-greedy 算法与非策略 epsilon-greedy 算法之间的区别？ 我读了两篇不同的文章： 1. https://builtin.com/machine-learning/sarsa 这篇文章说，使用 epsilon-greedy 动作选择是 on-policy，因为当我们利用时，我们会从目标策略中选择一个动作  https://www.baeldung.com/cs/epsilon-贪婪-q-学习这篇文章说，使用epsilon-greedy动作选择是离策略的，因为当我们探索时，我们随机选择一个动作  事实是，这两者文章定义了相同的动作选择函数。那么是哪一个呢？在策略还是离策略？   由   提交/u/bean_217  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1agdtah/understanding_behavior_policy/</guid>
      <pubDate>Thu, 01 Feb 2024 15:38:36 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习数据中心</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ag5t8q/offline_rl_data_hub/</link>
      <description><![CDATA[查看 torchrl 的数据中心：https ://pytorch.org/rl/reference/data.html#datasets 它是最大的单一格式离线强化学习数据库。所有数据集都是可互换和/或可组合的。 目前，它包括 AtariDQN、D4RL、VD4RL、Roboset、所有 OpenX Examples、Minari 和 GenDGRL。 它基于 torchrl 的重放缓冲区实现，以便您可以像使用重播缓冲区一样使用它们（即，它们是完全可组合的并接受转换）。它的速度很快，采样速度真的非常快！   由   提交/u/AdCool8270  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ag5t8q/offline_rl_data_hub/</guid>
      <pubDate>Thu, 01 Feb 2024 07:54:28 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：为什么 env.reset(seed=seed) 有助于学习具有相同起始位置的确定性 env (frozenlake)？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afp5sd/need_help_why_does_envresetseedseed_facilitate/</link>
      <description><![CDATA[环境：FrozenLake-v1、4x4、slippery=false。无论种子如何，起始 obs（位置）始终为 0，并且地图（障碍物）不应改变。 我一直在每个训练集开始时调用 env.reset(seed=seed)。使用不同的随机种子，我的算法（A2C）能够解决这个问题。然而，当我删除重置中的种子（仅重置，而不是火炬或其他任何东西）时，策略会收敛到次优非解决方案。为什么？ 还有什么是 env 种子控制的随机因素？我什至尝试将我的主种子设置为 X，将重置种子设置为 Y。 健身房的冰雪湖教程也在每集之前设置重置种子，即使滑溜也关闭了。请参阅此处：https: //gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/#:~:text=state%20%3D%20env.reset(seed%3Dparams.seed)%5B0%5D%5B0%5D)&lt; /p&gt; 有什么想法吗？谢谢！ 编辑：弄清楚了。我的代码中有一个错误，其中 is_slippery 实际上设置为 true。因此，通过将种子设置为重置，每个情节的转换状态都是相同的（即每次在位置 x 执行操作 1 都有相同的效果），因此我的模型本质上是记住种子设置的某个概率转换。坏消息：我的设置显然无法在不确定的湿滑环境中工作。我也无法让它与 sb3 的 A2C 一起工作。   由   提交 /u/rl_ninja_rl_ninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afp5sd/need_help_why_does_envresetseedseed_facilitate/</guid>
      <pubDate>Wed, 31 Jan 2024 18:44:03 GMT</pubDate>
    </item>
    <item>
      <title>Flappy Bird 1.6 小时 2100 管，你觉得学习速度如何？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afmw1h/flappy_bird_2100_pipes_in_16_hours_how_do_you/</link>
      <description><![CDATA[https://reddit.com /link/1afmw1h/video/vsymq3l66tfc1/player ​ 我们在 ~1.6 中使用 Unity 中的 DQN 算法（这不是 mlAgents）训练了 FlappyBird小时。 由于一切都是从头开始编写的（以及神经网络），因此可以更改许多参数。划分环境也有助于加快这一过程。 100个特工同时训练，数量逐渐减少。 ​ 我想拍一个视频或者详细写一下，所以在我想知道你之前意见：与其他方法或现有插件相比，它是快还是慢，其他人会感兴趣吗？   由   提交/u/Fazoway  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afmw1h/flappy_bird_2100_pipes_in_16_hours_how_do_you/</guid>
      <pubDate>Wed, 31 Jan 2024 17:12:31 GMT</pubDate>
    </item>
    </channel>
</rss>