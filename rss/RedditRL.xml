<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 17 Jan 2024 09:14:49 GMT</lastBuildDate>
    <item>
      <title>国际象棋代理的奖励思路</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198b38r/reward_idea_for_chess_agent/</link>
      <description><![CDATA[嗨。我是强化学习新手，试图在 Tensorflow 和 TensorFlow 中实现国际象棋代理/Cli 程序时学习主要概念。 C++。 我在数学方面没有很强的背景，我所做的所有学习都是通过在互联网上阅读（我确实在监督学习和张量流方面有一些经验）。&lt; /p&gt; 我计划让我的代理使用 DQN（或者可能是 DDQN 仍在尝试了解其工作原理） 现在，我最初计划使我的奖励函数成为一个简单的 1，如果代理获胜，平局则为 0，输了则为 -1。但我想到了跟踪每场比赛中所走棋子 Q 值的变化（即状态中 q 值最高的棋子）的变化，当游戏结束时，采用该函数和其他一些函数（线性函数） ，对数或其他）并使奖励为这两个函数的负积分差。 这个想法是，在国际象棋游戏中，如果你玩得好，你的位置应该会随着游戏的进行而变得更好. 这听起来不错还是我只是随意编造的东西？这种方法有名字吗？ 希望有一个好的解释。另外，如果您对 DQN 的架构或一般代理有任何建议，我很乐意听到！ 谢谢！  &amp;# 32；由   提交 /u/C0L0Rpunch   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198b38r/reward_idea_for_chess_agent/</guid>
      <pubDate>Tue, 16 Jan 2024 19:11:56 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议以加快稳定基线下的 PPO 模型训练3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</link>
      <description><![CDATA[嘿各位 Reddit 用户！ 我目前正在使用 Stable Baselines3 训练金融日交易模型，并且我&#39;我面临着训练速度的挑战。每天（每集）涉及大约 250 万个数据点，当采取随机操作时，我的模拟器可以在大约 60-70 秒内迭代它们。 训练我的 PPO 模型时会出现问题，因为它需要每集长达 40-45 分钟。我只在剧集结束时执行一次更新，没有有限的水平线截断。当模拟器可以在一分钟左右完成训练时，为什么要花这么长时间来训练一集？有什么提示或技巧可以加速这个训练过程吗？接受建议和见解！   由   提交 /u/Bunny_lad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</guid>
      <pubDate>Tue, 16 Jan 2024 14:00:29 GMT</pubDate>
    </item>
    <item>
      <title>如何学习犯罪学？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1980su2/how_to_study_criminology/</link>
      <description><![CDATA[如何学习犯罪学？ 嗨，我的朋友想在美国学习犯罪学。我们不知道有什么要求、考试以及哪些大学有这样的教师。她攻读设计学士学位，想要攻读本科犯罪学。请帮助我们，她该如何开始？ （她不在美国）   由   提交 /u/DevilSummoned   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1980su2/how_to_study_criminology/</guid>
      <pubDate>Tue, 16 Jan 2024 11:30:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络可以处理高于 1 的奖励吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19809re/can_a_neural_network_handle_rewards_above_1/</link>
      <description><![CDATA[我知道在 -1 和 1 之间传递值可以提高稳定性，但我想知道模型是否可以容忍传递更高的值？不幸的是我现在没有环境可以测试它   由   提交 /u/sogha   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19809re/can_a_neural_network_handle_rewards_above_1/</guid>
      <pubDate>Tue, 16 Jan 2024 10:58:32 GMT</pubDate>
    </item>
    <item>
      <title>PPO 特工与随机玩家进行神奇宝贝对决。你知道为什么平均奖励如此不稳定吗？ lr=1e-3，8 个并行环境在训练前进行 60 次移动，num_epochs=3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</link>
      <description><![CDATA[       由   提交 /u/moisturemeister   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</guid>
      <pubDate>Tue, 16 Jan 2024 09:57:47 GMT</pubDate>
    </item>
    <item>
      <title>TicTacToe 的表格 Q-Learning - 仅最后一个状态/动作对存储在 Q-Table 字典中，其值不为 0</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yxc1/tabular_qlearning_for_tictactoe_only_the_last/</link>
      <description><![CDATA[我在 tictactoe 3x3 板的表格 q-learning 实现中遇到问题。 ​ 问题在于，只有最后一步（获胜、失败、平局）及其各自的棋盘状态存储在 q 值不是“0.0”的 q 表中。导致最后移动的所有其他状态和动作对仍然具有值“0.0”。我在下面添加了 q 表，其中显示最后一步的值为“0.2”。但之前所有的移动的值为“0.0”。这只是第一集。即使增加了剧集也不会改变任何事情。只有最后一个动作的 q 值不是“0.0” ​ 非常感谢任何帮助。我花了几天时间尝试修复它... :( class Mark(enum.StrEnum): CROSS = &quot;X&quot; NAUGHT = &quot;O&quot;; EMPTY = &quot;; _&quot; class Reward(enum.IntEnum): WIN = 1 LosE = -1 TIE = -0.065 NON_TERMINAL = -0.01 # Q-Learning 常量 EPSILON = 0.1 # 探索因子 ALPHA = 0.2 # 学习率 GAMMA = 0.95 # 折扣因子 TOTAL_EPISODES = 1 # 代理将玩的游戏总数 BOARD = np.array([Mark.EMPTY] * BOARD_SIZE)  ​  def update_q_table(board,action,reward,new_board): board_key = &quot;&quot;.join(board) new_board_key = &quot;&quot;.join(new_board) old_value = Q_TABLE_DICT.get((board_key, action), 0) if game_over (new_board): # 如果是最终状态，则没有未来奖励可以考虑 next_max = 0 else: # 估计最优未来值 next_max = max( Q_TABLE_DICT.get((new_board_key, a), 0) for a in possible_moves( new_board) ) # 使用贝尔曼方程更新当前状态-动作对的 Q 值 q_value = old_value + ALPHA * (reward + GAMMA * next_max - old_value) Q_TABLE_DICT[(board_key, action)] = q_value &lt; /pre&gt; ​ def train_q_learning_agent(): for Episode in range(TOTAL_EPISODES): board = np.array([Mark.EMPTY] * BOARD_SIZE) # 重置board current_mark = Mark.CROSS while not game_over(board): # Q-learning 代理 (X) 采取行动 if current_mark == Mark.CROSS: action = Choose_action_q_learning(board, Training=True) new_board = make_move_to(board, action, current_mark)reward = get_reward(new_board, current_mark) print(new_board) update_q_table(board, action,reward, new_board) # 随机玩家 (O) 采取行动 else: action = get_random_move(board) new_board = make_move_to(board, action, current_mark) ) board = new_board current_mark = Mark.NAUGHT if current_mark == Mark.CROSS else Mark.CROSS  ​ def Choose_action_q_learning(board,训练=真）-&gt; int: 如果训练且 random.uniform(0, 1) &lt; EPSILON: # 探索：选择一个随机动作 return np.random.choice(possible_moves(board)) else: # 探索：根据当前 Q 表选择最佳动作 board_key = &quot;&quot;.join(board) q_values = {操作: Q_TABLE_DICT.get((board_key, action), 0) for action in possible_moves(board) } return max(q_values, key=q_values.get)  ​ 第一集的 Q-Table 字典为 json： ​ { &quot;(&#39;_________&#39;, 0)&quot;: 0.0，“(&#39;XO_______&#39;，2)”：0.0，“(&#39;XOX____O_&#39;，3)”：0.0，“(&#39;XOXX___OO&#39;，4)”：0.0，“(&#39;XOXXXO_OO&#39; , 6)”: 0.2 }  ​   由   提交/u/faux190  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yxc1/tabular_qlearning_for_tictactoe_only_the_last/</guid>
      <pubDate>Tue, 16 Jan 2024 09:29:46 GMT</pubDate>
    </item>
    <item>
      <title>具有狄利克雷作用分布的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</link>
      <description><![CDATA[嗨！我正在通过 PPO 培训政策。该模型输出的 logits 成为狄利克雷分布的参数。这些操作的总和应为 1，并且在 [0, 1]（单纯形）范围内。问题是，随着动作大小（维度）的增加，动作的对数概率也会增加。这反过来最终会放大 ppo 使用的替代损失中的 logp 比率。 我的单纯形操作空间是长度为 400 的一维向量。对数概率通常在 2200 - 3000 的范围内。 e^(logp_1 - logp_2) 的 logp 比率会有很大的变化，从而破坏 pytorch 的梯度计算。导致看起来有效但梯度包含 NaN 值的损失。 有人知道如何在保持理论基础健全的同时抵消这个问题吗？或者也许我在某个地方的推理中犯了错误？ 提前致谢！   由   提交 /u/JMvanWestendorp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</guid>
      <pubDate>Tue, 16 Jan 2024 09:16:42 GMT</pubDate>
    </item>
    <item>
      <title>调整法学硕士与让他们接地有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197uwu3/how_is_aligning_llms_different_from_grounding_them/</link>
      <description><![CDATA[是的，这就是问题所在 - 在具体的环境中，我想知道这些任务会有什么不同。我想会有不同的政策，但在高层次上谁能解释一下发生了什么？   由   提交/u/dumber_9734   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197uwu3/how_is_aligning_llms_different_from_grounding_them/</guid>
      <pubDate>Tue, 16 Jan 2024 05:18:35 GMT</pubDate>
    </item>
    <item>
      <title>SB3 的随机启动状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</link>
      <description><![CDATA[我正在使用 SB3 的 DDPG，但在学习时无法加载具有不同启动状态的文件。我每次都尝试在重置方法中更改它。我的猜测是训练黑鬼只发生在一个情节中，因为没有调用重置方法，所以没有变化。也用 PPO 尝试过。另外，我如何控制训练次数和时间步长？ 在网上搜索的绳索结束🙂 我的代码：代码 环境：自定义 Boid 植绒 框架：开放 AI Gym   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</guid>
      <pubDate>Mon, 15 Jan 2024 23:44:37 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习：综合调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.10256 摘要：  多代理应用程序的流行遍及我们的各种互连系统日常生活。尽管它们无处不在，但在共享环境中集成和开发智能决策代理对其有效实施提出了挑战。这项调查深入研究了多智能体系统 (MAS) 领域，特别强调阐明 MAS 框架内学习最优控制的复杂性，通常称为多智能体强化学习 (MARL)。本次调查的目的是提供对 MAS 各个方面的全面见解，揭示无数机会，同时强调多代理应用程序所面临的固有挑战。我们希望不仅有助于更深入地了解 MAS 景观，而且还为研究人员和从业者提供有价值的观点。通过这样做，我们的目标是在 MAS 的动态领域内促进知情探索并促进发展，认识到在解决 MARL 中出现的复杂性方面需要适应性策略和持续发展。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</guid>
      <pubDate>Mon, 15 Jan 2024 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您对强化学习的真实体验是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</link>
      <description><![CDATA[ 由   提交 /u/Smallpaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</guid>
      <pubDate>Mon, 15 Jan 2024 21:31:30 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助 - 检查输入时出错：期望 flatten_input 有 3 个维度，但得到形状为 (4, 1) 的数组</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197f70n/need_help_error_when_checking_input_expected/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197f70n/need_help_error_when_checking_input_expected/</guid>
      <pubDate>Mon, 15 Jan 2024 18:02:27 GMT</pubDate>
    </item>
    <item>
      <title>我的 AIRL 无法正常工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1978gsl/my_airl_is_not_working/</link>
      <description><![CDATA[专家轨迹的概率在增加，而策略生成的轨迹在减少，但策略无法从推断的奖励函数中学习。   由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1978gsl/my_airl_is_not_working/</guid>
      <pubDate>Mon, 15 Jan 2024 13:15:28 GMT</pubDate>
    </item>
    <item>
      <title>奖励稀疏，剧集长度长。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196rt1n/sparse_reward_with_long_episode_length/</link>
      <description><![CDATA[嗨！我正在尝试使用 PPO 算法找到一个好的策略来优化本地搜索启发式中的参数。挑战在于我只能在每集结束时评估策略的性能，其中提供 [0,1] 范围内的稀疏奖励。剧集长度固定为 1000 步。在这种情况下是否有机会学习成功的政策？到目前为止，即使采用非常简单的观察结构，我也没有取得任何积极的成果。也许我可以尝试一些技巧。预先感谢您的帮助！   由   提交 /u/OpportunityHot7289   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196rt1n/sparse_reward_with_long_episode_length/</guid>
      <pubDate>Sun, 14 Jan 2024 22:10:28 GMT</pubDate>
    </item>
    <item>
      <title>强化学习优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196idl8/reinforcement_learning_for_optimization/</link>
      <description><![CDATA[有没有人尝试使用 RL 来解决优化问题，例如旅行商问题或类似问题，我检查了几篇他们使用 DQN 的论文，但在实际实现后我还没有即使对于简单的问题，例如将盒子从迷宫的一端移到另一端，也没有得到任何实际的结果。我还担心基于 DQN 的解决方案能否在未见过的数据上表现良好。欢迎提出任何建议。   由   提交 /u/HSaurabh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196idl8/reinforcement_learning_for_optimization/</guid>
      <pubDate>Sun, 14 Jan 2024 15:29:51 GMT</pubDate>
    </item>
    </channel>
</rss>