<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 23 Aug 2024 03:17:36 GMT</lastBuildDate>
    <item>
      <title>彻底改变机器人行为：Transformer 如何通过动作分块提升模仿学习能力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyvevk/revolutionizing_robot_behavior_how_transformers/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyvevk/revolutionizing_robot_behavior_how_transformers/</guid>
      <pubDate>Thu, 22 Aug 2024 21:41:34 GMT</pubDate>
    </item>
    <item>
      <title>扑克解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyoa2i/poker_solution/</link>
      <description><![CDATA[我有一个类似于扑克的任务，即有全押、弃牌、过牌、跟注、加注等操作。但本质是比较牌的等级，等级较高的获胜。我决定实施其中一种 CFR 算法。但我不知道是哪一种。 所以我有几个问题：  实施的最佳算法是什么（Vanilla CFR、CFR+、MCCFR、Deep CFR 或其他（例如 PPO））。 处理加注选择的最佳方法是什么？我应该将其离散化（例如检查、跟注、弃牌、加注 0.5bb、1bb 等）还是有其他方法？ 是否有可以找到这些 CFR 算法的现成实现的来源？     提交人    /u/silenthnowakeup   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyoa2i/poker_solution/</guid>
      <pubDate>Thu, 22 Aug 2024 16:48:58 GMT</pubDate>
    </item>
    <item>
      <title>pybullet 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyhdme/pybullet_question/</link>
      <description><![CDATA[我正在做一个四足动物项目，我想获取关于它的脚是否接触地面的布尔值，有人知道怎么获取吗？    提交人    /u/youssef_naderr   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyhdme/pybullet_question/</guid>
      <pubDate>Thu, 22 Aug 2024 11:54:34 GMT</pubDate>
    </item>
    <item>
      <title>MARL 的框架/库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyd8rw/framework_library_for_marl/</link>
      <description><![CDATA[嗨， 我在为 MARL 寻找类似于 CleanRL/SB3 的东西。 有人能推荐一下吗？我看到了 BenchMARL，但添加自己的环境看起来有点奇怪。我还看到了 epymarl 和 mava，但不确定哪个最好。理想情况下，我更喜欢 torch 中的某些东西。 期待您的推荐！ 谢谢 !    提交人    /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyd8rw/framework_library_for_marl/</guid>
      <pubDate>Thu, 22 Aug 2024 07:25:55 GMT</pubDate>
    </item>
    <item>
      <title>神经进化 + 强化学习问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ey13vs/neuroevolution_rl_question/</link>
      <description><![CDATA[我最近完成了（大概）从头开始编写 PPO。我遇到的最大问题是，很难将其扩大规模并给它更多的时间进行训练，因为它最终会忘记一切，或者也可能会陷入局部最大值。我对神经进化有点兴趣，据我所知，神经进化的主要问题是它不能很好地扩展到更大的网络，并且需要大量的计算。  所以我的问题是，为什么没有很多神经进化 + RL 研究，或者没有普遍使用的实现？（如果有，请留下链接或名称）据我所知，独立训练一群 RL 代理并选择/交叉最好的代理应该可以解决神经进化和 RL 单独存在的缺点。    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ey13vs/neuroevolution_rl_question/</guid>
      <pubDate>Wed, 21 Aug 2024 21:09:26 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的条件动作问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exvagq/conditional_action_problem_in_reinforcement/</link>
      <description><![CDATA[      大家好， 希望你们都过得好。我需要你的帮助来为在连续两室环境中运行的机器人设计一个决策过程。场景如下：  机器人从房间 A 开始，然后移动到房间 B，并继续此顺序 - 首先是房间 A，然后是房间 B，依此类推。 在每个房间中，机器人有四个动作选项：静止不动、向前移动、左转或右转。 如果它静止不动，它会保留所有这些选项以用于下一步。 如果它向前移动，它可以在下一步中静止不动或移动到下一个房间（房间 B 或返回房间 A，具体取决于它所在的位置）。 如果它左转，它可以在下一步中静止不动或右转，然后在下一步中，它可以再次静止不动或移动到下一个房间。 同样，如果它右转，它可以静止不动或左转在下一步中，然后站着不动或移动到下一个房间。 它最多可以连续站立三步。之后，它必须选择另一个动作。  我正在尝试弄清楚如何最好地制定这些动作。有人能建议最合适的算法或方法来处理这些条件和顺序动作吗？我知道这个场景可能有点复杂，如果它令人困惑，我深表歉意。任何指导或建议都将不胜感激！ https://preview.redd.it/ll6daz5qv1kd1.png?width=757&amp;format=png&amp;auto=webp&amp;s=8985d702bd80e892a7d1519f58e6c7367f7237ea    提交人    /u/muttahirulislam   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exvagq/conditional_action_problem_in_reinforcement/</guid>
      <pubDate>Wed, 21 Aug 2024 17:15:41 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Gym Custom 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exu802/help_with_gym_custom_environment/</link>
      <description><![CDATA[      嘿！需要一些 Gymnasium 自定义环境方面的帮助。我有这种布局，其中我需要对象仅沿多边形移动而不会发生碰撞。此类对象的数量不受限制。我已在此处附上布局以供您参考。 我几乎已为该环境编写了 pygame 代码，该代码处理该过程的 gui 和实时坐标跟踪。 我现在如何将其描述为自定义健身房环境？我知道需要制作的各种方法，例如 init、step、reset 等，但我不确定在我的特定情况下这些方法的内容是什么。  如能得到任何帮助，我们将不胜感激！ https://preview.redd.it/z3nvb8ntn1kd1.png?width=3895&amp;format=png&amp;auto=webp&amp;s=6836e2f9a79623c5da364180934baf96bcc7709a    提交人    /u/Strange-Durian3382   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exu802/help_with_gym_custom_environment/</guid>
      <pubDate>Wed, 21 Aug 2024 16:33:43 GMT</pubDate>
    </item>
    <item>
      <title>解决位置控制延迟问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exq77k/solution_for_delay_for_positional_control/</link>
      <description><![CDATA[我目前正在使用 pybullet 教虚拟四足机器人行走。我正在使用位置控制，这意味着模型输出关节的所需位置，然后内置的 pid 控制器开始将关节的实际位置移动到所需位置。这会造成延迟，可能会损害学习过程，有没有针对这种延迟的实际解决方案？     提交人    /u/youssef_naderr   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exq77k/solution_for_delay_for_positional_control/</guid>
      <pubDate>Wed, 21 Aug 2024 13:52:33 GMT</pubDate>
    </item>
    <item>
      <title>多大的行动空间才算太大？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exo9dz/how_large_of_an_action_space_is_too_large/</link>
      <description><![CDATA[我是强化学习的新手，所以我不确定这是否是一个合理的担忧。目前，我正在从事一个关于 THz 波段通信的研究项目。我正在编写一个深度 Q 学习算法来选择传输数据的最佳频带。我有 1217 个频带可供选择。我正在使用 OpenAI 的 gymnasium 框架。因此，我的动作空间如下所示： self.action_space = space.MultiBinary(1217)  我为代理选择使用的通道分配 1，为它选择传输数据的通道分配 0。这个动作空间是否太大？我是否应该增加波段的大小以减少可供选择的波段数量？或者是否有另一种方法允许代理从大列表中选择多个项目？应该允许代理选择它想要选择的任意数量。    提交人    /u/Hailwel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exo9dz/how_large_of_an_action_space_is_too_large/</guid>
      <pubDate>Wed, 21 Aug 2024 12:25:04 GMT</pubDate>
    </item>
    <item>
      <title>开局 & (m,n,k)=(5,5,4)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exn49t/open_spiel_mnk554/</link>
      <description><![CDATA[我在游戏 m,n,k=5,5,4 中使用 open_spiel。我在为这个游戏创建 rl 机器人时遇到了问题。我尝试使用 alpha_zero 和 mcts 进行 100k 次模拟，并且总是第一个玩家获胜。对于 m、n、k 或井字游戏等游戏，最好的机器人是什么？    提交人    /u/Present_Formal2674   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exn49t/open_spiel_mnk554/</guid>
      <pubDate>Wed, 21 Aug 2024 11:26:15 GMT</pubDate>
    </item>
    <item>
      <title>他们经过一些训练后回来了</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ex4yli/theyre_back_after_some_training/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ex4yli/theyre_back_after_some_training/</guid>
      <pubDate>Tue, 20 Aug 2024 19:43:23 GMT</pubDate>
    </item>
    <item>
      <title>国家组成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ex3zp9/state_composition/</link>
      <description><![CDATA[大家好，我正在研究集体运动（群体）和强化学习。 每个学习者都是独立的，只知道其本地环境。定义状态时，简单的方法是为邻居的每种组合创建一个状态，这种方法会很快爆发。我试图通过创建子状态来简化状态，每个子状态都与单个邻居交互。 然后，为每个子状态选择的操作是几何向量。 最后，所有动作向量都添加到最终动作中。 我的问题是，是否有关于这种状态构建的文献？ 谢谢！    提交人    /u/Klutzy_Dream_4263   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ex3zp9/state_composition/</guid>
      <pubDate>Tue, 20 Aug 2024 19:04:26 GMT</pubDate>
    </item>
    <item>
      <title>帮助在我的计算机上设置 cuda</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ex2f6l/help_with_setting_up_cuda_on_my_machine/</link>
      <description><![CDATA[最近开始接触强化学习，解决了很多 openAI-GYM 环境。 在做这件事的时候，我在为我的模型训练时间而苦苦挣扎，所以我希望利用我笔记本电脑上的 nvidia-gpu。 但是在我的计算机上获取 cuda 工具包、cudnn 和 nvidia 驱动程序一直是一个大问题。 我在 Windows 笔记本电脑上双启动了 ubuntu 22.04 用于编程目的。 发送任何对你有用的指南或一般提示。    提交人    /u/Unlikely_Teacher_614   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ex2f6l/help_with_setting_up_cuda_on_my_machine/</guid>
      <pubDate>Tue, 20 Aug 2024 18:01:14 GMT</pubDate>
    </item>
    <item>
      <title>FrozenLake-v1 中的 MCTS 算法性能不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ex1szp/poor_performance_with_mcts_algorithm_in/</link>
      <description><![CDATA[大家好， 我目前正在 Gymnasium 的 FrozenLake-v1 环境中实施蒙特卡洛树搜索 (MCTS) 算法。虽然实施运行时没有错误，但我遇到了性能非常差的问题。成功率一直很低，并且代理很难学习有效的策略。 以下是我的设置的一些细节：  我使用树的上限置信区间 (UCT) 来平衡探索和利用。 状态动作值 (Q) 和访问计数 (N) 在每次模拟后都会初始化和更新。 我尝试调整探索权重，但并没有带来显着的改进。 环境是随机的，我怀疑这可能是导致问题的原因。  我正在寻找有关如何在此特定环境中提高 MCTS 性能的建议。是否有其他人在随机环境（如 FrozenLake）中遇到过类似的 MCTS 问题？任何关于如何调整算法的建议或任何处理此类环境的策略都将不胜感激。 提前致谢！    提交人    /u/EAG2705   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ex1szp/poor_performance_with_mcts_algorithm_in/</guid>
      <pubDate>Tue, 20 Aug 2024 17:36:38 GMT</pubDate>
    </item>
    <item>
      <title>PPO-Agent 缺少最后一点学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ewrzuy/ppoagent_missing_the_last_bit_of_learning/</link>
      <description><![CDATA[大家好， 首先感谢大家的所有帖子，尤其是这里的回答！你们已经帮了我很多。 我目前正在为我的硕士论文训练一个 PPO-Agent，他应该学习机器的调度操作。为此，代理连接到模拟并获得奖励。安排一项行动会让他获得奖励（平均处理时间乘以最大可能完成时间），不安排任何行动会让他获得负奖励（负平均处理时间乘以最大可能完成时间）。最后，代理会获得奖励，即最大可能完成时间与已实现完成时间乘以最大可能完成时间之间的差值。 随着平均完成时间的减少，代理肯定在学习如何避免调度中断，但在某个时候他会停止改进（距离最佳值约 10% 到 20%）。但是，我可以看到一些随机推出实现了更好的完成时间（有时甚至是最佳的）。现在我的问题是如何说服我的代理理解并学习那个相当小的休息。欢迎任何想法和建议！ 谢谢！    提交人    /u/Vfbs1997   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ewrzuy/ppoagent_missing_the_last_bit_of_learning/</guid>
      <pubDate>Tue, 20 Aug 2024 10:21:49 GMT</pubDate>
    </item>
    </channel>
</rss>