<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 07 Sep 2024 01:09:54 GMT</lastBuildDate>
    <item>
      <title>任何好的‘Rl Agent’设计资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faqpef/any_good_rl_agent_design_resources/</link>
      <description><![CDATA[我目前对编程 RL 代理没有太多经验。我完成了 hugging face Deep RL 课程，也参加了 David Sliver 的课程（真的很棒，顺便说一下，它是免费的），所以我对一些 RL 中常用的机器学习算法背后的一些概念有一定的了解，但我从未做过“项目”或任何事情 所以我开始实施不同的 RL 算法来与 Open AI Gym 环境进行交互。但我设计的实际课程总是到处都是。  例如，我经常在代理类中存储健身房环境的第二个表示，然后从内部函数训练代理，这对我来说似乎不正确。 我在编程方面也不是很擅长（就项目和课程设计而言）——我已经做了一年的软件工程师，所以我可以很好地编写代码，只是整个设计方面对我来说很难哈哈 在构建 RL 项目时，从结构到类设计，有没有什么好的资源/例子来介绍一些好的编程实践。    提交人    /u/Amazing_Track4881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faqpef/any_good_rl_agent_design_resources/</guid>
      <pubDate>Fri, 06 Sep 2024 21:35:21 GMT</pubDate>
    </item>
    <item>
      <title>强化学习奖励可以是当前状态和新状态的结合吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</link>
      <description><![CDATA[      我正在为我的 RL 代理构建奖励函数，并考虑采取行动后当前状态和新状态的组合。据我所知，基于 Sutton &amp;，这是可能的Barto，特别是公式 3.6，其中奖励是状态-动作对和结果状态的函数。我想确保我的方法符合 RL 理论。 有人可以确认这是否有效或分享见解吗？ https://preview.redd.it/7wndf4ny98nd1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=547a77aeecc75edcee9fbd41ec45dc95c772d3d1    提交人    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</guid>
      <pubDate>Fri, 06 Sep 2024 17:57:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习竞赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</link>
      <description><![CDATA[大家好，我通过强化学习开始训练，正在提升自己。我可以在网上参加和竞争这个领域的哪些比赛，我如何才能在这个领域有更好的发展？    提交人    /u/Weary-Ad-7225   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</guid>
      <pubDate>Fri, 06 Sep 2024 17:46:06 GMT</pubDate>
    </item>
    <item>
      <title>PPO 外汇交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</link>
      <description><![CDATA[      我正在使用 SB3 为外汇交易环境训练带有动作掩码的 PPO。该模型似乎学习了低点和高点枢轴点，但准确性非常可疑，而且它会反转动作，所以它在高点买入，在低点卖出！代码中哪里可能出错？  https://preview.redd.it/4dwdxlclj7nd1.png?width=986&amp;format=png&amp;auto=webp&amp;s=60ac1f2a3ce5cdd386d77d13c57c233a37be56a6    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</guid>
      <pubDate>Fri, 06 Sep 2024 15:31:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将客户数据集添加到 d4rl？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fahao9/how_to_add_customer_dataset_into_d4rl/</link>
      <description><![CDATA[谢谢。    提交人    /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fahao9/how_to_add_customer_dataset_into_d4rl/</guid>
      <pubDate>Fri, 06 Sep 2024 14:59:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 进行黑盒目标函数优化 - 需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</link>
      <description><![CDATA[大家好， 我是强化学习领域的新手，目前正在研究一个让我有点困惑的问题。我正在尝试使用强化学习来优化黑盒目标函数，但我不确定我是否走在正确的轨道上。我非常感谢这个 subreddit 中经验丰富的人提供的任何指导或见解！ 所以，事情是这样的：我有一个目标函数，它接受两个输入（x，y）并输出标量值 z。目标是找到这个函数的全局最大值。棘手的部分是我无法访问该函数的内部工作原理。我所能做的就是从一个随机位置 (x, y) 开始，然后采取小步骤来探索函数的格局。 目标函数可能看起来像这样： def objective_function(x, y): return -(x**2 - 10 * np.cos(2 * np.pi * x) + y**2 - 10 * np.cos(2 * np.pi * y) + 20)  这个想法是使用 Q 值估计和神经网络来学习最大化目标函数的最佳策略。 以下是我一直在尝试的粗略概述：  使用 PyTorch 设置 DQN 模型，该模型以当前状态 (x, y) 作为输入并输出每个动作的 Q 值。 创建重放缓冲区来存储转换（状态、动作、奖励、 在训练期间，使用 epsilon-greedy 探索根据当前状态选择一个动作。 采取行动并观察下一个状态和奖励。 将转换存储在重放缓冲区中。 如果重放缓冲区有足够的样本，则对一批转换进行采样并执行梯度下降以更新 DQN 模型。  训练后，从随机状态开始并根据学习到的 Q 值贪婪地选择动作，找到最佳解决方案。  现在，我有点不确定，需要一些帮助：  由于这个问题没有明确的终点或终端状态，所以我不确定如何调整 Q 值估计。我应该将 Q 值估计为固定步数的累积奖励，而不是直到达到终止状态？我可能离题太远了，所以如果我误解了什么，请纠正我！ 如果没有终止状态，由于起点随机且步数有限，Q 值每次都会不同。这是预料之中的，还是我以错误的方式处理这个问题？  我将非常感谢这个 subreddit 中出色的人提供的任何指导、建议或替代方法。如果您有类似的优化问题经验，或者对如何改进此场景的 DQN 方法有想法，请分享您的智慧！ 非常感谢您的帮助。我真的很高兴向大家学习，并希望在这个具有挑战性的问题上取得一些进展！    提交人    /u/Technical-Vehicle888   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</guid>
      <pubDate>Fri, 06 Sep 2024 12:50:41 GMT</pubDate>
    </item>
    <item>
      <title>元强化学习框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</link>
      <description><![CDATA[大家好， 我目前正在探索元 RL 的主题，但在寻找合适的框架/库时遇到了问题。  虽然我已经找到了一些关于元学习主题的一般内容（例如 learn2learn、更高版本），但 RL 方面往往是次要关注点。  您是否使用过某些框架？可以推荐一些吗？    提交人    /u/RandomAgentIml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</guid>
      <pubDate>Fri, 06 Sep 2024 11:32:18 GMT</pubDate>
    </item>
    <item>
      <title>大家好</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fabk03/hey_everyone/</link>
      <description><![CDATA[是否有一些有趣的模拟器允许您创建一个可自定义的环境，其中代理可以学习控制四轴飞行器并跟随物体，例如骑自行车的人或汽车？ 我正在尝试训练代理在各种环境中跟随给定物体并避开障碍物。  我做了一些研究并找到了一些模拟器，但它们要么不允许物体跟踪，要么没有正确记录或没有正确的 Python API。    提交人    /u/Skirlaxx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fabk03/hey_everyone/</guid>
      <pubDate>Fri, 06 Sep 2024 10:09:45 GMT</pubDate>
    </item>
    <item>
      <title>同时进行 RL 和 DL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa5lao/rl_and_dl_at_the_same_time/</link>
      <description><![CDATA[我可以同时学习强化学习和深度学习吗？    提交人    /u/exlp_   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa5lao/rl_and_dl_at_the_same_time/</guid>
      <pubDate>Fri, 06 Sep 2024 03:30:20 GMT</pubDate>
    </item>
    <item>
      <title>“深度贝叶斯老虎机对决：汤普森抽样的贝叶斯深度网络实证比较”，Riquelme 等人 2018 年 {G}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa2qfb/deep_bayesian_bandits_showdown_an_empirical/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa2qfb/deep_bayesian_bandits_showdown_an_empirical/</guid>
      <pubDate>Fri, 06 Sep 2024 01:03:03 GMT</pubDate>
    </item>
    <item>
      <title>“探索的长期价值：测量、发现和算法”，Su 等人 2023 {G}（推荐者）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa28ph/longterm_value_of_exploration_measurements/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa28ph/longterm_value_of_exploration_measurements/</guid>
      <pubDate>Fri, 06 Sep 2024 00:38:27 GMT</pubDate>
    </item>
    <item>
      <title>主要的 RL 实验室都有盈利吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9ykt6/are_any_of_the_major_rl_labs_profitable/</link>
      <description><![CDATA[      我很好奇为什么 DeepMind 放弃了他们的历史项目，并停止了 AlphaZero 作为国际象棋引擎的开发。但后来我看了他们的利润，一切都说得通了。 即使是最近几年，主要客户也是他们的所有者，因此这主要是创造性会计，他们可以随心所欲地支付内部项目的费用。那么，RL 对任何更大规模的人来说都是有利可图的吗？或者这只是纯粹的研究资金浪费？ 我相信你可以让 DeepMind 真正盈利，你只需要一个新的领导层，与人民更紧密地联系在一起。 https://preview.redd.it/how5yh9a92nd1.png?width=774&amp;format=png&amp;auto=webp&amp;s=d93fb31fa812bff823ca899e3d0166697c39f4e9   由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9ykt6/are_any_of_the_major_rl_labs_profitable/</guid>
      <pubDate>Thu, 05 Sep 2024 21:51:50 GMT</pubDate>
    </item>
    <item>
      <title>在多任务/迁移学习中使用 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9pweb/using_rl_in_multitasktransfer_learning/</link>
      <description><![CDATA[我感兴趣的是看看神经网络如何高效地编码魔方，同时还能执行多项不同的任务。如果有人有多任务或迁移学习的经验，我想知道强化学习是否是一项适合纳入网络编码器部分训练中的好任务。    提交人    /u/thebrilliot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9pweb/using_rl_in_multitasktransfer_learning/</guid>
      <pubDate>Thu, 05 Sep 2024 15:56:19 GMT</pubDate>
    </item>
    <item>
      <title>我不知道如何开始我的第一个 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</link>
      <description><![CDATA[我过去一直是一名 Unity 游戏开发者，我想尝试做一些强化学习 - 两个 AI 玩家之间的标签游戏。我一直在使用 anaconda 和 jupyter 笔记本进行我的第一个机器学习项目，到目前为止我很喜欢它。现在我想知道我该如何实现我的想法？什么环境？我应该使用 pygame 吗？有人有这方面的教程/课程吗？ 我不介意浏览无尽的库文档。    提交人    /u/JMB4200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</guid>
      <pubDate>Thu, 05 Sep 2024 11:00:04 GMT</pubDate>
    </item>
    <item>
      <title>“RTX 4060 足以使用 Isaac Sim 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</link>
      <description><![CDATA[      我正在使用 Isaac Gym 进行运动任务训练，但我想在安装了 RTX 4060 的家用电脑上进行训练。它足以完成这项任务吗？需求文档列出的最低要求是 GeForce RTX 3070，这比 4060 略好一些。如果有人有使用 4060 的经验，请告诉我它是否足够。 https://preview.redd.it/vx4oohnhsymd1.png?width=1120&amp;format=png&amp;auto=webp&amp;s=ad49a07ed80612f57ad6b871ccc6180f85ffe03c    由    /u/Vegetable_Pirate_263  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</guid>
      <pubDate>Thu, 05 Sep 2024 10:04:32 GMT</pubDate>
    </item>
    </channel>
</rss>