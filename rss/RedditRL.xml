<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 04 Dec 2023 12:25:50 GMT</lastBuildDate>
    <item>
      <title>解释 RLlib 中 PPO 的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18a930l/explaining_the_metrics_for_ppo_in_rllib/</link>
      <description><![CDATA[       您能否解释一下 Ray Rllib 图中的policy_loss、vf_loss 和total_loss 的预期趋势？目前，我的保单损失为负。它先下降然后增加，我认为这是由于高学习率造成的。 Total_loss 和 vf_loss 均为正且递减。  除了损失和平均奖励之外，我还应该关注哪些值来确定我的策略是否经过训练并表现良好？ ​  https://preview.redd.it/pcrssmq0m64c1.png？ width=354&amp;format=png&amp;auto=webp&amp;s=6f5b8150ad5fa7872168e6a2a2eef13647a4fdfd   由   提交 /u/Beautiful_Basis8441   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18a930l/explaining_the_metrics_for_ppo_in_rllib/</guid>
      <pubDate>Mon, 04 Dec 2023 01:34:53 GMT</pubDate>
    </item>
    <item>
      <title>帮我解决依赖问题！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependencies_issue/</link>
      <description><![CDATA[尝试在 google colab 上运行 DQN pong。  我使用了这行代码作为依赖项： !pip installgym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate ==3.1.* ​ 我收到此错误消息： 收集gym[box2d]==0.17.* 使用缓存的gym -0.17.3-py3-none-any.whl 已满足要求：/usr/local/lib/python3.10/dist-packages 中的 pyvirtualdisplay==0.2.* (0.2.5) 已满足要求：PyOpenGL==3.1 .* 在 /usr/local/lib/python3.10/dist-packages (3.1.7) 已满足要求： PyOpenGL-accelerate==3.1.* 在 /usr/local/lib/python3.10/dist-packages ( 3.1.7）已满足要求：/usr/local/lib/python3.10/dist-packages中的scipy（来自gym[box2d]==0.17.*）（1.11.4）已满足要求：numpy&gt;=1.10。 4 /usr/local/lib/python3.10/dist-packages (来自gym[box2d]==0.17.*) (1.23.5) 收集 pyglet&lt;=1.5.0,&gt;=1.4.0 (来自gym [box2d]==0.17.*) 使用缓存的 pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB) 收集 cloudpickle&lt;1.7.0,&gt;=1.2.0 (来自gym[box2d]= =0.17.*) 使用缓存的cloudpickle-1.6.0-py3-none-any.whl (23 kB) 收集box2d-py~=2.3.5 (来自gym[box2d]==0.17.*) 使用缓存的box2d-py -2.3.8.tar.gz (374 kB) 正在准备元数据 (setup.py) ... 已完成 要求已满足： /usr/local/lib/python3.10/dist-packages 中的 EasyProcess （来自 pyvirtualdisplay==0.2。 *）（1.1）已满足要求：未来在 /usr/local/lib/python3.10/dist-packages 中（来自 pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym[box2d]==0.17 .*) (0.18.3) 为收集的包构建轮子：box2d-py 错误：subprocess-exited-with-error × python setup.py bdist_wheel 未成功运行。 │ 退出码：1 ╰─&gt;输出见上文。  注意：此错误源自子进程，并且可能不是 pip 的问题。为 box2d-py 构建轮子（setup.py）...错误错误：为 box2d-py 构建轮子失败为 box2d-py 运行 setup.py clean 无法构建 box2d-py 错误：无法为 box2d-py 构建轮子，这是安装基于 pyproject.toml 的项目所必需的 ​ 任何帮助将不胜感激！   由   提交 /u/Opening-Ocelot1748    reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependency_issue/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependencies_issue/</guid>
      <pubDate>Sun, 03 Dec 2023 23:34:08 GMT</pubDate>
    </item>
    <item>
      <title>LayerZer0 1214 美元</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189zzoi/layerzer0_1214/</link>
      <description><![CDATA[https://zer0-labs.network   由   提交/u/Horror-Scratch5718  /u/Horror-Scratch5718 reddit.com/r/reinforcementlearning/comments/189zzoi/layerzer0_1214/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189zzoi/layerzer0_1214/</guid>
      <pubDate>Sun, 03 Dec 2023 18:41:17 GMT</pubDate>
    </item>
    <item>
      <title>在规划中结合空间和时间抽象以实现更好的泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189r06i/combining_spatial_and_temporal_abstraction_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.00229 OpenReview：https:// /openreview.net/forum?id=eo9dHwtTFt 代码： https://github.com/mila-iqia/skipper 博客文章：http://mingde.world/combining-spatial-and-temporal-abstraction-in-planning/ 摘要:  受人类意识规划的启发，我们提出了Skipper，这是一种基于模型的强化学习代理，它利用空间和时间抽象来概括所学技能新颖的情况。它自动将手头的任务分解为更小规模、更易于管理的子任务，从而实现稀疏决策并将其计算集中在环境的相关部分。这依赖于表示为有向图的高级代理问题的定义，其中使用事后知识端到端地学习顶点和边。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法预计会有所帮助的地方。与现有最先进的分层规划方法相比，以泛化为中心的实验验证了 Skipper 在零样本泛化方面的显着优势。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189r06i/combining_spatial_and_temporal_abstraction_in/</guid>
      <pubDate>Sun, 03 Dec 2023 10:41:44 GMT</pubDate>
    </item>
    <item>
      <title>如何让SARSA模型正确预测？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189obpu/how_to_make_a_sarsa_model_predict_correctly/</link>
      <description><![CDATA[ 由   提交 /u/camelCase_Dev   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189obpu/how_to_make_a_sarsa_model_predict_correctly/</guid>
      <pubDate>Sun, 03 Dec 2023 07:25:19 GMT</pubDate>
    </item>
    <item>
      <title>寻求电池存储系统强化学习项目的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189llo9/seeking_help_with_reinforcement_learning_project/</link>
      <description><![CDATA[我目前正在开发一个项目，该项目涉及使用强化学习 (RL) 管理电池存储系统。尽管问题很简单，但我遇到了困难 - 似乎没有标准的 RL 方法（PPO、A2C、DQN）可以有效地学习。 我已经分享了所有项目 文件，其中包括详细的问题描述（这是一个简单的问题）、CSV 格式的输入数据、环境文件和代理初始化文件。&lt; /p&gt; 我非常感谢您提供的任何见解、建议或指导！如果您熟悉 RL 方法并且可以抽出时间看一下，您的帮助将非常有价值。   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189llo9/seeking_help_with_reinforcement_learning_project/</guid>
      <pubDate>Sun, 03 Dec 2023 04:30:18 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家：π(A|S,θ) 是什么形状？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189drnd/actor_critic_what_is_the_shape_of_πasθ/</link>
      <description><![CDATA[π(A|S,θ) 在 softmax 情况下为 exp(θ⊤Ax(S))/ Σexp(θ⊤Ax(S) )). 形状应该是什么？ 给定动作数量 = k 和特征数量 = d ​   由   提交 /u/Fashism   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189drnd/actor_critic_what_is_the_shape_of_πasθ/</guid>
      <pubDate>Sat, 02 Dec 2023 21:35:56 GMT</pubDate>
    </item>
    <item>
      <title>具有快速且健忘记忆的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1896eu4/reinforcement_learning_with_fast_and_forgetful/</link>
      <description><![CDATA[   /u/smorad  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1896eu4/reinforcement_learning_with_fast_and_forgetful/</guid>
      <pubDate>Sat, 02 Dec 2023 15:45:19 GMT</pubDate>
    </item>
    <item>
      <title>简单的 Q 学习与跨智能体共享值表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18902nv/simple_qlearning_with_value_table_sharing_across/</link>
      <description><![CDATA[围绕测试轨道进行朴素 q 学习的简单示例，使用 C++ 中的 raylib 实现 - 输入：5 个光线投射每个传感器分为 3 个区域：危险近、中、远。 （提供3^5=243个状态） - 动作：以匀速v直行，以v/2向左转向，以v/2向右转向 结束时每个episode，都会计算所有智能体之间的q表平均值并将其设置回智能体，以便在每个episode之后共享知识。当 epsilon 降至零时，我们只看到 1 次均匀移动，而不是 30 次，因为所有智能体始终选择贪婪动作。 https://reddit.com/link/18902nv/video/hnje6htdou3c1/player   由   提交 /u/goksankobe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18902nv/simple_qlearning_with_value_table_sharing_across/</guid>
      <pubDate>Sat, 02 Dec 2023 09:26:38 GMT</pubDate>
    </item>
    <item>
      <title>我能知道绑定 k 是如何绑定的吗？来源：强化学习：理论与算法，作者：Alekh Agarwal、Nan Jiang、Sham M. Kakade 和 Wen Sun https://rltheorybook.github.io/rltheorybook_AJKS.pdf</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188xrhh/can_i_know_how_the_bound_k_was_bound_origin/</link>
      <description><![CDATA[       由   提交/u/Professional_Card176   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188xrhh/can_i_know_how_the_bound_k_was_bound_origin/</guid>
      <pubDate>Sat, 02 Dec 2023 06:41:09 GMT</pubDate>
    </item>
    <item>
      <title>多代理强化学习基线</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188ox6r/mutli_agent_reinforcement_learning_baselines/</link>
      <description><![CDATA[我正在寻找稳定基线 3 的等效版本，但顾名思义，它适用于 MARL，以便我可以在自定义 MARL 环境上对算法进行基准测试。提到的大多数方法都利用与不同库（例如 Tianshou、CleanRL 和 Stable Baselines 3 本身）共享参数。不同的 MARL 算法还有其他常用的标准化基线吗？   由   提交/u/blitzkreig3  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188ox6r/mutli_agent_reinforcement_learning_baselines/</guid>
      <pubDate>Fri, 01 Dec 2023 22:55:03 GMT</pubDate>
    </item>
    <item>
      <title>当元学习遇到在线和持续学习时：一项调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188o8s1/when_metalearning_meets_online_and_continual/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.05241 摘要： &lt; blockquote&gt; 在过去的十年中，深度神经网络在使用涉及广泛数据集的小批量随机梯度下降的训练方案方面取得了巨大的成功。在这一成就的基础上，探索神经网络在其他学习场景中应用的研究激增。元学习是一个引起广泛关注的著名框架。通常被描述为“学会学习”，元学习是一种数据驱动的方法来优化学习算法。其他感兴趣的分支是持续学习和在线学习，两者都涉及使用流数据增量更新模型。虽然这些框架最初是独立开发的，但最近的工作已经开始研究它们的组合，提出新颖的问题设置和学习算法。然而，由于复杂性增加且缺乏统一术语，即使对于经验丰富的研究人员来说，辨别学习框架之间的差异也可能具有挑战性。为了促进清晰的理解，本文提供了一项全面的调查，使用一致的术语和正式的描述来组织各种问题设置。通过概述这些学习范式，我们的工作旨在促进这一有前途的研究领域的进一步进步。  https://preview.redd.it/ag7xaviaer3c1.png?width=1249&amp;format=png&amp;auto=webp&amp;s=c4c0aa093ac 20ae7f6f567c9427f9a9e76e3a7ee&lt; /a&gt;   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188o8s1/when_metalearning_meets_online_and_continual/</guid>
      <pubDate>Fri, 01 Dec 2023 22:24:42 GMT</pubDate>
    </item>
    <item>
      <title>估计具有固定步数的环境的状态值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188jh9d/estimating_state_value_for_environments_with/</link>
      <description><![CDATA[我正在自定义环境中训练 RL 代理。我的环境在执行一定数量的操作后终止。我的直觉是，当涉及到通过价值网络估计状态价值时，剩余的步骤数应该是价值网络输入的一部分。 （目前我没有将此作为输入的一部分。）这是正确的吗？我很想听听您对此的想法，如果您能给我指出任何现有的实现或完成此操作的论文，那就太好了？谢谢！   由   提交 /u/morakorvai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188jh9d/estimating_state_value_for_environments_with/</guid>
      <pubDate>Fri, 01 Dec 2023 18:56:14 GMT</pubDate>
    </item>
    <item>
      <title>强化学习如何处理多个动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188iiew/how_does_rl_work_with_multiple_motions/</link>
      <description><![CDATA[例如，这是使用Unity创建的，还是它自己的gym？ https://www.youtube.com/watch?v=SsJ_AusntiU&amp;pp=ygUncmVpbmZvcmNlbWVudCBsZ WFybmluZyB0ZWFjaCBob3cgdG8gYm94&lt; /p&gt; 从视频中很难看出。  令人困惑的是它们是如何链接在一起的，以及机器学习代理是否可以实现这一点。我了解如何训练模型站立，但如何同时站立、起身、走动和装箱？   这一切都是在数十亿次重复中通过相同的观察和相同的奖励完成的吗？  或者是否有 1 个模型学会了如何站立，1 个模型学会了如何站起来。 1 个学习如何移动的简化模型。等等。   然后这些模型使用布娃娃物理原理相互绑定。本质上，正在学习的模型上的关节与了解每个单独运动的模型绑定在一起。  我问的原因是我发现 Unity ML 代理有点有限。就像动作太随机一样，我的平均奖励几乎没有增长。这些动作是否应该在学习过程中得到改进，或者它们只是在整个学习过程中随机进行？即使是像移动到目标这样简单的事情，我也会发现代理不断地抖动并随机地来回移动   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188iiew/how_does_rl_work_with_multiple_motions/</guid>
      <pubDate>Fri, 01 Dec 2023 18:14:51 GMT</pubDate>
    </item>
    <item>
      <title>Unity 之外允许导入模型等的任何物理引擎健身房。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188iazm/any_physics_engine_gyms_outside_of_unity_that/</link>
      <description><![CDATA[我发现用于统一的 ML Agent 有点过时且有局限性，并且比机器人更适合游戏开发，我想创建自己的健身房对于像 2D 游戏这样的简单东西，它非常简单，但我想做一些 3D 机器人模型并使用稳定的基线来驱动它们，为此我需要一个具有碰撞和物理以及模型/的 3D 引擎纹理导入、一些着色，当然还有渲染。最好是也可以渲染到 webgl 环境的东西。 我假设稳定基线可以与任何 3D 开源引擎一起使用。是否有一些非常基本的东西可以与 OpenAI Gym 一起使用来导入模型、添加刚体和碰撞参数等。或者 OpenAI Gym 是否已经具备所有这些功能？ ​ ​   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188iazm/any_physics_engine_gyms_outside_of_unity_that/</guid>
      <pubDate>Fri, 01 Dec 2023 18:05:56 GMT</pubDate>
    </item>
    </channel>
</rss>