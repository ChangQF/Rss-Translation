<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 04 Nov 2024 06:26:10 GMT</lastBuildDate>
    <item>
      <title>“机器人操作模仿学习中的数据缩放规律”，Lin 等人 2024 年（多样性 > n）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gj2qc3/data_scaling_laws_in_imitation_learning_for/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gj2qc3/data_scaling_laws_in_imitation_learning_for/</guid>
      <pubDate>Mon, 04 Nov 2024 01:18:42 GMT</pubDate>
    </item>
    <item>
      <title>什么样的状态对于 LSTM 层来说是有用的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gisqx6/what_kind_of_state_is_useful_for_lstm_layers/</link>
      <description><![CDATA[我正在使用 Unity 的 mlagents 解决具有离散 SAC 的网格迷宫环境。没有内存，它也能很好地解决。但是，如果我启用内存，性能会下降到最低水平。我怀疑我当前的环境表示不适合 LSTM 层 最初的状态是（对于 4 个方向中的每一个）：对象的类型（墙壁、出口、无）、房间被访问的次数（只有 0 次是墙壁）。然后我尝试将每个房间和代理本身的位置添加到状态中，但这使情况变得更糟。到目前为止，只保留对象的类型是最好的选择，性能下降速度较慢，但​​代理仍然没有学习    提交人    /u/Aydiagam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gisqx6/what_kind_of_state_is_useful_for_lstm_layers/</guid>
      <pubDate>Sun, 03 Nov 2024 17:52:33 GMT</pubDate>
    </item>
    <item>
      <title>寻找应用强化学习和机器人技术的研究实习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gipwq6/looking_for_research_internship_in_applied_rl/</link>
      <description><![CDATA[我是 Mila 的博士生，致力于不同机器人应用的强化学习（研究过挖掘机自动化、基于物理的角色动画和自动驾驶等应用）。我目前正在寻找 2025 年的暑期研究实习，我对任何专注于应用强化学习或具身人工智能的职位都非常感兴趣。 以下是我迄今为止的研究历程：  自动奖励建模：开发了从 Vortex Simulator 中挖掘机自动化的专家演示中得出奖励函数的方法。（在 NeurIPS RL for Real-life Applications 研讨会上发表。） 样本高效强化学习：通过基于 Transformer 的离散世界建模提高了 Atari 基准的样本效率。 （ICML 2024） 多任务 RL 的组合运动先验：我目前正在使用 Isaac Gym 研究具有组合运动先验的机器人运动多任务学习。 自动驾驶 RL：设计了一种在 CARLA 模拟器上进行自动驾驶的课程学习方法，无需复杂的奖励塑造。 （Inria 研究生）。  我还在探索将扩散模型与 RL 结合使用，以实现稳定、多样化的控制策略。 如果有人知道相关的职位空缺，或者对可能重视应用 RL 研究的地方有任何建议，我将不胜感激。 非常感谢您的任何线索或建议！ 我的简历和更多详细信息请访问我的 https://pranaval.github.io/。    提交人    /u/Personal_Click_6502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gipwq6/looking_for_research_internship_in_applied_rl/</guid>
      <pubDate>Sun, 03 Nov 2024 15:50:41 GMT</pubDate>
    </item>
    <item>
      <title>模仿学习的最新进展是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gilwqy/what_is_stateoftheart_in_imitation_learning/</link>
      <description><![CDATA[  由    /u/Better_Working5900  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gilwqy/what_is_stateoftheart_in_imitation_learning/</guid>
      <pubDate>Sun, 03 Nov 2024 12:38:40 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity AI PRO - 1 年计划优惠 - 75% 折扣</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1giims3/perplexity_ai_pro_1_year_plan_offer_75_off/</link>
      <description><![CDATA[      作为标题：我们提供一年计划的 Perplexity AI PRO 优惠券代码。  订购：https://cheapgpts.store  接受的付款：  PayPal。 （100％买家保护） Revolut。     提交人    /u/A2uniquenickname   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1giims3/perplexity_ai_pro_1_year_plan_offer_75_off/</guid>
      <pubDate>Sun, 03 Nov 2024 08:50:18 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为策略游戏 Polytopia 设计 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gicg22/need_help_engineering_rl_algorithm_for_strategy/</link>
      <description><![CDATA[      大家好，我是 RL 新手，需要一些帮助来为策略游戏 Polytopia 设计算法。 我正在尝试为基于图块的策略游戏 Polytopia 制作 RL 代理。使用 OpenAI Gym，我制作了该游戏的原始版本。观察空间由 121 个图块组成，每个图块都有数据：（地形、资源、改进、气候、边界、改进所有者、单位所有者、单位类型、单位健康、改进进度、已攻击、已移动）以及玩家的星数。下面是游戏的示例（这是全局视图，所以没有雾，但各个代理看不到雾外面） https://preview.redd.it/aqpsk4c0elyd1.png?width=706&amp;format=png&amp;auto=webp&amp;s=9a2973083cc8512467aa2c6dcfbda9181946cb97 目前，我已将行动过程分为三个步骤。首先，代理从 1 到 121 中挑选一个方块（121 个动作）。其次，代理挑选要在该方块上执行的动作类型（8 种动作），例如：移动/攻击、收获资源、训练单位等。第三步仅当动作涉及目标方块时才会发生，例如：移动/攻击，代理从 1 到 121 中挑选一个代表目标方块的方块。示例动作序列为：59、1、49；这将选择方块 59，选择移动/攻击单位动作类型，并选择目标方块 49，这将导致骑手攻击战士。这是我的图表的链接：https://docs.google.com/presentation/d/1DPhYymGDfQIfVKAYlzK8lBkkiPoGlqbxRJ5JycDQI_U/edit?usp=sharing 我应该使用什么算法？处理这种多阶段操作的最佳方法是什么？我应该输入哪些参数？我的神经网络应该是模块化的还是分层的？PyTorch 是这类事情的好选择吗？任何关于如何开始学习过程的建议或链接都​​将不胜感激！    提交人    /u/Kingofath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gicg22/need_help_engineering_rl_algorithm_for_strategy/</guid>
      <pubDate>Sun, 03 Nov 2024 01:59:38 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助模拟人体运动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gi7t14/need_help_in_simulation_of_human_motion/</link>
      <description><![CDATA[基本上，我已经使用 HumanML3D 数据集生成了人体运动，现在想要使用 IssacGym/Mujoco 使用 RL（PPO）使其具有物理感知能力，有人可以提供一些资源来帮助我吗？非常感谢所有帮助。    提交人    /u/Character-Aioli-4356   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gi7t14/need_help_in_simulation_of_human_motion/</guid>
      <pubDate>Sat, 02 Nov 2024 22:09:50 GMT</pubDate>
    </item>
    <item>
      <title>对强化学习感兴趣可以申请哪些行业、什么职位？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gi12cc/which_industries_and_what_positions_can_we_apply/</link>
      <description><![CDATA[您好，我是密歇根大学马里兰分校的研究生，刚刚进入强化学习领域，到目前为止，我很喜欢它，这让我很好奇哪些行业可以申请实习，以及如何在此领域发展我的职业生涯。但就目前而言，我对强化学习的实际应用比研究更感兴趣。 我希望在这方面得到一些指导。 谢谢    提交人    /u/Odd-Pangolin4370   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gi12cc/which_industries_and_what_positions_can_we_apply/</guid>
      <pubDate>Sat, 02 Nov 2024 17:03:00 GMT</pubDate>
    </item>
    <item>
      <title>呼叫所有 ML 开发人员！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ghh9yz/calling_all_ml_developers/</link>
      <description><![CDATA[我正在开展一个研究项目，这将有助于我的博士论文。 这是一项用户研究，其中 ML 开发人员回答调查以了解 ML 开发人员构建隐私保护模型的问题、挑战和需求。  如果您从事 ML 产品或服务工作，或者您是从事 ML 工作的团队的一员，请帮助我回答以下问卷：https://pitt.co1.qualtrics.com/jfe/form/SV_6myrE7Xf8W35Dv0。 用于分享研究： LinkedIn：https://www.linkedin.com/feed/update/urn:li:activity:7245786458442133505?utm_source=share&amp;utm_medium=member_desktop 请随时与其他开发人员分享此调查。 感谢您的时间和支持！    提交人    /u/MaryAD_24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ghh9yz/calling_all_ml_developers/</guid>
      <pubDate>Fri, 01 Nov 2024 22:07:22 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习尚未奏效。发布于 2018 年。六年后，您认为情况发生了多大变化，哪些保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ghf83z/deep_reinforcement_learning_doesnt_work_yet/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ghf83z/deep_reinforcement_learning_doesnt_work_yet/</guid>
      <pubDate>Fri, 01 Nov 2024 20:35:14 GMT</pubDate>
    </item>
    <item>
      <title>变压器ppo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gh4qcv/transformer_ppo/</link>
      <description><![CDATA[我知道 cleanrl 已经发布了精益版本。有没有人有经验，可以告诉 transformer ppo 是否能取得更好的结果？更强大？比 gru 好吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gh4qcv/transformer_ppo/</guid>
      <pubDate>Fri, 01 Nov 2024 12:57:32 GMT</pubDate>
    </item>
    <item>
      <title>“π~0~：用于通用机器人控制的视觉-语言-动作流模型”，Black 等人 2024 年{物理智能}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggv2o3/π0_a_visionlanguageaction_flow_model_for_general/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggv2o3/π0_a_visionlanguageaction_flow_model_for_general/</guid>
      <pubDate>Fri, 01 Nov 2024 02:03:51 GMT</pubDate>
    </item>
    <item>
      <title>自然语言强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggpkub/rl_with_natural_language/</link>
      <description><![CDATA[我一直在研究一些将语言纳入强化学习框架的新论文，比如微软雷德蒙德的这篇论文(https://arxiv.org/pdf/1511.04636)、Reader(https://aclanthology.org/2023.emnlp-main.1032/)、Ready to Fight Monsters，以及最近的Learning to Model the World with Language(https://arxiv.org/abs/2308.01399)，我想知道这里是否有人可以指点一下强化学习领域其他有趣的作品。此字段。    提交人    /u/potatodafish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggpkub/rl_with_natural_language/</guid>
      <pubDate>Thu, 31 Oct 2024 21:28:34 GMT</pubDate>
    </item>
    <item>
      <title>[项目] PyMAB：一个用于多臂老虎机的探索性 Python 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gglajl/project_pymab_an_exploratory_python_library_for/</link>
      <description><![CDATA[大家好！我很高兴与大家分享 PyMAB，这是我为多臂老虎机 (MAB) 算法开发的 Python 库。它被设计为研究人员和强化学习爱好者尝试比较多种 MAB 算法和配置的实验工具。 📦 安装 pip install pymab 或者访问我们的 github 页面：https://github.com/danielaLopes/pymab 🎯 主要特点 多种 MAB 算法：  贪婪和 ε-greedy 汤普森采样（高斯和伯努利） 上限置信区间 (UCB) 贝叶斯 UCB 上下文 Bandits  多种环境：  平稳 非平稳  渐进式 突变 随机交换手臂   内置可视化：  奖励曲线 遗憾分析 动作分布 策略比较  📊 快速示例 以下是如何使用 PyMAB 的简单示例： from pymab.policies import ThompsonSamplingPolicy from pymab.game import Game # 初始化 Thompson Sampling policy = ThompsonSamplingPolicy(n_bandits=5) # 创建并运行模拟游戏 = Game(n_episodes=1000, n_steps=1000, strategies=[policy], n_bandits=5) game.game_loop() # 可视化结果 game.plot_average_reward_by_step() 该 repo 包含多个 jupyter-notebooks 示例。  如果您有任何问题或建议，请告诉我！我正在积极监控这个帖子，并很高兴收到您的反馈。 这是一个正在进行的项目，我们一直在寻找建议和贡献。如果您有任何想法或想提供帮助，请联系我们！ 标签：#MultiArmedBandits #ReinforcementLearning    提交人    /u/danielalopes97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gglajl/project_pymab_an_exploratory_python_library_for/</guid>
      <pubDate>Thu, 31 Oct 2024 18:21:52 GMT</pubDate>
    </item>
    <item>
      <title>现已在 YouTube 上提供 - 观看 Emma Brunskill 主持的斯坦福 CS234 强化学习的所有讲座</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gghbth/now_available_on_youtube_stream_all_the_lectures/</link>
      <description><![CDATA[  由    /u/Stanford_Online  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gghbth/now_available_on_youtube_stream_all_the_lectures/</guid>
      <pubDate>Thu, 31 Oct 2024 15:31:42 GMT</pubDate>
    </item>
    </channel>
</rss>