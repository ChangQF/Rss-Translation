<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 07 Mar 2024 03:12:56 GMT</lastBuildDate>
    <item>
      <title>无法理解 RL 代码中的大多数数学公式！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/</link>
      <description><![CDATA[在我的研究中，我认为在论文中计算公式很重要，这样我就可以更清楚地了解作者想要传达的内容以及主要贡献他们如何改进。我认为是时候学习更多数学来对强化学习进行更深入的研究了。  我认为强化学习和优化控制非常相似。那么也许学习一些凸优化和函数分析或者变异分析会很好？ 困惑我需要什么来提高我的 RL 知识，帮助！   由   提交/u/Tight-Ad789  /u/Tight-Ad789  reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/</guid>
      <pubDate>Wed, 06 Mar 2024 17:31:49 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定的基线训练两个竞争网络/策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b846eo/training_two_competing_networkspolicies_using/</link>
      <description><![CDATA[我的环境中，2 个以上的竞争代理根据不同的奖励函数有完全不同的目标。此外，这些目标会根据环境的情况而变化。我想训练代表“个性”的不同网络/策略。 一种方法是训练一个通用代理，该代理适用于在观察空间中编码的任何目标和奖励函数配置，但我希望这会增加网络的复杂性。 我希望另一种方法是同时进行 2 个以上的网络/策略训练。步骤/重置返回 {&#39;agent_0&#39;: [...], &#39;agent_1&#39;: [...]} 包含每个代理的观察和奖励，是否可以将一个用于网络 1，另一个用于网络 2 ？ 我正在使用 Stable Baselines 3 进行训练（使用 supersuit 转换 PettingZoo 环境，以便 sb3 可以在其上进行训练），目前训练是通过 sb3 抽象出来的：  env = ss.pettingzoo_env_to_vec_env_v1(env) env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=“stable_baselines3”) model = SAC(MlpPolicy, env) model.learn(total_timesteps=steps) model.save(run_name )  我想到的方法有意义吗？如果是这样，我如何以这种方式同时训练 2 个以上网络？   由   提交/u/davidschep  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b846eo/training_two_competing_networkspolicies_using/</guid>
      <pubDate>Wed, 06 Mar 2024 16:34:46 GMT</pubDate>
    </item>
    <item>
      <title>RL 在流体动力学与控制中的应用以及仿真软件选择的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b83n0h/questions_on_applying_rl_on_hydrodynamics_control/</link>
      <description><![CDATA[我正在写我的论文，该论文基本上围绕水下水翼艇展开（示例）和纵向（也可能是横向）运动的控制。主要思想是将其视为高度/俯仰/横滚控制问题（状态是高于水面的高度、俯仰/横滚角度及其一阶导数），并实现 RL 作为制定控制算法的手段。 &lt; p&gt;没有真正的船来尝试算法，所以我们的想法是使用模拟环境并为其提供船、箔、支柱等参数/属性。我的问题基本上是：  使用基于模型的方法（我可以实现修剪点的相对简单的线性表示）而不是仅仅使用是否更好？暴力破解它吗？ 您是否知道任何可以与空气/流体动力学很好地配合的模拟环境（尽管我认为计算升力和阻力系数并将它们直接插入力会更精确）？我之前曾从事过 Unity3D 工作，我喜欢在游戏对象上轻松拖放脚本。我还听说过 Gazebo （特别是 VRX 或 UVSim 包）。  模拟环境可以无头运行吗？我认为运行飞行器的可视化表示会非常耗时。如果我可以关闭图形并只期望物理引擎运行刚体/海交互，那就更好了。  注释我感兴趣的模拟是稳定的迎头航向（无机动），并且仅当飞船处于箔伯恩状态时（无需模拟船体与水的相互作用）。  ​   由   提交 /u/John_Skoun   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b83n0h/questions_on_applying_rl_on_hydrodynamics_control/</guid>
      <pubDate>Wed, 06 Mar 2024 16:13:23 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器实际上提供开环控制吗？有些人也将其称为任务实例的操作序列。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7zdhd/is_the_decision_transformer_actually_providing/</link>
      <description><![CDATA[ 由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7zdhd/is_the_decision_transformer_actually_providing/</guid>
      <pubDate>Wed, 06 Mar 2024 13:13:29 GMT</pubDate>
    </item>
    <item>
      <title>机器人研究场景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7wj0u/scenario_for_robotics_research/</link>
      <description><![CDATA[大家好，我很想知道以下公司在人工智能、计算机视觉和机器人领域的交叉研究的当前趋势：  特斯拉 谷歌 微软 亚马逊 苹果 Meta&lt; /li&gt;  正在/曾经在这些公司工作的机器人科学家/机器人研究人员，您能否介绍一下这些公司的研究标准，尤其是机器人和人工智能方面的研究标准？ 我的简介：我是一名有志于研究人工智能和机器人技术的人。我从事机器学习、计算机视觉、深度学习方面的工作。在我的硕士学位论文中，我正在研究使用计算机视觉和深度强化学习的室内移动机器人导航。如果有人与我从事同一主题，请发表评论并发送聊天请求:)）） 和平 ✌   &amp; #32；由   提交 /u/Quirky_Assignment707   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7wj0u/scenario_for_robotics_research/</guid>
      <pubDate>Wed, 06 Mar 2024 10:31:50 GMT</pubDate>
    </item>
    <item>
      <title>分子生成：优化复杂的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7var0/molecular_generation_optimizing_a_complex/</link>
      <description><![CDATA[你好， 我编写了一个基于神经网络的模型，其目标是优化某些 &lt;输入分子的strong&gt;属性。该属性由以下函数提供的eval(molecule)函数给出例如rdkit。一个示例属性可以是分子量。 对于那些不知道的人，分子可以表示为标记图，其中节点是原子，边是键。继该领域最近的工作之后，我的分子优化方法是通过“附加”分子来完成的。从预先构建的词汇表（也称为 Motif）到输入分子的图表。 我的模型遵循以下步骤： 给定输入分子， 1. 从词汇 (GNN+MLP) 2. 从 Motif 中选择应与 Motif 形成连接的候选原子 (GNN+MLP) 3. 从 Motif 中选择应与 Motif 形成连接的候选原子Molecule (GNN+MLP) 4. 最后画出结合两组附件的 BondType (NONE, SINGLE, DOUBLE, TRIPLE) 我认为这个模型是代理的策略函数： a = pi_theta(s) s（状态）：输入分子 a（动作）：基序、基序附着原子和键类型的元组输入分子 theta：模型参数 动作空间是离散的并且非常大。 策略函数应该由代理迭代使用，直到出现END主题被选中，表明生成已结束。考虑多次迭代： s* = PI_theta(s) s* 是代理生成的优化分子，可最大化属性，给定输入分子。 仅评估最终结果 s\* 是有意义的。作用时，药剂可能会产生多个中间分子；如果根据这些评估属性分数，它甚至可能低于初始分子。 所以我的优化问题将类似于： ARGMAX_theta eval( PI_theta(s) )&lt; /p&gt; 其中 eval( ) 是一个函数，给定一个分子图（例如 s*），它可以告诉属性分数。 但是我不相信一些事情： - eval 不可微分 - PI_theta 强烈依赖于输入：给定 s，优化所需的迭代次数可以换。此外，GNN 将根据绘制的 Motif 在 s 的不同图表上工作 您有任何想法/建议/类似问题可以请指出我以处理该网络的优化吗？  谢谢   由   提交/u/rutay_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7var0/molecular_generation_optimizing_a_complex/</guid>
      <pubDate>Wed, 06 Mar 2024 09:08:40 GMT</pubDate>
    </item>
    <item>
      <title>罗纳德·威廉姆斯（REINFORCE，1992）上个月去世 (2024-02-16)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7pjar/ronald_williams_reinforce_1992_died_last_month/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7pjar/ronald_williams_reinforce_1992_died_last_month/</guid>
      <pubDate>Wed, 06 Mar 2024 03:30:58 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7o6uy/rl_algo_failing_to_learn/</link>
      <description><![CDATA[      ​ https://preview.redd.it/ohq2w15akmmc1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=de47e6683213af03218fa754ce18a65c700fc9c1  我正在尝试重新实现最近的一篇论文，其中奖励是基于偏好的。基础代理是 SAC，但代理似乎无法改进，在 150 到 60 集回报范围之间波动，有时在 350 左右达到峰值。回报波动背后有什么建议或原因吗？ &lt; !-- SC_ON --&gt;  由   提交/u/kengsleh  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7o6uy/rl_algo_failing_to_learn/</guid>
      <pubDate>Wed, 06 Mar 2024 02:28:08 GMT</pubDate>
    </item>
    <item>
      <title>小心小型网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7k3oh/careful_with_small_networks/</link>
      <description><![CDATA[ 由   提交 /u/FriendlyStandard5985   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7k3oh/careful_with_small_networks/</guid>
      <pubDate>Tue, 05 Mar 2024 23:24:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 PPO 中 new_prob_policy 和 old_prob_policy 之间的比率不是 1？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7dlav/why_in_ppo_the_ratio_between_new_prob_policy_and/</link>
      <description><![CDATA[理论上我不明白为什么 new_prob_policy 和 old_prob_policy 之间的比率不同于 1。当我玩游戏时，我使用 NN 来选择动作我和代理保存状态、操作和 old_prob_policy。然后，当我获得足够长的轨迹时，我计算 GAE，然后计算 new_prob_policy 和 old_prob_policy 之间的比率。要获取 new_prob_policy 我应该做什么？据我所知，到目前为止，我必须再次以相同的状态向神经网络提供信息，并收集我已经采取的相同操作的 new_prob_policy 。但这样它应该总是等于 1 因为永远不会改变。所以我的程序在某个地方是错误的，但我没有 Sundstrand 如何获得 new_prob_policy 的概率比不同于 1。请解释我错在哪里！  &amp; #32；由   提交 /u/Capittain-Nemo-9294   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7dlav/why_in_ppo_the_ratio_between_new_prob_policy_and/</guid>
      <pubDate>Tue, 05 Mar 2024 19:08:48 GMT</pubDate>
    </item>
    <item>
      <title>深度MCCFR项目分享</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b74kok/deep_mccfr_project_sharing/</link>
      <description><![CDATA[https://github.com/davpat108/CITADELS_self_play 经过几个月的空闲时间工作，我终于完成了项目的工作版本。您对此有何看法？ 我唯一无法破解的是使用神经网络不仅可以猜测节点值，还可以提供探索的基本策略。游戏似乎太大了，使得网络的输出过于复杂。有什么办法解决这个问题吗？   由   提交/u/YellowOk1956   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b74kok/deep_mccfr_project_sharing/</guid>
      <pubDate>Tue, 05 Mar 2024 12:57:23 GMT</pubDate>
    </item>
    <item>
      <title>具有多代理设置的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b72gek/rl_with_a_multiagent_setting/</link>
      <description><![CDATA[大家好， 我目前正在生产调度的框架内探索多智能体强化学习（MARL），其中每个代理对应一台机器，代理必须选择要处理的操作。由于处理时间的多样性，并非所有机器都可以在一个决策时间点进行调度决策。在许多现有的 MARL 研究中，采用了联合行动的概念，即所有智能体共同选择一个行动。在此设置中，当前正在使用的机器会将所选操作（操作）合并到其队列中。这引发了关于联合行动有效性的问题，特别是在职位插入事件等动态场景中。我正在考虑是否可以为那些在决策点仍处于占用状态的机器引入虚拟操作。在这种情况下，如果我采用集中式评论家网络，它将如何处理这些虚拟动作？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b72gek/rl_with_a_multiagent_setting/</guid>
      <pubDate>Tue, 05 Mar 2024 10:52:05 GMT</pubDate>
    </item>
    <item>
      <title>如果可观察状态不遵循马尔可夫链性质怎么办</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6xpig/what_if_observable_state_doesnt_follow_markov/</link>
      <description><![CDATA[因此，在我的 RL 问题中，我的代理从环境中观察到了 3 个可观察状态。 一个是当前状态，一个是当前状态是微分状态（即 Sn - Sn-1），最后一个状态是所有先前状态的总和。 因此最后一个状态不遵循马尔可夫链属性，因为它依赖于所有其他先前状态。  p&gt; 它将如何影响我的强化学习训练和训练策略？？？   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6xpig/what_if_observable_state_doesnt_follow_markov/</guid>
      <pubDate>Tue, 05 Mar 2024 05:38:30 GMT</pubDate>
    </item>
    <item>
      <title>关于机器人强化学习的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6ii26/question_regarding_reinforcement_learning_in/</link>
      <description><![CDATA[我是 FRC（第一届机器人竞赛）团队的一名高中生，正在研究在我们的机器人中使用强化学习。我在传统机器学习方面有一些经验，我们在 onshape 中有机器人的 CAD。我非常感谢有关机器人模拟等后续步骤的帮助... 编辑：顺便说一句，我们无法支付任何订阅费用。  &amp;# 32；由   提交/u/Cellini_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6ii26/question_regarding_reinforcement_learning_in/</guid>
      <pubDate>Mon, 04 Mar 2024 18:38:09 GMT</pubDate>
    </item>
    <item>
      <title>关于创建定制健身房环境的疑问</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6dh5x/doubts_regarding_creating_a_custom_gymnasium/</link>
      <description><![CDATA[ 为什么在开发自己的自定义健身房环境时需要创建包  我是阅读此处提供的文档 - https://gymnasium.farama.org/ Tutorials/gymnasium_basics/environment_creation/#make-your-own-custom-environment 它具有以下信息 - 最后一步是将我们的代码构建为Python 包。这涉及配置gym-examples/setup.py。如何执行此操作的最小示例如下： from setuptools import setup setup( name=&quot;gym_examples&quot;, version=&quot;0.0.1&quot;, install_requires=[&quot;gymnasium ==0.26.0&quot;, &quot;pygame==2.1.0&quot;], )  我想知道为什么需要创建这个包？令人惊讶的是，如果我不执行此步骤，我会收到错误 - `gymnasium.error.NamespaceNotFound：未找到命名空间gym_examples。您是否为gym_examples安装了正确的软件包？  注册环境意味着什么？我检查了代码，它似乎是一个初始化变量的函数。这就是它的全部作用吗？    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1b6dh5x/doubts_regarding_creating_a_custom_gymnasium/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6dh5x/doubts_regarding_creating_a_custom_gymnasium/</guid>
      <pubDate>Mon, 04 Mar 2024 15:20:18 GMT</pubDate>
    </item>
    </channel>
</rss>