<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 26 Jul 2024 18:20:02 GMT</lastBuildDate>
    <item>
      <title>为什么在使用动作之前要将其与 action_scale 相乘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</link>
      <description><![CDATA[我发现在很多 RL 例子中，动作都乘以了某些动作比例值。 这是为了从策略中调整动作的“影响力”吗？    提交人    /u/Open-Safety-1585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</guid>
      <pubDate>Fri, 26 Jul 2024 12:04:12 GMT</pubDate>
    </item>
    <item>
      <title>参加空气曲棍球挑战！构建并训练可以玩空气曲棍球的代理。击败您的竞争对手，赢取 3000 美元，并有机会在真正的机器人设置上试用您的代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</link>
      <description><![CDATA[        提交人    /u/elizabeth_duhh04   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</guid>
      <pubDate>Fri, 26 Jul 2024 11:26:31 GMT</pubDate>
    </item>
    <item>
      <title>如何实现风摩擦力作用于猎豹模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecjura/how_to_realize_wind_frictions_acting_on_the/</link>
      <description><![CDATA[嗨， 我是一名学生，我正在尝试使用 MuJoCo 在不同环境中训练基于模型的强化学习代理。主要目标是扩展基于模型的强化学习方法以处理非平稳环境，例如动态（例如改变质量、重力、增加风摩擦）和/或奖励（例如改变目标速度）随时间变化的环境。目前，我专注于 [Gymnasium](https://gymnasium.farama.org/environments/mujoco/) 中基于 MuJoCo 的环境 我正在寻求一些帮助来定义作用于 [Cheetah 模型](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/half\_cheetah\_v5.py) 的风摩擦。我目前的想法是添加额外的执行器，作用于模型移动方向上或相反方向的某些关节。 下面你只能看到我修改的 Cheetah 模型的一部分，即执行器部分。所有带齿轮参数设置的电机均来自原始 Cheetah 模型。此外，我定义了一个执行器连接到特定关节，其控制范围限制在 -50 到 50 之间。我将额外的执行器从代理的动作空间中排除，以便代理无法控制它们（请检查下面的函数），我在运行时明确设置它们的值。 我有两个问题：  我不确定是否需要为每个附加执行器设置齿轮参数？ 此外，根据文档，多个执行器作用于单个关节没有问题，但一切对我来说都很新，不知道我是否以正确的方式更改模式？  这是修改后的执行器部分： &lt;details&gt; ``` &lt;actuator&gt; &lt;motor gear=&quot;120&quot;关节=&quot;bthigh&quot; name=&quot;bthigh&quot;/&gt; &lt;马达齿轮=&quot;90&quot; 关节=&quot;bshin&quot; name=&quot;bshin&quot;/&gt; &lt;马达齿轮=&quot;60&quot; 关节=&quot;bfoot&quot; name=&quot;bfoot&quot;/&gt; &lt;马达齿轮=&quot;120&quot; 关节=&quot;fthigh&quot; name=&quot;fthigh&quot;/&gt; &lt;马达齿轮=&quot;60&quot; 关节=&quot;fshin&quot; name=&quot;fshin&quot;/&gt; &lt;马达齿轮=&quot;30&quot; 关节=&quot;ffoot&quot; name=&quot;ffoot&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;rootx&quot; name=&quot;frictionrootx&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bthigh&quot; name=&quot;frictionbthigh&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bshin&quot; name=&quot;frictionbshin&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bfoot&quot; name=&quot;frictionbfoot&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;fthigh&quot; name=&quot;frictionfthigh&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;fshin&quot; name=&quot;frictionfshin&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-30 30&quot; joint=&quot;ffoot&quot; name=&quot;frictionffoot&quot;/&gt; &lt;/actuator&gt; ``` def exclude_wind_friction_from_action_space(self): &quot;&quot;&quot;用来实现风摩擦的额外执行器不应该是代理动作空间的一部分！&quot;&quot;&quot; bounds = self.model.actuator_ctrlrange.copy().astype(np.float32)[:-7] low, high = bounds.T self.action_space = Box(low=low, high=high, dtype=np.float32) &lt;/details&gt; 欢迎任何反馈/建议:)    由    /u/CertainLoad1589  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecjura/how_to_realize_wind_frictions_acting_on_the/</guid>
      <pubDate>Fri, 26 Jul 2024 09:30:06 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 CORL 和 D4RL 数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecdsgd/how_to_setup_corl_and_d4rl_datasets/</link>
      <description><![CDATA[我正在尝试使用从 D4RL 下载的 maze2d-umaze-v1 数据集运行 sac_n.py。我在 sac_n.py 上使用 github 的 CORL 实现。  我是一名新手，正在尝试弄清楚如何使用下载的数据集（当前位于我的 ~/.d4rl/datasets 文件夹中）作为 sac_n.py 文件的输入。  我目前在 VScode 文件夹中将两个文件并排放在一起，但正在努力寻找有意义的突破，以便使用数据集下载文件作为 ORL 算法文件的输入。 提前感谢您的时间和考虑。    提交人    /u/Constant_Koala_7744   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecdsgd/how_to_setup_corl_and_d4rl_datasets/</guid>
      <pubDate>Fri, 26 Jul 2024 03:07:12 GMT</pubDate>
    </item>
    <item>
      <title>使用纯强化学习制作国际象棋引擎的可行性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ec76vw/feasibility_of_using_pure_rl_to_make_a_chess/</link>
      <description><![CDATA[今年夏天，我正在与当地一所大学合作进行强化学习研究，现在应该开始着手我的最终项目（2.5 周后完成）。我想知道仅使用强化学习来训练国际象棋引擎是否可行，无论是使用自对弈还是在某些随机游戏数据集上进行离线学习。我担心训练强化学习国际象棋引擎需要多少能力，而且我认为我无法使用与训练 AlphaZero、Leela Chess Zero 等相同类型的东西。同时，我也不会追求它们所展示的深度水平。训练如此大型的游戏是否可行？或者我应该尝试更简单的游戏（Connect 4、跳棋等）？    提交人    /u/dmann1945   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ec76vw/feasibility_of_using_pure_rl_to_make_a_chess/</guid>
      <pubDate>Thu, 25 Jul 2024 21:55:50 GMT</pubDate>
    </item>
    <item>
      <title>稳态误差补偿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ebrr0k/steady_state_error_compensation/</link>
      <description><![CDATA[您好， 我正在使用 RL 和 DDPG 来控制无人机。总体而言，训练效果非常好，准确度高，误差很小（RMSE 为 0.008）。但是，稳态误差很小，为 0.5%。任务是将无人机从 5 米降落到 0 米，这个稳态误差很明显。 最初，我尝试减少步进时间，但问题几乎相同。然后，​​我读了一篇名为“基于 RL 的控制的稳态误差补偿”的论文。在这篇论文中，他们提出了一种扩展方法，可将稳态误差降低 52%。他们使用的方法涉及将积分组件合并到奖励函数中，这有助于通过惩罚代理不随时间减少稳态误差来最小化稳态误差。 我尝试了这种方法，但没有看到很大的改进。  所以我的问题是：1.强化学习能否实现0稳态误差，还是不可能？2.如果可以，我该怎么做才能实现？ 请参阅下面的奖励函数： ```matlab函数[reward，integralError] = rewardFunction（z，zt，integralError）％参数scaling_factor = 300;％积分分量的缩放因子integral_limit = 0.2;％限制积分以避免结束 ％计算距离（误差）dist = abs（z - zt）;％更新误差的积分integralError = integrationError + dist;％限制积分以避免结束integralError = max（min（integralError，integral_limit），-integral_limit）;％使用更平滑的梯度减少距离的基本奖励reward = 5 - 0.5 * dist; % 非常接近目标的额外奖励 if dist &lt; 1 奖励 = 奖励 + 35; end if dist &lt; 0.4 奖励 = 奖励 + 45; end if dist &lt; 0.01 奖励 = 奖励 + 55; end % 将积分部分合并到奖励中 奖励 = 奖励 - scaling_factor * integrationError;  end ``` 提前致谢！    提交人    /u/OkFig243   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ebrr0k/steady_state_error_compensation/</guid>
      <pubDate>Thu, 25 Jul 2024 10:43:34 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能在不同的状态下训练好 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ebmjnj/how_can_i_train_ppo_well_in_diverse_state/</link>
      <description><![CDATA[假设山地车环境是用随机生成的斜率和随机定位的目标初始化的。 在这种情况下，训练 PPO 有什么技巧？ 我目前正在做一个项目，它是一个优化问题（优化放射治疗计划中的辐射光束角度）。 我想训练代理来调整随机设置的光束角度。 而更困难的问题是，我想让代理优化光束，而不管任何患者的解剖几何形状如何。 所以我设置了自定义环境，在每个 env.reset() 中，环境都会设置不同的患者（当然是不同的肿瘤位置和大小）。 是否有可能训练 DRL 算法以在那种多样化的患者观察中很好地工作？ 如果可能的话，有什么技巧可以解决它？ 我在想的是为了在这些多样化的观察中表现良好，批次大小应该足够大以覆盖许多不同的患者几何形状。所以我目前将 n_steps（horizo​​n）设置为很大但它还不起作用.. 我将保留我之前在 reddit 中讨论过的链接供您参考。 https://www.reddit.com/r/reinforcementlearning/comments/1eapb34/any_rl_study_about_observing_3d_data/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ebmjnj/how_can_i_train_ppo_well_in_diverse_state/</guid>
      <pubDate>Thu, 25 Jul 2024 05:00:15 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的 VRP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eb0lpd/vrp_with_reinforcement_learning/</link>
      <description><![CDATA[大家好，我正在尝试使用 RL 解决 VRP，有人能帮我提供材料、课程和书籍吗？    提交人    /u/Fuzzy_mind491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eb0lpd/vrp_with_reinforcement_learning/</guid>
      <pubDate>Wed, 24 Jul 2024 12:47:02 GMT</pubDate>
    </item>
    <item>
      <title>近期最喜欢的 RL 论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eazyxp/favorite_recent_rl_papers/</link>
      <description><![CDATA[您最近最喜欢的 RL 论文是什么，比如说过去 2-4 年发表的论文？只是好奇你们最近都觉得什么令人兴奋。    提交人    /u/Obsesdian   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eazyxp/favorite_recent_rl_papers/</guid>
      <pubDate>Wed, 24 Jul 2024 12:15:24 GMT</pubDate>
    </item>
    <item>
      <title>我这样做对吗？我正在尝试创建一个小数据集。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eazoi8/am_i_doing_this_right_im_trying_to_create_a_small/</link>
      <description><![CDATA[我正在尝试使用 Opentron API 对其 OT-2 和 Flex 机器人的模拟数据。我所做的事情涉及机器人进行稀释的协议，协议的代码为此处。模拟此代码后，我创建了一个文件，其中包含我提取的数据，其格式基于操作、使用的量和移液机器人上的位置。 extracted dataset text.xlsx 。目的是使用模拟来提取状态、动作和图像。此步骤涉及创建轨迹，每个轨迹都是数据集的一个样本。实现传统的深度 RL 解决方案并评估它们在创建的数据集上的性能。 这种格式对 RL 来说好吗？我需要做哪些更改？ 我在网上搜索了不同的 RL 模型，比如 DQN 或 DDPG，但如何让它们提取出我需要绘制图表的数据呢？有些使用了图像，所以我想到使用 ROS 和 Gazebo 进行模拟来获取我要创建的数据集的所述图像。我在尝试下载 gazebo 时遇到了问题，所以我没有任何链接， 当谈到使用 RL 时，我是否甚至需要使用 gazebo 来获取图像？我如何将所述信息插入模型或算法以从中获取某些东西？ 我完全糊涂了，我的问题也可能因此而令人困惑，所以我会在收到回复后进行编辑以添加更多内容。    提交人    /u/Own_Bat7296   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eazoi8/am_i_doing_this_right_im_trying_to_create_a_small/</guid>
      <pubDate>Wed, 24 Jul 2024 12:00:47 GMT</pubDate>
    </item>
    <item>
      <title>多代理库[Python]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eazdnc/multiagent_librarypython/</link>
      <description><![CDATA[有任何关于 MARL 库的建议吗？ 好的，谢谢 Byi    提交人    /u/OccupyFood101   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eazdnc/multiagent_librarypython/</guid>
      <pubDate>Wed, 24 Jul 2024 11:44:33 GMT</pubDate>
    </item>
    <item>
      <title>代理模拟入门</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eay362/getting_started_with_agent_simulation/</link>
      <description><![CDATA[大家好，我想开始使用多智能体模拟，因为这是我计划在大学最后一年做的项目。这对我来说是一个全新的概念，大学里也没有教过，所以我想学习，我想知道我该如何开始学习？附言：我已经做了研究，但没有找到明确的学习路径。谢谢    提交人    /u/turbanator300   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eay362/getting_started_with_agent_simulation/</guid>
      <pubDate>Wed, 24 Jul 2024 10:30:09 GMT</pubDate>
    </item>
    <item>
      <title>“通过扭曲序贯蒙特卡洛进行语言模型中的概率推理”，Zhao 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eaq9tl/probabilistic_inference_in_language_models_via/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eaq9tl/probabilistic_inference_in_language_models_via/</guid>
      <pubDate>Wed, 24 Jul 2024 02:27:29 GMT</pubDate>
    </item>
    <item>
      <title>有任何关于观察 3D 数据的 RL 研究吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eapb34/any_rl_study_about_observing_3d_data/</link>
      <description><![CDATA[嗨，有没有使用 3D 空间数据来观察状态的研究？ 我正在做一个 RL 项目，它的观察空间是 3D。特别是病人的 CT 扫描。 3D 扫描的尺寸非常大（缩小到 128*128*64），所以我使用 3D CNN 编码器来减小尺寸。 除了深度学习方法之外，我没有研究过太多 RL，而且似乎构建网络架构与深度学习非常不同（例如，RL 网络要小得多，大多数层只是 MLP，据我在 atari 教程中看到的那样，CNN 编码器中没有规范化）。 有人可以分享任何使用 3D 编码器来编码状态的论文或代码吗？    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eapb34/any_rl_study_about_observing_3d_data/</guid>
      <pubDate>Wed, 24 Jul 2024 01:40:19 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习：与无模型强化学习的区别令人困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eachu8/modelbased_rl_confused_about_the_differences/</link>
      <description><![CDATA[在互联网上，人们可以找到许多帖子来解释 MBRL 和 MFRL 之间的区别。即使在 Reddit 上，也有一个很好的直观帖子。那么，为什么还要问一个关于同一主题的无聊问题呢？ 因为当我读到类似这样的定义时： 基于模型的强化学习 (MBRL) 是一个用于在部分理解的环境中解决任务的迭代框架。有一个代理反复尝试解决问题，积累状态和操作数据。利用这些数据，代理创建一个结构化的学习工具——动态模型——来推理世界。利用动态模型，代理通过预测未来来决定如何行动。通过这些操作，代理可以收集更多数据，改进所述模型，并有望改进未来的操作。 (source)。 那么对我来说，MBRL 和 MFRL 之间只有一个区别：在模型自由的情况下，您将问题视为黑匣子。然后你实际上运行双或数百万个步骤来了解黑匣子的工作原理。但这里的问题是：与 MBRL 有什么区别？ 另一个问题是，当我读到时，您不需要 MBRL 的模拟器，因为算法在训练阶段可以理解动态。好的。这对我来说很清楚…… 但是假设您有一辆正在行驶的汽车（没有摄像头，只有汽车在跑道上行驶的形状）并且您想要应用 MBRL，那么您需要一个汽车模拟器，因为模拟器会生成代理所需的图片，以便代理能够真正看到汽车是否在路上。  所以即使我认为，我理解了两者之间的理论区别，但当我试图弄清楚何时需要模拟器，何时不需要模拟器时，我仍然停滞不前。 从字面上讲：即使我在 Gymnasium 中为 Cartpole 环境训练一个简单的代理（并使用无模型方法），我也需要一个模拟器。 但是，如果我想使用 GPS（基于模型），那么无论如何我都需要那个环境。  如果您能帮助我理解，我真的很感激。 谢谢   由    /u/WilhelmRedemption  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eachu8/modelbased_rl_confused_about_the_differences/</guid>
      <pubDate>Tue, 23 Jul 2024 16:40:21 GMT</pubDate>
    </item>
    </channel>
</rss>