<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 01 Nov 2024 21:15:29 GMT</lastBuildDate>
    <item>
      <title>深度强化学习尚未奏效。发布于 2018 年。六年后，您认为情况发生了多大变化，哪些保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ghf83z/deep_reinforcement_learning_doesnt_work_yet/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ghf83z/deep_reinforcement_learning_doesnt_work_yet/</guid>
      <pubDate>Fri, 01 Nov 2024 20:35:14 GMT</pubDate>
    </item>
    <item>
      <title>变压器ppo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gh4qcv/transformer_ppo/</link>
      <description><![CDATA[我知道 cleanrl 已经发布了精益版本。有没有人有经验，可以告诉 transformer ppo 是否能取得更好的结果？更强大？比 gru 好吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gh4qcv/transformer_ppo/</guid>
      <pubDate>Fri, 01 Nov 2024 12:57:32 GMT</pubDate>
    </item>
    <item>
      <title>“π~0~：用于通用机器人控制的视觉-语言-动作流模型”，Black 等人 2024 年{物理智能}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggv2o3/π0_a_visionlanguageaction_flow_model_for_general/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggv2o3/π0_a_visionlanguageaction_flow_model_for_general/</guid>
      <pubDate>Fri, 01 Nov 2024 02:03:51 GMT</pubDate>
    </item>
    <item>
      <title>自然语言强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggpkub/rl_with_natural_language/</link>
      <description><![CDATA[我一直在研究一些将语言纳入强化学习框架的新论文，比如微软雷德蒙德的这篇论文(https://arxiv.org/pdf/1511.04636)、Reader(https://aclanthology.org/2023.emnlp-main.1032/)、Ready to Fight Monsters，以及最近的Learning to Model the World with Language(https://arxiv.org/abs/2308.01399)，我想知道这里是否有人可以指点一下强化学习领域其他有趣的作品。此字段。    提交人    /u/potatodafish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggpkub/rl_with_natural_language/</guid>
      <pubDate>Thu, 31 Oct 2024 21:28:34 GMT</pubDate>
    </item>
    <item>
      <title>[项目] PyMAB：一个用于多臂老虎机的探索性 Python 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gglajl/project_pymab_an_exploratory_python_library_for/</link>
      <description><![CDATA[大家好！我很高兴与大家分享 PyMAB，这是我为多臂老虎机 (MAB) 算法开发的 Python 库。它被设计为研究人员和强化学习爱好者尝试比较多种 MAB 算法和配置的实验工具。 📦 安装 pip install pymab 或者访问我们的 github 页面：https://github.com/danielaLopes/pymab 🎯 主要特点 多种 MAB 算法：  贪婪和 ε-greedy 汤普森采样（高斯和伯努利） 上限置信区间 (UCB) 贝叶斯 UCB 上下文 Bandits  多种环境：  平稳 非平稳  渐进式 突变 随机交换手臂   内置可视化：  奖励曲线 遗憾分析 动作分布 策略比较  📊 快速示例 以下是如何使用 PyMAB 的简单示例： from pymab.policies import ThompsonSamplingPolicy from pymab.game import Game # 初始化 Thompson Sampling policy = ThompsonSamplingPolicy(n_bandits=5) # 创建并运行模拟游戏 = Game(n_episodes=1000, n_steps=1000, strategies=[policy], n_bandits=5) game.game_loop() # 可视化结果 game.plot_average_reward_by_step() 该 repo 包含多个 jupyter-notebooks 示例。  如果您有任何问题或建议，请告诉我！我正在积极监控这个帖子，并很高兴收到您的反馈。 这是一个正在进行的项目，我们一直在寻找建议和贡献。如果您有任何想法或想提供帮助，请联系我们！ 标签：#MultiArmedBandits #ReinforcementLearning    提交人    /u/danielalopes97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gglajl/project_pymab_an_exploratory_python_library_for/</guid>
      <pubDate>Thu, 31 Oct 2024 18:21:52 GMT</pubDate>
    </item>
    <item>
      <title>[R] 我们针对 AI 评估器尝试了不同的训练目标，结果如下：</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggjisd/r_our_results_experimenting_with_different/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggjisd/r_our_results_experimenting_with_different/</guid>
      <pubDate>Thu, 31 Oct 2024 17:05:51 GMT</pubDate>
    </item>
    <item>
      <title>MBRL 文本建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gghi1c/mbrl_text_recommendations/</link>
      <description><![CDATA[大家好！ 我有兴趣进一步了解基于连续动作模型的强化学习算法（来自动态和控制背景）。我读过 Farsi 和 Liu 的书，但如果有人有好的推荐，我正在寻找更多好书/论文！    提交人    /u/Voltimeters   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gghi1c/mbrl_text_recommendations/</guid>
      <pubDate>Thu, 31 Oct 2024 15:39:05 GMT</pubDate>
    </item>
    <item>
      <title>现已在 YouTube 上提供 - 观看 Emma Brunskill 主持的斯坦福 CS234 强化学习的所有讲座</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gghbth/now_available_on_youtube_stream_all_the_lectures/</link>
      <description><![CDATA[  由    /u/Stanford_Online  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gghbth/now_available_on_youtube_stream_all_the_lectures/</guid>
      <pubDate>Thu, 31 Oct 2024 15:31:42 GMT</pubDate>
    </item>
    <item>
      <title>关于DQN训练的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggejj6/question_about_dqn_training/</link>
      <description><![CDATA[在每一集之后进行训练而不是逐步进行训练可以吗？任何答案都会有所帮助。谢谢    提交人    /u/Sea-Collection-8844   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggejj6/question_about_dqn_training/</guid>
      <pubDate>Thu, 31 Oct 2024 13:29:17 GMT</pubDate>
    </item>
    <item>
      <title>用于知识蒸馏的决策转换器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggbau7/decision_transformer_for_knowledge_distillation/</link>
      <description><![CDATA[我正在研究一个模仿学习问题，我希望产生一个动作，让代理根据当前状态观察和先前的动作重现参考状态。我目前的想法是开发一个 MoE 或 MCP 策略，该策略可以查询一组预训练的 MLP，以查找代理可能遇到的不同“问题”。然后，我想将其提炼为一个可以独立运行的单一策略。 我正在研究各种选项，对于这个应用程序来说，使用转换器似乎很合理，因为从我的理解来看，我的问题的时间顺序特征可以从转换器中受益，我希望它可以提高模仿看不见的参考状态的策略的通用性。 但是，我对几件事不确定。理想情况下，可以使用 PPO 在线提炼/训练，但在线决策变压器似乎在更广泛的文献中未经测试（除非我不擅长找到它），并且奖励的适应性对我来说不是很清楚。 我也见过有人在决策变压器中放弃奖励，但仍然选择离线训练和在线调整。 或者，我可以使用另一个网络（例如 VAE）来提炼信息并进行完全在线训练，但我目前有兴趣探索除此之外的东西，除非它真的是最好的选择。  我很感激对此的一些意见，因为我是这些更先进/新颖的 RL 技术的新手，并且确切地知道应该何时应用它们。     提交人    /u/nalliable   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggbau7/decision_transformer_for_knowledge_distillation/</guid>
      <pubDate>Thu, 31 Oct 2024 10:30:59 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggawov/deep_reinforcement_learning_survey/</link>
      <description><![CDATA[深度强化学习中泛化的分析调查 链接：https://arxiv.org/pdf/2401.02349v2    提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggawov/deep_reinforcement_learning_survey/</guid>
      <pubDate>Thu, 31 Oct 2024 10:03:20 GMT</pubDate>
    </item>
    <item>
      <title>“CodeIt：具有优先后视重放功能的自我改进语言模型”，Butt 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gg3zbi/codeit_selfimproving_language_models_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gg3zbi/codeit_selfimproving_language_models_with/</guid>
      <pubDate>Thu, 31 Oct 2024 02:13:03 GMT</pubDate>
    </item>
    <item>
      <title>使用此 YouTube 聊天工具学习 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfrasj/learn_rl_using_this_youtube_chat_tool/</link>
      <description><![CDATA[      对于 RL 的自学者，我知道有大量超过一小时的 RL YouTube 视频。所以我创建了这个 YouTube 聊天，这样我就可以与视频聊天并进行总结。欢迎尝试！  www.arcanx-search.com https://preview.redd.it/8sefolufbxxd1.png?width=2988&amp;format=png&amp;auto=webp&amp;s=5a7e4ec3b2b1c5b92f75ea9a677c9102efcf58e9    提交人    /u/Limp-Run-9075   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfrasj/learn_rl_using_this_youtube_chat_tool/</guid>
      <pubDate>Wed, 30 Oct 2024 16:54:08 GMT</pubDate>
    </item>
    <item>
      <title>有没有论文研究过RL算法和优化器之间的关系？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfm7a3/are_there_any_papers_that_have_studied_the/</link>
      <description><![CDATA[我个人正在尝试很多简单的算法，我已经考虑这个问题一段时间了：adam 的动量可以帮助模型快速收敛，但当目标值发生变化时，这会有害吗，就像 RL 一样？如果大部分动量超出预测，并且目标网络复制了这种超出，则模型可能会发散。有人对此做过研究吗？    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfm7a3/are_there_any_papers_that_have_studied_the/</guid>
      <pubDate>Wed, 30 Oct 2024 13:13:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 实现的 PPO，学习动作分布的平均值和标准差 σ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfgwwq/ppo_implementation_in_python_with_learned_mean/</link>
      <description><![CDATA[我正在寻找有关 Python 软件包的推荐，这些软件包允许训练 PPO 代理，其中动作分布的平均值 (μ) 和标准差 (σ) 都是学习到的。 具体来说，我需要模型在给定当前观察的情况下在推理过程中输出动作和相应的标准差。这将帮助我评估代理对其预测的信心。 我见过关于 skrl 和 AgileRL 软件包的讨论，它们据称是模块化的。有人用过这些吗？他们也可以提供学习到的 σ 吗？ 任何建议或见解都将不胜感激。 谢谢。    提交人    /u/Disastrous_Effort725   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfgwwq/ppo_implementation_in_python_with_learned_mean/</guid>
      <pubDate>Wed, 30 Oct 2024 07:23:26 GMT</pubDate>
    </item>
    </channel>
</rss>