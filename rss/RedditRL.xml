<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Thu, 20 Feb 2025 12:34:34 GMT</lastBuildDate>
    <item>
      <title>增强学习的书籍[代码+理论]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itxbjj/books_for_reinforcement_learning_code_theory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ 该代码似乎有些复杂，因为很难编程我在RL中介绍的初始理论。  关于加固学习，哪些书可以阅读以了解代码以及代码部分。  另外，阅读RL理论和概念的时间是多少，一个人可以开始编码rl。 请告诉我！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/internationalwill912     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itxbjj/books_for_reinforcement_learning_code_theory/</guid>
      <pubDate>Thu, 20 Feb 2025 12:26:38 GMT</pubDate>
    </item>
    <item>
      <title>代理不学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itwfgc/agent_not_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://reddit.com /link/1Itwfgc/video/ggfrxkxf4ake1/player   大家好，我目前正在进行自动化车辆模拟。我已经进行了汽车和当前的训练，以使其绕着赛道。但是，尽管训练了超过100k的步骤，但代理似乎没有学到任何东西。这里可能有什么问题？奖励/惩罚点是否未正确给出？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lonely_joke944     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itwfgc/agent_not_learning/</guid>
      <pubDate>Thu, 20 Feb 2025 11:32:22 GMT</pubDate>
    </item>
    <item>
      <title>稳定生物素的子保护</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itw58z/subprocvecenv_from_stablebaselines/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试使用start_method =＆quot =&#39;fork subprocvecenv中的稳定baselines2中的多量量，但它无效，无法找到＆quort的上下文; fork&#39;&#39;。我使用的是稳定的 - 冰淇淋3 2.6.0a1，打印了所有可用的方法，我唯一可以使用的方法是“ Spawn”我不知道为什么。有人知道我该怎么做才能修复它？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/clightable-button264     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itw58z/subprocvecenv_from_stablebaselines/</guid>
      <pubDate>Thu, 20 Feb 2025 11:13:37 GMT</pubDate>
    </item>
    <item>
      <title>最佳RL存储库具有简单的SOTA算法实现，这些算法易于编辑？ （最好在JAX中）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ituera/best_rl_repo_with_simple_implementations_of_sota/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/godireallyhateyoutim     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ituera/best_rl_repo_with_simple_implementations_of_sota/</guid>
      <pubDate>Thu, 20 Feb 2025 09:11:51 GMT</pubDate>
    </item>
    <item>
      <title>对于那些通过模拟进行加固学习（RL）的人，我已经在NVIDIA ISAAC实验室上播放了10个视频</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itu6na/for_those_looking_into_reinforcement_learning_rl/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32;态href =“ https://www.youtube.com/watch?v=sl1wcfp9tru＆amp；   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itu6na/for_those_looking_into_reinforcement_learning_rl/</guid>
      <pubDate>Thu, 20 Feb 2025 08:55:36 GMT</pubDate>
    </item>
    <item>
      <title>我紧急需要RL资源！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itrl50/i_need_rl_resources_urgently/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在TMR上进行考试，如果您可以共享YouTube资源，请分享，如果知道这些是 &gt; 1.Multi -armed Bandit    ucb     3.TIC TAC TOE    MDP  梯度强盗＆amp;非固定问题   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/codeprocastinator     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itrl50/i_need_rl_resources_urgently/</guid>
      <pubDate>Thu, 20 Feb 2025 05:57:55 GMT</pubDate>
    </item>
    <item>
      <title>好奇你们用作DRL算法的库。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1itpjje/curious_on_what_you_guys_use_as_a_library_for_drl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我已经在练习强化学习（RL）已经有一段时间了。最初，我曾经根据研究论文编码算法，但是如今，我使用体育馆图书馆和具有稳定的基本线3（SB3）的RL代理来开发环境，并在必要时创建自定义策略。  i&#39;&#39;&#39;很想知道您在从事的工作以及用于环境和算法的哪些库。此外，如果行业中有任何专业人员，我很想听听您是否使用任何特定库或使用代码库。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/wild_wolf19    href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1itpjje/curious_on_what_you_guys_as_a_a_a_a_library_for_for_for_for_drl/”&gt; [links]       &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1itpjje/curious_on__ what_you_guys_ause_ause_a_a_a_a_library_for_for_for_drl/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1itpjje/curious_on_what_you_guys_use_as_a_library_for_drl/</guid>
      <pubDate>Thu, 20 Feb 2025 03:59:12 GMT</pubDate>
    </item>
    <item>
      <title>样品效率（MBRL）与腿部现象的SIM2REAL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1it4d70/sample_efficiency_mbrl_vs_sim2real_for_legged/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想研究腿部运动的RL（双皮亚，类人动物），我很好奇当前哪种研究方法似乎更可行 - 培训模拟和培训通过提高样品效率（也许使用MBRL）来直接致力于改善SIM2REAL，直接培训物理机器人。这两种方法之间是否有明确的偏好？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1it4d70/sample_efficiency_mbrl_vs_sim2real_for_legged/</guid>
      <pubDate>Wed, 19 Feb 2025 12:30:29 GMT</pubDate>
    </item>
    <item>
      <title>从字面上重新创建了数学推理和DeepSeek的AHA时刻，不到10美元，通过最终简单的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1it2zhv/literally_recreated_mathematical_reasoning_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://medium.com/@rjusnba/overnight-end-end-eend-to-end-raind-raining-a-3b-model-on-a-a grade-school-school-math-math-dataset-leads-leads-to-to-to-to-to-rounconing-df61410c04c6   我感到惊讶！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/unightent-life9355     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1it2zhv/literally_recreated_mathematical_reasoning_and/</guid>
      <pubDate>Wed, 19 Feb 2025 11:06:27 GMT</pubDate>
    </item>
    <item>
      <title>纸牌游戏RL项目的硬件/软件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iszy7r/hardwaresoftwarr_for_card_game_rl_projects/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在跳入RL，想在诸如巫师或类似纸牌游戏的纸牌游戏上训练AI。 Chatgpt在Python上使用stable_baselines3给了我一个不错的开端。它似乎工作得很好，但是我不确定我是否长期正确。您是否对我应该考虑的软件和库有建议？您是否会建议特定的硬件来大大加快流程？我目前有一个带有Ryzen 5600和3060TI GPU的系统。培训约为1200fps（如果有任何用途）。我可以升级到5950x，但也想考虑一台专用的迷你PC，如果配合得当。 提前感谢！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tot-chance9372     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iszy7r/hardwaresoftwarr_for_card_game_rl_projects/</guid>
      <pubDate>Wed, 19 Feb 2025 07:32:12 GMT</pubDate>
    </item>
    <item>
      <title>RL研究组？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isz4eh/study_group_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有RL研究组？美国时区 更新： 您会添加 时区或位置 当前ML背景  P&gt;对RL的重点或兴趣，即传统的RL，Deep RL，理论和论文，Pytorch等/p&gt;  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/best_fish_2941     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isz4eh/study_group_for_rl/</guid>
      <pubDate>Wed, 19 Feb 2025 06:36:15 GMT</pubDate>
    </item>
    <item>
      <title>TD学习以估算OpenAI体育馆中杂技环境中选择的随机固定政策的价值功能。如何应对持续状态空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isq0rd/tdlearning_to_estimate_the_value_function_for_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有这项作业，我们需要使用TD学习来估算OpenAI Gym中杂技演员环境中所选随机固定策略的价值函数。持续的状态空间阻碍了我，我不知道我应该如何离散。即使有少数间隔，我也会获得六维空间。提交由＆＃32; /u/u/basic_exit_4317     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isq0rd/tdlearning_to_estimate_the_value_function_for_a/</guid>
      <pubDate>Tue, 18 Feb 2025 23:03:30 GMT</pubDate>
    </item>
    <item>
      <title>研究主题基础艾伯塔省计划</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iskafj/research_topics_basis_the_alberta_plan/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我听说了理查德·萨顿（Richard Sutton）的艾伯塔省计划，但是由于我是初学者。 RL中是否有一个特定的研究主题，我可以在接下来的几年中探索我的研究？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iskafj/research_topics_basis_the_alberta_plan/</guid>
      <pubDate>Tue, 18 Feb 2025 18:51:06 GMT</pubDate>
    </item>
    <item>
      <title>两足动力现在是解决问题的问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isewx8/is_bipedal_locomotion_a_solved_problem_now/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我刚刚遇到了Unitree的发展，我只是想知道是否公平地假设双皮亚运动（对于人叶子）已经实现了（忽略诸如价格和其他东西之类的因素）。 现在，从研究的角度来看，人形机器人是一个解决的问题？   &lt;！ -  sc_on - &gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isewx8/is_bipedal_locomotion_a_solved_problem_now/</guid>
      <pubDate>Tue, 18 Feb 2025 15:14:01 GMT</pubDate>
    </item>
    <item>
      <title>必须阅读强化学习论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，大家好，所以我是CS毕业，并且在深度学习和计算机视觉方面有体面的知识。我现在想学习强化学习（特别是用于飞行机器人的自动导航）。因此，您能从您的经验中告诉我，哪些论文是一本强制性的阅读，可以开始并在强化学习方面保持体面。预先感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/dronesanddynamite     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</guid>
      <pubDate>Tue, 18 Feb 2025 07:26:22 GMT</pubDate>
    </item>
    </channel>
</rss>