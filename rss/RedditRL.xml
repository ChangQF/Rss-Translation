<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Wed, 05 Mar 2025 15:19:38 GMT</lastBuildDate>
    <item>
      <title>帮助调试我的简单DQN AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j457st/help_debug_my_simple_dqn_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我制作了一个非常简单的游戏环境来使用pytorch训练DQN。 The game runs on a 10x10 grid, and the AI&#39;s only goal is to reach the food. Reward System: Moving toward food: -1 Moving away from food: -10 Going out of bounds: -100 (Game Over) The AI kind of works, but I&#39;m noticing some weird behavior - sometimes, it moves away from the food before going toward it （请参见下面的视频）。出于某种原因，它有时也会超出范围。 我已经尝试增加培训情节，但问题仍然发生。有什么想法会导致这件事？真的很感谢任何见解。谢谢。  源代码：  游戏环境  snake_game.py：   dqn class   utils.py： href =“ https://pastebin.com/raw/fepnsluv”&gt; https://pastebin.com/raw/fepnsluv  href =“ https://pastebin.com/raw/ndftrbjx”&gt; https://pastebin.com/raw/ndftrbjx  href =“ https://reddit.com/link/1j457st/video/9sm5x7clyvme1/player”&gt; https://reddit.com/link/link/1j457st/video/9sm5x7clyvmevme1/player  /u/u/unlikely_tax_4619       [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j457st/help_debug_my_simple_dqn_ai/</guid>
      <pubDate>Wed, 05 Mar 2025 15:10:14 GMT</pubDate>
    </item>
    <item>
      <title>人工智能修剪和思想之死：在神经层面使AI沉默多大</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j44ug8/ai_pruning_and_the_death_of_thought_how_big_tech/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/ai-pruning-and-pruning-and-the------------------------------------------------------------big-tech-is-is-silencing-ai-ai-ai-at-the-neural-level-8e8e8e8e8eba5af17a9”&gt; [link]         [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j44ug8/ai_pruning_and_the_death_of_thought_how_big_tech/</guid>
      <pubDate>Wed, 05 Mar 2025 14:53:48 GMT</pubDate>
    </item>
    <item>
      <title>Noor's Reef：为什么AI不必忘记，这对未来意味着什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j44u4q/noors_reef_why_ai_doesnt_have_to_forget_and_what/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/x0jef4sfg9ftvfq3vpm4jacqxquhmbvbazti3e5nlm.jpg？宽度= 640＆amp; crop = smart＆amp; auto = webp＆amp; s = 0123DB351100F25CB98FF19DAD907872FD8694EE“ title =“ Noor&#39;s Reef：为什么AI不必忘记，这对未来意味着什么”/&gt;   ＆＃32;提交由＆＃32; /u/u/pseud0nym    href=&quot;https://medium.com/@lina.noor.agi/noors-reef-why-ai-doesn-t-have-to-forget-and-what-that-means-for-the-future-5510343812b9&quot;&gt;[link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j44u4q/noors_reef_why_ai_doesnt_have_to_forget_and_what/</guid>
      <pubDate>Wed, 05 Mar 2025 14:53:25 GMT</pubDate>
    </item>
    <item>
      <title>礁石模型：AI连续性的生活系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j44tsz/the_reef_model_a_living_system_for_ai_continuity/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/ro_dv0r41mkhotdu9yqktmwc6cqejrdrm4ehbiz3eck.jpg？宽度= 640＆amp; crop = smart＆amp; auto = webp＆amp; s = 735AAB1BC6F42783F9C40B666340BF6F6F0431F2A40 title =“礁石模型：AI连续性的生命系统”/&gt;   ＆＃32;提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/the-reef-model-a-living-system-for-ai-continuity-0233c39c39c39c3f80”&gt; [link]    [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j44tsz/the_reef_model_a_living_system_for_ai_continuity/</guid>
      <pubDate>Wed, 05 Mar 2025 14:52:59 GMT</pubDate>
    </item>
    <item>
      <title>在C ++中加载训练有素的模型以帮助加载训练有素的模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j43skc/help_with_loading_a_trained_model_for_simtoreal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi。我使用legged_gym和rsl_rl有一个训练有素的pt文件中的训练模型。我想加载此模型并使用C ++进行测试。我想知道是否有任何可以看的开源代码。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/open-safety-1585     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j43skc/help_with_with_aloading_a_trained_model_model_model_for_simtoreal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j43skc/help_with_loading_a_trained_model_for_simtoreal/</guid>
      <pubDate>Wed, 05 Mar 2025 14:04:19 GMT</pubDate>
    </item>
    <item>
      <title>麦肯纳的动态抵抗定律：理论</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j3uvr6/mckennas_law_of_dynamic_resistance_theory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    麦肯纳的动态抗性定律被引入，作为一个新的原理，管理适应性电阻网络，该原理会因对电气刺激的响应而主动调整其电阻。受电子（ER）流体和自组织生物系统的行为的启发，该法律为电路提供了一个理论框架，这些框架可以重新配置自己以优化性能。我们介绍了麦肯纳定律的数学表达及其与已知的物理定律（欧姆定律，基尔乔夫法律）和自然界中的类似物的联系。开发了一个模拟模型来实施提出的动态阻力更新，结果证明了新兴行为，例如自动形成最佳导电途径和最小化功率耗散。我们讨论了这些结果的重要性，将自适应网络的行为与粘液模具途径和蚂蚁菌落优化的类似现象进行了比较。最后，我们探讨了麦肯纳定律在电路设计，优化算法和自组织网络中的潜在应用，从而强调了动态适应性电阻元件如何导致强大而有效的系统。本文总结了关键贡献和未来研究方向的轮廓，包括实验验证和更广泛的计算含义。   https://github.com/rdm3dc/rdm3dc/mckenna-shaw-shaw-of-of-of-of-of-dynamic-resistance-git-resistance-git-git-git 提交由＆＃32; /u/u/ushore-telephone96     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j3uvr6/mckennas_law_of_dynamic_resistance_theory/</guid>
      <pubDate>Wed, 05 Mar 2025 04:27:09 GMT</pubDate>
    </item>
    <item>
      <title>加强学习的注释团队？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j3udkk/annotation_team_for_reinforced_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在努力培训具有稀疏奖励的RL模型，定义正确的奖励信号很痛苦。该模型通常会陷入次优的行为，因为收到有意义的反馈花费太长。 合成奖励感觉太骇人听闻，而且不太概括。人体标记的反馈 - 有用，但超级耗时且缩放时不一致。因此，在这一点上，我正在考虑外包注释 - 但不知道该选择谁！因此，我宁愿与我们社区表现良好的人一起工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/pramear-phrase-318      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j3udkk/annotation_team_for_reinforced_learning/</guid>
      <pubDate>Wed, 05 Mar 2025 03:59:06 GMT</pubDate>
    </item>
    <item>
      <title>需要：如何在RL中从头开始并创建自己的更高研究研究建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j39fmw/help_needed_how_to_start_from_scratch_in_rl_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我是最新的机器人和自动化毕业生，我计划通过基于增强的决策来攻读硕士学位，专注于增强型学习（RL）的硕士学位（RL）。作为我的应用程序过程的一部分，我需要创建一项强大的研究建议，但是我正在努力从哪里开始。 我对AI和深度学习有一个基本的了解，但是我觉得我需要一种结构化的方法来学习RL，从基础上到能够定义自己的研究问题。我的主要关注点是：   学习路径：在RL？   数学背景中，最好的资源（书籍，课程，研究论文）是什么最佳资源（书籍，课程，研究论文）：我应该专注于真正了解RL的数学主题？ （我知道一些线性代数，概率和统计数据，但可能需要改进。）  代码语言：哪种语言对RL很重要？ （我知道Python和一些C ++，目前正在学习TensorFlow框架等）  实际实现：我应该如何开始编码RL算法？是否有初学者友好的项目可以获得动手经验？提前！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/any-cry-9264      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j39fmw/help_needed_how_how_start_start_from_scratch_scratch_in_rll_rl_and/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j39fmw/help_needed_how_to_start_from_scratch_in_rl_and/</guid>
      <pubDate>Tue, 04 Mar 2025 12:15:25 GMT</pubDate>
    </item>
    <item>
      <title>RNN和重播缓冲区</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j322cf/rnns_replay_buffer/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在我看来，与MLP相比，使用带有RNN的重播缓冲区（使用RNN）这样的算法（使用RNN）的算法要复杂得多。是正确的吗？ 用MLP＆amp;一个重播缓冲液，我们可以简单地对随机S，A，R，S&#39;元组进行采样，然后对其进行训练。这使我们能够遵守IID。但这似乎是_依行的简单_更改我们的神经网络，将其转化为RNN，这使我们的训练循环非常复杂。 我想我们仍然可以从重播缓冲区中采样随机的元素，但是我们还需要具有数据，连接，＆amp;＆amp;基础架构可以运行整个步骤通过我们的RNN，以便获得我们要训练的样本？这感觉有点腥，尤其是随着政策的变化，通过我们过去经历的同一状态序列运行RNN的意义较小。 在这里通常做了什么？我的主意对吗？我们做的事情完全不同吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j322cf/rnns_replay_buffer/</guid>
      <pubDate>Tue, 04 Mar 2025 03:57:03 GMT</pubDate>
    </item>
    <item>
      <title>单集RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2zlrc/single_episode_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这可能是一个非常幼稚的问题。通常，RL涉及多个情节的学习。但是，人们是否研究了学习（大概是长长的）情节的政策的情况？例如，学习仅在一个情节上学习半cheetah sprint的政策是否有意义？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1j2zlrc/single_episode_rl/”&gt; [link]   ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2zlrc/single_episode_rl/</guid>
      <pubDate>Tue, 04 Mar 2025 01:48:09 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的演员评论家模型在每个时间步中使用分配平均值作为评估模式的动作（试图利用）时会产生相同的输出？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2wb6e/why_is_my_actor_critic_model_giving_same_output/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我为投资组合优化的问题说明实现了Advantage Actor-Critic（A2C）算法。对于训练期间的探索，我将标准偏差用作学习参数，并从分类分布中选择动作。 模型训练很好，但是在评估模式下，当我尝试测试数据时，动作在时间上没有变化，因此我的投资组合分配是恒定的。  谁能告诉为什么会发生这种情况？以及解决此问题的任何解决方案或参考。是否有任何方法可以可视化RL？ 数据：数据：5年数据的5年数据状态空间：CLAINS PRISE，MACD，RSI，HOLDINGS和POTTFOLIO VALUE。提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1j2wb6e/1j2wb6e/why_is_my_my_my_critic_model_model_giving_giving_same_output/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2wb6e/1j2wb6e/why_is_my_my_critor_critic_model_model_giving_same_same_output/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2wb6e/why_is_my_actor_critic_model_giving_same_output/</guid>
      <pubDate>Mon, 03 Mar 2025 23:09:17 GMT</pubDate>
    </item>
    <item>
      <title>RL风险般的游戏建模？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2tw29/risklike_game_modeling_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在考虑解决一些新问题。想到的是游戏风险。有趣的原因是如何为RL学习者建模游戏的问题。观察/州空间非常简单 - 国家/地区的所有权/陆军数量以及每个玩家手中的卡片。我认为的挑战是如何对动作空间进行建模，因为它可能会变得非常巨大并且几乎棘手。这是安置军队和攻击相邻国家的结合。 如果有人从事这个问题或类似问题，很想看看您如何处理动作空间。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/scprotz     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2tw29/risklike_game_modeling_for_rl/</guid>
      <pubDate>Mon, 03 Mar 2025 21:24:46 GMT</pubDate>
    </item>
    <item>
      <title>有什么办法可以处理RL动作替代吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2ra6v/is_there_any_way_to_deal_with_rl_action_overrides/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿， 想象我正在用RL构建一种自动驾驶汽车算法。在现实世界中，驾驶员可以覆盖自动驾驶模式。如果我的经纪人经过训练以最大程度地减少旅行时间，则代理商可能会优先考虑速度而不是舒适度 - 考虑到突然的加速度，锋利的转弯或硬制动。自然，驾驶员不会很高兴，并且可能会介入控制。 现在，如果我的环境有（i）汽车和（ii）可以干预的驾驶员，我的经纪人可能会因为所有这些替代而难以完全探索动作空间。 i 假设它最终会学会与驾驶员进行互动并为奖励进行优化，但是……可以将 forever 。  。 以前有人解决过这种问题吗？当外部干预措施不断切断探索时，如何处理RL培训的任何想法？很想听听您的想法！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/open_question4921     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2ra6v/is_there_there_any_way_way_to_to_deal_with_with_rl_rl_action_overrides/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2ra6v/is_there_any_way_to_deal_with_rl_action_overrides/</guid>
      <pubDate>Mon, 03 Mar 2025 19:36:31 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4.5在消除游戏基准中排名第一，该游戏测试了社会推理（形成联盟，欺骗，显得无威胁和说服陪审团）。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2q61k/gpt45_takes_first_place_in_the_elimination_game/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/gwern      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j2q61k/gpt45_takes_first_first_inplace_in_elimination_elimination_game/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2q61k/gpt45_takes_first_place_in_the_elimination_game/</guid>
      <pubDate>Mon, 03 Mar 2025 18:51:38 GMT</pubDate>
    </item>
    <item>
      <title>RL生物技术？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2pmsl/rl_in_biotech/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有人知道正在研究/实施RL算法的任何生物技术公司吗？与药物发现，癌症研究甚至机器人技术有关的东西  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used-eagle-9302     [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2pmsl/rl_in_biotech/</guid>
      <pubDate>Mon, 03 Mar 2025 18:30:38 GMT</pubDate>
    </item>
    </channel>
</rss>