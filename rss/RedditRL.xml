<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 31 Oct 2024 12:32:49 GMT</lastBuildDate>
    <item>
      <title>用于知识蒸馏的决策转换器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggbau7/decision_transformer_for_knowledge_distillation/</link>
      <description><![CDATA[我正在研究一个模仿学习问题，我希望产生一个动作，让代理根据当前状态观察和先前的动作重现参考状态。我目前的想法是开发一个 MoE 或 MCP 策略，该策略可以查询一组预训练的 MLP，以查找代理可能遇到的不同“问题”。然后，我想将其提炼为一个可以独立运行的单一策略。 我正在研究各种选项，对于这个应用程序来说，使用转换器似乎很合理，因为从我的理解来看，我的问题的时间顺序特征可以从转换器中受益，我希望它可以提高模仿看不见的参考状态的策略的通用性。 但是，我对几件事不确定。理想情况下，可以使用 PPO 在线提炼/训练，但在线决策变压器似乎在更广泛的文献中未经测试（除非我不擅长找到它），并且奖励的适应性对我来说不是很清楚。 我也见过有人在决策变压器中放弃奖励，但仍然选择离线训练和在线调整。 或者，我可以使用另一个网络（例如 VAE）来提炼信息并进行完全在线训练，但我目前有兴趣探索除此之外的东西，除非它真的是最好的选择。  我很感激对此的一些意见，因为我是这些更先进/新颖的 RL 技术的新手，并且确切地知道应该何时应用它们。     提交人    /u/nalliable   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggbau7/decision_transformer_for_knowledge_distillation/</guid>
      <pubDate>Thu, 31 Oct 2024 10:30:59 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ggawov/deep_reinforcement_learning_survey/</link>
      <description><![CDATA[深度强化学习中泛化的分析调查 链接：https://arxiv.org/pdf/2401.02349v2    提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ggawov/deep_reinforcement_learning_survey/</guid>
      <pubDate>Thu, 31 Oct 2024 10:03:20 GMT</pubDate>
    </item>
    <item>
      <title>RL 问题的 MDP 类型分类（半 MDP 或半 POMDP）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gg6fif/classification_of_mdp_type_for_rl_problem_semimdp/</link>
      <description><![CDATA[我正在研究基于 RL 的电动汽车路线问题 (VRP)，并正在寻求有关如何最好地对此设置的 MDP 类型进行分类的建议。以下是主要特征：  操作：有三种类型的操作：(1) 行驶到特定的起点-目的地 (OD) 对，(2) 充电，以及 (3) 空闲。每个驾驶操作都与特定的 OD 对相关联，并且持续时间根据目的地而变化。充电和空闲操作的持续时间均为 5 分钟。 决策间隔：决策间隔为每 5 分钟。但是，驾驶操作所需的时间各不相同，此后车辆再次变为空转状态。 状态空间：状态空间将车辆的位置离散化以指示 OD 对或充电站 ID，而不是精确位置。 动作掩码：RL 代理在每个决策步骤都会收到一个有效的动作掩码，以将其选择限制为可行的动作。  在这种设置下，我是否应该将此问题视为 半 MDP（由于驾驶操作的持续时间可变），或者是否应该将其归类为 半 POMDP（因为状态空间仅提供离散化位置而不是精确的车辆位置）？ 如果您能提供任何见解，说明有限的位置细节是否证明了半 POMDP 标签的合理性，或者是否更适合描述为半 MDP，我们将不胜感激！   由    /u/Furious-Scientist  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gg6fif/classification_of_mdp_type_for_rl_problem_semimdp/</guid>
      <pubDate>Thu, 31 Oct 2024 04:28:32 GMT</pubDate>
    </item>
    <item>
      <title>“CodeIt：具有优先后视重放功能的自我改进语言模型”，Butt 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gg3zbi/codeit_selfimproving_language_models_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gg3zbi/codeit_selfimproving_language_models_with/</guid>
      <pubDate>Thu, 31 Oct 2024 02:13:03 GMT</pubDate>
    </item>
    <item>
      <title>我正在为想要创建和协作创新项目的人工智能人士构建一个在线平台！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfruhy/im_building_an_online_platform_for_people_in_ai/</link>
      <description><![CDATA[大家好 :) 我有一些很酷的东西想和你们分享，在过去的几个月里，我一直在努力寻找实现梦想的方法 我正在为那些关心技术创新并通过建设和参与项目产生积极影响的人工智能人士创建一个在线中心 这个中心将是一个寻找志同道合的人联系并一起从事激情项目的地方。 目前，我们正在编写一个平台，以便每个人都能找到彼此并相互了解 在我们获得一些初始用户后，我们将从短期建设者计划开始，个人和团队可以参加在线竞赛，最突出的项目可以获得一些奖品 :) 我们的目标是通过帮助他人做同样的事情来让世界变得更美好 如果您喜欢我们的倡议，请在下面的网站上注册！ https://www.yournewway-ai.com/  几周后，一旦我们准备就绪，我们将向您发送加入我们平台的邀请 :)    提交人    /u/unknownstudentoflife   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfruhy/im_building_an_online_platform_for_people_in_ai/</guid>
      <pubDate>Wed, 30 Oct 2024 17:16:40 GMT</pubDate>
    </item>
    <item>
      <title>使用此 YouTube 聊天工具学习 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfrasj/learn_rl_using_this_youtube_chat_tool/</link>
      <description><![CDATA[      对于 RL 的自学者，我知道有大量超过一小时的 RL YouTube 视频。所以我创建了这个 YouTube 聊天，这样我就可以与视频聊天并进行总结。欢迎尝试！  www.arcanx-search.com https://preview.redd.it/8sefolufbxxd1.png?width=2988&amp;format=png&amp;auto=webp&amp;s=5a7e4ec3b2b1c5b92f75ea9a677c9102efcf58e9    提交人    /u/Limp-Run-9075   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfrasj/learn_rl_using_this_youtube_chat_tool/</guid>
      <pubDate>Wed, 30 Oct 2024 16:54:08 GMT</pubDate>
    </item>
    <item>
      <title>有没有论文研究过RL算法和优化器的关系？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfm7a3/are_there_any_papers_that_have_studied_the/</link>
      <description><![CDATA[我个人正在尝试很多简单的算法，我已经考虑这个问题一段时间了：adam 的动量可以帮助模型快速收敛，但当目标值发生变化时，这会有害吗，就像 RL 一样？如果大部分动量超出预测，并且目标网络复制了这种超出，则模型可能会发散。有人对此做过研究吗？    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfm7a3/are_there_any_papers_that_have_studied_the/</guid>
      <pubDate>Wed, 30 Oct 2024 13:13:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 实现的 PPO，学习动作分布的平均值和标准差 σ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfgwwq/ppo_implementation_in_python_with_learned_mean/</link>
      <description><![CDATA[我正在寻找有关 Python 软件包的推荐，这些软件包允许训练 PPO 代理，其中动作分布的平均值 (μ) 和标准差 (σ) 都是学习到的。 具体来说，我需要模型在给定当前观察的情况下在推理过程中输出动作和相应的标准差。这将帮助我评估代理对其预测的信心。 我见过关于 skrl 和 AgileRL 软件包的讨论，它们据称是模块化的。有人用过这些吗？他们也可以提供学习到的 σ 吗？ 任何建议或见解都将不胜感激。 谢谢。    提交人    /u/Disastrous_Effort725   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfgwwq/ppo_implementation_in_python_with_learned_mean/</guid>
      <pubDate>Wed, 30 Oct 2024 07:23:26 GMT</pubDate>
    </item>
    <item>
      <title>Meta FAIR CodeGen 团队的新 RL 实习机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gfam2x/new_rl_internship_at_meta_fair_codegen_team/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gfam2x/new_rl_internship_at_meta_fair_codegen_team/</guid>
      <pubDate>Wed, 30 Oct 2024 00:58:26 GMT</pubDate>
    </item>
    <item>
      <title>“Centaur：人类认知的基础模型”，Binz 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gf6nsf/centaur_a_foundation_model_of_human_cognition/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gf6nsf/centaur_a_foundation_model_of_human_cognition/</guid>
      <pubDate>Tue, 29 Oct 2024 21:55:42 GMT</pubDate>
    </item>
    <item>
      <title>开源火箭联盟环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gf3q6d/open_source_rocket_league_environment/</link>
      <description><![CDATA[        提交人    /u/compressor0101   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gf3q6d/open_source_rocket_league_environment/</guid>
      <pubDate>Tue, 29 Oct 2024 19:51:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 Memoroids 进行循环强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gekts9/recurrent_reinforcement_learning_with_memoroids/</link>
      <description><![CDATA[  由    /u/smorad  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gekts9/recurrent_reinforcement_learning_with_memoroids/</guid>
      <pubDate>Tue, 29 Oct 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>强化学习不仅能解决难题，还能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1geez5d/rl_solves_hard_problems_but_also/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1geez5d/rl_solves_hard_problems_but_also/</guid>
      <pubDate>Mon, 28 Oct 2024 22:21:13 GMT</pubDate>
    </item>
    <item>
      <title>期待明年开始攻读 RL 博士学位——寻求建议！🤞🏼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdyx82/looking_forward_to_start_phd_in_rl_next_year/</link>
      <description><![CDATA[大家好！我准备明年开始攻读强化学习博士学位，但需要一些关于积累研究经验和与教授联系的建议。  背景：我做过很多实际的强化学习项目（MARL、PettingZoo、策略梯度工作），但我缺乏正式的研究经验。我也有一年的 ML/LLM 实习经历，但寻找强化学习的研究实习机会一直很困难。 挑战：我担心接触 LOR，因为我没有直接与强化学习的教授合作过。任何关于有助于展示我的能力的项目建议或接触技巧都将非常有帮助。另外，如果有人有兴趣合作，请直接发信息给我！ &lt;3  任何见解都值得赞赏 - 谢谢大家！    提交人    /u/cheenchann   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdyx82/looking_forward_to_start_phd_in_rl_next_year/</guid>
      <pubDate>Mon, 28 Oct 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>哪些 RL 算法适用于计算心理学？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdytbm/which_rl_algorithms_for_computational_psychology/</link>
      <description><![CDATA[我是一名数据科学家，希望通过多个代理模拟人类社交互动。我只是想知道是否有人可以指点一下要探索哪些算法。例如，如果我想鼓励反馈循环和突发行为以更真实地描述人类行为，我应该使用无模型算法还是基于模型的算法？ 从我最初的研究中，我听说了关于 Decision Transformer 和 DreamerV3 的好评。 感谢您的时间！    提交人    /u/culturedindividual   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdytbm/which_rl_algorithms_for_computational_psychology/</guid>
      <pubDate>Mon, 28 Oct 2024 10:33:59 GMT</pubDate>
    </item>
    </channel>
</rss>