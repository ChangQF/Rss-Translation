<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 28 Aug 2024 06:22:50 GMT</lastBuildDate>
    <item>
      <title>混合动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f32pe1/hybrid_action_space/</link>
      <description><![CDATA[嗨，我在选择混合动作空间的正确算法时遇到了问题。通常，在许多研究论文中，他们提到混合动作空间是一种分层情况，即当您选择一个离散动作，然后根据离散动作选择一个连续动作时。但是，在我的项目中，一个动作需要 2 个参数，它们是离散的和连续的。它们彼此独立，但都会影响状态、奖励。因此，我想问一下我应该将其分成 2 个代理还是有算法可以解决这个问题？非常感谢！    提交人    /u/Fish_Chandle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f32pe1/hybrid_action_space/</guid>
      <pubDate>Wed, 28 Aug 2024 05:46:47 GMT</pubDate>
    </item>
    <item>
      <title>学习环境模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2wew8/learning_environment_model/</link>
      <description><![CDATA[你好。由于其样本效率，我想尝试基于模型的 RL。 但是，当我尝试在玩具环境中学习一个模型时，该模型具有大小为 51 的 1d 向量输入和大小为 10 的输出，该模型很难学习。该模型接收当前观察，然后采取行动预测下一个观察、奖励和终止标志。 观察和行动在 0~1 之间。但模型的 L2 误差从 0.1 下降得太慢了。它正在学习。但太慢了！ 这很奇怪，因为使用 td3 快速学习了一个好的策略。 有人可以分享他们在基于模型的 RL 方面的经验或一些好的材料吗？谢谢！    提交人    /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2wew8/learning_environment_model/</guid>
      <pubDate>Wed, 28 Aug 2024 00:08:30 GMT</pubDate>
    </item>
    <item>
      <title>Bandit Learning 研究/行业机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2t9qi/bandit_learning_research_industry_opportunities/</link>
      <description><![CDATA[大家好，我开始学习 bandit 学习的基础知识，但我真的很关心这个领域的实际应用或就业市场。这是一个有趣的领域，但到目前为止，我不知道这个领域的求职“关键词”是什么，不知道是否要继续？    提交人    /u/math--lover   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2t9qi/bandit_learning_research_industry_opportunities/</guid>
      <pubDate>Tue, 27 Aug 2024 21:49:57 GMT</pubDate>
    </item>
    <item>
      <title>在口袋妖怪绿宝石中击败第三个健身房</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2pvtl/beating_third_gym_in_pokemon_emerald/</link>
      <description><![CDATA[        提交人    /u/nicimunty   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2pvtl/beating_third_gym_in_pokemon_emerald/</guid>
      <pubDate>Tue, 27 Aug 2024 19:28:54 GMT</pubDate>
    </item>
    <item>
      <title>“多镜头情境学习”，Agarwal 等人，2024 年 {G}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2inwj/manyshot_incontext_learning_agarwal_et_al_2024_g/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2inwj/manyshot_incontext_learning_agarwal_et_al_2024_g/</guid>
      <pubDate>Tue, 27 Aug 2024 14:36:54 GMT</pubDate>
    </item>
    <item>
      <title>C# 深度强化学习比 sb3 快 300 倍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2ifcf/c_deep_reinforcement_learning_300_times_faster/</link>
      <description><![CDATA[        提交人    /u/asieradzk   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2ifcf/c_deep_reinforcement_learning_300_times_faster/</guid>
      <pubDate>Tue, 27 Aug 2024 14:27:06 GMT</pubDate>
    </item>
    <item>
      <title>如何将我自定义的凉亭世界导入健身房进行强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2clog/how_do_i_import_my_custom_gazebo_world_to_gym_for/</link>
      <description><![CDATA[我是强化学习的新手，想尝试一下我的自定义世界。但是我找不到将我的世界导入健身房的方法。我该如何将我的 Gazebo 环境导入健身房？我的 Gazebo 世界包含 urdf、world 和 launch 文件，这是带有 Gazebo 环境的 git 存储库。我使用的是带有 Gazebo 11 和 ros1 的 ubuntu 20.04。    提交人    /u/brian22lee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2clog/how_do_i_import_my_custom_gazebo_world_to_gym_for/</guid>
      <pubDate>Tue, 27 Aug 2024 09:15:17 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的反作弊系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</link>
      <description><![CDATA[我对创建基于 ML 的国际象棋反作弊系统很感兴趣。您知道任何关于使用 ML 方法进行反作弊的论文吗？我正在寻找概念。但到目前为止找不到任何类似的东西。有很多困难：例如，数据集中作弊者的例子很少，但公平玩家数据很多。在公平玩家数据上使用离线 RL 算法来预测异常行为是否有意义？    提交人    /u/HimitsuNoShougakusei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</guid>
      <pubDate>Tue, 27 Aug 2024 08:49:53 GMT</pubDate>
    </item>
    <item>
      <title>未获得官方支持的多代理 PPO 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f25yg9/multiagent_ppo_environment_without_official/</link>
      <description><![CDATA[问题陈述： 我想使用 gym 和 sb3 创建一个多智能体环境。目前，我有一个自定义的 Boid 群集环境，其中有一个神经网络，它采取所有动作并产生输出，即来自 sb3 的 PPO。 我想要实现的目标：我希望每个 boid 都有自己的演员评论家，以分散的方式执行，但 gym 默认不支持多智能体。我该如何实现呢？ 我正在使用 PPO，并希望有效地完成 MAPPO 所做的事情。集中训练，分散执行 (CTDE)。    提交人    /u/Ok-Teacher208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f25yg9/multiagent_ppo_environment_without_official/</guid>
      <pubDate>Tue, 27 Aug 2024 02:13:01 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f257ww/rl_for_manipulation/</link>
      <description><![CDATA[大家好！我正在深入研究强化/模仿学习和远程操作。我主修机械工程，在运动强化学习研究方面有经验，并且了解现代控制理论/ML/DL（我的理论理解还有待提高，但我都参加了研究生课程）。期待建议！我希望得到建议的论文、资源或开源项目，以便我可以提高我的整体理解。谢谢    提交人    /u/Sea-Hovercraft4777   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f257ww/rl_for_manipulation/</guid>
      <pubDate>Tue, 27 Aug 2024 01:36:44 GMT</pubDate>
    </item>
    <item>
      <title>“利用精选数据的自消费生成模型可证明优化人类偏好”，Ferbach 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f204co/selfconsuming_generative_models_with_curated_data/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f204co/selfconsuming_generative_models_with_curated_data/</guid>
      <pubDate>Mon, 26 Aug 2024 21:42:35 GMT</pubDate>
    </item>
    <item>
      <title>如何处理各种输出类型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1zsbd/how_to_deal_with_various_output_types/</link>
      <description><![CDATA[Sup r/Reinforcementlearning, 考虑一下深度强化学习问题：  您有一个上下文 您面前有一个经典的 10 臂老虎机 老虎机不做任何事情（0 奖励），但是当代理拉动它们时，每个老虎机都会输出一个带有浮点数、布尔输入字段（有时还有更多老虎机）的“表单”。每种形式都不同，包含 1-4 个输入。  因此，有不同的输出 - 首先是老虎机问题，然后是连续的数据输入，最好用单个步骤填充。 神经网络如何在不同的步骤中输出不同的输出格式？可以吗？ 干杯。（也许这是学位课程中教授的内容，但我是自学的。）    提交人    /u/JustZed32   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1zsbd/how_to_deal_with_various_output_types/</guid>
      <pubDate>Mon, 26 Aug 2024 21:28:14 GMT</pubDate>
    </item>
    <item>
      <title>在状态动作节点之后有多个分支指向相同状态但不同的奖励，这样有备份图合法吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1wt93/is_it_legit_to_have_a_backup_diagram_where_after/</link>
      <description><![CDATA[我正在阅读 sutton 和 barto 的 RL 书，当引入备份图的概念时，有一个隐含的假设，即在我们在状态“s”中执行动作“a”之后，离开 a-s 节点的所有分支都指向每个可能的下一个状态（每个状态都有一个奖励 r），但是当你可以从 s 转到 ŝ，动作为 a1，但有多个可能的奖励（假设为 r1 和 r2）时会发生什么？在这种情况下，备份图会是什么样子？ 我猜是这样的，但我不确定  s |_a1 | |_r1_ŝ | |_r2_ŝ | ... | |_r__s&#39; | |_a2     提交人    /u/samas69420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1wt93/is_it_legit_to_have_a_backup_diagram_where_after/</guid>
      <pubDate>Mon, 26 Aug 2024 19:22:26 GMT</pubDate>
    </item>
    <item>
      <title>迷失于现实</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1klw6/lost_in_rl/</link>
      <description><![CDATA[大家好， 我一直在深入研究强化学习，但最近我感觉有点不知所措，不确定自己的方向。这个领域很广阔，很难看到一条清晰的前进道路。 我开始怀疑自己的选择，担心自己的努力可能徒劳无功。我对强化学习充满热情，但不确定性正在造成影响。 还有其他人有这种感觉吗？你克服过类似的挑战吗？如果是这样，你用什么策略来保持动力和专注力？    提交人    /u/Eng-Epsilon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1klw6/lost_in_rl/</guid>
      <pubDate>Mon, 26 Aug 2024 10:11:08 GMT</pubDate>
    </item>
    <item>
      <title>过山车大亨之旅中的列车特工</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</link>
      <description><![CDATA[嗨！ 我只是想分享我的项目，我尝试创建一个代理，在 Rollercoaster Tycoon 中构建自定义轨道，并使用游戏自己的乘坐评级作为适应度函数来训练代理构建最好的过山车。 目前，它使用 UI 单击按钮并读取游戏屏幕，这非常缓慢且脆弱，但如果它显示出任何实际学习某些东西的迹象，我将尝试为 OpenRCT2 创建一个插件，公开代理可以使用的真实 API 而不是 UI。然后应该有可能在无头模式下运行游戏，并且使用 API 和同时在多个 OpenRCT2 实例上更快地构建代理。 很高兴听到您的反馈，您可以在这里找到代码：https://github.com/ZerxXxes/openrct2_gym    提交人    /u/ZerxXxes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</guid>
      <pubDate>Mon, 26 Aug 2024 08:00:32 GMT</pubDate>
    </item>
    </channel>
</rss>