<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 10 Mar 2024 15:12:07 GMT</lastBuildDate>
    <item>
      <title>对于决策转换器和强化学习的未来持什么立场？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbavui/what_is_the_stance_on_decision_transformers_and/</link>
      <description><![CDATA[嗨， 这些天我正在研究决策转换器。  有争议的是，在试图找到最重要的论文时，我注意到强化学习领域似乎没有发生太多事情。我注意到研究的重点是优化 Transformer 和训练巨大的语言和视觉模型（被视为监督模型）。这是 RL 中的新大事吗？  强化学习的最新趋势是什么？  ​   由   提交/u/__Julia  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbavui/what_is_the_stance_on_decision_transformers_and/</guid>
      <pubDate>Sun, 10 Mar 2024 13:54:56 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    <item>
      <title>QLearning推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1basms8/qlearning_recommender_system/</link>
      <description><![CDATA[大家好，我是强化学习新手，我正在尝试在 Movielens ML-100k 数据集上使用 QLearning 制作强化学习推荐系统，我将状态定义为用户，将动作定义为电影，我正在尝试预测评分。我将评分文件转换为用户项矩阵，并用 0 填充 NA 值。 对于我正在检查的奖励函数如果评级可用并且我返回实际评级-预测评级，否则我只返回 0。 对于我使用 epsilon 贪婪的策略。 我的问题是我没有得到好的结果，我猜我的奖励功能需要工作。如果有人想看一下我的代码，那就是 python。谢谢 编辑：这是我的 Kaggle 笔记本 https://www.kaggle.com/code/asribachir/reinforcement-learning-ml100k    由   提交 /u/Fredybec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1basms8/qlearning_recommender_system/</guid>
      <pubDate>Sat, 09 Mar 2024 21:23:51 GMT</pubDate>
    </item>
    <item>
      <title>模型性能评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ba8tq6/model_performance_evaluation/</link>
      <description><![CDATA[我在做什么： 我正在制作一个自定义Open AI Gym 中的 Boid 植绒环境具有稳定的基线 3. 工作原理：  我传递了一个 boids 的位置文件。 对其进行 3000 个时间步长的模型测试，并输出每个情节的奖励，即位置文件  训练初始位置与测试不同. 我关心的是我的模型的性能。当从不同的初始位置陈述时，它会输出类似的奖励，并且机器人按照预期移动，我还生成了一个移动视频文件。 看似正确工作的模型的输出 ​  奖励函数 defcalculate_combined_reward(self,agent,neighbor_positions):total_reward=0out_of_flock=False if(len(neighbor_positions)&gt;0):forneighbor_positionsinneighbor_positions : 距离 = np.linalg.norm(agent.position - neighbour_position) if (距离  问题：但是，当我在不更改任何内容的情况下重新训练几次并进行测试时，只是为了保持一致性，它的表现不佳，我的剧集奖励大多是负面的。虽然我什么也没改变。幸运的是，我保存了具有最佳性能的模型。 关注：是我的模型，我训练并输出了正确的性能，我附上了照片，一个训练好的模型，侥幸或过度拟合？  ​   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ba8tq6/model_performance_evaluation/</guid>
      <pubDate>Sat, 09 Mar 2024 04:18:30 GMT</pubDate>
    </item>
    <item>
      <title>大家好！我将从事旨在使用 RL 算法稳定无人机的模拟项目，我想过使用 matlab 和 simulink，但我找不到要测试的模型，任何人都可以指导我吗？谢谢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9zscg/hi_everyone_im_gonna_work_on_simulation_project/</link>
      <description><![CDATA[ 由   提交/u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9zscg/hi_everyone_im_gonna_work_on_simulation_project/</guid>
      <pubDate>Fri, 08 Mar 2024 21:26:25 GMT</pubDate>
    </item>
    <item>
      <title>在覆盖重播缓冲区时是否存在优先考虑陈旧内存的范例？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9qxhk/is_there_a_paradigm_on_which_stale_memories_to/</link>
      <description><![CDATA[我开始思考，因为我发誓范例总是循环先进先出，但对于我正在研究的后进先出的实现似乎工作得更好。  另外，在优先体验重放的情况下，为什么不丢弃最低的 TD 内存呢？  或者当缓冲区满时随机更换内存？    由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9qxhk/is_there_a_paradigm_on_which_stale_memories_to/</guid>
      <pubDate>Fri, 08 Mar 2024 15:36:40 GMT</pubDate>
    </item>
    <item>
      <title>🚀 DIAMBRA 与 Hugging Face 合作推动强化学习研究和采用！ 🚀</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9nw82/diambra_teams_up_with_hugging_face_to_push/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9nw82/diambra_teams_up_with_hugging_face_to_push/</guid>
      <pubDate>Fri, 08 Mar 2024 13:27:03 GMT</pubDate>
    </item>
    <item>
      <title>问题：关于单环境与多环境 RL 训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9le2c/question_regarding_single_environment_vs_multi/</link>
      <description><![CDATA[大家好， 我正在研究机械臂模拟，以对机器人执行高级控制以抓取物体。我正在使用 Unity 中的 ML Agent 作为环境平台。同时，使用 PPO 训练机器人，我能够在大约 8 小时的训练时间内成功执行它。为了减少时间，我尝试增加在同一环境中工作的代理数量（有一个内置的训练区域复制器，它只是使用代理复制整个机器人单元）。根据mlagents源代码，多个代理应该只是加速轨迹收集（因为有许多代理根据相同的策略尝试针对不同随机情况的操作，所以更新缓冲区应该更快地填满）。但是，由于某种原因，我的策略无法正确训练。它在零回报处持平（从 - 1 开始改善，但稳定在 0 左右。+1 是一个情节的最大回报）。增加代理数量时是否需要进行一些特定的更改？增加环境数量时需要记住的其他一些事项。欢迎任何意见或建议。提前致谢。    由   提交 /u/Flaky-Drag-31   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9le2c/question_regarding_single_environment_vs_multi/</guid>
      <pubDate>Fri, 08 Mar 2024 10:56:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 ray 实现自定义 RL Agent</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9kqh8/implement_custom_rl_agent_with_ray/</link>
      <description><![CDATA[嘿，我，目前正在尝试使用 ray (tune.trainable) 实现我自己的 RL 代理，并使用基于群体的训练调度程序来训练它们。根据文档，有 6 种方法可以实现： https://docs.ray.io/en /latest/tune/api/trainable.html 出现了许多疑问和问题。如果有人能为我回答，我将不胜感激。 我的问题是，每次在训练期间加载检查点时都会调用 setup() 方法，而不仅仅是在开始之后（我设置了reuse_agent 标志）在 Tuner 配置中，并且 reset?config() 方法返回 true）。此外，我不确定在 cleanup() 方法中要做什么。 step() 方法应该在环境中执行一步，还是在纪元上执行一步？并且重播缓冲区应该在不同的代理之间共享，或者每个代理应该有自己的重播缓冲区。 如果有人已经实现了自定义光线可训练，我将不胜感激 github 链接:) ​   由   提交 /u/ChiefAlu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9kqh8/implement_custom_rl_agent_with_ray/</guid>
      <pubDate>Fri, 08 Mar 2024 10:09:41 GMT</pubDate>
    </item>
    <item>
      <title>修补强化学习所需的知识水平</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b90c96/level_of_knowledge_needed_to_tinker_with_rl/</link>
      <description><![CDATA[我是普通的 3D 打印机/机器人修补匠，从我所看到的 RL 的外观来看，它不仅有趣而且有趣。但当我浏览这里的帖子时，它们都写得超出了我的理解范围。当我在网上查看“如何进入……”的内容时，里面充满了让我延伸很远的概念。对我来说，显然我不知道这一切的背景是什么。这是您非常需要学位或大学水平的教育和理解才能开始的事情吗？我发现很多涉及强化学习的例子都是人们的课程项目。我相信你可以做你想做的事，但我希望知道我可能需要付出多大的努力，以及这对我来说是否值得。你的背景是什么？   由   提交 /u/UltimateThrowawayNam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b90c96/level_of_knowledge_needed_to_tinker_with_rl/</guid>
      <pubDate>Thu, 07 Mar 2024 17:30:27 GMT</pubDate>
    </item>
    <item>
      <title>不同数量的动作元素和潜在的大离散动作空间......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8w0x1/varying_number_of_action_elements_and_potentially/</link>
      <description><![CDATA[您好， 作为深度强化学习的新手，我正在寻求有关以下问题的建议。请随时对整体设置提出一般性评论，因为我可能无法识别核心问题... 我的空间环境的状态是由不同的数字定义的（n）个元素，每个元素都有一个标量属性 d 以及相应的 x 和 y 坐标。请注意，n 的上限没有严格限制，而是受到环境动态的“软限制”。在每个时间步，代理可以决定删除哪些元素：“杀死”一个元素会生成奖励信号，并根据剩余元素的数量计算下一个状态。 当前形式，此设置有两个主要问题： (a) 状态空间和动作空间的大小各不相同，具体取决于当前存在的元素数量； (b) 取决于在当前的元素数量上，操作空间的大小可能相当大（在 100 个元素的情况下为 2^100） 我正在寻求通用方法的建议，因为我的第一个天真尝试没有成功。 如何处理a）不同数量的元素和b）（可能）大量离散操作？ 到目前为止我的想法 - 小心，也许是废话：  关于a）我尝试将网络的输入节点数量（到目前为止我使用了PPO）设置为某个固定值（比如100），并将参与者输出的数量设置为相同的值，并根据某些固定规则（例如按属性“d”排名）将元素分配给输入和输出节点。这涉及到如果 n 超过 100，则从决策和推理中排除元素，以及在 n &gt; 100 的情况下用“伪 NA”“填充”空输入节点。 100. 我可以通过这种方法获得一些学习成果，但最终的政策远非最佳。我认为这种情况下的问题确实是 b)，据我所知，我需要一些能力来概括，不仅对状态，而且对行动。我读过 Dulac-Arnold 等人，觉得这种方法可能有所帮助，但是我不知道如何构建我的动作空间的方式允许有效地将标量、连续的原始动作映射到一组+-相似的离散动作...    ;由   提交 /u/ionatura   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8w0x1/varying_number_of_action_elements_and_potentially/</guid>
      <pubDate>Thu, 07 Mar 2024 14:29:53 GMT</pubDate>
    </item>
    <item>
      <title>模拟 5G 环境中的深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8qhe3/deep_rl_in_simulated_5g_environment/</link>
      <description><![CDATA[我目前正在撰写硕士论文，目的是在简单的模拟 5G 环境中创建基于 AI 的自适应软件。我正在运行由发送节点和接收节点组成的模拟。请记住，模拟应该很简单，因为模拟不是论文的主要部分。发送方向接收方发送一批数据包，然后接收方检查丢失了多少数据包以及发送数据包的负载（网络拥塞）。 我正在使用带有连续操作空间的 Gym介于 -1 和 1 之间，表示代理可以更改带宽的值。 -1 和 1 是 -50 到 50 的标准化值，因此 -1 的操作会以 50 Mbit/s 的速度减少带宽，而 1 则会以 50 Mbit/s 的速度增加带宽，依此类推。观察空间由三个维度组成，其中第一个维度是 0 到 1 之间的带宽（范围 1-50 的标准化值），第二个维度是当前批次发送的负载，第三个维度是之前的负载。获取当前负载和先前负载的原因是为了查看负载是增加还是减少。我正在考虑只有二维，带宽为一，前一个和当前负载的导数为第二个，但我还没有测试过。代理只能更改其带宽值，因为它不能影响负载或之前的负载。 主要目标是减少丢失数据包的总量并减少发送数据包所需的总时间。一定数量的数据包。我希望代理在负载低时增加带宽，在负载高时减少带宽。这是因为高负载意味着更高的数据包丢失率，因此我们不希望在大多数数据包丢失时发送大量数据包。尽管带宽较低，速度会较慢，但我们会丢失较少数量的数据包。 我正在使用和比较的模型是 PPO 和 DDPG，我遇到的问题是创建一个合适的奖励函数，平衡最小化数据包丢失和最小化总时间之间的比率。我已经取得了一些进展，代理知道在负载非常高的情况下选择低带宽，在负载非常低的情况下选择高带宽，但它在两者之间感到困惑。如果有人知道导出奖励函数的好方法，我将不胜感激。我的方法是对奖励函数进行有根据的猜测，但它们在平衡我的问题方面还不够好   由   提交 /u/axeljnsson   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8qhe3/deep_rl_in_simulated_5g_environment/</guid>
      <pubDate>Thu, 07 Mar 2024 09:27:41 GMT</pubDate>
    </item>
    <item>
      <title>是否有一种具有多种输出类型的强化学习算法？ （Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8onq6/is_there_an_rlalgorithm_with_multiple_types_of/</link>
      <description><![CDATA[嘿大家 我正在尝试用游戏 CS2 做一些实验项目(相关帖子），我想知道是否有任何 RL 算法可以输出多种类型。 例如：对于移动，我们使用 float，对于射击/攻击，我们使用 int(0: 不攻击 / 1: 攻击) 例如输出如下：[float, float, int]  有可能有这样的东西吗？是否可以将其作为健身房环境？   由   提交 /u/Mr_Lucifer_666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8onq6/is_there_an_rlalgorithm_with_multiple_types_of/</guid>
      <pubDate>Thu, 07 Mar 2024 07:29:16 GMT</pubDate>
    </item>
    <item>
      <title>负解释方差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8mjz4/negative_explained_variance/</link>
      <description><![CDATA[   https://wandb .ai/kingsignificant5097/uncategorized?workspace=user-kingsignificant5097 https://preview.redd.it/wqbzrf6dgumc1.png?width=3516&amp;format=png&amp;auto=webp&amp;s=eacf2d106283d3bc2143a9a9e3f7f68f5 bcbdb67 我一直在致力于一个项目，试图亲身体验一些强化学习。 我正在建模的环境是一个时间序列，我正在训练一个代理通过交易金融衍生品来最大化财务回报，特别是杠杆永续期货。因此，它需要最大化财务收益并最小化财务损失。每当代理平仓时都会给予奖励，奖励与相对于初始投资的收益/损失百分比成正比。 动作空间是一个具有 4 个动作的多离散空间，不执行任何操作，开仓持仓（以及要使用的资金百分比）、平仓、结束事件。观察空间约为 100 个，包含当前开盘/收盘/交易量和一些财务指标（技术分析）以及某些历史时期的聚合指标。 使用的算法是 dd-ppo（56 个 cpu） ）使用 rllib 进行一些修改，特别是使用实现操作屏蔽的自定义模型。例如，确保代理只有在有仓位存在时才可以平仓，并且只有在没有持仓并且总收益或损失&gt; 1时才能结束一个episode。 （某些％），或者总剧集长度大于某些步数。 我还使用 具有 8 个头和 4 个变压器的注意力网络，并使用 RE3  用于探索。 通过以上所有内容，我能够获得良好的结果，并且代理似乎确实在学习，基于平均奖励和基于针对未见数据检查点的实证测试。  但是，在查看 TensorBoard 指标时，我看到了一些对我来说没有多大意义的事情，因此我在这里寻求任何人的帮助或建议： &lt; ol&gt; 是什么导致解释方差为负？我应该预计这个数字会开始增加吗？鉴于奖励均值在如此复杂的环境中不断增加，这样的负方差有何意义？ 为什么熵并没有真正下降？这很重要吗？ 为什么损失继续缓慢增加？我应该期望这个值会在某个时候开始下降吗？这是代理仍在学习的标志吗？  提前感谢您的指点！ 一些修改的参数，默认值来自 rllib for ppo 和 dd-ppo 覆盖： &lt;代码&gt;地平线 = 60 .training( train_batch_size = 地平线 * 4, sgd_minibatch_size = 地平线 * 2, num_sgd_iter = 2, gamma = 1.0, lambda_ = 1.0, entropy_coeff = 0.001, grad_clip = 10, model = { “fcnet_hiddens”: [1024 ，1024]，“max_seq_len”：地平线，“attention_num_transformer_units”：4，“attention_num_heads”：8，}） &lt;！-- SC_ON --&gt;   ;由   提交 /u/KingSignificant5097   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8mjz4/negative_explained_variance/</guid>
      <pubDate>Thu, 07 Mar 2024 05:29:18 GMT</pubDate>
    </item>
    <item>
      <title>测试时无法扩展自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxow/cant_scale_custom_environment_when_testing/</link>
      <description><![CDATA[我正在制作自定义Open AI Gym 中的 Boid 植绒环境具有稳定的基线 3.  ​ 错误1： 我已经创建了环境并测试了3个boids，但是当我测试10个boids时，它给出了错误： 错误 自定义环境 我从未对 boids 的数量进行硬编码，也没有其他明显的问题。 ​ 错误 2： 我用 20 个不同的初始化位置测试了模型，但对于大多数情况，我得到了这个奖励，600200，他们按预期移动。  但是，当我重新训练模型时，它的表现不佳，参数完全相似。这是一个训练有素的模型，是侥幸还是过度拟合？  剧集奖励  祝你有美好的一天！   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxow/cant_scale_custom_environment_when_testing/</guid>
      <pubDate>Thu, 07 Mar 2024 04:56:12 GMT</pubDate>
    </item>
    </channel>
</rss>