<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 15 Nov 2024 18:23:16 GMT</lastBuildDate>
    <item>
      <title>帮帮我2帮你：你的流程中哪个部分最耗时？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gs2oq4/help_me_2_help_you_what_part_of_your_process/</link>
      <description><![CDATA[大家好，我是 Mr. For Example，Comfy3D 的作者，由于全世界的研究人员都没有得到足够的支持来开展他们开创性的工作，因此我考虑构建一些工具来帮助研究人员节省时间和精力 因此，对于所有研究人员、科学家和工程师来说，在研究过程中，以下哪个步骤最耗费您的时间或让您付出最大的努力？ 查看投票    提交人    /u/MrForExample   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gs2oq4/help_me_2_help_you_what_part_of_your_process/</guid>
      <pubDate>Fri, 15 Nov 2024 18:10:42 GMT</pubDate>
    </item>
    <item>
      <title>电能质量强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grzcow/reinforcement_learning_for_power_quality/</link>
      <description><![CDATA[我正在使用 actor-critic DQN 解决多微电网系统中的电能质量问题。我的神经网络没有收敛，似乎在采取随机行动。有没有人可以和我通电话，讨论一下这个问题，了解我哪里做错了？刚开始研究机器学习，认为自己是这个领域的新手。 谢谢    提交人    /u/theguywithyoda   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grzcow/reinforcement_learning_for_power_quality/</guid>
      <pubDate>Fri, 15 Nov 2024 15:50:10 GMT</pubDate>
    </item>
    <item>
      <title>就状态值函数而言的动作值函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grv082/actionvalue_function_in_terms_of_state_value/</link>
      <description><![CDATA[      https://preview.redd.it/bs4ena7c421e1.png?width=637&amp;format=png&amp;auto=webp&amp;s=b7625cb7538e6ead3183e8f14cbd0acd613581f5 我正在阅读 Sutton&amp;Barto 的书。我在练习 3.13 处卡住了。问题是用 vπ 和 p(s′,r∣s,a) 写出 qπ。我追踪了上述步骤。我怎样才能从那里继续，或者我的逻辑是正确的？     提交人    /u/demirbey05   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grv082/actionvalue_function_in_terms_of_state_value/</guid>
      <pubDate>Fri, 15 Nov 2024 12:14:08 GMT</pubDate>
    </item>
    <item>
      <title>具有动态预测辅助任务的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grsrs5/ppo_with_dynamics_prediction_auxiliary_task/</link>
      <description><![CDATA[嘿，找不到任何关于它的文章。是否有人尝试或知道有关将 ppo 与奖励预测或动态预测等辅助任务结合使用的文章，以及它是否会提高性能？（纯粹是 ppo 训练方式，而不是梦想家风格） 编辑：我知道 2016 年关于辅助任务的文章，但想知道是否还有更多与 ppo 相关的内容     提交人    /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grsrs5/ppo_with_dynamics_prediction_auxiliary_task/</guid>
      <pubDate>Fri, 15 Nov 2024 09:38:15 GMT</pubDate>
    </item>
    <item>
      <title>RL 角色的面试流程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grrdo5/interview_process_for_rl_roles/</link>
      <description><![CDATA[大家好！ 目前我是一名大四本科生，我也在一个机器人实验室工作，我们主要在那里使用 RL。我最初是因控制理论/朴素 ML 而被聘用的，后来被调到了 RL 团队。 从明年开始，我将在 RL 领域寻找工作或攻读博士学位的机会，我想知道这种类型的职位需要经历什么样的面试过程。它是否类似于 ML/SWE 职位，需要进行几轮技术面试和分配任务，还是完全不同？ 此外，我目前在中等影响力的会议和期刊上发表了几篇论文。那么对于攻读博士学位的机会，我应该尝试至少在高影响力的期刊上发表文章还是谨慎行事？ 谢谢你的帮助🙏    提交人    /u/oz_zey   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grrdo5/interview_process_for_rl_roles/</guid>
      <pubDate>Fri, 15 Nov 2024 07:52:09 GMT</pubDate>
    </item>
    <item>
      <title>旋转起来</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grlksx/spinning_up/</link>
      <description><![CDATA[这对我更好地理解 RL 来说是一个很好的起点吗？ https://spinningup.openai.com/en/latest/user/introduction.html#what-this-is    提交人    /u/iconic_sentine_001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grlksx/spinning_up/</guid>
      <pubDate>Fri, 15 Nov 2024 02:01:58 GMT</pubDate>
    </item>
    <item>
      <title>Yann LeCun 仍然不认为 RL 对 AI 系统至关重要。他认为只有无监督/监督学习/SSL 算法才能处理 RL 所用到的问题类型，例如顺序决策，或者它们将如何处理诸如探索之类的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</link>
      <description><![CDATA[    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1grk1lb/yann_lecun_still_doesnt_see_rl_as_being_essential/</guid>
      <pubDate>Fri, 15 Nov 2024 00:44:11 GMT</pubDate>
    </item>
    <item>
      <title>Catan AI 所需的规格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gra5pz/specs_needed_for_catan_ai/</link>
      <description><![CDATA[嗨，我正在训练一个人工智能来玩 Catan 棋盘游戏，我需要一些建议，关于进行训练的计算机需要什么规格。我可能会使用 pytorch 或 tenserflow 进行训练。有什么想法吗？ 我考虑租用虚拟机进行训练，你推荐吗？    提交人    /u/FunMetJoel   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gra5pz/specs_needed_for_catan_ai/</guid>
      <pubDate>Thu, 14 Nov 2024 17:29:53 GMT</pubDate>
    </item>
    <item>
      <title>有人有 DreamerV3 实现吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gr5ixz/anybody_has_a_dreamerv3_implementation/</link>
      <description><![CDATA[大家好 r/reinforcementlearning， 我正在尝试使用 DreamerV3 模型，这是迄今为止性能最高的 RL 模型。 问题是，它的代码是一个自我实现的半 Jax 半 numpy 半 python 操作；有自定义线程管理（使用 Jax 时），以及大多数开箱即用的 ML 库支持的许多其他代码。它很难使用。 有人有 jittable jax 实现吗？我有一个用 Jax 编写的环境，因此对其进行研究完全有意义，其他许多研究人员也是如此。 也许有人可以分享/开源他们的实现？ 干杯。    提交人    /u/JustZed32   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gr5ixz/anybody_has_a_dreamerv3_implementation/</guid>
      <pubDate>Thu, 14 Nov 2024 14:06:52 GMT</pubDate>
    </item>
    <item>
      <title>基于脉冲神经网络的强化学习模型性能提升建议 [P] [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gr2yyl/advice_for_improving_the_performance_of_my/</link>
      <description><![CDATA[大家好！我正在开展一个项目，专注于使用脉冲神经网络 (SNN) 训练强化学习代理。我的目标是提高模型的性能，尤其是通过“做梦”体验（离线训练）有效学习的能力。 简要项目背景（基于模型的强化学习）： 代理与环境（游戏 Pong）交互，在主动训练阶段（“清醒”）和离线学习的“做梦”阶段之间交替。 我面临的挑战： 学习速度很慢，而且有些不稳定。我尝试了一些优化，但仍然没有达到理想的性能。具体来说，我注意到增加网络（代理和模型）中的神经元数量并没有提高性能；在某些情况下，甚至会恶化。我降低了模型的学习率，但没有看到任何改进。我还通过在清醒阶段禁用学习来测试模型，以仅查看其在做梦阶段的行为。我发现模型在 1-2 个梦中有所改进，但当达到 3 个梦时性能会下降。 问题：  您是否知道任何可以在 SNN 环境中提高模型稳定性和收敛性的技术？ 您有什么建议或意见吗？ 使用重放缓冲区会有所帮助吗？     提交人    /u/Embri21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gr2yyl/advice_for_improving_the_performance_of_my/</guid>
      <pubDate>Thu, 14 Nov 2024 11:45:48 GMT</pubDate>
    </item>
    <item>
      <title>美国有哪些教授和实验室在机器人强化学习方面进行了出色的研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqzdjf/any_professors_labs_doing_good_research_in/</link>
      <description><![CDATA[我正在美国申请硕士学位，我不知道哪位教授或学院最适合机器人强化学习。    提交人    /u/Different_Prune_9756   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqzdjf/any_professors_labs_doing_good_research_in/</guid>
      <pubDate>Thu, 14 Nov 2024 07:14:53 GMT</pubDate>
    </item>
    <item>
      <title>有人成功实现了 DeepMind、INRIA 等论文《无模型强化学习中的反事实信用分配》吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqyuqs/anyone_found_success_implementing_the_paper/</link>
      <description><![CDATA[我一直在尝试实现这篇论文的连续动作版本 https://arxiv.org/pdf/2011.09464 。任何成功实现这项工作并愿意分享如何实现它的见解的人。我已经实现了这项工作的连续动作版本，但得到了好坏参半的结果，现在不确定我是否正确地实现了它。    提交人    /u/AvisekEECS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqyuqs/anyone_found_success_implementing_the_paper/</guid>
      <pubDate>Thu, 14 Nov 2024 06:37:06 GMT</pubDate>
    </item>
    <item>
      <title>真正从事 RL 研究员工作的人们，你们是如何获得这份工作的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gquq8o/people_who_really_work_as_an_rl_researcher_how/</link>
      <description><![CDATA[真正从事 RL 研究员工作的人（主要从事 RL 项目）。   您在哪里工作？ 您什么时候得到这份工作的？ 您的背景？  我的博士研究主要涉及 RL，但我现在以 MLE 的身份从事各种 ML/DL/RL 项目。我提交了几份申请，通过了最后的面试，但作为行业中的 RL 研究员，我还是不够格。 LinkedIn 上真正纯粹与 RL 相关的工作总是少于 30 个。 我想知道人们如何获得纯粹的 RL 研究员工作？    提交人    /u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gquq8o/people_who_really_work_as_an_rl_researcher_how/</guid>
      <pubDate>Thu, 14 Nov 2024 02:37:18 GMT</pubDate>
    </item>
    <item>
      <title>PPO 之后是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqr1k3/whats_after_ppo/</link>
      <description><![CDATA[我最近完成了从 PyTorch 实现 PPO 以及任何看似相关的实现细节（vec envs、GAE lambda）。我还做了少量的行为克隆 (DAgger) 和多智能体强化学习 (IPPO)。 我想知道是否有人对下一步该怎么做有指示或建议？也许你正在研究某些东西，对 PPO 的改进是我完全错过的，或者只是一篇有趣的读物。到目前为止，我的兴趣只是游戏 AI。    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqr1k3/whats_after_ppo/</guid>
      <pubDate>Wed, 13 Nov 2024 23:37:38 GMT</pubDate>
    </item>
    <item>
      <title>“当你的人工智能欺骗你时：从人类反馈进行强化学习的部分可观察性挑战”，Lang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gqibc6/when_your_ais_deceive_you_challenges_of_partial/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gqibc6/when_your_ais_deceive_you_challenges_of_partial/</guid>
      <pubDate>Wed, 13 Nov 2024 17:27:54 GMT</pubDate>
    </item>
    </channel>
</rss>