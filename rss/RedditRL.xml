<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 10 Nov 2024 12:29:08 GMT</lastBuildDate>
    <item>
      <title>语音合成中的微调与迁移学习 - INGOAMPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnz44s/finetuning_vs_transfer_learning_in_voice/</link>
      <description><![CDATA[        由    /u/Potential_Arrival326  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnz44s/finetuning_vs_transfer_learning_in_voice/</guid>
      <pubDate>Sun, 10 Nov 2024 11:59:21 GMT</pubDate>
    </item>
    <item>
      <title>语音合成中的微调与迁移学习 - INGOAMPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnz3ej/finetuning_vs_transfer_learning_in_voice/</link>
      <description><![CDATA[        由    /u/Potential_Arrival326  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnz3ej/finetuning_vs_transfer_learning_in_voice/</guid>
      <pubDate>Sun, 10 Nov 2024 11:57:59 GMT</pubDate>
    </item>
    <item>
      <title>刚刚发现了很棒的 Python 强化学习库！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gntun0/just_discovered_great_reinforcement_learning/</link>
      <description><![CDATA[大家好！ 我最近偶然发现了一个名为 DeepRL (deeprlearn) 的强化学习库，我认为值得一试。它专为中小型 RL 项目而设计，并提供一些简洁的功能，使其既灵活又可扩展。它基于 PyTorch，因此如果您已经将 PyTorch 用于其他项目，那么这应该会非常直观。 以下是它提供的一些亮点：  动态规划算法：开箱即用地实现价值迭代和策略迭代代理。 奖励塑造：带有内置策略，如基于潜力的塑造、基于距离的塑造等。非常适合像 FrozenLake 或 MountainCar 这样奖励稀疏的环境。 函数近似：包括用于 RBF 核、多项式特征甚至神经网络的工具（如果您想走这条路）。 Gymnasium 集成：与 Gym 环境无缝协作。 进度跟踪：一种方便的详细模式，用于监控训练奖励、步骤和探索率。 模型保存/加载：可以非常轻松地保存和重用经过训练的代理。  我一直在尝试它，到目前为止，我已经在 FrozenLake-v1（防滑）上训练了一个价值迭代代理。它出奇的简单，详细模式使监控进度变得容易。 该库仍在不断发展，但它对贡献持开放态度。如果您正在寻找用于 RL 实验的轻量级但功能强大的东西，那么这可能是一个不错的选择！ GitHub Repo：deeprl 如果您尝试过或有任何反馈，请告诉我。看到更多人尝试这个会很棒！    提交人    /u/Traditional-Rate8550   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gntun0/just_discovered_great_reinforcement_learning/</guid>
      <pubDate>Sun, 10 Nov 2024 05:46:16 GMT</pubDate>
    </item>
    <item>
      <title>安全、无幻觉的 AI 编码提案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnsq65/a_proposal_for_safe_and_hallucinationfree_coding/</link>
      <description><![CDATA[我写了一篇论文《安全且无幻觉的 AI 编码提案》(https://gasstationmanager.github.io/ai/2024/11/04/a-proposal.html)，其中我提议就一项研究议程开展开源合作，我相信这最终将导致编码具有超人级能力、无幻觉且安全的 AI。 强化学习，尤其是 AlphaZero，是我提出的解决方案的一部分。但 AlphaZero 通常在容易获得基本事实的领域效果很好，比如围棋和国际象棋……我提出了一种将代码生成问题表述为可以根据基本事实验证候选解决方案的方法。  欢迎发表评论！如果您有兴趣探索强化学习或该计划的其他方面的想法，请告诉我！     提交人    /u/Admirable_Sorbet_544   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnsq65/a_proposal_for_safe_and_hallucinationfree_coding/</guid>
      <pubDate>Sun, 10 Nov 2024 04:37:28 GMT</pubDate>
    </item>
    <item>
      <title>一些 RL 算法的分类</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnd86b/classification_of_some_rl_algorithms/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnd86b/classification_of_some_rl_algorithms/</guid>
      <pubDate>Sat, 09 Nov 2024 16:01:07 GMT</pubDate>
    </item>
    <item>
      <title>适用于 2 个以上代理的 AI 扑克健身房环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn9nux/ai_poker_gym_environment_for_more_than_2_agents/</link>
      <description><![CDATA[大家好，我是一名计算机科学专业的学生，​​我想为我的 AI 课程期末项目做一个 AI 扑克锦标赛。我的想法是让 4/5 个不同的代理都使用不同的 RL 算法进行训练，让它们互相玩扑克，看看谁赢。我找到了几个不同的扑克环境，但它们都是针对 2 个代理的。有人知道任何能够与 &gt;3 个代理一起工作的环境吗？任何帮助或建议如何使我的项目更好都将不胜感激。    提交人    /u/Livid-Ant3549   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn9nux/ai_poker_gym_environment_for_more_than_2_agents/</guid>
      <pubDate>Sat, 09 Nov 2024 13:03:55 GMT</pubDate>
    </item>
    <item>
      <title>改变观察形状不会扰乱 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn8bi5/will_changing_observation_shapes_not_mess_up_rl/</link>
      <description><![CDATA[您好 r/reinforcementlearning， 我编写应用程序已有一段时间了，基于一个假设：一个数组环境，其中形状每一步都会改变（由于模型的操作，完整的 MDP）不会弄乱模型编码器（我相信这是“对状态的理解”）。 状态名称本身不会改变，但在某一步会像这样： print(a.shape)= (10,20) 然后像这样： print(a.shape)=(10,22) 我正在使用 vanilla Dreamerv3。  您认为这听起来合理吗？ 干杯。    提交人    /u/JustZed32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn8bi5/will_changing_observation_shapes_not_mess_up_rl/</guid>
      <pubDate>Sat, 09 Nov 2024 11:42:48 GMT</pubDate>
    </item>
    <item>
      <title>帮助我实现研究论文的最佳模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn85vo/best_model_to_help_me_implement_a_research_paper/</link>
      <description><![CDATA[请为我提供最好的 LLM（免费或付费）来实现研究论文。由于聊天 GPT 4.0 不足以满足我的实现要求。我想在我的自定义环境中实现多智能体 td3 模型。需要一个聊天机器人来帮助我更快地实现它    提交人    /u/CurrentBoss9530   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn85vo/best_model_to_help_me_implement_a_research_paper/</guid>
      <pubDate>Sat, 09 Nov 2024 11:32:37 GMT</pubDate>
    </item>
    <item>
      <title>多Agent TD3模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn83ho/mulitagent_td3_model/</link>
      <description><![CDATA[有人能给出一个用于实现多代理 td3 模型的工作存储库的代码吗？无论是环境、自定义还是 gym 环境。我只希望它是一个工作模型，因为其中许多模型不起作用或不合法。    提交人    /u/CurrentBoss9530   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn83ho/mulitagent_td3_model/</guid>
      <pubDate>Sat, 09 Nov 2024 11:27:58 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该先将我的 RL 论文提交给 arXiv 以保护新颖性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn6y6f/should_i_submit_my_rl_paper_to_arxiv_first_to/</link>
      <description><![CDATA[大家好！ 我一直在努力改进强化学习算法，并且取得了一些不错的成果，很高兴与大家分享。在准备撰写论文时，我想知道在将其发送到机器学习期刊之前，是否最好先将其提交给 arXiv。我主要关心的是确保我的研究的新颖性得到保护，因为我听说在 arXiv 上发表文章有助于确定贡献的时间戳。 所以，我很想知道：  在强化学习研究中，先在 arXiv 上发表文章，然后再提交给期刊，这是否是一种常见的惯例？ 在 arXiv 上发表文章真的有助于保护研究的新颖性吗？  在向期刊提交文章之前，我有什么理由避免在 arXiv 上发表文章吗？  任何经历过这个过程或有 RL 出版经验的人的建议都会非常有帮助！提前谢谢了！😊    提交人    /u/Tonight223   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn6y6f/should_i_submit_my_rl_paper_to_arxiv_first_to/</guid>
      <pubDate>Sat, 09 Nov 2024 10:05:06 GMT</pubDate>
    </item>
    <item>
      <title>关于扩展 DRL 中的观察和动作空间的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn318r/advice_on_scaling_observation_and_action_spaces/</link>
      <description><![CDATA[大家好！我正在开展一个项目，训练深度强化学习 (DRL) 代理在不同的电网网络架构（例如 13 母线和 34 母线系统）中运行。我的目标是在较小的系统（如 13 母线）上训练代理，然后在较大的系统（如 34 母线）上测试它。但是，随着网络规模的扩大，观察和操作空间也会发生变化：例如，我对 13 母线系统的观察空间是 (17, 3)，但对于 34 母线系统，它变成了 (47, 3)。这种维度变化带来了挑战，因为我的当前模型（使用 Stable Baselines3 构建）捕获了观察空间，因此很难在不同规模上进行推广。 我的导师建议探索节点级图形网络来帮助解决这个扩展问题。我很好奇是否有人有以下方面的经验或建议：  在可变大小环境中扩展 DRL 的观察和操作空间的方法。 关于在强化学习中使用节点级图形网络实现可扩展性的相关论文或资源。 调整稳定基线 3（或替代库）以处理可变观察和操作空间的方法。  任何关于在不同规模环境中训练和测试 DRL 代理的见解都将非常有帮助。提前感谢您分享的任何建议或资源！    提交人    /u/wild_wolf19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn318r/advice_on_scaling_observation_and_action_spaces/</guid>
      <pubDate>Sat, 09 Nov 2024 05:26:29 GMT</pubDate>
    </item>
    <item>
      <title>帮助运行 Minigrid</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn1air/help_with_running_minigrid/</link>
      <description><![CDATA[大家好， 我正在尝试运行一些分层 RL 算法，并且一直在研究各种四室版本的健身房环境，偶然发现 https://minigrid.farama.org/environments/minigrid/FourRoomsEnv/ 但出于某种原因，健身房 1.0 似乎没有这些环境，有人成功逃离了这些微网格环境（使用健身房 1.0？）。 抱歉，如果这是重复的帖子，我确实尝试搜索它但没有成功，谢谢。 更新：我猜健身房 1.0 有这个问题，将其降级为健身房==0.29.0，一切正常很好    由   提交  /u/Nervous_Studio_7689   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn1air/help_with_running_minigrid/</guid>
      <pubDate>Sat, 09 Nov 2024 03:44:13 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉问题的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gmfx3u/reinforcement_learning_on_computer_vision_problems/</link>
      <description><![CDATA[大家好， 我是一名计算机视觉研究员，主要从事 3D 视觉任务。最近，我开始研究 RL，意识到许多视觉问题可以重新表述为某种策略或价值学习结构。进行和遵循这种重新表述是否有好处？是否有任何重要的工作取得了比监督学习更好的结果？    提交人    /u/Foreign-Associate-68   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gmfx3u/reinforcement_learning_on_computer_vision_problems/</guid>
      <pubDate>Fri, 08 Nov 2024 10:57:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我创建了一个可以自动优化你的 LLM 提示的 RL 代理！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm64gy/p_i_created_a_rl_agent_that_can_autooptimize_your/</link>
      <description><![CDATA[大家好！ 我和我的团队开发了一个自动优化 LLM 提示的系统，该系统具有可视化功能，可以跟踪提示结构和学习进度。 请看这里：https://nomadic-ml.github.io/nomadic/cookbooks/Nomadic_Prompt_Optimization_Report.html 也请查看我们的网站：Nomadic ML。 以下是 GIF 格式的预览！ https://i.redd.it/l57uimn7qkzd1.gif https://i.redd.it/ybkf74aaqkzd1.gif 关于此可视化的工作原理：RL Prompt Optimizer 采用强化学习框架来迭代改进用于语言模型评估的提示。在每一集中，代理都会根据状态表示选择一个操作来修改当前提示，该状态表示对提示的特征进行编码。代理会根据对模型响应的多指标评估获得奖励，从而鼓励开发能够引出高质量答案的提示。 查看我们的 github repo 并给我们一个星星！ https://github.com/nomadic-ml/nomadic    由   提交  /u/vnkn17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm64gy/p_i_created_a_rl_agent_that_can_autooptimize_your/</guid>
      <pubDate>Fri, 08 Nov 2024 00:48:51 GMT</pubDate>
    </item>
    <item>
      <title>您是否同意深度强化学习 (Deep RL) 目前正在经历 Imagenet 时刻这一观点？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gls0qm/do_you_agree_with_this_take_that_deep_rl_is_going/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gls0qm/do_you_agree_with_this_take_that_deep_rl_is_going/</guid>
      <pubDate>Thu, 07 Nov 2024 14:43:12 GMT</pubDate>
    </item>
    </channel>
</rss>