<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 25 Oct 2024 06:24:41 GMT</lastBuildDate>
    <item>
      <title>Decision Transformer 学习不正确</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gbntqe/decision_transformer_not_learning_properly/</link>
      <description><![CDATA[嗨，如果能得到一些帮助，让我的决策转换器能够用于离线学习，我将不胜感激。 我正在尝试对多周期混合问题进行建模，为此我创建了一个自定义环境。我有一个从线性求解器获得的 60k 个状态/动作对的数据集。我正尝试在数据上训练 DT，但训练速度极慢，损失仅略有减少。 我认为我的环境并不是特别困难，而且我在简单的环境中使用 PPO 获得了一些不错的结果。 有关更多上下文，这是我的 repo：https://github.com/adamelyoumi/BlendingRL；我在 DT 存储库中使用修改版的 experiment.py。 谢谢    提交人    /u/cheese_n_potato   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gbntqe/decision_transformer_not_learning_properly/</guid>
      <pubDate>Fri, 25 Oct 2024 06:24:15 GMT</pubDate>
    </item>
    <item>
      <title>关于离线多代理环境的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gbknms/advice_on_offline_multi_agent_environment/</link>
      <description><![CDATA[我在多代理环境中工作，并收集了每个代理的数据（我可以指定每个代理执行的操作）。这些数据是每个代理在某些步骤中采取的操作，但不是每个步骤。然后，假设我是过去采取过行动的代理之一，之后完全忘记了我的策略。问题是我如何学习我之前的策略？我想知道为什么我在那个特定时刻采取了这些行动。（我的代理内部“状态”） 也许一种方法是使用监督学习。恢复部分可观察环境的一些特征，并尝试从我的行为和我的实际行为之前的状态中的特征中学习一些东西。但我认为这个问题最适合 RL。 我最近开始学习 RL，但我听说过很多高级主题，但没有好好研究，以确定它们是否适合这个问题。模仿学习或离线 RL 在这里有用吗？  更多背景信息，问题是离线的，所以我无法再次与环境交互，我不知道我的奖励函数，也不知道我的策略是否最佳（如果是这种情况，我可能会选择模仿学习），cueck！......我只是想了解我为什么要执行这些操作。 如果有人能帮助我提出一些我需要学习的方向或算法类别并且也许可以在这里工作，我将不胜感激。    提交人    /u/Odd-Appointment-4685   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gbknms/advice_on_offline_multi_agent_environment/</guid>
      <pubDate>Fri, 25 Oct 2024 03:06:52 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity AI PRO - 1 年优惠 - 便宜近 75%！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gbbpqc/perplexity_ai_pro_1_year_offer_almost_75_cheaper/</link>
      <description><![CDATA[      订购：https://cheapgpt.store/product/perplexity-ai-pro-subscription-one-year-plan 接受的付款方式： - PayPal。 （100% 买家保护。- Revolut。    提交人    /u/Verza-   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gbbpqc/perplexity_ai_pro_1_year_offer_almost_75_cheaper/</guid>
      <pubDate>Thu, 24 Oct 2024 19:54:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gb6efv/working_rl_in_practice/</link>
      <description><![CDATA[我知道 RL 在实践中很脆弱，很难发挥作用，但如果做得好，它也会非常强大，例如 Deepmind 与 AlphaZero 合作等。你知道 RL 在现实生活中应用的任何令人信服的例子吗？让你心存疑虑的东西？    提交人    /u/FriendlyStandard5985   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gb6efv/working_rl_in_practice/</guid>
      <pubDate>Thu, 24 Oct 2024 16:11:51 GMT</pubDate>
    </item>
    <item>
      <title>您会推荐虚幻引擎或 Unity 来构建环境吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gah9xw/would_you_recommend_unreal_engine_for_building/</link>
      <description><![CDATA[我喜欢的两款游戏引擎  虚幻引擎（Carla 就是用这个引擎开发的）。对于研究人员来说，许可很友好，如果你是典型的苦苦挣扎的研究学生，也不会花费太多。更难学习，蓝图可能会使事情变得更加复杂，如果不是典型的话。直接支持 C++。规格非常高。 Unity。非常容易构建，但对于视频游戏以外的任何事物来说，许可都太严格了。即使你不赚钱，每年也要 2.5k，研究似乎不在游戏/娱乐范围内。在 3d 之外更灵活。  两者似乎都有无头模式。 你会推荐哪款游戏引擎来构建 RL 环境，并且让它与 python 健身房训练工作流程一起工作的过程最不痛苦？    提交人    /u/I_will_delete_myself   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gah9xw/would_you_recommend_unreal_engine_for_building/</guid>
      <pubDate>Wed, 23 Oct 2024 18:09:15 GMT</pubDate>
    </item>
    <item>
      <title>需要解决部分观察到的迷宫环境的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gah5t5/need_advice_for_solving_partially_observed_maze/</link>
      <description><![CDATA[我在 Unity 中创建了一个自定义环境，其中包含简单的基于网格的迷宫，如下所示 (https://imgur.com/a/0rNmmUg)。代理只能看到（通过矢量观察，而不是图像）自身周围。它向 8 个方向发射射线（图中为红色），并获取有关其击中的内容（墙壁、出口或空无）以及到击中点的距离的信息。并且发现的房间数量也会被输入到观察中。至于奖励，它每一步都会得到 -1，发现新房间并找到出口时会得到正奖励。目标是探索迷宫并找到出口 问题是代理一直粘在墙上，四处转圈并随机行动。我尝试过将停留在一个房间的惩罚加倍，将高速度和其他事情的正奖励加倍。我还能尝试什么？我可以完全控制环境，并且不受代理的确切设计约束。我正在使用具有自动熵的 SAC     提交人    /u/Aydiagam   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gah5t5/need_advice_for_solving_partially_observed_maze/</guid>
      <pubDate>Wed, 23 Oct 2024 18:04:30 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：大型状态/动作空间的批量大小和更新频率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gabt9r/seeking_advice_batch_size_and_update_frequency/</link>
      <description><![CDATA[大家好！ 我正在做一个关于云资源分配的项目，我真的需要你的建议。我的目标是尽量减少服务器的总体能耗，我正在处理连续随机的作业到达。 下面是一个快速概述： 我处理每个作业块有 10 个作业，每个作业都有多个依赖任务。对于每个块，我运行 10 次迭代和 12 个情节来收集轨迹，然后使用离策略模式更新我的模型。 经过这 12 个情节 的 一次迭代 之后，我的重播缓冲区最终获得了大约 499,824 个经验！现在，我需要你的帮助：  您认为从重播缓冲区采样的最佳批次大小是多少？ 我应该多久更新一次模型参数？  由于作业不断到达以及任务和资源可用性的变化，我的状态和操作空间非常大且动态。 （我正在使用策略梯度架构。） 您能分享的任何见解或经验都将非常有帮助！ 非常感谢！   由    /u/TeamTop4542  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gabt9r/seeking_advice_batch_size_and_update_frequency/</guid>
      <pubDate>Wed, 23 Oct 2024 14:24:06 GMT</pubDate>
    </item>
    <item>
      <title>设置 Claude 计算机在本地使用演示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ga1dhl/setting_up_claude_computer_use_demo_locally/</link>
      <description><![CDATA[  由    /u/Weary-Database-8713  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ga1dhl/setting_up_claude_computer_use_demo_locally/</guid>
      <pubDate>Wed, 23 Oct 2024 03:48:37 GMT</pubDate>
    </item>
    <item>
      <title>需要有关构建 RL 代理的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9v667/need_advice_for_building_an_rl_agent/</link>
      <description><![CDATA[大家好，感谢您的阅读和提供的任何意见。 我有一个 Web 应用程序，每天有 50 多人使用，他们查看数据库并寻找可见的错误。这个过程已经使用了几年，并且不可扩展，因为我们即将将数据库大小乘以 100 倍。 因此，我试图构建一个代理，通过保存 UI 的状态和用户的操作以及对它们的训练来模仿人类浏览数据库。目标是让它发现最明显的错误，这样只有困难（和有趣的）错误才会留给我们训练有素的人工校对员（某种模仿学习）。 我有几个问题：  您认为可以采用哪种 RL 结构？ 什么是 RL 的最佳输入类型？以及 RL 准确度如何随着数据集大小而扩展？ 您知道哪些陷阱，我应该注意哪些陷阱？ 您有什么想法吗？:)  如果需要，我可以通过数据库获得很多与人类相关的东西（如果有效的话，拥有这个 AI 代理将大大改善我们校对人员的工作方式，因此他们有兴趣帮助收集数据）。 （我来自计算机视觉/数学背景，并具有构建更多 UNET/GNN 类型模型的经验。我正在尽可能快地浏览我在网上找到的资源）    提交人    /u/Delicious_Wall3597   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9v667/need_advice_for_building_an_rl_agent/</guid>
      <pubDate>Tue, 22 Oct 2024 22:36:37 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic：“通过新的 Claude 3.5 Sonnet 介绍‘计算机使用’”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9qvgo/anthropic_introducing_computer_use_with_a_new/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9qvgo/anthropic_introducing_computer_use_with_a_new/</guid>
      <pubDate>Tue, 22 Oct 2024 19:33:51 GMT</pubDate>
    </item>
    <item>
      <title>现实生活中的应用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9adre/real_life_application/</link>
      <description><![CDATA[嗨 在 reddit、medium 和其他网站的许多帖子中，我通常会看到关于玩具问题或模拟的应用。我认为我从未见过现实生活中的应用，即使是在一个简单的系统上。 在我看来，RL 从未进入现实生活，而是停留在模拟中。然而，这不可能存在，而且必然有人已经在现实中使用它了。 您是否曾在模拟中实现过？哪种方法在模拟中有效，但在真实系统中无效？在模拟过程中需要考虑哪些具体要点以确保成功转移到现实生活中？您遇到了哪些困难，又是如何克服的？ 感谢您的反馈。    提交人    /u/seb59   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9adre/real_life_application/</guid>
      <pubDate>Tue, 22 Oct 2024 04:59:19 GMT</pubDate>
    </item>
    <item>
      <title>QMIX 代理做出类似的动作模式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g981cz/qmix_agent_makes_similar_action_patterns/</link>
      <description><![CDATA[      https://preview.redd.it/2eg00ql418wd1.png?width=162&amp;format=png&amp;auto=webp&amp;s=1640f5e4966c57241d8388480abf010c0dc86747 我正在尝试在多智能体环境中使用 QMIX 和 gfootball。如图所示，使用 QMIX 学习的结果是所有智能体都在追球。我该如何解决这个问题？    提交人    /u/TelephoneStrange9364   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g981cz/qmix_agent_makes_similar_action_patterns/</guid>
      <pubDate>Tue, 22 Oct 2024 02:46:21 GMT</pubDate>
    </item>
    <item>
      <title>模型是否对短剧集长度产生偏差？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g8y5co/model_became_biased_for_short_episode_length/</link>
      <description><![CDATA[嘿！ 我正在使用 SB3 的 PPO 训练交易代理。我正在使用基于事件的回测器并使用数月的 HFT 数据。为了使代理更加稳健，我决定在整个数据中选择一个随机起始位置并交易预设数量的步骤，之后我提供构成一个情节的 truncated ==True, Done == False 信号。然后由模型重置环境并选择另一个随机起始位置。 我正在使用 make_vec_env，创建大量并行环境（大约 40 个）并使用 VecNormalize。该模型收敛得很好，我在 TensorBoard 上看到了很好的奖励值。 但是当我在已保存的模型上使用 assess_policy（当然使用已保存的 VecNormalize 统计数据）时，即使在我用于训练的数据上，我也会看到巨大的负奖励曲线。应该提到的一件重要的事情是 - 当我使用evaluate_policy时，我不会将数据限制在几天内，我会让代理运行整个月的数据。 可能发生两件事：  我在保存 VecNormalize 数据或保存/加载模型或通过某种方式提供错误的截断信号传递环境方面做错了（但我非常怀疑） 或者模型学会在短情节长度下获得可观的利润，并且当情节继续进行模型习惯的更多步骤时，这是一个很大的惊喜并且它开始失败。   第二个假设得到了这样的事实的支持：当我在训练期间改变情节拆分成的步骤数时，我会看到评估图上大约相同步骤数的奖励曲线上升。 那么这可能吗？如果可能，克服这个问题的正确方法是什么？ 附言：我决定不在起始帖子中堆满源代码，但如果有必要，我很乐意提供它们。    提交人    /u/StabbMe   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g8y5co/model_became_biased_for_short_episode_length/</guid>
      <pubDate>Mon, 21 Oct 2024 19:17:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO 扩展奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g8smlo/ppo_scaling_reward/</link>
      <description><![CDATA[大家好， 我目前正在尝试解决一个问题，该问题的奖励可能非常大（例如 -100），也可能很小（最多约 9） 由于惩罚高于奖励（即更重要）很重要，因此除了缩放之外，我不“允许”更改奖励函数。 我读过几次，在 -1 和 1 之间缩放奖励对于 PPO 和其他方法很重要。 如果我将此奖励缩放（div 100），使得 -1 是最高惩罚，我只会获得较小的奖励。因此奖励在范围内（-1, 0.09） 奖励没有达到 1 是个问题吗？   由    /u/luigi1603  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g8smlo/ppo_scaling_reward/</guid>
      <pubDate>Mon, 21 Oct 2024 15:36:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 编写的 RL 代理，用于 Java 编写的棋盘游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g84lzs/rl_agent_in_python_for_board_game_in_java/</link>
      <description><![CDATA[嘿 :) 我想用 Python 为我的 Java 棋盘游戏实现一个 RL 代理（可能是 DQN）。我面临的问题是，据我所知，大多数 RL 框架都设计为主动部分，而游戏环境仅对代理的操作做出反应并提供反馈。我现在的问题是，是否可以反过来做？棋盘游戏（Cascadia）已经用 Java 实现，并为 AI 玩家提供了界面。因此，每当轮到代理时，我计划用 Python 对我的代理进行 REST 调用，提供编码的游戏状态和可能的移动，并得到“最佳”移动作为回报（Java 客户端决定何时调用代理）。这完全可能吗，还是我必须更改我的环境，以便 Python 代理可以成为主动部分？提前感谢您的帮助！   由    /u/ItchyRoyal212  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g84lzs/rl_agent_in_python_for_board_game_in_java/</guid>
      <pubDate>Sun, 20 Oct 2024 17:53:34 GMT</pubDate>
    </item>
    </channel>
</rss>