<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 28 Feb 2024 03:14:03 GMT</lastBuildDate>
    <item>
      <title>帮助我理解：为什么使用政策网而不仅仅是价值网？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1te73/help_me_understand_why_use_a_policy_net_instead/</link>
      <description><![CDATA[上下文：比如说 AlphaZero。一种确定性游戏，你采取一个行动，很容易计算该行动后的后续状态。 在深入研究人们如何在此类场景中使用强化学习的示例时，我看到了很多训练两者的示例一个用于评估行动的政策网络和一个用于评估状态的价值网络。 我困惑的是，为什么两者都是？为什么不只用一个网络来评估状态，然后根据它将导致的状态来选择操作？我不知道这是一个困难的、依赖于上下文的问题，还是一个我很快就会听到的有明确答案的简单问题，或者是那种答案只是因为某种原因它效果更好的问题。我的直觉是，状态评估会比政策评估效果更好，而且只需要训练一个网络就会是一种优势。 我能想到的一些可能性：  &lt; li&gt;在一个状态上运行网络以获得 N 个动作的输出比在 N 个状态上运行网络要便宜得多（这感觉可能是答案的一部分） 不计算后续状态比在 N 个状态上运行网络便宜得多计算它们 它泛化到下一个状态未知的情况 当动作的表示比状态小得多时它效果更好 有一些拥有两个独立网络的额外好处 将政策网络训练到一定水平比将价值网训练到相同的准确性水平更容易，因为它关心的事情更少 &lt; /ul&gt;   由   提交/u/seventythird  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1te73/help_me_understand_why_use_a_policy_net_instead/</guid>
      <pubDate>Wed, 28 Feb 2024 01:31:31 GMT</pubDate>
    </item>
    <item>
      <title>没有顶级 ML 论文的人，你在哪里工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1suv1/people_with_no_toptier_ml_papers_where_are_you/</link>
      <description><![CDATA[我即将毕业，我的博士学位。研究的是强化学习算法及其应用。但是，我没能在顶级 ML 会议（NeurIPS、ICLR、ICML）上发表论文。但是我的领域有好几篇论文，如何才能获得我面试过一些移动和电子商务 (RecSys) 公司，但都失败了。  我不想做博士后，我对与学术界相关的任何事情都不感兴趣。  如果初创公司有任何机会，或者我还没有探索过的其他职位，请告诉我。   由   提交/u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1suv1/people_with_no_toptier_ml_papers_where_are_you/</guid>
      <pubDate>Wed, 28 Feb 2024 01:06:45 GMT</pubDate>
    </item>
    <item>
      <title>PPO 无法学习简单任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1l6h5/ppo_failing_to_learn_simple_task/</link>
      <description><![CDATA[我创建了一个简单的环境： - 动作空间是离散的，长度为 10 -状态空间是长度为10的二进制，初始化为全0。 -智能体选择一个动作，状态规范中相应的索引更新为1，并给予0.1的奖励。  &gt;- 如果代理选择了一个已经选择的动作，则奖励为 0，并且剧集结束。 ​ 我将其构建为工作的简单基础做一些更复杂的事情，但我很惊讶我在这个环境中无法学到任何东西......我正在尝试使用 RLLib 进行 PPO 和 DQN 。我希望这能够很容易地学会。但随着训练的进行，奖励不断下降，随着探索的减少，智能体似乎每次都学会只选择相同的动作。 想知道是否有人对此有任何见解？我已经尝试调整超参数了很多，但没有任何东西能够学习来解决环境问题。这是环境代码： importgymnasiumasgymfromgymnasiumimportspacesimportnumpyasnpclassSimpleSim(gym.Env): &quot;&quot;&quot;&quot;遵循健身房界面的自定义环境。 ”“”元数据 = {&#39;render.modes&#39;: [&#39;人类&#39;]} def __init__(self): super(SimpleSim, self).__init__() self.num_actions = 10 self.action_space = space.Discrete(self.num_actions) self.观察空间 = 空间.MultiBinary(self.num_actions) self.selection_reward = 0.1 self.repeated_selection_penalty = 0 self.all_selected_reward = 0 self.episode_end_penalty = 0 def 重置(self, *, seed=None, options=None): self.state = np.zeros(self.num_actions, dtype=int) # 重置操作 return self.state, {} def step(self, action): did = False 奖励 = 0 if self.state[action] == 0: # 操作有之前没有被选择 # 更新状态并给予奖励 self.state[action] = 1reward = self.selection_reward else: # 之前选择了动作，结束剧集奖励 = self.repeated_selection_penalty did = True if np.all(self.state = = 1): # 所有操作均已选择完成 = True return self.state,reward,done,False,{}  我尝试过的其他内容： - 经常搞乱奖励函数，试图增加对过早停止的惩罚 - 不终止剧集，而是应用负奖励 我觉得我一定错过了一些东西，因为这应该是很容易的任务。   由   提交 /u/SkittlesUSA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1l6h5/ppo_failing_to_learn_simple_task/</guid>
      <pubDate>Tue, 27 Feb 2024 19:52:40 GMT</pubDate>
    </item>
    <item>
      <title>用代码解释上下文强盗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1ecfw/contextual_bandits_explained_with_codes/</link>
      <description><![CDATA[大家好，看看什么是 Contextual Bandits，它们与 Multi Armed bandits 有何不同，以及在本教程中实现 Contextual Bandits 的基线代码 https://youtu.be/A5jRcD8XihI?si=kzD4nSGmmze5J90X  &amp; #32；由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1ecfw/contextual_bandits_explained_with_codes/</guid>
      <pubDate>Tue, 27 Feb 2024 15:19:24 GMT</pubDate>
    </item>
    <item>
      <title>图观察空间中的奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1d8pm/rewarding_in_graph_observation_space/</link>
      <description><![CDATA[我有一个环境，其中观察空间是一组图，并且在每一步中我选择图中的一个节点并执行一个操作，该操作将产生成本。目标是最小化成本。 我使用以下奖励公式： 对于给定的图表，如果这一集发生的总成本小于直到现在它获得了 +10 奖励和 -10 奖励，到目前为止，它的表现还不能比最好的更好。 我在这种环境下使用 PPO 算法，奖励似乎没有改善. 我需要一些可以在此设置中使用的奖励制定建议？   由   提交/u/Emergency_Pen6429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1d8pm/rewarding_in_graph_observation_space/</guid>
      <pubDate>Tue, 27 Feb 2024 14:32:15 GMT</pubDate>
    </item>
    <item>
      <title>离线演员评论家强化学习可扩展到大型模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b1917j/offline_actorcritic_reinforcement_learning_scales/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.05546 摘要：  我们证明离线演员评论家强化学习可以大规模扩展模型 - 例如变压器 - 并遵循与监督学习类似的缩放法则。我们发现，在包含 132 个连续控制任务的次优行为和专家行为的大型数据集上进行多任务训练时，离线 Actor-Critic 算法的性能优于强大的、有监督的行为克隆基线。我们介绍了一种基于感知器的行动者批评家模型，并阐明了使离线强化学习与自注意力和交叉注意力模块一起工作所需的关键模型特征。总的来说，我们发现：i）简单的离线演员批评算法是逐渐摆脱当前主导的行为克隆范式的自然选择，ii）通过离线强化学习，可以学习同时掌握多个领域的多任务策略，包括来自次优演示或自我生成数据的真实机器人任务。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b1917j/offline_actorcritic_reinforcement_learning_scales/</guid>
      <pubDate>Tue, 27 Feb 2024 10:50:11 GMT</pubDate>
    </item>
    <item>
      <title>关于 RL 任务的特征删除/选择有什么有力的文献/想法吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b0u3qu/any_strong_literatureideas_on_feature/</link>
      <description><![CDATA[大家好， 我有兴趣研究特征选择作为提高连续动作空间 RL 收敛速度的方法任务 - 特别是在全宇宙艾萨克健身房环境中工作。到目前为止，我首先训练模型，使用 SHAP 和 SAGE 等库计算特征重要性值，然后删除一些低重要观察值进行重新训练，但这种策略不适用于我一直在尝试的其他环境。  我知道 SHAP 和 SAGE 以及特征重要性一般来说并不是特征选择的最佳方法，所以我在这里联系看看是否有人有任何不同的方向可以指出我。其中一些任务有 50 多个观察空间大小，我想看看是否有任何方法可以帮助确定可以在不牺牲模型奖励最大值的情况下删除哪些特征 感谢任何帮助和讨论，谢谢&lt; /p&gt;   由   提交/u/bbri826  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b0u3qu/any_strong_literatureideas_on_feature/</guid>
      <pubDate>Mon, 26 Feb 2024 21:57:53 GMT</pubDate>
    </item>
    <item>
      <title>帮助建模逆向学习实验数据</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b0swej/help_with_modeling_reversal_learning_experimental/</link>
      <description><![CDATA[这里是神经科学研究人员。我对编程中的强化学习相对较新，但目前正在开展一项实验，该实验需要对行为任务进行描述性建模。 ​ 上下文： 我正在运行一个简单的逆向学习任务，其中动物可以使用两个喷口，但只有其中一个获得奖励。有大量的试验，在每次试验中都会播放提示，然后提供奖励。经过指定次数的试验后，意外情况会发生变化，之前未获得奖励的喷口将获得奖励。我在任务期间连续跟踪舔，并记录一些其他更复杂的神经数据。  我想看看动物如何根据结果历史和其他参数在奖励交付之前灵活地调整对每个喷口的预期舔舐。我的目标是将行为数据拟合到某个模型，该模型描述每个动作的相对价值的变化。我想看看神经数据能够如何很好地描述价值，尽管我想从行为测量建模开始。  我研究了 Rescorla Wagner 和 Q-learning 等各种模型，但不确定哪种方法最适合对我的数据进行建模。另外，如果可能的话，我希望将所有编程限制为 Python。 ​ 任何有关如何继续进行此数据分析的提示将不胜感激。 .   由   提交/u/Any-Captain5070  /u/Any-Captain5070 reddit.com/r/reinforcementlearning/comments/1b0swej/help_with_modeling_reversal_learning_experimental/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b0swej/help_with_modeling_reversal_learning_experimental/</guid>
      <pubDate>Mon, 26 Feb 2024 21:10:59 GMT</pubDate>
    </item>
    <item>
      <title>这5个领域的好项目在GSOC注册中脱颖而出</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b0sggn/good_projects_in_these_5_domains_to_stand_out_in/</link>
      <description><![CDATA[     &lt; td&gt; https://preview.redd.it/yjxy8stgtzkc1.png?width=1093&amp;format=png&amp;auto=webp&amp;s=f1e6ea35386318e4d3292c4ee90682f96dc b2ecf GSOC 2024，毫升包。这些都是他们所期待的想法。希望了解相关的中等难度项目，这将使我比其他人更有优势。 还有关于 GSOC 的任何其他提示，我们将不胜感激  &amp; #32；由   提交 /u/PandeyyJi   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b0sggn/good_projects_in_these_5_domains_to_stand_out_in/</guid>
      <pubDate>Mon, 26 Feb 2024 20:54:22 GMT</pubDate>
    </item>
    <item>
      <title>对MuZero的怀疑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b0g4gr/doubt_about_muzero/</link>
      <description><![CDATA[我对 MuZero 的理解是，从给定状态开始，我们使用蒙特卡罗树搜索算法将搜索树扩展到未来的 K 步。但与标准 MCTS 不同的是，我们有一个深度模型，a) 产生下一个状态和给定动作的奖励，b) 产生一个价值函数，这样我们就不需要在每个节点模拟整个情节的延续。  两个问题：  最后一点正确吗？ IE。在树搜索期间没有进行任何模拟，仅使用价值函数来估计当前节点以后的未来回报？ 这种树扩展机制仅在训练时使用还是在火车时间？论文的某些部分似乎表明确实如此，但我不明白政策头的目的是什么    由   提交 /u/fedetask   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b0g4gr/doubt_about_muzero/</guid>
      <pubDate>Mon, 26 Feb 2024 12:17:30 GMT</pubDate>
    </item>
    <item>
      <title>寻求研究项目想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b0fgep/seeking_for_research_project_ideas/</link>
      <description><![CDATA[我是一名最后一年的本科生，对这个领域还很陌生。我正在致力于在这个领域构建一个项目，我想在其中带来一些新奇的东西。我正在寻求有关如何实现这一目标的建议和想法。    由   提交/u/fa_anony__mous   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b0fgep/seeking_for_research_project_ideas/</guid>
      <pubDate>Mon, 26 Feb 2024 11:38:25 GMT</pubDate>
    </item>
    <item>
      <title>在 Julia 中实施 RL 项目是否值得？有什么第一手的经验吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b0dh6r/is_it_worth_the_effort_implementing_rl_project_in/</link>
      <description><![CDATA[我刚刚开始一个深度强化学习的新研究项目。现在我想知道这个项目在性能和可用性方面是否值得切换到 Julia。 之前我已经使用 Ray 的 Rllibs 实现了我的项目，或者编写了自己的代码（首先使用tensorflow，然后使用迁移到 pytorch）。我听说 Julia 在可用性和速度方面表现出色，但我不确定这是否也适用于深度学习和强化学习。 我很感激任何第一手经验:)    由   提交 /u/Tortoise_vs_Hare   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b0dh6r/is_it_worth_the_effort_implementing_rl_project_in/</guid>
      <pubDate>Mon, 26 Feb 2024 09:28:52 GMT</pubDate>
    </item>
    <item>
      <title>[Q]关于线性强盗中的马哈拉诺比斯范数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b096ky/q_about_mahalanobis_norm_in_linear_bandits/</link>
      <description><![CDATA[我对线性老虎机设置下使用的马哈拉诺比斯范数感到困惑。需要线性代数吗？   由   提交 /u/RalCauchy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b096ky/q_about_mahalanobis_norm_in_linear_bandits/</guid>
      <pubDate>Mon, 26 Feb 2024 04:53:13 GMT</pubDate>
    </item>
    <item>
      <title>我似乎无法让 DeepMind 导航迷宫在 WSL 上工作 - 它是为运行纯 Linux 的系统构建的还是我只是做错了什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1azyrwy/i_cant_seem_to_get_the_deepmind_nav_maze_to_work/</link>
      <description><![CDATA[我有 bazel，我基本上只是想把它作为一个测试环境来运行，我可以在其中运行 dreamerv3，但它就是不运行似乎正在加载 - 我不断收到构建错误   由   提交/u/dagangsta2012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1azyrwy/i_cant_seem_to_get_the_deepmind_nav_maze_to_work/</guid>
      <pubDate>Sun, 25 Feb 2024 20:59:50 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 代码很难读</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1azgrc4/dreamerv3_code_is_so_hard_to_read/</link>
      <description><![CDATA[大家好， 最近我被分配了一项工作来研究 SOTA 世界模型 DreamerV3 [^1]。我花了 3 个月的时间来理解这篇论文（我是 ML 新手）和代码。基本上，我查看了 3 个代码库：  作者的实现，用 Jax 编写：https:// github.com/danijar/dreamerv3 NM512 的 PyTorch 实现：https://github。 com/NM512/dreamerv3-torch sheeprl 的 PyTorch 实现：https://github。 com/Eclectic-Sheep/sheeprl  (1)使用Jax，看起来很复杂，(3)提供了一系列博客进行解释。所以我选择（3）。然而，即使 (3) 也相当复杂。 Sheeprl 希望他们的框架适用于所有 RL 算法。由于牺牲了通用性，程序逻辑变得难以阅读。我对这项任务感到不知所措，不知道该怎么办。  也许我应该回到 Jax 版本，即使没有关于它的文档。我觉得 Dreamer 里的设计和技巧太多了:( 有没有（代码、博客、研究）推荐？也许我应该回到 David Ha 的 2018 年世界模型论文[^2]来做我的研究，因为它应该比 Dreamer 更容易。 非常感谢！ [^1]: https://arxiv.org/abs/2301.04104 [^2]：https:// arxiv.org/abs/1803.10122    提交者    /u/AdministrativeCar545   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1azgrc4/dreamerv3_code_is_so_hard_to_read/</guid>
      <pubDate>Sun, 25 Feb 2024 06:04:52 GMT</pubDate>
    </item>
    </channel>
</rss>