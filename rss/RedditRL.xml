<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 29 Jul 2024 18:20:00 GMT</lastBuildDate>
    <item>
      <title>平均情节奖励差异，为什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</link>
      <description><![CDATA[      嗨， 我有一个简单的环境，我使用 SB3 中的各种算法进行训练作为练习。这是 DDPG 和 SAC 的 tensorboard 情节平均奖励（针对完全相同的环境）。在学习完成并保存模型时，报告的奖励约为 6。但是，当我在保存的模型上使用 SB3 assesse_policy (with n_eval_episodes=10) 时，我看到以下内容： 对于 SAC：平均奖励：1.5123082560196053 +/- 0.0008870645563937467 对于 DDPG：平均奖励：0.6197831666923037 +/- 0.000696591922367452 我预计平均奖励约为 6。这种预期是错误的吗？ https://preview.redd.it/xxag55fswgfd1.png?width=765&amp;format=png&amp;auto=webp&amp;s=e6903abf29863ad230d84451e83b162e265eb101    submitted by    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</guid>
      <pubDate>Mon, 29 Jul 2024 14:32:31 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线-3 工人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</link>
      <description><![CDATA[我正在研究 stable-baselines3 库 的一些算法的实现。 具体来说，我看到一些算法（A2C、PPO）的文档提到该实现使用了多个工作器。  对于其他算法，它没有提及任何内容，所以我的问题是：  我可以假设其他算法只使用一个学习者吗？ 为了进一步解答我的疑问，在所有算法的摘要表中显示，每个算法都支持“多处理”。但是，如果我理解正确的话，那仅指使用矢量化环境，尽管我不会打赌它。 最后说明：我尝试通过查看A2C（其中提到使用工作者）和SAC（其中没有提到工作者）的代码来回答我的问题，但这没有帮助，所以我在这里   由    /u/Frank-the-hank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</guid>
      <pubDate>Mon, 29 Jul 2024 07:27:37 GMT</pubDate>
    </item>
    <item>
      <title>创建博弈论论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</link>
      <description><![CDATA[在我国，对于原创博弈，制定博弈论与策略优化实施大纲的传统方式是什么？    提交人    /u/Former_Ad_4221   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</guid>
      <pubDate>Mon, 29 Jul 2024 06:24:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PPO 算法没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</link>
      <description><![CDATA[我尝试了代码中几乎所有的超级参数，但奖励只停留在 8-11 之间，从未超出这个范围。我让它运行了 10-20 分钟，它仍然只在这两个值之间波动。我很困惑我做错了什么，请帮帮我，我花了一整天的时间调试，我打印出了演员的 nn.parameters，它们正在发生变化。 编辑：我一直在尝试所有不同的超参数/模型架构，我现在想知道我的代码是否存在更根本的错误，特别是在演员对独热张量进行采样然后通过它进行反向传播时。 编辑：我的操作在 with set_exploration_type(ExplorationType.MEAN), torch.no_grad(): 块内都是 NaN，不确定为什么 import torch from torch import nn from torchrl.envs import Compose, ObservationNorm, DoubleToFloat, StepCounter, TransformedEnv from torchrl.envs.libs.gym import GymEnv from torchrl.envs.utils import check_env_specs, set_exploration_type, ExplorationType 从 torchrl.modules 导入 ProbabilisticActor、OneHotCategorical、ValueOperator 从 torchrl.collectors 导入 SyncDataCollector 从 torchrl.data.replay_buffers 导入 ReplayBuffer 从 torchrl.data.replay_buffers.storages 导入 LazyTensorStorage 从 torchrl.data.replay_buffers.samplers 导入 SamplerWithoutReplacement 从 torchrl.objectives.value 导入 GAE 从 torchrl.objectives 导入 ClipPPOLoss 从 tensordict.nn 导入 TensorDictModule torch.set_printoptions(threshold=16384) device=&quot;cuda&quot; base_env = GymEnv(&#39;CartPole-v0&#39;, device=device) env = TransformedEnv( base_env, Compose( ObservationNorm(in_keys=[&quot;observation&quot;]), DoubleToFloat(), StepCounter() ) ) env.transform[0].init_stats(1024) check_env_specs(env) actor_net = nn.Sequential( nn.Linear(env.observation_spec[&quot;observation&quot;].shape[-1], 256, device=device), nn.Sigmoid(), nn.Linear(256, 256, device=device), nn.Sigmoid(), nn.Linear(256, 256, device=device), nn.Sigmoid(), nn.Linear(256, env.action_spec.shape[-1],设备=设备））actor_module = TensorDictModule（actor_net，in_keys = [“observation”]，out_keys = [“logits”]）actor = ProbabilisticActor（module = actor_module，spec = env.action_spec，in_keys = [“logits”]，distribution_class = OneHotCategorical，return_log_prob = True）value_net = nn.Sequential（nn.Linear（env.observation_spec [“observation”]。shape [-1]，16，设备=设备），nn.Sigmoid（），nn.Linear（16，16，设备=设备），nn.Sigmoid（），nn.Linear（16，16，设备=设备），nn.Sigmoid（），nn.Linear（16，1，设备=设备） ）value_module = ValueOperator（module = value_net，in_keys = [“observation”]）frames_per_batch = 1024 total_frames = 1048576收集器 = SyncDataCollector（env，actor，frames_per_batch = frames_per_batch，total_frames = total_frames，split_trajs = True，reset_at_each_iter = True，设备=设备）replay_buffer = ReplayBuffer（存储 = LazyTensorStorage（max_size = frame_per_batch），采样器 = SamplerWithoutReplacement（））advantage_module = GAE（gamma = 0.99，lmbda = 0.95，value_network = value_module，average_gae = True）entropy_eps = 1e-4 loss_module = ClipPPOLoss（actor_network = actor，critic_network = value_module，clip_epsilon = 0.2，entropy_bonus = bool（entropy_eps），entropy_coef = entropy_eps）optim = torch.optim.Adam（loss_module.parameters（），lr = 1e-4）scheduler = torch.optim.lr_scheduler.CosineAnnealingLR（optim，total_frames // frames_per_batch）sub_batch_size = 64 for i，tensordict_data in enumerate（collector）：for _ in range（8）：advantage_module（tensordict_data）replay_buffer.extend（tensordict_data.reshape（-1）.cpu（））for _ in range（frames_per_batch // sub_batch_size）：data = replay_buffer.sample（sub_batch_size）loss = loss_module（data.to（device））loss_value = loss[“loss_objective”] + loss[&quot;loss_critic&quot;] + loss[&quot;loss_entropy&quot;] loss_value.backward() optim.step() optim.zero_grad() scheduler.step() if i % 16 == 0: with set_exploration_type(ExplorationType.MEAN), torch.no_grad(): rollout = env.rollout(1024, actor) print(rollout[&quot;next&quot;,&quot;reward&quot;].sum()) del rollout actor = ProbabilisticActor( module = actor_module, spec = env.action_spec, in_keys = [&quot;logits&quot;], distribution_class = OneHotCategorical, return_log_prob = True )     提交人    /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</guid>
      <pubDate>Mon, 29 Jul 2024 05:00:03 GMT</pubDate>
    </item>
    <item>
      <title>用于构建 RL 项目的简单可视化工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</link>
      <description><![CDATA[      我计划制作这个用于 RL 开发的简单工具。这个想法是快速构建和训练 RL 代理，无需代码。这对于快速开始新项目或轻松进行实验以调试 RL 代理非常有用。 目前设计中有 3 个选项卡：环境、网络和代理。我计划添加第四个选项卡，称为“实验”，用户可以在其中定义超参数实验并直观地查看每个实验的结果，以便调整代理。这个设计是一个非常早期的原型，可能会随着时间的推移而改变。 你们觉得怎么样？ https://preview.redd.it/sb5awqjys8fd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d1046c3b7e195dba0b7779ee55f11c9330ec3d12    提交人    /u/Charming-Quiet-2617   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</guid>
      <pubDate>Sun, 28 Jul 2024 11:12:20 GMT</pubDate>
    </item>
    <item>
      <title>关于 Stable Baselines3 和 AgileRL 的其他问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</link>
      <description><![CDATA[我尝试使用 SB3 实现我的 RL 算法，但是我对该框架中使用的术语感到很困惑。 我所有的旧 RL 算法都有一个这样的结构： untill episode &lt; max_episode_numbers: for step in max_steps_number: s&#39;, r, d, info = env.step 因此，基本上，我定义了最大 episode 数和最大 step 数，然后让算法运行。 现在在 SB3 中（以 PPO 算法 为例），有许多不同的术语，例如： n_epochs (int) – 优化替代损失时的 epoch 数 total_timesteps (int) – 要训练的总样本数（env 步骤） 据我所知，n_epochs 并不等同于我的 max_episode_numbers上面的示例中，但它是调用代理损失的优化的内部属性。 但如何定义要执行的最大情节数？即使他们论坛上有趣的线程也没有太大帮助。 我个人想出了类似以下的东西，但我不确定它是否有意义（我的想法来自这里）： for episode &lt; max_number_episode： model.learn(total_timesteps = 10_000) model.save(path_model) 更糟糕的是，我试图将该框架与 AgileRL 框架进行比较，后者看起来非常有前途。 在 AgileRL 中，明确定义了最大剧集数量的参数： &quot;EPISODES&quot;: 1000, # 要训练的剧集数量EPISODES&quot;: 1000, # 要训练的剧集数量 甚至是一集中的 max_number 步数： &quot;MAX_STEPS&quot;: 500, # 代理在环境中采取的最大步数 但是对于外部循环，它不采用情节数，而是采用步数。 # 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []# 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []  现在我完全对术语、它们的含义和功能感到困惑    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</guid>
      <pubDate>Sun, 28 Jul 2024 10:24:11 GMT</pubDate>
    </item>
    <item>
      <title>使用多臂老虎机的推荐系统参考资料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</link>
      <description><![CDATA[我正在尝试学习如何将多臂老虎机应用到推荐系统中，但我不知道如何将原始的老虎机问题转换为有数据的场景。例如，这个库 https://github.com/fidelity/mab2rec 声称他们使用多臂老虎机，但他们没有解释如何使用。我试着阅读源代码，但我不太明白。 有人可以推荐一些资源来学习这种特定的应用程序吗？    提交人    /u/VanBloot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</guid>
      <pubDate>Sat, 27 Jul 2024 22:44:09 GMT</pubDate>
    </item>
    <item>
      <title>MADDPG 未学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edo44n/maddpg_not_learning/</link>
      <description><![CDATA[大家好，我最近在简单的对手环境中训练了一个 MADDPG。模型开始运行，但结果很糟糕，代理没有学到任何东西。我已经尝试调试了几个星期，但没有成功。 我知道提供的信息有限，很多事情都可能出错，但如果你感兴趣并帮助查看我的代码（在 Google Collab 上集成到一个页面中），我将不胜感激。提前致谢。 代码：https://colab.research.google.com/drive/1bRV803GR2vnjX0jy7bkTEy3A8VLYFPB6?usp=sharing    提交人    /u/TransportationOk2251   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edo44n/maddpg_not_learning/</guid>
      <pubDate>Sat, 27 Jul 2024 19:14:40 GMT</pubDate>
    </item>
    <item>
      <title>用于构建和测试 RL 算法的软件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ed62l3/software_used_for_building_and_testing_rl/</link>
      <description><![CDATA[在观看了 AI 击败各种游戏的视频后，我最近对强化学习 (RL) 着迷。我计划在接下来的几个月内完成一些深度学习项目后深入研究 RL。 我偶然看到了一段视频，其中 AI 在 Trackmania 中打破了多项世界纪录 ，我很好奇这些视频中用于设计汽车和赛道元素的软件。有人知道在 Trackmania 中构建这些环境和测试 RL 算法可能会使用什么工具或软件吗？ 提前感谢您的帮助！    提交人    /u/iam_raito   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ed62l3/software_used_for_building_and_testing_rl/</guid>
      <pubDate>Sat, 27 Jul 2024 02:46:56 GMT</pubDate>
    </item>
    <item>
      <title>如何管理巨大的行动空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ed0642/how_to_manage_huge_action_spaces/</link>
      <description><![CDATA[我对深度强化学习还很陌生。我正在尝试解决一个问题，其中代理学习在 NxN 网格中绘制矩形。这需要代理选择两个坐标点，每个坐标点都是 2 个数字的元组。动作空间多项式 N4。我目前使用 DQN 算法处理 N=4 的情况。在此算法中，神经网络输出动作的 N4 个 q 值。对于 20x20 网格，我需要一个具有 160,000 个输出的神经网络，这太荒谬了。我应该如何处理这种动作空间巨大的问题？参考论文也将不胜感激。    提交人    /u/medwatt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ed0642/how_to_manage_huge_action_spaces/</guid>
      <pubDate>Fri, 26 Jul 2024 21:58:57 GMT</pubDate>
    </item>
    <item>
      <title>利用机器的传感器来定义观察空间是否（总是）有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecw0i3/does_make_always_sense_to_take_the_sensors_of_a/</link>
      <description><![CDATA[在许多机械臂示例和框架中，观察几乎是一本包含以下项目的字典： obs = {&#39;observation&#39;: [x, y, z], &#39;achieved_goal&#39;: [x, y, z], &#39;desired_goal&#39;: [x, y, z]&gt; 所以基本上末端执行器和目标的空间坐标被传递给网络。 但我的观点是：不太可能观察末端执行器在空间中的 3D 位置。是的，您可以使用相机并使用一些三角测量方法来检索末端执行器的位置。 但获取传感器读取的关节值不是更好吗？更有可能有一个编码器可以给我关节的旋转。这会更加现实，也更精确。 我为什么要问这个问题？因为理论上它应该可以简化很多问题，因为我观察到的空间会更小（一个能给我 +/- 120° 之间角度的编码器肯定比空间中的 3D 坐标更好）。    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecw0i3/does_make_always_sense_to_take_the_sensors_of_a/</guid>
      <pubDate>Fri, 26 Jul 2024 18:59:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么在使用动作之前要将其与 action_scale 相乘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</link>
      <description><![CDATA[我发现在很多 RL 例子中，动作都乘以了某些动作比例值。 这是为了从策略中调整动作的“影响力”吗？    提交人    /u/Open-Safety-1585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</guid>
      <pubDate>Fri, 26 Jul 2024 12:04:12 GMT</pubDate>
    </item>
    <item>
      <title>参加空气曲棍球挑战！构建并训练可以玩空气曲棍球的代理。击败您的竞争对手，赢取 3000 美元，并有机会在真正的机器人设置上试用您的代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</link>
      <description><![CDATA[        提交人    /u/elizabeth_duhh04   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</guid>
      <pubDate>Fri, 26 Jul 2024 11:26:31 GMT</pubDate>
    </item>
    <item>
      <title>如何实现风摩擦力作用于猎豹模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecjura/how_to_realize_wind_frictions_acting_on_the/</link>
      <description><![CDATA[嗨， 我是一名学生，我正在尝试使用 MuJoCo 在不同环境中训练基于模型的强化学习代理。主要目标是扩展基于模型的强化学习方法以处理非平稳环境，例如动态（例如改变质量、重力、增加风摩擦）和/或奖励（例如改变目标速度）随时间变化的环境。目前，我专注于 [Gymnasium](https://gymnasium.farama.org/environments/mujoco/) 中基于 MuJoCo 的环境 我正在寻求一些帮助来定义作用于 [Cheetah 模型](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/half\_cheetah\_v5.py) 的风摩擦。我目前的想法是添加额外的执行器，作用于模型移动方向上或相反方向的某些关节。 下面你只能看到我修改的 Cheetah 模型的一部分，即执行器部分。所有带齿轮参数设置的电机均来自原始 Cheetah 模型。此外，我定义了一个执行器连接到特定关节，其控制范围限制在 -50 到 50 之间。我将额外的执行器从代理的动作空间中排除，以便代理无法控制它们（请检查下面的函数），我在运行时明确设置它们的值。 我有两个问题：  我不确定是否需要为每个附加执行器设置齿轮参数？ 此外，根据文档，多个执行器作用于单个关节没有问题，但一切对我来说都很新，不知道我是否以正确的方式更改模式？  这是修改后的执行器部分： &lt;details&gt; ``` &lt;actuator&gt; &lt;motor gear=&quot;120&quot;关节=&quot;bthigh&quot; name=&quot;bthigh&quot;/&gt; &lt;马达齿轮=&quot;90&quot; 关节=&quot;bshin&quot; name=&quot;bshin&quot;/&gt; &lt;马达齿轮=&quot;60&quot; 关节=&quot;bfoot&quot; name=&quot;bfoot&quot;/&gt; &lt;马达齿轮=&quot;120&quot; 关节=&quot;fthigh&quot; name=&quot;fthigh&quot;/&gt; &lt;马达齿轮=&quot;60&quot; 关节=&quot;fshin&quot; name=&quot;fshin&quot;/&gt; &lt;马达齿轮=&quot;30&quot; 关节=&quot;ffoot&quot; name=&quot;ffoot&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;rootx&quot; name=&quot;frictionrootx&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bthigh&quot; name=&quot;frictionbthigh&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bshin&quot; name=&quot;frictionbshin&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bfoot&quot; name=&quot;frictionbfoot&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;fthigh&quot; name=&quot;frictionfthigh&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;fshin&quot; name=&quot;frictionfshin&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-30 30&quot; joint=&quot;ffoot&quot; name=&quot;frictionffoot&quot;/&gt; &lt;/actuator&gt; ``` def exclude_wind_friction_from_action_space(self): &quot;&quot;&quot;用来实现风摩擦的额外执行器不应该是代理动作空间的一部分！&quot;&quot;&quot; bounds = self.model.actuator_ctrlrange.copy().astype(np.float32)[:-7] low, high = bounds.T self.action_space = Box(low=low, high=high, dtype=np.float32) &lt;/details&gt; 欢迎任何反馈/建议:)    由    /u/CertainLoad1589  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecjura/how_to_realize_wind_frictions_acting_on_the/</guid>
      <pubDate>Fri, 26 Jul 2024 09:30:06 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 CORL 和 D4RL 数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecdsgd/how_to_setup_corl_and_d4rl_datasets/</link>
      <description><![CDATA[我正在尝试使用从 D4RL 下载的 maze2d-umaze-v1 数据集运行 sac_n.py。我在 sac_n.py 上使用 github 的 CORL 实现。  我是一名新手，正在尝试弄清楚如何使用下载的数据集（当前位于我的 ~/.d4rl/datasets 文件夹中）作为 sac_n.py 文件的输入。  我目前在 VScode 文件夹中将两个文件并排放在一起，但正在努力寻找有意义的突破，以便使用数据集下载文件作为 ORL 算法文件的输入。 提前感谢您的时间和考虑。    提交人    /u/Constant_Koala_7744   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecdsgd/how_to_setup_corl_and_d4rl_datasets/</guid>
      <pubDate>Fri, 26 Jul 2024 03:07:12 GMT</pubDate>
    </item>
    </channel>
</rss>