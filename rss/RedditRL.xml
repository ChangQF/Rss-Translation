<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 05 Apr 2024 00:58:24 GMT</lastBuildDate>
    <item>
      <title>普通策略梯度在简单的网格世界任务上失败</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bw4gxj/vanilla_policy_gradient_failing_on_a_simple/</link>
      <description><![CDATA[      我编写了一个普通的策略梯度，并尝试在网格世界（冰冻湖）任务上对其进行训练。当代理正在学习时，它非常嘈杂，这令人惊讶，因为任务很微不足道。 这是我的代码&lt; /a&gt; 这是结果 - https://preview.redd.it/hf615ri06ksc1.png?width=599&amp;format=png&amp;auto=webp&amp;s=05c3d63454b5be6136c387ac6adcb9ce22eedcca 这是环境 - https://preview.redd.it/gljggve46ksc1.png?width=309&amp;format=png&amp;auto=webp&amp;s =7fef7793eb2fcc64d71853a93cbe98a066a8e2eb 有人可以帮我理解问题是什么吗？我想保持简单，如果可能的话不添加任何优势计算。   由   提交 /u/Academic-Rent7800    reddit.com/r/reinforcementlearning/comments/1bw4gxj/vanilla_policy_gradient_failing_on_a_simple/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bw4gxj/vanilla_policy_gradient_failing_on_a_simple/</guid>
      <pubDate>Fri, 05 Apr 2024 00:49:39 GMT</pubDate>
    </item>
    <item>
      <title>积极与消极奖励+可屏蔽行动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bvyqig/positive_vs_negative_rewards_maskable_actions/</link>
      <description><![CDATA[我与 GPT3.5 就奖励机制进行了对话。 这是否足够，或者有什么遗漏？您会在这些答案中添加其他内容吗？ ---- 在强化学习中，当我们提供负奖励和正奖励时，代理是否会学得更好，或者正奖励就足够了？ ​  在强化学习中，将负奖励与正奖励一起使用的有效性取决于具体的任务和环境代理经营所在的地方。积极和消极的奖励都有助于指导学习过程，但它们的目的不同。  积极奖励通常用于强化期望的行为或行动，使代理人更接近实现其目标。这些奖励向代理提供反馈，表明其处于正确的轨道上，并鼓励其继续执行这些操作。  负奖励，也称为惩罚或惩罚，用于阻止导致代理偏离其目标或走向失败状态的不良行为或行动。通过接受负奖励，智能体学会在未来避免这些行为，从而改善其决策过程。  在许多情况下，结合使用积极和消极的奖励可以带来更有效的学习。正向奖励会激励智能体探索和发现有益的策略，而负向奖励则可以帮助智能体了解应该避免哪些行为，并可以通过对不良选择提供即时反馈来加快学习过程。  然而，奖励函数的设计至关重要，因为奖励指定不当可能会导致意外行为或学习失败。必须仔细考虑平衡正向奖励和负向奖励，以确保智能体有效地学习所需的行为。  在某些情况下，仅使用积极的奖励就有可能获得令人满意的表现，特别是在较简单的环境或最佳行为相对简单的任务中。然而，在具有多个目标或潜在失败状态的更复杂的环境中，使用负奖励可以极大地增强学习过程并带来更好的性能。  ​ 好的，但是可屏蔽操作呢？如果我们掩盖所有不良行动的可能性，智能体就不会学会不采取这些行动。在这种情况下，最好是强迫代理犯一点错误吗？   可屏蔽操作是指可以屏蔽或阻止代理根据以下条件选择的操作：某些条件或限制。在某些情况下，掩盖所有不良行为的可能性确实会阻碍学习过程，特别是当智能体没有暴露于这些不良行为的后果时。  在所有不良可能性都被掩盖的情况下，代理可能没有机会了解为什么这些行为是不可取的。这种暴露的缺乏可能会导致政策不够稳健，并且对未见过的情况的概括性有限。  在这种情况下，在代理的行为中引入一定程度的探索或随机性可能是有益的，即使其中一些行为会导致负面结果。这种探索使智能体能够从错误中学习，并更好地理解不同行为的后果，即使它们最初被掩盖了。  这种方法通常被称为“噪声探索”。平衡探索的需要和避免明显不良行为的愿望。通过允许代理偶尔犯错误，它可以从这些经验中学习并随着时间的推移改进其决策过程。  但是，应该仔细控制探索的程度，以防止代理重复做出有害的操作，从而显着降低其性能。诸如 epsilon-greedy 探索之类的技术，其中代理选择具有一定概率 epsilon 的随机动作，可用于调节探索与利用的权衡。  总体而言，虽然掩盖不良行动的可能性可以提供安全的学习环境，但引入受控探索可以帮助代理更有效地学习并制定更稳健的策略。  ​   由   提交/u/dark0618  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bvyqig/positive_vs_negative_rewards_maskable_actions/</guid>
      <pubDate>Thu, 04 Apr 2024 21:00:20 GMT</pubDate>
    </item>
    <item>
      <title>“使用前瞻树搜索的序列到序列神经网络系统”，Leblond 等人 2022 {DM}（美国专利申请#US20240104353A1）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bvxjxi/sequenceto_sequence_neural_network_systems_using/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bvxjxi/sequenceto_sequence_neural_network_systems_using/</guid>
      <pubDate>Thu, 04 Apr 2024 20:15:14 GMT</pubDate>
    </item>
    <item>
      <title>艾萨克健身房要求帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bvrl5d/isaac_gym_requirements_help/</link>
      <description><![CDATA[嗨，我是一名大学生，我是这个世界的新手。我被选中参加一个需要 isaac 健身房的项目。由于我的电脑有点旧，我想知道你们是否可以帮助我满足我需要的系统要求（以便为此目的组装最好的电脑）。  现在我知道我可以在互联网上搜索它，但我只找到了 isaacSim 的要求，我真的不知道它们是否与 isaacgym 完全相同。 请原谅我的无知，但我对这一切都很陌生。谢谢！   由   提交/u/fartinsim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bvrl5d/isaac_gym_requirements_help/</guid>
      <pubDate>Thu, 04 Apr 2024 16:27:49 GMT</pubDate>
    </item>
    <item>
      <title>SAC + HER 的成功率不能超过 0.8 左右</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bvhj3t/sac_her_cant_exceed_success_rate_around_08/</link>
      <description><![CDATA[    的成功率   亲爱的大家，我正在研究小型无人船自主导航的算法。我使用 sb3 SAC + HER 缓冲液进行训练。 env 的规则非常简单。   重置后的飞船位于观察空间[0.5,0,5]的中间。  然后在空间[0-1,0-1]上随机选择目的地。奖励计算为 - 距当前位置和目标的距离（欧几里德）。  成功定义为船舶接近目标点（距目标的半径为 0.05）。 完成是指船舶到达目标或撞到墙壁（并获得负奖励-1)。 动作空间为spaces.Box(low=np.array([-1, 0]), high=np.array([1, 1] ），它映射到航向变化和速度的变化，如 action = np.array([action[0]*10, action[1] * 10], dtype=np.float32)。 模型定义为： model = SAC(&quot;MultiInputPolicy&quot;, env, buffer_size=buffer_size, replay_buffer_class=HerReplayBuffer, replay_buffer_kwargs=dict( n_sampled_goal=4, goal_selection_strategy=&#39;future&#39;, copy_info_dict=True ), verbose=1,batch_size=batch_size, gamma=gamma,learning_rate=learning_rate,policy_kwargs=dict(net_arch=net_arch), tensorboard_log=log_dir,learning_starts=8000)  我尝试使用以下值优化 optuna 库中的超参数： buffer_size = Trial .suggest_categorical(&#39;buffer_size&#39;, [100000])  batch_size = Trial.suggest_categorical(&#39;batch_size&#39;, [64,128])  gamma = Trial.suggest_loguniform(&#39;gamma&#39;, 0.95 , 0.99) learning_rate = Trial.suggest_loguniform(&#39;learning_rate&#39;, 6.7e-4, 8.5e-4)  net_arch = Trial.suggest_categorical(&#39;net_arch&#39;, [[1024 , 1024, 1024],[2048,2048,2048],[1024,1024,1024,1024]])  经过多次试验，我估计成功率不能超过0.85。环境很简单，我是根据 https://highway-env.farama.org/environments/parking/&lt; /a&gt;.我的环境中的运动模型很简单。请给我一些建议，因为我坚持了几个星期。谢谢!!! ​ https://preview.redd.it/xdznm9q05fsc1.png?width=1913&amp;format=png&amp;auto=webp&amp;s=b9c500b213b32993d5b5bf7dd4fd2 5772280b581 https://preview.redd.it/smglb5i22fsc1.png?width= 962&amp;format=png&amp;auto=webp&amp;s=af42ede221c5e64048f113e4c162618ed4efd02d https://preview.redd.it/5xosgnl42fsc1.png?width=3002&amp;format=png&amp;auto=webp&amp;s=f2a2c3068451c98224fbd97cd4 e6e8d67b84b816   由   提交/u/Sharp-Record1600  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bvhj3t/sac_her_cant_exceed_success_rate_around_08/</guid>
      <pubDate>Thu, 04 Apr 2024 07:54:59 GMT</pubDate>
    </item>
    <item>
      <title>斯坦福 CS 25 变形金刚课程（向所有人开放 | 明天开始）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bve6ua/stanford_cs_25_transformers_course_open_to/</link>
      <description><![CDATA[Tl;dr：斯坦福大学最热门的研讨会课程之一。我们通过 Zoom 向公众开放课程。讲座将于明天（星期四）下午 4:30-5:50（太平洋夏令时间）开始，地点为 Zoom 链接。课程网站： https://web.stanford.edu/class/cs25/&lt; /a&gt; 对风靡全球的深度学习模型《变形金刚》感兴趣吗？想与研究人员进行亲密讨论吗？如果是这样，那么本课程适合您！您并不是每天都能亲自听到您所读论文的作者的来信并与他们聊天！ 每周，我们都会邀请处于 Transformers 研究前沿的人们讨论法学硕士架构的最新突破从 GPT 和 Gemini 到生成艺术（例如 DALL-E 和 Sora）、生物学和神经科学应用、机器人技术等方面的创意用例！ CS25 已成为斯坦福大学最热门、最令人兴奋的研讨会课程之一。我们邀请了最酷的演讲者，如 Andrej Karpathy、Geoffrey Hinton、Jim Fan、Ashish Vaswani 以及来自 OpenAI、Google、NVIDIA 等的人员。我们的课程在斯坦福大学内外广受欢迎，&lt;&lt;的总观看次数约为 100 万次。 a href=&quot;https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&quot;&gt;YouTube。我们的 Andrej Karpathy 课程是 2023 年斯坦福大学上传的第二受欢迎的 YouTube 视频，观看次数超过 50 万意见！ 我们在 2024 年春季进行了重大改进，包括大型演讲厅、专业录音和直播（向公众）、社交活动和潜在的一对一网络！学生唯一的作业是每周参加讲座/讲座。此外，所有人都可以进行直播和审核。欢迎亲自审核或加入 Zoom 直播。 我们还有一个用于 Transformers 讨论的 Discord 服务器（超过 1500 名成员）。我们将其作为一个“变形金刚社区”向公众开放。欢迎加入并与数百名其他人讨论变形金刚！ P.S.是的，谈话将被记录！它们可能会在 YouTube 上上传并提供。每次讲座后 2 周。   由   提交/u/MLPhDStudent  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bve6ua/stanford_cs_25_transformers_course_open_to/</guid>
      <pubDate>Thu, 04 Apr 2024 04:23:13 GMT</pubDate>
    </item>
    <item>
      <title>DQN 更新</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bv7iwk/update_of_dqn/</link>
      <description><![CDATA[       社区您好，我需要您的帮助 我正在使用自定义的 DQN 网络，该网络由四个独立的输出层 A、B、C 和 D 组成，每个输出层输出一个操作。并根据A（有3个离散输出）的输出，选择B、C或D中所选择的动作（仅执行一组动作）。在这种情况下，我不知道更新函数是否正确： https://preview.redd.it/sy29zf4xicsc1.png?width=1729&amp;format=png&amp;auto=webp&amp;s=8bfe4547eeb750aee4117b0902308 cb9a10463f5 https://preview.redd.it/ug0rws02jcsc1.png ?width=1750&amp;format=png&amp;auto=webp&amp;s=37a8628663ae4db1b971dd49a770b01307f66a21   由   提交/u/GuavaAgreeable208  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bv7iwk/update_of_dqn/</guid>
      <pubDate>Wed, 03 Apr 2024 23:14:15 GMT</pubDate>
    </item>
    <item>
      <title>A2C反复学习和死亡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bv2mbh/a2c_learns_and_dies_repeatedly/</link>
      <description><![CDATA[      我目前正在致力于解决倒立摆问题的 A2C 实施。 （奖励 = 0 * 高度{0-1}**2，剧集长度 = 1000，下降率= 0.98） 出于某种原因，尽管它在学习过程中不断死亡，并且对 750 多个游戏没有任何作用。这很奇怪，因为我在将优势输入到参与者网络之前将其标准化，将 π(s|a) 上限设置为 0.05（这样 A/π(s|a) 就不会爆炸），学习率为 0.001演员为 0.005，评论家为 0.005，并且网络不应过度拟合（演员：3-16-16-3，评论家：4-32-16-1）。此外，如果它获得如此低的分数，则意味着它在 90% 的情况下选择不执行 3 个操作。我唯一能想到的可能是评论家网络会感到困惑，因为在扣除未来奖励后，状态的值范围可以从 -20 到 60。 我认为这不是问题在环境、奖励或参与者网络中，因为我的 REINFORCE 实现具有平滑的学习曲线并获得相当好的性能 编辑： https://preview.redd.it/5s5vk93jwgsc1.png?width=2130&amp;format=png&amp; auto=webp&amp;s=c8aba0f8bcfe32f0242de2a6bb56501930af3467 我认为这绝对是批评家的问题。由于某种原因，在某些时候它会收到每个时间步大约 20000 个错误值。   由   提交/u/AUser213  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bv2mbh/a2c_learns_and_dies_repeatedly/</guid>
      <pubDate>Wed, 03 Apr 2024 20:01:15 GMT</pubDate>
    </item>
    <item>
      <title>适用于表格 RL 环境的软件包？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1buy67d/packages_for_tabular_rl_environments/</link>
      <description><![CDATA[是否有一个库，其中包含健身房/体育馆格式的流行 RL 环境的表格版本？理想情况下，状态空间是一个单热向量，它同时具有确定性和随机环境，以及稀疏和密集的奖励。似乎这是一个常见的需求，但除了简单的 gridworld 包之外我找不到任何东西。   由   提交 /u/asdfwaevc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1buy67d/packages_for_tabular_rl_environments/</guid>
      <pubDate>Wed, 03 Apr 2024 17:12:45 GMT</pubDate>
    </item>
    <item>
      <title>还有其他 RLHF/数据注释/标签公司吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bupqu8/any_other_rlhfdata_annotationlabeling_company/</link>
      <description><![CDATA[尝试比较和编写所有 RLHF 和数据注释/标签公司的工作。这是我的清单，你知道我错过了什么吗？谢谢！ Scale Labelbox Argilla Toloka SuperAnnotate HumanSignal Kili Watchfull Datasaur.ai Refuel iMerit Anote M47 Snorkel Ango AI AIMMO Alegion Sama CloudFactory    ;由   提交 /u/MeowCatalog   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bupqu8/any_other_rlhfdata_annotationlabeling_company/</guid>
      <pubDate>Wed, 03 Apr 2024 11:01:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么我在训练循环中陷入这个永无休止的输出？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bul9f2/why_am_i_stuck_on_this_never_ending_output_for/</link>
      <description><![CDATA[   /u/One-Character-1724   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bul9f2/why_am_i_stuck_on_this_never_ending_output_for/</guid>
      <pubDate>Wed, 03 Apr 2024 06:02:00 GMT</pubDate>
    </item>
    <item>
      <title>《人工智能奥林匹克数学竞赛-进步一等奖》（截止日期：2024-06-27，3个月）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bug017/ai_mathematical_olympiad_progress_prize_1/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bug017/ai_mathematical_olympiad_progress_prize_1/</guid>
      <pubDate>Wed, 03 Apr 2024 01:24:17 GMT</pubDate>
    </item>
    <item>
      <title>如何使用反向传播奖励正确计算多个情节之间的损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1buefgg/how_to_properly_calculate_loss_among_multiple/</link>
      <description><![CDATA[我正在研究一种算法，其中一个情节可以是 X 个时间步长，直到达到奖励。当达到奖励时，我们将该值反向传播到本集中之前的所有时间步。我知道诸如合并衰减之类的方法，但在这种情况下，我希望将一个情节的所有动作视为对产生奖励具有同等影响力。 话虽这么说，我正在对所有情节进行训练一次，模型的损失将从所有剧集的样本中计算出来。我面临的困境是试图了解在每集的基础上对样本进行加权是否更有意义。 一方面，如果我们在每集的基础上对样本进行加权（平均误差）对于剧集中的所有样本），我们将所有剧集视为相同。例如，即使一个情节只有 10 个样本，我们也会关心预测这些样本，就像假设一个情节有 1000 个样本一样。 但是，我也看到了想要只处理的观点单独的样本，因为从技术上讲，正确预测样本 A 与正确预测样本 B 一样重要。 不确定此类问题是否有先例。任何指导将不胜感激。   由   提交/u/Yogi_DMT  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1buefgg/how_to_properly_calculate_loss_among_multiple/</guid>
      <pubDate>Wed, 03 Apr 2024 00:11:23 GMT</pubDate>
    </item>
    <item>
      <title>欧洲 RL 博士</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bu2p2n/rl_phd_in_europe/</link>
      <description><![CDATA[您知道我可以在欧洲申请博士学位的实验室吗？    由   提交 /u/Peight_een   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bu2p2n/rl_phd_in_europe/</guid>
      <pubDate>Tue, 02 Apr 2024 16:13:10 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>