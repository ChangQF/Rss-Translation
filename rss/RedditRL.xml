<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 07 Jul 2024 09:16:08 GMT</lastBuildDate>
    <item>
      <title>使用 MuZero 训练现代桌面游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dx15bv/using_muzero_to_train_modern_tabletop_games/</link>
      <description><![CDATA[嗨。我在一家棋盘游戏公司（捷克游戏版）工作，我对 AI 玩现代棋盘游戏非常感兴趣。我想看看 MuZero 是否是一个值得尝试的好选择。我读过很多文章和教程，探索过 MuZero 的几种实现，包括 C++ 变体和 OpenSpiel。我只找到了相对简单的游戏。我试图回答几个问题： 1) 我可以用现有的计算能力训练一款普通的现代棋盘游戏吗？即使使用普通 GPU 几天/几周，我也能得到结果吗？ 2) 如何从棋盘、几个令牌库以及几个玩家和全局单数指针创建观察空间？ 3) 如果我想将特定数字而不是随机数放入推理中以测试 AI 与现场玩家之间的对抗，那么它应该是一个新的虚拟玩家（“游戏管理员”）吗？ 4) 如果游戏板发生变化，但在一个游戏会话中保持不变，我可以将其“形状”添加到观察空间，以便 AI 能够玩其他类似的棋盘吗？ 到目前为止，对于我们的游戏，我们一直非常成功地使用简单的 MCTS，仅用于确定单个玩家移动中的动作（我们不模拟对手的动作）。我们没有使用神经网络的经验。我觉得我们可以创造一个更强大的 AI 对手，所以我正在学习和探索。    提交人    /u/damucz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dx15bv/using_muzero_to_train_modern_tabletop_games/</guid>
      <pubDate>Sat, 06 Jul 2024 22:16:03 GMT</pubDate>
    </item>
    <item>
      <title>新的（更具数学性的）强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</link>
      <description><![CDATA[嗨  我已经参加了强化学习课程，我认为我对这些概念有很好的掌握，但正在寻找强化学习领域算法开发的当前前沿 有什么算法 / 主题的想法可以让我开始吗？    提交人    /u/Total-Ad-4461   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 17:09:36 GMT</pubDate>
    </item>
    <item>
      <title>张量的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwpl60/the_problem_with_tensors/</link>
      <description><![CDATA[大家好，我在使用 DQN 时遇到了一个问题，即张量。当模型开始训练时，模型会收到以下维度的张量：torch.Size([1, 64, 64])，大约 10 秒后，模型会收到以下数据：torch.Size([32, 1, 64, 64])，因此我们收到此错误： Traceback (most recent call last): File &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;, line 141, in &lt;module&gt; agent.update_model(state, new_state, action, reward, compl) 文件 &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;，第 95 行，在 update_model 中 q_values = self.model(states) ^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl 中 return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;，第 40 行，在 forward x = self.linear_block(x) ^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在_wrapped_call_impl 返回 self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl 中返回 forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\container.py&quot;，第 217 行，在 forward 输入 = module(input) ^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl 中返回 self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl 中返回 forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\linear.py&quot;，第 116 行，在正向返回 F.linear(input, self.weight, self.bias) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: mat1 和 mat2 形状无法相乘 (32x1024 和 16x11) [56.0 秒内完成]  这是模型更新代码： def update_model(self, state, new_state, action, reward, compl): # 用 память self.memory.append([state, new_state, action, reward, compl]) # 完成页面后，如果 len(self.memory) &gt; 则返回结果self.max_memory_size: self.memory.pop(0) # 获取模型，然后根据条件判断 if len(self.memory) &gt;= self.batch_size: # 随机取值 batch = random.sample(self.memory, self.batch_size) # 随机取值 states = torch.stack([transition[0] for transition in batch]) new_states = torch.stack([transition[1] for transition in batch]) action = torch.tensor([transition[2] for transition in batch]) rewards = torch.tensor([transition[3] for transition in batch]) finishes = torch.tensor([transition[4] for transition in batch]) # 计算 Q 值 q_values = self.model(states) target_q_values = q_values.clone() max_next_q = torch.max(self.model(new_states), dim=1)[0] target_q_values[torch.arange(self.batch_size), action] = rewards + self.gamma * (1 - finishes) * max_next_q # 计算并返回 loss = self.criterion(q_values, target_q_values) self.optimizer.zero_grad() loss.backward() self.optimizer.step()     提交人    /u/Kepler-nn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwpl60/the_problem_with_tensors/</guid>
      <pubDate>Sat, 06 Jul 2024 13:27:10 GMT</pubDate>
    </item>
    <item>
      <title>如何开始学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwpicd/how_to_start_learning/</link>
      <description><![CDATA[大家好，我是新来的，我对使用强化学习创建自己的项目非常感兴趣。我想为自己做一些小项目，这可能会帮助我建立自己的投资组合。  关于我应该从哪里开始学习？什么/哪里是最好的源材料。    提交人    /u/Educational-Gene3665   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwpicd/how_to_start_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 13:23:22 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些帮助来完成我正在进行的项目，即微尺度的路径规划算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</guid>
      <pubDate>Sat, 06 Jul 2024 11:19:31 GMT</pubDate>
    </item>
    <item>
      <title>您好，我正在使用 DDPG 进行四轴飞行器的轨迹跟踪，根据这个训练图我可以得出什么结论？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwm21c/hello_im_using_ddpg_for_trajectory_tracking_for_a/</link>
      <description><![CDATA[        由    /u/OkFig243   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwm21c/hello_im_using_ddpg_for_trajectory_tracking_for_a/</guid>
      <pubDate>Sat, 06 Jul 2024 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>DQN 在自定义健身环境中无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</link>
      <description><![CDATA[大家好， 我一直想尝试自定义 gym 环境。我使用 tensorflow 用 DQN 解决了​​冰冻湖问题 - 并且想创建一个类似的环境并用 DQN 解决它。 我的环境是一个 4x5 网格单元仓库，里面有一个机器人、一个目标和一个障碍物。所以，与冰冻湖非常相似。 我遇到的问题是，一旦 epsilon 衰减，并且算法正在采取贪婪行动，它采取的行动就是错误的（总是相同的，取决于初始化，但例如，它会告诉机器人始终向上） 这是我尝试过的和我所知道的：  环境应该正常工作。我对其进行了广泛的测试，&amp;用经典 Q 学习解决了这个问题，而且效果很好 我几乎从我的 frosting lake 代码中复制/粘贴了我的 DQN 代码。  我玩过超参数，增加了网络的复杂性，但没有任何效果，而且我对此持怀疑态度，因为我的自定义环境和 frosting lake 之间应该没有太大区别（我想？）  知道我应该在哪里寻找吗？我可以在这里分享代码，但它有点密集，我不确定要分享哪一部分，所以我不想淹没，但如果你想要代码的任何特定部分，我很乐意分享     提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</guid>
      <pubDate>Sat, 06 Jul 2024 08:01:03 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 制作一款回合制策略游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</link>
      <description><![CDATA[        由    /u/Novel_Can_6870  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</guid>
      <pubDate>Sat, 06 Jul 2024 02:23:55 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理的行为正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</link>
      <description><![CDATA[      大家好 我刚刚创建了我的第一个 PPO 代理。在 Cartpole-v1 环境中进行训练时，我收到以下响应： https://preview.redd.it/8kufxuh59sad1.png?width=640&amp;format=png&amp;auto=webp&amp;s=ddf17252bea091b0738973645612f1095364489e 为什么在代理似乎最终收敛时，性能在 500 次迭代后仍会出现一些突然下降的情况？ 谢谢前进。 编辑：在实施 GAE 之后，经过几次迭代后，时间线在 cartpole 步骤上完全稳定（不幸的是，无法在编辑中分享图表）。    提交人    /u/Muscle_Robot   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</guid>
      <pubDate>Fri, 05 Jul 2024 23:13:27 GMT</pubDate>
    </item>
    <item>
      <title>移动平均数对于评估模型性能是否必要</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dw2kxk/is_moving_average_necessary_for_evaluating_model/</link>
      <description><![CDATA[我正在用 Pong 游戏练习 RL！    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dw2kxk/is_moving_average_necessary_for_evaluating_model/</guid>
      <pubDate>Fri, 05 Jul 2024 16:52:05 GMT</pubDate>
    </item>
    <item>
      <title>Mamba 背后的思想：Albert Gu 谈论状态空间模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</link>
      <description><![CDATA[        由    /u/phoneixAdi  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</guid>
      <pubDate>Fri, 05 Jul 2024 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>赛车开放式 AI 健身房 - 推出是否应覆盖整个赛道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</link>
      <description><![CDATA[我试图复制此代码，但对 rollout 生成感到非常困惑。 步骤 4：生成随机 rollout 对于赛车环境，VAE 和 RNN 都可以使用随机 rollout 数据 - 即通过在每个时间步随机采取行动而生成的观察数据。实际上，我们使用伪随机动作，迫使汽车最初加速，以使其离开起跑线。 每次推出都应该覆盖整个赛道吗？ https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459 我一直在尝试运行代码，但推出后赛道从未完成。训练后我也看不到它完成。    提交人    /u/Competitive-Chip5872   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</guid>
      <pubDate>Fri, 05 Jul 2024 06:15:32 GMT</pubDate>
    </item>
    <item>
      <title>针对多个 TSP/VRP/和其他变体的多智能体强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</link>
      <description><![CDATA[你好，我是一名最近开始进行 RL 研究的学生。 我对包括 CO 和 MARL 在内的领域很感兴趣。即使有很多好的 RL 和启发式算法，我也想在更大规模和更复杂的领域尝试一些实验。 但是，对于多旅行商问题或 VRP，我很难找到 SOTA MARL 方法。（我在 github 上找到了一些论文，但通常没有代码实现） 除了论文之外，如果你能分享你的知识和见解，那将非常有帮助。    提交人    /u/QingdaoCraftBeer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</guid>
      <pubDate>Fri, 05 Jul 2024 04:57:29 GMT</pubDate>
    </item>
    <item>
      <title>“在 HATETRIS 中创下世界纪录”，Dave & Filipe 2022（AlphaZero 失败后高度优化的光束搜索）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</guid>
      <pubDate>Thu, 04 Jul 2024 16:14:25 GMT</pubDate>
    </item>
    <item>
      <title>“AlphaZero 的蒙特卡洛图搜索”，Czech 等人 2020 年（将树转换为 DAG 以节省空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</guid>
      <pubDate>Thu, 04 Jul 2024 16:01:49 GMT</pubDate>
    </item>
    </channel>
</rss>