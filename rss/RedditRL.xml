<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 25 Jun 2024 12:28:07 GMT</lastBuildDate>
    <item>
      <title>理解和诊断深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</link>
      <description><![CDATA[发表于 ICML 2024 https://openreview.net/pdf?id=s9RKqT7jVM    由   提交  /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</guid>
      <pubDate>Tue, 25 Jun 2024 09:31:31 GMT</pubDate>
    </item>
    <item>
      <title>HPC 集群中超参数搜索和训练速度缓慢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1cim/slow_hyperparameter_search_and_training_in_hpc/</link>
      <description><![CDATA[大家好， 我目前正在使用 PPO 训练自主决策卫星星座，最多可进行 86400 个模拟步骤（每秒 1 步）。尽管使用了强大的硬件，包括 4 个 NVIDIA A100 GPU 和 11 个 CPU，但训练过程仍然非常缓慢。以下是我的设置和配置的摘要： **软件和库版本：** * **容器基础：**NVIDIA PyTorch 22.09 py3 * **库：** numpy==1.23.5 gymnasium==0.28.1 matplotlib==3.7.1 pandas==1.5.3 ray==2.10.0 ray[tune]==2.10.0 typer==0.7.0 dm_tree tree scikit-image lz4 gputil==1.4.0 pyarrow **环境配置：** * num_targets：10 * num_observers：10 * time_step：1 秒 * 持续时间：86400 秒 - 24 小时 **训练配置：** * batch_mode：“complete_episodes” * rollout_fragment_length：“auto” * num_rollout_workers：10 * num_envs_per_worker：1 * num_cpus_per_worker：1 * num_gpus_per_worker：0 * num_learner_workers：4 * num_cpus_per_learner_worker: 1 * num_gpus_per_learner_worker: 1 **搜索空间配置：** * fcnet_hiddens: [[64, 64], [128, 128], [256, 256], [64, 64, 64]] * num_sgd_iter: [10, 30, 50] * lr: [1e-5, 1e-3] * gamma: [0.9, 0.99] * lambda: [0.9, 1.0] * train_batch_size: [512, 1024, 2048, 4096] * sgd_minibatch_size：[32, 64, 128, 512] 训练过程非常缓慢，单次迭代 1 小时都无法完成（30 个样本的超参数搜索，20 次迭代将花费一个多月）。我期望使用 4 个 A100 GPU 进行更快的训练。以下是我尝试和观察到的几件事： * 大部分时间都花在训练上，而不是环境模拟上。使用 10 个推出的工作人员、每个工作人员 1 个环境和每个工作人员 1 个 CPU，大约需要 5 分钟来模拟环境。 * 将每个工作人员的 CPU 数量增加到 7 个（70 个 CPU）实际上减慢了该过程，需要 15 分钟左右才能完成模拟。 * 我可以在我的 MacBook Pro M2（大约 20 次迭代）以及 NVIDIA Jetson AGX Orin 上进行训练，但当然每次迭代大约需要 90 分钟，而且它们的批次大小更小并且参数经过调整。因此，我希望使用新硬件进行更快的训练。 * 我不完全理解以下示例中的这一部分：0.0/1.0 accelerater_type:A100 这里有一个例子： 试用状态：1 正在运行 | 3 PENDING 当前时间：2024-06-25 10:33:42。总运行时间：1小时1分6秒 逻辑资源使用情况：11.0/80 CPU、4.0/4 GPU（0.0/1.0 accelerater_type:A100） ╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮ │ 试验名称状态 gamma lr train_batch_size sgd_minibatch_size num_sgd_iter lambda model/fcnet_hiddens │ ═──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤ │ PPO_FSS_env-v0_17a5c_00000 正在运行 0.928133 1.38519e-05 2048 128 30 0.905579 [128, 128] │ │ PPO_FSS_env-v0_17a5c_00001 正在处理 0.972168 8.7419e-05 1024 128 10 0.957066 [256, 256] │ │ PPO_FSS_env-v0_17a5c_00002 待定 0.906353 2.12536e-05 2048 512 50 0.968513 [64, 64] │ │ PPO_FSS_env-v0_17a5c_00003 待定 0.948159 0.000202029 512 64 30 0.997584 [64, 64] │ ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ 如有任何关于进一步优化训练过程的建议或见解，我们将不胜感激！    提交人    /u/CLEMENMAN   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1cim/slow_hyperparameter_search_and_training_in_hpc/</guid>
      <pubDate>Tue, 25 Jun 2024 08:51:31 GMT</pubDate>
    </item>
    <item>
      <title>“探索大型语言模型中上下文学习的决策边界”，Zhao 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnud65/probing_the_decision_boundaries_of_incontext/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnud65/probing_the_decision_boundaries_of_incontext/</guid>
      <pubDate>Tue, 25 Jun 2024 01:41:05 GMT</pubDate>
    </item>
    <item>
      <title>“主题：人工智能反馈中的内在动机”，Klissarov 等人 2023 {FB}（Nethack 法学硕士的标签表明这是一种学习奖励）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntush/motif_intrinsic_motivation_from_artificial/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntush/motif_intrinsic_motivation_from_artificial/</guid>
      <pubDate>Tue, 25 Jun 2024 01:14:09 GMT</pubDate>
    </item>
    <item>
      <title>“神经语言代理的差异历史”，Piterbarg 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntmy1/diff_history_for_neural_language_agents_piterbarg/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntmy1/diff_history_for_neural_language_agents_piterbarg/</guid>
      <pubDate>Tue, 25 Jun 2024 01:02:47 GMT</pubDate>
    </item>
    <item>
      <title>“使用 LLM 玩 NetHack：作为零次元特工的潜力与局限性”，Jeurissen 等人 2024 (gpt-4-turbo)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntgl1/playing_nethack_with_llms_potential_limitations/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntgl1/playing_nethack_with_llms_potential_limitations/</guid>
      <pubDate>Tue, 25 Jun 2024 00:54:08 GMT</pubDate>
    </item>
    <item>
      <title>强化学习库在超参数搜索和优化方面的现状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnromj/the_current_state_of_rl_libraries_in_terms_of/</link>
      <description><![CDATA[目前有许多库和框架为 RL 提供基线和即插即用算法。  由于众所周知 RL 对超参数非常敏感，我想问 RL 社区，在您使用的库和框架中，它们在系统超参数搜索和优化方面的经验排名如何？哪一个为其提供了更好的界面和/或效率更高？或者即使您使用过任何第三方库。     提交人    /u/Human_Professional94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnromj/the_current_state_of_rl_libraries_in_terms_of/</guid>
      <pubDate>Mon, 24 Jun 2024 23:28:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 制作动态视频游戏 NPC [博客文章]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnr4lz/making_dynamic_video_game_npcs_with_ppo_blog_post/</link>
      <description><![CDATA[查看我的强化学习博客文章 https://macjgames.net/Blog_Posts/RL_Shooter_NPC.html 我下一步应该做什么样的项目？ ![video]( &quot;战斗学校！&quot;)    提交人    /u/theLanguageSprite   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnr4lz/making_dynamic_video_game_npcs_with_ppo_blog_post/</guid>
      <pubDate>Mon, 24 Jun 2024 23:02:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习课程项目帮助/建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnpj7g/machine_learning_course_project_helpadvice/</link>
      <description><![CDATA[大家好，我在大学的机器学习课程中需要做一个项目，在这个项目中，我将训练一个人工智能在教授给我们的 pybullet 模拟器中玩乒乓球，通过在我的代码中添加客户端，我可以调用 get_state() 从模拟器中获取 37 个状态值，通过 send_joints() 我可以发回构成玩游戏的机器人的 11 个关节部位的运动。在最终的比赛中，AI 必须能够在 uni pc 上运行，因此我无法导入已编码的模型库，这就是为什么我让 OpenAI 的 ddpg 实现启动起来（我尝试使用 ppo，但找不到不需要修改多个文件即可工作的模型，并且对于连续空间，我们在课堂上只讨论了 ddpg），我使用 gymnasium 通过在里面插入客户端来创建自定义环境（可能可以用它来训练 ai，但不能用它来运行 AI），我实现环境和 ai 的方式非常简单，获取状态，计算移动球拍以击中球的确切点，将点和我从 37 个状态变量中提取的值作为输入（我认为其中一些是 ai 不需要的）让他吐出关节并将它们发送到环境中。 我的奖励函数也很简单，对移动过多的 ai 进行惩罚，惩罚等于预测点和桨当前位置之间的距离，最后给予非常大的得分奖励。 最不确定的是奖励函数和超参数的选择。如果您想要查看我当前的代码，请访问我的 github https://github.com/MaxiMoraru/ddpg，我知道它很混乱，但重要的文件是 core.py（取自 openAI spinning up） ddpg.py（spinning up 实现，做了一些小改动，例如删除了记录器） custom_gym_env.py（我的环境使用 gym 制作，并解释了状态变量） start.bat（仅使用 1 个命令启动服务器和客户端）。 欢迎提出任何批评/建议，提前谢谢大家！ 编辑：忘记补充说，服务器每秒发送 50 次状态并期望关节返回，我得到大约 800集数/小时我应该期待多少小时的训练？    提交人    /u/Expensive_Internet17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnpj7g/machine_learning_course_project_helpadvice/</guid>
      <pubDate>Mon, 24 Jun 2024 21:53:31 GMT</pubDate>
    </item>
    <item>
      <title>在 ROS+Gazebo 中开始使用 DAgger</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</link>
      <description><![CDATA[大家好， 由于普通的强化学习未能满足我的期望，因此我希望转向模仿学习。这是一个基于操纵器的项目，带有眼手深度摄像头。我有一个可以成为专家的经典算法。所有这些都是在 ROS+gazebo 中完成的。 我该如何使用模仿学习来做到这一点。具体来说，我可以使用任何现成的库吗？我如何在 Gazebo 中保存来自我的专家的演示。如果有人以前这样做过    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnod0l/getting_started_with_dagger_in_rosgazebo/</guid>
      <pubDate>Mon, 24 Jun 2024 21:03:55 GMT</pubDate>
    </item>
    <item>
      <title>这难道不是《IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO》这篇论文中的问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</link>
      <description><![CDATA[      我正在阅读这篇论文：“深度强化学习中的实施问题：PPO 和 TRPO 案例研究” [pdf 链接]。 我认为我对这篇论文的信息有疑问。看看这个表格： https://preview.redd.it/uaw20jf6fk8d1.png?width=1056&amp;format=png&amp;auto=webp&amp;s=e3c698529ec45dc4ad71b807f587572db2988dba 根据这个表格，作者认为 TRPO+ 即 TRPO 加上 PPO 的代码级优化优于 PPO。因此，这表明代码级优化比算法更重要。我的问题是，他们说他们对 TRPO+ 中打开和关闭代码级优化的所有可能组合进行网格搜索，而对于 PPO，则是将所有优化都打开。  我的问题是，通过进行网格搜索，他们给了 TRPO+ 更多的机会来获得一次良好的运行。我知道他们使用种子，但有 10 个种子。根据 Henderson 的说法，这还不够，因为即使我们做 10 个随机种子，将它们分组为两个 5 个种子并绘制奖励和标准差，我们也会得到完全分离的图，这表明方差太高，无法被 5 个种子或我猜甚至 10 个种子捕获。  因此，我不知道他们的论点在他们正在进行的网格搜索下如何成立。至少，他们也应该对 PPO 进行网格搜索。  我遗漏了什么？   由    /u/miladink  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</guid>
      <pubDate>Mon, 24 Jun 2024 18:53:12 GMT</pubDate>
    </item>
    <item>
      <title>“Rho-1：并非所有代币都是你所需要的”，Lin 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkuzs/rho1_not_all_tokens_are_what_you_need_lin_et_al/</guid>
      <pubDate>Mon, 24 Jun 2024 18:38:52 GMT</pubDate>
    </item>
    <item>
      <title>无模型 Stewart 平台</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</link>
      <description><![CDATA[        由    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</guid>
      <pubDate>Mon, 24 Jun 2024 18:35:11 GMT</pubDate>
    </item>
    <item>
      <title>这是 RL 的一个巧妙应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</link>
      <description><![CDATA[数据驱动的强化学习，实现洗衣机中的最佳电机控制    提交人    /u/Obsesdian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnho2z/heres_a_neat_application_of_rl/</guid>
      <pubDate>Mon, 24 Jun 2024 16:27:03 GMT</pubDate>
    </item>
    <item>
      <title>编程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</link>
      <description><![CDATA[        提交人    /u/chagdubbish   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</guid>
      <pubDate>Sun, 23 Jun 2024 20:06:01 GMT</pubDate>
    </item>
    </channel>
</rss>