<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 29 Jan 2025 03:18:37 GMT</lastBuildDate>
    <item>
      <title>“Robopair：越狱 LLM 控制的机器人”，Robey 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icbl8v/robopair_jailbreaking_llmcontrolled_robots_robey/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icbl8v/robopair_jailbreaking_llmcontrolled_robots_robey/</guid>
      <pubDate>Tue, 28 Jan 2025 20:35:41 GMT</pubDate>
    </item>
    <item>
      <title>一种可能取代 Transformer 的结构 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ic12qo/a_structure_that_potentially_replaces_transformer/</link>
      <description><![CDATA[我有一个替换 Transformer 结构的想法，这里有一个简短的解释。在 Transformer 架构中，它使用权重来选择值以生成新值，但如果我们这样做，新值不够精确。假设输入向量长度为​​ N。在此方法中，它首先使用特殊的 RNN 单元遍历序列的所有输入，并生成长度为 M 的嵌入。然后，它使用此嵌入与形状为 (N X N) X M 的矩阵进行线性变换。接下来，将得到的向量重塑为形状为 N x N 的矩阵。这个矩阵是动态的，它的值取决于输入，而之前的 (N X N) X M 矩阵是固定的和经过训练的。然后，将所有输入向量与矩阵相乘以输出长度为 N 的新向量。以上所有步骤是结构的一层，可以重复多次。几层之后，将所有层的输出连接起来。如果您有 Z 层，则新向量的长度将为 ZN。最后，使用特殊的 RNN 单元处理整个序列以得出最终结果（添加几个 Dense 层后）。完整的细节在此代码中，包括 RNN 单元的工作原理以及如何添加位置编码：https://github.com/yanlong5/loong_style_model/blob/main/loong_style_model.ipynb 如果您对该算法感兴趣，请联系我，我的名字是 Yanlong，我的电子邮件是 y35lyu@uwaterloo.ca     提交人    /u/Yanlong5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ic12qo/a_structure_that_potentially_replaces_transformer/</guid>
      <pubDate>Tue, 28 Jan 2025 13:04:20 GMT</pubDate>
    </item>
    <item>
      <title>昨天读了这篇文章，我想看看社区对这个 Google Deepmind MONA 的看法。你们觉得怎么样？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibzlpp/read_this_yesterday_and_i_wanted_to_see_what_the/</link>
      <description><![CDATA[        提交者    /u/GreyBamboo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibzlpp/read_this_yesterday_and_i_wanted_to_see_what_the/</guid>
      <pubDate>Tue, 28 Jan 2025 11:36:36 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习和无模型的强化学习有什么区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibyio9/whats_the_difference_between_modelbased_and/</link>
      <description><![CDATA[我试图理解基于模型和无模型强化学习之间的区别。据我所知：  无模型方法直接从真实经验中学习。它们观察当前状态，采取行动，然后以下一个状态和奖励的形式接收反馈。这些模型没有任何内部表示或对环境的理解；它们只是依靠反复试验来随着时间的推移改进其行动。 基于模型的方法则通过创建“模型”或环境模拟来学习。它们不仅仅是对状态和奖励做出反应，还试图模拟未来会发生什么。这些模型可以使用监督学习或学习函数（如 s′=F(s,a)s&#39; = F(s, a)s′=F(s,a) 和 R(s)R(s)R(s)）来预测未来状态和奖励。他们本质上建立了一个环境模型，并用它来规划行动。  因此，关键的区别在于基于模型的方法使用其学习到的模型来近似未来并提前规划，而无模型方法仅通过直接与环境交互来学习，而不尝试模拟它。 这样说对吗，还是我遗漏了什么？    提交人    /u/volvol7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibyio9/whats_the_difference_between_modelbased_and/</guid>
      <pubDate>Tue, 28 Jan 2025 10:21:25 GMT</pubDate>
    </item>
    <item>
      <title>自适应/在线 LQR 的研究合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibw7nq/research_collaboration_in_adaptativeonline_lqr/</link>
      <description><![CDATA[作为我博士研究的一部分，我已经从深度强化学习过渡到探索在线 LQR。具体来说，我一直在深入研究这篇论文中提出的想法。 我已经开发了一些我认为可能非常高效的算法思想。但是，我的背景主要是实践，我缺乏对这些方法进行严格理论分析的理论基础。 如果有人对这个主题感兴趣并希望在理论方面进行合作，我很乐意联系。:)    提交人    /u/riiswa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibw7nq/research_collaboration_in_adaptativeonline_lqr/</guid>
      <pubDate>Tue, 28 Jan 2025 07:21:03 GMT</pubDate>
    </item>
    <item>
      <title>需要项目规划方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibud58/need_help_for_planning_a_project/</link>
      <description><![CDATA[你好， 我需要为我的大学做一个项目。这是一个马尔可夫游戏，我应该对其进行建模然后解决它（使用不同的方法为其找到最优/近乎最优的策略。这是一个双人零和游戏。我可以使用哪些方法来解决它？您通常如何处理这种问题？从哪里开始？我知道如何在博弈论中对其进行建模，但我无法真正使用不同的算法来解决它，无法对它进行良好的可视化等等。    提交人    /u/Upset_Cauliflower320   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibud58/need_help_for_planning_a_project/</guid>
      <pubDate>Tue, 28 Jan 2025 05:16:50 GMT</pubDate>
    </item>
    <item>
      <title>需要基于项目的课程建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibnmmb/need_project_based_courses_recommendations/</link>
      <description><![CDATA[和标题一样多。我想一边做项目一边学习，所以如果有这方面的建议，请告诉我。如果你觉得这可能不是学习 RL 的最佳方法，请再次告诉我     提交人    /u/arrshsh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibnmmb/need_project_based_courses_recommendations/</guid>
      <pubDate>Mon, 27 Jan 2025 23:33:59 GMT</pubDate>
    </item>
    <item>
      <title>您推荐哪个库来处理具有临时变压器的预训练模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibn7k2/which_library_do_you_recommend_to_deal_with_a/</link>
      <description><![CDATA[我有一个自动驾驶模型（2 个摄像头和 2 个传感器），该模型已使用监督学习进行了预训练。该模型具有由时间转换器管理的时间序列，预训练的结果非常好，但是当我将其带到模拟器时，它会损失很多。我的想法是使用强化训练来提高模拟器中模型的性能。我一直在尝试 stable-baselines 3，这是我最常用的，但它似乎不支持转换器。有什么关于如何解决这个问题的想法吗？    提交人    /u/Luquin-as   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibn7k2/which_library_do_you_recommend_to_deal_with_a/</guid>
      <pubDate>Mon, 27 Jan 2025 23:15:19 GMT</pubDate>
    </item>
    <item>
      <title>帮助请求：努力提高地下城导航项目中的 AI 性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibm1pw/help_request_struggling_with_improving_ai/</link>
      <description><![CDATA[大家好， 我正在为我的大学开展一个 AI 项目，我正在训练一个 AI 穿越随机生成的 11x11 网格地牢。目标是让 AI 避开火势，捡起宝箱，然后走到出口。 我正在使用 Stablebaselines3 PPO 算法。 我花了很多时间调整项目的各个方面，包括：  调整参数 修改奖励系统 更改隐藏层  然而，尽管进行了所有这些更改，AI 的性能却没有提高。我只看到解释方差的微小变化（2-10％的增加或减少），但评估/平均奖励要么停滞不前，要么上下波动，但始终趋向于零。 以下是我尝试过的一些方法：  改变学习率（更高和更低） 尝试不同的奖励机制 增加剪辑范围 使用更宽和/或更深的隐藏层 尝试这些更改的不同组合  尽管付出了这些努力，但似乎没有任何方法对改善评估/平均奖励产生重大影响。 有人遇到过类似的问题或对我可能遗漏的内容有什么建议吗？我非常感谢任何建议或见解。 提前致谢！   由    /u/Competitive-Bar-5882  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibm1pw/help_request_struggling_with_improving_ai/</guid>
      <pubDate>Mon, 27 Jan 2025 22:25:48 GMT</pubDate>
    </item>
    <item>
      <title>具有决策转换器和多种行动的经验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibltys/experience_with_decision_transformer_and_multiple/</link>
      <description><![CDATA[你好 :) 我想知道是否有人有使用决策转移进行多项操作的经验？我有一个组合图定位问题，其中动作空间可以考虑如下： - 动作 1：要添加的节点 - 动作 2：要添加的节点 从技术上讲，我可以将所有组合编码为一个标记，但这会随着节点数量的增加而呈二次方增长。所以，我想避免这种情况。 传统上，在 Decision Transformer 中，我们针对一项操作进行优化。直观地说，多项操作也应该可以工作，因为我们将它们传递到嵌入层，然后连接起来：动作、状态和奖励嵌入。 你有这方面的经验吗？    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibltys/experience_with_decision_transformer_and_multiple/</guid>
      <pubDate>Mon, 27 Jan 2025 22:16:54 GMT</pubDate>
    </item>
    <item>
      <title>“在大型地下采矿环境中部署空中多智能体系统以实现自动任务执行”，Dhalquist 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibebim/deployment_of_an_aerial_multiagent_system_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibebim/deployment_of_an_aerial_multiagent_system_for/</guid>
      <pubDate>Mon, 27 Jan 2025 17:14:35 GMT</pubDate>
    </item>
    <item>
      <title>GRPO 可以用于多圈 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.03300 有些人可能已经看到了 PPO 的 RL 替代方案，即组相对策略优化 (GRPO)，其中不是训练价值模型，而是多次采样策略，获得平均奖励，并使用它来找出优势。 从审查实现来看，对话中只有一个回合，因为 LLM 要么正确解决数学问题，要么失败，所以在这种情况下奖励和价值是相同的，因为预期的未来奖励就是奖励。 GRPO 是否可以应用于多回合 RL 或更长远的项目，其中策略与环境多次交互？   提交者    /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</guid>
      <pubDate>Mon, 27 Jan 2025 16:05:02 GMT</pubDate>
    </item>
    <item>
      <title>钟摆政策不会学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ib4iem/pendulum_policy_doesnt_learn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ib4iem/pendulum_policy_doesnt_learn/</guid>
      <pubDate>Mon, 27 Jan 2025 09:35:57 GMT</pubDate>
    </item>
    <item>
      <title>旧的 RL 课程仍然有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</link>
      <description><![CDATA[大家好。我想知道我应该从哪门课程开始学习 RL。我想从 2024 年的斯坦福 234 课程开始，但我不知道它是否教授基础知识。我还听说 David Silver 的课程很棒，但它是近 10 年前的，我不知道我应该从哪门课程开始。 TL;DR 开始学习 RL 的最佳课程是什么？    提交人    /u/madcraft256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</guid>
      <pubDate>Mon, 27 Jan 2025 04:31:32 GMT</pubDate>
    </item>
    <item>
      <title>构建定制的机械臂环境并训练 AI 代理来控制它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</link>
      <description><![CDATA[        由    /u/Fabulous-Extension76  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</guid>
      <pubDate>Sun, 26 Jan 2025 03:59:44 GMT</pubDate>
    </item>
    </channel>
</rss>