<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 12 Oct 2024 06:21:21 GMT</lastBuildDate>
    <item>
      <title>“更大、更规则、更乐观：计算和样本高效连续控制的扩展”，Nauman 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1il8n/bigger_regularized_optimistic_scaling_for_compute/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2405.16158 摘要：  强化学习 (RL) 中的样本效率传统上由算法增强驱动。在这项工作中，我们证明扩展也可以带来显着的改进。我们对扩展模型容量和特定领域的 RL 增强之间的相互作用进行了彻底的研究。这些实证发现为我们提出的 BRO（更大，正则化，乐观）算法的设计选择提供了信息。BRO 背后的关键创新是强正则化允许有效扩展评论家网络，这与乐观探索相结合，可带来卓越的性能。 BRO 取得了最先进的成果，在 DeepMind Control、MetaWorld 和 MyoSuite 基准的 40 个复杂任务中，其表现显著优于领先的基于模型和无模型的算法。BRO 是第一个在极具挑战性的狗和人形任务中实现近乎最优策略的无模型算法。    [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1il8n/bigger_regularized_optimistic_scaling_for_compute/</guid>
      <pubDate>Fri, 11 Oct 2024 19:57:17 GMT</pubDate>
    </item>
    <item>
      <title>“奖励进步：扩展 LLM 推理的自动化流程验证器”，Setlur 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1igfb/rewarding_progress_scaling_automated_process/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2410.08146 摘要：  一种用于改进大型语言模型推理的有前途的方法是使用过程奖励模型 (PRM)。PRM 在多步推理跟踪的每个步骤中提供反馈，与仅在最后一步提供反馈的结果奖励模型 (ORM) 相比，可能改善信用分配。但是，收集密集的每步人工标签是不可扩展的，并且迄今为止，从自动标记的数据训练 PRM 带来的收益有限。为了通过针对 PRM 运行搜索或将其用作强化学习 (RL) 的密集奖励来改进基础策略，我们问：“我们应该如何设计过程奖励？”。我们的关键见解是，为了有效，步骤的过程奖励应该衡量进度：在采取该步骤之前和之后，未来产生正确响应的可能性的变化，与 RL 中的步骤级优势概念相对应。至关重要的是，应该在不同于基础策略的证明者策略下衡量这一进展。我们从理论上描述了一组好的证明者，我们的结果表明，优化这些证明者的过程奖励可以改善测试时搜索和在线 RL 期间的探索。事实上，我们的描述表明，弱证明者策略可以显著改善更强大的基础策略，我们也通过经验观察到了这一点。我们通过训练过程优势验证器 (PAV)来预测此类证明器下的进展，从而验证了我们的说法，并表明与 ORM 相比，针对 PAV 的测试时搜索准确率高出 8% 以上，计算效率高出 1.5-5 倍。通过 PAV 密集奖励的在线 RL 实现了 首批结果之一，与 ORM 相比，样本效率提高了 5-6 倍，准确率提高了 6% 以上。    [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1igfb/rewarding_progress_scaling_automated_process/</guid>
      <pubDate>Fri, 11 Oct 2024 19:51:19 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero MCTS 的搜索深度有多深？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1iavg/how_deep_does_alphazero_mcts_search/</link>
      <description><![CDATA[我在 A0 的论文中看到，他们在训练期间运行了 1600 次 MCTS 迭代，与单独使用策略网络相比，搜索结果可获得 1000+ elo 增益。但是，假设在围棋中，每个状态有 5 种合理的走法，那么经过 5 步深度后，5^5 = 3125 &gt; 1600。显然，这是一个粗略的估计，可能由于剪枝能力而被低估，但直观地看，1600 感觉很少。有没有发布 alphazero 搜索深度的信息，或者 alphago lee 在与李世石对弈时搜索的深度？    提交人    /u/DumplingLife7584   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1iavg/how_deep_does_alphazero_mcts_search/</guid>
      <pubDate>Fri, 11 Oct 2024 19:44:19 GMT</pubDate>
    </item>
    <item>
      <title>按顺序运行数据点或选择随机点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1df5n/running_though_the_datapoints_sequentially_or/</link>
      <description><![CDATA[我正在使用 sb3 训练交易环境，我的数据集由 250k 个数据点组成。我不确定我是否应该让环境始终从数据集的开头开始并按顺序运行直到 250k 结束以计算奖励，或者我应该使用固定的情节长度（例如 50k）并让它从每个情节的随机点开始。哪个可以让训练效果更好？    提交人    /u/Acceptable_Egg6552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1df5n/running_though_the_datapoints_sequentially_or/</guid>
      <pubDate>Fri, 11 Oct 2024 16:08:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 Mario 和 AI（深度强化学习）交易比特币</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g19oxu/trading_bitcoin_using_mario_ai_deep_reinforcement/</link>
      <description><![CDATA[大家好， 在过去的两个月里，我一直在做一个很酷的小项目，并决定制作一个视频，以鼓励自己做更多这样的项目。视频标题为： 使用 Mario 和 AI（深度强化学习）交易比特币 这是关于利用强化学习来交易比特币。这项工作非常酷，我玩得很开心。如果你有兴趣，请观看并告诉我你的想法！ https://www.youtube.com/watch?v=dACkVX5PkVc     提交人    /u/Kibo178   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g19oxu/trading_bitcoin_using_mario_ai_deep_reinforcement/</guid>
      <pubDate>Fri, 11 Oct 2024 13:23:08 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习和脉冲神经网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g15gw3/model_based_reinforcement_learning_and_spiking/</link>
      <description><![CDATA[有人知道是否有基于模型的强化学习和脉冲神经网络的相关论文吗？或者只是关于带有 snn 的模型的相关论文？    提交人    /u/Embri21   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g15gw3/model_based_reinforcement_learning_and_spiking/</guid>
      <pubDate>Fri, 11 Oct 2024 09:01:31 GMT</pubDate>
    </item>
    <item>
      <title>“评估生成模型中隐含的世界模型”，Vafa 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0tqvr/evaluating_the_world_model_implicit_in_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0tqvr/evaluating_the_world_model_implicit_in_a/</guid>
      <pubDate>Thu, 10 Oct 2024 21:28:59 GMT</pubDate>
    </item>
    <item>
      <title>这个问题可以用 RL 解决吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0kz70/can_this_problem_be_solved_with_rl/</link>
      <description><![CDATA[您好， 我是 RL 的新手，正在研究一个问题，电动汽车需要决定何时何地充电，以最大限度地减少等待时间和充电成本（价格随时间波动）。 我最初的想法是将每辆电动汽车视为一个代理，每个电动汽车都有自己的观察结果，例如电池状态、充电站位置、电价以及每个充电站的排队长度。 行动空间为： • 0：延迟充电（下一小时再决定） • 1：在充电站 1 充电 • 2：在充电站 2 充电 每个情节有 24 个时间段，代理只有在选择充电站后才会获得奖励。 我的问题是： 一旦电动汽车选择了一个充电站，它就会停止做出决策，因此轨迹会提前结束。例如，某些轨迹可能是 {0,0,0,1}（在 t=4 时转到 CS1），而其他轨迹可能是 {2}（在 t=0 时转到 CS2）。只有当 EV 选择充电站时，我才会获得奖励。 MARL 在这里仍然是一种好方法吗？ 我也不确定这个问题是否适合 MDP 框架，因为我见过的大多数论文都集中处理分配，当代理收到充电请求时，他们会立即决定充电站。 提前谢谢您！    提交人    /u/Full_Friendship8349   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0kz70/can_this_problem_be_solved_with_rl/</guid>
      <pubDate>Thu, 10 Oct 2024 15:07:19 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Q-Learning 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0ikbh/help_in_a_qlearning_project/</link>
      <description><![CDATA[      嘿，我是 RL 的新手，正在我的一位教授手下做一个项目。最初的任务之一是训练一个代理，使其能够在 5x5 网格中找到从随机初始化的起点到随机初始化的终点的最佳路径。 我学习了一些理论（主要来自 Medium 文章和 Chatgpt），并认为使用 Q 学习是一种很好的方法。然而我似乎陷入了困境，无论我如何更改参数或更改奖励结构都无济于事。训练结束时所采用的平均时间步长约为 16-17，对于这个简单的问题来说，这个数字确实很高，而且代理总体上表现不佳。 这是我的奖励结构+超参数+训练循环的片段 我尝试将奖励设为常数，减少（甚至消除）时间步长惩罚，并增加/减少几乎所有的超参数，但并没有取得太大的进步。 如果这是一个非常简单的问题，我很抱歉在这里发布，我可能犯了很多新手和基本错误。我将非常感激你们提供的所有帮助以及任何可以加深我理解的资源。谢谢！    提交人    /u/Hot_Program2634   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0ikbh/help_in_a_qlearning_project/</guid>
      <pubDate>Thu, 10 Oct 2024 13:15:59 GMT</pubDate>
    </item>
    <item>
      <title>强化学习提高化学反应性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0e0pj/reinforcement_learning_for_improving_chemical/</link>
      <description><![CDATA[我很高兴地告诉大家，我们孟买印度理工学院的研究小组（RBS 小组）最近在著名的《美国化学会志》（JACS）上发表了一篇论文，重点介绍了强化学习 (RL) 在提高化学反应性能方面的应用。 在复杂的化学世界中，优化反应条件可能是一项艰巨的任务，通常需要大量的反复试验。我们的论文提出了一种利用 RL 算法来预测和改善反应结果的新方法。通过将优化过程视为动态决策问题，我们能够显著提高反应产量和选择性。 我们希望我们的工作能够激发人工智能与化学交叉领域的进一步探索，促进该领域复杂问题的创新解决方案。 这里是链接https://pubs.acs.org/doi/full/10.1021/jacs.4c08866    提交人    /u/Kindly-Mortgage-2459   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0e0pj/reinforcement_learning_for_improving_chemical/</guid>
      <pubDate>Thu, 10 Oct 2024 08:20:47 GMT</pubDate>
    </item>
    <item>
      <title>梦想家与旧论文非常相似</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g0d22d/dreamer_is_very_similar_to_an_older_paper/</link>
      <description><![CDATA[我随意浏览了 Yannic Kilcher 的旧视频，发现了这个视频，内容是关于 David Ha 和 Jürgen Schmidhuber 的论文“World Models”。我很惊讶地发现，尽管没有被引用或作者相同，但它提出了与 Dreamer（发表时间稍晚）非常相似的想法。 两者都涉及学习潜在动态，可以产生“梦想”环境，在这种环境中，RL 策略可以在不需要在真实环境中进行部署的情况下进行训练。即使是架构也基本相同，从观察自动编码器到处理实际前向演化的 RNN/LSTM 模型。 但是，尽管这些大体内容相同，但实际的论文结构却截然不同。 Dreamer 的论文有更好的实验和数值结果，以及不同想法的呈现方式。 我不确定这是否只是巧合，或者作者是否有一些共同的圈子。无论如何，我认为鉴于 Dreamer 的受欢迎程度，早期的论文应该得到更多的认可。    提交人    /u/irrelevant_sage   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g0d22d/dreamer_is_very_similar_to_an_older_paper/</guid>
      <pubDate>Thu, 10 Oct 2024 07:04:38 GMT</pubDate>
    </item>
    <item>
      <title>如何生成这样的图表？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fzz4b4/how_to_generate_such_diagram/</link>
      <description><![CDATA[      大家好， 阅读论文时，我看到许多像这样的图表： https://preview.redd.it/2q6a0gn02std1.png?width=811&amp;format=png&amp;auto=webp&amp;s=b00db0b0980dc291c269128beae1a6d134fd9661 取自 Online Decision Transformer 论文（source）。 查看左图，您可以看到一条蓝色和一条红色实线。它们周围有一个阴影，颜色相同但几乎透明。 我猜实线是平均值，阴影是标准差，对吗？ 我真的很好奇：您知道如何制作这样的图表吗？有 Python 库或类似的东西吗？ 谢谢    提交人    /u/WilhelmRedemption   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fzz4b4/how_to_generate_such_diagram/</guid>
      <pubDate>Wed, 09 Oct 2024 18:57:21 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium v​​1.0 发布（核心 API 现已稳定）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fzbbnd/gymnasium_v10_release_core_api_now_stable/</link>
      <description><![CDATA[我们很高兴地宣布 Gymnasium v​​1.0 的发布，它是 OpenAI Gym 的一个维护分支，用于定义强化学习环境。阅读发布说明，了解我们所做的所有更改。这是我们出色的志愿者在过去 3 年的共同努力。在此期间，我们稳步改进了库 - 修复错误、添加新功能和我们认为必要的 API 更改。使用 v1.0，这将是 Gymnasium 的第一个稳定版本，没有计划更改核心 API（Env、Space 或 VectorEnv），这意味着如果您正在等待更新项目，现在是时候了，请参阅我们的迁移指南了解更多信息。    提交人    /u/jkterry1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fzbbnd/gymnasium_v10_release_core_api_now_stable/</guid>
      <pubDate>Tue, 08 Oct 2024 21:36:21 GMT</pubDate>
    </item>
    <item>
      <title>持续动态的策略迭代</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fz7c8k/policy_iteration_for_continuous_dynamics/</link>
      <description><![CDATA[我正在开展一个项目，旨在构建应用于连续动态环境的策略迭代 (PI)实现。值函数 (VF)使用离散状态空间的每个单纯形内的线性插值来近似。插值系数的作用类似于随机过程中的概率，这有助于使用离散马尔可夫决策过程 (MDP)来近似连续动态。该算法由 Gymnasium 提供的 Cartpole 和 Mountain car 环境进行了测试。 Github 链接：DynamicProgramming    提交人    /u/Grouchy_Ad_4112   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fz7c8k/policy_iteration_for_continuous_dynamics/</guid>
      <pubDate>Tue, 08 Oct 2024 18:47:32 GMT</pubDate>
    </item>
    <item>
      <title>杜拉克的人工智能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fz3swl/ai_for_durak/</link>
      <description><![CDATA[我正在开展一个项目，为 Durak 构建 AI，Durak 是一款流行的俄罗斯纸牌游戏，具有不完全信息和多个代理。挑战类似于扑克，但也有一些不同。例如，与扑克中的 52 选择 2 不同，Durak 在发牌时的初始状态为 36 选择 7，这比扑克的状态多 6,000 倍，并且每场游戏中的决策数量要多得多，所以我不确定相同的方法是否可以很好地扩展。玩家拥有不完全信息，但可以根据对手的行为做出推断（例如，如果某人没有防御一张牌，他们可能没有那套花色）。 我正在寻找有关我应该为这种类型的游戏使用哪种 AI 技术或技术组合的建议。我一直在研究的一些事情：  蒙特卡洛树搜索 (MCTS) 和推出 来处理不确定性 强化学习 贝叶斯推理或某种形式的对手建模，根据对手的动作估计隐藏信息 基于规则的启发式，用于捕捉 Durak 独有的特定类人策略  编辑：我假设这个游戏中可能存在纳什均衡，但我主要关心的是考虑到复杂性，计算是否可行。 Durak 的扩展速度非常快，特别是当你增加玩家数量或从 36 张牌的牌组切换到 52 张牌的牌组时。每个玩家开始时有 6 张牌，因此可能的游戏状态数量很快就会变得比扑克还要多得多。 无论是牌组合还是玩家互动，可能性的激增让我担心 MCTS 和 RL 等方法是否能在合理的时间范围内处理游戏的复杂性。    提交人    /u/Iezgin   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fz3swl/ai_for_durak/</guid>
      <pubDate>Tue, 08 Oct 2024 16:20:04 GMT</pubDate>
    </item>
    </channel>
</rss>