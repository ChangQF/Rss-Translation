<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 22 May 2024 12:27:35 GMT</lastBuildDate>
    <item>
      <title>我有一个深度强化学习 PPO 代理，它使用离散状态并输出离散动作。我正在考虑将其转换为 DRL 变压器。我应该如何在国家代币化或任何其他方法方面取得进展？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxxfrn/i_have_a_deep_reinforcement_learning_ppo_agent/</link>
      <description><![CDATA[嗨， 我目前有一个正在运行的 PPO 代理，但我正在考虑将我的环境扩展到一个更复杂的环境。因此，我正在考虑将当前的PPO智能体转换为PPO+变压器智能体。 状态类型：离散 动作类型：离散 我已经完成了对变压器的结构进行了一些研究，但我仍然有几个问题： 1）如何标记离散状态？ （我可以将状态转换为句子，然后将其传递到变压器中，但我认为这不是一个好方法，因为它涉及转换为字符串，然后再转换回数字。） 2）如何我该如何将变压器输出转换回状态？ 3) 我应该使用哪种变压器类型：仅编码器、编码器-解码器或仅解码器？ 我是新手变压器，所以任何帮助或建议都会非常有帮助。 提前致谢！   由   提交/u/Appressive_Bag1262   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxxfrn/i_have_a_deep_reinforcement_learning_ppo_agent/</guid>
      <pubDate>Wed, 22 May 2024 10:59:23 GMT</pubDate>
    </item>
    <item>
      <title>加入应用 RL 优化数据中心的挑战</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxx1m6/join_challenge_that_applies_rl_to_optimize_data/</link>
      <description><![CDATA[大家好，我正在与 Fruitpunch AI 合作组织一项挑战赛，人们可以在其中应用强化学习来优化数据中心的能源使用（请参阅这里https://app.fruitpunch.ai/challenge/ai-for-greener-datacenters ）。我认为这里可能有些人有兴趣在现实世界的用例中应用强化学习。如果您有任何疑问，请给我留言。    由   提交 /u/Lalalendalf   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxx1m6/join_challenge_that_applies_rl_to_optimize_data/</guid>
      <pubDate>Wed, 22 May 2024 10:33:06 GMT</pubDate>
    </item>
    <item>
      <title>DQN 显示损失有所改善，但缺乏奖励改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxws11/dqn_showing_loss_improvements_but_lacking_reward/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxws11/dqn_showing_loss_improvements_but_lacking_reward/</guid>
      <pubDate>Wed, 22 May 2024 10:15:17 GMT</pubDate>
    </item>
    <item>
      <title>健身房环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</link>
      <description><![CDATA[我有实时系统的数据集，我可以使用强化学习算法的数据表创建基于深度神经网络的环境吗？ 请尽快回复。紧急，陷入项目工作   由   提交 /u/Past-News-1373   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</guid>
      <pubDate>Wed, 22 May 2024 09:14:03 GMT</pubDate>
    </item>
    <item>
      <title>很难阅读大多数 RL 算法的 CS 符号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</link>
      <description><![CDATA[在观看带有示例的视频后，我理解了贝尔曼方程，但是有没有更直观的方法来理解任何论文中的任何政策制定。并不是所有的论文都能很好地打破他们的方程的作用，而是让比我智力更高的人尝试从论文中挖掘它 &lt;！-- SC_ON - -&gt;  由   提交 /u/baboolasiquala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</guid>
      <pubDate>Wed, 22 May 2024 04:16:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么是零和？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</link>
      <description><![CDATA[在很多论文中，你都可以看到“因为游戏是零和的”、“我们以零和的方式解决问题”之类的短语博弈”等，但为什么零和很重要？零和的哪些属性使训练变得更容易？ 例如，在 2P 设置中，很容易看出无论玩家 A 赚到什么，玩家 B 都会输。多人（&gt;2）游戏怎么样？ 再举一个例子，想象一个玩家拥有能力的游戏。玩家 A 有能力为自己生产 10 块金币。 B 从另一位选择的玩家那里偷走了 5 个金币。 C使所有其他玩家失去5金。这显然不是零和游戏，但是你不能按照正常方式训练代理吗？   由   提交 /u/HyogoKita19C   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</guid>
      <pubDate>Wed, 22 May 2024 01:07:16 GMT</pubDate>
    </item>
    <item>
      <title>棋盘游戏神经网络架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</link>
      <description><![CDATA[有人有过在棋盘游戏中尝试不同神经网络架构的经验吗？ 目前使用 PPO 进行数独 - 输入 I我正在考虑只是一个扁平的板向量，因此神经网络是一个简单的 MLP。但我没有得到很好的结果——想知道 MLP 架构是否是问题所在？  AlphaGo 论文使用了 CNN，很想知道你们尝试过什么。感谢任何建议    由   提交 /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</guid>
      <pubDate>Tue, 21 May 2024 23:07:06 GMT</pubDate>
    </item>
    <item>
      <title>将 MAB 集成到 Web 应用程序中</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxl655/integrate_mab_into_a_web_application/</link>
      <description><![CDATA[大家好，我正在寻求将 MAB（多武装强盗）RL 集成到网络或移动应用程序中，该项目是为了我的硕士论文和它是关于使用主动学习和强化学习的自适应推荐系统，所以我已经在 Kaggle 中运行了所有内容（我正在做离线评估并使用重放），并且由于我是 RL 的新手，我想知道我应该如何解决整合这项工作进入网络或移动应用程序（我可以在网络/应用程序开发中管理自己）。谢谢大家。我选择使用 movielens Ml 1m 数据集。如果您需要任何其他信息，请告诉我。   由   提交 /u/Fredybec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxl655/integrate_mab_into_a_web_application/</guid>
      <pubDate>Tue, 21 May 2024 22:47:58 GMT</pubDate>
    </item>
    <item>
      <title>这个项目创意好不好-自驾代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</link>
      <description><![CDATA[我正在为自动驾驶代理构建一个项目，我计划包括改变状态的动态交通灯和在网格中随机移动的动态行人，独立于中介。包含动态行人有多困难，并且可以在网格中随机生成灯光和行人的位置吗？还使用 q-learning 来实现 我怎样才能使这个项目更加随机？我正在考虑引入天气影响，在每次模拟之前使用概率分布，并且根据天气预报，这将影响车辆的燃油消耗，惩罚会导致燃油消耗大幅增加。本质上，代理处于生存模式，以确保不发生碰撞、遵守交通法规并且不达到最大燃油容量。   由   提交/u/amulli21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</guid>
      <pubDate>Tue, 21 May 2024 15:56:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用人类演示来预训练 RL 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</link>
      <description><![CDATA[我想要一个代码示例来了解如何执行此操作。有人可以帮助我吗？   由   提交 /u/SebyR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</guid>
      <pubDate>Mon, 20 May 2024 18:16:01 GMT</pubDate>
    </item>
    <item>
      <title>“Mobile ALOHA：通过低成本全身远程操作学习双手移动操作”，Fu 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</guid>
      <pubDate>Mon, 20 May 2024 17:35:15 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的对抗性攻击和对抗性训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /对抗性强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</guid>
      <pubDate>Mon, 20 May 2024 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>q 学习和井字游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</link>
      <description><![CDATA[tttrl - Pastebin.com KDT85/Tic-Tac-Toe-RL：使用强化学习的 Tic-Tac-Toe 游戏 (github.com)&lt; /a&gt; 上面是我使用强化学习的井字棋游戏的代码，你们觉得怎么样。  它似乎可以工作，但 q 表没有填满，这正常吗？其训练次数为 20000000 集。  GitHub 中包含 q 表作为 Excel 文件。  谢谢！    由   提交 /u/chinfuk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</guid>
      <pubDate>Mon, 20 May 2024 15:12:36 GMT</pubDate>
    </item>
    <item>
      <title>致力于为 Azul Board Game 开发成功的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</link>
      <description><![CDATA[大家好， 我目前正在做一个项目，旨在训练一个模型精通桌游 Azul 。我用 Python 编写了自己的游戏引擎，并且使用 SB3 PPO 实现。我的观察是一个由大约 700 位组成的二进制向量（这是整个游戏状态的二进制表示，如工厂、玩家板等），我的动作空间是 300 大小的向量，每个位对应于某个动作（在 azul 游戏中，每回合你必须做出 3 个决定：选择哪个工厂、什么颜色以及将其放置在哪里，并且你有 10 个工厂（我的模型正在玩 4 人游戏）、5 种颜色和 6 条图案线，因此你有 300 种可能我已经多次运行它，使用不同的激活函数、层大小、层数、学习率、伽马等，我尝试使用分数作为奖励，也尝试使用一些任意奖励，例如进行有效的移动等。 ，但我从未达到令人满意的结果，该模型从平均 -90 分开始，逐渐达到 -30/-20，但这仍然是非常糟糕的结果，因为 azul 中的负分意味着它做了很多废话平均球员每场得分超过 40/50 分，所以我与我的模型还相去甚远。  azul 环境的巨大问题是，在 300 个动作中，通常只有 40 个有效动作，因此这意味着只有大约 10-15% 的动作是有效的。首先，我的模型将以某种方式工作，它采取具有最高神经元值的操作，如果有效，则执行此操作，如果建议的操作无效（例如，模型想要从工厂 2 获取蓝色瓷砖，并将其放在模式行3中，但该工厂没有蓝色瓷砖），它将采取第二高神经元值动作并检查它是否有效等等，直到达到有效动作。  我最近的想法是对模型输出应用一些屏蔽，这样当模型观察棋盘并返回动作向量时，它会过滤所有无效动作，将它们的值设置为 0，然后然后我们只有有效动作的动作向量，第二个变化是我不选择最高值，而是将 softmax 应用于这些值并使用计算的概率随机选择它们。  不幸的是，我没有得到任何结果，该模型根本没有学习。你们对我可以在哪里改进我的算法、为什么我的模型无法学习等有什么建议吗？我将非常感谢所有的帮助，我也愿意为那些感兴趣的人分享代码，所以请告诉我，  请帮助我（：    提交者    /u/Different-Leave8202   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</guid>
      <pubDate>Mon, 20 May 2024 12:34:04 GMT</pubDate>
    </item>
    <item>
      <title>有人真的部署了一个模型来用于推理吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</link>
      <description><![CDATA[我只是好奇有人实际上使用 RL 训练的策略来帮助解决哪些应用程序或控制问题，您用什么 Algo 进行训练？在此过程中遇到的主要挑战是什么？    由   提交 /u/Aggressive-Reach1657    reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</guid>
      <pubDate>Mon, 20 May 2024 07:01:20 GMT</pubDate>
    </item>
    </channel>
</rss>