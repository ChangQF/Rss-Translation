<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 05 Jan 2025 18:20:29 GMT</lastBuildDate>
    <item>
      <title>教 PPO“绘画”有困难</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hu4s6t/trouble_teaching_ppo_to_draw/</link>
      <description><![CDATA[我正尝试教神经网络在 colab 中“绘制”。这个想法是，给定一个输入画布和一个参考图像，网络需要输出两个 x 和 y 坐标以及一个 rgba 值，并在输入画布顶部绘制一个具有 rgba 颜色的矩形。画布上带有矩形，然后是新状态。然后重复该过程。 我正在使用 PPO 训练这个网络。据我所知，这是一种用于连续动作的良好 DRL 算法。 奖励是放置矩形之前和之后与参考图像相比的 mse 差异。此外，对于完全相同或非常接近的坐标，还会受到惩罚。通常，初始网络会吐出非常接近的坐标，导致绘制矩形时没有奖励。 一开始，损失似乎在下降，但过了一段时间就停滞了，我想找出我做错了什么。 我最后一次使用强化学习是在 2019 年，现在我有点生疏了。我已经订购了 Grokking DRL 书，10 天后到货。同时，我有几个问题： - PPO 是这个问题的正确算法选择吗？ - 我的 PPO 实现看起来正确吗？ - 您是否发现我的奖励函数有任何问题？ - 网络是否足够大以学习这个问题？（小得多的 CPPN 能够做得不错，但它们是符号网络） - 您认为我的网络也可以从参考图像作为输入中受益吗？即第二个 CNN 输入流，用于参考图像，我将其输出展平并将其连接到线性层的另一个输入流。    提交人    /u/matigekunst   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hu4s6t/trouble_teaching_ppo_to_draw/</guid>
      <pubDate>Sun, 05 Jan 2025 11:10:46 GMT</pubDate>
    </item>
    <item>
      <title>强化学习Flappy Bird代理失败！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htzwke/reinforcement_learning_flappy_bird_agent_failing/</link>
      <description><![CDATA[我尝试使用 DQN 为 Flappy Bird 创建强化学习代理，但代理根本没有学习。它一直与管道和地面发生碰撞，我不知道哪里出了问题。我不确定问题出在奖励系统、神经网络还是我实现的游戏机制上。有人能帮我吗？我会分享我的 GitHub 存储库链接以供参考。 GitHub 链接    提交人    /u/uddith   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htzwke/reinforcement_learning_flappy_bird_agent_failing/</guid>
      <pubDate>Sun, 05 Jan 2025 06:50:18 GMT</pubDate>
    </item>
    <item>
      <title>训练期间目标 Q 值告诉我什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htzpk6/what_does_the_target_qvalue_tell_me_during/</link>
      <description><![CDATA[大家好， 我正在训练一个 TD3 代理，想知道目标 Q 值能告诉我关于训练的什么信息。 我知道最基本的知识，即如果我们遵循某些最佳策略，它是预期的折扣奖励。那么如果它开始收敛到某个值，然后稍微减少，然后一遍又一遍地增加（有点像在 2 个点之间反弹），它是否学到了一些次优策略？还是训练还没有完成？对于奖励稀疏的环境来说，这尤其令人困惑，那么它是否可以成为一个有用的指标，表明在训练的哪个点它会达到最优策略？我之所以问这个问题，是因为在连续 5 个左右的场景中，环境会得到解决，然后出现不利的表现。这让我想到了以下问题： 如果动作中总是添加噪音，那么目标 Q 值是否有助于告诉我噪音是否妨碍了训练？至于具体细节，我确实将噪声衰减到了 0.1，这意味着添加的随机噪声是从标准差为 0.1 的正态分布中采样的。我觉得这可能会影响一些目标 Q 值？ 我觉得这是一个开放式的问题，所以我很乐意详细说明任何事情。 非常感谢！    提交人    /u/Sea_Farmer5942   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htzpk6/what_does_the_target_qvalue_tell_me_during/</guid>
      <pubDate>Sun, 05 Jan 2025 06:37:15 GMT</pubDate>
    </item>
    <item>
      <title>“无流程标签的免费流程奖励”，Yuan 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1httwti/free_process_rewards_without_process_labels_yuan/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1httwti/free_process_rewards_without_process_labels_yuan/</guid>
      <pubDate>Sun, 05 Jan 2025 01:17:53 GMT</pubDate>
    </item>
    <item>
      <title>“Aviary：训练语言代理完成具有挑战性的科学任务”，Narayanan 等人 2024 {Futurehouse}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htsw86/aviary_training_language_agents_on_challenging/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htsw86/aviary_training_language_agents_on_challenging/</guid>
      <pubDate>Sun, 05 Jan 2025 00:28:52 GMT</pubDate>
    </item>
    <item>
      <title>需要一点帮助来验证项目想法 - gym-super-mario-bros</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htr6ve/need_a_little_help_verifying_an_idea_for_a/</link>
      <description><![CDATA[大家好！ 我正在开始一个新项目，这也是我对强化学习的介绍。我当时想创建一个学习玩超级马里奥兄弟的人工智能模型（我知道，这很有创意 :)）。但问题是，我想实现一个系统，在这个系统中，模型在一定数量的帧内不能切换他选择的动作。例如，如果他的动作是按下跳跃按钮，他必须按住跳跃按钮几帧。这个想法是，用户可以输入他的反应时间（假设为 200 毫秒），然后根据该值，我们得到他不能“改变”输入的帧数（游戏以 60 帧/1000 毫秒的速度运行，因此在这个例子中，人工智能必须坚持相同的动作至少 12 帧）。  背后的原因是我想根据用户的反应时间创建“个性化”半速通指南。然后添加覆盖层，显示“在给定时刻按下哪个按钮”。  话虽如此，我不知道使用 gym ai 是否可能实现这种事情（？）。是否有更有经验的人愿意验证我的想法是否可行？我打算在这个项目中使用 gym-super-mario-bros 7.4.0。  干杯 :)     提交人    /u/Broad-Ball-1131   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htr6ve/need_a_little_help_verifying_an_idea_for_a/</guid>
      <pubDate>Sat, 04 Jan 2025 23:10:53 GMT</pubDate>
    </item>
    <item>
      <title>梦想家架构中行动空间的变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htoxs7/changing_action_spaces_in_dreamer_architecture/</link>
      <description><![CDATA[您好 r/reinforcementlearning， 因此，我正在设计一个模型来完成某种类型的复杂工作。 本质上，我设计环境的方式涉及在不同的动作空间上工作。 我认为，为了创建不同的动作空间，我只需更改 Agent 的动作空间即可；但是我检查了代码，似乎 。空间的数量非常有限（大约 30 个不同的动作空间），但它们是不同的 - 有时它只是一个从 1 到 3 的 uint，有时它是（3 个 float32 选择、一个 bool 选择、另一个但不同的 3 个 float32 选择）；或者有时它是一个包含 127 个布尔值的向量，模型应该选择真/假。 这绝对比使用单个 action 参数更复杂。 有人处理过这个问题吗？怎么做？ 干杯。 &gt; 我担心的一件事是不同的 dtypes。从技术上讲，我可以有 3 个输出，分别为布尔值、整数和浮点数，并惩罚不必要的操作，但是……我已经将所有环境编码为静态操作，此外，我很确定在这个环境中较少的循环是好的 - 我已经完成了数千个离散步骤才能实现它。    提交人    /u/JustZed32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htoxs7/changing_action_spaces_in_dreamer_architecture/</guid>
      <pubDate>Sat, 04 Jan 2025 21:29:13 GMT</pubDate>
    </item>
    <item>
      <title>从基于模型到无模型的强化学习：转换我的旋转倒立摆解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htma2j/from_modelbased_to_modelfree_rl_transitioning_my/</link>
      <description><![CDATA[嗨，RL 爱好者们！我最近为旋转倒立摆问题实现了一个基于模型的强化学习解决方案，现在我希望迈出无模型领域的下一步。我正在寻求有关实现这一转变的最佳方法的建议。 当前设置  问题：旋转倒立摆 方法：基于模型的 RL 状态：成功实施并正在运行  目标 我的目标是：  过渡到无模型 RL 方法 保持或提高性能 深入了解基于模型和无模型方法之间的差异  问题  对于这个特定问题，您会推荐哪种无模型算法？ （例如 DQN、DDPG、SAC） 对于旋转倒立摆，从基于模型的 RL 转向无模型 RL 时，我应该预见到哪些主要挑战？ 我应该考虑哪些特定的修改或技术来使我当前的解决方案适应无模型框架？ 如何有效地将我当前基于模型的解决方案的性能与新的无模型方法进行比较？  如果您能分享任何见解、资源或个人经历，我将不胜感激。提前感谢你的帮助！    提交人    /u/Fit-Orange5911   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htma2j/from_modelbased_to_modelfree_rl_transitioning_my/</guid>
      <pubDate>Sat, 04 Jan 2025 19:32:52 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助选择研究主题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htdek0/need_help_picking_research_topic/</link>
      <description><![CDATA[我最近开始攻读强化学习博士学位，说实话，我有点迷茫。我应该从强化学习领域中选择一个研究问题。我真的知道如何找到研究差距以及要寻找什么或如何寻找它？我真的很感激任何帮助/指导（找到这个特定主题或研究差距的程序以及任何想法）。    提交人    /u/No-Ranger-2702   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htdek0/need_help_picking_research_topic/</guid>
      <pubDate>Sat, 04 Jan 2025 12:29:32 GMT</pubDate>
    </item>
    <item>
      <title>截断和终止之间的区别有多重要？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ht65uu/how_important_is_the_difference_between/</link>
      <description><![CDATA[我最近一直在查看多个 RL 环境框架，并注意到许多（据我所知）环境/gym API 没有为终止和截断提供单独的标志/返回值。许多 API 只是报告“完成”或“终端” Farama 的人们已经更新了他们的 Gynasium API，以在环境 step() 函数中返回来自终止和截断的单独值。 他们在 2023 年 10 月发布的关于这一重大 API 更改的帖子似乎非常引人注目：https://farama.org/Gymnasium-Terminated-Truncated-Step-API 将终止和截断视为相同的 RL 框架列表： - brax - JaxMARL - Gymnax - jym 具有单独终止和截断值的 RL 框架列表： - Farama Gymnasium - PGX - StableBaselines3 - Jumanji 所以我的问题是，为什么没有更多的 RL 框架采用类似的能力来辨别截断和终止？终止和截断之间的区别是否没有我想象的那么重要？我感觉我错过了其他人已经弄清楚的东西。 难道当使用端到端 Jax 进行环境和训练时，大规模并行环境的速度提升会完全消除因不对终止和截断进行不同处理而导致的低效率？ 编辑：将 StableBaselines3 添加到具有单独终止 + 截断的框架列表中；至少在我从其 repo 链接的特定代码示例中。将 Jumanji 移至具有单独截断和终止的列表。    提交人    /u/aloecar   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ht65uu/how_important_is_the_difference_between/</guid>
      <pubDate>Sat, 04 Jan 2025 04:17:10 GMT</pubDate>
    </item>
    <item>
      <title>“使用计算高效传感器的战术射击类人机器人”，Justesen 等人 2025 年（Valorant / Riot Games）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ht5lwv/humanlike_bots_for_tactical_shooters_using/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ht5lwv/humanlike_bots_for_tactical_shooters_using/</guid>
      <pubDate>Sat, 04 Jan 2025 03:47:09 GMT</pubDate>
    </item>
    <item>
      <title>随机线性老虎机的最优算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ht4t9n/optimal_algorithms_for_stochastic_linear_bandits/</link>
      <description><![CDATA[我想知道是否有人见过一篇论文，其中匹配了以下设置的遗憾下限。一个线性老虎机，其中在每个时间步骤中根据 1、...、n 上的分布随机选择 n 个损失函数之一。学习者在 d 维连续空间中选择一个动作。我假设老虎机反馈；学习者每次只观察每个动作的损失。 据我所知，下限为 O( d \sqrt{T} ) 已被证明；但是，我认为尚未出现任何算法将预期遗憾的上限设置为 O( d \sqrt{T} )。我见过的最佳算法（尽可能广泛地看待）保证 O( d \sqrt{T log T} ) 预期遗憾。如果有一种算法保证 O( d \sqrt{T} )，我希望这篇论文很容易找到。 是否有人知道一种算法，可以在老虎机反馈下削减对数因子并保证 O( d \sqrt{T} )？    提交人    /u/Few_Art1572   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ht4t9n/optimal_algorithms_for_stochastic_linear_bandits/</guid>
      <pubDate>Sat, 04 Jan 2025 03:04:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习课程建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsv489/reinforcement_learning_course_suggestion/</link>
      <description><![CDATA[我是来自印度的 CS 硕士生，正在准备担任 AI 工程师的职位，我对 ML 和 DL 概念有很好的理解，已经完成了 CS 229（斯坦福 ML）、MIT 6.S191（DL）和其他课程。我对 RL 有基本的了解（作为我的 DL 课程的一部分）并且我想深入研究 RL 概念和实际实施，您能给我推荐免费的在线资源吗？我一直在寻找 YouTube 讲座，然后我遇到了以下内容：1. 斯坦福的 CS 234，最新课程于 2 个月前上传，但没有笔记并且仍然受到限制。（2019 年课程笔记可在线获取）2. 加州大学伯克利分校的 CS 285 Deep R，讲座和幻灯片均可用。 3. David Silver 的 RL 课程（2015 年课程，旧了吗） 4. 阿尔伯塔大学在 Coursea 上的强化学习专业化（我有高级订阅）。 我应该从哪门课程开始，我应该如何进行？ 非常感谢您的建议🙏    提交人    /u/Radiant_Number9202   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsv489/reinforcement_learning_course_suggestion/</guid>
      <pubDate>Fri, 03 Jan 2025 19:44:02 GMT</pubDate>
    </item>
    <item>
      <title>简单的双足机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsu179/simple_bipedal_robot/</link>
      <description><![CDATA[Johnny five 还活着，而且他现在有腿了！ 双足机器人行走 使用 PPO 训练。您可以在此处试用（需要支持 SharedArrayBuffer 的浏览器）：https://play.prototwin.com/?model=BipedalRobot 单击并拖动以进行交互。右键单击以旋转相机。鼠标中键可平移相机。重置按钮位于屏幕的右上角。 训练脚本可用：https://github.com/prototwin/RLExamples/blob/main/bipedal/bipedal.py 可以在Onshape上找到机器人的 CAD。 如果您有 VR 耳机，则可以单击 VR 按钮。   由    /u/kareem_pt  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsu179/simple_bipedal_robot/</guid>
      <pubDate>Fri, 03 Jan 2025 18:59:48 GMT</pubDate>
    </item>
    <item>
      <title>Advantage Actor-Critic 无法正常工作。（OpenAI gym Cart Pole + Pytorch）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsqmrf/advantage_actorcritic_not_working_properly_openai/</link>
      <description><![CDATA[我试图实现 Advantage Actor Critic 算法来训练 OpenAI 的健身房环境中的 Cart Pole Agent。但即使经过大量参数调整，我也无法产生良好的训练结果。我相信我已经正确实现了它，尽管我看到了同一算法的几个略有不同的变体。 我在这里附上代码。要运行它，您需要使用 pytorch、pygame、gymnasium 和 gymnasium[classic-control] 包。代码已标记且可读。  https://github.com/Utsab-2010/RL-Tests/blob/main/Cart_Pole_A2C-Copy1.ipynb 如果有人能指出到底发生了什么，或者提供一些值得遵循的好资源，我将不胜感激。    提交人    /u/Otaku_boi1833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsqmrf/advantage_actorcritic_not_working_properly_openai/</guid>
      <pubDate>Fri, 03 Jan 2025 16:40:31 GMT</pubDate>
    </item>
    </channel>
</rss>