<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 17 Sep 2024 21:15:23 GMT</lastBuildDate>
    <item>
      <title>QR-DQN 爆炸值域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj6fvy/qrdqn_exploding_value_range/</link>
      <description><![CDATA[我正在研究分布式强化学习，目前正在尝试实现 QR-DQN。 Github 中有一个直观的解释，但对环境的简短解释是代理从 (0,0,0) 开始。向“左”或“右”是随机选择的，向左会导致最左边的 0 被 -1 替换，向右会将最左边的 0 替换为 +1。每个非终止步骤的奖励为 0。一旦代理到达终点，奖励将计算为  s=(-1,-1,-1) =&gt; r=0 s=(-1,-1,1) =&gt; r=1 . . . s=(1,1,1) =&gt; r=7 请注意，QR-DQN 不会采取任何行动，它只是试图预测奖励分布。这意味着在状态 s=(0,0,0) 时，分布应在 0 到 7 之间均匀分布，在状态 s=(1,0,0) 时，分布应在 4 到 7 之间均匀分布，等等。 但是，QR-DQN 输出的分布范围从 -20,000 到 +20,000，并且似乎永远不会收敛。我很确定这是一个引导问题，但我不知道如何解决它。 代码：https://github.com/Wung8/QR-DQN/blob/main/qr_dqn_demo.ipynb    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj6fvy/qrdqn_exploding_value_range/</guid>
      <pubDate>Tue, 17 Sep 2024 18:13:40 GMT</pubDate>
    </item>
    <item>
      <title>关于在情景强化学习设置中使用演员评论家架构的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</link>
      <description><![CDATA[大家好，RL 的朋友们， 我最近遇到了一个问题，我正在将多智能体 PPO 与演员-评论家相结合应用于一个问题，由于问题的性质，我首先实施了它的一个情节版本作为初始实施。 我理解拥有评论家的优势之一是可以使用情节中估计的值来更新演员，从而无需等到情节结束时才能用奖励来更新演员。但是，如果无论如何都在情节设置中，使用评论家而不是实际奖励有什么好处吗？    提交人    /u/Ingenuity39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</guid>
      <pubDate>Tue, 17 Sep 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>用于实现 RL 以优化数学函数的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</link>
      <description><![CDATA[有人可以推荐任何资源作为 RL 实现示例来优化数学函数/测试函数吗？因为我能找到的大多数东西基本上都在 gym 环境中。但我正在寻找一个带有代码的示例，它可以对数学函数进行优化（最好使用 actor critical，但其他方法也可以）。如果有人知道这样的资源，请提出建议。提前谢谢您。    提交人    /u/anikbis17   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</guid>
      <pubDate>Tue, 17 Sep 2024 15:46:46 GMT</pubDate>
    </item>
    <item>
      <title>预订建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</link>
      <description><![CDATA[我需要什么书来进行强化学习？ 我希望书既直观又具有数学性，我能理解艰难的数学，因为我有很强的数学背景。 向我推荐一些有很好解释并且包含很好数学内容的书。    提交人    /u/Evening-Passenger311   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</guid>
      <pubDate>Tue, 17 Sep 2024 14:25:13 GMT</pubDate>
    </item>
    <item>
      <title>如何优化奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</link>
      <description><![CDATA[我一直在用强化学习训练汽车，但一直遇到奖励函数问题。我希望汽车保持较高的恒定速度，并一直使用 speed 和最近的 progress 等参数来奖励它。但是，我注意到，当仅根据速度进行奖励时，汽车有时会加速，但会立即减速，而进度似乎根本没有影响。我还奖励了其他动作，例如 all_wheel_on_track，这很有帮助，因为每次汽车偏离赛道都会受到 5 秒的惩罚。 附注：这是 aws deep racer 比赛，如果您愿意，可以在此处查看参数。    提交人    /u/KatCelest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</guid>
      <pubDate>Tue, 17 Sep 2024 07:41:31 GMT</pubDate>
    </item>
    <item>
      <title>在没有标签的情况下使用强化学习创建合成数据</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fifx0s/synthetic_data_creation_using_reinforcement/</link>
      <description><![CDATA[那么。假设我们有每周的电力数据，但我们想创建一个模型来捕捉可能导致停电的使用高峰。强化学习代理能否在一周的不同日子中创建使用分布。代理能否捕捉模式并使用模拟数据知道根据其模拟，在特定的星期四而不是星期三或星期日将会发生停电？如果没有每日数据，它将如何评估其预测？    提交人    /u/No_Refrigerator_7841   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fifx0s/synthetic_data_creation_using_reinforcement/</guid>
      <pubDate>Mon, 16 Sep 2024 21:03:57 GMT</pubDate>
    </item>
    <item>
      <title>推荐阅读因果强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</link>
      <description><![CDATA[嗨， 我的经济学背景是因果推理（据我所知，我的背景是鲁宾学派，而不是 Pearls 学派），我想了解更多关于因果 RL 的知识。我已经观看了关于因果强化学习的本教程，但我仍然不太明白它在做什么。 有推荐阅读材料吗？这篇论文是一个好的开始吗？ 此外，我目前的理解是“传统”因果推理假设因果关系，而（一些）RL 则从数据中学习它们而不做假设？这是正确的吗？ 谢谢！    提交人    /u/WinnieXi   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</guid>
      <pubDate>Mon, 16 Sep 2024 15:36:16 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的 OpenAI Gymnasium 向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi4sus/openai_gymnasium_vector_in_observation_space/</link>
      <description><![CDATA[大家好，我在真实设备上使用 Stable Baselines3 (SB3)，并使用自定义 OpenAI Gymnasium 环境在 Python 和 Arduino 之间创建了一个接口。我想将之前的观察结果包含在我的观察空间中。目前，我的观察空间如下所示： self.high = np.array([self.maxPos, self.minDelta, self.maxVel, self.maxPow], dtype=np.float32) self.low = np.array([self.minPos, self.minDelta, self.minVel, self.minPow], dtype=np.float32) self.observation_space = space.Box(self.low, self.high, dtype=np.float32) 其中 min 和 max 值为 np.float32。我的 state 定义为： self.state = [self.ballPosition, self.ballPosition - self.desiredBallPos, self.ballVelocity, self.lastFanPower] 我想将先前位置的向量添加到我的状态中，如下所示： self.posHist = [self.stateHist[-1][0], self.stateHist[-2][0], self.stateHist[-3][0], self.stateHist[-4][0]] 然后： self.state = [self.ballPosition, self.ballPosition - self.desiredBallPos, self.ballVelocity, self.lastFanPower, self.posHist] 我应该如何更改我的self.observation_space? 问题：我应该如何修改我的 self.observation_space 以适应这些先前的位置？我想要添加此信息的原因是向网络提供有关先前状态和系统动态的数据，因为通信存在一些延迟。如果您发现此方法存在任何问题，请告诉我。我对 RL 还很陌生，仍在学习。    提交人    /u/Enroot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi4sus/openai_gymnasium_vector_in_observation_space/</guid>
      <pubDate>Mon, 16 Sep 2024 13:35:24 GMT</pubDate>
    </item>
    <item>
      <title>需要在目标机器人环境中使用 DDPG+HER 实现 MAML 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi190y/need_help_with_maml_implementation_with_ddpgher/</link>
      <description><![CDATA[大家好， 我正在做一个使用 DDPG+HER+MPI 实现 MAML 的项目。我使用 Tianhong Dai 的 hindsight-experience-replay 作为基础，并希望使用 Gymnasium fetch robotics 和 panda-gym 环境测试我的实现。目前。我面临一些挑战，希望得到一些建议来推动这一进程。 为了测试我的实现，我没有使用多个任务进行训练，而是首先尝试使用单个环境来检查实现是否有效。我可以通过调整 alpha 和 beta 参数来训练简单的环境，如 fetch-reach 或 panda-reach。但是，当我转而测试更复杂的任务（如推送或 pnp）时，即使使用不同的超参数变化，训练也会遇到困难。 当我尝试训练多个任务时，情况会变得更糟，例如使用 fetch-push 和 fetch-pnp 作为训练环境，同时尝试学习 fetch-slide 作为保留任务。 我知道将 MAML 与 DDPG（使用重放缓冲区）等离策略算法相结合并不常规，但我很想探索这种方法，看看这里是否有潜力。 我已经将代码上传到这里，如果有人想看看，请提供一些关于如何修复它的建议。 https://github.com/ncbdrck/maml_ddpg_her    由   提交  /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi190y/need_help_with_maml_implementation_with_ddpgher/</guid>
      <pubDate>Mon, 16 Sep 2024 10:34:17 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器和机器人学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhyzae/decision_transformer_and_robot_learning/</link>
      <description><![CDATA[有人知道任何文章或论文中使用决策变换器来解决机器人手臂操纵任务吗？    提交人    /u/Significant-Gene1539   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhyzae/decision_transformer_and_robot_learning/</guid>
      <pubDate>Mon, 16 Sep 2024 07:43:10 GMT</pubDate>
    </item>
    <item>
      <title>是否有可能用 SFT 来模仿 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhw29h/is_it_possible_to_mimic_rl_with_sft/</link>
      <description><![CDATA[（不确定这是否是询问此问题的正确论坛） 我有一个使用 OpenAI API 运行的代理，我想通过 RL 微调该代理。  但是，鉴于 OpenAI 仅提供 SFT API，我想知道是否可以执行以下操作 -   基于当前模型的样本情节（在采样期间融入探索） 计算每集的奖励（或者对于简单情况，获胜的奖励始终为 1） 对于每个获胜情节，创建监督标签（状态、动作） 从 3 开始对数据集应用微调  重复此过程几轮。  这会起作用吗？这实际上等同于为代理运行 RL 吗？     提交人    /u/WriterAccomplished65   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhw29h/is_it_possible_to_mimic_rl_with_sft/</guid>
      <pubDate>Mon, 16 Sep 2024 04:20:52 GMT</pubDate>
    </item>
    <item>
      <title>惩罚是否可以在任一方向（+ 和 -）改变权重？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhs3xa/is_it_feasible_for_punishments_to_alter_weights/</link>
      <description><![CDATA[我很难想出一个部署强化学习算法的策略，如果能得到任何见解，我将不胜感激。我正在构建一个模型，无论模型采取什么操作，该模型的状态都会有 96% 的时间失败。这会导致对权重的惩罚总是过早地降到最低点（这意味着它会降到 0，除非我将奖励提高到天文数字，否则永远不会回升）。我知道这是一个奇怪的问题，但如果惩罚不是严格地减去权重，这是否有意义？我的想法是，奖励会保持相同的权重，惩罚可以是加法或减法，如果这有意义的话。我只是担心它只会朝一个方向（向下）发展，而实际上，如果这有意义的话，我希望权重在两个方向（向上和向下）波动得更自由一些。    提交人    /u/Correct_Truth9920   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhs3xa/is_it_feasible_for_punishments_to_alter_weights/</guid>
      <pubDate>Mon, 16 Sep 2024 00:53:03 GMT</pubDate>
    </item>
    <item>
      <title>Dagger 手册专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhpy4q/manual_expert_for_dagger/</link>
      <description><![CDATA[大家好， 我正在研究一个结合运动规划的模仿学习问题。我有一个专家，他给出了 EEf 姿势，我用它来收集数据。行为克隆工作得还不错，符合预期。 我想继续使用 Dagger，但我将不得不花费大量时间来设置专家，以通过 Dagger 处理在线查询，而且每次迭代可能都很慢。 鉴于我的系统不是高频的，并且每集有 10 个转换，每个查询的手动输入是否可行？    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhpy4q/manual_expert_for_dagger/</guid>
      <pubDate>Sun, 15 Sep 2024 23:07:56 GMT</pubDate>
    </item>
    <item>
      <title>“扩散强制：下一个标记预测与全序列扩散相结合”，Chen 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</guid>
      <pubDate>Sun, 15 Sep 2024 16:44:39 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>