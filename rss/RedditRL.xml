<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 14 Jan 2024 12:23:46 GMT</lastBuildDate>
    <item>
      <title>减少迭代次数或其他方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196dxse/reduce_number_of_iterations_or_other_methods/</link>
      <description><![CDATA[嘿， ​ 我目前正在写我的硕士论文，涉及强化学习在代码生成中的应用。我的重点是一种新开发的领域特定语言 (DSL)，它的可用示例有限，因为还没有用这种语言编写的功能程序的广泛数据库。 我的目标是训练能够在这个新的 DSL 中编写代码的模型。对于环境，我有能力执行代码以确定它是否产生预期的输出。目前，我的方法包括随机选择 1 到 200 个操作来验证每次迭代中生成的代码是否正确。然而，事实证明这种方法非常耗时。 您能否建议我一种减少迭代次数的方法？任何见解或建议将不胜感激。 ​ 谢谢！   由   提交/u/mim549276  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196dxse/reduce_number_of_iterations_or_other_methods/</guid>
      <pubDate>Sun, 14 Jan 2024 11:32:34 GMT</pubDate>
    </item>
    <item>
      <title>奇怪的行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/196boz1/strange_behaviour/</link>
      <description><![CDATA[我正在使用 Q 学习开发剪刀石头布代理 - https://github.com/revyu/RPS .在玩mrugesh时没有任何问题，它的胜率相当稳定，但在kris上它玩得很糟糕。它可以玩 {&#39;player&#39;: 400, &#39;opponent&#39;: 201, &#39;tie&#39;: 399}, winrate=0.400000 或 {&#39;player&#39;: 0, &#39;opponent&#39;: 1000, &#39;tie&#39;: 0}, winrate=0.000000 ，没有中间结果。我对机器学习和强化学习还很陌生，无法理解发生了什么。最让我惊讶的不是算法表现不佳，而是它的结果正好位于彼此相距很远的两个点上。   由   提交 /u/revyakin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/196boz1/strange_behaviour/</guid>
      <pubDate>Sun, 14 Jan 2024 08:59:46 GMT</pubDate>
    </item>
    <item>
      <title>“潜伏特工：通过安全培训持续存在的训练欺骗性法学硕士”，Hubinger 等人 2024 {Anthropic}（RLHF 和对抗性训练未能消除法学硕士中的后门）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195x2tw/sleeper_agents_training_deceptive_llms_that/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195x2tw/sleeper_agents_training_deceptive_llms_that/</guid>
      <pubDate>Sat, 13 Jan 2024 20:17:21 GMT</pubDate>
    </item>
    <item>
      <title>“语言模型可以解决计算机任务”，Kim 等人 2023（MiniWoB++ 的内心独白）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195v9nt/language_models_can_solve_computer_tasks_kim_et/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195v9nt/language_models_can_solve_computer_tasks_kim_et/</guid>
      <pubDate>Sat, 13 Jan 2024 19:00:04 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自学</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/195l81f/reinforcement_learning_self_taught/</link>
      <description><![CDATA[大家好， 我想进入强化学习，但不知道从哪里开始，因此我想问一下如果有人对从哪里开始有任何建议，并且可能有一些资源来这样做。我是一名 STEM 专业的大学生，拥有 Python 经验，想开始深入研究强化学习，因为它看起来非常有趣且具有挑战性。我很想听听您是如何学习的，以及关于我如何学习的任何建议。 提前致谢   由   提交 /u/Simozzzo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/195l81f/reinforcement_learning_self_taught/</guid>
      <pubDate>Sat, 13 Jan 2024 10:35:08 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 学习梯度下降步长</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</link>
      <description><![CDATA[       问题陈述： 我一直在研究一个使用强化学习加速梯度下降收敛的项目。我想学习一种策略，可以将梯度下降的当前状态映射到最佳动作，即本例中的步长。提醒一下：梯度下降迭代由 x_k+1 = x_k - gamma*grad(f) 给出，其中 gamma 为步长。目前，我只考虑 f(x) = x&#39;Qx 形式的凸二次函数。我想在函数分布上训练策略，以便在预测时它可以泛化到该分布中的所有函数，以及在训练期间未见过的函数。通过在每次迭代中预测最佳步长的策略，目标是梯度下降在该分布内的所有函数的迭代次数较少的情况下收敛。 ​ 当前方法： 目前，我一直在使用无模型的强化学习算法，如 Soft Actor-Critic (SAC) 和 Twin Delayed Deep Definitive Policy Gradient (TD3) 来训练策略，但我发现即使对于某个特定函数过度拟合的简单情况，所需的内存和计算量也非常高。此外，当您过度拟合（对同一函数进行训练和评估）时，您会期望奖励收敛到某个值。如图所示，奖励确实增加了，但在某些时候代理完全忘记了它所学到的东西。我使用稀疏奖励：每次迭代中收敛时为 0，未收敛时为 -1。也许最好有一个奖励，说明每次迭代中残差（=梯度范数）的减少，这样代理不仅会在回合结束时接收信息。对于状态，我尝试了不同的方法，但仅包含当前梯度似乎或多或少有效。我使用的算法是SAC，它似乎比TD3更快。演员和评论家均由神经网络参数化，每个神经网络有 3 个隐藏层和 128 个节点。我使用了 Stable-Baselines 3 的实现。 ​ 我的问题： - 是无模型的RL 解决这个问题的正确方法是什么？它的计算成本非常高。是否有更好的方法，例如基于模型的强化学习或某种策略搜索？ - 在图中，为什么奖励突然减少？它与重放缓冲区的大小有关系吗？目前我可以分配 120Gb 的内存，这已经是相当多了。 - RL 理论通常基于马尔可夫过程。因此它假设马尔可夫性质，即当前状态完全独立于先前的状态。但是，最好添加一些有关先前梯度的信息以增加动量（例如 Nesterov 加速）。在这个框架中这可能吗？ ​ https://preview.redd.it/lnn1k9s333cc1.jpg?width=937&amp;format=pjpg&amp;auto=webp&amp;s=4254e662c840e4b4ca719b1f 70a488041376fad2   由   提交 /u/Lennitar   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</guid>
      <pubDate>Fri, 12 Jan 2024 22:15:47 GMT</pubDate>
    </item>
    <item>
      <title>解释方差增加，但随后稳定在低值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1956qfb/explained_variance_increases_but_then_stabilizes/</link>
      <description><![CDATA[      您好， 我正在使用 Stable Baselines 3 在我的自定义环境中训练 A2C 模型。我的自定义环境是离散的、多维的 (4 * 21)，用于操作和观察。我一直在尝试调整超参数，但似乎对于所有超参数集，都存在一个常见问题，即解释的方差首先增加，但随后保持在 &lt;&lt; 30%。 此外，当我评估我的策略时，似乎 model.predict(obs) 结果（动作预测）始终是单个动作，不依赖于观察。这是因为低解释方差表明仅使用“平均动作”并没有更好吗？谢谢！ ​ https://preview.redd.it/sr4twq7ux2cc1.png?width=1191&amp;format=png&amp;auto=webp&amp;s=0a3653b2c228fdb97306d8f8093ec5ec0926f3 b1 https://preview.redd.it/vux1wq7ux2cc1.png?width=1178&amp;格式=png&amp;auto=webp&amp;s=5da34c134fde44f7340fb30e9e678e86fe74e5bf   由   提交 /u/polymerase2   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1956qfb/explained_variance_increases_but_then_stabilizes/</guid>
      <pubDate>Fri, 12 Jan 2024 21:45:11 GMT</pubDate>
    </item>
    <item>
      <title>如何裁剪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</link>
      <description><![CDATA[我熟悉 RL 一个月了，想在一些 Atari 游戏上练习我的知识。我阅读了 DeepMind 的文章，了解他们如何使用 DQN 执行 Atari 任务 (https://arxiv.org/abs/1312.5602)。他们在论文中表示： “直接处理原始 Atari 帧（这些帧是具有 128 种调色板的 210 × 160 像素图像）的计算要求很高，因此我们应用了一个基本的预处理步骤，旨在减少输入维度。对原始帧进行预处理，首先将其 RGB 表示转换为灰度并将其下采样为 110×84 图像。最终的输入表示是通过裁剪图像的 84 × 84 区域来获得的，该区域大致捕获了游戏区域。” 这对我来说似乎很合理，但我想知道他们是如何以编程方式做到这一点的？我阅读了gymnasium（不是gym）文档（https://gymnasium.farama.org/api/wrappers/）但是，尽管他们有 FrameStack 和 GreyScale 的包装器，但下采样和裁剪包装器似乎不可用。 有人知道如何做到这一点吗？  非常感谢你们。   由   提交/u/Q_H_Chu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</guid>
      <pubDate>Fri, 12 Jan 2024 13:07:01 GMT</pubDate>
    </item>
    <item>
      <title>混合模拟和抽象进行物理推理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194orj6/blending_simulation_and_abstraction_for_physical/</link>
      <description><![CDATA[论文：https： //osf.io/preprints/psyarxiv/f9ukv 代码： https ://github.com/flxsosa/physicals_abstraction 摘要：  人们如何能够理解日常的物理事件这么轻松？一种假设表明人们使用对世界的近似概率模拟。一个对比假设是人们使用抽象或特征的集合。这两个假设解释了物理推理的互补方面。我们开发了一个综合了两种假设的“混合模型”：在某些条件下，模拟被视觉空间抽象（线性路径投影）取代。这种抽象以保真度为代价换取了效率，混合模型预测，只要满足应用抽象的条件，人们就会犯系统性错误。我们在两个实验中测试了这一预测，参与者对下落的球是否会接触目标进行判断。首先，我们表明，当直线路径不可用时，即使仿真时间保持固定，响应时间也会更长，这与纯仿真模型（实验 1）相反。其次，我们表明人们以与线性路径投影一致的方式错误地判断了球的轨迹（实验2）。我们的结论是，人们可以使用灵活的心理物理引擎，但会在有用时自适应地调用更有效的抽象。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194orj6/blending_simulation_and_abstraction_for_physical/</guid>
      <pubDate>Fri, 12 Jan 2024 06:52:59 GMT</pubDate>
    </item>
    <item>
      <title>[问题] 具有连续动作空间的 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194mfnt/question_dqn_with_continuous_action_spaces/</link>
      <description><![CDATA[        由   提交 /u/tengboss   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194mfnt/question_dqn_with_continuous_action_spaces/</guid>
      <pubDate>Fri, 12 Jan 2024 04:40:45 GMT</pubDate>
    </item>
    <item>
      <title>太空战争 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</link>
      <description><![CDATA[   /u/_Linux_AI_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</guid>
      <pubDate>Fri, 12 Jan 2024 03:50:56 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习中正则化和（有效）贴现之间的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1947wv7/relationship_between_regularization_and_effective/</link>
      <description><![CDATA[我在 minigrid 型环境。训练后，我将智能体置于一系列人为的情况下并测量其 Q 值，然后从这些 Q 值推断其有效折扣率（例如，根据前进价值如何随着接近目标而变化来推断折扣因子） ）。 当我以这种方式测量有效折扣因子时，它与我使用的显式折扣因子 (𝛾) 设置相匹配。 但是如果我添加对网络进行非常强的 L2 正则化（权重衰减），即使我没有更改代理的 𝛾 设置，推断的折扣因子也会降低。 有人可以帮我思考为什么会发生这种情况吗？谢谢！   由   提交/u/Beneficial_Price_560   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1947wv7/relationship_between_regularization_and_effective/</guid>
      <pubDate>Thu, 11 Jan 2024 17:57:43 GMT</pubDate>
    </item>
    <item>
      <title>《Marvin Minsky’s Vision of the Future》，Bernstein 1981（明斯基的研究生涯，包括神经网络 SNARC 小鼠）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1946okz/marvin_minskys_vision_of_the_future_bernstein/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1946okz/marvin_minskys_vision_of_the_future_bernstein/</guid>
      <pubDate>Thu, 11 Jan 2024 17:08:38 GMT</pubDate>
    </item>
    <item>
      <title>“计算机双陆棋”，Hans J. Berliner 1980（“BKG 9.8 是第一个在棋盘或纸牌游戏中击败世界冠军的计算机程序”）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1945ymh/computer_backgammon_hans_j_berliner_1980_bkg_98/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1945ymh/computer_backgammon_hans_j_berliner_1980_bkg_98/</guid>
      <pubDate>Thu, 11 Jan 2024 16:39:07 GMT</pubDate>
    </item>
    <item>
      <title>完成 RL 博士学位后我可以在哪里工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194406q/where_can_i_work_after_finishing_a_phd_in_rl/</link>
      <description><![CDATA[ 由   提交 /u/Trevorego   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194406q/where_can_i_work_after_finishing_a_phd_in_rl/</guid>
      <pubDate>Thu, 11 Jan 2024 15:14:40 GMT</pubDate>
    </item>
    </channel>
</rss>