<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 08 Dec 2023 12:25:41 GMT</lastBuildDate>
    <item>
      <title>在几秒钟内学会飞行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18df2os/learning_to_fly_in_seconds/</link>
      <description><![CDATA[       由   提交/u/jonas-eschmann   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18df2os/learning_to_fly_in_seconds/</guid>
      <pubDate>Fri, 08 Dec 2023 04:52:42 GMT</pubDate>
    </item>
    <item>
      <title>通过 4 个开创性项目向初学者介绍强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dbmy6/a_beginners_intro_to_rl_through_4_seminal_projects/</link>
      <description><![CDATA[   /u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dbmy6/a_beginners_intro_to_rl_through_4_seminal_projects/</guid>
      <pubDate>Fri, 08 Dec 2023 01:49:29 GMT</pubDate>
    </item>
    <item>
      <title>“利用基于优势的离线策略梯度改进语言模型”，Baheti 等人，2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dagry/improving_language_models_with_advantagebased/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dagry/improving_language_models_with_advantagebased/</guid>
      <pubDate>Fri, 08 Dec 2023 00:49:32 GMT</pubDate>
    </item>
    <item>
      <title>我的定制体育馆环境似乎根本没有学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18d8tkc/my_custom_gymnasium_env_seems_not_learning_at_all/</link>
      <description><![CDATA[训练了一整天似乎没有学到是强化学习还是环境问题 https://github.com/jonnytracker/Flappy-Bird-RL &lt;!-- SC_ON - -&gt;  由   提交/u/jonnytracker2020   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18d8tkc/my_custom_gymnasium_env_seems_not_learning_at_all/</guid>
      <pubDate>Thu, 07 Dec 2023 23:29:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Gumbel-softmax 与 TD3 一起使用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18d8ib1/why_does_gumbelsoftmax_work_with_td3/</link>
      <description><![CDATA[我的假设是“在单臂老虎机设置中，选择正确的箱子将返回 1 的奖励，返回函数将成为一块”明智的常数函数。因此，critic的梯度几乎到处都是0”。 但是，我尝试了。 TD3 以 Gumbel-softmax 作为输出层。它会学习！我不知道为什么它会学习。  此外，设置温度太低无法学习&#39; 谁能解释一下发生了什么以及我错过了什么？谢谢   由   提交/u/Lopside_Hall_9750   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18d8ib1/why_does_gumbelsoftmax_work_with_td3/</guid>
      <pubDate>Thu, 07 Dec 2023 23:14:25 GMT</pubDate>
    </item>
    <item>
      <title>规划的循环网络模型解释了海马体重放和人类行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cu0wa/a_recurrent_network_model_of_planning_explains/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2023.01.16.523429v2 代码：https://github.com/KrisJensen/planning_code 摘要：  当面对新的情况时，人类常常会花大量时间思考可能的未来。为了使这种计划变得合理，行为的好处必须补偿花费在思考上的时间。在这里，我们通过开发神经网络模型来捕获人类行为的这些特征，其中计划本身由前额叶皮层控制。该模型由一个元强化学习代理组成，该代理具有通过从其自己的策略中采样想象的动作序列来进行计划的能力，我们称之为“推出”。当计划有益时，代理会学习计划，解释人类思维时代的经验变化。此外，人工智能体所采用的策略推出模式与最近在空间导航过程中记录的啮齿动物海马回放模式非常相似。我们的工作提供了一种新理论，说明大脑如何通过前额叶-海马体相互作用来实施规划，其中海马体重放是由前额叶动态触发并适应性影响的。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cu0wa/a_recurrent_network_model_of_planning_explains/</guid>
      <pubDate>Thu, 07 Dec 2023 11:55:55 GMT</pubDate>
    </item>
    <item>
      <title>强化学习“最方便”的期刊俱乐部</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cphho/most_convenient_journal_club_for_reinforcement/</link>
      <description><![CDATA[大家好！我正在为 RL 社区中的一些人解决问题，并希望得到您的意见/想法。 问题：  没有时间阅读最酷、最新的论文 单独处理论文并不那么有趣，而且通常很困难。 无法承诺参加每周的会议，需要完全的灵活性 &lt; /ol&gt; 为了解决这些问题，我建立了一个名为 DenseLayers.com 的期刊俱乐部网站。 帮助我需要的是反馈！如果可能，请至少回答其中一个问题。 :)   您目前阅读论文的频率如何，以上 3 个问题中的哪一个与您产生共鸣？ 您目前正在采取哪些措施来解决该问题？什么可以完全消除这个问题？ 您喜欢阅读和理解哪些流行的强化学习论文（如果您有时间的话）？  谢谢！   由   提交 /u/mngrwl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cphho/most_convenient_journal_club_for_reinforcement/</guid>
      <pubDate>Thu, 07 Dec 2023 06:30:13 GMT</pubDate>
    </item>
    <item>
      <title>对于 MAPPO，批评者网络应该如何处理全局状态输入？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cmzi9/for_mappo_how_should_the_critic_network_handle/</link>
      <description><![CDATA[我正在使用MAPPO算法来解决多无人机协作导航问题。其中，无人机代理的观测信息是图像和自身速度。我想知道批评者网络如何以最合适的方式处理输入的全局信息？我目前想到两种方法。第一种方法是将所有无人机图像堆叠起来，并使用大型CNN网络对其进行处理以获得特征向量。第二种方法是使用小型CNN网络分别处理每个无人机图像以获得特征向量，然后将这些向量拼接在一起。哪种方法更好？或者有更好的方法来解决这个问题吗？   由   提交/u/Cute-Heron-1709   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cmzi9/for_mappo_how_should_the_critic_network_handle/</guid>
      <pubDate>Thu, 07 Dec 2023 04:04:40 GMT</pubDate>
    </item>
    <item>
      <title>强化学习找到的解距离最优解有多远？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cfm38/how_far_is_the_solution_found_by_reinforcement/</link>
      <description><![CDATA[例如，在像西洋跳棋这样完全解决的游戏中（肯定会以双方完美的平局结束），是否有任何研究比较深度学习与最优策略？像 AlphaZero 这样的深度强化学习方法能否在有限的训练时间内以最优策略实现 99% 的绘图概率？   由   提交 /u/ZealousidealRub8250   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cfm38/how_far_is_the_solution_found_by_reinforcement/</guid>
      <pubDate>Wed, 06 Dec 2023 22:04:50 GMT</pubDate>
    </item>
    <item>
      <title>节目介绍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18c3gm4/program_induction/</link>
      <description><![CDATA[有人可以解释一下程序归纳方法如何在元学习机器学习模型上发挥作用   由   提交/u/NoShoe6803  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18c3gm4/program_induction/</guid>
      <pubDate>Wed, 06 Dec 2023 13:00:19 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗树搜索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bu5wh/monte_carlo_tree_search/</link>
      <description><![CDATA[MCTS可以用于多代理问题吗？   由   提交/u/Meta_Sage_247   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bu5wh/monte_carlo_tree_search/</guid>
      <pubDate>Wed, 06 Dec 2023 03:01:02 GMT</pubDate>
    </item>
    <item>
      <title>“越野自动驾驶汽车的多模态动力学建模”，Tremblay 等人，2020</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bo4rd/multimodal_dynamics_modeling_for_offroad/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bo4rd/multimodal_dynamics_modeling_for_offroad/</guid>
      <pubDate>Tue, 05 Dec 2023 22:19:56 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习中时间信用分配的调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bai3f/a_survey_of_temporal_credit_assignment_in_deep/</link>
      <description><![CDATA[https://arxiv.org/abs/2312.01072   由   提交/u/Conscious_Heron_9133   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bai3f/a_survey_of_temporal_credit_assignment_in_deep/</guid>
      <pubDate>Tue, 05 Dec 2023 12:02:23 GMT</pubDate>
    </item>
    <item>
      <title>持续学习：应用和前进之路</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18b8ial/continual_learning_applications_and_the_road/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2311.11908 OpenReview：https:// /openreview.net/forum?id=axBIMcGZn9 Dagstuhl 研讨会 23122 总结：https://drops.dagstuhl.de/entities/document/10.4230/DagRep.13.3.74 摘要&lt; /strong&gt;:  持续学习是机器学习的一个子领域，其目的是让机器学习模型能够不断地学习新数据，通过积累知识而不忘记过去学到的东西。在这项工作中，我们退一步问：“为什么人们首先应该关心持续学习？”。我们通过调查最近在三个主要机器学习会议上发表的持续学习论文来奠定基础，并表明内存受限的设置在该领域占据主导地位。然后，我们讨论了机器学习中的五个开放问题，尽管乍一看它们似乎与持续学习无关，但我们表明持续学习将不可避免地成为其解决方案的一部分。这些问题包括模型编辑、个性化、设备上学习、更快（重新）训练和强化学习。最后，通过比较这些未解决问题的需求和当前持续学习的假设，我们强调并讨论了持续学习研究的四个未来方向。我们希望这项工作为持续学习的未来提供一个有趣的视角，同时展示其潜在价值以及我们为了使其成功而必须追求的路径。这项工作是作者在 2023 年 3 月的 Dagstuhl 深度持续学习研讨会上进行多次讨论的结果。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18b8ial/continual_learning_applications_and_the_road/</guid>
      <pubDate>Tue, 05 Dec 2023 09:44:59 GMT</pubDate>
    </item>
    <item>
      <title>拼图解算器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18b4p6c/nonogram_solver/</link>
      <description><![CDATA[您好！我在 ML 和神经网络方面有一些经验，但这是我第一次涉足 RL。我希望能够解决非图（数字难题），并且我已经找到了一种将约束数据预处理为 10x10 浮点网格（0-1（含）之间）的方法。我希望输出是二进制值或布尔值的 10x10 网格，但是我不太确定如何构建它。我觉得我遇到的问题是如何维持状态。使用预处理网格作为“游戏板”并让代理在其顶部放置 0 和 1（在使浮动范围不包含之后）是一个好主意吗，即使这会删除该网格中的先前信息正方形？有没有一种方法可以为约束信息设置一个单独的网格，并为游戏板设置一个单独的网格来保持空间数据的相关性？欢迎任何想法，谢谢！   由   提交/u/FissioN47   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18b4p6c/nonogram_solver/</guid>
      <pubDate>Tue, 05 Dec 2023 05:22:30 GMT</pubDate>
    </item>
    </channel>
</rss>