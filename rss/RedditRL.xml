<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 29 Oct 2024 12:33:27 GMT</lastBuildDate>
    <item>
      <title>Perplexity AI PRO - 1 年计划优惠 - 近 75% 折扣！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ges81f/perplexity_ai_pro_1_year_plan_offer_almost_75_off/</link>
      <description><![CDATA[      如标题：我们提供一年计划的 Perplexity AI PRO 优惠券代码。  订购：https://cheapgpt.store/product/perplexity-ai-pro-subscription-one-year-plan 接受的付款方式： - PayPal。 （100% 买家保护） - Revolut。    提交人    /u/MReus11R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ges81f/perplexity_ai_pro_1_year_plan_offer_almost_75_off/</guid>
      <pubDate>Tue, 29 Oct 2024 11:25:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 Memoroids 进行循环强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gekts9/recurrent_reinforcement_learning_with_memoroids/</link>
      <description><![CDATA[  由    /u/smorad  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gekts9/recurrent_reinforcement_learning_with_memoroids/</guid>
      <pubDate>Tue, 29 Oct 2024 02:58:28 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家模型对所有输入采取相同的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gek86t/actor_critic_model_taking_same_action_for_all/</link>
      <description><![CDATA[大家好， 我正在研究 Actor Critic 模型，以便根据时间序列数据做出决策。我尝试了不同的模型架构，如 LSTM、Transformer 甚至 Mamba，以检查任何不同的结果，但没有任何变化。基本上，我将时间序列数据传递给我的模型，以便在每个时间步骤中从 6 个动作中选择一个动作。 对于输入数据，我尝试了 2 种不同的方法；全上下文长度和输入形状如 (seq_len, hidden_​​dim)，在这种情况下，模型将采取所有时间步骤并为每个时间步骤提供动作输出。我还尝试了固定上下文长度和类似 (batch_size, seq_len, hidden_​​dim) 的输入形状，在这种情况下，模型为每个批次而不是时间步长创建一个动作输出。 我还实现了 Epsilon Greedy 进行探索，并在每个 epoch 结束时打印每个动作的选定动作百分比作为输出，以检查模型输出。 我的问题从这一点开始，我正在使用 epsilon 退火进行 epoch，在训练时它正在减少 epsilon 数以减少探索。当我检查我的 Critic Loss 值随时间的变化时，它正在显着减少（Critic 的 MSE Loss 从 0.9 左右开始，对于 +1 和 -1 之间的奖励值下降到 0.002）所以我认为批评家正在学习。 在每个 epoch 结束时，我还在运行评估以查看状态，在评估时，我还在检查每个动作的选定动作百分比。问题是，无论输入是什么，我的模型都会选择相同的动作。在训练中，我使用贪婪的 epsilon 分类抽样，因此它选择了不同的动作，但是当我在评估步骤使用 argmax 时，它选择了相同的动作。  我尝试过的；具有各种参数大小的不同模型架构。不同的学习率，从太高到太低。不同的初始 epsilon 值。不同的输入类型（固定长度和完整上下文）  它们都没有任何区别。我从这个例子中实现了我的演员评论家模型，并仔细检查了是否有任何错误； https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f 经过这一切，我找不到任何解决方案。有人有什么想法吗？    由    /u/BagComprehensive79  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gek86t/actor_critic_model_taking_same_action_for_all/</guid>
      <pubDate>Tue, 29 Oct 2024 02:26:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习不仅能解决难题，还能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1geez5d/rl_solves_hard_problems_but_also/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1geez5d/rl_solves_hard_problems_but_also/</guid>
      <pubDate>Mon, 28 Oct 2024 22:21:13 GMT</pubDate>
    </item>
    <item>
      <title>多头PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gee2o7/multihead_ppo/</link>
      <description><![CDATA[大家好， 我有一个有四个头的网络。一个头（有三个动作）决定是否执行第二个、第三个或第四个头的动作。第二个头的动作有助于情节结束，而第三个和第四个头的动作对于实现主要目标很重要。我正在使用 PPO，但有时网络会卡在第三个或第四个头，因此情节永远不会结束。有人知道是什么导致了这种行为吗？    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gee2o7/multihead_ppo/</guid>
      <pubDate>Mon, 28 Oct 2024 21:43:11 GMT</pubDate>
    </item>
    <item>
      <title>机器人深度强化学习：现实世界的成功调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ge2hn5/deep_reinforcement_learning_for_robotics_a_survey/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ge2hn5/deep_reinforcement_learning_for_robotics_a_survey/</guid>
      <pubDate>Mon, 28 Oct 2024 13:48:41 GMT</pubDate>
    </item>
    <item>
      <title>组合 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ge11zq/combining_dqns/</link>
      <description><![CDATA[将 3 个 DQN 组合成一个 DQN 的最佳方法是什么。每个 DQN 都有类似的参数，就像它们在不同的任务上工作但仍然相似。例如，假设我们有一个有敌人和状态的游戏。首先，您可以使用 3 个动作。  使用剑 使用弓 使用魔法  如果您使用剑，您可以使用 2 种不同的动作，如轻攻击或重攻击。如果您使用弓，您可以用它击中敌人的近战，或者使用箭（如果有）等。 我不想创建一个可以决定第一个动作（将使用什么样的武器）然后为每种武器决定将采取什么样的动作的 DQN，而是想为每种武器创建一个 DQN，它确切地知道如何使用一种武器，然后将它们组合成 1。最终的网络应该从状态中了解将使用哪种武器以及这些武器将采取什么动作。    提交人    /u/volvol7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ge11zq/combining_dqns/</guid>
      <pubDate>Mon, 28 Oct 2024 12:43:08 GMT</pubDate>
    </item>
    <item>
      <title>期待明年开始攻读 RL 博士学位——寻求建议！🤞🏼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdyx82/looking_forward_to_start_phd_in_rl_next_year/</link>
      <description><![CDATA[大家好！我准备明年开始攻读强化学习博士学位，但需要一些关于积累研究经验和与教授联系的建议。  背景：我做过很多实际的强化学习项目（MARL、PettingZoo、策略梯度工作），但我缺乏正式的研究经验。我也有一年的 ML/LLM 实习经历，但寻找强化学习的研究实习机会一直很困难。 挑战：我担心接触 LOR，因为我没有直接与强化学习的教授合作过。任何关于有助于展示我的能力的项目建议或接触技巧都将非常有帮助。另外，如果有人有兴趣合作，请直接发信息给我！ &lt;3  任何见解都值得赞赏 - 谢谢大家！    提交人    /u/cheenchann   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdyx82/looking_forward_to_start_phd_in_rl_next_year/</guid>
      <pubDate>Mon, 28 Oct 2024 10:41:28 GMT</pubDate>
    </item>
    <item>
      <title>哪些 RL 算法适用于计算心理学？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdytbm/which_rl_algorithms_for_computational_psychology/</link>
      <description><![CDATA[我是一名数据科学家，希望通过多个代理模拟人类社交互动。我只是想知道是否有人可以指点一下要探索哪些算法。例如，如果我想鼓励反馈循环和突发行为以更真实地描述人类行为，我应该使用无模型算法还是基于模型的算法？ 从我最初的研究中，我听说了关于 Decision Transformer 和 DreamerV3 的好评。 感谢您的时间！    提交人    /u/culturedindividual   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdytbm/which_rl_algorithms_for_computational_psychology/</guid>
      <pubDate>Mon, 28 Oct 2024 10:33:59 GMT</pubDate>
    </item>
    <item>
      <title>对抗性稳健深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdyrwe/adversarial_robust_deep_reinforcement_learning/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz/adversarial-reinforcement-learning    由   提交  /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdyrwe/adversarial_robust_deep_reinforcement_learning/</guid>
      <pubDate>Mon, 28 Oct 2024 10:31:18 GMT</pubDate>
    </item>
    <item>
      <title>本科生在现实生活中的研究想法！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdvrw0/undergraduate_research_in_rl_ideas/</link>
      <description><![CDATA[我希望完成与 RL 相关的荣誉项目。今年夏天，我将与我的教授一起开展一个小型 RL 代理项目，我的任务是制作一个 RL 蠕虫。我对 RL 的世界还很陌生，希望能得到任何建议或地方，让新手开始研究这个主题！    提交人    /u/Future-Catch-4896   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdvrw0/undergraduate_research_in_rl_ideas/</guid>
      <pubDate>Mon, 28 Oct 2024 06:41:06 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关 PettingZoo 和 MARL 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdq509/seeking_help_with_pettingzoo_and_marl_in_general/</link>
      <description><![CDATA[大家好！我正在开发一个小型 MARL 项目，希望可以扩展到更大的项目。我的想法是基于网格的，MPE 的一些自定义变体可能是可行的方法。你们当中有谁有使用 PettingZoo 或其他 MARL 库的经验，可以帮我快速上手吗？我愿意付钱。谢谢！    提交人    /u/zarmesan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdq509/seeking_help_with_pettingzoo_and_marl_in_general/</guid>
      <pubDate>Mon, 28 Oct 2024 00:58:02 GMT</pubDate>
    </item>
    <item>
      <title>我是否以正确的方式使用 PPO 来完成这项连续（算法交易）任务？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gdaof1/am_i_using_ppo_the_right_way_for_this_continuous/</link>
      <description><![CDATA[大家好。 我是 RL 的新手，希望经验丰富的 RL 用户能够回答我的一些问题，我将不胜感激。 所以我有这个被包装到 gymnasium env 中的环境，但基本上它是一个基于事件的回测，处理 tick 级别的 HFT 数据。我有一个做市交易策略代码，每 100 毫秒向交易所发送订单，每 5 秒更新一次滚动指标。根据滚动指标的值，它决定从中间价发送订单的深度以及根据当前库存如何积极地扭曲价格。该策略正在实时交易，并且通常有利可图，但我读过一篇精彩的论文，其中讨论了在给定更密集的特征集的情况下动态调整交易策略的超参数，我对这个想法很着迷，所以我试图部分复制它。 因此，代理的任务是评估当前状态并返回两个连续动作的向量，交易策略将在接下来的 5 秒内使用这两个超参数。RL 代理使用的特征集比策略本身的特征集更加多样化，因此这是一个更智能的代理帮助更机器人化的代理的情况。这两个超参数的值首先是在通过回测对策略进行贝叶斯优化期间找到的（因此此时还没有 RL），然后动作向量是一个连续空间，其中最佳参数位于中间，例如 min_value = optimal_value * 0.75，max_value = optimal_value * 1.25。这样做是为了不让代理偏离策略的最佳参数值太多，而只是让它更灵活一点。 奖励函数设置为最后 5 秒步骤和前一个步骤期间 pnl 平均值之间的变化。还添加了一些惩罚项以供实验。 我为此使用了 SB3。env 被包装到 VecNormalize 中，action_space 介于 -1 和 1 之间，然后在环境中被反规范化。由于代理是在使用历史数据的回测器上训练的，而我只有有限的历史数据，因此我按以下方式设置训练： - 使用 make_vec_env 和 SubprocVecEnv 实例化 48（CPU 数量）个环境 - 启动（或重置）环境时，从可用数据中随机挑选一天（通常是 10-20 天的数据） - 当这一天用尽时，向模型发送 Truncated 信号 - 继续执行预设的步骤数 问题是模型似乎根本没有学习。当我将数据限制为 10 小时的数据时，它可以很好地学习（可能只是过度拟合），但是当我向它提供 10-20 天的数据并要求它找到提供超参数的最佳策略时，它却无法做到。事实是，如果它始终只是选择行动的中间值，它至少会获得较大的累积奖励。  所以我有以下问题： - 这是设置这种固有连续任务的正确方法吗（例如使用截断情节）？ - PPO 是否是适合此类任务的算法？ - 由于环境仅部分可观察，应该特别注意哪些超参数？ - 由于我们为接下来 5 秒的交易策略提供了参数值，而没有人知道接下来 5 秒会发生什么，我们是否应该更积极地折扣未来的奖励？ - 使用离散动作空间是否会让代理更容易学习正确的策略？ 我目前正在尝试较大的 n_step 值，当乘以步数时，需要一整天的交易（总共约 200K 步）和更大的批量大小，这显示出一些改进，但仍然没有学习。我允许它训练 20M 步。    提交人    /u/StabbMe   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gdaof1/am_i_using_ppo_the_right_way_for_this_continuous/</guid>
      <pubDate>Sun, 27 Oct 2024 13:13:50 GMT</pubDate>
    </item>
    <item>
      <title>介绍将军机器人：一个快节奏的战略游戏环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gda8hj/introducing_generalsbots_a_fastpaced_strategy/</link>
      <description><![CDATA[        由    /u/shrekofspeed  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gda8hj/introducing_generalsbots_a_fastpaced_strategy/</guid>
      <pubDate>Sun, 27 Oct 2024 12:50:35 GMT</pubDate>
    </item>
    <item>
      <title>我一直在尝试“Simba：深度强化学习中扩大参数的简单偏差”，TQC 和这个的结合真是一个怪物！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gd8j5c/ive_been_trying_out_simba_simplicity_bias_for/</link>
      <description><![CDATA[      https://preview.redd.it/pay0fmh36axd1.png?width=1500&amp;format=png&amp;auto=webp&amp;s=42e8188a85ccbc51da4345f99f62a8a59b32b29a 我看到了关于辛巴的帖子（链接) 并立即在我管理的玩具项目存储库中实现它，只需切换到它就可以看到非常显着的性能提升，最显着的是在 TQC 中。实现如下：https://github.com/tinker495/jax-baseline 看到如此优秀的研究在我自己的代码中带来的好处非常令人兴奋，我感谢 SonyResearch 分享这些研究！    提交人    /u/New_East832   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gd8j5c/ive_been_trying_out_simba_simplicity_bias_for/</guid>
      <pubDate>Sun, 27 Oct 2024 11:05:51 GMT</pubDate>
    </item>
    </channel>
</rss>