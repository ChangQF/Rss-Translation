<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 03 Jul 2024 12:28:49 GMT</lastBuildDate>
    <item>
      <title>DRL 算法是否有可能像监督学习一样面临过度拟合问题？如果是这样，那么我们如何检查和缓解它？我将感谢任何与此相关的信息。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dud2i0/does_there_is_possibility_to_drl_algorithm_to/</link>
      <description><![CDATA[  由    /u/Correct-Jaguar-339  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dud2i0/does_there_is_possibility_to_drl_algorithm_to/</guid>
      <pubDate>Wed, 03 Jul 2024 12:25:33 GMT</pubDate>
    </item>
    <item>
      <title>寻求 RL 研究和实验出版标准的指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duc2dc/seeking_guidance_on_rl_research_and_experiment/</link>
      <description><![CDATA[大家好， 我是强化学习 (RL) 研究领域的新手，有几个问题。如果您能提供任何帮助，我将不胜感激。请原谅我的英语不好。  强化学习研究中的实验结果标准：  我正在努力改进 Soft Actor-Critic (SAC) 算法。在知名的机器学习期刊上发表实验结果的惯例或标准是什么？例如，在 DeepMind Control Suite 或 OpenAI Gym 等知名基准上，平均总奖励提高 5%-10% 就足够了吗？  实验步骤和重复：  我注意到一些 RL 研究论文进行了 200 万步的实验，并重复每个实验五次。如果我进行 100 万步实验并重复每个实验三次，结果是否仍然足以令人信服，使我的论文被顶级 ML 期刊接受？产生令人信服的结果所需的最少步骤和重复次数是多少？  RL 训练的硬件建议：  我应该投资 M2 Ultra 或配备 i9-14900K 和 RTX 4090 的 PC 来训练我的模型吗？在 M2 Ultra 上使用 Docker 运行实验是否可行，结果是否足以令人信服地发表？ 提前感谢您的指导！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duc2dc/seeking_guidance_on_rl_research_and_experiment/</guid>
      <pubDate>Wed, 03 Jul 2024 11:30:51 GMT</pubDate>
    </item>
    <item>
      <title>JAX 可以以 10 倍于我的速度在 6 天内训练出 27B Gemma。对还是错。1T 代币。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du9not/jax_can_10x_my_speed_train_27b_gemma_in_6_days/</link>
      <description><![CDATA[您好，我有 8xh100 GPU，我想用它们来训练 JAX。 我可以训练 GEMMA 27B 并使用 JAX 来缩短训练时间，以便在 7 天内完成吗？ 我想知道我的 200GB 数据集是否太小？ 如果不可能，可能是因为 - 法律语料库很难分块，因此很难标记以允许 JIT 库正确优化 GPU？ 您会建议使用哪些库？ 获取更大的数据集？这会增加多少时间？ 哪种类型的数据集（我正在使用简单的指示提示）最难训练，哪些在花费的时间方面最容易？ 如果不是 7 天，实际上需要多长时间？ 如果我想提高效率并充分利用 JAX，您会建议我做什么？  训练 3B phi？ 只是微调更大的 LLM 改进我的分块，如果是的话，我应该从什么资源中学习？ 学习某些库和线性代数 forJAX 以使用提示训练指示数据集？     提交人    /u/Pleasant_Bit_4562   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du9not/jax_can_10x_my_speed_train_27b_gemma_in_6_days/</guid>
      <pubDate>Wed, 03 Jul 2024 08:50:54 GMT</pubDate>
    </item>
    <item>
      <title>利用代理来产生自己的塑造奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du8jf9/leverage_an_agent_that_generate_its_own_shaping/</link>
      <description><![CDATA[我有一个非马尔可夫问题，具有部分可观察状态，但我能够通过单独的方式为该状态生成价值函数。 以不同的方式表述，我可以独立于环境生成塑造奖励。但是，知道你离目标有多远，并不等同于知道该做什么。 关于如何利用此类信息的任何想法或文献？    提交人    /u/Omnes_mundum_facimus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du8jf9/leverage_an_agent_that_generate_its_own_shaping/</guid>
      <pubDate>Wed, 03 Jul 2024 07:31:19 GMT</pubDate>
    </item>
    <item>
      <title>DRL 算法中填充过多</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du4vkf/overfilled_in_drl_algorithm/</link>
      <description><![CDATA[大家好， 我想知道 DQN 算法和其他 DRL 算法是否会像监视学习一样面临过度拟合问题。如果是这样，那么我们如何检查并缓解它？    提交人    /u/Correct-Jaguar-339   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du4vkf/overfilled_in_drl_algorithm/</guid>
      <pubDate>Wed, 03 Jul 2024 03:42:24 GMT</pubDate>
    </item>
    <item>
      <title>强化学习解决装配线平衡问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du4de0/reinforcement_learning_for_assembly_line/</link>
      <description><![CDATA[大家好！我想听听您对使用深度强化学习解决装配线平衡问题 (ALBP) 的看法。这是工业工程和运筹学中一个众所周知的优化问题，重点是如何有效地组织装配线上的任务，以最大限度地提高生产率，并最大限度地减少闲置时间或瓶颈。 以下是一些需要牢记的关键概念： 任务：这些是组装产品所需的单个操作或活动，每个操作或活动都有其特定的处理时间。 工作站：这些是装配线上执行任务的指定区域。 周期时间：这是指每个工作站完成其分配的任务所允许的最长时间，它决定了装配线的生产率。 优先约束：由于产品组装的性质，某些任务必须在其他任务之前完成。这些关系在优先图中表示。    提交人    /u/Icy_Bar_681   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du4de0/reinforcement_learning_for_assembly_line/</guid>
      <pubDate>Wed, 03 Jul 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>测试 dreamerv3 的最佳库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtxw7t/best_library_for_testing_out_dreamerv3/</link>
      <description><![CDATA[嘿，我想测试一下 dreamerv3，我尝试使用 rayrl，但我认为 tensorflow 的最新更新破坏了它，或者他们必须更新他们的东西。无论如何，什么是尝试 d​​reamerv3 并在其他环境中进行训练的最佳库？    提交人    /u/hinsonan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtxw7t/best_library_for_testing_out_dreamerv3/</guid>
      <pubDate>Tue, 02 Jul 2024 21:57:00 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Geometric、强化学习和 OpenAI Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtmo5l/pytorch_geometric_reinforcement_learning_and/</link>
      <description><![CDATA[大家好。 正如标题所述，我正在尝试实现 openai gymnasium frostylake-v1 环境，以 pytorch 几何知识图谱表示，其中每个单元都是一个知识图谱节点，并且每条边都连接到玩家可以采取的可能路线。但是，我遇到了一个问题，即除非节点特征包含唯一值（无论是唯一节点索引还是它们在 4x4 地图中的位置），否则我的模型无法生成良好的结果。 我需要它独立于这些唯一索引，并且可能在一张地图上进行训练，然后将训练有素的代理放在一张新地图上，在那里他仍然能够对好动作和坏动作有一些概念（例如，掉进洞里总是不好的）。我该如何扩展这个问题？我做错了什么？如需更多信息，请在评论中留下，我一定会回答。 我正在写一篇论文，这个 openai gym 与我将在最终论文中进行训练的环境类似。所以我真的需要帮助解决这个特定问题。  编辑以获取进一步的深入信息： 我正在尝试将深度强化学习与图神经网络相结合以支持图环境。我使用 GNN 来估计 Dueling Double Deep Q-Network 架构中的 Q 值。我已经用 2 到 4 个 pytorch 几何 GNN（GCN、GAT 或 GPS）层替换了 MLP 层。 观察空间 为了测试这个架构，我使用了 frostylake-v1 环境的包装器，将观察空间转换为图形表示。每个节点都通过边连接到与其相邻的其他节点，代表一个就像正常人所看到的网格一样。 情况 1，具有位置编码： 每个节点具有 3 个特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 第二和第三个特征表示单元格的位置编码（单元格 x/y 坐标）： 第二个特征表示单元格列。 第三个特征表示单元格行。   情况 2，没有位置编码，使用单元格类型作为特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 单元格的类型。如果它是一个正常单元，则为 0；如果它是一个洞，则为 -1；如果它是目标，则为 1。  动作空间 动作空间与 openai gym freezelake 文档中的完全相同。代理对 frostinglake-1 环境有 4 种可能的操作（0=左、1=下、2=右、3=上）。 奖励空间 奖励空间与 openai gym frostinglake 文档中的完全相同。 问题 我已成功实现了具有所有默认单元的默认 4x4 网格环境的策略收敛。在我的实验中，代理只能在案例 1 中描述的观察空间中实现这种收敛。  我试图理解为什么需要位置编码才能实现收敛？ 在实施观察空间案例 2 时，即使在长时间训练的探索过程中多次获得最终奖励，代理也永远不会收敛。 由于与 transformer 相同的原因，GNN 是否也需要位置嵌入？ 如果我在小型网格环境中使用足够的消息传递 2 到 4 层，每个节点都应该具有来自图中每个其他节点的信息，那么网络是否应该能够在这种情况下隐式学习位置嵌入？ 我也尝试过使用其他位置嵌入 (PE) 方法，例如随机游走（5-40 次游走）和拉普拉斯向量（2-6 K 值），但我无法使用此 PE 实现收敛方法。 奇怪的是，我也尝试过使用随机化的唯一节点索引作为特征，而不是位置编码，并且代理能够收敛。我不明白为什么代理在这些条件下能够收敛，但在 PE 情况和观察空间情况 2 中却不能收敛。     提交人    /u/SmkWed   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtmo5l/pytorch_geometric_reinforcement_learning_and/</guid>
      <pubDate>Tue, 02 Jul 2024 14:05:23 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习教 AI 玩 BurgerTime - Python</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtlltk/teaching_ai_to_play_burgertime_with_reinforced/</link>
      <description><![CDATA[这是我在这个 subreddit 中的第一篇帖子，如果有更合适的帖子或需要更多信息，请告诉我。 Chilis 餐厅正在他们的网站上做促销，他们有一个浏览器游戏，基本上是 BurgerTime 的克隆，非常有趣 - chilisburgertime dot com 。我玩了一段时间，发现它非常可预测，有些级别甚至可能确定性地容易，我认为尝试制作一个 AI 来玩这个游戏会很酷。 我从事 IT 工作，我的编码有点生疏，但我一直在使用 ChatGPT 指导我用 Python 编写脚本。到目前为止，我已经能够编写一个脚本来监控分数、生命计数器，并在生命变为 0 时执行所有点击以开始新游戏和新试验。 然而，我的人工智能的训练/改进非常乏味。控制角色只能向左、向右、向上、向下移动和射击胡椒，每个屏幕上有 3 个敌人 + 一个老板，有 6 个级别，每个级别都有不同的老板。我很高兴它能通过一个级别。我试图强化它以最大化得分并最大化死亡前的生存，但它仍然像 5 岁的孩子一样玩。 我应该研究什么？我甚至不知道从哪里开始尝试识别屏幕上的所有精灵和游戏机制。如果我没有实际的游戏代码并且它在浏览器中播放，OpenAI Gym 会很好吗？如果没有数十小时的投入和大量的 AI 编码技能，这个游戏是否太复杂了？我怎样才能让它监控我的游戏玩法并从中获取有意义的信息？感谢您的时间！！    提交人    /u/Benhoffer87   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtlltk/teaching_ai_to_play_burgertime_with_reinforced/</guid>
      <pubDate>Tue, 02 Jul 2024 13:16:46 GMT</pubDate>
    </item>
    <item>
      <title>Minigrid Babyai 帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtfk71/minigrid_babyai_help/</link>
      <description><![CDATA[嗨， 我希望在 babyai 环境中有一个特定的结构，训练的每一集都以此结构开始。我真正想要的是为网格中的某些对象赋予特定位置。我如何在 minigrid 中实现这一点？    提交人    /u/cosmic_2000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtfk71/minigrid_babyai_help/</guid>
      <pubDate>Tue, 02 Jul 2024 06:59:33 GMT</pubDate>
    </item>
    <item>
      <title>“使用稀疏自动编码器解释偏好模型”，Riggs & Brinkmann</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dt9e2r/interpreting_preference_models_wsparse/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dt9e2r/interpreting_preference_models_wsparse/</guid>
      <pubDate>Tue, 02 Jul 2024 01:06:36 GMT</pubDate>
    </item>
    <item>
      <title>40k 个 episode 之后的奖励图下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dt0tg7/downside_of_reward_graph_after_40k_episodes/</link>
      <description><![CDATA[代码  train.py -&gt; https://www.pythonmorsels.com/p/33x5u/    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dt0tg7/downside_of_reward_graph_after_40k_episodes/</guid>
      <pubDate>Mon, 01 Jul 2024 18:52:21 GMT</pubDate>
    </item>
    <item>
      <title>研究顾问绝望</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dswpav/research_advisor_despair/</link>
      <description><![CDATA[嗨，我写这篇文章是为了寻求建议。我是一名本科生，梦想有一天能去研究生院学习强化学习。我已经和一位教授一起完成了一个使用应用机器学习的研究项目；从那时起，我一直想进入一个从事理论工作的实验室。问题是，即使发了几十封定制的冷邮件，我甚至还没有收到任何回复。 我知道这个领域非常受欢迎，竞争非常激烈，但我不知道该怎么做了。我知道对于北美顶尖的研究生课程来说，发表论文是必不可少的，但我不确定我是否有机会。在这一点上，我想我已经放弃了在没有导师的情况下发表论文的想法。这是一个好的做法吗？我应该继续尝试联系教授吗？任何见解都将不胜感激。    提交人    /u/Open-Ad2530   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dswpav/research_advisor_despair/</guid>
      <pubDate>Mon, 01 Jul 2024 16:04:22 GMT</pubDate>
    </item>
    <item>
      <title>从我的第一个大项目中学到的一些经验教训</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsswl0/some_lessons_from_getting_my_first_big_project/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsswl0/some_lessons_from_getting_my_first_big_project/</guid>
      <pubDate>Mon, 01 Jul 2024 13:25:05 GMT</pubDate>
    </item>
    <item>
      <title>只是想分享我的快乐，我的第一个主要 RL 项目不再变得糟糕。感谢我从这里得到的帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsnlqj/just_wanted_to_share_my_happiness_my_first_major/</link>
      <description><![CDATA[       由    /u/Breck_Emert  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsnlqj/just_wanted_to_share_my_happiness_my_first_major/</guid>
      <pubDate>Mon, 01 Jul 2024 08:05:41 GMT</pubDate>
    </item>
    </channel>
</rss>