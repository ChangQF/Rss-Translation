<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 16 Jun 2024 03:16:30 GMT</lastBuildDate>
    <item>
      <title>“选择的单位和级别”，SEP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgxtco/units_and_levels_of_selection_sep/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgxtco/units_and_levels_of_selection_sep/</guid>
      <pubDate>Sun, 16 Jun 2024 02:34:58 GMT</pubDate>
    </item>
    <item>
      <title>“人工智能搜索：更惨痛的教训”，麦克劳林（回顾 Leela Zero 与 Stockfish 的较量，以及在解决法学硕士问题时钟摆摆回搜索问题）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgxmnj/ai_search_the_bitterer_lesson_mclaughlin/</link>
      <description><![CDATA[       由    /u/gwern  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgxmnj/ai_search_the_bitterer_lesson_mclaughlin/</guid>
      <pubDate>Sun, 16 Jun 2024 02:23:47 GMT</pubDate>
    </item>
    <item>
      <title>“新兴世界表征：探索在综合任务上训练的序列模型”，li 等人，2022 年（Othello GPT 从动作中学习游戏的世界模型）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgw9ak/emergent_world_representations_exploring_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgw9ak/emergent_world_representations_exploring_a/</guid>
      <pubDate>Sun, 16 Jun 2024 01:05:18 GMT</pubDate>
    </item>
    <item>
      <title>“将价值迭代网络扩展至 5000 层以实现超长期规划”，Wang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dguw5x/scaling_value_iteration_networks_to_5000_layers/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dguw5x/scaling_value_iteration_networks_to_5000_layers/</guid>
      <pubDate>Sat, 15 Jun 2024 23:50:16 GMT</pubDate>
    </item>
    <item>
      <title>有哪些适合学习 RLHF 和 DPO 的玩具问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgowls/good_toy_problems_for_learning_about_rlhf_and_dpo/</link>
      <description><![CDATA[我想了解有关这些主题的更多信息，但它们通常应用于非常大的语言模型，我不想仅仅为了学习它们而使用这么大的模型和数据集。 我想知道是否有人知道某种与 cartpole 或其他东西相当的东西，但这更面向人类对序列模型的评分。    提交人    /u/radarsat1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgowls/good_toy_problems_for_learning_about_rlhf_and_dpo/</guid>
      <pubDate>Sat, 15 Jun 2024 18:56:48 GMT</pubDate>
    </item>
    <item>
      <title>“语言模型能否充当基于文本的世界模拟器？”，Wang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgoskw/can_language_models_serve_as_textbased_world/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgoskw/can_language_models_serve_as_textbased_world/</guid>
      <pubDate>Sat, 15 Jun 2024 18:51:25 GMT</pubDate>
    </item>
    <item>
      <title>训练机器人在 MJX 中表演足球技巧</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgonk9/train_a_robot_to_do_football_tricks_in_mjx/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgonk9/train_a_robot_to_do_football_tricks_in_mjx/</guid>
      <pubDate>Sat, 15 Jun 2024 18:44:47 GMT</pubDate>
    </item>
    <item>
      <title>“安全性协调不应只停留在几个代币层面”，Qi 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgkkr3/safety_alignment_should_be_made_more_than_just_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgkkr3/safety_alignment_should_be_made_more_than_just_a/</guid>
      <pubDate>Sat, 15 Jun 2024 15:33:03 GMT</pubDate>
    </item>
    <item>
      <title>人类生物的现实生活...足够安全吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgkafm/rl_for_humanoids_safe_enough/</link>
      <description><![CDATA[      看看这个视频哈哈 - 你无法像其他机器人那样仅使用紧急停止来处理故障。有希望用 RL 解决这个问题吗？ [来源：https://x.com/_wenlixiao/status/1801808951601705258?t=PyYeg362j-mzZkb73NkwKQ&amp;s=19 和 https://x.com/_wenlixiao/status/1801305252760850903?t=S2KzQzXigYI4zyOqaSydXA&amp;s=19 ]    提交人    /u/Boring_Focus_9710   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgkafm/rl_for_humanoids_safe_enough/</guid>
      <pubDate>Sat, 15 Jun 2024 15:19:43 GMT</pubDate>
    </item>
    <item>
      <title>关于 SB3/RLlib 的常见问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgjmfc/general_question_about_sb3rllib/</link>
      <description><![CDATA[我是 RL 新手，我想先使用 RLlib 或 SB3 来使用 PPO 训练我的代理。 这将是一个机器人项目，最终我想使用 Pytorch 和 c++ 在现实世界中运行推理，而不需要任何库（RLlib/SB3）。这能做到吗？ 此外，这些库如何进行自定义？我可以在 Pytorch 中定义自己的神经网络，然后告诉 RLlib/SB3 使用它吗？ 提前谢谢大家！    提交人    /u/FutureComedian7749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgjmfc/general_question_about_sb3rllib/</guid>
      <pubDate>Sat, 15 Jun 2024 14:49:12 GMT</pubDate>
    </item>
    <item>
      <title>即使参与者损失的负面影响不断增加，PPO 代理仍在学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgigr9/ppo_agent_is_learning_even_if_the_negative_of/</link>
      <description><![CDATA[      即使整体目标函数最初是最大化而不是最小化，我的 PPO 代理也会学习。参与者损失和总体目标函数首先增加，然后减少，最后趋于零。在整个过程中，它一直在学习它想要学习的东西。评论家损失和熵正在最小化（正如预期的那样）。原因可能是什么？附注：我知道参与者损失应该最大化，但我说的是参与者损失的负值，理想情况下应该使用 ADAM 优化器将其最小化，但事实并非如此。    提交人    /u/Low-Advertising-1892   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgigr9/ppo_agent_is_learning_even_if_the_negative_of/</guid>
      <pubDate>Sat, 15 Jun 2024 13:52:55 GMT</pubDate>
    </item>
    <item>
      <title>2024 年最好的强化学习算法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgdtp2/what_is_the_best_reinforcement_learning_algorithm/</link>
      <description><![CDATA[自 PPO 引入以来，似乎没有出现任何突破性的 RL 算法。    提交人    /u/galaxy_hu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgdtp2/what_is_the_best_reinforcement_learning_algorithm/</guid>
      <pubDate>Sat, 15 Jun 2024 09:02:26 GMT</pubDate>
    </item>
    <item>
      <title>我为 Blokus 棋盘游戏创建了一个 RL 环境！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dg3z08/i_made_an_rl_environment_for_the_board_game_of/</link>
      <description><![CDATA[我上周发现了 Blokus，非常喜欢它！所以我把它变成了一个 RL 环境。训练 RL 代理的工作正在进行中，所以请随意贡献！:)  查看/投一颗星：https://github.com/roger-creus/blokus-ai    由   提交 /u/xWh0am1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dg3z08/i_made_an_rl_environment_for_the_board_game_of/</guid>
      <pubDate>Fri, 14 Jun 2024 23:09:00 GMT</pubDate>
    </item>
    <item>
      <title>“蚂蚁”机器人有货吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dfzfqe/ant_robot_availability/</link>
      <description><![CDATA[你们中的一些人可能知道经典的 Ant 环境，它在 RL 中提供了一个相对容易学习的 3D 物理问题。有人知道“蚂蚁”机器人的实际物理实现是否存在吗？我试着四处寻找，但找不到任何东西（当然，搜索起来有点困难）。我愿意自己建造它，特别是如果有一种相对简单的方法来 3D 打印它的组件，但我真的希望能够执行迁移学习，将经过 MuJoCo 模拟训练的模型放在物理机器人中运行，这需要模型/机器人非常相似。 如果没有，有人推荐类似的机器人吗？我认为找到一个可用的机器人并基于它创建一个 MuJoCo 模型比反过来更容易，但理想情况下，已经存在为这种工作设置的东西。我特别想专注于这种运动，所以机器人手/手臂或车辆之类的东西对我来说不起作用。    提交人    /u/Nater5000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dfzfqe/ant_robot_availability/</guid>
      <pubDate>Fri, 14 Jun 2024 19:44:37 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 特工大师 Super Hang-On（世嘉 Genesis）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dfo1rz/dreamerv3_agent_masters_super_hangon_sega_genesis/</link>
      <description><![CDATA[我之前发布过我让模型玩过的最后一款游戏，大家似乎很喜欢它，所以我想发布这个。 我训练了一个 DreamerV3 模型，该模型能够完成世嘉 Genesis 的 Super Hang-On 的所有 4 个课程。 DreamerV3 在游戏的 64x64 像素 RGB 图像上进行训练，使用 SheepRL 的实现 ( https://github.com/Eclectic-Sheep/sheeprl )，使用 4 帧跳过且无帧堆叠。使用了 4 个并行的 gym 环境。 在 AI 评论部分，Gym 环境基本上会返回一些有关游戏状态的数据，然后我将这些数据形成文本提示，并将其输入到开源 LLM 中，以便它可以对游戏玩法做出一些简单的评论，然后将其转换为 TTS，同时让 Whisper 模型将我的 SpeechToText 转换为文本，这样我也可以与角色交谈（当我说出角色的名字时触发）。所有这些都连接到我制作的 UE5 应用程序中，该应用程序包含虚拟角色和环境。 这是我的视频链接：https://www.youtube.com/watch?v=IxrNMrVxxCs    提交人    /u/disastorm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dfo1rz/dreamerv3_agent_masters_super_hangon_sega_genesis/</guid>
      <pubDate>Fri, 14 Jun 2024 10:54:45 GMT</pubDate>
    </item>
    </channel>
</rss>