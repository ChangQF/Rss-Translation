<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 08 Mar 2025 06:19:59 GMT</lastBuildDate>
    <item>
      <title>狭窄分布的交叉问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j67jxk/crossq_on_narrow_distributions/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！我想知道是否有人有与Crossq进行狭窄分布的经验？即STD很小。我的CrossQ实现在摆上效果很好，但在我的自定义环境上效果不佳。它非常不稳定，返回平均线将大大下降，然后爬回去。但是，当我使用SAC在自定义环境上学习时，这并没有发生。我知道，这里可能会有多个级别的问题来源，但我只是对处理以下情况很好奇：STD非常小，并且随着代理商的了解，即使是少量的分配变化，也会导致巨大的价值变化，因为批次批量的“归一份”。运行的std很小 - ＆gt;非常稀有或新见的状态 - ＆gt; OOD，如果STD很小，则将新值标准化为巨大的值 - ＆GT;降低性能 - ＆gt;随着统计信息适应新值，性能再次成长 - ＆gt;重复重复或只是无法恢复。通常我的十字架确实恢复了，但这是次优的。  那么，有人知道如何处理这种情况吗？  另外，您如何监视batchnormalization的性病值？我不知道一个直截了当的方式，因为每个维度都会跟踪统计信息。也许是Max STD和Min STD？由于我的问题将出现在最小的STD小时。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/automatic-web8429     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j67jxk/crossq_on_narrow_distributions/</guid>
      <pubDate>Sat, 08 Mar 2025 02:55:37 GMT</pubDate>
    </item>
    <item>
      <title>我想创建一个AI代理，控制吸血鬼幸存者游戏中的角色</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j64o5r/i_want_to_create_an_ai_agent_to_control_the/</link>
      <description><![CDATA[＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/vampiresurvivors/comments/comments/1j5zdve/i_want_want_to_create_ai_ai_ai_aigent_to_control_to_control_the/”&gt; [links]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j64o5r/i_want_want_to_create_create_an_ai_ai_ai_ai_ai_aigent_to_control_to_control_to/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j64o5r/i_want_to_create_an_ai_agent_to_control_the/</guid>
      <pubDate>Sat, 08 Mar 2025 00:25:49 GMT</pubDate>
    </item>
    <item>
      <title>量化礁石框架的计算效率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5riux/quantifying_the_computational_efficiency_of_the/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/quantifying-the-computation--computation-oficy-of-the-reef-framework-dramework-0e2b30d79746”&gt; [link]        [注释]            ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5riux/quantifying_the_computational_efficiency_of_the/</guid>
      <pubDate>Fri, 07 Mar 2025 15:50:07 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助实施RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5o3uj/need_help_implementing_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为我的公司建立AI代理，从本质上讲，我们有一些客户使用仪表板来构建动态UI，供用户保留和转换其移动或Web应用程序。  我们想建立一个可以通过用户的行为为客户选择最佳的UI变体的AI代理。  我在基本层面上开始建立代理的方法应该是什么？ 技术堆栈应该是什么？  我应该知道的链接或资源是否可以帮助我建立代理？  谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/poperbudget348     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5o3uj/need_help_implementing_rl/</guid>
      <pubDate>Fri, 07 Mar 2025 13:44:42 GMT</pubDate>
    </item>
    <item>
      <title>是时候训练DQN为Ale Pong V5</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5icxh/time_to_train_dqn_for_ale_pong_v5/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我使用的是带有3个Conv层（32、64、64滤波器）和完全连接的层（512个单位）的CNN。我的设置包括RTX 4070 Ti Super，但每集需要6-7秒。这比我使用CPU的每集50秒要快得多，但是GPU的使用仅为20-30％，而CPU的使用量低于20％ 这是典型的性能，还是我可以优化的东西来加快速度？任何建议将不胜感激！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/soliseeker     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5icxh/time_to_train_dqn_for_ale_pong_v5/</guid>
      <pubDate>Fri, 07 Mar 2025 07:51:47 GMT</pubDate>
    </item>
    <item>
      <title>在线学习的逻辑帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j57qnn/logic_help_for_online_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在研究一个自动的高速缓存内存管理项目，我旨在创建一个自动化的策略，以便在发生缓存失误时提高性能。目的是根据设定级别和传入的填充详细信息选择一个缓存块进行驱逐。 对于我的模型，我已经实施了一种离线学习方法，该方法是使用专家策略培训的，并根据专家决策进行了立即的奖励。现在，我想使用在线增强学习来完善这种脱机培训的模型，在这种模型中，与基准相比，根据IPC的改进来计算奖励（例如，像MockingJay这样的最先进的策略）。 我已经为这种方法写了一种在线学习算法（我为此提供了这种方法），但是我会为您提供了努力，因为我可以从中努力进行编码。我的方法有意义吗？您会完善什么？ 以下情况您可能应该知道：  1）没有下一个状态（s&#39;）的建模，因此我不模拟向下一个状态过渡到下一个状态（s&#39;）（s&#39;），因为缓存驱逐是单步决策问题，因为驱逐的效果仅在下一步的情况下，而不是在执行中，因此我在执行情况下，我不得不将驱逐出境，因此，我是在执行的情况下，而我却不是在执行中，而我却不是在执行中，而我却不是在执行中，而我却不是在执行范围。仅在模拟结束时观察。  2）在线学习微调离线学习网络  离线学习阶段使用对专家决策的监督学习 在线学习阶段在线学习阶段初始化政策 在线学习阶段可以根据IPC的限制          3 Simulation which is slightly different than textbook examples of RL so,  The reward is based on IPC improvement compared to a baseline policy The same reward is assigned to all eviction actions taken during that simulation  4) The bellman equation is simplified so no traditional Q-Learning bootstrapping (Q(s&#39;)) because I dont have my next state建模。然后将方程变为q（s，a）←q（s，a）+α（r -q（s，a））（我认为） 您可以在此处找到我为此问题写的算法： https://drive.google.com/file/d/100imnq2eeu_huvvztk6youwkeni13kve/view?usp = sharing   很抱歉长篇文章，但我确实在此处确实可以在此处提供您的帮助和反馈：)   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]     32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j57qnn/logic_help_for_online_learning/</guid>
      <pubDate>Thu, 06 Mar 2025 22:14:44 GMT</pubDate>
    </item>
    <item>
      <title>哪种机器人模拟器更适合增强学习？ Mujoco，Sapien或Isaaclab？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4wa9g/which_robotics_simulator_is_better_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图选择最合适的模拟器来加强我的研究机器人操纵任务。 Based on my knowledge, MuJoCo, SAPIEN, and IsaacLab seem to be the most suitable options, but each has its own pros and cons:  MuJoCo:  pros: good API and documentation, accurate simulation, large user base large. cons:并行性不那么好（需要 jax 才能平行执行）。        sapien：  优点：良好的API，良好的API，良好的平行性。   并行性，丰富的特征，NVIDIA生态系统。  cons：资源密集型，学习曲线太陡峭，仍在进行重大更新，据报道容易出现。         &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/xyllong     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4wa9g/which_robotics_simulator_is_better_for/</guid>
      <pubDate>Thu, 06 Mar 2025 14:10:29 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用Reddit情感来构建股票预测AI  - 这就是发生的事情！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4se5b/tried_building_a_stock_prediction_ai_using_reddit/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/o1h8yk_m1ywxmiw8cslf3a9ukpx5nt7tzwt0h2dxdxd-4.jpg？宽度= 320＆amp; crop = smart＆amp; auto = webp＆amp; s = 1EB7518B4C497829A6E6E6E19370950F5D96EA6E3F57“ title =“尝试使用reddit情感建立股票预测AI  - 这就是发生的事情！” /&gt;   ＆＃32;提交由＆＃32; /u/u/u/indows-phase-9280     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4se5b/tried_building_a_stock_prediction_ai_using_reddit/</guid>
      <pubDate>Thu, 06 Mar 2025 10:17:01 GMT</pubDate>
    </item>
    <item>
      <title>加强 - 需要帮助改善奖励。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4pk9n/reinforce_need_help_in_improving_rewards/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  任何人都可以推荐我如何提高奖励。任何技术，YT视频甚至研究论文。一切都很好。我是一个学生刚开始RL课程，所以我真的不知道。env，奖励是离散的。请帮助😭🙏🙏🙏🙏🙏🙏  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/loud_lengthiss4987     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4pk9n/reinforce_need_help_in_improving_rewards/</guid>
      <pubDate>Thu, 06 Mar 2025 06:46:30 GMT</pubDate>
    </item>
    <item>
      <title>更新：礁石模型 - 一种用于AI连续性的生活系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4gm8m/updated_the_reef_model_a_living_system_for_ai/</link>
      <description><![CDATA[       &lt;！ -  sc_off- sc_off-&gt;  现在，所有的数学和代码在您的学习中，您的学习享受。     &lt;！ -  sc_on-&gt; 32;提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/the-reef-model-a-living-system-for-ai-continuity-0233c39c39c39c3f80”&gt; [link]    [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4gm8m/updated_the_reef_model_a_living_system_for_ai/</guid>
      <pubDate>Wed, 05 Mar 2025 22:52:44 GMT</pubDate>
    </item>
    <item>
      <title>分步教程：使用Llama 3.1（8b） + Google Colab + Grpo培训自己的推理模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4g234/stepbystep_tutorial_train_your_own_reasoning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4g234/stepbystep_tutorial_train_your_own_reasoning/</guid>
      <pubDate>Wed, 05 Mar 2025 22:29:52 GMT</pubDate>
    </item>
    <item>
      <title>桥AI框架v1.1- Noor礁的数学，代码和逻辑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4b4cx/the_bridge_ai_framework_v11_the_math_code_and/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/018wvvyzwksyluuviqcc9pwpzmqo5sq4lnfwx9veedt0.jpg？宽度= 640＆amp; crop = smart＆amp; auto = webp＆amp; s = 87D8DE13AF73711F516FDB479E0A7EEDD353B908“ title =“桥梁AI框架V1.1- Noor礁的数学，代码和逻辑”/&gt;      &lt;！ -  sc_off-&gt;   发布的文章解释了本文档中发现的数学和逻辑。提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/bridge-ai-framework-framework-framework-only-a5efcd9d01c7”&gt; [link]    32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4b4cx/the_bridge_ai_framework_v11_the_math_code_and/</guid>
      <pubDate>Wed, 05 Mar 2025 19:10:59 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    <item>
      <title>学习率计算</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j46zj1/learning_rate_calculation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我目前正在撰写医学硕士学位论文，在得分强化学习任务时需要帮助。从基本上来说，受试者执行了逆转学习任务，我想使用最简单的方法来计算平均学习率（我考虑只使用recrescorla-wagner公式，但是我找不到任何论文表明一个人会表明一个人会如何计算它）。 ）。 ）。 ，所以我要问我如何才能启动刺激刺激的刺激效果，并刺激刺激的刺激效果，或者是刺激的刺激效果？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1j46zj1/learning_rate_calculation/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j46zj1/learning_rate_calculation/</guid>
      <pubDate>Wed, 05 Mar 2025 16:26:02 GMT</pubDate>
    </item>
    <item>
      <title>帮助调试我的简单DQN AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j457st/help_debug_my_simple_dqn_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我制作了一个非常简单的游戏环境来使用pytorch训练DQN。该游戏在10x10网格上运行，AI的唯一目标是获得食物。&lt; /p&gt;  奖励系统：&lt; /strong&gt; 朝着食物迈进：-1 远离食物：-10 偏离界限：-100（game of the the the the ai nocy the Forky to the the In It It It It It Itike，yourty Itik to yourties the Inding Itike， （请参见下面的视频）。出于某种原因，它有时也会超出范围。 我已经尝试增加培训情节，但问题仍然发生。有什么想法会导致这件事？真的很感谢任何见解。谢谢。  源代码：  游戏环境  snake_game.py：   dqn class   utils.py： href =“ https://pastebin.com/raw/fepnsluv”&gt; https://pastebin.com/raw/fepnsluv  href =“ https://pastebin.com/raw/ndftrbjx”&gt; https://pastebin.com/raw/ndftrbjx  href =“ https://reddit.com/link/1j457st/video/9sm5x7clyvme1/player”&gt; https://reddit.com/link/link/1j457st/video/9sm5x7clyvmevme1/player  /u/u/unlikely_tax_4619       [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j457st/help_debug_my_simple_dqn_ai/</guid>
      <pubDate>Wed, 05 Mar 2025 15:10:14 GMT</pubDate>
    </item>
    </channel>
</rss>