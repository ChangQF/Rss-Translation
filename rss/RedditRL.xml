<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 10 Dec 2024 06:26:39 GMT</lastBuildDate>
    <item>
      <title>推荐与 Gymnasium 兼容的 Safe-RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hamadk/recommended_gymnasium_compatible_saferl/</link>
      <description><![CDATA[大家好， 希望这个问题适合这里。 我正在使用 CMDP（约束马尔可夫决策过程），并且正在寻找支持 CMDP 的环境，但主要是实现成本感知 RL 算法，如 PPO-Langrangian。 有人可以推荐一个具有此类实现的优秀 github repo 吗？ 或者也许是可以自己扩展的最佳 repo？    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hamadk/recommended_gymnasium_compatible_saferl/</guid>
      <pubDate>Mon, 09 Dec 2024 22:25:34 GMT</pubDate>
    </item>
    <item>
      <title>图像提示工程和高级提示技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hajlk9/image_prompt_engineering_and_advanced_prompt/</link>
      <description><![CDATA[根据 Image Arena 的众包偏好，Recraft v3 在文本转图像模型排行榜上位居 Flux1.1[pro]、Midjourney 和 SD3 之首。在新的视频教程中，我介绍了  - 提示的艺术样式 - 使用括号方法对提示进行加权以生成逼真的图像。 - 样式和定位控制等高级功能[实验性]。 - 使用 Recraft V3 Mockup 在生成的 AI 图像上放置图像。 此外，我还创建了一个详细的指南并在 GitHub 上分享了它。您可以在视频描述中找到存储库链接🎨  立即观看：https://www.youtube.com/watch?v=d3nUG28-jIc    提交人    /u/External_Ad_11   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hajlk9/image_prompt_engineering_and_advanced_prompt/</guid>
      <pubDate>Mon, 09 Dec 2024 20:32:31 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity AI Pro 1 年优惠券 - 仅需 25 美元（23 欧元）| 订阅后付款！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hajguy/perplexity_ai_pro_1year_coupon_only_25_23/</link>
      <description><![CDATA[以 25 美元（原价 200 美元）的价格获得 1 年 Perplexity Pro 代码 这包括访问以下模型： » GPT-4o、o1 Mini 用于推理和Llama 3.1 » Claude 3.5 Sonnet、Claude 3.5 Haiku、Grok-2 » 图像生成器：Flux.1、DALL-E 3、Playground v3 Stable Diffusion XL 只要您没有有效的 Pro 订阅，就可以在全球范围内使用。 工作原理：  加入 🔗Discord 330+ 成员 通过 PayPal 付款以获得买家保障。 我会向您发送兑换链接。  买家担保，反馈 2、反馈 3、反馈 4、反馈 5    提交人    /u/AICentralZA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hajguy/perplexity_ai_pro_1year_coupon_only_25_23/</guid>
      <pubDate>Mon, 09 Dec 2024 20:27:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 的自定义环境的奖励没有改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1haezxl/reward_not_improving_for_a_custom_environment/</link>
      <description><![CDATA[      我一直在尝试在我使用 gym 实现的自定义环境中训练代理，其目标是通过调整每个节点的有功功率（负载）来解决电网中的电压违规问题。我主要尝试了两种算法，通过 stable-baselines3、PPO 和 DDPG。但是，这两种方法都给我带来了一些非常糟糕的结果（比如奖励随着时间的推移而减少），我希望有人能帮助我找到更好的方向。 因此，代理进行包含每个节点的电压值和一些其他连续值的观察，然后执行能够改变电网每个节点负载的操作（因此是一个具有 24 个连续值的数组），然后执行功率流以确定新的电压值，然后根据这些新的电压值计算奖励。 我希望我的代理尽可能少地采取行动，并在最短的时间步内解决违规问题。因此，我按以下方式构建了奖励函数：  如果没有违规，我会给它 10 的奖励，并且情节终止 在每个步骤中，如果有违规，我会施加一个基本惩罚，并且我会添加与调整幅度成比例的额外惩罚 如果代理所做的调整过于极端，以至于我的功率流算法无法收敛并停止工作，我会施加 -10 的惩罚并结束情节。  情节设置：情节从包含违规的初始观察开始。当一个情节结束时，下一个情节将从另一个具有电压的观察开始（其中一些具有违规行为）。 我的 PPO 模型具有以下参数： model = PPO(&quot;MlpPolicy&quot;, env, verbose=1, n_steps=256, tensorboard_log=&quot;C:\\Users\\antonio\\Downloads\\RL&quot;, ent_coef=0.01, gamma=0.9) 我选择了较低的伽马，因为代理需要优先快速解决违规行为。 以下是 10k 步 PPO 尝试的指标： https://preview.redd.it/hqxnmef5wu5e1.png?width=1518&amp;format=png&amp;auto=webp&amp;s=bf4cfb3757835ae7729133f646648680a37d4cd7 就是这样，抱歉发了这么长的帖子。无论如何，您有什么建议可以给我吗？    提交者    /u/Sasukespc   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1haezxl/reward_not_improving_for_a_custom_environment/</guid>
      <pubDate>Mon, 09 Dec 2024 17:24:40 GMT</pubDate>
    </item>
    <item>
      <title>训练多智能体（人类代理/地面实况）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ha4mj2/training_multiagents_human_proxyground_truth/</link>
      <description><![CDATA[大家好， 我对 RL 非常陌生，我正在阅读这篇论文 https://arxiv.org/abs/1910.05789，该论文介绍了使用人类数据训练代理如何优于其他方法（即自我游戏）。过度训练的环境适用于协作多智能体（2 个）。  我想知道他们是如何训练 Human_proxy（基本事实）的，它用于与不同的自我智能体配对，以比较它们可以获得的分数。我读了这篇论文，但我仍然不明白这个概念。他们是否使用分离的轨迹和 BC 算法训练 H_proxy 独自游戏，还是 H_proxy 也与其他人一起训练。  就像我说的，我对此非常陌生，所以我的解释可能没有意义。。 谢谢！感谢您对理解这一点的任何帮助。     提交人    /u/Complete-Internet509   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ha4mj2/training_multiagents_human_proxyground_truth/</guid>
      <pubDate>Mon, 09 Dec 2024 07:39:29 GMT</pubDate>
    </item>
    <item>
      <title>您对“雄心勃勃”的 RL 项目有何想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h9g88l/your_ideas_on_ambitious_rl_projects/</link>
      <description><![CDATA[在阅读此帖子的评论后，我很好奇什么会被认为与 ChatGPT/LLM/Transformer 一样雄心勃勃，但对于 RL 来说呢？  我猜这将需要像上面那样的 3 个组件： 1. 一项科学突破：Transformer 2. 一个大规模版本：LLM 3. 一个应用程序：ChatGPT 重新表述我的问题，对于上面的每个组件，您认为它们会是什么？ 我基于纯粹想象的镜头： 1. 通用 RL + 推理 + 低功耗 2. Isaac Lab 3. 机器人足够强大，以便人们开始部署这些机器人而不是人类。    提交人    /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h9g88l/your_ideas_on_ambitious_rl_projects/</guid>
      <pubDate>Sun, 08 Dec 2024 10:39:15 GMT</pubDate>
    </item>
    <item>
      <title>如果没有开放的“OpenAI”，强化学习还能真正进步吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h9d14c/will_rl_truly_advance_without_the_open_openai/</link>
      <description><![CDATA[OpenAI 在实际的非盈利时代推动了现实生活中的实用 RL，到目前为止，策略 RL 上的无模型说实话是最实用的...因为世界具有非平稳特性，通常需要课程并且本质上是多智能体...所有这些结合在一起并不容易适合离策略 RL。 可悲的是，OpenAI 主导了 trpo/ppo 领域，而现在，人工智能货币化的竞赛才是真正的目标...你对 RL 的未来有什么期待，因为我怀疑一些进步是隐藏的并且不公开可用     提交人    /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h9d14c/will_rl_truly_advance_without_the_open_openai/</guid>
      <pubDate>Sun, 08 Dec 2024 06:47:26 GMT</pubDate>
    </item>
    <item>
      <title>将图搜索问题构建为 RL 问题是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h96fkm/does_it_make_sense_to_frame_a_graph_search/</link>
      <description><![CDATA[我是一名计算机科学研究员，最近开始探索如何使用强化学习来学习图上 NP 难题的启发式方法。 我正在研究的问题是这样的：从初始图 G0 开始，您可以在两个操作（操作 1 或操作 2）之间进行选择，这两个操作分别产生直接成本r1(G0) 和 r2(G0)（均 &gt; 0）。每个操作都会将图修改为较小的图，即 A1(G0) 或 A2(G0)，节点较少。该过程持续到图为空，总成本是沿途产生的所有成本的总和。目标是选择操作以最小化成本总和。  我想将其定义为一个 RL 问题，其目标是学习一个策略 π(G)，该策略可在任意输入图 G 的两个动作之间进行选择。例如，π(G)∈(0,1) 可以表示选择动作 1 的概率，π(G) 可以使用图神经网络进行建模。 这个公式作为 RL 问题有意义吗？此外，您是否建议使用类似 AlphaGo 风格的方法来训练它，或者像深度 Q 学习这样的原始方法就足够了？    提交人    /u/Local-Assignment-657   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h96fkm/does_it_make_sense_to_frame_a_graph_search/</guid>
      <pubDate>Sun, 08 Dec 2024 00:28:02 GMT</pubDate>
    </item>
    <item>
      <title>简单、超级干净的 DreamerV3，但效果不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8xu0k/simple_super_clean_dreamerv3_that_doesnt_quite/</link>
      <description><![CDATA[      我正在寻找社区的一点帮助和智慧，以获得有关我的业余 DreamerV3 pytorch 实现的一些评论和反馈。我需要一些指导，以了解可能出现的问题，因为在我的 rtx 3060 笔记本电脑上训练需要很长时间，因此无法测试所有小变化。而且我也没有主意了。   过去几个月，我一直在实现自己的 DreamerV3 算法，该算法写得非常清楚，我可以轻松地将它教给其他人，并直观地了解它是如何工作的以及为什么有效。 我已经实现了大部分技巧，但简化了一些部分，例如，我没有包括连续预测，假设我的任务永远持续下去（我想在解决更多问题之前击败连续的 CarRacing 健身房环境），并且我没有 twohot 损失，只有 mse 损失。但是，这些剩余的技巧应该取决于它的效果如何，而不是它是否有效。 这就是我的问题，尽管我研究了其他实现，多次阅读了论文，确保训练循环符合预期，但它仍然不起作用。 如果任何知道 DreamerV3 工作原理的人能看看我写的内容，如果一切正常，至少扫描一下，我将不胜感激。世界模型训练中的梯度有些不对劲，所以我会先在那里查找问题。 除此之外，也许还有一些简单的参数调整可以立即提高性能？也许我应该重写一些东西，也许 twohot 损失是必要的？ 我正在寻找任何更有经验的人可以赐予我的智慧，因为我自己学习一切。 我已经盯着它看了很久，我可能会错过显而易见的东西，所以欢迎每一位新人。我也不掩饰我在这方面不太有经验的事实，所以我只是在尝试一些东西。 现在，我已经在 71k 梯度步骤上测试了我的最新代码（我晚上每 10 小时只能训练大约 30k 步）。我可以走得更远，但它似乎根本没有改善（也没有以任何方式改变）。它看起来只是收敛得不好，所以我确定梯度不对。代理在未经训练的权重上获得大约 7 个总奖励，目前在 70k 步之后，它平均获得 -9，向右转动并无休止地旋转。如果它什么都不做，只是呆在原地，-19 奖励就像是绝对的底部。 到目前为止，我对强化学习的了解，或多或少只需要一个位置合适的 .detach() 就可以让它工作。 我将在 publicResults 文件夹中发布一些训练结果。 提前致谢。 Repo：https://github.com/InexperiencedMe/dreamer   最新运行在 html 图上的 publicResults 文件夹中，名称为 AAAAAA_FULLSTATE_DETACHED，如下所示。有趣的是，我过去已知的糟糕训练循环，即在单个非批处理序列上学习的循环，在运行之间表现更好，但存在一些差异。我最新的代码刚刚收敛到全速前进，全速右转，没有刹车，无休止旋转。 https://preview.redd.it/eeqq7z3frg5e1.png?width=1701&amp;format=png&amp;auto=webp&amp;s=a9799787cc17b4c2e5ca87a33d17e91fa365fa89      提交人    /u/Inexperienced-Me   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8xu0k/simple_super_clean_dreamerv3_that_doesnt_quite/</guid>
      <pubDate>Sat, 07 Dec 2024 17:46:35 GMT</pubDate>
    </item>
    <item>
      <title>设计奖励结构以控制自动驾驶汽车，实现交通平稳化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8xqa7/reward_structure_design_to_control_an_autonomous/</link>
      <description><![CDATA[大家好，我需要你们的帮助。 我正在完成一项关于使用 Q 学习 进行最优控制的大学作业。 我的环境如下：我有一个由人类驾驶的车辆组成的交通流，其中只有 一辆自动驾驶汽车 (ADV)。我的目标是训练代理（ADV 的输入）与领头车辆（以我选择的间隔刹车）保持 最佳距离，从而保持 最佳速度，以便为跟随它的车辆平滑交通波。 我想在代理与领头车辆保持的距离大于最佳距离的情况下以及代理保持接近最佳速度的情况下“奖励”代理，因为这样就不会产生走走停停的交通波。推测一下，当相反的情况发生时，我会“惩罚”他。 问题是代理似乎没有学习。 我对以下内容存有疑虑： - 奖励或阻止他的条件； - 奖励和惩罚的标志和价值。 实际上我的奖励结构是： IsDone = max(abs(x_next) &gt; StateThreshold) || (distance_1 &lt; eta) || (distance_2 &lt; eta); distance_penalty = 100 * double(((x1-x2)-s_star)&lt;0); velocity_deviation_penalty = (v2 - v_star)^2 * double((v2 - v_star)&gt;0); 成本 = distance_penalty + velocity_deviation_penalty; distance_reward= - 100 * double(((x1-x2)-s_star)&gt;0); velocity_target_reward = - (v2 - v_star)^2 * double((v2 - v_star)&lt;(-1) &amp;&amp; (v2-v_star)&gt;1); 如果IsDone == 1 Reward = -1000; else Reward = -Cost + velocity_target_reward + distance_reward; end 有人能帮我吗？    提交人    /u/SpiritedPotato2844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8xqa7/reward_structure_design_to_control_an_autonomous/</guid>
      <pubDate>Sat, 07 Dec 2024 17:41:38 GMT</pubDate>
    </item>
    <item>
      <title>现实生活中的 MDP 与 POMDP。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8rnx0/mdps_vs_pomdps_in_real_life_settings/</link>
      <description><![CDATA[大家好， 我只是想听听你对 MDP 与 POMDP 的看法。 当我有一个在现实世界中运行的机器人代理时，将其建模为 MDP 而不是 POMDP 是否有意义？ 我觉得在世界上每个机器人在现实世界中运行的场景中，将其建模为 POMDP 而不是 MDP 更有意义。 你们觉得呢？    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8rnx0/mdps_vs_pomdps_in_real_life_settings/</guid>
      <pubDate>Sat, 07 Dec 2024 12:41:16 GMT</pubDate>
    </item>
    <item>
      <title>PyBullet 问题 - 搭建简单的积木塔（Jenga）时，它总是会倒塌</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8mlc8/pybullet_problem_when_building_a_simple_tower_of/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8mlc8/pybullet_problem_when_building_a_simple_tower_of/</guid>
      <pubDate>Sat, 07 Dec 2024 06:35:13 GMT</pubDate>
    </item>
    <item>
      <title>欧洲多智能体强化学习 (MARL) 研究小组</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8mhvx/multiagent_reinforcement_learning_marl_research/</link>
      <description><![CDATA[大家好！我目前正在攻读机械工程硕士学位，我的大部分研究都集中在多智能体强化学习 (MARL) 和智能交通系统上。我现在正计划申请该领域的博士学位，特别是在欧洲。 我对涉及将 MARL 应用于机器人、无人机或动态环境中的智能决策的研究非常感兴趣。您能推荐欧洲活跃于该领域的任何研究小组或实验室吗？    提交人    /u/Glittering_Rip_6841   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8mhvx/multiagent_reinforcement_learning_marl_research/</guid>
      <pubDate>Sat, 07 Dec 2024 06:28:34 GMT</pubDate>
    </item>
    <item>
      <title>我是不是别无选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h87hx1/am_i_left_with_no_other_option/</link>
      <description><![CDATA[ROS 太烂了 我是一名计算机科学专业的学生，​​之前做过一些 ROS，我发现为我的 RL 项目设置（Gazebo）非常烦人。我没有 FLOSS 选项了吗？ 有没有办法可以在没有 ROS 的情况下模拟环境？    提交人    /u/iconic_sentine_001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h87hx1/am_i_left_with_no_other_option/</guid>
      <pubDate>Fri, 06 Dec 2024 18:02:21 GMT</pubDate>
    </item>
    <item>
      <title>“奖励基础：一种自适应获取多种奖励类型的简单机制”，Millidge 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h82gh2/reward_bases_a_simple_mechanism_for_adaptive/</link>
      <description><![CDATA[   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h82gh2/reward_bases_a_simple_mechanism_for_adaptive/</guid>
      <pubDate>Fri, 06 Dec 2024 14:23:34 GMT</pubDate>
    </item>
    </channel>
</rss>