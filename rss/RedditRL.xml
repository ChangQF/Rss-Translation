<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 23 Nov 2024 01:16:39 GMT</lastBuildDate>
    <item>
      <title>“对未知情况和环境的元认知 (MUSE)”，Valiente & Pilly 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxno5j/metacognition_for_unknown_situations_and/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxno5j/metacognition_for_unknown_situations_and/</guid>
      <pubDate>Sat, 23 Nov 2024 01:07:37 GMT</pubDate>
    </item>
    <item>
      <title>“Marco-o1：面向开放式解决方案的开放推理模型”，赵等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxngh4/marcoo1_towards_open_reasoning_models_for/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxngh4/marcoo1_towards_open_reasoning_models_for/</guid>
      <pubDate>Sat, 23 Nov 2024 00:57:31 GMT</pubDate>
    </item>
    <item>
      <title>关于 Wordle 性能不佳的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxdv1y/advice_regarding_poor_performance_on_wordle/</link>
      <description><![CDATA[大家好， 我正在寻找有关如何处理这个强化学习问题的建议。我正在尝试教一个编码器转换器模型玩 wordle。它是基于字符的，所以有 26 个标记 + 5 个特殊标记。输入是棋盘空间，因此它可以访问以前的猜测和反馈，以及显示猜测开始/结束位置等的特殊标记。 我目前使用的算法是 PPO，我已经将游戏简化为只需要猜测一个单词的极其简单的场景，我预计这会非常容易（但是由于我的 RL 知识有限，显然我搞砸了）。 我正在寻找有关在哪里寻找此问题根源的建议。该模型确实“最终”赢了一两次，但它似乎并没有停留在那里。此外，它似乎只能持续猜测两个或三个字母。 示例。目标单词是 Amble 该模型可以一致地猜测“aabak”，围绕 an 和 b 的逻辑结果是合理的，因为奖励结构会支持该猜测。我不知道为什么 k 会得到强化，或者为什么其他字母不那么普遍。 此外，我尝试了教师强制，即强制模型做出正确的猜测并获胜，但无济于事。有什么建议吗？ 编辑：此外，该游戏是“可赢的”，我创建了伪游戏并在这些游戏上训练了模型。不是真正的离线 RL，因为我使用了 CE 损失。但是，在模型已经训练过的单词上，它的表现足够好，即使是它没有见过的单词，它的表现也不错，足以展示对模式的“理解”。    提交人    /u/ur_a_glizzy_gobbler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxdv1y/advice_regarding_poor_performance_on_wordle/</guid>
      <pubDate>Fri, 22 Nov 2024 17:54:05 GMT</pubDate>
    </item>
    <item>
      <title>关于简单单过程囊实验的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxb5rw/question_regarding_simple_single_process_sac/</link>
      <description><![CDATA[即使我设置了正确的超参数并按照论文中的说法制定公式，仍然不足以相信代理会实现目标吗？ 奖励缩放是否必要？例如，对于半cheeath？    提交人    /u/Round_Apple2573   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxb5rw/question_regarding_simple_single_process_sac/</guid>
      <pubDate>Fri, 22 Nov 2024 16:01:30 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML-Agents 代理变得越来越笨，我的想法已经用完了。我需要帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</guid>
      <pubDate>Fri, 22 Nov 2024 10:42:24 GMT</pubDate>
    </item>
    <item>
      <title>灾害管理中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</link>
      <description><![CDATA[最近，我深入研究了 RL 的灾害管理，并阅读了几篇相关论文。许多论文都提到了与之相关的算法，但没有以某种方式对其进行模拟。是否有任何平台具有与 RL 相关的模拟来展示其应用？此外，如果您有关于此方面的任何其他优秀论文的信息，请提及。     提交人    /u/SuitSecret6497   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</guid>
      <pubDate>Fri, 22 Nov 2024 07:40:51 GMT</pubDate>
    </item>
    <item>
      <title>策略梯度公式检查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwujcm/policy_gradient_formulas_check/</link>
      <description><![CDATA[      你好， 我正在写关于 RL 中的策略梯度方法，我对这些方程式有疑问。我理解目标是最大化目标函数 J(θ) 的值，即给定策略 (πθ) 的轨迹 (τ) 的总回报。这给了我们表达式 J(θ) = E τ∼πθ [R(τ)]。 从那里并使用梯度，我们可以推断出表达式 ∇θ J(θ) = E τ∼πθ [∑t ∇θ log πθ(at|st) R(τ)]。 我的问题是这些算法的以下目标函数是否正确（第一个是 REINFORCE）： https://preview.redd.it/prqve7kfhc2e1.png?width=754&amp;format=png&amp;auto=webp&amp;s=76e48fb82eb0ac1710352705b1c84c3869453d39 如果您能提供任何关于改进或以其他方式表达这些功能的建议，我将不胜感激。    提交人    /u/Street-Vegetable-117   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwujcm/policy_gradient_formulas_check/</guid>
      <pubDate>Fri, 22 Nov 2024 00:11:18 GMT</pubDate>
    </item>
    <item>
      <title>适用于 ML/AI/RL 的 Blue Sky 研究员入门包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</link>
      <description><![CDATA[大家好，许多研究人员正在加入 Blue Sky，而且似乎进展顺利，所以我想我会留下一些研究人员“入门包”供大家关注。欢迎随意发布您自己的内容 :)  RL：https://bsky.app/3WPHcHg AI：https://bsky.app/SipA7it NLP：https://bsky.app/SngwGeS ML 理论：https://bsky.app/21nFz12 AI 机器人：https://bsky.app/DfAoaJ1 贝叶斯 ML：https://bsky.app/2Bqtn6T 人工智能中的女性：https://bsky.app/LaGDpqg 人工智能和新闻：https://bsky.app/5sFqVNS 科学人工智能：https://bsky.app/JeFdryY  医学人工智能：https://bsky.app/N5UUARF     提交人    /u/secondterm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</guid>
      <pubDate>Thu, 21 Nov 2024 21:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何开始机器人操作器的强化学习研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</link>
      <description><![CDATA[你好， 我是一名研究生，对应用人工智能技术（特别是强化学习）来控制机器人操纵器（机械臂）感兴趣。 为了做到这一点，我不知道从哪里开始学习并决定研究主题。  有哪些基础论文和资源可以帮助我了解这个领域？ 有哪些最近的评论或调查论文可以帮助我了解该领域的现状？ 或者，为了研究人工智能机器人技术，我应该阅读哪些论文？   如有任何建议或意见，我们将不胜感激！ 谢谢！ 使用 DeepL.com （免费版） 进行翻译   提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 21 Nov 2024 18:59:33 GMT</pubDate>
    </item>
    <item>
      <title>帮我用 Unity3D 制作这辆 DDPG 自动驾驶汽车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</link>
      <description><![CDATA[我被这个项目困住了，我不知道我哪里做错了，可能是在脚本中，也可能是在 Unity 中。请帮助我解决和调试问题。请直接给我发私信，获取脚本和更多信息。    提交人    /u/pendalkumar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</guid>
      <pubDate>Thu, 21 Nov 2024 13:41:59 GMT</pubDate>
    </item>
    <item>
      <title>另一个调试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</link>
      <description><![CDATA[大家好， 我正在解决声音领域中具有连续动作的问题。 该模型是一个表示声音的 CNN。表示与一些参数一起被输入到 MLP 以获得值和动作。 在研究损失函数（在我们的例子中是奖励）之后，它是参数和动作的凸函数。我的意思是，对于给定的参数 + 声音，作为动作函数的奖励信号是凸的。 不幸的是，我们偶然发现了一个可以实现收敛的网络参数的良好初始化。问题是模型几乎一直都不会收敛。 如何调试问题的根源？我是否只需要等待足够长的时间？我是否扩大了模型？ 谢谢 编辑：我意识到我没有指定我正在使用的算法。PPO，A2C，Reinforce，OptionCritic，PPOC。 所有这些算法的作用本质上都是相同的。    提交人    /u/sagivborn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</guid>
      <pubDate>Thu, 21 Nov 2024 09:43:42 GMT</pubDate>
    </item>
    <item>
      <title>你如何训练 Agent 进行国际象棋之类的游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gw7itl/how_do_you_train_agent_for_something_like_chess/</link>
      <description><![CDATA[到目前为止我还没有做过任何 RL，我想开始使用 RL 研究类似国际象棋模型的东西，但不知道从哪里开始    提交人    /u/Ok_Orchid_7408   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gw7itl/how_do_you_train_agent_for_something_like_chess/</guid>
      <pubDate>Thu, 21 Nov 2024 04:07:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在深度强化学习中处理多通道输入</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gw1vit/how_to_handle_multi_channel_input_in_deep/</link>
      <description><![CDATA[大家好。我正在尝试制作一个代理，它将学习如何使用深度强化学习下棋。我使用的是 pettingzoo 的 chess_v6 环境 (https://pettingzoo.farama.org/environments/classic/chess/)，它使用棋盘的观察空间，其形状为 (8,8,111)。我的问题是如何将此观察空间输入到深度学习模型中，因为它是一个多通道输入，以及哪种架构最适合我的 DL 模型。请随时分享您可能拥有的任何提示或我可以阅读的有关该主题或我使用的环境的任何资源。    提交人    /u/Livid-Ant3549   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gw1vit/how_to_handle_multi_channel_input_in_deep/</guid>
      <pubDate>Wed, 20 Nov 2024 23:35:37 GMT</pubDate>
    </item>
    <item>
      <title>“物理智能：将人工智能带入物理世界的十亿美元初创公司内部情况”（pi）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvzupn/physical_intelligence_inside_the_billiondollar/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvzupn/physical_intelligence_inside_the_billiondollar/</guid>
      <pubDate>Wed, 20 Nov 2024 21:20:01 GMT</pubDate>
    </item>
    <item>
      <title>RLtools：最快的深度强化学习库（C++；仅标头；无依赖项）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</link>
      <description><![CDATA[        提交人    /u/jonas-eschmann   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</guid>
      <pubDate>Wed, 20 Nov 2024 17:00:36 GMT</pubDate>
    </item>
    </channel>
</rss>