<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 11 Jun 2024 18:20:16 GMT</lastBuildDate>
    <item>
      <title>NVidia Omniverse 接管了我的计算机</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ddkw1g/nvidia_omniverse_took_over_my_computer/</link>
      <description><![CDATA[      https://preview.redd.it/6ecu4wdfgz5d1.png?width=967&amp;format=png&amp;auto=webp&amp;s=d56ddf933839a6a3f938d83b72aaed1b5fb9372d 我只是想使用 Nvidia ISAAC sim 来测试一些强化学习。但它安装了整个套件。在我设法删除一些之前，还有更多的流程和服务。我需要所有这些吗？我只想能够编写一些脚本来学习和回放 Python。这可能吗，还是我需要所有这些服务才能使其运行？  这比使用带有 MLAgents 的 Unity 更好吗，它看起来几乎是同一件事。     提交人    /u/No_Way_352   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ddkw1g/nvidia_omniverse_took_over_my_computer/</guid>
      <pubDate>Tue, 11 Jun 2024 18:12:02 GMT</pubDate>
    </item>
    <item>
      <title>基于模型与无模型的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcqdu5/distinction_of_modelbased_vs_modelfree/</link>
      <description><![CDATA[无论我看到多少教科书定义，某些机器学习定义都毫无意义。 基于模型与无模型，假设我们采用系统识别视角。在这种情况下，我们已经定义了动态结构，并希望找到模拟参数，使事情像在现实生活中一样展开。希望以迭代数据驱动的方式执行此操作可以节省大量劳动。但是，我们通过结构对模型施加的偏差应该同时帮助对动态进行建模，同时保持足够的灵活性以进行调整。这似乎从根本上存在缺陷，无法将其与无模型方法区分开来。无模型我们不强加任何动态结构。我认为，我们不会限制动态的展开。在这两种情况下，我们都在从根本上用数据或经验调整参数。（尽管参数少得多）。最终，问题取决于由（初始）偏见选择的未来经验，而这种偏见推动了“寻找参数”的目标。偏见要么来自以前的经验，要么由模型引起，等等。对我来说，这归结为探索与利用的问题，而不是方法问题。 正确的思考方式是什么？    提交人    /u/FriendlyStandard5985   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcqdu5/distinction_of_modelbased_vs_modelfree/</guid>
      <pubDate>Mon, 10 Jun 2024 16:55:43 GMT</pubDate>
    </item>
    <item>
      <title>独家专访“Unitree G1 - 人形代理 AI 化身”软机器人播客</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcni17/exclusive_interview_unitree_g1_humanoid_agent_ai/</link>
      <description><![CDATA[        提交人    /u/meldiwin   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcni17/exclusive_interview_unitree_g1_humanoid_agent_ai/</guid>
      <pubDate>Mon, 10 Jun 2024 14:56:27 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家算法代码帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dckr5g/actor_critic_algorithm_code_help/</link>
      <description><![CDATA[对于连续的动作和状态，A2C 算法代码帮助的优势 动作没有改变，它给出恒定的单一动作    提交人    /u/Past-News-1373   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dckr5g/actor_critic_algorithm_code_help/</guid>
      <pubDate>Mon, 10 Jun 2024 12:52:10 GMT</pubDate>
    </item>
    <item>
      <title>🐙 Octo：开源通才机器人政策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcit8c/octo_an_opensource_generalist_robot_policy/</link>
      <description><![CDATA[  由    /u/HimitsuNoShougakusei  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcit8c/octo_an_opensource_generalist_robot_policy/</guid>
      <pubDate>Mon, 10 Jun 2024 11:02:59 GMT</pubDate>
    </item>
    <item>
      <title>模拟退火与强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</link>
      <description><![CDATA[当考虑启发式竞争编程任务时会出现这个问题。让我们考虑一个非常基本的例子，旅行商问题（或者最近的这个竞赛，很多人都在讨论 RL 的可能性，但大多数人都不是专家（包括我自己，最终也使用了模拟退火，但事后却很痛苦，因为我本来想做点不同的事情））。 几乎所有这些比赛都是使用模拟退火或其他变体赢得的。对于不熟悉的人来说，所有这些变体都是从某个解决方案开始，然后通过某种变异过程迭代改进它，以摆脱局部最小值。对于旅行商问题，您可以提出一个初始的随机城市列表，然后随机交换一些城市，直到它改进了您的解决方案，然后将这个新解决方案作为最佳解决方案，依此类推。再加上一些突变以逃避局部最小值（例如，意味着对列表的一小部分进行改组 - 我显然是在简化）。 什么会阻止人们在这些问题上使用强化学习（实际上没有人，这篇文章中已经针对旅行商问题完成了此操作：https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/tje2.12303 - 如果我没看错的话，作者甚至提到了模拟退火，但没有将结果与它进行比较）。奖励函数通常不难想出（我在竞赛中提到的奖励函数甚至比 TSP 更容易，因为每次“怪物”死亡后你都会获得“金币”，你会尝试最大化它（累计金额））。 我对不使用强化学习的假设是：  尽管强化学习的样本效率更高，但这些问题实际上很容易模拟，因此更新神经网络或任何函数近似器的开销都太高。只有当运行一集的成本非常高时，强化学习才会有趣。否则，用 C 编写简单的遗传算法总是比用 Python 编写的 RL 更有效（时间方面）。 无需概括，这些比赛的测试用例已经给出，你只需要想出最佳的行动序列来影响环境（例如，在我的第二个例子中要杀死哪些怪物）并在这些测试用例中获得最高奖励。如果比赛内容相同，但他们在比赛结束前三十分钟公布测试用例，那么在 8000 个线程上运行模拟退火三十分钟的效率不如使用预先训练好的代理，该代理在 GPU 上经过几天的大量不同虚构测试用例的训练。 RL 在多代理设置（零和游戏等）中真正显示了其主导地位，其中模拟退火和变体不易实现（尽管 MARL 优化的每一步都在尝试利用当前最佳策略组合，这可以通过遗传算法来完成 - 但我认为这被称为 RL，它只是没有梯度的 RL）。 但同时，RL 比其他技术更复杂，所以也许人们只是因为没有专业知识而不去那里，而 RL 专家实际上会在其中一些比赛中表现出色？  我遗漏了什么吗？您们这些 RL 专家怎么看？Rich 会怎么说。萨顿说了什么？    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcin6z/simulated_annealing_vs_reinforcement_learning/</guid>
      <pubDate>Mon, 10 Jun 2024 10:52:31 GMT</pubDate>
    </item>
    <item>
      <title>关于PPO中小批量的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dcfqfd/question_about_minibatch_in_ppo/</link>
      <description><![CDATA[你好，我是强化学习的新手，对训练 PPO 时设置小批量有疑问。 在基于 DQN 的算法中，我理解小批量对应于从重放缓冲区中获取的样本数量。 但在像 PPO 这样的基于策略的算法中，需要一集的完整轨迹来计算策略梯度。 这是否意味着 PPO 中的小批量对应于集数？ 这非常令人困惑，因为在 DQN 中我们可以使用 TD 误差来计算梯度，但基于策略的算法需要全长来计算梯度。    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dcfqfd/question_about_minibatch_in_ppo/</guid>
      <pubDate>Mon, 10 Jun 2024 07:22:25 GMT</pubDate>
    </item>
    <item>
      <title>如何构建 Mujoco Envs。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dc44c7/how_to_build_mujoco_envs/</link>
      <description><![CDATA[我想为 Kinova 的 jaco2 手臂构建一个 Mujoco 环境，我有 CAD，并且存在 ROS Gazebo 环境，但我一生都讨厌 ROS，而且我更喜欢 RL 而不是机器人技术。我真的在寻找除 XML 之外的另一种构建环境的方法。让我知道。     提交人    /u/elonmusk-A12   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dc44c7/how_to_build_mujoco_envs/</guid>
      <pubDate>Sun, 09 Jun 2024 21:00:14 GMT</pubDate>
    </item>
    <item>
      <title>“奖励黑客行为可以推广到所有任务”，Nishimura-Gasparian 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dc3obb/reward_hacking_behavior_can_generalize_across/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dc3obb/reward_hacking_behavior_can_generalize_across/</guid>
      <pubDate>Sun, 09 Jun 2024 20:41:36 GMT</pubDate>
    </item>
    <item>
      <title>将实验移至健身房</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbpceo/moving_experiment_to_gym/</link>
      <description><![CDATA[大家好， 我有点绝望，因为我正在试图弄清楚这一点，而且没有多少时间了。 我目前正处于论文的十字路口，论文将在三周内截止。深入研究并尝试了稳定基线和简单模型后，我意识到，要使我的项目正常运行并使用稳定基线或 CleanRL，我需要创建一个自定义的 gym 环境。 在我的项目中，我必须使用物理模拟器 (NS3)，它由多辆试图同时传输信息的汽车组成，并尝试使用 RL 来了解如何减少网络延迟。（观察空间、交通状态先前的延迟；每辆车传输信号时的动作空间）。  我已经有一个使用 DQN 构建的代码，但是性能一直很差，我的“论文”是关于探索新算法的。 因此，我想利用这里的 reddit 读者的经验和智慧 - 我应该 (1) 将我的环境移至 Gym 吗？ (2) 在 GitHub 上尝试一些原生 PPO 算法吗？如果您能提供任何可以查看的地方，那就太棒了！！ 任何想法都将不胜感激 &lt;3！非常感谢！    提交人    /u/No-Jelly-233   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbpceo/moving_experiment_to_gym/</guid>
      <pubDate>Sun, 09 Jun 2024 08:52:22 GMT</pubDate>
    </item>
    <item>
      <title>“克劳德的性格”，Anthropic（设计克劳德-3助手角色）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbedkn/claudes_character_anthropic_designing_the_claude3/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbedkn/claudes_character_anthropic_designing_the_claude3/</guid>
      <pubDate>Sat, 08 Jun 2024 22:20:01 GMT</pubDate>
    </item>
    <item>
      <title>DQN 与 Vanilla 策略梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbb6m7/dqn_vs_vanilla_policy_gradient/</link>
      <description><![CDATA[      嗨， 我正在尝试比较两种深度强化学习算法的性能：Vanilla Policy Gradient (REINFORCE) 和 DQN。我希望我的算法能够学会在一组机器上调度作业，同时尽可能紧密地打包它们。观察空间是尺寸为 (20, 164) 的图像，代理必须从 10 台机器中选择 1 台。我在训练期间使用 100 个不同的作业序列，并使用一组新作业进行测试。每集使用一个作业序列。 令人惊讶的是，我的 DQN 算法的表现比 Vanilla Policy Gradient (VPG) 更差。我尝试调整 tau、重放缓冲区大小、批量大小、折扣因子、学习率等，但结果似乎没有 VPG 那么高。例如，这里是具有以下 DQN 配置的两者的折扣平均奖励图表  G = 0.95 重放内存 = 100_000  最小重放内存 = 10_000  批量大小 = 64  tau = 10  lr = 0.001 epochs = 600（在每个 epoch 运行 20 episodes 后训练网络） episodes = 20 隐藏节点 = 128 epochs 数量 = 600 每个 epoch 的 episodes 数量 = 20 否。每集的工作量 = 200  .您能帮助我理解为什么会发生这种情况吗？我在每个时期后都会训练主网络。 https://preview.redd.it/3gg0dz3zje5d1.png?width=1924&amp;format=png&amp;auto=webp&amp;s=ac90a3c378d0817f00acc79600e87a34cb7c0fb3   由    /u/hifzak  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbb6m7/dqn_vs_vanilla_policy_gradient/</guid>
      <pubDate>Sat, 08 Jun 2024 19:49:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习产品化的必要之恶</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dbagrf/a_necessary_evil_for_productionizing_rl/</link>
      <description><![CDATA[        提交人    /u/bin_und_zeit   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dbagrf/a_necessary_evil_for_productionizing_rl/</guid>
      <pubDate>Sat, 08 Jun 2024 19:16:54 GMT</pubDate>
    </item>
    <item>
      <title>RLZoo 超参数中的“normalize”是什么意思？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1db61s7/what_does_normalize_mean_in_rlzoo_hyperparameters/</link>
      <description><![CDATA[我正在查看这里给出的 RLZoo 超级参数。例如，让我们采用以下代码 - InvertedDoublePendulumBulletEnv-v0: normalize: true n_envs: 8 n_timesteps: !!float 2e6 policy: &#39;MlpPolicy&#39; n_steps: 2048 batch_size: 64 gae_lambda: 0.95 gamma: 0.99 n_epochs: 10 ent_coef: 0.0 learning_rate: 2.5e-4 clip_range: 0.2   这里的 `normalize: true` 是什么意思？我假设这可能是观察标准化，但是，这里有这样的代码 -normalize: &quot;{&#39;norm_obs&#39;: False, &#39;norm_reward&#39;: True}&quot;因此，当我们执行 `normalize: true` 时，什么是被规范化的： rlzoo 中是否有用于动作规范化的超参数？我没有找到任何，但也许它可能会有所帮助？     提交人    /u/Academic-Rent7800   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1db61s7/what_does_normalize_mean_in_rlzoo_hyperparameters/</guid>
      <pubDate>Sat, 08 Jun 2024 15:55:29 GMT</pubDate>
    </item>
    <item>
      <title>我的动作空间有 90% 被掩盖了，我希望从中获得计算上的好处</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</link>
      <description><![CDATA[我的 q-learning 任务中有一个包含 5000 个动作的大型动作空间，但在任何时候都只有几百个合法动作，因此我会屏蔽它们，但是，我只是在模型预测步骤之后进行屏蔽。有没有办法事先屏蔽它并获得大幅加速？  def get_predictions(self, state, legal_mask): state = np.reshape(state, [1, self.state_size) act_values = [act_values[i] if legal_mask[i] == 1 else -np.inf for i in range(len(act_values))] return act_values     submitted by    /u/Breck_Emert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</guid>
      <pubDate>Sat, 08 Jun 2024 07:03:17 GMT</pubDate>
    </item>
    </channel>
</rss>