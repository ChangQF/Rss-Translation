<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sun, 09 Mar 2025 12:25:21 GMT</lastBuildDate>
    <item>
      <title>机器人技术定制体育馆环境设计。包装纸还是阶级继承？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为水下机器人构建自定义环境。我已经尝试使用一个快速且脏的整体环境，但是如果我尝试修改环境以添加更多传感器，转换输出，重用代码，以完成另一个任务等，我现在会遇到问题。 ，我想重构代码并必须做出一些设计选择：我应该使用一个不适用的spass类并在每个任务中使用训练和训练的范围，或者我要训练劳动范围，或者我要训练劳动范围，或者我要求职。我只有一个基类，并将其他所有内容添加为包装器（包括传感器配置，任务奖励 +逻辑等）？ 如果您知道环境创建的良好资源，这将不胜感激）  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/equiald-diver     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</guid>
      <pubDate>Sun, 09 Mar 2025 09:18:42 GMT</pubDate>
    </item>
    <item>
      <title>Han等人“一般推理需要学习从一开始推理”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</guid>
      <pubDate>Sun, 09 Mar 2025 08:05:53 GMT</pubDate>
    </item>
    <item>
      <title>软动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6t9rx/soft_action_masking/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有一个想法“软动作遮罩”？对于那些对强化学习的原始数学贴心的人，我将提前道歉。我的想法还没有正式的数学。 让我以一个例子来说明我的想法。想象一个具有以下约束的环境：   - 代理商的可用动作之一是“什么都不做”。   - 每秒发送太多动作是一件坏事。但是，这里不知道具体数字。也许我们有一些数据，即每秒大约10个动作是最大的。有时13/秒是可以的，有时8/秒是不希望的。 防止代理在给定时间范围内采取太多操作的一种方法是使用动作掩蔽。如果最大的动作率是一个明确定义的数量，例如，在最后一秒钟内，代理已经采取了10个操作，则代理将被迫“无所事事”。通过动作面具。一旦最后一秒钟的动作数量降至10以下，我们将不再使用面具并让代理自由选择。 现在，现在考虑到我们的模糊要求，我们是否可以逐渐强迫我们的代理商选择“无所事事”。动作越来越接近极限？我故意不会在数学上正式描述这个想法，因为我认为这在很大程度上取决于您使用的算法类型。相反，我会尝试描述直觉。如上所述，在环境限制中，我们的速率限制每秒约为8-13个动作。如果代理商在最后一秒钟已经采取了10次措施，并且非常有信心它希望采取其他措施，也许我们应该允许它。但是，如果它在篱笆上，只有与什么无所作为相比，只有稍微倾向于采取其他动作，也许我们应该稍微推动它，以便它选择什么都不做。随着动作数量的增加，这个“ nuding”变得越来越强。一旦达到13，在此示例中，我们本质上使用了上述典型的动作掩盖方法，并迫使代理什么都不做，无论其偏好如何。 在策略梯度算法中，这种方法在我看来更有意义。我可以想象，仅将灰心的动作偏好乘以（0,1）中的值。传统的动作掩蔽可能会完全乘以0。我还没有考虑到基于价值的算法。 你们都在想什么？这似乎是有用的吗？我在一个自己的项目和大脑冲击解决方案中大致遇到了这个问题。我可以实现的另一个解决方案是奖励函数，它不鼓励超过限制，但是直到代理商实际学习奖励功能的这一方面，它可能会大大超过限制，并且我需要在任何方面实施一些艰难的动作掩盖。另外，这样的奖励功能似乎很棘手，因为利率限制奖励可能与我实际想学习的奖励是正交的。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6t9rx/soft_action_masking/</guid>
      <pubDate>Sat, 08 Mar 2025 22:52:00 GMT</pubDate>
    </item>
    <item>
      <title>学习ISAAC SIM / ISAAC实验室的最快方法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6rkdf/fastest_way_to_learn_isaac_sim_isaac_lab/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 机电一体化工程师在这里具有ROS/凉亭的经验和表面水平Pybullet + Pybullet + Gymnasium体验。我正在训练RL代理在某个任务上进行训练，我需要进行一些域随机化，因此将其并行化将有很大的帮助。最快的最低工作示例最快的最快是什么。学习ISAAC SIM/ISAAC实验室框架的方法或来源用于模拟RL代理的培训？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1j6rkdf/fastest_way_way_to_to_lealed_isaac_sim_isaac_isaac_lab/&gt; [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6rkdf/fastest_way_to_learn_isaac_sim_isaac_lab/</guid>
      <pubDate>Sat, 08 Mar 2025 21:33:06 GMT</pubDate>
    </item>
    <item>
      <title>训练将四个代理与自我播放联系起来</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6ntrv/training_connect_four_agents_with_selfplay/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好！ 我目前正在使用ML-Agents创建可以通过使用自我游戏来玩连接四场游戏的代理。 我已经训练了代理商多个小时的训练，但是我的代理人仍然太弱了，无法赢得我的胜利。我注意到的是，代理商将始终尝试先于董事会的中心部分，据我所知。 href =“ https://imgur.com/a/0lcejny”&gt; https://imgur.com/a/0lcejny    我认为，值1应该始终代表自己的代理，而-1代表-1代表对手。列满满后，我掩盖了此列，以便代理不能将更多件放入列中。插入一件作品后，始终检查胜利条件。在获胜时，获胜的球员将获得+1，输球球员-1。在抽奖时，两者都会收到0。 这是我的问题：  在国际象棋中查看elo时，尚未达到3000的评级。但是我的代理商已经在ELO 65000处，但仍然输了。 Elo应该有点封闭吗？我觉得拥有5个数字的Elos应该已经是无与伦比的。 我的设置足以训练四个吗？我觉得自从我看到进步以来，我应该没事，但我认为这很慢。我看到的主要问题是即使经过大约5000万步，代理商仍然不会阻止对手的胜利/如果可能的话，如果可能的话，不要以下一步的方式赢得比赛    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/cuuuubee     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6ntrv/training_connect_four_agents_with_selfplay/</guid>
      <pubDate>Sat, 08 Mar 2025 18:43:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么功能近似会导致折扣RL的问题，但不会平均奖励RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6mr4u/why_does_function_approximation_cause_issues_in/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在 强化学习简介（第10.3章）中，萨顿介绍了平均奖励设置，没有打折的地方，并且代理值延迟延迟延迟的重新延迟，以立即奖励。他提到功能近似可能会导致折扣设置的问题，这是使用平均奖励的原因之一。 我了解平均奖励设置如何工作，但我不完全了解为什么功能近似值在折扣下差异。有人可以解释问题吗？ 为什么在证据中，萨顿实际上表明，折现设置在数学上等同于未估计的设置（成比例为1/（1 -γ）1/（1  -  \ gamma）1/（1  -  \ gamma）1/（1-γ）1/（1-γ）），所以我不明确地构成折扣的问题。具有政策改进定理，它可以确保提高一个国家的价值会导致整体政策改善。但是据我所知，这个问题适用于持续和情节任务，所以我仍然不明白为什么平均奖励是一个更好的选择。 有人可以在这里阐明动机吗？  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/nigilt_hippo1724     [link]   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6mr4u/why_does_function_approximation_cause_issues_in/</guid>
      <pubDate>Sat, 08 Mar 2025 17:56:37 GMT</pubDate>
    </item>
    <item>
      <title>有关培训基于RL的发电机的建议，具有改变奖励功能的高维物理模拟的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6lwli/advice_on_training_a_rlbased_generator_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我对机器学习和增强学习相对较新，我将其用于另一个领域的研究。我正在培训MLP，以生成一组高维参数（〜500–1000），以运行与物理相关的模拟。目的是生成两者的参数集：   满足必要的条件（条件x）  - 这与特征值有关，并且需要模拟运行。     产生一个模拟结果，可以匹配实验性数据的范围，但如果可以满足的范围，则可以 挑战是模拟本身是非常昂贵的，因此我想避免在无效的参数上浪费计算，并且想法是，该生成器应该能够生成大量有效的参数集。 我目前的计划是： 我的计划：  ph st phl strong &lt;强&gt; &lt;强&gt;  第1阶段：训练发电机生成定期满足条件X的参数集（例如他所有生成的集合的80％）。    阶段2：一旦模型擅长满足条件x，就可以从模拟的结果中引入奖励信号，以提高与实验性数据的匹配。 我还没有发现有关切换奖励功能中期训练的文献 - 这是RL中的已知/标准方法吗？是否有支持这种类型的奖励优化的论文或框架？ 这种两阶段方法对我的情况是否合理？ 我目前正在使用 Evolution策略（ES）进行优化吗？我是否应该将优化技术从第1阶段切换到2阶段？ 我知道奖励功能的重要性吗？只需添加tp tp tp tp the阶段1奖励2阶段的模拟的奖励？ 从第1阶段i i i i i lir lir lig s of erse sot sod of erse sod able of erse a of erse a of erse a of erse a of e彼此在空间（仍然尊重x exep x），所以我可以对2 expe of seper of separe 2我求解。这是否仅通过在Pahse 1中给予探索的奖励（例如，如果它产生尊重的条件X彼此远离的情况，就可以给予奖励）？  预先感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sangalewata     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6lwli/advice_on_training_a_rlbased_generator_with/</guid>
      <pubDate>Sat, 08 Mar 2025 17:18:32 GMT</pubDate>
    </item>
    <item>
      <title>输入/输出建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6itiu/inputoutput_recommendation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是强化学习的新手，我真的不知道我的输入和输出应该如何优化学习。  它们是否应该在0到1或-1和1之间，我应该尝试最小化其数字并更多地依靠0和1之间的实际值等等...  您是否有任何资源（YouTube视频，文书工作）可以帮助我找到我正在寻找的东西？提交由＆＃32; /u/poppyshit     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6itiu/inputoutput_recommendation/</guid>
      <pubDate>Sat, 08 Mar 2025 14:54:55 GMT</pubDate>
    </item>
    <item>
      <title>兼容的RL算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6euod/compatible_rl_algorythims/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在启动我在计算机科学方面的硕士论文。我的目标是训练ISAAC实验室中的四倍机器人，并比较不同算法如何学习和对环境变化的反应。我计划使用SKRL库，其中具有以下算法可用：   对抗运动priors （ amp  ）      cross-entropy方法（ cem  ）     深层确定性策略梯度（ ddpg ）    &lt;&gt;  double q-network （ ddqn ） href =“ https://skrl.readthedocs.io/en/latest/api/api/agents/dqn.html”&gt; deep q-network （ dqn ） href =“ https://skrl.readthedocs.io/en/latest/api/apen/agents/ppo.html”&gt;近端策略优化（ ppo  ）      q-learning （ q-learning ）））   强大的策略优化（ rpo  ）      soft actor-critic （ sac   ）     国家行动奖励国家行动（ sarsa  ）      twin-delayed ddpg （ td3 ） href =“ https://skrl.readthedocs.io/en/latest/api/apent/trpo.html”&gt;信任区域策略优化（ trpo ））                &#39;我是否可以在iSAAC中实现所有效果。我还试图找到哪种算法更有趣，因为我无法使用所有算法。我认为3-4是最佳选择。任何帮助都将不胜感激，我在这个领域很新。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/solodres123     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6euod/compatible_rl_algorythims/</guid>
      <pubDate>Sat, 08 Mar 2025 11:01:46 GMT</pubDate>
    </item>
    <item>
      <title>GRPO在体育馆</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6eezp/grpo_in_gymnasium/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在适应 grpo 算法（最初是为LLM提出的），以在体育馆中使用Mujoco使用Mujoco进行连续的加固学习问题。  g 相应的奖励以计算相对优势。 对于连续行动任务，我的解释是，在每个时间步长，我需要：  示例 g 与策略分布相比。  使用这些奖励来计算相对优势，因此，在各自的环境中的行动。在连续行动环境中更有效？任何提高效率的建议或建议将不胜感激！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/totokk55     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6eezp/grpo_in_gymnasium/</guid>
      <pubDate>Sat, 08 Mar 2025 10:29:17 GMT</pubDate>
    </item>
    <item>
      <title>初学者项目：使用机器学习和图像处理检测假图像的A-Agent！”🚀</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6e6ve/beginner_project_aiagent_that_detects_fake_images/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/indows-phase-9280     [link]   ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6e6ve/beginner_project_aiagent_that_detects_fake_images/</guid>
      <pubDate>Sat, 08 Mar 2025 10:12:22 GMT</pubDate>
    </item>
    <item>
      <title>初学者QS关于范围和自动启动应用程序爱好项目的可行性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6cd1a/beginner_qs_about_scope_and_feasibility_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  感谢您阅读我的问题。首先让我说这是一个爱好项目，我知道它的局限性并非微不足道 - 尤其是，无法在尚未记录的市场数据上执行回测，这些市场数据与指标信号状态在应用程序运行的原始时间生成的指标状态。  话虽如此，这是项目：  p.1）记录指标信号和相关目标价格水平的状态，这些状态通过与信号状态词典进行比较来验证。 （完成）  p.2）构建“贸易设置”作为信号状态的汇合或组合。设置的定义是用布尔语表达解析器定义的，该纸张将表达式转换为机器的表达式（例如，“ RSI”过多买了状态，而Moving_average_50已上面;每个设置都由（1）条目，（2）输入目标（例如市场价格与输出以特定指标状态输出的价格）的定义（布尔表达式）组成，（3）出口；（4）出口目标。 （完成）  p.3）执行虚拟交易（部分完成），该虚拟交易模拟位置大小复合和从条目和退出（完成的）  p.4）  p.4）可以在每个蜡烛的时间邮票上记录所有指标状态，以及烛台的价格值。因此，例如，您可以在录制100个指标的数据时运行该应用程序两个星期或一个月。  从ML角度来看，问题的问题是：使用上述四个功能与机器学习库使用ML算法发现在该记录期间产生最佳P/L的设置定义吗？对我来说，这似乎是一个伸展的部分是，以信号状态定义设置的布尔表达式的构建似乎包含无限的可能性：受支持的操作员是（a）通过括号嵌套，（b）和（c）或（c）或（d）不嵌套。即使您对每个定义中可以使用多少操作数设定了一定的限制，当您考虑到“贸易设置”有四个子定义时，即使您可以使用多少操作数。 （进入，进入目标，退出，退出目标 - 尽管要明确的是目标定义，但如果信号x具有状态y，则非常简单，然后以“ else x”状态限制订单以其输出的价格值限制订单，而是市场订单的价格＆quot＆quot;），它似乎太复杂了（对于ML Fitness Onculation而言，可能性太高了，似乎太确定了）。但是我真的不知道。  我作为该领域的新手的问题是：  1）游戏笔记本电脑可以执行此类ML任务以找到在数据期间生成最佳P/L的最佳设置定义，如果是这样，则需要进行多少小时/天/天以进行分析？   2）是否有有充分记录的开源C＃库，最适合此？  3）如果您认为这是值得尝试的，那么您想在开始这样的鞋子之前，如果您在我的鞋子中脱颖而出，或者如果您认为这是不合时宜的，那么您是否会在我的鞋子中进行任何可能的p1 practial，否则在我的鞋子上又有其他任何可能的p1 p1 pry of u1 p1 p1 p1，您会想知道任何技巧或建议吗？  我很高兴能在这里获得知识渊博的观点的好处。 我在考虑它时，我相信第一阶段是训练它以使其做出有效的布尔表达方式。我已经有一个表达式解析器，它将接受任何输入，然后返回对输入是有效还是无效的判断（作为交易设置定义）。因此，如果有一种方法可以在预处理阶段中合并该解析器，在该阶段中，ML算法首先必须学习如何制作有效的定义，那么一旦它学会了如何制定有效的定义，我认为它可能有可能学习达到的最佳p/L p/L c/L c/l compeces。提交由＆＃32; /u/u/lmk99     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6cd1a/beginner_qs_about_scope_and_feasibility_for/</guid>
      <pubDate>Sat, 08 Mar 2025 07:55:08 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助我的研究的DRL实施！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6bc5v/need_help_for_my_researchs_drl_implementation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对所有人的问候，我想对那些愿意帮助我为研究的事情表示感谢。我目前被困在DRL实施中，这是我要做的事情：  1）我正在研究类似网格的，基于转弯的战术RPG。我选择了PPO作为DRL框架的骨干。我在策略网络中使用多模式设计用于状态表示：第一分支=空间数据，例如地形，定位等，第二个分支=字符状态。这两个分支都将通过处理层进行处理层，例如卷积层，嵌入，FC和最后连接到单个矢量中，并再次通过FC层。  2）我计划为策略网络使用共享的网络体系结构。   3）我想乘坐多i-discrete Action Space，E。tup cotive vector vector vector vector vector a tup cotife vector a tup vector a tup vector，e.g.g.g.g.，tup v。瓷砖，动作选择1，使用项目1（只是非常快的示例以说明）。 In other words, for every turn, the enemy AI model will yield these three decisions as a tuple at once. 4) I want to implement the hierarchical DRL for the decision-making, whereby the macro strategy decides whether the NPC should play aggressively, carefully, or neutral, while the micro strategy decides the movement, action choice, and item (which aligns to the output).我想动态训练决策。&lt; / p&gt;  5）我的问题 /混乱是，我应该在哪里实施层次结构设计？是在多模式体系结构的FC层之后的一层吗？还是在政策网络之外？还是在策略更新中？同样，当向量通过FC层（以防万一）通过FC层（完全连接的层）时，将将向量转换为非开采格式，而只是处理过的信息。那我该如何连接到我之前提到的层次设计？ 我不确定我是否正确地设计了此设计，或者是否有更好的方法来执行此操作。但是，我必须保留的实现是PPO，多模式设计和输出格式。如果我提供的上下文还不够清楚，请感谢您的帮助。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hengyewken96    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j6bc5v/need_help_for_my_my_researchs_drl_implementation/”&gt; [link]    [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6bc5v/need_help_for_my_researchs_drl_implementation/</guid>
      <pubDate>Sat, 08 Mar 2025 06:43:18 GMT</pubDate>
    </item>
    <item>
      <title>狭窄分布的交叉问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j67jxk/crossq_on_narrow_distributions/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！我想知道是否有人有与Crossq进行狭窄分布的经验？即STD很小。我的CrossQ实现在摆上效果很好，但在我的自定义环境上效果不佳。它非常不稳定，返回平均线将大大下降，然后爬回去。但是，当我使用SAC在自定义环境上学习时，这并没有发生。我知道，这里可能会有多个级别的问题来源，但是我很好奇地处理以下情况：STD非常小，并且随着代理商的了解，即使是较小的分配变化也会导致巨大的价值变化，因为批次批量的“归一部分”。运行的std很小 - ＆gt;非常稀有或新见的状态 - ＆gt; OOD，如果STD很小，则将新值标准化为巨大的值 - ＆GT;降低性能 - ＆gt;随着统计信息适应新值，性能再次成长 - ＆gt;重复重复或只是无法恢复。通常，我的十字架确实恢复了，但这是次优的。 那么，有人知道如何处理这种情况吗？ ，您如何监视batchnormalizations的性病值？我不知道一个直截了当的方式，因为每个维度都会跟踪统计信息。也许是Max STD和Min STD？由于我的问题将出现在最小std很小的时候。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/automatic-web8429     [link]    32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j67jxk/crossq_on_narrow_distributions/</guid>
      <pubDate>Sat, 08 Mar 2025 02:55:37 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>