<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 25 Aug 2024 01:13:43 GMT</lastBuildDate>
    <item>
      <title>使用 Python 中的 RL 解决离散时间 LQR 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezvegd/struggling_to_solve_discretetime_lqr_with_rl_in/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezvegd/struggling_to_solve_discretetime_lqr_with_rl_in/</guid>
      <pubDate>Sat, 24 Aug 2024 03:03:42 GMT</pubDate>
    </item>
    <item>
      <title>[讨论] 对施米德胡贝的看法？人工智能之父，几乎不受关注</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezn907/discussion_thoughts_on_schmidhuber_father_of_ai/</link>
      <description><![CDATA[      我通过一些非常受人尊敬的技术人员发现了尤尔根·施米德胡贝，他们说他是最好的人工智能科学家。他的 LSTM 论文被引用超过 10 万次，声称为 transformers 奠定了基础，大量 RL 工作都是通过好奇心驱动的学习和建模世界进行的。他的论文非常棒，通俗易懂，信息量很大。他在自己的网站上炫耀二头肌和胡闹真的很有趣。 https://preview.redd.it/sp7160dn6hkd1.png?width=880&amp;format=png&amp;auto=webp&amp;s=9a82f2a764dca2a6ccc7c61c4875e2a8d43233d5 然后有些人告诉我不要读他的论文。其他一些顶级人工智能人物也对他不满。到目前为止，我发现的主要争议只是他有点讨厌地强迫别人正确引用他（我不知道谁是对的，但这听起来像是一个典型的问题，即将可以用其他方式充分解释的事情归咎于恶意。就像他们不引用他，因为他们真的没有使用他的工作，只是没有深入挖掘或类似的东西）。 我开始崇拜这个人，因为他在实验室所做的开创性工作和他退休前建立比自己更聪明的人工智能的人生使命。论文读起来很美。他说英语就像一个超级恶棍。只是一个整体有趣的角色。但这个领域的人怎么看他呢？我还没有和任何知道谣言和有内幕消息的人谈过这件事。 就目前而言，尽管有一些怪癖，但我会尽一切努力成为他的学徒，这是假设性的。似乎是这份工作的最佳人选之一。我会阅读他的所有论文，同时将这些想法付诸实践。此外，他的公司 NNAISENSE 似乎越来越不活跃了。我想知道发生了什么     提交人    /u/Inexperienced-Me   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezn907/discussion_thoughts_on_schmidhuber_father_of_ai/</guid>
      <pubDate>Fri, 23 Aug 2024 20:42:58 GMT</pubDate>
    </item>
    <item>
      <title>真实游戏的 Rl 框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezn25c/rl_framwork_for_real_games/</link>
      <description><![CDATA[向各位书呆子问好。我只是在寻找一个可以用于真实游戏的框架（或任何可以完成工作的东西）。例如卢克索游戏。我知道真正的控制台游戏是“gym retro”（我有 ps3 gens 及以上版本。没有那么好玩的游戏，更多的是炫耀图形和政治故事）。无论如何。有没有什么框架可以实现？或者有其他方法吗？ 我的一个朋友建议：构建一个应用程序来获取您的屏幕，然后使用 opencv，然后使用带有 kayboard 的整数......等等，所以不建议这样做。 非常感谢。    提交人    /u/matin1099   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezn25c/rl_framwork_for_real_games/</guid>
      <pubDate>Fri, 23 Aug 2024 20:35:01 GMT</pubDate>
    </item>
    <item>
      <title>Open AI Gym 从定义的空间中创建一个“空元素”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezjciq/open_ai_gym_create_an_empty_element_from_a/</link>
      <description><![CDATA[大家好， 关于如何使用 gymnasium.spaces，我有一个相对简单的问题。假设我定义了以下观察空间 observation_space = space.Dict( { &quot;local-obs&quot;: space.Box(low=-1, high=1, shape=(3,3,1)), &quot;global-obs&quot;: space.Box(low=-1, high=1, shape=(7,7,3)) } )  是否存在现有方法可以让我“初始化”满足上述观察空间结构的观察？即一个字典，其中 local-obs 是一个形状为 (3,3,1) 的全零数组，等等。 我可以不使用我定义的空间结构来单独创建这样的观察，但我觉得应该有更好的方法来强制执行该结构。 提前致谢！    提交人    /u/GammaYankee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezjciq/open_ai_gym_create_an_empty_element_from_a/</guid>
      <pubDate>Fri, 23 Aug 2024 17:59:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们没有为每款棋盘游戏配备超人 AI？是兴趣还是知识？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ezjbkj/why_dont_we_have_superhuman_ai_for_every_board/</link>
      <description><![CDATA[我是 RL 的新手，所以如果这是一个愚蠢的问题，请别介意。 我想知道为什么大多数棋盘游戏中没有超人 AI，因为我们在国际象棋、围棋和扑克中都有超人 AI。 有很多流行的棋盘游戏，我真的很好奇超人 AI 会是什么样子，但却找不到任何类似的东西。例如，《卡坦岛》、《银河竞逐》、《辉煌》、《风险》等。 我们没有超人 AI 的原因是以下任何一种（或组合）吗？  现成的 RL 算法不能用于为每款棋盘游戏创建超人 AI，需要通过一些理论工作来适应这些游戏，这对非 RL 研究人员来说是一个很难解决的问题。 现成的 RL 算法可以由 RL 爱好者适应这些游戏，但人们对此兴趣不大（或者 RL 爱好者不够多） 还有什么原因吗？     提交人    /u/hellofloss   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ezjbkj/why_dont_we_have_superhuman_ai_for_every_board/</guid>
      <pubDate>Fri, 23 Aug 2024 17:57:58 GMT</pubDate>
    </item>
    <item>
      <title>2024 年学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez8qrv/learning_rl_in_2024/</link>
      <description><![CDATA[您好，2024 年有哪些不错的免费在线资源（课程、笔记）可以学习 RL？ 谢谢！    提交人    /u/spacejunk99   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez8qrv/learning_rl_in_2024/</guid>
      <pubDate>Fri, 23 Aug 2024 09:51:18 GMT</pubDate>
    </item>
    <item>
      <title>强化算法之前的游戏中的对手？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez7g8b/opponents_in_games_before_reinforcement_algorithms/</link>
      <description><![CDATA[我目前正在学习（深度）强化学习。我的教授向我们展示了实施算法来解决雅达利游戏的经典示例。我们讨论的算法可以追溯到 1990 年代末到 2010 年代初。 这让我对以下事情产生了疑问：小时候我曾经玩过例如 Yu-Gi-Oh Power of Chaos 系列或 Fifa 98，在这些游戏中，你的对手相当聪明（至少在我小时候的记忆中是这样），据我所知，你甚至可以设定对手的难度。当时已经实施了这些算法吗？或者这些只是硬编码的规则集？    提交人    /u/Vast-Signature-8138   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez7g8b/opponents_in_games_before_reinforcement_algorithms/</guid>
      <pubDate>Fri, 23 Aug 2024 08:22:13 GMT</pubDate>
    </item>
    <item>
      <title>我如何知道我的 RL 股票交易模型是否表现优异，因为它真的很好，或者是因为代码有故障？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez7896/how_can_i_know_whether_my_rl_stock_trading_model/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez7896/how_can_i_know_whether_my_rl_stock_trading_model/</guid>
      <pubDate>Fri, 23 Aug 2024 08:05:54 GMT</pubDate>
    </item>
    <item>
      <title>TRPO 论文中的随机变量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ez3vih/random_variable_in_trpo_paper/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ez3vih/random_variable_in_trpo_paper/</guid>
      <pubDate>Fri, 23 Aug 2024 04:26:43 GMT</pubDate>
    </item>
    <item>
      <title>彻底改变机器人行为：Transformer 如何通过动作分块提升模仿学习能力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyvevk/revolutionizing_robot_behavior_how_transformers/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyvevk/revolutionizing_robot_behavior_how_transformers/</guid>
      <pubDate>Thu, 22 Aug 2024 21:41:34 GMT</pubDate>
    </item>
    <item>
      <title>扑克解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyoa2i/poker_solution/</link>
      <description><![CDATA[我有一个类似于扑克的任务，即有全押、弃牌、过牌、跟注、加注等操作。但本质是比较牌的等级，等级较高的获胜。我决定实施其中一种 CFR 算法。但我不知道是哪一种。 所以我有几个问题：  实施的最佳算法是什么（Vanilla CFR、CFR+、MCCFR、Deep CFR 或其他（例如 PPO））。 处理加注选择的最佳方法是什么？我应该将其离散化（例如检查、跟注、弃牌、加注 0.5bb、1bb 等）还是有其他方法？ 是否有可以找到这些 CFR 算法的现成实现的来源？     提交人    /u/silenthnowakeup   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyoa2i/poker_solution/</guid>
      <pubDate>Thu, 22 Aug 2024 16:48:58 GMT</pubDate>
    </item>
    <item>
      <title>pybullet 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyhdme/pybullet_question/</link>
      <description><![CDATA[我正在做一个四足动物的项目，我想得到布尔值，看看它的脚是否接触地面，有人知道怎么得到它吗？    提交人    /u/youssef_naderr   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyhdme/pybullet_question/</guid>
      <pubDate>Thu, 22 Aug 2024 11:54:34 GMT</pubDate>
    </item>
    <item>
      <title>MARL 的框架/库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eyd8rw/framework_library_for_marl/</link>
      <description><![CDATA[嗨， 我在为 MARL 寻找类似于 CleanRL/SB3 的东西。 有人能推荐一下吗？我看到了 BenchMARL，但添加自己的环境看起来有点奇怪。我还看到了 epymarl 和 mava，但不确定哪个最好。理想情况下，我更喜欢 torch 中的某些东西。 期待您的推荐！ 谢谢 !    提交人    /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eyd8rw/framework_library_for_marl/</guid>
      <pubDate>Thu, 22 Aug 2024 07:25:55 GMT</pubDate>
    </item>
    <item>
      <title>神经进化 + 强化学习问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ey13vs/neuroevolution_rl_question/</link>
      <description><![CDATA[我最近完成了（大概）从头开始编写 PPO。我遇到的最大问题是，很难将其扩大规模并给它更多的时间进行训练，因为它最终会忘记一切，或者也可能会陷入局部最大值。我对神经进化有点兴趣，据我所知，神经进化的主要问题是它不能很好地扩展到更大的网络，并且需要大量的计算。  所以我的问题是，为什么没有很多神经进化 + RL 研究，或者没有普遍使用的实现？（如果有，请留下链接或名称）据我所知，独立训练一群 RL 代理并选择/交叉最好的代理应该可以解决神经进化和 RL 单独存在的缺点。    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ey13vs/neuroevolution_rl_question/</guid>
      <pubDate>Wed, 21 Aug 2024 21:09:26 GMT</pubDate>
    </item>
    <item>
      <title>多大的行动空间才算太大？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1exo9dz/how_large_of_an_action_space_is_too_large/</link>
      <description><![CDATA[我是强化学习的新手，所以我不确定这是否是一个合理的担忧。目前，我正在从事一个关于 THz 波段通信的研究项目。我正在编写一个深度 Q 学习算法来选择传输数据的最佳频带。我有 1217 个频带可供选择。我正在使用 OpenAI 的 gymnasium 框架。因此，我的动作空间如下所示： self.action_space = space.MultiBinary(1217)  我为代理选择使用的通道分配 1，为它选择传输数据的通道分配 0。这个动作空间是否太大？我是否应该增加波段的大小以减少可供选择的波段数量？或者是否有另一种方法允许代理从大列表中选择多个项目？应该允许代理选择它想要选择的任意数量。    提交人    /u/Hailwel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1exo9dz/how_large_of_an_action_space_is_too_large/</guid>
      <pubDate>Wed, 21 Aug 2024 12:25:04 GMT</pubDate>
    </item>
    </channel>
</rss>