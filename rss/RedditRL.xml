<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 23 May 2024 12:26:30 GMT</lastBuildDate>
    <item>
      <title>Cartpole 返回奇怪的东西。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</link>
      <description><![CDATA[我正在从头开始制作一个 PPO 代理（没有 Torch，没有 TF），它进展顺利，直到 env 突然返回一个维度为 5 的二维列表， 4而不是4，经过一番调试后，我发现这可能不是我的错，因为我没有对回报进行分配或执行任何操作，它只是在随机时间范围内发生并破坏了我的整个事情。有人知道为什么吗？   由   提交 /u/Mehcoder1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</guid>
      <pubDate>Thu, 23 May 2024 12:10:48 GMT</pubDate>
    </item>
    <item>
      <title>代表萨顿和巴托的“杰克的汽车租赁”问题的动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyib25/representing_the_dynamics_in_sutton_and_bartos/</link>
      <description><![CDATA[大家好！我正在学习 Sutton 和 Barto，并已完成有关动态规划的第 4 章。本章中的一个早期示例涉及一家汽车租赁公司（问题文本此处截图供参考）。作者使用这个玩具问题来说明策略迭代。 书中介绍的策略迭代是一个相当简单的算法。我陷入困境的是以编程方式表示问题的动态，即函数 p(s&#39;, r|s,a)。我不是在问“我如何计算出这些概率”，而是在问“我如何计算出这些概率”。而是“如何将它们存储在代码中以便于使用？” 对于本书中前面的示例，在处理值函数时，我通常会编写一个类似  的函数p&gt; defdynamic(s, a): “”“返回一个类似 [(s&#39;, r, p)] 的列表，其中 s&#39;是下一个状态，r 是观察到的奖励，p 是从状态 s&quot;&quot;&quot; 开始采取操作 a 后观察到 r 并最终达到状态 s&#39; 的概率 这使得通过动态（s，a）中的 sp，r，p_spr_sa 计算可能观察到的奖励和可能的下一个状态的总和变得非常方便： 此函数有效“包含”是指。环境，因为它决定了环境将如何对代理的操作做出反应。 对于汽车租赁问题，我正在努力以这种格式表示动态。事情比前面的示例更复杂，因为从 (s,a) 到 (s&#39;, r, p) 的逻辑映射要复杂得多。如果我想像我一直在做的那样实现这个功能，我想我可以，但是我想知道是否有人对如何更好地实现这一点有建议。任何关于动力学的这种表示是如何“典型地”的想法都可以理解。完成了吗？非常感谢！   由   提交/u/GorillaManStan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyib25/representing_the_dynamics_in_sutton_and_bartos/</guid>
      <pubDate>Thu, 23 May 2024 02:38:28 GMT</pubDate>
    </item>
    <item>
      <title>基于强化学习的八旋翼飞行器控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cycb22/control_of_an_octoplcopter_based_on_rl/</link>
      <description><![CDATA[大家好，我正在写我的硕士论文，其中涉及使用 Simulink 和 MATLAB 控制同轴八轴飞行器降落在移动目标上。我的系统有 20 个观测值并生成 3 个动作：推力、x 输入和 y 输入。由于我有两个 PD 控制器，并且将输入添加到 Ux 和 Uy 以控制级联控制器设置中的横滚和俯仰，因此复杂性增加。我确信我的模型是正确的，但我已经努力了两周才让代理收敛。它始终陷入 Q 值恒定为正的次优策略（我使用的是 DDPG）。尽管尝试了多次奖励函数修改来使无人机跟踪轨迹，但每一集的奖励仍然非常负面并且没有改善。考虑到大量的观察和系统的复杂性，这项工作是否可行？或者我应该考虑简化环境？任何建议或见解将不胜感激。谢谢   由   提交 /u/OkFig243   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cycb22/control_of_an_octoplcopter_based_on_rl/</guid>
      <pubDate>Wed, 22 May 2024 21:47:23 GMT</pubDate>
    </item>
    <item>
      <title>寻找对我的论文的反馈：迷宫导航强化学习中的 Sim2Real 迁移</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cy80jk/looking_for_feedback_on_my_thesis_sim2real/</link>
      <description><![CDATA[TL;DR: 寻找有关我的论文的反馈，该论文有关 RL 中的 Sim2Real 传输用于迷宫导航。 Google 表单或在下面发表评论。 GitHub 项目 | 论文草稿。  大家好， 我是 Lucas，目前正在为人工智能工程学士学位撰写论文，我正在寻找有关我的学士论文的反馈，题为“探索 Sim2Real 迁移在迷宫导航强化学习中的可行性”。 这些是一些“简单”的问题，因为我需要从人们那里得到一些外部反馈:) 注意：这个项目的实践时间仅限于 3 周，所以我没有有很多时间去尝试，这影响了我很多的选择。我现在正在考虑反思这些。 当前设置：  自定义 OpenAI Gym 环境及操作：前进、左、右 DFRobot 2WD miniQ 机器人底盘上的遥控车 3 个 HC-SR04 传感器用于墙距测量 DDQN 代理  有关该项目的更多信息（存储库有点混乱，提前抱歉）：Github 项目 或者您可以阅读我的论文草稿 问题：   哪种遥控汽车设置（2WD 与 4WD）更适合我的项目目标和方法？您推荐的任何具体型号或品牌以及原因？ 我的遥控汽车在这个项目中受到限制吗？ （两轮驱动且相当大）如果是，请解释如何实现。 哪种传感器/遥控汽车套件可能是更好的选择？ （例如，使用相机代替 HC-SR04） 我应该扩大虚拟环境中的动作空间吗？例如，让 OpenAI Gym 环境直接访问电机，而不是限制其向前、向右和向前移动。 您认为与使用自上而下的虚拟双胞胎相比，虚拟双胞胎能增加更多价值吗？查看摄像头可以帮助找到遥控汽车的位置（作为代理/环境的额外输入）？如果是，为什么？ 您认为我的研究成果有哪些实际应用？我的工作可能会对哪些特定行业或应用产生重大影响？  请填写此Google 表单或回答评论中的问题。感谢您的时间和反馈！或者，如果您有任何其他问题/反馈，我会非常高兴听到！！ 提前谢谢您:)   由   提交 /u/Practical-Apricot-71   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cy80jk/looking_for_feedback_on_my_thesis_sim2real/</guid>
      <pubDate>Wed, 22 May 2024 18:49:16 GMT</pubDate>
    </item>
    <item>
      <title>如何为 RL 创建基于神经网络的健身房环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxzuam/how_to_create_neural_network_based_gym/</link>
      <description><![CDATA[ 由   提交 /u/Past-News-1373   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxzuam/how_to_create_neural_network_based_gym/</guid>
      <pubDate>Wed, 22 May 2024 13:09:15 GMT</pubDate>
    </item>
    <item>
      <title>非对称游戏的单一模型还是多个模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxzli6/single_model_or_multiple_models_for_asymmetric/</link>
      <description><![CDATA[我对训练某些多人游戏的代理感兴趣。我对一些棋盘游戏特别感兴趣，在这些游戏中，玩家可以采取不同的行动，甚至可能有不同的获胜条件。  我的问题是为每个玩家训练一个网络还是为所有玩家训练一个网络是否更有意义。我计划使用自我对弈（我猜如果涉及多个模型，你就不能称之为自我对弈，但你明白了）和 MCTS 来训练模型，就像《Alpha Zero》中的那样。 &lt;我看到的大多数文献和例子都集中在两人游戏和相当对称的游戏上，比如国际象棋和围棋，其中唯一的不对称是游戏顺序。在这些游戏中，双方使用相同的网络非常简单，但我想知道具有不同动作空间的 3 人或 4 人游戏是否仍然如此。  考虑像 PPO 这样的 actor-critic 模型，使用策略的动作掩码和每个玩家一个值很容易实现。我主要想知道使用单个模型学习不同策略的影响。   由   提交 /u/OperaRotas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxzli6/single_model_or_multiple_models_for_asymmetric/</guid>
      <pubDate>Wed, 22 May 2024 12:57:36 GMT</pubDate>
    </item>
    <item>
      <title>我有一个深度强化学习 PPO 代理，它使用离散状态并输出离散动作。我正在考虑将其转换为 DRL 变压器。我应该如何在国家代币化或任何其他方法方面取得进展？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxxfrn/i_have_a_deep_reinforcement_learning_ppo_agent/</link>
      <description><![CDATA[嗨， 我目前有一个正在运行的 PPO 代理，但我正在考虑将我的环境扩展到一个更复杂的环境。因此，我正在考虑将当前的PPO智能体转换为PPO+变压器智能体。 状态类型：离散 动作类型：离散 我已经完成了对变压器的结构进行了一些研究，但我仍然有几个问题： 1）如何标记离散状态？ （我可以将状态转换为句子，然后将其传递到变压器中，但我认为这不是一个好方法，因为它涉及转换为字符串，然后再转换回数字。） 2）如何我该如何将变压器输出转换回状态？ 3) 我应该使用哪种变压器类型：仅编码器、编码器-解码器或仅解码器？ 我是新手变压器，所以任何帮助或建议都会非常有帮助。 提前致谢！   由   提交/u/Appressive_Bag1262   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxxfrn/i_have_a_deep_reinforcement_learning_ppo_agent/</guid>
      <pubDate>Wed, 22 May 2024 10:59:23 GMT</pubDate>
    </item>
    <item>
      <title>加入应用 RL 优化数据中心的挑战</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxx1m6/join_challenge_that_applies_rl_to_optimize_data/</link>
      <description><![CDATA[大家好，我正在与 Fruitpunch AI 合作组织一项挑战赛，人们可以在其中应用强化学习来优化数据中心的能源使用（请参阅这里https://app.fruitpunch.ai/challenge/ai-for-greener-datacenters ）。我认为这里可能有些人有兴趣在现实世界的用例中应用强化学习。如果您有任何疑问，请给我留言。    由   提交 /u/Lalalendalf   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxx1m6/join_challenge_that_applies_rl_to_optimize_data/</guid>
      <pubDate>Wed, 22 May 2024 10:33:06 GMT</pubDate>
    </item>
    <item>
      <title>DQN 显示损失有所改善，但缺乏奖励改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxws11/dqn_showing_loss_improvements_but_lacking_reward/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxws11/dqn_showing_loss_improvements_but_lacking_reward/</guid>
      <pubDate>Wed, 22 May 2024 10:15:17 GMT</pubDate>
    </item>
    <item>
      <title>健身房环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</link>
      <description><![CDATA[我有实时系统的数据集，我可以使用强化学习算法的数据表创建基于深度神经网络的环境吗？ 请尽快回复。紧急，陷入项目工作   由   提交 /u/Past-News-1373   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</guid>
      <pubDate>Wed, 22 May 2024 09:14:03 GMT</pubDate>
    </item>
    <item>
      <title>很难阅读大多数 RL 算法的 CS 符号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</link>
      <description><![CDATA[在观看带有示例的视频后，我理解了贝尔曼方程，但是有没有更直观的方法来理解任何论文中的任何政策制定。并不是所有的论文都能很好地打破他们的方程的作用，而是让比我智力更高的人尝试从论文中挖掘它 &lt;！-- SC_ON - -&gt;  由   提交 /u/baboolasiquala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</guid>
      <pubDate>Wed, 22 May 2024 04:16:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么是零和？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</link>
      <description><![CDATA[在很多论文中，你都可以看到“因为游戏是零和的”、“我们以零和的方式解决问题”之类的短语博弈”等，但为什么零和很重要？零和的哪些属性使训练变得更容易？ 例如，在 2P 设置中，很容易看出无论玩家 A 赚到什么，玩家 B 都会输。多人（&gt;2）游戏怎么样？ 再举一个例子，想象一个玩家拥有能力的游戏。玩家 A 有能力为自己生产 10 块金币。 B 从另一位选择的玩家那里偷走了 5 个金币。 C使所有其他玩家失去5金。这显然不是零和游戏，但是你不能按照正常方式训练代理吗？   由   提交 /u/HyogoKita19C   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</guid>
      <pubDate>Wed, 22 May 2024 01:07:16 GMT</pubDate>
    </item>
    <item>
      <title>棋盘游戏神经网络架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</link>
      <description><![CDATA[有人有过在棋盘游戏中尝试不同神经网络架构的经验吗？ 目前使用 PPO 进行数独 - 输入 I我正在考虑只是一个展平的板向量，因此神经网络是一个简单的 MLP。但我没有得到很好的结果——想知道 MLP 架构是否是问题所在？  AlphaGo 论文使用了 CNN，很想知道你们尝试过什么。感谢任何建议    由   提交 /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</guid>
      <pubDate>Tue, 21 May 2024 23:07:06 GMT</pubDate>
    </item>
    <item>
      <title>将 MAB 集成到 Web 应用程序中</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxl655/integrate_mab_into_a_web_application/</link>
      <description><![CDATA[大家好，我正在寻求将 MAB（多武装强盗）RL 集成到网络或移动应用程序中，该项目是为了我的硕士论文和它是关于使用主动学习和强化学习的自适应推荐系统，所以我已经在 Kaggle 中运行了所有内容（我正在做离线评估并使用重播），并且由于我是 RL 的新手，我想知道我应该如何解决整合这项工作进入网络或移动应用程序（我可以在网络/应用程序开发中管理自己）。谢谢大家。我选择使用 movielens Ml 1m 数据集。如果您需要任何其他信息，请告诉我。   由   提交 /u/Fredybec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxl655/integrate_mab_into_a_web_application/</guid>
      <pubDate>Tue, 21 May 2024 22:47:58 GMT</pubDate>
    </item>
    <item>
      <title>这个项目创意好不好-自驾代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</link>
      <description><![CDATA[我正在为自动驾驶代理构建一个项目，我计划加入会改变状态的动态交通信号灯和独立于代理在网格中随机移动的动态行人。加入动态行人有多难？在网格中随机生成信号灯和行人的位置可以吗？还可以使用 q-learning 实现 我怎样才能使这个项目更具随机性？我考虑在每次模拟之前引入与概率分布一起使用的天气影响，并且根据天气预报，这将影响车辆的燃料消耗，惩罚会导致燃料消耗的百分比大幅增加。本质上，代理处于生存模式，以确保不会发生碰撞、遵守交通法规并且不会达到最大燃料容量。    提交人    /u/amulli21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</guid>
      <pubDate>Tue, 21 May 2024 15:56:21 GMT</pubDate>
    </item>
    </channel>
</rss>