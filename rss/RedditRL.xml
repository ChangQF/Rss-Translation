<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 23 Dec 2024 12:32:59 GMT</lastBuildDate>
    <item>
      <title>我使用 Genesis 训练了四足动物步行策略并将其部署到物理机器人上</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkms4c/i_trained_a_quadruped_walking_policy_using/</link>
      <description><![CDATA[新发布的（生成式）物理模拟器 Genesis 宣称其在 GPU 上并行模拟的训练速度将达到未来水平（据说快 10-80 倍）。这是真的吗？实际上，虽然我在腿部运动的强化学习方面做了一些工作，但我对模拟器知之甚少。但我对训练速度的提升持怀疑态度。因此我尝试使用 Genesis 训练运动策略，并在此总结我的发现。 训练代码现已在 Github 上开源：https://github.com/lupinjia/genesis_lr 与 IsaacGym 相比的训练速度 实验设置如下：  机器：RTX 3080 10G num_envs：4096 任务：go2 在平地上行走  对于具有相同参数的相同任务，与 IsaacGym 相比，Genesis 的训练速度可以是 1.3 倍，而图形内存使用量大约只有一半。  Genesis 中的训练速度  ![](https://imgur.com/RaYRhoW)  IsaacGym 中的训练速度  ![](https://imgur.com/ltMivab)  Genesis 中的内存使用情况  ![](https://imgur.com/KUfNyr6)  IsaacGym 中的内存使用情况  ![](https://imgur.com/4vn7Uul) 由于内存使用量较小，我们可以运行更多并行环境，从而进一步加速训练。 策略测试（模拟+部署） 模拟 对于 go2 在平面上行走任务，训练一个包含 10000 个 env 和 600 个 ites（即 144M 步）的策略大约需要 12 分钟。播放结果如下： ![](https://imgur.com/S87NijF) 部署 同样，对于 go2 在平面上行走任务，训练一个包含 10000 个 env 和 1k 个 ites 的策略+显式估计器大约需要 23 分钟。部署结果如下： ![](https://imgur.com/g0EiOfG)  总结一下，Genesis 确实在一定程度上提升了训练速度，并保持了保真度。但这里展示的任务只是一个非常简单的任务。它是否能够模拟更多接触丰富、更困难的任务仍有待检验。    submitted by    /u/Awkward_Swimmer_5649   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkms4c/i_trained_a_quadruped_walking_policy_using/</guid>
      <pubDate>Mon, 23 Dec 2024 12:31:39 GMT</pubDate>
    </item>
    <item>
      <title>GPT 1 培训</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkm8ba/gpt_1_training/</link>
      <description><![CDATA[有人能告诉我训练 GPT 1 花了多长时间、使用了多少 GPU 以及进行了多少个 epoch 吗？    提交人    /u/notanhumanonlyai25   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkm8ba/gpt_1_training/</guid>
      <pubDate>Mon, 23 Dec 2024 11:56:42 GMT</pubDate>
    </item>
    <item>
      <title>通过仅玩游戏的最后步骤来伪造课程学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkm1y5/faking_curriculum_learning_by_in_self_play_by/</link>
      <description><![CDATA[我正在尝试教 ppo 玩各种游戏，当游戏很复杂、奖励稀少且完全是最终奖励时，建议进行课程学习。  如果游戏有固定的长度，比如说 5000 步，我可以通过在每个场景中随机播放前 4500 步来开始训练，然后让 PPO 播放最后 500 步，并且在训练进行的同时，我让 ppo 越来越早地开始玩，直到它自己玩整个游戏？ 当然，如果游戏的最后步骤在很大程度上取决于早期游戏，那么网络直到训练的很后期才会看到各种游戏状态，但对于具有重复/周期性结构的游戏来说，这应该没问题，比如说开始。 我的想法正确吗？这种技术有名字吗？   由    /u/drblallo  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkm1y5/faking_curriculum_learning_by_in_self_play_by/</guid>
      <pubDate>Mon, 23 Dec 2024 11:44:24 GMT</pubDate>
    </item>
    <item>
      <title>我通过强化学习和训练构建了一个玩《黑暗之魂》的人工智能。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkk837/i_built_a_ai_to_play_dark_souls_through/</link>
      <description><![CDATA[你好， 我构建了一个可直接与 Dark Souls 交互并玩游戏的 AI。Dark Souls 没有 API，因此这是一个持续的复杂过程，需要反复尝试。 到目前为止，该过程已取得良好结果，尤其是对于在非常大且复杂的环境中盲目运行且奖励稀少的代理而言。 为了促进 AI，我设计了一个非常大且量身定制的奖励塑造框架，专门针对 Dark Souls 环境，模拟类似 API 的奖励结构以进行指导和发展。正如人们所说，罗马不是一天建成的，但它带来了数次飞跃的进步和突发行为。 我还设计了两个新系统，试图帮助指导代理并促进学习和进步。  第一个方法称为 Vivid，这个过程允许代理直接从视频输入中学习，比如它所在精确区域的专业演练。这种方法跳过了传统的图片和数据文件帧提取过程，而是从直接视频帧中学习，提高了映射到动作和奖励结构的效率和准确性。  第二个方法称为 TGRL（文本引导强化学习），它允许代理直接从基于文本的演练中学习，该演练以基于脚本的步骤解析信息，通过关键词检测和动作映射进行上下文排序，并与代理跟随和学习的奖励结构相关联。  到目前为止，它已经在代理和进展中产生了一些有趣的结果和行为变化。  有一次，它甚至在游戏中执行了一个我从未遇到过或知道可能做到的动作，也没有在其他任何地方见过。  我当前的挑战是指导。虽然目前的奖励结构运行良好，但代理仍然处于反复试验的环境中，没有像 API 那样明确的游戏进程统一性方向。  如果有人对如何让代理在游戏中“定向移动”（应该是这样）以减少随机性有任何建议，我很乐意接受帮助。  当前进度包括：  挑选第一个牢房钥匙 打开第一个牢房门 杀死前三个被动空洞 成功爬上第一个梯子  下一个预期进度：  在第一堆篝火旁点燃并休息 进入并导航第一个 Boss 竞技场  可以执行游戏中的所有操作。菜单导航、设备导航和升级机制尚未设计或实施。     由    /u/UndyingDemon 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkk837/i_built_a_ai_to_play_dark_souls_through/</guid>
      <pubDate>Mon, 23 Dec 2024 09:28:52 GMT</pubDate>
    </item>
    <item>
      <title>具身沟通游戏：强化学习代理的任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkib91/the_embodied_communication_game_a_task_for/</link>
      <description><![CDATA[我一直在为我的数据科学硕士学位开展一个独立研究项目，我偶然发现了一个非常有趣的多智能体游戏，该游戏是在本文中设计和运行的（在人类身上）：https://thomscottphillips.wordpress.com/wp-content/uploads/2014/08/scott-phillips-et-al-2009-signalling-signalhood.pdf 这个游戏本质上是这样设计的：两个玩它的智能体必须协调他们各自对世界的观察所分配的信息。要想 100% 准确地做到这一点，唯一的办法就是根据玩家在游戏空间中的行为设计和使用通信系统。人类能够在这款游戏中进行交流，并实现 100% 的成功率（尽管并非每对人类都能做到这一点），而无需事先知道交流是否必要甚至可能。 我在 Gymnasium 用 PettingZoo 和 SuperSuit 的帮助设计了具身交流游戏 (ECG)，并最终在原始游戏和简化版本中训练了一些 Stable-Baselines3 RL 模型。我写了一篇论文，详细介绍了我所做的努力和取得的成果（毫不奇怪，对于模型之间紧急通信的结果持负面态度），虽然它还没有达到科学论文的水平，但我为撰写它而感到自豪，并且愿意接受批评和评论，所以我想我会把它发布在这里。 我也很好奇你们中是否有人将这个问题（如上面链接的论文中概述的具身通信游戏）作为多智能体 RL 问题来处理？这是一个非常有趣的问题，而且似乎很有可能用当前 RL 模型的某些版本来解决。 这是我关于 ECG 的论文的链接： https://evanmccormick37.github.io/independent-study-F24-learning-RL-with-gymnasium/    提交人    /u/EvanMcCormick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkib91/the_embodied_communication_game_a_task_for/</guid>
      <pubDate>Mon, 23 Dec 2024 07:00:22 GMT</pubDate>
    </item>
    <item>
      <title>使用行为策略时在 PPO/TRPO 中正确实现 Rollout</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hki1ob/correct_implementation_of_rollout_in_ppotrpo_when/</link>
      <description><![CDATA[      您好， 在 TRPO 论文的公式 14 中，作者建议可以使用不同的分布来对动作进行采样  https://preview.redd.it/h1hb15abnj8e1.png?width=259&amp;format=png&amp;auto=webp&amp;s=26e87c810443f4fd10692175f7d5cc631d30ca7a 如果我按照均匀分布对动作进行采样，其中 q 不是 pi_old，那么我该如何计算 Q_theta_old？这个 Q 不是来自 pi_old 吗？我应该将 Q 乘以重要性比率吗？我很困惑。本质上，我正在尝试使用均匀分布（基于本文后面提到的 Vine 方法）实现简短的推出，但优势函数或 Q 函数基于旧策略，而动作是从行为函数 q 中采样的，因此使用此动作计算的优势是否属于旧策略或行为函数 q ？    提交人    /u/gtm2122   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hki1ob/correct_implementation_of_rollout_in_ppotrpo_when/</guid>
      <pubDate>Mon, 23 Dec 2024 06:41:26 GMT</pubDate>
    </item>
    <item>
      <title>具有 O（log（T））遗憾界限的随机强盗算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk9kl7/stochastic_bandit_algorithms_with_ologt_regret/</link>
      <description><![CDATA[是否有任何随机老虎机算法具有比 O(sqrt(T)) 更好的遗憾界限？不是 O( \sqrt(T log T) )，而是 O( \log(T) ) 或至少 O( sqrt(log T) )？ 我知道具有 K 个臂的 UCB 具有 O( \sqrt(kT) ) 的遗憾界限。它是否表明 UCB 或其变体可以实现 O( log T ) 遗憾，或者至少 O( sqrt(log T) )？    提交人    /u/Anxious_Positive3998   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk9kl7/stochastic_bandit_algorithms_with_ologt_regret/</guid>
      <pubDate>Sun, 22 Dec 2024 22:34:42 GMT</pubDate>
    </item>
    <item>
      <title>SAC 中的 Pytorch 梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk8wlz/pytorch_gradients_in_sac/</link>
      <description><![CDATA[我在理解如何在 pytorch 中实现软演员评论家时计算梯度时遇到了麻烦。具体来说，多个神经网络的梯度是如何耦合的，以及何时计算梯度，何时不计算梯度。 我有一个基本的实现，与 Phil Tabor 的 YouTube 视频非常相似。我的实现是这里。我的问题是我不明白 `retain_graph=True` 字段是如何工作的。我在这个实现中也没有使用 .detach() 或 torch.no_grad。也许这是一个单独的问题，但是如果我只是要在调用 .backwards() 之前将梯度归零，那么执行 .detach() 有什么意义呢？ 我的目标是执行相同的实现，但不使用 `retain_graph=True` 参数，因为我不完全理解它的用途。我知道它在反向传播后会保持梯度，但我不明白在 SAC 中这样做的目的。我尝试在没有这个参数的情况下执行此操作，但是，我就是无法让它工作。该代码可以在此处看到。 任何帮助都将不胜感激！    提交人    /u/LostBandard   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk8wlz/pytorch_gradients_in_sac/</guid>
      <pubDate>Sun, 22 Dec 2024 22:02:20 GMT</pubDate>
    </item>
    <item>
      <title>训练模型来学习在大规模离散动作空间中进行跟踪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk1pjv/training_a_model_to_learn_tracing_on_massive/</link>
      <description><![CDATA[大家好！我正在开展一个实验性的强化学习项目，用于 PCB 布线。该项目的理念是训练强化学习代理连接巨大的 2D 电路板上的输入和输出点（始终位于中间），电路板可能大到 20,000×20,000 个单元，并带有 3 像素宽的迹线、10 像素间距、不可通行点等限制。 目前，我正在尝试尽可能简化问题，减少限制，但仍打算在大型电路板上进行训练。现在，我只想让模型学习如何从随机输入/输出对中跟踪一条线，即使这样也相当具有挑战性。 它基本上感觉像是一种寻路或“贪吃蛇游戏”设置，但我不确定强化学习是否真的可以处理这种大小的输入。我还没有找到任何类似的项目可以比较，所以我怀疑 RL 是否是合适的工具。 我尝试过 DQN 方法（很快就被丢弃了，在 50x50 以上的棋盘上很吃力），基于 CNN 的方法（在大型棋盘上很吃力），现在我正在探索分层 RL。这感觉最有前途，但我仍然不确定它的扩展性如何。 我是这个领域的初学者，之前主要解决的是一些较小的问题。任何建议或参考都将不胜感激！ 谢谢！    提交人    /u/Deranged_Koala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk1pjv/training_a_model_to_learn_tracing_on_massive/</guid>
      <pubDate>Sun, 22 Dec 2024 16:19:03 GMT</pubDate>
    </item>
    <item>
      <title>如何学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hk03kn/how_to_learn_reinforcement_learning/</link>
      <description><![CDATA[您好。我是一个编程经验超过 40 年的老家伙，想学习更多关于强化学习的知识，也许可以用强化学习编写一个简单的游戏，比如跳棋。 我想更好地理解强化学习的数学。我已经几十年没有接触过微积分了，但我相信只要努力，我就能学会。而且，我更喜欢亲自动手做一些编码的事情，以证明我确实理解了我正在学习的内容。 我在网上看过一些教程，它们似乎都使用了一些 RL 库，我假设这些库只是封装并隐藏了实际的数学知识，或者它们是对数学的高级讨论。 在哪里可以找到在线或书籍形式的关于理论和数学或机器学习的讨论，并在编程世界中进行应用练习？    提交人    /u/EricTheNerd2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hk03kn/how_to_learn_reinforcement_learning/</guid>
      <pubDate>Sun, 22 Dec 2024 14:59:56 GMT</pubDate>
    </item>
    <item>
      <title>Mac mini m4 适合现实生活吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hjtjos/mac_mini_m4_for_rl/</link>
      <description><![CDATA[对于现实生活来说，这是否是个不错的选择？我正在考虑在这两者之间做出选择，或者自己组装一台电脑，但在我住的地方，一台像样的电脑可能要花两倍的钱。  我工作的环境并不复杂。    提交人    /u/Hulksulk666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hjtjos/mac_mini_m4_for_rl/</guid>
      <pubDate>Sun, 22 Dec 2024 07:13:00 GMT</pubDate>
    </item>
    <item>
      <title>实施 DQN 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hj6s2c/probem_implementing_dqn/</link>
      <description><![CDATA[您好，我是一名计算机工程师，正在攻读人工智能和机器人学硕士学位。我碰巧必须实现深度学习论文，总的来说，我没有遇到任何问题。我正在学习 RL，我试图通过阅读论文从头开始编写 DQN 的实现。然而，尽管架构很简单，但我在实现它时遇到了问题。 他们具体说：  第一个隐藏层将 16 个 8 × 8 滤波器与输入图像进行卷积，步长为 4，并应用整流器非线性 [10, 18]。第二个隐藏层将 32 个 4 × 4 滤波器与步长为 2 进行卷积，然后再次进行整流器非线性。最后的隐藏层是完全连接的，由 256 个整流单元组成。  让我认为有两个卷积层后面跟着一个完全连接层。我在 Hugging Face 上找到的示意图证实了这一点  ![示意图](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/deep-q-network.jpg) 然而在 PyTorch RL 教程 中，他们使用了这个网络： ```python class DQN(nn.Module): def init(self, nobservations, n_actions): super(DQN, self).init_() self.layer1 = nn.Linear(n_observations, 128) self.layer2 = nn.Linear(128, 128) self.layer3 = nn.Linear(128, n_actions) # 在优化期间使用一个元素调用以确定下一个操作或批次 #。返回 tensor([[left0exp,right0exp]...])。def forward(self, x): x = F.relu(self.layer1(x)) x = F.relu(self.layer2(x)) return self.layer3(x)  ``` 我不完全确定 128 来自哪里。 原始实现证实了这是预期的做法（我不是 LUA 专家，但看起来非常相似） Lua function nql:createNetwork() local n_hid = 128 local mlp = nn.Sequential() mlp:add(nn.Reshape(self.hist_len*self.ncols*self.state_dim)) mlp:add(nn.Linear(self.hist_len*self.ncols*self.state_dim, n_hid)) mlp:add(nn.Rectifier()) mlp:add(nn.Linear(n_hid, n_hid)) mlp:add(nn.Rectifier()) mlp:add(nn.Linear(n_hid, self.n_actions)) return mlp end  我在网上找到了各种实现，都使用了相同的架构。我显然遗漏了一些东西，但有人知道问题可能是什么吗？     提交人    /u/Puddino   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hj6s2c/probem_implementing_dqn/</guid>
      <pubDate>Sat, 21 Dec 2024 09:56:00 GMT</pubDate>
    </item>
    <item>
      <title>“偷取免费午餐：揭露 Dyna 式强化学习的局限性”，Barkley 和 Fridovich-Keil</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</link>
      <description><![CDATA[      我认为这是适合讨论这个问题的地方，但无论如何，对于这种无耻的自我推销，我深表歉意…… 论文：https://arxiv.org/abs/2412.14312 关于 X 的相当不错的 TLDR：https://x.com/bebark99/status/1869941518435512712 最近，作为研究的一部分，我对基于模型的 RL 非常感兴趣，但很快遇到了一个大问题：该领域一些主要方法的原始 PyTorch 实现速度非常慢。除非您有数百个 GPU 小时可供使用，否则几乎不可能完成任何合理的工作。 因此，我决定在 JAX 中重新实现一个知名算法，即基于模型的策略优化 (MBPO)，这让事情运行得更快。然而，经过大量的故障排除和测试后，我遇到了另一个惊喜：只要您尝试在不同于他们论文中测试的基准（即 DMC 而不是 Gym）上从头开始训练它，它的表现就会比简单的离线策略算法差，后者需要的训练时间要少几个数量级。 这涉及 6 个 gym 环境和 15 个 DMC 环境，因此非常一致。 这让我很好奇，经过一番挖掘，我和我的导师最终写了一篇关于它和其他 Dyna 风格的基于模型的 RL 方法的论文。剧透：并非所有 dyna 风格的方法都无法在基准测试中发挥作用，但当 Dyna 失败时，这并不是一个孤立或简单的问题。 如果有人有兴趣尝试的话，JAX 实现应该会在明年推出。希望得到大家的反馈！ https://preview.redd.it/ei78d7hbz28e1.png?width=3600&amp;format=png&amp;auto=webp&amp;s=c981756b0f0803e12b2e0e8e6a7816eedb085cfe    提交人    /u/darthbark   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</guid>
      <pubDate>Fri, 20 Dec 2024 22:32:37 GMT</pubDate>
    </item>
    <item>
      <title>表格软 q 学习 停留在简单的网格世界</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</link>
      <description><![CDATA[您好，我正在为一个简单的 5x5 网格世界开发一个简单的表格软 q 学习代理。经过几次尝试后，它卡在了特定状态。我不知道这是实施错误还是超参数不好。我将在下面附上代码。还有其他建议吗？ 谢谢 import numpy as np import time import os class Env(): def __init__(self): self.height = 5 self.width = 5 self.posX = 0 self.posY = 0 self.endX = self.width-1 self.endY = self.height-1 self.actions = [0, 1, 2, 3] self.stateCount = self.height*self.width self.actionCount = len(self.actions) def reset(self): self.posX = 0 self.posY = 0 self.done = False return 0, 0, False # 采取行动 def step(self, action): if action==0: # 左 self.posX = self.posX-1 if self.posX&gt;0 else self.posX if action==1: # 右 self.posX = self.posX+1 if self.posX&lt;self.width-1 else self.posX if action==2: # 向上 self.posY = self.posY-1 if self.posY&gt;0 else self.posY if action==3: # 向下 self.posY = self.posY+1 if self.posY&lt;self.height-1 else self.posY done = self.posX==self.endX and self.posY==self.endY # 将 (x,y) 位置映射到 0 到 5x5-1=24 之间的数字 nextState = self.width*self.posY + self.posX reward = 1 if done else -0.1 return nextState, reward, done # 返回一个随机动作 def randomAction(self): return np.random.choice(self.actions) # 显示环境 def render(self): for i in range(self.height): for j in range(self.width): if self.posY==i and self.posX==j: print(&quot;O&quot;, end=&#39;&#39;) elif self.endY==i and self.endX==j: print(&quot;T&quot;, end=&#39;&#39;) else: print(&quot;.&quot;, end=&#39;&#39;) print(&quot;&quot;) def softmax(x): e_x = np.exp(x - np.max(x)) # 为了数值稳定性 return e_x / e_x.sum() class Agent: def __init__(self, stateCount, actionCount, env, max_steps = 100, epochs = 50, discount_factor = 0.99, lr = 0.1, temp = 1): # Q 表：包含每个 (state,action) 对的 Q 值 self.Q = np.zeros((stateCount, actionCount)) # 超参数 self.temp = temp self.lr = lr self.epochs = epochs self.discount_factor = discount_factor # 环境 self.env = env self.max_steps = max_steps def getV(self, q_value): return self.temp * np.log(np.sum(np.exp(q_value / self.temp))) def choose_action(self, state): # q = self.Q[state] # v = self.getV(q) # dist = np.exp((q - v) / self.temp) # action_probs = dist / np.sum(dist) # return np.random.choice(env.actions, p=action_probs) action_probs = softmax((self.Q[state] - self.getV(self.Q[state])) / self.temp) return np.random.choice(env.actions, p=action_probs) # 训练循环 def run(self): for i in range(self.epochs): state, reward, done = self.env.reset() steps = 0 while not done: os.system(&#39;cls&#39;) # print(self.Q) print(&quot;epoch #&quot;, i+1, &quot;/&quot;, self.epochs) self.env.render() time.sleep(0.01) # 计数完成游戏的步骤 steps += 1 # 选择软 q 学习动作 action = self.choose_action(state) # 采取行动 next_state, reward, done = self.env.step(action) # 使用贝尔曼方程更新 Q 表值 # target = reward + self.discount_factor * np.sum(action_probs * self.Q[next_state]) # target = reward + self.discount_factor * self.getV(self.Q[next_state]) target = reward + (1 - done) * self.discount_factor * self.getV(self.Q[next_state]) self.Q[state][action] += self.lr * (target - self.Q[state][action]) # 更新状态 state = next_state if steps &gt;= self.max_steps: break print(&quot;\nDone in&quot;, steps, &quot;steps&quot;.format(steps)) time.sleep(0.8) def print_q_table(self): for i in range(0,len(self.Q)): for j in range(0,len(self.Q[i])): print(self.Q[i][j], end=&quot; &quot;, flush=True) print(&quot;&quot;) if __name__ == &quot;__main__&quot;: # 创建 CartPole 类的实例 env = Env() resolver = Agent(env.stateCount, env.actionCount, env) resolver.run()    由    /u/Majestic-Tap1577  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</guid>
      <pubDate>Fri, 20 Dec 2024 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>[AHT] 对 SOMALI CAT 环境的查询</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</link>
      <description><![CDATA[我刚刚阅读了两篇关于临时团队合作的非常有趣的论文，重点关注沟通如何提高此类任务的效率，&quot;A Penny for Your Thoughts: The Value of Communication in Ad Hoc Teamwork&quot; 和 &quot; Expected Value of Communication for Planning in Ad Hoc Teamworks&quot;。作者曾在一个名为 SOMALI CAT 的环境中工作过，我很好奇：这里有没有人曾经在这个环境中工作过，或者您是否知道它是否可以在某些通用 Python RL 测试框架（例如 Gymnasium）上使用。 感谢您的帮助    提交人    /u/potatodafish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</guid>
      <pubDate>Fri, 20 Dec 2024 18:43:58 GMT</pubDate>
    </item>
    </channel>
</rss>