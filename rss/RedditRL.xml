<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 04 Sep 2024 09:17:18 GMT</lastBuildDate>
    <item>
      <title>随着损失的增加，调试拟合 Q 评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8lhrg/debug_fitted_qevaluation_with_increasing_loss/</link>
      <description><![CDATA[嗨，专家，我正在使用 FQE 进行离线离策略评估。但是，我发现在训练过程中我的 FQE 损失并没有减少。  我的环境是离散动作空间和连续状态/奖励空间。我尝试了几种修改来调试根本原因：  更改超参数：学习率、FQE 的时期数 更改/规范化奖励函数 确保数据解析正确  上述方法均不起作用。 以前我有一个类似的数据集，我很确定我的训练/评估流程是正确的并且运行良好。 您还会检查/实验什么以确保 FQE 正在学习？    提交人    /u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8lhrg/debug_fitted_qevaluation_with_increasing_loss/</guid>
      <pubDate>Wed, 04 Sep 2024 05:23:31 GMT</pubDate>
    </item>
    <item>
      <title>体育馆渲染问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8fq7g/gymnasium_rendering_question/</link>
      <description><![CDATA[我知道目前 gymnasium box2d 环境有 2 种渲染模式：human 和 rgb_array。有没有办法通过删除所有查看器渲染来加快程序速度？ 编辑：我应该指定这一点，因为使用 rgb_array 运行似乎花费的时间几乎是 human 模式的两倍    提交人    /u/Admirable-Length-465   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8fq7g/gymnasium_rendering_question/</guid>
      <pubDate>Wed, 04 Sep 2024 00:27:06 GMT</pubDate>
    </item>
    <item>
      <title>在矢量输入环境上重置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8fg21/resetting_on_vector_input_environments/</link>
      <description><![CDATA[大家好，有人尝试过在矢量观测环境中使用策略软重置吗？因为我的代理在软重置甚至 0.1 正常噪声后都无法恢复。 我根据 P. D&#39;Oro 2023 尝试过。    提交人    /u/Automatic-Web8429   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8fg21/resetting_on_vector_input_environments/</guid>
      <pubDate>Wed, 04 Sep 2024 00:14:25 GMT</pubDate>
    </item>
    <item>
      <title>数据质量对训练模仿学习模型的影响：使用 Aloha Kit 进行实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</link>
      <description><![CDATA[        由    /u/Trossen_Robotics  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8at8j/the_impact_of_data_quality_on_training_imitation/</guid>
      <pubDate>Tue, 03 Sep 2024 20:45:06 GMT</pubDate>
    </item>
    <item>
      <title>在 cudnn 中使用 Pytorch 寻找计算二阶导数的更好方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</link>
      <description><![CDATA[嗨，我试图将安全算法：CPO 应用于我当前的 RL 代码。我使用 pytorch 和 GPU（cuda）运行我的代码。当计算 CPO 中的二阶导数时，cuda 会自动使用 cudnn 进行此类计算。问题是，由于 CuDNN API 的限制，CuDNN RNN 不支持 Double Backing。我也在 stackoverflow 中描述了这个问题，但没有得到答案。而且当我搜索时，这个错误， NotImplementedError：&#39;_cudnn_rnn_backward&#39; 的导数未实现。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs)NotImplementedError: 未实现“_cudnn_rnn_backward”的导数。由于 CuDNN API 的限制，CuDNN RNN 不支持双重向后计算。要运行双重向后计算，请在运行 RNN 的前向传递时暂时禁用 CuDNN 后端。例如：使用 torch.backends.cudnn.flags(enabled=False): output = model(inputs) 在 RNN 和相关模型中的双重向后计算中更常见。如果你们知道解决方案，那将会很有帮助   由    /u/dAmiBouY539  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f89z05/finding_a_better_way_to_calculate_second/</guid>
      <pubDate>Tue, 03 Sep 2024 20:10:56 GMT</pubDate>
    </item>
    <item>
      <title>分享我的玩具项目“JAxtar”，用于解谜的纯 jax 和 jittable A* 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</link>
      <description><![CDATA[嗨，我想介绍一下我的玩具项目 JAxtar。 我将其发布到 r/reinforcementlearning 这里以及 r/JAX，因为它与典型的 RL 不同，但它是为使用神经启发式算法的 RL 编写的，例如 DeepCubeA，我计划朝这个方向改进它。 这不是很多人会觉得有用的代码，但我在编写它时用 Jax 完成了大部分杂技，我认为它可能会激励其他人使用 Jax 的人。 我的硕士论文是关于使用 RL 进行 A* 和神经启发式训练以解决 15 个难题，但当我反思它时，最大的头痛是 CPU 和 GPU 之间数据传输的频率高且长度长。几乎一半的执行时间都花在这些通信瓶颈上。这个问题的另一种解决方案是 DeepCubeA 提出的分批 A*，但我觉得这不是一个完整的解决方案。 有一天，我偶然发现了 mctx，这是 google deepmind 用纯 jax 编写的 mcts 库。 我对这种方法很着迷，并多次尝试用 Jax 编写 A*，但都没有成功。问题在于哈希表和优先级队列。 毕业后经过很长一段时间，研究了许多例子，绞尽脑汁，我终于设法编写了一些可用的代码。 我很自豪地说，此代码有几个特殊元素  用于将定义的状态转换为哈希键的 hash_func_builder 用于并行查找和插入的哈希表 可以批处理、推送和弹出的优先级队列 用于拼图的完全 jitted A* 算法。  我希望这个项目可以成为任何喜欢 Jax 和这种类型的 RL（带启发式的 A*）的人的鼓舞人心的例子    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f81v75/sharing_my_toy_project_jaxtar_the_pure_jax_and/</guid>
      <pubDate>Tue, 03 Sep 2024 14:50:19 GMT</pubDate>
    </item>
    <item>
      <title>作为输出的动作向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</link>
      <description><![CDATA[大家好，我计划在即将开展的项目中利用强化学习。在这种情况下，我的输入（或状态）将由两个矩阵组成：一个表示车辆的位置，另一个表示车辆的速度。输出或动作将是一个具有四个连续元素的向量，每个元素的范围在 0 到 1 之间。例如，一个步骤之后的一个可能动作可能是 [0.51, 0.76, 0.9, 0.12]。 有人能建议哪种强化学习算法最适合这种类型的问题吗？提前感谢您的帮助！    提交人    /u/muttahirulislam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f8066f/a_vector_of_actions_as_output/</guid>
      <pubDate>Tue, 03 Sep 2024 13:38:02 GMT</pubDate>
    </item>
    <item>
      <title>随情节变化动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</link>
      <description><![CDATA[当动作空间本身随着情节而变化时，开关策略算法的预期行为是什么。这会导致非平稳性？ 动作空间是连续的。在 Mujoco Ant Cheetah 等中，典型情况是它代表扭矩。假设在一集中动作空间是 [1, -1] 下一集是 [1.2, -0.8] 下一集是 [1.4, -0.6] ... ... 未来的某一集是 [2, 0] .. 动作空间范围的变化由某些函数控制，并且在每个情节开始之前都会随着情节而变化。像 ppo trpo ddpg sac td3 这样的算法的预期行为应该是什么？他们能够处理吗？对于像 mappo maddpg matrpo matd3 等 marl 算法也有类似的问题。 这是由于动态变化而导致的非平稳性吗？是否存在任何无效的操作范围。我们可以将总体范围限制为某个高低值，但范围会随着情节而改变。    提交人    /u/Intrepid_Discount_67   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ykfk/changing_action_space_over_episodes/</guid>
      <pubDate>Tue, 03 Sep 2024 12:24:25 GMT</pubDate>
    </item>
    <item>
      <title>深度 CFR 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</link>
      <description><![CDATA[我从原始 Deep CFR 文章中获取了代码 链接：https://arxiv.org/pdf/1811.00164 我现在正在为我的游戏实现 Deep CFR 算法，当我编写完整算法时，我遇到了很多错误。所以我的问题是，本文末尾的代码是否完全正确，错误是否在我的代码中？如果有人已经为他们自己的任务实现了 Deep CFR 并且可以分享提示/代码，那就太好了 如果有人愿意提供帮助，我可以上传我的实现。它需要最终确定，而且我没有足够的经验来了解如何正确实现它。 这是我的代码： nn.py https://codeshare.io/KWByyK memory.py https://codeshare.io/obp99r deep_cfr.py https://codeshare.io/k0zAAA game_tree.py https://codeshare.io/mP6MMp utils.py https://codeshare.io/ldQppd 对于游戏，我从这个库中获取了 texasholdem：https://github.com/SirRender00/texasholdem。    提交人    /u/silenthnowakeup   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7ndur/deep_cfr_implementation/</guid>
      <pubDate>Tue, 03 Sep 2024 01:11:53 GMT</pubDate>
    </item>
    <item>
      <title>“运动物理学”及其对人类模仿学习的启示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f7i153/motor_physics_and_implications_for_imitation/</guid>
      <pubDate>Mon, 02 Sep 2024 21:09:17 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习项目征集及创新建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f78wnt/request_for_deep_reinforcement_learning_projects/</link>
      <description><![CDATA[基本上就是标题！ 您能推荐一些带有代码的深度强化学习项目吗？此外，如果您能让我知道是否有可能添加新颖性或更改代码，我将不胜感激。谢谢！    提交人    /u/Sweet_Speed9010   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f78wnt/request_for_deep_reinforcement_learning_projects/</guid>
      <pubDate>Mon, 02 Sep 2024 15:01:36 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Gymnasium 创建自定义环境来训练 CloudSimPlus 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</link>
      <description><![CDATA[基本上，我从基于 Java 的模拟器（CloudSimPlus）中提取指标，并使用 ProcessBuilder 将其发送到 py 脚本，该脚本反过来应该做出响应，决定如何更改云中的基础设施 我想要构建一个环境，其中状态表示为 5 个单独的离散值，并使用 3 种可能的操作训练代理 我已经按照这里的建议尝试了一个基本版本，但现在有点困惑，因为它没有按照我的意图工作    提交人    /u/Automatic_You_1939   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f723ji/how_do_i_create_a_custom_env_using_gymnasium_to/</guid>
      <pubDate>Mon, 02 Sep 2024 08:57:14 GMT</pubDate>
    </item>
    <item>
      <title>政策绩效分解证明问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f6hgtt/problem_with_proof_of_decomposition_of_policy/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f6hgtt/problem_with_proof_of_decomposition_of_policy/</guid>
      <pubDate>Sun, 01 Sep 2024 15:42:04 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的元学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</link>
      <description><![CDATA[您好，RL 中的大多数元学习似乎已应用于策略空间，而很少应用于 DQN 中的价值空间。我想知道为什么如此注重将策略适应新任务，而不是将价值网络适应新任务。Meta Q Learning 论文似乎是唯一一篇使用 Q 网络进行元学习的论文。这是真的吗？如果是，为什么？    提交人    /u/Sea-Collection-8844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f68r4l/meta_learning_in_rl/</guid>
      <pubDate>Sun, 01 Sep 2024 07:22:46 GMT</pubDate>
    </item>
    <item>
      <title>寻找一个可供人类和代理合作完成任务的环境，其中存在多种可能的策略/子任务。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f64qvt/looking_for_an_environment_for_a_human_and_agent/</link>
      <description><![CDATA[大家好。我正在计划一个硕士研究项目，重点关注人类和 RL 代理如何协调共同完成任务。我正在寻找一个相对简单（理想情况下是 2D 和离散的）但仍允许团队采用不同高级策略的游戏式环境。这很重要，因为我的大多数潜在研究主题都集中在人类代理团队如何协调选择然后执行该高级策略。 到目前为止，Overcooked 环境 是我见过的最有前途的。在这种情况下，不同的高级策略可能是 (1) 拾取食材、(2) 烹饪食材、(3) 交付订单、(4) 丢弃垃圾。但所有这些策略都非常简单，所以我希望有更多选择。例如，在游戏中，代理可以决定是否收集资源、攻击敌人、治愈、探索地图等。任何建议都绝对值得赞赏。    提交人    /u/chowder138   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f64qvt/looking_for_an_environment_for_a_human_and_agent/</guid>
      <pubDate>Sun, 01 Sep 2024 03:09:40 GMT</pubDate>
    </item>
    </channel>
</rss>