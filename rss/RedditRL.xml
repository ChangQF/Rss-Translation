<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 18 Nov 2024 18:23:34 GMT</lastBuildDate>
    <item>
      <title>RL 中的迁移/适应</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gub4y3/transferadaptation_in_rl/</link>
      <description><![CDATA[我们可以使用基于域的目标进行初始化，而不是随机初始化目标，是否有与领域启发目标相关的论文用于评论家更新？    提交人    /u/theboyisnemo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gub4y3/transferadaptation_in_rl/</guid>
      <pubDate>Mon, 18 Nov 2024 18:05:45 GMT</pubDate>
    </item>
    <item>
      <title>强化学习期刊（RLJ）第一版已经出版！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gu979k/the_first_edition_of_the_reinforcement_learning/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gu979k/the_first_edition_of_the_reinforcement_learning/</guid>
      <pubDate>Mon, 18 Nov 2024 16:48:12 GMT</pubDate>
    </item>
    <item>
      <title>使用游戏开发引擎 Godot 的 RL Agents</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gu501w/rl_agents_with_the_game_dev_engine_godot/</link>
      <description><![CDATA[      大家好！ 我对 AI 有一些了解，我想用我在 Godot 上找到的这个 Dark Souls 模板做一个使用 RL 的项目：DS 模板链接，但我在尝试连接 RL Agents Library 来控制 DS 模板上的播放器时遇到了很大困难，有没有人有这种连接经验，可以帮帮我吗？我肯定会非常感激！ https://preview.redd.it/m3wbbe2amo1e1.png?width=1315&amp;format=png&amp;auto=webp&amp;s=035da87010b8c52799ac2d531fab8a423da82628 提前谢谢您！    提交人    /u/lordgvp   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gu501w/rl_agents_with_the_game_dev_engine_godot/</guid>
      <pubDate>Mon, 18 Nov 2024 13:44:12 GMT</pubDate>
    </item>
    <item>
      <title>在 ML-Agents（Unity 3D）中努力使用 PPO 训练代理：需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtvxsv/struggling_to_train_an_agent_with_ppo_in_mlagents/</link>
      <description><![CDATA[      大家好！我在使用 Unity 3D 中的 ML-Agents 训练代理时遇到了麻烦。经过 50 个并行环境 8 多个小时的训练后，代理仍然无法逃离一个简单的房间。我想分享一些细节，并听听您对可能出现问题的建议。 场景描述 • 代理目标：在房间中导航，收集特定目标（物件），然后打开门逃脱。• 环境：• 房间有基本的障碍物和分散的物件。• 通过连续动作（移动和旋转）和离散动作（跳跃）控制代理。• 当代理访问几乎所有物件时，门会打开。  PPO 配置 • 批次大小：1024 • 缓冲区大小：10240 • 学习率：3.0e-4（线性衰减） • Epsilon：0.2 • Beta：5.0e-3 • Gamma（折扣）：0.99 • 时间范围：64 • 隐藏单元：128 • 层数：3 • 好奇心模块：已启用（强度：0.10）  观察 1. 训练期间的表现： • 代理探索房间，但似乎陷入了随机的运动模式。 • 它偶尔会达到一两个目标，但不会进一步逃脱。 2. 奖励和惩罚： • 奖励：达到目标可得 +1.0，几乎完成任务可得 +0.5。 • 惩罚：超出时间限制 -0.5，碰撞 -0.1，空闲 -0.0002。 • 我还为持续移动添加了一个小奖励（+0.01）。 3. 训练设置： • 我使用 50 个环境副本（num-envs：50）来最大化训练效率。 • 情节时间上限为 30 秒。 • 房间有随机生成点以防止过度拟合。  问题 1. 超参数：对于这种类型的问题，这些参数是否有任何不合适的？ 2. 奖励：奖励/惩罚系统是否会对学习过程产生偏差？ 3. 观察：代理是否会被不相关的信息（如射线投射或堆叠观察）淹没？ 4. 延长训练：我应该大幅增加训练步骤的数量，还是我遗漏了一些重要的东西？  如有任何帮助，我们将不胜感激！如果需要，我愿意测试参数调整或修改代码结构。提前致谢！    提交人    /u/Popular_Lunch_3244   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtvxsv/struggling_to_train_an_agent_with_ppo_in_mlagents/</guid>
      <pubDate>Mon, 18 Nov 2024 03:48:35 GMT</pubDate>
    </item>
    <item>
      <title>学习 RL 的资源？？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtt7pi/resources_for_learning_rl/</link>
      <description><![CDATA[您好，我想从头开始学习 RL。了解主要用于计算机视觉领域的深度神经网络。需要深入了解理论。我是硕士一年级学生。 如果可能，请列出理论资源，甚至从简单到复杂的模型编码。 感谢任何帮助。    提交人    /u/iInventor_0134   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtt7pi/resources_for_learning_rl/</guid>
      <pubDate>Mon, 18 Nov 2024 01:24:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么奖励规范化中的奖励在 RND 中以“相反方向”（向后）折扣？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtplc6/why_are_the_rewards_in_reward_normalisation/</link>
      <description><![CDATA[在随机网络蒸馏中，由于存在内在和外在奖励，奖励被标准化。然而，在 CleanRL 实现中，用于计算标准偏差的奖励（其本身用于标准化奖励）不会像往常一样打折。据我所知，打折的方向与通常的做法相反，我们希望远期奖励的折扣比现在奖励的折扣更大。就上下文而言，gymnasium 提供了一个 NormalizeReward 包装器，其中奖励也在“相反方向”折扣。 下面您可以看到，在 RND 的 CleanRL 实现中，奖励以正常顺序传递（即，不是从时间的最后一步到时间的第一步）。 curiosity_reward_per_env = np.array([discounted_reward.update(reward_per_step) for reward_per_step in wondering_rewards.cpu().data.numpy().T]) mean, std, count = (np.mean(curiosity_reward_per_env), np.std(curiosity_reward_per_env), len(curiosity_reward_per_env),) reward_rms.update_from_moments(mean, std**2, count) wondering_rewards /= np.sqrt(reward_rms.var)  下面您可以看到负责计算折扣奖励的类，然后使用这些奖励计算 CleanRL 中奖励正则化的标准差。 class RewardForwardFilter: def __init__(self, gamma): self.rewems = None self.gamma = gamma def update(self, rews): if self.rewems is None: self.rewems = rews else: self.rewems = self.rewems * self.gamma + rews return self.rewems  在GitHub 上，RND 论文的一位作者指出 &quot;需要注意的是，为了方便起见，我们沿时间向后而不是向前进行折现（这样很方便，因为在任何时候，过去都是完全可用的，而未来尚未到来）。&quot; 我的问题是，为什么我们可以使用在&quot;相反方向&quot; 折现的奖励的标准差？ （向后）使已经（或将要）折现的奖励正常化（即，我们希望未来的相同奖励价值低于现在的相同奖励）。 另请参阅：https://ai.stackexchange.com/questions/47243/rl-why-are-the-rewards-in-reward-normalisation-discounted-in-the-opposite-dire    提交人    /u/Glass_Artist7835   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtplc6/why_are_the_rewards_in_reward_normalisation/</guid>
      <pubDate>Sun, 17 Nov 2024 22:30:54 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 任务培训建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtlmgu/advice_for_training_on_mujoco_tasks/</link>
      <description><![CDATA[您好，我正在研究一种新的非策略深度强化学习优先级方案。 我从可靠的存储库中获得了 SAC 和 TD3 的 torch 实现。我使用原始 ER、PER 和我的方法在 Hopper-v5 和 Ant-v5 上进行实验。我在 3 个种子上运行实验。我训练 250k 或 500k 步以查看训练进展如何。我通过运行代理 10 集并平均每 2.5k 步奖励一次来进行评估。我使用与 SAC 和 TD3 的论文和官方实现相同的超参数。 我注意到评估分数中存在非常不规则的模式。这些曲线看起来不稳定，非常好的评估分数在某些步骤后突然下降。它多次上升和下降。这种不稳定的行为也存在于原始 ER 版本中。我从他们的官方存储库中获取了 TD3 和 SAC，因此我对这些评估分数感到困惑。这是正常的吗？在论文中，评估分数的行为更加单调。我应该为每个 Mujoco 任务搜索超参数吗？    提交人    /u/TheMefe   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtlmgu/advice_for_training_on_mujoco_tasks/</guid>
      <pubDate>Sun, 17 Nov 2024 19:36:01 GMT</pubDate>
    </item>
    <item>
      <title>DDQN 无法收敛，可能出现灾难性遗忘</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtihzc/ddqn_not_converging_with_possible_catastrophic/</link>
      <description><![CDATA[      我正在训练 DDQN 代理进行股票交易，从下面的损失可以看出，在前 30k 步中，损失正在减少，然后直到 450k 步，模型似乎不再收敛 https://preview.redd.it/yc6o9wynwh1e1.png?width=592&amp;format=png&amp;auto=webp&amp;s=aef476c7eb177f82fe112d0d1fd5a95ef90c6917 此外，从投资组合价值的进展情况可以看出，模型似乎忘记了每集所学的内容。 这些是我的超参数，请注意，我使用的是固定的情节长度= 50k 步，并且每个情节都从一个随机点开始  learning_rate=0.00001, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update=1000, buffer_capacity=20000, batch_size=128,  可能是什么问题以及如何解决它？    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtihzc/ddqn_not_converging_with_possible_catastrophic/</guid>
      <pubDate>Sun, 17 Nov 2024 17:19:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助需求：如何在不造成训练推理差距的情况下为 RLHF 中的每个标记分配奖励分数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gth0yi/helped_needs_how_to_assign_reward_scores_to_each/</link>
      <description><![CDATA[在 RLHF 中，我一直在努力解决如何有效地将奖励分数分配给各个标记的问题。 奖励模型通常使用成对比较进行训练，输出一个评估句子整体质量的单个标量。然而，在 RLHF 期间，为了训练价值函数（用于 PPO 等技术），我们需要计算累积奖励： $$Rt = \sum{t’=t&gt;T r(s{t’}, a{t’})$$ 这是我的主要问题： - 我们如何将这个句子级别的奖励分解为 token 级别的奖励？ 我正在考虑的一种简单方法是： - 直接将训练有素的线性层应用于每个 token 的隐藏状态以预测其奖励分数。 但是，我担心这可能会引发两个主要问题：1. 训练推理差距：奖励模型经过训练以评估整个句子，但这种逐个 token 的分解可能会与 RM 的原始训练设置不同。 2. 性能下降：推理过程中的奖励分布可能与真正的奖励信号不一致，从而可能损害策略优化。 我正在寻找社区的建议或见解： - 有没有更好的方法将句子级奖励分解为 token 级分数？ - 我们如何验证 token 奖励分解的有效性？ 我非常感谢任何想法或建议。谢谢！    提交人    /u/ForJadeForest   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gth0yi/helped_needs_how_to_assign_reward_scores_to_each/</guid>
      <pubDate>Sun, 17 Nov 2024 16:16:41 GMT</pubDate>
    </item>
    <item>
      <title>常规 RL 和 LORA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gtb77g/regular_rl_and_lora/</link>
      <description><![CDATA[是否有任何 GitHub 示例用于微调常规 ppo，例如使用 lora 解决简单的 rl 问题？就像从一个 Atari 游戏到另一个  编辑用例：假设您有一个问题，其中有很多初始条件，如速度、方向等……95% 的初始条件已得到解决，5% 无法解决（尽管它们是可解的）但是您很少遇到它因为它只有 5% 的“样本”所以现在你想更多地训练这 5%，并且在训练期间增加它的数量..并且你不想“忘记”或破坏以前的成功。（这主要针对开启策略而不是具有高级回复缓冲区的关闭策略）...    提交人    /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gtb77g/regular_rl_and_lora/</guid>
      <pubDate>Sun, 17 Nov 2024 11:00:11 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Lab 中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</link>
      <description><![CDATA[您好，我是在模拟中训练机器人的新手。我刚刚设置了我的 isaac 实验室，但我不知道如何在其中训练我自己的模型。关于它的文档也不多（我知道 NVidia 文档，但就是这样）。有人能为我提供更多关于如何入门的信息吗？另外，没有关于它的教程/视频/文档，因为它是新的还是不好的？它什么时候向公众开放的？谢谢！    提交人    /u/North_Set8162   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gszk3y/rl_in_isaac_lab/</guid>
      <pubDate>Sat, 16 Nov 2024 23:06:43 GMT</pubDate>
    </item>
    <item>
      <title>银行业有趣的研究课题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</link>
      <description><![CDATA[我目前是一名计算机科学 (ML 专业) 的兼职硕士生，在银行业担任数据工程师，我计划通过在银行业务场景中应用 RL 代理来撰写研究报告。我能想到一些事情，比如贷款决策或欺诈检测，但对我来说没有什么真正有趣的。有什么建议可以告诉我可以研究什么吗？我理想情况下想要一些我们有一些开源数据的东西。    提交人    /u/SatwikGu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsyuxu/interesting_research_topics_in_banking_industry/</guid>
      <pubDate>Sat, 16 Nov 2024 22:33:22 GMT</pubDate>
    </item>
    <item>
      <title>“可解释的对比蒙特卡洛树搜索推理”，Gao 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsxqpo/interpretable_contrastive_monte_carlo_tree_search/</guid>
      <pubDate>Sat, 16 Nov 2024 21:40:25 GMT</pubDate>
    </item>
    <item>
      <title>人的手臂</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</link>
      <description><![CDATA[你好。我想制作一个人体手臂模型，并使用强化学习使其达到目标。 我知道这很难实现（如果可能的话，大量的 DOF、较长的训练时间），所以我尝试使用简单的模型和完全增加的模型来构建它。 如果需要，我很乐意制作自己的 urdf 模型，但也很乐意使用已经存在的东西。 你会推荐从哪里开始？最好的算法是什么（可能是 PPO、SAC、DDPG）？最好的平台是什么（可能是 pybullet、MuJoCo、ROS 和 Gazebo）？ 任何帮助表示感谢。    提交人    /u/tedthemouse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsw1lc/human_arm/</guid>
      <pubDate>Sat, 16 Nov 2024 20:20:38 GMT</pubDate>
    </item>
    <item>
      <title>为研究论文编写方程式并组织人员</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</link>
      <description><![CDATA[大家好，我目前是强化学习和迁移学习领域的博士生。我正在准备写我的第一篇论文，写方程式及其证明、推导等感觉很不舒服。我想知道经验丰富的研究人员是如何做到的？他们使用什么样的工具？在整个项目中，他们如何不断写下所有这些数学符号和方程式，他们如何呈现它们、跟踪它们，并同时维护多个项目。对于工具，你们使用 iPad 之类的工具吗？我理解 overleaf 的用途，但我觉得亲手写它们更有成就感。你们能分享一下你们是如何用数学和代码等开发系统的吗？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gsro06/writing_equations_for_research_papers_and/</guid>
      <pubDate>Sat, 16 Nov 2024 17:01:50 GMT</pubDate>
    </item>
    </channel>
</rss>