<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 28 May 2024 21:14:36 GMT</lastBuildDate>
    <item>
      <title>反事实遗憾最小化（CFR）中的部分修剪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2p41o/partial_pruning_in_counterfactual_regret/</link>
      <description><![CDATA[我最近一直在重新实现一些 CFR 变体，并开始研究修剪。在 MCCFR 论文（Lanctot，2009）中提到 &quot;对于 vanilla CFR，我们使用了一种称为修剪的实现技巧来显着减少每次迭代的工作量。在更新一个玩家的遗憾时，如果另一个玩家没有达到当前历史的概率，则可以为当前迭代修剪该历史的整个子树，而不会对结果计算产生影响。&quot; （这种形式的修剪后来在 Noam Brown 的作品中称为&quot;部分修剪&quot;。） 但是，由于 CFR 在平均迭代中收敛到平衡，我想知道为什么这在我们不交替更新哪个玩家的情况下有效（因为我们仍然需要记录我们玩的策略）。在 pyspiel MCCFR 实现中，除非两个玩家的到达概率均为 0，否则似乎无法完成此操作（第 86 行，https://github.com/google-deepmind/open\_spiel/blob/7d3a355b4927acf4d21fd76e2ea799d9f5c0bb7a/open\_spiel/python/algorithms/external\_sampling\_mccfr.py) 谢谢！    由    /u/dieplstks  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2p41o/partial_pruning_in_counterfactual_regret/</guid>
      <pubDate>Tue, 28 May 2024 17:29:35 GMT</pubDate>
    </item>
    <item>
      <title>tensorboard 日志在 stable_baselines3 进度条之前就完成了</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2og8z/tensorboard_logs_finishes_wayyy_before_stable/</link>
      <description><![CDATA[大家好。我正在使用 tensorboard 通过 stable_baselines3 记录来自代理的训练运行。我注意到一些有趣的事情，但我不太明白。当我查看 tensorboard 时，它会在 stable_baselines 训练运行完成之前为我提供直到最后时间步的值。例如，假设我有 50,000 个训练时间步，我可以看到 50,000 个时间步的 tensorboard 输出指标，但当我切换回笔记本时，stable_baselines3 进度条只有 5-10%。如果我等待它完成，然后评估模型，它会给我与 tensorboard 完成时看到的结果相同的结果。这里还有其他事情发生吗？我不确定差异是什么。    提交人    /u/oldyoungin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2og8z/tensorboard_logs_finishes_wayyy_before_stable/</guid>
      <pubDate>Tue, 28 May 2024 17:02:11 GMT</pubDate>
    </item>
    <item>
      <title>LSTM-SAC 代码帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2o539/lstmsac_code_help/</link>
      <description><![CDATA[我一直在尝试使用 lstm 层对 SAC 进行编程，这样在 POMDP 中效果会更好。我见过在 BipedalWalker gym env 上测试的类似内容，所以我目前在那里测试它，因为我在该环境中有一个有效的 SAC 实现。我以为我做得正确，因为完整代码的第一次测试比我预期的要好，但在运行几次之后，似乎这纯属巧合，而且大多数时候它都没有学到任何东西。这是我尝试实现 LSTM 层和序列的方式的问题吗？还是我的超参数的问题？管理隐藏状态重要吗？ 我的超参数与我在正常 SAC 中使用的超参数相同。在执行 LSTM 时，我唯一改变的是确保我的维度是 [batch_size、sequnce_length、任何正常维度]，并且重放缓冲区可以处理序列。最初我的代码是所有代码的 [batch_size,normal dim]。例如，第一个动作的 batch_size 为 1，但对演员/评论家的所有其他调用的 batch_size 都与重播缓冲区中给定的 batch_size 匹配。 我希望最终在硬核双足步行者上使用它，但目前它在正常情况下无法持续工作。并不是说我的正常 sac 曾经学习过硬核，但我认为这是一个超参数问题，因为它们没有经过有目的地调整。任何建议或帮助都将不胜感激。 LSTM 代码：https://github.com/jhunter533/Machine-Learning/blob/main/RSACBiped.py 正常 SAC：https://github.com/jhunter533/Machine-Learning/blob/main/SACBiped.py    提交人    /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2o539/lstmsac_code_help/</guid>
      <pubDate>Tue, 28 May 2024 16:49:04 GMT</pubDate>
    </item>
    <item>
      <title>用于 RL 和 Pytorch 的库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2hhqn/library_to_use_for_rl_and_pytorch/</link>
      <description><![CDATA[大家好，我过去做过很多理论强化学习，从头开始实现了我的大多数算法（PPO、DQl 等），以理解它们并对其进行一些调整。但是，现在我需要强化学习来解决一个实际问题，我不想花太多时间实现每个算法。你会推荐哪些库与 Pytorch 实现良好的协同作用？ 就我个人而言，我对所有 Pytorch 库和子库都有很好的体验，这就是为什么我认为 TorchRL 在这里很有用。但是，由于它还处于起步阶段，资源有点稀缺。所以我不太确定是否有其他拥有更大社区的库在这里更受欢迎。    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2hhqn/library_to_use_for_rl_and_pytorch/</guid>
      <pubDate>Tue, 28 May 2024 11:46:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 学习图的色数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2fuzp/learning_chromatic_number_of_graphs_with_ppo/</link>
      <description><![CDATA[我正在使用 PPO+GNN 解决色数问题，其中代理必须学习用最少的颜色为图形着色。作为输入给出的颜色数量始终大于色数。假设图形的色数为 3，我们为代理提供了 10 种颜色，并希望它用 3 种颜色为图形着色。这就是正在发生的事情，它在很短的几步内学会了用 5 种颜色为图形着色，但它并没有从那里收敛到最佳解决方案。我该怎么做才能让它收敛到最佳解决方案？附注：最初参与者损失很高，一旦它收敛到次优解（5），它就会变得非常少。此外，该算法为 MIS 问题提供了最佳解决方案和良好的收敛性。    提交人    /u/Low-Advertising-1892   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2fuzp/learning_chromatic_number_of_graphs_with_ppo/</guid>
      <pubDate>Tue, 28 May 2024 10:04:14 GMT</pubDate>
    </item>
    <item>
      <title>帮助 SB3 包装器和日志记录</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2dzm1/help_with_sb3_wrappers_and_logging/</link>
      <description><![CDATA[嗨， 我收到此警告： UserWarning：警告：用于从其他包装器获取变量的 env.Instance_attribute 已被弃用，将在 v1.0 中删除，要获取此变量，您可以对环境变量执行 `env.unwrapped.Instance_attribute` 或执行 `env.get_wrapper_attr(&#39;Instance_attribute&#39;)`，它将搜索提醒的包装器。 class CustomCallback(BaseCallback): def __init__(self, verbose: int = 0): super().__init__(verbose) def _on_step(self) -&gt;; bool: Instance_attribute_1 = sum(self.training_env.get_attr(&quot;Instance_attribute&quot;)[0]) self.logger.record(&quot;Instance_attribute 1&quot;, Instance_attribute_1) return True  env 正在用 `Monitor` 包装器和 DummyVecEnv 包装。 有人知道如何消除这个警告吗？    提交人    /u/k0ldsoul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2dzm1/help_with_sb3_wrappers_and_logging/</guid>
      <pubDate>Tue, 28 May 2024 07:47:15 GMT</pubDate>
    </item>
    <item>
      <title>通过克罗内克积证明价值函数的梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2brn9/proof_of_gradient_of_value_function_via_kronecker/</link>
      <description><![CDATA[嗨，我对我在赵诗宇的《强化学习的数学基础》中找到的一个证明有疑问。  我把它发布在 stackexchange 上，因为我觉得格式化会更容易。    提交人    /u/jthat92   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2brn9/proof_of_gradient_of_value_function_via_kronecker/</guid>
      <pubDate>Tue, 28 May 2024 05:13:50 GMT</pubDate>
    </item>
    <item>
      <title>训练 DQN 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2bn3k/training_dqn_agent/</link>
      <description><![CDATA[嗨，我正在定制环境中工作，而对于该训练 DQN 代理，我没有得到。 有什么教程或线索吗？ 谢谢     提交人    /u/ComprehensiveOil566   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2bn3k/training_dqn_agent/</guid>
      <pubDate>Tue, 28 May 2024 05:05:57 GMT</pubDate>
    </item>
    <item>
      <title>v2 和 v4 MuJoCO 环境之间的差异 D4RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d278ah/difference_between_v2_and_v4_mujoco_environments/</link>
      <description><![CDATA[我想知道 D4RL 基准测试中提供的离线 RL MuJoCo-v2 数据集是否与 v3 和 v4 版本兼容。如果我最终想用这些数据集 (D4RL mujoco v2) 训练模型，然后使用相应的 v3 或 v4 版本在线测试模型，那么各个版本的奖励和环境动态是否仍然相同？谢谢    提交人    /u/Particular_Rip_6148   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d278ah/difference_between_v2_and_v4_mujoco_environments/</guid>
      <pubDate>Tue, 28 May 2024 00:58:42 GMT</pubDate>
    </item>
    <item>
      <title>在 RL 模型中集成 GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d24iht/integrating_gnns_in_rl_models/</link>
      <description><![CDATA[大家好， 我目前正在撰写我的学士论文，论文主题是优化动态变化的服务器网络。为了对网络结构进行建模，我使用了一个图，这使得在这里使用 GNN 非常诱人。虽然我在 GNN 和 RL 方面有着扎实的背景（主要是理论方面的，大多数算法都是从头开始编写的），但我从未将两者结合起来。所以我想知道您是否有将 GNN 与常见的 RL 库（如 stable-baseline）结合使用的经验？    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d24iht/integrating_gnns_in_rl_models/</guid>
      <pubDate>Mon, 27 May 2024 22:44:55 GMT</pubDate>
    </item>
    <item>
      <title>我进行的深度 Q 学习正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d23cek/am_i_doing_deep_qlearning_right/</link>
      <description><![CDATA[我正在尝试使用深度 q 学习解决 OpenAI Gym cartpole 问题，我刚刚从头开始编写了一些东西，但我不知道这样做是否正确。有谁更了解 RL 可以帮忙吗？这是我正在开发的脚本，它是一个“双重深度 q 学习”：https://github.com/rlucasfm/Q-learning-tf/blob/master/double_dqn_tf.py。 在同一个存储库中有一个简单的深度 Q 学习脚本。    提交人    /u/WitnessedWrath   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d23cek/am_i_doing_deep_qlearning_right/</guid>
      <pubDate>Mon, 27 May 2024 21:51:30 GMT</pubDate>
    </item>
    <item>
      <title>可变大小的状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1w9rn/variablesized_states/</link>
      <description><![CDATA[在我的项目中，我试图了解应用程序 UI 的状态。在执行任何给定操作后，应用程序都会向我提供有关 UI 中发生的一个或多个更改的数据（组件更改、添加/删除了哪些新组件等），我从中构建状态。 问题在于 - 对话框和组件的数量和身份在会话之间可能有很大差异，并且不知道可能会添加哪些组件。 有什么好办法可以解决这个问题？零填充并不实用，因为组件数量可能非常大，而且我们已经为每个组件提取了大量数据。此外，这意味着对应用程序的任何新更改都需要从头开始重新训练。    提交人    /u/CJIsABusta   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1w9rn/variablesized_states/</guid>
      <pubDate>Mon, 27 May 2024 16:59:58 GMT</pubDate>
    </item>
    <item>
      <title>我可以用 4 A30 进行对齐吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1njta/can_i_do_alignment_with_4_a30/</link>
      <description><![CDATA[实验室的主要重点是 RL，但我发现 rlhf 对我更有吸引力。 我手头有 4xA30 24G，可以研究 6B-8B LLM alignmnet 吗？ 任何回复都会有帮助。    提交人    /u/siyuan01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1njta/can_i_do_alignment_with_4_a30/</guid>
      <pubDate>Mon, 27 May 2024 09:21:49 GMT</pubDate>
    </item>
    <item>
      <title>如何确定重播缓冲区的最小大小</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</link>
      <description><![CDATA[我正在 Unity 中训练 dqn，它使用 [偏航、横向矢量和前向矢量] 作为我的操作空间来控制游戏对象。我读到人们在开始训练之前会为他们的重播缓冲区获得最少的经验数量（在某些帖子中，读到这个最小值高达 10000）。我发现我的模型基本上为每次推理都提供了相同的动作，因为它现在完全未经训练。在我看来，在开始训练之前收集大量这样的数据大部分是多余的，因为它并没有真正尝试各种动作。如果我的批量大小是 64，如果我想使用重播缓冲区（假设 256）的经验开始训练，有什么我应该注意的吗？我假设这将有助于我尽早增加训练数据的多样性。 我是个菜鸟，所以我非常愿意被推荐到可能与此相关的论文，提前谢谢。    提交人    /u/ProtectionFrosty5393   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</guid>
      <pubDate>Mon, 27 May 2024 04:55:29 GMT</pubDate>
    </item>
    <item>
      <title>我可以在电子商务网站中使用强化学习进行产品推荐吗？有没有关于这个主题的资源？或者有哪些其他最佳选择。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</link>
      <description><![CDATA[嗨，我是一家餐饮/电商服务提供商的实习生，我被指派为餐饮和电商网站创建推荐引擎，请给我推荐一些关于基于现实生活的推荐系统或任何其他有效方法的好指导材料，注意：数据有限，哪种方法可能是最好的？    提交人    /u/Educational-Town-710   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</guid>
      <pubDate>Mon, 27 May 2024 02:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>