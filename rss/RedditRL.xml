<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 12 May 2024 18:19:19 GMT</lastBuildDate>
    <item>
      <title>“SOPHON: Non-Fine-Tunable Learning to Retrain Task Transferability For Pre-trained Models”，Deng et al 2024（MAML 在微调时会导致目标任务的灾难性遗忘）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</guid>
      <pubDate>Sun, 12 May 2024 18:05:11 GMT</pubDate>
    </item>
    <item>
      <title>尝试寻找有关 q 学习的学习率和伽玛设置的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cq7dkk/trying_to_find_papers_on_learningrate_and_gamma/</link>
      <description><![CDATA[大家好。 我正在写关于 Q-learning 的学校期末论文。简而言之，我的项目基于 Netlogo 中的 99x99 环境，该环境具有包含七种类型地面类型（人行道、草地等）的网格。每种地面类型都有不同的奖励。该特工被设置为跨越 16 个不同的地点，并在连续 10 集稳定时收敛。我对 q-learning 参数的设置是learning-rate = 0,9 和 gamma = 1.0。 qlearning 收敛在 6500-8000 集左右。情节的定义是到达目标位置或击中建筑物/障碍物并开始新的情节。当代理收敛并找到最佳路线时，它会更新我尝试过这些值（0-30）的该路径的奖励。因此，当下一个代理启动时，一些补丁已从上一个代理更新。我对 100 个代理运行此程序以找到最佳路径。当所有 100 个智能体都找到最佳路径时，我会为路径着色，并与现实生活中的环境足迹观察结果进行比较来评估路径。环境基于真实位置，并且该项目基于收集这些足迹值的先前工作。 如果我没记错的话，当我与老师交谈时，这些高参数设置的原因是因为算法搜索的空间很大。 但是我需要一个关于为什么我选择这些设置的来源，你们有任何论文或可以推荐这些设置的东西吗？ 感谢您的帮助   由   提交 /u/TheAmitySloth   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cq7dkk/trying_to_find_papers_on_learningrate_and_gamma/</guid>
      <pubDate>Sun, 12 May 2024 13:02:14 GMT</pubDate>
    </item>
    <item>
      <title>多轿厢电梯（Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cq6uaf/multi_car_elevator_python/</link>
      <description><![CDATA[大家好， 我正在尝试查找最新的模拟器（Python 版本）和/或论文关于多轿厢电梯系统（基本上一个电梯井中有多辆轿厢）。有谁知道模拟器吗？或者有什么想法可以搜索吗？   由   提交/u/emreyavas20   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cq6uaf/multi_car_elevator_python/</guid>
      <pubDate>Sun, 12 May 2024 12:33:03 GMT</pubDate>
    </item>
    <item>
      <title>蛇 DQN 绕圈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpzz6e/snake_dqn_going_in_circles/</link>
      <description><![CDATA[我正在尝试实现 DQN 来玩贪吃蛇。目前，智能体学习了一些知识，例如如何避免撞到墙上，并且通常能够吃一两个水果，但随后就会陷入原地打转。我将网络状态表示为两个堆叠的框架，每个框架都是 20x20 的网格，其中 -1 标记蛇的头部，-0.5 标记身体，1 标记水果位置。使用两个连续帧背后的想法是确保代理知道蛇要去哪个方向。 奖励结构如下：  -1 表示击中蛇墙壁或撞到自己的身体 吃水果为1 每离开水果一步-0.05（试图阻止它绕圈） &gt; 朝着水果每走一步 0.025  我还限制了剧集长度，以避免蛇最终绕圈的情况。 这是我的神经网络的结构： class Net(nn.Module): def __init__(self, dim, action_size): super(Net, self).__init__() self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, 偏差=False) self.conv2 = nn.Conv2d(4, 16, kernel_size=8, stride=4) self.flatten = 16 * 1 * 1 self.fc1 = nn.Linear(self.flatten, 512) self.fc2 = nn.Linear(512, action_size) defforward(self, x): x = x.type(torch.FloatTensor) x = self.conv1( x) x = F.relu(x) x = self.conv2(x) x = F.relu(x) x = x.view(-1, self.flatten) x = self.fc1(x) x = F .relu(x) x = self.fc2(x) return x  这是我的训练代码： epsilon_start = 0.99 epsilon_final = 0.01 epsilon_decay = 0.0001 lr = 1e-4 batch_size = 32 max_timesteps = int(1e8) learn_every = 4 update_every = 2 Episode_reward = 0 curr_episode = 0 save_every = 10 moving_average_length = 25 max_episode_length = 800 game = Game() Brain = Brain(dim = game.dim , action_size = 4) prev_frame = torch.tensor(game.get_frame()) game.step(game.DOWN) curr_frame = torch.tensor(game.get_frame()) def get_epsilon(step): return epsilon_final + (epsilon_start - epsilon_final ) * np.exp(-epsilon_decay * step) moving_average = deque(maxlen = moving_average_length)starting_timestep = 0 for timestep in range(max_timesteps): state = torch.stack([prev_frame, curr_frame]) action = 0 epsilon = get_epsilon(timestep) ) if(ε &lt; random.uniform(0, 1)): action = Brain.pick_action(state) else: action = random.randint(0, 3) (奖励，终端) = game.step(action) game.render() Episode_reward +=奖励 next_frame = torch.tensor(game.get_frame()) next_state = torch.stack([curr_frame, next_frame]) Brain.add_to_memory(状态, 动作, 奖励, next_state, 终端) prev_frame = curr_frame curr_frame = next_frame if(timestep % learn_every == 0): Brain.learn() if(timestep % update_every == 0): Brain.update() if(terminal or timestep -starting_timestep &gt;= max_episode_length): moving_average.append(len(game.snake)) debug = &#39;剧集&#39; + str(curr_episode) + &#39; 蛇长度 &#39; + str(len(game.snake)) if(len(moving_average) &gt;= moving_average_length): debug += &#39; 移动平均 &#39; + str(float(sum( moving_average)) / float(len(moving_average))) print(debug) curr_episode += 1 if(curr_episode % save_every == 0): Brain.save_model(os.path.join(os.path.dirname(__file__), &#39; model&#39;)) Episode_reward = 0 game = Game() prev_frame = torch.tensor(game.get_frame()) game.step(game.DOWN) curr_frame = torch.tensor(game.get_frame())starting_timestep = timestep + 1 Brain .save_model(os.path.join(os.path.dirname(__file__), &#39;model&#39;))  如有任何建议，我们将不胜感激！   由   提交/u/xxgetrektxx2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpzz6e/snake_dqn_going_in_circles/</guid>
      <pubDate>Sun, 12 May 2024 04:50:08 GMT</pubDate>
    </item>
    <item>
      <title>“记忆的持久性和短暂性”，Richards & Frankland 2017</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpxen3/the_persistence_and_transience_of_memory_richards/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpxen3/the_persistence_and_transience_of_memory_richards/</guid>
      <pubDate>Sun, 12 May 2024 02:18:47 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗方法解决赌徒问题没有给出最优解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpwtxe/monte_carlo_method_for_gamblers_problem_not/</link>
      <description><![CDATA[     &lt; /td&gt; 大家好，我最近开始学习强化学习，并尝试用赌徒问题实现一个简单的蒙特卡罗学习算法。我用 python 编写了一些代码，但经过 100000000 次迭代后得出的解决方案是这样的： 从 0 到 99 的各个资本级别的最佳下注大小 我的代码是这样的： M = 100000000 alpha = 0.1 # 假设 H = 1, T = 0 def Flip_coin() -&gt; int: return random.choices([0, 1], Weights=[0.6, 0.4], k=1)[0] def policy(Q, epsilon, Capital : int) -&gt; int: &#39;&#39;&#39; :param Capital: 玩家目前拥有的资本等级 :return: 赌注大小 &#39;&#39;&#39; gredy_action = Q[ Capital, : ].argmax() do_greedy = random.choices([0, 1], Weights= [epsilon, 1 - epsilon], k=1)[0] if do_greedy: 返回greedy_action else: return random.randint(0,capital) def play_game(Q, epsilon): &#39;&#39;&#39; :return: 包含状态的列表列表、行动（投注）和最终奖励（100 或 0） &#39;&#39;&#39; 历史 = [] 资本 = 50 而资本 != 0 且资本 &lt; 100：断言资本&lt; 100 断言资本 != 0 赌注 = 策略(Q, epsilon, 资本) coin = Flip_coin() History.append([资本,赌注]) if coin == 1: Capital += bet elif coin == 0: Capital -=下注 如果大写 == 0: 返回 [历史记录, 大写] 如果大写 &gt;= 100: 返回 [历史记录, 100] def mc_control(track_history=True): &#39;&#39;&#39; :param track_history: :return: Q, Q_hist &#39;&#39;&#39; Q = np.zeros((100,100)) # 索引 = 资本，列 = 下注大小 Q_hist = [] t = 0 for m in tqdm(range(M + 1)): t += 1 epsilon = max(0.1, 1 - 1e-5 * t) # 线性衰减 epsilon 历史，reward = play_game(Q,epsilon) if track_history: Q_hist.append(Q) for state_action in History: Capital = state_action[0] bet = state_action[1] Q[capital , 赌注] = Q[资本, 赌注] + alpha * (奖励 - Q[资本, 赌注]) 返回 Q, Q_hist     ;由   提交 /u/yuriIsLifeFuckYou   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpwtxe/monte_carlo_method_for_gamblers_problem_not/</guid>
      <pubDate>Sun, 12 May 2024 01:46:38 GMT</pubDate>
    </item>
    <item>
      <title>SAC 代码帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cptxaz/sac_code_help/</link>
      <description><![CDATA[我一直在尝试实现 SAC 以在 openAI 的体育馆中使用。我有一个适用于钟摆环境的 DDPG 算法，但我的 SAC 实现永远不会学习。我怀疑问题出在我的某个地方的逻辑上。从打印的大部分步骤来看，我的 q 末尾充满了 nan，但我不明白为什么。如果有人可以提供帮助，我们将不胜感激。 编辑：删除代码块。添加github链接。 https://github.com/jhunter533/Machine-Learning/blob/main/TorchSAC.py&lt; /a&gt; 该项目位于一个更大的存储库中，但这是唯一适用的文件。也可以随意忽略测试、保存、加载函数，自从几次迭代之前我就没有修复它们，它们不是问题我正在尝试修复 rn。   由   提交 /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cptxaz/sac_code_help/</guid>
      <pubDate>Sat, 11 May 2024 23:15:24 GMT</pubDate>
    </item>
    <item>
      <title>“通过海马脑机接口自愿激活远程地点表征”，Lai 等人，2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cprwor/volitional_activation_of_remote_place/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cprwor/volitional_activation_of_remote_place/</guid>
      <pubDate>Sat, 11 May 2024 21:38:55 GMT</pubDate>
    </item>
    <item>
      <title>寻找张量板日志参数及其含义的引用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpqwjw/looking_for_citations_for_tensorboard_log/</link>
      <description><![CDATA[我正在写我的论文，想添加一章来说明我将如何判断学习策略的性能。我主要关注奖励、熵系数和损失。尽管它很简单，但有没有我可以引用的任何论文或官方来源。   由   提交 /u/pvmodayil   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpqwjw/looking_for_citations_for_tensorboard_log/</guid>
      <pubDate>Sat, 11 May 2024 20:51:14 GMT</pubDate>
    </item>
    <item>
      <title>连续行动空间：固定/计划与学习与预测标准偏差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpq4a2/continuous_action_space_fixedscheduled_vs_learned/</link>
      <description><![CDATA[据我所知，有 3 种方法可以在连续动作空间设置中设置动作分布的标准差：  固定/预定的 std，在训练开始时设置为超参数 可学习的参数张量，其初始值可以设置为超参数。 SB3 使用此方法 https://github.com/ DLR-RM/stable-baselines3/blob/285e01f64aa8ba4bd15aa339c45876d56ed0c3b4/stable_baselines3/common/distributions.py#L150 标准也是“预测”的网络就像动作的平均值  在什么情况下你会使用哪种方法？  方法 2 和方法 2 3 对我来说似乎有点危险，因为优化器可能会将 std 设置为非常低的值，从而阻碍探索并且基本上“过度拟合”。到目前的政策。但由于 SB3 使用方法 2，情况似乎并非如此。 感谢您提供任何见解！   由   提交 /u/TheBrn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpq4a2/continuous_action_space_fixedscheduled_vs_learned/</guid>
      <pubDate>Sat, 11 May 2024 20:13:53 GMT</pubDate>
    </item>
    <item>
      <title>tmrl 的多代理实现（Track Mania RL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpnxg2/multi_agent_implementation_for_tmrl_track_mania_rl/</link>
      <description><![CDATA[我有兴趣为 TrackMania RL (TMRL) 开发多代理系统 (https://github.com/trackmania-rl/tmrl）。有谁知道与 TMRL 相关的现有多代理实现或文档吗？ Git 存储库将不胜感激。如果没有，您能否分享一些关于如何启动此流程的见解或资源，特别是修改多个代理的 TMRL 及其交互策略？谢谢！   由   提交 /u/msquaresproperty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpnxg2/multi_agent_implementation_for_tmrl_track_mania_rl/</guid>
      <pubDate>Sat, 11 May 2024 18:31:25 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用强化学习来代替穷举搜索以获得更好的时间效率吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpnd7r/can_i_use_reinforcement_learning_to_replace/</link>
      <description><![CDATA[我对 RL 非常陌生，想知道是否可以使用它来解决这个特定问题。  我有一个 53 个点的列表，每个点都有相关的损失，然后我为这些点生成一个 n 组合列表。这个想法是，例如，我采用这些 (53C3) 的 3 个组合（大约 23,000 个组合），并更改原始 53 中的这些值，并评估每个组合的损失。所以我现在有一个大约 23,000 个项目的列表，每个项目都有一个相关的损失，我只想找到损失最高的组合。 到目前为止，我已经能够使用详尽的方法来做到这一点搜索。但由于我通过神经网络对每个组合进行推理，因此需要大约 25 分钟来迭代每个组合以找到损失最高的组合。穷举搜索的代码如下： attack_combinations = Combinations(X_train_filtered.columns, num_attacked_ap) Attack_combinations_list = [] for Attack in Attack_combinations: Attack_combinations_list.append(attack) with open(&#39;noise -high.txt&#39;) as file: file_contents = file.read() const_noise = np.fromstring(file_contents, sep=&#39; &#39;) aps_loss = {} best_loss = 0 best_combination = 0 for Attack_combination in tqdm(attack_combinations_list): # 应用攻击to the dataset X_val_attacked = deepcopy(X_val_filtered) for col_name in X_val_attacked.columns: if col_name in Attack_combination: Noise = const_noise X_val_attacked[col_name] += Noise # 在被攻击数据集上评估模型 loss, precision, mse = model.evaluate(X_val_attacked , y_val_filtered, verbose=0) aps_loss[tuple(attack_combination)] = mse 如果 mse &gt; best_loss: best_loss = mse best_combination = Attack_combination print(&quot;最佳组合：&quot;, best_combination) print(f&quot;{num_attacked_ap}-组合的最高 DL MSE:&quot;, best_loss)  但是，我想尝试更大的 n 组合，在这种情况下，组合的数量会增加到数百万，从而使穷举搜索变得不可行。因此，我想知道 RL 是否可以在这里使用以及如何使用。我很高兴能指出正确的方向，提前谢谢您。   由   提交/u/CapedCrusader10   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpnd7r/can_i_use_reinforcement_learning_to_replace/</guid>
      <pubDate>Sat, 11 May 2024 18:04:58 GMT</pubDate>
    </item>
    <item>
      <title>关于如何在 Metaworld 中训练 pickplacev2 任务的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpl5e6/questions_about_how_to_train_pickplacev2_task_in/</link>
      <description><![CDATA[嗨， 有人在 pickplacev2 MetaWorld 中的任务？我尝试使用自己的实现以及使用 Stable Baselines3 等库来训练代理，但代理无法有效学习，甚至难以达到目标。我的代码看起来是正确的，因为它可以很好地处理其他任务，例如 reachv2、windowclosev2 和其他几个任务。 有趣的是，我已经成功训练了 DDPG代理通过采用特定技术来完成此任务。我认为失败有两个原因。首先是pickplacev2的奖励函数没有设计。 pickplacev2 中到达阶段的奖励信号非常弱，在初始位置通常小于 0.03，这可能会增加难度。这令人困惑，因为该论文提出了一种改进的奖励设计。其次，探索对于这项任务来说似乎很重要。对于 DDPG，修改噪声尺度至关重要，这表明探索对于这项任务很重要 如果您取得了成功，您能否分享有关特定配置、超参数或有助于成功训练的修改的见解或技巧？    由   提交/u/DF_13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpl5e6/questions_about_how_to_train_pickplacev2_task_in/</guid>
      <pubDate>Sat, 11 May 2024 16:21:39 GMT</pubDate>
    </item>
    <item>
      <title>这是一个询问基于理论/数学的强化学习科目途径的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpkog7/here_is_a_question_asking_for_a_theorymathbased/</link>
      <description><![CDATA[这是一个询问机器学习或强化学习科目的基于理论/数学的途径的问题： 您可以吗概述涵盖机器学习和强化学习的理论和数学基础的课程或课程路径？我正在寻找深入研究这些领域背后的理论基础、概率模型、优化技术和数学分析的主题，而不仅仅是关注应用算法或编码实现。目标是在转向更实际的应用之前建立强大的理论掌握。   由   提交/u/Background_Bowler236   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpkog7/here_is_a_question_asking_for_a_theorymathbased/</guid>
      <pubDate>Sat, 11 May 2024 16:00:36 GMT</pubDate>
    </item>
    <item>
      <title>对 RL 库/项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</link>
      <description><![CDATA[嗨， 我目前正在休学一年，致力于自学和强化学习项目。我将于 9 月开始攻读硕士学位，目标是准备 RL 博士学位。 由于我还有 4 个月的时间，我正在寻找其他项目来改善我即将到来的简历博士申请。 我考虑过开发一个可用于研究的库，但很难选择一个特定的主题。以下是我目前的指导方针：  框架：最好是 JAX，因为它最近越来越受欢迎，而且生态系统仍在扩展。我认为有机会从更成熟的框架（例如 Pytorch）中调整现有库，或者为 JAX 中缺乏合适软件的特定 RL 问题提供便捷的解决方案。 -  领域：我对开放性（课程学习、无监督环境设计）和多代理设置特别感兴趣。然而，已经有一些流行的库涵盖了这些领域的大量用例（Minimax、JaxMARL、JaxUED...）。我最终想在未来更深入地研究元和进化强化学习等其他领域。据我所知，进化强化学习也有一些不错的库（evojax、evosax 等），但我对元强化学习不太确定。 时间范围 ：大约 5 个月  我对社区对以下问题的见解很感兴趣：  是否有特定的 RL 领域缺乏有用的库贾克斯？ （在我引用的或其他人中） 如果没有，为现有库做出贡献会是更好的主意吗？ 是否更需要高效的环境和基准？   感谢您的帮助！   由   提交 /u/OptimalBandicoot1671   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</guid>
      <pubDate>Sat, 11 May 2024 15:11:46 GMT</pubDate>
    </item>
    </channel>
</rss>