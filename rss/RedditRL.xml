<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 21 Jul 2024 01:11:32 GMT</lastBuildDate>
    <item>
      <title>我的 PPO 代理出现奇怪的周期性峰值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</guid>
      <pubDate>Sat, 20 Jul 2024 23:02:51 GMT</pubDate>
    </item>
    <item>
      <title>DQN 高估问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</link>
      <description><![CDATA[      我一直在尝试从头开始实现 DQN，并创建了一个测试环境来展示我遇到的问题。环境在每一步奖励 1，并在 100 步后终止。 DQN 不采取任何行动，仅尝试根据步骤号预测每一步的累积奖励。 问题是，DQN 为后续状态输出极高的 Q 值，这与实际值相反，并且需要几个时期才能开始输出半准确值。 https://preview.redd.it/n1fd4g054jdd1.png?width=915&amp;format=png&amp;auto=webp&amp;s=5d32fa33a3ee048528b39b61239d26b9dfaf0d05 代码（根据用户 dieplstks 的评论略作修改）： import torch from torch import nn, optim import matplotlib.pyplot 作为 plt 导入随机 导入 numpy 作为 np 比例 = 100 折扣 = .9 奖励 = [1 for i in range(scale-1)] + [0] net = nn.Sequential( nn.Linear(4,32), nn.Mish(), nn.Linear(32,32), nn.Mish(), nn.Linear(32,1)) opt = optim.Adam(net.parameters()，lr=1e-2) loss = nn.MSELoss() def step(c): global discount, rewards, net, opt opt.zero_grad() qvals = [] for i in range(scale): inp = torch.tensor([[i/20 for _ in range(4)]], dtype=torch.float32) q = net(inp) qvals.append(q) if i == scale-1: next_q = torch.zeros(1,1) else: next_inp = torch.tensor([[(i+1)/20 for _ in range(4)]], dtype=torch.float32) next_q = net(next_inp).detach() target = discount*next_q + rewards[i] q_loss = loss(q, target) #opt.zero_grad() q_loss.backward() nn.utils.clip_grad_norm_(net.parameters(), 1) opt.step() if c%1 == 0: plt.plot(list(range(scale)), actual_vals) plt.plot(list(range(scale)), torch.concat(qvals).detach().numpy()) plt.savefig(f&#39;zoo{c}.png&#39;) plt.clf() #plt.show() actual_vals = np.zeros((scale),dtype=np.float32) next_val = 0 for i in reversed(range(scale)): current_val = next_val * discount + rewards[i] actual_vals[i] = current_val next_val = current_val for i in range(100): step(i)     提交人    /u/AUser213   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</guid>
      <pubDate>Fri, 19 Jul 2024 19:51:56 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习用于推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</link>
      <description><![CDATA[嗨：我正在寻找合适的 Python（最好是基于 PyTorch）深度强化学习库来实现多会话对话推荐引擎。理想情况下，强化学习将无模型且脱离策略，并且必须与数据库系统交互以获取用户偏好和其他上下文数据。有什么建议吗？    提交人    /u/Extra_Reflection9056   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</guid>
      <pubDate>Fri, 19 Jul 2024 08:00:25 GMT</pubDate>
    </item>
    <item>
      <title>通过将实时屏幕截图作为输入并预测要模拟的 Windows 鼠标/键盘输入，训练 DQN 代理来玩自定义 Fortnite 地图。以下是可视化的卷积过滤器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</link>
      <description><![CDATA[        提交人    /u/voidupdate   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</guid>
      <pubDate>Fri, 19 Jul 2024 01:35:34 GMT</pubDate>
    </item>
    <item>
      <title>RL 教科书包含逆向强化学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</link>
      <description><![CDATA[寻找包含逆强化学习 (IRL) 部分的 RL 教科书。我只熟悉 Dixon 的《金融机器学习》一书，这本书很棒，但我还在寻找更多值得阅读的内容。  任何建议都值得赞赏！此外，如果您知道任何带有相应 github 的 IRL 论文，我也会喜欢的。    提交人    /u/Voltimeters   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</guid>
      <pubDate>Fri, 19 Jul 2024 00:28:38 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 openAI Gym 中自定义环境的平等约束问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</link>
      <description><![CDATA[您好， 我正在为优化主题创建自定义环境。以下是一些详细信息： 代理观察一个连续变量：a 代理采取两个操作：b 和 c 我希望我的代理学习采取操作 b、c，使得 b + c = a 且 c 最大。 我知道这很简单。在这种情况下，代理应该采取操作 c，使得 c=a，但这只是我整个环境的一个小组成部分。 * 如何以尊重此约束的方式对环境进行建模（应始终尊重约束） * 如何对奖励进行建模。我试图将奖励作为 c，但由于这是一个绝对值，因此代理不会改善行为。 * 知道我的观察和行动都是连续变量，哪种类型的算法最适合这类问题。 提前谢谢您。    提交人    /u/Effective_Farm_4844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</guid>
      <pubDate>Thu, 18 Jul 2024 18:11:33 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 更新了，有什么不同</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</link>
      <description><![CDATA[      DreamerV3 最近已更新。论文中有一些变化。不幸的是，我找不到与 2023 版相比发生了哪些变化的表格。我注意到了一些变化。例如，动态损失权重从 0.5 变为 1。评论家使用真实转换来计算重放损失。优化器已经改变，他们使用自动梯度标准剪辑。我想知道是否有人注意到其他重大变化。如果有人有更改表并愿意分享就太好了！ https://preview.redd.it/r94zls5159dd1.png?width=640&amp;format=png&amp;auto=webp&amp;s=babb912867b877dd94ed98f5de6b52ddb46a1f3a    提交人    /u/yulinzxc   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</guid>
      <pubDate>Thu, 18 Jul 2024 10:14:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</link>
      <description><![CDATA[您好， 我正在学习 Richard Sutton 的书，对强化学习有了一些了解，也将其应用于一些项目。 我想写一篇研究论文：希望申请研究型硕士学位。 你们有什么建议吗？我应该考虑哪些事情？你们如何进行研究？ 谢谢！    提交人    /u/Original_Phrase1902   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 18 Jul 2024 09:55:25 GMT</pubDate>
    </item>
    <item>
      <title>设计具有多维行动空间的 PPO AC 框架时遇到的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e65l1b/an_issue_in_designing_an_ac_framework_with_a/</link>
      <description><![CDATA[我正在使用 PPO 编写自己的环境，其中涉及多维动作空间。在适配 PyTorch 框架时，我在更新策略网络的批量采样训练过程中遇到了问题。由于我正在处理多维数组，pi 比率的样本大小与优势函数的大小不同，因此无法将它们相乘以计算总损失函数。该如何解决？ 如： for _ in range(self.K_epochs): for index in BatchSampler(SubsetRandomSampler(range(self.batch_size)), self.mini_batch_size, False): action_mean_now = self.actor(s[index]) dist_now = Categorical(probs=action_mean_now) dist_entropy = dist_now.entropy().view(-1, 1) a_logprob_now = dist_now.log_prob(a[index].squeeze()).view(-1, 1) a_logprob = a_logprob[index].view(-1, 1) ratios = torch.exp(a_logprob_now - a_logprob) surr1 = ratios * adv[index] surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * adv[index] actor_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy  发生这种情况： surr1 = ratios * adv[index] RuntimeError：张量 a 的大小（128）必须与非单例维度 0 处的张量 b 的大小（64）匹配  如何确保策略可以学习多维动作空间的特征，同时避免与优势函数发生大小冲突？    提交人    /u/VermicelliBrave1931   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e65l1b/an_issue_in_designing_an_ac_framework_with_a/</guid>
      <pubDate>Thu, 18 Jul 2024 07:32:48 GMT</pubDate>
    </item>
    <item>
      <title>有人在 RL 中实现过 OGD（正交梯度下降）吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e63o8r/has_anyone_implemented_ogd_orthogonal_gradient/</link>
      <description><![CDATA[我正在通过持续强化学习研究机器人手臂操作，我花了几周时间在现实生活中实现 OGD。但似乎 OGD 在现实生活中不起作用。我认为这是因为与监督学习任务相比，现实生活中的任务太复杂了。有人成功实现过 OGD 吗？ 对于那些还没有听说过 OGD 的人，这里有一个链接：https://arxiv.org/pdf/1910.07104    提交人    /u/ContestOk7604   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e63o8r/has_anyone_implemented_ogd_orthogonal_gradient/</guid>
      <pubDate>Thu, 18 Jul 2024 05:24:56 GMT</pubDate>
    </item>
    <item>
      <title>用于混合 MDP 和腿部运动任务的 PPO/SAC 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e5ivvt/pposac_implementation_for_hybrid_mdp_and_for/</link>
      <description><![CDATA[大家好， 我对强化学习领域还很陌生，我很乐意与该领域更有经验的人讨论我在强化学习任务中遇到的问题。 我在这里长话短说，但基本上，我试图以分层方式将模型预测控制与强化学习结合起来。在我看来，MPC 更多地充当参考跟踪等内容的本地控制器，而代理则负责更复杂的推理。具体来说，我的目标是运动任务，目前针对四足机器人。 MPC 允许跟踪扭曲参考并公开一些参数以在运行时选择接触阶段。 现在，正如您所想象的，我的动作空间自然是连续-离散混合，其中代理必须输出扭曲命令和 4 个离散（现在为布尔值）变量来选择是否要步进。 我现在处理这个混合空间的方式是天真地根本不处理它。我正在使用 PPO 和 SAC 的一些自定义实现，并对连续动作空间进行微小修改；然后简单地在环境级别，我根据步进（连续）动作变量的阈值选择是否要步进。 到目前为止，我在训练这种任务时遇到了困难。我开始使用四处走动的机器人获得一些勉强不错的结果，但我觉得这种近似离散动作的方式可能会大大减慢/阻碍收敛。此外，由于某种原因，SAC 比 PPO 困难得多（我已经使用 mujoco 环境测试了我的两个自定义实现，它们都按照 SoA 执行）。 您对此事有何看法？我的方法是否存在我忽略的固有问题？您是否知道我可以测试的混合 MDP 的某些 PPO/SAC/both 实现？    提交人    /u/Majestic-Product1179   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e5ivvt/pposac_implementation_for_hybrid_mdp_and_for/</guid>
      <pubDate>Wed, 17 Jul 2024 13:59:05 GMT</pubDate>
    </item>
    <item>
      <title>观察重要吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e56cwq/does_observation_matter/</link>
      <description><![CDATA[在设计观测时，我认为提供用于奖励函数的状态的完整信息比提供部分信息更好。 但在阅读一些论文时，我发现情况并非总是如此。例如，机器人底座的高度没有作为输入输入到神经网络，但在这篇论文 [1] 中它被用于跟踪高度奖励函数。相反，机器人底座的旋转速度用于观测，但在另一篇论文 [2] 中从未在任何奖励函数中使用过。 那么，设计观测空间的正确方法是什么？或者有“好”的方法吗？ 参考文献： [1] Li, Zhongyu, et al. &quot;Robust and versatile bipedal jump control through reinforcement learning.&quot; arXiv preprint arXiv:2302.09450 (2023)。 [2] Siekmann, Jonah 等人。“通过周期性奖励组合实现所有常见双足步态的模拟到真实学习。”2021 年 IEEE 国际机器人与自动化会议 (ICRA)。IEEE，2021 年。    提交人    /u/Open-Safety-1585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e56cwq/does_observation_matter/</guid>
      <pubDate>Wed, 17 Jul 2024 01:56:50 GMT</pubDate>
    </item>
    <item>
      <title>我的神经进化代理陷入了循环！我错过了什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e4wm1y/my_neuroevolution_agent_is_stuck_in_a_loop_what/</link>
      <description><![CDATA[大家好， 我一直在学习神经进化，灵感来自 The Coding Train 的播放列表，并根据他们的玩具神经网络教程构建了一个 Python 库。为了测试我的理解，我创建了一个简单的 Pygame 环境，其中代理会导航基于网格的地图以到达随机放置的目标板。 目前，我的神经网络设置涉及 4 个输入和 4 个输出。输入是代理和目标板的标准化坐标（例如，[agent_x_norm、agent_y_norm、plate_x_norm、plate_y_norm]）。输入这些输入后，我得到了 [0.482、0.209、0.705、0.791] 之类的输出，它们对应于向上、向下、向左或向右移动等动作。 我尚未实现突变或其他进化机制。我首先测试了前馈机制是否按预期工作。但是，我注意到在单个游戏会话中，代理倾向于重复选择相同的动作。例如，如果输出建议向右移动，代理就会向右移动，但下一个状态只会发生轻微变化，因为只有代理的 x 坐标会发生变化。新输入可能看起来像 [new_agent_x_norm、agent_y_norm、plate_x_norm、plate_y_norm]，而 new_agent_x_norm 只会发生轻微变化。 由于只有一个输入值略有变化，神经网络的输出也会略有变化，从而产生一组非常相似的输出。因此，最高输出保持不变，导致代理不断重复向同一方向（例如向右）移动。 这种行为与我在 The Coding Train 视频中观察到的情况形成鲜明对比，在视频中，输出随时间的变化更大。我想知道我是否忽略了什么，或者我的方法是否犯了错误。 如果您能提供任何关于如何在开发的早期阶段为我的代理行为引入更多变化和探索性的见解或建议，我将不胜感激。提前感谢你的帮助！    提交人    /u/PricePretty4971   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e4wm1y/my_neuroevolution_agent_is_stuck_in_a_loop_what/</guid>
      <pubDate>Tue, 16 Jul 2024 18:52:32 GMT</pubDate>
    </item>
    <item>
      <title>完成多智能体强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e4nsn5/completed_multiagent_reinforcement_learning/</link>
      <description><![CDATA[我潜伏在这个 subreddit 一段时间了，时不时地，我会看到一些想要开始 MARL 项目的人发帖。这些人中很多人都是这个领域的新手，尽管难度非常大，但他们（可以理解）还是想在这个最令人兴奋的子领域之一工作。话虽如此，但在最初阶段之后，我并没有看到太多关于它的讨论。 在我自己的工作中，我发现了几十个库，其中一些有自己的出版物，但在 Github 上查找它们，发现使用它们的（公共）存储库相对较少，尽管它们的星级很高。入门活动和已完成项目的数量之间似乎出现了惊人的下降，甚至比其他热门领域（如生成建模）的下降幅度更大。我知道这是一个有点不合常规的问题，但是，在尝试过 MARL 的人中，你们的情况怎么样？您是否有任何想要分享的项目，无论是作为存储库还是战争故事？    提交人    /u/Efficient_Star_1336   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e4nsn5/completed_multiagent_reinforcement_learning/</guid>
      <pubDate>Tue, 16 Jul 2024 12:48:36 GMT</pubDate>
    </item>
    <item>
      <title>元 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3th8a/meta_marl/</link>
      <description><![CDATA[嗨，我是一名博士新生，专注于 MARL。 当我深入研究最近的出版物时，有一件事让我印象深刻：关注元强化学习的论文几乎比元 MARL 案例多出数百倍。事实上，在过去的两年里，这个主题的论文不到 10 篇。 这真的让我很困惑，因为多智能体系统中的智能体共享策略并在某种程度上进行通信，乍一看，“学会学习”的框架应该带来一些好处吗？还是有一些奇怪和奇怪的小事情？    提交人    /u/No-Deer3657   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3th8a/meta_marl/</guid>
      <pubDate>Mon, 15 Jul 2024 12:24:22 GMT</pubDate>
    </item>
    </channel>
</rss>