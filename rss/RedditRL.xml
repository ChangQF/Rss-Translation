<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 22 Jun 2024 01:03:06 GMT</lastBuildDate>
    <item>
      <title>C++ 库到 JAX？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dlil7i/c_library_to_jax/</link>
      <description><![CDATA[大家好 r/MLQuestions，我读过这篇 2022 年的论文 https://chrislu.page/blog/meta-disco/，它讨论了通过纯 Jax 并行化将所有计算从 CPU 转移到 GPU 和 TPU。本质上，他们已经将整个 RL 步骤矢量化以进行 GPU 计算，并且由于令人难以置信的并行化，能够在 9 小时内在 Atari 基准测试中训练 50 万个代理，与 CleanRL 实现相比，这实现了 4000 倍的加速。  我认为现在它更常见了，但在 2022 年，这可能是一个重大突破。  现在，我也想要这个 我遇到了一个问题：在纯 jax 中工作。现在，我正在创建一个需要复杂 3d 数学引擎的产品 - 这些复杂的库很大并且用 C++ 编写。我也需要其中的很多库。  例如，一个是 OpenCASCADE [https://github.com/Open-Cascade-SAS] 。仅“src”文件夹就有 97mb，包含 16000 个文件。它非常庞大，我需要将其插入 JAX 以提高速度。  如果我不对我的应用程序进行超并行化，我将会陷入 10 倍的循环时间和 200 倍的硬件并行线减少的困境，这在我的领域意味着长时间的计算。  现在我懂数学，也许可以重写我需要的函数，但是我有没有可能用某种……装饰器将文件适配到 Jax ？毕竟，jax 也是用 C++ 编写的。或者也许可以分叉它并进行调整。任何可以让函数在 Jax 中工作的东西，并且自定义实现不会花费 &gt;month 的时间。 如果您知道任何可行的方法，请告诉我。谢谢大家！    提交人    /u/JustZed32   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dlil7i/c_library_to_jax/</guid>
      <pubDate>Sat, 22 Jun 2024 00:07:06 GMT</pubDate>
    </item>
    <item>
      <title>AgileRL - 用于最先进深度强化学习的进化型 RLOps</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</link>
      <description><![CDATA[嗨，我之前发布过关于我们的强化学习进化超参数优化实现 SOTA 结果的帖子，但我想分享的是，我们的开源框架现在已经发布了 v1.0.0 版本！ 请查看！https://github.com/AgileRL/AgileRL 该库最初专注于通过开创强化学习的进化 HPO 技术来减少训练模型和超参数优化所需的时间。进化 HPO 已被证明可以通过自动收敛到最佳超参数来大幅减少总体训练时间，而无需进行大量的训练运行。 我们不断添加更多算法和功能。 AgileRL 已经包含了最先进的可进化的在线策略、离策略、离线、多智能体和上下文多臂老虎机强化学习算法以及分布式训练。 我很乐意收到您的反馈！    提交人    /u/nicku_a   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</guid>
      <pubDate>Fri, 21 Jun 2024 17:50:05 GMT</pubDate>
    </item>
    <item>
      <title>在强化学习（或者更确切地说，对于任何算法来说，机器学习）的背景下，像策略梯度这样的算法的单调改进意味着什么，为什么它是算法的重要参数。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dl0meg/what_does_monotonic_improvement_of_an_algorithm/</link>
      <description><![CDATA[我一直在阅读不同的文本，但仍然不明白真正的含义。有人可以解释一下吗。     提交人    /u/aabra__ka__daabra   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dl0meg/what_does_monotonic_improvement_of_an_algorithm/</guid>
      <pubDate>Fri, 21 Jun 2024 10:19:25 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习人形v4问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkuvm7/about_reinforcement_learning_humanoid_v4_problem/</link>
      <description><![CDATA[大家好，这是我在 colab 上为人形机器人 mujoco_humanoid.ipynb - Colab (google.com) 编写的代码（您可以在线运行）。但我不知道哪一步错了，结果就是网络没法学习机器人控制:( 真的需要帮助。    submitted by    /u/Inevitable_Sea_8466   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkuvm7/about_reinforcement_learning_humanoid_v4_problem/</guid>
      <pubDate>Fri, 21 Jun 2024 03:57:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中设置种子的实用规则</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkh0kp/practical_rules_for_setting_seed_in_rl/</link>
      <description><![CDATA[大家好！如果这个问题重复了，我很抱歉，但在阅读了许多与此相关的帖子后，我仍然找不到我的问题的答案。 假设我正在开发一个模型，其性能根据初始种子而有很大差异。 我应该固定一个种子或一组种子来比较不同特征或奖励函数的影响，还是应该始终使用随机种子和平均值运行，即使在开发模型时也是如此？ 如果是第二种情况，您如何了解性能改进是由于种子还是由于管道中的其他变化？    提交人    /u/ParfaitFinancial9765   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkh0kp/practical_rules_for_setting_seed_in_rl/</guid>
      <pubDate>Thu, 20 Jun 2024 17:15:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么重要性采样比例的期望值是1呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkfrzp/why_is_the_expected_value_of_the_importance/</link>
      <description><![CDATA[        提交人    /u/hearthstoneplayer100   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkfrzp/why_is_the_expected_value_of_the_importance/</guid>
      <pubDate>Thu, 20 Jun 2024 16:23:57 GMT</pubDate>
    </item>
    <item>
      <title>RLHF 的启动代码库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkf3yr/starter_code_repos_for_rlhf/</link>
      <description><![CDATA[大家好， 我即将开始 LLM 研究，特别是 RLHF。我正在寻找可以作为起点的开放课程库。我发现了以下内容：  https://github.com/OpenLLMAI/OpenRLHF https://github.com/huggingface/trl https://github.com/CarperAI/trlx  所有这些似乎都与 transformers 库兼容，而该库又支持完整的开源（代码+数据，而不仅仅是权重）模型，例如 Pythia。所有这些似乎都得到了相当程度的更新。1) 和 3) 支持分布式训练。您会推荐哪一个？还有其他建议吗？ 抱歉，我的问题可能有些幼稚。我是 LLM 新手 :)    提交人    /u/South-Conference-395   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkf3yr/starter_code_repos_for_rlhf/</guid>
      <pubDate>Thu, 20 Jun 2024 15:56:11 GMT</pubDate>
    </item>
    <item>
      <title>用状态价值基线来强化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dk9lwm/reinforce_with_statevalue_baseline/</link>
      <description><![CDATA[嗨，我是 RL 的新手，我正在尝试了解如何使用状态值基线（在本例中为第二个神经网络）进行 REINFORCE。例如，在有两个玩家的棋盘游戏中，我训练一个代理对抗一个随机对手，何时应该存储奖励和价值？我应该在代理移动后才考虑奖励并估计价值，还是在对手移动后也考虑奖励并估计价值？ 此外，我不确定我是否正确理解了如何计算策略损失和价值策略损失。我的理解是，对于策略损失，我必须计算折扣奖励，从中减去值以获得优势，然后将对数概率与优势相乘以获得缩放对数概率，最后将它们全部相加（聚合）。对于价值策略，我的理解是我应该计算估计值和折扣奖励之间的 MSE 损失。 对吗？    提交人    /u/miroshuSan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dk9lwm/reinforce_with_statevalue_baseline/</guid>
      <pubDate>Thu, 20 Jun 2024 11:45:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 制作分类器，遇到内存错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dk6npn/making_a_classifier_using_ppo_encountering_memory/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dk6npn/making_a_classifier_using_ppo_encountering_memory/</guid>
      <pubDate>Thu, 20 Jun 2024 08:30:16 GMT</pubDate>
    </item>
    <item>
      <title>如果 PPO 受到稀疏奖励的影响，那么 InstructGPT 和 Learning to Summarize 是如何使其发挥作用的呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</link>
      <description><![CDATA[我见过很多关于 PPO 如何难以在稀疏奖励环境中发挥作用的讨论。在 instructGPT 和学习从人类反馈中总结的情况下，仅在一长串采样标记的最后一个标记处给予奖励。特别是对于一些涉及非常长的代数（1000 个动作范围）且最后只有一个奖励的现代 RLHF 任务 - PPO 如何在这里取得成功？我是否遗漏了优化？    提交人    /u/idioticfuse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</guid>
      <pubDate>Thu, 20 Jun 2024 01:01:39 GMT</pubDate>
    </item>
    <item>
      <title>“GUI-WORLD：面向 GUI 的多模态 LLM 代理的数据集”，Chen 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djxzhm/guiworld_a_dataset_for_guioriented_multimodal/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djxzhm/guiworld_a_dataset_for_guioriented_multimodal/</guid>
      <pubDate>Thu, 20 Jun 2024 00:00:43 GMT</pubDate>
    </item>
    <item>
      <title>“在训练和推理中权衡计算：我们探索了几种在训练或推理上花费更多资源之间进行权衡的技术，并描述了这种权衡的属性。我们概述了对人工智能治理的一些影响”，EpochAI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djxm5z/trading_off_compute_in_training_and_inference_we/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djxm5z/trading_off_compute_in_training_and_inference_we/</guid>
      <pubDate>Wed, 19 Jun 2024 23:43:06 GMT</pubDate>
    </item>
    <item>
      <title>stablebaselines jax 加速 vs pytorch</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djwnlz/stablebaselines_jax_speedup_vs_pytorch/</link>
      <description><![CDATA[他们在文档中声称这相当于 20 倍加速。这是怎么回事？我在 pytorch 中编写了一个环境。 我正在 pytorch 中自己编写 PPO 代理（用于学习）。但是对于大型实验，如果速度如此之快，我是否需要将其移植到 JAX？我假设我无法在不绕过 CPU 内存的情况下将我的 pytorch 张量转移到 Jax 张量。 有什么想法吗？    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djwnlz/stablebaselines_jax_speedup_vs_pytorch/</guid>
      <pubDate>Wed, 19 Jun 2024 22:58:55 GMT</pubDate>
    </item>
    <item>
      <title>“围棋 AI 能否具有对抗鲁棒性？”，Tseng 等人，2024 年（KataGo 的“绕圈”攻击可以被击败，但仍然可以找到更多攻击；这并不是由于 CNN）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djv1r3/can_go_ais_be_adversarially_robust_tseng_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djv1r3/can_go_ais_be_adversarially_robust_tseng_et_al/</guid>
      <pubDate>Wed, 19 Jun 2024 21:48:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 RLlib 进行多智能体供应链优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djltne/multiagent_supply_chain_optimization_with_rllib/</link>
      <description><![CDATA[亲爱的社区， 我正在寻找有关我目前在项目中面临的挑战性问题的见解和帮助。 问题： 仓库 X 面临着一项复杂的任务，即优化其供应链流程，以应对来自 200 多家供应商的 1000 种不同产品。目标是避免库存过剩、防止缺货并始终满足客户需求。产品分为易腐烂和不易腐烂两类。 我们提出的解决方案： 多智能体环境，其中每个智能体都会学习产品线的策略。我们使用了通过 Ray RLlib 实现的 PPO 算法。我们的 RL 环境旨在考虑以下操作：  何时下采购订单 订购哪些产品 每种产品的订购数量 考虑哪个供应商  代理根据过去一年的历史销售情况进行训练。环境由 N 个观察值组成，代表库存水平和趋势，以及 2 个操作（订购数量和供应商 ID）。代理在具有共享策略选项集的多代理环境中进行训练，每个情节由 365 个时间步骤组成。奖励是根据一系列行动后在一个情节中获得的净利润计算的。此外，奖励塑造用于指导代理做出正确的决策。当错过需求、订单延迟和选择错误的供应商（价格更高或交货时间更长）时，我们在每个时间步骤中施加惩罚。实际上，代理在时间步骤“t”做出的每个决定都会对未来时间步骤“t+M”产生负面/正面影响。 在每个时间步骤，历史数据的需求都用于更新观察值。因此，预计代理将达到或超过去年的净利润。 关于神经网络，我们使用了一个自定义模型，该模型由具有 ReLU 激活的线性层组成，分支为 softmax 用于供应商选择，并为数量分支输出值使用自定义激活函数，范围为 [0, max_purchase_quantity]。已应用批量归一化来增强收敛并减少过度拟合，并已结合注意力机制来关注观察空间中的关键值。 遇到的问题： 尽管付出了努力，但我在实现最佳收敛和性能方面仍面临挑战。训练过程明显很慢（使用 Nvidia GPU GeForce RTX 3080 10GB）。在对 50 种产品进行了 15,000 次迭代（历时 28 小时）之后，代理仍未达到预期的奖励，无法自主设置订单数量和供应商 ID。我们尝试在奖励函数中对错误决策实施惩罚，尝试了不同的探索策略，使用了课程学习（逐步应用惩罚），并应用了奖励规范化。然而，结果并不如预期。 您可以如何提供帮助： 我正在寻求帮助和新观点。如果您有供应链优化领域强化学习的经验，那么关于提高收敛速度、处理复杂动作空间或有效探索策略的见解的建议将非常有价值。 此外，如果您遇到过类似的挑战或在供应链环境中成功实施了 RL 代理，我们将非常感激您对调整超参数、设计有效奖励函数的指导或任何其他相关建议。 提前感谢您考虑这个主题。 我热切期待您的回复和见解。    提交人    /u/WoodenDot8305   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djltne/multiagent_supply_chain_optimization_with_rllib/</guid>
      <pubDate>Wed, 19 Jun 2024 15:22:31 GMT</pubDate>
    </item>
    </channel>
</rss>