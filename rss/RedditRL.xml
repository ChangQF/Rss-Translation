<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 15 Jul 2024 21:14:52 GMT</lastBuildDate>
    <item>
      <title>对 MARLLib 的看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e42o68/views_on_marllib/</link>
      <description><![CDATA[您好，我最近问了一个有关我正在开发的项目的问题：https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button 但我发现还有另一个名为 MARLLib 的库，它与 RLlib 类似，但专门用于 MARL。你试过了吗？如果试过，你会推荐使用它吗？谢谢 :)    提交人    /u/Miss_Bat   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e42o68/views_on_marllib/</guid>
      <pubDate>Mon, 15 Jul 2024 18:47:07 GMT</pubDate>
    </item>
    <item>
      <title>关于 MuZero 终端状态的价值目标的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3xes8/question_about_value_targets_for_terminal_states/</link>
      <description><![CDATA[我一直在阅读有关 MuZero 的文章，但对终端节点的处理方式有些不清楚。一般来说，我们使用 n 步回报作为我们的价值目标，但这是否也适用于终端节点？我没有看到任何关于强制将终端状态的价值目标设为 0 的提及，但我也不明白如果我们只使用通常的 n 步回报，学习是如何可能的，因为从终端状态开始的任何序列的 lambda 回报都等于该状态的当前价值估计。似乎整个价值函数都会被终端状态初始化的任何值完全搞乱。强制将终端状态的价值目标设为 0 似乎是必要的，但我没有在 MuZero 或 Stochastic MuZero 论文中看到这一点。 需要说明的是，我不是在谈论搜索过程中使用的值 - 我知道 MuZero 在 MCTS 期间不会将终端节点视为特殊。我特别想问的是训练期间使用的价值目标。    提交人    /u/YellowishWhite   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3xes8/question_about_value_targets_for_terminal_states/</guid>
      <pubDate>Mon, 15 Jul 2024 15:19:47 GMT</pubDate>
    </item>
    <item>
      <title>元 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3th8a/meta_marl/</link>
      <description><![CDATA[嗨，我是一名博士新生，专注于 MARL。 当我深入研究最近的出版物时，有一件事让我印象深刻：关注元强化学习的论文几乎比元 MARL 案例多出数百倍。事实上，在过去的两年里，这个主题的论文不到 10 篇。 这真的让我很困惑，因为多智能体系统中的智能体共享策略并在某种程度上进行通信，乍一看，“学会学习”的框架应该带来一些好处吗？还是有一些奇怪和奇怪的小事情？    提交人    /u/No-Deer3657   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3th8a/meta_marl/</guid>
      <pubDate>Mon, 15 Jul 2024 12:24:22 GMT</pubDate>
    </item>
    <item>
      <title>DQN 中的损失函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3gkro/loss_function_in_dqn/</link>
      <description><![CDATA[大家好， 请问一下深度 Q 学习算法中损失函数的作用和功能/目的是什么     提交人    /u/Correct-Jaguar-339   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3gkro/loss_function_in_dqn/</guid>
      <pubDate>Sun, 14 Jul 2024 23:47:27 GMT</pubDate>
    </item>
    <item>
      <title>“使用强化学习解决《流放之路》物品制作问题”（价值迭代）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e37obz/solving_path_of_exile_item_crafting_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e37obz/solving_path_of_exile_item_crafting_with/</guid>
      <pubDate>Sun, 14 Jul 2024 17:19:11 GMT</pubDate>
    </item>
    <item>
      <title>我计划为我的大学做一个自动驾驶汽车项目。我想要与自动驾驶汽车中的 RL 相关的指导。我应该如何在开放的 AI 健身房中为汽车创建训练环境。有很多问题 TT。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e32t8f/i_am_planning_to_make_a_self_driving_car_project/</link>
      <description><![CDATA[我从谷歌中提取了一个 3D 地图，并计划进一步将其导入到 Gazebo 中进行模拟和物理之类的事情。现在我有一个问题，我应该如何在 Gazebo 和 ROS 中使用 RL 来实现自动驾驶汽车。出于训练目的，考虑使用谷歌的 3D 地图。有哪位专家可以指导我吗？    提交人    /u/manas_otaku   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e32t8f/i_am_planning_to_make_a_self_driving_car_project/</guid>
      <pubDate>Sun, 14 Jul 2024 13:51:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 适用于 ML 应用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2xfz6/rl_for_ml_applications/</link>
      <description><![CDATA[大家好！我即将攻读与人工智能和机器学习相关的计算机科学博士学位。我想研究一些包含或结合强化学习和机器学习问题的东西。我曾参与过与控制系统和机器人相关的强化学习项目，这些项目主要涉及训练代理执行任务。但是，我想在一些机器学习问题中使用强化学习。根据目前的趋势，有人能建议一些有趣的领域或应用吗？其中考虑的一些是用于 NLP 任务的强化学习或用于图像分类的强化学习。我将研究更多可能性，但任何方向的指导都将不胜感激。    提交人    /u/shazfu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2xfz6/rl_for_ml_applications/</guid>
      <pubDate>Sun, 14 Jul 2024 08:38:39 GMT</pubDate>
    </item>
    <item>
      <title>寻找从零开始到完善代理的 RL 内容。任何主题或媒体</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2o9en/looking_for_rl_content_from_scratch_to_polished/</link>
      <description><![CDATA[我正在寻找涵盖从头开始到完善代理的环境编码内容。 我想了解他们从训练简单模型中获得的所有超参数和见解，因为他们建立了更复杂的奖励、惩罚或其他任何使代理工作的东西。 例如 https://www.youtube.com/watch?v=SX08NT55YhA https://www.youtube.com/watch?v=DcYLT37ImBY&amp;t=1294s 书面/视频很好，如果也有训练日志那就最好了。建议在哪里看？    由   提交  /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2o9en/looking_for_rl_content_from_scratch_to_polished/</guid>
      <pubDate>Sat, 13 Jul 2024 23:43:06 GMT</pubDate>
    </item>
    <item>
      <title>[R] 理解强化学习中离散表示的不合理有效性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2gqcn/r_understanding_the_unreasonable_effectiveness_of/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2gqcn/r_understanding_the_unreasonable_effectiveness_of/</guid>
      <pubDate>Sat, 13 Jul 2024 18:05:58 GMT</pubDate>
    </item>
    <item>
      <title>弃用函数近似中的折扣奖励（Sutton 10.4）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</link>
      <description><![CDATA[Sutton 指出，在持续问题中使用函数近似时，使用折扣奖励不再有意义。 这对我来说真的没有意义，有人可以详细说明一下吗？    提交人    /u/federicom01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</guid>
      <pubDate>Sat, 13 Jul 2024 16:40:41 GMT</pubDate>
    </item>
    <item>
      <title>用大约 13 分钟解释我 2 年的 RL 研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</link>
      <description><![CDATA[        由    /u/ejmejm1 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</guid>
      <pubDate>Sat, 13 Jul 2024 15:11:20 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 算法中的大批量会起作用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</link>
      <description><![CDATA[通常我看到人们在实施 REINFORCE 算法（使用神经网络）时会这样做： for state, action, reward in episodes: update (batch size is 1)  如果游戏长度为 50 轮，我们也可以将所有状态、动作和奖励连接到批量大小为 50 的张量中并进行更新。我尝试过，并且取得了相当不错的成功，值得注意的是（并且不出所料）它大大加快了训练速度。 所以我在想，是什么会阻止我们进行更多连接。假设我们不是每 50 轮游戏更新一次，而是每 10 轮游戏更新一次。张量的维度足够小，这将显著提高计算速度，并可能导致更好的梯度估计。但是，我们最终进行的更新较少。这是我们在监督学习中看到的标准 batch_size 超参数权衡问题。 为什么没人尝试过？或者，也许我只是不擅长搜索是否有人尝试过。 在尝试之前想问一下，因为模拟一切有时需要几天时间。 在你来找我之前，是的，我知道有更好的算法，我只是喜欢先探索非常非常简单的算法。    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</guid>
      <pubDate>Sat, 13 Jul 2024 13:46:53 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：在 pokerenv 中 >>: 'list' 和 'int' 的操作数类型不受支持</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e23q7r/typeerror_unsupported_operand_types_for_list_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e23q7r/typeerror_unsupported_operand_types_for_list_and/</guid>
      <pubDate>Sat, 13 Jul 2024 06:32:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于连续组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e22p2t/rl_for_continuous_combinatorics/</link>
      <description><![CDATA[您好， 我在工作中遇到一种情况，我正尝试使用机器学习模型来解决连续组合问题。本质上，想象一系列变量 x1-xN，它们可以是任意数字（连续）。我的梯度非常陡峭，很难通过 SGD 等方法导航。我需要找到这些变量的组合，以便优化从这些变量派生的某些属性。有什么建议吗？    提交人    /u/elaraxelara   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e22p2t/rl_for_continuous_combinatorics/</guid>
      <pubDate>Sat, 13 Jul 2024 05:28:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DQN 和 DRL 有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</link>
      <description><![CDATA[我知道 NN 是函数近似器，但要近似某些东西，您必须知道真实值（监督学习）。而且您永远无法在 TD 方法（如 Q-Learning 和一般 RL）中看到真实值。它将样本估计作为客观值与我们已经拥有的东西（更多样本估计）进行比较。为什么它效果这么好？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</guid>
      <pubDate>Fri, 12 Jul 2024 22:57:05 GMT</pubDate>
    </item>
    </channel>
</rss>