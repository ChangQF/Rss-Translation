<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 22 Apr 2024 12:26:36 GMT</lastBuildDate>
    <item>
      <title>用于知识图路径查找/排名的最先进的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ca4f03/state_of_the_art_rl_for_knowledge_graph_path/</link>
      <description><![CDATA[大家好， 我正在寻找一种最先进的 RL 算法来找到两个节点之间最有趣的路径知识图。 我发现 DeepPath 适合我的任务。但它相当“旧”（2017 年），并且仅在相当小的 KG（15.000 个节点，310.000 个三元组）上进行了测试，而我的 KG 有 680 万个节点，大约有 680 万个节点。 50.000.000 个三元组。 我的问题：RL 适合这个规模吗？如果是，您会建议哪种算法？ 提前非常感谢！   由   提交 /u/Zaaesar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ca4f03/state_of_the_art_rl_for_knowledge_graph_path/</guid>
      <pubDate>Mon, 22 Apr 2024 07:10:35 GMT</pubDate>
    </item>
    <item>
      <title>用于简单电池控制的 DQN 不学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9yd87/dqn_for_simple_battery_control_not_learning/</link>
      <description><![CDATA[我遇到电池控制问题。有一个由电池、电网和负载组成的能源系统。我想根据电费和负载需求了解电池何时充电、何时放电。当剧集大小为一个月时，我当前使用的 DQN 代理无法学习。它实际上了解剧集大小是否为 1 天（我将数据集设为一天）。另一方面，PPO 和 A2C 学习得很好。 我尝试将动作空间从 3 个元素（充电、放电、不执行任何操作）增加到 9 个元素，但没有任何改进。我还尝试使用渐变裁剪。这是否意味着 DQN 无法学习这一点？或者我可以用什么东西来让它学习？我应该修改 DQN 神经网络吗？我应该缩小剧集大小吗？我应该强制梯度裁剪吗？  作为参考，环境代码：https://github.com/MFHCehade/Reinforcement-Learning-for-Battery-Management/blob/main/environments/energy_management_env.py 对于 DQN，我在这里使用实现：https://avandekleut.github.io/dqn/ 非常感谢 &amp;# x200b; ​   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9yd87/dqn_for_simple_battery_control_not_learning/</guid>
      <pubDate>Mon, 22 Apr 2024 01:24:15 GMT</pubDate>
    </item>
    <item>
      <title>无法渲染我的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9rtr0/cannot_render_my_env/</link>
      <description><![CDATA[晚上好 我已经使用 pettingzoo 的规范创建了一个自定义 MARL 环境，我正在尝试使用 rllib 进行训练，我是将 render_env 设置为 true 但渲染方法被忽略。我什至尝试过 rllib 示例来渲染 env，但似乎没有渲染任何内容。我错过了什么吗？ 我正在使用 ParallelPettingZoo() 函数来注册我的环境   由   提交 /u/Zenphirt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9rtr0/cannot_render_my_env/</guid>
      <pubDate>Sun, 21 Apr 2024 20:28:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么不能将下界 \sqrt{A^{SH} T} 应用于 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9r7hj/why_cannot_apply_the_lower_bound_sqrtash_t_to_rl/</link>
      <description><![CDATA[考虑 MDP M=(S,A, H, {P_h}, {r_h}) 中的遗憾最小化。学习协议是，对于总 T 个情节中的每个情节 t，我将尝试确定性策略 pi^{(t)}。目标是尽量减少遗憾：R(T) = \sum_{t=1}^T V_1^* - V_1^{\pi^{(t)}}。 我可以查看此内容问题作为多臂老虎机问题，其中臂集 \Pi 是所有可能的确定性策略的集合，即臂数为 A^{SH}。这是因为每当我将 \pi \in \Pi 取入 \Pi 时，我都可以推出策略 \pi 来获得 V_1^{\pi} 的嘈杂（无偏差）版本。相反，对于任何配备 A^{SH} 的 bandit 实例，我可以将其转换回 MDP 实例。如果是这样，我的问题是，到底是什么阻止我将 A^{SH} 武装老虎机问题的下界 Omega(\sqrt{A^{SH} T}) 应用于 MDP 问题？这不可能是真的，因为 UCB 算法给出了 \sqrt{H^3 SA T} 的上限来求解 MDP，但我不确定为什么下限 Omega(\sqrt{A^{SH}} T) 不能应用. 提前谢谢您！   由   提交/u/conan279  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9r7hj/why_cannot_apply_the_lower_bound_sqrtash_t_to_rl/</guid>
      <pubDate>Sun, 21 Apr 2024 20:03:05 GMT</pubDate>
    </item>
    <item>
      <title>DummyVecEnv & VecNormalize 使奖励图表消失（稳定基线 3）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9krih/dummyvecenv_vecnormalize_makes_the_reward_chart/</link>
      <description><![CDATA[    &lt; /a&gt;  当我使用这两行时，我正在使用屏蔽 PPO SB3 训练用于股票交易的自定义环境为了标准化我的奖励，会发生以下情况：  张量板中的奖励图表消失。 解释的方差似乎工作正常并达到正值。 &lt; li&gt;Model 似乎没有正确使用 ActionMasker。因为它交易太多，造成巨大损失   env = DummyVecEnv([lambda: env]) env = VecNormalize(env)  现在，如果我做了一点改变，并且只规范化奖励，而不是观察空间，通过这样做 env = VecNormalize(env,norm_obs=False)  code&gt; 除了模型似乎正确使用“ActionMasker”且损失要低得多之外，上述几点仍然会发生。 Tensorboard with &#39;VecNormalize(env,norm_obs=False) &#39; 如果我删除了以上两行编写代码并使用 MinMaxScaler 手动标准化观察结果，我在张量板中获得奖励图表，但解释的方差始终为负并且仅收敛到零，如下所示 没有“VecNormalize(env)”的张量板   由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9krih/dummyvecenv_vecnormalize_makes_the_reward_chart/</guid>
      <pubDate>Sun, 21 Apr 2024 15:33:46 GMT</pubDate>
    </item>
    <item>
      <title>“V-STaR：自学推理者的培训验证者”，Hosseini 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9jrc6/vstar_training_verifiers_for_selftaught_reasoners/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9jrc6/vstar_training_verifiers_for_selftaught_reasoners/</guid>
      <pubDate>Sun, 21 Apr 2024 14:52:04 GMT</pubDate>
    </item>
    <item>
      <title>“从 _r_ 到 Q*：你的语言模型实际上是一个 Q 函数”，Rafailov 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9jnk1/from_r_to_q_your_language_model_is_secretly_a/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9jnk1/from_r_to_q_your_language_model_is_secretly_a/</guid>
      <pubDate>Sun, 21 Apr 2024 14:47:34 GMT</pubDate>
    </item>
    <item>
      <title>模型的随机性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9e03u/randomness_in_model/</link>
      <description><![CDATA[我有一个自定义的 MARL Boid 植绒，使用奖励函数训练的环境。我尝试多次训练模型而不更改任何参数。它已经在 2/40 训练运行中学会了适当的行为，即训练了一个成功的模型。我有 Open AI Gym 和带 PPO 的 SB3，无法使用健身房，因为无法安装它和其他健身房。我什至使用种子： seed_val = 23 env.seed(seed_val)  为什么它如此随机以及我该如何克服它。欢迎提出想法 代码：https://drive.google.com /file/d/1zhmLnigj_BqNOxuSri0Va0fMRRJs8slW/view?usp=drivesdk   由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9e03u/randomness_in_model/</guid>
      <pubDate>Sun, 21 Apr 2024 09:43:19 GMT</pubDate>
    </item>
    <item>
      <title>参数噪声与输入噪声的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9dvwt/parametric_noise_over_input_noise/</link>
      <description><![CDATA[我在 2017 年看到了这项研究，其中讨论了使用“参数噪声”而不是基于输入的噪声。我尝试在基于 PPO 的 Boid 植绒自定义环境中使用它，但得出的结论是这是不可能的，因为 SB3 不支持它。 有关如何添加或使用它的任何建议。我认为它需要开放的人工智能基线，即使这样，人们又该如何去做呢，因为目前没有找到任何文档，并且是否有任何环境限制，比如我们不能使用它？ 使用参数噪声进行更好的探索   由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9dvwt/parametric_noise_over_input_noise/</guid>
      <pubDate>Sun, 21 Apr 2024 09:35:06 GMT</pubDate>
    </item>
    <item>
      <title>构建用于计算卸载的奖励函数。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9du95/build_a_reward_functions_for_computation/</link>
      <description><![CDATA[您好，我想为计算卸载构建奖励函数 我卸载到边缘或云端或在本地执行任务 我的目标是最大限度地减少延迟和能耗，成本，并最大限度地提高安全性（信任）。我在我的奖励函数中使用了这些指标并进行了加权总和，但我对我的函数不满意。谁可以为我提供一些想法。提前致谢。   由   提交/u/RynTSukinomi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9du95/build_a_reward_functions_for_computation/</guid>
      <pubDate>Sun, 21 Apr 2024 09:31:44 GMT</pubDate>
    </item>
    <item>
      <title>DQN 培训和测试的黑白差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c98hiy/discrepancy_bw_dqn_training_testing/</link>
      <description><![CDATA[上下文：https： //www.reddit.com/r/reinforcementlearning/comments/1bxw4ci/need_some_feedback_on_an_idea_for_using/ 我一直在使用 DQN 模型，并注意到一些我希望得到的不一致之处一些见解。 在训练阶段，我一直在使用 dqn_training 函数并加载权重。然而，它在测试期间采取的行动似乎与训练期间采取的行动有很大不同。尽管奖励减少，但它继续执行相同的操作（最终目标是获得更高的奖励。训练期间就是这种情况） 我特别有兴趣了解为什么之间存在差异训练与测试中采取的操作，以及 dqn.test 是否正确，以及 dqn.forward 是否是更好的函数 任何见解或建议将不胜感激。    由   提交 /u/Broncosslayer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c98hiy/discrepancy_bw_dqn_training_testing/</guid>
      <pubDate>Sun, 21 Apr 2024 03:43:59 GMT</pubDate>
    </item>
    <item>
      <title>通过想象力、探索和批评实现法学硕士的自我完善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c97orc/toward_selfimprovement_of_llms_via_imagination/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.12253 摘要：  尽管大型语言模型（LLM）在各种方面具有令人印象深刻的能力任务中，他们仍然在处理涉及复杂推理和计划的场景。最近的工作提出了先进的提示技术以及使用高质量数据进行微调以增强法学硕士推理能力的必要性。然而，这些方法本质上受到数据可用性和质量的限制。有鉴于此，自我纠正和自我学习成为可行的解决方案，采用的策略允许法学硕士改进他们的成果并从自我评估的奖励中学习。然而，法学硕士在自我完善其反应方面的有效性，特别是在复杂的推理和规划任务中，仍然值得怀疑。在本文中，我们引入了用于LLM自我改进的AlphaLLM，它将蒙特卡罗树搜索（MCTS）与LLM结合起来，建立一个自我改进循环，从而在无需额外注释的情况下增强LLM的能力。 AlphaLLM 从 AlphaGo 的成功中汲取灵感，解决了将 MCTS 与 LLM 相结合以实现自我提升的独特挑战，包括数据稀缺、语言任务的巨大搜索空间以及语言任务中反馈的主观性。 AlphaLLM 由即时合成组件、专为语言任务量身定制的高效 MCTS 方法以及用于精确反馈的三个批评模型组成。我们在数学推理任务中的实验结果表明，AlphaLLM 在无需额外注释的情况下显着增强了法学硕士的性能，显示了法学硕士自我改进的潜力。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c97orc/toward_selfimprovement_of_llms_via_imagination/</guid>
      <pubDate>Sun, 21 Apr 2024 02:57:26 GMT</pubDate>
    </item>
    <item>
      <title>数独实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c8muem/sudoku_implementation/</link>
      <description><![CDATA[嗨！我开始通过强化学习来实现数独求解器，实际上我没有取得很好的结果，因为我在这个领域比较新，而且我认为任务并不那么简单。如果您擅长，请考虑在此处帮助我！   由   提交/u/Cri_Sti_An   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c8muem/sudoku_implementation/</guid>
      <pubDate>Sat, 20 Apr 2024 10:34:53 GMT</pubDate>
    </item>
    <item>
      <title>推理不会以使用自定义数据集 llama-2 模型进行微调的 QLoRa 结束（模型在无限循环中生成输入和响应）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c8ibe3/inference_doesnt_end_in_a_qlora_finetuned_with_a/</link>
      <description><![CDATA[       嘿，伙计们，我通过使用位和字节量化来训练 llama 2 模型，然后使用以下格式的自定义数据集对其进行训练： 系统提示： 输入：  响应： 当我运行推理时，模型的行为符合我想要的方式（有点）——它生成回复，但也会在无限循环中回复自身，直到 max_new_tokens 为达到，即它生成“###响应”但不会停止并且还会生成“### 输入”并循环回复自身。为什么会发生这种情况？这是标记器的设置方式吗？我是否使用了错误的格式来训练模型？ 我将非常感谢任何有关此事的帮助、评论、反馈或资源链接。请参阅下面的附图，看看模型的响应是什么样的。预先感谢您。 https： //preview.redd.it/wv1khhbomkvc1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=f6e8f754de5d6fda134b9f0e5cc7bb468355bd6e    ;由   提交/u/guccicupcake69   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c8ibe3/inference_doesnt_end_in_a_qlora_finetuned_with_a/</guid>
      <pubDate>Sat, 20 Apr 2024 05:35:37 GMT</pubDate>
    </item>
    <item>
      <title>SERL：用于样本高效的机器人强化学习的软件套件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c8g6t6/serl_a_software_suite_for_sampleefficient_robotic/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.16013 SERL 代码：https： //github.com/rail-berkeley/serl 机器人控制器代码：https://github.com/rail-berkeley/serl_franka_controllers 项目页面：https://serl-robot.github.io/ 视频：https://www.youtube.com/watch?v=Um4CjBmHdcw 摘要： &lt; blockquote&gt; 近年来，机器人强化学习 (RL) 领域取得了重大进展，实现了处理复杂图像观察、在现实世界中进行训练以及合并辅助数据（例如演示和先前经验）的方法。然而，尽管取得了这些进步，机器人强化学习仍然很难使用。从业者公认，这些算法的特定实现细节对于性能而言通常与算法的选择一样重要（如果不是更重要的话）。我们认为，机器人强化学习的广泛采用以及机器人强化学习方法的进一步发展所面临的一个重大挑战是此类方法相对难以获得。为了应对这一挑战，我们开发了一个精心实现的库，其中包含一个高效的离策略深度强化学习方法样本，以及计算奖励和重置环境的方法、一个用于广泛采用的机器人的高质量控制器，以及一个大量具有挑战性的示例任务。我们提供这个库作为社区的资源，描述其设计选择，并展示实验结果。也许令人惊讶的是，我们发现我们的实施可以实现非常高效的学习，获取 PCB 板组装、电缆布线和对象重新定位的策略，平均每个策略需要 25 到 50 分钟的培训，比报告的最先进结果有所改进对于文献中的类似任务。这些政策实现了完美或近乎完美的成功率，即使在扰动下也具有极高的鲁棒性，并表现出紧急恢复和纠正行为。我们希望这些有希望的结果和我们高质量的开源实现将为机器人社区提供一个工具，以促进机器人强化学习的进一步发展。我们的代码、文档和视频可以在 此 https URL  &lt; !-- SC_ON --&gt;  由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c8g6t6/serl_a_software_suite_for_sampleefficient_robotic/</guid>
      <pubDate>Sat, 20 Apr 2024 03:35:08 GMT</pubDate>
    </item>
    </channel>
</rss>