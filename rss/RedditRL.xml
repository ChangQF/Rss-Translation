<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 06 Dec 2024 03:33:37 GMT</lastBuildDate>
    <item>
      <title>利用强化学习实现火箭着陆：Unity 中的 2D 和 3D 模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7qbay/landing_rockets_with_reinforcement_learning_2d/</link>
      <description><![CDATA[大家好， 我和你们中的许多人一样，也是一名强化学习爱好者，我想分享两个我使用 Unity 创建的强化学习场景的视频。 第一个视频是 2D 模拟，目标是将星际飞船火箭降落在一个平台上，并遵守两个关键规则：它不能用尽所有推进剂，并且不能与平台相撞 - 换句话说，它需要软着陆。 第二个视频是 3D 版本，与第一个视频类似，但现在增加了一个维度。 使用的算法是 PPO。 我很想听听您的想法和反馈！ 这两次训练都是在我的 MacBook Pro 上完成的，配备 M2 Pro Max 芯片和 96 GB RAM。第一次模拟只花了几个小时，而第二次模拟大约花了 42 小时才完成。 https://reddit.com/link/1h7qbay/video/8rzfwow4x45e1/player https://reddit.com/link/1h7qbay/video/56dbqc5mv45e1/player    提交人    /u/bbzzo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7qbay/landing_rockets_with_reinforcement_learning_2d/</guid>
      <pubDate>Fri, 06 Dec 2024 02:00:22 GMT</pubDate>
    </item>
    <item>
      <title>【竞赛】机器人跑者联盟：实时协调数千个机器人！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7m54r/competition_the_league_of_robot_runners/</link>
      <description><![CDATA[你好，机器和强化学习者！ 这是一项公告，呼吁参加 2024 年机器人跑步者联盟，这是一项多季节 🚀 竞赛和研究计划 🚀 ，旨在解决工业优化中最具挑战性的问题之一：多机器人路径规划（有时也称为多智能体路径查找）。 这项比赛的灵感来自依赖移动机器人技术的当前和新兴应用 🦾🤖。例如，亚马逊的自动化仓库，成千上万的机器人协同工作，确保包裹安全高效地递送🧸📦🚚❤️。 现在已进入第二季，比赛主要关注两个核心挑战：  任务调度，由您决定哪个机器人执行哪项任务。 路径规划，由您协调机器人，使它们尽快到达目的地并且不会发生碰撞。  这两种设置都是在线和实时的，这意味着在您计算时时钟会滴答作响。在时间耗尽之前，尽可能多地完成任务！ 我们认为 🧠 基于学习的算法 🧠 对于解决这些类型的问题有几个优势：  大规模计算机器人运动需要极快的策略，而学习非常适合这种情况 基于学习的策略也很容易更新，这对于处理动态拥塞非常重要 总是有更多任务，这意味着没有固定的全局最优  参加本次比赛是向全球学术界和行业专家展示您的 💡 ML/RL 技能 💡 的好方法。比赛结束后，问题实例和提交内容将开源，这会增加您的知名度，降低其他人的进入门槛，并帮助社区成长和学习👩‍🏫🤔📚🎓。 对于三个不同类别的🌟出色表现🌟，我们将提供10,000 美元的奖金池。我们还以 1,000 美元 AWS 积分的形式提供培训奖励，以帮助参与者降低线下培训成本😻。 提交内容随时开放，评估结果可立即在我们的实时排行榜上查看。比赛将持续到📅2025 年 2 月 16 日📅，结果将于 2025 年 3 月公布。 入门很容易！我们为您提供模拟器和代码线束（“入门套件”）、许多示例问题以及用于探索生成的解决方案的可视化工具。您还可以访问去年表现最佳的规划器作为基准。请访问我们的网站了解所有详细信息（www.leagueofrobotrunners.org）或在此处发帖询问！    提交人    /u/robotrunnersofficial   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7m54r/competition_the_league_of_robot_runners/</guid>
      <pubDate>Thu, 05 Dec 2024 22:44:24 GMT</pubDate>
    </item>
    <item>
      <title>解释法学硕士强化学习基础知识的技术指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7kixk/technical_guide_explaining_the_fundamentals_of/</link>
      <description><![CDATA[       由    /u/Legaltech_buff  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7kixk/technical_guide_explaining_the_fundamentals_of/</guid>
      <pubDate>Thu, 05 Dec 2024 21:34:18 GMT</pubDate>
    </item>
    <item>
      <title>研究在 RL 中使用 Transformer 架构的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7jhmk/resources_to_study_the_use_of_transformer/</link>
      <description><![CDATA[我读过关于在 RL 中使用 transformers 的调查论文，我知道它们是 RL 的 RNN 替代品。这些论文并没有对实现提供太多见解。我知道决策 transformers，但这不是我想要使用的，因为它没有任何策略网络，我想使用 RL 算法，但 NN 架构必须使用 transformers 来突出状态的长期依赖性。有人用过这个吗？如果有，如果你能分享资源会很有帮助。    提交人    /u/anchit_rana   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7jhmk/resources_to_study_the_use_of_transformer/</guid>
      <pubDate>Thu, 05 Dec 2024 20:51:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h772b4/reinforcement_learning_courses/</link>
      <description><![CDATA[对于强化学习，以下哪门课程是首选-   UCL X DeepMind ⁠Stanford CS234 ⁠David Silver 的 RL 课程     提交人    /u/momosspicy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h772b4/reinforcement_learning_courses/</guid>
      <pubDate>Thu, 05 Dec 2024 11:28:41 GMT</pubDate>
    </item>
    <item>
      <title>强化学习适合解决刽子手游戏吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6ynzs/is_reinforcement_learning_suitable_for_solving/</link>
      <description><![CDATA[在过去的 3 周里，我一直在使用 RL 迭代不同的策略来解决 Hangman 游戏，但到目前为止，我的准确率并不高（1000 场游戏中解决的不到 100 场）。说实话，我对 RL 的概念还很陌生。我一开始使用 Deep Q-Learning 框架来解决这个问题。我的方法相当简单：  将训练单词的隐藏状态转换为两个向量（一个大小为 26 的二进制向量用于跟踪猜测的字母，另一个大小为 45 的规范化向量用于跟踪正确猜测的字母的位置），并将其作为输入传递给神经网络，同时传递剩余的猜测次数和要发现的隐藏字母数量。总输入大小为 73x1。 以 1 的探索率初始化训练循环，随着训练的进行，探索率衰减为 0.1。探索部分的工作原理是，使用训练单词列表，根据隐藏单词的长度，从预先计算的字典中选择最常见的字母。 对每个单词开始训练过程，游戏一直进行到代理猜出正确的单词或猜错 6 次为止，计算损失并在每次猜测后更新参数。对每个单词重复此过程。使用 q 学习网络获得的下一步计算损失。为在每个优化步骤中完成的重放设置了批量大小。 然后将训练好的网络用于一组看不见的验证词，然后使用这些词来衡量准确性。 我使用的奖励结构是每个正确字母 +1，每个错误字母 -1，如果单词猜对了则 +10，如果单词猜错了则 -10。然后我对其进行了更新，以便连续猜对字母可获得更高的奖励，连续猜错可获得更高的负奖励。 状态和奖励计算的更新是通过单独的 Hangman 模块完成的。每次猜测每个单词后都会调用该模块。  训练从 1000 场游戏的 10% 准确率开始，然后随着探索率的下降，准确率降低到 1%，这也反映在验证中。 现在我觉得我做错了。但你们认为这是一种合适的方法吗？ 编辑：我错误地说我使用两个二进制向量将有关隐藏状态的信息作为输入传递给神经网络。我实际上传递了一个二进制向量和一个在 0 和 1 之间标准化的向量。    提交人    /u/hpnr0724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6ynzs/is_reinforcement_learning_suitable_for_solving/</guid>
      <pubDate>Thu, 05 Dec 2024 02:38:33 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习中的奖励分配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6u396/reward_distribution_in_offline_rl/</link>
      <description><![CDATA[我正在使用保守 Q 学习和 SAC 模型。我的奖励分布非常不平衡：所有奖励都在 0 到 1 之间，95% 低于 0.02，40% 低于 0.001。 我应该如何转换它？我之所以问这个问题，是因为我的预测 Q 值为负：这没有意义，因为所有奖励都是正的。我已经排除了所有错误的可能性（我认为是这样）。我也得到了带有非常小惩罚的负 q 值（以及没有惩罚：我确实看到 q 函数在潜水前很长一段时间都是正的：潜水与 q 值方差的增加有关）。任何指针都值得赞赏！    提交人    /u/electricsheep123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6u396/reward_distribution_in_offline_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 23:06:15 GMT</pubDate>
    </item>
    <item>
      <title>“指南：实时人形代理”，Zhang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6prin/guide_realtime_humanshaped_agents_zhang_et_al_2024/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6prin/guide_realtime_humanshaped_agents_zhang_et_al_2024/</guid>
      <pubDate>Wed, 04 Dec 2024 20:08:16 GMT</pubDate>
    </item>
    <item>
      <title>LoRA研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6lepr/lora_research/</link>
      <description><![CDATA[最近，我发现关于 LoRA 替代品的论文激增。您认为人们正在探索哪些研究方向？ 您认为它有可能以某种方式与 RL 相结合吗？    提交人    /u/KevinBeicon   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6lepr/lora_research/</guid>
      <pubDate>Wed, 04 Dec 2024 17:15:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Q_Learning 算法无法正常学习？（更新）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6l4jl/why_is_my_q_learning_algorithm_not_learning/</link>
      <description><![CDATA[嗨，这是我几天前的另一篇文章的后续文章 ( https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/ ) 我阅读了您的评论并且 u/scprotz 告诉我即使是德文的，拥有代码也会很有用。这是我的代码：https://codefile.io/f/F8mGtSNXMX 我通常不会在网上分享我的代码，所以如果网站不是最好的选择，我很抱歉。不同的类通常位于不同的文档中（您可以在导入中看到），我运行 Spiel（即游戏）文件来启动程序。我希望这会有所帮助，如果您发现任何看起来奇怪或不正确的东西，请发表评论，因为尽管搜索了几个小时，我还是没有找到问题所在。    提交人    /u/_waterstar_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6l4jl/why_is_my_q_learning_algorithm_not_learning/</guid>
      <pubDate>Wed, 04 Dec 2024 17:04:30 GMT</pubDate>
    </item>
    <item>
      <title>预训练 VAE 与强化学习期间训练的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6imk0/difference_between_pretrained_vaes_versus/</link>
      <description><![CDATA[      我的目标是开发一个用于 Carla 模拟器进行自动驾驶的代理。为了实现这一点，我实现了 Soft Actor-Critic (SAC) 算法。在将图像输入 SAC 算法之前，我使用了变分自动编码器 (VAE)。 VAE 没有经过预训练，因为我假设它会在强化学习过程中得到训练和改进。这种方法有缺陷吗？如果有，为什么，如何改进？我采取这一步骤的理由是受到观察 DQN 中 CNN 在 Frozen Lake 环境中的使用情况的启发。我的代码可以在 GitHub 上找到，供感兴趣的人使用：https://github.com/b-gtr/Soft-Actor-Critic 这里还有一个简单的代码工作原理说明：    提交人    /u/Fair_Device_4961   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6imk0/difference_between_pretrained_vaes_versus/</guid>
      <pubDate>Wed, 04 Dec 2024 15:24:25 GMT</pubDate>
    </item>
    <item>
      <title>为 RL 代理编码棋盘游戏 Cascadia 中的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6ggjq/encode_actions_from_board_game_cascadia_for_rl/</link>
      <description><![CDATA[      嘿 :) 我目前正在为我的 Cascadia 实现开发一个代理。游戏逻辑是用 Java 实现的，我创建了 Rest Endpoints 来从外部操纵游戏。因此我可以开始游戏、获取当前游戏状态、重置剧集、计算奖励、返回合法行动等等。 我的计划是使用一些众所周知的 Python 框架（我想到稳定基线、tensorforce 或 RL_Coach）训练代理，而我面临的挑战是编码代理在每个状态下可以采取的可能/合法行动。 我实现的图片显示了代理可以在其上执行操作的棋盘。在每个回合，代理/玩家可以选择四对中的一对（由景观和动物组成）并将它们放置在他的棋盘上。请注意，只有在没有其他动物已经放置并且景观允许该动物类型的情况下，动物才能放置在景观瓷砖上。新景观瓷砖的合法位置在图片中标记为绿色。 已经放置的动物用绿色圆圈标记。 为简单起见，我可以确保代理始终有 3 个瓷砖可以放置动物，并且他可以选择的对中至少有一对包含他可以实际放置在棋盘上的动物。我甚至可以想出一个解决方案，其中选择部分是随机的，代理必须使用他得到的任何一对。 是否可以训练单个代理来执行这 3 个连续的任务（选择对 -&gt; 放置景观 -&gt; 放置动物）？如果是这样，我该如何编码动作空间以将其提供给代理？ https://preview.redd.it/m4axweoj5u4e1.jpg?width=1011&amp;format=pjpg&amp;auto=webp&amp;s=d0b7cb547ad36574b9ad82998ab8966b72b20351    submitted by    /u/ItchyRoyal212   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6ggjq/encode_actions_from_board_game_cascadia_for_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 13:51:07 GMT</pubDate>
    </item>
    <item>
      <title>帮助 - Minesweeper RL 执行重复操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6e6xo/help_minesweeper_rl_executing_repeated_actions/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6e6xo/help_minesweeper_rl_executing_repeated_actions/</guid>
      <pubDate>Wed, 04 Dec 2024 11:49:48 GMT</pubDate>
    </item>
    <item>
      <title>在多代理环境中训练代理的最佳方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6dvqo/best_way_to_train_agents_in_multi_agent/</link>
      <description><![CDATA[我正在开展一个国际象棋 RL 项目，其中 2 个使用不同算法训练的代理相互对抗。我想知道训练代理的最佳方法是什么。我应该让他们互相对抗，分别训练代理对抗对手的随机动作，还是让他们分别训练，让游戏中的两个对手都使用相同的算法。并且建议会很有帮助     提交人    /u/Livid-Ant3549   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6dvqo/best_way_to_train_agents_in_multi_agent/</guid>
      <pubDate>Wed, 04 Dec 2024 11:29:40 GMT</pubDate>
    </item>
    <item>
      <title>除了样本复杂性之外，还有其他理由使用基于模型的 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</link>
      <description><![CDATA[当我们能够进行大规模环境并行化时，使用基于模型的算法真的有意义吗？ 基本上，我想知道是否存在像 DreamerV3 这样的算法可以解决而 PPO 无法解决的环境？例如，DreamerV3 论文表明，PPO 和 IMPALA 无法解决最困难的 Minecraft 任务，但如果有大量计算，PPO 最终会解决这些任务吗？ 除了降低样本复杂度之外，还有其他理由使用基于模型的算法吗？    提交人    /u/vandelay_inds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</guid>
      <pubDate>Wed, 04 Dec 2024 01:39:56 GMT</pubDate>
    </item>
    </channel>
</rss>