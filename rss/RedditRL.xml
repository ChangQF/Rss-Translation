<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 13 Jul 2024 01:06:30 GMT</lastBuildDate>
    <item>
      <title>为什么 DQN 和 DRL 有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</link>
      <description><![CDATA[我知道 NN 是函数近似器，但要近似某些东西，您必须知道真实值（监督学习）。而且您永远无法在 TD 方法（如 Q-Learning 和一般 RL）中看到真实值。它将样本估计作为客观值与我们已经拥有的东西（更多样本估计）进行比较。为什么它效果这么好？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</guid>
      <pubDate>Fri, 12 Jul 2024 22:57:05 GMT</pubDate>
    </item>
    <item>
      <title>平均奖励与奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1lyk2/mean_reward_vs_reward/</link>
      <description><![CDATA[为什么在每个 episode 的简单奖励上使用平均奖励而不是简单奖励？    提交人    /u/Glum_Flower_8682   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1lyk2/mean_reward_vs_reward/</guid>
      <pubDate>Fri, 12 Jul 2024 16:29:26 GMT</pubDate>
    </item>
    <item>
      <title>这个 Chatgpt 的 NEAT 实现会起作用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1jf8a/will_this_chatgpts_neat_implementation_work/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1jf8a/will_this_chatgpts_neat_implementation_work/</guid>
      <pubDate>Fri, 12 Jul 2024 14:44:41 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫性质被尊重吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1gi7j/is_markov_property_respected/</link>
      <description><![CDATA[我的系统由离散动作和有限状态组成 在状态 1 中执行动作后，动作将不再产生任何效果。我的意思是：&lt;状态 1，任何动作，奖励取决于动作，Same\_Next\_State2&gt;。无论代理尝试什么动作，它都会转到相同的 next_state 并根据动作获得奖励。    提交人    /u/Glum_Flower_8682   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1gi7j/is_markov_property_respected/</guid>
      <pubDate>Fri, 12 Jul 2024 12:31:59 GMT</pubDate>
    </item>
    <item>
      <title>交易中的造型奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1fhdj/shape_reward_in_trading/</link>
      <description><![CDATA[大家好， 我正在交易中实施 PPO 算法，对于具有稀疏奖励的买入、持有、卖出操作，只有在获利或亏损后才会给予奖励。我们如何为这个场景塑造奖励，有人有在交易中塑造奖励的经验吗？比如在持有和等待的场景中，奖励应该是什么？    提交人    /u/laxuu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1fhdj/shape_reward_in_trading/</guid>
      <pubDate>Fri, 12 Jul 2024 11:38:17 GMT</pubDate>
    </item>
    <item>
      <title>人形训练-v4 借助外力进行步行训练。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e18pjf/humanoid_training_v4_walk_training_with_external/</link>
      <description><![CDATA[您好，我正在使用 Stable-Baseline3 训练 mujoco 的人形机器人向前行走。我已经能够证明 SAC 可以很好地实现这一目标。我想证明代理可以承受外力并仍然实现相同的目标。有人可以提供如何使用 mujoco 环境实现这一点的指示吗？     提交人    /u/lulislomelo   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e18pjf/humanoid_training_v4_walk_training_with_external/</guid>
      <pubDate>Fri, 12 Jul 2024 04:27:45 GMT</pubDate>
    </item>
    <item>
      <title>“关于通过弱 LLM 评判强 LLM 的可扩展监督”，Kenton 等人 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e14e7r/on_scalable_oversight_with_weak_llms_judging/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e14e7r/on_scalable_oversight_with_weak_llms_judging/</guid>
      <pubDate>Fri, 12 Jul 2024 00:41:34 GMT</pubDate>
    </item>
    <item>
      <title>实验——招募参与者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e0pxpf/experiment_recruiting_participants/</link>
      <description><![CDATA[嘿！您能帮我完成我的大学调查吗？有点与强化学习有关（计划谬误）。 作为奖励 - 您将有机会赢得 100 英镑的亚马逊代金券，因为我只需要 100 名参与者，这是一个赢得 100 英镑的好机会。 https://uniofbath.questionpro.eu/t/AB3u1qYZB3vuhE 该研究包括一份问卷，其中包括您选择您希望第二天花时间进行的活动和一份 30 个问题的测试，该测试将衡量您的冲动程度。第二天，您将被要求记录您在所选活动上花费的时间，并回答一些后续问题。 如有任何问题 - 请告诉我！    提交人    /u/ImACashBadger   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e0pxpf/experiment_recruiting_participants/</guid>
      <pubDate>Thu, 11 Jul 2024 14:12:53 GMT</pubDate>
    </item>
    <item>
      <title>使用“stable_baselines3”时，如何为“gymnasium”环境重置播种？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e0comw/how_to_seed_gymnasium_environment_resets_when/</link>
      <description><![CDATA[我想为我的体育馆环境播种。从[官方文档][1]中，我这样做的方式是 -  ```py import gymnasium as gym env = gym.make(&quot;LunarLander-v2&quot;, render_mode=&quot;human&quot;) observer, info = env.reset(seed=42)  ``` 但是，stable_baselines3 似乎不需要从用户端进行重置，如下面的程序所示 -  ``` import gymnasium as gym from stable_baselines3 import PPO from stable_baselines3.common.env_util import make_vec_env # Parallel environment vec_env = make_vec_env(&quot;CartPole-v1&quot;, n_envs=4) model = PPO(&quot;MlpPolicy&quot;, vec_env, verbose=1) model.learn(total_timesteps=25000) model.save(&quot;ppo_cartpole&quot;) del model # 删除以演示保存和加载 model = PPO.load(&quot;ppo_cartpole&quot;)  ``` 如何使用 `stable_baselines3` 放置种子？我尝试放置 `np.random.seed(24)`，但没有成功。 [1]: https://gymnasium.farama.org/    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e0comw/how_to_seed_gymnasium_environment_resets_when/</guid>
      <pubDate>Thu, 11 Jul 2024 01:19:00 GMT</pubDate>
    </item>
    <item>
      <title>具有自定义环境的 A3C 俄罗斯方块中的次优解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzordj/suboptimal_solutions_in_a3c_tetris_with_custom/</link>
      <description><![CDATA[最近，我选择 A3C 作为我的最终项目的首选算法。我使用 https://github.com/dgriff777/rl_a3c_pytorch 作为参考，但没有使用 LSTM。我使用的环境是定制的：观察空间基本上是具有 3 个通道图像的两块俄罗斯方块（第一个通道显示只有堆叠的四格骨牌的棋盘图像，第二个通道显示当前下落的四格骨牌，第三个通道显示下一个四格骨牌）。在运行了数千步之后，我的代理似乎陷入了次优解决方案，它倾向于将块堆叠在棋盘的左侧。我该如何解决这个问题？ 注意：我正在使用 dellacherie 评估函数来奖励我的代理    提交人    /u/handshakers01   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzordj/suboptimal_solutions_in_a3c_tetris_with_custom/</guid>
      <pubDate>Wed, 10 Jul 2024 06:22:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO算法的训练速度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzoolu/the_training_speed_of_ppo_algorithm/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzoolu/the_training_speed_of_ppo_algorithm/</guid>
      <pubDate>Wed, 10 Jul 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 AlphaZero 学习 Tichu（纸牌游戏）——寻求有关状态 + 网络设计的建议/推测</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzo93h/learning_tichu_card_game_with_alphazero_seeking/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzo93h/learning_tichu_card_game_with_alphazero_seeking/</guid>
      <pubDate>Wed, 10 Jul 2024 05:50:06 GMT</pubDate>
    </item>
    <item>
      <title>如何建立强化学习的方程式？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dznob5/how_to_develop_the_equations_for_reinforcement/</link>
      <description><![CDATA[最近我开始学习强化学习，我看到的所有视频或博客都只是提供一个模拟环境并描述强化学习的过程，有没有你们知道的博客或教程，他们教如何开发状态模型，如何形成策略中使用的所有方程式。那将非常有帮助，任何帮助都将不胜感激     提交人    /u/Away_Elk_6826   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dznob5/how_to_develop_the_equations_for_reinforcement/</guid>
      <pubDate>Wed, 10 Jul 2024 05:13:34 GMT</pubDate>
    </item>
    <item>
      <title>Craftium：用于创建强化学习环境的可扩展框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz9bp5/craftium_an_extensible_framework_for_creating/</link>
      <description><![CDATA[嗨！我一直在开发 Craftium：一个易于使用的 RL 环境创建框架。Craftium 将开源体素游戏引擎 Minetest 包装在一个实现 Gymnasium API 的 Python 库中。虽然 Craftium 旨在用于环境创建，但我们也包含了一些现成的环境和任务（请在此处 查看）。目前，Craftium 处于早期开发阶段，但已准备好使用和测试。希望您觉得它很有趣！欢迎提供反馈！！    由    /u/mikelma 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz9bp5/craftium_an_extensible_framework_for_creating/</guid>
      <pubDate>Tue, 09 Jul 2024 18:12:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么与离策略算法相比，状态表示学习方法（通过辅助损失）较少应用于 PPO 等在线策略 RL 算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</link>
      <description><![CDATA[我已经看到了不同的状态表征学习方法（通过辅助损失，无论是自我预测还是基于结构化探索），它们已经与离线策略方法（如 DQN、Rainbow、SAC 等）一起应用。例如，SPR（自我预测表征）已与 Rainbow 一起使用，CURL（强化学习的对比无监督表征）已与 DQN、Rainbow 和 SAC 一起使用，RA-LapRep（通过图拉普拉斯进行表征学习）已与 DDPG 和 DQN 一起使用。我很好奇为什么这些方法没有像 PPO（近端策略优化）这样的在线策略算法得到广泛应用。将这些表示学习技术与在线策略算法学习相结合是否存在理论问题？    提交人    /u/C7501   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</guid>
      <pubDate>Tue, 09 Jul 2024 10:56:37 GMT</pubDate>
    </item>
    </channel>
</rss>