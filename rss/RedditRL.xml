<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 07 Jun 2024 09:15:58 GMT</lastBuildDate>
    <item>
      <title>在 RL 中可视化和比较 PPO 与 REINFORCE 的状态和动作的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da5lig/best_ways_to_visualize_and_compare_states_and/</link>
      <description><![CDATA[大家好， 我正在使用 LunarLanderContinuous-v2 环境比较 PPO 和 REINFORCE。在训练两种算法之后，我希望有效地可视化和比较它们所采取的状态和操作。 具体来说，我正在寻找有关以下方面的建议：  如何可视化每个算法访问的状态分布。 如何表示每个算法所采取的操作中的多样性和模式。 任何可以帮助使这些比较清晰且富有洞察力的工具或技术。  我已经在训练期间记录了状态和动作，但我正在努力如何以有意义的方式呈现这些数据，至少我真的没有一个清晰的想法。 我还可以展示除累积奖励之外的其他差异，但我真的不知道是什么。 谢谢！    提交人    /u/An4rcyst   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da5lig/best_ways_to_visualize_and_compare_states_and/</guid>
      <pubDate>Fri, 07 Jun 2024 08:19:00 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在机器人腿平衡任务中实现奖励/重置系统？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9znxi/how_should_i_implement_the_rewardsreset_system_in/</link>
      <description><![CDATA[大家好。我设计了一个机器人腿，希望将来能教它如何走路（显然是 2），但现在我正尝试使用 Omniverse Isaac Gym 和稳定基线 3 来教一条腿如何在脚上保持平衡，只是为了了解 RL 并看看我的关节是否足够强壮以保持腿的抬起。我想说的是，我是一个初学者，除了训练推车杆、人形机器人和蚂蚁的例子外，没有太多经验。 我的任务有点奏效了，但我认为在如何重置腿以及如何计算奖励和惩罚方面存在一些明显的问题。但我还没有走得足够远来能够告诉 我的问题是，经过几次重置后，机器人就会飞起来，整个模拟基本上就崩溃了......我的重置方法是，一旦机器人掉落（脚上的接触传感器达到某个值以下），代码就会将机器人重置/惩罚到它的起始世界位置，关节设置为 0。也许这不是正确的方法？也许我应该让它保持在跌倒的地方，然后试着让它学会如何重新站起来？我不知道.. 我从人形训练任务中观察到，机器人每次掉落时都会重置并从原来的地方开始，但也许我不明白到底发生了什么？ 任何帮助都将不胜感激.. 我还想提一下，我确实尝试过 pybullet，但它无法正确加载我的 URDF..不知道为什么🤷‍♂️。   由    /u/StoryReader90  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9znxi/how_should_i_implement_the_rewardsreset_system_in/</guid>
      <pubDate>Fri, 07 Jun 2024 02:11:56 GMT</pubDate>
    </item>
    <item>
      <title>深度学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9wxf7/deep_learning_projects/</link>
      <description><![CDATA[我正在攻读数据科学和人工智能理科硕士学位。我将于 2025 年 4 月毕业。我正在寻找深度学习项目的想法。1) 为 LLM 实施的深度学习 2) 为 CVision 实施的深度学习 我在网上查了一下，但大多数都是非常标准的项目。来自 Kaggle 的数据集是通用的。我大约有 12 个月的时间，我想做一些好的研究级项目，可能在 NeuraIPS 上发表它。我的优势是我擅长解决问题，一旦确定了问题，但我不擅长识别和构建问题..目前，我正在尝试判断什么是一个好的研究领域？    提交人    /u/Rogue260   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9wxf7/deep_learning_projects/</guid>
      <pubDate>Thu, 06 Jun 2024 23:53:15 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型中对齐的基本限制”，Wolf 等人 2023（提示对动作不安全后验的先验）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9le1u/fundamental_limitations_of_alignment_in_large/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9le1u/fundamental_limitations_of_alignment_in_large/</guid>
      <pubDate>Thu, 06 Jun 2024 15:47:15 GMT</pubDate>
    </item>
    <item>
      <title>风险：统治全球 体育馆观景空间设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kzfp/risk_global_domination_gymnasium_observation/</link>
      <description><![CDATA[大家好 我一直在构建一个 Risk: 全球统治环境。我的目标是稍后使用 RL 和 DQN 来构建机器人。 我是 RL 新手。 我目前处于一个阶段，我在本地运行一个简单版本的游戏，确定性机器人相互对战。有 4 个领地和 2 个玩家 我现在想要完成的是将其转换为 Gymnasium 环境，并实施强化学习，专注于单一行动可能性：选择攻击哪个领地。所有其他行动都将确定性地处理。这是为了简化实施，我希望这是一个明智的举措。 我现在需要定义观察空间。对于那些不熟悉 Risk 游戏的人来说，1v1 标准游戏的重要信息包括：  哪个领土由哪个玩家控制，每个领土上有多少军队 游戏的历史（虽然我想我现在会跳过这一个） 领土如何定位（哪个连接到哪个，以预测路径） 大陆 - 包括哪些领土以及它们奖励多少军队（但我现在也会跳过它）  所以，我想要做以下事情： # 谁控制着每个领土。只有两个玩家，所以值为 0 或 1 membership_space = rooms.Box(low=0, high=1, shape=(4,), dtype=int) # 理论上最大部队数量可以无限，所以让我们将体育馆记录的限制设置为整数：2**63 - 2 soldiers_space = rooms.Box(low=0, high=2**63-2, shape=(4,), dtype=int) # 大小为 (num_territories x num_territories) 的邻接矩阵 connections_space = rooms.Box(low=0, high=1, shape=(4, 4), dtype=int) # 知道机器人是哪个玩家 player_id_space = rooms.Box(low=0, high=1, shape=(), dtype=int) self.observation_space = gym.spaces.Dict({ &quot;ownership&quot;: membership_space, &quot;troops&quot;: soldiers_space, &quot;connectivity&quot;: connection_space, &quot;player_id&quot;: player_id_space })  我使用 Boxes 来表示我的向量，这是最好的选择吗？ 我认为让机器人知道它是什么玩家很重要？ 此外，我认为有更好的方法来表示领土连通性。 再次，我对 RL 非常陌生，对实现它的所有选项有点迷茫。 谢谢你的帮助 编辑：我实际上需要一个观察变量来指定我从哪个领土攻击，因为在我的简化版本的 DQN 机器人中，机器人唯一可以选择的就是攻击哪个领土。    提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kzfp/risk_global_domination_gymnasium_observation/</guid>
      <pubDate>Thu, 06 Jun 2024 15:29:48 GMT</pubDate>
    </item>
    <item>
      <title>反过来想，在 RL 中“使用采样命令进行探索”有什么意义呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</link>
      <description><![CDATA[在行为函数更新后，RL upside down 选择最佳的前 k 个近期情节并汇总来自它们的信息以提出目标命令，这些命令是这些精英情节的奖励和范围之和的平均值。然后这些命令用于生成更多情节以添加到重放缓冲区中。但我注意到在生成情节时，初始状态是重置状态，这意味着前进的轨迹可能与这些命令完全无关，因为那些新看到的状态可能不会导致那些提出的命令，从而导致随机采样动作。在这方面，RL upside down 是否依靠神经网络的泛化来获得更高的奖励轨迹？否则我不明白它是如何工作的。    提交人    /u/OutOfCharm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</guid>
      <pubDate>Thu, 06 Jun 2024 15:06:44 GMT</pubDate>
    </item>
    <item>
      <title>多模式 Mamba/mamba+Transformers 可以使用文本进行在线 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d96n3c/can_multimodal_mambamambatransformers_do_online/</link>
      <description><![CDATA[你好 r/ReinforcementLearning 所以我正在解决一个比文本/图片/机器人更多（更多）的问题，并且基本上没有解决方案数据集可以训练，除了书籍和博客。  动作空间是一组离散、图形和多二进制动作，观察空间是动作空间+在其上执行的一些计算。 是否可以将大量文本输入模型，给它推理（实际推理），并期望模型在初步反复试验后使用文本知识来回答离散的非文本问题？ 此外，是否可以使用类似 Mamba+Transformers 架构的东西来进行这种类型的在线无模型 RL？  在这里做我的第一个模型......谢谢大家！    由    /u/JustZed32  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d96n3c/can_multimodal_mambamambatransformers_do_online/</guid>
      <pubDate>Thu, 06 Jun 2024 01:35:02 GMT</pubDate>
    </item>
    <item>
      <title>从这往哪儿走？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</link>
      <description><![CDATA[我有一个需要 RL 的项目，我学习了 Sutton 撰写的 RL 简介的前 200 页，掌握了基础知识和所有基本理论信息。你们有什么建议可以开始实际实施我的 RL 项目想法，比如从 OpenAI Gym 中的基本想法开始，或者我不知道我是新手，你们能给我一些建议，告诉我如何在实践方面做得更好吗？ 更新：谢谢大家，我会检查所有这些建议，这个 subreddit 很棒！    提交人    /u/Signal-Ad3628   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</guid>
      <pubDate>Thu, 06 Jun 2024 01:32:05 GMT</pubDate>
    </item>
    <item>
      <title>如何才能正确地实时地将正确的操作添加到内存缓冲区？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d95s2y/how_can_i_correctly_add_the_right_action_to_the/</link>
      <description><![CDATA[我正在尝试训练 PPO 代理来玩 Geometry Dash，我认为尝试使用 OpenCV 并使用视频源作为输入会很有趣，而不是创建游戏副本并直接使用游戏内存/状态。事实证明这比我想象的要困难得多。 实施的高级概述 我训练了 YoloV4 来检测代理是活着还是死了。我认为这将是一种确定代理是否已死亡的好方法。当代理活着时，它将根据给定的 4 帧堆栈作为输入来选择操作。如果代理存活，则每帧都会收到 +1 奖励，如果代理死亡，则收到 0。 问题 我注意到的一个问题是，存储在代理内存缓冲区中的操作与游戏中发生的操作并不完全匹配（即，代理因跳入尖刺而死亡，但内存缓冲区会存储它没有跳跃）。 我认为问题的一部分是玩家无法在空中做任何事情，但代理不知道这一点，并试图选择操作，这会覆盖它实际上能够成功执行的最后一个操作，然后将其存储在其内存中。 我尝试过的一件事是引入跳跃的冷却时间。但是，这是一个问题，因为关卡的某些部分需要连续多次跳跃而不停止。 我非常感谢您对如何改进项目的反馈！    提交人    /u/EducationalChicken_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d95s2y/how_can_i_correctly_add_the_right_action_to_the/</guid>
      <pubDate>Thu, 06 Jun 2024 00:50:57 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 的不完美信息游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d92kvj/imperfect_information_games_using_rl/</link>
      <description><![CDATA[如何使用 RL 制定多人不完美/不完整信息游戏？MARL 是唯一的方法还是还有其他方法/算法？    提交人    /u/Meta_Sage_247   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d92kvj/imperfect_information_games_using_rl/</guid>
      <pubDate>Wed, 05 Jun 2024 22:18:32 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型中出现了欺骗能力”，Hagendorff 2024（法学硕士设定目标，内心独白越来越容易被操纵）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8rxjn/deception_abilities_emerged_in_large_language/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8rxjn/deception_abilities_emerged_in_large_language/</guid>
      <pubDate>Wed, 05 Jun 2024 14:54:56 GMT</pubDate>
    </item>
    <item>
      <title>“国际象棋神经网络中学习前瞻的证据”，Erik Jenner 2024（Leela Chess Zero 在前向传递过程中至少会前瞻两次）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8ruai/evidence_of_learned_lookahead_in_a_chessplaying/</link>
      <description><![CDATA[        提交人    /u/gwern   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8ruai/evidence_of_learned_lookahead_in_a_chessplaying/</guid>
      <pubDate>Wed, 05 Jun 2024 14:50:58 GMT</pubDate>
    </item>
    <item>
      <title>我的 TD3 继续自杀，即使奖励更糟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8r7kz/my_td3_keep_suiciding_even_if_the_reward_is_worse/</link>
      <description><![CDATA[        提交人    /u/antoine12e9   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8r7kz/my_td3_keep_suiciding_even_if_the_reward_is_worse/</guid>
      <pubDate>Wed, 05 Jun 2024 14:24:04 GMT</pubDate>
    </item>
    <item>
      <title>[P] Python 中 NEAT 的一个实际工作、简化实现（sNEAT）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8nsjl/p_an_actually_working_simplified_implementation/</link>
      <description><![CDATA[目前可用的神经进化论拓扑 (Kenneth O. Stanley, 2002) 的开源实现要么被极度弃用，要么根本无法提供预期的结果。 我用 Python 重写了它，并在 GPLv3 下提供它，因为我认为我不是唯一一个对这种非常有趣的进化方法的状态感到失望的人。 您可以在Github上找到它。    提交人    /u/Sthatic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8nsjl/p_an_actually_working_simplified_implementation/</guid>
      <pubDate>Wed, 05 Jun 2024 11:39:18 GMT</pubDate>
    </item>
    <item>
      <title>确实是更好的短语</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d8fvug/truly_the_better_phrase/</link>
      <description><![CDATA[        提交人    /u/scruffy0014   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d8fvug/truly_the_better_phrase/</guid>
      <pubDate>Wed, 05 Jun 2024 03:06:40 GMT</pubDate>
    </item>
    </channel>
</rss>