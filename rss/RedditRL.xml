<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 17 Mar 2024 15:13:45 GMT</lastBuildDate>
    <item>
      <title>Cognition AI 推出 Devin：“金牌编码员构建了一个可以为他们完成工作的人工智能”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bggkqu/devin_launched_by_cognition_ai_goldmedalist/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bggkqu/devin_launched_by_cognition_ai_goldmedalist/</guid>
      <pubDate>Sat, 16 Mar 2024 21:26:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习背景下的迁移学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bgb0f9/transfer_learning_in_the_context_of_rl/</link>
      <description><![CDATA[有没有人经历过与此相关的实用框架？ 我的搜索产生的大部分解决方案并不能完全解决我的具体问题. 我正在处理的问题是确定各种互动的最佳时机，每种互动的目的都是促使某些人采取积极行动。 I拥有有关这些人的初步信息，每次状态都是根据之前与其进行的交互以及这些交互的结果来定义的 我正在寻找实用的工具来执行转移群体之间的学习。   由   提交/u/Murky-Name4868  /u/Murky-Name4868 reddit.com/r/reinforcementlearning/comments/1bgb0f9/transfer_learning_in_the_context_of_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bgb0f9/transfer_learning_in_the_context_of_rl/</guid>
      <pubDate>Sat, 16 Mar 2024 17:15:13 GMT</pubDate>
    </item>
    <item>
      <title>预训练/内置宠物动物园代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bg9qf5/pretrainedbuildin_pettingzoo_agents/</link>
      <description><![CDATA[      是否有任何可以使用的预训练/内置 PZ 代理？我正在尝试在 atari pong 中实施对抗性政策，但努力训练可靠的 PPO 代理。已经尝试使用纯粹的自我对战，但代理很难学习 - 即使我开始对抗随机移动代理。  ​ 我还尝试编写一个自定义包装器，它将模仿 SB3 包装器以使用 StableBaseline3 代理之一 - 但由于某种原因，使用相同的 openCV 函数进行转换从 RGB 到灰度会产生不同的值。 （SB3 env 看起来“蓝色”） ​ 你知道有什么预训练的智能体可以用来训练固体 PPO 智能体或在对抗训练中使用它吗？  也许您知道其他带有预训练/内置代理的 RL 库，这些库也允许在训练期间设置多个代理策略？  sb3 环境&lt; /p&gt; ​  我还尝试编写一个自定义包装器来使用 StableBaseline3 代理 - 但由于某种原因，使用相同的 openCV 函数从 RGB 转换为灰度会导致不同的值。 （SB3 env 看起来“蓝色”） ​   由   提交/u/MrCogito_hs   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bg9qf5/pretrainedbuildin_pettingzoo_agents/</guid>
      <pubDate>Sat, 16 Mar 2024 16:18:11 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bg049g/multiagent_reinforcement_learning/</link>
      <description><![CDATA[我有一个在环境中训练的模型，并为其使用了 3 个代理以及来自 Stable Baselines3 的 PPO。 我想测试现在有 10 个智能体的模型，但 OpenAIgym 给出了观察空间不匹配的错误。我怎样才能实现这个目标？   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bg049g/multiagent_reinforcement_learning/</guid>
      <pubDate>Sat, 16 Mar 2024 06:51:23 GMT</pubDate>
    </item>
    <item>
      <title>健身房观察套装</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfwfr1/gym_observation_set/</link>
      <description><![CDATA[       大家好！我想改变我的env.reset和env.step返回的观察空间，例如hopper，我想让它返回(11,)，但实际上它返回(12,0)，因为我使用的gym版本太低，无法通过参数“exclude_current_positions_from_observation”更改。你能帮我解决一下吗？万分感谢！  https://preview.redd .it/wpsryhp65moc1.png?width=1298&amp;format=png&amp;auto=webp&amp;s=6635141bf5a7b6223d3f8e55e0b9e8b6e0b32347   由   提交/u/alleZhou  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfwfr1/gym_observation_set/</guid>
      <pubDate>Sat, 16 Mar 2024 03:12:42 GMT</pubDate>
    </item>
    <item>
      <title>“持续预训练大型语言模型的简单且可扩展的策略”，Ibrahim 等人 2024（循环 LR 和重播或多样化数据）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfulix/simple_and_scalable_strategies_to_continually/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfulix/simple_and_scalable_strategies_to_continually/</guid>
      <pubDate>Sat, 16 Mar 2024 01:39:47 GMT</pubDate>
    </item>
    <item>
      <title>MuZero简单实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfr08n/muzero_simple_implementation/</link>
      <description><![CDATA[我想尝试对随机 MuZero 与经典/原始 MuZero 的比较进行基准测试。我发现的所有实现都经过了大量优化。我正在寻找一个简单易读的 MuZero 实现，我可以轻松理解或修改它。您有什么建议吗？   由   提交 /u/_Hardric   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfr08n/muzero_simple_implementation/</guid>
      <pubDate>Fri, 15 Mar 2024 22:54:07 GMT</pubDate>
    </item>
    <item>
      <title>PPO 在训练期间（有探索）学习并表现良好，但在评估期间（没有探索）表现不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfmuwd/ppo_learns_and_performs_perfectly_during_training/</link>
      <description><![CDATA[我尝试过调整学习率、折扣因子、lambda、剪辑值、熵正则化系数和 L2 正则化系数，但到目前为止还没有成功。有什么建议来解决这个问题吗？  提前致谢！   由   提交/u/Appressive_Bag1262   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfmuwd/ppo_learns_and_performs_perfectly_during_training/</guid>
      <pubDate>Fri, 15 Mar 2024 19:54:54 GMT</pubDate>
    </item>
    <item>
      <title>迈向通用情境学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfbb3d/towards_generalpurpose_incontext_learning_agents/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=eDZJTdUsfe 演讲和幻灯片：https://neurips.cc/virtual/2023/79880 博客文章：http://louiskirsch.com/glas 摘要：  强化学习（RL）算法通常是手工制作，由人类研究和工程驱动。另一种方法是通过元学习使该研究过程自动化。一个特别雄心勃勃的目标是从头开始自动发现新的强化学习算法，使用上下文学习来完全从数据中学习如何学习，同时推广到广泛的环境。这些强化学习算法完全在神经网络中实现，通过根据环境中的先前经验进行调节，在元测试时没有任何基于优化的显式例程。为了实现泛化，这需要在多样化和具有挑战性的环境中进行广泛的任务分配。我们基于 Transformer 的通用学习代理 (GLA) 是朝这个方向迈出的重要的第一步。我们的 GLA 使用监督学习技术在离线数据集上进行元训练，并利用 RL 环境中的经验，并通过随机投影进行增强，以生成任务多样性。在元测试期间，我们的代理对完全不同的机器人控制问题（例如 Reacher、Cartpole 或 HalfCheetah）执行上下文元强化学习，这些问题不在元训练分布中。  &lt; !-- SC_ON --&gt;  由   提交/u/SunsetOneSix  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfbb3d/towards_generalpurpose_incontext_learning_agents/</guid>
      <pubDate>Fri, 15 Mar 2024 11:02:09 GMT</pubDate>
    </item>
    <item>
      <title>监督学习与离线强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</link>
      <description><![CDATA[我从 RL 开始，这些可能是非常琐碎的问题，但我想尽我所能地解决所有问题。如果您有任何资源可以为强化学习应用提供良好的直觉，也请在评论中提供：）谢谢。 问题：  我们在哪些场景中使用与离线强化学习相比，更喜欢监督学习？ 样本数量如何影响每个案例的训练？监督学习收敛得更快吗？ 有哪些例子可以同时使用和比较它们进行比较分析？  直觉：  监督学习可以很好地预测给定状态的奖励，但我们不能依赖它来最大化未来的奖励。由于它不使用推出来最大化奖励，并且不进行规划，因此我们不能期望在预期延迟奖励的情况下使用它。 此外，在非独立同分布的动态环境中，每个动作都会影响状态，然后影响进一步采取的动作。因此，对于连续设置，我们在大多数情况下考虑了 RL 的分布变化。 监督学习尝试为每个状态找到最佳动作，这在大多数情况下可能是正确的，但它是一个非常好的方法。针对不断变化的环境采取僵化而愚蠢的方法。强化学习可以自我学习，并且适应性更强。  对于答案，如果可能的话，提供单行，然后任何细节和答案来源也将不胜感激。我希望这篇文章能够为任何尝试应用强化学习的人提供一个很好的指南。我将编辑和更新下面回答的任何问题的答案，以汇总我获得的所有信息。如果您认为我应该考虑任何其他重大问题和疑虑，也请提及。谢谢！ ​ [编辑]：我找到的与此相关的资源： Sergey Levine 的 RAIL 讲座：模仿学习与离线强化学习 Sergey Levine 的中型帖子：数据决策：离线强化学习将如何改变我们的使用方式机器学习 Sergey Levine 的中型帖子：通过行动了解世界：强化学习作为可扩展自我监督学习的基础 Sergey Levine 的研究论文：我们什么时候应该更喜欢离线强化学习而不是行为克隆？ Sergey Levine 的研究论文：RVS：通过监督学习实现离线强化学习的基本要素是什么？   由   提交 /u/StwayneXG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</guid>
      <pubDate>Fri, 15 Mar 2024 05:17:56 GMT</pubDate>
    </item>
    <item>
      <title>对于较小的网络来说表示学习值得吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bey25b/is_representation_learning_worth_it_for_smaller/</link>
      <description><![CDATA[我阅读了很多关于表示学习作为实际 RL 任务预训练的文献。我目前正在处理顺序传感器数据作为输入。因此，很多数据都是冗余且嘈杂的。因此，代理需要首先从原始输入时间序列中学习语义特征。  由于强化学习中奖励的梯度信号与无监督学习过程相比非常弱，我认为对特征编码器（又名表示学习）进行无监督预训练是值得的。  现在几乎所有的文献都在处理巨大的神经网络的比较和巨大的数据集。我正在处理大约 200k-1M 个参数和大约 1M 个可用于预训练的样本。  我的问题是：当人工神经网络相对较小时，是否值得进行预训练？我的 RL 训练时间目前约为 60 小时，我希望通过预训练大幅缩短训练时间。    由   提交/u/flxh13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bey25b/is_representation_learning_worth_it_for_smaller/</guid>
      <pubDate>Thu, 14 Mar 2024 22:28:54 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习的成功例子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bept24/successful_examples_of_transfer_learning/</link>
      <description><![CDATA[有谁知道是否有任何论文讨论/成功地将某种迁移学习从一项强化学习任务应用到相关任务？例如，如果我有一个迷你网格世界，代理接受了导航培训，然后将其移动到类似的迷你网格，但现在将块推入目标位置。或者说这种事情大部分都是通过多任务学习来完成的？    由   提交 /u/Impallion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bept24/successful_examples_of_transfer_learning/</guid>
      <pubDate>Thu, 14 Mar 2024 16:50:11 GMT</pubDate>
    </item>
    <item>
      <title>“为什么效应定律不会消失”，Dennett 1974（基于模型的强化学习的演变）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bem6z4/why_the_law_of_effect_will_not_go_away_dennett/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bem6z4/why_the_law_of_effect_will_not_go_away_dennett/</guid>
      <pubDate>Thu, 14 Mar 2024 14:15:41 GMT</pubDate>
    </item>
    <item>
      <title>2024 年暑期学校</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1belslq/summer_schools_for_2024/</link>
      <description><![CDATA[您好，我正在寻找 2024 年一些好的 RL 暑期学校，但我发现它对不同的可能性有点不知所措。  这里有人有任何经验/建议吗？    由   提交 /u/IAmNotMarcus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1belslq/summer_schools_for_2024/</guid>
      <pubDate>Thu, 14 Mar 2024 13:58:04 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>