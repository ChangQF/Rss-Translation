<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 05 Jul 2024 12:27:24 GMT</lastBuildDate>
    <item>
      <title>Mamba 背后的思想：Albert Gu 谈论状态空间模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</link>
      <description><![CDATA[        由    /u/phoneixAdi  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</guid>
      <pubDate>Fri, 05 Jul 2024 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>赛车开放式 AI 健身房 - 推出是否应覆盖整个赛道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</link>
      <description><![CDATA[我试图复制此代码，但对 rollout 生成感到非常困惑。 步骤 4：生成随机 rollout 对于赛车环境，VAE 和 RNN 都可以使用随机 rollout 数据 - 即通过在每个时间步随机采取行动而生成的观察数据。实际上，我们使用伪随机动作，迫使汽车最初加速，以使其离开起跑线。 每次推出都应该覆盖整个赛道吗？ https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459 我一直在尝试运行代码，但推出后赛道从未完成。训练后我也看不到它完成。    提交人    /u/Competitive-Chip5872   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</guid>
      <pubDate>Fri, 05 Jul 2024 06:15:32 GMT</pubDate>
    </item>
    <item>
      <title>针对多个 TSP/VRP/和其他变体的多智能体强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</link>
      <description><![CDATA[你好，我是一名最近开始进行 RL 研究的学生。 我对包括 CO 和 MARL 在内的领域很感兴趣。即使有很多好的 RL 和启发式算法，我也想在更大规模和更复杂的领域尝试一些实验。 但是，对于多旅行商问题或 VRP，我很难找到 SOTA MARL 方法。（我在 github 上找到了一些论文，但通常没有代码实现） 除了论文之外，如果你能分享你的知识和见解，那将非常有帮助。    提交人    /u/QingdaoCraftBeer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</guid>
      <pubDate>Fri, 05 Jul 2024 04:57:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Gym 训练动作分类模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvoovw/using_gymnasium_to_train_an_action_classification/</link>
      <description><![CDATA[在任何人说之前，我明白这不是 RL 问题，谢谢。但我必须提到，我是团队的一员，我们都在尝试不同的方法，而我得到了这个。 首先，下面是我的代码： # 乒乓球课的自定义健身环境 TableTennisEnv(gym.Env): def __init__(self, frame_tensors, labels, frame_size=(3, 30, 180, 180)): super(TableTennisEnv, self).__init__() self.frame_tensors = frame_tensors self.labels = labels self.current_step = 0 self.frame_size = frame_size self.n_actions = 20 # 唯一动作的数量 self.observation_space = space.Box(low=0, high=255, shape=frame_size, dtype=np.float32) self.action_space = space.Discrete(self.n_actions) self.normalize_images = False self.count_reset = 0 self.count_step = 0 def reset(self, seed=None): 全局 total_reward, maximum_reward self.count_reset += 1 print(&quot;Reset called: &quot;, self.count_reset) self.current_step = 0 total_reward = 0 maximum_reward = 0 return self.frame_tensors[self.current_step], {} def step(self, action): 全局 total_reward, maximum_reward act_ten = torch.tensor(action, dtype=torch.int8) 如果 act_ten == self.labels[self.current_step]: 奖励 = 1 total_reward += 1 else: 奖励 = -1 total_reward -= 1 maximum_reward += 1 print(&quot;实际： &quot;, self.labels[self.current_step]) print(&quot;预测： &quot;, action) self.current_step += 1 print(&quot;步骤： &quot;, self.current_step) done = self.current_step &gt;= len(self.frame_tensors) obs = self.frame_tensors[self.current_step] if not done else np.zeros_like(self.frame_tensors[0]) truncated = False if done: print(&quot;最大奖励： &quot;, maximum_reward) print(&quot;获得的奖励： &quot;, total_reward) print(&quot;准确度： &quot;, (total_reward/maximum_reward)*100) return obs, reward, done, truncated, {} def render(self, mode=&#39;human&#39;): pass # 通过较小的批次处理来减少内存使用量 env = DummyVecEnv([lambda: TableTennisEnv(frame_tensors, labels, frame_size=(3, 30, 180, 180))]) timesteps = 100000 try: # 用较小的批量大小初始化 PPO 模型 model1 = PPO(&quot;MlpPolicy&quot;, env, verbose=1, learning_rate=0.03, batch_size=5, n_epochs=50, n_steps=4, tensorboard_log=&quot;./ppo_tt_tensorboard/&quot;) # 训练模型 model1.learn(total_timesteps=timesteps) # 保存训练好的模型 model1.save(&quot;ppo_table_tennis_3_m1_MLP&quot;) print(&quot;模型 1 训练和保存成功完成。&quot;) tr1 = total_reward mr1 = maximum_reward total_reward = 0 maximum_reward = 0 print(&quot;模型 1 的准确率（100 Epochs）：&quot;, (tr1/mr1)*100) except Exception as e: print(f&quot;模型训练或保存期间发生错误：{e}&quot;)  有 1514 个视频剪辑用于训练，已转换为矢量。每个视频剪辑矢量的尺寸为 (180x180x3)x30，因为我提取了 30 帧作为输入。 问题出现在训练期间。在最初几个步骤中，模型运行良好。过了一会儿，预测的动作停止变化。它只会一遍又一遍地预测 1-20 中的一个数字。我是体育馆库的新手，因此我不确定是什么导致了这个问题。我已经在 StackOverflow 上发布了这个问题，到目前为止我还没有收到太多帮助。 如能得到您的任何意见，我们将不胜感激。谢谢。    提交人    /u/Farenhytee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvoovw/using_gymnasium_to_train_an_action_classification/</guid>
      <pubDate>Fri, 05 Jul 2024 03:40:03 GMT</pubDate>
    </item>
    <item>
      <title>[META] 有学习 RL 的小组吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvdjcr/meta_group_for_learning_rl/</link>
      <description><![CDATA[嗨，正在寻找想要以自上而下的方式（先了解大局，然后了解细节）开始学习 RL 的人，例如 fast.ai 课程。谁都感兴趣？    提交人    /u/Designer-Air8060   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvdjcr/meta_group_for_learning_rl/</guid>
      <pubDate>Thu, 04 Jul 2024 18:17:30 GMT</pubDate>
    </item>
    <item>
      <title>在 Q-Learning 算法中加入专家知识</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvbgts/including_expert_knowledge_in_qlearning_algorithms/</link>
      <description><![CDATA[大家好 :) 您是否有任何经验或知道一些将专家知识纳入深度 q 学习算法的最佳实践方法？ 目前，我认为只是在重放缓冲区中预先包含一些状态、动作和奖励对。这是一种合适的方法吗？    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvbgts/including_expert_knowledge_in_qlearning_algorithms/</guid>
      <pubDate>Thu, 04 Jul 2024 16:48:23 GMT</pubDate>
    </item>
    <item>
      <title>“在 HATETRIS 中创下世界纪录”，Dave & Filipe 2022（AlphaZero 失败后高度优化的光束搜索）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</guid>
      <pubDate>Thu, 04 Jul 2024 16:14:25 GMT</pubDate>
    </item>
    <item>
      <title>“AlphaZero 的蒙特卡洛图搜索”，Czech 等人 2020 年（将树转换为 DAG 以节省空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</guid>
      <pubDate>Thu, 04 Jul 2024 16:01:49 GMT</pubDate>
    </item>
    <item>
      <title>“《赛道狂飙》中的机器学习历史”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dva3yv/the_history_of_machine_learning_in_trackmania/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dva3yv/the_history_of_machine_learning_in_trackmania/</guid>
      <pubDate>Thu, 04 Jul 2024 15:49:37 GMT</pubDate>
    </item>
    <item>
      <title>非平稳环境中的策略梯度算法收敛。嗨，我正在尝试阅读有关非平稳环境中的 PG。在这样的环境中如何保证收敛到局部最优。在这方面有没有好的教程或论文。谢谢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duyxd7/policy_gradient_algorithm_convergence_in_non/</link>
      <description><![CDATA[在非平稳环境中，还需要哪些其他因素来处理。     提交人    /u/aabra__ka__daabra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duyxd7/policy_gradient_algorithm_convergence_in_non/</guid>
      <pubDate>Thu, 04 Jul 2024 05:14:48 GMT</pubDate>
    </item>
    <item>
      <title>帮助 NEAT-python - 贪吃蛇游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duy1dz/help_with_neatpython_snake_game/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duy1dz/help_with_neatpython_snake_game/</guid>
      <pubDate>Thu, 04 Jul 2024 04:21:28 GMT</pubDate>
    </item>
    <item>
      <title>如果重量极化稳定下来，是不是很糟糕？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duxsdl/is_weight_polarization_bad_if_it_stabilizes/</link>
      <description><![CDATA[        提交人    /u/Breck_Emert   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duxsdl/is_weight_polarization_bad_if_it_stabilizes/</guid>
      <pubDate>Thu, 04 Jul 2024 04:06:53 GMT</pubDate>
    </item>
    <item>
      <title>扩散器/决策扩散器在什么样的视野上进行训练和生成？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1durt7n/what_horizon_does_diffuserdecision_diffuser_train/</link>
      <description><![CDATA[这里有人用过 Janner 的扩散器或 Ajay 的决策扩散器吗？ 我想知道他们为 d4rl 任务训练扩散模型的范围（即序列长度）是否与他们生成的计划的范围（序列长度）相同。  根据论文或代码库配置，目前尚不清楚；但直觉上我会想象，为了完成任务，生成的计划的序列长度应该比他们训练的序列长度更长，特别是如果训练序列最终没有达到目标或只是达到目标的序列的子集。    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1durt7n/what_horizon_does_diffuserdecision_diffuser_train/</guid>
      <pubDate>Wed, 03 Jul 2024 22:57:49 GMT</pubDate>
    </item>
    <item>
      <title>无法决定使用异步还是同步中间件来与 TensorFlow 模型交互自定义模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/</link>
      <description><![CDATA[大家好。 我目前正在 MARL n 上进行一个项目，我将在一个 Docker 容器中运行 Python 模拟，在另一个容器中运行 Tensorflow 模型。我知道通过使用 docker compose，我将创建一个包含这些容器的本地网络，但无法真正决定是使用消息队列来通信代理和模型 [他们将询问模型下一步要做什么操作，并且他们还将为其提供数据（无需任何响应）来训练它]；还是使用 Rest API + 模型从中消耗的缓冲区。 您能推荐我任何涉及该主题的参考资料吗？提前谢谢您，祝您有美好的一天     提交人    /u/Miss_Bat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/</guid>
      <pubDate>Wed, 03 Jul 2024 22:22:48 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 与 Jax 2024 在强化学习环境/代理方面的比较</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dupwpu/pytorch_vs_jax_2024_for_rl_environmentsagents/</link>
      <description><![CDATA[只是为了澄清一下。我正在编写一个自定义环境。RL 算法设置为在 JAX 中运行最快（例如稳定基线），因此即使在 Pytorch/JAX 中运行环境的速度一样快，使用 JAX 更明智，因为您可以直接传递数据，或者从 pytorch 到 cpu 再到 jax（用于训练代理）的数据传输速度如此之快，在增加的时间方面是微不足道的？ 或者 pytorch 生态系统是否足够强大，它与 jax 实现一样快    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dupwpu/pytorch_vs_jax_2024_for_rl_environmentsagents/</guid>
      <pubDate>Wed, 03 Jul 2024 21:33:18 GMT</pubDate>
    </item>
    </channel>
</rss>