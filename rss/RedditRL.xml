<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 11 Aug 2024 12:27:26 GMT</lastBuildDate>
    <item>
      <title>RL 模型中的状态变量是否需要直接更新方程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epek38/do_state_variables_in_rl_models_need_direct/</link>
      <description><![CDATA[我正在开发一个模拟模型，其中我正在使用 RL 优化目标函数。我的模拟提供了对许多状态变量的访问，但我的 RL 代理只需要这些状态的一个子集。 我不确定 RL 模型中的每个状态变量是否都需要使用我的状态空间中的可用状态进行直接更新方程。例如，我有一个“到达时间”变量 (T_{arrival})，它是根据车辆的当前位置和目的地计算得出的。如果我不在状态空间中包含位置和目的地，我是否仍可以使用 T_{arrival}，因为它是由模拟环境直接提供的？ 同样，对于充电基础设施，我有一个“充电站最早可用时间”的状态变量 (T_{charge}^{avail})。我是否需要在状态空间中包含每辆车的充电开始（T_{charge}^{start}）和结束（T_{charge}^{end}）时间来更新这一点，或者模拟可以通过内部计算状态来处理这个问题吗？ 是否有必要为每个状态变量提供明确的更新函数，或者我可以依靠模拟直接提供某些状态？    提交人    /u/Furious-Scientist   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epek38/do_state_variables_in_rl_models_need_direct/</guid>
      <pubDate>Sun, 11 Aug 2024 06:57:36 GMT</pubDate>
    </item>
    <item>
      <title>寻求强化学习算法改进研究的实验设置建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epayf7/seeking_advice_on_experimental_setup_for_rl/</link>
      <description><![CDATA[大家好， 我目前正在研究改进强化学习（RL）算法，非常希望能得到一些关于我的实验设置的建议。我计划在不同的任务中进行一系列 11 个实验：  **DeepMind Control Suite 任务：**  Cheetah-Run Finger-Spin Walker-Walk Walker-Stand Reacher-Easy Reacher-Hard  **PyBullet 环境：**  AntBulletEnv-v0 HalfCheetahBulletEnv-v0 HopperBulletEnv-v0 ReacherBulletEnv-v0 Walker2DBulletEnv-v0   我有访问权限到多台 Windows 和 Mac 计算机，我的计划是在不同的机器上运行每个实验/任务（使用多个种子和比较算法）。我的理由是算法在不同的硬件上的表现应该类似，并且由于将按实验/任务分析性能，所以这种方法应该没问题。 为了确保一致性，我在每台设备上创建了具有相同版本 Python 和其他依赖项（例如 PyTorch、TensorFlow）的 conda 环境。 **我的问题是：**  在多台设备上的这个实验设置正确吗？ 我选择的实验/任务是否适合评估和改进 RL 算法？  任何反馈或建议都将不胜感激！ 提前致谢！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epayf7/seeking_advice_on_experimental_setup_for_rl/</guid>
      <pubDate>Sun, 11 Aug 2024 03:21:45 GMT</pubDate>
    </item>
    <item>
      <title>绘制注意力图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epasze/plot_attention_map/</link>
      <description><![CDATA[      嗨！ 我最近读了一篇论文“仔细观察：弥合自我中心和我最近在“使用 Transformers 实现机器人操作的第三人称视角”上发表了一篇文章，对如何在没有 cls 标记的情况下绘制注意力图（如下图所示）感到好奇。 原始论文中的图 5 据我所知，绘制注意力图的大多数方法都是使用 cls 标记来绘制叠加图像。但是，本文中实现的视觉变换器不包含 cls 标记。 提前感谢您的时间和帮助！  这是官方实现的链接： https://github.com/jangirrishabh/look-closer.git    提交人    /u/UpperSearch4172   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epasze/plot_attention_map/</guid>
      <pubDate>Sun, 11 Aug 2024 03:13:41 GMT</pubDate>
    </item>
    <item>
      <title>利用建设性模拟进行强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eowptk/harnessing_constructive_simulations_for/</link>
      <description><![CDATA[        提交人    /u/chunkyks   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eowptk/harnessing_constructive_simulations_for/</guid>
      <pubDate>Sat, 10 Aug 2024 16:11:27 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型可以训练使用像搅拌机这样的复杂应用程序吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eourrs/can_a_rl_model_be_trained_to_use_complex_apps/</link>
      <description><![CDATA[我不是 ML 大师，我仍在学习，我对 RL 有一些兴趣。 RL 模型是否可以训练为使用复杂环境（如 blender）进行 3D 建模和动画以及其他复杂环境，还是不行？    提交人    /u/BEE_LLO   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eourrs/can_a_rl_model_be_trained_to_use_complex_apps/</guid>
      <pubDate>Sat, 10 Aug 2024 14:47:18 GMT</pubDate>
    </item>
    <item>
      <title>最后一年项目的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eoucj6/ideas_for_the_final_year_project/</link>
      <description><![CDATA[大家好，我正在攻读人工智能学士学位，我必须和我的团队一起完成最后一年的项目。我希望这个项目能与众不同，所以我联系了他们。所以任何关于这个项目的想法都会受到赞赏。大家如果能提供想法、方向、人工智能的任何分支、任何新问题、与现实生活相关的问题、任何大大小小的简单复杂的想法，都会受到赞赏。谢谢     提交人    /u/PerformanceChoice310   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eoucj6/ideas_for_the_final_year_project/</guid>
      <pubDate>Sat, 10 Aug 2024 14:28:25 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eopzn2/ppo_implementation/</link>
      <description><![CDATA[嗨，我是一名大学生，正在做我的期末项目，使用强化学习进行无人机轨迹优化。我以“H. Bayerlein、P. De Kerret 和 D. Gesbert 的《通过强化学习实现自主飞行基站轨迹优化》IEEE 第 19 届无线通信信号处理进展国际研讨会 (SPAWC)，2018 年，第 1-5 页”这篇论文为参考，并使用 DQN 和 PPO 实现了相同的功能。我需要证明 PPO&gt; DQN&gt; Q 学习是我的成果。但 PPO 实现的学习速度并不比 DQN 快。您能检查一下并建议一些修改，让 PPO 实现更好吗？这是我的 github 链接 https://github.com/Divakar070/UAV/tree/main/PPO 或者建议一种前进的方式以获得所需的输出。    提交人    /u/Adventurous_Emu_5287   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eopzn2/ppo_implementation/</guid>
      <pubDate>Sat, 10 Aug 2024 10:38:08 GMT</pubDate>
    </item>
    <item>
      <title>利用脉冲神经网络进行强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eonxac/reinforcement_learning_with_spiking_neural/</link>
      <description><![CDATA[大家好，我刚刚开始我的硕士论文，我的项目是实现两个脉冲神经网络模块。更准确地说，我有一个“清醒”阶段，在这个阶段，代理和模型与环境（开放式人工智能健身房）交互并相互交互。另一个阶段是“做梦”，在这个阶段，环境处于离线状态，代理从模型中学习。你对如何实现这两个 SNN（代理和模型）有什么想法吗？从某种意义上说，在实现之前，我如何想象我的网络？    提交人    /u/Embri21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eonxac/reinforcement_learning_with_spiking_neural/</guid>
      <pubDate>Sat, 10 Aug 2024 08:14:38 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 的学习速度真的很慢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eo8927/damn_reinforce_is_really_slow_at_learning/</link>
      <description><![CDATA[      https://preview.redd.it/q8993hr5tohd1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=68a054614d29fd1a2b647af9f2854651814891d0 这是一场 1v1v1 的游戏 - 我正在使用 Self-Play 训练强化算法，在最佳反应迭代 20 次之后，它需要 10000*200=200 万场游戏模拟，一场比赛需要 100 轮（1 天的计算），才能接近 33% 的获胜概率，对抗之前最佳反应的混合策略。 只是想分享，没有别的。RIP 我“只尝试我完全理解的简单算法”的方法，将不得不尝试使用 Actor Critic。我已经在使用基线，但它并没有多大帮助。    提交人    /u/Lindayz   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eo8927/damn_reinforce_is_really_slow_at_learning/</guid>
      <pubDate>Fri, 09 Aug 2024 19:12:35 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 代理永远不会学习，奖励也保持不变</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1envd6y/ddpg_agent_never_learns_and_rewards_remain_flat/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1envd6y/ddpg_agent_never_learns_and_rewards_remain_flat/</guid>
      <pubDate>Fri, 09 Aug 2024 09:28:17 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 解决追击-躲避游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1enkquu/use_rl_to_solve_a_pursuitevasion_game/</link>
      <description><![CDATA[我有一个博弈论问题，想看看强化学习能产生什么样的解决方案。问题如下：在平面上，一只狼 p_0 追逐两只羊 p_1 和 p_2。狼和羊没有大小。狼的速度矢量幅值为 2，羊的速度矢量幅值为 1。狼的决策函数为 a_0(p)，它将所有三个实体 p = (p_0, p_1, p_2) 的位置作为输入，并确定狼在当前时刻的速度方向 a_0(p)。两只羊也具有类似的决策函数 a_1 和 a_2。如果函数 a_i 的性质足够好，则微分方程组 dp_i/dt = v_i(p) 在给定初始条件的情况下具有唯一解。因此狼和羊的位置是明确定义的。假设经过T时间后，狼把两只羊都抓走了，狼的奖励是-T，羊的奖励是T，现在我们希望找到这个博弈的纳什均衡。 我对强化学习了解不多，想问一下哪些强化学习算法适合解决这个问题，应该从哪些方向找资料。    submitted by    /u/SignalLivid2685   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1enkquu/use_rl_to_solve_a_pursuitevasion_game/</guid>
      <pubDate>Thu, 08 Aug 2024 23:32:13 GMT</pubDate>
    </item>
    <item>
      <title>主题建议在 RL 中写解释</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1endfsq/topic_suggestions_to_wrie_explanation_in_rl/</link>
      <description><![CDATA[嗨，我计划为 RL 主题撰写带有编程示例的解释文章。我之前在 RL 工作，我的硕士论文是关于 IRL 和模仿学习的。我很喜欢 RL，有兴趣从 PPO、DQN、DDQN 等开始写一些文章。我想知道是否有人对 RL 中需要更多文章的特定内容感兴趣。    提交人    /u/Calm-Vermicelli1079   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1endfsq/topic_suggestions_to_wrie_explanation_in_rl/</guid>
      <pubDate>Thu, 08 Aug 2024 18:27:48 GMT</pubDate>
    </item>
    <item>
      <title>Actor 评论家算法的收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1endban/convergence_of_actor_critic_algorthim/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1endban/convergence_of_actor_critic_algorthim/</guid>
      <pubDate>Thu, 08 Aug 2024 18:22:51 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器：关于</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1en17o0/decision_transformer_a_basic_question_about/</link>
      <description><![CDATA[几个月前，我对 Decision Transformers 产生了浓厚的兴趣，尤其是在阅读了这里这篇文章之后。 因此，我阅读了这篇论文，观看了 YT 上的一些视频，并开始寻找真实的例子。 但即使阅读了这个 reddit 帖子，这个一个，这个一个和最后一个一个，有一个问题，我不是 100% 确定。问题是：由于 DT 需要大量离线数据进行训练，它们基本上就像过滤器一样，它们不会生成任何新数据，但它们会在最佳策略中“寻找”，哪个/什么动作会带来最高奖励。基本上它们不与任何环境交互。  这更有意义，因为 NLP 中的 Transformers 不是通过发明新词来生成新语言，而是使用字典中的现有单词通过一个接一个地输出单词来生成新句子。 因此，即使我真的对在 RL 中应用 Transformers 感兴趣，它们对于一般的 RL 问题也没有多大意义。 我发现了一篇关于 Online-DT 的论文。它看起来很有前途也很有趣，但基本上它只是为现有数据集添加了新的轨迹。 经过这么长的演讲：我是否忽略了关于 DT 的一个重要方面？ 非常感谢    提交人    /u/WilhelmRedemption   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1en17o0/decision_transformer_a_basic_question_about/</guid>
      <pubDate>Thu, 08 Aug 2024 09:06:23 GMT</pubDate>
    </item>
    <item>
      <title>RL 创建用于训练神经网络的优化器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1en0y54/rl_to_create_optimizers_for_training_neural_nets/</link>
      <description><![CDATA[大家好， 我最近读了一篇论文 **VeLO：通过扩展训练多功能学习优化器** (https://arxiv.org/abs/2211.09760)，该论文训练了一个灵活的神经网络优化器来优化多个不同的模型架构，而无需进行超参数调整。 因此，我直觉地认为，理论上，您可以将超参数搜索框定为 MDP，其中奖励是模型在 n 个时期后的负损失。我的意思是，对于较大的网络来说，这可能不可扩展，并且参数必须在训练过程中进行调整。 但是，您知道该领域有什么工作吗？使用 RL 动态高效地调整训练神经网络的超参数？ 此致敬意！    提交人    /u/No_Individual_7831   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1en0y54/rl_to_create_optimizers_for_training_neural_nets/</guid>
      <pubDate>Thu, 08 Aug 2024 08:48:43 GMT</pubDate>
    </item>
    </channel>
</rss>