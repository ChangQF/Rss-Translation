<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 23 May 2024 18:18:25 GMT</lastBuildDate>
    <item>
      <title>MDP 是否已经过时了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyz491/is_mdp_getting_obsolete/</link>
      <description><![CDATA[ 由   提交/u/SubstantialBed9917  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyz491/is_mdp_getting_obsolete/</guid>
      <pubDate>Thu, 23 May 2024 17:55:46 GMT</pubDate>
    </item>
    <item>
      <title>“Vernor Vinge 的小说《真名实姓》后记”，Minsky 1984（偏好学习和安全代理的挑战）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</guid>
      <pubDate>Thu, 23 May 2024 16:23:56 GMT</pubDate>
    </item>
    <item>
      <title>MDP：为失去迄今为止在游戏中积累的东西设置奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywuda/mdp_set_reward_for_losing_what_you_have/</link>
      <description><![CDATA[假设我有一个游戏，开始时奖励为 0。我可以采取以下两种行动之一：  玩：一个随机过程，我要么获得金钱，要么失去迄今为止的奖励  离开：退出游戏并保留我累积的奖励。   我正在尝试将此游戏建模为具有“开始”状态和“结束”状态的 MDP，其中结束状态的值为 0。开始”，您可以执行“播放”或“离开”操作。在“结束”时，游戏结束——无需采取任何行动。  我试图弄清楚两件事：  如何为我失去累积奖励的随机过程的结果设置奖励。  如何为随机过程的结果设置奖励。 p&gt; 如何设置戒烟奖励。   对于 1： 如果“start”的值为 V(in)，我认为对于我选择“play”的结果，失去我累积的奖励，我可以用“-V(in)”来代表奖励。  Q(in, play) 的方程如下所示：  sum(p(lose)*(-V(in) + V(end)) + p(win )*(R + V(in)));  对于 2： 对于 Q(in, quit) --&gt;最终，奖励为V(in)。这样，如果您退出，Q(in, quit) 始终等于您开始时的值。  我的结果似乎与我的目标不符，而且我似乎无法找出我的方法的问题。我正在考虑我的方法（1）可能不准确，因为V（in）并不代表我累积的奖励，而是代表我累积奖励的预期值。但我的头脑中并没有很好地具体化这一点。  我在 MDP 中设置奖励的方式是否存在任何明显的问题？或者甚至是我的整个 MDP 中的状态/操作？    由   提交 /u/Squamply   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywuda/mdp_set_reward_for_losing_what_you_have/</guid>
      <pubDate>Thu, 23 May 2024 16:21:02 GMT</pubDate>
    </item>
    <item>
      <title>Cartpole 返回奇怪的东西。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</link>
      <description><![CDATA[我正在从头开始制作一个 PPO 代理（没有 Torch，没有 TF），它进展顺利，直到 env 突然返回一个维度为 5 的二维列表， 4而不是4，经过一番调试后，我发现这可能不是我的错，因为我没有对回报进行分配或执行任何操作，它只是在随机时间范围内发生并破坏了我的整个事情。有人知道为什么吗？   由   提交 /u/Mehcoder1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</guid>
      <pubDate>Thu, 23 May 2024 12:10:48 GMT</pubDate>
    </item>
    <item>
      <title>代表萨顿和巴托的“杰克的汽车租赁”问题的动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyib25/representing_the_dynamics_in_sutton_and_bartos/</link>
      <description><![CDATA[大家好！我正在学习 Sutton 和 Barto，并已完成有关动态规划的第 4 章。本章中的一个早期示例涉及一家汽车租赁公司（问题文本此处截图供参考）。作者使用这个玩具问题来说明策略迭代。 书中介绍的策略迭代是一个相当简单的算法。我陷入困境的是以编程方式表示问题的动态，即函数 p(s&#39;, r|s,a)。我不是在问“我如何计算出这些概率”，而是在问“我如何计算出这些概率”。而是“如何将它们存储在代码中以便于使用？” 对于本书中前面的示例，在处理值函数时，我通常会编写一个类似  的函数p&gt; defdynamic(s, a): “”“返回一个类似 [(s&#39;, r, p)] 的列表，其中 s&#39;是下一个状态，r 是观察到的奖励，p 是从状态 s&quot;&quot;&quot; 开始采取操作 a 后观察到 r 并最终达到状态 s&#39; 的概率 这使得通过动态（s，a）中的 sp，r，p_spr_sa 计算可能观察到的奖励和可能的下一个状态的总和变得非常方便： 此函数有效“包含”是指。环境，因为它决定了环境将如何对代理的操作做出反应。 对于汽车租赁问题，我正在努力以这种格式表示动态。事情比前面的示例更复杂，因为从 (s,a) 到 (s&#39;, r, p) 的逻辑映射要复杂得多。如果我想像我一直在做的那样实现这个功能，我想我可以，但是我想知道是否有人对如何更好地实现这一点有建议。任何关于动力学的这种表示是如何“典型地”的想法都可以理解。完成了吗？非常感谢！   由   提交/u/GorillaManStan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyib25/representing_the_dynamics_in_sutton_and_bartos/</guid>
      <pubDate>Thu, 23 May 2024 02:38:28 GMT</pubDate>
    </item>
    <item>
      <title>基于强化学习的八旋翼飞行器控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cycb22/control_of_an_octoplcopter_based_on_rl/</link>
      <description><![CDATA[大家好，我正在写我的硕士论文，其中涉及使用 Simulink 和 MATLAB 控制同轴八轴飞行器降落在移动目标上。我的系统有 20 个观测值并生成 3 个动作：推力、x 输入和 y 输入。由于我有两个 PD 控制器，并且将输入添加到 Ux 和 Uy 以控制级联控制器设置中的横滚和俯仰，因此复杂性增加。我确信我的模型是正确的，但我已经努力了两周才让代理收敛。它始终陷入 Q 值恒定为正的次优策略（我使用的是 DDPG）。尽管尝试了多次奖励函数修改来使无人机跟踪轨迹，但每一集的奖励仍然非常负面并且没有改善。考虑到大量的观察和系统的复杂性，这项工作是否可行？或者我应该考虑简化环境？任何建议或见解将不胜感激。谢谢   由   提交 /u/OkFig243   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cycb22/control_of_an_octoplcopter_based_on_rl/</guid>
      <pubDate>Wed, 22 May 2024 21:47:23 GMT</pubDate>
    </item>
    <item>
      <title>寻找对我的论文的反馈：迷宫导航强化学习中的 Sim2Real 迁移</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cy80jk/looking_for_feedback_on_my_thesis_sim2real/</link>
      <description><![CDATA[TL;DR: 寻找有关我的论文的反馈，该论文有关 RL 中的 Sim2Real 传输用于迷宫导航。 Google 表单或在下面发表评论。 GitHub 项目 | 论文草稿。  大家好， 我是 Lucas，目前正在为人工智能工程学士学位撰写论文，我正在寻找有关我的学士论文的反馈，题为“探索 Sim2Real 迁移在迷宫导航强化学习中的可行性”。 这些是一些“简单”的问题，因为我需要从人们那里得到一些外部反馈:) 注意：这个项目的实践时间仅限于 3 周，所以我没有有很多时间去尝试，这影响了我很多的选择。我现在正在考虑反思这些。 当前设置：  自定义 OpenAI Gym 环境及操作：前进、左、右 DFRobot 2WD miniQ 机器人底盘上的遥控车 3 个 HC-SR04 传感器用于墙距测量 DDQN 代理  有关该项目的更多信息（存储库有点混乱，提前抱歉）：Github 项目 或者您可以阅读我的论文草稿 问题：   哪种遥控汽车设置（2WD 与 4WD）更适合我的项目目标和方法？您推荐的任何具体型号或品牌以及原因？ 我的遥控汽车在这个项目中受到限制吗？ （两轮驱动且相当大）如果是，请解释如何实现。 哪种传感器/遥控汽车套件可能是更好的选择？ （例如，使用相机代替 HC-SR04） 我应该扩大虚拟环境中的动作空间吗？例如，让 OpenAI Gym 环境直接访问电机，而不是限制其向前、向右和向前移动。 您认为与使用自上而下的虚拟双胞胎相比，虚拟双胞胎能增加更多价值吗？查看摄像头可以帮助找到遥控汽车的位置（作为代理/环境的额外输入）？如果是，为什么？ 您认为我的研究成果有哪些实际应用？我的工作可能会对哪些特定行业或应用产生重大影响？  请填写此Google 表单或回答评论中的问题。感谢您的时间和反馈！或者，如果您有任何其他问题/反馈，我会非常高兴听到！！ 提前谢谢您:)   由   提交 /u/Practical-Apricot-71   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cy80jk/looking_for_feedback_on_my_thesis_sim2real/</guid>
      <pubDate>Wed, 22 May 2024 18:49:16 GMT</pubDate>
    </item>
    <item>
      <title>如何为 RL 创建基于神经网络的健身房环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxzuam/how_to_create_neural_network_based_gym/</link>
      <description><![CDATA[ 由   提交 /u/Past-News-1373   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxzuam/how_to_create_neural_network_based_gym/</guid>
      <pubDate>Wed, 22 May 2024 13:09:15 GMT</pubDate>
    </item>
    <item>
      <title>非对称游戏的单一模型还是多个模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxzli6/single_model_or_multiple_models_for_asymmetric/</link>
      <description><![CDATA[我对训练某些多人游戏的代理感兴趣。我对一些棋盘游戏特别感兴趣，在这些游戏中，玩家可以采取不同的行动，甚至可能有不同的获胜条件。  我的问题是为每个玩家训练一个网络还是为所有玩家训练一个网络是否更有意义。我计划使用自我对弈（我猜如果涉及多个模型，你就不能称之为自我对弈，但你明白了）和 MCTS 来训练模型，就像《Alpha Zero》中的那样。 &lt;我看到的大多数文献和例子都集中在两人游戏和相当对称的游戏上，比如国际象棋和围棋，其中唯一的不对称是游戏顺序。在这些游戏中，双方使用相同的网络非常简单，但我想知道具有不同动作空间的 3 人或 4 人游戏是否仍然如此。  考虑像 PPO 这样的 actor-critic 模型，使用策略的动作掩码和每个玩家一个值是很容易实现的。我主要想知道使用单个模型学习不同策略的影响。   由   提交 /u/OperaRotas   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxzli6/single_model_or_multiple_models_for_asymmetric/</guid>
      <pubDate>Wed, 22 May 2024 12:57:36 GMT</pubDate>
    </item>
    <item>
      <title>我有一个深度强化学习 PPO 代理，它使用离散状态并输出离散动作。我正在考虑将其转换为 DRL 转换器。在标记状态或任何其他方法方面，我应该如何前进？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxxfrn/i_have_a_deep_reinforcement_learning_ppo_agent/</link>
      <description><![CDATA[嗨， 我目前有一个正在运行的 PPO 代理，但我正在考虑将我的环境扩展为更复杂的环境。因此，我正在考虑将当前的 PPO 代理转换为 PPO + 转换器代理。 状态类型：离散 动作类型：离散 我对转换器的结构进行了一些研究，但仍有几个问题： 1) 如何标记离散状态？ （我可以将状态转换成一个句子，然后将其传递到转换器中，但我认为这不是一个好方法，因为它涉及转换为字符串然后再转换回数字。） 2）如何将转换器输出转换回状态？ 3）我应该使用哪种转换器类型：仅编码器、编码器解码器还是仅解码器？ 我是转换器的新手，所以任何帮助或建议都会非常有帮助。 提前致谢！    提交人    /u/Apprehensive_Bag1262   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxxfrn/i_have_a_deep_reinforcement_learning_ppo_agent/</guid>
      <pubDate>Wed, 22 May 2024 10:59:23 GMT</pubDate>
    </item>
    <item>
      <title>DQN 显示损失有所改善，但缺乏奖励改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxws11/dqn_showing_loss_improvements_but_lacking_reward/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxws11/dqn_showing_loss_improvements_but_lacking_reward/</guid>
      <pubDate>Wed, 22 May 2024 10:15:17 GMT</pubDate>
    </item>
    <item>
      <title>健身房环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</link>
      <description><![CDATA[我有实时系统的数据集，我可以使用强化学习算法的数据表创建基于深度神经网络的环境吗？ 请尽快回复。紧急，陷入项目工作   由   提交 /u/Past-News-1373   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</guid>
      <pubDate>Wed, 22 May 2024 09:14:03 GMT</pubDate>
    </item>
    <item>
      <title>很难阅读大多数 RL 算法的 CS 符号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</link>
      <description><![CDATA[在观看带有示例的视频后，我理解了贝尔曼方程，但是有没有更直观的方法来理解任何论文中的任何政策制定。并不是所有的论文都能很好地打破他们的方程的作用，而是让比我智力更高的人尝试从论文中挖掘它 &lt;！-- SC_ON - -&gt;  由   提交 /u/baboolasiquala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</guid>
      <pubDate>Wed, 22 May 2024 04:16:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么是零和？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</link>
      <description><![CDATA[在很多论文中，你都可以看到“因为游戏是零和的”、“我们以零和的方式解决问题”之类的短语博弈”等，但为什么零和很重要？零和的哪些属性使训练变得更容易？ 例如，在 2P 设置中，很容易看出无论玩家 A 赚到什么，玩家 B 都会输。多人（&gt;2）游戏怎么样？ 再举一个例子，想象一个玩家拥有能力的游戏。玩家 A 有能力为自己生产 10 块金币。 B 从另一位选择的玩家那里偷走了 5 个金币。 C使所有其他玩家失去5金。这显然不是零和游戏，但是你不能按照正常方式训练代理吗？   由   提交 /u/HyogoKita19C   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</guid>
      <pubDate>Wed, 22 May 2024 01:07:16 GMT</pubDate>
    </item>
    <item>
      <title>棋盘游戏神经网络架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</link>
      <description><![CDATA[有人有过在棋盘游戏中尝试不同神经网络架构的经验吗？ 目前使用 PPO 进行数独 - 输入 I我正在考虑只是一个展平的板向量，因此神经网络是一个简单的 MLP。但我没有得到很好的结果——想知道 MLP 架构是否是问题所在？  AlphaGo 论文使用了 CNN，很想知道你们尝试过什么。感谢任何建议    由   提交 /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</guid>
      <pubDate>Tue, 21 May 2024 23:07:06 GMT</pubDate>
    </item>
    </channel>
</rss>