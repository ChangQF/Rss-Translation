<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 16 Oct 2024 15:23:16 GMT</lastBuildDate>
    <item>
      <title>Unity ML 代理和贪吃蛇等游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4xhqd/unity_ml_agents_and_games_like_snake/</link>
      <description><![CDATA[大家好， 我一直在尝试理解神经网络和游戏 AI 的训练。但我目前在努力玩 Snake。我想“好吧，让我们给它一些射线传感器、一个摄像头传感器，吃食物时给予奖励，与自身或墙壁碰撞时给予负面奖励”。 我想说它学得很好，但并不完美！在 10x10 的游戏场中，它的最高分约为 50，但到目前为止它从未掌握游戏。 有人能给我一些建议或线索，如何更好地处理使用 PPO 进行蛇 AI 训练吗？ 射线传感器可以检测墙壁、蛇本身和食物（3 个不同的传感器，每个传感器有 16 条射线） 摄像头传感器的分辨率为 50x50，也可以看到墙壁、蛇头以及蛇周围的蛇尾。它是一个尺寸为 8 的正交相机，因此它可以看到整个运动场。 首先，我只使用射线传感器进行测试，然后我添加了相机传感器，我可以说的是，它通过相机视觉观察学习得更快，但最后它的最高分大约相同。 我正在并行训练 10 个代理。 网络设置为： 50x50x1 视觉观察输入 大约 100 个射线观察输入 512 个隐藏神经元 2 个隐藏层 4 个离散输出动作 我目前正在尝试使用 buffer_size 为 25000 和 batch_size 为 2500。学习率为 0.0003，Num Epoch 为 3。时间范围设置为 250。 是否有人使用过 Unity 的 ML Agents Toolkit 并能帮助我一点？ 我做错了什么吗？ 我将感谢你们给予我的每一次帮助！ 这里有一个小视频，你可以在其中看到大约第 150 万步的培训： https://streamable.com/tecde6    提交人    /u/Seismoforg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4xhqd/unity_ml_agents_and_games_like_snake/</guid>
      <pubDate>Wed, 16 Oct 2024 11:51:50 GMT</pubDate>
    </item>
    <item>
      <title>修改策略迭代？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4ukf4/modified_policy_iteration/</link>
      <description><![CDATA[我是强化学习的新手，仍在学习中。我现在正在学习策略迭代和值迭代。 因此，据我所知，在策略迭代中，我们首先通过获取所有状态的状态值函数来评估当前策略，然后使用它们进行贪婪操作更新策略，然后通过再次获取所有状态的状态值函数来评估更新后的策略，并对其进行迭代，直到获得最佳策略。 我阅读了关于修改后的策略迭代的文章，对此我的看法不一。我现在可以看到两种方法：  修改后的策略迭代就是策略迭代，只不过我们只进行 k 次迭代？ 我们只评估部分状态？  我之所以问这个问题，是因为从我读到的内容来看，第一种方法似乎是正确的，但是我在使用的书中看到的图表和其他人的解释（他现在也是第一次学习 RL）表明它是第二种方法。    提交人    /u/AdBitter9336   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4ukf4/modified_policy_iteration/</guid>
      <pubDate>Wed, 16 Oct 2024 08:31:37 GMT</pubDate>
    </item>
    <item>
      <title>如何应对SAC的灾难性遗忘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4tklf/how_to_deal_with_the_catastrophic_forgetting_of/</link>
      <description><![CDATA[      嗨！ 我建立了一个使用SAC进行训练的自定义任务。成功率曲线在稳步上升后逐渐下降。在查阅了一些相关讨论后，我发现这种现象可能是灾难性的遗忘。 https://preview.redd.it/i5bxwet9j2vd1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=81ea533917317b57ebd924668f24fdd59e275c43 我尝试规范奖励并自动调整 alpha 的值来控制探索和利用之间的平衡。其次，我还降低了 actor 和 critic 的学习率，但这只会减慢学习过程并降低整体成功率。 我想得到一些关于如何进一步稳定这个训练过程的建议。 提前感谢您的时间和帮助！    提交人    /u/UpperSearch4172   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4tklf/how_to_deal_with_the_catastrophic_forgetting_of/</guid>
      <pubDate>Wed, 16 Oct 2024 07:13:25 GMT</pubDate>
    </item>
    <item>
      <title>什么可能导致我的 Q-Loss 值出现分歧（SAC + Godot <-> Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4t57g/what_could_be_causing_my_qloss_values_to_diverge/</link>
      <description><![CDATA[TLDR; 我正在开发一个使用 SAC 的 PyTorch 项目，类似于我的一个旧 Tensorflow 项目：https://www.youtube.com/watch?v=Jg7\_PM-q\_Bk。我无法让它与 PyTorch 一起工作，因为我的 Q-Loses 和 Policy 损失要么增长，要么收敛到 0 太快。你知道为什么会这样吗？  我已经在 Godot 中创建了一个游戏，通过套接字与 SAC 的 PyTorch 实现进行通信：https://github.com/philipjball/SAC_PyTorch 游戏是： 代理需要靠近目标，但它没有自己的位置或目标位置作为输入，而是有 6 个输入，表示目标与代理在特定角度的距离。始终有且只有 1 个输入的值不为 1。 代理输出 2 个值：移动的方向和沿该方向移动的幅度。 输入在 [0,1] 范围内（由最大距离标准化），2 个输出在 [-1,1] 范围内。 奖励为： score = -distance if score &gt;= -300: score = (300 - abs(score )) * 3 score = (score / 650.0) * 2 # 650 是最大距离，100 是每步的最大范围 return score * abs(score )  问题是： 两个评论家和策略的 Q-Loss 都在随着时间的推移缓慢增长。我尝试了几种不同的网络拓扑，但层数或每层中的节点数似乎对 Q-Loss 没有影响 我能做的最好的就是让奖励非常小，但这会导致 Q-Loss 和 Policy loss 收敛到 0，即使代理没有学到任何东西。 如果您做到了这一点，并且有兴趣提供帮助，我很乐意向您支付导师的费用，通过屏幕共享电话审查我的方法，并帮助我更好地了解如何让 SAC 代理工作。 提前谢谢您！！    提交人    /u/stokaty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4t57g/what_could_be_causing_my_qloss_values_to_diverge/</guid>
      <pubDate>Wed, 16 Oct 2024 06:41:22 GMT</pubDate>
    </item>
    <item>
      <title>我使用深度强化学习（使用 Unity ML Agents）制作了一个消防员 AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4iy4q/i_made_a_firefighter_ai_using_deep_rl_using_unity/</link>
      <description><![CDATA[视频链接：https://www.youtube.com/watch?v=REYx9UznOG4 我之前做过这个视频，花了好几个小时才制作出来，却没有人关注，这让我很沮丧，所以我现在在攻读人工智能博士学位，而不是成为一名 YouTuber，哈哈。 我想如果人们觉得它很有趣，现在为它做广告也不错。我确保添加了一些旁白和有趣的部分，这样它就不会无聊了。我希望这里的一些人能觉得它和我做这个项目一样有趣。 我对这个主题很感兴趣，所以如果有人有问题，我会在有时间的时候回答他们:D   由    /u/usernumero  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4iy4q/i_made_a_firefighter_ai_using_deep_rl_using_unity/</guid>
      <pubDate>Tue, 15 Oct 2024 21:28:04 GMT</pubDate>
    </item>
    <item>
      <title>NoisyLinears 之后的 LayerNor/Adanorm？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4gbc7/layernoradanorm_after_noisylinears/</link>
      <description><![CDATA[除了最后一个噪声输出层之外，对 NN 中的所有噪声层应用层范数或 adanorm 有什么想法/经验吗？ 任何一个范数层基本上都会扼杀噪声线性/探索吗？    提交人    /u/dekiwho   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4gbc7/layernoradanorm_after_noisylinears/</guid>
      <pubDate>Tue, 15 Oct 2024 19:36:13 GMT</pubDate>
    </item>
    <item>
      <title>“解读 DPO 和 PPO：从偏好反馈中解开学习的最佳实践”，Ivison 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4cnnx/unpacking_dpo_and_ppo_disentangling_best/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4cnnx/unpacking_dpo_and_ppo_disentangling_best/</guid>
      <pubDate>Tue, 15 Oct 2024 17:02:15 GMT</pubDate>
    </item>
    <item>
      <title>Simba：深度强化学习中扩大参数的简单性偏差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g460jl/simba_simplicity_bias_for_scaling_up_parameters/</link>
      <description><![CDATA[      想要更快、更智能的强化学习？查看 SimBa – 我们可疯狂扩展的新架构！ 📄 项目页面：https://sonyresearch.github.io/simba 📄 arXiv：https://arxiv.org/abs/2410.09754 🔗 代码：https://github.com/SonyResearch/simba 🚀 厌倦了深度 RL 中缓慢的训练时间和不尽人意的结果？ 使用 SimBa，您可以毫不费力地扩展参数并达到最先进的性能 - 而无需更改核心 RL 算法。 💡 它是如何工作的？ 只需将您的 MLP 网络换成 SimBa，然后观看奇迹发生！在单个 Nvidia RTX 3090 上，只需 1-3 小时，您就可以训练出在 DMC、MyoSuite 和 HumanoidBench 等基准测试中表现最佳的代理。 🦾 ⚙️ 为什么它很棒： 即插即用，支持 SAC、DDPG、TD-MPC2、PPO 和 METRA 等 RL 算法。 无需调整您最喜欢的算法 - 只需切换到 SimBa 并让扩展能力接管即可。 训练更快、更智能、更好 - 非常适合研究人员、开发人员和任何探索深度 RL 的人！ 🎯 立即尝试并观察您的 RL 模型演变！ https://i.redd.it/olxmmgyauwud1.gif    提交人    /u/joonleesky   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g460jl/simba_simplicity_bias_for_scaling_up_parameters/</guid>
      <pubDate>Tue, 15 Oct 2024 12:03:38 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型玩星际争霸 II：基准测试和总结链方法”，Ma 等人 2023 年（让 LLM 玩星际争霸的文本）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3sqr7/large_language_models_play_starcraft_ii/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3sqr7/large_language_models_play_starcraft_ii/</guid>
      <pubDate>Mon, 14 Oct 2024 22:32:24 GMT</pubDate>
    </item>
    <item>
      <title>“具身代理界面：具身决策的 LLM 基准测试”，Li 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3p9cr/embodied_agent_interface_benchmarking_llms_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3p9cr/embodied_agent_interface_benchmarking_llms_for/</guid>
      <pubDate>Mon, 14 Oct 2024 20:02:46 GMT</pubDate>
    </item>
    <item>
      <title>如何训练代理进行任意长度的二进制加法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3hdrt/how_to_train_an_agent_to_do_binary_addition_of/</link>
      <description><![CDATA[大家好。 这个问题突然出现在我的脑海里，我知道它可能有点琐碎，但我很想知道答案。    提交人    /u/blablawawawa   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3hdrt/how_to_train_an_agent_to_do_binary_addition_of/</guid>
      <pubDate>Mon, 14 Oct 2024 14:40:44 GMT</pubDate>
    </item>
    <item>
      <title>TorchRL 中针对 MARL 的动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3cjnw/action_masking_in_torchrl_for_marl/</link>
      <description><![CDATA[您好！我目前正在使用 TorchRL 解决我的 MARL 问题。我使用的是自定义 pettingzoo 环境和 pettingzoo 包装器。我的自定义环境的观察结果中包含一个动作掩码。在 TorchRL 中处理它的最简单方法是什么？因为我觉得 MultiAgentMLP 和 ProbabilisticActor 不能与动作掩码一起使用，对吗？ 谢谢！    提交人    /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3cjnw/action_masking_in_torchrl_for_marl/</guid>
      <pubDate>Mon, 14 Oct 2024 10:22:58 GMT</pubDate>
    </item>
    <item>
      <title>适合我的强化学习项目的 ubuntu/ROS2/Gazebo 版本</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3blyl/suitable_ubunturos2gazebo_versions_for_my/</link>
      <description><![CDATA[大家好，我将在 Gazebo 模拟器中对 epuck 模型机器人进行强化学习（我有一个来自 Gazebo Classic 的 urdf 模型，我必须适应新版本），我对 ros2 和 Gazebo 有基本的先验知识，但我想知道适合我的项目的版本，它是关于使用 RL 技术进行自主导航，我将非常感谢您的帮助。    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3blyl/suitable_ubunturos2gazebo_versions_for_my/</guid>
      <pubDate>Mon, 14 Oct 2024 09:12:31 GMT</pubDate>
    </item>
    <item>
      <title>不同的 RL 算法真的有很大影响吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</link>
      <description><![CDATA[我现在正在进行 RL 项目来解决组合优化问题，由于复杂的约束，这个问题很难用数学来表达。我正在使用 A2C 训练我的代理，这是最简单的入门方法。 我只是想知道其他算法（如 TRPO、PPO）在实践中是否真的效果更好，而不是像在基准测试中那样。 有没有人尝试过 SOTA 算法（论文中声称）并真的看到了差异？ 我觉得设计奖励比算法本身重要得多。    提交人    /u/Electronic_Estate854   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</guid>
      <pubDate>Mon, 14 Oct 2024 01:53:08 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND：世界建模的扩散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</link>
      <description><![CDATA[DIAMOND 💎 世界建模的扩散：Atari 中的视觉细节很重要 项目网页：https://diamond-wm.github.io/ 代码、代理和可玩世界模型：https://github.com/eloialonso/diamond 论文：https://arxiv.org/pdf/2405.12399 摘要  RL 代理是由 REINFORCE 训练的演员-评论家。  演员和评论家网络除最后一层外共享权重。这些共享层由一个卷积“主干”和一个 LSTM 单元组成。卷积主干有四个带有 2x2 最大池化的残差块。 每次训练运行都需要 500 万帧，在一台 Nvidia RTX 4090 上持续 12 天。  世界模型是一个带有 U-Net 2D 的 2D 扩散模型。它不是潜在扩散模型。它直接从视频游戏中生成帧。 该模型将最后 4 帧和动作以及扩散噪声水平作为条件。 在 RTX 3090 上以 ~10 FPS 运行。 他们使用 EDM 采样器从扩散模型中采样，即使每帧只有 1 个扩散步骤，它仍然可以很好地训练 RL 代理。     由    /u/furrypony2718  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</guid>
      <pubDate>Mon, 14 Oct 2024 01:13:23 GMT</pubDate>
    </item>
    </channel>
</rss>