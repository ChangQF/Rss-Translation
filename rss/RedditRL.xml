<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚çš„ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä½³åœ°è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Fri, 22 Mar 2024 09:14:58 GMT</lastBuildDate>
    <item>
      <title>éœ€è¦ DDQN è‡ªåŠ¨é©¾é©¶æ±½è½¦é¡¹ç›®çš„å¸®åŠ©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</link>
      <description><![CDATA[      æˆ‘æœ€è¿‘å¼€å§‹å­¦ä¹ å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘ç”¨ddqnåšäº†ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶æ±½è½¦é¡¹ç›®ï¼Œè¾“å…¥æ˜¯è¿™äº›å…‰çº¿çš„é•¿åº¦è¾“å‡ºæ˜¯å‘å‰ã€å‘åã€å‘å·¦ã€å‘å³ï¼Œä»€ä¹ˆéƒ½ä¸åšã€‚æˆ‘çš„é—®é¢˜æ˜¯ rl Agent éœ€è¦å¤šå°‘æ—¶é—´æ¥å­¦ä¹ ï¼Ÿå³ä½¿å·²ç»æ’­å‡ºäº† 40 é›†ï¼Œå®ƒä»ç„¶æ²¡æœ‰è¾¾åˆ°å¥–åŠ±é—¨æ§›ã€‚æˆ‘è¿˜æ ¹æ®å‰è¿›é€Ÿåº¦ç»™äºˆ 0-1 å¥–åŠ±   ç”±   æäº¤/u/Invicto_50  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</guid>
      <pubDate>Fri, 22 Mar 2024 02:31:00 GMT</pubDate>
    </item>
    <item>
      <title>â€œRewardBenchï¼šè¯„ä¼°è¯­è¨€å»ºæ¨¡çš„å¥–åŠ±æ¨¡å‹â€ï¼ŒLambert ç­‰äºº 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkmtdy/rewardbench_evaluating_reward_models_for_language/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkmtdy/rewardbench_evaluating_reward_models_for_language/</guid>
      <pubDate>Fri, 22 Mar 2024 00:56:53 GMT</pubDate>
    </item>
    <item>
      <title>å¤„ç† Deep Q ç½‘ç»œä¸­ä¸åŒå¤§å°çš„çŠ¶æ€</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkj8ve/dealing_with_states_of_varying_size_in_deep_q/</link>
      <description><![CDATA[é—®å€™ï¼Œ â€‹ æˆ‘æ˜¯å¼ºåŒ–å­¦ä¹ æ–°æ‰‹ï¼Œæˆ‘å†³å®šç”¨ Python åˆ¶ä½œä¸€ä¸ªç®€å•çš„è´ªåƒè›‡æ¸¸æˆï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥è®­ç»ƒ DQN ä»£ç†æ¥ç©å®ƒã€‚åœ¨æ¸¸æˆçš„çŠ¶æ€è¡¨ç¤ºä¸­ï¼Œæˆ‘ä¼ é€’ç»™å®ƒçš„å˜é‡ä¹‹ä¸€æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰ Snake å½“å‰ä½ç½®çš„åˆ—è¡¨ï¼ˆå³ï¼ŒSnake ä¸»ä½“å æ®çš„æ¯ä¸ªä½ç½®éƒ½æœ‰ä¸€ä¸ªå…ƒç»„ (x,y)ï¼‰ã€‚åœ¨è®­ç»ƒä¸­ï¼Œä¸€æ—¦ Snake åƒæ‰é£Ÿç‰©é¢—ç²’å¹¶é•¿å¤§ï¼Œä»£ç†æ€»æ˜¯ä¼šå´©æºƒï¼Œå› ä¸ºçŠ¶æ€å¤§å°ä¸åˆå§‹å€¼ä¸åŒã€‚ â€‹ æˆ‘åœ¨äº’è”ç½‘ä¸Šæœç´¢äº†è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•ã€‚  ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯åœ¨çŠ¶æ€ä¸Šä»…è¡¨ç¤ºè›‡çš„å¤´ï¼Œå¹¶æ·»åŠ å››ä¸ªå˜é‡æ¥åˆ¤æ–­æ˜¯å¦æœ‰ä¸Š/ä¸‹ã€å·¦/å³éšœç¢ç‰©ã€‚è¿™ä¸ªè§£å†³æ–¹æ¡ˆä¼¼ä¹å¹¶æ²¡æœ‰æ•è·æ‰€æœ‰çš„åŸºæœ¬ä¿¡æ¯ï¼Œæ‰€ä»¥æˆ‘æ€€ç–‘ä»£ç†å³ä½¿è®­ç»ƒäº†æ•°åƒå¹´ä¹Ÿæ— æ³•å‘æŒ¥æœ€ä½³æ€§èƒ½ã€‚  å¦ä¸€ä¸ªè§£å†³æ–¹æ¡ˆæ˜¯å°†è›‡çš„èº«ä½“è¡¨ç¤ºä¸ºé•¿åº¦ç­‰äºå…¶æœ€å¤§å¯å®ç°å¤§å°çš„åˆ—è¡¨ï¼Œè¿™ç¡®å®æ•è·äº†æ‰€æœ‰åŸºæœ¬ä¿¡æ¯ï¼Œä½†å¦‚æœæˆ‘å°†åœ°å›¾å¤§å°å¢åŠ åˆ°å¤§å€¼ï¼Œå¯èƒ½ä¼šå‡æ…¢è¯¥è¿‡ç¨‹. â€‹ æˆ‘æƒ³çŸ¥é“ï¼Œæœ‰æ²¡æœ‰åŠæ³•å¤„ç†æ·±åº¦ Q ç½‘ç»œä¸­ä¸åŒå¤§å°çš„çŠ¶æ€ï¼Ÿç»™ä»£ç†çš„åˆå§‹çŠ¶æ€å¤§å°æ˜¯å¦å®šä¹‰äº†æ‰€æœ‰åç»­çŠ¶æ€çš„å¤§å°ï¼Ÿ   ç”±   æäº¤ /u/Clovergheister   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkj8ve/dealing_with_states_of_varying_size_in_deep_q/</guid>
      <pubDate>Thu, 21 Mar 2024 22:18:15 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘åº”è¯¥ç»§ç»­è®­ç»ƒä»–å—ï¼Ÿå› ä¸ºæ˜¾ç„¶ä»–ä¸æƒ³...</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkghvu/should_i_continue_training_him_cause_apparently/</link>
      <description><![CDATA[   /u/DisDoh  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkghvu/should_i_continue_training_him_cause_apparently/</guid>
      <pubDate>Thu, 21 Mar 2024 20:26:39 GMT</pubDate>
    </item>
    <item>
      <title>æ˜¯å¦æœ‰åŸºäº JAX çš„ Arcade å­¦ä¹ ç¯å¢ƒå®ç°ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bka5nu/are_there_any_jaxbased_implementations_of_arcade/</link>
      <description><![CDATA[Arcade å­¦ä¹ ç¯å¢ƒæœ‰åŸºäº JAX çš„å®ç°å—ï¼Ÿæˆ‘æƒ³ä½¿ç”¨ PureJaxRL åœ¨ GPU ä¸ŠåŠ é€Ÿ ALE ç¯å¢ƒï¼Œä¾‹å¦‚ Gymnax/BRAX ç¯å¢ƒï¼Ÿ   ç”±   æäº¤/u/C7501  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bka5nu/are_there_any_jaxbased_implementations_of_arcade/</guid>
      <pubDate>Thu, 21 Mar 2024 16:07:42 GMT</pubDate>
    </item>
    <item>
      <title>SACå®æ–½</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk944r/sac_implementation/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘æ­£åœ¨å°è¯•åœ¨æ²¡æœ‰ä»·å€¼ç½‘ç»œçš„æƒ…å†µä¸‹å®ç° Soft Actor Critic ç®—æ³•ï¼Œæˆ‘æƒ³æˆ‘å·²ç»æ¥è¿‘å¹¶ç›¸ä¿¡æˆ‘çš„æ‰€æœ‰æ¢¯åº¦éƒ½æ˜¯æ˜¯çš„ï¼Œä½†æˆ‘æ‰¾ä¸åˆ°é—®é¢˜ã€‚  æˆ‘æŒ‰ç…§ spin-up çš„è§£é‡Šé˜…è¯»äº†åŸæ–‡ã€‚æˆ‘çš„ä»£ç å—åˆ° youtube é¢‘é“â€œä¸ Phil çš„æœºå™¨å­¦ä¹ â€çš„å¯å‘ â€‹ æˆ‘å°è¯•åœ¨ Pendulum-v1 ç¯å¢ƒä¸Šè¿è¡Œå®ƒï¼Œä½†å®ƒä¸æ”¶æ•›å¹¶ä¸”æœ€ç»ˆæ€»æ˜¯é€‰æ‹©ç›¸åŒçš„æ“ä½œã€‚ æœ‰äººå¯ä»¥å¸®åŠ©æˆ‘å¤„ç†æˆ‘çš„ä»£ç å—ï¼Ÿ  def learn(self): if self.memory.mem_cntr &lt; p&gt;â€‹ class ActorNetwork(keras.Model): def init(self, n_actions, Noise=1e-6): super(ActorNetwork, self).init() self. n_actions = n_actions self.noise = å™ªå£° self.fc1 = Dense(400, æ¿€æ´»=â€œreluâ€) self.fc2 = Dense(200, æ¿€æ´»=â€œreluâ€) self.mu = Dense(n_actions, æ¿€æ´»=æ— ) self.sigma = Dense(n_actions,activation=None) def call(self, state): value = self.fc1(state) value = self.fc2(value) mu = self.mu(value) sigma = tf.clip_by_value( self.sigma(value), self.noise, 1) return mu, sigma def sample_normal(self, state): mu, sigma = self.call(state) # print(tf.reduce_mean(mu).numpy(), tf .reduce_mean(sigma).numpy()) action = tf.random.normal(mu.shape, mu, sigma, dtype=tf.float32) # log_prob = tf.math.log(tf.math.exp(-0.5 * tf.math.pow((action - mu) / sigma, 2)) / (sigma*tf.math.sqrt(2*NP_PI))) log_prob = -0.5 * tf.math.pow((action-mu)/ sigma, 2) - tf.math.log(sigma*tf.math.sqrt(2*NP_PI)) # åªæ˜¯é«˜æ–¯åˆ†å¸ƒçš„ç®€åŒ– log_prob -= tf.math.log(1-tf.math.square(tf.math) .tanh(action))) # è¯·å‚é˜… Haarnoja2019â€œé™„å½• C æ‰§è¡Œæ“ä½œè¾¹ç•Œâ€è¿”å›æ“ä½œï¼Œlog_prob    ç”±   æäº¤/u/antobom  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk944r/sac_implementation/</guid>
      <pubDate>Thu, 21 Mar 2024 15:23:11 GMT</pubDate>
    </item>
    <item>
      <title>æ–¯ç“¦äºšç‰¹æœºå™¨äºº|å°åº¦ |æå…¶åŠ¨æ€å¤æ‚çš„äº¤é€šåŠ¨æ€</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk80o9/swaayatt_robots_india_extremely_dynamiccomplex/</link>
      <description><![CDATA[       ç”±   æäº¤/u/shani_786  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk80o9/swaayatt_robots_india_extremely_dynamiccomplex/</guid>
      <pubDate>Thu, 21 Mar 2024 14:36:58 GMT</pubDate>
    </item>
    <item>
      <title>DDPGç®—æ³•ä¼ªä»£ç latex</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk2jdg/ddpg_algorithm_psudocode_latex/</link>
      <description><![CDATA[è°èƒ½æä¾› DDPG çš„ peusdocode çš„ tex ä»£ç å—ï¼Ÿ æˆ‘çŸ¥é“å®ƒå¾ˆæ‡’ï¼Œä½†æˆ‘ç›®å‰çš„å·¥ä½œé‡å¤ªå¤§äº†ï¼Œè°¢è°¢æå‰   ç”±   æäº¤ /u/Wide-Chef-7011   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk2jdg/ddpg_algorithm_psudocode_latex/</guid>
      <pubDate>Thu, 21 Mar 2024 09:30:20 GMT</pubDate>
    </item>
    <item>
      <title>Rich Sutton ä»Šå¤©æ¥å‚åŠ æˆ‘ä»¬çš„ Zoom è®²åº§ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjvzar/rich_sutton_came_to_our_zoom_lecture_today/</link>
      <description><![CDATA[       ç”±   æäº¤/u/chunchblooden  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjvzar/rich_sutton_came_to_our_zoom_lecture_today/</guid>
      <pubDate>Thu, 21 Mar 2024 02:35:28 GMT</pubDate>
    </item>
    <item>
      <title>RL ä¸­çš„é‡ç½®åŠŸèƒ½ï¼šåˆå§‹çŠ¶æ€è¿˜æ˜¯å½“å‰çŠ¶æ€ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ˜¯å¼ºåŒ–å­¦ä¹ æ–°æ‰‹ã€‚æˆ‘ç›®å‰æ­£åœ¨è‡´åŠ›äºå®ç°ä¸€ä¸ª RL ç¯å¢ƒï¼Œä»¥ä½¿ç”¨æ’æ¸©å™¨æ§åˆ¶æˆ¿é—´çš„æ¸©åº¦ã€‚æˆ‘æ­£å¤„äºé‡ç½®åŠŸèƒ½çš„åå­—è·¯å£ï¼Œå¸Œæœ›è·å¾—ä¸€äº›è§è§£ã€‚ è¯¥ç¯å¢ƒä»£è¡¨ä¸€ä¸ªå¸¦æœ‰æ’æ¸©å™¨çš„æˆ¿é—´ï¼Œå…¶ç›®æ ‡æ˜¯å°†æ¸©åº¦ä¿æŒåœ¨æ‰€éœ€çš„è®¾å®šå€¼ï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°å‡å°‘èƒ½æºæ¶ˆè´¹ã€‚ç¯å¢ƒçŠ¶æ€ç”±å½“å‰æ¸©åº¦å’Œèƒ½è€—è¡¨ç¤ºï¼Œæ“ä½œåŒ…æ‹¬è°ƒæ•´æ’æ¸©å™¨è®¾ç½®ä»¥å¢åŠ æˆ–å‡å°‘æ¸©åº¦è®¾å®šç‚¹ã€‚  æˆ‘æƒ³åˆ°äº†ä¸¤ç§æ–¹æ³•ï¼š  é‡ç½®ä¸ºåˆå§‹çŠ¶æ€ï¼šæ­¤é€‰é¡¹æ¶‰åŠåœ¨æ¯ä¸ªæ–¹æ³•å¼€å§‹æ—¶å°†ç¯å¢ƒé‡ç½®ä¸ºå…¶åˆå§‹é…ç½®æƒ…èŠ‚ï¼Œå…¶ä¸­æ¸©åº¦è®¾ç½®ä¸ºé¢„å®šä¹‰çš„èµ·å§‹å€¼ã€‚ ï¼ˆè®¾ç½®ä¸ºæœ€å°çŠ¶æ€ï¼‰ã€‚ é‡ç½®ä¸ºå½“å‰çŠ¶æ€ï¼šæˆ–è€…ï¼Œé‡ç½®åŠŸèƒ½å¯ä»¥å°†ç¯å¢ƒè¿”å›åˆ°æˆ¿é—´çš„å½“å‰çŠ¶æ€ã€‚ &lt; /ol&gt; æˆ‘ç‰¹åˆ«å–œæ¬¢é‡ç½®å½“å‰çŠ¶æ€çš„æƒ³æ³•ï¼Œå› ä¸ºå®ƒä¸æˆ‘åœ¨ä»£ç†å†³ç­–è¿‡ç¨‹ä¸­çš„æ–¹æ³•ä¸€è‡´ã€‚åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œå½“ä»£ç†é€‰æ‹©ä¸€ä¸ªæ“ä½œæ—¶ï¼Œæˆ‘è®¡åˆ’æ£€æŸ¥ä»£ç†æ˜¯å¦å¯ä»¥æ ¹æ®çŠ¶æ€å‘é‡ä¸­è¡¨ç¤ºçš„å½“å‰çŠ¶æ€æ‰§è¡Œè¯¥æ“ä½œï¼Œç„¶åç»™å‡ºæ­£å¥–åŠ±ï¼Œå¦åˆ™ç»™å‡ºè´Ÿå¥–åŠ±ã€‚  é—®é¢˜ï¼šåœ¨è¿™ç§ç¯å¢ƒä¸‹ï¼Œæ‚¨è®¤ä¸ºå“ªç§æ–¹æ³•æ›´é€‚åˆæˆ‘çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸­çš„é‡ç½®åŠŸèƒ½ï¼Ÿæˆ‘è¿˜éœ€è¦è€ƒè™‘ä»»ä½•å…¶ä»–æ³¨æ„äº‹é¡¹å—ï¼Ÿ æå‰æ„Ÿè°¢æ‚¨çš„å¸®åŠ©ï¼   ç”±   æäº¤/u/Few-Papaya101  /u/Few-Papaya101 reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/&quot;&gt;[é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/</guid>
      <pubDate>Wed, 20 Mar 2024 19:47:19 GMT</pubDate>
    </item>
    <item>
      <title>ä¸å†³æ–—+c51ä½œæ–—äº‰ï¼Œéœ€è¦ç¬¬äºŒåŒçœ¼ç›ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjlmsa/struggling_with_duellingc51_need_a_second_pair_of/</link>
      <description><![CDATA[ç¼–è¾‘ï¼š ç»§æ‰¿æ—¶æˆ‘ç¡®å®çŠ¯äº†ä¸€ä¸ªé”™è¯¯ï¼Œæˆ‘ä¸å°å¿ƒè¦†ç›–äº†ä¾èµ–æ³¨å…¥ã€‚è¿™å¯¼è‡´ä¼˜åŒ–å™¨æŒ‡å‘é”™è¯¯çš„ç½‘ç»œã€‚ åŸä¸ºï¼š ```cs  public DQNPerNoisy(DQNAgentOptions opts, List&gt;; envs, IDQNNetProvider netProvider = null) : base(opts, envs, new DuelingNoisyNetworkProvider(opts.Width, opts.Depth, opts.NumAtoms))  { }  ``` â€‹ åº”è¯¥æ˜¯ï¼š ``` cs  public DQNPerNoisy(DQNAgentOptions opts, List&gt; envs, IDQNNetProvider netProvider = null) : base(opts, envs, netProvider ?? new DuelingNoisyNetworkProvider(opts.Width, opts.Depth, opts.NumAtoms)) { } ``` â€‹ è‡³å°‘æˆ‘åœ¨ä¿®å¤é”™è¯¯åç«‹å³å¼€å§‹å·¥ä½œï¼Œæ‰€ä»¥æˆ‘çš„ç®—æ³•å’Œå¼ é‡å½¢çŠ¶éƒ½å¾ˆå¥½ ğŸ˜  ğŸ˜ ğŸ˜ä½†æ˜¯å¤æ‚æ€§dueling+c51 æ‰€åšçš„äº‹æƒ…æ˜¯æˆ‘åˆ°å¤„å¯»æ‰¾è€Œä¸æ˜¯é”™è¯¯æ‰€åœ¨ã€‚  äº²çˆ±çš„ RL ç¤¾åŒºï¼Œ åœ¨å°è¯•äº†å¾ˆå¤šä¸ªå°æ—¶åœ¨æˆ‘çš„æ¡†æ¶ä¸­å®ç° c51+dueling åï¼Œæˆ‘ç»ˆäºå‡†å¤‡æ”¾å¼ƒäº†ã€‚ä½œä¸ºæˆ‘æœ€åçš„åŠªåŠ›ï¼Œæˆ‘å‘å¸ƒæ­¤å†…å®¹æ˜¯å¸Œæœ›ä¸€äº›å¤©æ‰å¯ä»¥å¸®åŠ©æˆ‘æ‰¾åˆ°ç®—æ³•ä¸­çš„é”™è¯¯ã€‚ æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨ TorchSharp åœ¨ C# ä¸­åˆ›å»ºä¸€ä¸ª DRL åº“ï¼Œå¹¶ä¸”æˆ‘ä¸€ç›´åœ¨å°è¯•è¿˜æä¾›å½©è™¹ã€‚äº‹å®è¯æ˜è¿™çœŸçš„å¾ˆå›°éš¾ï¼ˆæˆ–è€…æˆ‘è¿˜æ²¡æœ‰å‡†å¤‡å¥½ï¼‰ é•¿è¯çŸ­è¯´ï¼Œæˆ‘çš„æŸå¤±å¹¶æ²¡æœ‰å‡å°‘ï¼Œæˆ‘ä¸ç¡®å®šæˆ‘åœ¨å®ç°ç®—æ³•æ—¶åœ¨å“ªé‡ŒçŠ¯äº†é”™è¯¯ï¼Œæˆ‘å°è¯•äº†å¾ˆå¤šäº‹æƒ…å¹¶ä¸”æ— æ³•å¼„æ¸…æ¥šã€‚ æœ‰äººå¯ä»¥çœ‹ä¸€ä¸‹å¹¶å°è¯•å‘ç°æˆ‘çš„é”™è¯¯å—ï¼Ÿ æˆ‘çš„ä»£ç åœ¨è¿™é‡Œï¼šç½‘ç»œï¼šhttps://github.com/asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/ DQN/NN/Variants/DuelingDNQ_C51_Noisy.cs ä»£ç†ï¼šhttps://github.com/asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/DQN/Variants/DQNRainbow.cs Python ç¿»è¯‘å¯¹äºé‚£äº›è®¨åŒ C# çš„äººï¼šhttps://github.com/ asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/DQN/Variants/PythonRainbowTranslation.txt ä¹Ÿæ¬¢è¿é“å¾·æ”¯æŒã€‚   ç”±   æäº¤ /u/DotNetEvangeliser   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjlmsa/struggling_with_duellingc51_need_a_second_pair_of/</guid>
      <pubDate>Wed, 20 Mar 2024 19:12:20 GMT</pubDate>
    </item>
    <item>
      <title>SB3 çš„ DQN ä¸èµ·ä½œç”¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bj8nh9/dqn_from_sb3_doesnt_work/</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€ä¸ªè‡ªå®šä¹‰çš„ boid æ¤ç»’ç¯å¢ƒå¹¶ä½¿ç”¨ SB3 å®ç°äº† DQNã€‚ ä½†æ˜¯ï¼Œå®ƒç»§ç»­è¿è¡Œå¹¶ä¸”æ²¡æœ‰è¾“å‡ºä»»ä½•å†…å®¹ã€‚ä¼¼ä¹æ— æ³•æ‰¾å‡ºé”™è¯¯ã€‚ ä»£ç ï¼šhttps://drive .google.com/drive/folders/1zoQSrLOVO13TBGtoJhkg5LwVoKEmM2gT?usp=sharing   ç”±   æäº¤/u/Sadboi1010   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bj8nh9/dqn_from_sb3_doesnt_work/</guid>
      <pubDate>Wed, 20 Mar 2024 08:31:23 GMT</pubDate>
    </item>
    <item>
      <title>å°è¯•åœ¨ PyTorch ä¸­å®ç° crossQ ä¸èµ·ä½œç”¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bj3rln/trying_to_implement_crossq_in_pytorch_does_not/</link>
      <description><![CDATA[ä½ å¥½ï¼Œæˆ‘æ­£åœ¨å°è¯•å®ç°ï¼š https://openreview.net/pdf?id=PczQtTsTIX è€Œä¸”ä¼¼ä¹æ— æ³•è·å¾—å¥½çš„ç»“æœã€‚æˆ‘å·²ç»åœ¨ Ant-v4 ä¸­å°è¯•è¿‡äº† åœ¨æ›´æ–°è´å°”æ›¼æ–¹ç¨‹ä¸­å¯¹æˆ‘çš„ SAC ä»£ç è¿›è¡Œäº†ä»¥ä¸‹æ›´æ”¹ï¼š def update_critic(self, state_batch: torch.tensor, action_batchï¼štorch.tensorï¼Œreward_batchï¼štorch.tensorï¼Œnext_state_batchï¼štorch.tensorï¼Œmask_batchï¼štorch.tensorï¼‰-&gt; torch.tensor: â€œâ€â€ä½¿ç”¨ Soft Bellman æ–¹ç¨‹æ›´æ–°æ‰¹è¯„å®¶ï¼šparam state_batchï¼šä»å†…å­˜ä¸­æå–çš„çŠ¶æ€æ‰¹æ¬¡ï¼šparam action_batchï¼šä»å†…å­˜ä¸­æå–çš„æ“ä½œæ‰¹æ¬¡ï¼šparamreward_batchï¼šä»å†…å­˜ä¸­æå–çš„å¥–åŠ±æ‰¹æ¬¡ï¼šparam next_state_batchï¼šä»å†…å­˜ä¸­æå–çš„ä¸‹ä¸€ä¸ªçŠ¶æ€æ‰¹æ¬¡æ‰¹æ¬¡ï¼šparam mask_batchï¼šå®Œæˆçš„æ©ç ï¼šè¿”å›ï¼šQ1 çš„æµ®åŠ¨æŸå¤±ï¼ŒQ2 çš„æµ®åŠ¨æŸå¤±â€œâ€â€œâ€ next_state_action_batch, next_state_log_pi_batch, _ = self.policy.sample(next_state_batch, False, False) if self.config_agent[&#39;crossqstyle&#39;]: # (bsz x 2, nstate) cat_states = torch.cat([state_batch, next_state_batch], 0) # (bsz x 2, nact) cat_actions = torch.cat([action_batch, next_state_action_batch], 0) # (bsz x 2, 1) qfull1, qfull2 = self.critic(cat_states, cat_actions) # åˆ†ç¦» Q q1, q1next = torch .chunk(qfull1, chunks=2, dim=0) q2, q2next = torch.chunk(qfull2, chunks=2, dim=0) min_qnext = torch.min(q1next, q2next) - self.alpha * next_state_log_pi_batch next_qvalue = (å¥–åŠ±_batch + mask_batch * self.config_agent[&#39;gamma&#39;] * min_qnext).detach() å¦åˆ™ï¼šä½¿ç”¨ torch.no_grad(): q1next_target, q2next_target = self.critic_target(next_state_batch, next_state_action_batch) min_qnext = torch.min(q1next_target, q2next_target) - self.alpha * next_state_log_pi_batch next_qvalue =reward_batch + mask_batch * self.config_agent[&#39;gamma&#39;] * min_qnext q1, q2 = self.critic(state_batch, action_batch) q_loss, q1_loss, q2_loss = self.calculate_q_loss(q1, q2, next_qvalue) # é»˜è®¤ self.critic_optim.zero_grad() q_loss.backward() self.critic_optim.step()  æˆ‘è¿˜åœ¨æ‰¹è¯„è€…å’Œæ¼”å‘˜ä¸­æ·»åŠ äº†æ‰¹è§„èŒƒå±‚ã€‚ä¾‹å¦‚æ‰¹è¯„è€…çš„åˆå§‹åŒ–ï¼š # Activations if activate == &quot;relu&quot;: self.activation = nn.ReLU() elifactivation == &quot;leaky_relu&quot;: self.activation = nn.LeakyReLU() elif æ¿€æ´» == &quot;tanh&quot;: self.activation = nn.Tanh() else: å¦‚æœ bn_mode == &quot;bn&quot; åˆ™å¼•å‘ NotImplementedError: BN = nn.BatchNorm1d elif bn_mode == &quot;brn&quot; : BN = BatchRenorm1d else: raise NotImplementedError # Layers self.q1_list = nn.ModuleList() self.q2_list = nn.ModuleList() # BNå±‚0 - æ ¹æ®crossQçš„ä»£ç  if use_batch_norm: self.q1_list.append(BN( state_dim + action_dimï¼ŒåŠ¨é‡=bn_momentumï¼‰ï¼‰ self.q2_list.appendï¼ˆBNï¼ˆstate_dim + action_dimï¼ŒåŠ¨é‡=bn_momentumï¼‰ï¼‰ self.q1_list.appendï¼ˆnn.Linearï¼ˆintï¼ˆï¼ˆstate_dim + action_dimï¼‰ï¼‰ï¼Œhidden_â€‹â€‹dimï¼‰ï¼‰ self. q1_list.append(self.activation) self.q2_list.append(nn.Linear(int((state_dim + action_dim)),hidden_â€‹â€‹dim)) self.q2_list.append(self.activation) if use_batch_norm: self.q1_list.append(BN (hidden_â€‹â€‹dim,åŠ¨é‡=bn_momentum)) self.q2_list.append(BN(hidden_â€‹â€‹dim,åŠ¨é‡=bn_momentum)) for i in range(num_layers - 1): self.q1_list.append(nn.Linear(hidden_â€‹â€‹dim,hidden_â€‹â€‹dim)) self. q1_list.append(self.activation) self.q2_list.append(nn.Linear(hidden_â€‹â€‹dim,hidden_â€‹â€‹dim)) self.q2_list.append(self.activation) å¦‚æœuse_batch_norm: self.q1_list.append(BN(hidden_â€‹â€‹dim)) self.q2_list .append(BN(hidden_â€‹â€‹dim)) self.q1_list.append(nn.Linear(hidden_â€‹â€‹dim, 1)) self.q2_list.append(nn.Linear(hidden_â€‹â€‹dim, 1))  æ˜¯å¦æœ‰äººæœ‰ä»€ä¹ˆæƒ³æ³•å—ï¼Ÿ æˆ‘çš„ SAC ç‰ˆæœ¬å­¦ä¹ å¾—å¾ˆå¥½ã€‚ç„¶è€Œï¼Œåœ¨è®ºæ–‡ä¸­ï¼Œä»–ä»¬è¿˜æŠ¥å‘Šäº†ä»¥æ‰¹è¯„è€…ä¸­çš„ tanh æ¿€æ´»å‡½æ•°ä½œä¸ºåŸºçº¿çš„ SACï¼Œåœ¨æˆ‘çš„ä¾‹å­ä¸­ï¼Œè¿™ä¹Ÿä¸èµ·ä½œç”¨ï¼ˆé»˜è®¤æ¿€æ´»æ˜¯ ReLUï¼‰ã€‚æˆ‘å°è¯•ä½¿ç”¨è®ºæ–‡ä¸­æŠ¥å‘Šçš„è¶…å‚æ•°ã€‚ ä¹Ÿè®¸æˆ‘å¿˜è®°äº†ä¸€äº›æŠ€å·§ï¼Ÿ   ç”±   æäº¤/u/LazyButAmbitious  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bj3rln/trying_to_implement_crossq_in_pytorch_does_not/</guid>
      <pubDate>Wed, 20 Mar 2024 03:14:41 GMT</pubDate>
    </item>
    <item>
      <title>â€œé€šè¿‡è€è™æœºä¼˜åŒ–è¯†åˆ«ä¸€èˆ¬ååº”æ¡ä»¶â€ï¼ŒWang ç­‰äºº 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biu16l/identifying_general_reaction_conditions_by_bandit/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biu16l/identifying_general_reaction_conditions_by_bandit/</guid>
      <pubDate>Tue, 19 Mar 2024 20:14:24 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ— éœ€æœç´¢çš„å¤§å¸ˆçº§å›½é™…è±¡æ£‹â€ï¼ŒRuoss ç­‰äºº 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>