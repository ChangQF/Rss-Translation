<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 21 Apr 2024 12:24:24 GMT</lastBuildDate>
    <item>
      <title>模型的随机性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9e03u/randomness_in_model/</link>
      <description><![CDATA[我有一个使用奖励函数训练的自定义 MARL 环境。我尝试多次训练模型而不更改任何参数。它已经在 2/40 训练运行中学会了适当的行为，即训练了一个成功的模型。我有 Open AI Gym 和带 PPO 的 SB3，无法使用健身房，因为无法安装它和其他健身房。我什至使用种子： seed_val = 23 env.seed(seed_val)  为什么它如此随机以及我该如何克服它。欢迎提出想法   由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9e03u/randomness_in_model/</guid>
      <pubDate>Sun, 21 Apr 2024 09:43:19 GMT</pubDate>
    </item>
    <item>
      <title>参数噪声与输入噪声的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9dvwt/parametric_noise_over_input_noise/</link>
      <description><![CDATA[我在 2017 年看到了这项研究，其中讨论了使用“参数噪声”而不是基于输入的噪声。我尝试在基于 PPO 的 Boid 植绒自定义环境中使用它，但得出的结论是这是不可能的，因为 SB3 不支持它。 有关如何添加或使用它的任何建议。我认为它需要开放的人工智能基线，即使这样，人们又该如何去做呢，因为目前没有找到任何文档，并且是否有任何环境限制，比如我们不能使用它？ 使用参数噪声进行更好的探索   由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9dvwt/parametric_noise_over_input_noise/</guid>
      <pubDate>Sun, 21 Apr 2024 09:35:06 GMT</pubDate>
    </item>
    <item>
      <title>构建用于计算卸载的奖励函数。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9du95/build_a_reward_functions_for_computation/</link>
      <description><![CDATA[您好，我想为计算卸载构建奖励函数 我卸载到边缘或云端或在本地执行任务 我的目标是最大限度地减少延迟和能耗，成本，并最大限度地提高安全性（信任）。我在我的奖励函数中使用了这些指标并进行了加权总和，但我对我的函数不满意。谁可以为我提供一些想法。提前致谢。   由   提交/u/RynTSukinomi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9du95/build_a_reward_functions_for_computation/</guid>
      <pubDate>Sun, 21 Apr 2024 09:31:44 GMT</pubDate>
    </item>
    <item>
      <title>DQN 培训和测试的黑白差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c98hiy/discrepancy_bw_dqn_training_testing/</link>
      <description><![CDATA[上下文：https： //www.reddit.com/r/reinforcementlearning/comments/1bxw4ci/need_some_feedback_on_an_idea_for_using/ 我一直在使用 DQN 模型，并注意到一些我希望得到的不一致之处一些见解。 在训练阶段，我一直在使用 dqn_training 函数并加载权重。然而，它在测试期间采取的行动似乎与训练期间采取的行动有很大不同。尽管奖励减少，但它继续执行相同的操作（最终目标是获得更高的奖励。训练期间就是这种情况） 我特别有兴趣了解为什么之间存在差异训练与测试中采取的操作，以及 dqn.test 是否正确，以及 dqn.forward 是否是更好的函数 任何见解或建议将不胜感激。    由   提交 /u/Broncosslayer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c98hiy/discrepancy_bw_dqn_training_testing/</guid>
      <pubDate>Sun, 21 Apr 2024 03:43:59 GMT</pubDate>
    </item>
    <item>
      <title>通过想象力、探索和批评实现法学硕士的自我完善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c97orc/toward_selfimprovement_of_llms_via_imagination/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.12253 摘要：  尽管大型语言模型（LLM）在各种方面具有令人印象深刻的能力任务中，他们仍然在处理涉及复杂推理和计划的场景。最近的工作提出了先进的提示技术以及使用高质量数据进行微调以增强法学硕士推理能力的必要性。然而，这些方法本质上受到数据可用性和质量的限制。有鉴于此，自我纠正和自我学习成为可行的解决方案，采用的策略允许法学硕士改进他们的成果并从自我评估的奖励中学习。然而，法学硕士在自我完善其反应方面的有效性，特别是在复杂的推理和规划任务中，仍然值得怀疑。在本文中，我们引入了用于LLM自我改进的AlphaLLM，它将蒙特卡罗树搜索（MCTS）与LLM结合起来，建立一个自我改进循环，从而在无需额外注释的情况下增强LLM的能力。 AlphaLLM 从 AlphaGo 的成功中汲取灵感，解决了将 MCTS 与 LLM 相结合以实现自我提升的独特挑战，包括数据稀缺、语言任务的巨大搜索空间以及语言任务中反馈的主观性。 AlphaLLM 由即时合成组件、专为语言任务量身定制的高效 MCTS 方法以及用于精确反馈的三个批评模型组成。我们在数学推理任务中的实验结果表明，AlphaLLM 在无需额外注释的情况下显着增强了法学硕士的性能，显示了法学硕士自我改进的潜力。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c97orc/toward_selfimprovement_of_llms_via_imagination/</guid>
      <pubDate>Sun, 21 Apr 2024 02:57:26 GMT</pubDate>
    </item>
    <item>
      <title>数独实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c8muem/sudoku_implementation/</link>
      <description><![CDATA[嗨！我开始通过强化学习来实现数独求解器，实际上我没有取得很好的结果，因为我在这个领域比较新，而且我认为任务并不那么简单。如果您擅长，请考虑在此处帮助我！   由   提交/u/Cri_Sti_An   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c8muem/sudoku_implementation/</guid>
      <pubDate>Sat, 20 Apr 2024 10:34:53 GMT</pubDate>
    </item>
    <item>
      <title>推理不会以使用自定义数据集 llama-2 模型进行微调的 QLoRa 结束（模型在无限循环中生成输入和响应）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c8ibe3/inference_doesnt_end_in_a_qlora_finetuned_with_a/</link>
      <description><![CDATA[       嘿，伙计们，我通过使用位和字节量化来训练 llama 2 模型，然后使用以下格式的自定义数据集对其进行训练： 系统提示： 输入：  响应： 当我运行推理时，模型的行为符合我想要的方式（有点）——它生成回复，但也会在无限循环中回复自身，直到 max_new_tokens 为达到，即它生成“###响应”但不会停止并且还会生成“### 输入”并循环回复自身。为什么会发生这种情况？这是标记器的设置方式吗？我是否使用了错误的格式来训练模型？ 我将非常感谢任何有关此事的帮助、评论、反馈或资源链接。请参阅下面的附图，看看模型的响应是什么样的。预先感谢您。 https： //preview.redd.it/wv1khhbomkvc1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=f6e8f754de5d6fda134b9f0e5cc7bb468355bd6e    ;由   提交/u/guccicupcake69   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c8ibe3/inference_doesnt_end_in_a_qlora_finetuned_with_a/</guid>
      <pubDate>Sat, 20 Apr 2024 05:35:37 GMT</pubDate>
    </item>
    <item>
      <title>SERL：用于样本高效的机器人强化学习的软件套件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c8g6t6/serl_a_software_suite_for_sampleefficient_robotic/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.16013 SERL 代码：https： //github.com/rail-berkeley/serl 机器人控制器代码：https://github.com/rail-berkeley/serl_franka_controllers 项目页面：https://serl-robot.github.io/ 视频：https://www.youtube.com/watch?v=Um4CjBmHdcw 摘要： &lt; blockquote&gt; 近年来，机器人强化学习 (RL) 领域取得了重大进展，实现了处理复杂图像观察、在现实世界中进行训练以及合并辅助数据（例如演示和先前经验）的方法。然而，尽管取得了这些进步，机器人强化学习仍然很难使用。从业者公认，这些算法的特定实现细节对于性能而言通常与算法的选择一样重要（如果不是更重要的话）。我们认为，机器人强化学习的广泛采用以及机器人强化学习方法的进一步发展所面临的一个重大挑战是此类方法相对难以获得。为了应对这一挑战，我们开发了一个精心实现的库，其中包含一个高效的离策略深度强化学习方法样本，以及计算奖励和重置环境的方法、一个用于广泛采用的机器人的高质量控制器，以及一个大量具有挑战性的示例任务。我们提供这个库作为社区的资源，描述其设计选择，并展示实验结果。也许令人惊讶的是，我们发现我们的实施可以实现非常高效的学习，获取 PCB 板组装、电缆布线和对象重新定位的策略，平均每个策略需要 25 到 50 分钟的训练时间，比报告的最先进结果有所改进对于文献中的类似任务。这些政策实现了完美或近乎完美的成功率，即使在扰动下也具有极高的鲁棒性，并表现出紧急恢复和纠正行为。我们希望这些有希望的结果和我们高质量的开源实现将为机器人社区提供一个工具，以促进机器人强化学习的进一步发展。我们的代码、文档和视频可以在 此 https URL  &lt; !-- SC_ON --&gt;  由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c8g6t6/serl_a_software_suite_for_sampleefficient_robotic/</guid>
      <pubDate>Sat, 20 Apr 2024 03:35:08 GMT</pubDate>
    </item>
    <item>
      <title>定制 DQN 中的数学</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c837oh/mathematics_in_a_customized_dqn/</link>
      <description><![CDATA[你好， 我只想验证它在数学上是否正确：如果 DQN 具有共享网络，后跟四个网络分支，其中第一个分支确定是否执行第二个、第三个或第四个分支的操作​​。也就是说，在决策时只有两个分支处于活动状态。 要计算损失，仅计算活动分支的损失（将它们相加）并将奖励纳入其中是否正确（例如它们是相同的）：  selector_expected_q_values =reward + self.gamma * (1 - did) * target_selector_probs.detach().max(1)[0] loss_combined += self. loss_fn(selector_q_values, selector_expected_q_values.unsqueeze(1))  target_active_branch_q_values = 奖励 + self.gamma * (1 - 完成) * (target_active_branch_probs.detach().max(1)[0]) loss_combined += self.loss_fn(active_branch_q_values, target_active_branch_q_values.unsqueeze(1))  ​   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c837oh/mathematics_in_a_customized_dqn/</guid>
      <pubDate>Fri, 19 Apr 2024 17:51:11 GMT</pubDate>
    </item>
    <item>
      <title>强化学习可以用来解决这个具有挑战性的优化问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7z1m4/could_rl_be_used_to_solve_this_challenging/</link>
      <description><![CDATA[大家好， 我正在处理工作中的挑战，涉及包含人员信息的大型数据集。每行包含一个计数和各种维度（如性别、城市、年龄）。我们需要对计数低于特定阈值（例如小于 5）的条目进行匿名化，作为保护隐私的初步步骤。 数据集还包含每个维度的聚合数字。这意味着如果一个维度中只有一行被匿名，则可以推导出模糊值。为了防止这种情况，我们在同一维度上模糊另一个值，这反过来可能需要在其他维度上进一步模糊。这种迭代模糊通常会导致过度匿名化，这并不是最佳的。现在，这是由一组高级 SQL 查询执行的自动化过程。但是，当有多个选项时，选择模糊哪个值是一个随机选择。 Colud 强化学习（RL）可以用来优化这个过程吗？  数据集的大小不固定；它的行数各不相同，从而创建了可变的观察或状态空间。 数据集的复杂性也是可变的，在匿名化过程中需要考虑不同的维数。 匿名化的阈值很灵活，通常设置为最小值（例如 5），但可以根据具体的隐私要求进行调整。  是否还应考虑其他方法？对于优化方法有什么想法吗？   由   提交/u/Purple_Investment_97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7z1m4/could_rl_be_used_to_solve_this_challenging/</guid>
      <pubDate>Fri, 19 Apr 2024 15:01:11 GMT</pubDate>
    </item>
    <item>
      <title>动态定位的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7y4ct/reinforcement_learning_for_dynamic_positionning/</link>
      <description><![CDATA[你好！ 我目前正在开发一个模型来解决无人机的动态定位问题。  我处于早期阶段，所以没有动态定位，哈哈。我只是想解决一个问题，即具有一组静态用户的无人机根据到这些点的距离给予奖励（将来，我不会知道这些点的位置，并且会有是运动，所以它不是那么微不足道）。  我正在解决这个问题，以熟悉 DQN 和 DRL 的内容。但我遇到了一些问题：  状态空间太大；状态由 (x,y) 定义，位置是整数，因此它是一个相当充足的状态空间，因为它没有约束。在经典的强化学习算法中，这实际上是不可能解决的（据我所知），但由于内部有一个神经网络，我不知道它如何“压缩”的结果。的信息。  动态动作空间。定位开始时，移动量足够大，可以快速到达最佳点，但当接近最佳点时，移动步长应减小。这是另一个问题，我需要弄清楚它是如何工作的。 未来的扩展：只有一架无人机，但将来会有几架，并且这也很难解决，也许是多臂强盗或类似的东西。   目前，当我进行训练时，我已经相当成功地解决了这个问题，其中每次执行时无人机的起点都是相同的，但是当我随机化它时初始位置，我没有取得成功。在我的问题中，没有明确的“结局”。执行的过程，并且在大多数教程中，都有一个明确的结局。 对于这个新手问题，我很抱歉，但我很困惑。  谢谢！！   由   提交 /u/RikoteMasterrrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7y4ct/reinforcement_learning_for_dynamic_positionning/</guid>
      <pubDate>Fri, 19 Apr 2024 14:21:59 GMT</pubDate>
    </item>
    <item>
      <title>关于提高两条腿机器人代理性能的技巧？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7y4ci/tips_on_increasing_performance_of_a_two_legged/</link>
      <description><![CDATA[在过去的几周里，我一直在尝试教一个两足机器人从稳定的坐姿转变为站立姿势，但没有成功。具体来说，我正在使用 NVIDIA 的 Isaac Sim 及其强化学习的实现。该代理是一个两足机器人，有 6 个伺服系统，每条腿各 3 个：膝盖、腿和臀部。该环境由同时模拟的多个机器人（4096）组成。 对于奖励函数，我决定采用一个简单的函数：它获取机器人相对于地板和身体的高度方向。身体越垂直、越接近期望的高度，奖励就越高。然后，我会根据它在不跌倒的情况下实现的模拟步骤越多，添加额外的奖励。每集步数为 25，实时为 5 秒。想法是让它在不到5s的时间内站立并保持站立。 至于算法实现，这是配置： algo: name: a2c_discrete model:名称：multi_discrete_a2c 网络：名称：actor_critic 单独：False 空间：multi_discrete：mu_activation：无 sigma_activation：无 mu_init：名称：默认 sigma_init：名称：const_initializer val：0 固定_sigma：True mlp：单位：[512, 256, 128] 激活： elu d2rl：假初始化器：名称：默认正则化器：名称：无配置：ppo：真混合精度：真正常化_输入：真正常化_值：真num_actors：$ {....task.env.numEnvs}奖励_整形器：规模_值：1正常化_优势：真gamma：0.99 tau：0.95learning_rate：0.003lr_schedule：自适应kl_threshold：0.008grad_norm：1.0entropy_coef：0.02truncate_grads：Truee_clip：0.2horizo​​n_length：25minibatch_size：32768mini_epochs：5critic_coef：2clip_值：真实 seq_length：4bounds_loss_coef：0.0001 max_epochs： ${resolve_default:300,${....max_iterations}} save_best_after: 300 save_Frequency: 25 Score_to_win: 200000 print_stats: True  到目前为止，总共 30M 步之后，有时机器人到达站立位置，但身体向后倾斜并摔倒。 我对强化学习还很陌生，所以我不知道问题是否出在奖励函数、算法配置、步骤数或同时执行所有步骤。到目前为止，我们已经在配置上进行了反复试验，考虑到训练 30M 步骤大约需要 21 小时，速度非常慢。 我尝试过寻找如何提高性能，但是迄今为止最好的答案是“这取决于问题”。您有什么建议，这样就不仅仅是反复试验？   由   提交 /u/mishaurus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7y4ci/tips_on_increasing_performance_of_a_two_legged/</guid>
      <pubDate>Fri, 19 Apr 2024 14:21:58 GMT</pubDate>
    </item>
    <item>
      <title>具有集中批评者的多代理 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7y2hy/multiagent_ppo_with_centralized_critic/</link>
      <description><![CDATA[我想制作一个具有集中训练和分散评估的 PPO 版本，用于使用 PPO 的合作（共同奖励）多代理设置。  对于 PPO 实现，我遵循了此存储库（https://github.com/ericyangyu/PPO -for-Beginners），然后根据我的需要进行了一些调整。问题是我发现自己目前陷入了如何处理实现的某些部分的困境。 我知道集中批评家将输入所有代理的组合状态空间，然后输出一般状态值数。问题是我不明白这如何在 PPO 的推出（学习）阶段发挥作用。特别是我不明白以下事情：  我们如何计算批评者损失？由于在多智能体 PPO 中，它应该由每个智能体单独计算 我们如何在智能体的学习阶段查询批评家网络？由于现在每个代理（具有分散的批评家）都有一个比批评家网络小得多的观察空间（因为它具有所有观察空间的总和）  提前感谢您帮助！   由   提交/u/blrigo99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7y2hy/multiagent_ppo_with_centralized_critic/</guid>
      <pubDate>Fri, 19 Apr 2024 14:19:52 GMT</pubDate>
    </item>
    <item>
      <title>q 表的重新训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7xtlz/retraining_of_qtables/</link>
      <description><![CDATA[所以我希望这是提出这个问题的正确位置。  我目前正在写我的硕士论文，我想研究机器人“个性”/“偏好”对共同学习的影响（对于某些人来说，这是人类和机器人之间的学习过程）共同的共享任务）。  由于学习需要在现实生活中进行，因此强化学习算法的选择仅限于非常简单的 Q-learning 算法。我计划嵌入的首选项与对象切换的位置和方向相关。因为这些直接由状态表示，所以偏好的嵌入是通过使用适当的奖励函数对 q 表进行离线训练来实现的。 （所以最后我们有一个 q 表，其中填充了已解决环境的值） 现在我遇到的问题是，一旦嵌入这些首选项，它们就需要能够重新学习，而不是如果任务目标失败，则要快速（这是共同学习的要求）。  现在我的问题是： - 是否有文献讨论 q 表的重新学习？ - 是否有文献讨论如何在线嵌入和更改具体学习任务的偏好？ - 有没有办法设置机器人/算法在保持其偏好不改变它们方面的“持久性”程度 如果有人能指出我正确的方向，我将不胜感激！    由   提交 /u/MickeyMouseEngineer2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7xtlz/retraining_of_qtables/</guid>
      <pubDate>Fri, 19 Apr 2024 14:09:05 GMT</pubDate>
    </item>
    <item>
      <title>“如何培训数据高效的法学硕士”，Sachdeva 等人 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7dbyn/how_to_train_dataefficient_llms_sachdeva_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7dbyn/how_to_train_dataefficient_llms_sachdeva_et_al/</guid>
      <pubDate>Thu, 18 Apr 2024 20:14:20 GMT</pubDate>
    </item>
    </channel>
</rss>