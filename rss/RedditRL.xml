<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 11 May 2024 12:24:05 GMT</lastBuildDate>
    <item>
      <title>有关使用首次访问策略蒙特卡罗和新奖励信号解决冰冻湖问题的反馈。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpdzxf/feedback_regarding_solving_frozen_lake_using/</link>
      <description><![CDATA[我试图用道馆图书馆提供的默认奖励设置来解决冰冻湖。  使用默认的奖励设置，我发现无法解决冰冻的湖泊环境。特工会在没有到达目标的情况下撞入洞中，或者继续撞到墙壁上，然后最终坠毁。  因为我在读 Sutton 和 Barto 的《强化学习》一书，我知道对于二十一点问题，书中提到了负奖励。  然后我决定测试新的奖励系统。如果智能体掉进洞里，奖励将为-1。如果智能体撞到墙，奖励将为-0.1，如果智能体达到目标，奖励将为+10。  令我惊讶的是，这个奖励系统运行得非常好，代理更快地找到目标。  我想得到这个子的反馈。  是否可以使用蒙特卡罗以默认奖励信号来解决 Frozen Lake 问题？  或者真的有必要使用新的奖励信号来改变它吗？  顺便说一句，我已经使用动态规划和时间差分方法用默认奖励信号解决了冰冻湖。  谢谢。    由   提交/u/mono1110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpdzxf/feedback_regarding_solving_frozen_lake_using/</guid>
      <pubDate>Sat, 11 May 2024 09:58:24 GMT</pubDate>
    </item>
    <item>
      <title>我迫切需要学习方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cp6xc6/im_in_desperate_need_for_help_with_learning/</link>
      <description><![CDATA[你好， 我第一次遇到强化学习，是在 3 年前（当时我还在读大二） CS 本科学位）在浏览 Coursera 时发现了阿尔伯塔大学的 RL 专业。我感到深深的联系，并继续完成了 4 门课程。这个 Reddit 子版块中的许多人可能已经意识到，专业化并没有深入太多，它提供的信息足以应用一些最基本的 RL 算法和技术。我能够利用这些知识来完成我的本科论文，涉及强化学习在云计算问题中的应用，并从这项工作中获得了一些相当不错的出版物。  毕业已经一年了，一直从事初级软件开发人员的工作。我想从事 RL 方面的研究。我相信我缺乏直接攻读博士学位所需的个人资料。因此，我计划在一所优秀的学校攻读基于论文的硕士学位课程，这可以取得博士学位。  即使我注册了基于论文的硕士课程，在大多数学校，获得 PI 也是我的责任。每当我打开任何研究强化学习的教授的教员页面时，我都会看到一些我根本不理解的东西。每个人似乎都在研究 RL 中的一些超级小众的东西，而对我来说 RL 本身似乎是一个相当小众的领域。我听人们说，要获得成功的研究型项目经验，我必须拥有“相同的兴趣”作为我的 PI 等等。 我现在面临的主要问题是，我缺乏对该领域的知识深度，甚至不知道自己想要什么。这让我找到了 YouTube 上的一些材料和一些高级强化学习教科书。但我很快发现我所拥有的数学知识水平不足以理解这些高级材料。那么，我现在该怎么办？对于一个试图进入基于论文的硕士课程的本科生来说，只对自己感兴趣的领域有基本的了解可以吗？如果没有，我如何获得我确信自己缺乏的知识？ 我知道这是一个混合问题，不仅涉及强化学习，还涉及一般研究生院的建议。抱歉，如果它不属于此子项。 感谢您的任何建议。   由   提交 /u/SeaworthinessHot5365   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cp6xc6/im_in_desperate_need_for_help_with_learning/</guid>
      <pubDate>Sat, 11 May 2024 02:25:34 GMT</pubDate>
    </item>
    <item>
      <title>数据质量网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cosbwj/dqn/</link>
      <description><![CDATA[&lt;表&gt;      I我一直在尝试从头开始实现 DQN，但预测值似乎有些不对劲，所以我在这里做了一个简单的测试。这里，有一系列价值为 1 的奖励，最终没有奖励。 DQN（净）应该预测折扣奖励。然而，由于某种原因，对后续状态的估计会急剧上升，这与实际曲线相反，并且 DQN 需要几个时期才能意识到其错误并将初始状态向上移动。这里可能发生什么？ https ://preview.redd.it/qok0jda3bmzc1.png?width=915&amp;format=png&amp;auto=webp&amp;s=fde8029f66d4a40c761a90c23f36a657d85b1b9b 代码： 从torch导入nn，optim导入matplotlib.pyplot作为plt导入随机导入numpy作为np规模= 100折扣= 0.9奖励= [1 for i in range(scale-1)] + [0] net = nn.Sequential ( nn.Linear(4,4), nn.Mish(), nn.Linear(4,1)) opt = optim.RMSprop(net.parameters(), lr=0.01) def 步骤(c): 全局折扣,奖励，净，opt qvals = [] for i in range(scale): inp = torch.tensor([[i/50 for _ in range(4)]], dtype=torch.float32) #inp = torch.tensor ([[i/50 for _ in range(1)]+[random.random() for _ in range(3)]], dtype=torch.float32) 输出 = net(inp) q = output.detach() .item() qvals.append(q) 如果 i == scale-1: next_q = 0 否则: next_inp = torch.tensor([[(i+1)/50 for _ in range(4)]], dtype= torch.float32) next_q = net(next_inp).detach().item() td_error = 折扣 * next_q + 奖励[i] - q grad = torch.tensor([[-td_error/scale]], dtype=torch.float32 ) #grad = torch.tensor([[-td_error / np.sqrt(1 + td_error**2) / 100]], dtype=torch.float32) 输出.backward(grad) nn.utils.clip_grad_norm_(net.parameters (), 1) opt.step() opt.zero_grad() if c%1 == 0: plt.plot(list(range(scale)), qvals) plt.savefig(f&#39;zoo{c}.png&#39; ) plt.clf() #plt.show() for i in range(50): step(i)    由   提交/u/AUser213  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cosbwj/dqn/</guid>
      <pubDate>Fri, 10 May 2024 15:30:39 GMT</pubDate>
    </item>
    <item>
      <title>CrossQ：深度强化学习中的批量标准化，以提高样本效率和简单性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1corrqu/crossq_batch_normalization_in_deep_reinforcement/</link>
      <description><![CDATA[     &lt; /td&gt; 论文和代码：http:// /adityab.github.io/CrossQ CrossQ 是一种无模型的离策略方法，它无需执行额外的梯度步骤就超越了当前的 SOTA。它本质上是：  采用 SAC（6yo 方法） 删除目标网络 添加批量归一化  这些简单的编辑足以超越 REDQ 和 DroQ 的强大性能，仅需要 5% 的梯度步长。 Twitter：https://twitter.com/aditya_bhatt/status/1768342823747674377 ICLR 2024 焦点演讲：https://iclr.cc/virtual/2024/poster/18699 （这里是第一位合著者，很乐意提供帮助！） https://preview.redd.it/0bhb82z77mzc1.png?width=3582&amp; amp;格式=png&amp;auto=webp&amp;s=41f3071cb84445734a6338e672658c99044ea560   由   提交 /u/RoboticsLiker   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1corrqu/crossq_batch_normalization_in_deep_reinforcement/</guid>
      <pubDate>Fri, 10 May 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL Baselines3 Zoo 进行分布式优化的启动试验有多少次</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</link>
      <description><![CDATA[您好，我正在 RL Baselines3 Zoo 中的 6 个进程中分配优化（github），它使用 Optuna 。 我关心的参数代码位于 rl_zoo3/train.py 中：  parser.add_argument( &quot;--n-trials&quot;, help=&quot;优化超参数的试验次数。&quot; &quot;这适用于每个优化运行器，而不是整个优化过程。&quot;, type=int, default= 500, ) parser.add_argument(&quot;--n-startup-trials&quot;, help=&quot;使用 optuna 采样器之前的试验次数&quot;,  我知道我是否使用每个6 个进程 --n-trials 100 那么我将获得 600 次试验。 但是 --n-startup-Trials 10 会怎么样呢？是 10 次还是 60 次启动试验？    提交ufoludek3000&quot;&gt; /u/ufoludek3000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</guid>
      <pubDate>Fri, 10 May 2024 10:25:03 GMT</pubDate>
    </item>
    <item>
      <title>生成式人工智能已经达到顶峰了吗？ - 电脑爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co4gsj/has_generative_ai_already_peaked_computerphile/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/FedeRivade  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co4gsj/has_generative_ai_already_peaked_computerphile/</guid>
      <pubDate>Thu, 09 May 2024 18:38:09 GMT</pubDate>
    </item>
    <item>
      <title>“通过强化学习出现类似信念的表征”，Hennig 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</guid>
      <pubDate>Thu, 09 May 2024 16:06:37 GMT</pubDate>
    </item>
    <item>
      <title>AlphaMath 几乎为零：流程无流程监督</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnuaab/alphamath_almost_zero_process_supervision_without/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.03553 代码：https ://github.com/MARIO-Math-Reasoning/Super_MARIO 模型：https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B 摘要： &lt; blockquote&gt; 大型语言模型 (LLM) 的最新进展极大地增强了他们的数学推理能力。然而，这些模型仍然难以解决需要多个推理步骤的复杂问题，经常导致逻辑或数值错误。虽然数字错误很大程度上可以通过集成代码解释器来解决，但识别中间步骤中的逻辑错误更具挑战性。此外，手动注释这些培训步骤不仅成本高昂，而且需要专业知识。在本研究中，我们引入了一种创新方法，通过利用蒙特卡罗树搜索（MCTS）框架自动生成过程监督和评估信号，从而消除了手动注释的需要。本质上，当法学硕士经过良好的预训练时，只需要数学问题及其最终答案来生成我们的训练数据，而不需要解决方案。我们继续训练一个阶梯级价值模型，旨在改进法学硕士在数学领域的推理过程。我们的实验表明，使用由 MCTS 增强的法学硕士自动生成的解决方案可以显着提高模型处理复杂数学推理任务的能力。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnuaab/alphamath_almost_zero_process_supervision_without/</guid>
      <pubDate>Thu, 09 May 2024 10:49:53 GMT</pubDate>
    </item>
    <item>
      <title>用于在网格地图上寻找路径的 RL - SLAM。成功率问题（SAC + HER）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnr2w8/rl_for_path_finding_on_the_grid_map_slam_success/</link>
      <description><![CDATA[      大家好！我一直在努力解决涉及车辆在黑白地图上移动的自定义环境。您可以将地图视为二进制网格。车辆的目标是到达白色区域内随机选择的目的地。当车辆到达目的地、撞到地图上的黑色区域或撞上边界墙时，游戏结束。 我使用 SB3 中的 SAC + HER，但我一直在使用一个多月以来，一直在努力实现 0.6 以上的成功率。这是我尝试过的方法：  使用熵系数进行实验 添加模拟激光雷达光束和到最近黑色像素的距离等观测结果 使用不同的超参数集进行实验（超过 500 次运行） 调整缓冲区大小  大多数情况下，代理很难到达起始点正前方黑墙之外的区域点。 我的奖励计算如下： rew = -np.power(np.dot(np.abs(achieved_goal[:1] -desired_goal), Weights_array ), 0.5)  观察空间定义为： self.observation_space =spaces.Dict({ &#39;observation&#39;:spaces.Box(low= 0，高= 1，形状=（6 + self.num_lidar_beams，），dtype = np.float32），&#39;实现的目标&#39;：spaces.Box（低= 0，高= 1，形状=（6，），dtype = np .float32), &#39;desired_goal&#39;:spaces.Box(low=0, high=1, shape=(6,), dtype=np.float32), })  动作空间：  self.action_space = space.Box(low=np.array([-1, 0]), high=np.array([1, 1]), dtype=np.float32 )  这些动作控制车辆的转向角度和速度。 状态是： self.state[0 ] = self.normalize_state(self.x,self.screen_width,0) self.state[1] = self.normalize_state(self.y,self.screen_height,0) self.state[2] = hdg_norm self.state[3 ] = v self.state[4] = self.normalize_state(np.cos(np.deg2rad(hdg)), 1, 0) self.state[5] = self.normalize_state(np.sin(np.deg2rad(hdg) )), 1, 0) self.state[-self.num_lidar_beams:] = litar_distances # 更新该州的激光雷达数据  我附上了四张图片：  环境的外观 环境的另一个视图 按区域说明成功率的地图 显示随时间变化的成功率的图  https:// /preview.redd.it/6dmfb58qnczc1.png?width=3364&amp;format=png&amp;auto=webp&amp;s=6b13c219c427f66e1035cf16bf78b4baefed7fc2 自从我工作以来，我非常感谢任何建议对此已经有一段时间了。有没有人从事过类似的项目并愿意分享代码？或者你知道 GitHub 上类似 2D 问题的示例吗（不是停车环境，因为我已经广泛研究过这些问题）？ 如果没有地图（所有像素都是白色），成功率会达到 1.0&lt; /p&gt;   由   提交/u/Sharp-Record1600  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnr2w8/rl_for_path_finding_on_the_grid_map_slam_success/</guid>
      <pubDate>Thu, 09 May 2024 07:06:19 GMT</pubDate>
    </item>
    <item>
      <title>将 tanh 应用于正常样本后，SAC 估计概率密度函数的视觉表示。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnnhye/visual_representation_for_sac_estimated/</link>
      <description><![CDATA[Desmos PDF 图表 我创建这个是因为我不明白如何从策略输出中跨越我的 std（标准差）。我听说将值限制在区间 (0, 1] 中很好，但是简单地限制时会有一些梯度损失，该策略可以开始输出低于 0 并且永远不会恢复。所以我尝试使用 sigmoid 作为 std 的激活，但过了一会儿，它使熵停留在 1，从而破坏了策略。 查看 PDF 后，我想我会尝试使用 sigmoid * 0.5，但也许我应该将 alpha 更改为αH(a(s)) 是动态的，但我不知道，如果你在熵正则化方面也遇到了麻烦，并且动态改变 alpha 对你有帮助，请告诉我。无论如何，我在互联网上的其他地方没有发现这个看起来有帮助。 p&gt;    提交者 /u/O_CLIPE   [链接]   ; [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnnhye/visual_representation_for_sac_estimated/</guid>
      <pubDate>Thu, 09 May 2024 03:22:53 GMT</pubDate>
    </item>
    <item>
      <title>努力从头开始实施 PPO。 （健身房）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnhcoa/struggling_with_ppo_from_scratch_implementation/</link>
      <description><![CDATA[过去 5 个月我一直致力于从头开始实施 PPO。除了 numpy 等数值计算库之外，我大部分工作都是从头开始做的。它从监督学习网络开始到现在。我似乎无法理解。我读过的每篇论文都是 A. 过时/不正确 B. 不完整。没有论文完整描述它们的作用以及它们使用的超级参数。我尝试阅读 SB3 代码，但它与我的实现太不同了，我只是不明白发生了什么，因为它只有这么多文件，我找不到细节。所以我只是发布我的后向方法，如果有人想阅读它并告诉我一些错误/建议。会很好！旁注：我使用标准梯度下降进行了优化，而批评家只采用状态。我没有使用 GAE，因为我试图最大限度地减少潜在的故障点。所有超参数都是标准值。 def back(self): T = len(self.trajectory[&#39;actions&#39;]) for i in range(T): G = 0 for j in range(i, T): current = self.trajectory[&#39;rewards&#39;][j] G += current * pow(self.gamma, j - i) # G = np.clip(G, 0, 15) # CRITIC STUFF if np.isnan(G):break state_t = self.trajectory[&#39;states&#39;][i] action_t = self.trajectory[&#39;actions&#39;][i] # 计算state_t的批评值critic_value = self.critic(state_t) # print(f&quot;Critic: {critic_value}&quot;) # print(f&quot;G: {G}&quot;) # 计算状态-动作对的优势 Advantages = G -ritic_value # print(f&quot;&quot;&quot;&quot;Return: {G} # 预期回报：{critic}&quot;&quot;&quot;) # 旧参数内容 new_policy = self.forward(state_t, 1000) # PPO 内容 Ratio = new_policy / action_t Clipped_ratio = np.clip(ratio, 1.0 - self .clip, 1.0 + self.clip) surrogate_loss = -np.minimum(ratio * 优势, Clipped_ratio * 优势) # entropy_loss = -np.mean(np.sum(action_t * np.log(action_t), axis=1)) # 参数向量weights_w = self.hidden.weights.flatten()weights_x = self.hidden.bias.flatten()weights_y = self.output.weights.flatten()weights_z = self.output.bias.flatten()weights_w = np .concatenate((weights_w,weights_x))weights_w = np.concatenate((weights_w,weights_y)) param_vec = np.concatenate((weights_w,weights_z)) param_vec.flatten() loss = np.mean(surrogate_loss) # + self. l2_regularization(param_vec) # print(f&quot;loss: {loss}&quot;) # 反向传播 next_weights = self.output.weights self.hidden.layer_loss(next_weights, loss, tanh_derivative) self.hidden.zero_grad() self.output.zero_grad () self.hidden.backward() self.output.backward(loss) self.hidden.update_weights() self.output.update_weights() self.critic_backward(G)  &lt; !-- SC_ON --&gt;  由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnhcoa/struggling_with_ppo_from_scratch_implementation/</guid>
      <pubDate>Wed, 08 May 2024 22:26:26 GMT</pubDate>
    </item>
    <item>
      <title>MARL 零和博弈中自我对战 vs 双神谕</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnei6i/self_play_vs_double_oracle_in_marl_zerosum_games/</link>
      <description><![CDATA[据我了解： 在自我对战中，智能体直接与自身版本（历史版本或当前版本）进行比赛复制）以改进其策略。基本上计算出“到目前为止的一切”的最佳响应。另一方面，双预言算法涉及为两个代理维护显式策略集，并逐步扩展。但扩展是相似的。两者都根据其他代理模型和过去模型的混合策略计算最佳响应，并将这个新模型添加到自己的模型列表中。 我理解对吗？坦率地说，我看到的谈论这些概念的论文对我来说太复杂了，但似乎基本原理并不那么深刻。也许我在理解这些算法时遗漏了一些重要的东西？ 在我们使用函数逼近器（例如神经网络）的情况下，我们什么时候停止在这两种情况下寻找最佳响应？  p&gt; 一个比另一个更好吗？什么是权衡？也许更有可能收敛到纳什均衡？   由   提交 /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnei6i/self_play_vs_double_oracle_in_marl_zerosum_games/</guid>
      <pubDate>Wed, 08 May 2024 20:27:14 GMT</pubDate>
    </item>
    <item>
      <title>具有重放缓冲区的 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cndw4j/rl_algorithms_with_replay_buffers/</link>
      <description><![CDATA[什么是具有重播缓冲区但适用于离散动作空间（除了 DQN）的 RL 算法？   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cndw4j/rl_algorithms_with_replay_buffers/</guid>
      <pubDate>Wed, 08 May 2024 20:01:35 GMT</pubDate>
    </item>
    <item>
      <title>对高斯策略的策略梯度的质疑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cna10j/doubt_about_policy_gradient_with_gaussian_policy/</link>
      <description><![CDATA[      嗨， 我正在观看 YouTube 上的 Sergey Levine 课程。  我现在使用策略梯度，到目前为止零问题，但一些数学表达式让我感到困惑。 在此视频中 https://youtu.be/VSPYKXm_hMA?si=WIdrh41TX8RHXYu3&amp;t=238 at 3:58 他使用了以下等式： https://preview.redd.it /dl9qxk3vj8zc1.png?width=426&amp;format=png&amp;auto=webp&amp;s=b6f322d291b88dd3627cd16812bb0532fa684fcb 我不明白为什么对数概率等于这个距离以及如何计算梯度这个的。  谢谢。   由   提交 /u/RikoteMasterrrr   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cna10j/doubt_about_policy_gradient_with_gaussian_policy/</guid>
      <pubDate>Wed, 08 May 2024 17:18:37 GMT</pubDate>
    </item>
    <item>
      <title>问题：DQN 纸牌游戏 让代理了解他可以玩哪些纸牌有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cn19em/question_dqn_card_game_does_it_make_sense_to_have/</link>
      <description><![CDATA[我正在尝试训练 AI 使用 DQN 玩纸牌游戏。 相关规则的简短摘要概述纸牌游戏： ~40 张纸牌，4 名玩家。每个玩家获得 10 张随机牌。进行 10 轮，每个玩家打出 1 张牌（连续）。现在，您并不总是被允许打出手中的每张牌，因为这很大程度上取决于该轮中已经打出的牌。 我目前正在做的是在我的神经网络中输出 40 个动作（40 张牌），然后从所有 40 张中取出价值最高的一张。如果这张牌不在特工手中，或者在给定情况下玩不合法，我就会取消游戏，给它一个不好的奖励，然后继续下一个。  现在我想知道这是否有意义，或者与在给定情况下从所有法律行动中选择具有最高价值的行动相比是否没有任何好处，这样我会显然，不必学习有关哪些牌可以合法玩以及经纪人手中有哪些牌的所有规则。 （我现在使用的状态是：handcards（40），cardsonboard（40）。One Hot Encoded） 我真的没有看到我当前的做法有任何优势，我只是不确定是否从所有操作的不断变化的子集中选择代理操作时，在训练过程中可能会出现任何问题吗？   由   提交 /u/TratanusII   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cn19em/question_dqn_card_game_does_it_make_sense_to_have/</guid>
      <pubDate>Wed, 08 May 2024 10:29:44 GMT</pubDate>
    </item>
    </channel>
</rss>