<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 21 Mar 2024 00:58:40 GMT</lastBuildDate>
    <item>
      <title>RL 中的重置功能：初始状态还是当前状态？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/</link>
      <description><![CDATA[大家好， 我是强化学习新手。我目前正在致力于实现一个 RL 环境，以使用恒温器控制房间的温度。我正处于重置功能的十字路口，希望获得一些见解。 该环境代表一个带有恒温器的房间，其目标是将温度保持在所需的设定值，同时最大限度地减少能源消费。环境状态由当前温度和能耗表示，操作包括调整恒温器设置以增加或减少温度设定点。  我想到了两种方法：  重置为初始状态：此选项涉及在每个方法开始时将环境重置为其初始配置情节，其中温度设置为预定义的起始值。 （设置为最小状态）。 重置为当前状态：或者，重置功能可以将环境返回到房间的当前状态。 &lt; /ol&gt; 我特别喜欢重置当前状态的想法，因为它与我在代理决策过程中的方法一致。在每个步骤中，当代理选择一个操作时，我计划检查代理是否可以根据状态向量中表示的当前状态执行该操作，然后给出正奖励，否则给出负奖励。  问题：在这种环境下，您认为哪种方法更适合我的强化学习算法中的重置功能？我还需要考虑任何其他注意事项吗？ 提前感谢您的帮助！   由   提交/u/Few-Papaya101  /u/Few-Papaya101 reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjmh6g/reset_function_in_rl_initial_state_or_current/</guid>
      <pubDate>Wed, 20 Mar 2024 19:47:19 GMT</pubDate>
    </item>
    <item>
      <title>与决斗+c51作斗争，需要第二双眼睛。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjlmsa/struggling_with_duellingc51_need_a_second_pair_of/</link>
      <description><![CDATA[亲爱的 RL 社区，  在尝试了很多个小时在我的框架中实现 c51+dueling 后，我终于准备好给予向上。作为我最后的努力，我发布此内容是希望一些天才可以帮助我找到算法中的错误。  我正在尝试使用 TorchSharp 在 C# 中创建一个 DRL 库，并且我一直在尝试提供 Rainbow。事实证明这真的很困难（或者我还没有准备好） 长话短说，我的损失并没有减少，我不确定我在实现算法时在哪里犯了错误，我尝试了很多事情并且想不通。  有人可以看一下并尝试找出我的错误吗？  我的代码在这里： 网络： https://github.com/asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/DQN/NN/Variants/DuelingDNQ_C51_Noisy.cs  代理： https://github.com /asieradzk/RL_Matrix/blob/master/src/RLMatrix/Agents/DQN/Variants/DQNRainbow.cs  为那些讨厌 C# 的人提供的 Python 翻译： https://github.com/asieradzk/RL_Matrix/blob/master/src/RLMatrix /Agents/DQN/Variants/PythonRainbowTranslation.txt  也欢迎道德支持。    由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjlmsa/struggling_with_duellingc51_need_a_second_pair_of/</guid>
      <pubDate>Wed, 20 Mar 2024 19:12:20 GMT</pubDate>
    </item>
    <item>
      <title>SB3 的 DQN 不起作用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bj8nh9/dqn_from_sb3_doesnt_work/</link>
      <description><![CDATA[我有一个自定义的 boid 植绒环境并使用 SB3 实现了 DQN。 但是，它继续运行并且没有输出任何内容。似乎无法找出错误。 代码：https://drive .google.com/drive/folders/1zoQSrLOVO13TBGtoJhkg5LwVoKEmM2gT?usp=sharing   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bj8nh9/dqn_from_sb3_doesnt_work/</guid>
      <pubDate>Wed, 20 Mar 2024 08:31:23 GMT</pubDate>
    </item>
    <item>
      <title>尝试在 PyTorch 中实现 crossQ 不起作用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bj3rln/trying_to_implement_crossq_in_pytorch_does_not/</link>
      <description><![CDATA[你好，我正在尝试实现： https://openreview.net/pdf?id=PczQtTsTIX 而且似乎无法获得好的结果。我已经在 Ant-v4 中尝试过了 在更新贝尔曼方程中对我的 SAC 代码进行了以下更改： def update_critic(self, state_batch: torch.tensor, action_batch：torch.tensor，reward_batch：torch.tensor，next_state_batch：torch.tensor，mask_batch：torch.tensor）-&gt; torch.tensor: “””使用 Soft Bellman 方程更新批评家：param state_batch：从内存中提取的状态批次：param action_batch：从内存中提取的操作批次：paramreward_batch：从内存中提取的奖励批次：param next_state_batch：从内存中提取的下一个状态批次批次：param mask_batch：完成的掩码：返回：Q1 的浮动损失，Q2 的浮动损失“”“” next_state_action_batch, next_state_log_pi_batch, _ = self.policy.sample(next_state_batch, False, False) if self.config_agent[&#39;crossqstyle&#39;]: # (bsz x 2, nstate) cat_states = torch.cat([state_batch, next_state_batch], 0) # (bsz x 2, nact) cat_actions = torch.cat([action_batch, next_state_action_batch], 0) # (bsz x 2, 1) qfull1, qfull2 = self.critic(cat_states, cat_actions) # 分离 Q q1, q1next = torch .chunk(qfull1, chunks=2, dim=0) q2, q2next = torch.chunk(qfull2, chunks=2, dim=0) min_qnext = torch.min(q1next, q2next) - self.alpha * next_state_log_pi_batch next_qvalue = (奖励_batch + mask_batch * self.config_agent[&#39;gamma&#39;] * min_qnext).detach() 否则：使用 torch.no_grad(): q1next_target, q2next_target = self.critic_target(next_state_batch, next_state_action_batch) min_qnext = torch.min(q1next_target, q2next_target) - self.alpha * next_state_log_pi_batch next_qvalue =reward_batch + mask_batch * self.config_agent[&#39;gamma&#39;] * min_qnext q1, q2 = self.critic(state_batch, action_batch) q_loss, q1_loss, q2_loss = self.calculate_q_loss(q1, q2, next_qvalue) # 默认 self.critic_optim.zero_grad() q_loss.backward() self.critic_optim.step()  我还在批评者和演员中添加了批规范层。例如批评者的初始化： # Activations if activate == &quot;relu&quot;: self.activation = nn.ReLU() elifactivation == &quot;leaky_relu&quot;: self.activation = nn.LeakyReLU() elif 激活 == &quot;tanh&quot;: self.activation = nn.Tanh() else: 如果 bn_mode == &quot;bn&quot; 则引发 NotImplementedError: BN = nn.BatchNorm1d elif bn_mode == &quot;brn&quot; : BN = BatchRenorm1d else: raise NotImplementedError # Layers self.q1_list = nn.ModuleList() self.q2_list = nn.ModuleList() # BN层0 - 根据crossQ的代码 if use_batch_norm: self.q1_list.append(BN( state_dim + action_dim，动量=bn_momentum）） self.q2_list.append（BN（state_dim + action_dim，动量=bn_momentum）） self.q1_list.append（nn.Linear（int（（state_dim + action_dim）），hidden_​​dim）） self. q1_list.append(self.activation) self.q2_list.append(nn.Linear(int((state_dim + action_dim)),hidden_​​dim)) self.q2_list.append(self.activation) if use_batch_norm: self.q1_list.append(BN (hidden_​​dim,动量=bn_momentum)) self.q2_list.append(BN(hidden_​​dim,动量=bn_momentum)) for i in range(num_layers - 1): self.q1_list.append(nn.Linear(hidden_​​dim,hidden_​​dim)) self. q1_list.append(self.activation) self.q2_list.append(nn.Linear(hidden_​​dim,hidden_​​dim)) self.q2_list.append(self.activation) 如果use_batch_norm: self.q1_list.append(BN(hidden_​​dim)) self.q2_list .append(BN(hidden_​​dim)) self.q1_list.append(nn.Linear(hidden_​​dim, 1)) self.q2_list.append(nn.Linear(hidden_​​dim, 1))  是否有人有什么想法吗？ 我的 SAC 版本学习得很好。然而，在论文中，他们还报告了以批评家中的 tanh 激活函数作为基线的 SAC，在我的例子中，这也不起作用（默认激活是 ReLU）。我尝试使用论文中报告的超参数。 也许我忘记了一些技巧？   由   提交/u/LazyButAmbitious  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bj3rln/trying_to_implement_crossq_in_pytorch_does_not/</guid>
      <pubDate>Wed, 20 Mar 2024 03:14:41 GMT</pubDate>
    </item>
    <item>
      <title>“通过老虎机优化识别一般反应条件”，Wang 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biu16l/identifying_general_reaction_conditions_by_bandit/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biu16l/identifying_general_reaction_conditions_by_bandit/</guid>
      <pubDate>Tue, 19 Mar 2024 20:14:24 GMT</pubDate>
    </item>
    <item>
      <title>关于马厩基线 3 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bis0k4/question_about_stables_baselines_3/</link>
      <description><![CDATA[我可以将 Stable Baselines3 用于纯数字强化学习项目吗？或者它只适合基于图像的项目？   由   提交 /u/spacecowboyalien   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bis0k4/question_about_stables_baselines_3/</guid>
      <pubDate>Tue, 19 Mar 2024 18:53:34 GMT</pubDate>
    </item>
    <item>
      <title>寻找一些基线逆强化学习环境来修补</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1birzi3/looking_for_some_baseline_inverse_rl_environments/</link>
      <description><![CDATA[大家好， 我正在对某些强化学习环境中的特征重要性进行一些分析研究。我想了解当模型在工程奖励函数和一些新的逆学习奖励函数上进行训练时，特征的重要性值如何变化。想要尝试一下，所以我正在寻找你们都检查过的任何 github 存储库/软件包，我可以看到有关在我的计算机上旋转的信息  提前致谢！   由   提交/u/bbri826  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1birzi3/looking_for_some_baseline_inverse_rl_environments/</guid>
      <pubDate>Tue, 19 Mar 2024 18:52:22 GMT</pubDate>
    </item>
    <item>
      <title>为多个代理寻找简单、对抗性、可扩展的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bip33c/looking_for_simple_adversary_scalable_environment/</link>
      <description><![CDATA[嗨， 我正在寻找允许以下操作的环境： 1. 控制多个代理的策略（是否不必同时支持多个代理训练）  相对简单 - 比国际象棋、围棋等简单 观察形状是相对较小 - 比 atari 游戏小 可以改变观察形状，最好是代理数量。   ​ 这种环境的一个例子是类似 petting-zoo connect-four/tic-tac-toe 但会允许控制代理数量或数字板大小（例如，使 tic tac toe 板 10x10） 我知道这可以通过编写自定义环境来完成，但我更愿意花更多时间来测试不同的算法以及代理数量和观察规模对其的影响。  这种环境的一个例子是类似 petting-zoo connect-four/tic-tac-toe 的东西，但允许控制代理的数量或数字板的大小（例如，制作 tic tac-toe 板） 10x10) 感谢您的所有建议！   由   提交/u/MrCogito_hs   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bip33c/looking_for_simple_adversary_scalable_environment/</guid>
      <pubDate>Tue, 19 Mar 2024 16:54:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DreamerV3 使用 actor-critic 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biky9x/why_dreamerv3_uses_the_actorcritic_models/</link>
      <description><![CDATA[       &amp;# 32；由   提交/u/yulinzxc   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biky9x/why_dreamerv3_uses_the_actorcritic_models/</guid>
      <pubDate>Tue, 19 Mar 2024 14:00:28 GMT</pubDate>
    </item>
    <item>
      <title>重新审视记忆幺半群的循环强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bii3f1/revisiting_recurrent_reinforcement_learning_with/</link>
      <description><![CDATA[ 由   提交/u/smorad  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bii3f1/revisiting_recurrent_reinforcement_learning_with/</guid>
      <pubDate>Tue, 19 Mar 2024 11:35:17 GMT</pubDate>
    </item>
    <item>
      <title>“卡尔曼 -> 基于模型的强化学习”的文献建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1biggav/literature_advice_for_kalman_modelbased_rl/</link>
      <description><![CDATA[你们中有人知道一本好书吗？它概括了从卡尔曼滤波器的信号处理、系统识别到基于模型的强化学习中的世界模型？    由   提交 /u/carlowilhelm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1biggav/literature_advice_for_kalman_modelbased_rl/</guid>
      <pubDate>Tue, 19 Mar 2024 09:49:51 GMT</pubDate>
    </item>
    <item>
      <title>了解用于自定义 mujoco env 的gymnasium.make()</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bifl5f/understanding_gymnasiummake_for_custom_mujoco_envs/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bifl5f/understanding_gymnasiummake_for_custom_mujoco_envs/</guid>
      <pubDate>Tue, 19 Mar 2024 08:45:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习库 PyTorch</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bifiho/reinforcement_learning_library_pytorch/</link>
      <description><![CDATA[嗨， 我是强化学习领域的新手，目前正在寻找合适的库来实施我的深度强化学习研究项目。它应该是一个 Python 库，并使用 PyTorch 进行深度学习集成。我想在（多个）GPU 上并行运行多个环境。由于我的环境将包含神经网络，因此它应该能够直接在 GPU 上运行并使用张量。 有关合适库的任何经验。到目前为止，我发现了两个有趣的库  TorchRL TorchRL - torchrl 主要文档（pytorch.org）  自主学习库自主学习库 —autonomous-learning-library 0.9.1 文档  我想听听您的意见和经历。谢谢  &amp;# 32；由   提交/u/Opposite_Youth_442   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bifiho/reinforcement_learning_library_pytorch/</guid>
      <pubDate>Tue, 19 Mar 2024 08:39:56 GMT</pubDate>
    </item>
    <item>
      <title>给对强化学习感兴趣的澳大利亚人的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bi9v8q/advice_for_an_australian_with_an_interest_in_rl/</link>
      <description><![CDATA[您好，我是澳大利亚人，目前在墨尔本 RMIT 攻读计算机科学学士学位。我对深度学习，更具体地说是强化学习非常感兴趣，因此我打算明年开始攻读人工智能硕士学位，然后再攻读深度学习博士学位。 我真的很想进入研究领域在一家大公司工作并尽可能地帮助挑战极限，但我不确定我在澳大利亚的工作/研究前景。此外，我不确定墨尔本大学莫纳什大学的博士学位是否会给我足够好的证书来实现我想要的目标。 我真的只是想知道这是否最适合我学习在这里并在这里找到研究，或者是否去海外更好，如果是的话在什么阶段（在博士学位之前或之后等）。任何建议将不胜感激。   由   提交 /u/TrueExcaliburGaming   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bi9v8q/advice_for_an_australian_with_an_interest_in_rl/</guid>
      <pubDate>Tue, 19 Mar 2024 02:47:35 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>