<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 22 Jul 2024 06:22:31 GMT</lastBuildDate>
    <item>
      <title>如何在 Linux 上安装 D4RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e94noy/how_to_install_d4rl_on_linux/</link>
      <description><![CDATA[如果能给我一些关于在我的 Ubuntu 24.04 操作系统上安装 D4RL 的指导，我将非常感激。  我尝试按照 github 上的步骤安装 D4RL，但是在从互联网上下载的 mujoco 文件和 mujoco-py 之间遇到了错误。  提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e94noy/how_to_install_d4rl_on_linux/</guid>
      <pubDate>Mon, 22 Jul 2024 03:39:39 GMT</pubDate>
    </item>
    <item>
      <title>训练现实世界 RL 模型以对 3D 可变形物体进行长距离操控</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8www2/training_realworld_rl_model_of_longhorizon/</link>
      <description><![CDATA[嗨， 我正在研究一个机器人操作任务，该任务可以处理 3D 可变形物体 (DO)，例如生肉。我的一项任务是使用 RL 训练 Delta 机器人使用真空吸盘拾取和投掷。 由于 3D DO 的属性未知，并且大多数模拟目前都在 1D 和 2D 线性 DO 上进行，因此我决定进行连续动作空间、无模型真实世界训练，而无需任何模拟。 我知道我不能进行超过 400-500 次实验，因此数据有限，策略也不是最优的。但是，此处训练的目的是在这些测试中获得最佳策略。因此，我想问几个问题，并希望听取您的建议：  您应该推荐哪种方法？端到端训练 https://arxiv.org/abs/2406.13453 或任务分解，如 https://doi.org/10.3390/biomimetics8020240 我应该选择哪种算法来提高数据效率？DDPG、D4PG、HAC、.... 如果我错了，请纠正我。由于这是操纵 DO 的一系列动作，因此在执行所选动作后，“下一个状态”毫无用处。视觉系统是分开的，所以我没有使用图像作为状态，状态是{初始 S_obj 和 S_ee}。  我刚刚开始学习 RL，所以如果你们能帮助我，我将不胜感激。由于这对我来说还很新，我可能会问更多后续问题，我非常感谢你们的帮助。非常感谢！    提交人    /u/Fish_Chandle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8www2/training_realworld_rl_model_of_longhorizon/</guid>
      <pubDate>Sun, 21 Jul 2024 21:19:05 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义 Python 和 Unity 引擎完成的虚拟 AI 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</link>
      <description><![CDATA[       由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</guid>
      <pubDate>Sun, 21 Jul 2024 15:19:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MARL 解决 N vs N 追击-逃避游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</link>
      <description><![CDATA[有谁知道使用 MARL 解决 N vs N 追击规避游戏的 SOTA 工作吗？我研究过基于策略的方法（例如 MADDPG、MAPPO）和基于价值的方法（例如 QMIX、VDN、COMA 等）。大部分工作都是针对完全竞争场景（例如 GRF、SMAC、hanabi 挑战、使用启发式或随机移动的固定对手的 MPE）完成的。MADDPG 仅在混合动机场景（N vs 1）中解决了 MPE（多智能体粒子环境），其中合作智能体数量较少，对手为 1 个，由 MADDPG vs DDPG 捕获。我认为它在 N vs N 情况下无法扩展，因为 N 更高。如果有人能提出使用 MARL 进行混合动机和完全竞争设置的任何工作或想法，那将很有帮助。    提交人    /u/Meta_Sage_247   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</guid>
      <pubDate>Sun, 21 Jul 2024 13:36:51 GMT</pubDate>
    </item>
    <item>
      <title>“学习用语言模拟世界”，Lin 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8bbw1/learning_to_model_the_world_with_language_lin_et/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8bbw1/learning_to_model_the_world_with_language_lin_et/</guid>
      <pubDate>Sun, 21 Jul 2024 01:47:37 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理出现奇怪的周期性峰值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</guid>
      <pubDate>Sat, 20 Jul 2024 23:02:51 GMT</pubDate>
    </item>
    <item>
      <title>DQN 高估问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</link>
      <description><![CDATA[      我一直在尝试从头开始实现 DQN，并创建了一个测试环境来展示我遇到的问题。环境在每一步奖励 1，并在 100 步后终止。 DQN 不采取任何行动，仅尝试根据步骤号预测每一步的累积奖励。 问题是，DQN 为后续状态输出极高的 Q 值，这与实际值相反，并且需要几个时期才能开始输出半准确值。 https://preview.redd.it/n1fd4g054jdd1.png?width=915&amp;format=png&amp;auto=webp&amp;s=5d32fa33a3ee048528b39b61239d26b9dfaf0d05 代码（根据用户 dieplstks 的评论略作修改）： import torch from torch import nn, optim import matplotlib.pyplot 作为 plt 导入随机 导入 numpy 作为 np 比例 = 100 折扣 = .9 奖励 = [1 for i in range(scale-1)] + [0] net = nn.Sequential( nn.Linear(4,32), nn.Mish(), nn.Linear(32,32), nn.Mish(), nn.Linear(32,1)) opt = optim.Adam(net.parameters()，lr=1e-2) loss = nn.MSELoss() def step(c): global discount, rewards, net, opt opt.zero_grad() qvals = [] for i in range(scale): inp = torch.tensor([[i/20 for _ in range(4)]], dtype=torch.float32) q = net(inp) qvals.append(q) if i == scale-1: next_q = torch.zeros(1,1) else: next_inp = torch.tensor([[(i+1)/20 for _ in range(4)]], dtype=torch.float32) next_q = net(next_inp).detach() target = discount*next_q + rewards[i] q_loss = loss(q, target) #opt.zero_grad() q_loss.backward() nn.utils.clip_grad_norm_(net.parameters(), 1) opt.step() if c%1 == 0: plt.plot(list(range(scale)), actual_vals) plt.plot(list(range(scale)), torch.concat(qvals).detach().numpy()) plt.savefig(f&#39;zoo{c}.png&#39;) plt.clf() #plt.show() actual_vals = np.zeros((scale),dtype=np.float32) next_val = 0 for i in reversed(range(scale)): current_val = next_val * discount + rewards[i] actual_vals[i] = current_val next_val = current_val for i in range(100): step(i)     提交人    /u/AUser213   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</guid>
      <pubDate>Fri, 19 Jul 2024 19:51:56 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习用于推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</link>
      <description><![CDATA[嗨：我正在寻找合适的 Python（最好是基于 PyTorch）深度强化学习库来实现多会话对话推荐引擎。理想情况下，强化学习将无模型且脱离策略，并且必须与数据库系统交互以获取用户偏好和其他上下文数据。有什么建议吗？    提交人    /u/Extra_Reflection9056   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</guid>
      <pubDate>Fri, 19 Jul 2024 08:00:25 GMT</pubDate>
    </item>
    <item>
      <title>通过将实时屏幕截图作为输入并预测要模拟的 Windows 鼠标/键盘输入，训练 DQN 代理来玩自定义 Fortnite 地图。以下是可视化的卷积过滤器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</link>
      <description><![CDATA[        提交人    /u/voidupdate   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</guid>
      <pubDate>Fri, 19 Jul 2024 01:35:34 GMT</pubDate>
    </item>
    <item>
      <title>RL 教科书包含逆向强化学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</link>
      <description><![CDATA[寻找包含逆强化学习 (IRL) 部分的 RL 教科书。我只熟悉 Dixon 的《金融机器学习》一书，这本书很棒，但我还在寻找更多值得阅读的内容。  任何建议都值得赞赏！此外，如果您知道任何带有相应 github 的 IRL 论文，我也会喜欢的。    提交人    /u/Voltimeters   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</guid>
      <pubDate>Fri, 19 Jul 2024 00:28:38 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 openAI Gym 中自定义环境的平等约束问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</link>
      <description><![CDATA[您好， 我正在为优化主题创建自定义环境。以下是一些详细信息： 代理观察一个连续变量：a 代理采取两个操作：b 和 c 我希望我的代理学习采取操作 b、c，使得 b + c = a 且 c 最大。 我知道这很简单。在这种情况下，代理应该采取操作 c，使得 c=a，但这只是我整个环境的一个小组成部分。 * 如何以尊重此约束的方式对环境进行建模（应始终尊重约束） * 如何对奖励进行建模。我试图将奖励作为 c，但由于这是一个绝对值，因此代理不会改善行为。 * 知道我的观察和行动都是连续变量，哪种类型的算法最适合这类问题。 提前谢谢您。    提交人    /u/Effective_Farm_4844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</guid>
      <pubDate>Thu, 18 Jul 2024 18:11:33 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 更新了，有什么不同</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</link>
      <description><![CDATA[      DreamerV3 最近已更新。论文中有一些变化。不幸的是，我找不到与 2023 版相比发生了哪些变化的表格。我注意到了一些变化。例如，动态损失权重从 0.5 变为 1。评论家使用真实转换来计算重放损失。优化器已经改变，他们使用自动梯度标准剪辑。我想知道是否有人注意到其他重大变化。如果有人有更改表并愿意分享就太好了！ https://preview.redd.it/r94zls5159dd1.png?width=640&amp;format=png&amp;auto=webp&amp;s=babb912867b877dd94ed98f5de6b52ddb46a1f3a    提交人    /u/yulinzxc   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</guid>
      <pubDate>Thu, 18 Jul 2024 10:14:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</link>
      <description><![CDATA[您好， 我正在学习 Richard Sutton 的书，对强化学习有了一些了解，也将其应用于一些项目。 我想写一篇研究论文：希望申请研究型硕士学位。 你们有什么建议吗？我应该考虑哪些事情？你们如何进行研究？ 谢谢！    提交人    /u/Original_Phrase1902   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 18 Jul 2024 09:55:25 GMT</pubDate>
    </item>
    <item>
      <title>设计具有多维行动空间的 PPO AC 框架时遇到的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e65l1b/an_issue_in_designing_an_ac_framework_with_a/</link>
      <description><![CDATA[我正在使用 PPO 编写自己的环境，其中涉及多维动作空间。在适配 PyTorch 框架时，我在更新策略网络的批量采样训练过程中遇到了问题。由于我正在处理多维数组，pi 比率的样本大小与优势函数的大小不同，因此无法将它们相乘以计算总损失函数。该如何解决？ 如： for _ in range(self.K_epochs): for index in BatchSampler(SubsetRandomSampler(range(self.batch_size)), self.mini_batch_size, False): action_mean_now = self.actor(s[index]) dist_now = Categorical(probs=action_mean_now) dist_entropy = dist_now.entropy().view(-1, 1) a_logprob_now = dist_now.log_prob(a[index].squeeze()).view(-1, 1) a_logprob = a_logprob[index].view(-1, 1) ratios = torch.exp(a_logprob_now - a_logprob) surr1 = ratios * adv[index] surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * adv[index] actor_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy  发生这种情况： surr1 = ratios * adv[index] RuntimeError：张量 a 的大小（128）必须与非单例维度 0 处的张量 b 的大小（64）匹配  如何确保策略可以学习多维动作空间的特征，同时避免与优势函数发生大小冲突？    提交人    /u/VermicelliBrave1931   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e65l1b/an_issue_in_designing_an_ac_framework_with_a/</guid>
      <pubDate>Thu, 18 Jul 2024 07:32:48 GMT</pubDate>
    </item>
    <item>
      <title>有人在 RL 中实现过 OGD（正交梯度下降）吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e63o8r/has_anyone_implemented_ogd_orthogonal_gradient/</link>
      <description><![CDATA[我正在通过持续强化学习研究机器人手臂操作，我花了几周时间在现实生活中实现 OGD。但似乎 OGD 在现实生活中不起作用。我认为这是因为与监督学习任务相比，现实生活中的任务太复杂了。有人成功实现过 OGD 吗？ 对于那些还没有听说过 OGD 的人，这里有一个链接：https://arxiv.org/pdf/1910.07104    提交人    /u/ContestOk7604   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e63o8r/has_anyone_implemented_ogd_orthogonal_gradient/</guid>
      <pubDate>Thu, 18 Jul 2024 05:24:56 GMT</pubDate>
    </item>
    </channel>
</rss>