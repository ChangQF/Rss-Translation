<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 26 Apr 2024 03:15:45 GMT</lastBuildDate>
    <item>
      <title>“法学硕士的偏好微调应该利用次优的、符合政策的数据”，Tajwar 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cd8rxr/preference_finetuning_of_llms_should_leverage/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cd8rxr/preference_finetuning_of_llms_should_leverage/</guid>
      <pubDate>Fri, 26 Apr 2024 01:25:53 GMT</pubDate>
    </item>
    <item>
      <title>经历灾难性遗忘的常见深度强化学习实验有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cd05d4/what_are_the_common_deep_rl_experiments_that/</link>
      <description><![CDATA[我一直在通过深度学习理论的视角研究灾难性遗忘，我希望通过 RL 实验来获得一些实证结果。我可以进行一些常见的实验吗？ （在这种情况下，我实际上希望看到遗忘）   由   提交 /u/TitaniumDroid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cd05d4/what_are_the_common_deep_rl_experiments_that/</guid>
      <pubDate>Thu, 25 Apr 2024 19:26:02 GMT</pubDate>
    </item>
    <item>
      <title>Humanoid-v4 步行目标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cczsdr/humanoidv4_walking_objective/</link>
      <description><![CDATA[大家好，我很难知道在使用 REINFORCE 算法时标准偏差网络是否也需要通过 torch 的 back() 进行更新。政策网络正在制定 17 项行动。以及来自单独网络的 17 stddv。我对这个领域比较陌生，希望有人能给我关于如何通过健身房从 Mujoco 环境中训练 Humanoid-v4 f 的指示/示例。    由   提交 /u/lulislomelo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cczsdr/humanoidv4_walking_objective/</guid>
      <pubDate>Thu, 25 Apr 2024 19:12:31 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 OpenAI Gymnasium 定义 flappy Bird 游戏的观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccxs48/how_to_define_observation_space_for_flappy_bird/</link>
      <description><![CDATA[我是 ML 新手，正在为我的 flappy Bird 游戏创建强化学习机器人。我不确定如何定义正确的观察空间，因为观察空间会有所不同。 对于我的观察数据，我返回每个障碍物的所有 X-Y 位置的元组（参见图片）。 然而，屏幕上的障碍物数量可能在 1-20 之间变化。因此，我返回的观察元组的长度可能会有所不同。我不确定如何在我的观察空间中实现这一点。 我当前选择使用的观察空间： observation_space =gym.spaces.Box(- np.inf, np.inf,shape=(2,),dtype=np.float32)  但我很确定这不会有帮助 正在处理img hbe25a0synwc1...   由   提交/u/pubGGWP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccxs48/how_to_define_observation_space_for_flappy_bird/</guid>
      <pubDate>Thu, 25 Apr 2024 17:59:24 GMT</pubDate>
    </item>
    <item>
      <title>DQN 为 CartPole 收敛，但不适用于月球着陆器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccx073/dqn_converges_for_cartpole_but_not_for_lunar/</link>
      <description><![CDATA[我是强化学习新手，我打算脱离 2015 年的论文来实现 DQN，我让它收敛于 cartpole 问题，但它不会用于登月游戏。不确定它是超参数问题、架构问题还是我编码错误。感谢任何帮助或建议 class Model(nn.Module): def __init__(self, in_features=8, h1=64, h2=128, h3=64, out_features=4) - &gt;无： super().__init__() self.fc1 = nn.Linear(in_features,h1) self.fc2 = nn.Linear(h1,h2) self.fc3 = nn.Linear(h2, h3) self.out = nn .Linear(h3, out_features) defforward(self, x): x = F.relu(self.fc1(x)) x = F.dropout(x, 0.2) x = F.relu(self.fc2(x) ) x = F.dropout(x, 0.2) x = F.relu(self.fc3(x)) x = self.out(x) 返回 x policy_network = Model() 导入数学 def epsilon_decay(epsilon, t, min_exploration_prob, Total_episodes): epsilon = max(epsilon - t/total_episodes, min_exploration_prob) 从集合中返回 epsilon import dequelearning_rate = 0.01discount_factor = 0.8exploration_prob = 1.0min_exploration_prob = 0.1decay = 0.999 epochs = 5000 replay_buffer_batch_size = 128 min_replay_buffer_大小 = 5000 replay_buffer = 双端队列(maxlen =min_replay_buffer_size) target_network = Model() target_network.load_state_dict(policy_network.state_dict()) 优化器 = torch.optim.Adam(policy_network.parameters(),learning_rate) loss_function = nn.MSELoss() 奖励 = [] 损失 = [] 损失= -100 for i in range(epochs) :exploration_prob = epsilon_decay(exploration_prob, i, min_exploration_prob, epochs) Terminal = False if i % 30 == 0 : target_network.load_state_dict(policy_network.state_dict()) current_state = env.reset( ) returnssum = 0 p = False while not end : # env.render() if np.random.rand() &lt;探索_prob：action = env.action_space.sample（）否则：state_tensor = torch.tensor（np.array（[current_state]），dtype = torch.float32）与torch.no_grad（）：q_values =policy_network（state_tensor）action = torch .argmax(q_values).item() next_state,reward,terminal,info = env.step(action)rewardsum+=reward replay_buffer.append((current_state,action,terminal,reward,next_state))if(len(replay_buffer)&gt; = min_replay_buffer_size）：minibatch = random.sample（replay_buffer，replay_buffer_batch_size）batch_states = torch.tensor（[transition[0]用于小批量中的过渡]，dtype=torch.float32）batch_actions = torch.tensor（[transition[1]用于过渡）在小批量中]，dtype = torch.int64）batch_terminal = torch.tensor（[小批量中的过渡[2]]，dtype = torch.bool）batch_rewards = torch.tensor（[小批量中的过渡[3]]， dtype=torch.float32）batch_next_states = torch.tensor（[transition[4]用于小批量中的转换]，dtype=torch.float32）与torch.no_grad（）：q_values_next = target_network（batch_next_states）.detach（）max_q_values_next = q_values_next。 max(1)[0] y = batch_rewards + (discount_factor * max_q_values_next * (~batch_terminal)) q_values = policy_network(batch_states).gather(1, batch_actions.unsqueeze(-1)).squeeze(-1) loss = loss_function( y,q_values)loss.append(loss)optimizer.zero_grad()loss.backward()torch.nn.utils.clip_grad_norm_(policy_network.parameters(),10)optimizer.step()如果i%100==0而不是p: print(loss) p = True current_state = next_state returns.append(rewardsum) torch.save(policy_network, &#39;lunar_game.pth&#39;)    由   提交 /u/BigSmoke42169   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccx073/dqn_converges_for_cartpole_but_not_for_lunar/</guid>
      <pubDate>Thu, 25 Apr 2024 17:32:07 GMT</pubDate>
    </item>
    <item>
      <title>稳定的 PPO 基线没有学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccr5gk/stable_baseline_ppo_isnt_learning/</link>
      <description><![CDATA[您好， 我是强化学习社区的新手，我尝试使用自定义环境制作我的第一个模型。这个概念非常简单：一辆必须留在赛道上和道路中间的汽车。它的速度和对它与侧面墙壁的距离的位置的反应会得到奖励，如果这些距离的差异太大，它就会受到惩罚。问题是，当我在训练后测试它时，它似乎会做出随机决定。我使用 python、openaigym 和 stable_baselines3 的 PPO。 这是 2 个代码： trainer：pastebin&lt; /a&gt; 测试人员：pastebin 任何帮助将不胜感激，请原谅我的英语不好。   由   提交/u/PokPok3515  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccr5gk/stable_baseline_ppo_isnt_learning/</guid>
      <pubDate>Thu, 25 Apr 2024 12:51:52 GMT</pubDate>
    </item>
    <item>
      <title>简单的库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccnzn3/simple_libraries/</link>
      <description><![CDATA[我正在寻找 python 中的强化学习简单库。如果它有简单的算法，对我来说就足够了。我专注于棋盘游戏（井字游戏、围棋）。我发现像 rllib 这样的东西对于简单的应用程序来说似乎太复杂了    由   提交 /u/Present_Formal2674   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccnzn3/simple_libraries/</guid>
      <pubDate>Thu, 25 Apr 2024 09:53:39 GMT</pubDate>
    </item>
    <item>
      <title>预期 SARSA 是否存在最大化偏差？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccnmme/does_expected_sarsa_suffer_from_maximization_bias/</link>
      <description><![CDATA[最大化偏差是 Q-Learning 来自更新方程中的 max 运算符。类似地，SARSA（带有 epsilon 贪婪目标）也选择下一个动作，并在大多数情况下根据 max 更新 Q 值。然而，预期的SARSA在更新规则中没有这个最大操作。那么在非贪婪目标政策下，预期的SARSA是否会存在最大化偏差？    由   提交 /u/Then-Law2937    reddit.com/r/reinforcementlearning/comments/1ccnmme/does_expected_sarsa_suffer_from_maximization_bias/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccnmme/does_expected_sarsa_suffer_from_maximization_bias/</guid>
      <pubDate>Thu, 25 Apr 2024 09:28:59 GMT</pubDate>
    </item>
    <item>
      <title>PPO、Mlp 策略中 NN 的输入函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccaphh/input_function_for_nn_in_ppo_mlp_policy/</link>
      <description><![CDATA[我有一个自定义环境，其中包含来自 SB3 的 PPO 并使用 Mlp 策略，但每层有 512 个神经元，共 8 层。我有以下观察空间： min_obs = np.array([[-np.inf, -np.inf, -2.5, -2.5]] * len(self.agents), dtype=np .float32) max_obs = np.array([[np.inf, np.inf, 2.5, 2.5]] * len(self.agents), dtype=np.float32) &lt; p&gt;self.observation_space = space.Box(low=min_obs, high=max_obs, dtype=np.float32) 我知道所有代理的观察结果都被连接起来并传递到输入层，但是如何呢？我的观察可能会随着观察中包含其信息的代理数量的变化而变化。使用的数学函数是什么？它们如何转换为两个观察神经元的 NN 输入？ 代码：https://drive.google.com/file/d/1zhmLnigj_BqNOxuSri0Va0fMRRJs8slW   由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccaphh/input_function_for_nn_in_ppo_mlp_policy/</guid>
      <pubDate>Wed, 24 Apr 2024 21:50:14 GMT</pubDate>
    </item>
    <item>
      <title>具有优先体验重放功能的 DQN 性能突然下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cc7wva/dqn_with_prioritized_experience_replay_sudden/</link>
      <description><![CDATA[      我一直在尝试为车杆环境实现 DQN，但由于某种原因，我不断看到性能突然下降。我认为添加 PER 是解决方案，但它似乎对解决问题没有帮助。 重播缓冲区的大小为 1M，每个游戏的最大长度为 1,000 步，50 步后模型对 50,000 个观察结果进行了采样。奖励是 5 x height^2，折扣率为 0.98，学习率为 0.001，我将梯度范数剪裁为 1。 任何帮助/评论将不胜感激。 代码： https://github.com/Wung8/RL/tree/main  &lt; a href=&quot;https://preview.redd.it/wt5epnk0fhwc1.png?width=895&amp;format=png&amp;auto=webp&amp;s=d4317532b76418837eb12b28fb72570331900ad7&quot;&gt;https://preview.redd.it/wt5epnk0fhwc1.png?width =895&amp;format=png&amp;auto=webp&amp;s=d4317532b76418837eb12b28fb72570331900ad7 https://preview.redd.it/rz1a9jv1fhwc1.png?width=870&amp;format=png&amp;auto=webp&amp;s=30efb5f8f9be21254902340ce8 5b3137b1c842bf https://preview.redd.it/xi269203fhwc1.png ?width=906&amp;format=png&amp;auto=webp&amp;s=870046059e4aeeaf3855c83f089b8fde859183b6   由   提交/u/AUser213  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cc7wva/dqn_with_prioritized_experience_replay_sudden/</guid>
      <pubDate>Wed, 24 Apr 2024 19:57:39 GMT</pubDate>
    </item>
    <item>
      <title>我应该对该数据集使用什么算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbugwv/what_algorithm_should_i_use_for_this_dataset/</link>
      <description><![CDATA[我正在构建一个使用强化学习的推荐系统。 （交互式推荐系统 - 将从 UI 请求用户的反馈/评分） 数据集如下所示： Title/Place_Name nCategories (20) 示例： 标题，度假村，营地，瀑布...... XYZ营地，0 ,1 , 0 . . . p&gt; . . 任何添加、修改或建议都会有用。 （相关教程、指南/任何会用到的东西） 或其他基于 RL 的推荐系统/其他对此数据集有用的推荐系统。 我已经检查了 epsilon 贪婪、linucb ，汤姆森采样但不确定。 谢谢   由   提交/u/be10x  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbugwv/what_algorithm_should_i_use_for_this_dataset/</guid>
      <pubDate>Wed, 24 Apr 2024 10:09:03 GMT</pubDate>
    </item>
    <item>
      <title>规范观察、奖励和价值目标的标准方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbpj9q/what_is_the_standard_way_of_normalizing/</link>
      <description><![CDATA[我正在观看 John Schulman 的深度强化学习实验的具体细节https://www.youtube.com/watch?v=8EcdaCk9KaQ&amp;t=687s&amp;ab_channel=AIPrism 他提到你应该规范奖励、观察，价值目标。我想知道这是否真的完成了，因为我没有在 RL 代码库中看到它。你能分享一些建议吗？   由   提交 /u/miladink   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbpj9q/what_is_the_standard_way_of_normalizing/</guid>
      <pubDate>Wed, 24 Apr 2024 04:35:02 GMT</pubDate>
    </item>
    <item>
      <title>ReBeL 贝叶斯更新</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbgpck/rebel_bayesian_updating/</link>
      <description><![CDATA[我已阅读ReBeL 论文，感觉我已经很好地掌握了 CFR 的结构以及围绕训练的主要思想，并且对使用 Kuhn 扑克进行公众信仰状态的 CFR 工作版本（尽管我知道有时简单版本中不会出现错误）。  我的问题实际上是在论文中基本上是一次性的一行：  当这个游戏开始时，每个玩家对其私人卡牌的信念分布是均匀随机的。 但是，在裁判的每次动作之后，玩家可以通过贝叶斯规则更新他们对自己持有哪张牌的信念分布。同样，玩家可以通过相同的操作更新他们对对手的私人牌的信念分布。因此，在这个游戏中，每个玩家持有每张私人牌的概率是所有玩家在任何时候都知道的。  这是有道理的：主要思想是，如果你有一个分布玩家 a (p_a) 的状态，并且您知道玩家 a 的策略 (policy_a)，那么您可以使用贝叶斯更新玩家 a 对自己状态的信念。该策略明确为 P(action | p_a)。  要更新玩家 b 的状态，您需要导出 P(action | p_b)。您可以通过边缘化玩家 a 的持股并应用相互排他性来做到这一点。  我所困扰的是，是否有一些技巧可以使这种计算在大动作空间和多个玩家的情况下变得易于处理。如果您进行了一场完整的环形游戏，那么您将获得约 1326 ^ 9 个状态的联合分布。我知道使用不兼容矩阵可能有一些技巧，但据我所知，这些技巧中的大多数都会具体化在多人游戏设置中非常大的联合矩阵。 def update_players_states(state_matrix, actor_index ,policy_matrix,action): num_players,num_states = state_matrix.shape new_state = np.zeros_like(state_matrix) p_action = np.dot(state_matrix[actor_index, :],policy_matrix[:,action]) # 获取 np 中索引的所有可能状态.ndindex((num_states,) * num_players): if len(set(indices)) == num_players: # 作为快捷方式，只需使两个状态不兼容，只有当它们是完全相同的状态时 prob = 1.0 available = list(range(num_states)) # 仍可供选择的状态 for i, state in enumerate(indices): state_probability = state_matrix[i, state] / sum(state_matrix[i, available]) # 可用状态的条件 prob * = state_probability available.remove(state) action_prob = policy_matrix[indices[actor_index], action] joint_action_prob = prob * action_prob p_joint_given_action = joint_action_prob # 累加枚举(indices)中的player_idx、state_idx: new_state[player_idx, state_idx] += p_joint_given_action new_state / = p_action return new_state  为了摆脱 ReBeL 的困扰，这里有一个更简单的问题版本。 你正在玩一个有 N 个玩家和 N 张牌的游戏。所有牌最初面朝下并随机分配（每个人持有 N 张牌中任意一张的概率为 1/N）。  裁判说“玩家 1 有 20% 的机会拿到 2”。根据该声明，您如何（有效地）更新每个人的信念？   由   提交 /u/Dhdjskk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbgpck/rebel_bayesian_updating/</guid>
      <pubDate>Tue, 23 Apr 2024 21:35:37 GMT</pubDate>
    </item>
    <item>
      <title>从什么框架开始学习强化学习？艾萨克·西姆？穆乔科RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbfraz/what_framework_to_start_learning_rl_with_isaacsim/</link>
      <description><![CDATA[嗨，我刚刚开始研究强化学习并尝试各种 cartpole 实现。 哪个框架被认为是在 pytorch 中训练强化学习算法的“最佳”以及其优点/缺点是什么IsaacSim、openAI Gym、基于 Mujoco 的框架？我什至不确定哪些是最突出的，我需要在这里进行一些冷启动。谢谢。   由   提交/u/Specialist_Ice_5715   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbfraz/what_framework_to_start_learning_rl_with_isaacsim/</guid>
      <pubDate>Tue, 23 Apr 2024 20:58:38 GMT</pubDate>
    </item>
    <item>
      <title>稳定的基线3 DQN、A2C 和 PPO 在 Pong 上成绩不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbch1y/stable_baselines3_dqn_a2c_and_ppo_getting_bad/</link>
      <description><![CDATA[     &lt; td&gt; 你好。  我更改超参数已经有一段时间了，但没有迹象表明可以得到更好的结果。 https://preview.redd.it/ffcwkk2mx9wc1.png?width=930&amp;format=png&amp;auto=webp&amp;s=af5d9e10bf70a9fe9 5ce5ae3b26730be96990e1a ~DQN~ vec_env: VecEnv = make_vec_env(&quot;ALE/Pong-ram-v5&quot; ;, n_envs=4) vec_env = VecFrameStack(vec_env, n_stack=4)policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[64, 32]) model = DQN(“MlpPolicy”, vec_env,learning_rate=0.0001 ，target_update_interval = 1000，train_freq = 4，buffer_size = 10000，learning_starts = 5000，batch_size = 32，exploration_fraction = 0.1，exploration_final_eps = 0.01，gamma = 0.99，gamma = 0.99，polition_kwargs = polition_kwargs = polition_kwargs = kwargs = qualtepe = kwargs = qualteps = retimest（reality） /pre&gt; ~A2C~ vec_env: VecEnv = make_vec_env(&quot;ALE/Pong-ram-v5&quot;, n_envs = 4）vec_env = VecFrameStack（vec_env，n_stack = 4）policy_kwargs = dict（activation_fn = torch.nn.ReLU，net_arch = [dict（pi = [32, 16]，vf = [32, 16]）]，ortho_init =真）模型= A2C（“MlpPolicy”，vec_env，learning_rate=1.4e-5，n_steps=512，gamma=0.983，gae_lambda=0.95，max_grad_norm = 0.36，ent_coef=0.01，policy_kwargs=policy_kwargs）model.learn（total_timesteps） =2000000)  ~PPO~ vec_env: VecEnv = make_vec_env(&quot;ALE /Pong-ram-v5”，n_envs = 4） vec_env = VecFrameStack（vec_env，n_stack = 4）policy_kwargs = dict（activation_fn = torch.nn.ReLU，net_arch = dict（pi = [32, 32]，vf = [32） ，32]））模型= PPO（“MlpPolicy”，vec_env，learning_rate=2.5e-4，n_steps=128，batch_size=256，n_epochs=4，gamma=0.99，gae_lambda=0.95，clip_range=0.3，ent_coef=0.1 ，vf_coef=0.5，policy_kwargs=policy_kwargs，device=“cpu”）model.learn（total_timesteps=2000000） 此外，通过这个确切的设置，我使用了不同的体系结构，例如{[64, 32, 16] [64, 32, 16]}，{[32, 16]，[32, 16]} 等等。我还尝试过使用游戏作为图像和 CnnPolicy。   由   提交/u/ufoludek3000   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbch1y/stable_baselines3_dqn_a2c_and_ppo_getting_bad/</guid>
      <pubDate>Tue, 23 Apr 2024 18:47:45 GMT</pubDate>
    </item>
    </channel>
</rss>