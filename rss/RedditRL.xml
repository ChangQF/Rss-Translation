<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Tue, 18 Feb 2025 12:32:28 GMT</lastBuildDate>
    <item>
      <title>RL代理：DQN和Doubel DQN在Lunarlander环境中不融合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is93td/rl_agent_dqn_and_doubel_dqn_not_converging_in_the/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我一直在开发各种RL代理，并将它们应用于不同的OpenAI健身环境。到目前为止，我已经实施了DQN，Double-DQN和一位香草策略梯度代理，在Cartpole和Lunar Lander环境中测试它们。  DQN和Double-DQN模型成功地求解了Cartpole（达到200和200 500步），但在Lunar Lander表现不佳。相比之下，政策梯度代理可以解决Cartpole（200和500步）和Lunar Lander。我怀疑我的实施可能存在问题，因为我知道其他人已经能够解决它，只是无法弄清楚原因。我尝试了许多不同的参数（网络结构，软更新等，在某些情节之后，在一集中的每个步骤之后进行培训，..）如果有人对可能发生的事情有洞察力或建议，我将感谢您的建议！我已经在下面的链接中附上了DQN的木星笔记本和double-dqn。 .google.com/drive/folders/1xOeZpYVwbN5ZQn-U-ibBqzJuJbd-DIXc?usp=sharing&quot;&gt;https://drive.google.com/drive/folders/1xOeZpYVwbN5ZQn-U-ibBqzJuJbd-DIXc?usp=sharing   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/usiss_drop_7402       [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1is93td/rl_agent_dqn_dqn_and_and_doubel_dqn_not_not_not_not_not_not_not_not_not_converging_in_in_in_in_in_the/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is93td/rl_agent_dqn_and_doubel_dqn_not_converging_in_the/</guid>
      <pubDate>Tue, 18 Feb 2025 09:46:20 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些指导来解决这个问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is8jc1/i_need_some_guidance_resolving_this_problem/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好， 我对增强学习领域相对较新，我已经完成了一些课程并阅读了一些有关它还做了一些工作（小型项目）。 我目前正在研究我的问题，我想知道我需要使用强化学习来解决这个问题，我需要采用哪种算法/方法。 我有一个建筑游戏，其目标是在最大允许的建筑地形数量上建造最大数量的房屋数量。每个可能的建筑地形都可以拥有或没有地雷（这会破坏您的房屋并使您失去游戏）。拥有该地雷的可能性仅基于您的房屋的分布。例如，某个分布可能会导致相同的建筑物具有地雷，但另一个分布可能会导致该建筑物没有它。 对于培训，代理商可以收到对每个房屋的反馈（是否在地雷上天气）。&lt; /p&gt; 通常，该建筑游戏有很多建筑规则，例如房屋之间的间距等...但是我希望我的经纪人隐含地学习这些建筑规则并能够应用它们。在我的培训结束时，我希望能够拥有一个代理商图解了最好，最启程的建筑策略（房屋数量的最大数量），这概括了他从不同环境中汲取的模式，这些模式将在太空中变化，但会有相同的规则，这意味着从培训中学到的模式可以适用在任何其他环境中。你们是否有一个想法，可以使用哪种奖励策略来解决此问题，算法等...？随时要求我澄清。&lt; /p&gt; 谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/iminderentpainter86     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is8jc1/i_need_some_guidance_resolving_this_problem/</guid>
      <pubDate>Tue, 18 Feb 2025 09:03:52 GMT</pubDate>
    </item>
    <item>
      <title>必须阅读强化学习论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，大家好，所以我是CS毕业，并且在深度学习和计算机视觉方面有体面的知识。我现在想学习强化学习（特别是用于飞行机器人的自动导航）。因此，您能从您的经验中告诉我，哪些论文是一本强制性的阅读，可以开始并在强化学习方面保持体面。预先感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/dronesanddynamite     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</guid>
      <pubDate>Tue, 18 Feb 2025 07:26:22 GMT</pubDate>
    </item>
    <item>
      <title>有人熟悉RESQ/RESZ（价值分解MARL）吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irzt84/anyone_familiar_with_resqresz_value_factorization/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/losthero_12     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irzt84/anyone_familiar_with_resqresz_value_factorization/</guid>
      <pubDate>Tue, 18 Feb 2025 00:39:39 GMT</pubDate>
    </item>
    <item>
      <title>高参数调整库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irxqij/hyperparameter_tuning_libraries/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在从事一个使用深钢筋学习的项目，并需要为我的网络找到最佳的超参数。我有一种具有TensorFlow构建的算法，但我还使用稳定基线的PPO。有谁知道有任何与TF和SB一起使用的库，如果是，您可以给我一个文档的链接吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/livid-ant3549     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irxqij/hyperparameter_tuning_libraries/</guid>
      <pubDate>Mon, 17 Feb 2025 23:04:31 GMT</pubDate>
    </item>
    <item>
      <title>RL项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irugdg/advice_on_rl_project/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在从事一个深入的RL项目，我想在其中将一个图像与另一个图像对齐，例如。一张笑脸的两张照片，一张照片可能与另一张照片相比向右移动。我正在编码这个项目，但是有问题，并且想在此方面获得一些帮助。  方法：    状态 s_t = [image1_reference，image2_query]   代理/策略：CNN输入状态并预测 [旋转，缩放，scaleing，translate_x，translate_y] 是图像转换参数。具体而言，它将输出平均向量和STD向量，该向量将在这些参数上参数化正态分布。从此分布中采样了一个动作。 环境：环境在空间上转换给定动作的查询图像，并产生 s_t+1 = [image1_reference，image2_query_transed] 。 奖励函数：目前基于两个图像的相似性（基于MSE损失）。 情节终止标准：情节终止于100个步骤超过100个步骤。我还终止了转换是否太大（将图像缩放到一无所有，或将其从屏幕上翻译），给出-100的奖励。  rl算法：我正在使用增强。我希望稍后再尝试PPO之类的算法，但现在想到，增强功能可以正常工作。  错误/问题：我的模型没有真正学习任何东西，每个情节都在早期终止有-100的奖励，因为查询图像被大量扭曲。关于可能发生的事情以及如何解决的想法？  问题：     我觉得我的奖励系统是&#39;对。当图像对齐时，应该在剧集结束时给予奖励，还是应该与每个步骤给出？     MSE应该是奖励，还是应该是基于整数的奖励（+/- 10）？    我希望我的代理人尽可能少的步骤对图像对齐，而不是预测剧集的终止标准，或者我应该将其作为一个终止标准惩罚？或两者兼而有之？   会喜欢的一些建议，我对RL很新，所以不确定最好的行动是什么！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/eChocomprehension925     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irugdg/advice_on_rl_project/</guid>
      <pubDate>Mon, 17 Feb 2025 20:50:16 GMT</pubDate>
    </item>
    <item>
      <title>关于政策梯度的快速问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irt72m/quick_question_about_policy_gradient/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我突然对一件事感到困惑。让我们以香草策略梯度算法： https：&gt; https：///en.wikipedia.org/wikipedia.org/wiki/policy/policy_gracy_gradient_metereient_meterod#reindunporwinforef  我们都知道那里的引理，它指出了毕业生的期望（log（pi））为0。假设我们有一个玩具示例，其中动作空间和状态空间很小，我们不需要进行随机策略更新。每次我们都有所有可能的情节/轨迹。因此，即使策略不是最佳的，梯度也将为0。这种情况如何进行学习？ 我了解渐变更新不会是0，以便在那里进行学习。  &lt;！ -  sc_on-&gt;＆＃32 ;提交由＆＃32; /u/u/nereuszz     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irt72m/quick_question_about_policy_gradient/</guid>
      <pubDate>Mon, 17 Feb 2025 20:00:02 GMT</pubDate>
    </item>
    <item>
      <title>RL项目需要一点帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iromau/need_a_little_help_with_rl_project/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨。有点远，但我是一名大学生，正在研究我的论文项目的加强学习可再生能源工程。我试图通过创建Q学习功能来建立项目的基础，该功能在高峰和非高峰关税时间内放电和充电电池以最大程度地降低成本，但是我努力让代理商达到目标成本。我已将代码附加到这篇文章上。有恒定的负载需求，没有光伏的生成，只有代理从电网购买能量以充电然后放电电池。我知道这是一个远景，但是如果有人能提供帮助，我将永远感激，因为我会发疯。我尝试了一切，包括不同的探索和剥削策略和自适应衰减。谢谢 。代码a&gt;   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/odd-odd-entrepreneur6453      [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iromau/need_a_little_help_with_rl_project/</guid>
      <pubDate>Mon, 17 Feb 2025 17:00:50 GMT</pubDate>
    </item>
    <item>
      <title>微调从政策方法到政策的政策是有意义的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irhln4/does_it_make_sense_to_finetune_a_policy_from_an/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我的问题是，对于我的设置，一个步骤需要一段时间，因此我想减少训练过程中所需步骤的数量。首先训练单盘方法，然后将其转移到改善发现的基线的政策方法上是否有意义？加载策略网络是否足够（例如，从SAC到PPO）。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aure20     [link]   ＆＃32;   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irhln4/does_it_make_sense_to_finetune_a_policy_from_an/</guid>
      <pubDate>Mon, 17 Feb 2025 11:16:24 GMT</pubDate>
    </item>
    <item>
      <title>rl属于机器人技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irgnmc/rl_spplied_to_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是一名机器人软件工程师，具有多年的运动计划经验以及在自动驾驶汽车的轨迹跟踪方面具有一些控制经验。我希望更深入地研究RL，并且通常适用于机器人技术，尤其是在计划和障碍/碰撞等领域。我对ML和DL的早期工作经验适用于视觉以及对流行RL算法的一些知识。任何建议，资源/课程/书籍或项目创意都将不胜感激！  ps：我并不是真正希望学习适用于机器人技术的ML。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/karthi_wolf     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irgnmc/rl_spplied_to_robotics/</guid>
      <pubDate>Mon, 17 Feb 2025 10:10:29 GMT</pubDate>
    </item>
    <item>
      <title>在研究项目中需要帮助学习强化学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irg4n7/need_help_in_learning_reinforcement_learning_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我有数学背景，目前正在从事供应链风险管理。在审查文献时，我发现了在加强学习（RL）供应链管理中应用的研究差距。我还发现了一个可能有用的数值数据集。 我试图说服我的主管我们可以使用此数据集在供应链管理中演示我们的RL框架。但是，我对RL是否需要实施数据感到困惑。我在这里听起来可能没有经验 - 我是我，我是 - 这就是为什么我寻求帮助。 我的想法是通过模拟供应链环境，然后将数据集使用到供应链环境中，然后将数据集使用到验证或证明我们的结果。但是，我不确定哪种RL算法最合适。 有人可以指导我从哪里开始学习以及如何将RL应用于此问题吗？从我的理解来看，RL与传统的机器学习算法有所不同，并且不需要预先存在的数据进行培训。 道歉，如果有任何意义，请事先感谢您的帮助！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/midder-coat-388     link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irg4n7/need_help_in_learning_reinforcement_learning_for/</guid>
      <pubDate>Mon, 17 Feb 2025 09:32:23 GMT</pubDate>
    </item>
    <item>
      <title>通过平行GPU培训，用于增强学习的最佳物理引擎？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ireyww/best_physics_engine_for_reinforcement_learning/</link>
      <description><![CDATA[在深度学习框架以及增强学习 我已经探索了几种物理引擎，包括Pybullet，Mujoco，Mujoco，Isaac Gym，Gazebo，Brax和Gymnasium。 我的主要问题是：&lt; /p&gt;  支持的碰撞类型（例如，使用mano的凹面网格碰撞） 如果您有经验这些引擎中的任何一个，我都会感谢您听到您的见解。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/uny_way2779     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ireyww/best_physics_engine_for_reinforcement_learning/</guid>
      <pubDate>Mon, 17 Feb 2025 08:08:26 GMT</pubDate>
    </item>
    <item>
      <title>OpenSource项目贡献</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ir1ij0/opensource_project_to_contribute/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，  RL中有任何开源项目，以便我可以成为它的参与者并定期贡献？&lt;&lt;&lt; /p&gt; 任何线索都高度赞赏。 谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/commistigansion 566     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ir1ij0/opensource_project_to_contribute/</guid>
      <pubDate>Sun, 16 Feb 2025 20:20:05 GMT</pubDate>
    </item>
    <item>
      <title>硕士论文主题帮助marl</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iqwb9w/masters_thesis_topic_help_marl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在完成主人的工作，我一直在试图找出论文的主题。我想在MARL上做论文，并且以前曾研究并实施了一些单一代理算法。 我一直在研究合作任务，尤其是在沟通和社会学习方面。问题是，我很难在这些领域找出一个可能适合师父级别的问题。我可以对此获得一些想法和建议吗？ 如果对一般研究有任何建议，这将不胜感激。 谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/naaffi     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iqwb9w/masters_thesis_topic_help_marl/</guid>
      <pubDate>Sun, 16 Feb 2025 16:44:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么RLHF中没有值函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iquzxv/why_is_there_no_value_function_in_rlhf/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在RLHF中，大多数论文似乎只关注奖励模型，而不是真正引入了传统RL中常见的价值函数。您认为这背后的理由是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/__ fivid__     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iquzxv/why_is_there_no_value_function_in_rlhf/</guid>
      <pubDate>Sun, 16 Feb 2025 15:47:04 GMT</pubDate>
    </item>
    </channel>
</rss>