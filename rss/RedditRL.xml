<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 15 Feb 2024 06:18:18 GMT</lastBuildDate>
    <item>
      <title>帮助自定义环境 Pettingzoo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</link>
      <description><![CDATA[我有一个自定义环境，我将代理设置为  self.agents = [&quot;EV_&quot; + str(r) for r in range(num_agents)]  当以以下形式测试环境时，我收到下一个错误： from EVenv import DepotEnv from pettingzoo.test import parallel_api_test from pettingzoo.test import api_test if __name__ == &quot;__main__&quot;: env = DepotEnv(num_agents=3) parallel_api_test(env, num_cycles=1000)  ，第 7 行，在  parallel_api_test(env, num_cycles=1000) 文件“C:\Users\” luisb\EVCHARGING\EVenv\lib\site-packages\pettingzoo\test\parallel_test.py”，第 122 行，parallel_api_test  assert ( AssertionError: [&#39;EV_0&#39;, &#39; EV_1&#39;, &#39;EV_2&#39;] != set()  我到处都找不到解决方案，我尝试设置代理，但它也不起作用，有什么想法吗？     提交者   /u/Barbajan22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</guid>
      <pubDate>Thu, 15 Feb 2024 01:11:18 GMT</pubDate>
    </item>
    <item>
      <title>自然语言强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqwxf0/natural_language_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2402.07157 OpenReview：https:// /openreview.net/forum?id=0VzU2H13qj 摘要：  强化学习（RL）在以下方面表现出了卓越的能力：学习决策任务的策略。然而，强化学习常常受到样本效率低、缺乏可解释性和监督信号稀疏等问题的阻碍。为了解决这些限制，我们从人类学习过程中汲取灵感，引入了自然语言强化学习 (NLRL)，它创新地将强化学习原理与自然语言表示相结合。具体来说，NLRL 重新定义了自然语言空间中的任务目标、策略、价值函数、贝尔曼方程和策略迭代等 RL 概念。我们介绍如何利用 GPT-4 等大型语言模型 (LLM) 的最新进展来实际实施 NLRL。对表格 MDP 的初步实验证明了 NLRL 框架的有效性、效率和可解释性。    由   提交 /u/FastestGPU   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqwxf0/natural_language_reinforcement_learning/</guid>
      <pubDate>Wed, 14 Feb 2024 20:28:52 GMT</pubDate>
    </item>
    <item>
      <title>为机器人技术贡献提出重要的强化学习建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqq0df/suggest_important_rl_for_robotics_contributions/</link>
      <description><![CDATA[我多年来一直在研究应用强化学习，并且很幸运能够获得博士学位。人形机器人强化学习候选者。很兴奋！ :) 你能提示我一些 RL + 机器人领域必读的文献吗？   由   提交 /u/seawee1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqq0df/suggest_important_rl_for_robotics_contributions/</guid>
      <pubDate>Wed, 14 Feb 2024 15:47:49 GMT</pubDate>
    </item>
    <item>
      <title>帮助确定为什么我的 PPO 的 JAX 实现比 PyTorch 实现慢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqeiyo/help_determining_why_my_jax_implementation_of_ppo/</link>
      <description><![CDATA[大家好！我正在学习 JAX，作为学习的一部分，我尝试重新创建一个简单的离散操作版本的 PPO（最初基于 cleanRL JAX PPO 和 cleanRL PyTorch PPO）。然而，我发现它比本质上非常相似的代码的 PyTorch 版本要慢得多。谁能告诉我我在 JAX 实现中可能做错了什么？我在这里故意避免使用 envpool，只是为了坚持更简单的 Gymnasium 设置。 这是我的 JAX 脚本（全部在一个文件中，并且可以在一个文件中运行，如果您有必要的话，只需复制并粘贴即可）包）：https://pastes.io/kronipluiy 这是等效的 PyTorch 脚本：https://pastes.io/u5oz948e27   由   提交 /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqeiyo/help_determining_why_my_jax_implementation_of_ppo/</guid>
      <pubDate>Wed, 14 Feb 2024 04:43:06 GMT</pubDate>
    </item>
    <item>
      <title>应用于 FrozeLakeV1（体育馆）的 QLearning 无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aq5dfj/qlearning_applied_to_frozelakev1_gymnasium_doesnt/</link>
      <description><![CDATA[      我的代码与 本教程。然而，与我的教程中的情况不同，代理根本不会学习。奖励始终为 0（因为代理永远无法达到目标。（下面是奖励（y 轴）和剧集（x 轴）的图表。我的代码可以找到 这里。 https://preview.redd.it/yeejhatv6fic1.png?width=567&amp;format=png&amp;auto=webp&amp;s=7021334596b72295082ca9bbde6303684ffe956a &lt;!-- SC_ON - -&gt;   提交者    /u/Miggus_amogus   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aq5dfj/qlearning_applied_to_frozelakev1_gymnasium_doesnt/</guid>
      <pubDate>Tue, 13 Feb 2024 21:39:42 GMT</pubDate>
    </item>
    <item>
      <title>有 Minigrid 的 RL 基准吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aq1i8o/any_rl_benchmarks_for_minigrid/</link>
      <description><![CDATA[嗨，有人知道在 minigrid env 中运行的代理的基准/排行榜/预训练权重吗？谢谢！   由   提交/u/pengzhenghao  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aq1i8o/any_rl_benchmarks_for_minigrid/</guid>
      <pubDate>Tue, 13 Feb 2024 19:03:01 GMT</pubDate>
    </item>
    <item>
      <title>quilterai 筹集 1000 万美元，构建 RL 支持的硬件编译器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</link>
      <description><![CDATA[      强化学习最令人兴奋的行业应用之一即将规模化！    由   提交 /u/mccrearyd   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</guid>
      <pubDate>Tue, 13 Feb 2024 18:03:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在整个阅读过程中应用萨顿和巴托的概念</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</link>
      <description><![CDATA[目前正在通过阅读这本书自学强化学习萨顿巴托：从头到尾介绍强化学习。我已经读了 4 章半了，感觉被它所强加的所有理论淹没了，有没有关于如何应用这些概念的随附材料或指南，以便我可以放慢一点速度，并真正内化这些概念我正在阅读的内容？理想情况下，这些将应用于编程环境。 如果有人有时间提供一些建议，我将非常感激！  &amp;# 32；由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</guid>
      <pubDate>Tue, 13 Feb 2024 18:02:05 GMT</pubDate>
    </item>
    <item>
      <title>开始 RL 是否需要 ML/DL 背景？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apgn0t/is_a_background_in_mldl_required_to_start_in_rl/</link>
      <description><![CDATA[我现在正在学习 ML，是为了深入研究 RL，我是在浪费时间吗？为了更深入地了解 RL，我可以学习什么作为先决条件？   由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apgn0t/is_a_background_in_mldl_required_to_start_in_rl/</guid>
      <pubDate>Tue, 13 Feb 2024 01:06:48 GMT</pubDate>
    </item>
    <item>
      <title>稳定的基线会减慢。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apf6lx/stable_baselines_slows_down/</link>
      <description><![CDATA[我正在制作一款物理摆游戏，但在游戏迭代 50 次左右后，稳定基线崩溃了。如果我继续给它 0 扭矩，电机会在 1000 秒的迭代中正常响应位置，因此我将其范围缩小到问题所在的稳定基线。   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apf6lx/stable_baselines_slows_down/</guid>
      <pubDate>Tue, 13 Feb 2024 00:00:18 GMT</pubDate>
    </item>
    <item>
      <title>机器人模型文件根本不存在</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apdfi4/robot_model_files_are_just_nonexistent/</link>
      <description><![CDATA[我从事机器人技术已有 7 年，从事强化学习已有 4 年。我是一名研究员，我遇到的最大问题之一是在线查找 URDF 和 MCJF。我的很多同行也面临着同样的问题。还有其他人有这个问题吗？如果是的话，我建立了一个在线免费存储库，任何人都可以上传文件，我们可以共享它们。我还计划添加一种在线模拟器，您可以在其中模拟这些机器人。加入我的discord服务器以获取更多信息，我将在那里发布测试版的链接：https://discord.gg/SJy2jV7n&lt; /p&gt;   由   提交/u/elonmusk-A12   /u/elonmusk-A12 reddit.com/r/reinforcementlearning/comments/1apdfi4/robot_model_files_are_just_nonexistent/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apdfi4/robot_model_files_are_just_nonexistent/</guid>
      <pubDate>Mon, 12 Feb 2024 22:44:49 GMT</pubDate>
    </item>
    <item>
      <title>选择本科论文项目有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apbxm6/any_tips_on_choosing_an_undergrad_thesis_project/</link>
      <description><![CDATA[ 由   提交 /u/BadMeditator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apbxm6/any_tips_on_choosing_an_undergrad_thesis_project/</guid>
      <pubDate>Mon, 12 Feb 2024 21:43:20 GMT</pubDate>
    </item>
    <item>
      <title>播客：法学硕士时代的强化学习（Kamyar Azizzadenesheli）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apa7qz/podcast_reinforcement_learning_in_the_age_of_llms/</link>
      <description><![CDATA[       由   提交/u/Smallpaul   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apa7qz/podcast_reinforcement_learning_in_the_age_of_llms/</guid>
      <pubDate>Mon, 12 Feb 2024 20:34:47 GMT</pubDate>
    </item>
    <item>
      <title>根据两个子奖励计算奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ap8t3m/reward_calculation_based_on_two_sub_rewards/</link>
      <description><![CDATA[大家好， 我想为连续动作设计一个奖励函数。我有 10 个动作，其中 9 个是相互依赖的，一个是独立的。所以，我的想法是计算前 9 个动作的一个奖励，并分别计算最后一个动作的其他奖励。然后我计算提供给代理的总奖励（PPO 政策）。奖励范围为-1...1。  tota_reward = (reward1 +reward2) / 2 这种方法有意义吗？ 我的 PPO 设置，我的剧集长度约为 165 步： &gt; PPO(“MlpPolicy”, self.env, n_steps=512, n_epochs=10, verbose=0, create_eval_env=False, batch_size =128、gae_lambda=0.95、ent_coef=0.001、vf_coef=0.5、gamma=0.99、learning_rate=0.0003、clip_range=0.2、use_sde=False、tensorboard_log=&#39;./tensorboard&#39;)   由   提交/u/Inevitable_Engineer5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ap8t3m/reward_calculation_based_on_two_sub_rewards/</guid>
      <pubDate>Mon, 12 Feb 2024 19:40:11 GMT</pubDate>
    </item>
    <item>
      <title>寻找适合演员-评论家模型的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ap7qhb/finding_resources_for_fitting_actorcritic_models/</link>
      <description><![CDATA[我正在将强化学习的计算模型与行为数据进行拟合，并希望获得一些帮助来寻找资源来帮助完成此过程。  行为任务非常简单（两个选项之间的选择和响应在概率上得到强化）。我在拟合各种 Q 学习模型方面取得了一些成功，但想探索演员批评家框架。一旦指定了基本框架，就可以通过最大化对数似然来将模型拟合到行为数据。我希望实现的一些示例可以在这里找到： https ://www.ncbi.nlm.nih.gov/pmc/articles/PMC9272137/ https://pubmed.ncbi.nlm.nih.gov/27986430/ 我正在寻找代码结构的示例，以便我可以确定我已经正确实现我的框架。然而，在搜索资源（教程、github 等）时，我发现的所有内容都与深度 RL 相关。有谁知道我如何改进我的搜索或知道任何有用的东西。 Python 是理想的选择，但任何东西都会有帮助。 非常感谢！ &lt; p&gt;​   由   提交/u/bigfuds  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ap7qhb/finding_resources_for_fitting_actorcritic_models/</guid>
      <pubDate>Mon, 12 Feb 2024 18:57:40 GMT</pubDate>
    </item>
    </channel>
</rss>