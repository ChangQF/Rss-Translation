<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 29 Nov 2023 09:14:13 GMT</lastBuildDate>
    <item>
      <title>关于“Q*”推测：法学硕士和合成数据搜索的一些相关研究背景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186fhih/on_q_speculation_some_relevant_research/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186fhih/on_q_speculation_some_relevant_research/</guid>
      <pubDate>Wed, 29 Nov 2023 03:20:02 GMT</pubDate>
    </item>
    <item>
      <title>“学习少量模仿作为文化传播”，Bhoopchand 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ejkw/learning_fewshot_imitation_as_cultural/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ejkw/learning_fewshot_imitation_as_cultural/</guid>
      <pubDate>Wed, 29 Nov 2023 02:37:31 GMT</pubDate>
    </item>
    <item>
      <title>DQN Agent 学得不太好</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/</link>
      <description><![CDATA[大家好！ 我目前正在学习强化学习。我写信询问是否有解决方案，因为我的代理在通过决斗 dqn 强化 Atari 突破时没有正确学习。就我而言，我成功地挖了一条隧道并举起了球，但球升起后，特工开始没有采取任何行动。我的猜测是，我了解到最好的做法就是在投球后什么也不做。但即使球落下来，这也是一个问题，因为他什么也没做。还有其他方法可以解决这个问题吗？ 如果您需要检查代码，请告诉我，我给您链接   由   提交/u/king-god_general  /u/king-god_general  reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/</guid>
      <pubDate>Tue, 28 Nov 2023 16:20:46 GMT</pubDate>
    </item>
    <item>
      <title>销售和预测销售环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185ykik/sales_and_forecasted_sales_environment/</link>
      <description><![CDATA[假设我有一组产品的历史销售额和预测销售额，并且我想使用代理（RL 或其他）来管理库存决策。关于如何使用这些历史数据来创建学习环境有什么想法吗？当然，我可以将分布拟合到历史销售和样本中，但是预测方法是基于对销售趋势的基本假设，而这样做会丢失这些假设。 理想情况下，我会有一个环境生成大量未来销售轨迹和相关预测，让代理通过数千次迭代学习基于预测的库存策略。但销售轨迹和预测应该以与生成基础预测模型相同的方式关联，否则该策略就毫无价值。  也许查看预测的历史误差并从未来的单一预测生成随机轨迹会更有意义？这里的问题是，根据给定的销售轨迹，预测可能会在 3 周内发生变化。 任何见解或类似的工作将不胜感激！ &lt;!-- SC_ON - -&gt;  由   提交 /u/aaronunderwater   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185ykik/sales_and_forecasted_sales_environment/</guid>
      <pubDate>Tue, 28 Nov 2023 15:25:12 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线 3 + JAX (SBX) 未在 GPU 上运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185tpdo/stable_baselines_3_jax_sbx_not_running_on_gpu/</link>
      <description><![CDATA[我已经使用“pip install sbx-rl”安装了 SBX 库（最新版本）用于我的 Stable Baselines 3 + JAX PPO 实施，以提高训练速度。我想使用 GPU (RTX 4090) 进行训练，但由于某种原因 SBX 始终默认使用 CPU。  SBX 无法识别我的 GPU，但是当我执行“nvidia-smi”时，SBX 无法识别我的 GPU。它被清楚地检测到+ pytorch 本身也确实检测到它。我怀疑 SBX 与我的 CUDA 版本（当前为 12.2）不兼容，但我找不到任何有关支持哪些 CUDA 版本的文档。  我花了几天时间试图解决这个问题，但没有成功。有人有想法吗？   由   提交/u/ClassicAppropriate78  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185tpdo/stable_baselines_3_jax_sbx_not_running_on_gpu/</guid>
      <pubDate>Tue, 28 Nov 2023 11:15:31 GMT</pubDate>
    </item>
    <item>
      <title>受版权游戏启发的研究环境（例如 Hanabi 挑战等）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185splc/research_environment_inspired_from_copyrighted/</link>
      <description><![CDATA[大家好！ 我想知道为游戏创建强化学习环境（仅用于研究）的法律方面仍然由创建它的相应公司拥有版权和销售权。 以 Hanabi 挑战为例 https:// arxiv.org/pdf/1902.00506.pdf，快速谷歌显示该游戏仍在销售。论文作者是否事先询问过游戏发行商是否允许他们创建环境？他们刚刚就这么做了吗？这里的合法性是什么？  感谢和干杯！   由   提交 /u/Arconer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185splc/research_environment_inspired_from_copyrighted/</guid>
      <pubDate>Tue, 28 Nov 2023 10:09:18 GMT</pubDate>
    </item>
    <item>
      <title>离策略演员批评家目标函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</link>
      <description><![CDATA[      我正在阅读 Silver 的 DPG 论文。在这里，如下所示，目标函数已使用行为策略 beta 进行了修改。我很好奇，如果使用梯度最大化下面的目标，目标策略的目标函数（通常的策略目标）是否会最大化？ ​ &lt; a href=&quot;https://preview.redd.it/1779aomlyz2c1.png?width=737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e&quot;&gt;https://preview.redd.it/1779aomlyz2c1.png?width =737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e   由   提交 /u/RealJuney   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</guid>
      <pubDate>Tue, 28 Nov 2023 02:12:27 GMT</pubDate>
    </item>
    <item>
      <title>寻找职业建议。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</link>
      <description><![CDATA[大家好，过去 3 年我一直对机器学习感兴趣，我的大部分注意力都集中在监督学习上，但是在过去 3 个月里 RL引起了我的注意，我相信人工智能的下一个重大事件将来自该领域。我有兴趣通过学术界，因为我只有计算机科学学士学位，并且不会找到工作，因为我在津巴布韦，而我们在技术方面还没有达到这个水平。我申请在美国攻读博士学位，但拒绝的次数越来越多，所以我很可能最终会去中国获得奖学金。我想要一些建议，因为最终我想在西方的大公司从事研发工作。如果可以的话，请告诉我在中国攻读硕士学位期间我可以做些什么，以便在 2026/27 年毕业后让我更接近这个目标。 PS：我也在中国获得了学士学位。   由   提交/u/congo43  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</guid>
      <pubDate>Mon, 27 Nov 2023 19:53:09 GMT</pubDate>
    </item>
    <item>
      <title>多头DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</link>
      <description><![CDATA[大家好，我正在应用 DQN 每次选择一组元素（一次一个或多个）。如何避免动作 [0, 0, 0,…]，即如何强制代理选择至少一个元素？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</guid>
      <pubDate>Mon, 27 Nov 2023 19:01:00 GMT</pubDate>
    </item>
    <item>
      <title>确定性参数总是输出相同的动作（PPO）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1859bj1/deterministic_parameter_always_output_the_same/</link>
      <description><![CDATA[      我使用 SB3 中的 PPO 和动作掩码来训练具有以下超参数的环境。在训练中，模型似乎学会了采取不同的行动。然而，在测试阶段，模型似乎做同样的事情并且表现正常，除了当我使用“确定性 = True”时，模型始终只选择一个操作。 我的代码： action，_states = model.predict（obs，action_masks=env.valid_action_mask()，确定性=True）  initial_learning_rate = 0.00005  model = MaskablePPO(MaskableActorCriticPolicy, env, tensorboard_log=&quot;./tensorboard&quot; ,n_steps=2048 ,learning_rate=initial_learning_rate) # 创建根据奖励调整学习率的回调 callback = RewardBasedLearningRateSchedule() for i in range (1,202): model.learn(total_timesteps=TIMESTEPS , tb_log_name = &#39;PPO2&#39; , reset_num_timesteps=False,callback=callback) &lt;代码&gt;model.save(f&quot;{models_dir}/{TIMESTEPS*i}&quot;) ​ tensororad的输出： https://preview.redd.it/cpdvwhxnkx2c1.png?width=1652&amp;format=png&amp;auto=webp&amp;s=a6d3bdef2f5040ce935a6534b66de2d934029af6   由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1859bj1/deterministic_parameter_always_output_the_same/</guid>
      <pubDate>Mon, 27 Nov 2023 18:07:01 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 3.0 对阵 Isaac Gym</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</link>
      <description><![CDATA[您好， 对于那些尝试过并且熟悉 Mujoco 3.0 和 Isaac Gym 的人，建议他们使用哪一个学习以及为什么？   由   提交 /u/anointedninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</guid>
      <pubDate>Mon, 27 Nov 2023 16:59:33 GMT</pubDate>
    </item>
    <item>
      <title>用于图像分类的 DQN（阿尔茨海默病）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1856abz/dqn_for_image_classification_alzheimer/</link>
      <description><![CDATA[您好，我正在使用 2D MRI 扫描进行研究。有4个班。我想创建一个可以执行分类任务的 DQN。有人能帮我吗？   由   提交/u/armaghanbz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1856abz/dqn_for_image_classification_alzheimer/</guid>
      <pubDate>Mon, 27 Nov 2023 16:01:44 GMT</pubDate>
    </item>
    <item>
      <title>“Open AI Gym”如何跟踪 CartPole 环境中超过 500 的步数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1853fmr/how_did_open_ai_gym_keep_track_of_steps_exceeding/</link>
      <description><![CDATA[我正在查看 这里，我没有看到 `step` 函数（或任何其他函数）如何确保代理不会跨越 500 步 - &lt; /p&gt; ``` def step(self, action): err_msg = f&quot;{action!r} ({type(action)}) 无效&quot; &gt; assert self.action_space.contains(action), err_msg assert self.state is not None, &quot;在使用step方法之前调用reset。&quot; x, x_dot, theta, theta_dot = self.state force = self.force_mag if action == 1 else -self.force_mag costheta = math.cos(theta) sintheta = math.sin(theta) ​ # 对于感兴趣的读者： # https://coneural.org/florian/papers/05_cart_pole.pdf temp = ( force + self.polemass_length * theta_dot**2 * sintheta ) / self.total_mass thetaacc = (self.gravity * sintheta - costheta * temp) / ( self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass) ) xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass ​ 如果 self.kinematics_integrator == &quot;euler&quot;: x = x + self.tau * x_dot x_dot = x_dot + self.tau * xacc theta = theta + self.tau * theta_dot theta_dot = theta_dot + self.tau * thetaacc&lt; /p&gt; else: # 半隐式欧拉 x_dot = x_dot + self.tau * xacc x = x + self.tau * x_dot &lt; p&gt;theta_dot = theta_dot + self.tau * thetaacc theta = theta + self.tau * theta_dot ​ self.state = ( x, x_dot, theta, theta_dot) ​ 终止 = bool( x &lt;; -self.x_threshold 或 x &gt; self.x_threshold 或 theta &lt; -self.theta_threshold_radians 或 theta &gt; self.theta_threshold_radians ) ​ 如果没有终止： 奖励 = 1.0  elif self.steps_beyond_termminate is None: # 杆子刚刚掉下来！ self.steps_beyond_termminate = 0 reward = 1.0 else:  if self.steps_beyond_terminate == 0: logger.warn( “您正在调用 &#39;step()&#39;，即使这样” “环境已返回终止 = True。您” “一旦收到“终止 =”，就应始终调用“reset()”  “正确”——任何进一步的步骤都是未定义的行为。” ) self.steps_beyond_termerated += 1 reward = 0.0 ​ 如果 self.render_mode == “人类”: self.render() return np.array(self .state, dtype=np.float32), 奖励, 终止, False, {} ```  但 Farama Gymnasium 的情况并非如此。 步骤函数具有以下代码来确保它 -  ``` 截断 = self.steps &gt;= self.max_episode_steps ​ ```  不幸的是，我应该运行“gym”环境。我目前面临的问题是，即使代理跨越 500 步，也不会停止。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1853fmr/how_did_open_ai_gym_keep_track_of_steps_exceeding/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1853fmr/how_did_open_ai_gym_keep_track_of_steps_exceeding/</guid>
      <pubDate>Mon, 27 Nov 2023 13:52:19 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线推出/ep_rew_mean</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1850jcz/stable_baselines_rolloutep_rew_mean/</link>
      <description><![CDATA[我正在尝试做一些需要我能够获取每 4 集打印到命令提示符中的 ep_rew_mean 值的事情。但我只是无法弄清楚这些值存储在哪里，以便我可以在回调函数中访问它们。我正在从“ep_info_buffer”访问最后一集奖励，但这与我需要的 ep_rew_mean 有很大不同。我为此查看了记录器、OffPolicyAlgorithm 和 SAC 文件，但仍然找不到它。这些信息在打印之前存储在哪里？我如何访问它？   由   提交 /u/aliaslight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1850jcz/stable_baselines_rolloutep_rew_mean/</guid>
      <pubDate>Mon, 27 Nov 2023 11:08:48 GMT</pubDate>
    </item>
    <item>
      <title>没有批评家模型的策略梯度算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/184wmg5/policy_gradient_algorithms_without_critic_model/</link>
      <description><![CDATA[是否可以在没有 actor-critic 的情况下实现像 NPG、PPO 这样的策略梯度算法，或者它会破坏这些算法中当前更快的收敛性吗？  如果我想这样做，我可以从哪里开始呢？    由   提交 /u/eles0range   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/184wmg5/policy_gradient_algorithms_without_critic_model/</guid>
      <pubDate>Mon, 27 Nov 2023 06:37:39 GMT</pubDate>
    </item>
    </channel>
</rss>