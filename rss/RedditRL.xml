<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 06 Sep 2024 18:21:58 GMT</lastBuildDate>
    <item>
      <title>强化学习奖励可以是当前状态和新状态的结合吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</link>
      <description><![CDATA[      我正在为我的 RL 代理构建奖励函数，并考虑采取行动后当前状态和新状态的组合。据我所知，基于 Sutton &amp;，这是可能的Barto，特别是公式 3.6，其中奖励是状态-动作对和结果状态的函数。我想确保我的方法符合 RL 理论。 有人可以确认这是否有效或分享见解吗？ https://preview.redd.it/7wndf4ny98nd1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=547a77aeecc75edcee9fbd41ec45dc95c772d3d1    提交人    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</guid>
      <pubDate>Fri, 06 Sep 2024 17:57:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习竞赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</link>
      <description><![CDATA[大家好，我通过强化学习开始训练，正在提升自己。我可以在网上参加和竞争这个领域的哪些比赛，我如何才能在这个领域有更好的发展？    提交人    /u/Weary-Ad-7225   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</guid>
      <pubDate>Fri, 06 Sep 2024 17:46:06 GMT</pubDate>
    </item>
    <item>
      <title>PPO 外汇交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</link>
      <description><![CDATA[      我正在使用 SB3 为外汇交易环境训练带有动作掩码的 PPO。该模型似乎学习了低点和高点枢轴点，但准确性非常可疑，而且它会反转动作，所以它在高点买入，在低点卖出！代码中哪里可能出错？  https://preview.redd.it/4dwdxlclj7nd1.png?width=986&amp;format=png&amp;auto=webp&amp;s=60ac1f2a3ce5cdd386d77d13c57c233a37be56a6    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</guid>
      <pubDate>Fri, 06 Sep 2024 15:31:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将客户数据集添加到 d4rl？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fahao9/how_to_add_customer_dataset_into_d4rl/</link>
      <description><![CDATA[谢谢。    提交人    /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fahao9/how_to_add_customer_dataset_into_d4rl/</guid>
      <pubDate>Fri, 06 Sep 2024 14:59:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 进行黑盒目标函数优化 - 需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</link>
      <description><![CDATA[大家好， 我是强化学习领域的新手，目前正在研究一个让我有点困惑的问题。我正在尝试使用强化学习来优化黑盒目标函数，但我不确定我是否走在正确的轨道上。我非常感谢这个 subreddit 中经验丰富的人提供的任何指导或见解！ 所以，事情是这样的：我有一个目标函数，它接受两个输入（x，y）并输出标量值 z。目标是找到这个函数的全局最大值。棘手的部分是我无法访问该函数的内部工作原理。我所能做的就是从一个随机位置 (x, y) 开始，然后采取小步骤来探索函数的格局。 目标函数可能看起来像这样： def objective_function(x, y): return -(x**2 - 10 * np.cos(2 * np.pi * x) + y**2 - 10 * np.cos(2 * np.pi * y) + 20)  这个想法是使用 Q 值估计和神经网络来学习最大化目标函数的最佳策略。 以下是我一直在尝试的粗略概述：  使用 PyTorch 设置 DQN 模型，该模型以当前状态 (x, y) 作为输入并输出每个动作的 Q 值。 创建重放缓冲区来存储转换（状态、动作、奖励、 在训练期间，使用 epsilon-greedy 探索根据当前状态选择一个动作。 采取行动并观察下一个状态和奖励。 将转换存储在重放缓冲区中。 如果重放缓冲区有足够的样本，则对一批转换进行采样并执行梯度下降以更新 DQN 模型。  训练后，从随机状态开始并根据学习到的 Q 值贪婪地选择动作，找到最佳解决方案。  现在，我有点不确定，需要一些帮助：  由于这个问题没有明确的终点或终端状态，所以我不确定如何调整 Q 值估计。我应该将 Q 值估计为固定步数的累积奖励，而不是直到达到终止状态？我可能离题太远了，所以如果我误解了什么，请纠正我！ 如果没有终止状态，由于起点随机且步数有限，Q 值每次都会不同。这是预料之中的，还是我以错误的方式处理这个问题？  我将非常感谢这个 subreddit 中出色的人提供的任何指导、建议或替代方法。如果您有类似的优化问题经验，或者对如何改进此场景的 DQN 方法有想法，请分享您的智慧！ 非常感谢您的帮助。我真的很高兴向大家学习，并希望在这个具有挑战性的问题上取得一些进展！    提交人    /u/Technical-Vehicle888   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</guid>
      <pubDate>Fri, 06 Sep 2024 12:50:41 GMT</pubDate>
    </item>
    <item>
      <title>元强化学习框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</link>
      <description><![CDATA[大家好， 我目前正在探索元 RL 的主题，但在寻找合适的框架/库时遇到了问题。  虽然我已经找到了一些关于元学习主题的一般内容（例如 learn2learn、更高版本），但 RL 方面往往是次要关注点。  您是否使用过某些框架？可以推荐一些吗？    提交人    /u/RandomAgentIml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</guid>
      <pubDate>Fri, 06 Sep 2024 11:32:18 GMT</pubDate>
    </item>
    <item>
      <title>大家好</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fabk03/hey_everyone/</link>
      <description><![CDATA[是否有一些有趣的模拟器允许您创建一个可自定义的环境，其中代理可以学习控制四轴飞行器并跟随物体，例如骑自行车的人或汽车？ 我正在尝试训练代理在各种环境中跟随给定物体并避开障碍物。  我做了一些研究并找到了一些模拟器，但它们要么不允许物体跟踪，要么没有正确记录或没有正确的 Python API。    提交人    /u/Skirlaxx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fabk03/hey_everyone/</guid>
      <pubDate>Fri, 06 Sep 2024 10:09:45 GMT</pubDate>
    </item>
    <item>
      <title>同时进行 RL 和 DL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa5lao/rl_and_dl_at_the_same_time/</link>
      <description><![CDATA[我可以同时学习强化学习和深度学习吗？    提交人    /u/exlp_   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa5lao/rl_and_dl_at_the_same_time/</guid>
      <pubDate>Fri, 06 Sep 2024 03:30:20 GMT</pubDate>
    </item>
    <item>
      <title>“深度贝叶斯老虎机对决：汤普森抽样的贝叶斯深度网络实证比较”，Riquelme 等人 2018 年 {G}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa2qfb/deep_bayesian_bandits_showdown_an_empirical/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa2qfb/deep_bayesian_bandits_showdown_an_empirical/</guid>
      <pubDate>Fri, 06 Sep 2024 01:03:03 GMT</pubDate>
    </item>
    <item>
      <title>“探索的长期价值：测量、发现和算法”，Su 等人 2023 {G}（推荐者）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fa28ph/longterm_value_of_exploration_measurements/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fa28ph/longterm_value_of_exploration_measurements/</guid>
      <pubDate>Fri, 06 Sep 2024 00:38:27 GMT</pubDate>
    </item>
    <item>
      <title>主要的 RL 实验室都有盈利吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9ykt6/are_any_of_the_major_rl_labs_profitable/</link>
      <description><![CDATA[      我很好奇为什么 DeepMind 放弃了他们的历史项目，并停止了 AlphaZero 作为国际象棋引擎的开发。但后来我看了他们的利润，一切都说得通了。 即使是最近几年，主要客户也是他们的所有者，因此这主要是创造性会计，他们可以随心所欲地支付内部项目的费用。那么，RL 对任何更大规模的人来说都是有利可图的吗？或者这只是纯粹的研究资金浪费？ 我相信你可以让 DeepMind 真正盈利，你只需要一个新的领导层，与人民更紧密地联系在一起。 https://preview.redd.it/how5yh9a92nd1.png?width=774&amp;format=png&amp;auto=webp&amp;s=d93fb31fa812bff823ca899e3d0166697c39f4e9   由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9ykt6/are_any_of_the_major_rl_labs_profitable/</guid>
      <pubDate>Thu, 05 Sep 2024 21:51:50 GMT</pubDate>
    </item>
    <item>
      <title>在多任务/迁移学习中使用 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9pweb/using_rl_in_multitasktransfer_learning/</link>
      <description><![CDATA[我感兴趣的是看看神经网络如何高效地编码魔方，同时还能执行多项不同的任务。如果有人有多任务或迁移学习的经验，我想知道强化学习是否是一项适合纳入网络编码器部分训练中的好任务。    提交人    /u/thebrilliot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9pweb/using_rl_in_multitasktransfer_learning/</guid>
      <pubDate>Thu, 05 Sep 2024 15:56:19 GMT</pubDate>
    </item>
    <item>
      <title>创建代理来玩 Atomas 的指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9ml9e/guidance_in_creating_an_agent_to_play_atomas/</link>
      <description><![CDATA[我用 python 重新创建了一个我经常玩的游戏，叫做 atomas，主要目标是组合相似的原子并创建尽可能大的原子。它与 2048 非常相似，但不是在固定范围内生成新标题，而是中心原子范围每 40 步扩展一次。 原子可以放置在板中的任意两个原子之间，因此我决定用长度为 18 的列表来表示板（游戏结束前原子的最大数量）我用原子数量填充它，因为这是唯一重要的方面，其余的都留零。 我不确定这是否是表示板的最佳方式，但我无法想象更好的方式，中心原子随后进行编码，我将板中的原子数量以及移动次数包括在内。  我尝试过将值标准化为 0,1，将特殊原子编码为负值或仅高于可能的最大原子值。将所有内容标准化为 0,1 -1, 1。我尝试过 PPO，DQN 使用了掩码，因为动作空间是 19 0,17 是放置原子的索引，18 用于将中心原子转换为正值（有时由于特殊原子而可能）。 奖励函数变得非常复杂，但仍然没有提供良好的结果。由于大多数动作都不是特别好或特别坏，因此很难确定哪个是最佳动作。 到了我稍微编辑了奖励函数并将其转变为确定下一步动作的规则的地步，它的表现比任何算法都要好得多。我认为问题不在于训练时间，因为训练了 10k 集的算法表现与训练了 1M 集的算法相同或更差，并且它们都被硬编码规则所超越。  我知道有些问题并不是用 RL 来解决的，但我很确定 DRL 可能会培养出一个还不错的球员。  我愿意接受任何关于我如何改进以尝试获得可用代理的意见或指导。     提交人    /u/woimbouttamakeaname   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9ml9e/guidance_in_creating_an_agent_to_play_atomas/</guid>
      <pubDate>Thu, 05 Sep 2024 13:32:26 GMT</pubDate>
    </item>
    <item>
      <title>我不知道如何开始我的第一个 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</link>
      <description><![CDATA[我过去一直是一名 Unity 游戏开发者，我想尝试做一些强化学习 - 两个 AI 玩家之间的标签游戏。我一直在使用 anaconda 和 jupyter 笔记本进行我的第一个机器学习项目，到目前为止我很喜欢它。现在我想知道我该如何实现我的想法？什么环境？我应该使用 pygame 吗？有人有这方面的教程/课程吗？ 我不介意浏览无尽的库文档。    提交人    /u/JMB4200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9jl86/im_clueless_as_to_how_to_start_my_first_rl_project/</guid>
      <pubDate>Thu, 05 Sep 2024 11:00:04 GMT</pubDate>
    </item>
    <item>
      <title>“RTX 4060 足以使用 Isaac Sim 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</link>
      <description><![CDATA[      我正在使用 Isaac Gym 进行运动任务训练，但我想在安装了 RTX 4060 的家用电脑上进行训练。它足以完成这项任务吗？需求文档列出的最低要求是 GeForce RTX 3070，这比 4060 略好一些。如果有人有使用 4060 的经验，请告诉我它是否足够。 https://preview.redd.it/vx4oohnhsymd1.png?width=1120&amp;format=png&amp;auto=webp&amp;s=ad49a07ed80612f57ad6b871ccc6180f85ffe03c    由    /u/Vegetable_Pirate_263  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f9iqjr/is_the_rtx_4060_sufficient_for_using_isaac_sim/</guid>
      <pubDate>Thu, 05 Sep 2024 10:04:32 GMT</pubDate>
    </item>
    </channel>
</rss>