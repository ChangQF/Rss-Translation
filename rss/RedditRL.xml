<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 11 Jan 2025 18:20:51 GMT</lastBuildDate>
    <item>
      <title>Perplexity Pro 1 年仅需 25 美元（通常为 240 美元）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hyu0kf/perplexity_pro_1_year_for_only_25_usually_240/</link>
      <description><![CDATA[大家好， 我从我的英国移动提供商那里获得了更多 Perplexity Pro 促销代码，一整年只需 25 美元——正常价格为 240 美元，所以几乎打了 90% 的折扣！ 快来加入我们 Discord 中的 700 多名成员，并获取 促销代码。我接受 PayPal（为了买家保护）和加密货币（为了隐私）。 我还可以访问 ChatGPT Pro 以及 LinkedIn Career &amp; Business Premium、Spotify、NordVPN 和 IPTV 的优惠。 2025 年快乐！    提交人    /u/minemateinnovation   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hyu0kf/perplexity_pro_1_year_for_only_25_usually_240/</guid>
      <pubDate>Sat, 11 Jan 2025 11:30:37 GMT</pubDate>
    </item>
    <item>
      <title>你们如何尝试设置用于训练 DQN 网络的超参数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hye7x6/how_do_you_guys_experiment_with_setting_up/</link>
      <description><![CDATA[我在寒假期间从头实现了一个吃豆人游戏，并且正在努力制作一个表现相对良好的模型。他们似乎都在学习，因为他们一开始只是跌跌撞撞，但后来实际上吃了鹅卵石并逃离了鬼魂，但没有什么太高级的。  我尝试使用的所有超参数都在我的 github readme repo 的底部，这里： https://github.com/Blewbsam/pacman-reinforced ，指定的模型标签可以在 model.py 中找到。 我对深度学习还不熟悉，一直在关于调整超参数的不同策略的文献中迷失方向，最后感到困惑。你们建议我应该如何尝试找出哪些超参数和模型效果最好？    提交人    /u/AskUnfair764   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hye7x6/how_do_you_guys_experiment_with_setting_up/</guid>
      <pubDate>Fri, 10 Jan 2025 20:34:04 GMT</pubDate>
    </item>
    <item>
      <title>人形机器人竞赛 - 寻找首批参与者/测试者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy8f7m/humanoid_race_competition_looking_for_first/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy8f7m/humanoid_race_competition_looking_for_first/</guid>
      <pubDate>Fri, 10 Jan 2025 16:33:41 GMT</pubDate>
    </item>
    <item>
      <title>RL 宠物项目构想</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy6i6w/rl_pet_project_idea/</link>
      <description><![CDATA[大家好， 我是二进制分析/反编译的研究员。反编译是尝试找到编译为给定可执行文件的源代码程序的问题。 作为一个宠物项目，我有一个想法，尝试使用 RL 框架创建 https://eschulte.github.io/data/bed.pdf 的开源实现。在非常高的层次上，本文尝试使用距离度量来搜索精确编译为目标可执行文件的源代码程序。（这不是大多数反编译器的工作方式。） 我有几个问题：  这听起来像 RL 问题吗？ 有没有什么项目可以作为起点？感觉好像有人一定已经创建了一些环境来将修改/合成源代码作为操作，但我很难找到任何简单的 gym 环境来修改源代码。  任何其他提示/建议/指导都将不胜感激。谢谢。    提交人    /u/edmcman   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy6i6w/rl_pet_project_idea/</guid>
      <pubDate>Fri, 10 Jan 2025 15:10:34 GMT</pubDate>
    </item>
    <item>
      <title>我的 RL 学习方法（Isaac Lab 实践）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy4zcy/my_rl_learning_approach_handson_with_isaac_lab/</link>
      <description><![CDATA[大家好， 我只是想分享我学习 RL/机器人技术的历程。 我的 TL;DR 背景： 机械工程专业应届毕业生 一年前学会了 Python 编码 关于 NVIDIA 的 Omniverse Isaac Sim Replicator（AI/计算机视觉）的论文 开始攻读硕士学位，3 个月后退出硕士学位（不如预期） 大约 1 个月前，我开始对 RL/机器人技术产生巨大的动力。为了跟上所有 RL 术语和算法，我开始观看免费的 YT 教育视频。尽管那里有很多内容，但大多非常理论化，不适合初学者。作为一个喜欢通过动手方式学习更多知识的人，我很挣扎。  但是，由于我已经熟悉 NVIDIA 的 Isaac Sim，我开始探索 Isaac Lab，并立即着迷。我开始阅读教程和文档，并加入 Omniverse Discord Server 上的学习小组，学习 RL 感觉容易多了。至少对我来说，首先采取实用方法（构建机器人、场景等）并同时学习理论感觉更直观。 我并不是说 Isaac Lab 是学习 RL 的窍门，学习 API 肯定需要时间和精力，但实际上自己创建环境并观察机器人学习会让它变得非常有趣。我强烈建议您尝试一下！ 如果您想加入我，一起踏上 Isaac Lab RL 之旅，我开始在 YouTube 上创建 Isaac Lab 教程，以帮助每个人更轻松地完成任务（同时也可以跟踪我自己的进度）： https://www.youtube.com/playlist?list=PLQQ577DOyRN_hY6OAoxBh8K5mKsgyJi-r    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy4zcy/my_rl_learning_approach_handson_with_isaac_lab/</guid>
      <pubDate>Fri, 10 Jan 2025 13:59:12 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习的一些注意事项和建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy35ga/some_notes_and_suggestions_for_learning/</link>
      <description><![CDATA[我已经开始为我的主要项目进行强化学习，有人可以建议一个路线图或笔记来学习和研究它吗？    提交人    /u/momosspicy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy35ga/some_notes_and_suggestions_for_learning/</guid>
      <pubDate>Fri, 10 Jan 2025 12:17:48 GMT</pubDate>
    </item>
    <item>
      <title>预训练模型库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hy2ahk/pretrained_models_repository/</link>
      <description><![CDATA[大家好， 是否有一个公共的模型库，其中包含使用强化学习预先训练的模型，用于控制车辆（无人机、汽车等）？    提交人    /u/RamenKomplex   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hy2ahk/pretrained_models_repository/</guid>
      <pubDate>Fri, 10 Jan 2025 11:22:36 GMT</pubDate>
    </item>
    <item>
      <title>大多数 RL 工作都需要博士学位吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxwgz3/do_most_rl_jobs_need_a_phd/</link>
      <description><![CDATA[我是机器人学硕士生，我的论文是将强化学习应用于操控。我可能无法想出一些新的算法，但我擅长理解和应用。 我有兴趣将机器人学习作为职业，但似乎我看到的每份工作都需要博士学位。这是常态吗？我如何准备简历上的项目，以便仅凭硕士学位就能找到一份从事操控/人形机器人的工作？任何建议和意见都是有帮助的。 鉴于机器人技术就业市场的状况，我有点担心..    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxwgz3/do_most_rl_jobs_need_a_phd/</guid>
      <pubDate>Fri, 10 Jan 2025 04:35:46 GMT</pubDate>
    </item>
    <item>
      <title>isaac 健身房 vs isaac 模拟 vs isaac 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxr7gk/isaac_gym_vs_isaac_sim_vs_isaac_lab/</link>
      <description><![CDATA[大家好， 有人能帮我理解一下这里的一些基本分类法吗？isaac gym、isaac sim 和 isaac lab 有什么区别？ 谢谢，干杯！    提交人    /u/iawdib_da   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxr7gk/isaac_gym_vs_isaac_sim_vs_isaac_lab/</guid>
      <pubDate>Fri, 10 Jan 2025 00:01:08 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助解决涉及 2D 网格的扫雷 RL 训练问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxr48a/need_help_with_a_minesweeper_rl_training_issue/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxr48a/need_help_with_a_minesweeper_rl_training_issue/</guid>
      <pubDate>Thu, 09 Jan 2025 23:57:00 GMT</pubDate>
    </item>
    <item>
      <title>DQN 实施过程中的损失增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxl6o1/loss_increasing_for_dqn_implementation/</link>
      <description><![CDATA[      我正在使用 DQN 实现来最大限度地减少四轴飞行器控制器的损失。目标是让我的 RL 程序更改控制器的一些参数，然后接收从每个参数更改计算出的损失，算法的奖励是损失的负数。我运行了我的程序两次，随着时间的推移，损失都趋于增加（奖励减少），我不确定会发生什么。任何建议都将不胜感激，如果需要，我可以分享代码示例。  第一张图 以上是第一张图的结果。我再次训练了它，做了一些改变：增加批量大小、内存缓冲区大小、降低学习率、增加探索概率衰减，虽然奖励值更接近应有的值，但它们仍然像上面一样呈下降趋势。任何建议都将不胜感激。     提交人    /u/kwasi3114   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxl6o1/loss_increasing_for_dqn_implementation/</guid>
      <pubDate>Thu, 09 Jan 2025 19:38:19 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA ACE</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxju3b/nvidia_ace/</link>
      <description><![CDATA[有人有关于 NVIDIA ACE AI 的更多信息吗？我还没有深入研究这个话题（由于时间限制），但我的理解是，它将根据“NPC/AI 所犯的错误”调整 NPC 的决策。有人知道任何技术细节或相应论文的链接吗？    提交人    /u/Intelligent-Put1607   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxju3b/nvidia_ace/</guid>
      <pubDate>Thu, 09 Jan 2025 18:41:29 GMT</pubDate>
    </item>
    <item>
      <title>实现多智能体算法的参考资料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hxaqv8/reference_materials_for_implementing_multiagent/</link>
      <description><![CDATA[您好， 我目前正在研究多智能体系统。  最近，我一直在阅读 多智能体 PPO 论文并致力于其实现。  是否有任何简单的参考资料，例如 minimalRL，我可以参考？    提交人    /u/audi_etron   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hxaqv8/reference_materials_for_implementing_multiagent/</guid>
      <pubDate>Thu, 09 Jan 2025 11:26:45 GMT</pubDate>
    </item>
    <item>
      <title>选择硕士论文主题：拦截无人机的强化学习。好主意吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hx8j29/choosing_master_thesis_topic_reinforcement/</link>
      <description><![CDATA[对于我的航空航天工程硕士论文（为期 9 个月），我正在探索使用强化学习 (RL) 来训练能够动态响应威胁的拦截无人机的想法。转折点是引入对抗网络来模拟猎物无人机的行为。 我想研究一个既相关又有影响力的论文主题。鉴于廉价无人机目前带来的威胁，我发现反无人机措施特别有趣。但是，我对 RL 是否是拦截无人机的轨迹规划和控制输入的正确方法有些怀疑。 你觉得这个想法怎么样？它有潜力和相关性吗？如果您有任何其他建议，我很乐意听取！    提交人    /u/Marco_878a   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hx8j29/choosing_master_thesis_topic_reinforcement/</guid>
      <pubDate>Thu, 09 Jan 2025 08:41:22 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像扩散模型的密集奖励 + RLHF</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hx5vxl/dense_reward_rlhf_for_texttoimage_diffusion_models/</link>
      <description><![CDATA[分享我们的 ICML&#39;24 论文“基于偏好对齐文本到图像扩散的密集奖励视图”！（不，它还没有过时！） 在本文中，我们采用了密集奖励视角并开发了一种新颖的对齐目标，该目标打破了 DPO 式对齐损失中的时间对称性。我们的方法特别适合文本到图像扩散模型的生成层次（例如稳定扩散），通过强调扩散逆链/过程的初始步骤 --- 开始是艰难的！ 实验上，我们的密集奖励目标在将文本到图像扩散模型与人类/人工智能偏好相结合的有效性和效率方面显着优于经典的DPO损失（源自稀疏奖励）。  论文：https://arxiv.org/abs/2402.08265 海报：https://icml.cc/media/PosterPDFs/ICML%202024/32707.png?t=1717872664.0844204 代码： https://github.com/Shentao-YANG/Dense_Reward_T2I     提交人    /u/Leading-Contract7979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hx5vxl/dense_reward_rlhf_for_texttoimage_diffusion_models/</guid>
      <pubDate>Thu, 09 Jan 2025 05:34:55 GMT</pubDate>
    </item>
    </channel>
</rss>