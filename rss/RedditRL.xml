<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 15 Sep 2024 15:15:17 GMT</lastBuildDate>
    <item>
      <title>批判情景环境中的重要性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fh8mlq/critic_importance_in_episodic_environments/</link>
      <description><![CDATA[大家好，在这篇文章中：https://ai.stackexchange.com/questions/25739/what-are-the-advantages-of-rl-with-actor-critic-methods-over-actor-only-methods 有以下段落：  一个实际的好处是，批评者可以使用 TD 学习进行引导，使他们能够在线学习每一步……像 REINFORCE 这样的纯演员算法……需要情节问题。它们可以学习的最小单位是整个情节。这是因为，如果没有评论家提供价值估计，那么估计回报的唯一方法就是从一集的结尾抽样实际回报。  我想进一步了解这一点。如果任何状态已经显示出一些奖励，为什么我需要评论家价值解释？ 我认为另一种提问方式是 - 假设对于每个状态，我预测每个奖励的概率，我可以使用这个分布的平均值作为该状态的评论家值吗？    提交人    /u/Potential_Hippo1724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fh8mlq/critic_importance_in_episodic_environments/</guid>
      <pubDate>Sun, 15 Sep 2024 09:27:00 GMT</pubDate>
    </item>
    <item>
      <title>需求变化对 RL 训练稳定性的影响</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fgt1ld/impact_of_varying_demand_on_rl_training_stability/</link>
      <description><![CDATA[      我正在使用不同的需求文件（代表汽车到达流程/时间表）来训练我的 RL 算法。每个文件包含不同数量的车辆，范围在 800 到 1200 之间。在我的问题中，如果车辆未与客户匹配，则它们会在一定时间后离开系统。累积奖励基于已服务的车辆和未服务的车辆。因此，如果我们在需求文件中拥有更多车辆，那么如果我们对未服务的车辆保持谨慎，我们就有可能积累更多奖励。 实际上，我有一个更复杂的问题，但我试图尽可能地简化它。 在训练过程中，我注意到累积奖励在各个情节中存在显着波动（我在每集结束时记录累积奖励）。 我的问题是：需求文件中车辆数量的不同是否会导致学习过程不稳定？如果是的话，我应该如何处理才能稳定训练并提高学习效果？ https://preview.redd.it/0iv7qojvmtod1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ec5d15963c9eb2d4c8f306c5ff9a06babf7eb11d    submitted by    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fgt1ld/impact_of_varying_demand_on_rl_training_stability/</guid>
      <pubDate>Sat, 14 Sep 2024 18:52:09 GMT</pubDate>
    </item>
    <item>
      <title>当思路链链接了太多的想法时。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fgc0wm/when_the_chainofthought_chains_too_many_thoughts/</link>
      <description><![CDATA[        提交人    /u/moschles   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fgc0wm/when_the_chainofthought_chains_too_many_thoughts/</guid>
      <pubDate>Sat, 14 Sep 2024 02:25:04 GMT</pubDate>
    </item>
    <item>
      <title>努力设置 unity mlagents</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg8ywi/struggling_to_setup_unity_mlagents/</link>
      <description><![CDATA[我几乎得到了我需要的所有东西，尽管我遇到了一个无论我怎么尝试都无法解决的问题。mlagents-envs 需要 numpy==1.21.2，但我认为 pytorch 所需的 scipy 有要求：2.3。&gt;=1.22.4。我到底该如何解决这个问题？我不想使用 tensorflow 代替 pytorch，当我尝试使用 tensorflow 时我遇到了更糟糕的问题，并且只会在万不得已的情况下才会这样做    提交人    /u/JMB4200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg8ywi/struggling_to_setup_unity_mlagents/</guid>
      <pubDate>Fri, 13 Sep 2024 23:44:58 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    <item>
      <title>关于 o1 的每篇最新帖子</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffxvj2/every_recent_post_about_o1/</link>
      <description><![CDATA[        提交人    /u/quiteconfused1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffxvj2/every_recent_post_about_o1/</guid>
      <pubDate>Fri, 13 Sep 2024 15:41:56 GMT</pubDate>
    </item>
    <item>
      <title>目前 rl 的实际应用有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffsx8f/what_are_the_actual_applications_of_rl_being_used/</link>
      <description><![CDATA[我知道 RL 在很多机器人和游戏开发领域理论上都有应用，甚至在自动驾驶和 sim2real 机器人领域也有实际应用。但这就是我的知识范围。 我见过很多 RL 被用于解决大型系统中的小问题的案例，比如在数据分析中，所以我想了解这个领域在现实生活中的实际用途。 我认为大多数 RL 将用于整体行为训练，有点保留了 RL 的精神，但事实并非如此吗？    提交人    /u/Kae1506   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffsx8f/what_are_the_actual_applications_of_rl_being_used/</guid>
      <pubDate>Fri, 13 Sep 2024 11:59:08 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI 推出先思考后回答的 o1 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffpcfg/openai_introduces_o1_model_that_thinks_before/</link>
      <description><![CDATA[       由    /u/webbs3  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffpcfg/openai_introduces_o1_model_that_thinks_before/</guid>
      <pubDate>Fri, 13 Sep 2024 07:58:39 GMT</pubDate>
    </item>
    <item>
      <title>关于 CPI、TRPO 和 PPO 的一些问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffoutv/few_questions_surrounding_cpi_trpo_and_ppo/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffoutv/few_questions_surrounding_cpi_trpo_and_ppo/</guid>
      <pubDate>Fri, 13 Sep 2024 07:21:11 GMT</pubDate>
    </item>
    <item>
      <title>帮助我开始 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffnxxw/help_me_get_started_with_rl/</link>
      <description><![CDATA[大家好，我一直在学习 RL，但还没有实现。帮我开始写代码。我想从 MDP 开始。请分享一些笔记和教程，帮助我学习编写 RL 代码    提交人    /u/Sea-Application4821   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffnxxw/help_me_get_started_with_rl/</guid>
      <pubDate>Fri, 13 Sep 2024 06:15:17 GMT</pubDate>
    </item>
    <item>
      <title>RLC 录音</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ffatc0/rlc_recordings/</link>
      <description><![CDATA[由于我无法参加，因此我想观看 RLC &#39;24 演讲的录音。在 RLC 的常见问题解答中，它声明了以下内容：“所有演讲都将被录制并在获得作者许可后公开。您无需注册即可查看演讲录音。” [https://rl-conference.cc/help.html ] 有人知道这些录音在哪里可以找到吗？我搜索过但没有找到任何东西。 此外，我发现与 RL 相关的会议录音通常很难找到，有人知道一些可以观看它们的地方吗？ 非常感谢花时间回复的任何人！ 编辑：我从 RL discord 上的某人那里听说他们仍在制作视频。上传后我会在这里发布链接！    提交人    /u/two_armed_bandit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ffatc0/rlc_recordings/</guid>
      <pubDate>Thu, 12 Sep 2024 19:15:28 GMT</pubDate>
    </item>
    <item>
      <title>大家好，我是 RL 的一个应用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff9j09/an_application_of_rl_everyone/</link>
      <description><![CDATA[        提交人    /u/nimageran   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff9j09/an_application_of_rl_everyone/</guid>
      <pubDate>Thu, 12 Sep 2024 18:22:15 GMT</pubDate>
    </item>
    <item>
      <title>离线 RL 模型 OPE 的 Python 包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff6xqs/python_package_for_ope_of_offline_rl_models/</link>
      <description><![CDATA[嘿！我已经开发了一个 Python 包，用于对现实世界数据执行离线 RL 模型的离策略评估！https://github.com/joshuaspear/offline_rl_ope 它经过单元测试，并且具有张量维度的运行时类型检查，因此祝愿它易于使用！使用示例可在 repo 的示例文件夹中找到 - 非常欢迎反馈！ 干杯    提交人    /u/bean_the_great   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff6xqs/python_package_for_ope_of_offline_rl_models/</guid>
      <pubDate>Thu, 12 Sep 2024 16:35:08 GMT</pubDate>
    </item>
    <item>
      <title>最好的软件提交会议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff6dvr/best_conference_for_software_submission/</link>
      <description><![CDATA[大家好 - 有人推荐一个好的软件提交会议吗？JMLR 需要坚实的用户基础 - 我还没有！ 谢谢    提交人    /u/bean_the_great   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff6dvr/best_conference_for_software_submission/</guid>
      <pubDate>Thu, 12 Sep 2024 16:12:12 GMT</pubDate>
    </item>
    <item>
      <title>你好呀！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fewq1s/hi_there/</link>
      <description><![CDATA[我正在构建一个 AI 框架，该框架利用两个领域的强化学习来自动清理杂乱的数据以增强数据管理。 背景：基本上，来自科学研究人员的可能包含缺失值或异常的原始数据被提交到数据存储库，在那里对这些提交的数据执行手动数据管理。提交的数据多种多样（例如：地球数据、生物数据、化学数据……）。我计划专注于两个不同的领域，并创建一个框架，可以自动清理或自动检测提交数据中的错误。对于自动清理或自动检测错误/异常值/缺失值，我必须计划使用强化学习，它可以了解该领域并在数据提交者提交数据之前建议或突出显示数据错误。 我想知道：  这是一个广泛的话题吗？ 我在概念上是否走在正确的道路上。 对研究论文有什么建议。我发现关于这方面的论文很少。  我承认我是个菜鸟。任何建议都会有帮助！    提交人    /u/Dazzling_Rose_0708   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fewq1s/hi_there/</guid>
      <pubDate>Thu, 12 Sep 2024 07:22:37 GMT</pubDate>
    </item>
    </channel>
</rss>