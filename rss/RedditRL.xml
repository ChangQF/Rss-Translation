<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 30 Jul 2024 12:28:43 GMT</lastBuildDate>
    <item>
      <title>你在PPO算法中遇到过这个问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</link>
      <description><![CDATA[我正在创建一个用于任务路由的深度强化学习环境，使用基于策略的 DRL 方法 PPO。在我的环境中，想法是每次任务到达一个节点时，都会生成一个概率，最终找到目的地并结束游戏。因此，每次任务到达一个节点时，都会生成相邻节点的概率，形成到达终点的连续路径。但是，存在一个问题：为每个状态生成的概率正在收敛。理论上，它们应该不同。例如，在选择一个节点后，应该生成像 s1[0.2,0.5,0.3] 和 s2[0.4,0.1,0.5] 这样的概率，但目前，每个状态都有相同的概率，例如 s1[0.2,0.5,0.3] 和 s2[0.2,0.5,0.3]。 我怀疑我的奖励设置可能有问题。我给每一步都设置了与时间参数相关的负奖励，完成的奖励为10，遇到路由循环就得到-10的负奖励，难道我的设计有问题？这让我很纳闷。    submitted by    /u/VermicelliBrave1931   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</guid>
      <pubDate>Tue, 30 Jul 2024 07:11:58 GMT</pubDate>
    </item>
    <item>
      <title>“对受试者进行反馈的序贯实验分析”，Diaconis & Graham 1981</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</guid>
      <pubDate>Mon, 29 Jul 2024 20:30:26 GMT</pubDate>
    </item>
    <item>
      <title>平均情节奖励差异，为什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</link>
      <description><![CDATA[      嗨， 我有一个简单的环境，我使用 SB3 中的各种算法进行训练作为练习。这是 DDPG 和 SAC 的 tensorboard 情节平均奖励（针对完全相同的环境）。在学习完成并保存模型时，报告的奖励约为 6。但是，当我在保存的模型上使用 SB3 assesse_policy (with n_eval_episodes=10) 时，我看到以下内容： 对于 SAC：平均奖励：1.5123082560196053 +/- 0.0008870645563937467 对于 DDPG：平均奖励：0.6197831666923037 +/- 0.000696591922367452 我预计平均奖励约为 6。这种预期是错误的吗？ https://preview.redd.it/xxag55fswgfd1.png?width=765&amp;format=png&amp;auto=webp&amp;s=e6903abf29863ad230d84451e83b162e265eb101    submitted by    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</guid>
      <pubDate>Mon, 29 Jul 2024 14:32:31 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线-3 工人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</link>
      <description><![CDATA[我正在研究 stable-baselines3 库 的一些算法的实现。 具体来说，我看到一些算法（A2C、PPO）的文档提到该实现使用了多个工作器。  对于其他算法，它没有提及任何内容，所以我的问题是：  我可以假设其他算法只使用一个学习者吗？ 为了进一步解答我的疑问，在所有算法的摘要表中显示，每个算法都支持“多处理”。但是，如果我理解正确的话，那仅指使用矢量化环境，尽管我不会打赌它。 最后说明：我尝试通过查看A2C（其中提到使用工作者）和SAC（其中没有提到工作者）的代码来回答我的问题，但这没有帮助，所以我在这里   由    /u/Frank-the-hank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</guid>
      <pubDate>Mon, 29 Jul 2024 07:27:37 GMT</pubDate>
    </item>
    <item>
      <title>创建博弈论论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</link>
      <description><![CDATA[在我国，对于原创博弈，制定博弈论与策略优化实施大纲的传统方式是什么？    提交人    /u/Former_Ad_4221   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</guid>
      <pubDate>Mon, 29 Jul 2024 06:24:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PPO 算法没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</guid>
      <pubDate>Mon, 29 Jul 2024 05:00:03 GMT</pubDate>
    </item>
    <item>
      <title>用于构建 RL 项目的简单可视化工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</link>
      <description><![CDATA[      我计划制作这个用于 RL 开发的简单工具。这个想法是快速构建和训练 RL 代理，无需代码。这对于快速开始新项目或轻松进行实验以调试 RL 代理非常有用。 目前设计中有 3 个选项卡：环境、网络和代理。我计划添加第四个选项卡，称为“实验”，用户可以在其中定义超参数实验并直观地查看每个实验的结果，以便调整代理。这个设计是一个非常早期的原型，可能会随着时间的推移而改变。 你们觉得怎么样？ https://preview.redd.it/sb5awqjys8fd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d1046c3b7e195dba0b7779ee55f11c9330ec3d12    提交人    /u/Charming-Quiet-2617   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</guid>
      <pubDate>Sun, 28 Jul 2024 11:12:20 GMT</pubDate>
    </item>
    <item>
      <title>关于 Stable Baselines3 和 AgileRL 的其他问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</link>
      <description><![CDATA[我尝试使用 SB3 实现我的 RL 算法，但是我对该框架中使用的术语感到很困惑。 我所有的旧 RL 算法都有一个这样的结构： untill episode &lt; max_episode_numbers: for step in max_steps_number: s&#39;, r, d, info = env.step 因此，基本上，我定义了最大 episode 数和最大 step 数，然后让算法运行。 现在在 SB3 中（以 PPO 算法 为例），有许多不同的术语，例如： n_epochs (int) – 优化替代损失时的 epoch 数 total_timesteps (int) – 要训练的总样本数（env 步骤） 据我所知，n_epochs 并不等同于我的 max_episode_numbers上面的示例中，但它是调用代理损失的优化的内部属性。 但如何定义要执行的最大情节数？即使他们论坛上有趣的线程也没有太大帮助。 我个人想出了类似以下的东西，但我不确定它是否有意义（我的想法来自这里）： for episode &lt; max_number_episode： model.learn(total_timesteps = 10_000) model.save(path_model) 更糟糕的是，我试图将该框架与 AgileRL 框架进行比较，后者看起来非常有前途。 在 AgileRL 中，明确定义了最大剧集数量的参数： &quot;EPISODES&quot;: 1000, # 要训练的剧集数量EPISODES&quot;: 1000, # 要训练的剧集数量 甚至是一集中的 max_number 步数： &quot;MAX_STEPS&quot;: 500, # 代理在环境中采取的最大步数 但是对于外部循环，它不采用情节数，而是采用步数。 # 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []# 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []  现在我完全对术语、它们的含义和功能感到困惑    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</guid>
      <pubDate>Sun, 28 Jul 2024 10:24:11 GMT</pubDate>
    </item>
    <item>
      <title>使用多臂老虎机的推荐系统参考资料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</link>
      <description><![CDATA[我正在尝试学习如何将多臂老虎机应用到推荐系统中，但我不知道如何将原始的老虎机问题转换为有数据的场景。例如，这个库 https://github.com/fidelity/mab2rec 声称他们使用多臂老虎机，但他们没有解释如何使用。我试着阅读源代码，但我不太明白。 有人可以推荐一些资源来学习这种特定的应用程序吗？    提交人    /u/VanBloot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</guid>
      <pubDate>Sat, 27 Jul 2024 22:44:09 GMT</pubDate>
    </item>
    <item>
      <title>MADDPG 未学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edo44n/maddpg_not_learning/</link>
      <description><![CDATA[大家好，我最近在简单的对手环境中训练了一个 MADDPG。模型开始运行，但结果很糟糕，代理没有学到任何东西。我已经尝试调试了几个星期，但没有成功。 我知道提供的信息有限，很多事情都可能出错，但如果你感兴趣并帮助查看我的代码（在 Google Collab 上集成到一个页面中），我将不胜感激。提前致谢。 代码：https://colab.research.google.com/drive/1bRV803GR2vnjX0jy7bkTEy3A8VLYFPB6?usp=sharing    提交人    /u/TransportationOk2251   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edo44n/maddpg_not_learning/</guid>
      <pubDate>Sat, 27 Jul 2024 19:14:40 GMT</pubDate>
    </item>
    <item>
      <title>用于构建和测试 RL 算法的软件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ed62l3/software_used_for_building_and_testing_rl/</link>
      <description><![CDATA[在观看了 AI 击败各种游戏的视频后，我最近对强化学习 (RL) 着迷。我计划在接下来的几个月内完成一些深度学习项目后深入研究 RL。 我偶然看到了一段视频，其中 AI 在 Trackmania 中打破了多项世界纪录 ，我很好奇这些视频中用于设计汽车和赛道元素的软件。有人知道在 Trackmania 中构建这些环境和测试 RL 算法可能会使用什么工具或软件吗？ 提前感谢您的帮助！    提交人    /u/iam_raito   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ed62l3/software_used_for_building_and_testing_rl/</guid>
      <pubDate>Sat, 27 Jul 2024 02:46:56 GMT</pubDate>
    </item>
    <item>
      <title>如何管理巨大的行动空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ed0642/how_to_manage_huge_action_spaces/</link>
      <description><![CDATA[我对深度强化学习还很陌生。我正在尝试解决一个问题，其中代理学习在 NxN 网格中绘制矩形。这需要代理选择两个坐标点，每个坐标点都是 2 个数字的元组。动作空间多项式 N4。我目前使用 DQN 算法处理 N=4 的情况。在此算法中，神经网络输出动作的 N4 个 q 值。对于 20x20 网格，我需要一个具有 160,000 个输出的神经网络，这太荒谬了。我应该如何处理这种动作空间巨大的问题？参考论文也将不胜感激。    提交人    /u/medwatt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ed0642/how_to_manage_huge_action_spaces/</guid>
      <pubDate>Fri, 26 Jul 2024 21:58:57 GMT</pubDate>
    </item>
    <item>
      <title>利用机器的传感器来定义观察空间是否（总是）有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecw0i3/does_make_always_sense_to_take_the_sensors_of_a/</link>
      <description><![CDATA[在许多机械臂示例和框架中，观察几乎是一本包含以下项目的字典： obs = {&#39;observation&#39;: [x, y, z], &#39;achieved_goal&#39;: [x, y, z], &#39;desired_goal&#39;: [x, y, z]&gt; 所以基本上末端执行器和目标的空间坐标被传递给网络。 但我的观点是：不太可能观察末端执行器在空间中的 3D 位置。是的，您可以使用相机并使用一些三角测量方法来检索末端执行器的位置。 但获取传感器读取的关节值不是更好吗？更有可能有一个编码器可以给我关节的旋转。这会更加现实，也更精确。 我为什么要问这个问题？因为理论上它应该可以简化很多问题，因为我观察到的空间会更小（一个能给我 +/- 120° 之间角度的编码器肯定比空间中的 3D 坐标更好）。    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecw0i3/does_make_always_sense_to_take_the_sensors_of_a/</guid>
      <pubDate>Fri, 26 Jul 2024 18:59:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么在使用动作之前要将其与 action_scale 相乘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</link>
      <description><![CDATA[我发现在很多 RL 例子中，动作都乘以了某些动作比例值。 这是为了从策略中调整动作的“影响力”吗？    提交人    /u/Open-Safety-1585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</guid>
      <pubDate>Fri, 26 Jul 2024 12:04:12 GMT</pubDate>
    </item>
    <item>
      <title>参加空气曲棍球挑战！构建并训练可以玩空气曲棍球的代理。击败您的竞争对手，赢取 3000 美元，并有机会在真正的机器人设置上试用您的代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</link>
      <description><![CDATA[        提交人    /u/elizabeth_duhh04   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</guid>
      <pubDate>Fri, 26 Jul 2024 11:26:31 GMT</pubDate>
    </item>
    </channel>
</rss>