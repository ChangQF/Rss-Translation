<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 08 Jul 2024 21:15:13 GMT</lastBuildDate>
    <item>
      <title>神经网络调试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyjkv6/neural_network_debugging/</link>
      <description><![CDATA[大家好， 我知道神经网络调试的基础知识。但我想知道是否有人可以分享在训练、测试和生产阶段进行调试的技巧。我相信这在这里会非常有帮助。    提交人    /u/MuscleML   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyjkv6/neural_network_debugging/</guid>
      <pubDate>Mon, 08 Jul 2024 20:43:56 GMT</pubDate>
    </item>
    <item>
      <title>Rnd 与 rnn 网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy8g0z/rnd_with_rnn_networks/</link>
      <description><![CDATA[我有带有 rnn 网络的 Ppo 用于策略和参与者。我想添加像 rnd 这样的好奇心机制，我想知道目标和预测网络是否也应该包括 rnn……有人有这方面的经验吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy8g0z/rnd_with_rnn_networks/</guid>
      <pubDate>Mon, 08 Jul 2024 13:05:17 GMT</pubDate>
    </item>
    <item>
      <title>我到底该怎么做？当操作无法影响即将到来的状态时出现问题...</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy6y4d/what_exactly_should_i_do_problem_when_actions/</link>
      <description><![CDATA[您好， 考虑一个非常简单的玩具问题。您有一辆汽车，它沿 x 轴移动。汽车以一定的起始速度在 1 维中移动。您的汽车有 2 个刹车，一旦打开刹车，就无法收回。这意味着：如果您打开第一个刹车，就无法再次关闭它，您已经失去了机会。如果您在错误的时间打开第一个刹车而犯了一个错误，那么您必须小心，在最佳时间打开第二个刹车。 环境：  状态：&lt;位置、速度、刹车状态 (0：无、1：刹车\_1 打开、2：刹车\_2 打开)&gt; 操作：0 或 1 (当前刹车状态 += 操作 --&gt; 根据当前状态添加操作) 奖励 = - (最后位置 - 目标位置) ^ 2 起始速度 = 10，起始位置 = 0，目标位置 = 55 如果打开，第一个刹车接合：速度 -= 1 如果打开，第二个刹车接合：速度-= 2 情节结束 -&gt; （如果位置 &gt; 65 或速度 &lt; 0）  -&gt; 解决方案是：在第 3 步打开第 1 个制动器，在第 4 步打开第 2 个制动器 10 + 10 + 10（在此处打​​开 Brake_1）+ 9（在此处打​​开 Brake_2）+ 7 + 5 + 3 + 1 = 55 我的问题： 打开 Brake_2 后，操作将不再产生任何效果。我的意思是：&lt;State, Action=0, Same\_Reward, Same\_Next\_State&gt;，&lt;State, Action=1, Same\_Reward, Same\_Next\_State&gt;。无论代理尝试什么操作，它都会进入相同的 next_state 并获得相同的奖励。基本上，代理已经失去了改变即将到来的状态的能力。 如果我添加一个由关闭中断组成的机制，代理将继续具有塑造即将到来的状态的能力，这意味着塑造汽车的最后位置直到情节结束。这已经奏效，代理能够找到正确的动作组合。动作基本上是可逆的。 如果我创建如上所述的环境，由于 Break_2 之后，保持对汽车速度的控制的能力将消失，代理开始努力解决问题。动作基本上是不可逆的。 总而言之，如果在某些事件之后动作对下一个状态和奖励实际上没有影响，我该怎么办？忽略中间的 &lt;状态、动作、奖励、下一个状态、完成&gt;，等到情节结束，并将最后一个状态和奖励与 Break_2 打开的上一个状态相结合？    提交人    /u/OpenToAdvices96   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy6y4d/what_exactly_should_i_do_problem_when_actions/</guid>
      <pubDate>Mon, 08 Jul 2024 11:51:32 GMT</pubDate>
    </item>
    <item>
      <title>创建街头霸王 II：世界战士 AI 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy5fjt/creating_a_street_fighter_ii_the_world_warrior_ai/</link>
      <description><![CDATA[是否可以在 Python 中在 GymRetro 或 StableRetro 中玩游戏？如果可以，我是否可以上传自己的游戏方式（按下按钮）以用于训练我自己的 AI 模型。非常感谢！    提交人    /u/More-Background-1626   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy5fjt/creating_a_street_fighter_ii_the_world_warrior_ai/</guid>
      <pubDate>Mon, 08 Jul 2024 10:21:46 GMT</pubDate>
    </item>
    <item>
      <title>不同种子的 SAC 性能存在差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy0t9m/variability_in_performance_of_sac_from_seed_to/</link>
      <description><![CDATA[您好！我目前正在尝试为离散动作空间实现 SAC，以便在 OpenAI LunarLander 环境中使用。然而，在训练过程中，我遇到了代理在一个种子上表现良好，但在另一个种子上（具有相同的超参数）表现较差的问题。我该如何解决这个问题？任何帮助都将不胜感激，因为我已经为此奋斗了几个月。 代码 Alpha/Actor Loss Alpha 值和情景奖励 Q 函数损失 每个彩色图表代表一次训练运行，它们之间的唯一区别是不同的种子。    提交人    /u/Tight_Apple_678   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy0t9m/variability_in_performance_of_sac_from_seed_to/</guid>
      <pubDate>Mon, 08 Jul 2024 05:13:36 GMT</pubDate>
    </item>
    <item>
      <title>纯探索中的顺序减半算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dxqs6v/sequential_halving_algorithm_in_pure_exploration/</link>
      <description><![CDATA[      在 Tor Lattimore 和 Csaba Szepsvari 的书的第 33 章中 https://tor-lattimore.com/downloads/book/book.pdf#page=412 他们展示了顺序减半算法，如下图所示。我的问题是为什么在第 6 行我们必须忘记来自其他迭代 $l$ 的所有样本？我尝试实现该算法，记住上次运行中采样的样本，并且效果很好，但我不明白算法中提到的忘记过去迭代中生成的所有样本的原因。 https://preview.redd.it/ufmxz837u5bd1.png?width=1275&amp;format=png&amp;auto=webp&amp;s=87a37f7eadb3fc9faf70d1423b5998289765cb34    由    /u/VanBloot  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dxqs6v/sequential_halving_algorithm_in_pure_exploration/</guid>
      <pubDate>Sun, 07 Jul 2024 20:59:16 GMT</pubDate>
    </item>
    <item>
      <title>将 LLM 环境转换为基于文本的环境；还有更多示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dxo8nd/converting_environments_to_textbased_for_llms_any/</link>
      <description><![CDATA[https://github.com/histmeisah/Large-Language-Models-play-StarCraftII 可能只需将我的环境转换为基于文本即可尝试类似操作。还有其他类似的例子吗？我依稀记得几年前的决策转换器，但从那以后就再也没有见过它们。    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dxo8nd/converting_environments_to_textbased_for_llms_any/</guid>
      <pubDate>Sun, 07 Jul 2024 19:10:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 MuZero 训练现代桌面游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dx15bv/using_muzero_to_train_modern_tabletop_games/</link>
      <description><![CDATA[嗨。我在一家棋盘游戏公司（捷克游戏版）工作，我对 AI 玩现代棋盘游戏非常感兴趣。我想看看 MuZero 是否是一个值得尝试的好选择。我读过很多文章和教程，探索过 MuZero 的几种实现，包括 C++ 变体和 OpenSpiel。我只找到了相对简单的游戏。我试图回答几个问题： 1) 我可以用现有的计算能力训练一款普通的现代棋盘游戏吗？即使使用普通 GPU 几天/几周，我也能得到结果吗？ 2) 如何从棋盘、几个令牌库以及几个玩家和全局单数指针创建观察空间？ 3) 如果我想将特定数字而不是随机数放入推理中以测试 AI 与现场玩家之间的对抗，那么它应该是一个新的虚拟玩家（“游戏管理员”）吗？ 4) 如果游戏板发生变化，但在一个游戏会话中保持不变，我可以将其“形状”添加到观察空间，以便 AI 能够玩其他类似的棋盘吗？ 到目前为止，对于我们的游戏，我们一直非常成功地使用简单的 MCTS，仅用于确定单个玩家移动中的动作（我们不模拟对手的动作）。我们没有使用神经网络的经验。我觉得我们可以创造一个更强大的 AI 对手，所以我正在学习和探索。    提交人    /u/damucz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dx15bv/using_muzero_to_train_modern_tabletop_games/</guid>
      <pubDate>Sat, 06 Jul 2024 22:16:03 GMT</pubDate>
    </item>
    <item>
      <title>新的（更具数学性的）强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</link>
      <description><![CDATA[嗨  我已经参加了强化学习课程，我认为我对这些概念有很好的掌握，但正在寻找强化学习领域算法开发的当前前沿 有什么算法 / 主题的想法可以让我开始吗？    提交人    /u/Total-Ad-4461   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 17:09:36 GMT</pubDate>
    </item>
    <item>
      <title>张量的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwpl60/the_problem_with_tensors/</link>
      <description><![CDATA[大家好，我在使用 DQN 时遇到了一个问题，即张量。当模型开始训练时，模型会收到以下维度的张量：torch.Size([1, 64, 64])，大约 10 秒后，模型会收到以下数据：torch.Size([32, 1, 64, 64])，因此我们收到此错误： Traceback (most recent call last): File &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;, line 141, in &lt;module&gt; agent.update_model(state, new_state, action, reward, compl) 文件 &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;，第 95 行，在 update_model 中 q_values = self.model(states) ^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl 中 return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;，第 40 行，在 forward x = self.linear_block(x) ^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在_wrapped_call_impl 返回 self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl 中返回 forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\container.py&quot;，第 217 行，在 forward 输入 = module(input) ^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl 中返回 self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl 中返回 forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\linear.py&quot;，第 116 行，在正向返回 F.linear(input, self.weight, self.bias) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: mat1 和 mat2 形状无法相乘 (32x1024 和 16x11) [56.0 秒内完成]  这是模型更新代码： def update_model(self, state, new_state, action, reward, compl): # 用 память self.memory.append([state, new_state, action, reward, compl]) # 完成页面后，如果 len(self.memory) &gt; 则返回结果self.max_memory_size: self.memory.pop(0) # 获取模型，然后根据条件判断 if len(self.memory) &gt;= self.batch_size: # 随机取值 batch = random.sample(self.memory, self.batch_size) # 随机取值 states = torch.stack([transition[0] for transition in batch]) new_states = torch.stack([transition[1] for transition in batch]) action = torch.tensor([transition[2] for transition in batch]) rewards = torch.tensor([transition[3] for transition in batch]) finishes = torch.tensor([transition[4] for transition in batch]) # 计算 Q 值 q_values = self.model(states) target_q_values = q_values.clone() max_next_q = torch.max(self.model(new_states), dim=1)[0] target_q_values[torch.arange(self.batch_size), action] = rewards + self.gamma * (1 - finishes) * max_next_q # 计算并返回 loss = self.criterion(q_values, target_q_values) self.optimizer.zero_grad() loss.backward() self.optimizer.step()     提交人    /u/Kepler-nn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwpl60/the_problem_with_tensors/</guid>
      <pubDate>Sat, 06 Jul 2024 13:27:10 GMT</pubDate>
    </item>
    <item>
      <title>如何开始学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwpicd/how_to_start_learning/</link>
      <description><![CDATA[大家好，我是新来的，我对使用强化学习创建自己的项目非常感兴趣。我想为自己做一些小项目，这可能会帮助我建立自己的投资组合。  关于我应该从哪里开始学习？什么/哪里是最好的源材料。    提交人    /u/Educational-Gene3665   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwpicd/how_to_start_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 13:23:22 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些帮助来完成我正在进行的项目，即微尺度的路径规划算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</guid>
      <pubDate>Sat, 06 Jul 2024 11:19:31 GMT</pubDate>
    </item>
    <item>
      <title>DQN 在自定义健身环境中无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</link>
      <description><![CDATA[大家好， 我一直想尝试自定义健身房环境。我使用 tensorflow 用 DQN 解决了​​冰冻湖问题 - 并且想创建一个类似的环境并用 DQN 解决它。 我的环境是一个 4x5 网格单元仓库，里面有一个机器人、一个目标和一个障碍物。所以，与冰冻湖非常相似。 我的问题是，一旦 epsilon 衰减，并且算法正在采取贪婪行动，它采取的行动就是错误的（总是相同的，取决于初始化，但例如，它会告诉机器人始终向上） 这是我尝试过的和我所知道的：  环境应该正常工作。我对其进行了广泛的测试，&amp;用经典 Q 学习解决了这个问题，而且效果很好 我几乎从我的冻湖代码中复制/粘贴了我的 DQN 代码。  我玩过超参数，增加了网络的复杂性，但没有任何效果，而且我对此持怀疑态度，因为我的自定义环境和冻湖之间应该没有太大区别（我想？）  知道我应该在哪里寻找吗？我可以在这里分享代码，但它有点密集，我不确定要分享哪一部分，所以我不想淹没，但如果你想要代码的任何特定部分，我很乐意分享     提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</guid>
      <pubDate>Sat, 06 Jul 2024 08:01:03 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 制作一款回合制策略游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</link>
      <description><![CDATA[        由    /u/Novel_Can_6870  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</guid>
      <pubDate>Sat, 06 Jul 2024 02:23:55 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理的行为正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</link>
      <description><![CDATA[      大家好 我刚刚创建了我的第一个 PPO 代理。在 Cartpole-v1 环境中进行训练时，我收到以下响应： https://preview.redd.it/8kufxuh59sad1.png?width=640&amp;format=png&amp;auto=webp&amp;s=ddf17252bea091b0738973645612f1095364489e 为什么在代理似乎最终收敛时，性能在 500 次迭代后仍会出现一些突然下降的情况？ 谢谢前进。 编辑：在实现 GAE 之后，经过几次迭代后，时间线在 cartpole 步骤上完全稳定（不幸的是，无法在编辑中分享图表）。    提交人    /u/Muscle_Robot   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</guid>
      <pubDate>Fri, 05 Jul 2024 23:13:27 GMT</pubDate>
    </item>
    </channel>
</rss>