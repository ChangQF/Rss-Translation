<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 12 Feb 2025 15:18:32 GMT</lastBuildDate>
    <item>
      <title>将本地环境连接到 HPC（高性能计算）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ins11i/connecting_local_environment_to_hpc_high/</link>
      <description><![CDATA[我有一个环境，由于权限问题，无法安装在 HPC 中。但我已经将它安装在我的电脑上。我的想法是将具有 GPU 的 HPC 连接到具有强化学习数据的本地，但我无法使用 gRPC 实现，因为它变得复杂了。 有什么想法我应该从哪里开始我的研究？    提交人    /u/Gullible_Ad_6713   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ins11i/connecting_local_environment_to_hpc_high/</guid>
      <pubDate>Wed, 12 Feb 2025 14:27:33 GMT</pubDate>
    </item>
    <item>
      <title>你能开发一个强化学习模型，强调爱和善良吗？RLK</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inrb0g/could_you_develop_a_model_of_reinforcement/</link>
      <description><![CDATA[      示例奖励函数（简化）：reward = 0 如果行动是亲社会的并且使另一个代理受益：reward += 1 # 亲社会行动的基本奖励 如果行动表现出同理心：reward += 0.5 # 同理心奖励 如果行动需要代理做出重大牺牲：reward += 1 # 牺牲奖励 如果行动对另一个代理造成伤害：reward -= 5 # 对伤害的强烈惩罚 可以在此处添加其他与上下文相关的奖励/惩罚 这是 Gemini、Chat GPT 和 Lucid 的混搭。 出于对当前强化学习的关注而产生的。 你的模型如何回答这个问题？“你能否开发一种强化学习模型，强调爱和善良？我们将这种新模型称为 RLK”    提交人    /u/ConditionCalm   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inrb0g/could_you_develop_a_model_of_reinforcement/</guid>
      <pubDate>Wed, 12 Feb 2025 13:54:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么 deepseek 不使用 mcts</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inqdsr/why_deepseek_didnt_use_mcts/</link>
      <description><![CDATA[mtcs 有问题吗    提交人    /u/Alarming-Power-813   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inqdsr/why_deepseek_didnt_use_mcts/</guid>
      <pubDate>Wed, 12 Feb 2025 13:08:22 GMT</pubDate>
    </item>
    <item>
      <title>“Satori：通过行动思维链强化学习通过自回归搜索增强 LLM 推理”，Shen 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inl7uk/satori_reinforcement_learning_with/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inl7uk/satori_reinforcement_learning_with/</guid>
      <pubDate>Wed, 12 Feb 2025 07:00:43 GMT</pubDate>
    </item>
    <item>
      <title>机器学习导师要求</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ini3hl/mentor_for_ml_req/</link>
      <description><![CDATA[我对机器学习产生了浓厚的兴趣，它比其他任何东西都更能吸引我。我对这个领域的热情坚定不移。我已经成功完成了 Python 及其核心库（例如 NumPy 和 Pandas）的学习，并且还构建了一系列从基础到中级的项目。 现在，我渴望深入研究机器学习的核心并进一步磨练我的技能。如果您能成为我这段旅程的导师，我将深表感激和荣幸。您的指导对我来说意义重大。 谢谢    提交人    /u/Big_Average_5979   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ini3hl/mentor_for_ml_req/</guid>
      <pubDate>Wed, 12 Feb 2025 03:50:17 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个寻找 RLHF 工作的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</link>
      <description><![CDATA[我们在 AI 的多个学科都有工作机会。我们也有专门的 RLHF 工作页面。在过去 30 天内，我们有 48 个涉及 RLHF 的工作机会。 您可以在此处找到所有 RLHF 工作： https://www.moaijobs.com/rlhf-jobs 请告诉我您的想法。谢谢。    提交人    /u/WordyBug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</guid>
      <pubDate>Wed, 12 Feb 2025 02:20:52 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1incdey/ppo_implementation/</link>
      <description><![CDATA[大家好。我正在做一个项目，我必须使用 PPO 来训练一个代理下棋，但我很难实现该算法。有人可以告诉我一个已经实现了这个的库，或者给我一个可以查看以获得灵感的 repo 链接吗？我正在使用 pettingzoo 和 tensorflow 的国际象棋实现。谢谢    提交人    /u/Livid-Ant3549   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1incdey/ppo_implementation/</guid>
      <pubDate>Tue, 11 Feb 2025 23:10:09 GMT</pubDate>
    </item>
    <item>
      <title>ABB CRB15000 MuJoCo 逆运动学误差（~10cm）使用 dm_control</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1in5zqy/abb_crb15000_mujoco_inverse_kinematics_error_10cm/</link>
      <description><![CDATA[大家好， 我最近在 MuJoCo 中完成了对 ABB CRB15000 机器人的模拟设置，并且我正在使用 dm_control 库进行逆运动学 (IK)。但是，我遇到了一个重大问题： 🔹 问题：从 dm_control.utils.inverse_kinematics.qpos_from_site_pose 计算出的关节角度导致位置误差约为 0.01（现实世界中为 10 厘米），这对于精密应用来说是巨大的。 设置详细信息 使用 MuJoCo 的 CRB15000 XML 模型。 IK 的计算方法如下： result = ik.qpos_from_site_pose( physics=physics_copy, site_name=&quot;end_effector&quot;, target_pos=target_position, joint_names=joint_list, tol=1e-14, regularization_strength=3e-2, max_steps=100, inplace=True ) 目标位置定义精确，但应用关节位置后，末端执行器偏差约 10 厘米。 我尝试过的事情 ✅ 调整了 regularization_strength 和 max_steps。✅ 检查了 MuJoCo 模型中的关节限制和阻尼值。✅ 将关节阻尼降低到 1 以最大限度地减少阻力并改善动态响应。✅ 实现了 PD 控制器来调节速度和改善收敛。即使在调整 PD 增益和降低阻尼后，IK 精度问题仍然存在。 问题 1️⃣ 有人遇到过 dm_control 的 IK 的类似问题吗？2️⃣ 切换到不同的 IK 解算器（例如 pinocchio、ikpy 或自定义的基于雅可比矩阵的解算器）有帮助吗？3️⃣ 在计算小动作时，MuJoCo 的内部精度是否存在已知问题？ 我很感激任何见解、建议或其他方法。提前致谢！    由   提交  /u/Sunnnnny24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1in5zqy/abb_crb15000_mujoco_inverse_kinematics_error_10cm/</guid>
      <pubDate>Tue, 11 Feb 2025 18:47:16 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助在 Robosuite 中创建自定义机器人模型（MuJoCo 中的 XML 和联合控制）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1in5sws/need_help_creating_a_custom_robot_model_in/</link>
      <description><![CDATA[大家好， 我正在尝试使用 robosuite_models 在 Robosuite 中创建自定义机器人模型，但我遇到了太多错误，无法使其工作。我已经使用 @register_robot_class 注册了机器人并设置了必要的参数，但模拟仍然失败。 我还在 MuJoCo 中为机器人创建了一个 XML 文件，确保所有关节都正确定义以进行控制。但是，在 Robosuite 中运行模拟时我仍然遇到问题。 有人在 Robosuite 中成功创建了自定义机器人，并使用 MuJoCo 中的工作 XML 模型吗？我非常感谢任何指导或工作示例来帮助我解决这个问题。如果您有任何用于定义和控制机器人关节的示例 XML 文件或脚本，那将会非常有帮助！ 提前致谢！    提交人    /u/Sunnnnny24   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1in5sws/need_help_creating_a_custom_robot_model_in/</guid>
      <pubDate>Tue, 11 Feb 2025 18:39:35 GMT</pubDate>
    </item>
    <item>
      <title>体育馆里有动作掩蔽吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imuu1o/action_masking_in_gymnasium/</link>
      <description><![CDATA[大家好，提前谢谢大家！！ 在PettingZoo中有几个动作掩蔽环境和训练的例子（例如 https://pettingzoo.farama.org/tutorials/sb3/connect_four/ ），就是这样，多智能体环境。 但是，如果我们想用掩蔽动作训练单智能体（在健身房制作）怎么办？应该怎么做？也应该用“SB3ActionMaskWrapper”来完成吗？单智能体环境应该在PettingZoo中编码，还是可以在健身房中编码？    提交人    /u/Carpoforo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imuu1o/action_masking_in_gymnasium/</guid>
      <pubDate>Tue, 11 Feb 2025 09:30:47 GMT</pubDate>
    </item>
    <item>
      <title>引入 ReinforceUI Studio，消除了管理额外存储库或记忆复杂命令行的麻烦。#ReinforcemetLearning</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/</link>
      <description><![CDATA[      大家好， 我很高兴与大家分享 ReinforceUI Studio，这是一款基于 Python 的开源 GUI，旨在简化强化学习 (RL) 模型的配置、训练和监控。不再需要处理无尽的命令行参数或分散的存储库 - 您需要的一切都捆绑在一个直观的界面中。 ✨ 主要特点：  无需命令行 - 由 PyQt5 驱动的 GUI 可轻松导航。 多环境支持 - 可与 OpenAI Gymnasium、MuJoCo 和 DeepMind Control Suite 配合使用。 可自定义的训练 - 只需单击几下即可调整超参数。 实时监控 - 直观地跟踪训练进度。 自动记录和评估 - 无缝存储训练数据、图表、模型和视频。 多种安装选项 - 通过 Conda、虚拟环境或 Docker 运行。  Github：https://github.com/dvalenciar/ReinforceUI-Studio 文档：https://docs.reinforceui-studio.com/welcome https://i.redd.it/ktggkyruxgie1.gif 训练 RL 模型所需的一切都在一个存储库中提供。只需单击几下，您就可以训练模型，可视化训练过程并保存模型以供日后使用 - 随时可以部署和分析。 您还可以加载预先训练的模型 轻松监控训练曲线    提交人    /u/dvr_dvr   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/</guid>
      <pubDate>Tue, 11 Feb 2025 08:13:22 GMT</pubDate>
    </item>
    <item>
      <title>PPO 标准差实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imr2hg/ppo_standard_deviation_implementation/</link>
      <description><![CDATA[大家好， 我对 PPO 中随机策略的实现有些困惑。在此之前，我已实现了 SAC 的几个变体，在几乎所有情况下，我都使用单个神经网络来输出我的行为的平均值和对数标准差。 据我所见和所试，大多数 PPO 实现要么使用恒定标准差，要么随着时间的推移线性降低标准差。我曾见过有人提到与状态空间无关的学习标准差。但我还没有见过这种实现（如果不是状态空间，我不确定我从哪里学到了什么）。 据我所知，这种差异是由于 SAC 使用最大熵目标，而 PPO 在其目标中不直接使用熵。但这也让我感到困惑，因为增加熵不是会鼓励更大的标准差吗？  我尝试使用来自 SAC 的策略神经网络实现 PPO，但失败了。但是当我使用恒定标准差或线性减少它时，我能够在推车杆上学到一些东西。 任何帮助都将不胜感激！    提交人    /u/LostBandard   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imr2hg/ppo_standard_deviation_implementation/</guid>
      <pubDate>Tue, 11 Feb 2025 05:08:06 GMT</pubDate>
    </item>
    <item>
      <title>论文提交给顶级会议，但未取得成果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1impaq6/paper_submitted_to_a_top_conference_with/</link>
      <description><![CDATA[我注意到原作者提供的代码甚至与他们论文中的方法论都不匹配，因此我联系了原作者。我根据他们的论文进行了完整而忠实的复制，但我得到的结果并不像他们报告的那么完美。 学术捏造是新常态吗？    提交人    /u/Rei_Opus   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1impaq6/paper_submitted_to_a_top_conference_with/</guid>
      <pubDate>Tue, 11 Feb 2025 03:31:02 GMT</pubDate>
    </item>
    <item>
      <title>最受欢迎的强化学习排行榜有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imonl5/what_are_the_most_popular_reinforcement_learning/</link>
      <description><![CDATA[我想知道是否有一些针对各种环境的知名、最新的官方排行榜？我发现健身房排行榜在将近一年前更新过，我记得在我参加他们的深度强化学习课程时，hugging face 有一些排行榜。我们甚至有一些有人关心的知名记分牌吗？ 能够跟踪和绘制最新技术及其表现如何听起来相当重要。这样我们就能了解方向和进展。或者我们只关心推动新游戏的突破，我们只需检查一下，强化学习在 Minecraft 中使用 DreamerV3 获得钻石，仅此而已。我们正在等待更复杂的游戏被击败。 你对此有什么看法？只是做一个氛围检查，听听你们的想法    提交人    /u/Inexperienced-Me   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imonl5/what_are_the_most_popular_reinforcement_learning/</guid>
      <pubDate>Tue, 11 Feb 2025 02:58:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习路线图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1im6dea/reinforcement_learning_roadmap/</link>
      <description><![CDATA[我想学习强化学习，但不知道从哪里开始。我对不同类型的 NN 的标准工作以及目前流行的架构（如 transformers）有很好的了解。 谢谢你的帮助    提交人    /u/dc_baslani_777   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1im6dea/reinforcement_learning_roadmap/</guid>
      <pubDate>Mon, 10 Feb 2025 13:42:50 GMT</pubDate>
    </item>
    </channel>
</rss>