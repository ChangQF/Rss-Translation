<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 04 Jul 2024 09:17:26 GMT</lastBuildDate>
    <item>
      <title>非平稳环境中的策略梯度算法收敛。嗨，我正在尝试阅读有关非平稳环境中的 PG。在这样的环境中如何保证收敛到局部最优。在这方面有没有好的教程或论文。谢谢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duyxd7/policy_gradient_algorithm_convergence_in_non/</link>
      <description><![CDATA[在非平稳环境中，还需要哪些其他因素来处理。     提交人    /u/aabra__ka__daabra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duyxd7/policy_gradient_algorithm_convergence_in_non/</guid>
      <pubDate>Thu, 04 Jul 2024 05:14:48 GMT</pubDate>
    </item>
    <item>
      <title>帮助 NEAT-python - 贪吃蛇游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duy1dz/help_with_neatpython_snake_game/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duy1dz/help_with_neatpython_snake_game/</guid>
      <pubDate>Thu, 04 Jul 2024 04:21:28 GMT</pubDate>
    </item>
    <item>
      <title>如果重量极化稳定下来，是不是很糟糕？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duxsdl/is_weight_polarization_bad_if_it_stabilizes/</link>
      <description><![CDATA[        提交人    /u/Breck_Emert   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duxsdl/is_weight_polarization_bad_if_it_stabilizes/</guid>
      <pubDate>Thu, 04 Jul 2024 04:06:53 GMT</pubDate>
    </item>
    <item>
      <title>扩散器/决策扩散器在什么样的视野上进行训练和生成？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1durt7n/what_horizon_does_diffuserdecision_diffuser_train/</link>
      <description><![CDATA[这里有人用过 Janner 的扩散器或 Ajay 的决策扩散器吗？ 我想知道他们为 d4rl 任务训练扩散模型的范围（即序列长度）是否与他们生成的计划的范围（序列长度）相同。  根据论文或代码库配置，目前尚不清楚；但直觉上我会想象，为了完成任务，生成的计划的序列长度应该比他们训练的序列长度更长，特别是如果训练序列最终没有达到目标或只是达到目标的序列的子集。    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1durt7n/what_horizon_does_diffuserdecision_diffuser_train/</guid>
      <pubDate>Wed, 03 Jul 2024 22:57:49 GMT</pubDate>
    </item>
    <item>
      <title>无法决定使用异步还是同步中间件来与 TensorFlow 模型交互自定义模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/</link>
      <description><![CDATA[大家好。 我目前正在 MARL n 上进行一个项目，我将在一个 Docker 容器中运行 Python 模拟，在另一个容器中运行 Tensorflow 模型。我知道通过使用 docker compose，我将创建一个包含这些容器的本地网络，但无法真正决定是使用消息队列来通信代理和模型 [他们将询问模型下一步要做什么操作，并且他们还将为其提供数据（无需任何响应）来训练它]；还是使用 Rest API + 模型从中消耗的缓冲区。 您能推荐我任何涉及该主题的参考资料吗？提前谢谢您，祝您有美好的一天     提交人    /u/Miss_Bat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/</guid>
      <pubDate>Wed, 03 Jul 2024 22:22:48 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 与 Jax 2024 在强化学习环境/代理方面的比较</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dupwpu/pytorch_vs_jax_2024_for_rl_environmentsagents/</link>
      <description><![CDATA[只是为了澄清一下。我正在编写一个自定义环境。RL 算法设置为在 JAX 中运行最快（例如稳定基线），因此即使在 Pytorch/JAX 中运行环境的速度一样快，使用 JAX 更明智，因为您可以直接传递数据，或者从 pytorch 到 cpu 再到 jax（用于训练代理）的数据传输速度如此之快，在增加的时间方面是微不足道的？ 或者 pytorch 生态系统是否足够强大，它与 jax 实现一样快    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dupwpu/pytorch_vs_jax_2024_for_rl_environmentsagents/</guid>
      <pubDate>Wed, 03 Jul 2024 21:33:18 GMT</pubDate>
    </item>
    <item>
      <title>梦想家 V3 - Rllib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duphip/dreamer_v3_rllib/</link>
      <description><![CDATA[您好， 我正在尝试将 Dreamer V3 实现到 cartpole。我的代码应该可以工作，但我收到此错误；  &quot;ValueError: 注册优化器中的一个参数 (&lt;KerasVariable shape=(4, 256), dtype=float32, path=dreamer\_model/vector\_encoder/dense/kernel&gt;) 不是 tf.Variable！&quot;  我认为这与 DreamerV3Config 有关。该错误是由 DreamerV3Config 和 TensorFlow 之间的关系引起的。但这似乎很奇怪。 DreamerV3Config 由 rllib 提供，因此这不应包含错误。 您对我该如何解决这个问题有什么想法吗？ 这是我的代码； 来自 ray.rllib.algorithms.dreamerv3.dreamerv3 导入 DreamerV3Config 来自 ray.tune.logger 导入 pretty_print  config = ( DreamerV3Config() .environment(&quot;CartPole-v1&quot;) .training( model_size=&quot;XS&quot;, training_ratio=1024, ) )  algo = config.build()  for i in range(10): result = algo.train() print(pretty_print(result))  if i % 5 == 0: checkpoint_dir = algo.save().checkpoint.path print(f&quot;检查点保存在目录 {checkpoint_dir}&quot;)    提交人    /u/Plenty-Context-8935   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duphip/dreamer_v3_rllib/</guid>
      <pubDate>Wed, 03 Jul 2024 21:15:13 GMT</pubDate>
    </item>
    <item>
      <title>最简单的 RL 环境无法通过 SB3 PPO 进行学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duh0d2/simplest_rl_environment_not_learning_with_sb3_ppo/</link>
      <description><![CDATA[      大家好， 我是 RL 的初学者。我已经构建了一个非常简单的 Env，我想训练一个代理将其用作课程培训的第一步。Env 随机选择一个高度值 (desiredAlt)。env 将动作映射到高度 (选定高度)，误差就是差值。我使用误差的负绝对值作为奖励，希望奖励随着时间的推移而变小。  class Env(gym.Env): metadata = {&#39;render_modes&#39;: [&#39;human&#39;],&#39;render_fps&#39;: 30} MAX_ALT = 5000 def __init__(self, render_mode): super(Env, self).__init__() self.action_space = space.Box(low=-0, high=1, shape=(1,), dtype=np.float32) self.observation_space = space.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32) self.render_mode = render_mode self.reset() def reset(self, seed=None, options=None): self.done = False self.current_step = 0 self.desiredAlt = np.random.random() * self.MAX_ALT self.altError = 0 返回self._get_obs(), {} def _get_obs(self): return np.array([ self.desiredAlt, self.altError, ], dtype=np.float32) def step(self, action): assert self.action_space.contains(action), &quot;无效操作&quot; self.altitude = action[0] * self.MAX_ALT self.altError = self.desiredAlt - self.altitude self.reward = -np.absolute(self.altError) truncated = False self.done = self.altError &lt; 1 或 self.current_step &gt;= 100 self.current_step += 1 return self._get_obs(), self.reward, self.done, truncated, {} def render(self, mode=&#39;human&#39;): if self.render_mode == &#39;human&#39;: print( f&quot;{self.current_step} | &quot; f&quot;Desired Alt: {self.desiredAlt:.2f}&quot; f&quot; Alt Error after action:{self.altError:.2f}&quot; f&quot; Reward:{self.reward:.6f}&quot; )  我正在使用 SB3 PPO。这是我的 Trainer.py 中的相关部分   class CustomCallback(BaseCallback): def __init__(self, verbose=0): super(CustomCallback, self).__init__(verbose) def _on_step(self) -&gt; bool: return True vec_env = make_vec_env(lambda: env, n_envs=1) model = PPO(&#39;MlpPolicy&#39;, vec_env, verbose=1, tensorboard_log=&quot;./tensorboard/&quot;) model.learn(total_timesteps=500 * 1000, callback=CustomCallback())  这个环境有意义吗？代理是否会知道它只需从给定的奖励（在本例中为错误）中选择正确的高度值？ 我的 ep_rew_mean 如下。它似乎不会比某个值更好，然后在某些时间步骤中也会变得明显更糟。这是为什么呢？ https://preview.redd.it/4d9b4n0fmbad1.png?width=1289&amp;format=png&amp;auto=webp&amp;s=1d05970fe010c9b173742977ba73c0b219e9d9d9    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duh0d2/simplest_rl_environment_not_learning_with_sb3_ppo/</guid>
      <pubDate>Wed, 03 Jul 2024 15:21:33 GMT</pubDate>
    </item>
    <item>
      <title>DRL 算法是否有可能像监督学习一样面临过度拟合问题？如果是这样，那么我们如何检查和缓解它？我将感谢任何与此相关的信息。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dud2i0/does_there_is_possibility_to_drl_algorithm_to/</link>
      <description><![CDATA[  由    /u/Correct-Jaguar-339  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dud2i0/does_there_is_possibility_to_drl_algorithm_to/</guid>
      <pubDate>Wed, 03 Jul 2024 12:25:33 GMT</pubDate>
    </item>
    <item>
      <title>寻求 RL 研究和实验出版标准的指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duc2dc/seeking_guidance_on_rl_research_and_experiment/</link>
      <description><![CDATA[大家好， 我是强化学习 (RL) 研究领域的新手，有几个问题。如果您能提供任何帮助，我将不胜感激。请原谅我的英语不好。  强化学习研究中的实验结果标准：  我正在努力改进 Soft Actor-Critic (SAC) 算法。在知名的机器学习期刊上发表实验结果的惯例或标准是什么？例如，在 DeepMind Control Suite 或 OpenAI Gym 等知名基准上，平均总奖励提高 5%-10% 就足够了吗？  实验步骤和重复：  我注意到一些 RL 研究论文进行了 200 万步的实验，并重复每个实验五次。如果我进行 100 万步实验并重复每个实验三次，结果是否仍然足以令人信服，使我的论文被顶级 ML 期刊接受？产生令人信服的结果所需的最少步骤和重复次数是多少？  RL 训练的硬件建议：  我应该投资 M2 Ultra 或配备 i9-14900K 和 RTX 4090 的 PC 来训练我的模型吗？在 M2 Ultra 上使用 Docker 运行实验是否可行，结果是否足以令人信服地发表？ 提前感谢您的指导！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duc2dc/seeking_guidance_on_rl_research_and_experiment/</guid>
      <pubDate>Wed, 03 Jul 2024 11:30:51 GMT</pubDate>
    </item>
    <item>
      <title>利用代理来产生自己的塑造奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du8jf9/leverage_an_agent_that_generate_its_own_shaping/</link>
      <description><![CDATA[我有一个非马尔可夫问题，具有部分可观察状态，但我能够通过单独的方式为该状态生成价值函数。 以不同的方式表述，我可以独立于环境生成塑造奖励。但是，知道你离目标有多远，并不等同于知道该做什么。 关于如何利用此类信息的任何想法或文献？    提交人    /u/Omnes_mundum_facimus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du8jf9/leverage_an_agent_that_generate_its_own_shaping/</guid>
      <pubDate>Wed, 03 Jul 2024 07:31:19 GMT</pubDate>
    </item>
    <item>
      <title>DRL 算法中填充过多</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du4vkf/overfilled_in_drl_algorithm/</link>
      <description><![CDATA[大家好， 我想知道 DQN 算法和其他 DRL 算法是否会像监视学习一样面临过度拟合问题。如果是这样，那么我们如何检查并缓解它？    提交人    /u/Correct-Jaguar-339   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du4vkf/overfilled_in_drl_algorithm/</guid>
      <pubDate>Wed, 03 Jul 2024 03:42:24 GMT</pubDate>
    </item>
    <item>
      <title>强化学习解决装配线平衡问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du4de0/reinforcement_learning_for_assembly_line/</link>
      <description><![CDATA[大家好！我想听听您对使用深度强化学习解决装配线平衡问题 (ALBP) 的看法。这是工业工程和运筹学中一个众所周知的优化问题，重点是如何有效地组织装配线上的任务，以最大限度地提高生产率，并最大限度地减少闲置时间或瓶颈。 以下是一些需要牢记的关键概念： 任务：这些是组装产品所需的单个操作或活动，每个操作或活动都有其特定的处理时间。 工作站：这些是装配线上执行任务的指定区域。 周期时间：这是指每个工作站完成其分配的任务所允许的最长时间，它决定了装配线的生产率。 优先约束：由于产品组装的性质，某些任务必须在其他任务之前完成。这些关系在优先图中表示。    提交人    /u/Icy_Bar_681   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du4de0/reinforcement_learning_for_assembly_line/</guid>
      <pubDate>Wed, 03 Jul 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>测试 dreamerv3 的最佳库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtxw7t/best_library_for_testing_out_dreamerv3/</link>
      <description><![CDATA[嘿，我想测试一下 dreamerv3，我尝试使用 rayrl，但我认为 tensorflow 的最新更新破坏了它，或者他们必须更新他们的东西。无论如何，什么是尝试 d​​reamerv3 并在其他环境中进行训练的最佳库？    提交人    /u/hinsonan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtxw7t/best_library_for_testing_out_dreamerv3/</guid>
      <pubDate>Tue, 02 Jul 2024 21:57:00 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Geometric、强化学习和 OpenAI Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtmo5l/pytorch_geometric_reinforcement_learning_and/</link>
      <description><![CDATA[大家好。 正如标题所述，我正在尝试实现 openai gymnasium frostylake-v1 环境，以 pytorch 几何知识图谱表示，其中每个单元都是一个知识图谱节点，并且每条边都连接到玩家可以采取的可能路线。但是，我遇到了一个问题，即除非节点特征包含唯一值（无论是唯一节点索引还是它们在 4x4 地图中的位置），否则我的模型无法生成良好的结果。 我需要它独立于这些唯一索引，并且可能在一张地图上进行训练，然后将训练有素的代理放在一张新地图上，在那里他仍然能够对好动作和坏动作有一些概念（例如，掉进洞里总是不好的）。我该如何扩展这个问题？我做错了什么？如需更多信息，请在评论中留下，我一定会回答。 我正在写一篇论文，这个 openai gym 与我将在最终论文中进行训练的环境类似。所以我真的需要帮助解决这个特定问题。  编辑以获取进一步的深入信息： 我正在尝试将深度强化学习与图神经网络相结合以支持图环境。我使用 GNN 来估计 Dueling Double Deep Q-Network 架构中的 Q 值。我已经用 2 到 4 个 pytorch 几何 GNN（GCN、GAT 或 GPS）层替换了 MLP 层。 观察空间 为了测试这个架构，我使用了 frostylake-v1 环境的包装器，将观察空间转换为图形表示。每个节点都通过边连接到与其相邻的其他节点，代表一个就像正常人所看到的网格一样。 情况 1，具有位置编码： 每个节点具有 3 个特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 第二和第三个特征表示单元格的位置编码（单元格 x/y 坐标）： 第二个特征表示单元格列。 第三个特征表示单元格行。   情况 2，没有位置编码，使用单元格类型作为特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 单元格的类型。如果它是一个正常单元，则为 0；如果它是一个洞，则为 -1；如果它是目标，则为 1。  动作空间 动作空间与 openai gym freezelake 文档中的完全相同。代理对 frostinglake-1 环境有 4 种可能的操作（0=左、1=下、2=右、3=上）。 奖励空间 奖励空间与 openai gym frostinglake 文档中的完全相同。 问题 我已成功实现了具有所有默认单元的默认 4x4 网格环境的策略收敛。在我的实验中，代理只能在案例 1 中描述的观察空间中实现这种收敛。  我试图理解为什么需要位置编码才能实现收敛？ 在实施观察空间案例 2 时，即使在长时间训练的探索过程中多次获得最终奖励，代理也永远不会收敛。 由于与 transformer 相同的原因，GNN 是否也需要位置嵌入？ 如果我在小型网格环境中使用足够的消息传递 2 到 4 层，每个节点都应该具有来自图中每个其他节点的信息，那么网络是否应该能够在这种情况下隐式学习位置嵌入？ 我也尝试过使用其他位置嵌入 (PE) 方法，例如随机游走（5-40 次游走）和拉普拉斯向量（2-6 K 值），但我无法使用此 PE 实现收敛方法。 奇怪的是，我也尝试过使用随机化的唯一节点索引作为特征，而不是位置编码，并且代理能够收敛。我不明白为什么代理在这些条件下能够收敛，但在 PE 情况和观察空间情况 2 中却不能收敛。     提交人    /u/SmkWed   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtmo5l/pytorch_geometric_reinforcement_learning_and/</guid>
      <pubDate>Tue, 02 Jul 2024 14:05:23 GMT</pubDate>
    </item>
    </channel>
</rss>