<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 18 Mar 2024 06:18:35 GMT</lastBuildDate>
    <item>
      <title>MuZero 应用程序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh7x8z/muzero_applications/</link>
      <description><![CDATA[嘿伙计们！ 我最近建立了自己的库来训练 MuZero 和 AlphaZero 模型，我意识到我从来没有看到了该算法的许多应用（除了来自 DeepMind 的）。  所以我想问一下你是否曾经使用 MuZero 做过什么？如果是这样，您的应用程序是什么？   由   提交/u/Skirlaxx  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh7x8z/muzero_applications/</guid>
      <pubDate>Sun, 17 Mar 2024 20:37:34 GMT</pubDate>
    </item>
    <item>
      <title>我应该选择哪个 Q 值作为深度 Q 网络输出的操作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh4jbw/which_qvalue_do_i_select_as_the_action_from_the/</link>
      <description><![CDATA[我正在开发一个项目，该项目涉及使用 Deep Q-Learning Agent 来学习选择 1000 节点 NetworkX 中某些节点的适当方法图形。我的 observation_space 是一个 (1000, 3) 数组，每行代表节点标签、度数和变量/属性（0、1 或 2）。 action_space 具有 (1000, 1) 形状，每个元素对应于在指定节点上采取的操作。  这是我的 Deep Q 网络的代码： ``` def nnmodel(observation_space, action_space): model = tf.keras .models.Sequential（） model.add（tf.keras.layers.Dense（128，input_shape =（无，observation_space.shape [0]，observation_space.shape [1]），activation =&#39;relu&#39;））model.add (tf.keras.layers.Dense(256，激活=&#39;relu&#39;)) model.add(tf.keras.layers.Dense(256，激活=&#39;relu&#39;)) model.add(tf.keras.layers.Dense (len(action_space),activation=&#39;线性&#39;)) model.compile(optimizer=Adam(),loss=&#39;mse&#39;,metrics = [&#39;accuracy&#39;]) 返回模型   ``` 从理论上讲，据我所知，应该给我来自   q_values = model.predict(observation_space) 的 q 值  但是，我的 q_values 的形状为 (1000, 1000)，我不确定哪个“最高 q 值”是我应该考虑对应于代理应该执行操作的节点。它是最高 q 值条目，其行/列对应于代理应选择的节点吗？或者它是最大的行/列总和？或者完全是另外一回事？我在网上查看的示例通常使用 np.argmax(q_values[0])，我觉得这不适用于我的情况。  此外，我的 input_shape 看起来对于我所描述的问题是否正确？  任何帮助表示赞赏！  max_q = np.max(q_values) position = np.where(q_values == max_q) print(position)  这返回最大 q 值的索引。我不确定这是否意味着我应该为第 i 行或第 j 列选择第 i/j 个节点。    由   提交/u/No_Type_2250   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh4jbw/which_qvalue_do_i_select_as_the_action_from_the/</guid>
      <pubDate>Sun, 17 Mar 2024 18:23:43 GMT</pubDate>
    </item>
    <item>
      <title>近端策略优化有帮助吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh3j6h/proximal_policy_optimization_help/</link>
      <description><![CDATA[      我想知道是否有人了解近端策略优化？我一直很难让它发挥作用，我花了几个星期。我正在 LUA 上创建一个编译版本。我认为这就是发生此错误的地方。  https://preview.redd .it/4aua8cchlxoc1.png?width=756&amp;format=png&amp;auto=webp&amp;s=a229c9dff65f4ba0dd64d7df41de63420ec7a03d   由   提交/u/Diligent_Marzipan65   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh3j6h/proximal_policy_optimization_help/</guid>
      <pubDate>Sun, 17 Mar 2024 17:43:01 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习 - PettingZoo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bh2llu/multiagent_reinforcement_learning_pettingzoo/</link>
      <description><![CDATA[我有一款竞争性的团队射击游戏，我已将其转换为 PettingZoo 环境。然而，我现在面临一些问题。   是否有任何好的教程或库可以引导我使用 PettingZoo 环境来训练 MARL 策略？ 是否有任何简单的方法来实现自我对弈？ （只要它以某种能力存在，它就可以是非常基本的） 有什么好的方法来检查我的 PettingZoo 环境是否合规？每次我使用不同的库（即到目前为止我尝试过的 TianShou 和 TorchRL）时，它都会针对我的代码的错误给出不同的错误，并且每个库都要求 env 的格式完全不同。 &lt; /li&gt;  到目前为止，我已经尝试遵循 https://pytorch.org/rl/tutorials /multiagent_ppo.html，TorchRL 和 PettingZooWrapper 中都有 EnvBase，但两者都不起作用。除此之外，我尝试过 https://tianshou.org/en/master/01_tutorials/04_tictactoe.html  但修改它以适应我的环境。  通过“不工作”，我的意思是它给了我一些模糊的错误，在我理解它想要所有内容的格式之前我无法真正修复它，但我找不到关于什么的良好文档每个图书馆实际上都想要。 我绝对没有把我的工作留到最后一刻。 我真的很感激任何有关这方面的帮助，甚至是指向一个稍微有一些图书馆的指针。所有这一切的更清晰的文档。谢谢！   由   提交/u/SinglePhrase7  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bh2llu/multiagent_reinforcement_learning_pettingzoo/</guid>
      <pubDate>Sun, 17 Mar 2024 17:03:58 GMT</pubDate>
    </item>
    <item>
      <title>Cognition AI 推出 Devin：“金牌编码员构建了一个可以为他们完成工作的人工智能”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bggkqu/devin_launched_by_cognition_ai_goldmedalist/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bggkqu/devin_launched_by_cognition_ai_goldmedalist/</guid>
      <pubDate>Sat, 16 Mar 2024 21:26:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习背景下的迁移学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bgb0f9/transfer_learning_in_the_context_of_rl/</link>
      <description><![CDATA[有没有人经历过与此相关的实用框架？ 我的搜索产生的大部分解决方案并不能完全解决我的具体问题. 我正在处理的问题是确定各种互动的最佳时机，每种互动的目的都是促使某些人采取积极行动。 I拥有有关这些人的初步信息，并且每次状态都是根据之前与其进行的交互以及这些交互的结果来定义的 我正在寻找实用的工具来执行转移群体之间的学习。   由   提交/u/Murky-Name4868  /u/Murky-Name4868 reddit.com/r/reinforcementlearning/comments/1bgb0f9/transfer_learning_in_the_context_of_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bgb0f9/transfer_learning_in_the_context_of_rl/</guid>
      <pubDate>Sat, 16 Mar 2024 17:15:13 GMT</pubDate>
    </item>
    <item>
      <title>预训练/内置宠物动物园代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bg9qf5/pretrainedbuildin_pettingzoo_agents/</link>
      <description><![CDATA[      是否有任何可以使用的预训练/内置 PZ 代理？我正在尝试在 atari pong 中实施对抗性政策，但努力训练可靠的 PPO 代理。已经尝试使用纯粹的自我对战，但代理很难学习 - 即使我开始对抗随机移动代理。  ​ 我还尝试编写一个自定义包装器，它将模仿 SB3 包装器以使用 StableBaseline3 代理之一 - 但由于某种原因，使用相同的 openCV 函数进行转换从 RGB 到灰度会产生不同的值。 （SB3 env 看起来“蓝色”） ​ 你知道有什么预训练的智能体可以用来训练固体 PPO 智能体或在对抗训练中使用它吗？  也许您知道其他带有预训练/内置代理的 RL 库，这些库也允许在训练期间设置多个代理策略？  sb3 环境&lt; /p&gt; ​  我还尝试编写一个自定义包装器来使用 StableBaseline3 代理 - 但由于某种原因，使用相同的 openCV 函数从 RGB 转换为灰度会导致不同的值。 （SB3 env 看起来“蓝色”） ​   由   提交/u/MrCogito_hs   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bg9qf5/pretrainedbuildin_pettingzoo_agents/</guid>
      <pubDate>Sat, 16 Mar 2024 16:18:11 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bg049g/multiagent_reinforcement_learning/</link>
      <description><![CDATA[我有一个在环境中训练的模型，并为其使用了 3 个代理以及来自 Stable Baselines3 的 PPO。 我想测试现在有 10 个智能体的模型，但 OpenAIgym 给出了观察空间不匹配的错误。我怎样才能实现这个目标？   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bg049g/multiagent_reinforcement_learning/</guid>
      <pubDate>Sat, 16 Mar 2024 06:51:23 GMT</pubDate>
    </item>
    <item>
      <title>健身房观察套装</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfwfr1/gym_observation_set/</link>
      <description><![CDATA[       大家好！我想改变我的env.reset和env.step返回的观察空间，例如hopper，我想让它返回(11,)，但实际上它返回(12,0)，因为我使用的gym版本太低，无法通过参数“exclude_current_positions_from_observation”更改。你能帮我解决一下吗？万分感谢！  https://preview.redd .it/wpsryhp65moc1.png?width=1298&amp;format=png&amp;auto=webp&amp;s=6635141bf5a7b6223d3f8e55e0b9e8b6e0b32347   由   提交/u/alleZhou  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfwfr1/gym_observation_set/</guid>
      <pubDate>Sat, 16 Mar 2024 03:12:42 GMT</pubDate>
    </item>
    <item>
      <title>“持续预训练大型语言模型的简单且可扩展的策略”，Ibrahim 等人 2024（循环 LR 和重播或多样化数据）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfulix/simple_and_scalable_strategies_to_continually/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfulix/simple_and_scalable_strategies_to_continually/</guid>
      <pubDate>Sat, 16 Mar 2024 01:39:47 GMT</pubDate>
    </item>
    <item>
      <title>MuZero简单实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfr08n/muzero_simple_implementation/</link>
      <description><![CDATA[我想尝试对随机 MuZero 与经典/原始 MuZero 的比较进行基准测试。我发现的所有实现都经过了大量优化。我正在寻找一个简单易读的 MuZero 实现，我可以轻松理解或修改它。您有什么建议吗？   由   提交 /u/_Hardric   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfr08n/muzero_simple_implementation/</guid>
      <pubDate>Fri, 15 Mar 2024 22:54:07 GMT</pubDate>
    </item>
    <item>
      <title>PPO 在训练期间（有探索）学习并表现良好，但在评估期间（没有探索）表现不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfmuwd/ppo_learns_and_performs_perfectly_during_training/</link>
      <description><![CDATA[我尝试过调整学习率、折扣因子、lambda、剪辑值、熵正则化系数和 L2 正则化系数，但到目前为止还没有成功。有什么建议来解决这个问题吗？  提前致谢！   由   提交/u/Appressive_Bag1262   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfmuwd/ppo_learns_and_performs_perfectly_during_training/</guid>
      <pubDate>Fri, 15 Mar 2024 19:54:54 GMT</pubDate>
    </item>
    <item>
      <title>迈向通用情境学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bfbb3d/towards_generalpurpose_incontext_learning_agents/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=eDZJTdUsfe 演讲和幻灯片：https://neurips.cc/virtual/2023/79880 博客文章：http://louiskirsch.com/glas 摘要：  强化学习（RL）算法通常是手工制作，由人类研究和工程驱动。另一种方法是通过元学习使该研究过程自动化。一个特别雄心勃勃的目标是从头开始自动发现新的强化学习算法，使用上下文学习来完全从数据中学习如何学习，同时推广到广泛的环境。这些强化学习算法完全在神经网络中实现，通过根据环境中的先前经验进行调节，在元测试时没有任何基于优化的显式例程。为了实现泛化，这需要在多样化和具有挑战性的环境中进行广泛的任务分配。我们基于 Transformer 的通用学习代理 (GLA) 是朝这个方向迈出的重要的第一步。我们的 GLA 使用监督学习技术在离线数据集上进行元训练，并利用 RL 环境中的经验，并通过随机投影进行增强，以生成任务多样性。在元测试期间，我们的智能体对完全不同的机器人控制问题（例如不在元训练分布中的 Reacher、Cartpole 或 HalfCheetah）执行上下文元强化学习。  &lt; !-- SC_ON --&gt;  由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bfbb3d/towards_generalpurpose_incontext_learning_agents/</guid>
      <pubDate>Fri, 15 Mar 2024 11:02:09 GMT</pubDate>
    </item>
    <item>
      <title>监督学习与离线强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</link>
      <description><![CDATA[我从 RL 开始，这些可能是非常琐碎的问题，但我想尽我所能地解决所有问题。如果您有任何资源可以为强化学习应用提供良好的直觉，也请在评论中提供：）谢谢。 问题：  我们在哪些场景中使用与离线强化学习相比，更喜欢监督学习？ 样本数量如何影响每个案例的训练？监督学习收敛得更快吗？ 有哪些例子将两者用于比较分析？  直觉：  监督学习可以很好地预测给定状态的奖励，但我们不能依赖它来最大化未来的奖励。由于它不使用推出来最大化奖励，并且不进行规划，因此我们不能期望在预期延迟奖励的情况下使用它。 此外，在非独立同分布的动态环境中，每个动作都会影响状态，然后影响进一步采取的动作。因此，对于连续设置，我们在大多数情况下考虑了 RL 的分布变化。 监督学习尝试为每个状态找到最佳动作，这在大多数情况下可能是正确的，但它是一个非常好的方法。针对不断变化的环境采取僵化而愚蠢的方法。强化学习可以自我学习，并且适应性更强。  对于答案，如果可能的话，提供单行，然后任何细节和答案来源也将不胜感激。我希望这篇文章能够为任何尝试应用强化学习的人提供一个很好的指南。我将编辑和更新下面回答的任何问题的答案，以汇总我获得的所有信息。如果您认为我应该考虑任何其他重大问题和疑虑，也请提及。谢谢！ ​ [编辑]：我找到的与此相关的资源： Sergey Levine 的 RAIL 讲座：模仿学习与离线强化学习 Sergey Levine 的中型帖子：数据决策：离线强化学习将如何改变我们的使用方式机器学习 Sergey Levine 的中型帖子：通过行动了解世界：强化学习作为可扩展自我监督学习的基础 Sergey Levine 的研究论文：我们什么时候应该更喜欢离线强化学习而不是行为克隆？ Sergey Levine 的研究论文：RVS：通过监督学习实现离线强化学习的基本要素是什么？   由   提交 /u/StwayneXG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</guid>
      <pubDate>Fri, 15 Mar 2024 05:17:56 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>