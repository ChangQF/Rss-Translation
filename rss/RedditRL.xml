<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 02 Jan 2025 03:20:19 GMT</lastBuildDate>
    <item>
      <title>🚀 使用大型语言模型增强数学问题解决能力：分而治之的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hr4dvz/enhancing_mathematical_problem_solving_with_large/</link>
      <description><![CDATA[大家好！ 我很高兴与大家分享我们的最新项目：使用大型语言模型 (LLM) 增强数学问题解决能力。我们的团队开发了一种新颖的方法，利用分而治之的策略来提高 LLM 在数学应用中的准确性。 主要亮点：  专注于计算挑战，而不是基于证明的问题。 在各种测试中实现最先进的性能。 开源代码可供任何人探索和贡献！  在此处查看我们的 GitHub 存储库：DaC-LLM 我们正在寻找有兴趣推进该领域研究的反馈和潜在合作者。如有任何疑问，请随时联系我们或发表评论！ 感谢您的支持！    提交人    /u/jasonhon2013   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hr4dvz/enhancing_mathematical_problem_solving_with_large/</guid>
      <pubDate>Wed, 01 Jan 2025 14:53:09 GMT</pubDate>
    </item>
    <item>
      <title>grokking 的书好看吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hr03om/is_the_grokkings_book_any_good/</link>
      <description><![CDATA[我正在寻找优秀的 RL 书籍。我知道 Sutton 和 Barto 的书是标准，但我发现它的 pdf 有点吓人。我正在寻找可以帮助我快速学习概念的书籍，最好数学内容较少。另一本书是 Grokkings 书，想知道它是否值得购买（在我的国家它非常昂贵）。如果您有其他推荐的书籍，请告诉我。谢谢    提交人    /u/insightfuleffect   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hr03om/is_the_grokkings_book_any_good/</guid>
      <pubDate>Wed, 01 Jan 2025 10:02:45 GMT</pubDate>
    </item>
    <item>
      <title>关于博士录取</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqvckw/regarding_phd_admissions/</link>
      <description><![CDATA[我想攻读 RL、ML（主要是理论部分）博士学位，我是一名机械工程本科生。不想攻读硕士学位。但我想知道 GPA 在获得博士学位录取时有多重要，我知道它非常重要，但如果 GPA 很差，比如 6/10，会考不上吗？研究论文能否弥补由于 GPA 造成的这种差距？假设某人在医学应用机器学习方面有一篇第一作者方法论文。还有另一篇关于神经符号 AI 的机器人技术论文，以及一些关于机器人策略的经验，以及不错的 ML、RL 课程项目背景等    提交人    /u/vyknot4wongs   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqvckw/regarding_phd_admissions/</guid>
      <pubDate>Wed, 01 Jan 2025 04:11:27 GMT</pubDate>
    </item>
    <item>
      <title>RL 博客？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqsyw2/rl_blogs/</link>
      <description><![CDATA[你们听说过 TLDR AI 吗？它让我对 AI 的发展有了很好的了解。我是一名初学者，想跟上 RL 的步伐。我可以阅读哪些关于 RL 的博客/文章？    提交人    /u/Cereal_killer09   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqsyw2/rl_blogs/</guid>
      <pubDate>Wed, 01 Jan 2025 01:39:15 GMT</pubDate>
    </item>
    <item>
      <title>探索性的定义（Barton 和 Sutton 问题）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqn7o1/definition_of_exploratory_barton_and_sutton/</link>
      <description><![CDATA[我正在阅读 Barto 和 Sutton 的《强化学习导论》第二版，有一个基本问题。 练习 2.1 &quot;在 e-贪婪动作选择中，对于两个动作且 e = 0.5 的情况，选择贪婪动作的概率是多少？&quot; 我的解决方案是 0.75，因为有 50% 的机会选择随机选择，而此后，只有 50% 的机会选择非贪婪动作。但其他几个在线资源表明是 0.50。 作为参考，此文本包含在书中。 “一个简单的替代方法是大多数时间都贪婪地行事，但偶尔，以小概率从所有具有相同概率的动作中随机选择，独立于动作值估计。” 所以要么我误解了这一点，要么探索故意省略了贪婪动作，要么我错过了一个微妙的语义问题。我也有可能是对的：） 任何帮助都将不胜感激。这是一个非常沉重的文本，我想确保我理解了。    提交人    /u/EricTheNerd2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqn7o1/definition_of_exploratory_barton_and_sutton/</guid>
      <pubDate>Tue, 31 Dec 2024 20:24:44 GMT</pubDate>
    </item>
    <item>
      <title>学习 Issac Sim 的资源？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqa2x2/resources_to_learn_issac_sim/</link>
      <description><![CDATA[最近开始为我的毕业项目在现实世界中研究多代理 RL 实现。在阅读了有关 Unity MLAgents、Mujoco、Gazebo 和 IssacSim 的信息后，决定使用它。有没有什么好的资源可以学习如何使用 IssacSim？    提交人    /u/Excellent_Mood_3906   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqa2x2/resources_to_learn_issac_sim/</guid>
      <pubDate>Tue, 31 Dec 2024 07:59:39 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hq8iwu/need_help/</link>
      <description><![CDATA[      我正在为一家公司解决优化问题。 我有生产时间范围内 5 个变量的时间序列数据。 4 个参数被视为输入（尽管其中一个是温度我对是否将其用作输入参数有疑问）和 1 个参数作为输出（密度），困难在于输出会因一些变化的时间而滞后。 我训练了一个 LSTM 来捕捉系统的行为，它工作得很好，接受 5 个输入并输出 1 个输出。 现在我在制作控制器时遇到困难，假设我的 LSTM 是一个环境。 查看评论中的图表    提交人    /u/Old_Shine_4985   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hq8iwu/need_help/</guid>
      <pubDate>Tue, 31 Dec 2024 06:11:59 GMT</pubDate>
    </item>
    <item>
      <title>“基于协同效应的机器人群体行为的自动设计”，Salman 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpvi1r/automatic_design_of_stigmergybased_behaviours_for/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpvi1r/automatic_design_of_stigmergybased_behaviours_for/</guid>
      <pubDate>Mon, 30 Dec 2024 19:39:53 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo 基线3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpt4tr/pettingzoo_baselines3/</link>
      <description><![CDATA[我有 2 个具有不同角色的代理，问题是如何让模型在预测中理解（在测试加载的模型时）每个代理具有哪个角色（异构多代理系统）？我所做的是在 obs 中添加一个布尔值来区分角色，但我想知道我是否可以发出它并在测试时简单地使用 2 个不同的模型。 我目前正在测试（aec） model = PPO.load(latest_policy) # print(env.possible_agents) rewards = {agent: 0 for agent in env.possible_agents} # 注意：我们使用并行 API 进行训练，但使用 AEC API 进行评估# SB3 模型是为单代理设置设计的，我们通过对每个代理使用相同的模型来解决这个问题 for i in range(num_games): env.reset(seed=i) for agent in env.agent_iter(): obs, reward, terminology, truncation, info = env.last() # print(obs) if reward &gt; 0：rewards[agent] += reward 如果终止或截断：act = None else：act = model.predict(obs, deterministic=True)[0] print(f&quot;\nAgent: {agent}, Observation: {obs}, Reward: {reward}, Action: {act}&quot;) env.step(act) env.close()  在训练（并行）中我有 model = PPO( MlpPolicy, env, verbose=3, learning_rate=1e-3, batch_size=256, tensorboard_log=log_dir ) while True：model.learn(total_timesteps=steps, reset_num_timesteps=False) save_path = os.path.join(model_dir, f&quot;{env.unwrapped.metadata.get(&#39;name&#39;)}_{time.strftime(&#39;%Y%m%d-%H%M%S&#39;)}&quot;) model.save(save_path)     提交人    /u/More_Peanut1312   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpt4tr/pettingzoo_baselines3/</guid>
      <pubDate>Mon, 30 Dec 2024 17:59:58 GMT</pubDate>
    </item>
    <item>
      <title>关于为动态定价 RL 任务创建合成数据的建议。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpsnl4/advice_on_creating_synthetic_data_for_dynamic/</link>
      <description><![CDATA[大家好！ 我正在使用强化学习开展电子商务动态定价项目。由于我没有真实数据，因此我尝试生成合成数据进行训练。我的计划是比较 DQN 和 PPO 来完成此任务，并在自定义环境中设置价格以最大化收入或利润。 到目前为止，我了解了：  线性模型：价格上涨 → 需求下降（价格弹性）。 Logit 模型：基于经济模型的建模。 季节性：由于时间/事件导致的需求波动。  我希望数据能够模拟真实世界的行为，例如价格敏感度、季节性变化和一些随机性。我见过很多论文使用 DQN 进行离线学习，但我很想尝试 PPO 并比较结果。  我很乐意获得有关如何构建此类模型或应该包含哪些内容以使数据更真实的任何建议。 这是我第一次尝试从头开始创建环境（我只调整过健身房环境），所以我很乐意听取您的建议。    提交人    /u/Professional_Ant_140   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpsnl4/advice_on_creating_synthetic_data_for_dynamic/</guid>
      <pubDate>Mon, 30 Dec 2024 17:39:19 GMT</pubDate>
    </item>
    <item>
      <title>当回报在 1e6 和 1e10 之间时，如何标准化奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpsned/how_would_you_normalize_the_rewards_when_the/</link>
      <description><![CDATA[嘿，我正在努力使用 FQI 以外的任何其他方法获得良好的性能，以适应基于 https://orbi.uliege.be/bitstream/2268/13367/1/CDC_2006.pdf 的环境，最大时间步长为 200。观察空间的形状为 (6,)，动作空间是离散的 (4) 我不确定如何规范化奖励，因为随机代理获得的回报约为 1e7，而最佳代理应该获得 5e10。到目前为止，我得到的最佳结果是使用带有以下包装器的 PPO：  log(max(obs, 0) + 1) 将最后一个操作附加到 obs TimeAwareObservation FrameStack(10) VecNormalize  到目前为止，我尝试使用各种奖励标准化的 PPO 和 DQN，但没有成功（使用 sb3）：  使用 sb3 中的 VecNormalize 无标准化 除以 1e10（仅在 dqn 上尝试） 除以回报的运行平均值（仅在 dqn 上尝试） 除以回报的运行最大值（仅在 dqn 上尝试）  现在我有点绝望，并尝试使用 python-neat 运行 NEAT（性能低下）。 您可以在此处找到我对 env 的实现：https://pastebin.com/7ybwavEW 任何关于如何使用现代技术处理这种环境的建议都会受到欢迎！    提交人    /u/Butanium_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpsned/how_would_you_normalize_the_rewards_when_the/</guid>
      <pubDate>Mon, 30 Dec 2024 17:39:06 GMT</pubDate>
    </item>
    <item>
      <title>输出概率与初始初始化时没有变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hps2gg/output_probabilities_are_not_changing_from/</link>
      <description><![CDATA[因此，我正在实施一种 RL 方法用于股票交易，因此我有两个代理，一个决定进入哪个方向，另一个管理交易，因此进入模型输出买入和卖出之间的概率，它们初始化在 49、50 左右，但问题在于开发（验证），如果模型初始化时有一个略微优势的动作，它总是选择那个动作。我正在检查梯度并监控 wandb 上的权重，甚至概率比，尽管它们很小，但一切似乎都在移动，但输出保持不变。我的经理代理也是如此。所以我已经运行了五集，但单个训练集有 300 笔交易，每个交易都有六个训练阶段，所以我认为足够的训练可以看到概率分布的一些变化，但没有看到任何变化，奖励是不稳定的，所以我不能肯定地使用该指标来判断，至于验证，它一直执行单一动作，不像在训练中它探索并获得可观的回报，这可能是问题所在    提交人    /u/sk3ptica1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hps2gg/output_probabilities_are_not_changing_from/</guid>
      <pubDate>Mon, 30 Dec 2024 17:14:28 GMT</pubDate>
    </item>
    <item>
      <title>接受论文摘要的会议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpr46m/conferences_for_accepting_abstract_papers/</link>
      <description><![CDATA[大家好， 有任何会议/研讨会接受摘要论文吗？我现在全职工作。我没有太多时间进行实验，但我有一些想要发表的想法，有什么建议吗？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpr46m/conferences_for_accepting_abstract_papers/</guid>
      <pubDate>Mon, 30 Dec 2024 16:33:51 GMT</pubDate>
    </item>
    <item>
      <title>这里有业余爱好者或者独立现实爱好者吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpqy24/anybody_hobbyist_or_indie_rl_enthusiast_here/</link>
      <description><![CDATA[我有一些典型计算机科学的背景和经验，但在人工智能方面没有专业知识。所以我称自己为业余爱好者。我对解决现实世界的问题不感兴趣；我满足于追随大师在国际象棋或五子棋等已经征服的领域的成就。无论如何，我想将 RL 应用于双人参与抽象策略游戏。这个 subreddit 上有没有人尝试过类似的东西？    提交人    /u/Gloomy-Status-9258   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpqy24/anybody_hobbyist_or_indie_rl_enthusiast_here/</guid>
      <pubDate>Mon, 30 Dec 2024 16:26:32 GMT</pubDate>
    </item>
    <item>
      <title>比较 RL 代理的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpqsur/metrics_for_comparing_rl_agents/</link>
      <description><![CDATA[大家好！👋 我正在一个小型大学项目上工作，该项目在太空侵略者的背景下探索强化学习。我想将传统的 Q-Learning 代理与 DQN 进行比较，并且正在考虑使用哪些指标进行分析。 到目前为止，我已决定绘制：  每集得分 每集平均奖励 每集平均游戏时间  我还在考虑绘制平均 Q 值。但是，我对这是否合适有些怀疑。具体来说，我不确定如何解释由于每集步骤数的差异，Q 值在各集之间可能会有显著差异这一事实。 附注：我完全清楚 Q-Learning 是一种表格方法，不太适合具有大状态空间的环境。这一限制将是我进行比较分析的关键部分。 提前致谢！    提交人    /u/Lonely-Eye-8313   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpqsur/metrics_for_comparing_rl_agents/</guid>
      <pubDate>Mon, 30 Dec 2024 16:20:07 GMT</pubDate>
    </item>
    </channel>
</rss>