<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 12 Jan 2024 18:17:40 GMT</lastBuildDate>
    <item>
      <title>如何裁剪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</link>
      <description><![CDATA[我熟悉 RL 一个月了，想在一些 Atari 游戏上练习我的知识。我阅读了 DeepMind 的文章，了解他们如何使用 DQN 执行 Atari 任务 (https://arxiv.org/abs/1312.5602)。他们在论文中表示： “直接处理原始 Atari 帧（这些帧是具有 128 个调色板的 210 × 160 像素图像）的计算要求可能很高，因此我们应用了一个基本的预处理步骤，旨在减少输入维度。对原始帧进行预处理，首先将其 RGB 表示转换为灰度并将其下采样为 110×84 图像。最终的输入表示是通过裁剪图像的 84 × 84 区域来获得的，该区域大致捕获了游戏区域。” 这对我来说似乎很合理，但我想知道他们是如何以编程方式做到这一点的？我阅读了gymnasium（不是gym）文档（https://gymnasium.farama.org/api/wrappers/）但是，尽管他们有 FrameStack 和 GreyScale 的包装器，但下采样和裁剪包装器似乎不可用。 有人知道如何做到这一点吗？  非常感谢你们。   由   提交/u/Q_H_Chu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</guid>
      <pubDate>Fri, 12 Jan 2024 13:07:01 GMT</pubDate>
    </item>
    <item>
      <title>混合模拟和抽象进行物理推理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194orj6/blending_simulation_and_abstraction_for_physical/</link>
      <description><![CDATA[论文：https： //osf.io/preprints/psyarxiv/f9ukv 代码： https ://github.com/flxsosa/physicals_abstraction 摘要：  人们如何能够理解日常的物理事件这么轻松？一种假设表明人们使用对世界的近似概率模拟。一个对比假设是人们使用抽象或特征的集合。这两个假设解释了物理推理的互补方面。我们开发了一个综合了两种假设的“混合模型”：在某些条件下，模拟被视觉空间抽象（线性路径投影）取代。这种抽象以保真度为代价换取了效率，混合模型预测，只要满足应用抽象的条件，人们就会犯系统性错误。我们在两个实验中测试了这一预测，参与者对下落的球是否会接触目标进行判断。首先，我们表明，当直线路径不可用时，即使仿真时间保持固定，响应时间也会更长，这与纯仿真模型（实验 1）相反。其次，我们表明人们以与线性路径投影一致的方式错误地判断了球的轨迹（实验2）。我们的结论是，人们可以使用灵活的心理物理引擎，但会在有用时自适应地调用更有效的抽象。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194orj6/blending_simulation_and_abstraction_for_physical/</guid>
      <pubDate>Fri, 12 Jan 2024 06:52:59 GMT</pubDate>
    </item>
    <item>
      <title>[问题] 具有连续动作空间的 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194mfnt/question_dqn_with_continuous_action_spaces/</link>
      <description><![CDATA[        由   提交 /u/tengboss   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194mfnt/question_dqn_with_continuous_action_spaces/</guid>
      <pubDate>Fri, 12 Jan 2024 04:40:45 GMT</pubDate>
    </item>
    <item>
      <title>太空战争 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</link>
      <description><![CDATA[   /u/_Linux_AI_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</guid>
      <pubDate>Fri, 12 Jan 2024 03:50:56 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习中正则化和（有效）贴现之间的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1947wv7/relationship_between_regularization_and_effective/</link>
      <description><![CDATA[我在 minigrid 型环境。训练后，我将智能体置于一系列人为的情况下并测量其 Q 值，然后从这些 Q 值推断其有效折扣率（例如，根据前进价值如何随着接近目标而变化来推断折扣因子） ）。 当我以这种方式测量有效折扣因子时，它与我使用的显式折扣因子 (𝛾) 设置相匹配。 但是如果我添加对网络进行非常强的 L2 正则化（权重衰减），即使我没有更改代理的 𝛾 设置，推断的折扣因子也会降低。 有人可以帮我思考为什么会发生这种情况吗？谢谢！   由   提交/u/Beneficial_Price_560   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1947wv7/relationship_between_regularization_and_effective/</guid>
      <pubDate>Thu, 11 Jan 2024 17:57:43 GMT</pubDate>
    </item>
    <item>
      <title>《Marvin Minsky’s Vision of the Future》，Bernstein 1981（明斯基的研究生涯，包括神经网络 SNARC 小鼠）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1946okz/marvin_minskys_vision_of_the_future_bernstein/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1946okz/marvin_minskys_vision_of_the_future_bernstein/</guid>
      <pubDate>Thu, 11 Jan 2024 17:08:38 GMT</pubDate>
    </item>
    <item>
      <title>“计算机双陆棋”，Hans J. Berliner 1980（“BKG 9.8 是第一个在棋盘或纸牌游戏中击败世界冠军的计算机程序”）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1945ymh/computer_backgammon_hans_j_berliner_1980_bkg_98/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1945ymh/computer_backgammon_hans_j_berliner_1980_bkg_98/</guid>
      <pubDate>Thu, 11 Jan 2024 16:39:07 GMT</pubDate>
    </item>
    <item>
      <title>完成 RL 博士学位后我可以在哪里工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194406q/where_can_i_work_after_finishing_a_phd_in_rl/</link>
      <description><![CDATA[ 由   提交 /u/Trevorego   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194406q/where_can_i_work_after_finishing_a_phd_in_rl/</guid>
      <pubDate>Thu, 11 Jan 2024 15:14:40 GMT</pubDate>
    </item>
    <item>
      <title>复杂系统中的涌现和因果关系：因果涌现和相关定量研究的调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193v7hf/emergence_and_causality_in_complex_systems_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.16815 摘要：  涌现和因果关系是理解复杂系统的两个基本概念。它们是相互关联的。一方面，涌现是指宏观性质不能仅仅归因于个体性质的原因的现象。另一方面，因果关系可以表现出出现，这意味着随着我们提高抽象水平，新的因果律可能会出现。因果涌现理论旨在弥合这两个概念，甚至采用因果关系措施来量化涌现。本文全面回顾了因果涌现的定量理论和应用的最新进展。解决了两个关键问题：量化因果出现并在数据中识别它。解决后者需要使用机器学习技术，从而在因果出现和人工智能之间建立联系。我们强调，用于识别因果出现的架构由因果表示学习、因果模型抽象和基于世界模型的强化学习共享。因此，这些领域中任何一个领域的进步都可以使其他领域受益。评论的最后部分还讨论了潜在的应用和未来的前景。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193v7hf/emergence_and_causality_in_complex_systems_a/</guid>
      <pubDate>Thu, 11 Jan 2024 06:37:04 GMT</pubDate>
    </item>
    <item>
      <title>二维动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193ud0k/twodimensional_action_space/</link>
      <description><![CDATA[如何处理二维的动作空间，我说的不是二维的向量，而是二维的张量。 例如，一个 lstm actor，其输入是（timesteps，features=256），输出另一个特征序列（timesteps，features=4096）。特征数量如此之大，是否可以将输出展平以将其视为一维动作空间？ 为了避免混淆，上面的时间步长不是环境的时间步长，而是环境的时间步长输入数据（状态）。   由   提交 /u/FancyUsual7476   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193ud0k/twodimensional_action_space/</guid>
      <pubDate>Thu, 11 Jan 2024 05:47:15 GMT</pubDate>
    </item>
    <item>
      <title>“图式学习和重新绑定作为上下文学习和涌现的机制”，Swaminathan 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193m3bx/schemalearning_and_rebinding_as_mechanisms_of/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193m3bx/schemalearning_and_rebinding_as_mechanisms_of/</guid>
      <pubDate>Wed, 10 Jan 2024 23:04:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO 代理无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193kw1i/ppo_agent_fails_to_learn/</link>
      <description><![CDATA[我正在做一个基于PPO算法的路径规划项目。在我的实验中，有一个16*16的网格地图，其中有几个障碍物区域作为环境，目的是训练代理直到找到到达目标的路径。这是我的模型的详细主要信息：  我部署了一个 A2C 机制，其中 Actor 和 Critic 网络都是四层结构，其中 Actor 具有 2*512*512*8 和评论家 2*512*512*1。二维输入是当前位置作为状态，八维向量输出是八个动作朝相应方向移动的概率。  超参数设置如下： 学习率：actor：3e-04，cirtic：4e-04 两个网络的最大等级：1.5&lt; /li&gt; 策略剪辑：epsilon = 0.3 折扣：gamma = 0.99 GAE 参数：lambda = 0.95 熵正则化参数：0.005&lt; /li&gt; Batchsize = 512 奖励函数设置规则如下：  当当前节点到目标的曼哈顿距离为时，获得正奖励（+1.0）较小，否则为负值（-1.0）。  当智能体移动到障碍物时，奖励为-10.0。 如果智能体从与障碍物相邻的节点离开，则奖励为+10.0。  &gt;奖励重塑：如果智能体不断朝着正确的方向前进，奖励将会额外增加。    实验结果：根据我对两个网络损失值的追踪结果，我可以非常确定该模型是迭代期间收敛。然而，收敛效果并不如我预期。 MSE 的批评者血统损失从一开始的 100, 000.00 以上开始，最终停留在 4,000.00 左右，然后开始波动。另一方面，演员损失可以从超过 100.00 开始减少，但在 60.00 左右停止减少。简而言之，培训有效果，但效果不佳。  此外，经过 5000 次训练后，回报率仍然没有明显改善。在相当大的概率下，智能体仍然会选择远离目标或移动到障碍物的动作。每集的平均回报率一直保持在 -2.0 左右，而且从未有任何改善。   ​ 在这里，我真的希望有人能帮助我解决这个问题。我将非常感谢您的帮助和建议。非常感谢！  ​ ​   由   提交/u/Tight_Boysenberry692   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193kw1i/ppo_agent_fails_to_learn/</guid>
      <pubDate>Wed, 10 Jan 2024 22:15:32 GMT</pubDate>
    </item>
    <item>
      <title>软演员评论家：Huber 与 MSE 损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1939z6c/soft_actorcritic_huber_vs_mse_loss/</link>
      <description><![CDATA[Huber（或平滑 L1）损失似乎常用于 DQN 算法中以提高稳定性，这对我来说很有意义，因为目标 Q 网络是最初可能是垃圾。然而，我见过的大多数 SAC 实现都使用 MSE 作为批评者，而且我还无法仅通过谷歌搜索找到理由。 是否有任何直觉为什么 MSE 可能比 Huber 工作得更好专门针对SAC的损失？它可能与问题相关吗？人们是否因为 MSE 效果足够好而懒得尝试 Huber 损失？   由   提交 /u/DoNotAbsquatulate   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1939z6c/soft_actorcritic_huber_vs_mse_loss/</guid>
      <pubDate>Wed, 10 Jan 2024 14:49:28 GMT</pubDate>
    </item>
    <item>
      <title>通过体操游戏训练 CNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1939cpb/train_cnn_with_gymnasium_games/</link>
      <description><![CDATA[大家好， 我是大学讲师，想向我的学生展示 CNN 和 Deep Q 的结合-学习。他们应该接受一项任务，让代理解决一个简单的游戏（简单是因为他们应该能够使用“普通”笔记本来解决它）。我刚刚看了gymnasium的文档，但没有找到可以将图像作为状态传递的游戏。图书馆里没有这样的东西吗？ 提前感谢大家的帮助:)   由   提交/u/MarcoX0395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1939cpb/train_cnn_with_gymnasium_games/</guid>
      <pubDate>Wed, 10 Jan 2024 14:20:53 GMT</pubDate>
    </item>
    <item>
      <title>和谐世界模型：提高基于模型的强化学习的样本效率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1931dhs/harmony_world_models_boosting_sample_efficiency/</link>
      <description><![CDATA[OpenReview: https: //openreview.net/forum?id=RN7RzMxwjC arXiv：https ://arxiv.org/abs/2310.00344 摘要：  基于模型的强化学习（MBRL）持有通过利用世界模型来实现样本高效学习，该模型对环境如何工作进行建模，通常包含两个任务的组件：观察建模和奖励建模。在本文中，通过专门的实证研究，我们更深入地了解了每个任务在世界模型中所扮演的角色，并通过协调观察和奖励模型之间的干扰，揭示了更高效的 MBRL 被忽视的潜力。我们的主要见解是，虽然显式 MBRL 的流行方法试图通过观测模型来恢复环境的丰富细节，但由于环境的复杂性和有限的模型容量，这是很困难的。另一方面，奖励模型虽然在隐式 MBRL 中占主导地位并且擅长学习以任务为中心的动态，但在没有更丰富的学习信号的情况下不足以进行样本有效的学习。利用这些见解和发现，我们提出了一种简单而有效的方法，Harmony World Models (HarmonyWM)，它引入了一个轻量级协调器来维持两个任务之间的动态平衡在世界模型学习中。我们对三个视觉控制域的实验表明，配备 HarmonyWM 的基本 MBRL 方法获得了 10%-55% 的绝对性能提升。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1931dhs/harmony_world_models_boosting_sample_efficiency/</guid>
      <pubDate>Wed, 10 Jan 2024 06:06:47 GMT</pubDate>
    </item>
    </channel>
</rss>