<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 13 Feb 2024 18:17:08 GMT</lastBuildDate>
    <item>
      <title>quilterai 筹集 1000 万美元，构建 RL 支持的硬件编译器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</link>
      <description><![CDATA[      强化学习最令人兴奋的行业应用之一即将规模化！    由   提交 /u/mccrearyd   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</guid>
      <pubDate>Tue, 13 Feb 2024 18:03:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在整个阅读过程中应用萨顿和巴托的概念</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</link>
      <description><![CDATA[目前正在通过阅读这本书自学强化学习萨顿巴托：从头到尾介绍强化学习。我已经读了 4 章半了，感觉被它所强加的所有理论淹没了，有没有关于如何应用这些概念的随附材料或指南，以便我可以放慢一点速度，并真正内化这些概念我正在阅读的内容？理想情况下，这些将应用于编程环境。 如果有人有时间提供一些建议，我将非常感激！  &amp;# 32；由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</guid>
      <pubDate>Tue, 13 Feb 2024 18:02:05 GMT</pubDate>
    </item>
    <item>
      <title>开始 RL 是否需要 ML/DL 背景？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apgn0t/is_a_background_in_mldl_required_to_start_in_rl/</link>
      <description><![CDATA[我现在正在学习 ML，是为了深入研究 RL，我是在浪费时间吗？为了更深入地了解 RL，我可以学习什么作为先决条件？   由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apgn0t/is_a_background_in_mldl_required_to_start_in_rl/</guid>
      <pubDate>Tue, 13 Feb 2024 01:06:48 GMT</pubDate>
    </item>
    <item>
      <title>稳定的基线会减慢。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apf6lx/stable_baselines_slows_down/</link>
      <description><![CDATA[我正在制作一款物理摆游戏，但在游戏迭代 50 次左右后，稳定基线崩溃了。如果我继续给它 0 扭矩，电机会在 1000 秒的迭代中正常响应位置，因此我将其范围缩小到问题所在的稳定基线。   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apf6lx/stable_baselines_slows_down/</guid>
      <pubDate>Tue, 13 Feb 2024 00:00:18 GMT</pubDate>
    </item>
    <item>
      <title>机器人模型文件根本不存在</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apdfi4/robot_model_files_are_just_nonexistent/</link>
      <description><![CDATA[我从事机器人技术已有 7 年，从事强化学习已有 4 年。我是一名研究员，我遇到的最大问题之一是在线查找 URDF 和 MCJF。我的很多同行也面临着同样的问题。还有其他人有这个问题吗？如果是的话，我建立了一个在线免费存储库，任何人都可以上传文件，我们可以共享它们。我还计划添加一种在线模拟器，您可以在其中模拟这些机器人。加入我的discord服务器以获取更多信息，我将在那里发布测试版的链接：https://discord.gg/SJy2jV7n&lt; /p&gt;   由   提交/u/elonmusk-A12   /u/elonmusk-A12 reddit.com/r/reinforcementlearning/comments/1apdfi4/robot_model_files_are_just_nonexistent/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apdfi4/robot_model_files_are_just_nonexistent/</guid>
      <pubDate>Mon, 12 Feb 2024 22:44:49 GMT</pubDate>
    </item>
    <item>
      <title>选择本科论文项目有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apbxm6/any_tips_on_choosing_an_undergrad_thesis_project/</link>
      <description><![CDATA[ 由   提交 /u/BadMeditator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apbxm6/any_tips_on_choosing_an_undergrad_thesis_project/</guid>
      <pubDate>Mon, 12 Feb 2024 21:43:20 GMT</pubDate>
    </item>
    <item>
      <title>播客：法学硕士时代的强化学习（Kamyar Azizzadenesheli）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apa7qz/podcast_reinforcement_learning_in_the_age_of_llms/</link>
      <description><![CDATA[       由   提交/u/Smallpaul   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apa7qz/podcast_reinforcement_learning_in_the_age_of_llms/</guid>
      <pubDate>Mon, 12 Feb 2024 20:34:47 GMT</pubDate>
    </item>
    <item>
      <title>根据两个子奖励计算奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ap8t3m/reward_calculation_based_on_two_sub_rewards/</link>
      <description><![CDATA[大家好， 我想为连续动作设计一个奖励函数。我有 10 个动作，其中 9 个是相互依赖的，一个是独立的。所以，我的想法是计算前 9 个动作的一个奖励，并分别计算最后一个动作的其他奖励。然后我计算提供给代理的总奖励（PPO 政策）。奖励范围为-1...1。  tota_reward = (reward1 +reward2) / 2 这种方法有意义吗？ 我的 PPO 设置，我的剧集长度约为 165 步： &gt; PPO(“MlpPolicy”, self.env, n_steps=512, n_epochs=10, verbose=0, create_eval_env=False, batch_size =128、gae_lambda=0.95、ent_coef=0.001、vf_coef=0.5、gamma=0.99、learning_rate=0.0003、clip_range=0.2、use_sde=False、tensorboard_log=&#39;./tensorboard&#39;)   由   提交/u/Inevitable_Engineer5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ap8t3m/reward_calculation_based_on_two_sub_rewards/</guid>
      <pubDate>Mon, 12 Feb 2024 19:40:11 GMT</pubDate>
    </item>
    <item>
      <title>寻找适合演员-评论家模型的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ap7qhb/finding_resources_for_fitting_actorcritic_models/</link>
      <description><![CDATA[我正在将强化学习的计算模型与行为数据进行拟合，并希望获得一些帮助来寻找资源来帮助完成此过程。  行为任务非常简单（两个选项之间的选择和响应在概率上得到加强）。我在拟合各种 Q 学习模型方面取得了一些成功，但想探索演员批评家框架。一旦指定了基本框架，就可以通过最大化对数似然来将模型拟合到行为数据。我希望实现的一些示例可以在这里找到： https ://www.ncbi.nlm.nih.gov/pmc/articles/PMC9272137/ https://pubmed.ncbi.nlm.nih.gov/27986430/ 我正在寻找代码结构的示例，以便我可以确定我已经正确实现我的框架。然而，在搜索资源（教程、github 等）时，我发现的所有内容都与深度 RL 相关。有谁知道我如何改进我的搜索或知道任何有用的东西。 Python 是理想的选择，但任何东西都会有帮助。 非常感谢！ &lt; p&gt;​   由   提交/u/bigfuds  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ap7qhb/finding_resources_for_fitting_actorcritic_models/</guid>
      <pubDate>Mon, 12 Feb 2024 18:57:40 GMT</pubDate>
    </item>
    <item>
      <title>什么是多臂强盗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ap1jno/what_is_multiarmed_bandits/</link>
      <description><![CDATA[在本教程中查看什么是 MAB https:// youtu.be/sxuvJVk1L4M?si=DgwTtE_VfXuCK7dE   由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ap1jno/what_is_multiarmed_bandits/</guid>
      <pubDate>Mon, 12 Feb 2024 14:42:23 GMT</pubDate>
    </item>
    <item>
      <title>如何为我的 RL 创建自己的自定义 API？（针对游戏）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aovj9j/how_to_create_my_own_custom_api_for_my_rlfor_games/</link>
      <description><![CDATA[嘿大家 所以在我询问允许我们直接通过 python 与其交互的游戏之前，但我没有找到任何吸引我眼球的游戏 （对于不知道的人，你可以阅读 /u/Mr_Lucifer_666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aovj9j/how_to_create_my_own_custom_api_for_my_rlfor_games/</guid>
      <pubDate>Mon, 12 Feb 2024 08:43:04 GMT</pubDate>
    </item>
    <item>
      <title>RL 在端到端学习中应用于机器人技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aoam6n/rl_applied_to_robotics_in_end2end_learning/</link>
      <description><![CDATA[我想知道以端到端的方式训练强化学习策略以使用图像进行抓取或操作任务的最佳方式是什么。我可以想三个办法。   从头开始使用 CNN + RL：不实用，因为训练 CNN 可能很困难。 使用预训练网络（例如 resnet）作为特征提取器。冻结 CNN 的权重，仅训练策略网络。 使用预训练网络，但不冻结 CNN 的权重。在学习策略的同时微调整个网络。  有人可以帮助我了解其中哪一种在实践中有效吗？这些技术有哪些缺点？有没有我可以轻松使用的 git 存储库，其中包括模拟器和易于使用/理解的代码？   由   提交 /u/EnthuMinInvert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aoam6n/rl_applied_to_robotics_in_end2end_learning/</guid>
      <pubDate>Sun, 11 Feb 2024 15:34:08 GMT</pubDate>
    </item>
    <item>
      <title>[2402.05290] Transformer 世界模型是否提供更好的策略梯度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ao5u62/240205290_do_transformer_world_models_give_better/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ao5u62/240205290_do_transformer_world_models_give_better/</guid>
      <pubDate>Sun, 11 Feb 2024 11:22:44 GMT</pubDate>
    </item>
    <item>
      <title>如今强化学习有多大前景？让我们讨论一下对科技和社会的未来影响</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ao4ie6/how_promising_is_reinforcement_learning_today/</link>
      <description><![CDATA[大家好！我最近一直在深入研究强化学习 (RL) 的世界，我对它重塑技术乃至社会的潜力非常着迷。从掌握复杂的游戏到驾驶下一波自动驾驶汽车，强化学习似乎处于人工智能进军新领域的最前沿。 但我很想听听这个社区的声音：您认为前景如何RL现在还好吗？哪些强化学习领域最热门的话题和突破引起了您的关注？更重要的是，您认为强化学习在未来会在哪些方面产生最重大的影响？   由   提交 /u/Goddespeed   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ao4ie6/how_promising_is_reinforcement_learning_today/</guid>
      <pubDate>Sun, 11 Feb 2024 09:52:00 GMT</pubDate>
    </item>
    <item>
      <title>AI 在老派 RuneScape 中学习 PvP（强化学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1anv4gf/ai_learns_pvp_in_old_school_runescape/</link>
      <description><![CDATA[大家好，过去一年我一直在开发一个项目，利用强化学习在 Old School RuneScape 中学习 PvP。我终于达到了对结果感到满意的程度，因此我开源了该项目的（大部分），并发布了一个 YouTube 视频，从高层次上介绍了它的工作原理。 ​  GitHub：https://github.com/Naton1 /osrs-pvp-reinforcement-learning Youtube：https://youtu.be/jArLZ8nC5Nw   ​ 该视频非常高级，可以使其易于访问，但代码很全面，并且有很多很酷的内容，包括：  完整的 PPO 实现 自我对弈策略，包括优先的过去自我对弈 具有动作屏蔽的自回归和参数化多离散动作 &lt; li&gt;评论家网络的完整游戏状态可见性（可以看到完整的玩家和对手信息） 可定制的模型架构 奖励和观察标准化 使用新颖性奖励运行观察统计 AsyncIO 矢量化环境 使用 Ray 分发 rollout 集合  ​ 太多了列在这里，所以如果你好奇的话请查看代码！ ​ 对于那些可以理解的担忧的人，请注意，这里没有发布任何软件可以允许人们在真实的游戏中使用这些模型。开源代码纯粹用于模拟训练和评估。   由   提交/u/Naton1-  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1anv4gf/ai_learns_pvp_in_old_school_runescape/</guid>
      <pubDate>Sun, 11 Feb 2024 00:41:09 GMT</pubDate>
    </item>
    </channel>
</rss>