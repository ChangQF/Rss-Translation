<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 11 Mar 2024 15:14:24 GMT</lastBuildDate>
    <item>
      <title>Tensorboard：“当前数据集没有活动的仪表板。”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bc3rdq/tensorboard_no_dashboards_are_active_for_the/</link>
      <description><![CDATA[大家好， 几天前我完成了我的第一个 RL 代理的设置，并直接开始训练它。我将日志文件保存到我的 anaconda 环境文件夹中，并使用 logdir 打开它们以查看张量板。一开始我遇到了问题，没有显示任何内容，但经过两个小时的重新启动和耐心之后，它突然起作用了... 现在，我将文件移动到云存储中，现在我想从中打开我的张量板会话。由于我移动了文件，因此出现了与以前相同的错误。没有显示任何数据，并显示消息“当前数据集没有活动的仪表板。” 如果你们中的任何人之前遇到过类似的问题，我将不胜感激。非常感谢！   由   提交 /u/QuiGon-GinTonic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bc3rdq/tensorboard_no_dashboards_are_active_for_the/</guid>
      <pubDate>Mon, 11 Mar 2024 13:55:19 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bc0xf8/graph_neural_network/</link>
      <description><![CDATA[您好， 我正在研究灵活作业车间的生产调度（即，任务是安排操作，以便每个操作可以在许多机器上处理）。我通过将状态建模为基于动态图的结构来利用图神经网络（节点数量在每个决策时间根据可用作业和空闲机器而变化）。 我有三个主要问题： A.我应该选择哪个选项？ 1- 将状态建模为析取图（在许多学术论文中使用） 2- 将状态建模为图模型 (nx.DiGraph() ）并将状态作为 Data(x=, edge_index=) 馈送到 GNN B.哪一个更好？ 1- 训练 GAT 进行特征嵌入提取，然后训练 DRL 代理进行动作选择 2- 将 GAT 集成到 DRL 代理中，直接根据状态选择动作表示为数据，然后训练整个网络 C.我在图中有三个节点类别，如果输入表示为数据，GNN是否应该分别处理图中的三个节点类别中的每一个（这样我每次都有一个机器节点（多代理设置），几个作业节点, ...)?   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bc0xf8/graph_neural_network/</guid>
      <pubDate>Mon, 11 Mar 2024 11:25:49 GMT</pubDate>
    </item>
    <item>
      <title>监控强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbzkai/monitoring_rl_agents/</link>
      <description><![CDATA[您好，我有两个一般性问题： 在训练和探索时监控 RL 代理性能的方法是什么？我知道奖励函数行为反映了 RL 的性能，但是，我们是否可以跟踪任何其他标准来了解代理的学习情况？ 最先进的安全方法是什么？除了用例特定方法之外的探索技术？   由   提交/u/alysavalan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbzkai/monitoring_rl_agents/</guid>
      <pubDate>Mon, 11 Mar 2024 09:56:51 GMT</pubDate>
    </item>
    <item>
      <title>监控 RL 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbzgth/monitoring_the_rl_agent/</link>
      <description><![CDATA[您好，我有两个一般性问题： 在训练和探索时监控 RL 代理性能的方法是什么？我知道奖励函数行为反映了 RL 的性能，但是，我们是否可以跟踪任何其他标准来了解代理的学习情况？ 最先进的安全方法是什么？除了用例特定方法之外的探索技术？   由   提交/u/raminhashemi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbzgth/monitoring_the_rl_agent/</guid>
      <pubDate>Mon, 11 Mar 2024 09:49:31 GMT</pubDate>
    </item>
    <item>
      <title>使用新地图进行训练时，DQN 模型会失去之前解决地图问题的能力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbh5sv/dqn_model_losing_previous_hability_to_solve_a_map/</link>
      <description><![CDATA[我正在训练 DQN 模型来解决游戏问题。输入是地图。如果我设法使用特定地图获得良好的结果，保存模型，然后使用另一张地图再次训练它（保留权重），它会失去解决前一张地图的能力，因为它需要探索和学习新地图吗？还是通过在每次迭代时探索所有地图来一次训练所有地图更好（这在计算上会相当昂贵）？ 在我的游戏环境中，DQN 算法需要大量探索，因此 epsilon 衰减非常慢，初始 epsilon 值为 1。   由   提交/u/libichi  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbh5sv/dqn_model_losing_previous_hability_to_solve_a_map/</guid>
      <pubDate>Sun, 10 Mar 2024 18:26:05 GMT</pubDate>
    </item>
    <item>
      <title>对于决策转换器和强化学习的未来持什么立场？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbavui/what_is_the_stance_on_decision_transformers_and/</link>
      <description><![CDATA[嗨， 这些天我正在研究决策转换器。  有争议的是，在试图找到最重要的论文时，我注意到强化学习领域似乎没有发生太多事情。我注意到研究的重点是优化 Transformer 和训练巨大的语言和视觉模型（被视为监督模型）。这是 RL 中的新大事吗？  强化学习的最新趋势是什么？  ​   由   提交/u/__Julia  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbavui/what_is_the_stance_on_decision_transformers_and/</guid>
      <pubDate>Sun, 10 Mar 2024 13:54:56 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    <item>
      <title>QLearning推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1basms8/qlearning_recommender_system/</link>
      <description><![CDATA[大家好，我是强化学习新手，我正在尝试在 Movielens ML-100k 数据集上使用 QLearning 制作强化学习推荐系统，我将状态定义为用户，将动作定义为电影，我正在尝试预测评分。我将评分文件转换为用户项矩阵，并用 0 填充 NA 值。 对于我正在检查的奖励函数如果评级可用并且我返回实际评级-预测评级，否则我只返回 0。 对于我使用 epsilon 贪婪的策略。 我的问题是我没有得到好的结果，我猜我的奖励功能需要工作。如果有人想看一下我的代码，那就是 python。谢谢 编辑：这是我的 Kaggle 笔记本 https://www.kaggle.com/code/asribachir/reinforcement-learning-ml100k    由   提交 /u/Fredybec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1basms8/qlearning_recommender_system/</guid>
      <pubDate>Sat, 09 Mar 2024 21:23:51 GMT</pubDate>
    </item>
    <item>
      <title>模型性能评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ba8tq6/model_performance_evaluation/</link>
      <description><![CDATA[我在做什么： 我正在制作一个自定义Open AI Gym 中的 Boid 植绒环境具有稳定的基线 3. 工作原理：  我传递了一个 boids 的位置文件。 对其进行 3000 个时间步长的模型测试，并输出每个情节的奖励，即位置文件  训练初始位置与测试不同. 我关心的是我的模型的性能。当从不同的初始位置陈述时，它会输出类似的奖励，并且机器人按照预期移动，我还生成了一个移动视频文件。 看似正确工作的模型的输出 ​  奖励函数 defcalculate_combined_reward(self,agent,neighbor_positions):total_reward=0out_of_flock=False if(len(neighbor_positions)&gt;0):forneighbor_positionsinneighbor_positions : 距离 = np.linalg.norm(agent.position - neighbour_position) if (距离  问题：但是，当我在不更改任何内容的情况下重新训练几次并进行测试时，只是为了保持一致性，它的表现不佳，我的剧集奖励大多是负面的。虽然我什么也没改变。幸运的是，我保存了具有最佳性能的模型。 关注：是我的模型，我训练并输出了正确的性能，我附上了照片，一个训练好的模型，侥幸或过度拟合？  ​   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ba8tq6/model_performance_evaluation/</guid>
      <pubDate>Sat, 09 Mar 2024 04:18:30 GMT</pubDate>
    </item>
    <item>
      <title>大家好！我将从事旨在使用 RL 算法稳定无人机的模拟项目，我想过使用 matlab 和 simulink，但我找不到要测试的模型，任何人都可以指导我吗？谢谢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9zscg/hi_everyone_im_gonna_work_on_simulation_project/</link>
      <description><![CDATA[ 由   提交/u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9zscg/hi_everyone_im_gonna_work_on_simulation_project/</guid>
      <pubDate>Fri, 08 Mar 2024 21:26:25 GMT</pubDate>
    </item>
    <item>
      <title>在覆盖重播缓冲区时是否存在优先考虑陈旧内存的范例？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9qxhk/is_there_a_paradigm_on_which_stale_memories_to/</link>
      <description><![CDATA[我开始思考，因为我发誓范例总是循环先进先出，但对于我正在研究的后进先出的实现似乎工作得更好。  另外，在优先体验重放的情况下，为什么不丢弃最低的 TD 内存呢？  或者当缓冲区满时随机更换内存？    由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9qxhk/is_there_a_paradigm_on_which_stale_memories_to/</guid>
      <pubDate>Fri, 08 Mar 2024 15:36:40 GMT</pubDate>
    </item>
    <item>
      <title>🚀 DIAMBRA 与 Hugging Face 合作推动强化学习研究和采用！ 🚀</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9nw82/diambra_teams_up_with_hugging_face_to_push/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9nw82/diambra_teams_up_with_hugging_face_to_push/</guid>
      <pubDate>Fri, 08 Mar 2024 13:27:03 GMT</pubDate>
    </item>
    <item>
      <title>问题：关于单环境与多环境 RL 训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9le2c/question_regarding_single_environment_vs_multi/</link>
      <description><![CDATA[大家好， 我正在研究机械臂模拟，以对机器人执行高级控制以抓取物体。我正在使用 Unity 中的 ML Agent 作为环境平台。同时，使用 PPO 训练机器人，我能够在大约 8 小时的训练时间内成功执行它。为了减少时间，我尝试增加在同一环境中工作的代理数量（有一个内置的训练区域复制器，它只是使用代理复制整个机器人单元）。根据mlagents源代码，多个代理应该只是加速轨迹收集（因为有许多代理根据相同的策略尝试针对不同随机情况的操作，所以更新缓冲区应该更快地填满）。但是，由于某种原因，我的策略无法正确训练。它在零回报处持平（从 - 1 开始改善，但稳定在 0 左右。+1 是一个情节的最大回报）。增加代理数量时是否需要进行一些特定的更改？增加环境数量时需要记住的其他一些事项。欢迎任何意见或建议。提前致谢。    由   提交 /u/Flaky-Drag-31   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9le2c/question_regarding_single_environment_vs_multi/</guid>
      <pubDate>Fri, 08 Mar 2024 10:56:15 GMT</pubDate>
    </item>
    <item>
      <title>使用 ray 实现自定义 RL Agent</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b9kqh8/implement_custom_rl_agent_with_ray/</link>
      <description><![CDATA[嘿，我，目前正在尝试使用 ray (tune.trainable) 实现我自己的 RL 代理，并使用基于群体的训练调度程序来训练它们。根据文档，有 6 种方法可以实现： https://docs.ray.io/en /latest/tune/api/trainable.html 出现了许多疑问和问题。如果有人能为我回答，我将不胜感激。 我的问题是，每次在训练期间加载检查点时都会调用 setup() 方法，而不仅仅是在开始之后（我设置了reuse_agent 标志）在 Tuner 配置中，并且 reset?config() 方法返回 true）。此外，我不确定在 cleanup() 方法中要做什么。 step() 方法应该在环境中执行一步，还是在纪元上执行一步？并且重播缓冲区应该在不同的代理之间共享，或者每个代理应该有自己的重播缓冲区。 如果有人已经实现了自定义光线可训练，我将不胜感激 github 链接:) ​   由   提交 /u/ChiefAlu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b9kqh8/implement_custom_rl_agent_with_ray/</guid>
      <pubDate>Fri, 08 Mar 2024 10:09:41 GMT</pubDate>
    </item>
    <item>
      <title>修补强化学习所需的知识水平</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b90c96/level_of_knowledge_needed_to_tinker_with_rl/</link>
      <description><![CDATA[我是一名普通的 3D 打印机/机器人修补匠，从我所看到的 RL 的外观来看，它不仅有趣而且有趣。但当我浏览这里的帖子时，它们都写得超出了我的理解范围。当我在网上查看“如何进入……”的内容时，里面充满了让我延伸很远的概念。对我来说，显然我不知道这一切的背景是什么。这是您非常需要学位或大学水平的教育和理解才能开始的事情吗？我发现很多涉及强化学习的例子都是人们的课程项目。我相信你可以做你想做的事，但我希望知道我可能需要付出多大的努力，以及这对我来说是否值得。你的背景是什么？   由   提交 /u/UltimateThrowawayNam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b90c96/level_of_knowledge_needed_to_tinker_with_rl/</guid>
      <pubDate>Thu, 07 Mar 2024 17:30:27 GMT</pubDate>
    </item>
    </channel>
</rss>