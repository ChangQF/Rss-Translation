<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 07 Dec 2024 03:32:07 GMT</lastBuildDate>
    <item>
      <title>PyBullet 中的积木塔正在倒塌</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8bnym/blocks_tower_is_collapsing_in_pybullet/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8bnym/blocks_tower_is_collapsing_in_pybullet/</guid>
      <pubDate>Fri, 06 Dec 2024 21:02:35 GMT</pubDate>
    </item>
    <item>
      <title>我是不是别无选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h87hx1/am_i_left_with_no_other_option/</link>
      <description><![CDATA[ROS 太烂了 我是一名计算机科学专业的学生，​​之前做过一些 ROS，我发现为我的 RL 项目设置（Gazebo）非常烦人。我没有 FLOSS 选项了吗？ 有没有办法可以在没有 ROS 的情况下模拟环境？    提交人    /u/iconic_sentine_001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h87hx1/am_i_left_with_no_other_option/</guid>
      <pubDate>Fri, 06 Dec 2024 18:02:21 GMT</pubDate>
    </item>
    <item>
      <title>“奖励基础：一种自适应获取多种奖励类型的简单机制”，Millidge 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h82gh2/reward_bases_a_simple_mechanism_for_adaptive/</link>
      <description><![CDATA[   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h82gh2/reward_bases_a_simple_mechanism_for_adaptive/</guid>
      <pubDate>Fri, 06 Dec 2024 14:23:34 GMT</pubDate>
    </item>
    <item>
      <title>PPO 代理完成目标，但解释方差变得更糟？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7wwcq/ppo_agent_completing_objective_but_explained/</link>
      <description><![CDATA[      我目前正在训练一个 RecurrentPPO 代理完成一个简单的交易任务。 基本上每一步它都可以决定是做空还是做多。没有花哨的持有。 奖励是每个时间步的风险调整回报。 输入是来自几个基本特征的 7 个标准化 pca 特征。 代理似乎理解了这项任务，并且执行得相当好。 然而，虽然它在解决任务方面的表现在逐渐提高，但解释的方差基本上越来越差，或者在 0 附近波动。 https://preview.redd.it/d8tt468ut65e1.png?width=2528&amp;format=png&amp;auto=webp&amp;s=6695e68fc80b2149fd8f47d7ce31eba0c7428cbe 这是 sb3 中的当前策略： policy_kwargs = dict( net_arch=dict(pi=[256, 256], vf=[256, 256]),activation_fn=torch.nn.Tanh,ortho_init=True,enable_critic_lstm=False,lstm_hidden_​​size=28,optimizer_class=AdamW,share_features_extractor=True,features_extractor_class=IdentityFeatureExtractor,）模型=RecurrentPPO（“MlpLstmPolicy”，env，verbose=0，learning_rate=0.00001，n_steps=400，batch_size=100，clip_range=0.2，clip_range_vf=0.2，ent_coef=0.1，vf_coef=0.1，gamma=0.99，gae_lambda=0.95，种子=42，policy_kwargs=policy_kwargs，tensorboard_log=log_dir， max_grad_norm=0.5, n_epochs=4, stats_window_size=2, normalize_advantage=True)  结果： RecurrentActorCriticPolicy( (features_extractor): IdentityFeatureExtractor() (pi_features_extractor): IdentityFeatureExtractor() (vf_features_extractor): IdentityFeatureExtractor() (mlp_extractor): MlpExtractor( (policy_net): Sequential( (0): Linear(in_features=28, out_features=256, bias=True) (1): Tanh() (2): Linear(in_features=256, out_features=256, bias=True) (3): Tanh() ) (value_net): Sequential( (0): Linear(in_features=28, out_features=256, bias=True) (1): Tanh() (2): Linear(in_features=256, out_features=256, bias=True) (3): Tanh() ) ) (action_net): Linear(in_features=256, out_features=2, bias=True) (value_net): Linear(in_features=256, out_features=1, bias=True) (lstm_actor): LSTM(6, 28) (critic): Linear(in_features=6, out_features=28, bias=True) )  我是否遗漏了某些关键信息，或者如果代理实现了预期目标，我是否不应该关心解释方差？    提交人    /u/Educational_Study553   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7wwcq/ppo_agent_completing_objective_but_explained/</guid>
      <pubDate>Fri, 06 Dec 2024 08:32:43 GMT</pubDate>
    </item>
    <item>
      <title>利用强化学习实现火箭着陆：Unity 中的 2D 和 3D 模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7qbay/landing_rockets_with_reinforcement_learning_2d/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7qbay/landing_rockets_with_reinforcement_learning_2d/</guid>
      <pubDate>Fri, 06 Dec 2024 02:00:22 GMT</pubDate>
    </item>
    <item>
      <title>【竞赛】机器人跑者联盟：实时协调数千个机器人！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7m54r/competition_the_league_of_robot_runners/</link>
      <description><![CDATA[你好，机器和强化学习者！ 这是一项公告，呼吁参加 2024 年机器人跑步者联盟，这是一项多季节 🚀 竞赛和研究计划 🚀 ，旨在解决工业优化中最具挑战性的问题之一：多机器人路径规划（有时也称为多智能体路径查找）。 这项比赛的灵感来自依赖移动机器人技术的当前和新兴应用 🦾🤖。例如，亚马逊的自动化仓库，成千上万的机器人协同工作，确保包裹安全高效地递送🧸📦🚚❤️。 现在已进入第二季，比赛主要关注两个核心挑战：  任务调度，由您决定哪个机器人执行哪项任务。 路径规划，由您协调机器人，使它们尽快到达目的地并且不会发生碰撞。  这两种设置都是在线和实时的，这意味着在您计算时时钟会滴答作响。在时间耗尽之前，尽可能多地完成任务！ 我们认为 🧠 基于学习的算法 🧠 对于解决这些类型的问题有几个优势：  大规模计算机器人运动需要极快的策略，而学习非常适合这种情况 基于学习的策略也很容易更新，这对于处理动态拥塞非常重要 总是有更多任务，这意味着没有固定的全局最优  参加本次比赛是向全球学术界和行业专家展示您的 💡 ML/RL 技能 💡 的好方法。比赛结束后，问题实例和提交内容将开源，这会增加您的知名度，降低其他人的进入门槛，并帮助社区成长和学习👩‍🏫🤔📚🎓。 对于三个不同类别的🌟出色表现🌟，我们将提供10,000 美元的奖金池。我们还以 1,000 美元 AWS 积分的形式提供培训奖励，以帮助参与者降低线下培训成本😻。 提交内容随时开放，评估结果可立即在我们的实时排行榜上查看。比赛将持续到📅2025 年 2 月 16 日📅，结果将于 2025 年 3 月公布。 入门很容易！我们为您提供模拟器和代码线束（“入门套件”）、许多示例问题以及用于探索生成的解决方案的可视化工具。您还可以访问去年表现最佳的规划器作为基准。请访问我们的网站了解所有详细信息（www.leagueofrobotrunners.org）或在此处发帖询问！    提交人    /u/robotrunnersofficial   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7m54r/competition_the_league_of_robot_runners/</guid>
      <pubDate>Thu, 05 Dec 2024 22:44:24 GMT</pubDate>
    </item>
    <item>
      <title>解释法学硕士强化学习基础知识的技术指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7kixk/technical_guide_explaining_the_fundamentals_of/</link>
      <description><![CDATA[       由    /u/Legaltech_buff  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7kixk/technical_guide_explaining_the_fundamentals_of/</guid>
      <pubDate>Thu, 05 Dec 2024 21:34:18 GMT</pubDate>
    </item>
    <item>
      <title>研究在 RL 中使用 Transformer 架构的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7jhmk/resources_to_study_the_use_of_transformer/</link>
      <description><![CDATA[我读过关于在 RL 中使用 transformers 的调查论文，我知道它们是 RL 的 RNN 替代品。这些论文并没有对实现提供太多见解。我知道决策 transformers，但这不是我想要使用的，因为它没有任何策略网络，我想使用 RL 算法，但 NN 架构必须使用 transformers 来突出状态的长期依赖性。有人用过这个吗？如果有，如果你能分享资源会很有帮助。    提交人    /u/anchit_rana   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7jhmk/resources_to_study_the_use_of_transformer/</guid>
      <pubDate>Thu, 05 Dec 2024 20:51:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h772b4/reinforcement_learning_courses/</link>
      <description><![CDATA[对于强化学习，以下哪门课程是首选-   UCL X DeepMind ⁠Stanford CS234 ⁠David Silver 的 RL 课程     提交人    /u/momosspicy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h772b4/reinforcement_learning_courses/</guid>
      <pubDate>Thu, 05 Dec 2024 11:28:41 GMT</pubDate>
    </item>
    <item>
      <title>强化学习适合解决刽子手游戏吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6ynzs/is_reinforcement_learning_suitable_for_solving/</link>
      <description><![CDATA[在过去的 3 周里，我一直在使用 RL 迭代不同的策略来解决 Hangman 游戏，但到目前为止，我的准确率并不高（1000 场游戏中解决的不到 100 场）。说实话，我对 RL 的概念还很陌生。我一开始使用 Deep Q-Learning 框架来解决这个问题。我的方法相当简单：  将训练单词的隐藏状态转换为两个向量（一个大小为 26 的二进制向量用于跟踪猜测的字母，另一个大小为 45 的规范化向量用于跟踪正确猜测的字母的位置），并将其作为输入传递给神经网络，同时传递剩余的猜测次数和要发现的隐藏字母数量。总输入大小为 73x1。 以 1 的探索率初始化训练循环，随着训练的进行，探索率衰减为 0.1。探索部分的工作原理是，使用训练单词列表，根据隐藏单词的长度，从预先计算的字典中选择最常见的字母。 对每个单词开始训练过程，游戏一直进行到代理猜出正确的单词或猜错 6 次为止，计算损失并在每次猜测后更新参数。对每个单词重复此过程。使用 q 学习网络获得的下一步计算损失。为在每个优化步骤中完成的重放设置了批量大小。 然后将训练好的网络用于一组看不见的验证词，然后使用这些词来衡量准确性。 我使用的奖励结构是每个正确字母 +1，每个错误字母 -1，如果单词猜对了则 +10，如果单词猜错了则 -10。然后我对其进行了更新，以便连续猜对字母可获得更高的奖励，连续猜错可获得更高的负奖励。 状态和奖励计算的更新是通过单独的 Hangman 模块完成的。每次猜测每个单词后都会调用该模块。  训练从 1000 场游戏的 10% 准确率开始，然后随着探索率的下降，准确率降低到 1%，这也反映在验证中。 现在我觉得我做错了。但你们认为这是一种合适的方法吗？ 编辑：我错误地说我使用两个二进制向量将有关隐藏状态的信息作为输入传递给神经网络。我实际上传递了一个二进制向量和一个在 0 和 1 之间标准化的向量。    提交人    /u/hpnr0724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6ynzs/is_reinforcement_learning_suitable_for_solving/</guid>
      <pubDate>Thu, 05 Dec 2024 02:38:33 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习中的奖励分配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6u396/reward_distribution_in_offline_rl/</link>
      <description><![CDATA[我正在使用保守 Q 学习和 SAC 模型。我的奖励分布非常不平衡：所有奖励都在 0 到 1 之间，95% 低于 0.02，40% 低于 0.001。 我应该如何转换它？我之所以问这个问题，是因为我的预测 Q 值为负：这没有意义，因为所有奖励都是正的。我已经排除了所有错误的可能性（我认为是这样）。我也得到了带有非常小惩罚的负 q 值（以及没有惩罚：我确实看到 q 函数在潜水前很长一段时间都是正的：潜水与 q 值方差的增加有关）。任何指针都值得赞赏！    提交人    /u/electricsheep123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6u396/reward_distribution_in_offline_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 23:06:15 GMT</pubDate>
    </item>
    <item>
      <title>“指南：实时人形代理”，Zhang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6prin/guide_realtime_humanshaped_agents_zhang_et_al_2024/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6prin/guide_realtime_humanshaped_agents_zhang_et_al_2024/</guid>
      <pubDate>Wed, 04 Dec 2024 20:08:16 GMT</pubDate>
    </item>
    <item>
      <title>LoRA研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6lepr/lora_research/</link>
      <description><![CDATA[最近，我发现关于 LoRA 替代品的论文激增。您认为人们正在探索哪些研究方向？ 您认为它有可能以某种方式与 RL 相结合吗？    提交人    /u/KevinBeicon   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6lepr/lora_research/</guid>
      <pubDate>Wed, 04 Dec 2024 17:15:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Q_Learning 算法无法正常学习？（更新）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6l4jl/why_is_my_q_learning_algorithm_not_learning/</link>
      <description><![CDATA[嗨，这是我几天前的另一篇文章的后续文章 ( https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/ ) 我阅读了您的评论并且 u/scprotz 告诉我即使是德文的，拥有代码也会很有用。这是我的代码：https://codefile.io/f/F8mGtSNXMX 我通常不会在网上分享我的代码，所以如果网站不是最好的选择，我很抱歉。不同的类通常位于不同的文档中（您可以在导入中看到），我运行 Spiel（即游戏）文件来启动程序。我希望这会有所帮助，如果您发现任何看起来奇怪或不正确的东西，请发表评论，因为尽管搜索了几个小时，我还是没有找到问题所在。    提交人    /u/_waterstar_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6l4jl/why_is_my_q_learning_algorithm_not_learning/</guid>
      <pubDate>Wed, 04 Dec 2024 17:04:30 GMT</pubDate>
    </item>
    <item>
      <title>预训练 VAE 与强化学习期间训练的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6imk0/difference_between_pretrained_vaes_versus/</link>
      <description><![CDATA[      我的目标是开发一个用于 Carla 模拟器进行自动驾驶的代理。为了实现这一点，我实现了 Soft Actor-Critic (SAC) 算法。在将图像输入 SAC 算法之前，我使用了变分自动编码器 (VAE)。 VAE 没有经过预训练，因为我假设它会在强化学习过程中得到训练和改进。这种方法有缺陷吗？如果有，为什么，如何改进？我采取这一步骤的理由是受到观察 DQN 中 CNN 在 Frozen Lake 环境中的使用情况的启发。我的代码可以在 GitHub 上找到，供感兴趣的人使用：https://github.com/b-gtr/Soft-Actor-Critic 这里还有一个简单的代码工作原理说明：    提交人    /u/Fair_Device_4961   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6imk0/difference_between_pretrained_vaes_versus/</guid>
      <pubDate>Wed, 04 Dec 2024 15:24:25 GMT</pubDate>
    </item>
    </channel>
</rss>