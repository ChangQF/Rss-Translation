<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 27 Dec 2023 09:13:28 GMT</lastBuildDate>
    <item>
      <title>艾未未无需任何编码就能学会玩《地铁跑酷》。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rttda/ai_learns_to_play_subway_surfers_without_any/</link>
      <description><![CDATA[    /u/Worldly-Daikon5001   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rttda/ai_learns_to_play_subway_surfers_without_any/</guid>
      <pubDate>Wed, 27 Dec 2023 06:19:29 GMT</pubDate>
    </item>
    <item>
      <title>GridWorld 的 Q-Learning 有时无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rtkso/qlearning_for_gridworld_occasionally_failing_to/</link>
      <description><![CDATA[我有一个 10x10 网格世界环境，我正在其中尝试实现 Q-Learning。所有单元格的奖励为 0.1，而终端单元格的奖励为 10。折扣因子为 0.9。有时，Q-Learning 无法收敛（例如每 10 次收敛一次），并且代理会卡在远离终端单元的位置。我尝试了衰减 epsilon，但这只会让训练变慢。请在此处找到该作品的链接。谢谢。    由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rtkso/qlearning_for_gridworld_occasionally_failing_to/</guid>
      <pubDate>Wed, 27 Dec 2023 06:05:30 GMT</pubDate>
    </item>
    <item>
      <title>我为我的 NeurIPS 2023 论文制作了一个 7 分钟的讲解视频。我希望你喜欢它 ：）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rti1y/i_made_a_7minute_explanation_video_of_my_neurips/</link>
      <description><![CDATA[       由   提交/u/delayed_reward  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rti1y/i_made_a_7minute_explanation_video_of_my_neurips/</guid>
      <pubDate>Wed, 27 Dec 2023 06:01:13 GMT</pubDate>
    </item>
    <item>
      <title>“拒绝的理由？将语言模型与判断相结合”，Xu et al 2023 {腾讯}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rnxw1/reasons_to_reject_aligning_language_models_with/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rnxw1/reasons_to_reject_aligning_language_models_with/</guid>
      <pubDate>Wed, 27 Dec 2023 01:19:14 GMT</pubDate>
    </item>
    <item>
      <title>“ER-MRL：元强化学习的进化储存库”，Léger 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rmu8o/ermrl_evolving_reservoirs_for_meta_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rmu8o/ermrl_evolving_reservoirs_for_meta_reinforcement/</guid>
      <pubDate>Wed, 27 Dec 2023 00:27:03 GMT</pubDate>
    </item>
    <item>
      <title>我可以直接改变基于策略的方法中的动作概率吗？ 【安全勘探相关】</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rlzly/can_i_directly_alter_the_action_probability_in/</link>
      <description><![CDATA[假设我想在基于网格的环境中使用强化学习来执行一些规划任务，我希望代理在训练中偶尔避开某些单元。  在像 Q 学习这样的基于值的简单方法中，我可以减少与该操作相关的值，从而降低采取该操作的概率（假设我使用 softmax）。基于策略的方法或其他基于价值的方法是否有类似的东西？  这背后的直觉是，我想告诉智能体：“如果你可能因行动 X 而陷入危险状态，请降低在此状态下采取行动 X 的概率”。我不希望代理完全停止进入该状态，因为我仍然希望它能够探索需要进入该状态的轨迹。我总是不希望代理仅通过试错来学习这个概率，我想给代理一些先验知识。  我是否在考虑直接改变动作概率？还有其他方法可以像这样预先注入吗？  我希望这是有道理的！  谢谢！    由   提交 /u/AlloyEnt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rlzly/can_i_directly_alter_the_action_probability_in/</guid>
      <pubDate>Tue, 26 Dec 2023 23:49:04 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助，比特币交易的奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r8w1g/help_needed_reward_function_for_bitcoin_trading/</link>
      <description><![CDATA[在过去的一年里，我一直致力于使用 Gym + Stable Baselines 来训练 PPO 代理交易比特币。每一步，代理都会“看到”60 个历史定价点 + MACD、RSI 等技术指标。然后代理可以买入、卖出或持有。这个想法是让代理人低价买入，持有……然后高价卖出。我一直在完善环境、提高速度、探索等。我仍然在努力解决的一件事是可靠的奖励功能。  我目前的实验奖励看起来像这样（见下面列出），我很好奇这里是否有人有更好的奖励想法。很乐意一起合作/集思广益。  核心概念  交易序列：奖励函数将每组交易行为（买入、卖出、持有）作为序列进行跟踪。序列以“买入”开始，以“卖出”结束，包括其间的任何“持有”操作。 基于操作的奖励：每个操作 - 买入、卖出，或持有 - 以特定方式贡献总体奖励：   买入：开始新的交易序列。 卖出：完成交易序列并触发奖励计算。 持有：影响正在进行的交易序列，并可能产生少量奖励或基于市场趋势的惩罚。  奖励计算  交易利润/损失：当交易序列结束时（“卖出”） &#39; 动作发生），奖励是根据该序列的利润或损失来计算的。盈利会带来积极的回报，而亏损会带来惩罚。 持有行动动态：根据市场趋势，持有可能有利也可能有害。当价格趋势与最后的交易行为一致时（例如，购买后价格上涨），该函数会对持有者给予少量奖励，而在其他情况下则给予少量惩罚。这种方法鼓励战略持有。 亏损交易惩罚：为了阻止鲁莽交易，该功能对亏损交易施加惩罚。这种惩罚只是损失的一小部分，促进谨慎和深思熟虑的交易决策。  为什么采用这种方法？  现实交易模拟：通过考虑利润、损失和市场趋势，该功能反映了真实世界的交易场景，让代理为实际情况做好准备。 战略决策：细致入微的奖励惩罚系统鼓励代理人制定超越简单利润最大化的策略，例如何时持有和何时执行交易。 平衡风险管理：损失惩罚确保代理学习有效地管理风险，避免过于激进或冒险的策略。    由   提交/u/ClassicAppropriate78  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r8w1g/help_needed_reward_function_for_bitcoin_trading/</guid>
      <pubDate>Tue, 26 Dec 2023 14:09:18 GMT</pubDate>
    </item>
    <item>
      <title>“自我预测通用人工智能”（Self-AIXI）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r792j/selfpredictive_universal_ai_selfaixi/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=psXVkKO9No 摘要：  强化学习（RL）算法通常利用学习和/或制定有效政策的规划技术。事实证明，两种方法的集成在解决复杂的顺序决策挑战方面非常成功，AlphaZero 和 MuZero 等算法就证明了这一点，这些算法将规划过程整合到参数搜索策略中。 AIXI 是理论上最有效的通用智能体，它通过综合搜索进行规划作为寻找最优策略的主要手段。在这里，我们定义了一个替代的通用代理，我们称之为Self-AIXI，与A​​IXI相反，它最大限度地利用学习来获得良好的策略。它通过自我预测自己的动作数据流来实现这一点，与其他 TD(0) 代理类似，该数据流是通过对当前在策略（通用混合策略）Q 值估计采取动作最大化步骤来生成的。我们证明了Self-AIXI收敛于AIXI，并继承了最大Legg-Hutter智能和自优化特性等一系列特性。   &amp; #32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r792j/selfpredictive_universal_ai_selfaixi/</guid>
      <pubDate>Tue, 26 Dec 2023 12:36:43 GMT</pubDate>
    </item>
    <item>
      <title>GAE 来估计优势还是回报？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r3p15/gae_to_estimate_advantage_or_also_returns/</link>
      <description><![CDATA[嗨，在旋转 Ppo 时，他们使用 GAE 计算优势，并且仅使用奖励计算回报（蒙特卡罗估计）但是，其他植入使用 GAE 来计算近似回报和优势。有什么意见或想法吗？   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r3p15/gae_to_estimate_advantage_or_also_returns/</guid>
      <pubDate>Tue, 26 Dec 2023 08:36:09 GMT</pubDate>
    </item>
    <item>
      <title>PPO 与状态相关标准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r3l8z/ppo_with_state_dependent_std/</link>
      <description><![CDATA[嗨，有谁知道 Ppo 实现与可学习的 Logstd 取决于状态，而不仅仅是一个参数（例如 cleanrl）我尝试实现，但是它非常不稳定，我可以使用类似 sac 实现的东西，但试图找到 ppo 稳定的东西 谢谢   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r3l8z/ppo_with_state_dependent_std/</guid>
      <pubDate>Tue, 26 Dec 2023 08:28:56 GMT</pubDate>
    </item>
    <item>
      <title>[帮助] Stable Baselines3 中的 Dict 操作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/</link>
      <description><![CDATA[大家好。 我在创建对产品列表页面中的项目进行排序的 RL 代理时遇到了一些问题。我有一个产品列表，我希望观察/状态是按一定顺序排列的产品 ID 列表。例如：[0,2,4,1,2]。这意味着 id 0 的产品是页面上的第一个项目，第二个是产品 2..etc  该操作将是一个带有产品 id 的字典以及是否将其在列表中向上移动，放下或将其留在原处。 这是我的做法： fromgymnasium.spaces import Dict, Discrete, Sequence, MultiDiscrete class CustomEnvironment(gym.Env)： def __init__(self, number_products, seeds=None): self.number_products = number_products # 随机选择起始状态 self.starter_state = np.array([i for i in range(number_products)]) random.Random(seed).shuffle( self.starter_state) self.current_state = self.starter_state # 0 = 向上，1 = 没有变化，2 = 向下 self.action_space = Dict({&quot;product&quot;: Discrete(number_products), &quot;move&quot;: Discrete(3 )}) self.observation_space = MultiDiscrete([number_products] * number_products)  我想使用稳定基线3，但当我运行稳定基线&#39;.check_env时，我收到以下警告： 用户警告：操作空间不是基于 numpy 数组。通常这意味着它是字典或元组空间。 Stable Baselines 3 目前不支持这种类型的操作空间。您应该尝试使用包装器来展平操作。  知道如何解决这个问题吗？ 任何帮助将不胜感激:) 谢谢！ 更新： 我可以通过用 MultiDiscrete([num_products, 3]) 替换 Dict 来使其工作，它基本上做同样的事情。 这是我的第一个 RL 项目，所以如果有人有意见或建议，我会洗耳恭听:) ​   由   提交 /u/Rich-Professional171   /u/Rich-Professional171 reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/</guid>
      <pubDate>Tue, 26 Dec 2023 00:57:34 GMT</pubDate>
    </item>
    <item>
      <title>“ReBRAC：重新审视离线强化学习的极简方法”，Tarasov 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qrlxm/rebrac_revisiting_the_minimalist_approach_to/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qrlxm/rebrac_revisiting_the_minimalist_approach_to/</guid>
      <pubDate>Mon, 25 Dec 2023 21:31:40 GMT</pubDate>
    </item>
    <item>
      <title>强化学习以片段而非步骤的形式进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qgg22/rl_training_in_episodes_instead_of_steps/</link>
      <description><![CDATA[        由   提交/u/TwTC8  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qgg22/rl_training_in_episodes_instead_of_steps/</guid>
      <pubDate>Mon, 25 Dec 2023 10:50:20 GMT</pubDate>
    </item>
    <item>
      <title>训练人形机器人如何行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qbd50/training_humanoid_how_to_walk/</link>
      <description><![CDATA[     &lt; td&gt; 大力投资特斯拉期权，决定将我的钱投入人形机器人。将使用强化学习训练站立和行走。如果有人想聚会的话，我住在纽约。   由   提交 /u/Logical_Flatworm8179   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qbd50/training_humanoid_how_to_walk/</guid>
      <pubDate>Mon, 25 Dec 2023 04:32:17 GMT</pubDate>
    </item>
    <item>
      <title>矢量化训练会导致性能下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pz7rq/performance_degrades_with_vectorized_training/</link>
      <description><![CDATA[我对 RL 还很陌生，但在读完 Sutton 和 Barto 的书后，我决定自己尝试实现一些 RL 算法。我根据书中的算法实现了一种非常简单的深度演员评论家算法，并且在正确的学习率下，性能出奇地好。我什至能够在没有回复缓冲区的体育馆中的月球着陆器上获得不错的结果。我决定尝试在多个环境中同时训练它，认为这会提高稳定性并加快学习速度，但令人惊讶的是，它似乎产生了相反的效果。使用更多矢量化环境时，算法变得越来越不稳定。有谁知道可能是什么原因造成的？   由   提交/u/YouPspecial8085   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pz7rq/performance_degrades_with_vectorized_training/</guid>
      <pubDate>Sun, 24 Dec 2023 17:07:54 GMT</pubDate>
    </item>
    </channel>
</rss>