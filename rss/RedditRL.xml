<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 27 Dec 2023 00:58:22 GMT</lastBuildDate>
    <item>
      <title>“ER-MRL：元强化学习的进化储存库”，Léger 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rmu8o/ermrl_evolving_reservoirs_for_meta_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rmu8o/ermrl_evolving_reservoirs_for_meta_reinforcement/</guid>
      <pubDate>Wed, 27 Dec 2023 00:27:03 GMT</pubDate>
    </item>
    <item>
      <title>我可以直接改变基于策略的方法中的动作概率吗？ 【安全勘探相关】</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rlzly/can_i_directly_alter_the_action_probability_in/</link>
      <description><![CDATA[假设我想在基于网格的环境中使用强化学习来执行一些规划任务，我希望代理在训练中偶尔避开某些单元。  在像 Q 学习这样的基于值的简单方法中，我可以减少与该操作相关的值，从而降低采取该操作的概率（假设我使用 softmax）。基于策略的方法或其他基于价值的方法是否有类似的东西？  这背后的直觉是，我想告诉智能体：“如果你可能因行动 X 而陷入危险状态，请降低在此状态下采取行动 X 的概率”。我不希望代理完全停止进入该状态，因为我仍然希望它能够探索需要进入该状态的轨迹。我总是不希望代理仅通过试错来学习这个概率，我想给代理一些先验知识。  我是否在考虑直接改变动作概率？还有其他方法可以像这样预先注入吗？  我希望这是有道理的！  谢谢！    由   提交 /u/AlloyEnt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rlzly/can_i_directly_alter_the_action_probability_in/</guid>
      <pubDate>Tue, 26 Dec 2023 23:49:04 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助，比特币交易的奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r8w1g/help_needed_reward_function_for_bitcoin_trading/</link>
      <description><![CDATA[在过去的一年里，我一直致力于使用 Gym + Stable Baselines 来训练 PPO 代理交易比特币。每一步，代理都会“看到”60 个历史定价点 + MACD、RSI 等技术指标。然后代理可以买入、卖出或持有。这个想法是让代理人低价买入，持有……然后高价卖出。我一直在完善环境、提高速度、探索等。我仍然在努力解决的一件事是可靠的奖励功能。  我目前的实验奖励看起来像这样（见下面列出），我很好奇这里是否有人有更好的奖励想法。很乐意一起合作/集思广益。  核心概念  交易序列：奖励函数将每组交易行为（买入、卖出、持有）作为序列进行跟踪。序列以“买入”开始，以“卖出”结束，包括其间的任何“持有”操作。 基于操作的奖励：每个操作 - 买入、卖出，或持有 - 以特定方式贡献总体奖励：   买入：开始新的交易序列。 卖出：完成交易序列并触发奖励计算。 持有：影响正在进行的交易序列，并可能产生少量奖励或基于市场趋势的惩罚。  奖励计算  交易利润/损失：当交易序列结束时（“卖出”） &#39; 动作发生），奖励是根据该序列的利润或损失来计算的。盈利会带来积极的回报，而亏损会带来惩罚。 持有行动动态：根据市场趋势，持有可能有利也可能有害。当价格趋势与最后的交易行为一致时（例如，购买后价格上涨），该函数会对持有者给予少量奖励，而在其他情况下则给予少量惩罚。这种方法鼓励战略持有。 亏损交易惩罚：为了阻止鲁莽交易，该功能对亏损交易施加惩罚。这种惩罚只是损失的一小部分，促进谨慎和深思熟虑的交易决策。  为什么采用这种方法？  现实交易模拟：通过考虑利润、损失和市场趋势，该功能反映了真实世界的交易场景，让代理为实际情况做好准备。 战略决策：细致入微的奖励惩罚系统鼓励代理人制定超越简单利润最大化的策略，例如何时持有和何时执行交易。 平衡风险管理：损失惩罚确保代理学习有效地管理风险，避免过于激进或冒险的策略。    由   提交/u/ClassicAppropriate78  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r8w1g/help_needed_reward_function_for_bitcoin_trading/</guid>
      <pubDate>Tue, 26 Dec 2023 14:09:18 GMT</pubDate>
    </item>
    <item>
      <title>“自我预测通用人工智能”（Self-AIXI）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r792j/selfpredictive_universal_ai_selfaixi/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=psXVkKO9No 摘要：  强化学习（RL）算法通常利用学习和/或制定有效政策的规划技术。事实证明，两种方法的集成在解决复杂的顺序决策挑战方面非常成功，AlphaZero 和 MuZero 等算法就证明了这一点，这些算法将规划过程整合到参数搜索策略中。 AIXI 是理论上最有效的通用智能体，它通过综合搜索进行规划作为寻找最优策略的主要手段。在这里，我们定义了一个替代的通用代理，我们称之为Self-AIXI，与A​​IXI相反，它最大限度地利用学习来获得良好的策略。它通过自我预测自己的动作数据流来实现这一点，与其他 TD(0) 代理类似，该数据流是通过对当前在策略（通用混合策略）Q 值估计采取动作最大化步骤来生成的。我们证明了Self-AIXI收敛于AIXI，并继承了最大Legg-Hutter智能和自优化特性等一系列特性。   &amp; #32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r792j/selfpredictive_universal_ai_selfaixi/</guid>
      <pubDate>Tue, 26 Dec 2023 12:36:43 GMT</pubDate>
    </item>
    <item>
      <title>GAE 来估计优势还是回报？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r3p15/gae_to_estimate_advantage_or_also_returns/</link>
      <description><![CDATA[嗨，在旋转 Ppo 时，他们使用 GAE 计算优势，并且仅使用奖励计算回报（蒙特卡罗估计）但是，其他植入使用 GAE 来计算近似回报和优势。有什么意见或想法吗？   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r3p15/gae_to_estimate_advantage_or_also_returns/</guid>
      <pubDate>Tue, 26 Dec 2023 08:36:09 GMT</pubDate>
    </item>
    <item>
      <title>PPO 与状态相关标准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18r3l8z/ppo_with_state_dependent_std/</link>
      <description><![CDATA[嗨，有谁知道 Ppo 实现与可学习的 Logstd 取决于状态，而不仅仅是一个参数（例如 cleanrl）我尝试实现，但是它非常不稳定，我可以使用类似 sac 实现的东西，但试图找到 ppo 稳定的东西 谢谢   由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18r3l8z/ppo_with_state_dependent_std/</guid>
      <pubDate>Tue, 26 Dec 2023 08:28:56 GMT</pubDate>
    </item>
    <item>
      <title>[帮助] Stable Baselines3 中的 Dict 操作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/</link>
      <description><![CDATA[大家好。 我在创建对产品列表页面中的项目进行排序的 RL 代理时遇到了一些问题。我有一个产品列表，我希望观察/状态是按一定顺序排列的产品 ID 列表。例如：[0,2,4,1,2]。这意味着 id 0 的产品是页面上的第一个项目，第二个是产品 2..etc  该操作将是一个带有产品 id 的字典以及是否将其在列表中向上移动，放下或将其留在原处。 这是我的做法： fromgymnasium.spaces import Dict, Discrete, Sequence, MultiDiscrete class CustomEnvironment(gym.Env)： def __init__(self, number_products, seeds=None): self.number_products = number_products # 随机选择起始状态 self.starter_state = np.array([i for i in range(number_products)]) random.Random(seed).shuffle( self.starter_state) self.current_state = self.starter_state # 0 = 向上，1 = 没有变化，2 = 向下 self.action_space = Dict({&quot;product&quot;: Discrete(number_products), &quot;move&quot;: Discrete(3 )}) self.observation_space = MultiDiscrete([number_products] * number_products)  我想使用稳定基线3，但当我运行稳定基线&#39;.check_env时，我收到以下警告： 用户警告：操作空间不是基于 numpy 数组。通常这意味着它是字典或元组空间。 Stable Baselines 3 目前不支持这种类型的操作空间。您应该尝试使用包装器来展平操作。  知道如何解决这个问题吗？ 任何帮助将不胜感激:) 谢谢！ 更新： 我可以通过用 MultiDiscrete([num_products, 3]) 替换 Dict 来使其工作，它基本上做同样的事情。 这是我的第一个 RL 项目，所以如果有人有意见或建议，我会洗耳恭听:) ​   由   提交/u/Rich-Professional171  /u/Rich-Professional171 reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/</guid>
      <pubDate>Tue, 26 Dec 2023 00:57:34 GMT</pubDate>
    </item>
    <item>
      <title>“ReBRAC：重新审视离线强化学习的极简方法”，Tarasov 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qrlxm/rebrac_revisiting_the_minimalist_approach_to/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qrlxm/rebrac_revisiting_the_minimalist_approach_to/</guid>
      <pubDate>Mon, 25 Dec 2023 21:31:40 GMT</pubDate>
    </item>
    <item>
      <title>强化学习以片段而非步骤的形式进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qgg22/rl_training_in_episodes_instead_of_steps/</link>
      <description><![CDATA[        由   提交/u/TwTC8  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qgg22/rl_training_in_episodes_instead_of_steps/</guid>
      <pubDate>Mon, 25 Dec 2023 10:50:20 GMT</pubDate>
    </item>
    <item>
      <title>训练人形机器人如何行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qbd50/training_humanoid_how_to_walk/</link>
      <description><![CDATA[     &lt; td&gt; 大力投资特斯拉期权，决定将我的钱投入人形机器人。将使用强化学习训练站立和行走。如果有人想聚会的话，我住在纽约。   由   提交/u/Logical_Flatworm8179   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qbd50/training_humanoid_how_to_walk/</guid>
      <pubDate>Mon, 25 Dec 2023 04:32:17 GMT</pubDate>
    </item>
    <item>
      <title>超越人类数据：利用语言模型扩展自我训练以解决问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pzr9g/beyond_human_data_scaling_selftraining_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.06585 摘要：  对人类生成的语言模型~（LM）进行微调数据仍然是一种普遍的做法。然而，此类模型的性能通常受到高质量人类数据的数量和多样性的限制。在本文中，我们探讨了在我们可以获得标量反馈的任务中是否可以超越人类数据，例如，在可以验证正确性的数学问题上。为此，我们研究了一种基于期望最大化的简单自我训练方法，我们称之为 ReSTEM，其中我们（1）从模型生成样本并使用二进制反馈对其进行过滤，（2）在这些样本上微调模型，以及（3）重复此过程几次。使用 PaLM-2 模型对高级 MATH 推理和 APPS 编码基准进行测试，我们发现 ReSTEM 可以很好地随模型大小进行扩展，并且显着超越仅在人类数据上进行的微调。总的来说，我们的研究结果表明，带有反馈的自我训练可以大大减少对人类生成数据的依赖。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pzr9g/beyond_human_data_scaling_selftraining_for/</guid>
      <pubDate>Sun, 24 Dec 2023 17:35:04 GMT</pubDate>
    </item>
    <item>
      <title>矢量化训练会导致性能下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pz7rq/performance_degrades_with_vectorized_training/</link>
      <description><![CDATA[我对 RL 还很陌生，但在读完 Sutton 和 Barto 的书后，我决定自己尝试实现一些 RL 算法。我根据书中的算法实现了一种非常简单的深度演员评论家算法，并且在正确的学习率下，性能出奇地好。我什至能够在没有回复缓冲区的体育馆中的月球着陆器上获得不错的结果。我决定尝试在多个环境中同时训练它，认为这会提高稳定性并加快学习速度，但令人惊讶的是，它似乎产生了相反的效果。使用更多矢量化环境时，算法变得越来越不稳定。有谁知道可能是什么原因造成的？   由   提交/u/YouPspecial8085   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pz7rq/performance_degrades_with_vectorized_training/</guid>
      <pubDate>Sun, 24 Dec 2023 17:07:54 GMT</pubDate>
    </item>
    <item>
      <title>Python RL 中设置训练“基本法则”的最佳位置是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pgh6z/the_best_place_in_python_rl_to_set_the/</link>
      <description><![CDATA[嗨 我一直在努力理解放置“基本法则”的最佳位置。对于 PPO 模型。我在某处读到它应该在环境中，而在其他地方，我读到它最好在预处理器中完成。就我而言（玩股票交易 RL 只是为了好玩），我只希望它在位置 = 0 时买入。我当然可以在环境中对此进行调整，但它“感觉”不理想。错误（不知道为什么..）。我宁愿把它放在代理那边，但也许我弄错了..    由   提交/u/Forward-Cranberry-30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pgh6z/the_best_place_in_python_rl_to_set_the/</guid>
      <pubDate>Sat, 23 Dec 2023 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>Pearl：生产就绪的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18p82ug/pearl_a_productionready_reinforcement_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.03814 代码：https://github .com/facebookresearch/pearl 项目页面：https://pearlagent. github.io/ 摘要：  强化学习（RL）为实现长期目标提供了一个多功能框架。它的通用性使我们能够形式化现实世界智能系统遇到的各种问题，例如处理延迟奖励、处理部分可观察性、解决探索和利用困境、利用离线数据提高在线性能以及确保安全约束遇见了。尽管强化学习研究社区在解决这些问题方面取得了相当大的进展，但现有的开源强化学习库往往只关注强化学习解决方案管道的一小部分，而其他方面基本上无人关注。本文介绍了 Pearl，这是一个生产就绪的 RL 代理软件包，专门设计用于以模块化方式应对这些挑战。除了提供初步基准测试结果外，本文还重点介绍了 Pearl 的行业采用情况，以证明其已做好生产使用的准备。 Pearl 在 Github 上开源，网址为 此 http URL，其官方网站位于 这个http URL。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18p82ug/pearl_a_productionready_reinforcement_learning/</guid>
      <pubDate>Sat, 23 Dec 2023 15:32:28 GMT</pubDate>
    </item>
    <item>
      <title>ReCoRe：世界模型的正则化对比表示学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.09056 摘要：  虽然最近的无模型强化学习（RL）方法已经证明尽管在游戏环境中的人类水平的有效性，他们在视觉导航等日常任务中的成功受到限制，特别是在显着的外观变化下。这种限制源于 (i) 样本效率差和 (ii) 过度拟合训练场景。为了应对这些挑战，我们提出了一个世界模型，该模型使用（i）对比无监督学习和（ii）干预不变正则化器来学习不变特征。学习世界动态的显式表示（即世界模型）可以提高样本效率，而对比学习隐式地强制学习不变特征，从而提高泛化能力。然而，由于缺乏视觉编码器的监督信号，对比损失与世界模型的简单集成失败了，因为基于世界模型的强化学习方法独立地优化了表示学习和代理策略。为了克服这个问题，我们提出了一种以辅助任务（例如深度预测、图像去噪等）形式存在的干预不变正则化器，它明确地强制风格干预的不变性。我们的方法优于当前最先进的基于模型和无模型的 RL 方法，并且在 iGibson 基准评估的分布外点导航任务上表现显着。我们进一步证明，我们的方法仅通过视觉观察，优于最近的语言引导的点导航基础模型，这对于在计算能力有限的机器人上部署至关重要。最后，我们证明我们提出的模型在 Gibson 基准上的感知模块的模拟到真实转换方面表现出色。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</guid>
      <pubDate>Fri, 22 Dec 2023 13:36:49 GMT</pubDate>
    </item>
    </channel>
</rss>