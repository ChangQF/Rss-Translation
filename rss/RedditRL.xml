<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 08 Mar 2024 06:17:18 GMT</lastBuildDate>
    <item>
      <title>深度强化学习中的决策边界相似性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b93ou6/decision_boundary_similarities_in_deep/</link>
      <description><![CDATA[在 AAAI 中发布 https ://twitter.com/EzgiKorkmazAI/status/1765019973917741057   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b93ou6/decision_boundary_similarities_in_deep/</guid>
      <pubDate>Thu, 07 Mar 2024 19:59:12 GMT</pubDate>
    </item>
    <item>
      <title>修补强化学习所需的知识水平</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b90c96/level_of_knowledge_needed_to_tinker_with_rl/</link>
      <description><![CDATA[我是一名普通的 3D 打印机/机器人修补匠，从我所看到的 RL 的外观来看，它不仅有趣而且有趣。但当我浏览这里的帖子时，它们都写得超出了我的理解范围。当我在网上查看“如何进入……”的内容时，里面充满了让我延伸很远的概念。对我来说，显然我不知道这一切的背景是什么。这是您非常需要学位或大学水平的教育和理解才能开始的事情吗？我发现很多涉及强化学习的例子都是人们的课程项目。我相信你可以做你想做的事，但我希望知道我可能需要付出多大的努力，以及这对我来说是否值得。你的背景是什么？   由   提交 /u/UltimateThrowawayNam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b90c96/level_of_knowledge_needed_to_tinker_with_rl/</guid>
      <pubDate>Thu, 07 Mar 2024 17:30:27 GMT</pubDate>
    </item>
    <item>
      <title>CartPole V1 学习方向相反！！！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8ysyt/cartpole_v1_learning_in_the_opposite_direction/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8ysyt/cartpole_v1_learning_in_the_opposite_direction/</guid>
      <pubDate>Thu, 07 Mar 2024 16:24:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 和 DDPG 的 Trackmania RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8w84q/trackmania_rl_using_dqn_and_ddpg/</link>
      <description><![CDATA[     &lt; /td&gt; 我的第一个强化学习项目！经过与我的 3 位室友几个月的合作，我们制作了一个 Trackmania 机器人，能够在简单的赛道上行驶。看到它最终学会是非常有益的！ 查看：GitHub（代码欢迎提出建议！） https://reddit.com/link/1b8w84q/video/qrnm23k6bxmc1 /player ​   由   提交/u/giorgiocav123  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8w84q/trackmania_rl_using_dqn_and_ddpg/</guid>
      <pubDate>Thu, 07 Mar 2024 14:38:30 GMT</pubDate>
    </item>
    <item>
      <title>不同数量的动作元素和潜在的大离散动作空间......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8w0x1/varying_number_of_action_elements_and_potentially/</link>
      <description><![CDATA[您好， 作为深度强化学习的新手，我正在寻求有关以下问题的建议。请随时对整体设置提出一般性评论，因为我可能无法识别核心问题... 我的空间环境的状态是由不同的数字定义的（n）个元素，每个元素都有一个标量属性 d 以及相应的 x 和 y 坐标。请注意，n 的上限没有严格限制，而是受到环境动态的“软限制”。在每个时间步，代理可以决定删除哪些元素：“杀死”一个元素会生成奖励信号，并根据剩余元素的数量计算下一个状态。 当前形式，此设置有两个主要问题： (a) 状态空间和动作空间的大小各不相同，具体取决于当前存在的元素数量； (b) 取决于在当前的元素数量上，操作空间的大小可能相当大（在 100 个元素的情况下为 2^100） 我正在寻求通用方法的建议，因为我的第一个天真尝试没有成功。 如何处理a）不同数量的元素和b）（可能）大量离散操作？ 到目前为止我的想法 - 小心，也许是废话：  关于a）我尝试将网络的输入节点数量（到目前为止我使用了PPO）设置为某个固定值（比如100），并将参与者输出的数量设置为相同的值，并根据某些固定规则（例如按属性“d”排名）将元素分配给输入和输出节点。这涉及到如果 n 超过 100，则从决策和推理中排除元素，以及在 n &gt; 100 的情况下用“伪 NA”“填充”空输入节点。 100. 我可以通过这种方法获得一些学习成果，但最终的政策远非最佳。我认为这种情况下的问题确实是 b)，据我所知，我需要一些能力来概括，不仅对状态，而且对行动。我读过 Dulac-Arnold 等人，觉得这种方法可能有所帮助，但是我不知道如何构建我的动作空间的方式允许有效地将标量、连续的原始动作映射到一组+-相似的离散动作...    ;由   提交 /u/ionatura   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8w0x1/varying_number_of_action_elements_and_potentially/</guid>
      <pubDate>Thu, 07 Mar 2024 14:29:53 GMT</pubDate>
    </item>
    <item>
      <title>模拟 5G 环境中的深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8qhe3/deep_rl_in_simulated_5g_environment/</link>
      <description><![CDATA[我目前正在撰写硕士论文，目的是在简单的模拟 5G 环境中创建基于 AI 的自适应软件。我正在运行由发送节点和接收节点组成的模拟。请记住，模拟应该很简单，因为模拟不是论文的主要部分。发送方向接收方发送一批数据包，然后接收方检查丢失了多少数据包以及发送数据包的负载（网络拥塞）。 我正在使用带有连续操作空间的 Gym介于 -1 和 1 之间，表示代理可以更改带宽的值。 -1 和 1 是 -50 到 50 的标准化值，因此 -1 的操作会以 50 Mbit/s 的速度减少带宽，而 1 则会以 50 Mbit/s 的速度增加带宽，依此类推。观察空间由三个维度组成，其中第一个维度是 0 到 1 之间的带宽（范围 1-50 的标准化值），第二个维度是当前批次发送的负载，第三个维度是之前的负载。获取当前负载和先前负载的原因是为了查看负载是增加还是减少。我正在考虑只有二维，带宽为一，前一个和当前负载的导数为第二个，但我还没有测试过。代理只能更改其带宽值，因为它不能影响负载或之前的负载。 主要目标是减少丢失数据包的总量并减少发送数据包所需的总时间。一定数量的数据包。我希望代理在负载低时增加带宽，在负载高时减少带宽。这是因为高负载意味着更高的数据包丢失率，因此我们不希望在大多数数据包丢失时发送大量数据包。尽管带宽较低，速度会较慢，但我们会丢失较少数量的数据包。 我正在使用和比较的模型是 PPO 和 DDPG，我遇到的问题是创建一个合适的奖励函数，平衡最小化数据包丢失和最小化总时间之间的比率。我已经取得了一些进展，代理知道在负载非常高的情况下选择低带宽，在负载非常低的情况下选择高带宽，但它在两者之间感到困惑。如果有人知道导出奖励函数的好方法，我将不胜感激。我的方法是对奖励函数进行有根据的猜测，但它们在平衡我的问题方面还不够好   由   提交 /u/axeljnsson   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8qhe3/deep_rl_in_simulated_5g_environment/</guid>
      <pubDate>Thu, 07 Mar 2024 09:27:41 GMT</pubDate>
    </item>
    <item>
      <title>是否有一种具有多种输出类型的强化学习算法？ （Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8onq6/is_there_an_rlalgorithm_with_multiple_types_of/</link>
      <description><![CDATA[嘿大家 我正在尝试用游戏 CS2 做一些实验项目(相关帖子），我想知道是否有任何 RL 算法可以输出多种类型。 例如：对于移动，我们使用 float，对于射击/攻击，我们使用 int(0: 不攻击 / 1: 攻击) 例如输出如下：[float, float, int]  有可能有这样的东西吗？是否可以将其作为健身房环境？   由   提交 /u/Mr_Lucifer_666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8onq6/is_there_an_rlalgorithm_with_multiple_types_of/</guid>
      <pubDate>Thu, 07 Mar 2024 07:29:16 GMT</pubDate>
    </item>
    <item>
      <title>负解释方差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8mjz4/negative_explained_variance/</link>
      <description><![CDATA[   https://wandb .ai/kingsignificant5097/uncategorized?workspace=user-kingsignificant5097 https://preview.redd.it/wqbzrf6dgumc1.png?width=3516&amp;format=png&amp;auto=webp&amp;s=eacf2d106283d3bc2143a9a9e3f7f68f5 bcbdb67 我一直在致力于一个项目，试图亲身体验一些强化学习。 我正在建模的环境是一个时间序列，我正在训练一个代理通过交易金融衍生品来最大化财务回报，特别是杠杆永续期货。因此，它需要最大化财务收益并最小化财务损失。每当代理平仓时都会给予奖励，奖励与相对于初始投资的收益/损失百分比成正比。 动作空间是一个具有 4 个动作的多离散空间，不执行任何操作，开仓持仓（以及要使用的资金百分比）、平仓、结束事件。观察空间约为 100 个，包含当前开盘/收盘/交易量和一些财务指标（技术分析）以及某些历史时期的聚合指标。 使用的算法是 dd-ppo（56 个 cpu） ）使用 rllib 进行一些修改，特别是使用实现操作屏蔽的自定义模型。例如，确保代理只有在有仓位存在时才可以平仓，并且只有在没有持仓并且总收益或损失&gt; 1时才能结束一个episode。 （某些％），或者总剧集长度大于某些步数。 我还使用 具有 8 个头和 4 个变压器的注意力网络，并使用 RE3  用于探索。 通过上述所有内容，我能够获得良好的结果，并且代理似乎确实在学习，基于平均奖励和基于针对未见数据检查点的实证测试。  但是，在查看 TensorBoard 指标时，我看到了一些对我来说没有多大意义的事情，因此我在这里寻求任何人的帮助或建议： &lt; ol&gt; 是什么导致解释方差为负？我应该预计这个数字会开始增加吗？鉴于奖励均值在如此复杂的环境中不断增加，这样的负方差有何意义？ 为什么熵并没有真正下降？这很重要吗？ 为什么损失继续缓慢增加？我应该期望这个值会在某个时候开始下降吗？这是代理仍在学习的标志吗？  提前感谢您的指点！ 一些修改的参数，默认值来自 rllib for ppo 和 dd-ppo 覆盖： &lt;代码&gt;地平线 = 60 .training( train_batch_size = 地平线 * 4, sgd_minibatch_size = 地平线 * 2, num_sgd_iter = 2, gamma = 1.0, lambda_ = 1.0, entropy_coeff = 0.001, grad_clip = 10, model = { “fcnet_hiddens”: [1024 ，1024]，“max_seq_len”：地平线，“attention_num_transformer_units”：4，“attention_num_heads”：8，}） &lt;！-- SC_ON --&gt;   ;由   提交 /u/KingSignificant5097   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8mjz4/negative_explained_variance/</guid>
      <pubDate>Thu, 07 Mar 2024 05:29:18 GMT</pubDate>
    </item>
    <item>
      <title>测试时无法扩展自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxow/cant_scale_custom_environment_when_testing/</link>
      <description><![CDATA[我正在制作自定义Open AI Gym 中的 Boid 植绒环境具有稳定的基线 3.  ​ 错误1： 我已经创建了环境并测试了3个boids，但是当我测试10个boids时，它给出了错误： 错误 自定义环境 我从未对 boids 的数量进行硬编码，也没有其他明显的问题。 ​ 错误 2： 我用 20 个不同的初始化位置测试了模型，但对于大多数情况，我得到了这个奖励，600200，他们按预期移动。  但是，当我重新训练模型时，它的表现不佳，参数完全相似。这是一个训练有素的模型，是侥幸还是过度拟合？  剧集奖励  祝你有美好的一天！   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxow/cant_scale_custom_environment_when_testing/</guid>
      <pubDate>Thu, 07 Mar 2024 04:56:12 GMT</pubDate>
    </item>
    <item>
      <title>测试时无法扩展自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxop/cant_scale_custom_environment_when_testing/</link>
      <description><![CDATA[        由   提交/u/Sadboi1010   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxop/cant_scale_custom_environment_when_testing/</guid>
      <pubDate>Thu, 07 Mar 2024 04:56:12 GMT</pubDate>
    </item>
    <item>
      <title>无法理解 RL 代码中的大多数数学公式！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/</link>
      <description><![CDATA[在我的研究中，我认为在论文中计算公式很重要，这样我就可以更清楚地了解作者想要传达的内容以及主要贡献他们如何改进。我认为是时候学习更多数学来对强化学习进行更深入的研究了。  我认为强化学习和优化控制非常相似。那么也许学习一些凸优化和函数分析或者变异分析会很好？ 困惑我需要什么来提高我的 RL 知识，帮助！   由   提交/u/Tight-Ad789  /u/Tight-Ad789  reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/</guid>
      <pubDate>Wed, 06 Mar 2024 17:31:49 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定的基线训练两个竞争网络/策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b846eo/training_two_competing_networkspolicies_using/</link>
      <description><![CDATA[我的环境中，2 个以上的竞争代理根据不同的奖励函数有完全不同的目标。此外，这些目标会根据环境的情况而变化。我想训练代表“个性”的不同网络/策略。 一种方法是训练一个通用代理，该代理适用于在观察空间中编码的任何目标和奖励函数配置，但我希望这会增加网络的复杂性。 我希望另一种方法是同时进行 2 个以上的网络/策略训练。步骤/重置返回 {&#39;agent_0&#39;: [...], &#39;agent_1&#39;: [...]} 包含每个代理的观察和奖励，是否可以将一个用于网络 1，另一个用于网络 2 ？ 我正在使用 Stable Baselines 3 进行训练（使用 supersuit 转换 PettingZoo 环境，以便 sb3 可以在其上进行训练），目前训练是通过 sb3 抽象出来的：  env = ss.pettingzoo_env_to_vec_env_v1(env) env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=“stable_baselines3”) model = SAC(MlpPolicy, env) model.learn(total_timesteps=steps) model.save(run_name )  我想到的方法有意义吗？如果是这样，我如何以这种方式同时训练 2 个以上网络？   由   提交/u/davidschep  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b846eo/training_two_competing_networkspolicies_using/</guid>
      <pubDate>Wed, 06 Mar 2024 16:34:46 GMT</pubDate>
    </item>
    <item>
      <title>RL 在流体动力学与控制中的应用以及仿真软件选择的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b83n0h/questions_on_applying_rl_on_hydrodynamics_control/</link>
      <description><![CDATA[我正在写我的论文，该论文基本上围绕水下水翼艇展开（示例）和纵向（也可能是横向）运动的控制。主要思想是将其视为高度/俯仰/横滚控制问题（状态为水面以上高度、俯仰/横滚角度及其一阶导数），并实现 RL 作为制定控制算法的手段。 &lt; p&gt;没有真正的船来尝试算法，所以我们的想法是使用模拟环境并为其提供船、箔、支柱等参数/属性。我的问题基本上是：  使用基于模型的方法（我可以实现修剪点的相对简单的线性表示）而不是仅仅使用是否更好？暴力破解它吗？ 您是否知道任何可以与空气/流体动力学很好地配合的模拟环境（尽管我认为计算升力和阻力系数并将它们直接插入力会更精确）？我之前曾从事过 Unity3D 工作，我喜欢在游戏对象上轻松拖放脚本。我还听说过 Gazebo （特别是 VRX 或 UVSim 包）。  模拟环境可以无头运行吗？我认为运行飞行器的可视化表示会非常耗时。如果我可以关闭图形并只期望物理引擎运行刚体/海交互，那就更好了。  注释我感兴趣的模拟是稳定的迎头航向（无机动），并且仅当飞船处于箔伯恩状态时（无需模拟船体与水的相互作用）。  ​   由   提交 /u/John_Skoun   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b83n0h/questions_on_applying_rl_on_hydrodynamics_control/</guid>
      <pubDate>Wed, 06 Mar 2024 16:13:23 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器实际上提供开环控制吗？有些人也将其称为任务实例的操作序列。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7zdhd/is_the_decision_transformer_actually_providing/</link>
      <description><![CDATA[ 由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7zdhd/is_the_decision_transformer_actually_providing/</guid>
      <pubDate>Wed, 06 Mar 2024 13:13:29 GMT</pubDate>
    </item>
    <item>
      <title>罗纳德·威廉姆斯（REINFORCE，1992）上个月去世 (2024-02-16)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7pjar/ronald_williams_reinforce_1992_died_last_month/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7pjar/ronald_williams_reinforce_1992_died_last_month/</guid>
      <pubDate>Wed, 06 Mar 2024 03:30:58 GMT</pubDate>
    </item>
    </channel>
</rss>