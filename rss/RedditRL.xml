<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 14 Aug 2024 01:08:02 GMT</lastBuildDate>
    <item>
      <title>PPO 澄清</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ere8hc/ppo_clarification/</link>
      <description><![CDATA[在尝试实施 PPO 时，我对策略比率代表什么感到有些困惑：pi_theta/pi_theta_old。我不明白的是，我们如何计算 pi_theta，即新策略概率？我们尚未更新我们的策略，因为这是损失的重点（通过它进行反向传播并更新我们的策略），所以 pi_theta 不是=pi_theta_old 吗？这个损失与我们收集的数据有什么关系？    提交人    /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ere8hc/ppo_clarification/</guid>
      <pubDate>Tue, 13 Aug 2024 17:44:10 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习解决 NP-hard 图形问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ere13q/model_based_reinforcement_learning_to_solve/</link>
      <description><![CDATA[大家好，我目前正在研究基于模型的强化学习方法，用于解决 NP 难题 (DFJSP)，以图形建模。我实施了 MCTS，并进行了一些调整（渐进式加宽、修剪、RAVE）。我目前正在寻找更多方法，解决巨大的动作和状态空间、收敛时间和内存优化问题。最后，我想通过将原始 MCTS 与所做的修改进行比较来展示结果，并将它们与其他现有方法（也可能是无模型的）进行比较。该论文背后的想法是，鉴于问题的动态和不确定性，基于模型的强化学习将能够更好地适应动态事件并提高样本效率。你们对更高级的方法有什么想法吗？我考虑过使用 GAT 架构来估计价值和策略以及 PUCT。也许还有 Thompson 采样或 Gumbal 噪声用于选择。也许还有非 MCTS 方法？我愿意接受任何想法！     提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ere13q/model_based_reinforcement_learning_to_solve/</guid>
      <pubDate>Tue, 13 Aug 2024 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习模型没有学到任何东西（有用）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er8lbf/deep_q_learning_model_doesnt_learn_anything_useful/</link>
      <description><![CDATA[  由    /u/ZazaGaza213  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er8lbf/deep_q_learning_model_doesnt_learn_anything_useful/</guid>
      <pubDate>Tue, 13 Aug 2024 13:58:45 GMT</pubDate>
    </item>
    <item>
      <title>拥有专门的探索行动。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er6sgh/having_a_dedicated_action_for_exploration/</link>
      <description><![CDATA[大家好，我一直在使用 PPO 来学习我工作环境的预测模型。代码运行良好，但当然也存在一些探索问题。 我在想，既然我有一个模型，我可以用它来鼓励探索。在我做的这个问题的非策略版本中，我有一个贪婪的 epsilon，但我没有选择贪婪动作，而是根据正在学习的模型的另一个成本函数选择了一个动作。所以这是一种基于模型的无模型混合。 将其转化为 PPO 并不那么简单，因为动作选择是随机的。 我的问题是，如果有一个额外的动作，当被选中时，允许代理以直接鼓励探索而不是根据策略采取行动的方式选择其他动作之一，这会是奇怪还是不好的做法。这样，探索仍然被编码在策略中，但不是“直接”的？ 我的其他选择是使用环境模型中的一些度量（例如熵）来更新损失函数，这似乎最终会变得不稳定，使用某种 epsilon 贪婪选择选择一个探索性动作并给它当前 epsilon 的对数概率（或者通过网络的前向传递？？？） 我对数学的直觉还不是很好，想知道是否有人对这些选项有更具体的见解。谢谢！    提交人    /u/CC-Twip   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er6sgh/having_a_dedicated_action_for_exploration/</guid>
      <pubDate>Tue, 13 Aug 2024 12:37:52 GMT</pubDate>
    </item>
    <item>
      <title>MDP 与 POMDP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er21m5/mdp_vs_pomdp/</link>
      <description><![CDATA[尝试理解 MDP 和子程序，以便对 RL 有基本的了解，但事情变得有点棘手。根据我的理解，MDP 仅使用当前状态来决定采取哪种操作，而真实状态是已知的。然而在 POMDP 中，由于代理无法访问真实状态，因此它利用其观察和历史记录。 在这种情况下，如果 POMDP 使用来自历史记录的信息，即从先前观察（即 t-3，...）中检索到的信息，那么它如何具有马尔可夫特性（它甚至被称为 MDP）。 非常感谢你们！    提交人    /u/Internal-Sir-5393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er21m5/mdp_vs_pomdp/</guid>
      <pubDate>Tue, 13 Aug 2024 07:48:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO (SB3) 中的动作掩蔽。我如何才能真正掩蔽动作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eqkk4k/action_masking_in_ppo_sb3_how_can_i_actually_mask/</link>
      <description><![CDATA[因此，本质上我有一个多二进制环境，我不希望模型在该环境中再次重复所述操作。  我没有从网站找到太多帮助：stable-baselines3-contrib/docs/modules/ppo_mask.rst at master · Stable-Baselines-Team/stable-baselines3-contrib (github.com) 有人可以指导我/帮助我实施掩码吗？  尝试通过惩罚重复的选项来解决它似乎无法按预期工作。   由    /u/CampMaster69  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eqkk4k/action_masking_in_ppo_sb3_how_can_i_actually_mask/</guid>
      <pubDate>Mon, 12 Aug 2024 17:58:03 GMT</pubDate>
    </item>
    <item>
      <title>四足动物 RL 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eqhwra/quadruped_rl_question/</link>
      <description><![CDATA[嗨 我目前正在研究一个机器狗 RL 项目，目标是教它如何走路。 我正在使用 PPO，我有一个机器狗的 urdf 文件，我将其上传到 pybullet 进行训练，奖励函数包含以下内容： 学习率 = 1e-4 熵 = 0.02  奖励前进速度，-ve 奖励后退（前进是根据身体的前进方向，而不是一般的前进） 因使用过多能量而受到的能量惩罚 稳定性惩罚（因不稳定而受到的惩罚） 跌倒惩罚（因跌倒而受到的惩罚） 平滑度惩罚（因急剧改变速度而受到的惩罚） 对称性惩罚（因以对称形式行走而受到的奖励）  我尝试过这些奖励的尺度，有时会删除其中一些，只关注主要奖励，如前向和稳定性，但不幸的是，在大约 700k 步之后，代理没有学到任何东西；我只尝试了稳定性和前向奖励，我只尝试了前向奖励，我尝试了所有这些，其余奖励使用小权重，前向运动使用大权重。但模型仍然没有学到任何行为 当我大幅增加能量权重并使其主导奖励函数时，我得到的唯一反应是，在大约 300k 步之后，代理学会以更慢、更稳定的方式行走，但在 500k 之后它就停止移动了。这是可以理解的注意：我采用了行走缓慢、在 30 万步后保持稳定的模型，其奖励函数仅关注能量，我尝试将其用作迁移学习方法，我采用它然后在更完整的奖励函数上对其进行训练，该奖励函数具有前向运动奖励，但过了一段时间后，它又开始随机行为，并且变得不如开始时那么稳定 但是，我的问题是，在每次其他试验中我都没有看到任何效果，例如我没有看到模型向前移动但不稳定，或者我根本没有看到模型学习任何东西，它只是不断随机移动和下降并且我不认为 70 万步是一个短暂的训练期，我认为在此之后我至少应该看到任何细小的行为变化，不一定是积极的变化，但任何变化都给我提示下一步该尝试什么 注意：除了奖励函数之外，我没有尝试调整其他任何东西 如果有人知道任何事情，请帮忙   由    /u/youssef_naderr  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eqhwra/quadruped_rl_question/</guid>
      <pubDate>Mon, 12 Aug 2024 16:15:46 GMT</pubDate>
    </item>
    <item>
      <title>DeepMimic 中的提前终止实际上是如何运作的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eq8fak/how_early_termination_in_deepmimic_actually_works/</link>
      <description><![CDATA[在他们的代码中，他们使用提前终止后的所有样本，奖励为 0。即使在提前终止后，情节也会达到最大步骤。提前终止仅在触发后将奖励设置为 0。  DeepMimic - 6.2 提前终止 ... 一旦触发提前终止，角色在情节的剩余时间内将获得零奖励。这种提前终止的实例提供了另一种塑造奖励函数的方法，以阻止不良行为。 ...  奖励计算代码 [提前终止时的奖励为 0] 剧集重置条件代码 [提前终止不包含在条件中] 因此，我认为在触发提前终止后，情节实际上并没有结束。它看起来更像是一种奖励塑造技术，在意外情况下将奖励设置为 0。他们可能希望控制跌倒后的抖动奖励，并通过将奖励设置为零（这是可能的最低奖励）来强烈惩罚跌倒。 这是我的问题：实际结束情节比仅使用提前终止进行奖励塑造更好吗？你怎么看？    提交人    /u/Any_Way2779   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eq8fak/how_early_termination_in_deepmimic_actually_works/</guid>
      <pubDate>Mon, 12 Aug 2024 08:31:23 GMT</pubDate>
    </item>
    <item>
      <title>维拉·鲁宾巡天望远镜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epuchy/the_vera_rubin_survey_telescope/</link>
      <description><![CDATA[该望远镜每 3 天将拍摄一次整个夜空的照片，并将这些数据汇编成大约 60 PB 的信息。这是亚马逊当前包含的数据量的 15 倍。 他们要求我们帮助训练他们将用来识别星系的人工智能，然后让它自行完成过去 5 年的工作。 我觉得这太神奇了，你和我一样对此感到好奇吗？    提交人    /u/Sotomexw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epuchy/the_vera_rubin_survey_telescope/</guid>
      <pubDate>Sun, 11 Aug 2024 20:17:25 GMT</pubDate>
    </item>
    <item>
      <title>内在奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epsdkz/intrinsic_rewards/</link>
      <description><![CDATA[嘿，与原始 RND 论文相反，我发现大多数实现只是添加了外在和内在奖励......为什么？有人证明它足够好吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epsdkz/intrinsic_rewards/</guid>
      <pubDate>Sun, 11 Aug 2024 18:53:20 GMT</pubDate>
    </item>
    <item>
      <title>有人可以用 AlphaZero 和 DQN 作为例子解释一下基于模型和无模型的 RL 之间的区别吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eprsxu/can_someone_explain_the_difference_between/</link>
      <description><![CDATA[大家好， 我理解基于模型的方法（如 MCTS）需要模型来实际执行而不是仅仅估计未来状态和奖励，从而模拟未来状态和奖励。我还理解无模型方法（如 Q-Learning）不需要模型，而是只需执行操作并估计未来状态和奖励。 我不明白为什么 AlphaZero 被认为是基于模型的，而 DQN 被认为是无模型的。 AlphaZero 构建搜索树，使用 PUCT 平衡探索和利用。mcts 的推出部分被 ResNet 架构取代，用于估计策略和价值。这里如何涉及模型？ DQN 使用神经网络估计 q 函数值。与策略和价值估计的区别在哪里？如果 Alpha Zero 是基于模型的，为什么它是无模型的？    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eprsxu/can_someone_explain_the_difference_between/</guid>
      <pubDate>Sun, 11 Aug 2024 18:29:09 GMT</pubDate>
    </item>
    <item>
      <title>RL 是纯粹随机的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eppwzx/is_rl_purely_random/</link>
      <description><![CDATA[大家好，我想在这里复习一下我的 RL 基础知识，有个问题想问。 RL 中的训练，即寻找最优策略，它总是纯粹随机的吗？我们能否保证，如果我们的代理发现自己处于某种状态，它们是否会 100% 找到与找到不同起点的另一个代理相同的最优策略？还是它仅仅取决于环境？    提交人    /u/TittyMcSwag619   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eppwzx/is_rl_purely_random/</guid>
      <pubDate>Sun, 11 Aug 2024 17:09:33 GMT</pubDate>
    </item>
    <item>
      <title>NPG 的参数化不变性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epo03a/parametrization_invariance_of_npg/</link>
      <description><![CDATA[嗨，我正在研究自然策略梯度，但我不太明白 NPG 所谓的参数化不变性。显然，网络的参数化并不重要，梯度在策略空间中指向同一个方向。 我想我感到困惑的是，什么才算是一个网络到另一个网络的重新参数化？我假设两个网络必须具有相同数量的参数。为什么这种不变性会因原始策略梯度而失效？    提交人    /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epo03a/parametrization_invariance_of_npg/</guid>
      <pubDate>Sun, 11 Aug 2024 15:49:33 GMT</pubDate>
    </item>
    <item>
      <title>[MORL] 尝试掌握一种开发代码的直观方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epkgq5/morl_trying_to_grasp_an_intuitive_approach_for/</link>
      <description><![CDATA[几周以来，我一直试图通过阅读大量论文来了解 MORL。但问题是，即使阅读了这些论文，我也无法直观地掌握这种算法的工作原理。 假设我有一个 PPO 算法，它可以在许多环境中工作。在新的环境中，我想在多目标设置中使用它（当然具有不同的超参数）。我期望的是，让不同的代理学习不同的策略（我不考虑使用标量化函数的情况）。 所以现在我遇到了一个问题，我需要“权衡”不同的代理，以达到一个大目标（例如，使用末端执行器到达 3D 空间中的某个位置）。在这篇论文中，我发现了一个可能的概念：他们的代码返回一个 V 函数向量，每个代理一个。但他们使用 DQN 而不是 PPO。我的系统是连续的（观察、行动和奖励空间是连续的），我需要一个强大的算法来在我的环境中训练我的代理。 我查看了 GPI（我完全不理解）和 PGMORL（理论上更容易，但在实施中仍然存在问题）。最近，我想出了以下论文，描述了 MO-MPO。 但我正在寻找的是 MORL 实现的直观图景，以便能够理解这篇论文。遗憾的是，似乎关于这个主题的每篇论文都朝着不同的方向发展，因此很难得到一个具有统一理论的清晰图景（至少对我来说）。 是否有可能描述一种伪算法，使 MORL 实现在理论上如何工作更清晰？ 顺便说一句，我甚至找到了框架 AgileRL，它混合了进化技术的元素。遗憾的是，它非常接近 MORL，但并不完全相同。    提交人    /u/WilhelmRedemption   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epkgq5/morl_trying_to_grasp_an_intuitive_approach_for/</guid>
      <pubDate>Sun, 11 Aug 2024 13:11:21 GMT</pubDate>
    </item>
    <item>
      <title>绘制注意力图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epasze/plot_attention_map/</link>
      <description><![CDATA[      嗨！ 我最近读了一篇论文“仔细观察：弥合自我中心和我最近在“使用 Transformers 实现机器人操作的第三人称视角”上发表了一篇文章，对如何在没有 cls 标记的情况下绘制注意力图（如下图所示）感到好奇。 原始论文中的图 5 据我所知，绘制注意力图的大多数方法都是使用 cls 标记来绘制叠加图像。但是，本文中实现的视觉变换器不包含 cls 标记。 提前感谢您的时间和帮助！  这是官方实现的链接： https://github.com/jangirrishabh/look-closer.git    提交人    /u/UpperSearch4172   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epasze/plot_attention_map/</guid>
      <pubDate>Sun, 11 Aug 2024 03:13:41 GMT</pubDate>
    </item>
    </channel>
</rss>