<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 11 Nov 2024 09:18:41 GMT</lastBuildDate>
    <item>
      <title>使用 RL 玩 ARPG 的问题和想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</link>
      <description><![CDATA[我是一名计算机科学专业的最后一年学生，我想创建一个代理来玩我计划在 Unity 中制作的 ARPG。现在我想知道这是否可行，因为我需要概念验证直到 1 月。 我想指定代理执行的操作是导航关卡并在此过程中杀死一些敌人，关卡应该具有一定的复杂性，它不会是一条直线，但也不会是一个超级复杂的迷宫。最好它会有一些发散的路径。 现在，如果我在 Unity 中制作游戏，如果我当前的设置由 r5 5600 和 rtx 4070 12GB vram 组成，是否可以训练代理？它不会很快，但我可以让电脑全天候运行。 提前感谢您的回复。    提交人    /u/Marmotacuparlacur   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gonrvc/using_rl_to_play_an_arpg_question_and_ideas/</guid>
      <pubDate>Mon, 11 Nov 2024 08:42:01 GMT</pubDate>
    </item>
    <item>
      <title>在 QMIX 中，每个代理在像 SMAC 这样的环境中是否被忽略？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1goal6i/in_qmix_is_peragent_done_ignored_in_an/</link>
      <description><![CDATA[大家好！我想理解一个相当简单的问题，我正在查看 JaxMARL QMIX 代码，我注意到即使我们使用每个代理的单独完成状态来重置隐藏状态，这些完成状态也不会在计算 q 函数目标时使用，而只是整体环境完成：https://github.com/FLAIROx/JaxMARL/blob/main/baselines/QLearning/qmix_rnn.py#L477 有人能解释一下这是为什么吗？是不是因为我们已经通过考虑可用操作与不可用操作隐式地屏蔽了 q 值，当代理在本地完成但环境本身尚未终止时，这些操作将发生变化？    提交人    /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1goal6i/in_qmix_is_peragent_done_ignored_in_an/</guid>
      <pubDate>Sun, 10 Nov 2024 20:52:07 GMT</pubDate>
    </item>
    <item>
      <title>Decisions & Dragons：一个回答常见 RL 问题的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</link>
      <description><![CDATA[多年来，我在各种社交媒体平台上回答了很多强化学习问题。我决定是时候在自己的网站上收集和扩展它们了，我把这个网站叫做 Decisions and Dragons。 虽然这个网站是面向初学者的，但我认为它对更高级的从业者有帮助，可以作为对核心概念的复习。它推出了 8 个深入的答案，我将在未来添加它。 我不确定它会有多受欢迎，但我希望它至少能帮助你们中的一些人！ https://www.decisionsanddragons.com/    提交人    /u/Born_Preparation_308   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go4n7f/decisions_dragons_a_website_to_answer_common_rl/</guid>
      <pubDate>Sun, 10 Nov 2024 16:37:01 GMT</pubDate>
    </item>
    <item>
      <title>自定义卡牌游戏环境建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go23os/environment_recommendations_for_custom_card_game/</link>
      <description><![CDATA[我正在用 Python 实现一个纸牌游戏（自定义游戏，自定义规则）。它是双人、非零和、不完美信息。然后我想创建几个简单的代理，然后通过对更简单的代理进行训练来训练 RL 代理。 为此，我需要先用 Python 重新创建游戏。我应该从头开始编写代码（规则相当简单）还是有像 Gymnasium 这样的库对纸牌游戏有很好的支持？据我所知，Gymnasium “仅”适用于视频游戏（Atari 等），而不太适合纸牌游戏。 到目前为止，我已经能够找到 Gymnasium 和 OpenSpiel。    提交人    /u/n0stalghia   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go23os/environment_recommendations_for_custom_card_game/</guid>
      <pubDate>Sun, 10 Nov 2024 14:41:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q-Learning 帮助无人机自主穿越未知环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</link>
      <description><![CDATA[我们的任务是使用无人机覆盖未知区域并在搜索过程中确定关键点。我们假设了一种场景，即必须覆盖受灾地区，并希望确定幸存者。目前，我们将问题抽象为使用 2D 网格表示搜索区域，然后可视化无人机在其中移动的情况。 我们是强化学习的新手，不清楚如何在这种情况下使用 q-learning。当您试图一次性覆盖一个区域并且您不知道环境是什么样子，只知道要搜索的区域的边界时，q-learning 是否有效？当幸存者很可能只是随机分布时，它甚至可以学习什么样的模式？任何见解/指导都将不胜感激。    提交人    /u/naepalm7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1gkx/using_qlearning_to_help_uavs_autonomously/</guid>
      <pubDate>Sun, 10 Nov 2024 14:10:02 GMT</pubDate>
    </item>
    <item>
      <title>PPO 在 AirSim 中不起作用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go1f9v/ppo_doesnt_work_in_airsim/</link>
      <description><![CDATA[大家好， 我正在研究我的论文项目，在 AirSim 的邻里环境中实施 PPO 以进行汽车驾驶。代理需要学会很好地驾驶，所以我正在处理这种情况：我必须在连续空间中估计 1 个动作（固定速度）（动作范围从 -1 到 1，我使用正态分布来抽样有效动作）。我已经测试了各种方法好几天了，但是代理仍然无法有效地学习。 对于 PPO 算法，我从几个 GitHub 代码中获得了灵感，对于网络，我使用了以下结构： self.conv1 = Conv2D(32, (8, 8), strides=4, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.conv2 = Conv2D(64, (4, 4), strides=2, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.conv3 = Conv2D(64, (3, 3), strides=1, padding=&#39;valid&#39;, kernel_initializer=VarianceScaling(2.0,),activation=&#39;relu&#39;, use_bias=False)  self.flatten = Flatten()  self.ad1 = Dense(512,activation=&#39;relu&#39;)  self.ad2_mean = Dense(1,activation=&#39;tanh&#39;) self.val = Dense(1)  目前，我保持 std 固定，因为当我尝试估计它时，它会爆炸式增长到非常高的值。 我想问一下是否有人从事过类似的项目并能给我一些建议或指点！    提交人    /u/MonfoTibetano   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go1f9v/ppo_doesnt_work_in_airsim/</guid>
      <pubDate>Sun, 10 Nov 2024 14:08:13 GMT</pubDate>
    </item>
    <item>
      <title>RMSprop 方法应用于 Q 学习，实现自适应动态学习率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go0h35/rmsprop_approach_applied_to_qlearning_for/</link>
      <description><![CDATA[我正在实施受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是让学习率根据时间差 (TD) 误差的大小随时间进行调整。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减小。 具体来说： 我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但相反，它似乎在增长。因此，我的学习率（从 0.001 开始）并没有像预期的那样随着时间的推移而增加，而是低于预期甚至在下降。下面是我正在使用的 Q 学习更新函数：**python 复制代码**def update_q_table(self, state, action, reward, next_state): &quot;&quot;&quot;使用 Q 学习更新规则更新 Q 表。&quot;&quot;&quot; def update_q_table(self, state, action, reward, next_state): best_next_action = np.argmax(self.q_table[next_state, :]) td_target = reward + self.discount_factor * self.q_table[next_state, best_next_action] td_error = td_target - self.q_table[state, action] # 更新 RMSprop 的平方梯度移动平均值 E[g^2] self.gradient_Q[state, action] = ( self.beta * self.gradient_Q[state, action] + (1 - self.beta) * ((td_error) ** 2) ) self.learning_rate = self.initial_learning_rate/ (np.sqrt(self.gradient_Q[state, action]) + self.epsilon) self.learning_rate_history.append(self.learning_rate) # 使用固定学习率和 TD 误差更新 Q 值 self.q_table[state, action] += self.learning_rate * td_error # 存储 E[g^2] 值用于跟踪 self.gradient_history.append(self.gradient_Q[state, action])  我正在实现受 RMSprop 启发的动态学习率 Q 学习，遵循我在一篇文章中找到的方法。目标是根据时间差异 (TD) 误差的大小随时间调整学习率。但是，我遇到了一个问题，梯度似乎随着时间的推移而增加，而理想情况下，随着代理对环境的了解越来越多，梯度应该会减少。 具体来说： 我预计梯度（TD 误差）会随着 Q 值的收敛而逐渐减小，但事实并非如此，它似乎在增长。因此，我的学习率（从 0.001 开始）并没有像预期的那样随着时间的推移而增加，而是低于预期甚至下降。    提交人    /u/Busy-Acadia5601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go0h35/rmsprop_approach_applied_to_qlearning_for/</guid>
      <pubDate>Sun, 10 Nov 2024 13:18:31 GMT</pubDate>
    </item>
    <item>
      <title>PPO 和最后观察</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1go00r6/ppo_and_last_observations/</link>
      <description><![CDATA[在常见的 Python 演员-评论家代理实现中，例如 stable_baselines3 库中的实现，PPO 是否真的使用它从终端状态收到的最后一个观察值？例如，如果我们使用一个 PPO 代理，无论当前操作如何，它都会在 n 步后终止 MDP 或 POMDP（这意味着终端状态仅取决于步数，而不取决于操作选择），PPO 是否仍会在计算中使用最后一个观察值？ 如果 n=1，PPO 是否本质上像一个情境老虎机，因为它从观察开始，并在单步情节中立即以奖励结束？    提交人    /u/Krnl_plt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1go00r6/ppo_and_last_observations/</guid>
      <pubDate>Sun, 10 Nov 2024 12:53:26 GMT</pubDate>
    </item>
    <item>
      <title>语音合成中的微调与迁移学习 - INGOAMPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnz44s/finetuning_vs_transfer_learning_in_voice/</link>
      <description><![CDATA[        由    /u/Potential_Arrival326  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnz44s/finetuning_vs_transfer_learning_in_voice/</guid>
      <pubDate>Sun, 10 Nov 2024 11:59:21 GMT</pubDate>
    </item>
    <item>
      <title>语音合成中的微调与迁移学习 - INGOAMPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnz3ej/finetuning_vs_transfer_learning_in_voice/</link>
      <description><![CDATA[        由    /u/Potential_Arrival326  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnz3ej/finetuning_vs_transfer_learning_in_voice/</guid>
      <pubDate>Sun, 10 Nov 2024 11:57:59 GMT</pubDate>
    </item>
    <item>
      <title>刚刚发现了很棒的 Python 强化学习库！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gntun0/just_discovered_great_reinforcement_learning/</link>
      <description><![CDATA[大家好！ 我最近偶然发现了一个名为 DeepRL (deeprlearn) 的强化学习库，我认为值得一试。它专为中小型 RL 项目而设计，并提供一些简洁的功能，使其既灵活又可扩展。它基于 PyTorch，因此如果您已经将 PyTorch 用于其他项目，那么这应该会非常直观。 以下是它提供的一些亮点：  动态规划算法：开箱即用地实现价值迭代和策略迭代代理。 奖励塑造：带有内置策略，如基于潜力的塑造、基于距离的塑造等。非常适合像 FrozenLake 或 MountainCar 这样奖励稀疏的环境。 函数近似：包括用于 RBF 核、多项式特征甚至神经网络的工具（如果您想走这条路）。 Gymnasium 集成：与 Gym 环境无缝协作。 进度跟踪：一种方便的详细模式，用于监控训练奖励、步骤和探索率。 模型保存/加载：可以非常轻松地保存和重用经过训练的代理。  我一直在尝试它，到目前为止，我已经在 FrozenLake-v1（防滑）上训练了一个价值迭代代理。它出奇的简单，详细模式使监控进度变得容易。 该库仍在不断发展，但它对贡献持开放态度。如果您正在寻找用于 RL 实验的轻量级但功能强大的东西，那么这可能是一个不错的选择！ GitHub Repo：deeprl 如果您尝试过或有任何反馈，请告诉我。看到更多人尝试这个会很棒！    提交人    /u/Traditional-Rate8550   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gntun0/just_discovered_great_reinforcement_learning/</guid>
      <pubDate>Sun, 10 Nov 2024 05:46:16 GMT</pubDate>
    </item>
    <item>
      <title>安全、无幻觉的 AI 编码提案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnsq65/a_proposal_for_safe_and_hallucinationfree_coding/</link>
      <description><![CDATA[我写了一篇论文《安全且无幻觉的 AI 编码提案》(https://gasstationmanager.github.io/ai/2024/11/04/a-proposal.html)，其中我提议就一项研究议程开展开源合作，我相信这最终将导致编码具有超人级能力、无幻觉且安全的 AI。 强化学习，尤其是 AlphaZero，是我提出的解决方案的一部分。但 AlphaZero 通常在容易获得基本事实的领域效果很好，比如围棋和国际象棋……我提出了一种将代码生成问题表述为可以根据基本事实验证候选解决方案的方法。  欢迎发表评论！如果您有兴趣探索强化学习或该计划的其他方面的想法，请告诉我！     提交人    /u/Admirable_Sorbet_544   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnsq65/a_proposal_for_safe_and_hallucinationfree_coding/</guid>
      <pubDate>Sun, 10 Nov 2024 04:37:28 GMT</pubDate>
    </item>
    <item>
      <title>一些 RL 算法的分类</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gnd86b/classification_of_some_rl_algorithms/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gnd86b/classification_of_some_rl_algorithms/</guid>
      <pubDate>Sat, 09 Nov 2024 16:01:07 GMT</pubDate>
    </item>
    <item>
      <title>适用于 2 个以上代理的 AI 扑克健身房环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn9nux/ai_poker_gym_environment_for_more_than_2_agents/</link>
      <description><![CDATA[大家好，我是一名计算机科学专业的学生，​​我想为我的 AI 课程期末项目做一个 AI 扑克锦标赛。我的想法是让 4/5 个不同的代理都使用不同的 RL 算法进行训练，让它们互相玩扑克，看看谁赢。我找到了几个不同的扑克环境，但它们都是针对 2 个代理的。有人知道任何能够与 &gt;3 个代理一起工作的环境吗？任何帮助或建议如何使我的项目更好都将不胜感激。    提交人    /u/Livid-Ant3549   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn9nux/ai_poker_gym_environment_for_more_than_2_agents/</guid>
      <pubDate>Sat, 09 Nov 2024 13:03:55 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该先将我的 RL 论文提交给 arXiv 以保护新颖性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn6y6f/should_i_submit_my_rl_paper_to_arxiv_first_to/</link>
      <description><![CDATA[大家好！ 我一直在努力改进强化学习算法，并且取得了一些不错的成果，很高兴与大家分享。在准备撰写论文时，我想知道在将其发送到机器学习期刊之前，是否最好先将其提交给 arXiv。我主要关心的是确保我的研究的新颖性得到保护，因为我听说在 arXiv 上发表文章有助于确定贡献的时间戳。 所以，我很想知道：  在强化学习研究中，先在 arXiv 上发表文章，然后再提交给期刊，这是否是一种常见的惯例？ 在 arXiv 上发表文章真的有助于保护研究的新颖性吗？  在向期刊提交文章之前，我有什么理由避免在 arXiv 上发表文章吗？  任何经历过这个过程或有 RL 出版经验的人的建议都会非常有帮助！提前谢谢了！😊    提交人    /u/Tonight223   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn6y6f/should_i_submit_my_rl_paper_to_arxiv_first_to/</guid>
      <pubDate>Sat, 09 Nov 2024 10:05:06 GMT</pubDate>
    </item>
    </channel>
</rss>