<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sun, 02 Mar 2025 09:16:52 GMT</lastBuildDate>
    <item>
      <title>使用DQN帮助解决山车问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好， 在开始之前，我想道歉，问问这个问题，因为我猜想这个问题可能已经被问到很多次了。我试图教自己增强学习，并且正在研究这款MountrainCar迷你项目。  我的模型似乎根本没有融合。我使用情节持续时间与情节编号的情节来检查/分析性能。我注意到的是，有时，对于我尝试过的所有架构，情节持续时间都会有所减少，然后再次增加。  I have tried doing the following things:  Changing the architecture of the Fully Connected Neural network. Changing the learning rate Changing the epsilon value, and the epsilon decay values.  For neither of these changes, I got a model that seems to converge during training.我平均培训了1500个持续时间。这就是每个模型通常看起来的图：   有没有适合此特定问题的技巧，特定的DQN体系结构和超参数范围？还有一组指南，应该牢记并用来创建这些DQN模型？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/lowkeysuicidal14     [link]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</guid>
      <pubDate>Sun, 02 Mar 2025 04:17:05 GMT</pubDate>
    </item>
    <item>
      <title>帮助2D峰搜索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1fdsa/help_with_2d_peak_search/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我使用不同的体育馆环境有很多RL经验，使用SB3，CleanRl以及我自己实施的算法，获得了相当不错的性能。这就是为什么我对一个事实感到恼火的是，我似乎无法在玩具问题上取得任何进展，我已经为我的工程领域中的某些优化任务实现了RL。 这个问题本质上是一个优化问题，在该问题中，该代理的任务是在2D空间中找到最佳的启动参数（对于启动器中的最佳参数），以便以启动以进行启动，以供某些参数，以供一些参数，以供一些参数，以供某些参数，以便以7个访问量。在所使用的一组参数上的分布是值的，有些不连续性，这就是为什么我制作了一个玩具环境的原因，在每个情节中，都会产生测量值的高斯分布，具有不同的均值和协方差。该代理的任务是选择一组值，范围从0-36，使用CNN策略使SB3实现更简单，然后以该集的参数集以分布值的形式接收反馈。状态空间是测量值的2D图像，所有初始值均设置为0，随着代理探索的填充。我正在使用的动作空间是一个多discrete空间，[0-36，0-36，0-1]，最后一个动作是代理是否认为这套参数是最佳的。我尝试使用PPO和A2C，而性能差异很小。 现在，问题是取决于我如何构造奖励的方式，我无法找到最佳的参数集。提供1个以找到正确参数的反馈的天真方法通常会失败，这可以通过在此环境中的随机策略的相当稀疏的奖励来解释。因此，我试图为每项动作提供增量奖励，该动作会根据上次动作进行改进，这取决于分布的值或距离到最佳的距离，如果实际上找到了峰值，则具有很大的奖励。这有点好，但是代理商总是为了一项政策而定，即它在山上中途走了一半，然后就解决了，再也没有找到实际的峰值。我没有对进行大量测量的任何惩罚（现在），以便代理商可以进行详尽的搜索，但这永远不会做到这一点。  在我如何设置环境或结构奖励的方式中，我是否缺少什么？我是否可以研究类似的项目或纸张？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j1fdsa/help_with_with_2d_peak_search/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1fdsa/help_with_2d_peak_search/</guid>
      <pubDate>Sun, 02 Mar 2025 01:30:54 GMT</pubDate>
    </item>
    <item>
      <title>如何将RL与刚体机器人与流体相互作用的刚性机器人集成？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j17hl3/how_to_integrate_rl_with_rigid_body_robots/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想使用强化学习来教2-3个链接机器人鱼游泳。机器人鱼是一个三维固体物体，它将感觉到各个方面的水力。什么模拟器将有用，以便我可以对刚体机器人和周围的流体力之间的相互作用进行建模？  我需要它能够将RL集成到其中。与基于CFD的模拟（COMSOL，ANSYS，基于FEM等）不同，它也应该很快呈现物理学。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kingalvez     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j17hl3/how_to_integrate_rl_with_rigid_body_robots/</guid>
      <pubDate>Sat, 01 Mar 2025 19:25:09 GMT</pubDate>
    </item>
    <item>
      <title>帮助交易的Q学习模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j14faj/help_with_qlearning_model_for_trading/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好， 我已经使用健身房环境实现了一个Q学习的交易机器人，但我注意到一些奇怪的（至少对我来说）结果。在训练了1500集的Q桌子之后，特定股票的市场回报率为156％，而投资组合收益（由Q-table策略生成）是极高的 76,445.94％，这对我来说似乎是不现实的。 Could this be a case of overfitting or another issue? When testing, the results are:  Market Return: 33.87% Portfolio Return: 31.61%  我还拥有每集总奖励的情节，并在情节中累积了奖励： 如有必要，我可以共享我的代码，以便有人可以帮助我解决这个问题。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ligabo69     [link]       [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j14faj/help_with_qlearning_model_for_trading/</guid>
      <pubDate>Sat, 01 Mar 2025 17:14:34 GMT</pubDate>
    </item>
    <item>
      <title>LLM微调的分布式RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0ze7a/distributed_rl_for_llm_finetuning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在为使用 ray  and  unsploth  进行  的仍然是一个进步的工作，但是我很高兴   forng&gt; strong&gt; strong&gt; strong&gt; pros for fork it fors fors fors form forme，如果您有兴趣，请查看！提交由＆＃32; /u/sedidrl     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0ze7a/distributed_rl_for_llm_finetuning/</guid>
      <pubDate>Sat, 01 Mar 2025 13:24:55 GMT</pubDate>
    </item>
    <item>
      <title>离线RL算法对10^-6的奖励敏感？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0wyol/offline_rl_algorithm_sensitive_to_perturbations/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在在D4RL基准测试台上数据集（特别是Hopper重播数据​​集）上运行离线RL算法（特别是隐式Q学习）。我看到，奖励的小扰动，以10^-6的顺序导致训练结果截然不同。当然，这是所有东西上的固定种子。  我知道RL在许多方面（超参数，模型体系结构，奖励等）中的小扰动可能非常敏感。但是，这对我的奖励的变化很敏感，这使我感到惊讶。对于那些具有更多实施这些算法的经验的人，您认为这是期望的吗？还是会暗示算法实现出了问题？ 如果有些预期，这是否会引起疑问，这在离线RL中已发表的许多已发表的工作吗？例如，您可以修复种子和超级参数，但是随后在CUDA与CPU上运行奖励模型可以导致奖励值的差异为10^-6    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used-eagle-9302     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0wyol/offline_rl_algorithm_sensitive_to_perturbations/</guid>
      <pubDate>Sat, 01 Mar 2025 10:53:49 GMT</pubDate>
    </item>
    <item>
      <title>提高样品效率的最有希望的技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0rgu4/most_promising_techniques_to_improve_sample/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我知道的几个是mbrl，模仿学习（逆RL）。还有其他良好的研究领域重点是解决样本效率的提高吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0rgu4/most_promising_techniques_to_improve_sample/</guid>
      <pubDate>Sat, 01 Mar 2025 04:41:26 GMT</pubDate>
    </item>
    <item>
      <title>rllama🦙-教学语言模型，带有内存的RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0hgm1/rllama_teaching_language_models_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家， 我想分享一个与LLM微调实验中出现的项目。在使用[ llamagym ]并遇到了一些内存管理挑战后，我开发了   rllama      （[[[[[[[ 核心思想是改善模型在训练过程中如何保留和利用经验。可配置的压缩策略 快速启动😼🦙  python3：pip安装rllama  我特别感兴趣的是听到有关：   - 替代记忆体系结构     - 潜在的      - 潜在的  - 潜在的  -  p&gt;  -  p&gt;  -  pervormation-performance optimions    pocitiond  sode  sodeed sode and soper and sope and and（Inde）。请随时贡献或建议改进 - 欢迎PR和问题！  [评论中有兴趣的人的实施详细信息]   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/cheenchann     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0hgm1/rllama_teaching_language_models_with/</guid>
      <pubDate>Fri, 28 Feb 2025 20:29:19 GMT</pubDate>
    </item>
    <item>
      <title>如果我有一个巨大的数据集，我应该选择什么选择重播缓冲区？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_i_go_for_if_i/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在实现自动缓存内存管理的RL模型，并且我的数据集的示例处于以下形式（状态，操作，奖励）。我的数据集相当庞大（我们正在谈论数万亿个数据示例）。从我的未知数中，我们首先将数据集洗净，然后将其加载到重播缓冲区（这是数据集大小合理的情况）。  对于我的情况，我正在使用iterabledataSet和pytorch的数据加载器（https://pytorch.org/tutorials/beginner/basics/data\_tutorial.html) and basically it treats my data as a large stream of info so it&#39;s not loaded into memory立即导致开销。我的问题是，在这种情况下，将整个数据集加载到重播缓冲液中并不可行，那么这里最好的方法是什么？而且有很多类型的重播缓冲区，所以哪一种是我的情况最适合使用的？提交由＆＃32; /u/u/saffarini9     [links]      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_should_should_go_go_for_if_if_if_i/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_i_go_for_if_i/</guid>
      <pubDate>Fri, 28 Feb 2025 13:18:51 GMT</pubDate>
    </item>
    <item>
      <title>如何计算L_CLIP的梯度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j055j8/how_to_compute_the_gradient_of_l_clip/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我最近阅读了有关PPO的信息，但是我还没有理解如何得出梯度，因为在算法中，剪辑行为取决于R_T（Theta）（theta），这是未知的。最好的方法是什么？我听说某种迭代可以实施，但我尚未理解。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/purplebumblebee5620     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j055j8/how_to_compute_the_gradient_of_l_clip/</guid>
      <pubDate>Fri, 28 Feb 2025 11:02:00 GMT</pubDate>
    </item>
    <item>
      <title>PPO重置每个时间步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j00jqi/ppo_resets_every_timestep/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  编辑：已解决 - 问题是从我用来生成观测值的软件包中返回的截断变量中的问题。 原始帖子： 是什么可以实现这一目标？我是RL的新手，但是我已经在数据科学领域工作了几年，所以我希望我只是缺少简单的东西。 我正在使用MultiiinputPolicy运行单个ENV。使用.learn（），Env在开始时重置，一步，再次重置，然后继续此周期，直到完成时间段。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j00jqi/ppo_resets_every_timestep/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j00jqi/ppo_resets_every_timestep/</guid>
      <pubDate>Fri, 28 Feb 2025 05:32:22 GMT</pubDate>
    </item>
    <item>
      <title>从RL Newbie到重新实现PPO：我的学习冒险</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izv5zs/from_rl_newbie_to_reimplementing_ppo_my_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我是一名CS学生，大约一年前开始潜入ML和DL。直到最近，RL才是我还没有探索太多的东西。我唯一的经验是在拥抱Face的TRL实现中，以将RL应用于LLM，但老实说，我当时不知道自己在做什么。 很长一段时间以来，我认为RL一直在恐吓 - 就像这是深度学习的最终高峰。对我来说，所有最酷的突破，例如Alphago，Alphazero和Robotics，似乎都与RL息息相关，这使它感到遥不可及。但是随后DeepSeek发行了GRPO，我真的很想了解它如何运作和跟随论文。这引发了一个想法：两周前，我决定通过重新进化一些核心RL算法来启动一个项目，以从头开始建立我的RL知识。 到目前为止，我已经解决了一些。我从DQN开始，这是我迄今为止重新实现的唯一基于价值的方法。然后，我继续进行策略梯度方法。我的第一个尝试是使用奖励前进的基本增强算法的香草政策梯度。我还为此添加了评论家，因为我看到两种方法都是可能的。接下来，我接受了TRPO，这是迄今为止最难实施的。但是，通过它为我带来了一个真正的“尤里卡”时刻 - 我终于掌握了监督学习与RL的优化之间的根本差异。即使由于二阶方法的成本，TRPO不再被广泛使用，但我强烈建议将其重新实现为任何学习RL的人。这是构建直觉的好方法。 现在，我刚刚完成了PPO的重新进化，这是那里最受欢迎的算法之一。我选择了剪辑版本，尽管在TRPO之后，KL-Divergence版本对我来说更为直观。我一直在简单的控制环境上测试这些算法。我知道我可能应该尝试一些更复杂的事情，但是这些倾向于训练。 老实说，这个项目使我意识到RL甚至有效。以乒乓球为例：在培训的早期，您的政策很糟糕，每次都会失去。它需要20个步骤（4帧跳过），只是将球从一侧转到另一侧。在这20个步骤中，您将获得19个零，也许是+1或-1奖励。这种稀疏性是疯狂的，令人震惊的是，它最终会弄清楚事情。 接下来，我打算在将重点转移到连续的动作空间之前实施GRPO，到目前为止，我只与离散的动作合作，所以我很高兴能探索这一点。我还坚持基本的MLP和Convnets的政策和价值功能，但是我正在考虑尝试为连续动作空间进行扩散模型。他们看起来很自然。展望未来，我很想在我很快完成上学后尝试一些机器人项目，并为这样的附带项目提供更多空闲时间。 我的大外卖？ RL并不像我想象的那么恐怖。大多数主要的算法可以很快地在单个文件中重新完成。就是说，培训是一个完全不同的故事 - 由于RL铲球的性质，它可能令人沮丧和恐吓。对于这个项目，我依靠Openai的旋转指南和每种算法的原始论文，这非常有帮助。如果您很好奇，我一直在用回购工作，称为“ rl-arena”。去！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/awkward-can-8933      [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izv5zs/from_rl_newbie_to_reimplementing_ppo_my_learning/</guid>
      <pubDate>Fri, 28 Feb 2025 00:38:30 GMT</pubDate>
    </item>
    <item>
      <title>“培训语言模型，用于通过多机构增强学习学习”，Sarkar等2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/gwern       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</guid>
      <pubDate>Thu, 27 Feb 2025 16:59:12 GMT</pubDate>
    </item>
    <item>
      <title>国际象棋样品效率人与sota rl</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</link>
      <description><![CDATA[From what I know, SOTA chess RL like AlphaZero reached GM level after training on many more games than a human GM played throughout their lives before becoming GM Even if u include solved puzzles, incomplete games, and everything in between, humans reached GM with much lesser games than SOTA RL did (pls correct me if I&#39;m wrong about这）。 是否有比人类效率较低的特定原因/障碍？对于提高国际象棋SOTA RL样本效率的研究是否有希望？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:40:39 GMT</pubDate>
    </item>
    <item>
      <title>离线RL的动作将是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  所以，我是RL的新手，我必须实现离线RL模型，然后在在线RL阶段中微调它。从我的承诺中，离线学习阶段最初的策略和在线学习阶段将使用实时反馈来完善政策。对于离线学习阶段，我将有一个数据集d = {（si，ai，ri）}。数据集中每个示例的操作是否是收集数据时采取的动作（即专家行动）？还是所有可能的动作？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]    ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:03:21 GMT</pubDate>
    </item>
    </channel>
</rss>