<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 09 May 2024 01:00:31 GMT</lastBuildDate>
    <item>
      <title>努力从头开始实施 PPO。 （健身房）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnhcoa/struggling_with_ppo_from_scratch_implementation/</link>
      <description><![CDATA[过去 5 个月我一直致力于从头开始实施 PPO。除了 numpy 等数值计算库之外，我大部分工作都是从头开始做的。它从监督学习网络开始到现在。我似乎无法理解。我读过的每篇论文都是 A. 过时/不正确 B. 不完整。没有论文完整描述它们的作用以及它们使用的超级参数。我尝试阅读 SB3 代码，但它与我的实现太不同了，我只是不明白发生了什么，因为它只有这么多文件，我找不到细节。所以我只是想发布我的后向方法，如果有人想阅读它并告诉我一些错误/建议。会很好！旁注：我使用标准梯度下降进行了优化，而批评家只采用状态。我没有使用 GAE，因为我试图最大限度地减少潜在的故障点。所有超参数都是标准值。 def back(self): T = len(self.trajectory[&#39;actions&#39;]) for i in range(T): G = 0 for j in range(i, T): current = self.trajectory[&#39;rewards&#39;][j] G += current * pow(self.gamma, j - i) # G = np.clip(G, 0, 15) # CRITIC STUFF if np.isnan(G):break state_t = self.trajectory[&#39;states&#39;][i] action_t = self.trajectory[&#39;actions&#39;][i] # 计算state_t的批评值critic_value = self.critic(state_t) # print(f&quot;Critic: {critic_value}&quot;) # print(f&quot;G: {G}&quot;) # 计算状态-动作对的优势 Advantages = G -ritic_value # print(f&quot;&quot;&quot;&quot;Return: {G} # 预期回报：{critic}&quot;&quot;&quot;) # 旧参数内容 new_policy = self.forward(state_t, 1000) # PPO 内容 Ratio = new_policy / action_t Clipped_ratio = np.clip(ratio, 1.0 - self .clip, 1.0 + self.clip) surrogate_loss = -np.minimum(ratio * 优势, Clipped_ratio * 优势) # entropy_loss = -np.mean(np.sum(action_t * np.log(action_t), axis=1)) # 参数向量weights_w = self.hidden.weights.flatten()weights_x = self.hidden.bias.flatten()weights_y = self.output.weights.flatten()weights_z = self.output.bias.flatten()weights_w = np .concatenate((weights_w,weights_x))weights_w = np.concatenate((weights_w,weights_y)) param_vec = np.concatenate((weights_w,weights_z)) param_vec.flatten() loss = np.mean(surrogate_loss) # + self. l2_regularization(param_vec) # print(f&quot;loss: {loss}&quot;) # 反向传播 next_weights = self.output.weights self.hidden.layer_loss(next_weights, loss, tanh_derivative) self.hidden.zero_grad() self.output.zero_grad () self.hidden.backward() self.output.backward(loss) self.hidden.update_weights() self.output.update_weights() self.critic_backward(G)  &lt; !-- SC_ON --&gt;  由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnhcoa/struggling_with_ppo_from_scratch_implementation/</guid>
      <pubDate>Wed, 08 May 2024 22:26:26 GMT</pubDate>
    </item>
    <item>
      <title>MARL 零和游戏中的自我博弈与双重预言机</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnei6i/self_play_vs_double_oracle_in_marl_zerosum_games/</link>
      <description><![CDATA[据我所知： 在自我对弈中，代理直接与自己的版本（历史或当前副本）对弈以改进其策略。基本上计算对“迄今为止的一切”的最佳响应。另一方面，双预言机算法涉及为两个代理维护显式策略集，这些策略集会逐步扩展。但扩展是相似的。两者都针对对方代理的模型和过去模型的混合策略计算最佳响应，并将这个新模型添加到自己的模型列表中。 我理解得对吗？我见过的关于这些概念的论文坦率地说对我来说太复杂了，但似乎底层原理并没有那么深奥。也许我在理解这些算法时遗漏了一些重要的东西？ 在使用函数逼近器（如神经网络）的情况下，我们何时停止在两种情况下寻找最佳响应？ 一个比另一个好吗？什么是权衡？也许一个人更有可能收敛到纳什均衡？    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnei6i/self_play_vs_double_oracle_in_marl_zerosum_games/</guid>
      <pubDate>Wed, 08 May 2024 20:27:14 GMT</pubDate>
    </item>
    <item>
      <title>具有重放缓冲区的 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cndw4j/rl_algorithms_with_replay_buffers/</link>
      <description><![CDATA[什么是具有重播缓冲区但适用于离散动作空间（除了 DQN）的 RL 算法？   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cndw4j/rl_algorithms_with_replay_buffers/</guid>
      <pubDate>Wed, 08 May 2024 20:01:35 GMT</pubDate>
    </item>
    <item>
      <title>DDPG - 输出问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnbp86/ddpg_issues_with_the_output/</link>
      <description><![CDATA[大家好， 所以我正在训练我的 DDPG 算法以进行 AP 选择，其中模型或动作的输出是基于移动代理的某些因素的 AP ID 列表，即基本上是一个向量。最初，该模型在探索总共 4 个 AP 的所有 AP 组合方面做得相当好，如下所示：输出：[2, 2, 3, 2, 1, 4] 但是那么在120集左右之后，输出总是在1和2的选择之间，即输出：[2, 1, 1, 1, 1, 2]而且它甚至没有选择这些AP基于其他参数，因为这些 AP 显然不是最佳选择。我尝试将 OUNoise max_sigma 增加到 4.0（我知道它非常高），但它在剧集图表的平均奖励中产生峰值。 OUNoise(action_space=self.action_size, max_sigma =10.0，min_sigma=0.01，decay_period=1000000)  我也尝试将学习率降低为= 0.01。我什至为输出冗余设置了负奖励，但它不起作用。正如你所看到的，这是我第一次实施 DDPG 或 DRL，我很恐慌！有人可以告诉我我做错了什么吗？   由   提交 /u/NecessaryThat2571   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnbp86/ddpg_issues_with_the_output/</guid>
      <pubDate>Wed, 08 May 2024 18:29:07 GMT</pubDate>
    </item>
    <item>
      <title>对高斯策略的策略梯度的质疑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cna10j/doubt_about_policy_gradient_with_gaussian_policy/</link>
      <description><![CDATA[      嗨， 我正在观看 YouTube 上的 Sergey Levine 课程。  我现在使用策略梯度，到目前为止零问题，但一些数学表达式让我感到困惑。 在此视频中 https://youtu.be/VSPYKXm_hMA?si=WIdrh41TX8RHXYu3&amp;t=238 at 3:58 他使用了以下等式： https://preview.redd.it /dl9qxk3vj8zc1.png?width=426&amp;format=png&amp;auto=webp&amp;s=b6f322d291b88dd3627cd16812bb0532fa684fcb 我不明白为什么对数概率等于这个距离以及如何计算梯度这个的。  谢谢。   由   提交 /u/RikoteMasterrrr   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cna10j/doubt_about_policy_gradient_with_gaussian_policy/</guid>
      <pubDate>Wed, 08 May 2024 17:18:37 GMT</pubDate>
    </item>
    <item>
      <title>问题：DQN 纸牌游戏让代理了解他可以玩哪些纸牌是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cn19em/question_dqn_card_game_does_it_make_sense_to_have/</link>
      <description><![CDATA[我正在尝试训练 AI 使用 DQN 玩纸牌游戏。 相关规则的简短摘要概述纸牌游戏： ~40 张纸牌，4 名玩家。每个玩家获得 10 张随机牌。进行 10 轮，每个玩家打出 1 张牌（连续）。现在，您并不总是被允许打出手中的每张牌，因为这很大程度上取决于该轮中已经打出的牌。 我目前正在做的是在我的神经网络中输出 40 个动作（40 张牌），然后从所有 40 张中取出价值最高的一张。如果这张牌不在特工手中，或者在给定情况下玩不合法，我就会取消游戏，给它一个不好的奖励，然后继续下一个。  现在我想知道这是否有意义，或者与在给定情况下从所有法律行动中选择具有最高价值的行动相比是否没有任何好处，这样我会显然，不必学习有关哪些牌是合法的，以及代理人手里有哪些牌的所有规则。 （我现在使用的状态是：handcards（40），cardsonboard（40）。One Hot Encoded） 我真的没有看到我当前的做法有任何优势，我只是不确定是否从所有操作的不断变化的子集中选择代理操作时，在训练过程中可能会出现任何问题吗？   由   提交 /u/TratanusII   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cn19em/question_dqn_card_game_does_it_make_sense_to_have/</guid>
      <pubDate>Wed, 08 May 2024 10:29:44 GMT</pubDate>
    </item>
    <item>
      <title>PID 类型 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cmvqqk/pid_type_rl/</link>
      <description><![CDATA[大家好， 我正在尝试用 stable_baselines3 的 RL 替换 PID（对于初学者来说似乎是最简单的） ）。一切都工作正常，但无论我使用什么算法或策略，我的操作总是非常嘈杂，并且不适用于我的真实系统。我的系统非常慢（大约 200 步才能得到响应）。 有人对算法、策略或如何配置超参数以实现缓慢且连续的操作有一些建议吗？我尝试配置奖励来平静行动，但即使我试图鼓励最后行动与实际行动之间的差异较小，它总是以饱和到最小值或最大值结束。 非常感谢您救命……我快要疯了！   由   提交/u/dos145  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cmvqqk/pid_type_rl/</guid>
      <pubDate>Wed, 08 May 2024 04:15:59 GMT</pubDate>
    </item>
    <item>
      <title>有街道行人行为模拟器吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cmpv69/is_there_a_simulator_for_street_pedestrian/</link>
      <description><![CDATA[我有一辆带摄像头的遥控汽车，我想知道是否可以针对街上行人的行为训练强化学习策略在模拟器中，然后在现实世界中尝试。   由   提交/u/mymooh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cmpv69/is_there_a_simulator_for_street_pedestrian/</guid>
      <pubDate>Tue, 07 May 2024 23:18:58 GMT</pubDate>
    </item>
    <item>
      <title>MPE 简单利差基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cmebnq/mpe_simple_spread_benchmarks/</link>
      <description><![CDATA[MARL PettingZoo 环境“Simple Spread”有明确的基准测试结果吗？ 对此我只能找到类似的论文Papoudakis 等人的“合作任务中的多代理深度强化学习算法基准测试”。 （https://arxiv.org/abs/2006.07869），其中作者报告了非常大的负面奖励（平均约为 - 130）对于 Simple Spread，3 个特工的“最大剧集长度为 25”。  据我了解，这是不可能的，因为通过我的测试，我发现这个数字应该低得多（小于-100），因此我很难理解论文中的结果。考虑到我将剧集结束奖励计算为 3 个特工的不同奖励之和。 我对此有什么误解吗？或者也许还有其他基准需要考虑？ 如果这是一个非常愚蠢的问题，我提前道歉，但我已经在这个问题上坐了一段时间而没有理解...... &lt; /div&gt;  由   提交/u/blrigo99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cmebnq/mpe_simple_spread_benchmarks/</guid>
      <pubDate>Tue, 07 May 2024 15:15:09 GMT</pubDate>
    </item>
    <item>
      <title>如何制定强化学习的预测区间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cmcvfl/how_do_i_develop_prediction_intervals_for/</link>
      <description><![CDATA[有人研究过为强化学习开发预测区间吗？我不太熟悉预测区间的概念。我读了几篇关于如何为 DL 做准备的文章，并想为 RL 获取一些东西，因为我的项目需要它。基本上，我正在使用无模型强化学习。因此，我的算法的输入是状态，输出是动作。因此，我认为每个州都会有自己单独的预测区间。另外，我们假设连续的动作空间。如果我的问题不清楚，请告诉我。  谢谢。   由   提交 /u/Academic-Rent7800    reddit.com/r/reinforcementlearning/comments/1cmcvfl/how_do_i_develop_prediction_intervals_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cmcvfl/how_do_i_develop_prediction_intervals_for/</guid>
      <pubDate>Tue, 07 May 2024 14:12:04 GMT</pubDate>
    </item>
    <item>
      <title>在 Gym 等并行环境中求解 QP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cm4vc9/solving_qp_in_parallel_environments_like_gym/</link>
      <description><![CDATA[之前有人在Gym这样的并行环境下解决过QP吗？  我一直在使用 OSQP（但仅在 Gazebo 的单一环境中），但找不到与在多环境中并行使用 OSQP 相关的太多信息。 所以我真的会喜欢获得有关如何在并行环境中解决 QP 的任何提示或建议和/或其他一些库工具的建议。提前致谢！！   由   提交/u/Open-Safety-1585   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cm4vc9/solving_qp_in_parallel_environments_like_gym/</guid>
      <pubDate>Tue, 07 May 2024 06:02:55 GMT</pubDate>
    </item>
    <item>
      <title>你是如何将强化学习变成你的职业的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cm37ba/how_did_you_make_rl_into_a_career/</link>
      <description><![CDATA[我是一名刚毕业的学生，​​对 RL 有一定的基础。（我的最后一年项目就是在这个基础上完成的）。但是，当我寻找 RL 工作时，我只找到硕士或博士职位。我很想听听人们如何在 RL 中找到工作的故事？关于探索的行业或我的下一步应该做什么有什么建议吗？    提交人    /u/Guilty-Cheesecake660   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cm37ba/how_did_you_make_rl_into_a_career/</guid>
      <pubDate>Tue, 07 May 2024 04:18:41 GMT</pubDate>
    </item>
    <item>
      <title>预订RL？专门针对 3D 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clzrss/book_for_rl_specifically_for_3d_environment/</link>
      <description><![CDATA[大家好， 我想制作一个 RL 模型来学习如何在 3D 环境中行走。我相信穆乔科？人们用的是什么？我的目标是给这个代理一个背包（和其他物品），看看它如何影响它的行走能力。 有什么关于这方面的书籍我可以用来自学吗？ 谢谢   由   提交 /u/Grand_Comparison2081   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clzrss/book_for_rl_specifically_for_3d_environment/</guid>
      <pubDate>Tue, 07 May 2024 01:22:00 GMT</pubDate>
    </item>
    <item>
      <title>QT-Opt/CEM 与 SAC 的实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clrbyr/qtoptcem_vs_sac_in_practice/</link>
      <description><![CDATA[看来 SAC 可能是目前最流行的离策略方法。然而，QT-Opt 论文建议使用交叉熵方法 (CEM)，而不是演员批评方法（大概是 SAC/TD3/等）。 本演示文稿链接到多项研究，这些研究表明 CEM 正在发挥作用-与 TD3 相当。 我很好奇是否有人有使用 CEM 的经验，以及它在实践中与 SAC/TD3 相比如何。 CEM 的随机性与可调采样参数的结合似乎可以防止高估问题并引入更多探索。我猜想在 CEM 中优化 Q 函数也比 SAC/TD3 中 Actor 和 Critic 的迭代优化更稳定。   由   提交 /u/smorad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clrbyr/qtoptcem_vs_sac_in_practice/</guid>
      <pubDate>Mon, 06 May 2024 19:11:07 GMT</pubDate>
    </item>
    <item>
      <title>RLlib 多智能体并行的复杂观察空间包装</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clfqo7/complex_observation_spaces_wrapping_for_rllib/</link>
      <description><![CDATA[大家好， 我正在使用 RLlib 开发一个多智能体环境，其中每个智能体的观察空间都使用 spaces.Dict 定义。我的环境设置运行良好，直到与 RLlib 的训练循环集成时，我遇到了与处理这些复杂空间相关的反复出现的问题。 我使用 spaces.Dict 定义我的智能体的观察空间，其中包括各种子空间，例如 spaces.Box 和 spaces.MultiBinary 。由于我试图找到空间错误的解决方案，因此我有几个定义。我附上了观察和动作空间的定义。我意识到我必须包装复杂的观察，但我对此还很陌生。 所以我的问题是，我应该如何包装它们或修改它们以便能够在并行多智能体训练中使用它们？ self._observation_spaces = space.Dict( observer_satellites = space.Box(low=-np.inf, high=np.inf, shape=(self.num_observers, len(self.orbital_params_order))), target_satellites = space.Box(low=-np.inf, high=np.inf, shape=(self.num_targets, len(self.orbital_params_order_targets))), availability = space.MultiBinary(1), battery = space.Box(low=0, high=1, shape=(self.num_observers, 1)), storage =spaces.Box(low=0, high=1,形状=（self.num_observers，1）），observation_status = space.Box（low=0，high=3，形状=（self.num_targets，）），pointing_accuracy = space.Box（low=-np.inf，high=np.inf，形状=（self.num_observers，self.num_targets）），communication_status = space.Box（low=0，high=1，形状=（self.num_observers，），dtype=np.int8），communication_ability = space.MultiBinary（self.num_observers））self._action_spaces = space.Discrete（2 + self.num_targets）self.infos = {agent：{} for agent in self.possible_agents} @property def observer_spaces（self）：return {agent：self.observation_space（agent）for agent in self.possible_agents} @property def action_spaces(self): return {agent: self.action_space(agent) for agent in self.possible_agents} @functools.lru_cache(maxsize=None) def observer_space(self, agent): print(f&quot;返回 {agent} 的观察空间：{self._observation_spaces}&quot;) return self._observation_spaces @functools.lru_cache(maxsize=None) def action_space(self, agent): print(f&quot;返回 {agent} 的动作空间：{self._action_spaces}&quot;) return self._action_spaces     提交人    /u/CLEMENMAN   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clfqo7/complex_observation_spaces_wrapping_for_rllib/</guid>
      <pubDate>Mon, 06 May 2024 10:29:51 GMT</pubDate>
    </item>
    </channel>
</rss>