<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 28 Aug 2024 18:20:25 GMT</lastBuildDate>
    <item>
      <title>强化学习中的低计算研究领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3c848/low_compute_research_areas_in_rl/</link>
      <description><![CDATA[我现在是本科四年级，必须为我的论文选择一个研究主题。我之前上过 ML/DL/RL 的课程，所以我有基础知识。 问题是我在这里无法访问适当的 GPU 资源。（当然，云是存在的，但价格昂贵。）我们大学只有一个简单的消费级 GPU（RTX 3090）和一台始终供不应求的 HPC 服务器，我的笔记本电脑中有 GTX 1650Ti。 所以，我正在寻找 RL 中需要相对较少计算的研究领域。我对理论和实践主题都持开放态度，但理想情况下，我想研究一些可以在我可用的硬件上实现和测试的东西。 我研究过的一些领域是迁移学习、元 RL、安全 RL 和逆 RL。我认为我的硬件很难处理 MARL。 您可以推荐研究领域、应用领域，甚至是可能有趣的特定论文。 此外，任何关于如何最大限度提高我的硬件在 RL 实验中的效率的建议都将不胜感激。 谢谢！！    提交人    /u/Abominable_Liar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3c848/low_compute_research_areas_in_rl/</guid>
      <pubDate>Wed, 28 Aug 2024 14:52:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么DP中的策略迭代会收敛到真实值函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f38w85/why_policy_iteration_in_dp_converges_to_true/</link>
      <description><![CDATA[给定一个特定的确定性策略和一个已知的 MDP 环境，如果迭代贝尔曼期望方程，是否可以收敛到给定策略的所有状态的真实值函数？ 我想它说无论如何 DP 策略迭代都有效，但我不确定它为什么有效。    提交人    /u/Latter-Tomorrow-6850   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f38w85/why_policy_iteration_in_dp_converges_to_true/</guid>
      <pubDate>Wed, 28 Aug 2024 12:27:26 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f373dl/learning_reinforcement_learning/</link>
      <description><![CDATA[您好，我已经使用 Python 编码 3 年了，并且具备 C 语言基础知识。在哪里可以了解有关强化学习的更多信息并提高我的知识。如果有资源可以编写自己的代理，我会很高兴。 谢谢。    提交人    /u/Competitive_Yak7223   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f373dl/learning_reinforcement_learning/</guid>
      <pubDate>Wed, 28 Aug 2024 10:50:04 GMT</pubDate>
    </item>
    <item>
      <title>混合动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f32pe1/hybrid_action_space/</link>
      <description><![CDATA[嗨，我在选择混合动作空间的正确算法时遇到了问题。通常，在许多研究论文中，他们提到混合动作空间是一种分层情况，即当您选择一个离散动作，然后根据离散动作选择一个连续动作时。但是，在我的项目中，一个动作需要 2 个参数，它们是离散的和连续的。它们彼此独立，但都会影响状态、奖励。因此，我想问一下我应该将其分成 2 个代理还是有算法可以解决这个问题？非常感谢！    提交人    /u/Fish_Chandle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f32pe1/hybrid_action_space/</guid>
      <pubDate>Wed, 28 Aug 2024 05:46:47 GMT</pubDate>
    </item>
    <item>
      <title>学习环境模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2wew8/learning_environment_model/</link>
      <description><![CDATA[你好。由于其样本效率，我想尝试基于模型的 RL。 但是，当我尝试在玩具环境中学习一个模型时，该模型具有大小为 51 的 1d 向量输入和大小为 10 的输出，该模型很难学习。该模型接收当前观察，然后采取行动预测下一个观察、奖励和终止标志。 观察和行动在 0~1 之间。但模型的 L2 误差从 0.1 下降得太慢了。它正在学习。但太慢了！ 这很奇怪，因为使用 td3 快速学习了一个好的策略。 有人可以分享他们在基于模型的 RL 方面的经验或一些好的材料吗？谢谢！    提交人    /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2wew8/learning_environment_model/</guid>
      <pubDate>Wed, 28 Aug 2024 00:08:30 GMT</pubDate>
    </item>
    <item>
      <title>Bandit Learning 研究/行业机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2t9qi/bandit_learning_research_industry_opportunities/</link>
      <description><![CDATA[大家好，我开始学习 bandit 学习的基础知识，但我真的很关心这个领域的实际应用或就业市场。这是一个有趣的领域，但到目前为止，我不知道这个领域的求职“关键词”是什么，不知道是否要继续？    提交人    /u/math--lover   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2t9qi/bandit_learning_research_industry_opportunities/</guid>
      <pubDate>Tue, 27 Aug 2024 21:49:57 GMT</pubDate>
    </item>
    <item>
      <title>在口袋妖怪绿宝石中击败第三个健身房</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2pvtl/beating_third_gym_in_pokemon_emerald/</link>
      <description><![CDATA[        提交人    /u/nicimunty   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2pvtl/beating_third_gym_in_pokemon_emerald/</guid>
      <pubDate>Tue, 27 Aug 2024 19:28:54 GMT</pubDate>
    </item>
    <item>
      <title>“多镜头情境学习”，Agarwal 等人，2024 年 {G}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2inwj/manyshot_incontext_learning_agarwal_et_al_2024_g/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2inwj/manyshot_incontext_learning_agarwal_et_al_2024_g/</guid>
      <pubDate>Tue, 27 Aug 2024 14:36:54 GMT</pubDate>
    </item>
    <item>
      <title>C# 深度强化学习比 sb3 快 300 倍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2ifcf/c_deep_reinforcement_learning_300_times_faster/</link>
      <description><![CDATA[        提交人    /u/asieradzk   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2ifcf/c_deep_reinforcement_learning_300_times_faster/</guid>
      <pubDate>Tue, 27 Aug 2024 14:27:06 GMT</pubDate>
    </item>
    <item>
      <title>如何将我自定义的凉亭世界导入健身房进行强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2clog/how_do_i_import_my_custom_gazebo_world_to_gym_for/</link>
      <description><![CDATA[我是强化学习的新手，想尝试一下我的自定义世界。但是我找不到将我的世界导入健身房的方法。我该如何将我的 Gazebo 环境导入健身房？我的 Gazebo 世界包含 urdf、world 和 launch 文件，这是带有 Gazebo 环境的 git 存储库。我使用的是带有 Gazebo 11 和 ros1 的 ubuntu 20.04。    提交人    /u/brian22lee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2clog/how_do_i_import_my_custom_gazebo_world_to_gym_for/</guid>
      <pubDate>Tue, 27 Aug 2024 09:15:17 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的反作弊系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</link>
      <description><![CDATA[我对创建基于 ML 的国际象棋反作弊系统很感兴趣。您知道任何关于使用 ML 方法进行反作弊的论文吗？我正在寻找概念。但到目前为止找不到任何类似的东西。有很多困难：例如，数据集中作弊者的例子很少，但公平玩家数据很多。在公平玩家数据上使用离线 RL 算法来预测异常行为是否有意义？    提交人    /u/HimitsuNoShougakusei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</guid>
      <pubDate>Tue, 27 Aug 2024 08:49:53 GMT</pubDate>
    </item>
    <item>
      <title>未获得官方支持的多代理 PPO 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f25yg9/multiagent_ppo_environment_without_official/</link>
      <description><![CDATA[问题陈述： 我想使用 gym 和 sb3 创建一个多智能体环境。目前，我有一个自定义的 Boid 群集环境，其中有一个神经网络，它采取所有动作并产生输出，即来自 sb3 的 PPO。 我想要实现的目标：我希望每个 boid 都有自己的演员评论家，以分散的方式执行，但 gym 默认不支持多智能体。我该如何实现呢？ 我正在使用 PPO，并希望有效地完成 MAPPO 所做的事情。集中训练，分散执行 (CTDE)。    提交人    /u/Ok-Teacher208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f25yg9/multiagent_ppo_environment_without_official/</guid>
      <pubDate>Tue, 27 Aug 2024 02:13:01 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f257ww/rl_for_manipulation/</link>
      <description><![CDATA[大家好！我正在深入研究强化/模仿学习和远程操作。我主修机械工程，在运动强化学习研究方面有经验，并且了解现代控制理论/ML/DL（我的理论理解还有待提高，但我都参加了研究生课程）。期待建议！我希望得到建议的论文、资源或开源项目，以便我可以提高我的整体理解。谢谢    提交人    /u/Sea-Hovercraft4777   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f257ww/rl_for_manipulation/</guid>
      <pubDate>Tue, 27 Aug 2024 01:36:44 GMT</pubDate>
    </item>
    <item>
      <title>“利用精选数据的自消费生成模型可证明优化人类偏好”，Ferbach 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f204co/selfconsuming_generative_models_with_curated_data/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f204co/selfconsuming_generative_models_with_curated_data/</guid>
      <pubDate>Mon, 26 Aug 2024 21:42:35 GMT</pubDate>
    </item>
    <item>
      <title>过山车大亨之旅中的列车特工</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</link>
      <description><![CDATA[嗨！ 我只是想分享我的项目，我尝试创建一个代理，在 Rollercoaster Tycoon 中构建自定义轨道，并使用游戏自己的乘坐评级作为适应度函数来训练代理构建最好的过山车。 目前，它使用 UI 单击按钮并读取游戏屏幕，这非常缓慢且脆弱，但如果它显示出任何实际学习某些东西的迹象，我将尝试为 OpenRCT2 创建一个插件，公开代理可以使用的真实 API 而不是 UI。然后应该有可能在无头模式下运行游戏，并且使用 API 和同时在多个 OpenRCT2 实例上更快地构建代理。 很高兴听到您的反馈，您可以在这里找到代码：https://github.com/ZerxXxes/openrct2_gym    提交人    /u/ZerxXxes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f1iqxm/train_agent_on_rollercoaster_tycoon_rides/</guid>
      <pubDate>Mon, 26 Aug 2024 08:00:32 GMT</pubDate>
    </item>
    </channel>
</rss>