<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 16 Dec 2024 15:19:02 GMT</lastBuildDate>
    <item>
      <title>OpenAI Gym 环境表不起作用。替代品在哪里？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hfjxbb/openai_gym_table_of_environments_not_working/</link>
      <description><![CDATA[我是 RL 的完全初学者，如果这是常识的话，我很抱歉。我刚刚开始学习该主题的课程。 这是 OpenAI 的 github 链接，他们在那里保存环境表：https://github.com/openai/gym/wiki/Table-of-environments 单击此表中的任何链接（例如 CartPole-v0）都会将您重定向到 gym.openai.com 的某个页面，据我从这篇 reddit 帖子 中理解。它已被 https://www.gymlibrary.dev/ 取代。 现在我可以在哪里找到这些环境的链接？    提交人    /u/iamconfusion1996   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hfjxbb/openai_gym_table_of_environments_not_working/</guid>
      <pubDate>Mon, 16 Dec 2024 14:10:56 GMT</pubDate>
    </item>
    <item>
      <title>仅参与者的 REINFORCE 算法的性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hfjklc/performance_of_actoronly_reinforce_algorithm/</link>
      <description><![CDATA[嗨， 这个问题似乎毫无意义，但我感兴趣的是，具有以下属性的算法的性能可能如何：  仅限演员 强化优化（使用完整情节生成梯度并计算累积奖励） 参数集较少。例如：2 层 CNN + 2 线性层（假设 LL 上有 200 个隐藏参数） 除了使帧更小（例如 64x64）外，不对帧进行预处理 1e-6 学习率  在长情节环境中。例如 atari pong 可能需要 3000 帧才能获得 -21 奖励，可能需要 10k 帧甚至更多。 这样的算法可以在足够多的（数千场游戏？数百万场？）迭代之后掌握游戏吗？ 在实践中，我试图了解改进该算法的最有效方法是什么，因为我不想增加参数数量（但可以将模型本身从 cnn 更改为其他东西）    提交人    /u/Potential_Hippo1724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hfjklc/performance_of_actoronly_reinforce_algorithm/</guid>
      <pubDate>Mon, 16 Dec 2024 13:53:46 GMT</pubDate>
    </item>
    <item>
      <title>AI 学习使用虚幻引擎来平衡球！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hffktt/ai_learns_to_balance_a_ball_using_unreal_engine/</link>
      <description><![CDATA[        提交人    /u/Cyber​​Eng   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hffktt/ai_learns_to_balance_a_ball_using_unreal_engine/</guid>
      <pubDate>Mon, 16 Dec 2024 09:39:01 GMT</pubDate>
    </item>
    <item>
      <title>1 年 Perplexity Pro 促销代码仅需 25 美元（节省 175 美元！）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hf9k1l/1year_perplexity_pro_promo_code_for_only_25_save/</link>
      <description><![CDATA[仅需 25 美元即可获得 1 年 Perplexity Pro 促销代码（节省 175 美元！） 以合理的价格使用顶级模型和工具增强您的 AI 体验： 高级 AI 模型：访问 GPT-4o、o1 和Llama 3.1 还利用了 Claude 3.5 Sonnet、Claude 3.5 Haiku 和 Grok-2。 图像生成：探索 Flux.1、DALL-E 3 和 Playground v3 Stable Diffusion XL 可供无有效 Pro 订阅的用户使用，可在全球范围内访问。 简单的购买流程： 加入我们的社区： Discord 拥有 450 名成员。 安全付款：使用 PayPal 保障您的安全和买家保护。 即时访问：通过简单的促销链接接收您的代码。 为什么选择我们？ 我们的业绩记录不言而喻。 查看我们经过验证的已验证买家 + VIP 买家和客户反馈 2、反馈 3、反馈 4、反馈 5    提交人    /u/AICentralZA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hf9k1l/1year_perplexity_pro_promo_code_for_only_25_save/</guid>
      <pubDate>Mon, 16 Dec 2024 02:58:09 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么关于训练 ppo/dqn 解决迷宫的技巧？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hf6lwr/any_tips_for_training_ppodqn_on_solving_mazes/</link>
      <description><![CDATA[创建了我自己的健身环境，其中观察由一个形状为 4 + 20 的单个 numpy 数组组成（agent_x、agent_y、target_x、target_y 和 20 个障碍物 x 和 y）。代理获得的基本奖励为 (distancebefore - distanceafter)（使用 astar），每一步为 -1 或 0 或 1，到达目标时获得奖励 = 100，与墙壁相撞时获得 -1（如果我使用 distancebefore - distanceafter，则为 0）。 我正在尝试训练 ppo 或 dqn 代理（都试过）来解决带有动态墙壁的 10x10 迷宫 你们有什么技巧可以让我尝试，以便我的代理可以在我的环境中学习？ 欢迎任何帮助和提示，我之前从未在迷宫上训练过代理，我想知道是否有什么特别的地方需要考虑。如果有其他模型更好，请告诉你 在我的用例中，我想要解决的是一个迷宫，每次调用 reset() 时代理都从随机位置开始，并且每次重置时障碍物都会发生变化。这个迷宫能解决吗？ 我使用 baselines3 作为模型 （我也尝试了 sb3_contrib qrdqn 和 recurrent ppo 和 maskable ppo） https://imgur.com/a/SWfGCPy    提交人    /u/More_Peanut1312   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hf6lwr/any_tips_for_training_ppodqn_on_solving_mazes/</guid>
      <pubDate>Mon, 16 Dec 2024 00:26:01 GMT</pubDate>
    </item>
    <item>
      <title>我正在进行的项目需要帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hekanv/need_help_in_a_project_im_doing/</link>
      <description><![CDATA[我正在使用 stable_baselines3 中的 TD3 模型，并尝试训练机器人进行导航。我在 Mujoco 物理模拟器中有一个机器人，它能够在 x 和 y 方向上获取速度。它正试图到达目标位置。 我的观察空间是机器人位置、目标位置和与箱子的距离。迈出一步会得到一个小的负奖励，向目标移动会得到一个小的正奖励，到达目标会得到一个大的奖励，与障碍物碰撞会得到一个大的负奖励。 我无法到达目标。我观察到机器人会随机选择一条对角线并沿着它移动，而不管目标位置如何。这可能是什么原因造成的？如果有帮助，我可以分享我的代码，但我不知道这里是否允许这样做。 如果有人愿意帮助我，我将不胜感激。  提前致谢。    由    /u/Rishinc 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hekanv/need_help_in_a_project_im_doing/</guid>
      <pubDate>Sun, 15 Dec 2024 03:52:07 GMT</pubDate>
    </item>
    <item>
      <title>第一个 Isaac Lab 教程！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1he514n/first_isaac_lab_tutorial/</link>
      <description><![CDATA[昨天，我发了一篇展示 Isaac Lab 的帖子，得到了很好的反馈。在问你们是否希望我制作教程视频后，你们中的很多人都表现出了兴趣，我立即开始录制。 好了，这是我的第一个 Isaac Lab 教程，希望你们喜欢它！ https://www.youtube.com/watch?v=sL1wCfp9tRU 因为这是我第一次录制自己的声音，所以我知道我还有很多需要改进的地方，所以我恳请你们提供反馈。 祝大家有美好的一天~    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1he514n/first_isaac_lab_tutorial/</guid>
      <pubDate>Sat, 14 Dec 2024 15:18:18 GMT</pubDate>
    </item>
    <item>
      <title>1 年 Perplexity Pro 代码仅需 25 美元（节省 175 美元！）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1he4bxq/1year_perplexity_pro_code_for_only_25_save_175/</link>
      <description><![CDATA[      仅需 25 美元即可获得 1 年 Perplexity Pro 促销代码（节省 175 美元！） 以合理的价格通过顶级模型和工具提升您的 AI 体验：  高级 AI 模型：访问 GPT-4o、o1 Mini 进行推理，&amp; Llama 3.1 Creative Suite：利用 Claude 3.5 Sonnet、Claude 3.5 Haiku 和 Grok-2 图像生成：探索 Flux.1、DALL-E 3 和 Playground v3 Stable Diffusion XL  可供没有有效 Pro 订阅的用户使用，可在全球范围内访问。 简单的购买流程：  加入我们的社区：在 Discord 上与超过 400 名成员的 AI 爱好者联系。 安全付款：使用 PayPal 来保障您的安全和买家保护。 即时访问：通过简单的兑换链接接收您的代码。  为什么选择我们？ 我们的业绩记录不言而喻。查看我们经过验证的买家凭证和客户反馈 2、反馈 3、反馈 4、反馈 5 https://preview.redd.it/oiw24jaest6e1.png?width=384&amp;format=png&amp;auto=webp&amp;s=38ad8035db651abd7ef10ade2a4f467610e8b721    提交人    /u/AICentralZA   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1he4bxq/1year_perplexity_pro_code_for_only_25_save_175/</guid>
      <pubDate>Sat, 14 Dec 2024 14:44:30 GMT</pubDate>
    </item>
    <item>
      <title>Sb3 中的 DummyVecEnv 导致 API 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hdcgdi/dummyvecenv_from_sb3_causes_api_problems/</link>
      <description><![CDATA[嗨 :) 我按照 gym 界面构建了一个自定义环境。step、reset 和 action_mask 方法调用我的棋盘游戏在 java 中提供的 Rest-Endpoint。sb3 中的 check_env 方法运行没有问题，但是当我尝试在该环境上训练代理时，我收到 HTTP 500 Server Errros。我认为这是因为 sb3 从我的 CustomEnv 创建了一个 DummyVecEnv，并且 API 仅支持一次运行一个游戏。有没有办法不使用 DummyVecEnv？我知道训练会变慢，但现在我只想让它正常工作 xD 如果有帮助，我可以分享错误日志，但我不想在这里发送太多文本... 提前谢谢 :)    提交人    /u/ItchyRoyal212   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hdcgdi/dummyvecenv_from_sb3_causes_api_problems/</guid>
      <pubDate>Fri, 13 Dec 2024 13:42:23 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Lab 太疯狂了（Nvidia Omniverse）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd9byq/isaac_lab_is_insane_nvidia_omniverse/</link>
      <description><![CDATA[大家好，我最近对 ​​Nvidia Omniverse 及其 Isaac Lab（基于 Isaac Sim 构建）非常感兴趣。它对于强化学习非常强大，你一定要看看。 我甚至有动力制作一个视频来展示它的用例（我不知道是否可以在这里上传）。 https://www.youtube.com/watch?v=NfNC03rZssU    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd9byq/isaac_lab_is_insane_nvidia_omniverse/</guid>
      <pubDate>Fri, 13 Dec 2024 10:23:46 GMT</pubDate>
    </item>
    <item>
      <title>按 NeurIPS 2024 论文数量计算，强化学习是第三受欢迎的领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd9bac/rl_is_the_third_most_popular_area_by_number_of/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd9bac/rl_is_the_third_most_popular_area_by_number_of/</guid>
      <pubDate>Fri, 13 Dec 2024 10:22:17 GMT</pubDate>
    </item>
    <item>
      <title>寻求强化学习（RL）个人项目的想法和指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hd5ef7/looking_for_ideas_and_guidance_for_personal/</link>
      <description><![CDATA[大家好！ 我刚刚完成硕士课程的第一年，并获得了计算机科学学士学位，主修人工智能。在过去的几年里，我通过工作、实习和研究获得了丰富的经验，特别是在我真正喜欢的领域，比如应用于车辆和无人机系统的强化学习 (RL)。 现在，我希望深入研究 RL 中的个人项目，以探索新的想法并加深我的知识。您对有趣的基于 RL 的个人项目有什么建议吗？我特别喜欢涉及机器人、无人机或自主系统的项目，但我也愿意接受任何有创意的建议。 此外，我很想听听关于如何开始个人 RL 项目的建议——您会向处于我这个位置的人推荐哪些工具、框架或资源？我认为自己非常精通 Python 及其相关知识。 提前感谢您的想法和提示！    提交人    /u/CJPeso   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hd5ef7/looking_for_ideas_and_guidance_for_personal/</guid>
      <pubDate>Fri, 13 Dec 2024 05:30:06 GMT</pubDate>
    </item>
    <item>
      <title>学术背景调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcw8aa/academic_background_poll/</link>
      <description><![CDATA[大家好， 出于好奇，我想看看这里社区的背景分布是怎样的。 ​ 查看投票    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcw8aa/academic_background_poll/</guid>
      <pubDate>Thu, 12 Dec 2024 21:42:12 GMT</pubDate>
    </item>
    <item>
      <title>在整个轨迹中改变观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</link>
      <description><![CDATA[嗨， 有人知道关于在轨迹过程中代理的观察空间的场景的任何先前工作吗？ 例如，如果具有多个传感器的机器人决定在轨迹期间转动其中一个传感器（可能出于能量考虑）。  据我所知，最常用的算法没有考虑到轨迹过程中观察空间的变化。 很想听听大家的想法    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</guid>
      <pubDate>Thu, 12 Dec 2024 14:20:01 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 MATD3 和 MADDPG 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</link>
      <description><![CDATA[问候， 我需要在某些环境中运行这两个算法（无所谓）来证明多智能体学习确实有效！（是的，这非常简单，但很难！） 问题就在这里。找不到一个单一的框架来在环境中植入算法（现在基本上是宠物动物园 mpe）， 我做了一些研究：  Marllib 没有很好的文档记录。最后我还是搞不懂。 agileRL 很棒但是有一个 bug 我无法解决，（如果您能解决这个 bug 请告诉我）。 Thianshou ，我必须植入算法！！ CleanRL，嗯...我没搞懂。我的意思是我应该在我的主脚本中使用这些算法 .py 文件吗？  好吧请帮忙.......... 带着爱    提交人    /u/matin1099   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</guid>
      <pubDate>Thu, 12 Dec 2024 11:50:04 GMT</pubDate>
    </item>
    </channel>
</rss>