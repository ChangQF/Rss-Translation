<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 27 Dec 2024 18:21:58 GMT</lastBuildDate>
    <item>
      <title>AAAI 2025 教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</link>
      <description><![CDATA[AI 安全：从强化学习到基础模型 链接：https://aaai.org/conference/aaai/aaai-25/tutorial-and-lab-list/#TQ10     提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</guid>
      <pubDate>Fri, 27 Dec 2024 13:07:15 GMT</pubDate>
    </item>
    <item>
      <title>TD3 算法第二次更新时策略严重偏向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hndzen/heavily_biased_policy_on_the_second_update_for/</link>
      <description><![CDATA[大家好， 我为演员和评论家的架构实现了一个带有 CNN 和线性层的 TD3 算法。它被用于解决 Open AI Gym 的 CarRacing 环境。动作空间是连续的，其中最大动作为 [1, 1, 1]，最小动作为 [-1, 0, 0]，分别用于转向、加油和刹车。策略更新是在情节结束时完成的，即环境中的步骤处于完成状态时。环境中发生的时间步数用于在情节结束后多次更新策略。重放缓冲区也使用 1000 个时间步的随机动作初始化。 为什么在第一集之后，演员会选择立即开始向左大力转向（-1）的动作？转向几乎始终为 1 且高于 0.9，并且只发生在第二集中。第一集中选择的动作与随机采样动作本质上相同。 非常感谢    提交人    /u/Sea_Farmer5942   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hndzen/heavily_biased_policy_on_the_second_update_for/</guid>
      <pubDate>Fri, 27 Dec 2024 12:37:05 GMT</pubDate>
    </item>
    <item>
      <title>O(sqrt(T)) 遗憾比 O(sqrt(T \log T)) 遗憾更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hn2me2/is_osqrtt_regret_better_than_osqrtt_log_t_regret/</link>
      <description><![CDATA[从数学上讲，sqrt(T) 优于 sqrt(T \log T)，但如果我提交论文，sqrt(T) 遗憾算法会被认为比 sqrt(T \log T) 遗憾算法更好吗？我正在阅读一篇论文，作者声称他们的算法是 \tildeO( \sqrt(T) )；尽管在论文正文中报告遗憾值为 O( \sqrt(T log T) )。我有点困惑，因为我认为 \tilde 应该是“忽略常量/模型参数”，但 log T 不是 T 方面的常数。他们还提到了一个特殊情况，其中遗憾是 O( \sqrt(T) )。我还查看了高概率遗憾与预期遗憾，他们似乎在说预期遗憾的上限为 O( sqrt (T log T ) )。 O( \sqrt(T) ) 是否被认为比 O( \sqrt(T \log T) ) 更好，或者差异可以忽略不计？    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hn2me2/is_osqrtt_regret_better_than_osqrtt_log_t_regret/</guid>
      <pubDate>Fri, 27 Dec 2024 00:41:34 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以学习价值函数近似，包括例子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmzjhm/where_can_i_learn_value_function_approximation/</link>
      <description><![CDATA[我正在学习 David Silver 的强化学习课程，我学到了第 6 讲，内容是关于价值函数近似的。我理解了这堂课之前的所有内容，但这堂课的内容对我来说毫无意义，我认为这是因为原来的班级的学生似乎有机器学习的背景，所以他跳过了很多基础知识。有没有什么地方可以让我正确地从头开始学习，最好是有很多例子的地方？    提交人    /u/Hekkowow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmzjhm/where_can_i_learn_value_function_approximation/</guid>
      <pubDate>Thu, 26 Dec 2024 22:14:13 GMT</pubDate>
    </item>
    <item>
      <title>DQN 中的训练图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmxh3f/training_plot_in_dqn/</link>
      <description><![CDATA[      大家好， 圣诞快乐，节日快乐！ 我在阅读 DQN 代理的训练图时遇到了麻烦，因为它似乎没有太大的改进，但如果我将它与随机代理进行比较，它的结果会更好。 此外，它还有很多噪音，我认为这不是一个好的事情。 我见过一些人在验证剧集中监控奖励图 对于剧集 = 2000： （在 4096 步上进行训练，然后在其中一集上进行验证并使用其奖励进行绘图） 剧集++ 另外，我已经阅读了有关奖励标准化的内容，我应该尝试一下吗？ returns =（returns - returns.mean()）/（returns.std() + eps） 期待任何见解和训练情节已附上。 提前致谢 https://preview.redd.it/hcs944u6899e1.png?width=971&amp;format=png&amp;auto=webp&amp;s=a9480a660c2b3e02d736b95b5c24f9c95434941d    提交人    /u/Dry-Image8120   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmxh3f/training_plot_in_dqn/</guid>
      <pubDate>Thu, 26 Dec 2024 20:40:51 GMT</pubDate>
    </item>
    <item>
      <title>强化问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmk9pm/reinforcement_problem/</link>
      <description><![CDATA[我忍不住将我 8 个月大的宝宝当作强化学习问题。设计适当的环境和奖励。只需要研究一种算法……    由    /u/tedthemouse 提交   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmk9pm/reinforcement_problem/</guid>
      <pubDate>Thu, 26 Dec 2024 08:33:09 GMT</pubDate>
    </item>
    <item>
      <title>GAE 和 Actor Critic 方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmepx8/gae_and_actor_critic_methods/</link>
      <description><![CDATA[      我使用单独的参与者和评论家网络实现了相当经典的 GAE 方法。在 CartPole 任务上测试，使用的批处理大小为 8。看起来只有 GAE(lambda=1) 或接近 1 的某个 lambda 才能使参与者模型起作用。这相当于使用经验奖励来计算 td 误差（我对此进行了单独的实现，结果看起来几乎相同）。  任何较小的 lambda 值基本上都不起作用。预期的情节长度（达到步骤的批次平均值）要么永远不会大于 40；要么显示非常坎坷的曲线（在达到相当大的步骤数后迅速变得更糟）；或者只是收敛到一个非常小的值，例如低于 10。  我试图了解这是否是“预期的”。我理解我们不希望策略损失保持/收敛到 0（无论其质量如何都成为确定性策略）。这实际上发生在较小的 lambda 值上。  这纯粹是由于偏差-方差权衡吗？对于较大的（或 1.0）lambda 值，我们期望偏差较低但方差较高。从 Sergey Levine 的课程来看，我们似乎希望总体上避免这种情况？然而，这种“经验蒙特卡罗”方法似乎是唯一适合我的情况的方法。 此外，我们应该监控策略梯度方法的哪些指标？从我目前的观察来看，策略网络的损失或评论模型损失几乎毫无用处……唯一重要的事情似乎是预期的总回报？  分享一些我的 tensorboard 的截图： https://preview.redd.it/x7bcpud9s39e1.png?width=1572&amp;format=png&amp;auto=webp&amp;s=8dec61d8a3f0f0f1a4798a2da7fd15c5d0e7a23a    提交人    /u/encoreway2020   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmepx8/gae_and_actor_critic_methods/</guid>
      <pubDate>Thu, 26 Dec 2024 02:22:35 GMT</pubDate>
    </item>
    <item>
      <title>超大的观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hm5cnh/extremely_large_observation_space/</link>
      <description><![CDATA[根据标题，我一直在解决一个观察空间为 5 元组的问题，对于元组内的所有元素，低-高都是 int 0-100。动作空间只有离散的 3。 以前有人处理过这么大的空间吗？你认为哪种神经网络模型/管道能产生最好的结果？    提交人    /u/Neither_Canary_7726   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hm5cnh/extremely_large_observation_space/</guid>
      <pubDate>Wed, 25 Dec 2024 17:52:00 GMT</pubDate>
    </item>
    <item>
      <title>以 25 美元的价格购买 Perplexity Pro 1 年期（正常价格：200 美元）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlzekm/get_perplexity_pro_1_year_for_25_normal_price_200/</link>
      <description><![CDATA[嗨， 我的服务提供商提供一项优惠，让我可以以 25 美元的价格使用 Perplexity Pro 一年 - 通常价格为 200 美元/年（约 75% 折扣） 我有大约 27 个促销代码，应在 12 月 31 日前兑换。 加入拥有 600 多名成员的 Discord，我将发送一个促销代码，您可以兑换。 我接受 PayPal 来保护买家，并接受加密货币来保护隐私。 我还有 LinkedIn Career Premium、Spotify Premium 和Xbox GamePass Ultimate。 再次感谢！    由    /u/minemateinnovation 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlzekm/get_perplexity_pro_1_year_for_25_normal_price_200/</guid>
      <pubDate>Wed, 25 Dec 2024 11:52:18 GMT</pubDate>
    </item>
    <item>
      <title>在世界模型中想象国家推出有什么好处？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlz6p3/what_is_the_benefit_of_imagined_state_rollouts_in/</link>
      <description><![CDATA[      大家好 :) 我有一个关于例如 https://arxiv.org/pdf/1803.10122 或 https://arxiv.org/pdf/1811.04551 中想象状态轨迹背后动机的问题。对我来说，这一切都说得通，我们如何做到这一点以及背后的原因对我来说也很清楚。但我仍然无法弄清楚为什么使用“模拟”的模型会更好未来的轨迹（在潜在空间还是在像素空间，无关紧要），当我们有机会以相同的成本与环境交互时，甚至更便宜（环境查询与通过 LSTM 等顺序模型的前向传递）。我们只会尝试重建已经存在的事物？ 我的意思是，这在交互成本很高的环境中是有意义的，但本文使用的示例大多是 OpenAI-Gym 环境，运行起来非常便宜。  Schmidhuber 和 Ha 在 World Model 论文中使用的算法 https://preview.redd.it/ahns2m2ddz8e1.png?width=1040&amp;format=png&amp;auto=webp&amp;s=6d5a1c7d579c98701cd7bd2afad8f27fd4b5f871 也在环境中执行该步骤。我看不出在这里使用顺序生成模型有什么好处？我们也可以使用一个非常强大的状态编码器来捕获过去的 k 个观测值。  也许，RNN 的顺序性质为我们提供了 h 中的更多信息，但是，我们也可以使用一个编码器来做到这一点，该编码器将过去的 k 个观测值映射到潜在空间，而无需任何世界模型。 那么，为什么我们要构建一个尝试重建可用数据的世界模型？    提交人    /u/No_Individual_7831   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlz6p3/what_is_the_benefit_of_imagined_state_rollouts_in/</guid>
      <pubDate>Wed, 25 Dec 2024 11:34:53 GMT</pubDate>
    </item>
    <item>
      <title>任何结合 RL 和 LLM 的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hly9nh/any_work_present_combining_rl_llms/</link>
      <description><![CDATA[有人知道将 RL 和 LLM 结合起来的一些工作吗？我已经看到了一些可以使用的提议方法，但到目前为止还没有实际应用。     提交人    /u/Wide-Chef-7011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hly9nh/any_work_present_combining_rl_llms/</guid>
      <pubDate>Wed, 25 Dec 2024 10:21:53 GMT</pubDate>
    </item>
    <item>
      <title>寻求指导：将 RL 应用于控制器设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hly2e9/looking_for_guidance_applying_rl_for_controller/</link>
      <description><![CDATA[大家好， 首先，祝整个 RL 社区圣诞快乐！🎄 我是一名控制理论家，在数学建模、经典控制、最优控制和刚体动力学方面有 7 年的经验。最近，我对探索如何应用强化学习 (RL) 算法来设计在不确定环境中表现出色的控制器产生了浓厚的兴趣。 我已经迈出了这一旅程的第一步，但我对将我的控制理论背景与 RL 领域联系起来的最佳方式感到有点迷茫。我希望找到一个结构化的路线图或实用建议来指导我走这条路。 如果您走过类似的道路，或者对课程、研究论文或其他可能有帮助的资源有任何建议，我将不胜感激。听到您关于浏览这个空间的经历或技巧对我来说也意义重大。 提前谢谢您！    提交人    /u/ValueSeekerAgent   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hly2e9/looking_for_guidance_applying_rl_for_controller/</guid>
      <pubDate>Wed, 25 Dec 2024 10:05:14 GMT</pubDate>
    </item>
    <item>
      <title>PPO 算法中总损失是如何使用的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hllhps/how_is_total_loss_used_in_ppo_algorithm/</link>
      <description><![CDATA[在 PPO 中，有两种损失：策略损失和价值损失。价值损失用于优化价值函数，而策略损失用于优化策略函数。但策略和价值损失（带有系数参数）结合在总损失函数中。 总损失函数有什么作用？我理解每个网络都使用自己的损失进行优化。那么总损失优化了什么？ 或者我理解错了，两个网络都使用相同的总损失进行优化，而不是使用各自的损失？    提交人    /u/BitShifter1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hllhps/how_is_total_loss_used_in_ppo_algorithm/</guid>
      <pubDate>Tue, 24 Dec 2024 20:10:43 GMT</pubDate>
    </item>
    <item>
      <title>具有离线 RL 的 GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlh6v7/gnn_with_offline_rl/</link>
      <description><![CDATA[我想使用离线 RL，即不与环境交互，仅使用过去的数据，这些数据可以组织为经验 (s、a、s&#39;、r)。代理 - 使用 Pytorch Geometric 的 GNN。状态 - 我使用 Pytorch Geometric 的 HeteroData 类型，这是一个异构图。算法 - CQN（保守 Q 学习）。动作空间 - 离散。奖励 - 仅在每集结束时。 有谁知道哪个 RL 框架可以最轻松地进行定制，而不必深入研究？ 到目前为止，我知道有 rllib、torchRL、d3RL、cleanRL、stable baselines、tianshou 几年前我只使用过稳定基线，做我需要的定制需要付出很多努力。我希望这次能避免这种情况。也许最好从头开始写？     由    /u/Aggravating_Rip_1882  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlh6v7/gnn_with_offline_rl/</guid>
      <pubDate>Tue, 24 Dec 2024 16:37:40 GMT</pubDate>
    </item>
    <item>
      <title>如何在基于 VAPI 的语音 AI 系统中创建/添加 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlg2ue/how_to_createadd_rl_in_vapibased_voice_ai_system/</link>
      <description><![CDATA[我有一个基于 VAPI 的 AI 语音助手，用于我的咨询业务，目前已与 Twilio（电话）、Deepgram（语音识别）、GPT-4（语言理解）和 ElevenLabs（文本转语音）集成。此外，此语音助手集成到 GoHighLevel CRM 系统中以存储客户信息。我想通过两个关键功能增强 AI 语音助手：1. 强化学习 (RL)，从用户交互中学习并不断改进响应。  检索增强生成 (RAG) 以确保从知识库（例如常见问题解答、政策文档或矢量数据库）获得事实和有根据的答案。  您能否：•提供将 RL 添加到我现有的 VAPI AI 工作流程中的分步说明示例？ •推荐一个向量数据库（FAISS、Pinecone 等）并概述如何为 RAG 构建检索管道？ •分享处理来自 Twilio/Deepgram 的语音数据、收集用户反馈以及定期更新模型的最佳实践？ 我的目标是让语音助手在每次通话中都更加准确、更具适应性。谢谢！    提交人    /u/IntelligentOil2047   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlg2ue/how_to_createadd_rl_in_vapibased_voice_ai_system/</guid>
      <pubDate>Tue, 24 Dec 2024 15:42:01 GMT</pubDate>
    </item>
    </channel>
</rss>