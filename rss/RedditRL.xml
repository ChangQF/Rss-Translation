<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 06 Aug 2024 21:14:24 GMT</lastBuildDate>
    <item>
      <title>“通过模拟寻找纳什均衡”（用 Python 可视化博弈论的收益和结果）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1elr14u/finding_nash_equilibria_through_simulation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1elr14u/finding_nash_equilibria_through_simulation/</guid>
      <pubDate>Tue, 06 Aug 2024 19:38:30 GMT</pubDate>
    </item>
    <item>
      <title>分享我基于 JAX 的 RL 算法库 - 包括 BBF 和 TD7 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1elikrd/sharing_my_jaxbased_rl_algorithms_repository/</link>
      <description><![CDATA[你好 r/reinforcementlearning！ 我很高兴与大家分享一个我一直在维护的存储库，用于我的实验、论文实现和基于 JAX 的 RL 优化：https://github.com/tinker495/jax-baseline 主要特点：  使用 JAX 实现高效的 RL 优化 可灵活选择 Flax 和 DeepMind Haiku 进行网络实现 包括最近的近 SOTA 算法：BBF（Big-Better-Faster）和 TD7  亮点：BBF 和 TD7 的实现已证明与其他算法相比，性能更高，与各自论文中提出的结果一致。如果您对这些算法感兴趣，此存储库可以作为有价值的参考。 重点是实现通用算法，而不是特定领域的优化。内容如下： 基于 DQN：  DQN C51 QRDQN IQN FQF SPR（带 SR-SPR 选项） BBF  基于 A2C：  A2C PPO IMPALA IMPALA-PPO（引用 rllib 实现）  基于 DDPG：  DDPG TD3 SAC TQC TD7  虽然它可能不是开创性的，但我希望这个存储库可以作为对于使用或学习这些 RL 算法的人来说，这是一个有用的资源。 请随意探索，欢迎任何反馈！    提交人    /u/New_East832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1elikrd/sharing_my_jaxbased_rl_algorithms_repository/</guid>
      <pubDate>Tue, 06 Aug 2024 14:01:44 GMT</pubDate>
    </item>
    <item>
      <title>一切都很好，直到这个变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1el959m/everything_was_fine_until_this_change/</link>
      <description><![CDATA[      您好， 我有一个环境，与我分享给您的链接相同，该链接每集有 3500 步。我只是将步长从 3500 增加到 4000，延长了道路，代理开始努力寻找最佳解决方案集。 我的环境有一个最简单的奖励系统，每一步 +1，仅此而已。与 CartPole-v1 相同。如果代理撞到线上，情节就结束。 我使用 N-Step Dueling Double DQN 代理来解决它，它已经达到了最佳奖励，即 3500。延长道路后，代理一直停留在 [2800, 3200] 之间的奖励。 为了增强我的代理，我应该怎么做？如果情节长度使得代理在某个点之后陷入挣扎，你会怎么做？ 添加优先经验重放对我来说不起作用，或者它的参数 alpha=0.6、Beta=0.4 不是一个好的选择？  新算法？ 新的超参数？（增加批次大小、增加经验缓冲区大小、增加隐藏层、增​​加 N_Step、增加 LR 等等？） 用 LSTM 代替简单的神经元？  我想听听您的宝贵建议！ 谢谢……    提交人    /u/OpenToAdvices96   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1el959m/everything_was_fine_until_this_change/</guid>
      <pubDate>Tue, 06 Aug 2024 04:53:26 GMT</pubDate>
    </item>
    <item>
      <title>John Schulman（PPO、OA 联合创始人、后训练/RLHF）离开 OpenAI 加盟 Anthropic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1el3td4/john_schulman_ppo_oa_cofounder_posttrainingrlhf/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1el3td4/john_schulman_ppo_oa_cofounder_posttrainingrlhf/</guid>
      <pubDate>Tue, 06 Aug 2024 00:23:31 GMT</pubDate>
    </item>
    <item>
      <title>[R] 偏好学习：RLHF、最佳 n 采样（BoN）还是直接偏好优化（DPO）？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ekxfih/r_preference_learning_rlhf_bestofn_sampling_bon/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ekxfih/r_preference_learning_rlhf_bestofn_sampling_bon/</guid>
      <pubDate>Mon, 05 Aug 2024 19:56:34 GMT</pubDate>
    </item>
    <item>
      <title>在 Q 学习决斗网络中使用 V(s') 的平均值而不是 argmax Q(s',a) 进行引导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ekui8b/bootstrapping_in_qlearning_dueling_network_using/</link>
      <description><![CDATA[      我一直在做一些实验，我不明白为什么这不起作用。 我使用状态动作架构，我需要对每个动作进行前向传递，然后网络输出单个 q 值估计（Q（s，a）= V（s）+ A（S，a））。我这样做是因为动作空间非常大，我不能使用状态架构。 https://preview.redd.it/5hbcgz450wgd1.png?width=665&amp;format=png&amp;auto=webp&amp;s=38c9b009b7e80c0d2877f86e7aaa5a8d18e7a1e2 我尝试这种方法是因为我有一个非常大的动作空间，计算 S&#39; 中所有动作的 arg max 非常昂贵。通过这种方法，我打算解决这个问题，方法是对所有可用操作的子集进行前向传递并直接使用决斗网络的 V 输出，然后通过对操作子集输出的 V(s&#39;) 取平均值来计算 V(s&#39;) 的估计值。    提交人    /u/RjRdrG   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ekui8b/bootstrapping_in_qlearning_dueling_network_using/</guid>
      <pubDate>Mon, 05 Aug 2024 18:00:31 GMT</pubDate>
    </item>
    <item>
      <title>对于应开展什么项目，有何建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ekug4s/advice_on_what_to_work_on_as_a_project/</link>
      <description><![CDATA[嘿，我有点为难了。我真的不知道我的下一个项目/目标是什么。我觉得我的兴趣很广泛，我真的需要一些建议。 今年秋天我将进入第四年，最近我一直在考虑强化学习方面的研究前景，并希望了解它的理论，也许可以做一些该领域的项目等。然而，我也有点喜欢创业公司/微型软件即服务的想法，并构建产品并将其作为副业进行扩展。毕业后，也可以选择参加 leetcode 以获得更好的全职工作。 我觉得我的兴趣无处不在，因为我看到这些领域有很多很酷的项目/研究，尽管我目前正在做一些事情，但我仍然对下一步该做什么感到困惑。 你们是如何决定要做什么的？感谢你们对此的见解。    提交人    /u/MysteriousAppl3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ekug4s/advice_on_what_to_work_on_as_a_project/</guid>
      <pubDate>Mon, 05 Aug 2024 17:58:22 GMT</pubDate>
    </item>
    <item>
      <title>训练 DDPG 作为 3DOF 飞机的精细调节控制器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ekp2er/training_a_ddpg_to_act_as_a_finely_tuned/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ekp2er/training_a_ddpg_to_act_as_a_finely_tuned/</guid>
      <pubDate>Mon, 05 Aug 2024 14:22:40 GMT</pubDate>
    </item>
    <item>
      <title>比较在线学习/基于模型的算法与离线学习/无模型的算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ek1v0v/comparing_online_learningmodelbased_with_offline/</link>
      <description><![CDATA[嗨，我正在尝试比较基于模型的 RL（即 mcts）和无模型 RL（类似于 ppo）以解决 np-hard 问题。有人对在平等/公平的先决条件下对两种 RL 类型的性能评估/比较有什么建议吗？例如，我如何权衡 mcts 的卓越样本效率与离线学习导致的重新训练需求？应该可以推断出哪一个更适合现实世界的应用。有什么想法吗？此外，我该如何处理超参数？我只想基于性能进行比较，我需要确保超参数不包含在等式中。我将非常感谢任何建议    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ek1v0v/comparing_online_learningmodelbased_with_offline/</guid>
      <pubDate>Sun, 04 Aug 2024 18:16:15 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家连续动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ek1f1d/actor_critic_continuous_action_space/</link>
      <description><![CDATA[我正在尝试在 Keras 中实现 Actor Critic 算法，但无法找到有关如何设置演员网络的全面概述。我已经将 tanh 用于 mu，将 softplus 用于 log_std。有人可以建议这是否正确吗？ def create_actor(self): &quot;&quot;&quot; 创建连续演员神经网络 &quot;&quot;&quot; mu_output = layer.Dense(self.action_shape[0],activation=&#39;tanh&#39;)(self.shared_output) log_std_output = layer.Dense(self.action_shape[0],activation=&#39;softplus&#39;(self.shared_output) self.actor = Model(inputs=self.shared_input,outputs=[mu_output,log_std_output],name=&#39;actor&#39;)     提交人    /u/Electronic_Ad4530   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ek1f1d/actor_critic_continuous_action_space/</guid>
      <pubDate>Sun, 04 Aug 2024 17:57:59 GMT</pubDate>
    </item>
    <item>
      <title>通俗地说，“帕累托”是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ek0rsq/pareto_in_laymans_terms/</link>
      <description><![CDATA[在研究多目标多目标强化学习时，人们会多次遇到“帕累托”这个词。老实说，我甚至无法直观地理解这个意思。现在，我正在阅读一篇论文，它直接指出：  在这项工作中，我们表明，获得多目标机器人控制最佳性能权衡的有效表示是控制策略的帕累托集。我们通过经验表明，使用单个连续策略系列无法有效地表示帕累托集。  对于作者来说，这个意思是理所当然的。 也许你们中有人能帮我吗？ 谢谢    提交人    /u/WilhelmRedemption   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ek0rsq/pareto_in_laymans_terms/</guid>
      <pubDate>Sun, 04 Aug 2024 17:30:49 GMT</pubDate>
    </item>
    <item>
      <title>在峰值 ep_rew_mean 处保存模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejtu0v/save_model_at_peak_ep_rew_mean/</link>
      <description><![CDATA[      嗨， 我有这个简单的自定义环境，我用 SB3 DDPG 和 PPO 进行训练并对它们进行比较。当使用 DDPG 学习时，我观察到 ep_rew_mean 达到峰值水平，然后无法超越它，实际上它无法达到相同的水平并且训练结束。 https://preview.redd.it/nxk1ky332ngd1.png?width=802&amp;format=png&amp;auto=webp&amp;s=0d0a8c98637d8114291745bf95b6ef002e682d19 训练结束时我有了模型。我理解，保存的模型（因此推理性能）将反映次优策略。 我想我可以使用回调来观察最大 ep_rew_mean 达到峰值并仅在那时保存它，而不是在训练结束时保存它。 这是一种常见/好的做法吗？ 或者 ep_rew_mean 没有单调增加，表明我的 Env 或超参数存在问题？ 谢谢亲爱的伙伴们！    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejtu0v/save_model_at_peak_ep_rew_mean/</guid>
      <pubDate>Sun, 04 Aug 2024 12:14:34 GMT</pubDate>
    </item>
    <item>
      <title>这个机器学习库可以让你轻松地训练代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejsz6y/this_machine_learning_library_allows_you_to/</link>
      <description><![CDATA[https://github.com/NoteDance/Note    由   提交  /u/NoteDancing   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejsz6y/this_machine_learning_library_allows_you_to/</guid>
      <pubDate>Sun, 04 Aug 2024 11:23:49 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejkhnp/rl_model_survey/</link>
      <description><![CDATA[我想知道用于少数任务的模型的粗略大小。您在哪个项目中使用了什么模型大小？您的数据规模是多少？一些具有里程碑意义的 RL 里程碑的模型大小是多少？    提交人    /u/SandSnip3r   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejkhnp/rl_model_survey/</guid>
      <pubDate>Sun, 04 Aug 2024 02:21:33 GMT</pubDate>
    </item>
    <item>
      <title>更大的 RL 模型总是更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ejek2r/are_larger_rl_models_always_better/</link>
      <description><![CDATA[大家好，我目前正在我的自定义 RL 环境中尝试使用 stablebaselines 3 中不同大小的 PPO 模型。我假设较大的模型总是比较小的模型更好地最大化平均奖励。但我的环境/奖励函数似乎恰恰相反。这是正常的还是表示有错误？ 此外，训练/学习时间如何随模型大小而变化？是不是一个大得多的模型需要比小模型长 10 到 100 倍的训练时间，而只需更长的训练时间就可以解决我的问题？ 作为参考，该任务与本文中的情况非常相似 https://github.com/yininghase/multi-agent-control。当我谈论小模型时，我指的是 2 个 64 层的模型，而大模型则是 ~5 个 512 层的模型。 谢谢你的帮助 &lt;3    提交人    /u/Adorable-Spot-7197   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ejek2r/are_larger_rl_models_always_better/</guid>
      <pubDate>Sat, 03 Aug 2024 21:32:48 GMT</pubDate>
    </item>
    </channel>
</rss>