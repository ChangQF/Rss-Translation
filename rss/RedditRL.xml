<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 09 Jul 2024 15:15:36 GMT</lastBuildDate>
    <item>
      <title>使用 RL 生成路径</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz4mcu/path_generation_using_rl/</link>
      <description><![CDATA[我一直在尝试训练一个 RL 模型，这样给定一个 3D 空间中的目标和一个初始起始位置，就会采取小步骤来达到目标​​。所以基本上就像从初始位置到最终位置的路径生成。我一直在使用稳定的基线 3 和健身房。动作空间是连续的，在 x、y 和 z 方向上从 -1 到 1。观察空间也是连续的，在 x、y 和 z 方向上从 -10 到 10。我给出的奖励要么是密集奖励，这取决于从当前位置到目标的距离，要么是稀疏奖励（如果到目标的距离大于某个阈值，则为 -1，否则奖励为 0）。 我尝试了各种情节长度和步长。我遗漏了什么或做错了什么吗？    提交人    /u/Necessary-Cabinet-43   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz4mcu/path_generation_using_rl/</guid>
      <pubDate>Tue, 09 Jul 2024 15:02:16 GMT</pubDate>
    </item>
    <item>
      <title>如何训练相同的 RL 代理（具有单独的策略）以在每个时间步骤中传递信息？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz33aa/how_to_train_identical_rl_agents_with_individual/</link>
      <description><![CDATA[你好 我正在 StabeBaseline3 中使用 SAC 进行强化学习，它对于单个代理来说运行良好。现在，我想训练多个相同的代理（使用它们自己的单独策略），以便每个代理的操作都用于为所有代理生成新的观察结果。 你能建议我一些可以帮助我完成此操作的库吗？一个小型的工作示例将是最好的选择。我看过 RLIB (ray)，但仍然不知道如何在其中使用 stablebaseline3。 谢谢    提交人    /u/Previous-Advance-921   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz33aa/how_to_train_identical_rl_agents_with_individual/</guid>
      <pubDate>Tue, 09 Jul 2024 13:58:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么不使用 Sigmoid？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz1xbn/why_isnt_sigmoid_used/</link>
      <description><![CDATA[大家好，我正在使用 Unity 在 C# 中从头开始制作一个简单的策略梯度学习算法，没有库，我想知道为什么没有人使用强化学习中的 sigmoid 函数作为输出 一切都可以在网上找到，每个人都使用 softmax 函数来输出代理可以采取的动作的概率分布，然后他们随机（偏向更高的动作）选择一个动作，但这种方法只允许代理在每个状态下执行一个动作，例如。它既可以向前移动，也可以射击，但我不能同时做这两件事，我知道有方法可以解决这个问题，即为代理可以采取的每组动作制作多个输出层，但我想知道你是否也可以有一个映射到动作的 S 形输出层  比如如果我让一个代理学习走路和射击敌人，使用软最大值，你会有一个用于走路的输出层和一个用于射击的输出层，但使用 S 形，你只需要一个输出层，其中 5 个神经元映射到 4 个方向的移动和射击，这取决于神经元输出的值是否大于 0.5 TLDR：而不是使用软最大值函数层或多层，你可以使用一个大的层，其中 S 形函数映射到基于值是否大于 0.5 的动作    提交人    /u/DaMrStick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz1xbn/why_isnt_sigmoid_used/</guid>
      <pubDate>Tue, 09 Jul 2024 13:05:37 GMT</pubDate>
    </item>
    <item>
      <title>来自深度视频馈送的端到端控制网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz1hs0/end_to_end_control_network_from_depth_video_feed/</link>
      <description><![CDATA[我正在尝试创建一个网络，该网络使用黑白图像（深度图）以及基本惯性测量（速度、姿态）来自主控制四轴飞行器并具有避障能力。拟议的网络将输出身体速率命令，这些命令将被馈送到低级 PID 控制器。 这有多可行？有没有这种端到端解决方案在现实世界中成功的例子？（不限于无人机） 通常，我们首先做一些像 SLAM 和 VIO 这样的操作来进行定位和状态估计，然后我们必须制定一个规划算法，然后我们才能有一个真正发出身体速率命令的神经网络控制器，这听起来像是一堆不必要的抽象 + 工作。 我非常好奇我是否发现了什么，我应该实现它😅   由    /u/FutureComedian7749  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz1hs0/end_to_end_control_network_from_depth_video_feed/</guid>
      <pubDate>Tue, 09 Jul 2024 12:45:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么与离策略算法相比，状态表示学习方法（通过辅助损失）较少应用于 PPO 等在线策略 RL 算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</link>
      <description><![CDATA[我已经看到了不同的状态表征学习方法（通过辅助损失，无论是自我预测还是基于结构化探索），它们已经与离线策略方法（如 DQN、Rainbow、SAC 等）一起应用。例如，SPR（自我预测表征）已与 Rainbow 一起使用，CURL（强化学习的对比无监督表征）已与 DQN、Rainbow 和 SAC 一起使用，以及RA-LapRep（通过拉普拉斯表征进行表征学习）已与 DDPG 和 DQN 一起使用。我很好奇为什么这些方法没有像 PPO（近端策略优化）这样的在线策略算法得到广泛应用。将这些表示学习技术与在线策略算法学习相结合是否存在理论问题？    提交人    /u/C7501   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</guid>
      <pubDate>Tue, 09 Jul 2024 10:56:37 GMT</pubDate>
    </item>
    <item>
      <title>有没有任何具有随机性的离线 RL 基准？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</link>
      <description><![CDATA[我正在研究风险敏感的离线强化学习，发现许多众所周知的基准并不合适，因为它们缺乏随机性。Mujoco 环境几乎是确定性的；它们的内部代码不包含任何随机性，除了在“重置”期间阶段。 [参见：https://arxiv.org/abs/2205.15967\]  我发现 NeoRL 也是如此 [https://arxiv.org/abs/2102.00714.\] 您可以轻松验证运行 TD3PlusBC 时没有差异。 那么，有针对具有随机性的离线 RL 的基准吗？    提交人    /u/korsyoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</guid>
      <pubDate>Tue, 09 Jul 2024 10:09:56 GMT</pubDate>
    </item>
    <item>
      <title>强化学习代理没有采取现实行动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyxh40/reinforcement_learning_agent_not_taking_realistic/</link>
      <description><![CDATA[我在 Simulink 环境中使用 PPO 代理，但代理产生的操作似乎是离散的。具体来说，代理仅输出上限或下限。您知道为什么会发生这种情况吗？我使用 RL Toolbox 进行训练。以下是有关我的设置的一些详细信息：  我使用带有 ode23t 求解器的可变步长时间 Simulink 模型。 我的 Simulink 模型使用 Simscape 热流体库并模拟简化的区域供热网络。DHN 有 2 个分支：北 (NORD) 和南 (SUD)。 我尝试使用 RL 代理来优化控制，最初专注于通过更改分支中的质量流量来最大限度地降低能源成本。   关于代理的超参数，我使用的是具有以下参数的 RL 工具箱：   采样时间 = 3600  折扣因子 = 0.99  GPU  批次大小 = 512  学习率 = 1e-3（对于演员和评论家）   我怀疑我的模型或代理可能有问题。 我将附上 Simulink 模型（应事先加载属性表）。 我希望问题清楚，并且有人可以提供帮助！ 提前谢谢您！    提交人    /u/Resident_Wish9453   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyxh40/reinforcement_learning_agent_not_taking_realistic/</guid>
      <pubDate>Tue, 09 Jul 2024 08:42:43 GMT</pubDate>
    </item>
    <item>
      <title>如何处理3D体素观察？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</link>
      <description><![CDATA[我目前在使用 3D 体素状态训练 PPO 时遇到了困难。 3D 体素的形状为 [64, 128, 128]，区域信息很重要。 只使用 3D CNN 编码器可以吗？ 我是强化学习的新手，我还没有看到任何使用 3D 编码器的论文，而且大多数 RL 教程都使用 2d CNN 编码器或仅使用 MLP     提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</guid>
      <pubDate>Tue, 09 Jul 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>RLHub：强化学习环境的统一平台 - 寻求反馈！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</link>
      <description><![CDATA[嗨，我的 RL 伙伴们！ 我是加州大学伯克利分校的博士后，和几个朋友一起在开展一个名为 RLHub 的新项目，我很想听听你们的想法。我们的想法是创建一个标准化的强化学习环境平台，类似于 Hugging Face 为 NLP 模型所做的工作。 主要功能：1. 适用于各种 RL 环境（mujoco、unity、gym 等）的统一 API 2. 轻松上传和共享自定义环境 3. 自动依赖项管理 4. 本地和云执行选项 5. 标准化元数据和文档 可能的附加功能：- 标准化主要算法（PPO、DDPG、TD3……）UI，用于在云端训练代理 目标是简化查找、使用和共享 RL 环境的过程。研究人员可以轻松地在多种环境中尝试他们的算法，环境创建者可以接触到更广泛的受众。 我有一些问题：1. 这对您的工作有用吗？2. 您会优先考虑哪些功能？3. 对标准化有什么顾虑？4. 关于在 MVP 中包含云执行的想法？ 我特别想听听 RL 研究人员和从业人员的意见。您对当前 RL 环境管理有哪些痛点，可以解决这些痛点吗？ 提前感谢您的任何反馈！    提交人    /u/elonmusk-A12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</guid>
      <pubDate>Tue, 09 Jul 2024 03:03:51 GMT</pubDate>
    </item>
    <item>
      <title>神经网络调试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyjkv6/neural_network_debugging/</link>
      <description><![CDATA[大家好， 我知道神经网络调试的基础知识。但我想知道是否有人可以分享在训练、测试和生产阶段进行调试的技巧。我相信这在这里会非常有帮助。    提交人    /u/MuscleML   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyjkv6/neural_network_debugging/</guid>
      <pubDate>Mon, 08 Jul 2024 20:43:56 GMT</pubDate>
    </item>
    <item>
      <title>Rnd 与 rnn 网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy8g0z/rnd_with_rnn_networks/</link>
      <description><![CDATA[我有带有 rnn 网络的 Ppo 用于策略和参与者。我想添加像 rnd 这样的好奇心机制，我想知道目标和预测网络是否也应该包括 rnn……有人有这方面的经验吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy8g0z/rnd_with_rnn_networks/</guid>
      <pubDate>Mon, 08 Jul 2024 13:05:17 GMT</pubDate>
    </item>
    <item>
      <title>我到底该怎么做？当操作无法影响即将到来的状态时出现问题...</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy6y4d/what_exactly_should_i_do_problem_when_actions/</link>
      <description><![CDATA[您好， 考虑一个非常简单的玩具问题。您有一辆汽车，它沿 x 轴移动。汽车以一定的起始速度在 1 维中移动。您的汽车有 2 个刹车，一旦打开刹车，就无法收回。这意味着：如果您打开第一个刹车，就无法再次关闭它，您已经失去了机会。如果您在错误的时间打开第一个刹车而犯了一个错误，那么您必须小心，在最佳时间打开第二个刹车。 环境：  状态：&lt;位置、速度、刹车状态 (0：无、1：刹车\_1 打开、2：刹车\_2 打开)&gt; 操作：0 或 1 (当前刹车状态 += 操作 --&gt; 根据当前状态添加操作) 奖励 = - (最后位置 - 目标位置) ^ 2 起始速度 = 10，起始位置 = 0，目标位置 = 55 如果打开，第一个刹车接合：速度 -= 1 如果打开，第二个刹车接合：速度-= 2 情节结束 -&gt; （如果位置 &gt; 65 或速度 &lt; 0）  -&gt; 解决方案是：在第 3 步打开第 1 个制动器，在第 4 步打开第 2 个制动器 10 + 10 + 10（在此处打​​开 Brake_1）+ 9（在此处打​​开 Brake_2）+ 7 + 5 + 3 + 1 = 55 我的问题： 打开 Brake_2 后，操作将不再产生任何效果。我的意思是：&lt;State, Action=0, Same\_Reward, Same\_Next\_State&gt;，&lt;State, Action=1, Same\_Reward, Same\_Next\_State&gt;。无论代理尝试什么操作，它都会进入相同的 next_state 并获得相同的奖励。基本上，代理已经失去了改变即将到来的状态的能力。 如果我添加一个由关闭中断组成的机制，代理将继续具有塑造即将到来的状态的能力，这意味着塑造汽车的最后位置直到情节结束。这已经奏效，代理能够找到正确的动作组合。动作基本上是可逆的。 如果我创建如上所述的环境，由于 Break_2 之后，保持对汽车速度的控制的能力将消失，代理开始努力解决问题。动作基本上是不可逆的。 总而言之，如果在某些事件之后动作对下一个状态和奖励实际上没有影响，我该怎么办？忽略中间的 &lt;状态、动作、奖励、下一个状态、完成&gt;，等到情节结束，并将最后一个状态和奖励与 Break_2 打开的上一个状态相结合？    提交人    /u/OpenToAdvices96   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy6y4d/what_exactly_should_i_do_problem_when_actions/</guid>
      <pubDate>Mon, 08 Jul 2024 11:51:32 GMT</pubDate>
    </item>
    <item>
      <title>创建街头霸王 II：世界战士 AI 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy5fjt/creating_a_street_fighter_ii_the_world_warrior_ai/</link>
      <description><![CDATA[是否可以在 Python 中在 GymRetro 或 StableRetro 中玩游戏？如果可以，我是否可以上传自己的游戏方式（按下按钮）以用于训练我自己的 AI 模型。非常感谢！    提交人    /u/More-Background-1626   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy5fjt/creating_a_street_fighter_ii_the_world_warrior_ai/</guid>
      <pubDate>Mon, 08 Jul 2024 10:21:46 GMT</pubDate>
    </item>
    <item>
      <title>不同种子的 SAC 性能存在差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dy0t9m/variability_in_performance_of_sac_from_seed_to/</link>
      <description><![CDATA[您好！我目前正在尝试为离散动作空间实现 SAC，以便在 OpenAI LunarLander 环境中使用。然而，在训练过程中，我遇到了代理在一个种子上表现良好，但在另一个种子上（具有相同的超参数）表现较差的问题。我该如何解决这个问题？任何帮助都将不胜感激，因为我已经为此奋斗了几个月。 代码 Alpha/Actor Loss Alpha 值和情景奖励 Q 函数损失 每个彩色图表代表一次训练运行，它们之间的唯一区别是不同的种子。    提交人    /u/Tight_Apple_678   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dy0t9m/variability_in_performance_of_sac_from_seed_to/</guid>
      <pubDate>Mon, 08 Jul 2024 05:13:36 GMT</pubDate>
    </item>
    <item>
      <title>纯探索中的顺序减半算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dxqs6v/sequential_halving_algorithm_in_pure_exploration/</link>
      <description><![CDATA[      在 Tor Lattimore 和 Csaba Szepsvari 的书的第 33 章中 https://tor-lattimore.com/downloads/book/book.pdf#page=412 他们展示了顺序减半算法，如下图所示。我的问题是为什么在第 6 行我们必须忘记来自其他迭代 $l$ 的所有样本？我尝试实现该算法，记住上次运行中采样的样本，并且效果很好，但我不明白算法中提到的忘记过去迭代中生成的所有样本的原因。 https://preview.redd.it/ufmxz837u5bd1.png?width=1275&amp;format=png&amp;auto=webp&amp;s=87a37f7eadb3fc9faf70d1423b5998289765cb34    由    /u/VanBloot  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dxqs6v/sequential_halving_algorithm_in_pure_exploration/</guid>
      <pubDate>Sun, 07 Jul 2024 20:59:16 GMT</pubDate>
    </item>
    </channel>
</rss>