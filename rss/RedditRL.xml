<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 20 Feb 2024 12:23:37 GMT</lastBuildDate>
    <item>
      <title>通过 PPO 进行迟滞作用，这有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auwexp/hysteresis_as_action_via_ppo_does_it_make_sense/</link>
      <description><![CDATA[我有一个 PPO 代理和持续观察和行动的空间。如果某物处于正确/不正确的位置，则一个操作应该提供信息。但为了避免切换问题，位置应该是稳定值。我们可以先说约后。如果物体处于正确/不正确的位置，则需要 20 个步骤。这意味着一旦物体处于正确位置并更改为错误位置，操作应从 1 降至 -1，反之亦然。  我使用高斯函数作为奖励函数，但在执行此操作之前，我将操作转换为范围 0...1。  奖励 = 1 * math.exp(-(math.pow((action - Expected_action), 2)) / (2 * math.pow(0.3, 2))) 观察有20行历史记录： Box(low=-1, high=1, shape=(20, 6), dtype=np.float64) 我的问题是的，PPO无法解决这个问题，我尝试通过rl-baselines3-zoo优化超参数，但没有好的结果。我的意思是剧集长度约为 160 步，我达到最大奖励 120，然后甚至奖励也在下降。我已停用其他操作的奖励计算，以便仅针对滞后计算检查代理的行为和性能。  我知道，这可以通过监督学习来解决，但我还有更多的行动。  长话短说，我做错了什么？ ​    ;由   提交/u/Inevitable_Engineer5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auwexp/hysteresis_as_action_via_ppo_does_it_make_sense/</guid>
      <pubDate>Mon, 19 Feb 2024 19:40:06 GMT</pubDate>
    </item>
    <item>
      <title>是否有与 JAX 的 vmap 兼容的实验日志框架？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auqhfb/is_there_an_experiment_logging_framework/</link>
      <description><![CDATA[大家好！随着我对 JAX 越来越熟悉，一件非常酷的事情是我可以 vmap 整个训练循环来测试多个种子： # Seeding key = jax.random。 PRNGKey(args.seed) keywords = jax.random.split(key, 5) # 编译训练函数 train_vjit = jax.jit(jax.vmap(make_train(args))) # 运行训练 train_output = jax.block_until_ready(train_vjit(keys) ))  但是，当我使用 Wandb 时，我认为我只能记录其中一次运行，因为我不确定如何为每个 vmapped 实验设置 wandb 初始化。有没有办法做到这一点，或者是否有一些替代框架支持 vmapped 训练函数并单独记录每个 vmapped 种子而不是覆盖？  &amp;# 32；由   提交 /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auqhfb/is_there_an_experiment_logging_framework/</guid>
      <pubDate>Mon, 19 Feb 2024 15:50:43 GMT</pubDate>
    </item>
    <item>
      <title>也有助于制定阶跃函数和环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auq8w5/help_in_formulating_a_step_function_and/</link>
      <description><![CDATA[我必须接受大约 100~200 的多个输入（取决于测试用例），然后将其传递到方程中，然后输出 i get 是我必须最小化的。输入都有自己的范围，并且输入的不同子集也有自己应等于的值。我需要指导如何描述环境、观察和行动空间。如果有人可以帮助我理解，必须如何实现步骤函数才能最小化它？我现在打算使用 A2C。 我不知道这是问这个问题的合适地方，但任何指导都会有所帮助。谢谢。   由   提交 /u/MysteryManav   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auq8w5/help_in_formulating_a_step_function_and/</guid>
      <pubDate>Mon, 19 Feb 2024 15:40:59 GMT</pubDate>
    </item>
    <item>
      <title>用于收集卷展的独立库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auq20n/standalone_library_for_collecting_rollouts/</link>
      <description><![CDATA[我发现自己不断地重新实现标准的 rollout 收集器循环 (env.reset(seed), env.step （政策（州）））。是否有任何库提供一个模块，该模块使用策略步进环境，并返回非嵌套的输出字典。  python results = env.rollout(policy) print(results[&#39;reward&#39;], results[&#39;action&#39;] ...)   TorchRL 中的一个很接近，但要求您的策略是 TensorDictModule，创建一堆 TorchSpecs，并且有大量依赖项。我宁愿不依赖任何东西，除了 numpy （或 jax，可以简单地转换为 numpy）。   由   提交 /u/smorad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auq20n/standalone_library_for_collecting_rollouts/</guid>
      <pubDate>Mon, 19 Feb 2024 15:33:16 GMT</pubDate>
    </item>
    <item>
      <title>Alpha Zero 适用于每回合包含两个动作的游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aumn42/alpha_zero_for_games_where_each_turn_consists_of/</link>
      <description><![CDATA[嘿， 我正在修改一个 alpha 零存储库以用于名为 野永。两名玩家各有一个由 19 个方块和 3 个令牌组成的六边形棋盘。 在这个游戏中，每一回合都包括将三个令牌之一移动到六个不同方向之一。之后，您可以通过将 19 个方块中的一个移动到另一个位置来修改实际的棋盘。 我已经实现了定义合法移动和游戏结束条件的逻辑。然而，我很难对这些动作进行编码。据我对国际象棋的理解，每个可能的 8x8x73 (=4672) 个动作都由神经网络中的输出神经元表示。 为了简单起见，我们假设 Nonaga 的最大字段大小为 8x8 (实际上它可以变得更大）。为了沿六个方向之一移动令牌之一，我考虑将它们编码为 8 x 8 x 6 (= 384) 个神经元。 对于理论上的图块，有 8 x 8 起始位置和 8 x 8 目标其中大多数位置无效，具体取决于单独导致 4096 个操作的场景。如果我现在尝试将 384 个令牌操作与 4096 个图块操作结合起来，我最终会得到大约 150 万个操作，这似乎不可行。 我怎样才能避免这种情况？我想到了神经网络的两个政策负责人。这行得通吗？提前致谢！   由   提交 /u/LuckerNo1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aumn42/alpha_zero_for_games_where_each_turn_consists_of/</guid>
      <pubDate>Mon, 19 Feb 2024 12:56:07 GMT</pubDate>
    </item>
    <item>
      <title>无限循环</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aujcu9/infinity_loop/</link>
      <description><![CDATA[你好， 我训练了一个代理来玩类似俄罗斯方块的益智游戏。在每一步中，智能体都可以决定将棋子放置在棋盘上的可能位置或随机获得一个新棋子（6 种可能性）。我设置了奖励，以便首选具有尽可能少的部分的解决方案。尽管如此，他仍然可能达到这样一种状态：每个可能的随机块都以决定更喜欢获得新的随机块而告终。这将创造一个无限的游戏。我怎样才能避免这种行为？ 我的意思是代理训练有素，因此不会在每场比赛中都会发生，但如果发生会发生什么？我不能接受一个永无止境的游戏，游戏在解决之前不可能停止。总有一个可能的解决方案，因为即使是 1x1 大小的碎片也存在。 示例：https ://en-wiki.metin2.gameforge.com/index.php/Fishing_Jigsaw 我会感谢每一个想法和支持。从您的经验中了解您将如何设置奖励以及您将训练多少交互也将很有趣。您会选择什么复杂度的 dqn 神经网络？   由   提交/u/Reasonable_Cry8854  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aujcu9/infinity_loop/</guid>
      <pubDate>Mon, 19 Feb 2024 09:30:06 GMT</pubDate>
    </item>
    <item>
      <title>围棋游戏的无模型强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auh6v7/modelfree_rl_for_game_of_go/</link>
      <description><![CDATA[是否有任何无模型强化学习算法（没有 AlphaGO 系列中的 MCTS）可以在围棋比赛中超越人类专家？  如果是这样，它的性能与基于模型的方法相比如何？    由   提交 /u/RebornHugo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auh6v7/modelfree_rl_for_game_of_go/</guid>
      <pubDate>Mon, 19 Feb 2024 07:05:19 GMT</pubDate>
    </item>
    <item>
      <title>在 Pendulum v1 的 PPO 中使用熵正则化和 TanhNormal</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1au1hcd/using_entropy_regularization_with_tanhnormal_in/</link>
      <description><![CDATA[我正在 Pendulum v1 上使用 PPO，对动作空间使用 TanhNormal 分布。在尝试计算熵正则化的熵时，我遇到了负值，据我所知，由于分布在某些区域的高度集中，微分熵可能会发生负值。这给我提出了几个问题： 在像 Pendulum v1 这样的连续动作空间中，TanhNormal 分布得到负微分熵是否很常见？ 在处理问题时使用熵正则化仍然有意义吗？具有微分熵，尤其是当它可以为负时？ 在这种情况下如何通过 MC 采样准确计算熵？我一直在使用： x = new_dist.rsample(sample_shape=torch.Size([10000])) entropy = -torch.mean(new_dist.log_prob(x))  new_dist 是来自 torchrl 的 TanhNormal 分布，根据动作空间用特定的最小和最大边界定义。 我很好奇其他人如何处理类似的熵计算和正则化场景。熵正则化的方法是否会随着负熵值的可能性而改变？任何见解或参考将不胜感激！   由   提交 /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1au1hcd/using_entropy_regularization_with_tanhnormal_in/</guid>
      <pubDate>Sun, 18 Feb 2024 18:50:26 GMT</pubDate>
    </item>
    <item>
      <title>算法收敛于单一行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1atqur7/algorithm_converge_to_single_behaviour/</link>
      <description><![CDATA[嘿 我有一艘飞船需要躲避导弹。 奖励是导弹经过的最短距离宇宙飞船。我随机化船舶偏航角度（方向），我使用 td3，但有些角度更容易解决，这会导致所有角度的单一机动，因为奖励传播到更具挑战性的角度。 （这个动作可能对这些角度来说是最佳的）这是一个连续的问题，我尝试使用常规噪声和奥恩斯坦噪声。 （我宁愿不使用SAC） TL;DR飞船躲避导弹类型，有些角度更容易学习，所有角度都收敛到更容易的角度学习的行为 &lt; !-- SC_ON --&gt;  由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1atqur7/algorithm_converge_to_single_behaviour/</guid>
      <pubDate>Sun, 18 Feb 2024 10:08:51 GMT</pubDate>
    </item>
    <item>
      <title>有人可以帮助我了解如何进行策略迭代吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1atafso/could_anyone_please_help_me_understand_how_to_do/</link>
      <description><![CDATA[      我观看了很多视频，但我很难理解政策的流程迭代。有人可以提供使用所附示例图像实现最佳策略的分步指南吗？ ​ https://preview.redd.it/zzloc9ey87jc1.png?width=1102&amp;format=png&amp;auto= webp&amp;s=3399d830ce0107b5ff48637b3949f1e35a17849a 在此场景中，转移概率如下：A = 0.61、B = 0.39、C = 0.47、D = 0.53、E = 0.84 和 F = 0.16。将连续迭代之间的最大误差 (ε) 视为 0.01，将折扣因子 (γ) 视为 0.2。利用策略迭代方法，“Standing”状态的值是多少？ 提前谢谢。   由   提交 /u/thesmudgelord   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1atafso/could_anyone_please_help_me_understand_how_to_do/</guid>
      <pubDate>Sat, 17 Feb 2024 19:45:12 GMT</pubDate>
    </item>
    <item>
      <title>从头开始在 Unity 中训练 FlappyBird：5 分钟内 10k 个管道！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1asl0ob/training_flappybird_in_unity_from_scratch_10k/</link>
      <description><![CDATA[   /u/imitagent  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1asl0ob/training_flappybird_in_unity_from_scratch_10k/</guid>
      <pubDate>Fri, 16 Feb 2024 22:04:19 GMT</pubDate>
    </item>
    <item>
      <title>专家的混合解锁深度强化学习的参数缩放</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ashob2/mixtures_of_experts_unlock_parameter_scaling_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.08609 摘要：  （自）监督学习模型最近的快速进展是很大一部分是通过经验缩放定律预测的：模型的性能与其大小成正比。然而，对于强化学习领域来说，类似的缩放定律仍然难以捉摸，增加模型的参数数量通常会损害其最终性能。在本文中，我们证明了将专家混合 (MoE) 模块，特别是软 MoE（Puigcerver 等人，2023）纳入基于价值的网络会产生更多参数可扩展的模型，性能的显着提高就证明了这一点跨越各种训练制度和模型大小。因此，这项工作为制定强化学习的缩放法则提供了强有力的经验证据。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ashob2/mixtures_of_experts_unlock_parameter_scaling_for/</guid>
      <pubDate>Fri, 16 Feb 2024 19:46:19 GMT</pubDate>
    </item>
    <item>
      <title>RL 目前有什么用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1arnyvq/what_is_rl_good_for_currently/</link>
      <description><![CDATA[ 由   提交 /u/BadMeditator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1arnyvq/what_is_rl_good_for_currently/</guid>
      <pubDate>Thu, 15 Feb 2024 19:34:51 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 PPO 导航问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/</link>
      <description><![CDATA[      我正在尝试使用 PPO 算法来解决一个简单的机器人导航问题。这是环境截图。 ​ https://preview.redd.it/ja7tq5v5uqic1.png?width=577&amp;format=png&amp;auto=webp&amp;s=e20b72e7f0c29b51c9e890b 3c33cec686d2327f1 &lt; p&gt;机器人（纯蓝色）必须导航到目标配置（空蓝色圆圈）。 演员网络设置为将单个灰度图像作为输入并输出下一个代理动作。  批评者网络将图像和当前时间步作为输入，并输出预期回报。 动作集为  wait&lt; /li&gt; 向前移动 向后移动 旋转 30 度 旋转 -30 度  每一步的奖励由以下公式给出： -0.1 + (dist_prev - dist_curr) + 100（如果达到目标）- 10（如果撞墙） 我使用的网络模型与 Atari DQN Nature 论文中使用的网络模型大致相同。 我面临的困难是，智能体在几千集之后似乎没有学到任何东西。 这些是我的 PPO 超参数：  GAMMA = 0.95 TRAJECTORIES_PER_LEARNING_STEP = 10 UPDATES_PER_LEARNING_STEP = 10 MAX_STEPS_PER_EPISODE = 100 ENTROPY_LOSS_COEF = 0 V_LOSS_CEOF = 0.5 CLIP = 0.2 LR = 3e-4  ​ 这是每集奖励的平滑图，它似乎只表现出随机行为。 ​ https://preview.redd.it/ 1yg4c1acwqic1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=cd55d2c2b4b55bf66dce593b0934a6bc60f24987 问题：  为什么不起作用？&lt; /li&gt; 它应该有效吗？ 您希望它需要多少集？  如果需要，我很乐意分享代码。预先感谢您的评论！ ​   由   提交/u/david-wb  /u/david-wb  reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/</guid>
      <pubDate>Thu, 15 Feb 2024 12:52:14 GMT</pubDate>
    </item>
    <item>
      <title>帮助自定义环境 Pettingzoo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</link>
      <description><![CDATA[我有一个自定义环境，我将代理设置为  self.agents = [&quot;EV_&quot; + str(r) for r in range(num_agents)]  当以以下形式测试环境时，我收到下一个错误： from EVenv import DepotEnv from pettingzoo.test import parallel_api_test from pettingzoo.test import api_test if __name__ == &quot;__main__&quot;: env = DepotEnv(num_agents=3) parallel_api_test(env, num_cycles=1000)  ，第 7 行，在  parallel_api_test(env, num_cycles=1000) 文件“C:\Users\” luisb\EVCHARGING\EVenv\lib\site-packages\pettingzoo\test\parallel_test.py”，第 122 行，parallel_api_test  assert ( AssertionError: [&#39;EV_0&#39;, &#39; EV_1&#39;, &#39;EV_2&#39;] != set()  我到处都找不到解决方案，我尝试设置代理，但它也不起作用，有什么想法吗？     提交者   /u/Barbajan22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</guid>
      <pubDate>Thu, 15 Feb 2024 01:11:18 GMT</pubDate>
    </item>
    </channel>
</rss>