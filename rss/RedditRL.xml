<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 23 Sep 2024 09:19:49 GMT</lastBuildDate>
    <item>
      <title>帮助解决 PPO 图结构最短路径搜索问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnbseg/help_with_ppo_graph_structure_shortest_path/</link>
      <description><![CDATA[      我是韩国强化学习专业的本科生，正在尝试使用 PPO 算法解决受限图结构中的最短路径搜索问题。附件是环境的屏幕截图。 https://preview.redd.it/rsj22c0xehqd1.png?width=2345&amp;format=png&amp;auto=webp&amp;s=3f261bf3b11c533f3fdbcd05172495d8a24c7911 演员和评论家网络使用 GCN（图卷积网络）处理图结构，利用邻接矩阵和节点特征矩阵。节点特征矩阵的设计为每个节点的特征值如下：[节点 ID（节点索引号）、相邻节点号 1、相邻节点号 2]。如果一个节点只有一个邻居，则第二个邻居用 -1 填充。换句话说，矩阵的大小为 [节点数，特征数]。 此外，网络状态值包括代理的状态，该状态由 [当前代理位置（节点索引号）、目标位置（节点索引号）、根据 Dijkstra 算法的剩余路径长度] 组成。 参与者网络使用邻接矩阵和节点特征矩阵通过 GCN 嵌入节点特征，然后展平嵌入的节点特征并将其与代理的状态连接起来。连接的结果通过完全连接层，该层预测动作。动作空间由 3 个选项组成：前进、左转和右转。 对于奖励设计，如果代理在单行道上并且没有选择前进动作，则情节立即结束，并应用 -0.001 的惩罚。如果代理在路口并选择前进，则情节立即结束，并施加 -0.001 的惩罚。如果代理选择左转或右转，并且到达目的地的路径缩短，则给予 0.001 的奖励。当代理到达目的地时，给予 1 的奖励。如果代理未能在 1200 个时间步内到达目的地，则情节以 -0.001 的惩罚结束。我在记录了 120,000 个时间步的经验后更新了模型。 尽管进行了长时间的训练，但在早期阶段，情节成功率和累积奖励有所增加，但在某个时间点之后，性能会停滞在令人不满意的水平。 我的 PPO 超参数如下：  GAMMA = 0.99 TRAJECTORIES_PER_LEARNING_STEP = 512 UPDATES_PER_LEARNING_STEP = 10 MAX_STEPS_PER_EPISODE = 1200 ENTROPY_LOSS_COEF = 0 V_LOSS_CEOF = 0.5 CLIP = 0.2 LR = 0.0003  问题：  为什么这不起作用？ 我的状态表示设计不正确吗？  我的英语水平很差，但感谢您的阅读！    提交人    /u/Latter-Parsnip4425   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnbseg/help_with_ppo_graph_structure_shortest_path/</guid>
      <pubDate>Mon, 23 Sep 2024 03:57:06 GMT</pubDate>
    </item>
    <item>
      <title>Agent 选择相同的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fn4ull/agent_selects_the_same_action/</link>
      <description><![CDATA[大家好， 我正在开发一个 DQN，它根据当前状态从多个规则中一次选择一个规则。但是，无论状态如何，代理都倾向于选择相同的操作。它已训练了 1,000 集，其中 500 集专门用于探索。 该任务涉及维护计划，每次可用时，代理都会选择一条规则来选择要维护的机器。 有人遇到过类似的问题吗？    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fn4ull/agent_selects_the_same_action/</guid>
      <pubDate>Sun, 22 Sep 2024 21:59:04 GMT</pubDate>
    </item>
    <item>
      <title>需要有关如何更好地实施的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmpci8/need_advice_on_getting_better_at_implementation/</link>
      <description><![CDATA[TLDR；从理论到实施的最顺利的过渡方式是什么？ 我目前正在学习 MARL 课程，我们的作业要求我们使用 DP 和 MC 解决 TSP 和推箱子问题。 我们在 Gymnasium 中获得了一些样板代码（用于 TSP），但必须自己实施策略（以及推箱子的环境）。 虽然我了解它们背​​后的概念和数学，但我在实施、为策略使用什么数据结构以及理解 Gymnaisum 方面却举步维艰。 任何建议都将不胜感激    提交人    /u/Illustrious_Sir_2913   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmpci8/need_advice_on_getting_better_at_implementation/</guid>
      <pubDate>Sun, 22 Sep 2024 09:31:45 GMT</pubDate>
    </item>
    <item>
      <title>入门帮助请求。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmlfkm/getting_started_help_request/</link>
      <description><![CDATA[我想创建 RL 来玩西洋双陆棋的变体。  我想写入接口并利用预先存在的 RL 引擎。 是否有可以满足我的需求的 GitHub 存储库？ 或者云服务？ 谢谢， Hal Heinrich    提交人    /u/halheinrich   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmlfkm/getting_started_help_request/</guid>
      <pubDate>Sun, 22 Sep 2024 04:51:42 GMT</pubDate>
    </item>
    <item>
      <title>可以以切片方式采样的离线 RL 数据集？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmc5g5/offline_rl_datasets_that_one_can_sample_in_slice/</link>
      <description><![CDATA[您好， 我目前正在从事一个受这篇论文启发的项目，并且遇到了对可以以切片方式采样的转换数据集的需求。 （大小为 (B, S, *) 或 (S, B, *) 的批次，其中 S 是相同轨迹的连续切片的维度） 我正在尝试使 d4rl-atari 数据集工作，但是在让它对连续切片进行采样时遇到了一些麻烦，所以我想知道这里是否有人有什么建议。 域本身并不是太重要，但我更喜欢使用像素观测。    提交人    /u/Ayy_Limao   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmc5g5/offline_rl_datasets_that_one_can_sample_in_slice/</guid>
      <pubDate>Sat, 21 Sep 2024 20:29:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习，SUMO模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm9cja/reainforcement_learning_sumo_simulation/</link>
      <description><![CDATA[        提交人    /u/IllIntroduction9410   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm9cja/reainforcement_learning_sumo_simulation/</guid>
      <pubDate>Sat, 21 Sep 2024 18:20:11 GMT</pubDate>
    </item>
    <item>
      <title>IsaacLab：如何将它与 TorchRL 一起使用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm99jv/isaaclab_how_to_use_it_with_torchrl/</link>
      <description><![CDATA[有人知道如何将 TorchRL 与 IsaacLab 一起使用吗？不幸的是，TorchRL 没有包装器。我可以轻松构建自己的包装器吗？或者存在任何其他解决方案吗？    提交人    /u/Salt_Classroom_7380   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm99jv/isaaclab_how_to_use_it_with_torchrl/</guid>
      <pubDate>Sat, 21 Sep 2024 18:16:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] RL 中的 LTL 的当前状态是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm6k4h/d_what_is_the_current_state_of_ltl_in_rl/</link>
      <description><![CDATA[我想知道为什么在将线性时间逻辑和模型检查纳入强化学习方面没有那么多论文。更具体地说，在无模型的 POMDP 场景中。在我看来，这是保证此类关键设备安全性的一个非常重要的部分，但谈论它的论文并没有得到很多引用。这些技术是否不够实用（我意识到它们通常会扩展状态空间以在采样轨迹时直接检查 LTL）？还有其他我不知道的技术吗？我真的很想知道你的经历。谢谢！    提交人    /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm6k4h/d_what_is_the_current_state_of_ltl_in_rl/</guid>
      <pubDate>Sat, 21 Sep 2024 16:13:42 GMT</pubDate>
    </item>
    <item>
      <title>日常生活中的 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fli9u5/rl_in_your_day_to_day/</link>
      <description><![CDATA[嗨，RL 社区， 我有 6 年的电子通讯/技术 DS 经验，但主要专注于实验和建模。我正在寻找下一个机会，希望更多地转向 RL。 我很想听听社区的意见，他们实际上是在为他们的日常角色构建 RL 系统。更具体地说，您正在解决什么类型的问题，您正在构建哪种类型的算法，等等。我针对角色领域/问题类型进行了民意调查，但也欢迎您发表评论，详细说明您使用 RL 的目的。谢谢！ 查看民意调查    提交人    /u/Djekob   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fli9u5/rl_in_your_day_to_day/</guid>
      <pubDate>Fri, 20 Sep 2024 17:58:23 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习与策略梯度在网络规模方面的比较</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fld2f3/deep_qlearning_vs_policy_gradient_in_terms_of/</link>
      <description><![CDATA[我一直在使用策略梯度和深度 Q 网络算法处理 CartPole 任务。我观察到，策略梯度算法在较小的网络（一个 16 个神经元的隐藏层）中的表现优于深度 Q 网络，后者需要更大的网络（两个分别有 1024 个和 512 个神经元的隐藏层）。学术界是否就这两种算法实现可比性能所需的网络规模达成共识？    提交人    /u/Atreya95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fld2f3/deep_qlearning_vs_policy_gradient_in_terms_of/</guid>
      <pubDate>Fri, 20 Sep 2024 14:15:30 GMT</pubDate>
    </item>
    <item>
      <title>证明遗憾的界限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1flcctx/proving_regret_bounds/</link>
      <description><![CDATA[我是一名本科生，我的研究是尝试证明在线学习问题的遗憾界限。 有没有人有资源可以帮助我从头开始熟悉遗憾分析？这些资源可以假设本科生概率的舒适度。 更新：感谢大家的建议！我最终阅读了一些论文和资源，查看了示例，这给了我一个证明的想法。我最终完成了一个遗憾界限证明！    提交人    /u/Mysterious-Ad-3855   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1flcctx/proving_regret_bounds/</guid>
      <pubDate>Fri, 20 Sep 2024 13:43:53 GMT</pubDate>
    </item>
    <item>
      <title>在哪里以及为什么使用折扣累积奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl9g6r/where_and_why_is_discounted_cumulative_reward_used/</link>
      <description><![CDATA[嗨，我是强化学习的新手，我现在正在学习一些基本术语。我遇到了“折扣累积奖励”这个术语，我理解即时奖励比未来奖励更有价值，但我不明白折扣累积奖励何时会用到。我用谷歌搜索了它，但我找到的都是“折扣累积奖励”是什么，但没有具体的例子说明它可能在哪里使用。它是否仅用于估计累积奖励，其中后期奖励被折扣，因为它们不太可预测？有没有具体的真实例子说明它可能在哪里使用？    提交人    /u/AdBitter9336   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl9g6r/where_and_why_is_discounted_cumulative_reward_used/</guid>
      <pubDate>Fri, 20 Sep 2024 11:13:26 GMT</pubDate>
    </item>
    <item>
      <title>推荐涵盖最新算法的调查/学习材料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</link>
      <description><![CDATA[您好，有人可以推荐一些涵盖较新算法/技术（td-mpc2、dreamerv3、diffusion policy）的调查/学习材料吗？其格式类似于 openai 的 spinningup/lilianweng 的博客，现在有点过时了？谢谢    提交人    /u/saintshing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</guid>
      <pubDate>Fri, 20 Sep 2024 07:14:21 GMT</pubDate>
    </item>
    <item>
      <title>LeanRL：一个简单的 PyTorch RL 库，用于快速（>5 倍）训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</link>
      <description><![CDATA[我们很高兴地宣布，我们已经开源了LeanRL，这是一个轻量级的 PyTorch 强化学习库，它使用 torch.compile 和 CUDA 图表提供快速 RL 训练的方法。 通过利用这些工具，与原始 CleanRL 实现相比，我们实现了显着的加速 - 速度提高了 6 倍！ RL 训练的问题 强化学习是出了名的 CPU 受限，因为小型 CPU 操作（例如从模块中检索参数或在 Python 和 C++ 之间转换）的频率很高。幸运的是，PyTorch 强大的编译器可以帮助缓解这些问题。但是，输入编译后的代码也有其自身的成本，例如检查保护以确定是否需要重新编译。对于 RL 中使用的小型网络，这种开销可能会抵消编译的好处。 进入 LeanRL LeanRL 通过提供简单的方法来加速您的训练循环并更好地利用您的 GPU，从而解决了这一挑战。受到 gpt-fast 和 sam-fast 等项目的启发，我们证明了 CUDA 图可以与 torch.compile 结合使用，以实现前所未有的性能提升。我们的结果表明：  使用 PPO（Atari）可提高 6.8 倍速度 使用 SAC 可提高 5.7 倍速度 使用 TD3 可提高 3.4 倍速度 使用 PPO（连续动作）可提高 2.7 倍速度  此外，LeanRL 可以更有效地利用 GPU，让您可以同时训练多个网络而不会牺牲性能。 主要特点  具有最小依赖性的 RL 算法的单文件实现 所有技巧都在 README 中进行了说明 从流行的 CleanRL 分叉   在 https://github.com/pytorch-labs/leanrl 上查看 LeanRL    由    /u/AdCool8270  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</guid>
      <pubDate>Fri, 20 Sep 2024 00:22:12 GMT</pubDate>
    </item>
    <item>
      <title>介绍 OpenAI GPT-4 o1：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>