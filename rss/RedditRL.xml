<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯æ—¨åœ¨æ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå’Œå­¦ä¹ å¦‚ä½•æœ€ä½³è·å¾—å¥–åŠ±çš„AI/ç»Ÿè®¡æ•°æ®çš„å­åœºã€‚ä¾‹å¦‚Alphagoï¼Œä¸´åºŠè¯•éªŒå’ŒA/Bæµ‹è¯•ä»¥åŠAtariæ¸¸æˆã€‚</description>
    <lastBuildDate>Thu, 27 Feb 2025 03:27:42 GMT</lastBuildDate>
    <item>
      <title>è·¨é¢†åŸŸçš„å‡‰çˆ½è‡ªæˆ‘æ ¡æ­£æœºåˆ¶ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  ä»æ§åˆ¶ç†è®ºçš„åé¦ˆå¾ªç¯å’Œå¡å°”æ›¼è¿‡æ»¤åˆ°è‡ªç„¶é€‰æ‹©ï¼ŒDNAä¿®å¤ï¼Œå¤šæ•°æŠ•ç¥¨å’Œå¼•å¯¼ - æ— æ•°çš„æ–¹å¼ç³»ç»Ÿè‡ªæˆ‘æ ¡æ­£é”™è¯¯ï¼Œå°¤å…¶æ˜¯å½“åœ°é¢çœŸç†æœªçŸ¥æ—¶ï¼ç”Ÿç‰©å­¦ï¼Œäººå·¥æ™ºèƒ½ï¼Œç»Ÿè®¡ç”šè‡³è¿›åŒ–ä¼¼ä¹éƒ½èåˆäº†æœºåˆ¶ä»¥éåˆ¶è¯¯å·®ä¼ æ’­ã€‚æƒ³çŸ¥é“æ‚¨é‡åˆ°çš„æœ‰è¶£çš„è‡ªæˆ‘çº æ­£æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Œæ— è®ºæ˜¯è‡ªç„¶ç•Œï¼Œå“²å­¦ï¼Œå·¥ç¨‹è¿˜æ˜¯ä»¥åï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/neat_comparison_2726      [link]        [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</guid>
      <pubDate>Wed, 26 Feb 2025 19:48:15 GMT</pubDate>
    </item>
    <item>
      <title>ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨GRPOï¼ˆ5GB VRAMæœ€å°å€¼ï¼‰è®­ç»ƒè‡ªå·±çš„æ¨ç†æ¨¡å‹ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å˜¿ï¼Œå¾ˆæ£’çš„äººï¼ç¬¬ä¸€ç¯‡æ–‡ç« åœ¨è¿™é‡Œï¼ä»Šå¤©ï¼Œæˆ‘å¾ˆé«˜å…´åœ°å®£å¸ƒï¼Œæ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨grpo +ä½¿ç”¨Grpo +æˆ‘ä»¬çš„å¼€æ”¾å¼é¡¹ç›®unsplothä½¿ç”¨5GB VRAMè®­ç»ƒè‡ªå·±çš„æ¨ç†æ¨¡å‹ï¼š httpsï¼š&gt; httpsï¼š&gt; httpsï¼š&gt;æ˜¯DeepSeek-R1èƒŒåçš„ç®—æ³•ä»¥åŠå¦‚ä½•å—è¿‡è®­ç»ƒã€‚å®ƒæ¯”PPOæ›´æœ‰æ•ˆï¼Œæˆ‘ä»¬è®¾æ³•å°†VRAMä½¿ç”¨é™ä½äº†90ï¼…ã€‚æ‚¨éœ€è¦ä¸€ä¸ªå¤§çº¦500è¡Œï¼Œç­”æ¡ˆå¯¹å’Œå¥–åŠ±åŠŸèƒ½çš„æ•°æ®é›†ï¼Œç„¶åå¯ä»¥å¯åŠ¨æ•´ä¸ªè¿‡ç¨‹ï¼ è¿™å…è®¸å°†ä»»ä½•å¼€æ”¾çš„LLMï¼ˆå¦‚Llameï¼ŒMisstralï¼ŒPhiç­‰ï¼‰ç­‰å¼€æ”¾ï¼Œå¯ä»¥å°†å…¶è½¬æ¢ä¸ºå…·æœ‰é“¾é“¾è¿‡ç¨‹çš„æ¨ç†æ¨¡å‹ã€‚å…³äºGRPOçš„æœ€å¥½çš„éƒ¨åˆ†æ˜¯ï¼Œä¸æ›´å¤§çš„å‹å·ç›¸æ¯”ï¼Œä¸æ›´å¤§çš„è®­ç»ƒæ—¶é—´ç›¸æ¯”ï¼Œä¸è¾ƒå¤§çš„è®­ç»ƒæ—¶é—´ç›¸æ¯”ï¼Œä¸è¾ƒå¤§çš„å‹å·ç›¸æ¯”ï¼Œè®­ç»ƒå°å‹å‹å·ä¸è¾ƒå¤§çš„å‹å·æ— å…³ç´§è¦ï¼Œå› æ­¤æœ€ç»ˆç»“æœå°†éå¸¸ç›¸ä¼¼ï¼æ‚¨ä¹Ÿå¯ä»¥åœ¨æ‰§è¡Œå…¶ä»–æ“ä½œçš„åŒæ—¶ï¼Œåœ¨PCçš„èƒŒæ™¯ä¸‹è¿›è¡ŒGRPOåŸ¹è®­ï¼  ç”±äºæˆ‘ä»¬æ–°æ·»åŠ çš„æœ‰æ•ˆçš„GRPOç®—æ³•ï¼Œè¿™ä½¿å¾— 10xæ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦é•¿åº¦  90ï¼…ä½¿ç”¨ 90ï¼…çš„vram  vram/strong&gt; vraM/strong&gt; lora/qlora/qula li&gt; li afteraive  li&gt; field afteraiment &lt;0&gt; æ ‡å‡†GRPOè®¾ç½®ï¼ŒLlama 3.1ï¼ˆ8bï¼‰20Kä¸Šä¸‹æ–‡é•¿åº¦çš„åŸ¹è®­éœ€è¦510.8GBçš„VRAMã€‚ä½†æ˜¯ï¼ŒUnsplothçš„90ï¼…VRAMå‡å°‘çš„è¦æ±‚ä½¿åŒä¸€è®¾ç½®ä¸­çš„éœ€æ±‚ä»…ä¸º54.3GBã€‚ æˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬çš„æ¸å˜â€æ£€æŸ¥ algorithmï¼Œæˆ‘ä»¬å‘å¸ƒäº†ä¸€ä¸ªaLgorithmã€‚å®ƒå¯ä»¥å·§å¦™åœ°å°†ä¸­é—´æ¿€æ´»å¸è½½åˆ°ç³»ç»ŸRAMå¼‚æ­¥ï¼ŒåŒæ—¶ä»…æ…¢1ï¼…ã€‚æ­¤å‰ƒé¡»372GB VRAM ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦num \ _ generations = 8ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸­é—´æ¢¯åº¦ç´¯ç§¯è¿›ä¸€æ­¥å‡å°‘æ­¤å†…å­˜ä½¿ç”¨ã€‚ ä½¿ç”¨Googleçš„å…è´¹ä¸Šä¸‹æ–‡ä½¿ç”¨æˆ‘ä»¬çš„GRPO Notebookï¼Œä½¿ç”¨Googleçš„å…è´¹gpusï¼š href =â€œ https://colab.research.google.com/github/unslothai/notebooks/blob/blob/main/nb/llama3.1_(8Bâ€&gt; llama 3.1ï¼ˆ8bï¼‰on colab  -grpo.ipynbï¼‰ä»¥åŠæ›´å¤šï¼š align =â€œ leftâ€&gt; metric   unsploth   trl + fa2           training Moregre Costï¼ˆGBï¼‰ align =â€œå·¦â€&gt; 414GB      grpoå†…å­˜æˆæœ¬ï¼ˆgbï¼‰   9.8gb    78.3gb  78.3gb  78.3gb    0gb   16gb      æ¨ç†20Kä¸Šä¸‹æ–‡ï¼ˆGBï¼‰   2.5GB  2.5gb  æ€»å†…å­˜ä½¿ç”¨   54.3GBï¼ˆå°‘90ï¼…ï¼‰       510.8GB              æˆ‘ä»¬åœ¨æ‰€æœ‰æ–¹é¢éƒ½èŠ±äº†å¾ˆå¤šæ—¶é—´ï¼ˆpboty&gt;  ï¼šd   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/yoracale     [link]   [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</guid>
      <pubDate>Wed, 26 Feb 2025 19:41:48 GMT</pubDate>
    </item>
    <item>
      <title>ç­–åˆ’å¯å¡‘æ€§æŸå¤±çš„è®ºæ–‡æ¸…å•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å—¨ï¼Œ æˆ‘å·²ç»åˆ›å»ºäº†ä¸€ä¸ªå­˜å‚¨åº“ï¼Œå…¶ä¸­åŒ…å«æœ‰å…³å¯å¡‘æ€§æŸå¤±çš„è®ºæ–‡åˆ—è¡¨ã€‚é‡ç‚¹æ˜¯æ·±åº¦RLï¼Œä½†æ˜¯é‚£é‡Œä¹Ÿæœ‰ä¸€äº›æŒç»­çš„å­¦ä¹ ã€‚  httpsï¼š//github.com/github.com/github.com/probabilistic--interactive-mlaw---interactive-mlaw yourplastive yourplastive plapery plapery plapery  æˆ‘ä»¬è¿˜åœ¨æ’°å†™æœ‰å…³è¯¥ä¸»é¢˜çš„è°ƒæŸ¥ï¼Œä½†ä»å¤„äºæ—©æœŸé˜¶æ®µï¼šå¾ˆå¤šç‰µå¼•åŠ›ï¼Œæˆ‘å¸Œæœ›è¿™å¯ä»¥å¸®åŠ©äººä»¬åŠ å¿«é€Ÿåº¦ï¼šï¼‰  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/timo_kk     [link]        [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</guid>
      <pubDate>Wed, 26 Feb 2025 16:40:39 GMT</pubDate>
    </item>
    <item>
      <title>ä»¥éå¸¸æŠ˜æ‰£ä»·çš„å›°æƒ‘pro</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  ä»»ä½•æœ‰å…´è¶£ä»¥50ï¼…æŠ˜æ‰£ä»·è·å¾—å›°æƒ‘Proçš„äººï¼Œè¯·ä¸æˆ‘è”ç³»  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/u/beast_of_iit    href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1iykcbt/perplexity_pro_at_a_very_very_very_very_very_very_very_discounted_price/â€&gt; [link]    [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</guid>
      <pubDate>Wed, 26 Feb 2025 10:15:47 GMT</pubDate>
    </item>
    <item>
      <title>RLä»£ç†å½“å‰åœ¨ä¸æ¿€åŠ±ç‰¹å®šè¡Œä¸ºçš„æƒ…å†µä¸‹æœ€ä½³æ‰§è¡Œçš„æœ€å¤æ‚ç¯å¢ƒæ˜¯ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘å¾ˆæƒ³çŸ¥é“sotaåœ¨ç¯å¢ƒå¤æ‚æ€§æ–¹é¢ï¼Œåœ¨ä¸éœ€è¦ä»»ä½•ä¸­çº§å¥–åŠ±çš„æƒ…å†µä¸‹ï¼ŒRLä»£ç†æ‰§è¡Œçš„æ€§èƒ½ - åªæ˜¯+1 +1 forâ€œ winâ€å’Œ-1ä¸ºâ€œæŸå¤±â€   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/aliaslight     [link]     32;   [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 07:34:50 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆæŸäº›ç¯å¢ƒï¼ˆä¾‹å¦‚Minecraftï¼‰å¤ªå›°éš¾äº†ï¼Œè€Œå¦ä¸€äº›ç¯å¢ƒï¼ˆä¾‹å¦‚Openai's Hide N See Seekï¼‰æ˜¯å¯è¡Œçš„ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;   tldrï¼šæ˜¯ä»€ä¹ˆè®©hide nå¯»æ±‚å¯è§£å†³çš„ç¯å¢ƒï¼Œä½†æ˜¯æˆ‘å¾ˆéš¾è§£å†³çš„æˆ‘çš„minecraftæˆ–ç®€åŒ–çš„Minecraftç¯å¢ƒï¼Ÿ æˆ‘æ²¡æœ‰é‡åˆ°ä»»ä½•RLä»£ç†åœ¨Minecraftä¸­æˆåŠŸç”Ÿå­˜çš„ä»»ä½•RLä»£ç†ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå¦‚æœæ ¹æ®ä»£ç†å•†çš„æ´»åŠ›æ¥ç»™äºˆå¥–åŠ±ï¼Œå®ƒè‡³å°‘åº”è¯¥ä¸ºé£Ÿç‰©å»ºç«‹åº‡æŠ¤æ‰€å’Œå†œåœºã€‚ ï¼Œ ï¼ŒOpenaiçš„hide n of from 5å¹´å‰ä»5å¹´å‰å¼€å§‹å¯»æ±‚è§†é¢‘ï¼Œä»åˆ’ç—•ä¸­ï¼Œåœ¨é‚£ä¸ªç¯å¢ƒä¸­å­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼Œç”šè‡³æ²¡æœ‰æ¿€åŠ±ä»»ä½•è¡Œä¸ºã€‚ä¸ºä»€ä¹ˆä¸é€‚ç”¨äºMinecraftï¼Ÿæœ‰ä¸€ä¸ªæ›´å®¹æ˜“çš„ç¯å¢ƒç§°ä¸ºæ‰‹å·¥è‰ºè€…ï¼Œä½†å³ä½¿æ˜¯è¿™æ ·çš„å¥–åŠ±ä¼¼ä¹æ˜¯è¿™æ ·è®¾è®¡çš„ï¼Œä»¥è‡³äºæœ€ä½³è¡Œä¸ºè¢«æ¿€åŠ±ï¼Œè€Œä¸ä»…ä»…æ˜¯åŸºäºç”Ÿå­˜çš„å¥–åŠ±ï¼Œè€Œæœ€ä½³ç»©æ•ˆï¼ˆDreamerï¼‰ä»ç„¶æ²¡æœ‰ä¸äººç±»çš„ç»©æ•ˆç›¸æ¯”ã€‚ æ˜¯ä»€ä¹ˆè®©hide nå¯»æ±‚å¯è§£å†³çš„ç¯å¢ƒï¼Œä½†å¯ä»¥è§£å†³ï¼Œä½†å¯ä»¥è§£å†³ï¼Œä½†æ˜¯æ˜¯å¦‚æ­¤ï¼Œä½†æ˜¯å¦‚æ­¤éš¾ä»¥è§£å†³çš„æˆ–ç®€åŒ–çš„Minecraftç¯å¢ƒï¼Œä»¥æ±‚è§£Minecraft solveï¼Ÿæäº¤ç”±ï¼†ï¼ƒ32; /u/aliaslight     [link]   [æ³¨é‡Š] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</guid>
      <pubDate>Wed, 26 Feb 2025 05:29:58 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨æ·±RLçš„è‡ªæˆ‘æ ‡è®°æ±½è½¦</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iya5jf/selfparking_car_using_deep_rl/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘æƒ³è®­ç»ƒä¸€ä¸ªPPOå‹å·ä»¥å¹¶è¡Œåœè½¦ï¼Œå¯æˆåŠŸåœ°å°†è½¦åœåœ¨æ±½è½¦ä¸Šã€‚ä½ ä»¬çŸ¥é“æˆ‘å¯ä»¥ä¸ºæ­¤ç›®çš„ä½¿ç”¨çš„ä»»ä½•æ¨¡æ‹Ÿç¯å¢ƒå—ï¼Ÿå¦å¤–ï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹ä¼šå¾ˆé•¿å—ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1iya5jf/selfparking_car_used_usis_deep_rl/â€‹â€‹â€&gt; [link]   [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iya5jf/selfparking_car_using_deep_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 00:12:17 GMT</pubDate>
    </item>
    <item>
      <title>äº‹åç»éªŒé‡æ’­ï¼ˆå¥¹ï¼‰è¡¨ç°çš„ä¸»è¦è´¡çŒ®è€…æ˜¯ä»€ä¹ˆ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</link>
      <description><![CDATA[Hello, I have been studying Hindsight Experience Replay (HER) recently, and Iâ€™ve been examining the mechanism by which HER significantly improves performance in sparse reward environments. In my view, HER enhances performance in two aspects:  Enhanced Exploration:  In sparse rewardç¯å¢ƒï¼Œå¦‚æœä»£ç†æœªèƒ½è¾¾åˆ°æœ€åˆçš„ç›®æ ‡ï¼Œå®ƒå‡ ä¹æ— æ³•è·å¾—ä»»ä½•å¥–åŠ±ï¼Œå¯¼è‡´ç¼ºä¹å­¦ä¹ ä¿¡å·å¹¶è¿«ä½¿ä»£ç†ç»§ç»­éšæœºè¿›è¡Œéšæœºæ¢ç´¢ã€‚ å¥¹é€šè¿‡ä½¿ç”¨æœ€ç»ˆçŠ¶æ€ä½œä¸ºç›®æ ‡æ¥é‡æ–°å®šä¹‰ç›®æ ‡ï¼Œä½¿ä»£ç†äººå¯ä»¥é€šè¿‡å®é™…ä¸Šå¯ä»¥é€šè¿‡è¯¥è¿‡ç¨‹æ¥å®ç°å„ä¸ªå›½å®¶çš„å¯ç”¨ã€‚     ç­–ç•¥æ¦‚æ‹¬ï¼š  å¥¹å°†ç›®æ ‡ä¸å›½å®¶ä¸€èµ·èå…¥äº†ç½‘ç»œçš„è¾“å…¥ä¸­ï¼Œä½¿æ”¿ç­–èƒ½å¤Ÿæœ‰æ¡ä»¶åœ°å­¦ä¹ ï¼Œä»¥å®ç°å·å’Œè§„å®šçš„ç›®æ ‡ã€‚  å› æ­¤ï¼Œé€šè¿‡æ•è·å„ç§ç›®æ ‡ä¹‹é—´çš„å…³ç³»å¹¶æ²¡æœ‰ç›´æ¥å®ç°çš„ç›®æ ‡ï¼Œä»æŸç§ç¨‹åº¦ä¸Šæ¥å®ç°ã€‚       åœ¨è¿™äº›è¦ç‚¹ä¸Šï¼Œæˆ‘å¾ˆå¥½å¥‡ï¼Œæˆ‘åœ¨å“ªäº›å› ç´ ä¸Š - æ¢ç´¢æˆ–æ”¿ç­–æ™®éæ€§åœ°å¦‚æœçŠ¶æ€ç©ºé—´ä¸ºr  2 &lt; /sup&gt;ï¼Œç›®æ ‡æ˜¯ï¼ˆ2,2ï¼‰ï¼Œä½†æ˜¯ä»£ç†äººæ°å¥½ä»…åœ¨ç¬¬äºŒä¸ªè±¡é™å†…æ¢ç´¢ï¼Œåˆ™æœ€ç»ˆçŠ¶æ€å°†è¢«é™åˆ¶åœ¨è¯¥åœ°åŒºã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥æ”¿ç­–å¯èƒ½å¾ˆéš¾å°†å…¶æ¨å¹¿åˆ°æ¢ç´¢åŒºåŸŸä¹‹å¤–çš„ï¼ˆ2,2ï¼‰ä¹‹ç±»çš„ç›®æ ‡ã€‚è¿™æ ·çš„é™åˆ¶ä¼šå¦‚ä½•å½±å“å¥¹çš„è¡¨ç°ï¼Ÿæäº¤ç”±ï¼†ï¼ƒ32; /u/drlc_     [link]    [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</guid>
      <pubDate>Tue, 25 Feb 2025 18:20:38 GMT</pubDate>
    </item>
    <item>
      <title>Qå­¦ä¹ ï¼ŒæŠ˜æ‰£ç³»æ•°ä¸º0ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å—¨ï¼Œæˆ‘æ­£åœ¨ç ”ç©¶ä¸€ä¸ªé¡¹ç›®ï¼Œä»¥å®ç°Q-å­¦ä¹ çš„ä»£ç†ã€‚æˆ‘åªæ˜¯æ„è¯†åˆ°å¯¹ç¯å¢ƒï¼ŒçŠ¶æ€å’ŒåŠ¨ä½œè¿›è¡Œäº†é…ç½®ï¼Œå› æ­¤å½“å‰çš„è¡ŒåŠ¨ä¸ä¼šå½±å“æœªæ¥çš„çŠ¶æ€æˆ–å¥–åŠ±ã€‚æˆ‘è®¤ä¸ºåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæŠ˜ç°å› å­åº”è¯¥ç­‰äºé›¶ï¼Œä½†æ˜¯æˆ‘ä¸çŸ¥é“Qå­¦ä¹ ä»£ç†æ˜¯å¦æœ‰æ„ä¹‰è§£å†³æ­¤ç±»é—®é¢˜ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œå®ƒæ¯”MDPæ›´åƒæ˜¯ä¸Šä¸‹æ–‡çš„å¼ºç›—é—®é¢˜ã€‚ Qå­¦ä¹ ç®—æ³•çš„åç§°ä¸º0ï¼Œæˆ–ç­‰æ•ˆç®—æ³•ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevericeslearning/comments/1ixzkgs/qlearning_with_a_a_discount_factor_of_0/â€&gt; [link]        [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</guid>
      <pubDate>Tue, 25 Feb 2025 16:50:04 GMT</pubDate>
    </item>
    <item>
      <title>ç²¾ç¡®çš„ä»¿çœŸæ¨¡å‹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixtjk0/precise_simulationmodel/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å˜¿ï¼Œ æˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨Bipedalæœºå™¨äººä»äº‹å¤§å­¦é¡¹ç›®ã€‚æˆ‘æƒ³å®ç°ä¸€ä¸ªåŸºäºRLçš„æ§åˆ¶å™¨è¿›è¡Œè¡Œèµ°ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œæœ‰å¿…è¦æ‹¥æœ‰ä¸€ä¸ªç²¾ç¡®çš„å­¦ä¹ æ¨¡å‹ï¼Œä»¥ä¾¿æˆåŠŸåœ°è·³å…¥SIM2REALå·®è·ã€‚æˆ‘ä»¬åœ¨NXä¸­æœ‰ä¸€ä¸ªCADå‹å·ï¼Œæˆ‘å¬è¯´æœ‰ä¸€ä¸ªé€‰æ‹©å°†CADè½¬æ¢ä¸ºIsaac Simä¸­çš„UDFã€‚ ï¼Œä½†æ˜¯å“ªç§å·¥ä¸šâ€œé»„é‡‘æ ‡å‡†â€æ–¹æ³•æ˜¯ä¸ºæ¨¡æ‹Ÿçš„è‰¯å¥½æ¨¡å‹å—ï¼Ÿ   &lt;ï¼ -  sc_on- sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/theoneandonly_ncb      [link]   ï¼†ï¼ƒ32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixtjk0/precise_simulationmodel/</guid>
      <pubDate>Tue, 25 Feb 2025 12:12:12 GMT</pubDate>
    </item>
    <item>
      <title>ç°åœ¨ï¼Œå¢å¼ºå‹å¥—å›¾æ”¯æŒPPOï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</link>
      <description><![CDATA[       &lt;ï¼ -  sc_off-&gt;  å˜¿ï¼Œå¤§å®¶ï¼Œ  renforceui-studioç°åœ¨åŒ…æ‹¬æ¥è¿‘ç­–ç•¥ä¼˜åŒ–ï¼ˆppoï¼‰ï¼ğŸš€ href=&quot;https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/&quot;&gt;here), I introduced ReinforceUI-Studio as a tool to make training RL models easier. I received many requests for PPO, and it&#39;s finally here!å¦‚æœæ‚¨æœ‰å…´è¶£ï¼Œè¯·æ£€æŸ¥ä¸€ä¸‹ï¼Œè®©æˆ‘çŸ¥é“æ‚¨çš„æƒ³æ³•ã€‚å¦å¤–ï¼Œä¿æŒç®—æ³•è¯·æ±‚çš„åˆ°æ¥ - æ‚¨çš„åé¦ˆæœ‰åŠ©äºä½¿å·¥å…·å˜å¾—æ›´å¥½ï¼  æ–‡æ¡£ï¼š https://docs.reinforceui-studio.com/algorithms/algorithm_list/algorithm_list  href =â€œ https://github.com/dvalenciar/reinforceui-studioâ€&gt; https://github.com/dvalenciar/reinforceui-studio       &lt;ï¼æäº¤ç”±ï¼†ï¼ƒ32; /u/u/dvr_dvr     [link]        [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</guid>
      <pubDate>Tue, 25 Feb 2025 08:18:11 GMT</pubDate>
    </item>
    <item>
      <title>DDPGé—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</guid>
      <pubDate>Tue, 25 Feb 2025 00:02:36 GMT</pubDate>
    </item>
    <item>
      <title>ä¸RLä¸€èµ·ä½¿ç”¨çš„æœ€ä½³æœºå™¨äººæ¨¡æ‹Ÿå™¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å—¨ï¼Œæˆ‘æ­£åœ¨å°è¯•æ¨¡æ‹Ÿæˆ‘çš„æœºå™¨äººå¿…é¡»ä¸è¿æ¥åˆ°æœ«ç«¯æ•ˆåº”å™¨ä¸Šçš„ä¼ æ„Ÿå™¨è®¾å¤‡è¿›è¡Œäº¤äº’çš„ç¯å¢ƒï¼Œå¹¶ä½¿ç”¨RLè¿›è¡Œè¯»æ•°ã€‚æˆ‘å¸Œæœ›ç„¶ååœ¨å®é™…çš„ç¡¬ä»¶ä¸Šä½¿ç”¨è¿™ä¸ªè®­ç»ƒæœ‰ç´ çš„ä»£ç†ã€‚æ‚¨ä¼šæ¨èä»€ä¹ˆæ¨¡æ‹Ÿå™¨ï¼Ÿæˆ‘çœ‹è¿‡Pybulletå’ŒGuazeboã€‚ä½†æ˜¯æˆ‘ä¸ç¡®å®šå“ªä¸ªä¼¼ä¹æ˜¯æœ€ç®€å•ï¼Œæœ€ä½³çš„æ–¹æ³•ï¼Œå› ä¸ºæˆ‘åœ¨æ¨¡æ‹Ÿæ–¹é¢å‡ ä¹æ²¡æœ‰ç»éªŒã€‚   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/bananaoramama   href =â€œ https://www.reddit.com/r/reinforevericeslearning/comments/1ix8eux/best_robotic_simulator_to_to_use_with_with_rl/â€&gt; [link]       [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</guid>
      <pubDate>Mon, 24 Feb 2025 18:01:00 GMT</pubDate>
    </item>
    <item>
      <title>å¥–åŠ±æˆå‹æƒ³æ³•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘å¯¹å¥–åŠ±æˆå‹å½¢å¼æœ‰ä¸€ä¸ªæƒ³æ³•ï¼Œæƒ³çŸ¥é“å¤§å®¶éƒ½åœ¨è€ƒè™‘è¿™ä¸€ç‚¹ã€‚ æƒ³è±¡æ‚¨æ‹¥æœ‰è¶…çº§ç¨€ç–çš„å¥–åŠ±åŠŸèƒ½ï¼Œä¾‹å¦‚+1èµ¢å¾—èƒœåˆ©å’Œ-1çš„æŸå¤±ï¼Œæƒ…èŠ‚å¾ˆé•¿ã€‚è¿™ä¸ªå¥–åŠ±åŠŸèƒ½æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚  å½“ç„¶ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ç¨€ç–çš„å¥–åŠ±åŠŸèƒ½å¾ˆéš¾å­¦ä¹ ã€‚å› æ­¤ï¼Œå¼•å…¥å¯†é›†çš„å¥–åŠ±åŠŸèƒ½ä¼¼ä¹å¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªå‡½æ•°ï¼Œè¡¨æ˜æˆ‘ä»¬çš„ä»£ç†å•†æ­£æœæ­£ç¡®æˆ–é”™è¯¯çš„æ–¹å‘è¡Œé©¶ã€‚ It is often really tricky to define such a reward function that exactly matches our true reward function, so I think it only makes sense to temporarily use this reward function to initially get our agent in roughly the right area in policy space. As a disclaimer, I must say that I&#39;ve not read any research on reward shaping, so forgive me if my ideas are silly. One thing I&#39;ve done in the past with a DQN-like algorithm isåœ¨åŸ¹è®­è¿‡ç¨‹ä¸­ï¼Œé€æ¸ä»ä¸€ä¸ªå¥–åŠ±åŠŸèƒ½è½¬ç§»åˆ°å¦ä¸€ä¸ªå¥–åŠ±åŠŸèƒ½ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘ä½¿ç”¨äº†100ï¼…çš„è‡´å¯†å¥–åŠ±åŠŸèƒ½å’Œç¨€ç–çš„0ï¼…ã€‚ä¸€æ®µæ—¶é—´åï¼Œæˆ‘å¼€å§‹é€æ¸â€œé€€ç«â€ã€‚è¿™ä¸ªæ¯”ç‡ç›´åˆ°æˆ‘åªä½¿ç”¨çœŸæ­£çš„ç¨€ç–å¥–åŠ±åŠŸèƒ½ã€‚æˆ‘çœ‹å¾—å¾ˆå¥½ã€‚ æˆ‘è¿™æ ·åšçš„åŸå› æ˜¯â€œé€€ç«â€ã€‚æ˜¯å› ä¸ºæˆ‘è®¤ä¸ºQå­¦ä¹ ç®—æ³•å¾ˆéš¾é€‚åº”å®Œå…¨ä¸åŒçš„å¥–åŠ±åŠŸèƒ½ã€‚ä½†æ˜¯æˆ‘ç¡®å®æƒ³çŸ¥é“é€€ç«ç‡æµªè´¹äº†å¤šå°‘æ—¶é—´ã€‚æˆ‘ä¹Ÿä¸å–œæ¬¢é€€ç«ç‡æ˜¯å¦ä¸€ä¸ªè¶…å‚æ•°ã€‚ æˆ‘çš„æƒ³æ³•æ˜¯å°†å¥–åŠ±å‡½æ•°çš„ç¡¬è½¬æ¢åº”ç”¨äºæ¼”å‘˜æ‰¹è¯„ç®—æ³•ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è®­ç»ƒåœ¨å¯†é›†çš„å¥–åŠ±åŠŸèƒ½ä¸Šã€‚æˆ‘ä»¬å‡è®¾æˆ‘ä»¬å¾—å‡ºäº†ä¸€é¡¹ä½“é¢çš„æ”¿ç­–ï¼Œä¹Ÿæ˜¯è¯„è®ºå®¶çš„ä½“é¢ä»·å€¼ä¼°è®¡ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å†»ç»“æ¼”å‘˜ï¼Œç¡¬å‡»å¥–åŠ±åŠŸèƒ½ï¼Œå¹¶é‡æ–°å®¡æŸ¥è¯„è®ºå®¶ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥æ¶ˆé™¤é«˜å‚æ•°ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬å¯ä»¥è®­ç»ƒï¼Œç›´åˆ°è¯„è®ºå®¶çš„é”™è¯¯è¾¾åˆ°ä¸€å®šçš„é—¨æ§›ä¸ºæ­¢ã€‚æˆ‘æƒ³è¿™æ˜¯ä¸€ä¸ªæ–°çš„è¶…å‚æ•°ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬ä¼šè§£å¼€æ¼”å‘˜å¹¶æ¢å¤æ­£å¸¸çš„åŸ¹è®­ã€‚ æˆ‘è®¤ä¸ºè¿™åœ¨å®è·µä¸­åº”è¯¥å¾ˆå¥½ã€‚æˆ‘è¿˜æ²¡æœ‰æœºä¼šå°è¯•ã€‚ä½ ä»¬éƒ½å¯¹è¿™ä¸ªæƒ³æ³•æœ‰ä½•çœ‹æ³•ï¼Ÿæœ‰ä»€ä¹ˆç†ç”±æœŸæœ›å®ƒè¡Œä¸é€šå—ï¼Ÿæˆ‘ä¸æ˜¯æ¼”å‘˜ - æ‰¹è¯„ç®—æ³•çš„ä¸“å®¶ï¼Œæ‰€ä»¥è¿™ä¸ªæƒ³æ³•ç”šè‡³æ²¡æœ‰æ„ä¹‰ã€‚ è®©æˆ‘çŸ¥é“ï¼è°¢è°¢ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/sandsnip3r     [link]   ï¼†ï¼ƒ32;   [comment]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</guid>
      <pubDate>Mon, 24 Feb 2025 15:12:25 GMT</pubDate>
    </item>
    <item>
      <title>Simbav2ï¼šå¯æ‰©å±•æ·±åº¦å¢å¼ºå­¦ä¹ çš„è¶…é€æ˜æ ‡å‡†åŒ–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</link>
      <description><![CDATA[    ä»‹ç» simbav2ï¼   ğŸ“„é¡¹ç›®é¡µé¢ï¼š https://arxiv.org/abs/2502.15280   ğŸ”—ä»£ç ï¼š https://github.com/dojeon-ai/simbav2      simbav2æ˜¯ä¸€ç§ç®€å•ï¼Œå¯æ‰©å±•çš„RLä½“ç³»ç»“æ„ï¼Œå¯ç¨³å®š simberal formant br br br br br br br brs ã€‚æ¼”å‘˜è¯„è®ºå®¶åœ¨57ä¸ªè¿ç»­çš„æ§åˆ¶ä»»åŠ¡ï¼ˆMujocoï¼Œdmcontrolï¼ŒMyosuiteï¼Œhumyoid-Benchï¼‰ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼ˆSOTAï¼‰ã€‚  å®ƒä¸ä½“è‚²é¦†1.0.0 api   - å°è¯•ä¸€ä¸‹ï¼ ï¼Œå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶ä¸ä¹‹ä¼¸å‡ºæ´æ‰‹ï¼šï¼‰  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/joonleesky     [link]     [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</guid>
      <pubDate>Mon, 24 Feb 2025 11:43:23 GMT</pubDate>
    </item>
    </channel>
</rss>