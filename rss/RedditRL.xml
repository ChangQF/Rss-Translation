<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 15 Jan 2025 21:14:46 GMT</lastBuildDate>
    <item>
      <title>大富翁强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i26zft/monopoly_reinforcement_learning_project/</link>
      <description><![CDATA[大家好，我是大学数学专业的毕业生，正在申请计量经济学和精算学统计学硕士学位。我对人工智能很感兴趣，目前我愿意在人工智能和强化学习方面做我的第一个项目，做一个人工智能模型来模拟大富翁游戏，并提供策略和交易来赢得游戏...我知道在哪里以及如何获取数据和其他东西。我想问你们，我目前需要做什么才能完成这个项目，因为我是数学专业的学生，​​对这个领域没有太多的想法，所以我希望得到一些帮助和建议！谢谢     提交人    /u/good-mathematician-   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i26zft/monopoly_reinforcement_learning_project/</guid>
      <pubDate>Wed, 15 Jan 2025 20:30:47 GMT</pubDate>
    </item>
    <item>
      <title>我为 TensorFlow 和 Keras 编写了优化器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i21sgm/i_wrote_optimizers_for_tensorflow_and_keras/</link>
      <description><![CDATA[大家好，我为 TensorFlow 和 Keras 编写了优化器，它们的使用方式与 Keras 优化器相同。 https://github.com/NoteDance/optimizers    提交人    /u/NoteDancing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i21sgm/i_wrote_optimizers_for_tensorflow_and_keras/</guid>
      <pubDate>Wed, 15 Jan 2025 16:51:04 GMT</pubDate>
    </item>
    <item>
      <title>关于 DQN 及其变体的收敛性的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i20m49/question_on_convergence_of_dqn_and_its_variants/</link>
      <description><![CDATA[大家好， 我是一名接受过 DSP 正式培训的电子工程专业学生，多年来一直在航空航天行业工作。几年前，我开始将我的视野扩展到深度学习 (DL) 和机器学习，但经验有限。几周前，我开始研究强化学习，特别是 DQN 及其变体。而且，我很惊讶地发现，即使对于像 CartPole-v1 这样的简单环境，DQN 及其变体也不能保证收敛。换句话说，当查看 Total Reward 与 Episode 的图时，它真的很丑。我这里遗漏了什么吗？    提交人    /u/Tasty_Road_3519   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i20m49/question_on_convergence_of_dqn_and_its_variants/</guid>
      <pubDate>Wed, 15 Jan 2025 16:00:26 GMT</pubDate>
    </item>
    <item>
      <title>对转学 RL 论文有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i1ytan/recommendations_for_transfer_rl_papers/</link>
      <description><![CDATA[我将在迁移 RL 中做一个项目，并希望阅读一些关于该主题的最新论文。具体来说，我将尝试训练 DQN 来玩一款游戏，然后使用迁移学习将技能迁移到其他游戏中。我找到了一些调查，但如果有人对这个主题的好论文有建议，我会非常感激。    提交人    /u/TheGreatBritishNinja   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i1ytan/recommendations_for_transfer_rl_papers/</guid>
      <pubDate>Wed, 15 Jan 2025 14:40:18 GMT</pubDate>
    </item>
    <item>
      <title>尽管有合理的剧集奖励，但 PPO 并没有学到任何东西</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i1xj8z/ppo_doesnt_learn_anything_despite_reasonable/</link>
      <description><![CDATA[      大家好，我正在使用 Stable Baselines 3 为具有多维离散观察空间的自定义环境训练 PPO 代理，但代理在评估过程中基本上一直在重复相同的无意义动作，尽管进行了多次迭代超参数变化和不同的奖励函数。我是否对我的环境或模型做了一些明显的错误？PPO 是否不适合多维问题？请检查一下我，我要疯了.. 一些细节：环境模拟三维空间中的 50 个体素，它们处于预定义的初始位置，并且必须通过一系列命令到达目标位置（例如：体素 1 向下移动）。出于测试目的，初始位置和目标位置在所有运行中都是恒定的，但模型理想情况下应该学会使用随机配置来执行此操作。 观察空间由 50 个列表组成，包含 6 个元素：每个立方体的当前 x、y、z 和目标 x、y、z 坐标。动作空间由两个数字组成，一个选择立方体，另一个选择移动它的方向。立方体的移动方式有一些限制，因此通常大约 40-60％ 的动作空间会导致非法移动。 self.observation_space = space.Box( low=-100, high=100, shape=(50, 6), dtype=np.float32 ) self.action_space = space.MultiDiscrete([ 50, # Voxel ID 6 # Move ID (0 to 5) ])  奖励函数非常简单，我只是想让模型不要选择无效的移动，并且可能将一些立方体移动到正确的方向。我曾尝试根据距离目标的接近程度来奖励它，删除无效移动惩罚，改变惩罚和奖励之间的比例，但这并没有带来明显的改善。 def calculate_reward(self, action): reward = 0 # 惩罚无效移动 if not is_legal(action): reward -= 1 return reward # 如果位置不正确的体素移动到正确位置，则奖励模型 if removed_voxel in self.target_positions and removed_voxel not in self.reached_target_positions: reward += 1 # 如果正确定位的体素移动到不正确的位置，则减少奖励 elif removed_voxel not in target_positions and removed_voxel in self.reached_target_positions: reward -= 1 # 对做出任何举动以阻止长解决方案的惩罚 reward -= 0.1 return reward  关于超参数，我曾尝试过上下移动学习率、熵系数、步数和批量大小，到目前为止都无济于事。 from stable_baselines3 import PPO model = PPO( &quot;MlpPolicy&quot;, env, device=&#39;cpu&#39;, policy_kwargs=dict( net_arch=[256, 512, 256] ) n_steps=2000, batch_size=100, gae_lambda=0.95, gamma=0.99, n_epochs=10, clip_range=0.2, ent_coef=0.02, vf_coef=0.5, learning_rate=5e-5, max_grad_norm=0.5 ) model.learn(total_timesteps=10_000_000) obs = env.reset() # 评估模型 for i in range(100): action, _state = model.predict(obs, deterministic=True) obs, reward, done, info = env.step(action) print(f&quot;Action: {action}, Reward: {reward}, Done: {done}&quot;) if done.any(): obs = env.reset()  我并行运行多个环境，奖励和观察空间得到规范化。就这样。 env = VoxelEnvironment() env = SubprocVecEnv([make_env() for _ in range(8)]) env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_reward=1, clip_obs=100)  Tensorboard 统计数据表明，该模型有时能够获得积极的奖励，但当我评估它时，它继续做同样久经考验的“永远重复相同的动作”策略。 https://preview.redd.it/nkrthwj5v6de1.png?width=1407&amp;format=png&amp;auto=webp&amp;s=6663c87ffdfd8678234af24add52e16b29cf2708 提前致谢！    提交人    /u/Kriegnitz   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i1xj8z/ppo_doesnt_learn_anything_despite_reasonable/</guid>
      <pubDate>Wed, 15 Jan 2025 13:36:55 GMT</pubDate>
    </item>
    <item>
      <title>RL 代理：保真度似乎在上升，但回报却没有。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i1vq0x/rl_agent_fidelity_seems_to_be_going_up_but_reward/</link>
      <description><![CDATA[      大家好！我对 RL 还很陌生，决定用它尝试一些项目。  我注意到我的奖励一直在下降。但我的保真度一直在上升。我对这意味着什么感到非常困惑，因为它的原始性能本质上越来越好，但它的奖励却越来越差。以下是一些超参数： lr：3e-4，余弦退火至 5e-5 情节：10,000，每集大约 27 步 PER 缓冲区大小：100,000 https://preview.redd.it/d0gg2gdtb5de1.png?width=1102&amp;format=png&amp;auto=webp&amp;s=777707a003ee1dde12cf9078365402a352797544 提前感谢大家！     提交人    /u/TopSigmaNoCap79970   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i1vq0x/rl_agent_fidelity_seems_to_be_going_up_but_reward/</guid>
      <pubDate>Wed, 15 Jan 2025 11:50:58 GMT</pubDate>
    </item>
    <item>
      <title>好像 ppo 还没有经过训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i1ua6x/it_seems_like_ppo_is_not_trained/</link>
      <description><![CDATA[状态数7200，动作10，状态范围-5~5，奖励-1~1。 episodes超过100，steps数20-30。 在评估阶段，加载并测试模型，无论状态如何都会选择动作。 无论状态如何，都会按照一定的模式选择动作。 无论我怎么搜索，都找不到原因。请帮帮我.. https://pastebin.com/dD7a14eC 代码在这里    submitted by    /u/Dry-Jicama-6874   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i1ua6x/it_seems_like_ppo_is_not_trained/</guid>
      <pubDate>Wed, 15 Jan 2025 10:07:07 GMT</pubDate>
    </item>
    <item>
      <title>奖励规范化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i1r4dd/reward_normalization/</link>
      <description><![CDATA[我的情景环境具有非常延迟和稀疏的奖励（最后只有 1 或 0）。我可以在 DQN 算法中使用奖励规范化吗？    提交人    /u/No-Eggplant154   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i1r4dd/reward_normalization/</guid>
      <pubDate>Wed, 15 Jan 2025 06:06:15 GMT</pubDate>
    </item>
    <item>
      <title>PPO 在多维离散环境中没有学到任何东西</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i1m8p6/ppo_not_learning_anything_in_multidimensional/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i1m8p6/ppo_not_learning_anything_in_multidimensional/</guid>
      <pubDate>Wed, 15 Jan 2025 01:30:40 GMT</pubDate>
    </item>
    <item>
      <title>一款小型浏览器游戏，对手由经过 RL 训练的计算机控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i17ckc/a_little_browser_game_with_an_rltrained/</link>
      <description><![CDATA[我最近在开发一款小游戏时，遇到了一个使用强化学习训练过的计算机控制对手，这给了我不少乐趣，您可以直接在浏览器中玩这个游戏：https://adamheins.com/projects/shadows/web/ 这是一款 2D 小型捉迷藏游戏，当您不是“它”时，通过收集宝藏可以获得积分（而当您是“它”时，对手收集宝藏会扣分）。环境中有障碍物，而且由于您在障碍物后面的视线被阻挡，游戏难度加大。 计算机控制的代理使用两种不同的 SAC 模型：一种用于“它”，一种用于非“它”。目前，游戏并不完全“公平”因为计算机可以访问玩家的当前位置（即，它不必担心视线被遮挡，或者换句话说，它不必处理部分可观测性）。另一种方法是直接从像素训练模型，我尝试过，但 (1) 正如您所料，模型更难学习，并且 (2) 在浏览器实现中更难/更慢地进行图像观察。我使用 Python 版本的游戏进行实际训练，然后将模型导出到 ONNX 以在浏览器中运行。代码在这里：https://github.com/adamheins/shadows 享受！    提交人    /u/adamheins   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i17ckc/a_little_browser_game_with_an_rltrained/</guid>
      <pubDate>Tue, 14 Jan 2025 14:33:02 GMT</pubDate>
    </item>
    <item>
      <title>对 RLC 的看法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i15qi9/views_on_rlc/</link>
      <description><![CDATA[大家好，我是三年级的博士生，正在研究 Bandits 和 MDP。我想知道是否有人可以提供关于强化学习会议 (RLC) 的评论，作为提交的潜在场所。 我确实看到它的咨询委员会很好，但考虑到这是一个新会议，我想知道是否值得在那里提交    提交人    /u/Fantastic-Nerve-4056   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i15qi9/views_on_rlc/</guid>
      <pubDate>Tue, 14 Jan 2025 13:12:04 GMT</pubDate>
    </item>
    <item>
      <title>无真实数据的分割</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i14rdb/segmentation_without_groundtruth/</link>
      <description><![CDATA[大家好， 我对使用时间和奖励信息进行没有基本事实的分割很感兴趣。以下场景特别有趣：  前景检测：（示例）给定一段足球比赛的视频 - 分割球员和球 元素检测：（示例）给定游戏 Pong 的轨迹（特别是帧+奖励）- 分割球员和球  我想要的是能够区分视频/轨迹中的“重要”元素，而不依赖于给定分布的先验知识。依赖时间信息是可以的。即。在天空中一架飞机的视频中，通过飞机的运动来检测飞机是有意义的。 有没有关于这种情况的研究？ 我认为正在使用基础的segment-anything模型。    提交人    /u/Potential_Hippo1724   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i14rdb/segmentation_without_groundtruth/</guid>
      <pubDate>Tue, 14 Jan 2025 12:16:27 GMT</pubDate>
    </item>
    <item>
      <title>离策略确定性策略梯度的推导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i138r5/derivation_of_offpolicy_deterministic_policy/</link>
      <description><![CDATA[      嗨！这是我在这个主题上的第一个问题，所以如果缺少任何有助于您回答问题的内容，请告诉我。 我一直在研究确定性策略梯度论文（Silver 等人，2014 年），并试图理解方程 15。据我目前的理解，方程 14 指出，我们可以使用从行为策略获得的状态分布来修改性能目标，因为我们正在尝试推导离策略确定性策略梯度。并且看起来根据定理 1 的推导过程，对策略参数求 14 的微分将直接导致（离策略）性能目标的梯度。 所以我不明白为什么会有方程 15。作者提到他们已经删除了一个依赖于 Q 函数相对于的梯度的项。策略参数，但我不明白为什么应该删除它，因为当我们区分方程 14 时，该术语根本不存在。此外，我还对方程 15 的第二行很好奇，其中策略分布 $\mu_{\theta}(a|s)$ 变成了 $\mu_{\theta}$。 如果有人能回答我的问题，我将不胜感激。 编辑）我能够（粗略地）推导出方程 15 并附上推导。如果有任何错误或想要讨论的地方，请告诉我:) https://preview.redd.it/i3weoi3264de1.jpg?width=1400&amp;format=pjpg&amp;auto=webp&amp;s=15845eb04c4ac4d0d61d64f430e5f774e6854390    提交人    /u/Fragrant-Leading8167   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i138r5/derivation_of_offpolicy_deterministic_policy/</guid>
      <pubDate>Tue, 14 Jan 2025 10:33:38 GMT</pubDate>
    </item>
    <item>
      <title>如果有人有兴趣参与我的棋盘游戏 Splendor 的 DDQN 最后部分，请告诉我。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i0v68g/if_anybody_is_interested_in_collaborating_for_the/</link>
      <description><![CDATA[除了最后一部分，也就是最有趣的部分，我不需要任何帮助。调整模型并使其工作。我在 TensorBoard 中记录了大量内容，以实现良好的可视化效果，您可以添加任何内容 - 我不期望任何编码帮助，因为它是一个相当大的代码库。但如果您想要，您完全可以。只是希望有人坐下来讨论如何让模型发挥性能。 https://github.com/BreckEmert/Splendor-AI 我正在研究的最大问题，我将从刚发给某人的消息中重新粘贴： 我很好奇你对我如何为棋盘游戏 Splendor. DQN 设置动作空间的初步想法。除了“获取宝石”之外，整个动作和状态空间都很容易处理。轮到您时，您可以从 5 种宝石类型池中获取 3 种不同的宝石，或者从同一种宝石中获取 2 种。因此，您会认为动作空间是 5 选择 3 + 5（选择 1）。但问题是您的库存中最多有 10 颗宝石，因此您还必须丢弃至 10 颗。因此，如果您有 10 颗宝石，则必须选择 3 颗并丢弃 3 颗，或者如果没有完整的 3 颗可用，则您只能选择 2 颗，等等。最后，我们至少要考虑（15 种获取宝石的方法）*（1800 种丢弃宝石的方法）。不知道这是否很混乱。 我决定在代理选择任何“获取宝石”动作时将其锁定在购买序列中。无论它选择 10 个选项中的哪个，它都会被迫连续进行最多 6 次移动（通过在 argmax 期间将其他选项设置为 -inf）。它最多可以得到三颗宝石，从 5 个动作空间中挑选。然后它会丢弃它需要丢弃的宝石，从另外 5 个动作空间中挑选。现在我的所有动作空间总共只有 15 个。我不知道这看起来是很棒还是很愚蠢，哈哈，但无论如何我的模型性能都很糟糕；它根本没有学习。    提交人    /u/Breck_Emert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i0v68g/if_anybody_is_interested_in_collaborating_for_the/</guid>
      <pubDate>Tue, 14 Jan 2025 02:00:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 6 自由度机器人进行拾取和投掷强化学习 - 寻求有关真实世界设置的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i0jksn/reinforcement_learning_with_pick_and_throw_using/</link>
      <description><![CDATA[大家好，我目前正在开展一个关于使用 6-DOF 机器人进行拾取和投掷强化学习 (RL)的项目。我发现了两篇与此主题相关的有趣论文，链接如下：  论文 1 – arxiv: 2405.19001 论文 2 – arxiv: 2406.13453  但是，我在在现实世界中设置系统时遇到了困难，我希望得到有关几个具体问题的建议：  验证投掷的准确性：我无法弄清楚这些论文如何处理对投掷是否落在正确位置的验证。在现实世界的设置中，我如何确认物体已被准确投掷？使用 RGB-D 相机 来估计箱子的位置，并使用另一台相机来验证物体是否被成功抛出，这是一种好方法吗？ 训练期间的域随机化：在论文中，域随机化用于在训练期间改变箱子的位置。当转移到现实世界时，我是否应该通过将箱子的位置直接包含在动作空间中并不断更新来简化事情，还是有更好的方法来处理这个问题？ 拾取和投掷的单独模型：我正在考虑两种不同的方法： 方法 1：将拾取和投掷任务组合成一个 RL 模型。 方法 2：将两个任务分成不同的模型 - 对拾取步骤使用固定坐标（因此机器人将夹持器移动到预定义位置），并仅对投掷步骤应用 RL 以优化投掷动作。这种分离是否会使问题在实践中变得更容易、更可行？   如果有人在现实世界的机器人系统中使用过 RL 或曾经处理过类似的问题，我将非常感谢您的任何见解或建议。 非常感谢您的阅读！    提交人    /u/Sunnnnny24   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i0jksn/reinforcement_learning_with_pick_and_throw_using/</guid>
      <pubDate>Mon, 13 Jan 2025 17:39:05 GMT</pubDate>
    </item>
    </channel>
</rss>