<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 04 Dec 2024 06:26:38 GMT</lastBuildDate>
    <item>
      <title>基于 Symphony S2 的 DDPGII</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h69m82/ddpgii_based_on_symphony_s2/</link>
      <description><![CDATA[        提交人    /u/Timur_1988   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h69m82/ddpgii_based_on_symphony_s2/</guid>
      <pubDate>Wed, 04 Dec 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>[CartPole-v1] Vanilla PG 损失增加，但总奖励也增加。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h65o4z/cartpolev1_vanilla_pg_loss_increases_while_total/</link>
      <description><![CDATA[从这个简单的健身环境开始学习 RL。我观察到 PG 损失和奖励有相同的趋势。我难道不应该预期损失减少而奖励增加吗？ 我使用以下命令计算了一批 (状态、动作、奖励) 三元组的损失： loss = -torch.mean(logits * rewards)  奖励是使用带折扣的奖励公式计算的，类似于： out = torch.zeros(size=rewards.size(), dtype=torch.float32) out[-1] = rewards[-1] for i in range(rewards.size(-1)-1)[::-1]: out[i] = rewards[i] + r * out[i+1]     submitted by    /u/encoreway2020   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h65o4z/cartpolev1_vanilla_pg_loss_increases_while_total/</guid>
      <pubDate>Wed, 04 Dec 2024 02:41:33 GMT</pubDate>
    </item>
    <item>
      <title>除了样本复杂性之外，还有其他理由使用基于模型的 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</link>
      <description><![CDATA[当我们能够进行大规模环境并行化时，使用基于模型的算法真的有意义吗？ 基本上，我想知道是否存在像 DreamerV3 这样的算法可以解决而 PPO 无法解决的环境？例如，DreamerV3 论文表明，PPO 和 IMPALA 无法解决最困难的 Minecraft 任务，但如果有大量计算，PPO 最终会解决这些任务吗？ 除了降低样本复杂度之外，还有其他理由使用基于模型的算法吗？    提交人    /u/vandelay_inds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</guid>
      <pubDate>Wed, 04 Dec 2024 01:39:56 GMT</pubDate>
    </item>
    <item>
      <title>“BALROG：对游戏上的 Agentic LLM 和 VLM 推理进行基准测试”，Paglieri 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h63tu6/balrog_benchmarking_agentic_llm_and_vlm_reasoning/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h63tu6/balrog_benchmarking_agentic_llm_and_vlm_reasoning/</guid>
      <pubDate>Wed, 04 Dec 2024 01:11:04 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型的算法合谋”，Fish 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h63tjd/algorithmic_collusion_by_large_language_models/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h63tjd/algorithmic_collusion_by_large_language_models/</guid>
      <pubDate>Wed, 04 Dec 2024 01:10:40 GMT</pubDate>
    </item>
    <item>
      <title>如何处理复杂、嵌套的动作空间。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5zgvq/how_to_deal_with_complex_nested_action_spaces/</link>
      <description><![CDATA[我正在开发一个可以与 miniwob++ 体育馆环境配合使用的代理。我从未使用过像这样的非平面动作空间，并且想知道是否有人对制定它的最佳方法有任何指导，因为根据要采取的操作，需要不同的参数。具体来说，如果相关的话，尝试使用 Pytorch 来解决这个问题。    提交人    /u/m_js   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5zgvq/how_to_deal_with_complex_nested_action_spaces/</guid>
      <pubDate>Tue, 03 Dec 2024 21:59:50 GMT</pubDate>
    </item>
    <item>
      <title>狄利克雷掩蔽动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5wd0z/dirichlet_masked_action_space/</link>
      <description><![CDATA[嗨， 我对强化学习中的掩码动作有点陌生。我有一个强化学习问题，其中我的动作空间总和为 1（资源分配问题），但随着时间步长 t 的增加，我可用的资源越来越少，即前 t 个索引必须等于零。 我尝试使用狄利克雷分布并将环境中的第一个索引归零，但我一直得到次优结果。 我的奖励函数是两个项的凸组合 - 一个尝试优化可信度，另一个尝试优化盈利能力。对于两个特定场景，我知道什么是最优的，但在所有其他场景中我都不知道。 我正在使用 RLLIB，并且我尝试使用 PPO（基于注意力的模型）和 SAC。不知何故，SAC 比 PPO 效果更好，我不确定为什么，但仍然不是最优的。 问问想法我做错了什么或者我应该怎么做？ 提前谢谢！    提交人    /u/Brief-Host-6484   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5wd0z/dirichlet_masked_action_space/</guid>
      <pubDate>Tue, 03 Dec 2024 19:52:43 GMT</pubDate>
    </item>
    <item>
      <title>小型 LLM 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5l312/small_sized_llm_models/</link>
      <description><![CDATA[嗨，我知道这是 RL 子版块，但有人知道任何小型 LLM 预训练模型吗，比如 5-12 GB 大小的模型。 它应该能够回答非常基本的问题，仅此而已。如果是这样，请分享 提前致谢 :))    提交人    /u/Wide-Chef-7011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5l312/small_sized_llm_models/</guid>
      <pubDate>Tue, 03 Dec 2024 11:12:26 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到 Gym 环境中 RL 算法的可靠基准？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5a7iw/where_can_i_find_reliable_benchmarks_for_rl/</link>
      <description><![CDATA[首先我要说的是，我对 RL 还很陌生。我之前的工作是法学硕士 (LLM)，通常根据模型在给定基准上的表现对模型进行排名和堆叠（这些基准非常重要，初创公司仅凭他们的分数就能赚到很多钱）。 最近开始在 MuJoCo 环境中训练模型，我想弄清楚我的算法是否表现得还不错。当然，我可以使用 SB3 的默认 PPO 和 MlpPolicy 让 Ant-v5 行走，但它真的有多好？ 是否有一些基准或存储库，我可以将我的结果与使用默认 MuJoCo（或任何其他健身房）奖励函数的其他人算法的学习曲线进行比较？当然，假设我们使用相同的环境和奖励函数，但鉴于 Gymnasium 很受欢迎并提供良好的默认设置，我想应该有很多数据可用。 我在 google 上搜索过，只找到了稀疏的结果。为什么 RL 中的基准不如 LLM 中的基准那么大？    提交人    /u/geepytee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5a7iw/where_can_i_find_reliable_benchmarks_for_rl/</guid>
      <pubDate>Tue, 03 Dec 2024 00:15:28 GMT</pubDate>
    </item>
    <item>
      <title>现在人们对基因算法的关注度是否下降了？如果是这样，原因是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h59gl2/is_there_less_attention_towards_genetic_algos_now/</link>
      <description><![CDATA[遗传算法 (GA) 已经存在很长时间了（大约在 20 世纪 60 年代）。它们似乎既非常直观，又特别适用于黑箱问题，但它们目前并不是“主流”。在 2017 年，OpenAI 对进化算法非常看好，并列举了它们的优点，包括可并行化、稳健，并且能够处理具有不明确的值/适应度函数的长期问题。有没有更新的最新信息？哪些算法正在击败 GA？ 对于低维问题，贝叶斯优化可能具有更好的统计保证/渐近性。GA 有任何保证吗，还是我们完全不知道？    提交人    /u/zarmesan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h59gl2/is_there_less_attention_towards_genetic_algos_now/</guid>
      <pubDate>Mon, 02 Dec 2024 23:41:34 GMT</pubDate>
    </item>
    <item>
      <title>RL 问题 - 具有无限块（不同颜色）的 2D 网格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h566gp/rl_problem_2d_grid_with_endless_pieces_of_varying/</link>
      <description><![CDATA[大家好， 我在工作中遇到了一个非常先进且困难的问题。我遇到了这样一个问题场景：我有这个 2D 离散网格系统。这个网格的大小可以变化。除此之外，我还有多个可以放入这些网格中的不同方框。每个方框的侧面可以有不同的纯色。当两个不同的方框相邻时，我需要找到最大化相同颜色相邻的最佳位置。 我一直在研究 AlphaZero 方法，但对于我的方法，我无法指定零件数量，并且每个方框可以有不同的颜色。关于如何解决这个问题有什么建议吗？    提交人    /u/Ill_Paper_6854   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h566gp/rl_problem_2d_grid_with_endless_pieces_of_varying/</guid>
      <pubDate>Mon, 02 Dec 2024 21:22:12 GMT</pubDate>
    </item>
    <item>
      <title>我从 DQN 获得的操作问题有时超出范围。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4m96d/problem_with_action_that_i_get_from_dqn_is/</link>
      <description><![CDATA[大家好， 我是强化学习的初学者。目前正在研究 DQN，并遇到了 DQN 预测动作的问题。我的问题如下：  Atari 游戏有 16 个一般动作空间，但每个游戏都有来自这个一般空间的自己的动作子集。 假设我正在探索 5 个不同的游戏，每个游戏都有不同数量的有效动作。 在进化过程中，一个游戏只有 3 个有效动作，但您的 DQN 返回的是这个有效空间之外的动作。  一种解决方案是用动作 0（NOOP）替换现在有效的动作，这意味着什么也不做。 还有其他方法可以有效地处理这种情况吗？ 提前谢谢您    提交人    /u/Grasmit_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4m96d/problem_with_action_that_i_get_from_dqn_is/</guid>
      <pubDate>Mon, 02 Dec 2024 04:00:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DreamerV3 的炒作程度不如 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</link>
      <description><![CDATA[据我所知，PPO 通常是强化学习任务的首选算法。为什么不改用 DreamerV3？它似乎更稳定，并且需要更少的超参数调整。    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</guid>
      <pubDate>Mon, 02 Dec 2024 01:22:39 GMT</pubDate>
    </item>
    <item>
      <title>寻找合作者 - 强化学习运筹学问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4fwym/looking_for_collaborators_rl_for_operations/</link>
      <description><![CDATA[大家好 :) 我目前正在攻读硕士学位，一直专注于强化学习。我现在已经在这个领域完成了几个项目，并且我目前还在与一些合作者合作撰写一篇关于强化学习在不确定随机环境中的研究的出版物，这些研究可以建模为图形。我在做本科论文时注意到了运筹学领域，我将一个随机组合问题构建为强化学习问题，并使用图形神经网络对其进行了解决。在那段时间以及从那时起，我阅读了一些最近的出版物，这些出版物试图用现代强化学习方法解决传统的 OR 问题（例如 https://arxiv.org/abs/2312.15658）。 我认为这总体上仍未得到充分探索，但同时也非常有趣。更现代的神经网络架构（例如 GNN）似乎非常适合与 RL 结合解决许多 OR 问题。因此，我也想专注于图形机器学习方法（例如 GNN），但我也对任何其他建模方法持开放态度。此外，还有一个 OR gym 存储库（https://github.com/hubbs5/or-gym），我想探索并用它来做一些新方法的实验。 因此，我正在寻找一些愿意加入我并共同努力用更现代的基于 RL 的方法解决其中一些问题的人。我还没有想过每周要花多少时间在这些项目上，因为从逻辑上讲，我还有很多事情要做（大学、工作、出版）。因此，如果有兴趣，我们可以联系并找到适合我们所有人的良好设置:)  我个人在设计、构建和流水线大规模神经网络方面拥有丰富的经验，并且非常乐意与来自不同背景的人合作。 喜欢收到您的来信！    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4fwym/looking_for_collaborators_rl_for_operations/</guid>
      <pubDate>Sun, 01 Dec 2024 22:48:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的顺序动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</link>
      <description><![CDATA[我有一个代理，其操作必须遵循顺序：A、B、C。但是，环境不稳定或不可预测。代理根据当前情况采取行动，完成其序列（A、B、C）。完成序列后，代理将根据工作完成情况获得一些奖励。它等待环境并分析下一个情况，然后再决定并执行下一组操作。我们如何在这个场景中使用 RL？我们如何训练模型以具有适当的意识来采取行动。每个动作对于获得良好的动作都同样重要。  总之，环境是不可预测的，但我们必须找到一些隐藏的模式来采取这个动作序列。  提前谢谢您！    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</guid>
      <pubDate>Sun, 01 Dec 2024 19:47:34 GMT</pubDate>
    </item>
    </channel>
</rss>