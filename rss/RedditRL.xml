<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 15 Jul 2024 06:24:07 GMT</lastBuildDate>
    <item>
      <title>DQN 中的损失函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e3gkro/loss_function_in_dqn/</link>
      <description><![CDATA[大家好， 请问一下深度 Q 学习算法中损失函数的作用和功能/目的是什么     提交人    /u/Correct-Jaguar-339   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e3gkro/loss_function_in_dqn/</guid>
      <pubDate>Sun, 14 Jul 2024 23:47:27 GMT</pubDate>
    </item>
    <item>
      <title>“使用强化学习解决《流放之路》物品制作问题”（价值迭代）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e37obz/solving_path_of_exile_item_crafting_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e37obz/solving_path_of_exile_item_crafting_with/</guid>
      <pubDate>Sun, 14 Jul 2024 17:19:11 GMT</pubDate>
    </item>
    <item>
      <title>我计划为我的大学做一个自动驾驶汽车项目。我想要与自动驾驶汽车中的 RL 相关的指导。我应该如何在开放的 AI 健身房中为汽车创建训练环境。有很多问题 TT。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e32t8f/i_am_planning_to_make_a_self_driving_car_project/</link>
      <description><![CDATA[我从谷歌中提取了一个 3D 地图，并计划进一步将其导入到 Gazebo 中进行模拟和物理之类的事情。现在我有一个问题，我应该如何在 Gazebo 和 ROS 中使用 RL 来实现自动驾驶汽车。出于训练目的，考虑使用谷歌的 3D 地图。有哪位专家可以指导我吗？    提交人    /u/manas_otaku   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e32t8f/i_am_planning_to_make_a_self_driving_car_project/</guid>
      <pubDate>Sun, 14 Jul 2024 13:51:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 适用于 ML 应用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2xfz6/rl_for_ml_applications/</link>
      <description><![CDATA[大家好！我即将攻读与人工智能和机器学习相关的计算机科学博士学位。我想研究一些包含或结合强化学习和机器学习问题的东西。我曾参与过与控制系统和机器人相关的强化学习项目，这些项目主要涉及训练代理执行任务。但是，我想在一些机器学习问题中使用强化学习。根据目前的趋势，有人能建议一些有趣的领域或应用吗？其中考虑的一些是用于 NLP 任务的强化学习或用于图像分类的强化学习。我将研究更多可能性，但任何方向的指导都将不胜感激。    提交人    /u/shazfu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2xfz6/rl_for_ml_applications/</guid>
      <pubDate>Sun, 14 Jul 2024 08:38:39 GMT</pubDate>
    </item>
    <item>
      <title>寻找从零开始到完善代理的 RL 内容。任何主题或媒体</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2o9en/looking_for_rl_content_from_scratch_to_polished/</link>
      <description><![CDATA[我正在寻找涵盖从头开始到完善代理的环境编码内容。 我想了解他们从训练简单模型中获得的所有超参数和见解，因为他们建立了更复杂的奖励、惩罚或其他任何使代理工作的东西。 例如 https://www.youtube.com/watch?v=SX08NT55YhA https://www.youtube.com/watch?v=DcYLT37ImBY&amp;t=1294s 书面/视频很好，如果也有训练日志那就最好了。建议在哪里看？    由   提交  /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2o9en/looking_for_rl_content_from_scratch_to_polished/</guid>
      <pubDate>Sat, 13 Jul 2024 23:43:06 GMT</pubDate>
    </item>
    <item>
      <title>计算 Forward View Lambda 回报</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2n2aa/calculating_forward_view_lambda_return/</link>
      <description><![CDATA[        提交人    /u/ikevblack   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2n2aa/calculating_forward_view_lambda_return/</guid>
      <pubDate>Sat, 13 Jul 2024 22:46:21 GMT</pubDate>
    </item>
    <item>
      <title>[R] 理解强化学习中离散表示的不合理有效性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2gqcn/r_understanding_the_unreasonable_effectiveness_of/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2gqcn/r_understanding_the_unreasonable_effectiveness_of/</guid>
      <pubDate>Sat, 13 Jul 2024 18:05:58 GMT</pubDate>
    </item>
    <item>
      <title>弃用函数近似中的折扣奖励（Sutton 10.4）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</link>
      <description><![CDATA[Sutton 指出，在持续问题中使用函数近似时，使用折扣奖励不再有意义。 这对我来说真的没有意义，有人可以详细说明一下吗？    提交人    /u/federicom01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2eqg8/deprecating_discounted_reward_in_function/</guid>
      <pubDate>Sat, 13 Jul 2024 16:40:41 GMT</pubDate>
    </item>
    <item>
      <title>用大约 13 分钟解释我 2 年的 RL 研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</link>
      <description><![CDATA[        由    /u/ejmejm1 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2co3h/explaining_2_years_of_my_rl_research_in_13_minutes/</guid>
      <pubDate>Sat, 13 Jul 2024 15:11:20 GMT</pubDate>
    </item>
    <item>
      <title>REINFORCE 算法中的大批量会起作用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</link>
      <description><![CDATA[通常我看到人们在实施 REINFORCE 算法（使用神经网络）时会这样做： for state, action, reward in episodes: update (batch size is 1)  如果游戏长度为 50 轮，我们也可以将所有状态、动作和奖励连接到批量大小为 50 的张量中并进行更新。我尝试过，并且取得了相当不错的成功，值得注意的是（并且不出所料）它大大加快了训练速度。 所以我在想，是什么会阻止我们进行更多连接。假设我们不是每 50 轮游戏更新一次，而是每 10 轮游戏更新一次。张量的维度足够小，这将显著提高计算速度，并可能导致更好的梯度估计。但是，我们最终进行的更新较少。这是我们在监督学习中看到的标准 batch_size 超参数权衡问题。 为什么没人尝试过？或者，也许我只是不擅长搜索是否有人尝试过。 在尝试之前想问一下，因为模拟一切有时需要几天时间。 在你来找我之前，是的，我知道有更好的算法，我只是喜欢先探索非常非常简单的算法。    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e2ath6/would_large_batches_in_the_reinforce_algorithm/</guid>
      <pubDate>Sat, 13 Jul 2024 13:46:53 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：在 pokerenv 中 >>: 'list' 和 'int' 的操作数类型不受支持</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e23q7r/typeerror_unsupported_operand_types_for_list_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e23q7r/typeerror_unsupported_operand_types_for_list_and/</guid>
      <pubDate>Sat, 13 Jul 2024 06:32:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于连续组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e22p2t/rl_for_continuous_combinatorics/</link>
      <description><![CDATA[您好， 我在工作中遇到一种情况，我正尝试使用机器学习模型来解决连续组合问题。本质上，想象一系列变量 x1-xN，它们可以是任意数字（连续）。我的梯度非常陡峭，很难通过 SGD 等方法导航。我需要找到这些变量的组合，以便优化从这些变量派生的某些属性。有什么建议吗？    提交人    /u/elaraxelara   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e22p2t/rl_for_continuous_combinatorics/</guid>
      <pubDate>Sat, 13 Jul 2024 05:28:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DQN 和 DRL 有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</link>
      <description><![CDATA[我知道 NN 是函数近似器，但要近似某些东西，您必须知道真实值（监督学习）。而且您永远无法在 TD 方法（如 Q-Learning 和一般 RL）中看到真实值。它将样本估计作为客观值与我们已经拥有的东西（更多样本估计）进行比较。为什么它效果这么好？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1v5re/why_dqn_and_drl_work/</guid>
      <pubDate>Fri, 12 Jul 2024 22:57:05 GMT</pubDate>
    </item>
    <item>
      <title>平均奖励与奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1lyk2/mean_reward_vs_reward/</link>
      <description><![CDATA[为什么在每个 episode 的简单奖励上使用平均奖励而不是简单奖励？    提交人    /u/Glum_Flower_8682   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1lyk2/mean_reward_vs_reward/</guid>
      <pubDate>Fri, 12 Jul 2024 16:29:26 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫性质被尊重吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e1gi7j/is_markov_property_respected/</link>
      <description><![CDATA[我的系统由离散动作和有限状态组成 在状态 1 中执行动作后，动作将不再产生任何效果。我的意思是：&lt;状态 1，任何动作，奖励取决于动作，Same\_Next\_State2&gt;。无论代理尝试什么动作，它都会转到相同的 next_state 并根据动作获得奖励。    提交人    /u/Glum_Flower_8682   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e1gi7j/is_markov_property_respected/</guid>
      <pubDate>Fri, 12 Jul 2024 12:31:59 GMT</pubDate>
    </item>
    </channel>
</rss>