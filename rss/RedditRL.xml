<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 29 Nov 2024 18:23:33 GMT</lastBuildDate>
    <item>
      <title>需要帮助微调 ML-Agents PPO 培训 (TensorBoard Insights)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2oagh/need_help_finetuning_mlagents_ppo_training/</link>
      <description><![CDATA[大家好！ 我目前正在训练 ML-Agents PPO 代理逃离一系列房间，其中每个房间都有特定的时间限制。代理设法找到出路并完成每个步骤，但逃离房间花费的时间太长了。我相信通过更好的参数调整，训练可以更高效、更快速。以下是我的观察结果、TensorBoard 的屏幕截图以及我的设置细节。 屏幕截图  环境指标： 图片 1 损失： 图片 2 策略指标： 图片 3  观察结果  累积奖励正在稳步提高，但我认为它可能会更快。 损失（好奇心、策略、价值）正在减少，但存在波动 - 我应该调整学习率还是缓冲区大小？ 策略熵早期显著下降——这是否表明随着时间的推移探索不足？  训练设置 以下是我为 PPO 代理使用的参数： 训练器配置 behaviors: NavigationAgentController: trainer_type: ppo hyperparameters: batch_size: 1024 buffer_size: 102400 learning_rate: 3.0e-4 beta: 0.01 epsilon: 0.2 lambd: 0.95 num_epoch: 5 learning_rate_schedule: constant beta_schedule: linear epsilon_schedule: constant network_settings: normalize: true hidden_​​units: 128 num_layers: 3 vis_encode_type: simple memory: serial_length: 256 memory_size: 256 reward_signals: extrinsic: gamma：0.99 强度：1 好奇心： gamma：0.99 强度：0.01 学习率：0.0003 网络设置： encoding_size：256 num_layers：4 max_steps：10000000000000 time_horizo​​n：64 summary_freq：20000 keep_checkpoints：5 checkpoint_interval：500000 torch_settings： device：true  奖励结构 [Header(&quot;奖励设置&quot;)] public float fastPlateReward = 1.0f; public float mediumPlateReward = 0.9f; public float slowPlateReward = 0.8f; public float explorationReward = 0.05f; public float fallPenalty = -0.5f; public float wallPenalty = -0.1f; public float groundPenalty = -0.05f; public float timePenalty = -0.05f; [Header(&quot;跳跃惩罚设置&quot;)] public float jumpPenalty = -0.05f; // 跳跃的惩罚 public float jumpPenaltyInterval = 1f; // 应用惩罚的间隔 [Header(&quot;逃离房间的奖励&quot;)] public float escapeRoomReward = 5.0f; private bool rewardGranted = false; [Header(&quot;门奖励设置&quot;)] public float doorProximityReward = 0.5f; // 每个接近单位的奖励 public floatreachingDoorReward = 2.0f; // 到达门的固定奖励 public float maxRewardDistance = 10.0f; // 考虑奖励的最大距离 public float distanceToReachDoor = 2.0f; // 考虑到达门的距离  TensorBoard Metrics  熵：早期迅速下降。 好奇心前向损失：迅速降至接近零 - 我应该增加好奇心：强度吗？ 策略损失：训练中期出现波动。我是否需要更大的缓冲区大小？ 外在奖励：稳步改善但进展缓慢。  我正在寻找什么  这些 TensorBoard 指标中是否存在明显的瓶颈？ 我是否应该调整奖励信号或超参数以使训练更快或更稳健？ 有任何关于改进探索和策略稳定性的建议吗？  非常希望得到您的反馈！提前感谢您抽出宝贵的时间来提供帮助。    提交人    /u/Popular_Lunch_3244   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2oagh/need_help_finetuning_mlagents_ppo_training/</guid>
      <pubDate>Fri, 29 Nov 2024 15:31:18 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 SAC 方法是否过度拟合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</link>
      <description><![CDATA[      https://preview.redd.it/r1tizdsxbt3e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=0ba0880704571a2868c12a02d3780bb3244d34aa 我是强化学习的初学者，正在使用软演员-评论家（SAC）方法对电动汽车的智能充电进行优化。目标是优化多个电动汽车在离散时间段内的充电计划，以最大限度地降低成本，同时满足电池和电网约束。我已经实现了一个带有优先采样的重放缓冲区，并添加了优先级衰减和动态采样等技术来增强训练稳定性并解决潜在的过度拟合问题。但是，我不确定是否发生了过度拟合，以及如何根据训练和评估奖励之间的差距确定合适的停止标准。我希望得到有关改进模型学习和确保更好的泛化的指导。    提交人    /u/Ok_Efficiency_1318   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</guid>
      <pubDate>Fri, 29 Nov 2024 10:02:52 GMT</pubDate>
    </item>
    <item>
      <title>强化学习模型如何在训练和测试期间处理序列信息</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2idp0/how_reinforcement_learning_models_handle/</link>
      <description><![CDATA[我对强化学习 (RL) 模型在训练和测试期间如何处理序列有一些疑问：  在训练期间学习序列：  当我们一次将一个状态传递给模型时，它如何学习达到目标所需的状态序列？ 模型如何理解序列中不同状态之间的联系？  传递序列信息：  我们如何向模型提供有关状态序列的信息，以便它在做出决策时记住过去的动作或事件？  在测试期间使用序列：  在测试时，模型如何使用过去的状态和动作序列进行预测？ 它是否依赖于在训练期间学到的内容，还是有办法记住并处理过去的信息？   列出一些有用的资源。 提前谢谢你    提交人    /u/laxuu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2idp0/how_reinforcement_learning_models_handle/</guid>
      <pubDate>Fri, 29 Nov 2024 09:47:22 GMT</pubDate>
    </item>
    <item>
      <title>Q 学习中的 Epsilon 贪婪收敛到最优策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ae18/epsilon_greedy_convergence_to_optimal_policy_in/</link>
      <description><![CDATA[      大家好， 我正在研究不同参数如何影响收敛速度，以及算法是否在全部。在我的实验中，我运行了一个非常简单的网格世界 - 本质上是试图找到迷宫中的终点。除了迷宫出口的奖励为 1 之外，其他地方的奖励都是 0。环境是非平稳的，并在附图中用蓝色虚线表示的 3 个点处发生变化。我目前没有给代理足够的时间来适应这些变化，但这不是我目前主要关心的问题。 我正在运行一个具有 epsilon 贪婪策略的 Q-Learning 代理。使用固定的 0.25 epsilon，代理比衰减的 epsilon 更快地找到出路。但是，如果我根据这些代理的 Q 值比较贪婪策略，通过检查我可以看到固定的 0.25 epsilon 实际上不是最佳的，而衰减的 epsilon 的代理是最佳的。具有固定 epsilon 的代理确实会从其起点找到最佳路线，但是，我看到的是，在不在最佳路径上的状态下，策略不会指向最佳方向。 我认为，使用固定 epsilon，代理实际上会随着时间的推移进行更多探索，因此这些状态应该在其 Q 值中更加清晰。所以我认为我在这里缺乏一些直觉，如果有人能提供帮助，将不胜感激！ https://preview.redd.it/cu21nn6wpq3e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=dea0446dd51e85dcb5107afa4ab86e11c7581780    提交人    /u/LostBandard   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ae18/epsilon_greedy_convergence_to_optimal_policy_in/</guid>
      <pubDate>Fri, 29 Nov 2024 01:14:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于音乐生成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h26qo5/rl_for_music_generation/</link>
      <description><![CDATA[我在考虑是否有可能将演员-评论家算法与音乐生成结合起来。我认为这对我的硕士论文来说很有趣。明天我应该告诉我的导师我将研究哪个主题，我不确定这个主题是否可行。请帮帮我    提交人    /u/KevinBeicon   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h26qo5/rl_for_music_generation/</guid>
      <pubDate>Thu, 28 Nov 2024 21:55:47 GMT</pubDate>
    </item>
    <item>
      <title>易于设置的环境，模拟四足动物，同时尽可能逼真</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2680o/easytosetup_environments_to_simulate_quadrupeds/</link>
      <description><![CDATA[我正在寻找以下内容：  易于安装 具有 Python API 且易于使用（如健身房环境） 具有摄像头和其他传感器信息  考虑到我的要求，Isaac Lab 似乎是完美的选择，但不幸的是我的硬件不受 Isaac Lab 支持。还有其他一些专门实现（类似狗的）四足动物的项目吗？    提交人    /u/carlml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2680o/easytosetup_environments_to_simulate_quadrupeds/</guid>
      <pubDate>Thu, 28 Nov 2024 21:29:32 GMT</pubDate>
    </item>
    <item>
      <title>用于 DRL 的 C++ 与 Rust</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h22jn0/c_vs_rust_for_drl/</link>
      <description><![CDATA[我一直在使用 Python 和 JAX 开发 DRL 框架。我之所以选择 JAX，是因为它能够极大地提高速度。但是，最近我一直在考虑放弃 Python，采用一种高性能语言，希望获得更快的速度。我正在考虑使用 C++ 或 Rust。 你会建议使用哪种语言？    提交人    /u/Muscle_Robot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h22jn0/c_vs_rust_for_drl/</guid>
      <pubDate>Thu, 28 Nov 2024 18:34:24 GMT</pubDate>
    </item>
    <item>
      <title>我应该从 MineRL 还是 MineDojo 开始学习人工智能和强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h20gpl/should_i_start_with_minerl_or_minedojo_for/</link>
      <description><![CDATA[我是强化学习 (RL) 的新手，想在基于 Minecraft 的环境中开始实验。我遇到过两个主要平台：MineRL 和 MineDojo，但我不确定哪一个更适合像我这样的初学者。    提交人    /u/-Dav_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h20gpl/should_i_start_with_minerl_or_minedojo_for/</guid>
      <pubDate>Thu, 28 Nov 2024 17:02:05 GMT</pubDate>
    </item>
    <item>
      <title>我做错了吗？奖励达到上限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1vskf/am_i_doing_something_wrong_reward_is_hitting_a_cap/</link>
      <description><![CDATA[      这是我的大学项目。我正在训练一架无人机在环境中导航并避开障碍物。该方法是使用深度估计模型来确定场景的深度并确定具有清晰路径的区域。然后，无人机学习将感兴趣的区域收敛到其 FOV 的中心。 该方法是使用具有离散动作的策略梯度方法。这些动作是调整滚动、俯仰、偏航和目标高度。输入是所有传感器读数和感兴趣区域的像素中心。然后，将策略网络选择的动作提供给 PID 控制器以控制转子速度来执行动作。 我正在链接包含所有代码的 github 存储库。我正在使用一款名为 webots 的软件，但无论代码如何，它都与任何其他软件一样相似。 https://github.com/BiradarSiddhant02/autonomous-drone-navigation.git 下面是深度图和感兴趣区域究竟发生了什么情况的一些图像 https://preview.redd.it/3fto1axp7n3e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=bb5c7b29fce957d5f64d5f7f576cb952f03707b6 无人机尝试将红十字带向中心。这样做会获得更多奖励。    提交人    /u/Constant_Suspect_317   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1vskf/am_i_doing_something_wrong_reward_is_hitting_a_cap/</guid>
      <pubDate>Thu, 28 Nov 2024 13:19:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么梯度是相对于近似值函数而不是真实值函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1pz07/why_is_the_gradient_wrt_the_approximate_value/</link>
      <description><![CDATA[      有人可以解释一下为什么梯度相对于近似值函数和不是真实值函数。 对于一组参数 w，Del_w = alpha。E[v_pi(S) - v_hat(S, w)) Del_w v_hat(S,w)  请查看屏幕截图以获得更好的书面方程式。 David Silver 第 6 讲从 45:55 开始的解释](https://www.youtube.com/watch?v=UoPei5o4fps&amp;t=2755s)我没看懂。 我的理解方式（在观众中的人提出问题并引发回应之前）是 - 真实值函数就像一个常数。近似值函数是我们试图弄清楚的 - 它相对于参数 w 是变量。相对于真实函数的偏导数为 0。因此，我们仅考虑 delta_w v_hat。 但 David Silver 接着说，存在一些复杂的方法，它们会考虑相对于真实函数和近似函数的梯度。因此，我的理解（以上）可能不正确。    提交人    /u/datashri   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1pz07/why_is_the_gradient_wrt_the_approximate_value/</guid>
      <pubDate>Thu, 28 Nov 2024 06:42:15 GMT</pubDate>
    </item>
    <item>
      <title>政策绩效是否受情境强盗的限制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1izwx/policy_performance_bound_for_contextual_bandits/</link>
      <description><![CDATA[嗨，我正在研究上下文老虎机，以在线方式更新策略，类似于 PPO。本质上，我在缓冲区中收集一批样本，并使用 PPO 损失进行更新，不同之处在于我为所选操作获得单一奖励，即老虎机反馈。鉴于这种设置，RL 文献中的策略改进界限（参考：约束策略优化，推论 1，等式 5）是否适用于上下文老虎机？    提交人    /u/noob_simp_phd   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1izwx/policy_performance_bound_for_contextual_bandits/</guid>
      <pubDate>Thu, 28 Nov 2024 00:09:45 GMT</pubDate>
    </item>
    <item>
      <title>多智能体 RL 的近端策略优化 (PettingZoo)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h1cdfo/proximal_policy_optimization_for_multiagent_rl/</link>
      <description><![CDATA[嗨！我是初学者。 据我所知，从多智能体的角度来看，标准 PPO（不是 MAPPO）是一种去中心化技术（智能体假设其他智能体的策略随时间变化是环境本身的一部分）。如果遵循这种逻辑，那么可以安全地假设，当在环境（合作设置）中使用多个 PPO 智能体时，收敛的概率可能不高，或者至少肯定不能保证。 在基于多智能体的 RL 框架中，特别是 PettingZoo，存在标准 PPO 的现有实现示例。 PettingZoo 环境中的 PPO 实现。 我的问题是，为什么在这样的框架中使用 PPO，但文档中没有提到非平稳性？多个 PPO 代理等方法是否有可能收敛？我主要只是想知道如果代理都使用 PPO，它们合作的现实性如何。另外，我没有考虑其他处理非平稳性的技术，如 IPPO，只是想了解 PPO。 我不知道我是否以足够精确的方式表达了我所想的内容，使其有意义 + 英语不是我的母语，所以我希望它被理解。 提前谢谢！:)    提交人    /u/Classic_Mongoose3182   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h1cdfo/proximal_policy_optimization_for_multiagent_rl/</guid>
      <pubDate>Wed, 27 Nov 2024 19:16:44 GMT</pubDate>
    </item>
    <item>
      <title>需要有关迷宫求解强化学习任务的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h15gq5/need_help_regarding_maze_solving_reinforcement/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h15gq5/need_help_regarding_maze_solving_reinforcement/</guid>
      <pubDate>Wed, 27 Nov 2024 14:23:59 GMT</pubDate>
    </item>
    <item>
      <title>安全离线 MARL 数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h14a11/safe_offline_marl_datasets/</link>
      <description><![CDATA[大家好， 有没有带安全约束的 MARL 环境的开源数据集？    提交人    /u/ImaginaryEducation55   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h14a11/safe_offline_marl_datasets/</guid>
      <pubDate>Wed, 27 Nov 2024 13:27:33 GMT</pubDate>
    </item>
    <item>
      <title>如何学习 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0xs0i/how_to_learn_rl/</link>
      <description><![CDATA[我是新手，请给我一些建议    提交人    /u/Clean_Tip3272   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0xs0i/how_to_learn_rl/</guid>
      <pubDate>Wed, 27 Nov 2024 06:12:01 GMT</pubDate>
    </item>
    </channel>
</rss>