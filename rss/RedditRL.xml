<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 23 Jan 2024 06:19:14 GMT</lastBuildDate>
    <item>
      <title>迷宫游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d9a63/maze_game/</link>
      <description><![CDATA[Q-learning 项目，智能体自行学习找到迷宫内的出口。该项目是作为基于关卡的游戏实现的。 ​ https ://github.com/F-a-b-r-i-z-i-o/maze-game   由   提交/u/Stunning_Ad_1539   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d9a63/maze_game/</guid>
      <pubDate>Mon, 22 Jan 2024 23:04:55 GMT</pubDate>
    </item>
    <item>
      <title>有人知道斯坦福强化学习 XCS234 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d7mey/does_anyone_know_about_stanford_reinforcement/</link>
      <description><![CDATA[大家好， 我正在考虑这个在线课程。不过，我有一份全职工作。我的工作日程非常灵活，但这并不意味着我可以忽略所有的会议。  我看到描述说它不是学生节奏而是教师节奏。那么这是否意味着一旦我错过了课程，我就错过了？那么完成硬件并拿到证书会不会很麻烦？  这里有人以前上过课吗？有评论吗？谢谢   由   提交/u/sunson29  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d7mey/does_anyone_know_about_stanford_reinforcement/</guid>
      <pubDate>Mon, 22 Jan 2024 21:56:45 GMT</pubDate>
    </item>
    <item>
      <title>Cogment Lab 简介 - 用于人机循环 RL 的开发人员工具包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d6owp/introducing_cogment_lab_a_developers_toolkit_for/</link>
      <description><![CDATA[      你好你好，我&#39;我很高兴终于能分享我过去几个月在 AI-R 致力于的开源项目：Cogment Lab！ ​ tl ;博士，如果您想在有人参与的情况下运行 Gymnasium 或 PettingZoo 环境，现在就可以了。 ​ 您可以执行的操作的非详尽列表使用 Cogment Lab 轻松完成： 在 Gymnasium/PZ 中收集人类演示以进行模仿学习 观察学习代理并覆盖其行为 运行实验PettingZoo 环境中的混合人类与人工智能团队（与您的 RL 代理合作，或在竞争性游戏中击败它） 根据人类干预设置奖励 训练基于奖励的 RL 交织在一起实时行为克隆 ​ 该库仍在开发中，但应该完全可用。绝对欢迎任何建议、错误报告和贡献。 ​ Repo 链接：https://github.com/cogment/cogment-lab 教程：https://github.com/cogment/cogment-lab/tree/develop/ ​ ​ &gt; https://preview.redd.it/ 8t8ec4b162ec1.png?width=1081&amp;format=png&amp;auto=webp&amp;s=7ec357da5ac1e318d18ec4c7bde566be39c9c03b ​ PS 我很确定我的老板还在没有注意到这个标志，但它一直保持这种状态，直到有人强迫我让它变得更专业，并与公司在垂直业务或其他方面的协同作用保持一致  &amp;# 32；由   提交 /u/RedTachyon   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d6owp/introducing_cogment_lab_a_developers_toolkit_for/</guid>
      <pubDate>Mon, 22 Jan 2024 21:19:37 GMT</pubDate>
    </item>
    <item>
      <title>帮助表示感谢！尝试让代理在 Unity ML Agents 中投篮</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d2fm0/help_appreciated_trying_to_get_an_agent_to_shoot/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d2fm0/help_appreciated_trying_to_get_an_agent_to_shoot/</guid>
      <pubDate>Mon, 22 Jan 2024 18:24:19 GMT</pubDate>
    </item>
    <item>
      <title>最大化探索：融合估计、规划和探索的一个目标函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d210i/maximize_to_explore_one_objective_function_fusing/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2305.18258 OpenReview：https:// /openreview.net/forum?id=A57UMlUJdc 代码：https： //github.com/agentification/MEX 摘要：  在线强化学习（在线RL）中，平衡探索开发对于以样本有效的方式找到最优策略至关重要。为了实现这一目标，现有的样本高效在线强化学习算法通常由三个部分组成：估计、规划和探索。然而，为了应对通用函数逼近器，它们中的大多数都涉及不切实际的算法组件来激励探索，例如数据相关水平集内的优化或复杂的采样程序。为了应对这一挑战，我们提出了一种易于实现的强化学习框架，称为最大化探索 (MEX)，它只需要优化 &lt; em&gt;不受约束一个单一目标，集成了估计和规划组件，同时自动平衡勘探和开发。理论上，我们证明 MEX 通过马尔可夫决策过程（MDP）的一般函数逼近实现了亚线性遗憾，并且可以进一步扩展到两人零和马尔可夫游戏（MG）。同时，我们采用深度 RL 基线以无模型和基于模型的方式设计 MEX 的实用版本，在各种奖励稀疏的 MuJoCo 环境中，其性能可以稳定地优于基线。与现有的具有一般函数逼近的样本高效在线强化学习算法相比，MEX 实现了相似的样本效率，同时具有更低的计算成本，并且与现代深度强化学习方法更加兼容。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d210i/maximize_to_explore_one_objective_function_fusing/</guid>
      <pubDate>Mon, 22 Jan 2024 18:08:16 GMT</pubDate>
    </item>
    <item>
      <title>Mistral.AI 的 Mistral 7B - 完整白皮书概述</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d1bdy/mistral_7b_from_mistralai_full_whitepaper_overview/</link>
      <description><![CDATA[       由   提交 /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d1bdy/mistral_7b_from_mistralai_full_whitepaper_overview/</guid>
      <pubDate>Mon, 22 Jan 2024 17:39:47 GMT</pubDate>
    </item>
    <item>
      <title>内在奖励快速收敛的随机网络蒸馏。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cwg4h/random_network_distillation_for_intrinsic_reward/</link>
      <description><![CDATA[      你好， TLDR：随机网络蒸馏发生得如此之快没有进行任何探索。 我一直在尝试将随机网络蒸馏应用于问题以鼓励探索。虽然原则上一切正常，但我遇到了随机固定网络的问题快速地提取到我的探索网络中，即随机嵌入和预测嵌入之间的距离减小得如此之快，以至于在进行任何探索之前损失几乎为零。因此，历元的损失曲线和内在奖励看起来像这样：  https://preview.redd.it/3afnzfvo00ec1.png?width=696&amp;format=png&amp;auto=webp&amp;s=9bdcf4838d922ab324f0481f53bb1f44ac89d1ff  我猜这是因为状态表示相对简单（想想由 CNN 编码的代理、墙壁、物体等位置传递的几个布尔掩码），但不幸的是，这是我的文献中此环境的标准表示因此我无法改变它。顺便说一句，这并不会让环境变得容易。 RND 将以这种方式运行，而我的代理无法观察到任何外在奖励（简单空间中所需的复杂动作序列）。  关于如何使其更具挑战性有什么想法吗？我尝试扩大和缩小探索网络的网络架构，但遗憾的是没有成功。  谢谢！   由   提交 /u/Arconer   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cwg4h/random_network_distillation_for_intrinsic_reward/</guid>
      <pubDate>Mon, 22 Jan 2024 14:09:05 GMT</pubDate>
    </item>
    <item>
      <title>珍珠 vs 火炬RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cr8ih/pearl_vs_torchrl/</link>
      <description><![CDATA[这里有人使用过这两个框架，或者对这两个框架有足够的了解吗？   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cr8ih/pearl_vs_torchrl/</guid>
      <pubDate>Mon, 22 Jan 2024 08:44:00 GMT</pubDate>
    </item>
    <item>
      <title>我用 3D 动画教这个机器人自己行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</link>
      <description><![CDATA[       由   提交/u/djessimb   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</guid>
      <pubDate>Mon, 22 Jan 2024 07:08:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 进行深度 SARSA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cko2w/deep_sarsa_with_tensorflow/</link>
      <description><![CDATA[大家好。我的任务是在工作中创建 Deep SARSA 模型，我唯一可以使用的工具是tensorflow（出于安全原因，我无法安装任何其他库，例如 tf_agents）。所以，我的问题是：是否可以像使用 Pytorch 一样使用张量流创建深度 SARSA 模型？就像能够调用优化器、重置梯度、应用反向传播并更新目标值 NN 的权重一样，就像 Pytorch 让我们这样做的方式 ​ This这是我的意思的一个例子（我之前用 Pytorch 实现了 Deep SARSA 模型） https://github.com/edseldim/reinforcement_learning/blob/master/6_deep_sarsa_ideas.ipynb ​ 非常感谢您的回答和建议:)   由   提交/u/Confident_Watch8207  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cko2w/deep_sarsa_with_tensorflow/</guid>
      <pubDate>Mon, 22 Jan 2024 02:16:00 GMT</pubDate>
    </item>
    <item>
      <title>编程…</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</link>
      <description><![CDATA[       由   提交/u/Throwawaybutlove  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</guid>
      <pubDate>Mon, 22 Jan 2024 01:27:44 GMT</pubDate>
    </item>
    <item>
      <title>“基于模型的贝叶斯探索”，Dearden 等人 2013</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ch8dg/modelbased_bayesian_exploration_dearden_et_al_2013/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ch8dg/modelbased_bayesian_exploration_dearden_et_al_2013/</guid>
      <pubDate>Sun, 21 Jan 2024 23:31:05 GMT</pubDate>
    </item>
    <item>
      <title>纯 C# 深度强化学习彗星到 Godot 作为 nuget</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ccqxy/pure_c_deep_reinforcement_learning_comets_to/</link>
      <description><![CDATA[    /u/DotNetEvangeliser   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ccqxy/pure_c_deep_reinforcement_learning_comets_to/</guid>
      <pubDate>Sun, 21 Jan 2024 20:22:59 GMT</pubDate>
    </item>
    <item>
      <title>DQN 代理奖励向后断言错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bihrt/dqn_agent_reward_backward_assertion_error/</link>
      <description><![CDATA[我正在学习强化学习并尝试从论文中复制模型；目标是控制（1x 连续动作）1/4 汽车悬架系统并最大限度地减少随机路面上的悬架行程。我正在使用 keras.rl2 中的深度 Q 网络。 我将代码上传到 github：https://github.com/htmdn/QuarterCarSuspControl/blob/main/DDPG_Susp_Control_02.ipynb 这是我收到的错误：  断言错误回溯（最近一次调用最后）单元格In[16]，第1行----&gt; 1 dqn.fit(env, nb_steps=50000, Visualize=False, verbose=1) 文件C:\apps\anaconda3\envs\x\lib\site-packages \rl\core.py:193，在 Agent.fit(self、env、nb_steps、action_repetition、回调、详细、可视化、nb_max_start_steps、start_step_policy、log_interval、nb_max_episode_steps) 190 if nb_max_episode_steps and Episode_step &gt;= nb_max_episode_steps - 1: 191 # 强制进入终止状态。 192 完成 = 正确 --&gt; 193 指标= self.backward（奖励，终端=完成） 194 Episode_reward +=奖励 196 步骤_logs = { 197 &#39;action&#39;：行动， 198 &#39;观察&#39;：观察，（...） 202 &#39;info&#39;:cumulative_info, 203 } 文件 C:\apps\anaconda3\envs\x\lib\site-packages\rl\agents\dqn.py:271，位于 DQNAgent.backward 中(self,reward,terminal) 269terminal1_batch = np.array(terminal1_batch) 270reward_batch = np.array(reward_batch) --&gt; 271 断言reward_batch.shape == (self.batch_size,) 272 断言terminal1_batch.shape ==reward_batch.shape 273 断言 len(action_batch) == len(reward_batch)   非常感谢任何反馈！   由   提交 /u/htmdn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bihrt/dqn_agent_reward_backward_assertion_error/</guid>
      <pubDate>Sat, 20 Jan 2024 18:38:05 GMT</pubDate>
    </item>
    <item>
      <title>MuDreamer：无需重建即可学习预测世界模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bdlml/mudreamer_learning_predictive_world_models/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=9pe38WpsbX 摘要：  DreamerV3 代理最近展示了状态 -在不同领域中表现最先进，使用像素重建损失在潜在空间中学习强大的世界模型。然而，虽然重建损失对于 Dreamer 的性能至关重要，但它也需要对不必要的信息进行建模。因此，梦想家有时无法感知解决任务所需的关键要素，从而极大地限制了其潜力。在本文中，我们提出了 MuDreamer，这是一种基于 DreamerV3 算法的强化学习代理，通过学习预测世界模型而无需重建输入信号。隐藏表示不是依赖于像素重建，而是通过预测环境值函数和先前选择的动作来学习。与图像的预测自监督方法类似，我们发现批量归一化的使用对于防止学习崩溃至关重要。我们还研究了模型后验损失和先验损失之间的 KL 平衡对收敛速度和学习稳定性的影响。我们在广泛使用的 DeepMind Visual Control Suite 上评估 MuDreamer，并获得与 DreamerV3 相当的性能。 MuDreamer 还在 Atari100k 基准测试中展示了可喜的结果。研究代码将公开。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bdlml/mudreamer_learning_predictive_world_models/</guid>
      <pubDate>Sat, 20 Jan 2024 15:00:47 GMT</pubDate>
    </item>
    </channel>
</rss>