<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 01 Dec 2024 21:15:33 GMT</lastBuildDate>
    <item>
      <title>强化学习中的顺序动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</link>
      <description><![CDATA[我有一个代理，其操作必须遵循顺序：A、B、C。但是，环境不稳定或不可预测。代理根据当前情况采取行动，完成其序列（A、B、C）。完成序列后，代理将根据工作完成情况获得一些奖励。它等待环境并分析下一个情况，然后再决定并执行下一组操作。我们如何在这个场景中使用 RL？我们如何训练模型以具有适当的意识来采取行动。每个动作对于获得良好的动作都同样重要。  总之，环境是不可预测的，但我们必须找到一些隐藏的模式来采取这个动作序列。  提前谢谢您！    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</guid>
      <pubDate>Sun, 01 Dec 2024 19:47:34 GMT</pubDate>
    </item>
    <item>
      <title>“通过语言游戏实现无边界的苏格拉底式学习”，Schaul 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h48l24/boundless_socratic_learning_with_language_games/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h48l24/boundless_socratic_learning_with_language_games/</guid>
      <pubDate>Sun, 01 Dec 2024 17:36:34 GMT</pubDate>
    </item>
    <item>
      <title>在离策略 PPO 中训练 Ant 时超参数是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h45ovi/what_are_the_hyperparameters_when_training_ant_in/</link>
      <description><![CDATA[我是强化学习的新手。使用 torchrl 作为库，我已按照以下教程创建了 PPO 代码，并确认 InvertedDoublePendulum 可以进行离线策略训练。 接下来我想尝试使用 Ant-v4 进行学习，因此我将环境名称更改为 Ant-v4 并开始学习。它似乎学得不太好，所以我将 `frame_skip` 设置为 5，`frames_per_batch` 设置为 `50_000 // frame_skip`，`total_frames` 设置为 `60_000_000 // frame_skip`，`sub_batch_size` 设置为 2500 来增加训练量（其他超参数与上一个教程中的相同）。 然而，Ant 的奖励来来去去，水平很低，并没有达到预期效果，如下面的视频所示。鉴于增加学习量不起作用，我认为这是超参数设置的问题，但我没有从谷歌获得任何好的见解。 我的代码中缺少什么？ https://reddit.com/link/1h45ovi/video/ewie7pmk994e1/player    提交人    /u/Novel-Resolve-1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h45ovi/what_are_the_hyperparameters_when_training_ant_in/</guid>
      <pubDate>Sun, 01 Dec 2024 15:28:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Q_Learning 算法不能正常学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/</link>
      <description><![CDATA[嗨，我目前正在编写一个 AI，该 AI 应该使用 Q-Learning 学习井字游戏。我的问题是，该模型在开始时学习了一点，但随后变得越来越糟，并没有变得更好。我正在使用  old_qvalue + self.alpha * (reward + self.gamma * max_qvalue_nextstate - old_qvalue) 更新 QValues，其中 alpha 为 0.3，gamma 为 0.9。我还使用 Epsilon Greedy 策略和衰减的 Epsilon，从 0.9 开始，每回合减少 0.0005，在 0.1 时停止减少。对手是一个 Minimax 算法。我没有发现代码中的任何缺陷，Chat GPT 也没有，我想知道我做错了什么。如果有人有任何提示，我将不胜感激。不幸的是，代码是德文的，我现在没有设置 Github 帐户。    提交人    /u/_waterstar_   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/</guid>
      <pubDate>Sat, 30 Nov 2024 15:19:12 GMT</pubDate>
    </item>
    <item>
      <title>SAC（Soft Actor Critc）无法解决某些任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h3bey8/sac_soft_actor_critc_cannot_solve_some_tasks/</link>
      <description><![CDATA[我编写了一个软演员评论家算法，我想稍后将其用于 carla 模拟器的自动驾驶。我的代码管理器可以解决简单的任务，但是当我在 carla 上尝试它时，即使我的奖励基于硕士论文，我也会得到糟糕的表现。硕士论文获得了更好的表现。如果有人可以检查我的代码是否存在数学或编程错误，那就太好了。你可以在 github 上找到我的代码：https://github.com/b-gtr/Soft-Actor-Critic    提交人    /u/Fair_Device_4961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h3bey8/sac_soft_actor_critc_cannot_solve_some_tasks/</guid>
      <pubDate>Sat, 30 Nov 2024 12:19:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我训练 PPO 时 std 会增加？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h39upr/why_is_std_increasing_when_i_train_ppo/</link>
      <description><![CDATA[我正在演员-评论家设置中，对具有连续动作（高斯分布）的简单任务测试 PPO。演员网络正在快速学习平均值的最优值，但标准差不断增加（最优解是确定性的，标准差越高，回报越差）。发生这种情况的原因可能是什么？我没有使用任何奖励进行探索。标准差与状态无关。    提交人    /u/Ok_Amoeba_9527   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h39upr/why_is_std_increasing_when_i_train_ppo/</guid>
      <pubDate>Sat, 30 Nov 2024 10:29:01 GMT</pubDate>
    </item>
    <item>
      <title>我无法让这个 dqn 在网格世界中收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h374em/i_cannot_get_this_dqn_to_converge_on_grid_world/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h374em/i_cannot_get_this_dqn_to_converge_on_grid_world/</guid>
      <pubDate>Sat, 30 Nov 2024 07:09:35 GMT</pubDate>
    </item>
    <item>
      <title>“机器人学习方式的革命：未来一代的机器人不会被编程来完成特定任务。相反，它们将使用人工智能进行自我教育”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2wveu/a_revolution_in_how_robots_learn_a_future/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2wveu/a_revolution_in_how_robots_learn_a_future/</guid>
      <pubDate>Fri, 29 Nov 2024 21:53:05 GMT</pubDate>
    </item>
    <item>
      <title>确定性策略的预期收益公式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ud32/expected_return_formula_of_deterministic_policy/</link>
      <description><![CDATA[      我有一个关于确定性策略的预期回报如何写出的问题。我发现在某些情况下使用 Q 函数，如表达式 5 所示。但是，我不完全理解与随机策略相反地获得它的步骤。获得表达式 5 的步骤或理由是什么？ https://preview.redd.it/q6ykgkjzbw3e1.png?width=711&amp;format=png&amp;auto=webp&amp;s=b4ad5f9bbd75430a83d6b240395f52431fed3486    提交人    /u/Street-Vegetable-117   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ud32/expected_return_formula_of_deterministic_policy/</guid>
      <pubDate>Fri, 29 Nov 2024 19:58:19 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 动作模仿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2t726/mujoco_motion_imitation/</link>
      <description><![CDATA[编写一个程序，获取 bvh 数据并尝试用 mujoco 人形机器人模仿它，这有多可行。我认为代理必须在大量数据上进行训练，我已经将 CMU 数据集确定为常用数据集。有人能指出实现这一点的项目吗，或者描述它将如何执行？谢谢。    提交人    /u/snotrio   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2t726/mujoco_motion_imitation/</guid>
      <pubDate>Fri, 29 Nov 2024 19:06:21 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助微调 ML-Agents PPO 培训 (TensorBoard Insights)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2oagh/need_help_finetuning_mlagents_ppo_training/</link>
      <description><![CDATA[大家好！ 我目前正在训练 ML-Agents PPO 代理逃离一系列房间，其中每个房间都有特定的时间限制。代理设法找到出路并完成每个步骤，但逃离房间花费的时间太长了。我相信通过更好的参数调整，训练可以更高效、更快速。以下是我的观察结果、TensorBoard 的屏幕截图以及我的设置细节。 屏幕截图  环境指标： 图片 1 损失： 图片 2 策略指标： 图片 3  观察结果  累积奖励正在稳步提高，但我认为它可能会更快。 损失（好奇心、策略、价值）正在减少，但存在波动 - 我应该调整学习率还是缓冲区大小？ 策略熵早期显著下降——这是否表明随着时间的推移探索不足？  训练设置 以下是我为 PPO 代理使用的参数： 训练器配置 behaviors: NavigationAgentController: trainer_type: ppo hyperparameters: batch_size: 1024 buffer_size: 102400 learning_rate: 3.0e-4 beta: 0.01 epsilon: 0.2 lambd: 0.95 num_epoch: 5 learning_rate_schedule: constant beta_schedule: linear epsilon_schedule: constant network_settings: normalize: true hidden_​​units: 128 num_layers: 3 vis_encode_type: simple memory: serial_length: 256 memory_size: 256 reward_signals: extrinsic: gamma：0.99 强度：1 好奇心： gamma：0.99 强度：0.01 学习率：0.0003 网络设置： encoding_size：256 num_layers：4 max_steps：10000000000000 time_horizo​​n：64 summary_freq：20000 keep_checkpoints：5 checkpoint_interval：500000 torch_settings： device：true  奖励结构 [Header(&quot;奖励设置&quot;)] public float fastPlateReward = 1.0f; public float mediumPlateReward = 0.9f; public float slowPlateReward = 0.8f; public float explorationReward = 0.05f; public float fallPenalty = -0.5f; public float wallPenalty = -0.1f; public float groundPenalty = -0.05f; public float timePenalty = -0.05f; [Header(&quot;跳跃惩罚设置&quot;)] public float jumpPenalty = -0.05f; // 跳跃的惩罚 public float jumpPenaltyInterval = 1f; // 应用惩罚的间隔 [Header(&quot;逃离房间的奖励&quot;)] public float escapeRoomReward = 5.0f; private bool rewardGranted = false; [Header(&quot;门奖励设置&quot;)] public float doorProximityReward = 0.5f; // 每个接近单位的奖励 public floatreachingDoorReward = 2.0f; // 到达门的固定奖励 public float maxRewardDistance = 10.0f; // 考虑奖励的最大距离 public float distanceToReachDoor = 2.0f; // 考虑到达门的距离  TensorBoard Metrics  熵：早期迅速下降。 好奇心前向损失：迅速降至接近零 - 我应该增加好奇心：强度吗？ 策略损失：训练中期出现波动。我是否需要更大的缓冲区大小？ 外在奖励：稳步改善但进展缓慢。  我正在寻找什么  这些 TensorBoard 指标中是否存在明显的瓶颈？ 我是否应该调整奖励信号或超参数以使训练更快或更稳健？ 有任何关于改进探索和策略稳定性的建议吗？  非常希望得到您的反馈！提前感谢您抽出宝贵的时间来提供帮助。    提交人    /u/Popular_Lunch_3244   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2oagh/need_help_finetuning_mlagents_ppo_training/</guid>
      <pubDate>Fri, 29 Nov 2024 15:31:18 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 SAC 方法是否过度拟合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</link>
      <description><![CDATA[      https://preview.redd.it/r1tizdsxbt3e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=0ba0880704571a2868c12a02d3780bb3244d34aa 我是强化学习的初学者，正在使用软演员-评论家（SAC）方法对电动汽车的智能充电进行优化。目标是优化多个电动汽车在离散时间段内的充电计划，以最大限度地降低成本，同时满足电池和电网约束。我已经实现了一个带有优先采样的重放缓冲区，并添加了优先级衰减和动态采样等技术来增强训练稳定性并解决潜在的过度拟合问题。但是，我不确定是否发生了过度拟合，以及如何根据训练和评估奖励之间的差距确定合适的停止标准。我希望得到有关改进模型学习和确保更好的泛化的指导。    提交人    /u/Ok_Efficiency_1318   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</guid>
      <pubDate>Fri, 29 Nov 2024 10:02:52 GMT</pubDate>
    </item>
    <item>
      <title>Q 学习中的 Epsilon 贪婪收敛到最优策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ae18/epsilon_greedy_convergence_to_optimal_policy_in/</link>
      <description><![CDATA[      大家好， 我正在研究不同参数如何影响收敛速度，以及算法是否在全部。在我的实验中，我运行了一个非常简单的网格世界 - 本质上是试图找到迷宫中的终点。除了迷宫出口的奖励为 1 之外，其他地方的奖励都是 0。环境是非平稳的，并在附图中用蓝色虚线表示的 3 个点处发生变化。我目前没有给代理足够的时间来适应这些变化，但这不是我目前主要关心的问题。 我正在运行一个具有 epsilon 贪婪策略的 Q-Learning 代理。使用固定的 0.25 epsilon，代理比衰减的 epsilon 更快地找到出路。但是，如果我根据这些代理的 Q 值比较贪婪策略，通过检查我可以看到固定的 0.25 epsilon 实际上不是最佳的，而衰减的 epsilon 的代理是最佳的。具有固定 epsilon 的代理确实会从其起点找到最佳路线，但是，我看到的是，在不在最佳路径上的状态下，策略不会指向最佳方向。 我认为，使用固定 epsilon，代理实际上会随着时间的推移进行更多探索，因此这些状态应该在其 Q 值中更加清晰。所以我认为我在这里缺乏一些直觉，如果有人能提供帮助，将不胜感激！ https://preview.redd.it/cu21nn6wpq3e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=dea0446dd51e85dcb5107afa4ab86e11c7581780    提交人    /u/LostBandard   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ae18/epsilon_greedy_convergence_to_optimal_policy_in/</guid>
      <pubDate>Fri, 29 Nov 2024 01:14:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于音乐生成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h26qo5/rl_for_music_generation/</link>
      <description><![CDATA[我在考虑是否有可能将演员-评论家算法与音乐生成结合起来。我认为这对我的硕士论文来说很有趣。明天我应该告诉我的导师我将研究哪个主题，我不确定这个主题是否可行。请帮帮我    提交人    /u/KevinBeicon   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h26qo5/rl_for_music_generation/</guid>
      <pubDate>Thu, 28 Nov 2024 21:55:47 GMT</pubDate>
    </item>
    <item>
      <title>用于 DRL 的 C++ 与 Rust</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h22jn0/c_vs_rust_for_drl/</link>
      <description><![CDATA[我一直在使用 Python 和 JAX 开发 DRL 框架。我之所以选择 JAX，是因为它能够极大地提高速度。但是，最近我一直在考虑放弃 Python，采用一种高性能语言，希望获得更快的速度。我正在考虑使用 C++ 或 Rust。 你会建议使用哪种语言？    提交人    /u/Muscle_Robot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h22jn0/c_vs_rust_for_drl/</guid>
      <pubDate>Thu, 28 Nov 2024 18:34:24 GMT</pubDate>
    </item>
    </channel>
</rss>