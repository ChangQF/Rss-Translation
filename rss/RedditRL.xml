<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 02 Oct 2024 03:21:36 GMT</lastBuildDate>
    <item>
      <title>TD3 在智能列车优化中的应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ftwjrn/td3_in_smart_train_optimization/</link>
      <description><![CDATA[我有一个模拟环境，火车可以在其中启动、加速和在车站停靠。但是，当使用 TD3 代理进行 1,000 集时，它很难掌握场景。我尝试调整超参数、奖励和神经网络层，但代理在测试期间仍采取类似的操作值。 在我的设置中，操作控制火车的加速度，具有距离、速度、到达车站的时间和模拟动作等特征。奖励函数采用各种指标设计，在开始时应用较大的惩罚，并在火车接近目标时减少惩罚以激励前进。 我将原始数据传递给策略而没有进行规范化。这个问题可能与奖励结构、模型本身有关，还是我应该考虑添加其他功能？    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ftwjrn/td3_in_smart_train_optimization/</guid>
      <pubDate>Tue, 01 Oct 2024 19:20:10 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 .pt 文件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ftiwj7/how_do_i_use_a_pt_file/</link>
      <description><![CDATA[大家好...我对强化学习、机器学习、神经网络等概念还不熟悉。我有一个 .pt 文件，这是我在 isaac sim/lab 环境中训练机器人后获得的策略...我想使用 .pt 文件并向其提供来自模拟传感器的输入，并在现实世界中运行电机...有人可以给我指出一些可以让我做到这一点的资源吗...这项练习背后的主要动机是使用策略并在现实世界中移动执行器。    提交人    /u/Grand-Date4504   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ftiwj7/how_do_i_use_a_pt_file/</guid>
      <pubDate>Tue, 01 Oct 2024 08:07:10 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 构建算法交易代理的教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fthite/tutorial_on_using_rl_to_build_algo_trading_agent/</link>
      <description><![CDATA[https://www.aion-research.com/post/building-a-reinforcement-learning-agent-for-algorithmic-trading 这是一个简化的示例，因此请勿将其用于您的实际交易。我还没有能够将 RL 应用于我的真实量化金融工作，所以如果有人之前成功过，请告诉我！    提交人    /u/MarketMood   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fthite/tutorial_on_using_rl_to_build_algo_trading_agent/</guid>
      <pubDate>Tue, 01 Oct 2024 06:24:26 GMT</pubDate>
    </item>
    <item>
      <title>简单的 JavaScript 代码可以保护平民免受美国政府在国内外进行的无人机袭击</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ftdgo9/simple_javascript_code_that_could_protect/</link>
      <description><![CDATA[        由    /u/AnthonyofBoston 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ftdgo9/simple_javascript_code_that_could_protect/</guid>
      <pubDate>Tue, 01 Oct 2024 02:24:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习在线讲座</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ft5e8u/online_lectures_on_reinforcement_learning/</link>
      <description><![CDATA[大家好，我想与大家分享我在 YouTube 上关于强化学习的讲座：   https://www.youtube.com/playlist?list=PLW4eqbV8qk8YUmaN0vIyGxUNOVqFzC2pd   每周三和周日早上都会发布新的视频。您可以订阅我的YouTube频道（https://www.youtube.com/tyucelen）并开启通知以随时关注！如果您能将这些讲座转发给您的同事/学生，我将不胜感激。   以下是要涵盖的主题：    强化学习简介（已发布） 马尔可夫决策过程（已发布） 动态规划（已发布） Q 函数迭代 Q 学习 带有 Matlab 代码的 Q 学习示例 SARSA 带有 Matlab 代码的 SARSA 示例 神经网络 连续空间中的强化学习 神经 Q 学习 带有 Matlab 代码的神经 Q 学习示例 神经 SARSA 带有 Matlab 代码的神经 SARSA 示例 经验重播 运行时保证 带有 Matlab 代码的 Gridworld 示例  祝一切顺利， Tansel Tansel Yucelen，博士 自主、控制、信息和系统实验室主任（LACIS) 机械工程系副教授 南佛罗里达大学，美国佛罗里达州坦帕市 33620 X，领英, YouTube, 770-331-8496 (手机)    提交人    /u/Original-Promise-312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ft5e8u/online_lectures_on_reinforcement_learning/</guid>
      <pubDate>Mon, 30 Sep 2024 20:16:21 GMT</pubDate>
    </item>
    <item>
      <title>[演讲] Rich Sutton，迈向更好的深度学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ft1e84/talk_rich_sutton_toward_a_better_deep_learning/</link>
      <description><![CDATA[       由    /u/atgctg  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ft1e84/talk_rich_sutton_toward_a_better_deep_learning/</guid>
      <pubDate>Mon, 30 Sep 2024 17:32:37 GMT</pubDate>
    </item>
    <item>
      <title>google colab 中的 DGL 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fsx98t/issues_with_dgl_in_google_colab/</link>
      <description><![CDATA[大家好， 我一直在 Google Colab 中使用 DGL，之前它运行良好。但是，最近我在安装和导入它时遇到了问题。 这是我使用的安装命令： !pip install dgl dglgo -f https://data.dgl.ai/wheels/repo.html 我看到的错误是： ---&gt; 24 来自 torch.utils._import_utils 导入 dill_available 25 来自 torch.utils.data.datapipes.utils.common 导入 _check_unpickable_fn 26  ModuleNotFoundError：没有名为“torch.utils._import_utils”的模块 有没有其他人遇到过这个问题或找到了解决方案？ 谢谢！    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fsx98t/issues_with_dgl_in_google_colab/</guid>
      <pubDate>Mon, 30 Sep 2024 14:44:42 GMT</pubDate>
    </item>
    <item>
      <title>RL 初学者指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fstre9/rl_beginner_guide/</link>
      <description><![CDATA[您好，有没有关于使用 Python（最好是 PyTorch）从头开始讲解 RL 的帖子或指南？    提交人    /u/LahmeriMohamed   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fstre9/rl_beginner_guide/</guid>
      <pubDate>Mon, 30 Sep 2024 12:00:02 GMT</pubDate>
    </item>
    <item>
      <title>强化学习速查表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fsrtgf/reinforcement_learning_cheat_sheet/</link>
      <description><![CDATA[大家好！ 我刚刚在 Medium 上发表了我的第一篇文章，还创建了一个强化学习备忘单。🎉 我很乐意听到您的反馈、建议或任何关于如何改进它们的想法！ 请随时查看它们，并提前感谢您的支持！😊 https://medium.com/@ruipcf/reinforcement-learning-cheat-sheet-39bdecb8b5b4    提交人    /u/Prudent_Nose921   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fsrtgf/reinforcement_learning_cheat_sheet/</guid>
      <pubDate>Mon, 30 Sep 2024 09:55:27 GMT</pubDate>
    </item>
    <item>
      <title>防止机器人抖动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fsouk4/prevent_jittery_motions_on_robot/</link>
      <description><![CDATA[嗨， 我正在训练速度跟踪策略，在保持机器人静止时不抖动方面遇到了一些麻烦。我确实对动作速率进行了惩罚，但这似乎仍然无法阻止它疯狂地抖动。 我确实对我的真实机器人进行了加速度限制，以尝试减轻这些抖动运动，但我也担心这会扩大模拟与真实之间的动态差距，因为在我的模拟器平台中似乎没有添加加速度限制的选项。 （IsaacLab/Sim）  谢谢！ https://reddit.com/link/1fsouk4/video/8boi27311wrd1/player    提交人    /u/diamondspork   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fsouk4/prevent_jittery_motions_on_robot/</guid>
      <pubDate>Mon, 30 Sep 2024 06:07:08 GMT</pubDate>
    </item>
    <item>
      <title>RL 用于运动提示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fslhhv/rl_for_motion_cueing/</link>
      <description><![CDATA[        提交人    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fslhhv/rl_for_motion_cueing/</guid>
      <pubDate>Mon, 30 Sep 2024 02:41:16 GMT</pubDate>
    </item>
    <item>
      <title>“效率太高会让一切变得更糟：过度拟合和强版古德哈特定律”，Jascha Sohl-Dickstein 2022</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fshvfx/too_much_efficiency_makes_everything_worse/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fshvfx/too_much_efficiency_makes_everything_worse/</guid>
      <pubDate>Sun, 29 Sep 2024 23:32:16 GMT</pubDate>
    </item>
    <item>
      <title>来自游戏屏幕的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fs8zxa/reinforcement_learning_model_from_gamescreen/</link>
      <description><![CDATA[您好，我不知道这是否是它的正确子版块，但我对强化学习有一个问题。我知道模型需要状态来确定动作。但对于像 Pokémon 这样的游戏，我实际上无法获得状态。所以我想知道游戏屏幕是否可以用作状态。理论上应该可以，也许我需要手动从屏幕中提取关键信息并创建该状态。但我想避免这种情况，因为我希望模型能够发挥 Pokémon 的两个方面，即探索和战斗。 我正在考虑的第二个问题是，每当模型执行某项操作时，我将如何确定要给予的奖励时间和金额。由于我没有从游戏中获取任何数据，所以我不知道它何时赢得战斗，也不知道当它的神奇宝贝生命值较低时它会何时治愈它们。 由于我对机器学习没有那么多经验，几乎没有，所以我开始怀疑这是否有可能。有人可以对这个想法发表意见并给我一些指点吗？我很想了解更多，但我找不到一个好的起点。    提交人    /u/tirodokter   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fs8zxa/reinforcement_learning_model_from_gamescreen/</guid>
      <pubDate>Sun, 29 Sep 2024 16:56:47 GMT</pubDate>
    </item>
    <item>
      <title>单步情节的 RL（连续空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fs8ixh/rl_for_single_step_episodes_continuous_spaces/</link>
      <description><![CDATA[大家好。我目前正在从事一个与控制图参数自动调整相关的项目。其中重要的部分是，我正在处理连续有界空间，包括观察和动作，但最重要的是，我当前的实现依赖于单步情节，或者更好的是连续的一个 0 步骤和一个实际步骤：代理向系统提供一个身份图只是为了获得一个观察（可能会有所不同，因此它不是固定的初始条件），它选择一个动作（参数向量），获得奖励并结束情节。 目前我正在使用 PPO 作为商品，但我相信有更适合的方法来解决这样的问题。有什么建议吗？    提交人    /u/Krnl_plt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fs8ixh/rl_for_single_step_episodes_continuous_spaces/</guid>
      <pubDate>Sun, 29 Sep 2024 16:36:03 GMT</pubDate>
    </item>
    <item>
      <title>策略梯度定理和 TRPO/PPO 之间没有联系？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fs80xm/no_link_between_policy_gradient_theorem_and/</link>
      <description><![CDATA[您好， 我写这篇文章只是为了确认一些事情。 许多深度强化学习资源都遵循经典的解释路径，即介绍策略梯度定理，并应用它推导出一些最基本的策略梯度算法，如简单策略梯度、REINFORCE、带基线的 REINFORCE 和 VPG 等等。（例如 Spinning Up） 然后，他们使用不同的目标进入 TRPO/PPO 算法。我们是否清楚，TRPO 和 PPO 算法根本不使用策略梯度定理？而且，甚至不使用相同的目标？ 我认为这经常被忽视。 注意：本文（近端策略梯度https://arxiv.org/abs/2010.09933）在 VPG 上应用了与 PPO 相同的剪辑思想。    提交人    /u/alexandretorres_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fs80xm/no_link_between_policy_gradient_theorem_and/</guid>
      <pubDate>Sun, 29 Sep 2024 16:14:21 GMT</pubDate>
    </item>
    </channel>
</rss>