<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 15 Mar 2024 09:13:06 GMT</lastBuildDate>
    <item>
      <title>监督学习与离线强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</link>
      <description><![CDATA[我从 RL 开始，这些可能是非常琐碎的问题，但我想尽我所能地解决所有问题。如果您有任何资源可以为强化学习应用提供良好的直觉，也请在评论中提供：）谢谢。 问题：  我们在哪些场景中使用与离线强化学习相比，更喜欢监督学习？ 样本数量如何影响每个案例的训练？监督学习收敛得更快吗？ 有哪些例子可以同时使用和比较它们进行比较分析？  直觉：  监督学习可以很好地预测给定状态的奖励，但我们不能依赖它来最大化未来的奖励。由于它不使用推出来最大化奖励，并且不进行规划，因此我们不能期望在预期延迟奖励的情况下使用它。 此外，在非独立同分布的动态环境中，每个动作都会影响状态，然后影响进一步采取的动作。因此，对于连续设置，我们在大多数情况下考虑了 RL 的分布变化。 监督学习尝试为每个状态找到最佳动作，这在大多数情况下可能是正确的，但它是一个非常好的方法。针对不断变化的环境采取僵化而愚蠢的方法。强化学习可以自我学习，并且适应性更强。  对于答案，如果可能的话，提供单行，然后任何细节和答案来源也将不胜感激。我希望这篇文章能够为任何尝试应用强化学习的人提供一个很好的指南。我将编辑和更新下面回答的任何问题的答案，以汇总我获得的所有信息。如果您认为我应该考虑任何其他重大问题和疑虑，也请提及。谢谢！   由   提交 /u/StwayneXG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bf6fhq/supervised_learning_vs_offline_reinforcement/</guid>
      <pubDate>Fri, 15 Mar 2024 05:17:56 GMT</pubDate>
    </item>
    <item>
      <title>对于较小的网络来说表示学习值得吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bey25b/is_representation_learning_worth_it_for_smaller/</link>
      <description><![CDATA[我阅读了很多关于表示学习作为实际 RL 任务预训练的文献。我目前正在处理顺序传感器数据作为输入。因此，很多数据都是冗余且嘈杂的。因此，代理需要首先从原始输入时间序列中学习语义特征。  由于强化学习中奖励的梯度信号与无监督学习过程相比非常弱，我认为对特征编码器（又名表示学习）进行无监督预训练是值得的。  现在几乎所有的文献都在处理巨大的神经网络的比较和巨大的数据集。我正在处理大约 200k-1M 个参数和大约 1M 个可用于预训练的样本。  我的问题是：当人工神经网络相对较小时，是否值得进行预训练？我的 RL 训练时间目前约为 60 小时，我希望通过预训练大幅缩短训练时间。    由   提交/u/flxh13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bey25b/is_representation_learning_worth_it_for_smaller/</guid>
      <pubDate>Thu, 14 Mar 2024 22:28:54 GMT</pubDate>
    </item>
    <item>
      <title>迁移学习的成功例子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bept24/successful_examples_of_transfer_learning/</link>
      <description><![CDATA[有谁知道是否有任何论文讨论/成功地将某种迁移学习从一项强化学习任务应用到相关任务？例如，如果我有一个迷你网格世界，代理接受了导航培训，然后将其移动到类似的迷你网格，但现在将块推入目标位置。或者说这种事情大部分都是通过多任务学习来完成的？    由   提交 /u/Impallion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bept24/successful_examples_of_transfer_learning/</guid>
      <pubDate>Thu, 14 Mar 2024 16:50:11 GMT</pubDate>
    </item>
    <item>
      <title>“为什么效应定律不会消失”，Dennett 1974（基于模型的强化学习的演变）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bem6z4/why_the_law_of_effect_will_not_go_away_dennett/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bem6z4/why_the_law_of_effect_will_not_go_away_dennett/</guid>
      <pubDate>Thu, 14 Mar 2024 14:15:41 GMT</pubDate>
    </item>
    <item>
      <title>2024 年暑期学校</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1belslq/summer_schools_for_2024/</link>
      <description><![CDATA[您好，我正在寻找 2024 年一些好的 RL 暑期学校，但我发现它对不同的可能性有点不知所措。  这里有人有任何经验/建议吗？    由   提交 /u/IAmNotMarcus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1belslq/summer_schools_for_2024/</guid>
      <pubDate>Thu, 14 Mar 2024 13:58:04 GMT</pubDate>
    </item>
    <item>
      <title>任何人都想购买我的课程学术帐户。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bekp5h/any_body_wants_to_buy_my_coursea_academic_account/</link>
      <description><![CDATA[      您可以免费注册任何课程。我想以 9000 印度卢比的终身价格出售它    由   提交/u/Efficient_Ambition34  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bekp5h/any_body_wants_to_buy_my_coursea_academic_account/</guid>
      <pubDate>Thu, 14 Mar 2024 13:05:03 GMT</pubDate>
    </item>
    <item>
      <title>“如何生成和使用合成数据进行微调”，Eugene Yan</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be20hu/how_to_generate_and_use_synthetic_data_for/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be20hu/how_to_generate_and_use_synthetic_data_for/</guid>
      <pubDate>Wed, 13 Mar 2024 20:34:15 GMT</pubDate>
    </item>
    <item>
      <title>Boid植绒环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be1j8p/boid_flocking_environment/</link>
      <description><![CDATA[我有一个自定义的 Boid 植绒环境在 OpenAI Gym 中使用来自 StableBaselines3 的 PPO。我希望它能够实现聚集。 问题： 我试图让 boids 聚集，但尽管有合理的距离，但它们不会一起移动，并且我的损失范围是 10e^4 - 10e^5。  我的方法： 我有两个奖励函数，一个检查凝聚力和分离，另一个检查对齐，如果群体远离邻居并给予奖励，它们就会受到惩罚-100 奖励，否则奖励本质上是标量。 训练中的情节在发生碰撞等时重置。 视频： 错误行为 奖励功能：  def Reward1(self, agent, neighbor_velocities): out_of_flock=False Total_reward=0 multiplier=(len(neighbor_velocities)) if (len(neighbor_velocities) &gt; 0):average_velocity = np.mean(neighbor_velocities) , axis=0)desired_orientation=average_velocity-agent.velocityorientation_diff=np.arctan2(desired_orientation[1],desired_orientation[0])-np.arctan2(agent.velocity[1],agent.velocity[0])#距离如果方向_差异&gt; np.pi：orientation_diff -= 2 * np.pi eliforientation_diff &lt; 0:orientation_diff += 2*np.pitotal_reward=100*乘数*(1-np.abs(orientation_diff))elif(len(neighbor_velocities)==0):out_of_flock=Truetotal_reward-=100returntotal_reward,out_of_flockdefReward2 (self,agent,neighbor_positions):total_reward=0 multiplier=(len(neighbor_positions)) if (len(neighbor_positions)&gt;0):对于neighbor_positions中的neighbor_position:distance = np.linalg.norm(agent.position - neighbor_position) if (distance  完整代码：代码 如有任何建议，我们将不胜感激。我应该如何重新制定我的奖励函数等？ ​   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be1j8p/boid_flocking_environment/</guid>
      <pubDate>Wed, 13 Mar 2024 20:15:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 rllib 的感觉如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1be12gr/how_it_feels_using_rllib/</link>
      <description><![CDATA[     &lt; td&gt; 由   提交 /u/rl_is_best_pony   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1be12gr/how_it_feels_using_rllib/</guid>
      <pubDate>Wed, 13 Mar 2024 19:56:42 GMT</pubDate>
    </item>
    <item>
      <title>批量强化学习帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdzp2z/batch_reinforcement_learning_help/</link>
      <description><![CDATA[ 由   提交/u/pokes41  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdzp2z/batch_reinforcement_learning_help/</guid>
      <pubDate>Wed, 13 Mar 2024 19:01:53 GMT</pubDate>
    </item>
    <item>
      <title>连续 Alphazero 算法的状态和有效性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdvt39/state_and_effectiveness_of_continuous_alphazero/</link>
      <description><![CDATA[您好， 我有兴趣在具有连续状态和动作空间的环境中使用基于 MCTS 的 RL 算法。 Moerland 等人在 2018 年针对此设置提出了 Alphazero 的变体。该论文仅获得 61 次引用（大部分来自评论） ，所以看起来这并没有被广泛采用。  所以我想知道是否有人尝试过这个算法，或者知道其他连续动作空间的方法。    由   提交/u/Playmad37  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdvt39/state_and_effectiveness_of_continuous_alphazero/</guid>
      <pubDate>Wed, 13 Mar 2024 16:30:26 GMT</pubDate>
    </item>
    <item>
      <title>从像素中学习。你预训练 CNN 了吗？你知道好的 3x3 过滤器吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdsoxm/learning_from_pixels_do_you_pretrain_cnn_do_you/</link>
      <description><![CDATA[亲爱的社区， 最近我开始使用 RGB 相机的灰度像素 (32x32) 来增强状态。为什么我不愿意这样做-&gt;因为 State 可能是冗余的，并且可能需要训练 CNN 网络，这会使 State 出错且不稳定。 如果我训练 CNN 网络，那么 State 一开始就明显有噪声。你训练它吗？ 如果我决定不训练它：那么我需要过滤器。我知道 2 边缘 3x3 过滤器值。但我需要更多， 如果深度是(1,4,8)那么我需要4+8=12个过滤器。 我清楚地理解为什么CNN这么好，因为它掌握像素之间的局部关系。 除了这些过滤器之外，您还有好的过滤器吗： [[1, 0, -1], [0 , 0, 0], [-1, 0, 1]] [[0, 1, 0], [1, -4 , 1], [0, 1, 0]] ​   由   提交/u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdsoxm/learning_from_pixels_do_you_pretrain_cnn_do_you/</guid>
      <pubDate>Wed, 13 Mar 2024 14:22:25 GMT</pubDate>
    </item>
    <item>
      <title>内存学习：大型语言模型的声明式学习框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdoco3/inmemory_learning_a_declarative_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.02757 摘要：  探索智能体是否可以在不依赖环境的情况下与环境保持一致人类标记的数据提出了一个有趣的研究主题。从智能生物体中观察到的对齐过程中汲取灵感，其中陈述性记忆在总结过去的经验中发挥着关键作用，我们提出了一种新颖的学习框架。代理熟练地从过去的经验中提取见解，完善和更新现有笔记，以提高他们在环境中的表现。整个过程发生在记忆组件内，并通过自然语言实现，因此我们将这个框架描述为内存学习。我们还深入研究了旨在评估自我改进过程的基准的主要特征。通过系统的实验，我们证明了我们的框架的有效性，并提供了对该问题的见解。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdoco3/inmemory_learning_a_declarative_learning/</guid>
      <pubDate>Wed, 13 Mar 2024 10:40:29 GMT</pubDate>
    </item>
    <item>
      <title>用代码分享我的强化学习教程系列。从基础知识开始，使用 PyTorch 中的 PPO 扩展到深度强化学习。让我知道你的想法！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdfxz2/sharing_my_tutorial_series_on_reinforcement/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交 /u/MrSirLRD   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdfxz2/sharing_my_tutorial_series_on_reinforcement/</guid>
      <pubDate>Wed, 13 Mar 2024 02:17:57 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>