<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 20 Jan 2024 21:11:55 GMT</lastBuildDate>
    <item>
      <title>采访麻省理工学院林肯实验室的 Zack Serlin：正式方法......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bkd00/interview_with_zack_serlin_mit_lincoln/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bkd00/interview_with_zack_serlin_mit_lincoln/</guid>
      <pubDate>Sat, 20 Jan 2024 19:58:50 GMT</pubDate>
    </item>
    <item>
      <title>DQN 代理奖励向后断言错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bihrt/dqn_agent_reward_backward_assertion_error/</link>
      <description><![CDATA[我正在学习强化学习并尝试从论文中复制模型；目标是控制（1x 连续动作）1/4 汽车悬架系统并最大限度地减少随机路面上的悬架行程。我正在使用 keras.rl2 中的深度 Q 网络。 我将代码上传到 github：https://github.com/htmdn/QuarterCarSuspControl/blob/main/DDPG_Susp_Control_02.ipynb 这是我收到的错误：  断言错误回溯（最近一次调用最后）单元格In[16]，第1行----&gt; 1 dqn.fit(env, nb_steps=50000, Visualize=False, verbose=1) 文件C:\apps\anaconda3\envs\x\lib\site-packages \rl\core.py:193，在 Agent.fit(self、env、nb_steps、action_repetition、回调、详细、可视化、nb_max_start_steps、start_step_policy、log_interval、nb_max_episode_steps) 190 if nb_max_episode_steps and Episode_step &gt;= nb_max_episode_steps - 1: 191 # 强制进入终止状态。 192 完成 = 正确 --&gt; 193 指标= self.backward（奖励，终端=完成） 194 Episode_reward +=奖励 196 步骤_logs = { 197 &#39;action&#39;：行动， 198 &#39;观察&#39;：观察，（...） 202 &#39;info&#39;:cumulative_info, 203 } 文件 C:\apps\anaconda3\envs\x\lib\site-packages\rl\agents\dqn.py:271，位于 DQNAgent.backward 中(self,reward,terminal) 269terminal1_batch = np.array(terminal1_batch) 270reward_batch = np.array(reward_batch) --&gt; 271 断言reward_batch.shape == (self.batch_size,) 272 断言terminal1_batch.shape ==reward_batch.shape 273 断言 len(action_batch) == len(reward_batch)   非常感谢任何反馈！   由   提交 /u/htmdn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bihrt/dqn_agent_reward_backward_assertion_error/</guid>
      <pubDate>Sat, 20 Jan 2024 18:38:05 GMT</pubDate>
    </item>
    <item>
      <title>MuDreamer：无需重建即可学习预测世界模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bdlml/mudreamer_learning_predictive_world_models/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=9pe38WpsbX 摘要：  DreamerV3 代理最近展示了状态 -在不同领域中表现最先进，使用像素重建损失在潜在空间中学习强大的世界模型。然而，虽然重建损失对于 Dreamer 的性能至关重要，但它也需要对不必要的信息进行建模。因此，梦想家有时无法感知解决任务所需的关键要素，从而极大地限制了其潜力。在本文中，我们提出了 MuDreamer，这是一种基于 DreamerV3 算法的强化学习代理，通过学习预测世界模型而无需重建输入信号。隐藏表示不是依赖于像素重建，而是通过预测环境值函数和先前选择的动作来学习。与图像的预测自监督方法类似，我们发现批量归一化的使用对于防止学习崩溃至关重要。我们还研究了模型后验损失和先验损失之间的 KL 平衡对收敛速度和学习稳定性的影响。我们在广泛使用的 DeepMind Visual Control Suite 上评估 MuDreamer，并获得与 DreamerV3 相当的性能。 MuDreamer 还在 Atari100k 基准测试中展示了可喜的结果。研究代码将公开。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bdlml/mudreamer_learning_predictive_world_models/</guid>
      <pubDate>Sat, 20 Jan 2024 15:00:47 GMT</pubDate>
    </item>
    <item>
      <title>在首次达到目标状态之前，具有优势归一化的 PPO 如何在 MountainCar-v0 中学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19b6ivo/how_does_ppo_with_advantage_normalization_learn/</link>
      <description><![CDATA[我试图首先弄清楚 PPO 如何在像体育馆的 MountainCar-v0 这样的稀疏环境中学习任何东西。曾经达到目标状态。 特别关注 stable_baselines3 对 PPO 的实现 env = make_vec_env(&#39;MountainCar-v0&#39;, n_envs=16) model = PPO(&#39; MlpPolicy&#39;、env、verbose=1、learning_rate=1e-3、gamma=0.99、gae_lambda=0.98、ent_coef=0.0、n_steps=16、normalize_advantage=True)  我进行了不同的实验并在环境首先达到目标状态时记录。 在上述设置中，通常会在大约 50-150k 时间步内首先达到目标状态。 我进行了一个单独的实验，其中我只是随机选择每一步的操作（因此没有进行“学习”），并且它基本上永远不会达到目标状态（在 200 步情节限制内）。如果学习率设置为 0（仅模仿随机动作），情况也是如此，因此似乎正在进行某种学习。 此外，当 n_envs 设置为 1 或 normalize_advantage 时如果它看到的每个状态都会给出相同的奖励（-1），我很困惑 PPO 在第一次达到目标状态之前如何学习任何东西。我在 MountainCar-v0 中没有看到任何奖励塑造，并且在 PPO 实现中没有看到任何好奇心。 我错过了什么？谢谢   由   提交/u/happysushi2  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19b6ivo/how_does_ppo_with_advantage_normalization_learn/</guid>
      <pubDate>Sat, 20 Jan 2024 07:42:51 GMT</pubDate>
    </item>
    <item>
      <title>Lunai：Lunai是一款无代码、简单易用的GUI、强化学习Ai</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19b4y35/lunai_lunai_is_a_codefree_simple_and_easy_to_use/</link>
      <description><![CDATA[   /u/Feralzi  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19b4y35/lunai_lunai_is_a_codefree_simple_and_easy_to_use/</guid>
      <pubDate>Sat, 20 Jan 2024 06:02:30 GMT</pubDate>
    </item>
    <item>
      <title>试验自定义游戏环境算法的最佳实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19az7f4/best_practice_for_experimenting_with_algorithms/</link>
      <description><![CDATA[我是个 RL 菜鸟。我的目标是创建一个信息不完善的多人棋盘游戏环境，并训练代理在其中玩游戏。 我应该遵循哪些最佳实践？我应该从头开始实现所有逻辑吗？我可以实现哪些库和接口以获得更连贯的体验并学习使用 RL 中使用的规范包？   由   提交/u/fool126  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19az7f4/best_practice_for_experimenting_with_algorithms/</guid>
      <pubDate>Sat, 20 Jan 2024 00:59:28 GMT</pubDate>
    </item>
    <item>
      <title>对我的项目概述的反馈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19aokgw/feedback_on_my_project_overview/</link>
      <description><![CDATA[我正在编写项目的高级描述。我不需要深入思考技术实现，但我需要对有意义的实现有一个概述。请指导我应该添加、修改或更改哪些内容。  我也不知道哪种学习方法最适合这里。  这是我到目前为止所写的内容： \section{强化学习} \subsection{概述} 强化学习从根本上围绕代理通过接收以下形式的反馈进行学习：奖励或惩罚，在环境中做出决策以实现预定目标。就我们而言，我们的目标是在整个城市战略性地放置自行车共享站。为了实现强化学习模型，我们需要从状态、动作和奖励的角度来阐述我们的问题。代表我们模型的代理将学习决策，而环境就是城市。放置站点的最佳位置的特点是人口密度高和站点之间有足够的间距等因素；这两个限制都意味着将车站放置在对公众最有利的地方。 \subsection{潜在的实现} 城市，我们的环境，被描述为由单元组成的网格地图布局。每个单元格将： \begin{enumerate} \item 为空 \item 为自行车站 \item 其他占用空间（建筑物、住宅等） \item 为道路 \end{enumerate} 单元格类型 1-3 将有一个相关值代表附近的人口密度，而单元格类型 4 表示附近是否有共享单车站点。代理的动作空间由三个离散的动作组成：放置新的自行车站、删除现有的自行车站或选择不放置。这些动作可以表示为元组（x，y，放置/删除/传递），其中x和y表示所选位置的坐标。奖励结构必须准确考虑不同的情况，才能有效地学习。根据附近位置的高人气、靠近兴趣点（企业或景点）以及靠近自行车道，放置站点会获得积极的奖励。如果站点与其他站点距离太近，则会受到处罚。每个行动都会附带一个小的负面奖励，以阻止过度不必要的行动。地图将随着模型交互而修改，模拟放置自行车站后的变化。  ​   由   提交 /u/obvslynot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19aokgw/feedback_on_my_project_overview/</guid>
      <pubDate>Fri, 19 Jan 2024 17:31:16 GMT</pubDate>
    </item>
    <item>
      <title>我想知道是否有一个考虑时间维度的策略/价值函数？例如，在时间 t 处于状态 s 的价值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19amupa/i_am_wondering_if_there_is_a_policyvalue_function/</link>
      <description><![CDATA[ 由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19amupa/i_am_wondering_if_there_is_a_policyvalue_function/</guid>
      <pubDate>Fri, 19 Jan 2024 16:20:40 GMT</pubDate>
    </item>
    <item>
      <title>“受行为塑造启发的课程学习训练神经网络采用类似动物的决策策略”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19am4sp/curriculum_learning_inspired_by_behavioral/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2024.01.12.575461 摘要：  经常性神经网络 (RNN) 在神经科学中广泛使用，用于捕获神经动力学和生命系统的行为。然而，当涉及复杂的认知任务时，传统的 RNN 训练方法可能无法捕捉动物行为的关键方面。为了应对这一挑战，我们利用了实验神经科学家工具包中常用的（尽管很少受到赞赏）的方法：行为塑造。以之前在老鼠身上研究的时间投注任务为目标，我们设计了一个包含更简单认知任务的预训练课程，这是良好执行该任务的先决条件。这些预训练任务不是时间投注任务的简化版本，而是反映了相关的子计算。我们表明，这种方法对于 RNN 来说是采取与大鼠类似的策略所必需的，包括对潜在状态进行长时间尺度的推断，而传统的预训练方法无法捕获这些策略。从机制上讲，我们的预训练支持实现推理和基于价值的决策所需的关键动力系统功能的开发。总的来说，我们的方法通过结合动物的归纳偏差来解决神经网络模型训练中的差距，这在对依赖于过去经验获得的计算能力的复杂行为进行建模时非常重要。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19am4sp/curriculum_learning_inspired_by_behavioral/</guid>
      <pubDate>Fri, 19 Jan 2024 15:50:45 GMT</pubDate>
    </item>
    <item>
      <title>本文如何将两项政策结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</link>
      <description><![CDATA[      我正在阅读这篇论文，我对算法 1 中的一些细节有点迷失 -  ​ ​ https://preview.redd.it/ f0hhmaye8edc1.png?width=845&amp;format=png&amp;auto=webp&amp;s=a99793f4a5bc099c1a165145aeb44e4db441d95c 在第 3 行中，它们似乎通过 $h 组合了 $h$ 和 $f$ (s) = \pi(s) + f(s)$。我不明白这是怎么发生的。  他们在本节中称 $h$ 为混合策略，但我不明白 -  ​ https://preview.redd.it/4flfgn1x8edc1.png?width=851&amp;format =png&amp;auto=webp&amp;s=a2108806ba92d46e9afcd7668dc9c78ee58120e9 ​ 如果需要任何说明，请告诉我。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</guid>
      <pubDate>Fri, 19 Jan 2024 12:52:43 GMT</pubDate>
    </item>
    <item>
      <title>[需要建议/反馈]第2部分：决斗双倍DQN奖励大多在0和2之间波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</guid>
      <pubDate>Fri, 19 Jan 2024 09:19:09 GMT</pubDate>
    </item>
    <item>
      <title>RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a9nli/rl/</link>
      <description><![CDATA[大家好 我想你会发现这篇文章很愚蠢。但我没有NN专业背景。我了解反向传播的基础知识以及有关神经网络的其他简单知识，我编写简单的神经网络进行函数预测。我在基本级别上也了解 RL 应该如何工作。为了好玩，我尝试使用 chatGPT 与 RL 代理创建基本游戏，该游戏使用输入作为整个屏幕。 我的游戏是两个 2d 玩家（圆圈）可以旋转、前进并发射射弹。寻找良好的奖励函数有点困难。因为如果我写下仅通过命中和惩罚来获得奖励，当射弹击中我时，我会错过特工所做的很多动作，而且我不知道如何奖励它。我想我错过了一些重要的事情。也许我应该记录几个动作并将它们奖励为 1 个动作。当射弹击中我时的惩罚是过去几次行动的结果，所以仅仅惩罚最后一个行动看起来是不正确的。  我的项目中有3个文件，如果你愿意的话可以查看ofc。但一般建议也会非常有帮助。  main.py（主游戏循环逻辑） https://pastebin.com/cLm8DC5g&lt; /p&gt; ​ DRLAgent.py。给我写信的逻辑是ChatGPT。我理解代码的概念，但我不明白什么对我的任务有好处，什么对我的任务没有好处。 https://pastebin.com /ST3Pfkk8 ​ DQNCNN.py。也是由 chatGPT 编写的。我完全不明白这段代码。我只知道这是我的屏幕输入的调用层，但我不知道转换层是如何工作的。 https://pastebin.com/ RXfB0meN ​ 最后一个愚蠢的问题。我有两个由同一个代理控制的玩家，我给他们屏幕框架和两个额外的功能，具体取决于哪个玩家 RL 尝试预测更好的动作 [1.0, 0.0] 或 [0.0, 1.0]。我希望他能找到这 2 个功能和屏幕上的颜色之间的依赖关系（因为 2 个玩家有不同的颜色），但我也认为这是一个糟糕的解决方案。 我应该给 RL 额外的信息，比如屏幕上的玩家位置和玩家回转？  非常感谢您的关注。 ​ ​ ​ ​   由   提交/u/Specialist_Soup_4994   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a9nli/rl/</guid>
      <pubDate>Fri, 19 Jan 2024 03:42:26 GMT</pubDate>
    </item>
    <item>
      <title>专门在密室中培训 MiniGrid 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a7mfw/training_minigrid_agents_specifically_the/</link>
      <description><![CDATA[大家好， 我目前正在尝试专门解决 MiniGrid 环境： https://minigrid.farama.org/environments/minigrid/LockedRoomEnv/  这个环境非常稀疏，我一直在尝试用 PPO 解决这个问题，尝试了不同的网络和超参数调整，但没有成功。 是否有人已经解决了这个问题或者知道如何解决它？ p&gt; 另外我想知道是否有学习实用深度强化学习的资源（理论资源很多，但实用性不高）。 谢谢！   由   提交/u/Key_Lie_7975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a7mfw/training_minigrid_agents_specifically_the/</guid>
      <pubDate>Fri, 19 Jan 2024 02:01:49 GMT</pubDate>
    </item>
    <item>
      <title>MARL 逐帧持续学习（格斗游戏研究）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a19ck/frame_by_frame_continuous_learning_for_marl/</link>
      <description><![CDATA[你好！ ​ 我和我的朋友正在研究如何使用 MARL格斗游戏的上下文，其中演员/代理同时提交输入，然后由格斗游戏物理引擎解析。有许多论文在格斗游戏的背景下讨论 DL / RL / 一些 MARL，但值得注意的是，它们不包含源代码或实际上谈论其方法，而是谈论普遍的发现/见解。 ​ 现在我们正在考虑使用 Pytorch（在 CUDA 上运行以提高训练速度）和 Petting Zoo（MARL 体育馆的扩展），特别是使用 AgileRL 库进行超参数优化。我们很清楚，超参数如此之多，当我们试图改进问题时，知道要改变什么是很棘手的。我们设想，我们有 8 个左右的研究游戏引擎实例（我有 10 个核心 CPU）连接到 10 个宠物动物园（可能是敏捷 RL 修改版）训练环境的实例，其中输入/输出在引擎和训练环境，来回。 ​ 我想我是在寻求一些关于我们正在使用的工具的一般建议/提示和反馈。如果您知道解决类似问题的特定教科书、GitHub 存储库的研究论文，那可能会非常有帮助。我们有一些关于超参数优化的资源以及一些关于如何摆弄设置的想法，但是仅仅为了进行人工智能学习而设计的项目初始结构/启动代码有点棘手。我们确实有一个 MARL 工作的 Connect 4 训练示例，由 AgileRL 提供。但我们正在寻求将其从轮流输入提交调整为同时输入提交（这当然是可能的，MARL 用于 MOBA 等实时游戏中）。 ​ ​ p&gt; 您可以提供给我们的任何信息都是一种祝福并且很有帮助。非常感谢您抽出时间。    由   提交/u/stardoge42  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a19ck/frame_by_frame_continuous_learning_for_marl/</guid>
      <pubDate>Thu, 18 Jan 2024 21:22:30 GMT</pubDate>
    </item>
    <item>
      <title>TMRL 和 vgamepad 现在可以在 Windows 和 Linux 上运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199ujyt/tmrl_and_vgamepad_now_work_on_both_windows_and/</link>
      <description><![CDATA[亲爱的社区，您好， 你们中的一些人要求我使这些库与 Linux 兼容，并在我们的帮助下我们刚刚做了伟大的贡献者。 对于那些不熟悉的人，tmrl 是一个开源项目面向机器人专家的强化学习框架，因为它支持对数据管道的实时控制和细粒度控制，在自动驾驶社区中因其在 TrackMania2020 视频游戏中基于视觉的管道而闻名。另一方面，vgamepad 是支持此应用程序中游戏手柄模拟的开源库，它可以模拟 Xbox 360 和 PS4 Python 中的游戏手柄适合您的应用程序。 Linux 支持刚刚推出，我真的很想找到测试人员和新的贡献者来改进它，特别是对于“vgamepad”，它并不支持 Windows 版本的所有功能在 Linux 中还没有。如果您有兴趣贡献...请加入:)   由   提交 /u/yannbouteiller   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199ujyt/tmrl_and_vgamepad_now_work_on_both_windows_and/</guid>
      <pubDate>Thu, 18 Jan 2024 16:48:54 GMT</pubDate>
    </item>
    </channel>
</rss>