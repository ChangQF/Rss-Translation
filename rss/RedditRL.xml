<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 06 Jul 2024 12:25:32 GMT</lastBuildDate>
    <item>
      <title>我需要一些帮助来完成我正在进行的项目，即微尺度的路径规划算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</guid>
      <pubDate>Sat, 06 Jul 2024 11:19:31 GMT</pubDate>
    </item>
    <item>
      <title>您好，我正在使用 DDPG 进行四轴飞行器的轨迹跟踪，根据这个训练图我可以得出什么结论？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwm21c/hello_im_using_ddpg_for_trajectory_tracking_for_a/</link>
      <description><![CDATA[        由    /u/OkFig243   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwm21c/hello_im_using_ddpg_for_trajectory_tracking_for_a/</guid>
      <pubDate>Sat, 06 Jul 2024 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>DQN 在自定义健身环境中无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</link>
      <description><![CDATA[大家好， 我一直想尝试自定义 gym 环境。我使用 tensorflow 用 DQN 解决了​​冰冻湖问题 - 并且想创建一个类似的环境并用 DQN 解决它。 我的环境是一个 4x5 网格单元仓库，里面有一个机器人、一个目标和一个障碍物。所以，与冰冻湖非常相似。 我遇到的问题是，一旦 epsilon 衰减，并且算法正在采取贪婪行动，它采取的行动就是错误的（总是相同的，取决于初始化，但例如，它会告诉机器人始终向上） 这是我尝试过的和我所知道的：  环境应该正常工作。我对其进行了广泛的测试，&amp;用经典 Q 学习解决了这个问题，而且效果很好 我几乎从我的 frosting lake 代码中复制/粘贴了我的 DQN 代码。  我玩过超参数，增加了网络的复杂性，但没有任何效果，而且我对此持怀疑态度，因为我的自定义环境和 frosting lake 之间应该没有太大区别（我想？）  知道我应该在哪里寻找吗？我可以在这里分享代码，但它有点密集，我不确定要分享哪一部分，所以我不想淹没，但如果你想要代码的任何特定部分，我很乐意分享     提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</guid>
      <pubDate>Sat, 06 Jul 2024 08:01:03 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 制作一款回合制策略游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</link>
      <description><![CDATA[        由    /u/Novel_Can_6870  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</guid>
      <pubDate>Sat, 06 Jul 2024 02:23:55 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理的行为正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</link>
      <description><![CDATA[      大家好 我刚刚创建了我的第一个 PPO 代理。在 Cartpole-v1 环境中进行训练时，我收到以下响应： https://preview.redd.it/8kufxuh59sad1.png?width=640&amp;format=png&amp;auto=webp&amp;s=ddf17252bea091b0738973645612f1095364489e 为什么在代理似乎最终收敛时，性能在 500 次迭代后仍会出现一些突然下降的情况？ 谢谢提前。    由    /u/Muscle_Robot   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</guid>
      <pubDate>Fri, 05 Jul 2024 23:13:27 GMT</pubDate>
    </item>
    <item>
      <title>移动平均数对于评估模型性能是否必要</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dw2kxk/is_moving_average_necessary_for_evaluating_model/</link>
      <description><![CDATA[我正在用 Pong 游戏练习 RL！    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dw2kxk/is_moving_average_necessary_for_evaluating_model/</guid>
      <pubDate>Fri, 05 Jul 2024 16:52:05 GMT</pubDate>
    </item>
    <item>
      <title>这篇应用范畴论论文“范畴控制论中的强化学习”在该领域有用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dw159f/is_this_applied_category_theory_paper/</link>
      <description><![CDATA[偶然看到了这篇论文。这篇论文对 RL 有用或相关吗？ https://arxiv.org/pdf/2404.02688    提交人    /u/vent-doux   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dw159f/is_this_applied_category_theory_paper/</guid>
      <pubDate>Fri, 05 Jul 2024 15:49:41 GMT</pubDate>
    </item>
    <item>
      <title>Mamba 背后的思想：Albert Gu 谈论状态空间模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</link>
      <description><![CDATA[        由    /u/phoneixAdi  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</guid>
      <pubDate>Fri, 05 Jul 2024 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>赛车开放式 AI 健身房 - 推出是否应覆盖整个赛道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</link>
      <description><![CDATA[我试图复制此代码，但对 rollout 生成感到非常困惑。 步骤 4：生成随机 rollout 对于赛车环境，VAE 和 RNN 都可以使用随机 rollout 数据 - 即通过在每个时间步随机采取行动而生成的观察数据。实际上，我们使用伪随机动作，迫使汽车最初加速，以使其离开起跑线。 每次推出都应该覆盖整个赛道吗？ https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459 我一直在尝试运行代码，但推出后赛道从未完成。训练后我也看不到它完成。    提交人    /u/Competitive-Chip5872   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</guid>
      <pubDate>Fri, 05 Jul 2024 06:15:32 GMT</pubDate>
    </item>
    <item>
      <title>针对多个 TSP/VRP/和其他变体的多智能体强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</link>
      <description><![CDATA[你好，我是一名最近开始进行 RL 研究的学生。 我对包括 CO 和 MARL 在内的领域很感兴趣。即使有很多好的 RL 和启发式算法，我也想在更大规模和更复杂的领域尝试一些实验。 但是，对于多旅行商问题或 VRP，我很难找到 SOTA MARL 方法。（我在 github 上找到了一些论文，但通常没有代码实现） 除了论文之外，如果你能分享你的知识和见解，那将非常有帮助。    提交人    /u/QingdaoCraftBeer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</guid>
      <pubDate>Fri, 05 Jul 2024 04:57:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Gym 训练动作分类模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvoovw/using_gymnasium_to_train_an_action_classification/</link>
      <description><![CDATA[在任何人说之前，我明白这不是 RL 问题，谢谢。但我必须提到，我是团队的一员，我们都在尝试不同的方法，而我得到了这个。 首先，下面是我的代码： # 乒乓球课的自定义健身环境 TableTennisEnv(gym.Env): def __init__(self, frame_tensors, labels, frame_size=(3, 30, 180, 180)): super(TableTennisEnv, self).__init__() self.frame_tensors = frame_tensors self.labels = labels self.current_step = 0 self.frame_size = frame_size self.n_actions = 20 # 唯一动作的数量 self.observation_space = space.Box(low=0, high=255, shape=frame_size, dtype=np.float32) self.action_space = space.Discrete(self.n_actions) self.normalize_images = False self.count_reset = 0 self.count_step = 0 def reset(self, seed=None): 全局 total_reward, maximum_reward self.count_reset += 1 print(&quot;Reset called: &quot;, self.count_reset) self.current_step = 0 total_reward = 0 maximum_reward = 0 return self.frame_tensors[self.current_step], {} def step(self, action): 全局 total_reward, maximum_reward act_ten = torch.tensor(action, dtype=torch.int8) 如果 act_ten == self.labels[self.current_step]: 奖励 = 1 total_reward += 1 else: 奖励 = -1 total_reward -= 1 maximum_reward += 1 print(&quot;实际： &quot;, self.labels[self.current_step]) print(&quot;预测： &quot;, action) self.current_step += 1 print(&quot;步骤： &quot;, self.current_step) done = self.current_step &gt;= len(self.frame_tensors) obs = self.frame_tensors[self.current_step] if not done else np.zeros_like(self.frame_tensors[0]) truncated = False if done: print(&quot;最大奖励： &quot;, maximum_reward) print(&quot;获得的奖励： &quot;, total_reward) print(&quot;准确度： &quot;, (total_reward/maximum_reward)*100) return obs, reward, done, truncated, {} def render(self, mode=&#39;human&#39;): pass # 通过较小的批次处理来减少内存使用量 env = DummyVecEnv([lambda: TableTennisEnv(frame_tensors, labels, frame_size=(3, 30, 180, 180))]) timesteps = 100000 try: # 用较小的批量大小初始化 PPO 模型 model1 = PPO(&quot;MlpPolicy&quot;, env, verbose=1, learning_rate=0.03, batch_size=5, n_epochs=50, n_steps=4, tensorboard_log=&quot;./ppo_tt_tensorboard/&quot;) # 训练模型 model1.learn(total_timesteps=timesteps) # 保存训练好的模型 model1.save(&quot;ppo_table_tennis_3_m1_MLP&quot;) print(&quot;模型 1 训练和保存成功完成。&quot;) tr1 = total_reward mr1 = maximum_reward total_reward = 0 maximum_reward = 0 print(&quot;模型 1 的准确率（100 Epochs）：&quot;, (tr1/mr1)*100) except Exception as e: print(f&quot;模型训练或保存期间发生错误：{e}&quot;)  有 1514 个视频剪辑用于训练，已转换为矢量。每个视频剪辑矢量的尺寸为 (180x180x3)x30，因为我提取了 30 帧作为输入。 问题出现在训练期间。在最初几个步骤中，模型运行良好。过了一会儿，预测的动作停止变化。它只会一遍又一遍地预测 1-20 中的一个数字。我是体育馆库的新手，因此我不确定是什么导致了这个问题。我已经在 StackOverflow 上发布了这个问题，到目前为止我还没有收到太多帮助。 如能得到您的任何意见，我们将不胜感激。谢谢。    提交人    /u/Farenhytee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvoovw/using_gymnasium_to_train_an_action_classification/</guid>
      <pubDate>Fri, 05 Jul 2024 03:40:03 GMT</pubDate>
    </item>
    <item>
      <title>在 Q-Learning 算法中加入专家知识</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvbgts/including_expert_knowledge_in_qlearning_algorithms/</link>
      <description><![CDATA[大家好 :) 您是否有任何经验或知道一些将专家知识纳入深度 q 学习算法的最佳实践方法？ 目前，我考虑先验地在重放缓冲区中包含一些状态、动作和奖励对。这是一种合适的方法吗？    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvbgts/including_expert_knowledge_in_qlearning_algorithms/</guid>
      <pubDate>Thu, 04 Jul 2024 16:48:23 GMT</pubDate>
    </item>
    <item>
      <title>“在 HATETRIS 中创下世界纪录”，Dave & Filipe 2022（AlphaZero 失败后高度优化的光束搜索）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</guid>
      <pubDate>Thu, 04 Jul 2024 16:14:25 GMT</pubDate>
    </item>
    <item>
      <title>“AlphaZero 的蒙特卡洛图搜索”，Czech 等人 2020 年（将树转换为 DAG 以节省空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</guid>
      <pubDate>Thu, 04 Jul 2024 16:01:49 GMT</pubDate>
    </item>
    <item>
      <title>“《赛道狂飙》中的机器学习历史”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dva3yv/the_history_of_machine_learning_in_trackmania/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dva3yv/the_history_of_machine_learning_in_trackmania/</guid>
      <pubDate>Thu, 04 Jul 2024 15:49:37 GMT</pubDate>
    </item>
    </channel>
</rss>