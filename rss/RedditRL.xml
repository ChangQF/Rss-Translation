<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 09 Jan 2024 06:19:23 GMT</lastBuildDate>
    <item>
      <title>限制机器人的适配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19275hp/restricting_the_adaptation_of_robot/</link>
      <description><![CDATA[虽然我希望机器人比人类有所改进，但你看，人类对什么是对的，什么是错的，我们有一定的感觉。尽早定义我们的性格，我们是什么，一旦我们陷入新环境，我们就开始放松我们的性格，开始变得像新环境中的人一样，即使我们的性格与新环境非常相反，但我们开始适应事物这是我们不想要的。这就是为什么（根据我的直觉）逆强化学习并不是一个训练机器人的好主意，如果它们落入我们不希望的新环境中，它就会忘记它的原则，所以我们可以做什么如何使这些机器人在其原理上更加稳健？因为随着人类思维的发展或人类反馈的强化学习的发展，它将被鼓励/奖励去适应环境。如果它的这些原则太强，它就会被迫离开那个环境，因为如果没有任何东西符合它的原则，它就什么也做不了。所以我们希望机器人能够在环境中生存，但不要忘记它的原则。任何直观的答案都可以。   由   提交/u/vyknot4wongs  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19275hp/restricting_the_adaptation_of_robot/</guid>
      <pubDate>Tue, 09 Jan 2024 05:29:42 GMT</pubDate>
    </item>
    <item>
      <title>使用非 MARL 库进行 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1926w72/using_nonmarl_library_for_marl/</link>
      <description><![CDATA[Stable Baselines 3(SB3) 显然不支持 MARL。我正在使用带有 SB3 PPO 的自定义环境，以 CTDE 方法进行 MARL Boid 植绒。  我想知道我的设置是否已在代码中成功实现了 MARL，或者是否存在问题，我需要采用不同的方式来进行。 我的代码&lt; /strong&gt;: Boid 植绒   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1926w72/using_nonmarl_library_for_marl/</guid>
      <pubDate>Tue, 09 Jan 2024 05:14:56 GMT</pubDate>
    </item>
    <item>
      <title>关于使用 LLM 解决顺序控制问题的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1922g57/questions_about_using_llms_for_sequential_control/</link>
      <description><![CDATA[我对法学硕士/基础模型非常陌生。我正在尝试一些开源 LLM 模型，我发现通过直接提示将它们用于类似 RL 的问题非常耗时。 （LLM 的时间步长选定操作大约需要 10 秒）而对于深度强化学习模型，可能需要不到 0.001 秒（？）。 我还没有深入研究它，但我什至想知道如果我使用 API 调用。如果我使用最快、最先进的模型，它会达到与深度强化学习模型相同的速度吗？ （我知道法学硕士非常庞大，是否有可能加快他们的推理速度？） ​   由   提交/u/Blasphemer666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1922g57/questions_about_using_llms_for_sequential_control/</guid>
      <pubDate>Tue, 09 Jan 2024 01:37:23 GMT</pubDate>
    </item>
    <item>
      <title>Lunai 简介 - 无需任何编码的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1921w5e/introducing_lunai_reinforcement_learning_without/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/Feralzi  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1921w5e/introducing_lunai_reinforcement_learning_without/</guid>
      <pubDate>Tue, 09 Jan 2024 01:11:19 GMT</pubDate>
    </item>
    <item>
      <title>导入错误：libmujoco150.so：无法打开共享对象文件：没有这样的文件或目录</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/</link>
      <description><![CDATA[我正在尝试构建一个包含 mujoco 的 Docker 映像。此外，我希望它出现在我的自定义地址中。  ​ 这是我创建的 Dockerfile。我引用了此处使用的环境变量 -  FROM ubuntu:22.04 WORKDIR /app SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;] RUN mkdir -p myhome/house ENV HOME=&quot;/myhome/house:${PATH}&quot; RUN 回显“Hello World！”运行 apt-get update &amp;&amp; apt-get install -y\libosmesa6-dev\sudo\wget\curl\unzip\gcc\g++\&amp;&amp; apt-get install \ libosmesa6-dev \ &amp;&amp; rm -rf /var/lib/apt/lists/* ENV DEBIAN_FRONTEND=非交互式 ENV PATH=&quot;/miniconda3/bin:${PATH}&quot; ARG PATH=“/miniconda3/bin:${PATH}”运行 cd / \ &amp;&amp; mkdir -p /miniconda3 \ &amp;&amp; wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /miniconda3/miniconda.sh \ &amp;&amp; bash /miniconda3/miniconda.sh -b -u -p /miniconda3 \ &amp;&amp; /miniconda3/bin/conda init bash \ &amp;&amp;源 ~/.bashrc \ &amp;&amp; conda init \ &amp;&amp; conda create -y -n myenv python=3.8 \ &amp;&amp; conda update -y conda WORKDIR /~ RUN wget https://roboti.us/download/mjpro150_linux.zip \ &amp;&amp;解压mjpro150_linux.zip \ &amp;&amp; mkdir ~/.mujoco \ &amp;&amp; mv mjpro150 ~/.mujoco \ &amp;&amp; wget https://roboti.us/file/mjkey.txt \ &amp;&amp; mv mjkey.txt ~/.mujoco \ &amp;&amp; rm mjpro150_linux.zip ENV MJLIB_PATH=“/myhome/house/.mujoco/mjpro150/bin/libmujoco150.so:${MJLIB_PATH}” ENV LD_LIBRARY_PATH=“/myhome/house/.mujoco/mjpro150/bin:${LD_LIBRARY_PATH}” ENV MUJOCO_PY_MUJOCO_PATH=“/myhome/house/.mujoco/mjpro150:${MUJOCO_PY_MUJOCO_PATH}” ENV MUJOCO_PY_MJKEY_PATH=“/myhome/house/.mujoco/mjkey.txt:${MUJOCO_PY_MJKEY_PATH}”运行 cd /miniconda3/envs/myenv/lib/ &amp;&amp; mv libstdc++.so.6 libstdc++.so.6.old &amp;&amp; ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6 SHELL [“conda”, “run”, “-n”, “myenv”, “/ bin/bash”，“-c”] EXPOSE 5003 RUN pip install --no-cache-dir “Cython&lt;3” RUN pip install mujoco-py==1.50.1.0  构建不断失败，错误显示在顶部。有人可以帮忙解决这个问题吗？   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191yvwz/importerror_libmujoco150so_cannot_open_shared/</guid>
      <pubDate>Mon, 08 Jan 2024 23:00:50 GMT</pubDate>
    </item>
    <item>
      <title>最佳强化学习研究框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191yu6y/best_rl_research_framework/</link>
      <description><![CDATA[我需要启动一个新的强化学习项目，并问自己哪个强化学习库或框架最适合学术研究。我假设我将使用gymnasium 来构建我需要构建的自定义环境，但我不确定策略（算法）的库。这个想法是能够在自定义环境中切换到几种不同的算法。我过去使用稳定的基线，然后从头开始编写 PPO 实现，并使用了一段时间。现在我想过渡到更灵活的东西，我不必从头开始实现不同的算法。稳定的基线仍然是最好的选择吗？    由   提交 /u/alebrini   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191yu6y/best_rl_research_framework/</guid>
      <pubDate>Mon, 08 Jan 2024 22:59:01 GMT</pubDate>
    </item>
    <item>
      <title>Rich Sutton 的 10 个人工智能口号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191waws/rich_suttons_10_ai_slogans/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191waws/rich_suttons_10_ai_slogans/</guid>
      <pubDate>Mon, 08 Jan 2024 21:16:55 GMT</pubDate>
    </item>
    <item>
      <title>[D] 采访里奇·萨顿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191rmsl/d_interview_with_rich_sutton/</link>
      <description><![CDATA[ 由   提交 /u/atgctg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191rmsl/d_interview_with_rich_sutton/</guid>
      <pubDate>Mon, 08 Jan 2024 18:10:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么奖励价值高于累积奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/191gc9m/why_reward_to_go_values_over_cumulative_rewards/</link>
      <description><![CDATA[嗨，强化学习新手，目前正在研究基于序列建模或基于离线强化学习的方法。当他们使用像架构这样的 GPT 时，我发现他们经常似乎将奖励作为每个时间步骤的代币嵌入之一以及动作和状态，而不是在该时间步骤中累计获得的奖励的天真的奖励-step？ 如果我错了，请纠正我，谢谢！   由   提交 /u/alchemistensei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/191gc9m/why_reward_to_go_values_over_cumulative_rewards/</guid>
      <pubDate>Mon, 08 Jan 2024 08:18:45 GMT</pubDate>
    </item>
    <item>
      <title>机器人课程项目调查！任何经验都有帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</link>
      <description><![CDATA[大家好， 我正在与卡耐基梅隆大学和宾夕法尼亚大学的一些机器人专业学生合作开展一个课堂项目，以研究疼痛学者和行业专业人士在从事机器人开发时面临的问题。如果您从事或认识从事机器人开发流程任何部分的人，并且有 10 分钟的空闲时间，我们将非常感谢您的意见。我们希望获得广泛经验水平的意见。因此，我们重视刚开始接触机器人技术的人们以及具有多年经验的人们的意见。回复是匿名的，绝不反映绩效，因此我们要求您诚实回答。我们计划在 1 月 14 日之前收集回复（但如果调查之后开放，请随时贡献您的想法！）。  https://forms.gle/Mx247TgeDbEydY426 谢谢，   由   提交/u/awkyu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1916pti/robotics_class_project_survey_any_experience_is/</guid>
      <pubDate>Sun, 07 Jan 2024 23:56:05 GMT</pubDate>
    </item>
    <item>
      <title>演奏乐器的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</link>
      <description><![CDATA[寻找任何已知的演奏乐器的模拟环境。例如，一个灵巧的特工在弹吉他。   由   提交 /u/Ultra-Neural   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19136bk/environments_for_playing_instruments/</guid>
      <pubDate>Sun, 07 Jan 2024 21:31:00 GMT</pubDate>
    </item>
    <item>
      <title>这是从模型停止的地方继续训练的正确方法吗？稳定基线3、Pytorch、Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</link>
      <description><![CDATA[嗨， 我正在训练一个模型，昨天我保存并关闭了，因为已经很晚了，我需要睡觉。现在，今天我想从上次停下来的地方继续训练，但谷歌的结果好坏参半，2018 年、19 年、20 年的答案等等。 这是我的代码，如果有人可以确认这是正确的序列，我会很感激。 log_dir = &quot;/path/where/I/want/logs/saved&quot; model_dir =“/path/to/saved/zip/file” env = MyENV() env.reset () model = PPO(&quot;MlpPolicy&quot;, env, verbose = 1,tensorboard_log=log_dir) model .set_parameters(model_path, True) TIMESTEPS = 10000 CONTINUE_BOOKMARK = 35 #最新保存的文件是340000，所以350,000 将是下一个邮政编码... for i in range(CONTINUE_BOOKMARK, 51): model.learn （total_timesteps=TIMESTEPS、reset_num_timesteps=False、tb_log_name=“log_name_here”） model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 我即将运行它，但我担心我可能做得不对，如果它确实有效，那只是巧合。 ​ 编辑： 我最终使用了类似的东西 model.save\(f&quot;{model_dir}/{TIMESTEPS*i}&quot;) ​ env .close() 唯一的事情是，tensorboard 日志看起来没有从之前的日志继续... &lt;!-- SC_ON - -&gt;  由   提交 /u/phantomBlurrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1910ipu/is_this_the_correct_way_to_pick_up_where_the/</guid>
      <pubDate>Sun, 07 Jan 2024 19:44:19 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化分析调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/190qhw5/a_survey_analyzing_generalization_in_deep/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.02349 存储库：https ://github.com/EzgiKorkmaz/generalization-reinforcement-learning 摘要：  强化学习研究取得重大成功和注意力利用深度神经网络来解决高维状态或动作空间中的问题。虽然深度强化学习策略目前被部署在从医疗应用到自动驾驶汽车的许多不同领域，但该领域仍然存在一些关于深度强化学习策略的泛化能力的问题。在本文中，我们将概述深度强化学习策略遇到限制其鲁棒性和泛化能力的过拟合问题的根本原因。此外，我们将形式化和统一不同的解决方案以提高泛化性，并克服状态-动作价值函数的过度拟合。我们相信我们的研究可以为当前深度强化学习的进展提供紧凑、系统的统一分析，并有助于构建具有更高泛化能力的鲁棒深度神经策略。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/190qhw5/a_survey_analyzing_generalization_in_deep/</guid>
      <pubDate>Sun, 07 Jan 2024 11:49:27 GMT</pubDate>
    </item>
    <item>
      <title>如何获得 AI/ML 和强化学习方面的经验来担任研究职位？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/190l2rc/how_to_get_experience_in_aiml_and_reinforcement/</link>
      <description><![CDATA[您好，我是一名计算机科学专业的新生，对 AI/ML 研究非常感兴趣，尤其是强化学习。我想向教授寻求研究机会，但我没有太多经验可以展示。我已经完成了一些在线课程，阅读了教科书等，但除了我完成了一些编码作业作为其中的一部分之外，我没有什么可以展示的。您对我可以做些什么来获得强化学习经验有什么建议吗？我可以向教授展示这些经验，以证明我已经准备好在他们的实验室进行研究？我一直在考虑从头开始实现一些论文和/或做一些涉及机器学习的副项目。这是一个好的起点吗？   由   提交/u/meemaowie  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/190l2rc/how_to_get_experience_in_aiml_and_reinforcement/</guid>
      <pubDate>Sun, 07 Jan 2024 05:46:30 GMT</pubDate>
    </item>
    <item>
      <title>没有的极限是什么？ PPO 中的观察结果是否有助于良好且快速的训练？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zym15/whats_the_limit_of_no_of_observations_in_ppo_for/</link>
      <description><![CDATA[我是 PPO 的新手，我有一个疑问，比如什么是一个好的数字（观察数量），它将通过 PPO 提供良好的训练结果算法？就像更多的观察意味着更多的信息和快速学习或者什么......   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zym15/whats_the_limit_of_no_of_observations_in_ppo_for/</guid>
      <pubDate>Sat, 06 Jan 2024 12:04:19 GMT</pubDate>
    </item>
    </channel>
</rss>