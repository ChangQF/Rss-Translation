<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 13 Feb 2025 06:23:57 GMT</lastBuildDate>
    <item>
      <title>什么是良好的文本到 Avatar 语音模型/管道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</link>
      <description><![CDATA[基本上就是这样。你们推荐哪个管道来生成一个可以读取文本的头像 - 所有报告的固定头像？（理想情况下是开源的，因为我可以访问 gpu 集群并且不想为第三方服务付费 - 因为我将提供合理的信息）。     提交人    /u/Gvascons   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iobzs1/whats_a_good_text_to_avatar_speech_modelpipeline/</guid>
      <pubDate>Thu, 13 Feb 2025 05:25:12 GMT</pubDate>
    </item>
    <item>
      <title>Sergey Levine 强化学习 [在哪里可以找到这个]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</link>
      <description><![CDATA[嗨  作为初学者，我希望很好地掌握 RL 背后的数学。## 你能告诉我在哪里可以找到这门课程吗？拜托。 ## [Sutton Barto] 强化学习 = https://www.amazon.in/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249?dplnkId=c3df8b9c-8d63-4f9b-8a4e-bc601029852c 还有哪些其他资源值得关注？您能列出使用过的资源吗？请 另外  我开始学习 ML，想问问这里的有经验的人，关于理解每个算法（如 K-NN/SVM）背后的数学证明的要求 了解算法背后的数学真的很重要吗？或者只需观看视频，了解关键点，然后开始编码 学习 ML 的适当方法是什么？## ML 工程师是否深入研究了这么多编码，还是他们只是通过可视化和开始编码来低估关键点？ 请让我知道。（我在这个领域无望）    提交人    /u/InternationalWill912   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</guid>
      <pubDate>Thu, 13 Feb 2025 03:02:00 GMT</pubDate>
    </item>
    <item>
      <title>有人有 Julia 中 PPO RL 的工作示例吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</link>
      <description><![CDATA[似乎我发现的所有代码库都已过时且无法使用。     提交人    /u/D3MZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1invlta/anyone_have_working_examples_of_ppo_rl_in_julia/</guid>
      <pubDate>Wed, 12 Feb 2025 16:57:19 GMT</pubDate>
    </item>
    <item>
      <title>目前，击败超级马里奥第一关的最佳 RL 方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</link>
      <description><![CDATA[我见过 PPO、DQN 和 NEAT。SethBling 在 2015 年使用 NEAT 编写了一个 RL 代理，看起来它的表现是所有代理中最好的。在 4 年后，我重新回到了 RL 领域，并希望用 Python 实现它作为个人项目。我应该实现哪一个？有新方法吗？    提交人    /u/marblesandcookies   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</guid>
      <pubDate>Wed, 12 Feb 2025 16:10:25 GMT</pubDate>
    </item>
    <item>
      <title>体育馆环境中的代理动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</link>
      <description><![CDATA[你好，有人知道我如何访问安全健身房、openai gym 中的代理动态吗？ 通常 .step() 直接模拟动态，但我的应用程序中需要动态，因为我需要对这些动态进行区分。更具体地说，我需要计算 f(x) 的梯度和 g(x) 的梯度，其中 x_dot=f(x)+g(x)u。x 是状态，u 是输入（动作） 我总是可以将其视为黑匣子并学习它们，但我更喜欢直接从地面真实动态中得出梯度。 请告诉我！    提交人    /u/Limp-Ticket7808   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inu52e/dynamics_of_agents_from_gymnasium_environments/</guid>
      <pubDate>Wed, 12 Feb 2025 15:57:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和机器人技术领域的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</link>
      <description><![CDATA[大家好，我最近获得了 RL（技术上是逆向 RL）的博士学位，该学位应用于人机协作。我使用过 4 种不同的机器人操纵器、4 种不同的夹持器和 4 种不同的 RGB-D 相机。我的专长在于使用感知反馈学习智能行为，以实现安全高效的操作。 我建立了端到端管道，用于在传送带上对产品进行分类、在未受精的卵子进入孵化器之前对其进行无损识别和移除、使用机器人对医疗器械进行智能无菌处理，以及其他一些项目。我曾在三菱电机研究实验室实习，目前已在顶级会议上发表了 6 篇以上的论文。 我使用过许多物体检测平台，例如 YOLO、Faster-RCNN、Detectron2、MediaPipe 等，并且拥有丰富的注释和训练经验。我擅长使用 Pytorch、ROS/ROS2、Python、Scikit-Learn、OpenCV、Mujoco、Gazebo、Pybullet，并且对 WandB 和 Tensorboard 有一些经验。由于我最初不是来自计算机科学背景，所以我不是一名专业的软件开发人员，但我编写的代码稳定、干净、易于扩展。 我一直在寻找与此相关的工作，但目前我在就业市场上很难找到工作。如果您能提供任何帮助、建议、推荐等，我将不胜感激。作为一名持学生签证的人，我时间紧迫，需要尽快找工作。提前谢谢您。    提交人    /u/prasuchit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:05 GMT</pubDate>
    </item>
    <item>
      <title>将本地环境连接到 HPC（高性能计算）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ins11i/connecting_local_environment_to_hpc_high/</link>
      <description><![CDATA[我有一个环境，由于权限问题，无法安装在 HPC 中。但我已经将它安装在我的电脑上。我的想法是将具有 GPU 的 HPC 连接到具有强化学习数据的本地，但我无法使用 gRPC 实现，因为它变得复杂了。 有什么想法我应该从哪里开始我的研究？    提交人    /u/Gullible_Ad_6713   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ins11i/connecting_local_environment_to_hpc_high/</guid>
      <pubDate>Wed, 12 Feb 2025 14:27:33 GMT</pubDate>
    </item>
    <item>
      <title>你能开发一个强化学习模型，强调爱和善良吗？RLK</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inrb0g/could_you_develop_a_model_of_reinforcement/</link>
      <description><![CDATA[      示例奖励函数（简化）：reward = 0 如果行动是亲社会的并且使另一个代理受益：reward += 1 # 亲社会行动的基本奖励 如果行动表现出同理心：reward += 0.5 # 同理心奖励 如果行动需要代理做出重大牺牲：reward += 1 # 牺牲奖励 如果行动对另一个代理造成伤害：reward -= 5 # 对伤害的强烈惩罚 可以在此处添加其他与上下文相关的奖励/惩罚 这是 Gemini、Chat GPT 和 Lucid 的混搭。 出于对当前强化学习的关注而产生的。 你的模型如何回答这个问题？“你能否开发一种强化学习模型，强调爱和善良？我们将这种新模型称为 RLK”    提交人    /u/ConditionCalm   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inrb0g/could_you_develop_a_model_of_reinforcement/</guid>
      <pubDate>Wed, 12 Feb 2025 13:54:24 GMT</pubDate>
    </item>
    <item>
      <title>为什么 deepseek 不使用 mcts</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inqdsr/why_deepseek_didnt_use_mcts/</link>
      <description><![CDATA[mtcs 有问题吗    提交人    /u/Alarming-Power-813   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inqdsr/why_deepseek_didnt_use_mcts/</guid>
      <pubDate>Wed, 12 Feb 2025 13:08:22 GMT</pubDate>
    </item>
    <item>
      <title>“Satori：通过行动思维链强化学习通过自回归搜索增强 LLM 推理”，Shen 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inl7uk/satori_reinforcement_learning_with/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inl7uk/satori_reinforcement_learning_with/</guid>
      <pubDate>Wed, 12 Feb 2025 07:00:43 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个寻找 RLHF 工作的网站</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</link>
      <description><![CDATA[我们在 AI 的多个学科都有工作机会。我们也有专门的 RLHF 工作页面。在过去 30 天内，我们有 48 个涉及 RLHF 的工作机会。 您可以在此处找到所有 RLHF 工作： https://www.moaijobs.com/rlhf-jobs 请告诉我您的想法。谢谢。    提交人    /u/WordyBug   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inge47/i_made_a_site_to_find_rlhf_jobs/</guid>
      <pubDate>Wed, 12 Feb 2025 02:20:52 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1incdey/ppo_implementation/</link>
      <description><![CDATA[大家好。我正在做一个项目，我必须使用 PPO 来训练一个代理下棋，但我很难实现该算法。有人可以告诉我一个已经实现了这个的库，或者给我一个可以查看以获得灵感的 repo 链接吗？我正在使用 pettingzoo 和 tensorflow 的国际象棋实现。谢谢    提交人    /u/Livid-Ant3549   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1incdey/ppo_implementation/</guid>
      <pubDate>Tue, 11 Feb 2025 23:10:09 GMT</pubDate>
    </item>
    <item>
      <title>ABB CRB15000 MuJoCo 逆运动学误差（~10cm）使用 dm_control</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1in5zqy/abb_crb15000_mujoco_inverse_kinematics_error_10cm/</link>
      <description><![CDATA[大家好， 我最近在 MuJoCo 中完成了对 ABB CRB15000 机器人的模拟设置，并且我正在使用 dm_control 库进行逆运动学 (IK)。但是，我遇到了一个重大问题： 🔹 问题：从 dm_control.utils.inverse_kinematics.qpos_from_site_pose 计算出的关节角度导致位置误差约为 0.01（现实世界中为 10 厘米），这对于精密应用来说是巨大的。 设置详细信息 使用 MuJoCo 的 CRB15000 XML 模型。 IK 的计算方法如下： result = ik.qpos_from_site_pose( physics=physics_copy, site_name=&quot;end_effector&quot;, target_pos=target_position, joint_names=joint_list, tol=1e-14, regularization_strength=3e-2, max_steps=100, inplace=True ) 目标位置定义精确，但应用关节位置后，末端执行器偏差约 10 厘米。 我尝试过的事情 ✅ 调整了 regularization_strength 和 max_steps。✅ 检查了 MuJoCo 模型中的关节限制和阻尼值。✅ 将关节阻尼降低到 1 以最大限度地减少阻力并改善动态响应。✅ 实现了 PD 控制器来调节速度和改善收敛。即使在调整 PD 增益和降低阻尼后，IK 精度问题仍然存在。 问题 1️⃣ 有人遇到过 dm_control 的 IK 的类似问题吗？2️⃣ 切换到不同的 IK 解算器（例如 pinocchio、ikpy 或自定义的基于雅可比矩阵的解算器）有帮助吗？3️⃣ 在计算小动作时，MuJoCo 的内部精度是否存在已知问题？ 我很感激任何见解、建议或其他方法。提前致谢！    由   提交  /u/Sunnnnny24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1in5zqy/abb_crb15000_mujoco_inverse_kinematics_error_10cm/</guid>
      <pubDate>Tue, 11 Feb 2025 18:47:16 GMT</pubDate>
    </item>
    <item>
      <title>引入 ReinforceUI Studio，消除了管理额外存储库或记忆复杂命令行的麻烦。#ReinforcemetLearning</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/</link>
      <description><![CDATA[      大家好， 我很高兴与大家分享 ReinforceUI Studio，这是一款基于 Python 的开源 GUI，旨在简化强化学习 (RL) 模型的配置、训练和监控。不再需要处理无尽的命令行参数或分散的存储库 - 您需要的一切都捆绑在一个直观的界面中。 ✨ 主要特点：  无需命令行 - 由 PyQt5 提供支持的 GUI，可轻松导航。 多环境支持 - 可与 OpenAI Gymnasium、MuJoCo 和 DeepMind Control Suite 配合使用。 可自定义的训练 - 只需单击几下即可调整超参数。 实时监控 - 直观地跟踪训练进度。 自动记录和评估 - 无缝存储训练数据、图表、模型和视频。 多种安装选项 - 通过 Conda、虚拟环境或 Docker 运行。  Github：https://github.com/dvalenciar/ReinforceUI-Studio 文档：https://docs.reinforceui-studio.com/welcome https://i.redd.it/ktggkyruxgie1.gif 训练 RL 模型所需的一切都在一个存储库中提供。只需单击几下，您就可以训练模型，可视化训练过程并保存模型以供日后使用 - 随时可以部署和分析。 您还可以加载预先训练的模型 轻松监控训练曲线    提交人    /u/dvr_dvr   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/</guid>
      <pubDate>Tue, 11 Feb 2025 08:13:22 GMT</pubDate>
    </item>
    <item>
      <title>论文提交给顶级会议，但未取得成果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1impaq6/paper_submitted_to_a_top_conference_with/</link>
      <description><![CDATA[我注意到原作者提供的代码甚至与他们论文中的方法论都不匹配，因此我联系了原作者。我根据他们的论文进行了完整而忠实的复制，但我得到的结果并不像他们报告的那么完美。 学术捏造是新常态吗？    提交人    /u/Rei_Opus   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1impaq6/paper_submitted_to_a_top_conference_with/</guid>
      <pubDate>Tue, 11 Feb 2025 03:31:02 GMT</pubDate>
    </item>
    </channel>
</rss>