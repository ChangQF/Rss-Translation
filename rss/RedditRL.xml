<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 20 Sep 2024 09:21:05 GMT</lastBuildDate>
    <item>
      <title>二维装箱问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl6bxp/2d_bin_packing_problem/</link>
      <description><![CDATA[嗨！我正在研究 2D BPP 问题，需要一些指导。有一个定义的托盘和 3 种定义的箱子。我们希望用箱子填满托盘，每次一个。每个箱子都有定义的到达概率  允许箱子旋转 我们希望最好填满托盘的周长 我们避免挤压箱子（在其他箱子之间），因为这个问题是机器人问题，并且存在不确定性 我们必须在箱子到达时放置它们，不能跳过它们。一旦没有空间，我们就会终止  我使用启发式方法解决了这个问题，比较剩余的空间并选择最佳放置坐标。我还对周长使用了不同的搜索：通过沿着托盘周长的较大边优先填充边缘。我不知道如何将其变成学习问题并接受建议！    提交人    /u/Sea-Hovercraft4777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl6bxp/2d_bin_packing_problem/</guid>
      <pubDate>Fri, 20 Sep 2024 07:23:21 GMT</pubDate>
    </item>
    <item>
      <title>推荐涵盖最新算法的调查/学习材料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</link>
      <description><![CDATA[您好，有人可以推荐一些涵盖较新算法/技术（td-mpc2、dreamerv3、diffusion policy）的调查/学习材料吗？其格式类似于 openai 的 spinningup/lilianweng 的博客，现在有点过时了？谢谢    提交人    /u/saintshing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</guid>
      <pubDate>Fri, 20 Sep 2024 07:14:21 GMT</pubDate>
    </item>
    <item>
      <title>帮助对 LLM 进行微调</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl4w1p/help_in_alignment_fine_tuning_llm/</link>
      <description><![CDATA[有人能帮帮我吗，我有一些用于生成 llama 3.1 的二进制反馈数据，是否有一种方法或任何其他算法可以用来使用二进制反馈数据对 llm 进行微调。 数据格式： 用户查询 - 文本 LLM 输出 - 文本标签 - 布尔值    提交人    /u/TuringComplete-Model   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl4w1p/help_in_alignment_fine_tuning_llm/</guid>
      <pubDate>Fri, 20 Sep 2024 05:37:00 GMT</pubDate>
    </item>
    <item>
      <title>LeanRL：一个简单的 PyTorch RL 库，用于快速（>5 倍）训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</link>
      <description><![CDATA[我们很高兴地宣布，我们已经开源了LeanRL，这是一个轻量级的 PyTorch 强化学习库，它使用 torch.compile 和 CUDA 图表提供快速 RL 训练的方法。 通过利用这些工具，与原始 CleanRL 实现相比，我们实现了显着的加速 - 速度提高了 6 倍！ RL 训练的问题 强化学习是出了名的 CPU 受限，因为小型 CPU 操作（例如从模块中检索参数或在 Python 和 C++ 之间转换）的频率很高。幸运的是，PyTorch 强大的编译器可以帮助缓解这些问题。但是，输入编译后的代码也有其自身的成本，例如检查保护装置以确定是否需要重新编译。对于 RL 中使用的小型网络，这种开销可能会抵消编译的好处。 进入 LeanRL LeanRL 通过提供简单的方法来加速您的训练循环并更好地利用您的 GPU，从而解决了这一挑战。受到 gpt-fast 和 sam-fast 等项目的启发，我们证明了 CUDA 图可以与 torch.compile 结合使用，以实现前所未有的性能提升。我们的结果表明：  使用 PPO（Atari）可提高 6.8 倍速度 使用 SAC 可提高 5.7 倍速度 使用 TD3 可提高 3.4 倍速度 使用 PPO（连续动作）可提高 2.7 倍速度  此外，LeanRL 可以更有效地利用 GPU，让您可以同时训练多个网络而不会牺牲性能。 主要特点  具有最少依赖性的 RL 算法的单文件实现 所有技巧都在 README 中进行了说明 从流行的 CleanRL 分叉   在 https://github.com/pytorch-labs/leanrl 上查看 LeanRL    由    /u/AdCool8270  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</guid>
      <pubDate>Fri, 20 Sep 2024 00:22:12 GMT</pubDate>
    </item>
    <item>
      <title>E[G_(t+1) | S_t=s] = V(S_(t+1)) 总是正确的吗？如何证明？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkuvsy/is_it_always_true_that_eg_t1_s_ts_vs_t1_how_to/</link>
      <description><![CDATA[编辑：在第二个成员中，我的意思是 E[V(S(t+1)) | S_t=s] 而不仅仅是 V(S(t+1)) 也许我淹没在一杯什么中，但是你如何证明这个等式成立？我的目标是证明 E[G_t|S_t=s] = E[R_(t+1) + gamma* V(S_(t+1)) | S_t=s ] 就像 sutton 和 barto 的等式 4.3 一样，老实说，我对为什么会发生这种情况有一个直观的想法，但我正在寻找一种更正式的方式来展示这个属性    提交人    /u/samas69420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkuvsy/is_it_always_true_that_eg_t1_s_ts_vs_t1_how_to/</guid>
      <pubDate>Thu, 19 Sep 2024 20:55:18 GMT</pubDate>
    </item>
    <item>
      <title>对于这个特定于图形的任务，多智能体还是分层 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkmed4/multiagent_or_hierarchical_rl_for_this/</link>
      <description><![CDATA[我正在研究一个图形应用问题，其中一个RL代理必须将图形编码视为状态，并选择涉及一对节点和它们之间的一种操作的操作。 我正在考虑将此问题分解为具有多个代理的子任务，如下所示：  代理 1：接收图形编码并选择源节点。 代理 2：接收图形编码和所选源节点，然后选择目标节点。 代理 3：接收图形编码和所选源节点和目标节点，然后在它们之间选择一个操作。  我想到两个解决方案：  分层 RL：虽然任务看起来是分层的，但这可能并不完全合适。对于每个主要操作，必须执行所有三个代理（选项），并且需要按固定顺序执行。他们的行动应该是一步行动。我不确定分层 RL 是否最适合，因为问题没有明确的层次结构，而是顺序合作。 多智能体 RL：这可以构建为具有共同团队奖励的合作多智能体设置，其中执行顺序是固定的，每个智能体都会看到图形编码和先前智能体的操作（根据顺序）。  哪种方法——分层 RL 或多智能体 RL——更适合这个问题？是否有与此类问题相符的现有公式或框架？    提交人    /u/fterranova   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkmed4/multiagent_or_hierarchical_rl_for_this/</guid>
      <pubDate>Thu, 19 Sep 2024 14:36:09 GMT</pubDate>
    </item>
    <item>
      <title>聘请 RL 研究人员——构建下一代专家系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</link>
      <description><![CDATA[嗨！我们是 Atman Labs，一家位于伦敦的 AI 初创公司，在软件中模拟人类专家。我们认为，业界需要超越法学硕士 (LLM)，构建能够解决复杂、知识密集型任务的系统，这些任务需要多步推理。我们的研究使用强化学习来探索知识图谱，以形成针对目标的语义基础策略，并代表了一条模拟专家推理的新颖、可靠的途径。 如果您对 RL 充满热情，并希望构建和商业化下一代智能系统，那么您可能非常适合我们的创始团队。让我们聊聊吧 :) https://atmanlabs.ai/team/rl-founding-engineer    提交人    /u/Tricky_Amphibian_836   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</guid>
      <pubDate>Thu, 19 Sep 2024 12:04:36 GMT</pubDate>
    </item>
    <item>
      <title>CleanRL 现已为 PPO + Transformer-XL 提供基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkeqa9/cleanrl_has_now_a_baseline_for_ppo_transformerxl/</link>
      <description><![CDATA[之前，我们的 PPO-Transformer-XL 基线已发布至 Github。 此实现最终已完善为单文件实现以加入 CleanRL！它在 Memory Gym 的新颖无尽环境中重现了原始结果。 文档：https://docs.cleanrl.dev/rl-algorithms/ppo-trxl/ 论文：https://arxiv.org/abs/2309.17207 视频：https://marcometer.github.io/ 我们希望这将进一步改善有效使用变压器的方式并在基于内存的深度强化学习中高效运行。当然，接下来需要解决一些限制：  加快推理速度：与 GRU 和 LSTM 相比，数据采样成本高昂 节省 GPU 内存：缓存 TrXL 的隐藏状态以进行优化成本高昂     提交人    /u/LilHairdy   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkeqa9/cleanrl_has_now_a_baseline_for_ppo_transformerxl/</guid>
      <pubDate>Thu, 19 Sep 2024 06:52:11 GMT</pubDate>
    </item>
    <item>
      <title>如何将离线数据的动作空间重新映射到原始动作空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkahjl/how_to_remap_the_action_space_of_offline_data_to/</link>
      <description><![CDATA[嗨！ 我想用一些预定义的原始动作来训练厨房任务。但是，原始动作空间是 9 自由度（即 7 个​​手臂关节和 2 个夹持关节）。我应该如何将原始 9 自由度动作重新映射到原始动作以计算参与者损失？ 提前感谢您的帮助！    提交人    /u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkahjl/how_to_remap_the_action_space_of_offline_data_to/</guid>
      <pubDate>Thu, 19 Sep 2024 02:31:17 GMT</pubDate>
    </item>
    <item>
      <title>我可以将 DPO（直接偏好优化）应用于仅具有（y_win，y_loss）一侧的训练数据吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjwqvl/can_i_apply_dpo_direct_preference_optimization_to/</link>
      <description><![CDATA[我有一堆 (x_i, y_i, win_or_lose) 的标记数据。RLHF 论文的大部分内容都使用成对损失函数，这需要 (x_i, y_i_win) 和 (x_i, y_i_lose)，而我没有。我还能将 DPO 用于单侧训练数据吗？ 将缺失侧的隐式奖励值设置为 0，然后仍然应用反向传播，这样可以吗？    提交人    /u/PuzzleheadedBasis951   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjwqvl/can_i_apply_dpo_direct_preference_optimization_to/</guid>
      <pubDate>Wed, 18 Sep 2024 16:14:35 GMT</pubDate>
    </item>
    <item>
      <title>我目前遇到一个问题。给定一组项目，我需要选择一个子集并将其传递给黑盒，然后我将获得该值。我的目标是最大化该值，项目集包含大约 200 个项目。在这种情况下，sota 模型是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjj8mk/i_am_currently_encountering_an_issue_given_a_set/</link>
      <description><![CDATA[  由    /u/Fast-Ad3508  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjj8mk/i_am_currently_encountering_an_issue_given_a_set/</guid>
      <pubDate>Wed, 18 Sep 2024 03:19:42 GMT</pubDate>
    </item>
    <item>
      <title>关于在情景强化学习设置中使用演员评论家架构的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</link>
      <description><![CDATA[大家好，RL 的朋友们， 我最近遇到了一个问题，我正在将多智能体 PPO 与演员-评论家相结合应用于一个问题，由于问题的性质，我首先实施了它的一个情节版本作为初始实施。 我理解拥有评论家的优势之一是可以使用情节中估计的值来更新演员，从而无需等到情节结束时才能用奖励来更新演员。但是，如果无论如何都在情节设置中，使用评论家而不是实际奖励有什么好处吗？    提交人    /u/Ingenuity39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</guid>
      <pubDate>Tue, 17 Sep 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>用于实现 RL 以优化数学函数的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</link>
      <description><![CDATA[有人可以推荐任何资源作为 RL 实现示例来优化数学函数/测试函数吗？因为我能找到的大多数东西基本上都在 gym 环境中。但我正在寻找一个带有代码的示例，它可以对数学函数进行优化（最好使用 actor critical，但其他方法也可以）。如果有人知道这样的资源，请提出建议。提前谢谢您。    提交人    /u/anikbis17   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</guid>
      <pubDate>Tue, 17 Sep 2024 15:46:46 GMT</pubDate>
    </item>
    <item>
      <title>预订建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</link>
      <description><![CDATA[我需要什么书来进行强化学习？ 我希望书既直观又具有数学性，我能理解艰难的数学，因为我有很强的数学背景。 向我推荐一些有很好解释并且包含很好数学内容的书。    提交人    /u/Evening-Passenger311   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</guid>
      <pubDate>Tue, 17 Sep 2024 14:25:13 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>