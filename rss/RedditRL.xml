<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 26 Oct 2024 18:20:30 GMT</lastBuildDate>
    <item>
      <title>[R] 寻求针对 CVPR、ICML 等会议正在进行的完整论文的合作。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gcjgue/r_looking_for_collaborations_on_ongoing/</link>
      <description><![CDATA[大家好， 我们小组，印度理工学院鲁尔基分校视觉与语言小组，最近有三篇研讨会论文被 NeurIPS 研讨会接受！🚀 我们还建立了一个网站 👉 VLG，展示我们参与过的其他出版物，因此我们的团队正在稳步建立 ML 和 AI 研究组合。目前，我们正在合作撰写几篇正在进行的论文，目的是向 CVPR 和 ICML 等顶级会议提交全文。 话虽如此，我们还有更多让我们兴奋的想法。尽管如此，我们的主要限制之一是无法获得适当的指导和 GPU 和 API 的资金，这对于试验和扩展我们的一些概念至关重要。如果您或您的实验室有兴趣一起工作，我们很乐意探索我们感兴趣领域的交集以及您可能带来的任何新想法！ 如果您有可用资源或有兴趣讨论潜在的合作，请随时联系我们！期待着建立联系并共同建立有影响力的东西！这是我们的 Open Slack 的链接👉 Open Slack    提交人    /u/vlg_iitr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gcjgue/r_looking_for_collaborations_on_ongoing/</guid>
      <pubDate>Sat, 26 Oct 2024 12:01:00 GMT</pubDate>
    </item>
    <item>
      <title>需要反馈来改进我的 RL 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gchxzd/need_feedback_to_improve_my_rl_model/</link>
      <description><![CDATA[大家好 我正在尝试设置一个 RL 环境来优化电子邮件营销活动，其中变量是 email.variants（3 种类型）、可能发送电子邮件的时间（3 种类型）和星期几（5 种类型）。 我正在尝试设置环境并使用合成数据（客户对上述变量具有预定义的偏好）。如果采取的行动与客户的偏好相匹配，则会向模型提供奖励。这是模型和我问的同样的问题在堆栈溢出https://stackoverflow.com/questions/79116878/query-on-reinforcement-learning-model-not-converging 我需要有关以下方面的建议 1. 环境设置是否正确？ 2. 无论任何合成数据样本，模型似乎总是收敛在同一组动作上。 为什么会这样？ 3. 关于尝试不同模型或探索策略的任何建议。    提交人    /u/Lanky_Association_57   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gchxzd/need_feedback_to_improve_my_rl_model/</guid>
      <pubDate>Sat, 26 Oct 2024 10:18:41 GMT</pubDate>
    </item>
    <item>
      <title>验证我的 DQN 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gchroh/validating_my_dqn_implementation/</link>
      <description><![CDATA[大家好， 我正在尝试为一个项目使用一个简单的 DQN 代理，我在 github 上找到了 Phil 的 DQN 实现（https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code/tree/master/DQN）。我所做的唯一更改是在 DeepQNetwork 类上，因为我的输入是向量而不是矩阵，所以我删除了 conv 函数，只是通过层转发我的输入。 我的问题是：  有没有办法用已经存在的环境（可能是 openAI）来验证此代码，其中观察结果是向量而不是矩阵？ 代码本身正确吗？例如。我在 github 页面上看到一个关于在 q_target 计算之前使用 torch.no_grad() 的问题，但尚未解决。  （编辑）：我们在训练期间也使用稳定的 epsilon，而不是随着轮次的进行不断降低它。此外，标题具有误导性，它不是“我的”实现，而是来自 Phil Tabor    提交人    /u/CrazyRabb1tStyle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gchroh/validating_my_dqn_implementation/</guid>
      <pubDate>Sat, 26 Oct 2024 10:06:21 GMT</pubDate>
    </item>
    <item>
      <title>PPO 中熵系数衰减的正确方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gcf09f/right_way_to_decay_entropy_coeff_in_ppo/</link>
      <description><![CDATA[大家好， 我正在使用 PPO 训练一个中等复杂的代理环境问题。我想知道衰减熵系数的正确方法是什么（考虑到问题的性质，它必须衰减）。 我使用线性衰减还是指数衰减？两者的优点/缺点是什么？我知道它完全取决于熵曲线，但当涉及到熵曲线的形状时，“经验法则”是什么？它是从 1 到 0.1 的逐渐线性下降吗？还是完全取决于问题和 rew 进展。 告诉我你的经历。提前谢谢！    提交人    /u/Famous-Explanation56   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gcf09f/right_way_to_decay_entropy_coeff_in_ppo/</guid>
      <pubDate>Sat, 26 Oct 2024 06:36:37 GMT</pubDate>
    </item>
    <item>
      <title>寻求技巧，使我的 PPO 交易机器人保持出色的性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gc45xu/seeking_tips_to_make_exceptional_performance/</link>
      <description><![CDATA[      大家好， 我目前正在运行一个经过广泛测试的 PPO 交易机器人 - 每个设置大约运行 50 次。我注意到，这些测试中的一项通常会以出色的表现脱颖而出（回报率高达 300%），而其他测试的回报率则徘徊在 +/- 50% 左右。 我正在寻找洞察力，了解为什么一次运行往往会表现出色，以及如何使这种出色的表现在所有测试中更加一致，而不是偶尔出现异常值。我的机器人对每个测试都使用相同的市场条件，所以我很想知道是否有人知道哪些关键因素或修改可以帮助使这种行为成为默认行为。 我一直在尝试参数调整和贝叶斯优化，但任何其他提示都将不胜感激。提前致谢！ PS：今天，我甚至在泛化测试中达到了+1738％的回报。 https://preview.redd.it/oiemun1crywd1.png?width=3840&amp;format=png&amp;auto=webp&amp;s=1a585f07c9c604c1a54c431bba938f1f6a47333f    提交人    /u/nalman1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gc45xu/seeking_tips_to_make_exceptional_performance/</guid>
      <pubDate>Fri, 25 Oct 2024 20:39:13 GMT</pubDate>
    </item>
    <item>
      <title>与 SAC 相比，PPO 采取的行动范围更高。为什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gc2z6y/ppo_takes_upper_range_of_actions_compared_to_sac/</link>
      <description><![CDATA[      我有一个补料分批发酵模拟（或游戏），我正在使用强化学习（RL）算法进行控制。控制参数是进料量（动作空间），范围从 0 到 0.1，而观察空间包括时间步长和产品浓度。我使用 Stable Baselines 3 将不同的 RL 算法应用于此自定义发酵环境。目标是优化进料（0 - 0.1）以最大化产品产量。 当我使用 PPO 时，我注意到它倾向于支持动作空间的上限，通常选择 0.1。相比之下，SAC 的行为不同，通常选择更接近下限的值，例如 0.01 或 0.02，并在情节结束时逐渐将动作增加到更高的值，例如 ~0.1。 这两种行为都可能有效，但我很好奇为什么这两种算法以不同的方式解决问题，特别是因为它们从不同的动作空间值开始。关于训练稳定性，我注意到 PPO 在最终奖励中波动更大，而 SAC 更稳定，即使在预测期间也是如此。 如何解释这些差异？  PPO SAC    提交人    /u/Fr4gg3r_   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gc2z6y/ppo_takes_upper_range_of_actions_compared_to_sac/</guid>
      <pubDate>Fri, 25 Oct 2024 19:47:42 GMT</pubDate>
    </item>
    <item>
      <title>[寻求 DeepRL 导师（博士）]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gc0hz7/looking_for_mentoring_in_deeprl_phd/</link>
      <description><![CDATA[您好， 我正在寻找一位导师来指导我完成深度强化学习的博士项目，最好是在 MentorCruise 这样的平台上。 谢谢！    提交人    /u/TeamTop4542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gc0hz7/looking_for_mentoring_in_deeprl_phd/</guid>
      <pubDate>Fri, 25 Oct 2024 17:59:47 GMT</pubDate>
    </item>
    <item>
      <title>帮助这位 PPO 初学者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gbpp0j/help_this_beginner_for_ppo/</link>
      <description><![CDATA[我正在从事一个任务分配项目。我的 PPO 没有学习；就像从较大的负值开始，在 500 集内达到最小值并在该区域振荡。我不知道可能是什么问题。我需要做些什么才能更好地学习！    提交人    /u/Different_Prune_9756   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gbpp0j/help_this_beginner_for_ppo/</guid>
      <pubDate>Fri, 25 Oct 2024 08:48:15 GMT</pubDate>
    </item>
    <item>
      <title>Decision Transformer 学习不正确</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gbntqe/decision_transformer_not_learning_properly/</link>
      <description><![CDATA[嗨，如果能得到一些帮助，让我的决策转换器能够用于离线学习，我将不胜感激。 我正在尝试对多周期混合问题进行建模，为此我创建了一个自定义环境。我有一个从线性求解器获得的 60k 个状态/动作对的数据集。我正尝试在数据上训练 DT，但训练速度极慢，损失仅略有减少。 我认为我的环境并不是特别困难，而且我在简单的环境中使用 PPO 获得了一些不错的结果。 有关更多上下文，这是我的 repo：https://github.com/adamelyoumi/BlendingRL；我在 DT 存储库中使用修改版的 experiment.py。 谢谢    提交人    /u/cheese_n_potato   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gbntqe/decision_transformer_not_learning_properly/</guid>
      <pubDate>Fri, 25 Oct 2024 06:24:15 GMT</pubDate>
    </item>
    <item>
      <title>关于离线多代理环境的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gbknms/advice_on_offline_multi_agent_environment/</link>
      <description><![CDATA[我在多代理环境中工作，并收集了每个代理的数据（我可以指定每个代理执行的操作）。这些数据是每个代理在某些步骤中采取的操作，但不是每个步骤。然后，假设我是过去采取过行动的代理之一，之后完全忘记了我的策略。问题是我如何学习我之前的策略？我想知道为什么我在那个特定时刻采取了这些行动。（我的代理内部“状态”） 也许一种方法是使用监督学习。恢复部分可观察环境的一些特征，并尝试从我的行为和我的实际行为之前的状态中的特征中学习一些东西。但我认为这个问题最适合 RL。 我最近开始学习 RL，但我听说过很多高级主题，但没有好好研究，以确定它们是否适合这个问题。模仿学习或离线 RL 在这里有用吗？  更多背景信息，问题是离线的，所以我无法再次与环境交互，我不知道我的奖励函数，也不知道我的策略是否最佳（如果是这种情况，我可能会选择模仿学习），cueck！......我只是想了解我为什么要执行这些操作。 如果有人能帮助我提出一些我需要学习的方向或算法类别并且也许可以在这里工作，我将不胜感激。    提交人    /u/Odd-Appointment-4685   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gbknms/advice_on_offline_multi_agent_environment/</guid>
      <pubDate>Fri, 25 Oct 2024 03:06:52 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gb6efv/working_rl_in_practice/</link>
      <description><![CDATA[我知道 RL 在实践中很脆弱，很难发挥作用，但如果做得好，它也会非常强大，例如 Deepmind 与 AlphaZero 合作等。你知道 RL 在现实生活中应用的任何令人信服的例子吗？让你心存疑虑的东西？    提交人    /u/FriendlyStandard5985   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gb6efv/working_rl_in_practice/</guid>
      <pubDate>Thu, 24 Oct 2024 16:11:51 GMT</pubDate>
    </item>
    <item>
      <title>您会推荐虚幻引擎或 Unity 来构建环境吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gah9xw/would_you_recommend_unreal_engine_for_building/</link>
      <description><![CDATA[我喜欢的两款游戏引擎  虚幻引擎（Carla 就是用这个引擎开发的）。对于研究人员来说，许可很友好，如果你是典型的苦苦挣扎的研究学生，也不会花费太多。更难学习，蓝图可能会使事情变得更加复杂，如果不是典型的话。直接支持 C++。规格非常高。 Unity。非常容易构建，但对于视频游戏以外的任何事物来说，许可都太严格了。即使你不赚钱，每年也要 2.5k，研究似乎不在游戏/娱乐范围内。在 3d 之外更灵活。  两者似乎都有无头模式。 你会推荐哪款游戏引擎来构建 RL 环境，并且让它与 python 健身房训练工作流程一起工作的过程最不痛苦？    提交人    /u/I_will_delete_myself   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gah9xw/would_you_recommend_unreal_engine_for_building/</guid>
      <pubDate>Wed, 23 Oct 2024 18:09:15 GMT</pubDate>
    </item>
    <item>
      <title>需要解决部分观察到的迷宫环境的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gah5t5/need_advice_for_solving_partially_observed_maze/</link>
      <description><![CDATA[我在 Unity 中创建了一个自定义环境，其中包含简单的基于网格的迷宫，如下所示 (https://imgur.com/a/0rNmmUg)。代理只能看到（通过矢量观察，而不是图像）自身周围。它向 8 个方向发射射线（图中为红色），并获取有关其击中的内容（墙壁、出口或空无）以及到击中点的距离的信息。并且发现的房间数量也会被输入到观察中。至于奖励，它每一步都会得到 -1，发现新房间并找到出口时会得到正奖励。目标是探索迷宫并找到出口 问题是代理一直粘在墙上，四处转圈并随机行动。我尝试过将停留在一个房间的惩罚加倍，将高速度和其他事情的正奖励加倍。我还能尝试什么？我可以完全控制环境，并且不受代理的确切设计约束。我正在使用具有自动熵的 SAC     提交人    /u/Aydiagam   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gah5t5/need_advice_for_solving_partially_observed_maze/</guid>
      <pubDate>Wed, 23 Oct 2024 18:04:30 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：大型状态/动作空间的批量大小和更新频率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gabt9r/seeking_advice_batch_size_and_update_frequency/</link>
      <description><![CDATA[大家好！ 我正在做一个关于云资源分配的项目，我真的需要你的建议。我的目标是尽量减少服务器的总体能耗，我正在处理连续随机的作业到达。 下面是一个快速概述： 我处理每个作业块有 10 个作业，每个作业都有多个依赖任务。对于每个块，我运行 10 次迭代和 12 个情节来收集轨迹，然后使用离策略模式更新我的模型。 经过这 12 个情节 的 一次迭代 之后，我的重播缓冲区最终获得了大约 499,824 个经验！现在，我需要你的帮助：  您认为从重播缓冲区采样的最佳批次大小是多少？ 我应该多久更新一次模型参数？  由于作业不断到达以及任务和资源可用性的变化，我的状态和操作空间非常大且动态。 （我正在使用策略梯度架构。） 您能分享的任何见解或经验都将非常有帮助！ 非常感谢！   由    /u/TeamTop4542  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gabt9r/seeking_advice_batch_size_and_update_frequency/</guid>
      <pubDate>Wed, 23 Oct 2024 14:24:06 GMT</pubDate>
    </item>
    <item>
      <title>需要有关构建 RL 代理的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9v667/need_advice_for_building_an_rl_agent/</link>
      <description><![CDATA[大家好，感谢您的阅读和提供的任何意见。 我有一个 Web 应用程序，每天有 50 多人使用，他们查看数据库并寻找可见的错误。这个过程已经使用了几年，并且不可扩展，因为我们即将将数据库大小乘以 100 倍。 因此，我试图构建一个代理，通过保存 UI 的状态和用户的操作以及对它们的训练来模仿人类浏览数据库。目标是让它发现最明显的错误，这样只有困难（和有趣的）错误才会留给我们训练有素的人工校对员（某种模仿学习）。 我有几个问题：  您认为可以采用哪种 RL 结构？ 什么是 RL 的最佳输入类型？以及 RL 准确度如何随着数据集大小而扩展？ 您知道哪些陷阱，我应该注意哪些陷阱？ 您有什么想法吗？:)  如果需要，我可以通过数据库获得很多与人类相关的东西（如果有效的话，拥有这个 AI 代理将大大改善我们校对人员的工作方式，因此他们有兴趣帮助收集数据）。 （我来自计算机视觉/数学背景，并具有构建更多 UNET/GNN 类型模型的经验。我正在尽可能快地浏览我在网上找到的资源）    提交人    /u/Delicious_Wall3597   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9v667/need_advice_for_building_an_rl_agent/</guid>
      <pubDate>Tue, 22 Oct 2024 22:36:37 GMT</pubDate>
    </item>
    </channel>
</rss>