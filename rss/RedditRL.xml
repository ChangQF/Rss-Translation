<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 23 Jul 2024 18:20:12 GMT</lastBuildDate>
    <item>
      <title>启动图书馆学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eaeng7/spinnig_up_library_learning_sessions/</link>
      <description><![CDATA[大家好，我是一名二年级博士生，研究方向为强化学习的计算机科学。我有一些强化学习的背景（观看了 David Silver 的 YouTube 讲座，阅读了 DeepMind 的几篇论文，如 AlphaGo 和 Muzero，也熟悉主要的 RL/DRL 算法），并希望在实施方面更深入、更好地理解强化学习。华盛顿特区、马里兰州或弗吉尼亚州地区是否有人愿意安排学习课程来阅读 Spinningup 图书馆文档并观看其 YouTube 讲座？    提交人    /u/atb1399   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eaeng7/spinnig_up_library_learning_sessions/</guid>
      <pubDate>Tue, 23 Jul 2024 18:07:15 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习：与无模型强化学习的区别令人困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eachu8/modelbased_rl_confused_about_the_differences/</link>
      <description><![CDATA[在互联网上，人们可以找到许多帖子来解释 MBRL 和 MFRL 之间的区别。即使在 Reddit 上，也有一个很好的直观帖子。那么，为什么还要问一个关于同一主题的无聊问题呢？ 因为当我读到类似这样的定义时： 基于模型的强化学习 (MBRL) 是一个用于在部分理解的环境中解决任务的迭代框架。有一个代理反复尝试解决问题，积累状态和操作数据。利用这些数据，代理创建一个结构化的学习工具——动态模型——来推理世界。利用动态模型，代理通过预测未来来决定如何行动。通过这些操作，代理可以收集更多数据，改进所述模型，并有望改进未来的操作。 (source)。 那么对我来说，MBRL 和 MFRL 之间只有一个区别：在模型自由的情况下，您将问题视为黑匣子。然后你实际上运行双或数百万个步骤来了解黑匣子的工作原理。但这里的问题是：与 MBRL 有什么区别？ 另一个问题是，当我读到时，您不需要 MBRL 的模拟器，因为算法在训练阶段可以理解动态。好的。这对我来说很清楚…… 但是假设您有一辆正在行驶的汽车（没有摄像头，只有汽车在跑道上行驶的形状）并且您想要应用 MBRL，那么您需要一个汽车模拟器，因为模拟器会生成代理所需的图片，以便代理能够真正看到汽车是否在路上。  所以即使我认为，我理解了两者之间的理论区别，但当我试图弄清楚何时需要模拟器，何时不需要模拟器时，我仍然停滞不前。 从字面上讲：即使我在 Gymnasium 中为 Cartpole 环境训练一个简单的代理（并使用无模型方法），我也需要一个模拟器。 但是，如果我想使用 GPS（基于模型），那么无论如何我都需要那个环境。  如果您能帮助我理解，我真的很感激。 谢谢   由    /u/WilhelmRedemption  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eachu8/modelbased_rl_confused_about_the_differences/</guid>
      <pubDate>Tue, 23 Jul 2024 16:40:21 GMT</pubDate>
    </item>
    <item>
      <title>预训练自动编码器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ea4310/pretrained_autoencoders/</link>
      <description><![CDATA[我过去曾尝试过使用预训练自动编码器来玩 Atari 等游戏，但我发现预训练并没有太大帮助。我也对变分自动编码器进行了一些尝试，发现它们也没有加快训练速度。我通常使用 MSE 重构误差进行预训练，然后冻结编码器权重并附加策略网络或 Q 函数。我知道也有一些研究声称对比自动编码器效果更好，但这依赖于正/负对。 我很好奇是否有其他人遇到过类似的自动编码器问题，或者您是否有任何用于 RL 预训练自动编码器的实用技巧。    提交人    /u/smorad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ea4310/pretrained_autoencoders/</guid>
      <pubDate>Tue, 23 Jul 2024 10:05:06 GMT</pubDate>
    </item>
    <item>
      <title>cleanrl ppo 中 num_step 的含义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9ydvf/meaning_of_num_step_in_cleanrl_ppo/</link>
      <description><![CDATA[cleanrl github 嗨，我对推出时步骤数（repo 中的 num_steps）的含义有疑问。 我以为步骤数是一集的最大长度（例如，在我的环境中，游戏的最大长度是 25 步）。 所以我认为当一集终止时，步骤中的迭代应该关闭，但在 repo 中，迭代仍在继续。即使环境被重置了。 有什么我误解的吗？    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9ydvf/meaning_of_num_step_in_cleanrl_ppo/</guid>
      <pubDate>Tue, 23 Jul 2024 03:58:25 GMT</pubDate>
    </item>
    <item>
      <title>d4rl maze2d 最小和最大分数计算？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9u3z5/d4rl_maze2d_minimum_and_maximum_scores_calculation/</link>
      <description><![CDATA[抱歉，这个问题可能有点琐碎；我想了解为什么 d4rl &#39;maze2d-umaze-v1&#39; 随机代理得分为 23.85，而专家得分为 161.86。 编辑：一集的总奖励似乎是可变的。maze2d 在完成之前是否会为一集返回多个非零奖励？    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9u3z5/d4rl_maze2d_minimum_and_maximum_scores_calculation/</guid>
      <pubDate>Tue, 23 Jul 2024 00:25:30 GMT</pubDate>
    </item>
    <item>
      <title>目前 RL 领域最顶尖的研究人员有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9mxbr/what_are_the_top_researchers_in_rl_right_now/</link>
      <description><![CDATA[我病了，接下来的几天会待在家里，我想我应该补上读书。 有人对顶级研究人员有什么建议吗，这样我就可以查阅他们的论文了？谢谢！    提交人    /u/phantomBlurrr   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9mxbr/what_are_the_top_researchers_in_rl_right_now/</guid>
      <pubDate>Mon, 22 Jul 2024 19:24:52 GMT</pubDate>
    </item>
    <item>
      <title>我已经训练机器人与 RL 进行战斗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9l5q3/ive_trained_robots_to_fight_with_rl/</link>
      <description><![CDATA[大家好， 我想回答动漫中一个永恒的问题：“哪种武术风格更胜一筹？”😁 这是我最近尝试的视频：https://www.youtube.com/watch?v=7AnJAlDFTN0 一些背景知识：一切都由神经网络控制，包括关节和高级策略。我决定使用 Unity ML-Agents，因为我非常喜欢在 Unity 中开发。训练需要很长时间，但绝对值得😁 我想添加更多具有各种身体结构和尺寸的机器人，当然还有更多的战斗风格。    提交人    /u/bmind7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9l5q3/ive_trained_robots_to_fight_with_rl/</guid>
      <pubDate>Mon, 22 Jul 2024 18:13:59 GMT</pubDate>
    </item>
    <item>
      <title>强化学习，您可以在奖励函数空间中同时进行优化。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9kmdo/reinforcement_learning_where_you_simultaneously/</link>
      <description><![CDATA[考虑一个问题，其中您有一个固定的环境，其中状态为 s，动作为 a。我们还有一个奖励函数 R(s; v)，除了依赖于状态 s 之外，还具有可调参数 v。例如，假设 R(s;v) 是一个神经网络，它接受 s 并产生奖励，而 v 是它的权重。 给定一个固定的 v，训练一个在 R(v,s) 上表现良好的代理是一个标准的 RL 问题。 但是，我还有第二个目标 O(v) = E_s ( F[ R(v,s) ] ) ，其中 E_s 是针对该奖励进行训练的代理的最佳表现的期望值，我希望优化以获得最佳的 v。 简单地说，这个问题的解决方法是逐一运行所有 v，为该问题训练一个代理，计算 O(v)，然后重复。 但是，似乎很明显应该可以实现某种迭代算法，在其中可以同时改进奖励函数和代理，直到两者都收敛。 有人能帮我找到这种方法的名称或一些参考文献吗？    提交人    /u/Acceptable_Trainer53   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9kmdo/reinforcement_learning_where_you_simultaneously/</guid>
      <pubDate>Mon, 22 Jul 2024 17:52:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PPO 可以学习而 SAC 却会失败呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9h49d/why_would_sac_fail_where_ppo_can_learn/</link>
      <description><![CDATA[      大家好， 我有这个我编写的超级简单的 Env。我已经设法用 SB3 PPO 训练了一个代理，但仍然无法达到情节长度 120 步。此外，奖励低于理论最大值 0.37。 我决定尝试 SAC，并使用默认学习参数从 PPO 更改为 SAC。我是 RL 的初学者，因此当我的尝试失败时我并不感到非常惊讶，但我想了解以下内容表示什么。这是从 SAC 中学习到的，平均奖励和情节长度下降并卡在某个水平。 显然，由于我使用默认学习参数并且是新手，也许我不应该期望 SAC 开箱即用，我想了解的是这种学习告诉我什么？ PPO vs SAC。相同环境。    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9h49d/why_would_sac_fail_where_ppo_can_learn/</guid>
      <pubDate>Mon, 22 Jul 2024 15:28:44 GMT</pubDate>
    </item>
    <item>
      <title>用于强化学习的可视化节点编程工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9agso/visual_nodes_programming_tool_for_reinforcement/</link>
      <description><![CDATA[目前存在用于机器学习的可视化编程工具，如 Visual Blocks。但是我还没有看到任何专门用于强化学习的工具。在我看来，现有的工具（如 Visual Blocks）对强化学习来说并不是很好。 拥有一个用于强化学习的可视化编程工具可能会很有用，因为它可以让开发人员快速制作原型和调试强化学习模型。 我正在考虑制作这样一个工具，它将支持现有的强化学习库，如 Tensorforce、Stable Baselines、RL_Coach 和 OpenAI Gym。 你们觉得这个想法怎么样？你知道这是否已经存在，它是否对你的职业或业余项目有用？    提交人    /u/Charming-Quiet-2617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9agso/visual_nodes_programming_tool_for_reinforcement/</guid>
      <pubDate>Mon, 22 Jul 2024 10:00:57 GMT</pubDate>
    </item>
    <item>
      <title>我找不到在 Twitter 上看到的有关新算法的帖子 (x)。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e99tmb/i_cant_find_a_post_about_the_new_algorithm_that_i/</link>
      <description><![CDATA[昨天我看到了一篇帖子，但没有保存（是的，那是一个错误），它写了一种新算法，并且成功地在 2 分钟内训练了智能体运行。我认为它是 Mujoco 环境，其中有一个类人机器人、一只蜘蛛和所有可以行走的智能体。找到它会很不错。如果您看到这篇帖子，请发送链接。 “2 分钟”肯定写在那里。 而且它也可能“近端”“扩散”“优化”。 下面是视频结果，有智能体在跑来跑去。 谢谢。    提交人    /u/imitagent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e99tmb/i_cant_find_a_post_about_the_new_algorithm_that_i/</guid>
      <pubDate>Mon, 22 Jul 2024 09:17:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Linux 上安装 D4RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e94noy/how_to_install_d4rl_on_linux/</link>
      <description><![CDATA[如果能给我一些关于在我的 Ubuntu 24.04 操作系统上安装 D4RL 的指导，我将非常感激。  我尝试按照 github 上的步骤安装 D4RL，但是在从互联网上下载的 mujoco 文件和 mujoco-py 之间遇到了错误。  提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e94noy/how_to_install_d4rl_on_linux/</guid>
      <pubDate>Mon, 22 Jul 2024 03:39:39 GMT</pubDate>
    </item>
    <item>
      <title>训练现实世界 RL 模型以对 3D 可变形物体进行长距离操控</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8www2/training_realworld_rl_model_of_longhorizon/</link>
      <description><![CDATA[嗨， 我正在研究一个机器人操作任务，该任务可以处理 3D 可变形物体 (DO)，例如生肉。我的一项任务是使用 RL 训练 Delta 机器人使用真空吸盘拾取和投掷。 由于 3D DO 的属性未知，并且大多数模拟目前都在 1D 和 2D 线性 DO 上进行，因此我决定进行连续动作空间、无模型真实世界训练，而无需任何模拟。 我知道我不能进行超过 400-500 次实验，因此数据有限，策略也不是最优的。但是，此处训练的目的是在这些测试中获得最佳策略。因此，我想问几个问题，并希望听取您的建议：  您应该推荐哪种方法？端到端训练 https://arxiv.org/abs/2406.13453 或任务分解，如 https://doi.org/10.3390/biomimetics8020240 我应该选择哪种算法来提高数据效率？DDPG、D4PG、HAC、.... 如果我错了，请纠正我。由于这是操纵 DO 的一系列动作，因此在执行所选动作后，“下一个状态”毫无用处。视觉系统是分开的，所以我没有使用图像作为状态，状态是{初始 S_obj 和 S_ee}。  我刚刚开始学习 RL，所以如果你们能帮助我，我将不胜感激。由于这对我来说还很新，我可能会问更多后续问题，我非常感谢你们的帮助。非常感谢！    提交人    /u/Fish_Chandle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8www2/training_realworld_rl_model_of_longhorizon/</guid>
      <pubDate>Sun, 21 Jul 2024 21:19:05 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义 Python 和 Unity 引擎完成的虚拟 AI 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</link>
      <description><![CDATA[       由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</guid>
      <pubDate>Sun, 21 Jul 2024 15:19:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MARL 解决 N vs N 追击-逃避游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</link>
      <description><![CDATA[有谁知道使用 MARL 解决 N vs N 追击规避游戏的 SOTA 工作吗？我研究过基于策略的方法（例如 MADDPG、MAPPO）和基于价值的方法（例如 QMIX、VDN、COMA 等）。大部分工作都是针对完全竞争场景（例如 GRF、SMAC、hanabi 挑战、使用启发式或随机移动的固定对手的 MPE）完成的。MADDPG 仅在混合动机场景（N vs 1）中解决了 MPE（多智能体粒子环境），其中合作智能体数量较少，对手为 1 个，由 MADDPG vs DDPG 捕获。我认为它在 N vs N 情况下无法扩展，因为 N 更高。如果有人能提出使用 MARL 进行混合动机和完全竞争设置的任何工作或想法，那将很有帮助。    提交人    /u/Meta_Sage_247   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</guid>
      <pubDate>Sun, 21 Jul 2024 13:36:51 GMT</pubDate>
    </item>
    </channel>
</rss>