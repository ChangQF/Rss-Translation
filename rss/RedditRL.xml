<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 23 Oct 2024 03:26:25 GMT</lastBuildDate>
    <item>
      <title>需要有关构建 RL 代理的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9v667/need_advice_for_building_an_rl_agent/</link>
      <description><![CDATA[大家好，感谢您的阅读和提供的任何意见。 我有一个 Web 应用程序，每天有 50 多人使用，他们查看数据库并寻找可见的错误。这个过程已经使用了几年，并且不可扩展，因为我们即将将数据库大小乘以 100 倍。 因此，我试图构建一个代理，通过保存 UI 的状态和用户的操作以及对它们的训练来模仿人类浏览数据库。目标是让它发现最明显的错误，这样只有困难（和有趣的）错误才会留给我们训练有素的人工校对员（某种模仿学习）。 我有几个问题：  您认为可以采用哪种 RL 结构？ 什么是 RL 的最佳输入类型？以及 RL 准确度如何随着数据集大小而扩展？ 您知道哪些陷阱，我应该注意哪些陷阱？ 您有什么想法吗？:)  如果需要，我可以通过数据库获得很多与人类相关的东西（如果有效的话，拥有这个 AI 代理将大大改善我们校对人员的工作方式，因此他们有兴趣帮助收集数据）。 （我来自计算机视觉/数学背景，并具有构建更多 UNET/GNN 类型模型的经验。我正在尽可能快地浏览我在网上找到的资源）    提交人    /u/Delicious_Wall3597   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9v667/need_advice_for_building_an_rl_agent/</guid>
      <pubDate>Tue, 22 Oct 2024 22:36:37 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic：“通过新的 Claude 3.5 Sonnet 介绍‘计算机使用’”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9qvgo/anthropic_introducing_computer_use_with_a_new/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9qvgo/anthropic_introducing_computer_use_with_a_new/</guid>
      <pubDate>Tue, 22 Oct 2024 19:33:51 GMT</pubDate>
    </item>
    <item>
      <title>现实生活中的应用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g9adre/real_life_application/</link>
      <description><![CDATA[嗨 在 reddit、medium 和其他网站的许多帖子中，我通常会看到关于玩具问题或模拟的应用。我认为我从未见过现实生活中的应用，即使是在一个简单的系统上。 在我看来，RL 从未进入现实生活，而是停留在模拟中。然而，这不可能存在，而且必然有人已经在现实中使用它了。 您是否曾在模拟中实现过？哪种方法在模拟中有效，但在真实系统中无效？在模拟过程中需要考虑哪些具体要点以确保成功转移到现实生活中？您遇到了哪些困难，又是如何克服的？ 感谢您的反馈。    提交人    /u/seb59   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g9adre/real_life_application/</guid>
      <pubDate>Tue, 22 Oct 2024 04:59:19 GMT</pubDate>
    </item>
    <item>
      <title>QMIX 代理做出类似的动作模式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g981cz/qmix_agent_makes_similar_action_patterns/</link>
      <description><![CDATA[      https://preview.redd.it/2eg00ql418wd1.png?width=162&amp;format=png&amp;auto=webp&amp;s=1640f5e4966c57241d8388480abf010c0dc86747 我正在尝试在多智能体环境中使用 QMIX 和 gfootball。如图所示，使用 QMIX 学习的结果是所有智能体都在追球。我该如何解决这个问题？    提交人    /u/TelephoneStrange9364   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g981cz/qmix_agent_makes_similar_action_patterns/</guid>
      <pubDate>Tue, 22 Oct 2024 02:46:21 GMT</pubDate>
    </item>
    <item>
      <title>模型是否对短剧集长度产生偏差？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g8y5co/model_became_biased_for_short_episode_length/</link>
      <description><![CDATA[嘿！ 我正在使用 SB3 的 PPO 训练交易代理。我正在使用基于事件的回测器并使用数月的 HFT 数据。为了使代理更加稳健，我决定在整个数据中选择一个随机起始位置并交易预设数量的步骤，之后我提供构成一个情节的 truncated ==True, Done == False 信号。然后由模型重置环境并选择另一个随机起始位置。 我正在使用 make_vec_env，创建大量并行环境（大约 40 个）并使用 VecNormalize。该模型收敛得很好，我在 TensorBoard 上看到了很好的奖励值。 但是当我在已保存的模型上使用 assess_policy（当然使用已保存的 VecNormalize 统计数据）时，即使在我用于训练的数据上，我也会看到巨大的负奖励曲线。应该提到的一件重要的事情是 - 当我使用evaluate_policy时，我不会将数据限制在几天内，我会让代理运行整个月的数据。 可能发生两件事：  我在保存 VecNormalize 数据或保存/加载模型或通过某种方式提供错误的截断信号传递环境方面做错了（但我非常怀疑） 或者模型学会在短情节长度下获得可观的利润，并且当情节继续进行模型习惯的更多步骤时，这是一个很大的惊喜并且它开始失败。   第二个假设得到了这样的事实的支持：当我在训练期间改变情节拆分成的步骤数时，我会看到评估图上大约相同步骤数的奖励曲线上升。 那么这可能吗？如果可能，克服这个问题的正确方法是什么？ 附言：我决定不在起始帖子中堆满源代码，但如果有必要，我很乐意提供它们。    提交人    /u/StabbMe   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g8y5co/model_became_biased_for_short_episode_length/</guid>
      <pubDate>Mon, 21 Oct 2024 19:17:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO 扩展奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g8smlo/ppo_scaling_reward/</link>
      <description><![CDATA[大家好， 我目前正在尝试解决一个问题，该问题的奖励可能非常大（例如 -100），也可能很小（最多约 9） 由于惩罚高于奖励（即更重要）很重要，因此除了缩放之外，我不“允许”更改奖励函数。 我读过几次，在 -1 和 1 之间缩放奖励对于 PPO 和其他方法很重要。 如果我将此奖励缩放（div 100），使得 -1 是最高惩罚，我只会获得较小的奖励。因此奖励在范围内（-1, 0.09） 奖励没有达到 1 是个问题吗？   由    /u/luigi1603  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g8smlo/ppo_scaling_reward/</guid>
      <pubDate>Mon, 21 Oct 2024 15:36:31 GMT</pubDate>
    </item>
    <item>
      <title>使用 Python 编写的 RL 代理，用于 Java 编写的棋盘游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g84lzs/rl_agent_in_python_for_board_game_in_java/</link>
      <description><![CDATA[嘿 :) 我想用 Python 为我的 Java 棋盘游戏实现一个 RL 代理（可能是 DQN）。我面临的问题是，据我所知，大多数 RL 框架都设计为主动部分，而游戏环境仅对代理的操作做出反应并提供反馈。我现在的问题是，是否可以反过来做？棋盘游戏（Cascadia）已经用 Java 实现，并为 AI 玩家提供了界面。因此，每当轮到代理时，我计划用 Python 对我的代理进行 REST 调用，提供编码的游戏状态和可能的移动，并得到“最佳”移动作为回报（Java 客户端决定何时调用代理）。这完全可能吗，还是我必须更改我的环境，以便 Python 代理可以成为主动部分？提前感谢您的帮助！   由    /u/ItchyRoyal212  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g84lzs/rl_agent_in_python_for_board_game_in_java/</guid>
      <pubDate>Sun, 20 Oct 2024 17:53:34 GMT</pubDate>
    </item>
    <item>
      <title>有人读过 Bigger, Regularized, Optimistic (BRO) 论文吗？我对论文中的一些内容不太理解。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7ta07/has_anyone_read_the_bigger_regularized_optimistic/</link>
      <description><![CDATA[      https://preview.redd.it/lgumivd24vvd1.png?width=909&amp;format=png&amp;auto=webp&amp;s=67b981aa8b692bdf7136430b79415866ff577ab5 论文中明确指出使用了 Quantile Critic，但从附录中我看到的内容来看，它更像是 MSE。它不应该使用 Quantile Huber 损失，而不是仅对多个标量回归取平均值吗？    提交人    /u/New_East832   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7ta07/has_anyone_read_the_bigger_regularized_optimistic/</guid>
      <pubDate>Sun, 20 Oct 2024 07:23:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么 BBF 不使用 ReDo 来对抗休眠神经元？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7skg1/why_doesnt_bbf_use_redo_to_combat_dormant_neurons/</link>
      <description><![CDATA[在 BBF 论文 [1] 中，作者使用了 Shrink 和 Perturb [2] 等技术以及定期重置来解决可塑性损失和过度拟合等问题。但是，ReDo [3] 是一种专门设计用于回收休眠神经元并在整个训练过程中保持网络表达能力的方法，这似乎对较大的网络很有用。您认为 BBF 为什么不采用 ReDo 来对抗休眠神经元？ReDo 解决的问题与 BBF 架构和训练策略不太相关吗？BBF 作者一定知道这一点，因为他们中的一些人被列为 5 个月前发表的 ReDo 论文的作者。 很想听听社区的任何想法或见解！ [1] Schwarzer、Max、Johan Obando-Ceron、Aaron Courville、Marc Bellemare、Rishabh Agarwal 和 Pablo Samuel Castro。 “更大、更好、更快：具有人类级效率的人类级 Atari。” arXiv，2023 年 11 月 13 日。http://arxiv.org/abs/2305.19452。 [2] D’Oro, Pierluca、Max Schwarzer、Evgenii Nikishin、Pierre-Luc Bacon、Marc G Bellemare 和 Aaron Courville。“通过打破重放比率障碍实现样本高效强化学习”，2023 年。 [3] Sokar, Ghada、Rishabh Agarwal、Pablo Samuel Castro 和 Utku Evci。“深度强化学习中的休眠神经元现象。” arXiv，2023 年 6 月 13 日。 http://arxiv.org/abs/2302.12902。    由   提交  /u/two_armed_bandit   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7skg1/why_doesnt_bbf_use_redo_to_combat_dormant_neurons/</guid>
      <pubDate>Sun, 20 Oct 2024 06:31:22 GMT</pubDate>
    </item>
    <item>
      <title>PPO 帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7pjut/help_with_ppo/</link>
      <description><![CDATA[我正在使用 PPO（从稳定基线开始）开发汽车 AI，我是这方面的新手。过去 8 天我一直在研究这个问题。当时的环境包含汽车和它需要到达的随机点。我遇到的问题是汽车没有学会转向，我已经更改了超参数、奖励函数和许多其他函数，但它仍然在挣扎。我想我的价值函数也不太好用。以下是其他详细信息 观察：您的观察空间包含 11 个特征（模型的输入）：1. CarX – 汽车位置的 X 坐标。2. CarY – 汽车位置的 Y 坐标。3. CarVelocity – 汽车的速度（标准化）。4. CarRotation – 汽车的当前旋转（标准化）。 5. CarSteer – 汽车的转向角度（标准化）。 6. TargetX – 目标点的 X 坐标。 7. TargetY – 目标点的 Y 坐标。 8. TargetDistance – 汽车与目标之间的距离。 9. TargetAngle – 汽车方向与目标方向之间的角度（标准化）。 10. LocalX – 表示目标位于汽车的哪一侧，标准化： - 正数：目标在右侧。 - 负数：目标在左侧。 11. LocalY – 表示目标在汽车前方或后方的相对位置： - 负数：目标在前方。 - 正面：目标在后面。 动作： 动作空间包含两个输出： 1. 转向 - 控制汽车的转向： - -1：左转。 - 0：不转向。 - 1：右转。  加速 - 控制汽车的加速度：  0：不加速。 1：加速前进。   奖励：  对准奖励：汽车因与目标对准而获得正面奖励。这可能意味着汽车的方向和目标方向之间的角度 (TargetAngle) 很小，奖励更好的对齐。 速度和增量距离奖励：汽车根据其速度和与目标的距离变化 (增量距离) 获得奖励。如果汽车快速移动并缩短与目标的距离，则会给予积极奖励。 朝正确的方向转向：根据目标相对于汽车的位置 (LocalX/LocalY)，汽车因朝正确的方向转向而获得奖励。如果汽车朝目标转向（例如，当目标在左侧时向左转），它会得到积极的奖励。  请帮忙    提交人    /u/ItsTanPi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7pjut/help_with_ppo/</guid>
      <pubDate>Sun, 20 Oct 2024 03:08:54 GMT</pubDate>
    </item>
    <item>
      <title>DeepMind 2023 年财务报告：预算 15 亿英镑（+5 亿英镑）[约 19 亿美元，+6 亿美元]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7lh05/deepmind_2023_financial_filings_15_billion_budget/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7lh05/deepmind_2023_financial_filings_15_billion_budget/</guid>
      <pubDate>Sat, 19 Oct 2024 23:22:32 GMT</pubDate>
    </item>
    <item>
      <title>[付费] 需要有人写一篇关于线性二次 (LQ) 最优控制的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g7ivyn/paid_need_someone_to_do_a_paper_on/</link>
      <description><![CDATA[您好，我想找人帮我写一篇关于线性二次 (LQ) 最优控制强化学习机制的论文。我有更多细节可以在 DM 中分享。愿意为这项任务支付 100 美元。 相信我，我从来没有这样做过，但说实话，我应该在 3 年前完成这项作业，而现在我只想提交论文来获得班级成绩并获得学位。我实际上在班级考试中表现很好，只需要写这篇论文来完成手续。谢谢    提交人    /u/Used_Chapter007   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g7ivyn/paid_need_someone_to_do_a_paper_on/</guid>
      <pubDate>Sat, 19 Oct 2024 21:14:58 GMT</pubDate>
    </item>
    <item>
      <title>帮助-TD3 仅返回极值（即动作空间的边界值）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g77fs3/help_td3_only_returns_extreme_values_ie_bounding/</link>
      <description><![CDATA[嗨， 我对连续控制问题还不熟悉，由于我的背景，我更了解理论而不是实践方面。所以我正在训练一个基于 TD3 的代理来解决连续控制问题（在观察空间中交易带有情绪分数的多种资产）。  连续动作空间（如下所示）如下所示： Box([-1. -1. -1. -1. -1. -1. -1. -1. -1. 0. 0. 0. 0. 0. 0. 0. 0.], 1.0, (16,), float32) 解释一下：我在环境中交易 8 种资产，动作空间的前 8 个条目（范围从 -1 到 1）表示仓位（卖出、持有、买入 —— 在环境中从连续决策转化为离散决策），而后 8 个条目（范围从 0 到 1）表示动作的百分比金额（卖出仓位的百分比或用于买入动作的现金的百分比）。  我的模型目前在 100 集上进行训练（一集大约有 1250 个交易日/观察，大小为 81，这里只是简要介绍一下这个项目）。目前，代理只会无一例外地返回进入极端位置的动作（使用来自动作空间的边界值）。示例： [ 1. -1. 1. -1. 1. -1. -1. -1. 0. 0. 0. 1. 1. 0. 0. 0.] 我现在的问题是，在训练的早期阶段，这是否正常，或者这是否表明模型、环境或其他方面存在问题？由于在这样的环境中训练需要大量计算（= 成本高），我只想在训练（并可能支付）大量时间的训练之前澄清这是否可能是代码/算法本身的问题。    提交人    /u/Intelligent-Put1607   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g77fs3/help_td3_only_returns_extreme_values_ie_bounding/</guid>
      <pubDate>Sat, 19 Oct 2024 12:07:16 GMT</pubDate>
    </item>
    <item>
      <title>与我一起学习/合作从零开始学习 DRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g713de/study_collab_with_me_learning_drl_from_almost/</link>
      <description><![CDATA[大家好 👋 我几乎从零开始学习 DRL。对 NN、反向传播、LSTM 有一些了解，并使用我在互联网上能找到的任何东西（非常简单的东西）制作了一些模型。没有 SOTA。现在从“grokking DRL”这本书中学习。我有一种不同的方法来设计交易引擎，我正在用 golang（为了提高效率和扩展性）和 python（用于 ML 部分）构建它，还有很多东西需要解开。我认为我有一些关于交易的有趣想法可以在 DRL、LSTM 和 NEAT 中测试，但至少需要 6-8 个月才能产生任何有成效的东西。我正在寻找好奇的人一起工作。如果您愿意研究一些新的假设，只需按下 DM。我希望得到一些关于 DRL 的指导，理解已完成工作背后的所有理论非常耗时。 PS：如果您很了解这些内容并希望提供帮助，我可以在任何程度上帮助您处理数据结构、Web 开发、系统设计，如果您愿意的话。只是说说而已。    提交人    /u/WarBroWar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g713de/study_collab_with_me_learning_drl_from_almost/</guid>
      <pubDate>Sat, 19 Oct 2024 04:26:32 GMT</pubDate>
    </item>
    <item>
      <title>演员 评论家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6mlv2/actor_critic/</link>
      <description><![CDATA[https://arxiv.org/abs/1704.03732 是否有任何演员-评论家类似物可以将专家演示整合到演员-评论家学习中，就像 DQN 一样？    提交人    /u/Key-Faithlessness113   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6mlv2/actor_critic/</guid>
      <pubDate>Fri, 18 Oct 2024 16:40:32 GMT</pubDate>
    </item>
    </channel>
</rss>