<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Sat, 08 Jun 2024 12:24:55 GMT</lastBuildDate>
    <item>
      <title>æˆ‘çš„åŠ¨ä½œç©ºé—´æœ‰ 90% è¢«æ©ç›–äº†ï¼Œæˆ‘å¸Œæœ›ä»ä¸­è·å¾—è®¡ç®—ä¸Šçš„å¥½å¤„</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</link>
      <description><![CDATA[æˆ‘çš„ q-learning ä»»åŠ¡ä¸­æœ‰ä¸€ä¸ªåŒ…å« 5000 ä¸ªåŠ¨ä½œçš„å¤§å‹åŠ¨ä½œç©ºé—´ï¼Œä½†åœ¨ä»»ä½•æ—¶å€™éƒ½åªæœ‰å‡ ç™¾ä¸ªåˆæ³•åŠ¨ä½œï¼Œå› æ­¤æˆ‘ä¼šå±è”½å®ƒä»¬ï¼Œä½†æ˜¯ï¼Œæˆ‘åªæ˜¯åœ¨æ¨¡å‹é¢„æµ‹æ­¥éª¤ä¹‹åè¿›è¡Œå±è”½ã€‚æœ‰æ²¡æœ‰åŠæ³•äº‹å…ˆå±è”½å®ƒå¹¶è·å¾—å¤§å¹…åŠ é€Ÿï¼Ÿ  def get_predictions(self, state, legal_mask): state = np.reshape(state, [1, self.state_size) act_values = [act_values[i] if legal_mask[i] == 1 else -np.inf for i in range(len(act_values))] return act_values     submitted by    /u/Breck_Emert   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</guid>
      <pubDate>Sat, 08 Jun 2024 07:03:17 GMT</pubDate>
    </item>
    <item>
      <title>[CfP] ä¸ RealAIGym åˆä½œä¸¾åŠç¬¬äºŒå±Šäººå·¥æ™ºèƒ½å¥¥è¿ä¼šï¼šIROS 2024 æœºå™¨äººç«èµ›â€”â€”ç«‹å³åŠ å…¥ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dahea9/cfp_2nd_ai_olympics_with_realaigym_robotics/</link>
      <description><![CDATA[        æäº¤äºº    /u/Dense-Positive6651   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dahea9/cfp_2nd_ai_olympics_with_realaigym_robotics/</guid>
      <pubDate>Fri, 07 Jun 2024 18:04:52 GMT</pubDate>
    </item>
    <item>
      <title>è®¡ç®—ä¸¤ä¸ª Q å­¦ä¹ ç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dacrpc/calculating_kldivergence_between_two_qlearning/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æƒ³è®¡ç®—ä½¿ç”¨ Q-learning è®­ç»ƒçš„ä¸¤ä¸ªç­–ç•¥ä¹‹é—´çš„ KL æ•£åº¦ã€‚ç”±äº Q-learning æ ¹æ®æœ€é«˜ Q å€¼é€‰æ‹©åŠ¨ä½œï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒï¼Œè¿™äº›ç­–ç•¥æ˜¯å¦åº”è¯¥è¡¨ç¤ºä¸ºç‹¬çƒ­å‘é‡ï¼Ÿå¦‚æœæ˜¯è¿™æ ·ï¼Œè€ƒè™‘åˆ°ç‹¬çƒ­å‘é‡ä¸­æ¦‚ç‡ä¸ºé›¶çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¦‚ä½•è®¡ç®— KL æ•£åº¦ï¼Ÿ    æäº¤äºº    /u/Sea-Collection-8844   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dacrpc/calculating_kldivergence_between_two_qlearning/</guid>
      <pubDate>Fri, 07 Jun 2024 14:55:45 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ ä¸­å¯ä»¥ä»äº‹å“ªäº›å·¥ä½œ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dacmz0/which_jobs_in_reinforcement_learning/</link>
      <description><![CDATA[å˜¿ï¼Œ ç”±äºæˆ‘å¯¹å¼ºåŒ–å­¦ä¹ è¶Šæ¥è¶Šæ„Ÿå…´è¶£ï¼Œæˆ‘è¯•å›¾äº†è§£å¦‚ä½•å¯»æ‰¾ä½¿ç”¨å®ƒçš„å·¥ä½œã€‚ å¤§å¤šæ•°æ•°æ®ç§‘å­¦å®¶ã€mlopsã€æ•°æ®å·¥ç¨‹å¸ˆèŒä½ä¼¼ä¹éƒ½åœ¨ä½¿ç”¨ç›‘ç£æœºå™¨å­¦ä¹ ï¼Œä¹Ÿè®¸åœ¨è¥é”€ç­‰æ–¹é¢ä½¿ç”¨æ— ç›‘ç£å­¦ä¹ ã€‚ é™¤äº†æœºå™¨äººå…¬å¸ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•ä½¿ç”¨ RL çš„èŒä½æè¿°ã€‚ ä½ å¦‚ä½•å¯»æ‰¾å¯ä»¥åº”ç”¨ RL çš„å·¥ä½œï¼Ÿ æ­¤å¤–ï¼ŒRL åœ¨æ¸¸æˆå’Œæœºå™¨äººèƒŒåçš„åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ æˆ‘å°†åˆ—å‡ºå‡ ä¸ªæˆ‘æ„Ÿå…´è¶£çš„ä¸»é¢˜ï¼Œä½†æˆ‘ä¸ç¡®å®šéœ€è¦åº”ç”¨å“ªç§ç±»å‹çš„æœºå™¨å­¦ä¹ æ¥è§£å†³è¿™äº›é—®é¢˜ï¼š  äº¤é€šä¼˜åŒ–ï¼Œ æ”¿åºœé¢„ç®—ä¼˜åŒ– æ‹¥æœ‰æ›´â€œå®Œæ•´â€è§†é¢‘æ¸¸æˆä½“éªŒ - å³ä¸ NPC åˆä½œï¼Œå¦‚ä½•ä¸ä»–ä»¬äº’åŠ¨ç­‰ã€‚  æ¨¡æ‹Ÿç¯å¢ƒï¼ˆä¸çŸ¥é“æ˜¯å¦å­˜åœ¨ï¼Œä½†å‡è®¾æ‚¨å°è¯•æ¨¡æ‹ŸæŸé¡¹æ–°æ³•å¾‹å°†å¯¹äººå£äº§ç”Ÿä»€ä¹ˆå½±å“ï¼Ÿï¼‰ æœºå™¨äººæŠ€æœ¯     æäº¤äºº    /u/BoxingBytes   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dacmz0/which_jobs_in_reinforcement_learning/</guid>
      <pubDate>Fri, 07 Jun 2024 14:50:11 GMT</pubDate>
    </item>
    <item>
      <title>è§„åˆ™æ‰‹å†Œâ€”â€”ç”¨äºç¼–å†™é«˜æ•ˆè¾“å…¥å¯†é›†å‹ç¯å¢ƒçš„ DSL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dabtbu/rulebook_a_dsl_to_write_efficient_input_intensive/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘æ˜¯ rulebook çš„ä¸»è¦å¼€å‘äººå‘˜ã€‚ Rulebook è¯­è¨€æ—¨åœ¨ç®€åŒ–å¤æ‚æœºå™¨å­¦ä¹ ç¯å¢ƒçš„å¼€å‘ã€ç»´æŠ¤å’Œå¯é‡ç”¨æ€§ï¼Œæ— è®ºæ˜¯åœ¨è®­ç»ƒæœŸé—´è¿˜æ˜¯è®­ç»ƒä¹‹åã€‚ ä»æ¦‚å¿µä¸Šè®²ï¼Œæ‚¨å¯ä»¥å°†æ­¤å·¥å…·è§†ä¸º&quot;æ‚¨ç¼–å†™æ‰€éœ€çš„ gym ç¯å¢ƒçš„éå¸¸é«˜çº§çš„ä¼ªä»£ç ï¼Œç„¶å `rulebook` è¿”å›ä¸€ä¸ªç”¨ C ç¼–å†™çš„ä¼˜åŒ– gym ç¯å¢ƒï¼Œå¹¶åŒ…è£…åœ¨ python ä¸­&quot; ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ­£åœ¨ç”¨ä½œæµ‹è¯•çš„çœŸå®ä¸–ç•Œç°æˆçš„æ£‹ç›˜æ¸¸æˆï¼Œå…¶ä¸­åŒ…æ‹¬åŠ¨æ€åˆ—è¡¨å¹¶å…·æœ‰ 1000 å¤šä¸ªåŠ¨ä½œï¼ˆåœ¨å½“ç„¶ï¼Œä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç»„ï¼‰å¯ä»¥ç”¨~1000 è¡Œä»£ç ï¼ˆä¸åŒ…æ‹¬æµ‹è¯•ï¼‰æ¥ç¼–å†™ã€‚è¯¥æ¸¸æˆä¸åŒ…å«ä¸€è¡Œæœºå™¨å­¦ä¹ ä¸“ç”¨ä»£ç ï¼Œä½†æ—¢å¯ç”¨äºæœºå™¨å­¦ä¹ ï¼Œä¹Ÿå¯ç”¨äºå…¶ä»–ç›®çš„ï¼ˆä¾‹å¦‚ï¼Œå°†å…¶éƒ¨ç½²åœ¨æ¸¸æˆå¼•æ“ä¸­ï¼‰ã€‚ åœ¨è¯¥æ¸¸æˆä¸­æ·»åŠ å’Œåˆ é™¤åŠ¨ä½œå°±åƒåœ¨å¯ä»¥æ‰§è¡Œæ­¤ç±»åŠ¨ä½œçš„ä»£ç ç‚¹æ·»åŠ å’Œåˆ é™¤ä»¥ä¸‹ä»£ç ä¸€æ ·ç®€å•ã€‚  act toggle_door(UnitArgType unit_id) { # action board.unit_id_is_valid(unit_id.value), # precondition board.units.get(unit_id.value).action_points != 0, board.is_facing_door(board.units.get(unit_id.value)) } ref unit = board.units.get(unit_id.value) # effect unit.action_points = unit.action_points - 1 board.toggle_door(unit)  æ‚¨å¯ä»¥é˜…è¯»æœ‰å…³è¯­è¨€åŸç†è¿™é‡Œ ã€‚ æ‚¨å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æœ‰å…³æ›´å¤§çš„é¡¹ç›®åŸç†ã€‚ æ‚¨å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æˆ‘ä»¬å¦‚ä½•åœ¨ç°æˆçš„æ¸¸æˆä¸­ä½¿ç”¨å®ƒã€‚ æ‚¨å¯ä»¥åœ¨è¿™é‡Œå°è¯•å®ƒï¼ˆå¦‚æœæ‚¨å·²ç»çŸ¥é“ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Œè¯·è·³è¿‡ä»‹ç»å¹¶è½¬åˆ°å…³äºæœ¬æ–‡æ¡£éƒ¨åˆ†ï¼‰ã€‚æ‚¨éœ€è¦ä¸€å°è£…æœ‰ python3.8 çš„ linux x64 æœºå™¨æ‰èƒ½æ‰§è¡Œæ­¤æ“ä½œã€‚ æœ¬æ•™ç¨‹åˆšåˆšåˆ›å»ºï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå±•ç¤ºå®ƒä»¥æ”¶é›†æœ‰å…³å®ƒå’Œé¡¹ç›®çš„åé¦ˆã€‚å› æ­¤æ¬¢è¿ä»»ä½•ç±»å‹çš„åé¦ˆã€‚    æäº¤äºº    /u/drblallo   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dabtbu/rulebook_a_dsl_to_write_efficient_input_intensive/</guid>
      <pubDate>Fri, 07 Jun 2024 14:15:42 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ Rllib è‡ªå®šä¹‰ç¯å¢ƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1daar1d/custom_env_with_rllib/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ˜¯ RL ç¤¾åŒºçš„æ–°æ‰‹ï¼Œæˆ‘æ­£åœ¨å°è¯•å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Rllibï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™æ˜¯ä¸€æ¬¡ä¸å¹¸çš„æ—…ç¨‹ æˆ‘åšäº†ä¸€ä¸ªç®€å•çš„æ¸¸æˆï¼Œä½†æˆ‘åœ¨è¿™é‡Œæ·»åŠ å¹¶ç®€åŒ–äº†å®ƒï¼Œä»¥é˜²ä½ ä»¬ä¸­çš„ä¸€äº›äººæƒ³å¸®å¿™ï¼Œå±å¹•ä¸Šéšæœºç”Ÿæˆä¸€ä¸ªåœ†åœˆï¼Œç„¶åä½ å¿…é¡»ç‚¹å‡»å®ƒé‡Œé¢æ‰èƒ½èµ¢ï¼Œä½ æœ€å¤šæœ‰ 10 æ¬¡æœºä¼šç‚¹å‡»å®ƒé‡Œé¢ï¼Œå¦åˆ™ä½ å°±è¾“äº†ï¼Œç¯å¢ƒä¼šè¢«é‡ç½®ï¼ˆåœ†åœˆæ˜¯éšæœºé‡æ–°ç”Ÿæˆçš„ï¼‰ æˆ‘æ­£åœ¨åŠªåŠ›å®šä¹‰ç¯å¢ƒå¹¶ä½¿ç”¨ PPO æ¥è®­ç»ƒå®ƒï¼Œæˆ‘åªæ˜¯æ— æ³•æ¯«æ— é”™è¯¯åœ°å®ç°å®ƒï¼Œå³ä½¿å®ƒçœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½ ä»¬èƒ½å¸®å¿™å¦‚ä½•å®ç°å®ƒå¹¶å°†å®ƒè¿æ¥åˆ°æ¸¸æˆå—ï¼Ÿ æˆ‘åªå¸Œæœ›å®ƒèƒ½å·¥ä½œï¼Œä¹‹åæˆ‘ä¼šæ ¹æ®æˆ‘çš„éœ€è¦è°ƒæ•´å®ƒï¼Œ å¦‚æœä½ ä»¬æœ‰æœ‰å…³å¦‚ä½•ä½¿ç”¨ rllib åˆ¶ä½œè‡ªå®šä¹‰ç®€å•æ¸¸æˆçš„ç¤ºä¾‹ï¼Œæˆ‘å¾ˆæƒ³çœ‹çœ‹å®ƒï¼Œå› ä¸ºæˆ‘è¿˜æ²¡èƒ½æ‰¾åˆ°æœ€è¿‘çš„ä¸œè¥¿ã€‚éå¸¸æ„Ÿè°¢  import cv2 import numpy as np import random class CircleGame: def __init__(self): self.img = None self.center = None self.radius = None self.attempts = 0 self.done = False self.action = None self.reset() def draw_random_circle(self): height, width, _ = self.img.shape radius = random.randint(20, 50) center = (random.randint(radius, width - radius), random.randint(radius, height - radius)) color = (0, 255, 0) thick = 2 cv2.circle(self.img, center, radius, color, thick) return center, radius def is_inside_circle(self, point): return (point[0] - self.center[0]) ** 2 + (point[1] - self.center[1]) ** 2 &lt;= self.radius ** 2 def reset(self): self.img = np.ones((600, 800, 3), dtype=&quot;uint8&quot;) * 255 self.center, self.radius = self.draw_random_circle() self.attempts = 10 self.done = False self.action = None return self.get_state() def step(self, action): if self.done: raise ValueError(&quot;æ¸¸æˆå·²å®Œæˆã€‚è¯·é‡ç½®ç¯å¢ƒã€‚&quot;) point = action if self.is_inside_circle(point): reward = 1.0 self.done = True print(&quot;ä½ èµ¢äº†ï¼&quot;) else: self.attempts -= 1 reward = -0.1 color = (0, 0, 255) cv2.circle(self.img, point, 5, color, -1) # åœ¨çŒœçš„ç‚¹å¤„ç”»ä¸€ä¸ªå°åœ†åœˆ if self.attempts == 0: self.done = True reward = -1.0 æ‰“å°ï¼ˆâ€œä½ è¾“äº†ï¼é‡æ–°å¼€å§‹æ¸¸æˆã€‚â€) return self.get_state(), reward, self.done, {} def get_state(self): return self.img.copy(), self.attempts def render(self): cv2.imshow(&#39;Game&#39;, self.img) cv2.setMouseCallback(&#39;Game&#39;, self.mouse_callback) cv2.waitKey(1) def mouse_callback(self, event, x, y, flags, param): if event == cv2.EVENT_LBUTTONDOWN: self.action = (x, y) if not self.done: state, reward, done, _ = self.step(self.action) print(f&quot;Reward: {reward}, Done: {done}&quot;) if done: self.reset() def close(self): cv2.destroyAllWindows() if __name__ == &quot;__main__&quot;: game = CircleGame() game.reset() while True: game.render() if game.done: game.reset() key = cv2.waitKey(1) &amp; 0xFF if key == 27: # æŒ‰ ESC é€€å‡ºæ¸¸æˆ break game.close()     æäº¤äºº    /u/Snoo37129   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1daar1d/custom_env_with_rllib/</guid>
      <pubDate>Fri, 07 Jun 2024 13:31:07 GMT</pubDate>
    </item>
    <item>
      <title>ç¨³å®šåŸºçº¿è®­ç»ƒæ­¥éª¤ä¸é¢„æœŸå­˜åœ¨å·®å¼‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da8fq9/discrepency_in_stable_baselines_training_steps_vs/</link>
      <description><![CDATA[æ‚¨å¥½ï¼Œ æˆ‘æ˜¯ RL æ–°æ‰‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªè‡ªå®šä¹‰è¿ç»­ 2D è¿·å®«ç¯å¢ƒã€‚æˆ‘æƒ³è®­ç»ƒä¸€ä¸ªä»£ç†æ¥å¯¼èˆªè¿·å®«å¹¶åˆ°è¾¾ç›®æ ‡ã€‚æˆ‘çš„ç¯å¢ƒ max_timesteps è®¾ç½®ä¸º 500ã€‚æˆ‘æ­£åœ¨ä½¿ç”¨ç¨³å®šåŸºçº¿ PPO è¿›è¡Œè®­ç»ƒã€‚æˆ‘çš„å‚æ•°æ˜¯ï¼š # config.yaml ppo: policy: &#39;MlpPolicy&#39; n_steps: 250 learning_rate: 0.0003 batch_size: 10 n_epochs: 10 gamma: 0.99 gae_lambda: 0.95 clip_range: 0.2 ent_coef: 0.01 vf_coef: 0.5 max_grad_norm: 0.5 use_sde: true sde_sample_freq: -1 tensorboard_log: &quot;./ppo_tensorboard/&quot; seed: 123 device: &quot;cpu&quot;  é—®é¢˜æ˜¯ï¼Œæ— è®ºæˆ‘å‘Šè¯‰ model.learn è®­ç»ƒå¤šå°‘æ­¥ï¼Œæˆ‘éƒ½ä¼šè¿™æ ·åšã€‚ä¾‹å¦‚ï¼Œæˆ‘è®­ç»ƒäº† model.learn(total_timesteps=10000)ã€‚å¯¹äºæ¯æ¬¡è¿­ä»£æœ€å¤š 500 æ­¥çš„ 10,000 ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘é¢„è®¡ä¼šæœ‰ 20 æ¬¡è¿­ä»£ï¼Œä½†æ­¤å›è°ƒæŒ‡ç¤º 4 æ¬¡è¿­ä»£å’Œ 12000 ä¸ªæ—¶é—´æ­¥ã€‚çŸ¥é“æ˜¯ä»€ä¹ˆåŸå› é€ æˆçš„å—ï¼Ÿ ----------------------------------------------------- | rollout/ | | | ep_len_mean | 480 | | ep_rew_mean | -2.77e+03 | | time/ | | | fps | 83 | | iterations | 4 | | time_elapsed | 143 | | total_timesteps | 12000 | | train/ | | | approx_kl | 0.021791738 | | clip_range | 0.2 | | entropy_loss | -9.48 | | explained_variance | 0.0275 | | learning_rate | 0.0003 | | loss | 4.38e+04 | | n_updates | 30 | | policy_gradient_loss | -0.109 | | std | 0.998 | | value_loss | 3.06e+04 | -----------------------------------------     æäº¤äºº    /u/lujan-002   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da8fq9/discrepency_in_stable_baselines_training_steps_vs/</guid>
      <pubDate>Fri, 07 Jun 2024 11:34:28 GMT</pubDate>
    </item>
    <item>
      <title>Unity ML-Agents ä¸ Unreal å­¦ä¹ ä»£ç†</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da7j72/unity_mlagents_vs_unreal_learning_agents/</link>
      <description><![CDATA[å¯¹äºæˆ‘å³å°†åˆ°æ¥çš„å­¦å£«é¡¹ç›®ï¼Œæˆ‘å’Œæˆ‘çš„å›¢é˜Ÿå†³å®šå¼€å‘ä¸€æ¬¾å¸¦æœ‰ RL ä»£ç†çš„æ¸¸æˆã€‚æ¸¸æˆæ¦‚å¿µå°šæœª 100% ç¡®å®šã€‚æˆ‘ä»¬ç°åœ¨çš„ä¸»è¦é—®é¢˜æ˜¯å†³å®šæˆ‘ä»¬åº”è¯¥ä½¿ç”¨å“ªç§æ¸¸æˆå¼•æ“ã€‚ä½ ä»¬å½“ä¸­æœ‰è°åŒæ—¶ä½¿ç”¨è¿‡ Unity ML-Agents å’Œ Unreal çš„å­¦ä¹ ä»£ç†ï¼Œå¯ä»¥å¯¹è¿™ä¸¤ä¸ªç¯å¢ƒè¿›è¡Œç®€å•çš„æ¯”è¾ƒå—ï¼Ÿ å°±æˆ‘ä¸ªäººè€Œè¨€ï¼Œæˆ‘æ›´å–œæ¬¢ Unity çš„æ¡†æ¶ï¼Œå› ä¸ºæˆ‘æ‹…å¿ƒ Unreal çš„å­¦ä¹ ä»£ç†è¿˜â€œä¸å¤Ÿå¥½â€ã€‚    æäº¤äºº    /u/Cuuuubee   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da7j72/unity_mlagents_vs_unreal_learning_agents/</guid>
      <pubDate>Fri, 07 Jun 2024 10:38:50 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨ RL ä¸­å¯è§†åŒ–å’Œæ¯”è¾ƒ PPO ä¸ REINFORCE çš„çŠ¶æ€å’ŒåŠ¨ä½œçš„æœ€ä½³æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da5lig/best_ways_to_visualize_and_compare_states_and/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨ä½¿ç”¨ LunarLanderContinuous-v2 ç¯å¢ƒæ¯”è¾ƒ PPO å’Œ REINFORCEã€‚åœ¨è®­ç»ƒå®Œä¸¤ç§ç®—æ³•ä¹‹åï¼Œæˆ‘å¸Œæœ›æœ‰æ•ˆåœ°å¯è§†åŒ–å’Œæ¯”è¾ƒå®ƒä»¬æ‰€é‡‡å–çš„çŠ¶æ€å’Œæ“ä½œã€‚ å…·ä½“æ¥è¯´ï¼Œæˆ‘æ­£åœ¨å¯»æ‰¾ä»¥ä¸‹æ–¹é¢çš„å»ºè®®ï¼š  å¦‚ä½•å¯è§†åŒ–æ¯ä¸ªç®—æ³•è®¿é—®çš„çŠ¶æ€åˆ†å¸ƒã€‚ å¦‚ä½•è¡¨ç¤ºæ¯ä¸ªç®—æ³•æ‰€é‡‡å–çš„æ“ä½œä¸­çš„å¤šæ ·æ€§å’Œæ¨¡å¼ã€‚ ä»»ä½•å¯ä»¥å¸®åŠ©ä½¿è¿™äº›æ¯”è¾ƒæ¸…æ™°ä¸”å¯Œæœ‰æ´å¯ŸåŠ›çš„å·¥å…·æˆ–æŠ€æœ¯ã€‚  æˆ‘å·²ç»åœ¨è®­ç»ƒæœŸé—´è®°å½•äº†çŠ¶æ€å’ŒåŠ¨ä½œï¼Œä½†æˆ‘æ­£åœ¨åŠªåŠ›å¦‚ä½•ä»¥æœ‰æ„ä¹‰çš„æ–¹å¼å‘ˆç°è¿™äº›æ•°æ®ï¼Œè‡³å°‘æˆ‘çœŸçš„æ²¡æœ‰ä¸€ä¸ªæ¸…æ™°çš„æƒ³æ³•ã€‚ æˆ‘è¿˜å¯ä»¥å±•ç¤ºé™¤ç´¯ç§¯å¥–åŠ±ä¹‹å¤–çš„å…¶ä»–å·®å¼‚ï¼Œä½†æˆ‘çœŸçš„ä¸çŸ¥é“æ˜¯ä»€ä¹ˆã€‚ è°¢è°¢ï¼    æäº¤äºº    /u/An4rcyst   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da5lig/best_ways_to_visualize_and_compare_states_and/</guid>
      <pubDate>Fri, 07 Jun 2024 08:19:00 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘åº”è¯¥å¦‚ä½•åœ¨æœºå™¨äººè…¿å¹³è¡¡ä»»åŠ¡ä¸­å®ç°å¥–åŠ±/é‡ç½®ç³»ç»Ÿï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9znxi/how_should_i_implement_the_rewardsreset_system_in/</link>
      <description><![CDATA[å¤§å®¶å¥½ã€‚æˆ‘è®¾è®¡äº†ä¸€ä¸ªæœºå™¨äººè…¿ï¼Œå¸Œæœ›å°†æ¥èƒ½æ•™å®ƒå¦‚ä½•èµ°è·¯ï¼ˆæ˜¾ç„¶æ˜¯ 2ï¼‰ï¼Œä½†ç°åœ¨æˆ‘æ­£å°è¯•ä½¿ç”¨ Omniverse Isaac Gym å’Œç¨³å®šåŸºçº¿ 3 æ¥æ•™ä¸€æ¡è…¿å¦‚ä½•åœ¨è„šä¸Šä¿æŒå¹³è¡¡ï¼Œåªæ˜¯ä¸ºäº†äº†è§£ RL å¹¶çœ‹çœ‹æˆ‘çš„å…³èŠ‚æ˜¯å¦è¶³å¤Ÿå¼ºå£®ä»¥ä¿æŒè…¿çš„æŠ¬èµ·ã€‚æˆ‘æƒ³è¯´çš„æ˜¯ï¼Œæˆ‘æ˜¯ä¸€ä¸ªåˆå­¦è€…ï¼Œé™¤äº†è®­ç»ƒæ¨è½¦æ†ã€äººå½¢æœºå™¨äººå’Œèš‚èšçš„ä¾‹å­å¤–ï¼Œæ²¡æœ‰å¤ªå¤šç»éªŒã€‚ æˆ‘çš„ä»»åŠ¡æœ‰ç‚¹å¥æ•ˆäº†ï¼Œä½†æˆ‘è®¤ä¸ºåœ¨å¦‚ä½•é‡ç½®è…¿ä»¥åŠå¦‚ä½•è®¡ç®—å¥–åŠ±å’Œæƒ©ç½šæ–¹é¢å­˜åœ¨ä¸€äº›æ˜æ˜¾çš„é—®é¢˜ã€‚ä½†æˆ‘è¿˜æ²¡æœ‰èµ°å¾—è¶³å¤Ÿè¿œæ¥èƒ½å¤Ÿå‘Šè¯‰ æˆ‘çš„é—®é¢˜æ˜¯ï¼Œç»è¿‡å‡ æ¬¡é‡ç½®åï¼Œæœºå™¨äººå°±ä¼šé£èµ·æ¥ï¼Œæ•´ä¸ªæ¨¡æ‹ŸåŸºæœ¬ä¸Šå°±å´©æºƒäº†......æˆ‘çš„é‡ç½®æ–¹æ³•æ˜¯ï¼Œä¸€æ—¦æœºå™¨äººæ‰è½ï¼ˆè„šä¸Šçš„æ¥è§¦ä¼ æ„Ÿå™¨è¾¾åˆ°æŸä¸ªå€¼ä»¥ä¸‹ï¼‰ï¼Œä»£ç å°±ä¼šå°†æœºå™¨äººé‡ç½®/æƒ©ç½šåˆ°å®ƒçš„èµ·å§‹ä¸–ç•Œä½ç½®ï¼Œå…³èŠ‚è®¾ç½®ä¸º 0ã€‚ä¹Ÿè®¸è¿™ä¸æ˜¯æ­£ç¡®çš„æ–¹æ³•ï¼Ÿä¹Ÿè®¸æˆ‘åº”è¯¥è®©å®ƒä¿æŒåœ¨è·Œå€’çš„åœ°æ–¹ï¼Œç„¶åè¯•ç€è®©å®ƒå­¦ä¼šå¦‚ä½•é‡æ–°ç«™èµ·æ¥ï¼Ÿæˆ‘ä¸çŸ¥é“.. æˆ‘ä»äººå½¢è®­ç»ƒä»»åŠ¡ä¸­è§‚å¯Ÿåˆ°ï¼Œæœºå™¨äººæ¯æ¬¡æ‰è½æ—¶éƒ½ä¼šé‡ç½®å¹¶ä»åŸæ¥çš„åœ°æ–¹å¼€å§‹ï¼Œä½†ä¹Ÿè®¸æˆ‘ä¸æ˜ç™½åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ ä»»ä½•å¸®åŠ©éƒ½å°†ä¸èƒœæ„Ÿæ¿€.. æˆ‘è¿˜æƒ³æä¸€ä¸‹ï¼Œæˆ‘ç¡®å®å°è¯•è¿‡ pybulletï¼Œä½†å®ƒæ— æ³•æ­£ç¡®åŠ è½½æˆ‘çš„ URDF..ä¸çŸ¥é“ä¸ºä»€ä¹ˆğŸ¤·â€â™‚ï¸ã€‚   ç”±    /u/StoryReader90  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9znxi/how_should_i_implement_the_rewardsreset_system_in/</guid>
      <pubDate>Fri, 07 Jun 2024 02:11:56 GMT</pubDate>
    </item>
    <item>
      <title>æ·±åº¦å­¦ä¹ é¡¹ç›®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9wxf7/deep_learning_projects/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨æ”»è¯»æ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½ç†ç§‘ç¡•å£«å­¦ä½ã€‚æˆ‘å°†äº 2025 å¹´ 4 æœˆæ¯•ä¸šã€‚æˆ‘æ­£åœ¨å¯»æ‰¾æ·±åº¦å­¦ä¹ é¡¹ç›®çš„æƒ³æ³•ã€‚1) ä¸º LLM å®æ–½çš„æ·±åº¦å­¦ä¹  2) ä¸º CVision å®æ–½çš„æ·±åº¦å­¦ä¹  æˆ‘åœ¨ç½‘ä¸ŠæŸ¥äº†ä¸€ä¸‹ï¼Œä½†å¤§å¤šæ•°éƒ½æ˜¯éå¸¸æ ‡å‡†çš„é¡¹ç›®ã€‚æ¥è‡ª Kaggle çš„æ•°æ®é›†æ˜¯é€šç”¨çš„ã€‚æˆ‘å¤§çº¦æœ‰ 12 ä¸ªæœˆçš„æ—¶é—´ï¼Œæˆ‘æƒ³åšä¸€äº›å¥½çš„ç ”ç©¶çº§é¡¹ç›®ï¼Œå¯èƒ½åœ¨ NeuraIPS ä¸Šå‘è¡¨å®ƒã€‚æˆ‘çš„ä¼˜åŠ¿æ˜¯æˆ‘æ“…é•¿è§£å†³é—®é¢˜ï¼Œä¸€æ—¦ç¡®å®šäº†é—®é¢˜ï¼Œä½†æˆ‘ä¸æ“…é•¿è¯†åˆ«å’Œæ„å»ºé—®é¢˜..ç›®å‰ï¼Œæˆ‘æ­£åœ¨å°è¯•åˆ¤æ–­ä»€ä¹ˆæ˜¯ä¸€ä¸ªå¥½çš„ç ”ç©¶é¢†åŸŸï¼Ÿ    æäº¤äºº    /u/Rogue260   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9wxf7/deep_learning_projects/</guid>
      <pubDate>Thu, 06 Jun 2024 23:53:15 GMT</pubDate>
    </item>
    <item>
      <title>â€œå¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯¹é½çš„åŸºæœ¬é™åˆ¶â€ï¼ŒWolf ç­‰äºº 2023ï¼ˆæç¤ºå¯¹åŠ¨ä½œä¸å®‰å…¨åéªŒçš„å…ˆéªŒï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9le1u/fundamental_limitations_of_alignment_in_large/</link>
      <description><![CDATA[        æäº¤äºº    /u/gwern   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9le1u/fundamental_limitations_of_alignment_in_large/</guid>
      <pubDate>Thu, 06 Jun 2024 15:47:15 GMT</pubDate>
    </item>
    <item>
      <title>é£é™©ï¼šç»Ÿæ²»å…¨çƒ ä½“è‚²é¦†è§‚æ™¯ç©ºé—´è®¾è®¡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kzfp/risk_global_domination_gymnasium_observation/</link>
      <description><![CDATA[å¤§å®¶å¥½ æˆ‘ä¸€ç›´åœ¨æ„å»ºä¸€ä¸ª Risk: å…¨çƒç»Ÿæ²»ç¯å¢ƒã€‚æˆ‘çš„ç›®æ ‡æ˜¯ç¨åä½¿ç”¨ RL å’Œ DQN æ¥æ„å»ºæœºå™¨äººã€‚ æˆ‘æ˜¯ RL æ–°æ‰‹ã€‚ æˆ‘ç›®å‰å¤„äºä¸€ä¸ªé˜¶æ®µï¼Œæˆ‘åœ¨æœ¬åœ°è¿è¡Œä¸€ä¸ªç®€å•ç‰ˆæœ¬çš„æ¸¸æˆï¼Œç¡®å®šæ€§æœºå™¨äººç›¸äº’å¯¹æˆ˜ã€‚æœ‰ 4 ä¸ªé¢†åœ°å’Œ 2 ä¸ªç©å®¶ æˆ‘ç°åœ¨æƒ³è¦å®Œæˆçš„æ˜¯å°†å…¶è½¬æ¢ä¸º Gymnasium ç¯å¢ƒï¼Œå¹¶å®æ–½å¼ºåŒ–å­¦ä¹ ï¼Œä¸“æ³¨äºå•ä¸€è¡ŒåŠ¨å¯èƒ½æ€§ï¼šé€‰æ‹©æ”»å‡»å“ªä¸ªé¢†åœ°ã€‚æ‰€æœ‰å…¶ä»–è¡ŒåŠ¨éƒ½å°†ç¡®å®šæ€§åœ°å¤„ç†ã€‚è¿™æ˜¯ä¸ºäº†ç®€åŒ–å®æ–½ï¼Œæˆ‘å¸Œæœ›è¿™æ˜¯ä¸€ä¸ªæ˜æ™ºçš„ä¸¾æªã€‚ æˆ‘ç°åœ¨éœ€è¦å®šä¹‰è§‚å¯Ÿç©ºé—´ã€‚å¯¹äºé‚£äº›ä¸ç†Ÿæ‚‰ Risk æ¸¸æˆçš„äººæ¥è¯´ï¼Œ1v1 æ ‡å‡†æ¸¸æˆçš„é‡è¦ä¿¡æ¯åŒ…æ‹¬ï¼š  å“ªä¸ªé¢†åœŸç”±å“ªä¸ªç©å®¶æ§åˆ¶ï¼Œæ¯ä¸ªé¢†åœŸä¸Šæœ‰å¤šå°‘å†›é˜Ÿ æ¸¸æˆçš„å†å²ï¼ˆè™½ç„¶æˆ‘æƒ³æˆ‘ç°åœ¨ä¼šè·³è¿‡è¿™ä¸€ä¸ªï¼‰ é¢†åœŸå¦‚ä½•å®šä½ï¼ˆå“ªä¸ªè¿æ¥åˆ°å“ªä¸ªï¼Œä»¥é¢„æµ‹è·¯å¾„ï¼‰ å¤§é™† - åŒ…æ‹¬å“ªäº›é¢†åœŸä»¥åŠå®ƒä»¬å¥–åŠ±å¤šå°‘å†›é˜Ÿï¼ˆä½†æˆ‘ç°åœ¨ä¹Ÿä¼šè·³è¿‡å®ƒï¼‰  æ‰€ä»¥ï¼Œæˆ‘æƒ³è¦åšä»¥ä¸‹äº‹æƒ…ï¼š # è°æ§åˆ¶ç€æ¯ä¸ªé¢†åœŸã€‚åªæœ‰ä¸¤ä¸ªç©å®¶ï¼Œæ‰€ä»¥å€¼ä¸º 0 æˆ– 1 membership_space = rooms.Box(low=0, high=1, shape=(4,), dtype=int) # ç†è®ºä¸Šæœ€å¤§éƒ¨é˜Ÿæ•°é‡å¯ä»¥æ— é™ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°†ä½“è‚²é¦†è®°å½•çš„é™åˆ¶è®¾ç½®ä¸ºæ•´æ•°ï¼š2**63 - 2 soldiers_space = rooms.Box(low=0, high=2**63-2, shape=(4,), dtype=int) # å¤§å°ä¸º (num_territories x num_territories) çš„é‚»æ¥çŸ©é˜µ connections_space = rooms.Box(low=0, high=1, shape=(4, 4), dtype=int) # çŸ¥é“æœºå™¨äººæ˜¯å“ªä¸ªç©å®¶ player_id_space = rooms.Box(low=0, high=1, shape=(), dtype=int) self.observation_space = gym.spaces.Dict({ &quot;ownership&quot;: membership_space, &quot;troops&quot;: soldiers_space, &quot;connectivity&quot;: connection_space, &quot;player_id&quot;: player_id_space })  æˆ‘ä½¿ç”¨ Boxes æ¥è¡¨ç¤ºæˆ‘çš„å‘é‡ï¼Œè¿™æ˜¯æœ€å¥½çš„é€‰æ‹©å—ï¼Ÿ æˆ‘è®¤ä¸ºè®©æœºå™¨äººçŸ¥é“å®ƒæ˜¯ä»€ä¹ˆç©å®¶å¾ˆé‡è¦ï¼Ÿ æ­¤å¤–ï¼Œæˆ‘è®¤ä¸ºæœ‰æ›´å¥½çš„æ–¹æ³•æ¥è¡¨ç¤ºé¢†åœŸè¿é€šæ€§ã€‚ å†æ¬¡ï¼Œæˆ‘å¯¹ RL éå¸¸é™Œç”Ÿï¼Œå¯¹å®ç°å®ƒçš„æ‰€æœ‰é€‰é¡¹æœ‰ç‚¹è¿·èŒ«ã€‚ è°¢è°¢ä½ çš„å¸®åŠ© ç¼–è¾‘ï¼šæˆ‘å®é™…ä¸Šéœ€è¦ä¸€ä¸ªè§‚å¯Ÿå˜é‡æ¥æŒ‡å®šæˆ‘ä»å“ªä¸ªé¢†åœŸæ”»å‡»ï¼Œå› ä¸ºåœ¨æˆ‘çš„ç®€åŒ–ç‰ˆæœ¬çš„ DQN æœºå™¨äººä¸­ï¼Œæœºå™¨äººå”¯ä¸€å¯ä»¥é€‰æ‹©çš„å°±æ˜¯æ”»å‡»å“ªä¸ªé¢†åœŸã€‚    æäº¤äºº    /u/BoxingBytes   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kzfp/risk_global_domination_gymnasium_observation/</guid>
      <pubDate>Thu, 06 Jun 2024 15:29:48 GMT</pubDate>
    </item>
    <item>
      <title>åè¿‡æ¥æƒ³ï¼Œåœ¨ RL ä¸­â€œä½¿ç”¨é‡‡æ ·å‘½ä»¤è¿›è¡Œæ¢ç´¢â€æœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</link>
      <description><![CDATA[åœ¨è¡Œä¸ºå‡½æ•°æ›´æ–°åï¼ŒRL upside down é€‰æ‹©æœ€ä½³çš„å‰ k ä¸ªè¿‘æœŸæƒ…èŠ‚å¹¶æ±‡æ€»æ¥è‡ªå®ƒä»¬çš„ä¿¡æ¯ä»¥æå‡ºç›®æ ‡å‘½ä»¤ï¼Œè¿™äº›å‘½ä»¤æ˜¯è¿™äº›ç²¾è‹±æƒ…èŠ‚çš„å¥–åŠ±å’ŒèŒƒå›´ä¹‹å’Œçš„å¹³å‡å€¼ã€‚ç„¶åè¿™äº›å‘½ä»¤ç”¨äºç”Ÿæˆæ›´å¤šæƒ…èŠ‚ä»¥æ·»åŠ åˆ°é‡æ”¾ç¼“å†²åŒºä¸­ã€‚ä½†æˆ‘æ³¨æ„åˆ°åœ¨ç”Ÿæˆæƒ…èŠ‚æ—¶ï¼Œåˆå§‹çŠ¶æ€æ˜¯é‡ç½®çŠ¶æ€ï¼Œè¿™æ„å‘³ç€å‰è¿›çš„è½¨è¿¹å¯èƒ½ä¸è¿™äº›å‘½ä»¤å®Œå…¨æ— å…³ï¼Œå› ä¸ºé‚£äº›æ–°çœ‹åˆ°çš„çŠ¶æ€å¯èƒ½ä¸ä¼šå¯¼è‡´é‚£äº›æå‡ºçš„å‘½ä»¤ï¼Œä»è€Œå¯¼è‡´éšæœºé‡‡æ ·åŠ¨ä½œã€‚åœ¨è¿™æ–¹é¢ï¼ŒRL upside down æ˜¯å¦ä¾é ç¥ç»ç½‘ç»œçš„æ³›åŒ–æ¥è·å¾—æ›´é«˜çš„å¥–åŠ±è½¨è¿¹ï¼Ÿå¦åˆ™æˆ‘ä¸æ˜ç™½å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚    æäº¤äºº    /u/OutOfCharm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</guid>
      <pubDate>Thu, 06 Jun 2024 15:06:44 GMT</pubDate>
    </item>
    <item>
      <title>ä»è¿™å¾€å“ªå„¿èµ°ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</link>
      <description><![CDATA[æˆ‘æœ‰ä¸€ä¸ªéœ€è¦ RL çš„é¡¹ç›®ï¼Œæˆ‘å­¦ä¹ äº† Sutton æ’°å†™çš„ RL ç®€ä»‹çš„å‰ 200 é¡µï¼ŒæŒæ¡äº†åŸºç¡€çŸ¥è¯†å’Œæ‰€æœ‰åŸºæœ¬ç†è®ºä¿¡æ¯ã€‚ä½ ä»¬æœ‰ä»€ä¹ˆå»ºè®®å¯ä»¥å¼€å§‹å®é™…å®æ–½æˆ‘çš„ RL é¡¹ç›®æƒ³æ³•ï¼Œæ¯”å¦‚ä» OpenAI Gym ä¸­çš„åŸºæœ¬æƒ³æ³•å¼€å§‹ï¼Œæˆ–è€…æˆ‘ä¸çŸ¥é“æˆ‘æ˜¯æ–°æ‰‹ï¼Œä½ ä»¬èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®ï¼Œå‘Šè¯‰æˆ‘å¦‚ä½•åœ¨å®è·µæ–¹é¢åšå¾—æ›´å¥½å—ï¼Ÿ æ›´æ–°ï¼šè°¢è°¢å¤§å®¶ï¼Œæˆ‘ä¼šæ£€æŸ¥æ‰€æœ‰è¿™äº›å»ºè®®ï¼Œè¿™ä¸ª subreddit å¾ˆæ£’ï¼    æäº¤äºº    /u/Signal-Ad3628   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</guid>
      <pubDate>Thu, 06 Jun 2024 01:32:05 GMT</pubDate>
    </item>
    </channel>
</rss>