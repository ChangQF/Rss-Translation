<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚çš„ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä½³åœ°è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Thu, 11 Apr 2024 09:14:17 GMT</lastBuildDate>
    <item>
      <title>å…³äºA2Cå®ç°ä¸­çš„æŸå¤±å’Œä¼˜åŒ–å™¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c18roz/about_the_loss_and_optimizer_in_a2c_implementation/</link>
      <description><![CDATA[æˆ‘ç›®å‰æ­£åœ¨å®ç° A2C ç®—æ³•ï¼Œåœ¨ä»£ç å®ç°ä¸­é‡åˆ°äº†å…³äºæŸå¤±å’Œä¼˜åŒ–å™¨çš„å›°æƒ‘ã€‚ å½“æˆ‘ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶å­¦ä¹ äº†A2Cï¼Œæˆ‘æåˆ°çš„ä»£ç ä¸ºactorå’Œcriticåˆ†é…å•ç‹¬çš„ä¼˜åŒ–å™¨ï¼Œå¹¶åˆ†åˆ«ä¼˜åŒ–ï¼Œå°±åƒï¼š```Python def learn(self,transition_dict): states = torch.tensor(np.array(transition_dict[&#39;states) &#39;]), dtype=torch.float).view(-1, self.state_dim).to(self.device) # å°† ndarray åˆ—è¡¨è½¬æ¢ä¸º ndarrayï¼Œå› ä¸ºå°† ndarray åˆ—è¡¨è½¬æ¢ä¸ºå¼ é‡éå¸¸æ…¢ actions = torch.tensor (transition_dict[&#39;actions&#39;], dtype=torch.long).view(-1, 1).to(self.device)  è¿”å›ï¼Œä¼˜ç‚¹ = self.compute_returns_and_advantages(transition_dict, self .lamda) log_probs = torch.log(self.actor(states).gather(1, actions)) # actionsä¸­çš„å…ƒç´ å¿…é¡»æ˜¯ä¸‹æ ‡æ‰èƒ½ä½¿ç”¨gather() actor_loss = torch.mean(-log_probs *ä¼˜ç‚¹.detach()) # detach()é˜²æ­¢ä¸´æ—¶åŠ è½½ï¼Œå…±äº«æ•°æ®å†…å­˜ï¼Œå€¼ç›¸åŒ # td_error ä¸ actor å‚æ•°æ— å…³ï¼Œå¯ä»¥çœ‹ä½œå¸¸é‡ Critical_loss = F.mse_loss(self.critic(states), returns.detach()) # å‡†å¤‡update self.actor_optimizer.zero_grad() self.critic_optimizer.zero_grad() # è®¡ç®—æ¢¯åº¦ actor_loss.backward()ritic_loss.backward() # æ›´æ–°æƒé‡ self.actor_optimizer.step() self.critic_optimizer.step() &lt; /pre&gt;  ä½†å½“æˆ‘å›é¡¾æµè¡Œçš„å¼€æº RL åº“ä¸­çš„ä»£ç æ—¶ï¼Œæˆ‘å‘ç°å®ƒä»¬éƒ½ä½¿ç”¨äº†é’ˆå¯¹ actor å’Œ critic çš„é€šç”¨ä¼˜åŒ–å™¨ï¼Œå¹¶å¯¹ actorã€å€¼å’Œç†µçš„æŸå¤±æ±‚å’Œæ¥è¿›è¡Œä¼˜åŒ–ã€‚ [stable-baselines3](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/a2c/a2c.py#L132) çš„å®ç°å¦‚ä¸‹ï¼š Python def train(è‡ªæˆ‘ï¼‰-&gt;æ— ï¼šâ€œâ€ä½¿ç”¨å½“å‰æ”¶é›†çš„æ¨å‡ºç¼“å†²åŒºæ›´æ–°ç­–ç•¥ï¼ˆæ•´ä¸ªæ•°æ®ä¸Šçš„ä¸€ä¸ªæ¢¯åº¦æ­¥éª¤ï¼‰ã€‚ â€â€œâ€ # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ï¼ˆè¿™ä¼šå½±å“æ‰¹é‡æ ‡å‡†åŒ–/dropoutï¼‰ self.policy.set_training_mode(True)  # æ›´æ–°ä¼˜åŒ–å™¨å­¦ä¹ ç‡ self._update_learning_rate(self.policy.optimizer) # è¿™åªä¼šå¾ªç¯ä¸€æ¬¡ï¼ˆä¸€æ¬¡æ€§è·å–æ‰€æœ‰æ•°æ®ï¼‰ for rollout_data in self.rollout_buffer.get(batch_size=None): actions = rollout_data.actions if isinstance(self.action_space,spaces.Discrete): # å°†ç¦»æ•£åŠ¨ä½œä»æµ®ç‚¹åŠ¨ä½œè½¬æ¢ä¸ºé•¿åŠ¨ä½œ= actions.long().flatten()values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)values =values.flatten() # æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆåŸå§‹å®ç°ä¸­ä¸å­˜åœ¨ï¼‰ä¼˜ç‚¹ = rollout_data.ä¼˜ç‚¹ if self.normalize_advantage: ä¼˜ç‚¹ = (ä¼˜ç‚¹ - ä¼˜ç‚¹.mean()) / (advantages.std() + 1e-8) # ç­–ç•¥æ¢¯åº¦æŸå¤± policy_loss = -(advantages * log_prob).mean() # ä½¿ç”¨TD(gae_lambda) target value_loss = F.mse_loss(rollout_data.returns, value) # ç†µæŸå¤±æœ‰åˆ©äºæ¢ç´¢ if entropy is None: # æ— åˆ†æå½¢å¼æ—¶çš„è¿‘ä¼¼ç†µ entropy_loss = -th.mean(-log_prob) else: entropy_loss = -th .mean(entropy) loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss # ä¼˜åŒ–æ­¥éª¤ self.policy.optimizer.zero_grad() loss.backward() # å‰ªè¾‘æ¢¯åº¦èŒƒæ•° th.nn.utils.clip_grad_norm_(self .policy.parameters(), self.max_grad_norm) self.policy.optimizer.step()  ``` æˆ‘çš„é—®é¢˜ï¼šä¸ºä»€ä¹ˆè¦å®ç° Actor-Criticè¿™æ ·ï¼Ÿæ˜¯ä¸ºäº†æ–¹ä¾¿å—ï¼Ÿè¿˜æ˜¯ä¸ºäº†æ€§èƒ½ï¼Ÿ   ç”±   æäº¤ /u/Awkward_Swimmer_5649   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c18roz/about_the_loss_and_optimizer_in_a2c_implementation/</guid>
      <pubDate>Thu, 11 Apr 2024 07:07:36 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦æœ‰å…³ RL å½•å–çš„å»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c17bwu/need_advice_on_an_rl_admit/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘åœ¨å·´å¡ç½—é‚£ UPF ç”³è¯·äº†æ™ºèƒ½ä¿¡æ¯ç³»ç»Ÿç¡•å£«å­¦ä½ï¼Œæˆ‘è®¤ä¸ºæˆ‘å·²è¢«æ¥å—ï¼Œå› ä¸ºç”³è¯·é—¨æˆ·çŠ¶æ€æ˜¾ç¤ºå·²æ¥å—ï¼Œä½†æ˜¯å°šæœªæ”¶åˆ°ä»»ä½•æ­£å¼çš„é‚®ä»¶æ²Ÿé€šã€‚ æˆ‘å¯¹å¼ºåŒ–å­¦ä¹ éå¸¸æ„Ÿå…´è¶£ï¼ŒReddit ä¸Šçš„ä¸€ç¯‡å¸–å­å‘æˆ‘æ¨èäº†è¿™æ‰€å¤§å­¦ï¼Œè€Œä¸”è¿™é‡Œçš„æ•™æˆä¼¼ä¹ä¹Ÿéå¸¸å¥½ã€‚ æˆ‘æƒ³çŸ¥é“æˆ‘åœ¨è¿™é‡Œæ˜¯å¦æœ‰è‰¯å¥½çš„å­¦ä¹ èŒƒå›´å’Œæœºä¼šã€‚æˆ‘çš„æœ€ç»ˆç›®æ ‡æ˜¯æˆä¸ºä¸€åæ•™æˆã€‚ æˆ‘å”¯ä¸€æ€€ç–‘çš„åŸå› æ˜¯è¿™æ‰€å¤§å­¦ç›¸å½“å¹´è½»~ 30 å¹´ã€‚è€Œæ¬§æ´²å…¶ä»–å¤§å­¦å·²æœ‰ä¸€ç™¾å¤šå¹´çš„å†å²ã€‚ è¿™æ˜¯æˆ‘å”¯ä¸€å¯èƒ½æ‰¿è®¤çš„ï¼Œæˆ‘æ›¾å‘å…¶ä»– 3 ä¸ªåœ°æ–¹ç”³è¯·ç›´æ¥åšå£«å­¦ä½ï¼Œä½†éƒ½è¢«æ‹’ç»äº†ã€‚ å¤§å®¶è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿå¦‚æœæˆ‘è·å¾—ä¸€é—¨è¯¾ç¨‹æˆ–ç”³è¯· 2025 å¹´ 9 æœˆå¼€å§‹çš„ç¡•å£«è¯¾ç¨‹ï¼Œæˆ‘åº”è¯¥æ¥å—å½•å–å—ï¼Ÿ å¦‚æœ‰ä»»ä½•å¸®åŠ©ï¼Œæˆ‘ä»¬å°†ä¸èƒœæ„Ÿæ¿€ï¼Œè°¢è°¢ï¼   ç”±   æäº¤ /u/FlyTrain1011   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c17bwu/need_advice_on_an_rl_admit/</guid>
      <pubDate>Thu, 11 Apr 2024 05:38:25 GMT</pubDate>
    </item>
    <item>
      <title>NEAT + Q å­¦ä¹ </title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c16gry/neat_q_learning/</link>
      <description><![CDATA[æˆ‘ä¸€ç›´åœ¨å¯»æ‰¾å°† NEAT ä¸ DQN ç»“åˆä½¿ç”¨çš„è®ºæ–‡ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°ã€‚æˆ‘æƒ³çŸ¥é“è¿™å¯¹äº OpenAI Gym ä¸­çš„ 2D è‡ªåŠ¨é©¾é©¶æ±½è½¦æ¥è¯´æ˜¯å¦æ˜¯ä¸ªå¥½ä¸»æ„ã€‚   ç”±   æäº¤/u/fa_anony__mous   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c16gry/neat_q_learning/</guid>
      <pubDate>Thu, 11 Apr 2024 04:47:41 GMT</pubDate>
    </item>
    <item>
      <title>æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c147yn/reward_in_modelfree_reinforcement_learning/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘çœ‹åˆ°æœ‰äº›è®ºæ–‡æŠŠ Reward å†™æˆ R --&gt; S X A X S&#39; å’Œå…¶ä»–ä¸€äº›ä½¿ç”¨ R-&gt; çš„è®ºæ–‡S X A å“ªä¸€ä¸ªå¯¹äºæ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æœ‰æ•ˆï¼Ÿ   ç”±   æäº¤/u/Sea-Collection-8844   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c147yn/reward_in_modelfree_reinforcement_learning/</guid>
      <pubDate>Thu, 11 Apr 2024 02:46:20 GMT</pubDate>
    </item>
    <item>
      <title>æœ‰äººè®© mujoco-py åœ¨ Linux ä¸Šè¿è¡Œå—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0zytk/has_anyone_gotten_mujocopy_working_on_linux/</link>
      <description><![CDATA[æˆ‘çš„ä¸€ä½åŒäº‹ç»™äº†æˆ‘ä¸€äº›ä½¿ç”¨â€œpen- human-v1â€çš„ä»£ç æ•°æ®é›†ï¼Œå®ƒæ˜¯éœ€è¦ Mujocoï¼ˆç‰¹åˆ«æ˜¯ Mujoco-pyï¼‰çš„ 3d Gym ç¯å¢ƒä¹‹ä¸€ã€‚ Mujoco-py å·²ç»æœ‰ 7 å¹´å†å²äº†ï¼Œä¼¼ä¹å·²ç»è¿‡æ—¶äº† - æˆ‘æ— æ³•è®©å®ƒåœ¨ Windows æˆ– Linux ä¸Šè¿è¡Œï¼ˆæˆ‘è®¤ä¸ºæˆ‘çš„åŒäº‹åœ¨ Mac ä¸Šï¼‰ã€‚æˆ‘ç›¸ä¿¡ Gymnasium ç°åœ¨ä½¿ç”¨æœ€æ–°çš„ Mujoco ç»‘å®šï¼Œä¸éœ€è¦ä¸€å †é¢å¤–çš„å®‰è£…æ­¥éª¤ï¼Œä½†å°šä¸æ¸…æ¥šå¦‚ä½•ä½¿ d4rl ä½¿ç”¨ Gymnasium è€Œä¸æ˜¯ Gymã€‚æ‰€ä»¥æˆ‘ä¸ç¡®å®šäººä»¬å¦‚ä½•è®© d4rl åœ¨ Linux ä¸Šå·¥ä½œï¼ˆä½¿ç”¨ä¸åŒçš„å‘è¡Œç‰ˆä¼šæ›´å¥½å—ï¼Ÿï¼‰   ç”±   æäº¤/u/hearthstoneplayer100  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0zytk/has_anyone_gotten_mujocopy_working_on_linux/</guid>
      <pubDate>Wed, 10 Apr 2024 23:26:48 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨è®­ç»ƒæœŸé—´å°†å˜åŒ–çš„ä¼½ç›ä¼ é€’ç»™ DQN æˆ– PPOï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0q5o6/how_to_pass_a_varying_gamma_to_dqn_or_ppo_during/</link>
      <description><![CDATA[å¼ºåŒ–å­¦ä¹ å’Œ SB3 å®ç°åº”ç”¨å…¸å‹çš„å¸¸é‡ gamma åœ¨å­¦ä¹ æ—¶å¯¹æœªæ¥å€¼è¿›è¡ŒæŠ˜æ‰£ã€‚è¿™å¯¹äºç¦»æ•£æ—¶é—´ç¯å¢ƒæ¥è¯´å¾ˆå¥½ï¼Œå…¶ä¸­å¯¹äºæ¯ä¸ªæ“ä½œï¼Œæœªæ¥å€¼éƒ½ä¼šè¢«æŠ˜æ‰£ä¸ºæ¯ä¸ªæ­¥éª¤çš„å¸¸æ•°ã€‚ æˆ‘æœ‰ä¸€ä¸ªè‡ªå®šä¹‰çš„å¥èº«æˆ¿ç¯å¢ƒï¼Œå…¶ä¸­æˆ‘çš„ç¯å¢ƒåœ¨ç¦»æ•£å†³ç­–æ—¶æœŸä¸­æ­¥è¿›ï¼Œä½†æ¯ä¸ªæ“ä½œéƒ½éœ€è¦ä¸€ä¸ªä¸åŒçš„æ—¶é—´é‡ã€‚å› æ­¤ï¼Œä»¥æ’å®šæ¯”ç‡è´´ç°æœªæ¥ä»·å€¼æ˜¯ä¸æ­£ç¡®çš„ã€‚æˆ‘éœ€è¦åšçš„æ˜¯ç”¨ gamma æ¥æŠ˜æ‰£æœªæ¥å€¼ï¼Œgamma æ˜¯åœ¨ç¯å¢ƒä¸­æ‰§è¡Œæ“ä½œæ‰€éœ€æ—¶é—´çš„å‡½æ•°ã€‚ æ— è®ºå¦‚ä½•ï¼Œæ˜¯å¦å¯ä»¥å°† gamma ä½œä¸ºå‡½æ•°æˆ–å¼ é‡ä¼ é€’åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ˜ å°„åˆ°æ¯ä¸ª (s, a, s&#39;, r) å…ƒç»„ï¼Ÿä¹Ÿè®¸å¯ä»¥ä½¿ç”¨ç°æœ‰åŠŸèƒ½æˆ–å›è°ƒï¼Ÿå¦‚æœå¯èƒ½çš„è¯ï¼Œæˆ‘å¸Œæœ›é¿å…åˆ†å‰å­˜å‚¨åº“ã€‚ ä»»ä½•æ„è§éƒ½å°†ä¸èƒœæ„Ÿæ¿€ï¼Œå› ä¸ºæˆ‘å·²ç»åœ¨è¿™æ–¹é¢åšæŒäº†ä¸€æ®µæ—¶é—´äº†ã€‚æå‰è‡´è°¢ï¼   ç”±   æäº¤ /u/Real_Zesty   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0q5o6/how_to_pass_a_varying_gamma_to_dqn_or_ppo_during/</guid>
      <pubDate>Wed, 10 Apr 2024 16:44:13 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å°†å¼ºåŒ–å­¦ä¹ ä¸ GYM åº“åº”ç”¨äº Flappy Bird ç­‰ 2D è§†é¢‘æ¸¸æˆã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0nx8l/how_to_apply_reinforcement_learning_with_gym/</link>
      <description><![CDATA[æˆ‘ä½¿ç”¨ç®—æ³•åˆ›å»ºäº†ä¸€ä¸ª flappy Bird æœºå™¨äººã€‚å®ƒå·¥ä½œå¾—å¾ˆå¥½å¹¶ä¸”å¯ä»¥æ— é™è¿è¡Œã€‚ç°åœ¨æˆ‘æƒ³å°è¯•ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œçœ‹çœ‹å®ƒèƒ½æé«˜å¤šå°‘ã€‚æˆ‘åœ¨ Python ä¸­å‘ç°äº†ä¸€ä¸ªåä¸º Gym çš„åº“ï¼Œå®ƒä¼¼ä¹å¯ä»¥ç®€åŒ–äº‹æƒ…ã€‚æˆ‘å°è¯•åœ¨ç½‘ä¸Šå¯»æ‰¾ç±»ä¼¼çš„ç¤ºä¾‹ï¼Œä½†æ‰¾ä¸åˆ°ä»»ä½•é€‚åˆåˆå­¦è€…åˆ›å»ºè‡ªå·±çš„ç¯å¢ƒçš„å†…å®¹ã€‚æ‰€ä»¥æˆ‘å¸Œæœ›æœ‰äººèƒ½å¼•å¯¼æˆ‘æœç€æ­£ç¡®çš„æ–¹å‘å»åšè¿™ä»¶äº‹ã€‚ è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘åˆ¶ä½œçš„ç¦»çº¿æ¸¸æˆï¼Œä¸è¿åä»»ä½•æ¡æ¬¾ã€‚ æˆ‘å·²ç»å¯ä»¥åœ¨åæ ‡ä¸­æ£€æµ‹åˆ°å±å¹•ä¸Šçš„é¸Ÿå’Œéšœç¢ç‰©ï¼ˆå‚è§ä¸‹é¢çš„ä»£ç ï¼‰ã€‚æ‰€ä»¥æˆ‘ç¡®å®æœ‰æ•°æ®ï¼Œæˆ‘åªéœ€è¦å›´ç»•å®ƒç¼–å†™æœºå™¨å­¦ä¹ éƒ¨åˆ†ã€‚ # æ¸¸æˆçª—å£æˆªå›¾ game_frame = pyautogui.screenshot(region=(200, 190, 500 , 860)) deflocate_flappy_bird(game_frame): # æ‰¾åˆ° flappy é¸Ÿã€‚ Returns X,Y flappy_bird_location = pyautogui.locate(â€œ800px-1080pxBirdEye2.jpgâ€, game_frame,confidence=0.85) return flappy_bird_location deflocate_obstacles(game_frame): # å®šä½å±å¹•ä¸Šçš„æ‰€æœ‰éšœç¢ç‰©ã€‚è¿”å› pyautogui.locateAll(â€œObstacle_Ball.jpgâ€, game_frame,confidence=0.90) ä¸­éšœç¢ç‰©çš„ X,Y è·ç¦» = pow(10, 2) éšœç¢ç‰©ä½ç½® = [] åˆ—è¡¨: if all(abs(obstacle.top - y[ 0]) &gt; y åœ¨éšœç¢ç‰©ä½ç½®ä¸­çš„è·ç¦»): éšœç¢ç‰©ä½ç½®.append((int(obstacle.top), int(obstacle.left))) è¿”å›éšœç¢ç‰©ä½ç½® current_location_flappy =locate_flappy_bird(game_frame) éšœç¢ç‰©ä½ç½® =locate_obstacles(game_frame) # flappy ä½ç½®( x, y) = [(240, 20)] # éšœç¢ç‰©ä½ç½®(x ,y) = [(218, 115), (361, 115), (566, 334), (727, 334)]     ç”±   æäº¤/u/pubGGWP   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0nx8l/how_to_apply_reinforcement_learning_with_gym/</guid>
      <pubDate>Wed, 10 Apr 2024 15:11:00 GMT</pubDate>
    </item>
    <item>
      <title>è®­ç»ƒæ— æ³•è¿›è¡Œåˆ°ä¸‹ä¸€é›†ï¼Œå¹¶ä¸”åœ¨åˆ°è¾¾ç¬¬ä¸€é›†ç»“æŸå matlab å´©æºƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0h0hc/training_is_not_proceeding_to_the_next_episode/</link>
      <description><![CDATA[   å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨ä½¿ç”¨ç”¨äºè®­ç»ƒçƒ­æ³µ Simulink æ¨¡å‹çš„ Matlab RL å·¥å…·ç®±ã€‚æˆ‘å·²æ­£ç¡®é™„åŠ æ‰€æœ‰è¿æ¥ï¼Œå› ä¸ºæˆ‘èƒ½å¤Ÿçœ‹åˆ°æ­£åœ¨ç”Ÿæˆçš„å¥–åŠ±ä»¥åŠä» simulink æ¨¡å‹å†…çš„ FMU ä¸­æå–çš„è§‚å¯Ÿç»“æœã€‚ä½†æˆ‘é¢ä¸´çš„ä¸»è¦é—®é¢˜æ˜¯åœ¨ç»“æŸä¸€ä¸ªå‰§é›†åï¼Œæˆ‘çš„æ•´ä¸ªè½¯ä»¶å´©æºƒå¹¶å…³é—­ï¼Œå› æ­¤æˆ‘åªèƒ½çœ‹åˆ°ç¬¬ä¸€é›†æ­£åœ¨è¿è¡Œã€‚è¿™æ˜¯æˆ‘æ”¶åˆ°çš„å´©æºƒæŠ¥å‘Šå’Œæˆ‘çš„æ¨¡å‹çš„ä¸€äº›å±å¹•æˆªå›¾ã€‚  å¦‚æœæœ‰äººå¯ä»¥åˆ†äº«ä»–ä»¬çš„è§‚ç‚¹ï¼Œæˆ‘å¯èƒ½åšé”™äº†ä»€ä¹ˆï¼Œé‚£å°†æ˜¯å·¨å¤§çš„å¸®åŠ©ã€‚æ„Ÿè°¢æ‚¨å®è´µçš„æ—¶é—´ã€‚ â€‹ https://preview.redd.it/11yg0c4o9mtc1.png?width=2486&amp;format=png&amp;auto=webp&amp;s=a0ef73e0e56c0ed4bc328fa83b322a a73d3d667c  https://preview.redd.it/hefmuz6i9mtc1.pngï¼Ÿ width=3426&amp;format=png&amp;auto=webp&amp;s=15eddef2f12fdffc8490753d1079237c3e2067eb â€‹ https://preview.redd.it/27m4mrvl9mtc1.png?width=2021&amp;format=png&amp;auto=webæ™®å’Œæ–¯=4061a4bba776260c04a39e37a06c6e92b924729b https ://preview.redd.it/gcpj8svl9mtc1.png?width=2014&amp;format=png&amp;auto=webp&amp;s=cbb0b1c7e2868321250bfec935d274a252120497 https://preview.redd.it/1vz3xsvl9mtc1.png?width=2024&amp;format=png&amp; ;è‡ªåŠ¨=webp&amp; ;s=c604061c27535460f5498636270aaf9968ecf025 https://preview.redd.it/p34c5uvl9mtc1.png?width=2033&amp;format=png&amp;auto=webp&amp;s=eefac9ab0919f9d5113bd5ec27f3a8d982b8969e   ç”±   æäº¤/u/slimshadyy18   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0h0hc/training_is_not_proceeding_to_the_next_episode/</guid>
      <pubDate>Wed, 10 Apr 2024 08:57:16 GMT</pubDate>
    </item>
    <item>
      <title>å›¾ç€è‰²é—®é¢˜ä¸­çš„å¥–åŠ±è®¾è®¡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c0c8jq/reward_design_in_graph_coloring_problem/</link>
      <description><![CDATA[        ç”±   æäº¤ /u/FangYu_   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c0c8jq/reward_design_in_graph_coloring_problem/</guid>
      <pubDate>Wed, 10 Apr 2024 03:50:55 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆæˆ‘çš„ä»£ç å¯ä»¥è§£å†³ Fetch&Reachï¼Œä½†æ— æ³•è§£å†³ PointMaze-OpenDenseï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c09xc5/why_my_code_can_solve_fetchreach_but_cant_solve/</link>
      <description><![CDATA[æˆ‘ä¸Šæ¬¡ä¹Ÿé—®è¿‡ç±»ä¼¼çš„é—®é¢˜ï¼Œä½†æˆ‘è¿˜æ˜¯ä¸æ˜ç™½ä¸ºä»€ä¹ˆåŒæ ·çš„ä»£ç ä¸èƒ½è§£å†³è¿™ä¹ˆç®€å•çš„ç¯å¢ƒã€‚ &lt; p&gt;è¯¥ä»£ç åœ¨ Fetch&amp;Reach çš„ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­å®Œç¾è¿è¡Œï¼Œä½†å®ƒæ— æ³•è§£å†³ç¯å¢ƒ PointMaze-Openï¼Œå³ä½¿å¯¹äºå¯†é›†å¥–åŠ±çš„æƒ…å†µä¹Ÿæ˜¯å¦‚æ­¤ã€‚æˆ‘æƒ³ä»–ä»¬çš„éš¾åº¦åº”è¯¥æ˜¯å·®ä¸å¤šçš„ã€‚æˆ‘æƒ³åˆ°çš„å”¯ä¸€å¯èƒ½çš„è§£é‡Šæ˜¯åŠ¨é‡ã€‚ PointMaze-Open æ˜¯å› ä¸ºåŠ¨é‡è€Œå˜å¾—éå¸¸å›°éš¾çš„ç¯å¢ƒå—ï¼Ÿè¿˜æ˜¯ä½“è‚²é¦†ç¯å¢ƒç‰ˆæœ¬é—®é¢˜ï¼Ÿ â€‹ è¿·å®«ç¯å¢ƒæ–‡æ¡£ï¼šhttps://robotics.farama.org/envs/maze/point_maze/ æˆ‘æ­£åœ¨ä½¿ç”¨ https://github.com/TianhongDai/hindsight-experience-replay â€‹ â€‹ â€‹   ç”±   æäº¤ /u/Chips-HalfPrice   /u/Chips-HalfPrice reddit.com/r/reinforcementlearning/comments/1c09xc5/why_my_code_can_solve_fetchreach_but_cant_solve/&quot;&gt;[é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c09xc5/why_my_code_can_solve_fetchreach_but_cant_solve/</guid>
      <pubDate>Wed, 10 Apr 2024 01:56:11 GMT</pubDate>
    </item>
    <item>
      <title>PPO - ä»·å€¼æŸå¤±å¿«é€Ÿæ”¶æ•›ä¸”è§£é‡Šçš„æ–¹å·®å§‹ç»ˆä¸ºè´Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzzbu0/ppo_value_loss_converges_quickly_explained/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨ä½¿ç”¨ sb3 è®­ç»ƒäº¤æ˜“ PPO è‡ªå®šä¹‰ç¯å¢ƒï¼Œå¹¶è¿›è¡Œ 3 ä¸ªç¦»æ•£æ“ä½œï¼ˆä¹°å…¥ã€å–å‡ºã€æŒæœ‰ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œä»·å€¼æŸå¤±ä»ä¸€å¼€å§‹å°±å‡ ä¹æ”¶æ•›ï¼Œè€Œè§£é‡Šçš„æ–¹å·®å§‹ç»ˆä¸ºè´Ÿï¼Œæœ€ç»ˆæ”¶æ•›åˆ°é›¶ã€‚  å°±å¥–åŠ±è€Œè¨€ï¼Œä¸€å¼€å§‹å®ƒä¼šä»¥ä¸€ä¸ªå¾ˆå¥½çš„é€Ÿç‡å¢åŠ ï¼Œç„¶åå®ƒä¼šä»¥éå¸¸å°çš„é€Ÿç‡æŒç»­å¢åŠ ï¼ˆä¾‹å¦‚åœ¨ 200k æ­¥åä» 0.12 å¢åŠ åˆ° 0.14ï¼‰ https://preview.redd.it/9lo9e26sxhtc1.png ?width=1658&amp;format=png&amp;auto=webp&amp;s=0b8c9e282c4b50e6bb56be538a1be836af935078 æˆ‘å·²ç»å°è¯•äº†å¾ˆå¤šè¶…å‚æ•°è°ƒæ•´ï¼š  å­¦ä¹ ç‡ ç†µç³»æ•° gamma gae_lambda  æˆ‘æœ‰ä»¥ä¸‹é—®é¢˜ï¼š  æ¨¡å‹æ˜¯å¦å·²ç»æ‰¾åˆ°æœ€ä¼˜ç­–ç•¥ï¼Ÿ æˆ‘å¯ä»¥è¿›è¡Œä»€ä¹ˆæ ·çš„è¶…å‚æ•°è°ƒæ•´æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ     ç”±   æäº¤ /u/Acceptable_Egg6552   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzzbu0/ppo_value_loss_converges_quickly_explained/</guid>
      <pubDate>Tue, 09 Apr 2024 18:26:40 GMT</pubDate>
    </item>
    <item>
      <title>Gazebo + ros2 RLç¯å¢ƒï¼ˆä¸å«gymï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzw3ys/gazebo_ros2_rl_environment_without_gym/</link>
      <description><![CDATA[æˆ‘å®ç°äº†ä¸€ä¸ª RL ç¯å¢ƒï¼Œå…è®¸æ‚¨ä¸ Gazebo ä¸­çš„æœºå™¨äººäº¤äº’ã€‚ ä¸ç°æœ‰ç¯å¢ƒçš„åŒºåˆ«ï¼š p&gt;  ç¯å¢ƒä½¿ç”¨åœ°å›¾æ–‡ä»¶ï¼ˆä¸€ä¸ªå¸¦æœ‰äºŒç»´æ•´æ•°æ•°ç»„çš„ Python æ–‡ä»¶æ¥å®ä¾‹åŒ–ï¼Œè¯¥æ•°ç»„å®šä¹‰å¢™å£ (1)ã€èµ·å§‹ä½ç½® (2) å’Œå¯åˆ°è¾¾ç©ºé—´ (0)ã€‚ ç¯å¢ƒä½¿ç”¨æ­¤åœ°å›¾æ¥ç”Ÿæˆ Gazebo ä½¿ç”¨çš„ .world æ–‡ä»¶ï¼Œå¹¶ä¸”è¿˜å¯ä»¥äº†è§£å¢™å£çš„ä½ç½®ï¼Œä»¥ä¾¿åœ¨å¯åˆ°è¾¾çš„åŒºåŸŸä¸­å‡åŒ€åœ°å¯¹ç›®æ ‡è¿›è¡Œé‡‡æ ·ã€‚ ç¯å¢ƒçš„ __init__ å‡½æ•°åœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­å¯åŠ¨ Gazeboï¼Œå¹¶è‡ªåŠ¨æ£€æµ‹ ros2 ä¸»é¢˜å’ŒæœåŠ¡ï¼Œå› æ­¤æ‚¨åªéœ€å®‰è£…/æº ros å¹¶å®ä¾‹åŒ–ç¯å¢ƒå³å¯ã€‚ ç¯å¢ƒä¸çœŸå®ç¯å¢ƒå…¼å®¹-time rlï¼ˆåœ¨gazeboæ¨¡æ‹Ÿä¸­æ²¡æœ‰æš‚åœï¼‰æˆ–ç»å…¸rlï¼ˆåœ¨æ¯ä¸€æ­¥ç¯å¢ƒéƒ½ä¼šæ’­æ”¾xç§’ï¼Œç„¶ååœ¨å°†æ–°çŠ¶æ€å‘é€ç»™ä»£ç†ä¹‹å‰å†æ¬¡æš‚åœï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡__init__å‚æ•°è®¾ç½®ã€‚  ç›®å‰åªæœ‰â€œirobotâ€æœºå™¨äººå¯ç”¨ï¼ˆä¸€ç§æ‰å¹³çš„æµ·é¾Ÿæœºå™¨äººï¼‰ï¼Œä½†æ·»åŠ æ–°æœºå™¨äººåº”è¯¥ä¸æ˜¯ä»€ä¹ˆå¤§é—®é¢˜ã€‚ æ¥æºï¼š https://github.com/hbonnavaud/RLFramework/tree/main/environments/robotic_environment   ç”±   æäº¤ /u/hbonnavaud   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzw3ys/gazebo_ros2_rl_environment_without_gym/</guid>
      <pubDate>Tue, 09 Apr 2024 16:14:34 GMT</pubDate>
    </item>
    <item>
      <title>IMPALA å®æ–½ä¸ä¸æ–­ä¸Šå‡çš„æ‰¹è¯„æŸå¤±ï¼šéœ€è¦æ´å¯Ÿï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzqheu/impala_implementation_with_rising_critic_loss/</link>
      <description><![CDATA[   æˆ‘åˆšåˆšå®Œæˆäº†ä¸€ä¸ª IMPALA å®æ–½é¡¹ç›®ï¼Œåœ¨è¯¥é¡¹ç›®ä¸­æˆ‘é€šè¿‡å¥—æ¥å­—é€šä¿¡è®­ç»ƒäº†å¤šä¸ªå‚ä¸è€…ã€‚è¿™æ˜¯åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ çš„ä¸€æ®µä»¤äººç€è¿·çš„æ—…ç¨‹ï¼Œä½†æˆ‘é‡åˆ°äº†ä¸€ä¸ªéšœç¢ï¼Œæˆ‘å¸Œæœ›ä½ èƒ½å¸®å¿™è§£å†³ã€‚ å°½ç®¡æ¼”å‘˜è¡¨ç°å‡ºæœ‰å¸Œæœ›çš„æ€§èƒ½æ”¹è¿›ï¼Œä½†æˆ‘çœ‹åˆ°äº†æŒç»­ä¸”ä»¤äººä¸å®‰çš„é—®é¢˜è¶‹åŠ¿ï¼šæ‰¹è¯„è€…æŸå¤±ä¸ä»…ä¸ç¨³å®šï¼›å®ƒå®é™…ä¸Šéšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ã€‚è¿™å¾ˆä»¤äººè´¹è§£ï¼Œå› ä¸ºéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œä½ ä¼šæœŸæœ›æ›´å¥½çš„æ¼”å‘˜è¡¨ç°æ¥ç¨³å®šç”šè‡³å‡å°‘æ‰¹è¯„è€…æŸå¤±ï¼Œå¯¹å§ï¼Ÿ å¦å¤–ï¼Œè¯·ç»§ç»­å…³æ³¨ğŸ˜Šæˆ‘è®¡åˆ’è¿›ä¸€æ­¥æ¢ç´¢åˆ†å¸ƒå¼å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå¹¶å°†ä¸å¤§å®¶åˆ†äº«æˆ‘çš„æ—…ç¨‹ã€‚æ‚¨ä»Šå¤©çš„è§è§£å¯èƒ½æ˜¯è¯¥æ¢ç´¢çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼ æå‰æ„Ÿè°¢æ‚¨çš„æƒ³æ³•å’Œå»ºè®®ï¼ https://github.com/seolhokim/SimpleDistributedRL https://preview.redd.it/tv46xs5p5ktc1.png?width=580&amp;format=png&amp;auto=webp&amp;s=4dd916a4482b7d2c03a4d4 1b3d64dd46b73b972b   ç”±   æäº¤ /u/Spiritual_Fig3632   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzqheu/impala_implementation_with_rising_critic_loss/</guid>
      <pubDate>Tue, 09 Apr 2024 12:06:18 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ Q-learning ä¸ºå¥èº«æˆ¿ä¸­çš„ MountainCar æä¾›å¥–åŠ±åŠŸèƒ½</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bzm0qp/reward_function_for_mountaincar_in_gym_using/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘ä¸€ç›´åœ¨å°è¯•ä½¿ç”¨ Qlearning è®­ç»ƒä»£ç†æ¥è§£å†³å¥èº«æˆ¿ä¸­çš„ MountainCar é—®é¢˜ï¼Œä½†æ— æ³•è®©æˆ‘çš„ä»£ç†åˆ°è¾¾æ——å¸œã€‚å½“æˆ‘ä½¿ç”¨è¿”å›çš„é»˜è®¤å¥–åŠ±æ—¶ï¼Œå®ƒæ°¸è¿œä¸ä¼šåˆ°è¾¾æ ‡å¿—ï¼ˆæ¯ä¸€æ­¥ä¸º -1ï¼Œåˆ°è¾¾æ ‡å¿—æ—¶ä¸º 0ï¼‰ï¼Œæˆ‘è®©å®ƒè¿è¡Œ 200,000 é›†ï¼Œä½†æ— æ³•åˆ°è¾¾é‚£é‡Œã€‚æ‰€ä»¥ï¼Œæˆ‘å°è¯•ç¼–å†™è‡ªå·±çš„å¥–åŠ±å‡½æ•°ï¼Œæˆ‘å°è¯•äº†ä¸€å † - è·ç¦»æ——å¸œè¶Šè¿‘ï¼Œå¥–åŠ±å‘ˆæŒ‡æ•°çº§è¶Šé«˜ï¼Œæ——å¸œå¤„çš„å¥–åŠ±è¶Šå¤§ï¼Œå¥–åŠ±ABSï¼ˆåŠ é€Ÿåº¦ï¼‰å’Œé¡¶éƒ¨çš„å¥–åŠ±è¶Šå¤§ï¼Œç­‰ç­‰ã€‚ä½†æˆ‘åªæ˜¯æ— æ³•è®©æˆ‘çš„ä»£ç†ä¸€è·¯åˆ°è¾¾é¡¶éƒ¨ - å…¶ä¸­ä¸€ä¸ªåŠŸèƒ½éå¸¸æ¥è¿‘ï¼Œå°±åƒéå¸¸æ¥è¿‘ï¼Œä½†éšåå†³å®šå…¨åŠ›æ·±å…¥æ½œæ°´ï¼ˆå¯èƒ½æ˜¯å› ä¸ºæˆ‘å¥–åŠ±åŠ é€Ÿï¼Œä½†æˆ‘è®¾ç½®äº†ä¸€ä¸ªæ ‡å¿—ä»…åœ¨ç¬¬ä¸€æ¬¡å‘å·¦ç§»åŠ¨æ—¶å¥–åŠ±åŠ é€Ÿï¼Œä½†æˆ‘çš„ä»£ç†ä»ç„¶å†³å®šå‘ä¸‹æ½œï¼‰ã€‚æˆ‘ä¸æ˜ç™½ï¼Œæœ‰äººå¯ä»¥å»ºè®®æˆ‘åº”è¯¥å¦‚ä½•è§£å†³å®ƒå—ï¼Ÿ æˆ‘ä¸çŸ¥é“æˆ‘åšé”™äº†ä»€ä¹ˆï¼Œå› ä¸ºæˆ‘åœ¨ç½‘ä¸Šçœ‹åˆ°äº†æ•™ç¨‹ï¼Œè€Œä¸”ä»£ç†ä¹Ÿåœ¨é‚£é‡Œä»…ä½¿ç”¨é»˜è®¤å¥–åŠ±å°±éå¸¸å¿«ï¼ˆ&lt;4000 é›†ï¼‰ï¼Œæˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆå³ä½¿ä½¿ç”¨ç›¸åŒçš„å‚æ•°ä¹Ÿæ— æ³•å¤åˆ¶å®ƒã€‚æˆ‘éå¸¸æ„Ÿè°¢ä»»ä½•å¸®åŠ©å’Œå»ºè®®ã€‚ è¿™æ˜¯ github é“¾æ¥ å¦‚æœæœ‰äººæƒ³çœ‹ä¸€ä¸‹ä»£ç ã€‚ ã€ŠQ-learning-å±±è½¦ã€‹æ˜¯åº”è¯¥å·¥ä½œçš„ä»£ç ï¼Œä¸å‘å¸ƒçš„ OpenAI ç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼Œä½†ç»è¿‡ä¿®æ”¹ä»¥åœ¨gym 0.26ä¸Šå·¥ä½œï¼› copy å’Œ new æ˜¯æˆ‘ä¸€ç›´åœ¨å°è¯•å¥–åŠ±åŠŸèƒ½çš„åœ°æ–¹ã€‚ éå¸¸æ„Ÿè°¢ä»»ä½•æ„è§ã€æŒ‡å¯¼æˆ–å»ºè®®ã€‚æå‰è‡´è°¢ã€‚ ç¼–è¾‘ï¼šåœ¨è¯„è®ºä¸­è§£å†³ã€‚å¦‚æœæœ‰äººæ¥è‡ªæœªæ¥å¹¶ä¸”é¢ä¸´ä¸æˆ‘ç›¸åŒçš„é—®é¢˜ï¼Œè§£å†³çš„ä»£ç å°†ä¸Šä¼ åˆ°ä¸Šé¢é“¾æ¥çš„ github å­˜å‚¨åº“ã€‚   ç”±   æäº¤/u/guccicupcake69   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bzm0qp/reward_function_for_mountaincar_in_gym_using/</guid>
      <pubDate>Tue, 09 Apr 2024 07:14:53 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ— éœ€æœç´¢çš„å¤§å¸ˆçº§å›½é™…è±¡æ£‹â€ï¼ŒRuoss ç­‰äºº 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>