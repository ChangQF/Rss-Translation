<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 06 Jul 2024 21:15:05 GMT</lastBuildDate>
    <item>
      <title>新的（更具数学性的）强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</link>
      <description><![CDATA[嗨  我已经参加了强化学习课程，我认为我对这些概念有很好的掌握，但正在寻找强化学习领域算法开发的当前前沿 有什么算法 / 主题的想法可以让我开始吗？    提交人    /u/Total-Ad-4461   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwuexy/new_more_mathematical_reinforcement_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 17:09:36 GMT</pubDate>
    </item>
    <item>
      <title>张量的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwpl60/the_problem_with_tensors/</link>
      <description><![CDATA[大家好，我在使用 DQN 时遇到了一个问题，即张量。当模型开始训练时，模型会收到以下维度的张量：torch.Size([1, 64, 64])，大约 10 秒后，模型会收到以下数据：torch.Size([32, 1, 64, 64])，因此我们收到此错误： Traceback (most recent call last): File &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;, line 141, in &lt;module&gt; agent.update_model(state, new_state, action, reward, compl) 文件 &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;，第 95 行，在 update_model 中 q_values = self.model(states) ^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl 中 return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl return forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\Desktop\ai project\Kaori\Osu_DQN\main.py&quot;，第 40 行，在 forward x = self.linear_block(x) ^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在_wrapped_call_impl 返回 self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl 中返回 forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\container.py&quot;，第 217 行，在 forward 输入 = module(input) ^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1532 行，在 _wrapped_call_impl 中返回 self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py&quot;，第 1541 行，在 _call_impl 中返回 forward_call(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\Tim\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\linear.py&quot;，第 116 行，在正向返回 F.linear(input, self.weight, self.bias) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ RuntimeError: mat1 和 mat2 形状无法相乘 (32x1024 和 16x11) [56.0 秒内完成]  这是模型更新代码： def update_model(self, state, new_state, action, reward, compl): # 用 память self.memory.append([state, new_state, action, reward, compl]) # 完成页面后，如果 len(self.memory) &gt; 则返回结果self.max_memory_size: self.memory.pop(0) # 获取模型，然后根据条件判断 if len(self.memory) &gt;= self.batch_size: # 随机取值 batch = random.sample(self.memory, self.batch_size) # 随机取值 states = torch.stack([transition[0] for transition in batch]) new_states = torch.stack([transition[1] for transition in batch]) action = torch.tensor([transition[2] for transition in batch]) rewards = torch.tensor([transition[3] for transition in batch]) finishes = torch.tensor([transition[4] for transition in batch]) # 计算 Q 值 q_values = self.model(states) target_q_values = q_values.clone() max_next_q = torch.max(self.model(new_states), dim=1)[0] target_q_values[torch.arange(self.batch_size), action] = rewards + self.gamma * (1 - finishes) * max_next_q # 计算并返回 loss = self.criterion(q_values, target_q_values) self.optimizer.zero_grad() loss.backward() self.optimizer.step()     提交人    /u/Kepler-nn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwpl60/the_problem_with_tensors/</guid>
      <pubDate>Sat, 06 Jul 2024 13:27:10 GMT</pubDate>
    </item>
    <item>
      <title>如何开始学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwpicd/how_to_start_learning/</link>
      <description><![CDATA[大家好，我是新来的，我对使用强化学习创建自己的项目非常感兴趣。我想为自己做一些小项目，这可能会帮助我建立自己的投资组合。  关于我应该从哪里开始学习？什么/哪里是最好的源材料。    提交人    /u/Educational-Gene3665   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwpicd/how_to_start_learning/</guid>
      <pubDate>Sat, 06 Jul 2024 13:23:22 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些帮助来完成我正在进行的项目，即微尺度的路径规划算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwnc10/i_need_some_help_with_my_on_going_project_that_is/</guid>
      <pubDate>Sat, 06 Jul 2024 11:19:31 GMT</pubDate>
    </item>
    <item>
      <title>您好，我正在使用 DDPG 进行四轴飞行器的轨迹跟踪，根据这个训练图我可以得出什么结论？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwm21c/hello_im_using_ddpg_for_trajectory_tracking_for_a/</link>
      <description><![CDATA[        由    /u/OkFig243   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwm21c/hello_im_using_ddpg_for_trajectory_tracking_for_a/</guid>
      <pubDate>Sat, 06 Jul 2024 09:50:09 GMT</pubDate>
    </item>
    <item>
      <title>DQN 在自定义健身环境中无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</link>
      <description><![CDATA[大家好， 我一直想尝试自定义健身房环境。我使用 tensorflow 用 DQN 解决了​​冰冻湖问题 - 并且想创建一个类似的环境并用 DQN 解决它。 我的环境是一个 4x5 网格单元仓库，里面有一个机器人、一个目标和一个障碍物。所以，与冰冻湖非常相似。 我的问题是，一旦 epsilon 衰减，并且算法正在采取贪婪行动，它采取的行动就是错误的（总是相同的，取决于初始化，但例如，它会告诉机器人始终向上） 这是我尝试过的和我所知道的：  环境应该正常工作。我对其进行了广泛的测试，&amp;用经典 Q 学习解决了这个问题，而且效果很好 我几乎从我的冻湖代码中复制/粘贴了我的 DQN 代码。  我玩过超参数，增加了网络的复杂性，但没有任何效果，而且我对此持怀疑态度，因为我的自定义环境和冻湖之间应该没有太大区别（我想？）  知道我应该在哪里寻找吗？我可以在这里分享代码，但它有点密集，我不确定要分享哪一部分，所以我不想淹没，但如果你想要代码的任何特定部分，我很乐意分享     提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwkl4n/dqn_not_learning_in_custom_gym_environment/</guid>
      <pubDate>Sat, 06 Jul 2024 08:01:03 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用 RL 制作一款回合制策略游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</link>
      <description><![CDATA[        由    /u/Novel_Can_6870  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwf3hl/im_making_a_turnbased_strategy_game_using_rl/</guid>
      <pubDate>Sat, 06 Jul 2024 02:23:55 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理的行为正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</link>
      <description><![CDATA[      大家好 我刚刚创建了我的第一个 PPO 代理。在 Cartpole-v1 环境中进行训练时，我收到以下响应： https://preview.redd.it/8kufxuh59sad1.png?width=640&amp;format=png&amp;auto=webp&amp;s=ddf17252bea091b0738973645612f1095364489e 为什么在代理似乎最终收敛时，性能在 500 次迭代后仍会出现一些突然下降的情况？ 谢谢前进。 编辑：在实现 GAE 之后，经过几次迭代后，时间线在 cartpole 步骤上完全稳定（不幸的是，无法在编辑中分享图表）。    提交人    /u/Muscle_Robot   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dwbc25/is_my_ppo_agent_behaving_correctly/</guid>
      <pubDate>Fri, 05 Jul 2024 23:13:27 GMT</pubDate>
    </item>
    <item>
      <title>移动平均数对于评估模型性能是否必要</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dw2kxk/is_moving_average_necessary_for_evaluating_model/</link>
      <description><![CDATA[我正在用 Pong 游戏练习 RL！    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dw2kxk/is_moving_average_necessary_for_evaluating_model/</guid>
      <pubDate>Fri, 05 Jul 2024 16:52:05 GMT</pubDate>
    </item>
    <item>
      <title>Mamba 背后的思想：Albert Gu 谈论状态空间模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</link>
      <description><![CDATA[        由    /u/phoneixAdi  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvutc8/the_mind_behind_mamba_albert_gu_talks_about_state/</guid>
      <pubDate>Fri, 05 Jul 2024 10:29:00 GMT</pubDate>
    </item>
    <item>
      <title>赛车开放式 AI 健身房 - 推出是否应覆盖整个赛道？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</link>
      <description><![CDATA[我试图复制此代码，但对 rollout 生成感到非常困惑。 步骤 4：生成随机 rollout 对于赛车环境，VAE 和 RNN 都可以使用随机 rollout 数据 - 即通过在每个时间步随机采取行动而生成的观察数据。实际上，我们使用伪随机动作，迫使汽车最初加速，以使其离开起跑线。 每次推出都应该覆盖整个赛道吗？ https://medium.com/applied-data-science/how-to-build-your-own-world-model-using-python-and-keras-64fb388ba459 我一直在尝试运行代码，但推出后赛道从未完成。训练后我也看不到它完成。    提交人    /u/Competitive-Chip5872   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvr624/carracing_open_ai_gym_should_rollout_cover_the/</guid>
      <pubDate>Fri, 05 Jul 2024 06:15:32 GMT</pubDate>
    </item>
    <item>
      <title>针对多个 TSP/VRP/和其他变体的多智能体强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</link>
      <description><![CDATA[你好，我是一名最近开始进行 RL 研究的学生。 我对包括 CO 和 MARL 在内的领域很感兴趣。即使有很多好的 RL 和启发式算法，我也想在更大规模和更复杂的领域尝试一些实验。 但是，对于多旅行商问题或 VRP，我很难找到 SOTA MARL 方法。（我在 github 上找到了一些论文，但通常没有代码实现） 除了论文之外，如果你能分享你的知识和见解，那将非常有帮助。    提交人    /u/QingdaoCraftBeer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvpy88/multiagent_reinforcement_learning_for_multiple/</guid>
      <pubDate>Fri, 05 Jul 2024 04:57:29 GMT</pubDate>
    </item>
    <item>
      <title>使用 Gym 训练动作分类模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvoovw/using_gymnasium_to_train_an_action_classification/</link>
      <description><![CDATA[在任何人说之前，我明白这不是 RL 问题，谢谢。但我必须提到，我是团队的一员，我们都在尝试不同的方法，而我得到了这个。 首先，下面是我的代码： # 乒乓球课的自定义健身环境 TableTennisEnv(gym.Env): def __init__(self, frame_tensors, labels, frame_size=(3, 30, 180, 180)): super(TableTennisEnv, self).__init__() self.frame_tensors = frame_tensors self.labels = labels self.current_step = 0 self.frame_size = frame_size self.n_actions = 20 # 唯一动作的数量 self.observation_space = space.Box(low=0, high=255, shape=frame_size, dtype=np.float32) self.action_space = space.Discrete(self.n_actions) self.normalize_images = False self.count_reset = 0 self.count_step = 0 def reset(self, seed=None): 全局 total_reward, maximum_reward self.count_reset += 1 print(&quot;Reset called: &quot;, self.count_reset) self.current_step = 0 total_reward = 0 maximum_reward = 0 return self.frame_tensors[self.current_step], {} def step(self, action): 全局 total_reward, maximum_reward act_ten = torch.tensor(action, dtype=torch.int8) 如果 act_ten == self.labels[self.current_step]: 奖励 = 1 total_reward += 1 else: 奖励 = -1 total_reward -= 1 maximum_reward += 1 print(&quot;实际： &quot;, self.labels[self.current_step]) print(&quot;预测： &quot;, action) self.current_step += 1 print(&quot;步骤： &quot;, self.current_step) done = self.current_step &gt;= len(self.frame_tensors) obs = self.frame_tensors[self.current_step] if not done else np.zeros_like(self.frame_tensors[0]) truncated = False if done: print(&quot;最大奖励： &quot;, maximum_reward) print(&quot;获得的奖励： &quot;, total_reward) print(&quot;准确度： &quot;, (total_reward/maximum_reward)*100) return obs, reward, done, truncated, {} def render(self, mode=&#39;human&#39;): pass # 通过较小的批次处理来减少内存使用量 env = DummyVecEnv([lambda: TableTennisEnv(frame_tensors, labels, frame_size=(3, 30, 180, 180))]) timesteps = 100000 try: # 用较小的批量大小初始化 PPO 模型 model1 = PPO(&quot;MlpPolicy&quot;, env, verbose=1, learning_rate=0.03, batch_size=5, n_epochs=50, n_steps=4, tensorboard_log=&quot;./ppo_tt_tensorboard/&quot;) # 训练模型 model1.learn(total_timesteps=timesteps) # 保存训练好的模型 model1.save(&quot;ppo_table_tennis_3_m1_MLP&quot;) print(&quot;模型 1 训练和保存成功完成。&quot;) tr1 = total_reward mr1 = maximum_reward total_reward = 0 maximum_reward = 0 print(&quot;模型 1 的准确率（100 Epochs）：&quot;, (tr1/mr1)*100) except Exception as e: print(f&quot;模型训练或保存期间发生错误：{e}&quot;)  有 1514 个视频剪辑用于训练，已转换为矢量。每个视频剪辑矢量的尺寸为 (180x180x3)x30，因为我提取了 30 帧作为输入。 问题出现在训练期间。在最初几个步骤中，模型运行良好。过了一会儿，预测的动作停止变化。它只会一遍又一遍地预测 1-20 中的一个数字。我是体育馆库的新手，因此我不确定是什么导致了这个问题。我已经在 StackOverflow 上发布了这个问题，到目前为止我还没有收到太多帮助。 如能得到您的任何意见，我们将不胜感激。谢谢。    提交人    /u/Farenhytee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvoovw/using_gymnasium_to_train_an_action_classification/</guid>
      <pubDate>Fri, 05 Jul 2024 03:40:03 GMT</pubDate>
    </item>
    <item>
      <title>“在 HATETRIS 中创下世界纪录”，Dave & Filipe 2022（AlphaZero 失败后高度优化的光束搜索）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</guid>
      <pubDate>Thu, 04 Jul 2024 16:14:25 GMT</pubDate>
    </item>
    <item>
      <title>“AlphaZero 的蒙特卡洛图搜索”，Czech 等人 2020 年（将树转换为 DAG 以节省空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</guid>
      <pubDate>Thu, 04 Jul 2024 16:01:49 GMT</pubDate>
    </item>
    </channel>
</rss>