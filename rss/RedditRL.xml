<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 17 Sep 2024 15:17:41 GMT</lastBuildDate>
    <item>
      <title>预订建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</link>
      <description><![CDATA[我需要什么书来进行强化学习？ 我希望书既直观又具有数学性，我能理解艰难的数学，因为我有很强的数学背景。 向我推荐一些有很好解释并且包含很好数学内容的书。    提交人    /u/Evening-Passenger311   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</guid>
      <pubDate>Tue, 17 Sep 2024 14:25:13 GMT</pubDate>
    </item>
    <item>
      <title>如何优化奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</link>
      <description><![CDATA[我一直在用强化学习训练汽车，但一直遇到奖励函数问题。我希望汽车保持较高的恒定速度，并一直使用 speed 和最近的 progress 等参数来奖励它。但是，我注意到，当仅根据速度进行奖励时，汽车有时会加速，但会立即减速，而进度似乎根本没有影响。我还奖励了其他动作，例如 all_wheel_on_track，这很有帮助，因为每次汽车偏离赛道都会受到 5 秒的惩罚。 附注：这是 aws deep racer 比赛，如果您愿意，可以在此处查看参数。    提交人    /u/KatCelest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</guid>
      <pubDate>Tue, 17 Sep 2024 07:41:31 GMT</pubDate>
    </item>
    <item>
      <title>在没有标签的情况下使用强化学习创建合成数据</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fifx0s/synthetic_data_creation_using_reinforcement/</link>
      <description><![CDATA[那么。假设我们有每周的电力数据，但我们想创建一个模型来捕捉可能导致停电的使用高峰。强化学习代理能否在一周的不同日子中创建使用分布。代理能否捕捉模式并使用模拟数据知道根据其模拟，在特定的星期四而不是星期三或星期日将会发生停电？如果没有每日数据，它将如何评估其预测？    提交人    /u/No_Refrigerator_7841   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fifx0s/synthetic_data_creation_using_reinforcement/</guid>
      <pubDate>Mon, 16 Sep 2024 21:03:57 GMT</pubDate>
    </item>
    <item>
      <title>推荐阅读因果强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</link>
      <description><![CDATA[嗨， 我的经济学背景是因果推理（据我所知，我的背景是鲁宾学派，而不是 Pearls 学派），我想了解更多关于因果 RL 的知识。我已经观看了关于因果强化学习的本教程，但我仍然不太明白它在做什么。 有推荐阅读材料吗？这篇论文是一个好的开始吗？ 此外，我目前的理解是“传统”因果推理假设因果关系，而（一些）RL 则从数据中学习它们而不做假设？这是正确的吗？ 谢谢！    提交人    /u/WinnieXi   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</guid>
      <pubDate>Mon, 16 Sep 2024 15:36:16 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的 OpenAI Gymnasium 向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi4sus/openai_gymnasium_vector_in_observation_space/</link>
      <description><![CDATA[大家好，我在真实设备上使用 Stable Baselines3 (SB3)，并使用自定义 OpenAI Gymnasium 环境在 Python 和 Arduino 之间创建了一个接口。我想将之前的观察结果包含在我的观察空间中。目前，我的观察空间如下所示： self.high = np.array([self.maxPos, self.minDelta, self.maxVel, self.maxPow], dtype=np.float32) self.low = np.array([self.minPos, self.minDelta, self.minVel, self.minPow], dtype=np.float32) self.observation_space = space.Box(self.low, self.high, dtype=np.float32) 其中 min 和 max 值为 np.float32。我的 state 定义为： self.state = [self.ballPosition, self.ballPosition - self.desiredBallPos, self.ballVelocity, self.lastFanPower] 我想将先前位置的向量添加到我的状态中，如下所示： self.posHist = [self.stateHist[-1][0], self.stateHist[-2][0], self.stateHist[-3][0], self.stateHist[-4][0]] 然后： self.state = [self.ballPosition, self.ballPosition - self.desiredBallPos, self.ballVelocity, self.lastFanPower, self.posHist] 我应该如何更改我的self.observation_space? 问题：我应该如何修改我的 self.observation_space 以适应这些先前的位置？我想要添加此信息的原因是向网络提供有关先前状态和系统动态的数据，因为通信存在一些延迟。如果您发现此方法存在任何问题，请告诉我。我对 RL 还很陌生，仍在学习。    提交人    /u/Enroot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi4sus/openai_gymnasium_vector_in_observation_space/</guid>
      <pubDate>Mon, 16 Sep 2024 13:35:24 GMT</pubDate>
    </item>
    <item>
      <title>需要在目标机器人环境中使用 DDPG+HER 实现 MAML 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi190y/need_help_with_maml_implementation_with_ddpgher/</link>
      <description><![CDATA[大家好， 我正在做一个使用 DDPG+HER+MPI 实现 MAML 的项目。我使用 Tianhong Dai 的 hindsight-experience-replay 作为基础，并希望使用 Gymnasium fetch robotics 和 panda-gym 环境测试我的实现。目前。我面临一些挑战，希望得到一些建议来推动这一进程。 为了测试我的实现，我没有使用多个任务进行训练，而是首先尝试使用单个环境来检查实现是否有效。我可以通过调整 alpha 和 beta 参数来训练简单的环境，如 fetch-reach 或 panda-reach。但是，当我转而测试更复杂的任务（如推送或 pnp）时，即使使用不同的超参数变化，训练也会遇到困难。 当我尝试训练多个任务时，情况会变得更糟，例如使用 fetch-push 和 fetch-pnp 作为训练环境，同时尝试学习 fetch-slide 作为保留任务。 我知道将 MAML 与 DDPG（使用重放缓冲区）等离策略算法相结合并不常规，但我很想探索这种方法，看看这里是否有潜力。 我已经将代码上传到这里，如果有人想看看，请提供一些关于如何修复它的建议。 https://github.com/ncbdrck/maml_ddpg_her    由   提交  /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi190y/need_help_with_maml_implementation_with_ddpgher/</guid>
      <pubDate>Mon, 16 Sep 2024 10:34:17 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器和机器人学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhyzae/decision_transformer_and_robot_learning/</link>
      <description><![CDATA[有人知道任何文章或论文中使用决策变换器来解决机器人手臂操纵任务吗？    提交人    /u/Significant-Gene1539   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhyzae/decision_transformer_and_robot_learning/</guid>
      <pubDate>Mon, 16 Sep 2024 07:43:10 GMT</pubDate>
    </item>
    <item>
      <title>是否有可能用 SFT 来模仿 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhw29h/is_it_possible_to_mimic_rl_with_sft/</link>
      <description><![CDATA[（不确定这是否是询问此问题的正确论坛） 我有一个使用 OpenAI API 运行的代理，我想通过 RL 微调该代理。  但是，鉴于 OpenAI 仅提供 SFT API，我想知道是否可以执行以下操作 -   基于当前模型的样本情节（在采样期间融入探索） 计算每集的奖励（或者对于简单情况，获胜的奖励始终为 1） 对于每个获胜情节，创建监督标签（状态、动作） 从 3 开始对数据集应用微调  重复此过程几轮。  这会起作用吗？这实际上等同于为代理运行 RL 吗？     提交人    /u/WriterAccomplished65   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhw29h/is_it_possible_to_mimic_rl_with_sft/</guid>
      <pubDate>Mon, 16 Sep 2024 04:20:52 GMT</pubDate>
    </item>
    <item>
      <title>惩罚是否可以在任一方向（+ 和 -）改变权重？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhs3xa/is_it_feasible_for_punishments_to_alter_weights/</link>
      <description><![CDATA[我很难想出一个部署强化学习算法的策略，如果能得到任何见解，我将不胜感激。我正在构建一个模型，无论模型采取什么操作，该模型的状态都会有 96% 的时间失败。这会导致对权重的惩罚总是过早地降到最低点（这意味着它会降到 0，除非我将奖励提高到天文数字，否则永远不会回升）。我知道这是一个奇怪的问题，但如果惩罚不是严格地减去权重，这是否有意义？我的想法是，奖励会保持相同的权重，惩罚可以是加法或减法，如果这有意义的话。我只是担心它只会朝一个方向（向下）发展，而实际上，如果这有意义的话，我希望权重在两个方向（向上和向下）波动得更自由一些。    提交人    /u/Correct_Truth9920   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhs3xa/is_it_feasible_for_punishments_to_alter_weights/</guid>
      <pubDate>Mon, 16 Sep 2024 00:53:03 GMT</pubDate>
    </item>
    <item>
      <title>Dagger 手册专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhpy4q/manual_expert_for_dagger/</link>
      <description><![CDATA[大家好， 我正在研究一个结合运动规划的模仿学习问题。我有一个专家，他给出了 EEf 姿势，我用它来收集数据。行为克隆工作得还不错，符合预期。 我想继续使用 Dagger，但我必须花费大量时间来设置专家，以便通过 Dagger 处理在线查询，而且每次迭代可能都很慢。 鉴于我的系统不是高频的，并且每集有 10 个转换，每次查询的手动输入是否可行？    提交人    /u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhpy4q/manual_expert_for_dagger/</guid>
      <pubDate>Sun, 15 Sep 2024 23:07:56 GMT</pubDate>
    </item>
    <item>
      <title>“扩散强制：下一个标记预测与全序列扩散相结合”，Chen 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</guid>
      <pubDate>Sun, 15 Sep 2024 16:44:39 GMT</pubDate>
    </item>
    <item>
      <title>批判情景环境中的重要性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fh8mlq/critic_importance_in_episodic_environments/</link>
      <description><![CDATA[大家好，在这篇文章中：https://ai.stackexchange.com/questions/25739/what-are-the-advantages-of-rl-with-actor-critic-methods-over-actor-only-methods 有以下段落：  一个实际的好处是，批评者可以使用 TD 学习进行引导，使他们能够在线学习所采取的每一步……像 REINFORCE 这样的纯演员算法……需要情节问题。它们可以学习的最小单位是整个情节。这是因为，如果没有评论家提供价值估计，那么估计回报的唯一方法就是从一集的结尾抽样实际回报。  我想进一步了解这一点。如果任何状态已经显示出一些奖励，为什么我需要评论家价值解释？ 我认为另一种提问方式是 - 假设对于每个状态，我预测每个奖励的概率，我可以使用这个分布的平均值作为该状态的评论家值吗？    提交人    /u/Potential_Hippo1724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fh8mlq/critic_importance_in_episodic_environments/</guid>
      <pubDate>Sun, 15 Sep 2024 09:27:00 GMT</pubDate>
    </item>
    <item>
      <title>需求变化对 RL 训练稳定性的影响</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fgt1ld/impact_of_varying_demand_on_rl_training_stability/</link>
      <description><![CDATA[      我正在使用不同的需求文件（代表汽车到达流程/时间表）来训练我的 RL 算法。每个文件包含不同数量的车辆，范围在 800 到 1200 之间。在我的问题中，如果车辆未与客户匹配，则它们会在一定时间后离开系统。累积奖励基于已服务的车辆和未服务的车辆。因此，如果我们在需求文件中拥有更多车辆，那么如果我们对未服务的车辆保持谨慎，我们就有可能积累更多奖励。 实际上，我有一个更复杂的问题，但我试图尽可能地简化它。 在训练过程中，我注意到累积奖励在各个情节中存在显着波动（我在每集结束时记录累积奖励）。 我的问题是：需求文件中车辆数量的不同是否会导致学习过程不稳定？如果是的话，我应该如何处理才能稳定训练并提高学习效果？ https://preview.redd.it/0iv7qojvmtod1.png?width=864&amp;format=png&amp;auto=webp&amp;s=ec5d15963c9eb2d4c8f306c5ff9a06babf7eb11d    submitted by    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fgt1ld/impact_of_varying_demand_on_rl_training_stability/</guid>
      <pubDate>Sat, 14 Sep 2024 18:52:09 GMT</pubDate>
    </item>
    <item>
      <title>当思路链链接了太多的想法时。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fgc0wm/when_the_chainofthought_chains_too_many_thoughts/</link>
      <description><![CDATA[        提交人    /u/moschles   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fgc0wm/when_the_chainofthought_chains_too_many_thoughts/</guid>
      <pubDate>Sat, 14 Sep 2024 02:25:04 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>