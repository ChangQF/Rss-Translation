<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 07 Mar 2024 06:19:11 GMT</lastBuildDate>
    <item>
      <title>负解释方差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8mjz4/negative_explained_variance/</link>
      <description><![CDATA[   我一直在致力于一个项目，试图亲身体验一些强化学习。 我的环境m 建模是一个时间序列，我正在训练一个代理通过交易金融衍生品（特别是杠杆永续期货）来最大化财务回报。因此，它需要最大化财务收益并最小化财务损失。每当代理平仓时都会给予奖励，奖励与相对于初始投资的收益/损失百分比成正比。 动作空间是一个具有 4 个动作的多离散空间，不执行任何操作，开仓持仓（以及要使用的资金百分比）、平仓、结束事件。观察空间约为 100 个，包含当前开盘/收盘/交易量和一些财务指标（技术分析）以及某些历史时期的聚合指标。 使用的算法是 DDPPO（56 个 cpu），使用RLLIB 进行了一些修改，特别是使用实现操作屏蔽的自定义模型。例如，确保代理只能在有仓位存在时才可以平仓，并且只有在没有持仓并且总收益或损失&gt; 40％时才能结束一个情节，或者总情节长度大于某个数量步骤。 我还使用 具有 8 个头和 4 个变压器的注意力网络，并使用 RE3 进行探索。 与上述所有内容我都能够获得良好的结果，并且代理似乎确实在学习，基于平均奖励和针对未见数据的检查点的实证测试。 但是，看看 TensorBoard 指标，我我看到一些对我来说没有多大意义的事情，所以我在这里寻求任何帮助或建议：  是什么导致解释方差为负？我应该预计这个数字会开始增加吗？考虑到在如此复杂的环境中奖励意味着增加，如此低的方差如何有意义？ 为什么熵并没有真正下降？这很重要吗？ 为什么损失继续缓慢增加？我预计这个数字会在某个时候开始下降？这是代理仍在学习的标志吗？  提前感谢您的指点！ 一些配置参数： 地平线 = 60 .training( train_batch_size = 地平线 * 4, sgd_minibatch_size = 地平线 * 2, num_sgd_iter = 2, gamma = 1.0, lambda_ = 1.0, entropy_coeff = 0.001, grad_clip = 10, model={ “fcnet_hiddens”: [1024, 1024 ], “max_seq_len”: 地平线, “attention_num_transformer_units”: 4, “attention_num_heads”: 8, } )  https://wandb.ai/kingsignificant5097/uncategorized?workspace=user-kingsignificant5097 https://preview.redd.it/wqbzrf6dgumc1.png?width=3516&amp;format=png&amp;auto=webp&amp; amp; s=eacf2d106283d3bc2143a9a9e3f7f68f5bcbdb67   由   提交/u/KingSignificant5097   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8mjz4/negative_explained_variance/</guid>
      <pubDate>Thu, 07 Mar 2024 05:29:18 GMT</pubDate>
    </item>
    <item>
      <title>测试时无法扩展自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxow/cant_scale_custom_environment_when_testing/</link>
      <description><![CDATA[我正在制作自定义Open AI Gym 中的 Boid 植绒环境具有稳定的基线 3.  ​ 错误1： 我已经创建了环境并测试了3个boids，但是当我测试10个boids时，它给出了错误： 错误 自定义环境 我从未对 boids 的数量进行硬编码，也没有其他明显的问题。 ​ 错误 2： 我用 20 个不同的初始化位置测试了模型，但对于大多数情况，我得到了这个奖励，600200，他们按预期移动。  但是，当我重新训练模型时，它的表现不佳，参数完全相似。这是一个训练有素的模型，是侥幸还是过度拟合？  剧集奖励  祝你有美好的一天！   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxow/cant_scale_custom_environment_when_testing/</guid>
      <pubDate>Thu, 07 Mar 2024 04:56:12 GMT</pubDate>
    </item>
    <item>
      <title>测试时无法扩展自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxop/cant_scale_custom_environment_when_testing/</link>
      <description><![CDATA[        由   提交/u/Sadboi1010   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b8lxop/cant_scale_custom_environment_when_testing/</guid>
      <pubDate>Thu, 07 Mar 2024 04:56:12 GMT</pubDate>
    </item>
    <item>
      <title>无法理解 RL 代码中的大多数数学公式！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/</link>
      <description><![CDATA[在我的研究中，我认为在论文中计算公式很重要，这样我就可以更清楚地了解作者想要传达的内容以及主要贡献他们如何改进。我认为是时候学习更多数学来对强化学习进行更深入的研究了。  我认为强化学习和优化控制非常相似。那么也许学习一些凸优化和函数分析或者变异分析会很好？ 困惑我需要什么来提高我的 RL 知识，帮助！   由   提交/u/Tight-Ad789  /u/Tight-Ad789  reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b85or6/cant_understand_most_mathematical_formula_in_rl/</guid>
      <pubDate>Wed, 06 Mar 2024 17:31:49 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定的基线训练两个竞争网络/策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b846eo/training_two_competing_networkspolicies_using/</link>
      <description><![CDATA[我的环境中，2 个以上的竞争代理根据不同的奖励函数有完全不同的目标。此外，这些目标会根据环境的情况而变化。我想训练代表“个性”的不同网络/策略。 一种方法是训练一个通用代理，该代理适用于在观察空间中编码的任何目标和奖励函数配置，但我希望这会增加网络的复杂性。 我希望另一种方法是同时进行 2 个以上的网络/策略训练。步骤/重置返回 {&#39;agent_0&#39;: [...], &#39;agent_1&#39;: [...]} 包含每个代理的观察和奖励，是否可以将一个用于网络 1，另一个用于网络 2 ？ 我正在使用 Stable Baselines 3 进行训练（使用 supersuit 转换 PettingZoo 环境，以便 sb3 可以在其上进行训练），目前训练是通过 sb3 抽象出来的：  env = ss.pettingzoo_env_to_vec_env_v1(env) env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=“stable_baselines3”) model = SAC(MlpPolicy, env) model.learn(total_timesteps=steps) model.save(run_name )  我想到的方法有意义吗？如果是这样，我如何以这种方式同时训练 2 个以上网络？   由   提交/u/davidschep  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b846eo/training_two_competing_networkspolicies_using/</guid>
      <pubDate>Wed, 06 Mar 2024 16:34:46 GMT</pubDate>
    </item>
    <item>
      <title>RL 在流体动力学与控制中的应用以及仿真软件选择的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b83n0h/questions_on_applying_rl_on_hydrodynamics_control/</link>
      <description><![CDATA[我正在写我的论文，该论文基本上围绕水下水翼艇展开（示例）和纵向（也可能是横向）运动的控制。主要思想是将其视为高度/俯仰/横滚控制问题（状态是高于水面的高度、俯仰/横滚角度及其一阶导数），并实现 RL 作为制定控制算法的手段。 &lt; p&gt;没有真正的船来尝试算法，所以我们的想法是使用模拟环境并为其提供船、箔、支柱等参数/属性。我的问题基本上是：  使用基于模型的方法（我可以实现修剪点的相对简单的线性表示）而不是仅仅使用是否更好？暴力破解它吗？ 您是否知道任何可以与空气/流体动力学很好地配合的模拟环境（尽管我认为计算升力和阻力系数并将它们直接插入力会更精确）？我之前曾从事过 Unity3D 工作，我喜欢在游戏对象上轻松拖放脚本。我还听说过 Gazebo （特别是 VRX 或 UVSim 包）。  模拟环境可以无头运行吗？我认为运行飞行器的可视化表示会非常耗时。如果我可以关闭图形并只期望物理引擎运行刚体/海交互，那就更好了。  注释我感兴趣的模拟是稳定的迎头航向（无机动），并且仅当飞船处于箔伯恩状态时（无需模拟船体与水的相互作用）。  ​   由   提交 /u/John_Skoun   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b83n0h/questions_on_applying_rl_on_hydrodynamics_control/</guid>
      <pubDate>Wed, 06 Mar 2024 16:13:23 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器实际上提供开环控制吗？有些人也将其称为任务实例的操作序列。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7zdhd/is_the_decision_transformer_actually_providing/</link>
      <description><![CDATA[ 由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7zdhd/is_the_decision_transformer_actually_providing/</guid>
      <pubDate>Wed, 06 Mar 2024 13:13:29 GMT</pubDate>
    </item>
    <item>
      <title>机器人研究场景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7wj0u/scenario_for_robotics_research/</link>
      <description><![CDATA[大家好，我很想知道以下公司在人工智能、计算机视觉和机器人领域的交叉研究的当前趋势：  特斯拉 谷歌 微软 亚马逊 苹果 Meta&lt; /li&gt;  正在/曾经在这些公司工作的机器人科学家/机器人研究人员，您能否介绍一下这些公司的研究标准，尤其是机器人和人工智能方面的研究标准？ 我的简介：我是一名有志于研究人工智能和机器人技术的人。我从事机器学习、计算机视觉、深度学习方面的工作。在我的硕士学位论文中，我正在研究使用计算机视觉和深度强化学习的室内移动机器人导航。如果有人与我从事同一主题，请发表评论并发送聊天请求:)）） 和平 ✌   &amp; #32；由   提交 /u/Quirky_Assignment707   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7wj0u/scenario_for_robotics_research/</guid>
      <pubDate>Wed, 06 Mar 2024 10:31:50 GMT</pubDate>
    </item>
    <item>
      <title>分子生成：优化复杂的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7var0/molecular_generation_optimizing_a_complex/</link>
      <description><![CDATA[你好， 我编写了一个基于神经网络的模型，其目标是优化某些 &lt;输入分子的strong&gt;属性。该属性由以下函数提供的eval(molecule)函数给出例如rdkit。一个示例属性可以是分子量。 对于那些不知道的人，分子可以表示为标记图，其中节点是原子，边是键。继该领域最近的工作之后，我的分子优化方法是通过“附加”分子来完成的。从预先构建的词汇表（也称为 Motif）到输入分子的图表。 我的模型遵循以下步骤： 给定输入分子， 1. 从词汇 (GNN+MLP) 2. 从 Motif 中选择应与 Motif 形成连接的候选原子 (GNN+MLP) 3. 从 Motif 中选择应与 Motif 形成连接的候选原子Molecule (GNN+MLP) 4. 最后画出结合两组附件的 BondType (NONE, SINGLE, DOUBLE, TRIPLE) 我认为这个模型是代理的策略函数： a = pi_theta(s) s（状态）：输入分子 a（动作）：基序、基序附着原子和键类型的元组输入分子 theta：模型参数 动作空间是离散的并且非常大。 策略函数应该由代理迭代使用，直到出现END主题被选中，表明生成已结束。考虑多次迭代： s* = PI_theta(s) s* 是代理生成的优化分子，可最大化属性，给定输入分子。 仅评估最终结果 s\* 是有意义的。作用时，药剂可能会产生多个中间分子；如果根据这些评估属性分数，它甚至可能低于初始分子。 所以我的优化问题将类似于： ARGMAX_theta eval( PI_theta(s) )&lt; /p&gt; 其中 eval( ) 是一个函数，给定一个分子图（例如 s*），它可以告诉属性分数。 但是我不相信一些事情： - eval 不可微分 - PI_theta 强烈依赖于输入：给定 s，优化所需的迭代次数可以换。此外，GNN 将根据绘制的 Motif 在 s 的不同图表上工作 您有任何想法/建议/类似问题可以请指出我以处理该网络的优化吗？  谢谢   由   提交/u/rutay_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7var0/molecular_generation_optimizing_a_complex/</guid>
      <pubDate>Wed, 06 Mar 2024 09:08:40 GMT</pubDate>
    </item>
    <item>
      <title>罗纳德·威廉姆斯（REINFORCE，1992）上个月去世 (2024-02-16)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7pjar/ronald_williams_reinforce_1992_died_last_month/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7pjar/ronald_williams_reinforce_1992_died_last_month/</guid>
      <pubDate>Wed, 06 Mar 2024 03:30:58 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7o6uy/rl_algo_failing_to_learn/</link>
      <description><![CDATA[      ​ https://preview.redd.it/ohq2w15akmmc1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=de47e6683213af03218fa754ce18a65c700fc9c1  我正在尝试重新实现最近的一篇论文，其中奖励是基于偏好的。基础代理是 SAC，但代理似乎无法改进，在 150 到 60 集回报范围之间波动，有时在 350 左右达到峰值。回报波动背后有什么建议或原因吗？ &lt; !-- SC_ON --&gt;  由   提交/u/kengsleh  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7o6uy/rl_algo_failing_to_learn/</guid>
      <pubDate>Wed, 06 Mar 2024 02:28:08 GMT</pubDate>
    </item>
    <item>
      <title>小心小型网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7k3oh/careful_with_small_networks/</link>
      <description><![CDATA[ 由   提交 /u/FriendlyStandard5985   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7k3oh/careful_with_small_networks/</guid>
      <pubDate>Tue, 05 Mar 2024 23:24:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 PPO 中 new_prob_policy 和 old_prob_policy 之间的比率不是 1？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b7dlav/why_in_ppo_the_ratio_between_new_prob_policy_and/</link>
      <description><![CDATA[理论上我不明白为什么 new_prob_policy 和 old_prob_policy 之间的比率不同于 1。当我玩游戏时，我使用 NN 来选择动作我和代理保存状态、操作和 old_prob_policy。然后，当我获得足够长的轨迹时，我计算 GAE，然后计算 new_prob_policy 和 old_prob_policy 之间的比率。要获取 new_prob_policy 我应该做什么？据我所知，到目前为止，我必须再次以相同的状态向神经网络提供信息，并收集我已经采取的相同操作的 new_prob_policy 。但这样它应该总是等于 1 因为永远不会改变。所以我的程序在某个地方是错误的，但我没有 Sundstrand 如何获得 new_prob_policy 的概率比不同于 1。请解释我错在哪里！  &amp; #32；由   提交 /u/Capittain-Nemo-9294   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b7dlav/why_in_ppo_the_ratio_between_new_prob_policy_and/</guid>
      <pubDate>Tue, 05 Mar 2024 19:08:48 GMT</pubDate>
    </item>
    <item>
      <title>深度MCCFR项目分享</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b74kok/deep_mccfr_project_sharing/</link>
      <description><![CDATA[https://github.com/davpat108/CITADELS_self_play 经过几个月的空闲时间工作，我终于完成了项目的工作版本。您对此有何看法？ 我唯一无法破解的是使用神经网络不仅可以猜测节点值，还可以提供探索的基本策略。游戏似乎太大了，使得网络的输出过于复杂。有什么办法解决这个问题吗？   由   提交/u/YellowOk1956   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b74kok/deep_mccfr_project_sharing/</guid>
      <pubDate>Tue, 05 Mar 2024 12:57:23 GMT</pubDate>
    </item>
    <item>
      <title>具有多代理设置的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b72gek/rl_with_a_multiagent_setting/</link>
      <description><![CDATA[大家好， 我目前正在生产调度的框架内探索多智能体强化学习（MARL），其中每个代理对应一台机器，代理必须选择要处理的操作。由于处理时间的多样性，并非所有机器都可以在一个决策时间点进行调度决策。在许多现有的 MARL 研究中，采用了联合行动的概念，即所有智能体共同选择一个行动。在此设置中，当前正在使用的机器会将所选操作（操作）合并到其队列中。这引发了关于联合行动有效性的问题，特别是在职位插入事件等动态场景中。我正在考虑是否可以为那些在决策点仍处于占用状态的机器引入虚拟操作。在这种情况下，如果我采用集中式评论家网络，它将如何处理这些虚拟动作？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b72gek/rl_with_a_multiagent_setting/</guid>
      <pubDate>Tue, 05 Mar 2024 10:52:05 GMT</pubDate>
    </item>
    </channel>
</rss>