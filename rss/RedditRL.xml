<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 04 Jun 2024 21:13:21 GMT</lastBuildDate>
    <item>
      <title>有人尝试用 RL 来模拟人类行为/情感吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d883of/anyone_worked_at_simulating_human/</link>
      <description><![CDATA[大家好，如果有人尝试用 RL 模拟人类行为或情感，我很乐意与您讨论。我一直在使用 TD 来实现这一点。但是当我们尝试模拟除恐惧、希望、快乐、悲伤之外的更多情绪时，它非常复杂。 我为什么要研究这个？我试图让 NPC 更人性化，比如模拟一些情况，比如流行病或战争。 如果有人正在研究这个，请联系。    提交人    /u/anchit_rana   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d883of/anyone_worked_at_simulating_human/</guid>
      <pubDate>Tue, 04 Jun 2024 21:02:46 GMT</pubDate>
    </item>
    <item>
      <title>利用多台计算机的计算能力训练PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d82mnt/training_ppo_using_computing_power_of_multiple/</link>
      <description><![CDATA[嗨， 我正在考虑使用多台计算机来训练我的 PPO 代理。有人可以建议我该怎么做吗？（就 ppo 训练架构和计算机通信而言） 我正在考虑在多台计算机上运行情节并计算后向传播梯度，然后将梯度发送回主计算机以计算平均值，然后再更新神经网络。 但我不确定这是正确的方法还是最好的方法。我想提高收敛速度和收敛稳定性。 如果有人能提供任何建议或指出我正确的资源，我将不胜感激。 谢谢。    提交人    /u/unknown_unknown_001   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d82mnt/training_ppo_using_computing_power_of_multiple/</guid>
      <pubDate>Tue, 04 Jun 2024 17:18:30 GMT</pubDate>
    </item>
    <item>
      <title>“使用 Antithesis SDK 解决 Zelda 问题”：探索 Zelda 并在关键状态下使用类似 Go-Explore 的重置来查找错误/黑客</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d7y12w/solving_zelda_with_the_antithesis_sdk_exploring/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d7y12w/solving_zelda_with_the_antithesis_sdk_exploring/</guid>
      <pubDate>Tue, 04 Jun 2024 14:06:39 GMT</pubDate>
    </item>
    <item>
      <title>很难理解 PPO 损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d7wqqy/hard_time_understanding_ppo_loss/</link>
      <description><![CDATA[      我正在实施 PPO 方法，到目前为止，它被证明是成功的。我成功地在健身房的月球着陆器上训练了它。但最终的损失图对我来说没有意义。据我所知，我们正在尝试将其最小化，因此损失越低，模型越好。但是看看损失和平均奖励图表： https://preview.redd.it/jbqf5bxczj4d1.png?width=996&amp;format=png&amp;auto=webp&amp;s=892e907eb860242d3a4d38309e9f0ce231056371 大约 25-50 步时，损失大幅减少，这应该意味着模型变得更好。但平均奖励也大幅下降。大约 100 步损失增加，因此平均奖励也增加。看起来更高的损失意味着更好的模型，但这对我来说没有意义    提交人    /u/Aydiagam   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d7wqqy/hard_time_understanding_ppo_loss/</guid>
      <pubDate>Tue, 04 Jun 2024 13:09:01 GMT</pubDate>
    </item>
    <item>
      <title>我是一名 AI 毕业生，想学习 RL。我该怎么做？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d7t5qj/i_am_an_ai_diploma_grad_that_wants_to_pick_up_rl/</link>
      <description><![CDATA[大家好，我是新来的。刚刚获得应用人工智能和分析文凭。我们在这里学到的 RL 很少，主要是监督/无监督/深度学习。 我做了一些研究，tensorflow RL 在 GitHub 上提供了 10 个教程，但不使用 gymnasium。Gymnasium 本身并没有提供很多关于如何修改环境的教程。PyTorch 有 4 个关于 RL 的教程。好的 YouTube 视频来自 2020 年，已经过时了。 学习 RL 的最佳方法是什么？同样重要的是，RL 的行业标准是什么样的？RL 开发人员通常使用哪些库 -PyTorch、keras RL、Tensorflow RL、稳定基线 3(?)。 任何建议都将不胜感激。感谢您花时间查看这篇文章。     由    /u/CoconutOperative 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d7t5qj/i_am_an_ai_diploma_grad_that_wants_to_pick_up_rl/</guid>
      <pubDate>Tue, 04 Jun 2024 09:40:00 GMT</pubDate>
    </item>
    <item>
      <title>自然政策梯度和 TRPO 资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d7s4lc/ressources_on_natural_policy_gradient_and_trpo/</link>
      <description><![CDATA[除了原始论文之外，我还在寻找有关自然策略梯度和 TRPO 的资源/文献？那里是否有一些深入研究数学的标准工作？    提交人    /u/jthat92   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d7s4lc/ressources_on_natural_policy_gradient_and_trpo/</guid>
      <pubDate>Tue, 04 Jun 2024 08:22:29 GMT</pubDate>
    </item>
    <item>
      <title>“无悔等待模型：最大化小费的多臂老虎机方法”（讽刺）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d77c8a/the_no_regrets_waiting_model_a_multiarmed_bandit/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d77c8a/the_no_regrets_waiting_model_a_multiarmed_bandit/</guid>
      <pubDate>Mon, 03 Jun 2024 15:34:10 GMT</pubDate>
    </item>
    <item>
      <title>旧国家与新国家的“形态”重要吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d73g32/does_the_shape_of_old_state_vs_new_state_matter/</link>
      <description><![CDATA[我是强化学习的新手，如果这个论坛不适合这种级别的新手问题，我提前道歉。 在 DQN 网络的标准拟合中，当前状态和选定的操作会导致新状态和奖励，据我所知：新旧状态是否彼此“相似”并与操作相关会影响学习吗？换句话说，状态的表示本质上只是节点索引（因此它们可以是任何随机的唯一二进制数），还是网络通过以某种方式将状态相互比较来学习？请温柔一点 :-)    提交人    /u/suggestive_cumulus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d73g32/does_the_shape_of_old_state_vs_new_state_matter/</guid>
      <pubDate>Mon, 03 Jun 2024 12:41:01 GMT</pubDate>
    </item>
    <item>
      <title>如何设计一款适合双人棋盘游戏的优秀 DeepQ？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6ysnr/how_to_design_a_good_deepq_for_2_player_board_game/</link>
      <description><![CDATA[我正在制作一款双人棋盘游戏。我需要训练 deepQ AI。AI 使用 e-greedy 策略。 赢了奖励 +1，输了奖励 -1。否则为 0。 游戏规则很简单。它是一个 7x7 的棋盘。 有 4 种不同的瓷砖状态。{ USER_tile, AGENT_tile, EMPTY_tile, CLOSED_tile }。用户瓷砖和代理瓷砖是游戏玩家的当前位置。玩家从相对边缘的中间瓷砖（pos（0,3）和pos（6,3））开始。 每回合玩家都应该朝 8 个方向之一移动到 EMPTY_tile，然后关闭一个 EMPTY_tile。游戏的目标是通过关闭对手周围的 8 个瓷砖使对手无法移动。这样下一回合对手将无法移动，因为周围没有空的瓷砖。 玩家无法移动到封闭的瓷砖中。 我正在使用自我游戏训练代理，但训练后 AI 无法与人类对战。 如何使用 DeepQ 让 AI 与人类对战？    提交人    /u/erenpal01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6ysnr/how_to_design_a_good_deepq_for_2_player_board_game/</guid>
      <pubDate>Mon, 03 Jun 2024 07:31:15 GMT</pubDate>
    </item>
    <item>
      <title>Google AI 提出 PERL：一种参数高效的强化学习技术，可以训练奖励模型并使用 LoRA 调整语言模型策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6tt7s/google_ai_proposes_perl_a_parameter_efficient/</link>
      <description><![CDATA[Google 推出了一种革命性的方法，称为参数高效强化学习 (PERL)，该方法使用 LoRA 技术更有效地优化模型，从而降低计算和内存要求。PERL 实现的结果与传统 RLHF 方法相似，但参数效率显著提高。 来源：https://app.daily.dev/posts/google-ai-proposes-perl-a-parameter-efficient-reinforcement-learning-technique-that-c​​an-train-a-rew-frsirw4h0    提交人    /u/Fit_Stop7509   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6tt7s/google_ai_proposes_perl_a_parameter_efficient/</guid>
      <pubDate>Mon, 03 Jun 2024 02:19:40 GMT</pubDate>
    </item>
    <item>
      <title>“LAMP：用于预训练强化学习的语言奖励调节”，Adeniji 等人 2023 年（提示 LLM 作为多样化奖励）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6tfid/lamp_language_reward_modulation_for_pretraining/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6tfid/lamp_language_reward_modulation_for_pretraining/</guid>
      <pubDate>Mon, 03 Jun 2024 01:59:23 GMT</pubDate>
    </item>
    <item>
      <title>“人工智能欺骗：示例、风险和潜在解决方案的调查”，Park 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6sale/ai_deception_a_survey_of_examples_risks_and/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6sale/ai_deception_a_survey_of_examples_risks_and/</guid>
      <pubDate>Mon, 03 Jun 2024 00:59:21 GMT</pubDate>
    </item>
    <item>
      <title>“利用深度强化学习实现冠军级无人机竞赛”，Kaufmann 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6qkbc/championlevel_drone_racing_using_deep/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6qkbc/championlevel_drone_racing_using_deep/</guid>
      <pubDate>Sun, 02 Jun 2024 23:30:53 GMT</pubDate>
    </item>
    <item>
      <title>“Hoodwinked：语言模型文本游戏中的欺骗与合作”，O'Gara 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6p825/hoodwinked_deception_and_cooperation_in_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6p825/hoodwinked_deception_and_cooperation_in_a/</guid>
      <pubDate>Sun, 02 Jun 2024 22:27:27 GMT</pubDate>
    </item>
    <item>
      <title>RL 理论与实际应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6kecs/rl_theory_practical_usage/</link>
      <description><![CDATA[我是一名刚开始学习强化学习和机器学习的本科生。上学期我参与了一个强化学习理论的研究项目，这是我第一次接触强化学习。我想知道强化学习理论（复杂性结果等）和强化学习中的实际方法之间的关系。理论似乎落后了很大差距。例如，Q 学习是几十年前发明的，但最佳遗憾结果仅在几年前才被证明。 我想知道强化学习理论的价值。理论工作是否指导人们设计更好的实用算法？来自理论世界的洞察力如何帮助推进强化学习？    提交人    /u/mziycfh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6kecs/rl_theory_practical_usage/</guid>
      <pubDate>Sun, 02 Jun 2024 18:53:44 GMT</pubDate>
    </item>
    </channel>
</rss>