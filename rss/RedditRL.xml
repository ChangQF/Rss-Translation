<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚çš„ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä½³åœ°è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 15 Apr 2024 18:17:00 GMT</lastBuildDate>
    <item>
      <title>â€œDRPOï¼šRLHF çš„æ•°æ®é›†é‡ç½®ç­–ç•¥ä¼˜åŒ–â€ï¼ŒChang ç­‰äºº 2024ï¼ˆç¦»çº¿ RLï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c4ozd9/drpo_dataset_reset_policy_optimization_for_rlhf/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c4ozd9/drpo_dataset_reset_policy_optimization_for_rlhf/</guid>
      <pubDate>Mon, 15 Apr 2024 15:25:37 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦ä¸€äº›æ¢ç´¢æƒ³æ³•è¯·ğŸ™</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c4fe7n/need_some_exploration_idea_please/</link>
      <description><![CDATA[åƒè¿™æ ·æ€è€ƒä¸€ä¸ªéå¸¸ç®€å•çš„é—®é¢˜ã€‚æ‚¨å¤„äºä¸€ç»´ç©ºé—´ä¸­ï¼Œæ‚¨çš„æ±½è½¦æ­£åœ¨è¿™æ¡çº¿ä¸Šç§»åŠ¨ã€‚æœ‰ä¸€ä¸ªç›®çš„åœ°ç‚¹ï¼Œæ‚¨éœ€è¦åœ¨è¯¥ç‚¹åœè½¦ã€‚æ‚¨åªèƒ½æ¿€æ´»ä¸€æ¬¡ä¼‘æ¯ï¼Œä¸€æ—¦æ¿€æ´»ä¼‘æ¯ï¼Œå°±æ— æ³•å†æ¬¡å°†å…¶åœç”¨ã€‚å½“æ‚¨æ¿€æ´»åˆ¹è½¦æ—¶ï¼Œæ‚¨çš„æ±½è½¦åªèƒ½å¤šç§»åŠ¨ 10 æ­¥ã€‚  çŠ¶æ€ç©ºé—´-&gt;æ±½è½¦çš„ä½ç½®ï¼š[0, 1, 2, â€¦, 250] åŠ¨ä½œç©ºé—´ -&gt;æ¿€æ´»æˆ–ä¸æ¿€æ´»ä¸­æ–­ï¼š[1, 0]  å¥–åŠ±-&gt; -|DestinationPoint - FinishedPoint| ç¤ºä¾‹ï¼šæ±½è½¦å§‹ç»ˆä» 0 å¼€å§‹å¹¶ä»¥ç›¸åŒçš„é€Ÿåº¦ç§»åŠ¨ã€‚å‡è®¾æˆ‘çš„ç›®çš„åœ°æ˜¯ 140ï¼Œæ‰€ä»¥æˆ‘éœ€è¦åœ¨ 130 å¤„æ¿€æ´»ä¼‘æ¯ã€‚å¦‚æœæˆ‘åœ¨ 100 å¤„æ¿€æ´»ä¼‘æ¯ï¼Œæˆ‘ä¼šåœ¨ 110 å¤„å®Œæˆæ¯”èµ›å¹¶ç»“æŸã€‚æˆ‘å¾—åˆ°çš„å¥–åŠ±æ˜¯ -30ã€‚ çœ‹èµ·æ¥è¿™ä¸ªé—®é¢˜å¾ˆç®€å•ï¼Œä½†æ˜¯ä½œä¸ºæ¢ç´¢çš„ epsilon è´ªå©ªåœ¨è¿™é‡Œå¤±è´¥äº†ã€‚ä¸ºä»€ä¹ˆï¼Ÿæˆ‘å¯ä»¥ä»¥ 50% çš„æ¦‚ç‡é‡‡å– 0 æˆ– 1 çš„éšæœºæ“ä½œã€‚è¿™æ„å‘³ç€ä»£ç†å¾ˆå¯èƒ½ä¼šåœ¨ 10 ä¸ªæ­¥éª¤ä¸­é€‰æ‹©æ“ä½œ 1ã€‚ ï¼ˆç›´åˆ°ç¬¬10æ­¥æ‰é‡‡å–è¡ŒåŠ¨1çš„ç™¾åˆ†æ¯”æ˜¯ï¼ˆ1/2ï¼‰10ï¼‰ è®©æˆ‘ä»¬çœ‹çœ‹ç›´åˆ°130æ‰å¯åŠ¨åˆ¹è½¦çš„æ¦‚ç‡ã€‚-&gt; (1/2)130 è¿™æ˜¯éå¸¸ä½çš„ã€‚è¿™æ„å‘³ç€å½“æˆ‘ä½¿ç”¨ epsilon è´ªå©ªæ—¶ï¼Œä»£ç†å°†æ— æ³•æœ‰æ•ˆåœ°æ¢ç´¢ï¼Œå¹¶ä¸”å®ƒå°†æ— æ³•è¿›ä¸€æ­¥æ¢ç´¢ï¼‰ æˆ‘ç ”ç©¶äº†å…¶ä»–æŠ€æœ¯ï¼Œä¾‹å¦‚åŸºäºè®¡æ•°çš„æ¢ç´¢ã€é¢„æµ‹-åŸºäºæ¢ç´¢ï¼Œä½†æˆ‘ä¸ç¡®å®šå®ƒä»¬æ˜¯å¦æœ‰æ•ˆã€‚ æˆ‘çš„é—®é¢˜å¾ˆç®€å•ï¼Œå› ä¸ºå®ƒåœ¨çŠ¶æ€ç©ºé—´åªæœ‰ 1 ä¸ªç‰¹å¾ï¼Œä½†å½“æˆ‘åœ¨çŠ¶æ€ç©ºé—´æœ‰ 100 ä¸ªç‰¹å¾æ—¶ï¼Œä¼šæƒ³åˆ°åŒæ ·çš„é—®é¢˜ã€‚è›®åŠ›æ•ˆç‡ä¸é«˜ã€‚ æˆ‘æƒ³äº†è§£ä¸€ä¸‹ä½ å¯¹è¿™ç±»é—®é¢˜çš„æ¢ç´¢æ€è·¯ã€‚ ï¼ˆä½ ä¸èƒ½æ”¶å›ä½ çš„è¡ŒåŠ¨ï¼Œä½ éœ€è¦ç­‰å¾…é‡‡å–è¡ŒåŠ¨çš„æœ€ä½³æ—¶æœºï¼Œå¹¶ä¸”ä½ æœ‰ä¸€æ¬¡é‡‡å–è¡ŒåŠ¨çš„æœºä¼šï¼‰   ç”±   æäº¤/u/OpenToAdvices96   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c4fe7n/need_some_exploration_idea_please/</guid>
      <pubDate>Mon, 15 Apr 2024 06:31:51 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•è¡¡é‡å›ºå®šç­–ç•¥çš„å­¦ä¹ ä»·å€¼å‡½æ•°çš„å‡†ç¡®æ€§ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c4erop/how_to_measure_accuracy_of_learned_value_function/</link>
      <description><![CDATA[ä½ å¥½ï¼Œ å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç»™å®šçš„ç­–ç•¥ï¼Œå…¶ä»·å€¼å‡½æ•°éœ€è¦è¯„ä¼°ã€‚è·å–ä»·å€¼å‡½æ•°çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨é¢„æœŸçš„ SARSAï¼Œå¦‚æ­¤å †æ ˆäº¤æ¢ç­”æ¡ˆä¸­æ‰€ç¤ºã€‚ç„¶è€Œï¼Œæˆ‘çš„ MDP çš„çŠ¶æ€ç©ºé—´å¾ˆå¤§ï¼Œå› æ­¤æˆ‘ä½¿ç”¨ DQN çš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œæˆ‘ç§°ä¹‹ä¸ºæ·±åº¦é¢„æœŸ SARSAã€‚ DQN çš„å”¯ä¸€å˜åŒ–æ˜¯ç›®æ ‡ç­–ç•¥ä»â€œè´ªå©ª wrtâ€æ›´æ”¹ä¸ºâ€œè´ªå©ª wrtâ€ã€‚ç°åœ¨ï¼Œåœ¨ä½¿ç”¨æ·±åº¦é¢„æœŸ SARSA è®­ç»ƒä»·å€¼å‡½æ•°æ—¶ï¼Œæˆ‘çœ‹åˆ°çš„æŸå¤±æ›²çº¿å¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºä¸‹é™çš„è¶‹åŠ¿ã€‚æˆ‘è¿˜åœ¨ç½‘ä¸Šè¯»åˆ°ï¼ŒDQN æŸå¤±æ›²çº¿ä¸å¿…æ˜¾ç¤ºä¸‹é™è¶‹åŠ¿ï¼Œå¯ä»¥å¢åŠ ï¼Œè¿™æ²¡å…³ç³»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæŸå¤±æ›²çº¿ä¸ä¸€å®šä¼šå‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œé‚£ä¹ˆå¦‚ä½•è¡¡é‡å­¦ä¹ å€¼å‡½æ•°çš„å‡†ç¡®æ€§ï¼Ÿæˆ‘å”¯ä¸€çš„æƒ³æ³•æ˜¯å°†ï¼ˆsï¼Œaï¼‰å¤„å­¦ä¹ å€¼å‡½æ•°çš„è¾“å‡ºä¸ä»ï¼ˆsï¼Œaï¼‰å¼€å§‹å¹¶éµå¾ªç»™å®šç­–ç•¥çš„è®¸å¤šéƒ¨ç½²çš„å¹³å‡å›æŠ¥ä¼°è®¡çš„é¢„æœŸå›æŠ¥è¿›è¡Œæ¯”è¾ƒã€‚ æˆ‘å·²ç»æ­¤æ—¶æœ‰ä¸¤ä¸ªé—®é¢˜  æ˜¯å¦æœ‰æ¯”æ·±åº¦é¢„æœŸ SARSA æ›´å¥½çš„æ–¹æ³•æ¥å­¦ä¹ ä»·å€¼å‡½æ•°ï¼Ÿåœ¨æ–‡çŒ®ä¸­æ‰¾ä¸åˆ°ä»»ä½•è¿™æ ·åšçš„å†…å®¹ã€‚ æœ‰æ›´å¥½çš„æ–¹æ³•æ¥è¡¡é‡å­¦ä¹ ä»·å€¼å‡½æ•°çš„å‡†ç¡®æ€§å—ï¼Ÿ  éå¸¸æ„Ÿè°¢æ‚¨çš„å®è´µæ—¶é—´!   ç”±   æäº¤/u/Longjumping_March368   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c4erop/how_to_measure_accuracy_of_learned_value_function/</guid>
      <pubDate>Mon, 15 Apr 2024 05:51:04 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero PUCT å…¬å¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c46igr/alphazero_puct_formula/</link>
      <description><![CDATA[æˆ‘äº†è§£ UCBï¼ˆæ­¦è£…å¼ºç›—ï¼‰å’Œ UCT (MCTS) é€‰æ‹©ä¸­ç½®ä¿¡åŒºé—´çš„ç»Ÿè®¡ç‰¹æ€§ã€‚ AlphaZero å’Œ MuZero ä¸­ä½¿ç”¨çš„ PUCT å…¬å¼èƒŒåæ˜¯å¦æœ‰ä»»ä½•å±æ€§ï¼Ÿè¿™çœ‹èµ·æ¥å¾ˆéšæ„ï¼Œè€Œä¸”ä»–ä»¬æ²¡æœ‰åœ¨è®ºæ–‡ä¸­è§£é‡Šå®ƒã€‚   ç”±   æäº¤ /u/_Hardric   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c46igr/alphazero_puct_formula/</guid>
      <pubDate>Sun, 14 Apr 2024 22:43:06 GMT</pubDate>
    </item>
    <item>
      <title>æœ€åä¸€å¹´çš„é¡¹ç›®æƒ³æ³•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3yf8y/final_year_project_ideas/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨æ”»è¯»æ•°æ®ç§‘å­¦å­¦å£«å­¦ä½ï¼Œæœ€åä¸€å¹´å³å°†åˆ°æ¥ã€‚æˆ‘ä»¬å¿…é¡»åœ¨ 2-3 åæˆå‘˜ç»„æˆçš„å°ç»„ä¸­è¿›è¡Œä¸€ä¸ªå…·æœ‰å‰ç«¯çš„ç ”ç©¶å’Œ/æˆ–è¡Œä¸šèŒƒå›´é¡¹ç›®ã€‚æˆ‘ä»ç„¶å¯¹è¯¥é¡¹ç›®çš„èŒƒå›´æ„Ÿåˆ°å›°æƒ‘ï¼ˆä¸€ä¸ªå­¦å£«å­¦ç”Ÿå®é™…ä¸Šåº”è¯¥èµ°å¤šè¿œï¼‰ï¼Œä½†æˆ‘çŸ¥é“ä¸€ä¸ªâ€œå¥½çš„â€äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼ï¼ï¼ï¼‰é¡¹ç›®é€šå¸¸ä½äºåŒ»å­¦é¢†åŸŸä¸è®¡ç®—æœºè§†è§‰ä¸€èµ·ï¼Œæˆ–è€…ä¸æ³•å­¦ç¡•å£«ä¸€èµ·åˆ›å»ºè¯­éŸ³åˆ°æ–‡æœ¬çš„èŠå¤©æœºå™¨äººã€‚ è¿™é‡Œæœ‰ä¸€äº›æˆ‘å·²ç»åšè¿‡çš„é¡¹ç›®ï¼ˆæ— å‰ç«¯ï¼‰ï¼Œåªæ˜¯ä¸ºäº†è¡¨æ˜æˆ‘çš„ç›®æ ‡æ˜¯åšæ¯”è¿™äº›æ›´å¤§çš„äº‹æƒ…å¯¹äºæˆ‘çš„æœ€ç»ˆé¡¹ç›®ï¼š  ä¸åŒæŸ“è‰²çš„æ˜¾å¾®ç»†èƒå›¾åƒä¸­çš„æœ‰ä¸åˆ†è£‚æ£€æµ‹ ä½¿ç”¨ç½‘ç»œæŠ“å–ï¼ˆselenium + bs4ï¼‰çš„è‰ºæœ¯é£æ ¼æ£€æµ‹å™¨ å¹´é¾„ä½¿ç”¨è‡ªå®šä¹‰ CNN è¿›è¡Œæ€§åˆ«/ç­‰è¯†åˆ« ä½¿ç”¨ VGG16/19 è¿›è¡Œå†…çª¥é•œåˆ†ç±» å¤šè¯­è¨€æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æ æ—¶é—´åºåˆ—åˆ†æ è‚¡ç¥¨å¸‚åœºé¢„æµ‹ åŸºäº RNN çš„å®éªŒå®¤ä»»åŠ¡  æˆ‘çš„ç›®æ ‡æ˜¯é€šè¿‡å‡ºè‰²çš„é¡¹ç›®è·å¾—è‰¯å¥½çš„ç¡•å£«å­¦ä½ã€‚æˆ‘å¯¹æ³•å­¦ç¡•å£«å’Œå¼ºåŒ–å­¦ä¹ å¾ˆå¥½å¥‡ï¼Œä½†å¸Œæœ›æä¾›æ›´å…·ä½“çš„å¸®åŠ©ï¼   ç”±   æäº¤/u/kafkaskewers  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3yf8y/final_year_project_ideas/</guid>
      <pubDate>Sun, 14 Apr 2024 17:06:15 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨ä¸åŒæ—¶é—´å°ºåº¦åšå‡ºå¤šä¸ªå†³ç­–çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3tgwk/rl_algorithm_for_making_multiple_decisions_at/</link>
      <description><![CDATA[æ˜¯å¦æœ‰ç‰¹å®šçš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¯ä»¥åœ¨ä¸åŒçš„æ—¶é—´å°ºåº¦ï¼ˆæ¥è‡ªå¤šä¸ªåŠ¨ä½œç©ºé—´ï¼‰åšå‡ºå¤šä¸ªå†³ç­–ï¼Ÿä¾‹å¦‚ï¼Œå‡è®¾æ¸¸æˆä¸­æœ‰ä¸¤ç§ç±»å‹çš„å†³ç­–ï¼Œæ¯nï¼1æ­¥åšå‡ºæˆ˜ç•¥å†³ç­–ï¼Œè€Œæ¯ä¸€æ­¥åšå‡ºæ“ä½œå†³ç­–ã€‚ RLç®—æ³•å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ    ç”±   æäº¤/u/Intelligent_Bee_114   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3tgwk/rl_algorithm_for_making_multiple_decisions_at/</guid>
      <pubDate>Sun, 14 Apr 2024 13:22:11 GMT</pubDate>
    </item>
    <item>
      <title>NEAT-python ä¸ Kivy åœ¨å®Œæˆæ¯ä¸€ä»£çš„è®­ç»ƒåç”Ÿæˆé»˜è®¤è®­ç»ƒæŠ¥å‘Šï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3p12z/neatpython_with_kivy_for_generating_default/</link>
      <description><![CDATA[å„ä½å…ˆç”Ÿï¼Œ ç”±äº Kivy æ˜¯å•çº¿ç¨‹ä¸”äº‹ä»¶é©±åŠ¨çš„ï¼Œæˆ‘ä¿®æ”¹äº† population.run(run_simulation, 2) åˆ°ä¸‹é¢çš„ä»£ç ã€‚è¯¥ç¨‹åºä¼¼ä¹è¿è¡Œè‰¯å¥½ã€‚ä½†run_simulationè‡ªåŠ¨è¿”å›åæ— æ³•æ‰“å°é»˜è®¤çš„è®­ç»ƒæŠ¥å‘Šã€‚æ£€æŸ¥äº†æ–‡æ¡£å’Œäº’è”ç½‘ï¼Œä»ç„¶æ‰¾ä¸åˆ°åˆé€‚çš„è§£å†³æ–¹æ³•æ¥ç”Ÿæˆé»˜è®¤çš„åŸ¹è®­æŠ¥å‘Šã€‚æœ‰ä»€ä¹ˆæ¨èå—ï¼Ÿ  å¦‚æœ self.simu_count &lt; 2: # è¿è¡Œäººå£è§„æ¨¡ä¸º 1 çš„æ¨¡æ‹Ÿ self.population.run (self.run_simulation, 1) æå‰è‡´è°¢ï¼Œ Jefio   ç”±   æäº¤ /u/jefiochen   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3p12z/neatpython_with_kivy_for_generating_default/</guid>
      <pubDate>Sun, 14 Apr 2024 08:51:06 GMT</pubDate>
    </item>
    <item>
      <title>å‡Œä¹±çš„å›¾è¡¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c3ersj/messy_graph/</link>
      <description><![CDATA[       æˆ‘æ­£åœ¨ä¸ºæˆ‘çš„ä»£ç†åœ¨æ¯ä¸ªæ—¶é—´æ­¥ç»˜åˆ¶ç´¯ç§¯å¥–åŠ±ã€‚è¯¥å›¾è¡¨ç»“æœä¸å¥½ã€‚  æœ‰åŠæ³•è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ â€‹ å¥–åŠ±æ–‡ä»¶  å›¾è¡¨   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c3ersj/messy_graph/</guid>
      <pubDate>Sat, 13 Apr 2024 23:01:56 GMT</pubDate>
    </item>
    <item>
      <title>JK æ˜¾ç„¶ RL æ¯”è›®åŠ›æ›´æœ‰æ•ˆâ€¦â€¦æˆ–è€…çœŸçš„æ˜¯è¿™æ ·å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c39u00/jk_obviously_rl_is_way_more_efficient_than_brute/</link>
      <description><![CDATA[   /u/tottombemon  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c39u00/jk_obviously_rl_is_way_more_efficient_than_brute/</guid>
      <pubDate>Sat, 13 Apr 2024 19:22:47 GMT</pubDate>
    </item>
    <item>
      <title>è‡ªæ‰“è¾“çƒè®©ppoâ€œææƒ§â€</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2ym5s/losing_in_self_play_makes_ppo_fearfull/</link>
      <description><![CDATA[æˆ‘å·²ç»æˆåŠŸåœ°è®© ppo ä»£ç†åœ¨å¯¹æ‰‹éšæœºèµ°æ£‹æ—¶ç©ç›¸å½“å¤æ‚çš„æ¸¸æˆã€‚æ¸¸æˆæ˜¯é›¶å’Œæ¸¸æˆï¼Œä½†å®ƒè¶³å¤Ÿå¤æ‚ï¼Œæˆ‘éœ€è¦åœ¨çœŸå®åˆ†æ•°ä¹‹ä¸Šæä¾›ä¸€äº›å¯å‘å¼æ–¹æ³•ï¼Œä»¥ä¾¿ä»£ç†èƒ½å¤Ÿå¼„æ¸…æ¥šä»–ä»¬éœ€è¦åšä»€ä¹ˆæ‰èƒ½è·èƒœã€‚ ç°åœ¨æˆ‘å·²ç»å¯ç”¨äº†é€šè¿‡æ¯éš” X æ­¥ä¿å­˜ç½‘ç»œçŠ¶æ€æ¥è¿›è¡Œè‡ªæˆ‘å¯¹å¼ˆï¼Œæœ‰æ—¶è®©ä»£ç†ä¸é‚£äº›æ—§ç‰ˆæœ¬çš„ç½‘ç»œè¿›è¡Œå¯¹æˆ˜ã€‚  æ‰€å‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œç½‘é˜Ÿä¼šå¾ˆå¥½åœ°å­¦ä¹ ï¼Œç›´åˆ°å¼„æ¸…æ¥šå¦‚ä½•è·èƒœï¼Œè¿™ä¼šä¸ºå‚ä¸çš„å‚ä¸è€…æä¾›å¤§é‡å¥–åŠ±/è´Ÿå¥–åŠ±ï¼Œä»¥è¡¨æ˜èµ¢å¾—æ¯”èµ›æ¯”ç®€å•åœ°è·Ÿéšä»·å€¼è¾ƒä½çš„æ¯”èµ›æ›´é‡è¦å¯å‘å¼ã€‚  ç„¶åä¼šå‘ç”Ÿçš„æƒ…å†µæ˜¯ï¼Œå½“å…¶ä¸­ä¸€ä¸ªç©å®¶è·èƒœæ—¶ï¼Œå¦ä¸€ä¸ªç©å®¶ä¼šå˜å¾—ä¿å®ˆå¹¶æ”¾å¼ƒå…¶ç§¯ç´¯çš„ç§¯åˆ†ä»¥ç¡®ä¿å¯¹æ–¹ä¸ä¼šè·èƒœï¼Œå¹¶ä¸”å®ƒä¼šå¼€å§‹å­¦ä¹ æ›´æ…¢ï¼Œè¿™æ˜¯é¢„æœŸçš„ã€‚å‡ºä¹æ„æ–™çš„æ˜¯ï¼Œæ¯æ¬¡è¿™ä¸ªè¿‡ç¨‹å‘ç”Ÿæ—¶ï¼Œå¤±è´¥çš„ç½‘ç»œéƒ½ä¼šå˜å¾—è¶Šæ¥è¶Šä¿å®ˆï¼Œç›´åˆ°å®ƒä¸å†å°è¯•å¢åŠ è‡ªå·±çš„å¹³å‡å¥–åŠ±ï¼Œä½†å®ƒåªä¼šè¾¾åˆ°å¯¹æ‰‹ä¸ä¼šè·èƒœçš„å·²çŸ¥ç‚¹ã€‚  æˆ‘å°è¯•åœ¨èµ¢å¾—æ¯”èµ›æ—¶å‡å°‘å¥–é‡‘å¥–åŠ±ï¼Œä½†è¿™æ²¡æœ‰å¸®åŠ©ã€‚  æ¥ä¸‹æ¥æˆ‘è¦å°è¯•çš„ * æ˜¯å¥–åŠ±æ ‡å‡†åŒ– * åªç»™èƒœåˆ©è€…å¥–åŠ±ï¼Œè€Œä¸ç»™å¤±è´¥è€…ï¼Œ * ä»¥æé«˜æ¢ç´¢ç‡ã€‚ * æœ‰æ—¶è®©ä»£ç†ä¸éšæœºä»£ç†å¯¹æˆ˜ã€‚  æœ‰ä»€ä¹ˆæŠ€å·§å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ   ç”±   æäº¤/u/drblallo   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2ym5s/losing_in_self_play_makes_ppo_fearfull/</guid>
      <pubDate>Sat, 13 Apr 2024 10:15:45 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦ä¸€äº›å…³äº JAX ä¸­ Actor-Critic çš„å®ç°çš„å¸®åŠ©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2t8ym/need_some_help_regarding_the_implementation_of/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2t8ym/need_some_help_regarding_the_implementation_of/</guid>
      <pubDate>Sat, 13 Apr 2024 04:18:30 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºA2Cç®—æ³•çš„ä¸¤ç§ä¸åŒå®ç°æ–¹å¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c2avvb/about_two_different_ways_for_implementing_a2c/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c2avvb/about_two_different_ways_for_implementing_a2c/</guid>
      <pubDate>Fri, 12 Apr 2024 14:41:56 GMT</pubDate>
    </item>
    <item>
      <title>æ¨¡å‹è¡Œä¸ºçš„éšæœºæ€§</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1xekr/randomness_in_model_behavior/</link>
      <description><![CDATA[     &lt; td&gt; æˆ‘æœ‰ä¸€ä¸ªè‡ªå®šä¹‰å¤šä»£ç†Boid&lt; /a&gt; ä½¿ç”¨æ¥è‡ª SB3 çš„ PPO è¿›è¡Œæ¤ç»’ç¯å¢ƒã€‚ ä»£ç  æˆ‘æ­£åœ¨åšä»€ä¹ˆï¼š æˆ‘å·²ç»ç”¨ 100 ä¸‡ä¸ªæ—¶é—´æ­¥è®­ç»ƒäº†ç¯å¢ƒï¼Œæ¯ä¸ª boid ä»ä¸åŒ/éšæœºçš„åˆå§‹ä½ç½®å¼€å§‹ï¼ˆæ–‡ä»¶ï¼‰ã€‚å°½ç®¡åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ä½ç½®ä¿æŒä¸å˜ã€‚ â€‹ ç»ˆç«¯æ¡ä»¶ï¼š 1) ä»»ä½• Boid ç¢°æ’ï¼Œä»£ç†è·ç¦»å°äºå®‰å…¨åŠå¾„ 2) ä»»ä½• Boid è¿œç¦»ç¾¤ä½“ï¼Œä»£ç†è·ç¦»å¤§äºé‚»åŸŸåŠå¾„ã€‚ â€‹ &lt; strong&gt;å¥–åŠ±å‡½æ•°ï¼š defå¥–åŠ±(self, agent, neighbor_velocities, neighbor_positions): multiplier=1 Total_reward=0 outofflock=False if(len(neighbor_positions) &gt; 0) ï¼šå¯¹äºneighbor_positionsä¸­çš„neighbor_positionï¼šè·ç¦»= np.linalg.normï¼ˆagent.position - neighbour_positionï¼‰å¦‚æœè·ç¦»&lt; SimulationVariables[â€œSafetyRadiusâ€]: # è¾ƒå¤§çš„æƒ©ç½šï¼Œä»¥é˜»æ­¢ä»£ç†è¿‡äºæ¥è¿‘total_reward -= 10 multiplier = 1 elif SimulationVariables[â€œSafetyRadiusâ€] &lt;è·ç¦»&lt; SimulationVariables[â€œNeighborhoodRadiusâ€]: multiplier=10 # å¥–åŠ±çš„è¡°å‡æŒ‡æ•°å‡½æ•° alpha = 0.1 # æ ¹æ®éœ€è¦è°ƒæ•´æ­¤å‚æ•°total_reward += np.exp(-alpha * distance) if (len(neighbor_velocities) &gt; 0):average_velocity = np.mean(neighbor_velocities, axis=0)desired_orientation =average_velocity -agent.velocityorientation_diff = np.arctan2(desired_orientation[1],desired_orientation[0])-np.arctan2(agent.velocity[1],agent.velocity[ 0]) å¦‚æœorientation_diff &gt; np.piï¼šorientation_diff -= 2 * np.pi eliforientation_diff &lt; 0:orientation_diff += 2 * np.pialignment = 1 - np.abs(orientation_diff) if (alignment &lt; 0.5):total_reward -= 50 * (alignment) else:total_reward += 50 * (alignment) else: # Ifæ²¡æœ‰é‚»å±…ï¼Œé¼“åŠ±æ™ºèƒ½ä½“å¯»æ‰¾ç¾Šç¾¤total_reward -=50 outofflock=True è¿”å›total_rewardï¼Œoutofflock  æµ‹è¯•è¾“å‡ºï¼š â€‹ ; å¥–åŠ± 10 ä¸ªä¸åŒçš„å‰§é›† -  é—®é¢˜ï¼š åœ¨æˆ‘ç¬¬ä¸€æ¬¡å°è¯•æµ‹è¯•æ—¶ï¼Œå¥–åŠ±åœ¨ 600000 èŒƒå›´å†…çš„ boids æ²¡æœ‰æ­£ç¡®ç§»åŠ¨ï¼Œè€Œ 1 Mil èŒƒå›´å†…çš„å¥–åŠ±åˆ™æ­£ç¡®ã€‚ç°åœ¨å®ƒä»¬éƒ½å¯ä»¥æ­£ç¡®ç§»åŠ¨ï¼Œä½†å¥–åŠ±å€¼ä¸åŒï¼Œå³åœ¨ 600000 èŒƒå›´å†…ä»¥åŠ 1 Mil èŒƒå›´å†…ã€‚æˆ‘ä¸çŸ¥é“æ¨¡å‹æ˜¯å¦æ­£ç¡®å­¦ä¹ ï¼Œä¹Ÿä¸çŸ¥é“ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§å·®å¼‚ã€‚ â€‹ è¾“å‡ºï¼ˆå½“å‰è¡Œä¸ºï¼‰ï¼š â€‹ Boid è¿åŠ¨æœ‰ 4 ä¸ªä¸åŒçš„åˆå§‹ä½ç½®ï¼ˆæ¯ä¸ªæŒç»­æ—¶é—´10 ç§’ï¼‰ â€‹ æˆ‘çš„æ”¹è¿›æƒ³æ³•ï¼š æ›´å¤šç”¨äºè®­ç»ƒçš„ä½ç½®æ–‡ä»¶é˜²æ­¢è¿‡åº¦æ‹Ÿåˆã€‚ â€‹ æ¬¢è¿ä»»ä½•å»ºè®®ã€‚    [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1xekr/randomness_in_model_behavior/</guid>
      <pubDate>Fri, 12 Apr 2024 02:08:02 GMT</pubDate>
    </item>
    <item>
      <title>æœ‰æ²¡æœ‰æ²¡æœ‰ä¼˜åŠ¿å‡½æ•°çš„æ¼”å‘˜æ‰¹è¯„ç®—æ³•ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1nqmt/is_there_any_actor_critic_algorithm_without_the/</link>
      <description><![CDATA[ä¼˜åŠ¿å‡½æ•°çš„ä½¿ç”¨å¯ä»¥åœ¨æ¼”å‘˜è¯„è®ºå®¶ç®—æ³•ä¸­çœ‹åˆ°ï¼Œå…¶ä¸­ä½¿ç”¨ TD(0) æ–¹æ³•æ¥è¿‘ä¼¼ä¼˜åŠ¿ï¼Œä½†æ˜¯æ˜¯å¦å­˜åœ¨ä»»ä½•æ–¹å¼éƒ½å¯ä»¥åˆ›é€ ä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒæ˜¯çœŸå®çš„æ–¹ç¨‹ A(s,a) = Q(s,a) - V(s)ï¼Œå…¶ä¸­ V(s) å¯ä»¥å®šä¹‰ä¸º \sum\pi(a|s)Q(s ,a)ï¼ˆps åªéœ€éµå¾ªæ–¹ç¨‹ï¼‰å¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨æ­¤è¿‡ç¨‹æ”¶æ•›ï¼æˆ‘ä»¬æ˜¯å¦ä¹Ÿå¯ä»¥åœ¨ä¸ä½¿ç”¨ä¼˜åŠ¿å‡½æ•°çš„æƒ…å†µä¸‹åˆ›å»ºä¸€ä¸ªæ¼”å‘˜æ‰¹è¯„ç®—æ³•ï¼Ÿ   ç”±   æäº¤ /u/Professional_Pound63   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1nqmt/is_there_any_actor_critic_algorithm_without_the/</guid>
      <pubDate>Thu, 11 Apr 2024 19:24:22 GMT</pubDate>
    </item>
    <item>
      <title>Anterion â€“ å¼€æºäººå·¥æ™ºèƒ½è½¯ä»¶å·¥ç¨‹å¸ˆï¼ˆSWE-agent å’Œ OpenDevinï¼‰- RL Baseline</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c1gyif/anterion_opensource_ai_software_engineer_sweagent/</link>
      <description><![CDATA[       ç”±   æäº¤ /u/Ok-Alps-7918   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c1gyif/anterion_opensource_ai_software_engineer_sweagent/</guid>
      <pubDate>Thu, 11 Apr 2024 14:48:16 GMT</pubDate>
    </item>
    </channel>
</rss>