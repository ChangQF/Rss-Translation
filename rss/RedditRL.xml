<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 12 Sep 2024 01:11:36 GMT</lastBuildDate>
    <item>
      <title>关于 RPG 沙盒的 BOSS 战技巧（最好使用 Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fec8bs/tips_on_a_boss_fight_rpg_sandbox_preferrably_with/</link>
      <description><![CDATA[大家好， 我正在为我的计算机工程学位做毕业论文，我将使用 RL 探索 AI 在类似灵魂的 Boss 上的应用，所以我真的只需要一个有 2 个实体的区域，1 个是 Boss，1 个是玩家。 你们认为最简单的方法是什么？这意味着我的大部分注意力将集中在 AI 模型上。 我需要的几乎是一个带有两个实体的沙盒，问题是它需要是 3D 的，我在 youtube 上寻找教程但找不到，而且我不知道哪个库或框架更适合我的问题。 任何建议都会有很大的帮助。 谢谢！    提交人    /u/lordgvp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fec8bs/tips_on_a_boss_fight_rpg_sandbox_preferrably_with/</guid>
      <pubDate>Wed, 11 Sep 2024 14:59:03 GMT</pubDate>
    </item>
    <item>
      <title>模拟推荐系统环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1feamwu/stimulating_a_recommder_system_environment/</link>
      <description><![CDATA[嗨，我是 python 开发人员。有人知道像 RecSim 这样的软件包可以模拟推荐系统来测试 RL 模型吗？RecSim 似乎不受支持且可能无法使用（不知道 tensorflow 1.13.1 是否还能使用）。 非常感谢！    提交人    /u/pot8o118   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1feamwu/stimulating_a_recommder_system_environment/</guid>
      <pubDate>Wed, 11 Sep 2024 13:51:14 GMT</pubDate>
    </item>
    <item>
      <title>Tetris Gymnasium：可定制的俄罗斯方块强化学习环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fea6bs/tetris_gymnasium_a_customizable_reinforcement/</link>
      <description><![CDATA[今天，Tetris Gymnasium 的第一个版本发布了，对于从事强化学习相关工作或想要涉足该领域的人来说，这可能很有趣。 这是什么？Tetris Gymnasium 是 Tetris 作为强化学习环境的简洁实现，并与 Gymnasium 集成。它可以自定义（例如棋盘尺寸、重力等），并包含许多有关如何使用它的示例，如训练脚本。 为什么是俄罗斯方块？尽管许多 Atari 游戏在强化学习方面取得了重大进展，但俄罗斯方块仍然是 AI 面临的一个挑战。它结合了 NP 难复杂性、随机元素和长期规划需求，使其成为强化学习研究中一个持续存在的未解问题。到目前为止，还没有出版物可以在不使用手工制作的特征向量或其他简化的情况下很好地与游戏配合使用。 我能用它做什么？请不要犹豫，尝试一下该环境以进入强化学习。好处是俄罗斯方块很容易理解，你可以观看代理玩并清楚地看到它所犯的错误。如果您已经进入 RL，您可以将其用作可自定义的环境，可以很好地与其他框架（如 Gymnasium 和 W&amp;B）集成。 GitHub：https://github.com/Max-We/Tetris-Gymnasium 在存储库中，您还可以找到我们的短文“逐个步骤：为俄罗斯方块组装模块化强化学习环境”的预印本其中更详细地解释了背景、实施情况以及学生和研究人员的机会。 如果您尝试该环境，欢迎您留下星星或打开一个问题！    提交人    /u/Npoes   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fea6bs/tetris_gymnasium_a_customizable_reinforcement/</guid>
      <pubDate>Wed, 11 Sep 2024 13:30:43 GMT</pubDate>
    </item>
    <item>
      <title>我想将强化学习应用到机械臂上。寻求建议！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fe73oa/i_want_to_apply_reinforcement_learning_to_a/</link>
      <description><![CDATA[您好， 我是一名正在学习强化学习的大学生。我正尝试首次将强化学习应用于机械手，感觉有点不知所措，因此，如果您能提供任何建议，我将不胜感激。  模拟器推荐：我不确定哪种模拟器最适合将强化学习应用于机械手。我听说过 PyBullet、MuJoCo、Gazebo 和其他几种模拟器。哪种模拟器使用最广泛，推荐度最高？ 论文推荐：如果您知道任何关于将强化学习应用于机械手的重要论文或评论文章，我将非常感激您的推荐。尤其是作为初学者，我想知道我应该从哪些论文开始。 推荐的学习资源：如果有任何网站或资源提供该领域的组织良好的学习材料，我将不胜感激。或者，如果您有将 RL 应用于操纵器的建议课程或学习路径，那将非常有帮助。  任何针对初学者的资源或建议都将不胜感激！提前谢谢大家。    提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fe73oa/i_want_to_apply_reinforcement_learning_to_a/</guid>
      <pubDate>Wed, 11 Sep 2024 10:46:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的自学研究方法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdz4jl/selflearning_research_methods_for_rl/</link>
      <description><![CDATA[你好！ 我想攻读 RL 博士学位。但是，我的潜在导师回复说我没有研究经验。 我在 PhD SubReddit 上发帖询问如何自学研究。我注意到似乎有更适合 CS 的研究方法。 我想我可以向这里的集体智慧寻求更具体的建议。我在哪里可以开始自学如何为 RL 空间进行研究和学术写作？ 有人知道好的资源吗？ 天方夜谭：如果这里有博士生导师并且愿意直接留言，我很想知道向你这样的人推销申请的最佳知识和经验基础是什么。 附注：我住在澳大利亚。   由    /u/jeroku  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdz4jl/selflearning_research_methods_for_rl/</guid>
      <pubDate>Wed, 11 Sep 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>学习一个价值函数，然后通过最小化相应的 Q 函数来学习一个策略，最后使用该策略来热启动最优控制求解器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdybbo/learning_a_value_function_then_learning_a_policy/</link>
      <description><![CDATA[任何在优化和最优控制、强化学习编程方面有专业知识的人可以自由联系我 [tesfayedmu@gmail.com](mailto:tesfayedmu@gmail.com)    提交人    /u/AsleepCreme5489   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdybbo/learning_a_value_function_then_learning_a_policy/</guid>
      <pubDate>Wed, 11 Sep 2024 01:19:49 GMT</pubDate>
    </item>
    <item>
      <title>没有任何经验并且只有一个强化学习项目的学生可以在 RL 领域找到工作吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdxgry/can_students_with_no_experience_and_one_project/</link>
      <description><![CDATA[没有发表过论文但有一个 RL 大项目的硕士生能在 RL 领域找到工作吗？    提交人    /u/optimum_point   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdxgry/can_students_with_no_experience_and_one_project/</guid>
      <pubDate>Wed, 11 Sep 2024 00:35:55 GMT</pubDate>
    </item>
    <item>
      <title>分享我的副项目 raice - 现实生活中的特工在 F1 赛道上的赛车比赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdtpn8/sharing_my_side_project_raice_a_racing/</link>
      <description><![CDATA[      嗨！  让我向您展示 https://github.com/Fer14/raice，这是使用不同算法训练的 RL 代理之间的赛车比赛。 不确定如何发布此内容，但自从学习 RL 以来，我认为让所有这些不同的算法以某种方式相互竞争会很有趣。因此，我有了这个想法，我从 YouTube 视频中获取了一个简洁的代理在自定义赛道上训练并实施更多算法（可能还会有更多算法）以查看谁在 F1 赛道上表现最佳。我不是 F1 的狂热爱好者，但我认为添加一条真正的赛道并举办一整场 F1 比赛会很有趣，所以这就是我现在正在做的事情，我认为分享会很有趣。 我不指望它能完美运行，一旦一切完成，我想做一些调整，但现在我认为它很酷！    提交人    /u/Fer14x   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdtpn8/sharing_my_side_project_raice_a_racing/</guid>
      <pubDate>Tue, 10 Sep 2024 21:40:20 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 无法学习简单的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdr8zu/ddpg_fails_to_learn_simple_environment/</link>
      <description><![CDATA[我在 https://github.com/JijaProGamer/Car-Racer-AI &gt; src/GPU/model.js 上有一个非常简单的 DDPG 代码 并且大部分代码都是从 https://keras.io/examples/rl/ddpg_pendulum/ 复制而来，只是用 TFJS（以及扩展的 JS）重写。 这里有一些内容：我使用与 keras 示例相同的超参数，我相信我得到了噪声类工作，并且没有错误，只是一些静默问题。 DQN 也有效（对于离散动作空间），所以我知道这不是环境或内存问题，而是 DDPG 特定代码的问题。 我不确定哪里出了问题，因为一切都看起来像 keras 中的 99%，并且演员损失不断上升，而批评家不断降低（趋于负值）。    提交人    /u/ZazaGaza213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdr8zu/ddpg_fails_to_learn_simple_environment/</guid>
      <pubDate>Tue, 10 Sep 2024 19:56:54 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习中的泛化和安全性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdgne3/generalization_and_safety_in_deep_reinforcement/</link>
      <description><![CDATA[https://blogs.ucl.ac.uk/steapp/2023/12/20/adversarial-attacks-robustness-and-generalization-in-deep-reinforcement-learning/    提交人    /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdgne3/generalization_and_safety_in_deep_reinforcement/</guid>
      <pubDate>Tue, 10 Sep 2024 12:29:13 GMT</pubDate>
    </item>
    <item>
      <title>如何在不违反马尔可夫特性的情况下处理强化学习中的延迟奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd9s9i/how_to_handle_delayed_rewards_in_rl_without/</link>
      <description><![CDATA[大家好，我正在研究一个强化学习问题，其中代理控制交通信号以最小化队列长度和碰撞风险。奖励函数有两个组成部分：  即时奖励：每个时间步骤通过交叉路口的车辆数量。 延迟奖励：只有在完成一个完整的信号周期（4 个阶段）后才能计算的碰撞风险分数。计算出这个碰撞风险分数后，我需要将其分配到前面的步骤中。  奖励=−(队列长度+碰撞风险) 挑战如下：  在每个步骤（操作：延长当前阶段或更改阶段）中，我可以根据通过的车辆数量立即计算奖励（例如，步骤 1：队列长度 = 4，步骤 2：队列长度 = 6，等等）。 但是，碰撞风险评分会延迟，并在整个信号周期之后计算。然后，我想将这个碰撞风险奖励分配到周期的前面几个步骤中（例如，步骤 1 获得一部分碰撞风险）。  示例：  步骤 1：队列长度 = 4，尚无碰撞风险 步骤 2：队列长度 = 6，尚无碰撞风险 步骤 3：队列长度 = 2，尚无碰撞风险 步骤 4：队列长度 = 5，碰撞风险 = 4（仅在此步骤之后已知） 信号周期结束后，我将碰撞风险分数向后分配到前面的步骤中（例如，步骤 1 奖励 = -(4+1)，步骤 2 奖励 = -(6+1)，等等）  问题：  我能否均匀地在不违反马尔可夫特性的情况下将碰撞风险向后分布到各个步骤中（因为奖励通常仅根据当前状态和动作来计算）？ 如果不是，如何在 RL 中正确处理这种延迟奖励，同时保留马尔可夫特性？是否有任何替代技术可以帮助，例如部分可观察的 MDP、N 步 TD 或分层 RL？     提交人    /u/muttahirulislam   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd9s9i/how_to_handle_delayed_rewards_in_rl_without/</guid>
      <pubDate>Tue, 10 Sep 2024 04:41:30 GMT</pubDate>
    </item>
    <item>
      <title>最佳强化学习和人工智能代理资源？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd9r00/best_reinforcement_learning_and_ai_agents/</link>
      <description><![CDATA[我在本科期间曾学习过机器学习和深度学习（监督学习）。 今年我已经毕业了，现在我对 RL 和 AI 代理产生了兴趣。我可以从哪些资源（最好是最新的）中学习？我还希望能够在学习后建立项目，因此如果资源还包含实践知识，那就最好了    提交人    /u/CS_UGRAD24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd9r00/best_reinforcement_learning_and_ai_agents/</guid>
      <pubDate>Tue, 10 Sep 2024 04:39:19 GMT</pubDate>
    </item>
    <item>
      <title>“Carpentopod：一个行走桌项目”（进化出更平滑的滚动腿）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd2h89/carpentopod_a_walking_table_project_evolving/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd2h89/carpentopod_a_walking_table_project_evolving/</guid>
      <pubDate>Mon, 09 Sep 2024 22:29:11 GMT</pubDate>
    </item>
    <item>
      <title>呼吁采取行动：对问题做出反应以帮助提供资金</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</link>
      <description><![CDATA[嗨，RL 的朋友们， 我和我的同事们一直在开发 Tianshou 库，并且即将将其变成真正造福社区的东西。这是我所知道的唯一一个具有广泛范围（各种算法、离线 RL、marl 等，最先进的性能和快速吞吐量）的库，旨在为研究人员和应用程序开发人员提供帮助。 现在我们公司发生了一些变化，导致新经理是一位非技术人员，不熟悉战略。如果我们不能以他们理解的方式展示社区的兴趣，该项目的资金可能会被取消。 我已经写了一个非常详细的计划，将 Tianshou 带到下一个主要版本，我相信结果对整个 RL 社区非常有用。我非常感谢您的支持，您只需对此问题留下点赞或评论即可。 当然，我也很高兴在那里就该计划和图书馆本身进行任何建设性的讨论！    提交人    /u/Left-Orange2267   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</guid>
      <pubDate>Mon, 09 Sep 2024 20:50:01 GMT</pubDate>
    </item>
    <item>
      <title>你们在哪里使用强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</link>
      <description><![CDATA[嗨，朋友们！ 我正在研究 RL，我想知道哪些公司正在应用 RL 来解决业务问题。当我搜索这个主题时，我只找到旧案例和来自大型科​​技公司的案例。 你们在学术界使用 RL 吗？你们在初创公司使用 RL 吗？只是想知道你们如何使用它并试图了解市场。 谢谢！    提交人    /u/embedding_turtle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</guid>
      <pubDate>Mon, 09 Sep 2024 20:48:33 GMT</pubDate>
    </item>
    </channel>
</rss>