<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 31 Jan 2025 15:15:53 GMT</lastBuildDate>
    <item>
      <title>“RL + Transformer = 通用问题解决器”，Rentschler & Roberts 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ieefn3/rl_transformer_a_generalpurpose_problem_solver/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ieefn3/rl_transformer_a_generalpurpose_problem_solver/</guid>
      <pubDate>Fri, 31 Jan 2025 13:10:50 GMT</pubDate>
    </item>
    <item>
      <title>搞砸了 DQN 编码面试。感觉很尴尬！！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ie7glt/messed_up_dqn_coding_interview_feel_embarrassing/</link>
      <description><![CDATA[我被一位 RL 科学家面试过。我很好地回答了所有理论问题，但是我搞砸了 DQN 的损失函数编码。我愣住了，写不出来。一个字也写不出来。所以我只写了关于代码逻辑的注释。我有 5 分钟的时间来写，但只有 4 行。做不到。面试结束后，我花了 10 分钟才写出来。我把代码发给他们，但我认为他们不会接受。我觉得我不会被选中进入下一轮。    提交人    /u/Remote_Marzipan_749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ie7glt/messed_up_dqn_coding_interview_feel_embarrassing/</guid>
      <pubDate>Fri, 31 Jan 2025 05:08:26 GMT</pubDate>
    </item>
    <item>
      <title>RL 的发展方向是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ie2l1a/where_is_rl_headed/</link>
      <description><![CDATA[大家好，我是一名在 RL 领域工作的博士生。尽管我从事这一领域的工作，但我对它的发展方向没有很强的认识，特别是在现实世界应用的可用性方面。除了 RL 的 Deepseek/GPT 用途（有些人认为这实际上不是 RL）之外，我经常感到沮丧，因为这个领域没有发展方向，我花在摆弄挑剔算法上的所有时间都浪费了。 我想听听你的想法。您预见到未来几年 RL 的趋势是什么？您预见到 RL 在不久的将来会在哪些行业应用领域有用？    提交人    /u/Sudden-Eagle-9302   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ie2l1a/where_is_rl_headed/</guid>
      <pubDate>Fri, 31 Jan 2025 00:51:15 GMT</pubDate>
    </item>
    <item>
      <title>土匪研究新手</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idq2go/newbie_in_bandit_research/</link>
      <description><![CDATA[我最近开始研究老虎机问题，并注意到许多相关文献都是高度理论化的，通常侧重于证明遗憾界限和类似的结果。我一直在阅读 Tor Lattimore 和 Csaba Szepesvári 的老虎机算法，除了测度理论方面，我可以轻松地理解大约 90% 的内容，并且对数学充满信心。此外，我还阅读了许多与我的具体问题相关的论文，并且可以理解数学论证。 然而，我的主要挑战是理解解决这些问题的整体方法。在老虎机研究中，目标通常是推导出遗憾界限，但我很难对新问题或算法进行分析。这似乎没有一个明确的标准框架，论文中的许多技术感觉像是凭空而来的。例如，一些论文引入了看似不相关的引理，然后以一种不太明显的方式将它们与主要分析联系起来。 如果有人能分享他们研究理论 RL/bandit 的经验或见解，我将不胜感激！    提交人    /u/Careless-Breath2913   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idq2go/newbie_in_bandit_research/</guid>
      <pubDate>Thu, 30 Jan 2025 15:54:58 GMT</pubDate>
    </item>
    <item>
      <title>极快的优先采样</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idl178/blazingly_fast_prioritized_sampling/</link>
      <description><![CDATA[  由    /u/JacksOngoingPresence  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idl178/blazingly_fast_prioritized_sampling/</guid>
      <pubDate>Thu, 30 Jan 2025 11:36:35 GMT</pubDate>
    </item>
    <item>
      <title>有关测量误差的论文？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idkp6u/papers_on_measurement_error/</link>
      <description><![CDATA[你们知道关于测量误差和模型在决策过程中导致不准确估计的任何书面文章吗？    提交人    /u/mowmail   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idkp6u/papers_on_measurement_error/</guid>
      <pubDate>Thu, 30 Jan 2025 11:13:44 GMT</pubDate>
    </item>
    <item>
      <title>对多智能体环境的质疑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idk7cn/doubt_regarding_multi_agent_environments/</link>
      <description><![CDATA[大家好！我有使用 DRL 的经验，但环境只有一个代理。现在我正在研究 GitHub 上 CybORG repo 的多代理“Scenario1b”，并尝试使用 Stable-Baselines3 训练一些代理。我已经用 PettingZoo 制作了一个包装器，我有几个疑问： 1- 在那种环境中，通常有一个很大的动作空间，但实际上可以执行的动作较少（即可以执行的动作已被过滤并从那些不能执行的动作中筛选出来）。这通常被称为“动作掩蔽”。我的疑问是，这是否可以包含在步骤方法本身中，还是必须像本例 (https://pettingzoo.farama.org/tutorials/sb3/connect_four/) 一样单独实现？ 2- 据说 SB3 不支持“dict”动作空间，但是在这个使用 SB3 的多代理环境的示例 (https://pettingzoo.farama.org/tutorials/sb3/kaz/) 中，它确实有一个 Dict 动作空间。这怎么理解呢？ 提前谢谢！！    提交人    /u/Carpoforo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idk7cn/doubt_regarding_multi_agent_environments/</guid>
      <pubDate>Thu, 30 Jan 2025 10:38:13 GMT</pubDate>
    </item>
    <item>
      <title>特斯拉会使用强化学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idjngw/is_tesla_going_to_use_reinforcement_learning/</link>
      <description><![CDATA[强化学习在现实世界中还没有真正应用，特斯拉有可能使用强化学习吗？    提交人    /u/thamizhan1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idjngw/is_tesla_going_to_use_reinforcement_learning/</guid>
      <pubDate>Thu, 30 Jan 2025 09:56:55 GMT</pubDate>
    </item>
    <item>
      <title>与我们都在做的 RL 相比，为什么 LLM 上的 RL 微调如此容易和稳定？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</link>
      <description><![CDATA[我一直在观察各种人尝试重现 Deepseek 训练配方，与我习惯的 RL 相比，我对这种配方的稳定性感到震惊。 经过大约 50 个训练步骤后，他们在数学问题上的准确率可靠地达到了 50%。他们尝试了几种不同的 RL 算法，并报告说它们的效果大致相同，而无需任何超参数调整。 如果我能在仅 50 个训练步骤中将平衡手推车的成功率提高 50%，我会认为自己很幸运。而且我可能必须为每个任务调整超参数。  （我的理论：由于无监督的预训练，这很容易。该模型已经学习了良好的表示和背景知识 - 即使它无法在 RL 之前完成任务 - 这使得问题变得容易得多。也许我们应该在 RL 中做更多这样的事。）    提交人    /u/currentscurrents   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</guid>
      <pubDate>Wed, 29 Jan 2025 19:31:43 GMT</pubDate>
    </item>
    <item>
      <title>关于连续 Cartpole 的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id1pu3/question_on_continuous_cartpole/</link>
      <description><![CDATA[我修改了 cartpole 环境，让动作空间连续，自然训练时间就长了很多。我用的算法是 A2C，每集更新一次。不知道有没有人用 DDPG 或其他处理连续动作空间的算法建过类似的模型，能加速训练吗？现在解决 cartpole 大概需要 20k 集。    submitted by    /u/Key-Entrance8005   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id1pu3/question_on_continuous_cartpole/</guid>
      <pubDate>Wed, 29 Jan 2025 19:06:29 GMT</pubDate>
    </item>
    <item>
      <title>关于离线强化学习的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icy4si/question_on_offline_rl/</link>
      <description><![CDATA[嗨，我对 RL 还比较陌生，我有一个问题，在离线 RL 中，关键点是我们在各处学习最佳策略。我的问题是，我们是否也在各处学习最佳价值函数和最佳 q 函数？ 具体来说，我想知道如何最好地从离线数据集中仅学习价值函数（不​​一定是策略），并且我想使用离线 RL 工具在各处学习最佳价值函数，但我对要研究什么以了解更多信息感到困惑。我想这样做来学习 V 作为状态的安全指标。 我希望我说得有道理。    提交人    /u/Limp-Ticket7808   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icy4si/question_on_offline_rl/</guid>
      <pubDate>Wed, 29 Jan 2025 16:42:46 GMT</pubDate>
    </item>
    <item>
      <title>我尝试构建一个 alphazero 来掌握井字游戏，但它找不到最佳动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icw7bw/i_tried_to_build_a_alphazero_to_master_tictactoe/</link>
      <description><![CDATA[github: https://github.com/asdeq20062/tictactoe_alphazero.git 这是我的井字游戏 alphazero，但经过这么多次训练，AI 总是在第一个回合移动到中心。最佳移动应该是角落。 有人能帮我检查哪里出了问题吗？谢谢。 main.py -&gt; 此文件是训练的起点     提交人    /u/Upstairs-Lead-2601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icw7bw/i_tried_to_build_a_alphazero_to_master_tictactoe/</guid>
      <pubDate>Wed, 29 Jan 2025 15:22:30 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能随着情节的增加而下降——动作重复和不稳定的奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icsv3j/dqn_performance_drops_with_more_episodes_action/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icsv3j/dqn_performance_drops_with_more_episodes_action/</guid>
      <pubDate>Wed, 29 Jan 2025 12:41:06 GMT</pubDate>
    </item>
    <item>
      <title>谁在奖励 DeepSeek R1？在 RL 中，您需要一些奖励功能或手动奖励，不是吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</link>
      <description><![CDATA[他们说他们没有在大数据上进行自监督学习，那么奖励模型必须以某种方式在某些数据上进行训练，ChatGPT API 或 LLAMA 可以用作奖励工具，或者谁知道在没有奖励基线的情况下思想链是如何工作的？ PS：据我了解，他们根据版本进度使用 LLAMA 作为基线 LLM 模型，并且它是公开可用的    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</guid>
      <pubDate>Wed, 29 Jan 2025 11:13:03 GMT</pubDate>
    </item>
    <item>
      <title>频繁奖励环境中 Q 值的收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icpjwh/problems_with_convergence_of_qvalues_in/</link>
      <description><![CDATA[大家好 :) 我有一个组合优化问题：这是一个设施位置问题，我需要选择一个可能的位置子集，在每个时间步骤中满足客户的需求。我将其建模为一个图形，其中边缘表示客户和设施之间的距离。在我的例子中，设施是服务器，距离是延迟，目标是在客户在白天移动时最小化延迟。 因此，奖励是（负）延迟。但是，此奖励在每个时间步骤都会立即给出。我已经使用 DQN 进行了离线策略训练，但 q 值似乎无限增长。所以，我根本看不到收敛行为。 我最初的猜测是奖励的频率可能是一个问题，因为每个 Q 值都需要表示所有未来奖励的折扣总和，而奖励频率较低时更容易实现（也许这不是真的，我只是有过这样的经历）。  所以我的问题是，您是否有过类似的经历，即决策总是会立即获得奖励反馈，而对于达到某种“获胜”状态或类似情况则没有“长期”奖励？ 在这种情况下，您的方法是什么？ 祝一切顺利，提前谢谢您 :)    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icpjwh/problems_with_convergence_of_qvalues_in/</guid>
      <pubDate>Wed, 29 Jan 2025 08:50:22 GMT</pubDate>
    </item>
    </channel>
</rss>