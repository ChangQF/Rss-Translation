<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Tue, 11 Mar 2025 09:19:31 GMT</lastBuildDate>
    <item>
      <title>Renforceui Studio现在支持DQN和离散的动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j8lndf/reinforceui_studio_now_supports_dqn_discrete/</link>
      <description><![CDATA[       &lt;！&lt;！ -  sc_off-&gt;      Readforceui Studio现在支持DQN＆amp;离散的动作空间！ 🎉  大家好， 正如我在开源 GUI旨在简化RL培训，配置和监视，而没有更多的命令行斗争！最初，我们专注于连续的动作空间，但是你们中的许多人都要求支持 dqn和离散的动作空间算法  - 所以这就是！ 🕹️ ✨有什么新鲜事物？   dqn＆amp;离散的动作空间支持  - 训练和可视化离散的RL模型。 更多的环境兼容性  - 扩展不仅仅是连续的动作环境。   尝试一下！  github： https://github.com/github.com/github.com/dvalenciar/dvalenciar/reinforceui-reinforceui-studio-studio--reinforceui-studio-studio  https://docs.reinforceui-studio.com/welcome     让我知道其他RL算法您可以看到什么！您的反馈有助于塑造增强工作室。 到目前为止，增强式工作室支持以下算法：        algorithM   ctd4  连续的分布参与者 - 批判剂与多个评论家的kalman融合      ddpg  ddpg  align =“ left”&gt; dqn  深q-network      ppo   ofximal politization&gt;近端策略plocization                actor-Critic     td3  双延迟的深层确定性策略梯度        tqc  tqc 评论家     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/dvr_dvr     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecriceslearning/comments/1j8lndf/reinforceui_studio_now_now_supports_dqn_dqn_discrete/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j8lndf/reinforceui_studio_now_supports_dqn_discrete/</guid>
      <pubDate>Tue, 11 Mar 2025 07:56:26 GMT</pubDate>
    </item>
    <item>
      <title>通过增强学习学习俄罗斯方块</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j8ek56/learning_tetris_through_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  刚完成了我的第一个RL项目。那些AI学习如何玩游戏的视频总是看起来很有趣，所以我想试一试。我的github上有一个演示视频。我有GPT帮助在README中组织我的思考过程。如果从事类似项目的工作，也许其他人可以发现有用的东西。我对这个话题非常新鲜，因此欢迎任何反馈。  https：//github.com/truong.com/truonging/truonging/tetris-a.i  提交由＆＃32; /u/truonging     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j8ek56/learning_tetris_through_reinforcement_learning/</guid>
      <pubDate>Tue, 11 Mar 2025 00:52:02 GMT</pubDate>
    </item>
    <item>
      <title>使用GSM8K数据集将GRPO应用于QWEN-0.5B教学，最终将输出低表现的指令模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j8cwzx/applying_grpo_to_qwen05binstruct_using_gsm8k/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对于上下文：上周我刚刚阅读并了解了GRPO。本周，我决定通过在GSM8K数据集中培训QWEN-0.5B教学来应用这种方法。使用TRL的grpotrainer，我设置了2个训练时期和参考模型每25个步骤同步。 I only used two reward functions: strict formatting (i.e., must follow &lt;reasoning&gt;...&lt;/reasoning&gt;&lt;answer&gt;...&lt;/answer&gt; format) and accuracy (i.e., must output the correct answer). However when I tried to ask it a simple question after training phase was done, it wasn&#39;t able to answer it.它只是回答\ n（newline）字符。我检查了奖励功能的图，它们是“稳定的”。在培训结束时1.0。 我错过了什么吗？想听听你的想法。谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j8cwzx/applying_grpo_grpo_to_qwen05binstruct_using_gsm8k/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j8cwzx/applying_grpo_grpo_to_qwen05binstruct_using_gsmusit_gsm8k/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j8cwzx/applying_grpo_to_qwen05binstruct_using_gsm8k/</guid>
      <pubDate>Mon, 10 Mar 2025 23:33:59 GMT</pubDate>
    </item>
    <item>
      <title>使用RL探索电力市场竞标中的NASH平衡 - 寻求反馈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j88v0y/exploring_nash_equilibria_in_electricity_market/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在研究一个研究项目，我们旨在在使用加强学习的电力市场竞标中探索 nash equilibria 。核心问题是：  “在竞争性的电力市场中，代理人自然会像古典经济理论那样竞标其生产成本？ Or does strategic behavior emerge, leading to a different market equilibrium?&quot; Approach  Baseline Model (Perfect Competition &amp; Social Welfare Maximization):  We first model the electricity market using Pyomo, solving an optimization problem where all agents (generators and consumers) bid their  true成本。 这会导致最佳调度，最大化社会福利并用作基准。           rlib）允许代理人学习他们的最佳招标策略。 每个代理商都会提交出价，通过 pyomo 进行竞标，并根据利润分配奖励。平衡没有代理可以单方面改进。     比较＆amp;见解：  我们比较了基于 rl的NASH平衡与完美竞争基准  这使我们能够评估战略性竞标市场是市场操纵或效率           将模型扩展到多处拍卖，代理商会随着时间的流逝学习最佳策略。 探索混合竞争性合件设置，当地社区中的代理商在当地社区中的代理商进行了合作，但与其他社区竞争。竞标。  寻找反馈！  您是否在市场模拟之前多代理RL ？   在这种情况下，在这种情况下进行建模在这种情况下进行建模的建议？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/iminderent-milk5530     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j88v0y/exploring_nash_equilibria_in_electricity_market/</guid>
      <pubDate>Mon, 10 Mar 2025 20:42:57 GMT</pubDate>
    </item>
    <item>
      <title>为什么在SARSA算法的非政策n-Step版本中，重要性采样率不仅会使整个错误乘以整个错误？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j85tzj/why_in_the_offpolicy_nstep_version_of_sarsa/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对我的理解，我们使用重要的采样比&#39;rho;加重在遵循行为政策时观察到的回报。根据目标策略“ pi”观察相同轨迹的概率。然后，如果我们考虑对许多回报的期望以及行为政策给出的概率的期望，我们将获得相同的价值，就好像我们接受了相同的回报的期望，但使用目标策略的概率，直观地，我认为这就像考虑加权收益rho•g一样，就像考虑到目标策略的目标，但在这种情况下，更新规则是Q＆lt; q q＆lt; q＆lt;  - 写为Q＆lt;  -  q + alpha•rho•（g-q）我们如何获得该表格？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/samas69420     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j85tzj/why_in_the_offpolicy_nstep_version_of_sarsa/</guid>
      <pubDate>Mon, 10 Mar 2025 18:36:10 GMT</pubDate>
    </item>
    <item>
      <title>LLM可以学会看吗？微调QWEN 0.5B用于SFT + GRPO的视觉任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j815kg/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/</link>
      <description><![CDATA[     &lt;！ -  sc_off-&gt;  嘿大家！ 我刚刚发布了一个博客，分解了一个博客，分解了数学 群体相对策略优化    grpo，depsseek r1 r1 步骤！   有趣的实验包括： i微调 qwen 2.5 0.5b ，a  folly&gt;语言 -  模型，没有事先视觉训练，使用 sft + sft + grpo    &lt;强&gt; href =“ https://preview.redd.it/ujxzkscccrvne1.png?width = 927＆amp; format = png＆amp; amp; amp; amp; amp; am https://preview.redd.it/ujxzksccrvne1.png?width=927＆amp; format = png＆amp; amp; amp; amp; auto = webppumppumpp；  完整博客&gt; href =“ https://github.com/jacksoncakes/vision-r1”&gt; github    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforeverctionlearning/comments/1j815kg/can_an_an_llm_llm_learn_learn_to_to_see_fine_fine_tuning_qwen_05b_for//”&gt; [link]      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j815kg/can_an_an_llm_llm_llm_learn_to_to_to_fine_fine_tuning_qwen_05b_05b_for/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j815kg/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/</guid>
      <pubDate>Mon, 10 Mar 2025 15:24:11 GMT</pubDate>
    </item>
    <item>
      <title>让SAC在大型并行模拟器上工作（第一部分）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</link>
      <description><![CDATA[&quot;As researchers, we tend to publish only positive results, but I think a lot of valuable insights are lost in our unpublished failures.&quot; This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with并行模拟了数千个机器人。如果您遵循旅程，您将了解可能对性能产生重大影响的任务设计和算法实现的细节。 扰流板警报：链接： https://araffin.github.io/post/post/sac-massive-sim/   提交由＆＃32; /u/u/araffin2     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</guid>
      <pubDate>Mon, 10 Mar 2025 08:27:24 GMT</pubDate>
    </item>
    <item>
      <title>复制DeepSeek-R1 RL所需的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi rl社区，我想继续复制DeepSeek R1的RL训练管道中的小数据集。我对培训语言模型感到满意，但对培训RL代理不满意。我对深度RL的经典RL和中等理论理解有体面的理论理解。  我认为我需要逐步加强困难，以训练推理语言模型。因此，最近，我开始培训PPO实施方法来解决一些更轻松的健身环境，这确实在努力... 1周，我仍然无法再现低保真性，尽管基本上抬起了稳定的 - 贝赛3。&gt; 我想了解我的最终目标是否正确。一方面，如果我不能RL训练简单的代理商，我将如何使用RL训练语言模型。另一方面，我与我的朋友进行了限制RL经验的交谈，他提到，由于RL培训语言模型的代码已经在那里，而且挑战是正确的...    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/u/complect-media-8074      [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j7t5j4/advice_needed_needed_reproducing_deepseekr1_rl/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</guid>
      <pubDate>Mon, 10 Mar 2025 07:25:05 GMT</pubDate>
    </item>
    <item>
      <title>VINTIX：通过文化强化学习的动作模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我们刚刚发布了我们在离线范围内的初步努力（诸如Laskin等人，2022年的算法蒸馏）时，将其放到离线范围内的范围。虽然我们正在从经典的元元素意义上寻求概括，但初步结果令人鼓舞，表现出对参数变化的适度概括，而仅在总共87个任务下接受了87个任务。 我们的关键要点在它上工作：（1）数据策划ICLR的数据curation for Iclr hard tweaking是一定的。希望所述的数据收集方法将有所帮助。而且我们还发布了数据集（约200mln元组）。 （2）即使在不同的数据集下，也可能对适度参数变化的概括。这是令人鼓舞的。但是，即使在类似jat的建筑中，也不是那么可怕（但很近）。   nb：随着我们进一步努力扩展并使状态和动作空间不变 - 也许您有一些有趣的环境/域名/元元学习基准，您希望在即将到来的工作中看到吗？ href =“ https://github.com/dunnolab/vintix”&gt; https://github.com/dunnolab/vintix    如果您传播词：https://x.com/vladkurenkov/status/1898823752995033299   提交由＆＃32; /u/u/vkurenkov     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</guid>
      <pubDate>Sun, 09 Mar 2025 21:03:08 GMT</pubDate>
    </item>
    <item>
      <title>关于多目标增强学习的环境的概括</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</link>
      <description><![CDATA[      现实世界中的顺序决策任务通常涉及平衡相互矛盾的目标之间的权衡并在各种环境中概括。尽管它很重要，但尚未有一项工作在本文中研究多目标环境中的环境概括！ 我们在多目标增强学习（MORL）中正式化概括以及如何评估。我们还介绍了 Morl Generalization 基准测试，具有各种多样性域具有带有参数化的环境配置，以促进该领域的研究。 我们对当前最新的最新莫尔尔算法的基线评估2关键洞察力：  li li a li a li a li a li a li a li a li a li algorith 与单目标增强学习相比，莫尔（Morl）表现出更大的学习适应性行为的潜力。事后看来，这是可以预期的，因为多目标奖励结构更具表现力，并允许学习更多的行为！ 😲  我们坚信，在未来几年中，开发能够跨多种环境和目标概括的代理将成为一个至关重要的研究方向。有许多有希望的途径用于进一步的探索和研究，尤其是在单一目标RL概括研究中的适应技术和见解中，以解决这个更严重的问题设置！我期待着与有兴趣推进这一新的研究领域的任何人交往！ 🔗纸： https://arxiv.org/arxiv.org/abs/2503.00799999999999999       https://github.com/jaydenteoh         &lt;！ -  SC_ON-&gt;＆＃32;不同环境概括      &lt;！提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j7bdr3/on_generalization_carers_environments_in/”&gt; [link]   ＆＃32;   [注释]    ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</guid>
      <pubDate>Sun, 09 Mar 2025 16:31:56 GMT</pubDate>
    </item>
    <item>
      <title>Python和Unity的RL环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我想训练AI使用Python玩游戏，并以Unity（C＃）形象可视化游戏。目前，我需要在Python中创建环境，以学习实际游戏玩法。有没有办法创建我可以在Python和Unity中使用的环境？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tot-chance9372     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</guid>
      <pubDate>Sun, 09 Mar 2025 14:37:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的模型不能学会在连续的网格世界中玩游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好。由于我正在研究深度Q学习算法，因此我正在尝试从头开始实施它。我创建了一个在网格世界中玩的简单游戏，我的目标是开发一个玩此游戏的代理商。在我的游戏中，状态空间是连续的，但是动作空间是离散的。这就是为什么我认为DQN算法应该起作用的原因。我的游戏具有3种不同的角色类型：主角（代理），目标和球。目标是在不与球相撞的情况下到达目标，而球线性移动。我的动作值剩下，右上，向下和什么都不是，总共进行了5个离散操作。 我使用Pygame Rect在Python中编码了游戏，用于目标，角色和球。我奖励代理如下：   +5，与角色相撞  -5，与球相撞，以  +0.7   +0.7，以靠近目标（使用Manhattan距离）   -1远离目标（使用Manhattan距离移动）（使用Manhattan距离远处）。我尝试了不同的状态表示形式，但是在最好的情况下，我的经纪人只学会避免球并达到目标。在大多数情况下，代理根本不会避免球，或者有时它会连续向左和向右进入摇摆的运动，而不是达到目标。&lt; /p&gt; 我给出了状态表示，如下所示：&lt; /p&gt;  agent.rect.rect.lect.lect.lect.lect.lect.rect.rect.rect.rect.rect.right.rect.right， agent.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.lect--rect-rect- egent， target.rect.bottom， agent.rect.bottom--  Agent.Rect.bottom-  ball.Rect.top ， ball_direction_in_x，ball_dircection_in_in_in_in_in_iin_y_y_y  partive（part）这描述了对经纪人的比赛状态，提供了球和目标的相对位置以及球的方向。但是，我的模型的表现令人惊讶地差。相反，我将状态分类如下：  如果目标在左边，则为-1。 如果目标在右边为+1。游戏中很少或没有球），模型的性能大大提高。当我从游戏中删除球时，分类的状态代表学得很好。但是，当出现球时，即使表现形式连续，该模型也非常慢，最终它过度拟合。 我不想屏幕截图游戏屏幕并将其馈入CNN。我想使用密集的层将游戏的信息直接提供给模型，然后学习。为什么我的模型不学习？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j786hr/1j786hr/why_cant_my_my_my_model_model_lealed_to_to_play_in_continuul/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</guid>
      <pubDate>Sun, 09 Mar 2025 13:58:09 GMT</pubDate>
    </item>
    <item>
      <title>机器人技术定制体育馆环境设计。包装纸还是阶级继承？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为水下机器人构建自定义环境。我已经尝试使用一个快速且脏的整体环境，但是如果我尝试修改环境以添加更多传感器，转换输出，重用代码，以完成另一个任务等，我现在会遇到问题。 ，我想重构代码并必须做出一些设计选择：我应该使用一个不适用的spass类并在每个任务中使用训练和训练的范围，或者我要训练劳动范围，或者我要训练劳动范围，或者我要求职。我只有一个基类，并将其他所有内容添加为包装器（包括传感器配置，任务奖励 +逻辑等）？ 如果您知道环境创建的良好资源，这将不胜感激）  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/equiald-diver     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</guid>
      <pubDate>Sun, 09 Mar 2025 09:18:42 GMT</pubDate>
    </item>
    <item>
      <title>Han等人“一般推理需要学习从一开始推理”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</guid>
      <pubDate>Sun, 09 Mar 2025 08:05:53 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>