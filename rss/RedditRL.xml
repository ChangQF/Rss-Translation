<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sun, 16 Feb 2025 06:22:08 GMT</lastBuildDate>
    <item>
      <title>亲社会的内在动机</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iql3h5/prosocial_intrinsic_motivation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我遇到了这篇关于制作优化爱心善良的AI的帖子，我想呼应他们的意图： https://wwwww.reddit.com/r/reinforeveresslearning/s/gmgxfbxw2e 因为这就是我们可以直接对更美好的世界进行优化的方式。如果它不是针对正确的目标，世界上所有的情报都不是好的。我要求此Subreddit上的人在AI上工作，该AI直接针对集体公用事业。我将用于此问题的框架是针对集体实用性问题的协作逆增强学习（CIRL）。试想一下，如果规范是在适用的任何RL部署之上添加亲社会固有驱动器，将会有多大的影响。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neuropyrox     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iql3h5/prosocial_intrinsic_motivation/</guid>
      <pubDate>Sun, 16 Feb 2025 05:22:08 GMT</pubDate>
    </item>
    <item>
      <title>帮助线性函数近似确定性策略梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iqi20i/help_with_linear_function_approximation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在对特定的应用程序区域应用不同的强化学习算法，但是我坚持如何使用确定性策略扩展线性函数近似方法梯度定理。我正在尝试实施COPDAC-GQ（Silver ET提出的算法兼容的非政策确定性的con-Critic-Critic-Critic-Critic-Critic-Critic-Critic）。 Al。，在他们的开创性DPG论文中，但在我看来，尺寸在方程式中没有奏效。特别是，theta重量矢量更新方程。  功能（或状态）的数量为n。动作维度的数量为m。有3个重量向量，theta，w和v。theta是nxm，w和v是nx1。作者说的“按照惯例”是 jacobian 矩阵，因此每列是策略相对于相对于相对于策略的DTH动作维度的梯度∇θ[μθ（s）d策略参数θ。这不是经典的雅各布矩阵，但是我认为如果您删除“雅各布”，则该陈述是正确的。从声明。我已经解释了策略函数∇θμθ（s）的梯度，为nxm矩阵，使每列是MTH动作维度的策略函数的梯度，而部分衍生物则在mth中占据了每个theta重量Theta列。  这是问题所在的地方。在银纸中，它们定义了COPDAC-GQ算法中每个重量向量的更新步骤。除了    theta_next = theta_current + alpha*∇θμθ（s）*（∇θμθ（s）&#39;*w_current）的theta更新方程外，所有尺寸是转置操作员。  我缺少什么？ theta需要是nxm和alpha*∇θμθ（s）*（∇θμθ（s）&#39;*w_current）的作用为nx1。   d。 Silver，G。Lever，N。Heess，T。Degris，D。Wierstra和M. Riedmiller，“确定性政策梯度算法”，在第31届机器学习国际会议上，PMLR，PMLR，PMLR， 2014年1月，第387–395页。访问：2024年11月5日。[在线]。可用： https://proceedings.mlr.press/v32/silver14.html  &gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ecelectrybear45     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iqi20i/help_with_linear_function_approximation/</guid>
      <pubDate>Sun, 16 Feb 2025 02:25:42 GMT</pubDate>
    </item>
    <item>
      <title>RL收敛和OpenAI人形生物环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iq6oji/rl_convergence_and_openai_humanoid_environment/</link>
      <description><![CDATA[       class =“ md”&gt; 嗨， 我在航空航天行业，最近开始学习和实验增强学习。我从Cartpole环境上的DQN开始，在我看来，如果我没错的话，很难融合（不是平均趋势或平滑的总奖励）。但是，无论如何，我试图重新发明轮子并以不同的种子组合进行测试。我的融合目标似乎至少已经实现了。收敛的结果如下所示：  收敛地图  ，以下是测试重量以限制到最大步骤的视频。   https://reddit.com/link/link/link/1iq6oji/video/7s53ncy19cje19cje1/player 加强学习，我想晋升到连续的动作空间。我发现Openai的人形V5学习如何行走。但是，我感到惊讶的是，我找不到任何成功的结果/视频。这太困难了还是环境问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/tasty_road_3519     [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iq6oji/rl_convergence_and_openai_humanoid_environment/</guid>
      <pubDate>Sat, 15 Feb 2025 17:40:50 GMT</pubDate>
    </item>
    <item>
      <title>DQN-动态2D避免</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iq3og6/dqn_dynamic_2d_obstacle_avoidance/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在开发一个RL模型，代理需要避免在2D空间中移动敌人。敌人不断产生并反弹墙。环境似乎很动态和混乱。  nn Input  有5个功能定义每个敌人的输入：  与代理商的距离 速度 相对于代理的角度 相对x位置 相对y位置   此外，最终输入包括代理的X和Y位置。&lt; /p&gt; 因此，对于给定数量的10个敌人，总输入大小为52（10 * 5 + 2）。 10敌人对应于距代理商的10个最接近的敌人，那些可能造成需要避免的碰撞的敌人。 关注点 是我的正确方法来定义状态？ 当前，我根据距代理的上升距离对这些功能进行排序。我的理由是，更亲密的敌人对生存更为至关重要。这里的伽玛？固有的动态和混乱的环境是否倾向于减少它？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/pm4tt_     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iq3og6/dqn_dynamic_2d_obstacle_avoidance/</guid>
      <pubDate>Sat, 15 Feb 2025 15:25:29 GMT</pubDate>
    </item>
    <item>
      <title>可解释的RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iq37rp/explainable_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用RL基于 simglucose 。我想在使用Shap或Policy Explantion测试的算法中添加解释性。我一直在阅读该领域的当前研究论文，但是我可以从任何特定的角度开始吗？我可以尝试实施基本的东西，以了解最新论文中使用的大数学。我想知道我们甚至如何确切地使RL可以解释，寻找哪些功能等。  ps：我是ECE的最后一年。我读过Barto和Sutton，观看了David Silver的UCL讲座，阅读了一本关于RL的数学理解的书。考虑到解释性，我知道Shap是如何工作的，并且我是Christoph Molnar的可解释的机器学习书（很好）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/uickulweeber99     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iq37rp/explainable_rl/</guid>
      <pubDate>Sat, 15 Feb 2025 15:03:46 GMT</pubDate>
    </item>
    <item>
      <title>关于项目主题，我应该为我的强化课程选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iq12d4/regarding_project_topic_should_i_choose_for_my/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我的教授给了我们一个截止日期，直到周一选择一个项目主题，该项目可以基于研究或基于应用程序。作为该领域的新手，我想提出一些建议，最好是基于研究的主题。我真的很感谢任何支持。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/u/glum_inflation_421      [link]  ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iq12d4/regarding_project_topic_should_i_choose_for_my/</guid>
      <pubDate>Sat, 15 Feb 2025 13:13:31 GMT</pubDate>
    </item>
    <item>
      <title>“重新评估不完美信息游戏的策略梯度方法”，Rudolph等。 2025年（PPO竞争不完美的INFO游戏的定制算法）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzsqe/reevaluating_policy_gradient_methods_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  摘要：“在过去的十年中，受到对抗性不完美信息游戏中幼稚的自我扮演深度强化学习（DRL）的假定失败的动机，研究人员已经根据虚拟游戏（FP），双重甲骨文（DO）和反事实遗憾最小化（CFR）开发了许多DRL算法。鉴于磁性镜下降算法的最新结果，我们假设PPO（例如PPO）具有更简单的通用策略梯度方法具有竞争力或优于这些FP，DO和基于CFR的DRL方法。为了促进该假设的解决，我们针对四个大型游戏实施并发布了第一个可访问的确切可剥削性计算。使用这些游戏，我们对不完美的信息游戏进行了DRL算法的有史以来最大的可利用性比较。超过5600次培训运行，FP，DO和基于CFR的方法无法超越通用策略梯度方法。”   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mothmatic     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzsqe/reevaluating_policy_gradient_methods_for/</guid>
      <pubDate>Sat, 15 Feb 2025 11:53:25 GMT</pubDate>
    </item>
    <item>
      <title>UnrealMlagents 1.0.0：开源深钢筋学习框架！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzl5v/unrealmlagents_100_opensource_deep_reinforcement/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/cybereng     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzl5v/unrealmlagents_100_opensource_deep_reinforcement/</guid>
      <pubDate>Sat, 15 Feb 2025 11:39:00 GMT</pubDate>
    </item>
    <item>
      <title>多目标PPO指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzdjn/guidance_on_multiobjective_ppo/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试在动态环境中实现用于PPO（作为新手）的多目标算法（作为新手）。这里有两个主要奖励指标，我可以根据环境的当前状态成功计算出来：1）预期碰撞时间和2）当前速度和所需速度之间的差异（朝着目标方向的速度朝着目标的方向汽车的最大速度）。大多数研究论文的线性函数作为奖励功能，在该功能中，系数是手工调整的。到目前为止，我已经理解的是（遇到困难和混乱）是，我们不会立即标明奖励，而是我们计算每个奖励目标的政策，然后最终将其汇总。无论出于何种原因，我都找不到特定于多目标PPO的研究论文。你有建议吗？您甚至认为这是正确的方法吗？感谢您的时间（请帮助我，我迷路了）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/med-grade-8440     [link]  ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzdjn/guidance_on_multiobjective_ppo/</guid>
      <pubDate>Sat, 15 Feb 2025 11:23:20 GMT</pubDate>
    </item>
    <item>
      <title>关于我的HRL接下来我应该尝试什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipyfqv/suggestion_on_what_should_i_try_next_for_my_hrl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图通过使用称为rware的预先固定程序在网格世界中实现仓库任务分配。我正在HRL（继承人的增强学习）中使用封建网络。如果将架子带入世界上的目标障碍，则奖励Rware的提供仅为+1。奖励稀疏还是可以拥有这样的奖励系统？我只是有一个代理。我不能让代理商去做。屁股hrl很好。我该怎么做才能实现学习？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/decter_prune_9756       [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipyfqv/suggestion_on_what_should_i_try_next_for_my_hrl/</guid>
      <pubDate>Sat, 15 Feb 2025 10:13:33 GMT</pubDate>
    </item>
    <item>
      <title>[r]在增强学习方面的标签经验，以进行有效检索。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipxgd1/r_labelling_experiences_in_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好 r/reaceforceleconlearning ， p&gt; 我正在研究加强学习问题，因为我是一名创业者，所以我没有时间写论文，所以我想我应该在这里分享。 因此，我们目前在经验重播中使用随机样本。为1K样品提供缓冲区，然后获取随机物品。有人在“好奇的重播”上撰写了一篇论文，这使该模型为重播分配了“好奇心得分”，并更频繁地获取它们。和使用世界模型的火车，这实际上是SOTA可以重播的SOTA，但是我认为我们可以更深入地进行。 好奇心重播是不错的，但是这样考虑：当您（代理人）越过时，街，您重播了关于越过街道的回忆。人类不考虑烹饪或机器学习，当他们过马路时，我们会想到越过街道，，因为这很危险。 所以我们如何标记体验有类似VAE的编码器结构，它可以为缓冲区中的项目分配“标签空间”概率？然后，使用相同的体验编码器，编码当前状态（或世界模型）（编码为上述标签空间），并将其与所有缓冲体验进行比较。在任何匹配的地方，都会更有可能显示这种缓冲体验。 比较可以通过深网或简单的日志损失（二进制跨透明拷贝）进行比较。我认为，这种修改在SOTA世界模型中特别有用，在SOTA世界模型中，我们需要预测50个步骤，并且拥有更多相关的输入数据将是100％有用的 在最坏的情况下，我们会牺牲一点性能和随机样品，充其量，我们获得了非常扎实的经验重播。  watchu认为伙计们？ 我想到了这一点，因为我正在工作解决最难的RL AGI之后的问题，我需要这种边缘才能使我的模型更具性能。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/stzed32     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipxgd1/r_labelling_experiences_in_reinforcement_learning/</guid>
      <pubDate>Sat, 15 Feb 2025 08:57:51 GMT</pubDate>
    </item>
    <item>
      <title>寻找具有语言定义目标的培训RL代理的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipiuwo/looking_for_work_on_training_rl_agents_with/</link>
      <description><![CDATA[在自然语言。具体来说，我正在寻找探索的工作：  使用语言作为灵活的奖励信号  培训政策以为条件文本中的描述   通过LLMS    层次结构rl与语言引导的子搜索   我很想阅读任何论文，存储库或博客文章探索此主题的内容。如果您从事类似的事情，我也很乐意讨论想法或协作！ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃ 32;提交由＆＃32; /u/u/foricas-ad2641     link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipiuwo/looking_for_work_on_training_rl_agents_with/</guid>
      <pubDate>Fri, 14 Feb 2025 19:33:38 GMT</pubDate>
    </item>
    <item>
      <title>需要RL学习伙伴</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipa8fy/need_study_partner_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在担任2.5 Yoe的数据科学家，主要是在经典ML和NLP上工作，但想探索RL，因为我可能有使用情况我的工作是从YT上观看David Silver演讲开始的，但是它的数学太重了（目前在第二LEC上），如果我能够完成PR，我会失去信心彼此之间明确的怀疑。请随意dm me !!   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/shirish0500     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipa8fy/need_study_partner_for_rl/</guid>
      <pubDate>Fri, 14 Feb 2025 13:10:53 GMT</pubDate>
    </item>
    <item>
      <title>实验室可以在欧洲的RL上获得博士学位</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip9lur/labs_to_do_a_phd_in_rl_in_europe/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在寻找2026年的博士带有RL等的型号。我不是在研究纯MDP或土匪之类的东西。我想要更适用的东西，例如可塑性研究，终身学习，甚至更好的RL架构，或多代理或分层RL，RL + LLM，RL +扩散等。更多的ML喜欢更好的变压器体系结构，状态空间模型等。我在EPFL，ETH和DARMSTADT上看到了一些实验室。但是真的很感激。 ＆＃32;提交由＆＃32; /u/u/no_carpenter7252      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip9lur/labs_to_do_a_phd_in_rl_in_europe/</guid>
      <pubDate>Fri, 14 Feb 2025 12:35:32 GMT</pubDate>
    </item>
    <item>
      <title>RL教程的业余爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://github.com /google-deepmind/mujoco/descordions/2404    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/goncalogordo     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</guid>
      <pubDate>Fri, 14 Feb 2025 09:06:19 GMT</pubDate>
    </item>
    </channel>
</rss>