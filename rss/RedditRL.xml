<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 03 Jan 2024 21:12:23 GMT</lastBuildDate>
    <item>
      <title>持续强化学习的定义</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2307.11046 OpenReview：https:// /openreview.net/forum?id=ZZS9WEWYbD 摘要：  在强化学习问题的标准视图中，代理的目标是有效地确定最大化长期回报的策略。然而，这种观点是基于一种有限的观点，即学习是寻找解决方案，而不是将学习视为无休止的适应。相反，持续强化学习是指最好的智能体永远不会停止学习的环境。尽管持续强化学习很重要，但社区缺乏一个简单的问题定义来强调其承诺并使其主要概念准确清晰。为此，本文致力于仔细定义持续强化学习问题。我们将“永不停止学习”的代理概念正式化。通过一种新的数学语言来分析和编目代理。使用这种新语言，我们将持续学习代理定义为可以无限期地执行隐式搜索过程的代理，并将持续强化学习定义为最佳代理都是持续学习代理的设置。我们提供了两个激励性的例子，说明多任务强化学习和持续监督学习的传统观点是我们定义的特例。总的来说，这些定义和观点形式化了学习核心的许多直观概念，并开辟了围绕持续学习代理的新研究途径。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</guid>
      <pubDate>Wed, 03 Jan 2024 21:09:29 GMT</pubDate>
    </item>
    <item>
      <title>暑期实习在 RL/Embodied AI 领域处于领先地位。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xos1z/summer_internship_leads_in_rlembodied_ai_space/</link>
      <description><![CDATA[我正在寻找 2024 年暑期实习的一些线索，以寻找潜在的（如果可能的话研究）实习机会。 我不是非常了解在这个领域工作的所有初创公司/组织，因为我自己对它非常陌生。我没有太多经验，但希望在这些实习中获得一些经验。如果有任何帮助，我们将不胜感激！   由   提交 /u/gchhablani   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xos1z/summer_internship_leads_in_rlembodied_ai_space/</guid>
      <pubDate>Wed, 03 Jan 2024 17:33:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何获得研究想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xlwli/how_do_i_get_ideas_for_research/</link>
      <description><![CDATA[事情的简短版本：我是一名 CS 硕士生。我已经完成了一年半的尝试并努力进入 EAI 领域。我还有大约一年的时间，我想充分利用这一年。我有一些 NLP 专业背景，也有一些研究经验，但不是很深入。我了解监督学习，并且擅长 PyTorch 之类的东西。我目前面临两个主要问题：  我想发表一些第一作者的作品，但无法获得强大的创造力在我体内流淌。我该如何开发它？ 我想从语言理解方面攻击 RL 的各个方面，但各个方面的研究都在疯狂增长（LLM、VLM） 、RL、变形金刚），我很难理解该读什么、不该读什么。  来自该领域有经验的人的任何想法或提示类似问题或只是有足够的经验？ 如果这不是合适的论坛，请指出我可以讨论此问题的地方。 &lt;!-- SC_ON - -&gt;  由   提交 /u/gchhablani   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xlwli/how_do_i_get_ideas_for_research/</guid>
      <pubDate>Wed, 03 Jan 2024 15:24:28 GMT</pubDate>
    </item>
    <item>
      <title>RL玩游戏：从哪里开始？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xk6a8/rl_to_play_games_where_to_start/</link>
      <description><![CDATA[我正在启动一个有趣的项目，训练 AI 玩 2 种类型的游戏： - 1v1 游戏联盟/Dota 风格，基于图像的输入  - 1v1 回合制游戏，玩家在单元格上移动并拥有一堆咒语，具有基于特征的输入 所以我读过 Atari、AlphaGo/Star、OpenAI Five...想知道人们是否有其他/较新的参考资料或相关项目，在这些情况下最好使用哪种类型的算法？过去我只在所有事情上使用 PPO。 我还计划自己编写游戏代码（在非常基础的水平上） - 想知道人们是否已经这样做了并且对使用什么语言/框架有建议以最大速度写入游戏。我最近看到了 Madrona Engine，看起来很有趣，但我还没有尝试过。 顺便说一句，到目前为止我只是自己做 RL，有没有大型的 RL 社区除了这个 Reddit 子版块之外还应该知道什么？谢谢你！ :)   由   提交/u/frenchhusky  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xk6a8/rl_to_play_games_where_to_start/</guid>
      <pubDate>Wed, 03 Jan 2024 14:05:40 GMT</pubDate>
    </item>
    <item>
      <title>逆向强化学习的现状？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xjndc/current_state_of_inverse_reinforcement_learning/</link>
      <description><![CDATA[ 由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xjndc/current_state_of_inverse_reinforcement_learning/</guid>
      <pubDate>Wed, 03 Jan 2024 13:40:46 GMT</pubDate>
    </item>
    <item>
      <title>用于控制器调优 inn python 的 env</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xgqu8/env_for_controller_tuning_inn_python/</link>
      <description><![CDATA[       这里有人尝试过使用 RL 在 python 中进行 PI/PID 控制器调整 如果可以的话，您可以提供您的 env 文件或帮助我创建相同的环境。&lt; /p&gt; （环境由代理、源、系统模型以及系统输出到源的反馈组成）  /u/Wide-Chef-7011   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xgqu8/env_for_controller_tuning_inn_python/</guid>
      <pubDate>Wed, 03 Jan 2024 10:57:38 GMT</pubDate>
    </item>
    <item>
      <title>衰减剪辑因子和熵损失权重</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xevbt/decaying_clip_factor_and_entropy_loss_weight/</link>
      <description><![CDATA[有没有办法可以将衰减熵损失权重和裁剪因子合并到 matlab 中的 PPO 算法中？ 我知道 Rl matlab 有问题，但如果可能的话仍然存在    由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xevbt/decaying_clip_factor_and_entropy_loss_weight/</guid>
      <pubDate>Wed, 03 Jan 2024 08:51:34 GMT</pubDate>
    </item>
    <item>
      <title>需要github项目的合作者（股票交易的深度强化学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18x0vtz/need_collaborator_for_github_project_deep/</link>
      <description><![CDATA[​ 有人有兴趣合作开发一个使用深度强化学习进行股票交易的 Python 库项目吗？  您可以在此处找到 github 存储库：https://github.com/RezaSoleymanifar/neuralHFT ​ 这是一个正在进行的项目，目前有超过 15,000 行代码处理端到端的所有事务，从连接到交易 API、下载历史数据、数据集创建、DRL算法/网络设计、训练并最终部署在交易账户中。  ​ 我计划在 ICAIF 2024（ACM AI in Finance）会议上发表一篇关于这个库的论文。如果您是学者，这是我们可以讨论的另一个途径。   由   提交 /u/RezaSoleymanifar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18x0vtz/need_collaborator_for_github_project_deep/</guid>
      <pubDate>Tue, 02 Jan 2024 21:27:53 GMT</pubDate>
    </item>
    <item>
      <title>MARL 中的通用全局奖励与个人奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18x0o5k/common_global_reward_vs_individual_reward_in_marl/</link>
      <description><![CDATA[我正在解决一个问题，其中有 2 个智能体在一个域中移动，根据奖励函数的最佳方法是：另一个代理人。因此，它们预计会汇聚成围绕彼此绕圈移动。奖励函数基于各自的代理，完全独立于其他代理。  在情况1中，他们只关心自己的奖励，即代理1的奖励是r1，代理2的奖励是r2。在情况2中，他们得到相同的奖励，这是他们两个奖励的平均值，即r_common = (r1+r2)/2。在情况 2 中，它们最终只会互相绕圈。但是，我预计它们都会收敛到这一点，因为每个智能体最大化自己的个人奖励也应该导致他们互相跟随绕圈。有人能给我任何见解吗？ 编辑以添加更多详细信息   由   提交/u/aish2995  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18x0o5k/common_global_reward_vs_individual_reward_in_marl/</guid>
      <pubDate>Tue, 02 Jan 2024 21:19:28 GMT</pubDate>
    </item>
    <item>
      <title>DQL 没有改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18wwsxt/dql_not_improving/</link>
      <description><![CDATA[我尝试从头开始实现蛇深度 Q 学习，但它似乎没有改进，也不知道为什么。任何帮助或建议或提示都会有帮助。 链接https://colab。 Research.google.com/drive/1H3VdTwS4vAqHbmCbQ4iZHytvULpi9Lvz?usp=sharing 通常我使用 Jupyter Notebook，google colab 只是为了共享 为我自私的请求道歉， 提前致谢   由   提交/u/Witty_Fan_5776   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18wwsxt/dql_not_improving/</guid>
      <pubDate>Tue, 02 Jan 2024 18:45:23 GMT</pubDate>
    </item>
    <item>
      <title>如何开始为 Steam 游戏创建自定义环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18wt4pf/how_would_one_even_begin_to_create_a_custom/</link>
      <description><![CDATA[我最近对 ​​rl 有了更多的了解，这让我对创建一个可以玩电脑游戏的代理的潜力感到好奇。唯一的问题是我什至不知道创造环境是否可行。然而，以前在《DOTA》和《火箭联盟》等游戏中就已经做过这样的事情，这让我想知道他们是如何做到的。我很好奇是否有一种实际的方法可以设置带有状态、动作和奖励的环境的流行游戏？   由   提交/u/Scruffy004   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18wt4pf/how_would_one_even_begin_to_create_a_custom/</guid>
      <pubDate>Tue, 02 Jan 2024 16:17:12 GMT</pubDate>
    </item>
    <item>
      <title>[R] 大语言模型世界国际象棋锦标赛🏆♟️ (GPT-4 > Gemini-Pro)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18wez9h/r_large_language_models_world_chess_championship/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18wez9h/r_large_language_models_world_chess_championship/</guid>
      <pubDate>Tue, 02 Jan 2024 03:11:03 GMT</pubDate>
    </item>
    <item>
      <title>额外训练 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w7vf0/additional_training_a_rl_algorithm/</link>
      <description><![CDATA[我正在训练 RL 模型。我想知道是否可以让模型一次仅使用一个奖励函数进行学习，然后取消注释并使其使用另一个互斥的函数进行学习？理论上可行吗，我如何在代码中实现它。  我是新手。 我的训练制度： Name = rf&#39;Agents_Allignment{SimulationVariables[&quot;SimAgents&quot;]}_PPO_{SimulationVariables [“LearningTimeSteps”]}&#39; env = DummyVecEnv([lambda: FlockingEnv()]) model = PPO(“MlpPolicy”, env,tensorboard_log=“./ppo_Agents_tensorboard/”, verbose=1) model.learn( Total_timesteps=SimulationVariables[“LearningTimeSteps”]) # 调整乘数 # 保存模型 model.save(Name) env.close() # 加载模型 env = FlockingEnv() model = PPO.load(Name) # 运行 10范围内剧集的剧集（1，RLVariables[&#39;Episodes&#39;]）：obs = env.reset（）完成=错误奖励= 0 Position_dict = {i：[] for i in range（len（env.agents））}时间步长= 0reward_log=[] print(“Episode”,episode) # 完成条件 while((timestep &lt;=SimulationVariables[“EvalTimeSteps”]) and (not did)): action, state = model.predict(obs) obs,reward,done,info = env.step(action) ######### env.step() #添加碰撞退出条件reward_log.append(reward) print(reward) for i, agent in enumerate(env.agents): Positions_dict[i].append(agent.position.tolist()) with open(rf&#39;{Results[&quot;EpRewards&quot;]}_Allignment_{episode}.json&#39;, &#39;w&#39;) as f : json.dump(reward_log, f, indent=4) timestep = timestep + 1 # print(reward_log) with open(rf&#39;agent_positionsTestAllignment_{episode}.json&#39;, &#39;w&#39;) as f: #添加到参数文件 json. dump(positions_dict, f, indent=4) env.close()  ​   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w7vf0/additional_training_a_rl_algorithm/</guid>
      <pubDate>Mon, 01 Jan 2024 21:53:51 GMT</pubDate>
    </item>
    <item>
      <title>COOM：持续强化学习的游戏基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w24o8/coom_a_game_benchmark_for_continual_reinforcement/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=qmCxdPkNsa 代码：https ://github.com/hyintell/COOM 视频：https://www.youtube.com/watch?v=FUm2B8MZ6d0 摘要：  进步持续强化学习（RL）一直面临着各种障碍，包括标准化的指标和评估协议、苛刻的计算要求以及缺乏广泛接受的标准基准。为了应对这些挑战，我们提出了COOM（Continual DOOM），这是一个为基于像素的强化学习量身定制的连续强化学习基准。 COOM 提供了一套在视觉上不同的 3D 环境中精心设计的任务序列，作为一个强大的评估框架来评估持续强化学习的关键方面，例如灾难性遗忘、知识转移和样本高效学习。在对流行的持续学习（CL）方法进行深入的实证评估后，我们查明了它们的局限性，提供了对基准的宝贵见解，并强调了独特的算法挑战。这使得我们的工作成为第一个在具有具体感知的 3D 环境中对基于图像的 CRL 进行基准测试的工作。 COOM 基准的主要目标是为研究界提供有价值且具有成本效益的挑战。它旨在加深我们对强化学习环境中当前和即将推出的 CL 方法的功能和局限性的理解。代码和环境是开源的，可以在 GitHub 上访问。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w24o8/coom_a_game_benchmark_for_continual_reinforcement/</guid>
      <pubDate>Mon, 01 Jan 2024 17:53:01 GMT</pubDate>
    </item>
    <item>
      <title>关闭策略策略梯度定理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/</link>
      <description><![CDATA[嗨，我真的很想逐行理解离策略策略梯度算法。 本文由 Degris 撰写， T.、怀特、M. 和萨顿，R.S. (2012).论文链接：(https://arxiv.org/pdf/1205.4839.pdf) 因此，在论文的 2.2 节中，作者指出，在离策略 pg 中，我们通过省略全梯度公式中的附加项来使用真实 pg 的近似值。  现在，在附录 A 中，作者试图首先在各国共享一个参数化政策的向量 u 的一般情况下证明这一点。  我理解第一点，如果我们使用在不同状态和动作对评估的加性梯度来更新参数，新参数最终将为我们提供更高的目标函数。在此目标中，状态和动作对的价值函数保持不变，但是具有较高价值的 $Q{\pi_u, \gamma}(s,a)$ 在 $\ pi_{u&#39;, \gamma}$。  但是，我无法完全理解，并且我正在努力以一种非常数学上稳健的方式看待它，为什么如果我们开始使用$\pi_{u&#39;, \gamma}$ 依次。  本质上让我困惑的是证明中的政策改进部分（参见附图2）。   由   提交 /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/</guid>
      <pubDate>Mon, 01 Jan 2024 17:22:29 GMT</pubDate>
    </item>
    </channel>
</rss>