<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 20 Jun 2024 06:20:18 GMT</lastBuildDate>
    <item>
      <title>如果 PPO 受到稀疏奖励的影响，那么 InstructGPT 和 Learning to Summarize 是如何使其发挥作用的呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</link>
      <description><![CDATA[我见过很多关于 PPO 如何难以在稀疏奖励环境中发挥作用的讨论。在 instructGPT 和学习从人类反馈中总结的情况下，仅在一长串采样标记的最后一个标记处给予奖励。特别是对于一些涉及非常长的代数（1000 个动作范围）且最后只有一个奖励的现代 RLHF 任务 - PPO 如何在这里取得成功？我是否遗漏了优化？    提交人    /u/idioticfuse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</guid>
      <pubDate>Thu, 20 Jun 2024 01:01:39 GMT</pubDate>
    </item>
    <item>
      <title>“GUI-WORLD：面向 GUI 的多模态 LLM 代理的数据集”，Chen 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djxzhm/guiworld_a_dataset_for_guioriented_multimodal/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djxzhm/guiworld_a_dataset_for_guioriented_multimodal/</guid>
      <pubDate>Thu, 20 Jun 2024 00:00:43 GMT</pubDate>
    </item>
    <item>
      <title>“在训练和推理中权衡计算：我们探索了几种在训练或推理上花费更多资源之间进行权衡的技术，并描述了这种权衡的属性。我们概述了对人工智能治理的一些影响”，EpochAI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djxm5z/trading_off_compute_in_training_and_inference_we/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djxm5z/trading_off_compute_in_training_and_inference_we/</guid>
      <pubDate>Wed, 19 Jun 2024 23:43:06 GMT</pubDate>
    </item>
    <item>
      <title>stablebaselines jax 加速 vs pytorch</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djwnlz/stablebaselines_jax_speedup_vs_pytorch/</link>
      <description><![CDATA[他们在文档中声称这相当于 20 倍加速。这是怎么回事？我在 pytorch 中编写了一个环境。 我正在 pytorch 中自己编写 PPO 代理（用于学习）。但是对于大型实验，如果速度如此之快，我是否需要将其移植到 JAX？我假设我无法在不绕过 CPU 内存的情况下将我的 pytorch 张量转移到 Jax 张量。 有什么想法吗？    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djwnlz/stablebaselines_jax_speedup_vs_pytorch/</guid>
      <pubDate>Wed, 19 Jun 2024 22:58:55 GMT</pubDate>
    </item>
    <item>
      <title>努力实现带有动作掩码的 DQN，用于四子棋 RLLIB</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djv9pe/struggling_to_implement_dqn_with_action_mask_for/</link>
      <description><![CDATA[嗨，我是 RL 的新手。我正在尝试使用动作掩码为 pettingzoo 的 Connect four 实现 DQN，但无论我运行代码多长时间，性能似乎都没有什么提升。我的代码如下： import os import ray from ray import tune from ray.rllib.algorithms.dqn.dqn import DQNConfig from ray.rllib.env import PettingZooEnv from ray.rllib.models import ModelCatalog from ray.rllib.algorithms.dqn.dqn_torch_model import DQNTorchModel from ray.rllib.utils.framework import try_import_torch from ray.tune.registry import register_env from ray.rllib.utils.torch_utils import FLOAT_MAX from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC from pettingzoo.classic import connect_four_v3 from ray.rllib.models.torch.torch_modelv2 import TorchModelV2 import torch.nn. functional as F from ray.rllib.utils.annotations 导入覆盖 导入 numpy 作为 np torch，nn = try_import_torch（） 类 CustomDQNTorchModel（TorchModelV2，nn.Module）： def __init__（self，obs_space，action_space，num_outputs，model_config，name）： TorchModelV2.__init__（self，obs_space，action_space，num_outputs，model_config，name） nn.Module.__init__（self） self.conv1 = nn.Conv2d（obs_space.shape[0]，32，kernel_size=8，stride=2） self.conv2 = nn.Conv2d（32，64，kernel_size=4，stride=2） self.conv3 = nn.Conv2d（64，128，kernel_size=4，stride=2） self.fc1 = nn.Linear（128 * 7 * 7, 512） self.fc2 = nn.Linear（512，action_space.n）@override（DQNTorchModel）def forward（self，input_dict，state，seq_lens）：x = F.relu（self.conv1（input_dict [“obs”]））x = F.relu（self.conv2（x））x = F.relu（self.conv3（x））x = x.view（x.size（0），-1）x = F.relu（self.fc1（x））返回self.fc2（x），状态@override（DQNTorchModel）def get_q_value_distributions（self，model_out）：返回model_out，model_out，model_out@override（DQNTorchModel）def get_state_value（self，model_out）：返回torch.zeros（model_out.size（0）） class TorchMaskedActions（CustomDQNTorchModel）： def __init__（self，obs_space，action_space，num_outputs，model_config，name，**kwargs）： orig_space = getattr（obs_space，“original_space”，obs_space） CustomDQNTorchModel.__init__（self，obs_space，action_space，num_outputs，model_config，name） self.internal_model = TorchFC（orig_space[“observation”]，action_space，num_outputs，model_config，name +“_internal”，） def forward（self，input_dict，state，seq_lens）： action_mask = input_dict[“obs”][“action_mask”] logits，_ = self.internal_model({&quot;obs&quot;: input_dict[&quot;obs&quot;][&quot;observation&quot;]}) inf_mask = torch.clamp(torch.log(action_mask), min = np.float32(-1e10)) masked_logits = logits + inf_mask return masked_logits, state if __name__ ==&quot;__main__&quot;: ray.init() alg_name =&quot;DQN&quot; ModelCatalog.register_custom_model(&quot;pa_model&quot;, TorchMaskedActions) def env_creator(): env = connect_four_v3.env() return env env_name =&quot;connect_four_v3&quot; register_env（env_name，lambda 配置：PettingZooEnv（env_creator（）））test_env = PettingZooEnv（env_creator（））obs_space = test_env.observation_space act_space = test_env.action_space 配置 = （DQNConfig（）.environment（env=env_name）.rollouts（num_rollout_workers=9，rollout_fragment_length=5）.training（train_batch_size=256，lr=1e-5，gamma=0.99，n_step=1，replay_buffer_config={“type”：“MultiAgentReplayBuffer”，“capacity”：100000，}，hiddens=[]，dueling=False，model={“custom_model”：“pa_model”}， ） .resources（num_gpus = int（os.environ.get（＆quot;RLLIB_NUM_GPUS＆quot;，＆quot;1＆quot;））） .debugging（log_level =＆quot;ERROR＆quot;） .framework（framework =＆quot;torch＆quot;） .multi_agent（policy = {＆quot;player_0＆quot;：（None，obs_space，act_space，{}），＆quot;player_1＆quot;：（None，obs_space，act_space，{}），}，policy_mapping_fn =（lambda agent_id，* args，** kwargs：agent_id），） .exploration（exploration_config = {＆quot;type＆quot;：＆quot;EpsilonGreedy＆quot;，＆quot;initial_epsilon＆quot;：0.8，＆quot;final_epsilon＆quot;： 0.01, “epsilon_timesteps”: 100000, } ) ) config.rl_module(_enable_rl_module_api=False) config.training(_enable_learner_api=False) tune.run( alg_name, name=“DQN”, stop={“timesteps_total”: 10000000 if not os.environ.get(“CI”) else 50000}, checkpoint_freq=10, config=config.to_dict(), )     提交人    /u/Academic_Painting417   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djv9pe/struggling_to_implement_dqn_with_action_mask_for/</guid>
      <pubDate>Wed, 19 Jun 2024 21:58:01 GMT</pubDate>
    </item>
    <item>
      <title>“围棋 AI 能否具有对抗鲁棒性？”，Tseng 等人，2024 年（KataGo 的“绕圈”攻击可以被击败，但仍然可以找到更多攻击；这并不是由于 CNN）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djv1r3/can_go_ais_be_adversarially_robust_tseng_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djv1r3/can_go_ais_be_adversarially_robust_tseng_et_al/</guid>
      <pubDate>Wed, 19 Jun 2024 21:48:09 GMT</pubDate>
    </item>
    <item>
      <title>MADRL 2024 年秋季博士学位授予权</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djqaqp/phd_offers_for_fall_2024_in_madrl/</link>
      <description><![CDATA[我即将结束我的 AI 工程学硕士学位，由于我最近在研究多智能体 DRL，所以我希望攻读该领域的博士学位。我所在的实验室可能会邀请我继续攻读博士学位，因为他们有一个博士生团队在研究 MAS（更准确地说是多智能体系统、无人机和卫星），但我想先把“B 计划”放在一边，以防万一我没有得到这样的机会，因为我只有口头承诺。 但是，我发现这个领域的论文录取通知书很少，老实说，我开始申请有点晚了，因为大多数截止日期都在 4 月之前 :/ 如果您知道任何此类机会仍开放申请，请告诉我 :D    提交人    /u/ham_bam0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djqaqp/phd_offers_for_fall_2024_in_madrl/</guid>
      <pubDate>Wed, 19 Jun 2024 18:28:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 RLlib 进行多智能体供应链优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djltne/multiagent_supply_chain_optimization_with_rllib/</link>
      <description><![CDATA[亲爱的社区， 我正在寻找有关我目前在项目中面临的挑战性问题的见解和帮助。 问题： 仓库 X 面临着一项复杂的任务，即优化其供应链流程，以应对来自 200 多家供应商的 1000 种不同产品。目标是避免库存过剩、防止缺货并始终满足客户需求。产品分为易腐烂和不易腐烂两类。 我们提出的解决方案： 多智能体环境，其中每个智能体都会学习产品线的策略。我们使用了通过 Ray RLlib 实现的 PPO 算法。我们的 RL 环境旨在考虑以下操作：  何时下采购订单 订购哪些产品 每种产品的订购数量 考虑哪个供应商  代理根据过去一年的历史销售情况进行训练。环境由 N 个观察值组成，代表库存水平和趋势，以及 2 个操作（订购数量和供应商 ID）。代理在具有共享策略选项集的多代理环境中进行训练，每个情节由 365 个时间步骤组成。奖励是根据一系列行动后在一个情节中获得的净利润计算的。此外，奖励塑造用于指导代理做出正确的决策。当错过需求、订单延迟和选择错误的供应商（价格更高或交货时间更长）时，我们在每个时间步骤中施加惩罚。实际上，代理在时间步骤“t”做出的每个决定都会对未来时间步骤“t+M”产生负面/正面影响。 在每个时间步骤，历史数据的需求都用于更新观察值。因此，预计代理将达到或超过去年的净利润。 关于神经网络，我们使用了一个自定义模型，该模型由具有 ReLU 激活的线性层组成，分支为 softmax 用于供应商选择，并为数量分支输出值使用自定义激活函数，范围为 [0, max_purchase_quantity]。已应用批量归一化来增强收敛并减少过度拟合，并已结合注意力机制来关注观察空间中的关键值。 遇到的问题： 尽管付出了努力，但我在实现最佳收敛和性能方面仍面临挑战。训练过程明显很慢（使用 Nvidia GPU GeForce RTX 3080 10GB）。在对 50 种产品进行了 15,000 次迭代（历时 28 小时）之后，代理仍未达到预期的奖励，无法自主设置订单数量和供应商 ID。我们尝试在奖励函数中对错误决策实施惩罚，尝试了不同的探索策略，使用了课程学习（逐步应用惩罚），并应用了奖励规范化。然而，结果并不如预期。 您可以如何提供帮助： 我正在寻求帮助和新观点。如果您有供应链优化领域强化学习的经验，那么关于提高收敛速度、处理复杂动作空间或有效探索策略的见解的建议将非常有价值。 此外，如果您遇到过类似的挑战或在供应链环境中成功实施了 RL 代理，我们将非常感激您对调整超参数、设计有效奖励函数的指导或任何其他相关建议。 提前感谢您考虑这个主题。 我热切期待您的回复和见解。    提交人    /u/WoodenDot8305   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djltne/multiagent_supply_chain_optimization_with_rllib/</guid>
      <pubDate>Wed, 19 Jun 2024 15:22:31 GMT</pubDate>
    </item>
    <item>
      <title>电池管理系统中的DQN代理训练问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djfbza/dqn_agent_training_problem_in_battery_management/</link>
      <description><![CDATA[      大家好， 我正在研究一种电池管理系统，它的工作方式如下：在主电网能源价格低时充电，在价格高时放电以产生收入。 如果您看到第二个下面的图，您会发现它仍然没有正确充电/放电意味着无法利用低价和高价。 我保留的奖励函数如下： reward = sell_benefit - buy_cost - 0.01*(battery_cost) -&gt; battery_cost = energy**2*0.001 t+23 时的价格如下 ACTIONS = [i for i in range(-80, 90, 10)]  有什么建议吗？ https://preview.redd.it/hzc9mv1v1i7d1.png?width=938&amp;format=png&amp;auto=webp&amp;s=966b40cb1ca2f8be6e4730fcd0a9494e0f484a09 https://preview.redd.it/nbsyfn8u1i7d1.png?width=562&amp;format=png&amp;auto=webp&amp;s=62e9058ef6dc7d5bf04abd683b35b4768acb71b4    提交人    /u/Dry-Image8120   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djfbza/dqn_agent_training_problem_in_battery_management/</guid>
      <pubDate>Wed, 19 Jun 2024 09:53:36 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习蛇</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djehei/deep_q_learning_snake/</link>
      <description><![CDATA[大家好！我正在尝试为一个项目编写一个深度 q 学习蛇。我提供的代码最初是为使用遗传算法训练的神经网络编写的，我试图将其转换为使用强化学习训练的神经网络。 我尝试学习最大值，但我对这种算法还很陌生，似乎无法让它工作。 特别是，我的代码可以工作，但没有学习，因为没有使用更新、学习和训练函数。每次我尝试在 choose_next_move 函数中实现它们时都会出现问题。 我需要一点帮助！ 我的代码可以在这里获得：https://github.com/ouroborosses/Deep-Q-learning-Snake    提交人    /u/mariemeier_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djehei/deep_q_learning_snake/</guid>
      <pubDate>Wed, 19 Jun 2024 08:53:36 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习蛇</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djeg7b/deep_q_learning_snake/</link>
      <description><![CDATA[大家好！我正在尝试为一个项目编写一个深度 q 学习蛇。我提供的代码最初是为使用遗传算法训练的神经网络编写的，我试图将其转换为使用强化学习训练的神经网络。 我尝试学习最大值，但我对这种算法还很陌生，似乎无法让它工作。 特别是，我的代码可以工作，但没有学习，因为没有使用更新、学习和训练函数。每次我尝试在 choose_next_move 函数中实现它们时都会出现问题。 我需要一点帮助！ 我的代码可以在这里获得：https://github.com/ouroborosses/Deep-Q-learning-Snake    提交人    /u/mariemeier_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djeg7b/deep_q_learning_snake/</guid>
      <pubDate>Wed, 19 Jun 2024 08:51:11 GMT</pubDate>
    </item>
    <item>
      <title>将代理最后选择的离散动作（int）包含在观察空间中可以吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djay0a/is_it_ok_to_include_agents_last_chosen_discrete/</link>
      <description><![CDATA[  由    /u/against_all_odds_  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djay0a/is_it_ok_to_include_agents_last_chosen_discrete/</guid>
      <pubDate>Wed, 19 Jun 2024 04:52:56 GMT</pubDate>
    </item>
    <item>
      <title>DQN 损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dj9hha/dqn_loss/</link>
      <description><![CDATA[      嗨， 我正在使用自定义环境运行 DQN 算法。如果我每 20 集训练一次我的环境，或者我每集每一步训练我的环境，我的奖励就会收敛到相同的值。但是，我的损失看起来非常不同。您能告诉我损失对 DQN 的意义是什么吗？为什么在这种情况下它们会有所不同，我应该选择哪一个更接近我们对 DQN 的期望？ 结果 这些是我调整超参数后的结果。我试图更好地理解损失结果。它们有问题吗？ 谢谢。    提交人    /u/hifzak   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dj9hha/dqn_loss/</guid>
      <pubDate>Wed, 19 Jun 2024 03:27:36 GMT</pubDate>
    </item>
    <item>
      <title>扩散规划器（Janner 的扩散器）如何开环执行计划？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dj0q1k/how_does_diffuser_by_janner_diffusion_planner/</link>
      <description><![CDATA[扩散器在连续空间中生成 (s,a) 元组的计划，论文称它们开环执行生成的计划。 但扩散模型在连续空间中的动态方面肯定存在轻微的不准确性，即从 s_t 采取动作 a_t 并不能完美地产生计划中生成的 s_{t+1}。那么模型如何开环执行计划，它是否只是在动作 a_t 引起的实际 s_{t+1} 状态下作用 a_{t+1}？ 此外，有人玩过具有离散设置的扩散规划器吗？例如，使用扩散规划器玩棋盘游戏，直观地看，生成的计划会违反很多动态（即不能将棋子移动到那里）？    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dj0q1k/how_does_diffuser_by_janner_diffusion_planner/</guid>
      <pubDate>Tue, 18 Jun 2024 20:31:53 GMT</pubDate>
    </item>
    <item>
      <title>“从阿谀奉承到诡计多端：调查大型语言模型中的奖励篡改”，Denison 等人 2024 年 {Anthropic}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1die9r7/sycophancy_to_subterfuge_investigating/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1die9r7/sycophancy_to_subterfuge_investigating/</guid>
      <pubDate>Tue, 18 Jun 2024 01:05:23 GMT</pubDate>
    </item>
    </channel>
</rss>