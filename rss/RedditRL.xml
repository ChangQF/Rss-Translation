<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 14 Feb 2025 09:19:22 GMT</lastBuildDate>
    <item>
      <title>RL教程的业余爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://github.com /google-deepmind/mujoco/descordions/2404    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/goncalogordo     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6l9w/rl_tutorials_for_hobbyists/</guid>
      <pubDate>Fri, 14 Feb 2025 09:06:19 GMT</pubDate>
    </item>
    <item>
      <title>熵重</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip6d4b/entropy_weight/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在使用软演员评论家进行多代理强化学习。折扣奖励约为1000-1300。熵重的正确值是多少？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/fuzzy-plantain2402      link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip6d4b/entropy_weight/</guid>
      <pubDate>Fri, 14 Feb 2025 08:49:24 GMT</pubDate>
    </item>
    <item>
      <title>回顾我成为利基领域的RL研究人员的计划（AG/遗传学）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip51q9/review_my_plan_for_becoming_an_rl_researcher_in_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi。我从事遗传学和农业工作。我是平庸的软件工程师/定量遗传学家，数学背景相对较差。去年，我制作了一个简单的问题。在Pytorch中制作了一个环境，并使用SB3成功地训练代理，并成功地训练了一个非常简单的用例。我还审查了我的数学量，一旦我遇到泰勒系列（我从未在学校正式研究过）。 我已经完成了机器学习问题（计算机视觉）。在成功的行业中，对基金会的研究足以在可可/重新确定年龄的情况下进行监督/无监督的学习问题。  ，所以我的计划是做以下  1）继续研究DSA（阅读数据结构和算法的常识指南，第I卷，也许是第II卷）  2）阅读Grokking DL算法书籍  3）然后通过此内容（祈祷最近的LLM可以推动我解决此问题） https://github.com/mathfoundationrll /读书基础学习学习    all    4）在Jax中精心选择的功能重建我的环境然后再次重新进行我过去的SB3实验。 *虽然对我的实验非常公开  5）扩展实验以具有真实的分布式组件（在上下文中有意义）  我的目标是做这些工具在我的行业中的咨询...因此，一旦我的基础知识降低了，也许我可以开始独立发表论文。否则，我将不得不考虑申请其中一家大公司或攻读博士学位 我有6个月的合理跑道来实现这一目标，这使我希望我的基础很务实。我没有任何社区或指导可以与之讨论。如果我能在我放下头之前对此获得任何反馈，我会感谢它。另外，我不会拒绝任何经验丰富的人保持联系。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/foodisaweapon     [link]   ＆＃32;   [注释]     ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip51q9/review_my_plan_for_becoming_an_rl_researcher_in_a/</guid>
      <pubDate>Fri, 14 Feb 2025 07:10:51 GMT</pubDate>
    </item>
    <item>
      <title>人类会做RL，监督学习还是完全不同的事情？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip2cl2/do_humans_do_rl_supervised_learning_or_something/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我已经在加强学习已经几个月了，当我不得不汗水以定义该问题时，这个问题总是在我的脑海中正确的奖励。 我得到了这种感觉，我们能够根据真正的奖励创造中间奖励。就像为了在X Company找到工作一样，我必须以前磨削这些n步骤，并且每次执行此步骤时都会很高兴。 在RL中RL模型，如果您可以正确调整损失功能？ 我的问题似乎尚不清楚，并且非常开放。我只是觉得人类在RL和有监督的学习之间有一个中间的，我无法真正掌握我的头。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/delicious_wall3597      [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1ip2cl2/do_humans_do_do_rl_supervise_learning_learning_or_something/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip2cl2/do_humans_do_rl_supervised_learning_or_something/</guid>
      <pubDate>Fri, 14 Feb 2025 04:19:33 GMT</pubDate>
    </item>
    <item>
      <title>epymarl- mappo rware总是给予0奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ip1fj6/epymarl_mappo_rware_always_gives_0_reward/</link>
      <description><![CDATA[     你好， ，所以我正在使用epymarl  https：/ /Github.com/uoe-agents/epymarl 使用Mappo算法训练Rware。但是问题是即使我跑40m时间步骤，奖励总是0。   https：https：// pr = CFC968525607543A888330F7A01554C86A25944E7B   我对Marl有点陌生。如果有人已经使用了Rware，您可以告诉我我缺少的内容。 我没有更改EpyMarl repo   &lt;！&lt;！ -  sc_on-&gt;＆&gt;＆ ＃32;提交由＆＃32; /u/u/ajxbnu     [link]   ＆＃32;   [注释]   /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ip1fj6/epymarl_mappo_rware_always_gives_0_reward/</guid>
      <pubDate>Fri, 14 Feb 2025 03:28:01 GMT</pubDate>
    </item>
    <item>
      <title>RL无法改善基本监督模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</link>
      <description><![CDATA[      i具有基于序列预测的合理工作的基于基于的模型（RNN）。然后，我创建了一个PPO RL模型来调整PRE的输出 - 训练的RNN模型。问题：RL实际上降低了MSE度量。我有些惊讶RL实际上可能会受到如此巨大的伤害。&lt; /p&gt;   MSE，没有RL调整：0.000047  MSE带有RL调整：0.002053    验证MSE vs Iteration     &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/u/maadotaa     [link]   ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioxfqh/rl_does_not_improve_upon_the_base_supervised_model/</guid>
      <pubDate>Fri, 14 Feb 2025 00:00:09 GMT</pubDate>
    </item>
    <item>
      <title>“使用大型推理模型[O3]的竞争性编程”，El-Kishky等人2025 {oa}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iow321/competitive_programming_with_large_reasoning/</guid>
      <pubDate>Thu, 13 Feb 2025 22:56:18 GMT</pubDate>
    </item>
    <item>
      <title>Langevin Soft Actor-Critic：通过不确定性驱动的批评者学习，Ishfaq等人2025。ICLR 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</link>
      <description><![CDATA[        &lt;！ -  sc__off- sc_off-&gt;  现有的Actor-Critic算法在连续控制加强学习（RL）任务中很受欢迎，由于其中缺乏原则性的探索机制，其样本效率不佳。由于汤普森采样成功在RL中有效探索的动机，我们提出了一种新颖的无模型RL算法，\ emph {langevin soft Actor评论家}（LSAC）（LSAC）优先考虑通过对策略优化的不确定性估计来增强评论家的学习。 LSAC采用了三个关键创新：通过基于分布的LangeMonte Carlo（LMC）更新，近似汤普森采样，平行回火，用于探索该功能后部多种模式，以及与动作梯度正常化的综合状态行动样品。我们的广泛实验表明，LSAC的表现优于或匹配无连续控制任务主流模型RL算法的性能。值得注意的是，LSAC标志着基于LMC的Thompson采样在具有连续动作空间的连续控制任务中的首次成功应用  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/hmi2015     [link]  ＆＃32;   [注释]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioqcuo/langevin_soft_actorcritic_efficient_exploration/</guid>
      <pubDate>Thu, 13 Feb 2025 18:50:08 GMT</pubDate>
    </item>
    <item>
      <title>我的个人项目-Alphayinshzero（Blitz）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</link>
      <description><![CDATA[&lt; src =“ https://b.thumbs.redditmedia.com/qwtnkjppq5js2x​​jru2xjru2xv2ulsrfp8lpgjynuhfkuwg7i.jpg” title =“我的个人项目-Alphayinshzero（Blitz） - &gt;  我使用alphazero训练了闪电兹版本的AI模型，并且它能够在板空间上击败Smartbot。 请注意，Blitz版本是您尝试连续获得5个的地方。 这是迭代174对抗自己。  https://preview.redd.it/1ohn81p23yie1.png?width=866&amp;format=png&amp;auto=webp&amp;s=3d7261cdc446f7349eae31fe5bca4b66b8bcbfed&lt; /a&gt;  在训练期间，有充分的证据表明，闪电战版本具有第一球员优势，因为第一个球员逐渐攀升至最后的胜利率。 我是强化学习的新手，当涉及政策分配时，我可能会提出一种特殊的方法，因此请随时告诉我这是否是一种有效的方法，或者在AI培训中是否有问题。 我将yinsh表示为11 x 11数组，因此动作空间为121 + 1（通过转弯）。 我想避免使用大型政策分布，例如121（statter） * 121（目的地）= 14641  所以，我将游戏分为阶段：戒指放置（放置10个戒指），戒指选择（选择要移动的戒指）和标记放置（放置一个（放置一个）标记并移动选定的戒指）。 这样，单人的回合可以像这样工作： 转1-选择要移动的戒指。转弯2-对手通过。转3-选择要移动戒指的位置。&lt; /p&gt; 通过将其分为阶段，我可以使用121 + 1的动作空间。这种方法“感觉”对我而言。它似乎有效。  ...  我试图训练Yinsh的完整游戏，但这是不完整的。到目前为止，我对它的策略感到非常满意。 不满意，我的意思是它只是沿着边缘形成了一个密集的标记领域，他们不想互相互动。我真的希望AI战斗和造成混乱，但他们太宁静了 - 只是在关注自己的生意。通过沿着边缘形成致密的标记，标记变得难以置信。  AI的（天真？）方法只是：“让我像农民一样在边缘形成标记领域来自同一地区的5英寸排。”他们就像董事会对面的两个农民，宁静地制作了自己的标记领域。 闪电战版更令人兴奋，而人工智能互相战斗：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kiwigami     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iopv6i/my_personal_project_alphayinshzero_blitz/</guid>
      <pubDate>Thu, 13 Feb 2025 18:29:17 GMT</pubDate>
    </item>
    <item>
      <title>当您已经拥有体育馆环境时，SB3矢量化环境如何工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我无法完全理解。您只是使用他们的Vecenv包装它吗？还是我必须重写它？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/blearx     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioojbp/how_do_sb3_vectorised_environments_work_when_you/</guid>
      <pubDate>Thu, 13 Feb 2025 17:33:39 GMT</pubDate>
    </item>
    <item>
      <title>使用多个跑步者的rllib不会增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对不起，在这里绝对没有图片。  因此，我的问题是在RLLIB上使用SAC的24个ENV跑步者，根本没有学习。但是，使用2个Env跑步者确实学习了（有点）。 &lt; /p&gt; 详细信息： env-是简单的2D移动到目标位置，当目标状态达到-0.01时，稀疏奖励，每次步骤达到-0.01，带有500个框架限制，带有框（Shape =（10，））观察和框（-1,1）动作空间。我尝试了一堆超参数，但似乎没有用。对Rllib非常新。我曾经制作自己的RL库，但我想这次尝试rllib。 有人知道问题是什么吗？如果您需要更多信息，请问我！谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/automatic-web8429     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioksd7/rllib_using_multiple_runners_does_not_increase/</guid>
      <pubDate>Thu, 13 Feb 2025 14:53:55 GMT</pubDate>
    </item>
    <item>
      <title>参考丢失：带有RL算法分类法/本体的电子表格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在这里的某个地方看到了它，现在找不到它。我知道有一些调查RL算法的论文，但我试图找到“散布表”，其中一位成员在评论中发布。我相信这是Google文档的链接。 每行都有一些更高级别的分组，每个组和注释中都有算法。它通过其属性（例如连续的动作空间等）将算法分开。 有人知道该资源还是我可以找到的地方？ 编辑：找到它！  https://rl-picker.github.io/     &lt;！ -  sc_on- &gt;＆＃32;提交由＆＃32; /u/u/u/paramedicfabulou345     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ioh7j5/reference_lost_spreadsheet_with_rl_algorithm/</guid>
      <pubDate>Thu, 13 Feb 2025 11:41:11 GMT</pubDate>
    </item>
    <item>
      <title>谢尔盖·莱文（Sergey Levine）的增强学习[我在哪里可以找到这个]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi     作为初学者，我希望对RL背后的数学背后的数学掌握。 ##您能让我知道在哪里可以找到这门课程？请。 ##     [sutton barto]增强学习= https://www.amazon.in/Reinforcement-Learning-Introduction-Richard-Sutton/dp/0262039249?dplnkId=c3df8b9c-8d63-4f9b-8a4e-bc601029852c     遵循的其他资源是什么？您能吸引他们使用的它们吗？请   我也   我开始学习ML，并想问这里有经验的人有关理解数学的要求像k-nn/svm  一样，在每种算法后面证明了在算法后面的数学或可以观看视频，了解关键，然后开始编码 研究ML的合适方法是什么？ ## ML工程师会涉足很多编码，还是通过可视化和开始编码来底层crux？ 请让我知道。 （我在这个域中没有希望）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/unternationalwill912     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1io9gbn/sergey_levine_reinforcement_learning_where_can_i/</guid>
      <pubDate>Thu, 13 Feb 2025 03:02:00 GMT</pubDate>
    </item>
    <item>
      <title>击败当前超级马里奥的第一级的最佳RL方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我看过PPO，DQN和Neat。 Sethbling在2015年写了一位使用Neat的RL代理，看起来它表现出了最好的表现。休息了4年后，我正在回到RL空间，并希望在Python中实施个人项目。我应该实施哪一个？有新方法吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/marblesandcookies     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1inugiz/what_is_the_best_rl_method_for_beating_the_first/</guid>
      <pubDate>Wed, 12 Feb 2025 16:10:25 GMT</pubDate>
    </item>
    <item>
      <title>RL和机器人技术的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我最近在RL（技术上是反向RL）的博士学位毕业于人类机器人协作。我已经使用了4种不同的机器人操纵器，4个不同的抓钉和4台不同的RGB-D摄像头。我的专业知识在于使用感知反馈来学习智能行为，以进行安全有效的操纵。  我已经建造了端到端管道，用于在传送带上制作产品，在到达孵化器之前，非破坏性识别和去除不育卵，使用机器人对医疗器械进行智能无菌处理，还有一些其他项目。我已经在三菱电气研究实验室实习，到目前为止在顶级会议上发表了6篇论文。 我已经使用了许多对象检测平台，例如Yolo，例如Yolo，更快RCNN，detectron2，Mediapipe，Mediapipe，Mediapipe，等等，并具有丰富的注释和培训经验。我对Pytorch，ROS/ROS2，Python，Scikit-Learn，Opencv，Mujoco，Gazebo，Pybullet，并且在Wandb和Tensorboard方面有一定的经验。由于我不是来自CS背景，所以我不是专家软件开发人员，但是我写了稳定，干净，下降的代码，很容易扩展。 我一直在寻找与此相关的工作，但是我很难在就业市场上导航。我真的很感谢您提供的任何帮助，建议，建议等。作为一个学生签证的人，我正在时钟，需要尽快找到工作。预先感谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/prasuchit     [链接]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1intpup/jobs_in_rl_and_robotics/</guid>
      <pubDate>Wed, 12 Feb 2025 15:40:05 GMT</pubDate>
    </item>
    </channel>
</rss>