<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 20 Dec 2024 15:17:04 GMT</lastBuildDate>
    <item>
      <title>预测行动和奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</link>
      <description><![CDATA[大家好，我正在处理一个非专家级数据的数据集，并且正在使用决策转换器。现在我想比较性能，但我找不到可以同时预测动作和奖励的离线 rl 模型。有人有什么建议吗？    提交人    /u/Anonymusguy99   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</guid>
      <pubDate>Fri, 20 Dec 2024 13:45:40 GMT</pubDate>
    </item>
    <item>
      <title>ppo 多个可变长度游戏的推出和奖励传播</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hijvst/ppo_multiple_variable_length_games_rollouts_and/</link>
      <description><![CDATA[我正在阅读 open ppg 和 ppo 示例实现。https://github.com/openai/phasic-policy-gradient/blob/master/phasic_policy_gradient/roller.py#L155 据我所知，他们生成播放的滚轮从不检查游戏是否结束，它只是假设环境将自行重新启动。实际上，这意味着对于比推出深度短的游戏，多个游戏将出现在缓冲区中以进行训练迭代。这意味着训练迭代将尝试在多个游戏中最大化奖励，而不是最大化单个游戏的奖励。 如果游戏长度固定，这当然没问题，但如果代理的动作可以让游戏更快地结束，那就不正确了。在这种情况下，如果奖励没有以某种方式规范化，网络可能会学会更快地完成游戏，这样它就可以玩更多，并获得更多积分。 我的理解正确吗？如果是这样，是否有意避免这个问题？我猜可以将分数除以游戏长度，但这似乎只有当游戏在单个游戏中产生的奖励总和为 -1、0 或 1 时才是正确的。    提交人    /u/drblallo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hijvst/ppo_multiple_variable_length_games_rollouts_and/</guid>
      <pubDate>Fri, 20 Dec 2024 13:33:58 GMT</pubDate>
    </item>
    <item>
      <title>救命！我的 RL Agent 没有学习。（OpenAI Gym env + Pytorch）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</link>
      <description><![CDATA[我试图实现一个简单的深度 Q 网络，以便在 OpenAI Gymnasium 提供的 Cart Pole 环境中训练代理。我尝试调整超参数，但似乎没有任何效果。事实上，随着 epoch 的增加，情况似乎变得更糟（虽然不确定）。我觉得我已经正确地实现了一切。我正在使用 pytorch 来构建神经网络。我对 RL 和深度学习还不熟悉，所以如果我遗漏了什么，我深表歉意。 我附上了我的代码，以便您可以运行它。该笔记本非常不言自明，除了 pytorch 之外，您只需要 gyanasium[classic-control] 和 pygame 包。 https://github.com/Utsab-2010/OpenAI-Gym-RL-Tests/blob/main/Cart_Pole_Deep_QN.ipynb 任何建议或帮助都将不胜感激。    提交人    /u/Otaku_boi1833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</guid>
      <pubDate>Fri, 20 Dec 2024 08:04:59 GMT</pubDate>
    </item>
    <item>
      <title>DDPG、A2C、PPO 和 TRPO 实验在 StableBaselines3 中何时收敛？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</link>
      <description><![CDATA[我需要运行所有基于策略梯度的 RL 实验，以展示它们在使用和不使用我计划提出的自定义训练方案的情况下收敛的速度。对于环境，我正在考虑 OpenAI Gym 环境（CartPole、Pendulum、LunarLander 和 2-3 Mujoco 环境）。但我首先需要确定它们是否与通常的训练方法收敛。即使它们不收敛，也应该有改善的迹象（就情景回报而言）。SB3 上是否有任何资源提供在这些环境中工作的超参数或策略网络？此外，是否有人对记录反向传播的数量或类似的东西（任何其他有用的指标）以评估计算负载等有任何建议？    提交人    /u/WayOwn2610   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</guid>
      <pubDate>Fri, 20 Dec 2024 06:39:36 GMT</pubDate>
    </item>
    <item>
      <title>决策频率：‘信息’视角</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</link>
      <description><![CDATA[小动作重复潜力：细粒度控制问题：信用分配 大动作重复潜力：更明智的决策问题：延迟 如果决策之间没有足够的时间，代理将根据较少的信息采取行动。 如果所述时间很长，则“适应变化”会被延迟。  推荐解决方案的示例：分层 RL：存在在快速行动的较低级别与以较慢速度行动的较高级别之间进行通信的问题。 决策转换器：离线方法，因此无法在工作中学习 根据我的经验，这个问题与计算或模型容量无关。无论学习能力有多强，智能体采取行动的频率（或缺乏的信息）都是有限的。 你对这个困境有什么看法？    提交人    /u/XecutionStyle   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</guid>
      <pubDate>Thu, 19 Dec 2024 19:48:50 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线 3 进行 SAC 训练可停止 TensorBoard 更新，并在自定义环境中 3,000 步后加速</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhv1ij/sac_training_with_stable_baselines3_halts/</link>
      <description><![CDATA[大家好， 我在自定义环境中使用Soft Actor-Critic (SAC)算法，其中代理每次迭代都会调整另一个优化器的超参数。最初，训练和学习顺利进行，最多可达3,000 个时间步。然而，在此之后，TensorBoard 停止更新，并且训练速度急剧增加，但没有取得任何有意义的进展。 有人遇到过类似的问题或可以提出潜在的原因和解决方案吗？ 谢谢！    提交人    /u/YasinRL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhv1ij/sac_training_with_stable_baselines3_halts/</guid>
      <pubDate>Thu, 19 Dec 2024 15:14:06 GMT</pubDate>
    </item>
    <item>
      <title>“MaxInfoRL：通过信息增益最大化促进强化学习中的探索”，Sukhija 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</guid>
      <pubDate>Thu, 19 Dec 2024 06:30:27 GMT</pubDate>
    </item>
    <item>
      <title>SAC 代理在我们的自定义视频游戏环境中没有学到任何东西 - 救命！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</guid>
      <pubDate>Wed, 18 Dec 2024 20:57:41 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 和线下强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</link>
      <description><![CDATA[由于 LLM 模型以某种方式进行训练，例如行为克隆，那么使用离线 RL 进行训练怎么样？ 我知道奖励设计将是一个重大挑战和可扩展性等。 你怎么看？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</guid>
      <pubDate>Wed, 18 Dec 2024 17:29:39 GMT</pubDate>
    </item>
    <item>
      <title>Isaac健身房没有联系方式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh3rgi/no_contact_information_in_isaac_gym/</link>
      <description><![CDATA[有没有人有使用 Isaac gym 的经验，我正在使用 physx 引擎来获取两个刚体的接触信息，但无法获取。当我使用 flex 引擎并将软体与相同的刚体一起使用时，我确实会得到软接触。如果有人能分享他们对此的想法，那真的很有帮助???    提交人    /u/Horror_Photo8119   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh3rgi/no_contact_information_in_isaac_gym/</guid>
      <pubDate>Wed, 18 Dec 2024 15:17:17 GMT</pubDate>
    </item>
    <item>
      <title>努力训练用于路线优化的 Dueling DQN 模型 – 需要有关学习和计算要求的建议 😢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</link>
      <description><![CDATA[我正在使用 Dueling DQN 在自定义道路网络环境中进行路线优化项目，该环境具有许多节点和不同的动作空间。但是，该模型无法正确学习 - 训练结果不一致，并且代理难以找到最佳路径。 有人有兴趣贡献吗     提交人    /u/ProfessionalType9800   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</guid>
      <pubDate>Wed, 18 Dec 2024 14:54:11 GMT</pubDate>
    </item>
    <item>
      <title>强化学习前提条件方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgzrvg/help_on_prerequisites_for_reinforcement_learning/</link>
      <description><![CDATA[大家好！ 我已经完成了控制系统硕士学位，并将于 2025 年夏季开始攻读博士学位。由于我对控制系统中的 ML/数据驱动方法感兴趣，我的研究主管要求我在正式开始攻读博士学位之前研究强化学习（作为有前途的研究领域之一）。  根据我的理解，理解强化学习的先决条件是概率和统计、微积分和线性代数（如果我错了，请随时纠正我）。我对微积分和线性代数有很好的了解，但我在本科或硕士阶段没有学过任何概率和统计课程。（请随意添加除上述先决条件之外的任何其他先决条件和学习这些先决条件的良好资源。） 有很多可用于学习概率和统计的资源，但我不知道从工程的角度来看，其中哪些对理解强化学习真正有帮助。因此，如果您能推荐任何资源（视频讲座和/或书籍等）来帮助我了解概率和统计的概念，我将不胜感激。在我开始学习强化学习之前，请告诉我是否有任何我需要了解的概率和统计具体主题。     提交人    /u/reddit_agg   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgzrvg/help_on_prerequisites_for_reinforcement_learning/</guid>
      <pubDate>Wed, 18 Dec 2024 11:41:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 训练代理进行棋盘游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgy9g3/training_agent_with_dqn_for_board_game/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgy9g3/training_agent_with_dqn_for_board_game/</guid>
      <pubDate>Wed, 18 Dec 2024 09:47:16 GMT</pubDate>
    </item>
    <item>
      <title>David Silver 示例考试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</link>
      <description><![CDATA[      大家好， 我正在查看 David Silver 网站上的练习考试，但似乎无法理解本页最后一个问题的解决方案。对于状态一的 lambda 返回，它不应该是 0.5**2 x 1 而不是 0.5 x 1。之后，我完全不知道状态 2 和 3 的返回值了。    提交人    /u/LostBandard   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</guid>
      <pubDate>Wed, 18 Dec 2024 02:29:26 GMT</pubDate>
    </item>
    <item>
      <title>强化学习工作原理的示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</link>
      <description><![CDATA[  由    /u/A-Sexy-Name  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</guid>
      <pubDate>Tue, 17 Dec 2024 21:11:18 GMT</pubDate>
    </item>
    </channel>
</rss>