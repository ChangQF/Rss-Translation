<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 18 Sep 2024 09:18:19 GMT</lastBuildDate>
    <item>
      <title>您对 AI 的参与程度处于哪个水平？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjmhpp/which_level_of_ai_engagement_are_you_at/</link>
      <description><![CDATA[AI 用户级别：在此级别，个人使用现有的 AI 工具、框架和预训练模型，并进行最少的修改。他们可能会在自己的数据集上微调这些模型，但不会改变底层架构。这可能涉及使用 OpenAI 的 GPT、Hugging Face 或 AutoML 平台等工具。示例：业务分析师、使用 AI API 的数据科学家以及将预训练模型应用于分类、回归等特定任务的领域专家。 AI 协调器级别 此级别涉及采用预先存在的 AI 模型和架构，以新颖的方式修改或组合它们以创建新系统。这些人了解模型的内部工作原理，并可以通过重新组织组件（如层、模型管道或架构）来调整它们以更好地适应特定用例或数据集。示例：有人通过集成预先训练的模块（如 GAN 或 transformer）来构建自定义 AI 管道，以完成专门的任务。 AI 架构师级别架构师设计全新的架构或修改现有架构以扩展其功能。他们专注于创建突破当前 AI 模型所能实现的界限的解决方案，解决现有方法的特定限制或低效问题。这可能包括为特定类型的数据开发新的模型类型（例如视觉 transformer）。示例：研究人员开发新的神经网络架构，如 Transformer、Capsule Networks 或新的多任务学习方法。 AI 研究员（或 AI 工程师/开发人员）级别在最高级别，这些人不仅设计新的架构，还开发新的基础算法和优化技术。他们致力于 AI 的理论进步，通过学习范式、优化和算法的突破推动该领域的发展。这可能涉及创建全新的 AI 方法，从根本上改变模型的学习方式（例如强化学习突破或新的训练算法）。示例：OpenAI、DeepMind 等机构或学术实验室的研究人员，贡献新颖的优化技术或 AI 范例。 AI 研究员（或 AI 工程师/开发人员）在突破人工智能界限方面扮演着最关键的角色。这一层次是真正的创新发生的地方——创造新的算法、架构和优化方法，从而带来该领域的真正突破。 从事这一层次的人员引入了诸如 Transformer、GAN 或新颖的强化学习方法等概念，重塑了行业并为 AI 应用开辟了新的可能性。这些研究人员专注于深厚的理论知识、实验以及开发可以影响整个领域的新模型和系统，从自然语言处理到计算机视觉等等。 正是这种参与水平创造了 AI 的未来！    提交人    /u/OptimalComplex2250   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjmhpp/which_level_of_ai_engagement_are_you_at/</guid>
      <pubDate>Wed, 18 Sep 2024 06:41:13 GMT</pubDate>
    </item>
    <item>
      <title>我目前遇到一个问题。给定一组项目，我需要选择一个子集并将其传递给黑盒，之后我将获得该值。我的目标是最大化该值，项目集包含大约 200 个项目。在这种情况下，sota 模型是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fjj8mk/i_am_currently_encountering_an_issue_given_a_set/</link>
      <description><![CDATA[  由    /u/Fast-Ad3508  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fjj8mk/i_am_currently_encountering_an_issue_given_a_set/</guid>
      <pubDate>Wed, 18 Sep 2024 03:19:42 GMT</pubDate>
    </item>
    <item>
      <title>QR-DQN 爆炸值域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj6fvy/qrdqn_exploding_value_range/</link>
      <description><![CDATA[我正在研究分布式强化学习，目前正在尝试实现 QR-DQN。 Github 中有一个直观的解释，但对环境的简短解释是代理从 (0,0,0) 开始。向“左”或“右”是随机选择的，向左会导致最左边的 0 被 -1 替换，向右会将最左边的 0 替换为 +1。每个非终止步骤的奖励为 0。一旦代理到达终点，奖励将计算为  s=(-1,-1,-1) =&gt; r=0 s=(-1,-1,1) =&gt; r=1 . . . s=(1,1,1) =&gt; r=7 请注意，QR-DQN 不会采取任何行动，它只是试图预测奖励分布。这意味着在状态 s=(0,0,0) 时，分布应在 0 到 7 之间均匀分布，在状态 s=(1,0,0) 时，分布应在 4 到 7 之间均匀分布，等等。 但是，QR-DQN 输出的分布范围从 -20,000 到 +20,000，并且似乎永远不会收敛。我很确定这是一个引导问题，但我不知道如何解决它。 代码：https://github.com/Wung8/QR-DQN/blob/main/qr_dqn_demo.ipynb    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj6fvy/qrdqn_exploding_value_range/</guid>
      <pubDate>Tue, 17 Sep 2024 18:13:40 GMT</pubDate>
    </item>
    <item>
      <title>关于在情景强化学习设置中使用演员评论家架构的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</link>
      <description><![CDATA[大家好，RL 的朋友们， 我最近遇到了一个问题，我正在将多智能体 PPO 与演员-评论家相结合应用于一个问题，由于问题的性质，我首先实施了它的一个情节版本作为初始实施。 我理解拥有评论家的优势之一是可以使用情节中估计的值来更新演员，从而无需等到情节结束时才能用奖励来更新演员。但是，如果无论如何都在情节设置中，使用评论家而不是实际奖励有什么好处吗？    提交人    /u/Ingenuity39   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj66zq/question_about_using_actor_critic_architecture_in/</guid>
      <pubDate>Tue, 17 Sep 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>用于实现 RL 以优化数学函数的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</link>
      <description><![CDATA[有人可以推荐任何资源作为 RL 实现示例来优化数学函数/测试函数吗？因为我能找到的大多数东西基本上都在 gym 环境中。但我正在寻找一个带有代码的示例，它可以对数学函数进行优化（最好使用 actor critical，但其他方法也可以）。如果有人知道这样的资源，请提出建议。提前谢谢您。    提交人    /u/anikbis17   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj2mrv/resource_for_implementation_of_rl_to_optimize_a/</guid>
      <pubDate>Tue, 17 Sep 2024 15:46:46 GMT</pubDate>
    </item>
    <item>
      <title>预订建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</link>
      <description><![CDATA[我需要什么书来进行强化学习？ 我希望书既直观又具有数学性，我能理解艰难的数学，因为我有很强的数学背景。 向我推荐一些有很好解释并且包含很好数学内容的书。    提交人    /u/Evening-Passenger311   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fj0m0p/book_advice/</guid>
      <pubDate>Tue, 17 Sep 2024 14:25:13 GMT</pubDate>
    </item>
    <item>
      <title>如何优化奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</link>
      <description><![CDATA[我一直在用强化学习训练汽车，但一直遇到奖励函数问题。我希望汽车保持较高的恒定速度，并一直使用 speed 和最近的 progress 等参数来奖励它。但是，我注意到，当仅根据速度进行奖励时，汽车有时会加速，但会立即减速，而进度似乎根本没有影响。我还奖励了其他动作，例如 all_wheel_on_track，这很有帮助，因为每次汽车偏离赛道都会受到 5 秒的惩罚。 附注：这是 aws deep racer 比赛，如果您愿意，可以在此处查看参数。    提交人    /u/KatCelest   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fit226/how_to_optimize_a_reward_function/</guid>
      <pubDate>Tue, 17 Sep 2024 07:41:31 GMT</pubDate>
    </item>
    <item>
      <title>在没有标签的情况下使用强化学习创建合成数据</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fifx0s/synthetic_data_creation_using_reinforcement/</link>
      <description><![CDATA[那么。假设我们有每周的电力数据，但我们想创建一个模型来捕捉可能导致停电的使用高峰。强化学习代理能否在一周的不同日子中创建使用分布。代理能否捕捉模式并使用模拟数据知道根据其模拟，在特定的星期四而不是星期三或星期日将会发生停电？如果没有每日数据，它将如何评估其预测？    提交人    /u/No_Refrigerator_7841   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fifx0s/synthetic_data_creation_using_reinforcement/</guid>
      <pubDate>Mon, 16 Sep 2024 21:03:57 GMT</pubDate>
    </item>
    <item>
      <title>推荐阅读因果强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</link>
      <description><![CDATA[嗨， 我的经济学背景是因果推理（据我所知，我的背景是鲁宾学派，而不是 Pearls 学派），我想了解更多关于因果 RL 的知识。我已经观看了关于因果强化学习的本教程，但我仍然不太明白它在做什么。 有推荐阅读材料吗？这篇论文是一个好的开始吗？ 此外，我目前的理解是“传统”因果推理假设因果关系，而（一些）RL 则从数据中学习它们而不做假设？这是正确的吗？ 谢谢！    提交人    /u/WinnieXi   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi7phg/recommend_reading_on_causal_rl/</guid>
      <pubDate>Mon, 16 Sep 2024 15:36:16 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的 OpenAI Gymnasium 向量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi4sus/openai_gymnasium_vector_in_observation_space/</link>
      <description><![CDATA[大家好，我在真实设备上使用 Stable Baselines3 (SB3)，并使用自定义 OpenAI Gymnasium 环境在 Python 和 Arduino 之间创建了一个接口。我想将之前的观察结果包含在我的观察空间中。目前，我的观察空间如下所示： self.high = np.array([self.maxPos, self.minDelta, self.maxVel, self.maxPow], dtype=np.float32) self.low = np.array([self.minPos, self.minDelta, self.minVel, self.minPow], dtype=np.float32) self.observation_space = space.Box(self.low, self.high, dtype=np.float32) 其中 min 和 max 值为 np.float32。我的 state 定义为： self.state = [self.ballPosition, self.ballPosition - self.desiredBallPos, self.ballVelocity, self.lastFanPower] 我想将先前位置的向量添加到我的状态中，如下所示： self.posHist = [self.stateHist[-1][0], self.stateHist[-2][0], self.stateHist[-3][0], self.stateHist[-4][0]] 然后： self.state = [self.ballPosition, self.ballPosition - self.desiredBallPos, self.ballVelocity, self.lastFanPower, self.posHist] 我应该如何更改我的self.observation_space? 问题：我应该如何修改我的 self.observation_space 以适应这些先前的位置？我想要添加此信息的原因是向网络提供有关先前状态和系统动态的数据，因为通信存在一些延迟。如果您发现此方法存在任何问题，请告诉我。我对 RL 还很陌生，仍在学习。    提交人    /u/Enroot   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi4sus/openai_gymnasium_vector_in_observation_space/</guid>
      <pubDate>Mon, 16 Sep 2024 13:35:24 GMT</pubDate>
    </item>
    <item>
      <title>需要在目标机器人环境中使用 DDPG+HER 实现 MAML 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fi190y/need_help_with_maml_implementation_with_ddpgher/</link>
      <description><![CDATA[大家好， 我正在做一个使用 DDPG+HER+MPI 实现 MAML 的项目。我使用 Tianhong Dai 的 hindsight-experience-replay 作为基础，并希望使用 Gymnasium fetch robotics 和 panda-gym 环境测试我的实现。目前。我面临一些挑战，希望得到一些建议来推动这一进程。 为了测试我的实现，我没有使用多个任务进行训练，而是首先尝试使用单个环境来检查实现是否有效。我可以通过调整 alpha 和 beta 参数来训练简单的环境，如 fetch-reach 或 panda-reach。但是，当我转而测试更复杂的任务（如推送或 pnp）时，即使使用不同的超参数变化，训练也会遇到困难。 当我尝试训练多个任务时，情况会变得更糟，例如使用 fetch-push 和 fetch-pnp 作为训练环境，同时尝试学习 fetch-slide 作为保留任务。 我知道将 MAML 与 DDPG（使用重放缓冲区）等离策略算法相结合并不常规，但我很想探索这种方法，看看这里是否有潜力。 我已经将代码上传到这里，如果有人想看看，请提供一些关于如何修复它的建议。 https://github.com/ncbdrck/maml_ddpg_her    由   提交  /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fi190y/need_help_with_maml_implementation_with_ddpgher/</guid>
      <pubDate>Mon, 16 Sep 2024 10:34:17 GMT</pubDate>
    </item>
    <item>
      <title>决策转换器和机器人学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhyzae/decision_transformer_and_robot_learning/</link>
      <description><![CDATA[有人知道任何文章或论文中使用决策变换器来解决机器人手臂操纵任务吗？    提交人    /u/Significant-Gene1539   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhyzae/decision_transformer_and_robot_learning/</guid>
      <pubDate>Mon, 16 Sep 2024 07:43:10 GMT</pubDate>
    </item>
    <item>
      <title>是否有可能用 SFT 来模仿 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhw29h/is_it_possible_to_mimic_rl_with_sft/</link>
      <description><![CDATA[（不确定这是否是询问此问题的正确论坛） 我有一个使用 OpenAI API 运行的代理，我想通过 RL 微调该代理。  但是，鉴于 OpenAI 仅提供 SFT API，我想知道是否可以执行以下操作 -   基于当前模型的样本情节（在采样期间融入探索） 计算每集的奖励（或者对于简单情况，获胜的奖励始终为 1） 对于每个获胜情节，创建监督标签（状态、动作） 从 3 开始对数据集应用微调  重复此过程几轮。  这会起作用吗？这实际上等同于为代理运行 RL 吗？     提交人    /u/WriterAccomplished65   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhw29h/is_it_possible_to_mimic_rl_with_sft/</guid>
      <pubDate>Mon, 16 Sep 2024 04:20:52 GMT</pubDate>
    </item>
    <item>
      <title>“扩散强制：下一个标记预测与全序列扩散相结合”，Chen 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fhgzk5/diffusion_forcing_nexttoken_prediction_meets/</guid>
      <pubDate>Sun, 15 Sep 2024 16:44:39 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>