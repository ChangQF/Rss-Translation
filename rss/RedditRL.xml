<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 19 Jan 2025 06:21:18 GMT</lastBuildDate>
    <item>
      <title>RL 中的优化？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4s3rt/optimization_within_rl/</link>
      <description><![CDATA[我想通过调整边界约束将 RL 应用于受约束的线性规划。LP 的形式为：最大 c&#39;v，受 Ax=0 约束，x &lt; xub。因此，我希望我的代理对 xub（连续）的元素采取行动。我将使用 x 的一些预测值通过欧拉前向方法更新环境。奖励将是每个时间步的函数值，以及该情节的一些折扣值。这可能吗？我可以为每个时间步求解 LP 吗？SAC 方法在这里有效吗？非常感谢您的指导！    提交人    /u/Tako_Poke   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4s3rt/optimization_within_rl/</guid>
      <pubDate>Sun, 19 Jan 2025 06:15:06 GMT</pubDate>
    </item>
    <item>
      <title>不清楚何时使用 RL 还是数学优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4q30v/confused_about_when_to_use_rl_vs_mathematical/</link>
      <description><![CDATA[你好 我是 RL 的新手。 我们优化的问题是库存管理和车间作业调度。 我理解 RL 可以考虑更多动态方面，并且可以在未来进行调整。但我无法将其转化为实际术语 MO 技术何时会失效？ 建模时，如何在 MO 技术和 RL 之间做出选择？ 谢谢。    提交人    /u/20231027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4q30v/confused_about_when_to_use_rl_vs_mathematical/</guid>
      <pubDate>Sun, 19 Jan 2025 04:18:45 GMT</pubDate>
    </item>
    <item>
      <title>聊天机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4ozq2/chatbots/</link>
      <description><![CDATA[您好， 在伯克利 cs285 关于强化学习的第一讲中，展示了一张聊天机器人的图片，作为强化学习功能的示例。我需要学习哪些主题才能构建遵循自定义规则的自定义聊天机器人？    提交人    /u/throwaway-alphabet-1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4ozq2/chatbots/</guid>
      <pubDate>Sun, 19 Jan 2025 03:23:33 GMT</pubDate>
    </item>
    <item>
      <title>图式网络：利用直觉物理的生成因果模型进行零样本迁移</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4m0xh/schema_networks_zeroshot_transfer_with_a/</link>
      <description><![CDATA[  由    /u/moschles  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4m0xh/schema_networks_zeroshot_transfer_with_a/</guid>
      <pubDate>Sun, 19 Jan 2025 00:46:48 GMT</pubDate>
    </item>
    <item>
      <title>RL 代理根本没有学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4ilo3/rl_agent_not_learning_at_all/</link>
      <description><![CDATA[嗨，我是 RL 的新手，只是想让我的第一个代理运行。但是，似乎我的代理什么都没学到，我真的遇到了瓶颈，不知道该怎么做。 我为高尔夫纸牌游戏编写了一个简单的脚本，玩家可以与计算机对战。我制作了一些算法计算机玩家，但我真正想做的是教 RL 代理玩游戏。 即使对抗弱计算机玩家，代理在 5M 步内也学不到任何东西。所以我认为它有初始困难，因为它甚至无法在对抗弱玩家时获得足够的奖励。 所以我添加了一个完全随机的玩家，但即使对抗那个玩家，我的代理也根本没有学到任何东西。 好吧，我认为高尔夫对于 RL 来说可能有点难，因为它有两个不同的阶段：首先，你选一张牌，其次，你打出这张牌。我重构了代码，因此代理只需处理打牌，无需处理其他任何事情。但是，经过 5M 步后，代理仍然比一个非常简单的算法更愚蠢。 我尝试过 DQN 和 PPO，两者似乎都什么都没学到。 有人能指点我，我做错了什么吗？我认为我的奖励可能有问题，或者我不知道，我是一个初学者。 如果您有时间，单阶段 RL 代理的 repo 是 https://github.com/SakuOrdrTab/golf_card_game/tree/one-phase-RL 如果您想查看代理完成两个阶段的先前尝试，它是主分支。 谢谢大家！    提交人    /u/Some_Marionberry_403   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4ilo3/rl_agent_not_learning_at_all/</guid>
      <pubDate>Sat, 18 Jan 2025 22:00:51 GMT</pubDate>
    </item>
    <item>
      <title>如何将 RLLib Agent 与 Isaac Lab 一起使用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4hzq9/how_to_use_rllib_agent_with_isaac_lab/</link>
      <description><![CDATA[嘿，我正在做一个小项目，我想使用 rllib 中的算法在 IsaacLab 内部进行训练。我无法让它工作，因为我的经验有限，而且几乎没有关于这个组合的信息。 最大的问题是，rllib 需要一个 gym.env，但 IsaacLab 使用 ManagerBasedRLEnv ，它实际上是基于 gym.env。但我无法让转换工作。有什么想法吗？ 还有一件我还没有完全搞清楚的事情，我以为我让环境控制代理，但似乎代理需要环境作为输入。这是否也意味着在典型的 RL 项目中，代理通常会控制环境？提前致谢！    提交人    /u/mVIIIeus   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4hzq9/how_to_use_rllib_agent_with_isaac_lab/</guid>
      <pubDate>Sat, 18 Jan 2025 21:32:38 GMT</pubDate>
    </item>
    <item>
      <title>支持自定义 ResNet 实现的 RL 库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i48ulh/rl_library_that_supports_custom_resnet/</link>
      <description><![CDATA[我正在训练一个模型，以便在自定义 Gymnasium 环境中工作，使用 tf_agents 进行训练。不幸的是，tf_agents 似乎无法处理任何非直接的 NN。我可以处理多个输入，但一旦它们通过卷积层（必须是直接的），我只能一次合并它们，并且自定义选项有限。我当然不能使用 ResNet 块来尝试获得更好的结果。 是否有一个库具有与 tf_agents 相同的 RL 管理类型，可以处理这些更复杂的 NN 方案？我宁愿使用依赖于 Keras/Tensorflow 的东西，但如果这是除了自己构建之外的唯一选择，我可能会被说服切换到 PyTorch。显然，我宁愿使用现成的东西，而不是自己动手。   提交者    /u/Usual_Macaron8477   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i48ulh/rl_library_that_supports_custom_resnet/</guid>
      <pubDate>Sat, 18 Jan 2025 14:37:27 GMT</pubDate>
    </item>
    <item>
      <title>关于 TD3 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i3twt1/question_about_td3/</link>
      <description><![CDATA[在 TD3 的原始实现中，更新 q 函数时，您会使用 TD 目标的目标策略。但是，更新策略时，您使用的是 q 函数，而不是目标 q 函数。这是为什么呢？    提交人    /u/nyesslord   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i3twt1/question_about_td3/</guid>
      <pubDate>Fri, 17 Jan 2025 23:29:13 GMT</pubDate>
    </item>
    <item>
      <title>[P] 构建强化学习代理来玩《塞尔达传说》</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i3t4nt/p_building_an_reinforcement_learning_agent_to/</link>
      <description><![CDATA[  由    /u/DarkAutumn  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i3t4nt/p_building_an_reinforcement_learning_agent_to/</guid>
      <pubDate>Fri, 17 Jan 2025 22:52:32 GMT</pubDate>
    </item>
    <item>
      <title>当前最先进的连续动作空间方法是什么？（相当于 2024 个 SAC）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i3l41q/what_is_the_current_state_of_the_art_method_for/</link>
      <description><![CDATA[大家好！ 我最后一次深入研究强化学习是使用 SAC（软演员评论家：https://arxiv.org/abs/1801.01290）。当时它是​​ q 学习的“最先进”方法，其中动作空间可以是连续的。 我已经关注了 3 到 4 年，目前最先进的等效方法（和论文）是什么？ 谢谢！    提交人    /u/creeky123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i3l41q/what_is_the_current_state_of_the_art_method_for/</guid>
      <pubDate>Fri, 17 Jan 2025 17:04:19 GMT</pubDate>
    </item>
    <item>
      <title>创建/包装移动机器人环境时的最佳实践？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i3c2ov/best_practices_when_creatingwrapping_mobile_robot/</link>
      <description><![CDATA[我目前正在使用 HoloOcean 模拟器在海洋机器人环境中实现 rl。我想在他们的模拟器之上构建一个自定义环境，并在不同的框架中实现观察和操作（例如，相对于移位/旋转的世界框架的观察）。 是否有专门针对移动机器人/无人机构建和包装环境的资源/教程？    提交人    /u/Electric-Diver   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i3c2ov/best_practices_when_creatingwrapping_mobile_robot/</guid>
      <pubDate>Fri, 17 Jan 2025 08:42:43 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以学习模仿学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i3bm39/where_i_can_learn_imitation_learning/</link>
      <description><![CDATA[大家好， 我对强化学习和所有算法（包括 SAC、DDPG、DQN 等）都有很好的了解。我正在寻找一些模仿学习方面的指导，有人可以帮助我学习这个吗？    提交人    /u/Dizzy-Importance9208   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i3bm39/where_i_can_learn_imitation_learning/</guid>
      <pubDate>Fri, 17 Jan 2025 08:06:35 GMT</pubDate>
    </item>
    <item>
      <title>游戏相关研究方向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i34tcn/game_related_research_directions/</link>
      <description><![CDATA[我是一名博士生，主要研究与法学硕士相关的主题。然而，我一直觉得这些与游戏相关的项目（Atari、AlphaGo）非常酷，尽管它们似乎在大型实验室中已经失宠了。我想知道在有限的计算预算下，是否还有一些相关的方向可以追求。     提交人    /u/Alternative-Gain335   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i34tcn/game_related_research_directions/</guid>
      <pubDate>Fri, 17 Jan 2025 01:18:32 GMT</pubDate>
    </item>
    <item>
      <title>使用 vanilla 策略梯度法在连续动作空间上训练代理，不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i2v24h/training_agent_using_vanilla_poilcy_gradient/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i2v24h/training_agent_using_vanilla_poilcy_gradient/</guid>
      <pubDate>Thu, 16 Jan 2025 18:03:23 GMT</pubDate>
    </item>
    <item>
      <title>我构建了一个使用 Q-Learning 进行学习和适应的井字游戏 AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i2nkag/i_built_a_tic_tac_toe_ai_that_learns_and_adapts/</link>
      <description><![CDATA[我想展示并可能获得我为其制作视频的小项目的反馈，所以请告诉我您的想法！ https://www.youtube.com/watch?v=agyB8opESwA    提交人    /u/Loud-Cherry8396   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i2nkag/i_built_a_tic_tac_toe_ai_that_learns_and_adapts/</guid>
      <pubDate>Thu, 16 Jan 2025 12:13:13 GMT</pubDate>
    </item>
    </channel>
</rss>