<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 24 Jan 2025 21:14:38 GMT</lastBuildDate>
    <item>
      <title>如何确定扑克锦标赛中的最佳经纪人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</link>
      <description><![CDATA[我目前正在开展一个项目，该项目旨在确定哪种深度强化学习算法最适合复杂环境，例如无限注德州扑克。我正在使用 Tianshou 制作代理和 PettingZoo 环境。我已经完成了项目的这一部分，现在我必须确定哪个代理是最好的。我让每个代理相互对战了 30,000 场，并收集了大量数据。 起初，我认为赢得最多筹码的玩家应该是赢家，但这并不公平，因为一名玩家在与最弱的玩家之一的比赛中赢得了很多筹码，并输给了所有其他玩家，但这仍然使他成为赢得最多筹码的赢家。然后我考虑了 ELO 评级，但这也行不通，因为如果玩家赢得的钱很少，那么获胜就不重要了。 在其他游戏中最常用的 2 种情况的组合是 chip_won_by_A / (chips_won_by_A + chip_won_by_B)，但这种组合也行不通，因为这是一个零和游戏环境，chips_won_by_A = -chips_won_by_B，结果除以零。你对这类问题还有其他解决方案吗？我认为使用他们本可以赢得的筹码数量中赢得的筹码百分比可能是个好主意？欢迎提供任何帮助！    提交人    /u/komensalizam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</guid>
      <pubDate>Fri, 24 Jan 2025 17:42:05 GMT</pubDate>
    </item>
    <item>
      <title>策略迭代中的策略评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</link>
      <description><![CDATA[      在 Sutton 的书中，策略评估 (4.5) 是 pi(s,a) * q(s,a) 的总和。但是，当我们在策略迭代过程中使用策略评估时（图 4.3），为什么我们不需要对所有动作进行求和，而只需要对 pi(s) 进行评估呢？ https://preview.redd.it/5vo75evilyee1.png?width=1030&amp;format=png&amp;auto=webp&amp;s=77af1304d549008b8c2e24c9cd8dff034519acae    submitted by    /u/lalalagay   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</guid>
      <pubDate>Fri, 24 Jan 2025 15:22:14 GMT</pubDate>
    </item>
    <item>
      <title>仍然不太漂亮，但奖励函数略好一些</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</link>
      <description><![CDATA[       由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</guid>
      <pubDate>Fri, 24 Jan 2025 15:15:20 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Shadow Dextrous 的手在 pybullet 中抓取 3D 杯子模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</link>
      <description><![CDATA[你好。我正在尝试使用 PyBullet 来模拟假手抓握。我使用影子手 urdf 作为我的手，一个杯子的 3d 模型。我正在努力实现影子手抓取杯子。 我希望最终使用强化学习来优化对不同尺寸杯子的抓取，但我需要先运行没有任何 AI 的 Python 脚本，这样我才能有一个基线来与 RL 模型进行比较。有人知道可以帮助我的资源吗？提前谢谢了。    提交人    /u/Flamesword200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</guid>
      <pubDate>Fri, 24 Jan 2025 12:50:18 GMT</pubDate>
    </item>
    <item>
      <title>我确信从我的状态向量中排除可以通过使用其他信息来计算的维度并不是坏事......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8nazy/im_convinced_its_not_bad_to_disclude_dimensions/</link>
      <description><![CDATA[在我的游戏中，有 5 个维度代表棋盘的宝石供应。但是，这个宝石供应只是两个玩家宝石的总和，它们都处于该状态。我需要包括这个吗？ 核心问题：如果它不改变状态捕获的信息，它会增加复杂性吗？我要添加的 5 个维度将与其他两个维度的总和完美相关。当然，这更复杂，但我不确定相对于它必须学习的所有东西来说有多复杂。    提交人    /u/Breck_Emert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8nazy/im_convinced_its_not_bad_to_disclude_dimensions/</guid>
      <pubDate>Fri, 24 Jan 2025 04:29:14 GMT</pubDate>
    </item>
    <item>
      <title>关于强盗贪婪策略的新手问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8n57l/noob_question_about_greedy_strategy_on_bandits/</link>
      <description><![CDATA[考虑 10 臂老虎机问题，从每个动作的初始奖励估计 0 开始。假设代理尝试的第一个动作的奖励为正。该动作的平均奖励的真实值也为正。还假设此特定动作的奖励的“正态分布”几乎完全为正（因此，从此动作获得负奖励的可能性非常低）。 贪婪策略是否会探索任何其他动作？    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8n57l/noob_question_about_greedy_strategy_on_bandits/</guid>
      <pubDate>Fri, 24 Jan 2025 04:20:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么要对卷出缓冲区数据进行混排？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</link>
      <description><![CDATA[在 SB3 的循环缓冲区文件 (https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/recurrent/buffers.py) 中，第 182 行表示要在保留序列的同时对数据进行混洗，代码会在随机点拆分数据，交换每个拆分，然后将其重新连接在一起。  我的问题是，为什么这对于混洗来说已经足够好了，但我们为什么要首先对推出的数据进行混洗呢？   由    /u/AUser213  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</guid>
      <pubDate>Thu, 23 Jan 2025 23:31:30 GMT</pubDate>
    </item>
    <item>
      <title>IsaacSim 人形机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</link>
      <description><![CDATA[我需要一些帮助在 IsaacSim 中构建人形机器人演示，但除了开箱即用的人形机器人 (H1) 之外没有其他可用的东西，有人对 Neo、Sanctuary 等机器人的人形机器人政策有任何线索吗    提交人    /u/sohaib_01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</guid>
      <pubDate>Thu, 23 Jan 2025 21:18:29 GMT</pubDate>
    </item>
    <item>
      <title>aiXplain 的 Evolver：通过自主优化彻底改变代理 AI 系统 🚀</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i899m4/aixplains_evolver_revolutionizing_agentic_ai/</link>
      <description><![CDATA[嗨，RL 社区！👋 我们都知道 Agentic AI 系统在自动化流程和增强各行业决策方面具有多么大的变革性。但问题是：代理角色、任务和工作流程的手动微调一直是一个主要障碍。aiXplain 的 Evolver – 我们正在申请专利的完全自主框架，旨在改变游戏规则。 💡 aiXplain 的 Evolver 是一款下一代工具，它：  🔄 自主优化工作流程：通过自动微调 Agentic AI 系统，无需人工干预。 📈 利用 LLM 驱动的反馈循环：使用高级语言模型评估输出、提供反馈并推动持续改进。 🚀 提高效率和可扩展性：比以往更快地实现 AI 系统的最佳配置。  🌟 为什么重要 我们已在多个领域应用了 Evolver，并看到了令人惊叹的结果。以下是一些亮点： 1️⃣ 市场研究：市场分析师等专业角色提高了准确性并使策略与趋势保持一致。 2️⃣ 医疗保健 AI：提高了法规遵从性和可解释性，以更好地吸引患者。 3️⃣ 职业转型：帮助软件工程师以明确的目标和量身定制的专业知识转向 AI 角色。 4️⃣ 供应链外展：通过高级分析优化电子商务解决方案的外展策略。 5️⃣ LinkedIn 内容创建：创建以受众为中心的帖子，推动对 AI 趋势的参与。 6️⃣ 药物发现：为制药公司提供与利益相关者一致的见解。 7️⃣ EdTech 潜在客户生成：通过个性化的学习见解提高潜在客户质量。 每个案例研究都展示了由 Evolver 提供支持的专业角色和持续改进如何带来更高的评估分数和更好的结果。 📚 对技术细节感到好奇吗？在 Arxiv 上查看：通过迭代细化和 LLM 驱动的反馈循环自主优化代理 AI 解决方案的多 AI 代理系统 🔍 你怎么看？ 您如何看待此类工具塑造 AI 工作流程的未来？您认为 Evolver 可以在哪些行业或特定用例中发挥巨大作用？期待听到您的想法。😊    提交人    /u/k_yuksel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i899m4/aixplains_evolver_revolutionizing_agentic_ai/</guid>
      <pubDate>Thu, 23 Jan 2025 17:50:02 GMT</pubDate>
    </item>
    <item>
      <title>关于井字游戏中的贝尔曼方程。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i86uq1/about_bellman_equation_in_tic_tac_toe_game/</link>
      <description><![CDATA[一般来说，贝尔曼方程是 target_Q = Q(state, action) + gamma * Q(next_state, action) 但是，我很好奇我们是否应该使用 -gamma 而不是 gamma，因为下一个玩家是对手。如果我们添加其最大 q 值，我认为这没有意义，因为我们将对手的最大 q 值添加到此回合的 q 值中。  但我在互联网上找到了很多代码，他们会使用 target_Q = Q(state, action) + gamma * Q(next_state, action) 而不是 target_Q = Q(state, action) - gamma * Q(next_state, action)。为什么？    提交人    /u/Upstairs-Lead-2601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i86uq1/about_bellman_equation_in_tic_tac_toe_game/</guid>
      <pubDate>Thu, 23 Jan 2025 16:09:51 GMT</pubDate>
    </item>
    <item>
      <title>需要无人机模拟环境方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i820dg/need_some_help_with_simulation_environments_for/</link>
      <description><![CDATA[大家好，我目前正在模拟基于视觉的 SLAM 设置，用于在 GPS 拒绝环境中模拟 UAV。这意味着我计划使用仅接受两个传感器输入的 SLAM 算法：相机和 IMU。我需要帮助为这个项目选择正确的模拟环境。环境必须具有适用于相机和 IMU 的良好传感器模型，并且 3D 世界必须尽可能接近现实。我排除了带有 UE4 设置的 Airsim，因为 Microsoft 已存档 Airsim，并且不支持 UE5。当我尝试 UE4 时，我找不到要导入的 3D 世界，因为 UE 已升级其市场。 任何有关模拟环境的建议以及教程链接都将非常有帮助！此外，如果有人知道如何让 UE4 适用于这种应用程序，即使是这样也欢迎！    提交人    /u/techgeek1216   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i820dg/need_some_help_with_simulation_environments_for/</guid>
      <pubDate>Thu, 23 Jan 2025 12:15:38 GMT</pubDate>
    </item>
    <item>
      <title>民意调查：视频游戏 RL 的最佳框架？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7zxga/poll_best_frameworks_for_video_game_rl/</link>
      <description><![CDATA[各位强化学习老师们，大家好！您知道或使用哪些工具在现代闭源视频游戏上进行强化学习？我说的强化学习纯粹是从视频帧开始的，无法访问内部游戏状态。您是否使用任何特定的策略和算法来解决昂贵且缓慢的数据收集问题？是否有任何特定的技术适用于 FPS、ARPG 等游戏类型？如何使用导航菜单处理级别之间的视觉差异？用于模拟游戏手柄和键盘的库？ 我认为这对于业余项目来说是一个非常有趣的话题，我已经看到了一些相关的帖子。对这些方法非常好奇。    提交人    /u/mjolk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7zxga/poll_best_frameworks_for_video_game_rl/</guid>
      <pubDate>Thu, 23 Jan 2025 09:51:08 GMT</pubDate>
    </item>
    <item>
      <title>乐观的初始值如何鼓励探索？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7xig0/how_do_optimistic_initial_values_encourage/</link>
      <description><![CDATA[我正在研究（更新的）Sutton&amp;Barto 的书。 在 2.6 中，它说初始估计 +5 过于乐观。但这种乐观鼓励行动价值方法进行探索....即使一直选择贪婪动作，系统也会进行大量探索 这本书只讨论了一个常数 epsilon，其中以恒定概率选择随机动作。 所以，我不太明白乐观的 Q1 值和探索之间的关系。有人可以用简单的术语解释一下吗？    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7xig0/how_do_optimistic_initial_values_encourage/</guid>
      <pubDate>Thu, 23 Jan 2025 06:44:25 GMT</pubDate>
    </item>
    <item>
      <title>对于嘈杂观察环境有什么建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7rlhz/suggestions_for_noisy_observation_environments/</link>
      <description><![CDATA[嗨，我正在探索具有噪声观测的 RL。我在 OpenAI Gym Atari 中向像素添加了高斯噪声，但感觉太简单了。 对环境或更逼真的噪声模型有什么建议吗？有关高级噪声（例如遮挡、结构化噪声）或相关基准的提示将不胜感激。谢谢！    提交人    /u/AdministrativeCar545   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7rlhz/suggestions_for_noisy_observation_environments/</guid>
      <pubDate>Thu, 23 Jan 2025 01:14:05 GMT</pubDate>
    </item>
    <item>
      <title>这就是“糟糕”的奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</link>
      <description><![CDATA[        由    /u/goncalogordo 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</guid>
      <pubDate>Wed, 22 Jan 2025 19:48:44 GMT</pubDate>
    </item>
    </channel>
</rss>