<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 27 Sep 2024 12:33:21 GMT</lastBuildDate>
    <item>
      <title>离线强化学习中的规范奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fqfmnh/norm_rewards_in_offline_rl/</link>
      <description><![CDATA[我正在离线 RL 中开展一个项目。我正在尝试实现一些离线 RL 算法。但是，在离线 RL 中，结果通常通过规范化来报告。我不知道这是什么意思。这些奖励是如何计算的？他们是否使用专家数据奖励来规范化或什么。 谢谢你的帮助。    提交人    /u/Regular_Average_4169   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fqfmnh/norm_rewards_in_offline_rl/</guid>
      <pubDate>Fri, 27 Sep 2024 04:26:23 GMT</pubDate>
    </item>
    <item>
      <title>将强化学习与模型预测控制相结合，实现 HEMS</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fq66pg/merging_reinforcement_learning_and_model/</link>
      <description><![CDATA[大家好， 我正在做一个大学项目，主题与标题中描述的主题有关。HEMS = 家庭能源管理系统。 我正在考虑如何合并 RL 和 MPC 以利用它们的优势。我的主管希望我特别关注样本效率。由于我是这个主题的新手，我阅读了很多论文，但似乎不明白什么标准对我来说很重要，以及哪些算法符合该标准。 你会如何处理这个问题？ Br    提交人    /u/Bubi_Bums   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fq66pg/merging_reinforcement_learning_and_model/</guid>
      <pubDate>Thu, 26 Sep 2024 20:32:23 GMT</pubDate>
    </item>
    <item>
      <title>使用双手机器人探索钉插入的精度：使用 ACT 模型的实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fq436q/exploring_precision_with_peginsertion_using/</link>
      <description><![CDATA[        提交人    /u/Trossen_Robotics   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fq436q/exploring_precision_with_peginsertion_using/</guid>
      <pubDate>Thu, 26 Sep 2024 19:03:07 GMT</pubDate>
    </item>
    <item>
      <title>矩阵运算寻找 MDP 的最优解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpzgty/matrix_operations_to_find_the_optimal_solution_of/</link>
      <description><![CDATA[大家好。 我编写了一个程序来计算玩在线游戏的最佳动作序列，该程序可以简化为一个 MDP，其转换矩阵 T 形状为 [A, S, S]，奖励矩阵形状为 [S, A]。我还有一个形状为 [S, A] 的策略。 我现在正在应用策略迭代来获得 MDP 的解决方案：https://en.wikipedia.org/wiki/Markov_decision_process#Algorithms 因此，算法的一部分是计算与策略相关的转换概率矩阵，以将其简化为 [S, S] 矩阵。 我显然可以使用双重嵌套 for 循环通过元素操作来做到这一点，但我想知道是否有更优雅的矢量化解决方案。我一直在想这个问题，但也许是因为我学代数太久了，真的找不到解决办法。 我得到了一个丑陋的解决方案，这让我很不开心…… np.sum((np.diag(P.T.reshape(-1)) @ T.reshape(-1, nStates)).reshape(T.shape), axis=0)     提交人    /u/oruiog   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpzgty/matrix_operations_to_find_the_optimal_solution_of/</guid>
      <pubDate>Thu, 26 Sep 2024 15:50:07 GMT</pubDate>
    </item>
    <item>
      <title>2D 在线装箱不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpxl4a/2d_online_bin_packing_not_converging/</link>
      <description><![CDATA[      嗨！我正在研究 2D 约束在线装箱，但面临模型无法收敛的一些问题。 代理使用 PPO，并因最大化空间效率和优先考虑边缘位置而获得奖励，而对次优内部位置则施加惩罚。启发式策略指导最佳箱子放置，重点是先填充周长，然后再考虑内部网格，从而增强代理在装箱场景中的决策能力。该模型目前有效（有时），但我认为调整（尽管我确实尝试过）和对动作空间的探索不足可能是问题所在。我也可以分享我的代码！ https://preview.redd.it/g9gokw56z5rd1.png?width=993&amp;format=png&amp;auto=webp&amp;s=b64bcd8b1ea97dbf098c0a56edb52d0ff36ef5d4 上一篇文章：https://www.reddit.com/r/MachineLearning/comments/1fl69fa/p_2d_bin_packing_problem/    由   提交  /u/Sea-Hovercraft4777   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpxl4a/2d_online_bin_packing_not_converging/</guid>
      <pubDate>Thu, 26 Sep 2024 14:30:50 GMT</pubDate>
    </item>
    <item>
      <title>... 天网？集中式还是分散式聊天GPT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpf2of/skynet_centralized_or_decentralized_chatgpt/</link>
      <description><![CDATA[矿工试图解密区块链哈希中的随机数。 为什么不建立一个可以推翻科技巨头的庞大模型来赚点钱呢。 并行计算不是一个新领域。需要虚拟化任务，使其 100% 独立于硬件。 如果我们采用最强大的模型，例如决策变压器。为每个任务建立一个机器池，优先考虑最强大的机器，这样延迟就会很低，如果一台机器坏了，其他机器可以完成这项工作。这是 N 个并行作业。它甚至可以是三维的 - 池、并行任务、同时给出的并行系列并行任务。 人们可以考虑去中心化。如果我们添加具有一致哈希的区块链技术。但是与集中式...天网相比，并发性和能源效率较低。 普通人，我可以梦想利用我的旧台式电脑赚点钱吗....    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpf2of/skynet_centralized_or_decentralized_chatgpt/</guid>
      <pubDate>Wed, 25 Sep 2024 20:55:21 GMT</pubDate>
    </item>
    <item>
      <title>乐高遇见人工智能：BricksRL 被 NeurIPS 2024 接受！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/</link>
      <description><![CDATA[      我们很高兴地告诉大家，我们关于 BricksRL 的论文已被 NeurIPS 2024 接受为焦点论文，BricksRL 是一个 RL 算法库，可以在价格实惠的定制 LEGO 机器人上进行训练和部署！ 随着人工智能和机器学习继续掀起波澜，我们认为向社区提供可靠且价格合理的教育工具至关重要。并非每个人都可以使用数百个 GPU，而且了解 ML 在实践中的工作原理可能具有挑战性。 这就是我们一直在研究 BricksRL 的原因，这是庞培法布拉大学和 PyTorch 之间的合作项目。我们的目标是为人们提供一种有趣且引人入胜的方式来学习 AI、ML、机器人技术和 PyTorch，同时保持高标准的正确性和稳健性。 BricksRL 基于 Pybricks，可以部署在许多不同的 LEGO 集线器上。我们希望它能让世界各地的实验室以可承受的价格制作原型，而无需昂贵的机器人。 查看我们的网站：https://bricksrl.github.io/ProjectPage/ 该库在 GitHub 上根据 MIT 许可开源：https://github.com/BricksRL/bricksrl/ 阅读我们的论文：https://arxiv.org/abs/2406.17490 观看机器人的实际操作：https://www.youtube.com/watch?v=k_Vb30ZSatk&amp;t=10s 我们正在开展一些令人兴奋的后续项目，敬请期待！ 温哥华见 https://preview.redd.it/1ghfs9t9l0rd1.jpg?width=2006&amp;format=pjpg&amp;auto=webp&amp;s=868867adcd52825bd4ee719513a454527d017307    提交人    /u/AdCool8270   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/</guid>
      <pubDate>Wed, 25 Sep 2024 20:23:37 GMT</pubDate>
    </item>
    <item>
      <title>了解机器学习从业者在构建隐私保护模型方面面临的挑战和需求</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpaxuh/understanding_machine_learning_practitioners/</link>
      <description><![CDATA[您好 我们是匹兹堡大学的一支研究团队。我们正在研究 ML 开发人员在构建隐私保护模型时遇到的问题、挑战和需求。如果您从事 ML 产品或服务工作，请帮助我们回答以下问卷：https://pitt.co1.qualtrics.com/jfe/form/SV_6myrE7Xf8W35Dv0 谢谢！    提交人    /u/MaryAD_24   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpaxuh/understanding_machine_learning_practitioners/</guid>
      <pubDate>Wed, 25 Sep 2024 18:00:45 GMT</pubDate>
    </item>
    <item>
      <title>我上一篇关于最佳资源受到喜爱的文章。在这里，我分享了一条详细的路径，一步一步引导你顺利进入 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fp665f/my_last_post_on_best_resources_are_loved_here_i/</link>
      <description><![CDATA[    /u/Fair_Detective_6568   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fp665f/my_last_post_on_best_resources_are_loved_here_i/</guid>
      <pubDate>Wed, 25 Sep 2024 14:44:22 GMT</pubDate>
    </item>
    <item>
      <title>交易的策略梯度，窦性例子</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fp2dlb/policy_gradient_for_trading_toy_example_on_sinus/</link>
      <description><![CDATA[        提交人    /u/Grouchy_Purpose8206   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fp2dlb/policy_gradient_for_trading_toy_example_on_sinus/</guid>
      <pubDate>Wed, 25 Sep 2024 11:44:37 GMT</pubDate>
    </item>
    <item>
      <title>MARL 在代理之间共享训练示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fow21c/marl_with_sharing_of_training_examples_between/</link>
      <description><![CDATA[您好， 我是一名学生，刚刚开始对 RL 和 MARL 进行一些初步研究，我正在尝试适应不同的子领域。我所想象的场景具有以下特点：  训练是分散的；环境仅部分可观察；并且代理具有不相同的奖励 代理在训练期间相互通信 代理间通信包括（选择性）共享训练示例  这样的场景的一个例子可能是正在学习个性化推荐系统的移动应用程序网络，但在隐私敏感区域中，因此数据只能根据用户的隐私偏好进行共享，并且只能以用户可审核的方式共享（因此联合学习，直接共享模型参数或发明的语言不会这样做）。 如果这个问题有点模糊或格式不正确，请原谅。我真的只是在寻找一些可以帮助我进行研究的关键字或调查论文链接。 编辑： 我发现https://arxiv.org/pdf/2311.00865听起来就像我在说什么。    提交人    /u/ConceptOk2393   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fow21c/marl_with_sharing_of_training_examples_between/</guid>
      <pubDate>Wed, 25 Sep 2024 04:19:08 GMT</pubDate>
    </item>
    <item>
      <title>具有 LSTM、CNN、FC 层、图形注意力网络的多智能体强化学习 A2C</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fofr68/multi_agent_reinforcement_learning_a2c_with_lstm/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fofr68/multi_agent_reinforcement_learning_a2c_with_lstm/</guid>
      <pubDate>Tue, 24 Sep 2024 15:48:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 SB3 DQN 上的 LunarLander 性能不佳？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1foe4p8/why_does_my_lunarlander_on_sb3_dqn_not_perform/</link>
      <description><![CDATA[我从这里获得了最佳超参数。因此，我期望算法能够达到最佳性能，即在训练结束时频繁获得 200 的情景奖励。但这并没有发生。  我已在此处附加我的代码 - https://pastecode.io/s/evo1c0ku 有人可以帮忙吗？    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1foe4p8/why_does_my_lunarlander_on_sb3_dqn_not_perform/</guid>
      <pubDate>Tue, 24 Sep 2024 14:40:50 GMT</pubDate>
    </item>
    <item>
      <title>我正在学习 RL，并且取得了很大的进步。我总结了我认为非常有用的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fobu5v/im_learning_rl_and_making_good_progress_i/</link>
      <description><![CDATA[        由    /u/Fair_Detective_6568   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fobu5v/im_learning_rl_and_making_good_progress_i/</guid>
      <pubDate>Tue, 24 Sep 2024 12:57:34 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>