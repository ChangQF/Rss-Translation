<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 01 Jul 2024 03:17:39 GMT</lastBuildDate>
    <item>
      <title>RL 的用户，您希望 RL 做什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsiiao/users_of_rl_what_do_you_wish_rl_could_do/</link>
      <description><![CDATA[对于那些致力于 RL 应用的人来说，您的最终目标是什么？您希望 RL 能做什么？什么用例推动了您的工作？    提交人    /u/Obsesdian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsiiao/users_of_rl_what_do_you_wish_rl_could_do/</guid>
      <pubDate>Mon, 01 Jul 2024 02:47:47 GMT</pubDate>
    </item>
    <item>
      <title>制作了一个软演员评论家模型。但结果很糟糕，我哪里犯了错误？（pytorch，月球着陆器）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsi26i/made_a_soft_actor_critic_model_but_bad_result/</link>
      <description><![CDATA[   https://preview.redd.it/c3231tj2it9d1.png?width=855&amp;format=png&amp;auto=webp&amp;s=f47a4fd623a4fd884e55f539c863c954a0706d8a 这是我的代码https://github.com/lch3942/test &quot;训练开始了，但似乎学习过程没有正常工作。我遵循了这个伪代码，但我不确定它是否被正确实现，结果并不好。你能帮我找出我哪里出错了吗？谢谢你阅读我的帖子。     提交人    /u/Delicious_Bowl1645   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsi26i/made_a_soft_actor_critic_model_but_bad_result/</guid>
      <pubDate>Mon, 01 Jul 2024 02:23:30 GMT</pubDate>
    </item>
    <item>
      <title>列出你遇到的最佳 LLM 驱动的多智能体强化学习论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsf5y9/name_best_llmpowered_multi_agent_rl_papers_you/</link>
      <description><![CDATA[大家好！顾名思义，我正在对多智能体强化学习中 LLM 的最佳用途进行广泛的文献综述，特别是如果涉及学习/训练。在上下文中，学习也很重要。 如果有任何想法，请说出它们的名字，谢谢！    提交人    /u/miladink   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsf5y9/name_best_llmpowered_multi_agent_rl_papers_you/</guid>
      <pubDate>Sun, 30 Jun 2024 23:52:10 GMT</pubDate>
    </item>
    <item>
      <title>逻辑看似正确，但代理商却没有蜂拥而至</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds7wuj/seemingly_correct_logic_but_agents_not_flocking/</link>
      <description><![CDATA[我有一个boid 群集环境，并使用 PPO。 我的奖励函数是让它们群集。我有两个奖励，即对齐和凝聚力。从逻辑上讲，它们似乎没有错。最初我在凝聚力函数中使用了线性函数而不是指数函数，但结果相同。  邻域半径违规会导致 -100 奖励。 def reward(self, agent, neighbour_velocities, neighbour_positions): CohesionReward = 0 AlignmentReward = 0 total_reward = 0 outofflock = False midpoint = (SimulationVariables[&quot;SafetyRadius&quot;] + SimulationVariables[&quot;NeighborhoodRadius&quot;]) / 2 if len(neighbor_positions) &gt; 0: for neighbour_position in neighbour_positions: distance = np.linalg.norm(agent.position - neighbour_position) if distance &lt;= SimulationVariables[&quot;SafetyRadius&quot;]: CohesionReward -= 10 elif SimulationVariables[&quot;SafetyRadius&quot;] &lt; distance &lt;中点： 比率 = (距离 - SimulationVariables[&quot;SafetyRadius&quot;]) / (中点 - SimulationVariables[&quot;SafetyRadius&quot;]) CohesionReward += 10 * np.exp(比率 - 1) elif 中点 &lt;= 距离 &lt; SimulationVariables[&quot;NeighborhoodRadius&quot;]: 比率 = (距离 - 中点) / (SimulationVariables[&quot;NeighborhoodRadius&quot;] - 中点) CohesionReward += 10 * np.exp(-ratio) average_velocity = np.mean(neighbor_velocities, axis=0) dot_product = np.dot(average_velocity, agent.velocity) norm_product = np.linalg.norm(average_velocity) * np.linalg.norm(agent.velocity) 如果 norm_product == 0: cos_angle = 1.0 否则: cos_angle = dot_product / norm_product cos_angle = np.clip(cos_angle, -1.0, 1.0) orientation_diff = np.arccos(cos_angle) alignment = 1 - (orientation_diff / np.pi) AlignmentReward = -20 * alignment + 10 else: CohesionReward -= 100 outofflock = True # 引入基线奖励 total_reward = CohesionReward + AlignmentReward return total_reward, outofflock     提交人    /u/OccupyFood101   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds7wuj/seemingly_correct_logic_but_agents_not_flocking/</guid>
      <pubDate>Sun, 30 Jun 2024 18:23:47 GMT</pubDate>
    </item>
    <item>
      <title>“通过算法蒸馏实现上下文强化学习”，Laskin 等人 2022 年 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds5m71/incontext_reinforcement_learning_with_algorithm/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds5m71/incontext_reinforcement_learning_with_algorithm/</guid>
      <pubDate>Sun, 30 Jun 2024 16:39:44 GMT</pubDate>
    </item>
    <item>
      <title>“通过指令预测提高长视界模仿能力”，Hejna 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds5f7w/improving_longhorizon_imitation_through/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds5f7w/improving_longhorizon_imitation_through/</guid>
      <pubDate>Sun, 30 Jun 2024 16:31:02 GMT</pubDate>
    </item>
    <item>
      <title>《奥赛罗被破解了》泷泽2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds2f1o/othello_is_solved_takizawa_2023/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds2f1o/othello_is_solved_takizawa_2023/</guid>
      <pubDate>Sun, 30 Jun 2024 14:13:54 GMT</pubDate>
    </item>
    <item>
      <title>Q-Learning 的问题（代理无法正确学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1drm7w0/problem_with_qlearning_agent_is_not_learning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1drm7w0/problem_with_qlearning_agent_is_not_learning/</guid>
      <pubDate>Sat, 29 Jun 2024 22:01:37 GMT</pubDate>
    </item>
    <item>
      <title>ppo 损失的导数是多少，例如 dL/dA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1drk2la/what_is_the_derivative_of_the_loss_in_ppo_eg_dlda/</link>
      <description><![CDATA[因此，我正在为体育馆制作自己的 PPO 实现，并且我让所有损失计算正常工作，现在它正在进行梯度更新。我的优化完全正常工作，因为我已经使用正常的监督学习多次使其工作，但我得到了一个非常愚蠢的奇怪认识。由于 PPO 对损失做了一些事情并返回了一个标量，我不能只反向传播它，因为 NN 输出 = n 个动作。损失相对于激活（输出）的导数是什么。 TLDR：损失相对于激活（输出）的导数是什么。激活（输出）PPO 编辑：找到了： 如果加权剪切概率较小，则 dL/dA = 0，这表示梯度没有变化。 如果加权概率较小，则导数为 dL/dA = A_t（时间步骤 t 的优势）/ pi theta old（旧概率）    提交人    /u/meh_coder   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1drk2la/what_is_the_derivative_of_the_loss_in_ppo_eg_dlda/</guid>
      <pubDate>Sat, 29 Jun 2024 20:20:28 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习中的 Spinning up 替代方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1drhci0/alternatives_to_spinning_up_in_deep_rl/</link>
      <description><![CDATA[嗨， 有没有最新的推荐资源与深度强化学习中的 Spinning up 相当？我一直在努力安装所需的软件包，主要是因为它很久以前就发布了（python3.6 拒绝在我当前的设置上运行，Open AI gym 现在已经移至 gymnasium）。 编辑：经过一番摸索，我决定使用 Gymnasium 并按照这个 YouTube 频道使用 Gymnasium：Gymnasium (Deep) 强化学习教程。我很想听听大家对加速学习的想法。   由    /u/LostScientist5356  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1drhci0/alternatives_to_spinning_up_in_deep_rl/</guid>
      <pubDate>Sat, 29 Jun 2024 18:15:24 GMT</pubDate>
    </item>
    <item>
      <title>便宜但功能强大的机械臂和手，带有 api/sdk 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dra5ch/cheap_but_capable_robot_arm_and_hand_with_apisdk/</link>
      <description><![CDATA[嘿，我找不到这样的东西，我想训练一个机械手来为我做一些任务。基本上它应该像从肩膀切断的手臂 目前是否有这样的事物，包括一个简单的 sdk/api？我找不到任何几乎可以负担得起的非商业/实验用途的东西……另外我不想自己打印它，但如果这是唯一的方法，我也可以这样做 只是为了澄清：这是为了使用 RL 进行训练，这就是我在这里发布它的原因    提交人    /u/Iced-Rooster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dra5ch/cheap_but_capable_robot_arm_and_hand_with_apisdk/</guid>
      <pubDate>Sat, 29 Jun 2024 12:30:25 GMT</pubDate>
    </item>
    <item>
      <title>缩放定律真的是定律吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dr60en/is_scaling_law_really_a_law/</link>
      <description><![CDATA[首先，它的范围缩小到了 Transformer，而不是其他架构。一个问题是，它目前的经验发现是否适用于 MLP？其次，越来越多的证据表明，当模型尺寸变大时，确实有一个转折点，之后损失开始上升。那么，相信它可以无限扩大有什么意义呢？我能看到的是，数据方面确实达到了极限。而 LLM 的改进更多地来自数据清理等其他方面。    提交人    /u/OutOfCharm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dr60en/is_scaling_law_really_a_law/</guid>
      <pubDate>Sat, 29 Jun 2024 07:48:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么 transformer 不能处理较小的数据量？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dr5ts6/why_cant_transformer_work_with_small_data_size/</link>
      <description><![CDATA[当单词或经验的数量较少时，注意力模块是否本质上难以对单词或经验之间的关系进行建模？瓶颈可能是什么？更新率小，还是数据不足以揭示真实的单词（或数据）频率？    提交人    /u/OutOfCharm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dr5ts6/why_cant_transformer_work_with_small_data_size/</guid>
      <pubDate>Sat, 29 Jun 2024 07:34:28 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助在自定义环境中管理宠物动物园和多个相同类型的代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dr5phr/need_help_with_petting_zoo_and_multiple_agents_of/</link>
      <description><![CDATA[我在这里描述我的问题： https://stackoverflow.com/questions/78685312/pettingzoo-observations-and-steps-with-with-masked-actions-for-multiple-agents-o 希望有人能帮忙，因为这是我的学士论文的一部分    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dr5phr/need_help_with_petting_zoo_and_multiple_agents_of/</guid>
      <pubDate>Sat, 29 Jun 2024 07:26:13 GMT</pubDate>
    </item>
    <item>
      <title>“法学硕士驱动的自主代理”，Lilian Weng</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dqrm46/llm_powered_autonomous_agents_lilian_weng/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dqrm46/llm_powered_autonomous_agents_lilian_weng/</guid>
      <pubDate>Fri, 28 Jun 2024 19:10:22 GMT</pubDate>
    </item>
    </channel>
</rss>