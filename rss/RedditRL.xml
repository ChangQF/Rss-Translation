<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 26 Mar 2024 12:24:40 GMT</lastBuildDate>
    <item>
      <title>强化学习博士论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bo4qtu/phd_theses_in_reinforcement_learning/</link>
      <description><![CDATA[我正在收集 RL 领域有趣的博士论文列表。你还知道其他有趣的事情吗？请在下面发表评论！  Richard Sutton 1984 - 时间学分强化学习中的作业 Peter Dayan 1991 - 强化联结主义：学习统计方法 Ian Osband 2016 - 通过随机值函数进行深入探索 约翰·舒尔曼 2016 - 优化期望 皮埃尔-Luc Bacon 2018 - 时间表示学习    由   提交 /u/YouAgainShmidhoobuh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bo4qtu/phd_theses_in_reinforcement_learning/</guid>
      <pubDate>Tue, 26 Mar 2024 10:40:57 GMT</pubDate>
    </item>
    <item>
      <title>DDQN 代理未按预期工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bo1hu3/ddqn_agent_not_working_as_expected/</link>
      <description><![CDATA[我使用山地汽车环境来测试我的 DDQN 代理，这些是结果 https://imgur.com/a/AEUm0SP 这是我的模型https://github.com/arthiondaena/Car-game/blob/testing/ddqn_keras.py ddqn = DDQNAgent(gamma=0.9,batch_size =32、epsilon=1.0、epsilon_end=0.01、epsilon_dec=0.995、input_dims=env.observation_space.shape[0]、n_actions=env.action_space.n、fname=&#39;temp.keras&#39;）这些是我使用的超参数。    由   提交/u/Invicto_50  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bo1hu3/ddqn_agent_not_working_as_expected/</guid>
      <pubDate>Tue, 26 Mar 2024 07:00:22 GMT</pubDate>
    </item>
    <item>
      <title>mujoco rl 有什么好的教程吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnzplz/any_good_tutorials_for_mujoco_rl/</link>
      <description><![CDATA[我正在尝试结合稳定的基线 3 创建自定义 mujoco 环境。过去几天一直在努力。    由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnzplz/any_good_tutorials_for_mujoco_rl/</guid>
      <pubDate>Tue, 26 Mar 2024 05:03:39 GMT</pubDate>
    </item>
    <item>
      <title>利用强化学习优化电子商务定价和广告策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnsgik/leveraging_reinforcement_learning_for_optimizing/</link>
      <description><![CDATA[大家好， 我正在深入研究强化学习和电子商务的有趣交叉点，特别是针对产品的优化定价和广告竞价策略。传统上，我们依赖手工算法，这些算法虽然功能强大，但似乎错过了强化学习所承诺的动态适应和优化的潜力。 我们的目标不仅仅是盲目追逐最高奖励，但要改进并可能超越我们当前的算法。该计划是初始化模型，以从我们现有的策略中学习，然后允许其在定义的安全边界内进一步探索和优化。鉴于这涉及实际货币和预算限制，我非常重视样本效率和稳健、风险敏感的学习。 通过我的研究，Dreamer V3 和 EfficientZero V2 似乎处于领先地位。就最先进的性能而言。但是，我想知道这里是否有人在类似的背景下拥有这些或类似模型的实践经验？您如何确保 RL 代理对其样本保持高效，并且在学习时不会耗尽资金？ 此外，我很想知道是否有任何我应该注意的特殊注意事项或陷阱在这样一个财务敏感的环境中应用强化学习时的情况。当风险涉及实际收入和营销预算时，对奖励塑造、探索-利用平衡或安全约束有什么见解吗？ 最后，如果有人在这个领域有与 RL 相关的成功（或恐怖）故事，我会告诉你。听到他们的声音会很高兴。现实世界的例子和经验教训可以为这项工作提供很大的帮助。 渴望听到您的想法、经验和任何建议！   由   提交/u/stoner019  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnsgik/leveraging_reinforcement_learning_for_optimizing/</guid>
      <pubDate>Mon, 25 Mar 2024 23:20:24 GMT</pubDate>
    </item>
    <item>
      <title>使用 TensorFlow 2 为近端策略优化 (PPO) 应用程序寻求清晰易懂的代码示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bns7ho/seeking_clear_and_understandable_code_example_for/</link>
      <description><![CDATA[您好r/reinforcementlearning社区，&lt; /p&gt; 我对强化学习领域还比较陌生，我渴望更深入地研究近端策略优化 (PPO) 等算法。我想询问是否有人可以分享使用 TensorFlow 2 的清晰易懂的 PPO 代码示例。 我专门寻找演示 PPO 实现的关键组件的代码，例如策略网络、价值网络、优势估计和训练循环。 如果您有任何您认为有用的资源、GitHub 存储库或代码片段。 谢谢提前感谢您的帮助和贡献。    由   提交 /u/HassanMid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bns7ho/seeking_clear_and_understandable_code_example_for/</guid>
      <pubDate>Mon, 25 Mar 2024 23:10:01 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习需要精通什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnrqp5/what_do_you_need_to_be_proficient_in_to_learn/</link>
      <description><![CDATA[谷歌搜索没有结果，因此我们将不胜感激。例如，我需要学习Python吗？您能否推荐一些课程或 YouTube 视频系列？我是一个完全的新手，但我愿意学习。谢谢。   由   提交/u/Snoo72721   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnrqp5/what_do_you_need_to_be_proficient_in_to_learn/</guid>
      <pubDate>Mon, 25 Mar 2024 22:51:09 GMT</pubDate>
    </item>
    <item>
      <title>MADDPG Pytorch RuntimeError：梯度计算所需的变量之一已被就地操作修改：[torch.FloatTensor [64, 2]]，它是 AsStridedBackward0 的输出 0，版本为 3；预期是版本 2。提示：上面的回溯显示了 f 的操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnkv7q/maddpg_pytorch_runtimeerror_one_of_the_variables/</link>
      <description><![CDATA[嗨，我一直在尝试实现 MADDPG，但是当我的第二个代理向后传递时，程序崩溃，出现标题中的错误，我到处查看，然后仍然找不到解决方案。如果我只训练一个智能体，那么代码就可以工作，但当问题出现时，就会进入多个智能体。 def learn(self, memory, Episode): &lt; code&gt;[indent]if not memory.ready(): [indent][indent]return [indent]actor_states，状态，动作、奖励、actor_new_states、states_、terminations = memory.sample_buffer() [indent]device = self.agents[0].actor.device [indent]states = T.tensor(states, dtype=T.float32).to(device) [indent]actions_np = np.array(actions, dtype= np.float32)  [indent]actions = T.tensor(actions_np, dtype=T.float32).to(device) [indent] ]rewards = T.tensor(rewards, dtype = T.float32).to(device) [indent]states_ = T.tensor(states_, dtype = T.float32).to （设备） [indent]terminations = T.tensor(terminations).to(device) [indent]all_agents_new_actions = [ ] [indent]all_agents_new_mu_actions = [] [indent]old_agents_actions = [] [indent]for agent_idx, agent in enumerate(self.agents): #根据参与者网络估计下一个状态的动作值  [indent][indent]new_states = T.tensor(actor_new_states[agent_idx], dtype = T.float32).to(device) [indent][indent]new_charge_rate, new_charge_decision = agent .target_actor.forward(new_states) [indent][indent]all_agents_new_actions.append((new_charge_rate)) [indent][indent]all_agents_new_actions .append((new_charge_decision)) [indent][indent]#Action for current state from actor network [indent][ indent]mu_states = T.tensor(actor_states[agent_idx], dtype = T.float32).to(device) [indent][indent]charge_rate, charge_decision = agent. actor.forward(mu_states) [缩进][缩进]all_agents_new_mu_actions.append((charge_rate)) [缩进][缩进]all_agents_new_mu_actions.append((charge_decision)) [缩进][缩进]old_agents_actions.append(actions[agent_idx])  &lt; code&gt;[indent]new_actions = T.cat([all_agents_new_actions 中的行为的行为], dim = 1) [indent]mu = T.cat([为行为的行为in all_agents_new_mu_actions], dim = 1) [indent]old_actions =T.cat([acts for actions in old_agents_actions], dim = 1) [indent]for agent_idx, agent in enumerate(self.agents): #获取目标批评者网络的状态和新动作并将其展平。&lt; /code&gt; #critic 值与目标批评家 #One-step Lookahead TD-error: [缩进] [indent]critic_value_ = agent.target_critic.forward(states_, new_actions).flatten() [indent][indent]critic_value = agent.critic.forward(states, old_actions ).flatten() [indent][indent]target =rewards[:,agent_idx] + agent.gamma*critic_value_  #计算当前critic值的损失 [indent][indent]critic_loss = F.mse_loss(target, Critical_value) [indent] [缩进]self.writer.add_scalar（f“EV_{agent_idx}/Loss/Critic”，critic_loss，情节） [缩进][缩进]agent.critic.optimizer .zero_grad() [indent][indent]critic_loss.backward(retain_graph = True) [indent][indent]agent.critic。 Optimizer.step() [indent][indent]actor_loss = agent.critic.forward(states, mu).flatten() &lt; code&gt;[indent][indent]actor_loss = -T.mean(actor_loss) [indent][indent]self.writer.add_scalar(f&quot;EV_{agent_idx}/Loss/Actor&quot; ;、actor_loss、剧集） [缩进][缩进]agent.actor.optimizer.zero_grad() [缩进][ indent]actor_loss.backward(retain_graph = True) [indent][indent]agent.actor.optimizer.step() [indent] [indent]agent.update_network_parameters() 关于如何修改它以使代理学习的任何想法？ ***对缩进感到抱歉，我尝试在这里获得一个好的格式****   由   提交 /u/Barbajan22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnkv7q/maddpg_pytorch_runtimeerror_one_of_the_variables/</guid>
      <pubDate>Mon, 25 Mar 2024 18:24:55 GMT</pubDate>
    </item>
    <item>
      <title>连续状态和动作空间的近似策略迭代</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnkc06/approximate_policy_iteration_for_continuous_state/</link>
      <description><![CDATA[我遇到的大多数理论分析都处理有限状态或动作空间，或一些其他算法，如近似拟合迭代等。 当状态和动作空间连续时，有关于\epsilon近似策略迭代收敛的理论结果吗？ 我记得一篇单独的论文处理近似策略迭代，其中假设近似误差为随着时间的推移趋于零，但是如果误差是恒定的怎么办？ 此外，是否存在“正统”的误差？这种算法的实际版本与理论算法相匹配吗？   由   提交/u/_An_Other_Account_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnkc06/approximate_policy_iteration_for_continuous_state/</guid>
      <pubDate>Mon, 25 Mar 2024 18:03:26 GMT</pubDate>
    </item>
    <item>
      <title>ICLR 2024：可证明且实用：通过 Langevin Monte Carlo 对强化学习进行有效探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnfvrd/iclr_2024_provable_and_practical_efficient/</link>
      <description><![CDATA[ 由   提交/u/hmi2015  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnfvrd/iclr_2024_provable_and_practical_efficient/</guid>
      <pubDate>Mon, 25 Mar 2024 15:03:44 GMT</pubDate>
    </item>
    <item>
      <title>单代理或多代理设置</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bneekj/single_agent_or_multiagent_setting/</link>
      <description><![CDATA[社区您好， 我目前正在研究代理可以具有不同输出结构的情况。此外，我利用图神经网络来描述状态，其中包含许多类别，这些类别的数量在代理之间可能有所不同。考虑到这种情况，什么配置最合适？在多代理设置中，在我们的例子中，代理通常不会同时请求操作。在任何给定的决策点，我们可能有一个或多个代理寻求行动，而其他代理则很忙。在这种情况下，批评者网络应该接收什么输入？ 提前谢谢您   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bneekj/single_agent_or_multiagent_setting/</guid>
      <pubDate>Mon, 25 Mar 2024 14:01:10 GMT</pubDate>
    </item>
    <item>
      <title>【预测】关于预测的强化学习是否可以利用强化学习来预测河流水质预警系统。哪种方式最适合强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bndqxe/prediction_reinforcement_learning_about/</link>
      <description><![CDATA[ 由   提交/u/Abcsunny95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bndqxe/prediction_reinforcement_learning_about/</guid>
      <pubDate>Mon, 25 Mar 2024 13:32:09 GMT</pubDate>
    </item>
    <item>
      <title>【预测】关于预测的强化学习是否可以利用强化学习来预测河流水质预警系统。哪种方式最适合强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bndpwv/prediction_reinforcement_learning_about/</link>
      <description><![CDATA[ 由   提交/u/Abcsunny95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bndpwv/prediction_reinforcement_learning_about/</guid>
      <pubDate>Mon, 25 Mar 2024 13:30:54 GMT</pubDate>
    </item>
    <item>
      <title>帮助理解 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn7ui2/help_in_understanding_ppo/</link>
      <description><![CDATA[大家好！我在从学术论文和我在网上找到的一些代码暗示中理解有关 PPO 的事情时遇到一些问题。在论文中，我了解到旧模型和新模型的输出之间存在近似。这是如何运作的？如何更新模型，然后计算更新量？我是否需要始终保存 i-1 模型以便进行计算？现在是暗示。我正在使用 IsaacGym 并一次运行 n 个模拟。所有暗示都会根据一系列动作更新模型，直到游戏完成。我希望它从我的 n 个环境中的单个操作中随机批量运行，但我很难理解我需要保存和更改的内容。保存每次迭代需要哪些参数？我想到了：放弃、行动、奖励、价值（V 净输出）、对数概率。我是否遗漏了需要保存的东西？抱歉，如果这篇文章有点长，每一个帮助都会很棒。    由   提交/u/razton  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn7ui2/help_in_understanding_ppo/</guid>
      <pubDate>Mon, 25 Mar 2024 07:33:31 GMT</pubDate>
    </item>
    <item>
      <title>机器人强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn6nns/rl_for_robotics/</link>
      <description><![CDATA[大家好，我整理了一些学习强化学习的学习材料和资源： 1) 深度强化学习，作者：加州大学伯克利分校的 Sergey Levine 2) David Silver 讲座笔记 3) Google Deepmind 讲座视频 4) NPTEL IITM 强化学习  我也更喜欢学习材料具有足够的数学严谨性，能够深入解释算法。  同时引用一堆资源也令人生畏。有人可以为像我这样的初学者推荐上面列出的材料中的笔记和讲座视频吗？如果您还有其他资源，请在评论部分提及。    由   提交 /u/Quirky_Assignment707   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn6nns/rl_for_robotics/</guid>
      <pubDate>Mon, 25 Mar 2024 06:09:33 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>