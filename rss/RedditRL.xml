<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 15 Aug 2024 06:22:19 GMT</lastBuildDate>
    <item>
      <title>SB3 MaskablePPO 返回无效操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1esbt2l/sb3_maskableppo_returning_an_invalid_action/</link>
      <description><![CDATA[我正在使用 SB3 的 MaskablePPO，我注意到在训练一开始它就返回了无效操作，尽管我传递了正确的掩码。如果选择了无效操作，我的代码会抛出错误，因此我无法使用此方法训练模型。我无法在文档中找到解释，但这是正常行为吗？ 示例： 我正在使用大小为 2、4 的 MultiDiscrete 操作空间 传递有效的操作掩码 [True, True, True, False, False, True] 意味着无效操作为 [ [], [1, 2] ]。 我得到 [0 1] 作为操作，即使 1 是无效的    提交人    /u/khalifa30000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1esbt2l/sb3_maskableppo_returning_an_invalid_action/</guid>
      <pubDate>Wed, 14 Aug 2024 20:27:48 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到机器人领域的实习机会？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1es8e0n/where_to_find_internship_opportunities_in_rl_for/</link>
      <description><![CDATA[嗨， 我目前在德国攻读机器人学硕士学位。我还在寻找一份从 2025 年 4 月开始在欧盟地区专注于基于学习的控制的实习机会。在网上做了一些研究后，我只能找到一般的 ML 实习机会，而没有找到与 RL 或机器人模仿学习相关的实习机会。  我相信这个 subreddit 上的人以前一定遇到过类似的问题。请指导我在哪里寻找实习机会。 谢谢    提交人    /u/No_Bid_3602   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1es8e0n/where_to_find_internship_opportunities_in_rl_for/</guid>
      <pubDate>Wed, 14 Aug 2024 18:00:27 GMT</pubDate>
    </item>
    <item>
      <title>我在使用 mujoco 拖放时遇到了问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1erxkc7/im_having_an_issue_with_drag_and_drop_for_mujoco/</link>
      <description><![CDATA[      嘿，所以我在 Windows 中使用了模拟，它甚至没有响应，所以在 wsl 中使用，但模拟不允许拖放功能，有什么想法可以解决这个问题吗？ 图像直接在 Windows 上模拟时使用     提交人    /u/Actual-Magazine2609   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1erxkc7/im_having_an_issue_with_drag_and_drop_for_mujoco/</guid>
      <pubDate>Wed, 14 Aug 2024 09:52:10 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero 参与加密货币交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1erxbhl/alphazero_in_crypto_trading/</link>
      <description><![CDATA[我正在探索用于加密交易的高级方法，例如 AlphaZero 概念，包括概率分布、状态值和蒙特卡罗模拟。尽管训练了 LSTM、NN 和 CNN 模型，但我没有看到重大进展。我不确定如何继续，正在寻找在训练期间评估模型的有效方法。 是否有人为现实世界的交易实施，我很高兴听到你的话。    提交人    /u/laxuu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1erxbhl/alphazero_in_crypto_trading/</guid>
      <pubDate>Wed, 14 Aug 2024 09:35:45 GMT</pubDate>
    </item>
    <item>
      <title>MCTS 是不是 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ervkgb/is_mcts_rl_or_not/</link>
      <description><![CDATA[我目前正在研究 MCTS 和 RL。关于 MCTS 是强化学习还是只是一种基本的搜索/规划算法，似乎存在不少争议。我知道 mcts 不会明确更新价值函数或策略。不过，我也读到过一些观点，认为 MCTS 实际上是 RL（不指 alphezero）。这有点颠覆了我对基于模型的 RL 的理解。我一直认为基于模型的 RL 正是 MCTS 所做的。事实上，如果 MCTS 的选择/扩展甚至不是 RL，那么 AlphaZero 似乎不是基于模型的。那么还有基于模型的方法吗？似乎几乎每种基于模型的方法都在内部使用无模式强化学习。AlphaZero 使用 DeepNN 估计（不涉及模型），……    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ervkgb/is_mcts_rl_or_not/</guid>
      <pubDate>Wed, 14 Aug 2024 07:35:52 GMT</pubDate>
    </item>
    <item>
      <title>在强化学习中，最优策略总是确定性的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1erurar/is_the_optimal_policy_always_deterministic_in/</link>
      <description><![CDATA[您好， 我有一个关于最优策略的问题。 根据贝尔曼最优方程，最优策略是确定性的。但是，我很好奇这个结论是否适用于所有环境。 最优策略是否仅在平稳环境中是确定性的？在非平稳环境中，随机策略是否也可以是最优的？此外，是否存在即使在平稳环境中，随机策略也可能被视为最优的场景？ 如果您能提供任何见解或解释，我将不胜感激。谢谢！    提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1erurar/is_the_optimal_policy_always_deterministic_in/</guid>
      <pubDate>Wed, 14 Aug 2024 06:41:59 GMT</pubDate>
    </item>
    <item>
      <title>PPO 澄清</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ere8hc/ppo_clarification/</link>
      <description><![CDATA[在尝试实施 PPO 时，我对策略比率代表什么感到有些困惑：pi_theta/pi_theta_old。我不明白的是，我们如何计算 pi_theta，即新策略概率？我们尚未更新我们的策略，因为这是损失的重点（通过它进行反向传播并更新我们的策略），所以 pi_theta 不是=pi_theta_old 吗？这个损失与我们收集的数据有什么关系？    提交人    /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ere8hc/ppo_clarification/</guid>
      <pubDate>Tue, 13 Aug 2024 17:44:10 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习解决 NP-hard 图形问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ere13q/model_based_reinforcement_learning_to_solve/</link>
      <description><![CDATA[大家好，我目前正在研究基于模型的强化学习方法，用于解决 NP 难题 (DFJSP)，以图形建模。我实施了 MCTS，并进行了一些调整（渐进式加宽、修剪、RAVE）。我目前正在寻找更多方法，解决巨大的动作和状态空间、收敛时间和内存优化问题。最后，我想通过将原始 MCTS 与所做的修改进行比较来展示结果，并将它们与其他现有方法（也可能是无模型的）进行比较。该论文背后的想法是，鉴于问题的动态和不确定性，基于模型的强化学习将能够更好地适应动态事件并提高样本效率。你们对更高级的方法有什么想法吗？我考虑过使用 GAT 架构来估计价值和策略以及 PUCT。也许还有 Thompson 采样或 Gumbal 噪声用于选择。也许还有非 MCTS 方法？我愿意接受任何想法！     提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ere13q/model_based_reinforcement_learning_to_solve/</guid>
      <pubDate>Tue, 13 Aug 2024 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习模型没有学到任何东西（有用）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er8lbf/deep_q_learning_model_doesnt_learn_anything_useful/</link>
      <description><![CDATA[  由    /u/ZazaGaza213  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er8lbf/deep_q_learning_model_doesnt_learn_anything_useful/</guid>
      <pubDate>Tue, 13 Aug 2024 13:58:45 GMT</pubDate>
    </item>
    <item>
      <title>拥有专门的探索行动。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er6sgh/having_a_dedicated_action_for_exploration/</link>
      <description><![CDATA[大家好，我一直在使用 PPO 来学习我工作环境的预测模型。代码运行良好，但当然也存在一些探索问题。 我在想，既然我有一个模型，我可以用它来鼓励探索。在我做的这个问题的非策略版本中，我有一个贪婪的 epsilon，但我没有选择贪婪动作，而是根据正在学习的模型的另一个成本函数选择了一个动作。所以这是一种基于模型的无模型混合。 将其转化为 PPO 并不那么简单，因为动作选择是随机的。 我的问题是，如果有一个额外的动作，当被选中时，允许代理以直接鼓励探索而不是根据策略采取行动的方式选择其他动作之一，这会是奇怪还是不好的做法。这样，探索仍然被编码在策略中，但不是“直接”的？ 我的其他选择是使用环境模型中的一些度量（例如熵）来更新损失函数，这似乎最终会变得不稳定，使用某种 epsilon 贪婪选择选择一个探索性动作并给它当前 epsilon 的对数概率（或者通过网络的前向传递？？？） 我对数学的直觉还不是很好，想知道是否有人对这些选项有更具体的见解。谢谢！    提交人    /u/CC-Twip   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er6sgh/having_a_dedicated_action_for_exploration/</guid>
      <pubDate>Tue, 13 Aug 2024 12:37:52 GMT</pubDate>
    </item>
    <item>
      <title>MDP 与 POMDP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er21m5/mdp_vs_pomdp/</link>
      <description><![CDATA[尝试理解 MDP 和子程序，以便对 RL 有基本的了解，但事情变得有点棘手。根据我的理解，MDP 仅使用当前状态来决定采取哪种操作，而真实状态是已知的。然而在 POMDP 中，由于代理无法访问真实状态，因此它利用其观察和历史记录。 在这种情况下，如果 POMDP 使用来自历史记录的信息，即从先前观察（即 t-3，...）中检索到的信息，那么它如何具有马尔可夫特性（它甚至被称为 MDP）。 非常感谢你们！    提交人    /u/Internal-Sir-5393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er21m5/mdp_vs_pomdp/</guid>
      <pubDate>Tue, 13 Aug 2024 07:48:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO (SB3) 中的动作掩蔽。我如何才能真正掩蔽动作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eqkk4k/action_masking_in_ppo_sb3_how_can_i_actually_mask/</link>
      <description><![CDATA[因此，本质上我有一个多二进制环境，我不希望模型在该环境中再次重复所述操作。  我没有从网站找到太多帮助：stable-baselines3-contrib/docs/modules/ppo_mask.rst at master · Stable-Baselines-Team/stable-baselines3-contrib (github.com) 有人可以指导我/帮助我实施掩码吗？  尝试通过惩罚重复的选项来解决它似乎无法按预期工作。   由    /u/CampMaster69  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eqkk4k/action_masking_in_ppo_sb3_how_can_i_actually_mask/</guid>
      <pubDate>Mon, 12 Aug 2024 17:58:03 GMT</pubDate>
    </item>
    <item>
      <title>四足动物 RL 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eqhwra/quadruped_rl_question/</link>
      <description><![CDATA[嗨 我目前正在研究一个机器狗 RL 项目，目标是教它如何走路。 我正在使用 PPO，我有一个机器狗的 urdf 文件，我将其上传到 pybullet 进行训练，奖励函数包含以下内容： 学习率 = 1e-4 熵 = 0.02  奖励前进速度，-ve 奖励后退（前进是根据身体的前进方向，而不是一般的前进） 因使用过多能量而受到的能量惩罚 稳定性惩罚（因不稳定而受到的惩罚） 跌倒惩罚（因跌倒而受到的惩罚） 平滑度惩罚（因急剧改变速度而受到的惩罚） 对称性惩罚（因以对称形式行走而受到的奖励）  我尝试过这些奖励的尺度，有时会删除其中一些，只关注主要奖励，如前向和稳定性，但不幸的是，在大约 700k 步之后，代理没有学到任何东西；我只尝试了稳定性和前向奖励，我只尝试了前向奖励，我尝试了所有这些，其余奖励使用小权重，前向运动使用大权重。但模型仍然没有学到任何行为 当我大幅增加能量权重并使其主导奖励函数时，我得到的唯一反应是，在大约 300k 步之后，代理学会以更慢、更稳定的方式行走，但在 500k 之后它就停止移动了。这是可以理解的注意：我采用了行走缓慢、在 30 万步后保持稳定的模型，其奖励函数仅关注能量，我尝试将其用作迁移学习方法，我采用它然后在更完整的奖励函数上对其进行训练，该奖励函数具有前向运动奖励，但过了一段时间后，它又开始随机行为，并且变得不如开始时那么稳定 但是，我的问题是，在每次其他试验中我都没有看到任何效果，例如我没有看到模型向前移动但不稳定，或者我根本没有看到模型学习任何东西，它只是不断随机移动和下降并且我不认为 70 万步是一个短暂的训练期，我认为在此之后我至少应该看到任何细小的行为变化，不一定是积极的变化，但任何变化都给我提示下一步该尝试什么 注意：除了奖励函数之外，我没有尝试调整其他任何东西 如果有人知道任何事情，请帮忙   由    /u/youssef_naderr  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eqhwra/quadruped_rl_question/</guid>
      <pubDate>Mon, 12 Aug 2024 16:15:46 GMT</pubDate>
    </item>
    <item>
      <title>DeepMimic 中的提前终止实际上是如何运作的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eq8fak/how_early_termination_in_deepmimic_actually_works/</link>
      <description><![CDATA[在他们的代码中，他们使用提前终止后的所有样本，奖励为 0。即使在提前终止后，情节也会达到最大步骤。提前终止仅在触发后将奖励设置为 0。  DeepMimic - 6.2 提前终止 ... 一旦触发提前终止，角色在情节的剩余时间内将获得零奖励。这种提前终止的实例提供了另一种塑造奖励函数的方法，以阻止不良行为。 ...  奖励计算代码 [提前终止时的奖励为 0] 剧集重置条件代码 [提前终止不包含在条件中] 因此，我认为在触发提前终止后，情节实际上并没有结束。它看起来更像是一种奖励塑造技术，在意外情况下将奖励设置为 0。他们可能希望控制跌倒后的抖动奖励，并通过将奖励设置为零（这是可能的最低奖励）来强烈惩罚跌倒。 这是我的问题：实际结束情节比仅使用提前终止进行奖励塑造更好吗？你怎么看？    提交人    /u/Any_Way2779   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eq8fak/how_early_termination_in_deepmimic_actually_works/</guid>
      <pubDate>Mon, 12 Aug 2024 08:31:23 GMT</pubDate>
    </item>
    <item>
      <title>内在奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1epsdkz/intrinsic_rewards/</link>
      <description><![CDATA[嘿，与原始 RND 论文相反，我发现大多数实现只是添加了外在和内在奖励......为什么？有人证明它足够好吗？    提交人    /u/What_Did_It_Cost_E_T   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1epsdkz/intrinsic_rewards/</guid>
      <pubDate>Sun, 11 Aug 2024 18:53:20 GMT</pubDate>
    </item>
    </channel>
</rss>