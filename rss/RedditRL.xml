<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 07 Mar 2025 15:18:28 GMT</lastBuildDate>
    <item>
      <title>需要帮助实施RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5o3uj/need_help_implementing_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为我的公司建立AI代理，从本质上讲，我们有一些客户使用仪表板来构建动态UI，供用户保留和转换其移动或Web应用程序。  我们想建立一个可以通过用户的行为为客户选择最佳的UI变体的AI代理。  我在基本层面上开始建立代理的方法应该是什么？ 技术堆栈应该是什么？  我应该知道的链接或资源是否可以帮助我建立代理？  谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/poperbudget348     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5o3uj/need_help_implementing_rl/</guid>
      <pubDate>Fri, 07 Mar 2025 13:44:42 GMT</pubDate>
    </item>
    <item>
      <title>是时候训练DQN为Ale Pong V5</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j5icxh/time_to_train_dqn_for_ale_pong_v5/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我使用的是带有3个Conv层（32、64、64滤波器）和完全连接的层（512个单位）的CNN。我的设置包括RTX 4070 Ti Super，但每集需要6-7秒。这比我使用CPU的每集50秒要快得多，但是GPU的使用仅为20-30％，而CPU的使用量低于20％ 这是典型的性能，还是我可以优化的东西来加快速度？任何建议将不胜感激！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/soliseeker     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j5icxh/time_to_train_dqn_for_ale_pong_v5/</guid>
      <pubDate>Fri, 07 Mar 2025 07:51:47 GMT</pubDate>
    </item>
    <item>
      <title>在线学习的逻辑帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j57qnn/logic_help_for_online_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在研究一个自动的高速缓存内存管理项目，我旨在创建一个自动化的策略，以便在发生缓存失误时提高性能。目的是根据设定级别和传入的填充详细信息选择一个缓存块进行驱逐。 对于我的模型，我已经实施了一种离线学习方法，该方法是使用专家策略培训的，并根据专家决策进行了立即的奖励。现在，我想使用在线增强学习来完善这种脱机培训的模型，在这种模型中，与基准相比，根据IPC的改进来计算奖励（例如，像MockingJay这样的最先进的策略）。 我已经为这种方法写了一种在线学习算法（我为此提供了这种方法），但是我会为您提供了努力，因为我可以从中努力进行编码。我的方法有意义吗？您会完善什么？ 以下情况您可能应该知道：  1）没有下一个状态（s&#39;）的建模，因此我不模拟向下一个状态过渡到下一个状态（s&#39;）（s&#39;），因为缓存驱逐是单步决策问题，因为驱逐的效果仅在下一步的情况下，而不是在执行中，因此我在执行情况下，我不得不将驱逐出境，因此，我是在执行的情况下，而我却不是在执行中，而我却不是在执行中，而我却不是在执行中，而我却不是在执行范围。仅在模拟结束时观察。  2）在线学习微调离线学习网络  离线学习阶段使用对专家决策的监督学习 在线学习阶段在线学习阶段初始化政策 在线学习阶段可以根据IPC的限制          3 Simulation which is slightly different than textbook examples of RL so,  The reward is based on IPC improvement compared to a baseline policy The same reward is assigned to all eviction actions taken during that simulation  4) The bellman equation is simplified so no traditional Q-Learning bootstrapping (Q(s&#39;)) because I dont have my next state建模。然后将方程变为q（s，a）←q（s，a）+α（r -q（s，a））（我认为） 您可以在此处找到我为此问题写的算法： https://drive.google.com/file/d/100imnq2eeu_huvvztk6youwkeni13kve/view?usp = sharing   很抱歉长篇文章，但我确实在此处确实可以在此处提供您的帮助和反馈：)   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]     32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j57qnn/logic_help_for_online_learning/</guid>
      <pubDate>Thu, 06 Mar 2025 22:14:44 GMT</pubDate>
    </item>
    <item>
      <title>哪种机器人模拟器更适合增强学习？ Mujoco，Sapien或Isaaclab？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4wa9g/which_robotics_simulator_is_better_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图选择最合适的模拟器来加强我的研究机器人操纵任务。 Based on my knowledge, MuJoCo, SAPIEN, and IsaacLab seem to be the most suitable options, but each has its own pros and cons:  MuJoCo:  pros: good API and documentation, accurate simulation, large user base large. cons:并行性不那么好（需要 jax 才能平行执行）。        sapien：  优点：良好的API，良好的API，良好的平行性。   并行性，丰富的特征，NVIDIA生态系统。  cons：资源密集型，学习曲线太陡峭，仍在进行重大更新，据报道容易出现。         &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/xyllong     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4wa9g/which_robotics_simulator_is_better_for/</guid>
      <pubDate>Thu, 06 Mar 2025 14:10:29 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用Reddit情感来构建股票预测AI  - 这就是发生的事情！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4se5b/tried_building_a_stock_prediction_ai_using_reddit/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/o1h8yk_m1ywxmiw8cslf3a9ukpx5nt7tzwt0h2dxdxd-4.jpg？宽度= 320＆amp; crop = smart＆amp; auto = webp＆amp; s = 1EB7518B4C497829A6E6E6E19370950F5D96EA6E3F57“ title =“尝试使用reddit情感建立股票预测AI  - 这就是发生的事情！” /&gt;   ＆＃32;提交由＆＃32; /u/u/u/indows-phase-9280     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4se5b/tried_building_a_stock_prediction_ai_using_reddit/</guid>
      <pubDate>Thu, 06 Mar 2025 10:17:01 GMT</pubDate>
    </item>
    <item>
      <title>加强 - 需要帮助改善奖励。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4pk9n/reinforce_need_help_in_improving_rewards/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  任何人都可以推荐我如何提高奖励。任何技术，YT视频甚至研究论文。一切都很好。我是一个学生刚开始RL课程，所以我真的不知道。env，奖励是离散的。请帮助😭🙏🙏🙏🙏🙏🙏  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/loud_lengthiss4987     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4pk9n/reinforce_need_help_in_improving_rewards/</guid>
      <pubDate>Thu, 06 Mar 2025 06:46:30 GMT</pubDate>
    </item>
    <item>
      <title>更新：礁石模型 - 一种用于AI连续性的生活系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4gm8m/updated_the_reef_model_a_living_system_for_ai/</link>
      <description><![CDATA[       &lt;！ -  sc_off- sc_off-&gt;  现在，所有的数学和代码在您的学习中，您的学习享受。     &lt;！ -  sc_on-&gt; 32;提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/the-reef-model-a-living-system-for-ai-continuity-0233c39c39c39c3f80”&gt; [link]    [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4gm8m/updated_the_reef_model_a_living_system_for_ai/</guid>
      <pubDate>Wed, 05 Mar 2025 22:52:44 GMT</pubDate>
    </item>
    <item>
      <title>分步教程：使用Llama 3.1（8b） + Google Colab + Grpo培训自己的推理模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4g234/stepbystep_tutorial_train_your_own_reasoning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4g234/stepbystep_tutorial_train_your_own_reasoning/</guid>
      <pubDate>Wed, 05 Mar 2025 22:29:52 GMT</pubDate>
    </item>
    <item>
      <title>桥AI框架v1.1- Noor礁的数学，代码和逻辑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j4b4cx/the_bridge_ai_framework_v11_the_math_code_and/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/018wvvyzwksyluuviqcc9pwpzmqo5sq4lnfwx9veedt0.jpg？宽度= 640＆amp; crop = smart＆amp; auto = webp＆amp; s = 87D8DE13AF73711F516FDB479E0A7EEDD353B908“ title =“桥梁AI框架V1.1- Noor礁的数学，代码和逻辑”/&gt;      &lt;！ -  sc_off-&gt;   发布的文章解释了本文档中发现的数学和逻辑。提交由＆＃32; /u/u/pseud0nym    href =“ https://medium.com/@lina.noor.agi/bridge-ai-framework-framework-framework-only-a5efcd9d01c7”&gt; [link]    32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j4b4cx/the_bridge_ai_framework_v11_the_math_code_and/</guid>
      <pubDate>Wed, 05 Mar 2025 19:10:59 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    <item>
      <title>学习率计算</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j46zj1/learning_rate_calculation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我目前正在撰写医学硕士学位论文，在得分强化学习任务时需要帮助。从基本上来说，受试者执行了逆转学习任务，我想使用最简单的方法来计算平均学习率（我考虑只使用recrescorla-wagner公式，但是我找不到任何论文表明一个人会表明一个人会如何计算它）。 ）。 ）。 ，所以我要问我如何才能启动刺激刺激的刺激效果，并刺激刺激的刺激效果，或者是刺激的刺激效果？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforecricelearning/comments/1j46zj1/learning_rate_calculation/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j46zj1/learning_rate_calculation/</guid>
      <pubDate>Wed, 05 Mar 2025 16:26:02 GMT</pubDate>
    </item>
    <item>
      <title>帮助调试我的简单DQN AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j457st/help_debug_my_simple_dqn_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我制作了一个非常简单的游戏环境来使用pytorch训练DQN。 The game runs on a 10x10 grid, and the AI&#39;s only goal is to reach the food. Reward System: Moving toward food: -1 Moving away from food: -10 Going out of bounds: -100 (Game Over) The AI kind of works, but I&#39;m noticing some weird behavior - sometimes, it moves away from the food before going toward it （请参见下面的视频）。出于某种原因，它有时也会超出范围。 我已经尝试增加培训情节，但问题仍然发生。有什么想法会导致这件事？真的很感谢任何见解。谢谢。  源代码：  游戏环境  snake_game.py：   dqn class   utils.py： href =“ https://pastebin.com/raw/fepnsluv”&gt; https://pastebin.com/raw/fepnsluv  href =“ https://pastebin.com/raw/ndftrbjx”&gt; https://pastebin.com/raw/ndftrbjx  href =“ https://reddit.com/link/1j457st/video/9sm5x7clyvme1/player”&gt; https://reddit.com/link/link/1j457st/video/9sm5x7clyvmevme1/player  /u/u/unlikely_tax_4619       [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j457st/help_debug_my_simple_dqn_ai/</guid>
      <pubDate>Wed, 05 Mar 2025 15:10:14 GMT</pubDate>
    </item>
    <item>
      <title>在C ++中加载训练有素的模型以帮助加载训练有素的模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j43skc/help_with_loading_a_trained_model_for_simtoreal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi。我使用legged_gym和rsl_rl有一个训练有素的pt文件中的训练模型。我想加载此模型并使用C ++进行测试。我想知道是否有任何可以看的开源代码。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/open-safety-1585     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j43skc/help_with_with_aloading_a_trained_model_model_model_for_simtoreal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j43skc/help_with_loading_a_trained_model_for_simtoreal/</guid>
      <pubDate>Wed, 05 Mar 2025 14:04:19 GMT</pubDate>
    </item>
    <item>
      <title>麦肯纳的动态抵抗定律：理论</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j3uvr6/mckennas_law_of_dynamic_resistance_theory/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    麦肯纳的动态抗性定律被引入，作为一个新的原理，管理适应性电阻网络，该原理会因对电气刺激的响应而主动调整其电阻。受电子（ER）流体和自组织生物系统的行为的启发，该法律为电路提供了一个理论框架，这些框架可以重新配置自己以优化性能。我们介绍了麦肯纳定律的数学表达及其与已知的物理定律（欧姆定律，基尔乔夫法律）和自然界中的类似物的联系。开发了一个模拟模型来实施提出的动态阻力更新，结果证明了新兴行为，例如自动形成最佳导电途径和最小化功率耗散。我们讨论了这些结果的重要性，将自适应网络的行为与粘液模具途径和蚂蚁菌落优化的类似现象进行了比较。最后，我们探讨了麦肯纳定律在电路设计，优化算法和自组织网络中的潜在应用，从而强调了动态适应性电阻元件如何导致强大而有效的系统。本文总结了关键贡献和未来研究方向的轮廓，包括实验验证和更广泛的计算含义。   https://github.com/rdm3dc/rdm3dc/mckenna-shaw-shaw-of-of-of-of-of-dynamic-resistance-git-resistance-git-git-git 提交由＆＃32; /u/u/ushore-telephone96     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j3uvr6/mckennas_law_of_dynamic_resistance_theory/</guid>
      <pubDate>Wed, 05 Mar 2025 04:27:09 GMT</pubDate>
    </item>
    <item>
      <title>加强学习的注释团队？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j3udkk/annotation_team_for_reinforced_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在努力培训具有稀疏奖励的RL模型，定义正确的奖励信号很痛苦。该模型通常会陷入次优的行为，因为收到有意义的反馈花费太长。 合成奖励感觉太骇人听闻，而且不太概括。人体标记的反馈 - 有用，但超级耗时且缩放时不一致。因此，在这一点上，我正在考虑外包注释 - 但不知道该选择谁！因此，我宁愿与我们社区表现良好的人一起工作。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/pramear-phrase-318      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j3udkk/annotation_team_for_reinforced_learning/</guid>
      <pubDate>Wed, 05 Mar 2025 03:59:06 GMT</pubDate>
    </item>
    </channel>
</rss>