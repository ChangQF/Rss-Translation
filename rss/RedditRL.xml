<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 18 Apr 2024 15:14:32 GMT</lastBuildDate>
    <item>
      <title>关于在 SMAC 基准上训练 MARL 算法有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c75fm2/any_tips_on_training_marl_algorithms_on_the_smac/</link>
      <description><![CDATA[我正在尝试在一些 SMAC 基准上训练 MAPPO，但对超参数微调没有任何线索。值得注意的是，训练胜率始终为 0，而测试胜率在 0 到 95% 之间波动。这是正常的吗？  我在论文上看到的大多数情节都会收敛到胜率超过 90%。这很容易实现吗？ 非常感谢任何建议。   由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c75fm2/any_tips_on_training_marl_algorithms_on_the_smac/</guid>
      <pubDate>Thu, 18 Apr 2024 14:58:28 GMT</pubDate>
    </item>
    <item>
      <title>回报曲线平滑度有多重要？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6zlnc/how_important_is_returns_curve_smoothness/</link>
      <description><![CDATA[我一直在运行一些超参数搜索，以找到可以加快学习速度的配置。不幸的是，几乎每次搜索都会在探索窗口中产生相同的轨迹。 其中一条回报曲线比其他曲线明显平滑。鉴于几乎所有其他曲线看起来都一样，我应该认为这种平滑度是优点还是问题？我不确定从长远来看，这是否是探索不力的症状，从而收敛到局部最小值，或者是一个积极的信号，表明它不会不断地忘记重新学习小细节。    由   提交/u/drblallo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6zlnc/how_important_is_returns_curve_smoothness/</guid>
      <pubDate>Thu, 18 Apr 2024 10:05:49 GMT</pubDate>
    </item>
    <item>
      <title>环境可扩展性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6z5c7/environment_scalability/</link>
      <description><![CDATA[       我有一个使用 3 个代理训练的多代理强化学习环境。我的观察和操作空间是动态的： min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32) max_action = np.array ([5, 5] * len(self.agents), dtype=np.float32) self.action_space = space.Box(low=min_action, high=max_action, dtype=np.float32) min_obs = np.array([[ -np.inf, -np.inf, -2.5, -2.5]] * len(self.agents), dtype=np.float32) max_obs = np.array([[np.inf, np.inf, 2.5, 2.5 ]] * len(self.agents), dtype=np.float32) self.observation_space = space.Box(low=min_obs, high=max_obs, dtype=np.float32)  &lt; strong&gt;虽然我在 OpenAI Gym 自定义环境中使用 SB3 的 PPO，但我的代理设置也采用了多代理。我希望我的模型运行 10 个代理来测试它。 执行此操作时，我收到观察空间不匹配的错误： 错误 任何解决方案表示赞赏。   由   提交 /u/Hooooman101   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6z5c7/environment_scalability/</guid>
      <pubDate>Thu, 18 Apr 2024 09:34:30 GMT</pubDate>
    </item>
    <item>
      <title>机器人强化学习：您面临的最大问题是什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6trcp/robotics_rl_what_are_some_of_the_biggest_problems/</link>
      <description><![CDATA[很想听听将强化学习策略构建到机器人技术中的不好的部分。你最讨厌的事情你希望不存在！故意让它开放和含糊。希望听到任何反馈:)   由   提交/u/bluejae05  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6trcp/robotics_rl_what_are_some_of_the_biggest_problems/</guid>
      <pubDate>Thu, 18 Apr 2024 03:44:19 GMT</pubDate>
    </item>
    <item>
      <title>记录结果需要帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6pehy/help_needed_with_logging_results/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6pehy/help_needed_with_logging_results/</guid>
      <pubDate>Thu, 18 Apr 2024 00:04:50 GMT</pubDate>
    </item>
    <item>
      <title>“Ijon：通过模糊测试探索深层状态空间”，Aschermann 等人 2020</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6nvod/ijon_exploring_deep_state_spaces_via_fuzzing/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6nvod/ijon_exploring_deep_state_spaces_via_fuzzing/</guid>
      <pubDate>Wed, 17 Apr 2024 22:56:25 GMT</pubDate>
    </item>
    <item>
      <title>Zombie 2102：一款基于博弈论的可玩网页游戏，在浏览器中使用人工智能对手</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6ifwr/zombie_2102_a_playable_web_game_based_on_game/</link>
      <description><![CDATA[ 由   提交/u/bluboxsw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6ifwr/zombie_2102_a_playable_web_game_based_on_game/</guid>
      <pubDate>Wed, 17 Apr 2024 19:14:47 GMT</pubDate>
    </item>
    <item>
      <title>“逆合成规划的人工智能需要数据和专业知识”，Strieth-Kalthoff 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6i690/artificial_intelligence_for_retrosynthetic/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6i690/artificial_intelligence_for_retrosynthetic/</guid>
      <pubDate>Wed, 17 Apr 2024 19:03:44 GMT</pubDate>
    </item>
    <item>
      <title>（问题）A2C 和重播缓冲区中的 DQN 决斗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6f3gf/question_dueling_dqn_in_a2c_and_replay_buffers/</link>
      <description><![CDATA[TLDR：在 A2C 中使用决斗 DQN 作为批评者是否允许使用重播缓冲区？ 我的推理为什么 Actor-Critic 方法不能使用重放缓冲区，因为 Critic 只输出基于状态的值估计，这与基于状态和动作输出质量估计的 DQN 方法不同。  如果 DQN 代理从非最优策略的经验中学习，它将降低与非最优策略所采取的操作相关的值。然而，如果批评者这样做，就会降低国家的价值，而国家的价值可能是当前政策可以利用的最佳状态。  在这种情况下，为什么不直接使用一个可以区分批评者动作的模型并利用重播缓冲区呢？决斗 DQN 已经输出与每个动作相关的优势，该优势将直接输入到参与者中。    由   提交/u/AUser213  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6f3gf/question_dueling_dqn_in_a2c_and_replay_buffers/</guid>
      <pubDate>Wed, 17 Apr 2024 17:00:47 GMT</pubDate>
    </item>
    <item>
      <title>训练动力学模型来预测下一状态和奖励的高斯参数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c65ipa/training_a_dynamics_model_to_predict_the_gaussian/</link>
      <description><![CDATA[我目前正在开发一个项目，在 Stable Baselines 3 中实现基于模型的算法包装器。我才真正开始使用 RL 大约 6 年几个月前，所以还有很多东西仍然不熟悉，或者从数学角度我没有具体理解。现在我引用的是 Kurutach 等人。 2018 (https://arxiv.org/abs/1802.10592) 和高＆amp;王 2023 (https://www.sciencedirect.com/science/article/pii/S2352710223010318，其中也参考了 Kurutach）。 我对如何继续构建模型网络有些分歧。我理解模型应该将特征提取的状态和动作作为输入。我主要关心的是输出层。 如果我假设环境动态是确定性的，那么我知道我应该只是进行训练来预测确切的下一个状态（或下一个状态的变化，正如库鲁塔奇大部分时间所做的那样）。然而，如果我假设环境动态是随机的，那么根据高＆amp;王，我应该预测下一个状态高斯概率分布的参数。我的问题是，我不知道该怎么做。 所以 TLDR；训练动态模型密集前馈神经网络来预测下一状态高斯概率分布的参数的常见做法是什么？  如果我有任何不清楚的地方，请随时提问。我非常感谢在此事上提供的任何帮助。   由   提交/u/bean_217  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c65ipa/training_a_dynamics_model_to_predict_the_gaussian/</guid>
      <pubDate>Wed, 17 Apr 2024 09:29:48 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：RL Agent 没有学会在特定场景中输出零</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5nbf2/seeking_advice_rl_agent_not_learning_to_output/</link>
      <description><![CDATA[大家好， 我对强化学习领域还比较陌生，目前正在开展一个项目，我在该项目中我们为电动汽车充电系统开发了一个自定义环境，其中代理的目标是在两个电池存储之间优化分配能量，并决定何时从电网获取电力。 环境详细信息：   状态空间：状态S由9个值组成： 当前步骤所需的总能量 当前网格能源价格 两个电池（主电池和副电池）的充电状态 (SoC) 以及其他功能  动作空间：动作输出是三个值的元组： 主电池放电（在0和1之间连续） 辅助电池放电（在0之间连续）和 1) 电网能量消耗（在 0 和 1 之间连续）   奖励功能：&lt; br /&gt; 该奖励旨在惩罚不必要的电网使用，特别是当电网价格为正时，并奖励不依赖电网而满足需求的能源分配。如果电网价格为负，该函数将根据使用的电量按比例奖励使用电网电力。 使用的算法： Soft Actor Critic (SAC) 的 Stablebaselines3 实现）与 MLP 策略。 挑战： 第三个操作（电网能源消耗）出现了具体问题。在最好将此操作设置为零的情况下（即电池中有足够的能量和正的电网价格），代理不会学习这样做。这些值向零收敛（低至 0.1 或 0.04），但从未达到零。这种次优行为会显着影响性能。 我正在考虑将值手动修剪为零作为最后的手段，但我更希望代理自然地学习这种行为。  问题：  在训练智能体在特定条件下选择动作空间中的零值时，是否有人遇到过类似的挑战？ 有吗？对 SAC 算法或奖励函数进行具体调整，以鼓励更精确的行为？ 奖励逻辑中任何额外惩罚或修改的实施是否可以帮助智能体学会更可靠地选择零？ &gt;  任何见解、技巧或类似场景的分享经验将不胜感激！ 提前感谢您的帮助！   由   提交 /u/Nnarruqt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5nbf2/seeking_advice_rl_agent_not_learning_to_output/</guid>
      <pubDate>Tue, 16 Apr 2024 18:23:01 GMT</pubDate>
    </item>
    <item>
      <title>强化学习文学</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5kysz/rl_literature/</link>
      <description><![CDATA[哪些被认为是当前 RL 上最好的文本？我已经阅读了萨顿的第二版，但不确定下一步该去哪里。 有什么建议吗？   由   提交 /u/NSADataBot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5kysz/rl_literature/</guid>
      <pubDate>Tue, 16 Apr 2024 16:49:59 GMT</pubDate>
    </item>
    <item>
      <title>更新观察空间形状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5ff9t/update_the_observation_space_shape/</link>
      <description><![CDATA[我正在使用gymnasium进行RL对象检测，我想问是否有人知道如何在重置中加载新图像时更新我的​​观察空间形状方法。我使用的数据集具有相同形状的图像，但我在具有各种形状的 visdrone2019 数据集中进行了更改。我在 init 函数中初始化了观察空间的形状，但我可以找到一种稍后更改其形状的方法 self.observation_shape = self.current_image.shape self.observation_space =gym。 space.Box( low=0, high=255, shape=self.observation_shape, dtype=np.uint8 )  ​   由   提交 /u/Confident-Crab-5686   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5ff9t/update_the_observation_space_shape/</guid>
      <pubDate>Tue, 16 Apr 2024 12:56:33 GMT</pubDate>
    </item>
    <item>
      <title>Taxi-v3 和 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5ci7f/taxiv3_and_dqn/</link>
      <description><![CDATA[朋友们好！ 我目前正在尝试使用 DQN 解决出租车问题，我可以在日志中清楚地看到代理正在学习。然而，一个奇怪的现象正在发生：虽然代理在训练期间（常数 epsilon = 0.3）获得了不同的分数（在 -250 和 +9 之间），但在验证期间只有好或坏（当然 epsilon = 0）。我得到 -200 或正值作为分数。我使用一个 3 层的简单网络，lr 为 0.001。状态通过 one-hot 编码传递给代理。除此之外，它是具有经验回放（100,000 大小）和批量大小为 128 的标准 DQN。 以下是日志的摘录（这是 1000 集训练后的最后一次评估）：&lt; /p&gt; ----------------评估---------------- 第 1 集 |得分：12 |步骤：9 |损失：0 |持续时间：0.002709 | Epsilon：0 第 2 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.031263 | Epsilon：0 第 3 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.019805 | Epsilon: 0 第 4 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.015337 | Epsilon：0 第 5 集 |得分：9 |步骤：12 |损失：0 |持续时间：0.000748 | Epsilon：0 第 6 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.014757 | Epsilon: 0 第 7 集 |得分：8 |步骤：13 |损失：0 |持续时间：0.001071 | Epsilon: 0 第 8 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.029834 |厄普西隆：0 第 9 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.049129 |厄普西隆：0 第 10 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.016023 |厄普西隆：0 第 11 集 |得分：11 |步骤：10 |损失：0 |持续时间：0.000647 |厄普西隆：0 第 12 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.01529 |厄普西隆：0 第 13 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.019418 |厄普西隆：0 第 14 集 |得分：6 |步骤：15 |损失：0 |持续时间：0.002647 |厄普西隆：0 第 15 集 |得分：6 |步骤：15 |损失：0 |持续时间：0.001612 |厄普西隆：0 第 16 集 |得分：9 |步骤：12 |损失：0 |持续时间：0.001429 |厄普西隆：0 第 17 集 |得分：5 |步骤：16 |损失：0 |持续时间：0.00137 |厄普西隆：0 第 18 集 |分数：-200 |步数：200 |损失：0 |持续时间：0.022115 |厄普西隆：0 第 19 集 |得分：8 |步骤：13 |损失：0 |持续时间：0.001074 |厄普西隆：0 第 20 集 |得分：9 |步骤：12 |损失：0 |持续时间：0.001218 |埃普西隆：0 平均。剧集（评估）得分：-95.85  你们有人知道原因吗？或者我该如何修复它？   由   提交/u/MarcoX0395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5ci7f/taxiv3_and_dqn/</guid>
      <pubDate>Tue, 16 Apr 2024 10:13:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 PettingZoo 进行自定义并行环境训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c5acxd/custom_parallel_environment_training_with/</link>
      <description><![CDATA[大家好， 我了解了所有有关环境创建的教程，并且我已经制作了自己的并行环境，它通过并行测试API成功。但是，我没有找到任何有关如何在环境中训练代理的资源。我的意思是，PettingZoo 文档中没有明确的说明来解释如何训练自定义并行环境。到目前为止我所尝试的一切都没有奏效。如果有人帮助我，我将非常感激，我已经花了整整一周的时间来培训我的特工。提前谢谢您。   由   提交 /u/PutridBlood2123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c5acxd/custom_parallel_environment_training_with/</guid>
      <pubDate>Tue, 16 Apr 2024 07:44:19 GMT</pubDate>
    </item>
    </channel>
</rss>