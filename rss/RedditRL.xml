<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 04 Dec 2024 12:35:50 GMT</lastBuildDate>
    <item>
      <title>帮助 - 扫雷 RL 执行重复操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6e6xo/help_minesweeper_rl_executing_repeated_actions/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6e6xo/help_minesweeper_rl_executing_repeated_actions/</guid>
      <pubDate>Wed, 04 Dec 2024 11:49:48 GMT</pubDate>
    </item>
    <item>
      <title>在多代理环境中训练代理的最佳方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6dvqo/best_way_to_train_agents_in_multi_agent/</link>
      <description><![CDATA[我正在开展一个国际象棋 RL 项目，其中 2 个使用不同算法训练的代理相互对抗。我想知道训练代理的最佳方法是什么。我应该让他们互相对抗，分别训练代理对抗对手的随机动作，还是让他们分别训练，让游戏中的两个对手都使用相同的算法。并且建议会很有帮助     提交人    /u/Livid-Ant3549   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6dvqo/best_way_to_train_agents_in_multi_agent/</guid>
      <pubDate>Wed, 04 Dec 2024 11:29:40 GMT</pubDate>
    </item>
    <item>
      <title>教育部研究主任</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6dtv2/moe_rl/</link>
      <description><![CDATA[是否可以将混合专家 (MoE) 与强化学习 (RL) 结合起来？训练一个可以根据输入选择激活哪个或哪些专家的代理是否有意义？ 我有一个更复杂的想法：我想将 MoE 和 RL 与低秩自适应 (LoRA) 集成在一起。计划是拥有多个 LoRA 模块，并让代理根据输入识别最合适的模块。我打算将这种方法应用于各种 NLP 任务。这有意义吗？    提交人    /u/KevinBeicon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6dtv2/moe_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 11:26:05 GMT</pubDate>
    </item>
    <item>
      <title>基于 Symphony S2 的 DDPGII</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h69m82/ddpgii_based_on_symphony_s2/</link>
      <description><![CDATA[        提交人    /u/Timur_1988   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h69m82/ddpgii_based_on_symphony_s2/</guid>
      <pubDate>Wed, 04 Dec 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>[CartPole-v1] Vanilla PG 损失增加，但总奖励也增加。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h65o4z/cartpolev1_vanilla_pg_loss_increases_while_total/</link>
      <description><![CDATA[从这个简单的健身环境开始学习 RL。我观察到 PG 损失和奖励有相同的趋势。我难道不应该预期损失减少而奖励增加吗？ 我使用以下命令计算了一批 (状态、动作、奖励) 三元组的损失： loss = -torch.mean(logits * rewards)  奖励是使用带折扣的奖励公式计算的，类似于： out = torch.zeros(size=rewards.size(), dtype=torch.float32) out[-1] = rewards[-1] for i in range(rewards.size(-1)-1)[::-1]: out[i] = rewards[i] + r * out[i+1]     submitted by    /u/encoreway2020   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h65o4z/cartpolev1_vanilla_pg_loss_increases_while_total/</guid>
      <pubDate>Wed, 04 Dec 2024 02:41:33 GMT</pubDate>
    </item>
    <item>
      <title>除了样本复杂性之外，还有其他理由使用基于模型的 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</link>
      <description><![CDATA[当我们能够进行大规模环境并行化时，使用基于模型的算法是否真的有意义？ 基本上，我想知道是否存在像 DreamerV3 这样的算法可以解决而 PPO 无法解决的环境？例如，DreamerV3 论文表明，PPO 和 IMPALA 无法解决最困难的 Minecraft 任务，但如果有大规模计算，PPO 最终会解决这些任务吗？ 除了降低样本复杂度之外，还有其他理由使用基于模型的算法吗？    提交人    /u/vandelay_inds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</guid>
      <pubDate>Wed, 04 Dec 2024 01:39:56 GMT</pubDate>
    </item>
    <item>
      <title>“BALROG：对游戏上的 Agentic LLM 和 VLM 推理进行基准测试”，Paglieri 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h63tu6/balrog_benchmarking_agentic_llm_and_vlm_reasoning/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h63tu6/balrog_benchmarking_agentic_llm_and_vlm_reasoning/</guid>
      <pubDate>Wed, 04 Dec 2024 01:11:04 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型的算法合谋”，Fish 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h63tjd/algorithmic_collusion_by_large_language_models/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h63tjd/algorithmic_collusion_by_large_language_models/</guid>
      <pubDate>Wed, 04 Dec 2024 01:10:40 GMT</pubDate>
    </item>
    <item>
      <title>如何处理复杂、嵌套的动作空间。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5zgvq/how_to_deal_with_complex_nested_action_spaces/</link>
      <description><![CDATA[我正在开发一个可以与 miniwob++ 体育馆环境配合使用的代理。我从未使用过像这样的非平面动作空间，并且想知道是否有人对制定它的最佳方法有任何指导，因为根据要采取的操作，需要不同的参数。具体来说，如果相关的话，尝试使用 Pytorch 来解决这个问题。    提交人    /u/m_js   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5zgvq/how_to_deal_with_complex_nested_action_spaces/</guid>
      <pubDate>Tue, 03 Dec 2024 21:59:50 GMT</pubDate>
    </item>
    <item>
      <title>狄利克雷掩蔽动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5wd0z/dirichlet_masked_action_space/</link>
      <description><![CDATA[嗨， 我对强化学习中的掩码动作有点陌生。我有一个强化学习问题，其中我的动作空间总和为 1（资源分配问题），但随着时间步长 t 的增加，我可用的资源越来越少，即前 t 个索引必须等于零。 我尝试使用狄利克雷分布并将环境中的第一个索引归零，但我一直得到次优结果。 我的奖励函数是两个项的凸组合 - 一个尝试优化可信度，另一个尝试优化盈利能力。对于两个特定场景，我知道什么是最优的，但在所有其他场景中我都不知道。 我正在使用 RLLIB，并且我尝试使用 PPO（基于注意力的模型）和 SAC。不知何故，SAC 比 PPO 效果更好，我不确定为什么，但仍然不是最优的。 问问想法我做错了什么或者我应该怎么做？ 提前谢谢！    提交人    /u/Brief-Host-6484   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5wd0z/dirichlet_masked_action_space/</guid>
      <pubDate>Tue, 03 Dec 2024 19:52:43 GMT</pubDate>
    </item>
    <item>
      <title>小型 LLM 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5l312/small_sized_llm_models/</link>
      <description><![CDATA[嗨，我知道这是 RL 子版块，但有人知道任何小型 LLM 预训练模型吗，比如 5-12 GB 大小的模型。 它应该能够回答非常基本的问题，仅此而已。如果是这样，请分享 提前致谢 :))    提交人    /u/Wide-Chef-7011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5l312/small_sized_llm_models/</guid>
      <pubDate>Tue, 03 Dec 2024 11:12:26 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到 Gym 环境中 RL 算法的可靠基准？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5a7iw/where_can_i_find_reliable_benchmarks_for_rl/</link>
      <description><![CDATA[首先我要说的是，我对 RL 还很陌生。我之前的工作是法学硕士 (LLM)，通常根据模型在给定基准上的表现对模型进行排名和堆叠（这些基准非常重要，初创公司仅凭他们的分数就能赚到很多钱）。 最近开始在 MuJoCo 环境中训练模型，我想弄清楚我的算法是否表现得还不错。当然，我可以使用 SB3 的默认 PPO 和 MlpPolicy 让 Ant-v5 行走，但它真的有多好？ 是否有一些基准或存储库，我可以将我的结果与使用默认 MuJoCo（或任何其他健身房）奖励函数的其他人算法的学习曲线进行比较？当然，假设我们使用相同的环境和奖励函数，但鉴于 Gymnasium 很受欢迎并提供良好的默认设置，我想应该有很多数据可用。 我在 google 上搜索过，只找到了稀疏的结果。为什么 RL 中的基准不如 LLM 中的基准那么大？    提交人    /u/geepytee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5a7iw/where_can_i_find_reliable_benchmarks_for_rl/</guid>
      <pubDate>Tue, 03 Dec 2024 00:15:28 GMT</pubDate>
    </item>
    <item>
      <title>现在人们对基因算法的关注度是否下降了？如果是这样，原因是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h59gl2/is_there_less_attention_towards_genetic_algos_now/</link>
      <description><![CDATA[遗传算法 (GA) 已经存在很长时间了（大约在 20 世纪 60 年代）。它们似乎既非常直观，又特别适用于黑箱问题，但它们目前并不是“主流”。在 2017 年，OpenAI 对进化算法非常看好，并列举了它们的优点，包括可并行化、稳健，并且能够处理具有不明确的值/适应度函数的长期问题。有没有更新的最新信息？哪些算法正在击败 GA？ 对于低维问题，贝叶斯优化可能具有更好的统计保证/渐近性。GA 有任何保证吗，还是我们完全不知道？    提交人    /u/zarmesan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h59gl2/is_there_less_attention_towards_genetic_algos_now/</guid>
      <pubDate>Mon, 02 Dec 2024 23:41:34 GMT</pubDate>
    </item>
    <item>
      <title>RL 问题 - 具有无限块（不同颜色）的 2D 网格</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h566gp/rl_problem_2d_grid_with_endless_pieces_of_varying/</link>
      <description><![CDATA[大家好， 我在工作中遇到了一个非常先进且困难的问题。我遇到了这样一个问题场景：我有这个 2D 离散网格系统。这个网格的大小可以变化。除此之外，我还有多个可以放入这些网格中的不同方框。每个方框的侧面可以有不同的纯色。当两个不同的方框相邻时，我需要找到最大化相同颜色相邻的最佳位置。 我一直在研究 AlphaZero 方法，但对于我的方法，我无法指定零件数量，并且每个方框可以有不同的颜色。关于如何解决这个问题有什么建议吗？    提交人    /u/Ill_Paper_6854   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h566gp/rl_problem_2d_grid_with_endless_pieces_of_varying/</guid>
      <pubDate>Mon, 02 Dec 2024 21:22:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DreamerV3 的炒作程度不如 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</link>
      <description><![CDATA[据我所知，PPO 通常是强化学习任务的首选算法。为什么不改用 DreamerV3？它似乎更稳定，并且需要更少的超参数调整。    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</guid>
      <pubDate>Mon, 02 Dec 2024 01:22:39 GMT</pubDate>
    </item>
    </channel>
</rss>