<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 31 Mar 2024 03:14:31 GMT</lastBuildDate>
    <item>
      <title>d4rl 中的迷宫环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1brriu9/maze_environments_in_d4rl/</link>
      <description><![CDATA[在 D4RL 网站上，它说观察包含期望目标和实现目标的字典，但是当我在 d4rl 上执行 qlearning_dataset 时，它只显示球的位置和速度（4 个暗向量）我看到了目标条件的包装器，但当前的算法似乎使用 qlearning_dataset，所以我不明白球如何导航到目标？ minari 的好处是什么强化学习？   由   提交 /u/drainageleak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1brriu9/maze_environments_in_d4rl/</guid>
      <pubDate>Sat, 30 Mar 2024 20:30:49 GMT</pubDate>
    </item>
    <item>
      <title>必须学rl还是sb3就够了</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bro2n6/do_you_have_to_learn_rl_or_is_sb3_enough/</link>
      <description><![CDATA[我知道普通策略梯度和 dqn 然后发现了 sb3 并且刚刚使用这些算法，但没有学习它们是如何工作的，学习它们仍然有帮助吗他们工作或不   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bro2n6/do_you_have_to_learn_rl_or_is_sb3_enough/</guid>
      <pubDate>Sat, 30 Mar 2024 18:03:03 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1brj1r2/ppo_implementation/</link>
      <description><![CDATA[      社区您好， 我需要您的帮助来了解PPO实现的更新功能是否正确。我的场景涉及基于图形的状态（动态大小），以及具有选择器层来选择选项的参与者。然后，相应的选项层选择一个动作。我有三个选项，因此有三个选项层。两个选项层具有离散输出，一个选项层具有多头输出。你能帮我纠正这个问题吗？  select_action和update函数如下： 参与者网络快照 Select_action函数 ​ 更新函数 - 第 1 部分 ​ 更新函数 - 第 2 部分  ​ 更新函数 -第三部分   由   提交/u/GuavaAgreeable208  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1brj1r2/ppo_implementation/</guid>
      <pubDate>Sat, 30 Mar 2024 14:23:18 GMT</pubDate>
    </item>
    <item>
      <title>我想训练一个名为 halfcheetah 的 mujoco 模型，有人可以给我一些关于如何处理无限观察范围的建议吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1brbprs/i_want_to_train_a_mujoco_model_known_as/</link>
      <description><![CDATA[ 由   提交 /u/pratyaksh__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1brbprs/i_want_to_train_a_mujoco_model_known_as/</guid>
      <pubDate>Sat, 30 Mar 2024 07:04:10 GMT</pubDate>
    </item>
    <item>
      <title>“TextCrraftor：您的文本编码器可以成为图像质量控制器”，Li 等人 2024 {Snapchat}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1br4p9j/textcraftor_your_text_encoder_can_be_image/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1br4p9j/textcraftor_your_text_encoder_can_be_image/</guid>
      <pubDate>Sat, 30 Mar 2024 00:48:49 GMT</pubDate>
    </item>
    <item>
      <title>DIAMBRA Arena 环境用于让 OpenAI 和 MistralAI 法学硕士在旧金山 MistralAI 黑客马拉松的《街头霸王 III》中相互对战，由两家 YC 初创公司提供</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqpf2b/diambra_arena_environments_used_to_make_openai/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqpf2b/diambra_arena_environments_used_to_make_openai/</guid>
      <pubDate>Fri, 29 Mar 2024 13:34:00 GMT</pubDate>
    </item>
    <item>
      <title>muzero 对超参数非常敏感吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqlw32/is_muzero_insanely_sensitive_to_hyperparameters/</link>
      <description><![CDATA[50 多个小时以来，我一直在尝试使用各种开源实现来复制 muzero 结果。我几乎尝试了我能够找到并运行的所有实现。在所有这些实现中，我成功地看到 muzero 收敛了一次，找到了在 5x5 网格中行走的策略。在那次运行之后我无法复制它。我还没有设法让它学会玩 tic tac 游戏，目的是在任何公开可用的实现上绘制游戏。我最好的成绩是50%的成功率。我对我能做到的每个参数都坐立不安，但它几乎没有产生任何结果。 我错过了什么吗？ muzero 对超参数非常敏感吗？是否有一些论文或实现中未明确提及的秘密知识使其发挥作用？   由   提交/u/drblallo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqlw32/is_muzero_insanely_sensitive_to_hyperparameters/</guid>
      <pubDate>Fri, 29 Mar 2024 10:25:14 GMT</pubDate>
    </item>
    <item>
      <title>PPO收敛快，奖励增长慢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqkuaf/ppo_converges_quickly_rewards_increasing_slowly/</link>
      <description><![CDATA[        由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqkuaf/ppo_converges_quickly_rewards_increasing_slowly/</guid>
      <pubDate>Fri, 29 Mar 2024 09:16:56 GMT</pubDate>
    </item>
    <item>
      <title>再现割线</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqi4g0/reproducing_secant/</link>
      <description><![CDATA[嗨！ 最近，我一直在尝试在《Secant：自我专家克隆》中重现训练过程用于视觉策略的零样本泛化，这可以被视为一种知识蒸馏。该项目发布在 GitHub 上，但没有训练过程，只有一些基准环境和包装器。 我在使用 Pytorch 编写训练过程时遇到了一些困难。所以我想向这个社区的人们寻求帮助。有没有人在另一个可以参考的开源项目上尝试过这个过程？ 提前感谢您检查这篇文章。    ;由   提交/u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqi4g0/reproducing_secant/</guid>
      <pubDate>Fri, 29 Mar 2024 06:10:22 GMT</pubDate>
    </item>
    <item>
      <title>计算门槛低的小问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqh891/small_problems_with_low_computational_barrier_to/</link>
      <description><![CDATA[有哪些小的（不像超级计算机那样要求很高）开放式强化学习 (RL) 问题？一个非例子是玩星际争霸（这需要大量的计算资源）。另一个非例子是“解决”问题。通过 RL 进行井字游戏，搜索空间很小。    由   提交/u/fool126  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqh891/small_problems_with_low_computational_barrier_to/</guid>
      <pubDate>Fri, 29 Mar 2024 05:13:09 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习研究是否因不公平比较而受到损害？你的想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqdx9w/is_deep_reinforcement_learning_research_being/</link>
      <description><![CDATA[深度强化学习中我们需要一个开放的分层基准测试系统，例如 ImageNet。   由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqdx9w/is_deep_reinforcement_learning_research_being/</guid>
      <pubDate>Fri, 29 Mar 2024 02:21:36 GMT</pubDate>
    </item>
    <item>
      <title>Ray PPO 与许多其他 PPO 相比 - 为什么 Ray 更好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bq2en6/ray_ppo_vs_many_other_ppos_why_is_ray_so_much/</link>
      <description><![CDATA[我一直在 minworld 迷宫（纯粹基于视觉，使用地图覆盖范围作为奖励以及奖励框）和 ray 上测试 PPO实现突飞猛进。 我测试了sheeprl、cleanrl、sb3，但它们都一次又一次地碰壁，而ray的实现似乎像人类一样穿越迷宫。我通读了代码，看看到底是什么让 ray 变得更好，并且进行了消融研究，如果去掉 kl 损失项，ray 似乎会很困难，但在 cleanrl 中添加 kl 损失似乎根本没有帮助。  我还将 cleanrl 的 kl 近似值更改为真正的 kl div 损失，这似乎对它开始上升到较高的 ish 奖励然后下降到非常不智能的碰壁策略有所帮助。或绕小圈子。 我还将我的小批量大小设置为过大的数量（例如整个剧集 - 通过累积小批量的损失来完成，以在最后更新优化器）看看是否是小批量噪声，但似乎也不是。 在这样做时，我意识到 PPO 对实现和超参数有多么敏感，但我真的不明白为什么人们当它如此脆弱且变化无常时，请将其保留为通用的惊人算法。 鉴于 ray 糟糕的可读性，我几乎要抓狂了。有人用 cleanrl 单文件风格实现了 ray 的 ppo 吗？或者有谁知道 ray 使用其他流行库不使用的任何特定代码级别优化（为了智能，而不是速度或内存）？ 我可以提供性能图表、视频或实验详细信息，如果需要，但目前我不确定我能提供什么单一的东西可以帮助回答这个问题。    由   提交/u/dagangsta2012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bq2en6/ray_ppo_vs_many_other_ppos_why_is_ray_so_much/</guid>
      <pubDate>Thu, 28 Mar 2024 18:12:33 GMT</pubDate>
    </item>
    <item>
      <title>大规模强化学习的基础模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpy0ji/foundation_models_for_largescale_rl/</link>
      <description><![CDATA[针对大规模强化学习定制的基础模型有什么显着的进展吗？强化学习落后的一个主要原因是强化学习似乎与非常深的模型不兼容。   由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpy0ji/foundation_models_for_largescale_rl/</guid>
      <pubDate>Thu, 28 Mar 2024 15:13:59 GMT</pubDate>
    </item>
    <item>
      <title>用于能源管理的 DRL/寻找船舶发电机、电池和电网（充电站）之间的最佳负载平衡点，想法、示例、讨论。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bptiw5/drl_for_energy_managementfinding_optimal_load/</link>
      <description><![CDATA[   大家好。 我刚刚发现并加入了这个社区，我希望我可以讨论我正在进行的研究生研究项目。我希望任何人都可以提供想法、代码示例或想法。 问题在这篇文章的标题中进行了描述。我目前正在开展一个项目，管理对接船发电机负载平衡以及岸上电池和岸上电网。目标是找到最佳负载平衡点来决定/切换船舶发电机与岸上电池和岸上电网的连接。例如，如果船舶发电机每小时需要 50 千瓦为船舶供电，并由岸上电池充电，但船舶每小时只需要 25 千瓦，电池可能会过载，那么在什么时候切换连接陆上电网？文字描述可能会令人困惑；请参见图1。 我正在尝试在这项研究中使用DQN；但是，欢迎任何扩展组合 DRL 算法建议。我尝试使用 DRL 示例来应用最优价格和最优利润来解决这个问题，但没有成功。我也刚刚开始为这个研究项目学习强化学习和 DRL。尽管我对 ML/DL 有一些了解，但我还是 RL/DRL 领域的新手。我正在寻求意见、建议、任何代码示例或示例项目以及想法。 谢谢。 UPDATE_1：所以我向该项目发送了电子邮件领导者和项目领导者的顾问以获得有关问题和困惑的答案。我刚刚收到回复。显然存在一些沟通不畅，无论如何，重点不是找到负载平衡点，而是让发电机工作在最佳点。如果发电机产生更多的电力（工作高于负载），那么我们共享/为电池充电，如果电池已充电，那么我们共享/为电网充电。如果发电机功率小于负载，我们就从电池取电；如果电池放电了，我们就将其从电网中取出。负载会随机变化，研究方法旨在根据电池约束来管理功率，例如充电状态和电池的功率限制（充电限制）。由于图2是图1的修改版，我必须删除图1才能上传新图。抱歉。 https:// Preview.redd.it/wp2cds3jj9rc1.jpg?width=1535&amp;format=pjpg&amp;auto=webp&amp;s=854ed4b4d5230e65fcc9b281058f8ee3ab460246 图2：修改图。目的是使船舶柴油发电机工作在最佳点。   由   提交 /u/jyangcm   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bptiw5/drl_for_energy_managementfinding_optimal_load/</guid>
      <pubDate>Thu, 28 Mar 2024 11:43:52 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>