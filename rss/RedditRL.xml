<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 28 May 2024 06:20:21 GMT</lastBuildDate>
    <item>
      <title>通过克罗内克积证明价值函数的梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2brn9/proof_of_gradient_of_value_function_via_kronecker/</link>
      <description><![CDATA[您好，我对赵世宇强化学习数学基础中发现的证明有疑问。  我将其发布在 stackexchange  因为我认为格式化会更容易。   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2brn9/proof_of_gradient_of_value_function_via_kronecker/</guid>
      <pubDate>Tue, 28 May 2024 05:13:50 GMT</pubDate>
    </item>
    <item>
      <title>训练 DQN 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d2bn3k/training_dqn_agent/</link>
      <description><![CDATA[您好，我正在定制环境中工作，为此我没有得到训练 DQN 代理的内容。 任何教程或线索拜托？ 谢谢   由   提交/u/ComplianceOil566   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d2bn3k/training_dqn_agent/</guid>
      <pubDate>Tue, 28 May 2024 05:05:57 GMT</pubDate>
    </item>
    <item>
      <title>v2 和 v4 MuJoCO 环境 D4RL 之间的差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d278ah/difference_between_v2_and_v4_mujoco_environments/</link>
      <description><![CDATA[我想知道 D4RL 基准测试中提供的离线 RL MuJoCo-v2 数据集是否与 v3 和 v4 版本兼容。如果我最终想使用这些数据集 (D4RL mujoco v2) 训练模型，然后使用相应的 v3 或 v4 版本在线测试模型，那么跨版本的奖励和环境动态是否仍然相同？谢谢   由   提交/u/Pspecial_Rip_6148   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d278ah/difference_between_v2_and_v4_mujoco_environments/</guid>
      <pubDate>Tue, 28 May 2024 00:58:42 GMT</pubDate>
    </item>
    <item>
      <title>将 GNN 集成到 RL 模型中</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d24iht/integrating_gnns_in_rl_models/</link>
      <description><![CDATA[大家好， 我目前正在撰写关于优化动态变化的服务器网络的学士论文。为了对网络结构进行建模，我使用了一个图表，这使得 GNN 的使用在这里非常诱人。虽然我在 GNN 和 RL 方面拥有扎实的背景（主要是理论，大多数算法都是从头开始编写的），但我从未将两者结合起来。所以我想知道您是否有将 GNN 与稳定基线等常见 RL 库结合使用进行 RL 的经验？   由   提交/u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d24iht/integrating_gnns_in_rl_models/</guid>
      <pubDate>Mon, 27 May 2024 22:44:55 GMT</pubDate>
    </item>
    <item>
      <title>我的深度 Q 学习正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d23cek/am_i_doing_deep_qlearning_right/</link>
      <description><![CDATA[我正在尝试通过深度 q-learning 解决 OpenAI Gym cartpole 问题，我只是从头开始写了一些东西，但我不知道如果它做正确的事。有更了解 RL 的人来帮忙吗？这就是我正在编写的脚本，它是一个“双重深度 q-learning”：  https://github.com/rlucasfm/Q-learning-tf/blob/master/double_dqn_tf.py。 同一存储库上有一个用于简单深度 Q 学习的脚本。&lt; /p&gt;   由   提交 /u/WitnessedWrath   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d23cek/am_i_doing_deep_qlearning_right/</guid>
      <pubDate>Mon, 27 May 2024 21:51:30 GMT</pubDate>
    </item>
    <item>
      <title>可变大小的状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1w9rn/variablesized_states/</link>
      <description><![CDATA[在我的项目中，我试图了解应用程序 UI 的状态。在任何给定的操作之后，应用程序都会向我提供有关 UI 中发生的一个或多个更改的数据（组件更改、添加/删除了哪些新组件等），我可以从中构建状态。 问题是 - 会话之间对话框和组件的数量和身份可能会有很大差异，并且不知道可能添加哪些组件。 处理这个问题的好方法是什么？零填充并不实用，因为组件的数量可能非常大，而且我们已经为每个组件提取了大量数据。而且，这意味着对应用程序的任何新更改都需要从头开始重新训练。   由   提交/u/CJIsABusta  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1w9rn/variablesized_states/</guid>
      <pubDate>Mon, 27 May 2024 16:59:58 GMT</pubDate>
    </item>
    <item>
      <title>我可以用 4 个 A30 进行对齐吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1njta/can_i_do_alignment_with_4_a30/</link>
      <description><![CDATA[实验室的主要焦点是 RL，但我发现 rlhf 对我更有吸引力。 好吧，我手上有 4xA30 24G，是可以对 6B-8B LLMalignmnet 进行研究吗？ 任何回复都会有帮助。   由   提交/u/siyuan01  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1njta/can_i_do_alignment_with_4_a30/</guid>
      <pubDate>Mon, 27 May 2024 09:21:49 GMT</pubDate>
    </item>
    <item>
      <title>如何确定重播缓冲区的最小大小</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</link>
      <description><![CDATA[我正在 Unity 中训练 dqn，它使用 [偏航、横向矢量和前向矢量] 作为我的操作空间来控制游戏对象。我读到人们在开始训练之前会为他们的重播缓冲区获得最少的经验数量（在某些帖子中，读到这个最小值高达 10000）。我发现我的模型基本上为每次推理都提供了相同的动作，因为它现在完全未经训练。在我看来，在开始训练之前收集大量这样的数据大部分是多余的，因为它并没有真正尝试各种动作。如果我的批量大小是 64，如果我想使用重播缓冲区（假设 256）的经验开始训练，有什么我应该注意的吗？我假设这将有助于我尽早增加训练数据的多样性。 我是个菜鸟，所以我非常愿意被推荐到可能与此相关的论文，提前谢谢。    提交人    /u/ProtectionFrosty5393   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</guid>
      <pubDate>Mon, 27 May 2024 04:55:29 GMT</pubDate>
    </item>
    <item>
      <title>我可以在电子商务网站中使用强化学习进行产品推荐吗？有关于这个主题的可用资源吗？或任何其他最佳选择。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</link>
      <description><![CDATA[嗨，我是一家餐饮/电子商务服务提供商的实习生，我被指派为 The F&amp;B/E-commerce 服务提供商创建推荐引擎。 B和电子商务网站，建议我一些关于基于RL的推荐系统的好的指导材料或任何其他效果好的方法，注意：数据有限哪个可能是最好的？   由   提交/u/Educational-Town-710   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</guid>
      <pubDate>Mon, 27 May 2024 02:53:59 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关构建具有电池和电网互动的能源管理 RL 环境的建议和见解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</link>
      <description><![CDATA[大家好！ 我正在参与一个项目，重点是使用强化学习来模拟和优化能源管理系统。目标是管理电网和可再生能源的能源存储和消耗，并密切关注实时定价和能源需求。 这种设置代表了我在给定情况下提出的最佳方法。数据集和项目的紧急情况。（我愿意接受任何想法） 系统概述： 状态空间包括：  两个电池的充电状态 (SoC)。 当前电网电价。 能源使用的历史和预测数据。 &lt; li&gt;未来 6 小时的可再生能源发电预测。 一天中的时间和一周中的日期指示器。  行动空间涉及：  确定每个电池充电的电量。 （2 个连续值） 安排充电操作的开始时间（选项范围从立即到延迟 6 小时）。  数据和迭代： 我已经转换了我的数据集，以表示每小时的所有相关信息，例如负载、价格和可再生能源预测。环境中的每个模拟步骤对应一小时的实时时间，其中模型根据当前状态预测开始充电的最佳时间以及充电量。 主要挑战：&lt; /strong&gt; 延迟操作： 例如，如果 2024 年 1 月 1 日 00:00:00 决定在 5 小时后开始充电，我应该如何：  考虑操作对系统的延迟影响来计算奖励？ 更新模型的预测： 针对几小时后影响系统的操作，确定最佳方案模型重新评估和做出新预测的时间令人困惑。 学习的数据迭代： 假设环境的每个步骤处理来自转换后的数据集的一小时的数据：&lt; /p&gt; 如何确保 RL 模型有效地迭代数据集以实现最大程度的学习？ 建议采用哪些策略来处理这种每小时数据的连续流，特别是在集成操作的延迟效果时模型的学习过程？ 我正在寻找有关在强化学习环境中管理延迟操作和有效数据迭代的见解、建议或资源。 感谢您的指导和时间!   由   提交 /u/Nnarruqt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</guid>
      <pubDate>Sun, 26 May 2024 13:20:52 GMT</pubDate>
    </item>
    <item>
      <title>如何改进深度 RL 交易设置，使其在 1 小时时间范围内运行良好，但在 100 万时间范围内效果不佳？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d112xj/how_to_improve_a_deep_rl_setup_for_trading_that/</link>
      <description><![CDATA[嗨， 几个月来，我一直在研究如何设置 RL 模型进行交易。 无需详细了解设置本身（本质上我能够轻松配置我想要测试的所有参数），我有 RL 模型，我可以将其提供给经过处理的时间序列并让它们执行操作。 &lt; p&gt;到目前为止，我一直在针对 BTCUSDT 进行测试，主要是在 1 小时时间范围内，假设复利，我可以以 2 倍左右的速度击败 HODL（所以我的测试数据是 2024 年 1 月至 4 月，其中 HODL 似乎得到大约 41,000 美元，而我的模型可以达到 &gt; 81,000 美元）。 这还假设每次买卖都会产生 %0.1 的费用（以模拟经纪商当前的 SPOT 费用）。 大多数模型的交易都没有错误（每笔交易都盈利）。 现在，这一切看起来都很有希望，但有两个问题：  1) 大多数模型在这 4 个月内进行大约 60-90 笔交易，这意味着有时每 2 天只有一笔交易。对于在现实生活中与经纪人进行测试来说，这是一个问题，因为我必须等待很长时间才能看到任何操作。 2) 我尝试在 1m 时间范围上训练相同的精确设置，但结果远不及 1 小时。我尝试了许多配置（例如显示 1m + 1h 或 1m + 1h + 1d 时间范围），但似乎要处理的数据量增加，大大降低了模型学习方式的影响（事实上，在很多情况下模型执行 0 个动作）。使用学习率会有所帮助 - 但我似乎永远无法达到 1 小时帧得到的结果。 2 个问题： 1）有人有关于如何进行的任何提示吗？处理如此高频的数据，为什么与 1 小时的结果相比会有如此大的差异？ （我们甚至不讨论 1 秒的时间范围 :) ）  2）看来我开发的奖励系统运行良好，我很高兴讨论它，但也许有人知道如何做激励 RL 模型进行更多交易？在大多数情况下，这些模型似乎倾向于更大/更安全的波动，而不是更频繁地交易，这将显示复利的力量。我最近读到了有关多重奖励系统（矢量化奖励）的内容，但没有一个可用的库支持它（线性“近似”它本质上是我现在正在做的事情，但实际上并不是同一件事）。 感谢您就此事提供的任何意见或讨论。 PS。我还为经纪人配置了自动交易设置，我目前正在其上运行 1 小时模拟（在他们的测试环境中），但该环境不是最好的（由于那里处理交易的方式），所以我只是可能必须在那里上线并进行测试。   由   提交 /u/cloudjubei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d112xj/how_to_improve_a_deep_rl_setup_for_trading_that/</guid>
      <pubDate>Sun, 26 May 2024 13:12:53 GMT</pubDate>
    </item>
    <item>
      <title>学士论文游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</link>
      <description><![CDATA[嘿，我想为我的计算机科学学士学位论文训练一个人工智能来玩强化学习游戏。 我还没有强化学习的经验。 我可以选择哪些当时可行的游戏？   由   提交/u/TMG_Indi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</guid>
      <pubDate>Sun, 26 May 2024 08:40:15 GMT</pubDate>
    </item>
    <item>
      <title>环境复杂性与最优策略收敛的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</link>
      <description><![CDATA[大家好，是否有一些关于环境复杂性与学习到的最优策略本身之间关系的文献？例如，如果一个环境是由“世界模型”中的VAE生成的，那么环境复杂度和策略之间的关系是什么？   由   提交/u/Main_Pressure271   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</guid>
      <pubDate>Sun, 26 May 2024 06:58:00 GMT</pubDate>
    </item>
    <item>
      <title>经常性 SAC 指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</link>
      <description><![CDATA[我一直在尝试了解有关 LSTM 在 POMDP 强化学习中如何发挥作用的更多信息。我专门尝试与 SAC 合作，想知道是否有关于该主题的一些好的资源。    由   提交 /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</guid>
      <pubDate>Sun, 26 May 2024 06:43:50 GMT</pubDate>
    </item>
    <item>
      <title>最优随机策略是否存在？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</link>
      <description><![CDATA[我知道在 MDP 中总是存在唯一的最优确定性策略。对于最优随机策略也存在这样的说法吗？是否总是存在唯一的最优随机策略？它能比最优确定性策略更好吗？我想我不太明白。 谢谢！   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</guid>
      <pubDate>Sun, 26 May 2024 06:07:26 GMT</pubDate>
    </item>
    </channel>
</rss>