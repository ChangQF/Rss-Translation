<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 26 May 2024 18:17:25 GMT</lastBuildDate>
    <item>
      <title>寻求有关通过电池和电网交互构建能源管理 RL 环境的建议和见解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</link>
      <description><![CDATA[大家好！ 我正在参与一个项目，重点是使用强化学习来模拟和优化能源管理系统。目标是管理电网和可再生能源的能源存储和消耗，并密切关注实时定价和能源需求。 这种设置代表了我在给定情况下提出的最佳方法。数据集和项目的紧急情况。（我愿意接受任何想法） 系统概述： 状态空间包括：  两个电池的充电状态 (SoC)。 当前电网电价。 能源使用的历史和预测数据。 &lt; li&gt;未来 6 小时的可再生能源发电预测。 一天中的时间和一周中的日期指示器。  行动空间涉及：  确定每个电池充电的电量。 （2 个连续值） 安排充电操作的开始时间（选项范围从立即到延迟 6 小时）。  数据和迭代： 我已经转换了我的数据集，以表示每小时的所有相关信息，例如负载、价格和可再生能源预测。环境中的每个模拟步骤对应一小时的实时时间，其中模型根据当前状态预测开始充电的最佳时间以及充电量。 主要挑战：&lt; /strong&gt; 延迟操作： 例如，如果 2024 年 1 月 1 日 00:00:00 决定在 5 小时后开始充电，我应该如何：  考虑操作对系统的延迟影响来计算奖励？ 更新模型的预测： 针对几小时后影响系统的操作，确定最佳方案模型重新评估和做出新预测的时间令人困惑。 学习的数据迭代： 假设环境的每个步骤处理来自转换后的数据集的一小时的数据：&lt; /p&gt; 如何确保 RL 模型有效地迭代数据集以实现最大程度的学习？ 建议采用哪些策略来处理这种每小时数据的连续流，特别是在集成操作的延迟效果时模型的学习过程？ 我正在寻找有关在强化学习环境中管理延迟操作和有效数据迭代的见解、建议或资源。 感谢您的指导和时间!   由   提交 /u/Nnarruqt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</guid>
      <pubDate>Sun, 26 May 2024 13:20:52 GMT</pubDate>
    </item>
    <item>
      <title>如何改进深度 RL 交易设置，使其在 1 小时时间范围内运行良好，但在 100 万时间范围内效果不佳？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d112xj/how_to_improve_a_deep_rl_setup_for_trading_that/</link>
      <description><![CDATA[嗨， 几个月来，我一直在研究如何设置 RL 模型进行交易。 无需详细了解设置本身（本质上我能够轻松配置我想要测试的所有参数），我有 RL 模型，我可以将其提供给经过处理的时间序列并让它们执行操作。 &lt; p&gt;到目前为止，我一直在针对 BTCUSDT 进行测试，主要是在 1 小时时间范围内，假设复利，我可以以 2 倍左右的速度击败 HODL（所以我的测试数据是 2024 年 1 月至 4 月，其中 HODL 似乎得到大约 41,000 美元，而我的模型可以达到 &gt; 81,000 美元）。 这还假设每笔买卖都会产生 %0.1 的费用（以模拟经纪商当前的 SPOT 费用）。 大多数模型的交易都没有错误（每笔交易都盈利）。 现在，这一切看起来都很有希望，但有两个问题：  1) 大多数模型在这 4 个月内进行大约 60-90 笔交易，这意味着有时每 2 天只有一笔交易。对于在现实生活中与经纪人进行测试来说，这是一个问题，因为我必须等待很长时间才能看到任何操作。 2) 我尝试在 1m 时间范围上训练相同的精确设置，但结果远不及 1 小时。我尝试了许多配置（例如显示 1m + 1h 或 1m + 1h + 1d 时间范围），但似乎要处理的数据量增加，大大降低了模型学习方式的影响（事实上，在很多情况下模型执行 0 个动作）。使用学习率会有所帮助 - 但我似乎永远无法达到 1 小时帧得到的结果。 2 个问题： 1）有人有关于如何进行的任何提示吗？处理如此高频的数据，为什么与 1 小时的结果相比会有如此大的差异？ （我们甚至不讨论 1 秒的时间范围 :) ）  2）看来我开发的奖励系统运行良好，我很高兴讨论它，但也许有人知道如何做激励 RL 模型进行更多交易？在大多数情况下，模型似乎倾向于更大/更安全的波动，而不是更频繁地交易，这将显示复利的力量。我最近读到了有关多重奖励系统（矢量化奖励）的内容，但没有一个可用的库支持它（线性“近似”它基本上就是我现在正在做的事情，但实际上并不是同一件事）。 感谢您就此事提供的任何意见或讨论。 PS。我还为经纪人配置了自动交易设置，我目前正在其上运行 1 小时模拟（在他们的测试环境中），但该环境不是最好的（由于那里处理交易的方式），所以我只是可能必须在那里上线并进行测试。   由   提交 /u/cloudjubei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d112xj/how_to_improve_a_deep_rl_setup_for_trading_that/</guid>
      <pubDate>Sun, 26 May 2024 13:12:53 GMT</pubDate>
    </item>
    <item>
      <title>学士论文游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</link>
      <description><![CDATA[嘿，我想为我的计算机科学学士学位论文训练一个人工智能来玩强化学习游戏。 我还没有强化学习的经验。 我可以选择哪些当时可行的游戏？   由   提交/u/TMG_Indi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</guid>
      <pubDate>Sun, 26 May 2024 08:40:15 GMT</pubDate>
    </item>
    <item>
      <title>环境复杂性与最优策略收敛的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</link>
      <description><![CDATA[大家好，是否有一些关于环境复杂性与学习到的最优策略本身之间关系的文献？例如，如果一个环境是由“世界模型”中的VAE生成的，那么环境复杂度和策略之间的关系是什么？   由   提交/u/Main_Pressure271   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</guid>
      <pubDate>Sun, 26 May 2024 06:58:00 GMT</pubDate>
    </item>
    <item>
      <title>经常性 SAC 指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</link>
      <description><![CDATA[我一直在尝试了解有关 LSTM 在 POMDP 强化学习中如何发挥作用的更多信息。我专门尝试与 SAC 合作，想知道是否有关于该主题的一些好的资源。    由   提交 /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</guid>
      <pubDate>Sun, 26 May 2024 06:43:50 GMT</pubDate>
    </item>
    <item>
      <title>最优随机政策是否存在？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</link>
      <description><![CDATA[我知道在 MDP 中总是存在唯一的最优确定性策略。对于最优随机策略也存在这样的说法吗？是否总是存在唯一的最优随机策略？它能比最优确定性策略更好吗？我想我不太明白。 谢谢！   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</guid>
      <pubDate>Sun, 26 May 2024 06:07:26 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的矩阵（强化学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</link>
      <description><![CDATA[如果我希望代理显示 4 个空间，其中每个空间有 10 个组件，每个组件有 3 个变量。为观察空间定义一个矩阵是不是更好？因为这告诉我要做 ChatGPT    由   提交/u/Gullible_Capital_146   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</guid>
      <pubDate>Sun, 26 May 2024 04:02:53 GMT</pubDate>
    </item>
    <item>
      <title>“静息大脑标签记忆中的电‘涟漪’用于存储”：体验重播机制和选择睡眠期间优先重播的点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</guid>
      <pubDate>Sat, 25 May 2024 21:48:03 GMT</pubDate>
    </item>
    <item>
      <title>部分循环观察空间的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</link>
      <description><![CDATA[假设我有一个环境，其中代理必须在 2D 平面上移动以收集数量不等的硬币。代理可以在四个基本方向中的任何一个方向加速，并在收集硬币时获得 1 的奖励，之后将其从环境中移除。观察空间如下所示： 代理 x 代理 y 代理 vx 代理 vy [硬币 x, 硬币 y] * 硬币数量  我的假设是处理这个问题的方法是使用转换器 - 使用前馈网络对固定组件的表示进行编码，然后对每个非固定组件进行编码（我也可以使用转换器生成每种类型对象的固定长度编码并将其与固定组件连接起来，但直观地将所有内容放入同一个转换器中应该会更好，因为上下文是使转换器有用的原因）。本质上，这意味着编写一个自定义状态空间编码器，我认为两个大型库（Stable Baselines 和 Rllib）都支持它。伪代码如下所示： def encode(obs): general, coins = obs # 一个 1 x 4 np 数组和一个 k x 2 np 数组 coins_emb = self.coin_embed(c) # 一个将 2 映射到嵌入维度的前馈层 gen_emb = self.general_embed(general) # 4 --&gt; emb_dimcoded = self.encoder(torch.stack(gen_emb, coins_emb)) # torch.nn.TransformerEncoder，接受 N x emb_dim 输入，产生 N x h_dim 输出coded =coded[0] # BERT 使用特殊 [CLS] 标记的输出作为其固定长度输出。在这里，我们将输出用作起始标记。返回编码的 def policy(obs): 返回 self.policy_net(encode(obs)) def value(obs): 返回 self.value_net(encode(obs))  我记得 OpenAI 的捉迷藏环境使用了池化（IIRC 特征编码器后的最大池化）而不是变压器，但那是很久以前的事了。无论如何，我的问题的核心是是否有人对实现这种自定义观察网络时使用的最佳堆栈有建议。如果有人见过做这样的事情的项目（越新越好 - 语法似乎像季节一样变化），我也会非常感谢 github 链接。 谢谢！    提交人    /u/Dry-Sock7131   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</guid>
      <pubDate>Sat, 25 May 2024 16:41:35 GMT</pubDate>
    </item>
    <item>
      <title>教人形机器人用头部弹球的教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</link>
      <description><![CDATA[您好，刚刚在 github 上发布了一个新教程 - https ://github.com/goncalog/ai-robotics。您的反馈会很棒！   由   提交/u/goncalogordo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</guid>
      <pubDate>Sat, 25 May 2024 16:36:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自定义环境引擎</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</link>
      <description><![CDATA[我目前正在尝试使用 Isaac sim 构建一个自定义的强化学习环境。我已经构建了模型，但我不知道如何将其导入 vs code 以便真正对环境进行编程，而且我找不到任何相关教程。我也想知道我是否应该改用 mojuco？但我真的不知道如何用它创建模型。    提交人    /u/Teaser_404   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</guid>
      <pubDate>Sat, 25 May 2024 11:39:40 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习结果不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</guid>
      <pubDate>Sat, 25 May 2024 11:32:13 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND（扩散作为环境梦想的模型）是在扩散世界模型中训练的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</link>
      <description><![CDATA[   /u/clumma  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</guid>
      <pubDate>Fri, 24 May 2024 23:02:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Airsim 稳定基线3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czcjab/stable_baselines3_with_airsim/</link>
      <description><![CDATA[您好！我正在尝试在 AirSim 中制作一架无人机，以便在从一个点移动到另一个点时避开物体。我修改了该示例以满足我的需要。我使用来自 Stable Baselines3 和 Gymnasium 的 PPO 对其进行训练。问题是，一集的时间太长了。因此，我无法像在其他环境中那样提供足够的时间步长。我是 RL 的新手。请帮我解决这个问题。   由   提交 /u/Sandy_The_Adventurer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czcjab/stable_baselines3_with_airsim/</guid>
      <pubDate>Fri, 24 May 2024 04:34:01 GMT</pubDate>
    </item>
    <item>
      <title>“Vernor Vinge 的小说《真名实姓》后记”，Minsky 1984（偏好学习和安全代理的挑战）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</guid>
      <pubDate>Thu, 23 May 2024 16:23:56 GMT</pubDate>
    </item>
    </channel>
</rss>