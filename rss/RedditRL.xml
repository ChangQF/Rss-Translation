<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 06 Feb 2025 15:18:03 GMT</lastBuildDate>
    <item>
      <title>“长远交互式 LLM 代理的强化学习”，Chen 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij2nfz/reinforcement_learning_for_longhorizon/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij2nfz/reinforcement_learning_for_longhorizon/</guid>
      <pubDate>Thu, 06 Feb 2025 13:34:36 GMT</pubDate>
    </item>
    <item>
      <title>用于定制演员和评论家网络的 RL 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</link>
      <description><![CDATA[我希望在 PyTorch 中测试自定义神经网络，并使用演员评论家 RL 算法中的标准 MLP 对指标（即收敛率）进行基准测试。我浏览了 subreddit 并发现以下库被推荐用于实现此类网络：  RLLib rlpyt skrl TorchRL  对这些有什么看法或好的经验吗？我看到很多人对 RLLib 有爱有恨，但对最后三个的评价并不多。我试图避免使用 SB3，因为我认为我的神经网络不属于他们拥有的任何自定义策略类别，除非我严重误解了他们的自定义策略类的工作方式。    提交人    /u/Voltimeters   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</guid>
      <pubDate>Thu, 06 Feb 2025 12:55:17 GMT</pubDate>
    </item>
    <item>
      <title>关于 MAPPO 实施的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij0m2l/question_about_mappo_implementation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij0m2l/question_about_mappo_implementation/</guid>
      <pubDate>Thu, 06 Feb 2025 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>积极的在线运动规划和决策 | 印度 | Swaayatt 机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiyt2j/aggressive_online_motion_planning_and_decision/</link>
      <description><![CDATA[      Swaayatt Robots 为 5 级自动驾驶汽车开发了一种新颖的在线运动规划和决策框架，使它们能够以极快的速度行驶，同时实时避开交通锥等障碍物。 该系统可即时执行动态轨迹计算，对 24 米半径内的障碍物做出反应。演示展示了之字形和左车道避让模式，尽管车身侧倾挑战很大，但车辆仍保持 45 KMPH 以上的速度。 视频截图 Youtube_Link 该框架在单线程 i7 处理器上以 800+ Hz 运行，并集成了轨迹跟踪系统和纯追踪。未来计划包括通过端到端深度强化学习扩展框架 原始作者 LinkedIn：sanjeev_sharma_linkedin 原始 LinkedIn 帖子：pose_link    提交人    /u/shani_786   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiyt2j/aggressive_online_motion_planning_and_decision/</guid>
      <pubDate>Thu, 06 Feb 2025 09:36:49 GMT</pubDate>
    </item>
    <item>
      <title>需要有关高级 RL 资源的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</link>
      <description><![CDATA[大家好， 我已经深入研究强化学习一段时间了，但我遇到了瓶颈。我发现的几乎每门课程或资源都涵盖了相同的内容 — PPO、SAC、DDPG 等。它们对于理解基础知识非常有用，但我感觉陷入了困境。就好像我只是在相同的算法上打转，而没有真正取得进展。 我正在尝试弄清楚如何突破这一点，并进入更高级或最新的强化学习方法。诸如遗憾最小化、基于模型的强化学习，甚至多智能体系统和 HRL 之类的东西听起来很令人兴奋，但我不知道从哪里开始。 有没有人有这种感觉？如果你已经成功突破了这个瓶颈，你是怎么做到的？任何课程、论文甚至个人建议都会非常有帮助。 提前致谢！    提交人    /u/Helpful-Number1288   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</guid>
      <pubDate>Thu, 06 Feb 2025 08:15:09 GMT</pubDate>
    </item>
    <item>
      <title>对强化学习中的数学符号感到困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iivcd2/confused_about_math_notations_in_rl/</link>
      <description><![CDATA[大家好， 我一直在学习强化学习，但是我对一些数学符号，尤其是期望符号感到很困惑。例如，价值函数通常写成： V^π(s) = E_π [ R_t | s_t = s ] = E_π [ ∑_{k=0}^{∞} γ^k r_{t+k+1} | s_t = s ] 下标 E_π 到底是什么意思？我的理解是，下标应该表示概率分布或随机变量，但π是一种策略（函数），而不是通常意义上的分布。 这种混淆也出现在轨迹概率定义中，例如： P(τ | π) = ρ_0(s_0) ∏_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) π(a_t | s_t) π 是一个输出动作的函数。虽然动作是一个随机变量，但 π 本身不是（如果我错了，请纠正我）。 在以下情况下，情况甚至更糟（来自https://spinningup.openai.com/en/latest/spinningup/rl\_intro.html） V^\pi(s)=\mathbb{E}_{\tau \sim \pi}\left[R(\tau) \mid s_0=s\right] 作者在这里写了 $\tau \sim \pi}$，但轨迹 \tau 不是从策略 \pi 中采样的，因为 \tau 还包括由环境生成的状态。 类似地，像 E_π [ R(τ) | s_0 = s, a_0 = a ] 感觉很直观，但我发现它们在数学上并不严格，因为期望通常取自明确定义的概率分布。 更新： 我更担心的是，像 $E_\pi$ 这样的符号实际上是新的数学运算，不同于传统的期望运算。 我知道对于大多数 RL 这样的简单情况，它们不太可能无效或不完整。但我认为我们需要一个证明来表明它们的有效性。 电气工程师使用 Dx 表示 dx/dt，使用 1/Dx 表示 \integral x dt。我不知道是否有证据证明这一点，但微分算子具有非常明确的含义，而 E_\pi 则令人困惑。 任何见解都将不胜感激！    提交人    /u/AdministrativeCar545   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iivcd2/confused_about_math_notations_in_rl/</guid>
      <pubDate>Thu, 06 Feb 2025 05:30:02 GMT</pubDate>
    </item>
    <item>
      <title>RL 不适用于运动控制和学习！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</link>
      <description><![CDATA[我想知道是否有人知道使用 RL 进行运动学习的研究？我听说它从来都无法用于建模或控制现实世界中的运动。这是真的吗？    提交人    /u/BerkeleyYears   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</guid>
      <pubDate>Thu, 06 Feb 2025 04:55:56 GMT</pubDate>
    </item>
    <item>
      <title>人形机器人的强化学习控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</link>
      <description><![CDATA[嗨， 我对基于 RL 的人形控制器很感兴趣。如果您能列出一些很棒的资源作为起点，我将不胜感激。谢谢    提交人    /u/rua0ra1   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</guid>
      <pubDate>Thu, 06 Feb 2025 04:42:30 GMT</pubDate>
    </item>
    <item>
      <title>算法的设计旨在将“乐趣”的概念充分灌输到人工智能中。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iihtux/algorithm_designed_to_instill_the_concept_of_fun/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iihtux/algorithm_designed_to_instill_the_concept_of_fun/</guid>
      <pubDate>Wed, 05 Feb 2025 19:03:00 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和模型预测控制调查 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iidp8i/reinforcement_learning_and_model_predictive/</link>
      <description><![CDATA[  由    /u/tmms_  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iidp8i/reinforcement_learning_and_model_predictive/</guid>
      <pubDate>Wed, 05 Feb 2025 16:15:55 GMT</pubDate>
    </item>
    <item>
      <title>“改进 Transformer 世界模型以实现数据高效的强化学习”，Dedieu 等人。 2025年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ii9r1s/improving_transformer_world_models_for/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ii9r1s/improving_transformer_world_models_for/</guid>
      <pubDate>Wed, 05 Feb 2025 13:19:49 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ii9oxu/need_help/</link>
      <description><![CDATA[我正在尝试创建一个人工智能，通过让它与像 stockfish 这样训练有素的人工智能对战来学习下棋。 我计划用 python 来做这个，因为 python 已经有了 python-chess，使用 stockfish 更容易 我还计划在此期间学习这些人工智能是如何工作的 对于训练部分，我计划使用 Stable-Baseline3。 我对人工智能和如何训练代理有一些基本的了解，但我已经使用 ml-agents 在 unity 中训练了代理，所以我不知道这有多难？ 我应该做什么，我应该怎么做？ 谢谢。    提交人    /u/kungfuaryan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ii9oxu/need_help/</guid>
      <pubDate>Wed, 05 Feb 2025 13:16:31 GMT</pubDate>
    </item>
    <item>
      <title>思考数据：我想知道我的想法是否可行。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</link>
      <description><![CDATA[你好。我很快就要开始学习计算机科学了（今年秋天或明年秋天，取决于我的大学何时允许我选择并专注于一个专业），但我想在人工智能最迷人的部分之一：强化学习方面取得突破。 我的计划：制作多个可以学习玩游戏的人工智能，然后将它们连接在一起，这样感觉就像一个人工智能。但这还不是全部。首先，它会从一个游戏开始，然后我将内存复制并粘贴（并最有可能对其进行一些修改）到另一个文件中，它将在其中玩另一个游戏，这样它就可以在已经知道基本控制的情况下快速启动。一段时间后，我会让它玩更高级的游戏，希望它知道大多数游戏都有类似的控制结构。 最终目标：拥有一个可以玩多个游戏的多用途人工智能，了解游戏无障碍指南，然后在一个文件中拆分无障碍审查。哦，是的，也许可以使用语言模型与我聊天。 在理想世界中，我会使用现有的 RL 代理（当然是在开发人员的许可下）来帮助加快该过程，并使用 LLM 与其聊天并获取仅玩游戏的 AI 无法提供的信息。 不幸的是，我有一台 MSI GF75 Thin，配备 Intel i5-10300h、NVIDIA GTX 1650（配备 4gh VRAM）和 32gb Ram。我认为很多都很好，除了显卡（我觉得即使没有尝试制作 AI 也缺乏显卡），所以我无法用我当前的设置做很多事情。但这是我想长期考虑的事情，因为有一天将我的想法付诸实践真的很酷。    提交人    /u/Octo_Chara   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</guid>
      <pubDate>Wed, 05 Feb 2025 01:11:40 GMT</pubDate>
    </item>
    <item>
      <title>在 RL、决策智能应用方面有没有博士学位机会？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</link>
      <description><![CDATA[我是一名大四本科生，想申请 RL 或决策智能应用领域的直接攻读博士学位机会。  虽然我已经申请了一些大学，但我觉得我的机会很小。我已经后悔很久了，因为我去年没有跟踪申请或看清机会。如果你们当中有谁对 2025 年仍开放的直接攻读博士学位课程有所了解，请在这个 subreddit 中告诉我🙏    提交人    /u/Miserable_Ad2265   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</guid>
      <pubDate>Tue, 04 Feb 2025 14:59:07 GMT</pubDate>
    </item>
    <item>
      <title>关于 TRPO 论文的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</link>
      <description><![CDATA[      我正在研究 TRPO 论文，我有一个关于如何在以下优化问题中计算新策略的问题： https://preview.redd.it/l8fndz5ra4he1.png?width=940&amp;format=png&amp;auto=webp&amp;s=f49f53bedb23a9a6d04f6fbeaf79a643bde0052b 这个方程用于更新和寻找新策略，但我想知道如何计算π_θ(a|s)，因为它属于我们试图优化的策略——就像先有鸡还是先有蛋的问题。 论文提到，样本用于计算这个表达式：  1. 使用单路径或藤蔓程序收集一组状态-动作对以及它们的 Q 值的蒙特卡洛估计。 2. 通过对样本取平均值，构建公式 (14) 中的估计目标和约束。 3. 近似解决这个约束优化问题以更新策略的参数向量。我们使用共轭梯度算法，然后进行线搜索，这只比计算梯度本身稍微贵一点。详情请参阅附录 C。     提交人    /u/audi_etron   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</guid>
      <pubDate>Tue, 04 Feb 2025 12:38:38 GMT</pubDate>
    </item>
    </channel>
</rss>