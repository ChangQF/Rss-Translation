<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 03 Feb 2024 15:12:28 GMT</lastBuildDate>
    <item>
      <title>DQN 不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</link>
      <description><![CDATA[嗨，我正在尝试做一个蛇游戏 DQN，但在前 1k 次迭代中没有看到太多结果（如果有的话）。该模型似乎正在回归。 我想知道我的更新循环是否正确 信息：吃食物的奖励+1，碰撞-1，其他奖励 - 欧氏距离/来自食物的 l2 范数 我确实有一个重播缓冲区 def train_v2(self, state_tensor, action,reward, new_state_tensor, did): # state -&gt; (3,32,24) # 模型 -&gt; conv net action_tensor = torch.tensor(action, dtype=torch.float)reward_tensor = torch.tensor(reward, dtype=torch.float) if len(state_tensor.shape) == 3: # 转换为批处理形式 if single example state_tensor = torch.unsqueeze（state_tensor，dim = 0）action_tensor = torch.unsqueeze（action_tensor，dim = 0）reward_tensor = torch.unsqueeze（reward_tensor，dim = 0）new_state_tensor = torch.unsqueeze（new_state_tensor，dim = 0）完成=（ done,) for i in range(len(done)): # for idx in batch q_pred = self.model.forward(state_tensor[i]) if did[i]: q_next = torch.zeros(1) # 没有下一个状态否则： q_next = self.model_target.forward(new_state_tensor[i]).max(dim=1)[0] q_target =reward_tensor[i] + self.gamma*q_next self.optimizer.zero_grad() loss = self.loss_function( q_target, q_pred) loss.backward() self.optimizer.step()    由   提交/u/throtaway85633  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahous8/dqn_not_converging/</guid>
      <pubDate>Sat, 03 Feb 2024 05:39:48 GMT</pubDate>
    </item>
    <item>
      <title>pettingzoo tic-tac-toe 游戏中何时调用 reset() 函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</link>
      <description><![CDATA[我正在将 pettingzoo 环境用于 MARL 程序，并使用 tictactoe 环境（在此处找到）作为蓝图。现有环境似乎在每个单独的时期内多次调用重置函数，这对于我自己的目的来说是不可取的。在尝试查找重置调用的来源时，我将其追溯到“base.py”文件。文件位于 pettingzoo /utils/wrappers 目录中。我仍然无法准确确定何时调用重置。我想让它仅在每个纪元结束时调用重置，因为我积累了不想重置的值。 我复制了 tictactoe 测试代码来运行它。我在重置函数中放置了一个打印调用，以查看每个时期内调用重置的次数。我确认在井字棋游戏的每个时期都会多次调用重置。这样做的目的是什么？在我看来，你会想在每场比赛结束时调用重置。为什么要多次重置，如何更改重置被调用的次数？   由   提交/u/NobodySmart1617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahgdiv/when_is_reset_function_being_called_in_pettingzoo/</guid>
      <pubDate>Fri, 02 Feb 2024 22:36:23 GMT</pubDate>
    </item>
    <item>
      <title>Nuro 实现大规模强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ahehz4/nuro_enabling_reinforcement_learning_at_scale/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/recklessdesuka  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ahehz4/nuro_enabling_reinforcement_learning_at_scale/</guid>
      <pubDate>Fri, 02 Feb 2024 21:16:17 GMT</pubDate>
    </item>
    <item>
      <title>DQN 探索策略的收敛速度比贪婪策略快得多</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ah70er/dqn_exploration_policy_converges_much_faster_than/</link>
      <description><![CDATA[      我在解释以下结果时遇到一些困难。橙色线是奖励训练曲线，蓝色线是评估曲线。 https://preview.redd.it/ashlhzdl07gc1.png?width=1177&amp;format=png&amp;auto=webp&amp;s=0a409ee350a6b3d80c5784b62079f 37f8dfdb9f8 期间训练我使用 epsilon 贪婪策略，epsilon = 0.2。在评估过程中，我使用贪婪的 argmax 策略。这些结果表明，在我的环境中，贪婪策略需要大约 20 万步才能达到最优。然而，epsilon 贪婪策略使用与贪婪策略相同的模型，但以 20% 的概率采取随机行动，只需 50k 步就已经是最优的。 观察到这一点时，您的第一个想法是什么？    由   提交 /u/fedetask   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ah70er/dqn_exploration_policy_converges_much_faster_than/</guid>
      <pubDate>Fri, 02 Feb 2024 15:59:55 GMT</pubDate>
    </item>
    <item>
      <title>PPO算法动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1agx883/ppo_algorithm_actions/</link>
      <description><![CDATA[我知道 PPO 输出操作的平均值和标准差，但是我如何才能将我的操作限制在应用程序的安全范围内。或者还有其他我可以选择的算法来替代 PPO。   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1agx883/ppo_algorithm_actions/</guid>
      <pubDate>Fri, 02 Feb 2024 06:27:16 GMT</pubDate>
    </item>
    <item>
      <title>了解行为政策</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1agdtah/understanding_behavior_policy/</link>
      <description><![CDATA[我目前正在尝试了解 on-policy 和 off-policy 之间的差异。 到目前为止，我了解到： - 行为策略：代理用于选择操作的策略 - 目标策略：代理优化的策略 - On-Policy：行为策略 = 目标策略 - Off-Policy：行为策略 ≠ 目标策略 我最大的困惑是了解行为策略在同策略方法中的作用。在on-policy中，比如SARSA，如果智能体从Q表中选择行动，他们不是总是处于剥削状态并且从不探索吗？ 如果情况不是这样，那么什么是在策略 epsilon-greedy 算法与非策略 epsilon-greedy 算法之间的区别？ 我读了两篇不同的文章： 1. https://builtin.com/machine-learning/sarsa 这篇文章说，使用 epsilon-greedy 动作选择是 on-policy，因为当我们利用时，我们会从目标策略中选择一个动作  https://www.baeldung.com/cs/epsilon-贪婪-q-学习这篇文章说，使用epsilon-greedy动作选择是离策略的，因为当我们探索时，我们随机选择一个动作  事实是，这两者文章定义了相同的动作选择函数。那么是哪一个呢？在策略还是离策略？   由   提交/u/bean_217  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1agdtah/understanding_behavior_policy/</guid>
      <pubDate>Thu, 01 Feb 2024 15:38:36 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习数据中心</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ag5t8q/offline_rl_data_hub/</link>
      <description><![CDATA[查看 torchrl 的数据中心：https ://pytorch.org/rl/reference/data.html#datasets 它是最大的单一格式离线强化学习数据库。所有数据集都是可互换和/或可组合的。 目前，它包括 AtariDQN、D4RL、VD4RL、Roboset、所有 OpenX Examples、Minari 和 GenDGRL。 它基于 torchrl 的重播缓冲区实现，以便您可以像使用重播缓冲区一样使用它们（即，它们是完全可组合的并接受转换）。它的速度很快，采样速度真的非常快！   由   提交/u/AdCool8270  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ag5t8q/offline_rl_data_hub/</guid>
      <pubDate>Thu, 01 Feb 2024 07:54:28 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：为什么 env.reset(seed=seed) 有助于学习具有相同起始位置的确定性 env (frozenlake)？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afp5sd/need_help_why_does_envresetseedseed_facilitate/</link>
      <description><![CDATA[环境：FrozenLake-v1、4x4、slippery=false。无论种子如何，起始 obs（位置）始终为 0，并且地图（障碍物）不应改变。 我一直在每个训练集开始时调用 env.reset(seed=seed)。使用不同的随机种子，我的算法（A2C）能够解决这个问题。然而，当我删除重置中的种子（仅重置，而不是火炬或其他任何东西）时，策略会收敛到次优非解决方案。为什么？ 还有什么是 env 种子控制的随机因素？我什至尝试将我的主种子设置为 X，将重置种子设置为 Y。 健身房的冰雪湖教程也在每集之前设置重置种子，即使滑溜也关闭了。请参阅此处：https: //gymnasium.farama.org/tutorials/training_agents/FrozenLake_tuto/#:~:text=state%20%3D%20env.reset(seed%3Dparams.seed)%5B0%5D%5B0%5D)&lt; /p&gt; 有什么想法吗？谢谢！ 编辑：弄清楚了。我的代码中有一个错误，其中 is_slippery 实际上设置为 true。因此，通过将种子设置为重置，每个情节的转换状态都是相同的（即每次在位置 x 执行操作 1 都有相同的效果），因此我的模型本质上是记住种子设置的某个概率转换。坏消息：我的设置显然无法在不确定的湿滑环境中工作。我也无法让它与 sb3 的 A2C 一起工作。   由   提交 /u/rl_ninja_rl_ninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afp5sd/need_help_why_does_envresetseedseed_facilitate/</guid>
      <pubDate>Wed, 31 Jan 2024 18:44:03 GMT</pubDate>
    </item>
    <item>
      <title>MARLlib 中的自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afokh3/custom_environments_in_marllib/</link>
      <description><![CDATA[我一直在浏览 MARLlib 文档，尽管他们提到它是 Ray 和 RLlib 的混合体 (链接），我不太确定它是否像 RLlib 一样支持自定义环境。 我还没有遇到任何与此相关的信息。这里有人有 MARLlib 和自定义环境的经验吗？   由   提交/u/krm76  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afokh3/custom_environments_in_marllib/</guid>
      <pubDate>Wed, 31 Jan 2024 18:20:01 GMT</pubDate>
    </item>
    <item>
      <title>Flappy Bird 1.6 小时 2100 管，你觉得学习速度如何？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afmw1h/flappy_bird_2100_pipes_in_16_hours_how_do_you/</link>
      <description><![CDATA[https://reddit.com /link/1afmw1h/video/vsymq3l66tfc1/player ​ 我们在 ~1.6 中使用 Unity 中的 DQN 算法（这不是 mlAgents）训练了 FlappyBird小时。 由于一切都是从头开始编写的（以及神经网络），因此可以更改许多参数。划分环境也有助于加快这一过程。 100个特工同时训练，数量逐渐减少。 ​ 我想拍一个视频或者详细写一下，所以在我想知道你之前意见：与其他方法或现有插件相比，它是快还是慢，其他人会感兴趣吗？   由   提交/u/Fazoway  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afmw1h/flappy_bird_2100_pipes_in_16_hours_how_do_you/</guid>
      <pubDate>Wed, 31 Jan 2024 17:12:31 GMT</pubDate>
    </item>
    <item>
      <title>引擎上的强化学习：学习恒定轨迹而不是实际轨迹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afkx4i/rl_on_engine_learns_a_constant_trajectory_instead/</link>
      <description><![CDATA[      社区您好， 我对我的问题有一个概念性问题。我正在尝试使用 DDPG 代理学习引擎控制模型，而我的引擎有一个 LSTM 模型作为植物。我模拟给定随机轨迹的引擎，并使用引擎输出以及引擎状态（LSTM 状态）和负载轨迹作为我的代理的观察模型。 我正在尝试训练 DDPG 代理要求其遵循如下参考负载轨迹（左上图中的虚线）。我观察到，尽管尝试了各种网络架构/噪声选项&amp;学习率时，学习模型代理选择只提供 6 左右的恒定负载（左上图中的橙色线），而不是遵循给定的参考轨迹。输出似乎有合理的变化（此处为蓝色），但学习仍然不可接受。 我正在调整每一集的轨迹以帮助学习，因为这样它就可以看到各种负载配置文件。 &lt; p&gt;您能告诉我这里可能发生什么吗？ 其他信息：如果我要求控制器匹配恒定负载轨迹（每集 constnat，然后更改为另一个随机常数，则会发生相同的效果）下一集 ）。  提前致谢:) https://preview.redd.it/6qrpihpfrsfc1.png?width=2540&amp;format=png&amp;auto=webp&amp;s=f2f19cad1f71d411b6a6c2615274227d018e6 d57   由   提交/u/Doctor-Featherheart  /u/Doctor-Featherheart reddit.com/r/reinforcementlearning/comments/1afkx4i/rl_on_engine_learns_a_constant_trajectory_instead/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afkx4i/rl_on_engine_learns_a_constant_trajectory_instead/</guid>
      <pubDate>Wed, 31 Jan 2024 15:50:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么我不能使用基于流程的并行性有效地并行化我的强化学习程序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afik9g/why_cant_i_effectively_parallelize_my/</link>
      <description><![CDATA[我的目标是使用 Stable_Baselines3 库同时运行多个强化学习程序。我注意到，随着程序数量的增加，程序的迭代速度逐渐降低，这是相当令人惊讶的，因为每个程序应该运行在不同的进程（核心）上。  ​ 这是我的程序： ​ ```py &lt; p&gt;from joblib import 并行，延迟 ​ 导入gym # from sbx import SAC 导入torch&lt; /p&gt; ​ 从 stable_baselines3 导入 SAC def train(): ​ ​ env =gym.make(“Humanoid-v4”) ​ model = SAC(“MlpPolicy”) ;, env, verbose=1) model.learn(total_timesteps=7e5,progress_bar=True) ​ def train_model():  ​ train() ​ ​  ​ if __name__ == &#39;__main__&#39;: num_of_programs = 1 并行(n_jobs=10)(延迟(train)() for i in range(num_of_programs)) ``` ​ `num_of_programs` 用于控制我尝试的程序数量并行运行。  这里有一些统计数据 -  ​ 程序数量迭代速度 1 1 ~102 it/s&lt; /p&gt; 2 3 ~60 it/s 3 10 ~ 20 it/s ​ 我确定要请求足够的资源，这样就不会出现资源限制。这就是我使用 slurm 请求资源的方式 - `srun --time=10:00:00 --nodes=1 --cpus-per-task=16 --mem=32G --partition=gpu --gres=gpu :a100-pcie:1 --pty /usr/bin/bash` ​ 因此我有 16 个 cpu、32G 内存和 40 GB GPU。 p&gt; ​ 当我从 `stable_baselines3` 迁移到 `sbx` 时，我注意到了同样的问题。 `stable_baselines3` 使用 `torch` 作为深度学习库，而后者使用 `JAX`。 ​ ​   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1afik9g/why_cant_i_efficiently_parallelize_my/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afik9g/why_cant_i_effectively_parallelize_my/</guid>
      <pubDate>Wed, 31 Jan 2024 14:03:44 GMT</pubDate>
    </item>
    <item>
      <title>需要 MountainCarContinously 帮助 - 用于连续操作的 REINFORCE 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1affkro/need_help_with_mountaincarcontinuous_reinforce/</link>
      <description><![CDATA[      大家好，最近我一直在研究 REINFORCE 算法持续采取行动，但成效有限。最初，我想从简单的事情开始，所以我尝试为标准健身房环境开发一种算法。我相信我涵盖了所有必要的点，但正如你所看到的，我的代理正在上山，但它应该向前和向后移动，这很奇怪。有什么想法吗？  这是我的 colab 的链接。如果有人能抽出时间来帮助我，那就太好了。 https://colab.research.google.com/drive/1MrqEhww3rqZoZkKY1Jnwd4oPQHAN4xWH?hl=pl# scrollTo=sydH0wO1OFpJ https://reddit.com/link/1affkro/ video/1d1uomiserfc1/player   由   提交/u/Sharp-Record1600  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1affkro/need_help_with_mountaincarcontinuous_reinforce/</guid>
      <pubDate>Wed, 31 Jan 2024 11:16:56 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习和基于模型的强化学习在学习模型和控制方面有何区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1afebbj/difference_between_offline_and_modelbased_rl_in/</link>
      <description><![CDATA[我看到通常诸如“如何在 rl 中使用预先收集的数据集”之类的问题的答案，答案与离线 RL，建议首先通过监督学习来学习模型。但是基于模型的学习还假设模型是根据经验数据学习的。正在从批量数据中学习基于模型的模型 + 使用典型的 MBRL 方法比如计划/想象不正确？我必须在与真实环境交互的同时学习模型？   由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1afebbj/difference_between_offline_and_modelbased_rl_in/</guid>
      <pubDate>Wed, 31 Jan 2024 09:49:48 GMT</pubDate>
    </item>
    <item>
      <title>RL 介绍途径</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aepy9e/rl_intro_pathway/</link>
      <description><![CDATA[所以我今天在这里发帖，支持和答案都很好，但我确实感到有点不知所措，有没有任何指南/路径/我可以遵循决策树来对此有一个小小的了解？例如：你想要 rl 或 dnn + rl，然后选择这个，然后你想要多代理或单代理，然后这样做，然后你想要模型或没有模型等 我想我正在问一组一般性的问题，这些问题将帮助我选择什么是“最好的”。对于我的项目/我应该研究什么   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aepy9e/rl_intro_pathway/</guid>
      <pubDate>Tue, 30 Jan 2024 14:41:49 GMT</pubDate>
    </item>
    </channel>
</rss>