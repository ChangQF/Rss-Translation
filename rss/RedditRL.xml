<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 29 Sep 2024 18:20:38 GMT</lastBuildDate>
    <item>
      <title>来自游戏屏幕的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fs8zxa/reinforcement_learning_model_from_gamescreen/</link>
      <description><![CDATA[您好，我不知道这是否是它的正确子版块，但我对强化学习有一个问题。我知道模型需要状态来确定动作。但对于像 Pokémon 这样的游戏，我实际上无法获得状态。所以我想知道游戏屏幕是否可以用作状态。理论上应该可以，也许我需要手动从屏幕中提取关键信息并创建该状态。但我想避免这种情况，因为我希望模型能够发挥 Pokémon 的两个方面，即探索和战斗。 我正在考虑的第二个问题是，每当模型执行某项操作时，我将如何确定要给予的奖励时间和金额。由于我没有从游戏中获取任何数据，所以我不知道它何时赢得战斗，也不知道当它的神奇宝贝生命值较低时它会何时治愈它们。 由于我对机器学习没有那么多经验，几乎没有，所以我开始怀疑这是否有可能。有人可以对这个想法发表意见并给我一些指点吗？我很想了解更多，但我找不到一个好的起点。    提交人    /u/tirodokter   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fs8zxa/reinforcement_learning_model_from_gamescreen/</guid>
      <pubDate>Sun, 29 Sep 2024 16:56:47 GMT</pubDate>
    </item>
    <item>
      <title>单步情节的 RL（连续空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fs8ixh/rl_for_single_step_episodes_continuous_spaces/</link>
      <description><![CDATA[大家好。我目前正在从事一个与控制图参数自动调整相关的项目。其中重要的部分是，我正在处理连续有界空间，包括观察和动作，但最重要的是，我当前的实现依赖于单步情节，或者更好的是连续的一个 0 步骤和一个实际步骤：代理向系统提供一个身份图只是为了获得一个观察（可能会有所不同，因此它不是固定的初始条件），它选择一个动作（参数向量），获得奖励并结束情节。 目前我正在使用 PPO 作为商品，但我相信有更适合的方法来解决这样的问题。有什么建议吗？    提交人    /u/Krnl_plt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fs8ixh/rl_for_single_step_episodes_continuous_spaces/</guid>
      <pubDate>Sun, 29 Sep 2024 16:36:03 GMT</pubDate>
    </item>
    <item>
      <title>策略梯度定理和 TRPO/PPO 之间没有联系？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fs80xm/no_link_between_policy_gradient_theorem_and/</link>
      <description><![CDATA[您好， 我写这篇文章只是为了确认一些事情。 许多深度强化学习资源都遵循经典的解释路径，即介绍策略梯度定理，并应用它推导出一些最基本的策略梯度算法，如简单策略梯度、REINFORCE、带基线的 REINFORCE 和 VPG 等等。（例如 Spinning Up） 然后，他们使用不同的目标进入 TRPO/PPO 算法。我们是否清楚，TRPO 和 PPO 算法根本不使用策略梯度定理？而且，甚至不使用相同的目标？ 我认为这经常被忽视。 注意：本文（近端策略梯度https://arxiv.org/abs/2010.09933）在 VPG 上应用了与 PPO 相同的剪辑思想。    提交人    /u/alexandretorres_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fs80xm/no_link_between_policy_gradient_theorem_and/</guid>
      <pubDate>Sun, 29 Sep 2024 16:14:21 GMT</pubDate>
    </item>
    <item>
      <title>寻找合作者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fry25k/looking_for_collaborators/</link>
      <description><![CDATA[大家好， 我正在研究离线 RL 中的一个问题。我看到了一些性能改进，我正在寻找在这个领域更有经验的人来合作。如果有人感兴趣，请直接留言。我愿意合作。    提交人    /u/Regular_Average_4169   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fry25k/looking_for_collaborators/</guid>
      <pubDate>Sun, 29 Sep 2024 06:23:51 GMT</pubDate>
    </item>
    <item>
      <title>被强化学习方程式所困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1frwz31/confused_by_the_equations_as_learning/</link>
      <description><![CDATA[大家好。我是 RL 领域的新手。我目前正在读研究生，需要使用 RL 算法来完成一些任务。但问题是我不是 CS/ML 背景。虽然我有电子工程背景，但在观看 RL 教程时，我真的很困惑。比如更新 Q 表、奖励以及所有这些期望、偏差是怎么回事......我现在真的很困惑。有人能给我一些建议吗？顺便说一句，我了解 CNN、FCN 等基本神经网络。我也学习了它们的数学背景。但 RL 是另一回事。有人能帮忙给点建议吗？    提交人    /u/Adventurous_Fly_5564   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1frwz31/confused_by_the_equations_as_learning/</guid>
      <pubDate>Sun, 29 Sep 2024 05:08:40 GMT</pubDate>
    </item>
    <item>
      <title>Dagger 做出同样的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1frtvcj/dagger_gives_same_action/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1frtvcj/dagger_gives_same_action/</guid>
      <pubDate>Sun, 29 Sep 2024 02:01:51 GMT</pubDate>
    </item>
    <item>
      <title>伽马射线的振动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fri490/vibrations_on_gamma/</link>
      <description><![CDATA[如果 IMU 读数由于振动而剧烈波动，我是否应该增加或减少折扣因子？ 随机性意味着对读数的信心降低，因此我们应该降低 𝛾。 但这是否也意味着我们不应该立即做出反应，而应该进一步考虑未来的结果（即增加伽马）？    提交人    /u/FriendlyStandard5985   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fri490/vibrations_on_gamma/</guid>
      <pubDate>Sat, 28 Sep 2024 16:28:25 GMT</pubDate>
    </item>
    <item>
      <title>SB3 的设置很奇怪，它是 MlpPolicy 上的 PPO 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1frds17/weird_setup_of_sb3_its_a_ppo_model_on_mlppolicy/</link>
      <description><![CDATA[大家好，我有一个奇怪的 SB3 设置，它是 `MlpPolicy` 上的 PPO 模型，基本上我无法设置一个训练环境，所以我不得不手动进行观察，预测动作 (1,2,3,4(上、下、左、右))，并根据这两个获得奖励。然后我手动将其添加到我的模型推出数据库中，并计算出 `value` 和 `log_probs`。我还调整了学习函数以删除 `continue_training`，它是收集一定数量时间步长的数据（我认为）并手动增加时间步长以推出缓冲区大小，这使得学习函数运行直到缓冲区为空。  现在到了确保我所做的事情没问题的难点。我可以用我完成的运行来训练 AI。一次做 2048 个（观察、动作、奖励）。我的奖励范围是 (-1, 1)，游戏每步的平均奖励是 0.3，最后一步（死亡）的奖励是 -1，获胜（从未获胜）的奖励是 1 我在学习的第一帧就有这些值。我对此很陌生，但是从谷歌搜索“explained_variance”的负数发现它非常糟糕，我的剪辑分数从第一帧的 0.05 变为最后一帧的 0.9~。 我也不确定其他值是好是坏。 以下是学习的第一帧。 ```  | time/ | | | fps | 2 | | iterations | 2 | | time_elapsed | 0 | | total_timesteps | 1 | | train/ | | | approx_kl | 0.011088178 | | clip_fraction | 0.0567 | | clip_range | 0.2 | | entropy_loss | -1.38 | | explained_variance | -0.00553 | | learning_rate | 0.0001 | | loss | 0.4 | | n_updates | 19 | | policy_gradient_loss | -0.00562 | | value_loss | 1.2 |  ```  下面是第 744 帧 |时间/ | | | fps | 2 | | 迭代 | 744 | | time_elapsed | 270 | | 总时间步数 | 743 | | 训练/ | | | approx_kl | 0.13849491 | | clip_fraction | 0.877 | | clip_range | 0.2 | | 熵损失 | -1.25 | | 解释方差 | -0.00553 | | 学习率 | 0.0001 | | 损失 | -0.0276 | | n_updates | 7439 | | policy_gradient_loss | -0.143 | | value_loss | 0.304 |  如果有人知道我所做的事情是否超出范围，请告诉我，或者如果你可以建议我应该尝试的事情。    提交人    /u/XxKingsxX   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1frds17/weird_setup_of_sb3_its_a_ppo_model_on_mlppolicy/</guid>
      <pubDate>Sat, 28 Sep 2024 13:00:58 GMT</pubDate>
    </item>
    <item>
      <title>教人工智能如何现场玩我的世界！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fqzqzb/teaching_an_ai_how_to_play_minecraft_live/</link>
      <description><![CDATA[        提交人    /u/idan0405   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fqzqzb/teaching_an_ai_how_to_play_minecraft_live/</guid>
      <pubDate>Fri, 27 Sep 2024 22:21:05 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习中的规范奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fqfmnh/norm_rewards_in_offline_rl/</link>
      <description><![CDATA[我正在离线 RL 中开展一个项目。我正在尝试实现一些离线 RL 算法。但是，在离线 RL 中，结果通常通过规范化来报告。我不知道这是什么意思。这些奖励是如何计算的？他们是否使用专家数据奖励来规范化或什么。 谢谢你的帮助。    提交人    /u/Regular_Average_4169   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fqfmnh/norm_rewards_in_offline_rl/</guid>
      <pubDate>Fri, 27 Sep 2024 04:26:23 GMT</pubDate>
    </item>
    <item>
      <title>将强化学习与模型预测控制相结合，用于 HEMS</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fq66pg/merging_reinforcement_learning_and_model/</link>
      <description><![CDATA[大家好， 我正在做一个大学项目，主题与标题中描述的主题一致。HEMS = 家庭能源管理系统。 我正在考虑如何合并 RL 和 MPC 以利用它们的优势。我的主管希望我特别关注样本效率。由于我是这个主题的新手，我阅读了很多论文，但似乎不明白什么标准对我来说很重要，以及哪些算法符合该标准。 你会如何处理这个问题？ Br    提交人    /u/Bubi_Bums   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fq66pg/merging_reinforcement_learning_and_model/</guid>
      <pubDate>Thu, 26 Sep 2024 20:32:23 GMT</pubDate>
    </item>
    <item>
      <title>使用双手机器人探索钉插入的精度：使用 ACT 模型的实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fq436q/exploring_precision_with_peginsertion_using/</link>
      <description><![CDATA[        提交人    /u/Trossen_Robotics   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fq436q/exploring_precision_with_peginsertion_using/</guid>
      <pubDate>Thu, 26 Sep 2024 19:03:07 GMT</pubDate>
    </item>
    <item>
      <title>矩阵运算寻找 MDP 的最优解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpzgty/matrix_operations_to_find_the_optimal_solution_of/</link>
      <description><![CDATA[大家好。 我编写了一个程序来计算玩在线游戏的最佳动作序列，该程序可以简化为一个 MDP，其转换矩阵 T 形状为 [A, S, S]，奖励矩阵形状为 [S, A]。我还有一个形状为 [S, A] 的策略。 我现在正在应用策略迭代来获得 MDP 的解决方案：https://en.wikipedia.org/wiki/Markov_decision_process#Algorithms 因此，算法的一部分是计算与策略相关的转换概率矩阵，以将其简化为 [S, S] 矩阵。 我显然可以使用双重嵌套 for 循环通过元素操作来做到这一点，但我想知道是否有更优雅的矢量化解决方案。我一直在想这个问题，但也许是因为我学代数太久了，真的找不到解决办法。 我得到了一个丑陋的解决方案，这让我很不开心…… np.sum((np.diag(P.T.reshape(-1)) @ T.reshape(-1, nStates)).reshape(T.shape), axis=0)     提交人    /u/oruiog   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpzgty/matrix_operations_to_find_the_optimal_solution_of/</guid>
      <pubDate>Thu, 26 Sep 2024 15:50:07 GMT</pubDate>
    </item>
    <item>
      <title>2D 在线装箱不收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpxl4a/2d_online_bin_packing_not_converging/</link>
      <description><![CDATA[      嗨！我正在研究 2D 约束在线装箱，但面临模型无法收敛的一些问题。 代理使用 PPO，并因最大化空间效率和优先考虑边缘位置而获得奖励，而对次优内部位置则施加惩罚。启发式策略指导最佳箱子放置，重点是先填充周长，然后再考虑内部网格，从而增强代理在装箱场景中的决策能力。该模型目前有效（有时），但我认为调整（尽管我确实尝试过）和对动作空间的探索不足可能是问题所在。我也可以分享我的代码！ https://preview.redd.it/g9gokw56z5rd1.png?width=993&amp;format=png&amp;auto=webp&amp;s=b64bcd8b1ea97dbf098c0a56edb52d0ff36ef5d4 上一篇文章：https://www.reddit.com/r/MachineLearning/comments/1fl69fa/p_2d_bin_packing_problem/    由   提交  /u/Sea-Hovercraft4777   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpxl4a/2d_online_bin_packing_not_converging/</guid>
      <pubDate>Thu, 26 Sep 2024 14:30:50 GMT</pubDate>
    </item>
    <item>
      <title>乐高遇见人工智能：BricksRL 被 NeurIPS 2024 接受！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/</link>
      <description><![CDATA[      我们很高兴地告诉大家，我们关于 BricksRL 的论文已被 NeurIPS 2024 接受为焦点论文，BricksRL 是一个 RL 算法库，可以在价格实惠的定制 LEGO 机器人上进行训练和部署！ 随着人工智能和机器学习继续掀起波澜，我们认为向社区提供可靠且价格合理的教育工具至关重要。并非每个人都可以使用数百个 GPU，而且了解 ML 在实践中的工作原理可能具有挑战性。 这就是我们一直在研究 BricksRL 的原因，这是庞培法布拉大学和 PyTorch 之间的合作项目。我们的目标是为人们提供一种有趣且引人入胜的方式来学习 AI、ML、机器人技术和 PyTorch，同时保持高标准的正确性和稳健性。 BricksRL 基于 Pybricks，可以部署在许多不同的 LEGO 集线器上。我们希望它能让世界各地的实验室以可承受的价格制作原型，而无需昂贵的机器人。 查看我们的网站：https://bricksrl.github.io/ProjectPage/ 该库在 GitHub 上根据 MIT 许可开源：https://github.com/BricksRL/bricksrl/ 阅读我们的论文：https://arxiv.org/abs/2406.17490 观看机器人的实际操作：https://www.youtube.com/watch?v=k_Vb30ZSatk&amp;t=10s 我们正在开展一些令人兴奋的后续项目，敬请期待！ 温哥华见 https://preview.redd.it/1ghfs9t9l0rd1.jpg?width=2006&amp;format=pjpg&amp;auto=webp&amp;s=868867adcd52825bd4ee719513a454527d017307    提交人    /u/AdCool8270   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fpebw9/lego_meets_ai_bricksrl_accepted_at_neurips_2024/</guid>
      <pubDate>Wed, 25 Sep 2024 20:23:37 GMT</pubDate>
    </item>
    </channel>
</rss>