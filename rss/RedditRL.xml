<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 02 Aug 2024 21:15:08 GMT</lastBuildDate>
    <item>
      <title>EWRL 的选择性如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</link>
      <description><![CDATA[大家好， 正如标题所说，你们知道 EWRL 研讨会在接受论文方面有多挑剔吗？总的来说，你觉得这个研讨会好吗？一些个人故事将不胜感激。    由   提交  /u/sel20   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</guid>
      <pubDate>Fri, 02 Aug 2024 20:41:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Decision Transformer 在 OfflineRL 顺序决策领域有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</link>
      <description><![CDATA[谢谢。    提交人    /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</guid>
      <pubDate>Fri, 02 Aug 2024 15:02:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在 EPyMARL 中加载和测试训练模型以进行 MAPPO 算法评估？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eib4td/how_to_load_and_test_a_trained_model_in_epymarl/</link>
      <description><![CDATA[      大家好， 我是强化学习 (RL) 的新手，最近开始使用 EPyMARL 框架进行多智能体强化学习 (MARL)。我已经在我的健身房的自定义环境中使用 MAPPO 和其他类似算法成功训练了一个模型。现在，我想加载这个经过训练的模型并在我使用的案例中评估其性能，但我不确定如何做到这一点。 这是我用于训练的命令： python3 src/main.py --config=mappo --env-config=gymma with env_args.key=&quot;LoRaEnv-v0&quot;  下面是我尝试用于评估的命令： python3 src/main.py --config=mappo --env-config=gymma with env_args.key=&quot;LoRaEnv-v0&quot; checkpoint_path=&quot;my_save_model_path&quot; assess=True  我不确定这是否正确，或者我是否需要采取其他步骤，因为我尝试运行脚本但遇到了一些奇怪的错误。有人可以指导我如何正确加载和测试我训练过的模型以使用 EPyMARL 进行评估吗？ 以下是包含我保存的模型的目录的屏幕截图： 如能提供任何帮助或指点，我们将不胜感激！提前致谢！ 训练模型目录    提交人    /u/Interesting_Dingo983   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eib4td/how_to_load_and_test_a_trained_model_in_epymarl/</guid>
      <pubDate>Fri, 02 Aug 2024 14:14:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么代理没有学会如何到达立方体的位置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</link>
      <description><![CDATA[        提交人    /u/CoolestSlave   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</guid>
      <pubDate>Fri, 02 Aug 2024 12:17:29 GMT</pubDate>
    </item>
    <item>
      <title>CORL 和 D4RL 分数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</link>
      <description><![CDATA[我提前为一个可能微不足道的问题道歉： 在评估离线强化学习算法时，我们应该关心哪些分数？它们是 D4RL 标准化分数吗？还有其他需要考虑的吗？ 提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</guid>
      <pubDate>Thu, 01 Aug 2024 21:26:26 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型内部的 RL 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</link>
      <description><![CDATA[大家好。 我正在为纸牌游戏制作强化学习算法。我可以采取的可能动作取决于我手中的牌。如果我打出一张牌，我可以执行特定的操作。您可以假设这是一场 1v1 游戏。如果我的对手打出一张特定的牌，那么我必须决定给对手一张牌。 本质上，我希望有一个 RL 模型来决定打出哪张牌，然后另一个 RL 模型用于我必须给对手一张牌的特定情况。第一个 RL 模型将在每个回合激活，而第二个 RL 模型仅在我的对手打出迫使我给他一张自己的牌的特定牌时激活。 我想通过先与随机模型对战，然后再与自己对战来训练模型。 这可以在体育馆内完成吗？如果是这样，该怎么办？    提交人    /u/Practical-Resort7278   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</guid>
      <pubDate>Thu, 01 Aug 2024 12:30:12 GMT</pubDate>
    </item>
    <item>
      <title>真人秀 PS1 游戏 - 铁拳 3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</link>
      <description><![CDATA[我是 RL 领域的新手。我尝试过使用 atari 游戏的复古模拟作为 ENV 的 openai gym，但我想下一步。 我想在 PS1 上为格斗游戏 Tekken 3 实现 RL 模型。 据我所知，我必须使用 PS1 模拟器实现自己的交互层才能正确获取状态，例如健康、位置等。 我该如何正确处理？  尝试对 PS1 游戏进行逆向工程，并找出具有必要值的内存地址（通过 python 获取）。 使用 OpenCV 进行相同的检测，但从帧中  当涉及到创建自定义环境时，您将如何处理？    提交人    /u/Comprehensive_Cod331   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</guid>
      <pubDate>Thu, 01 Aug 2024 04:58:31 GMT</pubDate>
    </item>
    <item>
      <title>既然离线 RL 与环境无关，为什么很多论文实现仍然基于 gym？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</link>
      <description><![CDATA[谢谢。    由   提交  /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:33 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为不同的游戏选择不同的 RL 算法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</link>
      <description><![CDATA[我是荷兰 6 VWO 的一名 16 岁学生，目前正在参与一个关于强化学习 (RL) 在各种电脑游戏中的应用的学校研究项目。我的主要研究问题是：电脑游戏的具体特征如何影响不同 RL 算法的有效性？ 子问题：  不同类型的电脑游戏有哪些具体特征？ 有哪些 RL 算法可用，它们的特点是什么？ 游戏特征如何影响 RL 算法的性能？  实践部分：我计划将不同的 RL 算法应用于各种游戏。我正在考虑的游戏是：  超级马里奥兄弟 贪吃蛇 国际象棋 赛车  算法标准：  具有显著差异的算法。 最好是新算法。  反馈问题：  考虑到这些游戏的独特特点，您能否推荐适合这些游戏的特定 RL 算法？ 您认为我选择的游戏适合研究不同 RL 算法的有效性吗？如果不适合，您会建议什么游戏？     提交人    /u/matmoet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</guid>
      <pubDate>Wed, 31 Jul 2024 23:23:28 GMT</pubDate>
    </item>
    <item>
      <title>“彩虹团队：开放式生成多样化对抗提示”，Samvelyan 等人 2024 {FB}（用于质量多样性搜索的 MAP-Elites）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</guid>
      <pubDate>Wed, 31 Jul 2024 01:48:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 epsilon-greedy 算法的概率部分中包含贪婪动作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</link>
      <description><![CDATA[      新手在这里...我正在阅读 Sutton 和 Barto 关于强化学习的常年书籍。在他们对 epsilon-greedy 策略的描述中，他们认为在给定状态下选择一个动作应该以较小的 epsilon 概率发生；其余时间则选择贪婪动作。我理解这一点，因为你想鼓励探索，这反过来又允许人们满足先决条件，即在无限的时间轴下，最终将为给定状态选择所有动作，以收敛到接近最优的策略。 但是，我不明白即使在随机操作时也允许代理选择贪婪动作的原因。书中给出的这个公式就是一个例子，其中选择贪婪的概率是 1 减去选择任何动作（甚至是贪婪的动作）的概率 epsilon 加上随机选择贪婪动作的概率： https://preview.redd.it/vz26lao3vqfd1.png?width=142&amp;format=png&amp;auto=webp&amp;s=e3c7c0f6d48f1f349f9260f4a4e8897f438e2b42 当然，代理选择贪婪动作的次数会比选择其他非贪婪动作的次数多得多。因此，当整个重点是防止代理以小概率 epsilon 利用此操作时，为什么代理会在随机阶段选择最佳操作？ 谢谢， 一位充满激情的 RL 学习者。    提交人    /u/Soft-Establishment96   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</guid>
      <pubDate>Wed, 31 Jul 2024 00:02:51 GMT</pubDate>
    </item>
    <item>
      <title>“Auto Evol-Instruct：大型语言模型的自动指令进化”，Zeng 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efy7aj/auto_evolinstruct_automatic_instruction_evolving/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efy7aj/auto_evolinstruct_automatic_instruction_evolving/</guid>
      <pubDate>Tue, 30 Jul 2024 16:45:14 GMT</pubDate>
    </item>
    <item>
      <title>你在PPO算法中遇到过这个问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</link>
      <description><![CDATA[我正在创建一个用于任务路由的深度强化学习环境，使用基于策略的 DRL 方法 PPO。在我的环境中，想法是每次任务到达一个节点时，都会生成一个概率，最终找到目的地并结束游戏。因此，每次任务到达一个节点时，都会生成相邻节点的概率，形成到达终点的连续路径。但是，存在一个问题：为每个状态生成的概率正在收敛。理论上，它们应该不同。例如，在选择一个节点后，应该生成像 s1[0.2,0.5,0.3] 和 s2[0.4,0.1,0.5] 这样的概率，但目前，每个状态都有相同的概率，例如 s1[0.2,0.5,0.3] 和 s2[0.2,0.5,0.3]。 我怀疑我的奖励设置可能有问题。我给每一步都设置了与时间参数相关的负奖励，完成的奖励为10，遇到路由循环就得到-10的负奖励，难道我的设计有问题？这让我很纳闷。    submitted by    /u/VermicelliBrave1931   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</guid>
      <pubDate>Tue, 30 Jul 2024 07:11:58 GMT</pubDate>
    </item>
    <item>
      <title>“对受试者进行反馈的序贯实验分析”，Diaconis & Graham 1981</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</guid>
      <pubDate>Mon, 29 Jul 2024 20:30:26 GMT</pubDate>
    </item>
    <item>
      <title>平均情节奖励差异，为什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</link>
      <description><![CDATA[      嗨， 我有一个简单的环境，我使用 SB3 中的各种算法进行训练作为练习。这是 DDPG 和 SAC 的 tensorboard 情节平均奖励（针对完全相同的环境）。在学习完成并保存模型时，报告的奖励约为 6。但是，当我在保存的模型上使用 SB3 assesse_policy (with n_eval_episodes=10) 时，我看到以下内容： 对于 SAC：平均奖励：1.5123082560196053 +/- 0.0008870645563937467 对于 DDPG：平均奖励：0.6197831666923037 +/- 0.000696591922367452 我预计平均奖励约为 6。这种预期是错误的吗？ https://preview.redd.it/xxag55fswgfd1.png?width=765&amp;format=png&amp;auto=webp&amp;s=e6903abf29863ad230d84451e83b162e265eb101    submitted by    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</guid>
      <pubDate>Mon, 29 Jul 2024 14:32:31 GMT</pubDate>
    </item>
    </channel>
</rss>