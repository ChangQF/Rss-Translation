<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 25 Feb 2024 15:11:58 GMT</lastBuildDate>
    <item>
      <title>初学者课程/资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1azmwac/coursesresources_for_beginner/</link>
      <description><![CDATA[我已分配在 Q-tables 上工作并在我的公司进行探索。请推荐一些强化学习的初学者课程。我已经开始了拥抱脸课程，但有点难以理解。    由   提交/u/96_kishan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1azmwac/coursesresources_for_beginner/</guid>
      <pubDate>Sun, 25 Feb 2024 12:34:46 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 代码很难读</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1azgrc4/dreamerv3_code_is_so_hard_to_read/</link>
      <description><![CDATA[大家好， 最近我被分配了一项工作来研究 SOTA 世界模型 DreamerV3 [^1]。我花了 3 个月的时间来理解这篇论文（我是 ML 新手）和代码。基本上，我查看了 3 个代码库：  作者的实现，用 Jax 编写：https:// github.com/danijar/dreamerv3 NM512 的 PyTorch 实现：https://github。 com/NM512/dreamerv3-torch sheeprl 的 PyTorch 实现：https://github。 com/Eclectic-Sheep/sheeprl  (1)使用Jax，看起来很复杂，(3)提供了一系列博客进行解释。所以我选择（3）。然而，即使 (3) 也相当复杂。 Sheeprl 希望他们的框架适用于所有 RL 算法。由于牺牲了通用性，程序逻辑变得难以阅读。我对这项任务感到不知所措，不知道该怎么办。  也许我应该回到 Jax 版本，即使没有关于它的文档。我觉得 Dreamer 里的设计和技巧太多了:( 有没有（代码、博客、研究）推荐？也许我应该回到 David Ha 的 2018 年世界模型论文[^2]来做我的研究，因为它应该比 Dreamer 更容易。 非常感谢！ [^1]: https://arxiv.org/abs/2301.04104 [^2]：https:// arxiv.org/abs/1803.10122    提交者    /u/AdministrativeCar545   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1azgrc4/dreamerv3_code_is_so_hard_to_read/</guid>
      <pubDate>Sun, 25 Feb 2024 06:04:52 GMT</pubDate>
    </item>
    <item>
      <title>植绒奖励功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1az38z9/reward_function_for_flocking/</link>
      <description><![CDATA[目标： 我正在尝试制作自定义Boid 具有稳定基线的开放式 AI 健身房中的植绒环境3。我正在使用 PPO。 ​ 什么是 Boid 植绒：视频 我现在有两个带有附加性能的奖励函数： SafetyRadius=2 NeighborhoodRadius=10 &lt; /pre&gt;  对齐 defcalculate_combined_reward(self,agent,neighbor_velocities): total_reward = 0 if (len(neighbor_velocities) &gt; ; 0):average_velocity = np.mean(neighbor_velocities, axis=0)desired_orientation =average_velocity-agent.velocityorientation_diff = np.arctan2(desired_orientation[1],desired_orientation[0])-np.arctan2(agent.velocity[1]) , agent.velocity[0]) 如果orientation_diff &gt; np.pi：orientation_diff -= 2 * np.pi eliforientation_diff &lt; 0：orientation_diff += 2 * np.pi Total_reward = 1 - np.abs(orientation_diff) return (total_reward)   对齐  内聚 + 分离 defcalculate_combined_reward(self,agent,neighbor_positions):reward = 0  for neighbour_position in neighbour_positions: distance = np.linalg.norm(agent.position - neighbour_position) if distance &lt;; SimulationVariables[“SafetyRadius”]: # 较大的惩罚，以阻止智能体获得太接近的奖励 -= 10 elif SimulationVariables[“SafetyRadius”] &lt;距离&lt; SimulationVariables[“NeighborhoodRadius”]: # 奖励的衰减指数函数 alpha = 0.99 # 根据需要调整此参数reward += np.exp(-alpha * distance) returnreward  &lt; /ol&gt; 凝聚力 + 分离 &amp;# x200b; 问题： 一段时间后，两者都单独崩溃，我看不出问题所在。 ​ 附加信息： 我正在将凝聚力和分离力结合起来，否则它们会得到过于复杂的信号。 还没有惩罚对于碰撞或 done=True 也是如此。  视频中的每个奖励函数均以 500,000 个时间步长单独运行，以进行调试。在一起并不会变得更好。 ​ 感谢任何帮助。   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1az38z9/reward_function_for_flocking/</guid>
      <pubDate>Sat, 24 Feb 2024 19:37:43 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ayrwwn/stablebaselinese3/</link>
      <description><![CDATA[我目前正在使用稳定的 baslines QRDQN 开发交易机器人。我是人工智能世界的新手，但我很想知道制作自己的强化学习 nn 是否是一个更好的主意？因为我有两种奖励类型，一种是主要的，另一种是针对每个步骤的。主要的一个为整个交易提供奖励，而步骤则提供奖励以推动模型走向更好的交易并理解每个动作。我已经尝试了无数个小时来为剧集结束提供主要奖励，但我只是不知道如何做到这一点。我检查了 QRDQN 工作原理的代码，但一无所获。我想过只将整个交易奖励给予止盈操作，但认为这可能会让模型感到困惑。第二个问题，有人有办法解决这个问题吗？    由   提交/u/Born-Belt1991  /u/Born-Belt1991 reddit.com/r/reinforcementlearning/comments/1ayrwwn/stablebaselinese3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ayrwwn/stablebaselinese3/</guid>
      <pubDate>Sat, 24 Feb 2024 10:59:09 GMT</pubDate>
    </item>
    <item>
      <title>我一直在做MaxEntRL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ay82bp/i_was_doing_maxentrl_all_this_time/</link>
      <description><![CDATA[我有一个简单的问题。我最近遇到了最大熵强化学习。不过我不太懂这个领域。只要系数为 1，似乎只要在策略损失中添加熵正则化损失就可以使方法获得最大熵 RL？我错过了什么吗？我认为最大熵强化学习应该是一个更复杂的算法。  tldr；是否将熵正则化添加到您的 A2C/PPO 等策略损失中，系数为 1，从而实现最大熵 RL？   由   提交 /u/miladink   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ay82bp/i_was_doing_maxentrl_all_this_time/</guid>
      <pubDate>Fri, 23 Feb 2024 18:37:07 GMT</pubDate>
    </item>
    <item>
      <title>网格世界应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ay10w7/grid_world_application/</link>
      <description><![CDATA[ 由   提交/u/bwe587  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ay10w7/grid_world_application/</guid>
      <pubDate>Fri, 23 Feb 2024 13:55:50 GMT</pubDate>
    </item>
    <item>
      <title>需要工作代码示例（回购）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1axwgio/need_of_working_code_examples_repo/</link>
      <description><![CDATA[大家好。我从事 RL 工作有一段时间了，现在我失去了动力。在购买了几本书和 Udemy 课程后，我仍然缺乏代码的工作示例。书中显示的大多数算法都是在旧的库版本上解决的，因此无法正常工作。例如：https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python/tree/master/11.%20Actor%20Critic%20Methods%20-%20A2C%20and%20A3C  无法解决环境问题。例如 https://github.com/PacktPublishing/Tensorflow- 2-Reinforcement-Learning-Cookbook/blob/master/Chapter03/5_a3c_continuous.py 工作速度慢得要命（一集需要几分钟）。所以无法验证它的好坏。  所以我的问题是有人可以访问工作示例存储库（Gym）吗？我感兴趣的是 RL 的连续版本（连续动作）。   由   提交/u/Sharp-Record1600  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1axwgio/need_of_working_code_examples_repo/</guid>
      <pubDate>Fri, 23 Feb 2024 09:29:35 GMT</pubDate>
    </item>
    <item>
      <title>策略迭代：不同教科书中对改进步骤的不同定义</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1axvnq3/policy_iteration_different_definitions_of/</link>
      <description><![CDATA[     &lt; td&gt; 编辑： 在评论中解决 在教科书《人工智能，一种现代方法》中，策略改进步骤中评估动作值的方法： https://preview.redd.it/vo75je58qakc1.png?width=544&amp;format=png&amp;auto=webp&amp;s=648cd1ad34bda60966f2b027555e5623f041549 2 与此不同Sutton 和 Barto 的强化学习教科书： https://preview.redd.it/von4631aqakc1.png?width=515&amp;format=png&amp;auto=webp&amp;s=a20c09926e057fb02bf44c7cc5a1a1c8f4ec3a65  其中考虑奖励。有理由吗？此外，第一幅图像中的 U(s&#39;) 未定义为 R+ V(s&#39;)。 U 和 V 只是同一事物的不同表示法。感谢您的回答 ​ 编辑： 第一本书中的政策评估： ​ https://preview.redd.it /puujf9gvfckc1.png?width=566&amp;format=png&amp;auto=webp&amp;s=4cead60f6db3029a497fab18ec3f3cd761323f1f 第二张图像的完整算法： ​ https://preview.redd.it /bfckvah3gckc1.png?width=490&amp;format=png&amp;auto=webp&amp;s=b8e42bff391090bcb853708e03b655a800343d64   由   提交 /u/tengboss   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1axvnq3/policy_iteration_different_definitions_of/</guid>
      <pubDate>Fri, 23 Feb 2024 08:33:35 GMT</pubDate>
    </item>
    <item>
      <title>行动的价值和奖励之间有什么区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1axcqs8/whats_the_difference_between_the_value_of_an/</link>
      <description><![CDATA[跟随 Sutton 和 Barto，他们将动作“a”的值定义为选择“a”时奖励的数学期望，但直觉上它们不是一样的吗？如果我试图估计一个动作的价值，那么我试图估计该动作的奖励，我在这里缺少什么吗？    ;由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1axcqs8/whats_the_difference_between_the_value_of_an/</guid>
      <pubDate>Thu, 22 Feb 2024 17:51:49 GMT</pubDate>
    </item>
    <item>
      <title>2024 年学习强化学习的最佳书籍 -</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ax7bvg/best_books_to_learn_reinforcement_learning_in_2024/</link>
      <description><![CDATA[       由   提交 /u/Sreeravan   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ax7bvg/best_books_to_learn_reinforcement_learning_in_2024/</guid>
      <pubDate>Thu, 22 Feb 2024 14:12:12 GMT</pubDate>
    </item>
    <item>
      <title>在 A3C 实现中异步使用时，价值网络会出现分歧</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ax0ic9/value_network_diverges_when_used_asynchronously/</link>
      <description><![CDATA[我使用 CartPole-v1 环境。我构建了一个用于 VPG 和 A3C 的价值网络。在 VPG 中，它按预期工作。在 A3C 中，我创建了一个 work 方法，其中有 8 个工作线程在 pytorch/python 的多处理库的帮助下异步工作。 workers = [mp.Process(target=self.work, args=(rank,)) 表示范围内的排名(self.n_workers)] [w.start() 表示工人中的 w]; [w.join() for w in Workers]  我将类中的价值网络作为函数 self.value_model_fn = value_model_fn 并在 work 方法我创建本地价值网络来与 local_value_model = self.value_model_fn(nS) 一起使用，其中 nS 是状态大小。我对策略网络做了完全相同的事情，但策略网络在 VPG 和 A3C 中都按预期工作。我还加载了在 train 函数中初始化的共享策略和值模型，如下所示：  local_policy_model = self.policy_model_fn(nS, nA) local_policy_model.load_state_dict( self.shared_policy_model.state_dict()) local_value_model = self.value_model_fn(nS) local_value_model.load_state_dict(self.shared_value_model.state_dict())  当我运行 local_value_model(state)&lt; /code&gt; 代码进入永无休止的循环。 我尝试在 work 方法之外运行价值网络，它工作得很好。我尝试在不加载 shared_value_network 的情况下运行它，我还尝试在 local_policy_network 之前运行它，但当我在 work 中运行它时它不起作用代码&gt;方法。  它应该做的是返回以下形式的值：tensor([[0.0214]], grad_fn= 但它会永远循环。我不这样做真的知道要采取什么其他方向，我认为网络实现中不存在问题，因为它在工作方法和 VPG 实现之外工作得很好，所以我的猜测是多处理方面存在一些问题，使其发散。    提交者    /u/yuyututuyutu   [链接]   ; [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ax0ic9/value_network_diverges_when_used_asynchronously/</guid>
      <pubDate>Thu, 22 Feb 2024 07:24:31 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶中的 RL 和 SL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ax03wr/rl_and_sl_in_autonomous_driving/</link>
      <description><![CDATA[      我根据自己的观察和知识做出了这个。这是自动驾驶中应用 RL 和 SL 算法的高级概述，主要用于决策。接下来我将尝试涵盖 RL 和 SL 中使用的按键和端子。如果有错误请告诉我。    由   提交/u/Mysterious-Care1358  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ax03wr/rl_and_sl_in_autonomous_driving/</guid>
      <pubDate>Thu, 22 Feb 2024 06:59:42 GMT</pubDate>
    </item>
    <item>
      <title>机器人学习的下一个研究主题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1awhoy6/next_research_topic_for_robot_learning/</link>
      <description><![CDATA[经过长时间的强化学习研究后，我对机器人学习研究下一步该做什么感到困惑。  我对 RL 和 MARL 进行了一系列研究。现在我认为将强化学习研究转向模仿学习对我来说是明智的，因为这在现实中更可靠。 如果没有建立世界模型，我无法找到现实中机器人的强化学习。或者也许LLM的模仿学习可能是机器人学习的下一个方向？    由   提交/u/Tight-Ad789  /u/Tight-Ad789  reddit.com/r/reinforcementlearning/comments/1awhoy6/next_research_topic_for_robot_learning/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1awhoy6/next_research_topic_for_robot_learning/</guid>
      <pubDate>Wed, 21 Feb 2024 17:22:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习播放列表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1awg1rz/reinforcement_learning_playlist/</link>
      <description><![CDATA[大家好，查看我的强化学习播放列表，其中包含 28 个教程，涵盖基础知识和高级主题，示例包括 MAB、Contextual MAB、Monte Carlo、SARSA、 Q Learning、A2C、DDPG、REINFORCE、PPO、RLHF、Multi-Agent 算法和一些 OpenAI 健身房示例 https://youtube.com/playlist?list=PLnH2pfPCPZsIpNtTIwKQZeDerJEz2ccEV&amp;si=KLZ3FYlBWkAHI kxx&lt; /p&gt;   由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1awg1rz/reinforcement_learning_playlist/</guid>
      <pubDate>Wed, 21 Feb 2024 16:17:43 GMT</pubDate>
    </item>
    <item>
      <title>失去动力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw6mn4/losing_motivation/</link>
      <description><![CDATA[也许我只是反应过度，或者我太弱了。但请听我说完。在过去的几个月里，我一直在尝试自学强化学习。我正在学习 Coursera RL 专业课程的第二门课程，萨顿 &amp; 课程的第 8 章。巴托。起初我的一些朋友也想和我一起学习，但他们都不再这样做了。我还在做一份全职工作，这几乎耗尽了我所有的精力。我寻找导师，但没有找到。如今每个人都热衷于计算机视觉或 NLP。此外，很多人都说强化学习没有未来。所有这些加在一起真是太累了。 我真的不知道我在这里要找什么。如果你能分享你的旅程，那将会有帮助。另外，如果您能指导我（即使是一点点时间），我将永远感激不已。   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw6mn4/losing_motivation/</guid>
      <pubDate>Wed, 21 Feb 2024 07:55:27 GMT</pubDate>
    </item>
    </channel>
</rss>