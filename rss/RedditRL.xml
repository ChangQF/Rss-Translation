<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 03 Mar 2024 03:13:46 GMT</lastBuildDate>
    <item>
      <title>图注意力网络（GAT）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4wisu/graph_attention_network_gat/</link>
      <description><![CDATA[你好， 我有一个看似微不足道的查询（我是 GAT 的新手）。  目前，我有一个表示为 Data(x=[24, 6], edge_index=[2, 23]) 的状态。将其传递到图注意网络 (GAT) 层后，我获得了 torch.Size([24, 64]) 的输出形状。 随后，我的目标是将这个输出输入到一个模块中，该模块应该产生两个概率，但它返回 torch.Size([24, 2]) 的形状（对于每个节点，它输出 2 个概率）。我担心仅获得两个概率作为输出。您能否建议如何实现这一目标？下面是代码的快照： class Policy(nn.Module): def __init__(self, state_dim, action_set1_dim, action_set2_num_heads): super(Policy, self). __init__() self.gat_conv1 = GATConv(state_dim[-1], 64, 头=1, dropout=0.5) self.gat_conv2 = GATConv(64, 64, 头=1, dropout=0.5 )  # 选择器网络 self.selector = nn.Sequential( nn.Linear(64 + state_dim[-1], 32), nn.ReLU( ), nn.Linear(32, 2) ) def forward(self, state): # x：节点特征，edge_index：图连通性，mask：掩码填充值 masked_x = state.x * state.mask # 经过第一个GAT层 x_gat1 = F.relu(self.gat_conv1(masked_x, state.edge_index)) # 经过第二个GAT层 x_gat2 = F.relu(self.gat_conv2(x_gat1, state.edge_index)) # 选择器网络selector_probs = F.softmax(self.selector( x_gat2), dim=-1) 返回selector_probs   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4wisu/graph_attention_network_gat/</guid>
      <pubDate>Sat, 02 Mar 2024 19:31:23 GMT</pubDate>
    </item>
    <item>
      <title>帮助连续空间离线Q-Learning</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4tkda/help_with_continuous_space_offline_qlearning/</link>
      <description><![CDATA[我对强化学习还很陌生，并且计划在我的论文中以一种雄心勃勃的方式使用它。我了解在线强化学习的基础知识，并完成了几个小项目，但我什至很难理解在哪里寻找信息来实现我想要的目标。本质上，我只能访问历史数据，它由 12 个 60*100 图像描述的状态空间和由其中 2 个图像描述的动作空间组成。这些空间是连续且无界的。我还为每个状态定义了奖励函数，以及不同结果的一些最终状态。我的目标是评估所采取的每个动作，因此我想训练一个神经网络，它可以接收状态和动作作为输入，并输出 Q 值。我对寻找最佳政策没有兴趣。我发现了一种叫做 IQL 的东西，但我感觉有点难以理解，不确定它是否适用于我的问题。非常感谢您的帮助！   由   提交/u/macaco3001   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4tkda/help_with_continuous_space_offline_qlearning/</guid>
      <pubDate>Sat, 02 Mar 2024 17:27:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的奖励网络是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4qwsm/what_are_reward_networks_in_reinforcement_learning/</link>
      <description><![CDATA[我正在阅读以下文章 此处 -  两种逆强化学习 (IRL) 算法的目标（例如 AIRL,  GAIL）和偏好比较是为了发现奖励功能。在模仿学习中，这些发现的奖励由奖励网络参数化。   这是否意味着模仿学习的输出是奖励值？那么输入是什么？状态和行动？ 有一篇很好的论文解释奖励网络和模仿学习吗？我指的是莱文的讲座笔记。在幻灯片 5 中，模仿学习的输出似乎是一种策略，而不是奖励。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1b4qwsm/what_are_reward_networks_in_reinforcement_learning/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4qwsm/what_are_reward_networks_in_reinforcement_learning/</guid>
      <pubDate>Sat, 02 Mar 2024 15:32:58 GMT</pubDate>
    </item>
    <item>
      <title>奇才和 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4ld37/wizards_and_ppo/</link>
      <description><![CDATA[ 由   提交/u/nurgle100  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4ld37/wizards_and_ppo/</guid>
      <pubDate>Sat, 02 Mar 2024 10:34:38 GMT</pubDate>
    </item>
    <item>
      <title>只是分享我关于 Gran Turismo 的 RL 论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4klxv/just_sharing_my_dissertation_in_rl_for_gran/</link>
      <description><![CDATA[https://youtu.be/zpz3Dbmx1SI   由   提交/u/NDR008  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4klxv/just_sharing_my_dissertation_in_rl_for_gran/</guid>
      <pubDate>Sat, 02 Mar 2024 09:44:28 GMT</pubDate>
    </item>
    <item>
      <title>梦想家预测哪些奖励和折扣？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b48cfy/which_reward_and_discount_is_predicted_by_dreamer/</link>
      <description><![CDATA[嗨， 我对 Dreamer 的一种实现有疑问。据我了解，时间步 t 处的当前模型状态 s_t （确定性 + 随机）应该重建时间步 t 处的观察结果和通过在 t-1 处应用操作和 t-1 处观察而获得的奖励 现在我正在研究这个实现，其中缓冲区内的每个样本都包含（obs_t，act_t，r_t，done_t），其中奖励和 done 与下一个时间步相关联（应用 act_t 之后）。现在在采样函数中，他们有这个 _shift_sequences 来移动 obs到(seq:seq_len+L)，另一个到(seq-1:seq+L-1)。这是有道理的，因为观察结果与应该预测的奖励一致。 ​ 问题在于 世界模型训练，特别是在representation_loss中，他们计算reward_loss 为： reward_dist = self.RewardDecoder(post_modelstate[:-1] ) reward_loss = self._reward_loss(reward_dist,rewards[1:]) 这看起来像是他们在预测下一个奖励。这感觉不对，因为奖励还应该取决于操作，而在这种情况下，由于reward_dist仅由当前模型状态s_t计算，因此它不包括代理将执行的操作。我错过了什么还是奖励损失应该是： reward_dist = self.RewardDecoder(post_modelstate) reward_loss = self._reward_loss(reward_dist ，奖励） 当然，同样的疑问也适用于折扣预测器 ​ 编辑：https://arxiv.org/pdf/1912.01603.pdf 第 4 节“奖励”预测器”似乎只使用状态来预测下一个奖励   由   提交/u/ZioFranco1404   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b48cfy/which_reward_and_discount_is_predicted_by_dreamer/</guid>
      <pubDate>Fri, 01 Mar 2024 22:57:46 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco MJX 对于高自由度模型的性能？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4742e/mujoco_mjx_performance_for_high_dof_models/</link>
      <description><![CDATA[大家好，有人尝试过使用 Mujoco MJX 进行高自由度模型吗？我将 dm-control 啮齿动物模型修改为与 MJX 兼容并具有简单的批量推出基准测试，它似乎比使用具有多个进程（64 核）的 CPU Mujoco 慢几个数量级，这主要是由于增加批量大小时收益递减。我使用的是 A100 GPU。有没有其他人尝试过使用比 Deepmind 演示（Humanoid、Ant 等）更复杂的模型进行类似的基准测试？   由   提交 /u/Czcz_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4742e/mujoco_mjx_performance_for_high_dof_models/</guid>
      <pubDate>Fri, 01 Mar 2024 22:08:17 GMT</pubDate>
    </item>
    <item>
      <title>在训练像 AlphaZero/MuZero 这样的模型来玩平台游戏时，如何避免过度拟合到特定级别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b41brf/how_to_avoid_overfitting_to_a_specific_level_when/</link>
      <description><![CDATA[在训练过程中，我在理解如何确保模型保持足够通用以进入新水平时遇到了一些困难。特定级别的诗歌学习技巧。 在培训过程中轮换级别是最佳实践吗？不同类型的水位何时需要不同的模型（例如 SMW 水位与标准水位）？ ​ ​ &lt; /div&gt;  由   提交/u/drupadoo  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b41brf/how_to_avoid_overfitting_to_a_specific_level_when/</guid>
      <pubDate>Fri, 01 Mar 2024 18:19:22 GMT</pubDate>
    </item>
    <item>
      <title>Demis Hassabis 播客采访（2024-02）：“规模化、超人人工智能、法学硕士之上的 AlphaZero、Rogue Nations 威胁”（Dwarkesh Patel）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3ynfp/demis_hassabis_podcast_interview_202402_scaling/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3ynfp/demis_hassabis_podcast_interview_202402_scaling/</guid>
      <pubDate>Fri, 01 Mar 2024 16:35:15 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习实践。第三版</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3xj77/deep_reinforcement_learning_handson_third_edition/</link>
      <description><![CDATA[嘿伙计们， 正准备升级并购买本书的第二版，但后来偶然发现了 github 页面第三版似乎是一项正在进行中的积极工作：https://github.com/PacktPublishing/深度强化学习动手 3E。有谁知道这本书什么时候出新版吗？如果它们很快就会上市的话，我想避免购买它们。 ​ 谢谢！   由   提交 /u/Any_Camel_5977   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3xj77/deep_reinforcement_learning_handson_third_edition/</guid>
      <pubDate>Fri, 01 Mar 2024 15:50:40 GMT</pubDate>
    </item>
    <item>
      <title>什么是 RLHF？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3xb49/what_is_rlhf/</link>
      <description><![CDATA[RLHF (Reinforcement Learning with Human Feedback) 是一种使用奖励模型 + PPO 算法组合进一步微调 LLM 的方法，通常用于训练ChatGPT 也是如此。看看它是如何工作的：https://youtu.be/r35WvNUwY9E?si=oEXs_WSRWE2NzMVF   由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3xb49/what_is_rlhf/</guid>
      <pubDate>Fri, 01 Mar 2024 15:41:45 GMT</pubDate>
    </item>
    <item>
      <title>为什么在数学中进行自动定理证明的深度强化学习如此困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3vm43/why_is_it_so_difficult_to_do_deep_reinforcement/</link>
      <description><![CDATA[请记住，我的领域是数学。我是 ML/DL 的新手： 我们有 AlphaGo 和 AlphaFold 类型的系统，它们在各自的领域都是超人的。我读到研究人员很难为数学研究建立类似的系统，即“超人数学家”。那么，为什么困难呢？鉴于法学硕士以及上述系统的进步，我什么时候应该开始担心某些人工智能系统在各个方面都优于我？  作为一个新手，在我看来，这些系统的发展速度比任何人想象的都要快，但这也许是因为我看到的唯一新闻是大肆宣传的。我感受到的存在主义恐惧太多了。   由   提交 /u/dhhdhkvjdhdg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3vm43/why_is_it_so_difficult_to_do_deep_reinforcement/</guid>
      <pubDate>Fri, 01 Mar 2024 14:29:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么在决策后状态上使用 Q 学习而不是值迭代？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3u9nf/why_use_qlearning_instead_of_value_iteration_on/</link>
      <description><![CDATA[在《近似动态规划》第 390 页，Powell 认为 Q 学习使用“人工决策后状态……状态/动作对 (S, A）”。他进一步说“使用决策后状态变量的值函数进行迭代相当于 Q 学习”……只是更容易，因为决策后状态更紧凑 这对我来说似乎是合乎逻辑的。如果我们可以只处理该组合的最终结果，为什么我们还要关心（状态，动作）组合？ 但是，在我见过的文献和算法实现中，Q 学习似乎是一种更受欢迎。为什么是这样？直接使用（状态、动作）对有什么好处吗？   由   提交/u/lilganj710  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3u9nf/why_use_qlearning_instead_of_value_iteration_on/</guid>
      <pubDate>Fri, 01 Mar 2024 13:28:50 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习的奖励尊重子任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3rax7/rewardrespecting_subtasks_for_modelbased/</link>
      <description><![CDATA[论文：https://www.sciencedirect.com/science/article/pii/S0004370223001479 预印本版本：https://arxiv.org/abs/2202.03466 摘要：  &lt;为了实现人工智能的宏伟目标，强化学习必须包括使用状态和时间抽象的世界模型进行规划。深度学习在状态抽象方面取得了进展，但尽管基于选项框架的理论得到了广泛的发展，但时间抽象却很少被使用。原因之一是可能的选项空间巨大，并且先前提出的选项发现方法没有考虑选项模型将如何在规划中使用。通常通过提出辅助任务来发现选项，例如达到瓶颈状态或最大化除奖励之外的感官信号的累积和。解决每个子任务以产生一个选项，然后学习该选项的模型并将其提供给规划过程。在大多数以前的工作中，子任务忽略了原始问题的奖励，而我们提出的子任务使用原始奖励加上基于选项终止时状态特征的奖励。我们表明，从此类尊重奖励的子任务中获得的选项模型比特征选项、基于瓶颈状态的最短路径选项或由选项批评家生成的尊重奖励的选项更有可能在规划中有用。尊重子任务的奖励强烈限制了选项的空间，从而也为选项发现问题提供了部分解决方案。最后，我们展示了如何使用标准算法和通用价值函数在线和离线学习价值、策略、选项和模型。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3rax7/rewardrespecting_subtasks_for_modelbased/</guid>
      <pubDate>Fri, 01 Mar 2024 10:40:35 GMT</pubDate>
    </item>
    <item>
      <title>限制行走的关节范围有多重要？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3cof4/how_important_is_limiting_joint_range_for_walking/</link>
      <description><![CDATA[我有一只四足动物，我正在尝试教它走路。这是一个相当长期的项目，我希望最终有两种运动模式（步行和飞行），所以比例对于步行者来说有点奇怪，并且前肢上有一个额外的肢体部分，目前没有用于任何用途. 现在，我正在为两种运动模式提供关节的全方位运动，但我的训练进行得不太顺利。我正在使用 Mujoco + MJX/Brax 在云 GPU 上进行训练。在 MJX 示例中，它训练了一个与我的模型具有大约相同自由度的人形机器人，使其以大约 10 分钟/3000 万个时间步长行走，但我的机器人在 3000 万个时间步长后仍然在四处走动，尽管奖励曲线在我未经训练的眼睛看来还不错。  我尝试过将奖励权重更倾向于向前、直立运动，也尝试过改变位移和速度之间的向前奖励，但没有任何效果。当使用速度奖励时，它似乎学会了向前猛冲，然后因为摔倒而立即死亡。有了位置奖励，它并没有真正取得任何进展。 所以我想知道是否需要进一步限制关节范围以适应步行所需的范围。我还将尝试减少所有关节的执行器扭矩，我现在认为它们可能有太大的扭矩来进行精细控制。从政策角度来看，到目前为止我尝试过的都是 PPO，但根据我读到的所有说法，这不应该导致此类问题。 欢迎任何和所有想法/建议！我对强化学习完全陌生，所以我确信我犯了一些容易修复的新手错误，只需要弄清楚它们是什么。 模型 奖励曲线 项目仓库   由   提交 /u/AnAngryBirdMan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3cof4/how_important_is_limiting_joint_range_for_walking/</guid>
      <pubDate>Thu, 29 Feb 2024 21:57:46 GMT</pubDate>
    </item>
    </channel>
</rss>