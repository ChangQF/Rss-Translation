<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 26 Dec 2023 03:14:15 GMT</lastBuildDate>
    <item>
      <title>[帮助] Stable Baselines3 中的 Dict 操作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/</link>
      <description><![CDATA[大家好。 我在创建对产品列表页面中的项目进行排序的 RL 代理时遇到了一些问题。我有一个产品列表，我希望观察/状态是按一定顺序排列的产品 ID 列表。例如：[0,2,4,1,2]。这意味着 id 0 的产品是页面上的第一个项目，第二个是产品 2..etc  该操作将是一个带有产品 id 的字典以及是否将其在列表中向上移动，放下或将其留在原处。 这是我的做法： fromgymnasium.spaces import Dict, Discrete, Sequence, MultiDiscrete class CustomEnvironment(gym.Env)： def __init__(self, number_products, seeds=None): self.number_products = number_products # 随机选择起始状态 self.starter_state = np.array([i for i in range(number_products)]) random.Random(seed).shuffle( self.starter_state) self.current_state = self.starter_state # 0 = 向上，1 = 没有变化，2 = 向下 self.action_space = Dict({&quot;product&quot;: Discrete(number_products), &quot;move&quot;: Discrete(3 )}) self.observation_space = MultiDiscrete([number_products] * number_products)  我想使用稳定基线3，但当我运行稳定基线&#39;.check_env时，我收到以下警告： 用户警告：操作空间不是基于 numpy 数组。通常这意味着它是字典或元组空间。 Stable Baselines 3 目前不支持这种类型的操作空间。您应该尝试使用包装器来展平操作。  知道如何解决这个问题吗？ 任何帮助将不胜感激:) 谢谢！  ​   由   提交 /u/Rich-Professional171   /u/Rich-Professional171 reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qvovr/help_dict_action_space_in_stable_baselines3/</guid>
      <pubDate>Tue, 26 Dec 2023 00:57:34 GMT</pubDate>
    </item>
    <item>
      <title>“ReBRAC：重新审视离线强化学习的极简方法”，Tarasov 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qrlxm/rebrac_revisiting_the_minimalist_approach_to/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qrlxm/rebrac_revisiting_the_minimalist_approach_to/</guid>
      <pubDate>Mon, 25 Dec 2023 21:31:40 GMT</pubDate>
    </item>
    <item>
      <title>强化学习以片段而非步骤的形式进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qgg22/rl_training_in_episodes_instead_of_steps/</link>
      <description><![CDATA[        由   提交/u/TwTC8  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qgg22/rl_training_in_episodes_instead_of_steps/</guid>
      <pubDate>Mon, 25 Dec 2023 10:50:20 GMT</pubDate>
    </item>
    <item>
      <title>训练人形机器人如何行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18qbd50/training_humanoid_how_to_walk/</link>
      <description><![CDATA[     &lt; td&gt; 大力投资特斯拉期权，决定将我的钱投入人形机器人。将使用强化学习训练站立和行走。如果有人想聚会的话，我住在纽约。   由   提交 /u/Logical_Flatworm8179   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18qbd50/training_humanoid_how_to_walk/</guid>
      <pubDate>Mon, 25 Dec 2023 04:32:17 GMT</pubDate>
    </item>
    <item>
      <title>超越人类数据：利用语言模型扩展自我训练以解决问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pzr9g/beyond_human_data_scaling_selftraining_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.06585 摘要：  对人类生成的语言模型~（LM）进行微调数据仍然是一种普遍的做法。然而，此类模型的性能通常受到高质量人类数据的数量和多样性的限制。在本文中，我们探讨了在我们可以获得标量反馈的任务中是否可以超越人类数据，例如，在可以验证正确性的数学问题上。为此，我们研究了一种基于期望最大化的简单自我训练方法，我们称之为 ReSTEM，其中我们（1）从模型生成样本并使用二进制反馈对其进行过滤，（2）在这些样本上微调模型，以及（3）重复此过程几次。使用 PaLM-2 模型对高级 MATH 推理和 APPS 编码基准进行测试，我们发现 ReSTEM 可以很好地随模型大小进行扩展，并且显着超越仅在人类数据上进行的微调。总的来说，我们的研究结果表明，带有反馈的自我训练可以大大减少对人类生成数据的依赖。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pzr9g/beyond_human_data_scaling_selftraining_for/</guid>
      <pubDate>Sun, 24 Dec 2023 17:35:04 GMT</pubDate>
    </item>
    <item>
      <title>矢量化训练会导致性能下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pz7rq/performance_degrades_with_vectorized_training/</link>
      <description><![CDATA[我对 RL 还很陌生，但在读完 Sutton 和 Barto 的书后，我决定自己尝试实现一些 RL 算法。我根据书中的算法实现了一种非常简单的深度演员评论家算法，并且在正确的学习率下，性能出奇地好。我什至能够在没有回复缓冲区的体育馆中的月球着陆器上获得不错的结果。我决定尝试在多个环境中同时训练它，认为这会提高稳定性并加快学习速度，但令人惊讶的是，它似乎产生了相反的效果。使用更多矢量化环境时，算法变得越来越不稳定。有谁知道可能是什么原因造成的？   由   提交/u/YouPspecial8085   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pz7rq/performance_degrades_with_vectorized_training/</guid>
      <pubDate>Sun, 24 Dec 2023 17:07:54 GMT</pubDate>
    </item>
    <item>
      <title>Python RL 中设置训练“基本法则”的最佳位置是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pgh6z/the_best_place_in_python_rl_to_set_the/</link>
      <description><![CDATA[嗨 我一直在努力理解放置“基本法则”的最佳位置。对于 PPO 模型。我在某处读到它应该在环境中，而在其他地方，我读到它最好在预处理器中完成。就我而言（只是为了好玩而玩股票交易 RL），我只希望它在位置 = 0 时买入。我当然可以在环境中对此进行调整，但它“感觉”不太好。错误（不知道为什么..）。我宁愿把它放在代理那边，但也许我弄错了..    由   提交/u/Forward-Cranberry-30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pgh6z/the_best_place_in_python_rl_to_set_the/</guid>
      <pubDate>Sat, 23 Dec 2023 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>Pearl：生产就绪的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18p82ug/pearl_a_productionready_reinforcement_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.03814 代码：https://github .com/facebookresearch/pearl 项目页面：https://pearlagent. github.io/ 摘要：  强化学习（RL）为实现长期目标提供了一个多功能框架。它的通用性使我们能够形式化现实世界智能系统遇到的各种问题，例如处理延迟奖励、处理部分可观察性、解决探索和利用困境、利用离线数据提高在线性能以及确保安全约束遇见了。尽管强化学习研究社区在解决这些问题方面取得了相当大的进展，但现有的开源强化学习库往往只关注强化学习解决方案管道的一小部分，而其他方面基本上无人关注。本文介绍了 Pearl，这是一个生产就绪的 RL 代理软件包，专门设计用于以模块化方式应对这些挑战。除了提供初步基准测试结果外，本文还重点介绍了 Pearl 的行业采用情况，以证明其已做好生产使用的准备。 Pearl 在 Github 上开源，网址为 此 http URL，其官方网站位于 这个http URL。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18p82ug/pearl_a_productionready_reinforcement_learning/</guid>
      <pubDate>Sat, 23 Dec 2023 15:32:28 GMT</pubDate>
    </item>
    <item>
      <title>这个人工智能程序无需任何编码即可学习任何游戏。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oyeum/this_ai_program_can_learn_any_game_without_any/</link>
      <description><![CDATA[    /u/Worldly-Daikon5001   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oyeum/this_ai_program_can_learn_any_game_without_any/</guid>
      <pubDate>Sat, 23 Dec 2023 05:16:28 GMT</pubDate>
    </item>
    <item>
      <title>“MetaDiff：用于少样本学习的条件扩散元学习”，Zhang & Yu 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ori1g/metadiff_metalearning_with_conditional_diffusion/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ori1g/metadiff_metalearning_with_conditional_diffusion/</guid>
      <pubDate>Fri, 22 Dec 2023 23:06:12 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：TD3Policy.forward() 需要 2 到 3 个位置参数，但给出了 4 个（自定义多代理环境）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oq1y5/typeerror_td3policyforward_takes_from_2_to_3/</link>
      <description><![CDATA[我计划将 TD3 与 MultiInputPolicy 结合使用，该策略接受我的自定义多代理环境的 Dict 类型观察。  ...train.py”，第 114 行，在  中model = TD3( ^^^^ 文件“D:\anaconda3\Lib\site-packages\stable_baselines3\td3\td3.py”，第 137 行，在 __init__ self._setup_model() 文件“D:\anaconda3\Lib” \site-packages\stable_baselines3\td3\td3.py”，第 140 行，在 _setup_model super()._setup_model() 文件“D:\anaconda3\Lib\site-packages\stable_baselines3\common\off_policy_algorithm.py”，行199、在 _setup_model self.policy = self.policy_class( ^^^^^^^^^^^^^^^^^^^ 文件 &quot;D:\anaconda3\Lib\site-packages\torch\nn\modules\ module.py”，第 1518 行，在 _wrapped_call_impl 中 return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^ 文件“D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py”，第 1527 行，在 _call_impl 返回forward_call(*args, **kwargs) ^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^ TypeError: TD3Policy.forward() 采用 2 到 3 个位置参数，但给出了 4 个 &lt; p&gt;相关模型和策略定义：  model = TD3(policy=policy, env=env, ... )  我尝试替换 env使用 TD3 的已知工作健身房环境（“Pendulum-v1”），会产生相同的错误。因此，我开始研究策略定义：  policy = MultiInputPolicy( env.observation_space, env.action_space, lr_schedule, ... }  还有这个把我带回到环境中，是我的观察和动作空间有问题吗？请指教。 ``` ... self.action_space = Box( 0.0, +1.0, shape=(len(self .actions.keys()),), dtype=np.float32 )  self.observation_space = Dict( { &quot;a&quot;: Box( -2.0, +1.0, shape=(2 * r1 + 1, r2+ 1), dtype=np.float32, ), &quot;b&quot;: Box( -1.0, 1.0, shape=(2 * r1 + 1, r2+ 1), dtype=np.int32, ), “c”: Box( -1.0, 100.0, shape=(2 * r1 + 1, r2 + 1), dtype=np.float32, ), } ) ...   ```    提交者   / u/fatalStrike97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oq1y5/typeerror_td3policyforward_takes_from_2_to_3/</guid>
      <pubDate>Fri, 22 Dec 2023 21:58:44 GMT</pubDate>
    </item>
    <item>
      <title>dm_control中的cmu_ humanoid是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oiulk/what_is_the_cmu_humanoid_in_dm_control/</link>
      <description><![CDATA[嗨， 最近我一直在探索 dm_control 库并遇到了 cmu_ humanoid。现在我知道人形生物的样子了。我不确定他们为什么将其称为 cmu_ humanoid 。是因为他们用了cmu数据集的关节和骨骼吗？还是因为Humanoid直接兼容cmu数据集，可以直接在mujoco中使用？或者是其他什么？ 提前感谢您的宝贵时间和回复。   由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oiulk/what_is_the_cmu_humanoid_in_dm_control/</guid>
      <pubDate>Fri, 22 Dec 2023 16:29:47 GMT</pubDate>
    </item>
    <item>
      <title>ReCoRe：世界模型的正则化对比表示学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.09056 摘要：  虽然最近的无模型强化学习（RL）方法已经证明尽管在游戏环境中的人类水平的有效性，他们在视觉导航等日常任务中的成功受到限制，特别是在显着的外观变化下。这种限制源于 (i) 样本效率差和 (ii) 过度拟合训练场景。为了应对这些挑战，我们提出了一个世界模型，该模型使用（i）对比无监督学习和（ii）干预不变正则化器来学习不变特征。学习世界动态的显式表示（即世界模型）可以提高样本效率，而对比学习隐式地强制学习不变特征，从而提高泛化能力。然而，由于缺乏视觉编码器的监督信号，对比损失与世界模型的简单集成失败了，因为基于世界模型的强化学习方法独立地优化了表示学习和代理策略。为了克服这个问题，我们提出了一种以辅助任务（例如深度预测、图像去噪等）形式存在的干预不变正则化器，它明确地强制风格干预的不变性。我们的方法优于当前最先进的基于模型和无模型的 RL 方法，并且在 iGibson 基准评估的分布外点导航任务上表现显着。我们进一步证明，我们的方法仅通过视觉观察，优于最近的语言引导的点导航基础模型，这对于在计算能力有限的机器人上部署至关重要。最后，我们证明我们提出的模型在 Gibson 基准上的感知模块的模拟到真实转换方面表现出色。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</guid>
      <pubDate>Fri, 22 Dec 2023 13:36:49 GMT</pubDate>
    </item>
    <item>
      <title>是否有研究在 RL 期间使用 LoRA 和 QLoRA 等参数高效训练来进行预训练模型？我想在一些大型模型上运行强化学习，并且希望避免购买无数的 GPU。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oe4t2/has_there_been_any_research_into_using/</link>
      <description><![CDATA[我有一些相当大的预训练模型，我想在它们上运行强化学习，并且更愿意从成本较低的训练选项开始。有人知道是否有任何论文或文章详细介绍了使用 LoRA 等技术进行强化学习吗？   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oe4t2/has_there_been_any_research_into_using/</guid>
      <pubDate>Fri, 22 Dec 2023 12:46:22 GMT</pubDate>
    </item>
    <item>
      <title>“强化学习迁移的基础：知识模态分类”，Wulfmeier 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</guid>
      <pubDate>Fri, 22 Dec 2023 01:29:18 GMT</pubDate>
    </item>
    </channel>
</rss>