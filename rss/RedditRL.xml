<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 08 Feb 2024 00:56:59 GMT</lastBuildDate>
    <item>
      <title>使用 A2C 训练的纸牌游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1al84n0/cards_game_trained_using_a2c/</link>
      <description><![CDATA[查看我如何使用 Tensorflow 和 Openai Gym 使用 A2C 算法训练纸牌游戏代理。 https://youtu.be/Odaa9T6PxkQ?si=qned3bP2n60eBGma   由   提交 /u/mehulgupta7991   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1al84n0/cards_game_trained_using_a2c/</guid>
      <pubDate>Wed, 07 Feb 2024 17:17:48 GMT</pubDate>
    </item>
    <item>
      <title>MA 环境中的独立 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1al6i7x/independent_dqn_in_ma_environments/</link>
      <description><![CDATA[假设我们使用 3x3 棋盘玩经典的井字游戏。简单地使用 DQN 输入大小 18（每个代理的串联 one-hot 编码向量）和输出大小 9（动作空间）是否就足够了？  问题 1：状态是否应该仅在给定代理的回合中观察到：X 是否应该每隔一轮才添加到其重播缓冲区？ 问题 2：重播缓冲区应该如何设置结构化的？基于 DQN 架构，重播缓冲区似乎应包含 (s、a、s&#39;、r、terminal) 形式的元素，其中 s 是长度为 18 的向量。这是正确的方法吗？还是应该重播buffer 包含历史记录：(h_t, a_t, h_t+1, r_t+1,terminal)，其中 h_t 是状态序列：(s0, s1, ..., st) [此处也与 Q1 相关]。如果重播缓冲区应该是 (h_t, a_t, h_t+1, r_t+1,terminal)，那么应该如何构造 DQN 来接受像 h_t 这样可变大小的输入？   由   提交/u/Top_Method_4623  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1al6i7x/independent_dqn_in_ma_environments/</guid>
      <pubDate>Wed, 07 Feb 2024 16:11:32 GMT</pubDate>
    </item>
    <item>
      <title>用于离散、未知动态设置的控制技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akz6eq/control_techniques_for_discrete_unknowndynamics/</link>
      <description><![CDATA[我正在研究交通信号控制问题，我想尝试除常见的 RL 算法之外的其他类型的控制算法。 这个问题很困难，因为我们没有系统模型：  进入车辆的数量是随机的，它取决于一天中的时间和特定场景 出站车辆的数量也是随机的：当我们为某个通道提供绿灯时，通过的车辆数量取决于有多少车辆已经在等待（可测量）其他车辆的当前位置和速度。另一方面，状态空间和动作空间很小且离散，因此可以通过广泛的模型系列来处理它们/  除了常见的强化学习算法之外，您能否建议一些其他方法来解决这个问题？我对任何非强化学习解决方案持开放态度，但也对混合解决方案持开放态度。例如，我想尝试学习系统的监督模型并应用 MPC 或用它进行规划。我还可以尝试其他有趣的事情吗？   由   提交 /u/fedetask   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akz6eq/control_techniques_for_discrete_unknowndynamics/</guid>
      <pubDate>Wed, 07 Feb 2024 09:32:10 GMT</pubDate>
    </item>
    <item>
      <title>Deep Q网络的计算复杂度分析</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akfg53/computational_complexity_analysis_of_deep_q/</link>
      <description><![CDATA[大家好， 是否有用于计算训练 Deep Q 网络的计算复杂度的库？ &lt; p&gt;1）虽然使用品质因数（例如所花费的时间）来生成情节（例如情节与所花费的时间）是可以的。还可以使用哪些其他品质因数？ 2）DQN 的计算复杂度可以与标准前馈网络的计算复杂度相比较吗？  目前我的实现是基于 PyTorch 的。我期待任何想法。  谢谢。   由   提交/u/Putrid_Drummer_2870   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akfg53/computational_complexity_analysis_of_deep_q/</guid>
      <pubDate>Tue, 06 Feb 2024 17:37:31 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习训练 AI 相扑机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akeohu/training_an_ai_sumo_robot_with_reinforcement/</link>
      <description><![CDATA[   /u/ungluffy  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akeohu/training_an_ai_sumo_robot_with_reinforcement/</guid>
      <pubDate>Tue, 06 Feb 2024 17:05:20 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 用于非视觉控制任务？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1akcyjs/dreamerv3_for_nonvisual_control_tasks/</link>
      <description><![CDATA[tl;dr：Dreamer 是否是非视觉控制任务的适当选择？如果是，世界模型的估计会如何变化？  目前，我正在开展一项比较研究，将多种 RL 算法应用于非视觉问题（例如 DSGE 模型、基于代理的模型……）。 所以到目前为止，我只应用了无模型算法，并且我想在我的分析中至少包含一种基于模型的算法。 Dreamer 似乎是迄今为止最先进的算法之一。 我的问题有两个：  如果我有基于矢量的观察（所以没有像素，但有几个连续变量），我的序列模型的输入是什么/我的 z 是什么？只是矢量格式的观察结果？  dreamer 是否是适合此类任务的算法，或者是否有更好的拟合算法？在这种情况下，梦想家就像是用大锤敲碎坚果吗？    由   提交 /u/Tortoise_vs_Hare   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1akcyjs/dreamerv3_for_nonvisual_control_tasks/</guid>
      <pubDate>Tue, 06 Feb 2024 15:52:45 GMT</pubDate>
    </item>
    <item>
      <title>[DQN] 我听说过 DQN 中的灾难性遗忘。可能是这样，还是由于超参数或体验重放缓冲区造成的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ak99fw/dqn_i_have_heard_about_catastrophic_forgetting_in/</link>
      <description><![CDATA[   嗨！ 我的 DQN 算法不断输出类似的结果，其中平均奖励随着情节的推移缓慢增加，但在某些时候它开始波动很大。似乎价值从高值（可能是最大奖励）跳到接近于零，然后回到最大奖励。这几乎就像忘记了然后重新拾起并获得高额回报。这可能是灾难性的遗忘，还是与经验重放等有关？ DQN 用于股票交易，其中操作被映射到买入/卖出（操作空间=2）。夏普比率表示代理的盈利能力，在收敛之前会很好地增加。奖励功能是每天的利润。数据集由 1759 个训练期数据集和 504 个测试期数据集组成。正如我所描述的，平均奖励似乎只是波动。其他超参数有： # DQN 算法相关的默认参数gamma = 0.8learningRate = 0.0001targetNetworkUpdate = 1000learningUpdatePeriod = 1# Experience Replay 机制相关的默认参数capacity = 10000batchSize = 64experiencesRequired = 1000#深度神经网络numberOfNeurons_h1 = 32numberOfNeurons_h2 = 64numberOfNeurons_h3 = 128dropout = 0.2# 与Epsilon-Greedy探索技术相关的默认参数epsilonStart = 1.0epsilonEnd = 0.01epsilonDecay = 10000# 关于粘性动作的默认参数RL泛化技术alpha = 0.1# 与预处理filterOr相关的默认参数德尔 = 5 # 与梯度和RL裁剪相关的默认参数rewardsgradientClipping = 1rewardClipping = 1# 与L2正则化相关的默认参数L2Factor = 0.000001 ​ https://preview.redd.it/9fh06f7qqygc1.png?width=640&amp; ;format=png&amp;auto=webp&amp;s=34e0c90642f1db8124169b97f032a2a131377a93 https://preview.redd.it/nz0x7e7qqygc1.png?width=640&amp;format=png&amp;auto=webp&amp;s=f4f67b621d1b42a8ae24c245fc8b7c 1f3ef05e7c 这个是剧集长度较短的结果，可能具有稍微不同的超参数，但会导致平均奖励的类似波动。 ​ https://preview.redd.it/6rpqberzqygc1.png?width=640&amp;format=png&amp;auto= webp&amp;s=e178a8772f284094990e8babc8bf97c3cd223538 有谁知道这个问题是否可以避免，如果可以，如何避免？非常感谢！   由   提交/u/sakkee99  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ak99fw/dqn_i_have_heard_about_catastrophic_forgetting_in/</guid>
      <pubDate>Tue, 06 Feb 2024 13:03:40 GMT</pubDate>
    </item>
    <item>
      <title>您对稳定基线 3 有何看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ak893e/what_is_your_opinion_regarding_stable_baselines_3/</link>
      <description><![CDATA[Stable Baselines 3 是 PyTorch 中强化学习算法的一组可靠实现。  一开始我直接使用 pytorch/tensorflow 并尝试实现不同的模型，但这导致了大量的超参数调整。我发现稳定基线是创建完成任务的代理的一种更快的方法，但我没有看到这个子经常提到它。  使用 SB3 之类的东西有缺点吗？有更好的工具/框架吗？针对不同问题创建代理的一般方法是什么？   由   提交 /u/IntroDucktory_Clause   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ak893e/what_is_your_opinion_regarding_stable_baselines_3/</guid>
      <pubDate>Tue, 06 Feb 2024 12:07:31 GMT</pubDate>
    </item>
    <item>
      <title>收敛解是否取决于强化学习的起点？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ak2nqm/does_convergence_solution_depends_on_starting/</link>
      <description><![CDATA[对于 PPO 和 DDPG 等 RL 算法的收敛性，我有一个非常简单且愚蠢的疑问。 收敛解是否取决于强化学习的起点？就像当我开始随机训练我的强化学习算法时，收敛策略有多大不同。我知道不。训练步骤/片段的数量可能会改变（减少/增加），但最终策略将相同或与最佳策略有多大不同。   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ak2nqm/does_convergence_solution_depends_on_starting/</guid>
      <pubDate>Tue, 06 Feb 2024 05:46:40 GMT</pubDate>
    </item>
    <item>
      <title>Python/pygame Q-learning自定义flappybird环境没有学习，不知道问题是什么。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajyffx/pythonpygame_qlearning_custom_flappy_bird/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajyffx/pythonpygame_qlearning_custom_flappy_bird/</guid>
      <pubDate>Tue, 06 Feb 2024 02:02:00 GMT</pubDate>
    </item>
    <item>
      <title>DDQN目标模型更新频率</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajszta/ddqn_target_model_update_frequency/</link>
      <description><![CDATA[我想知道是否有人有关于目标模型更新频率和收敛的任何好的信息，或者可以给我相关信息。   由   提交/u/proturtle46  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajszta/ddqn_target_model_update_frequency/</guid>
      <pubDate>Mon, 05 Feb 2024 22:03:29 GMT</pubDate>
    </item>
    <item>
      <title>任何人都有处理gymnasium.spaces.Dict 的经验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajprnh/anyone_has_any_experience_with_handling/</link>
      <description><![CDATA[解决观察空间是一个 gymnasium.spaces.Dict 并包含 2 个 MultiDiscrete 组件的问题，例如  &gt; observation_dict = { &quot;height_map&quot;: MultiDiscrete(height_map_repr), &quot;visible_box_sizes&quot;: MultiDiscrete(box_repr), }  明显的解决方案是我可以将其展平并提供给我的神经网络，但我还有其他方法可以处理吗？我正在使用 pytorch。   由   提交 /u/schrodingershit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajprnh/anyone_has_any_experience_with_handling/</guid>
      <pubDate>Mon, 05 Feb 2024 19:53:33 GMT</pubDate>
    </item>
    <item>
      <title>在处理大型、多离散动作空间时，PPO 收敛到“始终相同的动作”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajo4ps/ppo_converges_to_same_action_always_when_dealing/</link>
      <description><![CDATA[您好， 我在尝试使用大型、多离散操作解决自定义环境时遇到了问题空间。环境状态表示为一个包含 100 个元素的数组，每个元素代表实际环境的一个 10x10 块。数组中的值表示相应图块上存在的当前库存（例如货币单位或其他）。对于每个图块，代理可以分配三个动作分量之一：“1”和“1”。 (实现现有库存)，“2” （投资新股）或“0” （没做什么）。环境通过移除已实现的库存(1)、初始化新库存(2)、增加未触及的库存以及随机“杀死”现有库存来计算下一个状态。目前库存（库存越高，可能性越大）。还有一些随机效应可以证明空间方法的合理性，以防万一您可能想知道......奖励信号的计算方式为已实现的库存减去已实现或初始化的每个图块的固定成本的总和。环境一开始是空的，并在 100 个时间步后被截断；理论上，最佳策略应该类似于“初始化空的图块，对低于某个阈值的图块不执行任何操作，并实现高于某个阈值的图块。” ;虽然细节可能有所不同，但初始化已经库存的瓷砖或收获空瓷砖肯定没有价值，因为这只会导致固定成本而没有相应的回报。 训练过程开始时相当平均情景回报迅速增长，远高于“每个动作组件的同等概率”基准。代理了解到，在大多数情况下，对大多数图块不执行任何操作是有用的，从而减少了实现和初始化的总体丰富度。然而，在某个（显然远离最佳点）点，情景回报突然停止增加，学习再也不会起飞。 我应该注意到它并没有急剧崩溃，也不会恢复；它只是不再改善了。到目前为止，我的所有运行中都出现了这种观察结果，并且更改一些超参数（学习率、批量大小和到目前为止的 n_epochs）只会改变整体学习速度 - 有些设置更早达到该点，有些设置更晚，但所有设置都在类似的范围内趋于平稳间歇性回报。  我调查了“终端”政策并意识到它们都是“始终采取相同的行动” （无论环境状态如何）和准确定性，这意味着它们总是在完全相同的图块上做完全相同的事情，完全忽略当前的观察。在我未经训练的眼睛看来，该算法似乎已经达到了某些局部最优值，并且无法摆脱它。 我正在寻求关于下一步该何去何从的建议。目前，我正在考虑改变我的奖励信号，以惩罚“愚蠢”的人。行动（实现空的瓷砖等）更加困难。然而，我更愿意在某种程度上保持奖励信号“人类可读”。我还想避免首先向代理提供太多的外部专家知识（并且只是通过改变奖励信号），但这主要是出于美观的原因。我还缺少哪些其他选择？进一步的超参数调整可能有任何价值吗？或者我应该看看我的代理的神经网络架构？目前，我正在使用开箱即用的 StableBaselines3 PPO...   由   提交 /u/ionatura   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajo4ps/ppo_converges_to_same_action_always_when_dealing/</guid>
      <pubDate>Mon, 05 Feb 2024 18:48:17 GMT</pubDate>
    </item>
    <item>
      <title>尽管奖励增加，PPO 模型仍无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajnub8/ppo_model_not_learning_despite_increasing_rewards/</link>
      <description><![CDATA[        由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajnub8/ppo_model_not_learning_despite_increasing_rewards/</guid>
      <pubDate>Mon, 05 Feb 2024 18:36:27 GMT</pubDate>
    </item>
    <item>
      <title>[建议]OpenAI GYM/Stable Baselines：如何设计动作空间的依赖动作子集？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ajikd2/advice_openai_gymstable_baselines_how_to_design/</link>
      <description><![CDATA[您好， 我正在开发自定义 OpenAI GYM/Stable Baseline 3 环境。假设我的环境中有总共 5 个操作 (0,1,2,3,4) 和 3 个状态 (A, B, Z)。在状态 A 中，我们只允许两个操作 (0,1)，状态 B 操作为 (2,3)  并且在状态 Z 中，所有 5 个都可供代理使用。 我一直在阅读各种文档/论坛（并且还实现了）允许所有动作在所有状态下都可用，但当在某个状态下执行无效动作时分配（大）负奖励。然而，在训练过程中，这会导致我出现奇怪的行为（特别是扰乱我的其他奖励/惩罚逻辑），这是我不喜欢的。 我想以编程方式清楚地消除每个动作中的无效动作状态，所以它们甚至不可用。使用动作组合的蒙版/向量对我来说也不可取。我还读到不建议动态改变操作空间（出于性能目的）？ TL;DR 我希望听到人们如何解决这个问题的最佳实践，我确信这对许多人来说都是常见情况。 编辑：我可能正在考虑的解决方案之一是返回 self.state 通过步骤循环中的info，然后实现一个自定义函数/lambda，它基于状态去除无效操作，但我认为这将是一个非常丑陋的黑客/干扰gym/sb的内部运作。 编辑2：再想一想，我认为上述想法真的很糟糕，因为它不允许模型学习训练阶段（循环阶段之前）的可用操作子集。因此，我认为这应该集成到环境的 Action Space 部分中。 编辑 3： 这个问题似乎也提到了 之前，但我没有使用 PPO 算法。   由   提交 /u/against_all_odds_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ajikd2/advice_openai_gymstable_baselines_how_to_design/</guid>
      <pubDate>Mon, 05 Feb 2024 15:02:50 GMT</pubDate>
    </item>
    </channel>
</rss>