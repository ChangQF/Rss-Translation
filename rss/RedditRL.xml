<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 22 Jul 2024 15:17:14 GMT</lastBuildDate>
    <item>
      <title>用于强化学习的可视化节点编程工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9agso/visual_nodes_programming_tool_for_reinforcement/</link>
      <description><![CDATA[目前存在用于机器学习的可视化编程工具，如 Visual Blocks。但是我还没有看到任何专门用于强化学习的工具。在我看来，现有的工具（如 Visual Blocks）对强化学习来说并不是很好。 拥有一个用于强化学习的可视化编程工具可能会很有用，因为它可以让开发人员快速制作原型和调试强化学习模型。 我正在考虑制作这样一个工具，它将支持现有的强化学习库，如 Tensorforce、Stable Baselines、RL_Coach 和 OpenAI Gym。 你们觉得这个想法怎么样？你知道这是否已经存在，它是否对你的职业或业余项目有用？    提交人    /u/Charming-Quiet-2617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9agso/visual_nodes_programming_tool_for_reinforcement/</guid>
      <pubDate>Mon, 22 Jul 2024 10:00:57 GMT</pubDate>
    </item>
    <item>
      <title>我找不到在 Twitter 上看到的有关新算法的帖子 (x)。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e99tmb/i_cant_find_a_post_about_the_new_algorithm_that_i/</link>
      <description><![CDATA[昨天我看到了一篇帖子，但没有保存（是的，那是一个错误），它写了一种新算法，并且成功地在 2 分钟内训练了智能体运行。我认为它是 Mujoco 环境，其中有一个类人机器人、一只蜘蛛和所有可以行走的智能体。找到它会很不错。如果您看到这篇帖子，请发送链接。 “2 分钟”肯定写在那里。 而且它也可能“近端”“扩散”“优化”。 下面是视频结果，有智能体在跑来跑去。 谢谢。    提交人    /u/imitagent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e99tmb/i_cant_find_a_post_about_the_new_algorithm_that_i/</guid>
      <pubDate>Mon, 22 Jul 2024 09:17:47 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Linux 上安装 D4RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e94noy/how_to_install_d4rl_on_linux/</link>
      <description><![CDATA[如果能给我一些关于在我的 Ubuntu 24.04 操作系统上安装 D4RL 的指导，我将非常感激。  我尝试按照 github 上的步骤安装 D4RL，但是在从互联网上下载的 mujoco 文件和 mujoco-py 之间遇到了错误。  提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e94noy/how_to_install_d4rl_on_linux/</guid>
      <pubDate>Mon, 22 Jul 2024 03:39:39 GMT</pubDate>
    </item>
    <item>
      <title>训练现实世界 RL 模型以对 3D 可变形物体进行长距离操控</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8www2/training_realworld_rl_model_of_longhorizon/</link>
      <description><![CDATA[嗨， 我正在研究一个机器人操作任务，该任务可以处理 3D 可变形物体 (DO)，例如生肉。我的一项任务是使用 RL 训练 Delta 机器人使用真空吸盘拾取和投掷。 由于 3D DO 的属性未知，并且大多数模拟目前都在 1D 和 2D 线性 DO 上进行，因此我决定进行连续动作空间、无模型真实世界训练，而无需任何模拟。 我知道我不能进行超过 400-500 次实验，因此数据有限，策略也不是最优的。但是，此处训练的目的是在这些测试中获得最佳策略。因此，我想问几个问题，并希望听取您的建议：  您应该推荐哪种方法？端到端训练 https://arxiv.org/abs/2406.13453 或任务分解，如 https://doi.org/10.3390/biomimetics8020240 我应该选择哪种算法来提高数据效率？DDPG、D4PG、HAC、.... 如果我错了，请纠正我。由于这是操纵 DO 的一系列动作，因此在执行所选动作后，“下一个状态”毫无用处。视觉系统是分开的，所以我没有使用图像作为状态，状态是{初始 S_obj 和 S_ee}。  我刚刚开始学习 RL，所以如果你们能帮助我，我将不胜感激。由于这对我来说还很新，我可能会问更多后续问题，我非常感谢你们的帮助。非常感谢！    提交人    /u/Fish_Chandle   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8www2/training_realworld_rl_model_of_longhorizon/</guid>
      <pubDate>Sun, 21 Jul 2024 21:19:05 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义 Python 和 Unity 引擎完成的虚拟 AI 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</link>
      <description><![CDATA[       由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</guid>
      <pubDate>Sun, 21 Jul 2024 15:19:07 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 MARL 解决 N vs N 追击-逃避游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</link>
      <description><![CDATA[有谁知道使用 MARL 解决 N vs N 追击规避游戏的 SOTA 工作吗？我研究过基于策略的方法（例如 MADDPG、MAPPO）和基于价值的方法（例如 QMIX、VDN、COMA 等）。大部分工作都是针对完全竞争场景（例如 GRF、SMAC、hanabi 挑战、使用启发式或随机移动的固定对手的 MPE）完成的。MADDPG 仅在混合动机场景（N vs 1）中解决了 MPE（多智能体粒子环境），其中合作智能体数量较少，对手为 1 个，由 MADDPG vs DDPG 捕获。我认为它在 N vs N 情况下无法扩展，因为 N 更高。如果有人能提出使用 MARL 进行混合动机和完全竞争设置的任何工作或想法，那将很有帮助。    提交人    /u/Meta_Sage_247   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8mi6u/how_to_solve_n_vs_n_pursuitevasion_games_using/</guid>
      <pubDate>Sun, 21 Jul 2024 13:36:51 GMT</pubDate>
    </item>
    <item>
      <title>“学习用语言模拟世界”，Lin 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8bbw1/learning_to_model_the_world_with_language_lin_et/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8bbw1/learning_to_model_the_world_with_language_lin_et/</guid>
      <pubDate>Sun, 21 Jul 2024 01:47:37 GMT</pubDate>
    </item>
    <item>
      <title>我的 PPO 代理出现奇怪的周期性峰值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8838y/weird_periodic_spikes_in_my_ppo_agent/</guid>
      <pubDate>Sat, 20 Jul 2024 23:02:51 GMT</pubDate>
    </item>
    <item>
      <title>DQN 高估问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</link>
      <description><![CDATA[      我一直在尝试从头开始实现 DQN，并创建了一个测试环境来展示我遇到的问题。环境在每一步奖励 1，并在 100 步后终止。 DQN 不采取任何行动，仅尝试根据步骤号预测每一步的累积奖励。 问题是，DQN 为后续状态输出极高的 Q 值，这与实际值相反，并且需要几个时期才能开始输出半准确值。 https://preview.redd.it/n1fd4g054jdd1.png?width=915&amp;format=png&amp;auto=webp&amp;s=5d32fa33a3ee048528b39b61239d26b9dfaf0d05 代码（根据用户 dieplstks 的评论略作修改）： import torch from torch import nn, optim import matplotlib.pyplot 作为 plt 导入随机 导入 numpy 作为 np 比例 = 100 折扣 = .9 奖励 = [1 for i in range(scale-1)] + [0] net = nn.Sequential( nn.Linear(4,32), nn.Mish(), nn.Linear(32,32), nn.Mish(), nn.Linear(32,1)) opt = optim.Adam(net.parameters()，lr=1e-2) loss = nn.MSELoss() def step(c): global discount, rewards, net, opt opt.zero_grad() qvals = [] for i in range(scale): inp = torch.tensor([[i/20 for _ in range(4)]], dtype=torch.float32) q = net(inp) qvals.append(q) if i == scale-1: next_q = torch.zeros(1,1) else: next_inp = torch.tensor([[(i+1)/20 for _ in range(4)]], dtype=torch.float32) next_q = net(next_inp).detach() target = discount*next_q + rewards[i] q_loss = loss(q, target) #opt.zero_grad() q_loss.backward() nn.utils.clip_grad_norm_(net.parameters(), 1) opt.step() if c%1 == 0: plt.plot(list(range(scale)), actual_vals) plt.plot(list(range(scale)), torch.concat(qvals).detach().numpy()) plt.savefig(f&#39;zoo{c}.png&#39;) plt.clf() #plt.show() actual_vals = np.zeros((scale),dtype=np.float32) next_val = 0 for i in reversed(range(scale)): current_val = next_val * discount + rewards[i] actual_vals[i] = current_val next_val = current_val for i in range(100): step(i)     提交人    /u/AUser213   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e7cqhq/dqn_overestimation_problem/</guid>
      <pubDate>Fri, 19 Jul 2024 19:51:56 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习用于推荐系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</link>
      <description><![CDATA[嗨：我正在寻找合适的 Python（最好是基于 PyTorch）深度强化学习库来实现多会话对话推荐引擎。理想情况下，强化学习将无模型且脱离策略，并且必须与数据库系统交互以获取用户偏好和其他上下文数据。有什么建议吗？    提交人    /u/Extra_Reflection9056   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6ydua/deep_rl_for_recommender_system/</guid>
      <pubDate>Fri, 19 Jul 2024 08:00:25 GMT</pubDate>
    </item>
    <item>
      <title>通过将实时屏幕截图作为输入并预测要模拟的 Windows 鼠标/键盘输入，训练 DQN 代理来玩自定义 Fortnite 地图。以下是可视化的卷积过滤器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</link>
      <description><![CDATA[        提交人    /u/voidupdate   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6rxzl/trained_a_dqn_agent_to_play_a_custom_fortnite_map/</guid>
      <pubDate>Fri, 19 Jul 2024 01:35:34 GMT</pubDate>
    </item>
    <item>
      <title>RL 教科书包含逆向强化学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</link>
      <description><![CDATA[寻找包含逆强化学习 (IRL) 部分的 RL 教科书。我只熟悉 Dixon 的《金融机器学习》一书，这本书很棒，但我还在寻找更多值得阅读的内容。  任何建议都值得赞赏！此外，如果您知道任何带有相应 github 的 IRL 论文，我也会喜欢的。    提交人    /u/Voltimeters   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6qmh1/rl_textbooks_with_inverse_reinforcement_learning/</guid>
      <pubDate>Fri, 19 Jul 2024 00:28:38 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 openAI Gym 中自定义环境的平等约束问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</link>
      <description><![CDATA[您好， 我正在为优化主题创建自定义环境。以下是一些详细信息： 代理观察一个连续变量：a 代理采取两个操作：b 和 c 我希望我的代理学习采取操作 b、c，使得 b + c = a 且 c 最大。 我知道这很简单。在这种情况下，代理应该采取操作 c，使得 c=a，但这只是我整个环境的一个小组成部分。 * 如何以尊重此约束的方式对环境进行建模（应始终尊重约束） * 如何对奖励进行建模。我试图将奖励作为 c，但由于这是一个绝对值，因此代理不会改善行为。 * 知道我的观察和行动都是连续变量，哪种类型的算法最适合这类问题。 提前谢谢您。    提交人    /u/Effective_Farm_4844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e6hwgc/help_with_equality_constraints_on_a_custom_env_in/</guid>
      <pubDate>Thu, 18 Jul 2024 18:11:33 GMT</pubDate>
    </item>
    <item>
      <title>DreamerV3 更新了，有什么不同</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</link>
      <description><![CDATA[      DreamerV3 最近已更新。论文中有一些变化。不幸的是，我找不到与 2023 版相比发生了哪些变化的表格。我注意到了一些变化。例如，动态损失权重从 0.5 变为 1。评论家使用真实转换来计算重放损失。优化器已经改变，他们使用自动梯度标准剪辑。我想知道是否有人注意到其他重大变化。如果有人有更改表并愿意分享就太好了！ https://preview.redd.it/r94zls5159dd1.png?width=640&amp;format=png&amp;auto=webp&amp;s=babb912867b877dd94ed98f5de6b52ddb46a1f3a    提交人    /u/yulinzxc   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67w8p/dreamerv3_updated_whats_the_difference/</guid>
      <pubDate>Thu, 18 Jul 2024 10:14:54 GMT</pubDate>
    </item>
    <item>
      <title>强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</link>
      <description><![CDATA[您好， 我正在学习 Richard Sutton 的书，对强化学习有了一些了解，也将其应用于一些项目。 我想写一篇研究论文：希望申请研究型硕士学位。 你们有什么建议吗？我应该考虑哪些事情？你们如何进行研究？ 谢谢！    提交人    /u/Original_Phrase1902   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e67lpl/research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 18 Jul 2024 09:55:25 GMT</pubDate>
    </item>
    </channel>
</rss>