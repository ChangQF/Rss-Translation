<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 09 Sep 2024 21:14:52 GMT</lastBuildDate>
    <item>
      <title>呼吁采取行动：对问题做出反应以帮助提供资金</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</link>
      <description><![CDATA[嗨，RL 的朋友们， 我和我的同事们一直在开发 Tianshou 库，并且即将将其变成真正造福社区的东西。这是我所知道的唯一一个具有广泛范围（各种算法、离线 RL、marl 等，最先进的性能和快速吞吐量）的库，旨在为研究人员和应用程序开发人员提供帮助。 现在我们公司发生了一些变化，导致新经理是一位非技术人员，不熟悉战略。如果我们不能以他们理解的方式展示社区的兴趣，该项目的资金可能会被取消。 我已经写了一个非常详细的计划，将 Tianshou 带到下一个主要版本，我相信结果对整个 RL 社区非常有用。我非常感谢您的支持，您只需在此问题上发表评论即可。 当然，我也很高兴在那里就该计划和图书馆本身进行任何建设性的讨论！    提交人    /u/Left-Orange2267   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd045g/call_for_action_react_to_an_issue_to_help_out/</guid>
      <pubDate>Mon, 09 Sep 2024 20:50:01 GMT</pubDate>
    </item>
    <item>
      <title>你们在哪里使用强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</link>
      <description><![CDATA[嗨，朋友们！ 我正在研究 RL，我想知道哪些公司正在应用 RL 来解决业务问题。当我搜索这个主题时，我只找到旧案例和来自大型科​​技公司的案例。 你们在学术界使用 RL 吗？你们在初创公司使用 RL 吗？只是想知道你们如何使用它并试图了解市场。 谢谢！    提交人    /u/embedding_turtle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fd02v6/where_you_guys_are_using_reinforcement_learning/</guid>
      <pubDate>Mon, 09 Sep 2024 20:48:33 GMT</pubDate>
    </item>
    <item>
      <title>深度Q迷宫</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fcjmzt/deep_q_maze/</link>
      <description><![CDATA[我正在使用深度 q 学习处理一个 8x8 的冰冻湖泊 - 它就像一个迷宫，但里面有洞，如果进入就会终止。我见过的许多其他实现直到迷宫至少完成一次才开始训练。我面临的问题是迷宫太大，无法偶然/不经过任何训练就解决（较小的迷宫效果很好）。当我允许它在不完成迷宫的情况下进行训练时，它会学会避开洞，而不是朝着终点前进。如果终止，奖励为 0，如果完成迷宫，奖励为 1 - 其他任何事情都使用奖励函数：0.9 * maxQ（下一个状态）。这个问题的解决方案是什么/我做错了什么？任何想法都将不胜感激。     提交人    /u/Magic__Mannn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fcjmzt/deep_q_maze/</guid>
      <pubDate>Mon, 09 Sep 2024 07:18:26 GMT</pubDate>
    </item>
    <item>
      <title>在价值迭代中，终端状态的值应该保持不变还是不保持不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</link>
      <description><![CDATA[我遵循了这个例子： https://courses.grainger.illinois.edu/cs440/fa2022/lectures/rl.html 这是一个网格世界，其中折扣 = 0.9，右上角的奖励为 +1，中右为 -1，而其他网格的奖励为零。 正如您所见，两个终端网格（右上和中右）的值在整个迭代过程中一直保持 +1 和 -1，这意味着它通过贝尔曼方程的迭代强制保持它们不变，但我在网上找到了多个其他数值例子，其中终端状态的值也根据贝尔曼方程进行更新。所以我的问题是：哪一种是正确的方法？非常感谢    提交人    /u/james_stevensson   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fc8wel/in_value_iteration_should_the_value_for_the/</guid>
      <pubDate>Sun, 08 Sep 2024 21:31:57 GMT</pubDate>
    </item>
    <item>
      <title>有沒有主动推理（自由能原理）的成功故事？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</link>
      <description><![CDATA[虽然自由能原理旨在通过最小化意外来开发一个统一的感知和控制框架，但据我所知，几乎没有经验结果可以证明其前景。有没有人听说过它在图像输入的连续控制问题或至少一些经典控制问题中的一些成功应用？    提交人    /u/OutOfCharm   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbu536/any_successful_story_of_active_inference_free/</guid>
      <pubDate>Sun, 08 Sep 2024 09:48:59 GMT</pubDate>
    </item>
    <item>
      <title>比较 V-Trace 和 Retrace</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbizt8/comparing_vtrace_and_retrace/</link>
      <description><![CDATA[看起来 V-Trace 和 Retrace 彼此非常相似，并且用途也非常相似，即纠正离策略数据的值估计。据我所知，唯一真正的区别是 V-Trace 增加了一个额外的裁剪参数（rho 参数），但我很难理解除此之外的区别。 到目前为止，我找不到两者的比较或任何提及它们相似性的文章（甚至 IMPALA 论文也没有提到 Retrace）。我还注意到，最近的作品提到了 Retrace，但没有提到 V-Trace（例如，Muesli 论文使用了 Retrace），尽管 V-Trace 的开发时间比 Retrace 晚两年。 所以我的问题是，我应该如何理解两者之间的关系？V-Trace 是否严格优于 Retrace，但它在 IMPALA 之外从未真正流行过？或者它们用于完全不同的事情，而我完全误解了一些东西？    提交人    /u/vandelay_inds   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbizt8/comparing_vtrace_and_retrace/</guid>
      <pubDate>Sat, 07 Sep 2024 22:30:02 GMT</pubDate>
    </item>
    <item>
      <title>基于 pybullet 的健身房环境中的 PPO 四足动物行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/</link>
      <description><![CDATA[嗨，我正在尝试训练四足动物在基于 pybullet 的自定义健身环境中行走。我使用 Stablebaselines3 中的 PPO 作为具有默认超参数的算法。四足动物每条腿有 3 个自由度（总共 12 个）。观察空间是四足动物的基本线性位置、基本线性速度、基本角位置、基本角速度、12 个关节位置、12 个关节速度以及每只脚接触地面时的 4 个二进制值。动作空间和观察空间都已标准化。 奖励是沿 x 轴的正向移动（向前移动），惩罚是侧向漂移（沿 y 轴移动）。情节持续 5000 个时间步，但如果身体翻转也会结束。  通过运行 12 个并行环境进行训练，总共运行 100,000,000 个时间步（大约需要半天时间）。 到目前为止，最好的结果是，充其量，腿部的一些振动运动会产生向前的运动，但根本不自然。 有人有什么好的提示/资源关于如何获得更自然的结果吗？    提交人    /u/Fit_Passion6272   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fbfk2i/ppo_quadruped_walking_in_pybulletbased_gym/</guid>
      <pubDate>Sat, 07 Sep 2024 19:51:26 GMT</pubDate>
    </item>
    <item>
      <title>A2C 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fb2v7w/a2c_algorithm/</link>
      <description><![CDATA[      你好， 当我阅读《强化学习实践手册》（第二版，packt）时，我发现作者使用来自环境的奖励来计算回报 R，这更接近于蒙特卡洛更新，而不是引导从评论家估计 V(st+1)，您会在照片中找到更新。 我的问题是这会影响学习吗，这是 A2C 的另一个版本吗？ 提前谢谢您的回答！ A2C 算法，来源：动手强化学习书籍（第二版，packt）    提交人    /u/Capital_Win8377   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fb2v7w/a2c_algorithm/</guid>
      <pubDate>Sat, 07 Sep 2024 09:14:33 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的规范化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fb0fke/normalization_in_rl/</link>
      <description><![CDATA[我正在使用标准缩放和 MinMax 缩放，但它们在 RL 实现中效果不佳。我有 10 个特征，而每个特征都需要缩放到特定范围，如 [0, 1]。因为我的特征范围从 60,000 到低至 0.001。 如何才能以有效处理特征尺度上这些巨大差异的方式规范化此数据集以进行强化学习？ 有哪些高级规范化技术可以处理特征中的这种范围差异，同时仍能保持强化学习模型的稳定性？    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fb0fke/normalization_in_rl/</guid>
      <pubDate>Sat, 07 Sep 2024 06:13:13 GMT</pubDate>
    </item>
    <item>
      <title>任何好的‘Rl Agent’设计资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faqpef/any_good_rl_agent_design_resources/</link>
      <description><![CDATA[我目前对编程 RL 代理没有太多经验。我完成了 hugging face Deep RL 课程，也参加了 David Sliver 的课程（真的很棒，顺便说一下，它是免费的），所以我对一些 RL 中常用的机器学习算法背后的一些概念有一定的了解，但我从未做过“项目”或任何事情 所以我开始实施不同的 RL 算法来与 Open AI Gym 环境进行交互。但我设计的实际课程总是到处都是。  例如，我经常在代理类中存储健身房环境的第二个表示，然后从内部函数训练代理，这对我来说似乎不正确。 我在编程方面也不是很擅长（就项目和课程设计而言）——我已经做了一年的软件工程师，所以我可以很好地编写代码，只是整个设计方面对我来说很难哈哈 在构建 RL 项目时，从结构到类设计，有没有什么好的资源/例子来介绍一些好的编程实践。    提交人    /u/Amazing_Track4881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faqpef/any_good_rl_agent_design_resources/</guid>
      <pubDate>Fri, 06 Sep 2024 21:35:21 GMT</pubDate>
    </item>
    <item>
      <title>强化学习奖励可以是当前状态和新状态的结合吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</link>
      <description><![CDATA[      我正在为我的 RL 代理构建奖励函数，并考虑采取行动后当前状态和新状态的组合。据我所知，基于 Sutton &amp;，这是可能的Barto，特别是公式 3.6，其中奖励是状态-动作对和结果状态的函数。我想确保我的方法符合 RL 理论。 有人可以确认这是否有效或分享见解吗？ https://preview.redd.it/7wndf4ny98nd1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=547a77aeecc75edcee9fbd41ec45dc95c772d3d1    提交人    /u/Furious-Scientist   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falkdy/can_reinforcement_learning_rewards_be_a/</guid>
      <pubDate>Fri, 06 Sep 2024 17:57:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习竞赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</link>
      <description><![CDATA[大家好，我通过强化学习开始训练，正在提升自己。我可以在网上参加和竞争这个领域的哪些比赛，我如何才能在这个领域有更好的发展？    提交人    /u/Weary-Ad-7225   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1falaec/competition_for_reinforcement_learning/</guid>
      <pubDate>Fri, 06 Sep 2024 17:46:06 GMT</pubDate>
    </item>
    <item>
      <title>PPO 外汇交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</link>
      <description><![CDATA[      我正在使用 SB3 为外汇交易环境训练带有动作掩码的 PPO。该模型似乎学习了低点和高点枢轴点，但准确性非常可疑，而且它会反转动作，所以它在高点买入，在低点卖出！代码中哪里可能出错？  https://preview.redd.it/4dwdxlclj7nd1.png?width=986&amp;format=png&amp;auto=webp&amp;s=60ac1f2a3ce5cdd386d77d13c57c233a37be56a6    提交人    /u/Acceptable_Egg6552   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fai2t0/ppo_forex_trading/</guid>
      <pubDate>Fri, 06 Sep 2024 15:31:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 进行黑盒目标函数优化 - 需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</link>
      <description><![CDATA[大家好， 我是强化学习领域的新手，目前正在研究一个让我有点困惑的问题。我正在尝试使用强化学习来优化黑盒目标函数，但我不确定我是否走在正确的轨道上。我非常感谢这个 subreddit 中经验丰富的人提供的任何指导或见解！ 所以，事情是这样的：我有一个目标函数，它接受两个输入（x，y）并输出标量值 z。目标是找到这个函数的全局最大值。棘手的部分是我无法访问该函数的内部工作原理。我所能做的就是从一个随机位置 (x, y) 开始，然后采取小步骤来探索函数的格局。 目标函数可能看起来像这样： def objective_function(x, y): return -(x**2 - 10 * np.cos(2 * np.pi * x) + y**2 - 10 * np.cos(2 * np.pi * y) + 20)  这个想法是使用 Q 值估计和神经网络来学习最大化目标函数的最佳策略。 以下是我一直在尝试的粗略概述：  使用 PyTorch 设置 DQN 模型，该模型以当前状态 (x, y) 作为输入并输出每个动作的 Q 值。 创建重放缓冲区来存储转换（状态、动作、奖励、 在训练期间，使用 epsilon-greedy 探索根据当前状态选择一个动作。 采取行动并观察下一个状态和奖励。 将转换存储在重放缓冲区中。 如果重放缓冲区有足够的样本，则对一批转换进行采样并执行梯度下降以更新 DQN 模型。  训练后，从随机状态开始并根据学习到的 Q 值贪婪地选择动作，找到最佳解决方案。  现在，我有点不确定，需要一些帮助：  由于这个问题没有明确的终点或终端状态，所以我不确定如何调整 Q 值估计。我应该将 Q 值估计为固定步数的累积奖励，而不是直到达到终止状态？我可能离题太远了，所以如果我误解了什么，请纠正我！ 如果没有终止状态，由于起点随机且步数有限，Q 值每次都会不同。这是预料之中的，还是我以错误的方式处理这个问题？  我将非常感谢这个 subreddit 中出色的人提供的任何指导、建议或替代方法。如果您有类似的优化问题经验，或者对如何改进此场景的 DQN 方法有想法，请分享您的智慧！ 非常感谢您的帮助。我真的很高兴向大家学习，并希望在这个具有挑战性的问题上取得一些进展！    提交人    /u/Technical-Vehicle888   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1faedz5/blackbox_objective_function_optimization_with_dqn/</guid>
      <pubDate>Fri, 06 Sep 2024 12:50:41 GMT</pubDate>
    </item>
    <item>
      <title>元强化学习框架</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</link>
      <description><![CDATA[大家好， 我目前正在探索元 RL 的主题，但在寻找合适的框架/库时遇到了问题。  虽然我已经找到了一些关于元学习主题的一般内容（例如 learn2learn、更高版本），但 RL 方面往往是次要关注点。  您是否使用过某些框架？可以推荐一些吗？    提交人    /u/RandomAgentIml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1facviu/metarl_framework/</guid>
      <pubDate>Fri, 06 Sep 2024 11:32:18 GMT</pubDate>
    </item>
    </channel>
</rss>