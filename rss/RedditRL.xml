<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 13 Jan 2024 03:14:23 GMT</lastBuildDate>
    <item>
      <title>使用 RL 学习梯度下降步长</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</link>
      <description><![CDATA[       问题陈述： 我一直在研究一个使用强化学习加速梯度下降收敛的项目。我想学习一种策略，可以将梯度下降的当前状态映射到最佳动作，即本例中的步长。提醒一下：梯度下降迭代由 x_k+1 = x_k - gamma*grad(f) 给出，其中 gamma 为步长。目前，我只考虑 f(x) = x&#39;Qx 形式的凸二次函数。我想在函数分布上训练策略，以便在预测时它可以泛化到该分布中的所有函数，以及在训练期间未见过的函数。通过在每次迭代中预测最佳步长的策略，目标是梯度下降在该分布内的所有函数的迭代次数较少的情况下收敛。 ​ 当前方法： 目前，我一直在使用无模型的强化学习算法，如 Soft Actor-Critic (SAC) 和 Twin Delayed Deep Definitive Policy Gradient (TD3) 来训练策略，但我发现即使对于某个特定函数过度拟合的简单情况，所需的内存和计算量也非常高。此外，当您过度拟合（对同一函数进行训练和评估）时，您会期望奖励收敛到某个值。如图所示，奖励确实增加了，但在某些时候代理完全忘记了它所学到的东西。我使用稀疏奖励：每次迭代中收敛时为 0，未收敛时为 -1。也许最好有一个奖励，说明每次迭代中残差（=梯度范数）的减少，这样代理不仅会在回合结束时接收信息。对于状态，我尝试了不同的方法，但仅包含当前梯度似乎或多或少有效。我使用的算法是SAC，它似乎比TD3更快。演员和评论家均由神经网络参数化，每个神经网络有 3 个隐藏层和 128 个节点。我使用了 Stable-Baselines 3 的实现。 ​ 我的问题： - 是无模型的RL 解决这个问题的正确方法是什么？它的计算成本非常高。是否有更好的方法，例如基于模型的强化学习或某种策略搜索？ - 在图中，为什么奖励突然减少？它与重放缓冲区的大小有关系吗？目前我可以分配 120Gb 的内存，这已经是相当多了。 - RL 理论通常基于马尔可夫过程。因此它假设马尔可夫性质，即当前状态完全独立于先前的状态。但是，最好添加一些有关先前梯度的信息以增加动量（例如 Nesterov 加速）。在这个框架中这可能吗？ ​ https://preview.redd.it/lnn1k9s333cc1.jpg?width=937&amp;format=pjpg&amp;auto=webp&amp;s=4254e662c840e4b4ca719b1f 70a488041376fad2   由   提交 /u/Lennitar   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1957gll/learning_the_gradient_descent_stepsize_with_rl/</guid>
      <pubDate>Fri, 12 Jan 2024 22:15:47 GMT</pubDate>
    </item>
    <item>
      <title>解释方差增加，但随后稳定在低值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1956qfb/explained_variance_increases_but_then_stabilizes/</link>
      <description><![CDATA[      您好， 我正在使用 Stable Baselines 3 在我的自定义环境中训练 A2C 模型。我的自定义环境是离散的、多维的 (4 * 21)，用于操作和观察。我一直在尝试调整超参数，但似乎对于所有超参数集，都存在一个常见问题，即解释的方差首先增加，但随后保持在 &lt;&lt; 30%。 此外，当我评估我的策略时，似乎 model.predict(obs) 结果（动作预测）始终是单个动作，不依赖于观察。这是因为低解释方差表明仅使用“平均动作”并没有更好吗？谢谢！ ​ https://preview.redd.it/sr4twq7ux2cc1.png?width=1191&amp;format=png&amp;auto=webp&amp;s=0a3653b2c228fdb97306d8f8093ec5ec0926f3 b1 https://preview.redd.it/vux1wq7ux2cc1.png?width=1178&amp;格式=png&amp;auto=webp&amp;s=5da34c134fde44f7340fb30e9e678e86fe74e5bf   由   提交 /u/polymerase2   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1956qfb/explained_variance_increases_but_then_stabilizes/</guid>
      <pubDate>Fri, 12 Jan 2024 21:45:11 GMT</pubDate>
    </item>
    <item>
      <title>如何裁剪</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</link>
      <description><![CDATA[我熟悉 RL 一个月了，想在一些 Atari 游戏上练习我的知识。我阅读了 DeepMind 的文章，了解他们如何使用 DQN 执行 Atari 任务 (https://arxiv.org/abs/1312.5602)。他们在论文中表示： “直接处理原始 Atari 帧（这些帧是具有 128 个调色板的 210 × 160 像素图像）的计算要求可能很高，因此我们应用了一个基本的预处理步骤，旨在减少输入维度。对原始帧进行预处理，首先将其 RGB 表示转换为灰度并将其下采样为 110×84 图像。最终的输入表示是通过裁剪图像的 84 × 84 区域来获得的，该区域大致捕获了游戏区域。” 这对我来说似乎很合理，但我想知道他们是如何以编程方式做到这一点的？我阅读了gymnasium（不是gym）文档（https://gymnasium.farama.org/api/wrappers/）但是，尽管他们有 FrameStack 和 GreyScale 的包装器，但下采样和裁剪包装器似乎不可用。 有人知道如何做到这一点吗？  非常感谢你们。   由   提交/u/Q_H_Chu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194umh1/how_to_crop_out/</guid>
      <pubDate>Fri, 12 Jan 2024 13:07:01 GMT</pubDate>
    </item>
    <item>
      <title>混合模拟和抽象进行物理推理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194orj6/blending_simulation_and_abstraction_for_physical/</link>
      <description><![CDATA[论文：https： //osf.io/preprints/psyarxiv/f9ukv 代码： https ://github.com/flxsosa/physicals_abstraction 摘要：  人们如何能够理解日常的物理事件这么轻松？一种假设表明人们使用对世界的近似概率模拟。一个对比假设是人们使用抽象或特征的集合。这两个假设解释了物理推理的互补方面。我们开发了一个综合了两种假设的“混合模型”：在某些条件下，模拟被视觉空间抽象（线性路径投影）取代。这种抽象以保真度为代价换取了效率，混合模型预测，只要满足应用抽象的条件，人们就会犯系统性错误。我们在两个实验中测试了这一预测，参与者对下落的球是否会接触目标进行判断。首先，我们表明，当直线路径不可用时，即使仿真时间保持固定，响应时间也会更长，这与纯仿真模型（实验 1）相反。其次，我们表明人们以与线性路径投影一致的方式错误地判断了球的轨迹（实验2）。我们的结论是，人们可以使用灵活的心理物理引擎，但会在有用时自适应地调用更有效的抽象。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194orj6/blending_simulation_and_abstraction_for_physical/</guid>
      <pubDate>Fri, 12 Jan 2024 06:52:59 GMT</pubDate>
    </item>
    <item>
      <title>[问题] 具有连续动作空间的 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194mfnt/question_dqn_with_continuous_action_spaces/</link>
      <description><![CDATA[        由   提交 /u/tengboss   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194mfnt/question_dqn_with_continuous_action_spaces/</guid>
      <pubDate>Fri, 12 Jan 2024 04:40:45 GMT</pubDate>
    </item>
    <item>
      <title>太空战争 RL 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</link>
      <description><![CDATA[   /u/_Linux_AI_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194li9v/space_war_rl_project/</guid>
      <pubDate>Fri, 12 Jan 2024 03:50:56 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习中正则化和（有效）贴现之间的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1947wv7/relationship_between_regularization_and_effective/</link>
      <description><![CDATA[我在 minigrid 型环境。训练后，我将智能体置于一系列人为的情况下并测量其 Q 值，然后从这些 Q 值推断其有效折扣率（例如，根据前进价值如何随着接近目标而变化来推断折扣因子） ）。 当我以这种方式测量有效折扣因子时，它与我使用的显式折扣因子 (𝛾) 设置相匹配。 但是如果我添加对网络进行非常强的 L2 正则化（权重衰减），即使我没有更改代理的 𝛾 设置，推断的折扣因子也会降低。 有人可以帮我思考为什么会发生这种情况吗？谢谢！   由   提交/u/Beneficial_Price_560   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1947wv7/relationship_between_regularization_and_effective/</guid>
      <pubDate>Thu, 11 Jan 2024 17:57:43 GMT</pubDate>
    </item>
    <item>
      <title>《Marvin Minsky’s Vision of the Future》，Bernstein 1981（明斯基的研究生涯，包括神经网络 SNARC 小鼠）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1946okz/marvin_minskys_vision_of_the_future_bernstein/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1946okz/marvin_minskys_vision_of_the_future_bernstein/</guid>
      <pubDate>Thu, 11 Jan 2024 17:08:38 GMT</pubDate>
    </item>
    <item>
      <title>“计算机双陆棋”，Hans J. Berliner 1980（“BKG 9.8 是第一个在棋盘或纸牌游戏中击败世界冠军的计算机程序”）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1945ymh/computer_backgammon_hans_j_berliner_1980_bkg_98/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1945ymh/computer_backgammon_hans_j_berliner_1980_bkg_98/</guid>
      <pubDate>Thu, 11 Jan 2024 16:39:07 GMT</pubDate>
    </item>
    <item>
      <title>完成 RL 博士学位后我可以在哪里工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/194406q/where_can_i_work_after_finishing_a_phd_in_rl/</link>
      <description><![CDATA[ 由   提交 /u/Trevorego   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/194406q/where_can_i_work_after_finishing_a_phd_in_rl/</guid>
      <pubDate>Thu, 11 Jan 2024 15:14:40 GMT</pubDate>
    </item>
    <item>
      <title>复杂系统中的涌现和因果关系：因果涌现和相关定量研究的调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193v7hf/emergence_and_causality_in_complex_systems_a/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.16815 摘要：  涌现和因果关系是理解复杂系统的两个基本概念。它们是相互关联的。一方面，涌现是指宏观性质不能仅仅归因于个体性质的原因的现象。另一方面，因果关系可以表现出出现，这意味着随着我们提高抽象水平，新的因果律可能会出现。因果涌现理论旨在弥合这两个概念，甚至采用因果关系措施来量化涌现。本文全面回顾了因果涌现的定量理论和应用的最新进展。解决了两个关键问题：量化因果出现并在数据中识别它。解决后者需要使用机器学习技术，从而在因果出现和人工智能之间建立联系。我们强调，用于识别因果出现的架构由因果表示学习、因果模型抽象和基于世界模型的强化学习共享。因此，这些领域中任何一个领域的进步都可以使其他领域受益。评论的最后部分还讨论了潜在的应用和未来的前景。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193v7hf/emergence_and_causality_in_complex_systems_a/</guid>
      <pubDate>Thu, 11 Jan 2024 06:37:04 GMT</pubDate>
    </item>
    <item>
      <title>二维动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193ud0k/twodimensional_action_space/</link>
      <description><![CDATA[如何处理二维的动作空间，我说的不是二维的向量，而是二维的张量。 例如，一个 lstm actor，其输入是（timesteps，features=256），输出另一个特征序列（timesteps，features=4096）。特征数量如此之大，是否可以将输出展平以将其视为一维动作空间？ 为了避免混淆，上面的时间步长不是环境的时间步长，而是环境的时间步长输入数据（状态）。   由   提交 /u/FancyUsual7476   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193ud0k/twodimensional_action_space/</guid>
      <pubDate>Thu, 11 Jan 2024 05:47:15 GMT</pubDate>
    </item>
    <item>
      <title>“图式学习和重新绑定作为上下文学习和涌现的机制”，Swaminathan 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193m3bx/schemalearning_and_rebinding_as_mechanisms_of/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193m3bx/schemalearning_and_rebinding_as_mechanisms_of/</guid>
      <pubDate>Wed, 10 Jan 2024 23:04:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO 代理无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/193kw1i/ppo_agent_fails_to_learn/</link>
      <description><![CDATA[我正在做一个基于PPO算法的路径规划项目。在我的实验中，有一个16*16的网格地图，其中有几个障碍物区域作为环境，目的是训练代理直到找到到达目标的路径。这是我的模型的详细主要信息：  我部署了一个 A2C 机制，其中 Actor 和 Critic 网络都是四层结构，其中 Actor 具有 2*512*512*8 和评论家 2*512*512*1。二维输入是当前位置作为状态，八维向量输出是八个动作朝相应方向移动的概率。  超参数设置如下： 学习率：actor：3e-04，cirtic：4e-04 两个网络的最大等级：1.5&lt; /li&gt; 策略剪辑：epsilon = 0.3 折扣：gamma = 0.99 GAE 参数：lambda = 0.95 熵正则化参数：0.005&lt; /li&gt; Batchsize = 512 奖励函数设置规则如下：  当当前节点到目标的曼哈顿距离为时，获得正奖励（+1.0）较小，否则为负值（-1.0）。  当智能体移动到障碍物时，奖励为-10.0。 如果智能体从与障碍物相邻的节点离开，则奖励为+10.0。  &gt;奖励重塑：如果智能体不断朝着正确的方向前进，奖励将会额外增加。    实验结果：根据我对两个网络损失值的追踪结果，我可以非常确定该模型是迭代期间收敛。然而，收敛效果并不如我预期。 MSE 的批评者血统损失从一开始的 100, 000.00 以上开始，最终停留在 4,000.00 左右，然后开始波动。另一方面，演员损失可以从超过 100.00 开始减少，但在 60.00 左右停止减少。简而言之，培训有效果，但效果不佳。  此外，经过 5000 次训练后，回报率仍然没有明显改善。在相当大的概率下，智能体仍然会选择远离目标或移动到障碍物的动作。每集的平均回报率一直保持在 -2.0 左右，而且从未有任何改善。   ​ 在这里，我真的希望有人能帮助我解决这个问题。我将非常感谢您的帮助和建议。非常感谢！  ​ ​   由   提交/u/Tight_Boysenberry692   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/193kw1i/ppo_agent_fails_to_learn/</guid>
      <pubDate>Wed, 10 Jan 2024 22:15:32 GMT</pubDate>
    </item>
    <item>
      <title>通过体操游戏训练 CNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1939cpb/train_cnn_with_gymnasium_games/</link>
      <description><![CDATA[大家好， 我是大学讲师，想向我的学生展示 CNN 和 Deep Q 的结合-学习。他们应该接受一项任务，让代理解决一个简单的游戏（简单是因为他们应该能够使用“普通”笔记本来解决它）。我刚刚看了gymnasium的文档，但没有找到可以将图像作为状态传递的游戏。图书馆里没有这样的东西吗？ 提前感谢大家的帮助:)   由   提交/u/MarcoX0395   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1939cpb/train_cnn_with_gymnasium_games/</guid>
      <pubDate>Wed, 10 Jan 2024 14:20:53 GMT</pubDate>
    </item>
    </channel>
</rss>