<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 25 Mar 2024 15:14:55 GMT</lastBuildDate>
    <item>
      <title>ICLR 2024：可证明且实用：通过 Langevin Monte Carlo 对强化学习进行有效探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnfvrd/iclr_2024_provable_and_practical_efficient/</link>
      <description><![CDATA[ 由   提交/u/hmi2015  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnfvrd/iclr_2024_provable_and_practical_efficient/</guid>
      <pubDate>Mon, 25 Mar 2024 15:03:44 GMT</pubDate>
    </item>
    <item>
      <title>单代理或多代理设置</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bneekj/single_agent_or_multiagent_setting/</link>
      <description><![CDATA[社区您好， 我目前正在研究代理可以具有不同输出结构的情况。此外，我正在利用图神经网络来描述状态，其中包含许多类别，这些类别的数量在代理之间可能有所不同。考虑到这种情况，什么配置最合适？在多代理设置中，在我们的例子中，代理通常不会同时请求操作。在任何给定的决策点，我们可能有一个或多个代理寻求行动，而其他代理则很忙。在这种情况下，批评者网络应该接收什么输入？ 提前谢谢您   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bneekj/single_agent_or_multiagent_setting/</guid>
      <pubDate>Mon, 25 Mar 2024 14:01:10 GMT</pubDate>
    </item>
    <item>
      <title>[预测]关于预测的强化学习是否可以利用强化学习来预测河流水质预警系统。哪种方式最适合强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bndqxe/prediction_reinforcement_learning_about/</link>
      <description><![CDATA[ 由   提交/u/Abcsunny95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bndqxe/prediction_reinforcement_learning_about/</guid>
      <pubDate>Mon, 25 Mar 2024 13:32:09 GMT</pubDate>
    </item>
    <item>
      <title>[预测]关于预测的强化学习是否可以利用强化学习来预测河流水质预警系统。哪种方式最适合强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bndpwv/prediction_reinforcement_learning_about/</link>
      <description><![CDATA[ 由   提交/u/Abcsunny95  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bndpwv/prediction_reinforcement_learning_about/</guid>
      <pubDate>Mon, 25 Mar 2024 13:30:54 GMT</pubDate>
    </item>
    <item>
      <title>帮助理解 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn7ui2/help_in_understanding_ppo/</link>
      <description><![CDATA[大家好！我在从学术论文和我在网上找到的一些代码暗示中理解有关 PPO 的事情时遇到一些问题。在论文中，我了解到旧模型和新模型的输出之间存在近似。这是如何运作的？如何更新模型，然后计算更新量？我是否需要始终保存 i-1 模型以便进行计算？现在是暗示。我正在使用 IsaacGym 并一次运行 n 个模拟。所有暗示都会根据一系列动作更新模型，直到游戏完成。我希望它从我的 n 个环境中的单个操作中随机批量运行，但我很难理解我需要保存和更改的内容。保存每次迭代需要哪些参数？我想到了：放弃、行动、奖励、价值（V 净输出）、对数概率。我是否遗漏了需要保存的东西？抱歉，如果这篇文章有点长，每一个帮助都会很棒。    由   提交/u/razton  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn7ui2/help_in_understanding_ppo/</guid>
      <pubDate>Mon, 25 Mar 2024 07:33:31 GMT</pubDate>
    </item>
    <item>
      <title>机器人强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn6nns/rl_for_robotics/</link>
      <description><![CDATA[大家好，我整理了一些学习强化学习的学习材料和资源： 1) 深度强化学习，作者：加州大学伯克利分校的 Sergey Levine 2) David Silver 讲座笔记 3) Google Deepmind 讲座视频 4) NPTEL IITM 强化学习  我也更喜欢学习材料具有足够的数学严谨性，能够深入解释算法。  同时引用一堆资源也令人生畏。有人可以为像我这样的初学者推荐上面列出的材料中的笔记和讲座视频吗？如果您还有其他资源，请在评论部分提及。    由   提交 /u/Quirky_Assignment707   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn6nns/rl_for_robotics/</guid>
      <pubDate>Mon, 25 Mar 2024 06:09:33 GMT</pubDate>
    </item>
    <item>
      <title>车子仍然无法在拐角处转弯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn5bss/the_car_still_isnt_able_to_turn_at_the_corner/</link>
      <description><![CDATA[https://github.com/arthiondaena/Car -game 这是我一直在做的项目。在第 60 集，汽车能够到达拐角，但没有转弯，而是与赛道边界相撞。  我以为几集后会有所改善，但即使在 500 集之后，它仍然在拐角处崩溃。花了12个小时才达到500集。我没有看到任何改进。 如果有人能指出代码中的问题，我将不胜感激。 谢谢    由   提交/u/Invicto_50  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn5bss/the_car_still_isnt_able_to_turn_at_the_corner/</guid>
      <pubDate>Mon, 25 Mar 2024 04:43:31 GMT</pubDate>
    </item>
    <item>
      <title>如何在健身房环境中显示视频指标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bn4ft0/how_to_show_metrics_for_videos_in_gymnasium/</link>
      <description><![CDATA[嗨！我知道如何使用 RecordVideo 包装器录制视频。但该视频不包含任何任意指标，例如奖励/集数或任何自定义指标。有什么方法可以轻松做到这一点，还是我需要更深入地研究 moviepy？ 提前致谢。   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bn4ft0/how_to_show_metrics_for_videos_in_gymnasium/</guid>
      <pubDate>Mon, 25 Mar 2024 03:54:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么带有 Cartpole 健身房环境的 stable_baselines3 模型通过 sutton_barto_reward 提高了平均剧集奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmx1pf/why_is_my_stable_baselines3_model_with_cartpole/</link>
      <description><![CDATA[当我运行此代码时，我看到剧集长度平均值不断增加，而剧集平均奖励保持不变为 -1，这就是sutton_barto_reward系统工作正常。 从cartpole导入gymnasium 从stable_baselines3导入CartPoleEnv 从stable_baselines3.ppo.policies导入PPO 导入MlpPolicy env = CartPoleEnv(sutton_barto_reward=True) model = PPO(&quot;MlpPolicy&quot;, env, gamma=1, verbose=1) model.learn(total_timesteps=30000) &lt; /p&gt; 但是，我不明白为什么会这样，因为折扣率已设置为 1。剧集长度平均值是否应该没有任何改善，因为剧集的累积奖励始终是一样吗？ ​ ​ ​   由   提交/u/uglyboi34  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmx1pf/why_is_my_stable_baselines3_model_with_cartpole/</guid>
      <pubDate>Sun, 24 Mar 2024 22:10:57 GMT</pubDate>
    </item>
    <item>
      <title>[D] Aleksa Godric 关于在 DeepMind 找到工作的帖子在今天仍然具有现实意义吗？ [是的]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bml5tw/d_is_aleksa_godrics_post_on_landing_a_job_at/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bml5tw/d_is_aleksa_godrics_post_on_landing_a_job_at/</guid>
      <pubDate>Sun, 24 Mar 2024 13:47:38 GMT</pubDate>
    </item>
    <item>
      <title>我究竟做错了什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bmgifh/what_am_i_doing_wrong/</link>
      <description><![CDATA[我正在尝试训练 cartpole 代理。但他似乎并没有学到任何东西。我也尝试调试和更改超参数，但它仍然没有学到任何东西。 请帮助这里可能出了什么问题？ gridworld/dqn.py 位于 main · bherwanisuraj/gridworld (github.com)  P.S.感谢先生们抽出宝贵的时间来帮助我。  问题现在已经解决了。显然，目标模型从未得到更新，因为我是在每个 update_target % epoch 而不是 epoch % update_target 更新它。 我知道这是一个愚蠢的错误。我会不断学习，让自己变得更好。再次感谢大家。   由   提交/u/purna_lingham  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bmgifh/what_am_i_doing_wrong/</guid>
      <pubDate>Sun, 24 Mar 2024 09:10:22 GMT</pubDate>
    </item>
    <item>
      <title>PPO 和 DreamerV3 代理完成了《愤怒之铁拳》。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm9sjk/ppo_and_dreamerv3_agent_completes_streets_of_rage/</link>
      <description><![CDATA[不太确定我们是否可以自我推销，但我看到有人发布了他们的经纪人完成《街头霸王 3》的视频，所以我希望它被允许。&lt; /p&gt; 我一直在训练特工玩《怒之铁拳》的第一个阶段，现在终于可以完成游戏了，我的视频更多是为了娱乐，所以没有太多技术，但我将在下面解释一些内容。无论如何，这里是视频的链接： https://www.youtube.com/watch? v=gpRdGwSonoo ​ 这总共由 8 个模型完成，每个阶段 1 个模型。前 4 个模型是使用 SB3 训练的 PPO 模型，后 4 个模型是使用 SheepRL 训练的 DreamerV3 模型。两者都在相同的稳定复古健身房环境中使用我的奖励函数进行训练。 DreamerV3 在游戏的 64x64 像素 RGB 图像上进行训练，具有 4 个跳帧且无帧堆叠。 PPO 在游戏的 160x112 像素单色图像上进行训练，具有 4 个跳帧和 4 个帧堆叠。 每个连续阶段的模型都是在最后一个阶段的基础上构建的，除了切换到 DreamerV3 时，因为我已经再次从头开始，而且除了第8阶段游戏由向右移动改为向左移动外，我决定再次从头开始。 至于“娱乐”，我决定重新开始。在视频方面，Gym 环境基本上返回一些有关游戏状态的数据，然后我将其形成文本提示，并将其输入到开源 LLM 中，以便它可以对转换为 TTS 的游戏玩法做出一些简单的评论，同时让 Whisper 模型将我的 SpeechToText 转换，以便我也可以与角色交谈（当我说出角色的名字时触发）。这一切都连接到我制作的 UE5 应用程序，其中包含虚拟角色和环境。 我断断续续地训练了模型大约 5 或 6 个月的时间（不是直接的），所以我不真的知道我总共训练了他们多少小时。我认为第 8 阶段模型的训练时间大约为 15-30 小时。 DreamerV3 模型在 4 个平行健身房环境中进行训练，而 PPO 模型在 8 个平行健身房环境中进行训练。不管怎样，我希望它很有趣。   由   提交/u/disastorm  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm9sjk/ppo_and_dreamerv3_agent_completes_streets_of_rage/</guid>
      <pubDate>Sun, 24 Mar 2024 02:15:55 GMT</pubDate>
    </item>
    <item>
      <title>为运动队制定时间表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm8lc5/building_schedules_for_sports_teams/</link>
      <description><![CDATA[免责声明：我是这个实践和这个子项目的新手，我正在尽可能快地学习，但仍然感觉我几乎一无所知 我正在尝试建立一个模型来创建一个时间表，其中 N 个团队在 (N-1)*2 周内互相比赛（例如 4 支团队，6 周）。 我认为 RL提供了一条成功之路。 我的具体问题涉及特工将采取的行动和状态，以及如何推进每个情节。我如何构建它以使代理/策略采取行动？一次安排一个团队还是尝试两人一组进行？我是在整个过程中提供奖励还是仅在最后为“有效”的奖励提供奖励？时间表（例如，没有球队自己比赛，对手的分布尽可能平等）。 我有网络编程背景，但这里的数学/代数很难掌握。 感谢任何帮助。 TIA   由   提交/u/brianmorton  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm8lc5/building_schedules_for_sports_teams/</guid>
      <pubDate>Sun, 24 Mar 2024 01:15:58 GMT</pubDate>
    </item>
    <item>
      <title>我从哪说起呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bm20ga/where_do_i_start/</link>
      <description><![CDATA[我的任务需要输入 10 个数字集，每个数字的范围从 1 到 50，产生大约 28,291,700,360 种可能的组合。 &lt;我的目标是识别大约 5 组不同的输入数字。我需要使用能够猜测这些数字组的算法。 问题是目前需要 1 个 CPU 核心和大约 2 分钟来评估单个组合。我正在考虑租用 500 个 CPU 核心或过渡到 GPU。 我应该从哪里开始？我应该研究什么方法？   由   提交 /u/oTHeReX   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bm20ga/where_do_i_start/</guid>
      <pubDate>Sat, 23 Mar 2024 20:27:13 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>