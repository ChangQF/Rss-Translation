<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 06 Jan 2024 09:12:55 GMT</lastBuildDate>
    <item>
      <title>元强化学习任务的程序生成</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zs3l1/procedural_generation_of_metareinforcement/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2302.05583 OpenReview：https:// /openreview.net/forum?id=16fkkkCeOC 代码：https://github.com/ThomasMiconi/Meta-Task-Generator 摘要：  开放性能够生成无限多样、具有挑战性的环境，将受益匪浅。一种特别有趣的挑战类型是元学习（“学会学习”），这是智能行为的标志。然而，文献中元学习环境的数量是有限的。在这里，我们描述了具有任意刺激的简单元强化学习（meta-RL）任务的参数化空间。参数化使我们能够随机生成任意数量的新颖的简单元学习任务。参数化的表达能力足以包含许多众所周知的元强化学习任务，例如强盗问题、Harlow 任务、T 迷宫、Daw 两步任务等。简单的扩展使其能够捕获基于二维拓扑空间的任务，例如完整迷宫或查找域。我们描述了许多随机生成的具有不同复杂度的元强化学习域，并讨论了随机生成产生的潜在问题。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zs3l1/procedural_generation_of_metareinforcement/</guid>
      <pubDate>Sat, 06 Jan 2024 05:05:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 新手：在剧集终止后可以继续观察吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zotah/newbie_to_rl_is_it_okay_to_keep_an_observation/</link>
      <description><![CDATA[使用 pytorch、开放式 AI 健身房、pygame。 我正在尝试训练代理玩贪吃蛇游戏，灵感来自于YouTube 上 Sentdex 的教程系列。我试图让代理停止做的事情之一是反复吃掉自己并终止。所以我想添加一个计数器，这样如果一个情节由于与自身碰撞（蛇吃自己）而终止，那么它会给计数器添加+1。如果情节不是由于与自身发生碰撞而终止，则该计数器会重置。所以效果是，如果智能体由于碰撞 w 本身而终止，那么它会因与自身碰撞而收到 -100。如果它再次执行，那么对于相同的终止条件，它将是 -200。如果智能体因不同原因幸存并终止，则计数器重置，并且下次智能体与自身碰撞时，奖励再次为-100。此外，我还给代理一个“与自我碰撞”的结果。标记为观察，如果由于其他原因而终止，则为 0；如果由于与自身碰撞而终止，则为 1。 我的问题是，这是否允许？我使用的变量是在 env INIT 上初始化的，而不是在 env RESET 中初始化的。是否允许使用这样的变量作为观察？ 新来的，如果我的术语也混淆了，我很抱歉，我的理解是每一帧都是一个步骤，如果代理喜欢撞墙或吃东西本身或类似的东西，然后*情节*终止，并调用重置。所以我目前正在做的就是从技术上讲对各个情节进行观察，对吧？这是允许的吗？ 另外我也不知道这个问题可接受的风格是什么，lmk。   由   提交 /u/phantomBlurrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zotah/newbie_to_rl_is_it_okay_to_keep_an_observation/</guid>
      <pubDate>Sat, 06 Jan 2024 02:18:20 GMT</pubDate>
    </item>
    <item>
      <title>“动物的随机搜索可能有助于它们狩猎：觅食和掠食动物的神经系统可能会促使它们沿着一种称为 Lévy 行走的特殊随机路径移动，以便在没有线索的情况下有效地寻找食物”（Lévy 航班）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18znfh9/random_search_wired_into_animals_may_help_them/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18znfh9/random_search_wired_into_animals_may_help_them/</guid>
      <pubDate>Sat, 06 Jan 2024 01:14:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么在强化学习中需要包含随机元素 epsilon？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zmrfl/why_do_you_need_to_include_a_random_element/</link>
      <description><![CDATA[假设您正在尝试自动化吃豆人游戏。您拥有所有 pacman 状态，并获取每个可能操作的 q 值。为什么要有随机性的因素呢？随机性如何发挥作用来获取 q 值？   由   提交 /u/Throwawaybutlove   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zmrfl/why_do_you_need_to_include_a_random_element/</guid>
      <pubDate>Sat, 06 Jan 2024 00:43:32 GMT</pubDate>
    </item>
    <item>
      <title>支持 3D 和物理的机器人强化学习的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zgqid/best_library_for_reinforcement_learning_in/</link>
      <description><![CDATA[我目前正在使用 Unity ML 代理，它相当直观且运行良好。我确实发现它有限制，尤其是最近的 Unity 戏剧，我不确定它是否可以免费使用或长期支持。 我想切换到开源的东西，这会给我带来好处作为程序员，我有更多的控制权 我使用 OpenCV 制作了一个自定义的 2D 健身房，用于稳定基线，并且效果很好。  我需要将 3D 用于机器人技术，并最终与真实系统交互并使用传感器进行反馈。  我对 PyChrono 感到很兴奋，它似乎拥有所有正确的功能，但我就是无法让它工作。  查看教程，他们只有 1 个关于强化学习的教程。 https://api.projectchrono.org/tutorial_pychrono_demo_tensorflow.html 当尝试遵循它时，它要求安装tensorflow-gpu=1.14，它非常旧，并且不能与其他安装指令使用的Python=3.9正确安装 而且他们的主库大约4个月前停止获取更新，不确定是否停止开发 总体而言，PyChrono 对 ML 的支持很差，使用起来会很麻烦。 有哪些更好的替代方案可以继续获得支持？&lt; /p&gt; OpenAI Gym 是否配备 3D/物理/渲染引擎？这会得到多年的支持吗？ 谢谢 编辑。我找到了 PyBullet。似乎正是我正在寻找的。对此有何建议？   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zgqid/best_library_for_reinforcement_learning_in/</guid>
      <pubDate>Fri, 05 Jan 2024 20:30:35 GMT</pubDate>
    </item>
    <item>
      <title>[问题]强化学习算法资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zd6dc/question_resource_for_reinforcemnt_learning/</link>
      <description><![CDATA[是否有任何资源可以解释所有最新的重要深度强化学习算法？我看过博客和文章。我还找到了以下论文： 2209.14940.pdf (arxiv.org) 谢谢   由   提交 /u/Top_Badger9050   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zd6dc/question_resource_for_reinforcemnt_learning/</guid>
      <pubDate>Fri, 05 Jan 2024 18:03:07 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习进行非线性最优控制的最优差距。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18zbznr/optimality_gap_in_using_reinforcement_learning/</link>
      <description><![CDATA[您好，我一直在研究文献，包括许多 ML 会议，以寻找使用 RL 解决非线性最优控制问题的论文。我看到使用 Lyapunov 函数和其他安全强化学习应用程序对安全性有很多保证，但与经典方法相比，我找不到任何关于强化学习优化此类问题的能力的理论研究。就像最优差距一样。例如，对于具有非线性系统的 L2 目标 xTQx+uTRu，我希望找到一些论文，说明如果您使用此代理、网络结构，您将获得更好的性能等，但我没有找到任何将强化学习或神经网络的设计与控制器的最优性差距联系起来的研究   由   提交 /u/Specialist_Welder553   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18zbznr/optimality_gap_in_using_reinforcement_learning/</guid>
      <pubDate>Fri, 05 Jan 2024 17:14:05 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法的分类</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18z9kt7/classification_of_rl_algorithms/</link>
      <description><![CDATA[大家好， 我想对 RL 算法进行分类。据我了解，分类有两个维度。第一个维度基于代理在学习过程中如何收集和利用数据：在策略学习和离策略学习。第二个维度基于一般策略：基于价值的方法、基于策略的方法、基于行动者批评家的方法。 ​ 现在我想根据这两个维度对以下算法进行分类： - Sarsa：在策略学习，基于价值的方法 - REINFORCE：在策略学习，基于策略的方法 - A2C：On-policy 学习，基于 actor-critic 的方法 - PPO：On-policy 学习，基于 actor-critic 的方法和基于策略的方法 - Q-Learning：Off-policy 学习， value-based method - DQN：Off-policylearning，基于value的方法 - TD3：Off-policylearning，基于actor-critic的方法 - DDPG：Off-policylearning，基于演员-评论家的方法 ​ 你对我的分类有何看法？这是对的吗？有时算法可能分为两类，例如 PPO，它是一种基于 actor-critic 的方法，也是一种基于策略的方法。    由   提交 /u/PBerit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18z9kt7/classification_of_rl_algorithms/</guid>
      <pubDate>Fri, 05 Jan 2024 15:32:51 GMT</pubDate>
    </item>
    <item>
      <title>我应该期望代理学习多快？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18z0hlj/how_fast_should_i_expect_the_agent_to_learn/</link>
      <description><![CDATA[我是 RL 新手，刚开始玩扫雷游戏。一开始，我希望模型学习的唯一一件事就是避免点击已经打开的方块。 我知道这可以通过动作屏蔽来完成，但我很好奇，想看看学习这个简单的行为需要多长时间。 点击打开的方块的奖励是 -10000，而做其他任何事情的奖励是 10。令我惊讶的是，训练持续了很长时间几个小时了，特工仍然没有学会避开已经打开的方块。目前，我看到大约 10% 的移动是点击一个打开的方块。 我只是想知道这是否太长。 以下是有关我的设置的更多信息：  p&gt;  我用的是gymnasium和stable-baselines3，型号是PPO 扫雷游戏是9*9的，有10个地雷。每个打开的方块都用 0-8 之间的数字表示，表示地雷的数量。未打开的方块用 -1 表示。 我使用配备 RTX 3060（120 瓦）的笔记本电脑进行训练。   &amp; #32；由   提交 /u/yzhjonathan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18z0hlj/how_fast_should_i_expect_the_agent_to_learn/</guid>
      <pubDate>Fri, 05 Jan 2024 06:48:09 GMT</pubDate>
    </item>
    <item>
      <title>使用世界模型进行基于梯度的规划</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18yy57s/gradientbased_planning_with_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.17227 摘要：  人工智能领域持久的挑战是控制系统以实现所需的行为。而对于由简单动力学方程控制的系统，线性二次调节（LQR）等方法在历史上已被证明是非常有效的，大多数现实世界的任务都需要通用的问题求解器，需要具有无法通过简单方程轻松描述的动力学的世界模型。因此，这些模型必须使用神经网络从数据中学习。大多数为视觉世界模型设计的模型预测控制（MPC）算法传统上都探索基于无梯度群体的优化方法，例如用于规划的交叉熵和模型预测路径积分（MPPI）。然而，我们提出了一种基于梯度的替代方案的探索，该替代方案充分利用了世界模型的可微性。在我们的研究中，我们对我们的方法和其他基于 MPC 的替代方案以及基于策略的算法进行了比较分析。在样本有效的设置中，与大多数任务中的替代方法相比，我们的方法取得了同等或更好的性能。此外，我们引入了一种结合了策略网络和基于梯度的 MPC 的混合模型，该模型的性能优于纯基于策略的方法，从而有望在复杂的现实任务中使用世界模型进行基于梯度的规划。  &lt; /div&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18yy57s/gradientbased_planning_with_world_models/</guid>
      <pubDate>Fri, 05 Jan 2024 04:38:57 GMT</pubDate>
    </item>
    <item>
      <title>优先重播缓冲区 - 真的有用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/</link>
      <description><![CDATA[您好，我有一个问题要问你们所有有实施和评估优先重播缓冲区经验的人。 我自己做了一个问题优先重放缓冲区的实现，并将其与双 dqn 实现进行比较。比较是在 pythongym 库的 Lunar Lander 环境中进行的。对于优先重播缓冲区的 alpha 和 beta 值，我使用 0.7（固定）和 0.4 作为初始 beta 值，该值在整个数字中线性变化到 1.0集数（即 4000）。 在我的比较中，double dqn 在大约 2700 集时完成训练（当它在最后 100 集中达到平均 230 累积奖励时），而使用优先级的训练重播缓冲区在大约 2800 到 2900 集时完成训练（同样，当它在最后 100 集达到平均 230 累积奖励时）。我尝试从 0.3、0.4 和其他值开始将 alpha 值线性移动到 1.0 ，但每次，它的性能最多与双 dqn 一样好。我期望在使用优先重放缓冲区进行训练时能够更快地达到接受标准（230 奖励的平均值），因为它应该提供对代理来说更有意义的体验样本（与以随机统一方式采样的正常重放缓冲区相反）。 因此，根据您的经验，您是否发现使用优先重放缓冲区与使用优先重放缓冲区相比有好处？正常重播缓冲区（并使用双 dqn）？您是否认为优先重播缓冲区在月球着陆器等环境中没有明显好的结果？ 任何提示、建议、意见（基于您的经验） ），想法或分享的经验非常有价值，欢迎并感谢。 干杯！   由   提交/u/kxy-yumkimil  /u/kxy-yumkimil reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/</guid>
      <pubDate>Thu, 04 Jan 2024 21:49:39 GMT</pubDate>
    </item>
    <item>
      <title>有关 PPO 奖励的快速信息</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ydc5s/quick_info_on_ppo_reward/</link>
      <description><![CDATA[我的奖励函数有 0.01 和 1 这样的常量 如果我将其设置为 0.1 和 10，会有什么不同吗？  我问这个问题是因为当我通过正缩放器（添加常数项）改变奖励函数时，它产生了影响。  &amp;# 32；由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ydc5s/quick_info_on_ppo_reward/</guid>
      <pubDate>Thu, 04 Jan 2024 13:36:07 GMT</pubDate>
    </item>
    <item>
      <title>“桥接离散和反向传播：直通和超越”，Liu 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18y0fee/bridging_discrete_and_backpropagation/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18y0fee/bridging_discrete_and_backpropagation/</guid>
      <pubDate>Thu, 04 Jan 2024 01:28:10 GMT</pubDate>
    </item>
    <item>
      <title>在 7 多万英里的纯骑手驾驶中，Waymo 的表现显着优于可比较的人类基准（Kusano 等人，2023 年）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xzuvh/waymo_significantly_outperforms_comparable_human/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xzuvh/waymo_significantly_outperforms_comparable_human/</guid>
      <pubDate>Thu, 04 Jan 2024 01:02:58 GMT</pubDate>
    </item>
    <item>
      <title>持续强化学习的定义</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2307.11046 OpenReview：https:// /openreview.net/forum?id=ZZS9WEWYbD 摘要：  在强化学习问题的标准视图中，代理的目标是有效地确定最大化长期回报的策略。然而，这种观点是基于一种有限的观点，即学习是寻找解决方案，而不是将学习视为无休止的适应。相反，持续强化学习是指最好的智能体永远不会停止学习的环境。尽管持续强化学习很重要，但社区缺乏一个简单的问题定义来强调其承诺并使其主要概念准确清晰。为此，本文致力于仔细定义持续强化学习问题。我们将“永不停止学习”的代理概念正式化。通过一种新的数学语言来分析和编目代理。使用这种新语言，我们将持续学习智能体定义为可以无限期地执行隐式搜索过程的智能体，并将持续强化学习定义为最好的智能体都是持续学习智能体的设置。我们提供了两个激励性的例子，说明多任务强化学习和持续监督学习的传统观点是我们定义的特例。总的来说，这些定义和观点形式化了学习核心的许多直观概念，并开辟了围绕持续学习代理的新研究途径。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</guid>
      <pubDate>Wed, 03 Jan 2024 21:09:29 GMT</pubDate>
    </item>
    </channel>
</rss>