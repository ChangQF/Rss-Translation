<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 05 Nov 2024 12:31:24 GMT</lastBuildDate>
    <item>
      <title>评价损失散度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk5h5q/critic_loss_divergence/</link>
      <description><![CDATA[      大家好， 我正在实施一个多头 PPO，其中每个头负责不同的（但相关的）任务。但是，我注意到每个头的批评者损失都在显著增加——有时从大约 10 增加到 1200 或更多。以下是输出的快照，供您参考。 我尝试过分别更新每个评论家以及同时更新所有评论家，并使用值剪辑。此外，在参与者网络中，我使用共享层（L1、L2），然后为每个头部使用不同的输出分支。然而，对于批评家来说，每个头部都有自己独立的 L1 和 L2 层。 这些架构选择是否会导致批评家损失不断增加，或者可能还有其他因素在起作用？ https://preview.redd.it/ecnmsv4lr2zd1.png?width=823&amp;format=png&amp;auto=webp&amp;s=f6bed4bb786ccab34c7a73848fe5a38ef5ee970a # Set1 value clipping value_set1_clipped = old_values_set1 + torch.clamp(value_set1 - old_values_set1, -self.value_clip_range, self.value_clip_range) value_set1_loss1 = F.mse_loss(value_set1, returns_set1) value_set1_loss2 = F.mse_loss(value_set1_clipped, returns_set1) critical_loss_set1 = torch.max(value_set1_loss1, value_set1_loss2) # Set2 值剪辑 value_set2_clipped = old_values_set2 + torch.clamp(value_set2 - old_values_set2, -self.value_clip_range, self.value_clip_range) value_set2_loss1 = F.mse_loss(value_set2, returns_set2) value_set2_loss2 = F.mse_loss(value_set2_clipped, returns_set2)评论家损失_set2 = torch.max(value_set2_loss1, value_set2_loss2) ####################################输出####################################### 演员损失：0.5793，熵：2.5832，评论家头部1损失：461.3597，评论家头部2损失：1024.5741，评论家头部3损失：21.0361 演员损失：0.5793，熵：2.5832，评论家头部1损失：461.3597，评论家头部2损失：1024.5741，评论家头部3损失：21.0361 演员损失：0.6495，熵：2.5602，评论家头部1损失： 266.5478，评论家头2损失：426.3173，评论家头3损失：16.1255演员损失：0.7650，熵：2.6232，评论家头1损失：427.5551，评论家头2损失：775.9523，评论家头3损失：44.9366演员损失：0.6635，熵：2.5855，评论家头1损失：501.3060，评论家头2损失：887.4315，评论家头3损失：30.6863演员损失：0.9118，熵：2.6160，评论家头1损失：432.1326，评论家头2损失：705.5318，评论家头部3损失：55.9993 演员损失：0.7652，熵：2.6095，评论家头部1损失：468.3109，评论家头部2损失：466.6273，评论家头部3损失：83.0151 演员损失：0.6764，熵：2.6375，评论家头部1损失：476.9982，评论家头部2损失：741.9779，评论家头部3损失：54.6600 演员损失：0.5160，熵：2.6646，评论家头部1损失：468.3273，评论家头部2损失：1085.3656，评论家头部 3 损失：19.7672 演员损失：0.6571，熵：2.5796，评论家头部 1 损失：455.7019，评论家头部 2 损失：688.5980，评论家头部 3 损失：66.3462 演员损失：0.7888，熵：2.5792，评论家头部 1 损失：437.5110，评论家头部 2 损失：601.6379，评论家头部 3 损失：71.4872     提交人    /u/GuavaAgreeable208   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk5h5q/critic_loss_divergence/</guid>
      <pubDate>Tue, 05 Nov 2024 12:26:40 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk54sa/deep_reinforcement_learning_generalization/</link>
      <description><![CDATA[理解和诊断深度强化学习。发表于国际机器学习会议 ICML 2024。 链接：https://proceedings.mlr.press/v235/korkmaz24a.html    提交人    /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk54sa/deep_reinforcement_learning_generalization/</guid>
      <pubDate>Tue, 05 Nov 2024 12:06:19 GMT</pubDate>
    </item>
    <item>
      <title>动态状态表示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk4q2r/dynamic_state_representation/</link>
      <description><![CDATA[大家好！ 我想问一下，是否有人听说过在代理的某个情节中状态空间可以发生变化的场景。 例如，假设我是一个在空房间里徘徊的代理，我的状态空间表示是我的 (x,y) 坐标。突然，我意识到我应该拿起一个位于我旁边房间里的物体。 然后我的状态空间可能会变为 (x,y,current_room,is_holding_anything)。 有谁知道以前的任何工作中存在这种情况？无论是规划还是 RL 领域。 提前谢谢！！    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk4q2r/dynamic_state_representation/</guid>
      <pubDate>Tue, 05 Nov 2024 11:41:12 GMT</pubDate>
    </item>
    <item>
      <title>我第一次使用强化学习来解决自己的问题！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjsr6q/my_first_use_of_reinforcement_learning_to_solve/</link>
      <description><![CDATA[        提交人    /u/JealousCookie1664   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjsr6q/my_first_use_of_reinforcement_learning_to_solve/</guid>
      <pubDate>Mon, 04 Nov 2024 23:35:27 GMT</pubDate>
    </item>
    <item>
      <title>ROS2 + Gazebo 环境上的 DRL SPS</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjh494/drl_sps_on_ros2_gazebo_envs/</link>
      <description><![CDATA[嗨，我想问一下这里有没有使用 ROS2 + Gazebo RL 环境经验的研究人员/机器人专家跟踪过他们的 SPS（每秒步数）以及该值是多少，无论是否进行优化。    提交人    /u/Necessary_Gear_1911   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjh494/drl_sps_on_ros2_gazebo_envs/</guid>
      <pubDate>Mon, 04 Nov 2024 15:35:58 GMT</pubDate>
    </item>
    <item>
      <title>帮助对 RL 算法进行分类</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjcj53/help_with_classification_of_rl_algorithms/</link>
      <description><![CDATA[大家好， 我正在做我的期末学位项目，关于强化学习在投资组合优化中的应用，正如标题所说，在强化学习算法的分类方面，我需要一点帮助。现在，我已经解释了如何根据学习方法（基于模型 vs 无模型）、学习目标（基于价值 vs 基于策略 vs 参与者评论家）以及学习策略的方式（在策略 vs 离策略）对它们进行区分。 现在，问题出现在尝试根据这些方法制作分类图时，因为我遇到过可能将相同算法放在不同类别中的来源。我的目标是制作如下图所示的图表（来源为 chatgpt），深入了解无模型算法： 强化学习算法 ═ ... │ │ │ ├── Soft Actor-Critic (SAC) │ │ │ │ ├── TRPO │ │ │ ├── PPO │ ├── Off-Policy │ │ ├── 基于价值的  │ │ │ ├── Q-Learning  │ │ ├── 深度 Q 网络 (DQN) │ │ ├── 基于策略的 │ │ │ ├── 确定性策略梯度 (DPG)  │ │ ├── 双延迟 DDPG (TD3)  │ │ ├── 软 Actor-Critic (SAC) ├ ── 基于模型 │ ├──基于价值 │ ═ ...  /u/Street-Vegetable-117   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjcj53/help_with_classification_of_rl_algorithms/</guid>
      <pubDate>Mon, 04 Nov 2024 12:00:09 GMT</pubDate>
    </item>
    <item>
      <title>从事多智能体学习研究的教授名单（不是我制作的）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjc6i1/list_of_professors_working_in_multiagent_learning/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjc6i1/list_of_professors_working_in_multiagent_learning/</guid>
      <pubDate>Mon, 04 Nov 2024 11:38:33 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity AI PRO - 1 年计划优惠 - 折扣</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjb3jn/perplexity_ai_pro_1_year_plan_offer_discounted/</link>
      <description><![CDATA[      如标题所示：我们提供Perplexity AI PRO 一年计划的优惠券代码。  订购：https://cheapgpts.store  接受的付款：  PayPal。 （100% 买家保护） Revolut。     提交人    /u/A2uniquenickname   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjb3jn/perplexity_ai_pro_1_year_plan_offer_discounted/</guid>
      <pubDate>Mon, 04 Nov 2024 10:24:44 GMT</pubDate>
    </item>
    <item>
      <title>在“普通”PyTorch 中使用 SB3 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gj9f5y/using_sb3_model_in_plain_pytorch/</link>
      <description><![CDATA[我需要在无法安装 SB3 的设备上采用 SB3 模型 (PPO、DQN)。 我只需要模型进行预测（因此学习发生在装有 SB3 的其他机器上）。 我在 SO 中遇到了这个问题（不幸的是，我再也找不到链接了）。我当然可以用这种方法预测动作。 但是，我不确定这是否真的是可行的方法 - 这样的转换有什么缺点吗？或者有更简单的解决方案吗？  class Wrapper(nn.Module): def __init__(self, sb3_model, device=&#39;cuda&#39;): super(Wrapper, self).__init__() self.device = device self.extractor = sb3_model.policy.mlp_extractor self.policy_net = sb3_model.policy.mlp_extractor.policy_net self.action_net = sb3_model.policy.action_net self.extractor.to(self.device) self.policy_net.to(self.device) self.action_net.to(self.device) def forward(self, x): x = x.to(self.device) x = self.policy_net(x) x = self.action_net(x) return x # 用法：model = PPO.load(...) wrapper = Wrapper(model)     由    /u/luigi1603  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gj9f5y/using_sb3_model_in_plain_pytorch/</guid>
      <pubDate>Mon, 04 Nov 2024 08:13:35 GMT</pubDate>
    </item>
    <item>
      <title>“机器人操作模仿学习中的数据缩放规律”，Lin 等人 2024 年（多样性 > n）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gj2qc3/data_scaling_laws_in_imitation_learning_for/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gj2qc3/data_scaling_laws_in_imitation_learning_for/</guid>
      <pubDate>Mon, 04 Nov 2024 01:18:42 GMT</pubDate>
    </item>
    <item>
      <title>什么样的状态对于 LSTM 层来说是有用的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gisqx6/what_kind_of_state_is_useful_for_lstm_layers/</link>
      <description><![CDATA[我正在使用 Unity 的 mlagents 解决具有离散 SAC 的网格迷宫环境。没有内存，它也能很好地解决。但是，如果我启用内存，性能会下降到最低水平。我怀疑我当前的环境表示不适合 LSTM 层 最初的状态是（对于 4 个方向中的每一个）：对象的类型（墙壁、出口、无）、房间被访问的次数（只有 0 次是墙壁）。然后我尝试将每个房间和代理本身的位置添加到状态中，但这使情况变得更糟。到目前为止，只保留对象的类型是最好的选择，性能下降速度较慢，但​​代理仍然没有学习    提交人    /u/Aydiagam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gisqx6/what_kind_of_state_is_useful_for_lstm_layers/</guid>
      <pubDate>Sun, 03 Nov 2024 17:52:33 GMT</pubDate>
    </item>
    <item>
      <title>寻找应用强化学习和机器人技术的研究实习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gipwq6/looking_for_research_internship_in_applied_rl/</link>
      <description><![CDATA[我是 Mila 的博士生，致力于不同机器人应用的强化学习（研究过挖掘机自动化、基于物理的角色动画和自动驾驶等应用）。我目前正在寻找 2025 年的暑期研究实习，我对任何专注于应用强化学习或具身人工智能的职位都非常感兴趣。 以下是我迄今为止的研究历程：  自动奖励建模：开发了从 Vortex Simulator 中挖掘机自动化的专家演示中得出奖励函数的方法。（在 NeurIPS RL for Real-life Applications 研讨会上发表。） 样本高效强化学习：通过基于 Transformer 的离散世界建模提高了 Atari 基准的样本效率。 （ICML 2024） 多任务 RL 的组合运动先验：我目前正在使用 Isaac Gym 研究具有组合运动先验的机器人运动多任务学习。 自动驾驶 RL：设计了一种在 CARLA 模拟器上进行自动驾驶的课程学习方法，无需复杂的奖励塑造。 （Inria 研究生）。  我还在探索将扩散模型与 RL 结合使用，以实现稳定、多样化的控制策略。 如果有人知道相关的职位空缺，或者对可能重视应用 RL 研究的地方有任何建议，我将不胜感激。 非常感谢您的任何线索或建议！ 我的简历和更多详细信息请访问我的 https://pranaval.github.io/。    提交人    /u/Personal_Click_6502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gipwq6/looking_for_research_internship_in_applied_rl/</guid>
      <pubDate>Sun, 03 Nov 2024 15:50:41 GMT</pubDate>
    </item>
    <item>
      <title>模仿学习的最新进展是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gilwqy/what_is_stateoftheart_in_imitation_learning/</link>
      <description><![CDATA[  由    /u/Better_Working5900  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gilwqy/what_is_stateoftheart_in_imitation_learning/</guid>
      <pubDate>Sun, 03 Nov 2024 12:38:40 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为策略游戏 Polytopia 设计 RL 算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gicg22/need_help_engineering_rl_algorithm_for_strategy/</link>
      <description><![CDATA[      大家好，我是 RL 新手，需要一些帮助来为策略游戏 Polytopia 设计算法。 我正在尝试为基于图块的策略游戏 Polytopia 制作 RL 代理。使用 OpenAI Gym，我制作了该游戏的原始版本。观察空间由 121 个图块组成，每个图块都有数据：（地形、资源、改进、气候、边界、改进所有者、单位所有者、单位类型、单位健康、改进进度、已攻击、已移动）以及玩家的星数。下面是游戏的示例（这是全局视图，所以没有雾，但各个代理看不到雾外面） https://preview.redd.it/aqpsk4c0elyd1.png?width=706&amp;format=png&amp;auto=webp&amp;s=9a2973083cc8512467aa2c6dcfbda9181946cb97 目前，我已将行动过程分为三个步骤。首先，代理从 1 到 121 中挑选一个方块（121 个动作）。其次，代理挑选要在该方块上执行的动作类型（8 种动作），例如：移动/攻击、收获资源、训练单位等。第三步仅当动作涉及目标方块时才会发生，例如：移动/攻击，代理从 1 到 121 中挑选一个代表目标方块的方块。示例动作序列为：59、1、49；这将选择方块 59，选择移动/攻击单位动作类型，并选择目标方块 49，这将导致骑手攻击战士。这是我的图表的链接：https://docs.google.com/presentation/d/1DPhYymGDfQIfVKAYlzK8lBkkiPoGlqbxRJ5JycDQI_U/edit?usp=sharing 我应该使用什么算法？处理这种多阶段操作的最佳方法是什么？我应该输入哪些参数？我的神经网络应该是模块化的还是分层的？PyTorch 是这类事情的好选择吗？任何关于如何开始学习过程的建议或链接都​​将不胜感激！    提交人    /u/Kingofath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gicg22/need_help_engineering_rl_algorithm_for_strategy/</guid>
      <pubDate>Sun, 03 Nov 2024 01:59:38 GMT</pubDate>
    </item>
    <item>
      <title>对强化学习感兴趣可以申请哪些行业、什么职位？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gi12cc/which_industries_and_what_positions_can_we_apply/</link>
      <description><![CDATA[您好，我是密歇根大学马里兰分校的研究生，刚刚进入强化学习领域，到目前为止，我很喜欢它，这让我很好奇哪些行业可以申请实习，以及如何在此领域发展我的职业生涯。但就目前而言，我对强化学习的实际应用比研究更感兴趣。 我希望在这方面得到一些指导。 谢谢    提交人    /u/Odd-Pangolin4370   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gi12cc/which_industries_and_what_positions_can_we_apply/</guid>
      <pubDate>Sat, 02 Nov 2024 17:03:00 GMT</pubDate>
    </item>
    </channel>
</rss>