<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 16 Oct 2024 03:25:41 GMT</lastBuildDate>
    <item>
      <title>我使用深度强化学习（使用 Unity ML Agents）制作了一个消防员 AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4iy4q/i_made_a_firefighter_ai_using_deep_rl_using_unity/</link>
      <description><![CDATA[视频链接：https://www.youtube.com/watch?v=REYx9UznOG4 我之前做过这个视频，花了好几个小时才制作出来，却没有人关注，这让我很沮丧，所以我现在在攻读人工智能博士学位，而不是成为一名 YouTuber，哈哈。 我想如果人们觉得它很有趣，现在为它做广告也不错。我确保添加了一些旁白和有趣的部分，这样它就不会无聊了。我希望这里的一些人能觉得它和我做这个项目一样有趣。 我对这个主题很感兴趣，所以如果有人有问题，我会在有时间的时候回答他们:D   由    /u/usernumero  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4iy4q/i_made_a_firefighter_ai_using_deep_rl_using_unity/</guid>
      <pubDate>Tue, 15 Oct 2024 21:28:04 GMT</pubDate>
    </item>
    <item>
      <title>NoisyLinears 之后的 LayerNor/Adanorm？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4gbc7/layernoradanorm_after_noisylinears/</link>
      <description><![CDATA[除了最后一个噪声输出层之外，对 NN 中的所有噪声层应用层范数或 adanorm 有什么想法/经验吗？ 任何一个范数层基本上都会扼杀噪声线性/探索吗？    提交人    /u/dekiwho   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4gbc7/layernoradanorm_after_noisylinears/</guid>
      <pubDate>Tue, 15 Oct 2024 19:36:13 GMT</pubDate>
    </item>
    <item>
      <title>“解读 DPO 和 PPO：从偏好反馈中解开学习的最佳实践”，Ivison 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4cnnx/unpacking_dpo_and_ppo_disentangling_best/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4cnnx/unpacking_dpo_and_ppo_disentangling_best/</guid>
      <pubDate>Tue, 15 Oct 2024 17:02:15 GMT</pubDate>
    </item>
    <item>
      <title>Simba：深度强化学习中扩大参数的简单性偏差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g460jl/simba_simplicity_bias_for_scaling_up_parameters/</link>
      <description><![CDATA[      想要更快、更智能的强化学习？查看 SimBa – 我们可疯狂扩展的新架构！ 📄 项目页面：https://sonyresearch.github.io/simba 📄 arXiv：https://arxiv.org/abs/2410.09754 🔗 代码：https://github.com/SonyResearch/simba 🚀 厌倦了深度 RL 中缓慢的训练时间和不尽人意的结果？ 使用 SimBa，您可以毫不费力地扩展参数并达到最先进的性能 - 而无需更改核心 RL 算法。 💡 它是如何工作的？ 只需将您的 MLP 网络换成 SimBa，然后观看奇迹发生！在单个 Nvidia RTX 3090 上，只需 1-3 小时，您就可以训练出在 DMC、MyoSuite 和 HumanoidBench 等基准测试中表现最佳的代理。 🦾 ⚙️ 为什么它很棒： 即插即用，支持 SAC、DDPG、TD-MPC2、PPO 和 METRA 等 RL 算法。 无需调整您最喜欢的算法 - 只需切换到 SimBa 并让扩展能力接管即可。 训练更快、更智能、更好 - 非常适合研究人员、开发人员和任何探索深度 RL 的人！ 🎯 立即尝试并观察您的 RL 模型演变！ https://i.redd.it/olxmmgyauwud1.gif    提交人    /u/joonleesky   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g460jl/simba_simplicity_bias_for_scaling_up_parameters/</guid>
      <pubDate>Tue, 15 Oct 2024 12:03:38 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型玩星际争霸 II：基准测试和总结链方法”，Ma 等人 2023 年（让 LLM 玩星际争霸的文本）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3sqr7/large_language_models_play_starcraft_ii/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3sqr7/large_language_models_play_starcraft_ii/</guid>
      <pubDate>Mon, 14 Oct 2024 22:32:24 GMT</pubDate>
    </item>
    <item>
      <title>“具身代理界面：具身决策的 LLM 基准测试”，Li 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3p9cr/embodied_agent_interface_benchmarking_llms_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3p9cr/embodied_agent_interface_benchmarking_llms_for/</guid>
      <pubDate>Mon, 14 Oct 2024 20:02:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么在这个SAC实现中action_dim乘以2？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3itn4/why_is_action_dim_multiplied_by_2_in_this_sac/</link>
      <description><![CDATA[嗨，我正在尝试重新实现 SAC，我注意到 Actor 的动作维度乘以了 2。我引用的项目是：https://github.com/philipjball/SAC_PyTorch/blob/master/sac.py#L37C9-L37C74 有人知道为什么需要乘以 2 吗？当我在项目中删除它时，操作减半 - 所以我明白这是必要的，但我不明白为什么。 self.network = MLPNetwork(state_dim, action_dim * 2, hidden_​​size)     提交人    /u/stokaty   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3itn4/why_is_action_dim_multiplied_by_2_in_this_sac/</guid>
      <pubDate>Mon, 14 Oct 2024 15:40:55 GMT</pubDate>
    </item>
    <item>
      <title>如何训练代理进行任意长度的二进制加法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3hdrt/how_to_train_an_agent_to_do_binary_addition_of/</link>
      <description><![CDATA[大家好。 这个问题突然出现在我的脑海里，我知道它可能有点琐碎，但我很想知道答案。    提交人    /u/blablawawawa   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3hdrt/how_to_train_an_agent_to_do_binary_addition_of/</guid>
      <pubDate>Mon, 14 Oct 2024 14:40:44 GMT</pubDate>
    </item>
    <item>
      <title>TorchRL 中针对 MARL 的动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3cjnw/action_masking_in_torchrl_for_marl/</link>
      <description><![CDATA[您好！我目前正在使用 TorchRL 解决我的 MARL 问题。我使用的是自定义 pettingzoo 环境和 pettingzoo 包装器。我的自定义环境的观察结果中包含一个动作掩码。在 TorchRL 中处理它的最简单方法是什么？因为我觉得 MultiAgentMLP 和 ProbabilisticActor 不能与动作掩码一起使用，对吗？ 谢谢！    提交人    /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3cjnw/action_masking_in_torchrl_for_marl/</guid>
      <pubDate>Mon, 14 Oct 2024 10:22:58 GMT</pubDate>
    </item>
    <item>
      <title>适合我的强化学习项目的 ubuntu/ROS2/Gazebo 版本</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3blyl/suitable_ubunturos2gazebo_versions_for_my/</link>
      <description><![CDATA[大家好，我将在 Gazebo 模拟器中对 epuck 模型机器人进行强化学习（我有一个来自 Gazebo Classic 的 urdf 模型，我必须适应新版本），我对 ros2 和 Gazebo 有基本的先验知识，但我想知道适合我的项目的版本，它是关于使用 RL 技术进行自主导航，我将非常感谢您的帮助。    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3blyl/suitable_ubunturos2gazebo_versions_for_my/</guid>
      <pubDate>Mon, 14 Oct 2024 09:12:31 GMT</pubDate>
    </item>
    <item>
      <title>RL 代理无法学习简单的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g3aw2u/rl_agent_not_able_to_learn_for_a_simple_problem/</link>
      <description><![CDATA[大家好。 我对 RL 还很陌生，想为我创建的一款非常简单的游戏实现深度 Q 学习 RL 算法： 代理从 1 到 10 之间的某个随机整数坐标开始。目标是到达位置 5。代理可以在两个操作之间进行选择：向左移动 0.5 或向右移动 0.5。当代理到达位置 5 时，我会给他们 1 的奖励。如果游戏结束时他们没有到达 5，他们会得到 -1。在所有其他情况下，他们会得到 0。 我有一个简单的 DNN Q 函数逼近器。它有一个带有一个特征（当前位置）的输入层，后面紧接着是带有两个值的输出层，这两个值对应于每个动作的预期值。只有一层，所以它实际上是一个线性函数逼近器。除非我错过了什么，否则这应该足以解决这个问题（因为它实际上只是在学习当前位置是否 &gt; 5）。 问题是模型一直在从总是选择向左转到总是选择向右。它似乎并没有意识到低于 5 的位置应该与高于 5 的位置区别对待。此外，每个状态的动作（预测）的预期值彼此非常接近。它正在学习一些东西，但无法区分类别。 你认为游戏和我设置奖励 fn 或 DNN 的方式存在问题吗，或者我的代码可能有问题？    提交人    /u/IDidItMyWay_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g3aw2u/rl_agent_not_able_to_learn_for_a_simple_problem/</guid>
      <pubDate>Mon, 14 Oct 2024 08:12:46 GMT</pubDate>
    </item>
    <item>
      <title>关于演员-评论家和在线学习中的策略梯度的困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g378u7/confusion_about_policy_gradient_in_actorcritic/</link>
      <description><![CDATA[      你好！我试图了解策略梯度是如何得出的，并在Actor-Critic 方法中使用，以及它如何与在线学习保持一致。 下图显示了 REINFORCE 和 Actor-Critic 的不同策略梯度公式： https://preview.redd.it/9i2nkp936nud1.png?width=1432&amp;format=png&amp;auto=webp&amp;s=8759231eeacc0d9a655807f87890345b800c02f4  在图片中的 Actor-Critic 方程中，期望中的和在最后一行消失了。然而，在其他材料中，我看到和仍然在期望中。如果我们从第一行推导出梯度（就像在 REINFORCE 中一样），那么总和似乎应该留在里面。我说得对吗？ 如果总和应该保留，那么 Actor-Critic 方法如何在在线学习中处理这个问题？我认为 Actor-Critic 方法可以在每一步（在线）更新策略，但我不确定在这些在线更新过程中如何处理所有步骤的总和。  如能对此作出任何澄清，我们将不胜感激！提前谢谢您。    提交人    /u/DRLC_   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g378u7/confusion_about_policy_gradient_in_actorcritic/</guid>
      <pubDate>Mon, 14 Oct 2024 03:35:36 GMT</pubDate>
    </item>
    <item>
      <title>不同的 RL 算法真的有很大影响吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</link>
      <description><![CDATA[我现在正在进行 RL 项目来解决组合优化问题，由于复杂的约束，这个问题很难用数学来表达。我正在使用 A2C 训练我的代理，这是最简单的入门方法。 我只是想知道其他算法（如 TRPO、PPO）在实践中是否真的效果更好，而不是像在基准测试中那样。 有没有人尝试过 SOTA 算法（论文中声称）并真的看到了差异？ 我觉得设计奖励比算法本身重要得多。    提交人    /u/Electronic_Estate854   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</guid>
      <pubDate>Mon, 14 Oct 2024 01:53:08 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND：世界建模的扩散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</link>
      <description><![CDATA[DIAMOND 💎 世界建模的扩散：Atari 中的视觉细节很重要 项目网页：https://diamond-wm.github.io/ 代码、代理和可玩世界模型：https://github.com/eloialonso/diamond 论文：https://arxiv.org/pdf/2405.12399 摘要  RL 代理是由 REINFORCE 训练的演员-评论家。  演员和评论家网络除最后一层外共享权重。这些共享层由一个卷积“主干”和一个 LSTM 单元组成。卷积主干有四个带有 2x2 最大池化的残差块。 每次训练运行都需要 500 万帧，在一台 Nvidia RTX 4090 上持续 12 天。  世界模型是一个带有 U-Net 2D 的 2D 扩散模型。它不是潜在扩散模型。它直接从视频游戏中生成帧。 该模型将最后 4 帧和动作以及扩散噪声水平作为条件。 在 RTX 3090 上以 ~10 FPS 运行。 他们使用 EDM 采样器从扩散模型中采样，即使每帧只有 1 个扩散步骤，它仍然可以很好地训练 RL 代理。     由    /u/furrypony2718  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</guid>
      <pubDate>Mon, 14 Oct 2024 01:13:23 GMT</pubDate>
    </item>
    <item>
      <title>资源推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g30ici/resource_recommendation/</link>
      <description><![CDATA[嗨！我对 RL 还很陌生，对于我的课程项目，我希望在多智能体系统中做一些事情来监视和跟踪目标。假设已知环境，我想最大化群体覆盖的区域。 我真的想为此做一个好的可视化。我希望在任何类型的模拟器上运行它。 有人可以推荐任何类似的项目/资源来参考吗？    提交人    /u/whatsinthaname   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g30ici/resource_recommendation/</guid>
      <pubDate>Sun, 13 Oct 2024 21:37:05 GMT</pubDate>
    </item>
    </channel>
</rss>