<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 19 Oct 2024 01:14:33 GMT</lastBuildDate>
    <item>
      <title>有人可以帮忙吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6pi8d/can_anyone_help/</link>
      <description><![CDATA[https://www.reddit.com/r/MachineLearning/s/8Vp8b3bGqI    由   提交  /u/Alarming-Power-813   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6pi8d/can_anyone_help/</guid>
      <pubDate>Fri, 18 Oct 2024 18:44:29 GMT</pubDate>
    </item>
    <item>
      <title>演员 评论家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6mlv2/actor_critic/</link>
      <description><![CDATA[https://arxiv.org/abs/1704.03732 是否有任何演员-评论家类似物可以将专家演示整合到演员-评论家学习中，就像 DQN 一样？    提交人    /u/Key-Faithlessness113   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6mlv2/actor_critic/</guid>
      <pubDate>Fri, 18 Oct 2024 16:40:32 GMT</pubDate>
    </item>
    <item>
      <title>我是 RL/DRL 的初学者。我有兴趣了解如何使用 DRL 解决非凸甚至凸优化问题（受约束或不受约束）。如果可能的话，有人可以分享使用 DRL 解决的代码吗...</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6he0y/i_am_a_beginner_to_rldrl_i_am_interested_to_know/</link>
      <description><![CDATA[我是 RL/DRL 的初学者。我有兴趣了解如何使用 DRL 解决非凸甚至凸优化问题（约束或无约束）。如果可能的话，有人可以分享使用 DRL 解决的代码吗？问题如下 最小化 (x + y-2)^2 受 xy &lt; 10 约束 并且 xy &gt; 1 x 和 y 是一些标量 以上是一个示例问题。也可以建议任何其他示例。但请保持建议和代码简单、可读且易于理解。    提交人    /u/gudduarnav   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6he0y/i_am_a_beginner_to_rldrl_i_am_interested_to_know/</guid>
      <pubDate>Fri, 18 Oct 2024 12:55:58 GMT</pubDate>
    </item>
    <item>
      <title>RL 学生课程主题材料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g6d5g9/material_on_topics_of_rl_for_student_course/</link>
      <description><![CDATA[我正在开设 RL 入门课程，希望学生熟悉给定的主题，然后将其介绍给剩余的课程。 为此，我正在寻找好的论文/文章/资源，理想情况下这些论文/文章/资源易于理解并提供有关该主题的良好概述。请分享任何适合这些主题的资源：  稀疏奖励 Sim2Real 可解释和可解释的 RL     提交人    /u/EmbarrassedCause3881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g6d5g9/material_on_topics_of_rl_for_student_course/</guid>
      <pubDate>Fri, 18 Oct 2024 08:20:25 GMT</pubDate>
    </item>
    <item>
      <title>“MLE-bench：在机器学习工程中评估机器学习代理”，Chan 等人 2024 {OA}（Kaggle 扩展）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5yr0p/mlebench_evaluating_machine_learning_agents_on/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5yr0p/mlebench_evaluating_machine_learning_agents_on/</guid>
      <pubDate>Thu, 17 Oct 2024 19:09:33 GMT</pubDate>
    </item>
    <item>
      <title>RL 用于系统的最优控制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5wecg/rl_for_optimal_control_of_systems/</link>
      <description><![CDATA[我最近看到了这篇 IEEE 论文，标题为“基于强化学习的使用 Carleman 线性化的非线性系统近似最优控制”。看起来他们在非线性系统的近似上使用某种形式的强化控制，并且与线性 RL 相比表现出良好的性能。 有人对这种 Carleman 近似方法有什么见解吗？    提交人    /u/Playful_Passage_2985   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5wecg/rl_for_optimal_control_of_systems/</guid>
      <pubDate>Thu, 17 Oct 2024 17:29:13 GMT</pubDate>
    </item>
    <item>
      <title>ADAS 的 RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5r81b/rl_implementation_for_adas/</link>
      <description><![CDATA[嘿。我想探索使用 RL 模型（本质上是一种基于奖励的模型）开发 ADAS 功能（如 FCW 或 ACC）的可能性，在这些功能中，将发出警告，并根据车辆采取的行动给予奖励。我希望有人能指导我如何做到这一点？我想使用 CARLA 来构建我的环境。     提交人    /u/SignificanceMotor285   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5r81b/rl_implementation_for_adas/</guid>
      <pubDate>Thu, 17 Oct 2024 13:44:54 GMT</pubDate>
    </item>
    <item>
      <title>从 DQN 到 Double DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5p8mi/from_dqn_to_double_dqn/</link>
      <description><![CDATA[我已经有一个 DQN 的实现。要将其更改为双 DQN，似乎只需要进行一点小改动：在 Q 值更新中，下一个状态（最佳）动作选择和该动作的评估均由 DQN 中的目标网络完成。而在双 DQN 中，下一个状态（最佳）动作选择由主网络完成，但该动作的评估由目标网络完成。 这似乎相当简单。我还遗漏了什么吗？    提交人    /u/No_Addition5961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5p8mi/from_dqn_to_double_dqn/</guid>
      <pubDate>Thu, 17 Oct 2024 12:02:35 GMT</pubDate>
    </item>
    <item>
      <title>何时使用强化学习，何时不使用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5mge9/when_to_use_reinforcement_learning_and_when_to/</link>
      <description><![CDATA[何时使用强化学习，何时不使用。我的意思是何时使用普通数据集来训练模型，何时使用强化学习     提交人    /u/Alarming-Power-813   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5mge9/when_to_use_reinforcement_learning_and_when_to/</guid>
      <pubDate>Thu, 17 Oct 2024 08:54:47 GMT</pubDate>
    </item>
    <item>
      <title>Chi Jin 的普林斯顿 RL 课程好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5leg1/is_chi_jins_princeton_rl_course_good/</link>
      <description><![CDATA[普林斯顿大学 ECE524 强化学习基础课程，2024 年春季。  本课程是研究生课程，重点介绍强化学习的理论基础。它涵盖马尔可夫决策过程 (MDP) 的基础知识、基于动态规划的算法、规划、探索、信息理论下界以及如何利用离线数据。还讨论了各种高级主题，包括策略优化、函数逼近、多机构和部分可观测性。本课程特别强调算法及其理论分析。需要具备线性代数、概率和统计学方面的先验知识。    提交人    /u/sahoosubramanyam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5leg1/is_chi_jins_princeton_rl_course_good/</guid>
      <pubDate>Thu, 17 Oct 2024 07:30:33 GMT</pubDate>
    </item>
    <item>
      <title>使用多智能体 RL 代理优化分布式系统中的工作平衡/通信</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g5avgf/using_multiagent_rl_agents_for_optimizing_work/</link>
      <description><![CDATA[我偶然发现了这篇名为 &quot;负载平衡并行粒子追踪的强化学习&quot; 的论文，它让我绞尽脑汁。他们使用多智能体 RL 在分布式系统中实现负载平衡，但我不确定这是否可行。 以下是本文的要点：  他们使用多智能体 RL 来平衡工作负载并优化并行粒子追踪中的通信 每个进程（最多 16,384 个！）都有自己的 RL 代理（用于其策略网络的单层感知器） 代理的操作是在进程之间移动工作块以平衡事物  我听说多智能体 RL 很难正常工作？有了这么多进程，由于每个代理都可能决定将工作转移到数千个其他进程中的任何一个，因此操作空间不是非常巨大吗？ 所以，我的问题是：这真的可行吗？或者，动作空间太大，无法在实践中发挥作用？我很想听听任何有 RL 或并行计算经验的人的意见。我是否遗漏了什么，或者这听起来很疯狂？ 谢谢！P.S. 如果有人真的尝试过这样的事情，我会非常感兴趣听听它进展如何！    提交人    /u/ypsoh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g5avgf/using_multiagent_rl_agents_for_optimizing_work/</guid>
      <pubDate>Wed, 16 Oct 2024 21:39:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么我无法使用`sbx`来为我的`DQN`程序提供种子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g599hv/why_am_i_unable_to_seed_my_dqn_program_usingsbx/</link>
      <description><![CDATA[我尝试在使用 `sbx` 时为我的 DQN 程序播种，但由于某种原因，我不断得到不同的结果。 这里尝试创建一个最小的可重现示例 - https://pastecode.io/s/nab6n3ib 结果相当令人惊讶。在多次运行此程序时，我得到了各种各样的结果。 这是我的结果 - 尝试 1： ``` 运行 = 0 使用种子：1 运行 = 1 使用种子：1 运行 = 2 使用种子：1 mean_rewards = [120.52, 120.52, 120.52] ``` 尝试 2： ``` 运行 = 0 使用种子：1 运行 = 1 使用种子：1 运行 = 2 使用种子：1 mean_rewards = [116.64, 116.64, 116.64] ``` 令人惊讶的是，在尝试中，我得到了相同的结果。但是当我再次运行该程序时，我得到了不同的结果。 我查看了从[此处][1]播种环境的文档，还阅读了以下内容 - “*无法保证在 PyTorch 版本或不同平台上完全可重现的结果。此外，即使使用相同的种子，结果也不必在 CPU 和 GPU 执行之间重现。”。但是，我想确保我这边没有错误。另外，我使用的是“sbx”而不是“stable-baselines3”。也许这是一个 `JAX` 问题？ 我还在 此处创建了一个 S.O 帖子 [1]: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html#reproducibility    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g599hv/why_am_i_unable_to_seed_my_dqn_program_usingsbx/</guid>
      <pubDate>Wed, 16 Oct 2024 20:30:52 GMT</pubDate>
    </item>
    <item>
      <title>帮助找到培训 Pool9 代理的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g553g6/help_to_find_a_way_to_train_pool9_agent/</link>
      <description><![CDATA[      嗨！ 我正在开发一个玩 Pool9 的 Agent 做出决定：在击球前，当所有球都处于静止位置时，会做出击球方向和力量的决定 观察： 1. 我首先放置球和球袋的标准化坐标 + 哪个球是目标的标志 2. 然后我开启使用方向和标准化到球的距离 3. 然后我添加了课程，它经过多次改进，最后的计划是 课程 0：学习触摸目标球 3 个球 随机目标 球的随机初始放置 触摸目标的奖励 课程 1：学习在触摸目标球后接住任何球 6 个球 随机目标 球的随机初始放置 触摸目标的奖励 + 接住任何球 非法击球的惩罚（目标球没有被触碰） 课程 2：游戏 9 个球 静态初始位置 目标编号 -有序 训练器：ppo 2-4 层 128-512 结果几乎相同，训练速度的差异， 但似乎代理无法预测轨迹：（ 有什么想法或建议吗？我将不胜感激 第 1 课从未达到 https://preview.redd.it/yu2wzujpl5vd1.png?width=3246&amp;format=png&amp;auto=webp&amp;s=5a783f500c84a05f2a80eba6286f3922db6e3fc6 https://reddit.com/link/1g553g6/video/vmkiuz9zl5vd1/player    由   提交  /u/Ecstatic-Ring3057   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g553g6/help_to_find_a_way_to_train_pool9_agent/</guid>
      <pubDate>Wed, 16 Oct 2024 17:33:00 GMT</pubDate>
    </item>
    <item>
      <title>Unity ML 代理和贪吃蛇等游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4xhqd/unity_ml_agents_and_games_like_snake/</link>
      <description><![CDATA[大家好， 我一直在尝试理解神经网络和游戏 AI 的训练。但我目前在努力玩 Snake。我想“好吧，让我们给它一些射线传感器、一个摄像头传感器，吃食物时给予奖励，与自身或墙壁碰撞时给予负面奖励”。 我想说它学得很好，但并不完美！在 10x10 的游戏场中，它的最高分约为 50，但到目前为止它从未掌握游戏。 有人能给我一些建议或线索，如何更好地处理使用 PPO 进行蛇 AI 训练吗？ 射线传感器可以检测墙壁、蛇本身和食物（3 个不同的传感器，每个传感器有 16 条射线） 摄像头传感器的分辨率为 50x50，也可以看到墙壁、蛇头以及蛇周围的蛇尾。它是一个尺寸为 8 的正交相机，因此它可以看到整个运动场。 首先，我只使用射线传感器进行测试，然后我添加了相机传感器，我可以说的是，它通过相机视觉观察学习得更快，但最后它的最高分大约相同。 我正在并行训练 10 个代理。 网络设置为： 50x50x1 视觉观察输入 大约 100 个射线观察输入 512 个隐藏神经元 2 个隐藏层 4 个离散输出动作 我目前正在尝试使用 buffer_size 为 25000 和 batch_size 为 2500。学习率为 0.0003，Num Epoch 为 3。时间范围设置为 250。 是否有人使用过 Unity 的 ML Agents Toolkit 并能帮助我一点？ 我做错了什么吗？ 我将感谢你们给予我的每一次帮助！ 这里有一个小视频，你可以在其中看到大约第 150 万步的培训： https://streamable.com/tecde6    提交人    /u/Seismoforg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4xhqd/unity_ml_agents_and_games_like_snake/</guid>
      <pubDate>Wed, 16 Oct 2024 11:51:50 GMT</pubDate>
    </item>
    <item>
      <title>如何应对SAC的灾难性遗忘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g4tklf/how_to_deal_with_the_catastrophic_forgetting_of/</link>
      <description><![CDATA[      嗨！ 我建立了一个使用SAC进行训练的自定义任务。成功率曲线在稳步上升后逐渐下降。在查阅了一些相关讨论后，我发现这种现象可能是灾难性的遗忘。 https://preview.redd.it/i5bxwet9j2vd1.png?width=1352&amp;format=png&amp;auto=webp&amp;s=81ea533917317b57ebd924668f24fdd59e275c43 我尝试规范奖励并自动调整 alpha 的值来控制探索和利用之间的平衡。其次，我还降低了 actor 和 critic 的学习率，但这只会减慢学习过程并降低整体成功率。 我想得到一些关于如何进一步稳定这个训练过程的建议。 提前感谢您的时间和帮助！    提交人    /u/UpperSearch4172   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g4tklf/how_to_deal_with_the_catastrophic_forgetting_of/</guid>
      <pubDate>Wed, 16 Oct 2024 07:13:25 GMT</pubDate>
    </item>
    </channel>
</rss>