<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 09 Dec 2024 12:37:12 GMT</lastBuildDate>
    <item>
      <title>训练多智能体（人类代理/地面实况）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ha4mj2/training_multiagents_human_proxyground_truth/</link>
      <description><![CDATA[大家好， 我对 RL 非常陌生，我正在阅读这篇论文 https://arxiv.org/abs/1910.05789，该论文介绍了使用人类数据训练代理如何优于其他方法（即自我游戏）。过度训练的环境适用于协作多智能体（2 个）。  我想知道他们是如何训练 Human_proxy（基本事实）的，它用于与不同的自我智能体配对，以比较它们可以获得的分数。我读了这篇论文，但我仍然不明白这个概念。他们是否使用分离的轨迹和 BC 算法训练 H_proxy 独自游戏，还是 H_proxy 也与其他人一起训练。  就像我说的，我对此非常陌生，所以我的解释可能没有意义。。 谢谢！感谢您对理解这一点的任何帮助。     提交人    /u/Complete-Internet509   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ha4mj2/training_multiagents_human_proxyground_truth/</guid>
      <pubDate>Mon, 09 Dec 2024 07:39:29 GMT</pubDate>
    </item>
    <item>
      <title>您对“雄心勃勃”的 RL 项目有何想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h9g88l/your_ideas_on_ambitious_rl_projects/</link>
      <description><![CDATA[在阅读此帖子的评论后，我很好奇什么会被认为与 ChatGPT/LLM/Transformer 一样雄心勃勃，但对于 RL 来说呢？  我猜这将需要像上面那样的 3 个组件： 1. 一项科学突破：Transformer 2. 一个大规模版本：LLM 3. 一个应用程序：ChatGPT 重新表述我的问题，对于上面的每个组件，您认为它们会是什么？ 我基于纯粹想象的镜头： 1. 通用 RL + 推理 + 低功耗 2. Isaac Lab 3. 机器人足够强大，以便人们开始部署这些机器人而不是人类。    提交人    /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h9g88l/your_ideas_on_ambitious_rl_projects/</guid>
      <pubDate>Sun, 08 Dec 2024 10:39:15 GMT</pubDate>
    </item>
    <item>
      <title>如果没有开放的“OpenAI”，强化学习还能真正进步吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h9d14c/will_rl_truly_advance_without_the_open_openai/</link>
      <description><![CDATA[OpenAI 在实际的非盈利时代推动了现实生活中的实用 RL，到目前为止，策略 RL 上的无模型说实话是最实用的...因为世界具有非平稳特性，通常需要课程并且本质上是多智能体...所有这些结合在一起并不容易适合离策略 RL。 可悲的是，OpenAI 主导了 trpo/ppo 领域，而现在，人工智能货币化的竞赛才是真正的目标...你对 RL 的未来有什么期待，因为我怀疑一些进步是隐藏的并且不公开可用     提交人    /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h9d14c/will_rl_truly_advance_without_the_open_openai/</guid>
      <pubDate>Sun, 08 Dec 2024 06:47:26 GMT</pubDate>
    </item>
    <item>
      <title>将图搜索问题构建为 RL 问题是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h96fkm/does_it_make_sense_to_frame_a_graph_search/</link>
      <description><![CDATA[我是一名计算机科学研究员，最近开始探索如何使用强化学习来学习图上 NP 难题的启发式方法。 我正在研究的问题是这样的：从初始图 G0 开始，您可以在两个操作（操作 1 或操作 2）之间进行选择，这两个操作分别产生直接成本r1(G0) 和 r2(G0)（均 &gt; 0）。每个操作都会将图修改为较小的图，即 A1(G0) 或 A2(G0)，节点较少。该过程持续到图为空，总成本是沿途产生的所有成本的总和。目标是选择操作以最小化成本总和。  我想将其定义为一个 RL 问题，其目标是学习一个策略 π(G)，该策略可在任意输入图 G 的两个动作之间进行选择。例如，π(G)∈(0,1) 可以表示选择动作 1 的概率，π(G) 可以使用图神经网络进行建模。 这个公式作为 RL 问题有意义吗？此外，您是否建议使用类似 AlphaGo 风格的方法来训练它，或者像深度 Q 学习这样的原始方法就足够了？    提交人    /u/Local-Assignment-657   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h96fkm/does_it_make_sense_to_frame_a_graph_search/</guid>
      <pubDate>Sun, 08 Dec 2024 00:28:02 GMT</pubDate>
    </item>
    <item>
      <title>简单、超级干净的 DreamerV3，但效果不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8xu0k/simple_super_clean_dreamerv3_that_doesnt_quite/</link>
      <description><![CDATA[      我正在寻找社区的一点帮助和智慧，以获得有关我的业余 DreamerV3 pytorch 实现的一些评论和反馈。我需要一些指导，以了解可能出现的问题，因为在我的 rtx 3060 笔记本电脑上训练需要很长时间，因此无法测试所有小变化。而且我也没有主意了。   过去几个月，我一直在实现自己的 DreamerV3 算法，该算法写得非常清楚，我可以轻松地将它教给其他人，并直观地了解它是如何工作的以及为什么有效。 我已经实现了大部分技巧，但简化了一些部分，例如，我没有包括连续预测，假设我的任务永远持续下去（我想在解决更多问题之前击败连续的 CarRacing 健身房环境），并且我没有 twohot 损失，只有 mse 损失。但是，这些剩余的技巧应该取决于它的效果如何，而不是它是否有效。 这就是我的问题，尽管我研究了其他实现，多次阅读了论文，确保训练循环符合预期，但它仍然不起作用。 如果任何知道 DreamerV3 工作原理的人能看看我写的内容，如果一切正常，至少扫描一下，我将不胜感激。世界模型训练中的梯度有些不对劲，所以我会先在那里查找问题。 除此之外，也许还有一些简单的参数调整可以立即提高性能？也许我应该重写一些东西，也许 twohot 损失是必要的？ 我正在寻找任何更有经验的人可以赐予我的智慧，因为我自己学习一切。 我已经盯着它看了很久，我可能会错过显而易见的东西，所以欢迎每一位新人。我也不掩饰我在这方面不太有经验的事实，所以我只是在尝试一些东西。 现在，我已经在 71k 梯度步骤上测试了我的最新代码（我晚上每 10 小时只能训练大约 30k 步）。我可以走得更远，但它似乎根本没有改善（也没有以任何方式改变）。它看起来只是收敛得不好，所以我确定梯度不对。代理在未经训练的权重上获得大约 7 个总奖励，目前在 70k 步之后，它平均获得 -9，向右转动并无休止地旋转。如果它什么都不做，只是呆在原地，-19 奖励就像是绝对的底部。 到目前为止，我对强化学习的了解，或多或少只需要一个位置合适的 .detach() 就可以让它工作。 我将在 publicResults 文件夹中发布一些训练结果。 提前致谢。 Repo：https://github.com/InexperiencedMe/dreamer   最新运行在 html 图上的 publicResults 文件夹中，名称为 AAAAAA_FULLSTATE_DETACHED，如下所示。有趣的是，我过去已知的糟糕训练循环，即在单个非批处理序列上学习的循环，在运行之间表现更好，但存在一些差异。我最新的代码刚刚收敛到全速前进，全速右转，没有刹车，无休止旋转。 https://preview.redd.it/eeqq7z3frg5e1.png?width=1701&amp;format=png&amp;auto=webp&amp;s=a9799787cc17b4c2e5ca87a33d17e91fa365fa89      提交人    /u/Inexperienced-Me   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8xu0k/simple_super_clean_dreamerv3_that_doesnt_quite/</guid>
      <pubDate>Sat, 07 Dec 2024 17:46:35 GMT</pubDate>
    </item>
    <item>
      <title>设计奖励结构以控制自动驾驶汽车，实现交通平稳化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8xqa7/reward_structure_design_to_control_an_autonomous/</link>
      <description><![CDATA[大家好，我需要你们的帮助。 我正在完成一项关于使用 Q 学习 进行最优控制的大学作业。 我的环境如下：我有一个由人类驾驶的车辆组成的交通流，其中只有 一辆自动驾驶汽车 (ADV)。我的目标是训练代理（ADV 的输入）与领头车辆（以我选择的间隔刹车）保持 最佳距离，从而保持 最佳速度，以便为跟随它的车辆平滑交通波。 我想在代理与领头车辆保持的距离大于最佳距离的情况下以及代理保持接近最佳速度的情况下“奖励”代理，因为这样就不会产生走走停停的交通波。推测一下，当相反的情况发生时，我会“惩罚”他。 问题是代理似乎没有学习。 我对以下内容存有疑虑： - 奖励或阻止他的条件； - 奖励和惩罚的标志和价值。 实际上我的奖励结构是： IsDone = max(abs(x_next) &gt; StateThreshold) || (distance_1 &lt; eta) || (distance_2 &lt; eta); distance_penalty = 100 * double(((x1-x2)-s_star)&lt;0); velocity_deviation_penalty = (v2 - v_star)^2 * double((v2 - v_star)&gt;0); 成本 = distance_penalty + velocity_deviation_penalty; distance_reward= - 100 * double(((x1-x2)-s_star)&gt;0); velocity_target_reward = - (v2 - v_star)^2 * double((v2 - v_star)&lt;(-1) &amp;&amp; (v2-v_star)&gt;1); 如果IsDone == 1 Reward = -1000; else Reward = -Cost + velocity_target_reward + distance_reward; end 有人能帮我吗？    提交人    /u/SpiritedPotato2844   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8xqa7/reward_structure_design_to_control_an_autonomous/</guid>
      <pubDate>Sat, 07 Dec 2024 17:41:38 GMT</pubDate>
    </item>
    <item>
      <title>现实生活中的 MDP 与 POMDP。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8rnx0/mdps_vs_pomdps_in_real_life_settings/</link>
      <description><![CDATA[大家好， 我只是想听听你对 MDP 与 POMDP 的看法。 当我有一个在现实世界中运行的机器人代理时，将其建模为 MDP 而不是 POMDP 是否有意义？ 我觉得在世界上每个机器人在现实世界中运行的场景中，将其建模为 POMDP 而不是 MDP 更有意义。 你们觉得呢？    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8rnx0/mdps_vs_pomdps_in_real_life_settings/</guid>
      <pubDate>Sat, 07 Dec 2024 12:41:16 GMT</pubDate>
    </item>
    <item>
      <title>PyBullet 问题 - 搭建简单的积木塔（Jenga）时，它总是会倒塌</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8mlc8/pybullet_problem_when_building_a_simple_tower_of/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8mlc8/pybullet_problem_when_building_a_simple_tower_of/</guid>
      <pubDate>Sat, 07 Dec 2024 06:35:13 GMT</pubDate>
    </item>
    <item>
      <title>欧洲多智能体强化学习 (MARL) 研究小组</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8mhvx/multiagent_reinforcement_learning_marl_research/</link>
      <description><![CDATA[大家好！我目前正在攻读机械工程硕士学位，我的大部分研究都集中在多智能体强化学习 (MARL) 和智能交通系统上。我现在正计划申请该领域的博士学位，特别是在欧洲。 我对涉及将 MARL 应用于机器人、无人机或动态环境中的智能决策的研究非常感兴趣。您能推荐欧洲活跃于该领域的任何研究小组或实验室吗？    提交人    /u/Glittering_Rip_6841   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8mhvx/multiagent_reinforcement_learning_marl_research/</guid>
      <pubDate>Sat, 07 Dec 2024 06:28:34 GMT</pubDate>
    </item>
    <item>
      <title>PyBullet 中的积木塔正在倒塌</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h8bnym/blocks_tower_is_collapsing_in_pybullet/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h8bnym/blocks_tower_is_collapsing_in_pybullet/</guid>
      <pubDate>Fri, 06 Dec 2024 21:02:35 GMT</pubDate>
    </item>
    <item>
      <title>我是不是别无选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h87hx1/am_i_left_with_no_other_option/</link>
      <description><![CDATA[ROS 太烂了 我是一名计算机科学专业的学生，​​之前做过一些 ROS，我发现为我的 RL 项目设置（Gazebo）非常烦人。我没有 FLOSS 选项了吗？ 有没有办法可以在没有 ROS 的情况下模拟环境？    提交人    /u/iconic_sentine_001   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h87hx1/am_i_left_with_no_other_option/</guid>
      <pubDate>Fri, 06 Dec 2024 18:02:21 GMT</pubDate>
    </item>
    <item>
      <title>“奖励基础：一种自适应获取多种奖励类型的简单机制”，Millidge 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h82gh2/reward_bases_a_simple_mechanism_for_adaptive/</link>
      <description><![CDATA[   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h82gh2/reward_bases_a_simple_mechanism_for_adaptive/</guid>
      <pubDate>Fri, 06 Dec 2024 14:23:34 GMT</pubDate>
    </item>
    <item>
      <title>PPO 代理完成目标，但解释方差变得更糟？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7wwcq/ppo_agent_completing_objective_but_explained/</link>
      <description><![CDATA[      我目前正在训练一个 RecurrentPPO 代理完成一个简单的交易任务。 基本上每一步它都可以决定是做空还是做多。没有花哨的持有。 奖励是每个时间步的风险调整回报。 输入是来自几个基本特征的 7 个标准化 pca 特征。 代理似乎理解了这项任务，并且执行得相当好。 然而，虽然它在解决任务方面的表现在逐渐提高，但解释的方差基本上越来越差，或者在 0 附近波动。 https://preview.redd.it/d8tt468ut65e1.png?width=2528&amp;format=png&amp;auto=webp&amp;s=6695e68fc80b2149fd8f47d7ce31eba0c7428cbe 这是 sb3 中的当前策略： policy_kwargs = dict( net_arch=dict(pi=[256, 256], vf=[256, 256]),activation_fn=torch.nn.Tanh,ortho_init=True,enable_critic_lstm=False,lstm_hidden_​​size=28,optimizer_class=AdamW,share_features_extractor=True,features_extractor_class=IdentityFeatureExtractor,）模型=RecurrentPPO（“MlpLstmPolicy”，env，verbose=0，learning_rate=0.00001，n_steps=400，batch_size=100，clip_range=0.2，clip_range_vf=0.2，ent_coef=0.1，vf_coef=0.1，gamma=0.99，gae_lambda=0.95，种子=42，policy_kwargs=policy_kwargs，tensorboard_log=log_dir， max_grad_norm=0.5, n_epochs=4, stats_window_size=2, normalize_advantage=True)  结果： RecurrentActorCriticPolicy( (features_extractor): IdentityFeatureExtractor() (pi_features_extractor): IdentityFeatureExtractor() (vf_features_extractor): IdentityFeatureExtractor() (mlp_extractor): MlpExtractor( (policy_net): Sequential( (0): Linear(in_features=28, out_features=256, bias=True) (1): Tanh() (2): Linear(in_features=256, out_features=256, bias=True) (3): Tanh() ) (value_net): Sequential( (0): Linear(in_features=28, out_features=256, bias=True) (1): Tanh() (2): Linear(in_features=256, out_features=256, bias=True) (3): Tanh() ) ) (action_net): Linear(in_features=256, out_features=2, bias=True) (value_net): Linear(in_features=256, out_features=1, bias=True) (lstm_actor): LSTM(6, 28) (critic): Linear(in_features=6, out_features=28, bias=True) )  我是否遗漏了某些关键信息，或者如果代理实现了预期目标，我是否不应该关心解释方差？    提交人    /u/Educational_Study553   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7wwcq/ppo_agent_completing_objective_but_explained/</guid>
      <pubDate>Fri, 06 Dec 2024 08:32:43 GMT</pubDate>
    </item>
    <item>
      <title>利用强化学习实现火箭着陆：Unity 中的 2D 和 3D 模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7qbay/landing_rockets_with_reinforcement_learning_2d/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7qbay/landing_rockets_with_reinforcement_learning_2d/</guid>
      <pubDate>Fri, 06 Dec 2024 02:00:22 GMT</pubDate>
    </item>
    <item>
      <title>解释法学硕士强化学习基础知识的技术指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h7kixk/technical_guide_explaining_the_fundamentals_of/</link>
      <description><![CDATA[       由    /u/Legaltech_buff  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h7kixk/technical_guide_explaining_the_fundamentals_of/</guid>
      <pubDate>Thu, 05 Dec 2024 21:34:18 GMT</pubDate>
    </item>
    </channel>
</rss>