<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 21 May 2024 18:18:22 GMT</lastBuildDate>
    <item>
      <title>这个项目 Idea 好吗 - 自动驾驶代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</link>
      <description><![CDATA[我正在为自动驾驶代理构建一个项目，我计划加入会改变状态的动态交通信号灯和独立于代理在网格中随机移动的动态行人。加入动态行人有多难？在网格中随机生成信号灯和行人的位置可以吗？还可以使用 q-learning 实现 我怎样才能使这个项目更具随机性？我考虑在每次模拟之前引入与概率分布一起使用的天气影响，并且根据天气预报，这将影响车辆的燃料消耗，惩罚会导致燃料消耗的百分比大幅增加。本质上，代理处于生存模式，以确保不会发生碰撞、遵守交通法规并且不会达到最大燃料容量。    提交人    /u/amulli21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</guid>
      <pubDate>Tue, 21 May 2024 15:56:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用人类演示来预训练 RL 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</link>
      <description><![CDATA[我想要一个代码示例，以便了解我该如何做到这一点。有人能帮帮我吗？    提交人    /u/SebyR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</guid>
      <pubDate>Mon, 20 May 2024 18:16:01 GMT</pubDate>
    </item>
    <item>
      <title>“Mobile ALOHA：通过低成本全身远程操作学习双手移动操作”，Fu 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</guid>
      <pubDate>Mon, 20 May 2024 17:35:15 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的对抗性攻击和对抗性训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /对抗性强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</guid>
      <pubDate>Mon, 20 May 2024 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：解决强化学习 PyTorch 模型中的“mat1 和 mat2 形状无法相乘”错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwj968/help_needed_solving_mat1_and_mat2_shapes_cannot/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwj968/help_needed_solving_mat1_and_mat2_shapes_cannot/</guid>
      <pubDate>Mon, 20 May 2024 16:08:17 GMT</pubDate>
    </item>
    <item>
      <title>q 学习和井字游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</link>
      <description><![CDATA[tttrl - Pastebin.com KDT85/Tic-Tac-Toe-RL：使用强化学习的 Tic-Tac-Toe 游戏 (github.com)&lt; /a&gt; 上面是我使用强化学习的井字棋游戏的代码，你们觉得怎么样。  它似乎可以工作，但 q 表没有填满，这正常吗？其训练次数为 20000000 集。  GitHub 中包含 q 表作为 Excel 文件。  谢谢！    由   提交 /u/chinfuk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</guid>
      <pubDate>Mon, 20 May 2024 15:12:36 GMT</pubDate>
    </item>
    <item>
      <title>致力于为 Azul Board Game 开发成功的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</link>
      <description><![CDATA[大家好， 我目前正在做一个项目，旨在训练一个模型精通桌游 Azul 。我用 Python 编写了自己的游戏引擎，并且使用 SB3 PPO 实现。我的观察是一个由大约 700 位组成的二进制向量（这是整个游戏状态的二进制表示，如工厂、玩家板等），我的动作空间是 300 大小的向量，每个位对应于某个动作（在 azul 游戏中，每回合你必须做出 3 个决定：选择哪个工厂、什么颜色以及将其放置在哪里，并且你有 10 个工厂（我的模型正在玩 4 人游戏）、5 种颜色和 6 条图案线，因此你有 300 种可能我已经多次运行它，使用不同的激活函数、层大小、层数、学习率、伽马等，我尝试使用分数作为奖励，也尝试使用一些任意奖励，例如进行有效的移动等。 ，但我从未达到令人满意的结果，该模型从平均 -90 分开始，逐渐达到 -30/-20，但这仍然是非常糟糕的结果，因为 azul 中的负分意味着它做了很多废话平均球员每场得分超过 40/50 分，所以我与我的模型还相去甚远。  azul 环境的巨大问题是，在 300 个动作中，通常只有 40 个有效动作，因此这意味着只有大约 10-15% 的动作是有效的。首先，我的模型将以某种方式工作，它采取具有最高神经元值的操作，如果有效，则执行此操作，如果建议的操作无效（例如，模型想要从工厂 2 获取蓝色瓷砖，并将其放在模式行3中，但该工厂没有蓝色瓷砖），它将采取第二高神经元值动作并检查它是否有效等等，直到达到有效动作。  我最近的想法是对模型输出应用一些屏蔽，这样当模型观察棋盘并返回动作向量时，它会过滤所有无效动作，将它们的值设置为 0，然后然后我们只有有效动作的动作向量，第二个变化是我不选择最高值，而是将 softmax 应用于这些值并使用计算的概率随机选择它们。  不幸的是，我没有得到任何结果，该模型根本没有学习。你们对我可以在哪里改进我的算法、为什么我的模型无法学习等有什么建议吗？我将非常感谢所有的帮助，我也愿意为那些感兴趣的人分享代码，所以请告诉我，  请帮助我（：    提交者    /u/Different-Leave8202   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</guid>
      <pubDate>Mon, 20 May 2024 12:34:04 GMT</pubDate>
    </item>
    <item>
      <title>有人真的部署了一个模型来用于推理吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</link>
      <description><![CDATA[我只是好奇有人实际上使用 RL 训练的策略来帮助解决哪些应用程序或控制问题，您用什么 Algo 进行训练？在此过程中遇到的主要挑战是什么？    由   提交 /u/Aggressive-Reach1657    reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</guid>
      <pubDate>Mon, 20 May 2024 07:01:20 GMT</pubDate>
    </item>
    <item>
      <title>“认识一下 Shakey：第一个电子人——拥有自己思想的机器的迷人而可怕的现实”，Darrach 1970</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</guid>
      <pubDate>Mon, 20 May 2024 01:39:13 GMT</pubDate>
    </item>
    <item>
      <title>最近有哪些您真正喜欢的 RL 应用示例？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw1yk3/any_recent_examples_of_rl_applications_you_really/</link>
      <description><![CDATA[  由    /u/paswut  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw1yk3/any_recent_examples_of_rl_applications_you_really/</guid>
      <pubDate>Sun, 19 May 2024 23:50:34 GMT</pubDate>
    </item>
    <item>
      <title>输入/输出关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw0ycm/inputoutput_relationships/</link>
      <description><![CDATA[社区您好， 假设我们有 N 个元素，每个元素都有一组特征（Xi1，Xi2）和任务DQN 的目的是选择其中一个元素（因此我们有 3 个输出）。假设输入向量为[X11, X21, X12, X22]，DQN如何将元素的每个输入特征与其对应的输出联系起来，例如即使存在遥远的距离，它如何理解X11和X12是元素1的特征? 我希望描述清楚   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw0ycm/inputoutput_relationships/</guid>
      <pubDate>Sun, 19 May 2024 23:02:22 GMT</pubDate>
    </item>
    <item>
      <title>RL 导师/专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</link>
      <description><![CDATA[我是一名本科生，目前正在完成论文。我接手了一个项目，该项目使用 RL 进行连续控制，通过 6d 位姿估计器来控制机器人。我看得很远，但强化学习机器人技术在我们国家可能还不够饱和。我试图寻找结构化的方法来学习这一点，就像使用 OpenAI 旋转强化学习以及 Sutton &amp; 的理论背景一样。巴托的书。我真的很渴望在明年之前完成这个项目，但我没有导师。甚至我们大学的教授也很快就会采用强化学习机器人技术。我从以前的帖子里看到，在这里请教导师是可以的，所以请原谅。如果我无法正确地提出问题，我深表歉意。 我想要实现这些目标： - 充分掌握 RL 基础知识，特别是在连续动作空间控制方面。 - 熟悉艾萨克·西姆。 - 了解如何为强化学习建模物理系统 - 将经过训练的模型部署到物理机器人 - 通过项目慢慢积累知识，最终引导我完成项目 - 寻找指导我完成整个工作流程的导师 &lt; p&gt;我所知道的： - 深度学习的背景 - RL 的基本原理（直到 MDP 和 TD） - RL 算法的背景 - DQN、DDPG、TD3 如何在高级抽象中工作 - 在高级抽象中体验重播缓冲区和 HER - ROS 2 基础知识 我想知道什么： - 我需要学习所有数学知识吗？或者我可以只参考现有的实现吗？ - 考虑到我的资源限制，我只能实现一个算法（我在第三世界国家），我应该使用它来实现完成项目的最大可能性。目前，我正在关注TD3。 - 一个本科生团队有可能完成这样的项目吗？ - 鉴于资源限制，我们应该使用哪个 Jetson 板来运行策略？ - 我们的目标是针对易碎处理进行优化，我们如何限制研究？ 我的努力我目前正在研究更多并建立关于算法和强化学习的直觉。最近，我迁移到 Ubuntu 并设置了模拟所需的所有软件和环境 (Isaac Sim)。 挫败感 在没有人交谈的情况下继续这个项目非常具有挑战性，因为每个人都几乎不感兴趣与RL。每个资源都有一个非常陡峭的学习曲线，当我认为我知道某些资源时，某些资源指向了我不知道的其他东西。我必须在明年之前完成这个工作，尽管我正在尽我所能地学习，但仍有很多我不知道的事情。    由   提交 /u/echialas22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</guid>
      <pubDate>Sun, 19 May 2024 21:04:03 GMT</pubDate>
    </item>
    <item>
      <title>重现“SOLVING THE OSHI-ZUMO GAME M. Buro 2004”的结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvy09r/reproducing_results_of_solving_the_oshizumo_game/</link>
      <description><![CDATA[嗨， 我想在 Oshi-Zumo 游戏中进行自我游戏或双重预言（我省略了一些细节，但这是一个双人零和游戏。每个玩家都有一定数量的硬币，他们同时下注一定数量。下注最高的玩家将硬币移向敌人。当硬币“落”在任何玩家的一侧时，该玩家就输了）。 首先，我想我最好通过使用线性规划而不是使用函数近似来实际解决游戏来获得纳什均衡策略。我遵循了 M. Buro 2004 年发表的论文 解决 OSHI-ZUMO 游戏，并使用了 open spiel python 库。 我（经过长时间的调试）成功解决了 [50,3,1]-Oshi-Zumo 游戏，并发现纳什均衡策略在位置 (50,50,0) 的结果完全相同（他们论文第 5 段开头）。同样，我在位置 (6, 3, -3) 的策略和预期收益完全相同（就在同一段的下方）。但是对于位置 (20, 32, 3)，我没有相同的策略（它们执行 17、18 和 20 动作的概率在我的策略中被“分组”，该策略选择仅玩 20 - 其余的对于涉及较少硬币的动作是相同的）。但是预期收益是相同的。我想，我可能刚刚达到了另一个纳什均衡，因为该游戏中可能存在几个混合纳什均衡（实际上很有可能）。 然后，我让我的策略对抗随机代理和一个硬编码代理，如果他们有超过 2 个硬币，则玩 2 个，否则玩 1 个（受该论文图 3 的启发）。而且我在对抗随机代理时赢的远不及，但在对抗那个硬编码代理时赢的更多（他们实际上输了一点）。我还模拟了 200,000 场游戏。 我的问题是：我“只是”达到另一个纳什均衡，因此我不必担心无法重现他们所做的？（他们提供的重现结果的链接不再可用）还是我在某个地方搞砸了？如果是这样，我该怎么做才能确保我搞砸了。如果我一直在输掉一个策略，那么我当然没有达到纳什均衡，但到目前为止情况并非如此。 附加问题：他们说“在配备 1-GHzPentium-IIICPU 的笔记本电脑上，解决标准 [50, 3, 1] 游戏只需 12 秒”。在我的计算机上花了 2 分钟多的时间。我没有最好的机器，但我认为我的 CPU 可能比他们的好得多。这个差距可以通过我使用 Python 而他们使用 C++ 来解释吗？虽然我正在使用 pulp 来解决我的线性问题，所以我猜这已经通过调用编译代码进行了优化。我唯一合理的解释是，我的线性问题的解决方案很可能只是更精确，因为程序花了很长时间才能得到大量的小数，对吗？ 我已将所有代码附加在 colab 笔记本 中。 即使不熟悉 open_spiel 库的人也应该很容易理解。    提交人    /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvy09r/reproducing_results_of_solving_the_oshizumo_game/</guid>
      <pubDate>Sun, 19 May 2024 20:49:27 GMT</pubDate>
    </item>
    <item>
      <title>开+RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</link>
      <description><![CDATA[KAN 擅长持续学习，有可能使 on-policy 变得鲁棒吗？    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</guid>
      <pubDate>Sun, 19 May 2024 13:15:45 GMT</pubDate>
    </item>
    <item>
      <title>协变：“当我们在更多数据上训练 RFM-1 时，我们的 [机器人手臂] 模型的性能可预测地提高 [在拣选]”：5 倍以上的数据使误差减半</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cv0yrg/covariant_as_we_train_rfm1_on_more_data_our_robot/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cv0yrg/covariant_as_we_train_rfm1_on_more_data_our_robot/</guid>
      <pubDate>Sat, 18 May 2024 16:21:55 GMT</pubDate>
    </item>
    </channel>
</rss>