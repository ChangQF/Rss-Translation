<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 09 Dec 2023 18:16:05 GMT</lastBuildDate>
    <item>
      <title>自定义 pettingzoo 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ego4t/custom_pettingzoo_environments/</link>
      <description><![CDATA[我正在疯狂地尝试使用我的自定义 pettingzoo （并行 API）环境。这是一个 Mario 64 环境，我花了很多时间来允许多个 Mario 并为每个人提供图像，但我真的无法让它与任何软件包一起工作。我尝试过 Agilerl、sb3、rllib 甚至 cleanRL。我知道 pettingzoo 不像体育馆那么成熟，但这太荒谬了。如果你喜欢 Mario 64，你可以尝试一下，如果你让它工作，请告诉我最新信息 https://github.com/ Gumbo64/sm64-AI agilerl 可以运行，但只适合 4 名或更少的玩家，我不知道它是否能正常学习，尽管我现在要把它留着过夜。由于某种原因，Ray rllib 在推出后冻结 我所做的随机动作测试工作得很好，不过，重置或动作/观察空间或其他什么都不应该成为问题   由   提交/u/Gumbo64  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ego4t/custom_pettingzoo_environments/</guid>
      <pubDate>Sat, 09 Dec 2023 16:04:56 GMT</pubDate>
    </item>
    <item>
      <title>如何通过仅在剧集结束时给出的奖励来升级AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18eeutp/how_to_upgrade_ai_with_rewards_only_given_at_the/</link>
      <description><![CDATA[例如，当我们只在剧集结束时给 AI 奖励，而不是针对它所做的每一个动作时，我们该怎么办AI 赢得了比赛，获得+1 奖励，如果输了，获得-1 奖励，这种情况下我们该怎么办？   由   提交/u/OneCommonMan123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18eeutp/how_to_upgrade_ai_with_rewards_only_given_at_the/</guid>
      <pubDate>Sat, 09 Dec 2023 14:35:19 GMT</pubDate>
    </item>
    <item>
      <title>PPO 的低奖励波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ebijk/low_reward_oscillations_in_ppo/</link>
      <description><![CDATA[我正在尝试实现 PPO 算法，其中每个情节只有 1 个步骤，采样时间为 800 秒（意味着每个情节长度为 800 秒）我获得的奖励在从低到高的奖励之间波动。 （参考图片）如何在训练时解决这个问题。    由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ebijk/low_reward_oscillations_in_ppo/</guid>
      <pubDate>Sat, 09 Dec 2023 11:17:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PPO 训练 LSTM 策略？伴随着复杂的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18eb69c/how_to_train_a_lstm_policy_with_ppo_with_complex/</link>
      <description><![CDATA[您好， 我正在尝试了解如何实现具有多个操作作为输出的训练循环和策略（这是自回归预测的），例如LSTM（Seq2Seq，输入序列经过 N 个观察，输出序列复杂的动作类似于 Openai Five 中的做法 https://openai.com/研究/openai-5）。我所说的复杂动作是指定义了 N 种类型的动作，例如具有多个类别（例如，LSTM 输出序列可能用于移动动作，例如 hide_0 -&gt; move_action -&gt; offset_x -&gt; offset_y）。  让我担心的一件事是如何将这些操作映射到相应的对数概率并执行反向传播步骤，另一个问题是探索，因为单个操作空间显着爆炸，我假设需要的时间由于探索，训练这种策略要高得多。   由   提交/u/basic_r_user  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18eb69c/how_to_train_a_lstm_policy_with_ppo_with_complex/</guid>
      <pubDate>Sat, 09 Dec 2023 10:56:08 GMT</pubDate>
    </item>
    <item>
      <title>“Eureka：通过编码大型语言模型进行人性化奖励设计”，Ma 等人 2023 {Nvidia}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dycua/eureka_humanlevel_reward_design_via_coding_large/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dycua/eureka_humanlevel_reward_design_via_coding_large/</guid>
      <pubDate>Fri, 08 Dec 2023 22:22:22 GMT</pubDate>
    </item>
    <item>
      <title>开放世界中的学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18du95j/learning_curricula_in_openended_worlds/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.03126 后记中列出的后续工作（第 7 章）。 代码&lt; /strong&gt;： https://github.com/facebookresearch/level-replay https://github.com/facebookresearch/dcd ACCEL 演示：https://accelagent.github.io/ 摘要：  深度强化学习（RL）为训练最佳顺序决策代理提供了强大的方法。由于收集现实世界的交互可能会带来额外的成本和安全风险，sim2real 的常见范例是在模拟器中进行训练，然后进行现实世界的部署。不幸的是，强化学习代理很容易过度适应模拟训练环境的选择，更糟糕的是，当代理掌握了一组特定的模拟环境时，学习就结束了。相比之下，现实世界是高度开放的，具有不断变化的环境和挑战，使得这种强化学习方法不适合。简单地对模拟环境进行随机化是不够的，因为它需要做出任意的分布假设，并且组合地采样对学习有用的特定环境实例的可能性较小。理想的学习过程应该自动适应训练环境，以在匹配或超越现实世界复杂性的开放式任务空间上最大限度地发挥智能体的学习潜力。本论文开发了一类名为无监督环境设计（UED）的方法，旨在产生这种开放式过程。给定环境设计空间，UED 在学习代理能力的前沿自动生成无限序列或训练环境课程。通过基于极小最大遗憾决策理论和博弈论的广泛实证研究和理论论证，本论文的研究结果表明，UED 自动课程可以产生 RL 代理，该代理对以前未见过的环境实例表现出显着提高的鲁棒性和泛化能力。这样的自动课程是通向开放式学习系统的有希望的途径，该系统通过不断生成和掌握自己设计的额外挑战来实现更通用的智能。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18du95j/learning_curricula_in_openended_worlds/</guid>
      <pubDate>Fri, 08 Dec 2023 19:16:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的力量：看看这个 DeepRL Sektor 模型如何在 DIAMBRA 竞赛平台上提交的视频中为《终极真人快打 3》找到一个智能、超酷的漏洞利用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dpb57/the_power_of_reinforcement_learning_look_how_this/</link>
      <description><![CDATA[   /u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dpb57/the_power_of_reinforcement_learning_look_how_this/</guid>
      <pubDate>Fri, 08 Dec 2023 15:31:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找基于语言的机器人/嵌入式人工智能方向的实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dmos1/d_looking_for_labs_working_in_the_direction_of/</link>
      <description><![CDATA[大家好，我来自欧洲一所知名大学，攻读机器人学硕士学位。我对在机器人技术中使用视觉语言模型感兴趣，目前正在研究使用 CLIP 嵌入进行基于对象的映射和导航。 一些论文可能会让您更多地了解我所说的基于语言的机器人技术的含义：- 去任何地方，Gervet 等人。 - 牧场上的奶牛：语言驱动的零样本对象导航的基线和基准，Gadre 等人。 - 用于机器人导航的视觉语言地图，Huang 等人。 我正在尝试寻找该领域的博士职位，并寻找从事此工作的实验室。根据我的研究，我发现这个方向的大多数论文都来自美国的实验室（Sergey Levine、Dhruv Batra、Jitendra Malik、Pieter Abeel 等教授） 我很惊讶地发现只有欧洲有一个实验室在这一领域开展工作，该实验室位于德国弗莱堡。 （Wolfram Burgard） 如果有人知道在该领域工作的实验室，可以为我提供更多提示，我将非常感激。我主要对欧洲感兴趣，但如果您知道我的列表中没有的美国实验室，请随时发表评论！   由   提交/u/Bluebird705  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dmos1/d_looking_for_labs_working_in_the_direction_of/</guid>
      <pubDate>Fri, 08 Dec 2023 13:21:03 GMT</pubDate>
    </item>
    <item>
      <title>在几秒钟内学会飞行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18df2os/learning_to_fly_in_seconds/</link>
      <description><![CDATA[       由   提交/u/jonas-eschmann   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18df2os/learning_to_fly_in_seconds/</guid>
      <pubDate>Fri, 08 Dec 2023 04:52:42 GMT</pubDate>
    </item>
    <item>
      <title>通过 4 个开创性项目向初学者介绍强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dbmy6/a_beginners_intro_to_rl_through_4_seminal_projects/</link>
      <description><![CDATA[   /u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dbmy6/a_beginners_intro_to_rl_through_4_seminal_projects/</guid>
      <pubDate>Fri, 08 Dec 2023 01:49:29 GMT</pubDate>
    </item>
    <item>
      <title>“利用基于优势的离线策略梯度改进语言模型”，Baheti 等人，2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dagry/improving_language_models_with_advantagebased/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dagry/improving_language_models_with_advantagebased/</guid>
      <pubDate>Fri, 08 Dec 2023 00:49:32 GMT</pubDate>
    </item>
    <item>
      <title>我的定制体育馆环境似乎根本没有学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18d8tkc/my_custom_gymnasium_env_seems_not_learning_at_all/</link>
      <description><![CDATA[训练了一整天似乎没有学到是强化学习还是环境问题 https://github.com/jonnytracker/Flappy-Bird-RL &lt;!-- SC_ON - -&gt;  由   提交/u/jonnytracker2020   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18d8tkc/my_custom_gymnasium_env_seems_not_learning_at_all/</guid>
      <pubDate>Thu, 07 Dec 2023 23:29:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Gumbel-softmax 与 TD3 一起使用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18d8ib1/why_does_gumbelsoftmax_work_with_td3/</link>
      <description><![CDATA[我的假设是“在单臂老虎机设置中，选择正确的箱子将返回 1 的奖励，返回函数将成为一块”明智的常数函数。因此，critic的梯度几乎到处都是0”。 但是，我尝试了。 TD3 以 Gumbel-softmax 作为输出层。它会学习！我不知道为什么它会学习。  此外，设置温度太低无法学习&#39; 谁能解释一下发生了什么以及我错过了什么？谢谢   由   提交/u/Lopside_Hall_9750   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18d8ib1/why_does_gumbelsoftmax_work_with_td3/</guid>
      <pubDate>Thu, 07 Dec 2023 23:14:25 GMT</pubDate>
    </item>
    <item>
      <title>规划的循环网络模型解释了海马体重放和人类行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cu0wa/a_recurrent_network_model_of_planning_explains/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2023.01.16.523429v2 代码：https://github.com/KrisJensen/planning_code 摘要：  当面对新的情况时，人类常常会花大量时间思考可能的未来。为了使这种计划变得合理，行为的好处必须补偿花费在思考上的时间。在这里，我们通过开发神经网络模型来捕获人类行为的这些特征，其中计划本身由前额叶皮层控制。该模型由一个元强化学习代理组成，该代理具有通过从其自己的策略中采样想象的动作序列来进行计划的能力，我们称之为“推出”。当计划有益时，代理会学习计划，解释人类思维时代的经验变化。此外，人工智能体所采用的策略推出模式与最近在空间导航过程中记录的啮齿动物海马回放模式非常相似。我们的工作提供了一种新理论，说明大脑如何通过前额叶-海马体相互作用来实施规划，其中海马体重放是由前额叶动态触发并适应性影响的。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cu0wa/a_recurrent_network_model_of_planning_explains/</guid>
      <pubDate>Thu, 07 Dec 2023 11:55:55 GMT</pubDate>
    </item>
    <item>
      <title>强化学习“最方便”的期刊俱乐部</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cphho/most_convenient_journal_club_for_reinforcement/</link>
      <description><![CDATA[大家好！我正在为 RL 社区中的一些人解决问题，并希望得到您的意见/想法。 问题：  没有时间阅读最酷、最新的论文 单独处理论文并不那么有趣，而且通常很困难。 无法承诺参加每周的会议，需要完全的灵活性 &lt; /ol&gt; 为了解决这些问题，我建立了一个名为 DenseLayers.com 的期刊俱乐部网站。 帮助我需要的是反馈！如果可能，请至少回答其中一个问题。 :)   您目前阅读论文的频率如何，以上 3 个问题中的哪一个与您产生共鸣？ 您目前正在采取哪些措施来解决该问题？什么可以完全消除这个问题？ 您喜欢阅读和理解哪些流行的强化学习论文（如果您有时间的话）？  谢谢！   由   提交 /u/mngrwl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cphho/most_convenient_journal_club_for_reinforcement/</guid>
      <pubDate>Thu, 07 Dec 2023 06:30:13 GMT</pubDate>
    </item>
    </channel>
</rss>