<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 05 May 2024 01:04:24 GMT</lastBuildDate>
    <item>
      <title>寻求有关使用 DQN 的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckcy7p/looking_for_advice_on_using_dqn/</link>
      <description><![CDATA[你好！ 我在强化学习方面不是很有经验，所以我非常感谢任何一般性的提示和建议。总而言之，我遇到了算法不收敛的事实，并且我不太清楚我还可以调整哪些内容来避免这种情况。接下来将为感兴趣的人提供更多上下文和数据。 我正在尝试使用 DQN 和修改（特别是 RainbowDQN）通过在 ARGoS 中进行模拟来解决觅食问题。机器人必须在环境中找到一个物体并带着它返回基地，不断重复。尽管损失似乎随着时间的推移而减少，但它对最终的奖励值和机器人行为影响不大。奖励看起来非常随机，尽管机器人获得了很高的奖励，但大多数情况下，情节在达到负奖励后都会以游戏结束结束。训练有素的机器人虽然似乎理解任务，找到物体，捡起它们，转身，到达基地，但通常只是随机旋转（当我将机器人的旋转角度呈现为余弦和正弦时，我以为我已经修复了它以避免 pi 突然更改为 -pi 的问题，但不是），并且不太注意最近的可见物体，可能只是开车到它们所在的区域。出现了以下问题： 1.我的奖励系统是否有问题？这个环境与我之前遇到的环境有些不同：在迭代之间，模拟中没有发生太多事情，并且算法可能很难看到路径之间的依赖关系以达到目标和任务的完成。因此，除了对拾取和交付物体给予奖励之外，我还尝试通过奖励朝正确方向移动的代理并惩罚其选择错误的方向来激励代理。这是一个好的解决方案吗？ 2.我应该以哪种方式更改超参数？我尝试过不同的层配置，较小的网络似乎处理得更差，但较大的网络也没有任何好处。批量大小看起来也合适。  超参数： 命名空间(seed=1626, eps_test=0.05, eps_train=0.1, buffer_size=20000, lr=0.0003, gamma= 0.9、n_step=3、target_update_freq=320、epoch=150、step_per_epoch=1500、step_per_collect=50、update_per_step=0.1、batch_size=64、hidden_​​sizes=[512、256、256、128]、training_num=10、test_num=10）  3.我使用的数据是否足够？网络接收机器人的位置和方向、到最近可见物体的方向向量、到底座的信息、是否在底座上、是否携带物体、是否发生了碰撞。我认为我不能在这里添加任何内容。 生成的图表：https://imgur.com/a/ WEg7ywi 感谢您的宝贵时间。   由   提交 /u/Legendorik   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckcy7p/looking_for_advice_on_using_dqn/</guid>
      <pubDate>Sat, 04 May 2024 22:58:49 GMT</pubDate>
    </item>
    <item>
      <title>大型非矢量化环境是否等同于小型矢量化环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ck32o2/are_large_nonvectorized_environments_equivalent/</link>
      <description><![CDATA[RL新手，刚刚接触到术语矢量化环境。 基于此SO回答我了解矢量化环境每一步接收多个环境。 矢量化环境是否只有助于加快训练速度（代理训练得更快）？或者它是否也有助于提高训练质量（矢量化训练可以带来更好的智能体）？ 这些训练环境在质量上是否相同？ 矢量化环境：每步 10 个环境，训练1,000 步。总共有 10,000 个环境。 非矢量化环境：每步 1 个环境，但训练有 10,000 个步骤。还有 10,000 总环境。 在这两种环境中训练的智能体大致相同吗？或者矢量化环境会带来更好的训练代理吗？   由   提交/u/mokenyon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ck32o2/are_large_nonvectorized_environments_equivalent/</guid>
      <pubDate>Sat, 04 May 2024 15:36:03 GMT</pubDate>
    </item>
    <item>
      <title>带 Gym 的自定义 MuJoCo 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cjxlh3/custom_mujoco_environment_with_gym/</link>
      <description><![CDATA[      您可以利用 Gymnasium 的所有优势在 MuJoCo 中训练您的自定义机器人。这是我的简单示例     提交人    /u/Repulsive_Air5342   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cjxlh3/custom_mujoco_environment_with_gym/</guid>
      <pubDate>Sat, 04 May 2024 10:55:07 GMT</pubDate>
    </item>
    <item>
      <title>项目提案帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cjwqaf/project_proposal_help/</link>
      <description><![CDATA[大家好 我精通 AI/ML、基于代理的建模、CFD/DEM、一般复杂系统，以及量子优化。  对于我的博士学位，我想开发一些类似复杂自适应模拟的东西，并以某种方式使用 MARL/基于代理的建模/量子优化/QML/QRL，然后这应该应用于价值链。  &gt; 帮我出点主意吧！干杯   由   提交 /u/Enough-Site-4309   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cjwqaf/project_proposal_help/</guid>
      <pubDate>Sat, 04 May 2024 09:54:53 GMT</pubDate>
    </item>
    <item>
      <title>项目提案帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cjwosy/project_proposal_help/</link>
      <description><![CDATA[大家好 我精通 AI/ML、基于代理的建模、CFD/DEM、一般复杂系统，以及量子优化。  对于我的博士学位，我想开发一些类似复杂自适应模拟的东西，并以某种方式使用 MARL/基于代理的建模/量子优化/QML/QRL，然后这应该应用于价值链。  &gt; 帮我出点主意吧！干杯   由   提交 /u/Enough-Site-4309   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cjwosy/project_proposal_help/</guid>
      <pubDate>Sat, 04 May 2024 09:51:49 GMT</pubDate>
    </item>
    <item>
      <title>克服知识障碍：通过预先训练的世界模型进行观察进行在线模仿学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cjqq5h/overcoming_knowledge_barriers_online_imitation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2404.18896 摘要：  结合计算机视觉和自然的预训练和微调的成功范例近年来，将语言处理纳入决策变得越来越流行。在本文中，我们通过预训练模型研究了观察模仿学习，并发现 BCO 和 AIME 等现有方法面临知识障碍，特别是具体知识障碍（EKB）和演示知识障碍（DKB），极大地限制了他们的表现。当预训练模型缺乏关于未见观察的知识时，EKB 就会出现，从而导致动作推理错误。 DKB 是基于有限示范的政策的结果，阻碍了对不同情况的适应。我们深入分析了这些障碍的根本机制，并提出了基于 AIME 的AIME-v2作为解决方案。 AIME-v2 使用与数据驱动的正则化器的在线交互来减轻 EKB，并通过引入替代奖励函数来增强策略训练来减轻 DKB。 DeepMind Control Suite 和 Meta-World 基准测试任务的实验结果证明了这些修改在提高样本效率和收敛性能方面的有效性。该研究为解决知识障碍以增强基于预训练的方法的决策提供了宝贵的见解。代码将在 此 https URL 处提供。    由   提交 /u/SurveySea7570   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cjqq5h/overcoming_knowledge_barriers_online_imitation/</guid>
      <pubDate>Sat, 04 May 2024 03:19:35 GMT</pubDate>
    </item>
    <item>
      <title>DIAMBRA Arena 中的新挑战：我们的环境阵容新增 3 个史诗级内容！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cj8299/new_challenges_in_diambra_arena_3_epic_additions/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cj8299/new_challenges_in_diambra_arena_3_epic_additions/</guid>
      <pubDate>Fri, 03 May 2024 12:59:07 GMT</pubDate>
    </item>
    <item>
      <title>人类在强化学习任务中表现如何？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cj4zlv/how_good_are_humans_in_rl_tasks/</link>
      <description><![CDATA[考虑传统的杆平衡任务。如果我们删除人类对任务的所有先验信息，哪一个会更好：人类还是计算机？ 因此，如果我们给人类两个按钮和四个输入（作为数字或颜色） ），我们没有告诉他们任务是什么，只是他们必须始终最大化第五个值（奖励），人类需要玩多少集才能找出一个好的策略？ 我的问题是，如果有关任务/目标的所有先验信息都被删除，人类可能会比好的强化学习算法更糟糕。有谁知道与此相关的任何研究吗？   由   提交/u/Ilmari86  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cj4zlv/how_good_are_humans_in_rl_tasks/</guid>
      <pubDate>Fri, 03 May 2024 10:07:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么我无法创建矢量化环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cixsw1/why_cant_i_create_a_vectorized_environment/</link>
      <description><![CDATA[我正在尝试使用以下代码创建矢量化环境 -  from stable_baselines3 import DQN from stable_baselines3.common .env_util import make_vec_env from stable_baselines3.common.vec_env import VecEnvWrapper from stable_baselines3.common.monitor import Monitor fromgym.wrappers import TransformReward importgym import numpy as np # 创建 Gym 环境 # 定义自定义奖励转换函数 defreward_transform(reward): return return *0.0 # 将奖励限制在 [-1, 1] 范围内 # 将 TransformReward 包装器应用到 Gym 环境 # 定义一个返回包装环境的函数 def make_wrapped_env(**env_kwargs): return TransformReward(gym.make( &quot;CartPole-v1&quot;, **env_kwargs),reward_transform) env_kwargs = {} env_kwargs[&quot;eval_env&quot;] = False # 使用包装的 Gym 环境的 4 个并行实例创建矢量化环境 vec_env = make_vec_env(make_wrapped_env, n_envs=4 , env_kwargs=env_kwargs) # 现在你可以使用 vec_env 来训练你的 RL 代理 # 创建并训练 DQN 模型 model = DQN(&quot;MlpPolicy&quot;, vec_env, verbose=1) model.learn(total_timesteps=100000, log_interval=4) model.save(“dqn_cartpole”)  但是，我收到以下错误 -  TypeError: __init__() 获得意外的关键字参数 &#39;eval_env &#39;  将“kwargs”添加到我的环境构造函数中会引发错误，这非常令人惊讶。 RLZoo 在这里做了同样的事情，但他们不这样做没有收到错误。   由   提交/u/Academic-Rent7800  /u/Academic-Rent7800  reddit.com/r/reinforcementlearning/comments/1cixsw1/why_cant_i_create_a_vectorized_environment/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cixsw1/why_cant_i_create_a_vectorized_environment/</guid>
      <pubDate>Fri, 03 May 2024 02:32:09 GMT</pubDate>
    </item>
    <item>
      <title>需要使用 RL 解决 3D Bin Packing 问题的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cifzuy/need_help_with_solving_3d_bin_packing_problem/</link>
      <description><![CDATA[大家好！昨天在工作中，我接到的任务是为我们的一位客户创建一种算法来处理 3D 装箱问题的修改版本。如果一切顺利，我将有一些时间来温习我的技能并制定解决方案。 事情是这样的：我们需要找出向送货卡车装载尽可能多货物的最佳方法包尽可能。每个包裹都带有几个关键细节：  与客户的距离（需要走多远）。 材料类型（由什么制成）。   li&gt;  从这些属性中可以看出三个限制  必须运输较远的包裹应包装在运输较短的包裹下面。 易碎包裹应包装在运输距离较短的包裹下面。坐在不太脆弱的上面。 解决方案必须在物理上稳定。  此外，我被要求使用强化学习来解决这个问题，因为我获得的知识将对未来的项目有用（我目前缺乏强化学习方面的知识，因此我将有大约一个月的时间在这方面进行自我培训）。 我一直在研究强化学习，以解决BPP。以下是高度总结的细节：  RL 最近在组合问题上取得了成功，比传统方法具有更好的性能。 RL 求解 BPP 的假设是这些框是先验未知的（在此问题中没有必要，因为所有框都是已知的）。  此 资源确实很有帮助。我认为我们可以在强化学习中利用这个关于盒子顺序的假设，例如我们可以先给智能体一系列非易碎盒子，然后再给智能体一系列易碎盒子，这样智能体就会把非易碎盒子放在底部。 你觉得怎么样？通过强化学习来解决这个问题是否可行，或者我应该向我的老板建议传统的启发式方法可能更合适？感谢您的任何意见！ 谢谢！   由   提交/u/dylannalex01  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cifzuy/need_help_with_solving_3d_bin_packing_problem/</guid>
      <pubDate>Thu, 02 May 2024 13:42:26 GMT</pubDate>
    </item>
    <item>
      <title>意外的观察形状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cia1gx/unexpected_observation_shape/</link>
      <description><![CDATA[我有一个 boid 集群环境。我制作了一个可以与 3 个 boids 配合使用的模型。我必须将 1D 数组动作 obs 重塑为 2D 才能通过，除此之外还可以。 现在我正在训练 10 个 boids。训练进行得很顺利，但是在测试时我收到了这个错误，尽管似乎没有什么问题。我也重塑它以传递 10 个操作。 ValueError: 错误: Box 环境出现意外的观察形状 (40,)，请使用 (12,) 或 (n_env, 12) 进行观察形状。 我在 OpenAI Gym 中使用 SB3 的 PPO。 动作和观察空间： min_action = np.array ([-5, -5] * len(self.agents), dtype=np.float32) max_action = np.array([5, 5] * len(self.agents), dtype=np.float32) self.action_space = space.Box(low=min_action, high=max_action, dtype=np.float32) min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32) max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32) self.observation_space = space.Box(low=min_obs , high=max_obs, dtype=np.float32)    由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cia1gx/unexpected_observation_shape/</guid>
      <pubDate>Thu, 02 May 2024 07:55:38 GMT</pubDate>
    </item>
    <item>
      <title>kanrl,用于强化学习的 Kolmogorov-Arnold 网络，初始实验,下载kanrl的源码_GitHub_帮酷</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cia0r3/github_riiswakanrl_kolmogorovarnold_network_for/</link>
      <description><![CDATA[    /u/riiswa   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cia0r3/github_riiswakanrl_kolmogorovarnold_network_for/</guid>
      <pubDate>Thu, 02 May 2024 07:54:15 GMT</pubDate>
    </item>
    <item>
      <title>如何将健身裹布与 SB3 结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/</link>
      <description><![CDATA[我正在尝试结合健身房包装器“TransformReward&quot;与SB3。这是我的代码 -  from stable_baselines3 import DQN from stable_baselines3.common.env_util import make_vec_env fromgym.wrappers import TransformReward defreward_transform(reward): # 奖励转换函数示例 returnreward * 0 # Scale将奖励加0以检查函数是否正常工作 # 创建向量化环境 env = make_vec_env(&quot;CartPole-v1&quot;, n_envs=1,wrapper_class=TransformReward,wrapper_kwargs={&quot;f&quot;:reward_transform}) # 创建并训练DQN 模型 model = DQN(&quot;MlpPolicy&quot;, env, verbose=1) model.learn(total_timesteps=100000, log_interval=4) model.save(&quot;dqn_cartpole&quot;)  As为了确保我的代码正常工作，我尝试将奖励缩放为 0。 但是，我看到代理正在学习并获得奖励 -  ``` &lt;前&gt;&lt;代码&gt;-------------------------------- |推出/ | | | ep_len_mean | 83.6 | 83.6 | ep_rew_mean | 83.6 | 83.6 |探索率 | 0.05 | 0.05 |时间/| | |剧集 | 3548 | 3548 |帧率 | 3401 | 3401 |已用时间 | 29 | 29 |总时间步数 | 99751 | |火车/ | | |学习率 | 0.0001 | 0.0001 |损失| 2.29e-06 | | n_更新 | 12437 | 12437 ----------------------------------  ``` 有人可以告诉我的代码有什么问题吗？谢谢！   由   提交/u/Academic-Rent7800  /u/Academic-Rent7800  reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/</guid>
      <pubDate>Wed, 01 May 2024 21:34:05 GMT</pubDate>
    </item>
    <item>
      <title>在我的计算机上设置 airSim 项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chvlkj/set_up_airsim_project_in_my_computer/</link>
      <description><![CDATA[大家好，我需要一些帮助。我一直在尝试在我的计算机上设置这个项目（它是一个准备使用的项目）并遵循提到的所有步骤，但我似乎无法管理它。当我到达“运行first.py以控制无人机”的步骤时，我找不到名为“first.py”的文件。我不确定这里缺少什么。如果已经尝试过的人可以帮助我，那将非常有帮助。 https://github。 com/lap98/RL-Drone-Stabilization   由   提交/u/gintooki_23  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chvlkj/set_up_airsim_project_in_my_computer/</guid>
      <pubDate>Wed, 01 May 2024 19:57:59 GMT</pubDate>
    </item>
    <item>
      <title>为什么这么多论文重新解释强化学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chq0w2/why_is_rl_reexplained_in_so_many_papers/</link>
      <description><![CDATA[我遇到的许多论文都花了第二章的整个开头重新解释 RL 的基础知识（状态/动作值函数、贝尔曼方程、q -学习）就好像这是最近的一些发现，没有人听说过，同时又如此快速地深入到如此多的细节，如果你不熟悉这些概念，你最好在其他地方学习它。 其他领域不会这样做，你不会看到每一篇物理论文都重新解释广义相对论是如何工作的。如果您正在阅读这篇论文，我们假设您对这个领域有些熟悉。 我很好奇，强化学习中是否存在这种情况的原因？我唯一能想到的是，如果每个人对强化学习的理解略有不同，他们会希望读者在同一页面上，但每次总是几乎相同。   由   提交/u/giorgiocav123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chq0w2/why_is_rl_reexplained_in_so_many_papers/</guid>
      <pubDate>Wed, 01 May 2024 16:11:44 GMT</pubDate>
    </item>
    </channel>
</rss>