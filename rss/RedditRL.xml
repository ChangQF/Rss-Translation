<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 03 Aug 2024 21:14:27 GMT</lastBuildDate>
    <item>
      <title>具有视觉输入的多模态模型玩口袋妖怪（尝试世界纪录）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej8rz0/multimodal_model_with_vision_input_plays_pokemon/</link>
      <description><![CDATA[       由    /u/nicimunty  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej8rz0/multimodal_model_with_vision_input_plays_pokemon/</guid>
      <pubDate>Sat, 03 Aug 2024 17:21:37 GMT</pubDate>
    </item>
    <item>
      <title>“NAVIX：使用 JAX 扩展 MiniGrid 环境”，Pignatelli 等人，2024 年（将代理 + 环境打包到 GPU 上以获得 OOM 收益）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej3upp/navix_scaling_minigrid_environments_with_jax/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej3upp/navix_scaling_minigrid_environments_with_jax/</guid>
      <pubDate>Sat, 03 Aug 2024 13:52:09 GMT</pubDate>
    </item>
    <item>
      <title>Efficient Zero V2 为何有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ej1m0s/why_does_efficient_zero_v2_work/</link>
      <description><![CDATA[ 如果价值函数知道更好的动作，它不会已经按照这种方式训练策略了吗？ 如果它不知道更好的动作，它不会错误地评价状态或动作，从而导致在蒙特卡罗树反向传播期间进行错误评估吗？     提交人    /u/Automatic-Web8429   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ej1m0s/why_does_efficient_zero_v2_work/</guid>
      <pubDate>Sat, 03 Aug 2024 11:57:25 GMT</pubDate>
    </item>
    <item>
      <title>一项新调查——离线策略学习的生成模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eipuq3/a_new_survey_generative_models_for_offline_policy/</link>
      <description><![CDATA[请查看我们新的 TMLR 工作：用于离线策略学习的深度生成模型。本文彻底回顾了深度生成模型在离线强化学习和模仿学习中的应用。我们涵盖的模型包括 VAE、GAN、规范化流、Transformers、扩散模型。    提交人    /u/Ashamed-Put-2344   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eipuq3/a_new_survey_generative_models_for_offline_policy/</guid>
      <pubDate>Sat, 03 Aug 2024 00:30:15 GMT</pubDate>
    </item>
    <item>
      <title>EWRL 的选择性如何</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</link>
      <description><![CDATA[大家好， 正如标题所说，你们知道 EWRL 研讨会在接受论文方面有多挑剔吗？总的来说，你觉得这个研讨会好吗？一些个人故事将不胜感激。    由   提交  /u/sel20   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eiko12/how_selective_is_ewrl/</guid>
      <pubDate>Fri, 02 Aug 2024 20:41:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Decision Transformer 在 OfflineRL 顺序决策领域有效？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</link>
      <description><![CDATA[谢谢。    提交人    /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eicb52/why_decision_transformer_works_in_offlinerl/</guid>
      <pubDate>Fri, 02 Aug 2024 15:02:21 GMT</pubDate>
    </item>
    <item>
      <title>如何在 EPyMARL 中加载和测试训练模型以进行 MAPPO 算法评估？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eib4td/how_to_load_and_test_a_trained_model_in_epymarl/</link>
      <description><![CDATA[      大家好， 我是强化学习 (RL) 的新手，最近开始使用 EPyMARL 框架进行多智能体强化学习 (MARL)。我已经在我的健身房的自定义环境中使用 MAPPO 和其他类似算法成功训练了一个模型。现在，我想加载这个经过训练的模型并在我使用的案例中评估其性能，但我不确定如何做到这一点。 这是我用于训练的命令： python3 src/main.py --config=mappo --env-config=gymma with env_args.key=&quot;LoRaEnv-v0&quot;  下面是我尝试用于评估的命令： python3 src/main.py --config=mappo --env-config=gymma with env_args.key=&quot;LoRaEnv-v0&quot; checkpoint_path=&quot;my_save_model_path&quot; assess=True  我不确定这是否正确，或者我是否需要采取其他步骤，因为我尝试运行脚本但遇到了一些奇怪的错误。有人可以指导我如何正确加载和测试我训练过的模型以使用 EPyMARL 进行评估吗？ 以下是包含我保存的模型的目录的屏幕截图： 如能提供任何帮助或指点，我们将不胜感激！提前致谢！ 训练模型目录    提交人    /u/Interesting_Dingo983   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eib4td/how_to_load_and_test_a_trained_model_in_epymarl/</guid>
      <pubDate>Fri, 02 Aug 2024 14:14:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么代理没有学会如何到达立方体的位置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</link>
      <description><![CDATA[        提交人    /u/CoolestSlave   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ei8ksh/why_does_the_agent_do_not_learn_to_get_to_the/</guid>
      <pubDate>Fri, 02 Aug 2024 12:17:29 GMT</pubDate>
    </item>
    <item>
      <title>CORL 和 D4RL 分数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</link>
      <description><![CDATA[我提前为一个可能微不足道的问题道歉： 在评估离线强化学习算法时，我们应该关心哪些分数？它们是 D4RL 标准化分数吗？还有其他需要考虑的吗？ 提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</guid>
      <pubDate>Thu, 01 Aug 2024 21:26:26 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型内部的 RL 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</link>
      <description><![CDATA[大家好。 我正在为纸牌游戏制作强化学习算法。我可以采取的可能动作取决于我手中的牌。如果我打出一张牌，我可以执行特定的操作。您可以假设这是一场 1v1 游戏。如果我的对手打出一张特定的牌，那么我必须决定给对手一张牌。 本质上，我希望有一个 RL 模型来决定打出哪张牌，然后另一个 RL 模型用于我必须给对手一张牌的特定情况。第一个 RL 模型将在每个回合激活，而第二个 RL 模型仅在我的对手打出迫使我给他一张自己的牌的特定牌时激活。 我想通过先与随机模型对战，然后再与自己对战来训练模型。 这可以在体育馆内完成吗？如果是这样，该怎么办？    提交人    /u/Practical-Resort7278   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</guid>
      <pubDate>Thu, 01 Aug 2024 12:30:12 GMT</pubDate>
    </item>
    <item>
      <title>真人秀 PS1 游戏 - 铁拳 3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</link>
      <description><![CDATA[我是 RL 领域的新手。我尝试过使用 atari 游戏的复古模拟作为 ENV 的 openai gym，但我想下一步。 我想在 PS1 上为格斗游戏 Tekken 3 实现 RL 模型。 据我所知，我必须使用 PS1 模拟器实现自己的交互层才能正确获取状态，例如健康、位置等。 我该如何正确处理？  尝试对 PS1 游戏进行逆向工程，并找出具有必要值的内存地址（通过 python 获取）。 使用 OpenCV 进行相同的检测，但从帧中  当涉及到创建自定义环境时，您将如何处理？    提交人    /u/Comprehensive_Cod331   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</guid>
      <pubDate>Thu, 01 Aug 2024 04:58:31 GMT</pubDate>
    </item>
    <item>
      <title>既然离线 RL 与环境无关，为什么很多论文实现仍然基于 gym？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</link>
      <description><![CDATA[谢谢。    由   提交  /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:33 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为不同的游戏选择不同的 RL 算法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</link>
      <description><![CDATA[我是荷兰 6 VWO 的一名 16 岁学生，目前正在参与一个关于强化学习 (RL) 在各种电脑游戏中的应用的学校研究项目。我的主要研究问题是：电脑游戏的具体特征如何影响不同 RL 算法的有效性？ 子问题：  不同类型的电脑游戏有哪些具体特征？ 有哪些 RL 算法可用，它们的特点是什么？ 游戏特征如何影响 RL 算法的性能？  实践部分：我计划将不同的 RL 算法应用于各种游戏。我正在考虑的游戏是：  超级马里奥兄弟 贪吃蛇 国际象棋 赛车  算法标准：  具有显著差异的算法。 最好是新算法。  反馈问题：  考虑到这些游戏的独特特点，您能否推荐适合这些游戏的特定 RL 算法？ 您认为我选择的游戏适合研究不同 RL 算法的有效性吗？如果不适合，您会建议什么游戏？     提交人    /u/matmoet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</guid>
      <pubDate>Wed, 31 Jul 2024 23:23:28 GMT</pubDate>
    </item>
    <item>
      <title>“彩虹团队：开放式生成多样化对抗提示”，Samvelyan 等人 2024 {FB}（用于质量多样性搜索的 MAP-Elites）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</guid>
      <pubDate>Wed, 31 Jul 2024 01:48:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 epsilon-greedy 算法的概率部分中包含贪婪动作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</link>
      <description><![CDATA[      新手在这里...我正在阅读 Sutton 和 Barto 关于强化学习的常年书籍。在他们对 epsilon-greedy 策略的描述中，他们认为在给定状态下选择一个动作应该以较小的 epsilon 概率发生；其余时间则选择贪婪动作。我理解这一点，因为你想鼓励探索，这反过来又允许人们满足先决条件，即在无限的时间轴下，最终将为给定状态选择所有动作，以收敛到接近最优的策略。 但是，我不明白即使在随机操作时也允许代理选择贪婪动作的原因。书中给出的这个公式就是一个例子，其中选择贪婪的概率是 1 减去选择任何动作（甚至是贪婪的动作）的概率 epsilon 加上随机选择贪婪动作的概率： https://preview.redd.it/vz26lao3vqfd1.png?width=142&amp;format=png&amp;auto=webp&amp;s=e3c7c0f6d48f1f349f9260f4a4e8897f438e2b42 当然，代理选择贪婪动作的次数会比选择其他非贪婪动作的次数多得多。因此，当整个重点是防止代理以小概率 epsilon 利用此操作时，为什么代理会在随机阶段选择最佳操作？ 谢谢， 一位充满激情的 RL 学习者。    提交人    /u/Soft-Establishment96   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</guid>
      <pubDate>Wed, 31 Jul 2024 00:02:51 GMT</pubDate>
    </item>
    </channel>
</rss>