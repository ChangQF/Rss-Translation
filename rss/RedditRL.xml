<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 01 Dec 2023 12:26:03 GMT</lastBuildDate>
    <item>
      <title>学习强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18880wg/learning_rl_research/</link>
      <description><![CDATA[大家好，我现在正在研究强化学习，我正在寻找加快学习速度的方法。我的计划是能够在该领域做好研究。现在我正在研究离线强化学习。  现在有帮助的是我们每周与我的博士同学进行一次会议。我们展示我们读过的新研究论文。轮到我的时候我会分享有关 RL 论文的内容。  但有一个问题是我们的主题非常多样化。一位致力于计算机视觉，一位致力于图神经网络，一位致力于时间序列预测。我是唯一一个从事强化学习的人。 如果我能与更多具有相同兴趣的人交谈，那就太好了。 为了补充背景信息，我来自菲律宾在这里很难获得支持或拥有专业知识的团体来指导我。当我感觉自己在学习过程中几乎是孤身一人时，就很难保持动力。虽然我在荷兰有一位教授给了我很多帮助。 您对在线小组等有什么建议吗？这可以帮助我更快地学习，包括潜在的指导、资源共享等？ 谢谢！   由   提交/u/111user222  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18880wg/learning_rl_research/</guid>
      <pubDate>Fri, 01 Dec 2023 09:44:48 GMT</pubDate>
    </item>
    <item>
      <title>“使用直接偏好优化 (DPO) 的扩散模型对齐”，Wallace 等人 2023 {Salesforce}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187wknu/diffusion_model_alignment_using_direct_preference/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187wknu/diffusion_model_alignment_using_direct_preference/</guid>
      <pubDate>Thu, 30 Nov 2023 23:27:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一周后我将采访 Rich Sutton，我应该问他什么问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187t5ir/d_im_interviewing_rich_sutton_in_a_week_what/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187t5ir/d_im_interviewing_rich_sutton_in_a_week_what/</guid>
      <pubDate>Thu, 30 Nov 2023 21:06:43 GMT</pubDate>
    </item>
    <item>
      <title>解决转型和奖励中的分配变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187nfvc/addressing_distributional_shifts_in_transitions/</link>
      <description><![CDATA[解决 RL 任务中的分布变化的最先进方法是什么？  我需要将强化学习应用于交通控制问题，其中交通分布（车辆到达率、出发地-目的地位置等）可能随时间变化。我确实可以在多个分布上进行训练，但我想使用一些方法，让我能够快速适应不同的分布，或者快速为其微调新模型。    由   提交 /u/fedetask   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187nfvc/addressing_distributional_shifts_in_transitions/</guid>
      <pubDate>Thu, 30 Nov 2023 17:04:33 GMT</pubDate>
    </item>
    <item>
      <title>创建跨多个环境运行的单个代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187mzev/creating_a_single_agent_that_acts_across_multiple/</link>
      <description><![CDATA[大家好，先感谢您的帮助！我正在开展一个 q-learning 项目，其中单个代理正在努力优化许多（数千台）独立机器的性能。所有机器都是相同的，并在定期更新的数据库上生成相同的指标。几年前，我读过这个社区的一个帖子，听起来好像可以在多个环境中应用单个代理。鉴于这些都是相同的环境并由唯一的 ID 表示，我想知道在构建模型时是否可以应用一个简单的函数分组来在更广泛的数据数组上进行训练，而不是之前在单个数据集上进行训练尝试将其部署到所有不同的环境中。再次感谢大家的帮助！   由   提交/u/Shikaze33_3   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187mzev/creating_a_single_agent_that_acts_across_multiple/</guid>
      <pubDate>Thu, 30 Nov 2023 16:45:03 GMT</pubDate>
    </item>
    <item>
      <title>在矢量基观测环境上实现 Dreamer v3 时出错</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187ktu6/error_implementation_dreamer_v3_on_vector_base/</link>
      <description><![CDATA[请问我做错了什么？我尝试在基于向量的观察的自定义环境中使用 Dreamerv3 训练我的代理。我按照 Sheaprl 方法添加自定义环境。  请问我做错了什么？我尝试在基于矢量的观察的自定义环境中使用 Dreamerv3 训练我的代理。我按照 Sheaprl 方法添加自定义环境。训练代理导航到其目标，同时避开障碍。  我想使用 Dreamer v3。我的观察空间是基于平面向量的观察（23），而不是图像，并且动作空间是连续的二维。我想编写一个使用 Dreamer v3 的脚本。如果可能，下面的脚本将仅适用于我的环境： - envs：我的环境 - configs.yaml - dreamer.py -exploration.py - models.py -networks.py -parallel.py -requirements.txt -tools.py 我想要一个特定的、干净的代码，以便我可以针对其他环境和基于图像的选项修改 Dreamer v3。这让我很难简化我想做的事情。我将分享我的环境和一个关于如何测试它的简单脚本。如果您能帮助我或指导我如何使用 Dreamer v3 和基于平面矢量的观察空间，我将不胜感激。谢谢！ 这是我的 zip 文件，其中包含我的环境： https://www.dropbox.com/scl/fi/z13wkqgjtuszlt3oimxd4/SacNav.zip?rlkey=f0y2yctxa5ajjt442na8abk4l&amp;dl=0   由   提交 /u/Goodluck_o   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187ktu6/error_implementation_dreamer_v3_on_vector_base/</guid>
      <pubDate>Thu, 30 Nov 2023 15:13:09 GMT</pubDate>
    </item>
    <item>
      <title>学习资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187kpy0/learning_sources/</link>
      <description><![CDATA[您好，我想听听你们是否对我如何继续我的学习之旅有任何建议。 到目前为止我相信我对神经网络的工作原理有深入的了解，包括它的数学原理和编码。我成功地在一个简单的环境中实现了一个普通的梯度下降算法。现在我正在考虑如何继续前进。我喜欢从数学角度理解事物，然后尝试用 numpy 自己实现它，以确保我理解一切。但最重要的是，我想开始学习使用现有的RL 和 NNS 的实用工具，例如 pytorch 或其他工具。 你们对解决理论和实践问题的资源有什么建议吗？ 顺便说一句，我在这里问是因为我对深度 RL 特别感兴趣虽然它看起来很一般。   由   提交 /u/urtropicretarded   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187kpy0/learning_sources/</guid>
      <pubDate>Thu, 30 Nov 2023 15:08:17 GMT</pubDate>
    </item>
    <item>
      <title>轻松实施并行训练。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187imzd/easily_implement_parallel_training/</link>
      <description><![CDATA[    /u/NoteDancing   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187imzd/easily_implement_parallel_training/</guid>
      <pubDate>Thu, 30 Nov 2023 13:33:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么我应该使用 RLLib 而不是 stable-baselines3？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187fahg/why_should_i_use_rllib_over_stablebaselines3/</link>
      <description><![CDATA[所以我已经使用 sb3 大约 3 年了，我发现设置实验、向 RL 添加功能修改/自定义网络架构非常简单算法，使用Optuna优化参数，并在训练后进行推理。除了了解其工作原理的基本动态之外，所有这些都不需要付出什么努力。  所以在我的新公司中，他们使用 RLLib，至少可以说，它是一个怪物，可以完成我在 sb3 中轻松完成的上述任何事情。 所以争论是通常认为使用 RLLib 部署模型很容易。但这真的那么容易吗？考虑到我们可以通过 FAST api 和 docker 组合轻松甚至更快地部署使用 sb3 训练的模型，是否值得仅仅为了它提供的额外算法而经历 RLLib 的困难？对于业内使用强化学习的人来说，您的体验如何？   由   提交/u/sharafath28  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187fahg/why_should_i_use_rllib_over_stablebaselines3/</guid>
      <pubDate>Thu, 30 Nov 2023 10:14:33 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么游戏可以让我们通过python与之交互呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187e4jj/is_there_any_game_that_allow_us_to_interact_with/</link>
      <description><![CDATA[我正在学习 RL“强化学习”，并且我已经完成了一些大大小小的项目来掌握它的窍门。 我写了一个代码（训练了一个 RL 模型），通过捕获屏幕（mss）和发送输入（PyDirectInputs）来玩游戏，但它太慢了；大概只有 1 fps。（无论是在训练中，还是在使用中） 所以我想知道是否有任何游戏可以让我们通过 python 与其交互？像一些 API、接口、软件等将我们的代码直接连接到游戏。或者类似的东西？   由   提交 /u/Mr_Lucifer_666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187e4jj/is_there_any_game_that_allow_us_to_interact_with/</guid>
      <pubDate>Thu, 30 Nov 2023 08:53:39 GMT</pubDate>
    </item>
    <item>
      <title>我想使用 Dreamerv3 (pytorch) 来训练我的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1870q9h/i_would_like_to_use_dreamerv3_pytorch_to_train_my/</link>
      <description><![CDATA[我想使用 Dreamerv3 (pytorch) 来训练我的环境。我的自定义环境是基于矢量的健身房环境。 import numpy as npimport os, sys, globimportgymfrom hparams import HyperParams as hpfrom hparams import RobotFrame_Continously_Datasets_Timestep_1 as data# from hparams import RobotFrame_Continously_Datasets_Timestep_3 as data# from hparams import RobotFrameContinouslyDatasetsTimestep_0_25 as data# from hparams import RobotFrameContinouslyDataset sTimestep_0_5 as dataimport syssys.path.append(&#39;./gsoc22-socnavenv&#39;)import randomimport socnavevimport osimport torchdef离散_to_连续_action(action:int):&quot;&quot;&quot;为给定的离散动作返回连续空间动作的函数&quot;&quot;&quot; ;# 将可能的操作值调整为 -0.5, 0, 0.5 或 1move_dict = {0: -0.5, 1: 0, 2: 0.5, 3: 1}turn_dict = {0: -0.5, 1: 0, 2 : 0.5, 3: 1}move = move_dict[action // 4]turn =turn_dict[action % 4]return np.array([move, Turn], dtype=np.float32)def preprocess_observation(obs):&quot;&quot; ;“将 dict 观察转换为 numpy 观察”“”assert(type(obs) == dict)obs2 = np.array(obs[“goal”][-2:], dtype=np.float32)人类 = obs[“人类”].flatten()for i in range(int(round( humans.shape[0]/(6+7)))):index = i*(6+7)obs2 = np .concatenate((obs2, humans[index+6:index+6+7]) )tables = obs[“tables”].flatten()for i in range(int(round(tables.shape[0]/()) 6+7)))):index = i*(6+7)obs2 = np.concatenate((obs2,tables[index+6:index+6+7]))plants = obs[“plants”]。展平（）for i in range（int（round（plants.shape [0] /（6 + 7））））：index = i *（6 + 7）obs2 = np.concatenate（（obs2，plants [index + 6:index+6+7]) )return torch.from_numpy(obs2)device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)def rollout():time_steps = data.time_stepsenv = gym.make(“SocNavEnv-v1”)env.configure(&#39;./configs/env_timestep_03.yaml&#39;)env.set_padded_observations(True)max_ep = 50_000feat_dir = data.data_diros.makedirs(feat_dir,exist_ok=True)env.seed (1) # 演示的确定性for ep in range(max_ep):obs_lst, action_lst,reward_lst, next_obs_lst, did_lst = [], [], [], [], []obs = env.reset()obs = preprocess_observation(obs )done = Falset = 0for t in range(time_steps+10):# env.render()# action_ = np.random.randint(0, 64) # 从 4 更新到 64，以匹配新的操作空间# action =离散_to_连续_action(action_)# action = np.round(action, 小数=2)action_ = np.random.randint(0, 16) # 从 4 更新到 16，以匹配新的操作空间# action_ = 9 # 选择一个初始值用于演示目的的操作action =离散_to_连续_action(action_)#action = np.round(action, 小数=2)# print(action)next_obs,reward,done,_ = env.step(action)#print(next_obs)next_obs = preprocess_observation( next_obs)action = torch.from_numpy(action)# print(next_obs)#np.savez(os.path.join(feat_dir, &#39;rollout_{:03d}_{:04d}&#39;.format(ep,t)),obs =obs,action=action,reward=reward,next_obs=next_obs,done=done,)obs_lst.append(obs)action_lst.append(action)reward_lst.append(reward)next_obs_lst.append(next_obs)done_lst.append(完成) obs = next_obsif did:print(&quot;剧集 [{}/{}] 在 {} 个时间步长后完成&quot;.format(ep + 1, max_ep, t),lush=True)obs = env.reset()obs_lst = torch. stack(obs_lst, dim=0).squeeze(1)next_obs_lst = torch.stack(next_obs_lst, dim=0).squeeze(1)done_lst = [int(d) for d in did_lst]done_lst = torch.tensor(done_lst) .unsqueeze(-1)action_lst = torch.stack(action_lst, dim=0).squeeze(1)reward_lst = torch.tensor(reward_lst).unsqueeze(-1)breaknp.savez(os.path.join(feat_dir, &#39; rollout_ep_{:03d}&#39;.format(ep)),obs=np.stack(obs_lst, axis=0), # (T, C, H, W)action=np.stack(action_lst, axis=0), # (T, a)reward=np.stack(reward_lst, axis=0), # (T, 1)next_obs=np.stack(next_obs_lst, axis=0), # (T, C, H, W)done=np .stack(done_lst, axis=0), # (T, 1))if __name__ == &#39;__main__&#39;:np.random.seed(123)rollout() 这是我的环境：我是基于平面向量的观察 (23) 且动作空间为 2。   由   提交 /u/Goodluck_o   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1870q9h/i_would_like_to_use_dreamerv3_pytorch_to_train_my/</guid>
      <pubDate>Wed, 29 Nov 2023 21:34:12 GMT</pubDate>
    </item>
    <item>
      <title>[需要帮助] 大学项目：带有传感器的孔寻路槽板 - 寻求建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186yi3z/help_needed_project_for_university_path_finding/</link>
      <description><![CDATA[      你好r/reinforcementlearning 社区， 对于我们的产品开发硕士，我们应该实施和构建一个基于强化学习的系统（具有真实的组件）。然而，我们都没有对强化学习有深刻的理解，所以到目前为止我们还没有能够成功地训练，不幸的是我们的教授无法再帮助我们，并鼓励我们寻求外部帮助。也许这里有人愿意接受挑战:)我们很高兴收到关于我们的代码的新想法或意见以及如何进一步进行。 项目概述： 代理应该到达棋盘顶部（我们喜欢称之为奶酪板），而不会掉入任何洞中。为了支持这项任务，他在其上方的左右对角位置有两个传感器，它们返回两个传感器位置之一是否有孔 代码片段： 训练和测试文件，以及系统的更多示例和我们这边的最新结果可以在我的 OneDrive 文件夹中访问：https://drive.google.com/drive/folders/1hP5XiLkf9auZncZ7xyvmv0bChadbie1N?usp=sharing在此文件夹中，您还可以找到更多见解来可视化我们的项目。&lt; /p&gt; 我们尝试过的： 我们尝试了我们想到的一切：- 不同的奖励函数- 不同的行动空间（省略向下运动）- 不同的强化学习参数（alpha、gamma、epsilon、epsilon 衰减等）- 在各种矩阵上训练的不同方法 总而言之，几周来我们尝试了所有想到的方法，但收效甚微。不过，从理论上讲，他只需要学习，例如，一旦右侧传感器检测到有洞，就稍微向左移动，否则继续向上移动。 具体问题：   有人在训练金融交易强化学习代理时遇到过类似的问题吗？ 对这种情况下奖励函数的潜在问题有什么见解吗？&lt; /li&gt;  我们非常感谢您的帮助，并期待您的想法:) 我们的问题示例，代理应该到达顶部并避开洞   由   提交/u/Flat_Chipmunk_9188   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186yi3z/help_needed_project_for_university_path_finding/</guid>
      <pubDate>Wed, 29 Nov 2023 19:58:10 GMT</pubDate>
    </item>
    <item>
      <title>将多处理与 CUDA 相结合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/</link>
      <description><![CDATA[我是高性能计算方面的新手。因此，如果我的问题看起来太微不足道，请原谅我。 ​ 我正在考虑将多重处理应用于 CUDA RL 程序。现在我正在尝试训练一个需要大约 9 小时训练的环境。我想尝试一下超参数和其他一些东西。因此，我想使用相同的 GPU 并行运行我的模型 10 次。  这是我在谷歌搜索后要处理的 -   GPU 大小 - 我知道我的 GPU 应该足够大，可以接收 10 个数据运行。  GPU 年份 - 显然较旧的 GPU 不允许多个程序访问它。   我还缺少什么吗？我的假设是，如果运行单个模型需要 9 小时，那么通过多重处理，只要满足上述约束，我就可以在 9 小时内运行 10 个模型。   由   提交 /u/Academic-Rent7800    reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/</guid>
      <pubDate>Wed, 29 Nov 2023 19:06:53 GMT</pubDate>
    </item>
    <item>
      <title>Rankitect：在元规模上与世界一流工程师对战的架构搜索排名</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.08430 摘要：  神经架构搜索 (NAS) 已经证明了其在计算机视觉方面的功效和排名系统的潜力。然而，之前的工作主要集中在学术问题上，这些问题是在良好控制的固定基线下进行小规模评估的。在行业系统中，例如 Meta 中的排名系统，尚不清楚文献中的 NAS 算法是否能够超越生产基线，因为：（1）规模 - Meta 排名系统为数十亿用户服务，（2）强大的基线 - 基线是生产自深度学习兴起以来，多年来数百到数千名世界级工程师优化了模型，(3) 动态基线 - 工程师可能在 NAS 搜索过程中建立了新的、更强的基线，(4) 效率 - 搜索管道必须产生结果快速与生产生命周期保持一致。在本文中，我们介绍了 Rankitect，这是一个用于 Meta 排名系统的 NAS 软件框架。 Rankitect 寻求通过从头开始构建低级构建块来构建全新的架构。 Rankitect 实现并改进了最先进 (SOTA) NAS 方法，以在同一搜索空间下进行全面、公平的比较，包括基于采样的 NAS、一次性 NAS 和可微分 NAS (DNAS)。我们通过与 Meta 上的多个生产排名模型进行比较来评估 Rankitect。我们发现 Rankitect 可以从头开始发现新模型，实现归一化熵损失和 FLOP 之间的竞争性权衡。当利用工程师设计的搜索空间时，Rankitect 可以生成比工程师更好的模型，实现积极的离线评估和 Meta 规模的在线 A/B 测试。  https://preview.redd.it/mlceky7x7b3c1.png?width=1379&amp;format=png&amp;auto =webp&amp;s=2acc4e0451db9fbf455b03f9e293e68cc61d25bf   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</guid>
      <pubDate>Wed, 29 Nov 2023 16:00:25 GMT</pubDate>
    </item>
    <item>
      <title>我的 Gym Cartpole 代理学习的运行平均值约为 200，但当我使用相同的 q 表进行游戏时，它仅运行 10 分。请帮忙。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ms1a/my_gym_cartpole_agent_learns_with_a_running_mean/</link>
      <description><![CDATA[gridworld/cartpole.ipynb at main · bherwanisuraj/gridworld (github.com)  这是我的 git 存储库的链接。如果有人能帮忙那就最好了。    由   提交 /u/tlevelup   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ms1a/my_gym_cartpole_agent_learns_with_a_running_mean/</guid>
      <pubDate>Wed, 29 Nov 2023 10:57:01 GMT</pubDate>
    </item>
    </channel>
</rss>