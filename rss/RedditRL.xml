<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 10 Feb 2025 01:17:19 GMT</lastBuildDate>
    <item>
      <title>稳定的基线3-在Model.Learn（）之外学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ilpeu9/stable_baselines3_learn_outside_of_modellearn/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有一个项目，我想将强化学习集成到解决导航的较大算法中。例如，RL机器人将学习如何在骑自行车（或其他控制助攻）上取得平衡并向前迈进，而有一种算法指定了哪些街道要达到目标。对于这个项目，即使在a* sessions期间，我也想对代理人进行挑战 - 这些会议的奖励更新政策。是否有一种简单的方法如何指定学习参数并更新 model.learn（） 之外的策略权重？如果不是，我需要编写和测试自定义PPO，这会减慢过程....  感谢所有答复，  michal    &lt; ！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/majklost21     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ilpeu9/stable_baselines3_learn_outside_of_modellearn/</guid>
      <pubDate>Sun, 09 Feb 2025 21:14:45 GMT</pubDate>
    </item>
    <item>
      <title>安全奖励和安全/约束RL之间有什么不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ildbz0/whats_so_different_between_rl_with_safety_rewards/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  安全/约束RL的目标是在保证安全探索或通过将约束率限制在某些阈值以下的约束率来确保限制。  ，但我想知道这与普通RL有何不同，如果违反安全性限制，则带来了一些奖励功能，从而给予负奖励。是什么使安全/约束的RL如此特别和/或不同？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/open-safety-1585     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ildbz0/1ildbz0/whats_so_so_different_between_rl_with_safety_reward/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ildbz0/whats_so_different_between_rl_with_safety_rewards/</guid>
      <pubDate>Sun, 09 Feb 2025 12:06:55 GMT</pubDate>
    </item>
    <item>
      <title>我可以将批量分成A2C的迷你批次</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il9odx/can_i_split_my_batch_into_mini_batches_for_a2c/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  优势演员评论家是一种policy RL算法，这意味着网络仅通过当前策略生成的经验来更新。 话虽如此，我知道我不能使用重播缓冲区来使算法更有效，我只能使用最新的体验来更新网络。 现在，假设我生成了一个最新政策的1000个样本。我应该在整个批次上运行梯度下降，计算梯度并进行一次更新，还是可以将批次分成10个较小的迷你批次并更新网络10次？这种最后的方法会违反“ policy”假设？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nigage-owl-4088      link]     &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/comments/1il9odx/can_i_split_my_my_my_mini_into_mini_mini_mini_mini_batches_for_a2c/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il9odx/can_i_split_my_batch_into_mini_batches_for_a2c/</guid>
      <pubDate>Sun, 09 Feb 2025 07:44:51 GMT</pubDate>
    </item>
    <item>
      <title>训练时的模拟时间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il9jeo/simulation_time_when_training/</link>
      <description><![CDATA[在优化物理模拟，但是物理模拟本身需要1分钟才能运行。如果我需要100万个步骤才能融合，则每个步骤可能需要2分钟。这是并行化，没有。这根本不可行，如何处理？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/milkyjuggernuts     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il9jeo/simulation_time_when_training/</guid>
      <pubDate>Sun, 09 Feb 2025 07:34:41 GMT</pubDate>
    </item>
    <item>
      <title>如何实现这一目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il8vg2/how_to_make_this_happen/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在ML中做了一个项目，这与主动学习有关。我现在想做更多的事情，我正在寻找项目，但是我也想做一些事情，我计划在网络上制作一个Wordle Clone，然后制作RL模型来播放它，但是如何继续前进去做这？欢迎任何资源和建议  tl; new to Ml，想制作一个文字克隆并训练RL模型来播放它，要求资源和建议。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/darklord-0708     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il8vg2/how_to_make_this_happen/</guid>
      <pubDate>Sun, 09 Feb 2025 06:48:14 GMT</pubDate>
    </item>
    <item>
      <title>PPO问题：政策损失和价值功能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il6erp/ppo_question_policy_loss_and_value_function/</link>
      <description><![CDATA[           &lt;！ - &gt;  嗨all， 我试图首次使用简单的蒙特卡洛估计器来实现PPO。我来自实施SAC和DQN。我在理解如何最大化策略更新的同时最大程度地减少价值功能损失时遇到了问题。我的优势本质上是g_t -v（s），策略神经网旨在最大化新政策与旧政策的比率乘以这一优势。另一方面，我的价值功能神经网旨在最大程度地减少相同的优势。显然，我在这里的某个地方有误解，因为我不应该尝试最大程度地减少和最大化相同的功能，因此我的实现没有学习任何东西。 我的损失功能如下： &lt; pre&gt; ＃计算优势= mc_returns -self.critic（states）.detach（）＃计算策略损失new_log_probs = self.policy.policy.get_get_log_prob（state，actions，actions）比率=（new_log_probs -old_log_log_probs -old_log_probs） =比率 *优势polition_loss2 = torch.clamp（比率，1- self.epsilon，1 + self.epsilon） *优势polition_loss = -torch.min（polican_loss1，polition_loss2，polition_loss2）.mean（）.mean（） .critic（dates）-mc_returns）** 2）.mean（）＃计算优势= mc_returns -self.critic（states）.detach（date）.detach（）＃计算策略损失new_log_probs = self.policy.policy.policy.get_log_prob（states，antates，anction）比例=（new_log_probs -old_log_probs）.exp（）polition_loss1 =比率 *优势polition_loss2 = torch.clamp（比率，1- self.epsilon，1 + self.epsilon） （）＃计算值损失值_loss =（（self.critic（states）-mc_returns）** 2）.mean（）   这是我如何计算蒙特卡洛（ISH）返回的方式。我在固定的时间步长或情节结束后计算回报在范围内（1，len（奖励） + 1）：如果i == 1且未完成：curr_return =奖励 + self.gamma*评论家（next_state）.detach（）else：curr_return = redward = rewards [-i] + self。 gamma*curr_return mc_returns [-i] = curr_return curr_return = 0在范围内（1，len（rewards） + 1）：如果i == 1且未完成：curr_return = reward = reward + self.gamma*crialted.gamma*评论家（next_state）。 distach（）else：curr_return = rewards [-i] + self.gamma*curr_return mc_returns [-i] = curr_return    如果有人可以帮助澄清我在这里缺少的东西感谢！ 编辑：新优势我正在使用：  https://preview.redd.it/zt0l6wpu67ie1.png?width=282&amp;format=png&amp;auto=webp&amp;s=51d7d33d96faf376c8e981c489b577a4033cbc1e  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lostbandard     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il6erp/ppo_question_policy_loss_and_value_function/</guid>
      <pubDate>Sun, 09 Feb 2025 04:15:21 GMT</pubDate>
    </item>
    <item>
      <title>“关于语言模型蒸馏的老师黑客攻击”，Tiapkin等人2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il2g2j/on_teacher_hacking_in_language_model_distillation/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il2g2j/on_teacher_hacking_in_language_model_distillation/</guid>
      <pubDate>Sun, 09 Feb 2025 00:45:44 GMT</pubDate>
    </item>
    <item>
      <title>增强学习控制跟踪轨迹轨迹6-DOF Quadcopter无人机</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ikvnbb/reinforcerment_learning_control_tracking/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ikvnbb/reinforcerment_learning_control_tracking/</guid>
      <pubDate>Sat, 08 Feb 2025 19:38:32 GMT</pubDate>
    </item>
    <item>
      <title>四轮摩托车无人机的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ikrw11/reinforcement_learning_for_quadcopter_uav/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ikrw11/reinforcement_learning_for_quadcopter_uav/</guid>
      <pubDate>Sat, 08 Feb 2025 17:01:21 GMT</pubDate>
    </item>
    <item>
      <title>“平行Q-学习（PQL）：在大规模平行模拟下缩放非政策的增强学习”，Li et al 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ikpy13/parallel_qlearning_pql_scaling_offpolicy/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ikpy13/parallel_qlearning_pql_scaling_offpolicy/</guid>
      <pubDate>Sat, 08 Feb 2025 15:37:30 GMT</pubDate>
    </item>
    <item>
      <title>RLHF实验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ikk6k2/rlhf_experiments/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  当前RLHF与LLM有关吗？我有兴趣在此域中进行一些实验，但对LLM不感兴趣（不是第一个至少一个）。因此，我正在考虑在Openai Gym环境中要做的事情，并以某些启发式充当人类。 Christiano等。 al。 （2017年）他们在Atari和Mujoco环境上进行了实验，但它早在2017年。如果不触及LLM？   &lt;！ - 在RLHF中发表研究的机会是否非常低。 -SC_ON-&gt;＆＃32;提交由＆＃32; /u/u/wayown2610     [link]  ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ikk6k2/rlhf_experiments/</guid>
      <pubDate>Sat, 08 Feb 2025 10:10:36 GMT</pubDate>
    </item>
    <item>
      <title>🚀训练四倍的增强学习：从零到英雄！ 🦾</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ik7dhn/training_quadrupeds_with_reinforcement_learning/</link>
      <description><![CDATA[    内部有什么？奖励功能&lt; /strong&gt;用于有效学习✅训练运动策略在模拟中使用 ppo &lt; /strong&gt;（以撒健身房，Mujoco等）。 /strong&gt;对现实世界部署的挑战 灵感来自 Genesis 的作品以及基于RL的机器人控制的进步，我们的教程为训练四足动物提供了结构化方法，无论您是&#39;是研究人员，工程师或爱好者。 一切都是 open-access   - 没有付费墙，只是纯RL知识！ 🚀  文章： 使四倍体学会走路&lt; /a&gt;  代码：  github repo     很想听听您的反馈并讨论机器人运动的RL策略！ 🙌   https://reddit.com/link./link./link/link/link/1ik7dhn/video/arizr9gikshe1/player &lt; /a&gt;   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/federicosarocco     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ik7dhn/training_quadrupeds_with_reinforcement_learning/</guid>
      <pubDate>Fri, 07 Feb 2025 22:16:46 GMT</pubDate>
    </item>
    <item>
      <title>“基于价值的深度RL量表可以预见”，Rybkin等人2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ik3274/valuebased_deep_rl_scales_predictably_rybkin_et/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ik3274/valuebaseed_deep_rl_scal_scales_predictaliquality_rybkin_et/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ik3274/valuebased_deep_rl_scales_predictably_rybkin_et/</guid>
      <pubDate>Fri, 07 Feb 2025 19:14:27 GMT</pubDate>
    </item>
    <item>
      <title>Schnell等人“时间差异学习：为什么会变得快速，如何更快”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijxavb/temporal_difference_learning_why_it_can_be_fast/</link>
      <description><![CDATA[    [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijxavb/temporal_difference_learning_why_it_can_be_fast/</guid>
      <pubDate>Fri, 07 Feb 2025 15:17:07 GMT</pubDate>
    </item>
    <item>
      <title>我们的RL框架将用于快速，进化HPO的任何网络/算法转换。我们应该使LLM可用于进化RL推理培训吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijr36h/our_rl_framework_converts_any_networkalgorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家，我们刚刚发布了agilerl v2.0！ 查看最新更新： https://github.com/agilerl/agilerl     agilerl是一个RL培训库，可实现任何网络的进化超参数优化。我们的基准测试比rllib的训练更快10倍。 这是我们添加的一些很酷的功能：  广义突变 - 一个完全模块化的，灵活的网络和RL的柔性突变框架超参数。  Evolvablenetwork API  - 在可演化的设置中使用任何网络架构，包括预验证的网络。  evolvableAlgorithm层次结构 - 简化的进化RL算法的实现。 EvolvableModule层次结构 - 跟踪复杂网络中突变的更聪明的方法。 支持复杂空间 - 使用EvolvableMultiinput无缝处理多输入空间。  知道是：我们应该将其完全扩展到LLM吗？当前大型型号的HPO并不是真正的，因为它们是如此难/昂贵。但是我们的框架可以使其更加有效。我已经意识到人们比较了用于在DeepSeek R0娱乐活动中获得更好结果的超参数的人，这意味着这可能很有用。我想知道您对进化HPO是否对培训大型推理模型有用的想法？而且，如果有人幻想有助于为这项工作做出贡献，我们会很喜欢您的帮助！谢谢   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nicku_a     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijr36h/our_rl_framework_converts_any_networkalgorithm/</guid>
      <pubDate>Fri, 07 Feb 2025 09:21:43 GMT</pubDate>
    </item>
    </channel>
</rss>