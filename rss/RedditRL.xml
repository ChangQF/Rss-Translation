<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 19 Apr 2024 15:14:14 GMT</lastBuildDate>
    <item>
      <title>强化学习可以用来解决这个具有挑战性的优化问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7z1m4/could_rl_be_used_to_solve_this_challenging/</link>
      <description><![CDATA[大家好， 我正在处理工作中的挑战，涉及包含人员信息的大型数据集。每行包含一个计数和各种维度（如性别、城市、年龄）。我们需要对计数低于特定阈值（例如小于 5）的条目进行匿名化，作为保护隐私的初步步骤。 数据集还包含每个维度的聚合数字。这意味着如果一个维度中只有一行被匿名，则可以推导出模糊值。为了防止这种情况，我们在同一维度上模糊另一个值，这反过来可能需要在其他维度上进一步模糊。这种迭代模糊通常会导致过度匿名化，这并不是最佳的。现在，这是由一组高级 SQL 查询执行的自动化过程。但是，当有多个选项时，选择模糊哪个值是一个随机选择。 Colud 强化学习（RL）可以用来优化这个过程吗？  数据集的大小不固定；它的行数各不相同，从而创建了可变的观察或状态空间。 数据集的复杂性也是可变的，在匿名化过程中需要考虑不同的维数。 匿名化的阈值很灵活，通常设置为最小值（例如 5），但可以根据具体的隐私要求进行调整。  是否还应考虑其他方法？对于优化方法有什么想法吗？   由   提交/u/Purple_Investment_97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7z1m4/could_rl_be_used_to_solve_this_challenging/</guid>
      <pubDate>Fri, 19 Apr 2024 15:01:11 GMT</pubDate>
    </item>
    <item>
      <title>动态定位的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7y4ct/reinforcement_learning_for_dynamic_positionning/</link>
      <description><![CDATA[你好！ 我目前正在开发一个模型来解决无人机的动态定位问题。  我处于早期阶段，所以没有动态定位，哈哈。我只是想解决一个问题，即具有一组静态用户的无人机根据到这些点的距离给予奖励（将来，我不会知道这些点的位置，并且会有是运动，所以它不是那么微不足道）。  我正在解决这个问题，以熟悉 DQN 和 DRL 的内容。但我遇到了一些问题：  状态空间太大；状态由 (x,y) 定义，位置是整数，因此它是一个相当充足的状态空间，因为它没有约束。在经典的强化学习算法中，这实际上是不可能解决的（据我所知），但由于内部有一个神经网络，我不知道它如何“压缩”的结果。的信息。  动态动作空间。定位开始时，移动量足够大，可以快速到达最佳点，但当接近最佳点时，移动步长应减小。这是另一个问题，我需要弄清楚它是如何工作的。 未来的扩展：只有一架无人机，但将来会有几架，并且这也很难解决，也许是多臂强盗或类似的东西。   目前，当我进行训练时，我已经相当成功地解决了这个问题，其中每次执行时无人机的起点都是相同的，但是当我随机化它时初始位置，我没有取得成功。在我的问题中，没有明确的“结局”。执行的过程，并且在大多数教程中，都有一个明确的结局。 对于这个新手问题，我很抱歉，但我很困惑。  谢谢！！   由   提交 /u/RikoteMasterrrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7y4ct/reinforcement_learning_for_dynamic_positionning/</guid>
      <pubDate>Fri, 19 Apr 2024 14:21:59 GMT</pubDate>
    </item>
    <item>
      <title>具有集中批评者的多代理 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7y2hy/multiagent_ppo_with_centralized_critic/</link>
      <description><![CDATA[我想制作一个具有集中训练和分散评估的 PPO 版本，用于使用 PPO 的合作（共同奖励）多代理设置。  对于 PPO 实现，我遵循了此存储库（https://github.com/ericyangyu/PPO -for-Beginners），然后根据我的需要进行了一些调整。问题是我发现自己目前陷入了如何处理实现的某些部分的困境。 我知道集中批评家将输入所有代理的组合状态空间，然后输出一般状态值数。问题是我不明白这如何在 PPO 的推出（学习）阶段发挥作用。特别是我不明白以下事情：  我们如何计算批评者损失？由于在多智能体 PPO 中，它应该由每个智能体单独计算 我们如何在智能体的学习阶段查询批评家网络？由于现在每个代理（具有分散的批评家）都有一个比批评家网络小得多的观察空间（因为它具有所有观察空间的总和）  提前感谢您帮助！   由   提交/u/blrigo99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7y2hy/multiagent_ppo_with_centralized_critic/</guid>
      <pubDate>Fri, 19 Apr 2024 14:19:52 GMT</pubDate>
    </item>
    <item>
      <title>q 表的重新训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7xtlz/retraining_of_qtables/</link>
      <description><![CDATA[所以我希望这是提出这个问题的正确位置。  我目前正在写我的硕士论文，我想研究机器人“个性”/“偏好”对共同学习的影响（对于某些人来说，这是人类和机器人之间的学习过程）共同任务）。  由于学习需要在现实生活中进行，因此强化学习算法的选择仅限于非常简单的 Q-learning 算法。我计划嵌入的首选项与对象切换的位置和方向相关。因为这些直接由状态表示，所以偏好的嵌入是通过使用适当的奖励函数对 q 表进行离线训练来实现的。 （所以最后我们有一个 q 表，其中填充了已解决环境的值） 现在我遇到的问题是，一旦嵌入这些首选项，它们就需要能够重新学习，而不是如果任务目标失败，请尽快（这是共同学习的要求）。  现在我的问题是： - 是否有文献讨论 q 表的重新学习？ - 是否有文献讨论如何在线嵌入和更改具体教学任务的偏好？ - 有没有办法设置机器人/算法如何“持久”地保持其偏好而不改变它们 如果有人能指出我正确的方向，我将不胜感激！    由   提交 /u/MickeyMouseEngineer2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7xtlz/retraining_of_qtables/</guid>
      <pubDate>Fri, 19 Apr 2024 14:09:05 GMT</pubDate>
    </item>
    <item>
      <title>“如何培训数据高效的法学硕士”，Sachdeva 等人 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7dbyn/how_to_train_dataefficient_llms_sachdeva_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7dbyn/how_to_train_dataefficient_llms_sachdeva_et_al/</guid>
      <pubDate>Thu, 18 Apr 2024 20:14:20 GMT</pubDate>
    </item>
    <item>
      <title>“确保大型语言模型的一致性和安全性的基本挑战”，Anwar 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7cwek/foundational_challenges_in_assuring_alignment_and/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7cwek/foundational_challenges_in_assuring_alignment_and/</guid>
      <pubDate>Thu, 18 Apr 2024 19:57:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 DDPG 在车杆环境中学习进度缓慢且奖励波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c7binb/slow_learning_progress_and_fluctuating_rewards_in/</link>
      <description><![CDATA[     &lt; /td&gt; 嘿 RL 社区， 我目前正在使用以下方法训练车杆环境的控制器强化学习（DDPG）。环境提供了对角度、位移及其导数的观察，动作空间范围从 -10 到 10。 我在学习过程中注意到了一个有趣的模式。最初，智能体似乎学习得很慢，然后突然获得很高的奖励，然后又恢复到之前的奖励水平。这个循环在剧集中重复，表明缺乏一致的学习进度。 我在显示奖励与剧集数量的图中可视化了这种行为。 https://preview.redd.it/utkta8hxbavc1.png?width=1738&amp;format= png&amp;auto=webp&amp;s=65d663612ec158c6b21b1590e60a27003632c5f0 我的一些参数如下： LearnRate=5e-03，GradientThreshold=1（对于评论家和演员） 可学习的数量：6.5k（对于评论家和演员） ExperienceBufferLength=1e5 TargetSmoothFactor=5e-3 MiniBatchSize=200  我将不胜感激任何见解或者关于为什么会发生这种模式以及我如何解决它以确保更稳定和持续的学习进度的建议。 提前感谢您提供的任何帮助或建议！ &lt; /div&gt;  由   提交 /u/Far-Cattle-1247   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c7binb/slow_learning_progress_and_fluctuating_rewards_in/</guid>
      <pubDate>Thu, 18 Apr 2024 19:02:46 GMT</pubDate>
    </item>
    <item>
      <title>MineDreamer：通过想象链学习遵循模拟世界控制的指令</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c775yg/minedreamer_learning_to_follow_instructions_via/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2403.12037 代码：https://github .com/Zhoues/MineDreamer 模型和数据集：https://huggingface. co/Zhoues 摘要：  设计一个可以遵循的通才体现的智能体是一个长期目标以类似人类的方式进行多样化的指令。然而，由于难以理解抽象和连续的自然语言指令，现有方法通常无法稳定地遵循指令。为此，我们推出了 MineDreamer，这是一种基于具有挑战性的 Minecraft 模拟器的开放式实体代理，其创新范例可增强低级控制信号生成中的指令跟踪能力。具体来说，MineDreamer 是在多模态大型语言模型 (MLLM) 和扩散模型的最新进展之上开发的，我们采用想象链 (CoI) 机制来想象执行指令的逐步过程，并将想象力转化为适合当前状态的更精确的视觉提示；随后，代理生成键盘和鼠标操作，以有效地实现这些想象，并稳定地遵循每一步的指令。大量实验表明，MineDreamer 能够稳定地遵循单步和多步指令，显着优于最佳通用代理基线，几乎使其性能提高了一倍。此外，对智能体想象力的定性分析揭示了其对开放世界的概括和理解。    由   提交 /u/SeawaterFlows   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c775yg/minedreamer_learning_to_follow_instructions_via/</guid>
      <pubDate>Thu, 18 Apr 2024 16:08:10 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于视频游戏/模拟器中的敌人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c763d2/reinforcement_learning_being_used_for_enemies_in/</link>
      <description><![CDATA[目前是否有任何向公众开放的视频游戏完全甚至部分使用强化学习来处理敌人的行为？如果没有您知道的。您认为强化学习如何用于动态模拟/游戏？即使它不直接影响结果，而是在幕后管理事情/   由   提交/u/a_normal_user1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c763d2/reinforcement_learning_being_used_for_enemies_in/</guid>
      <pubDate>Thu, 18 Apr 2024 15:25:06 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于视频游戏/模拟器中的敌人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c763cw/reinforcement_learning_being_used_for_enemies_in/</link>
      <description><![CDATA[目前是否有任何向公众开放的视频游戏完全甚至部分使用强化学习来处理敌人的行为？如果没有您知道的。您认为强化学习如何用于动态模拟/游戏？即使它不直接影响结果，而是在幕后管理事情/   由   提交/u/a_normal_user1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c763cw/reinforcement_learning_being_used_for_enemies_in/</guid>
      <pubDate>Thu, 18 Apr 2024 15:25:06 GMT</pubDate>
    </item>
    <item>
      <title>关于在 SMAC 基准上训练 MARL 算法有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c75fm2/any_tips_on_training_marl_algorithms_on_the_smac/</link>
      <description><![CDATA[我正在尝试在一些 SMAC 基准上训练 MAPPO，但对超参数微调没有任何线索。值得注意的是，训练胜率始终为 0，而测试胜率在 0 到 95% 之间波动。这是正常的吗？  我在论文上看到的大多数情节都会收敛到胜率超过 90%。这很容易实现吗？ 非常感谢任何建议。   由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c75fm2/any_tips_on_training_marl_algorithms_on_the_smac/</guid>
      <pubDate>Thu, 18 Apr 2024 14:58:28 GMT</pubDate>
    </item>
    <item>
      <title>回报曲线平滑度有多重要？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6zlnc/how_important_is_returns_curve_smoothness/</link>
      <description><![CDATA[我一直在运行一些超参数搜索，以找到可以加快学习速度的配置。不幸的是，几乎每次搜索都会在探索窗口中产生相同的轨迹。 其中一条回报曲线比其他曲线明显平滑。鉴于几乎所有其他曲线看起来都一样，我应该认为这种平滑度是优点还是问题？我不确定从长远来看，这是否是探索不力的症状，从而收敛到局部最小值，或者是一个积极的信号，表明它不会不断地忘记重新学习小细节。    由   提交/u/drblallo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6zlnc/how_important_is_returns_curve_smoothness/</guid>
      <pubDate>Thu, 18 Apr 2024 10:05:49 GMT</pubDate>
    </item>
    <item>
      <title>环境可扩展性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6z5c7/environment_scalability/</link>
      <description><![CDATA[       我有一个使用 3 个代理训练的多代理强化学习环境。我的观察和操作空间是动态的： min_action = np.array([-5, -5] * len(self.agents), dtype=np.float32) max_action = np.array ([5, 5] * len(self.agents), dtype=np.float32) self.action_space = space.Box(low=min_action, high=max_action, dtype=np.float32) min_obs = np.array([[ -np.inf, -np.inf, -2.5, -2.5]] * len(self.agents), dtype=np.float32) max_obs = np.array([[np.inf, np.inf, 2.5, 2.5 ]] * len(self.agents), dtype=np.float32) self.observation_space = space.Box(low=min_obs, high=max_obs, dtype=np.float32)  &lt; strong&gt;虽然我在 OpenAI Gym 自定义环境中使用 SB3 的 PPO，但我的代理设置也采用了多代理。我希望我的模型运行 10 个代理来测试它。 执行此操作时，我收到观察空间不匹配的错误： 错误 任何解决方案表示赞赏。   由   提交 /u/Hooooman101   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6z5c7/environment_scalability/</guid>
      <pubDate>Thu, 18 Apr 2024 09:34:30 GMT</pubDate>
    </item>
    <item>
      <title>机器人强化学习：您面临的最大问题是什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6trcp/robotics_rl_what_are_some_of_the_biggest_problems/</link>
      <description><![CDATA[很想听听将强化学习策略构建到机器人技术中的不好的部分。你最讨厌的事情你希望不存在！故意让它开放和含糊。希望听到任何反馈:)   由   提交/u/bluejae05  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6trcp/robotics_rl_what_are_some_of_the_biggest_problems/</guid>
      <pubDate>Thu, 18 Apr 2024 03:44:19 GMT</pubDate>
    </item>
    <item>
      <title>“Ijon：通过模糊测试探索深层状态空间”，Aschermann 等人 2020</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c6nvod/ijon_exploring_deep_state_spaces_via_fuzzing/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c6nvod/ijon_exploring_deep_state_spaces_via_fuzzing/</guid>
      <pubDate>Wed, 17 Apr 2024 22:56:25 GMT</pubDate>
    </item>
    </channel>
</rss>