<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 24 Sep 2024 15:18:07 GMT</lastBuildDate>
    <item>
      <title>为什么我的 SB3 DQN 上的 LunarLander 性能不佳？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1foe4p8/why_does_my_lunarlander_on_sb3_dqn_not_perform/</link>
      <description><![CDATA[我从这里获得了最佳超参数。因此，我期望算法能够达到最佳性能，即在训练结束时频繁获得 200 的情景奖励。但这并没有发生。  我已在此处附加我的代码 - https://pastecode.io/s/evo1c0ku 有人可以帮忙吗？    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1foe4p8/why_does_my_lunarlander_on_sb3_dqn_not_perform/</guid>
      <pubDate>Tue, 24 Sep 2024 14:40:50 GMT</pubDate>
    </item>
    <item>
      <title>我正在学习 RL，并且取得了很大的进步。我总结了我认为非常有用的资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fobu5v/im_learning_rl_and_making_good_progress_i/</link>
      <description><![CDATA[        由    /u/Fair_Detective_6568   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fobu5v/im_learning_rl_and_making_good_progress_i/</guid>
      <pubDate>Tue, 24 Sep 2024 12:57:34 GMT</pubDate>
    </item>
    <item>
      <title>用于一般和博弈（即合作）的 MuZero 风格算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fo7css/muzero_style_algorithms_for_generalsum_games_ie/</link>
      <description><![CDATA[大家好， 我对将 MuZero 应用于合作纸牌游戏很感兴趣。阅读论文 https://arxiv.org/pdf/1911.08265 时，我注意到附录 B 中提到“... 一种渐近收敛到零和游戏中的极小最大值函数的规划方法”。由于我正在处理一般和博弈，因此我对最大-最大方案更感兴趣。 这里有没有地方知道这样做的作品/项目/论文？ 谢谢！    提交人    /u/Arconer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fo7css/muzero_style_algorithms_for_generalsum_games_ie/</guid>
      <pubDate>Tue, 24 Sep 2024 08:08:07 GMT</pubDate>
    </item>
    <item>
      <title>有行为分析师吗？……你们在招人吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnv8vh/any_behavior_analysts_out_there_are_you_hiring/</link>
      <description><![CDATA[有没有公司了解 RL 中行为分析的价值？RL 来自行为分析，但这两个领域似乎没有太多交流。我正试图进入 RL 行业，但不确定如何传达我十多年的专业知识。    提交人    /u/1fission   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnv8vh/any_behavior_analysts_out_there_are_you_hiring/</guid>
      <pubDate>Mon, 23 Sep 2024 20:58:25 GMT</pubDate>
    </item>
    <item>
      <title>“AI 研究所” 到底是什么？似乎与波士顿动力公司有着密切的联系。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnszu8/what_is_the_ai_institute_all_about_seems_to_have/</link>
      <description><![CDATA[“AI 研究所”到底是什么？似乎与波士顿动力公司有着密切的联系。 但我听说他们是由现代资助的？他们的研究重点是什么？产品？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnszu8/what_is_the_ai_institute_all_about_seems_to_have/</guid>
      <pubDate>Mon, 23 Sep 2024 19:24:43 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习解决高度随机的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnptsy/solving_highly_stochastic_environments_using/</link>
      <description><![CDATA[我一直在研究一个高度随机环境中的强化学习 (RL) 问题，其中噪声的影响远远超过代理操作的影响。为了说明这一点，请考虑以下示例： $ s&#39; = s + a + \epsilon $ 其中：  $ \epsilon \sim \mathcal{N}(0, 0.3)$ 是均值为 0、标准差为 0.3 的高斯噪声。 $ a \in {-0.01, 0, 0.01}$ 是代理可以采取的操作。  在此设置中，噪声 $\epsilon $ 主导动态，相比之下，代理操作的影响可以忽略不计。因此，使用标准 Q 学习进行学习被证明是低效的，因为噪声会淹没学习信号。 问题：在随机性（或噪声）比代理操作的影响大得多的环境中，我如何才能有效地学习？是否有更适合处理这种情况的替代 RL 算法或方法？ PS：向状态添加额外信息是一种选择，但可能并不有利，因为它会增加我现在试图避免的状态空间。 任何关于如何解决这个问题的建议或对类似工作的引用都将不胜感激！有人遇到过类似的问题吗？你是怎么解决的？提前谢谢您！    提交人    /u/Hey--Macarena   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnptsy/solving_highly_stochastic_environments_using/</guid>
      <pubDate>Mon, 23 Sep 2024 17:15:24 GMT</pubDate>
    </item>
    <item>
      <title>PPO 学得很好，但随后奖励不断减少</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnldyh/ppo_learns_quite_well_but_then_reward_keeps/</link>
      <description><![CDATA[      嘿，我正在使用 SB3 中的 PPO（在自己的自定义环境中），使用以下设置： policy_kwargs = dict( net_arch=dict(pi=[64,64], vf=[64,64])) log_path = &quot;..&quot; # model = PPO.load(&quot;./models/model_step_1740000.zip&quot;, env=env) model = PPO(&quot;MlpPolicy&quot;, env, verbose=1, tensorboard_log=log_path, policy_kwargs=policy_kwargs, seed=42, n_steps=512, batch_size=32) model.set_logger(new_logger) model = model.learn(total_timesteps=1000000, callback=save_model_callback, progress_bar=True, )  该模型学习得很好，但似乎很快就“忘记”了它学到的东西。例如，请参见以下曲线，其中步骤 25k-50k 的高奖励区域是完美的，但随后奖励明显下降。你能看出其中的原因吗？ https://preview.redd.it/ve40nmtogkqd1.png?width=682&amp;format=png&amp;auto=webp&amp;s=ffd6d0b5f3cea8d89ad78e42798bbb2b759182d1   由    /u/luigi1603  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnldyh/ppo_learns_quite_well_but_then_reward_keeps/</guid>
      <pubDate>Mon, 23 Sep 2024 14:10:47 GMT</pubDate>
    </item>
    <item>
      <title>我希望 RL 能够成功并变得更加流行，但目前它的前景并不乐观。您对此有何看法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnki7h/i_want_rl_to_succeed_and_become_more_popular_but/</link>
      <description><![CDATA[RL 在 2010 年代后半期非常流行，但自那以后就大幅衰落了。Sutton 基本上被 deepmind 解雇了，deepmind 在投入大量资金于 RL 后似乎削减了很多 RL 项目和人员（没有来源，只是我从业内人士那里读到的），专注于 RL 的公司所占的市场份额比以前小得多，uni labs 在大量投资后也削减了 RL，即使是 RL 最具潜力的领域机器人技术仍然专注于 MPC。 我不是专家，所以我想知道这里的人的意见，我个人希望 RL 得到更多的使用，所以我欢迎被证明是错误的想法。 目前 RL 的最大用途是 RLHF，它甚至不是真正的 RL，甚至对 LLM 来说也不是那么必要。人们在 2010 年代谈论 RL 征服世界，但到目前为止，Lecun 关于 RL 只是锦上添花的说法似乎是正确的。 同样，如果您认为这篇文章是错误的，请写下您为什么这么认为，以及 RL 有哪些方法可以超越其他技术。 我对机器人技术中的 RL 特别好奇，以及它是否会超越非 RL 技术。    提交人    /u/cyberpunk00c   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnki7h/i_want_rl_to_succeed_and_become_more_popular_but/</guid>
      <pubDate>Mon, 23 Sep 2024 13:31:06 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 PPO 图结构最短路径搜索问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fnbseg/help_with_ppo_graph_structure_shortest_path/</link>
      <description><![CDATA[      我是韩国强化学习专业的本科生，正在尝试使用 PPO 算法解决受限图结构中的最短路径搜索问题。附件是环境的屏幕截图。 https://preview.redd.it/rsj22c0xehqd1.png?width=2345&amp;format=png&amp;auto=webp&amp;s=3f261bf3b11c533f3fdbcd05172495d8a24c7911 演员和评论家网络使用 GCN（图卷积网络）处理图结构，利用邻接矩阵和节点特征矩阵。节点特征矩阵的设计为每个节点的特征值如下：[节点 ID（节点索引号）、相邻节点号 1、相邻节点号 2]。如果一个节点只有一个邻居，则第二个邻居用 -1 填充。换句话说，矩阵的大小为 [节点数，特征数]。 此外，网络状态值包括代理的状态，该状态由 [当前代理位置（节点索引号）、目标位置（节点索引号）、根据 Dijkstra 算法的剩余路径长度] 组成。 参与者网络使用邻接矩阵和节点特征矩阵通过 GCN 嵌入节点特征，然后展平嵌入的节点特征并将其与代理的状态连接起来。连接的结果通过完全连接层，该层预测动作。动作空间由 3 个选项组成：前进、左转和右转。 对于奖励设计，如果代理在单行道上并且没有选择前进动作，则情节立即结束，并应用 -0.001 的惩罚。如果代理在路口并选择前进，则情节立即结束，并施加 -0.001 的惩罚。如果代理选择左转或右转，并且到达目的地的路径缩短，则给予 0.001 的奖励。当代理到达目的地时，给予 1 的奖励。如果代理未能在 1200 个时间步内到达目的地，则情节以 -0.001 的惩罚结束。我在记录了 120,000 个时间步的经验后更新了模型。 尽管进行了长时间的训练，但在早期阶段，情节成功率和累积奖励有所增加，但在某个时间点之后，性能会停滞在令人不满意的水平。 我的 PPO 超参数如下：  GAMMA = 0.99 TRAJECTORIES_PER_LEARNING_STEP = 512 UPDATES_PER_LEARNING_STEP = 10 MAX_STEPS_PER_EPISODE = 1200 ENTROPY_LOSS_COEF = 0 V_LOSS_CEOF = 0.5 CLIP = 0.2 LR = 0.0003  问题：  为什么这不起作用？ 我的状态表示设计不正确吗？  我的英语水平很差，但感谢您的阅读！    提交人    /u/Latter-Parsnip4425   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fnbseg/help_with_ppo_graph_structure_shortest_path/</guid>
      <pubDate>Mon, 23 Sep 2024 03:57:06 GMT</pubDate>
    </item>
    <item>
      <title>Agent 选择相同的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fn4ull/agent_selects_the_same_action/</link>
      <description><![CDATA[大家好， 我正在开发一个 DQN，它根据当前状态从多个规则中一次选择一个规则。但是，无论状态如何，代理都倾向于选择相同的操作。它已训练了 1,000 集，其中 500 集专门用于探索。 该任务涉及维护计划，每次可用时，代理都会选择一条规则来选择要维护的机器。 有人遇到过类似的问题吗？    提交人    /u/GuavaAgreeable208   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fn4ull/agent_selects_the_same_action/</guid>
      <pubDate>Sun, 22 Sep 2024 21:59:04 GMT</pubDate>
    </item>
    <item>
      <title>需要有关如何更好地实施的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmpci8/need_advice_on_getting_better_at_implementation/</link>
      <description><![CDATA[TLDR；从理论到实施的最顺利的过渡方式是什么？ 我目前正在学习 MARL 课程，我们的作业要求我们使用 DP 和 MC 解决 TSP 和推箱子问题。 我们在 Gymnasium 中获得了一些样板代码（用于 TSP），但必须自己实施策略（以及推箱子的环境）。 虽然我了解它们背​​后的概念和数学，但我在实施、为策略使用什么数据结构以及理解 Gymnaisum 方面却举步维艰。 任何建议都将不胜感激    提交人    /u/Illustrious_Sir_2913   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmpci8/need_advice_on_getting_better_at_implementation/</guid>
      <pubDate>Sun, 22 Sep 2024 09:31:45 GMT</pubDate>
    </item>
    <item>
      <title>入门帮助请求。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmlfkm/getting_started_help_request/</link>
      <description><![CDATA[我想创建 RL 来玩西洋双陆棋的变体。  我想写入接口并利用预先存在的 RL 引擎。 是否有可以满足我的需求的 GitHub 存储库？ 或者云服务？ 谢谢， Hal Heinrich    提交人    /u/halheinrich   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmlfkm/getting_started_help_request/</guid>
      <pubDate>Sun, 22 Sep 2024 04:51:42 GMT</pubDate>
    </item>
    <item>
      <title>可以以切片方式采样的离线 RL 数据集？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmc5g5/offline_rl_datasets_that_one_can_sample_in_slice/</link>
      <description><![CDATA[您好， 我目前正在从事一个受这篇论文启发的项目，并且遇到了对可以以切片方式采样的转换数据集的需求。 （大小为 (B, S, *) 或 (S, B, *) 的批次，其中 S 是相同轨迹的连续切片的维度） 我正在尝试使 d4rl-atari 数据集工作，但是在让它对连续切片进行采样时遇到了一些麻烦，所以我想知道这里是否有人有什么建议。 域本身并不是太重要，但我更喜欢使用像素观测。    提交人    /u/Ayy_Limao   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmc5g5/offline_rl_datasets_that_one_can_sample_in_slice/</guid>
      <pubDate>Sat, 21 Sep 2024 20:29:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习，SUMO模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm9cja/reainforcement_learning_sumo_simulation/</link>
      <description><![CDATA[        提交人    /u/IllIntroduction9410   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm9cja/reainforcement_learning_sumo_simulation/</guid>
      <pubDate>Sat, 21 Sep 2024 18:20:11 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>