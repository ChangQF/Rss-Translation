<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 31 Dec 2023 01:04:22 GMT</lastBuildDate>
    <item>
      <title>《利用部分动力学知识进行高效强化学习的样本》2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</link>
      <description><![CDATA[ 由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</guid>
      <pubDate>Sat, 30 Dec 2023 22:12:11 GMT</pubDate>
    </item>
    <item>
      <title>环境生成器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</link>
      <description><![CDATA[嘿，RL 爱好者， 我想知道，当我们做 RL 实验时，你们是否都被开发 RL 所需的开销所困扰？预先环境。我发现这非常烦人，因为我总是需要构建适合我的用例的东西。  据我们所知，我们只有 Farma 基金会（https://farama.org/）提供的十几个高质量环境。 org/)  欢迎任何想法！    由   提交 /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</guid>
      <pubDate>Sat, 30 Dec 2023 18:59:28 GMT</pubDate>
    </item>
    <item>
      <title>多目标场景的最佳强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</link>
      <description><![CDATA[你好， 我正在尝试训练室内无人机代理离开房间。无人机必须逃离日益严重的火势并到达 4 个出口中的任何一个。 我尝试过 DQN、A2C、PPO。这些算法的问题在于，一旦智能体学会了出口门，它总是尝试从那里退出，而其他门则未被探索。 我想知道哪种 RL 算法最适合这种情况，当更多没有一个进球。 谢谢！   由   提交/u/shahmirkhan21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</guid>
      <pubDate>Sat, 30 Dec 2023 16:24:36 GMT</pubDate>
    </item>
    <item>
      <title>Pangu-Agent：具有结构化推理的可微调多面手智能体</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18u2bym/panguagent_a_finetunable_generalist_agent_with/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.14878 摘要：  创建人工智能（AI）代理的关键方法是强化学习（RL）。然而，构建一个将感知直接映射到行动的独立强化学习策略会遇到严重的问题，其中最主要的是它缺乏跨多个任务的通用性以及需要大量的训练数据。主要原因是在制定政策时无法有效地将先验信息融入到感知-行动循环中。大型语言模型（LLM）作为将跨领域知识融入人工智能代理的基本方式而出现，但缺乏针对特定决策问题的关键学习和适应。本文提出了一个将结构化推理集成和学习到人工智能代理策略中的通用框架模型。我们的方法论受到人脑模块化的启发。该框架利用内在和外在函数的构造来添加先前对推理结构的理解。它还提供了学习每个模块或功能内部模型的自适应能力，与认知过程的模块化结构一致。我们深入描述了该框架，并将其与其他人工智能管道和现有框架进行了比较。本文探讨了实际应用，包括证明我们方法有效性的实验。我们的结果表明，当嵌入有组织的推理和先验知识时，人工智能代理的表现和适应能力要好得多。这为更具弹性和通用的人工智能代理系统打开了大门。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18u2bym/panguagent_a_finetunable_generalist_agent_with/</guid>
      <pubDate>Sat, 30 Dec 2023 00:41:00 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的哈密顿路径/循环</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18tvfgf/hamiltonian_pathcycle_with_rl/</link>
      <description><![CDATA[是否可以使用 PPO 和 5x5 矩阵中的 4 种可能的移动方式，通过常规方法求解哈密顿路径/循环？ &lt; /div&gt;  由   提交/u/marques576  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18tvfgf/hamiltonian_pathcycle_with_rl/</guid>
      <pubDate>Fri, 29 Dec 2023 19:39:05 GMT</pubDate>
    </item>
    <item>
      <title>感谢帮助 - 用于温度控制的 DQL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18tslxw/help_appreciated_dql_for_temperature_control/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18tslxw/help_appreciated_dql_for_temperature_control/</guid>
      <pubDate>Fri, 29 Dec 2023 17:37:55 GMT</pubDate>
    </item>
    <item>
      <title>平均奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18trd23/average_reward/</link>
      <description><![CDATA[       大家好，我是 DRL 新手。当我使用Carla自动驾驶车辆模拟器训练DQN模型时，我发现平均奖励曲线是这样的（有波动），解决方案是什么？ ​ https://preview.redd.it/knu4kkqzi99c1.png ?width=583&amp;format=png&amp;auto=webp&amp;s=b516976d2b65d61d610a0726301f23262962fa48   由   提交 /u/Chetioui_PHD   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18trd23/average_reward/</guid>
      <pubDate>Fri, 29 Dec 2023 16:43:49 GMT</pubDate>
    </item>
    <item>
      <title>Boid环境下未实现植绒</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18t82zx/not_achieveing_flocking_in_boid_environment/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18t82zx/not_achieveing_flocking_in_boid_environment/</guid>
      <pubDate>Thu, 28 Dec 2023 23:35:14 GMT</pubDate>
    </item>
    <item>
      <title>具有策略切换功能的指挥官风格强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18t6vkf/commander_style_rl_with_policy_switching/</link>
      <description><![CDATA[现在我面前有一个特别有趣的问题，我有一个想法。但我不确定这个想法以前是否已经实现过... 我的环境基本上会有一名经理和 4 名工人。可以为工人分配 3 种不同任务中的一种（比如做饭、上菜和洗碗）。每个任务都有自己的训练策略。这里有一个问题 - 经理可能会在剧集中间指示工作人员切换任务（例如：洗碗机可能需要在高峰期间帮助厨师，或者服务器可能需要在打烊时帮助洗碗） ）。这里本质上有两个主要的学习层次：每项任务的原始动作（烹饪的最佳方式、服务的最佳方式和洗碗的最佳方式）和资源分配任务（最好地利用 4 名工人来完成任务）。经营一家餐厅的最终目标）。 我对如何做到这一点有一些想法（但我不完全确定什么是可能的或合乎逻辑的）：  &lt; li&gt;将此视为某种分层学习问题 - 确实如此。经理每隔多个时间步设置任务分配，而工作人员则在每个时间步执行操作来完成这些任务。这是相当可行的，但我不确定如何完成策略切换。根据我的经验，每个多代理设置都有自己的训练策略，因此尝试创建 3 个通用策略并动态分配它们进行训练似乎......很棘手。 创建奖励函数而不是策略切换根据工人的分配奖励不同的结果。该分配可以是其观察空间的一部分（例如：当观察到工人是洗碗机时，奖励函数会奖励干净的盘子，但如果观察到工人是服务员，则奖励满水，等等）。将创建一项所有员工都可以共享的单一政策，并且他们的行为可能会根据他们观察到的任务而改变。 这似乎是最不合逻辑的，但也许我错过了一些东西......我可以分别对这 3 个策略中的每一个进行预训练，并将它们设为静态策略，供经理尝试将任务分配给工作人员。但是A）我认为这消除了策略之间的一些上下文依赖性（例如：在晚餐高峰期间，如果我只将厨师训练为单个实体，然后分配了两名厨师，则该行为可能不会接近最佳，因为以前从未见过）B）我仍然存在实际执行策略切换的问题。  ​ 所以我的问题由此可知，有两个方面。  一次事件期间的动态策略切换实际上可能吗？谁能给我举一些例子吗？ 其中哪一个实际上最有意义？  ​ TIA 提供任何建议并帮助！   由   提交/u/Cheap_Leather_6432   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18t6vkf/commander_style_rl_with_policy_switching/</guid>
      <pubDate>Thu, 28 Dec 2023 22:43:35 GMT</pubDate>
    </item>
    <item>
      <title>[R]可解释强化学习研究综合概述</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18szxuu/r_comprehensive_overview_of_explainable/</link>
      <description><![CDATA[ 由   提交/u/peppercat-2c4t9  /u/peppercat-2c4t9 reddit.com/r/MachineLearning/comments/18suecx/r_compressive_overview_of_explainable/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18szxuu/r_comprehensive_overview_of_explainable/</guid>
      <pubDate>Thu, 28 Dec 2023 17:51:48 GMT</pubDate>
    </item>
    <item>
      <title>“绘制行为结构的细胞基础”，El-Gaby 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18semw7/a_cellular_basis_for_mapping_behavioral_structure/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18semw7/a_cellular_basis_for_mapping_behavioral_structure/</guid>
      <pubDate>Wed, 27 Dec 2023 23:31:06 GMT</pubDate>
    </item>
    <item>
      <title>PASTA：预训练的动作状态转换器代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18se47h/pasta_pretrained_actionstate_transformer_agents/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2307.10936 OpenReview (1):  https://openreview.net/forum?id=pxK9MWuFF8 OpenReview (2): https://openreview.net/forum?id=ciBFYxzpBT 摘要：  自我-监督学习给各个计算领域带来了革命性的范式转变，包括 NLP、视觉和生物学。最近的方法涉及对大量未标记数据进行预训练 Transformer 模型，作为有效解决下游任务的起点。在强化学习中，研究人员最近采用了这些方法，开发了根据专家轨迹进行预训练的模型。这一进步使模型能够处理从机器人到推荐系统的广泛任务。然而，现有方法主要依赖于针对特定下游应用量身定制的复杂预训练目标。本文对模型进行了全面的研究，称为预训练动作状态转换代理（PASTA）。我们的研究涵盖了统一的方法论，并涵盖了广泛的一般下游任务，包括行为克隆、离线强化学习、传感器故障鲁棒性和动态变化适应。我们的目标是系统地比较各种设计选择，并提供有价值的见解，帮助从业者开发强大的模型。我们研究的主要亮点包括动作和状态的组件级别的标记化、基本预训练目标的使用（例如下一个标记预测或掩码语言建模）、跨多个领域的模型同步训练以及各种微调的应用策略。在这项研究中，开发的模型包含不到 700 万个参数，允许广泛的社区使用这些模型并重现我们的实验。我们希望这项研究能够鼓励进一步研究使用具有第一原理设计选择的变压器来表示 RL 轨迹，并为稳健的策略学习做出贡献。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18se47h/pasta_pretrained_actionstate_transformer_agents/</guid>
      <pubDate>Wed, 27 Dec 2023 23:09:02 GMT</pubDate>
    </item>
    <item>
      <title>RL IRL：2015-2019 年 Google 搜索中排名和偏好学习的使用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18scjqg/rl_irl_on_google_search_use_of_ranking/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18scjqg/rl_irl_on_google_search_use_of_ranking/</guid>
      <pubDate>Wed, 27 Dec 2023 22:03:10 GMT</pubDate>
    </item>
    <item>
      <title>GridWorld 的 Q-Learning 有时无法学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rtkso/qlearning_for_gridworld_occasionally_failing_to/</link>
      <description><![CDATA[我有一个 10x10 网格世界环境，我正在其中尝试实现 Q-Learning。所有单元格的奖励为 0.1，而终端单元格的奖励为 10。折扣因子为 0.9。有时，Q-Learning 无法收敛（例如每 10 次收敛一次），并且代理会卡在远离终端单元的位置。我尝试了衰减 epsilon，但这只会让训练变慢。请在此处找到该作品的链接。谢谢。    由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rtkso/qlearning_for_gridworld_occasionally_failing_to/</guid>
      <pubDate>Wed, 27 Dec 2023 06:05:30 GMT</pubDate>
    </item>
    <item>
      <title>我为我的 NeurIPS 2023 论文制作了一个 7 分钟的讲解视频。我希望你喜欢它 ：）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18rti1y/i_made_a_7minute_explanation_video_of_my_neurips/</link>
      <description><![CDATA[       由   提交 /u/delayed_reward   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18rti1y/i_made_a_7minute_explanation_video_of_my_neurips/</guid>
      <pubDate>Wed, 27 Dec 2023 06:01:13 GMT</pubDate>
    </item>
    </channel>
</rss>