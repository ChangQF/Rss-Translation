<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 24 Jun 2024 01:05:29 GMT</lastBuildDate>
    <item>
      <title>编程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</link>
      <description><![CDATA[        提交人    /u/chagdubbish   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmuxqd/programming/</guid>
      <pubDate>Sun, 23 Jun 2024 20:06:01 GMT</pubDate>
    </item>
    <item>
      <title>“对在符号多步推理任务上训练的 Transformer 的机制分析”，Brinkmann 等人，2024 年（Transformer 可以在前向传递中进行内部规划）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmt524/a_mechanistic_analysis_of_a_transformer_trained/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmt524/a_mechanistic_analysis_of_a_transformer_trained/</guid>
      <pubDate>Sun, 23 Jun 2024 18:46:40 GMT</pubDate>
    </item>
    <item>
      <title>为 Pong 游戏制作了 AI，但它仍然很笨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dms403/made_ai_for_pong_game_but_it_is_still_dumb/</link>
      <description><![CDATA[我是否缺少剧集数量？我不知道，但是当我在游戏中尝试时，它完全缺乏如何用击球手击球的知识（典型的 Pong 游戏） 这是train.py -&gt; https://www.pythonmorsels.com/p/2y9vw/ 这是env.py -&gt; https://www.pythonmorsels.com/p/2vdwb/    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dms403/made_ai_for_pong_game_but_it_is_still_dumb/</guid>
      <pubDate>Sun, 23 Jun 2024 18:00:49 GMT</pubDate>
    </item>
    <item>
      <title>关于dreamerv3的方程推导的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmiv75/question_about_the_equation_derivation_of/</link>
      <description><![CDATA[      嗨！ 我尝试学习 dreamerv3 及其代码实现。但是对于等式 10， https://preview.redd.it/9xv44gx5oa8d1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=8330ac94061559235a5b0448251c2561f64628d4 我对 ln_p 的计算感到困惑，它对应于以下代码片段 log_pred = self.logits - jax.scipy.special.logsumexp(self.logits, -1, keepdims=True) 你能向我解释一下上述实现吗？为什么要以这种方式推导 log_pred？ 提前致谢。    提交人    /u/UpperSearch4172   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmiv75/question_about_the_equation_derivation_of/</guid>
      <pubDate>Sun, 23 Jun 2024 10:03:25 GMT</pubDate>
    </item>
    <item>
      <title>股票交易的监督预学习。以前做过吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dmdn23/supervised_pre_learning_for_stock_trading_has/</link>
      <description><![CDATA[我正在尝试为论文提出新颖的想法，我有一个关于使用监督学习“热身”代理的想法。一般的想法是，例如，在 2005-01-01 到 2010-12-31 之间，代理将接受“理想”操作的训练。从 2011-01-01 到 2020-12-31，代理将再次接受重新训练。以前做过吗？有没有关于这种概念的论文我应该读一读？    提交人    /u/newjeison   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dmdn23/supervised_pre_learning_for_stock_trading_has/</guid>
      <pubDate>Sun, 23 Jun 2024 04:05:13 GMT</pubDate>
    </item>
    <item>
      <title>自适应四轴飞行器网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</link>
      <description><![CDATA[👋 嘿，我正在尝试想出一个四轴飞行器无人机自主控制的解决方案。 到目前为止，我设法让它使用简单的 MLP + PPO 进行航点导航。 这很有效，特别是在域随机化的情况下，但如果突然刮起一阵风会发生什么？四轴飞行器的重量超过了它在模拟中训练的重量？螺旋桨损坏，其中一个电机产生的推力减小？ 通过所有这些示例，我的问题是无人机无法适应变化。我想创建一个能够适应任何无法解释的外部干扰的网络。 我有两个计划，一个是将 LSTM 与 PPO 结合使用，因此代理会保留飞行数据。 我的另一个想法是使用我用于训练的模拟器来获取无人机在当前时间步长的预期速度/姿态/位置，并将其与我从传感器获得的实际值一起输入到策略中。 我希望更有经验的人可以对我的想法提供反馈，说实话我有点迷茫，我不确定这些是否可行，任何帮助都值得赞赏！    提交人    /u/FutureComedian7749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</guid>
      <pubDate>Sat, 22 Jun 2024 23:15:56 GMT</pubDate>
    </item>
    <item>
      <title>C++ 库到 JAX？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dlil7i/c_library_to_jax/</link>
      <description><![CDATA[大家好 r/MLQuestions，我读过这篇 2022 年的论文 https://chrislu.page/blog/meta-disco/，它讨论了通过纯 Jax 并行化将所有计算从 CPU 转移到 GPU 和 TPU。本质上，他们已经将整个 RL 步骤矢量化以进行 GPU 计算，并且由于令人难以置信的并行化，能够在 9 小时内在 Atari 基准测试中训练 50 万个代理，与 CleanRL 实现相比，这实现了 4000 倍的加速。  我认为现在它更常见了，但在 2022 年，这可能是一个重大突破。  现在，我也想要这个 我遇到了一个问题：在纯 jax 中工作。现在，我正在创建一个需要复杂 3d 数学引擎的产品 - 这些复杂的库很大并且用 C++ 编写。我也需要其中的很多库。  例如，一个是 OpenCASCADE [https://github.com/Open-Cascade-SAS] 。仅“src”文件夹就有 97mb，包含 16000 个文件。它非常庞大，我需要将其插入 JAX 以提高速度。  如果我不对我的应用程序进行超并行化，我将会陷入 10 倍的循环时间和 200 倍的硬件并行线减少的困境，这在我的领域意味着长时间的计算。  现在我懂数学，也许可以重写我需要的函数，但是我有没有可能用某种……装饰器将文件适配到 Jax ？毕竟，jax 也是用 C++ 编写的。或者也许分叉它并进行调整。任何能让函数在 Jax 中工作的东西，并且自定义实现不会花费 &gt; 个月的时间。 如果您知道任何可行的方法，请告诉我。谢谢大家！ 编辑：我的目标是：在 GPU 中运行所有内容，并以大规模并行方式运行所有内容。    提交人    /u/JustZed32   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dlil7i/c_library_to_jax/</guid>
      <pubDate>Sat, 22 Jun 2024 00:07:06 GMT</pubDate>
    </item>
    <item>
      <title>AgileRL - 用于最先进深度强化学习的进化型 RLOps</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</link>
      <description><![CDATA[嗨，我之前发布过关于我们针对强化学习的进化超参数优化实现 SOTA 结果的帖子，但我想分享的是，我们的开源框架现已发布 v1.0.0！ 请查看！https://github.com/AgileRL/AgileRL 该库最初专注于通过开创用于强化学习的进化 HPO 技术来减少训练模型和超参数优化所需的时间。事实证明，进化 HPO 可以通过自动收敛到最佳超参数来大幅减少总体训练时间，而无需进行大量的训练运行。 我们不断添加更多算法和功能。 AgileRL 已经包含了最先进的可进化的在线策略、离策略、离线、多智能体和上下文多臂老虎机强化学习算法以及分布式训练。 我很乐意收到您的反馈！    提交人    /u/nicku_a   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</guid>
      <pubDate>Fri, 21 Jun 2024 17:50:05 GMT</pubDate>
    </item>
    <item>
      <title>在强化学习（或者更确切地说，对于任何算法来说，机器学习）的背景下，像策略梯度这样的算法的单调改进意味着什么，为什么它是算法的重要参数。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dl0meg/what_does_monotonic_improvement_of_an_algorithm/</link>
      <description><![CDATA[我一直在阅读不同的文本，但仍然不明白真正的含义。有人可以解释一下吗。     提交人    /u/aabra__ka__daabra   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dl0meg/what_does_monotonic_improvement_of_an_algorithm/</guid>
      <pubDate>Fri, 21 Jun 2024 10:19:25 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习人形v4问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkuvm7/about_reinforcement_learning_humanoid_v4_problem/</link>
      <description><![CDATA[大家好，这是我在 colab 上为人形机器人 mujoco_humanoid.ipynb - Colab (google.com) 编写的代码（您可以在线运行）。但我不知道哪一步错了，结果就是网络没法学习机器人控制:( 真的需要帮助。    submitted by    /u/Inevitable_Sea_8466   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkuvm7/about_reinforcement_learning_humanoid_v4_problem/</guid>
      <pubDate>Fri, 21 Jun 2024 03:57:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中设置种子的实用规则</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkh0kp/practical_rules_for_setting_seed_in_rl/</link>
      <description><![CDATA[大家好！如果这个问题重复了，我很抱歉，但在阅读了许多与此相关的帖子后，我仍然找不到我的问题的答案。 假设我正在开发一个模型，其性能根据初始种子而有很大差异。 我应该固定一个种子或一组种子来比较不同特征或奖励函数的影响，还是应该始终使用随机种子和平均值运行，即使在开发模型时也是如此？ 如果是第二种情况，您如何了解性能改进是由于种子还是由于管道中的其他变化？    提交人    /u/ParfaitFinancial9765   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkh0kp/practical_rules_for_setting_seed_in_rl/</guid>
      <pubDate>Thu, 20 Jun 2024 17:15:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么重要性采样比例的期望值是1呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkfrzp/why_is_the_expected_value_of_the_importance/</link>
      <description><![CDATA[        提交人    /u/hearthstoneplayer100   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkfrzp/why_is_the_expected_value_of_the_importance/</guid>
      <pubDate>Thu, 20 Jun 2024 16:23:57 GMT</pubDate>
    </item>
    <item>
      <title>RLHF 的启动代码库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkf3yr/starter_code_repos_for_rlhf/</link>
      <description><![CDATA[大家好， 我刚刚开始研究 LLM，特别是 RLHF。我正在寻找可以作为起点的开放课程库。我发现了以下内容：  https://github.com/OpenLLMAI/OpenRLHF https://github.com/huggingface/trl https://github.com/CarperAI/trlx  所有这些似乎都与 transformers 库兼容，而该库又支持完整的开源（代码+数据，而不仅仅是权重）模型，例如 Pythia。所有这些似乎都得到了相当程度的更新。1) 和 3) 支持分布式训练。您会推荐哪一个？还有其他建议吗？ 抱歉，我的问题可能有些幼稚。我是 LLM 新手 :)    提交人    /u/South-Conference-395   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkf3yr/starter_code_repos_for_rlhf/</guid>
      <pubDate>Thu, 20 Jun 2024 15:56:11 GMT</pubDate>
    </item>
    <item>
      <title>用状态价值基线来强化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dk9lwm/reinforce_with_statevalue_baseline/</link>
      <description><![CDATA[嗨，我是 RL 的新手，我正在尝试了解如何使用状态值基线（在本例中为第二个神经网络）进行 REINFORCE。例如，在有两个玩家的棋盘游戏中，我训练一个代理对抗一个随机对手，何时应该存储奖励和价值？我应该在代理移动后才考虑奖励并估计价值，还是在对手移动后也考虑奖励并估计价值？ 此外，我不确定我是否正确理解了如何计算策略损失和价值策略损失。我的理解是，对于策略损失，我必须计算折扣奖励，从中减去值以获得优势，然后将对数概率与优势相乘以获得缩放对数概率，最后将它们全部相加（聚合）。对于价值策略，我的理解是我应该计算估计值和折扣奖励之间的 MSE 损失。 对吗？    提交人    /u/miroshuSan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dk9lwm/reinforce_with_statevalue_baseline/</guid>
      <pubDate>Thu, 20 Jun 2024 11:45:50 GMT</pubDate>
    </item>
    <item>
      <title>如果 PPO 受到稀疏奖励的影响，那么 InstructGPT 和 Learning to Summarize 是如何使其发挥作用的呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</link>
      <description><![CDATA[我见过很多关于 PPO 如何难以在稀疏奖励环境中发挥作用的讨论。在 instructGPT 和学习从人类反馈中总结的情况下，仅在一长串采样标记的最后一个标记处给予奖励。特别是对于一些涉及非常长的代数（1000 个动作范围）且最后只有一个奖励的现代 RLHF 任务 - PPO 如何在这里取得成功？我是否遗漏了优化？    提交人    /u/idioticfuse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</guid>
      <pubDate>Thu, 20 Jun 2024 01:01:39 GMT</pubDate>
    </item>
    </channel>
</rss>