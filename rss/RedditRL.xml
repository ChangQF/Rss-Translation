<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 03 Feb 2025 18:22:04 GMT</lastBuildDate>
    <item>
      <title>需要指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</link>
      <description><![CDATA[大家好， 我拥有数学学位，并选修了几门机器学习和强化学习 (RL) 课程。目前，我正在工作，但我对 RL 研究非常感兴趣。虽然我还没有太多的知识，但我在空闲时间学习 RL。 未来，我想从事 RL 研究工作，但我不知道该怎么做。我应该准备 GATE 并申请 IIT/IISc，还是应该直接申请顶尖的外国大学，尽管我没有研究经验？    提交人    /u/BigBuddy1276   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 13:24:45 GMT</pubDate>
    </item>
    <item>
      <title>结果的可重复性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</link>
      <description><![CDATA[您好！我正在尝试查找本文中提到的基于模型的 PPO 的实现：基于模型的探索的策略优化，以便重现结果并可能在我的论文中使用该架构。但似乎任何地方都没有官方实现。我已经给作者发了电子邮件，但也没有收到任何回复。 在像 AAAI 这样的大型会议上发表的论文没有任何可重现的实现，这正常吗？    提交人    /u/GamingOzz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</guid>
      <pubDate>Mon, 03 Feb 2025 13:18:10 GMT</pubDate>
    </item>
    <item>
      <title>“Kimi k1.5：使用 LLM 扩展强化学习”，Kimi 团队 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</guid>
      <pubDate>Mon, 03 Feb 2025 12:53:20 GMT</pubDate>
    </item>
    <item>
      <title>帮助消除错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</link>
      <description><![CDATA[嘿，我目前正在以深度 q 学习模型的形式训练我的第一个强化学习模型。在尝试使用 Python 中的 keras 时，我遇到了一些问题，如果有人愿意帮助我弄清楚如何解决这些问题，我将不胜感激。（它们对于我的项目来说非常具体，因此很难在 DM 之外解释😅）    提交人    /u/at_69_420   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</guid>
      <pubDate>Mon, 03 Feb 2025 11:08:58 GMT</pubDate>
    </item>
    <item>
      <title>第一届 Tinker AI 大赛优胜作品！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</link>
      <description><![CDATA[        提交人    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</guid>
      <pubDate>Mon, 03 Feb 2025 10:00:02 GMT</pubDate>
    </item>
    <item>
      <title>尝试复制普通的 k-bandits 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</link>
      <description><![CDATA[大家好， 我正在尝试实现 Barto Sutton 书中的第一个 k-Bandits 测试平台。Python 代码可在 Git 上找到，但我正尝试从头开始独立完成。 截至目前，我正在尝试生成图 2.2 中的平均奖励图。我的代码可以工作，但平均奖励图过早稳定下来，并且保持稳定，而不是像书中/git 中那样增加。我无法弄清楚我哪里做错了。 如果有人能看一下并分享一些技巧，那将非常有帮助。如果有人想运行/测试它，代码应该按原样工作。  非常感谢！ ``` 该程序实现了 k-bandit 问题的 n 次运行 import numpy as np import matplotlib.pyplot as plt bandit_reward_dist_mean = 0 bandit_reward_dist_sigma = 1 k_bandits = 10 bandit_sigma = 1 samples_per_bandit = 1000 epsilon = 0.01 def select_action(): r = np.random.randn() if r &lt; epsilon：action = np.random.randint（0，k_bandits）else：action = np.argmax（q_estimates） 返回操作 def update_action_count（A_t）：# 到目前为止已采取每个操作的次数n_action [A_t] + = 1 def update_action_reward_total（A_t，R_t）：# 到目前为止每个操作的总奖励action_rewards [A_t] + = R_t def generate_reward（mean，sigma）：# 从正态分布中为这个特定的bandit提取奖励#r = np.random.normal（mean，sigma）r = np.random.randn（）+mean# 类似于在Git repo中所做的return r def update_q（A_t，R_t）： q_estimates[A_t] += 0.1 * (R_t - q_estimates[A_t]) n_steps = 1000 n_trials = 2000 #每次试验使用一批新的老虎机运行 n_steps 所有试验中每一步的奖励矩阵 - 从零开始 rewards_episodes_trials = np.zeros((n_trials, n_steps)) for j in range(0, n_trials): #q_true = np.random.normal(bandit_reward_dist_mean, bandit_reward_dist_sigma, k_bandits) q_true = np.random.randn(k_bandits) # 尝试复制 book/git 结果 # 每个动作（老虎机）的 Q 值 - 从random q_estimates = np.random.randn(k_bandits) # 每个动作（bandit）的总奖励 - 从零开始 action_rewards = np.zeros(k_bandits) # 到目前为止每个动作已采取的次数 - 从零开始 n_action = np.zeros(k_bandits) # 每一步的奖励 - 从 0 开始 rewards_episodes = np.zeros(n_steps) for i in range(0, n_steps): A_t = select_action() R_t = generate_reward(q_true[A_t], bandit_sigma) rewards_episodes[i] = R_t  update_action_reward_total(A_t, R_t) update_action_count(A_t) update_q(A_t, R_t) rewards_episodes_trials[j,:] = rewards_episodes  所有运行中每步的平均奖励 average_reward_per_step = np.zeros(n_steps) for i in range(0, n_steps): average_reward_per_step[i] = np.mean(rewards_episodes_trials[:,i]) plt.plot(average_reward_per_step) plt.show() ```    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</guid>
      <pubDate>Mon, 03 Feb 2025 09:35:25 GMT</pubDate>
    </item>
    <item>
      <title>Vision RL 帮助和指导。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</link>
      <description><![CDATA[向聪明的人们问好。我一直在深入研究 RL，我认为那个男人跳入游泳池却撞到冰块的视频适用于我。 https://jacomoolman.co.za/reinforcementlearning/（向下滚动或直接搜索“vision”以跳过与我的问题无关的内容） 这是我迄今为止的进展。任何使用过视觉 RL 的人可能都能看出我做错了什么？我已经尝试了大约 2 个月，试图为模型提供图像而不是变量，但没有成功。    提交人    /u/TheRealMrJm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 07:01:10 GMT</pubDate>
    </item>
    <item>
      <title>这看起来像是稳定的 PPO 收敛吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</link>
      <description><![CDATA[      这看起来像稳定的 PPO 收敛吗？    提交人    /u/TopSigmaNoCap79970   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</guid>
      <pubDate>Mon, 03 Feb 2025 01:30:55 GMT</pubDate>
    </item>
    <item>
      <title>“深度研究简介”，OpenAI（基于 o3 的网页浏览/研究代理的 RL 训练）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</guid>
      <pubDate>Mon, 03 Feb 2025 01:29:39 GMT</pubDate>
    </item>
    <item>
      <title>“自我验证，人工智能的关键”，Sutton 2001（是什么让搜索发挥作用）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</guid>
      <pubDate>Sun, 02 Feb 2025 23:55:51 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 RL 来解决我的 opt. 控制问题吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iga0t5/can_i_use_rl_for_my_opt_control_problem/</link>
      <description><![CDATA[您好，我目前正在优化一个控制问题，试图将网络从随机状态转变为期望状态。 决策变量如下： - 一个包含 n 个连续变量的向量 - 一个包含 m 个二进制变量的向量 目前，我对连续变量使用标准差分进化，对二进制向量使用特殊的二进制差分进化算法。我想知道我是否可以为这个设置实现一个 RL 模型？ 更多信息：n 通常为 10-20，m 通常为 80-100。我经过大约 10000-20000 次不同的进化尝试后收敛到某种期望状态，这需要一些时间。主要是因为对于每次迭代，我都需要进行一些耗时的物理计算。对于不同的进化算法，可以并行处理这些计算，但是由于许可问题，需要花费很多钱。所以目前，我每次只能进行 1 次计算，大约需要 1 到 1.5 秒。总共需要 10000-30000 秒才能完成。 您认为我学习 RL 并尝试通过 RL 解决这个问题值得吗？请记住，RL 算法还需要为每次交互等待 1.5 秒，因为我会将其操作发送到许可软件并接收其输出。 当前流程：1）使用初始猜测启动算法 2）将所有初始猜测发送到黑盒软件 1by1 3）从黑盒软件 1by1 接收所有输出 4）启动差分进化循环 5）创建新的猜测 1by1 将它们发送到黑盒获取输出并对其进行评分 6）如果得分更高，则用新猜测替换旧猜测 7）重复 5-6 次，共 10000 次并获得最佳得分猜测    submitted by    /u/Icedkk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iga0t5/can_i_use_rl_for_my_opt_control_problem/</guid>
      <pubDate>Sun, 02 Feb 2025 22:46:09 GMT</pubDate>
    </item>
    <item>
      <title>2025 年秋季硕士/博士申请</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</link>
      <description><![CDATA[大家好！ 随着招生周期全面展开，我祝愿本周期申请的所有人好运！我正在申请，迫不及待地想去研究生院做 RL 研究（在我的国家这很少见）。 在评论中写下你申请的地方以及你最想进入的地方。也许宇宙会听到，机会就会对你有利！    提交人    /u/issyonibba   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</guid>
      <pubDate>Sun, 02 Feb 2025 19:47:59 GMT</pubDate>
    </item>
    <item>
      <title>GRPO 中的代币级优势</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig0a3h/tokenlevel_advantages_in_grpo/</link>
      <description><![CDATA[在 GRPO 损失函数中，我们看到每个输出 (o_i) 都有一个单独的优势，这是可以预料的，每个 token t 也有一个单独的优势。我有两个问题：  为什么需要 token 级优势？为什么不给输出中的所有 token 相同的优势？ 如何计算这个 token 级优势？  我这里遗漏了什么吗？从 Hugginface TRL 的实现来看，他们似乎没有执行令牌级别的优势： https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py#L507    提交人    /u/AdministrativeRub484   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig0a3h/tokenlevel_advantages_in_grpo/</guid>
      <pubDate>Sun, 02 Feb 2025 15:57:43 GMT</pubDate>
    </item>
    <item>
      <title>“迈向通用无模型强化学习”，Fujimoto 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</guid>
      <pubDate>Sun, 02 Feb 2025 12:02:03 GMT</pubDate>
    </item>
    <item>
      <title>我对学习 RL 的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</link>
      <description><![CDATA[我读过 Sutton 和 Barto 的书，有时我发现很难理解其中的一些概念。然后，我开始探索这个资源。现在，我真正理解了价值迭代和其他基本概念背后的含义。我认为这本书应该在 Sutton 和 Barto 的书之前或同时阅读。这真是一本很棒的书！    提交人    /u/demirbey05   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</guid>
      <pubDate>Sun, 02 Feb 2025 07:59:42 GMT</pubDate>
    </item>
    </channel>
</rss>