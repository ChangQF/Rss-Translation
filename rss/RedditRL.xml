<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 05 Dec 2024 09:19:26 GMT</lastBuildDate>
    <item>
      <title>强化学习适合解决刽子手游戏吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6ynzs/is_reinforcement_learning_suitable_for_solving/</link>
      <description><![CDATA[在过去的 3 周里，我一直在使用 RL 迭代不同的策略来解决 Hangman 游戏，但到目前为止，我的准确率并不高（1000 场游戏中解决的不到 100 场）。说实话，我对 RL 的概念还很陌生。我一开始使用 Deep Q-Learning 框架来解决这个问题。我的方法相当简单：  将训练单词的隐藏状态转换为两个二进制向量（一个向量大小为 26，用于跟踪猜测的字母，另一个向量大小为 45，用于跟踪正确猜测的字母的位置），并将其作为输入传递给神经网络，同时传递剩余的猜测次数和要发现的隐藏字母数量。总输入大小为 73。 以 1 的探索率初始化训练循环，随着训练的进行，探索率衰减为 0.1。探索部分的工作原理是，使用训练单词列表，根据隐藏单词的长度，从预先计算的字典中选择最常见的字母。 对每个单词开始训练过程，游戏一直进行到代理猜出正确的单词或猜错 6 次为止，计算损失并在每次猜测后更新参数。对每个单词重复此过程。使用 q 学习网络获得的下一步计算损失。为在每个优化步骤中完成的重放设置了批量大小。 然后将训练好的网络用于一组看不见的验证词，然后使用这些验证词来衡量准确性。 我使用的奖励结构是每个正确字母 +1，每个错误字母 -1，如果单词猜对了则 +10，如果单词猜错了则 -10。然后我对其进行了更新，以便连续猜对字母可获得更高的奖励，连续猜错可获得更高的负奖励。 状态和奖励计算的更新是通过单独的 Hangman 模块完成的。每次猜测每个单词后都会调用该模块。  训练从 1000 场游戏的 10% 准确率开始，然后随着探索率的衰减，准确率降低到 1%，这也反映在验证中。 现在我觉得我做错了。但你们认为这种方法合适吗？    提交人    /u/hpnr0724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6ynzs/is_reinforcement_learning_suitable_for_solving/</guid>
      <pubDate>Thu, 05 Dec 2024 02:38:33 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习中的奖励分配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6u396/reward_distribution_in_offline_rl/</link>
      <description><![CDATA[我正在使用保守 Q 学习和 SAC 模型。我的奖励分布非常不平衡：所有奖励都在 0 到 1 之间，95% 低于 0.02，40% 低于 0.001。 我应该如何转换它？我之所以问这个问题，是因为我的预测 Q 值为负：这没有意义，因为所有奖励都是正的。我已经排除了所有错误的可能性（我认为是这样）。我也得到了带有非常小惩罚的负 q 值（以及没有惩罚：我确实看到 q 函数在潜水前很长一段时间都是正的：潜水与 q 值方差的增加有关）。任何指针都值得赞赏！    提交人    /u/electricsheep123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6u396/reward_distribution_in_offline_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 23:06:15 GMT</pubDate>
    </item>
    <item>
      <title>“指南：实时人形代理”，Zhang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6prin/guide_realtime_humanshaped_agents_zhang_et_al_2024/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6prin/guide_realtime_humanshaped_agents_zhang_et_al_2024/</guid>
      <pubDate>Wed, 04 Dec 2024 20:08:16 GMT</pubDate>
    </item>
    <item>
      <title>LoRA研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6lepr/lora_research/</link>
      <description><![CDATA[最近，我发现关于 LoRA 替代品的论文激增。您认为人们正在探索哪些研究方向？ 您认为它有可能以某种方式与 RL 相结合吗？    提交人    /u/KevinBeicon   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6lepr/lora_research/</guid>
      <pubDate>Wed, 04 Dec 2024 17:15:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Q_Learning 算法无法正常学习？（更新）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6l4jl/why_is_my_q_learning_algorithm_not_learning/</link>
      <description><![CDATA[嗨，这是我几天前的另一篇文章的后续文章 ( https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/ ) 我阅读了您的评论并且 u/scprotz 告诉我即使是德文的，拥有代码也会很有用。这是我的代码：https://codefile.io/f/F8mGtSNXMX 我通常不会在网上分享我的代码，所以如果网站不是最好的选择，我很抱歉。不同的类通常位于不同的文档中（您可以在导入中看到），我运行 Spiel（即游戏）文件来启动程序。我希望这会有所帮助，如果您发现任何看起来奇怪或不正确的东西，请发表评论，因为尽管搜索了几个小时，我还是没有找到问题所在。    提交人    /u/_waterstar_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6l4jl/why_is_my_q_learning_algorithm_not_learning/</guid>
      <pubDate>Wed, 04 Dec 2024 17:04:30 GMT</pubDate>
    </item>
    <item>
      <title>预训练 VAE 与强化学习期间训练的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6imk0/difference_between_pretrained_vaes_versus/</link>
      <description><![CDATA[      我的目标是开发一个用于 Carla 模拟器进行自动驾驶的代理。为了实现这一点，我实现了 Soft Actor-Critic (SAC) 算法。在将图像输入 SAC 算法之前，我使用了变分自动编码器 (VAE)。 VAE 没有经过预训练，因为我假设它会在强化学习过程中得到训练和改进。这种方法有缺陷吗？如果有，为什么，如何改进？我采取这一步骤的理由是受到观察 DQN 中 CNN 在 Frozen Lake 环境中的使用情况的启发。我的代码可以在 GitHub 上供感兴趣的人使用：https://github.com/b-gtr/Soft-Actor-Critic 这里还有一个简单的代码工作原理说明：    提交人    /u/Fair_Device_4961   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6imk0/difference_between_pretrained_vaes_versus/</guid>
      <pubDate>Wed, 04 Dec 2024 15:24:25 GMT</pubDate>
    </item>
    <item>
      <title>为 RL 代理编码棋盘游戏 Cascadia 中的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6ggjq/encode_actions_from_board_game_cascadia_for_rl/</link>
      <description><![CDATA[      嘿 :) 我目前正在为我的 Cascadia 实现开发一个代理。游戏逻辑是用 Java 实现的，我创建了 Rest Endpoints 来从外部操纵游戏。因此我可以开始游戏、获取当前游戏状态、重置剧集、计算奖励、返回合法行动等等。 我的计划是使用一些众所周知的 Python 框架（我想到稳定基线、tensorforce 或 RL_Coach）训练代理，而我面临的挑战是编码代理在每个状态下可以采取的可能/合法行动。 我实现的图片显示了代理可以在其上执行操作的棋盘。在每个回合，代理/玩家可以选择四对中的一对（由景观和动物组成）并将它们放置在他的棋盘上。请注意，只有在没有其他动物已经放置并且景观允许该动物类型的情况下，动物才能放置在景观瓷砖上。新景观瓷砖的合法位置在图片中标记为绿色。 已经放置的动物用绿色圆圈标记。 为简单起见，我可以确保代理始终有 3 个瓷砖可以放置动物，并且他可以选择的对中至少有一对包含他可以实际放置在棋盘上的动物。我甚至可以想出一个解决方案，其中选择部分是随机的，代理必须使用他得到的任何一对。 是否可以训练单个代理来执行这 3 个连续的任务（选择对 -&gt; 放置景观 -&gt; 放置动物）？如果是这样，我该如何编码动作空间以将其提供给代理？ https://preview.redd.it/m4axweoj5u4e1.jpg?width=1011&amp;format=pjpg&amp;auto=webp&amp;s=d0b7cb547ad36574b9ad82998ab8966b72b20351    submitted by    /u/ItchyRoyal212   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6ggjq/encode_actions_from_board_game_cascadia_for_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 13:51:07 GMT</pubDate>
    </item>
    <item>
      <title>帮助 - 扫雷 RL 执行重复操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6e6xo/help_minesweeper_rl_executing_repeated_actions/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6e6xo/help_minesweeper_rl_executing_repeated_actions/</guid>
      <pubDate>Wed, 04 Dec 2024 11:49:48 GMT</pubDate>
    </item>
    <item>
      <title>在多代理环境中训练代理的最佳方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6dvqo/best_way_to_train_agents_in_multi_agent/</link>
      <description><![CDATA[我正在开展一个国际象棋 RL 项目，其中 2 个使用不同算法训练的代理相互对抗。我想知道训练代理的最佳方法是什么。我应该让他们互相对抗，分别训练代理对抗对手的随机动作，还是让他们分别训练，让游戏中的两个对手都使用相同的算法。并且建议会很有帮助     提交人    /u/Livid-Ant3549   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6dvqo/best_way_to_train_agents_in_multi_agent/</guid>
      <pubDate>Wed, 04 Dec 2024 11:29:40 GMT</pubDate>
    </item>
    <item>
      <title>教育部研究主任</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h6dtv2/moe_rl/</link>
      <description><![CDATA[是否可以将混合专家 (MoE) 与强化学习 (RL) 结合起来？训练一个可以根据输入选择激活哪个或哪些专家的代理是否有意义？ 我有一个更复杂的想法：我想将 MoE 和 RL 与低秩自适应 (LoRA) 集成在一起。计划是拥有多个 LoRA 模块，并让代理根据输入识别最合适的模块。我打算将这种方法应用于各种 NLP 任务。这有意义吗？    提交人    /u/KevinBeicon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h6dtv2/moe_rl/</guid>
      <pubDate>Wed, 04 Dec 2024 11:26:05 GMT</pubDate>
    </item>
    <item>
      <title>基于 Symphony S2 的 DDPGII</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h69m82/ddpgii_based_on_symphony_s2/</link>
      <description><![CDATA[        提交人    /u/Timur_1988   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h69m82/ddpgii_based_on_symphony_s2/</guid>
      <pubDate>Wed, 04 Dec 2024 06:19:47 GMT</pubDate>
    </item>
    <item>
      <title>除了样本复杂性之外，还有其他理由使用基于模型的 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</link>
      <description><![CDATA[当我们能够进行大规模环境并行化时，使用基于模型的算法是否真的有意义？ 基本上，我想知道是否存在像 DreamerV3 这样的算法可以解决而 PPO 无法解决的环境？例如，DreamerV3 论文表明，PPO 和 IMPALA 无法解决最困难的 Minecraft 任务，但如果有大规模计算，PPO 最终会解决这些任务吗？ 除了降低样本复杂度之外，还有其他理由使用基于模型的算法吗？    提交人    /u/vandelay_inds   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h64fql/are_there_reasons_to_use_model_based_rl_beyond/</guid>
      <pubDate>Wed, 04 Dec 2024 01:39:56 GMT</pubDate>
    </item>
    <item>
      <title>“BALROG：对游戏上的 Agentic LLM 和 VLM 推理进行基准测试”，Paglieri 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h63tu6/balrog_benchmarking_agentic_llm_and_vlm_reasoning/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h63tu6/balrog_benchmarking_agentic_llm_and_vlm_reasoning/</guid>
      <pubDate>Wed, 04 Dec 2024 01:11:04 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型的算法合谋”，Fish 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h63tjd/algorithmic_collusion_by_large_language_models/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h63tjd/algorithmic_collusion_by_large_language_models/</guid>
      <pubDate>Wed, 04 Dec 2024 01:10:40 GMT</pubDate>
    </item>
    <item>
      <title>如何处理复杂、嵌套的动作空间。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h5zgvq/how_to_deal_with_complex_nested_action_spaces/</link>
      <description><![CDATA[我正在开发一个可以与 miniwob++ 体育馆环境配合使用的代理。我从未使用过像这样的非平面动作空间，并且想知道是否有人对制定它的最佳方法有任何指导，因为根据要采取的操作，需要不同的参数。具体来说，如果相关的话，尝试使用 Pytorch 来解决这个问题。    提交人    /u/m_js   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h5zgvq/how_to_deal_with_complex_nested_action_spaces/</guid>
      <pubDate>Tue, 03 Dec 2024 21:59:50 GMT</pubDate>
    </item>
    </channel>
</rss>