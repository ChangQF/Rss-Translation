<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title></title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description></description>
    <lastBuildDate>Wed, 22 Jan 2025 18:22:49 GMT</lastBuildDate>
    <item>
      <title>强化学习算法的问题/解决方案参考指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7hpnm/a_problemsolution_reference_guide_for_rl/</link>
      <description><![CDATA[在学习 RL 课程时，我为几种算法创建了一个参考，并简要描述了它们解决了哪些限制。示例： 问题：SARSA 将 q 值推向当前策略，但理想情况下，我们想要的是最优值。 解决方案：在 TD 目标计算中使用最佳操作 -&gt; Q 学习 也许其他人会发现它很有用！可在 https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference 获得    提交人    /u/jac08_h   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7hpnm/a_problemsolution_reference_guide_for_rl/</guid>
      <pubDate>Wed, 22 Jan 2025 18:14:36 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7h25q/shortening_the_horizon_in_reinforce/</link>
      <description><![CDATA[大家好。我正在研究对具有动态状态（生成的状态是针对先前状态采取行动的结果）的建筑物进行强化学习，并且我正在使用纯 REINFORCE 算法并存储（s、a、r）转换。如果我想将一个时期分成几个情节，比如 10 个情节，（先前：一次运行 4000 个时间步，然后参数更新 --&gt;现在：400 个时间步，更新，另一个 400 个时间步，更新...），除了更改存储转换操作和学习函数的位置外，我还应该注意哪些事情才能正确进行此更改？您能告诉我可以学习的任何来源吗？谢谢。（我的 NN 框架在 Tensorflow 1.10 中）。    提交人    /u/Araf_fml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7h25q/shortening_the_horizon_in_reinforce/</guid>
      <pubDate>Wed, 22 Jan 2025 17:48:30 GMT</pubDate>
    </item>
    <item>
      <title>硕士学位决定</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7eiqz/masters_degree_decision/</link>
      <description><![CDATA[如果我有兴趣深入了解强化学习，有人能告诉我在欧洲哪里攻读硕士学位会更有益吗？    提交人    /u/Ok-Engineering4612   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7eiqz/masters_degree_decision/</guid>
      <pubDate>Wed, 22 Jan 2025 16:04:45 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7cxi6/td3_reward_not_increasing_over_time/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7cxi6/td3_reward_not_increasing_over_time/</guid>
      <pubDate>Wed, 22 Jan 2025 14:57:18 GMT</pubDate>
    </item>
    <item>
      <title>可重复性和建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i784rl/reproducability_and_suggestions/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i784rl/reproducability_and_suggestions/</guid>
      <pubDate>Wed, 22 Jan 2025 10:25:38 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7717b/pusher_task_not_learning/</link>
      <description><![CDATA[我正在尝试在 mujoco 推送器环境中训练一个模型，但它不起作用。基本上，我从 mujoco github repo 获得了推送器类并做了一些小改动。我试图实现的是让推送器将 3 个对象推送到 3 个不同的目标。这些对象一次出现一个，所以当第一个对象被推送到目标时，第二个对象就会出现，依此类推。所以我对 mujoco 提供的类做的唯一修改是我添加了在视图中更改要推送对象的机制。我尝试了 PPO 和 SAC，时间步长为 100 万，奖励仍然为负。这看起来像是一项简单的任务，但它不起作用    提交人    /u/Latinotech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7717b/pusher_task_not_learning/</guid>
      <pubDate>Wed, 22 Jan 2025 09:01:57 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6vye4/training_on_documents_about_reward_hacking/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6vye4/training_on_documents_about_reward_hacking/</guid>
      <pubDate>Tue, 21 Jan 2025 22:56:45 GMT</pubDate>
    </item>
    <item>
      <title>可微分模拟资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6h3xh/resources_for_differentiable_simulation/</link>
      <description><![CDATA[大家好， 我是 RL 方法控制腿式机器人的新博士生。最近，我看到使用可微分模拟训练 RL 控制代理的趋势正在蓬勃发展。我还没有理解这个新概念，例如，DiffSim 到底是什么，它与序数物理引擎有何不同，等等。因此，我很想有一些关于这个主题基础知识的材料。你有什么建议吗？非常感谢你的帮助！    提交人    /u/Mountain_Deez   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6h3xh/resources_for_differentiable_simulation/</guid>
      <pubDate>Tue, 21 Jan 2025 12:05:56 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6fx2u/insights_on_journal_paper_submission_for/</link>
      <description><![CDATA[大家好！我和我的朋友使用强化学习，对环境污染监测的一个用例进行了研究，即在我们自己创造的不同国家和地区的环境中繁殖动物。无论我们提交到哪里，审阅者都会很欣赏，但最终，由于他们不了解用例和其他内容，它会导致拒绝。我们也没有任何基础论文可以参考，但是是的，到目前为止，我们尽了最大努力在纸上制定公式，并尽了最大努力解释整个决策支持系统。到目前为止，我们在审查过程中收到了 4 次拒绝，由于范围之外的原因收到了 7 次拒绝。在将其提交到其他地方之前，我需要一些指示，以便在期刊出版物上发表（根据学术规定，必须是期刊）。  提前为没有全心全意披露这项工作而道歉。我的问题是针对所有非传统的、间接的、新颖的、从未尝试过的作品……     提交人    /u/Miserable_Ad2265   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6fx2u/insights_on_journal_paper_submission_for/</guid>
      <pubDate>Tue, 21 Jan 2025 10:45:59 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6ehaj/deep_reinforcement_learning/</link>
      <description><![CDATA[我有两本书  Richard S. Sutton 和 Andrew G. Barto 的《强化学习》  Miguel Morales 的《深度强化学习》 我发现两本书的内容表格都差不多。我即将自学 DQN、Actor Critic 和 PPO，但很难确定书中的重要主题。第一本书看起来更侧重于表格方法（？），对吗？  第二本书有几个章节和子章节，但我需要有人帮助指出里面的重要主题。我是一名普通软件工程师，在业余时间很难逐一消化所有概念。  有人可以帮忙指出哪个子主题很重要吗？如果我的想法正确，第一本书更侧重于表格方法？    由    /u/Best_Fish_2941  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6ehaj/deep_reinforcement_learning/</guid>
      <pubDate>Tue, 21 Jan 2025 08:56:48 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i65y2f/the_problem_with_reasoners_praying_for_transfer/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i65y2f/the_problem_with_reasoners_praying_for_transfer/</guid>
      <pubDate>Tue, 21 Jan 2025 00:35:02 GMT</pubDate>
    </item>
    <item>
      <title>高维连续动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i606pp/high_dimensional_continous_action_spaces/</link>
      <description><![CDATA[考虑实施 DDPG，但我可能需要多达 96 个动作输出，因此动作空间为 R ^ 96。我正在尝试优化 8 个形式为 I(t) 的函数，I: R -&gt; R，以达到某些基准。我考虑这样做的方法是将输入空间离散化为块，因此如果每个输入有 12 个块，我需要有 12 * 8 = 96 个实数输出。这是否合理可行？    提交人    /u/MilkyJuggernuts   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i606pp/high_dimensional_continous_action_spaces/</guid>
      <pubDate>Mon, 20 Jan 2025 20:30:00 GMT</pubDate>
    </item>
    <item>
      <title></title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5rcvu/facing_poor_performance_with_dqn_for_mission/</link>
      <description><![CDATA[      嗨！我是 RL 的初学者，我一直在学习 dqn 并致力于使用它来优化工业工厂中的任务分配。 我们有几个机器人（AGV）和任务。每个任务都有一系列要遵循的步骤。例如，任务 1 的步骤 1 可能需要从标签 1 移动到标签 2，这意味着我们需要阻止其他机器人访问这两个标签以避免碰撞。机器人必须访问的步骤顺序是预定义的。我将状态构建为一个列表，其中包括： - 空闲机器人， - 当前正在执行任务的机器人， - 停止服务的机器人， - 正在充电的机器人， - 未请求的任务， - 已请求的任务， - 正在进行的任务， - 标签可用性， - 机器人位置， - 每个机器人的任务步骤（默认为 1）， - 所有机器人的电池电量。 例如，如果有 4 个机器人和 4 个任务，状态可能如下所示： [[0, 1, 1, 1], [0, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 0], [0, 1, 1, 1], [1, 0, 0, 0], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2], [1, 1, 1, 1], [0.71, 0.34, 0.6, 0.4]] 动作以对的形式表示，如 (&#39;1&#39;, &#39;4&#39;)，表示“将任务 4 分配给机器人1&quot; 如果某个动作被认为不可行（例如，机器人已经很忙或任务正在进行中），它​​会触发当前情节的终止条件。步骤如下：  惩罚应用：分配 -80 的惩罚以阻止不可行的操作。 状态处理：下一个状态与当前状态保持相同，因为未执行任何有效操作。 经验存储：元组（当前状态、所选操作的索引、惩罚、下一个状态）被添加到重放缓冲区，允许代理从错误中吸取教训。 情节终止：当前情节的循环结束，系统继续下一个情节。  奖励函数：  电池电量（10%）：奖励当前执行任务的机器人和选定的机器人更高的平均电池电量。 接近任务（20%）：奖励执行任务的机器人和选定的机器人更短的距离，以减少行进时间。 任务持续时间（70%）：优先考虑较短的完成时间以提高效率。 最终状态奖励：如果完成所有任务，则增加最小化完工时间的奖励。  然后将元组（状态、所选动作的索引、奖励、下一个状态）添加到缓冲区。 尽管测试了不同的激活函数和参数，但模型表现不佳。结果是“随机的”或者预测的动作是重复的（对我测试的每个随机状态都得到相同的预测） https://preview.redd.it/e260toyssaee1.png?width=1197&amp;format=png&amp;auto=webp&amp;s=4357c60792c99cba432da5ca57ddab2e7d1df413 我不确定是什么原因导致的这种情况或如何改进它，有什么想法吗:&#39;)？。如果对我的实施有任何不清楚的地方，请告诉我！    提交人    /u/Dazzling-Prize3371   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5rcvu/facing_poor_performance_with_dqn_for_mission/</guid>
      <pubDate>Mon, 20 Jan 2025 14:29:26 GMT</pubDate>
    </item>
    <item>
      <title>Pong 的策略梯度代理没有学习（帮助）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5q8kp/policy_gradient_agent_for_pong_is_not_learning/</link>
      <description><![CDATA[嗨，我是强化学习的新手，正在尝试使用策略梯度法训练我的代理玩 Pong。我参考了深度强化学习：从像素开始玩 Pong。和使用 Cartpole 和 PyTorch 的策略梯度。因为我想学习 Pytorch，所以我决定使用它，但我的实现似乎缺少一些东西。我尝试了很多东西，但它所做的只是学习一次反弹然后停止（之后它什么也不做）。我认为问题出在我的损失计算上，所以我尝试改进它，但它仍然重复相同的过程。 这是 git：使用 pytorch 的 Pong 的 RL    提交人    /u/nightsy-owl   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5q8kp/policy_gradient_agent_for_pong_is_not_learning/</guid>
      <pubDate>Mon, 20 Jan 2025 13:33:42 GMT</pubDate>
    </item>
    <item>
      <title>偏差和方差：萨顿痛苦教训的重演</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i56xkh/bias_and_variance_a_redux_of_suttons_bitter_lesson/</link>
      <description><![CDATA[原始形式 20 世纪 90 年代，计算机开始在国际象棋比赛中击败人类大师。许多人研究了这些国际象棋代理所使用的技术，并谴责道：“它只是机械地死记硬背所有动作。这不是真正的智能！”  旨在模仿人类认知某些方面的手工算法将始终赋予 AI 系统更高的性能。而且这种性能提升将是暂时的。随着更强大的计算能力的涌入，从长远来看，依赖“无意识”深度搜索或大量数据（CONV 网络）的算法将胜过它们。  Richard Sutton 将此描述为一个惨痛教训，因为他声称，过去 70 年的 AI 研究就是对此的证明。  统计形式 2022 年夏天，牛津大学和伦敦大学学院的研究人员发表了一篇足以包含章节的论文。这是一项关于因果机器学习的调查。第 7 章涵盖了因果强化学习的主题。在那里，Jean Kaddour 和其他人提到了 Sutton 的苦涩教训，但它以新的眼光出现——通过统计和概率的观点进行反映和过滤。   我们将两个社区关注点不同的原因归因于各自处理的应用类型。绝大多数关于现代 RL 的文献评估了能够生成大量数据的合成数据模拟器上的方法。例如，流行的 AlphaZero 算法假设可以访问棋盘游戏模拟，允许代理在不受数据量限制的情况下玩许多游戏。它的一项重要创新是 tabula rasa 算法，它具有更少的手工制作知识和特定领域的数据增强。有些人可能会认为 AlphaZero 证明了 Sutton 的惨痛教训。从统计学的角度来看，它大致表明，在给定更多计算和训练数据的情况下，具有低偏差和高方差的通用算法优于具有高偏差和低方差的方法。  你会说这反映在你自己的研究吗？在实践中，具有低偏差和高方差的算法是否优于高偏差低方差算法？ 你的想法？    http://www.incompleteideas.net/IncIdeas/BitterLesson.html  https://arxiv.org/abs/2206.15475     提交人    /u/moschles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i56xkh/bias_and_variance_a_redux_of_suttons_bitter_lesson/</guid>
      <pubDate>Sun, 19 Jan 2025 19:38:55 GMT</pubDate>
    </item>
    </channel>
</rss>