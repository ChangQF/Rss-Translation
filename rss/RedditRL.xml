<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 23 Nov 2024 15:16:12 GMT</lastBuildDate>
    <item>
      <title>证明 v∗(s) = max(a∈A(s)) qπ∗(s,a)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxxwkw/proof_of_vs_maxaas_qπsa/</link>
      <description><![CDATA[      大家好，我正在研究 Sutton &amp; Barto 的书。在推导最佳状态值函数的贝尔曼方程时，作者从那里开始：  https://preview.redd.it/nm2284t13n2e1.png?width=369&amp;format=png&amp;auto=webp&amp;s=f73ef50c74d48b1d7032f068565acf7b2ffcc562 我以前没见过这样的东西。我们如何证明这个等式？     由    /u/demirbey05 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxxwkw/proof_of_vs_maxaas_qπsa/</guid>
      <pubDate>Sat, 23 Nov 2024 11:48:21 GMT</pubDate>
    </item>
    <item>
      <title>帮我创建一个关于如何选择强化学习算法的决策树</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxwwfd/help_me_create_a_decision_tree_about_how_to/</link>
      <description><![CDATA[      嘿！我是一名大学教授，我想在未来几年创建一个强化学习专业化课程。 我设法理解了各种经典算法，但我真的不知道在什么时候使用哪一种。我正在尝试在 chatgpt 的帮助下创建一个决策树。我可以得到您的一些评论和更正吗？    提交人    /u/Dougdaddyboy_off   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxwwfd/help_me_create_a_decision_tree_about_how_to/</guid>
      <pubDate>Sat, 23 Nov 2024 10:41:10 GMT</pubDate>
    </item>
    <item>
      <title>最近有任何关于基础 RL 改进的研究吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxudhp/any_research_regarding_the_fundamental_rl/</link>
      <description><![CDATA[我一直在 Google Scholar 上关注几位最负盛名的 RL 研究人员，我注意到他们中的许多人近年来已将重点转移到与 LLM 相关的研究上。 哪篇论文最引人注目，推动了 RL 的根本性改进？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxudhp/any_research_regarding_the_fundamental_rl/</guid>
      <pubDate>Sat, 23 Nov 2024 07:40:34 GMT</pubDate>
    </item>
    <item>
      <title>“对未知情况和环境的元认知 (MUSE)”，Valiente & Pilly 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxno5j/metacognition_for_unknown_situations_and/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxno5j/metacognition_for_unknown_situations_and/</guid>
      <pubDate>Sat, 23 Nov 2024 01:07:37 GMT</pubDate>
    </item>
    <item>
      <title>“Marco-o1：面向开放式解决方案的开放推理模型”，赵等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxngh4/marcoo1_towards_open_reasoning_models_for/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxngh4/marcoo1_towards_open_reasoning_models_for/</guid>
      <pubDate>Sat, 23 Nov 2024 00:57:31 GMT</pubDate>
    </item>
    <item>
      <title>关于 Wordle 性能不佳的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxdv1y/advice_regarding_poor_performance_on_wordle/</link>
      <description><![CDATA[大家好， 我正在寻找有关如何处理这个强化学习问题的建议。我正在尝试教一个编码器转换器模型玩 wordle。它是基于字符的，所以有 26 个标记 + 5 个特殊标记。输入是棋盘空间，因此它可以访问以前的猜测和反馈，以及显示猜测开始/结束位置等的特殊标记。 我目前使用的算法是 PPO，我已经将游戏简化为只需要猜测一个单词的极其简单的场景，我预计这会非常容易（但是由于我的 RL 知识有限，显然我搞砸了）。 我正在寻找有关在哪里寻找此问题根源的建议。该模型确实“最终”赢了一两次，但它似乎并没有停留在那里。此外，它似乎只能持续猜测两个或三个字母。 示例。目标单词是 Amble 该模型可以一致地猜测“aabak”，围绕 an 和 b 的逻辑结果是合理的，因为奖励结构会支持该猜测。我不知道为什么 k 会得到强化，或者为什么其他字母不那么普遍。 此外，我尝试了教师强制，即强制模型做出正确的猜测并获胜，但无济于事。有什么建议吗？ 编辑：此外，该游戏是“可赢的”，我创建了伪游戏并在这些游戏上训练了模型。不是真正的离线 RL，因为我使用了 CE 损失。但是，在模型已经训练过的单词上，它的表现足够好，即使是它没有见过的单词，它的表现也不错，足以展示对模式的“理解”。    提交人    /u/ur_a_glizzy_gobbler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxdv1y/advice_regarding_poor_performance_on_wordle/</guid>
      <pubDate>Fri, 22 Nov 2024 17:54:05 GMT</pubDate>
    </item>
    <item>
      <title>关于简单单过程囊实验的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxb5rw/question_regarding_simple_single_process_sac/</link>
      <description><![CDATA[即使我设置了正确的超参数并按照论文中的说法制定公式，仍然不足以相信代理会实现目标吗？ 奖励缩放是否必要？例如，对于半cheeath？    提交人    /u/Round_Apple2573   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxb5rw/question_regarding_simple_single_process_sac/</guid>
      <pubDate>Fri, 22 Nov 2024 16:01:30 GMT</pubDate>
    </item>
    <item>
      <title>我的 ML-Agents 代理变得越来越笨，我的想法已经用完了。我需要帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx50wm/my_mlagents_agent_keeps_getting_dumber_and_i_am/</guid>
      <pubDate>Fri, 22 Nov 2024 10:42:24 GMT</pubDate>
    </item>
    <item>
      <title>灾害管理中的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</link>
      <description><![CDATA[最近，我深入研究了 RL 的灾害管理，并阅读了几篇相关论文。许多论文都提到了与之相关的算法，但没有以某种方式对其进行模拟。是否有任何平台具有与 RL 相关的模拟来展示其应用？此外，如果您有关于此方面的任何其他优秀论文的信息，请提及。     提交人    /u/SuitSecret6497   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gx2lh8/rl_for_disaster_management/</guid>
      <pubDate>Fri, 22 Nov 2024 07:40:51 GMT</pubDate>
    </item>
    <item>
      <title>策略梯度公式检查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwujcm/policy_gradient_formulas_check/</link>
      <description><![CDATA[      你好， 我正在写关于 RL 中的策略梯度方法，我对这些方程式有疑问。我理解目标是最大化目标函数 J(θ) 的值，即给定策略 (πθ) 的轨迹 (τ) 的总回报。这给了我们表达式 J(θ) = E τ∼πθ [R(τ)]。 从那里并使用梯度，我们可以推断出表达式 ∇θ J(θ) = E τ∼πθ [∑t ∇θ log πθ(at|st) R(τ)]。 我的问题是这些算法的以下目标函数是否正确（第一个是 REINFORCE）： https://preview.redd.it/prqve7kfhc2e1.png?width=754&amp;format=png&amp;auto=webp&amp;s=76e48fb82eb0ac1710352705b1c84c3869453d39 如果您能提供任何关于改进或以其他方式表达这些功能的建议，我将不胜感激。    提交人    /u/Street-Vegetable-117   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwujcm/policy_gradient_formulas_check/</guid>
      <pubDate>Fri, 22 Nov 2024 00:11:18 GMT</pubDate>
    </item>
    <item>
      <title>适用于 ML/AI/RL 的 Blue Sky 研究员入门包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</link>
      <description><![CDATA[大家好，许多研究人员正在加入 Blue Sky，而且似乎进展顺利，所以我想我会留下一些研究人员“入门包”供大家关注。欢迎随意发布您自己的内容 :)  RL：https://bsky.app/3WPHcHg AI：https://bsky.app/SipA7it NLP：https://bsky.app/SngwGeS ML 理论：https://bsky.app/21nFz12 AI 机器人：https://bsky.app/DfAoaJ1 贝叶斯 ML：https://bsky.app/2Bqtn6T 人工智能中的女性：https://bsky.app/LaGDpqg 人工智能和新闻：https://bsky.app/5sFqVNS 科学人工智能：https://bsky.app/JeFdryY  医学人工智能：https://bsky.app/N5UUARF     提交人    /u/secondterm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwqp7s/blue_sky_researcher_starter_packs_for_mlairl/</guid>
      <pubDate>Thu, 21 Nov 2024 21:22:12 GMT</pubDate>
    </item>
    <item>
      <title>如何开始机器人操作器的强化学习研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</link>
      <description><![CDATA[你好， 我是一名研究生，对应用人工智能技术（特别是强化学习）来控制机器人操纵器（机械臂）感兴趣。 为了做到这一点，我不知道从哪里开始学习并决定研究主题。  有哪些基础论文和资源可以帮助我了解这个领域？ 有哪些最近的评论或调查论文可以帮助我了解该领域的现状？ 或者，为了研究人工智能机器人技术，我应该阅读哪些论文？   如有任何建议或意见，我们将不胜感激！ 谢谢！ 使用 DeepL.com （免费版） 进行翻译   提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 21 Nov 2024 18:59:33 GMT</pubDate>
    </item>
    <item>
      <title>帮我用 Unity3D 制作这辆 DDPG 自动驾驶汽车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</link>
      <description><![CDATA[我被这个项目困住了，我不知道我哪里做错了，可能是在脚本中，也可能是在 Unity 中。请帮助我解决和调试问题。请直接给我发私信，获取脚本和更多信息。    提交人    /u/pendalkumar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</guid>
      <pubDate>Thu, 21 Nov 2024 13:41:59 GMT</pubDate>
    </item>
    <item>
      <title>另一个调试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</link>
      <description><![CDATA[大家好， 我正在解决声音领域中具有连续动作的问题。 该模型是一个表示声音的 CNN。表示与一些参数一起被输入到 MLP 以获得值和动作。 在研究损失函数（在我们的例子中是奖励）之后，它是参数和动作的凸函数。我的意思是，对于给定的参数 + 声音，作为动作函数的奖励信号是凸的。 不幸的是，我们偶然发现了一个可以实现收敛的网络参数的良好初始化。问题是模型几乎一直都不会收敛。 如何调试问题的根源？我是否只需要等待足够长的时间？我是否扩大了模型？ 谢谢 编辑：我意识到我没有指定我正在使用的算法。PPO，A2C，Reinforce，OptionCritic，PPOC。 所有这些算法的作用本质上都是相同的。    提交人    /u/sagivborn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</guid>
      <pubDate>Thu, 21 Nov 2024 09:43:42 GMT</pubDate>
    </item>
    <item>
      <title>RLtools：最快的深度强化学习库（C++；仅标头；无依赖项）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</link>
      <description><![CDATA[        提交人    /u/jonas-eschmann   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</guid>
      <pubDate>Wed, 20 Nov 2024 17:00:36 GMT</pubDate>
    </item>
    </channel>
</rss>