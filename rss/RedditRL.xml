<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 23 Jun 2024 01:08:13 GMT</lastBuildDate>
    <item>
      <title>自适应四轴飞行器网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</link>
      <description><![CDATA[👋 嘿，我正在尝试想出一个四轴飞行器无人机自主控制的解决方案。 到目前为止，我设法让它使用简单的 MLP + PPO 进行航点导航。 这很有效，特别是在域随机化的情况下，但如果突然刮起一阵风会发生什么？四轴飞行器的重量超过了它在模拟中训练的重量？螺旋桨损坏，其中一个电机产生的推力减小？ 通过所有这些示例，我的问题是无人机无法适应变化。我想创建一个能够适应任何无法解释的外部干扰的网络。 我有两个计划，一个是将 LSTM 与 PPO 结合使用，因此代理会保留飞行数据。 我的另一个想法是使用我用于训练的模拟器来获取无人机在当前时间步长的预期速度/姿态/位置，并将其与我从传感器获得的实际值一起输入到策略中。 我希望更有经验的人可以对我的想法提供反馈，说实话我有点迷茫，我不确定这些是否可行，任何帮助都值得赞赏！    提交人    /u/FutureComedian7749   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dm8ba1/adaptive_quadcopter_network/</guid>
      <pubDate>Sat, 22 Jun 2024 23:15:56 GMT</pubDate>
    </item>
    <item>
      <title>C++ 库到 JAX？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dlil7i/c_library_to_jax/</link>
      <description><![CDATA[大家好 r/MLQuestions，我读过这篇 2022 年的论文 https://chrislu.page/blog/meta-disco/，它讨论了通过纯 Jax 并行化将所有计算从 CPU 转移到 GPU 和 TPU。本质上，他们已经将整个 RL 步骤矢量化以进行 GPU 计算，并且由于令人难以置信的并行化，能够在 9 小时内在 Atari 基准测试中训练 50 万个代理，与 CleanRL 实现相比，这实现了 4000 倍的加速。  我认为现在它更常见了，但在 2022 年，这可能是一个重大突破。  现在，我也想要这个 我遇到了一个问题：在纯 jax 中工作。现在，我正在创建一个需要复杂 3d 数学引擎的产品 - 这些复杂的库很大并且用 C++ 编写。我也需要其中的很多库。  例如，一个是 OpenCASCADE [https://github.com/Open-Cascade-SAS] 。仅“src”文件夹就有 97mb，包含 16000 个文件。它非常庞大，我需要将其插入 JAX 以提高速度。  如果我不对我的应用程序进行超并行化，我将会陷入 10 倍的循环时间和 200 倍的硬件并行线减少的困境，这在我的领域意味着长时间的计算。  现在我懂数学，也许可以重写我需要的函数，但是我有没有可能用某种……装饰器将文件适配到 Jax ？毕竟，jax 也是用 C++ 编写的。或者也许分叉它并进行调整。任何能让函数在 Jax 中工作的东西，并且自定义实现不会花费 &gt; 个月的时间。 如果您知道任何可行的方法，请告诉我。谢谢大家！ 编辑：我的目标是：在 GPU 中运行所有内容，并以大规模并行方式运行所有内容。    提交人    /u/JustZed32   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dlil7i/c_library_to_jax/</guid>
      <pubDate>Sat, 22 Jun 2024 00:07:06 GMT</pubDate>
    </item>
    <item>
      <title>AgileRL - 用于最先进深度强化学习的进化型 RLOps</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</link>
      <description><![CDATA[嗨，我之前发布过关于我们的强化学习进化超参数优化实现 SOTA 结果的帖子，但我想分享的是，我们的开源框架现在已经发布了 v1.0.0 版本！ 请查看！https://github.com/AgileRL/AgileRL 该库最初专注于通过开创强化学习的进化 HPO 技术来减少训练模型和超参数优化所需的时间。进化 HPO 已被证明可以通过自动收敛到最佳超参数来大幅减少总体训练时间，而无需进行大量的训练运行。 我们不断添加更多算法和功能。 AgileRL 已经包含了最先进的可进化的在线策略、离策略、离线、多智能体和上下文多臂老虎机强化学习算法以及分布式训练。 我很乐意收到您的反馈！    提交人    /u/nicku_a   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dla2e8/agilerl_evolutionary_rlops_for_stateoftheart_deep/</guid>
      <pubDate>Fri, 21 Jun 2024 17:50:05 GMT</pubDate>
    </item>
    <item>
      <title>在强化学习（或者更确切地说，对于任何算法来说，机器学习）的背景下，像策略梯度这样的算法的单调改进意味着什么，为什么它是算法的重要参数。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dl0meg/what_does_monotonic_improvement_of_an_algorithm/</link>
      <description><![CDATA[我一直在阅读不同的文本，但仍然不明白真正的含义。有人可以解释一下吗。     提交人    /u/aabra__ka__daabra   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dl0meg/what_does_monotonic_improvement_of_an_algorithm/</guid>
      <pubDate>Fri, 21 Jun 2024 10:19:25 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习人形v4问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkuvm7/about_reinforcement_learning_humanoid_v4_problem/</link>
      <description><![CDATA[大家好，这是我在 colab 上为人形机器人 mujoco_humanoid.ipynb - Colab (google.com) 编写的代码（您可以在线运行）。但我不知道哪一步错了，结果就是网络没法学习机器人控制:( 真的需要帮助。    submitted by    /u/Inevitable_Sea_8466   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkuvm7/about_reinforcement_learning_humanoid_v4_problem/</guid>
      <pubDate>Fri, 21 Jun 2024 03:57:38 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中设置种子的实用规则</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkh0kp/practical_rules_for_setting_seed_in_rl/</link>
      <description><![CDATA[大家好！如果这个问题重复了，我很抱歉，但在阅读了许多与此相关的帖子后，我仍然找不到我的问题的答案。 假设我正在开发一个模型，其性能根据初始种子而有很大差异。 我应该固定一个种子或一组种子来比较不同特征或奖励函数的影响，还是应该始终使用随机种子和平均值运行，即使在开发模型时也是如此？ 如果是第二种情况，您如何了解性能改进是由于种子还是由于管道中的其他变化？    提交人    /u/ParfaitFinancial9765   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkh0kp/practical_rules_for_setting_seed_in_rl/</guid>
      <pubDate>Thu, 20 Jun 2024 17:15:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么重要性采样比例的期望值是1呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkfrzp/why_is_the_expected_value_of_the_importance/</link>
      <description><![CDATA[        提交人    /u/hearthstoneplayer100   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkfrzp/why_is_the_expected_value_of_the_importance/</guid>
      <pubDate>Thu, 20 Jun 2024 16:23:57 GMT</pubDate>
    </item>
    <item>
      <title>RLHF 的启动代码库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dkf3yr/starter_code_repos_for_rlhf/</link>
      <description><![CDATA[大家好， 我刚刚开始研究 LLM，特别是 RLHF。我正在寻找可以作为起点的开放课程库。我发现了以下内容：  https://github.com/OpenLLMAI/OpenRLHF https://github.com/huggingface/trl https://github.com/CarperAI/trlx  所有这些似乎都与 transformers 库兼容，而该库又支持完整的开源（代码+数据，而不仅仅是权重）模型，例如 Pythia。所有这些似乎都得到了相当程度的更新。1) 和 3) 支持分布式训练。您会推荐哪一个？还有其他建议吗？ 抱歉，我的问题可能有些幼稚。我是 LLM 新手 :)    提交人    /u/South-Conference-395   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dkf3yr/starter_code_repos_for_rlhf/</guid>
      <pubDate>Thu, 20 Jun 2024 15:56:11 GMT</pubDate>
    </item>
    <item>
      <title>用状态价值基线来强化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dk9lwm/reinforce_with_statevalue_baseline/</link>
      <description><![CDATA[嗨，我是 RL 的新手，我正在尝试了解如何使用状态值基线（在本例中为第二个神经网络）进行 REINFORCE。例如，在有两个玩家的棋盘游戏中，我训练一个代理对抗一个随机对手，何时应该存储奖励和价值？我应该在代理移动后才考虑奖励并估计价值，还是在对手移动后也考虑奖励并估计价值？ 此外，我不确定我是否正确理解了如何计算策略损失和价值策略损失。我的理解是，对于策略损失，我必须计算折扣奖励，从中减去值以获得优势，然后将对数概率与优势相乘以获得缩放对数概率，最后将它们全部相加（聚合）。对于价值策略，我的理解是我应该计算估计值和折扣奖励之间的 MSE 损失。 对吗？    提交人    /u/miroshuSan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dk9lwm/reinforce_with_statevalue_baseline/</guid>
      <pubDate>Thu, 20 Jun 2024 11:45:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 制作分类器，遇到内存错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dk6npn/making_a_classifier_using_ppo_encountering_memory/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dk6npn/making_a_classifier_using_ppo_encountering_memory/</guid>
      <pubDate>Thu, 20 Jun 2024 08:30:16 GMT</pubDate>
    </item>
    <item>
      <title>如果 PPO 受到稀疏奖励的影响，那么 InstructGPT 和 Learning to Summarize 是如何使其发挥作用的呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</link>
      <description><![CDATA[我见过很多关于 PPO 如何难以在稀疏奖励环境中发挥作用的讨论。在 instructGPT 和学习从人类反馈中总结的情况下，仅在一长串采样标记的最后一个标记处给予奖励。特别是对于一些涉及非常长的代数（1000 个动作范围）且最后只有一个奖励的现代 RLHF 任务 - PPO 如何在这里取得成功？我是否遗漏了优化？    提交人    /u/idioticfuse   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djz7iw/if_ppo_suffers_from_sparse_reward_how_did/</guid>
      <pubDate>Thu, 20 Jun 2024 01:01:39 GMT</pubDate>
    </item>
    <item>
      <title>“GUI-WORLD：面向 GUI 的多模态 LLM 代理的数据集”，Chen 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djxzhm/guiworld_a_dataset_for_guioriented_multimodal/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djxzhm/guiworld_a_dataset_for_guioriented_multimodal/</guid>
      <pubDate>Thu, 20 Jun 2024 00:00:43 GMT</pubDate>
    </item>
    <item>
      <title>“在训练和推理中权衡计算：我们探索了几种在训练或推理上花费更多资源之间进行权衡的技术，并描述了这种权衡的属性。我们概述了对人工智能治理的一些影响”，EpochAI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djxm5z/trading_off_compute_in_training_and_inference_we/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djxm5z/trading_off_compute_in_training_and_inference_we/</guid>
      <pubDate>Wed, 19 Jun 2024 23:43:06 GMT</pubDate>
    </item>
    <item>
      <title>“围棋 AI 能否具有对抗鲁棒性？”，Tseng 等人，2024 年（KataGo 的“绕圈”攻击可以被击败，但仍然可以找到更多攻击；这并不是由于 CNN）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djv1r3/can_go_ais_be_adversarially_robust_tseng_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djv1r3/can_go_ais_be_adversarially_robust_tseng_et_al/</guid>
      <pubDate>Wed, 19 Jun 2024 21:48:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 RLlib 进行多智能体供应链优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djltne/multiagent_supply_chain_optimization_with_rllib/</link>
      <description><![CDATA[亲爱的社区， 我正在寻找有关我目前在项目中面临的挑战性问题的见解和帮助。 问题： 仓库 X 面临着一项复杂的任务，即优化其供应链流程，以应对来自 200 多家供应商的 1000 种不同产品。目标是避免库存过剩、防止缺货并始终满足客户需求。产品分为易腐烂和不易腐烂两类。 我们提出的解决方案： 多智能体环境，其中每个智能体都会学习产品线的策略。我们使用了通过 Ray RLlib 实现的 PPO 算法。我们的 RL 环境旨在考虑以下操作：  何时下采购订单 订购哪些产品 每种产品的订购数量 考虑哪个供应商  代理根据过去一年的历史销售情况进行训练。环境由 N 个观察值组成，代表库存水平和趋势，以及 2 个操作（订购数量和供应商 ID）。代理在具有共享策略选项集的多代理环境中进行训练，每个情节由 365 个时间步骤组成。奖励是根据一系列行动后在一个情节中获得的净利润计算的。此外，奖励塑造用于指导代理做出正确的决策。当错过需求、订单延迟和选择错误的供应商（价格更高或交货时间更长）时，我们在每个时间步骤中施加惩罚。实际上，代理在时间步骤“t”做出的每个决定都会对未来时间步骤“t+M”产生负面/正面影响。 在每个时间步骤，历史数据的需求都用于更新观察值。因此，预计代理将达到或超过去年的净利润。 关于神经网络，我们使用了一个自定义模型，该模型由具有 ReLU 激活的线性层组成，分支为 softmax 用于供应商选择，并为数量分支输出值使用自定义激活函数，范围为 [0, max_purchase_quantity]。已应用批量归一化来增强收敛并减少过度拟合，并已结合注意力机制来关注观察空间中的关键值。 遇到的问题： 尽管付出了努力，但我在实现最佳收敛和性能方面仍面临挑战。训练过程明显很慢（使用 Nvidia GPU GeForce RTX 3080 10GB）。在对 50 种产品进行了 15,000 次迭代（历时 28 小时）之后，代理仍未达到预期的奖励，无法自主设置订单数量和供应商 ID。我们尝试在奖励函数中对错误决策实施惩罚，尝试了不同的探索策略，使用了课程学习（逐步应用惩罚），并应用了奖励规范化。然而，结果并不如预期。 您可以如何提供帮助： 我正在寻求帮助和新观点。如果您有供应链优化领域强化学习的经验，那么关于提高收敛速度、处理复杂动作空间或有效探索策略的见解的建议将非常有价值。 此外，如果您遇到过类似的挑战或在供应链环境中成功实施了 RL 代理，我们将非常感激您对调整超参数、设计有效奖励函数的指导或任何其他相关建议。 提前感谢您考虑这个主题。 我热切期待您的回复和见解。    提交人    /u/WoodenDot8305   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djltne/multiagent_supply_chain_optimization_with_rllib/</guid>
      <pubDate>Wed, 19 Jun 2024 15:22:31 GMT</pubDate>
    </item>
    </channel>
</rss>