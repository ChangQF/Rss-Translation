<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 23 Jan 2025 09:17:39 GMT</lastBuildDate>
    <item>
      <title>乐观的初始值如何鼓励探索？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7xig0/how_do_optimistic_initial_values_encourage/</link>
      <description><![CDATA[我正在研究（更新的）Sutton&amp;Barto 的书。 在 2.6 中，它说初始估计 +5 过于乐观。但这种乐观鼓励行动价值方法进行探索....即使一直选择贪婪动作，系统也会进行大量探索 这本书只讨论了一个常数 epsilon，其中以恒定概率选择随机动作。 所以，我不太明白乐观的 Q1 值和探索之间的关系。有人可以用简单的术语解释一下吗？    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7xig0/how_do_optimistic_initial_values_encourage/</guid>
      <pubDate>Thu, 23 Jan 2025 06:44:25 GMT</pubDate>
    </item>
    <item>
      <title>对于嘈杂观察环境有什么建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7rlhz/suggestions_for_noisy_observation_environments/</link>
      <description><![CDATA[嗨，我正在探索具有噪声观测的 RL。我在 OpenAI Gym Atari 中向像素添加了高斯噪声，但感觉太简单了。 对环境或更逼真的噪声模型有什么建议吗？有关高级噪声（例如遮挡、结构化噪声）或相关基准的提示将不胜感激。谢谢！    提交人    /u/AdministrativeCar545   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7rlhz/suggestions_for_noisy_observation_environments/</guid>
      <pubDate>Thu, 23 Jan 2025 01:14:05 GMT</pubDate>
    </item>
    <item>
      <title>这就是“糟糕”的奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</link>
      <description><![CDATA[        由    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</guid>
      <pubDate>Wed, 22 Jan 2025 19:48:44 GMT</pubDate>
    </item>
    <item>
      <title>关于 RL 代理控制其他 RL 代理的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7j9ck/question_about_rl_agents_controlling_other_rl/</link>
      <description><![CDATA[      嗨，我是强化学习领域的初学者，目前对基于物理的运动控制感兴趣。 当我查看各种众所周知的环境（例如机械臂）时，我想到了一个问题，即如何尝试在基于物理的环境中表现良好，涉及控制此类模型以实现比简单地到达某个目的地更抽象的复杂任务。具体来说，该问题出现在这篇论文中，问题场景的图像如下所示。 https://preview.redd.it/wrvz16y7flee1.png?width=612&amp;format=png&amp;auto=webp&amp;s=5c24cb87a696247929aff41e8775833c617b9218 例如，假设我要创建一个物理模拟环境，其中机械臂旨在在线 3D 装箱问题场景中表现良好，其中机械臂从传送带上抓取各种尺寸的盒子并将它们放置在指定位置，尝试在受限空间内容纳尽可能多的盒子。（我想我可以将奖励建模为与放置的盒子凸包的体积相关？） 我可以想象，采用不同代理的多层方法可能会充分发挥作用，一个用于解决 3D-BPP 问题，一个用于控制机械臂的各个电机以将盒子移动到某个位置，以便 3D-BPP 求解器的输出可以作为机械臂控制器代理的输入。但是，我无法想象这两个代理会完全解耦，因为 3D-BPP 求解器的某些命令可能在物理上不适合机械臂的移动，而不会破坏先前放置的盒子。 在这种情况下，我想知道通常的方法是什么：  使用单个代理能够独自控制这些看似不同的任务（求解 3d-bpp 和控制机械臂）？ 实际上使用两个代理并在训练序列中引入一些复杂性，以便求解器可以考虑机械臂控制器的运动？  如果这是一个微不足道的问题，任何我可以阅读的适合初学者的文献链接都将不胜感激！    提交人    /u/RulerOfCakes   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7j9ck/question_about_rl_agents_controlling_other_rl/</guid>
      <pubDate>Wed, 22 Jan 2025 19:15:47 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法的问题/解决方案参考指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7hpnm/a_problemsolution_reference_guide_for_rl/</link>
      <description><![CDATA[在学习 RL 课程时，我为几种算法创建了一个参考，并简要描述了它们解决了哪些限制。示例： 问题：SARSA 将 q 值推向当前策略，但理想情况下，我们想要的是最优值。 解决方案：在 TD 目标计算中使用最佳操作 -&gt; Q 学习 也许其他人会发现它很有用！可在 https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference 获得    提交人    /u/jac08_h   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7hpnm/a_problemsolution_reference_guide_for_rl/</guid>
      <pubDate>Wed, 22 Jan 2025 18:14:36 GMT</pubDate>
    </item>
    <item>
      <title>缩短 REINFORCE 的期限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7h25q/shortening_the_horizon_in_reinforce/</link>
      <description><![CDATA[大家好。我正在研究对具有动态状态（生成的状态是针对先前状态采取行动的结果）的建筑物进行强化学习，并且我正在使用纯 REINFORCE 算法并存储（s、a、r）转换。如果我想将一个时期分成几个情节，比如 10 个情节，（先前：一次运行 4000 个时间步，然后参数更新 --&gt;现在：400 个时间步，更新，另一个 400 个时间步，更新...），除了更改存储转换操作和学习函数的位置外，我还应该注意哪些事情才能正确进行此更改？您能告诉我可以学习的任何来源吗？谢谢。（我的 NN 框架在 Tensorflow 1.10 中）。    提交人    /u/Araf_fml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7h25q/shortening_the_horizon_in_reinforce/</guid>
      <pubDate>Wed, 22 Jan 2025 17:48:30 GMT</pubDate>
    </item>
    <item>
      <title>硕士学位决定</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7eiqz/masters_degree_decision/</link>
      <description><![CDATA[如果我有兴趣深入了解强化学习，有人能告诉我在欧洲哪里攻读硕士学位会更有益吗？    提交人    /u/Ok-Engineering4612   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7eiqz/masters_degree_decision/</guid>
      <pubDate>Wed, 22 Jan 2025 16:04:45 GMT</pubDate>
    </item>
    <item>
      <title>TD3 奖励不会随着时间的推移而增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7cxi6/td3_reward_not_increasing_over_time/</link>
      <description><![CDATA[      嘿，对于一个大学项目，我已经实现了 td3，并尝试在使用分配的环境之前在 pendulum v1 上对其进行测试。 这是我的超参数列表：  &quot;actor_lr&quot;: 0.0001, &quot;critic_lr&quot;: 0.0001, &quot;discount&quot;: 0.95, &quot;tau&quot;: 0.005, &quot;batch_size&quot;: 128, &quot;hidden_​​dim_critic&quot;: [256, 256], &quot;hidden_​​dim_actor&quot;: [256, 256], &quot;noise&quot;: &quot;Gaussian&quot;, &quot;noise_clip&quot;: 0.3, &quot;noise_std&quot;: 0.2, &quot;policy_update_freq&quot;: 2, &quot;buffer_size&quot;: int(1e6),  我面临的问题是奖励随着时间的推移不断减少，并且达到饱和在一些剧集之后，大约在 -1450 左右。有人知道我的问题可能出在哪里吗？ 如果需要，我还可以提供您怀疑可能有错误的任何代码 随着时间的流逝奖励 提前感谢您的帮助！    提交人    /u/bela_u   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7cxi6/td3_reward_not_increasing_over_time/</guid>
      <pubDate>Wed, 22 Jan 2025 14:57:18 GMT</pubDate>
    </item>
    <item>
      <title>可重复性和建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i784rl/reproducability_and_suggestions/</link>
      <description><![CDATA[我是 RL 领域的新手，但根据我的经验，有时在复杂情况下算法的可重复性会有所欠缺，即当我尝试重现算法（来自论文）结果时，我面临的情况是只有使用非常精确的超参数和种子才能做到。 当前的 RL 是否有点脆弱，或者我遗漏了什么？ 此外，请提供方法建议 谢谢    提交人    /u/Accomplished-Lie8232   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i784rl/reproducability_and_suggestions/</guid>
      <pubDate>Wed, 22 Jan 2025 10:25:38 GMT</pubDate>
    </item>
    <item>
      <title>推送任务未学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7717b/pusher_task_not_learning/</link>
      <description><![CDATA[我正在尝试在 mujoco 推送器环境中训练一个模型，但它不起作用。基本上，我从 mujoco github repo 获得了推送器类并做了一些小改动。我试图实现的是让推送器将 3 个对象推送到 3 个不同的目标。这些对象一次出现一个，所以当第一个对象被推送到目标时，第二个对象就会出现，依此类推。所以我对 mujoco 提供的类做的唯一修改是我添加了在视图中更改要推送对象的机制。我尝试了 PPO 和 SAC，时间步长为 100 万，奖励仍然为负。这看起来像是一项简单的任务，但它不起作用    提交人    /u/Latinotech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7717b/pusher_task_not_learning/</guid>
      <pubDate>Wed, 22 Jan 2025 09:01:57 GMT</pubDate>
    </item>
    <item>
      <title>“对奖励黑客攻击文档进行训练会引发奖励黑客攻击”，Hu 等人 2025 {Anthropic}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6vye4/training_on_documents_about_reward_hacking/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6vye4/training_on_documents_about_reward_hacking/</guid>
      <pubDate>Tue, 21 Jan 2025 22:56:45 GMT</pubDate>
    </item>
    <item>
      <title>可微分模拟资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6h3xh/resources_for_differentiable_simulation/</link>
      <description><![CDATA[大家好， 我是 RL 方法控制腿式机器人的新博士生。最近，我看到使用可微分模拟训练 RL 控制代理的趋势正在蓬勃发展。我还没有理解这个新概念，例如，DiffSim 到底是什么，它与序数物理引擎有何不同，等等。因此，我很想有一些关于这个主题基础知识的材料。你有什么建议吗？非常感谢你的帮助！    提交人    /u/Mountain_Deez   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6h3xh/resources_for_differentiable_simulation/</guid>
      <pubDate>Tue, 21 Jan 2025 12:05:56 GMT</pubDate>
    </item>
    <item>
      <title>关于“非常规框架”期刊论文投稿的见解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6fx2u/insights_on_journal_paper_submission_for/</link>
      <description><![CDATA[大家好！我和我的朋友使用强化学习，对环境污染监测的一个用例进行了研究，即在我们自己创造的不同国家和地区的环境中繁殖动物。无论我们提交到哪里，审阅者都会很欣赏，但最终，由于他们不了解用例和其他内容，它会导致拒绝。我们也没有任何基础论文可以参考，但是是的，到目前为止，我们尽了最大努力在纸上制定公式，并尽了最大努力解释整个决策支持系统。到目前为止，我们在审查过程中收到了 4 次拒绝，由于范围之外的原因收到了 7 次拒绝。在将其提交到其他地方之前，我需要一些指示，以便在期刊出版物上发表（根据学术规定，必须是期刊）。  提前为没有全心全意披露这项工作而道歉。我的问题是针对所有非传统的、间接的、新颖的、从未尝试过的作品……     提交人    /u/Miserable_Ad2265   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6fx2u/insights_on_journal_paper_submission_for/</guid>
      <pubDate>Tue, 21 Jan 2025 10:45:59 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i6ehaj/deep_reinforcement_learning/</link>
      <description><![CDATA[我有两本书  Richard S. Sutton 和 Andrew G. Barto 的《强化学习》  Miguel Morales 的《深度强化学习》 我发现两本书的内容表格都差不多。我即将自学 DQN、Actor Critic 和 PPO，但很难确定书中的重要主题。第一本书看起来更侧重于表格方法（？），对吗？  第二本书有几个章节和子章节，但我需要有人帮助指出里面的重要主题。我是一名普通软件工程师，在业余时间很难逐一消化所有概念。  有人可以帮忙指出哪个子主题很重要吗？如果我的想法正确，第一本书更侧重于表格方法？    由    /u/Best_Fish_2941  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i6ehaj/deep_reinforcement_learning/</guid>
      <pubDate>Tue, 21 Jan 2025 08:56:48 GMT</pubDate>
    </item>
    <item>
      <title>“推理者的问题：祈求迁移学习”，艾丹·麦克劳克林（更多 RL 会修复 o1 风格的 LLM 吗？）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i65y2f/the_problem_with_reasoners_praying_for_transfer/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i65y2f/the_problem_with_reasoners_praying_for_transfer/</guid>
      <pubDate>Tue, 21 Jan 2025 00:35:02 GMT</pubDate>
    </item>
    </channel>
</rss>