<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 20 May 2024 09:18:10 GMT</lastBuildDate>
    <item>
      <title>有人真的部署了一个模型来用于推理吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</link>
      <description><![CDATA[我只是好奇有人实际上使用 RL 训练的策略来帮助解决哪些应用程序或控制问题，您用什么 Algo 进行训练？在此过程中遇到的主要挑战是什么？    由   提交 /u/Aggressive-Reach1657    reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</guid>
      <pubDate>Mon, 20 May 2024 07:01:20 GMT</pubDate>
    </item>
    <item>
      <title>“遇见 Shakey：第一个电子人——拥有自己思想的机器的迷人和可怕的现实”，Darrach 1970</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</guid>
      <pubDate>Mon, 20 May 2024 01:39:13 GMT</pubDate>
    </item>
    <item>
      <title>最近有哪些您真正喜欢的 RL 应用示例？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw1yk3/any_recent_examples_of_rl_applications_you_really/</link>
      <description><![CDATA[ 由   提交/u/paswut  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw1yk3/any_recent_examples_of_rl_applications_you_really/</guid>
      <pubDate>Sun, 19 May 2024 23:50:34 GMT</pubDate>
    </item>
    <item>
      <title>输入/输出关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw0ycm/inputoutput_relationships/</link>
      <description><![CDATA[社区您好， 假设我们有 N 个元素，每个元素都有一组特征（Xi1，Xi2）和任务DQN 的目的是选择其中一个元素（因此我们有 3 个输出）。假设输入向量为[X11, X21, X12, X22]，DQN如何将元素的每个输入特征与其对应的输出联系起来，例如即使存在遥远的距离，它如何理解X11和X12是元素1的特征? 我希望描述清楚   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw0ycm/inputoutput_relationships/</guid>
      <pubDate>Sun, 19 May 2024 23:02:22 GMT</pubDate>
    </item>
    <item>
      <title>RL 导师/专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</link>
      <description><![CDATA[我是一名本科生，目前正在完成论文。我接手了一个项目，该项目使用 RL 进行连续控制，通过 6d 位姿估计器来控制机器人。我看得很远，但强化学习机器人技术在我们国家可能还不够饱和。我试图寻找结构化的方法来学习这一点，就像使用 OpenAI 旋转强化学习以及 Sutton &amp; 的理论背景一样。巴托的书。我真的很渴望在明年之前完成这个项目，但我没有导师。甚至我们大学的教授也很快就会采用强化学习机器人技术。我从以前的帖子里看到，在这里请教导师是可以的，所以请原谅。如果我无法正确地提出问题，我深表歉意。 我想要实现这些目标： - 充分掌握 RL 基础知识，特别是在连续动作空间控制方面。 - 熟悉艾萨克·西姆。 - 了解如何为强化学习建模物理系统 - 将经过训练的模型部署到物理机器人 - 通过项目慢慢积累知识，最终引导我完成项目 - 寻找指导我完成整个工作流程的导师 &lt; p&gt;我所知道的： - 深度学习的背景 - RL 的基本原理（直到 MDP 和 TD） - RL 算法的背景 - DQN、DDPG、TD3 如何在高级抽象中工作 - 在高级抽象中体验重播缓冲区和 HER - ROS 2 基础知识 我想知道什么： - 我需要学习所有数学知识吗？或者我可以只参考现有的实现吗？ - 考虑到我的资源限制，我只能实现一个算法（我在第三世界国家），我应该使用它来实现完成项目的最大可能性。目前，我正在关注TD3。 - 一个本科生团队有可能完成这样的项目吗？ - 鉴于资源限制，我们应该使用哪个 Jetson 板来运行策略？ - 我们的目标是针对易碎处理进行优化，我们如何限制研究？ 我的努力我目前正在研究更多并建立关于算法和强化学习的直觉。最近，我迁移到 Ubuntu 并设置了模拟所需的所有软件和环境 (Isaac Sim)。 挫败感 在没有人交谈的情况下继续这个项目非常具有挑战性，因为每个人都几乎不感兴趣与RL。每个资源都有一个非常陡峭的学习曲线，当我认为我知道某些资源时，某些资源指向了我不知道的其他东西。我必须在明年之前完成这个工作，尽管我正在尽我所能地学习，但仍有很多我不知道的事情。    由   提交 /u/echialas22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</guid>
      <pubDate>Sun, 19 May 2024 21:04:03 GMT</pubDate>
    </item>
    <item>
      <title>再现“SOLVING THE OSHI-ZUMO GAME M. Buro 2004”的结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvy09r/reproducing_results_of_solving_the_oshizumo_game/</link>
      <description><![CDATA[嗨， 我想在 Oshi-Zumo 游戏上进行 Self-Play 或 Double Oracle（我省略了一些细节，但，这是一个两人的零和游戏，每个玩家都有一定数量的硬币，他们同时下注一定数量的硬币，当硬币“落在”敌人的一侧时。任何玩家，该玩家都输了）。 首先，我想我不妨通过使用线性规划而不是使用函数近似来实际解决游戏来获得纳什均衡策略。我遵循了 M. Buro 2004 年的论文 SOLVING THE OSHI-ZUMO GAME 并我使用了 open spiel python 库。 我成功了（经过长时间的调试）来解决 [50,3,1]-Oshi-Zumo 博弈，并在位置 (50,50,0) 处找到与纳什均衡策略完全相同的结果（论文第 5 段开头）。同样，我对位置 (6, 3, -3) 有完全相同的政策和预期回报（就在同一段落的下面）。但我对位置 (20, 32, 3) 没有相同的策略（他们的动作 17 18 等 20 的概率在我的策略中被“分组”，该策略选择只玩 20 - 其余的对于涉及较少的动作是相同的硬币）。但预期回报是相同的。我想，我可能刚刚达到了另一个纳什均衡，因为那个游戏中可能存在几个混合纳什均衡（实际上很有可能）。 然后，我做了我的策略反对随机代理和硬编码代理，如果他们有超过 2 个硬币，则玩 2，否则玩 1（受到该论文图 3 的启发）。我在对抗随机代理时并没有赢那么多，但在对抗硬编码代理时我赢了很多（他们实际上输了一点）。我还模拟了 200,000 场比赛。 我的问题是：我“刚刚”玩过吗？达到另一个纳什均衡，因此我不应该担心我无法重现他们所做的事情？ （他们提供的用于重现结果的链接不再可用）或者我是否在某个地方搞砸了？如果是这样，我该怎么做才能确保我确实搞砸了。如果我一直输给某种策略，那么我当然没有达到纳什均衡，但到目前为止情况并非如此。 附带的额外问题：他们说“在笔记本电脑上一个 1-GHzPentium-IIICPU，解决标准 [50,3,1] 游戏只需 12 秒”。我在我的机器上花了 2 分钟多。我没有最好的机器，但我认为我的 CPU 可能比他们的更好。这种差距可以用我使用 Python 而他们使用 C++ 来解释吗？虽然我使用 pulp 来解决我的线性问题，所以我想这已经通过调用编译代码进行了优化。我唯一合理的解释是，我的线性问题的解决方案很可能更精确，因为程序花了很长时间才能获得大量小数，对吗？ 我已附上我的所有内容colab 笔记本中的代码。 即使是也应该非常容易理解由不熟悉 open_spiel 库的人编写。   由   提交 /u/Lindayz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvy09r/reproducing_results_of_solving_the_oshizumo_game/</guid>
      <pubDate>Sun, 19 May 2024 20:49:27 GMT</pubDate>
    </item>
    <item>
      <title>需要帮忙！！强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvr5pt/need_help_rl_project/</link>
      <description><![CDATA[我目前正在使用 Q 学习算法为大学做一个期末项目。这里有没有人非常精通并且可以帮助我   由   提交/u/amulli21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvr5pt/need_help_rl_project/</guid>
      <pubDate>Sun, 19 May 2024 15:42:15 GMT</pubDate>
    </item>
    <item>
      <title>开+RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</link>
      <description><![CDATA[KAN 擅长持续学习，有可能使 on-policy 变得鲁棒吗？    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</guid>
      <pubDate>Sun, 19 May 2024 13:15:45 GMT</pubDate>
    </item>
    <item>
      <title>具有环境知识的演员评论家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cv6ph4/actorcritic_with_environment_knowledge/</link>
      <description><![CDATA[我想创建一个 RL actor-critic 模型来配置网络拓扑。输入是表示当前拓扑的矩阵，输出是表示优化拓扑的新矩阵。我的操作空间包含两个操作：  创建或删除设备（0 或 1）之间的链接。 设置带宽 [0;1]。  我的状态空间由输入矩阵中的设备组成。因此，每个纪元都涉及配置一个设备（矩阵中的整行），并且参与者必须始终通过所有状态。 我的问题是：参与者是否需要知道整个当前拓扑才能创建新的最佳方案？   由   提交/u/Soft_Web489  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cv6ph4/actorcritic_with_environment_knowledge/</guid>
      <pubDate>Sat, 18 May 2024 20:39:02 GMT</pubDate>
    </item>
    <item>
      <title>协变：“当我们在更多数据上训练 RFM-1 时，我们的 [机器人手臂] 模型的性能可预测地提高 [在拣选]”：5 倍以上的数据使误差减半</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cv0yrg/covariant_as_we_train_rfm1_on_more_data_our_robot/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cv0yrg/covariant_as_we_train_rfm1_on_more_data_our_robot/</guid>
      <pubDate>Sat, 18 May 2024 16:21:55 GMT</pubDate>
    </item>
    <item>
      <title>RL 已达到稳定水平了吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cuhcyb/has_rl_hit_a_plateau/</link>
      <description><![CDATA[大家好，我是强化学习 (RL) 领域的研究员，在过去的几年中，我对该领域的进展感到有点困惑年。看起来我们正处于局部最优状态。自从 DQN、AlphaGo 和 PPO 等突破引发的炒作以来，我观察到，尽管有一些非常酷的增量改进，但还没有任何类似于我们在 PPO 和 SAC 上看到的重大进步。 你对RL的现状有同感吗？我们是否正在经历一段停滞期，或者是否正在取得我没有看到的重大进展？我真的很想听听您的想法，以及您是否认为强化学习即将取得更多突破。   由   提交/u/Md_zouzou  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cuhcyb/has_rl_hit_a_plateau/</guid>
      <pubDate>Fri, 17 May 2024 21:52:33 GMT</pubDate>
    </item>
    <item>
      <title>MAB 每一步都有多种选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cuftyn/mab_for_multiple_choices_at_each_step/</link>
      <description><![CDATA[因此，我正在使用一个自定义环境，我需要在每个时间步选择一个大小为 N 的向量并获得全局奖励（以简化一下，动作 [1, 2] 可以返回不同的奖励 [2, 1])。我正在使用 MAB，特别是 UCB 和 epsilon-greedy，其中我有 N 个独立的 MAB 控制 M 臂。它基本上是一个多代理，但只有一个中央代理控制一切。我的问题是可能的操作数量 (MN) 以及缺乏“沟通”在各种选择之间进行选择，以达成更好的全球解决方案。我知道一些基于环境上其他模拟的好的解决方案，但 RL 无法通过自己达到，并且作为测试，当我“展示”时（强制采取行动）它是好的行动，但它不会学习它，因为旧的测试组合。我正在考虑使用 CMAB 来提高全球奖励。我可以使用其他算法来解决这个问题吗？   由   提交 /u/joaovitorblabres   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cuftyn/mab_for_multiple_choices_at_each_step/</guid>
      <pubDate>Fri, 17 May 2024 20:45:21 GMT</pubDate>
    </item>
    <item>
      <title>多任务强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cubiwb/multitask_rl/</link>
      <description><![CDATA[是否存在同时（不是按顺序或连续）学习多个独立任务的环境？举例来说，学习边瞄准边行走的任务或学习边走边玩杂耍的任务。我正在寻找动作空间保持相同的东西，并且在给定的时间步，代理预测两个不同任务的动作，类似于“复合运动学习”论文中使用的环境 https://www.youtube.com/watch?v=mcRAxwoTh3E   由   提交/u/Personal_Click_6502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cubiwb/multitask_rl/</guid>
      <pubDate>Fri, 17 May 2024 17:45:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的理想环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cu89sr/ideal_environment_for_rl/</link>
      <description><![CDATA[大家好，我一直想从事强化学习项目，但我一直遇到一些基本的不确定性，我不知道如何沟通，但我会尝试。我总是对我可能会做的很酷的项目有想法（RL 用于部落冲突、Minecraft、机器人等），但我不知道如何“与应用程序交互”？大多数教程似乎都重新创建了自己的游戏，以便他们可以轻松访问状态、奖励和操作信息，但我如何使用像 CoC 这样运行的实际应用程序来做到这一点，我使用什么操作系统以及如何做我与它交互（读取信息并进行击键）？我是否必须读取应用程序的内存才能找到某些值（时间、运行状况），我该怎么做？我应该只使用 GUI 显示来获取信息（以便整个屏幕和像素值成为状态）以及如何记录它？另外，如何阻止应用程序继续运行，直到我的 ML 模型做出决定然后我采取行动，从而防止未被注意到的状态更改？如果这一切听起来很愚蠢，我真的很抱歉，但我找不到任何似乎在真实游戏中做到这一点的教程，而且我觉得我缺少一些其他人都可以轻松完成的东西，因为也有例如，DeepMind 的一系列研究视频，玩《我的世界》之类的。我真的不知道从哪里开始   由   提交 /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cu89sr/ideal_environment_for_rl/</guid>
      <pubDate>Fri, 17 May 2024 15:34:13 GMT</pubDate>
    </item>
    <item>
      <title>torchrl 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cu08xk/issues_with_torchrl/</link>
      <description><![CDATA[       嗨， 我正在使用 torchrl 的 PPO 实现。 https://pytorch.org/rl/stable/tutorials/coding_ppo.html &lt;但是在训练我的代理时，我的权重出现爆炸，将网络推向纳米值。  我试图找出原因，然后我看到了这些极端值 损失熵和loss_objective中的一个非常相似但积极的问题。  根据我的分析，我怀疑 log_probs 在每一集中都增加到非常大的值，这可能是问题的原因。但是，我不确定确切的原因以及如何解决。 https://preview.redd.it/oypsspdt6y0d1.png?width=1409&amp;format=png&amp;auto=webp&amp;s=0cffba31d4138a2f0af79f869b8153 f3690c3a67    由   提交 /u/RikoteMasterrrr   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cu08xk/issues_with_torchrl/</guid>
      <pubDate>Fri, 17 May 2024 08:31:34 GMT</pubDate>
    </item>
    </channel>
</rss>