<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 29 Nov 2023 18:17:43 GMT</lastBuildDate>
    <item>
      <title>Rankitect：在元规模上与世界一流工程师对战的架构搜索排名</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.08430 摘要：  神经架构搜索 (NAS) 已经证明了其在计算机视觉方面的功效和排名系统的潜力。然而，之前的工作主要集中在学术问题上，这些问题是在良好控制的固定基线下进行小规模评估的。在行业系统中，例如 Meta 中的排名系统，尚不清楚文献中的 NAS 算法是否能够超越生产基线，因为：（1）规模 - Meta 排名系统为数十亿用户服务，（2）强大的基线 - 基线是生产自深度学习兴起以来，多年来数百到数千名世界级工程师优化了模型，(3) 动态基线 - 工程师可能在 NAS 搜索过程中建立了新的、更强的基线，(4) 效率 - 搜索管道必须产生结果快速与生产生命周期保持一致。在本文中，我们介绍了 Rankitect，这是一个用于 Meta 排名系统的 NAS 软件框架。 Rankitect 寻求通过从头开始构建低级构建块来构建全新的架构。 Rankitect 实现并改进了最先进 (SOTA) NAS 方法，以在同一搜索空间下进行全面、公平的比较，包括基于采样的 NAS、一次性 NAS 和可微分 NAS (DNAS)。我们通过与 Meta 上的多个生产排名模型进行比较来评估 Rankitect。我们发现 Rankitect 可以从头开始发现新模型，实现归一化熵损失和 FLOP 之间的竞争性权衡。当利用工程师设计的搜索空间时，Rankitect 可以生成比工程师更好的模型，实现积极的离线评估和 Meta 规模的在线 A/B 测试。  https://preview.redd.it/mlceky7x7b3c1.png?width=1379&amp;format=png&amp;auto =webp&amp;s=2acc4e0451db9fbf455b03f9e293e68cc61d25bf   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</guid>
      <pubDate>Wed, 29 Nov 2023 16:00:25 GMT</pubDate>
    </item>
    <item>
      <title>我的 Gym Cartpole 代理学习的运行平均值约为 200，但当我使用相同的 q 表进行游戏时，它仅运行 10 分。请帮忙。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ms1a/my_gym_cartpole_agent_learns_with_a_running_mean/</link>
      <description><![CDATA[gridworld/cartpole.ipynb at main · bherwanisuraj/gridworld (github.com)  这是我的 git 存储库的链接。如果有人能帮忙那就最好了。    由   提交 /u/tlevelup   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ms1a/my_gym_cartpole_agent_learns_with_a_running_mean/</guid>
      <pubDate>Wed, 29 Nov 2023 10:57:01 GMT</pubDate>
    </item>
    <item>
      <title>关于“Q*”推测：法学硕士和合成数据搜索的一些相关研究背景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186fhih/on_q_speculation_some_relevant_research/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186fhih/on_q_speculation_some_relevant_research/</guid>
      <pubDate>Wed, 29 Nov 2023 03:20:02 GMT</pubDate>
    </item>
    <item>
      <title>“学习少量模仿作为文化传播”，Bhoopchand 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ejkw/learning_fewshot_imitation_as_cultural/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ejkw/learning_fewshot_imitation_as_cultural/</guid>
      <pubDate>Wed, 29 Nov 2023 02:37:31 GMT</pubDate>
    </item>
    <item>
      <title>DQN Agent 学得不太好</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/</link>
      <description><![CDATA[大家好！ 我目前正在学习强化学习。我写信询问是否有解决方案，因为我的代理在通过决斗 dqn 强化 Atari 突破时没有正确学习。就我而言，我成功地挖了一条隧道并举起了球，但球升起后，特工开始没有采取任何行动。我的猜测是，我了解到最好的做法就是在投球后什么也不做。但即使球落下来，这也是一个问题，因为他什么也没做。还有其他方法可以解决这个问题吗？ 如果您需要检查代码，请告诉我，我给您链接   由   提交/u/king-god_general  /u/king-god_general  reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/</guid>
      <pubDate>Tue, 28 Nov 2023 16:20:46 GMT</pubDate>
    </item>
    <item>
      <title>销售和预测销售环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185ykik/sales_and_forecasted_sales_environment/</link>
      <description><![CDATA[假设我有一组产品的历史销售额和预测销售额，并且我想使用代理（RL 或其他）来管理库存决策。关于如何使用这些历史数据来创建学习环境有什么想法吗？当然，我可以将分布拟合到历史销售和样本中，但是预测方法是基于对销售趋势的基本假设，而这样做会丢失这些假设。 理想情况下，我会有一个环境生成大量未来销售轨迹和相关预测，让代理通过数千次迭代学习基于预测的库存策略。但销售轨迹和预测应该以与生成基础预测模型相同的方式关联，否则该策略就毫无价值。  也许查看预测的历史误差并从未来的单一预测生成随机轨迹会更有意义？这里的问题是，根据给定的销售轨迹，预测可能会在 3 周内发生变化。 任何见解或类似的工作将不胜感激！ &lt;!-- SC_ON - -&gt;  由   提交 /u/aaronunderwater   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185ykik/sales_and_forecasted_sales_environment/</guid>
      <pubDate>Tue, 28 Nov 2023 15:25:12 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线 3 + JAX (SBX) 未在 GPU 上运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185tpdo/stable_baselines_3_jax_sbx_not_running_on_gpu/</link>
      <description><![CDATA[我已经使用“pip install sbx-rl”安装了 SBX 库（最新版本）用于我的 Stable Baselines 3 + JAX PPO 实施，以提高训练速度。我想使用 GPU (RTX 4090) 进行训练，但由于某种原因 SBX 始终默认使用 CPU。  SBX 无法识别我的 GPU，但是当我执行“nvidia-smi”时，SBX 无法识别我的 GPU。它被清楚地检测到+ pytorch 本身也确实检测到它。我怀疑 SBX 与我的 CUDA 版本（当前为 12.2）不兼容，但我找不到任何有关支持哪些 CUDA 版本的文档。  我花了几天时间试图解决这个问题，但没有成功。有人有想法吗？   由   提交/u/ClassicAppropriate78  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185tpdo/stable_baselines_3_jax_sbx_not_running_on_gpu/</guid>
      <pubDate>Tue, 28 Nov 2023 11:15:31 GMT</pubDate>
    </item>
    <item>
      <title>受版权游戏启发的研究环境（例如 Hanabi 挑战等）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185splc/research_environment_inspired_from_copyrighted/</link>
      <description><![CDATA[大家好！ 我想知道为游戏创建强化学习环境（仅用于研究）的法律方面仍然由创建它的相应公司拥有版权和销售权。 以 Hanabi 挑战为例 https:// arxiv.org/pdf/1902.00506.pdf，快速谷歌显示该游戏仍在销售。论文作者是否事先询问过游戏发行商是否允许他们创建环境？他们刚刚就这么做了吗？这里的合法性是什么？  感谢和干杯！   由   提交 /u/Arconer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185splc/research_environment_inspired_from_copyrighted/</guid>
      <pubDate>Tue, 28 Nov 2023 10:09:18 GMT</pubDate>
    </item>
    <item>
      <title>离策略演员批评家目标函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</link>
      <description><![CDATA[      我正在阅读 Silver 的 DPG 论文。在这里，如下所示，目标函数已使用行为策略 beta 进行了修改。我很好奇，如果使用梯度最大化下面的目标，目标策略的目标函数（通常的策略目标）是否会最大化？ ​ &lt; a href=&quot;https://preview.redd.it/1779aomlyz2c1.png?width=737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e&quot;&gt;https://preview.redd.it/1779aomlyz2c1.png?width =737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e   由   提交 /u/RealJuney   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</guid>
      <pubDate>Tue, 28 Nov 2023 02:12:27 GMT</pubDate>
    </item>
    <item>
      <title>寻找职业建议。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</link>
      <description><![CDATA[大家好，过去 3 年我一直对机器学习感兴趣，我的大部分注意力都集中在监督学习上，但是在过去 3 个月里 RL引起了我的注意，我相信人工智能的下一个重大事件将来自该领域。我有兴趣通过学术界，因为我只有计算机科学学士学位，并且不会找到工作，因为我在津巴布韦，而我们在技术方面还没有达到这个水平。我申请在美国攻读博士学位，但拒绝的次数越来越多，所以我很可能最终会去中国获得奖学金。我想要一些建议，因为最终我想在西方的大公司从事研发工作。如果可以的话，请告诉我在中国攻读硕士学位期间我可以做些什么，以便在 2026/27 年毕业后让我更接近这个目标。 PS：我也在中国获得了学士学位。   由   提交/u/congo43  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</guid>
      <pubDate>Mon, 27 Nov 2023 19:53:09 GMT</pubDate>
    </item>
    <item>
      <title>多头DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</link>
      <description><![CDATA[大家好，我正在应用 DQN 每次选择一组元素（一次一个或多个）。如何避免动作 [0, 0, 0,…]，即如何强制代理选择至少一个元素？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</guid>
      <pubDate>Mon, 27 Nov 2023 19:01:00 GMT</pubDate>
    </item>
    <item>
      <title>确定性参数总是输出相同的动作（PPO）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1859bj1/deterministic_parameter_always_output_the_same/</link>
      <description><![CDATA[      我使用 SB3 中的 PPO 和动作掩码来训练具有以下超参数的环境。在训练中，模型似乎学会了采取不同的行动。然而，在测试阶段，模型似乎做同样的事情并且表现正常，除了当我使用“确定性 = True”时，模型始终只选择一个操作。 我的代码： action，_states = model.predict（obs，action_masks=env.valid_action_mask()，确定性=True）  initial_learning_rate = 0.00005  model = MaskablePPO(MaskableActorCriticPolicy, env, tensorboard_log=&quot;./tensorboard&quot; ,n_steps=2048 ,learning_rate=initial_learning_rate) # 创建根据奖励调整学习率的回调 callback = RewardBasedLearningRateSchedule() for i in range (1,202): model.learn(total_timesteps=TIMESTEPS , tb_log_name = &#39;PPO2&#39; , reset_num_timesteps=False,callback=callback) &lt;代码&gt;model.save(f&quot;{models_dir}/{TIMESTEPS*i}&quot;) ​ tensororad的输出： https://preview.redd.it/cpdvwhxnkx2c1.png?width=1652&amp;format=png&amp;auto=webp&amp;s=a6d3bdef2f5040ce935a6534b66de2d934029af6   由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1859bj1/deterministic_parameter_always_output_the_same/</guid>
      <pubDate>Mon, 27 Nov 2023 18:07:01 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 3.0 对阵 Isaac Gym</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</link>
      <description><![CDATA[您好， 对于那些尝试过并且熟悉 Mujoco 3.0 和 Isaac Gym 的人，建议他们使用哪一个学习以及为什么？   由   提交 /u/anointedninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</guid>
      <pubDate>Mon, 27 Nov 2023 16:59:33 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线推出/ep_rew_mean</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1850jcz/stable_baselines_rolloutep_rew_mean/</link>
      <description><![CDATA[我正在尝试做一些需要我能够获取每 4 集打印到命令提示符中的 ep_rew_mean 值的事情。但我只是无法弄清楚这些值存储在哪里，以便我可以在回调函数中访问它们。我正在从“ep_info_buffer”访问最后一集奖励，但这与我需要的 ep_rew_mean 有很大不同。我为此查看了记录器、OffPolicyAlgorithm 和 SAC 文件，但仍然找不到它。这些信息在打印之前存储在哪里？我如何访问它？   由   提交 /u/aliaslight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1850jcz/stable_baselines_rolloutep_rew_mean/</guid>
      <pubDate>Mon, 27 Nov 2023 11:08:48 GMT</pubDate>
    </item>
    <item>
      <title>没有批评家模型的策略梯度算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/184wmg5/policy_gradient_algorithms_without_critic_model/</link>
      <description><![CDATA[是否可以在没有 actor-critic 的情况下实现像 NPG、PPO 这样的策略梯度算法，或者它会破坏这些算法中当前更快的收敛性吗？  如果我想这样做，我可以从哪里开始呢？    由   提交/u/eles0range  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/184wmg5/policy_gradient_algorithms_without_critic_model/</guid>
      <pubDate>Mon, 27 Nov 2023 06:37:39 GMT</pubDate>
    </item>
    </channel>
</rss>