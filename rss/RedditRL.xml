<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 05 Jan 2024 09:13:44 GMT</lastBuildDate>
    <item>
      <title>我应该期望代理学习多快？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18z0hlj/how_fast_should_i_expect_the_agent_to_learn/</link>
      <description><![CDATA[我是 RL 新手，刚开始玩扫雷游戏。一开始，我希望模型学习的唯一一件事就是避免点击已经打开的方块。 我知道这可以通过动作屏蔽来完成，但我很好奇，想看看学习这个简单的行为需要多长时间。 点击打开的方块的奖励是 -10000，而做其他任何事情的奖励是 10。令我惊讶的是，训练持续了很长时间几个小时了，特工仍然没有学会避开已经打开的方块。目前，我看到大约 10% 的移动是点击一个打开的方块。 我只是想知道这是否太长。 以下是有关我的设置的更多信息：  p&gt;  我用的是gymnasium和stable-baselines3，型号是PPO 扫雷游戏是9*9的，有10个地雷。每个打开的方块都用 0-8 之间的数字表示，表示地雷的数量。未打开的方块用 -1 表示。 我使用配备 RTX 3060（120 瓦）的笔记本电脑进行训练。   &amp; #32；由   提交 /u/yzhjonathan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18z0hlj/how_fast_should_i_expect_the_agent_to_learn/</guid>
      <pubDate>Fri, 05 Jan 2024 06:48:09 GMT</pubDate>
    </item>
    <item>
      <title>使用世界模型进行基于梯度的规划</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18yy57s/gradientbased_planning_with_world_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.17227 摘要：  人工智能领域持久的挑战是控制系统以实现所需的行为。而对于由简单动力学方程控制的系统，线性二次调节（LQR）等方法在历史上已被证明是非常有效的，大多数现实世界的任务都需要通用的问题求解器，需要具有无法通过简单方程轻松描述的动力学的世界模型。因此，这些模型必须使用神经网络从数据中学习。大多数为视觉世界模型设计的模型预测控制（MPC）算法传统上都探索基于无梯度群体的优化方法，例如用于规划的交叉熵和模型预测路径积分（MPPI）。然而，我们提出了一种基于梯度的替代方案的探索，该替代方案充分利用了世界模型的可微性。在我们的研究中，我们对我们的方法和其他基于 MPC 的替代方案以及基于策略的算法进行了比较分析。在样本有效的设置中，与大多数任务中的替代方法相比，我们的方法取得了同等或更好的性能。此外，我们引入了一种结合了策略网络和基于梯度的 MPC 的混合模型，该模型的性能优于纯基于策略的方法，从而有望在复杂的现实任务中使用世界模型进行基于梯度的规划。  &lt; /div&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18yy57s/gradientbased_planning_with_world_models/</guid>
      <pubDate>Fri, 05 Jan 2024 04:38:57 GMT</pubDate>
    </item>
    <item>
      <title>优先重播缓冲区 - 真的有用吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/</link>
      <description><![CDATA[您好，我有一个问题要问你们所有有实施和评估优先重播缓冲区经验的人。 我自己做了一个问题优先重放缓冲区的实现，并将其与双 dqn 实现进行比较。比较是在 pythongym 库的 Lunar Lander 环境中进行的。对于优先重播缓冲区的 alpha 和 beta 值，我使用 0.7（固定）和 0.4 作为初始 beta 值，该值在整个数字中线性变化到 1.0集数（即 4000）。 在我的比较中，double dqn 在大约 2700 集时完成训练（当它在最后 100 集中达到平均 230 累积奖励时），而使用优先级的训练重放缓冲区在大约 2800 到 2900 集时完成训练（同样，当它在最后 100 集达到平均 230 个累积奖励时）。我尝试从 0.3、0.4 和其他值开始将 alpha 值线性移动到 1.0 ，但每次，它的性能最多与双 dqn 一样好。我期望在使用优先重播缓冲区进行训练时能够更快地达到接受标准（230 奖励的平均值），因为它应该提供对代理来说更有意义的体验样本（与以随机统一方式采样的正常重放缓冲区相反）。 因此，根据您的经验，您是否发现使用优先重放缓冲区与使用优先重放缓冲区相比有好处？正常重播缓冲区（并使用双 dqn）？您认为优先重播缓冲区在月球着陆器等环境中没有明显好的结果吗？ 任何提示、建议、意见（基于您的经验） ），想法或分享的经验非常有价值，欢迎并感谢。 干杯！   由   提交/u/kxy-yumkimil  /u/kxy-yumkimil reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18yozlt/prioritized_replay_buffer_really_useful/</guid>
      <pubDate>Thu, 04 Jan 2024 21:49:39 GMT</pubDate>
    </item>
    <item>
      <title>有关 PPO 奖励的快速信息</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ydc5s/quick_info_on_ppo_reward/</link>
      <description><![CDATA[我的奖励函数有 0.01 和 1 这样的常量 如果我将其设置为 0.1 和 10，会有什么不同吗？  我问这个问题是因为当我通过正缩放器（添加常数项）改变奖励函数时，它产生了影响。  &amp;# 32；由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ydc5s/quick_info_on_ppo_reward/</guid>
      <pubDate>Thu, 04 Jan 2024 13:36:07 GMT</pubDate>
    </item>
    <item>
      <title>指导我在 RL 领域取得良好的职业生涯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18y6lgk/guide_me_for_a_good_career_in_rl/</link>
      <description><![CDATA[您好，我最近刚毕业，获得机械工程学士学位，但我对 ML、RL 更感兴趣。作为初学者，我完成了 stanford online 的 ML 课程，完成了 DL 专业化课程，完成了 dl.ai 的 GANs 专业化课程，然后完成了 U-Alberta 的 RL 专业化课程，我还在 youtube 上观看了加州大学伯克利分校的深度 RL 讲座。我参与过几个小项目（至少 3 个写在简历上，一个是关于分割的，两个是关于序列模型的）小项目，其中一个与机器人操作系统（ros）相关的项目，尽管我没有深入、彻底地使用 ROS，并且2 个关于 3D CNN 的项目，将 DL 应用于医学领域（所以不是非常新颖的工作）。 而且我比我探索的任何领域都更喜欢 RL。但是，强化学习中的数学，至少现在确实让我着迷，我理解数学在这里试图做什么，它背后的直觉，但如果我必须自己写数学，那就是不可撤销的。我想在这个领域进行研究。 目前我正在做第二个项目，关于 3d CNN（GAN，与 RL 有一些关系），但我想做一些更好的事情，工业实习或与教授的学术实习是首选，但考虑到我在学士学位中的 GPA 非常非常低，我只是担心没有人会对指导/录取我感兴趣，以至于我什至没有达到人们在某些职位上录取的程度。而且我确信，如果教授考虑得更好，公司可能不会将我视为他们的第一选择。我不确定哪里有更好的机会，但我真的很想从事学术研究，尽管工业界可能会带来更好的钱。我只是想转向更好的地方，你能建议任何路径吗？首先当然是接触人们，接触博士生也可能是获得更多指导的好主意，不是吗？ P.s.我对 RL 的探索和奖励感兴趣   由   提交/u/vyknot4wongs  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18y6lgk/guide_me_for_a_good_career_in_rl/</guid>
      <pubDate>Thu, 04 Jan 2024 06:40:38 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型可以自学使用工具”，Schick 等人 2023 {FB}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18y1hh8/large_language_models_can_teach_themselves_to_use/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18y1hh8/large_language_models_can_teach_themselves_to_use/</guid>
      <pubDate>Thu, 04 Jan 2024 02:15:41 GMT</pubDate>
    </item>
    <item>
      <title>“桥接离散和反向传播：直通和超越”，Liu 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18y0fee/bridging_discrete_and_backpropagation/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18y0fee/bridging_discrete_and_backpropagation/</guid>
      <pubDate>Thu, 04 Jan 2024 01:28:10 GMT</pubDate>
    </item>
    <item>
      <title>在 7 多万英里的纯骑手驾驶中，Waymo 的表现显着优于可比较的人类基准（Kusano 等人，2023 年）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xzuvh/waymo_significantly_outperforms_comparable_human/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xzuvh/waymo_significantly_outperforms_comparable_human/</guid>
      <pubDate>Thu, 04 Jan 2024 01:02:58 GMT</pubDate>
    </item>
    <item>
      <title>“PASTA：预训练的动作状态转换器代理”，Boige 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xymab/pasta_pretrained_actionstate_transformer_agents/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xymab/pasta_pretrained_actionstate_transformer_agents/</guid>
      <pubDate>Thu, 04 Jan 2024 00:09:46 GMT</pubDate>
    </item>
    <item>
      <title>持续强化学习的定义</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2307.11046 OpenReview：https:// /openreview.net/forum?id=ZZS9WEWYbD 摘要：  在强化学习问题的标准视图中，代理的目标是有效地确定最大化长期回报的策略。然而，这种观点是基于一种有限的观点，即学习是寻找解决方案，而不是将学习视为无休止的适应。相反，持续强化学习是指最好的智能体永远不会停止学习的环境。尽管持续强化学习很重要，但社区缺乏一个简单的问题定义来强调其承诺并使其主要概念准确清晰。为此，本文致力于仔细定义持续强化学习问题。我们将“永不停止学习”的代理概念正式化。通过一种新的数学语言来分析和编目代理。使用这种新语言，我们将持续学习代理定义为可以无限期地执行隐式搜索过程的代理，并将持续强化学习定义为最佳代理都是持续学习代理的设置。我们提供了两个激励性的例子，说明多任务强化学习和持续监督学习的传统观点是我们定义的特例。总的来说，这些定义和观点形式化了学习核心的许多直观概念，并开辟了围绕持续学习代理的新研究途径。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xu67t/a_definition_of_continual_reinforcement_learning/</guid>
      <pubDate>Wed, 03 Jan 2024 21:09:29 GMT</pubDate>
    </item>
    <item>
      <title>暑期实习在 RL/Embodied AI 领域处于领先地位。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xos1z/summer_internship_leads_in_rlembodied_ai_space/</link>
      <description><![CDATA[我正在寻找 2024 年暑期实习的一些线索，以寻找潜在的（如果可能的话研究）实习机会。 我不是非常了解在这个领域工作的所有初创公司/组织，因为我自己对它非常陌生。我没有太多经验，但希望在这些实习中获得一些经验。如果有任何帮助，我们将不胜感激！   由   提交 /u/gchhablani   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xos1z/summer_internship_leads_in_rlembodied_ai_space/</guid>
      <pubDate>Wed, 03 Jan 2024 17:33:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何获得研究想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xlwli/how_do_i_get_ideas_for_research/</link>
      <description><![CDATA[事情的简短版本：我是一名 CS 硕士生。我已经完成了一年半的尝试并努力进入 EAI 领域。我还有大约一年的时间，我想充分利用这一年。我有一些 NLP 专业背景，也有一些研究经验，但不是很深入。我了解监督学习，并且擅长 PyTorch 之类的东西。我目前面临两个主要问题：  我想发表一些第一作者的作品，但无法获得强大的创造力在我体内流淌。我该如何开发它？ 我想从语言理解方面攻击 RL 的各个方面，但各个方面的研究都在疯狂增长（LLM、VLM） 、RL、变形金刚），我很难理解该读什么、不该读什么。  来自该领域有经验的人的任何想法或提示类似问题或只是有足够的经验？ 如果这不是合适的论坛，请指出我可以讨论此问题的地方。 &lt;!-- SC_ON - -&gt;  由   提交 /u/gchhablani   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xlwli/how_do_i_get_ideas_for_research/</guid>
      <pubDate>Wed, 03 Jan 2024 15:24:28 GMT</pubDate>
    </item>
    <item>
      <title>RL玩游戏：从哪里开始？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xk6a8/rl_to_play_games_where_to_start/</link>
      <description><![CDATA[我正在启动一个有趣的项目，训练 AI 玩 2 种类型的游戏： - 1v1 游戏联盟/Dota 风格，基于图像的输入  - 1v1 回合制游戏，玩家在单元格上移动并拥有一堆咒语，具有基于特征的输入 所以我读过 Atari、AlphaGo/Star、OpenAI Five...想知道人们是否有其他/较新的参考资料或相关项目，在这些情况下最好使用哪种类型的算法？过去我只在所有事情上使用 PPO。 我还计划自己编写游戏代码（在非常基础的水平上） - 想知道人们是否已经这样做了并且对使用什么语言/框架有建议以最大速度写入游戏。我最近看到了 Madrona Engine，看起来很有趣，但我还没有尝试过。 顺便说一句，到目前为止我只是自己做 RL，有没有大型的 RL 社区除了这个 Reddit 子版块之外还应该知道什么？谢谢你！ :)   由   提交/u/frenchhusky  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xk6a8/rl_to_play_games_where_to_start/</guid>
      <pubDate>Wed, 03 Jan 2024 14:05:40 GMT</pubDate>
    </item>
    <item>
      <title>逆向强化学习的现状？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xjndc/current_state_of_inverse_reinforcement_learning/</link>
      <description><![CDATA[ 由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xjndc/current_state_of_inverse_reinforcement_learning/</guid>
      <pubDate>Wed, 03 Jan 2024 13:40:46 GMT</pubDate>
    </item>
    <item>
      <title>用于控制器调优 inn python 的 env</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18xgqu8/env_for_controller_tuning_inn_python/</link>
      <description><![CDATA[       这里有人尝试过使用 RL 在 python 中进行 PI/PID 控制器调整 如果可以的话，您可以提供您的 env 文件或帮助我创建相同的环境。&lt; /p&gt; （环境由代理、源、系统模型以及系统输出到源的反馈组成）  /u/Wide-Chef-7011   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18xgqu8/env_for_controller_tuning_inn_python/</guid>
      <pubDate>Wed, 03 Jan 2024 10:57:38 GMT</pubDate>
    </item>
    </channel>
</rss>