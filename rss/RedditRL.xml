<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Wed, 19 Feb 2025 21:16:14 GMT</lastBuildDate>
    <item>
      <title>样品效率（MBRL）与腿部现象的SIM2REAL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1it4d70/sample_efficiency_mbrl_vs_sim2real_for_legged/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想研究腿部运动的RL（双皮亚，类人动物），我很好奇当前哪种研究方法似乎更可行 - 培训模拟和培训通过提高样品效率（也许使用MBRL）来直接致力于改善SIM2REAL，直接培训物理机器人。这两种方法之间是否有明确的偏好？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1it4d70/sample_efficiency_mbrl_vs_sim2real_for_legged/</guid>
      <pubDate>Wed, 19 Feb 2025 12:30:29 GMT</pubDate>
    </item>
    <item>
      <title>从字面上重新创建了数学推理和DeepSeek的AHA时刻，不到10美元，通过最终简单的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1it2zhv/literally_recreated_mathematical_reasoning_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;    https://medium.com/@rjusnba/overnight-end-end-eend-to-end-raind-raining-a-3b-model-on-a-a grade-school-school-math-math-dataset-leads-leads-to-to-to-to-to-rounconing-df61410c04c6   我感到惊讶！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/unightent-life9355     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1it2zhv/literally_recreated_mathematical_reasoning_and/</guid>
      <pubDate>Wed, 19 Feb 2025 11:06:27 GMT</pubDate>
    </item>
    <item>
      <title>纸牌游戏RL项目的硬件/软件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iszy7r/hardwaresoftwarr_for_card_game_rl_projects/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在研究RL，想在诸如向导或类似纸牌游戏上训练AI。 Chatgpt在Python上使用stable_baselines3给了我一个不错的开端。它似乎工作得很好，但是我不确定我是否长期正确。您是否对我应该考虑的软件和库有建议？您是否会建议特定的硬件来大大加快流程？我目前有一个带有Ryzen 5600和3060TI GPU的系统。培训约为1200fps（如果有任何用途）。我可以升级到5950x，但也想考虑一台专用的迷你PC，如果配合得当。 提前感谢！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tot-chance9372     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iszy7r/hardwaresoftwarr_for_card_game_rl_projects/</guid>
      <pubDate>Wed, 19 Feb 2025 07:32:12 GMT</pubDate>
    </item>
    <item>
      <title>RL研究组？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isz4eh/study_group_for_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有RL研究组？美国时区  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/best_fish_2941     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isz4eh/study_group_for_rl/</guid>
      <pubDate>Wed, 19 Feb 2025 06:36:15 GMT</pubDate>
    </item>
    <item>
      <title>TD学习以估算OpenAI体育馆中杂技环境中选择的随机固定政策的价值功能。如何应对持续状态空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isq0rd/tdlearning_to_estimate_the_value_function_for_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有这项作业，我们需要使用TD学习来估算OpenAI Gym中杂技演员环境中所选随机固定策略的价值函数。持续的状态空间阻碍了我，我不知道我应该如何离散。即使有少数间隔，我也会获得六维空间。提交由＆＃32; /u/u/basic_exit_4317     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isq0rd/tdlearning_to_estimate_the_value_function_for_a/</guid>
      <pubDate>Tue, 18 Feb 2025 23:03:30 GMT</pubDate>
    </item>
    <item>
      <title>两足动力的入门论文？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isn711/introductory_papers_for_bipedal_locomotion/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好， 您能否向我提供双足动力的介绍性论文？我正在寻找非常香草的东西。  ，如果您也知道简单的论文，其中RL用于“模仿”对同一主题的最佳控制！  谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/yaxleii   href =“ https://www.reddit.com/r/reinforevercylearning/comments/1isn711/introductory_papers_for_bipedal_locomotion/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isn711/introductory_papers_for_bipedal_locomotion/</guid>
      <pubDate>Tue, 18 Feb 2025 20:46:23 GMT</pubDate>
    </item>
    <item>
      <title>研究主题基础艾伯塔省计划</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iskafj/research_topics_basis_the_alberta_plan/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我听说了理查德·萨顿（Richard Sutton）的艾伯塔省计划，但是由于我是初学者。 RL中是否有一个特定的研究主题，我可以在接下来的几年中探索我的研究？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iskafj/research_topics_basis_the_alberta_plan/</guid>
      <pubDate>Tue, 18 Feb 2025 18:51:06 GMT</pubDate>
    </item>
    <item>
      <title>研究主题是为了寻找AGI的潜在进步吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isk777/research_topics_to_look_into_for_potential/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这是一个非常理想的和天真的问题，但是我计划尽快攻读博士学位，并想根据AGI决定一个方向听起来很令人兴奋。我认为AGI肯定需要了解其环境的管理原则，因此MBRL似乎是一个很好的研究领域，但我不确定。我听说过艾伯塔省的计划，但没有完成，但这听起来像是为了创建研究方向的好尝试。到目前为止，最好为此探索什么RL主题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isk777/research_topics_to_look_into_for_potential/</guid>
      <pubDate>Tue, 18 Feb 2025 18:47:38 GMT</pubDate>
    </item>
    <item>
      <title>两足动力现在是解决问题的问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isewx8/is_bipedal_locomotion_a_solved_problem_now/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我刚刚遇到了Unitree的发展，我只是想知道是否公平地假设双皮亚运动（对于人叶子）已经实现了（忽略诸如价格和其他东西之类的因素）。 现在，从研究的角度来看，人形机器人是一个解决的问题？   &lt;！ -  sc_on - &gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isewx8/is_bipedal_locomotion_a_solved_problem_now/</guid>
      <pubDate>Tue, 18 Feb 2025 15:14:01 GMT</pubDate>
    </item>
    <item>
      <title>如何处理不稳定的算法？ DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1isersx/how_to_handle_unstable_algorithms_dqn/</link>
      <description><![CDATA[在为了发现新领域和完成在已经探索的区域移动或撞到障碍物 的完成负面奖励 我正在使用DQN，并且它将很快学习以完成整个课程，这仅仅是基本的5x5 &lt; /p&gt; 通过200-500/1000的测试将是半一致的，但随机地将其延伸到一个更糟糕的状态，非常稳定地 ，因此在25个可探索的块中，它将坚持使用即使在以前找到更好的分数的完整解决方案，但我看到的解决方案始终如一地找到了18个解决方案？ 我已经看到可能会使用DQN的变体，但老实说，我不确定并且很困惑。我应该在看到它后立即保存正确的状态，还是我需要微调我的算法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/jetnjet     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1isersx/how_to_handle_unstable_algorithms_dqn/</guid>
      <pubDate>Tue, 18 Feb 2025 15:07:59 GMT</pubDate>
    </item>
    <item>
      <title>RL代理：DQN和Doubel DQN在Lunarlander环境中不融合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is93td/rl_agent_dqn_and_doubel_dqn_not_converging_in_the/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我一直在开发各种RL代理，并将它们应用于不同的OpenAI健身环境。到目前为止，我已经实施了DQN，Double-DQN和一位香草策略梯度代理，在Cartpole和Lunar Lander环境中测试它们。  DQN和Double-DQN模型成功地求解了Cartpole（达到200和200 500步），但在Lunar Lander表现不佳。相比之下，政策梯度代理可以解决Cartpole（200和500步）和Lunar Lander。我怀疑我的实施可能存在问题，因为我知道其他人已经能够解决它，只是无法弄清楚原因。我尝试了许多不同的参数（网络结构，软更新等，在某些情节之后，在一集中的每个步骤之后进行培训，..）如果有人对可能发生的事情有洞察力或建议，我将感谢您的建议！我已经在下面的链接中附上了DQN的木星笔记本和doubled-dqn。 非常感谢！   https://drive.google.com/drive/folders/1xoezpyvwbn5zqn-u-ibbqzjujbd-dixc?usp = sharing    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/evises_drop_7402     &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1is93td/rl_agent_dqn_dqn_and_doubel_doubel_dqn_not_not_not_not_not_not_not_not_not_not_not_converging_in_in_in_in_inthe/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is93td/rl_agent_dqn_and_doubel_dqn_not_converging_in_the/</guid>
      <pubDate>Tue, 18 Feb 2025 09:46:20 GMT</pubDate>
    </item>
    <item>
      <title>我需要一些指导来解决这个问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is8jc1/i_need_some_guidance_resolving_this_problem/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我对增强学习领域相对较新，我已经完成了一些课程并阅读了一些有关它还做了一些工作（小型项目）。 我目前正在研究我的问题，我想知道我需要使用强化学习来解决这个问题的算法/方法是什么样的算法/方法。 我有一个建筑游戏，其目标是在最大允许的建筑地形中建造最大数量的房屋数量。每个可能的建筑地形都可以拥有或没有地雷（这会破坏您的房屋并使您失去游戏）。拥有该地雷的可能性仅基于您的房屋的分布。例如，某个分布可能会导致相同的建筑物具有地雷，但另一个分布可能会导致该建筑物没有它。 对于培训，代理商可以收到对每个房屋的反馈（是否在地雷上天气）。&lt; /p&gt; 通常，该建筑游戏有很多建筑规则，例如间距房屋等...但是我希望我的经纪人隐式学习这些建筑规则并能够应用它们。在我的培训结束时，我希望能够拥有一个能够确定最好和最多的代理商时必的建筑策略（最大的房屋数量），这概括了他从不同环境中汲取的模式，这些模式将在空间中变化，但会有相同的规则，这意味着从培训中学到的模式可以适用于任何其他环境。&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; br /&gt;你们知道什么用于解决此问题的奖励策略，算法等...？ - &gt;＆＃32;提交由＆＃32; /u/u/IntelligentPainter86     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is8jc1/i_need_some_guidance_resolving_this_problem/</guid>
      <pubDate>Tue, 18 Feb 2025 09:03:52 GMT</pubDate>
    </item>
    <item>
      <title>必须阅读强化学习论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，大家好，所以我是CS毕业，并且在深度学习和计算机视觉方面有体面的知识。我现在想学习强化学习（特别是用于飞行机器人的自动导航）。因此，您能从您的经验中告诉我，哪些论文是一本强制性的阅读，可以开始并在强化学习方面保持体面。预先感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/dronesanddynamite     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1is773d/must_read_papers_for_reinforcement_learning/</guid>
      <pubDate>Tue, 18 Feb 2025 07:26:22 GMT</pubDate>
    </item>
    <item>
      <title>有人熟悉RESQ/RESZ（价值分解MARL）吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irzt84/anyone_familiar_with_resqresz_value_factorization/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/losthero_12     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irzt84/anyone_familiar_with_resqresz_value_factorization/</guid>
      <pubDate>Tue, 18 Feb 2025 00:39:39 GMT</pubDate>
    </item>
    <item>
      <title>RL项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irugdg/advice_on_rl_project/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在从事一个深入的RL项目，我想在其中将一个图像与另一个图像对齐，例如。一张笑脸的两张照片，一张照片可能与另一张照片相比向右移动。我正在编码这个项目，但是有问题，并且想在此方面获得一些帮助。  方法：    状态 s_t = [image1_reference，image2_query]   代理/策略：CNN输入状态并预测 [旋转，缩放，缩放，translate_x，translate_y] &gt;这是图像转换参数。具体而言，它将输出平均向量和STD向量，该向量将在这些参数上参数化正态分布。从此分布中采样了一个动作。 环境：环境在空间上转换给定动作的查询图像，并产生 s_t+1 = [image1_reference，image2_query_transed] 。 奖励功能：目前基于两个图像的相似性（基于MSE损失）。 情节终止标准：情节如果采取超过100个步骤，则终止。我还终止了转换是否太大（将图像缩放到一无所有，或将其从屏幕上翻译），给出-100的奖励。  rl算法：我正在使用增强。我希望稍后再尝试PPO之类的算法，但现在想到，增强功能可以正常工作。  错误/问题：我的模型没有真正学习任何东西，每个情节都在早期终止有-100的奖励，因为查询图像被大量扭曲。关于可能发生的事情以及如何解决的想法？  问题：     我觉得我的奖励系统是&#39;对。当图像对齐时，应该在剧集结束时给予奖励，还是应该与每个步骤给出？     MSE应该是奖励，还是应该是基于整数的奖励（+/- 10）？    我希望我的代理人尽可能少的步骤对图像对齐，而不是预测剧集的终止标准，或者我应该将其作为一个终止标准惩罚？或两者兼而有之？   会喜欢的一些建议，我对RL很新，所以不确定最好的行动是什么！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/echocomprehension925     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irugdg/advice_on_rl_project/</guid>
      <pubDate>Mon, 17 Feb 2025 20:50:16 GMT</pubDate>
    </item>
    </channel>
</rss>