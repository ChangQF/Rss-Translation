<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 01 Dec 2023 18:17:18 GMT</lastBuildDate>
    <item>
      <title>强化学习如何处理多个动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188iiew/how_does_rl_work_with_multiple_motions/</link>
      <description><![CDATA[例如，这是使用Unity创建的，还是它自己的gym？ https://www.youtube.com/watch?v=SsJ_AusntiU&amp;pp=ygUncmVpbmZvcmNlbWVudCBsZ WFybmluZyB0ZWFjaCBob3cgdG8gYm94&lt; /p&gt; 从视频中很难看出。  令人困惑的是它们是如何链接在一起的，以及机器学习代理是否可以实现这一点。我了解如何训练模型站立，但如何同时站立、起身、走动和装箱？   这一切都是在数十亿次重复中通过相同的观察和相同的奖励完成的吗？  或者是否有 1 个模型学会了如何站立，1 个模型学会了如何站起来。 1 个学习如何移动的简化模型。等等。   然后这些模型使用布娃娃物理原理相互绑定。本质上，正在学习的模型上的关节与了解每个单独运动的模型绑定在一起。  ​ 还有，这些天赋标签是怎么回事？我不明白其中任何一个   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188iiew/how_does_rl_work_with_multiple_motions/</guid>
      <pubDate>Fri, 01 Dec 2023 18:14:51 GMT</pubDate>
    </item>
    <item>
      <title>Unity 之外允许导入模型等的任何物理引擎健身房。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188iazm/any_physics_engine_gyms_outside_of_unity_that/</link>
      <description><![CDATA[我发现用于统一的 ML Agent 有点过时且有局限性，并且比机器人更适合游戏开发，我想创建自己的健身房对于像 2D 游戏这样的简单东西，它非常简单，但我想做一些 3D 机器人模型并使用稳定的基线来驱动它们，为此我需要一个具有碰撞和物理以及模型/的 3D 引擎纹理导入、一些着色，当然还有渲染。最好是也可以渲染到 webgl 环境的东西。 我假设稳定基线可以与任何 3D 开源引擎一起使用。是否有一些非常基本的东西可以与 OpenAI Gym 一起使用来导入模型、添加刚体和碰撞参数等。或者 OpenAI Gym 是否已经具备所有这些功能？ ​ ​   由   提交 /u/Sharp-Cat2319   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188iazm/any_physics_engine_gyms_outside_of_unity_that/</guid>
      <pubDate>Fri, 01 Dec 2023 18:05:56 GMT</pubDate>
    </item>
    <item>
      <title>🚀 DIAMBRA x OROBIX：发布 SheepRL 强化学习库集成！ 🤖🎮</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/188eslz/diambra_x_orobix_releasing_sheeprl_reinforcement/</link>
      <description><![CDATA[      我们很高兴宣布与高级 RL 库 SheepRL 的创建者 OROBIX 进行动态合作！ 🤝💡 🌟 为什么选择 SheepRL？ SheepRL 无缝集成到 DIAMBRA 中，为现代 RL 算法提供了一条清晰的路径，重点关注可扩展应用程序的分布式训练。 📚 入门：在我们的官方 DIAMBRA 文档中探索 SheepRL 的强大功能。 DIAMBRA Agents 存储库中包含源代码示例。 🚀 RL 探索的 Swift Launchpad： 只需几行 Python 代码，即可在我们所有的游戏上启动 RL 训练，让 AI 探索变得轻而易举！ /&gt; 👉 通过下面评论中的链接发现更多信息！ 加入我们，拥抱强化学习研究的未来。 🚀🤖✨ DIAMBRA x SheepRL&lt; /a&gt;   由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/188eslz/diambra_x_orobix_releasing_sheeprl_reinforcement/</guid>
      <pubDate>Fri, 01 Dec 2023 15:36:10 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18880wg/learning_rl_research/</link>
      <description><![CDATA[大家好，我现在正在研究强化学习，我正在寻找加快学习速度的方法。我的计划是能够在该领域做好研究。现在我正在研究离线强化学习。  现在有帮助的是我们每周与我的博士同学进行一次会议。我们展示我们读过的新研究论文。轮到我的时候我会分享有关 RL 论文的内容。  但有一个问题是我们的主题非常多样化。一项研究计算机视觉，一项研究图神经网络，一项研究时间序列预测。我是唯一一个从事强化学习的人。 如果我能与更多具有相同兴趣的人交谈，那就太好了。 为了补充背景信息，我来自菲律宾在这里很难获得支持或具有专业知识的团体来指导我。当我感觉自己在学习过程中几乎是孤身一人时，就很难保持动力。虽然我在荷兰有一位教授给了我很多帮助。 您对在线小组等有什么建议吗？这可以帮助我更快地学习，包括潜在的指导、资源共享等？ 谢谢！   由   提交/u/111user222  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18880wg/learning_rl_research/</guid>
      <pubDate>Fri, 01 Dec 2023 09:44:48 GMT</pubDate>
    </item>
    <item>
      <title>“使用直接偏好优化 (DPO) 的扩散模型对齐”，Wallace 等人 2023 {Salesforce}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187wknu/diffusion_model_alignment_using_direct_preference/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187wknu/diffusion_model_alignment_using_direct_preference/</guid>
      <pubDate>Thu, 30 Nov 2023 23:27:38 GMT</pubDate>
    </item>
    <item>
      <title>[D] 一周后我将采访 Rich Sutton，我应该问他什么问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187t5ir/d_im_interviewing_rich_sutton_in_a_week_what/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187t5ir/d_im_interviewing_rich_sutton_in_a_week_what/</guid>
      <pubDate>Thu, 30 Nov 2023 21:06:43 GMT</pubDate>
    </item>
    <item>
      <title>解决转型和奖励中的分配变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187nfvc/addressing_distributional_shifts_in_transitions/</link>
      <description><![CDATA[解决 RL 任务中的分布变化的最先进方法是什么？  我需要将强化学习应用于交通控制问题，其中交通分布（车辆到达率、出发地-目的地位置等）可能随时间变化。我确实可以在多个分布上进行训练，但我想使用一些方法，让我能够快速适应不同的分布，或者快速为其微调新模型。    由   提交 /u/fedetask   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187nfvc/addressing_distributional_shifts_in_transitions/</guid>
      <pubDate>Thu, 30 Nov 2023 17:04:33 GMT</pubDate>
    </item>
    <item>
      <title>创建跨多个环境运行的单个代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187mzev/creating_a_single_agent_that_acts_across_multiple/</link>
      <description><![CDATA[大家好，先感谢您的帮助！我正在开展一个 q-learning 项目，其中单个代理正在努力优化许多（数千台）独立机器的性能。所有机器都是相同的，并在定期更新的数据库上生成相同的指标。几年前，我读过这个社区的一个帖子，听起来好像可以在多个环境中应用单个代理。鉴于这些都是相同的环境并由唯一的 ID 表示，我想知道在构建模型时是否可以应用一个简单的函数分组来在更广泛的数据数组上进行训练，而不是之前在单个数据集上进行训练尝试将其部署到所有不同的环境中。再次感谢大家的帮助！   由   提交/u/Shikaze33_3   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187mzev/creating_a_single_agent_that_acts_across_multiple/</guid>
      <pubDate>Thu, 30 Nov 2023 16:45:03 GMT</pubDate>
    </item>
    <item>
      <title>在矢量基观测环境上实现 Dreamer v3 时出错</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187ktu6/error_implementation_dreamer_v3_on_vector_base/</link>
      <description><![CDATA[请问我做错了什么？我尝试在基于向量的观察的自定义环境中使用 Dreamerv3 训练我的代理。我按照 Sheaprl 方法添加自定义环境。  请问我做错了什么？我尝试在基于矢量的观察的自定义环境中使用 Dreamerv3 训练我的代理。我按照 Sheaprl 方法添加自定义环境。训练代理导航到其目标，同时避开障碍。  我想使用 Dreamer v3。我的观察空间是基于平面向量的观察（23），而不是图像，并且动作空间是连续的二维。我想编写一个使用 Dreamer v3 的脚本。如果可能，下面的脚本将仅适用于我的环境： - envs：我的环境 - configs.yaml - dreamer.py -exploration.py - models.py -networks.py -parallel.py -requirements.txt -tools.py 我想要一个特定的、干净的代码，以便我可以针对其他环境和基于图像的选项修改 Dreamer v3。这让我很难简化我想做的事情。我将分享我的环境和一个关于如何测试它的简单脚本。如果您能帮助我或指导我如何使用 Dreamer v3 和基于平面矢量的观察空间，我将不胜感激。谢谢！ 这是我的 zip 文件，其中包含我的环境： https://www.dropbox.com/scl/fi/z13wkqgjtuszlt3oimxd4/SacNav.zip?rlkey=f0y2yctxa5ajjt442na8abk4l&amp;dl=0   由   提交 /u/Goodluck_o   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187ktu6/error_implementation_dreamer_v3_on_vector_base/</guid>
      <pubDate>Thu, 30 Nov 2023 15:13:09 GMT</pubDate>
    </item>
    <item>
      <title>学习资源</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187kpy0/learning_sources/</link>
      <description><![CDATA[您好，我想听听你们是否对我如何继续我的学习之旅有任何建议。 到目前为止我相信我对神经网络的工作原理有深入的了解，包括它的数学原理和编码。我成功地在一个简单的环境中实现了一个普通的梯度下降算法。现在我正在考虑如何继续前进。我喜欢从数学角度理解事物，然后尝试用 numpy 自己实现它，以确保我理解一切。但最重要的是，我想开始学习使用现有的RL 和 NNS 的实用工具，例如 pytorch 或诸如此类的东西。 你们对解决理论和实践问题的资源有什么建议吗？ 顺便说一句，我在这里问是因为我对深度 RL 特别感兴趣虽然它看起来很一般。   由   提交 /u/urtropicretarded   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187kpy0/learning_sources/</guid>
      <pubDate>Thu, 30 Nov 2023 15:08:17 GMT</pubDate>
    </item>
    <item>
      <title>轻松实施并行训练。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187imzd/easily_implement_parallel_training/</link>
      <description><![CDATA[    /u/NoteDancing   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187imzd/easily_implement_parallel_training/</guid>
      <pubDate>Thu, 30 Nov 2023 13:33:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么我应该使用 RLLib 而不是 stable-baselines3？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187fahg/why_should_i_use_rllib_over_stablebaselines3/</link>
      <description><![CDATA[所以我已经使用 sb3 大约 3 年了，我发现设置实验、向 RL 添加功能修改/自定义网络架构非常简单算法，使用Optuna优化参数，并在训练后进行推理。除了了解其工作原理的基本动态之外，所有这些都不需要付出什么努力。  所以在我的新公司中，他们使用 RLLib，至少可以说，它是一个怪物，可以完成我在 sb3 中轻松完成的上述任何事情。 所以争论是通常认为使用 RLLib 部署模型很容易。但这真的那么容易吗？考虑到我们可以通过 FAST api 和 docker 组合轻松甚至更快地部署使用 sb3 训练的模型，是否值得仅仅为了它提供的额外算法而经历 RLLib 的困难？对于业内使用强化学习的人来说，您的体验如何？   由   提交/u/sharafath28  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187fahg/why_should_i_use_rllib_over_stablebaselines3/</guid>
      <pubDate>Thu, 30 Nov 2023 10:14:33 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么游戏可以让我们通过python与之交互呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/187e4jj/is_there_any_game_that_allow_us_to_interact_with/</link>
      <description><![CDATA[我正在学习 RL“强化学习”，并且我已经完成了一些大大小小的项目来掌握它的窍门。 我写了一个代码（训练了一个 RL 模型），通过捕获屏幕（mss）和发送输入（PyDirectInputs）来玩游戏，但它太慢了；大概只有 1 fps。（无论是在训练中，还是在使用中） 所以我想知道是否有任何游戏可以让我们通过 python 与其交互？像一些 API、接口、软件等将我们的代码直接连接到游戏。或者类似的东西？   由   提交 /u/Mr_Lucifer_666   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/187e4jj/is_there_any_game_that_allow_us_to_interact_with/</guid>
      <pubDate>Thu, 30 Nov 2023 08:53:39 GMT</pubDate>
    </item>
    <item>
      <title>将多处理与 CUDA 相结合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/</link>
      <description><![CDATA[我是高性能计算方面的新手。因此，如果我的问题看起来太微不足道，请原谅我。 ​ 我正在考虑将多重处理应用于 CUDA RL 程序。现在我正在尝试训练一个需要大约 9 小时训练的环境。我想尝试一下超参数和其他一些东西。因此，我想使用相同的 GPU 并行运行我的模型 10 次。  这是我在谷歌搜索后要处理的 -   GPU 大小 - 我知道我的 GPU 应该足够大，可以接收 10 个数据运行。  GPU 年份 - 显然较旧的 GPU 不允许多个程序访问它。   我还缺少什么吗？我的假设是，如果运行单个模型需要 9 小时，那么通过多重处理，只要满足上述约束，我就可以在 9 小时内运行 10 个模型。   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/</guid>
      <pubDate>Wed, 29 Nov 2023 19:06:53 GMT</pubDate>
    </item>
    <item>
      <title>Rankitect：在元规模上对架构搜索与世界级工程师进行排名</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.08430 摘要：  神经架构搜索 (NAS) 已经证明了其在计算机视觉方面的功效和排名系统的潜力。然而，之前的工作主要集中在学术问题上，这些问题是在良好控制的固定基线下进行小规模评估的。在行业系统中，例如 Meta 中的排名系统，尚不清楚文献中的 NAS 算法是否能够超越生产基线，因为：（1）规模 - Meta 排名系统为数十亿用户服务，（2）强大的基线 - 基线是生产自深度学习兴起以来，多年来数百到数千名世界级工程师优化了模型，(3) 动态基线 - 工程师可能在 NAS 搜索过程中建立了新的、更强的基线，(4) 效率 - 搜索管道必须产生结果快速与生产生命周期保持一致。在本文中，我们介绍了 Rankitect，这是一个用于 Meta 排名系统的 NAS 软件框架。 Rankitect 寻求通过从头开始构建低级构建块来构建全新的架构。 Rankitect 实现并改进了最先进 (SOTA) NAS 方法，以在同一搜索空间下进行全面、公平的比较，包括基于采样的 NAS、一次性 NAS 和可微分 NAS (DNAS)。我们通过与 Meta 上的多个生产排名模型进行比较来评估 Rankitect。我们发现 Rankitect 可以从头开始发现新模型，实现归一化熵损失和 FLOP 之间的竞争性权衡。当利用工程师设计的搜索空间时，Rankitect 可以生成比工程师更好的模型，实现积极的离线评估和 Meta 规模的在线 A/B 测试。  https://preview.redd.it/mlceky7x7b3c1.png?width=1379&amp;format=png&amp;auto =webp&amp;s=2acc4e0451db9fbf455b03f9e293e68cc61d25bf   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</guid>
      <pubDate>Wed, 29 Nov 2023 16:00:25 GMT</pubDate>
    </item>
    </channel>
</rss>