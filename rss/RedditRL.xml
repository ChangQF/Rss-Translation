<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Thu, 27 Feb 2025 21:16:53 GMT</lastBuildDate>
    <item>
      <title>“培训语言模型，用于通过多机构增强学习学习”，Sarkar等2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/gwern       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</guid>
      <pubDate>Thu, 27 Feb 2025 16:59:12 GMT</pubDate>
    </item>
    <item>
      <title>国际象棋样品效率人与sota rl</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</link>
      <description><![CDATA[From what I know, SOTA chess RL like AlphaZero reached GM level after training on many more games than a human GM played throughout their lives before becoming GM Even if u include solved puzzles, incomplete games, and everything in between, humans reached GM with much lesser games than SOTA RL did (pls correct me if I&#39;m wrong about这）。 是否有比人类效率较低的特定原因/障碍？对于提高国际象棋SOTA RL样本效率的研究是否有希望？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:40:39 GMT</pubDate>
    </item>
    <item>
      <title>离线RL的动作将是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  所以，我是RL的新手，我必须实现离线RL模型，然后在在线RL阶段中微调它。从我的承诺中，离线学习阶段最初的策略和在线学习阶段将使用实时反馈来完善政策。对于离线学习阶段，我将有一个数据集d = {（si，ai，ri）}。数据集中每个示例的操作是否是收集数据时采取的动作（即专家行动）？还是所有可能的动作？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]    ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:03:21 GMT</pubDate>
    </item>
    <item>
      <title>[付费]寻找有人在RL NASH差异游戏上编写快速Python程序</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izgmws/paid_looking_for_someone_to_write_a_quick_python/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，我正在寻找一个为零和非零和nash nash差异游戏编写python程序的python程序。  对于熟练的人来说，这应该是一个小时的工作。愿意为演出支付50美元。给我发消息以获取更多详细信息。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used_chapter007     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izgmws/paid_looking_for_someone_to_write_a_quick_python/</guid>
      <pubDate>Thu, 27 Feb 2025 14:10:05 GMT</pubDate>
    </item>
    <item>
      <title>我被困在瓶颈上，有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izdx0j/i_am_stuck_at_a_bottleneck_any_suggestions_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用称为Rware的RL环境。它给出了RGB数组，但仅在渲染窗口后才提供。因此，我的培训需要大量时间。是否有任何想法绕过或跳过渲染？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/decter_prune_9756      [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1izdx0j/i_am_astuck_at_a_bottleneck_any_suggestions_to//]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izdx0j/i_am_stuck_at_a_bottleneck_any_suggestions_to/</guid>
      <pubDate>Thu, 27 Feb 2025 11:48:05 GMT</pubDate>
    </item>
    <item>
      <title>跨领域的凉爽自我校正机制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  从控制理论的反馈循环和卡尔曼过滤到自然选择，DNA修复，多数投票和引导 - 无数的方式系统自我校正错误，尤其是当地面真理未知时！想知道您遇到的有趣的自我纠正机制是什么，无论是自然界，哲学，工程还是以后？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neat_comparison_2726      [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</guid>
      <pubDate>Wed, 26 Feb 2025 19:48:15 GMT</pubDate>
    </item>
    <item>
      <title>现在，您可以使用GRPO（5GB VRAM最小值）训练自己的推理模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，很棒的人！第一篇文章在这里！今天，我很高兴地宣布，您现在可以使用grpo +使用Grpo +我们的开放式项目unsploth使用5GB VRAM训练自己的推理模型： https：&gt; https：&gt;是DeepSeek-R1背后的算法以及如何受过训练。它比PPO更有效，我们设法将VRAM使用降低了90％。您需要一个大约500行，答案对和奖励功能的数据集，然后可以启动整个过程！ 这允许将任何开放的LLM（如Llame，Misstral，Phi等）等开放，可以将其转换为具有链链过程的推理模型。关于GRPO的最好的部分是，与更大的型号相比，与更大的训练时间相比，与较大的训练时间相比，与较大的型号相比，训练小型型号与较大的型号无关紧要，因此最终结果将非常相似！您也可以在执行其他操作的同时，在PC的背景下进行GRPO培训！  由于我们新添加的有效的GRPO算法，这使得 10x更长的上下文长度长度  90％使用 90％的vram  vram/strong&gt; vraM/strong&gt; lora/qlora/qula li&gt; li afteraive  li&gt; field afteraiment &lt;0&gt; 标准GRPO设置，Llama 3.1（8b）20K上下文长度的培训需要510.8GB的VRAM。但是，Unsploth的90％VRAM减少的要求使同一设置中的需求仅为54.3GB。 我们利用我们的渐变”检查 algorithm，我们发布了一个aLgorithm。它可以巧妙地将中间激活卸载到系统RAM异步，同时仅慢1％。此剃须372GB VRAM ，因为我们需要num \ _ generations = 8。我们可以通过中间梯度累积进一步减少此内存使用。 使用Google的免费上下文使用我们的GRPO Notebook，使用Google的免费gpus： href =“ https://colab.research.google.com/github/unslothai/notebooks/blob/blob/main/nb/llama3.1_(8B”&gt; llama 3.1（8b）on colab  -grpo.ipynb）以及更多： align =“ left”&gt; metric   unsploth   trl + fa2           training Moregre Cost（GB） align =“左”&gt; 414GB      grpo内存成本（gb）   9.8gb    78.3gb  78.3gb  78.3gb    0gb   16gb      推理20K上下文（GB）   2.5GB  2.5gb  总内存使用   54.3GB（少90％）       510.8GB              我们在所有方面都花了很多时间（pboty&gt;  ：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/yoracale     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</guid>
      <pubDate>Wed, 26 Feb 2025 19:41:48 GMT</pubDate>
    </item>
    <item>
      <title>策划可塑性损失的论文清单</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨， 我已经创建了一个存储库，其中包含有关可塑性损失的论文列表。重点是深度RL，但是那里也有一些持续的学习。  https：//github.com/github.com/github.com/probabilistic--interactive-mlaw---interactive-mlaw yourplastive yourplastive plapery plapery plapery  我们还在撰写有关该主题的调查，但仍处于早期阶段：很多牵引力，我希望这可以帮助人们加快速度：）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/timo_kk     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</guid>
      <pubDate>Wed, 26 Feb 2025 16:40:39 GMT</pubDate>
    </item>
    <item>
      <title>以非常折扣价的困惑pro</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  任何有兴趣以50％折扣价获得困惑Pro的人，请与我联系  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/beast_of_iit    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iykcbt/perplexity_pro_at_a_very_very_very_very_very_very_very_discounted_price/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</guid>
      <pubDate>Wed, 26 Feb 2025 10:15:47 GMT</pubDate>
    </item>
    <item>
      <title>RL代理当前在不激励特定行为的情况下最佳执行的最复杂环境是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我很想知道sota在环境复杂性方面，在不需要任何中级奖励的情况下，RL代理执行的性能 - 只是+1 +1 for“ win”和-1为“损失”   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]     32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 07:34:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么某些环境（例如Minecraft）太困难了，而另一些环境（例如Openai's Hide N See Seek）是可行的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   tldr：是什么让hide n寻求可解决的环境，但是我很难解决的我的minecraft或简化的Minecraft环境？ 我没有遇到任何RL代理在Minecraft中成功生存的任何RL代理。理想情况下，如果根据代理商的活力来给予奖励，它至少应该为食物建立庇护所和农场。 ， ，Openai的hide n of from 5年前从5年前开始寻求视频，从划痕中，在那个环境中学到了很多东西，甚至没有激励任何行为。为什么不适用于Minecraft？有一个更容易的环境称为手工艺者，但即使是这样的奖励似乎是这样设计的，以至于最佳行为被激励，而不仅仅是基于生存的奖励，而最佳绩效（Dreamer）仍然没有与人类的绩效相比。 是什么让hide n寻求可解决的环境，但可以解决，但可以解决，但是是如此，但是如此难以解决的或简化的Minecraft环境，以求解Minecraft solve？提交由＆＃32; /u/aliaslight     [link]   [注释] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</guid>
      <pubDate>Wed, 26 Feb 2025 05:29:58 GMT</pubDate>
    </item>
    <item>
      <title>使用深RL的自我标记汽车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iya5jf/selfparking_car_using_deep_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想训练一个PPO型号以并行停车，可成功地将车停在汽车上。你们知道我可以为此目的使用的任何模拟环境吗？另外，训练这样的模型会很长吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iya5jf/selfparking_car_used_usis_deep_rl/​​”&gt; [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iya5jf/selfparking_car_using_deep_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 00:12:17 GMT</pubDate>
    </item>
    <item>
      <title>事后经验重播（她）表现的主要贡献者是什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</link>
      <description><![CDATA[Hello, I have been studying Hindsight Experience Replay (HER) recently, and I’ve been examining the mechanism by which HER significantly improves performance in sparse reward environments. In my view, HER enhances performance in two aspects:  Enhanced Exploration:  In sparse reward环境，如果代理未能达到最初的目标，它几乎无法获得任何奖励，导致缺乏学习信号并迫使代理继续随机进行随机探索。 她通过使用最终状态作为目标来重新定义目标，这使得代理可以通过实际上可以从该过程中启用的各个过程来获得良好的范围。     策略概括：  她将目标与国家一起融入了网络的输入中，使政策能够有条件地学习，以实现州和规定的目标。  因此，通过捕获各种目标之间的关系并没有直接实现的目标，从某种程度上来实现。       在这些点上，我很好奇哪个因素 - 在她的范围或政策普遍的问题上，我在如果状态空间为r  2 &lt; /sup&gt;，目标是（2,2），但是代理人恰好仅在第二个象限内探索，则最终状态将被限制在该地区。在这种情况下，该政策可能很难将其推广到探索区域之外的（2,2）之类的目标。这样的限制会如何影响她的表现？提交由＆＃32; /u/drlc_     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iy1ta1/what_is_the_primary_contributor_to_hindsight/</guid>
      <pubDate>Tue, 25 Feb 2025 18:20:38 GMT</pubDate>
    </item>
    <item>
      <title>Q学习，折扣系数为0。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在研究一个项目，以实现Q-学习的代理。我只是意识到对环境，状态和动作进行了配置，因此当前的行动不会影响未来的状态或奖励。我认为在这种情况下，折现因子应该等于零，但是我不知道Q学习代理是否有意义解决此类问题。在我看来，它比MDP更像是上下文的强盗问题。 Q学习算法的名称为0，或等效算法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ixzkgs/qlearning_with_a_a_discount_factor_of_0/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixzkgs/qlearning_with_a_discount_factor_of_0/</guid>
      <pubDate>Tue, 25 Feb 2025 16:50:04 GMT</pubDate>
    </item>
    <item>
      <title>现在，增强型套图支持PPO！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  嘿，大家，  renforceui-studio现在包括接近策略优化（ppo）！🚀 href=&quot;https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/&quot;&gt;here), I introduced ReinforceUI-Studio as a tool to make training RL models easier. I received many requests for PPO, and it&#39;s finally here!如果您有兴趣，请检查一下，让我知道您的想法。另外，保持算法请求的到来 - 您的反馈有助于使工具变得更好！  文档： https://docs.reinforceui-studio.com/algorithms/algorithm_list/algorithm_list  href =“ https://github.com/dvalenciar/reinforceui-studio”&gt; https://github.com/dvalenciar/reinforceui-studio       &lt;！提交由＆＃32; /u/u/dvr_dvr     [link]        [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixq4nc/reinforceuistudio_now_supports_ppo/</guid>
      <pubDate>Tue, 25 Feb 2025 08:18:11 GMT</pubDate>
    </item>
    </channel>
</rss>