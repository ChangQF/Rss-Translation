<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 03 Mar 2025 18:24:08 GMT</lastBuildDate>
    <item>
      <title>对于RL项目中控制策略的输入，我应该包括重要但固定的信息吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2p4f9/for_the_observation_vector_as_input_to_the/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我正在尝试使用PPO算法来训练新型的机器人操纵器，以达到其工作空间中的目标位置。我应该将观察向量作为控制策略的输入的观察向量？当然，我应该在观察矢量中包括相关状态，例如当前的操纵器形状（关节角度）。 ，但我担心以下两个状态/信息在观察矢量中的包含：1）：最终效应子的位置，可以根据关节角度易于计算出最终效应子的位置。这是令人困惑的，因为最终效应子的位置是重要的状态/信息。它将用于计算最终效应器与目标位置之间的距离，以确定奖励，以终止情节。但是，我可以将末端效应子的位置排除在观察矢量之外，因为它可以从关节角度轻松确定。关节角度和关节角依赖性最终效应子是否形成冗余？  2）：障碍物的位置。障碍物的位置也是重要的状态/信息。它将用于计算/检测操纵器和障碍物之间的碰撞，以在检测到的碰撞时施加惩罚，如果检测到碰撞，则终止发作。但是，由于障碍物在整个学习过程中保持固定，我可以将障碍物的位置排除在观察矢量之外吗？我根本不会改变障碍物的位置。是否需要将障碍物包含在观察矢量中？ href =“ https://preview.redd.it/iblf1bo6mime1.png？ https://preview.redd.it/iblf1bo6mime1.png?width=626＆amp; format = png＆amp; auto = webpp＆s = 2b9c71e0e71e7cdaea637bad2bad2b74949e1dc08cf2cf2cf2005d88cf2005d8  发布了一个非常相似的问题 https://ai.stackexchange.com/questions/46173/the-observation-pace-space-of-a-robot-arm-should-should-include-include-the-tharget-target-target-position-or-inly 但没有答案。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tilly_shift7974     [link]        [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2p4f9/for_the_observation_vector_as_input_to_the/</guid>
      <pubDate>Mon, 03 Mar 2025 18:09:50 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习中的当前障碍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2p3u8/current_roadblocks_in_model_based_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   title   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2p3u8/current_roadblocks_in_model_based_reinforcement/</guid>
      <pubDate>Mon, 03 Mar 2025 18:09:11 GMT</pubDate>
    </item>
    <item>
      <title>寻找帮助培训在2D电路上学习AI的强化AI（Pygame + Gym + StableBaselines3）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2n47n/looking_for_help_training_a_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家， 我正在研究一个项目，在该项目中，我需要训练AI使用加固学习来导航2D电路。代理将收到以下输入：  5传感器（射线）：向前，左，向前，向前，右，右，右右→他们返回AI和障碍物之间的距离。 作为动作的加速度值。   我已经在Pygame中具有适用于Pygame的工作环境，并且我已经对其进行了适应的工作，并且可以兼顾它。但是，当我尝试使用stablebaselines3的模型3时，我会得到一个黑屏（根据Chatgpt，这可能是由于使用DummyveCenv进行了转换所致）。 因此，如果您知道简单而快速的方法可以有效地训练AI，或者有效地训练AI，或者我可以使用预先训练的型号，我可以使用它，我可以使用它！ sc_on-&gt;＆＃32;提交由＆＃32; /u/u/pt_quill     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2n47n/looking_for_help_training_a_reinforcement/</guid>
      <pubDate>Mon, 03 Mar 2025 16:48:56 GMT</pubDate>
    </item>
    <item>
      <title>多discrete offlicy</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2myun/multidiscrete_offpolicy/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是否有使用Multi-Discrete（带有Gumbel）（带gumbel）的算法（例如TD3/7 DDPG）的实现，或者我注定要使用PPO，如果我想使用多discrete Actions Actions Space（和不flatten）（并且不flattent It It）/u/what_did_it_it_cost_e_t      [link]   ＆＃32;   [comment]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2myun/multidiscrete_offpolicy/</guid>
      <pubDate>Mon, 03 Mar 2025 16:42:38 GMT</pubDate>
    </item>
    <item>
      <title>与（PSO）颗粒群优化杂交多代理增强学习（MARL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2kh0m/hybridizing_multi_agent_reinforcement_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是我的学士学位论文的计算机科学专业学生，我的主题是“基于智能算法的决策，用于搜索和救援中的群体机器人”。我对此一无所知，因此经过一些文献综述，我想我喜欢制作PSO+MARL混合算法的想法，以使群体机器人技术更快，更适应于搜索和救援环境。但是我仍然有0个背景，我不知道这是否是个好主意，我不知道它是否可行，所以我想知道是否有人知道如何开始或我应该改变我的方法？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/amrhesham2424    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j2kh0m/hhybridizing_multi_agent_reinforection_learche_learning/”&gt; [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2kh0m/hybridizing_multi_agent_reinforcement_learning/</guid>
      <pubDate>Mon, 03 Mar 2025 14:57:06 GMT</pubDate>
    </item>
    <item>
      <title>[D]没有赢家和未知最佳分数的游戏的加强学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j2e1w5/d_reinforcement_learning_for_games_with_no_winner/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在即将到来的项目中，我需要在笼子内部包装盒子和密集。但是，这些盒子将一次到达一个，并带有随机尺寸和形状。目的是尽可能地填充笼子（理想情况下是100％，但在大多数情况下这是无法达到的）。 这个问题在传统上是一个离散的优化问题，但是由于我们不知道这些包裹在到达前的包装上，所以我怀疑它们确实是正确的，而且我真的很认为，这实际上是我的一点点，如果没有我的一点点我，这实际上是一定的。以前的强化学习，但总是适用于有赢家和宽松的游戏。但是，在这种情况下，我们没有。因此，当我在游戏结束时唯一的数字是0-1之间的数字，而1是完美的，但在大多数游戏中也可能无法实现。 我认为我多次重复每个游戏。因此，您可以获得完全相同的软件包配置，因此可以与该配置上的以前的游戏进行比较，并根据模型比以前做得更好或更糟糕的是奖励该模型，但是我不确定这是否效果很好。 有人是否有这样的经验？提交由＆＃32; /u/u/alyflex     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j2e1w5/d_reinforcement_learning_for_games_with_no_winner/</guid>
      <pubDate>Mon, 03 Mar 2025 08:24:06 GMT</pubDate>
    </item>
    <item>
      <title>欧洲可以做什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1yynu/what_can_an_europoor_do/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我是欧盟公民。我在这里问，因为我不知道该如何处理我的RL激情。 我在应用数学方面有广泛的背景，并且在数据科学方面做了硕士学位。过去2年过去了，我一直在医疗保健行业担任AI工程师。自从我从事机器人技术的研究实习以来，我就爱上了RL。问题在于，我在欧盟看到了0个作业，我可以申请少数博士学位（他们不会在其他地方赞助我）。   ，但是，我觉得非学生没有博士学位的机会（没有网络），而且我没有选择。我正在考虑在Uni中使用一个良好的RL/Robotics Lab做另一个大师，即使这可能是浪费时间。关于去哪里或从这里遵循什么途径的任何建议？我一直想进行研究，但是它看起来显得黯淡。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;  /u/Exkur   [link] ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1yynu/what_can_an_europoor_do/</guid>
      <pubDate>Sun, 02 Mar 2025 19:21:53 GMT</pubDate>
    </item>
    <item>
      <title>我们如何在离线学习中使用重播缓冲区？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1vuxo/how_do_we_use_the_replay_buffer_in_offline/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 如果您收集了一个巨大的数据集用于我的离线学习。有数百万个例子。我在线阅读，通常您会将整个数据集上传到重播缓冲区中。但是，对于数据集很大的情况，这将是一个巨大的内存开销。您将如何解决此问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1vuxo/how_do_we_use_the_replay_buffer_in_offline/</guid>
      <pubDate>Sun, 02 Mar 2025 17:15:02 GMT</pubDate>
    </item>
    <item>
      <title>关于DQN的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1r1pj/a_problem_about_dqn/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   dqn算法的输出只能是一个操作？  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/clean_tip3272     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1r1pj/a_problem_about_dqn/</guid>
      <pubDate>Sun, 02 Mar 2025 13:39:08 GMT</pubDate>
    </item>
    <item>
      <title>最佳提交Tinker AI的第二次比赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1qekd/best_submission_of_tinker_ais_second_competition/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/goncalogordo       [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1qekd/best_submission_of_tinker_ais_second_competition/</guid>
      <pubDate>Sun, 02 Mar 2025 13:05:10 GMT</pubDate>
    </item>
    <item>
      <title>使用DQN帮助解决山车问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好， 在开始之前，我想道歉，问问这个问题，因为我猜想这个问题可能已经被问到很多次了。我试图教自己增强学习，并且正在研究这款MountrainCar迷你项目。  我的模型似乎根本没有融合。我使用情节持续时间与情节编号的情节来检查/分析性能。我注意到的是，有时，对于我尝试过的所有架构，情节持续时间都会有所减少，然后再次增加。  I have tried doing the following things:  Changing the architecture of the Fully Connected Neural network. Changing the learning rate Changing the epsilon value, and the epsilon decay values.  For neither of these changes, I got a model that seems to converge during training.我平均培训了1500个持续时间。这就是每个模型通常看起来的图：   有没有适合此特定问题的技巧，特定的DQN体系结构和超参数范围？还有一组指南，应该牢记并用来创建这些DQN模型？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/lowkeysuicidal14     [link]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</guid>
      <pubDate>Sun, 02 Mar 2025 04:17:05 GMT</pubDate>
    </item>
    <item>
      <title>帮助2D峰搜索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1fdsa/help_with_2d_peak_search/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我使用不同的体育馆环境有很多RL经验，使用SB3，CleanRl以及我自己实施的算法，获得了相当不错的性能。这就是为什么我对一个事实感到恼火的是，我似乎无法在玩具问题上取得任何进展，我已经为我的工程领域中的某些优化任务实现了RL。 这个问题本质上是一个优化问题，在该问题中，该代理的任务是在2D空间中找到最佳的启动参数（对于启动器中的最佳参数），以便以启动以进行启动，以供某些参数，以供一些参数，以供一些参数，以供某些参数，以便以7个访问量。在所使用的一组参数上的分布是值的，有些不连续性，这就是为什么我制作了一个玩具环境的原因，在每个情节中，都会产生测量值的高斯分布，具有不同的均值和协方差。该代理的任务是选择一组值，范围从0-36，使用CNN策略使SB3实现更简单，然后以该集的参数集以分布值的形式接收反馈。状态空间是测量值的2D图像，所有初始值均设置为0，随着代理探索的填充。我正在使用的动作空间是一个多discrete空间，[0-36，0-36，0-1]，最后一个动作是代理是否认为这套参数是最佳的。我尝试使用PPO和A2C，而性能差异很小。 现在，问题是取决于我如何构造奖励的方式，我无法找到最佳的参数集。提供1个以找到正确参数的反馈的天真方法通常会失败，这可以通过在此环境中的随机策略的相当稀疏的奖励来解释。因此，我试图为每项动作提供增量奖励，该动作会根据上次动作进行改进，这取决于分布的值或距离到最佳的距离，如果实际上找到了峰值，则具有很大的奖励。这有点好，但是代理商总是为了一项政策而定，即它在山上中途走了一半，然后就解决了，再也没有找到实际的峰值。我没有对进行大量测量的任何惩罚（现在），以便代理商可以进行详尽的搜索，但这永远不会做到这一点。  在我如何设置环境或结构奖励的方式中，我是否缺少什么？我是否可以研究类似的项目或纸张？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j1fdsa/help_with_with_2d_peak_search/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1fdsa/help_with_2d_peak_search/</guid>
      <pubDate>Sun, 02 Mar 2025 01:30:54 GMT</pubDate>
    </item>
    <item>
      <title>如何将RL与刚体机器人与流体相互作用的刚性机器人集成？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j17hl3/how_to_integrate_rl_with_rigid_body_robots/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想使用强化学习来教2-3个链接机器人鱼游泳。机器人鱼是一个三维固体物体，它将感觉到各个方面的水力。什么模拟器将有用，以便我可以对刚体机器人和周围的流体力之间的相互作用进行建模？  我需要它能够将RL集成到其中。与基于CFD的模拟（COMSOL，ANSYS，基于FEM等）不同，它也应该很快呈现物理学。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kingalvez     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j17hl3/how_to_integrate_rl_with_rigid_body_robots/</guid>
      <pubDate>Sat, 01 Mar 2025 19:25:09 GMT</pubDate>
    </item>
    <item>
      <title>帮助交易的Q学习模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j14faj/help_with_qlearning_model_for_trading/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好， 我已经使用健身房环境实现了一个Q学习的交易机器人，但我注意到一些奇怪的（至少对我来说）结果。在训练了1500集的Q桌子之后，特定股票的市场回报率为156％，而投资组合收益（由Q-table策略生成）是极高的 76,445.94％，这对我来说似乎是不现实的。 Could this be a case of overfitting or another issue? When testing, the results are:  Market Return: 33.87% Portfolio Return: 31.61%  我还拥有每集总奖励的情节，并在情节中累积了奖励： 如有必要，我可以共享我的代码，以便有人可以帮助我解决这个问题。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/ligabo69     [link]       [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j14faj/help_with_qlearning_model_for_trading/</guid>
      <pubDate>Sat, 01 Mar 2025 17:14:34 GMT</pubDate>
    </item>
    <item>
      <title>离线RL算法对10^-6的奖励敏感？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0wyol/offline_rl_algorithm_sensitive_to_perturbations/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我正在在D4RL基准测试台上数据集（特别是Hopper重播数据​​集）上运行离线RL算法（特别是隐式Q学习）。我看到，奖励的小扰动，以10^-6的顺序导致训练结果截然不同。当然，这是所有东西上的固定种子。  我知道RL在许多方面（超参数，模型体系结构，奖励等）中的小扰动可能非常敏感。但是，这对我的奖励的变化很敏感，这使我感到惊讶。对于那些具有更多实施这些算法的经验的人，您认为这是期望的吗？还是会暗示算法实现出了问题？ 如果有些预期，这是否会引起疑问，这在离线RL中已发表的许多已发表的工作吗？例如，您可以修复种子和超级参数，但是随后在CUDA与CPU上运行奖励模型可以导致奖励值的差异为10^-6    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used-eagle-9302     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0wyol/offline_rl_algorithm_sensitive_to_perturbations/</guid>
      <pubDate>Sat, 01 Mar 2025 10:53:49 GMT</pubDate>
    </item>
    </channel>
</rss>