<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 22 Jan 2024 09:15:42 GMT</lastBuildDate>
    <item>
      <title>珍珠 vs 火炬RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cr8ih/pearl_vs_torchrl/</link>
      <description><![CDATA[这里有人使用过这两个框架，或者对这两个框架有足够的了解吗？   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cr8ih/pearl_vs_torchrl/</guid>
      <pubDate>Mon, 22 Jan 2024 08:44:00 GMT</pubDate>
    </item>
    <item>
      <title>我用 3D 动画教这个机器人自己行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</link>
      <description><![CDATA[       由   提交/u/djessimb   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</guid>
      <pubDate>Mon, 22 Jan 2024 07:08:18 GMT</pubDate>
    </item>
    <item>
      <title>有人可以解释一下 Pytorch 对于有界连续动作空间的“反转梯度”吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cmxnn/could_someone_explain_this_pytorch_inverting/</link>
      <description><![CDATA[我无法真正理解反转渐变是如何实现的。我知道在原始论文中它是通过 TD 学习和确定性策略梯度（DPG）以及参数化动作空间设置来实现的。我试图理解它，以便可以通过正常的情景强化和学习的基线来实现它，但我无法真正理解它。乍一看似乎很直观，但当尝试实现时，我意识到我不知道发生了什么。 我意识到我可以将动作空间压缩到我想要的范围，[0,1 ]，但本文建议反转梯度更好。 乍一看，这是有道理的。如果梯度为正且 p 高于最大值，则梯度将反转，等等。但是为什么 Q 网络的梯度与参数 p （Hausknecht 论文中的操作）有关？是因为它使用了 DPG 吗？ 论文： https://arxiv.org/pdf /1511.04143 相关部分： https://i.imgur .com/fnJqY92.png 还有另一篇论文的描述略有不同。这清楚地表明它是策略网络（而不是 Q 网络）的衍生品。所以我很困惑。 https://arxiv.org/pdf/1910.02208 https://i.imgur.com/eH8UTE7.png   由   提交 /u/JustTaxLandLol   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cmxnn/could_someone_explain_this_pytorch_inverting/</guid>
      <pubDate>Mon, 22 Jan 2024 04:13:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 进行深度 SARSA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cko2w/deep_sarsa_with_tensorflow/</link>
      <description><![CDATA[大家好。我的任务是在工作中创建 Deep SARSA 模型，我唯一可以使用的工具是tensorflow（出于安全原因，我无法安装任何其他库，例如 tf_agents）。所以，我的问题是：是否可以像使用 Pytorch 一样使用张量流创建深度 SARSA 模型？就像能够调用优化器、重置梯度、应用反向传播并更新目标值 NN 的权重一样，就像 Pytorch 让我们这样做的方式 ​ This这是我的意思的一个例子（我之前用 Pytorch 实现了 Deep SARSA 模型） https://github.com/edseldim/reinforcement_learning/blob/master/6_deep_sarsa_ideas.ipynb ​ 非常感谢您的回答和建议:)   由   提交/u/Confident_Watch8207  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cko2w/deep_sarsa_with_tensorflow/</guid>
      <pubDate>Mon, 22 Jan 2024 02:16:00 GMT</pubDate>
    </item>
    <item>
      <title>编程…</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</link>
      <description><![CDATA[       由   提交/u/Throwawaybutlove  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</guid>
      <pubDate>Mon, 22 Jan 2024 01:27:44 GMT</pubDate>
    </item>
    <item>
      <title>“基于模型的贝叶斯探索”，Dearden 等人 2013</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ch8dg/modelbased_bayesian_exploration_dearden_et_al_2013/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ch8dg/modelbased_bayesian_exploration_dearden_et_al_2013/</guid>
      <pubDate>Sun, 21 Jan 2024 23:31:05 GMT</pubDate>
    </item>
    <item>
      <title>纯 C# 深度强化学习彗星到 Godot 作为 nuget</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ccqxy/pure_c_deep_reinforcement_learning_comets_to/</link>
      <description><![CDATA[    /u/DotNetEvangeliser   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ccqxy/pure_c_deep_reinforcement_learning_comets_to/</guid>
      <pubDate>Sun, 21 Jan 2024 20:22:59 GMT</pubDate>
    </item>
    <item>
      <title>采访麻省理工学院林肯实验室的 Zack Serlin：正式方法......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bkd00/interview_with_zack_serlin_mit_lincoln/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bkd00/interview_with_zack_serlin_mit_lincoln/</guid>
      <pubDate>Sat, 20 Jan 2024 19:58:50 GMT</pubDate>
    </item>
    <item>
      <title>DQN 代理奖励向后断言错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bihrt/dqn_agent_reward_backward_assertion_error/</link>
      <description><![CDATA[我正在学习强化学习并尝试从论文中复制模型；目标是控制（1x 连续动作）1/4 汽车悬架系统并最大限度地减少随机路面上的悬架行程。我正在使用 keras.rl2 中的深度 Q 网络。 我将代码上传到 github：https://github.com/htmdn/QuarterCarSuspControl/blob/main/DDPG_Susp_Control_02.ipynb 这是我收到的错误：  断言错误回溯（最近一次调用最后）单元格In[16]，第1行----&gt; 1 dqn.fit(env, nb_steps=50000, Visualize=False, verbose=1) 文件C:\apps\anaconda3\envs\x\lib\site-packages \rl\core.py:193，在 Agent.fit(self、env、nb_steps、action_repetition、回调、详细、可视化、nb_max_start_steps、start_step_policy、log_interval、nb_max_episode_steps) 190 if nb_max_episode_steps and Episode_step &gt;= nb_max_episode_steps - 1: 191 # 强制进入终止状态。 192 完成 = 正确 --&gt; 193 指标= self.backward（奖励，终端=完成） 194 Episode_reward +=奖励 196 步骤_logs = { 197 &#39;action&#39;：行动， 198 &#39;观察&#39;：观察，（...） 202 &#39;info&#39;:cumulative_info, 203 } 文件 C:\apps\anaconda3\envs\x\lib\site-packages\rl\agents\dqn.py:271，位于 DQNAgent.backward 中(self,reward,terminal) 269terminal1_batch = np.array(terminal1_batch) 270reward_batch = np.array(reward_batch) --&gt; 271 断言reward_batch.shape == (self.batch_size,) 272 断言terminal1_batch.shape ==reward_batch.shape 273 断言 len(action_batch) == len(reward_batch)   非常感谢任何反馈！   由   提交 /u/htmdn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bihrt/dqn_agent_reward_backward_assertion_error/</guid>
      <pubDate>Sat, 20 Jan 2024 18:38:05 GMT</pubDate>
    </item>
    <item>
      <title>MuDreamer：无需重建即可学习预测世界模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19bdlml/mudreamer_learning_predictive_world_models/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=9pe38WpsbX 摘要：  DreamerV3 代理最近展示了状态 -在不同领域中表现最先进，使用像素重建损失在潜在空间中学习强大的世界模型。然而，虽然重建损失对于 Dreamer 的性能至关重要，但它也需要对不必要的信息进行建模。因此，梦想家有时无法感知解决任务所需的关键要素，从而极大地限制了其潜力。在本文中，我们提出了 MuDreamer，这是一种基于 DreamerV3 算法的强化学习代理，通过学习预测世界模型而无需重建输入信号。隐藏表示不是依赖于像素重建，而是通过预测环境值函数和先前选择的动作来学习。与图像的预测自监督方法类似，我们发现批量归一化的使用对于防止学习崩溃至关重要。我们还研究了模型后验损失和先验损失之间的 KL 平衡对收敛速度和学习稳定性的影响。我们在广泛使用的 DeepMind Visual Control Suite 上评估 MuDreamer，并获得与 DreamerV3 相当的性能。 MuDreamer 还在 Atari100k 基准测试中展示了可喜的结果。研究代码将公开。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19bdlml/mudreamer_learning_predictive_world_models/</guid>
      <pubDate>Sat, 20 Jan 2024 15:00:47 GMT</pubDate>
    </item>
    <item>
      <title>在首次达到目标状态之前，具有优势归一化的 PPO 如何在 MountainCar-v0 中学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19b6ivo/how_does_ppo_with_advantage_normalization_learn/</link>
      <description><![CDATA[我试图首先弄清楚 PPO 如何在像体育馆的 MountainCar-v0 这样的稀疏环境中学习任何东西。曾经达到目标状态。 特别关注 stable_baselines3 的 PPO 实现 env = make_vec_env(&#39;MountainCar-v0&#39;, n_envs=16) model = PPO(&#39; MlpPolicy&#39;、env、verbose=1、learning_rate=1e-3、gamma=0.99、gae_lambda=0.98、ent_coef=0.0、n_steps=16、normalize_advantage=True)  我进行了不同的实验并在环境首先达到目标状态时记录。 在上述设置中，通常会在大约 50-150k 时间步内首先达到目标状态。 我进行了一个单独的实验，其中我只是随机选择每一步的操作（因此没有进行“学习”），并且它基本上永远不会达到目标状态（在 200 步情节限制内）。如果学习率设置为 0（仅模仿随机动作），情况也是如此，因此似乎正在进行某种学习。 此外，当 n_envs 设置为 1 或 normalize_advantage 时如果它看到的每个状态都会给出相同的奖励（-1），我很困惑 PPO 在第一次达到目标状态之前如何学习任何东西。我在 MountainCar-v0 中没有看到任何奖励塑造，并且在 PPO 实现中没有看到任何好奇心。 我错过了什么？谢谢   由   提交/u/happysushi2  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19b6ivo/how_does_ppo_with_advantage_normalization_learn/</guid>
      <pubDate>Sat, 20 Jan 2024 07:42:51 GMT</pubDate>
    </item>
    <item>
      <title>Lunai：Lunai是一款无代码、简单易用的GUI、强化学习Ai</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19b4y35/lunai_lunai_is_a_codefree_simple_and_easy_to_use/</link>
      <description><![CDATA[   /u/Feralzi  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19b4y35/lunai_lunai_is_a_codefree_simple_and_easy_to_use/</guid>
      <pubDate>Sat, 20 Jan 2024 06:02:30 GMT</pubDate>
    </item>
    <item>
      <title>试验自定义游戏环境算法的最佳实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19az7f4/best_practice_for_experimenting_with_algorithms/</link>
      <description><![CDATA[我是个 RL 菜鸟。我的目标是创建一个信息不完善的多人棋盘游戏环境，并训练代理在其中玩游戏。 我应该遵循哪些最佳实践？我应该从头开始实现所有逻辑吗？我可以实现哪些库和接口以获得更连贯的体验并学习使用 RL 中使用的规范包？   由   提交/u/fool126  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19az7f4/best_practice_for_experimenting_with_algorithms/</guid>
      <pubDate>Sat, 20 Jan 2024 00:59:28 GMT</pubDate>
    </item>
    <item>
      <title>本文如何将两项政策结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</link>
      <description><![CDATA[      我正在阅读这篇论文，我对算法 1 中的一些细节有点迷失 -  ​ ​ https://preview.redd.it/ f0hhmaye8edc1.png?width=845&amp;format=png&amp;auto=webp&amp;s=a99793f4a5bc099c1a165145aeb44e4db441d95c 在第 3 行中，它们似乎通过 $h 组合了 $h$ 和 $f$ (s) = \pi(s) + f(s)$。我不明白这是怎么发生的。  他们在本节中称 $h$ 为混合策略，但我不明白 -  ​ https://preview.redd.it/4flfgn1x8edc1.png?width=851&amp;format =png&amp;auto=webp&amp;s=a2108806ba92d46e9afcd7668dc9c78ee58120e9 ​ 如果需要任何说明，请告诉我。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</guid>
      <pubDate>Fri, 19 Jan 2024 12:52:43 GMT</pubDate>
    </item>
    <item>
      <title>[需要建议/反馈]第2部分：决斗双倍DQN奖励大多在0和2之间波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</guid>
      <pubDate>Fri, 19 Jan 2024 09:19:09 GMT</pubDate>
    </item>
    </channel>
</rss>