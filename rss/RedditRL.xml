<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 04 Mar 2024 12:27:54 GMT</lastBuildDate>
    <item>
      <title>项目主题帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b68x6x/help_for_project_topic/</link>
      <description><![CDATA[我这学期的科目是强化学习，我的期末评估是一个基于强化学习的项目。我计划在国际象棋中应用强化学习，但我的导师拒绝了这个主题，因为它已经被很多人完成了，并且与我的主题不“相关”。问题显然。现在我必须选择一个项目主题，我到处研究过，但找不到一个符合他期望的项目主题。我需要一个项目主题来工作，但很多人都没有做过，或者最糟糕的是举例一个可以进行对比分析的项目。请帮忙   由   提交/u/Karthic_2811   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b68x6x/help_for_project_topic/</guid>
      <pubDate>Mon, 04 Mar 2024 11:43:19 GMT</pubDate>
    </item>
    <item>
      <title>ml 药剂中的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b672v5/ppo_in_ml_agents/</link>
      <description><![CDATA[如果我有一个多代理问题，所有代理都按照相同的模型（同质代理）行事 - 进行训练，如果我使用 PPO，我会得到像 POCA 那样的多智能体倾斜？  我的意思是，在这种情况下，PPO 会利用联合状态行动空间吗？   由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b672v5/ppo_in_ml_agents/</guid>
      <pubDate>Mon, 04 Mar 2024 09:47:18 GMT</pubDate>
    </item>
    <item>
      <title>UniROS 简介：基于 ROS 的机器人强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5xepa/introducing_uniros_rosbased_reinforcement/</link>
      <description><![CDATA[大家好！  我很高兴与大家分享 UniROS，这是我开发的一个基于 ROS 的强化学习框架，旨在弥合模拟与现实世界机器人之间的差距。该框架包含两个关键包：  MultiROS：非常适合使用 ROS 和 Gazebo 创建并发 RL 模拟环境。 RealROS&lt; /strong&gt;：专为在真实的机器人环境中应用 ROS 而设计。  UniROS 的与众不同之处在于它可以轻松地从模拟过渡到现实世界的应用程序，使强化学习对于机器人专家来说更容易使用和有效. 我还为一些低级 ROS 功能添加了额外的 Python 绑定，从而增强了 RL 工作流程之外的可用性。 我很乐意获得您对这些的反馈和想法工具。让我们讨论如何应用和改进它们！ 在 GitHub 上查看它们：  UniROS： github.com/ncbdrck/UniROS RealROS：github.com/ncbdrck/realros MultiROS：  github.com/ncbdrck/multiros  ​   由   提交 /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5xepa/introducing_uniros_rosbased_reinforcement/</guid>
      <pubDate>Mon, 04 Mar 2024 00:48:51 GMT</pubDate>
    </item>
    <item>
      <title>适用于低维观测环境的基于模型的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5x7i3/modelbased_rl_for_environments_with_low/</link>
      <description><![CDATA[在阅读有关 MBRL 的论文时，我意识到所有这些方法都是通过基于像素的观察来评估其算法在环境中的性能。然而，很多时候，尤其是在机器人技术中，人们可以访问结构化特征，如 x 位置、y 位置、z 位置、旋转等。 创建一个模型是否有意义？这里的环境适合规划吗？因为即使可以访问结构化信息，模拟的计算成本仍然很高。因此，我认为 MBRL 在这里是有意义的，但我还没有找到关于该特定利基的任何工作。 如果有任何论文推荐，我将不胜感激。    由   提交/u/ijustwanttostudy123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5x7i3/modelbased_rl_for_environments_with_low/</guid>
      <pubDate>Mon, 04 Mar 2024 00:39:34 GMT</pubDate>
    </item>
    <item>
      <title>端到端 JAX 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5uep5/endtoend_jax_library/</link>
      <description><![CDATA[大家好，我想我应该在这里发个帖子来宣布我最近发布的一个项目。  该项目是一个流行算法实现库，但使用 DeepMind anakin 训练范例。这可能被某些人称为端到端强化学习，其中算法的每个方面都在硬件加速器上运行。对于那些了解 PureJaxRL 的人来说，它是该库的根本构建基础。我的目标是创建一个研究框架（随着时间的推移进行维护和改进），它提供 4 个主要功能： 1. 高效、快速且可扩展的 RL 算法实现，以允许快速迭代研究。 2. 一个易于破解的代码库作为研究项目的起点。 3.标准化基线的高性能算法实现。 4. 教育价值。 紧密遵循 cleanRL 的理念，每种算法的大部分代码都在单个文件中提供，但某些元素已标准化以便重用，例如网络架构、评估和日志记录。该代码库还提供了有用的功能，例如检查点。这个项目还比较新，我只在空闲时间做它。对于任何反馈，我们都表示感谢。如果您有任何疑问，请告诉我。该项目可以在这里找到： https://github.com/EdanToledo/Stoix   由   提交/u/WorkingManTech  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5uep5/endtoend_jax_library/</guid>
      <pubDate>Sun, 03 Mar 2024 22:40:22 GMT</pubDate>
    </item>
    <item>
      <title>离线政策学习的深度生成模型：教程、调查和对未来方向的展望</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5tq4v/deep_generative_models_for_offline_policy/</link>
      <description><![CDATA[ 由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5tq4v/deep_generative_models_for_offline_policy/</guid>
      <pubDate>Sun, 03 Mar 2024 22:13:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Haskell 中使用值迭代</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5qpbe/playing_with_value_iteration_in_haskell/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5qpbe/playing_with_value_iteration_in_haskell/</guid>
      <pubDate>Sun, 03 Mar 2024 20:12:37 GMT</pubDate>
    </item>
    <item>
      <title>持续强化学习和元强化学习研究社区</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5gwsi/continualrl_and_metarl_research_communities/</link>
      <description><![CDATA[我对 RL（连续 RL、元 RL、变压器）对超参数的敏感性和大量的训练时间越来越感到沮丧（我讨厌 RL 之后5年博士研究）。这在元强化学习连续强化学习中尤其成问题，其中一些基准要求长达 100 小时的训练。这使得优化超参数或快速验证新想法的空间很小。考虑到这些挑战以及我准备更深入地探索数学理论，包括学习所有可用的在线数学课程，采用基于证明的方法，以避免无休止的等待和训练循环，我对 2024 年密切相关的人工智能研究领域趋势感到好奇强化学习，但最多只需要 3 个小时的训练时间。有什么建议吗？   由   提交 /u/Noprocr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5gwsi/continualrl_and_metarl_research_communities/</guid>
      <pubDate>Sun, 03 Mar 2024 13:14:49 GMT</pubDate>
    </item>
    <item>
      <title>我应该编写自己的深度强化学习算法还是寻找开源项目？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5bqth/should_i_write_my_own_deep_reinforcement_learning/</link>
      <description><![CDATA[我是一名研究多智能体深度强化学习的研究生。最近在复现算法方面陷入了自我否定。 正如标题所说，我是应该使用star多的开源GitHub项目框架，还是按照教学博客手动实现算法或者教程？ 在此之前，我一直在根据论文、教学博客、教程等（如YOUTUBE、spin up（openai）等资源）复现算法。我已经成功复现了DQN、DDPG、SAC、MADDPG算法，并且我可以保证没有问题，因为所有的结果都收敛到了一定的良好效果。 这两天遇到的一个问题是当我训练自己的多智能体环境时，我发现我写的maddpg算法训练时间很长，大约需要3天才能收敛。使用的学习率为0.00001（只有使用这么小的学习率才能更好的收敛），有人推荐我使用github上很多star的深度强化学习算法框架，否定了我重新发明轮子的行为。 &lt;当然，我也承认我重新发明轮子的行为很可笑，但是从我自己的学习经历来看，强化学习是非常复杂的。我用于研究的环境是我自己搭建的，不是健身房。数据的处理贯穿强化学习。所以我想听听你的建议或经验。 最后，我找到了为什么我的代码训练这么慢。这是因为我的主循环处于一个纪元中。每执行一步后，神经网络梯度就会更新。我改为先填充经验池，然后多次训练神经网络，直到大约遍历整个经验池，然后停止训练，然后与环境交互来填充经验池。经过这个操作，我发现我可以将训练速度提高10倍。 （nvidia gpu使用cuda有启动时间吗，这个很有趣）   由   提交 /u/yuemingyue   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5bqth/should_i_write_my_own_deep_reinforcement_learning/</guid>
      <pubDate>Sun, 03 Mar 2024 07:51:56 GMT</pubDate>
    </item>
    <item>
      <title>图注意力网络（GAT）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4wisu/graph_attention_network_gat/</link>
      <description><![CDATA[你好， 我有一个看似微不足道的查询（我是 GAT 的新手）。  目前，我有一个表示为 Data(x=[24, 6], edge_index=[2, 23]) 的状态。将其传递到图注意网络 (GAT) 层后，我获得了 torch.Size([24, 64]) 的输出形状。 随后，我的目标是将这个输出输入到一个模块中，该模块应该产生两个概率，但它返回 torch.Size([24, 2]) 的形状（对于每个节点，它输出 2 个概率）。我担心仅获得两个概率作为输出。您能否建议如何实现这一目标？下面是代码的快照： class Policy(nn.Module): def __init__(self, state_dim, action_set1_dim, action_set2_num_heads): super(Policy, self). __init__() self.gat_conv1 = GATConv(state_dim[-1], 64, 头=1, dropout=0.5) self.gat_conv2 = GATConv(64, 64, 头=1, dropout=0.5 )  # 选择器网络 self.selector = nn.Sequential( nn.Linear(64 + state_dim[-1], 32), nn.ReLU( ), nn.Linear(32, 2) ) def forward(self, state): # x：节点特征，edge_index：图连通性，mask：掩码填充值 masked_x = state.x * state.mask # 经过第一个GAT层 x_gat1 = F.relu(self.gat_conv1(masked_x, state.edge_index)) # 经过第二个GAT层 x_gat2 = F.relu(self.gat_conv2(x_gat1, state.edge_index)) # 选择器网络selector_probs = F.softmax(self.selector( x_gat2), dim=-1) 返回selector_probs   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4wisu/graph_attention_network_gat/</guid>
      <pubDate>Sat, 02 Mar 2024 19:31:23 GMT</pubDate>
    </item>
    <item>
      <title>帮助连续空间离线Q-Learning</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4tkda/help_with_continuous_space_offline_qlearning/</link>
      <description><![CDATA[我对强化学习还很陌生，并且计划在我的论文中以一种雄心勃勃的方式使用它。我了解在线强化学习的基础知识，并完成了几个小项目，但我什至很难理解在哪里寻找信息来实现我想要的目标。本质上，我只能访问历史数据，它由 12 个 60*100 图像描述的状态空间和由其中 2 个图像描述的动作空间组成。这些空间是连续且无界的。我还为每个状态定义了奖励函数，以及不同结果的一些最终状态。我的目标是评估所采取的每个动作，因此我想训练一个神经网络，它可以接收状态和动作作为输入，并输出 Q 值。我对寻找最佳政策没有兴趣。我发现了一种叫做 IQL 的东西，但我感觉有点难以理解，不确定它是否适用于我的问题。非常感谢您的帮助！   由   提交/u/macaco3001   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4tkda/help_with_continuous_space_offline_qlearning/</guid>
      <pubDate>Sat, 02 Mar 2024 17:27:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的奖励网络是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4qwsm/what_are_reward_networks_in_reinforcement_learning/</link>
      <description><![CDATA[我正在阅读以下文章 此处 -  两种逆强化学习 (IRL) 算法的目标（例如 AIRL,  GAIL）和偏好比较是为了发现奖励功能。在模仿学习中，这些发现的奖励由奖励网络参数化。   这是否意味着模仿学习的输出是奖励值？那么输入是什么？状态和行动？ 有一篇很好的论文解释奖励网络和模仿学习吗？我指的是莱文的讲座笔记。在幻灯片 5 中，模仿学习的输出似乎是一种策略，而不是奖励。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1b4qwsm/what_are_reward_networks_in_reinforcement_learning/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4qwsm/what_are_reward_networks_in_reinforcement_learning/</guid>
      <pubDate>Sat, 02 Mar 2024 15:32:58 GMT</pubDate>
    </item>
    <item>
      <title>奇才和 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4ld37/wizards_and_ppo/</link>
      <description><![CDATA[ 由   提交/u/nurgle100  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4ld37/wizards_and_ppo/</guid>
      <pubDate>Sat, 02 Mar 2024 10:34:38 GMT</pubDate>
    </item>
    <item>
      <title>只是分享我关于 Gran Turismo 的 RL 论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4klxv/just_sharing_my_dissertation_in_rl_for_gran/</link>
      <description><![CDATA[https://youtu.be/zpz3Dbmx1SI   由   提交/u/NDR008  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4klxv/just_sharing_my_dissertation_in_rl_for_gran/</guid>
      <pubDate>Sat, 02 Mar 2024 09:44:28 GMT</pubDate>
    </item>
    <item>
      <title>为什么在数学中进行自动定理证明的深度强化学习如此困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3vm43/why_is_it_so_difficult_to_do_deep_reinforcement/</link>
      <description><![CDATA[请记住，我的领域是数学。我是 ML/DL 的新手： 我们有 AlphaGo 和 AlphaFold 类型的系统，它们在各自的领域都是超人的。我读到研究人员很难为数学研究建立类似的系统，即“超人数学家”。那么，为什么困难呢？鉴于法学硕士以及上述系统的进步，我什么时候应该开始担心某些人工智能系统在各个方面都优于我？  作为一个新手，在我看来，这些系统的发展速度比任何人想象的都要快，但这也许是因为我看到的唯一新闻是大肆宣传的。我感受到的存在主义恐惧太多了。   由   提交 /u/dhhdhkvjdhdg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3vm43/why_is_it_so_difficult_to_do_deep_reinforcement/</guid>
      <pubDate>Fri, 01 Mar 2024 14:29:52 GMT</pubDate>
    </item>
    </channel>
</rss>