<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 07 Feb 2025 06:28:07 GMT</lastBuildDate>
    <item>
      <title>谁能帮助我（自定义env + SB3）？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijektz/can_anyone_help_me_custom_env_sb3/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我创建了一个自定义的健身房环境，该环境与Java中的模拟器对话。基本上，它从光网络收集Infos。 OBS空间是拓扑，动作空间是一个路由和初始插槽，可以将流动分配。要处理的流是事件中断的流。每个事件都有一堆中断的流。我正在尝试训练一个代理商，为每条路线和插槽分配流程的每条流程做出明智的决策。一旦分配流动，拓扑就会发生变化，否则什么都不会改变。我正在使用SB3（DQN，MLPPOLICY），并将时间步骤设置为每个事件的流量数（这是必须这样做的方式，因为它与模拟器进行了对话）。问题是，当事件具有x流数时，Model.learn（）执行2或3个步骤，而不是流量数。它会引起混乱，因为模拟器试图处理新事件的新流程，但是它会从模型中重复流过。关于如何解决这个问题的想法？我可以共享代码和联系人，我真的需要解决此问题。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/leilaff89     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijektz/can_anyone_help_me_custom_env_sb3/</guid>
      <pubDate>Thu, 06 Feb 2025 21:52:27 GMT</pubDate>
    </item>
    <item>
      <title>“多基因填充：与多种推理链的自我改进”，Subramaniam等2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij81ye/multiagent_finetuning_self_improvement_with/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij81ye/multiagent_finetuning_self_improvement_with/</guid>
      <pubDate>Thu, 06 Feb 2025 17:27:45 GMT</pubDate>
    </item>
    <item>
      <title>Chen等人“对长马相互作用LLM代理的增强学习”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij2nfz/reinforcement_learning_for_longhorizon/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij2nfz/reinforcement_learning_for_longhorizon/</guid>
      <pubDate>Thu, 06 Feb 2025 13:34:36 GMT</pubDate>
    </item>
    <item>
      <title>用于定制演员和评论家网络的RL库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我希望测试Pytorch和基准指标中的自定义神经网络（即收敛速率）与Actor-Cricit-Critic RL算法中的标准MLP。我已经围绕着subreddit查看，并且已经建议使用以下库来实施此类网络：   rllib   rlpyt   skrl &lt;&gt; /li&gt;  torchrl   对这些意见有什么意见或良好的经验？我已经看到了对rllib的爱与恨，但在过去三个中没有太多的爱。我正在尝试避免SB3，因为我认为我的神经网络不属于他们拥有的任何自定义策略类别，除非我非常误解了他们的自定义策略类别的工作方式。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/voltimeters     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</guid>
      <pubDate>Thu, 06 Feb 2025 12:55:17 GMT</pubDate>
    </item>
    <item>
      <title>有关Mappo实施的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij0m2l/question_about_mappo_implementation/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij0m2l/question_about_mappo_implementation/</guid>
      <pubDate>Thu, 06 Feb 2025 11:41:18 GMT</pubDate>
    </item>
    <item>
      <title>积极的在线运动计划和决策|印度| Swaayatt机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiyt2j/aggressive_online_motion_planning_and_decision/</link>
      <description><![CDATA[    该系统即时执行动态轨迹计算，对24米内的障碍物做出反应半径。演示展示了曲折和左车道避免模式，尽管车身挑战很高，但车辆的速度仍高于45 kmph。 。 &gt; youtube_link   该框架在单线程i7处理器上以800+ Hz运行，并将轨迹跟踪系统集成了纯粹的追击系统。未来的计划包括使用端到端的深度强化学习扩展框架 原始作者LinkedIn： sanjeev_sharma_linkedin a&gt; 原始链接帖子： pose_link    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/shani_786     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiyt2j/aggressive_online_motion_planning_and_decision/</guid>
      <pubDate>Thu, 06 Feb 2025 09:36:49 GMT</pubDate>
    </item>
    <item>
      <title>需要有关高级RL资源的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我现在已经深入了强化学习，但是我撞墙了。我发现的几乎每个课程或资源都涵盖了相同的内容 -  PPO，SAC，DDPG等。它们非常适合理解基础知识，但我感到卡住了。就像我只是在相同的算法周围盘旋而没有真正前进。 我试图弄清楚如何摆脱它并进入更高级或更新的RL方法。诸如遗憾最小化，基于模型的RL，甚至是多代理系统＆amp＆amp; HRL听起来令人兴奋，但我不确定从哪里开始。 其他人有这种感觉吗？如果您设法推动了这个高原，您是如何做到的？任何课程，论文甚至个人提示都将非常有帮助。 事先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/helpful-number1288     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</guid>
      <pubDate>Thu, 06 Feb 2025 08:15:09 GMT</pubDate>
    </item>
    <item>
      <title>对RL中的数学符号感到困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iivcd2/confused_about_math_notations_in_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我一直在学习强化学习，但是我正在努力使用一些数学符号，尤其是期望符号。例如，值函数通常写为：  v^π（s）=e_π[r_t | s_t = s] =e_π[∑_ {k = 0}^{∞}γ^k r_ {t+k+1} | s_t = s]  下标e_π到底是什么意思？我的理解是下标应表示概率分布或随机变量，但是π是 policy（a函数）&lt; /strong&gt;，不是通常意义上的分布。 这种混乱也会在轨迹概率定义中产生：  p（τ|π）=ρ_0（s_0）∏_ { t = 0}^{t-1} p（s_ {t+1} | s_t，a_t）π（a_t | s_t） π是输出操作的函数。虽然动作是一个随机变量，但π本身不是（如果我错了，请修复我）。 在诸如（https://spinningup.openai.com/en/latest/spinningup/rl\_intro.html) V ^\ pi（s）= \ mathbb {e} _ {\ tau \ sim \ pi} \ left [r（\ tau）\ mid s_0 = s \ right]  作者作者写了$ \ tau \ sim \ pi} $在这里，但是轨迹\ tau是不是 从策略\ pi采样，因为\ tau还包括环境生成的状态。 类似地，表达式，表达式像 e_π[r（τ）| s_0 = s，a_0 = a]  感觉直觉，但我发现它们在数学上不严格，因为通常会在明确的概率分布上进行期望。 &lt; &lt; P&gt;更新： 我更担心的是，诸如$ e_ \ pi $之类的符号实际上是与传统期望操作不同的新数学操作。 我知道像大多数RL这样的简单案例，它们不太可能是无效或不完整的。但是我认为我们需要证明它们的有效性。 电气工程师使用dx表示DX/DT，而1/DX表示\ Integral X DT。我不知道是否有证据，但是差异操作员具有非常清晰的意义。 -SC_ON-&gt;＆＃32;提交由＆＃32; /u/u/udancionativecar545      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iivcd2/confused_about_math_notations_in_rl/</guid>
      <pubDate>Thu, 06 Feb 2025 05:30:02 GMT</pubDate>
    </item>
    <item>
      <title>RL不适用于运动控制和学习！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想知道有人知道使用RL用于运动学习的研究吗？我听说它从来没有能够为现实世界中的运动建模或控制运动而努力。这是真的吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/berkeleyears     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</guid>
      <pubDate>Thu, 06 Feb 2025 04:55:56 GMT</pubDate>
    </item>
    <item>
      <title>RL对类人形生物的控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我有兴趣从事基于RL的类人体控制器。如果您可以将一些重要的资源列为起点，我将非常感谢。谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/rua0ra1     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</guid>
      <pubDate>Thu, 06 Feb 2025 04:42:30 GMT</pubDate>
    </item>
    <item>
      <title>算法旨在将“娱乐”的概念完全灌输在AI中。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iihtux/algorithm_designed_to_instill_the_concept_of_fun/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iihtux/algorithm_designed_to_instill_the_concept_of_fun/</guid>
      <pubDate>Wed, 05 Feb 2025 19:03:00 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和模型预测控制调查2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iidp8i/reinforcement_learning_and_model_predictive/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/tmms_     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iidp8i/reinforcement_learning_and_model_predictive/</guid>
      <pubDate>Wed, 05 Feb 2025 16:15:55 GMT</pubDate>
    </item>
    <item>
      <title>Dedieu等人的“改善数据效率RL的变压器世界模型”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ii9r1s/improving_transformer_world_models_for/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ii9r1s/improving_transformer_world_models_for/</guid>
      <pubDate>Wed, 05 Feb 2025 13:19:49 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ii9oxu/need_help/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试创建一个AI，通过使训练有素的AI像Stockfish这样的AI来学习国际象棋。我计划制作在Python中，这已经拥有Python-Chess，并且更容易与Stockfish 一起工作，我还计划学习这些AI在此期间的工作在培训部分中如何使用稳定的baseline 3。&lt; /&lt; /&lt; / p&gt; 我对AI有一些基本知识以及如何培训代理商，但是我已经使用ML-Agents训练了Unity的代理商，所以我不知道这将有多困难？ 什么我应该这样做，该怎么做？谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/kungfuaryan     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ii9oxu/need_help/</guid>
      <pubDate>Wed, 05 Feb 2025 13:16:31 GMT</pubDate>
    </item>
    <item>
      <title>RL的博士学位机会，决策情报申请吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是本科生的最后一年，想在RL或决策情报申请领域申请直接的博士学位机会。  尽管我已经在某些大学申请，但我觉得我的机会很低。我已经很遗憾的是，去年没有跟踪申请或通过机会，这足以没有跟踪申请。如果你们中的任何一个人对仍在2025年摄入量开放的直接博士学位程序有所了解，请在此SubReddit🙏  &lt;！ -  sc_on-&gt;＆＃32中告诉我。提交由＆＃32; /u/u/misover_ad2265     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</guid>
      <pubDate>Tue, 04 Feb 2025 14:59:07 GMT</pubDate>
    </item>
    </channel>
</rss>