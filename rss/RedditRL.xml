<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 24 Dec 2023 15:12:26 GMT</lastBuildDate>
    <item>
      <title>Python RL 中设置训练“基本法则”的最佳位置是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18pgh6z/the_best_place_in_python_rl_to_set_the/</link>
      <description><![CDATA[嗨 我一直在努力理解放置“基本法则”的最佳位置。对于 PPO 模型。我在某处读到它应该在环境中，而在其他地方，我读到它最好在预处理器中完成。就我而言（只是为了好玩而玩股票交易 RL），我只希望它在位置 = 0 时买入。我当然可以在环境中对此进行调整，但它“感觉”不太好。错误（不知道为什么..）。我宁愿把它放在代理那边，但也许我弄错了..    由   提交/u/Forward-Cranberry-30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18pgh6z/the_best_place_in_python_rl_to_set_the/</guid>
      <pubDate>Sat, 23 Dec 2023 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>Pearl：生产就绪的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18p82ug/pearl_a_productionready_reinforcement_learning/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.03814 代码：https://github .com/facebookresearch/pearl 项目页面：https://pearlagent. github.io/ 摘要：  强化学习（RL）为实现长期目标提供了一个多功能框架。它的通用性使我们能够形式化现实世界智能系统遇到的各种问题，例如处理延迟奖励、处理部分可观察性、解决探索和利用困境、利用离线数据提高在线性能以及确保安全约束遇见了。尽管强化学习研究社区在解决这些问题方面取得了相当大的进展，但现有的开源强化学习库往往只关注强化学习解决方案管道的一小部分，而其他方面基本上无人关注。本文介绍了 Pearl，这是一个生产就绪的 RL 代理软件包，专门设计用于以模块化方式应对这些挑战。除了提供初步基准测试结果外，本文还重点介绍了 Pearl 的行业采用情况，以证明其已做好生产使用的准备。 Pearl 在 Github 上开源，位于 此 http URL，其官方网站位于 这个http URL。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18p82ug/pearl_a_productionready_reinforcement_learning/</guid>
      <pubDate>Sat, 23 Dec 2023 15:32:28 GMT</pubDate>
    </item>
    <item>
      <title>这个人工智能程序无需任何编码即可学习任何游戏。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oyeum/this_ai_program_can_learn_any_game_without_any/</link>
      <description><![CDATA[    /u/Worldly-Daikon5001   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oyeum/this_ai_program_can_learn_any_game_without_any/</guid>
      <pubDate>Sat, 23 Dec 2023 05:16:28 GMT</pubDate>
    </item>
    <item>
      <title>“MetaDiff：用于少样本学习的条件扩散元学习”，Zhang & Yu 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ori1g/metadiff_metalearning_with_conditional_diffusion/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ori1g/metadiff_metalearning_with_conditional_diffusion/</guid>
      <pubDate>Fri, 22 Dec 2023 23:06:12 GMT</pubDate>
    </item>
    <item>
      <title>TypeError：TD3Policy.forward() 需要 2 到 3 个位置参数，但给出了 4 个（自定义多代理环境）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oq1y5/typeerror_td3policyforward_takes_from_2_to_3/</link>
      <description><![CDATA[我计划将 TD3 与 MultiInputPolicy 结合使用，该策略接受我的自定义多代理环境的 Dict 类型观察。  ...train.py”，第 114 行，在  中model = TD3( ^^^^ 文件“D:\anaconda3\Lib\site-packages\stable_baselines3\td3\td3.py”，第 137 行，在 __init__ self._setup_model() 文件“D:\anaconda3\Lib” \site-packages\stable_baselines3\td3\td3.py”，第 140 行，在 _setup_model super()._setup_model() 文件“D:\anaconda3\Lib\site-packages\stable_baselines3\common\off_policy_algorithm.py”，行199、在 _setup_model self.policy = self.policy_class( ^^^^^^^^^^^^^^^^^^^ 文件 &quot;D:\anaconda3\Lib\site-packages\torch\nn\modules\ module.py”，第 1518 行，在 _wrapped_call_impl 中 return self._call_impl(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ^^^^ 文件“D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py”，第 1527 行，在 _call_impl 返回forward_call(*args, **kwargs) ^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^ TypeError: TD3Policy.forward() 采用 2 到 3 个位置参数，但给出了 4 个 &lt; p&gt;相关模型和策略定义：  model = TD3(policy=policy, env=env, ... )  我尝试替换 env使用 TD3 的已知工作健身房环境（“Pendulum-v1”），会产生相同的错误。因此，我开始研究策略定义：  policy = MultiInputPolicy( env.observation_space, env.action_space, lr_schedule, ... }  还有这个把我带回到环境中，是我的观察和动作空间有问题吗？请指教。 ``` ... self.action_space = Box( 0.0, +1.0, shape=(len(self .actions.keys()),), dtype=np.float32 )  self.observation_space = Dict( { &quot;a&quot;: Box( -2.0, +1.0, shape=(2 * r1 + 1, r2+ 1), dtype=np.float32, ), &quot;b&quot;: Box( -1.0, 1.0, shape=(2 * r1 + 1, r2+ 1), dtype=np.int32, ), “c”: Box( -1.0, 100.0, shape=(2 * r1 + 1, r2 + 1), dtype=np.float32, ), } ) ...   ```    提交者   / u/fatalStrike97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oq1y5/typeerror_td3policyforward_takes_from_2_to_3/</guid>
      <pubDate>Fri, 22 Dec 2023 21:58:44 GMT</pubDate>
    </item>
    <item>
      <title>dm_control中的cmu_ humanoid是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oiulk/what_is_the_cmu_humanoid_in_dm_control/</link>
      <description><![CDATA[嗨， 最近我一直在探索 dm_control 库并遇到了 cmu_ humanoid。现在我知道人形生物的样子了。我不确定他们为什么将其称为 cmu_ humanoid 。是因为他们用了cmu数据集的关节和骨骼吗？还是因为Humanoid直接兼容cmu数据集，可以直接在mujoco中使用？或者是其他什么？ 提前感谢您的宝贵时间和回复。   由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oiulk/what_is_the_cmu_humanoid_in_dm_control/</guid>
      <pubDate>Fri, 22 Dec 2023 16:29:47 GMT</pubDate>
    </item>
    <item>
      <title>ReCoRe：世界模型的正则化对比表示学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.09056 摘要：  虽然最近的无模型强化学习（RL）方法已经证明尽管在游戏环境中的人类水平的有效性，他们在视觉导航等日常任务中的成功受到限制，特别是在显着的外观变化下。这种限制源于 (i) 样本效率差和 (ii) 过度拟合训练场景。为了应对这些挑战，我们提出了一个世界模型，该模型使用（i）对比无监督学习和（ii）干预不变正则化器来学习不变特征。学习世界动态的显式表示（即世界模型）可以提高样本效率，而对比学习隐式地强制学习不变特征，从而提高泛化能力。然而，由于缺乏视觉编码器的监督信号，对比损失与世界模型的简单集成失败了，因为基于世界模型的强化学习方法独立地优化了表示学习和代理策略。为了克服这个问题，我们提出了一种以辅助任务（例如深度预测、图像去噪等）形式存在的干预不变正则化器，它明确地强制风格干预的不变性。我们的方法优于当前最先进的基于模型和无模型的 RL 方法，并且在 iGibson 基准评估的分布外点导航任务上表现显着。我们进一步证明，我们的方法仅通过视觉观察，优于最近的语言引导的点导航基础模型，这对于在计算能力有限的机器人上部署至关重要。最后，我们证明我们提出的模型在 Gibson 基准上的感知模块的模拟到真实转换方面表现出色。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18of39h/recore_regularized_contrastive_representation/</guid>
      <pubDate>Fri, 22 Dec 2023 13:36:49 GMT</pubDate>
    </item>
    <item>
      <title>是否有研究在 RL 期间使用 LoRA 和 QLoRA 等参数高效训练来进行预训练模型？我想在一些大型模型上运行强化学习，并且希望避免购买无数的 GPU。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18oe4t2/has_there_been_any_research_into_using/</link>
      <description><![CDATA[我有一些相当大的预训练模型，我想在它们上运行强化学习，并且更愿意从成本较低的训练选项开始。有人知道是否有任何论文或文章详细介绍了使用 LoRA 等技术进行强化学习吗？   由   提交 /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18oe4t2/has_there_been_any_research_into_using/</guid>
      <pubDate>Fri, 22 Dec 2023 12:46:22 GMT</pubDate>
    </item>
    <item>
      <title>“强化学习迁移的基础：知识模态分类”，Wulfmeier 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</guid>
      <pubDate>Fri, 22 Dec 2023 01:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 amass 数据集转换为 mujoco 格式？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18nrdhb/how_to_convert_the_amass_dataset_to_mujoco_format/</link>
      <description><![CDATA[嗨， 我想将 amass 数据集转换为 mujoco 格式，以便我能够在 mujoco 中使用运动数据知道如何做到这一点吗？ 我对 amass 和 mujoco 都很陌生，所以如果这似乎是一个愚蠢的问题，我深表歉意。 &lt;!-- SC_ON - -&gt;  由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18nrdhb/how_to_convert_the_amass_dataset_to_mujoco_format/</guid>
      <pubDate>Thu, 21 Dec 2023 16:48:28 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线3收集部署时出错</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18nqifb/error_in_collecting_rollouts_using/</link>
      <description><![CDATA[        由   提交/u/Ecstatic-Rain-2460   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18nqifb/error_in_collecting_rollouts_using/</guid>
      <pubDate>Thu, 21 Dec 2023 16:09:55 GMT</pubDate>
    </item>
    <item>
      <title>“评估现实自主任务上的语言模型代理”，Kinniment 等人 2023 {ARC}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18np4kd/evaluating_languagemodel_agents_on_realistic/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18np4kd/evaluating_languagemodel_agents_on_realistic/</guid>
      <pubDate>Thu, 21 Dec 2023 15:07:37 GMT</pubDate>
    </item>
    <item>
      <title>“利用大型语言模型进行自主化学研究”，Boiko 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18np0m3/autonomous_chemical_research_with_large_language/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18np0m3/autonomous_chemical_research_with_large_language/</guid>
      <pubDate>Thu, 21 Dec 2023 15:02:45 GMT</pubDate>
    </item>
    <item>
      <title>你们如何处理强化学习中的梯度爆炸？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ninvh/how_you_guys_handle_gradient_exploding_in_rl/</link>
      <description><![CDATA[ 适当的权重初始化 梯度裁剪 lr调度程序我还能做什么？&lt; /li&gt;    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ninvh/how_you_guys_handle_gradient_exploding_in_rl/</guid>
      <pubDate>Thu, 21 Dec 2023 08:57:35 GMT</pubDate>
    </item>
    <item>
      <title>利用离散表示进行持续强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ni6k8/harnessing_discrete_representations_for_continual/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.01203 OpenReview：https:// /openreview.net/forum?id=o4AydSd3Lp 摘要：  强化学习（RL）智能体什么都不用做决策但来自环境的观察结果在很大程度上依赖于这些观察结果的表征。尽管最近的一些突破使用了基于向量的观察分类表示（通常称为离散表示），但很少有工作明确评估这种选择的重要性。在这项工作中，我们对强化学习背景下将观察结果表示为分类值向量的优势进行了彻底的实证研究。我们对世界模型学习、无模型强化学习以及最终的连续强化学习问题进行评估，其中的好处最能满足问题设置的需求。我们发现，与传统的连续表示相比，通过离散表示学习的世界模型可以用更少的容量准确地模拟更多的世界，并且用离散表示训练的智能体用更少的数据学习更好的策略。在持续强化学习的背景下，这些好处转化为更快的适应代理。此外，我们的分析表明，观察到的性能改进可归因于潜在向量中包含的信息以及潜在的离散表示本身的编码。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ni6k8/harnessing_discrete_representations_for_continual/</guid>
      <pubDate>Thu, 21 Dec 2023 08:23:29 GMT</pubDate>
    </item>
    </channel>
</rss>