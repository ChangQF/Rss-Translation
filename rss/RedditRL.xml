<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 03 Jun 2024 18:20:31 GMT</lastBuildDate>
    <item>
      <title>“无悔等待模型：最大化小费的多臂老虎机方法”（讽刺）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d77c8a/the_no_regrets_waiting_model_a_multiarmed_bandit/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d77c8a/the_no_regrets_waiting_model_a_multiarmed_bandit/</guid>
      <pubDate>Mon, 03 Jun 2024 15:34:10 GMT</pubDate>
    </item>
    <item>
      <title>旧国家与新国家的“形态”重要吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d73g32/does_the_shape_of_old_state_vs_new_state_matter/</link>
      <description><![CDATA[我是强化学习的新手，如果这个论坛不适合这种级别的新手问题，我提前道歉。 在 DQN 网络的标准拟合中，当前状态和选定的操作会导致新状态和奖励，据我所知：新旧状态是否彼此“相似”并与操作相关会影响学习吗？换句话说，状态的表示本质上只是节点索引（因此它们可以是任何随机的唯一二进制数），还是网络通过以某种方式将状态相互比较来学习？请温柔一点 :-)    提交人    /u/suggestive_cumulus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d73g32/does_the_shape_of_old_state_vs_new_state_matter/</guid>
      <pubDate>Mon, 03 Jun 2024 12:41:01 GMT</pubDate>
    </item>
    <item>
      <title>如何设计一款适合双人棋盘游戏的优秀 DeepQ？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6ysnr/how_to_design_a_good_deepq_for_2_player_board_game/</link>
      <description><![CDATA[我正在制作一款双人棋盘游戏。我需要训练 deepQ AI。AI 使用 e-greedy 策略。 赢了奖励 +1，输了奖励 -1。否则为 0。 游戏规则很简单。它是一个 7x7 的棋盘。 有 4 种不同的瓷砖状态。{ USER_tile, AGENT_tile, EMPTY_tile, CLOSED_tile }。用户瓷砖和代理瓷砖是游戏玩家的当前位置。玩家从相对边缘的中间瓷砖（pos（0,3）和pos（6,3））开始。 每回合玩家都应该朝 8 个方向之一移动到 EMPTY_tile，然后关闭一个 EMPTY_tile。游戏的目标是通过关闭对手周围的 8 个瓷砖使对手无法移动。这样下一回合对手将无法移动，因为周围没有空的瓷砖。 玩家无法移动到封闭的瓷砖中。 我正在使用自我游戏训练代理，但训练后 AI 无法与人类对战。 如何使用 DeepQ 让 AI 与人类对战？    提交人    /u/erenpal01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6ysnr/how_to_design_a_good_deepq_for_2_player_board_game/</guid>
      <pubDate>Mon, 03 Jun 2024 07:31:15 GMT</pubDate>
    </item>
    <item>
      <title>Google AI 提出 PERL：一种参数高效的强化学习技术，可以训练奖励模型并使用 LoRA 调整语言模型策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6tt7s/google_ai_proposes_perl_a_parameter_efficient/</link>
      <description><![CDATA[Google 推出了一种革命性的方法，称为参数高效强化学习 (PERL)，该方法使用 LoRA 技术更有效地优化模型，从而降低计算和内存要求。PERL 实现的结果与传统 RLHF 方法相似，但参数效率显著提高。 来源：https://app.daily.dev/posts/google-ai-proposes-perl-a-parameter-efficient-reinforcement-learning-technique-that-c​​an-train-a-rew-frsirw4h0    提交人    /u/Fit_Stop7509   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6tt7s/google_ai_proposes_perl_a_parameter_efficient/</guid>
      <pubDate>Mon, 03 Jun 2024 02:19:40 GMT</pubDate>
    </item>
    <item>
      <title>“LAMP：用于预训练强化学习的语言奖励调节”，Adeniji 等人 2023 年（提示 LLM 作为多样化奖励）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6tfid/lamp_language_reward_modulation_for_pretraining/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6tfid/lamp_language_reward_modulation_for_pretraining/</guid>
      <pubDate>Mon, 03 Jun 2024 01:59:23 GMT</pubDate>
    </item>
    <item>
      <title>“人工智能欺骗：示例、风险和潜在解决方案的调查”，Park 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6sale/ai_deception_a_survey_of_examples_risks_and/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6sale/ai_deception_a_survey_of_examples_risks_and/</guid>
      <pubDate>Mon, 03 Jun 2024 00:59:21 GMT</pubDate>
    </item>
    <item>
      <title>“利用深度强化学习实现冠军级无人机竞赛”，Kaufmann 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6qkbc/championlevel_drone_racing_using_deep/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6qkbc/championlevel_drone_racing_using_deep/</guid>
      <pubDate>Sun, 02 Jun 2024 23:30:53 GMT</pubDate>
    </item>
    <item>
      <title>“Hoodwinked：语言模型文本游戏中的欺骗与合作”，O'Gara 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6p825/hoodwinked_deception_and_cooperation_in_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6p825/hoodwinked_deception_and_cooperation_in_a/</guid>
      <pubDate>Sun, 02 Jun 2024 22:27:27 GMT</pubDate>
    </item>
    <item>
      <title>“这款人工智能让古老的棋盘游戏复活，并让你玩它们”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6ln8r/this_ai_resurrects_ancient_board_gamesand_lets/</link>
      <description><![CDATA[        由    /u/gwern 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6ln8r/this_ai_resurrects_ancient_board_gamesand_lets/</guid>
      <pubDate>Sun, 02 Jun 2024 19:47:36 GMT</pubDate>
    </item>
    <item>
      <title>RL 理论与实际应用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d6kecs/rl_theory_practical_usage/</link>
      <description><![CDATA[我是一名刚开始学习强化学习和机器学习的本科生。上学期我参与了一个强化学习理论的研究项目，这是我第一次接触强化学习。我想知道强化学习理论（复杂性结果等）和强化学习中的实际方法之间的关系。理论似乎落后了很大差距。例如，Q 学习是几十年前发明的，但最佳遗憾结果仅在几年前才被证明。 我想知道强化学习理论的价值。理论工作是否指导人们设计更好的实用算法？来自理论世界的洞察力如何帮助推进强化学习？    提交人    /u/mziycfh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d6kecs/rl_theory_practical_usage/</guid>
      <pubDate>Sun, 02 Jun 2024 18:53:44 GMT</pubDate>
    </item>
    <item>
      <title>自我博弈、经验重放和稀疏奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d699fn/selfplay_experience_replay_and_sparse_rewards/</link>
      <description><![CDATA[我有几个问题。如果我们在 2 人游戏环境中使用 +1 表示赢，-1 表示输，该游戏会自我训练。我应该如何处理奖励为 0 的先前步骤？ 我应该将奖励为 0 的步骤放入经验重播中吗？    提交人    /u/erenpal01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d699fn/selfplay_experience_replay_and_sparse_rewards/</guid>
      <pubDate>Sun, 02 Jun 2024 09:11:01 GMT</pubDate>
    </item>
    <item>
      <title>需要有关快速消费品公司中 RL 在目标订单数量预测中的实施方式方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d62bqr/help_needed_regarding_way_of_implementation_of_rl/</link>
      <description><![CDATA[我最近被一家快速消费品公司选为实习生，他们要求我实施一种 RL 算法，该算法可帮助他们根据前 6 个月总订单量的销售数据分析来预测下个月的目标订单量 (TOQ)，同时测试当前月份的数据。  目前，他们正在实施一个 XGBRegressor 模型，他们说对于某些门店，该模型的预测还可以，对于某些门店，预测超出预期，而对于某些门店，预测低于预期。 除此之外，如果有人可以给我提供关于如何解决这个问题的粗略想法，以及是否有与此相关的论文或资源可以阅读，那就太好了。 提前致谢。    提交人    /u/wandering_soul_420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d62bqr/help_needed_regarding_way_of_implementation_of_rl/</guid>
      <pubDate>Sun, 02 Jun 2024 01:39:08 GMT</pubDate>
    </item>
    <item>
      <title>关于 DQN 的一些问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d60zdj/a_number_of_questions_re_dqn/</link>
      <description><![CDATA[因此，我正在尝试理解一些基本的 ML 概念 - 你知道，要具备一定的实用知识，以防在日常工作中遇到。 我正在尝试使用 TensorFlow 实现 DQN 来玩 Connect 4 - 只是一个玩具项目。我的代码目前位于 https://gist.github.com/lkingsford/a02cada950e2c911a02b7b1d76789ee5。这不是我的常用方法 - 但我使用 ChatGPT 部分来帮助处理 ML 位块（这自然需要进行相当多的更改才能真正发挥作用）。我会注意到代码已经可以运行了 - 它可以训练模型，并且相当快。 我知道我稍后会有更多的问题，因为我对这些概念有了更好的理解 - 但我有几个关于我可能做错的问题。 Zerothly - 我做的对吗？DQN 实际上适合这种用途吗？感觉无状态可能会限制我在编写游戏 AI 时的能力。 首先 - 我的 GPU 使用率真的很低。当 epsilon 很高，并且大多数操作都是随机的时，我在实例上获得了约 10% 的 GPU。当它开始预测更多动作时，它会下降到约 3%。我也没有使用特别强大的实例进行训练。我应该考虑线程吗？运行异步吗？我应该做些什么来更快地提供给 GPU？我看到了关于批处理的建议 - 但我不确定当有另一个需要响应的操作时它是如何工作的。 其次 - 我如何处理失败的奖励？如果游戏仍在进行，我的奖励函数返回 0，如果僵局，则返回 -2，如果获胜，则返回 10。但是，奖励函数仅在采取行动时奖励。如果 DQN 采取了获胜的行动，那么根据我的理解，导致其他玩家失败的一系列行动也应该有负奖励，因此预测可以更好地学习“如果我给其他玩家留下获胜的机会，我就会输”，而不仅仅是“如果我把石头放在这里，我就会赢”。 第三 - 我如何决定密集层的数量以及它们有多少个神经元？它是由随时可用的不同动作数量决定的吗？战略复杂性的数量，以及奖励需要多长时间？比如，如果我在制作《芝加哥快车》——它有一个大状态，以及更广泛的潜在行动，在你获得移动奖励之前有许多行动，在你发现自己是否真的赢之前有许多行动（至少与奖励很好地对应）——如果进展不顺利，我该如何确定是否应该增加神经元或层数？ 最后——我认为现在——我想我对正在发生的事情有一个非常基本的了解，但感觉有点黑箱。我从事软件工作已经很长时间了，以至于我不擅长魔法，特别是当我看不到内部结构的时候。你会去哪里学习第一性原理，而不是仅仅使用库来处理一堆我仍在尝试理解的东西？比如，我应该从头开始尝试一些基本的机器学习吗？还是找一本更概念化、理论化的书？ 感谢任何人提供的帮助。对这些特殊的黑匣子仍然很陌生，觉得有点费脑力。    提交人    /u/thelochok   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d60zdj/a_number_of_questions_re_dqn/</guid>
      <pubDate>Sun, 02 Jun 2024 00:26:04 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习理论和优化的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d5wy97/papers_regarding_rl_theory_and_optimization/</link>
      <description><![CDATA[嗨！我是一名本科生，一直在研究 RL 应用和实现。最近，我对多智能体和更实际的模拟越来越感兴趣，这些模拟的复杂程度越来越高。因此，我真的想尽可能地提高性能，以最大限度地缩短收敛时间和重新运行时间。 因此，我想联系一下，询问是否有人有最前沿的优化论文来源或链接，以及如何有效地训练 RL 模型。我浏览过许多旧资源，想看看当前算法或技术的最佳 SOTA 是什么。如果您有任何想法，请告诉我！    提交人    /u/anishfish   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d5wy97/papers_regarding_rl_theory_and_optimization/</guid>
      <pubDate>Sat, 01 Jun 2024 21:10:19 GMT</pubDate>
    </item>
    <item>
      <title>“DeTikZify：使用 TikZ 合成用于科学图形和草图的图形程序”，Belouadi 等人 2024（用于编写 Latex 编译为所需图像的 MCTS）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d5vjru/detikzify_synthesizing_graphics_programs_for/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d5vjru/detikzify_synthesizing_graphics_programs_for/</guid>
      <pubDate>Sat, 01 Jun 2024 20:04:15 GMT</pubDate>
    </item>
    </channel>
</rss>