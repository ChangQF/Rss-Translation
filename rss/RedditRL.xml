<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 02 Jul 2024 03:16:53 GMT</lastBuildDate>
    <item>
      <title>“使用稀疏自动编码器解释偏好模型”，Riggs & Brinkmann</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dt9e2r/interpreting_preference_models_wsparse/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dt9e2r/interpreting_preference_models_wsparse/</guid>
      <pubDate>Tue, 02 Jul 2024 01:06:36 GMT</pubDate>
    </item>
    <item>
      <title>40k 个 episode 之后的奖励图下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dt0tg7/downside_of_reward_graph_after_40k_episodes/</link>
      <description><![CDATA[代码  train.py -&gt; https://www.pythonmorsels.com/p/33x5u/    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dt0tg7/downside_of_reward_graph_after_40k_episodes/</guid>
      <pubDate>Mon, 01 Jul 2024 18:52:21 GMT</pubDate>
    </item>
    <item>
      <title>研究顾问绝望</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dswpav/research_advisor_despair/</link>
      <description><![CDATA[嗨，我写这篇文章是为了寻求建议。我是一名本科生，梦想有一天能去研究生院学习强化学习。我已经和一位教授一起完成了一个使用应用机器学习的研究项目；从那时起，我一直想进入一个从事理论工作的实验室。问题是，即使发了几十封定制的冷邮件，我甚至还没有收到任何回复。 我知道这个领域非常受欢迎，竞争非常激烈，但我不知道该怎么做了。我知道对于北美顶尖的研究生课程来说，发表论文是必不可少的，但我不确定我是否有机会。在这一点上，我想我已经放弃了在没有导师的情况下发表论文的想法。这是一个好的做法吗？我应该继续尝试联系教授吗？任何见解都将不胜感激。    提交人    /u/Open-Ad2530   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dswpav/research_advisor_despair/</guid>
      <pubDate>Mon, 01 Jul 2024 16:04:22 GMT</pubDate>
    </item>
    <item>
      <title>是否有一个带有 rollout-buffer 的在线策略算法（A2C/PPO）的有效实现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsvsux/is_there_a_working_implementation_of_an_onpolicy/</link>
      <description><![CDATA[我有一个可变长度情节的环境，其中当前状态取决于先前的操作。此环境无法可靠地模拟，并且只能向前迈出一步。训练数据是静态的，每集都有要实现的目标。此环境可以计算奖励，在情节结束时从 rollout-buffer 执行 env.step()，然后执行 model.train()，或者可能在每个步骤之后进行训练。  我尝试使用 SB3 实现这一点，但似乎从未使用 rollout-buffer 测试过此代码，并且没有示例。有人知道在这样的环境中可以有效实现像 A2C 这样的在线策略算法吗？    提交人    /u/principle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsvsux/is_there_a_working_implementation_of_an_onpolicy/</guid>
      <pubDate>Mon, 01 Jul 2024 15:27:59 GMT</pubDate>
    </item>
    <item>
      <title>从我的第一个大项目中学到的一些经验教训</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsswl0/some_lessons_from_getting_my_first_big_project/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsswl0/some_lessons_from_getting_my_first_big_project/</guid>
      <pubDate>Mon, 01 Jul 2024 13:25:05 GMT</pubDate>
    </item>
    <item>
      <title>我使用 DPO 进行 LLM 对齐，但只获得了微小的改进。DPO 对某些任务不起作用，这是正常的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsojuc/im_using_dpo_for_llm_alignment_and_only_get/</link>
      <description><![CDATA[你好！事实上，我没有强化学习方面的经验。我确实使用 OpenAI 教程学习了策略梯度算法和 PPO，并阅读了大量与 DPO 相关的论文。但是，当我在自己的多模态 LLM 上尝试 DPO 时，所需指标的改进很小（从 9 到 9.5）。我检查了我的实现，似乎损失是正确的。 我也尝试了偏好数据：我使用 ① 由参考模型生成的获胜-失败对，其中温度=1.0， ② 由参考模型和一些基本事实生成的获胜-失败对（我的任务有基本事实句子），以及 ③ 由参考模型生成的获胜-失败对，其中温度=1.0 和 2.0 以及基本事实。结果表明，① 产生了最大的改进（与提到数据分布偏移问题的原始论文一致）。 但是，在修复了我能想到的所有可能问题之后，我仍然无法使算法工作。因此，我迫切需要任何建议/经验分享。你认为这是 DPO 的容量限制，还是我的实现中存在一些未被发现的错误？    提交人    /u/Worth-Note9721   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsojuc/im_using_dpo_for_llm_alignment_and_only_get/</guid>
      <pubDate>Mon, 01 Jul 2024 09:12:05 GMT</pubDate>
    </item>
    <item>
      <title>只是想分享我的快乐，我的第一个主要 RL 项目不再变得糟糕。感谢我从这里得到的帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsnlqj/just_wanted_to_share_my_happiness_my_first_major/</link>
      <description><![CDATA[       由    /u/Breck_Emert  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsnlqj/just_wanted_to_share_my_happiness_my_first_major/</guid>
      <pubDate>Mon, 01 Jul 2024 08:05:41 GMT</pubDate>
    </item>
    <item>
      <title>RL 的用户，您希望 RL 做什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsiiao/users_of_rl_what_do_you_wish_rl_could_do/</link>
      <description><![CDATA[对于那些致力于 RL 应用的人来说，您的最终目标是什么？您希望 RL 能做什么？什么用例推动了您的工作？    提交人    /u/Obsesdian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsiiao/users_of_rl_what_do_you_wish_rl_could_do/</guid>
      <pubDate>Mon, 01 Jul 2024 02:47:47 GMT</pubDate>
    </item>
    <item>
      <title>制作了一个软演员评论家模型。但结果很糟糕，我哪里犯了错误？（pytorch，月球着陆器）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsi26i/made_a_soft_actor_critic_model_but_bad_result/</link>
      <description><![CDATA[   https://preview.redd.it/c3231tj2it9d1.png?width=855&amp;format=png&amp;auto=webp&amp;s=f47a4fd623a4fd884e55f539c863c954a0706d8a 这是我的代码https://github.com/lch3942/test &quot;训练开始了，但似乎学习过程没有正常工作。我遵循了这个伪代码，但我不确定它是否被正确实现，结果并不好。你能帮我找出我哪里出错了吗？谢谢你阅读我的帖子。     提交人    /u/Delicious_Bowl1645   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsi26i/made_a_soft_actor_critic_model_but_bad_result/</guid>
      <pubDate>Mon, 01 Jul 2024 02:23:30 GMT</pubDate>
    </item>
    <item>
      <title>列出你遇到的最佳 LLM 驱动的多智能体强化学习论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsf5y9/name_best_llmpowered_multi_agent_rl_papers_you/</link>
      <description><![CDATA[大家好！顾名思义，我正在对多智能体强化学习中 LLM 的最佳用途进行广泛的文献综述，特别是如果涉及学习/训练。在上下文中，学习也很重要。 如果有任何想法，请说出它们的名字，谢谢！    提交人    /u/miladink   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsf5y9/name_best_llmpowered_multi_agent_rl_papers_you/</guid>
      <pubDate>Sun, 30 Jun 2024 23:52:10 GMT</pubDate>
    </item>
    <item>
      <title>逻辑看似正确，但代理商却没有蜂拥而至</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds7wuj/seemingly_correct_logic_but_agents_not_flocking/</link>
      <description><![CDATA[我有一个boid 群集环境，并使用 PPO。 我的奖励函数是让它们群集。我有两个奖励，即对齐和凝聚力。从逻辑上讲，它们似乎没有错。最初我在凝聚力函数中使用了线性函数而不是指数函数，但结果相同。  邻域半径违规会导致 -100 奖励。 def reward(self, agent, neighbour_velocities, neighbour_positions): CohesionReward = 0 AlignmentReward = 0 total_reward = 0 outofflock = False midpoint = (SimulationVariables[&quot;SafetyRadius&quot;] + SimulationVariables[&quot;NeighborhoodRadius&quot;]) / 2 if len(neighbor_positions) &gt; 0: for neighbour_position in neighbour_positions: distance = np.linalg.norm(agent.position - neighbour_position) if distance &lt;= SimulationVariables[&quot;SafetyRadius&quot;]: CohesionReward -= 10 elif SimulationVariables[&quot;SafetyRadius&quot;] &lt; distance &lt;中点： 比率 = (距离 - SimulationVariables[&quot;SafetyRadius&quot;]) / (中点 - SimulationVariables[&quot;SafetyRadius&quot;]) CohesionReward += 10 * np.exp(比率 - 1) elif 中点 &lt;= 距离 &lt; SimulationVariables[&quot;NeighborhoodRadius&quot;]: 比率 = (距离 - 中点) / (SimulationVariables[&quot;NeighborhoodRadius&quot;] - 中点) CohesionReward += 10 * np.exp(-ratio) average_velocity = np.mean(neighbor_velocities, axis=0) dot_product = np.dot(average_velocity, agent.velocity) norm_product = np.linalg.norm(average_velocity) * np.linalg.norm(agent.velocity) 如果 norm_product == 0: cos_angle = 1.0 否则: cos_angle = dot_product / norm_product cos_angle = np.clip(cos_angle, -1.0, 1.0) orientation_diff = np.arccos(cos_angle) alignment = 1 - (orientation_diff / np.pi) AlignmentReward = -20 * alignment + 10 else: CohesionReward -= 100 outofflock = True # 引入基线奖励 total_reward = CohesionReward + AlignmentReward return total_reward, outofflock     提交人    /u/OccupyFood101   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds7wuj/seemingly_correct_logic_but_agents_not_flocking/</guid>
      <pubDate>Sun, 30 Jun 2024 18:23:47 GMT</pubDate>
    </item>
    <item>
      <title>“通过算法蒸馏实现上下文强化学习”，Laskin 等人 2022 年 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds5m71/incontext_reinforcement_learning_with_algorithm/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds5m71/incontext_reinforcement_learning_with_algorithm/</guid>
      <pubDate>Sun, 30 Jun 2024 16:39:44 GMT</pubDate>
    </item>
    <item>
      <title>“通过指令预测提高长视界模仿能力”，Hejna 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds5f7w/improving_longhorizon_imitation_through/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds5f7w/improving_longhorizon_imitation_through/</guid>
      <pubDate>Sun, 30 Jun 2024 16:31:02 GMT</pubDate>
    </item>
    <item>
      <title>《奥赛罗被破解了》泷泽2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ds2f1o/othello_is_solved_takizawa_2023/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ds2f1o/othello_is_solved_takizawa_2023/</guid>
      <pubDate>Sun, 30 Jun 2024 14:13:54 GMT</pubDate>
    </item>
    <item>
      <title>Q-Learning 的问题（代理无法正确学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1drm7w0/problem_with_qlearning_agent_is_not_learning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1drm7w0/problem_with_qlearning_agent_is_not_learning/</guid>
      <pubDate>Sat, 29 Jun 2024 22:01:37 GMT</pubDate>
    </item>
    </channel>
</rss>