<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 16 Aug 2024 21:14:41 GMT</lastBuildDate>
    <item>
      <title>关于将不可解析的参数传递给 gym 环境的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1etud8m/question_about_passing_unpickleable_argument_to/</link>
      <description><![CDATA[您好， 我有一个关于将参数传递给使用 make_vec_env 包装的自定义体育馆环境的问题。 如果我在 env_kwargs 中传递一些字符串参数，则以下代码有效。 venv0 = make_vec_env(WindFarmEnv_v1p3,vec_env_cls=SubprocVecEnv, n_envs=n_envs, seed=0, env_kwargs=env_kwargs,start_index = FFAddress) 但是，如果我尝试传递基于 cython 的类的实例（准确地说是 zeromq 服务器对象），我会收到错误，因为 stable-baselines3 似乎尝试 pickle env_kwargs 和基于 cython 的对象不可 pickle。完整的回溯包含在底部。 有没有办法绕过 env_kwargs 的 pickle 要求？如果没有，有没有办法以其他方式传递此类参数？ 谢谢    提交人    /u/dead_phoenix_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1etud8m/question_about_passing_unpickleable_argument_to/</guid>
      <pubDate>Fri, 16 Aug 2024 17:12:39 GMT</pubDate>
    </item>
    <item>
      <title>寻找适用于 MetaWorld 环境的 TD3/DDPG 的 MAML RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1etjf42/looking_for_maml_rl_implementation_with_td3ddpg/</link>
      <description><![CDATA[大家好， 我一直在寻找强化学习中模型无关元学习 (MAML) 的实现，该实现专门使用 TD3 或 DDPG 作为基础算法。到目前为止，我找到的唯一代码是 TRPO（像这个 one），但它不符合我的需求，因为我希望将其应用于 MetaWorld 环境或后来对其进行修改以与 HER 的目标条件环境一起使用。 是否有人知道任何现有的实现或有关如何修改现有 MAML 代码以与 TD3 或 DDPG 一起使用的提示？ 提前感谢任何帮助或指点！    提交人    /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1etjf42/looking_for_maml_rl_implementation_with_td3ddpg/</guid>
      <pubDate>Fri, 16 Aug 2024 08:21:55 GMT</pubDate>
    </item>
    <item>
      <title>构建一个 RL 代理来遵循轨迹（DeepMimic）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ethjfi/building_an_rl_agent_to_follow_a_trajectory/</link>
      <description><![CDATA[我正在寻找一个非常简单的代码示例，用于训练代理遵循特定轨迹（理想情况下是人形机器人）。我想遵循 DeepMimic 论文，但它陷入了 C++ 层，没有 pytorch 代码。如果有任何入口点可以查看，我将不胜感激    提交人    /u/soulslicer0   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ethjfi/building_an_rl_agent_to_follow_a_trajectory/</guid>
      <pubDate>Fri, 16 Aug 2024 06:13:21 GMT</pubDate>
    </item>
    <item>
      <title>使用 FQE 估计离线 RL 的学习曲线？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1et7m3n/learning_curve_using_fqe_to_estimate_offline_rl/</link>
      <description><![CDATA[      这是 ChatGPT 生成的，您觉得怎么样？    提交人    /u/Blasphemer666   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1et7m3n/learning_curve_using_fqe_to_estimate_offline_rl/</guid>
      <pubDate>Thu, 15 Aug 2024 21:51:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO 模型预测的回报与训练的回报截然不同</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1esre7t/ppo_model_predict_returning_radically_different/</link>
      <description><![CDATA[      大家好， 我训练了一个自定义环境，在我看来，奖励相对稳定。 https://preview.redd.it/mf8sshytwsid1.png?width=698&amp;format=png&amp;auto=webp&amp;s=48781a7f09559004d968ad5a88df8798a595b34c 在同一个脚本中，就在我调用 model.learn(...) 之后，我有以下几行：  from stable_baselines3.common.evaluation import assess_policy mean_reward, std_reward = assess_policy(model, env, n_eval_episodes=100) print(f&quot;平均奖励：{mean_reward} +/- {std_reward}&quot;)  有趣的是，它打印： 平均奖励：-980.4289540863037 +/- 136.99807648736126  问题：  什么原因导致这种情况？ 通常如何调试此类问题？  谢谢！    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1esre7t/ppo_model_predict_returning_radically_different/</guid>
      <pubDate>Thu, 15 Aug 2024 10:05:33 GMT</pubDate>
    </item>
    <item>
      <title>UCRL2：预期遗憾的下限和上限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1esqkzs/ucrl2_lower_and_upper_bounds_on_expected_regret/</link>
      <description><![CDATA[大家好， 我一直在阅读 Peter Auer、Thomas Jaksch 和 Ronald Ortner 撰写的文章《强化学习的近似最优遗憾界限》，我无法理解他们的定理 4 和 5 如何共存。 他们在定理 4 中指出，给定 MDP 的预期遗憾的上限由一个随 log T 增长的函数确定，其中 T 是 MDP 中的步数。 但是，在定理 5 中，他们指出预期遗憾的下限由一个随 T 的平方根增长的函数确定。 渐近地，T 的平方根函数将超过 T 的对数函数，所以我不明白这两个界限如何存在。我遗漏了什么？    提交人    /u/AdRepulsive7382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1esqkzs/ucrl2_lower_and_upper_bounds_on_expected_regret/</guid>
      <pubDate>Thu, 15 Aug 2024 09:09:54 GMT</pubDate>
    </item>
    <item>
      <title>MAPOCA 的统一</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1esok7v/mapoca_in_unity/</link>
      <description><![CDATA[你好 周围有谁有使用过 MAPOCA 训练器吗？ 需要一些帮助。    提交人    /u/SuaibProshanto999   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1esok7v/mapoca_in_unity/</guid>
      <pubDate>Thu, 15 Aug 2024 06:51:18 GMT</pubDate>
    </item>
    <item>
      <title>SB3 MaskablePPO 返回无效操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1esbt2l/sb3_maskableppo_returning_an_invalid_action/</link>
      <description><![CDATA[我正在使用 SB3 的 MaskablePPO，我注意到在训练一开始它就返回了无效操作，尽管我传递了正确的掩码。如果选择了无效操作，我的代码就会抛出错误，所以我无法使用它来训练模型。我在文档中找不到解释，但这是正常行为吗？ 示例： 我正在使用大小为 2、4 的 MultiDiscrete 动作空间 传递有效动作掩码 [True, True, True, False, False, True] 意味着无效动作为 [ [], [1, 2] ]。 我得到 [0 1] 作为动作，即使 1 无效 我发现了原因： 我在训练之前使用 check_env，它会忽略掩码。  如果您使用的是自定义环境并且想要使用 check_env 对其进行调试，它将执行方法 step 并向其传递随机动作（使用 action_space.sample()），而不考虑无效动作掩码  https://sb3-contrib.readthedocs.io/en/v2.1.0/modules/ppo_mask.html    由   提交  /u/khalifa30000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1esbt2l/sb3_maskableppo_returning_an_invalid_action/</guid>
      <pubDate>Wed, 14 Aug 2024 20:27:48 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以找到机器人领域的实习机会？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1es8e0n/where_to_find_internship_opportunities_in_rl_for/</link>
      <description><![CDATA[嗨， 我目前在德国攻读机器人学硕士学位。我还在寻找一份从 2025 年 4 月开始在欧盟地区专注于基于学习的控制的实习机会。在网上做了一些研究后，我只能找到一般的 ML 实习机会，而没有找到与 RL 或机器人模仿学习相关的实习机会。  我相信这个 subreddit 上的人以前一定遇到过类似的问题。请指导我在哪里寻找实习机会。 谢谢    提交人    /u/No_Bid_3602   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1es8e0n/where_to_find_internship_opportunities_in_rl_for/</guid>
      <pubDate>Wed, 14 Aug 2024 18:00:27 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero 参与加密货币交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1erxbhl/alphazero_in_crypto_trading/</link>
      <description><![CDATA[我正在探索用于加密交易的高级方法，例如 AlphaZero 概念，包括概率分布、状态值和蒙特卡罗模拟。尽管训练了 LSTM、NN 和 CNN 模型，但我没有看到重大进展。我不确定如何继续，正在寻找在训练期间评估模型的有效方法。 是否有人为现实世界的交易实施，我很高兴听到你的话。    提交人    /u/laxuu   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1erxbhl/alphazero_in_crypto_trading/</guid>
      <pubDate>Wed, 14 Aug 2024 09:35:45 GMT</pubDate>
    </item>
    <item>
      <title>MCTS 是不是 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ervkgb/is_mcts_rl_or_not/</link>
      <description><![CDATA[我目前正在研究 MCTS 和 RL。关于 MCTS 是强化学习还是只是一种基本的搜索/规划算法，似乎存在不少争议。我知道 mcts 不会明确更新价值函数或策略。不过，我也读到过一些观点，认为 MCTS 实际上是 RL（不指 alphezero）。这有点颠覆了我对基于模型的 RL 的理解。我一直认为基于模型的 RL 正是 MCTS 所做的。事实上，如果 MCTS 的选择/扩展甚至不是 RL，那么 AlphaZero 似乎不是基于模型的。那么还有基于模型的方法吗？似乎几乎每种基于模型的方法都在内部使用无模式强化学习。AlphaZero 使用 DeepNN 估计（不涉及模型），……    提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ervkgb/is_mcts_rl_or_not/</guid>
      <pubDate>Wed, 14 Aug 2024 07:35:52 GMT</pubDate>
    </item>
    <item>
      <title>在强化学习中，最优策略总是确定性的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1erurar/is_the_optimal_policy_always_deterministic_in/</link>
      <description><![CDATA[您好， 我有一个关于最优策略的问题。 根据贝尔曼最优方程，最优策略是确定性的。但是，我很好奇这个结论是否适用于所有环境。 最优策略是否仅在平稳环境中是确定性的？在非平稳环境中，随机策略是否也可以是最优的？此外，是否存在即使在平稳环境中，随机策略也可能被视为最优的场景？ 如果您能提供任何见解或解释，我将不胜感激。谢谢！    提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1erurar/is_the_optimal_policy_always_deterministic_in/</guid>
      <pubDate>Wed, 14 Aug 2024 06:41:59 GMT</pubDate>
    </item>
    <item>
      <title>PPO 澄清</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ere8hc/ppo_clarification/</link>
      <description><![CDATA[在尝试实施 PPO 时，我对策略比率代表什么感到有些困惑：pi_theta/pi_theta_old。我不明白的是，我们如何计算 pi_theta，即新策略概率？我们尚未更新我们的策略，因为这是损失的重点（通过它进行反向传播并更新我们的策略），所以 pi_theta 不是=pi_theta_old 吗？这个损失与我们收集的数据有什么关系？    提交人    /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ere8hc/ppo_clarification/</guid>
      <pubDate>Tue, 13 Aug 2024 17:44:10 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习解决 NP-hard 图形问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ere13q/model_based_reinforcement_learning_to_solve/</link>
      <description><![CDATA[大家好，我目前正在研究基于模型的强化学习方法，用于解决 NP 难题 (DFJSP)，以图形建模。我实施了 MCTS，并进行了一些调整（渐进式加宽、修剪、RAVE）。我目前正在寻找更多方法，解决巨大的动作和状态空间、收敛时间和内存优化问题。最后，我想通过将原始 MCTS 与所做的修改进行比较来展示结果，并将它们与其他现有方法（也可能是无模型的）进行比较。该论文背后的想法是，鉴于问题的动态和不确定性，基于模型的强化学习将能够更好地适应动态事件并提高样本效率。你们对更高级的方法有什么想法吗？我考虑过使用 GAT 架构来估计价值和策略以及 PUCT。也许还有 Thompson 采样或 Gumbal 噪声用于选择。也许还有非 MCTS 方法？我愿意接受任何想法！     提交人    /u/BeezyPineapple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ere13q/model_based_reinforcement_learning_to_solve/</guid>
      <pubDate>Tue, 13 Aug 2024 17:36:00 GMT</pubDate>
    </item>
    <item>
      <title>MDP 与 POMDP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1er21m5/mdp_vs_pomdp/</link>
      <description><![CDATA[尝试理解 MDP 和子程序，以便对 RL 有基本的了解，但事情变得有点棘手。根据我的理解，MDP 仅使用当前状态来决定采取哪种操作，而真实状态是已知的。然而在 POMDP 中，由于代理无法访问真实状态，因此它利用其观察和历史记录。 在这种情况下，如果 POMDP 使用来自历史记录的信息，即从先前观察（即 t-3，...）中检索到的信息，那么它如何具有马尔可夫特性（它甚至被称为 MDP）。 非常感谢你们！    提交人    /u/Internal-Sir-5393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1er21m5/mdp_vs_pomdp/</guid>
      <pubDate>Tue, 13 Aug 2024 07:48:53 GMT</pubDate>
    </item>
    </channel>
</rss>