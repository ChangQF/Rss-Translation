<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 09 Nov 2024 01:12:37 GMT</lastBuildDate>
    <item>
      <title>计算机视觉问题的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gmfx3u/reinforcement_learning_on_computer_vision_problems/</link>
      <description><![CDATA[大家好， 我是一名计算机视觉研究员，主要从事 3D 视觉任务。最近，我开始研究 RL，意识到许多视觉问题可以重新表述为某种策略或价值学习结构。进行和遵循这种重新表述是否有好处？是否有任何重要的工作取得了比监督学习更好的结果？    提交人    /u/Foreign-Associate-68   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gmfx3u/reinforcement_learning_on_computer_vision_problems/</guid>
      <pubDate>Fri, 08 Nov 2024 10:57:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我创建了一个可以自动优化你的 LLM 提示的 RL 代理！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm64gy/p_i_created_a_rl_agent_that_can_autooptimize_your/</link>
      <description><![CDATA[大家好！ 我和我的团队开发了一个自动优化 LLM 提示的系统，该系统具有可视化功能，可以跟踪提示结构和学习进度。 请看这里：https://nomadic-ml.github.io/nomadic/cookbooks/Nomadic_Prompt_Optimization_Report.html 也请查看我们的网站：Nomadic ML。 以下是 GIF 格式的预览！ https://i.redd.it/l57uimn7qkzd1.gif https://i.redd.it/ybkf74aaqkzd1.gif 关于此可视化的工作原理：RL Prompt Optimizer 采用强化学习框架来迭代改进用于语言模型评估的提示。在每一集中，代理都会根据状态表示选择一个操作来修改当前提示，该状态表示对提示的特征进行编码。代理会根据对模型响应的多指标评估获得奖励，从而鼓励开发能够引出高质量答案的提示。 查看我们的 github repo 并给我们一个星星！ https://github.com/nomadic-ml/nomadic    由   提交  /u/vnkn17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm64gy/p_i_created_a_rl_agent_that_can_autooptimize_your/</guid>
      <pubDate>Fri, 08 Nov 2024 00:48:51 GMT</pubDate>
    </item>
    <item>
      <title>关于如何构建 AGI 的有趣理论。参考强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm1oep/interesting_theory_on_how_to_build_agi_references/</link>
      <description><![CDATA[  由    /u/Radlib123  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm1oep/interesting_theory_on_how_to_build_agi_references/</guid>
      <pubDate>Thu, 07 Nov 2024 21:28:36 GMT</pubDate>
    </item>
    <item>
      <title>2024 年 GPU 和计算技术比较 – 第 7 天</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm07x3/gpu_and_computing_technology_comparison_2024_day_7/</link>
      <description><![CDATA[        由    /u/Potential_Arrival326   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm07x3/gpu_and_computing_technology_comparison_2024_day_7/</guid>
      <pubDate>Thu, 07 Nov 2024 20:27:28 GMT</pubDate>
    </item>
    <item>
      <title>您是否同意深度强化学习 (Deep RL) 目前正在经历 Imagenet 时刻这一观点？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gls0qm/do_you_agree_with_this_take_that_deep_rl_is_going/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gls0qm/do_you_agree_with_this_take_that_deep_rl_is_going/</guid>
      <pubDate>Thu, 07 Nov 2024 14:43:12 GMT</pubDate>
    </item>
    <item>
      <title>如何利用强化学习优化单台起重机作业调度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gllq93/how_can_i_optimize_single_crane_job_scheduling/</link>
      <description><![CDATA[      https://preview.redd.it/po4m8h1nufzd1.png?width=1886&amp;format=png&amp;auto=webp&amp;s=427bbc5edd094edebbd6a2a979e256c9f0cafdb0 我正在从事一个涉及具有双桅杆属性的单起重机作业调度的项目。让我详细解释一下每项工作：  工作 1：当两个托盘到达 A 时，将它们从 A 移动到 B。 工作 2：当两个托盘在 B 处的充电时间完成后，将它们从 B 移动到 C。 工作 3：当两个托盘在 C 处的充电时间完成后，将它们从 C 移动到 D。 工作 4：当两个托盘在 D 处的处理完成后，将它们从 D 移动到 E。 工作 5：当两个托盘在 E 处的处理完成后，将它们从 E 移动到 F。  在这个项目中，我打算将工作 1 到 5 定义为动作，同时将每个架子上托盘的存在情况和剩余的充电或处理时间视为状态。我的目标是使用强化学习来选择最佳动作。 我想讨论的是如何将这种状态转换为输入格式。目前，我计划使用 CNN 将这些状态输入到 DQN 中，但我想知道是否有更有效的方法。我想简明扼要地总结一下流程情况。你能推荐一种更有效的方法吗？    提交人    /u/Yunseol_IE   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gllq93/how_can_i_optimize_single_crane_job_scheduling/</guid>
      <pubDate>Thu, 07 Nov 2024 08:18:01 GMT</pubDate>
    </item>
    <item>
      <title>我当前的 RL 项目的直播</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1glgq9w/live_stream_of_my_current_rl_project/</link>
      <description><![CDATA[我要离开我的电脑，但我想检查我的机器、学习环境的进度，所以我设置了一个直播。 我在 Godot 中完成了这个项目，它使用套接字与 PyTorch 通信。目标是让代理找到并导航到目标，而无需知道目标位置。代理只知道它的位置、它的旋转、它的最后一个动作、步数和它的七条视线。 目标是看看我是否可以让这个代理使用一个简单的奖励函数，而不使用目标位置的知识。如果达到了目标，奖励函数只是将 100 分除以序列中每个移动的移动数，否则每个移动将获得 -100 除以序列中的移动数。 该流仅显示并行运行的 100 个模拟中的一个。我觉得这很有趣，我想你们可能也会喜欢。此外，如果有人有任何想法，如何改进它，请随时分享。    提交人    /u/stokaty   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1glgq9w/live_stream_of_my_current_rl_project/</guid>
      <pubDate>Thu, 07 Nov 2024 02:59:43 GMT</pubDate>
    </item>
    <item>
      <title>评论网络无法预测奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1glgcd6/critic_network_is_failling_to_predict_rewards/</link>
      <description><![CDATA[      https://preview.redd.it/y7o4wybc5ezd1.png?width=2058&amp;format=png&amp;auto=webp&amp;s=66421482f30fdad610078cf6e12499ced4​​e87810 我正在训练一个 Actor - Critic 模型，但它无法有效地学习任务。我意识到，Critic Loss 在训练时并没有减少，并决定获得 True 奖励和批评输出的输出，以比较批评网络的性能。正如您在图中看到的，它根本没有学习任何东西。我尝试使用 Vanilla LSTM 进行训练，也尝试使用另一个带有残差连接和前馈网络的自定义 LSTM 块的模型进行训练，但它们都做同样的事情。 我正在为 Actor 和 Critic 头使用共享层，并使用单个优化器进行训练。这里可能有什么问题？    提交人    /u/BagComprehensive79   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1glgcd6/critic_network_is_failling_to_predict_rewards/</guid>
      <pubDate>Thu, 07 Nov 2024 02:39:03 GMT</pubDate>
    </item>
    <item>
      <title>微调与迁移学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gl9w4b/fine_tune_vs_transfer_learning/</link>
      <description><![CDATA[        提交人    /u/employeeINGOAMPT   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gl9w4b/fine_tune_vs_transfer_learning/</guid>
      <pubDate>Wed, 06 Nov 2024 21:38:19 GMT</pubDate>
    </item>
    <item>
      <title>什么是适合 RL 的良好技术堆栈？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gkdebb/what_is_a_good_tech_stack_for_rl/</link>
      <description><![CDATA[目前正在研究 Cuda、Jax、CleanRL、PufferLib、Ray。我遗漏了什么吗？如果有的话，其中哪一个是多余的？    提交人    /u/True_Caregiver485   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gkdebb/what_is_a_good_tech_stack_for_rl/</guid>
      <pubDate>Tue, 05 Nov 2024 18:20:51 GMT</pubDate>
    </item>
    <item>
      <title>涌现：集体行为的隐藏力量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk8q2o/emergence_the_hidden_power_of_collective_behavior/</link>
      <description><![CDATA[我是自然界涌现的粉丝，写了几篇文章与大家分享。 https://medium.com/@ryanchen_1890/emergence-the-hidden-power-of-collective-behavior-e02e05c72786    提交人    /u/RyanlovesAI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk8q2o/emergence_the_hidden_power_of_collective_behavior/</guid>
      <pubDate>Tue, 05 Nov 2024 15:03:55 GMT</pubDate>
    </item>
    <item>
      <title>评价损失散度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk5h5q/critic_loss_divergence/</link>
      <description><![CDATA[      大家好， 我正在实施一个多头 PPO，其中每个头负责不同的（但相关的）任务。但是，我注意到每个头的批评者损失都在显著增加——有时从大约 10 增加到 1200 或更多。以下是输出的快照，供您参考。 我尝试过分别更新每个评论家以及同时更新所有评论家，并使用值剪辑。此外，在参与者网络中，我使用共享层（L1、L2），然后为每个头部使用不同的输出分支。然而，对于批评家来说，每个头部都有自己独立的 L1 和 L2 层。 这些架构选择是否会导致批评家损失不断增加，或者可能还有其他因素在起作用？ https://preview.redd.it/ecnmsv4lr2zd1.png?width=823&amp;format=png&amp;auto=webp&amp;s=f6bed4bb786ccab34c7a73848fe5a38ef5ee970a # Set1 value clipping value_set1_clipped = old_values_set1 + torch.clamp(value_set1 - old_values_set1, -self.value_clip_range, self.value_clip_range) value_set1_loss1 = F.mse_loss(value_set1, returns_set1) value_set1_loss2 = F.mse_loss(value_set1_clipped, returns_set1) critical_loss_set1 = torch.max(value_set1_loss1, value_set1_loss2) # Set2 值剪辑 value_set2_clipped = old_values_set2 + torch.clamp(value_set2 - old_values_set2, -self.value_clip_range, self.value_clip_range) value_set2_loss1 = F.mse_loss(value_set2, returns_set2) value_set2_loss2 = F.mse_loss(value_set2_clipped, returns_set2)评论家损失_set2 = torch.max(value_set2_loss1, value_set2_loss2) ####################################输出####################################### 演员损失：0.5793，熵：2.5832，评论家头部1损失：461.3597，评论家头部2损失：1024.5741，评论家头部3损失：21.0361 演员损失：0.5793，熵：2.5832，评论家头部1损失：461.3597，评论家头部2损失：1024.5741，评论家头部3损失：21.0361 演员损失：0.6495，熵：2.5602，评论家头部1损失： 266.5478，评论家头2损失：426.3173，评论家头3损失：16.1255演员损失：0.7650，熵：2.6232，评论家头1损失：427.5551，评论家头2损失：775.9523，评论家头3损失：44.9366演员损失：0.6635，熵：2.5855，评论家头1损失：501.3060，评论家头2损失：887.4315，评论家头3损失：30.6863演员损失：0.9118，熵：2.6160，评论家头1损失：432.1326，评论家头2损失：705.5318，评论家头部3损失：55.9993 演员损失：0.7652，熵：2.6095，评论家头部1损失：468.3109，评论家头部2损失：466.6273，评论家头部3损失：83.0151 演员损失：0.6764，熵：2.6375，评论家头部1损失：476.9982，评论家头部2损失：741.9779，评论家头部3损失：54.6600 演员损失：0.5160，熵：2.6646，评论家头部1损失：468.3273，评论家头部2损失：1085.3656，评论家头部 3 损失：19.7672 演员损失：0.6571，熵：2.5796，评论家头部 1 损失：455.7019，评论家头部 2 损失：688.5980，评论家头部 3 损失：66.3462 演员损失：0.7888，熵：2.5792，评论家头部 1 损失：437.5110，评论家头部 2 损失：601.6379，评论家头部 3 损失：71.4872     提交人    /u/GuavaAgreeable208   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk5h5q/critic_loss_divergence/</guid>
      <pubDate>Tue, 05 Nov 2024 12:26:40 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk54sa/deep_reinforcement_learning_generalization/</link>
      <description><![CDATA[理解和诊断深度强化学习。发表于国际机器学习会议 ICML 2024。 链接：https://proceedings.mlr.press/v235/korkmaz24a.html    提交人    /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk54sa/deep_reinforcement_learning_generalization/</guid>
      <pubDate>Tue, 05 Nov 2024 12:06:19 GMT</pubDate>
    </item>
    <item>
      <title>动态状态表示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk4q2r/dynamic_state_representation/</link>
      <description><![CDATA[大家好！ 我想问一下，是否有人听说过在代理的某个情节中状态空间可以发生变化的场景。 例如，假设我是一个在空房间里徘徊的代理，我的状态空间表示是我的 (x,y) 坐标。突然，我意识到我应该拿起一个位于我旁边房间里的物体。 然后我的状态空间可能会变为 (x,y,current_room,is_holding_anything)。 有谁知道以前的任何工作中存在这种情况？无论是规划还是 RL 领域。 提前谢谢！！    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk4q2r/dynamic_state_representation/</guid>
      <pubDate>Tue, 05 Nov 2024 11:41:12 GMT</pubDate>
    </item>
    <item>
      <title>我第一次使用强化学习来解决自己的问题！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjsr6q/my_first_use_of_reinforcement_learning_to_solve/</link>
      <description><![CDATA[        提交人    /u/JealousCookie1664   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjsr6q/my_first_use_of_reinforcement_learning_to_solve/</guid>
      <pubDate>Mon, 04 Nov 2024 23:35:27 GMT</pubDate>
    </item>
    </channel>
</rss>