<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 27 Jan 2025 09:18:42 GMT</lastBuildDate>
    <item>
      <title>我的系统是否真的需要 RL 模型，或者检测模型是否就足够了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ib04za/do_i_really_need_an_rl_model_for_my_system_or/</link>
      <description><![CDATA[大家好，希望你们一切顺利 我正在做一个项目，目标是确定何时在无线传感器网络中执行密钥刷新。一般的想法是识别节点中的异常行为（如受损或故障节点），然后决定是否需要密钥刷新。 密钥刷新需要大量资源，因此过于频繁地执行密钥刷新是一种浪费，但如果您不及时执行密钥刷新，您的网络将变得岌岌可危。 现在，我决定使用 RL 模型来做出这个决定，但我一直在质疑 RL 是否真的有必要，或者更简单的检测模型是否足够（然而检测传感器节点受损攻击非常困难）？特别是在这个子版块的一篇文章中，有人指出，确实可以使用简单的监督轻量级模型而不是 RL 模型来解决许多问题。 提前感谢您的建议！我很乐意回答任何问题。 PS：我只是一名计算机科学专业的学生，​​所以我对 RL 的了解有限，我发现它是最难理解的 ML 模型    提交人    /u/Sufficient-Lie-1632   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ib04za/do_i_really_need_an_rl_model_for_my_system_or/</guid>
      <pubDate>Mon, 27 Jan 2025 04:48:19 GMT</pubDate>
    </item>
    <item>
      <title>旧的 RL 课程仍然有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</link>
      <description><![CDATA[大家好。我想知道我应该从哪门课程开始学习 RL。我想从 2024 年的斯坦福 234 课程开始，但我不知道它是否教授基础知识。我还听说 David Silver 的课程很棒，但它是近 10 年前的，我不知道我应该从哪门课程开始。 TL;DR 开始学习 RL 的最佳课程是什么？    提交人    /u/madcraft256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</guid>
      <pubDate>Mon, 27 Jan 2025 04:31:32 GMT</pubDate>
    </item>
    <item>
      <title>4-7 年前的 PyTorch 代码能运行吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iahkrx/will_pytorch_code_from_47_years_ago_run/</link>
      <description><![CDATA[我发现很多 RL repos 上次更新是 4 到 7 年前，比如这个： https://github.com/Coac/never-give-up PyTorch 在过去几年里有过很多重大变化吗？修复旧代码以再次运行有多困难？    提交人    /u/exploring_stuff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iahkrx/will_pytorch_code_from_47_years_ago_run/</guid>
      <pubDate>Sun, 26 Jan 2025 15:38:11 GMT</pubDate>
    </item>
    <item>
      <title>Ray 2.40 上的 PBT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</link>
      <description><![CDATA[有人熟悉在 Ray 2.4 上做 PBT 吗？ 如果有人知道如何解决这个问题，我们将不胜感激： https://discuss.ray.io/t/metric-for-pbt-in-ray-2-40/21619 摘要：我想基于评估情节奖励均值指标使用 PBT 对 PPO 执行超参数优化，但我似乎无法使用该指标或任何有用的指标进行训练。    提交人    /u/nukelius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</guid>
      <pubDate>Sun, 26 Jan 2025 10:48:27 GMT</pubDate>
    </item>
    <item>
      <title>寻求与 RL 研究人员的合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</link>
      <description><![CDATA[大家好， 我是 UIUC 的计算机科学博士生，具有理论算法背景（在 SODA/ICALP/ESA 发表过文章；主要是近似算法、图问题的可扩展算法、在线算法等）。最近，我将重点转向 使用强化学习 (RL) 解决 NP 难图问题，我正在寻找有相同兴趣的合作者。 关于我的工作：  在理论会议（SODA、ESA）和 ML 会议（NeurIPS）上发表过文章。 最近开发了一种基于 RL 的 NP 难图问题方法，包括从头开始在 PyTorch 中编写自定义 GNN 框架。论文已提交给 ICML。 坚实的理论基础 + 良好的编码能力，旨在将理论与实践相结合。  寻找： 对将 RL 与图算法/组合优化问题相结合感兴趣的研究人员，尤其是以下人员：  研究 NP 难图问题（例如，TSP、顶点覆盖、图分区）。 关心为什么学习策略有效（例如，理论保证、泛化分析）。 想要构建既有原则又有实践效率的方法。  如果这与您的工作或兴趣重叠，请随时给我发私信！我很乐意分享我的论文草稿，讨论想法或探索合作。 （使用一次性帐户进行匿名，但可以通过电子邮件/ LinkedIn 进行验证。）    提交人    /u/ForAllEpsilonExists   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</guid>
      <pubDate>Sun, 26 Jan 2025 07:00:09 GMT</pubDate>
    </item>
    <item>
      <title>构建定制的机械臂环境并训练 AI 代理来控制它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</link>
      <description><![CDATA[        由    /u/Fabulous-Extension76  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</guid>
      <pubDate>Sun, 26 Jan 2025 03:59:44 GMT</pubDate>
    </item>
    <item>
      <title>特征选择/状态抽象方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</link>
      <description><![CDATA[大家好，有谁知道有什么论文/作品，其中代理具有非常高维的状态空间，并且可以通过某种方式减小尺寸？有没有什么常用的方法可以为代理选择最佳特征？    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</guid>
      <pubDate>Sat, 25 Jan 2025 23:25:36 GMT</pubDate>
    </item>
    <item>
      <title>“DeepSeek-R1：通过强化学习激励法学硕士中的推理能力”，Guo 等人 2025 {DeepSeek}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</guid>
      <pubDate>Sat, 25 Jan 2025 22:52:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Collab 中安装 MARLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</link>
      <description><![CDATA[我按照说明在 Collab 中安装 MARLib： https://marllib.readthedocs.io/en/latest/ conda create -n marllib python=3.8 conda activate marllib git clone cd MARLlib pip install --upgrade pip pip install -r requirements.txt # 我们推荐 gym 版本在 0.20.0~0.22.0 之间。 pip install gym&gt;=0.20.0,&lt;0.22.0 # 将补丁文件添加到 MARLlib python patch/add_patch.py​​ -yhttps://github.com/Replicable-MARL/MARLlib.git  要求安装到 ray 1.8.0，找不到该版本（我也尝试过 1.13 但找不到）。 删除版本会导致更多错误和更多不兼容性。总是显示相同的消息： 错误：subprocess-exited-with-error 当安装没有特定版本的所有内容时，调用 marl.algos.mappo 时，它会抛出： ModuleNotFoundError：没有名为“ray.rllib.agents”的模块 有人可以为我提供安装 MARLlib 的更新说明并且没有不兼容性吗？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</guid>
      <pubDate>Sat, 25 Jan 2025 22:33:04 GMT</pubDate>
    </item>
    <item>
      <title>文本推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</link>
      <description><![CDATA[大家好，我想知道你们有没有推荐的教科书，无论是在线的还是数字的，这些教科书可以从高层次深入到强化学习领域。就背景而言，我拥有电气硕士学位，并且做过相当多的机器学习工作，但我在强化学习方面做得最先进的是 cuda 中的批量 Q 学习。我甚至从未实现过自己的深度 Q 学习算法。希望能找到一些数学密集型的问题。主要关注机器人技术和寻路，但愿意看任何东西。    提交人    /u/Tassadon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</guid>
      <pubDate>Sat, 25 Jan 2025 01:57:22 GMT</pubDate>
    </item>
    <item>
      <title>对 5090 / GTC 2025 的思考</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</link>
      <description><![CDATA[有人对培训代理的 5090 感到兴奋吗？有什么特别的理由吗？ 此外，如果有人要去，廉价的边疆航班将让我今年第二次参加 GTC。很想喝点东西。去年我玩得很开心，将在周日参加其中一次培训，然后周二离开。    提交人    /u/ParamedicFabulous345   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</guid>
      <pubDate>Fri, 24 Jan 2025 21:36:59 GMT</pubDate>
    </item>
    <item>
      <title>如何确定扑克锦标赛中的最佳经纪人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</link>
      <description><![CDATA[我目前正在开展一个项目，该项目旨在确定哪种深度强化学习算法最适合复杂环境，例如无限注德州扑克。我正在使用 Tianshou 制作代理和 PettingZoo 环境。我已经完成了项目的这一部分，现在我必须确定哪个代理是最好的。我让每个代理相互对战了 30,000 场，并收集了大量数据。 起初，我认为赢得最多筹码的玩家应该是赢家，但这并不公平，因为一名玩家在与最弱的玩家之一的比赛中赢得了很多筹码，并输给了所有其他玩家，但这仍然使他成为赢得最多筹码的赢家。然后我考虑了 ELO 评级，但这也行不通，因为如果玩家赢得的钱很少，那么获胜就不重要了。 在其他游戏中最常用的 2 种情况的组合是 chip_won_by_A / (chips_won_by_A + chip_won_by_B)，但这种组合也行不通，因为这是一个零和游戏环境，chips_won_by_A = -chips_won_by_B，结果除以零。你对这类问题还有其他解决方案吗？我认为使用他们本可以赢得的筹码数量中赢得的筹码百分比可能是个好主意？欢迎提供任何帮助！    提交人    /u/komensalizam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</guid>
      <pubDate>Fri, 24 Jan 2025 17:42:05 GMT</pubDate>
    </item>
    <item>
      <title>策略迭代中的策略评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</link>
      <description><![CDATA[      在 Sutton 的书中，策略评估 (4.5) 是 pi(s,a) * q(s,a) 的总和。但是，当我们在策略迭代过程中使用策略评估时（图 4.3），为什么我们不需要对所有动作进行求和，而只需要对 pi(s) 进行评估呢？ https://preview.redd.it/5vo75evilyee1.png?width=1030&amp;format=png&amp;auto=webp&amp;s=77af1304d549008b8c2e24c9cd8dff034519acae    submitted by    /u/lalalagay   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</guid>
      <pubDate>Fri, 24 Jan 2025 15:22:14 GMT</pubDate>
    </item>
    <item>
      <title>仍然不太漂亮，但奖励函数略好一些</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</link>
      <description><![CDATA[       由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</guid>
      <pubDate>Fri, 24 Jan 2025 15:15:20 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Shadow Dextrous 的手在 pybullet 中抓取 3D 杯子模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</link>
      <description><![CDATA[你好。我正在尝试使用 PyBullet 来模拟假手抓握。我使用影子手 urdf 作为我的手，一个杯子的 3d 模型。我正在努力实现影子手抓取杯子。 我希望最终使用强化学习来优化对不同尺寸杯子的抓取，但我需要先运行没有任何 AI 的 Python 脚本，这样我才能有一个基线来与 RL 模型进行比较。有人知道可以帮助我的资源吗？提前谢谢了。    提交人    /u/Flamesword200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</guid>
      <pubDate>Fri, 24 Jan 2025 12:50:18 GMT</pubDate>
    </item>
    </channel>
</rss>