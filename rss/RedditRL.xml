<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 11 May 2024 21:13:04 GMT</lastBuildDate>
    <item>
      <title>寻找张量板日志参数及其含义的引用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpqwjw/looking_for_citations_for_tensorboard_log/</link>
      <description><![CDATA[我正在写我的论文，想添加一章来介绍我将如何判断学习策略的性能。我主要关注奖励、熵系数和损失。尽管它很简单，但有没有我可以引用的任何论文或官方来源。   由   提交 /u/pvmodayil   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpqwjw/looking_for_citations_for_tensorboard_log/</guid>
      <pubDate>Sat, 11 May 2024 20:51:14 GMT</pubDate>
    </item>
    <item>
      <title>连续行动空间：固定/计划与学习与预测标准偏差</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpq4a2/continuous_action_space_fixedscheduled_vs_learned/</link>
      <description><![CDATA[据我所知，有 3 种方法可以在连续动作空间设置中设置动作分布的标准差：  固定/预定的 std，在训练开始时设置为超参数 可学习的参数张量，其初始值可以设置为超参数。 SB3 使用此方法 https://github.com/ DLR-RM/stable-baselines3/blob/285e01f64aa8ba4bd15aa339c45876d56ed0c3b4/stable_baselines3/common/distributions.py#L150 标准也是“预测”的网络就像动作的平均值  在什么情况下你会使用哪种方法？  方法 2 和方法 2 3 对我来说似乎有点危险，因为优化器可能会将 std 设置为非常低的值，从而阻碍探索并且基本上“过度拟合”。到目前的政策。但由于 SB3 使用方法 2，情况似乎并非如此。 感谢您提供任何见解！   由   提交 /u/TheBrn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpq4a2/continuous_action_space_fixedscheduled_vs_learned/</guid>
      <pubDate>Sat, 11 May 2024 20:13:53 GMT</pubDate>
    </item>
    <item>
      <title>tmrl 的多代理实现（Track Mania RL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpnxg2/multi_agent_implementation_for_tmrl_track_mania_rl/</link>
      <description><![CDATA[我有兴趣为 TrackMania RL (TMRL) 开发多代理系统 (https://github.com/trackmania-rl/tmrl）。有谁知道与 TMRL 相关的现有多代理实现或文档吗？ Git 存储库将不胜感激。如果没有，您能否分享一些关于如何启动此过程的见解或资源，特别是修改多个代理的 TMRL 及其交互策略？谢谢！   由   提交 /u/msquaresproperty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpnxg2/multi_agent_implementation_for_tmrl_track_mania_rl/</guid>
      <pubDate>Sat, 11 May 2024 18:31:25 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用强化学习来代替穷举搜索以获得更好的时间效率吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpnd7r/can_i_use_reinforcement_learning_to_replace/</link>
      <description><![CDATA[我对 RL 非常陌生，想知道是否可以使用它来解决这个特定问题。  我有一个包含 53 个点的列表，每个点都有相关的损失，然后我为这些点生成一个 n 组合列表。这个想法是，例如，我采用这些 (53C3) 的 3 个组合（大约 23,000 个组合），并更改原始 53 中的这些值，并评估每个组合的损失。所以我现在有一个大约 23,000 个项目的列表，每个项目都有一个相关的损失，我只想找到损失最高的组合。 到目前为止，我已经能够使用详尽的方法来做到这一点搜索。但由于我通过神经网络对每个组合进行推理，因此需要大约 25 分钟来迭代每个组合以找到损失最高的组合。穷举搜索的代码如下： attack_combinations = Combinations(X_train_filtered.columns, num_attacked_ap) Attack_combinations_list = [] for Attack in Attack_combinations: Attack_combinations_list.append(attack) with open(&#39;noise -high.txt&#39;) as file: file_contents = file.read() const_noise = np.fromstring(file_contents, sep=&#39; &#39;) aps_loss = {} best_loss = 0 best_combination = 0 for Attack_combination in tqdm(attack_combinations_list): # 应用攻击to the dataset X_val_attacked = deepcopy(X_val_filtered) for col_name in X_val_attacked.columns: if col_name in Attack_combination: Noise = const_noise X_val_attacked[col_name] += Noise # 在被攻击数据集上评估模型 loss, precision, mse = model.evaluate(X_val_attacked , y_val_filtered, verbose=0) aps_loss[tuple(attack_combination)] = mse 如果 mse &gt; best_loss: best_loss = mse best_combination = Attack_combination print(&quot;最佳组合：&quot;, best_combination) print(f&quot;{num_attacked_ap}-组合的最高 DL MSE:&quot;, best_loss)  但是，我想尝试更大的 n 组合，在这种情况下，组合的数量会增加到数百万，从而使穷举搜索变得不可行。因此，我想知道 RL 是否可以在这里使用以及如何使用。我很高兴能指出正确的方向，提前谢谢您。   由   提交/u/CapedCrusader10   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpnd7r/can_i_use_reinforcement_learning_to_replace/</guid>
      <pubDate>Sat, 11 May 2024 18:04:58 GMT</pubDate>
    </item>
    <item>
      <title>关于如何在 Metaworld 中训练 pickplacev2 任务的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpl5e6/questions_about_how_to_train_pickplacev2_task_in/</link>
      <description><![CDATA[嗨， 有人在 pickplacev2 MetaWorld 中的任务？我尝试使用自己的实现以及使用 Stable Baselines3 等库来训练代理，但代理无法有效学习，甚至难以达到目标。我的代码看起来是正确的，因为它可以很好地处理其他任务，例如 reachv2、windowclosev2 和其他几个任务。 有趣的是，我已经成功训练了 DDPG代理通过采用特定技术来完成此任务。我认为失败有两个原因。首先是pickplacev2的奖励函数没有设计。 pickplacev2 中到达阶段的奖励信号非常弱，在初始位置通常小于 0.03，这可能会增加难度。这令人困惑，因为该论文提出了一种改进的奖励设计。其次，探索对于这项任务来说似乎很重要。对于 DDPG，修改噪声尺度至关重要，这表明探索对于这项任务很重要 如果您取得了成功，您能否分享有关特定配置、超参数或有助于成功训练的修改的见解或技巧？    由   提交/u/DF_13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpl5e6/questions_about_how_to_train_pickplacev2_task_in/</guid>
      <pubDate>Sat, 11 May 2024 16:21:39 GMT</pubDate>
    </item>
    <item>
      <title>这是一个询问基于理论/数学的强化学习科目途径的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpkog7/here_is_a_question_asking_for_a_theorymathbased/</link>
      <description><![CDATA[这是一个询问机器学习或强化学习科目的基于理论/数学的途径的问题： 您可以吗概述涵盖机器学习和强化学习的理论和数学基础的课程或课程路径？我正在寻找深入研究这些领域背后的理论基础、概率模型、优化技术和数学分析的主题，而不仅仅是关注应用算法或编码实现。目标是在转向更实际的应用之前建立强大的理论掌握。   由   提交/u/Background_Bowler236   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpkog7/here_is_a_question_asking_for_a_theorymathbased/</guid>
      <pubDate>Sat, 11 May 2024 16:00:36 GMT</pubDate>
    </item>
    <item>
      <title>对 RL 库/项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</link>
      <description><![CDATA[嗨， 我目前正在休学一年，致力于自学和强化学习项目。我将于 9 月份加入伦敦大学学院的计算统计和机器学习理学硕士学位，目标是准备 RL 博士学位。 在过去的几个月中，我一直在撰写两篇 RL 研究论文，这些论文将在分别提交给 NeurIPS（会议论文，第二作者）和 ICML（立场文件，共同作者）。由于我还有 4 个月的时间，我正在寻找其他项目来改善我即将到来的博士申请的简历。 我考虑过开发一个可用于研究的库，但很难选择一个具体主题。以下是我目前的指导方针：  框架：最好是 JAX，因为它最近越来越受欢迎，而且生态系统仍在扩展。我认为有机会从更成熟的框架（例如 Pytorch）中调整现有库，或者为 JAX 中缺乏合适软件的特定 RL 问题提供便捷的解决方案。 -  领域：我对开放性（课程学习、无监督环境设计）和多代理设置特别感兴趣。然而，已经有一些流行的库涵盖了这些领域的大量用例（Minimax、JaxMARL、JaxUED...）。我最终想在未来更深入地研究元和进化强化学习等其他领域。据我所知，进化强化学习也有一些不错的库（evojax、evosax 等），但我对元强化学习不太确定。 时间范围 ：大约 5 个月  我对社区对以下问题的见解很感兴趣：  是否有特定的 RL 领域缺乏有用的库贾克斯？ （在我引用的或其他人中） 如果没有，为现有库做出贡献会是更好的主意吗？ 是否更需要高效的环境和基准？   感谢您的帮助！   由   提交 /u/OptimalBandicoot1671   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</guid>
      <pubDate>Sat, 11 May 2024 15:11:46 GMT</pubDate>
    </item>
    <item>
      <title>需要帮忙</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpj3fy/need_help/</link>
      <description><![CDATA[我的模型工作正常。它是带有 Carla 模拟器和 td3 实现的变道模型。但是当我在environment.py 文件中添加深度和障碍物传感器时。看来我犯了一个错误。现在，车子不动了。它产卵并且不动它突然重生。我会付费寻求帮助。( 10$ ) 但很紧急   由   提交/u/Leather_Efficiency34   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpj3fy/need_help/</guid>
      <pubDate>Sat, 11 May 2024 14:45:52 GMT</pubDate>
    </item>
    <item>
      <title>[需要帮助] 将 SB3 DQN 训练脚本转换为 RLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpiyd7/help_needed_convert_sb3_dqn_training_script_to/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpiyd7/help_needed_convert_sb3_dqn_training_script_to/</guid>
      <pubDate>Sat, 11 May 2024 14:39:18 GMT</pubDate>
    </item>
    <item>
      <title>有关使用首次访问策略蒙特卡罗和新奖励信号解决冰冻湖问题的反馈。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpdzxf/feedback_regarding_solving_frozen_lake_using/</link>
      <description><![CDATA[我试图用道馆图书馆提供的默认奖励设置来解决冰冻湖。  使用默认的奖励设置，我发现无法解决冰冻的湖泊环境。特工会在没有到达目标的情况下撞入洞中，或者继续撞到墙壁上，然后最终坠毁。  因为我在读 Sutton 和 Barto 的《强化学习》一书，我知道对于二十一点问题，书中提到了负奖励。  然后我决定测试新的奖励系统。如果智能体掉进洞里，奖励将为-1。如果智能体撞到墙，奖励将为-0.1，如果智能体达到目标，奖励将为+10。  令我惊讶的是，这个奖励系统运行得非常好，代理更快地找到目标。  我想得到这个子的反馈。  是否可以使用蒙特卡罗以默认奖励信号来解决 Frozen Lake 问题？  或者真的有必要使用新的奖励信号来改变它吗？  顺便说一句，我已经使用动态规划和时间差分方法用默认奖励信号解决了冰冻湖。  谢谢。    由   提交/u/mono1110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpdzxf/feedback_regarding_solving_frozen_lake_using/</guid>
      <pubDate>Sat, 11 May 2024 09:58:24 GMT</pubDate>
    </item>
    <item>
      <title>我迫切需要学习方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cp6xc6/im_in_desperate_need_for_help_with_learning/</link>
      <description><![CDATA[你好， 我第一次遇到强化学习，是在 3 年前（当时我还在读大二） CS 本科学位）在浏览 Coursera 时发现了阿尔伯塔大学的 RL 专业。我感到深深的联系，并继续完成了 4 门课程。这个 Reddit 子版块中的许多人可能已经意识到，专业化并没有深入太多，它提供的信息足以应用一些最基本的 RL 算法和技术。我能够利用这些知识来完成我的本科论文，涉及强化学习在云计算问题中的应用，并从这项工作中获得了一些相当不错的出版物。  毕业已经一年了，一直从事初级软件开发人员的工作。我想从事 RL 方面的研究。我相信我缺乏直接攻读博士学位所需的个人资料。因此，我计划在一所优秀的学校攻读基于论文的硕士学位课程，这可以取得博士学位。  即使我注册了基于论文的硕士课程，在大多数学校，获得 PI 也是我的责任。每当我打开任何研究强化学习的教授的教员页面时，我都会看到一些我根本不理解的东西。每个人似乎都在研究 RL 中的一些超级小众的东西，而对我来说 RL 本身似乎是一个相当小众的领域。我听人们说，要获得成功的研究型项目经验，我必须拥有“相同的兴趣”作为我的 PI 等等。 我现在面临的主要问题是，我缺乏对该领域的知识深度，甚至不知道自己想要什么。这让我找到了 YouTube 上的一些材料和一些高级强化学习教科书。但我很快发现我所拥有的数学知识水平不足以理解这些高级材料。那么，我现在该怎么办？对于一个试图进入基于论文的硕士课程的本科生来说，只对自己感兴趣的领域有基本的了解可以吗？如果没有，我如何获得我确定缺乏的知识？ 我知道这是一个混合问题，不仅涉及强化学习，还涉及一般研究生院的建议。抱歉，如果它不属于此子项。 感谢您的任何建议。   由   提交 /u/SeaworthinessHot5365   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cp6xc6/im_in_desperate_need_for_help_with_learning/</guid>
      <pubDate>Sat, 11 May 2024 02:25:34 GMT</pubDate>
    </item>
    <item>
      <title>数据质量网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cosbwj/dqn/</link>
      <description><![CDATA[&lt;表&gt;      I我一直在尝试从头开始实现 DQN，但预测值似乎有些不对劲，所以我在这里做了一个简单的测试。这里，有一系列价值为 1 的奖励，最终没有奖励。 DQN（净）应该预测折扣奖励。然而，由于某种原因，对后续状态的估计会急剧上升，这与实际曲线相反，并且 DQN 需要几个时期才能意识到其错误并将初始状态向上移动。这里可能发生什么？ https ://preview.redd.it/qok0jda3bmzc1.png?width=915&amp;format=png&amp;auto=webp&amp;s=fde8029f66d4a40c761a90c23f36a657d85b1b9b 代码： 从torch导入nn，optim导入matplotlib.pyplot作为plt导入随机导入numpy作为np规模= 100折扣= 0.9奖励= [1 for i in range(scale-1)] + [0] net = nn.Sequential ( nn.Linear(4,4), nn.Mish(), nn.Linear(4,1)) opt = optim.RMSprop(net.parameters(), lr=0.01) def 步骤(c): 全局折扣,奖励，净，opt qvals = [] for i in range(scale): inp = torch.tensor([[i/50 for _ in range(4)]], dtype=torch.float32) #inp = torch.tensor ([[i/50 for _ in range(1)]+[random.random() for _ in range(3)]], dtype=torch.float32) 输出 = net(inp) q = output.detach() .item() qvals.append(q) 如果 i == scale-1: next_q = 0 否则: next_inp = torch.tensor([[(i+1)/50 for _ in range(4)]], dtype= torch.float32) next_q = net(next_inp).detach().item() td_error = 折扣 * next_q + 奖励[i] - q grad = torch.tensor([[-td_error/scale]], dtype=torch.float32 ) #grad = torch.tensor([[-td_error / np.sqrt(1 + td_error**2) / 100]], dtype=torch.float32) 输出.backward(grad) nn.utils.clip_grad_norm_(net.parameters (), 1) opt.step() opt.zero_grad() if c%1 == 0: plt.plot(list(range(scale)), qvals) plt.savefig(f&#39;zoo{c}.png&#39; ) plt.clf() #plt.show() for i in range(50): step(i)    由   提交/u/AUser213  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cosbwj/dqn/</guid>
      <pubDate>Fri, 10 May 2024 15:30:39 GMT</pubDate>
    </item>
    <item>
      <title>CrossQ：深度强化学习中的批量标准化，以提高样本效率和简单性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1corrqu/crossq_batch_normalization_in_deep_reinforcement/</link>
      <description><![CDATA[     &lt; /td&gt; 论文和代码：http:// /adityab.github.io/CrossQ CrossQ 是一种无模型的离策略方法，它无需执行额外的梯度步骤就超越了当前的 SOTA。它本质上是：  采用 SAC（6yo 方法） 删除目标网络 添加批量归一化  这些简单的编辑足以超越 REDQ 和 DroQ 的强大性能，仅需要 5% 的梯度步长。 Twitter：https://twitter.com/aditya_bhatt/status/1768342823747674377 ICLR 2024 焦点演讲：https://iclr.cc/virtual/2024/poster/18699 （这里是第一位合著者，很乐意提供帮助！） https://preview.redd.it/0bhb82z77mzc1.png?width=3582&amp; amp;格式=png&amp;auto=webp&amp;s=41f3071cb84445734a6338e672658c99044ea560   由   提交 /u/RoboticsLiker   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1corrqu/crossq_batch_normalization_in_deep_reinforcement/</guid>
      <pubDate>Fri, 10 May 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL Baselines3 Zoo 进行分布式优化的启动试验有多少次</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</link>
      <description><![CDATA[您好，我正在 RL Baselines3 Zoo 中的 6 个进程中分配优化（github），它使用 Optuna 。 我关心的参数代码位于 rl_zoo3/train.py 中：  parser.add_argument( &quot;--n-trials&quot;, help=&quot;优化超参数的试验次数。&quot; &quot;这适用于每个优化运行器，而不是整个优化过程。&quot;, type=int, default= 500, ) parser.add_argument(&quot;--n-startup-Trials&quot;, help=&quot;使用 optuna 采样器之前的试验次数&quot;,  我知道我是否使用以下每个6 个进程 --n-trials 100 那么我将获得 600 次试验。 但是 --n-startup-Trials 10 会怎么样呢？是 10 次还是 60 次启动试验？    提交ufoludek3000&quot;&gt; /u/ufoludek3000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</guid>
      <pubDate>Fri, 10 May 2024 10:25:03 GMT</pubDate>
    </item>
    <item>
      <title>“通过强化学习出现类似信念的表征”，Hennig 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</guid>
      <pubDate>Thu, 09 May 2024 16:06:37 GMT</pubDate>
    </item>
    </channel>
</rss>