<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 28 Apr 2024 15:13:37 GMT</lastBuildDate>
    <item>
      <title>在国际象棋或将棋中实现 alpha zero 是一个好的论文主题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cf4dob/is_implementation_of_alpha_zero_in_chess_or_shogi/</link>
      <description><![CDATA[我应该写一篇关于数据科学/机器学习的论文，我选择了在（国际象棋或将棋）中实现 alphazero（尚未选择游戏） ）但是这是一个有效的论文主题吗？如果是的话，它是一个好的主题吗？如果您有任何主题想法，请提出任何建议谢谢   由   提交 /u/General_Arm_7352   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cf4dob/is_implementation_of_alpha_zero_in_chess_or_shogi/</guid>
      <pubDate>Sun, 28 Apr 2024 11:39:02 GMT</pubDate>
    </item>
    <item>
      <title>动作在 100 步后重复状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cf41pk/action_repeats_states_after_100_steps/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cf41pk/action_repeats_states_after_100_steps/</guid>
      <pubDate>Sun, 28 Apr 2024 11:18:46 GMT</pubDate>
    </item>
    <item>
      <title>“超越 A*：通过搜索动态 Bootstrapping 使用 Transformer 进行更好的规划”，Lehnert 等人 2024 {FB}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cerhkh/beyond_a_better_planning_with_transformers_via/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cerhkh/beyond_a_better_planning_with_transformers_via/</guid>
      <pubDate>Sat, 27 Apr 2024 22:58:33 GMT</pubDate>
    </item>
    <item>
      <title>在使用 RL 的火箭着陆环境中需要帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ced23q/need_help_in_rocket_landing_environment_using_rl/</link>
      <description><![CDATA[嘿伙计们，希望你们一切顺利。我是 RL 新手，正在开发一个使用 pyflyt 火箭着陆健身房 的项目，我目前正在与 PPO 代理一起进行培训，但我没有得到很好的结果。一件好事是火箭至少正在非常接近着陆台的地方坠落。你们能帮我提供一些我可以尝试的想法和我可以使用的算法吗？预先感谢！ 我当前的 PPO 超参数是这些 policy_kwargs = { &quot;net_arch&quot;: [256, 256, 128], # 神经网络架构 } ppo_params = {“tensorboard_log”：“./”，“policy_kwargs”：policy_kwargs，“learning_rate”：0.0003，“clip_range”：0.2，“batch_size”：4096，“n_steps”：4096，“ gamma”: 0.99, “gae_lambda”: 0.95, “n_epochs”: 10, “ent_coef”: 0.01, “vf_coef”: 0.5, “max_grad_norm”: 0.5, }  &lt; /div&gt;  由   提交/u/Successful_Bug_5098   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ced23q/need_help_in_rocket_landing_environment_using_rl/</guid>
      <pubDate>Sat, 27 Apr 2024 12:10:27 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习约束</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cebn3g/deep_rl_constraints/</link>
      <description><![CDATA[有没有办法对 TD3 和 SAC 等与奖励函数无关的深度 RL 方法应用约束（即，除了惩罚违反行为的代理）约束）？   由   提交/u/Key-Scientist-3980   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cebn3g/deep_rl_constraints/</guid>
      <pubDate>Sat, 27 Apr 2024 10:45:49 GMT</pubDate>
    </item>
    <item>
      <title>DDPG可以解决高维环境吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ce8rwg/can_ddpg_solve_high_dimensional_environments/</link>
      <description><![CDATA[所以，我正在试验我的 DDPG 代码，发现它在低维状态动作空间（猎豹和漏斗）的环境中效果很好，但得到在高维空间上更糟（ant：111 + 8）。有没有人之前观察到类似的结果或者我的实现有问题？   由   提交/u/Interesting-Weeb-699   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ce8rwg/can_ddpg_solve_high_dimensional_environments/</guid>
      <pubDate>Sat, 27 Apr 2024 07:33:20 GMT</pubDate>
    </item>
    <item>
      <title>让代理在不确定状态下继续概率转换的策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ce3z1p/policy_letting_the_agent_continue_the_probability/</link>
      <description><![CDATA[我研究机器人技术，是强化学习的初学者。基本上，我正在构建一系列用于应对不确定性的功能，并且我正在尝试弄清楚策略如何让代理在与故障、错误和故障相关的内部和外部状态中保持转换。简而言之，我想了解的是政策如何帮助应对不确定性，目前基本概念对我来说已经足够了。   由   提交 /u/Alive-Opportunity-23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ce3z1p/policy_letting_the_agent_continue_the_probability/</guid>
      <pubDate>Sat, 27 Apr 2024 02:48:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么损失如此之高，而我的模型似乎没有在二维空间的强化模型上学到任何东西</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cdl8zw/why_is_the_loss_so_high_and_my_model_is_seemingly/</link>
      <description><![CDATA[​ 我创建了一个根据此视频改编的模型： &lt; a href=&quot;https://www.youtube.com/watch?v=L8ypSXwyBds&amp;t=3720s&amp;pp=ygUecHl0b3JjaCByZWluZm9yY2VtZW50IGxlYXJuaW5n&quot;&gt;https://www.youtube.com/watch?v=L8ypSXwyBds&amp;t=3720s&amp;pp =ygUecHl0b3JjaCByZWluZm9yY2VtZW50IGxlYXJuaW5n 模型工作的环境是一个带有坦克式控件的可控球，一个可以推动的球和一个作为目标的方框 模型会因玩家触球而获得奖励，并根据迭代结束时球与球门的距离而获得奖励。 ​ 此外，模型似乎没有学习任何东西 ​ 我尝试离散化数据， 我&#39;我尝试过调整学习率， 我已经调整了奖励（尽管我可能调整得不够） ​ 仍然没有似乎没有学到任何东西， ​ 我不确定我的模型是否存在深刻的缺陷，或者模型是否不太适合这个领域因为它最初是为了在类似网格的空间中运行蛇而设计的，而这个游戏并不是在网格中构建的 ​ 链接到代码库：  https://github.com/jamiemitch121/RLNN-2D ​   由   提交/u/jam1211  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cdl8zw/why_is_the_loss_so_high_and_my_model_is_seemingly/</guid>
      <pubDate>Fri, 26 Apr 2024 13:20:29 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的未来：趋势和方向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cdiylu/the_future_of_reinforcement_learning_trends_and/</link>
      <description><![CDATA[   /u/Emily-joe  /u/Emily-joe  artiba.org/blog/the-future-of-reinforcement-learning-trends-and-directions&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cdiylu/the_future_of_reinforcement_learning_trends_and/</guid>
      <pubDate>Fri, 26 Apr 2024 11:28:28 GMT</pubDate>
    </item>
    <item>
      <title>是否有将棋的 MuZero 实现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cdghrz/is_there_a_muzero_implementation_of_shogi/</link>
      <description><![CDATA[我想为将棋实现 MuZero 我寻找将棋的 MuZero 实现，但找不到任何理论，但没有找到实际实现本身。有谁知道将棋的 MuZero 实施的资源或指南吗？ 谢谢   由   提交 /u/General_Arm_7352   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cdghrz/is_there_a_muzero_implementation_of_shogi/</guid>
      <pubDate>Fri, 26 Apr 2024 08:51:08 GMT</pubDate>
    </item>
    <item>
      <title>适用于不同规模问题的 JSP RL-Agent。有可能吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cddzlo/jsp_rlagent_for_varying_problem_sizes_is_it_even/</link>
      <description><![CDATA[我目前正在解决调度问题，但在如何设置它时遇到了一些困难。我将我的问题表述为基于输入矩阵的排序问题，该输入矩阵保存序列中每个潜在下一个元素的信息。 棘手的部分是，我每次都没有相同大小的选项。这导致环境和动作空间不一致 我无法详细说明，但它有点类似于修改后的作业车间调度问题：我想要为以下对象进行生产计划一周。代理的主要关注点是为各个生产步骤确定正确的机器和顺序，以使一周内的工具变化最小化。根据顺序，我可以填写本周的机器计划，并通过尽早开始将任务一个接一个地放置来获得明确的解决方案。因此，我的奖励将基于之前设置中可以使用的工具数量减去必须完成的更改。我还会考虑诸如机器上的同等负载和任务本身的最后期限之类的事情，但这超出了我目前的阶段。通过屏蔽，在保证可行的灵魂方面，我可以大大降低复杂性，所以我觉得这应该可行 在这个类比中，我真的不知道在特定的一周内必须完成多少任务，所以我的环境大小不同，动作空间（又名。接下来选择哪个任务）也取决于此。 有没有任何已知的方法来处理这个问题，或者你们知道如何重新制定问题？也许我只是只见树木不见森林，但在文献中我发现了针对最小问题的类似方法，但从这个意义上来说，这些问题并没有真正规模化。  ​ 如果有人能指出我的方向，我将不胜感激。提前致谢    由   提交 /u/kaynbockmehr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cddzlo/jsp_rlagent_for_varying_problem_sizes_is_it_even/</guid>
      <pubDate>Fri, 26 Apr 2024 06:01:50 GMT</pubDate>
    </item>
    <item>
      <title>“法学硕士的偏好微调应该利用次优的、符合政策的数据”，Tajwar 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cd8rxr/preference_finetuning_of_llms_should_leverage/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cd8rxr/preference_finetuning_of_llms_should_leverage/</guid>
      <pubDate>Fri, 26 Apr 2024 01:25:53 GMT</pubDate>
    </item>
    <item>
      <title>经历灾难性遗忘的常见深度强化学习实验有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cd05d4/what_are_the_common_deep_rl_experiments_that/</link>
      <description><![CDATA[我一直在通过深度学习理论的视角研究灾难性遗忘，我希望通过 RL 实验来获得一些实证结果。我可以进行一些常见的实验吗？ （在这种情况下，我实际上希望看到遗忘）   由   提交 /u/TitaniumDroid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cd05d4/what_are_the_common_deep_rl_experiments_that/</guid>
      <pubDate>Thu, 25 Apr 2024 19:26:02 GMT</pubDate>
    </item>
    <item>
      <title>DQN 为 CartPole 收敛，但不适用于月球着陆器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccx073/dqn_converges_for_cartpole_but_not_for_lunar/</link>
      <description><![CDATA[我是强化学习的新手，我打算脱离 2015 年的论文来实现 DQN，我让它收敛于 cartpole 问题，但它不会用于登月游戏。不确定它是超参数问题、架构问题还是我编码错误。感谢任何帮助或建议 class Model(nn.Module): def __init__(self, in_features=8, h1=64, h2=128, h3=64, out_features=4) - &gt;无： super().__init__() self.fc1 = nn.Linear(in_features,h1) self.fc2 = nn.Linear(h1,h2) self.fc3 = nn.Linear(h2, h3) self.out = nn .Linear(h3, out_features) defforward(self, x): x = F.relu(self.fc1(x)) x = F.dropout(x, 0.2) x = F.relu(self.fc2(x) ) x = F.dropout(x, 0.2) x = F.relu(self.fc3(x)) x = self.out(x) 返回 x policy_network = Model() 导入数学 def epsilon_decay(epsilon, t, min_exploration_prob, Total_episodes): epsilon = max(epsilon - t/total_episodes, min_exploration_prob) 从集合中返回 epsilon import dequelearning_rate = 0.01discount_factor = 0.8exploration_prob = 1.0min_exploration_prob = 0.1decay = 0.999 epochs = 5000 replay_buffer_batch_size = 128 min_replay_buffer_大小 = 5000 replay_buffer = deque(maxlen =min_replay_buffer_size) target_network = Model() target_network.load_state_dict(policy_network.state_dict()) 优化器 = torch.optim.Adam(policy_network.parameters(),learning_rate) loss_function = nn.MSELoss() 奖励 = [] 损失 = [] 损失= -100 for i in range(epochs) :exploration_prob = epsilon_decay(exploration_prob, i, min_exploration_prob, epochs) 终端 = False 如果 i % 30 == 0 : target_network.load_state_dict(policy_network.state_dict()) current_state = env.reset( ) returnssum = 0 p = False while not end : # env.render() if np.random.rand() &lt;探索_prob：action = env.action_space.sample（）否则：state_tensor = torch.tensor（np.array（[current_state]），dtype = torch.float32）与torch.no_grad（）：q_values =policy_network（state_tensor）action = torch .argmax(q_values).item() next_state,reward,terminal,info = env.step(action)rewardsum+=reward replay_buffer.append((current_state,action,terminal,reward,next_state))if(len(replay_buffer)&gt; = min_replay_buffer_size）：minibatch = random.sample（replay_buffer，replay_buffer_batch_size）batch_states = torch.tensor（[transition[0]用于小批量中的过渡]，dtype=torch.float32）batch_actions = torch.tensor（[transition[1]用于过渡）在小批量中]，dtype = torch.int64）batch_terminal = torch.tensor（[小批量中的过渡[2]]，dtype = torch.bool）batch_rewards = torch.tensor（[小批量中的过渡[3]]， dtype=torch.float32）batch_next_states = torch.tensor（[transition[4]用于小批量中的转换]，dtype=torch.float32）与torch.no_grad（）：q_values_next = target_network（batch_next_states）.detach（）max_q_values_next = q_values_next。 max(1)[0] y = batch_rewards + (discount_factor * max_q_values_next * (~batch_terminal)) q_values = policy_network(batch_states).gather(1, batch_actions.unsqueeze(-1)).squeeze(-1) loss = loss_function( y,q_values)loss.append(loss)optimizer.zero_grad()loss.backward()torch.nn.utils.clip_grad_norm_(policy_network.parameters(),10)optimizer.step()如果i%100==0而不是p: print(loss) p = True current_state = next_state returns.append(rewardsum) torch.save(policy_network, &#39;lunar_game.pth&#39;)    由   提交 /u/BigSmoke42169   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccx073/dqn_converges_for_cartpole_but_not_for_lunar/</guid>
      <pubDate>Thu, 25 Apr 2024 17:32:07 GMT</pubDate>
    </item>
    <item>
      <title>简单的库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ccnzn3/simple_libraries/</link>
      <description><![CDATA[我正在寻找 python 中的强化学习简单库。如果它有简单的算法，对我来说就足够了。我专注于棋盘游戏（井字游戏、围棋）。我发现像 rllib 这样的东西对于简单的应用程序来说似乎太复杂了    由   提交 /u/Present_Formal2674   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ccnzn3/simple_libraries/</guid>
      <pubDate>Thu, 25 Apr 2024 09:53:39 GMT</pubDate>
    </item>
    </channel>
</rss>