<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 27 Jan 2024 21:11:11 GMT</lastBuildDate>
    <item>
      <title>为什么随机网络蒸馏选择鼓励状态探索而不是状态动作探索？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1abzszi/why_does_random_network_distillation_choose_to/</link>
      <description><![CDATA[我猜他们确实尝试过，发现情况更糟，尽管他们没有写任何相关内容。只是好奇是否有人对此有任何见解。我计划很快实现 RND，这听起来像是一个简单的修改。   由   提交 /u/JustTaxLandLol   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1abzszi/why_does_random_network_distillation_choose_to/</guid>
      <pubDate>Sat, 27 Jan 2024 02:14:23 GMT</pubDate>
    </item>
    <item>
      <title>僵尸2100：一款基于博弈论的可玩网页游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1abndfy/zombie_2100_a_playable_web_game_based_on_game/</link>
      <description><![CDATA[ 由   提交/u/bluboxsw   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1abndfy/zombie_2100_a_playable_web_game_based_on_game/</guid>
      <pubDate>Fri, 26 Jan 2024 17:10:30 GMT</pubDate>
    </item>
    <item>
      <title>训练 2D 汽车的 DQN 需要多长时间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1abmshs/how_long_does_training_a_dqn_for_2d_car_take/</link>
      <description><![CDATA[我一直在尝试通过在 Tensorflow 中使用 pygame 来训练我的 DQN 来驾驶 2D 汽车。 我正在运行我的代码及其已经 1 天播放 1000 集了，我仍然认为我的模型没有取得任何进展。我似乎在代码中找不到任何问题，所以我不知道是否应该再等一下。  任何帮助或建议表示赞赏。    由   提交/u/No_Sense_3563   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1abmshs/how_long_does_training_a_dqn_for_2d_car_take/</guid>
      <pubDate>Fri, 26 Jan 2024 16:46:30 GMT</pubDate>
    </item>
    <item>
      <title>物理系统的Q学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1abhpbu/q_learning_for_physical_system/</link>
      <description><![CDATA[嗨，我正在做一个项目，我使用 Q 学习来控制球梁平衡系统。（在可旋转梁上平衡球。） 。我使用的是 q 表，其中有球位置、球速度、各状态下的光束角度，然后随着 2 个动作使光束角度增加或减少。我得到的结果是球摆动得非常大（如果它靠近中心摆动，但它从一侧摆动到另一侧，甚至在拐角处稍等一下，那会很好）你知道我可以得到的任何论文或来源吗帮忙解决这个问题吗？    由   提交/u/sinanoglu  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1abhpbu/q_learning_for_physical_system/</guid>
      <pubDate>Fri, 26 Jan 2024 12:55:53 GMT</pubDate>
    </item>
    <item>
      <title>我需要定制游戏的剧集吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1abcye8/do_i_need_episodes_for_a_custom_game/</link>
      <description><![CDATA[我制作了一款没有剧集的物理摆游戏。我正在使用稳定的基线3 td3。如果只有一场比赛，神经网络会学习吗？我不想每次都重置它。   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1abcye8/do_i_need_episodes_for_a_custom_game/</guid>
      <pubDate>Fri, 26 Jan 2024 07:38:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL 进行自主四轴飞行器仿真 - 需要路线图建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19fbeld/autonomous_quadcopter_simulation_with_rl_need/</link>
      <description><![CDATA[大家好， 完成了 Andrew Ng 的机器学习课程，并对强化学习感到兴奋。发现了 Airsim 飞行模拟器，并想使用 RL 构建我自己的自主四轴飞行器。谁能分享一个简单的路线图来帮助我实现这一目标？ 非常感谢！   由   提交/u/Double_Inspection_88   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19fbeld/autonomous_quadcopter_simulation_with_rl_need/</guid>
      <pubDate>Thu, 25 Jan 2024 15:06:12 GMT</pubDate>
    </item>
    <item>
      <title>涉及概率论的强化学习研究领域。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19fadt1/research_areas_in_rl_that_involves_probability/</link>
      <description><![CDATA[嗨。我正在攻读统计学硕士学位，我论文的最初想法是在随机环境中进行随机游走。但在开始对这个领域进行更多研究后，我最终认为我不太喜欢它，所以我开始寻找其他领域。自从 12 月开始我的 RL 之旅以来，我学习了 DeepMind 课程和 Sutton 书中的大部分章节。现在我非常渴望将我的论文改为涉及强化学习的内容，我最感兴趣的主题是MultiAgent-RL。我和我的顾问谈过，他对这个变化非常怀疑，他担心现在的强化学习主要围绕深度学习，这是一个他没有太多经验的主题，而且因为我才刚刚开始学习，他认为我将无法找到一个特定的主题来工作。考虑到这一点，我想知道是否有人可以参考 RL 中本质上涉及概率论的文章或特定主题。    由   提交 /u/VanBloot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19fadt1/research_areas_in_rl_that_involves_probability/</guid>
      <pubDate>Thu, 25 Jan 2024 14:19:25 GMT</pubDate>
    </item>
    <item>
      <title>现在使用软 Q 学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f8c03/is_soft_qlearning_used_today/</link>
      <description><![CDATA[你好， 我是强化学习学科的新手，目前正在研究不同的 RL 算法。我发现软 Q 学习算法对于具有连续动作空间的代理很有吸引力，因为与大多数其他 RL 算法相比，代理的策略不是由单峰高斯参数化的。  多模式功能使其能够同时探索多种解决方案。我认为其他算法可以收敛到局部最小值。我认为这个想法有可能更多地探索解决方案空间，从而找到更好的（全球？）解决方案。  现在我有一种感觉，与 SAC 或 PPO 等其他算法相比，软 Q 学习现在并不真正流行。这是正确的观察吗？为什么是这样？这与不稳定的训练有关系吗？我无法找到有关此主题的大量信息。 ​ 谢谢！   由   提交 /u/DependentSecurity987   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f8c03/is_soft_qlearning_used_today/</guid>
      <pubDate>Thu, 25 Jan 2024 12:35:20 GMT</pubDate>
    </item>
    <item>
      <title>构建数据科学应用程序 - Scikit Learn 的 Gael Varoquaux 创始人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f7523/building_data_science_applications_gael_varoquaux/</link>
      <description><![CDATA[    /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f7523/building_data_science_applications_gael_varoquaux/</guid>
      <pubDate>Thu, 25 Jan 2024 11:22:22 GMT</pubDate>
    </item>
    <item>
      <title>学习在线课程，掌握职场技能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f3m9g/learning_mcts/</link>
      <description><![CDATA[您好，我对 MCTS 强化学习领域的工作非常感兴趣。我知道有些算法使用某种神经指导来解决 alphazero 和 muzero 等问题。我对此有几个问题。 了解 mcts 及其变体的最佳方法是什么？哪些算法首先出现，哪些算法比之前有所改进？ MCTS 在最近的过去有多重要，未来会有更多的发展吗？   由   提交/u/anonymous1084  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f3m9g/learning_mcts/</guid>
      <pubDate>Thu, 25 Jan 2024 07:10:52 GMT</pubDate>
    </item>
    <item>
      <title>DQN 论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19f2z5q/dqn_papers/</link>
      <description><![CDATA[我目前正在做最后一年的研究项目，标题为“使用 DRL 进行股票交易”。这是讲师提出的标题。我对 DRL 完全陌生，但我计划使用 DQN，因为它显然是最容易实现的。问题是，我对 DQN 也很困惑，我不知道如何解释这些理论和概念。有谁知道有什么期刊论文可以很好地解释 DQN 并且易于理解吗？   由   提交/u/cookiesandcream30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19f2z5q/dqn_papers/</guid>
      <pubDate>Thu, 25 Jan 2024 06:28:50 GMT</pubDate>
    </item>
    <item>
      <title>使用基于模型的轨迹优化解决稀疏奖励强化学习问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19erddl/solving_sparsereward_rl_problems_with_modelbased/</link>
      <description><![CDATA[​ DTC：深度跟踪控制 你好。我们是机器人系统实验室 (RSL)，我们研究控制腿式机器人的新颖策略。在我们最近的工作中，我们将轨迹优化与强化学习相结合，以合成准确且稳健的运动行为。 您可以在此处找到 ArXiv 打印：https://arxiv.org/abs/2309.15462 此方法进一步描述视频。 我们还在这个 视频。   由   提交 /u/leggedrobotics   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19erddl/solving_sparsereward_rl_problems_with_modelbased/</guid>
      <pubDate>Wed, 24 Jan 2024 21:09:05 GMT</pubDate>
    </item>
    <item>
      <title>需要对 DRL 中的 RNN 进行一些健全性检查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ecxyc/need_some_sanity_check_on_rnns_in_drl/</link>
      <description><![CDATA[嘿。  您通常如何使用单模型多代理 RNN DRL 处理隐藏状态？我正在考虑： -将隐藏状态从网络中拉出并保留在周围每次我的策略想要执行另一个步骤时进行植入。 -保留之前观察的历史记录，并为未来的每个步骤重新运行这些虚拟体验，以使隐藏状态达到应有的位置。  我认为拉动隐藏状态并缓存它是更好的方法，因为我不必进行前向传递来恢复它。  对于反向传播，这适用于 PPO，因为我默认对整个剧集进行采样，但不适用于 DQN。我想我应该修改它以采样整个剧集？  然后我还必须注意批处理和重置隐藏状态。  天啊，人们已经开始觉得 RNN 不应该属于 DRL。    由   提交 /u/DotNetEvangeliser   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ecxyc/need_some_sanity_check_on_rnns_in_drl/</guid>
      <pubDate>Wed, 24 Jan 2024 09:06:26 GMT</pubDate>
    </item>
    <item>
      <title>在 PPO 中梯度是否流过熵项？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19ecedk/in_ppo_do_gradients_flow_through_the_entropy_term/</link>
      <description><![CDATA[根据标题，添加熵损失时是否通过策略参数进行反向传播？   由   提交/u/Conscious_Heron_9133   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19ecedk/in_ppo_do_gradients_flow_through_the_entropy_term/</guid>
      <pubDate>Wed, 24 Jan 2024 08:25:57 GMT</pubDate>
    </item>
    <item>
      <title>第一个项目：蛇</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</link>
      <description><![CDATA[     &lt; td&gt; 算法是某种类型的强化（虽然不确定，我只是从课程中获取了 nn 更新部分），我有一个神经网络69m 参数。网络的输入是 3 个网格：苹果位置、蛇位置和地图外区域。我还根据蛇的旋转来旋转输入，因此它始终朝上   由   提交/u/thebrownfrog  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:18 GMT</pubDate>
    </item>
    </channel>
</rss>