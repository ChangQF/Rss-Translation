<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Tue, 04 Feb 2025 01:14:10 GMT</lastBuildDate>
    <item>
      <title>ç¬¬ 7 ç‰ˆ Isaac Lab æ•™ç¨‹å‘å¸ƒï¼ä¸‹ä¸€æ­¥æˆ‘åº”è¯¥è®²ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼åªæ˜¯æƒ³é¡ºä¾¿è¯´ä¸€å¥ï¼Œæ„Ÿè°¢å¤§å®¶å¯¹æˆ‘çš„ Isaac Lab æ•™ç¨‹çš„æ”¯æŒå’Œé¼“åŠ±ã€‚åé¦ˆéå¸¸æ£’ï¼Œå¾ˆé«˜å…´çœ‹åˆ°å®ƒä»¬å¯¹ä½ æœ‰å¤šæœ‰ç”¨ï¼Œè€å®è¯´ï¼Œæˆ‘åœ¨åˆ¶ä½œå®ƒä»¬çš„åŒæ—¶è‡ªå·±ä¹Ÿå­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼ æˆ‘åˆšåˆšåœ¨ä¸åˆ° 2 ä¸ªæœˆçš„æ—¶é—´å†…å‘å¸ƒäº†æˆ‘çš„ç¬¬ 7 ä¸ªæ•™ç¨‹ï¼Œæˆ‘æƒ³ä¿æŒè¿™ç§åŠ¿å¤´ã€‚æˆ‘ç°åœ¨å°†ç»§ç»­åˆ¶ä½œå®˜æ–¹æ–‡æ¡£ï¼Œä½†æ¥ä¸‹æ¥ä½ å¸Œæœ›çœ‹åˆ°ä»€ä¹ˆï¼Ÿ â€œä»é›¶åˆ°è‹±é›„â€ç³»åˆ—ä¼šå¾ˆæœ‰è¶£å—ï¼Ÿç±»ä¼¼äºï¼š - è®¾è®¡å’Œåœ¨ Isaac Sim ä¸­æ¨¡æ‹Ÿæœºå™¨äºº - åœ¨ Isaac Lab ä¸­ä»å¤´å¼€å§‹ä½¿ç”¨ RL å¯¹å…¶è¿›è¡Œè®­ç»ƒ - ï¼ˆæœ€ç»ˆï¼‰å°†å…¶éƒ¨ç½²åœ¨çœŸæ­£çš„æœºå™¨äººä¸Š......ä¸€æ—¦æˆ‘ä¹°å¾—èµ·ä¸€ä¸ªğŸ˜… è®©æˆ‘çŸ¥é“æ‚¨è§‰å¾—æœ€ä»¤äººå…´å¥‹æˆ–æœ€æœ‰å¸®åŠ©çš„å†…å®¹ï¼éšæ—¶æ¬¢è¿å»ºè®®ã€‚ æˆ‘åœ¨ YouTube ä¸Šä¸Šä¼ äº†è¿™äº›ï¼š Isaac Lab æ•™ç¨‹ - LycheeAI    æäº¤äºº    /u/LoveYouChee   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</guid>
      <pubDate>Mon, 03 Feb 2025 22:16:38 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ RL è¿›è¡Œå¸ƒå±€ç”Ÿæˆï¼ˆä¾‹å¦‚ï¼šé“è·¯å’Œæˆ¿å±‹ï¼‰çš„æœ€ä½³æ–¹æ³•ã€‚å½“å‰æ¨¡å‹æœªè¿›è¡Œå­¦ä¹ ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨ RL ç”Ÿæˆç®€å•éƒŠåŒºçš„å¸ƒå±€ï¼šé“è·¯ã€éšœç¢ç‰©å’Œæˆ¿å±‹ã€‚è¿™æ›´åƒæ˜¯ä¸€ä¸ªå®éªŒï¼Œä½†æˆ‘æœ€å¥½å¥‡çš„æ˜¯æƒ³çŸ¥é“æˆ‘æ˜¯å¦æœ‰ä»»ä½•å˜åŒ–å¯ä»¥ä½¿ç”¨ RL ä¸ºæ­¤ç±»é—®é¢˜æå‡ºåˆç†çš„è®¾è®¡ã€‚ tensorboard ç›®å‰æˆ‘å·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼ˆä½¿ç”¨ gymnasium å’Œ stable_baselines3ï¼‰ã€‚æˆ‘æœ‰ä¸€ä¸ªç®€å•çš„è®¾ç½®ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªåä¸º env çš„ç¯å¢ƒï¼Œæˆ‘å°†æˆ‘çš„ä¸–ç•Œè¡¨ç¤ºä¸ºç½‘æ ¼ï¼š  æˆ‘ä»ä¸€ä¸ªç©ºç½‘æ ¼å¼€å§‹ï¼Œé™¤äº†é“è·¯å…ƒç´ ï¼ˆå…¥å£ç‚¹ï¼‰å’Œä¸€äº›æ— æ³•ä½¿ç”¨çš„å•å…ƒæ ¼ï¼ˆéšœç¢ç‰©ï¼Œä¾‹å¦‚å°æ¹–ï¼‰ æ¨¡å‹é‡‡å–çš„æ“ä½œæ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥ï¼Œæ”¾ç½®ä¸€ä¸ªé“è·¯æˆ–æˆ¿å±‹çš„ç“·ç –ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šï¼ˆtile_positionï¼Œtile_typeï¼‰  è‡³äºæˆ‘çš„å¥–åŠ±ï¼Œå®ƒä¸æ•´ä½“è®¾è®¡ç›¸å…³ï¼ˆè€Œä¸ä»…ä»…æ˜¯å¯¹æœ€åä¸€æ­¥çš„å¥–åŠ±ï¼Œå› ä¸ºæ—©æœŸçš„é€‰æ‹©å¯èƒ½ä¼šå¯¹ä»¥åäº§ç”Ÿå½±å“ã€‚å¹¶ä¸”æœ€å¤§åŒ–è®¾è®¡çš„æ•´ä½“è´¨é‡ï¼Œè€Œä¸æ˜¯å±€éƒ¨çš„ï¼‰ï¼ŒåŸºæœ¬ä¸Šæœ‰ 3 ä¸ªåŠ æƒé¡¹ï¼š  é“è·¯ç½‘ç»œåº”è¯¥æœ‰æ„ä¹‰ï¼šè¿æ¥åˆ°å…¥å£ï¼Œæ¯ä¸ªç“·ç –åº”è¯¥è¿æ¥åˆ°è‡³å°‘ 1 ä¸ªå…¶ä»–é“è·¯ç“·ç –ã€‚å¹¶ä¸”æ²¡æœ‰ 2x2 çš„é“è·¯ç“·ç –é›†ã€‚-&gt; æ•´ä¸ªè®¾è®¡ï¼ˆæ‰€æœ‰é“è·¯ç“·ç –ï¼‰çš„æ€»å’Œï¼ˆæ¯ä¸ªå¥½ç“·ç –çš„å¥–åŠ±å¢åŠ ï¼Œæ¯ä¸ªåç“·ç –çš„å¥–åŠ±å‡å°‘ï¼‰ã€‚è¿˜å°è¯•äº†æ‰€æœ‰ç“·ç –çš„ min() åˆ†æ•°ã€‚ æˆ¿å±‹åº”å§‹ç»ˆè¿æ¥åˆ°è‡³å°‘ 1 æ¡é“è·¯ã€‚-&gt; æ•´ä¸ªè®¾è®¡ï¼ˆæ‰€æœ‰æˆ¿å±‹ç“·ç –ï¼‰çš„æ€»å’Œï¼ˆæ¯ä¸ªå¥½ç“·ç –çš„å¥–åŠ±å¢åŠ ï¼Œæ¯ä¸ªåç“·ç –çš„å¥–åŠ±å‡å°‘ï¼‰ã€‚è¿˜å°è¯•äº†æ‰€æœ‰ç“·ç –çš„ min() åˆ†æ•°ã€‚ æœ€å¤§åŒ–æˆ¿å±‹ç“·ç –çš„æ•°é‡ï¼ˆç“·ç –è¶Šå¤šï¼Œå¥–åŠ±å°±è¶Šé«˜ï¼‰  æ¯å½“æˆ‘å°è¯•è¿è¡Œå®ƒå¹¶è®©å®ƒå­¦ä¹ æ—¶ï¼Œæˆ‘éƒ½ä¼šä»è¾ƒä½çš„ entropy_lossï¼ˆ-5ï¼Œåœ¨ 100k æ­¥åæ…¢æ…¢çˆ¬å‡è‡³ 0ï¼‰å’ŒåŸºæœ¬ä¸Šä¸º 0 çš„ explained_variance å¼€å§‹ã€‚æˆ‘çš„ç†è§£æ˜¯ï¼šæ¨¡å‹æ°¸è¿œæ— æ³•æ­£ç¡®é¢„æµ‹å®ƒé‡‡å–çš„ç»™å®šåŠ¨ä½œçš„å¥–åŠ±æ˜¯ä»€ä¹ˆã€‚å¹¶ä¸”å®ƒé‡‡å–çš„è¡ŒåŠ¨å¹¶ä¸æ¯”éšæœºå¥½ã€‚ æˆ‘å¯¹ RL è¿˜å¾ˆé™Œç”Ÿï¼Œæˆ‘çš„èƒŒæ™¯æ›´â€œä¼ ç»Ÿâ€ MLã€NLPï¼Œå¹¶ä¸”éå¸¸ç†Ÿæ‚‰è¿›åŒ–ç®—æ³•ã€‚ æˆ‘è®¤ä¸ºè¿™å¯èƒ½åªæ˜¯ä¸€ä¸ªå†·å¯åŠ¨é—®é¢˜ï¼Œæˆ–è€…è¯¾ç¨‹å­¦ä¹ å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚ä½†å³ä½¿å¦‚æ­¤ï¼Œæˆ‘ä¹Ÿä»ç®€å•çš„è®¾è®¡å¼€å§‹ã€‚ä¾‹å¦‚ 6x6 ç½‘æ ¼ã€‚æˆ‘è§‰å¾—è¿™æ›´å¤šçš„æ˜¯æˆ‘çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹å¼çš„é—®é¢˜ã€‚æˆ–è€…ä¹Ÿè®¸ä¸æˆ‘å¦‚ä½•æ„å»ºé—®é¢˜æœ‰å…³ã€‚ ------ é—®é¢˜ï¼šåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸å¦‚ä½•å¤„ç†è¿™æ ·çš„é—®é¢˜ï¼Ÿæœ‰äº†å®ƒï¼Œæœ‰å“ªäº›æ ‡å‡†æ–¹æ³•å¯ä»¥â€œè°ƒè¯•â€æ­¤ç±»é—®é¢˜ï¼Ÿä¾‹å¦‚ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦æ›´å¤šåœ°ä¸æˆ‘é€‰æ‹©çš„æ“ä½œç±»å‹æœ‰å…³ï¼Œæˆ–è€…ä¸æˆ‘çš„å¥–åŠ±è®¾è®¡æ–¹å¼ç­‰æœ‰å…³    æäº¤äºº    /u/LostInGradients   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</guid>
      <pubDate>Mon, 03 Feb 2025 21:00:15 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦æŒ‡å¯¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ‹¥æœ‰æ•°å­¦å­¦ä½ï¼Œå¹¶é€‰ä¿®äº†å‡ é—¨æœºå™¨å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹  (RL) è¯¾ç¨‹ã€‚ç›®å‰ï¼Œæˆ‘æ­£åœ¨å·¥ä½œï¼Œä½†æˆ‘å¯¹ RL ç ”ç©¶éå¸¸æ„Ÿå…´è¶£ã€‚è™½ç„¶æˆ‘è¿˜æ²¡æœ‰å¤ªå¤šçš„çŸ¥è¯†ï¼Œä½†æˆ‘åœ¨ç©ºé—²æ—¶é—´å­¦ä¹  RLã€‚ æœªæ¥ï¼Œæˆ‘æƒ³ä»äº‹ RL ç ”ç©¶å·¥ä½œï¼Œä½†æˆ‘ä¸çŸ¥é“è¯¥æ€ä¹ˆåšã€‚æˆ‘åº”è¯¥å‡†å¤‡ GATE å¹¶ç”³è¯· IIT/IIScï¼Œè¿˜æ˜¯åº”è¯¥ç›´æ¥ç”³è¯·é¡¶å°–çš„å¤–å›½å¤§å­¦ï¼Œå°½ç®¡æˆ‘æ²¡æœ‰ç ”ç©¶ç»éªŒï¼Ÿ    æäº¤äºº    /u/BigBuddy1276   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 13:24:45 GMT</pubDate>
    </item>
    <item>
      <title>ç»“æœçš„å¯é‡å¤æ€§</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</link>
      <description><![CDATA[æ‚¨å¥½ï¼æˆ‘æ­£åœ¨å°è¯•æŸ¥æ‰¾æœ¬æ–‡ä¸­æåˆ°çš„åŸºäºæ¨¡å‹çš„ PPO çš„å®ç°ï¼šåŸºäºæ¨¡å‹çš„æ¢ç´¢çš„ç­–ç•¥ä¼˜åŒ–ï¼Œä»¥ä¾¿é‡ç°ç»“æœå¹¶å¯èƒ½åœ¨æˆ‘çš„è®ºæ–‡ä¸­ä½¿ç”¨è¯¥æ¶æ„ã€‚ä½†ä¼¼ä¹ä»»ä½•åœ°æ–¹éƒ½æ²¡æœ‰å®˜æ–¹å®ç°ã€‚æˆ‘å·²ç»ç»™ä½œè€…å‘äº†ç”µå­é‚®ä»¶ï¼Œä½†ä¹Ÿæ²¡æœ‰æ”¶åˆ°ä»»ä½•å›å¤ã€‚ åœ¨åƒ AAAI è¿™æ ·çš„å¤§å‹ä¼šè®®ä¸Šå‘è¡¨çš„è®ºæ–‡æ²¡æœ‰ä»»ä½•å¯é‡ç°çš„å®ç°ï¼Œè¿™æ­£å¸¸å—ï¼Ÿ    æäº¤äºº    /u/GamingOzz   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</guid>
      <pubDate>Mon, 03 Feb 2025 13:18:10 GMT</pubDate>
    </item>
    <item>
      <title>â€œKimi k1.5ï¼šä½¿ç”¨ LLM æ‰©å±•å¼ºåŒ–å­¦ä¹ â€ï¼ŒKimi å›¢é˜Ÿ 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</guid>
      <pubDate>Mon, 03 Feb 2025 12:53:20 GMT</pubDate>
    </item>
    <item>
      <title>å¸®åŠ©æ¶ˆé™¤é”™è¯¯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</link>
      <description><![CDATA[å˜¿ï¼Œæˆ‘ç›®å‰æ­£åœ¨ä»¥æ·±åº¦ q å­¦ä¹ æ¨¡å‹çš„å½¢å¼è®­ç»ƒæˆ‘çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚åœ¨å°è¯•ä½¿ç”¨ Python ä¸­çš„ keras æ—¶ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¦‚æœæœ‰äººæ„¿æ„å¸®åŠ©æˆ‘å¼„æ¸…æ¥šå¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘å°†ä¸èƒœæ„Ÿæ¿€ã€‚ï¼ˆå®ƒä»¬å¯¹äºæˆ‘çš„é¡¹ç›®æ¥è¯´éå¸¸å…·ä½“ï¼Œå› æ­¤å¾ˆéš¾åœ¨ DM ä¹‹å¤–è§£é‡ŠğŸ˜…ï¼‰    æäº¤äºº    /u/at_69_420   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</guid>
      <pubDate>Mon, 03 Feb 2025 11:08:58 GMT</pubDate>
    </item>
    <item>
      <title>ç¬¬ä¸€å±Š Tinker AI å¤§èµ›ä¼˜èƒœä½œå“ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</link>
      <description><![CDATA[        æäº¤äºº    /u/goncalogordo   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</guid>
      <pubDate>Mon, 03 Feb 2025 10:00:02 GMT</pubDate>
    </item>
    <item>
      <title>å°è¯•å¤åˆ¶æ™®é€šçš„ k-bandits é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨å°è¯•å®ç° Barto Sutton ä¹¦ä¸­çš„ç¬¬ä¸€ä¸ª k-Bandits æµ‹è¯•å¹³å°ã€‚Python ä»£ç å¯åœ¨ Git ä¸Šæ‰¾åˆ°ï¼Œä½†æˆ‘æ­£å°è¯•ä»å¤´å¼€å§‹ç‹¬ç«‹å®Œæˆã€‚ æˆªè‡³ç›®å‰ï¼Œæˆ‘æ­£åœ¨å°è¯•ç”Ÿæˆå›¾ 2.2 ä¸­çš„å¹³å‡å¥–åŠ±å›¾ã€‚æˆ‘çš„ä»£ç å¯ä»¥å·¥ä½œï¼Œä½†å¹³å‡å¥–åŠ±å›¾è¿‡æ—©ç¨³å®šä¸‹æ¥ï¼Œå¹¶ä¸”ä¿æŒç¨³å®šï¼Œè€Œä¸æ˜¯åƒä¹¦ä¸­/git ä¸­é‚£æ ·å¢åŠ ã€‚æˆ‘æ— æ³•å¼„æ¸…æ¥šæˆ‘å“ªé‡Œåšé”™äº†ã€‚ å¦‚æœæœ‰äººèƒ½çœ‹ä¸€ä¸‹å¹¶åˆ†äº«ä¸€äº›æŠ€å·§ï¼Œé‚£å°†éå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæœ‰äººæƒ³è¿è¡Œ/æµ‹è¯•å®ƒï¼Œä»£ç åº”è¯¥æŒ‰åŸæ ·å·¥ä½œã€‚  éå¸¸æ„Ÿè°¢ï¼ ``` è¯¥ç¨‹åºå®ç°äº† k-bandit é—®é¢˜çš„ n æ¬¡è¿è¡Œ import numpy as np import matplotlib.pyplot as plt bandit_reward_dist_mean = 0 bandit_reward_dist_sigma = 1 k_bandits = 10 bandit_sigma = 1 samples_per_bandit = 1000 epsilon = 0.01 def select_action(): r = np.random.randn() if r &lt; epsilonï¼šaction = np.random.randintï¼ˆ0ï¼Œk_banditsï¼‰elseï¼šaction = np.argmaxï¼ˆq_estimatesï¼‰ è¿”å›æ“ä½œ def update_action_countï¼ˆA_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢å·²é‡‡å–æ¯ä¸ªæ“ä½œçš„æ¬¡æ•°n_action [A_t] + = 1 def update_action_reward_totalï¼ˆA_tï¼ŒR_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªæ“ä½œçš„æ€»å¥–åŠ±action_rewards [A_t] + = R_t def generate_rewardï¼ˆmeanï¼Œsigmaï¼‰ï¼š# ä»æ­£æ€åˆ†å¸ƒä¸­ä¸ºè¿™ä¸ªç‰¹å®šçš„banditæå–å¥–åŠ±#r = np.random.normalï¼ˆmeanï¼Œsigmaï¼‰r = np.random.randnï¼ˆï¼‰+mean# ç±»ä¼¼äºåœ¨Git repoä¸­æ‰€åšçš„return r def update_qï¼ˆA_tï¼ŒR_tï¼‰ï¼š q_estimates[A_t] += 0.1 * (R_t - q_estimates[A_t]) n_steps = 1000 n_trials = 2000 #æ¯æ¬¡è¯•éªŒä½¿ç”¨ä¸€æ‰¹æ–°çš„è€è™æœºè¿è¡Œ n_steps æ‰€æœ‰è¯•éªŒä¸­æ¯ä¸€æ­¥çš„å¥–åŠ±çŸ©é˜µ - ä»é›¶å¼€å§‹ rewards_episodes_trials = np.zeros((n_trials, n_steps)) for j in range(0, n_trials): #q_true = np.random.normal(bandit_reward_dist_mean, bandit_reward_dist_sigma, k_bandits) q_true = np.random.randn(k_bandits) # å°è¯•å¤åˆ¶ book/git ç»“æœ # æ¯ä¸ªåŠ¨ä½œï¼ˆè€è™æœºï¼‰çš„ Q å€¼ - ä»random q_estimates = np.random.randn(k_bandits) # æ¯ä¸ªåŠ¨ä½œï¼ˆbanditï¼‰çš„æ€»å¥–åŠ± - ä»é›¶å¼€å§‹ action_rewards = np.zeros(k_bandits) # åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªåŠ¨ä½œå·²é‡‡å–çš„æ¬¡æ•° - ä»é›¶å¼€å§‹ n_action = np.zeros(k_bandits) # æ¯ä¸€æ­¥çš„å¥–åŠ± - ä» 0 å¼€å§‹ rewards_episodes = np.zeros(n_steps) for i in range(0, n_steps): A_t = select_action() R_t = generate_reward(q_true[A_t], bandit_sigma) rewards_episodes[i] = R_t  update_action_reward_total(A_t, R_t) update_action_count(A_t) update_q(A_t, R_t) rewards_episodes_trials[j,:] = rewards_episodes  æ‰€æœ‰è¿è¡Œä¸­æ¯æ­¥çš„å¹³å‡å¥–åŠ± average_reward_per_step = np.zeros(n_steps) for i in range(0, n_steps): average_reward_per_step[i] = np.mean(rewards_episodes_trials[:,i]) plt.plot(average_reward_per_step) plt.show() ```    æäº¤äºº    /u/datashri   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</guid>
      <pubDate>Mon, 03 Feb 2025 09:35:25 GMT</pubDate>
    </item>
    <item>
      <title>Vision RL å¸®åŠ©å’ŒæŒ‡å¯¼ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</link>
      <description><![CDATA[å‘èªæ˜çš„äººä»¬é—®å¥½ã€‚æˆ‘ä¸€ç›´åœ¨æ·±å…¥ç ”ç©¶ RLï¼Œæˆ‘è®¤ä¸ºé‚£ä¸ªç”·äººè·³å…¥æ¸¸æ³³æ± å´æ’åˆ°å†°å—çš„è§†é¢‘é€‚ç”¨äºæˆ‘ã€‚ https://jacomoolman.co.za/reinforcementlearning/ï¼ˆå‘ä¸‹æ»šåŠ¨æˆ–ç›´æ¥æœç´¢â€œvisionâ€ä»¥è·³è¿‡ä¸æˆ‘çš„é—®é¢˜æ— å…³çš„å†…å®¹ï¼‰ è¿™æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢çš„è¿›å±•ã€‚ä»»ä½•ä½¿ç”¨è¿‡è§†è§‰ RL çš„äººå¯èƒ½éƒ½èƒ½çœ‹å‡ºæˆ‘åšé”™äº†ä»€ä¹ˆï¼Ÿæˆ‘å·²ç»å°è¯•äº†å¤§çº¦ 2 ä¸ªæœˆï¼Œè¯•å›¾ä¸ºæ¨¡å‹æä¾›å›¾åƒè€Œä¸æ˜¯å˜é‡ï¼Œä½†æ²¡æœ‰æˆåŠŸã€‚    æäº¤äºº    /u/TheRealMrJm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 07:01:10 GMT</pubDate>
    </item>
    <item>
      <title>è¿™çœ‹èµ·æ¥åƒæ˜¯ç¨³å®šçš„ PPO æ”¶æ•›å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</link>
      <description><![CDATA[      è¿™çœ‹èµ·æ¥åƒç¨³å®šçš„ PPO æ”¶æ•›å—ï¼Ÿ    æäº¤äºº    /u/TopSigmaNoCap79970   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</guid>
      <pubDate>Mon, 03 Feb 2025 01:30:55 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ·±åº¦ç ”ç©¶ç®€ä»‹â€ï¼ŒOpenAIï¼ˆåŸºäº o3 çš„ç½‘é¡µæµè§ˆ/ç ”ç©¶ä»£ç†çš„ RL è®­ç»ƒï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</guid>
      <pubDate>Mon, 03 Feb 2025 01:29:39 GMT</pubDate>
    </item>
    <item>
      <title>â€œè‡ªæˆ‘éªŒè¯ï¼Œäººå·¥æ™ºèƒ½çš„å…³é”®â€ï¼ŒSutton 2001ï¼ˆæ˜¯ä»€ä¹ˆè®©æœç´¢å‘æŒ¥ä½œç”¨ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</guid>
      <pubDate>Sun, 02 Feb 2025 23:55:51 GMT</pubDate>
    </item>
    <item>
      <title>2025 å¹´ç§‹å­£ç¡•å£«/åšå£«ç”³è¯·</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ éšç€æ‹›ç”Ÿå‘¨æœŸå…¨é¢å±•å¼€ï¼Œæˆ‘ç¥æ„¿æœ¬å‘¨æœŸç”³è¯·çš„æ‰€æœ‰äººå¥½è¿ï¼æˆ‘æ­£åœ¨ç”³è¯·ï¼Œè¿«ä¸åŠå¾…åœ°æƒ³å»ç ”ç©¶ç”Ÿé™¢åš RL ç ”ç©¶ï¼ˆåœ¨æˆ‘çš„å›½å®¶è¿™å¾ˆå°‘è§ï¼‰ã€‚ åœ¨è¯„è®ºä¸­å†™ä¸‹ä½ ç”³è¯·çš„åœ°æ–¹ä»¥åŠä½ æœ€æƒ³è¿›å…¥çš„åœ°æ–¹ã€‚ä¹Ÿè®¸å®‡å®™ä¼šå¬åˆ°ï¼Œæœºä¼šå°±ä¼šå¯¹ä½ æœ‰åˆ©ï¼    æäº¤äºº    /u/issyonibba   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</guid>
      <pubDate>Sun, 02 Feb 2025 19:47:59 GMT</pubDate>
    </item>
    <item>
      <title>â€œè¿ˆå‘é€šç”¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ â€ï¼ŒFujimoto ç­‰äººï¼Œ2025 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</guid>
      <pubDate>Sun, 02 Feb 2025 12:02:03 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘å¯¹å­¦ä¹  RL çš„å»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</link>
      <description><![CDATA[æˆ‘è¯»è¿‡ Sutton å’Œ Barto çš„ä¹¦ï¼Œæœ‰æ—¶æˆ‘å‘ç°å¾ˆéš¾ç†è§£å…¶ä¸­çš„ä¸€äº›æ¦‚å¿µã€‚ç„¶åï¼Œæˆ‘å¼€å§‹æ¢ç´¢è¿™ä¸ªèµ„æºã€‚ç°åœ¨ï¼Œæˆ‘çœŸæ­£ç†è§£äº†ä»·å€¼è¿­ä»£å’Œå…¶ä»–åŸºæœ¬æ¦‚å¿µèƒŒåçš„å«ä¹‰ã€‚æˆ‘è®¤ä¸ºè¿™æœ¬ä¹¦åº”è¯¥åœ¨ Sutton å’Œ Barto çš„ä¹¦ä¹‹å‰æˆ–åŒæ—¶é˜…è¯»ã€‚è¿™çœŸæ˜¯ä¸€æœ¬å¾ˆæ£’çš„ä¹¦ï¼    æäº¤äºº    /u/demirbey05   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</guid>
      <pubDate>Sun, 02 Feb 2025 07:59:42 GMT</pubDate>
    </item>
    </channel>
</rss>