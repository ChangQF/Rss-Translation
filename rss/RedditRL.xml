<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 26 May 2024 06:18:59 GMT</lastBuildDate>
    <item>
      <title>最优随机策略是否存在？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</link>
      <description><![CDATA[我知道在 MDP 中总是存在唯一的最优确定性策略。对于最优随机策略也存在这样的说法吗？是否总是存在唯一的最优随机策略？它能比最优确定性策略更好吗？我想我不太明白。 谢谢！   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</guid>
      <pubDate>Sun, 26 May 2024 06:07:26 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的矩阵（强化学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</link>
      <description><![CDATA[如果我想要一个代理显示 4 个空间，其中每个空间有 10 个组件，每个组件有 3 个变量。为观察空间定义一个矩阵是否更好？因为这告诉我做 ChatGPT     提交人    /u/Gullible_Capital_146   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</guid>
      <pubDate>Sun, 26 May 2024 04:02:53 GMT</pubDate>
    </item>
    <item>
      <title>“静息大脑标签记忆中的电‘涟漪’用于存储”：体验重播机制和选择睡眠期间优先重播的点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</guid>
      <pubDate>Sat, 25 May 2024 21:48:03 GMT</pubDate>
    </item>
    <item>
      <title>部分循环观察空间的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</link>
      <description><![CDATA[假设我有一个环境，其中代理必须在 2D 平面上移动来收集硬币，硬币的数量各不相同。代理可以在四个基本方向中的任何一个方向上加速，并在收集硬币时获得 1 的奖励，然后将其从环境中移除。观察空间如下所示： Agent x Agent y Agent vx Agent vy [Coin x, Coin y] * 硬币数量 我的假设是处理这个问题的方法是使用变压器 - 使用前馈网络对固定组件的表示进行编码，然后是每个非固定组件之一（我还可以使用变压器来生成每种类型的固定长度编码对象并将其与固定组件连接起来，但直观地将所有内容放入同一个变压器中应该效果更好，因为上下文使变压器变得有用）。本质上，这意味着编写一个自定义状态空间编码器，我认为两个大库（Stable Baselines 和 Rllib）都支持。伪代码如下所示： defencode(obs): generic, coin = obs # a 1 x 4 np array and a k x 2 np array coin_emb = self.coin_embed(c) # a前馈层映射 2 到嵌入维度 gen_emb = self.general_embed(general) # 4 --&gt; emb_dim编码= self.encoder(torch.stack(gen_emb,coins_emb))#torch.nn.TransformerEncoder，接受Nxemb_dim输入，产生Nxh_dim输出encoded=encoded[0]#BERT将输出用于特殊的[CLS ] 标记作为其固定长度输出。在这里，我们使用输出作为开始标记。 returnied def policy(obs): return self.policy_net(encode(obs)) def value(obs): return self.value_net(encode(obs))  我记得OpenAI的隐藏并寻求环境使用池化（特征编码器之后的 IIRC 最大池化）而不是变压器，但那是不久前的事了。无论如何，我的问题的核心是是否有人对实现这种自定义观察网络时使用的最佳堆栈有建议。如果有人见过这样的项目（越新越好 - 语法似乎像季节一样变化），我也非常感谢 github 链接。 谢谢！   由   提交/u/Dry-Sock7131  /u/Dry-Sock7131 reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</guid>
      <pubDate>Sat, 25 May 2024 16:41:35 GMT</pubDate>
    </item>
    <item>
      <title>教人形机器人用头拍球的教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</link>
      <description><![CDATA[您好，刚刚在 github 上发布了一个新教程 - https ://github.com/goncalog/ai-robotics。您的反馈会很棒！   由   提交/u/goncalogordo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</guid>
      <pubDate>Sat, 25 May 2024 16:36:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自定义环境引擎</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</link>
      <description><![CDATA[我目前正在尝试使用 Isaac sim 构建用于强化学习的自定义环境。我已经构建了模型，但我不知道如何将其导入 VS Code 以便我真正对环境进行编程，而且我找不到任何相关教程。我也想知道我是否应该使用 mojuco 代替？但我真的不知道如何用它创建模型。   由   提交/u/Teaser_404  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</guid>
      <pubDate>Sat, 25 May 2024 11:39:40 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习结果不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</guid>
      <pubDate>Sat, 25 May 2024 11:32:13 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND（扩散作为环境梦想的模型）是在扩散世界模型中训练的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</link>
      <description><![CDATA[    /u/clumma   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</guid>
      <pubDate>Fri, 24 May 2024 23:02:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Airsim 稳定基线3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czcjab/stable_baselines3_with_airsim/</link>
      <description><![CDATA[您好！我正在尝试在 AirSim 中制作一架无人机，以便在从一个点移动到另一个点时避开物体。我修改了该示例以满足我的需要。我使用来自 Stable Baselines3 和 Gymnasium 的 PPO 对其进行训练。问题是，一集的时间太长了。因此，我无法像在其他环境中那样提供足够的时间步长。我是 RL 的新手。请帮我解决这个问题。   由   提交 /u/Sandy_The_Adventurer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czcjab/stable_baselines3_with_airsim/</guid>
      <pubDate>Fri, 24 May 2024 04:34:01 GMT</pubDate>
    </item>
    <item>
      <title>MDP 已经过时了吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyz491/is_mdp_getting_obsolete/</link>
      <description><![CDATA[ 由   提交/u/SubstantialBed9917  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyz491/is_mdp_getting_obsolete/</guid>
      <pubDate>Thu, 23 May 2024 17:55:46 GMT</pubDate>
    </item>
    <item>
      <title>“Vernor Vinge 小说《真实姓名》的后记”，Minsky 1984（偏好学习和安全代理的挑战）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</guid>
      <pubDate>Thu, 23 May 2024 16:23:56 GMT</pubDate>
    </item>
    <item>
      <title>MDP：为输掉迄今为止在游戏中积累的财富设定奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywuda/mdp_set_reward_for_losing_what_you_have/</link>
      <description><![CDATA[假设我有一个游戏，开始时奖励为 0。我可以采取以下两种行动之一：  玩：一个随机过程，我要么获得金钱，要么失去迄今为止的奖励  离开：退出游戏并保留我累积的奖励。   我正在尝试将此游戏建模为具有“开始”状态和“结束”状态的 MDP，其中结束状态的值为 0。开始”，您可以执行“播放”或“离开”操作。在“结束”时，游戏结束——无需采取任何行动。  我试图弄清楚两件事：  如何为我失去累积奖励的随机过程的结果设置奖励。  如何为随机过程的结果设置奖励。 p&gt; 如何设置戒烟奖励。   对于 1： 如果“start”的值为 V(in)，我认为对于我选择“play”的结果，失去我累积的奖励，我可以用“-V(in)”来代表奖励。  Q(in, play) 的方程如下所示：  sum(p(lose)*(-V(in) + V(end)) + p(win )*(R + V(in)));  对于 2： 对于 Q(in, quit) --&gt;最终，奖励为V(in)。这样，如果您退出，Q(in, quit) 始终等于您开始时的值。  我的结果似乎与我的目标不符，而且我似乎无法找出我的方法的问题。我正在考虑我的方法（1）可能不准确，因为V（in）并不代表我累积的奖励，而是代表我累积奖励的预期值。但我的头脑中并没有很好地具体化这一点。  我在 MDP 中设置奖励的方式是否存在任何明显的问题？或者甚至是我的整个 MDP 中的状态/操作？    由   提交 /u/Squamply   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywuda/mdp_set_reward_for_losing_what_you_have/</guid>
      <pubDate>Thu, 23 May 2024 16:21:02 GMT</pubDate>
    </item>
    <item>
      <title>Cartpole 返回奇怪的东西。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</link>
      <description><![CDATA[我正在从头开始制作一个 PPO 代理（没有 Torch，没有 TF），它进展顺利，直到 env 突然返回一个维度为 5 的二维列表， 4而不是4，经过一番调试后，我发现这可能不是我的错，因为我没有对回报进行分配或执行任何操作，它只是在随机时间范围内发生并破坏了我的整个事情。有人知道为什么吗？   由   提交 /u/Mehcoder1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</guid>
      <pubDate>Thu, 23 May 2024 12:10:48 GMT</pubDate>
    </item>
    <item>
      <title>代表萨顿和巴托的“杰克的汽车租赁”问题的动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyib25/representing_the_dynamics_in_sutton_and_bartos/</link>
      <description><![CDATA[大家好！我正在学习 Sutton 和 Barto，并已完成有关动态规划的第 4 章。本章中的一个早期示例涉及一家汽车租赁公司（问题文本此处截图供参考）。作者使用这个玩具问题来说明策略迭代。 书中介绍的策略迭代是一个相当简单的算法。我陷入困境的是以编程方式表示问题的动态，即函数 p(s&#39;, r|s,a)。我不是在问“我如何计算出这些概率”，而是在问“我如何计算出这些概率”。而是“如何将它们存储在代码中以便于使用？” 对于本书中前面的示例，在处理值函数时，我通常会编写一个类似  的函数p&gt; defdynamic(s, a): “”“返回一个类似 [(s&#39;, r, p)] 的列表，其中 s&#39;是下一个状态，r 是观察到的奖励，p 是从状态 s&quot;&quot;&quot; 开始采取操作 a 后观察到 r 并最终达到状态 s&#39; 的概率 这使得通过动态（s，a）中的 sp，r，p_spr_sa 计算可能观察到的奖励和可能的下一个状态的总和变得非常方便： 此函数有效“包含”。环境，因为它决定了环境将如何对代理的操作做出反应。 对于汽车租赁问题，我正在努力以这种格式表示动态。事情比前面的示例更复杂，因为从 (s,a) 到 (s&#39;, r, p) 的逻辑映射要复杂得多。如果我想像我一直在做的那样实现这个功能，我想我可以，但是我想知道是否有人对如何更好地实现这一点有建议。任何关于动力学的这种表示是如何“典型地”的想法都可以理解。完成了吗？非常感谢！   由   提交/u/GorillaManStan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyib25/representing_the_dynamics_in_sutton_and_bartos/</guid>
      <pubDate>Thu, 23 May 2024 02:38:28 GMT</pubDate>
    </item>
    <item>
      <title>基于强化学习的八旋翼飞行器控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cycb22/control_of_an_octoplcopter_based_on_rl/</link>
      <description><![CDATA[大家好，我正在写我的硕士论文，其中涉及使用 Simulink 和 MATLAB 控制同轴八轴飞行器降落在移动目标上。我的系统有 20 个观测值并生成 3 个动作：推力、x 输入和 y 输入。由于我有两个 PD 控制器，并且将输入添加到 Ux 和 Uy 以控制级联控制器设置中的横滚和俯仰，因此复杂性增加。我确信我的模型是正确的，但我已经努力了两周才让代理收敛。它始终陷入 Q 值恒定为正的次优策略（我使用的是 DDPG）。尽管尝试了多次奖励函数修改以使无人机跟踪轨迹，但每一集的奖励仍然非常负面并且没有改善。考虑到大量的观察和系统的复杂性，这项工作是否可行？或者我应该考虑简化环境？任何建议或见解将不胜感激。谢谢   由   提交 /u/OkFig243   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cycb22/control_of_an_octoplcopter_based_on_rl/</guid>
      <pubDate>Wed, 22 May 2024 21:47:23 GMT</pubDate>
    </item>
    </channel>
</rss>