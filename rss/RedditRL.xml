<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 23 Feb 2024 06:18:09 GMT</lastBuildDate>
    <item>
      <title>基于 JAX 的 SAC 迭代未训练，不确定我在哪里错过了 bug</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1axt3kr/jaxbased_sac_iteration_not_training_unsure_where/</link>
      <description><![CDATA[大家好，我带着另一个 JAX 问题回来了。这一次，我重新设计了之前版本的基于 JAX 的 SAC，使其与 PureJAXRL 格式类似，其中整个算法包含在一个函数中，然后可以对该函数进行 jitted、vmapped 等操作。 这是我到目前为止所拥有的：https://pastes.io/xlyddbpwb1（主要代码），https://pastes.io/o2jagqxrhi（简单重播缓冲区）。  我目前遇到的问题是它没有学习。我使用与之前版本的 JAX SAC 完全相同的损失计算，并且经过正确训练，因此我知道其中的数学是正确的。此版本的算法已上传到此处进行比较： https://pastes.io/qq3mia60gi 我还使用 JAX 重新实现了该代码我在这段代码中使用了基于重播缓冲区，并且似乎也训练得很好。  我有一种感觉，在使用 lax.scan 或 lax.cond 处理条件时可能会遇到错误。其他人能看一下我可能缺少什么吗？我已经盯着这个有一段时间了，我确信我可能错过了一些简单的东西。预先感谢您的帮助！   由   提交 /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1axt3kr/jaxbased_sac_iteration_not_training_unsure_where/</guid>
      <pubDate>Fri, 23 Feb 2024 05:50:37 GMT</pubDate>
    </item>
    <item>
      <title>行动的价值和奖励之间有什么区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1axcqs8/whats_the_difference_between_the_value_of_an/</link>
      <description><![CDATA[跟随 Sutton 和 Barto，他们将动作“a”的值定义为选择“a”时奖励的数学期望，但直觉上它们不是一样的吗？如果我试图估计一个动作的价值，那么我试图估计该动作的奖励，我在这里缺少什么吗？    ;由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1axcqs8/whats_the_difference_between_the_value_of_an/</guid>
      <pubDate>Thu, 22 Feb 2024 17:51:49 GMT</pubDate>
    </item>
    <item>
      <title>2024 年学习强化学习的最佳书籍 -</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ax7bvg/best_books_to_learn_reinforcement_learning_in_2024/</link>
      <description><![CDATA[       由   提交 /u/Sreeravan   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ax7bvg/best_books_to_learn_reinforcement_learning_in_2024/</guid>
      <pubDate>Thu, 22 Feb 2024 14:12:12 GMT</pubDate>
    </item>
    <item>
      <title>在 A3C 实现中异步使用时，价值网络会出现分歧</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ax0ic9/value_network_diverges_when_used_asynchronously/</link>
      <description><![CDATA[我使用 CartPole-v1 环境。我构建了一个用于 VPG 和 A3C 的价值网络。在 VPG 中，它按预期工作。在 A3C 中，我创建了一个 work 方法，其中有 8 个工作线程在 pytorch/python 的多处理库的帮助下异步工作。 workers = [mp.Process(target=self.work, args=(rank,)) 表示范围内的排名(self.n_workers)] [w.start() 表示工人中的 w]; [w.join() for w in Workers]  我将类中的价值网络作为函数 self.value_model_fn = value_model_fn 并在 work 方法我创建本地价值网络来与 local_value_model = self.value_model_fn(nS) 一起使用，其中 nS 是状态大小。我对策略网络做了完全相同的事情，但策略网络在 VPG 和 A3C 中都按预期工作。我还加载了在 train 函数中初始化的共享策略和值模型，如下所示：  local_policy_model = self.policy_model_fn(nS, nA) local_policy_model.load_state_dict( self.shared_policy_model.state_dict()) local_value_model = self.value_model_fn(nS) local_value_model.load_state_dict(self.shared_value_model.state_dict())  当我运行 local_value_model(state)&lt; /code&gt; 代码进入永无止境的循环。 我尝试在 work 方法之外运行价值网络，它工作得很好。我尝试在不加载 shared_value_network 的情况下运行它，我还尝试在 local_policy_network 之前运行它，但当我在 work 中运行它时它不起作用代码&gt;方法。  它应该做的是返回以下形式的值：tensor([[0.0214]], grad_fn= 但它会永远循环。我不这样做真的知道要采取什么其他方向，我认为网络实现中不存在问题，因为它在工作方法和 VPG 实现之外工作得很好，所以我的猜测是多处理方面存在一些问题，使其发散。    提交者    /u/yuyututuyutu   [链接]   ; [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ax0ic9/value_network_diverges_when_used_asynchronously/</guid>
      <pubDate>Thu, 22 Feb 2024 07:24:31 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶中的 RL 和 SL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ax03wr/rl_and_sl_in_autonomous_driving/</link>
      <description><![CDATA[      我根据自己的观察和知识做出了这个。这是自动驾驶中应用 RL 和 SL 算法的高级概述，主要用于决策。接下来我将尝试涵盖 RL 和 SL 中使用的按键和端子。如果有错误请告诉我。    由   提交/u/Mysterious-Care1358  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ax03wr/rl_and_sl_in_autonomous_driving/</guid>
      <pubDate>Thu, 22 Feb 2024 06:59:42 GMT</pubDate>
    </item>
    <item>
      <title>机器人学习的下一个研究主题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1awhoy6/next_research_topic_for_robot_learning/</link>
      <description><![CDATA[经过长时间的强化学习研究后，我对机器人学习研究下一步该做什么感到困惑。  我对 RL 和 MARL 进行了一系列研究。现在我认为将强化学习研究转向模仿学习对我来说是明智的，因为这在现实中更可靠。 如果没有建立世界模型，我无法找到现实中机器人的强化学习。或者也许LLM的模仿学习可能是机器人学习的下一个方向？    由   提交/u/Tight-Ad789  /u/Tight-Ad789  reddit.com/r/reinforcementlearning/comments/1awhoy6/next_research_topic_for_robot_learning/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1awhoy6/next_research_topic_for_robot_learning/</guid>
      <pubDate>Wed, 21 Feb 2024 17:22:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习播放列表</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1awg1rz/reinforcement_learning_playlist/</link>
      <description><![CDATA[大家好，查看我的强化学习播放列表，其中包含 28 个教程，涵盖基础知识和高级主题，示例包括 MAB、Contextual MAB、Monte Carlo、SARSA、 Q Learning、A2C、DDPG、REINFORCE、PPO、RLHF、Multi-Agent 算法和一些 OpenAI 健身房示例 https://youtube.com/playlist?list=PLnH2pfPCPZsIpNtTIwKQZeDerJEz2ccEV&amp;si=KLZ3FYlBWkAHI kxx&lt; /p&gt;   由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1awg1rz/reinforcement_learning_playlist/</guid>
      <pubDate>Wed, 21 Feb 2024 16:17:43 GMT</pubDate>
    </item>
    <item>
      <title>实践学习的图书馆选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1awc6rh/library_choice_for_practical_learning/</link>
      <description><![CDATA[嗨， 我打算开始深度强化学习，我想知道今天有哪些库组合。我找到了一些教程，但它们似乎已经过时了，使用 keras 2 因为 V3 已经可用。 由于我出于专业原因开始使用（而且我已经有编程经验），我想构建弦乐基础知识。因此，我并不害怕使用不太“用户友好”的东西。如果 API 从长远来看能够得到更好的维护。 kerasV3 + tf 和gym 和/或gymnasium 是一个不错的起始选择吗？例如，我发现很多使用 keras-rl2 的教程，如果我理解独立于 keras 正确维护，那么使用这个库而不是内置的 keras 强化学习算法到底有什么意义？ 此外，我没有找到 keras V3 的等效项。是因为 keras V3 使 keras-rl 库过时了还是还有其他原因？   由   提交/u/dougdoug110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1awc6rh/library_choice_for_practical_learning/</guid>
      <pubDate>Wed, 21 Feb 2024 13:36:03 GMT</pubDate>
    </item>
    <item>
      <title>改变体育馆中的双足步行者地形</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1awc0tt/changing_the_bipedal_walker_terrain_in_gymnasium/</link>
      <description><![CDATA[大家好， 我正在 python 中的 Bipedal walker 体育馆环境中尝试课程学习。 我正在努力编辑步行者行走的地形，即坑、树桩、楼梯等。 例如，我想仅在坑等上训练步行者 有谁知道这方面有什么好的资源吗？ 提前致谢。   由   提交/u/Hunter0708  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1awc0tt/changing_the_bipedal_walker_terrain_in_gymnasium/</guid>
      <pubDate>Wed, 21 Feb 2024 13:27:54 GMT</pubDate>
    </item>
    <item>
      <title>与 RL 相关的研究主题或论文？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw8d6t/research_topics_or_papers_related_to_rl/</link>
      <description><![CDATA[嘿，我即将完成数学硕士学位，需要为我的论文选择一个主题。我的研究重点是应用统计学和机器学习。最近，我开始接触强化学习，并且一直在考虑就此撰写硕士论文。我不太害怕编码，但我绝对想做理论研究。 由于我对 RL 的投入不是太深，所以我确实可以在我的论文中使用一些想法或论文建议。如果有任何建议，我将非常感激！ （最好是 RL）谢谢！   由   提交/u/d-eighties  /u/d-eighties  reddit.com/r/reinforcementlearning/comments/1aw8d6t/research_topics_or_papers_lated_to_rl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw8d6t/research_topics_or_papers_related_to_rl/</guid>
      <pubDate>Wed, 21 Feb 2024 09:53:34 GMT</pubDate>
    </item>
    <item>
      <title>有什么技巧可以帮助钉孔任务收敛吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw8399/is_there_any_trick_to_help_peginhole_tasks/</link>
      <description><![CDATA[嗨！ 我从一个简单的孔洞任务开始，但无论使用密集还是使用，都很难收敛稀疏奖励。 对于稀疏奖励，这里使用了随机目标位置的技巧论文有助于收敛。是否有任何智能技巧可用于帮助融合 peg-in-hole 任务？ 顺便说一句，是否有任何关于 peg-in-hole 的推荐开源存储库任务？ 任何帮助将不胜感激！谢谢！   由   提交/u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw8399/is_there_any_trick_to_help_peginhole_tasks/</guid>
      <pubDate>Wed, 21 Feb 2024 09:34:22 GMT</pubDate>
    </item>
    <item>
      <title>失去动力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw6mn4/losing_motivation/</link>
      <description><![CDATA[也许我只是反应过度，或者我太弱了。但请听我说完。在过去的几个月里，我一直在尝试自学强化学习。我正在学习 Coursera RL 专业课程的第二门课程，萨顿 &amp; 课程的第 8 章。巴托。起初我的一些朋友也想和我一起学习，但他们都不再这样做了。我还在做一份全职工作，这几乎耗尽了我所有的精力。我寻找导师，但没有找到。如今每个人都热衷于计算机视觉或 NLP。此外，很多人都说强化学习没有未来。所有这些加在一起真是太累了。 我真的不知道我在这里要找什么。如果你能分享你的旅程，那将会有帮助。另外，如果您能指导我（即使是一点点时间），我将永远感激不已。   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw6mn4/losing_motivation/</guid>
      <pubDate>Wed, 21 Feb 2024 07:55:27 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助理解研究论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw1ce3/need_help_with_understanding_a_research_paper/</link>
      <description><![CDATA[我需要帮助理解 DeepMind 在 2020 年发表的一篇论文，论文的名字是，“通过考虑未来的任务来避免副作用”。  我很难理解本文中讨论的几个概念。  如果有人愿意帮助我，请与我联系，提前致谢。   由   提交 /u/Donald-the-dramaduck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw1ce3/need_help_with_understanding_a_research_paper/</guid>
      <pubDate>Wed, 21 Feb 2024 03:06:19 GMT</pubDate>
    </item>
    <item>
      <title>训练 MuZero</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avzdmq/training_muzero/</link>
      <description><![CDATA[是否有人尝试使用此代码 - https: //pypi.org/project/muzero-baseline/ ? 我将其安装在桌面上并尝试训练它玩井字游戏 muzero = MuZero(tictactoe.Game, tictactoe.MuZeroConfig()) muzero.train() muzero .test(render=True) 运行2天后没有产生任何结果。 我注意到CPU的使用率很低，GPU的使用率也很低。  ​ ​   由   提交 /u/Competitive-Elk4319    reddit.com/r/reinforcementlearning/comments/1avzdmq/training_muzero/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avzdmq/training_muzero/</guid>
      <pubDate>Wed, 21 Feb 2024 01:34:55 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和统计的交叉点在哪里？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avr3kx/where_do_rl_and_statistics_intersect/</link>
      <description><![CDATA[我是一名统计研究生，对 RL/DRL 感兴趣，我希望最大限度地从学位中获益并专注于统计主题（和数值方法）如果可能的话，这将使我为 RL/DL 的博士课程做好准备，我需要特别注意哪些主题？   由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avr3kx/where_do_rl_and_statistics_intersect/</guid>
      <pubDate>Tue, 20 Feb 2024 19:57:12 GMT</pubDate>
    </item>
    </channel>
</rss>