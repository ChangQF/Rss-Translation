<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 17 Feb 2025 12:34:45 GMT</lastBuildDate>
    <item>
      <title>微调从政策方法到政策的政策是有意义的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irhln4/does_it_make_sense_to_finetune_a_policy_from_an/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我的问题是，对于我的设置，一个步骤需要一段时间，因此我想减少训练过程中所需步骤的数量。首先训练单盘方法，然后将其转移到改善发现的基线的政策方法上是否有意义？加载策略网络是否足够（例如，从SAC到PPO）。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aure20     [link]   ＆＃32;   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irhln4/does_it_make_sense_to_finetune_a_policy_from_an/</guid>
      <pubDate>Mon, 17 Feb 2025 11:16:24 GMT</pubDate>
    </item>
    <item>
      <title>rl属于机器人技术</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irgnmc/rl_spplied_to_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是一名机器人软件工程师，具有多年的运动计划经验以及对自动驾驶汽车的轨迹跟踪的某些经验。我希望更深入地研究RL，并且通常适用于机器人技术，尤其是在计划和障碍/碰撞等领域。我对ML和DL的早期工作经验适用于视觉以及对流行RL算法的一些知识。任何建议，资源/课程/书籍或项目创意都将不胜感激！  ps：我并不是真正希望学习适用于机器人技术的ML。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/karthi_wolf     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irgnmc/rl_spplied_to_robotics/</guid>
      <pubDate>Mon, 17 Feb 2025 10:10:29 GMT</pubDate>
    </item>
    <item>
      <title>在研究项目中需要帮助学习强化学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1irg4n7/need_help_in_learning_reinforcement_learning_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我有数学背景，目前正在从事供应链风险管理。在审查文献时，我发现了在加强学习（RL）供应链管理中应用的研究差距。我还发现了一个可能有用的数值数据集。 我试图说服我的主管我们可以使用此数据集在供应链管理中演示我们的RL框架。但是，我对RL是否需要实施数据感到困惑。我在这里听起来可能没有经验 - 我是我，我是 - 这就是为什么我寻求帮助。 我的想法是通过模拟供应链环境，然后将数据集使用到供应链环境中，然后将数据集使用到验证或证明我们的结果。但是，我不确定哪种RL算法最合适。 有人可以指导我从哪里开始学习以及如何将RL应用于此问题吗？从我的理解来看，RL与传统的机器学习算法有所不同，并且不需要先前存在的数据进行培训。 道歉，如果有任何意义，请提前感谢您的帮助！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/midder-coat-388     link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1irg4n7/need_help_in_learning_reinforcement_learning_for/</guid>
      <pubDate>Mon, 17 Feb 2025 09:32:23 GMT</pubDate>
    </item>
    <item>
      <title>通过平行GPU培训，用于增强学习的最佳物理引擎？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ireyww/best_physics_engine_for_reinforcement_learning/</link>
      <description><![CDATA[在深度学习框架以及增强学习 我已经探索了几种物理引擎，包括Pybullet，Mujoco，Mujoco，Isaac Gym，Gazebo，Brax和Gymnasium。 我的主要问题是：&lt; /p&gt;  支持的碰撞类型（例如，使用mano的凹面网格碰撞） 如果您有经验这些引擎中的任何一个，我都会感谢您听到您的见解。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/uny_way2779     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ireyww/best_physics_engine_for_reinforcement_learning/</guid>
      <pubDate>Mon, 17 Feb 2025 08:08:26 GMT</pubDate>
    </item>
    <item>
      <title>平均场比赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ir34hl/meanfield_games/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好RL爱好者！ 我正在伸出手来收集有趣的开放线程或在平均野外游戏和多人中的研究问题 - 代理增强学习。  分享您的想法！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/usase_hall_6610      [link]  ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ir34hl/meanfield_games/</guid>
      <pubDate>Sun, 16 Feb 2025 21:28:35 GMT</pubDate>
    </item>
    <item>
      <title>RL项目的医疗环境。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ir312w/medical_environment_for_a_rl_project/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在寻找一个有趣且相对容易的模拟环境，用于我的课程项目。它应该在医疗环境中（我想避免由于时间限制而自己创建一个人，而且la脚也是如此）。 目标是培训具有深Q学习算法的代理商，并有可能与其他算法进行比较。  是否有人在使用这种模拟方面有经验，并且可以推荐一个易于设置的模拟？  谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/nachtlicht_     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ir312w/medical_environment_for_a_rl_project/</guid>
      <pubDate>Sun, 16 Feb 2025 21:24:34 GMT</pubDate>
    </item>
    <item>
      <title>OpenSource项目贡献</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ir1ij0/opensource_project_to_contribute/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，  RL中有任何开源项目，以便我可以成为它的参与者并定期贡献？&lt;&lt;&lt; /p&gt; 任何线索都高度赞赏。 谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/commistigansion 566     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ir1ij0/opensource_project_to_contribute/</guid>
      <pubDate>Sun, 16 Feb 2025 20:20:05 GMT</pubDate>
    </item>
    <item>
      <title>硕士论文主题帮助marl</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iqwb9w/masters_thesis_topic_help_marl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi， 我正在完成主人的工作，我一直在试图找出论文的主题。我想在MARL上做论文，并且以前曾研究并实施了一些单一代理算法。 我一直在研究合作任务，尤其是在沟通和社会学习方面。问题是，我很难在这些领域找出一个可能适合师父级别的问题。我可以对此获得一些想法和建议吗？ 如果对一般研究有任何建议，这将不胜感激。 谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/naaffi     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iqwb9w/masters_thesis_topic_help_marl/</guid>
      <pubDate>Sun, 16 Feb 2025 16:44:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么RLHF中没有值函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iquzxv/why_is_there_no_value_function_in_rlhf/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在RLHF中，大多数论文似乎只关注奖励模型，而不是真正引入了传统RL中常见的价值函数。您认为这背后的理由是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/__ fivid__     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iquzxv/why_is_there_no_value_function_in_rlhf/</guid>
      <pubDate>Sun, 16 Feb 2025 15:47:04 GMT</pubDate>
    </item>
    <item>
      <title>迈向软件工程师LRM代理：紧急能力和强化学习 - 调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iqt03k/toward_software_engineer_lrm_agent_emergent/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/ivan_digital     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iqt03k/toward_software_engineer_lrm_agent_emergent/</guid>
      <pubDate>Sun, 16 Feb 2025 14:11:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个方程式错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iqokff/why_is_this_equation_wrong/</link>
      <description><![CDATA[      &lt; td&gt; &lt;！ -  sc_off-&gt;  我的胆量说我在这里写的第二个方程式是错误的，但我无法将其插入单词。您能帮我了解一下  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; &lt; //i.redd.it/lkw6vq8jygje1.jpeg&quot;&gt; link]  ＆＃32;   [commist]         &lt; /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iqokff/why_is_this_equation_wrong/</guid>
      <pubDate>Sun, 16 Feb 2025 09:22:11 GMT</pubDate>
    </item>
    <item>
      <title>亲社会的内在动机</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iql3h5/prosocial_intrinsic_motivation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我遇到了这篇关于制作优化爱心善良的AI的帖子，我想呼应他们的意图： https://wwwww.reddit.com/r/reinforeveresslearning/s/gmgxfbxw2e 因为这就是我们可以直接对更美好的世界进行优化的方式。如果它不是针对正确的目标，世界上所有的情报都不是好的。我要求此Subreddit上的人在AI上工作，该AI直接针对集体公用事业。我将用于此问题的框架是针对集体实用性问题的协作逆增强学习（CIRL）。试想一下，如果规范是在适用的任何RL部署之上添加亲社会固有驱动器，将会有多大的影响。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neuropyrox     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iql3h5/prosocial_intrinsic_motivation/</guid>
      <pubDate>Sun, 16 Feb 2025 05:22:08 GMT</pubDate>
    </item>
    <item>
      <title>帮助线性函数近似确定性策略梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iqi20i/help_with_linear_function_approximation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在对特定的应用程序区域应用不同的强化学习算法，但是我坚持如何使用确定性策略扩展线性函数近似方法梯度定理。我正在尝试实施COPDAC-GQ（Silver ET提出的算法兼容的非政策确定性的con-Critic-Critic-Critic-Critic-Critic-Critic-Critic）。 Al。，在他们的开创性DPG论文中，但在我看来，尺寸在方程式中没有奏效。特别是，theta重量矢量更新方程。  功能（或状态）的数量为n。动作维度的数量为m。有3个重量向量，theta，w和v。theta是nxm，w和v是nx1。作者说的“按照惯例”是 jacobian 矩阵，因此每列是策略相对于相对于相对于策略的DTH动作维度的梯度∇θ[μθ（s）d策略参数θ。这不是经典的雅各布矩阵，但是我认为如果您删除“雅各布”，则该陈述是正确的。从声明。我已经解释了策略函数∇θμθ（s）的梯度，为nxm矩阵，使每列是MTH动作维度的策略函数的梯度，而部分衍生物则在mth中占据了每个theta重量Theta列。  这是问题所在的地方。在银纸中，它们定义了COPDAC-GQ算法中每个重量向量的更新步骤。除了    theta_next = theta_current + alpha*∇θμθ（s）*（∇θμθ（s）&#39;*w_current）的所有尺寸外，是转置操作员。  我缺少什么？ theta需要是nxm和alpha*∇θμθ（s）*（∇θμθ（s）&#39;*w_current）的作用为Nx1。   d。 Silver，G。Lever，N。Heess，T。Degris，D。Wierstra和M. Riedmiller，“确定性政策梯度算法”，在第31届机器学习国际会议上，PMLR，PMLR，PMLR， 2014年1月，第387–395页。访问：2024年11月5日。[在线]。可用： https://proceedings.mlr.press/v32/silver14.html  &gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ecelectrybear45     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iqi20i/help_with_linear_function_approximation/</guid>
      <pubDate>Sun, 16 Feb 2025 02:25:42 GMT</pubDate>
    </item>
    <item>
      <title>可解释的RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iq37rp/explainable_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用RL基于 simglucose 。我想在使用Shap或Policy Explantion测试的算法中添加解释性。我一直在阅读该领域的当前研究论文，但是我可以从任何特定的角度开始吗？我可以尝试实施基本的东西，以了解最新论文中使用的大数学。我想知道我们甚至如何确切地使RL可以解释，寻找哪些功能等。  ps：我是ECE的最后一年。我读过Barto和Sutton，观看了David Silver的UCL讲座，阅读了一本关于RL的数学理解的书。考虑到解释性，我知道Shap是如何工作的，并且我是Christoph Molnar的可解释的机器学习书（很好）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/uickulweeber99     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iq37rp/explainable_rl/</guid>
      <pubDate>Sat, 15 Feb 2025 15:03:46 GMT</pubDate>
    </item>
    <item>
      <title>“重新评估不完美信息游戏的策略梯度方法”，Rudolph等。 2025年（PPO竞争不完美的INFO游戏的定制算法）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ipzsqe/reevaluating_policy_gradient_methods_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  摘要：“在过去的十年中，受到对抗性不完美信息游戏中幼稚的自我扮演深度强化学习（DRL）的假定失败的动机，研究人员已经根据虚拟游戏（FP），双重甲骨文（DO）和反事实遗憾最小化（CFR）开发了许多DRL算法。鉴于磁性镜下降算法的最新结果，我们假设PPO（例如PPO）具有更简单的通用策略梯度方法具有竞争力或优于这些FP，DO和基于CFR的DRL方法。为了促进该假设的解决，我们针对四个大型游戏实施并发布了第一个可访问的确切可剥削性计算。使用这些游戏，我们对不完美的信息游戏进行了DRL算法的有史以来最大的可利用性比较。超过5600次培训运行，FP，DO和基于CFR的方法无法超越通用策略梯度方法。”   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mothmatic     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ipzsqe/reevaluating_policy_gradient_methods_for/</guid>
      <pubDate>Sat, 15 Feb 2025 11:53:25 GMT</pubDate>
    </item>
    </channel>
</rss>