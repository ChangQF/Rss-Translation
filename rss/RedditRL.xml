<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Wed, 05 Feb 2025 06:23:43 GMT</lastBuildDate>
    <item>
      <title>æ€è€ƒæ•°æ®ï¼šæˆ‘æƒ³çŸ¥é“æˆ‘çš„æƒ³æ³•æ˜¯å¦å¯è¡Œã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</link>
      <description><![CDATA[ä½ å¥½ã€‚æˆ‘å¾ˆå¿«å°±è¦å¼€å§‹å­¦ä¹ è®¡ç®—æœºç§‘å­¦äº†ï¼ˆä»Šå¹´ç§‹å¤©æˆ–æ˜å¹´ç§‹å¤©ï¼Œå–å†³äºæˆ‘çš„å¤§å­¦ä½•æ—¶å…è®¸æˆ‘é€‰æ‹©å¹¶ä¸“æ³¨äºä¸€ä¸ªä¸“ä¸šï¼‰ï¼Œä½†æˆ‘æƒ³åœ¨äººå·¥æ™ºèƒ½æœ€è¿·äººçš„éƒ¨åˆ†ä¹‹ä¸€ï¼šå¼ºåŒ–å­¦ä¹ æ–¹é¢å–å¾—çªç ´ã€‚ æˆ‘çš„è®¡åˆ’ï¼šåˆ¶ä½œå¤šä¸ªå¯ä»¥å­¦ä¹ ç©æ¸¸æˆçš„äººå·¥æ™ºèƒ½ï¼Œç„¶åå°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ï¼Œè¿™æ ·æ„Ÿè§‰å°±åƒä¸€ä¸ªäººå·¥æ™ºèƒ½ã€‚ä½†è¿™è¿˜ä¸æ˜¯å…¨éƒ¨ã€‚é¦–å…ˆï¼Œå®ƒä¼šä»ä¸€ä¸ªæ¸¸æˆå¼€å§‹ï¼Œç„¶åæˆ‘å°†å†…å­˜å¤åˆ¶å¹¶ç²˜è´´ï¼ˆå¹¶æœ€æœ‰å¯èƒ½å¯¹å…¶è¿›è¡Œä¸€äº›ä¿®æ”¹ï¼‰åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå®ƒå°†åœ¨å…¶ä¸­ç©å¦ä¸€ä¸ªæ¸¸æˆï¼Œè¿™æ ·å®ƒå°±å¯ä»¥åœ¨å·²ç»çŸ¥é“åŸºæœ¬æ§åˆ¶çš„æƒ…å†µä¸‹å¿«é€Ÿå¯åŠ¨ã€‚ä¸€æ®µæ—¶é—´åï¼Œæˆ‘ä¼šè®©å®ƒç©æ›´é«˜çº§çš„æ¸¸æˆï¼Œå¸Œæœ›å®ƒçŸ¥é“å¤§å¤šæ•°æ¸¸æˆéƒ½æœ‰ç±»ä¼¼çš„æ§åˆ¶ç»“æ„ã€‚ æœ€ç»ˆç›®æ ‡ï¼šæ‹¥æœ‰ä¸€ä¸ªå¯ä»¥ç©å¤šä¸ªæ¸¸æˆçš„å¤šç”¨é€”äººå·¥æ™ºèƒ½ï¼Œäº†è§£æ¸¸æˆæ— éšœç¢æŒ‡å—ï¼Œç„¶ååœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­æ‹†åˆ†æ— éšœç¢å®¡æŸ¥ã€‚å“¦ï¼Œæ˜¯çš„ï¼Œä¹Ÿè®¸å¯ä»¥ä½¿ç”¨è¯­è¨€æ¨¡å‹ä¸æˆ‘èŠå¤©ã€‚ åœ¨ç†æƒ³ä¸–ç•Œä¸­ï¼Œæˆ‘ä¼šä½¿ç”¨ç°æœ‰çš„ RL ä»£ç†ï¼ˆå½“ç„¶æ˜¯åœ¨å¼€å‘äººå‘˜çš„è®¸å¯ä¸‹ï¼‰æ¥å¸®åŠ©åŠ å¿«è¯¥è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ LLM ä¸å…¶èŠå¤©å¹¶è·å–ä»…ç©æ¸¸æˆçš„ AI æ— æ³•æä¾›çš„ä¿¡æ¯ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘æœ‰ä¸€å° MSI GF75 Thinï¼Œé…å¤‡ Intel i5-10300hã€NVIDIA GTX 1650ï¼ˆé…å¤‡ 4gh VRAMï¼‰å’Œ 32gb Ramã€‚æˆ‘è®¤ä¸ºå¾ˆå¤šéƒ½å¾ˆå¥½ï¼Œé™¤äº†æ˜¾å¡ï¼ˆæˆ‘è§‰å¾—å³ä½¿æ²¡æœ‰å°è¯•åˆ¶ä½œ AI ä¹Ÿç¼ºä¹æ˜¾å¡ï¼‰ï¼Œæ‰€ä»¥æˆ‘æ— æ³•ç”¨æˆ‘å½“å‰çš„è®¾ç½®åšå¾ˆå¤šäº‹æƒ…ã€‚ä½†è¿™æ˜¯æˆ‘æƒ³é•¿æœŸè€ƒè™‘çš„äº‹æƒ…ï¼Œå› ä¸ºæœ‰ä¸€å¤©å°†æˆ‘çš„æƒ³æ³•ä»˜è¯¸å®è·µçœŸçš„å¾ˆé…·ã€‚    æäº¤äºº    /u/Octo_Chara   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</guid>
      <pubDate>Wed, 05 Feb 2025 01:11:40 GMT</pubDate>
    </item>
    <item>
      <title>ç ”ç©¶å®ä¹ ç”Ÿ - æ¬§æ´²</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihsdnf/research_intern_europe/</link>
      <description><![CDATA[ä¸ç¡®å®šè¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæ­£ç¡®çš„å­ä¸»é¢˜ï¼Œä½†æˆ‘æƒ³çŸ¥é“å¦‚ä½•æ‰èƒ½åœ¨æ¬§æ´²æ‰¾åˆ° RL çš„ç ”ç©¶å®ä¹ ç”ŸèŒä½ã€‚æœ€å¥½æ˜¯å¾·å›½ã€‚æˆ‘ä¸ç¡®å®šå¦‚ä½•æ‰èƒ½æ‰¾åˆ°è¿™æ ·çš„èŒä½ï¼Œå› ä¸ºä»–ä»¬ä¸»è¦å®£ä¼ ä¸ºåšå£«èŒä½ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚æˆ‘çš„èƒŒæ™¯å¹¶ä¸å®Œå…¨åŒ¹é…ï¼Œæ‰€ä»¥æˆ‘å®æ„¿å…ˆåšå®ä¹ ç”Ÿï¼Œç„¶åå†è½¬ä¸ºåšå£«ã€‚ä½†æˆ‘åº”è¯¥åœ¨å“ªé‡Œå¯»æ‰¾ï¼Ÿæˆ‘å¿…é¡»ç»™å®éªŒå®¤å‘å†·é‚®ä»¶å—ï¼Ÿæˆ‘å¾ˆå°‘çœ‹åˆ°ä»»ä½•å…¬å¼€å®£å¸ƒçš„èŒä½ã€‚æˆ‘å¾ˆæ„Ÿæ¿€ä»»ä½•å»ºè®®ã€‚     æäº¤äºº    /u/ProfileSad6040   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihsdnf/research_intern_europe/</guid>
      <pubDate>Tue, 04 Feb 2025 21:01:13 GMT</pubDate>
    </item>
    <item>
      <title>å»ºç«‹è¿·ä½ æ³•å­¦ç¡•å£«</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihlj8d/building_a_mini_llm/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨è€ƒè™‘ä»å¤´å¼€å§‹æ„å»ºä¸€ä¸ªè¿·ä½  LLMã€‚å¦‚ä½•åˆ›å»ºä¸€ä¸ªç¯å¢ƒï¼Œè®©ä»£ç†æä¾›æ–‡æœ¬ä¿¡æ¯ï¼Œå¹¶å¸Œæœ›å®ƒä½¿ç”¨é˜…è¯»ã€æ€»ç»“å’Œå›ç­”é—®é¢˜ç­‰ 3 ä¸ªåŠ¨ä½œè¿›è¡Œå­¦ä¹      æäº¤äºº    /u/iInventor_0134   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihlj8d/building_a_mini_llm/</guid>
      <pubDate>Tue, 04 Feb 2025 16:23:55 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨ RLã€å†³ç­–æ™ºèƒ½åº”ç”¨æ–¹é¢æœ‰æ²¡æœ‰åšå£«å­¦ä½æœºä¼šï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</link>
      <description><![CDATA[æˆ‘æ˜¯ä¸€åå¤§å››æœ¬ç§‘ç”Ÿï¼Œæƒ³ç”³è¯· RL æˆ–å†³ç­–æ™ºèƒ½åº”ç”¨é¢†åŸŸçš„ç›´æ¥æ”»è¯»åšå£«å­¦ä½æœºä¼šã€‚  è™½ç„¶æˆ‘å·²ç»ç”³è¯·äº†ä¸€äº›å¤§å­¦ï¼Œä½†æˆ‘è§‰å¾—æˆ‘çš„æœºä¼šå¾ˆå°ã€‚æˆ‘å·²ç»åæ‚”å¾ˆä¹…äº†ï¼Œå› ä¸ºæˆ‘å»å¹´æ²¡æœ‰è·Ÿè¸ªç”³è¯·æˆ–çœ‹æ¸…æœºä¼šã€‚å¦‚æœä½ ä»¬å½“ä¸­æœ‰è°å¯¹ 2025 å¹´ä»å¼€æ”¾çš„ç›´æ¥æ”»è¯»åšå£«å­¦ä½è¯¾ç¨‹æœ‰æ‰€äº†è§£ï¼Œè¯·åœ¨è¿™ä¸ª subreddit ä¸­å‘Šè¯‰æˆ‘ğŸ™    æäº¤äºº    /u/Miserable_Ad2265   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</guid>
      <pubDate>Tue, 04 Feb 2025 14:59:07 GMT</pubDate>
    </item>
    <item>
      <title>ä½“è‚²é¦† ClipAction åŒ…è£…å™¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihhcbx/gymnasium_clipaction_wrapper/</link>
      <description><![CDATA[æŒ‰ç…§æ–‡æ¡£ï¼Œæœ‰äººèƒ½å¸®åŠ©æˆ‘ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨åŒ…è£…å™¨å action_space å˜æˆ Box(-inf, inf, (3,), float32) å—ï¼Ÿ    æäº¤äºº    /u/glitchyfingers3187   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihhcbx/gymnasium_clipaction_wrapper/</guid>
      <pubDate>Tue, 04 Feb 2025 13:13:27 GMT</pubDate>
    </item>
    <item>
      <title>PPO é™·å…¥å±€éƒ¨æœ€ä¼˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgtec/ppo_stuck_in_local_optima/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨åšä¸€ä¸ªå¾®ç”µç½‘é—®é¢˜ï¼Œæˆ‘ä¹‹å‰ç”¨ DQN å®Œæˆäº†è¿™ä¸ªé—®é¢˜ï¼Œç»“æœè¿˜ä¸é”™ã€‚ ç°åœ¨æˆ‘æ­£åœ¨ç”¨ PPO è§£å†³ç›¸åŒçš„ç¯å¢ƒï¼Œä½†ç»“æœæ¯” DQN é—®é¢˜å·®ï¼ˆåŸºçº¿æ¨¡å‹æ˜¯ MILPï¼‰ã€‚ PPO ä»£ç†æ­£åœ¨å­¦ä¹ ï¼Œä½†è¿˜ä¸å¤Ÿå¥½ï¼Œæˆ‘æ­£åœ¨åˆ†äº«è®­ç»ƒçš„å›¾ç‰‡ã€‚ https://imgur.com/a/GHHYmow MG é—®é¢˜æ˜¯åœ¨ä¸»ç”µç½‘ä»·æ ¼ä½æ—¶å¯¹ç”µæ± å……ç”µï¼Œåœ¨ä»·æ ¼ä½æ—¶æ”¾ç”µã€‚ åŠ¨ä½œç©ºé—´æ˜¯ 4 ä¸ªç”µæ± çš„å……ç”µ/æ”¾ç”µï¼ˆæˆ‘ç¨ååœ¨ç”µæ± ä¸­å°†å…¶ä½œä¸ºæ ‡å‡†åŒ–å½¢å¼ï¼Œæˆ‘å°†ä¹˜ä»¥ 2.5ï¼Œå³æœ€å¤§ ch/dischï¼‰æˆ–è€…æˆ‘åº”è¯¥åˆå§‹åŒ– -2.5 åˆ° 2.5 å¦‚æœæœ‰å¸®åŠ©ï¼Ÿ self.action_space = space.Box(low=-1, high=1, dtype=np.float32, shape=(4,))  ä¸ºäº†å°†å…¶ä¿æŒåœ¨ -1 å’Œ 1 ä¹‹é—´ï¼Œæˆ‘æ­£åœ¨é™åˆ¶ NN çš„å¹³å‡å€¼ï¼Œç„¶ååœ¨ -1 åˆ° 1 ä¹‹é—´å¯¹æ“ä½œè¿›è¡Œé‡‡æ ·ï¼Œä»¥ç¡®ä¿ç”µæ± å……ç”µ/æ”¾ç”µä¸ä¼šè¶…å‡ºå®ƒï¼Œä½¿ç”¨ä¸‹é¢åˆ†äº«çš„è¿™ç§æ–¹å¼ã€‚ mean = torch.tanh(mean) action = dist.sample()  action = torch.clip(action, -1, 1) è¿˜æœ‰ä¸€ä»¶äº‹ï¼Œæˆ‘å¯¹ä¸‹é¢åˆ†äº«çš„ M æ­£æ€åˆ†å¸ƒä½¿ç”¨å›ºå®šåæ–¹å·®ï¼Œæ‰€æœ‰æ“ä½œçš„åæ–¹å·®ä¸º 0.5ã€‚ dist = MultivariateNormal(mean, self.cov_mat) è¯·åˆ†äº«æ‚¨çš„å»ºè®®ï¼Œæˆ‘ä»¬å°†éå¸¸æ„Ÿè°¢å¹¶è€ƒè™‘ã€‚ å¦‚æœæ‚¨éœ€è¦æ›´å¤šèƒŒæ™¯ä¿¡æ¯ï¼Œè¯·è¯¢é—®ã€‚    ç”±   æäº¤  /u/Dry-Image8120   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgtec/ppo_stuck_in_local_optima/</guid>
      <pubDate>Tue, 04 Feb 2025 12:45:10 GMT</pubDate>
    </item>
    <item>
      <title>å…³äº TRPO è®ºæ–‡çš„é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨ç ”ç©¶ TRPO è®ºæ–‡ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ä¸­è®¡ç®—æ–°ç­–ç•¥çš„é—®é¢˜ï¼š https://preview.redd.it/l8fndz5ra4he1.png?width=940&amp;format=png&amp;auto=webp&amp;s=f49f53bedb23a9a6d04f6fbeaf79a643bde0052b è¿™ä¸ªæ–¹ç¨‹ç”¨äºæ›´æ–°å’Œå¯»æ‰¾æ–°ç­–ç•¥ï¼Œä½†æˆ‘æƒ³çŸ¥é“å¦‚ä½•è®¡ç®—Ï€_Î¸(a|s)ï¼Œå› ä¸ºå®ƒå±äºæˆ‘ä»¬è¯•å›¾ä¼˜åŒ–çš„ç­–ç•¥â€”â€”å°±åƒå…ˆæœ‰é¸¡è¿˜æ˜¯å…ˆæœ‰è›‹çš„é—®é¢˜ã€‚ è®ºæ–‡æåˆ°ï¼Œæ ·æœ¬ç”¨äºè®¡ç®—è¿™ä¸ªè¡¨è¾¾å¼ï¼š  1. ä½¿ç”¨å•è·¯å¾„æˆ–è—¤è”“ç¨‹åºæ”¶é›†ä¸€ç»„çŠ¶æ€-åŠ¨ä½œå¯¹ä»¥åŠå®ƒä»¬çš„ Q å€¼çš„è’™ç‰¹å¡æ´›ä¼°è®¡ã€‚ 2. é€šè¿‡å¯¹æ ·æœ¬å–å¹³å‡å€¼ï¼Œæ„å»ºå…¬å¼ (14) ä¸­çš„ä¼°è®¡ç›®æ ‡å’Œçº¦æŸã€‚ 3. è¿‘ä¼¼è§£å†³è¿™ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ä»¥æ›´æ–°ç­–ç•¥çš„å‚æ•°å‘é‡ã€‚æˆ‘ä»¬ä½¿ç”¨å…±è½­æ¢¯åº¦ç®—æ³•ï¼Œç„¶åè¿›è¡Œçº¿æœç´¢ï¼Œè¿™åªæ¯”è®¡ç®—æ¢¯åº¦æœ¬èº«ç¨å¾®è´µä¸€ç‚¹ã€‚è¯¦æƒ…è¯·å‚é˜…é™„å½• Cã€‚     æäº¤äºº    /u/audi_etron   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</guid>
      <pubDate>Tue, 04 Feb 2025 12:38:38 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨å•å°æœºå™¨ä¸Šè¿è¡Œ Ray Tune çš„å¹¶è¡Œå®éªŒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ Ray çš„æ–°æ‰‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæµè¡Œçš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œå°¤å…¶é€‚ç”¨äº MLï¼Œæˆ‘ä¸€ç›´è‡´åŠ›äºå……åˆ†åˆ©ç”¨æˆ‘æœ‰é™çš„ä¸ªäººè®¡ç®—èµ„æºã€‚è¿™å¯èƒ½æ˜¯æˆ‘æƒ³è¦äº†è§£ Ray åŠå…¶åº“çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚å—¯ï¼Œæˆ‘ç›¸ä¿¡è®¸å¤šå­¦ç”Ÿå’Œä¸ªäººç ”ç©¶äººå‘˜éƒ½æœ‰ç›¸åŒçš„åŠ¨æœºã€‚åœ¨ä½¿ç”¨ Ray Tuneï¼ˆå…¨éƒ¨åŸºäº Pythonï¼‰è¿›è¡Œä¸€äº›å®éªŒåï¼Œæˆ‘å¼€å§‹æ„Ÿåˆ°ç–‘æƒ‘å¹¶æƒ³å¯»æ±‚å¸®åŠ©ã€‚ä»»ä½•å¸®åŠ©éƒ½å°†ä¸èƒœæ„Ÿæ¿€ï¼ ğŸ™ğŸ™ğŸ™ï¼š  Ray åœ¨å•å°æœºå™¨ä¸Šä»ç„¶æœ‰æ•ˆä¸”é«˜æ•ˆå—ï¼Ÿ æ˜¯å¦å¯ä»¥ä½¿ç”¨ Ray åœ¨å•å°æœºå™¨ä¸Šè¿è¡Œå¹¶è¡Œå®éªŒï¼ˆåœ¨æˆ‘çš„æƒ…å†µä¸‹æ˜¯ Tuneï¼‰ï¼Ÿ æˆ‘çš„è„šæœ¬æ˜¯å¦ä¸ºæ­¤ç›®çš„æ­£ç¡®è®¾ç½®ï¼Ÿ æˆ‘é—æ¼äº†ä»€ä¹ˆå—ï¼Ÿ  æ•…äº‹ï¼š* æˆ‘çš„è®¡ç®—èµ„æºéå¸¸æœ‰é™ï¼šä¸€å°é…å¤‡ 12 æ ¸ CPU å’Œ RTX 3080 Ti GPU ä»¥åŠ 12GB å†…å­˜çš„æœºå™¨ã€‚ * æˆ‘çš„ç©å…·å®éªŒæ²¡æœ‰å……åˆ†åˆ©ç”¨å¯ç”¨èµ„æºï¼šå•æ¬¡æ‰§è¡Œè€—è´¹ 11% çš„ GPU Util å’Œ 300MiB /11019MiBã€‚ * ä»ç†è®ºä¸Šè®²ï¼Œåº”è¯¥å¯ä»¥åœ¨è¿™æ ·çš„æœºå™¨ä¸ŠåŒæ—¶è¿›è¡Œ 8-9 ä¸ªè¿™æ ·çš„ç©å…·å®éªŒä¸€å°æœºå™¨ã€‚ * è‡ªç„¶è€Œç„¶ï¼Œæˆ‘æ±‚åŠ©äº Rayï¼Œå¸Œæœ›å®ƒèƒ½å¸®åŠ©ç®¡ç†å’Œè¿è¡Œå…·æœ‰ä¸åŒè¶…å‚æ•°ç»„çš„å¹¶è¡Œå®éªŒã€‚ * ä½†æ˜¯ï¼Œæ ¹æ®ä¸‹é¢çš„è„šæœ¬ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•å¹¶è¡Œæ‰§è¡Œï¼Œå³ä½¿æˆ‘åœ¨tune.run()ä¸­è®¾ç½®äº†max_concurrent_trialsã€‚æ ¹æ®æˆ‘çš„è§‚å¯Ÿï¼Œæ‰€æœ‰å®éªŒä¼¼ä¹ä¸€ä¸ªæ¥ä¸€ä¸ªè¿è¡Œã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä¸çŸ¥é“å¦‚ä½•ä¿®å¤æˆ‘çš„ä»£ç ä»¥å®ç°é€‚å½“çš„å¹¶è¡Œæ€§ã€‚ ğŸ˜­ğŸ˜­ğŸ˜­ï¼š* ä»¥ä¸‹æ˜¯æˆ‘çš„ ray tune è„šæœ¬ï¼ˆray_experiment.pyï¼‰ ```python import os import ray from ray import tune from ray.tune import CLIReporter from ray.tune.schedulers import ASHAScheduler from Simulation import run_simulations # Ray Tune ä¸­çš„å¯è®­ç»ƒå¯¹è±¡ from utils.trial_name_generator import trial_name_generator if name == &#39;maâ€‹â€‹in&#39;: ray.init() # è°ƒè¯•æ¨¡å¼ï¼šray.init(local_mode=True) # ray.init(num_cpus=12, num_gpus=1) print(ray.available_resources()) current_dir = os.path.abspath(os.getcwd()) # å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„params_groups = { &#39;exp_name&#39;: &#39;Ray_Tune&#39;, # æœç´¢ç©ºé—´ &#39;lr&#39;: tune.choice([1e-7, 1e-4]), &#39;simLength&#39;: tune.choice([400, 800]), } reporter = CLIReporter( metric_columns=[&quot;exp_progress&quot;, &quot;eval_episodes&quot;, &quot;best_r&quot;, &quot;current_r&quot;], print_intermediate_tables=True, ) analysis = tune.run( run_simulations, name=params_groups[&#39;exp_name&#39;], mode=&quot;max&quot;, config=params_groups, resources_per_trial={&quot;gpu&quot;: 0.25}, max_concurrent_trials=8, # scheduler=scheduler, storage_path=f&#39;{current_dir}/logs/&#39;, # ä¿å­˜æ—¥å¿—çš„ç›®å½• trial_dirname_creator=trial_name_generator, trial_name_creator=trial_name_generator, # resume=&quot;AUTO&quot; ) print(&quot;Best config:&quot;, analysis.get_best_config(metric=&quot;best_r&quot;, mode=&quot;max&quot;)) ray.shutdown()  ```    submitted by    /u/yxwmm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</guid>
      <pubDate>Tue, 04 Feb 2025 11:35:04 GMT</pubDate>
    </item>
    <item>
      <title>æ‰˜ç›˜è£…è½½é—®é¢˜ PPO æ¨¡å‹å®é™…ä¸Šä¸èµ·ä½œç”¨ - éœ€è¦å¸®åŠ©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihcbpo/pallet_loading_problem_ppo_model_is_not_really/</link>
      <description><![CDATA[      å› æ­¤ï¼Œæˆ‘æ­£åœ¨ç ”ç©¶ä¸€ç§ PPO å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åº”è¯¥èƒ½å¤Ÿä»¥æœ€ä½³æ–¹å¼å°†ç®±å­è£…è½½åˆ°æ‰˜ç›˜ä¸Šã€‚å­˜åœ¨ç¨³å®šæ€§ï¼ˆå¯èƒ½æ‚¬å‚ 20%ï¼‰å’Œç ´ç¢ï¼ˆæ¯ä¸ªç®±å­éƒ½æœ‰ä¸€ä¸ªç ´ç¢å‚æ•° - æ‚¨å¯ä»¥å°†ç®±å­å †å åœ¨å…·æœ‰æ›´å¤§ç ´ç¢å€¼çš„ç®±å­é¡¶éƒ¨ï¼‰çº¦æŸã€‚ æˆ‘æ­£åœ¨ä½¿ç”¨ç¦»æ•£è§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´ã€‚æˆ‘ä¸ºä»£ç†åˆ›å»ºäº†ä¸€ä¸ªå¯èƒ½ä½ç½®åˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨ä¼ é€’äº†æ‰€æœ‰çº¦æŸï¼Œç„¶åä»£ç†æœ‰ 5 ç§å¯èƒ½çš„æ“ä½œ - åœ¨ä½ç½®åˆ—è¡¨ä¸­å‰è¿›æˆ–åé€€ã€æ—‹è½¬ç®±å­ï¼ˆä»…åœ¨ä¸€ä¸ªè½´ä¸Šï¼‰ã€æ”¾ä¸‹ä¸€ä¸ªç®±å­å¹¶è·³è¿‡ä¸€ä¸ªç®±å­å¹¶è½¬åˆ°ä¸‹ä¸€ä¸ªã€‚ç®±å­å…ˆæŒ‰ç ´ç¢æ’åºï¼Œç„¶åæŒ‰é«˜åº¦æ’åºã€‚ è§‚å¯Ÿç©ºé—´å¦‚ä¸‹ï¼šæ‰˜ç›˜çš„é«˜åº¦å›¾ - æ‚¨å¯ä»¥æƒ³è±¡å®ƒå°±åƒä»é¡¶éƒ¨çœ‹æ‰˜ç›˜ - å¦‚æœå€¼ä¸º 0ï¼Œåˆ™è¡¨ç¤ºå®ƒæ˜¯åœ°é¢ï¼Œ1 - æ‰˜ç›˜å·²å¡«æ»¡ã€‚æˆ‘æ›¾å°è¯•ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œä½†æ²¡æœ‰ä»»ä½•æ”¹å˜ã€‚ç„¶åæˆ‘æœ‰ä»£ç†åæ ‡ï¼ˆxï¼Œyï¼Œzï¼‰ã€ç®±å­å‚æ•°ï¼ˆé•¿åº¦ã€å®½åº¦ã€é«˜åº¦ã€é‡é‡ã€ç ´ç¢ï¼‰ã€æ¥ä¸‹æ¥çš„ 5 ä¸ªç®±å­çš„å‚æ•°ã€ä¸‹ä¸€ä¸ªä½ç½®ã€å¯èƒ½çš„ä½ç½®æ•°é‡ã€ä½ç½®åˆ—è¡¨ä¸­çš„ç´¢å¼•ã€å‰©ä½™çš„ç®±å­æ•°ä»¥åŠç®±å­åˆ—è¡¨çš„ç´¢å¼•ã€‚ æˆ‘å°è¯•äº†å„ç§å¥–åŠ±å‡½æ•°ï¼Œä½†éƒ½æ²¡æœ‰æˆåŠŸã€‚ç›®å‰æˆ‘æœ‰è¿™æ ·çš„åŠŸèƒ½ï¼šæ— è®ºå¦‚ä½•åœ¨å¯¼èˆªä½ç½®åˆ—è¡¨æ—¶ -0.1ï¼Œå¯¹äºä¸å¦ä¸€ä¸ªç®±å­é«˜åº¦ç›¸ç­‰çš„ç®±å­çš„æ¯ä¸€æ¡è¾¹ +0.5ï¼Œå¦‚æœåœ¨æ”¹å˜ä½ç½®åè¿™äº›è¾¹çš„æ•°é‡æ›´å¤§ï¼Œåˆ™å¯¹äºæ¥è§¦å¦ä¸€ä¸ªç®±å­çš„æ¯ä¸€æ¡è¾¹ +0.5ã€‚æ—‹è½¬æ—¶å¥–åŠ±ç›¸åŒï¼Œåªæ¯”è¾ƒæœ€ä½ä½ç½®å’Œä½ç½®è®¡æ•°ã€‚é€‰æ‹©ä¸‹ä¸€ä¸ªç®±å­æ—¶ç›¸åŒï¼Œä½†æ¯”è¾ƒæœ€ä½é«˜åº¦ã€‚æœ€åï¼Œå½“æ”¾ä¸‹ä¸€ä¸ªç›’å­æ—¶ï¼Œæ¯ä¸ªæ¥è§¦è¾¹+1æˆ–å½¢æˆç›¸ç­‰çš„é«˜åº¦å’Œ+3å›ºå®šå¥–åŠ±ã€‚ æˆ‘çš„ç¥ç»ç½‘ç»œç”±ä¸€ä¸ªé¢å¤–çš„å±‚ç»„æˆï¼Œç”¨äºè§‚å¯Ÿä¸æ˜¯é«˜åº¦å›¾çš„è§‚å¯Ÿï¼ˆè¾“å‡º - 256ä¸ªç¥ç»å…ƒï¼‰ï¼Œç„¶åæ˜¯2ä¸ªéšè—å±‚ï¼Œåˆ†åˆ«æœ‰1024å’Œ512ä¸ªç¥ç»å…ƒï¼Œæœ€åæ˜¯æ¼”å‘˜è¯„è®ºå®¶å¤´ã€‚æˆ‘å¯¹é«˜åº¦å›¾å’Œæ¯ä¸ªåæ ‡è¿›è¡Œäº†æ ‡å‡†åŒ–ã€‚ æˆ‘ä½¿ç”¨çš„è¶…å‚æ•°ï¼š learningRate = 3e-4 betas = [0.9, 0.99] gamma = 0.995 epsClip = 0.2 epochs = 10 updateTimeStep = 500 entropyCoefficient = 0.01 gaeLambda = 0.98 è§£å†³é—®é¢˜ - æˆ‘çš„æ¨¡å‹æ— æ³•æ”¶æ•›ï¼ˆä»ç»˜åˆ¶ç»Ÿè®¡æ•°æ®å¯ä»¥çœ‹å‡ºï¼Œå®ƒä¼¼ä¹åœ¨é‡‡å–éšæœºåŠ¨ä½œã€‚æˆ‘å·²ç»è°ƒè¯•äº†ä»£ç å¾ˆé•¿æ—¶é—´ï¼Œä¼¼ä¹åŠ¨ä½œæ¦‚ç‡æ­£åœ¨å‘ç”Ÿå˜åŒ–ï¼ŒæŸå¤±è®¡ç®—æ­£åœ¨æ­£ç¡®å®Œæˆï¼Œåªæ˜¯å…¶ä»–åœ°æ–¹å‡ºäº†é—®é¢˜ã€‚å¯èƒ½æ˜¯ç”±äºè§‚å¯Ÿç©ºé—´ä¸å¥½å—ï¼Ÿç¥ç»ç½‘ç»œæ¶æ„ï¼Ÿä½ ä¼šæ¨èä½¿ç”¨ CNN å—ä¸å·ç§¯åçš„å…¶ä»–è§‚å¯Ÿç»“æœç›¸ç»“åˆï¼Ÿ æˆ‘é™„ä¸Šäº†æ¨¡å‹å’Œç»Ÿè®¡æ•°æ®çš„å¯è§†åŒ–ã€‚æå‰æ„Ÿè°¢æ‚¨çš„å¸®åŠ© https://preview.redd.it/kb9u2besp2he1.png?width=901&amp;format=png&amp;auto=webp&amp;s=b218e8573fd811d97cefcdd734a69590cbfd1dcd    æäº¤äºº    /u/bimbum12   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihcbpo/pallet_loading_problem_ppo_model_is_not_really/</guid>
      <pubDate>Tue, 04 Feb 2025 07:25:36 GMT</pubDate>
    </item>
    <item>
      <title>â€œé€šè¿‡éšæ€§å¥–åŠ±å¼ºåŒ–è¿‡ç¨‹â€ï¼ŒCui ç­‰äºº 2025 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</guid>
      <pubDate>Tue, 04 Feb 2025 06:21:10 GMT</pubDate>
    </item>
    <item>
      <title>ç¬¬ 7 ç‰ˆ Isaac Lab æ•™ç¨‹å‘å¸ƒï¼ä¸‹ä¸€æ­¥æˆ‘åº”è¯¥è®²ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼åªæ˜¯æƒ³é¡ºä¾¿è¯´ä¸€å¥ï¼Œæ„Ÿè°¢å¤§å®¶å¯¹æˆ‘çš„ Isaac Lab æ•™ç¨‹çš„æ”¯æŒå’Œé¼“åŠ±ã€‚åé¦ˆéå¸¸æ£’ï¼Œå¾ˆé«˜å…´çœ‹åˆ°å®ƒä»¬å¯¹ä½ æœ‰å¤šæœ‰ç”¨ï¼Œè€å®è¯´ï¼Œæˆ‘åœ¨åˆ¶ä½œå®ƒä»¬çš„åŒæ—¶è‡ªå·±ä¹Ÿå­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼ æˆ‘åˆšåˆšåœ¨ä¸åˆ° 2 ä¸ªæœˆçš„æ—¶é—´å†…å‘å¸ƒäº†æˆ‘çš„ç¬¬ 7 ä¸ªæ•™ç¨‹ï¼Œæˆ‘æƒ³ä¿æŒè¿™ç§åŠ¿å¤´ã€‚æˆ‘ç°åœ¨å°†ç»§ç»­åˆ¶ä½œå®˜æ–¹æ–‡æ¡£ï¼Œä½†æ¥ä¸‹æ¥ä½ å¸Œæœ›çœ‹åˆ°ä»€ä¹ˆï¼Ÿ â€œä»é›¶åˆ°è‹±é›„â€ç³»åˆ—ä¼šå¾ˆæœ‰è¶£å—ï¼Ÿç±»ä¼¼äºï¼š - è®¾è®¡å’Œåœ¨ Isaac Sim ä¸­æ¨¡æ‹Ÿæœºå™¨äºº - åœ¨ Isaac Lab ä¸­ä»å¤´å¼€å§‹ä½¿ç”¨ RL å¯¹å…¶è¿›è¡Œè®­ç»ƒ - ï¼ˆæœ€ç»ˆï¼‰å°†å…¶éƒ¨ç½²åœ¨çœŸæ­£çš„æœºå™¨äººä¸Š......ä¸€æ—¦æˆ‘ä¹°å¾—èµ·ä¸€ä¸ªğŸ˜… è®©æˆ‘çŸ¥é“æ‚¨è§‰å¾—æœ€ä»¤äººå…´å¥‹æˆ–æœ€æœ‰å¸®åŠ©çš„å†…å®¹ï¼éšæ—¶æ¬¢è¿å»ºè®®ã€‚ æˆ‘åœ¨ YouTube ä¸Šä¸Šä¼ äº†è¿™äº›ï¼š Isaac Lab æ•™ç¨‹ - LycheeAI    æäº¤äºº    /u/LoveYouChee   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</guid>
      <pubDate>Mon, 03 Feb 2025 22:16:38 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ RL è¿›è¡Œå¸ƒå±€ç”Ÿæˆï¼ˆä¾‹å¦‚ï¼šé“è·¯å’Œæˆ¿å±‹ï¼‰çš„æœ€ä½³æ–¹æ³•ã€‚å½“å‰æ¨¡å‹æœªè¿›è¡Œå­¦ä¹ ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨ RL ç”Ÿæˆç®€å•éƒŠåŒºçš„å¸ƒå±€ï¼šé“è·¯ã€éšœç¢ç‰©å’Œæˆ¿å±‹ã€‚è¿™æ›´åƒæ˜¯ä¸€ä¸ªå®éªŒï¼Œä½†æˆ‘æœ€å¥½å¥‡çš„æ˜¯æƒ³çŸ¥é“æˆ‘æ˜¯å¦æœ‰ä»»ä½•å˜åŒ–å¯ä»¥ä½¿ç”¨ RL ä¸ºæ­¤ç±»é—®é¢˜æå‡ºåˆç†çš„è®¾è®¡ã€‚ tensorboard ç›®å‰æˆ‘å·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼ˆä½¿ç”¨ gymnasium å’Œ stable_baselines3ï¼‰ã€‚æˆ‘æœ‰ä¸€ä¸ªç®€å•çš„è®¾ç½®ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªåä¸º env çš„ç¯å¢ƒï¼Œæˆ‘å°†æˆ‘çš„ä¸–ç•Œè¡¨ç¤ºä¸ºç½‘æ ¼ï¼š  æˆ‘ä»ä¸€ä¸ªç©ºç½‘æ ¼å¼€å§‹ï¼Œé™¤äº†é“è·¯å…ƒç´ ï¼ˆå…¥å£ç‚¹ï¼‰å’Œä¸€äº›æ— æ³•ä½¿ç”¨çš„å•å…ƒæ ¼ï¼ˆéšœç¢ç‰©ï¼Œä¾‹å¦‚å°æ¹–ï¼‰ æ¨¡å‹é‡‡å–çš„æ“ä½œæ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥ï¼Œæ”¾ç½®ä¸€ä¸ªé“è·¯æˆ–æˆ¿å±‹çš„ç“·ç –ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šï¼ˆtile_positionï¼Œtile_typeï¼‰  è‡³äºæˆ‘çš„å¥–åŠ±ï¼Œå®ƒä¸æ•´ä½“è®¾è®¡ç›¸å…³ï¼ˆè€Œä¸ä»…ä»…æ˜¯å¯¹æœ€åä¸€æ­¥çš„å¥–åŠ±ï¼Œå› ä¸ºæ—©æœŸçš„é€‰æ‹©å¯èƒ½ä¼šå¯¹ä»¥åäº§ç”Ÿå½±å“ã€‚å¹¶ä¸”æœ€å¤§åŒ–è®¾è®¡çš„æ•´ä½“è´¨é‡ï¼Œè€Œä¸æ˜¯å±€éƒ¨çš„ï¼‰ï¼ŒåŸºæœ¬ä¸Šæœ‰ 3 ä¸ªåŠ æƒé¡¹ï¼š  é“è·¯ç½‘ç»œåº”è¯¥æœ‰æ„ä¹‰ï¼šè¿æ¥åˆ°å…¥å£ï¼Œæ¯ä¸ªç“·ç –åº”è¯¥è¿æ¥åˆ°è‡³å°‘ 1 ä¸ªå…¶ä»–é“è·¯ç“·ç –ã€‚å¹¶ä¸”æ²¡æœ‰ 2x2 çš„é“è·¯ç“·ç –é›†ã€‚-&gt; æ•´ä¸ªè®¾è®¡ï¼ˆæ‰€æœ‰é“è·¯ç“·ç –ï¼‰çš„æ€»å’Œï¼ˆæ¯ä¸ªå¥½ç“·ç –çš„å¥–åŠ±å¢åŠ ï¼Œæ¯ä¸ªåç“·ç –çš„å¥–åŠ±å‡å°‘ï¼‰ã€‚è¿˜å°è¯•äº†æ‰€æœ‰ç“·ç –çš„ min() åˆ†æ•°ã€‚ æˆ¿å±‹åº”å§‹ç»ˆè¿æ¥åˆ°è‡³å°‘ 1 æ¡é“è·¯ã€‚-&gt; æ•´ä¸ªè®¾è®¡ï¼ˆæ‰€æœ‰æˆ¿å±‹ç“·ç –ï¼‰çš„æ€»å’Œï¼ˆæ¯ä¸ªå¥½ç“·ç –çš„å¥–åŠ±å¢åŠ ï¼Œæ¯ä¸ªåç“·ç –çš„å¥–åŠ±å‡å°‘ï¼‰ã€‚è¿˜å°è¯•äº†æ‰€æœ‰ç“·ç –çš„ min() åˆ†æ•°ã€‚ æœ€å¤§åŒ–æˆ¿å±‹ç“·ç –çš„æ•°é‡ï¼ˆç“·ç –è¶Šå¤šï¼Œå¥–åŠ±å°±è¶Šé«˜ï¼‰  æ¯å½“æˆ‘å°è¯•è¿è¡Œå®ƒå¹¶è®©å®ƒå­¦ä¹ æ—¶ï¼Œæˆ‘éƒ½ä¼šä»è¾ƒä½çš„ entropy_lossï¼ˆ-5ï¼Œåœ¨ 100k æ­¥åæ…¢æ…¢çˆ¬å‡è‡³ 0ï¼‰å’ŒåŸºæœ¬ä¸Šä¸º 0 çš„ explained_variance å¼€å§‹ã€‚æˆ‘çš„ç†è§£æ˜¯ï¼šæ¨¡å‹æ°¸è¿œæ— æ³•æ­£ç¡®é¢„æµ‹å®ƒé‡‡å–çš„ç»™å®šåŠ¨ä½œçš„å¥–åŠ±æ˜¯ä»€ä¹ˆã€‚å¹¶ä¸”å®ƒé‡‡å–çš„è¡ŒåŠ¨å¹¶ä¸æ¯”éšæœºå¥½ã€‚ æˆ‘å¯¹ RL è¿˜å¾ˆé™Œç”Ÿï¼Œæˆ‘çš„èƒŒæ™¯æ›´â€œä¼ ç»Ÿâ€ MLã€NLPï¼Œå¹¶ä¸”éå¸¸ç†Ÿæ‚‰è¿›åŒ–ç®—æ³•ã€‚ æˆ‘è®¤ä¸ºè¿™å¯èƒ½åªæ˜¯ä¸€ä¸ªå†·å¯åŠ¨é—®é¢˜ï¼Œæˆ–è€…è¯¾ç¨‹å­¦ä¹ å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚ä½†å³ä½¿å¦‚æ­¤ï¼Œæˆ‘ä¹Ÿä»ç®€å•çš„è®¾è®¡å¼€å§‹ã€‚ä¾‹å¦‚ 6x6 ç½‘æ ¼ã€‚æˆ‘è§‰å¾—è¿™æ›´å¤šçš„æ˜¯æˆ‘çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹å¼çš„é—®é¢˜ã€‚æˆ–è€…ä¹Ÿè®¸ä¸æˆ‘å¦‚ä½•æ„å»ºé—®é¢˜æœ‰å…³ã€‚ ------ é—®é¢˜ï¼šåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸å¦‚ä½•å¤„ç†è¿™æ ·çš„é—®é¢˜ï¼Ÿæœ‰äº†å®ƒï¼Œæœ‰å“ªäº›æ ‡å‡†æ–¹æ³•å¯ä»¥â€œè°ƒè¯•â€æ­¤ç±»é—®é¢˜ï¼Ÿä¾‹å¦‚ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦æ›´å¤šåœ°ä¸æˆ‘é€‰æ‹©çš„æ“ä½œç±»å‹æœ‰å…³ï¼Œæˆ–è€…ä¸æˆ‘çš„å¥–åŠ±è®¾è®¡æ–¹å¼ç­‰æœ‰å…³    æäº¤äºº    /u/LostInGradients   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</guid>
      <pubDate>Mon, 03 Feb 2025 21:00:15 GMT</pubDate>
    </item>
    <item>
      <title>ç»“æœçš„å¯é‡å¤æ€§</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</link>
      <description><![CDATA[æ‚¨å¥½ï¼æˆ‘æ­£åœ¨å°è¯•æŸ¥æ‰¾æœ¬æ–‡ä¸­æåˆ°çš„åŸºäºæ¨¡å‹çš„ PPO çš„å®ç°ï¼šåŸºäºæ¨¡å‹çš„æ¢ç´¢çš„ç­–ç•¥ä¼˜åŒ–ï¼Œä»¥ä¾¿é‡ç°ç»“æœå¹¶å¯èƒ½åœ¨æˆ‘çš„è®ºæ–‡ä¸­ä½¿ç”¨è¯¥æ¶æ„ã€‚ä½†ä¼¼ä¹ä»»ä½•åœ°æ–¹éƒ½æ²¡æœ‰å®˜æ–¹å®ç°ã€‚æˆ‘å·²ç»ç»™ä½œè€…å‘äº†ç”µå­é‚®ä»¶ï¼Œä½†ä¹Ÿæ²¡æœ‰æ”¶åˆ°ä»»ä½•å›å¤ã€‚ åœ¨åƒ AAAI è¿™æ ·çš„å¤§å‹ä¼šè®®ä¸Šå‘è¡¨çš„è®ºæ–‡æ²¡æœ‰ä»»ä½•å¯é‡ç°çš„å®ç°ï¼Œè¿™æ­£å¸¸å—ï¼Ÿ    æäº¤äºº    /u/GamingOzz   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</guid>
      <pubDate>Mon, 03 Feb 2025 13:18:10 GMT</pubDate>
    </item>
    <item>
      <title>ç¬¬ä¸€å±Š Tinker AI å¤§èµ›ä¼˜èƒœä½œå“ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</link>
      <description><![CDATA[        æäº¤äºº    /u/goncalogordo   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</guid>
      <pubDate>Mon, 03 Feb 2025 10:00:02 GMT</pubDate>
    </item>
    <item>
      <title>å°è¯•å¤åˆ¶æ™®é€šçš„ k-bandits é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨å°è¯•å®ç° Barto Sutton ä¹¦ä¸­çš„ç¬¬ä¸€ä¸ª k-Bandits æµ‹è¯•å¹³å°ã€‚Python ä»£ç å¯åœ¨ Git ä¸Šæ‰¾åˆ°ï¼Œä½†æˆ‘æ­£å°è¯•ä»å¤´å¼€å§‹ç‹¬ç«‹å®Œæˆã€‚ æˆªè‡³ç›®å‰ï¼Œæˆ‘æ­£åœ¨å°è¯•ç”Ÿæˆå›¾ 2.2 ä¸­çš„å¹³å‡å¥–åŠ±å›¾ã€‚æˆ‘çš„ä»£ç å¯ä»¥å·¥ä½œï¼Œä½†å¹³å‡å¥–åŠ±å›¾è¿‡æ—©ç¨³å®šä¸‹æ¥ï¼Œå¹¶ä¸”ä¿æŒç¨³å®šï¼Œè€Œä¸æ˜¯åƒä¹¦ä¸­/git ä¸­é‚£æ ·å¢åŠ ã€‚æˆ‘æ— æ³•å¼„æ¸…æ¥šæˆ‘å“ªé‡Œåšé”™äº†ã€‚ å¦‚æœæœ‰äººèƒ½çœ‹ä¸€ä¸‹å¹¶åˆ†äº«ä¸€äº›æŠ€å·§ï¼Œé‚£å°†éå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæœ‰äººæƒ³è¿è¡Œ/æµ‹è¯•å®ƒï¼Œä»£ç åº”è¯¥æŒ‰åŸæ ·å·¥ä½œã€‚  éå¸¸æ„Ÿè°¢ï¼ ``` è¯¥ç¨‹åºå®ç°äº† k-bandit é—®é¢˜çš„ n æ¬¡è¿è¡Œ import numpy as np import matplotlib.pyplot as plt bandit_reward_dist_mean = 0 bandit_reward_dist_sigma = 1 k_bandits = 10 bandit_sigma = 1 samples_per_bandit = 1000 epsilon = 0.01 def select_action(): r = np.random.randn() if r &lt; epsilonï¼šaction = np.random.randintï¼ˆ0ï¼Œk_banditsï¼‰elseï¼šaction = np.argmaxï¼ˆq_estimatesï¼‰ è¿”å›æ“ä½œ def update_action_countï¼ˆA_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢å·²é‡‡å–æ¯ä¸ªæ“ä½œçš„æ¬¡æ•°n_action [A_t] + = 1 def update_action_reward_totalï¼ˆA_tï¼ŒR_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªæ“ä½œçš„æ€»å¥–åŠ±action_rewards [A_t] + = R_t def generate_rewardï¼ˆmeanï¼Œsigmaï¼‰ï¼š# ä»æ­£æ€åˆ†å¸ƒä¸­ä¸ºè¿™ä¸ªç‰¹å®šçš„banditæå–å¥–åŠ±#r = np.random.normalï¼ˆmeanï¼Œsigmaï¼‰r = np.random.randnï¼ˆï¼‰+mean# ç±»ä¼¼äºåœ¨Git repoä¸­æ‰€åšçš„return r def update_qï¼ˆA_tï¼ŒR_tï¼‰ï¼š q_estimates[A_t] += 0.1 * (R_t - q_estimates[A_t]) n_steps = 1000 n_trials = 2000 #æ¯æ¬¡è¯•éªŒä½¿ç”¨ä¸€æ‰¹æ–°çš„è€è™æœºè¿è¡Œ n_steps æ‰€æœ‰è¯•éªŒä¸­æ¯ä¸€æ­¥çš„å¥–åŠ±çŸ©é˜µ - ä»é›¶å¼€å§‹ rewards_episodes_trials = np.zeros((n_trials, n_steps)) for j in range(0, n_trials): #q_true = np.random.normal(bandit_reward_dist_mean, bandit_reward_dist_sigma, k_bandits) q_true = np.random.randn(k_bandits) # å°è¯•å¤åˆ¶ book/git ç»“æœ # æ¯ä¸ªåŠ¨ä½œï¼ˆè€è™æœºï¼‰çš„ Q å€¼ - ä»random q_estimates = np.random.randn(k_bandits) # æ¯ä¸ªåŠ¨ä½œï¼ˆbanditï¼‰çš„æ€»å¥–åŠ± - ä»é›¶å¼€å§‹ action_rewards = np.zeros(k_bandits) # åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªåŠ¨ä½œå·²é‡‡å–çš„æ¬¡æ•° - ä»é›¶å¼€å§‹ n_action = np.zeros(k_bandits) # æ¯ä¸€æ­¥çš„å¥–åŠ± - ä» 0 å¼€å§‹ rewards_episodes = np.zeros(n_steps) for i in range(0, n_steps): A_t = select_action() R_t = generate_reward(q_true[A_t], bandit_sigma) rewards_episodes[i] = R_t  update_action_reward_total(A_t, R_t) update_action_count(A_t) update_q(A_t, R_t) rewards_episodes_trials[j,:] = rewards_episodes  æ‰€æœ‰è¿è¡Œä¸­æ¯æ­¥çš„å¹³å‡å¥–åŠ± average_reward_per_step = np.zeros(n_steps) for i in range(0, n_steps): average_reward_per_step[i] = np.mean(rewards_episodes_trials[:,i]) plt.plot(average_reward_per_step) plt.show() ```    æäº¤äºº    /u/datashri   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</guid>
      <pubDate>Mon, 03 Feb 2025 09:35:25 GMT</pubDate>
    </item>
    </channel>
</rss>