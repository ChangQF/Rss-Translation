<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 22 Sep 2024 06:21:19 GMT</lastBuildDate>
    <item>
      <title>入门帮助请求。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmlfkm/getting_started_help_request/</link>
      <description><![CDATA[我想创建 RL 来玩西洋双陆棋的变体。  我想写入接口并利用预先存在的 RL 引擎。 是否有可以满足我的需求的 GitHub 存储库？ 或者云服务？ 谢谢， Hal Heinrich    提交人    /u/halheinrich   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmlfkm/getting_started_help_request/</guid>
      <pubDate>Sun, 22 Sep 2024 04:51:42 GMT</pubDate>
    </item>
    <item>
      <title>可以以切片方式采样的离线 RL 数据集？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fmc5g5/offline_rl_datasets_that_one_can_sample_in_slice/</link>
      <description><![CDATA[您好， 我目前正在从事一个受这篇论文启发的项目，并且遇到了对可以以切片方式采样的转换数据集的需求。 （大小为 (B, S, *) 或 (S, B, *) 的批次，其中 S 是相同轨迹的连续切片的维度） 我正在尝试使 d4rl-atari 数据集工作，但是在让它对连续切片进行采样时遇到了一些麻烦，所以我想知道这里是否有人有什么建议。 域本身并不是太重要，但我更喜欢使用像素观测。    提交人    /u/Ayy_Limao   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fmc5g5/offline_rl_datasets_that_one_can_sample_in_slice/</guid>
      <pubDate>Sat, 21 Sep 2024 20:29:48 GMT</pubDate>
    </item>
    <item>
      <title>强化学习，SUMO模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm9cja/reainforcement_learning_sumo_simulation/</link>
      <description><![CDATA[        提交人    /u/IllIntroduction9410   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm9cja/reainforcement_learning_sumo_simulation/</guid>
      <pubDate>Sat, 21 Sep 2024 18:20:11 GMT</pubDate>
    </item>
    <item>
      <title>IsaacLab：如何将它与 TorchRL 一起使用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm99jv/isaaclab_how_to_use_it_with_torchrl/</link>
      <description><![CDATA[有人知道如何将 TorchRL 与 IsaacLab 一起使用吗？不幸的是，TorchRL 没有包装器。我可以轻松构建自己的包装器吗？或者存在任何其他解决方案吗？    提交人    /u/Salt_Classroom_7380   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm99jv/isaaclab_how_to_use_it_with_torchrl/</guid>
      <pubDate>Sat, 21 Sep 2024 18:16:27 GMT</pubDate>
    </item>
    <item>
      <title>[D] RL 中的 LTL 的当前状态是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fm6k4h/d_what_is_the_current_state_of_ltl_in_rl/</link>
      <description><![CDATA[我想知道为什么在将线性时间逻辑和模型检查纳入强化学习方面没有那么多论文。更具体地说，在无模型的 POMDP 场景中。在我看来，这是保证此类关键设备安全性的一个非常重要的部分，但谈论它的论文并没有得到很多引用。这些技术是否不够实用（我意识到它们通常会扩展状态空间以在采样轨迹时直接检查 LTL）？还有其他我不知道的技术吗？我真的很想知道你的经历。谢谢！    提交人    /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fm6k4h/d_what_is_the_current_state_of_ltl_in_rl/</guid>
      <pubDate>Sat, 21 Sep 2024 16:13:42 GMT</pubDate>
    </item>
    <item>
      <title>日常生活中的 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fli9u5/rl_in_your_day_to_day/</link>
      <description><![CDATA[嗨，RL 社区， 我有 6 年的电子通讯/技术 DS 经验，但主要专注于实验和建模。我正在寻找下一个机会，希望更多地转向 RL。 我很想听听社区的意见，他们实际上是在为他们的日常角色构建 RL 系统。更具体地说，您正在解决什么类型的问题，您正在构建哪种类型的算法，等等。我针对角色领域/问题类型进行了民意调查，但也欢迎您发表评论，详细说明您使用 RL 的目的。谢谢！ 查看民意调查    提交人    /u/Djekob   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fli9u5/rl_in_your_day_to_day/</guid>
      <pubDate>Fri, 20 Sep 2024 17:58:23 GMT</pubDate>
    </item>
    <item>
      <title>深度 Q 学习与策略梯度在网络规模方面的比较</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fld2f3/deep_qlearning_vs_policy_gradient_in_terms_of/</link>
      <description><![CDATA[我一直在使用策略梯度和深度 Q 网络算法处理 CartPole 任务。我观察到，策略梯度算法在较小的网络（一个 16 个神经元的隐藏层）中的表现优于深度 Q 网络，后者需要更大的网络（两个分别有 1024 个和 512 个神经元的隐藏层）。学术界是否就这两种算法实现可比性能所需的网络规模达成共识？    提交人    /u/Atreya95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fld2f3/deep_qlearning_vs_policy_gradient_in_terms_of/</guid>
      <pubDate>Fri, 20 Sep 2024 14:15:30 GMT</pubDate>
    </item>
    <item>
      <title>证明遗憾的界限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1flcctx/proving_regret_bounds/</link>
      <description><![CDATA[我是一名本科生，我的研究是尝试证明在线学习问题的遗憾界限。 有没有人有资源可以帮助我从头开始熟悉遗憾分析？这些资源可以假设本科生概率的舒适度。 更新：感谢大家的建议！我最终阅读了一些论文和资源，查看了示例，这给了我一个证明的想法。我最终完成了一个遗憾界限证明！    提交人    /u/Mysterious-Ad-3855   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1flcctx/proving_regret_bounds/</guid>
      <pubDate>Fri, 20 Sep 2024 13:43:53 GMT</pubDate>
    </item>
    <item>
      <title>强化学习用于解决类似 VRP 的优化问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1flbj32/rl_for_vrplike_optimization_problems/</link>
      <description><![CDATA[大家好。我想就此话题征求一下您的意见： 假设我有一个组合问题，例如 TSP 或更具体地说是具有松散约束的 VRP（关于公共交通优化）。 我的想法是，GNN 架构可以学习有用的特征来产生良好的启发式方法，最终旨在安排良好的路线，其目标函数在某种程度上取决于用户体验（比如总旅行时间）和预算约束（例如优化冗余路线等）。 我想知道强化学习是否是正确的框架，因为最终目标最终取决于从零或预先存在的时间表开始的路线选择轨迹。 您觉得如何？你们中有谁做过类似的事情，或者能给我推荐一些有趣的论文吗？ 另外再补充一点：我刚刚获得物理学和数据科学硕士学位，我的论文就是针对这个问题的。将 RL 融入其中的想法来自我，我很想在这个主题上深入研究，也许可以攻读博士学位来实现它。如果有人认识在 RL 方面投入精力并可能对这类问题感兴趣的教授或大学，那就太好了。谢谢大家，祝你有美好的一天！    提交人    /u/vaginedtable   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1flbj32/rl_for_vrplike_optimization_problems/</guid>
      <pubDate>Fri, 20 Sep 2024 13:05:13 GMT</pubDate>
    </item>
    <item>
      <title>在哪里以及为什么使用折扣累积奖励？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl9g6r/where_and_why_is_discounted_cumulative_reward_used/</link>
      <description><![CDATA[嗨，我是强化学习的新手，我现在正在学习一些基本术语。我遇到了“折扣累积奖励”这个术语，我理解即时奖励比未来奖励更有价值，但我不明白折扣累积奖励何时会用到。我用谷歌搜索了它，但我找到的都是“折扣累积奖励”是什么，但没有具体的例子说明它可能在哪里使用。它是否仅用于估计累积奖励，其中后期奖励被折扣，因为它们不太可预测？有没有具体的真实例子说明它可能在哪里使用？    提交人    /u/AdBitter9336   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl9g6r/where_and_why_is_discounted_cumulative_reward_used/</guid>
      <pubDate>Fri, 20 Sep 2024 11:13:26 GMT</pubDate>
    </item>
    <item>
      <title>二维装箱问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl6bxp/2d_bin_packing_problem/</link>
      <description><![CDATA[嗨！我正在研究 2D BPP 问题，需要一些指导。有一个定义的托盘和 3 种定义的箱子。我们希望用箱子填满托盘，每次一个。每个箱子都有定义的到达概率  允许箱子旋转 我们希望最好填满托盘的周长 我们避免挤压箱子（在其他箱子之间），因为这个问题是机器人问题，并且存在不确定性 我们必须在箱子到达时放置它们，不能跳过它们。一旦没有空间，我们就会终止  我使用启发式方法解决了这个问题，比较剩余的空间并选择最佳放置坐标。我还对周长使用了不同的搜索：通过沿着托盘周长的较大边优先填充边缘。我不知道如何将其变成学习问题并接受建议！    提交人    /u/Sea-Hovercraft4777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl6bxp/2d_bin_packing_problem/</guid>
      <pubDate>Fri, 20 Sep 2024 07:23:21 GMT</pubDate>
    </item>
    <item>
      <title>推荐涵盖最新算法的调查/学习材料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</link>
      <description><![CDATA[您好，有人可以推荐一些涵盖较新算法/技术（td-mpc2、dreamerv3、diffusion policy）的调查/学习材料吗？其格式类似于 openai 的 spinningup/lilianweng 的博客，现在有点过时了？谢谢    提交人    /u/saintshing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fl67ly/recommendation_for_surveyslearning_materials_that/</guid>
      <pubDate>Fri, 20 Sep 2024 07:14:21 GMT</pubDate>
    </item>
    <item>
      <title>LeanRL：一个简单的 PyTorch RL 库，用于快速（>5 倍）训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</link>
      <description><![CDATA[我们很高兴地宣布，我们已经开源了LeanRL，这是一个轻量级的 PyTorch 强化学习库，它使用 torch.compile 和 CUDA 图表提供快速 RL 训练的方法。 通过利用这些工具，与原始 CleanRL 实现相比，我们实现了显着的加速 - 速度提高了 6 倍！ RL 训练的问题 强化学习是出了名的 CPU 受限，因为小型 CPU 操作（例如从模块中检索参数或在 Python 和 C++ 之间转换）的频率很高。幸运的是，PyTorch 强大的编译器可以帮助缓解这些问题。但是，输入编译后的代码也有其自身的成本，例如检查保护以确定是否需要重新编译。对于 RL 中使用的小型网络，这种开销可能会抵消编译的好处。 进入 LeanRL LeanRL 通过提供简单的方法来加速您的训练循环并更好地利用您的 GPU，从而解决了这一挑战。受到 gpt-fast 和 sam-fast 等项目的启发，我们证明了 CUDA 图可以与 torch.compile 结合使用，以实现前所未有的性能提升。我们的结果表明：  使用 PPO（Atari）可提高 6.8 倍速度 使用 SAC 可提高 5.7 倍速度 使用 TD3 可提高 3.4 倍速度 使用 PPO（连续动作）可提高 2.7 倍速度  此外，LeanRL 可以更有效地利用 GPU，让您可以同时训练多个网络而不会牺牲性能。 主要特点  具有最小依赖性的 RL 算法的单文件实现 所有技巧都在 README 中进行了说明 从流行的 CleanRL 分叉   在 https://github.com/pytorch-labs/leanrl 上查看 LeanRL    由    /u/AdCool8270  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkzbjm/leanrl_a_simple_pytorch_rl_library_for_fast_5x/</guid>
      <pubDate>Fri, 20 Sep 2024 00:22:12 GMT</pubDate>
    </item>
    <item>
      <title>聘请 RL 研究人员——构建下一代专家系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</link>
      <description><![CDATA[嗨！我们是 Atman Labs，一家位于伦敦的 AI 初创公司，在软件中模拟人类专家。我们认为，业界需要超越法学硕士 (LLM)，构建能够解决复杂、知识密集型任务的系统，这些任务需要多步推理。我们的研究使用强化学习来探索知识图谱，以形成针对目标的语义基础策略，并代表了一条模拟专家推理的新颖、可靠的途径。 如果您对 RL 充满热情，并希望构建和商业化下一代智能系统，那么您可能非常适合我们的创始团队。让我们聊聊吧 :) https://atmanlabs.ai/team/rl-founding-engineer    提交人    /u/Tricky_Amphibian_836   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fkj51z/hiring_rl_researchers_build_the_next_generation/</guid>
      <pubDate>Thu, 19 Sep 2024 12:04:36 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI GPT-4 o1 介绍：用于内心独白的强化学习训练的 LLM</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fg750p/introducing_openai_gpt4_o1_rltrained_llm_for/</guid>
      <pubDate>Fri, 13 Sep 2024 22:17:44 GMT</pubDate>
    </item>
    </channel>
</rss>