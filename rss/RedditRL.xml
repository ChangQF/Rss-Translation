<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 11 Feb 2025 09:23:23 GMT</lastBuildDate>
    <item>
      <title>引入 ReinforceUI Studio，消除了管理额外存储库或记忆复杂命令行的麻烦。#ReinforcemetLearning</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/</link>
      <description><![CDATA[      大家好， 我很高兴与大家分享 ReinforceUI Studio，这是一款基于 Python 的开源 GUI，旨在简化强化学习 (RL) 模型的配置、训练和监控。不再需要处理无尽的命令行参数或分散的存储库 - 您需要的一切都捆绑在一个直观的界面中。 ✨ 主要特点：  无需命令行 - 由 PyQt5 驱动的 GUI 可轻松导航。 多环境支持 - 可与 OpenAI Gymnasium、MuJoCo 和 DeepMind Control Suite 配合使用。 可自定义的训练 - 只需单击几下即可调整超参数。 实时监控 - 直观地跟踪训练进度。 自动记录和评估 - 无缝存储训练数据、图表、模型和视频。 多种安装选项 - 通过 Conda、虚拟环境或 Docker 运行。  Github：https://github.com/dvalenciar/ReinforceUI-Studio 文档：https://docs.reinforceui-studio.com/welcome https://i.redd.it/ktggkyruxgie1.gif 训练 RL 模型所需的一切都在一个存储库中提供。只需单击几下，您就可以训练模型，可视化训练过程并保存模型以供日后使用 - 随时可以部署和分析。 您还可以加载预先训练的模型 轻松监控训练曲线    提交人    /u/dvr_dvr   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imtu96/introducing_reinforceui_studio_eliminates_the/</guid>
      <pubDate>Tue, 11 Feb 2025 08:13:22 GMT</pubDate>
    </item>
    <item>
      <title>PPO 标准差实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imr2hg/ppo_standard_deviation_implementation/</link>
      <description><![CDATA[大家好， 我对 PPO 中随机策略的实现有些困惑。在此之前，我已实现了 SAC 的几个变体，在几乎所有情况下，我都使用单个神经网络来输出我的行为的平均值和对数标准差。 据我所见和所试，大多数 PPO 实现要么使用恒定标准差，要么随着时间的推移线性降低标准差。我曾见过有人提到与状态空间无关的学习标准差。但我还没有见过这种实现（如果不是状态空间，我不确定我从哪里学到了什么）。 据我所知，这种差异是由于 SAC 使用最大熵目标，而 PPO 在其目标中不直接使用熵。但这也让我感到困惑，因为增加熵不是会鼓励更大的标准差吗？  我尝试使用来自 SAC 的策略神经网络实现 PPO，但失败了。但是当我使用恒定标准差或线性减少它时，我能够在推车杆上学到一些东西。 任何帮助都将不胜感激！    提交人    /u/LostBandard   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imr2hg/ppo_standard_deviation_implementation/</guid>
      <pubDate>Tue, 11 Feb 2025 05:08:06 GMT</pubDate>
    </item>
    <item>
      <title>论文提交给顶级会议，但未取得成果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1impaq6/paper_submitted_to_a_top_conference_with/</link>
      <description><![CDATA[我注意到原作者提供的代码甚至与他们论文中的方法论都不匹配，因此我联系了原作者。我根据他们的论文进行了完整而忠实的复制，但我得到的结果并不像他们报告的那么完美。 学术捏造是新常态吗？    提交人    /u/Rei_Opus   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1impaq6/paper_submitted_to_a_top_conference_with/</guid>
      <pubDate>Tue, 11 Feb 2025 03:31:02 GMT</pubDate>
    </item>
    <item>
      <title>最受欢迎的强化学习排行榜有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1imonl5/what_are_the_most_popular_reinforcement_learning/</link>
      <description><![CDATA[我想知道是否有一些针对各种环境的知名、最新的官方排行榜？我发现健身房排行榜在将近一年前更新过，我记得在我参加他们的深度强化学习课程时，hugging face 有一些排行榜。我们甚至有一些有人关心的知名记分牌吗？ 能够跟踪和绘制最新技术及其表现如何听起来相当重要。这样我们就能了解方向和进展。或者我们只关心推动新游戏的突破，我们只需检查一下，强化学习在 Minecraft 中使用 DreamerV3 获得钻石，仅此而已。我们正在等待更复杂的游戏被击败。 你对此有什么看法？只是做一个氛围检查，听听你们的想法    提交人    /u/Inexperienced-Me   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1imonl5/what_are_the_most_popular_reinforcement_learning/</guid>
      <pubDate>Tue, 11 Feb 2025 02:58:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习路线图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1im6dea/reinforcement_learning_roadmap/</link>
      <description><![CDATA[我想学习强化学习，但不知道从哪里开始。我对不同类型的 NN 的标准工作以及目前流行的架构（如 transformers）有很好的了解。 谢谢你的帮助    提交人    /u/dc_baslani_777   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1im6dea/reinforcement_learning_roadmap/</guid>
      <pubDate>Mon, 10 Feb 2025 13:42:50 GMT</pubDate>
    </item>
    <item>
      <title>屏蔽多二进制动作空间中的无效动作或额外约束</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1im466h/masking_invalid_actions_or_extra_constraints_in/</link>
      <description><![CDATA[大家好！ 我正在尝试在实现 gym 接口的自定义环境中训练代理。我查看了 SB3 和 SB3-contrib repos 中实现的算法，并发现了可屏蔽 PPO。我读到，如果无效操作的数量与有效操作相比相对较大，则屏蔽无效操作比惩罚它们更好。 我的动作空间是一个二进制矩阵，可屏蔽 PPO 支持屏蔽特定元素。换句话说，它将 action[i, j] 限制为 0。我想知道是否有办法定义其他约束，例如每一行都必须包含特定数量的 1。 提前致谢！    提交人    /u/officerKowalski   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1im466h/masking_invalid_actions_or_extra_constraints_in/</guid>
      <pubDate>Mon, 10 Feb 2025 11:40:02 GMT</pubDate>
    </item>
    <item>
      <title>“通过强化学习和推理扩展推进语言模型推理”，侯等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1im0bsf/advancing_language_model_reasoning_through/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1im0bsf/advancing_language_model_reasoning_through/</guid>
      <pubDate>Mon, 10 Feb 2025 07:02:18 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线 3 - 在 model.learn() 之外学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ilpeu9/stable_baselines3_learn_outside_of_modellearn/</link>
      <description><![CDATA[我有一个项目，我想将强化学习集成到一个更大的解决导航问题的算法中。例如，RL 机器人将学习如何在自行车（或其他控制任务）上保持平衡并向前移动，而 A* 算法可以指定要走哪条街道才能到达目标。对于这个项目，我想在 A* 会话期间微调代理 - 通过这些会话中的奖励更新策略。有没有一种简单的方法可以在稳定基线 3 中指定学习参数并在 model.learn() 之外更新策略权重？如果没有，我需要编写和测试自定义 PPO，这会减慢进程...... 感谢所有回复， Michal    提交人    /u/majklost21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ilpeu9/stable_baselines3_learn_outside_of_modellearn/</guid>
      <pubDate>Sun, 09 Feb 2025 21:14:45 GMT</pubDate>
    </item>
    <item>
      <title>具有安全奖励的 RL 与安全/受限的 RL 之间有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ildbz0/whats_so_different_between_rl_with_safety_rewards/</link>
      <description><![CDATA[安全/受约束的 RL 的目标是通过将约束回报限制在某些阈值以下来最大化回报，同时保证安全探索或满足约束。 但我想知道这与普通 RL 有何不同，普通 RL 具有一些奖励函数，如果违反安全约束，则会给出负奖励。是什么让安全/受约束的 RL 如此特别和/或不同？    提交人    /u/Open-Safety-1585   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ildbz0/whats_so_different_between_rl_with_safety_rewards/</guid>
      <pubDate>Sun, 09 Feb 2025 12:06:55 GMT</pubDate>
    </item>
    <item>
      <title>我可以将批次拆分为 A2C 的小批次吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il9odx/can_i_split_my_batch_into_mini_batches_for_a2c/</link>
      <description><![CDATA[Advantage Actor Critic 是一种基于策略的 RL 算法，这意味着网络仅使用当前策略生成的经验进行更新。 话虽如此，我明白我不能使用重放缓冲区来使算法更高效的采样，我只能使用最新的经验来更新网络。 现在，假设我使用最新策略生成一批 1000 个样本。我应该一次对整个批次运行梯度下降，计算梯度并进行一次更新，还是可以将批次分成 10 个较小的迷你批次并更新网络 10 次？最后一种方法会违反“基于策略”假设吗？    提交人    /u/Significant-Owl-4088   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il9odx/can_i_split_my_batch_into_mini_batches_for_a2c/</guid>
      <pubDate>Sun, 09 Feb 2025 07:44:51 GMT</pubDate>
    </item>
    <item>
      <title>训练时的模拟时间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il9jeo/simulation_time_when_training/</link>
      <description><![CDATA[嗨， 我担心的一件事是样本效率......我计划运行一个软演员评论家模型来优化物理模拟，但是物理模拟本身需要 1 分钟才能运行。如果我需要 100 万步才能收敛，那么每一步可能都需要 2 分钟。这是并行化的。这根本不可行，如何处理？    提交人    /u/MilkyJuggernuts   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il9jeo/simulation_time_when_training/</guid>
      <pubDate>Sun, 09 Feb 2025 07:34:41 GMT</pubDate>
    </item>
    <item>
      <title>如何实现这一点？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il8vg2/how_to_make_this_happen/</link>
      <description><![CDATA[我做了一个 ML 项目，它与主动学习有关。我现在想做更多的事情，我正在寻找项目，但我也有一些想法，我计划在网络上制作一个 WORDLE 克隆，然后制作一个 RL 模型来玩它，但如何继续做这件事？欢迎任何资源和建议 TL;DR ML 新手，想要制作一个 WORDLE 克隆并训练 RL 模型来玩它，请求资源和建议。     提交人    /u/DarkLord-0708   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il8vg2/how_to_make_this_happen/</guid>
      <pubDate>Sun, 09 Feb 2025 06:48:14 GMT</pubDate>
    </item>
    <item>
      <title>PPO 问题：策略损失和价值函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il6erp/ppo_question_policy_loss_and_value_function/</link>
      <description><![CDATA[      大家好， 我正在尝试首次使用简单的蒙特卡洛估计器来实现 PPO。我来自 SAC 和 DQN 的实现。我无法理解如何最大化策略更新，同时最小化价值函数损失。我的优势本质上是 G_t - V(s)，策略神经网络旨在最大化新策略与旧策略乘以该优势的比率。另一方面，我的价值函数神经网络旨在最小化相同的优势。显然，我在这里的某个地方有一个误解，因为我不应该试图最小化和最大化相同的功能，因此我的实现没有学到任何东西。 我的损失函数如下所示： # 计算优势 advantage = mc_returns - self.critic(states).detach() # 计算策略损失 new_log_probs = self.policy.get_log_prob(states, action) ratio = (new_log_probs - old_log_probs).exp() policy_loss1 = ratio * advantage policy_loss2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantage policy_loss = -torch.min(policy_loss1, policy_loss2).mean() # 计算价值损失 value_loss = ((self.critic(states) - mc_returns) ** 2).mean() # 计算优势 advantage = mc_returns - self.critic(states).detach() # 计算策略损失 new_log_probs = self.policy.get_log_prob(states, action) ratio = (new_log_probs - old_log_probs).exp() policy_loss1 = ratio * advantages policy_loss2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages policy_loss = -torch.min(policy_loss1, policy_loss2).mean() # 计算价值损失 value_loss = ((self.critic(states) - mc_returns) ** 2).mean()  下面是我计算蒙特卡洛（ish）回报的方法。我会在固定数量的时间步骤后或情节结束后计算回报，因此如果情节未完成，我会使用价值函数来估计最后的回报。 curr_return = 0 for i in range(1, len(rewards) + 1): if i == 1 and not done: curr_return = reward + self.gamma*critic(next_state).detach() else: curr_return = rewards[-i] + self.gamma*curr_return mc_returns[-i] = curr_return curr_return = 0 for i in range(1, len(rewards) + 1): if i == 1 and not done: curr_return = reward + self.gamma*critic(next_state).detach() else: curr_return = rewards[-i] + self.gamma*curr_return mc_returns[-i] = curr_return  如果有人能帮助澄清我在这里遗漏的内容，我将不胜感激！ 编辑：我正在使用的新优势： https://preview.redd.it/zt0l6wpu67ie1.png?width=282&amp;format=png&amp;auto=webp&amp;s=51d7d33d96faf376c8e981c489b577a4033cbc1e    提交人    /u/LostBandard   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il6erp/ppo_question_policy_loss_and_value_function/</guid>
      <pubDate>Sun, 09 Feb 2025 04:15:21 GMT</pubDate>
    </item>
    <item>
      <title>“论语言模型提炼中的教师黑客行为”，Tiapkin 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1il2g2j/on_teacher_hacking_in_language_model_distillation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1il2g2j/on_teacher_hacking_in_language_model_distillation/</guid>
      <pubDate>Sun, 09 Feb 2025 00:45:44 GMT</pubDate>
    </item>
    <item>
      <title>“并行 Q 学习 (PQL)：大规模并行模拟下的扩展离策略强化学习”，Li 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ikpy13/parallel_qlearning_pql_scaling_offpolicy/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ikpy13/parallel_qlearning_pql_scaling_offpolicy/</guid>
      <pubDate>Sat, 08 Feb 2025 15:37:30 GMT</pubDate>
    </item>
    </channel>
</rss>