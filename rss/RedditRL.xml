<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 11 Dec 2023 09:15:38 GMT</lastBuildDate>
    <item>
      <title>根据观测数据建模系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18fp5ru/modeling_system_from_observation_data/</link>
      <description><![CDATA[      你好，我正在尝试根据观察和操作数据对 cartpole 系统进行建模。 我已经使用 stable-baseline-s3 训练了 CartPole-v1 并将测试数据保存为 NumPy 文件。 https://i.redd.it/xeiix9nzcm5c1.gif 在其中，n 和 n + 1 分别是当前状态和下一个状态。 训练输入：动作 (n)、小车位置 (n)、小车速度 (n)、极角 (n) )，极角速度 (n)。 训练输出：小车位置 (n + 1)、小车速度 (n + 1)、极角 (n + 1)、极角速度(n + 1) 因此，训练数据将是： 输入 [[-0.024206114932894707, 0.04684637486934662, -0.02134183794260025, 0.03581790253520012, 0] , [-0.023269187659025192, -0.14796313643455505, -0.020625479519367218, 0.3216916024684906, 1], ...] 输出 [[ - 0.023269187659025192, -0.14796313643455505, -0.020625479519367218, 0.3216916024684906, 1], ...] 有哪些方法或工具可以帮助我完成此任务？是否有任何开源项目可以为通用数据集执行此任务（更复杂的系统，更多的输入和输出，...）？谢谢您的建议。   由   提交/u/Sea-Economist1465  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18fp5ru/modeling_system_from_observation_data/</guid>
      <pubDate>Mon, 11 Dec 2023 07:37:44 GMT</pubDate>
    </item>
    <item>
      <title>强化学习项目：值得吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18fmtw1/reinforcement_learning_project_is_it_worth_it/</link>
      <description><![CDATA[大家好， 我最近获得了理论数学博士学位，正在尝试向工业界转型。 我最近获得了理论数学博士学位。 p&gt; 我目前正在从事编码项目，以使我的数学博士学位更适合雇主。我开始阅读一些有关 spin up 的文章，并发现我可以慢慢地完成一些理论论文。 我了解这是一个利基领域，我不希望获得研究职位根据我目前的简历，在这样一个竞争激烈的领域。话虽如此，它与我的数学背景完美契合，帮助我在找工作时保持敏锐的理论技能，并且足够有趣，让我有动力投入工作以在项目上取得进展。 研究强化学习和实施强化学习算法是否可以将技能转移到机器学习的其他领域？雇主会喜欢看到强化学习项目吗？    由   提交/u/Intelligent_Test_248   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18fmtw1/reinforcement_learning_project_is_it_worth_it/</guid>
      <pubDate>Mon, 11 Dec 2023 05:03:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO - 能否将价值模型的最新输出作为输入提供给策略？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18f323f/ppo_can_the_policy_be_fed_the_most_recent_output/</link>
      <description><![CDATA[这是否太循环了？想要进行一个实验，但我不确定这是否是一个坏主意。   由   提交/u/30299578815310  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18f323f/ppo_can_the_policy_be_fed_the_most_recent_output/</guid>
      <pubDate>Sun, 10 Dec 2023 12:56:55 GMT</pubDate>
    </item>
    <item>
      <title>利用微型循环神经网络自动发现认知策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18f1msw/automatic_discovery_of_cognitive_strategies_with/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2023.04.12.536629 摘要：  规范贝叶斯推理和基于奖励的学习等建模框架为适应性行为的基本原理提供了宝贵的见解。然而，它们描述真实动物行为的能力受到通常少量拟合参数的限制，导致手工调整和模型比较的循环，容易产生研究主观性。在这里，我们提出了一种新颖的建模方法，利用循环神经网络自动发现管理动物决策的认知算法。我们证明，在三个经过充分研究的奖励学习任务中，仅具有一两个单元的神经网络可以比经典认知模型更准确地预测个体动物的选择，并且与更大的神经网络一样准确。然后，我们使用状态空间和定点吸引子等动态系统概念来解释训练后的网络，从而对不同认知模型进行统一比较，并详细描述动物选择背后的认知机制。我们的方法还估计行为维度，并提供对元强化学习代理中出现的算法的见解。总的来说，我们提出了一种系统方法，用于发现决策中可解释的认知策略，提供对神经机制的见解，并为检查健康和功能失调的认知奠定基础。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18f1msw/automatic_discovery_of_cognitive_strategies_with/</guid>
      <pubDate>Sun, 10 Dec 2023 11:29:26 GMT</pubDate>
    </item>
    <item>
      <title>多无人机强化学习适合什么仿真环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18f087i/what_simulation_environment_is_suitable_for/</link>
      <description><![CDATA[我正在使用MARL进行端到端多无人机视觉导航，所以我需要一个合适的模拟环境。我想找到一个可以自定义环境图并且支持模拟加速的模拟环境。有推荐的无人机模拟环境吗？ PS：我尝试过Airsim，但已经不再维护了。同时，在Airsim中使用时钟速度进行模拟加速会改变物理参数，这会对强化学习训练产生负面影响。   由   提交/u/Cute-Heron-1709   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18f087i/what_simulation_environment_is_suitable_for/</guid>
      <pubDate>Sun, 10 Dec 2023 09:54:21 GMT</pubDate>
    </item>
    <item>
      <title>观测空间噪声</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ew8jo/observation_space_noise/</link>
      <description><![CDATA[您好，我正在开发一个根据过去数据生成的环境。由于过去的数据本身是固定的，因此代理在火车环境（由火车数据生成的环境）上过度拟合。它在火车环境中表现良好，但在验证环境中表现不佳。我试图通过在火车环境中添加一些观察噪声来解决这个问题。这个想法符合逻辑吗？另外你们能给我一些关于数据生成环境培训的建议吗？谢谢。   由   提交 /u/RealJuney   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ew8jo/observation_space_noise/</guid>
      <pubDate>Sun, 10 Dec 2023 05:15:54 GMT</pubDate>
    </item>
    <item>
      <title>RL 比赛/世界纪录？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18em2dp/rl_competitions_world_records/</link>
      <description><![CDATA[嗨， 有一个致力于快速运行各种视频游戏的大型社区。我想知道强化学习是否有类似的东西，人们可以跟踪每场比赛的最佳机器人。人工智能可以像人类速通变体一样衡量速度，也可以衡量目前国际象棋中已经存在的竞争强度。  这样的社区存在吗？您认为现有的人工智能（例如 AlphaZero）会占主导地位吗？或者您是否认为现有的人工智能不会给业余程序员留下任何乐趣？  让我知道你的想法   由   提交/u/Aggravating_Lack_454   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18em2dp/rl_competitions_world_records/</guid>
      <pubDate>Sat, 09 Dec 2023 20:17:33 GMT</pubDate>
    </item>
    <item>
      <title>截断分位数批评 (TQC) 和 n 步学习算法的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ekb5x/problem_with_truncated_quantile_critics_tqc_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ekb5x/problem_with_truncated_quantile_critics_tqc_and/</guid>
      <pubDate>Sat, 09 Dec 2023 18:55:43 GMT</pubDate>
    </item>
    <item>
      <title>自定义 pettingzoo 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ego4t/custom_pettingzoo_environments/</link>
      <description><![CDATA[我正在疯狂地尝试使用我的自定义 pettingzoo （并行 API）环境。这是一个 Mario 64 环境，我花了很多时间来允许多个 Mario 并为每个人提供图像，但我真的无法让它与任何软件包一起工作。我尝试过 Agilerl、sb3、rllib 甚至 cleanRL。我知道 pettingzoo 不像体育馆那么成熟，但这太荒谬了。如果你喜欢 Mario 64，你可以尝试一下，如果你让它工作，请告诉我最新信息 https://github.com/ Gumbo64/sm64-AI agilerl 可以运行，但只适合 4 名或更少的玩家，我不知道它是否能正常学习，尽管我现在要把它留着过夜。由于某种原因，Ray rllib 在推出后冻结 我所做的随机动作测试工作得很好，不过，重置或动作/观察空间或其他什么都不应该成为问题   由   提交/u/Gumbo64  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ego4t/custom_pettingzoo_environments/</guid>
      <pubDate>Sat, 09 Dec 2023 16:04:56 GMT</pubDate>
    </item>
    <item>
      <title>如何通过仅在剧集结束时给出的奖励来升级AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18eeutp/how_to_upgrade_ai_with_rewards_only_given_at_the/</link>
      <description><![CDATA[例如，当我们只在剧集结束时给 AI 奖励，而不是针对它所做的每一个动作时，我们该怎么办AI 赢得了比赛，获得+1 奖励，如果输了，获得-1 奖励，这种情况下我们该怎么办？   由   提交/u/OneCommonMan123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18eeutp/how_to_upgrade_ai_with_rewards_only_given_at_the/</guid>
      <pubDate>Sat, 09 Dec 2023 14:35:19 GMT</pubDate>
    </item>
    <item>
      <title>PPO 的低奖励波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ebijk/low_reward_oscillations_in_ppo/</link>
      <description><![CDATA[我正在尝试实现 PPO 算法，其中每个情节只有 1 个步骤，采样时间为 800 秒（意味着每个情节长度为 800 秒）我获得的奖励在从低到高的奖励之间波动。 （参考图片）如何在训练时解决这个问题。    由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ebijk/low_reward_oscillations_in_ppo/</guid>
      <pubDate>Sat, 09 Dec 2023 11:17:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PPO 训练 LSTM 策略？伴随着复杂的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18eb69c/how_to_train_a_lstm_policy_with_ppo_with_complex/</link>
      <description><![CDATA[您好， 我正在尝试了解如何实现具有多个操作作为输出的训练循环和策略（这是自回归预测的），例如LSTM（Seq2Seq，输入序列经过 N 个观察，输出序列复杂的动作类似于 Openai Five 中的做法 https://openai.com/研究/openai-5）。我所说的复杂动作是指定义了 N 种类型的动作，例如具有多个类别（例如，LSTM 输出序列可能用于移动动作，例如 hide_0 -&gt; move_action -&gt; offset_x -&gt; offset_y）。  让我担心的一件事是如何将这些操作映射到相应的对数概率并执行反向传播步骤，另一个问题是探索，因为单个操作空间显着爆炸，我假设需要的时间由于探索，训练这种策略要高得多。   由   提交/u/basic_r_user  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18eb69c/how_to_train_a_lstm_policy_with_ppo_with_complex/</guid>
      <pubDate>Sat, 09 Dec 2023 10:56:08 GMT</pubDate>
    </item>
    <item>
      <title>“Eureka：通过编码大型语言模型进行人性化奖励设计”，Ma 等人 2023 {Nvidia}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dycua/eureka_humanlevel_reward_design_via_coding_large/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dycua/eureka_humanlevel_reward_design_via_coding_large/</guid>
      <pubDate>Fri, 08 Dec 2023 22:22:22 GMT</pubDate>
    </item>
    <item>
      <title>开放世界中的学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18du95j/learning_curricula_in_openended_worlds/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.03126 后记中列出的后续工作（第 7 章）。 代码&lt; /strong&gt;： https://github.com/facebookresearch/level-replay https://github.com/facebookresearch/dcd ACCEL 演示：https://accelagent.github.io/ 摘要：  深度强化学习（RL）为训练最佳顺序决策代理提供了强大的方法。由于收集现实世界的交互可能会带来额外的成本和安全风险，sim2real 的常见范例是在模拟器中进行训练，然后进行现实世界的部署。不幸的是，强化学习代理很容易过度适应模拟训练环境的选择，更糟糕的是，当代理掌握了一组特定的模拟环境时，学习就结束了。相比之下，现实世界是高度开放的，具有不断变化的环境和挑战，使得这种强化学习方法不适合。简单地对模拟环境进行随机化是不够的，因为它需要做出任意的分布假设，并且组合地采样对学习有用的特定环境实例的可能性较小。理想的学习过程应该自动适应训练环境，以在匹配或超越现实世界复杂性的开放式任务空间上最大限度地发挥智能体的学习潜力。本论文开发了一类名为无监督环境设计（UED）的方法，旨在产生这种开放式过程。给定环境设计空间，UED 在学习代理能力的前沿自动生成无限序列或训练环境课程。通过基于极小最大遗憾决策理论和博弈论的广泛实证研究和理论论证，本论文的研究结果表明，UED 自动课程可以产生 RL 代理，该代理对以前未见过的环境实例表现出显着提高的鲁棒性和泛化能力。这样的自动课程是通向开放式学习系统的有希望的途径，该系统通过不断生成和掌握自己设计的额外挑战来实现更通用的智能。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18du95j/learning_curricula_in_openended_worlds/</guid>
      <pubDate>Fri, 08 Dec 2023 19:16:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的力量：看看这个 DeepRL Sektor 模型如何在 DIAMBRA 竞赛平台上提交的视频中为《终极真人快打 3》找到一个智能、超酷的漏洞利用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dpb57/the_power_of_reinforcement_learning_look_how_this/</link>
      <description><![CDATA[   /u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dpb57/the_power_of_reinforcement_learning_look_how_this/</guid>
      <pubDate>Fri, 08 Dec 2023 15:31:26 GMT</pubDate>
    </item>
    </channel>
</rss>