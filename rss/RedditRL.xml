<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 18 Jan 2024 03:16:16 GMT</lastBuildDate>
    <item>
      <title>“通过离散扩散学习自动驾驶的无监督世界模型”，Zhang 等人 2023（MAE 规划）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</guid>
      <pubDate>Wed, 17 Jan 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>在决斗深度 Q 网络 (DQN) 训练中更改张量维度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</link>
      <description><![CDATA[我目前正在使用 PyTorch 实现决斗深度 Q 网络 (DQN)，以在 Gym 的 Ms. Pacman 环境中训练代理。训练似乎开始顺利，但经过几集（特别是大约 100 集之后），我意识到模型的输入张量的尺寸开始发生变化，导致训练不稳定。 I&#39; m 使用包含 ObservationBuffer、ExperienceBuffer、FrameSkippingAgent、DuelingDQN 和 Agent 等类的代码结构。该模型是使用 Double DQN 方法进行训练的，我很难在张量维度变化中识别此问题的根源。 一些重要的观察结果：我正在使用 GPU (CUDA) 来加速训练。 Dueling DQN 模型的输入观察维度一开始是正确的，但在超过 200 个回合后开始发生变化 问题：训练期间张量维度发生这些变化的原因是什么？ 预先感谢您提供任何可能有助于解决此问题的指导或建议。如果需要，我很乐意提供更多详细信息。 完整代码：https://stackoverflow.com/questions/77830358/changing-tensor-dimensions-in-dueling-deep-q-network-dqn-training 第 481 集，总奖励：300.0 第 482 集，总奖励：770.0 第 483 集，总奖励：210.0 第 484 集，总奖励：200.0 第 485 集，总奖励：280.0 运行时错误：给定组=1，大小权重 [ 64, 4, 8, 8]，预期输入[1, 84, 84, 1]有4个通道，但得到了84个通道   由   提交 /u/sigma_ks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</guid>
      <pubDate>Wed, 17 Jan 2024 21:03:19 GMT</pubDate>
    </item>
    <item>
      <title>关于动作分支论文的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</link>
      <description><![CDATA[大家好， 我正在尝试改编 这篇论文适合我的应用，但在实现过程中，论文的方法有些不清楚。 ​ 在第 4 页上，方程 5 和 6 讨论了如何将 d 目标值（每个分支 1 个目标值）减少到一个目标 y。  ​ 但是，方程 7 将损失描述为 Q 值与目标之间的平方差之和，分别针对所有分支！  ​ 那么，他们是否将目标聚合为一个目标值，从而形成 y - Q(s, a) 形式的损失方程，或者不聚合？我试图深入研究代码，但这并没有让我变得更明智。  ​ 如果您理解这一点，请告诉我，这将会有巨大的帮助！  &amp; #32；由   提交 /u/Abilitytofart   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</guid>
      <pubDate>Wed, 17 Jan 2024 17:16:34 GMT</pubDate>
    </item>
    <item>
      <title>关于强化学习中的softmax导数（问题）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198znmv/about_softmax_derivatives_in_reinforcement/</link>
      <description><![CDATA[选择“类别”时从雅可比矩阵中我选择哪一个，因为我不知道哪个是“正确的”？这通常用于强化学习。   由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198znmv/about_softmax_derivatives_in_reinforcement/</guid>
      <pubDate>Wed, 17 Jan 2024 15:48:58 GMT</pubDate>
    </item>
    <item>
      <title>分析强化学习泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /泛化强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</guid>
      <pubDate>Wed, 17 Jan 2024 09:54:16 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议以加快稳定基线下的 PPO 模型训练3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</link>
      <description><![CDATA[嘿各位 Reddit 用户！ 我目前正在使用 Stable Baselines3 训练金融日交易模型，并且我&#39;我面临着训练速度的挑战。每天（每集）涉及大约 250 万个数据点，当采取随机操作时，我的模拟器可以在大约 60-70 秒内迭代它们。 训练我的 PPO 模型时会出现问题，因为它需要每集长达 40-45 分钟。我只在剧集结束时执行一次更新，没有有限的水平线截断。当模拟器可以在一分钟左右完成训练时，为什么要花这么长时间来训练一集？有什么提示或技巧可以加速这个训练过程吗？接受建议和见解！   由   提交 /u/Bunny_lad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</guid>
      <pubDate>Tue, 16 Jan 2024 14:00:29 GMT</pubDate>
    </item>
    <item>
      <title>如何学习犯罪学？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1980su2/how_to_study_criminology/</link>
      <description><![CDATA[如何学习犯罪学？ 嗨，我的朋友想在美国学习犯罪学。我们不知道有什么要求、考试以及哪些大学有这样的教师。她攻读设计学士学位，想要攻读本科犯罪学。请帮助我们，她该如何开始？ （她不在美国）   由   提交 /u/DevilSummoned   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1980su2/how_to_study_criminology/</guid>
      <pubDate>Tue, 16 Jan 2024 11:30:39 GMT</pubDate>
    </item>
    <item>
      <title>神经网络可以处理高于 1 的奖励吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19809re/can_a_neural_network_handle_rewards_above_1/</link>
      <description><![CDATA[我知道在 -1 和 1 之间传递值可以提高稳定性，但我想知道模型是否可以容忍传递更高的值？不幸的是我现在没有环境可以测试它   由   提交 /u/sogha   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19809re/can_a_neural_network_handle_rewards_above_1/</guid>
      <pubDate>Tue, 16 Jan 2024 10:58:32 GMT</pubDate>
    </item>
    <item>
      <title>PPO 特工与随机玩家进行神奇宝贝对决。你知道为什么平均奖励如此不稳定吗？ lr=1e-3，8 个并行环境在训练前进行 60 次移动，num_epochs=3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</link>
      <description><![CDATA[       由   提交 /u/moisturemeister   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</guid>
      <pubDate>Tue, 16 Jan 2024 09:57:47 GMT</pubDate>
    </item>
    <item>
      <title>TicTacToe 的表格 Q-Learning - 仅最后一个状态/动作对存储在 Q-Table 字典中，其值不为 0</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yxc1/tabular_qlearning_for_tictactoe_only_the_last/</link>
      <description><![CDATA[我在 tictactoe 3x3 板的表格 q-learning 实现中遇到问题。 ​ 问题在于，只有最后一步（获胜、失败、平局）及其各自的棋盘状态存储在 q 值不是“0.0”的 q 表中。导致最后移动的所有其他状态和动作对仍然具有值“0.0”。我在下面添加了 q 表，其中显示最后一步的值为“0.2”。但之前所有的移动的值为“0.0”。这只是第一集。即使增加了剧集也不会改变任何事情。只有最后一个动作的 q 值不是“0.0” ​ 非常感谢任何帮助。我花了几天时间尝试修复它... :( class Mark(enum.StrEnum): CROSS = &quot;X&quot; NAUGHT = &quot;O&quot;; EMPTY = &quot;; _&quot; class Reward(enum.IntEnum): WIN = 1 LosE = -1 TIE = -0.065 NON_TERMINAL = -0.01 # Q-Learning 常量 EPSILON = 0.1 # 探索因子 ALPHA = 0.2 # 学习率 GAMMA = 0.95 # 折扣因子 TOTAL_EPISODES = 1 # 代理将玩的游戏总数 BOARD = np.array([Mark.EMPTY] * BOARD_SIZE)  ​  def update_q_table(board,action,reward,new_board): board_key = &quot;&quot;.join(board) new_board_key = &quot;&quot;.join(new_board) old_value = Q_TABLE_DICT.get((board_key, action), 0) if game_over (new_board): # 如果是最终状态，则没有未来奖励可以考虑 next_max = 0 else: # 估计最优未来值 next_max = max( Q_TABLE_DICT.get((new_board_key, a), 0) for a in possible_moves( new_board) ) # 使用贝尔曼方程更新当前状态-动作对的 Q 值 q_value = old_value + ALPHA * (reward + GAMMA * next_max - old_value) Q_TABLE_DICT[(board_key, action)] = q_value &lt; /pre&gt; ​ def train_q_learning_agent(): for Episode in range(TOTAL_EPISODES): board = np.array([Mark.EMPTY] * BOARD_SIZE) # 重置board current_mark = Mark.CROSS while not game_over(board): # Q-learning 代理 (X) 采取行动 if current_mark == Mark.CROSS: action = Choose_action_q_learning(board, Training=True) new_board = make_move_to(board, action, current_mark)reward = get_reward(new_board, current_mark) print(new_board) update_q_table(board, action,reward, new_board) # 随机玩家 (O) 采取行动 else: action = get_random_move(board) new_board = make_move_to(board, action, current_mark) ) board = new_board current_mark = Mark.NAUGHT if current_mark == Mark.CROSS else Mark.CROSS  ​ def Choose_action_q_learning(board,训练=真）-&gt; int: 如果训练且 random.uniform(0, 1) &lt; EPSILON: # 探索：选择一个随机动作 return np.random.choice(possible_moves(board)) else: # 探索：根据当前 Q 表选择最佳动作 board_key = &quot;&quot;.join(board) q_values = {操作: Q_TABLE_DICT.get((board_key, action), 0) for action in possible_moves(board) } return max(q_values, key=q_values.get)  ​ 第一集的 Q-Table 字典为 json： ​ { &quot;(&#39;_________&#39;, 0)&quot;: 0.0，“(&#39;XO_______&#39;，2)”：0.0，“(&#39;XOX____O_&#39;，3)”：0.0，“(&#39;XOXX___OO&#39;，4)”：0.0，“(&#39;XOXXXO_OO&#39; , 6)”: 0.2 }  ​   由   提交/u/faux190  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yxc1/tabular_qlearning_for_tictactoe_only_the_last/</guid>
      <pubDate>Tue, 16 Jan 2024 09:29:46 GMT</pubDate>
    </item>
    <item>
      <title>具有狄利克雷作用分布的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</link>
      <description><![CDATA[嗨！我正在通过 PPO 培训政策。该模型输出的 logits 成为狄利克雷分布的参数。这些操作的总和应为 1，并且在 [0, 1]（单纯形）范围内。问题是，随着动作大小（维度）的增加，动作的对数概率也会增加。这反过来最终会放大 ppo 使用的替代损失中的 logp 比率。 我的单纯形操作空间是长度为 400 的一维向量。对数概率通常在 2200 - 3000 的范围内。 e^(logp_1 - logp_2) 的 logp 比率会有很大的变化，从而破坏 pytorch 的梯度计算。导致看起来有效但梯度包含 NaN 值的损失。 有人知道如何在保持理论基础健全的同时抵消这个问题吗？或者也许我在某个地方的推理中犯了错误？ 提前致谢！   由   提交 /u/JMvanWestendorp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</guid>
      <pubDate>Tue, 16 Jan 2024 09:16:42 GMT</pubDate>
    </item>
    <item>
      <title>调整法学硕士与让他们接地有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197uwu3/how_is_aligning_llms_different_from_grounding_them/</link>
      <description><![CDATA[是的，这就是问题所在 - 在具体的环境中，我想知道这些任务会有什么不同。我想会有不同的政策，但在高层次上谁能解释一下发生了什么？   由   提交/u/dumber_9734   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197uwu3/how_is_aligning_llms_different_from_grounding_them/</guid>
      <pubDate>Tue, 16 Jan 2024 05:18:35 GMT</pubDate>
    </item>
    <item>
      <title>SB3 的随机启动状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</link>
      <description><![CDATA[我正在使用 SB3 的 DDPG，但在学习时无法加载具有不同启动状态的文件。我每次都尝试在重置方法中更改它。我的猜测是训练黑鬼只发生在一个情节中，因为没有调用重置方法，所以没有变化。也用 PPO 尝试过。另外，我如何控制训练次数和时间步长？ 在网上搜索的绳索结束🙂 我的代码：代码 环境：自定义 Boid 植绒 框架：开放 AI Gym   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</guid>
      <pubDate>Mon, 15 Jan 2024 23:44:37 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习：综合调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.10256 摘要：  多代理应用程序的流行遍及我们的各种互连系统日常生活。尽管它们无处不在，但在共享环境中集成和开发智能决策代理对其有效实施提出了挑战。这项调查深入研究了多智能体系统 (MAS) 领域，特别强调阐明 MAS 框架内学习最优控制的复杂性，通常称为多智能体强化学习 (MARL)。本次调查的目的是提供对 MAS 各个方面的全面见解，揭示无数机会，同时强调多代理应用程序所面临的固有挑战。我们希望不仅有助于更深入地了解 MAS 景观，而且还为研究人员和从业者提供有价值的观点。通过这样做，我们的目标是在 MAS 的动态领域内促进知情探索并促进发展，认识到在解决 MARL 中出现的复杂性方面需要适应性策略和持续发展。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</guid>
      <pubDate>Mon, 15 Jan 2024 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] 您对强化学习的真实体验是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</link>
      <description><![CDATA[ 由   提交 /u/Smallpaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</guid>
      <pubDate>Mon, 15 Jan 2024 21:31:30 GMT</pubDate>
    </item>
    </channel>
</rss>