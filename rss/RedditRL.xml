<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚çš„ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä½³åœ°è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Thu, 18 Jan 2024 03:16:16 GMT</lastBuildDate>
    <item>
      <title>â€œé€šè¿‡ç¦»æ•£æ‰©æ•£å­¦ä¹ è‡ªåŠ¨é©¾é©¶çš„æ— ç›‘ç£ä¸–ç•Œæ¨¡å‹â€ï¼ŒZhang ç­‰äºº 2023ï¼ˆMAE è§„åˆ’ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</guid>
      <pubDate>Wed, 17 Jan 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨å†³æ–—æ·±åº¦ Q ç½‘ç»œ (DQN) è®­ç»ƒä¸­æ›´æ”¹å¼ é‡ç»´åº¦</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</link>
      <description><![CDATA[æˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨ PyTorch å®ç°å†³æ–—æ·±åº¦ Q ç½‘ç»œ (DQN)ï¼Œä»¥åœ¨ Gym çš„ Ms. Pacman ç¯å¢ƒä¸­è®­ç»ƒä»£ç†ã€‚è®­ç»ƒä¼¼ä¹å¼€å§‹é¡ºåˆ©ï¼Œä½†ç»è¿‡å‡ é›†ï¼ˆç‰¹åˆ«æ˜¯å¤§çº¦ 100 é›†ä¹‹åï¼‰ï¼Œæˆ‘æ„è¯†åˆ°æ¨¡å‹çš„è¾“å…¥å¼ é‡çš„å°ºå¯¸å¼€å§‹å‘ç”Ÿå˜åŒ–ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚ I&#39; m ä½¿ç”¨åŒ…å« ObservationBufferã€ExperienceBufferã€FrameSkippingAgentã€DuelingDQN å’Œ Agent ç­‰ç±»çš„ä»£ç ç»“æ„ã€‚è¯¥æ¨¡å‹æ˜¯ä½¿ç”¨ Double DQN æ–¹æ³•è¿›è¡Œè®­ç»ƒçš„ï¼Œæˆ‘å¾ˆéš¾åœ¨å¼ é‡ç»´åº¦å˜åŒ–ä¸­è¯†åˆ«æ­¤é—®é¢˜çš„æ ¹æºã€‚ ä¸€äº›é‡è¦çš„è§‚å¯Ÿç»“æœï¼šæˆ‘æ­£åœ¨ä½¿ç”¨ GPU (CUDA) æ¥åŠ é€Ÿè®­ç»ƒã€‚ Dueling DQN æ¨¡å‹çš„è¾“å…¥è§‚å¯Ÿç»´åº¦ä¸€å¼€å§‹æ˜¯æ­£ç¡®çš„ï¼Œä½†åœ¨è¶…è¿‡ 200 ä¸ªå›åˆåå¼€å§‹å‘ç”Ÿå˜åŒ– é—®é¢˜ï¼šè®­ç»ƒæœŸé—´å¼ é‡ç»´åº¦å‘ç”Ÿè¿™äº›å˜åŒ–çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ é¢„å…ˆæ„Ÿè°¢æ‚¨æä¾›ä»»ä½•å¯èƒ½æœ‰åŠ©äºè§£å†³æ­¤é—®é¢˜çš„æŒ‡å¯¼æˆ–å»ºè®®ã€‚å¦‚æœéœ€è¦ï¼Œæˆ‘å¾ˆä¹æ„æä¾›æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚ å®Œæ•´ä»£ç ï¼šhttps://stackoverflow.com/questions/77830358/changing-tensor-dimensions-in-dueling-deep-q-network-dqn-training ç¬¬ 481 é›†ï¼Œæ€»å¥–åŠ±ï¼š300.0 ç¬¬ 482 é›†ï¼Œæ€»å¥–åŠ±ï¼š770.0 ç¬¬ 483 é›†ï¼Œæ€»å¥–åŠ±ï¼š210.0 ç¬¬ 484 é›†ï¼Œæ€»å¥–åŠ±ï¼š200.0 ç¬¬ 485 é›†ï¼Œæ€»å¥–åŠ±ï¼š280.0 è¿è¡Œæ—¶é”™è¯¯ï¼šç»™å®šç»„=1ï¼Œå¤§å°æƒé‡ [ 64, 4, 8, 8]ï¼Œé¢„æœŸè¾“å…¥[1, 84, 84, 1]æœ‰4ä¸ªé€šé“ï¼Œä½†å¾—åˆ°äº†84ä¸ªé€šé“   ç”±   æäº¤ /u/sigma_ks   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</guid>
      <pubDate>Wed, 17 Jan 2024 21:03:19 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºåŠ¨ä½œåˆ†æ”¯è®ºæ–‡çš„é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨å°è¯•æ”¹ç¼– è¿™ç¯‡è®ºæ–‡é€‚åˆæˆ‘çš„åº”ç”¨ï¼Œä½†åœ¨å®ç°è¿‡ç¨‹ä¸­ï¼Œè®ºæ–‡çš„æ–¹æ³•æœ‰äº›ä¸æ¸…æ¥šã€‚ â€‹ åœ¨ç¬¬ 4 é¡µä¸Šï¼Œæ–¹ç¨‹ 5 å’Œ 6 è®¨è®ºäº†å¦‚ä½•å°† d ç›®æ ‡å€¼ï¼ˆæ¯ä¸ªåˆ†æ”¯ 1 ä¸ªç›®æ ‡å€¼ï¼‰å‡å°‘åˆ°ä¸€ä¸ªç›®æ ‡ yã€‚  â€‹ ä½†æ˜¯ï¼Œæ–¹ç¨‹ 7 å°†æŸå¤±æè¿°ä¸º Q å€¼ä¸ç›®æ ‡ä¹‹é—´çš„å¹³æ–¹å·®ä¹‹å’Œï¼Œåˆ†åˆ«é’ˆå¯¹æ‰€æœ‰åˆ†æ”¯ï¼  â€‹ é‚£ä¹ˆï¼Œä»–ä»¬æ˜¯å¦å°†ç›®æ ‡èšåˆä¸ºä¸€ä¸ªç›®æ ‡å€¼ï¼Œä»è€Œå½¢æˆ y - Q(s, a) å½¢å¼çš„æŸå¤±æ–¹ç¨‹ï¼Œæˆ–è€…ä¸èšåˆï¼Ÿæˆ‘è¯•å›¾æ·±å…¥ç ”ç©¶ä»£ç ï¼Œä½†è¿™å¹¶æ²¡æœ‰è®©æˆ‘å˜å¾—æ›´æ˜æ™ºã€‚  â€‹ å¦‚æœæ‚¨ç†è§£è¿™ä¸€ç‚¹ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œè¿™å°†ä¼šæœ‰å·¨å¤§çš„å¸®åŠ©ï¼  &amp; #32ï¼›ç”±   æäº¤ /u/Abilitytofart   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</guid>
      <pubDate>Wed, 17 Jan 2024 17:16:34 GMT</pubDate>
    </item>
    <item>
      <title>å…³äºå¼ºåŒ–å­¦ä¹ ä¸­çš„softmaxå¯¼æ•°ï¼ˆé—®é¢˜ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198znmv/about_softmax_derivatives_in_reinforcement/</link>
      <description><![CDATA[é€‰æ‹©â€œç±»åˆ«â€æ—¶ä»é›…å¯æ¯”çŸ©é˜µä¸­æˆ‘é€‰æ‹©å“ªä¸€ä¸ªï¼Œå› ä¸ºæˆ‘ä¸çŸ¥é“å“ªä¸ªæ˜¯â€œæ­£ç¡®çš„â€ï¼Ÿè¿™é€šå¸¸ç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚   ç”±   æäº¤/u/meh_coder  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198znmv/about_softmax_derivatives_in_reinforcement/</guid>
      <pubDate>Wed, 17 Jan 2024 15:48:58 GMT</pubDate>
    </item>
    <item>
      <title>åˆ†æå¼ºåŒ–å­¦ä¹ æ³›åŒ–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /æ³›åŒ–å¼ºåŒ–å­¦ä¹    ç”±   æäº¤ /u/ml_dnn   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</guid>
      <pubDate>Wed, 17 Jan 2024 09:54:16 GMT</pubDate>
    </item>
    <item>
      <title>å¯»æ±‚å»ºè®®ä»¥åŠ å¿«ç¨³å®šåŸºçº¿ä¸‹çš„ PPO æ¨¡å‹è®­ç»ƒ3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</link>
      <description><![CDATA[å˜¿å„ä½ Reddit ç”¨æˆ·ï¼ æˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨ Stable Baselines3 è®­ç»ƒé‡‘èæ—¥äº¤æ˜“æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘&#39;æˆ‘é¢ä¸´ç€è®­ç»ƒé€Ÿåº¦çš„æŒ‘æˆ˜ã€‚æ¯å¤©ï¼ˆæ¯é›†ï¼‰æ¶‰åŠå¤§çº¦ 250 ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œå½“é‡‡å–éšæœºæ“ä½œæ—¶ï¼Œæˆ‘çš„æ¨¡æ‹Ÿå™¨å¯ä»¥åœ¨å¤§çº¦ 60-70 ç§’å†…è¿­ä»£å®ƒä»¬ã€‚ è®­ç»ƒæˆ‘çš„ PPO æ¨¡å‹æ—¶ä¼šå‡ºç°é—®é¢˜ï¼Œå› ä¸ºå®ƒéœ€è¦æ¯é›†é•¿è¾¾ 40-45 åˆ†é’Ÿã€‚æˆ‘åªåœ¨å‰§é›†ç»“æŸæ—¶æ‰§è¡Œä¸€æ¬¡æ›´æ–°ï¼Œæ²¡æœ‰æœ‰é™çš„æ°´å¹³çº¿æˆªæ–­ã€‚å½“æ¨¡æ‹Ÿå™¨å¯ä»¥åœ¨ä¸€åˆ†é’Ÿå·¦å³å®Œæˆè®­ç»ƒæ—¶ï¼Œä¸ºä»€ä¹ˆè¦èŠ±è¿™ä¹ˆé•¿æ—¶é—´æ¥è®­ç»ƒä¸€é›†ï¼Ÿæœ‰ä»€ä¹ˆæç¤ºæˆ–æŠ€å·§å¯ä»¥åŠ é€Ÿè¿™ä¸ªè®­ç»ƒè¿‡ç¨‹å—ï¼Ÿæ¥å—å»ºè®®å’Œè§è§£ï¼   ç”±   æäº¤ /u/Bunny_lad   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1983iwd/seeking_advice_to_speed_up_ppo_model_training_in/</guid>
      <pubDate>Tue, 16 Jan 2024 14:00:29 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å­¦ä¹ çŠ¯ç½ªå­¦ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1980su2/how_to_study_criminology/</link>
      <description><![CDATA[å¦‚ä½•å­¦ä¹ çŠ¯ç½ªå­¦ï¼Ÿ å—¨ï¼Œæˆ‘çš„æœ‹å‹æƒ³åœ¨ç¾å›½å­¦ä¹ çŠ¯ç½ªå­¦ã€‚æˆ‘ä»¬ä¸çŸ¥é“æœ‰ä»€ä¹ˆè¦æ±‚ã€è€ƒè¯•ä»¥åŠå“ªäº›å¤§å­¦æœ‰è¿™æ ·çš„æ•™å¸ˆã€‚å¥¹æ”»è¯»è®¾è®¡å­¦å£«å­¦ä½ï¼Œæƒ³è¦æ”»è¯»æœ¬ç§‘çŠ¯ç½ªå­¦ã€‚è¯·å¸®åŠ©æˆ‘ä»¬ï¼Œå¥¹è¯¥å¦‚ä½•å¼€å§‹ï¼Ÿ ï¼ˆå¥¹ä¸åœ¨ç¾å›½ï¼‰   ç”±   æäº¤ /u/DevilSummoned   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1980su2/how_to_study_criminology/</guid>
      <pubDate>Tue, 16 Jan 2024 11:30:39 GMT</pubDate>
    </item>
    <item>
      <title>ç¥ç»ç½‘ç»œå¯ä»¥å¤„ç†é«˜äº 1 çš„å¥–åŠ±å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19809re/can_a_neural_network_handle_rewards_above_1/</link>
      <description><![CDATA[æˆ‘çŸ¥é“åœ¨ -1 å’Œ 1 ä¹‹é—´ä¼ é€’å€¼å¯ä»¥æé«˜ç¨³å®šæ€§ï¼Œä½†æˆ‘æƒ³çŸ¥é“æ¨¡å‹æ˜¯å¦å¯ä»¥å®¹å¿ä¼ é€’æ›´é«˜çš„å€¼ï¼Ÿä¸å¹¸çš„æ˜¯æˆ‘ç°åœ¨æ²¡æœ‰ç¯å¢ƒå¯ä»¥æµ‹è¯•å®ƒ   ç”±   æäº¤ /u/sogha   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19809re/can_a_neural_network_handle_rewards_above_1/</guid>
      <pubDate>Tue, 16 Jan 2024 10:58:32 GMT</pubDate>
    </item>
    <item>
      <title>PPO ç‰¹å·¥ä¸éšæœºç©å®¶è¿›è¡Œç¥å¥‡å®è´å¯¹å†³ã€‚ä½ çŸ¥é“ä¸ºä»€ä¹ˆå¹³å‡å¥–åŠ±å¦‚æ­¤ä¸ç¨³å®šå—ï¼Ÿ lr=1e-3ï¼Œ8 ä¸ªå¹¶è¡Œç¯å¢ƒåœ¨è®­ç»ƒå‰è¿›è¡Œ 60 æ¬¡ç§»åŠ¨ï¼Œnum_epochs=3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</link>
      <description><![CDATA[       ç”±   æäº¤ /u/moisturemeister   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197zbyt/ppo_agent_playing_pokemon_showdown_vs_random/</guid>
      <pubDate>Tue, 16 Jan 2024 09:57:47 GMT</pubDate>
    </item>
    <item>
      <title>TicTacToe çš„è¡¨æ ¼ Q-Learning - ä»…æœ€åä¸€ä¸ªçŠ¶æ€/åŠ¨ä½œå¯¹å­˜å‚¨åœ¨ Q-Table å­—å…¸ä¸­ï¼Œå…¶å€¼ä¸ä¸º 0</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yxc1/tabular_qlearning_for_tictactoe_only_the_last/</link>
      <description><![CDATA[æˆ‘åœ¨ tictactoe 3x3 æ¿çš„è¡¨æ ¼ q-learning å®ç°ä¸­é‡åˆ°é—®é¢˜ã€‚ â€‹ é—®é¢˜åœ¨äºï¼Œåªæœ‰æœ€åä¸€æ­¥ï¼ˆè·èƒœã€å¤±è´¥ã€å¹³å±€ï¼‰åŠå…¶å„è‡ªçš„æ£‹ç›˜çŠ¶æ€å­˜å‚¨åœ¨ q å€¼ä¸æ˜¯â€œ0.0â€çš„ q è¡¨ä¸­ã€‚å¯¼è‡´æœ€åç§»åŠ¨çš„æ‰€æœ‰å…¶ä»–çŠ¶æ€å’ŒåŠ¨ä½œå¯¹ä»ç„¶å…·æœ‰å€¼â€œ0.0â€ã€‚æˆ‘åœ¨ä¸‹é¢æ·»åŠ äº† q è¡¨ï¼Œå…¶ä¸­æ˜¾ç¤ºæœ€åä¸€æ­¥çš„å€¼ä¸ºâ€œ0.2â€ã€‚ä½†ä¹‹å‰æ‰€æœ‰çš„ç§»åŠ¨çš„å€¼ä¸ºâ€œ0.0â€ã€‚è¿™åªæ˜¯ç¬¬ä¸€é›†ã€‚å³ä½¿å¢åŠ äº†å‰§é›†ä¹Ÿä¸ä¼šæ”¹å˜ä»»ä½•äº‹æƒ…ã€‚åªæœ‰æœ€åä¸€ä¸ªåŠ¨ä½œçš„ q å€¼ä¸æ˜¯â€œ0.0â€ â€‹ éå¸¸æ„Ÿè°¢ä»»ä½•å¸®åŠ©ã€‚æˆ‘èŠ±äº†å‡ å¤©æ—¶é—´å°è¯•ä¿®å¤å®ƒ... :( class Mark(enum.StrEnum): CROSS = &quot;X&quot; NAUGHT = &quot;O&quot;; EMPTY = &quot;; _&quot; class Reward(enum.IntEnum): WIN = 1 LosE = -1 TIE = -0.065 NON_TERMINAL = -0.01 # Q-Learning å¸¸é‡ EPSILON = 0.1 # æ¢ç´¢å› å­ ALPHA = 0.2 # å­¦ä¹ ç‡ GAMMA = 0.95 # æŠ˜æ‰£å› å­ TOTAL_EPISODES = 1 # ä»£ç†å°†ç©çš„æ¸¸æˆæ€»æ•° BOARD = np.array([Mark.EMPTY] * BOARD_SIZE)  â€‹  def update_q_table(board,action,reward,new_board): board_key = &quot;&quot;.join(board) new_board_key = &quot;&quot;.join(new_board) old_value = Q_TABLE_DICT.get((board_key, action), 0) if game_over (new_board): # å¦‚æœæ˜¯æœ€ç»ˆçŠ¶æ€ï¼Œåˆ™æ²¡æœ‰æœªæ¥å¥–åŠ±å¯ä»¥è€ƒè™‘ next_max = 0 else: # ä¼°è®¡æœ€ä¼˜æœªæ¥å€¼ next_max = max( Q_TABLE_DICT.get((new_board_key, a), 0) for a in possible_moves( new_board) ) # ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ›´æ–°å½“å‰çŠ¶æ€-åŠ¨ä½œå¯¹çš„ Q å€¼ q_value = old_value + ALPHA * (reward + GAMMA * next_max - old_value) Q_TABLE_DICT[(board_key, action)] = q_value &lt; /pre&gt; â€‹ def train_q_learning_agent(): for Episode in range(TOTAL_EPISODES): board = np.array([Mark.EMPTY] * BOARD_SIZE) # é‡ç½®board current_mark = Mark.CROSS while not game_over(board): # Q-learning ä»£ç† (X) é‡‡å–è¡ŒåŠ¨ if current_mark == Mark.CROSS: action = Choose_action_q_learning(board, Training=True) new_board = make_move_to(board, action, current_mark)reward = get_reward(new_board, current_mark) print(new_board) update_q_table(board, action,reward, new_board) # éšæœºç©å®¶ (O) é‡‡å–è¡ŒåŠ¨ else: action = get_random_move(board) new_board = make_move_to(board, action, current_mark) ) board = new_board current_mark = Mark.NAUGHT if current_mark == Mark.CROSS else Mark.CROSS  â€‹ def Choose_action_q_learning(board,è®­ç»ƒ=çœŸï¼‰-&gt; int: å¦‚æœè®­ç»ƒä¸” random.uniform(0, 1) &lt; EPSILON: # æ¢ç´¢ï¼šé€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œ return np.random.choice(possible_moves(board)) else: # æ¢ç´¢ï¼šæ ¹æ®å½“å‰ Q è¡¨é€‰æ‹©æœ€ä½³åŠ¨ä½œ board_key = &quot;&quot;.join(board) q_values = {æ“ä½œ: Q_TABLE_DICT.get((board_key, action), 0) for action in possible_moves(board) } return max(q_values, key=q_values.get)  â€‹ ç¬¬ä¸€é›†çš„ Q-Table å­—å…¸ä¸º jsonï¼š â€‹ { &quot;(&#39;_________&#39;, 0)&quot;: 0.0ï¼Œâ€œ(&#39;XO_______&#39;ï¼Œ2)â€ï¼š0.0ï¼Œâ€œ(&#39;XOX____O_&#39;ï¼Œ3)â€ï¼š0.0ï¼Œâ€œ(&#39;XOXX___OO&#39;ï¼Œ4)â€ï¼š0.0ï¼Œâ€œ(&#39;XOXXXO_OO&#39; , 6)â€: 0.2 }  â€‹   ç”±   æäº¤/u/faux190  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yxc1/tabular_qlearning_for_tictactoe_only_the_last/</guid>
      <pubDate>Tue, 16 Jan 2024 09:29:46 GMT</pubDate>
    </item>
    <item>
      <title>å…·æœ‰ç‹„åˆ©å…‹é›·ä½œç”¨åˆ†å¸ƒçš„ PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</link>
      <description><![CDATA[å—¨ï¼æˆ‘æ­£åœ¨é€šè¿‡ PPO åŸ¹è®­æ”¿ç­–ã€‚è¯¥æ¨¡å‹è¾“å‡ºçš„ logits æˆä¸ºç‹„åˆ©å…‹é›·åˆ†å¸ƒçš„å‚æ•°ã€‚è¿™äº›æ“ä½œçš„æ€»å’Œåº”ä¸º 1ï¼Œå¹¶ä¸”åœ¨ [0, 1]ï¼ˆå•çº¯å½¢ï¼‰èŒƒå›´å†…ã€‚é—®é¢˜æ˜¯ï¼Œéšç€åŠ¨ä½œå¤§å°ï¼ˆç»´åº¦ï¼‰çš„å¢åŠ ï¼ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ä¹Ÿä¼šå¢åŠ ã€‚è¿™åè¿‡æ¥æœ€ç»ˆä¼šæ”¾å¤§ ppo ä½¿ç”¨çš„æ›¿ä»£æŸå¤±ä¸­çš„ logp æ¯”ç‡ã€‚ æˆ‘çš„å•çº¯å½¢æ“ä½œç©ºé—´æ˜¯é•¿åº¦ä¸º 400 çš„ä¸€ç»´å‘é‡ã€‚å¯¹æ•°æ¦‚ç‡é€šå¸¸åœ¨ 2200 - 3000 çš„èŒƒå›´å†…ã€‚ e^(logp_1 - logp_2) çš„ logp æ¯”ç‡ä¼šæœ‰å¾ˆå¤§çš„å˜åŒ–ï¼Œä»è€Œç ´å pytorch çš„æ¢¯åº¦è®¡ç®—ã€‚å¯¼è‡´çœ‹èµ·æ¥æœ‰æ•ˆä½†æ¢¯åº¦åŒ…å« NaN å€¼çš„æŸå¤±ã€‚ æœ‰äººçŸ¥é“å¦‚ä½•åœ¨ä¿æŒç†è®ºåŸºç¡€å¥å…¨çš„åŒæ—¶æŠµæ¶ˆè¿™ä¸ªé—®é¢˜å—ï¼Ÿæˆ–è€…ä¹Ÿè®¸æˆ‘åœ¨æŸä¸ªåœ°æ–¹çš„æ¨ç†ä¸­çŠ¯äº†é”™è¯¯ï¼Ÿ æå‰è‡´è°¢ï¼   ç”±   æäº¤ /u/JMvanWestendorp   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197yqqj/ppo_with_dirichlet_action_distribution/</guid>
      <pubDate>Tue, 16 Jan 2024 09:16:42 GMT</pubDate>
    </item>
    <item>
      <title>è°ƒæ•´æ³•å­¦ç¡•å£«ä¸è®©ä»–ä»¬æ¥åœ°æœ‰ä½•ä¸åŒï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197uwu3/how_is_aligning_llms_different_from_grounding_them/</link>
      <description><![CDATA[æ˜¯çš„ï¼Œè¿™å°±æ˜¯é—®é¢˜æ‰€åœ¨ - åœ¨å…·ä½“çš„ç¯å¢ƒä¸­ï¼Œæˆ‘æƒ³çŸ¥é“è¿™äº›ä»»åŠ¡ä¼šæœ‰ä»€ä¹ˆä¸åŒã€‚æˆ‘æƒ³ä¼šæœ‰ä¸åŒçš„æ”¿ç­–ï¼Œä½†åœ¨é«˜å±‚æ¬¡ä¸Šè°èƒ½è§£é‡Šä¸€ä¸‹å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ   ç”±   æäº¤/u/dumber_9734   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197uwu3/how_is_aligning_llms_different_from_grounding_them/</guid>
      <pubDate>Tue, 16 Jan 2024 05:18:35 GMT</pubDate>
    </item>
    <item>
      <title>SB3 çš„éšæœºå¯åŠ¨çŠ¶æ€</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä½¿ç”¨ SB3 çš„ DDPGï¼Œä½†åœ¨å­¦ä¹ æ—¶æ— æ³•åŠ è½½å…·æœ‰ä¸åŒå¯åŠ¨çŠ¶æ€çš„æ–‡ä»¶ã€‚æˆ‘æ¯æ¬¡éƒ½å°è¯•åœ¨é‡ç½®æ–¹æ³•ä¸­æ›´æ”¹å®ƒã€‚æˆ‘çš„çŒœæµ‹æ˜¯è®­ç»ƒé»‘é¬¼åªå‘ç”Ÿåœ¨ä¸€ä¸ªæƒ…èŠ‚ä¸­ï¼Œå› ä¸ºæ²¡æœ‰è°ƒç”¨é‡ç½®æ–¹æ³•ï¼Œæ‰€ä»¥æ²¡æœ‰å˜åŒ–ã€‚ä¹Ÿç”¨ PPO å°è¯•è¿‡ã€‚å¦å¤–ï¼Œæˆ‘å¦‚ä½•æ§åˆ¶è®­ç»ƒæ¬¡æ•°å’Œæ—¶é—´æ­¥é•¿ï¼Ÿ åœ¨ç½‘ä¸Šæœç´¢çš„ç»³ç´¢ç»“æŸğŸ™‚ æˆ‘çš„ä»£ç ï¼šä»£ç  ç¯å¢ƒï¼šè‡ªå®šä¹‰ Boid æ¤ç»’ æ¡†æ¶ï¼šå¼€æ”¾ AI Gym   ç”±   æäº¤/u/Sadboi1010   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197nwq0/random_start_state_with_sb3/</guid>
      <pubDate>Mon, 15 Jan 2024 23:44:37 GMT</pubDate>
    </item>
    <item>
      <title>å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼šç»¼åˆè°ƒæŸ¥</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</link>
      <description><![CDATA[è®ºæ–‡ï¼šhttps:// arxiv.org/abs/2312.10256 æ‘˜è¦ï¼š  å¤šä»£ç†åº”ç”¨ç¨‹åºçš„æµè¡ŒéåŠæˆ‘ä»¬çš„å„ç§äº’è¿ç³»ç»Ÿæ—¥å¸¸ç”Ÿæ´»ã€‚å°½ç®¡å®ƒä»¬æ— å¤„ä¸åœ¨ï¼Œä½†åœ¨å…±äº«ç¯å¢ƒä¸­é›†æˆå’Œå¼€å‘æ™ºèƒ½å†³ç­–ä»£ç†å¯¹å…¶æœ‰æ•ˆå®æ–½æå‡ºäº†æŒ‘æˆ˜ã€‚è¿™é¡¹è°ƒæŸ¥æ·±å…¥ç ”ç©¶äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (MAS) é¢†åŸŸï¼Œç‰¹åˆ«å¼ºè°ƒé˜æ˜ MAS æ¡†æ¶å†…å­¦ä¹ æœ€ä¼˜æ§åˆ¶çš„å¤æ‚æ€§ï¼Œé€šå¸¸ç§°ä¸ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (MARL)ã€‚æœ¬æ¬¡è°ƒæŸ¥çš„ç›®çš„æ˜¯æä¾›å¯¹ MAS å„ä¸ªæ–¹é¢çš„å…¨é¢è§è§£ï¼Œæ­ç¤ºæ— æ•°æœºä¼šï¼ŒåŒæ—¶å¼ºè°ƒå¤šä»£ç†åº”ç”¨ç¨‹åºæ‰€é¢ä¸´çš„å›ºæœ‰æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¸Œæœ›ä¸ä»…æœ‰åŠ©äºæ›´æ·±å…¥åœ°äº†è§£ MAS æ™¯è§‚ï¼Œè€Œä¸”è¿˜ä¸ºç ”ç©¶äººå‘˜å’Œä»ä¸šè€…æä¾›æœ‰ä»·å€¼çš„è§‚ç‚¹ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ MAS çš„åŠ¨æ€é¢†åŸŸå†…ä¿ƒè¿›çŸ¥æƒ…æ¢ç´¢å¹¶ä¿ƒè¿›å‘å±•ï¼Œè®¤è¯†åˆ°åœ¨è§£å†³ MARL ä¸­å‡ºç°çš„å¤æ‚æ€§æ–¹é¢éœ€è¦é€‚åº”æ€§ç­–ç•¥å’ŒæŒç»­å‘å±•ã€‚    ç”±   æäº¤ /u/APaperADay   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197lq1j/multiagent_reinforcement_learning_a_comprehensive/</guid>
      <pubDate>Mon, 15 Jan 2024 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>[D] æ‚¨å¯¹å¼ºåŒ–å­¦ä¹ çš„çœŸå®ä½“éªŒæ˜¯ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</link>
      <description><![CDATA[ ç”±   æäº¤ /u/Smallpaul   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/197kl7z/d_what_is_your_honest_experience_with/</guid>
      <pubDate>Mon, 15 Jan 2024 21:31:30 GMT</pubDate>
    </item>
    </channel>
</rss>