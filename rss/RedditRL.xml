<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 06 May 2024 21:13:26 GMT</lastBuildDate>
    <item>
      <title>QT-Opt/CEM 与 SAC 的实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clrbyr/qtoptcem_vs_sac_in_practice/</link>
      <description><![CDATA[看来 SAC 可能是目前最流行的离策略方法。然而，QT-Opt 论文建议使用交叉熵方法 (CEM)，而不是演员批评方法（大概是 SAC/TD3/等）。 本演示文稿链接到多项研究，这些研究表明 CEM 正在发挥作用-与 TD3 相当。 我很好奇是否有人有使用 CEM 的经验，以及它在实践中与 SAC/TD3 相比如何。 CEM 的随机性与可调采样参数的结合似乎可以防止高估问题并引入更多探索。我猜想在 CEM 中优化 Q 函数也比 SAC/TD3 中 Actor 和 Critic 的迭代优化更稳定。   由   提交 /u/smorad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clrbyr/qtoptcem_vs_sac_in_practice/</guid>
      <pubDate>Mon, 06 May 2024 19:11:07 GMT</pubDate>
    </item>
    <item>
      <title>在并行环境中求解成本函数的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clpjpy/advice_for_solving_a_cost_function_in_parallel/</link>
      <description><![CDATA[我有一个二次问题，其中成本函数是最小化变量和标称值之间的差异，而等式约束是动态的。 我想要做的是在 RL 环境中实现这一点，但也要并行实现。我发​​现在多个并行环境中解决这个问题相当困难（source）。 因此，我尝试通过将这个 QP 转换为奖励函数来实现它，例如，如果差异变大，则将原始成本函数设置为惩罚，如果所有项加起来为零，则将等式约束设置为奖励。 但策略不会立即按照我想要的方式（即接近标称值）返回变量。我部分认为问题在于策略需要时间来学习这个“优化问题”我想要实现的一个解决方案，但另一方面，我仍然很难理解为什么它不能立即获得如此简单的 QP。 因此，如果我能从以下方面获得任何建议，我将不胜感激，例如但不限于：  在多并行环境（如数千个）中解决 QP 的方法 在 RL 环境中实现此优化的其他方法  这是我第一次在 reddit 上发布问题，真诚希望从这里获得任何帮助......谢谢！    提交人    /u/Open-Safety-1585   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clpjpy/advice_for_solving_a_cost_function_in_parallel/</guid>
      <pubDate>Mon, 06 May 2024 17:58:29 GMT</pubDate>
    </item>
    <item>
      <title>RLlib 多智能体并行的复杂观察空间包装</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clfqo7/complex_observation_spaces_wrapping_for_rllib/</link>
      <description><![CDATA[大家好， 我正在使用 RLlib 开发一个多代理环境，其中每个代理的观察空间是使用 spaces.Dict 。我的环境设置运行良好，直到与 RLlib 的训练循环集成为止，在集成中我遇到了与处理这些复杂空间相关的反复出现的问题。 我使用 spaces.Dict 定义代理的观察空间 code&gt; 其中包括各种子空间，例如 spaces.Box 和 spaces.MultiBinary 。因为我试图找到空格错误的解决方案，所以我有几个定义。我附上观察和行动空间的定义。我意识到我必须包装复杂的观察结果，但我对此还很陌生。 所以我的问题是，我应该如何包装它们或修改它们以便能够在并行多代理中使用它们训练？ self._observation_spaces =spaces.Dict(observer_satellites = space.Box(low=-np.inf, high=np.inf, shape=(self.num_observers, len(self. Orbital_params_order))), target_satellites = space.Box(low=-np.inf, high=np.inf, shape=(self.num_targets, len(self.orbital_params_order_targets))), 可用性 =spaces.MultiBinary(1), 电池= space.Box(low=0, high=1, shape=(self.num_observers, 1)), storage =spaces.Box(low=0, high=1, shape=(self.num_observers, 1)), Observation_status = space.Box(low=0, high=3, shape=(self.num_targets, )), pointing_accuracy = space.Box(low=-np.inf, high=np.inf, shape=(self.num_observers, self .nu​​m_targets)), communications_status = space.Box(low=0, high=1, shape=(self.num_observers, ), dtype=np.int8), communications_ability = space.MultiBinary(self.num_observers) ) self._action_spaces = space.Discrete(2 + self.num_targets) self.infos = {agent: {} for self.possible_agents 中的代理} @property def Observation_spaces(self): return {agent: self.observation_space(agent) for self.possible_agents 中的代理} @property def action_spaces(self): return {agent: self.action_space(agent) for agent in self.possible_agents} @functools.lru_cache(maxsize=None) def Observation_space(self, agent): print(f&quot;返回观察空间for {agent}: {self._observation_spaces}&quot;) return self._observation_spaces @functools.lru_cache(maxsize=None) def action_space(self, agent): print(f&quot;返回 {agent}: {self._action_spaces 的动作空间}&quot;) return self._action_spaces    由   提交/u/CLEMENMAN   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clfqo7/complex_observation_spaces_wrapping_for_rllib/</guid>
      <pubDate>Mon, 06 May 2024 10:29:51 GMT</pubDate>
    </item>
    <item>
      <title>动作空间逻辑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1clck2s/action_space_logic/</link>
      <description><![CDATA[我目前正在构建 RL 环境。状态空间是 3 维的，动作空间是 1 维的。在这种环境中，智能体选择的动作是下一个状态的第三个要素。由于动作直接是状态空间中的元素，是否存在可能导致的任何问题（即缺乏学习或困难探索问题）？  &amp; #32；由   提交/u/Key-Scientist-3980   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1clck2s/action_space_logic/</guid>
      <pubDate>Mon, 06 May 2024 06:45:15 GMT</pubDate>
    </item>
    <item>
      <title>希望与参加多伦多 DLRL 暑期学校的人们建立联系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cl5g41/looking_to_connect_with_people_attending_dlrl/</link>
      <description><![CDATA[嗨！我想与即将参加今年夏天（2024 年 7 月）在多伦多举办的 CIFAR DLRL 暑期学校的人们取得联系   由   提交/u/anam_812  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cl5g41/looking_to_connect_with_people_attending_dlrl/</guid>
      <pubDate>Mon, 06 May 2024 00:04:32 GMT</pubDate>
    </item>
    <item>
      <title>熵损失计算。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cl1vbk/entropy_loss_calculation/</link>
      <description><![CDATA[我的 PPO 代理在熵损失计算方面存在问题。如果我们看一下熵计算，它只是动作总和乘以动作日志的平均值。但是神经网络可以输出负数，并将动作限制为不低于零，或者只是为了熵而这样做，与熵混淆并增加损失，即使它在负数中探索，损失也会增加，这会继续产生爆炸坡度。我该如何解决这个问题，我尝试只是去除熵，但我的代理非常不稳定，它的学习完全取决于权重初始化。   由   提交/u/meh_coder  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cl1vbk/entropy_loss_calculation/</guid>
      <pubDate>Sun, 05 May 2024 21:23:20 GMT</pubDate>
    </item>
    <item>
      <title>ICML 2024 上的第一届情境学习 (ICL) 研讨会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cksue3/1st_workshop_on_incontext_learning_icl_at_icml/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cksue3/1st_workshop_on_incontext_learning_icl_at_icml/</guid>
      <pubDate>Sun, 05 May 2024 14:44:35 GMT</pubDate>
    </item>
    <item>
      <title>[R][D]更新价值函数时是否应该考虑探索步骤？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckpy7a/rdshould_exploration_step_be_considered_when/</link>
      <description><![CDATA[我问了GPT，它说有些方法更新所有步骤的值函数，而其他方法只更新那些与探索步骤无关的方法，我知道这两种方法是完全不同，但它们都有效，为什么呢？他们的本质区别是什么？欢迎任何意见，任何讨论都会很棒！   由   提交 /u/CrisYou   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckpy7a/rdshould_exploration_step_be_considered_when/</guid>
      <pubDate>Sun, 05 May 2024 12:16:49 GMT</pubDate>
    </item>
    <item>
      <title>利用 Kolmogorov-Arnold 网络使强化学习策略变得可解释！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckovv0/making_rl_policy_interpretable_with/</link>
      <description><![CDATA[   /u/riiswa   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckovv0/making_rl_policy_interpretable_with/</guid>
      <pubDate>Sun, 05 May 2024 11:11:42 GMT</pubDate>
    </item>
    <item>
      <title>如何解决这个数学难题，寻找学习而不是难题的答案。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cknjd9/how_to_solve_this_math_puzzle_looking_to_learn/</link>
      <description><![CDATA[大家好， 所以有一个数学难题，如下所示。 至目标是种植农作物赚尽可能多的钱。您有 100 个学分可以开始。您有 4 种作物要种植。每种作物都有不同的生长时间、成本和支出。没有其他外部因素。只有 60 个月（轮）可用。而且只能在60个月内收获。所以在第 60 个月种植显然是浪费信用。 这个谜题有一个完美的玩法，可能是暴力破解。但我认为这可能是尝试机器学习的好时机。我尝试过q-learning，但没有得到好的结果。结果似乎并没有收敛（如果这是正确的词） 我有行动  什么都不做 &lt; li&gt;种植农作物 1，奖励 +1 种植农作物 2，奖励 +3 种植农作物 3，奖励 +5 种植农作物 4，奖励 + 20 收获（第一次收获+10，第二次收获-20（第二次收获没有任何东西） 移至下一个月（0 奖励或 -10，如果移动到下个月且还有大量预算） 移动到下个月，收获将合并到一个操作中。  作为环境，我只有预算。 1：Q-learning 是为此设置的还是我开始的方向错误？ 2：我的方法看起来像吗？好（动作和环境）还是我错过了如何实现这些的要点。 编辑：添加了谜题描述 起始信用：100 农作物，成本、回报、生长时间 作物 1、10、12、2 个月 作物 2、5、8、4 个月 作物 3、15、25、4 个月 作物4、175、500、6 个月。 总的生长时间是 5 年。所以 60 个月。 你只能收获完全成熟的作物。生长时间&gt;=剩余月份。你可以种植农作物，但这只是浪费。 你种植农作物的月份算作第一个月。收获发生在每个月底。因此，如果您在第 1 个月种植作物 1，您可以在第 2 个月收获它并在第 3 个月花掉它。 编辑：删除不执行任何操作。联合收获并移至下个月。   由   提交/u/readtheroompeople  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cknjd9/how_to_solve_this_math_puzzle_looking_to_learn/</guid>
      <pubDate>Sun, 05 May 2024 09:37:05 GMT</pubDate>
    </item>
    <item>
      <title>Lily 在 Unitu ML Agents 和强化学习的帮助下学会走路</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckloat/lily_learns_to_walk_with_the_help_of_unitu_ml/</link>
      <description><![CDATA[      https://github.com/denisgriaznov/ReinforcementLearningSpyderWithUnityMLAgents   由   提交/u/Repulsive_Air5342   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckloat/lily_learns_to_walk_with_the_help_of_unitu_ml/</guid>
      <pubDate>Sun, 05 May 2024 07:22:19 GMT</pubDate>
    </item>
    <item>
      <title>海报展示的有趣热门话题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckht5o/interesting_and_hot_topics_for_a_poster_session/</link>
      <description><![CDATA[大家好。我这个学期在大学有强化学习课程，我们有一个海报会议，我们应该在其中搜索高级强化学习方法和类似的东西，并阅读有关该主题的新论文。显然，随着LLM的炒作，与其相关的科目也被提前选择了。您能否建议一些关于 RL 的新的、有趣的主题，或者建议我如何找到它们。提前谢谢了。    由   提交 /u/Brief-Emotion6291    reddit.com/r/reinforcementlearning/comments/1ckht5o/interesting_and_hot_topics_for_a_poster_session/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckht5o/interesting_and_hot_topics_for_a_poster_session/</guid>
      <pubDate>Sun, 05 May 2024 03:12:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么不直接使用 PPO 中上一步的价值目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckg0f8/why_not_just_use_the_value_target_from_the/</link>
      <description><![CDATA[对于PPO目标函数，为什么我们不能通过从目标状态值减去最后一个时间步的目标状态值来计算优势当前时间步长而不是训练网络来估计前一个时间步长的状态值？   由   提交 /u/MfkinBad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckg0f8/why_not_just_use_the_value_target_from_the/</guid>
      <pubDate>Sun, 05 May 2024 01:32:05 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关使用 DQN 的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ckcy7p/looking_for_advice_on_using_dqn/</link>
      <description><![CDATA[你好！ 我在强化学习方面不是很有经验，所以我非常感谢任何一般性的提示和建议。总而言之，我遇到了算法不收敛的事实，并且我不太清楚我还可以调整哪些内容来避免这种情况。接下来将为感兴趣的人提供更多上下文和数据。 我正在尝试使用 DQN 和修改（特别是 RainbowDQN）通过在 ARGoS 中进行模拟来解决觅食问题。机器人必须在环境中找到一个物体并带着它返回基地，不断重复。尽管损失似乎随着时间的推移而减少，但它对最终的奖励值和机器人行为影响不大。奖励看起来非常随机，尽管机器人获得了很高的奖励，但大多数情况下，情节在达到负奖励后都会以游戏结束结束。训练有素的机器人虽然似乎理解任务，找到物体，捡起它们，转身，到达基地，但通常只是随机旋转（当我将机器人的旋转角度呈现为余弦和正弦时，我以为我已经解决了这个问题）以避免 pi 突然更改为 -pi 的问题，但不是），并且不太注意最近的可见物体，可能只是开车到它们所在的区域。出现了以下问题： 1.我的奖励系统是否有问题？这个环境与我之前遇到的环境有些不同：在迭代之间，模拟中没有发生太多事情，并且算法可能很难看到路径之间的依赖关系以达到目标和任务的完成。因此，除了对拾取和交付物体给予奖励之外，我还尝试通过奖励朝正确方向移动的代理并惩罚其选择错误的方向来激励代理。这是一个好的解决方案吗？ 2.我应该以哪种方式更改超参数？我尝试过不同的层配置，较小的网络似乎处理得更差，但较大的网络也没有任何好处。批量大小看起来也合适。  超参数： 命名空间(seed=1626, eps_test=0.05, eps_train=0.1, buffer_size=20000, lr=0.0003, gamma= 0.9、n_step=3、target_update_freq=320、epoch=150、step_per_epoch=1500、step_per_collect=50、update_per_step=0.1、batch_size=64、hidden_​​sizes=[512、256、256、128]、training_num=10、test_num=10）  3.我使用的数据是否足够？网络接收机器人的位置和方向、到最近可见物体的方向向量、到底座的信息、是否在底座上、是否携带物体、是否发生了碰撞。我认为我不能在这里添加任何内容。 生成的图表：https://imgur.com/a/ WEg7ywi 感谢您的宝贵时间。   由   提交 /u/Legendorik   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ckcy7p/looking_for_advice_on_using_dqn/</guid>
      <pubDate>Sat, 04 May 2024 22:58:49 GMT</pubDate>
    </item>
    <item>
      <title>大型非矢量化环境是否等同于小型矢量化环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ck32o2/are_large_nonvectorized_environments_equivalent/</link>
      <description><![CDATA[RL新手，刚刚接触到术语矢量化环境。 基于此SO回答我了解矢量化环境每一步接收多个环境。 矢量化环境是否只有助于加快训练速度（代理训练得更快）？或者它是否也有助于提高训练质量（矢量化训练可以带来更好的智能体）？ 这些训练环境在质量上是否相同？ 矢量化环境：每步 10 个环境，训练1,000 步。总共有 10,000 个环境。 非矢量化环境：每步 1 个环境，但训练有 10,000 个步骤。还有 10,000 总环境。 在这两种环境中训练的智能体大致相同吗？或者矢量化环境会带来更好的训练代理吗？   由   提交/u/mokenyon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ck32o2/are_large_nonvectorized_environments_equivalent/</guid>
      <pubDate>Sat, 04 May 2024 15:36:03 GMT</pubDate>
    </item>
    </channel>
</rss>