<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 21 Nov 2024 21:16:11 GMT</lastBuildDate>
    <item>
      <title>如何开始机器人操作器的强化学习研究？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</link>
      <description><![CDATA[你好， 我是一名研究生，对应用人工智能技术（特别是强化学习）来控制机器人操纵器（机械臂）感兴趣。 为了做到这一点，我不知道从哪里开始学习并决定研究主题。  有哪些基础论文和资源可以帮助我了解这个领域？ 有哪些最近的评论或调查论文可以帮助我了解该领域的现状？ 或者，为了研究人工智能机器人技术，我应该阅读哪些论文？   如有任何建议或意见，我们将不胜感激！ 谢谢！ 使用 DeepL.com （免费版） 进行翻译   提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwn8aw/how_to_start_research_in_reinforcement_learning/</guid>
      <pubDate>Thu, 21 Nov 2024 18:59:33 GMT</pubDate>
    </item>
    <item>
      <title>帮我用 Unity3D 制作这辆 DDPG 自动驾驶汽车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</link>
      <description><![CDATA[我被这个项目困住了，我不知道我哪里做错了，可能是在脚本中，也可能是在 Unity 中。请帮助我解决和调试问题。请直接给我发私信，获取脚本和更多信息。    提交人    /u/pendalkumar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwgjms/help_me_with_this_ddpg_self_driving_car_made_with/</guid>
      <pubDate>Thu, 21 Nov 2024 13:41:59 GMT</pubDate>
    </item>
    <item>
      <title>另一个调试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</link>
      <description><![CDATA[大家好， 我正在解决声音领域中具有连续动作的问题。 该模型是一个表示声音的 CNN。表示与一些参数一起被输入到 MLP 以获得值和动作。 在研究损失函数（在我们的例子中是奖励）之后，它是参数和动作的凸函数。我的意思是，对于给定的参数 + 声音，作为动作函数的奖励信号是凸的。 不幸的是，我们偶然发现了一个可以实现收敛的网络参数的良好初始化。问题是模型几乎一直都不会收敛。 如何调试问题的根源？我是否只需要等待足够长的时间？我是否扩大了模型？ 谢谢 编辑：我意识到我没有指定我正在使用的算法。PPO，A2C，Reinforce，OptionCritic，PPOC。 所有这些算法的作用本质上都是相同的。    提交人    /u/sagivborn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwcl8l/yet_another_debugging_question/</guid>
      <pubDate>Thu, 21 Nov 2024 09:43:42 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 epymarl 来运行我的模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gwbkq1/how_can_i_use_epymarl_to_run_my_model/</link>
      <description><![CDATA[我尝试通过 README 做一些事情，但是无法成功。有人能帮我吗，如何通过 README 注册我自己的环境，谢谢。    提交人    /u/NationalBat6637   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gwbkq1/how_can_i_use_epymarl_to_run_my_model/</guid>
      <pubDate>Thu, 21 Nov 2024 08:26:40 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 Agent 进行国际象棋之类的游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gw7itl/how_do_you_train_agent_for_something_like_chess/</link>
      <description><![CDATA[到目前为止我还没有做过任何 RL，我想开始使用 RL 研究类似国际象棋模型的东西，但不知道从哪里开始    提交人    /u/Ok_Orchid_7408   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gw7itl/how_do_you_train_agent_for_something_like_chess/</guid>
      <pubDate>Thu, 21 Nov 2024 04:07:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在深度强化学习中处理多通道输入</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gw1vit/how_to_handle_multi_channel_input_in_deep/</link>
      <description><![CDATA[大家好。我正在尝试制作一个代理，它将学习如何使用深度强化学习下棋。我使用的是 pettingzoo 的 chess_v6 环境 (https://pettingzoo.farama.org/environments/classic/chess/)，它使用棋盘的观察空间，其形状为 (8,8,111)。我的问题是如何将此观察空间输入到深度学习模型中，因为它是一个多通道输入，以及哪种架构最适合我的 DL 模型。请随时分享您可能拥有的任何提示或我可以阅读的有关该主题或我使用的环境的任何资源。    提交人    /u/Livid-Ant3549   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gw1vit/how_to_handle_multi_channel_input_in_deep/</guid>
      <pubDate>Wed, 20 Nov 2024 23:35:37 GMT</pubDate>
    </item>
    <item>
      <title>“物理智能：将人工智能带入物理世界的十亿美元初创公司内部情况”（pi）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvzupn/physical_intelligence_inside_the_billiondollar/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvzupn/physical_intelligence_inside_the_billiondollar/</guid>
      <pubDate>Wed, 20 Nov 2024 21:20:01 GMT</pubDate>
    </item>
    <item>
      <title>RL 是否存在什么重大的局限性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvx2yb/are_there_any_significant_limitations_to_rl/</link>
      <description><![CDATA[我是在 DeepSeek 的新 R1 模型之后问这个问题的。它与 OpenAI 的 o1 大致相当，很快就会开源。这个问题听起来可能很蹩脚，但我很好奇这方面是否有任何强有力的数学结果。例如，我隐约知道维数灾难。    提交人    /u/dhhdhkvjdhdg   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvx2yb/are_there_any_significant_limitations_to_rl/</guid>
      <pubDate>Wed, 20 Nov 2024 18:55:46 GMT</pubDate>
    </item>
    <item>
      <title>RLtools：最快的深度强化学习库（C++；仅标头；无依赖项）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</link>
      <description><![CDATA[        提交人    /u/jonas-eschmann   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvu8eh/rltools_the_fastest_deep_reinforcement_learning/</guid>
      <pubDate>Wed, 20 Nov 2024 17:00:36 GMT</pubDate>
    </item>
    <item>
      <title>尽管我有 64 GB 的 RAM 和 24 GB 的 GPU RAM，但 RL 训练一段时间后还是会冻结</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvnw8k/rl_training_freezing_after_a_while_even_though_i/</link>
      <description><![CDATA[嗨，我有 64 GB RAM 和 24 GB GPU RAM。我正在训练一个乒乓球游戏的 RL 代理。训练在大约 120 万帧后冻结，我不知道为什么，即使 RAM 没有达到最大值。重播缓冲区大小约为 1_000_000。 [代码链接](https://github.com/VachanVY/Reinforcement-Learning/blob/main/dqn.py) 可能是什么原因以及如何解决这个问题？请帮忙。谢谢。    提交人    /u/VVY_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvnw8k/rl_training_freezing_after_a_while_even_though_i/</guid>
      <pubDate>Wed, 20 Nov 2024 12:02:24 GMT</pubDate>
    </item>
    <item>
      <title>正在寻找南部各州的硕士课程，有什么推荐吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvhoyv/looking_for_masters_programs_in_the_southern/</link>
      <description><![CDATA[嗨，我一直在寻找好的以研究为导向的硕士课程，在那里我可以专注于 RL 理论！所以我主要寻找的是在这个领域有良好研究的大学，这些大学并不是明显的首选。例如，您对亚利桑那州立大学、德克萨斯大学达拉斯分校和德克萨斯 A&amp;M 大学的看法是什么？    提交人    /u/the_real_custart   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvhoyv/looking_for_masters_programs_in_the_southern/</guid>
      <pubDate>Wed, 20 Nov 2024 04:47:41 GMT</pubDate>
    </item>
    <item>
      <title>双足步行者问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gvec28/bipedal_walker_problem/</link>
      <description><![CDATA[      有人知道如何修复代理只学会了如何在 1600 步中保持平衡，因为跌倒会得到 -100 奖励。我不确定是否有必要设计一种新的奖励机制来解决这个问题。    submitted by    /u/joshua_310274   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gvec28/bipedal_walker_problem/</guid>
      <pubDate>Wed, 20 Nov 2024 01:51:24 GMT</pubDate>
    </item>
    <item>
      <title>MuJoCo 动作完成？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gv8llw/mujoco_motion_completion/</link>
      <description><![CDATA[嗨 不确定这是否完全是强化学习，但我一直想知道是否有可能在 MuJoCo 中完成动作完成任务？就像神经网络接收一个简短的动作捕捉片段并尝试填充之后发生的事情一样…… 让我知道你的想法    由   提交  /u/snotrio   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gv8llw/mujoco_motion_completion/</guid>
      <pubDate>Tue, 19 Nov 2024 21:32:42 GMT</pubDate>
    </item>
    <item>
      <title>PPO 作为 MARL 的代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gv042k/ppo_as_agents_in_marl/</link>
      <description><![CDATA[大家好！ 有人能告诉我 PPO 代理是否可以在 MARL 中实现吗？ 谢谢。    提交人    /u/Dry-Image8120   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gv042k/ppo_as_agents_in_marl/</guid>
      <pubDate>Tue, 19 Nov 2024 15:43:34 GMT</pubDate>
    </item>
    <item>
      <title>关于伪代码中TRPO更新的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gut446/question_about_trpo_update_in_pseudocode/</link>
      <description><![CDATA[      嗨，我对以下伪代码中的 TRPO 策略参数更新有疑问： https://preview.redd.it/ljkz085eqt1e1.png?width=680&amp;format=png&amp;auto=webp&amp;s=78a0b5c4db5574f4e3b7eef35615f6f84ba97203 我见过一些例子，其中 θ 是当前策略参数，θ_{k} 是旧策略参数，θ_{k+1} 是新策略参数。我的问题是这是否是一个拼写错误，因为应该更新的是当前的而不是旧的，比如在更新时是否之前确实分配了 θ_{k} = θ 然后更新或者这是否正确。    提交人    /u/Street-Vegetable-117   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gut446/question_about_trpo_update_in_pseudocode/</guid>
      <pubDate>Tue, 19 Nov 2024 09:10:45 GMT</pubDate>
    </item>
    </channel>
</rss>