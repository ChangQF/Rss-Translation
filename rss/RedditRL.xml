<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 19 May 2024 18:17:34 GMT</lastBuildDate>
    <item>
      <title>需要帮忙！！强化学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvr5pt/need_help_rl_project/</link>
      <description><![CDATA[我目前正在使用 Q 学习算法为大学做一个期末项目。这里有没有人非常精通并且可以帮助我   由   提交/u/amulli21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvr5pt/need_help_rl_project/</guid>
      <pubDate>Sun, 19 May 2024 15:42:15 GMT</pubDate>
    </item>
    <item>
      <title>开+RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</link>
      <description><![CDATA[KAN 擅长持续学习，有可能使 on-policy 变得鲁棒吗？    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvo1pb/kan_rl/</guid>
      <pubDate>Sun, 19 May 2024 13:15:45 GMT</pubDate>
    </item>
    <item>
      <title>具有环境知识的演员评论家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cv6ph4/actorcritic_with_environment_knowledge/</link>
      <description><![CDATA[我想创建一个 RL actor-critic 模型来配置网络拓扑。输入是表示当前拓扑的矩阵，输出是表示优化拓扑的新矩阵。我的操作空间包含两个操作：  创建或删除设备（0 或 1）之间的链接。 设置带宽 [0;1]。  我的状态空间由输入矩阵中的设备组成。因此，每个纪元都涉及配置一个设备（矩阵中的整行），并且参与者必须始终通过所有状态。 我的问题是：参与者是否需要知道整个当前拓扑才能创建新的最佳方案？   由   提交/u/Soft_Web489  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cv6ph4/actorcritic_with_environment_knowledge/</guid>
      <pubDate>Sat, 18 May 2024 20:39:02 GMT</pubDate>
    </item>
    <item>
      <title>协变：“当我们使用更多数据训练 RFM-1 时，我们的 [机械臂] 模型的性能 [在拾取方面] 会可预测地提高”：5 倍以上的数据可将错误率减半</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cv0yrg/covariant_as_we_train_rfm1_on_more_data_our_robot/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cv0yrg/covariant_as_we_train_rfm1_on_more_data_our_robot/</guid>
      <pubDate>Sat, 18 May 2024 16:21:55 GMT</pubDate>
    </item>
    <item>
      <title>RL 已达到稳定水平了吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cuhcyb/has_rl_hit_a_plateau/</link>
      <description><![CDATA[大家好，我是强化学习 (RL) 领域的研究员，在过去的几年中，我对该领域的进展感到有点困惑年。看起来我们正处于局部最优状态。自从 DQN、AlphaGo 和 PPO 等突破引发的炒作以来，我观察到，尽管有一些非常酷的增量改进，但还没有任何类似于我们在 PPO 和 SAC 上看到的重大进步。 你对RL的现状有同感吗？我们是否正在经历一段停滞期，或者是否正在取得我没有看到的重大进展？我真的很想听听您的想法，以及您是否认为强化学习即将取得更多突破。   由   提交/u/Md_zouzou  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cuhcyb/has_rl_hit_a_plateau/</guid>
      <pubDate>Fri, 17 May 2024 21:52:33 GMT</pubDate>
    </item>
    <item>
      <title>MAB 每一步都有多种选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cuftyn/mab_for_multiple_choices_at_each_step/</link>
      <description><![CDATA[因此，我正在使用自定义环境，我需要在每个时间步骤中选择一个大小为 N 的向量并获得全局奖励（为简化起见，动作 [1, 2] 可以返回不同的奖励 [2, 1]）。我正在使用 MAB，特别是 UCB 和 epsilon-greedy，其中我有 N 个独立的 MAB 控制 M 个臂。它基本上是一个多代理，但只有一个中央代理控制一切。我的问题是可能的操作数量（MN） 以及选项之间缺乏“沟通”以达到更好的全局解决方案。我知道一些基于环境上的其他模拟的好解决方案，但 RL 无法自己达到，作为测试，当我“展示”（强制执行操作）好的动作时，它不会学习它，因为旧的测试组合。我正在考虑使用 CMAB 来提高全局奖励。我可以使用其他算法来解决这个问题吗？    提交人    /u/joaovitorblabres   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cuftyn/mab_for_multiple_choices_at_each_step/</guid>
      <pubDate>Fri, 17 May 2024 20:45:21 GMT</pubDate>
    </item>
    <item>
      <title>多任务强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cubiwb/multitask_rl/</link>
      <description><![CDATA[是否存在同时（不是按顺序或连续）学习多个独立任务的环境？举例来说，学习边瞄准边行走的任务或学习边走边玩杂耍的任务。我正在寻找动作空间保持相同的东西，并且在给定的时间步，代理预测两个不同任务的动作，类似于“复合运动学习”论文中使用的环境 https://www.youtube.com/watch?v=mcRAxwoTh3E   由   提交/u/Personal_Click_6502   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cubiwb/multitask_rl/</guid>
      <pubDate>Fri, 17 May 2024 17:45:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的理想环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cu89sr/ideal_environment_for_rl/</link>
      <description><![CDATA[大家好，我一直想从事强化学习项目，但我一直遇到一些基本的不确定性，我不知道如何沟通，但我会尝试。我总是对我可能会做的很酷的项目有想法（RL 用于部落冲突、Minecraft、机器人等），但我不知道如何“与应用程序交互”？大多数教程似乎都重新创建了自己的游戏，以便他们可以轻松访问状态、奖励和操作信息，但我如何使用像 CoC 这样运行的实际应用程序来做到这一点，我使用什么操作系统以及如何做我与它交互（读取信息并进行击键）？我是否必须读取应用程序的内存才能找到某些值（时间、运行状况），我该怎么做？我应该只使用 GUI 显示来获取信息（以便整个屏幕和像素值成为状态）以及如何记录它？另外，如何阻止应用程序继续运行，直到我的 ML 模型做出决定然后我采取行动，从而防止未被注意到的状态更改？如果这一切听起来很愚蠢，我真的很抱歉，但我找不到任何似乎在真实游戏中做到这一点的教程，而且我觉得我缺少一些其他人都可以轻松完成的东西，因为也有例如，DeepMind 的一系列研究视频，玩《我的世界》之类的。我真的不知道从哪里开始   由   提交 /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cu89sr/ideal_environment_for_rl/</guid>
      <pubDate>Fri, 17 May 2024 15:34:13 GMT</pubDate>
    </item>
    <item>
      <title>torchrl 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cu08xk/issues_with_torchrl/</link>
      <description><![CDATA[       嗨， 我正在使用 torchrl 的 PPO 实现。 https://pytorch.org/rl/stable/tutorials/coding_ppo.html &lt;但是在训练我的代理时，我的权重出现爆炸，将网络推向纳米值。  我试图找出原因，然后我看到了这些极端值 损失熵和loss_objective中的一个非常相似但积极的问题。  根据我的分析，我怀疑 log_probs 在每一集中都增加到非常大的值，这可能是问题的原因。但是，我不确定确切的原因以及如何解决。 https://preview.redd.it/oypsspdt6y0d1.png?width=1409&amp;format=png&amp;auto=webp&amp;s=0cffba31d4138a2f0af79f869b8153 f3690c3a67    由   提交 /u/RikoteMasterrrr   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cu08xk/issues_with_torchrl/</guid>
      <pubDate>Fri, 17 May 2024 08:31:34 GMT</pubDate>
    </item>
    <item>
      <title>使用ns3-ai开发DQN算法的主要方式是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ctjg13/what_is_the_main_way_of_using_ns3ai_to_develop/</link>
      <description><![CDATA[大家好， 我目前正在研究在无线网络（特别是 802.11ax）中使用 ML 算法进行速率自适应 (RAA) 的课题。我的研究目标是展示 ML 算法（在本例中为 DQN）的能力和有效性，以及它们如何在吞吐量方面超越 Minstrel 或 Ideal 等传统算法。 为了进行这项研究，我使用 Network Simulator 3 (NS3) 作为 DQN 算法的主要训练和测试环境。然后，为了解决 Python 和 NS3（用 C++ 编写）之间的通信问题，我使用了 ns3-ai 模块，该模块利用共享内存池方法来解决这种情况。 我遇到的主要问题是互联网上缺少关于此特定主题的任何地方的实现代码。我阅读了一些研究论文，例如“ns3-ai：深度 Q 网络对无线局域网的速率控制.pdf&amp;iconf=SISA&amp;year=2022&amp;vol=69&amp;number=SS1-6&amp;lang=E)&quot;、“NS3-AI：在 ns-3 中将人工智能应用于网络仿真”和“CSMA/CA 无线网络中的 Q 学习速率自适应&quot;。虽然他们确实给了我宝贵的见解，但他们从未深入了解代码的具体编写方式。 所以，我的问题是，我将如何在 NS3 和 ns3-ai 中使用 DQN？我将如何观察 NS3 中的状态并执行操作？鉴于它不是典型的体育馆环境，环境不会“接受”动作。  这是我尝试过的。  我试图理解/逆向工程提供的示例代码 我已经阅读了他们的教程，但是它只描述了在 Python 和 C++ 之间来回发送数据的简单场景，忽略了 NS3 模拟环境。 创建者说有骨架代码，但我找不到。  我将非常感谢任何帮助！    提交人    /u/SkullNighter   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ctjg13/what_is_the_main_way_of_using_ns3ai_to_develop/</guid>
      <pubDate>Thu, 16 May 2024 18:08:36 GMT</pubDate>
    </item>
    <item>
      <title>如果您知道自己正在做什么或一生要做什么，有多少？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ctfnaj/how_many_if_you_know_what_you_are_doing_or_going/</link>
      <description><![CDATA[每个人都对自己的计划充满信心吗？   由   提交/u/AdTrue6018   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ctfnaj/how_many_if_you_know_what_you_are_doing_or_going/</guid>
      <pubDate>Thu, 16 May 2024 15:29:18 GMT</pubDate>
    </item>
    <item>
      <title>供应链和补货中强化学习的缓慢学习问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ctc660/slow_learning_problem_of_rl_in_supply_chain_and/</link>
      <description><![CDATA[我正在尝试使用强化学习（例如 DQN）来创建一个系统来为我们的供应链和补货系统发出采购订单。但由于供应商通常有很长的交货时间，我们通常需要几周的时间才能找到订购操作的实际奖励和价值。如何处理这个学习缓慢的问题？   由   提交 /u/Sorry-Ad3369    reddit.com/r/reinforcementlearning/comments/1ctc660/slow_learning_problem_of_rl_in_supply_chain_and/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ctc660/slow_learning_problem_of_rl_in_supply_chain_and/</guid>
      <pubDate>Thu, 16 May 2024 12:51:57 GMT</pubDate>
    </item>
    <item>
      <title>Dominion：人工智能研究的新领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct9r6u/dominion_a_new_frontier_for_ai_research/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.06846 摘要：  近年来，机器学习方法取得了巨大的进步，达到了在围棋、雅达利和扑克游戏中的超人表现。这些游戏以及之前的其他游戏不仅充当了测试平台，而且还有助于突破人工智能研究的界限。延续这一传统，我们研究了桌面游戏《Dominion》，并讨论了使其非常适合作为下一代强化学习 (RL) 算法基准的属性。我们还展示了 Dominion Online 数据集，该数据集包含经验丰富的玩家在 Dominion Online 网络服务器上玩过的超过 2,000,000 款 Dominion 游戏。最后，我们介绍了一个 RL 基线机器人，它使用现有技术来击败常见的基于启发式的机器人，并显示出与之前最强的机器人、Provincial 的竞争性能。  &lt;!-- SC_ON - -&gt;  由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct9r6u/dominion_a_new_frontier_for_ai_research/</guid>
      <pubDate>Thu, 16 May 2024 10:29:26 GMT</pubDate>
    </item>
    <item>
      <title>[D]不同操作空间或状态相关操作空间的当前 SOTA</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct8wqi/dcurrent_sota_for_varying_action_spaces_or_state/</link>
      <description><![CDATA[你好。我的工作中一般不涉及强化学习。我主要使用 ML/DL 算法。但最近的问题陈述让我想知道是否可以使用 RL。目前的问题是，你当前处于一个状态，并且拥有所有过去状态的信息，并且知道当前可以从该状态采取哪些操作（这些操作随着每个状态的时间而不断变化，但它们是可数的操作），那么你会使用什么 RL 算法来完成这个任务。我再次在这方面是个菜鸟。    由   提交 /u/SmartEvening   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct8wqi/dcurrent_sota_for_varying_action_spaces_or_state/</guid>
      <pubDate>Thu, 16 May 2024 09:27:30 GMT</pubDate>
    </item>
    <item>
      <title>模组别生气</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ct0ywo/mods_dont_be_mad/</link>
      <description><![CDATA[       由   提交 /u/glowingcanoodle   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ct0ywo/mods_dont_be_mad/</guid>
      <pubDate>Thu, 16 May 2024 01:07:02 GMT</pubDate>
    </item>
    </channel>
</rss>