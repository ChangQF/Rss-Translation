<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 17 Jun 2024 06:22:10 GMT</lastBuildDate>
    <item>
      <title>为什么相关样本在基于策略的方法或更新 Actor 中不会出现问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhra6a/why_correlated_samples_are_not_problematic_in/</link>
      <description><![CDATA[在 DQN 中，我们说 s、s&#39; 是相关的，我们使用重放缓冲区来打破这种关系。  在 PPO 中，当我们更新价值网络时，相同的相关性问题不会影响更新吗？    提交人    /u/seatedrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhra6a/why_correlated_samples_are_not_problematic_in/</guid>
      <pubDate>Mon, 17 Jun 2024 06:13:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么不使用 Actor-Critic 和 PPO 中的目标网络来更新价值网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhr94u/why_not_use_a_target_network_in_actorcritic_ppo/</link>
      <description><![CDATA[在 DQN 中，我们使用目标网络来阻止目标移动。但是我们不对 PPO 或 Actor-Critic 方法使用相同的技巧。为什么移动目标对于这些方法的价值网络来说不是问题？在这两种方法中，目标 = r(s,a) + V(s&#39;)。因此更新 V(s) 将改变 V(s&#39;)，这不会影响稳定性吗？    提交人    /u/seatedrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhr94u/why_not_use_a_target_network_in_actorcritic_ppo/</guid>
      <pubDate>Mon, 17 Jun 2024 06:11:26 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Gym 什么时候被弃用了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhqx8y/when_did_isaac_gym_get_deprecated/</link>
      <description><![CDATA[我刚刚在下载网站上看到它被标记为“现已弃用”，并且将不再受支持，而应该考虑使用 Isaac Lab。 这到底是什么时候发生的？有人知道吗？ 我仍然需要研究 Isaac Lab，但有点担心是否会有很多新东西需要学习。我还是这个行业的新手。    提交人    /u/stop_stalking_me_plz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhqx8y/when_did_isaac_gym_get_deprecated/</guid>
      <pubDate>Mon, 17 Jun 2024 05:48:37 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习（DQN）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhopi4/deep_reinforcement_learning_dqn/</link>
      <description><![CDATA[我正在尝试为棋盘游戏开发深度强化学习模型，但训练效果并不理想。我需要与更有经验的人交流，帮助我改进模型。我来自中国，通过 GPT 找到了这个论坛。由于中国的互联网并未全球连接，我对 AI 相关知识的获取有限。我愿意付费服务，希望得到您的帮助。最低支付额为 700 美元，如果帮助有效则没有上限。请联系我 [xk520zyq@126.com]() 了解更多详情。    提交人    /u/Routine-Shift-2072   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhopi4/deep_reinforcement_learning_dqn/</guid>
      <pubDate>Mon, 17 Jun 2024 03:29:03 GMT</pubDate>
    </item>
    <item>
      <title>“创造力已远离聊天：消除语言模型偏见的代价”，Mohammedi 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhkn9o/creativity_has_left_the_chat_the_price_of/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhkn9o/creativity_has_left_the_chat_the_price_of/</guid>
      <pubDate>Sun, 16 Jun 2024 23:45:38 GMT</pubDate>
    </item>
    <item>
      <title>自定义体育馆环境：观察空间问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhilgl/custom_gymnasium_env_observation_space_problem/</link>
      <description><![CDATA[您好，我正在 pygame 中构建一个类似于 PvZ 的游戏，但它没有玩家，而是有一个应该学习如何玩游戏的代理。为此，我使用了 gymnasium，但我对这个模块还很陌生。 我目前正在尝试实现自定义 gym 环境，但在观察空间中遇到困难。 错误： 回溯（最近一次调用最后一次）： 文件 &quot;...\PVZ-RL\pvz_env.py&quot;，第 260 行，在 &lt;module&gt; check_env(env.unwrapped) 文件 &quot;...\Python\Python310\site-packages\gymnasium\utils\env_checker.py&quot;，第 301 行，在 check_env 中 check_reset_return_type(env) 文件&quot;...\Python\Python310\site-packages\gymnasium\utils\env_checker.py&quot;，第 214 行，在 env.observation_space 中的 check_reset_return_type obs 中 AssertionError: `env.reset()` 返回的第一个元素不在观察空间内。  代理在 9x5 的网格中移动，可以放置植物或收集太阳，就像在实际游戏中一样。 如视频所示，代理有 7 个动作： class AgentAction(Enum): UP=0 DOWN=1 LEFT=2 RIGHT=3 PLACE_PEASHOOTER= 4 PLACE_SUNFLOWER= 5 COLLECT_SUN = 6  我不太了解 observer_space 的高参数，但我选择了可​​能变量的最大数量作为值，例如例如： 最大植物数量 = 9*5（网格大小） 收集到的最大太阳数量 = 2000 代理的最大位置数量 = 9*5（网格大小） 杀死的最大僵尸数量 = 1000 我的自定义环境如下： class GameEnv(gym.Env): metadata = {&quot;render_modes&quot;: [&quot;human&quot;], &#39;render_fps&#39;: 30} def __init__(self, grid_rows=9, grid_cols=5, render_mode=None): self.zombies_killed_counter = 0 self.grid_rows = grid_rows self.grid_cols = grid_cols self.render_mode = render_mode self.pvz_game = Game(fps=self.metadata[&#39;render_fps&#39;]) self.action_space = space.Discrete(len(AgentAction)) self.observation_space = space.Box( low=0, high=np.array([9*5,2000,9*5,1000]), shape=(4,), dtype=np.int32 )  我的重置函数如下： def reset(self, seed=None, options=None): super().reset(seed=seed) self.pvz_game.reset(seed=seed) plants_owned = self.pvz_game.agent.get_plants_owned() suns = self.pvz_game.agent.get_suns() pos = self.pvz_game.agent.get_pos() zombies_killed = self.pvz_game.agent.get_zombies_killed() plants_owned = np.array(self.encode_plants(self.pvz_game.agent.get_plants_owned()), dtype=np.int32) suns = np.array([self.pvz_game.agent.get_suns()], dtype=np.int32) pos = np.array(self.pvz_game.agent.get_pos(), dtype=np.int32) zombies_killed = np.array([len(self.pvz_game.agent.get_zombies_killed())], dtype=np.int32) obs = np.concatenate((plants_owned, suns, pos, zombies_killed)) info = {} if self.render_mode == &#39;human&#39;: self.render()返回 obs, info  我不太明白这里的问题。    提交人    /u/Pyjam4a   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhilgl/custom_gymnasium_env_observation_space_problem/</guid>
      <pubDate>Sun, 16 Jun 2024 22:04:44 GMT</pubDate>
    </item>
    <item>
      <title>寻求使用 MuJoCo（或类似模拟器）进行机器人和人工智能项目的资源和建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhaixr/seeking_resources_and_advice_for_using_mujoco_or/</link>
      <description><![CDATA[大家好， 我是人工智能和深度学习领域的研究人员。虽然我在这些领域拥有丰富的知识，但我在图形和模拟器方面的经验有限。在学习计算机科学期间，我选修了一门与 Unity 相关的课程，但这就是我对模拟器的熟悉程度。 目前，我正在开展一个涉及认知架构的项目，并希望使用模拟器与我们使用的机器人一起工作。我正在寻找一个快速的模拟器，让我能够加快模拟速度，因为我有一个强大的 GPU (RTX)。 我听说我的一些同事使用 ROS，但我听说 Gazebo 可能不是快速模拟的最佳选择。出于这个原因，我正在考虑 MuJoCo，但我也愿意接受其他可能更适合我的项目的选择（我听说过 PyBullet 和 Isaac）。 您能否推荐一些资源，例如课程或教程，用于学习如何使用 MuJoCo 或其他快速高效的模拟器？如果能提供任何关于哪种模拟器最适合我的需求的建议，我将不胜感激 提前感谢您的帮助！    提交人    /u/SympathyOutside   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhaixr/seeking_resources_and_advice_for_using_mujoco_or/</guid>
      <pubDate>Sun, 16 Jun 2024 15:51:43 GMT</pubDate>
    </item>
    <item>
      <title>“使用大型语言模型发现偏好优化算法”，Lu 等人 2024（使用 LLM 编写新的 Python 损失函数发现 DPO 的小幅改进）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh9ts7/discovering_preference_optimization_algorithms/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh9ts7/discovering_preference_optimization_algorithms/</guid>
      <pubDate>Sun, 16 Jun 2024 15:19:22 GMT</pubDate>
    </item>
    <item>
      <title>在开始 RL 部分之前，如何设计自定义环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh5xnb/how_do_i_design_a_custom_environment_before/</link>
      <description><![CDATA[嘿，我只是想知道如何创建自定义 RL 环境。Youtube 视频通常只选择现成的自定义环境（例如 sentdex 在他的教程中选择了蛇游戏）。在开始担心 RL 的其他方面（如观察和动作空间、奖励函数等）之前，我想了解如何创建自己的环境。任何帮助都将不胜感激。 我需要数据集或任何东西来创建我的自定义环境吗？或者说如果我有布局的坐标，我可以使用它吗？    提交人    /u/Strange-Durian3382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh5xnb/how_do_i_design_a_custom_environment_before/</guid>
      <pubDate>Sun, 16 Jun 2024 11:52:34 GMT</pubDate>
    </item>
    <item>
      <title>现实生活中的形状值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh3al6/shapely_values_in_rl/</link>
      <description><![CDATA[有人可以建议如何在环境（如 cartpole 和 mountain car）上实现 shapely 值吗？任何库都会有所帮助。    提交人    /u/MarionberryVisual911   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh3al6/shapely_values_in_rl/</guid>
      <pubDate>Sun, 16 Jun 2024 08:42:51 GMT</pubDate>
    </item>
    <item>
      <title>哪个 RL 库最好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgys73/which_rl_library_is_best/</link>
      <description><![CDATA[我们正在寻求为一个项目实现一个自定义的 RL 环境。该环境相当复杂，涉及机场滑行布局，以分析飞机的滑行路线。对此有几个问题 -   哪个框架最适合这个？我们已经尝试使用 OpenAI Gym 的 stable-baselines3，但感觉非常受限。还看到了一些其他 RL 库，如 Acme、Ray (Rllibs) 等。 上述所有库是否都支持自定义环境，以及它对用户的友好程度如何？     提交人    /u/Strange-Durian3382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgys73/which_rl_library_is_best/</guid>
      <pubDate>Sun, 16 Jun 2024 03:32:55 GMT</pubDate>
    </item>
    <item>
      <title>“人工智能搜索：更惨痛的教训”，麦克劳林（回顾 Leela Zero 与 Stockfish 的较量，以及解决法学硕士问题后钟摆摆回搜索方向）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgxmnj/ai_search_the_bitterer_lesson_mclaughlin/</link>
      <description><![CDATA[       由    /u/gwern  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgxmnj/ai_search_the_bitterer_lesson_mclaughlin/</guid>
      <pubDate>Sun, 16 Jun 2024 02:23:47 GMT</pubDate>
    </item>
    <item>
      <title>训练机器人在 MJX 中表演足球技巧</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgonk9/train_a_robot_to_do_football_tricks_in_mjx/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgonk9/train_a_robot_to_do_football_tricks_in_mjx/</guid>
      <pubDate>Sat, 15 Jun 2024 18:44:47 GMT</pubDate>
    </item>
    <item>
      <title>人类生物的现实生活...足够安全吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgkafm/rl_for_humanoids_safe_enough/</link>
      <description><![CDATA[      看看这个视频哈哈 - 你无法像其他机器人那样仅使用紧急停止来处理故障。有希望用 RL 解决这个问题吗？ [来源：https://x.com/_wenlixiao/status/1801808951601705258?t=PyYeg362j-mzZkb73NkwKQ&amp;s=19 和 https://x.com/_wenlixiao/status/1801305252760850903?t=S2KzQzXigYI4zyOqaSydXA&amp;s=19 ]    提交人    /u/Boring_Focus_9710   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgkafm/rl_for_humanoids_safe_enough/</guid>
      <pubDate>Sat, 15 Jun 2024 15:19:43 GMT</pubDate>
    </item>
    <item>
      <title>即使参与者损失的负面影响不断增加，PPO 代理仍在学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgigr9/ppo_agent_is_learning_even_if_the_negative_of/</link>
      <description><![CDATA[      即使整体目标函数最初是最大化而不是最小化，我的 PPO 代理也会学习。参与者损失和总体目标函数首先增加，然后减少，最后趋于零。在整个过程中，它一直在学习它想要学习的东西。评论家损失和熵正在最小化（正如预期的那样）。原因可能是什么？附注：我知道参与者损失应该最大化，但我说的是参与者损失的负值，理想情况下应该使用 ADAM 优化器将其最小化，但事实并非如此。    提交人    /u/Low-Advertising-1892   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgigr9/ppo_agent_is_learning_even_if_the_negative_of/</guid>
      <pubDate>Sat, 15 Jun 2024 13:52:55 GMT</pubDate>
    </item>
    </channel>
</rss>