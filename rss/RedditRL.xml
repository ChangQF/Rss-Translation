<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Sat, 21 Dec 2024 06:21:31 GMT</lastBuildDate>
    <item>
      <title>â€œå·å–å…è´¹åˆé¤ï¼šæ­éœ² Dyna å¼å¼ºåŒ–å­¦ä¹ çš„å±€é™æ€§â€ï¼ŒBarkley å’Œ Fridovich-Keil</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</link>
      <description><![CDATA[      æˆ‘è®¤ä¸ºè¿™æ˜¯é€‚åˆè®¨è®ºè¿™ä¸ªé—®é¢˜çš„åœ°æ–¹ï¼Œä½†æ— è®ºå¦‚ä½•ï¼Œå¯¹äºè¿™ç§æ— è€»çš„è‡ªæˆ‘æ¨é”€ï¼Œæˆ‘æ·±è¡¨æ­‰æ„â€¦â€¦ è®ºæ–‡ï¼šhttps://arxiv.org/abs/2412.14312 å…³äº X çš„ç›¸å½“ä¸é”™çš„ TLDRï¼šhttps://x.com/bebark99/status/1869941518435512712 æœ€è¿‘ï¼Œä½œä¸ºç ”ç©¶çš„ä¸€éƒ¨åˆ†ï¼Œæˆ‘å¯¹åŸºäºæ¨¡å‹çš„ RL éå¸¸æ„Ÿå…´è¶£ï¼Œä½†å¾ˆå¿«é‡åˆ°äº†ä¸€ä¸ªå¤§é—®é¢˜ï¼šè¯¥é¢†åŸŸä¸€äº›ä¸»è¦æ–¹æ³•çš„åŸå§‹ PyTorch å®ç°é€Ÿåº¦éå¸¸æ…¢ã€‚é™¤éæ‚¨æœ‰æ•°ç™¾ä¸ª GPU å°æ—¶å¯ä¾›ä½¿ç”¨ï¼Œå¦åˆ™å‡ ä¹ä¸å¯èƒ½å®Œæˆä»»ä½•åˆç†çš„å·¥ä½œã€‚ å› æ­¤ï¼Œæˆ‘å†³å®šåœ¨ JAX ä¸­é‡æ–°å®ç°ä¸€ä¸ªçŸ¥åç®—æ³•ï¼Œå³åŸºäºæ¨¡å‹çš„ç­–ç•¥ä¼˜åŒ– (MBPO)ï¼Œè¿™è®©äº‹æƒ…è¿è¡Œå¾—æ›´å¿«ã€‚ç„¶è€Œï¼Œç»è¿‡å¤§é‡çš„æ•…éšœæ’é™¤å’Œæµ‹è¯•åï¼Œæˆ‘é‡åˆ°äº†å¦ä¸€ä¸ªæƒŠå–œï¼šåªè¦æ‚¨å°è¯•åœ¨ä¸åŒäºä»–ä»¬è®ºæ–‡ä¸­æµ‹è¯•çš„åŸºå‡†ï¼ˆå³ DMC è€Œä¸æ˜¯ Gymï¼‰ä¸Šä»å¤´å¼€å§‹è®­ç»ƒå®ƒï¼Œå®ƒçš„è¡¨ç°å°±ä¼šæ¯”ç®€å•çš„ç¦»çº¿ç­–ç•¥ç®—æ³•å·®ï¼Œåè€…éœ€è¦çš„è®­ç»ƒæ—¶é—´è¦å°‘å‡ ä¸ªæ•°é‡çº§ã€‚ è¿™æ¶‰åŠ 6 ä¸ª gym ç¯å¢ƒå’Œ 15 ä¸ª DMC ç¯å¢ƒï¼Œå› æ­¤éå¸¸ä¸€è‡´ã€‚ è¿™è®©æˆ‘å¾ˆå¥½å¥‡ï¼Œç»è¿‡ä¸€ç•ªæŒ–æ˜ï¼Œæˆ‘å’Œæˆ‘çš„å¯¼å¸ˆæœ€ç»ˆå†™äº†ä¸€ç¯‡å…³äºå®ƒå’Œå…¶ä»– Dyna é£æ ¼çš„åŸºäºæ¨¡å‹çš„ RL æ–¹æ³•çš„è®ºæ–‡ã€‚å‰§é€ï¼šå¹¶éæ‰€æœ‰ dyna é£æ ¼çš„æ–¹æ³•éƒ½æ— æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸­å‘æŒ¥ä½œç”¨ï¼Œä½†å½“ Dyna å¤±è´¥æ—¶ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå­¤ç«‹æˆ–ç®€å•çš„é—®é¢˜ã€‚ å¦‚æœæœ‰äººæœ‰å…´è¶£å°è¯•çš„è¯ï¼ŒJAX å®ç°åº”è¯¥ä¼šåœ¨æ˜å¹´æ¨å‡ºã€‚å¸Œæœ›å¾—åˆ°å¤§å®¶çš„åé¦ˆï¼ https://preview.redd.it/ei78d7hbz28e1.png?width=3600&amp;format=png&amp;auto=webp&amp;s=c981756b0f0803e12b2e0e8e6a7816eedb085cfe    æäº¤äºº    /u/darthbark   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hivlfz/stealing_that_free_lunch_exposing_the_limits_of/</guid>
      <pubDate>Fri, 20 Dec 2024 22:32:37 GMT</pubDate>
    </item>
    <item>
      <title>è¡¨æ ¼è½¯ q å­¦ä¹  åœç•™åœ¨ç®€å•çš„ç½‘æ ¼ä¸–ç•Œ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</link>
      <description><![CDATA[æ‚¨å¥½ï¼Œæˆ‘æ­£åœ¨ä¸ºä¸€ä¸ªç®€å•çš„ 5x5 ç½‘æ ¼ä¸–ç•Œå¼€å‘ä¸€ä¸ªç®€å•çš„è¡¨æ ¼è½¯ q å­¦ä¹ ä»£ç†ã€‚ç»è¿‡å‡ æ¬¡å°è¯•åï¼Œå®ƒå¡åœ¨äº†ç‰¹å®šçŠ¶æ€ã€‚æˆ‘ä¸çŸ¥é“è¿™æ˜¯å®æ–½é”™è¯¯è¿˜æ˜¯è¶…å‚æ•°ä¸å¥½ã€‚æˆ‘å°†åœ¨ä¸‹é¢é™„ä¸Šä»£ç ã€‚è¿˜æœ‰å…¶ä»–å»ºè®®å—ï¼Ÿ è°¢è°¢ import numpy as np import time import os class Env(): def __init__(self): self.height = 5 self.width = 5 self.posX = 0 self.posY = 0 self.endX = self.width-1 self.endY = self.height-1 self.actions = [0, 1, 2, 3] self.stateCount = self.height*self.width self.actionCount = len(self.actions) def reset(self): self.posX = 0 self.posY = 0 self.done = False return 0, 0, False # é‡‡å–è¡ŒåŠ¨ def step(self, action): if action==0: # å·¦ self.posX = self.posX-1 if self.posX&gt;0 else self.posX if action==1: # å³ self.posX = self.posX+1 if self.posX&lt;self.width-1 else self.posX if action==2: # å‘ä¸Š self.posY = self.posY-1 if self.posY&gt;0 else self.posY if action==3: # å‘ä¸‹ self.posY = self.posY+1 if self.posY&lt;self.height-1 else self.posY done = self.posX==self.endX and self.posY==self.endY # å°† (x,y) ä½ç½®æ˜ å°„åˆ° 0 åˆ° 5x5-1=24 ä¹‹é—´çš„æ•°å­— nextState = self.width*self.posY + self.posX reward = 1 if done else -0.1 return nextState, reward, done # è¿”å›ä¸€ä¸ªéšæœºåŠ¨ä½œ def randomAction(self): return np.random.choice(self.actions) # æ˜¾ç¤ºç¯å¢ƒ def render(self): for i in range(self.height): for j in range(self.width): if self.posY==i and self.posX==j: print(&quot;O&quot;, end=&#39;&#39;) elif self.endY==i and self.endX==j: print(&quot;T&quot;, end=&#39;&#39;) else: print(&quot;.&quot;, end=&#39;&#39;) print(&quot;&quot;) def softmax(x): e_x = np.exp(x - np.max(x)) # ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ return e_x / e_x.sum() class Agent: def __init__(self, stateCount, actionCount, env, max_steps = 100, epochs = 50, discount_factor = 0.99, lr = 0.1, temp = 1): # Q è¡¨ï¼šåŒ…å«æ¯ä¸ª (state,action) å¯¹çš„ Q å€¼ self.Q = np.zeros((stateCount, actionCount)) # è¶…å‚æ•° self.temp = temp self.lr = lr self.epochs = epochs self.discount_factor = discount_factor # ç¯å¢ƒ self.env = env self.max_steps = max_steps def getV(self, q_value): return self.temp * np.log(np.sum(np.exp(q_value / self.temp))) def choose_action(self, state): # q = self.Q[state] # v = self.getV(q) # dist = np.exp((q - v) / self.temp) # action_probs = dist / np.sum(dist) # return np.random.choice(env.actions, p=action_probs) action_probs = softmax((self.Q[state] - self.getV(self.Q[state])) / self.temp) return np.random.choice(env.actions, p=action_probs) # è®­ç»ƒå¾ªç¯ def run(self): for i in range(self.epochs): state, reward, done = self.env.reset() steps = 0 while not done: os.system(&#39;cls&#39;) # print(self.Q) print(&quot;epoch #&quot;, i+1, &quot;/&quot;, self.epochs) self.env.render() time.sleep(0.01) # è®¡æ•°å®Œæˆæ¸¸æˆçš„æ­¥éª¤ steps += 1 # é€‰æ‹©è½¯ q å­¦ä¹ åŠ¨ä½œ action = self.choose_action(state) # é‡‡å–è¡ŒåŠ¨ next_state, reward, done = self.env.step(action) # ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹æ›´æ–° Q è¡¨å€¼ # target = reward + self.discount_factor * np.sum(action_probs * self.Q[next_state]) # target = reward + self.discount_factor * self.getV(self.Q[next_state]) target = reward + (1 - done) * self.discount_factor * self.getV(self.Q[next_state]) self.Q[state][action] += self.lr * (target - self.Q[state][action]) # æ›´æ–°çŠ¶æ€ state = next_state if steps &gt;= self.max_steps: break print(&quot;\nDone in&quot;, steps, &quot;steps&quot;.format(steps)) time.sleep(0.8) def print_q_table(self): for i in range(0,len(self.Q)): for j in range(0,len(self.Q[i])): print(self.Q[i][j], end=&quot; &quot;, flush=True) print(&quot;&quot;) if __name__ == &quot;__main__&quot;: # åˆ›å»º CartPole ç±»çš„å®ä¾‹ env = Env() resolver = Agent(env.stateCount, env.actionCount, env) resolver.run()    ç”±    /u/Majestic-Tap1577  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisqkv/tabular_soft_q_learning_stuck_with_simple_grid/</guid>
      <pubDate>Fri, 20 Dec 2024 20:18:56 GMT</pubDate>
    </item>
    <item>
      <title>å¾®ç½‘æ ¼ 16x16 åŠ¨æ€éšœç¢ç‰©è§£å†³æ–¹æ¡ˆ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hisobe/minigrid_16x16_dynamic_obstacles_solution/</link>
      <description><![CDATA[æˆ‘ä½¿ç”¨ minigrid å’Œ rl å¯åŠ¨æ–‡ä»¶ã€‚æˆ‘æ­£åœ¨åŠªåŠ›è§£å†³ 16x16ï¼ˆå’Œ 8x8ï¼‰åŠ¨æ€è¿·å®«ï¼Œæˆ‘ç›®å‰æ­£åœ¨ä½¿ç”¨ python -m scripts.train --algo ppo --env MiniGrid-Dynamic-Obstacles-16x16-v0 --model DoorKey --save-interval 10 --recurrence 8ã€‚ä½ çŸ¥é“æˆ‘åº”è¯¥æ›´æ”¹å“ªäº›å…¶ä»–è¶…å‚æ•°å—ï¼Ÿåªæœ‰ 3x3 åŠ¨æ€è¿·å®«å¯¹æˆ‘æ¥è¯´æ˜¯å¯è§£çš„    æäº¤äºº    /u/More_Peanut1312   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hisobe/minigrid_16x16_dynamic_obstacles_solution/</guid>
      <pubDate>Fri, 20 Dec 2024 20:16:05 GMT</pubDate>
    </item>
    <item>
      <title>[AHT] å¯¹ SOMALI CAT ç¯å¢ƒçš„æŸ¥è¯¢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</link>
      <description><![CDATA[æˆ‘åˆšåˆšé˜…è¯»äº†ä¸¤ç¯‡å…³äºä¸´æ—¶å›¢é˜Ÿåˆä½œçš„éå¸¸æœ‰è¶£çš„è®ºæ–‡ï¼Œé‡ç‚¹å…³æ³¨æ²Ÿé€šå¦‚ä½•æé«˜æ­¤ç±»ä»»åŠ¡çš„æ•ˆç‡ï¼Œ&quot;A Penny for Your Thoughts: The Value of Communication in Ad Hoc Teamwork&quot; å’Œ &quot; Expected Value of Communication for Planning in Ad Hoc Teamworks&quot;ã€‚ä½œè€…æ›¾åœ¨ä¸€ä¸ªåä¸º SOMALI CAT çš„ç¯å¢ƒä¸­å·¥ä½œè¿‡ï¼Œæˆ‘å¾ˆå¥½å¥‡ï¼šè¿™é‡Œæœ‰æ²¡æœ‰äººæ›¾ç»åœ¨è¿™ä¸ªç¯å¢ƒä¸­å·¥ä½œè¿‡ï¼Œæˆ–è€…æ‚¨æ˜¯å¦çŸ¥é“å®ƒæ˜¯å¦å¯ä»¥åœ¨æŸäº›é€šç”¨ Python RL æµ‹è¯•æ¡†æ¶ï¼ˆä¾‹å¦‚ Gymnasiumï¼‰ä¸Šä½¿ç”¨ã€‚ æ„Ÿè°¢æ‚¨çš„å¸®åŠ©    æäº¤äºº    /u/potatodafish   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hiqmdu/aht_inquiries_over_somali_cat_environment/</guid>
      <pubDate>Fri, 20 Dec 2024 18:43:58 GMT</pubDate>
    </item>
    <item>
      <title>é¢„æµ‹è¡ŒåŠ¨å’Œå¥–åŠ±</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘æ­£åœ¨å¤„ç†ä¸€ä¸ªéä¸“å®¶çº§æ•°æ®çš„æ•°æ®é›†ï¼Œå¹¶ä¸”æ­£åœ¨ä½¿ç”¨å†³ç­–è½¬æ¢å™¨ã€‚ç°åœ¨æˆ‘æƒ³æ¯”è¾ƒæ€§èƒ½ï¼Œä½†æˆ‘æ‰¾ä¸åˆ°å¯ä»¥åŒæ—¶é¢„æµ‹åŠ¨ä½œå’Œå¥–åŠ±çš„ç¦»çº¿ rl æ¨¡å‹ã€‚æœ‰äººæœ‰ä»€ä¹ˆå»ºè®®å—ï¼Ÿ    æäº¤äºº    /u/Anonymusguy99   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hik3ke/predict_action_as_well_as_reward/</guid>
      <pubDate>Fri, 20 Dec 2024 13:45:40 GMT</pubDate>
    </item>
    <item>
      <title>æ•‘å‘½ï¼æˆ‘çš„ RL Agent æ²¡æœ‰å­¦ä¹ ã€‚ï¼ˆOpenAI Gym env + Pytorchï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</link>
      <description><![CDATA[æˆ‘è¯•å›¾å®ç°ä¸€ä¸ªç®€å•çš„æ·±åº¦ Q ç½‘ç»œï¼Œä»¥ä¾¿åœ¨ OpenAI Gymnasium æä¾›çš„ Cart Pole ç¯å¢ƒä¸­è®­ç»ƒä»£ç†ã€‚æˆ‘å°è¯•è°ƒæ•´è¶…å‚æ•°ï¼Œä½†ä¼¼ä¹æ²¡æœ‰ä»»ä½•æ•ˆæœã€‚äº‹å®ä¸Šï¼Œéšç€ epoch çš„å¢åŠ ï¼Œæƒ…å†µä¼¼ä¹å˜å¾—æ›´ç³Ÿï¼ˆè™½ç„¶ä¸ç¡®å®šï¼‰ã€‚æˆ‘è§‰å¾—æˆ‘å·²ç»æ­£ç¡®åœ°å®ç°äº†ä¸€åˆ‡ã€‚æˆ‘æ­£åœ¨ä½¿ç”¨ pytorch æ¥æ„å»ºç¥ç»ç½‘ç»œã€‚æˆ‘å¯¹ RL å’Œæ·±åº¦å­¦ä¹ è¿˜ä¸ç†Ÿæ‚‰ï¼Œæ‰€ä»¥å¦‚æœæˆ‘é—æ¼äº†ä»€ä¹ˆï¼Œæˆ‘æ·±è¡¨æ­‰æ„ã€‚ æˆ‘é™„ä¸Šäº†æˆ‘çš„ä»£ç ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è¿è¡Œå®ƒã€‚è¯¥ç¬”è®°æœ¬éå¸¸ä¸è¨€è‡ªæ˜ï¼Œé™¤äº† pytorch ä¹‹å¤–ï¼Œæ‚¨åªéœ€è¦ gyanasium[classic-control] å’Œ pygame åŒ…ã€‚ https://github.com/Utsab-2010/OpenAI-Gym-RL-Tests/blob/main/Cart_Pole_Deep_QN.ipynb ä»»ä½•å»ºè®®æˆ–å¸®åŠ©éƒ½å°†ä¸èƒœæ„Ÿæ¿€ã€‚    æäº¤äºº    /u/Otaku_boi1833   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hif4e3/help_my_rl_agent_is_not_learning_openai_gym_env/</guid>
      <pubDate>Fri, 20 Dec 2024 08:04:59 GMT</pubDate>
    </item>
    <item>
      <title>DDPGã€A2Cã€PPO å’Œ TRPO å®éªŒåœ¨ StableBaselines3 ä¸­ä½•æ—¶æ”¶æ•›ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</link>
      <description><![CDATA[æˆ‘éœ€è¦è¿è¡Œæ‰€æœ‰åŸºäºç­–ç•¥æ¢¯åº¦çš„ RL å®éªŒï¼Œä»¥å±•ç¤ºå®ƒä»¬åœ¨ä½¿ç”¨å’Œä¸ä½¿ç”¨æˆ‘è®¡åˆ’æå‡ºçš„è‡ªå®šä¹‰è®­ç»ƒæ–¹æ¡ˆçš„æƒ…å†µä¸‹æ”¶æ•›çš„é€Ÿåº¦ã€‚å¯¹äºç¯å¢ƒï¼Œæˆ‘æ­£åœ¨è€ƒè™‘ OpenAI Gym ç¯å¢ƒï¼ˆCartPoleã€Pendulumã€LunarLander å’Œ 2-3 Mujoco ç¯å¢ƒï¼‰ã€‚ä½†æˆ‘é¦–å…ˆéœ€è¦ç¡®å®šå®ƒä»¬æ˜¯å¦ä¸é€šå¸¸çš„è®­ç»ƒæ–¹æ³•æ”¶æ•›ã€‚å³ä½¿å®ƒä»¬ä¸æ”¶æ•›ï¼Œä¹Ÿåº”è¯¥æœ‰æ”¹å–„çš„è¿¹è±¡ï¼ˆå°±æƒ…æ™¯å›æŠ¥è€Œè¨€ï¼‰ã€‚SB3 ä¸Šæ˜¯å¦æœ‰ä»»ä½•èµ„æºæä¾›åœ¨è¿™äº›ç¯å¢ƒä¸­å·¥ä½œçš„è¶…å‚æ•°æˆ–ç­–ç•¥ç½‘ç»œï¼Ÿæ­¤å¤–ï¼Œæ˜¯å¦æœ‰äººå¯¹è®°å½•åå‘ä¼ æ’­çš„æ•°é‡æˆ–ç±»ä¼¼çš„ä¸œè¥¿ï¼ˆä»»ä½•å…¶ä»–æœ‰ç”¨çš„æŒ‡æ ‡ï¼‰ä»¥è¯„ä¼°è®¡ç®—è´Ÿè½½ç­‰æœ‰ä»»ä½•å»ºè®®ï¼Ÿ    æäº¤äºº    /u/WayOwn2610   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hidy1e/ddpg_a2c_ppo_and_trpo_experiments_converge_when/</guid>
      <pubDate>Fri, 20 Dec 2024 06:39:36 GMT</pubDate>
    </item>
    <item>
      <title>å†³ç­–é¢‘ç‡ï¼šâ€˜ä¿¡æ¯â€™è§†è§’</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</link>
      <description><![CDATA[å°åŠ¨ä½œé‡å¤æ½œåŠ›ï¼šç»†ç²’åº¦æ§åˆ¶é—®é¢˜ï¼šä¿¡ç”¨åˆ†é… å¤§åŠ¨ä½œé‡å¤æ½œåŠ›ï¼šæ›´æ˜æ™ºçš„å†³ç­–é—®é¢˜ï¼šå»¶è¿Ÿ å¦‚æœå†³ç­–ä¹‹é—´æ²¡æœ‰è¶³å¤Ÿçš„æ—¶é—´ï¼Œä»£ç†å°†æ ¹æ®è¾ƒå°‘çš„ä¿¡æ¯é‡‡å–è¡ŒåŠ¨ã€‚ å¦‚æœæ‰€è¿°æ—¶é—´å¾ˆé•¿ï¼Œåˆ™â€œé€‚åº”å˜åŒ–â€ä¼šè¢«å»¶è¿Ÿã€‚  æ¨èè§£å†³æ–¹æ¡ˆçš„ç¤ºä¾‹ï¼šåˆ†å±‚ RLï¼šå­˜åœ¨åœ¨å¿«é€Ÿè¡ŒåŠ¨çš„è¾ƒä½çº§åˆ«ä¸ä»¥è¾ƒæ…¢é€Ÿåº¦è¡ŒåŠ¨çš„è¾ƒé«˜çº§åˆ«ä¹‹é—´è¿›è¡Œé€šä¿¡çš„é—®é¢˜ã€‚ å†³ç­–è½¬æ¢å™¨ï¼šç¦»çº¿æ–¹æ³•ï¼Œå› æ­¤æ— æ³•åœ¨å·¥ä½œä¸­å­¦ä¹  æ ¹æ®æˆ‘çš„ç»éªŒï¼Œè¿™ä¸ªé—®é¢˜ä¸è®¡ç®—æˆ–æ¨¡å‹å®¹é‡æ— å…³ã€‚æ— è®ºå­¦ä¹ èƒ½åŠ›æœ‰å¤šå¼ºï¼Œæ™ºèƒ½ä½“é‡‡å–è¡ŒåŠ¨çš„é¢‘ç‡ï¼ˆæˆ–ç¼ºä¹çš„ä¿¡æ¯ï¼‰éƒ½æ˜¯æœ‰é™çš„ã€‚ ä½ å¯¹è¿™ä¸ªå›°å¢ƒæœ‰ä»€ä¹ˆçœ‹æ³•ï¼Ÿ    æäº¤äºº    /u/XecutionStyle   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hi1d9n/decision_frequency_an_information_perspective/</guid>
      <pubDate>Thu, 19 Dec 2024 19:48:50 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ç¨³å®šåŸºçº¿ 3 è¿›è¡Œ SAC è®­ç»ƒå¯åœæ­¢ TensorBoard æ›´æ–°ï¼Œå¹¶åœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­ 3,000 æ­¥ååŠ é€Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhv1ij/sac_training_with_stable_baselines3_halts/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘åœ¨è‡ªå®šä¹‰ç¯å¢ƒä¸­ä½¿ç”¨Soft Actor-Critic (SAC)ç®—æ³•ï¼Œå…¶ä¸­ä»£ç†æ¯æ¬¡è¿­ä»£éƒ½ä¼šè°ƒæ•´å¦ä¸€ä¸ªä¼˜åŒ–å™¨çš„è¶…å‚æ•°ã€‚æœ€åˆï¼Œè®­ç»ƒå’Œå­¦ä¹ é¡ºåˆ©è¿›è¡Œï¼Œæœ€å¤šå¯è¾¾3,000 ä¸ªæ—¶é—´æ­¥ã€‚ç„¶è€Œï¼Œåœ¨æ­¤ä¹‹åï¼ŒTensorBoard åœæ­¢æ›´æ–°ï¼Œå¹¶ä¸”è®­ç»ƒé€Ÿåº¦æ€¥å‰§å¢åŠ ï¼Œä½†æ²¡æœ‰å–å¾—ä»»ä½•æœ‰æ„ä¹‰çš„è¿›å±•ã€‚ æœ‰äººé‡åˆ°è¿‡ç±»ä¼¼çš„é—®é¢˜æˆ–å¯ä»¥æå‡ºæ½œåœ¨çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆå—ï¼Ÿ è°¢è°¢ï¼    æäº¤äºº    /u/YasinRL   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhv1ij/sac_training_with_stable_baselines3_halts/</guid>
      <pubDate>Thu, 19 Dec 2024 15:14:06 GMT</pubDate>
    </item>
    <item>
      <title>â€œMaxInfoRLï¼šé€šè¿‡ä¿¡æ¯å¢ç›Šæœ€å¤§åŒ–ä¿ƒè¿›å¼ºåŒ–å­¦ä¹ ä¸­çš„æ¢ç´¢â€ï¼ŒSukhija ç­‰äººï¼Œ2024 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</guid>
      <pubDate>Thu, 19 Dec 2024 06:30:27 GMT</pubDate>
    </item>
    <item>
      <title>SAC ä»£ç†åœ¨æˆ‘ä»¬çš„è‡ªå®šä¹‰è§†é¢‘æ¸¸æˆç¯å¢ƒä¸­æ²¡æœ‰å­¦åˆ°ä»»ä½•ä¸œè¥¿ - æ•‘å‘½ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</guid>
      <pubDate>Wed, 18 Dec 2024 20:57:41 GMT</pubDate>
    </item>
    <item>
      <title>æ³•å­¦ç¡•å£« (LLM) å’Œçº¿ä¸‹å¼ºåŒ–å­¦ä¹ </title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</link>
      <description><![CDATA[ç”±äº LLM æ¨¡å‹ä»¥æŸç§æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚è¡Œä¸ºå…‹éš†ï¼Œé‚£ä¹ˆä½¿ç”¨ç¦»çº¿ RL è¿›è¡Œè®­ç»ƒæ€ä¹ˆæ ·ï¼Ÿ æˆ‘çŸ¥é“å¥–åŠ±è®¾è®¡å°†æ˜¯ä¸€ä¸ªé‡å¤§æŒ‘æˆ˜å’Œå¯æ‰©å±•æ€§ç­‰ã€‚ ä½ æ€ä¹ˆçœ‹ï¼Ÿ    æäº¤äºº    /u/Blasphemer666   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</guid>
      <pubDate>Wed, 18 Dec 2024 17:29:39 GMT</pubDate>
    </item>
    <item>
      <title>åŠªåŠ›è®­ç»ƒç”¨äºè·¯çº¿ä¼˜åŒ–çš„ Dueling DQN æ¨¡å‹ â€“ éœ€è¦æœ‰å…³å­¦ä¹ å’Œè®¡ç®—è¦æ±‚çš„å»ºè®® ğŸ˜¢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ä½¿ç”¨ Dueling DQN åœ¨è‡ªå®šä¹‰é“è·¯ç½‘ç»œç¯å¢ƒä¸­è¿›è¡Œè·¯çº¿ä¼˜åŒ–é¡¹ç›®ï¼Œè¯¥ç¯å¢ƒå…·æœ‰è®¸å¤šèŠ‚ç‚¹å’Œä¸åŒçš„åŠ¨ä½œç©ºé—´ã€‚ä½†æ˜¯ï¼Œè¯¥æ¨¡å‹æ— æ³•æ­£ç¡®å­¦ä¹  - è®­ç»ƒç»“æœä¸ä¸€è‡´ï¼Œå¹¶ä¸”ä»£ç†éš¾ä»¥æ‰¾åˆ°æœ€ä½³è·¯å¾„ã€‚ æœ‰äººæœ‰å…´è¶£è´¡çŒ®å—     æäº¤äºº    /u/ProfessionalType9800   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</guid>
      <pubDate>Wed, 18 Dec 2024 14:54:11 GMT</pubDate>
    </item>
    <item>
      <title>David Silver ç¤ºä¾‹è€ƒè¯•é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</link>
      <description><![CDATA[      å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨æŸ¥çœ‹ David Silver ç½‘ç«™ä¸Šçš„ç»ƒä¹ è€ƒè¯•ï¼Œä½†ä¼¼ä¹æ— æ³•ç†è§£æœ¬é¡µæœ€åä¸€ä¸ªé—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚å¯¹äºçŠ¶æ€ä¸€çš„ lambda è¿”å›ï¼Œå®ƒä¸åº”è¯¥æ˜¯ 0.5**2 x 1 è€Œä¸æ˜¯ 0.5 x 1ã€‚ä¹‹åï¼Œæˆ‘å®Œå…¨ä¸çŸ¥é“çŠ¶æ€ 2 å’Œ 3 çš„è¿”å›å€¼äº†ã€‚    æäº¤äºº    /u/LostBandard   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</guid>
      <pubDate>Wed, 18 Dec 2024 02:29:26 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ å·¥ä½œåŸç†çš„ç¤ºä¾‹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</link>
      <description><![CDATA[  ç”±    /u/A-Sexy-Name  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</guid>
      <pubDate>Tue, 17 Dec 2024 21:11:18 GMT</pubDate>
    </item>
    </channel>
</rss>