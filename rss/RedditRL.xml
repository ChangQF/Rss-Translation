<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Tue, 04 Feb 2025 12:33:06 GMT</lastBuildDate>
    <item>
      <title>åœ¨å•å°æœºå™¨ä¸Šè¿è¡Œ Ray Tune çš„å¹¶è¡Œå®éªŒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ Ray çš„æ–°æ‰‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæµè¡Œçš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œå°¤å…¶é€‚ç”¨äº MLï¼Œæˆ‘ä¸€ç›´è‡´åŠ›äºå……åˆ†åˆ©ç”¨æˆ‘æœ‰é™çš„ä¸ªäººè®¡ç®—èµ„æºã€‚è¿™å¯èƒ½æ˜¯æˆ‘æƒ³è¦äº†è§£ Ray åŠå…¶åº“çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚å—¯ï¼Œæˆ‘ç›¸ä¿¡è®¸å¤šå­¦ç”Ÿå’Œä¸ªäººç ”ç©¶äººå‘˜éƒ½æœ‰ç›¸åŒçš„åŠ¨æœºã€‚åœ¨ä½¿ç”¨ Ray Tuneï¼ˆå…¨éƒ¨åŸºäº Pythonï¼‰è¿›è¡Œä¸€äº›å®éªŒåï¼Œæˆ‘å¼€å§‹æ„Ÿåˆ°ç–‘æƒ‘å¹¶æƒ³å¯»æ±‚å¸®åŠ©ã€‚ä»»ä½•å¸®åŠ©éƒ½å°†ä¸èƒœæ„Ÿæ¿€ï¼ ğŸ™ğŸ™ğŸ™ï¼š  Ray åœ¨å•å°æœºå™¨ä¸Šä»ç„¶æœ‰æ•ˆä¸”é«˜æ•ˆå—ï¼Ÿ æ˜¯å¦å¯ä»¥ä½¿ç”¨ Ray åœ¨å•å°æœºå™¨ä¸Šè¿è¡Œå¹¶è¡Œå®éªŒï¼ˆåœ¨æˆ‘çš„æƒ…å†µä¸‹æ˜¯ Tuneï¼‰ï¼Ÿ æˆ‘çš„è„šæœ¬æ˜¯å¦ä¸ºæ­¤ç›®çš„æ­£ç¡®è®¾ç½®ï¼Ÿ æˆ‘é—æ¼äº†ä»€ä¹ˆå—ï¼Ÿ  æ•…äº‹ï¼š* æˆ‘çš„è®¡ç®—èµ„æºéå¸¸æœ‰é™ï¼šä¸€å°é…å¤‡ 12 æ ¸ CPU å’Œ RTX 3080 Ti GPU ä»¥åŠ 12GB å†…å­˜çš„æœºå™¨ã€‚ * æˆ‘çš„ç©å…·å®éªŒæ²¡æœ‰å……åˆ†åˆ©ç”¨å¯ç”¨èµ„æºï¼šå•æ¬¡æ‰§è¡Œè€—è´¹ 11% çš„ GPU Util å’Œ 300MiB /11019MiBã€‚ * ä»ç†è®ºä¸Šè®²ï¼Œåº”è¯¥å¯ä»¥åœ¨è¿™æ ·çš„æœºå™¨ä¸ŠåŒæ—¶è¿›è¡Œ 8-9 ä¸ªè¿™æ ·çš„ç©å…·å®éªŒä¸€å°æœºå™¨ã€‚ * è‡ªç„¶è€Œç„¶ï¼Œæˆ‘æ±‚åŠ©äº Rayï¼Œå¸Œæœ›å®ƒèƒ½å¸®åŠ©ç®¡ç†å’Œè¿è¡Œå…·æœ‰ä¸åŒè¶…å‚æ•°ç»„çš„å¹¶è¡Œå®éªŒã€‚ * ä½†æ˜¯ï¼Œæ ¹æ®ä¸‹é¢çš„è„šæœ¬ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•å¹¶è¡Œæ‰§è¡Œï¼Œå³ä½¿æˆ‘åœ¨tune.run()ä¸­è®¾ç½®äº†max_concurrent_trialsã€‚æ ¹æ®æˆ‘çš„è§‚å¯Ÿï¼Œæ‰€æœ‰å®éªŒä¼¼ä¹ä¸€ä¸ªæ¥ä¸€ä¸ªè¿è¡Œã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä¸çŸ¥é“å¦‚ä½•ä¿®å¤æˆ‘çš„ä»£ç ä»¥å®ç°é€‚å½“çš„å¹¶è¡Œæ€§ã€‚ ğŸ˜­ğŸ˜­ğŸ˜­ï¼š* ä»¥ä¸‹æ˜¯æˆ‘çš„ ray tune è„šæœ¬ï¼ˆray_experiment.pyï¼‰ ```python import os import ray from ray import tune from ray.tune import CLIReporter from ray.tune.schedulers import ASHAScheduler from Simulation import run_simulations # Ray Tune ä¸­çš„å¯è®­ç»ƒå¯¹è±¡ from utils.trial_name_generator import trial_name_generator if name == &#39;maâ€‹â€‹in&#39;: ray.init() # è°ƒè¯•æ¨¡å¼ï¼šray.init(local_mode=True) # ray.init(num_cpus=12, num_gpus=1) print(ray.available_resources()) current_dir = os.path.abspath(os.getcwd()) # å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„params_groups = { &#39;exp_name&#39;: &#39;Ray_Tune&#39;, # æœç´¢ç©ºé—´ &#39;lr&#39;: tune.choice([1e-7, 1e-4]), &#39;simLength&#39;: tune.choice([400, 800]), } reporter = CLIReporter( metric_columns=[&quot;exp_progress&quot;,&quot;eval_episodes&quot;,&quot;best_r&quot;,&quot;current_r&quot;], print_intermediate_tables=True, ) analysis = tune.run( run_simulations, name=params_groups[&#39;exp_name&#39;], mode=&quot;max&quot;, config=params_groups, resources_per_trial={&quot;gpu&quot;: 0.25, &quot;cpu&quot;: 10}, max_concurrent_trials=8, # scheduler=scheduler, storage_path=f&#39;{current_dir}/logs/&#39;, # ä¿å­˜æ—¥å¿—çš„ç›®å½• trial_dirname_creator=trial_name_generator, trial_name_creator=trial_name_generator, # resume=&quot;AUTO&quot; ) print(&quot;Best config:&quot;, analysis.get_best_config(metric=&quot;best_r&quot;, mode=&quot;max&quot;)) ray.shutdown()  ```    submitted by    /u/yxwmm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</guid>
      <pubDate>Tue, 04 Feb 2025 11:35:04 GMT</pubDate>
    </item>
    <item>
      <title>æ‰˜ç›˜è£…è½½é—®é¢˜ PPO æ¨¡å‹å®é™…ä¸Šä¸èµ·ä½œç”¨ - éœ€è¦å¸®åŠ©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihcbpo/pallet_loading_problem_ppo_model_is_not_really/</link>
      <description><![CDATA[      å› æ­¤ï¼Œæˆ‘æ­£åœ¨ç ”ç©¶ä¸€ç§ PPO å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åº”è¯¥èƒ½å¤Ÿä»¥æœ€ä½³æ–¹å¼å°†ç®±å­è£…è½½åˆ°æ‰˜ç›˜ä¸Šã€‚å­˜åœ¨ç¨³å®šæ€§ï¼ˆå¯èƒ½æ‚¬å‚ 20%ï¼‰å’Œç ´ç¢ï¼ˆæ¯ä¸ªç®±å­éƒ½æœ‰ä¸€ä¸ªç ´ç¢å‚æ•° - æ‚¨å¯ä»¥å°†ç®±å­å †å åœ¨å…·æœ‰æ›´å¤§ç ´ç¢å€¼çš„ç®±å­é¡¶éƒ¨ï¼‰çº¦æŸã€‚ æˆ‘æ­£åœ¨ä½¿ç”¨ç¦»æ•£è§‚å¯Ÿå’Œè¡ŒåŠ¨ç©ºé—´ã€‚æˆ‘ä¸ºä»£ç†åˆ›å»ºäº†ä¸€ä¸ªå¯èƒ½ä½ç½®åˆ—è¡¨ï¼Œè¯¥åˆ—è¡¨ä¼ é€’äº†æ‰€æœ‰çº¦æŸï¼Œç„¶åä»£ç†æœ‰ 5 ç§å¯èƒ½çš„æ“ä½œ - åœ¨ä½ç½®åˆ—è¡¨ä¸­å‰è¿›æˆ–åé€€ã€æ—‹è½¬ç®±å­ï¼ˆä»…åœ¨ä¸€ä¸ªè½´ä¸Šï¼‰ã€æ”¾ä¸‹ä¸€ä¸ªç®±å­å¹¶è·³è¿‡ä¸€ä¸ªç®±å­å¹¶è½¬åˆ°ä¸‹ä¸€ä¸ªã€‚ç®±å­å…ˆæŒ‰ç ´ç¢æ’åºï¼Œç„¶åæŒ‰é«˜åº¦æ’åºã€‚ è§‚å¯Ÿç©ºé—´å¦‚ä¸‹ï¼šæ‰˜ç›˜çš„é«˜åº¦å›¾ - æ‚¨å¯ä»¥æƒ³è±¡å®ƒå°±åƒä»é¡¶éƒ¨çœ‹æ‰˜ç›˜ - å¦‚æœå€¼ä¸º 0ï¼Œåˆ™è¡¨ç¤ºå®ƒæ˜¯åœ°é¢ï¼Œ1 - æ‰˜ç›˜å·²å¡«æ»¡ã€‚æˆ‘æ›¾å°è¯•ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Œä½†æ²¡æœ‰ä»»ä½•æ”¹å˜ã€‚ç„¶åæˆ‘æœ‰ä»£ç†åæ ‡ï¼ˆxï¼Œyï¼Œzï¼‰ã€ç®±å­å‚æ•°ï¼ˆé•¿åº¦ã€å®½åº¦ã€é«˜åº¦ã€é‡é‡ã€ç ´ç¢ï¼‰ã€æ¥ä¸‹æ¥çš„ 5 ä¸ªç®±å­çš„å‚æ•°ã€ä¸‹ä¸€ä¸ªä½ç½®ã€å¯èƒ½çš„ä½ç½®æ•°é‡ã€ä½ç½®åˆ—è¡¨ä¸­çš„ç´¢å¼•ã€å‰©ä½™çš„ç®±å­æ•°ä»¥åŠç®±å­åˆ—è¡¨çš„ç´¢å¼•ã€‚ æˆ‘å°è¯•äº†å„ç§å¥–åŠ±å‡½æ•°ï¼Œä½†éƒ½æ²¡æœ‰æˆåŠŸã€‚ç›®å‰æˆ‘æœ‰è¿™æ ·çš„åŠŸèƒ½ï¼šæ— è®ºå¦‚ä½•åœ¨å¯¼èˆªä½ç½®åˆ—è¡¨æ—¶ -0.1ï¼Œå¯¹äºä¸å¦ä¸€ä¸ªç®±å­é«˜åº¦ç›¸ç­‰çš„ç®±å­çš„æ¯ä¸€æ¡è¾¹ +0.5ï¼Œå¦‚æœåœ¨æ”¹å˜ä½ç½®åè¿™äº›è¾¹çš„æ•°é‡æ›´å¤§ï¼Œåˆ™å¯¹äºæ¥è§¦å¦ä¸€ä¸ªç®±å­çš„æ¯ä¸€æ¡è¾¹ +0.5ã€‚æ—‹è½¬æ—¶å¥–åŠ±ç›¸åŒï¼Œåªæ¯”è¾ƒæœ€ä½ä½ç½®å’Œä½ç½®è®¡æ•°ã€‚é€‰æ‹©ä¸‹ä¸€ä¸ªç®±å­æ—¶ç›¸åŒï¼Œä½†æ¯”è¾ƒæœ€ä½é«˜åº¦ã€‚æœ€åï¼Œå½“æ”¾ä¸‹ä¸€ä¸ªç›’å­æ—¶ï¼Œæ¯ä¸ªæ¥è§¦è¾¹+1æˆ–å½¢æˆç›¸ç­‰çš„é«˜åº¦å’Œ+3å›ºå®šå¥–åŠ±ã€‚ æˆ‘çš„ç¥ç»ç½‘ç»œç”±ä¸€ä¸ªé¢å¤–çš„å±‚ç»„æˆï¼Œç”¨äºè§‚å¯Ÿä¸æ˜¯é«˜åº¦å›¾çš„è§‚å¯Ÿï¼ˆè¾“å‡º - 256ä¸ªç¥ç»å…ƒï¼‰ï¼Œç„¶åæ˜¯2ä¸ªéšè—å±‚ï¼Œåˆ†åˆ«æœ‰1024å’Œ512ä¸ªç¥ç»å…ƒï¼Œæœ€åæ˜¯æ¼”å‘˜è¯„è®ºå®¶å¤´ã€‚æˆ‘å¯¹é«˜åº¦å›¾å’Œæ¯ä¸ªåæ ‡è¿›è¡Œäº†æ ‡å‡†åŒ–ã€‚ æˆ‘ä½¿ç”¨çš„è¶…å‚æ•°ï¼š learningRate = 3e-4 betas = [0.9, 0.99] gamma = 0.995 epsClip = 0.2 epochs = 10 updateTimeStep = 500 entropyCoefficient = 0.01 gaeLambda = 0.98 è§£å†³é—®é¢˜ - æˆ‘çš„æ¨¡å‹æ— æ³•æ”¶æ•›ï¼ˆä»ç»˜åˆ¶ç»Ÿè®¡æ•°æ®å¯ä»¥çœ‹å‡ºï¼Œå®ƒä¼¼ä¹åœ¨é‡‡å–éšæœºåŠ¨ä½œã€‚æˆ‘å·²ç»è°ƒè¯•äº†ä»£ç å¾ˆé•¿æ—¶é—´ï¼Œä¼¼ä¹åŠ¨ä½œæ¦‚ç‡æ­£åœ¨å‘ç”Ÿå˜åŒ–ï¼ŒæŸå¤±è®¡ç®—æ­£åœ¨æ­£ç¡®å®Œæˆï¼Œåªæ˜¯å…¶ä»–åœ°æ–¹å‡ºäº†é—®é¢˜ã€‚å¯èƒ½æ˜¯ç”±äºè§‚å¯Ÿç©ºé—´ä¸å¥½å—ï¼Ÿç¥ç»ç½‘ç»œæ¶æ„ï¼Ÿä½ ä¼šæ¨èä½¿ç”¨ CNN å—ä¸å·ç§¯åçš„å…¶ä»–è§‚å¯Ÿç»“æœç›¸ç»“åˆï¼Ÿ æˆ‘é™„ä¸Šäº†æ¨¡å‹å’Œç»Ÿè®¡æ•°æ®çš„å¯è§†åŒ–ã€‚æå‰æ„Ÿè°¢æ‚¨çš„å¸®åŠ© https://preview.redd.it/kb9u2besp2he1.png?width=901&amp;format=png&amp;auto=webp&amp;s=b218e8573fd811d97cefcdd734a69590cbfd1dcd    æäº¤äºº    /u/bimbum12   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihcbpo/pallet_loading_problem_ppo_model_is_not_really/</guid>
      <pubDate>Tue, 04 Feb 2025 07:25:36 GMT</pubDate>
    </item>
    <item>
      <title>â€œé€šè¿‡éšæ€§å¥–åŠ±å¼ºåŒ–è¿‡ç¨‹â€ï¼ŒCui ç­‰äºº 2025 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</guid>
      <pubDate>Tue, 04 Feb 2025 06:21:10 GMT</pubDate>
    </item>
    <item>
      <title>ç¬¬ 7 ç‰ˆ Isaac Lab æ•™ç¨‹å‘å¸ƒï¼ä¸‹ä¸€æ­¥æˆ‘åº”è¯¥è®²ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼åªæ˜¯æƒ³é¡ºä¾¿è¯´ä¸€å¥ï¼Œæ„Ÿè°¢å¤§å®¶å¯¹æˆ‘çš„ Isaac Lab æ•™ç¨‹çš„æ”¯æŒå’Œé¼“åŠ±ã€‚åé¦ˆéå¸¸æ£’ï¼Œå¾ˆé«˜å…´çœ‹åˆ°å®ƒä»¬å¯¹ä½ æœ‰å¤šæœ‰ç”¨ï¼Œè€å®è¯´ï¼Œæˆ‘åœ¨åˆ¶ä½œå®ƒä»¬çš„åŒæ—¶è‡ªå·±ä¹Ÿå­¦åˆ°äº†å¾ˆå¤šä¸œè¥¿ï¼ æˆ‘åˆšåˆšåœ¨ä¸åˆ° 2 ä¸ªæœˆçš„æ—¶é—´å†…å‘å¸ƒäº†æˆ‘çš„ç¬¬ 7 ä¸ªæ•™ç¨‹ï¼Œæˆ‘æƒ³ä¿æŒè¿™ç§åŠ¿å¤´ã€‚æˆ‘ç°åœ¨å°†ç»§ç»­åˆ¶ä½œå®˜æ–¹æ–‡æ¡£ï¼Œä½†æ¥ä¸‹æ¥ä½ å¸Œæœ›çœ‹åˆ°ä»€ä¹ˆï¼Ÿ â€œä»é›¶åˆ°è‹±é›„â€ç³»åˆ—ä¼šå¾ˆæœ‰è¶£å—ï¼Ÿç±»ä¼¼äºï¼š - è®¾è®¡å’Œåœ¨ Isaac Sim ä¸­æ¨¡æ‹Ÿæœºå™¨äºº - åœ¨ Isaac Lab ä¸­ä»å¤´å¼€å§‹ä½¿ç”¨ RL å¯¹å…¶è¿›è¡Œè®­ç»ƒ - ï¼ˆæœ€ç»ˆï¼‰å°†å…¶éƒ¨ç½²åœ¨çœŸæ­£çš„æœºå™¨äººä¸Š......ä¸€æ—¦æˆ‘ä¹°å¾—èµ·ä¸€ä¸ªğŸ˜… è®©æˆ‘çŸ¥é“æ‚¨è§‰å¾—æœ€ä»¤äººå…´å¥‹æˆ–æœ€æœ‰å¸®åŠ©çš„æ˜¯ä»€ä¹ˆï¼éšæ—¶æ¬¢è¿å»ºè®®ã€‚ æˆ‘åœ¨ YouTube ä¸Šä¸Šä¼ äº†è¿™äº›ï¼š Isaac Lab æ•™ç¨‹ - LycheeAI    æäº¤äºº    /u/LoveYouChee   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ih1rch/7th_isaac_lab_tutorial_released_what_should_i/</guid>
      <pubDate>Mon, 03 Feb 2025 22:16:38 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ RL è¿›è¡Œå¸ƒå±€ç”Ÿæˆï¼ˆä¾‹å¦‚ï¼šé“è·¯å’Œæˆ¿å±‹ï¼‰çš„æœ€ä½³æ–¹æ³•ã€‚å½“å‰æ¨¡å‹æœªè¿›è¡Œå­¦ä¹ ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨ RL ç”Ÿæˆç®€å•éƒŠåŒºçš„å¸ƒå±€ï¼šé“è·¯ã€éšœç¢ç‰©å’Œæˆ¿å±‹ã€‚è¿™æ›´åƒæ˜¯ä¸€ä¸ªå®éªŒï¼Œä½†æˆ‘æœ€å¥½å¥‡çš„æ˜¯æƒ³çŸ¥é“æˆ‘æ˜¯å¦æœ‰ä»»ä½•å˜åŒ–å¯ä»¥ä½¿ç”¨ RL ä¸ºæ­¤ç±»é—®é¢˜æå‡ºåˆç†çš„è®¾è®¡ã€‚ tensorboard ç›®å‰æˆ‘å·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼ˆä½¿ç”¨ gymnasium å’Œ stable_baselines3ï¼‰ã€‚æˆ‘æœ‰ä¸€ä¸ªç®€å•çš„è®¾ç½®ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªåä¸º env çš„ç¯å¢ƒï¼Œæˆ‘å°†æˆ‘çš„ä¸–ç•Œè¡¨ç¤ºä¸ºç½‘æ ¼ï¼š  æˆ‘ä»ä¸€ä¸ªç©ºç½‘æ ¼å¼€å§‹ï¼Œé™¤äº†é“è·¯å…ƒç´ ï¼ˆå…¥å£ç‚¹ï¼‰å’Œä¸€äº›æ— æ³•ä½¿ç”¨çš„å•å…ƒæ ¼ï¼ˆéšœç¢ç‰©ï¼Œä¾‹å¦‚å°æ¹–ï¼‰ æ¨¡å‹é‡‡å–çš„æ“ä½œæ˜¯ï¼Œåœ¨æ¯ä¸€æ­¥ï¼Œæ”¾ç½®ä¸€ä¸ªé“è·¯æˆ–æˆ¿å±‹çš„ç“·ç –ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šï¼ˆtile_positionï¼Œtile_typeï¼‰  è‡³äºæˆ‘çš„å¥–åŠ±ï¼Œå®ƒä¸æ•´ä½“è®¾è®¡ç›¸å…³ï¼ˆè€Œä¸ä»…ä»…æ˜¯å¯¹æœ€åä¸€æ­¥çš„å¥–åŠ±ï¼Œå› ä¸ºæ—©æœŸçš„é€‰æ‹©å¯èƒ½ä¼šå¯¹ä»¥åäº§ç”Ÿå½±å“ã€‚å¹¶ä¸”æœ€å¤§åŒ–è®¾è®¡çš„æ•´ä½“è´¨é‡ï¼Œè€Œä¸æ˜¯å±€éƒ¨çš„ï¼‰ï¼ŒåŸºæœ¬ä¸Šæœ‰ 3 ä¸ªåŠ æƒé¡¹ï¼š  é“è·¯ç½‘ç»œåº”è¯¥æœ‰æ„ä¹‰ï¼šè¿æ¥åˆ°å…¥å£ï¼Œæ¯ä¸ªç“·ç –åº”è¯¥è¿æ¥åˆ°è‡³å°‘ 1 ä¸ªå…¶ä»–é“è·¯ç“·ç –ã€‚å¹¶ä¸”æ²¡æœ‰ 2x2 çš„é“è·¯ç“·ç –é›†ã€‚-&gt; æ•´ä¸ªè®¾è®¡ï¼ˆæ‰€æœ‰é“è·¯ç“·ç –ï¼‰çš„æ€»å’Œï¼ˆæ¯ä¸ªå¥½ç“·ç –çš„å¥–åŠ±å¢åŠ ï¼Œæ¯ä¸ªåç“·ç –çš„å¥–åŠ±å‡å°‘ï¼‰ã€‚è¿˜å°è¯•äº†æ‰€æœ‰ç“·ç –çš„ min() åˆ†æ•°ã€‚ æˆ¿å±‹åº”å§‹ç»ˆè¿æ¥åˆ°è‡³å°‘ 1 æ¡é“è·¯ã€‚-&gt; æ•´ä¸ªè®¾è®¡ï¼ˆæ‰€æœ‰æˆ¿å±‹ç“·ç –ï¼‰çš„æ€»å’Œï¼ˆæ¯ä¸ªå¥½ç“·ç –çš„å¥–åŠ±å¢åŠ ï¼Œæ¯ä¸ªåç“·ç –çš„å¥–åŠ±å‡å°‘ï¼‰ã€‚è¿˜å°è¯•äº†æ‰€æœ‰ç“·ç –çš„ min() åˆ†æ•°ã€‚ æœ€å¤§åŒ–æˆ¿å±‹ç“·ç –çš„æ•°é‡ï¼ˆç“·ç –è¶Šå¤šï¼Œå¥–åŠ±å°±è¶Šé«˜ï¼‰  æ¯å½“æˆ‘å°è¯•è¿è¡Œå®ƒå¹¶è®©å®ƒå­¦ä¹ æ—¶ï¼Œæˆ‘éƒ½ä¼šä»è¾ƒä½çš„ entropy_lossï¼ˆ-5ï¼Œåœ¨ 100k æ­¥åæ…¢æ…¢çˆ¬å‡è‡³ 0ï¼‰å’ŒåŸºæœ¬ä¸Šä¸º 0 çš„ explained_variance å¼€å§‹ã€‚æˆ‘çš„ç†è§£æ˜¯ï¼šæ¨¡å‹æ°¸è¿œæ— æ³•æ­£ç¡®é¢„æµ‹å®ƒé‡‡å–çš„ç»™å®šåŠ¨ä½œçš„å¥–åŠ±æ˜¯ä»€ä¹ˆã€‚å¹¶ä¸”å®ƒé‡‡å–çš„è¡ŒåŠ¨å¹¶ä¸æ¯”éšæœºå¥½ã€‚ æˆ‘å¯¹ RL è¿˜å¾ˆé™Œç”Ÿï¼Œæˆ‘çš„èƒŒæ™¯æ›´â€œä¼ ç»Ÿâ€ MLã€NLPï¼Œå¹¶ä¸”éå¸¸ç†Ÿæ‚‰è¿›åŒ–ç®—æ³•ã€‚ æˆ‘è®¤ä¸ºè¿™å¯èƒ½åªæ˜¯ä¸€ä¸ªå†·å¯åŠ¨é—®é¢˜ï¼Œæˆ–è€…è¯¾ç¨‹å­¦ä¹ å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚ä½†å³ä½¿å¦‚æ­¤ï¼Œæˆ‘ä¹Ÿä»ç®€å•çš„è®¾è®¡å¼€å§‹ã€‚ä¾‹å¦‚ 6x6 ç½‘æ ¼ã€‚æˆ‘è§‰å¾—è¿™æ›´å¤šçš„æ˜¯æˆ‘çš„å¥–åŠ±å‡½æ•°è®¾è®¡æ–¹å¼çš„é—®é¢˜ã€‚æˆ–è€…ä¹Ÿè®¸ä¸æˆ‘å¦‚ä½•æ„å»ºé—®é¢˜æœ‰å…³ã€‚ ------ é—®é¢˜ï¼šåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨é€šå¸¸å¦‚ä½•å¤„ç†è¿™æ ·çš„é—®é¢˜ï¼Ÿæœ‰äº†å®ƒï¼Œæœ‰å“ªäº›æ ‡å‡†æ–¹æ³•å¯ä»¥â€œè°ƒè¯•â€æ­¤ç±»é—®é¢˜ï¼Ÿä¾‹å¦‚ï¼Œçœ‹çœ‹é—®é¢˜æ˜¯å¦æ›´å¤šåœ°ä¸æˆ‘é€‰æ‹©çš„æ“ä½œç±»å‹æœ‰å…³ï¼Œæˆ–è€…ä¸æˆ‘çš„å¥–åŠ±è®¾è®¡æ–¹å¼ç­‰æœ‰å…³    æäº¤äºº    /u/LostInGradients   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igzve5/best_way_to_approach_layout_generation_ex_roads/</guid>
      <pubDate>Mon, 03 Feb 2025 21:00:15 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦æŒ‡å¯¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ‹¥æœ‰æ•°å­¦å­¦ä½ï¼Œå¹¶é€‰ä¿®äº†å‡ é—¨æœºå™¨å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹  (RL) è¯¾ç¨‹ã€‚ç›®å‰ï¼Œæˆ‘æ­£åœ¨å·¥ä½œï¼Œä½†æˆ‘å¯¹ RL ç ”ç©¶éå¸¸æ„Ÿå…´è¶£ã€‚è™½ç„¶æˆ‘è¿˜æ²¡æœ‰å¤ªå¤šçš„çŸ¥è¯†ï¼Œä½†æˆ‘åœ¨ç©ºé—²æ—¶é—´å­¦ä¹  RLã€‚ æœªæ¥ï¼Œæˆ‘æƒ³ä»äº‹ RL ç ”ç©¶å·¥ä½œï¼Œä½†æˆ‘ä¸çŸ¥é“è¯¥æ€ä¹ˆåšã€‚æˆ‘åº”è¯¥å‡†å¤‡ GATE å¹¶ç”³è¯· IIT/IIScï¼Œè¿˜æ˜¯åº”è¯¥ç›´æ¥ç”³è¯·é¡¶å°–çš„å¤–å›½å¤§å­¦ï¼Œå°½ç®¡æˆ‘æ²¡æœ‰ç ”ç©¶ç»éªŒï¼Ÿ    æäº¤äºº    /u/BigBuddy1276   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 13:24:45 GMT</pubDate>
    </item>
    <item>
      <title>ç»“æœçš„å¯é‡å¤æ€§</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</link>
      <description><![CDATA[æ‚¨å¥½ï¼æˆ‘æ­£åœ¨å°è¯•æŸ¥æ‰¾æœ¬æ–‡ä¸­æåˆ°çš„åŸºäºæ¨¡å‹çš„ PPO çš„å®ç°ï¼šåŸºäºæ¨¡å‹çš„æ¢ç´¢çš„ç­–ç•¥ä¼˜åŒ–ï¼Œä»¥ä¾¿é‡ç°ç»“æœå¹¶å¯èƒ½åœ¨æˆ‘çš„è®ºæ–‡ä¸­ä½¿ç”¨è¯¥æ¶æ„ã€‚ä½†ä¼¼ä¹ä»»ä½•åœ°æ–¹éƒ½æ²¡æœ‰å®˜æ–¹å®ç°ã€‚æˆ‘å·²ç»ç»™ä½œè€…å‘äº†ç”µå­é‚®ä»¶ï¼Œä½†ä¹Ÿæ²¡æœ‰æ”¶åˆ°ä»»ä½•å›å¤ã€‚ åœ¨åƒ AAAI è¿™æ ·çš„å¤§å‹ä¼šè®®ä¸Šå‘è¡¨çš„è®ºæ–‡æ²¡æœ‰ä»»ä½•å¯é‡ç°çš„å®ç°ï¼Œè¿™æ­£å¸¸å—ï¼Ÿ    æäº¤äºº    /u/GamingOzz   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</guid>
      <pubDate>Mon, 03 Feb 2025 13:18:10 GMT</pubDate>
    </item>
    <item>
      <title>â€œKimi k1.5ï¼šä½¿ç”¨ LLM æ‰©å±•å¼ºåŒ–å­¦ä¹ â€ï¼ŒKimi å›¢é˜Ÿ 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</guid>
      <pubDate>Mon, 03 Feb 2025 12:53:20 GMT</pubDate>
    </item>
    <item>
      <title>å¸®åŠ©æ¶ˆé™¤é”™è¯¯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</link>
      <description><![CDATA[å˜¿ï¼Œæˆ‘ç›®å‰æ­£åœ¨ä»¥æ·±åº¦ q å­¦ä¹ æ¨¡å‹çš„å½¢å¼è®­ç»ƒæˆ‘çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚åœ¨å°è¯•ä½¿ç”¨ Python ä¸­çš„ keras æ—¶ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¦‚æœæœ‰äººæ„¿æ„å¸®åŠ©æˆ‘å¼„æ¸…æ¥šå¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘å°†ä¸èƒœæ„Ÿæ¿€ã€‚ï¼ˆå®ƒä»¬å¯¹äºæˆ‘çš„é¡¹ç›®æ¥è¯´éå¸¸å…·ä½“ï¼Œå› æ­¤å¾ˆéš¾åœ¨ DM ä¹‹å¤–è§£é‡ŠğŸ˜…ï¼‰    æäº¤äºº    /u/at_69_420   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</guid>
      <pubDate>Mon, 03 Feb 2025 11:08:58 GMT</pubDate>
    </item>
    <item>
      <title>ç¬¬ä¸€å±Š Tinker AI å¤§èµ›è·èƒœä½œå“ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</link>
      <description><![CDATA[        æäº¤äºº    /u/goncalogordo   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</guid>
      <pubDate>Mon, 03 Feb 2025 10:00:02 GMT</pubDate>
    </item>
    <item>
      <title>å°è¯•å¤åˆ¶æ™®é€šçš„ k-bandits é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨å°è¯•å®ç° Barto Sutton ä¹¦ä¸­çš„ç¬¬ä¸€ä¸ª k-Bandits æµ‹è¯•å¹³å°ã€‚Python ä»£ç å¯åœ¨ Git ä¸Šæ‰¾åˆ°ï¼Œä½†æˆ‘æ­£å°è¯•ä»å¤´å¼€å§‹ç‹¬ç«‹å®Œæˆã€‚ æˆªè‡³ç›®å‰ï¼Œæˆ‘æ­£åœ¨å°è¯•ç”Ÿæˆå›¾ 2.2 ä¸­çš„å¹³å‡å¥–åŠ±å›¾ã€‚æˆ‘çš„ä»£ç å¯ä»¥å·¥ä½œï¼Œä½†å¹³å‡å¥–åŠ±å›¾è¿‡æ—©ç¨³å®šä¸‹æ¥ï¼Œå¹¶ä¸”ä¿æŒç¨³å®šï¼Œè€Œä¸æ˜¯åƒä¹¦ä¸­/git ä¸­é‚£æ ·å¢åŠ ã€‚æˆ‘æ— æ³•å¼„æ¸…æ¥šæˆ‘å“ªé‡Œåšé”™äº†ã€‚ å¦‚æœæœ‰äººèƒ½çœ‹ä¸€ä¸‹å¹¶åˆ†äº«ä¸€äº›æŠ€å·§ï¼Œé‚£å°†éå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæœ‰äººæƒ³è¿è¡Œ/æµ‹è¯•å®ƒï¼Œä»£ç åº”è¯¥æŒ‰åŸæ ·å·¥ä½œã€‚  éå¸¸æ„Ÿè°¢ï¼ ``` è¯¥ç¨‹åºå®ç°äº† k-bandit é—®é¢˜çš„ n æ¬¡è¿è¡Œ import numpy as np import matplotlib.pyplot as plt bandit_reward_dist_mean = 0 bandit_reward_dist_sigma = 1 k_bandits = 10 bandit_sigma = 1 samples_per_bandit = 1000 epsilon = 0.01 def select_action(): r = np.random.randn() if r &lt; epsilonï¼šaction = np.random.randintï¼ˆ0ï¼Œk_banditsï¼‰elseï¼šaction = np.argmaxï¼ˆq_estimatesï¼‰ è¿”å›æ“ä½œ def update_action_countï¼ˆA_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢å·²é‡‡å–æ¯ä¸ªæ“ä½œçš„æ¬¡æ•°n_action [A_t] + = 1 def update_action_reward_totalï¼ˆA_tï¼ŒR_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªæ“ä½œçš„æ€»å¥–åŠ±action_rewards [A_t] + = R_t def generate_rewardï¼ˆmeanï¼Œsigmaï¼‰ï¼š# ä»æ­£æ€åˆ†å¸ƒä¸­ä¸ºè¿™ä¸ªç‰¹å®šçš„banditæå–å¥–åŠ±#r = np.random.normalï¼ˆmeanï¼Œsigmaï¼‰r = np.random.randnï¼ˆï¼‰+mean# ç±»ä¼¼äºåœ¨Git repoä¸­æ‰€åšçš„return r def update_qï¼ˆA_tï¼ŒR_tï¼‰ï¼š q_estimates[A_t] += 0.1 * (R_t - q_estimates[A_t]) n_steps = 1000 n_trials = 2000 #æ¯æ¬¡è¯•éªŒä½¿ç”¨ä¸€æ‰¹æ–°çš„è€è™æœºè¿è¡Œ n_steps æ‰€æœ‰è¯•éªŒä¸­æ¯ä¸€æ­¥çš„å¥–åŠ±çŸ©é˜µ - ä»é›¶å¼€å§‹ rewards_episodes_trials = np.zeros((n_trials, n_steps)) for j in range(0, n_trials): #q_true = np.random.normal(bandit_reward_dist_mean, bandit_reward_dist_sigma, k_bandits) q_true = np.random.randn(k_bandits) # å°è¯•å¤åˆ¶ book/git ç»“æœ # æ¯ä¸ªåŠ¨ä½œï¼ˆè€è™æœºï¼‰çš„ Q å€¼ - ä»random q_estimates = np.random.randn(k_bandits) # æ¯ä¸ªåŠ¨ä½œï¼ˆbanditï¼‰çš„æ€»å¥–åŠ± - ä»é›¶å¼€å§‹ action_rewards = np.zeros(k_bandits) # åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªåŠ¨ä½œå·²é‡‡å–çš„æ¬¡æ•° - ä»é›¶å¼€å§‹ n_action = np.zeros(k_bandits) # æ¯ä¸€æ­¥çš„å¥–åŠ± - ä» 0 å¼€å§‹ rewards_episodes = np.zeros(n_steps) for i in range(0, n_steps): A_t = select_action() R_t = generate_reward(q_true[A_t], bandit_sigma) rewards_episodes[i] = R_t  update_action_reward_total(A_t, R_t) update_action_count(A_t) update_q(A_t, R_t) rewards_episodes_trials[j,:] = rewards_episodes  æ‰€æœ‰è¿è¡Œä¸­æ¯æ­¥çš„å¹³å‡å¥–åŠ± average_reward_per_step = np.zeros(n_steps) for i in range(0, n_steps): average_reward_per_step[i] = np.mean(rewards_episodes_trials[:,i]) plt.plot(average_reward_per_step) plt.show() ```    æäº¤äºº    /u/datashri   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</guid>
      <pubDate>Mon, 03 Feb 2025 09:35:25 GMT</pubDate>
    </item>
    <item>
      <title>Vision RL å¸®åŠ©å’ŒæŒ‡å¯¼ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</link>
      <description><![CDATA[å‘èªæ˜çš„äººä»¬é—®å¥½ã€‚æˆ‘ä¸€ç›´åœ¨æ·±å…¥ç ”ç©¶ RLï¼Œæˆ‘è®¤ä¸ºé‚£ä¸ªç”·äººè·³å…¥æ¸¸æ³³æ± å´æ’åˆ°å†°å—çš„è§†é¢‘é€‚ç”¨äºæˆ‘ã€‚ https://jacomoolman.co.za/reinforcementlearning/ï¼ˆå‘ä¸‹æ»šåŠ¨æˆ–ç›´æ¥æœç´¢â€œvisionâ€ä»¥è·³è¿‡ä¸æˆ‘çš„é—®é¢˜æ— å…³çš„å†…å®¹ï¼‰ è¿™æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢çš„è¿›å±•ã€‚ä»»ä½•ä½¿ç”¨è¿‡è§†è§‰ RL çš„äººå¯èƒ½éƒ½èƒ½çœ‹å‡ºæˆ‘åšé”™äº†ä»€ä¹ˆï¼Ÿæˆ‘å·²ç»å°è¯•äº†å¤§çº¦ 2 ä¸ªæœˆï¼Œè¯•å›¾ä¸ºæ¨¡å‹æä¾›å›¾åƒè€Œä¸æ˜¯å˜é‡ï¼Œä½†æ²¡æœ‰æˆåŠŸã€‚    æäº¤äºº    /u/TheRealMrJm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 07:01:10 GMT</pubDate>
    </item>
    <item>
      <title>è¿™çœ‹èµ·æ¥åƒæ˜¯ç¨³å®šçš„ PPO æ”¶æ•›å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</link>
      <description><![CDATA[      è¿™çœ‹èµ·æ¥åƒç¨³å®šçš„ PPO æ”¶æ•›å—ï¼Ÿ    æäº¤äºº    /u/TopSigmaNoCap79970   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</guid>
      <pubDate>Mon, 03 Feb 2025 01:30:55 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ·±åº¦ç ”ç©¶ç®€ä»‹â€ï¼ŒOpenAIï¼ˆåŸºäº o3 çš„ç½‘é¡µæµè§ˆ/ç ”ç©¶ä»£ç†çš„ RL è®­ç»ƒï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</guid>
      <pubDate>Mon, 03 Feb 2025 01:29:39 GMT</pubDate>
    </item>
    <item>
      <title>2025 å¹´ç§‹å­£ç¡•å£«/åšå£«ç”³è¯·</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ éšç€æ‹›ç”Ÿå‘¨æœŸå…¨é¢å±•å¼€ï¼Œæˆ‘ç¥æ„¿æœ¬å‘¨æœŸç”³è¯·çš„æ‰€æœ‰äººå¥½è¿ï¼æˆ‘æ­£åœ¨ç”³è¯·ï¼Œè¿«ä¸åŠå¾…åœ°æƒ³å»ç ”ç©¶ç”Ÿé™¢åš RL ç ”ç©¶ï¼ˆåœ¨æˆ‘çš„å›½å®¶è¿™å¾ˆå°‘è§ï¼‰ã€‚ åœ¨è¯„è®ºä¸­å†™ä¸‹ä½ ç”³è¯·çš„åœ°æ–¹ä»¥åŠä½ æœ€æƒ³è¿›å…¥çš„åœ°æ–¹ã€‚ä¹Ÿè®¸å®‡å®™ä¼šå¬åˆ°ï¼Œæœºä¼šå°±ä¼šå¯¹ä½ æœ‰åˆ©ï¼    æäº¤äºº    /u/issyonibba   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</guid>
      <pubDate>Sun, 02 Feb 2025 19:47:59 GMT</pubDate>
    </item>
    </channel>
</rss>