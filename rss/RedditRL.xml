<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 22 Dec 2023 09:13:58 GMT</lastBuildDate>
    <item>
      <title>“强化学习迁移的基础：知识模态分类”，Wulfmeier 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18o333v/foundations_for_transfer_in_reinforcement/</guid>
      <pubDate>Fri, 22 Dec 2023 01:29:18 GMT</pubDate>
    </item>
    <item>
      <title>如何将 amass 数据集转换为 mujoco 格式？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18nrdhb/how_to_convert_the_amass_dataset_to_mujoco_format/</link>
      <description><![CDATA[嗨， 我想将 amass 数据集转换为 mujoco 格式，以便我能够在 mujoco 中使用运动数据知道如何做到这一点吗？ 我对 amass 和 mujoco 都很陌生，所以如果这似乎是一个愚蠢的问题，我深表歉意。 &lt;!-- SC_ON - -&gt;  由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18nrdhb/how_to_convert_the_amass_dataset_to_mujoco_format/</guid>
      <pubDate>Thu, 21 Dec 2023 16:48:28 GMT</pubDate>
    </item>
    <item>
      <title>使用稳定基线3收集部署时出错</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18nqifb/error_in_collecting_rollouts_using/</link>
      <description><![CDATA[        由   提交/u/Ecstatic-Rain-2460   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18nqifb/error_in_collecting_rollouts_using/</guid>
      <pubDate>Thu, 21 Dec 2023 16:09:55 GMT</pubDate>
    </item>
    <item>
      <title>“评估现实自主任务上的语言模型代理”，Kinniment 等人 2023 {ARC}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18np4kd/evaluating_languagemodel_agents_on_realistic/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18np4kd/evaluating_languagemodel_agents_on_realistic/</guid>
      <pubDate>Thu, 21 Dec 2023 15:07:37 GMT</pubDate>
    </item>
    <item>
      <title>“利用大型语言模型进行自主化学研究”，Boiko 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18np0m3/autonomous_chemical_research_with_large_language/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18np0m3/autonomous_chemical_research_with_large_language/</guid>
      <pubDate>Thu, 21 Dec 2023 15:02:45 GMT</pubDate>
    </item>
    <item>
      <title>你们如何处理强化学习中的梯度爆炸？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ninvh/how_you_guys_handle_gradient_exploding_in_rl/</link>
      <description><![CDATA[ 正确的权重初始化 梯度裁剪 lr调度程序我还能做什么？&lt; /li&gt;    由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ninvh/how_you_guys_handle_gradient_exploding_in_rl/</guid>
      <pubDate>Thu, 21 Dec 2023 08:57:35 GMT</pubDate>
    </item>
    <item>
      <title>利用离散表示进行持续强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ni6k8/harnessing_discrete_representations_for_continual/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.01203 OpenReview：https:// /openreview.net/forum?id=o4AydSd3Lp 摘要：  强化学习（RL）智能体什么都不用做决策但来自环境的观察结果在很大程度上依赖于这些观察结果的表征。尽管最近的一些突破使用了基于向量的观察分类表示（通常称为离散表示），但很少有工作明确评估这种选择的重要性。在这项工作中，我们对强化学习背景下将观察结果表示为分类值向量的优势进行了彻底的实证研究。我们对世界模型学习、无模型强化学习以及最终的连续强化学习问题进行评估，其中的好处最能满足问题设置的需求。我们发现，与传统的连续表示相比，通过离散表示学习的世界模型可以用更少的容量准确地模拟更多的世界，并且用离散表示训练的智能体用更少的数据学习更好的策略。在持续强化学习的背景下，这些好处转化为更快的适应代理。此外，我们的分析表明，观察到的性能改进可归因于潜在向量中包含的信息以及潜在的离散表示本身的编码。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ni6k8/harnessing_discrete_representations_for_continual/</guid>
      <pubDate>Thu, 21 Dec 2023 08:23:29 GMT</pubDate>
    </item>
    <item>
      <title>《人类衰老的递减状态空间理论》，Eppinger 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18n2u46/diminished_state_space_theory_of_human_aging/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18n2u46/diminished_state_space_theory_of_human_aging/</guid>
      <pubDate>Wed, 20 Dec 2023 19:22:47 GMT</pubDate>
    </item>
    <item>
      <title>“ReST 与 ReAct 的结合：多步推理 LLM 代理的自我改进”，Aksitov 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18n1woy/rest_meets_react_selfimprovement_for_multistep/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18n1woy/rest_meets_react_selfimprovement_for_multistep/</guid>
      <pubDate>Wed, 20 Dec 2023 18:44:17 GMT</pubDate>
    </item>
    <item>
      <title>在您想要的任何环境中轻松训练类似 AlphaZero 的智能体！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18myr1m/easily_train_alphazerolike_agents_on_any/</link>
      <description><![CDATA[大家好， 我为那些想要训练自己的 AlphaZero 的人创建了一个简单的起点！&lt; /p&gt; 您所需要的只是一个训练代理的环境，其他一切都已设置完毕。将其视为 AlphaZero 代理的 Huggingface 变形金刚。 我想添加更多环境，因此需要帮助。请随意克隆存储库并提交 PR！ 让我知道您的想法，链接如下：https ://github.com/s-casci/tinyzero   由   提交/u/ayan0k0ji  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18myr1m/easily_train_alphazerolike_agents_on_any/</guid>
      <pubDate>Wed, 20 Dec 2023 16:34:09 GMT</pubDate>
    </item>
    <item>
      <title>AMAGO：自适应代理的可扩展上下文强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mszov/amago_scalable_incontext_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.09971 OpenReview：https:// /openreview.net/forum?id=M6XWoEdmwf 代码：https://github.com/UT-Austin-RPL/amago 项目页面：https://ut-austin-rpl.github.io/amago/ 摘要：  我们介绍 AMAGO，这是一种上下文强化学习 (RL) 代理，它使用序列模型来应对泛化、长期记忆和元学习的挑战。最近的研究表明，离策略学习可以使具有循环策略的上下文强化学习变得可行。尽管如此，这些方法需要大量的调整，并通过在代理的内存容量、规划范围和模型大小方面产生关键瓶颈来限制可扩展性。 AMAGO 重新审视并重新设计了脱离策略的上下文方法，以便在整个部署过程中与端到端 RL 并行地成功训练长序列 Transformer。我们的代理具有独特的可扩展性，适用于广泛的问题。我们根据经验证明了它在元强化学习和长期记忆领域的强大性能。 AMAGO 对稀疏奖励和离策略数据的关注也允许上下文学习扩展到具有挑战性探索的目标条件问题。当与新颖的事后重新标记方案相结合时，AMAGO 可以解决以前困难的开放世界领域类别，其中代理在程序生成的环境中完成许多可能的指令。我们在三个目标条件域上评估我们的代理，并研究其单独的改进如何连接以创建通用策略。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mszov/amago_scalable_incontext_reinforcement_learning/</guid>
      <pubDate>Wed, 20 Dec 2023 12:00:31 GMT</pubDate>
    </item>
    <item>
      <title>使用屏幕作为观察</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mq29t/use_the_screen_as_observations/</link>
      <description><![CDATA[大家好，需要一些建议。 我正在健身房环境中尝试视觉强化学习（lunarlander-v2） ）。我只是使用我之前表现良好的ppo程序，然后在actor和critic之前添加一个两层CNN网络，CNN接收屏幕截图作为输入，然后输出一个3k维张量作为观察。 我训练了5千次。但不幸的是它的性能非常糟糕。我什至没有看到损失收敛的趋势。显然这并不像我想象的那么容易。 我可以想一些方法来提高性能，比如使用预先训练的图像编码器。但我不知道主要原因在哪里，还是我有更大的误会。由于每次训练都需要很长时间，所以我不想在没有方向的情况下进行实验。有没有适合我的指南或者论文，非常感谢。 ​ ​ 最后，我这样做的原因是一场讨论。我认为对原始图像进行特征提取会对视觉强化学习有所帮助，但有些人认为这没有用。   由   提交 /u/Ruine_fff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mq29t/use_the_screen_as_observations/</guid>
      <pubDate>Wed, 20 Dec 2023 08:47:21 GMT</pubDate>
    </item>
    <item>
      <title>今天，DQN arXiv 十周年了！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mmsud/dqn_arxiv_turns_a_decade_old_today/</link>
      <description><![CDATA[ 由   提交/u/DeepQZero  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mmsud/dqn_arxiv_turns_a_decade_old_today/</guid>
      <pubDate>Wed, 20 Dec 2023 05:20:00 GMT</pubDate>
    </item>
    <item>
      <title>PettingZoo with SB3：评估时如何加载 vec_normalized 文件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18mj3qc/pettingzoo_with_sb3_how_to_load_vec_normalized/</link>
      <description><![CDATA[您好， 我在 PettingZoo 的帮助下构建了一个自定义环境。我使用 SB3 中的 VecNormalize 函数来标准化观察和奖励。另外，我使用 SuperSuit (ss) 来包装 SB3 的环境。训练期间，一切都很顺利。相关代码如下： env = env_name(render_mode=render_mode, **env_kwargs) env = ss.pettingzoo_env_to_vec_env_v1(env) env = ss.concat_vec_envs_v1(env, n_envs, num_cpus=1, base_class=&#39;stable_baselines3&#39;) env = VecNormalize(env, gamma=gamma) 我保存了 vec_normalized 文件“vec_normalize” .pkl”以及训练后训练好的模型文件。  但是，我不知道如何正确加载“vec_normalize.pkl”当我想评估经过训练的模型时，请提交文件。 有人可以告诉我该怎么做吗？    由   提交 /u/Signal-Past-9572   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18mj3qc/pettingzoo_with_sb3_how_to_load_vec_normalized/</guid>
      <pubDate>Wed, 20 Dec 2023 02:07:24 GMT</pubDate>
    </item>
    <item>
      <title>“经过训练调用符号求解器的节俭 LM 可实现参数高效的算术推理”，Dutta 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18md284/frugal_lms_trained_to_invoke_symbolic_solvers/</link>
      <description><![CDATA[ 由   提交 /u/gwern   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18md284/frugal_lms_trained_to_invoke_symbolic_solvers/</guid>
      <pubDate>Tue, 19 Dec 2023 21:29:05 GMT</pubDate>
    </item>
    </channel>
</rss>