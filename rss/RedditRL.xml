<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 03 Jul 2024 03:17:09 GMT</lastBuildDate>
    <item>
      <title>强化学习解决装配线平衡问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1du4de0/reinforcement_learning_for_assembly_line/</link>
      <description><![CDATA[大家好！我想听听您对使用深度强化学习解决装配线平衡问题 (ALBP) 的看法。这是工业工程和运筹学中一个众所周知的优化问题，重点是如何有效地组织装配线上的任务，以最大限度地提高生产率，并最大限度地减少闲置时间或瓶颈。 以下是一些需要牢记的关键概念： 任务：这些是组装产品所需的单个操作或活动，每个操作或活动都有其特定的处理时间。 工作站：这些是装配线上执行任务的指定区域。 周期时间：这是指每个工作站完成其分配的任务所允许的最长时间，它决定了装配线的生产率。 优先约束：由于产品组装的性质，某些任务必须在其他任务之前完成。这些关系在优先图中表示。    提交人    /u/Icy_Bar_681   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1du4de0/reinforcement_learning_for_assembly_line/</guid>
      <pubDate>Wed, 03 Jul 2024 03:14:20 GMT</pubDate>
    </item>
    <item>
      <title>测试 dreamerv3 的最佳库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtxw7t/best_library_for_testing_out_dreamerv3/</link>
      <description><![CDATA[嘿，我想测试一下 dreamerv3，我尝试使用 rayrl，但我认为 tensorflow 的最新更新破坏了它，或者他们必须更新他们的东西。无论如何，什么是尝试 d​​reamerv3 并在其他环境中进行训练的最佳库？    提交人    /u/hinsonan   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtxw7t/best_library_for_testing_out_dreamerv3/</guid>
      <pubDate>Tue, 02 Jul 2024 21:57:00 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Geometric、强化学习和 OpenAI Gymnasium</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtmo5l/pytorch_geometric_reinforcement_learning_and/</link>
      <description><![CDATA[大家好。 正如标题所述，我正在尝试实现 openai gymnasium frostylake-v1 环境，以 pytorch 几何知识图谱表示，其中每个单元都是一个知识图谱节点，并且每条边都连接到玩家可以采取的可能路线。但是，我遇到了一个问题，即除非节点特征包含唯一值（无论是唯一节点索引还是它们在 4x4 地图中的位置），否则我的模型无法生成良好的结果。 我需要它独立于这些唯一索引，并且可能在一张地图上进行训练，然后将训练有素的代理放在一张新地图上，在那里他仍然能够对好动作和坏动作有一些概念（例如，掉进洞里总是不好的）。我该如何扩展这个问题？我做错了什么？如需更多信息，请在评论中留下，我一定会回答。 我正在写一篇论文，这个 openai gym 与我将在最终论文中进行训练的环境类似。所以我真的需要帮助解决这个特定问题。  编辑以获取进一步的深入信息： 我正在尝试将深度强化学习与图神经网络相结合以支持图环境。我使用 GNN 来估计 Dueling Double Deep Q-Network 架构中的 Q 值。我已经用 2 到 4 个 pytorch 几何 GNN（GCN、GAT 或 GPS）层替换了 MLP 层。 观察空间 为了测试这个架构，我使用了 frostylake-v1 环境的包装器，将观察空间转换为图形表示。每个节点都通过边连接到与其相邻的其他节点，代表一个就像正常人所看到的网格一样。 情况 1，具有位置编码： 每个节点具有 3 个特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 第二和第三个特征表示单元格的位置编码（单元格 x/y 坐标）： 第二个特征表示单元格列。 第三个特征表示单元格行。   情况 2，没有位置编码，使用单元格类型作为特征：  如果字符位于该单元格中，则第一个特征为 1，否则为 0。 单元格的类型。如果它是一个正常单元，则为 0；如果它是一个洞，则为 -1；如果它是目标，则为 1。  动作空间 动作空间与 openai gym freezelake 文档中的完全相同。代理对 frostinglake-1 环境有 4 种可能的操作（0=左、1=下、2=右、3=上）。 奖励空间 奖励空间与 openai gym frostinglake 文档中的完全相同。 问题 我已成功实现了具有所有默认单元的默认 4x4 网格环境的策略收敛。在我的实验中，代理只能在案例 1 中描述的观察空间中实现这种收敛。  我试图理解为什么需要位置编码才能实现收敛？ 在实施观察空间案例 2 时，即使在长时间训练的探索过程中多次获得最终奖励，代理也永远不会收敛。 由于与 transformer 相同的原因，GNN 是否也需要位置嵌入？ 如果我在小型网格环境中使用足够的消息传递 2 到 4 层，每个节点都应该具有来自图中每个其他节点的信息，那么网络是否应该能够在这种情况下隐式学习位置嵌入？ 我也尝试过使用其他位置嵌入 (PE) 方法，例如随机游走（5-40 次游走）和拉普拉斯向量（2-6 K 值），但我无法使用此 PE 实现收敛方法。 奇怪的是，我也尝试过使用随机化的唯一节点索引作为特征，而不是位置编码，并且代理能够收敛。我不明白为什么代理在这些条件下能够收敛，但在 PE 情况和观察空间情况 2 中却不能收敛。     提交人    /u/SmkWed   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtmo5l/pytorch_geometric_reinforcement_learning_and/</guid>
      <pubDate>Tue, 02 Jul 2024 14:05:23 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习教 AI 玩 BurgerTime - Python</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtlltk/teaching_ai_to_play_burgertime_with_reinforced/</link>
      <description><![CDATA[这是我在这个 subreddit 中的第一篇帖子，如果有更合适的帖子或需要更多信息，请告诉我。 Chilis 餐厅正在他们的网站上做促销，他们有一个浏览器游戏，基本上是 BurgerTime 的克隆，非常有趣 - chilisburgertime dot com 。我玩了一段时间，发现它非常可预测，有些级别甚至可能确定性地容易，我认为尝试制作一个 AI 来玩这个游戏会很酷。 我从事 IT 工作，我的编码有点生疏，但我一直在使用 ChatGPT 指导我用 Python 编写脚本。到目前为止，我已经能够编写一个脚本来监控分数、生命计数器，并在生命变为 0 时执行所有点击以开始新游戏和新试验。 然而，我的人工智能的训练/改进非常乏味。控制角色只能向左、向右、向上、向下移动和射击胡椒，每个屏幕上有 3 个敌人 + 一个老板，有 6 个级别，每个级别都有不同的老板。我很高兴它能通过一个级别。我试图强化它以最大化得分并最大化死亡前的生存，但它仍然像 5 岁的孩子一样玩。 我应该研究什么？我甚至不知道从哪里开始尝试识别屏幕上的所有精灵和游戏机制。如果我没有实际的游戏代码并且它在浏览器中播放，OpenAI Gym 会很好吗？如果没有数十小时的投入和大量的 AI 编码技能，这个游戏是否太复杂了？我怎样才能让它监控我的游戏玩法并从中获取有意义的信息？感谢您的时间！！    提交人    /u/Benhoffer87   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtlltk/teaching_ai_to_play_burgertime_with_reinforced/</guid>
      <pubDate>Tue, 02 Jul 2024 13:16:46 GMT</pubDate>
    </item>
    <item>
      <title>Minigrid Babyai 帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dtfk71/minigrid_babyai_help/</link>
      <description><![CDATA[嗨， 我希望在 babyai 环境中有一个特定的结构，训练的每一集都以此结构开始。我真正想要的是为网格中的某些对象赋予特定位置。我如何在 minigrid 中实现这一点？    提交人    /u/cosmic_2000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dtfk71/minigrid_babyai_help/</guid>
      <pubDate>Tue, 02 Jul 2024 06:59:33 GMT</pubDate>
    </item>
    <item>
      <title>“使用稀疏自动编码器解释偏好模型”，Riggs & Brinkmann</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dt9e2r/interpreting_preference_models_wsparse/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dt9e2r/interpreting_preference_models_wsparse/</guid>
      <pubDate>Tue, 02 Jul 2024 01:06:36 GMT</pubDate>
    </item>
    <item>
      <title>40k 个 episode 之后的奖励图下降</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dt0tg7/downside_of_reward_graph_after_40k_episodes/</link>
      <description><![CDATA[代码  train.py -&gt; https://www.pythonmorsels.com/p/33x5u/    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dt0tg7/downside_of_reward_graph_after_40k_episodes/</guid>
      <pubDate>Mon, 01 Jul 2024 18:52:21 GMT</pubDate>
    </item>
    <item>
      <title>研究顾问绝望</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dswpav/research_advisor_despair/</link>
      <description><![CDATA[嗨，我写这篇文章是为了寻求建议。我是一名本科生，梦想有一天能去研究生院学习强化学习。我已经和一位教授一起完成了一个使用应用机器学习的研究项目；从那时起，我一直想进入一个从事理论工作的实验室。问题是，即使发了几十封定制的冷邮件，我甚至还没有收到任何回复。 我知道这个领域非常受欢迎，竞争非常激烈，但我不知道该怎么做了。我知道对于北美顶尖的研究生课程来说，发表论文是必不可少的，但我不确定我是否有机会。在这一点上，我想我已经放弃了在没有导师的情况下发表论文的想法。这是一个好的做法吗？我应该继续尝试联系教授吗？任何见解都将不胜感激。    提交人    /u/Open-Ad2530   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dswpav/research_advisor_despair/</guid>
      <pubDate>Mon, 01 Jul 2024 16:04:22 GMT</pubDate>
    </item>
    <item>
      <title>是否有一个带有 rollout-buffer 的在线策略算法（A2C/PPO）的有效实现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsvsux/is_there_a_working_implementation_of_an_onpolicy/</link>
      <description><![CDATA[我有一个可变长度情节的环境，其中当前状态取决于先前的操作。此环境无法可靠地模拟，并且只能向前迈出一步。训练数据是静态的，每集都有要实现的目标。此环境可以计算奖励，在情节结束时从 rollout-buffer 执行 env.step()，然后执行 model.train()，或者可能在每个步骤之后进行训练。  我尝试使用 SB3 实现这一点，但似乎从未使用 rollout-buffer 测试过此代码，并且没有示例。有人知道在这样的环境中可以有效实现像 A2C 这样的在线策略算法吗？    提交人    /u/principle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsvsux/is_there_a_working_implementation_of_an_onpolicy/</guid>
      <pubDate>Mon, 01 Jul 2024 15:27:59 GMT</pubDate>
    </item>
    <item>
      <title>从我的第一个大项目中学到的一些经验教训</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsswl0/some_lessons_from_getting_my_first_big_project/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsswl0/some_lessons_from_getting_my_first_big_project/</guid>
      <pubDate>Mon, 01 Jul 2024 13:25:05 GMT</pubDate>
    </item>
    <item>
      <title>我使用 DPO 进行 LLM 对齐，但只获得了微小的改进。DPO 对某些任务不起作用，这是正常的吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsojuc/im_using_dpo_for_llm_alignment_and_only_get/</link>
      <description><![CDATA[你好！事实上，我没有强化学习方面的经验。我确实使用 OpenAI 教程学习了策略梯度算法和 PPO，并阅读了大量与 DPO 相关的论文。但是，当我在自己的多模态 LLM 上尝试 DPO 时，所需指标的改进很小（从 9 到 9.5）。我检查了我的实现，似乎损失是正确的。 我也尝试了偏好数据：我使用 ① 由参考模型生成的获胜-失败对，其中温度=1.0， ② 由参考模型和一些基本事实生成的获胜-失败对（我的任务有基本事实句子），以及 ③ 由参考模型生成的获胜-失败对，其中温度=1.0 和 2.0 以及基本事实。结果表明，① 产生了最大的改进（与提到数据分布偏移问题的原始论文一致）。 但是，在修复了我能想到的所有可能问题之后，我仍然无法使算法工作。因此，我迫切需要任何建议/经验分享。你认为这是 DPO 的容量限制，还是我的实现中存在一些未被发现的错误？    提交人    /u/Worth-Note9721   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsojuc/im_using_dpo_for_llm_alignment_and_only_get/</guid>
      <pubDate>Mon, 01 Jul 2024 09:12:05 GMT</pubDate>
    </item>
    <item>
      <title>只是想分享我的快乐，我的第一个主要 RL 项目不再变得糟糕。感谢我从这里得到的帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsnlqj/just_wanted_to_share_my_happiness_my_first_major/</link>
      <description><![CDATA[       由    /u/Breck_Emert  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsnlqj/just_wanted_to_share_my_happiness_my_first_major/</guid>
      <pubDate>Mon, 01 Jul 2024 08:05:41 GMT</pubDate>
    </item>
    <item>
      <title>RL 的用户，您希望 RL 做什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsiiao/users_of_rl_what_do_you_wish_rl_could_do/</link>
      <description><![CDATA[对于那些致力于 RL 应用的人来说，您的最终目标是什么？您希望 RL 能做什么？什么用例推动了您的工作？    提交人    /u/Obsesdian   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsiiao/users_of_rl_what_do_you_wish_rl_could_do/</guid>
      <pubDate>Mon, 01 Jul 2024 02:47:47 GMT</pubDate>
    </item>
    <item>
      <title>制作了一个软演员评论家模型。但结果很糟糕，我哪里犯了错误？（pytorch，月球着陆器）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsi26i/made_a_soft_actor_critic_model_but_bad_result/</link>
      <description><![CDATA[   https://preview.redd.it/c3231tj2it9d1.png?width=855&amp;format=png&amp;auto=webp&amp;s=f47a4fd623a4fd884e55f539c863c954a0706d8a 这是我的代码https://github.com/lch3942/test “训练开始了，但似乎学习过程没有正常工作。我遵循了这个伪代码，但我不确定它是否被正确实现，结果并不好。你能帮我找出我哪里出错了吗？谢谢你阅读我的帖子。     提交人    /u/Delicious_Bowl1645   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsi26i/made_a_soft_actor_critic_model_but_bad_result/</guid>
      <pubDate>Mon, 01 Jul 2024 02:23:30 GMT</pubDate>
    </item>
    <item>
      <title>列出你遇到的最佳 LLM 驱动的多智能体强化学习论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dsf5y9/name_best_llmpowered_multi_agent_rl_papers_you/</link>
      <description><![CDATA[大家好！顾名思义，我正在对多智能体强化学习中 LLM 的最佳用途进行广泛的文献综述，特别是如果涉及学习/训练。在上下文中，学习也很重要。 如果有任何想法，请说出它们的名字，谢谢！    提交人    /u/miladink   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dsf5y9/name_best_llmpowered_multi_agent_rl_papers_you/</guid>
      <pubDate>Sun, 30 Jun 2024 23:52:10 GMT</pubDate>
    </item>
    </channel>
</rss>