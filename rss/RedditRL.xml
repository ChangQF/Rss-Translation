<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 30 Jan 2025 15:17:00 GMT</lastBuildDate>
    <item>
      <title>极快的优先采样</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idl178/blazingly_fast_prioritized_sampling/</link>
      <description><![CDATA[  由    /u/JacksOngoingPresence  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idl178/blazingly_fast_prioritized_sampling/</guid>
      <pubDate>Thu, 30 Jan 2025 11:36:35 GMT</pubDate>
    </item>
    <item>
      <title>有关测量误差的论文？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idkp6u/papers_on_measurement_error/</link>
      <description><![CDATA[你们知道关于测量误差和模型在决策过程中导致不准确估计的任何书面文章吗？    提交人    /u/mowmail   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idkp6u/papers_on_measurement_error/</guid>
      <pubDate>Thu, 30 Jan 2025 11:13:44 GMT</pubDate>
    </item>
    <item>
      <title>对多智能体环境的质疑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idk7cn/doubt_regarding_multi_agent_environments/</link>
      <description><![CDATA[大家好！我有使用 DRL 的经验，但环境只有一个代理。现在我正在研究 GitHub 上 CybORG repo 的多代理“Scenario1b”，并尝试使用 Stable-Baselines3 训练一些代理。我已经用 PettingZoo 制作了一个包装器，我有几个疑问： 1- 在那种环境中，通常有一个很大的动作空间，但实际上可以执行的动作较少（即可以执行的动作已被过滤并从那些不能执行的动作中筛选出来）。这通常被称为“动作掩蔽”。我的疑问是，这是否可以包含在步骤方法本身中，还是必须像本例 (https://pettingzoo.farama.org/tutorials/sb3/connect_four/) 一样单独实现？ 2- 据说 SB3 不支持“dict”动作空间，但是在这个使用 SB3 的多代理环境的示例 (https://pettingzoo.farama.org/tutorials/sb3/kaz/) 中，它确实有一个 Dict 动作空间。这怎么理解呢？ 提前谢谢！！    提交人    /u/Carpoforo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idk7cn/doubt_regarding_multi_agent_environments/</guid>
      <pubDate>Thu, 30 Jan 2025 10:38:13 GMT</pubDate>
    </item>
    <item>
      <title>特斯拉会使用强化学习吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1idjngw/is_tesla_going_to_use_reinforcement_learning/</link>
      <description><![CDATA[强化学习在现实世界中还没有真正应用，特斯拉有可能使用强化学习吗？    提交人    /u/thamizhan1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1idjngw/is_tesla_going_to_use_reinforcement_learning/</guid>
      <pubDate>Thu, 30 Jan 2025 09:56:55 GMT</pubDate>
    </item>
    <item>
      <title>与我们都在做的 RL 相比，为什么 LLM 上的 RL 微调如此容易和稳定？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</link>
      <description><![CDATA[我一直在观察不同的人尝试重现 Deepseek 训练配方，与我习惯的 RL 相比，我对这种配方的稳定性感到震惊。 经过大约 50 个训练步骤后，他们在数学问题上的准确率可靠地达到了 50%。他们尝试了几种不同的 RL 算法，并报告说它们的效果大致相同，而无需任何超参数调整。 如果我能在仅 50 个训练步骤中将平衡手推车的成功率提高 50%，我会认为自己很幸运。而且我可能必须为每个任务调整超参数。  （我的理论：由于无监督的预训练，这很容易。该模型已经学习了良好的表示和背景知识 - 即使它无法在 RL 之前完成任务 - 这使得问题变得容易得多。也许我们应该在 RL 中做更多这样的事。）    提交人    /u/currentscurrents   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id2cgv/why_is_rl_finetuning_on_llms_so_easy_and_stable/</guid>
      <pubDate>Wed, 29 Jan 2025 19:31:43 GMT</pubDate>
    </item>
    <item>
      <title>关于连续 Cartpole 的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1id1pu3/question_on_continuous_cartpole/</link>
      <description><![CDATA[我修改了 cartpole 环境，让动作空间连续，自然训练时间就长了很多。我用的算法是 A2C，每集更新一次。不知道有没有人用 DDPG 或其他处理连续动作空间的算法建过类似的模型，能加速训练吗？现在解决 cartpole 大概需要 20k 集。    submitted by    /u/Key-Entrance8005   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1id1pu3/question_on_continuous_cartpole/</guid>
      <pubDate>Wed, 29 Jan 2025 19:06:29 GMT</pubDate>
    </item>
    <item>
      <title>关于离线强化学习的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icy4si/question_on_offline_rl/</link>
      <description><![CDATA[嗨，我对 RL 还比较陌生，我有一个问题，在离线 RL 中，关键点是我们在各处学习最佳策略。我的问题是，我们是否也在各处学习最佳价值函数和最佳 q 函数？ 具体来说，我想知道如何最好地从离线数据集中仅学习价值函数（不​​一定是策略），并且我想使用离线 RL 工具在各处学习最佳价值函数，但我对要研究什么以了解更多信息感到困惑。我想这样做来学习 V 作为状态的安全指标。 我希望我说得有道理。    提交人    /u/Limp-Ticket7808   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icy4si/question_on_offline_rl/</guid>
      <pubDate>Wed, 29 Jan 2025 16:42:46 GMT</pubDate>
    </item>
    <item>
      <title>我尝试构建一个 alphazero 来掌握井字游戏，但它找不到最佳动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icw7bw/i_tried_to_build_a_alphazero_to_master_tictactoe/</link>
      <description><![CDATA[github: https://github.com/asdeq20062/tictactoe_alphazero.git 这是我的井字游戏 alphazero，但经过这么多次训练，AI 总是在第一个回合移动到中心。最佳移动应该是角落。 有人能帮我检查哪里出了问题吗？谢谢。 main.py -&gt; 此文件是训练的起点     提交人    /u/Upstairs-Lead-2601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icw7bw/i_tried_to_build_a_alphazero_to_master_tictactoe/</guid>
      <pubDate>Wed, 29 Jan 2025 15:22:30 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能随着情节的增加而下降——动作重复和不稳定的奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icsv3j/dqn_performance_drops_with_more_episodes_action/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icsv3j/dqn_performance_drops_with_more_episodes_action/</guid>
      <pubDate>Wed, 29 Jan 2025 12:41:06 GMT</pubDate>
    </item>
    <item>
      <title>谁在奖励 DeepSeek R1？在 RL 中，您需要一些奖励功能或手动奖励，不是吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</link>
      <description><![CDATA[他们说他们没有在大数据上进行自监督学习，那么奖励模型必须以某种方式在某些数据上进行训练，ChatGPT API 或 LLAMA 可以用作奖励工具，或者谁知道在没有奖励基线的情况下思想链是如何工作的？ PS：据我了解，他们根据版本进度使用 LLAMA 作为基线 LLM 模型，并且它是公开可用的    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icrgt5/who_is_rewarding_deepseek_r1_in_rl_you_need_some/</guid>
      <pubDate>Wed, 29 Jan 2025 11:13:03 GMT</pubDate>
    </item>
    <item>
      <title>频繁奖励环境中 Q 值的收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icpjwh/problems_with_convergence_of_qvalues_in/</link>
      <description><![CDATA[大家好 :) 我有一个组合优化问题：这是一个设施位置问题，我需要选择一个可能的位置子集，在每个时间步骤中满足客户的需求。我将其建模为一个图形，其中边缘表示客户和设施之间的距离。在我的例子中，设施是服务器，距离是延迟，目标是在客户在白天移动时最小化延迟。 因此，奖励是（负）延迟。但是，此奖励在每个时间步骤都会立即给出。我已经使用 DQN 进行了离线策略训练，但 q 值似乎无限增长。所以，我根本看不到收敛行为。 我最初的猜测是奖励的频率可能是一个问题，因为每个 Q 值都需要表示所有未来奖励的折扣总和，而奖励频率较低时更容易实现（也许这不是真的，我只是有过这样的经历）。  所以我的问题是，您是否有过类似的经历，即决策总是会立即获得奖励反馈，而对于达到某种“获胜”状态或类似情况则没有“长期”奖励？ 在这种情况下，您的方法是什么？ 祝一切顺利，提前谢谢您 :)    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icpjwh/problems_with_convergence_of_qvalues_in/</guid>
      <pubDate>Wed, 29 Jan 2025 08:50:22 GMT</pubDate>
    </item>
    <item>
      <title>“Robopair：越狱 LLM 控制的机器人”，Robey 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1icbl8v/robopair_jailbreaking_llmcontrolled_robots_robey/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1icbl8v/robopair_jailbreaking_llmcontrolled_robots_robey/</guid>
      <pubDate>Tue, 28 Jan 2025 20:35:41 GMT</pubDate>
    </item>
    <item>
      <title>昨天读了这篇文章，我想看看社区对这个 Google Deepmind MONA 的看法。你们觉得怎么样？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibzlpp/read_this_yesterday_and_i_wanted_to_see_what_the/</link>
      <description><![CDATA[        提交者    /u/GreyBamboo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibzlpp/read_this_yesterday_and_i_wanted_to_see_what_the/</guid>
      <pubDate>Tue, 28 Jan 2025 11:36:36 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习和无模型的强化学习有什么区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibyio9/whats_the_difference_between_modelbased_and/</link>
      <description><![CDATA[我试图理解基于模型和无模型强化学习之间的区别。据我所知：  无模型方法直接从真实经验中学习。它们观察当前状态，采取行动，然后以下一个状态和奖励的形式接收反馈。这些模型没有任何内部表示或对环境的理解；它们只是依靠反复试验来随着时间的推移改进其行动。 基于模型的方法则通过创建“模型”或环境模拟来学习。它们不仅仅是对状态和奖励做出反应，还试图模拟未来会发生什么。这些模型可以使用监督学习或学习函数（如 s′=F(s,a)s&#39; = F(s, a)s′=F(s,a) 和 R(s)R(s)R(s)）来预测未来状态和奖励。他们本质上建立了一个环境模型，并用它来规划行动。  因此，关键的区别在于基于模型的方法使用其学习到的模型来近似未来并提前规划，而无模型方法仅通过直接与环境交互来学习，而不尝试模拟它。 这样说对吗，还是我遗漏了什么？    提交人    /u/volvol7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibyio9/whats_the_difference_between_modelbased_and/</guid>
      <pubDate>Tue, 28 Jan 2025 10:21:25 GMT</pubDate>
    </item>
    <item>
      <title>自适应/在线 LQR 的研究合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibw7nq/research_collaboration_in_adaptativeonline_lqr/</link>
      <description><![CDATA[作为我博士研究的一部分，我已经从深度强化学习过渡到探索在线 LQR。具体来说，我一直在深入研究这篇论文中提出的想法。 我已经开发了一些我认为可能非常高效的算法思想。但是，我的背景主要是实践，我缺乏对这些方法进行严格理论分析的理论基础。 如果有人对这个主题感兴趣并希望在理论方面进行合作，我很乐意联系。:)    提交人    /u/riiswa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibw7nq/research_collaboration_in_adaptativeonline_lqr/</guid>
      <pubDate>Tue, 28 Jan 2025 07:21:03 GMT</pubDate>
    </item>
    </channel>
</rss>