<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 26 May 2024 12:24:31 GMT</lastBuildDate>
    <item>
      <title>学士论文游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</link>
      <description><![CDATA[嘿，我想为我的计算机科学学士学位论文训练一个人工智能来玩强化学习游戏。 我还没有强化学习的经验。 我可以选择哪些当时可行的游戏？   由   提交/u/TMG_Indi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</guid>
      <pubDate>Sun, 26 May 2024 08:40:15 GMT</pubDate>
    </item>
    <item>
      <title>环境复杂性与最优策略收敛的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</link>
      <description><![CDATA[大家好，是否有一些关于环境复杂性与学习到的最优策略本身之间关系的文献？例如，如果一个环境是由“世界模型”中的VAE生成的，那么环境复杂度和策略之间的关系是什么？   由   提交/u/Main_Pressure271   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</guid>
      <pubDate>Sun, 26 May 2024 06:58:00 GMT</pubDate>
    </item>
    <item>
      <title>经常性 SAC 指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</link>
      <description><![CDATA[我一直在尝试了解有关 LSTM 在 POMDP 强化学习中如何发挥作用的更多信息。我专门尝试与 SAC 合作，想知道是否有关于该主题的一些好的资源。    由   提交 /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</guid>
      <pubDate>Sun, 26 May 2024 06:43:50 GMT</pubDate>
    </item>
    <item>
      <title>最优随机策略是否存在？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</link>
      <description><![CDATA[我知道在 MDP 中总是存在唯一的最优确定性策略。对于最优随机策略也存在这样的说法吗？是否总是存在唯一的最优随机策略？它能比最优确定性策略更好吗？我想我不太明白。 谢谢！   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</guid>
      <pubDate>Sun, 26 May 2024 06:07:26 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的矩阵（强化学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</link>
      <description><![CDATA[如果我希望代理显示 4 个空间，其中每个空间有 10 个组件，每个组件有 3 个变量。为观察空间定义一个矩阵是不是更好？因为这告诉我要做 ChatGPT    由   提交/u/Gullible_Capital_146   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</guid>
      <pubDate>Sun, 26 May 2024 04:02:53 GMT</pubDate>
    </item>
    <item>
      <title>“静息大脑标签记忆中的电‘涟漪’用于存储”：体验重播机制和选择睡眠期间优先重播的点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</guid>
      <pubDate>Sat, 25 May 2024 21:48:03 GMT</pubDate>
    </item>
    <item>
      <title>部分循环观察空间的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</link>
      <description><![CDATA[假设我有一个环境，其中代理必须在 2D 平面上移动来收集硬币，硬币的数量各不相同。代理可以在四个基本方向中的任何一个方向上加速，并在收集硬币时获得 1 的奖励，然后将其从环境中移除。观察空间如下所示： Agent x Agent y Agent vx Agent vy [Coin x, Coin y] * 硬币数量 我的假设是处理这个问题的方法是使用变压器 - 使用前馈网络对固定组件的表示进行编码，然后是每个非固定组件之一（我还可以使用变压器来生成每种类型的固定长度编码对象并将其与固定组件连接起来，但直观地将所有内容放入同一个变压器中应该效果更好，因为上下文使变压器变得有用）。本质上，这意味着编写一个自定义状态空间编码器，我认为两个大库（Stable Baselines 和 Rllib）都支持。伪代码如下所示： defencode(obs): generic, coin = obs # a 1 x 4 np array and a k x 2 np array coin_emb = self.coin_embed(c) # a前馈层映射 2 到嵌入维度 gen_emb = self.general_embed(general) # 4 --&gt; emb_dim编码= self.encoder(torch.stack(gen_emb,coins_emb))#torch.nn.TransformerEncoder，接受Nxemb_dim输入，产生Nxh_dim输出encoded=encoded[0]#BERT将输出用于特殊的[CLS] ] 标记作为其固定长度输出。在这里，我们使用输出作为开始标记。 return generated def policy(obs): return self.policy_net(encode(obs)) def value(obs): return self.value_net(encode(obs))  我记得OpenAI的隐藏并寻求环境使用池化（特征编码器之后的 IIRC 最大池化）而不是变压器，但那是不久前的事了。无论如何，我的问题的核心是是否有人对实现这种自定义观察网络时使用的最佳堆栈有建议。如果有人见过这样的项目（越新越好 - 语法似乎像季节一样变化），我也非常感谢 github 链接。 谢谢！   由   提交 /u/Dry-Sock7131    reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</guid>
      <pubDate>Sat, 25 May 2024 16:41:35 GMT</pubDate>
    </item>
    <item>
      <title>教人形机器人用头部弹球的教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</link>
      <description><![CDATA[您好，刚刚在 github 上发布了一个新教程 - https ://github.com/goncalog/ai-robotics。您的反馈会很棒！   由   提交/u/goncalogordo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</guid>
      <pubDate>Sat, 25 May 2024 16:36:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自定义环境引擎</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</link>
      <description><![CDATA[我目前正在尝试使用 Isaac sim 构建用于强化学习的自定义环境。我已经构建了模型，但我不知道如何将其导入 VS Code 以便我真正对环境进行编程，而且我找不到任何相关教程。我也想知道我是否应该使用 mojuco 代替？但我真的不知道如何用它创建模型。   由   提交/u/Teaser_404  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</guid>
      <pubDate>Sat, 25 May 2024 11:39:40 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习结果不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</guid>
      <pubDate>Sat, 25 May 2024 11:32:13 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND（扩散作为环境梦想的模型）是在扩散世界模型中训练的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</link>
      <description><![CDATA[   /u/clumma  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</guid>
      <pubDate>Fri, 24 May 2024 23:02:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 Airsim 实现稳定的 Baselines3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czcjab/stable_baselines3_with_airsim/</link>
      <description><![CDATA[您好！我正在尝试在 AirSim 中制作一架无人机，以便在从一个点移动到另一个点时避开物体。我修改了该示例以满足我的需要。我使用来自 Stable Baselines3 和 Gymnasium 的 PPO 对其进行训练。问题是，一集的时间太长了。因此，我无法像在其他环境中那样提供足够的时间步长。我是 RL 的新手。请帮我解决这个问题。   由   提交 /u/Sandy_The_Adventurer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czcjab/stable_baselines3_with_airsim/</guid>
      <pubDate>Fri, 24 May 2024 04:34:01 GMT</pubDate>
    </item>
    <item>
      <title>“Vernor Vinge 小说《真实姓名》的后记”，Minsky 1984（偏好学习和安全代理的挑战）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywwz3/afterword_to_vernor_vinges_novel_true_names/</guid>
      <pubDate>Thu, 23 May 2024 16:23:56 GMT</pubDate>
    </item>
    <item>
      <title>MDP：为失去迄今为止在游戏中积累的东西设置奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cywuda/mdp_set_reward_for_losing_what_you_have/</link>
      <description><![CDATA[假设我有一个游戏，开始时奖励为 0。我可以采取以下两种行动之一：  玩：一个随机过程，我要么获得金钱，要么失去迄今为止的奖励  离开：退出游戏并保留我累积的奖励。   我正在尝试将此游戏建模为具有“开始”状态和“结束”状态的 MDP，其中结束状态的值为 0。开始”，您可以执行“播放”或“离开”操作。在“结束”时，游戏结束——无需采取任何行动。  我试图弄清楚两件事：  如何为我失去累积奖励的随机过程的结果设置奖励。  如何为随机过程的结果设置奖励。 p&gt; 如何设置戒烟奖励。   对于 1： 如果“start”的值为 V(in)，我认为对于我选择“play”的结果，失去我累积的奖励，我可以用“-V(in)”来代表奖励。  Q(in, play) 的方程如下所示：  sum(p(lose)*(-V(in) + V(end)) + p(win )*(R + V(in)));  对于 2： 对于 Q(in, quit) --&gt;最终，奖励为V(in)。这样，如果您退出，Q(in, quit) 始终等于您开始时的值。  我的结果似乎与我的目标不符，而且我似乎无法找出我的方法的问题。我正在考虑我的方法（1）可能不准确，因为V（in）并不代表我累积的奖励，而是代表我累积奖励的预期值。但我的头脑中并没有很好地具体化这一点。  我在 MDP 中设置奖励的方式是否存在任何明显的问题？或者甚至是我的整个 MDP 中的状态/操作？    由   提交 /u/Squamply   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cywuda/mdp_set_reward_for_losing_what_you_have/</guid>
      <pubDate>Thu, 23 May 2024 16:21:02 GMT</pubDate>
    </item>
    <item>
      <title>Cartpole 返回奇怪的东西。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</link>
      <description><![CDATA[我正在从头开始制作一个 PPO 代理（没有 Torch，没有 TF），它进展顺利，直到 env 突然返回一个维度为 5 的二维列表， 4而不是4，经过一番调试后，我发现这可能不是我的错，因为我没有对回报进行分配或执行任何操作，它只是在随机时间范围内发生并破坏了我的整个事情。有人知道为什么吗？   由   提交 /u/Mehcoder1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cyr8ku/cartpole_returns_weird_stuff/</guid>
      <pubDate>Thu, 23 May 2024 12:10:48 GMT</pubDate>
    </item>
    </channel>
</rss>