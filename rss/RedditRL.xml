<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 09 Nov 2024 06:21:05 GMT</lastBuildDate>
    <item>
      <title>关于扩展 DRL 中的观察和动作空间的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn318r/advice_on_scaling_observation_and_action_spaces/</link>
      <description><![CDATA[大家好！我正在开展一个项目，训练深度强化学习 (DRL) 代理在不同的电网网络架构（例如 13 母线和 34 母线系统）中运行。我的目标是在较小的系统（如 13 母线）上训练代理，然后在较大的系统（如 34 母线）上测试它。但是，随着网络规模的扩大，观察和操作空间也会发生变化：例如，我对 13 母线系统的观察空间是 (17, 3)，但对于 34 母线系统，它变成了 (47, 3)。这种维度变化带来了挑战，因为我的当前模型（使用 Stable Baselines3 构建）捕获了观察空间，因此很难在不同规模上进行推广。 我的导师建议探索节点级图形网络来帮助解决这个扩展问题。我很好奇是否有人有以下方面的经验或建议：  在可变大小环境中扩展 DRL 的观察和操作空间的方法。 关于在强化学习中使用节点级图形网络实现可扩展性的相关论文或资源。 调整稳定基线 3（或替代库）以处理可变观察和操作空间的方法。  任何关于在不同规模环境中训练和测试 DRL 代理的见解都将非常有帮助。提前感谢您分享的任何建议或资源！    提交人    /u/wild_wolf19   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn318r/advice_on_scaling_observation_and_action_spaces/</guid>
      <pubDate>Sat, 09 Nov 2024 05:26:29 GMT</pubDate>
    </item>
    <item>
      <title>帮助运行 Minigrid</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gn1air/help_with_running_minigrid/</link>
      <description><![CDATA[大家好， 我正在尝试运行一些分层 RL 算法，并且一直在研究各种四室版本的健身房环境，偶然发现了 https://minigrid.farama.org/environments/minigrid/FourRoomsEnv/ 但出于某种原因，健身房 1.0 似乎没有这些环境，有人成功逃离了这些微网格环境（使用健身房 1.0？）。  抱歉，如果这是重复的帖子，我确实尝试搜索它但没有成功，谢谢。    提交人    /u/Nervous_Studio_7689   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gn1air/help_with_running_minigrid/</guid>
      <pubDate>Sat, 09 Nov 2024 03:44:13 GMT</pubDate>
    </item>
    <item>
      <title>计算机视觉问题的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gmfx3u/reinforcement_learning_on_computer_vision_problems/</link>
      <description><![CDATA[大家好， 我是一名计算机视觉研究员，主要从事 3D 视觉任务。最近，我开始研究 RL，意识到许多视觉问题可以重新表述为某种策略或价值学习结构。进行和遵循这种重新表述是否有好处？是否有任何重要的工作取得了比监督学习更好的结果？    提交人    /u/Foreign-Associate-68   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gmfx3u/reinforcement_learning_on_computer_vision_problems/</guid>
      <pubDate>Fri, 08 Nov 2024 10:57:34 GMT</pubDate>
    </item>
    <item>
      <title>[P] 我创建了一个可以自动优化你的 LLM 提示的 RL 代理！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm64gy/p_i_created_a_rl_agent_that_can_autooptimize_your/</link>
      <description><![CDATA[大家好！ 我和我的团队开发了一个自动优化 LLM 提示的系统，该系统具有可视化功能，可以跟踪提示结构和学习进度。 请看这里：https://nomadic-ml.github.io/nomadic/cookbooks/Nomadic_Prompt_Optimization_Report.html 也请查看我们的网站：Nomadic ML。 以下是 GIF 格式的预览！ https://i.redd.it/l57uimn7qkzd1.gif https://i.redd.it/ybkf74aaqkzd1.gif 关于此可视化的工作原理：RL Prompt Optimizer 采用强化学习框架来迭代改进用于语言模型评估的提示。在每一集中，代理都会根据状态表示选择一个操作来修改当前提示，该状态表示对提示的特征进行编码。代理会根据对模型响应的多指标评估获得奖励，从而鼓励开发能够引出高质量答案的提示。 查看我们的 github repo 并给我们一个星星！ https://github.com/nomadic-ml/nomadic    由   提交  /u/vnkn17   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm64gy/p_i_created_a_rl_agent_that_can_autooptimize_your/</guid>
      <pubDate>Fri, 08 Nov 2024 00:48:51 GMT</pubDate>
    </item>
    <item>
      <title>关于如何构建 AGI 的有趣理论。参考强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm1oep/interesting_theory_on_how_to_build_agi_references/</link>
      <description><![CDATA[  由    /u/Radlib123  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm1oep/interesting_theory_on_how_to_build_agi_references/</guid>
      <pubDate>Thu, 07 Nov 2024 21:28:36 GMT</pubDate>
    </item>
    <item>
      <title>2024 年 GPU 和计算技术比较 – 第 7 天</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gm07x3/gpu_and_computing_technology_comparison_2024_day_7/</link>
      <description><![CDATA[        由    /u/Potential_Arrival326   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gm07x3/gpu_and_computing_technology_comparison_2024_day_7/</guid>
      <pubDate>Thu, 07 Nov 2024 20:27:28 GMT</pubDate>
    </item>
    <item>
      <title>您是否同意深度强化学习 (Deep RL) 目前正在经历 Imagenet 时刻这一观点？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gls0qm/do_you_agree_with_this_take_that_deep_rl_is_going/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gls0qm/do_you_agree_with_this_take_that_deep_rl_is_going/</guid>
      <pubDate>Thu, 07 Nov 2024 14:43:12 GMT</pubDate>
    </item>
    <item>
      <title>如何利用强化学习优化单台起重机作业调度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gllq93/how_can_i_optimize_single_crane_job_scheduling/</link>
      <description><![CDATA[      https://preview.redd.it/po4m8h1nufzd1.png?width=1886&amp;format=png&amp;auto=webp&amp;s=427bbc5edd094edebbd6a2a979e256c9f0cafdb0 我正在从事一个涉及具有双桅杆属性的单起重机作业调度的项目。让我详细解释一下每项工作：  工作 1：当两个托盘到达 A 时，将它们从 A 移动到 B。 工作 2：当两个托盘在 B 处的充电时间完成后，将它们从 B 移动到 C。 工作 3：当两个托盘在 C 处的充电时间完成后，将它们从 C 移动到 D。 工作 4：当两个托盘在 D 处的处理完成后，将它们从 D 移动到 E。 工作 5：当两个托盘在 E 处的处理完成后，将它们从 E 移动到 F。  在这个项目中，我打算将工作 1 到 5 定义为动作，同时将每个架子上托盘的存在情况和剩余的充电或处理时间视为状态。我的目标是使用强化学习来选择最佳动作。 我想讨论的是如何将这种状态转换为输入格式。目前，我计划使用 CNN 将这些状态输入到 DQN 中，但我想知道是否有更有效的方法。我想简明扼要地总结一下流程情况。你能推荐一种更有效的方法吗？    提交人    /u/Yunseol_IE   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gllq93/how_can_i_optimize_single_crane_job_scheduling/</guid>
      <pubDate>Thu, 07 Nov 2024 08:18:01 GMT</pubDate>
    </item>
    <item>
      <title>我当前的 RL 项目的直播</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1glgq9w/live_stream_of_my_current_rl_project/</link>
      <description><![CDATA[我要离开我的电脑，但我想检查我的机器、学习环境的进度，所以我设置了一个直播。 我在 Godot 中完成了这个项目，它使用套接字与 PyTorch 通信。目标是让代理找到并导航到目标，而无需知道目标位置。代理只知道它的位置、它的旋转、它的最后一个动作、步数和它的七条视线。 目标是看看我是否可以让这个代理使用一个简单的奖励函数，而不使用目标位置的知识。如果达到了目标，奖励函数只是将 100 分除以序列中每个移动的移动数，否则每个移动将获得 -100 除以序列中的移动数。 该流仅显示并行运行的 100 个模拟中的一个。我觉得这很有趣，我想你们可能也会喜欢。此外，如果有人有任何想法，如何改进它，请随时分享。    提交人    /u/stokaty   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1glgq9w/live_stream_of_my_current_rl_project/</guid>
      <pubDate>Thu, 07 Nov 2024 02:59:43 GMT</pubDate>
    </item>
    <item>
      <title>评论网络无法预测奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1glgcd6/critic_network_is_failling_to_predict_rewards/</link>
      <description><![CDATA[      https://preview.redd.it/y7o4wybc5ezd1.png?width=2058&amp;format=png&amp;auto=webp&amp;s=66421482f30fdad610078cf6e12499ced4​​e87810 我正在训练一个 Actor - Critic 模型，但它无法有效地学习任务。我意识到，Critic Loss 在训练时并没有减少，并决定获得 True 奖励和批评输出的输出，以比较批评网络的性能。正如您在图中看到的，它根本没有学习任何东西。我尝试使用 Vanilla LSTM 进行训练，也尝试使用另一个带有残差连接和前馈网络的自定义 LSTM 块的模型进行训练，但它们都做同样的事情。 我正在为 Actor 和 Critic 头使用共享层，并使用单个优化器进行训练。这里可能有什么问题？    提交人    /u/BagComprehensive79   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1glgcd6/critic_network_is_failling_to_predict_rewards/</guid>
      <pubDate>Thu, 07 Nov 2024 02:39:03 GMT</pubDate>
    </item>
    <item>
      <title>微调与迁移学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gl9w4b/fine_tune_vs_transfer_learning/</link>
      <description><![CDATA[        提交人    /u/employeeINGOAMPT   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gl9w4b/fine_tune_vs_transfer_learning/</guid>
      <pubDate>Wed, 06 Nov 2024 21:38:19 GMT</pubDate>
    </item>
    <item>
      <title>什么是适合 RL 的良好技术堆栈？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gkdebb/what_is_a_good_tech_stack_for_rl/</link>
      <description><![CDATA[目前正在研究 Cuda、Jax、CleanRL、PufferLib、Ray。我遗漏了什么吗？如果有的话，其中哪一个是多余的？    提交人    /u/True_Caregiver485   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gkdebb/what_is_a_good_tech_stack_for_rl/</guid>
      <pubDate>Tue, 05 Nov 2024 18:20:51 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk54sa/deep_reinforcement_learning_generalization/</link>
      <description><![CDATA[理解和诊断深度强化学习。发表于国际机器学习会议 ICML 2024。 链接：https://proceedings.mlr.press/v235/korkmaz24a.html    提交人    /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk54sa/deep_reinforcement_learning_generalization/</guid>
      <pubDate>Tue, 05 Nov 2024 12:06:19 GMT</pubDate>
    </item>
    <item>
      <title>动态状态表示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gk4q2r/dynamic_state_representation/</link>
      <description><![CDATA[大家好！ 我想问一下，是否有人听说过在代理的某个情节中状态空间可以发生变化的场景。 例如，假设我是一个在空房间里徘徊的代理，我的状态空间表示是我的 (x,y) 坐标。突然，我意识到我应该拿起一个位于我旁边房间里的物体。 然后我的状态空间可能会变为 (x,y,current_room,is_holding_anything)。 有谁知道以前的任何工作中存在这种情况？无论是规划还是 RL 领域。 提前谢谢！！    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gk4q2r/dynamic_state_representation/</guid>
      <pubDate>Tue, 05 Nov 2024 11:41:12 GMT</pubDate>
    </item>
    <item>
      <title>我第一次使用强化学习来解决自己的问题！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gjsr6q/my_first_use_of_reinforcement_learning_to_solve/</link>
      <description><![CDATA[        提交人    /u/JealousCookie1664   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gjsr6q/my_first_use_of_reinforcement_learning_to_solve/</guid>
      <pubDate>Mon, 04 Nov 2024 23:35:27 GMT</pubDate>
    </item>
    </channel>
</rss>