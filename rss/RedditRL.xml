<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 11 May 2024 18:17:18 GMT</lastBuildDate>
    <item>
      <title>我可以使用强化学习来代替穷举搜索以获得更好的时间效率吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpnd7r/can_i_use_reinforcement_learning_to_replace/</link>
      <description><![CDATA[我对 RL 非常陌生，想知道是否可以使用它来解决这个特定问题。  我有一个包含 53 个点的列表，每个点都有相关的损失，然后我为这些点生成一个 n 组合列表。这个想法是，例如，我采用这些 (53C3) 的 3 个组合（大约 23,000 个组合），并更改原始 53 中的这些值，并评估每个组合的损失。所以我现在有一个大约 23,000 个项目的列表，每个项目都有一个相关的损失，我只想找到损失最高的组合。 到目前为止，我已经能够使用详尽的方法来做到这一点搜索。但由于我通过神经网络对每个组合进行推理，因此需要大约 25 分钟来迭代每个组合以找到损失最高的组合。穷举搜索的代码如下： attack_combinations = Combinations(X_train_filtered.columns, num_attacked_ap) Attack_combinations_list = [] for Attack in Attack_combinations: Attack_combinations_list.append(attack) with open(&#39;noise -high.txt&#39;) as file: file_contents = file.read() const_noise = np.fromstring(file_contents, sep=&#39; &#39;) aps_loss = {} best_loss = 0 best_combination = 0 for Attack_combination in tqdm(attack_combinations_list): # 应用攻击to the dataset X_val_attacked = deepcopy(X_val_filtered) for col_name in X_val_attacked.columns: if col_name in Attack_combination: Noise = const_noise X_val_attacked[col_name] += Noise # 在被攻击数据集上评估模型 loss, precision, mse = model.evaluate(X_val_attacked , y_val_filtered, verbose=0) aps_loss[tuple(attack_combination)] = mse 如果 mse &gt; best_loss: best_loss = mse best_combination = Attack_combination print(&quot;最佳组合：&quot;, best_combination) print(f&quot;{num_attacked_ap}-组合的最高 DL MSE:&quot;, best_loss)  但是，我想尝试更大的 n 组合，在这种情况下，组合的数量会增加到数百万，从而使穷举搜索变得不可行。因此，我想知道 RL 是否可以在这里使用以及如何使用。我很高兴能指出正确的方向，提前谢谢您。   由   提交/u/CapedCrusader10   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpnd7r/can_i_use_reinforcement_learning_to_replace/</guid>
      <pubDate>Sat, 11 May 2024 18:04:58 GMT</pubDate>
    </item>
    <item>
      <title>关于如何在 Metaworld 中训练 pickplacev2 任务的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpl5e6/questions_about_how_to_train_pickplacev2_task_in/</link>
      <description><![CDATA[嗨， 有人在 pickplacev2 MetaWorld 中的任务？我尝试使用自己的实现以及使用 Stable Baselines3 等库来训练代理，但代理无法有效学习，甚至难以达到目标。我的代码看起来是正确的，因为它可以很好地处理其他任务，例如 reachv2、windowclosev2 和其他几个任务。 有趣的是，我已经成功训练了 DDPG代理通过采用特定技术来完成此任务。我认为失败有两个原因。首先是pickplacev2的奖励函数没有设计。 pickplacev2 中到达阶段的奖励信号非常弱，在初始位置通常小于 0.03，这可能会增加难度。这令人困惑，因为该论文提出了一种改进的奖励设计。其次，探索对于这项任务来说似乎很重要。对于 DDPG，修改噪声尺度至关重要，这表明探索对于这项任务很重要 如果您取得了成功，您能否分享有关特定配置、超参数或有助于成功训练的修改的见解或技巧？    由   提交/u/DF_13  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpl5e6/questions_about_how_to_train_pickplacev2_task_in/</guid>
      <pubDate>Sat, 11 May 2024 16:21:39 GMT</pubDate>
    </item>
    <item>
      <title>这是一个询问基于理论/数学的强化学习科目途径的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpkog7/here_is_a_question_asking_for_a_theorymathbased/</link>
      <description><![CDATA[这是一个询问机器学习或强化学习科目的基于理论/数学的途径的问题： 您可以吗概述涵盖机器学习和强化学习的理论和数学基础的课程或课程路径？我正在寻找深入研究这些领域背后的理论基础、概率模型、优化技术和数学分析的主题，而不仅仅是关注应用算法或编码实现。目标是在转向更实际的应用之前建立强大的理论掌握。   由   提交/u/Background_Bowler236   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpkog7/here_is_a_question_asking_for_a_theorymathbased/</guid>
      <pubDate>Sat, 11 May 2024 16:00:36 GMT</pubDate>
    </item>
    <item>
      <title>对 RL 库/项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</link>
      <description><![CDATA[嗨， 我目前正在休学一年，致力于自学和强化学习项目。我将于 9 月份加入伦敦大学学院的计算统计和机器学习理学硕士学位，目标是准备 RL 博士学位。 在过去的几个月中，我一直在撰写两篇 RL 研究论文，这些论文将在分别提交给 NeurIPS（会议论文，第二作者）和 ICML（立场文件，共同作者）。由于我还有 4 个月的时间，我正在寻找其他项目来改善我即将到来的博士申请的简历。 我考虑过开发一个可用于研究的库，但很难选择一个具体主题。以下是我目前的指导方针：  框架：最好是 JAX，因为它最近越来越受欢迎，而且生态系统仍在扩展。我认为有机会从更成熟的框架（例如 Pytorch）中调整现有库，或者为 JAX 中缺乏合适软件的特定 RL 问题提供便捷的解决方案。 -  领域：我对开放性（课程学习、无监督环境设计）和多代理设置特别感兴趣。然而，已经有一些流行的库涵盖了这些领域的大量用例（Minimax、JaxMARL、JaxUED...）。我最终想在未来更深入地研究元和进化强化学习等其他领域。据我所知，进化强化学习也有一些不错的库（evojax、evosax 等），但我对元强化学习不太确定。 时间范围 ：大约 5 个月  我对社区对以下问题的见解很感兴趣：  是否有特定的 RL 领域缺乏有用的库贾克斯？ （在我引用的或其他人中） 如果没有，为现有库做出贡献会是更好的主意吗？ 是否更需要高效的环境和基准？   感谢您的帮助！   由   提交 /u/OptimalBandicoot1671   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</guid>
      <pubDate>Sat, 11 May 2024 15:11:46 GMT</pubDate>
    </item>
    <item>
      <title>需要帮忙</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpj3fy/need_help/</link>
      <description><![CDATA[我的模型工作正常。它是带有 Carla 模拟器和 td3 实现的变道模型。但是当我在environment.py 文件中添加深度和障碍物传感器时。看来我犯了一个错误。现在，车子不动了。它产卵并且不动它突然重生。我会付费寻求帮助。( 10$ ) 但很紧急   由   提交/u/Leather_Efficiency34   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpj3fy/need_help/</guid>
      <pubDate>Sat, 11 May 2024 14:45:52 GMT</pubDate>
    </item>
    <item>
      <title>[需要帮助] 将 SB3 DQN 训练脚本转换为 RLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpiyd7/help_needed_convert_sb3_dqn_training_script_to/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpiyd7/help_needed_convert_sb3_dqn_training_script_to/</guid>
      <pubDate>Sat, 11 May 2024 14:39:18 GMT</pubDate>
    </item>
    <item>
      <title>有关使用首次访问策略蒙特卡罗和新奖励信号解决冰冻湖问题的反馈。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpdzxf/feedback_regarding_solving_frozen_lake_using/</link>
      <description><![CDATA[我试图用道馆图书馆提供的默认奖励设置来解决冰冻湖。  使用默认的奖励设置，我发现无法解决冰冻的湖泊环境。特工会在没有到达目标的情况下撞入洞中，或者继续撞到墙壁上，然后最终坠毁。  因为我在读 Sutton 和 Barto 的《强化学习》一书，我知道对于二十一点问题，书中提到了负奖励。  然后我决定测试新的奖励系统。如果智能体掉进洞里，奖励将为-1。如果智能体撞到墙，奖励将为-0.1，如果智能体达到目标，奖励将为+10。  令我惊讶的是，这个奖励系统运行得非常好，代理更快地找到目标。  我想得到这个子的反馈。  是否可以使用蒙特卡罗以默认奖励信号来解决 Frozen Lake 问题？  或者真的有必要使用新的奖励信号来改变它吗？  顺便说一句，我已经使用动态规划和时间差分方法用默认奖励信号解决了冰冻湖。  谢谢。    由   提交/u/mono1110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpdzxf/feedback_regarding_solving_frozen_lake_using/</guid>
      <pubDate>Sat, 11 May 2024 09:58:24 GMT</pubDate>
    </item>
    <item>
      <title>我迫切需要学习方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cp6xc6/im_in_desperate_need_for_help_with_learning/</link>
      <description><![CDATA[你好， 我第一次遇到强化学习，是在 3 年前（当时我还在读大二） CS 本科学位）在浏览 Coursera 时发现了阿尔伯塔大学的 RL 专业。我感到深深的联系，并继续完成了 4 门课程。这个 Reddit 子版块中的许多人可能已经意识到，专业化并没有深入太多，它提供的信息足以应用一些最基本的 RL 算法和技术。我能够利用这些知识来完成我的本科论文，涉及强化学习在云计算问题中的应用，并从这项工作中获得了一些相当不错的出版物。  毕业已经一年了，一直从事初级软件开发人员的工作。我想从事 RL 方面的研究。我相信我缺乏直接攻读博士学位所需的个人资料。因此，我计划在一所优秀的学校攻读基于论文的硕士学位课程，这可以取得博士学位。  即使我注册了基于论文的硕士课程，在大多数学校，获得 PI 也是我的责任。每当我打开任何研究强化学习的教授的教员页面时，我都会看到一些我根本不理解的东西。每个人似乎都在研究 RL 中的一些超级小众的东西，而对我来说 RL 本身似乎是一个相当小众的领域。我听人们说，要获得成功的研究型项目经验，我必须拥有“相同的兴趣”作为我的 PI 等等。 我现在面临的主要问题是，我缺乏对该领域的知识深度，甚至不知道自己想要什么。这让我找到了 YouTube 上的一些材料和一些高级强化学习教科书。但我很快发现我所拥有的数学知识水平不足以理解这些高级材料。那么，我现在该怎么办？对于一个试图进入基于论文的硕士课程的本科生来说，只对自己感兴趣的领域有基本的了解可以吗？如果没有，我如何获得我确定缺乏的知识？ 我知道这是一个混合问题，不仅涉及强化学习，还涉及一般研究生院的建议。抱歉，如果它不属于此子项。 感谢您的任何建议。   由   提交 /u/SeaworthinessHot5365   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cp6xc6/im_in_desperate_need_for_help_with_learning/</guid>
      <pubDate>Sat, 11 May 2024 02:25:34 GMT</pubDate>
    </item>
    <item>
      <title>数据质量网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cosbwj/dqn/</link>
      <description><![CDATA[&lt;表&gt;      I我一直在尝试从头开始实现 DQN，但预测值似乎有些不对劲，所以我在这里做了一个简单的测试。这里，有一系列价值为 1 的奖励，最终没有奖励。 DQN（净）应该预测折扣奖励。然而，由于某种原因，对后续状态的估计会急剧上升，这与实际曲线相反，并且 DQN 需要几个时期才能意识到其错误并将初始状态向上移动。这里可能发生什么？ https ://preview.redd.it/qok0jda3bmzc1.png?width=915&amp;format=png&amp;auto=webp&amp;s=fde8029f66d4a40c761a90c23f36a657d85b1b9b 代码： 从torch导入nn，optim导入matplotlib.pyplot作为plt导入随机导入numpy作为np规模= 100折扣= 0.9奖励= [1 for i in range(scale-1)] + [0] net = nn.Sequential ( nn.Linear(4,4), nn.Mish(), nn.Linear(4,1)) opt = optim.RMSprop(net.parameters(), lr=0.01) def 步骤(c): 全局折扣,奖励，净，opt qvals = [] for i in range(scale): inp = torch.tensor([[i/50 for _ in range(4)]], dtype=torch.float32) #inp = torch.tensor ([[i/50 for _ in range(1)]+[random.random() for _ in range(3)]], dtype=torch.float32) 输出 = net(inp) q = output.detach() .item() qvals.append(q) 如果 i == scale-1: next_q = 0 否则: next_inp = torch.tensor([[(i+1)/50 for _ in range(4)]], dtype= torch.float32) next_q = net(next_inp).detach().item() td_error = 折扣 * next_q + 奖励[i] - q grad = torch.tensor([[-td_error/scale]], dtype=torch.float32 ) #grad = torch.tensor([[-td_error / np.sqrt(1 + td_error**2) / 100]], dtype=torch.float32) 输出.backward(grad) nn.utils.clip_grad_norm_(net.parameters (), 1) opt.step() opt.zero_grad() if c%1 == 0: plt.plot(list(range(scale)), qvals) plt.savefig(f&#39;zoo{c}.png&#39; ) plt.clf() #plt.show() for i in range(50): step(i)    由   提交/u/AUser213  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cosbwj/dqn/</guid>
      <pubDate>Fri, 10 May 2024 15:30:39 GMT</pubDate>
    </item>
    <item>
      <title>CrossQ：深度强化学习中的批量标准化，以提高样本效率和简单性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1corrqu/crossq_batch_normalization_in_deep_reinforcement/</link>
      <description><![CDATA[     &lt; /td&gt; 论文和代码：http:// /adityab.github.io/CrossQ CrossQ 是一种无模型的离策略方法，它无需执行额外的梯度步骤就超越了当前的 SOTA。它本质上是：  采用 SAC（6yo 方法） 删除目标网络 添加批量归一化  这些简单的编辑足以超越 REDQ 和 DroQ 的强大性能，仅需要 5% 的梯度步长。 Twitter：https://twitter.com/aditya_bhatt/status/1768342823747674377 ICLR 2024 焦点演讲：https://iclr.cc/virtual/2024/poster/18699 （这里是第一位合著者，很乐意提供帮助！） https://preview.redd.it/0bhb82z77mzc1.png?width=3582&amp; amp;格式=png&amp;auto=webp&amp;s=41f3071cb84445734a6338e672658c99044ea560   由   提交 /u/RoboticsLiker   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1corrqu/crossq_batch_normalization_in_deep_reinforcement/</guid>
      <pubDate>Fri, 10 May 2024 15:06:25 GMT</pubDate>
    </item>
    <item>
      <title>使用 RL Baselines3 Zoo 进行分布式优化的启动试验有多少次</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</link>
      <description><![CDATA[您好，我正在 RL Baselines3 Zoo 中的 6 个进程中分配优化（github），它使用 Optuna 。 我关心的参数代码位于 rl_zoo3/train.py 中：  parser.add_argument( &quot;--n-trials&quot;, help=&quot;优化超参数的试验次数。&quot; &quot;这适用于每个优化运行器，而不是整个优化过程。&quot;, type=int, default= 500, ) parser.add_argument(&quot;--n-startup-Trials&quot;, help=&quot;使用 optuna 采样器之前的试验次数&quot;,  我知道我是否使用以下每个6 个进程 --n-trials 100 那么我将获得 600 次试验。 但是 --n-startup-Trials 10 会怎么样呢？是 10 次还是 60 次启动试验？    提交ufoludek3000&quot;&gt; /u/ufoludek3000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1com8sm/how_many_startup_trials_in_distributed/</guid>
      <pubDate>Fri, 10 May 2024 10:25:03 GMT</pubDate>
    </item>
    <item>
      <title>生成式人工智能已经达到顶峰了吗？ - 电脑爱好者</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co4gsj/has_generative_ai_already_peaked_computerphile/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/FedeRivade  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co4gsj/has_generative_ai_already_peaked_computerphile/</guid>
      <pubDate>Thu, 09 May 2024 18:38:09 GMT</pubDate>
    </item>
    <item>
      <title>“通过强化学习出现类似信念的表征”，Hennig 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1co0wb7/emergence_of_belieflike_representations_through/</guid>
      <pubDate>Thu, 09 May 2024 16:06:37 GMT</pubDate>
    </item>
    <item>
      <title>AlphaMath 几乎为零：流程无流程监督</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnuaab/alphamath_almost_zero_process_supervision_without/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2405.03553 代码：https ://github.com/MARIO-Math-Reasoning/Super_MARIO 模型：https://huggingface.co/MARIO-Math-Reasoning/AlaphaMath-7B 摘要： &lt; blockquote&gt; 大型语言模型 (LLM) 的最新进展极大地增强了他们的数学推理能力。然而，这些模型仍然难以解决需要多个推理步骤的复杂问题，经常导致逻辑或数值错误。虽然数字错误很大程度上可以通过集成代码解释器来解决，但识别中间步骤中的逻辑错误更具挑战性。此外，手动注释这些培训步骤不仅成本高昂，而且需要专业知识。在本研究中，我们引入了一种创新方法，通过利用蒙特卡罗树搜索（MCTS）框架自动生成过程监督和评估信号，从而消除了手动注释的需要。本质上，当法学硕士经过良好的预训练时，只需要数学问题及其最终答案来生成我们的训练数据，而不需要解决方案。我们继续训练一个阶梯级价值模型，旨在改进法学硕士在数学领域的推理过程。我们的实验表明，使用由 MCTS 增强的法学硕士自动生成的解决方案可以显着提高模型处理复杂数学推理任务的能力。    由   提交/u/EternalBlueFriday  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnuaab/alphamath_almost_zero_process_supervision_without/</guid>
      <pubDate>Thu, 09 May 2024 10:49:53 GMT</pubDate>
    </item>
    <item>
      <title>用于在网格地图上寻找路径的 RL - SLAM。成功率问题（SAC + HER）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cnr2w8/rl_for_path_finding_on_the_grid_map_slam_success/</link>
      <description><![CDATA[      大家好！我一直在努力解决涉及车辆在黑白地图上移动的自定义环境。您可以将地图视为二进制网格。车辆的目标是到达白色区域内随机选择的目的地。当车辆到达目的地、撞到地图上的黑色区域或撞上边界墙时，游戏结束。 我使用 SB3 中的 SAC + HER，但我一直在使用一个多月以来，一直在努力实现 0.6 以上的成功率。这是我尝试过的方法：  使用熵系数进行实验 添加模拟激光雷达光束和到最近黑色像素的距离等观测结果 使用不同的超参数集进行实验（超过 500 次运行） 调整缓冲区大小  大多数情况下，代理很难到达起始点正前方黑墙之外的区域点。 我的奖励计算如下： rew = -np.power(np.dot(np.abs(achieved_goal[:1] -desired_goal), Weights_array ), 0.5)  观察空间定义为： self.observation_space =spaces.Dict({ &#39;observation&#39;:spaces.Box(low= 0，高= 1，形状=（6 + self.num_lidar_beams，），dtype = np.float32），&#39;实现的目标&#39;：spaces.Box（低= 0，高= 1，形状=（6，），dtype = np .float32), &#39;desired_goal&#39;:spaces.Box(low=0, high=1, shape=(6,), dtype=np.float32), })  动作空间：  self.action_space = space.Box(low=np.array([-1, 0]), high=np.array([1, 1]), dtype=np.float32 )  这些动作控制车辆的转向角度和速度。 状态是： self.state[0 ] = self.normalize_state(self.x,self.screen_width,0) self.state[1] = self.normalize_state(self.y,self.screen_height,0) self.state[2] = hdg_norm self.state[3 ] = v self.state[4] = self.normalize_state(np.cos(np.deg2rad(hdg)), 1, 0) self.state[5] = self.normalize_state(np.sin(np.deg2rad(hdg) )), 1, 0) self.state[-self.num_lidar_beams:] = litar_distances # 更新该州的激光雷达数据  我附上了四张图片：  环境的外观 环境的另一个视图 按区域说明成功率的地图 显示随时间变化的成功率的图  https:// /preview.redd.it/6dmfb58qnczc1.png?width=3364&amp;format=png&amp;auto=webp&amp;s=6b13c219c427f66e1035cf16bf78b4baefed7fc2 自从我工作以来，我非常感谢任何建议对此已经有一段时间了。有没有人从事过类似的项目并愿意分享代码？或者你知道 GitHub 上类似 2D 问题的示例吗（不是停车环境，因为我已经广泛研究过这些问题）？ 如果没有地图（所有像素都是白色），成功率会达到 1.0&lt; /p&gt;   由   提交/u/Sharp-Record1600  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cnr2w8/rl_for_path_finding_on_the_grid_map_slam_success/</guid>
      <pubDate>Thu, 09 May 2024 07:06:19 GMT</pubDate>
    </item>
    </channel>
</rss>