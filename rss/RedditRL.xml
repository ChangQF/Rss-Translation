<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 28 Jan 2025 01:13:43 GMT</lastBuildDate>
    <item>
      <title>需要基于项目的课程建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibnmmb/need_project_based_courses_recommendations/</link>
      <description><![CDATA[和标题一样多。我想一边做项目一边学习，所以如果有这方面的建议，请告诉我。如果你觉得这可能不是学习 RL 的最佳方法，请再次告诉我     提交人    /u/arrshsh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibnmmb/need_project_based_courses_recommendations/</guid>
      <pubDate>Mon, 27 Jan 2025 23:33:59 GMT</pubDate>
    </item>
    <item>
      <title>您推荐哪个库来处理具有临时变压器的预训练模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibn7k2/which_library_do_you_recommend_to_deal_with_a/</link>
      <description><![CDATA[我有一个自动驾驶模型（2 个摄像头和 2 个传感器），该模型已使用监督学习进行了预训练。该模型具有由时间转换器管理的时间序列，预训练的结果非常好，但是当我将其带到模拟器时，它会损失很多。我的想法是使用强化训练来提高模拟器中模型的性能。我一直在尝试 stable-baselines 3，这是我最常用的，但它似乎不支持转换器。有什么关于如何解决这个问题的想法吗？    提交人    /u/Luquin-as   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibn7k2/which_library_do_you_recommend_to_deal_with_a/</guid>
      <pubDate>Mon, 27 Jan 2025 23:15:19 GMT</pubDate>
    </item>
    <item>
      <title>帮助请求：努力提高地下城导航项目中的 AI 性能</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibm1pw/help_request_struggling_with_improving_ai/</link>
      <description><![CDATA[大家好， 我正在为我的大学开展一个 AI 项目，我正在训练一个 AI 穿越随机生成的 11x11 网格地牢。目标是让 AI 避开火势，捡起宝箱，然后走到出口。 我正在使用 Stablebaselines3 PPO 算法。 我花了很多时间调整项目的各个方面，包括：  调整参数 修改奖励系统 更改隐藏层  然而，尽管进行了所有这些更改，AI 的性能却没有提高。我只看到解释方差的微小变化（2-10％的增加或减少），但评估/平均奖励要么停滞不前，要么上下波动，但始终趋向于零。 以下是我尝试过的一些方法：  改变学习率（更高和更低） 尝试不同的奖励机制 增加剪辑范围 使用更宽和/或更深的隐藏层 尝试这些更改的不同组合  尽管付出了这些努力，但似乎没有任何方法对改善评估/平均奖励产生重大影响。 有人遇到过类似的问题或对我可能遗漏的内容有什么建议吗？我非常感谢任何建议或见解。 提前致谢！   由    /u/Competitive-Bar-5882  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibm1pw/help_request_struggling_with_improving_ai/</guid>
      <pubDate>Mon, 27 Jan 2025 22:25:48 GMT</pubDate>
    </item>
    <item>
      <title>具有决策转换器和多种行动的经验</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibltys/experience_with_decision_transformer_and_multiple/</link>
      <description><![CDATA[你好 :) 我想知道是否有人有使用决策转移进行多项操作的经验？我有一个组合图定位问题，其中动作空间可以考虑如下： - 动作 1：要添加的节点 - 动作 2：要添加的节点 从技术上讲，我可以将所有组合编码为一个标记，但这会随着节点数量的增加而呈二次方增长。所以，我想避免这种情况。 传统上，在 Decision Transformer 中，我们针对一项操作进行优化。直观地说，多项操作也应该可以工作，因为我们将它们传递到嵌入层，然后连接起来：动作、状态和奖励嵌入。 你有这方面的经验吗？    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibltys/experience_with_decision_transformer_and_multiple/</guid>
      <pubDate>Mon, 27 Jan 2025 22:16:54 GMT</pubDate>
    </item>
    <item>
      <title>“在大型地下采矿环境中部署空中多智能体系统以实现自动任务执行”，Dhalquist 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibebim/deployment_of_an_aerial_multiagent_system_for/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibebim/deployment_of_an_aerial_multiagent_system_for/</guid>
      <pubDate>Mon, 27 Jan 2025 17:14:35 GMT</pubDate>
    </item>
    <item>
      <title>GRPO 可以用于多圈 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</link>
      <description><![CDATA[https://arxiv.org/abs/2402.03300 有些人可能已经看到了 PPO 的 RL 替代方案，即组相对策略优化 (GRPO)，其中不是训练价值模型，而是多次采样策略，获得平均奖励，并使用它来找出优势。 从审查实现来看，对话中只有一个回合，因为 LLM 要么正确解决数学问题，要么失败，所以在这种情况下奖励和价值是相同的，因为预期的未来奖励就是奖励。 GRPO 是否可以应用于多回合 RL 或更长远的项目，其中策略与环境多次交互？   提交者    /u/30299578815310   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ibclet/can_grpo_be_used_for_multiturn_rl/</guid>
      <pubDate>Mon, 27 Jan 2025 16:05:02 GMT</pubDate>
    </item>
    <item>
      <title>钟摆政策不会学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ib4iem/pendulum_policy_doesnt_learn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ib4iem/pendulum_policy_doesnt_learn/</guid>
      <pubDate>Mon, 27 Jan 2025 09:35:57 GMT</pubDate>
    </item>
    <item>
      <title>我的系统是否真的需要 RL 模型，或者检测模型是否就足够了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ib04za/do_i_really_need_an_rl_model_for_my_system_or/</link>
      <description><![CDATA[大家好，希望你们一切顺利 我正在做一个项目，目标是确定何时在无线传感器网络中执行密钥刷新。一般的想法是识别节点中的异常行为（如受损或故障节点），然后决定是否需要密钥刷新。 密钥刷新需要大量资源，因此过于频繁地执行密钥刷新是一种浪费，但如果您不及时执行密钥刷新，您的网络将变得岌岌可危。 现在，我决定使用 RL 模型来做出这个决定，但我一直在质疑 RL 是否真的有必要，或者更简单的检测模型是否足够（然而检测传感器节点受损攻击非常困难）？特别是在这个子版块的一篇文章中，有人指出，确实可以使用简单的监督轻量级模型而不是 RL 模型来解决许多问题。 提前感谢您的建议！我很乐意回答任何问题。 PS：我只是一名计算机科学专业的学生，​​所以我对 RL 的了解有限，我发现它是最难理解的 ML 模型    提交人    /u/Sufficient-Lie-1632   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ib04za/do_i_really_need_an_rl_model_for_my_system_or/</guid>
      <pubDate>Mon, 27 Jan 2025 04:48:19 GMT</pubDate>
    </item>
    <item>
      <title>旧的 RL 课程仍然有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</link>
      <description><![CDATA[大家好。我想知道我应该从哪门课程开始学习 RL。我想从 2024 年的斯坦福 234 课程开始，但我不知道它是否教授基础知识。我还听说 David Silver 的课程很棒，但它是近 10 年前的，我不知道我应该从哪门课程开始。 TL;DR 开始学习 RL 的最佳课程是什么？    提交人    /u/madcraft256   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iazuqs/are_old_rl_courses_still_relevant/</guid>
      <pubDate>Mon, 27 Jan 2025 04:31:32 GMT</pubDate>
    </item>
    <item>
      <title>4-7 年前的 PyTorch 代码能运行吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iahkrx/will_pytorch_code_from_47_years_ago_run/</link>
      <description><![CDATA[我发现很多 RL repos 上次更新是 4 到 7 年前，比如这个： https://github.com/Coac/never-give-up PyTorch 在过去几年里有过很多重大变化吗？修复旧代码以再次运行有多困难？    提交人    /u/exploring_stuff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iahkrx/will_pytorch_code_from_47_years_ago_run/</guid>
      <pubDate>Sun, 26 Jan 2025 15:38:11 GMT</pubDate>
    </item>
    <item>
      <title>Ray 2.40 上的 PBT</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</link>
      <description><![CDATA[有人熟悉在 Ray 2.4 上做 PBT 吗？ 如果有人知道如何解决这个问题，我们将不胜感激： https://discuss.ray.io/t/metric-for-pbt-in-ray-2-40/21619 摘要：我想基于评估情节奖励均值指标使用 PBT 对 PPO 执行超参数优化，但我似乎无法使用该指标或任何有用的指标进行训练。    提交人    /u/nukelius   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iabipj/pbt_on_ray_240/</guid>
      <pubDate>Sun, 26 Jan 2025 10:48:27 GMT</pubDate>
    </item>
    <item>
      <title>寻求与 RL 研究人员的合作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</link>
      <description><![CDATA[大家好， 我是 UIUC 的计算机科学博士生，具有理论算法背景（在 SODA/ICALP/ESA 发表过文章；主要是近似算法、图问题的可扩展算法、在线算法等）。最近，我将重点转向 使用强化学习 (RL) 解决 NP 难图问题，我正在寻找有相同兴趣的合作者。 关于我的工作：  在理论会议（SODA、ESA）和 ML 会议（NeurIPS）上发表过文章。 最近开发了一种基于 RL 的 NP 难图问题方法，包括从头开始在 PyTorch 中编写自定义 GNN 框架。论文已提交给 ICML。 坚实的理论基础 + 良好的编码能力，旨在将理论与实践相结合。  寻找： 对将 RL 与图算法/组合优化问题相结合感兴趣的研究人员，尤其是以下人员：  研究 NP 难图问题（例如，TSP、顶点覆盖、图分区）。 关心为什么学习策略有效（例如，理论保证、泛化分析）。 想要构建既有原则又有实践效率的方法。  如果这与您的工作或兴趣重叠，请随时给我发私信！我很乐意分享我的论文草稿，讨论想法或探索合作。 （使用一次性帐户进行匿名，但可以通过电子邮件/ LinkedIn 进行验证。）    提交人    /u/ForAllEpsilonExists   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia87tb/looking_for_collaborations_with_rl_researchers/</guid>
      <pubDate>Sun, 26 Jan 2025 07:00:09 GMT</pubDate>
    </item>
    <item>
      <title>构建定制的机械臂环境并训练 AI 代理来控制它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</link>
      <description><![CDATA[        由    /u/Fabulous-Extension76  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia5d52/built_a_custom_robotic_arm_environment_and/</guid>
      <pubDate>Sun, 26 Jan 2025 03:59:44 GMT</pubDate>
    </item>
    <item>
      <title>“DeepSeek-R1：通过强化学习激励法学硕士中的推理能力”，Guo 等人 2025 {DeepSeek}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</guid>
      <pubDate>Sat, 25 Jan 2025 22:52:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Collab 中安装 MARLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</link>
      <description><![CDATA[我按照说明在 Collab 中安装 MARLib： https://marllib.readthedocs.io/en/latest/ conda create -n marllib python=3.8 conda activate marllib git clone cd MARLlib pip install --upgrade pip pip install -r requirements.txt # 我们推荐 gym 版本在 0.20.0~0.22.0 之间。 pip install gym&gt;=0.20.0,&lt;0.22.0 # 将补丁文件添加到 MARLlib python patch/add_patch.py​​ -yhttps://github.com/Replicable-MARL/MARLlib.git  要求安装到 ray 1.8.0，找不到该版本（我也尝试过 1.13 但找不到）。 删除版本会导致更多错误和更多不兼容性。总是显示相同的消息： 错误：subprocess-exited-with-error 当安装没有特定版本的所有内容时，调用 marl.algos.mappo 时，它会抛出： ModuleNotFoundError：没有名为“ray.rllib.agents”的模块 有人可以为我提供安装 MARLlib 的更新说明并且没有不兼容性吗？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</guid>
      <pubDate>Sat, 25 Jan 2025 22:33:04 GMT</pubDate>
    </item>
    </channel>
</rss>