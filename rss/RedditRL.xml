<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sat, 08 Feb 2025 09:15:35 GMT</lastBuildDate>
    <item>
      <title>🚀训练四倍的增强学习：从零到英雄！ 🦾</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ik7dhn/training_quadrupeds_with_reinforcement_learning/</link>
      <description><![CDATA[    内部有什么？奖励功能&lt; /strong&gt;用于有效学习✅训练运动策略在模拟中使用 ppo &lt; /strong&gt;（以撒健身房，Mujoco等）。 /strong&gt;对现实世界部署的挑战 灵感来自 Genesis 的作品以及基于RL的机器人控制的进步，我们的教程为训练四足动物提供了结构化方法，无论您是&#39;是研究人员，工程师或爱好者。 一切都是 open-access   - 没有付费墙，只是纯RL知识！ 🚀  文章： 使四倍体学会走路&lt; /a&gt;  代码：  github repo     很想听听您的反馈并讨论机器人运动的RL策略！ 🙌   https://reddit.com/link./link./link/link/link/1ik7dhn/video/arizr9gikshe1/player &lt; /a&gt;   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/federicosarocco     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ik7dhn/training_quadrupeds_with_reinforcement_learning/</guid>
      <pubDate>Fri, 07 Feb 2025 22:16:46 GMT</pubDate>
    </item>
    <item>
      <title>为TrackMania构建RL模型 - 需要提取跟踪中心线的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ik4mbz/building_an_rl_model_for_trackmania_need_advice/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家控件。在进行培训之前，我需要一种可靠的方法来提取跟踪数据（特别是中心线），以帮助AI预测并保持课程。 最初，我试图从轨道文件中提取块数据使用gbx.net 2，但是由于轨道样式和块位置的多样性，我无法生成一致的中心线。鉴于这一挑战，我现在正在考虑一种替代方法：开发事先探索地图的侦察AI，通过反复试验识别轨道边界，然后计算中心线。 ，但是，在我投资大量之前是时候构建该系统了，我很想听听那些有更多经验的人。这是一种合理的方法，还是我可能会忽略的更有效的方法？ ，只是为了抢占一个共同的建议 - 我不想手动驱动轨道并记录数据。对我而言，AI的全部要点是编写代码，该代码一旦运行就可以接管任务。 期待任何见解！  &lt;！&lt;！ -  sc_on-- - &gt;＆＃32;提交由＆＃32; /u/_jaq0b_     [link]  ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ik4mbz/building_an_rl_model_model_model_for_trackmania_need_advice/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ik4mbz/building_an_rl_model_for_trackmania_need_advice/</guid>
      <pubDate>Fri, 07 Feb 2025 20:20:01 GMT</pubDate>
    </item>
    <item>
      <title>“基于价值的深度RL量表可以预见”，Rybkin等人2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ik3274/valuebased_deep_rl_scales_predictably_rybkin_et/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ik3274/valuebaseed_deep_rl_scal_scales_predictaliquality_rybkin_et/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ik3274/valuebased_deep_rl_scales_predictably_rybkin_et/</guid>
      <pubDate>Fri, 07 Feb 2025 19:14:27 GMT</pubDate>
    </item>
    <item>
      <title>关于RL用于LLM推理的RL教程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ik13q1/tutorials_about_rl_for_reasoning_in_llm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在寻找有关如何组合llm+rl+cot的教程。 我会在拥抱脸上打开 - R1，但我想知道是否有人知道其他来源？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/what_did_it_it_cost_e_t      [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ik13q1/tutorials_about_rl_for_reasoning_in_llm/</guid>
      <pubDate>Fri, 07 Feb 2025 17:54:24 GMT</pubDate>
    </item>
    <item>
      <title>微调欺诈检测的LLMS-我们现在在哪里？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijy8ah/finetuning_llms_for_fraud_detectionwhere_are_we/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  欺诈检测传统上依赖于基于规则的算法，但是随着欺诈策略变得更加复杂，许多公司现在正在探索AI驱动的解决方案。经过微调的LLM和AI代理商正在财务安全中进行测试：  交叉引用财务文件（发票，POS，收据）以检测不一致之处 识别网络钓鱼电子邮件与微调分类器进行骗局 分析实时分析欺诈风险评估的交易数据  问题仍然存在：微调LLM在识别财务方面的有效性如何与传统方法相比，欺诈？开发人员在培训这些模型方面面临哪些挑战，以减少误报，同时保持高检测率？ 即将举行的直播，展示了如何使用微型LLM和基于规则的技术来构建AI代理以进行欺诈检测。 ： https://ubiai.tools/webinar-landing-page/      &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ubiai     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijy8ah/finetuning_llms_for_fraud_detectionwhere_are_we/</guid>
      <pubDate>Fri, 07 Feb 2025 15:56:49 GMT</pubDate>
    </item>
    <item>
      <title>Schnell等人“时间差异学习：为什么会变得快速，如何更快”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijxavb/temporal_difference_learning_why_it_can_be_fast/</link>
      <description><![CDATA[    [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijxavb/temporal_difference_learning_why_it_can_be_fast/</guid>
      <pubDate>Fri, 07 Feb 2025 15:17:07 GMT</pubDate>
    </item>
    <item>
      <title>“用字母分析求解奥林匹克几何形状的金医师表现”，Chervonyi等2025 {dm}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijx584/goldmedalist_performance_in_solving_olympiad/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijx584/goldmedalist_performance_in_solving_olympiad/</guid>
      <pubDate>Fri, 07 Feb 2025 15:10:04 GMT</pubDate>
    </item>
    <item>
      <title>困惑Pro 7.99 $ / yr</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijvk6b/perplexity_pro_799_yr/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好！我以7.99 $/yr（每月的0.66 $！）的价格出售困惑Pro。    Pro访问可以直接在您的电子邮件中激活！您可以通过PayPal，Wise，USDT，ETH，UPI，PAYTM或其他方法轻松付款。  •不要错过这笔负担得起的交易！这是通过困惑Pro合作计划100％合法的。   dm me或在下面发表评论，如果有兴趣！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/panelable_smm     [link]  ＆＃32;   [注释]    /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijvk6b/perplexity_pro_799_yr/</guid>
      <pubDate>Fri, 07 Feb 2025 13:57:48 GMT</pubDate>
    </item>
    <item>
      <title>TMLR或UAI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijva4i/tmlr_or_uai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，伙计，博士学位ML学生这个方面。实际上，我对我的工作的潜在场所感到困惑。因此，如您所知，UAI截止日期是2月10日，此后，我看到的著名会议（以ML为核心）是神经，该神经在5月的截止日期。  所以我想知道TMLR是否比UAI更好，而我知道ICML，ICLR和Neurips游戏完全不同，我只是想知道我是否应该继续前进或更喜欢提交向TMLR工作。   ps：工作在在线学习的空间中，主要是为强盗文献做出贡献（高度理论），而LLM SPSCE   pps汲取了动机：不确定是否重要，但是我更倾向于在我的博士学位  &lt;！ -  sc_on-&gt;＆＃32;之后更倾向于行业角色。提交由＆＃32; /u/u/fanding-nerve-4056      fink]  ＆＃32;   [comment]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijva4i/tmlr_or_uai/</guid>
      <pubDate>Fri, 07 Feb 2025 13:43:49 GMT</pubDate>
    </item>
    <item>
      <title>您将如何使用很少的数据来为编程语言进行RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijujzz/how_would_you_go_about_doing_rl_for_a_programming/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  如果说我可以编译代码以使用错误作为奖励的一部分，那么训练LLM的最佳方法是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/new_description8537      [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ijujzz/how_would_you_go_go_about_doing_doing_rl_for_for_a_a_a_programming/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijujzz/how_would_you_go_about_doing_rl_for_a_programming/</guid>
      <pubDate>Fri, 07 Feb 2025 13:06:49 GMT</pubDate>
    </item>
    <item>
      <title>我们的RL框架将用于快速，进化HPO的任何网络/算法转换。我们应该使LLM可用于进化RL推理培训吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijr36h/our_rl_framework_converts_any_networkalgorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家，我们刚刚发布了agilerl v2.0！ 查看最新更新： https://github.com/agilerl/agilerl     agilerl是一个RL培训库，可实现任何网络的进化超参数优化。我们的基准测试比rllib的训练更快10倍。 这是我们添加的一些很酷的功能：  广义突变 - 一个完全模块化的，灵活的网络和RL的柔性突变框架超参数。  Evolvablenetwork API  - 在可演化的设置中使用任何网络架构，包括预验证的网络。  evolvableAlgorithm层次结构 - 简化的进化RL算法的实现。 EvolvableModule层次结构 - 跟踪复杂网络中突变的更聪明的方法。 支持复杂空间 - 使用EvolvableMultiinput无缝处理多输入空间。  知道是：我们应该将其完全扩展到LLM吗？当前大型型号的HPO并不是真正的，因为它们是如此难/昂贵。但是我们的框架可以使其更加有效。我已经意识到人们比较了用于在DeepSeek R0娱乐活动中获得更好结果的超参数的人，这意味着这可能很有用。我想知道您对进化HPO是否对培训大型推理模型有用的想法？而且，如果有人幻想有助于为这项工作做出贡献，我们会很喜欢您的帮助！谢谢   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nicku_a     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijr36h/our_rl_framework_converts_any_networkalgorithm/</guid>
      <pubDate>Fri, 07 Feb 2025 09:21:43 GMT</pubDate>
    </item>
    <item>
      <title>谁能帮助我（自定义env + SB3）？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ijektz/can_anyone_help_me_custom_env_sb3/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我创建了一个自定义的健身房环境，该环境与Java中的模拟器对话。基本上，它从光网络收集Infos。 OBS空间是拓扑，动作空间是一个路由和初始插槽，可以将流动分配。要处理的流是事件中断的流。每个事件都有一堆中断的流。我正在尝试训练一个代理商，为每条路线和插槽分配流程的每条流程做出明智的决策。一旦分配流动，拓扑就会发生变化，否则什么都不会改变。我正在使用SB3（DQN，MLPPOLICY），并将时间步骤设置为每个事件的流量数（这是必须这样做的方式，因为它与模拟器进行了对话）。问题是，当事件具有x流数时，Model.learn（）执行2或3个步骤，而不是流量数。它会引起混乱，因为模拟器试图处理新事件的新流程，但是它会从模型中重复流过。关于如何解决这个问题的想法？我可以共享代码和联系人，我真的需要解决此问题。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/leilaff89     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ijektz/can_anyone_help_me_custom_env_sb3/</guid>
      <pubDate>Thu, 06 Feb 2025 21:52:27 GMT</pubDate>
    </item>
    <item>
      <title>“多基因填充：与多种推理链的自我改进”，Subramaniam等2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij81ye/multiagent_finetuning_self_improvement_with/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/gwern     [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij81ye/multiagent_finetuning_self_improvement_with/</guid>
      <pubDate>Thu, 06 Feb 2025 17:27:45 GMT</pubDate>
    </item>
    <item>
      <title>用于定制演员和评论家网络的RL库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我希望测试Pytorch和基准指标中的自定义神经网络（即收敛速率）与Actor-Cricit-Critic RL算法中的标准MLP。我已经围绕着subreddit查看，并且已经建议使用以下库来实施此类网络：   rllib   rlpyt   skrl &lt;&gt; /li&gt;  torchrl   对这些意见有什么意见或良好的经验？我已经看到了对rllib的爱与恨，但在过去三个中没有太多的爱。我正在尝试避免SB3，因为我认为我的神经网络不属于他们拥有的任何自定义策略类别，除非我非常误解了他们的自定义策略类别的工作方式。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/voltimeters     [link]   ＆＃32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ij1v43/rl_libraries_for_customizing_actor_critic_networks/</guid>
      <pubDate>Thu, 06 Feb 2025 12:55:17 GMT</pubDate>
    </item>
    <item>
      <title>需要有关高级RL资源的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我现在已经深入了强化学习，但是我撞墙了。我发现的几乎每个课程或资源都涵盖了相同的内容 -  PPO，SAC，DDPG等。它们非常适合理解基础知识，但我感到卡住了。就像我只是在相同的算法周围盘旋而没有真正前进。 我试图弄清楚如何摆脱它并进入更高级或更新的RL方法。诸如遗憾最小化，基于模型的RL，甚至是多代理系统＆amp＆amp; HRL听起来令人兴奋，但我不确定从哪里开始。 其他人有这种感觉吗？如果您设法推动了这个高原，您是如何做到的？任何课程，论文甚至个人提示都将非常有帮助。 事先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/helpful-number1288     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</guid>
      <pubDate>Thu, 06 Feb 2025 08:15:09 GMT</pubDate>
    </item>
    </channel>
</rss>