<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 23 Mar 2024 12:24:43 GMT</lastBuildDate>
    <item>
      <title>PPO 价值损失立即收敛，而保单损失则陷入困境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1blqs2g/ppo_value_loss_converges_immediately_while_the/</link>
      <description><![CDATA[      我正在使用 SB3 训练 PPO 自定义环境。从张量板上可以看出，价值损失很快收敛到几乎为 0，而保单损失似乎随着时间的推移而恶化。此外，熵损失似乎被卡住了。  ​ https://preview.redd.it/qgcxv85ks2qc1.png?width=1648&amp;format=png&amp;auto=webp&amp;s=0d1609d756f42a0f2372b4006836d3eb4d4 e3744 但是，同时，奖励不断增加，但测试成绩却越来越差！  https://preview.redd .it/4vv051myr2qc1.png?width=556&amp;format=png&amp;auto=webp&amp;s=a1fe9ca87349d14c918bcb8e68766db6b02a3425 您能帮我了解问题出在哪里以及如何解决吗？  当前超参数： initial_learning_rate = 0.000005 model = MaskablePPO(MaskableActorCriticPolicy, env,tensorboard_log=&quot;./tensorboard&quot; ,n_steps=1024,learning_rate=initial_learning_rate,ent_coef=0.005)    由   提交 /u/Acceptable_Egg6552   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1blqs2g/ppo_value_loss_converges_immediately_while_the/</guid>
      <pubDate>Sat, 23 Mar 2024 12:15:32 GMT</pubDate>
    </item>
    <item>
      <title>尝试稳定的复古</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bl8ol6/experimenting_with_stable_retro/</link>
      <description><![CDATA[我最近一直在尝试复古稳定，它很复杂但很酷。它使用体育馆作为环境的基础和代理的稳定基线3，并在此基础上提供一个界面，以便人们可以使用 8/16 位时代的视频游戏。 首先整个包装系统看起来很复杂，因为你可以结合gymnasium和sbl3课程。更糟糕的是，网上有关于这些库的旧版本的旧文档，例如gym、gym-retro 和 stablebaselines。但是一旦你掌握了它，潜力是巨大的。 例如，我一直在用各种游戏训练不同的模型，即使使用 DQN 也有可能取得良好的结果。这并不总是容易的，但这是可能的。一种方法是跳帧，大约 10 帧左右。这可以加快训练速度并在比赛中产生良好的效果。特别是与堆叠几帧相结合，3 个就足够了。 还有一些未记录的功能可以在场景的 json 文件中配置，例如输入图像的裁剪或有效动​​作，但后者似乎无法正常工作，因此我发现创建一个限制操作并将其转换为代理自己的空间的包装器更有效。   由   提交/u/deney-kedeip    reddit.com/r/reinforcementlearning/comments/1bl8ol6/experimenting_with_stable_retro/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bl8ol6/experimenting_with_stable_retro/</guid>
      <pubDate>Fri, 22 Mar 2024 20:04:49 GMT</pubDate>
    </item>
    <item>
      <title>RL 热启动与 IL（SAC、PPO）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bl4r3x/rl_hot_start_with_il_sac_ppo/</link>
      <description><![CDATA[您好，我目前正在尝试创建一个社交导航环境，其中机器人试图达到目标，并且必须避免与障碍物和墙壁碰撞。  问题变得越来越复杂，尽管我尝试调整超参数，但我的 PPO 收敛得不太好。  Si我决定使用模仿学习来“热启动”。强化学习算法。我已经使用 MPC 生成了轨迹，并找到了一个用于 SAC 离线训练的库（ https://github.com/takuseno/d3rlpy ）工作正常，但我真的不知道如何将权重转移到 SAC 策略的网络，因为一切都已完成 我的问题是：解决我的初始问题的最佳方法是什么，如果 IL +强化学习是一个很好的方法，你们有这样做的资源或代码吗？对于连续空格 谢谢   由   提交/u/Ybrik410  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bl4r3x/rl_hot_start_with_il_sac_ppo/</guid>
      <pubDate>Fri, 22 Mar 2024 17:23:08 GMT</pubDate>
    </item>
    <item>
      <title>帮助跟踪权重和偏差的稳定基线的 TD 误差？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkyiwu/help_tracking_the_td_error_from_stable_baslines/</link>
      <description><![CDATA[嗨！我一生都无法弄清楚如何跟踪从 SB3 DQN 到权重和偏差的 TD 误差。有人成功做到了这一点吗？ 谢谢！   由   提交/u/bean_the_great   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkyiwu/help_tracking_the_td_error_from_stable_baslines/</guid>
      <pubDate>Fri, 22 Mar 2024 12:53:31 GMT</pubDate>
    </item>
    <item>
      <title>强化学习理论图景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bky3ej/landscape_of_theoretical_reinforcement_learning/</link>
      <description><![CDATA[嗨，我即将开始撰写强化学习数学硕士论文。我只是想知道 RL 的理论前景是什么样的。我们仍试图回答哪些重大问题？是否有一些特别有前景的特定领域？是否有可能与统计力学或最佳传输或其他看起来有前途的数学领域有联系？ 谢谢！    ;由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bky3ej/landscape_of_theoretical_reinforcement_learning/</guid>
      <pubDate>Fri, 22 Mar 2024 12:30:44 GMT</pubDate>
    </item>
    <item>
      <title>DeepRL 代理与 Ken 一起完成《街头霸王 III》！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkxoho/deeprl_agent_completing_street_fighter_iii_with/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkxoho/deeprl_agent_completing_street_fighter_iii_with/</guid>
      <pubDate>Fri, 22 Mar 2024 12:07:43 GMT</pubDate>
    </item>
    <item>
      <title>环境初始状态S0</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkwxnu/initial_state_of_environment_s0/</link>
      <description><![CDATA[我正在使用 ppo，我的环境每集只有 1 个操作，现在我的疑问是关于初始状态 S0，它是如何确定的？我的状态是错误、微分错误和积分错误。 编辑：我尝试仅运行一集两次，环境略有变化，我可以看到代理采取的初始操作始终相同，因此这意味着两种情况下初始状态 S0 也相同？    由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkwxnu/initial_state_of_environment_s0/</guid>
      <pubDate>Fri, 22 Mar 2024 11:24:32 GMT</pubDate>
    </item>
    <item>
      <title>需要 DDQN 自动驾驶汽车项目的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</link>
      <description><![CDATA[      我最近开始学习强化学习，我使用ddqn做了一个自动驾驶汽车项目，输入是这些光线的长度输出是向前、向后、向左、向右，什么都不做。我的问题是 rl Agent 需要多少时间来学习？即使已经播出了 40 集，它仍然没有达到奖励门槛。我还根据前进速度给予 0-1 奖励   由   提交/u/Invicto_50  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkoq06/need_help_with_ddqn_self_driving_car_project/</guid>
      <pubDate>Fri, 22 Mar 2024 02:31:00 GMT</pubDate>
    </item>
    <item>
      <title>“RewardBench：评估语言建模的奖励模型”，Lambert 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkmtdy/rewardbench_evaluating_reward_models_for_language/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkmtdy/rewardbench_evaluating_reward_models_for_language/</guid>
      <pubDate>Fri, 22 Mar 2024 00:56:53 GMT</pubDate>
    </item>
    <item>
      <title>处理 Deep Q 网络中不同大小的状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bkj8ve/dealing_with_states_of_varying_size_in_deep_q/</link>
      <description><![CDATA[问候， ​ 我是强化学习新手，我决定用 Python 制作一个简单的贪吃蛇游戏，这样我就可以训练 DQN 代理来玩它。在游戏的状态表示中，我传递给它的变量之一是一个包含所有 Snake 当前位置的列表（即，Snake 主体占据的每个位置都有一个元组 (x,y)）。在训练中，一旦 Snake 吃掉食物颗粒并长大，代理总是会崩溃，因为状态大小与初始值不同。 ​ 我在互联网上搜索了解决这个问题的方法。  一种解决方案是在状态上仅表示蛇的头，并添加四个变量来判断是否有上/下、左/右障碍物。这个解决方案似乎并没有捕获所有的基本信息，所以我怀疑代理即使训练了数千年也无法发挥最佳性能。  另一个解决方案是将蛇的身体表示为长度等于其最大可实现大小的列表，这确实捕获了所有基本信息，但如果我将地图大小增加到大值，可能会减慢该过程. ​ 我想知道，有没有办法处理深度 Q 网络中不同大小的状态？给代理的初始状态大小是否定义了所有后续状态的大小？   由   提交 /u/Clovergheister   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bkj8ve/dealing_with_states_of_varying_size_in_deep_q/</guid>
      <pubDate>Thu, 21 Mar 2024 22:18:15 GMT</pubDate>
    </item>
    <item>
      <title>是否有基于 JAX 的 Arcade 学习环境实现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bka5nu/are_there_any_jaxbased_implementations_of_arcade/</link>
      <description><![CDATA[Arcade 学习环境有基于 JAX 的实现吗？我想使用 PureJaxRL 在 GPU 上加速 ALE 环境，例如 Gymnax/BRAX 环境？   由   提交/u/C7501  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bka5nu/are_there_any_jaxbased_implementations_of_arcade/</guid>
      <pubDate>Thu, 21 Mar 2024 16:07:42 GMT</pubDate>
    </item>
    <item>
      <title>SAC实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk944r/sac_implementation/</link>
      <description><![CDATA[嗨，我正在尝试在没有价值网络的情况下实现 Soft Actor Critic 算法，我想我已经接近并相信我的所有梯度都是是的，但我找不到问题。  我按照 spin-up 的解释阅读了原文。我的代码受到 youtube 频道“与 Phil 的机器学习”的启发 ​ 我尝试在 Pendulum-v1 环境上运行它，但它不收敛并且最终总是选择相同的操作。 有人可以帮助我处理我的代码吗？  def learn(self): if self.memory.mem_cntr &lt; p&gt;​ class ActorNetwork(keras.Model): def init(self, n_actions, Noise=1e-6): super(ActorNetwork, self).init() self. n_actions = n_actions self.noise = 噪声 self.fc1 = Dense(400, 激活=“relu”) self.fc2 = Dense(200, 激活=“relu”) self.mu = Dense(n_actions, 激活=无) self.sigma = Dense(n_actions,activation=None) def call(self, state): value = self.fc1(state) value = self.fc2(value) mu = self.mu(value) sigma = tf.clip_by_value( self.sigma(value), self.noise, 1) return mu, sigma def sample_normal(self, state): mu, sigma = self.call(state) # print(tf.reduce_mean(mu).numpy(), tf .reduce_mean(sigma).numpy()) action = tf.random.normal(mu.shape, mu, sigma, dtype=tf.float32) # log_prob = tf.math.log(tf.math.exp(-0.5 * tf.math.pow((action - mu) / sigma, 2)) / (sigma*tf.math.sqrt(2*NP_PI))) log_prob = -0.5 * tf.math.pow((action-mu)/ sigma, 2) - tf.math.log(sigma*tf.math.sqrt(2*NP_PI)) # 只是高斯分布的简化 log_prob -= tf.math.log(1-tf.math.square(tf.math) .tanh(action))) # 请参阅 Haarnoja2019“附录 C 执行操作边界”返回操作，log_prob    由   提交 /u/antobom   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk944r/sac_implementation/</guid>
      <pubDate>Thu, 21 Mar 2024 15:23:11 GMT</pubDate>
    </item>
    <item>
      <title>斯瓦亚特机器人 |印度 |极其动态复杂的交通动态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bk80o9/swaayatt_robots_india_extremely_dynamiccomplex/</link>
      <description><![CDATA[       由   提交/u/shani_786  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bk80o9/swaayatt_robots_india_extremely_dynamiccomplex/</guid>
      <pubDate>Thu, 21 Mar 2024 14:36:58 GMT</pubDate>
    </item>
    <item>
      <title>Rich Sutton 今天来参加我们的 Zoom 讲座。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bjvzar/rich_sutton_came_to_our_zoom_lecture_today/</link>
      <description><![CDATA[       由   提交/u/chunchblooden  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bjvzar/rich_sutton_came_to_our_zoom_lecture_today/</guid>
      <pubDate>Thu, 21 Mar 2024 02:35:28 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>