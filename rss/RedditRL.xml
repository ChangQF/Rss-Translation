<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 16 Jun 2024 15:15:24 GMT</lastBuildDate>
    <item>
      <title>在开始 RL 部分之前，如何设计自定义环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh5xnb/how_do_i_design_a_custom_environment_before/</link>
      <description><![CDATA[嘿，我只是想知道如何创建自定义 RL 环境。Youtube 视频通常只选择现成的自定义环境（例如 sentdex 在他的教程中选择了蛇游戏）。在开始担心 RL 的其他方面（如观察和动作空间、奖励函数等）之前，我想了解如何创建自己的环境。任何帮助都将不胜感激。 我需要数据集或任何东西来创建我的自定义环境吗？或者说如果我有布局的坐标，我可以使用它吗？    提交人    /u/Strange-Durian3382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh5xnb/how_do_i_design_a_custom_environment_before/</guid>
      <pubDate>Sun, 16 Jun 2024 11:52:34 GMT</pubDate>
    </item>
    <item>
      <title>你们正在创造上帝</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh51vm/you_guys_are_building_god/</link>
      <description><![CDATA[https://youtu.be/AfQxyVuLeCs?si=8nX4PdLV3f5yajzc 所有抽象都会消失？机器之神会直接完成工作吗？    提交人    /u/Logical_Jaguar_3487   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh51vm/you_guys_are_building_god/</guid>
      <pubDate>Sun, 16 Jun 2024 10:54:06 GMT</pubDate>
    </item>
    <item>
      <title>现实生活中的形状值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dh3al6/shapely_values_in_rl/</link>
      <description><![CDATA[有人可以建议如何在环境（如 cartpole 和 mountain car）上实现 shapely 值吗？任何库都会有所帮助。    提交人    /u/MarionberryVisual911   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dh3al6/shapely_values_in_rl/</guid>
      <pubDate>Sun, 16 Jun 2024 08:42:51 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络训练人工智能驾驶</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgzw93/training_an_ai_to_drive_using_neural_network/</link>
      <description><![CDATA[        提交人    /u/Flimsy_Roll_5666   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgzw93/training_an_ai_to_drive_using_neural_network/</guid>
      <pubDate>Sun, 16 Jun 2024 04:42:08 GMT</pubDate>
    </item>
    <item>
      <title>哪个 RL 库最好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgys73/which_rl_library_is_best/</link>
      <description><![CDATA[我们正在寻求为一个项目实现一个自定义的 RL 环境。该环境相当复杂，涉及机场滑行布局，以分析飞机的滑行路线。对此有几个问题 -   哪个框架最适合这个？我们已经尝试使用 OpenAI Gym 的 stable-baselines3，但感觉非常受限。还看到了一些其他 RL 库，如 Acme、Ray (Rllibs) 等。 上述所有库是否都支持自定义环境，以及它对用户的友好程度如何？     提交人    /u/Strange-Durian3382   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgys73/which_rl_library_is_best/</guid>
      <pubDate>Sun, 16 Jun 2024 03:32:55 GMT</pubDate>
    </item>
    <item>
      <title>“选择的单位和级别”，SEP</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgxtco/units_and_levels_of_selection_sep/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgxtco/units_and_levels_of_selection_sep/</guid>
      <pubDate>Sun, 16 Jun 2024 02:34:58 GMT</pubDate>
    </item>
    <item>
      <title>“人工智能搜索：更惨痛的教训”，麦克劳林（回顾 Leela Zero 与 Stockfish 的较量，以及解决法学硕士问题后钟摆摆回搜索方向）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgxmnj/ai_search_the_bitterer_lesson_mclaughlin/</link>
      <description><![CDATA[       由    /u/gwern  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgxmnj/ai_search_the_bitterer_lesson_mclaughlin/</guid>
      <pubDate>Sun, 16 Jun 2024 02:23:47 GMT</pubDate>
    </item>
    <item>
      <title>“新兴世界表征：探索在综合任务上训练的序列模型”，li 等人，2022 年（Othello GPT 从动作中学习游戏的世界模型）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgw9ak/emergent_world_representations_exploring_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgw9ak/emergent_world_representations_exploring_a/</guid>
      <pubDate>Sun, 16 Jun 2024 01:05:18 GMT</pubDate>
    </item>
    <item>
      <title>“将价值迭代网络扩展至 5000 层以实现超长期规划”，Wang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dguw5x/scaling_value_iteration_networks_to_5000_layers/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dguw5x/scaling_value_iteration_networks_to_5000_layers/</guid>
      <pubDate>Sat, 15 Jun 2024 23:50:16 GMT</pubDate>
    </item>
    <item>
      <title>“语言模型能否充当基于文本的世界模拟器？”，Wang 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgoskw/can_language_models_serve_as_textbased_world/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgoskw/can_language_models_serve_as_textbased_world/</guid>
      <pubDate>Sat, 15 Jun 2024 18:51:25 GMT</pubDate>
    </item>
    <item>
      <title>训练机器人在 MJX 中表演足球技巧</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgonk9/train_a_robot_to_do_football_tricks_in_mjx/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgonk9/train_a_robot_to_do_football_tricks_in_mjx/</guid>
      <pubDate>Sat, 15 Jun 2024 18:44:47 GMT</pubDate>
    </item>
    <item>
      <title>“安全性协调不应只局限于几个代币”，Qi 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgkkr3/safety_alignment_should_be_made_more_than_just_a/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgkkr3/safety_alignment_should_be_made_more_than_just_a/</guid>
      <pubDate>Sat, 15 Jun 2024 15:33:03 GMT</pubDate>
    </item>
    <item>
      <title>人类生物的现实生活...足够安全吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgkafm/rl_for_humanoids_safe_enough/</link>
      <description><![CDATA[      看看这个视频哈哈 - 你无法像其他机器人那样仅使用紧急停止来处理故障。有希望用 RL 解决这个问题吗？ [来源：https://x.com/_wenlixiao/status/1801808951601705258?t=PyYeg362j-mzZkb73NkwKQ&amp;s=19 和 https://x.com/_wenlixiao/status/1801305252760850903?t=S2KzQzXigYI4zyOqaSydXA&amp;s=19 ]    提交人    /u/Boring_Focus_9710   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgkafm/rl_for_humanoids_safe_enough/</guid>
      <pubDate>Sat, 15 Jun 2024 15:19:43 GMT</pubDate>
    </item>
    <item>
      <title>即使参与者损失的负面影响不断增加，PPO 代理仍在学习。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgigr9/ppo_agent_is_learning_even_if_the_negative_of/</link>
      <description><![CDATA[      即使整体目标函数最初是最大化而不是最小化，我的 PPO 代理也会学习。参与者损失和总体目标函数首先增加，然后减少，最后趋于零。在整个过程中，它一直在学习它想要学习的东西。评论家损失和熵正在最小化（正如预期的那样）。原因可能是什么？附注：我知道参与者损失应该最大化，但我说的是参与者损失的负值，理想情况下应该使用 ADAM 优化器将其最小化，但事实并非如此。    提交人    /u/Low-Advertising-1892   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgigr9/ppo_agent_is_learning_even_if_the_negative_of/</guid>
      <pubDate>Sat, 15 Jun 2024 13:52:55 GMT</pubDate>
    </item>
    <item>
      <title>2024 年最好的强化学习算法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dgdtp2/what_is_the_best_reinforcement_learning_algorithm/</link>
      <description><![CDATA[自 PPO 引入以来，似乎没有出现任何突破性的 RL 算法。    提交人    /u/galaxy_hu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dgdtp2/what_is_the_best_reinforcement_learning_algorithm/</guid>
      <pubDate>Sat, 15 Jun 2024 09:02:26 GMT</pubDate>
    </item>
    </channel>
</rss>