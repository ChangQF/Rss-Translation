<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sun, 06 Apr 2025 03:34:15 GMT</lastBuildDate>
    <item>
      <title>将LLM申请视为POMDP，而不是代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jsalgg/think_of_llm_applications_as_pomdps_not_agents/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/bianconi     ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jsalgg/think_of_llm_applications_as_pomdps_not_agents/</guid>
      <pubDate>Sat, 05 Apr 2025 18:50:37 GMT</pubDate>
    </item>
    <item>
      <title>用于雷达障碍的多代理模式复制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jsaa1m/multiagent_pattern_replication_for_radar_jamming/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  序言，我对RL非常新，以前曾处理过CV。我正在雷达干扰空间中处理MARL问题。它涉及多个雷达，例如它们中的n在以模式的方式同时传输m频率（每个选项）。每个雷达的模式是为每个情节的随机初始初始初始初始初始初始初始初始初始初米的。 代理的任务是检测和复制此模式，以便成功地“障碍”。从本质上讲，这是一个多种模式复制问题。 我已经将其建模为一个可观察到的问题，每个代理都看到其动作对上一步中堵塞的雷达的影响，以及其他每个代理的动作（但没有效果）。代理选择一个雷达的频率到堵塞，并且堵塞带宽内的相邻频率也被堵塞了。动作和观测值都是具有多个离散值的嵌套数组。插曲的上限为1000个步骤，而模式为12个步骤（目前）。 我使用的是带有RMSPROP的DRQN，所有具有自己独立的重播缓冲区的代理共享的模型参数。重播缓冲液存储的发作序列，其长度大于重复模式，而重复模式均匀地采样。 代理在堵塞雷达传输的频率时会得到奖励，而雷达不会被任何其他药物堵塞。如果他们堵塞了错误的频率，或者如果多个雷达堵塞了相同的频率。 我正在通过每集中雷达传输的所有频率的百分比来测量代理的成功。 我遇到的问题是，我遇到的问题似乎没有学习任何东西。性能似乎是随机的，并且随着时间的流逝而降解。 解决问题的可能方法是什么？我试图使DRQN更深入，并调整奖励价值，这是没有成功的。是否有更好的序列采样方法更适合部分可观察到的多代理设置？观察空间需要调整吗？我的问题是否太随机了，我应该简化它吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/chaoticgood69     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jsaa1m/multiagent_pattern_replication_for_radar_jamming/</guid>
      <pubDate>Sat, 05 Apr 2025 18:36:51 GMT</pubDate>
    </item>
    <item>
      <title>新的在线强化学习聚会（论文讨论）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1js3njy/new_online_reinforcement_learning_meetup_paper/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我打算组建一个新的在线（不和谐）聚会，重点是加强学习论文讨论。它对对该领域感兴趣的每个人都开放，该计划是让一个人出示论文，小组讨论 /提出问题。 If you&#39;re interested, you can sign up (free), and as soon as enough people are interested, you&#39;ll get an invitation. More information: https://max-we.github.io/R1/ I&#39;m looking forward to seeing you at the meetup!  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/npoes     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1js3njy/new_online_reinforcement_learning_meetup_paper/</guid>
      <pubDate>Sat, 05 Apr 2025 13:38:12 GMT</pubDate>
    </item>
    <item>
      <title>我仍然需要帮助。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1js1l0g/i_still_need_help_with_this/</link>
      <description><![CDATA[https://www.reddit.com/r/reinforcementlearning/s/MhhJu9XcXw &lt;!-- sc_on-&gt;＆＃32;提交由＆＃32; /u/u/dizzy-importance9208     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1js1l0g/i_still_need_help_with_this/</guid>
      <pubDate>Sat, 05 Apr 2025 11:43:39 GMT</pubDate>
    </item>
    <item>
      <title>RL工程师作为新鲜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrzzp5/rl_engineer_as_a_fresher/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我只是想在这里问，有人对如何从更新鲜的强化学习中做出职业有任何想法吗？在上下文中，我很快就会得到MTECH，但是我看不到很多工作专注于RL（任何形式）。我应该重点关注的任何指针都将完全欢迎！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1jrzzp5/rl_engineer_as_a_a_fresher/”&gt; [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrzzp5/rl_engineer_as_a_fresher/</guid>
      <pubDate>Sat, 05 Apr 2025 09:55:10 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：带宽分配的RL（1个月，无RL背景）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrz124/need_help_rl_for_bandwidth_allocation_1_month_no/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，我正在研究一个项目，我需要在其中应用强化学习，以优化如何根据其要求的带宽将带宽分配给网络中的用户。目标是建立一个RL模型，该模型学会比传统的基线方法更有效地分配带宽。奖励功能基于RL模型的分配比（分配/请求）与基线的分配比率之间的差异。  catch：我没有先前的RL经验，只有1个月的时间来完成此操作 - 模型培训，超级参数调整和评估。设计环境吗？ 有什么技巧来制作有效的奖励功能？ 我应该使用稳定的baselines3还是尝试自己编码PPO？ 如果您在我的鞋子里，您会怎么做？    有什么建议或资源会得到超级感激。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/klutzy-confusion-542      [links]      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1jrz124/need_help_rl_for_for_bandwidth_allocation_1_month_no/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrz124/need_help_rl_for_bandwidth_allocation_1_month_no/</guid>
      <pubDate>Sat, 05 Apr 2025 08:41:18 GMT</pubDate>
    </item>
    <item>
      <title>人形机器人无法站立，而是坐着。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrxusc/humanoid_robot_is_not_able_to_stand_but_sit/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;   i用囊algrithm测试了Mujoco人类站立环境，但是机器人能够坐着并且无法站立，坐着后无法忍受。可能的原因是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/dizzy-importance9208      &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/1jrxusc/humanoid_robot_is_is_not_to_to_to_to_to_stand_but_sit/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrxusc/humanoid_robot_is_not_able_to_stand_but_sit/</guid>
      <pubDate>Sat, 05 Apr 2025 07:14:24 GMT</pubDate>
    </item>
    <item>
      <title>我应该从头开始编码整个RL算法还是使用库等稳定的基础？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrw0yr/should_i_code_the_entire_rl_algorithm_from/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  何时从头开始实现算法以及何时使用现有库？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/dizzy-importance9208     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrw0yr/should_i_code_the_entire_rl_algorithm_from/</guid>
      <pubDate>Sat, 05 Apr 2025 05:10:12 GMT</pubDate>
    </item>
    <item>
      <title>Tetris AI帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrpbgi/tetris_ai_help/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家都再次是我，所以我在AI方面取得了一些进展，但是我需要别人对Epsilon衰减和学习过程的看法。它的全部包含，任何人都可以独立运行它，因此，如果您可以查看并获得一些建议，我将非常感谢它。谢谢   tetris ai      &lt;！ -  sc_on- sc_on-&gt; 32;提交由＆＃32; /u/u/disastrous yare3441     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrpbgi/tetris_ai_help/</guid>
      <pubDate>Fri, 04 Apr 2025 23:03:44 GMT</pubDate>
    </item>
    <item>
      <title>关于VPO算法中的参数更新</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrlhlf/about_parameter_update_in_vpo_algorithm/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有人可以帮助我更好地理解策略梯度的基本概念吗？我了解到它是基于此   https://paperswithcode.com/method/method/method/reinforce       and theta and theta and theta and theta and theta and theTa nyta and theta and the theta nyta and the theta ny theta and the theTa。它是具有标量值的向量，矩阵还是一个变量？如果不是标量，则方程应具有相对于theta的每个元素的部分推导而具有更清晰的表达。  ，如果是这样，更令人困惑的是当我们更新theta时，请考虑t，s_t，a_t，t值。它是否从每个可能的S_T开始？那t呢？应该减少还是固定常数？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/best_fish_2941     [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrlhlf/about_parameter_update_in_vpo_algorithm/</guid>
      <pubDate>Fri, 04 Apr 2025 20:13:58 GMT</pubDate>
    </item>
    <item>
      <title>需要软AC RL帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jrgdpe/need_help_with_soft_ac_rl/</link>
      <description><![CDATA[https://github.com/km784/AC- Hi all, I am a 3rd year student trying to make an Actor critic policy with neural networks to create a value approximation function.我试图解决的问题是使用RL来优化微电网的成本节省。目前，我正在尝试实施一种正在起作用的演员评论家方法，但是它并不符合最佳政策。如果有人可以帮助这一点（上面的链接）将不胜感激。  我目前正在努力为论文选择结尾主题，因为我想比较一个表格Q学习功能，该功能我成功地完成了该功能，而该功能与价值近似功能相比，以最大程度地减少PV电池系统中的关税成本。在RL中，任何人都可以在这个领域中探索任何其他想法。如果有人可以帮助我使用此价值近似模型，这将非常感谢。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/odd-endrepreneur6453      [link]       [commist   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jrgdpe/need_help_with_soft_ac_rl/</guid>
      <pubDate>Fri, 04 Apr 2025 16:39:31 GMT</pubDate>
    </item>
    <item>
      <title>这里有人有PPO步行机器人的经验吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jra684/anyone_here_have_experience_with_ppo_walking/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在研究我的毕业论文，但是我很难应用PPO来使我的机器人学会走路。谁能给我一些提示或一些帮助？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/savictor3963     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jra684/anyone_here_have_experience_with_ppo_walking/</guid>
      <pubDate>Fri, 04 Apr 2025 12:02:53 GMT</pubDate>
    </item>
    <item>
      <title>是什么导致我的PPO代理在训练期间突然下降到0？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jr7e9s/what_could_be_causing_the_performance_of_my_ppo/</link>
      <description><![CDATA[    src =“ https://preview.itd.it/9om0y7nh8sse1.png？ PPO代理在训练期间突然下降到0？” /&gt;   ＆＃32;提交由＆＃32; /u/u/ttocs167      [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jr7e9s/what_could_be_causing_the_performance_of_my_ppo/</guid>
      <pubDate>Fri, 04 Apr 2025 08:59:09 GMT</pubDate>
    </item>
    <item>
      <title>对RL建立牢固理解的课程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1jr5fex/course_for_developing_a_solid_understanding_of_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我的目标是进行研究。 我正在寻找一门很好的课程，以对RL进行良好的理解，以舒适地阅读论文并开发。 我是在Balaraman（来自Nptel IIT）或数学基础上的Balararaman（来自Nptel IIT）的强化课程之间的 ，或者通过 我正在考虑将莱文或大卫·西尔弗（David Silver）作为第二个课程。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lecholax     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1jr5fex/course_for_developing_a_solid_understanding_of_rl/</guid>
      <pubDate>Fri, 04 Apr 2025 06:34:27 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>