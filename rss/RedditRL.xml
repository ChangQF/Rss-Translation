<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Sun, 23 Feb 2025 12:30:06 GMT</lastBuildDate>
    <item>
      <title>解决迷宫的RL代理：疑问</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw7g65/rl_agent_for_solving_mazes_doubts/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好。我将毕业于CS，并想创建一个关于在统一的沙盒环境中进行的关于迷宫式解决方案的强化学习的论文项目。我对AI和相关主题有基本知识，但是我对自己的首发想法有一些疑问。 我想在统一环境中进行强化学习的项目，重点关注代理商的发展能够解决迷宫。给定简单的迷宫，代理应该能够在其中导航并在最短的时间内到达出口。团结将作为代理商的测试环境。迷宫是由用户通过专用编辑器构建的。创建后，用户可以将代理放置在起点并定义奖励和惩罚权重，并根据这些参数训练AI。可以保存训练的模型，在新的迷宫上进行测试或通过不同的设置进行重新训练。  是否可以训练能够解决具有可变起点和出口的不同迷宫的好代理？也许程序中的变量不应该是这两个点，而是迷宫中的内容（例如障碍）或目标（而不是退出迷宫，而是要收集尽可能多的硬币） 您认为这个项目太雄心勃勃，无法在3个月内完成？ 与RL代理相比，A*算法是可以解决所有迷宫的算法。是真的吗？有什么区别？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/asiiaiapiazza     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw7g65/rl_agent_for_solving_mazes_doubts/</guid>
      <pubDate>Sun, 23 Feb 2025 10:41:23 GMT</pubDate>
    </item>
    <item>
      <title>学习政策以最大化满足B</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试学习一个控制策略，该策略在确保条件B时最大化变量。例如，机器人在将速度保持在给定范围内（b）的同时最大化能源效率（a）。 我的想法：将奖励定义为a *（b）。当B被满足B时，奖励将为= A，并且在违反B时为= 0。但是，这可能会在培训的早期引起稀疏的回报。我可能会使用模仿学习来初始化策略来帮助解决此问题。 是否有适合此类问题的现有框架或技术？我非常感谢任何方向或相关关键字！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iw3ijl/learning_policy_to_to_maximize_a_a_a_a_while_satisfying_b/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</guid>
      <pubDate>Sun, 23 Feb 2025 06:05:48 GMT</pubDate>
    </item>
    <item>
      <title>Gridworld RL培训：情节的奖励不会改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivx8mj/gridworld_rl_training_rewards_over_episodes/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivx8mj/gridworld_rl_training_rewards_over_episodes/</guid>
      <pubDate>Sun, 23 Feb 2025 00:20:14 GMT</pubDate>
    </item>
    <item>
      <title>博客：衡量政策梯度的理论观点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我在这里很新，很抱歉，如果它不符合规则（我找不到任何规则），但是我想与您分享我的博客，以衡量政策梯度的理论观点，我介绍了我们如何利用Raadon-Nikodym derivative不仅可以得出标准增强，还可以得出一些以后的版本，以及我们如何使用占用度量作为轨迹采样的倒入替代品。希望您能享受并给我一些反馈，因为我喜欢在RL  中分享直觉的重大解释，这是链接： https：//myxik.github.io/posts/measuretheoretic-view/    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforeverctionlearning/comments/1ivwzw9/blog_measure_theoretod_theoretic_theoretic_on_policy_gradients/”&gt; [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</guid>
      <pubDate>Sun, 23 Feb 2025 00:08:21 GMT</pubDate>
    </item>
    <item>
      <title>对于简单的MDP，此奖励值是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivu7uf/does_this_reward_values_makes_sense_for_a_simple/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！ 我正在尝试解决MDP，我为其定义了以下奖励，但是我有一个很难用价值迭代解决它。看来，国家价值功能不会收敛，经过一些迭代后，它将不再改善。所以，我在想也许问题是我的奖励结构？因为它有很大的变化。您认为这可能是一个原因吗？   r1 = {x1＆quord&#39;：500，&#39;x2＆quot” x2＆quot;：300，＆quort&#39;x3＆quot x3＆quort&#39;：100} r_2 = 1 r3 = -100 = -100 r4 = {x1＆quot;：-1000，x2＆quot;：-500，x3＆quort;：-200}    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/using_cauliflower320     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivu7uf/does_this_reward_values_makes_sense_for_a_simple/</guid>
      <pubDate>Sat, 22 Feb 2025 21:57:20 GMT</pubDate>
    </item>
    <item>
      <title>在美国顶级大学中录取的博士学位需要什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivr24m/what_is_required_for_a_phd_admit_in_a_top_tier_us/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有兴趣在强化学习中申请15个博士学位课程，并想了解一般的入学统计和期望。我目前是Virginia Tech的硕士学生，在RL撰写研究论文，是研究生水平深度RL课程的TA，并在计算机视觉方面具有先前的研究经验。如何使我的个人资料脱颖而出？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ivr24m/what_is_is_required_for_for_for_a_phd_admit_in_a_a_a_top_top_tier_us/”&gt; [links]      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/comments/1ivr24m/what_is_is_required_for_a_phd_phd_admit_in_a_a_a_a_a_a_top_top_top_tier_us/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivr24m/what_is_required_for_a_phd_admit_in_a_top_tier_us/</guid>
      <pubDate>Sat, 22 Feb 2025 19:37:07 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA CULE：“启用CUDA的ATARI 2600模拟器，该模拟器直接在GPU内存中呈现框架”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivog6c/nvidia_cule_a_cuda_enabled_atari_2600_emulator/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/masterscrat     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ivog6c/nvidia_cule_a_cuda_cuda_enabled_atari_atari_2600_emulator/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivog6c/nvidia_cule_a_cuda_enabled_atari_2600_emulator/</guid>
      <pubDate>Sat, 22 Feb 2025 17:46:51 GMT</pubDate>
    </item>
    <item>
      <title>强化学习是实现AGI的关键吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivns8i/is_reinforcement_learning_the_key_for_achieving/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是新RL。我已经看到了深刻的纸，他们非常强调RL。我知道GPT和其他LLM使用RL，但深刻的寻求使其成为主要。因此，我想学习RL，因为我想成为一名研究人员。是我的结论甚至正确，请验证它。如果是真的，请建议我来源。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tarnatraining822     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivns8i/is_reinforcement_learning_the_key_for_achieving/</guid>
      <pubDate>Sat, 22 Feb 2025 17:19:29 GMT</pubDate>
    </item>
    <item>
      <title>学习级研究项目思想</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivmj63/learninglevel_research_project_ideas/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在我收到任何仇恨评论我的问题之前，我想提一下，我知道它不是正确的心态，请选择“轻松问题” ，但是ID喜欢在3个月的时间范围内进行RL研究项目，以暴露于研究界，并深入研究我喜欢的RL。这是一种曝光，我想从事的一种破冰者的工作，大约一个月前开始学习的领域。 我想对社区的想法有一些乞egine的想法 - 我们可以冒险进入并涉足的友好RL研究领域。完成此操作后，我最终将进入RL的其他分支。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/extension-economy-78     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivmj63/learninglevel_research_project_ideas/</guid>
      <pubDate>Sat, 22 Feb 2025 16:27:23 GMT</pubDate>
    </item>
    <item>
      <title>基于物理的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivlnhu/physicsbased_environments/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，有机机器人， 我正在在物理模拟领域开发一个个人项目，并理解，通过流体动力学或热扩散。我一直在考虑应用程序不仅是为了设计目的，而且我目前对RL的兴趣，我一直在探索使用这些模拟在这些领域训练控制器的想法，例如在湍流下改进飞机控制或对数据的最佳控制中心冷却系统。  在此引言中，我想了解是否需要这些类型的环境来培训行业中的RL算法。  和裸露的牢记，我知道需要从模拟到权衡速度和准确性的不同水平的忠诚度 - 也许初步的训练低忠诚度，然后过渡到高忠诚度，将是无缝的一个加号。 我很想知道您对此的想法和/或知道行业对这类问题的需求。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/navier-gives-strokes     [link]   ＆＃32;   [comment]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivlnhu/physicsbased_environments/</guid>
      <pubDate>Sat, 22 Feb 2025 15:49:31 GMT</pubDate>
    </item>
    <item>
      <title>RL解决多个机器人问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用在共享环境中导航的多个移动机器人进行模拟。每个机器人都有一个预先加载的空间地图，并使用范围传感器（例如飞行传感器的时间）进行本地化。最初的全球路径计划是为每个机器人独立完成的，而无需考虑其他机器人。一旦开始移动，他们就可以检测到附近的机器人位置，速度和计划的途径以避免碰撞。 问题是，在紧密的空间中，他们经常被困在一种僵局中。在没有机器人可以移动的地方，他们都互相阻挡。一个人可以很容易地看到，如果说，1个机器人会向前移动一点，另一个机器人向前移动并转动一点，其余的都可以清除。但是在基于规则的系统中编码这种逻辑非常困难。 我正在考虑使用ML/ RL来解决此问题，但是我想知道这是否是一种实用方法。有没有人尝试解决RL的类似问题？您将如何处理？很想听听您的想法。谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/”&gt; [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivgx5l/rl_to_solve_a_multiple_robot_problem/</guid>
      <pubDate>Sat, 22 Feb 2025 11:41:21 GMT</pubDate>
    </item>
    <item>
      <title>我如何学习新手的模型预测控制。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivgjbg/how_can_i_learn_model_predictive_control_as_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是对控制方案的新手。我有一项在倒摆上实施的MPC任务。我需要学习。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/on_yesterday_2539      [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivgjbg/how_can_i_learn_model_predictive_control_as_a/</guid>
      <pubDate>Sat, 22 Feb 2025 11:14:47 GMT</pubDate>
    </item>
    <item>
      <title>GRPO与进化策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iv6ui7/grpo_vs_evolution_strategies/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   grpo看起来不像（或可以从在这里？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/majestic-tap1577     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iv6ui7/grpo_vs_evolution_strategies/</guid>
      <pubDate>Sat, 22 Feb 2025 01:12:41 GMT</pubDate>
    </item>
    <item>
      <title>多机构学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iv2jbn/multiagent_learning/</link>
      <description><![CDATA[在，游戏理论（决策理论），信息理论和动力学＆amp;控制。但是，我正在努力在该领域绘制一个清晰的研究路线图。感觉仍然像是一个相对较新的领域，当我遇到麻省理工学院的课程 多基金会学习中的主题 Gabriele Farina （看起来很棒！），我不确定绝对必要的领域我需要首先加强。 有点关于我：   背景：动态系统＆amp;控制  当前重点：学习深入强化学习  其他兴趣：认知科学（尤其是学习＆amp;决策）; 以前的竞争乒乓球运动员  当前状态：机器人技术中的PhD学生，但对我当前的项目感到非常无聊并渴望探索多构成系统并在其中建立职业。   如果您冒险进入多重RL，您是如何构建学习路径的？您会说哪些领域对于该领域的研究最关键？如果您有类似的兴趣，我很想听听您的想法！ 谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neat_comparison_2726      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iv2jbn/multiagent_learning/</guid>
      <pubDate>Fri, 21 Feb 2025 21:40:49 GMT</pubDate>
    </item>
    <item>
      <title>RL在监督学习中？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iuqwr7/rl_in_supervised_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！ 我对DRL有一个问题。我已经看到了有关在“入侵检测”，“异常检测”，“欺诈检测”等任务中使用DRL的几篇论文标题和新闻典型的监督学习，尽管根据我所读过的“ DRL是一种很好的技术，对这种任务都有良好的结果”。检查例如 https://www.cyberdb.co/top-5-deep-learning-techniques-for-enhancing-cyber-theat-detection/#: text = deep%20Revermention%20Rections%20Learningmentimpearmentilem ％20学习％20 from％20 their％20环境  问题是，在这些情况下，更具体地说，国家及其进化是如何建立DRL问题？代理的操作是明确的（例如，将数据标记为异常，什么都不做或将其标记为普通数据），但是由于我们处理数据集或数据集的集合，因此这些数据是不变的，不是吗？在这些情况下，它如何可能或如何完成，以使DRL系统的状态随代理的行为而变化？这很重要，因为它是马尔可夫决策过程的关键属性，因此是DRL系统的关键属性，不是吗？ 非常感谢您  &lt;！ - -SC_ON-&gt;＆＃32;提交由＆＃32; /u/u/carpoforo     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iuqwr7/rl_in_supervised_learning/</guid>
      <pubDate>Fri, 21 Feb 2025 13:29:36 GMT</pubDate>
    </item>
    </channel>
</rss>