<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 24 Feb 2025 06:27:01 GMT</lastBuildDate>
    <item>
      <title>我应该选择什么研究问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrokr/what_research_problem_should_i_pick/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是RL的新手，但是我处于需要立即为研究的情况下选择一个好的问题陈述。我试图浏览会议的论文以快速选择一些东西。是否可以研究任何特定的问题陈述？我只是在寻找经验丰富的人的线索。谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrokr/what_research_problem_should_i_pick/</guid>
      <pubDate>Mon, 24 Feb 2025 02:41:32 GMT</pubDate>
    </item>
    <item>
      <title>目前最紧迫的问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrkev/most_pressing_problems_currently/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  目前RL中最紧迫的问题是什么？是否有任何方法可以显示出良好的解决这些方法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/uiaslight     [link]   ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrkev/most_pressing_problems_currently/</guid>
      <pubDate>Mon, 24 Feb 2025 02:35:40 GMT</pubDate>
    </item>
    <item>
      <title>RL对于AGI，重点应该放在什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  那些认为RL是通往AGI的可行途径的人，当前需要专注于在RL中求解的当前局限性是什么？人们可以选择为此做出哪些研究问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</guid>
      <pubDate>Mon, 24 Feb 2025 02:33:15 GMT</pubDate>
    </item>
    <item>
      <title>帮助尝试了解SARSA半梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwqh6f/help_on_trying_to_understand_sarsa_semi_gradient/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  嘿，大家， 我是ML/AI爱好者，RL一直是我忽略的一周。我发现该算法很难解密，但是在阅读了LLM Architecture背后的论文后，我注意到其中很多倾向于经常使用RL概念。这让我意识到这是一个我不能真正忽略的领域。 为此，我一直在慢慢地通过Barto和Sutton的书进行仔细观察，我可以免费在线找到这些书籍。目前，我正在第10章中，我希望在结束时我应该能够利用我的其他AI/ML项目的经验来制作一些AI来玩一些尚未有AI项目的游戏，例如Spelunky或PVZ英雄。 当我阅读每个部分时，为了确保我了解算法和动力，我尝试使用这本书建议的算法对婴儿问题进行编码。我遇到的近期是SARSA半梯度。    我制作了一个非常简单的游戏，灵感来自Openai Mountain Car游戏，相反，您实际上只需要ASCII即可代表州和地形。代理商从左侧的A点开始，目标是到达B点，这一直在右侧。在路径中，代理可能会遇到向前（/）或向后（\）的斜率。这些可以允许代理分别获得或失去动力。还应注意，代理商的汽车的发动机非常薄。走下坡路，汽车可以加速进一步的动力，但是如果上坡，发动机的功率为零。 目标是以零动量到达B点，以获得积极的奖励和最终状态。其他终端状态包括过早达到零动量或撞击地形末端。这辆车还因试图保持低位而获得奖励。 我的实施可以在此处找到： rl_concepts/rollingcar.ipynb在main· JJ8428/rl_concepts   我发布的原因是我的代理人并没有真正学习如何解决游戏。我不确定这是游戏设计不佳的情况，游戏是否太复杂而无法用一层权重解决，或者我对算法的实现是否错误。从网上浏览中，我看到人们已经解决了sarsa semi grad的OpenAi MountainCar问题，到目前为止尚无n步，所以我有信心也可以解决这个游戏。 可以解决有人请去看我的代码，告诉我是否不知所措？我的代码不长，任何帮助或指针都将不胜感激。如果我的代码超级凌乱且不可读，请告诉我。可悲的是，自从我在Python重新访问OOP以来已经很久了。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/apricotslight9728     [links]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwqh6f/help_on_trying_to_understand_sarsa_semi_gradient/</guid>
      <pubDate>Mon, 24 Feb 2025 01:40:10 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的RL：开环控制是亚最佳选择，因为..？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在观看Sergei Levine的讲座。他是一个很好的资源；将学习理论的联系起来很多。使用数学测试的类比，通过开环控制通过开环控制是亚最佳的。我想象这个类比就像搜索树一样例如，但即使如此，它也有点清除。但是，要与抽象的例子保持在一起，为什么该模型不会基于以前与环境互动的经验产生可能性？ Sergei提到，如果我们选择测试，我们将得到正确的答案，但也意味着没有办法将这些信息传递给模型（在这种情况下，代理商的决策者）。感觉从现实中消除了，即如果可能的测试尺寸足够大，那么最佳的动作就是回家。如果您对参加测试的能力有任何信心（例如以前的推出经验），那么您的最佳策略会更改，但这是信息，您可以通过与以前的示例相同的分布来理解。  也许我缺少标记。为什么开放环控制次优？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iwhd9t/model_based_rl_rl_openloop_iscontrol_is_suboptimal/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1iwhd9t/model_based_rl_rl_openloop_control_is_is_suboptimal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</guid>
      <pubDate>Sun, 23 Feb 2025 18:52:18 GMT</pubDate>
    </item>
    <item>
      <title>我的代码未显示Dyna-Q和Dyna-Q+算法之间的差异。请帮助修复它</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwdj4k/difference_between_dynaq_and_dynaq_algorithm_not/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  首先，我在此env www.github.com/VachanVY/Reinforcement-Learning/blob/main/images/shortcut_maze_before_Dyna-Q_with_25_planning_steps.gif到达目标的路线更长。 然后，我从这里乘坐Q阀来训练dyna-q + algo在修改后的env上，其中包含通往目标的较短途径证明当env发生变化时，dyna-q + 更好，但是在以下代码应用后，我认为应用Dyna-Q+ algo没有区别，应该采取较短的路径。  www.github.com/vachanvy/reinforeccation-learning/blob/main/images/shortcut_maze_after_dyna-q+_with_with_25_25_planning_steps.gifs.gif    我看不到它所采取的路线的任何变化，就像强化学习Sutton和Barto  的介绍时所说的那样，``````````````&#39; ，日志：bool = false，q_values = none，epsilon = epsilon）：plan = true如果 num_planning_steps&gt;0 else False if not plan: assert not dyna_q_plus q_values = init_q_vals(NUM_STATES, NUM_ACTIONS) if q_values is None else q_values env_model = init_env_model(NUM_STATES, NUM_ACTIONS) if plan else None last_visited_time_step = init_last_visited_times（num_states，num_actions）    sum_rewards_episodes = []; timeStep_episodes = [] total_step = 0 for Epistion（1，num_episodes+1）：状态，info = env.reset（）; sum_rewards = float（0）在计数（1）中的TSTEP：total_step += 1 action = sample_action（q_values [state]，epsilon）next_state，奖励，完成，截断，nofe = ency.step（action）; sum_rewards +=奖励q_values [state] [action] += alpha *（奖励 +gamma * max（q_values [next_state]）） -  q_values [state] [action] [action]）last_visited_time_step [state] [state] ：env_model [state] [action] =（奖励，next_state）＃（奖励，next_state）如果完成或截断：break状态= next_state sum_rewards_episodes.append（sum_rewards）timestep_episodes.append.append（tstep）如果log：print：print（f&#39;epsisode：&#39;epsisode：{ponvision} ||奖励总和{sum_rewards} ＆quot”＃计划是否计划：用于Planning_step in range（num_planning_steps）：planning_state = rando_prev_observed_state（last_visited_time_step）＃随机观察到的状态planning_action_action_action_action_action_action_action_action_action_action_for_for_state（env_model [plance_state] env_model [planning_state] [planning_action]如果dyna_q_plus：＃为了鼓励测试＃长途动作的行为，则在涉及＃这些动作的模拟体验上给出了特殊的“奖励奖励”。特别是，如果对过渡的建模奖励为R，并且在τ时间步骤中尚未尝试过过渡＃，则**计划更新**就像该过渡＃产生了R +κ*的奖励一样（τ（τ） ）^0.5，对于一些小κ。这鼓励代理商继续测试所有可访问的状态过渡，甚至可以找到长长的操作序列以进行＃进行此类测试。 ＃当前步骤 - 上次访问plance_reward += kappa * nath.sqrt（total_visited_time_time_step [plance_state] [plance_actate] [plancy_action]）q_values [planning_state] [planning_state] [planning_action] += alpha * [planning_state] [planning_action]）打印（总步骤：＆quot; total_step）返回q_values，sum_rewards_episodes，timeStep_episodes    ``` 32;提交由＆＃32; /u/vvy_     [link]  &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/1iwdj4k/difference_between_dynaq_and_ynaq_and_dynaq_algorithm_not/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwdj4k/difference_between_dynaq_and_dynaq_algorithm_not/</guid>
      <pubDate>Sun, 23 Feb 2025 16:09:40 GMT</pubDate>
    </item>
    <item>
      <title>解决迷宫的RL代理：疑问</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw7g65/rl_agent_for_solving_mazes_doubts/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好。我将毕业于CS，并想创建一个关于在统一的沙盒环境中进行的关于迷宫式解决方案的强化学习的论文项目。我对AI和相关主题有基本知识，但是我对自己的首发想法有一些疑问。 我想在统一环境中进行强化学习的项目，重点关注代理商的发展能够解决迷宫。给定简单的迷宫，代理应该能够在其中导航并在最短的时间内到达出口。团结将作为代理商的测试环境。迷宫是由用户通过专用编辑器构建的。创建后，用户可以将代理放置在起点并定义奖励和惩罚权重，并根据这些参数训练AI。可以保存训练的模型，在新的迷宫上进行测试或通过不同的设置进行重新训练。  是否可以训练能够解决具有可变起点和出口的不同迷宫的好代理？也许程序中的变量不应该是这两个点，而是迷宫中的内容（例如障碍）或目标（而不是退出迷宫，而是要收集尽可能多的硬币） 您认为这个项目太雄心勃勃，无法在3个月内完成？ 与RL代理相比，A*算法是可以解决所有迷宫的算法。是真的吗？有什么区别？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/asiiaiapiazza     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw7g65/rl_agent_for_solving_mazes_doubts/</guid>
      <pubDate>Sun, 23 Feb 2025 10:41:23 GMT</pubDate>
    </item>
    <item>
      <title>学习政策以最大化满足B</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试学习一个控制策略，该策略在确保条件B时最大化变量。例如，机器人在将速度保持在给定范围内（b）的同时最大化能源效率（a）。 我的想法：将奖励定义为a *（b）。当B被满足B时，奖励将为= A，并且在违反B时为= 0。但是，这可能会在培训的早期引起稀疏的回报。我可能会使用模仿学习来初始化策略来帮助解决此问题。 是否有适合此类问题的现有框架或技术？我非常感谢任何方向或相关关键字！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iw3ijl/learning_policy_to_to_maximize_a_a_a_a_while_satisfying_b/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</guid>
      <pubDate>Sun, 23 Feb 2025 06:05:48 GMT</pubDate>
    </item>
    <item>
      <title>Gridworld RL培训：情节的奖励不会改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivx8mj/gridworld_rl_training_rewards_over_episodes/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivx8mj/gridworld_rl_training_rewards_over_episodes/</guid>
      <pubDate>Sun, 23 Feb 2025 00:20:14 GMT</pubDate>
    </item>
    <item>
      <title>博客：衡量政策梯度的理论观点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我在这里很新，很抱歉，如果它不符合规则（我找不到任何规则），但是我想与您分享我的博客，以衡量政策梯度的理论观点，我介绍了我们如何利用Raadon-Nikodym derivative不仅可以得出标准增强，还可以得出一些以后的版本，以及我们如何使用占用度量作为轨迹采样的倒入替代品。希望您能享受并给我一些反馈，因为我喜欢在RL  中分享直觉的重大解释，这是链接： https：//myxik.github.io/posts/measuretheoretic-view/    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforeverctionlearning/comments/1ivwzw9/blog_measure_theoretod_theoretic_theoretic_on_policy_gradients/”&gt; [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</guid>
      <pubDate>Sun, 23 Feb 2025 00:08:21 GMT</pubDate>
    </item>
    <item>
      <title>对于简单的MDP，此奖励值是否有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivu7uf/does_this_reward_values_makes_sense_for_a_simple/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！ 我正在尝试解决MDP，我为其定义了以下奖励，但是我有一个很难用价值迭代解决它。看来，国家价值功能不会收敛，经过一些迭代后，它将不再改善。所以，我在想也许问题是我的奖励结构？因为它有很大的变化。您认为这可能是一个原因吗？   r1 = {x1＆quord&#39;：500，&#39;x2＆quot” x2＆quot;：300，＆quort&#39;x3＆quot x3＆quort&#39;：100} r_2 = 1 r3 = -100 = -100 r4 = {x1＆quot;：-1000，x2＆quot;：-500，x3＆quort;：-200}    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/using_cauliflower320     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivu7uf/does_this_reward_values_makes_sense_for_a_simple/</guid>
      <pubDate>Sat, 22 Feb 2025 21:57:20 GMT</pubDate>
    </item>
    <item>
      <title>在美国顶级大学中录取的博士学位需要什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivr24m/what_is_required_for_a_phd_admit_in_a_top_tier_us/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有兴趣在强化学习中申请15个博士学位课程，并想了解一般的入学统计和期望。我目前是Virginia Tech的硕士学生，在RL撰写研究论文，是研究生水平深度RL课程的TA，并在计算机视觉方面具有先前的研究经验。如何使我的个人资料脱颖而出？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ivr24m/what_is_is_required_for_for_for_a_phd_admit_in_a_a_a_top_top_tier_us/”&gt; [links]      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/comments/1ivr24m/what_is_is_required_for_a_phd_phd_admit_in_a_a_a_a_a_a_top_top_top_tier_us/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivr24m/what_is_required_for_a_phd_admit_in_a_top_tier_us/</guid>
      <pubDate>Sat, 22 Feb 2025 19:37:07 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA CULE：“启用CUDA的ATARI 2600模拟器，该模拟器直接在GPU内存中呈现框架”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivog6c/nvidia_cule_a_cuda_enabled_atari_2600_emulator/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/masterscrat     [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1ivog6c/nvidia_cule_a_cuda_cuda_enabled_atari_atari_2600_emulator/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivog6c/nvidia_cule_a_cuda_enabled_atari_2600_emulator/</guid>
      <pubDate>Sat, 22 Feb 2025 17:46:51 GMT</pubDate>
    </item>
    <item>
      <title>强化学习是实现AGI的关键吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivns8i/is_reinforcement_learning_the_key_for_achieving/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是新RL。我已经看到了深刻的纸，他们非常强调RL。我知道GPT和其他LLM使用RL，但深刻的寻求使其成为主要。因此，我想学习RL，因为我想成为一名研究人员。是我的结论甚至正确，请验证它。如果是真的，请建议我来源。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tarnatraining822     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivns8i/is_reinforcement_learning_the_key_for_achieving/</guid>
      <pubDate>Sat, 22 Feb 2025 17:19:29 GMT</pubDate>
    </item>
    <item>
      <title>学习级研究项目思想</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivmj63/learninglevel_research_project_ideas/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在我收到任何仇恨评论我的问题之前，我想提一下，我知道它不是正确的心态，请选择“轻松问题” ，但是ID喜欢在3个月的时间范围内进行RL研究项目，以暴露于研究界，并深入研究我喜欢的RL。这是一种曝光，我想从事的一种破冰者的工作，大约一个月前开始学习的领域。 我想对社区的想法有一些乞egine的想法 - 我们可以冒险进入并涉足的友好RL研究领域。完成此操作后，我最终将进入RL的其他分支。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/extension-economy-78     [link]    32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivmj63/learninglevel_research_project_ideas/</guid>
      <pubDate>Sat, 22 Feb 2025 16:27:23 GMT</pubDate>
    </item>
    </channel>
</rss>