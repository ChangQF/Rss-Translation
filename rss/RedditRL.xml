<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 30 Nov 2023 01:00:27 GMT</lastBuildDate>
    <item>
      <title>我想使用 Dreamerv3 (pytorch) 来训练我的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1870q9h/i_would_like_to_use_dreamerv3_pytorch_to_train_my/</link>
      <description><![CDATA[我想使用 Dreamerv3 (pytorch) 来训练我的环境。我的自定义环境是基于矢量的健身房环境。 import numpy as npimport os, sys, globimportgymfrom hparams import HyperParams as hpfrom hparams import RobotFrame_Continously_Datasets_Timestep_1 as data# from hparams import RobotFrame_Continously_Datasets_Timestep_3 as data# from hparams import RobotFrameContinouslyDatasetsTimestep_0_25 as data# from hparams import RobotFrameContinouslyDataset sTimestep_0_5 as dataimport syssys.path.append(&#39;./gsoc22-socnavenv&#39;)import randomimport socnavevimport osimport torchdef离散_to_连续_action(action:int):&quot;&quot;&quot;为给定的离散动作返回连续空间动作的函数&quot;&quot;&quot; ;# 将可能的操作值调整为 -0.5, 0, 0.5 或 1move_dict = {0: -0.5, 1: 0, 2: 0.5, 3: 1}turn_dict = {0: -0.5, 1: 0, 2 : 0.5, 3: 1}move = move_dict[action // 4]turn =turn_dict[action % 4]return np.array([move, Turn], dtype=np.float32)def preprocess_observation(obs):&quot;&quot; ;“将 dict 观察转换为 numpy 观察”“”assert(type(obs) == dict)obs2 = np.array(obs[“goal”][-2:], dtype=np.float32)人类 = obs[“人类”].flatten()for i in range(int(round( humans.shape[0]/(6+7)))):index = i*(6+7)obs2 = np .concatenate((obs2, humans[index+6:index+6+7]) )tables = obs[“tables”].flatten()for i in range(int(round(tables.shape[0]/()) 6+7)))):index = i*(6+7)obs2 = np.concatenate((obs2,tables[index+6:index+6+7]))plants = obs[“plants”]。展平（）for i in range（int（round（plants.shape [0] /（6 + 7））））：index = i *（6 + 7）obs2 = np.concatenate（（obs2，plants [index + 6:index+6+7]) )return torch.from_numpy(obs2)device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)def rollout():time_steps = data.time_stepsenv = gym.make(“SocNavEnv-v1”)env.configure(&#39;./configs/env_timestep_03.yaml&#39;)env.set_padded_observations(True)max_ep = 50_000feat_dir = data.data_diros.makedirs(feat_dir,exist_ok=True)env.seed (1) # 演示的确定性for ep in range(max_ep):obs_lst, action_lst,reward_lst, next_obs_lst, did_lst = [], [], [], [], []obs = env.reset()obs = preprocess_observation(obs )done = Falset = 0for t in range(time_steps+10):# env.render()# action_ = np.random.randint(0, 64) # 从 4 更新到 64，以匹配新的操作空间# action =离散_to_连续_action(action_)# action = np.round(action, 小数=2)action_ = np.random.randint(0, 16) # 从 4 更新到 16，以匹配新的操作空间# action_ = 9 # 选择一个初始值用于演示目的的操作action =离散_to_连续_action(action_)#action = np.round(action, 小数=2)# print(action)next_obs,reward,done,_ = env.step(action)#print(next_obs)next_obs = preprocess_observation( next_obs)action = torch.from_numpy(action)# print(next_obs)#np.savez(os.path.join(feat_dir, &#39;rollout_{:03d}_{:04d}&#39;.format(ep,t)),obs =obs,action=action,reward=reward,next_obs=next_obs,done=done,)obs_lst.append(obs)action_lst.append(action)reward_lst.append(reward)next_obs_lst.append(next_obs)done_lst.append(完成) obs = next_obsif did:print(&quot;剧集 [{}/{}] 在 {} 个时间步长后完成&quot;.format(ep + 1, max_ep, t),lush=True)obs = env.reset()obs_lst = torch. stack(obs_lst, dim=0).squeeze(1)next_obs_lst = torch.stack(next_obs_lst, dim=0).squeeze(1)done_lst = [int(d) for d in did_lst]done_lst = torch.tensor(done_lst) .unsqueeze(-1)action_lst = torch.stack(action_lst, dim=0).squeeze(1)reward_lst = torch.tensor(reward_lst).unsqueeze(-1)breaknp.savez(os.path.join(feat_dir, &#39; rollout_ep_{:03d}&#39;.format(ep)),obs=np.stack(obs_lst, axis=0), # (T, C, H, W)action=np.stack(action_lst, axis=0), # (T, a)reward=np.stack(reward_lst, axis=0), # (T, 1)next_obs=np.stack(next_obs_lst, axis=0), # (T, C, H, W)done=np .stack(done_lst, axis=0), # (T, 1))if __name__ == &#39;__main__&#39;:np.random.seed(123)rollout() 这是我的环境：我是基于平面向量的观察 (23) 且动作空间为 2。   由   提交 /u/Goodluck_o   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1870q9h/i_would_like_to_use_dreamerv3_pytorch_to_train_my/</guid>
      <pubDate>Wed, 29 Nov 2023 21:34:12 GMT</pubDate>
    </item>
    <item>
      <title>[需要帮助] 大学项目：带有传感器的孔寻路槽板 - 寻求建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186yi3z/help_needed_project_for_university_path_finding/</link>
      <description><![CDATA[      你好r/reinforcementlearning 社区， 对于我们的产品开发硕士，我们应该实施和构建一个基于强化学习的系统（具有真实的组件）。然而，我们都没有对强化学习有深刻的理解，所以到目前为止我们还没有能够成功地训练，不幸的是我们的教授无法再帮助我们，并鼓励我们寻求外部帮助。也许这里有人愿意接受挑战:)我们很高兴收到关于我们的代码的新想法或意见以及如何进一步进行。 项目概述： 代理应该到达棋盘顶部（我们喜欢称之为奶酪板），而不会掉入任何洞中。为了支持这项任务，他在其上方的左右对角位置有两个传感器，它们返回两个传感器位置之一是否有孔 代码片段： 训练和测试文件，以及系统的更多示例和我们这边的最新结果可以在我的 OneDrive 文件夹中访问：https://drive.google.com/drive/folders/1hP5XiLkf9auZncZ7xyvmv0bChadbie1N?usp=sharing在此文件夹中，您还可以找到更多见解来可视化我们的项目。&lt; /p&gt; 我们尝试过的： 我们尝试了我们想到的一切：- 不同的奖励函数- 不同的行动空间（省略向下运动）- 不同的强化学习参数（alpha、gamma、epsilon、epsilon 衰减等）- 在各种矩阵上训练的不同方法 总而言之，几周来我们尝试了所有想到的方法，但收效甚微。不过，从理论上讲，他只需要学习，例如，一旦右侧传感器检测到有洞，就稍微向左移动，否则继续向上移动。 具体问题：   有人在训练金融交易强化学习代理时遇到过类似的问题吗？ 对这种情况下奖励函数的潜在问题有什么见解吗？&lt; /li&gt;  我们非常感谢您的帮助，并期待您的想法:) 我们的问题示例，代理应该到达顶部并避开洞   由   提交/u/Flat_Chipmunk_9188   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186yi3z/help_needed_project_for_university_path_finding/</guid>
      <pubDate>Wed, 29 Nov 2023 19:58:10 GMT</pubDate>
    </item>
    <item>
      <title>将多处理与 CUDA 相结合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/</link>
      <description><![CDATA[我是高性能计算方面的新手。因此，如果我的问题看起来太微不足道，请原谅我。 ​ 我正在考虑将多重处理应用于 CUDA RL 程序。现在我正在尝试训练一个需要大约 9 小时训练的环境。我想尝试一下超参数和其他一些东西。因此，我想使用相同的 GPU 并行运行我的模型 10 次。  这是我在谷歌搜索后要处理的 -   GPU 大小 - 我知道我的 GPU 应该足够大，可以接收 10 个数据运行。  GPU 年份 - 显然较旧的 GPU 不允许多个程序访问它。   我还缺少什么吗？我的假设是，如果运行单个模型需要 9 小时，那么通过多重处理，只要满足上述约束，我就可以在 9 小时内运行 10 个模型。   由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186xc15/combining_multiprocessing_with_cuda/</guid>
      <pubDate>Wed, 29 Nov 2023 19:06:53 GMT</pubDate>
    </item>
    <item>
      <title>Rankitect：在元规模上对架构搜索与世界级工程师进行排名</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</link>
      <description><![CDATA[      论文：https://arxiv.org/abs/2311.08430 摘要：  神经架构搜索 (NAS) 已经证明了其在计算机视觉方面的功效和排名系统的潜力。然而，之前的工作主要集中在学术问题上，这些问题是在良好控制的固定基线下进行小规模评估的。在行业系统中，例如 Meta 中的排名系统，尚不清楚文献中的 NAS 算法是否能够超越生产基线，因为：（1）规模 - Meta 排名系统为数十亿用户服务，（2）强大的基线 - 基线是生产自深度学习兴起以来，多年来数百到数千名世界级工程师优化了模型，(3) 动态基线 - 工程师可能在 NAS 搜索过程中建立了新的、更强的基线，(4) 效率 - 搜索管道必须产生结果快速与生产生命周期保持一致。在本文中，我们介绍了 Rankitect，这是一个用于 Meta 排名系统的 NAS 软件框架。 Rankitect 寻求通过从头开始构建低级构建块来构建全新的架构。 Rankitect 实现并改进了最先进 (SOTA) NAS 方法，以在同一搜索空间下进行全面、公平的比较，包括基于采样的 NAS、一次性 NAS 和可微分 NAS (DNAS)。我们通过与 Meta 上的多个生产排名模型进行比较来评估 Rankitect。我们发现 Rankitect 可以从头开始发现新模型，实现归一化熵损失和 FLOP 之间的竞争性权衡。当利用工程师设计的搜索空间时，Rankitect 可以生成比工程师更好的模型，实现积极的离线评估和 Meta 规模的在线 A/B 测试。  https://preview.redd.it/mlceky7x7b3c1.png?width=1379&amp;format=png&amp;auto =webp&amp;s=2acc4e0451db9fbf455b03f9e293e68cc61d25bf   由   提交 /u/APaperADay   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ssm9/rankitect_ranking_architecture_search_battling/</guid>
      <pubDate>Wed, 29 Nov 2023 16:00:25 GMT</pubDate>
    </item>
    <item>
      <title>我的 Gym Cartpole 代理学习的运行平均值约为 200，但当我使用相同的 q 表进行游戏时，它仅运行 10 分。请帮忙。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ms1a/my_gym_cartpole_agent_learns_with_a_running_mean/</link>
      <description><![CDATA[gridworld/cartpole.ipynb at main · bherwanisuraj/gridworld (github.com)  这是我的 git 存储库的链接。如果有人能帮忙那就最好了。    由   提交 /u/tlevelup   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ms1a/my_gym_cartpole_agent_learns_with_a_running_mean/</guid>
      <pubDate>Wed, 29 Nov 2023 10:57:01 GMT</pubDate>
    </item>
    <item>
      <title>关于“Q*”推测：法学硕士和合成数据搜索的一些相关研究背景</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186fhih/on_q_speculation_some_relevant_research/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186fhih/on_q_speculation_some_relevant_research/</guid>
      <pubDate>Wed, 29 Nov 2023 03:20:02 GMT</pubDate>
    </item>
    <item>
      <title>“学习少量模仿作为文化传播”，Bhoopchand 等人 2023 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/186ejkw/learning_fewshot_imitation_as_cultural/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/186ejkw/learning_fewshot_imitation_as_cultural/</guid>
      <pubDate>Wed, 29 Nov 2023 02:37:31 GMT</pubDate>
    </item>
    <item>
      <title>DQN Agent 学得不太好</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/</link>
      <description><![CDATA[大家好！ 我目前正在学习强化学习。我写信询问是否有解决方案，因为我的代理在通过决斗 dqn 强化 Atari 突破时没有正确学习。就我而言，我成功地挖了一条隧道并举起了球，但球升起后，特工开始没有采取任何行动。我的猜测是，我了解到最好的做法就是在投球后什么也不做。但即使球落下来，这也是一个问题，因为他什么也没做。还有其他方法可以解决这个问题吗？ 如果您需要检查代码，请告诉我，我给您链接   由   提交/u/king-god_general  /u/king-god_general  reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185zw1z/dqn_agent_do_not_learn_very_well/</guid>
      <pubDate>Tue, 28 Nov 2023 16:20:46 GMT</pubDate>
    </item>
    <item>
      <title>销售和预测销售环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185ykik/sales_and_forecasted_sales_environment/</link>
      <description><![CDATA[假设我有一组产品的历史销售额和预测销售额，并且我想使用代理（RL 或其他）来管理库存决策。关于如何使用这些历史数据来创建学习环境有什么想法吗？当然，我可以将分布拟合到历史销售和样本中，但是预测方法是基于对销售趋势的基本假设，而这样做会丢失这些假设。 理想情况下，我会有一个环境生成大量未来销售轨迹和相关预测，让代理通过数千次迭代学习基于预测的库存策略。但销售轨迹和预测应该以与生成基础预测模型相同的方式关联，否则该策略就毫无价值。  也许查看预测的历史误差并从未来的单一预测生成随机轨迹会更有意义？这里的问题是，根据给定的销售轨迹，预测可能会在 3 周内发生变化。 任何见解或类似的工作将不胜感激！ &lt;!-- SC_ON - -&gt;  由   提交 /u/aaronunderwater   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185ykik/sales_and_forecasted_sales_environment/</guid>
      <pubDate>Tue, 28 Nov 2023 15:25:12 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线 3 + JAX (SBX) 未在 GPU 上运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185tpdo/stable_baselines_3_jax_sbx_not_running_on_gpu/</link>
      <description><![CDATA[我已经使用“pip install sbx-rl”安装了 SBX 库（最新版本）用于我的 Stable Baselines 3 + JAX PPO 实施，以提高训练速度。我想使用 GPU (RTX 4090) 进行训练，但由于某种原因 SBX 始终默认使用 CPU。  SBX 无法识别我的 GPU，但是当我执行“nvidia-smi”时，SBX 无法识别我的 GPU。它被清楚地检测到+ pytorch 本身也确实检测到它。我怀疑 SBX 与我的 CUDA 版本（当前为 12.2）不兼容，但我找不到任何有关支持哪些 CUDA 版本的文档。  我花了几天时间试图解决这个问题，但没有成功。有人有想法吗？   由   提交/u/ClassicAppropriate78  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185tpdo/stable_baselines_3_jax_sbx_not_running_on_gpu/</guid>
      <pubDate>Tue, 28 Nov 2023 11:15:31 GMT</pubDate>
    </item>
    <item>
      <title>受版权游戏启发的研究环境（例如 Hanabi 挑战等）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185splc/research_environment_inspired_from_copyrighted/</link>
      <description><![CDATA[大家好！ 我想知道为游戏创建强化学习环境（仅用于研究）的法律方面仍然由创建它的相应公司拥有版权和销售权。 以 Hanabi 挑战为例 https:// arxiv.org/pdf/1902.00506.pdf，快速谷歌显示该游戏仍在销售。论文作者是否事先询问过游戏发行商是否允许他们创建环境？他们刚刚就这么做了吗？这里的合法性是什么？  感谢和干杯！   由   提交 /u/Arconer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185splc/research_environment_inspired_from_copyrighted/</guid>
      <pubDate>Tue, 28 Nov 2023 10:09:18 GMT</pubDate>
    </item>
    <item>
      <title>离策略演员批评家目标函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</link>
      <description><![CDATA[      我正在阅读 Silver 的 DPG 论文。在这里，如下所示，目标函数已使用行为策略 beta 进行了修改。我很好奇，如果使用梯度最大化下面的目标，目标策略的目标函数（通常的策略目标）是否会最大化？ ​ &lt; a href=&quot;https://preview.redd.it/1779aomlyz2c1.png?width=737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e&quot;&gt;https://preview.redd.it/1779aomlyz2c1.png?width =737&amp;format=png&amp;auto=webp&amp;s=bd351a72294f0ad0ef8c8bdaef08e173af00f96e   由   提交 /u/RealJuney   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185kuxr/offpolicy_actorcritic_objective_function/</guid>
      <pubDate>Tue, 28 Nov 2023 02:12:27 GMT</pubDate>
    </item>
    <item>
      <title>寻找职业建议。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</link>
      <description><![CDATA[大家好，过去 3 年我一直对机器学习感兴趣，我的大部分注意力都集中在监督学习上，但是在过去 3 个月里 RL引起了我的注意，我相信人工智能的下一个重大事件将来自该领域。我有兴趣通过学术界，因为我只有计算机科学学士学位，并且不会找到工作，因为我在津巴布韦，而我们在技术方面还没有达到这个水平。我申请在美国攻读博士学位，但拒绝的次数越来越多，所以我很可能最终会去中国获得奖学金。我想要一些建议，因为最终我想在西方的大公司从事研发工作。如果可以的话，请告诉我在中国攻读硕士学位期间我可以做些什么，以便在 2026/27 年毕业后让我更接近这个目标。 PS：我也在中国获得了学士学位。   由   提交/u/congo43  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185bwr6/looking_for_career_advice/</guid>
      <pubDate>Mon, 27 Nov 2023 19:53:09 GMT</pubDate>
    </item>
    <item>
      <title>多头DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</link>
      <description><![CDATA[大家好，我正在应用 DQN 每次选择一组元素（一次一个或多个）。如何避免动作 [0, 0, 0,…]，即如何强制代理选择至少一个元素？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/185aneo/multihead_dqn/</guid>
      <pubDate>Mon, 27 Nov 2023 19:01:00 GMT</pubDate>
    </item>
    <item>
      <title>Mujoco 3.0 对阵 Isaac Gym</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</link>
      <description><![CDATA[您好， 对于那些尝试过并且熟悉 Mujoco 3.0 和 Isaac Gym 的人，建议他们使用哪一个学习以及为什么？   由   提交 /u/anointedninja   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1857nn8/mujoco_30_vs_isaac_gym/</guid>
      <pubDate>Mon, 27 Nov 2023 16:59:33 GMT</pubDate>
    </item>
    </channel>
</rss>