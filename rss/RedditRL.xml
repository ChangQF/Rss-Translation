<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 29 Aug 2024 21:15:25 GMT</lastBuildDate>
    <item>
      <title>我正在寻找研究人员和人工智能开发团队的成员参与用户研究，以支持我的研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4d4vp/im_looking_for_researchers_and_members_of_ai/</link>
      <description><![CDATA[我们正在寻找年满 18 岁且在软件开发领域拥有 2 年以上经验的研究人员和 AI 开发团队成员，以参与一项匿名调查，以支持我在缅因大学的研究。这可能需要 20-30 分钟，并将调查您对您所在行业未来发展 AI 系统所带来的挑战的看法。如果您想参与，请在继续调查之前阅读以下招聘页面。完成调查后，您可以参加抽奖，赢取价值 25 美元的亚马逊礼品卡。 https://docs.google.com/document/d/1Jsry_aQXIkz5ImF-Xq_QZtYRKX3YsY1_AJwVTSA9fsA/edit    提交人    /u/wildercb   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4d4vp/im_looking_for_researchers_and_members_of_ai/</guid>
      <pubDate>Thu, 29 Aug 2024 20:49:20 GMT</pubDate>
    </item>
    <item>
      <title>用于 PPO 的 ResNet 迁移学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f47fuc/resnet_transfer_learning_for_ppo/</link>
      <description><![CDATA[我目前正在尝试在像素输入上训练 PPO，但是训练速度非常慢。与我提供代理环境信息时相比，对像素输入进行训练至少需要 10 倍的样本数量（可能接近 25 倍），并且反向传播所需的时间要长得多。  我曾考虑使用 Resnet18 作为特征提取器来加快速度，但该模型有很多 BatchNorm 层，我听说这对 RL 不利。如果我删除 BatchNorm 层，该模型是否仍然有效，或者是否存在专门针对 RL 的预训练模型？    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f47fuc/resnet_transfer_learning_for_ppo/</guid>
      <pubDate>Thu, 29 Aug 2024 16:55:47 GMT</pubDate>
    </item>
    <item>
      <title>CartPole v0 中的 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f425dn/dqn_in_cartpole_v0/</link>
      <description><![CDATA[我正在尝试实现一个具有重放缓冲区、epsilon 衰减、目标 q 网络定期更新的 DQN。我不知道问题是什么，但网络似乎根本没有进行训练。我尝试更改超参数。似乎仍然不起作用。 # %% from collections import deque import random import torch from torch import nn import torch.nn. functional as F import gymnasium as gym import matplotlib.pyplot as plt import numpy as np import imageio # %% env = gym.make(&#39;CartPole-v1&#39;, render_mode = &#39;rgb_array&#39;) # %% def save_frames_as_gif(frames, path=&#39;./&#39;, filename=&#39;gym_animationq.gif&#39;): &quot;&quot;&quot;将帧另存为 GIF。&quot;&quot;&quot; imageio.mimsave(路径 + 文件名, 帧, fps=30) # %% pos_space = np.linspace(-2.4, 2.4, 10) vel_space = np.linspace(-4, 4, 10) ang_space = np.linspace(-.2095, .2095, 10) ang_vel_space = np.linspace(-4, 4, 10) # %% x = np.linspace(-1, 1, 10) x[9] # %% def digitize(state): state_p = np.digitize(state[0], pos_space) state_v = np.digitize(state[1], vel_space) state_a = np.digitize(state[2], ang_space) state_av = np.digitize(state[3], ang_vel_space) return torch.FloatTensor([state_p, state_v, state_a, state_av]) # %% class DQN(nn.Module): def __init__(self, in_states, h1_nodes, out_actions): super().__init__() self.fc1 = nn.Linear(in_states, h1_nodes) self.out = nn.Linear(h1_nodes, out_actions) def forward(self, x): x = F.relu(self.fc1(x)) x = self.out(x) return x # 为 Experience Replay 定义内存 class ReplayMemory(): def __init__(self, maxlen): self.memory = deque([], maxlen=maxlen) def append(self, transition): self.memory.append(transition) def sample(self, sample_size): return random.sample(self.memory, sample_size) def __len__(self): 返回 len(self.memory) # %% discount_factor = 0.9 epsilon = 1 epsilon_decay_rate = 0.0001 frames = [] rewards_per_episode = [] i = 0 memory_size = 100000 replay_size = 128 network_sync_rate = 50 memory = ReplayMemory(memory_size) policy_dqn = DQN(in_states=4, h1_nodes=10, out_actions=2) target_dqn = DQN(in_states=4, h1_nodes=10, out_actions=2) target_dqn.load_state_dict(policy_dqn.state_dict()) optimizer = torch.optim.Adam(policy_dqn.parameters(), lr= 1e-4) loss_fn = nn.HuberLoss() # %% def optimize(mini_batch, policy_dqn, target_dqn): # print(&quot;optimizing&quot;) current_q_list = [] target_q_list = [] for state, action, new_state, reward, terminated in mini_batch: if terminated: target = torch.FloatTensor([reward]) else: with torch.no_grad(): target = torch.FloatTensor( reward + discount_factor * target_dqn(digitize(state)).max()) current_q = policy_dqn(digitize(state)) current_q_list.append(current_q) # 获取目标Q值集合 target_q = target_dqn(digitize(state)) # 将具体的action调整为刚刚计算出的目标 target_q[action] = target target_q_list.append(target_q) loss = loss_fn(torch.stack(current_q_list), torch.stack(target_q_list)) optimizer.zero_grad() loss.backward() optimizer.step() # %% while(True): i +=1 state = env.reset()[0] # 起始位置，起始速度始终为 0 deterministic = False rewards = 0 while (未终止且奖励 &lt; 10000): if np.random.uniform() &lt;= epsilon: action = env.action_space.sample() else: with torch.no_grad(): action = policy_dqn(digitize(state)).argmax().item() new_state, r_t, termined,_,_ = env.step(action) rewards += r_t memory.append((state, action, new_state, r_t, termined)) # print(memory.__len__()) if memory.__len__() &gt; replay_size：#打印（“hello”）minibatch = memory.sample（replay_size）优化（minibatch，policy_dqn，target_dqn）epsilon = max（epsilon - epsilon_decay_rate，0）#打印（epsilon，epsilon_decay_rate）如果（i + 1）％network_sync_rate == 0：target_dqn.load_state_dict（policy_dqn.state_dict（））state = new_staterewards_per_episode.append（rewards）mean_rewards = np.mean（rewards_per_episode [len（rewards_per_episode）-100：]）如果i％10 == 0：打印（f&#39;Episode：{i} {rewards} Epsilon：{epsilon：0.2f}平均奖励{mean_rewards：0.1f}&#39;）如果mean_rewards &gt; 50: break # %% env = gym.make(&#39;CartPole-v1&#39;, render_mode = &#39;rgb_array&#39;) import time frames = [] state = env.reset()[0] terminology = False while not deterd: action = policy_dqn(digitize(state)).argmax().item() new_state, reward, terminology, _, _ = env.step(action) frame = env.render() time.sleep(0.1) frames.append(frame) # 调整睡眠时间来控制渲染速度 state = new_state env.close() # %% save_frames_as_gif(frames) # %%     submitted by    /u/Caveman2k23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f425dn/dqn_in_cartpole_v0/</guid>
      <pubDate>Thu, 29 Aug 2024 13:16:18 GMT</pubDate>
    </item>
    <item>
      <title>理解和诊断深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3zeh7/understanding_and_diagnosing_deep_reinforcement/</link>
      <description><![CDATA[发表于国际机器学习会议 ICML 2024 论文：https://proceedings.mlr.press/v235/korkmaz24a.html    提交人    /u/x8pkh   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3zeh7/understanding_and_diagnosing_deep_reinforcement/</guid>
      <pubDate>Thu, 29 Aug 2024 10:53:23 GMT</pubDate>
    </item>
    <item>
      <title>机器人深度强化学习：现实世界的成功调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3twmr/deep_reinforcement_learning_for_robotics_a_survey/</link>
      <description><![CDATA[随着我们接近 DQN 诞生 10 周年的里程碑，现在正是反思深度强化学习在机器人技术领域发展历程的好时机。哪些现实世界的机器人挑战因深度强化学习而取得了最重大的进步？我们还有哪些工作要做？ 我们最近发布了一项调查，探讨了这些问题。本文对深度强化学习在各种机器人能力中在现实世界中的成功进行了分类，用新的问题定义和解决方法分类法分析了潜在趋势，并强调了机器人专家和强化学习研究人员面临的关键挑战以及未来工作的主要途径。 如果您感兴趣，可以在此处查看完整论文：arxiv 链接。这篇调查论文将发表在 2025 年《控制、机器人和自主系统年度评论》中。    提交人    /u/Best-Pension-8837   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3twmr/deep_reinforcement_learning_for_robotics_a_survey/</guid>
      <pubDate>Thu, 29 Aug 2024 04:33:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的模拟器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3l0uu/simulators_in_pytorch/</link>
      <description><![CDATA[最近，有在 JAX 中构建模拟器的趋势：jumanji、TORAX、pgx、minimax，全部根据 Anakin 架构 构建。 为什么不在 PyTorch 中？    提交人    /u/gepeto97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3l0uu/simulators_in_pytorch/</guid>
      <pubDate>Wed, 28 Aug 2024 21:30:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的低计算研究领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3c848/low_compute_research_areas_in_rl/</link>
      <description><![CDATA[我现在是本科四年级，必须为我的论文选择一个研究主题。我之前上过 ML/DL/RL 的课程，所以我有基础知识。 问题是我在这里无法访问适当的 GPU 资源。（当然，云是存在的，但价格昂贵。）我们大学只有一个简单的消费级 GPU（RTX 3090）和一台始终供不应求的 HPC 服务器，我的笔记本电脑中有 GTX 1650Ti。 所以，我正在寻找 RL 中需要相对较少计算的研究领域。我对理论和实践主题都持开放态度，但理想情况下，我想研究一些可以在我可用的硬件上实现和测试的东西。 我研究过的一些领域是迁移学习、元 RL、安全 RL 和逆 RL。我认为我的硬件很难处理 MARL。 您可以推荐研究领域、应用领域，甚至是可能有趣的特定论文。 此外，任何关于如何最大限度提高我的硬件在 RL 实验中的效率的建议都将不胜感激。 谢谢！！    提交人    /u/Abominable_Liar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3c848/low_compute_research_areas_in_rl/</guid>
      <pubDate>Wed, 28 Aug 2024 14:52:53 GMT</pubDate>
    </item>
    <item>
      <title>为什么DP中的策略迭代会收敛到真实值函数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f38w85/why_policy_iteration_in_dp_converges_to_true/</link>
      <description><![CDATA[给定一个特定的确定性策略和一个已知的 MDP 环境，如果迭代贝尔曼期望方程，是否可以收敛到给定策略的所有状态的真实值函数？ 我想它说无论如何 DP 策略迭代都有效，但我不确定它为什么有效。    提交人    /u/Latter-Tomorrow-6850   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f38w85/why_policy_iteration_in_dp_converges_to_true/</guid>
      <pubDate>Wed, 28 Aug 2024 12:27:26 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f373dl/learning_reinforcement_learning/</link>
      <description><![CDATA[您好，我已经使用 Python 编码 3 年了，并且具备 C 语言基础知识。在哪里可以了解有关强化学习的更多信息并提高我的知识。如果有资源可以编写自己的代理，我会很高兴。 谢谢。    提交人    /u/Competitive_Yak7223   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f373dl/learning_reinforcement_learning/</guid>
      <pubDate>Wed, 28 Aug 2024 10:50:04 GMT</pubDate>
    </item>
    <item>
      <title>混合动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f32pe1/hybrid_action_space/</link>
      <description><![CDATA[嗨，我在选择混合动作空间的正确算法时遇到了问题。通常，在许多研究论文中，他们提到混合动作空间是一种分层情况，即当您选择一个离散动作，然后根据离散动作选择一个连续动作时。但是，在我的项目中，一个动作需要 2 个参数，它们是离散的和连续的。它们彼此独立，但都会影响状态、奖励。因此，我想问一下我应该将其分成 2 个代理还是有算法可以解决这个问题？非常感谢！    提交人    /u/Fish_Chandle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f32pe1/hybrid_action_space/</guid>
      <pubDate>Wed, 28 Aug 2024 05:46:47 GMT</pubDate>
    </item>
    <item>
      <title>学习环境模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2wew8/learning_environment_model/</link>
      <description><![CDATA[你好。由于其样本效率，我想尝试基于模型的 RL。 但是，当我尝试在玩具环境中学习一个模型时，该模型具有大小为 51 的 1d 向量输入和大小为 10 的输出，该模型很难学习。该模型接收当前观察，然后采取行动预测下一个观察、奖励和终止标志。 观察和行动在 0~1 之间。但模型的 L2 误差从 0.1 下降得太慢了。它正在学习。但太慢了！ 这很奇怪，因为使用 td3 快速学习了一个好的策略。 有人可以分享他们在基于模型的 RL 方面的经验或一些好的材料吗？谢谢！    提交人    /u/Automatic-Web8429   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2wew8/learning_environment_model/</guid>
      <pubDate>Wed, 28 Aug 2024 00:08:30 GMT</pubDate>
    </item>
    <item>
      <title>Bandit Learning 研究/行业机会</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2t9qi/bandit_learning_research_industry_opportunities/</link>
      <description><![CDATA[大家好，我开始学习 bandit 学习的基础知识，但我真的很关心这个领域的实际应用或就业市场。这是一个有趣的领域，但到目前为止，我不知道这个领域的求职“关键词”是什么，不知道是否要继续？    提交人    /u/math--lover   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2t9qi/bandit_learning_research_industry_opportunities/</guid>
      <pubDate>Tue, 27 Aug 2024 21:49:57 GMT</pubDate>
    </item>
    <item>
      <title>在口袋妖怪绿宝石中击败第三个健身房</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2pvtl/beating_third_gym_in_pokemon_emerald/</link>
      <description><![CDATA[        提交人    /u/nicimunty   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2pvtl/beating_third_gym_in_pokemon_emerald/</guid>
      <pubDate>Tue, 27 Aug 2024 19:28:54 GMT</pubDate>
    </item>
    <item>
      <title>C# 深度强化学习比 sb3 快 300 倍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2ifcf/c_deep_reinforcement_learning_300_times_faster/</link>
      <description><![CDATA[        提交人    /u/asieradzk   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2ifcf/c_deep_reinforcement_learning_300_times_faster/</guid>
      <pubDate>Tue, 27 Aug 2024 14:27:06 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的反作弊系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</link>
      <description><![CDATA[我对创建基于 ML 的国际象棋反作弊系统很感兴趣。您知道任何关于使用 ML 方法进行反作弊的论文吗？我正在寻找概念。但到目前为止找不到任何类似的东西。有很多困难：例如，数据集中作弊者的例子很少，但公平玩家数据很多。在公平玩家数据上使用离线 RL 算法来预测异常行为是否有意义？    提交人    /u/HimitsuNoShougakusei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f2c93a/mlbased_anticheat_system/</guid>
      <pubDate>Tue, 27 Aug 2024 08:49:53 GMT</pubDate>
    </item>
    </channel>
</rss>