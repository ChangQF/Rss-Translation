<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 29 Mar 2024 09:13:09 GMT</lastBuildDate>
    <item>
      <title>再现割线</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqi4g0/reproducing_secant/</link>
      <description><![CDATA[嗨！ 最近，我一直在尝试在《Secant：自我专家克隆》中重现训练过程用于视觉策略的零样本泛化，这可以被视为一种知识蒸馏。该项目发布在 GitHub 上，但没有训练过程，只有一些基准环境和包装器。 我在使用 Pytorch 编写训练过程时遇到了一些困难。所以我想向这个社区的人们寻求帮助。有没有人在另一个可以参考的开源项目上尝试过这个过程？ 提前感谢您检查这篇文章。    ;由   提交/u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqi4g0/reproducing_secant/</guid>
      <pubDate>Fri, 29 Mar 2024 06:10:22 GMT</pubDate>
    </item>
    <item>
      <title>计算门槛低的小问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqh891/small_problems_with_low_computational_barrier_to/</link>
      <description><![CDATA[有哪些小的（不像超级计算机那样要求很高）开放式强化学习 (RL) 问题？一个非例子是玩星际争霸（这需要大量的计算资源）。另一个非例子是“解决”问题。通过 RL 进行井字游戏，搜索空间很小。    由   提交/u/fool126  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqh891/small_problems_with_low_computational_barrier_to/</guid>
      <pubDate>Fri, 29 Mar 2024 05:13:09 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习研究是否因不公平比较而受到损害？你的想法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqdx9w/is_deep_reinforcement_learning_research_being/</link>
      <description><![CDATA[深度强化学习中我们需要一个开放的分层基准测试系统，例如 ImageNet。   由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqdx9w/is_deep_reinforcement_learning_research_being/</guid>
      <pubDate>Fri, 29 Mar 2024 02:21:36 GMT</pubDate>
    </item>
    <item>
      <title>具有定制 Actor 架构的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqc5y5/ppo_with_a_customized_actor_architecture/</link>
      <description><![CDATA[你好， 我正在开发 PPO 的实现，其中参与者有一个选择器层和三个操作集，如图所示在下面的代码中（这只是一个例子）。选择器层用于选择动作集，然后仅执行所选动作集中的一个动作。在这种情况下，输入状态和动作的批评者网络的结构是什么，使得该动作是动作集 2 的整数和其他动作集的列表。我真的很痛苦。 ” class Policy(nn.Module): def __init__(self, state_dim, action_set1_num_heads, action_set2_dim, action_set3_num_heads, num_heads=1): super(Policy, self).__init__() # 选择器部分 self.selector = nn.Sequential( nn.Linear(state_dim, 32), nn.ReLU(), nn.Linear(32, 3) ) # 操作集 1：多头输出 self.action_set1_heads = nn.ModuleList([nn.Linear(state_dim, 3) for _ in range(action_set1_num_heads)]) # 动作集 2：7 个离散动作 self.action_set2 = nn.Linear(state_dim, action_set2_dim) # 动作集 3：多头输出 self.action_set3_heads = nn. ModuleList([nn.Linear(state_dim, 5) for _ in range(action_set3_num_heads)]) defforward(self, state): # 选择器网络selector_probs = F.softmax(self .selector(state), dim=-1) # 根据合法动作动态屏蔽动作集1 action_set1_probs = [F.softmax(head(state), dim=-1) for i, head在 enumerate(self.action_set1_heads) 中，如果 i &lt; legal_actions] # 动作集 2：选择离散动作之一 action_set2_probs = F.softmax(self.action_set2(state), dim=-1) action_set3_probs = [F .softmax(head(state), dim=-1) for i, head in enumerate(self.action_set3_heads)]返回selector_probs,action_set1_probs,action_set2_probs,action_set3_probs&quot;   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqc5y5/ppo_with_a_customized_actor_architecture/</guid>
      <pubDate>Fri, 29 Mar 2024 00:58:40 GMT</pubDate>
    </item>
    <item>
      <title>Ray PPO 与许多其他 PPO 相比 - 为什么 Ray 更好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bq2en6/ray_ppo_vs_many_other_ppos_why_is_ray_so_much/</link>
      <description><![CDATA[我一直在 minworld 迷宫（纯粹基于视觉，使用地图覆盖范围作为奖励以及奖励框）和 ray 上测试 PPO实现突飞猛进。 我测试了sheeprl、cleanrl、sb3，但它们都一次又一次地碰壁，而ray的实现似乎像人类一样穿越迷宫。我通读了代码，看看到底是什么让 ray 变得更好，并且进行了消融研究，如果去掉 kl 损失项，ray 似乎会很困难，但在 cleanrl 中添加 kl 损失似乎根本没有帮助。  我还将 cleanrl 的 kl 近似值更改为真正的 kl div 损失，这似乎对它开始上升到较高的 ish 奖励然后下降到非常不智能的碰壁策略有所帮助。或绕小圈子。 我还将我的小批量大小设置为过大的数量（例如整个剧集 - 通过累积小批量的损失来完成，以在最后更新优化器）看看是否是小批量噪声，但似乎也不是。 在这样做时，我意识到 PPO 对实现和超参数有多么敏感，但我真的不明白为什么人们当它如此脆弱且变化无常时，请将其保留为通用的惊人算法。 鉴于 ray 糟糕的可读性，我几乎要抓狂了。有人用 cleanrl 单文件风格实现了 ray 的 ppo 吗？或者有谁知道 ray 使用其他流行库不使用的任何特定代码级别优化（为了智能，而不是速度或内存）？ 我可以提供性能图表、视频或实验详细信息，如果需要，但目前我不确定我能提供什么单一的东西可以帮助回答这个问题。    由   提交/u/dagangsta2012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bq2en6/ray_ppo_vs_many_other_ppos_why_is_ray_so_much/</guid>
      <pubDate>Thu, 28 Mar 2024 18:12:33 GMT</pubDate>
    </item>
    <item>
      <title>大规模强化学习的基础模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpy0ji/foundation_models_for_largescale_rl/</link>
      <description><![CDATA[针对大规模强化学习定制的基础模型有什么显着的进展吗？强化学习落后的一个主要原因是强化学习似乎与非常深的模型不兼容。   由   提交 /u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpy0ji/foundation_models_for_largescale_rl/</guid>
      <pubDate>Thu, 28 Mar 2024 15:13:59 GMT</pubDate>
    </item>
    <item>
      <title>有人可以给我指一个简单的 numpy 的 a2c 在线学习示例吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpw3bd/can_someone_point_me_to_a_simple_a2c_online/</link>
      <description><![CDATA[我能找到的只是使用tensorflow或pytorch的批量剧集的示例。我的实现永远不会收敛。   由   提交/u/emas_eht   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpw3bd/can_someone_point_me_to_a_simple_a2c_online/</guid>
      <pubDate>Thu, 28 Mar 2024 13:50:08 GMT</pubDate>
    </item>
    <item>
      <title>强化学习课程推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpvrbv/rl_course_recommendation/</link>
      <description><![CDATA[在理论和实践项目方面，学习强化学习/深度强化学习的最佳课程有哪些？    由   提交/u/Happy_Ad_3742   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpvrbv/rl_course_recommendation/</guid>
      <pubDate>Thu, 28 Mar 2024 13:34:27 GMT</pubDate>
    </item>
    <item>
      <title>用于能源管理的 DRL/寻找船舶发电机、电池和电网（充电站）之间的最佳负载平衡点，想法、示例、讨论。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bptiw5/drl_for_energy_managementfinding_optimal_load/</link>
      <description><![CDATA[   大家好。 我刚刚发现并加入了这个社区，希望我可以讨论我正在进行的研究生研究项目。希望任何人都可以提供想法、代码示例或想法。 问题已在本文标题中描述，我目前正在从事一个项目，该项目是管理对接船发电机负载平衡和-岸上电池和岸上电网。目标是找到最佳负载平衡点来决定/切换船舶发电机与岸上电池和岸上电网的连接，例如，如果船舶发电机每小时为船舶供电 50 千瓦，并且用岸上电池充电，但船每小时只需要25千瓦，电池可能会过载，那么什么时候切换连接岸上电网。文字描述可能比较混乱，请参见图1。 图1 研究项目/问题图。 我在这项研究中尝试使用DQN，但是，欢迎任何扩展组合DRL 算法建议。我尝试使用 DRL 示例将最优价格和最优利润应用于此问题，但似乎无济于事。我刚刚开始为这个研究项目学习强化学习和 DRL，尽管对 ML/DL 仍有一些了解，但我还是 RL/DRL 领域的新手，寻求建议、建议、任何代码示例或示例项目以及想法。  谢谢。   由   提交 /u/jyangcm   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bptiw5/drl_for_energy_managementfinding_optimal_load/</guid>
      <pubDate>Thu, 28 Mar 2024 11:43:52 GMT</pubDate>
    </item>
    <item>
      <title>无限地平线环境的决策转换器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bppj26/decision_transformer_for_infinite_horizon_env/</link>
      <description><![CDATA[我正在尝试将决策转换器应用于我的工作（优化网络流量控制算法）。 -状态：网络流量统计 -奖励函数/目标：奖励函数最小化网络流量延迟并最大化吞吐量。 -操作：选择最佳传输速率 除非用户停止算法，否则不存在终止状态。  我认为没有折扣因子的所有状态的累积奖励总是无限值。 在这种情况下，我可以使用决策转换器吗？  如何正确选择RTG（rewards-to-go）？   由   提交 /u/Final-Confusion4484    reddit.com/r/reinforcementlearning/comments/1bppj26/decision_transformer_for_infinite_horizo​​n_env/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bppj26/decision_transformer_for_infinite_horizon_env/</guid>
      <pubDate>Thu, 28 Mar 2024 07:21:05 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习模拟火箭着陆</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bphg59/model_rocket_landing_using_reinforcement_learning/</link>
      <description><![CDATA[我希望通过 Arduino / Raspberry Pi 实现具有机器学习功能的模型火箭着陆。我想知道我需要哪些部件以及需要使用哪些技术来实现这一目标（如果可能的话）。我没有火箭或 Arduino 方面的经验，所以我很好奇是否可以使用强化学习模型来控制火箭。   由   提交/u/xxspicymilkxx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bphg59/model_rocket_landing_using_reinforcement_learning/</guid>
      <pubDate>Thu, 28 Mar 2024 00:07:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO 中的 log_prob 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpg1a4/issue_with_log_prob_in_ppo/</link>
      <description><![CDATA[您好， 我目前正在尝试应用具有操作屏蔽的单个代理 PPO，以从合法的操作中选择一个操作eadch 决策时间。这给了我们不同大小的 log_probs 轨迹，导致以下错误： ---&gt;优点 = 返回 - torch.stack(log_probs).detach()  运行时错误：堆栈期望每个张量大小相等，但在条目 0 处得到 [3]，在条目 2 处得到 [2]   您对此有什么解决方案吗？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpg1a4/issue_with_log_prob_in_ppo/</guid>
      <pubDate>Wed, 27 Mar 2024 23:08:10 GMT</pubDate>
    </item>
    <item>
      <title>AMD 的 ROCm 与 NVidia 的 CUDA 进行强化学习比较？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bp67b9/amds_rocm_vs_nvidias_cuda_for_reinforcement/</link>
      <description><![CDATA[我知道 CUDA 更成熟并且迄今为止使用最多，我只是好奇它们之间是否有任何可用的比较，或者是否有人知道如何使用它们可以堆叠吗？我知道在游戏中 7900xt 与 4080 差不多，但是在与人工智能相关的任务中这种差异如何扩大？基本上，HIP 有什么好处吗？或者您应该使用较旧的 10-20 系列 NVidia 卡而不是 ML/RL 的 7000 系列 AMD 卡   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bp67b9/amds_rocm_vs_nvidias_cuda_for_reinforcement/</guid>
      <pubDate>Wed, 27 Mar 2024 16:31:52 GMT</pubDate>
    </item>
    <item>
      <title>训练四足机器人搬运包裹就像……</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bowv5a/training_fourlegged_robot_to_carry_a_package_be/</link>
      <description><![CDATA[    /u/IAmMiddy   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bowv5a/training_fourlegged_robot_to_carry_a_package_be/</guid>
      <pubDate>Wed, 27 Mar 2024 08:26:47 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>