<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Tue, 25 Feb 2025 01:19:09 GMT</lastBuildDate>
    <item>
      <title>DDPG问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ixh6k0/ddpg_issue/</guid>
      <pubDate>Tue, 25 Feb 2025 00:02:36 GMT</pubDate>
    </item>
    <item>
      <title>地平线很长的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix9yx3/environments_with_extremely_long_horizons/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi ash all  我正在尝试查找具有剧集的环境，这些剧集要完成数以万计的步骤完成。 Starcraft 2（数千），DOTA 2（20K）和Minecraft（24K）属于这一类别。有人知道相关环境吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/lilhairdy     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix9yx3/environments_with_extremely_long_horizons/</guid>
      <pubDate>Mon, 24 Feb 2025 19:04:06 GMT</pubDate>
    </item>
    <item>
      <title>与RL一起使用的最佳机器人模拟器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我正在尝试模拟我的机器人必须与连接到末端效应器的传感器设备进行交互的环境，并使用RL进行读数。我希望然后在实际的硬件上使用这个训练有素的代理。您会推荐什么模拟器？我看过Pybullet和Guazebo。但是我不确定哪个似乎是最简单，最佳的方法，因为我在模拟方面几乎没有经验。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/bananaoramama   href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1ix8eux/best_robotic_simulator_to_to_use_with_with_rl/”&gt; [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</guid>
      <pubDate>Mon, 24 Feb 2025 18:01:00 GMT</pubDate>
    </item>
    <item>
      <title>奖励成型想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有一个奖励塑造形式的想法，想知道大家都在考虑它。 想象你有一个超级稀疏的奖励功能，例如+1赢得胜利和-1的损失，情节很长。这个奖励功能正是我们想要的。  当然，我们都知道稀疏的奖励功能很难学习。因此，引入密集的奖励功能似乎很有用。一个函数，表明我们的代理商正朝正确或错误的方向行驶。定义这样的奖励函数通常非常棘手，以与我们的真实奖励功能完全匹配，因此我认为暂时使用此奖励功能以最初使我们的代理在政策空间中大致适当的领域是有意义的。 作为免责声明，我必须说我没有阅读任何有关奖励成型的研究，所以如果我的想法很愚蠢，请原谅我。 我过去用DQN做的一件事 - 像算法一样在培训过程中，逐渐从一个奖励功能转移到另一个奖励功能。一开始，我使用了100％的致密奖励功能和稀疏的0％。一段时间后，我开始逐渐“退火”。这个比率直到我只使用真正的稀疏奖励功能。我看得很好。 我这样做的原因是“退火”。是因为我认为Q学习算法很难适应完全不同的奖励功能。但是我确实想知道退火率浪费了多少时间。我也不喜欢退火率是另一个超参数。 我的想法是将奖励函数的硬转换应用于演员批评算法。想象一下，我们将模型训练在密集的奖励功能上。我们假设我们得出了一项体面的政策，也是评论家的体面价值估计。现在，我们要做的就是冻结演员，硬击奖励功能，并重新审查评论家。我认为我们可以消除高参数，因为现在我们可以训练，直到评论家的错误达到一定的门槛为止。我想这是一个新的超参数。无论如何，我们会解开演员并恢复正常的培训。 我认为这在实践中应该很好。我还没有机会尝试。你们都对这个想法有何看法？有什么理由期望它行不通吗？我不是演员 - 批评算法的专家，所以这个想法甚至没有意义。 让我知道！谢谢。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</guid>
      <pubDate>Mon, 24 Feb 2025 15:12:25 GMT</pubDate>
    </item>
    <item>
      <title>200 for LLM FINETUNTINING的200个组合身份和定理数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix488a/200_combinatorial_identities_and_theorems_dataset/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/databaebee     [link]  ＆＃32;   [注释]    /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix488a/200_combinatorial_identities_and_theorems_dataset/</guid>
      <pubDate>Mon, 24 Feb 2025 15:09:58 GMT</pubDate>
    </item>
    <item>
      <title>Simbav2：可扩展深度增强学习的超透明标准化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</link>
      <description><![CDATA[     引入 simbav2！   📄项目页面： https://dojeon-ai.github.io/simbav2/  📄纸： https://arxiv.org/abs/2502.15280   🔗代码： https://github.com/dojeon-ai/simbav2     simbav2是一种简单，可扩展的RL体系结构通过简单地用Simbav2替换MLP，&lt; /strong&gt;。&lt; /strong&gt;。演员评论家在57个连续的控制任务（Mujoco，dmcontrol，Myosuite，humyoid-Bench）中实现了最先进的表现（SOTA）。  它与体育馆1.0.0 API   - 尝试一下！ ，如果您有任何疑问，请随时与之伸出援手：） &lt; /div&gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/joonleesky     [link]     [注释]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</guid>
      <pubDate>Mon, 24 Feb 2025 11:43:23 GMT</pubDate>
    </item>
    <item>
      <title>写了我关于生锈的强化学习的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwyveb/wrote_my_thesis_on_reinforcement_learning_in_rust/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/dashdeckers     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwyveb/wrote_my_thesis_on_reinforcement_learning_in_rust/</guid>
      <pubDate>Mon, 24 Feb 2025 10:18:10 GMT</pubDate>
    </item>
    <item>
      <title>我的张板的主要问题！请帮助我</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwyefh/major_issue_with_my_tensorboard_pls_help_me/</link>
      <description><![CDATA[      i我正在训练RL算法，并在张板中记录结果。我是Tensorboard的新手。当我仅记录数据时，我不知道时，我不知道时情节回报和长度是故障或错误。问题在于，日志以0个步骤开始，图表可以在100万步中可以使用，之后的奖励却以100万个IE的差距移动。最后一个数据仅具有20m至21m的图形。  我不知道我是什么错误的事情，你们可以指导我吗？  导入登录从torch.utils.utils.tensorboard import import import import os导入imports imports imports import import inct logger：def __init __（self，run_name，args）：self.log_name = f&#39;logs/{run_name}&#39;self.start_time = time.time（）self.n_eps = 0 os.makedirs（&#39;logs&#39;，stef_ok = true）os.makedirs（&#39;models&#39;，equent_ok = true）self.writer = summaryWriter（self.log_name）logging.basicconfig（level = = logging.debug，格式=&#39;％（asctime）s％（消息）s&#39;，handlers = [ logging.streamhandler（），logging.filehandler（f&#39;{self.log_name} .log&#39;，＆quot&#39;a＆quot;＆quot;＆quot;＆quort;]，]，datefmt =&#39;％y/％m/％m/％d％i：％m：％m：％s％s％p p p &#39;）logging.info（args）def log_scalars（self，scalar_dict，step）：对于键，val in scalar_dict.items（）：self.writer.add_scalar（key，val，step）def log_episode（self，info，step）：rewards = info = info [＆quort returns/epoindic_reward＆quorts;] length = infor = info = info = info; ＃使用长度而不是奖励完成的track情节=长度=长度＆gt; 0对于i在范围内（len（rewards）））：如果完成了_episodes [i]：self.n_eps += 1 powers_data = {＆querts; returns/ependodic_reward＆quot＆quot＆quot＆quot＆quot; } self.log_scalars（emotive_data，step）time_expired =（time.time（） - self.start_time） / 60 /60 logging.info（f＆quot; gt; ep = {self.n_eps} |总步骤= {step}＆quet; f＆quets; f＆quest; | reward = {rewards [rewards [i]} |长度}＆quot;我用来这样做的代码。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/decter_prune_9756      [link]   /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwyefh/major_issue_with_my_tensorboard_pls_help_me/</guid>
      <pubDate>Mon, 24 Feb 2025 09:46:11 GMT</pubDate>
    </item>
    <item>
      <title>如何掌握强化学习的可能性？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwwpmo/how_to_master_probability_for_reinforcement/</link>
      <description><![CDATA[在我的概率技能不是他们需要的地方。我在本科期间参加了一个概率课程，但是我忘记了大部分内容。 我不仅想刷新我的记忆，我想变得擅长于概率，直到我可以直观地将其应用于RL和机器学习的其他领域。 对于那些掌握概率的人来说，最适合您的人？有什么书籍，课程，问题集或每日习惯会带来很大的不同吗？ 很想听听您的建议！  &lt;！ -  sc_on-&gt;＆＃32 ;提交由＆＃32; /u/u/hudhuddz     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwwpmo/how_to_master_probability_for_reinforcement/</guid>
      <pubDate>Mon, 24 Feb 2025 07:43:38 GMT</pubDate>
    </item>
    <item>
      <title>我应该选择什么研究问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrokr/what_research_problem_should_i_pick/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是RL的新手，但是我处于需要立即为研究的情况下选择一个好的问题陈述。我试图浏览会议的论文以快速选择一些东西。是否可以研究任何特定的问题陈述？我只是在寻找经验丰富的人的线索。谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrokr/what_research_problem_should_i_pick/</guid>
      <pubDate>Mon, 24 Feb 2025 02:41:32 GMT</pubDate>
    </item>
    <item>
      <title>目前最紧迫的问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrkev/most_pressing_problems_currently/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  目前RL中最紧迫的问题是什么？是否有任何方法可以显示出良好的解决这些方法？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/uiaslight     [link]   ＆＃32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrkev/most_pressing_problems_currently/</guid>
      <pubDate>Mon, 24 Feb 2025 02:35:40 GMT</pubDate>
    </item>
    <item>
      <title>RL对于AGI，重点应该放在什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  那些认为RL是通往AGI的可行途径的人，当前需要专注于在RL中求解的当前局限性是什么？人们可以选择为此做出哪些研究问题？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</guid>
      <pubDate>Mon, 24 Feb 2025 02:33:15 GMT</pubDate>
    </item>
    <item>
      <title>帮助尝试了解SARSA半梯度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwqh6f/help_on_trying_to_understand_sarsa_semi_gradient/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  嘿，大家， 我是ML/AI爱好者，RL一直是我忽略的一周。我发现该算法很难解密，但是在阅读了LLM Architecture背后的论文后，我注意到其中很多倾向于经常使用RL概念。这让我意识到这是一个我不能真正忽略的领域。 为此，我一直在慢慢地通过Barto和Sutton的书进行仔细观察，我可以免费在线找到这些书籍。目前，我正在第10章中，我希望在结束时我应该能够利用我的其他AI/ML项目的经验来制作一些AI来玩一些尚未有AI项目的游戏，例如Spelunky或PVZ英雄。 当我阅读每个部分时，为了确保我了解算法和动力，我尝试使用这本书建议的算法对婴儿问题进行编码。我遇到的近期是SARSA半梯度。    我制作了一个非常简单的游戏，灵感来自Openai Mountain Car游戏，相反，您实际上只需要ASCII即可代表州和地形。代理商从左侧的A点开始，目标是到达B点，这一直在右侧。在路径中，代理可能会遇到向前（/）或向后（\）的斜率。这些可以允许代理分别获得或失去动力。还应注意，代理商的汽车的发动机非常薄。走下坡路，汽车可以加速进一步的动力，但是如果上坡，发动机的功率为零。 目标是以零动量到达B点，以获得积极的奖励和最终状态。其他终端状态包括过早达到零动量或撞击地形末端。这辆车还因试图保持低位而获得奖励。 我的实施可以在此处找到： rl_concepts/rollingcar.ipynb在main· JJ8428/rl_concepts   我发布的原因是我的代理人并没有真正学习如何解决游戏。我不确定这是游戏设计不佳的情况，游戏是否太复杂而无法用一层权重解决，或者我对算法的实现是否错误。从网上浏览中，我看到人们已经解决了sarsa semi grad的OpenAi MountainCar问题，到目前为止尚无n步，所以我有信心也可以解决这个游戏。 可以解决有人请去看我的代码，告诉我是否不知所措？我的代码不长，任何帮助或指针都将不胜感激。如果我的代码超级凌乱且不可读，请告诉我。可悲的是，自从我在Python重新访问OOP以来已经很久了。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/apricotslight9728     [links]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwqh6f/help_on_trying_to_understand_sarsa_semi_gradient/</guid>
      <pubDate>Mon, 24 Feb 2025 01:40:10 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的RL：开环控制是亚最佳选择，因为..？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在观看Sergei Levine的讲座。他是一个很好的资源；将学习理论的联系起来很多。使用数学测试的类比，通过开环控制通过开环控制是亚最佳的。我想象这个类比就像搜索树一样例如，但即使如此，它也有点清除。但是，要与抽象的例子保持在一起，为什么该模型不会基于以前与环境互动的经验产生可能性？ Sergei提到，如果我们选择测试，我们将得到正确的答案，但也意味着没有办法将这些信息传递给模型（在这种情况下，代理商的决策者）。感觉从现实中消除了，即如果可能的测试尺寸足够大，那么最佳的动作就是回家。如果您对参加测试的能力有任何信心（例如以前的推出经验），那么您的最佳策略会更改，但这是信息，您可以通过与以前的示例相同的分布来理解。  也许我缺少标记。为什么开放环控制次优？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iwhd9t/model_based_rl_rl_openloop_iscontrol_is_suboptimal/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1iwhd9t/model_based_rl_rl_openloop_control_is_is_suboptimal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</guid>
      <pubDate>Sun, 23 Feb 2025 18:52:18 GMT</pubDate>
    </item>
    <item>
      <title>学习政策以最大化满足B</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在尝试学习一个控制策略，该策略在确保条件B时最大化变量。例如，机器人在将速度保持在给定范围内（b）的同时最大化能源效率（a）。 我的想法：将奖励定义为a *（b）。当B被满足B时，奖励将为= A，并且在违反B时为= 0。但是，这可能会在培训的早期引起稀疏的回报。我可能会使用模仿学习来初始化策略来帮助解决此问题。 是否有适合此类问题的现有框架或技术？我非常感谢任何方向或相关关键字！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iw3ijl/learning_policy_to_to_maximize_a_a_a_a_while_satisfying_b/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</guid>
      <pubDate>Sun, 23 Feb 2025 06:05:48 GMT</pubDate>
    </item>
    </channel>
</rss>