<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚çš„ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä½³åœ°è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Wed, 13 Mar 2024 06:18:15 GMT</lastBuildDate>
    <item>
      <title>å¦‚ä½•è§£é‡Š Epoch 0 ä¸Šçš„å¹³å‡å‰§é›†å¥–åŠ±å·®å¼‚ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdhc2n/how_to_interpret_mean_episode_reward_differences/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨å…³æ³¨æ­¤ PyTorch æ•™ç¨‹ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡/PPO åœ¨ RL ç¯å¢ƒä¸­è®­ç»ƒä¸åŒçš„ç½‘ç»œæ¶æ„ã€‚æˆ‘æƒ³æ¯”è¾ƒä¸åŒé£æ ¼ç½‘ç»œçš„æ€§èƒ½ï¼Œå› æ­¤æˆ‘ç›®å‰å•ç‹¬è®­ç»ƒæ¯ä¸ªç½‘ç»œå¹¶ä¿å­˜æ¯ä¸ªè®­ç»ƒæ—¶æœŸçš„å¹³å‡æƒ…èŠ‚å¥–åŠ±å€¼ï¼›æˆ‘çš„æƒ³æ³•æ˜¯ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸€äº›ç½‘ç»œä¼šå­¦ä¹ å¾—æ›´å¿«ï¼Œå¹¶åœ¨å¹³å‡æƒ…èŠ‚å¥–åŠ±ä¸­è¡¨ç°å‡ºæ€¥å‰§çš„è·³è·ƒï¼Œè€Œå…¶ä»–ç½‘ç»œå¯èƒ½ä¼šæœ‰æ›´å¹³å¦çš„æ›²çº¿ã€‚ä½†æ˜¯ï¼Œæˆ‘çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š â€‹ X è½´ï¼šè®­ç»ƒçºªå…ƒ#ï¼ŒY è½´ï¼šæ¯é›†å¹³å‡å¥–åŠ± ä¼¼ä¹ä¸€ä¸ªç½‘ç»œçš„è¡¨ç°å·²ç»æ¯”ä¹‹å‰å¥½è¿‘ 4 å€å¦ä¸€ä¸ªæ˜¯ä» Epoch 0 å¼€å§‹è®­ç»ƒã€‚æˆ‘å¯¹ GAE loss / PPO å·¥ä½œåŸç†çš„ç†è§£æ˜¯ï¼Œé¦–å…ˆæ”¶é›†æ¯æ‰¹çš„æ‰€æœ‰å¸§ï¼Œç„¶åä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚æ›´æ–°åçš„å‚æ•°ç”¨äºä¸‹ä¸€è½®æ•°æ®æ”¶é›†ã€‚è¿™è®©æˆ‘å¯¹å¦‚ä½•è§£é‡Šè“è‰²ç½‘ç»œåœ¨ Epoch 0 ä¸Šçš„ç›¸å¯¹æˆåŠŸæ„Ÿåˆ°å›°æƒ‘ï¼ˆå¦‚æœå‚æ•°çš„æ›´æ–°ç›´åˆ°ä¸‹ä¸€è½®æ•°æ®æ”¶é›†æ‰ç”Ÿæ•ˆï¼‰ï¼Ÿè¿™æ˜¯å¦ä»…ä»…åæ˜ äº†ç½‘ç»œä¹‹é—´çš„æŸç§åˆå§‹åŒ–å·®å¼‚ï¼Œæˆ–è€…å®ƒä»¬æ˜¯å¦æœ‰å¯èƒ½â€œå­¦ä¹ â€äº†ç½‘ç»œä¹‹é—´çš„åˆå§‹åŒ–å·®å¼‚ï¼Ÿåœ¨è®¡ç®—ç¬¬ä¸€ä¸ªå¹³å‡å¥–åŠ±æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè®­ç»ƒå¾ªç¯ä»£ç åŸºæœ¬ä¸Šæ¥è‡ªé“¾æ¥çš„æ•™ç¨‹ï¼Œå¦‚ä¸‹ï¼š episode_reward_mean_list = [] for tensordict_data in Collector: with torch.no_grad(): GAE(tensordict_data, params=loss_module.critic_network_params, target_params =loss_module.target_critic_network_params, ) # è·å¾—ä¼˜åŠ¿ data_view = tensordict_data.reshape(-1) replay_buffer.extend(data_view) # é‡æ–°å¡«å……ç¼“å†²åŒº for ep in range(num_epochs): for _ in range(frames_per_batch // minibatch_size): subdata = replay_buffer .sample() loss_vals = loss_module(å­æ•°æ®) loss_value = ( loss_vals[&quot;loss_objective&quot;] + loss_vals[&quot;loss_critic&quot;] + loss_vals[&quot;loss_entropy&quot;] ) loss_value.backward() optim.step() optim.zero_grad ï¼ˆï¼‰collector.update_policy_weights_ï¼ˆï¼‰å®Œæˆ=tensordict_data.getï¼ˆï¼ˆâ€œä¸‹ä¸€ä¸ªâ€ï¼Œâ€œå®Œæˆâ€ï¼‰ï¼‰episode_reward_mean =ï¼ˆtensordict_data.getï¼ˆï¼ˆâ€œä¸‹ä¸€ä¸ªâ€ï¼Œâ€œepisode_rewardâ€ï¼‰ï¼‰.meanï¼ˆï¼‰.item () ) Episode_reward_mean_list.append(episode_reward_mean)    ç”±   æäº¤/u/brantacanadensis906   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdhc2n/how_to_interpret_mean_episode_reward_differences/</guid>
      <pubDate>Wed, 13 Mar 2024 03:22:21 GMT</pubDate>
    </item>
    <item>
      <title>ç”¨ä»£ç åˆ†äº«æˆ‘çš„å¼ºåŒ–å­¦ä¹ æ•™ç¨‹ç³»åˆ—ã€‚ä»åŸºç¡€çŸ¥è¯†å¼€å§‹ï¼Œä½¿ç”¨ PyTorch ä¸­çš„ PPO æ‰©å±•åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚è®©æˆ‘çŸ¥é“ä½ çš„æƒ³æ³•ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bdfxz2/sharing_my_tutorial_series_on_reinforcement/</link>
      <description><![CDATA[     &lt; /td&gt;  ç”±   æäº¤ /u/MrSirLRD   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bdfxz2/sharing_my_tutorial_series_on_reinforcement/</guid>
      <pubDate>Wed, 13 Mar 2024 02:17:57 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦ NEAT-python å¸®åŠ©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd9bga/need_help_with_neatpython/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•ä½¿ç”¨ç®€æ´çš„ python æ¥å®ç°åƒ BipedalWalker è¿™æ ·çš„ç®€å•ç¤ºä¾‹ï¼Œä½†è®­ç»ƒä»æœªå¼€å§‹ï¼Œç›¸åæˆ‘åªå¾—åˆ°ï¼š ****** Runningç¬¬ 0 ä»£ ******  è¿™æ˜¯ä»£ç ï¼š iå¯¼å…¥æ“ä½œç³»ç»Ÿ å¯¼å…¥pickle  å¯¼å…¥æ•´æ´ å°†numpyå¯¼å…¥ä¸ºnp #import cart_pole &lt; code&gt;å°†gymnasiumå¯¼å…¥ä¸ºgym  runs_per_net = 5 #ä½¿ç”¨NNç½‘ç»œè¡¨å‹å’Œç¦»æ•£æ‰§è¡Œå™¨åŠ›å‡½æ•°ã€‚ def eval_genome(genome, config): net=neat.nn.FeedForwardNetwork.create(genome, config)  &lt; p&gt;fitnesses = []  å¯¹äºèŒƒå›´å†…çš„è·‘æ­¥(runs_per_net)ï¼š env =gym.make(&quot; BipedalWalker-v3&quot;)  è§‚å¯Ÿï¼Œ_ = env.reset() fitness = 0.0 done = False æœªå®Œæˆæ—¶ï¼š  action = net.activate(observation) è§‚å¯Ÿã€å¥–åŠ±ã€å®Œæˆã€_,info = env.step(action) å¥èº«+=å¥–åŠ± #env.render () fitnesses.append(fitness)  return np.mean(fitnesses)  def eval_genomes(genomes, config): å¯¹äºgenome_idï¼ŒåŸºå› ç»„ä¸­çš„åŸºå› ç»„ï¼š genome.fitness = eval_genome(genome, config )  def run(): # åŠ è½½é…ç½®æ–‡ä»¶ï¼Œå‡å®šè¯¥æ–‡ä»¶ä¸æ­¤è„šæœ¬ä½äºåŒä¸€ç›®å½•ä¸­ã€‚  local_dir = os.path.dirname(__file__) config_path = os.path.join(local_dir, &#39;config.txt&#39;) config_path = os.path.join(local_dir, &#39;config.txt&#39;) code&gt; config = clean.Config(neat.DefaultGenome,neat.DefaultReProduction, neat.DefaultSpeciesSet,neat.DefaultStagnation, config_path)  pop=neat.Population(config) stats=neat.StatisticsReporter()&lt; br /&gt; pop.add_reporter(stats) pop.add_reporter(neat.StdOutReporter(True))  # Run NEAT ç®—æ³• winner = pop.run(eval_genomes)  # ä¿å­˜è·èƒœè€…ã€‚ with open(&#39;winner&#39;, &#39;wb&#39;) as f: pickle.dump(winner, f)  print(è·èƒœè€…ï¼‰ run() æˆ‘ç”¨äº†è¿™ä¸ªæ•™ç¨‹ã€‚é…ç½®æ–‡ä»¶åªæœ‰ pop å¤§å°ä» 250 æ›´æ”¹ä¸º 20ï¼Œä½†æˆ‘ä¸æ˜ç™½ä¸ºä»€ä¹ˆå®ƒä¸è®­ç»ƒ   ç”±   æäº¤ /u/SebyR   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd9bga/need_help_with_neatpython/</guid>
      <pubDate>Tue, 12 Mar 2024 21:34:35 GMT</pubDate>
    </item>
    <item>
      <title>Deepmindï¼šåœæ­¢å›å½’ï¼šé€šè¿‡å¯æ‰©å±•æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„åˆ†ç±»è®­ç»ƒä»·å€¼å‡½æ•°</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd7frj/deepmind_stop_regressing_training_value_functions/</link>
      <description><![CDATA[ ç”±   æäº¤/u/LushousLightfoot  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd7frj/deepmind_stop_regressing_training_value_functions/</guid>
      <pubDate>Tue, 12 Mar 2024 20:20:42 GMT</pubDate>
    </item>
    <item>
      <title>å¤§å¤šæ•° RL åº“ä¸­çš„æ§åˆ¶æ–¹å‘ä¼¼ä¹æ˜¯ç›¸åçš„</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd5b2f/the_direction_of_control_in_most_rl_libraries/</link>
      <description><![CDATA[çœ‹çœ‹åƒ Stable Baseline 3 è¿™æ ·çš„åº“ï¼Œåœ¨æˆ‘çœ‹æ¥ï¼Œæ§åˆ¶çš„æ–¹å‘ä¸åº”æœ‰çš„æ–¹å‘ç›¸åã€‚ æ®æˆ‘æ‰€çŸ¥ï¼Œæˆ‘çœ‹åˆ°çš„å‡ ä¹æ‰€æœ‰ RL ç¤ºä¾‹éƒ½å‡è®¾æœ‰ä¸€ä¸ªæ˜ç¡®å®šä¹‰çš„â€œç¯å¢ƒâ€ã€‚é£æ ¼å¯¹è±¡èƒ½å¤Ÿè¯„ä¼°åŠ¨ä½œå¹¶ä½œä¸ºå•ä¸ªç»Ÿä¸€å‡½æ•°äº§ç”Ÿå¥–åŠ±ï¼ˆä¾‹å¦‚ SB3 çš„ env è§„èŒƒçš„ stepï¼‰ ä¼¼ä¹è¿˜æœ‰ä¸€ä¸ªå…³äºè®­ç»ƒçš„å‡è®¾ï¼Œç„¶åéƒ¨ç½²ï¼ˆä»¥â€œèµ¢å¾—ç«äº‰â€çš„æ–¹å¼ï¼‰ã€‚ ç°åœ¨ï¼Œè¿™å¹¶æ²¡æœ‰ä»æ ¹æœ¬ä¸Šé™åˆ¶äººä»¬å¯ä»¥ä½¿ç”¨ SB3 ä¹‹ç±»çš„ä¸œè¥¿åšä»€ä¹ˆï¼Œä½†å®ƒä½¿â€œç°å®ä¸–ç•Œâ€æˆä¸ºç°å®ã€‚ç”¨ä¾‹æ›´åŠ å›°éš¾ã€‚  å‡è®¾ä¾‹å¦‚æ§åˆ¶æœºå™¨äººï¼Œå…¶ä¸­æœºå™¨äººçš„â€œåŠ¨ä½œâ€æ˜¯ç”±æœºå™¨äººæ‰§è¡Œçš„ã€‚å¯èƒ½æœ‰è®¸å¤šåˆ©ç›Šç›¸å…³è€…ï¼ˆè¿œç¨‹å‘½ä»¤ã€ç´§æ€¥åœæ­¢ã€å–ä»£ RL æ“ä½œçš„ç¡¬ç¼–ç è§„åˆ™ã€ä¿®æ”¹æ“ä½œçš„ç¡¬ç¼–ç çº¦æŸ...ç­‰ï¼‰ åœ¨è¿™ç§ç¯å¢ƒï¼ˆä»»ä½•ç°å®ä¸–ç•Œçš„åº”ç”¨ç¨‹åºï¼‰ä¸­ï¼Œå®ƒå°† RL ç¯å¢ƒä½œä¸ºä¸€ç§â€œæœåŠ¡â€æ˜¯æœ‰æ„ä¹‰çš„ã€‚æ ·å¼å®ä½“ï¼Œè€Œä¸æ˜¯æœ€é«˜çº§åˆ«çš„ç¼–æ’å™¨ - å³å…¬å¼€è¯¸å¦‚ recommend_action(inputs) ï¼ˆæˆ– predict(inputs)ï¼‰å’Œ reward(å¥–åŠ±ï¼Œ[prev_inputsï¼Œprev_outputs]ï¼‰ï¼ˆæˆ–stepæˆ–train ...åç§°ä¸é‚£ä¹ˆç›¸å…³ï¼‰ æˆ‘æ˜¯95% ç¡®ä¿¡â€œæˆ‘ä¸æ˜ç™½â€å‡ºäºæŸç§åŸå› ï¼Œç°åœ¨å¸¸è§çš„æ¥å£è¦ä¹ˆæ›´å¥½ï¼Œè¦ä¹ˆå¿…è¦ï¼Œä½†å¦‚æœæœ‰äººå¯ä»¥å¸®åŠ©å‡è½»æˆ‘å¯¹è¿™äº›è®¾è®¡é€‰æ‹©çš„å›°æƒ‘ï¼Œæˆ‘å¯èƒ½ä¼šæ›´å¤šåœ°äº†è§£ RL åº“ç”Ÿæ€ç³»ç»ŸåŠå…¶çº¦æŸæ¡ä»¶ã€‚è¿›åŒ–äº†ã€‚   ç”±   æäº¤/u/elcric_krej   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd5b2f/the_direction_of_control_in_most_rl_libraries/</guid>
      <pubDate>Tue, 12 Mar 2024 18:57:01 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘åˆ¶ä½œè¿™ä¸ªå¹¶è¡Œæ¨¡æ‹Ÿç®¡ç†å™¨å¾ˆæœ‰è¶£ï¼Œæ‰€ä»¥æˆ‘æƒ³åˆ†äº«å®ƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd1xyb/i_had_fun_making_this_parallel_simulation_manager/</link>
      <description><![CDATA[   /u/hbonnavaud  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd1xyb/i_had_fun_making_this_parallel_simulation_manager/</guid>
      <pubDate>Tue, 12 Mar 2024 16:46:16 GMT</pubDate>
    </item>
    <item>
      <title>PPO å¤šç¦»æ•£è¾“å‡º</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bd1wo4/ppo_multidiscrete_output/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘å¾ˆå¥½å¥‡æ˜¯å¦æœ‰äººå¯¹å¦‚ä½•æ”¹è¿› PPO ç®—æ³•ä»¥è¾“å‡ºå¤šç¦»æ•£æœ‰ä»»ä½•æç¤ºã€‚æˆ‘ç›®å‰æ­£åœ¨å°è¯•åšçš„æ˜¯åˆ›å»ºä¸€ä¸ªâ€œç»ç†â€æŒ‡å¯¼å…¶ä»–ä»£ç†å®Œæˆå“ªä¸ªä»»åŠ¡çš„ä»£ç†ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘éœ€è¦åŠ¨ä½œ 0ã€1 æˆ– 2 çš„ 4 ä¸ªä¸åŒè¾“å‡ºã€‚ä½†æˆ‘ä¸å¤ªç¡®å®šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä¸èƒ½åªåšä¸€ä¸ª softmax å±‚ã€‚æˆ‘å°è¯•è¿‡å»ºç«‹ä¸€ä¸ªå¤šå¤´ç½‘ç»œï¼Œåœ¨ 4 ä¸ªä¸åŒçš„å¤´ä¸Šæ‰§è¡Œ softmax å¹¶è¿”å›æ¯ä¸ªå¤´çš„ argmaxï¼Œä½†éšåæˆ‘æ‹…å¿ƒ 4 ä¸ªä¸åŒå¤´çš„æŸå¤±å‡½æ•°ã€‚ ä»»ä½•çš„æ„è§éƒ½å°†ä¼šæœ‰å¸®åŠ©ï¼è°¢è°¢ï¼   ç”±   æäº¤/u/Cheap_Leather_6432   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bd1wo4/ppo_multidiscrete_output/</guid>
      <pubDate>Tue, 12 Mar 2024 16:44:53 GMT</pubDate>
    </item>
    <item>
      <title>å°†çŸ©å½¢æ”¾ç½®åœ¨åŒºåŸŸï¼ˆ2Dï¼‰ä¸­ï¼ŒåŒæ—¶ä¼˜åŒ–å¤šä¸ªåŠŸèƒ½å¹¶é¿å…å®šä¹‰åŒºåŸŸ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcy873/placing_rectangles_in_area_2d_while_optimizing/</link>
      <description><![CDATA[æ‚¨å¥½ï¼Œ æˆ‘ä»¬æœ‰ä¸€ä¸ªå¯ä»¥æ”¾ç½®å¯¹è±¡ï¼ˆæ¥è‡ªå®šä¹‰çš„å¯¹è±¡åˆ—è¡¨ï¼‰çš„åŒºåŸŸã€‚è¯¥åŒºåŸŸç”±ç‚¹å®šä¹‰ï¼ˆä¸ºç®€å•èµ·è§ï¼Œå‡è®¾è¿æ¥çš„å››ä¸ªç‚¹æ˜¾ç¤ºä¸€ä¸ªæ­£æ–¹å½¢ï¼‰ã€‚åœ¨è¿™ä¸ªåŒºåŸŸä¸­ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æœ‰ä¸€äº›åŒºåŸŸï¼Œå…¶ä¸­ä¸èƒ½æ”¾ç½®å¯¹è±¡ï¼ˆä¹Ÿä¸èƒ½é‡å ï¼‰ã€‚è¿™è®©äººæƒ³èµ·äºŒç»´è£…ç®±é—®é¢˜ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬æ”¾ç½®çš„ç‰©å“ä¹Ÿæœ‰ä»·æ ¼å’Œå…¶ä»–å› ç´ ã€‚æˆ‘ä»¬å¸Œæœ›ä»¥ä¼˜åŒ–æ‰€æœ‰è¿™äº›å› ç´ ï¼ˆä½ä»·ç­‰...ï¼‰çš„æ–¹å¼æ”¾ç½®å¯¹è±¡ã€‚æˆ‘è¿‡å»åšè¿‡å¾ˆå¤šæœºå™¨å­¦ä¹ ï¼Œä½†ä»æœªåšè¿‡å¼ºåŒ–å­¦ä¹ ã€‚é¦–å…ˆï¼šè¿™æ˜¯ä¸€ä¸ª RL æ˜¯æœ€ä½³è§£å†³æ–¹æ¡ˆçš„é—®é¢˜å—ï¼Ÿå¯¹æˆ‘æ¥è¯´ä¼¼ä¹æ˜¯è¿™æ ·ï¼Œä½†æˆ‘ä¸ç¡®å®š...... æ‚¨å¯¹è§£å†³è¿™ä¸ªé—®é¢˜æœ‰ä»€ä¹ˆå»ºè®®æˆ–æŠ€å·§å—ï¼Ÿæˆ‘æ­£åœ¨è€ƒè™‘ä¸€ç§ç®—æ³•ï¼Œè¯¥ç®—æ³•é¦–å…ˆéšæœºæ”¾ç½®å¯¹è±¡ï¼ˆåœ¨å¯ä»¥æ”¾ç½®å¯¹è±¡çš„åŒºåŸŸå†…ï¼‰ï¼ŒåŒæ—¶å› ä»·æ ¼å’Œå…¶ä»–å¯ä»¥ä»è¾“å‡ºä¸­æ¨è¿Ÿçš„äº‹æƒ…è€Œå—åˆ°æƒ©ç½šï¼Œä»¥ä¾¿å®ƒå­¦ä¹ ... &lt; p&gt;â€‹ ä½ ä»¬è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ æå‰è°¢è°¢ï¼   ç”±   æäº¤ /u/Lucky_Funny_3259   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcy873/placing_rectangles_in_area_2d_while_optimizing/</guid>
      <pubDate>Tue, 12 Mar 2024 14:13:01 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨isaacgymenvä¸­ï¼Œå¦‚ä½•æŒ‡å®šstateå’Œactionä¸­å…³èŠ‚çš„é¡ºåºï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bctil2/in_isaacgymenv_how_to_specify_the_order_of_the/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘æ˜¯ RL æ–°æ‰‹ï¼Œåˆšåˆšå¼€å§‹ä½¿ç”¨ isaacgymenv å’Œ isaacgymã€‚ åœ¨æ§åˆ¶ CartPole æˆ– Franka ç­‰æœºå™¨äººæ—¶ï¼Œå¦‚ä½•ç¡®å®šè¿”å›çš„è§‚å¯Ÿæˆ–æä¾›çš„æ“ä½œä¸­çš„å“ªä¸ªå…ƒç´ å¯¹åº”äº URDF æ–‡ä»¶ä¸­å®šä¹‰çš„å…³èŠ‚ï¼Ÿ  ä¾‹å¦‚ï¼Œåœ¨IsaacgymEnvs/isaacgymenvs/tasks/cartpole.pyä¸­ ï¼ˆå¦‚å›¾æ‰€ç¤ºï¼‰ï¼Œ&#39;self.obs_buf&#39;ä¸­çš„å„ä¸ªå…ƒç´ æ˜¯å¦‚ä½•å®ç°çš„ &lt; p&gt;å¯¹åº”äºcartpole.urdfä¸­å®šä¹‰çš„å…³èŠ‚ã€‚ è°¢è°¢ï¼ â€‹  &amp; #32ï¼›ç”±   æäº¤/u/Slight_Rip_516   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bctil2/in_isaacgymenv_how_to_specify_the_order_of_the/</guid>
      <pubDate>Tue, 12 Mar 2024 09:58:43 GMT</pubDate>
    </item>
    <item>
      <title>å¡ç‰Œæ¸¸æˆçš„æç¤ºå’ŒæŒ‡ç¤º</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bct4re/tips_and_pointers_for_card_game/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘æ‰“ç®—åˆ¶ä½œä¸€ä¸ª RL æ¨¡å‹æ¥ç©æ¾³é—¨ã€‚è¿™æ˜¯æˆ‘çš„ç¬¬ä¸€ä¸ªé¡¹ç›®ï¼Œæ‰€ä»¥æˆ‘å°†ä¸èƒœæ„Ÿæ¿€ä»»ä½•å¯ä»¥å¸®åŠ©æˆ‘è®¾è®¡ä¸œè¥¿çš„æŒ‡ç¤ºå’ŒæŠ€å·§ã€‚ä¹Ÿè®¸æ‚¨ä¼šå‘ç°ä¸€äº›ç±»ä¼¼çš„è®¾è®¡è§†é¢‘ã€‚å¯èƒ½ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€äº›æ–‡çŒ®ï¼Œä½†å¾ˆéš¾æ‰¾åˆ°èƒ½å¤Ÿæ¶µç›–æˆ‘å®é™…ä¸Šè®¤ä¸ºæœ‰ç”¨çš„å†…å®¹çš„æ–‡çŒ®ã€‚ æœ‰ Unoï¼ˆæ¾³é—¨çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œéå¸¸éšæœºï¼Œç©å®¶çš„é€‰æ‹©å¹¶ä¸é‡è¦ï¼‰åœ¨æ¾³é—¨æœ‰è¿™æ ·çš„å½±å“ï¼‰æ¨¡å‹å·²ç»åœ¨ç½‘ä¸Šå®Œæˆï¼Œä½†å®ƒå¤ªç®€å•äº†ã€‚æˆ‘éœ€è¦æ›´å¤æ‚çš„ä¸€å¼ ï¼Œæˆ‘ä»€è‡³ä¸çŸ¥é“æ˜¯å¦å¯ä»¥åšåˆ°...æˆ‘è®¤ä¸ºå¯¹äºæ¾³é—¨ï¼Œæˆ‘éœ€è¦ä¸ºä»£ç†åˆ¶ä½œä¸€å¼ æ¡Œå­æ¥æ”¶é›†æ¯å¼ ç‰Œçš„çŠ¶æ€ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼šçº¢å¿ƒä¹‹ç‹[æ˜¯å¦åœ¨æ‰‹ï¼Œå¦‚æœåœ¨æ‰‹ç°åœ¨å¯ä»¥ç©å—ï¼Œå¦‚æœä¸èƒ½ç›´æ¥ç©å¯ä»¥æ‰¹é‡ç©å—ï¼ˆæ¾³é—¨å…è®¸ä¸€æ¬¡ç©å¤šå¼ ç›¸åŒå€¼çš„ç‰Œï¼‰ï¼Œè¢«å¯¹æ‰‹æŒæœ‰çš„æ¦‚ç‡ï¼Œåœ¨æ ˆé¡¶ï¼Œä½¿ç”¨å¹¶åœ¨å †æ ˆä¸­ï¼ˆé‡æ–°æ´—ç‰Œåä¼šé‡ç½®ï¼‰]æˆ‘å¯¹å¦‚ä½•è®¾è®¡è¯¥éƒ¨åˆ†æœ‰ä¸€å®šçš„äº†è§£ï¼Œä½†å®Œå…¨ä¸çŸ¥é“å¦‚ä½•ç®¡ç†ä»£ç†æ‰€åšçš„é€‰æ‹©...è€Œåœ¨ Uno ä¸­ï¼Œå°±åƒé€‰æ‹©äº†ä¸€å¼ å¯ç©çš„ç‰Œæˆ–æŠ½äº†ä¸€å¼ ç‰Œåœ¨æ¾³é—¨æ‰“ç‰Œæ›´åƒæ˜¯æŠ½ä¸€å¼ ç‰Œï¼ˆä¸ç®¡æ‰‹ä¸Šæ˜¯å¦æœ‰å¯ä»¥æ‰“çš„ç‰Œï¼ˆæœ‰æ—¶è¿™æ¯”æ‰“å‡ºä¸€å¼ ä½ èƒ½æ‰“çš„ç‰Œæ›´å¥½ï¼‰ï¼‰ï¼Œæ‰“è¿åŠ¨å‘˜å¹¶è®¾å®šä»·å€¼è¦æ±‚ï¼Œæ‰“Aå¹¶è®¾å®šé¢œè‰²è¦æ±‚ï¼Œç©æˆ˜æ–—å¡ï¼ˆå°±åƒunoä¸­çš„é€šé…ç¬¦ï¼Œä½†å¯ä»¥è¢«åå‡»ï¼‰ã€‚ æˆ‘è¶Šæƒ³å®ƒå°±è¶Šå¤æ‚ï¼Œä¸€ä¸ªæœˆå†…æœ‰æœ€åæœŸé™ï¼Œéœ€è¦ä¸€äº›å¸®åŠ©ğŸ™ &lt; /div&gt;  ç”±   æäº¤/u/Comfortable_Sleep988  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bct4re/tips_and_pointers_for_card_game/</guid>
      <pubDate>Tue, 12 Mar 2024 09:32:40 GMT</pubDate>
    </item>
    <item>
      <title>Sutton çš„ã€Šå¼ºåŒ–å­¦ä¹ ï¼šç®€ä»‹ã€‹ç¬¬äºŒç‰ˆæ›´å¥½å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcrpib/is_the_2nd_edition_of_reinforcement_learning_an/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘æƒ³é˜…è¯»å¼ºåŒ–å­¦ä¹ ï¼šè¨é¡¿å’Œå·´æ‰˜çš„ç®€ä»‹ã€‚ç„¶è€Œï¼Œè¾ƒæ–°çš„ç¬¬äºŒç‰ˆå†…å®¹ç›¸å½“å¹¿æ³›ã€‚æœ‰è°çŸ¥é“ç¬¬äºŒç‰ˆå’Œç¬¬ä¸€ç‰ˆæœ‰æ²¡æœ‰å¾ˆå¤§çš„åŒºåˆ«ï¼Ÿ   ç”±   æäº¤/u/d-eighties  /u/d-eighties  reddit.com/r/reinforcementlearning/comments/1bcrpib/is_the_2nd_edition_of_reinforcement_learning_an/&quot;&gt;[é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcrpib/is_the_2nd_edition_of_reinforcement_learning_an/</guid>
      <pubDate>Tue, 12 Mar 2024 07:52:46 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚æœä½ ä½¿ç”¨åƒ alphaGo è¿™æ ·çš„ä¸œè¥¿ä½œä¸º LLM çš„å¥–åŠ±æ¨¡å‹ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcr2sw/what_happens_if_you_use_something_like_alphago_as/</link>
      <description><![CDATA[è¿™å¯èƒ½ä¸æ˜¯ç¬¬ä¸€æ¬¡æœ‰äººé—®è¿™ä¸ªé—®é¢˜ï¼Œä½†ä½ èƒ½åœ¨ LLM ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ å¹¶ä½¿ç”¨ç±»ä¼¼äºå¥–åŠ±æ¨¡å‹çš„ AlphaGo å—ï¼Ÿä»»ä½•ä¸æ˜¯æœ‰æ•ˆç§»åŠ¨çš„ä»¤ç‰Œéƒ½ä¼šå—åˆ°æƒ©ç½š   ç”±   æäº¤ /u/rdyazdi   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcr2sw/what_happens_if_you_use_something_like_alphago_as/</guid>
      <pubDate>Tue, 12 Mar 2024 07:08:31 GMT</pubDate>
    </item>
    <item>
      <title>â€œè‰¯å¥½çš„è¡¨ç¤ºè¶³ä»¥å®ç°æ ·æœ¬é«˜æ•ˆå¼ºåŒ–å­¦ä¹ å—ï¼Ÿâ€ï¼ŒDu et al 2020</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcl32v/is_a_good_representation_sufficient_for_sample/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcl32v/is_a_good_representation_sufficient_for_sample/</guid>
      <pubDate>Tue, 12 Mar 2024 01:44:00 GMT</pubDate>
    </item>
    <item>
      <title>DQN æ€§èƒ½å…ˆä¸Šå‡åè¶‹å¹³</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcj5fm/dqn_performance_spikes_then_flatlines/</link>
      <description><![CDATA[      &lt; div class=&quot;md&quot;&gt;æˆ‘æ­£åœ¨æ ¹æ®æœ¬æ•™ç¨‹ä½¿ç”¨ DQN ä»£ç†ã€‚å½“åœ¨ cartpole ä¸Šè®­ç»ƒå®ƒæ—¶ï¼Œæ€§èƒ½ä¼¼ä¹ä¸€ç›´åœ¨æé«˜ï¼Œç›´åˆ°å¹³å‡æ€»å¥–åŠ±è¾¾åˆ° 500ï¼ˆè¿™æ˜¯ cartpole ç¯å¢ƒçš„æœ€å¤§å€¼ï¼‰ã€‚ç´§æ¥ç€ï¼Œåœ¨å…¶ä½™çš„è®­ç»ƒä¸­ï¼Œæ€§èƒ½å´©æºƒå¹¶ä»¥éå¸¸ä¸€è‡´çš„ 90-100 å¹³å‡å¥–åŠ±ç¨³å®šä¸‹æ¥ã€‚ æˆ‘è®¤ä¸ºè¿™å¾ˆæœ‰è¶£ï¼Œå¹¶ä¸”æƒ³çŸ¥é“æ˜¯å¦æœ‰äººæœ‰ä¸€ä¸ªç›´è§‚çš„åŸå› ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§æƒ…å†µå‘ç”Ÿäº†ã€‚è°¢è°¢ï¼ æˆ‘çš„è¶…å‚æ•°ï¼š self.learning_rate = 0.0005 self.gamma = 0.99 self.batch_size = 128 p&gt; self.start_epsilon = .9 self.end_epsilon = .1 self.epsilon_decay = 40000 self.tau = 0.005 p&gt; self.clip_grad_value = 100 self.loss_criterion = nn.SmoothL1Loss() self.optimizer = AdamW(self.policy_net.parameters(),lr=self .learning_rate,amsgrad=True) â€‹ è¿™æ˜¯è®­ç»ƒæœŸé—´è¯„ä¼°æ•°æ®çš„å±å¹•æˆªå›¾ï¼š https://preview.redd.it/kc9ic6dtpsnc1.png?width=1162&amp;format=png&amp;auto =webp&amp;s=c03202628c95595e2e86dc4bd89082a3c4421f61   ç”±   æäº¤ /u/PainindaAsh   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcj5fm/dqn_performance_spikes_then_flatlines/</guid>
      <pubDate>Tue, 12 Mar 2024 00:16:59 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ— éœ€æœç´¢çš„å¤§å¸ˆçº§å›½é™…è±¡æ£‹â€ï¼ŒRuoss ç­‰äºº 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ ç”±   æäº¤/u/gwern  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>