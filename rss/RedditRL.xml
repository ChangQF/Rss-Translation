<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 23 Apr 2024 15:13:49 GMT</lastBuildDate>
    <item>
      <title>使用 DreamerV3 训练鱼游任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cb0o62/training_a_fishswim_task_using_dreamerv3/</link>
      <description><![CDATA[大家好！我目前正在尝试使用 Hafner 等人的 DreamerV3（此处链接到论文）。我能够在我的计算机中对其进行设置，并能够运行他们在论文中所做的默认示例。  但是，我想要训练策略的主要任务是 Deepmind Control Suite 中的 fish-swim 任务，特别是使用 dmc_vision设置。我能够让它运行，但它训练的策略很差。即使我训练它 100 万步（默认设置），我得到的奖励也非常低。  你对我可以尝试什么有什么建议吗？以下是我到目前为止所做的：  这是我正在使用的 DreamerV3 的实现：https://github.com/NM512/dreamerv3-torch  我尝试使用dmc_proprio&lt;为fish-swim任务训练策略/code&gt; 我能够训练出良好的策略并获得良好的回报。但是，我想使用视觉输入让它工作。  我尝试使用状态空间和图像输入来增强编码器和解码器，但生成的策略仍然很差（与 dmc_vision 性能类似）。   我对强化学习也比较陌生，所以如果我没有正确使用术语，我深表歉意。如果我的问题中有任何需要澄清的地方，请告诉我。谢谢你！    由   提交/u/Learning-Robot-137   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cb0o62/training_a_fishswim_task_using_dreamerv3/</guid>
      <pubDate>Tue, 23 Apr 2024 09:56:07 GMT</pubDate>
    </item>
    <item>
      <title>DDQN 代理通过 keras 和 flappybird 收敛到次优策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cazr28/ddqn_agent_converges_to_a_suboptimal_policy_with/</link>
      <description><![CDATA[大家好，我正在尝试训练一个智能体来玩 flappybird，但是无论我如何调整我的超参数，我都会遇到收敛问题。我认为某处可能有错误，但我无法发现它，也许多一双眼睛可以提供帮助。这是我的实现： LINK 该实现支持多步骤，但目前我正在测试没有。 该算法与 atari 非常相似，只是修改了预期 Q 值的计算，该值使用了 H. van Hasselt 提出的优化。   由   提交 /u/Overall-Ask-3858   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cazr28/ddqn_agent_converges_to_a_suboptimal_policy_with/</guid>
      <pubDate>Tue, 23 Apr 2024 08:52:55 GMT</pubDate>
    </item>
    <item>
      <title>重要性抽样的书籍参考</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caxkyn/book_reference_for_importance_sampling/</link>
      <description><![CDATA[嗨，我正在寻找一些有关重要性采样数学的资料。我觉得所有课程都很好地介绍了它，但我想要我的论文有一个书源。我在一些概率书籍上查找过，但找不到任何参考资料。   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caxkyn/book_reference_for_importance_sampling/</guid>
      <pubDate>Tue, 23 Apr 2024 06:22:57 GMT</pubDate>
    </item>
    <item>
      <title>Isaacgym 中 Franka 的自定义动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1camkc3/custom_action_space_for_franka_in_isaacgym/</link>
      <description><![CDATA[大家好， 我正在尝试与 Franka 一起制作自定义 RL 环境。我想制作一个自定义的离散动作空间，基本上是全局坐标/姿势供机器人选择。有什么想法吗？   由   提交/u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1camkc3/custom_action_space_for_franka_in_isaacgym/</guid>
      <pubDate>Mon, 22 Apr 2024 21:27:36 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caljp2/issue_with_stable_baselines/</link>
      <description><![CDATA[使用 pybullet 创建自定义环境。环境运行良好，最高可达 10000 步左右。获得意想不到的观察形状。不知道当模拟运行 10k 步正常时会发生什么。   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caljp2/issue_with_stable_baselines/</guid>
      <pubDate>Mon, 22 Apr 2024 20:47:13 GMT</pubDate>
    </item>
    <item>
      <title>在专用服务器上运行的并行环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caj70y/parallelized_environments_running_on_a_dedicated/</link>
      <description><![CDATA[在 RL 中最大化 GPU 利用率通常相当困难，即使在并行环境中也是如此，因此我正在考虑在单独的仅包含 CPU 的服务器上运行并行模拟器使用最大可能数量的 CPU，并在任何云中的 GPU 节点上进行实际训练（在同一放置组内 + 所有可能的网络优化）。 这是否可行，或者额外的网络延迟是否太大而无法实现这可行吗？   由   提交/u/Ok-Entertainment-286   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caj70y/parallelized_environments_running_on_a_dedicated/</guid>
      <pubDate>Mon, 22 Apr 2024 19:05:50 GMT</pubDate>
    </item>
    <item>
      <title>动态奖励问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cag07l/dynamic_reward_problem/</link>
      <description><![CDATA[大家好！  我前几天在这里发帖，但我的问题仍然存在问题，但今天的问题完全不同。  我正在使用 DRL 方法（在本例中为 DQN）优化网络服务的位置。  但我有一个大问题：奖励函数。我正在尝试以非启发式的方式找到解决方案。因此，我决定使用为用户提供的吞吐量。  我的问题是，在研究的这些早期步骤中，我基本上试图让路由器位于 (0,0) 位置，并将用户集群置于 (1000,1000) （以米为单位测量） 。所以我的吞吐量很差，0.11 Mbps，而且要好，它至少必须到达集群附近 200 米，所以在大多数迭代中，它的奖励很低，永远不会学会到达那里。  此外，我想对位置初始化靠近集群的情况保持奖励。因此，不可能达到满足所有这些情况的奖励函数。有什么想法吗？ ​   由   提交 /u/RikoteMasterrrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cag07l/dynamic_reward_problem/</guid>
      <pubDate>Mon, 22 Apr 2024 17:00:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 rllib 训练时，episode_reward_max 始终为 0。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cafdtr/when_training_with_rllib_episode_reward_max_is/</link>
      <description><![CDATA[      我正在尝试训练一名单挑无限注德州扑克代理使用 rllib 的 PPO，环境为 PettingZoo 的 texas_holdem_no_limit。在训练期间，episode_reward_max 保持为 0，而 Episode_reward_min 保持为 -1。不过，根据我的理解，每轮结束时，玩家筹码数的变化应该作为奖励，并且这个值的最大值应该大于0。这种情况是否异常，还是我的理解不正确？ 这是训练代码的一部分： ray.init(num_gpus=8) env_name = &quot;poker&quot;; register_env(env_name, lambda _: PettingZooEnv( texas_holdem_no_limit.env() )) ModelCatalog.register_custom_model(“BaselineModel”, CNNModelV2) config = ( PPOConfig() .environment(env=env_name, Clip_actions=True,disable_env_checking=True) .rollouts (num_rollout_workers=4，rollout_fragment_length=128) .resources(num_gpus=8) .framework(framework=“torch”) .debugging(log_level=“ERROR”) .rl_module(_enable_rl_module_api=False) .training( _enable_learner_api=False, train_batch_size = 512，lr = 1e-4，gamma = 0.99，lambda_ = 0.9，use_gae = True，clip_param = 0.4，grad_clip =无，entropy_coeff = 0.1，vf_loss_coeff = 0.25，sgd_minibatch_size = 64，num_sgd_iter = 10，模型= {&quot; ;custom_model&quot;: &quot;BaselineModel&quot; } ) ) tune.Tuner( &quot;PPO&quot;, run_config=train.RunConfig( checkpoint_config=train.CheckpointConfig( checkpoint_Frequency=10, ), stop={&quot;timesteps_total&quot;: 10000000 如果不是操作系统.environ.get(&quot;CI&quot;) else 50000}, ), param_space=config, ).fit()  训练结果： https://preview.redd.it/371owm4d62wc1.png?width=2458&amp; format=png&amp;auto=webp&amp;s=06fd69d8789ca2a67b0551d50908437b34dfe785   由   提交 /u/euxcet   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cafdtr/when_training_with_rllib_episode_reward_max_is/</guid>
      <pubDate>Mon, 22 Apr 2024 16:35:50 GMT</pubDate>
    </item>
    <item>
      <title>MADDPG 只有一种代理表现出行为变化？这能说明什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cadzac/maddpg_only_one_agent_exhibits_changes_in/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cadzac/maddpg_only_one_agent_exhibits_changes_in/</guid>
      <pubDate>Mon, 22 Apr 2024 15:39:17 GMT</pubDate>
    </item>
    <item>
      <title>循环 PPO（LSTM）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caboyh/recurrent_ppo_lstm/</link>
      <description><![CDATA[大家好， 我目前正在尝试使用 LSTM 将当前的 PPO 实现“更新”为循环 PPO 实现对于我的演员和评论家网络。我目前不确定如何做到这一点，以及应该对正常的 PPO 实施进行哪些更改以纳入循环网络。只需更改网络实现中的几层即可完成工作，还是我必须做得更进一步？ 感谢有关此事的任何帮助，提前致谢！    由   提交/u/blrigo99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caboyh/recurrent_ppo_lstm/</guid>
      <pubDate>Mon, 22 Apr 2024 14:06:42 GMT</pubDate>
    </item>
    <item>
      <title>修改 Isaacgym 中的 Franka 动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ca9ptp/modify_franka_action_space_in_isaacgym/</link>
      <description><![CDATA[RL ppl，您好， 有人对动作空间进行过修改吗，尤其是 isaacgym 中的 Franka。我有一个 RL 设置，不需要夹具关节或夹具旋转，所以我只是将其设置为如下所示的固定值，但我认为这可能会混淆算法并浪费训练。你能提出一些建议吗？ IK&quot;: 目标 = self.control_ik(self._j_eef, hand_pos, hand_rot, goal_pos, goal_rot) #保持 2D 夹具方向相同 目标 = 目标 * self.actions[:,:7] * self.action_scale 目标 = torch. cat((targets, torch.zeros((targets.shape[0], 2), device=self.device)), dim=1) 目标[:,5:7] = self.franka_default_dof_pos[5:7] #保持 2D 夹具方向相同    由   提交/u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ca9ptp/modify_franka_action_space_in_isaacgym/</guid>
      <pubDate>Mon, 22 Apr 2024 12:41:14 GMT</pubDate>
    </item>
    <item>
      <title>用于知识图路径查找/排名的最先进的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ca4f03/state_of_the_art_rl_for_knowledge_graph_path/</link>
      <description><![CDATA[大家好， 我正在寻找一种最先进的 RL 算法来找到两个节点之间最有趣的路径知识图。 我发现 DeepPath 适合我的任务。但它相当“旧”（2017 年），并且仅在相当小的 KG（15.000 个节点，310.000 个三元组）上进行了测试，而我的 KG 有 680 万个节点，大约有 680 万个节点。 50.000.000 个三元组。 我的问题：RL 适合这个规模吗？如果是，您会建议哪种算法？ 提前非常感谢！   由   提交 /u/Zaaesar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ca4f03/state_of_the_art_rl_for_knowledge_graph_path/</guid>
      <pubDate>Mon, 22 Apr 2024 07:10:35 GMT</pubDate>
    </item>
    <item>
      <title>用于简单电池控制的 DQN 不学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9yd87/dqn_for_simple_battery_control_not_learning/</link>
      <description><![CDATA[我遇到电池控制问题。有一个由电池、电网和负载组成的能源系统。我想根据电费和负载需求了解电池何时充电、何时放电。当剧集大小为一个月时，我当前使用的 DQN 代理无法学习。它实际上了解剧集大小是否为 1 天（我将数据集设为一天）。另一方面，PPO 和 A2C 学习得很好。 我尝试将动作空间从 3 个元素（充电、放电、不执行任何操作）增加到 9 个元素，但没有任何改进。我还尝试使用渐变裁剪。这是否意味着 DQN 无法学习这一点？或者我可以用什么东西来让它学习？我应该修改 DQN 神经网络吗？我应该缩小剧集大小吗？我应该强制梯度裁剪吗？  作为参考，环境代码：https://github.com/MFHCehade/Reinforcement-Learning-for-Battery-Management/blob/main/environments/energy_management_env.py 对于 DQN，我在这里使用实现：https://avandekleut.github.io/dqn/ 非常感谢 &amp;# x200b; ​   由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9yd87/dqn_for_simple_battery_control_not_learning/</guid>
      <pubDate>Mon, 22 Apr 2024 01:24:15 GMT</pubDate>
    </item>
    <item>
      <title>无法渲染我的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9rtr0/cannot_render_my_env/</link>
      <description><![CDATA[晚上好 我已经使用 pettingzoo 的规范创建了一个自定义 MARL 环境，我正在尝试使用 rllib 进行训练，我是将 render_env 设置为 true 但渲染方法被忽略。我什至尝试过 rllib 示例来渲染 env，但似乎没有渲染任何内容。我错过了什么吗？ 我正在使用 ParallelPettingZoo() 函数来注册我的环境   由   提交 /u/Zenphirt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9rtr0/cannot_render_my_env/</guid>
      <pubDate>Sun, 21 Apr 2024 20:28:42 GMT</pubDate>
    </item>
    <item>
      <title>“从 _r_ 到 Q*：你的语言模型实际上是一个 Q 函数”，Rafailov 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1c9jnk1/from_r_to_q_your_language_model_is_secretly_a/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1c9jnk1/from_r_to_q_your_language_model_is_secretly_a/</guid>
      <pubDate>Sun, 21 Apr 2024 14:47:34 GMT</pubDate>
    </item>
    </channel>
</rss>