<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 28 Mar 2024 00:58:16 GMT</lastBuildDate>
    <item>
      <title>灾难性遗忘与熵正则化以及 RND 探索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpijs2/catastrophic_forgetting_vs_entropy_regularization/</link>
      <description><![CDATA[当使用探索技术或熵正则化来实现 onpolicy actor comment RL 时，我经常会看到性能下降的地方发生灾难性的遗忘。但称其为“灾难性遗忘”似乎还为时过早。这正是您所期望的。一旦解决了环境问题，优势就应该收敛到零。另一方面，熵正则化项是无界的，最终 RND 会对解决问题的状态空间“感到厌倦”。因此，无论哪种方式，一旦环境得到解决，这些其他项将占主导地位，算法甚至不会再优化目标。  有这方面的论文吗？   由   提交 /u/JustTaxLandLol   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpijs2/catastrophic_forgetting_vs_entropy_regularization/</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:07 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习模拟火箭着陆</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bphg59/model_rocket_landing_using_reinforcement_learning/</link>
      <description><![CDATA[我希望通过 Arduino / Raspberry Pi 实现具有机器学习功能的模型火箭着陆。我想知道我需要哪些部件以及需要使用哪些技术来实现这一目标（如果可能的话）。我没有火箭或 Arduino 方面的经验，所以我很好奇是否可以使用强化学习模型来控制火箭。   由   提交/u/xxspicymilkxx   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bphg59/model_rocket_landing_using_reinforcement_learning/</guid>
      <pubDate>Thu, 28 Mar 2024 00:07:29 GMT</pubDate>
    </item>
    <item>
      <title>PPO 中的 log_prob 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bpg1a4/issue_with_log_prob_in_ppo/</link>
      <description><![CDATA[您好， 我目前正在尝试应用具有操作屏蔽的单个代理 PPO，以从合法的操作中选择一个操作eadch 决策时间。这给了我们不同大小的 log_probs 轨迹，导致以下错误： ---&gt;优点 = 返回 - torch.stack(log_probs).detach()  运行时错误：堆栈期望每个张量大小相等，但在条目 0 处得到 [3]，在条目 2 处得到 [2]   您对此有什么解决方案吗？   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bpg1a4/issue_with_log_prob_in_ppo/</guid>
      <pubDate>Wed, 27 Mar 2024 23:08:10 GMT</pubDate>
    </item>
    <item>
      <title>大家好，刚刚接触到 PUBLIC AI。它与其他人工智能项目有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bp85n6/hey_everyone_just_came_across_public_ai_what/</link>
      <description><![CDATA[ 由   提交/u/mrwokee  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bp85n6/hey_everyone_just_came_across_public_ai_what/</guid>
      <pubDate>Wed, 27 Mar 2024 17:50:39 GMT</pubDate>
    </item>
    <item>
      <title>关于决斗特工 PPO 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bp7217/questions_about_ppo_for_duelling_agents/</link>
      <description><![CDATA[我正在编写一个 AI，它由 2 个不同的玩家组成，他们在不同维度的环境中玩游戏。所以，我需要知道我所做的事情在理论上是否正确。 ​ 我对代理使用 A2C nn，并且在每次移动后重新规范化概率减少代理人的行动空间。我为每个玩家收集一次展示（操作、状态、奖励、完成），仅考虑该玩家的状态和操作。然后我为每个玩家设置 128 次移动的地平线并计算 GAE，然后重新规范化 GAE 并在每次达到地平线时训练 nn。这在方法论上正确吗？我可以在训练期间切换环境以增加其维度吗？我错过了什么吗？   由   提交 /u/Capittain-Nemo-9294   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bp7217/questions_about_ppo_for_duelling_agents/</guid>
      <pubDate>Wed, 27 Mar 2024 17:05:58 GMT</pubDate>
    </item>
    <item>
      <title>AMD 的 ROCm 与 NVidia 的 CUDA 进行强化学习比较？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bp67b9/amds_rocm_vs_nvidias_cuda_for_reinforcement/</link>
      <description><![CDATA[我知道 CUDA 更成熟并且迄今为止使用最多，我只是好奇它们之间是否有任何可用的比较，或者是否有人知道如何使用它们可以堆叠吗？我知道在游戏中 7900xt 与 4080 差不多，但是在与人工智能相关的任务中这种差异如何扩大？基本上，HIP 有什么好处吗？或者您应该使用较旧的 10-20 系列 NVidia 卡而不是 ML/RL 的 7000 系列 AMD 卡   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bp67b9/amds_rocm_vs_nvidias_cuda_for_reinforcement/</guid>
      <pubDate>Wed, 27 Mar 2024 16:31:52 GMT</pubDate>
    </item>
    <item>
      <title>定制体育馆环境……观察空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bp5zvb/custom_gymnasium_environment_observation_space/</link>
      <description><![CDATA[嗨，感谢您阅读本文。 我正在尝试将“贪吃蛇”游戏改编为 Gym.Env( ）类，但很难让我的头脑了解观察空间。 游戏输出是一个 3 通道“屏幕”，由大小为（19,19,3）的 numpy 数组表示，包含1或0。 如何为“屏幕”数组定义合适的observation_space？我猜它使用 space.Box(low=np.array(0) , high=np.array(1) , shape=(19,19,3), dtype=np.int8) ...这是正确的吗？    由   提交/u/Low_Corner_9061   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bp5zvb/custom_gymnasium_environment_observation_space/</guid>
      <pubDate>Wed, 27 Mar 2024 16:23:24 GMT</pubDate>
    </item>
    <item>
      <title>“Lucy-SKG：学习如何玩_火箭联盟_有效地使用深度强化学习”，Moschopoulos 等人 2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bp4636/lucyskg_learning_to_play_rocket_league/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bp4636/lucyskg_learning_to_play_rocket_league/</guid>
      <pubDate>Wed, 27 Mar 2024 15:07:44 GMT</pubDate>
    </item>
    <item>
      <title>寻找可以合作开展定量强化学习项目的人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bowwyg/looking_for_someone_to_collaborate_with_on_a/</link>
      <description><![CDATA[大家好！我是一名刚毕业的计算机科学硕士，正在尝试定量强化学习。我对这个领域还很陌生，但我对此充满热情并愿意一路学习。我读到的研究有时对于一个人来说是难以承受的。我希望能够出版我们最终共同创作的作品。    由   提交 /u/EyeZealousideal3229   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bowwyg/looking_for_someone_to_collaborate_with_on_a/</guid>
      <pubDate>Wed, 27 Mar 2024 08:30:23 GMT</pubDate>
    </item>
    <item>
      <title>训练四足机器人搬运包裹就像……</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bowv5a/training_fourlegged_robot_to_carry_a_package_be/</link>
      <description><![CDATA[    /u/IAmMiddy   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bowv5a/training_fourlegged_robot_to_carry_a_package_be/</guid>
      <pubDate>Wed, 27 Mar 2024 08:26:47 GMT</pubDate>
    </item>
    <item>
      <title>在 DQN 上下文中查找有关平均绝对贝尔曼误差的文献</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bowchu/looking_for_literature_on_mean_absolute_bellman/</link>
      <description><![CDATA[我最近写了一个 DQN 来解决 MountainCar-v0。然而，当我尝试将相同的代码应用于 LunarLander-v2 时，我遇到了灾难性的遗忘问题。有一段时间，奖励似乎会增加。但随后，性能会在一个时期内崩溃 我最终将问题追溯到梯度爆炸。每隔一段时间，我的均方贝尔曼误差估计就会产生一个非常大的梯度。应用更新将会破坏主 Q 网络。更糟糕的是，这会渗透到目标网络，导致算法需要 100 纪元才能恢复 出于绝望，我尝试用（伪）平均绝对误差损失替换均方误差损失：  loss(x) = sqrt(1 + x2) - 1  在此更改之后，算法管理解决 LunarLander 你知道有哪些有趣的论文建议使用 MAE 而不是 MSE 来最小化贝尔曼误差吗？这总体上是一个有用的技巧，还是更像是一种仅适用于少数特定问题的“黑客”？   由   提交/u/lilganj710  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bowchu/looking_for_literature_on_mean_absolute_bellman/</guid>
      <pubDate>Wed, 27 Mar 2024 07:49:13 GMT</pubDate>
    </item>
    <item>
      <title>政策和行动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bov85j/policy_and_actions/</link>
      <description><![CDATA[假设我已经训练了我的智能体并且它已经学习了一个策略（这是智能体的决策函数（控制策略），它代表了一个映射）从状态到行动。）这里我的代理的动作空间是 [2 1]，状态向量是 [3 1]。 现在让我们说，当我在训练状态向量中的一个状态后部署我的代理时，其值是我的特工在训练期间没有见过。那么会发生什么？我的代理将如何反应/采取行动？ 现在让我们说，当我在训练后部署代理时，状态向量中的某个状态会采用我的代理在训练期间未见过的值。那么会发生什么呢？我的代理将如何反应/采取行动？   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bov85j/policy_and_actions/</guid>
      <pubDate>Wed, 27 Mar 2024 06:31:24 GMT</pubDate>
    </item>
    <item>
      <title>强化学习博士论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bo4qtu/phd_theses_in_reinforcement_learning/</link>
      <description><![CDATA[我正在收集 RL 领域有趣的博士论文列表。你还知道其他有趣的事情吗？请在下面评论！  P。 J. Werbos 1974 - 超越回归：行为科学中预测和分析的新工具 @ u/clorky123 理查德·萨顿 1984 - 强化学习中的时间信用分配 C. J. C. H. Watkins 1989 - 从延迟奖励中学习 @ u/clorky123 彼得·达扬 1991 - 强化联结主义：学习统计方法 Andrew Y. Ng (2003) - 强化学习中的塑造和策略搜索 @ u/S1gature Sham Kakade 2003 - 论强化学习的样本复杂性 @ &lt; a href=&quot;https://www.reddit.com/u/_An_Other_Account_&quot;&gt;u/_An_Other_Account_ 伊恩·奥斯班 2016 - 通过随机值函数进行深入探索 John Schulman 2016 - 优化期望 Pierre-Luc Bacon 2018 - 时间表示学习  编辑：随着新建议的出现，我正在更新帖子。   由   提交 /u/YouAgainShmidhoobuh   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bo4qtu/phd_theses_in_reinforcement_learning/</guid>
      <pubDate>Tue, 26 Mar 2024 10:40:57 GMT</pubDate>
    </item>
    <item>
      <title>学习强化学习需要精通什么</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bnrqp5/what_do_you_need_to_be_proficient_in_to_learn/</link>
      <description><![CDATA[谷歌搜索没有结果，因此我们将不胜感激。例如，我需要学习Python吗？您能否推荐一些课程或一系列 YouTube 视频？我是一个完全的新手，但我愿意学习。谢谢。   由   提交/u/Snoo72721  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bnrqp5/what_do_you_need_to_be_proficient_in_to_learn/</guid>
      <pubDate>Mon, 25 Mar 2024 22:51:09 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>