<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 01 Mar 2024 15:14:46 GMT</lastBuildDate>
    <item>
      <title>为什么在数学中进行自动定理证明的深度强化学习如此困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3vm43/why_is_it_so_difficult_to_do_deep_reinforcement/</link>
      <description><![CDATA[请记住，我的领域是数学。我是 ML/DL 的新手： 我们有 AlphaGo 和 AlphaFold 类型的系统，它们在各自的领域都是超人的。我读到研究人员很难为数学研究建立类似的系统，即“超人数学家”。那么，为什么困难呢？鉴于法学硕士以及上述系统的进步，我什么时候应该开始担心某些人工智能系统在各个方面都优于我？  作为一个新手，在我看来，这些系统的发展速度比任何人想象的都要快，但这也许是因为我看到的唯一新闻是大肆宣传的。我感受到的存在主义恐惧太多了。   由   提交 /u/dhhdhkvjdhdg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3vm43/why_is_it_so_difficult_to_do_deep_reinforcement/</guid>
      <pubDate>Fri, 01 Mar 2024 14:29:52 GMT</pubDate>
    </item>
    <item>
      <title>为什么在决策后状态上使用 Q 学习而不是值迭代？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3u9nf/why_use_qlearning_instead_of_value_iteration_on/</link>
      <description><![CDATA[在《近似动态规划》第 390 页，Powell 认为 Q 学习使用“人工决策后状态……状态/动作对 (S, A）”。他进一步说“使用决策后状态变量的值函数进行迭代相当于 Q 学习”……只是更容易，因为决策后状态更紧凑 这对我来说似乎是合乎逻辑的。如果我们可以只处理该组合的最终结果，为什么我们还要关心（状态，动作）组合？ 但是，在我见过的文献和算法实现中，Q 学习似乎是一种更受欢迎。为什么是这样？直接使用（状态、动作）对有什么好处吗？   由   提交/u/lilganj710  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3u9nf/why_use_qlearning_instead_of_value_iteration_on/</guid>
      <pubDate>Fri, 01 Mar 2024 13:28:50 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习的奖励尊重子任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3rax7/rewardrespecting_subtasks_for_modelbased/</link>
      <description><![CDATA[论文：https://www.sciencedirect.com/science/article/pii/S0004370223001479 预印本版本：https://arxiv.org/abs/2202.03466 摘要：  &lt;为了实现人工智能的宏伟目标，强化学习必须包括使用状态和时间抽象的世界模型进行规划。深度学习在状态抽象方面取得了进展，但尽管基于选项框架的理论得到了广泛的发展，但时间抽象却很少被使用。原因之一是可能的选项空间巨大，并且先前提出的选项发现方法没有考虑选项模型将如何在规划中使用。通常通过提出辅助任务来发现选项，例如达到瓶颈状态或最大化除奖励之外的感官信号的累积和。解决每个子任务以产生一个选项，然后学习该选项的模型并将其提供给规划过程。在大多数以前的工作中，子任务忽略了原始问题的奖励，而我们提出的子任务使用原始奖励加上基于选项终止时状态特征的奖励。我们表明，从此类尊重奖励的子任务中获得的选项模型比特征选项、基于瓶颈状态的最短路径选项或由选项批评家生成的尊重奖励的选项更有可能在规划中有用。尊重子任务的奖励强烈限制了选项的空间，从而也为选项发现问题提供了部分解决方案。最后，我们展示了如何使用标准算法和通用价值函数在线和离线学习价值、策略、选项和模型。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3rax7/rewardrespecting_subtasks_for_modelbased/</guid>
      <pubDate>Fri, 01 Mar 2024 10:40:35 GMT</pubDate>
    </item>
    <item>
      <title>我如何获得 CSI 学士学位并在机器人实验室工作，从而进入强化学习/研究工作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3f7ep/how_can_i_get_into_an_rlresearch_job_from_a_bs_in/</link>
      <description><![CDATA[正如标题所述，我在机器人实验室工作，从事机器视觉和嵌入式开发。我想过渡到 RL 职位，最好是做研究。我必须获得硕士学位吗？   由   提交/u/emas_eht   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3f7ep/how_can_i_get_into_an_rlresearch_job_from_a_bs_in/</guid>
      <pubDate>Thu, 29 Feb 2024 23:38:40 GMT</pubDate>
    </item>
    <item>
      <title>限制行走的关节范围有多重要？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b3cof4/how_important_is_limiting_joint_range_for_walking/</link>
      <description><![CDATA[我有一只四足动物，我正在尝试教它走路。这是一个相当长期的项目，我希望最终有两种运动模式（步行和飞行），所以比例对于步行者来说有点奇怪，并且前肢上有一个额外的肢体部分，目前没有用于任何用途. 现在，我正在为两种运动模式提供关节的全方位运动，但我的训练进行得不太顺利。我正在使用 Mujoco + MJX/Brax 在云 GPU 上进行训练。在 MJX 示例中，它训练了一个与我的模型具有大约相同自由度的人形机器人，使其以大约 10 分钟/3000 万个时间步长行走，但我的机器人在 3000 万个时间步长后仍然在四处走动，尽管奖励曲线在我未经训练的眼睛看来还不错。  我尝试过将奖励权重更倾向于向前、直立运动，也尝试过改变位移和速度之间的向前奖励，但没有任何效果。当使用速度奖励时，它似乎学会了向前猛冲，然后因为摔倒而立即死亡。有了位置奖励，它并没有真正取得任何进展。 所以我想知道是否需要进一步限制关节范围以适应步行所需的范围。我还将尝试减少所有关节的执行器扭矩，我现在认为它们可能有太大的扭矩来进行精细控制。从政策角度来看，到目前为止我尝试过的都是 PPO，但根据我读到的所有说法，这不应该导致此类问题。 欢迎任何和所有想法/建议！我对强化学习完全陌生，所以我确信我犯了一些容易修复的新手错误，只需要弄清楚它们是什么。 模型 奖励曲线 项目仓库   由   提交 /u/AnAngryBirdMan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b3cof4/how_important_is_limiting_joint_range_for_walking/</guid>
      <pubDate>Thu, 29 Feb 2024 21:57:46 GMT</pubDate>
    </item>
    <item>
      <title>使用 RLlib 进行活塞球训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b39c05/pistonball_training_with_rllib/</link>
      <description><![CDATA[大家好，我正在使用 RLlib 来使用 PPO 来训练 Pistonball 环境，其代码与 https://pettingzoo.farama.org/tutorials/rllib/pistonball/ 我使用的计算网格满足所有资源要求（&gt;=5CPU）。 ​ 但是，我发现训练时间相当长。一次迭代大约需要 1 小时，并且应该会达到 50000 次迭代... 我的配置可能有问题... &amp;# x200b; 有人尝试使用所有必要的计算资源来运行 Pistonball 代码吗？ 如果能够获得一些有关预期的已知信息，那就太好了训练持续时间仅供比较...   由   提交 /u/Sudden-Trainer-4046   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b39c05/pistonball_training_with_rllib/</guid>
      <pubDate>Thu, 29 Feb 2024 19:44:56 GMT</pubDate>
    </item>
    <item>
      <title>训练自动驾驶机器人的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b392wp/enviroment_to_train_a_self_drive_robot/</link>
      <description><![CDATA[我想模拟能够避开障碍物的简单机器人，但现在不知道我必须使用什么样的环境，应该使用 robo-gym 还是 ros2？&lt; /p&gt;   由   提交 /u/sigma_ks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b392wp/enviroment_to_train_a_self_drive_robot/</guid>
      <pubDate>Thu, 29 Feb 2024 19:34:43 GMT</pubDate>
    </item>
    <item>
      <title>关于即将举行的会议的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b383uf/question_on_upcoming_conferences/</link>
      <description><![CDATA[大家好， 我只是想问一下，除了 CORL* 之外，是否还有任何会议需要提交夏季和会议将在秋季晚些时候举行吗？ CORL 的提交期限最长为 6 月 6 日，但我想知道是否还有其他截止日期在此之后（夏季的某个时间），但会议仍为年底（最长）。  *我的希望是提交给 CORL，但我只是想不作为备份，如果无法在截止日期前完成还有其他什么吗？ &lt;!-- SC_ON - -&gt;  由   提交 /u/StrangerNo9431   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b383uf/question_on_upcoming_conferences/</guid>
      <pubDate>Thu, 29 Feb 2024 18:55:56 GMT</pubDate>
    </item>
    <item>
      <title>努力在 STOCKS_GOOGL 数据上训练 A2C 模型 - 熵和价值损失收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b358ua/struggling_to_train_a2c_model_on_stocks_googl/</link>
      <description><![CDATA[我一直在从事一个项目，尝试使用 stable_baselines3 中的 A2C（Advantage Actor-Critic）算法来训练模型STOCKS_GOOGL数据，旨在预测股价走势。但是，我在训练过程中遇到了 entropy_loss 和 value_loss 收敛的持续问题。 详细信息： 数据集： 我正在使用 STOCKS_GOOGL 数据集，其中包含 Google (GOOGL) 的历史股票价格数据。 模型架构：我已使用默认 MlpPolicyand 设置了 A2C 模型确保输入维度与数据集匹配。 训练过程：我已初始化 A2C 模型并尝试使用提供的数据集对其进行训练。然而，我面临着 entropy_loss 和 value_loss 收敛的挑战。 观察： entropy_loss 和 value_loss 指标没有收敛的迹象，这使得模型很难有效地学习和预测股票价格走势。 尝试的解决方案：我尝试过不同的超参数，例如调整学习率和网络架构（隐藏层和神经元的数量） 。但是，问题仍然存在。 ​ 任何有关如何改进 entropy_loss 和 value_loss 收敛以获得更好训练结果的指导将不胜感激。 &lt; p&gt;​ 代码片段： from stable_baselines3 import A2C from stable_baselines3.common.vec_env import DummyVecEnv import torch.nn as nn importgym_anytrading.envs as envs fromgym_anytrading.datasets import STOCKS_GOOGL import stock_utils StockEnvClass = envs.StocksEnv # StockEnvClass = stock_utils.MyStocksEnv # 训练 env window_size = 6 env_factory = lambda: StockEnvClass(STOCKS_GOOGL, window_size=window_size, frame_bound=(window_size, len) (STOCKS_GOOGL))) env = DummyVecEnv([env_factory]) # 测试环境 test_data = STOCKS_GOOGL.tail(150) test_env = StockEnvClass(test_data, window_size=window_size, frame_bound=(window_size, len(test_data))) # tain 模型 test_callback = stock_utils.TestModelCallback（eval_env = test_env，eval_freq = 5000）policy =“MlpPolicy” model = A2C(policy, env, verbose=1,learning_rate=0.0001) model.learn(total_timesteps=300_000,callback=test_callback, log_interval=5000) # 评估 print(“评估模型：”) stock_utils.evaluate_model2(model, test_env, True) input(&quot;完成！按 Enter 退出...&quot;)  Repo: https://github.com/myxdream2020/stocks_drl.git    由   提交 /u/bigsml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b358ua/struggling_to_train_a2c_model_on_stocks_googl/</guid>
      <pubDate>Thu, 29 Feb 2024 17:00:47 GMT</pubDate>
    </item>
    <item>
      <title>普通重要性抽样和加权重要性抽样之间的区别</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b33ijg/difference_between_ordinary_and_weighted/</link>
      <description><![CDATA[我正在学习 Sutton 和 Barto 的 RL 教科书，目前正在学习离策略评估，其中我们有两个策略，一个是目标策略 (确定性），另一个是行为策略（更具探索性），我无法理解加权重要性采样（我们用“rho”对每个回报进行加权，以获得“rhos”总和的回报）如何实现即使我们采取大量步骤，也会收敛到目标政策的真实值。  我可以理解，普通重要性采样将始终收敛于目标策略价值函数，因为它们乘以“rho”，但我不明白如何对加权重要性采样说同样的话，因为加权重要性采样的期望将是行为策略的期望   由   提交 /u/Unlikely_Spread_2618   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b33ijg/difference_between_ordinary_and_weighted/</guid>
      <pubDate>Thu, 29 Feb 2024 15:49:54 GMT</pubDate>
    </item>
    <item>
      <title>动态观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b32tjr/dynamic_observation_space/</link>
      <description><![CDATA[当观察空间大小动态变化时，最佳实践是什么？ 一些选项： 1. 定义最大大小并填充零缺失观测值 2. 定义较小的最大大小，以便始终抛出一些观测值（您的观测值总是多于允许的大小） 3. 使用 DNN 等将动态大小向量压缩为固定长度。 4. 还有什么吗？ 优点/缺点？   由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b32tjr/dynamic_observation_space/</guid>
      <pubDate>Thu, 29 Feb 2024 15:20:37 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的模因创作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b30nym/aidriven_meme_creation/</link>
      <description><![CDATA[我们为 Qbeast 的数据工程爱好者发布了一款人工智能驱动的模因生成器。这个有趣的项目帮助我们学习如何微调人工智能模型并定制幽默数据集。我们很高兴与其他对融合技术和创造力感兴趣的爱好者分享我们的经验。查看故事 https://qbeast.io/qbeasts-adventure-in-ai-驱动模因创作/。   由   提交/u/QbeastIO   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b30nym/aidriven_meme_creation/</guid>
      <pubDate>Thu, 29 Feb 2024 13:45:58 GMT</pubDate>
    </item>
    <item>
      <title>加权每决策重要性采样的一致性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2ydht/consistency_of_weighted_perdecision_importance/</link>
      <description><![CDATA[在第 5.9 节 Sutton 和 Barto 中，作者声称不存在一致的加权每决策 IS 估计器，但是在 Precup 的原始论文中(https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&amp; ;context=cs_faculty_pubs），提出以下主张：“这个加权每决策重要性采样估计器是一致的，但有偏差，就像加权重要性采样估计器一样”。 诚然，没有原始论文中提供了加权 PD 估计器的一致性证明。  有谁知道之前的陈述是否经过了异议证明/是否有任何资源证明原始论文中建议的加权 PD 估计量不一致？  干杯！   由   提交/u/bean_the_great   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2ydht/consistency_of_weighted_perdecision_importance/</guid>
      <pubDate>Thu, 29 Feb 2024 11:42:21 GMT</pubDate>
    </item>
    <item>
      <title>您会使用什么 RL 算法作为推荐算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2s64f/what_rl_algorithm_would_you_use_for_a/</link>
      <description><![CDATA[您使用什么算法/奖励函数来提高 Tinder 或 Pinterest 等应用的留存率或参与度。 &lt; !-- SC_ON --&gt;  由   提交/u/simoo42  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2s64f/what_rl_algorithm_would_you_use_for_a/</guid>
      <pubDate>Thu, 29 Feb 2024 05:04:36 GMT</pubDate>
    </item>
    <item>
      <title>帮助复制生命游戏风格棋盘游戏的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b2q5wi/help_with_reinforcement_learning_model_for/</link>
      <description><![CDATA[大家好， 我正在努力为一款逼真的游戏实现强化学习模型，其中两个玩家从 64 x 64 板上的一块开始。在每一步中，玩家可以选择攻击一个未占据的方格，该方格会复制他们的棋子。例如，从一个棋子开始，攻击一个方格会产生两个棋子，这两个棋子可以攻击空方格并产生 4 个棋子，依此类推。每个玩家选择自己的动作，并同时执行。多个棋子可以攻击同一个方格，获胜者由攻击该方格的棋子最多决定，否则如果均分则随机决定。 我目前面临设计强化学习算法的挑战对于这个游戏。我正在为国家代表和奖励机制而苦苦挣扎。具体来说，我不确定如何以捕获棋子复制动态的方式表示游戏状态以及如何定义适当的奖励。 以下是我当前方法的简要概述：  智能体倍增：我不确定如何对不同状态的智能体总数进行建模，在 Dota 等游戏中，使用 OpenAI 5，他们有 5 个可以单独训练的智能体，我&#39;我不确定如何让合作智能体在每个阶段的数字都发生变化的情况下集中工作。 奖励机制：我不确定应该分配什么奖励来鼓励模型战略性地复制片段，同时平衡其他目标例如控制棋盘或吃掉对手的棋子。  我读过各种论文，也读过有关使用 SAC 和 PPO 进行多智能体的内容，但我不确定到底我应该如何表示动作空间，以及我是否应该在每个回合或游戏结束时给予奖励，1表示获胜，-1表示失败，等等。 如果有人有经验通过强化学习或类似的棋盘游戏，我将非常感谢任何关于如何应对这些挑战的见解或建议。此外，如果有特定的资源或教程可以帮助我更好地理解这些概念，我将不胜感激。 提前感谢您的帮助！ &lt; !-- SC_ON --&gt;  由   提交/u/dickpham  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b2q5wi/help_with_reinforcement_learning_model_for/</guid>
      <pubDate>Thu, 29 Feb 2024 03:20:21 GMT</pubDate>
    </item>
    </channel>
</rss>