<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Thu, 11 Jul 2024 01:06:40 GMT</lastBuildDate>
    <item>
      <title>å…·æœ‰è‡ªå®šä¹‰ç¯å¢ƒçš„ A3C ä¿„ç½—æ–¯æ–¹å—ä¸­çš„æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzordj/suboptimal_solutions_in_a3c_tetris_with_custom/</link>
      <description><![CDATA[æœ€è¿‘ï¼Œæˆ‘é€‰æ‹© A3C ä½œä¸ºæˆ‘çš„æœ€ç»ˆé¡¹ç›®çš„é¦–é€‰ç®—æ³•ã€‚æˆ‘ä½¿ç”¨ https://github.com/dgriff777/rl_a3c_pytorch ä½œä¸ºå‚è€ƒï¼Œä½†æ²¡æœ‰ä½¿ç”¨ LSTMã€‚æˆ‘ä½¿ç”¨çš„ç¯å¢ƒæ˜¯å®šåˆ¶çš„ï¼šè§‚å¯Ÿç©ºé—´åŸºæœ¬ä¸Šæ˜¯å…·æœ‰ 3 ä¸ªé€šé“å›¾åƒçš„ä¸¤å—ä¿„ç½—æ–¯æ–¹å—ï¼ˆç¬¬ä¸€ä¸ªé€šé“æ˜¾ç¤ºåªæœ‰å †å çš„å››æ ¼éª¨ç‰Œçš„æ£‹ç›˜å›¾åƒï¼Œç¬¬äºŒä¸ªé€šé“æ˜¾ç¤ºå½“å‰ä¸‹è½çš„å››æ ¼éª¨ç‰Œï¼Œç¬¬ä¸‰ä¸ªé€šé“æ˜¾ç¤ºä¸‹ä¸€ä¸ªå››æ ¼éª¨ç‰Œï¼‰ã€‚åœ¨è¿è¡Œäº†æ•°åƒæ­¥ä¹‹åï¼Œæˆ‘çš„ä»£ç†ä¼¼ä¹é™·å…¥äº†æ¬¡ä¼˜è§£å†³æ–¹æ¡ˆï¼Œå®ƒå€¾å‘äºå°†å—å †å åœ¨æ£‹ç›˜çš„å·¦ä¾§ã€‚æˆ‘è¯¥å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ æ³¨æ„ï¼šæˆ‘æ­£åœ¨ä½¿ç”¨ dellacherie è¯„ä¼°å‡½æ•°æ¥å¥–åŠ±æˆ‘çš„ä»£ç†    æäº¤äºº    /u/handshakers01   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzordj/suboptimal_solutions_in_a3c_tetris_with_custom/</guid>
      <pubDate>Wed, 10 Jul 2024 06:22:29 GMT</pubDate>
    </item>
    <item>
      <title>PPOç®—æ³•çš„è®­ç»ƒé€Ÿåº¦</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzoolu/the_training_speed_of_ppo_algorithm/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzoolu/the_training_speed_of_ppo_algorithm/</guid>
      <pubDate>Wed, 10 Jul 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ AlphaZero å­¦ä¹  Tichuï¼ˆçº¸ç‰Œæ¸¸æˆï¼‰â€”â€”å¯»æ±‚æœ‰å…³çŠ¶æ€ + ç½‘ç»œè®¾è®¡çš„å»ºè®®/æ¨æµ‹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dzo93h/learning_tichu_card_game_with_alphazero_seeking/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dzo93h/learning_tichu_card_game_with_alphazero_seeking/</guid>
      <pubDate>Wed, 10 Jul 2024 05:50:06 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å»ºç«‹å¼ºåŒ–å­¦ä¹ çš„æ–¹ç¨‹å¼ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dznob5/how_to_develop_the_equations_for_reinforcement/</link>
      <description><![CDATA[æœ€è¿‘æˆ‘å¼€å§‹å­¦ä¹ å¼ºåŒ–å­¦ä¹ ï¼Œæˆ‘çœ‹åˆ°çš„æ‰€æœ‰è§†é¢‘æˆ–åšå®¢éƒ½åªæ˜¯æä¾›ä¸€ä¸ªæ¨¡æ‹Ÿç¯å¢ƒå¹¶æè¿°å¼ºåŒ–å­¦ä¹ çš„è¿‡ç¨‹ï¼Œæœ‰æ²¡æœ‰ä½ ä»¬çŸ¥é“çš„åšå®¢æˆ–æ•™ç¨‹ï¼Œä»–ä»¬æ•™å¦‚ä½•å¼€å‘çŠ¶æ€æ¨¡å‹ï¼Œå¦‚ä½•å½¢æˆç­–ç•¥ä¸­ä½¿ç”¨çš„æ‰€æœ‰æ–¹ç¨‹å¼ã€‚é‚£å°†éå¸¸æœ‰å¸®åŠ©ï¼Œä»»ä½•å¸®åŠ©éƒ½å°†ä¸èƒœæ„Ÿæ¿€     æäº¤äºº    /u/Away_Elk_6826   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dznob5/how_to_develop_the_equations_for_reinforcement/</guid>
      <pubDate>Wed, 10 Jul 2024 05:13:34 GMT</pubDate>
    </item>
    <item>
      <title>Craftiumï¼šç”¨äºåˆ›å»ºå¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„å¯æ‰©å±•æ¡†æ¶</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz9bp5/craftium_an_extensible_framework_for_creating/</link>
      <description><![CDATA[å—¨ï¼æˆ‘ä¸€ç›´åœ¨å¼€å‘ Craftiumï¼šä¸€ä¸ªæ˜“äºä½¿ç”¨çš„ RL ç¯å¢ƒåˆ›å»ºæ¡†æ¶ã€‚Craftium å°†å¼€æºä½“ç´ æ¸¸æˆå¼•æ“ Minetest åŒ…è£…åœ¨ä¸€ä¸ªå®ç° Gymnasium API çš„ Python åº“ä¸­ã€‚è™½ç„¶ Craftium æ—¨åœ¨ç”¨äºç¯å¢ƒåˆ›å»ºï¼Œä½†æˆ‘ä»¬ä¹ŸåŒ…å«äº†ä¸€äº›ç°æˆçš„ç¯å¢ƒå’Œä»»åŠ¡ï¼ˆè¯·åœ¨æ­¤å¤„ æŸ¥çœ‹ï¼‰ã€‚ç›®å‰ï¼ŒCraftium å¤„äºæ—©æœŸå¼€å‘é˜¶æ®µï¼Œä½†å·²å‡†å¤‡å¥½ä½¿ç”¨å’Œæµ‹è¯•ã€‚å¸Œæœ›æ‚¨è§‰å¾—å®ƒå¾ˆæœ‰è¶£ï¼æ¬¢è¿æä¾›åé¦ˆï¼ï¼    ç”±    /u/mikelma æäº¤   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz9bp5/craftium_an_extensible_framework_for_creating/</guid>
      <pubDate>Tue, 09 Jul 2024 18:12:24 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•åœ¨ Stable_Baselines3 ä¸­é‡ç½®åè®°å½•è§‚å¯Ÿç»“æœï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz885r/how_do_i_log_observations_after_reset_in_stable/</link>
      <description><![CDATA[æˆ‘æƒ³åœ¨ä½¿ç”¨ SB3 æ—¶ï¼Œè®°å½•è®­ç»ƒæœŸé—´`é‡ç½®`åè·å¾—çš„æ¯ä¸ª`è§‚å¯Ÿ`ã€‚ æ ¹æ® [this][1] é—®é¢˜æ¶ˆæ¯ï¼Œæˆ‘â€‹â€‹å†³å®šä½¿ç”¨`Monitor`åŒ…è£…å™¨è€Œä¸æ˜¯å›è°ƒã€‚ ä½†æ˜¯ï¼Œç›‘è§†å™¨åŒ…è£…å™¨ç»™äº†æˆ‘ä»¥ä¸‹é”™è¯¯ -  è¿™æ˜¯æˆ‘çš„ä»£ç  - ```py import gym from stable_baselines3 import PPO from stable_baselines3.common.callbacks import BaseCallback from stable_baselines3.common.monitor import Monitor class CustomMonitor(Monitor): def __init__(self, env, filename=None, allow_early_resets=True, reset_keywords=(), info_keywords=()): super(CustomMonitor, self).__init__(env) self.reset_observations = [] def reset(self, **kwargs): observer = super(CustomMonitor, self).reset(**kwargs) self.reset_observations.append(observation) return observer env = gym.make(&#39;LunarLander-v2&#39;) env = CustomMonitor(env) model = PPO(&#39;MlpPolicy&#39;, env, verbose=1) model.learn(total_timesteps=1000000) model.save(&quot;ppo_lunarlander_mutant&quot;)  ``` ä½†æ˜¯ï¼Œè¿è¡Œå®ƒä¹‹åï¼Œæˆ‘æ”¶åˆ°ä»¥ä¸‹é”™è¯¯ - ``` Traceback (most recent call last): File &quot;minimal_example.py&quot;, line 21, in &lt;module&gt;æ¨¡å‹ = PPOï¼ˆ&#39;MlpPolicy&#39;ï¼Œenvï¼Œverbose = 1ï¼‰æ–‡ä»¶â€œ/home/thoma/anaconda3/envs/wp/lib/python3.8/site-packages/stable_baselines3/ppo/ppo.pyâ€ï¼Œç¬¬ 109 è¡Œï¼Œåœ¨ __init__ superï¼ˆï¼‰ã€‚__init__ï¼ˆæ–‡ä»¶â€œ/home/thoma/anaconda3/envs/wp/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.pyâ€ï¼Œç¬¬ 85 è¡Œï¼Œåœ¨ __init__ superï¼ˆï¼‰ã€‚__init__ï¼ˆæ–‡ä»¶â€œ/home/thoma/anaconda3/envs/wp/lib/python3.8/site-packages/stable_baselines3/common/base_class.pyâ€ï¼Œç¬¬ 180 è¡Œï¼Œåœ¨ __init__ æ–­è¨€ isinstanceï¼ˆself.action_spaceï¼Œ supports_action_spaces), ( AssertionError: è¯¥ç®—æ³•ä»…æ”¯æŒ (&lt;class &#39;gymnasium.spaces.box.Box&#39;&gt;, &lt;class &#39;gymnasium.spaces.discrete.Discrete&#39;&gt;, &lt;class &#39;gymnasium.spaces.multi_discrete.MultiDiscrete&#39;&gt;, &lt;class &#39;gymnasium.spaces.multi_binary.MultiBinary&#39;&gt;) ä½œä¸ºåŠ¨ä½œç©ºé—´ï¼Œä½†æä¾›äº† Discrete(4)  ``` [1]: https://github.com/DLR-RM/stable-baselines3/issues/137#issuecomment-669862467    ç”±   æäº¤  /u/Academic-Rent7800   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz885r/how_do_i_log_observations_after_reset_in_stable/</guid>
      <pubDate>Tue, 09 Jul 2024 17:27:41 GMT</pubDate>
    </item>
    <item>
      <title>â€œè®¤çŸ¥æ ¡å‡†å’Œå¯»æ‰¾çœŸç›¸ç©ºé—´â€ï¼ŒLinus Leeï¼ˆåå¥½è°ƒæ•´å›¾åƒç”Ÿæˆå™¨æ¨¡å‹ä¸­çš„æ¨¡å¼å´©æºƒ - DALL-E 3 ä¸ 2 çš„æ— èŠä¹‹å¤„ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz5sbs/epistemic_calibration_and_searching_the_space_of/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz5sbs/epistemic_calibration_and_searching_the_space_of/</guid>
      <pubDate>Tue, 09 Jul 2024 15:49:18 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ RL ç”Ÿæˆè·¯å¾„</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz4mcu/path_generation_using_rl/</link>
      <description><![CDATA[æˆ‘ä¸€ç›´åœ¨å°è¯•è®­ç»ƒä¸€ä¸ª RL æ¨¡å‹ï¼Œè¿™æ ·ç»™å®šä¸€ä¸ª 3D ç©ºé—´ä¸­çš„ç›®æ ‡å’Œä¸€ä¸ªåˆå§‹èµ·å§‹ä½ç½®ï¼Œå°±ä¼šé‡‡å–å°æ­¥éª¤æ¥è¾¾åˆ°ç›®æ ‡â€‹â€‹ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šå°±åƒä»åˆå§‹ä½ç½®åˆ°æœ€ç»ˆä½ç½®çš„è·¯å¾„ç”Ÿæˆã€‚æˆ‘ä¸€ç›´åœ¨ä½¿ç”¨ç¨³å®šçš„åŸºçº¿ 3 å’Œå¥èº«æˆ¿ã€‚åŠ¨ä½œç©ºé—´æ˜¯è¿ç»­çš„ï¼Œåœ¨ xã€y å’Œ z æ–¹å‘ä¸Šä» -1 åˆ° 1ã€‚è§‚å¯Ÿç©ºé—´ä¹Ÿæ˜¯è¿ç»­çš„ï¼Œåœ¨ xã€y å’Œ z æ–¹å‘ä¸Šä» -10 åˆ° 10ã€‚æˆ‘ç»™å‡ºçš„å¥–åŠ±è¦ä¹ˆæ˜¯å¯†é›†å¥–åŠ±ï¼Œè¿™å–å†³äºä»å½“å‰ä½ç½®åˆ°ç›®æ ‡çš„è·ç¦»ï¼Œè¦ä¹ˆæ˜¯ç¨€ç–å¥–åŠ±ï¼ˆå¦‚æœåˆ°ç›®æ ‡çš„è·ç¦»å¤§äºæŸä¸ªé˜ˆå€¼ï¼Œåˆ™ä¸º -1ï¼Œå¦åˆ™å¥–åŠ±ä¸º 0ï¼‰ã€‚ æˆ‘å°è¯•äº†å„ç§æƒ…èŠ‚é•¿åº¦å’Œæ­¥é•¿ã€‚æˆ‘é—æ¼äº†ä»€ä¹ˆæˆ–åšé”™äº†ä»€ä¹ˆå—ï¼Ÿ    æäº¤äºº    /u/Necessary-Cabinet-43   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz4mcu/path_generation_using_rl/</guid>
      <pubDate>Tue, 09 Jul 2024 15:02:16 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•è®­ç»ƒç›¸åŒçš„ RL ä»£ç†ï¼ˆå…·æœ‰å•ç‹¬çš„ç­–ç•¥ï¼‰ä»¥åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ä¸­ä¼ é€’ä¿¡æ¯ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz33aa/how_to_train_identical_rl_agents_with_individual/</link>
      <description><![CDATA[ä½ å¥½ æˆ‘æ­£åœ¨ StabeBaseline3 ä¸­ä½¿ç”¨ SAC è¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œå®ƒå¯¹äºå•ä¸ªä»£ç†æ¥è¯´è¿è¡Œè‰¯å¥½ã€‚ç°åœ¨ï¼Œæˆ‘æƒ³è®­ç»ƒå¤šä¸ªç›¸åŒçš„ä»£ç†ï¼ˆä½¿ç”¨å®ƒä»¬è‡ªå·±çš„å•ç‹¬ç­–ç•¥ï¼‰ï¼Œä»¥ä¾¿æ¯ä¸ªä»£ç†çš„æ“ä½œéƒ½ç”¨äºä¸ºæ‰€æœ‰ä»£ç†ç”Ÿæˆæ–°çš„è§‚å¯Ÿç»“æœã€‚ ä½ èƒ½å»ºè®®æˆ‘ä¸€äº›å¯ä»¥å¸®åŠ©æˆ‘å®Œæˆæ­¤æ“ä½œçš„åº“å—ï¼Ÿä¸€ä¸ªå°å‹çš„å·¥ä½œç¤ºä¾‹å°†æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚æˆ‘çœ‹è¿‡ RLIB (ray)ï¼Œä½†ä»ç„¶ä¸çŸ¥é“å¦‚ä½•åœ¨å…¶ä¸­ä½¿ç”¨ stablebaseline3ã€‚ è°¢è°¢    æäº¤äºº    /u/Previous-Advance-921   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz33aa/how_to_train_identical_rl_agents_with_individual/</guid>
      <pubDate>Tue, 09 Jul 2024 13:58:04 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ Sigmoidï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz1xbn/why_isnt_sigmoid_used/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘æ­£åœ¨ä½¿ç”¨ Unity åœ¨ C# ä¸­ä»å¤´å¼€å§‹åˆ¶ä½œä¸€ä¸ªç®€å•çš„ç­–ç•¥æ¢¯åº¦å­¦ä¹ ç®—æ³•ï¼Œæ²¡æœ‰åº“ï¼Œæˆ‘æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæ²¡æœ‰äººä½¿ç”¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ sigmoid å‡½æ•°ä½œä¸ºè¾“å‡º ä¸€åˆ‡éƒ½å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ï¼Œæ¯ä¸ªäººéƒ½ä½¿ç”¨ softmax å‡½æ•°æ¥è¾“å‡ºä»£ç†å¯ä»¥é‡‡å–çš„åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œç„¶åä»–ä»¬éšæœºï¼ˆåå‘æ›´é«˜çš„åŠ¨ä½œï¼‰é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œä½†è¿™ç§æ–¹æ³•åªå…è®¸ä»£ç†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹æ‰§è¡Œä¸€ä¸ªåŠ¨ä½œï¼Œä¾‹å¦‚ã€‚å®ƒæ—¢å¯ä»¥å‘å‰ç§»åŠ¨ï¼Œä¹Ÿå¯ä»¥å°„å‡»ï¼Œä½†æˆ‘ä¸èƒ½åŒæ—¶åšè¿™ä¸¤ä»¶äº‹ï¼Œæˆ‘çŸ¥é“æœ‰æ–¹æ³•å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå³ä¸ºä»£ç†å¯ä»¥é‡‡å–çš„æ¯ç»„åŠ¨ä½œåˆ¶ä½œå¤šä¸ªè¾“å‡ºå±‚ï¼Œä½†æˆ‘æƒ³çŸ¥é“ä½ æ˜¯å¦ä¹Ÿå¯ä»¥æœ‰ä¸€ä¸ªæ˜ å°„åˆ°åŠ¨ä½œçš„ S å½¢è¾“å‡ºå±‚  æ¯”å¦‚å¦‚æœæˆ‘è®©ä¸€ä¸ªä»£ç†å­¦ä¹ èµ°è·¯å’Œå°„å‡»æ•Œäººï¼Œä½¿ç”¨è½¯æœ€å¤§å€¼ï¼Œä½ ä¼šæœ‰ä¸€ä¸ªç”¨äºèµ°è·¯çš„è¾“å‡ºå±‚å’Œä¸€ä¸ªç”¨äºå°„å‡»çš„è¾“å‡ºå±‚ï¼Œä½†ä½¿ç”¨ S å½¢ï¼Œä½ åªéœ€è¦ä¸€ä¸ªè¾“å‡ºå±‚ï¼Œå…¶ä¸­ 5 ä¸ªç¥ç»å…ƒæ˜ å°„åˆ° 4 ä¸ªæ–¹å‘çš„ç§»åŠ¨å’Œå°„å‡»ï¼Œè¿™å–å†³äºç¥ç»å…ƒè¾“å‡ºçš„å€¼æ˜¯å¦å¤§äº 0.5 TLDRï¼šè€Œä¸æ˜¯ä½¿ç”¨è½¯æœ€å¤§å€¼å‡½æ•°å±‚æˆ–å¤šå±‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªå¤§çš„å±‚ï¼Œå…¶ä¸­ S å½¢å‡½æ•°æ˜ å°„åˆ°åŸºäºå€¼æ˜¯å¦å¤§äº 0.5 çš„åŠ¨ä½œ    æäº¤äºº    /u/DaMrStick   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz1xbn/why_isnt_sigmoid_used/</guid>
      <pubDate>Tue, 09 Jul 2024 13:05:37 GMT</pubDate>
    </item>
    <item>
      <title>æ¥è‡ªæ·±åº¦è§†é¢‘é¦ˆé€çš„ç«¯åˆ°ç«¯æ§åˆ¶ç½‘ç»œ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dz1hs0/end_to_end_control_network_from_depth_video_feed/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•åˆ›å»ºä¸€ä¸ªç½‘ç»œï¼Œè¯¥ç½‘ç»œä½¿ç”¨é»‘ç™½å›¾åƒï¼ˆæ·±åº¦å›¾ï¼‰ä»¥åŠåŸºæœ¬æƒ¯æ€§æµ‹é‡ï¼ˆé€Ÿåº¦ã€å§¿æ€ï¼‰æ¥è‡ªä¸»æ§åˆ¶å››è½´é£è¡Œå™¨å¹¶å…·æœ‰é¿éšœèƒ½åŠ›ã€‚æ‹Ÿè®®çš„ç½‘ç»œå°†è¾“å‡ºèº«ä½“é€Ÿç‡å‘½ä»¤ï¼Œè¿™äº›å‘½ä»¤å°†è¢«é¦ˆé€åˆ°ä½çº§ PID æ§åˆ¶å™¨ã€‚ è¿™æœ‰å¤šå¯è¡Œï¼Ÿæœ‰æ²¡æœ‰è¿™ç§ç«¯åˆ°ç«¯è§£å†³æ–¹æ¡ˆåœ¨ç°å®ä¸–ç•Œä¸­æˆåŠŸçš„ä¾‹å­ï¼Ÿï¼ˆä¸é™äºæ— äººæœºï¼‰ é€šå¸¸ï¼Œæˆ‘ä»¬é¦–å…ˆåšä¸€äº›åƒ SLAM å’Œ VIO è¿™æ ·çš„æ“ä½œæ¥è¿›è¡Œå®šä½å’ŒçŠ¶æ€ä¼°è®¡ï¼Œç„¶åæˆ‘ä»¬å¿…é¡»åˆ¶å®šä¸€ä¸ªè§„åˆ’ç®—æ³•ï¼Œç„¶åæˆ‘ä»¬æ‰èƒ½æœ‰ä¸€ä¸ªçœŸæ­£å‘å‡ºèº«ä½“é€Ÿç‡å‘½ä»¤çš„ç¥ç»ç½‘ç»œæ§åˆ¶å™¨ï¼Œè¿™å¬èµ·æ¥åƒæ˜¯ä¸€å †ä¸å¿…è¦çš„æŠ½è±¡ + å·¥ä½œã€‚ æˆ‘éå¸¸å¥½å¥‡æˆ‘æ˜¯å¦å‘ç°äº†ä»€ä¹ˆï¼Œæˆ‘åº”è¯¥å®ç°å®ƒğŸ˜…   ç”±    /u/FutureComedian7749  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dz1hs0/end_to_end_control_network_from_depth_video_feed/</guid>
      <pubDate>Tue, 09 Jul 2024 12:45:40 GMT</pubDate>
    </item>
    <item>
      <title>ä¸ºä»€ä¹ˆä¸ç¦»ç­–ç•¥ç®—æ³•ç›¸æ¯”ï¼ŒçŠ¶æ€è¡¨ç¤ºå­¦ä¹ æ–¹æ³•ï¼ˆé€šè¿‡è¾…åŠ©æŸå¤±ï¼‰è¾ƒå°‘åº”ç”¨äº PPO ç­‰åœ¨çº¿ç­–ç•¥ RL ç®—æ³•ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</link>
      <description><![CDATA[æˆ‘å·²ç»çœ‹åˆ°äº†ä¸åŒçš„çŠ¶æ€è¡¨å¾å­¦ä¹ æ–¹æ³•ï¼ˆé€šè¿‡è¾…åŠ©æŸå¤±ï¼Œæ— è®ºæ˜¯è‡ªæˆ‘é¢„æµ‹è¿˜æ˜¯åŸºäºç»“æ„åŒ–æ¢ç´¢ï¼‰ï¼Œå®ƒä»¬å·²ç»ä¸ç¦»çº¿ç­–ç•¥æ–¹æ³•ï¼ˆå¦‚ DQNã€Rainbowã€SAC ç­‰ï¼‰ä¸€èµ·åº”ç”¨ã€‚ä¾‹å¦‚ï¼ŒSPRï¼ˆè‡ªæˆ‘é¢„æµ‹è¡¨å¾ï¼‰å·²ä¸ Rainbow ä¸€èµ·ä½¿ç”¨ï¼ŒCURLï¼ˆå¼ºåŒ–å­¦ä¹ çš„å¯¹æ¯”æ— ç›‘ç£è¡¨å¾ï¼‰å·²ä¸ DQNã€Rainbow å’Œ SAC ä¸€èµ·ä½¿ç”¨ï¼ŒRA-LapRepï¼ˆé€šè¿‡å›¾æ‹‰æ™®æ‹‰æ–¯è¿›è¡Œè¡¨å¾å­¦ä¹ ï¼‰å·²ä¸ DDPG å’Œ DQN ä¸€èµ·ä½¿ç”¨ã€‚æˆ‘å¾ˆå¥½å¥‡ä¸ºä»€ä¹ˆè¿™äº›æ–¹æ³•æ²¡æœ‰åƒ PPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰è¿™æ ·çš„åœ¨çº¿ç­–ç•¥ç®—æ³•å¾—åˆ°å¹¿æ³›åº”ç”¨ã€‚å°†è¿™äº›è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯ä¸åœ¨çº¿ç­–ç•¥ç®—æ³•å­¦ä¹ ç›¸ç»“åˆæ˜¯å¦å­˜åœ¨ç†è®ºé—®é¢˜ï¼Ÿ    æäº¤äºº    /u/C7501   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyzi9z/why_are_state_representation_learning_methods_via/</guid>
      <pubDate>Tue, 09 Jul 2024 10:56:37 GMT</pubDate>
    </item>
    <item>
      <title>æœ‰æ²¡æœ‰ä»»ä½•å…·æœ‰éšæœºæ€§çš„ç¦»çº¿ RL åŸºå‡†ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨ç ”ç©¶é£é™©æ•æ„Ÿçš„ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œå‘ç°è®¸å¤šä¼—æ‰€å‘¨çŸ¥çš„åŸºå‡†å¹¶ä¸åˆé€‚ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹éšæœºæ€§ã€‚Mujoco ç¯å¢ƒå‡ ä¹æ˜¯ç¡®å®šæ€§çš„ï¼›å®ƒä»¬çš„å†…éƒ¨ä»£ç ä¸åŒ…å«ä»»ä½•éšæœºæ€§ï¼Œé™¤äº†åœ¨â€œé‡ç½®â€æœŸé—´é˜¶æ®µã€‚ [å‚è§ï¼šhttps://arxiv.org/abs/2205.15967\]  æˆ‘å‘ç° NeoRL ä¹Ÿæ˜¯å¦‚æ­¤ [https://arxiv.org/abs/2102.00714.\] æ‚¨å¯ä»¥è½»æ¾éªŒè¯è¿è¡Œ TD3PlusBC æ—¶æ²¡æœ‰å·®å¼‚ã€‚ é‚£ä¹ˆï¼Œæœ‰é’ˆå¯¹å…·æœ‰éšæœºæ€§çš„ç¦»çº¿ RL çš„åŸºå‡†å—ï¼Ÿ    æäº¤äºº    /u/korsyoo   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyyr7j/any_benchmark_for_offline_rl_with_stochasticity/</guid>
      <pubDate>Tue, 09 Jul 2024 10:09:56 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å¤„ç†3Dä½“ç´ è§‚å¯Ÿï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</link>
      <description><![CDATA[æˆ‘ç›®å‰åœ¨ä½¿ç”¨ 3D ä½“ç´ çŠ¶æ€è®­ç»ƒ PPO æ—¶é‡åˆ°äº†å›°éš¾ã€‚ 3D ä½“ç´ çš„å½¢çŠ¶ä¸º [64, 128, 128]ï¼ŒåŒºåŸŸä¿¡æ¯å¾ˆé‡è¦ã€‚ åªä½¿ç”¨ 3D CNN ç¼–ç å™¨å¯ä»¥å—ï¼Ÿ æˆ‘æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ–°æ‰‹ï¼Œæˆ‘è¿˜æ²¡æœ‰çœ‹åˆ°ä»»ä½•ä½¿ç”¨ 3D ç¼–ç å™¨çš„è®ºæ–‡ï¼Œè€Œä¸”å¤§å¤šæ•° RL æ•™ç¨‹éƒ½ä½¿ç”¨ 2d CNN ç¼–ç å™¨æˆ–ä»…ä½¿ç”¨ MLP     æäº¤äºº    /u/MediocreAgency6070   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyvblp/how_to_handle_3d_voxel_observation/</guid>
      <pubDate>Tue, 09 Jul 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>RLHubï¼šå¼ºåŒ–å­¦ä¹ ç¯å¢ƒçš„ç»Ÿä¸€å¹³å° - å¯»æ±‚åé¦ˆï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘çš„ RL ä¼™ä¼´ä»¬ï¼ æˆ‘æ˜¯åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡çš„åšå£«åï¼Œå’Œå‡ ä¸ªæœ‹å‹ä¸€èµ·åœ¨å¼€å±•ä¸€ä¸ªåä¸º RLHub çš„æ–°é¡¹ç›®ï¼Œæˆ‘å¾ˆæƒ³å¬å¬ä½ ä»¬çš„æƒ³æ³•ã€‚æˆ‘ä»¬çš„æƒ³æ³•æ˜¯åˆ›å»ºä¸€ä¸ªæ ‡å‡†åŒ–çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒå¹³å°ï¼Œç±»ä¼¼äº Hugging Face ä¸º NLP æ¨¡å‹æ‰€åšçš„å·¥ä½œã€‚ ä¸»è¦åŠŸèƒ½ï¼š1. é€‚ç”¨äºå„ç§ RL ç¯å¢ƒï¼ˆmujocoã€unityã€gym ç­‰ï¼‰çš„ç»Ÿä¸€ API 2. è½»æ¾ä¸Šä¼ å’Œå…±äº«è‡ªå®šä¹‰ç¯å¢ƒ 3. è‡ªåŠ¨ä¾èµ–é¡¹ç®¡ç† 4. æœ¬åœ°å’Œäº‘æ‰§è¡Œé€‰é¡¹ 5. æ ‡å‡†åŒ–å…ƒæ•°æ®å’Œæ–‡æ¡£ å¯èƒ½çš„é™„åŠ åŠŸèƒ½ï¼š- æ ‡å‡†åŒ–ä¸»è¦ç®—æ³•ï¼ˆPPOã€DDPGã€TD3â€¦â€¦ï¼‰UIï¼Œç”¨äºåœ¨äº‘ç«¯è®­ç»ƒä»£ç† ç›®æ ‡æ˜¯ç®€åŒ–æŸ¥æ‰¾ã€ä½¿ç”¨å’Œå…±äº« RL ç¯å¢ƒçš„è¿‡ç¨‹ã€‚ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾åœ°åœ¨å¤šç§ç¯å¢ƒä¸­å°è¯•ä»–ä»¬çš„ç®—æ³•ï¼Œç¯å¢ƒåˆ›å»ºè€…å¯ä»¥æ¥è§¦åˆ°æ›´å¹¿æ³›çš„å—ä¼—ã€‚ æˆ‘æœ‰ä¸€äº›é—®é¢˜ï¼š1. è¿™å¯¹æ‚¨çš„å·¥ä½œæœ‰ç”¨å—ï¼Ÿ2. æ‚¨ä¼šä¼˜å…ˆè€ƒè™‘å“ªäº›åŠŸèƒ½ï¼Ÿ3. å¯¹æ ‡å‡†åŒ–æœ‰ä»€ä¹ˆé¡¾è™‘ï¼Ÿ4. å…³äºåœ¨ MVP ä¸­åŒ…å«äº‘æ‰§è¡Œçš„æƒ³æ³•ï¼Ÿ æˆ‘ç‰¹åˆ«æƒ³å¬å¬ RL ç ”ç©¶äººå‘˜å’Œä»ä¸šäººå‘˜çš„æ„è§ã€‚æ‚¨å¯¹å½“å‰ RL ç¯å¢ƒç®¡ç†æœ‰å“ªäº›ç—›ç‚¹ï¼Œå¯ä»¥è§£å†³è¿™äº›ç—›ç‚¹å—ï¼Ÿ æå‰æ„Ÿè°¢æ‚¨çš„ä»»ä½•åé¦ˆï¼    æäº¤äºº    /u/elonmusk-A12   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dyrza8/rlhub_a_unified_platform_for_reinforcement/</guid>
      <pubDate>Tue, 09 Jul 2024 03:03:51 GMT</pubDate>
    </item>
    </channel>
</rss>