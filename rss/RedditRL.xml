<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 10 Feb 2024 09:13:02 GMT</lastBuildDate>
    <item>
      <title>欧洲硕士研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1an9gwm/masters_research_in_europe/</link>
      <description><![CDATA[嘿，有人知道欧盟有哪些地方在强化学习方面有很好的研究吗？我正在攻读硕士学位，并想申请强化学习研究实力雄厚的地方。我已经申请了这些地方：1. UPF 巴塞罗那 2. ETH 苏黎世 任何帮助，我们将不胜感激。谢谢   由   提交 /u/FlyTrain1011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1an9gwm/masters_research_in_europe/</guid>
      <pubDate>Sat, 10 Feb 2024 06:00:05 GMT</pubDate>
    </item>
    <item>
      <title>学习离线和离线策略强化学习的最佳教程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1an90bl/best_tutorials_for_learning_offline_and_off/</link>
      <description><![CDATA[我想知道我能在离线强化学习中找到的最好的教程是什么？    由   提交/u/miladink   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1an90bl/best_tutorials_for_learning_offline_and_off/</guid>
      <pubDate>Sat, 10 Feb 2024 05:31:49 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助解决此问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1an36g0/need_help_solving_this_problem/</link>
      <description><![CDATA[      ​ https://preview.redd.it/ossvhnxujnhc1.png?width=1104&amp;format=png&amp;auto=webp&amp;s=794397399523f851bf125347b62f5c9250b cc6df 对于这个问题变量转换概率为 A = 0.69、B = 0.31、C = 0.63、D = 0.37、E = 0.79 和 F = 0.21。令连续迭代之间的最大误差ε=0.01，折扣因子γ=0.2。将答案四舍五入到小数点后两位，并使用句点作为小数分隔符。 使用策略迭代方法，状态“Standing”的值是多少？  我在理解这个概念上仍然遇到一些困难，并且非常感谢在解决这个问题上的一些帮助。谢谢！   由   提交 /u/thesmudgelord   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1an36g0/need_help_solving_this_problem/</guid>
      <pubDate>Sat, 10 Feb 2024 00:26:28 GMT</pubDate>
    </item>
    <item>
      <title>RL 的 Q-LP 公式 - 有限水平情况</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amxhnm/qlp_formulation_of_rl_finite_horizon_case/</link>
      <description><![CDATA[对于情景任务（有限视野）的情况，Q-LP 优化问题能否解决？    由   提交 /u/MomoSolar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amxhnm/qlp_formulation_of_rl_finite_horizon_case/</guid>
      <pubDate>Fri, 09 Feb 2024 20:16:09 GMT</pubDate>
    </item>
    <item>
      <title>洗衣折叠机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amq5d2/laundry_folding_bot/</link>
      <description><![CDATA[当前的 RL 是否可以实现洗衣折叠机器人？布料可能的褶皱组合空间是否太大、太复杂？大家的想法是什么   由   提交/u/nodel_official  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amq5d2/laundry_folding_bot/</guid>
      <pubDate>Fri, 09 Feb 2024 15:04:23 GMT</pubDate>
    </item>
    <item>
      <title>寻求建议：为 DQN 代理在日前和实时市场进行电力交易设计奖励功能”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ampte0/seeking_advice_designing_a_reward_function_for/</link>
      <description><![CDATA[我正在尝试为我的 DQN 代理设计一个奖励函数。实际上，代理决定在日前交易一些电力，并在实时市场中交易一些电力。日前的收入使用以下公式计算：DA 价格 * 交易单位，对于实时市场，我们将奖励计算为（实时单位 - 日前单位）* 实时价格。现在，有一些限制。在 Day Ahead 市场中，我最多只能购买 200 MW 并出售 200 MW，实时市场也是如此。在Day Ahead市场中，我们需要交易的最大单位是800，实时交易也是如此。现在，问题出现了：我不想通过实时持有头寸来赚钱，如果这些头寸是在未来一天持有的，除非实时时间是最低或最高。我想说的是，我想通过在两个市场的最低时段买入并在最高时段卖出来赚钱；我想赚钱没有其他方法。 我尝试使用多种奖励函数，但代理试图最大化累积收入，例如在 Day Ahead 中赔钱，然后通过持有来实时赚钱未来一天采取的立场。因此，我认为代理试图通过利用两个市场的 LMP 之间的差异来赚钱。 代理的观察包含所有重要的预测特征。 我需要帮助设计奖励函数。   由   提交/u/uonliaquat  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ampte0/seeking_advice_designing_a_reward_function_for/</guid>
      <pubDate>Fri, 09 Feb 2024 14:49:40 GMT</pubDate>
    </item>
    <item>
      <title>PPO 自动驾驶汽车代理培训突然崩溃</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amlfpd/ppo_training_for_autonomous_car_agent_suddenly/</link>
      <description><![CDATA[嗨， 我正在尝试训练 PPO 模型（使用稳定的基线3）来在 2D 中构建自动驾驶汽车模拟我注册为健身房环境的城市道路。任务是在地图上尽可能快地行驶而不终止（与墙壁或其他汽车相撞）。为此，我使用以下奖励函数： R_speed = |v - v_speedlimit | / v_speedlimit，其中 v 是当前速度。 R_angle = cos(alpha)，其中 alpha 是汽车与车道方向之间的角度。 R =（1 终止） x R_speed x R_angle - 终止 x 100 环境是一个完整城市的鸟瞰图，因此它是一个相当大的地图。问题是，尽管智能体在学习以限速行驶并采取弯道的同时，将其奖励增加到约 500k 时间步长的好点，但突然之间，它就像完全忘记了一切，高速直线行驶并撞上了障碍物它首先遇到。我读过类似的帖子并尝试了建议的超参数调整，但尚未解决问题。我怀疑一段时间后，它开始过度适应在直路或小曲率道路上执行的操作，因为它们占地图的 70%。由于特工不会再轻易死亡，所以它经常在这些道路上行驶。当它终止时，我将起点重置为随机位置。  我使用连续动作空间： - 油门和刹车的组合：[-1（完全刹车），1（全油门）] - 转向角度：[-1, 1]弧度 ​ 以下是我尝试过的一些超参数和网络配置： Net arch: [ 128,128]、[64,64]、[256,256] 等 激活 fn：tanh、ReLU、ReLU6 Ent。系数：0, 0.001, 0.01, 0.02 剪辑范围：0.2, 0.1，线性计划从 0.2 到 0.05 n_steps : 4096 n_envs : 1 (我无法并行化训练，这可能是问题所在吗？） batch size: 64   由   提交 /u/Few-Pen-9807   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amlfpd/ppo_training_for_autonomous_car_agent_suddenly/</guid>
      <pubDate>Fri, 09 Feb 2024 10:47:05 GMT</pubDate>
    </item>
    <item>
      <title>RL新手，但非常感兴趣</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amazbb/rl_newbie_but_extremely_interested/</link>
      <description><![CDATA[背景：我对人工智能着迷已经有一段时间了，自从我完成了我的batcelor之后，我就取得了一个拥有相当多人工智能的硕士学位选项，经过监督学习，非常喜欢它，我达到了强化学习课程的地步，我非常喜欢它，我发现自己在提前学习，并在网上寻找越来越多的问题我已经经历过：qlearning、dqn、double dqn、dueling dqn、rainbow dqn、DGP、DDGP、PPO、A2C、A3C，在我的列表中，我有分层强化对于他们中的大多数人，我了解这一切是如何运作的，虽然我不得不承认我对诸如决斗 DQN 之类的事情仍然有点模糊（我明白什么是不同的，以及为什么这样做，但对我来说感觉不自然）这是否给了我足够的基础来做一个对于我的课程来说，这个项目足够好？（老师提到，如果它足够好，我们可以把它变成一篇研究论文或其他东西，我有点计划在 RL 上完成我的硕士论文） 我想我所有的问题都变成了to：现在这些够了吗？我还应该探索哪些其他领域，以及什么是“足够好”？   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amazbb/rl_newbie_but_extremely_interested/</guid>
      <pubDate>Fri, 09 Feb 2024 00:33:12 GMT</pubDate>
    </item>
    <item>
      <title>PC组件的想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1amasij/pc_component_thoughts/</link>
      <description><![CDATA[所以我想为 ML 构建一台电脑，而且我真的对 RL 更感兴趣 我得到了一个 7900gre 所以 gpu 是尽管我知道 NVidia 在这个市场上表现出色，但我有点希望 AMD 能够加强 ROCm 或他们拥有的其他一些开放魔法，这样这款 GPU 至少是不错的（如果我可以与任何 NVidia GPU 进行比较，它会太棒了，这样我就知道我对它的立场了，比如它比 3060 更好吗？考虑到所有因素） 接下来是 ram 和 CPU，我想知道 cpu 数量、线程数量、cpu 是多少时钟，缓存大小对 RL 很重要，我正在研究 7600x（如果它真的不重要）/ 7700（如果它有点重要），或者 7900x（如果它真的很重要），也正在研究 32 GB 内存或 64 GB，如果这真的很重要（速度重要吗？） 我知道这篇文章到处都是，但我不知道如何构建它，我有很多问题，总体来说 cpu/ram 影响有多大ML/RL 应用？   由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1amasij/pc_component_thoughts/</guid>
      <pubDate>Fri, 09 Feb 2024 00:24:23 GMT</pubDate>
    </item>
    <item>
      <title>神经常微分方程的替代方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alyyqq/alternate_of_neural_ode/</link>
      <description><![CDATA[https://arxiv.org/abs/2401.01836 我看到这篇论文，其中 NODEC 是为了实现未知动力系统的最优控制而实现的，我想知道我们还可以使用哪些其他方法来解决类似的问题。我知道强化学习，但我正在寻找更有效的数据。表征学习或模仿学习可能吗？或者有其他方法可以改善结果？   由   提交/u/Striking-Cricket788  /u/Striking-Cricket788  reddit.com/r/reinforcementlearning/comments/1alyyqq/alternate_of_neural_ode/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alyyqq/alternate_of_neural_ode/</guid>
      <pubDate>Thu, 08 Feb 2024 15:59:57 GMT</pubDate>
    </item>
    <item>
      <title>有哪些必须知道的算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alsnyj/what_are_the_mustknow_algorithms/</link>
      <description><![CDATA[我 3 个月前开始攻读 RL/IL 博士学位，我想知道哪些是必须知道的算法，以免因为我缺乏知识而受到限制知识。当然，我可以全部学习，但是太多了，我不确定这是否有用。  目前我已经了解了Q学习、SARSA、DQN和PPO的机制和方程。接下来您有什么建议？   由   提交/u/Ybrik410  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alsnyj/what_are_the_mustknow_algorithms/</guid>
      <pubDate>Thu, 08 Feb 2024 10:24:21 GMT</pubDate>
    </item>
    <item>
      <title>我创建了很棒的持续强化学习存储库！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alrprr/i_created_awesome_continual_rl_repository/</link>
      <description><![CDATA[您好，我是一名研究生，对连续强化学习领域感兴趣。 我想获取有关各种信息关于这个领域的论文，所以我创建了一个很棒的存储库。 请随时给我任何建议！！ ​ https://github.com/windust7/awesome-continual-reinforcment-learning &lt; !-- SC_ON --&gt;  由   提交 /u/Mission-Lawyer1787    reddit.com/r/reinforcementlearning/comments/1alrprr/i_created_awesome_continual_rl_repository/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alrprr/i_created_awesome_continual_rl_repository/</guid>
      <pubDate>Thu, 08 Feb 2024 09:15:47 GMT</pubDate>
    </item>
    <item>
      <title>[DQN] 我在这个问题上完全迷失了方向，正在寻求可能出现问题的指导。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1alklc4/dqn_i_am_hopelessly_lost_on_this_issue_seeking/</link>
      <description><![CDATA[我一直在尝试创建一个 DQN 来解决井字游戏。我取得了成功，因此我尝试通过添加优先体验重播来改进它。 在获得实际 PER 之前，我只想开始自己进行随机采样。之前，我让 TensorFlow 通过以下代码进行采样： history = self.brain.fit(state_arr, qValueEstimates, batch_size=self.trainingBatchSize, verbose=0)&lt; /p&gt; 为了清楚起见：  state_arr 是 MxS，其中 S 是一维形状数组的大小，M 是体验回放的大小。 qValueEstimates 是 MxA，其中 A 是 1D 操作数组的大小。  ​ 当我将代码更改为： population = range(M)  sampleIndices = random.sample(population, self.trainingBatchSize)  &lt; code&gt;history = self.brain.fit(state_arr[sampleIndices], qValueEstimates[sampleIndices], batch_size=self.trainingBatchSize, verbose=0) 此命令运行，但算法不再学习。 python库不够随机吗？我不知道这里可能存在什么问题。   由   提交 /u/Garjiglio   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1alklc4/dqn_i_am_hopelessly_lost_on_this_issue_seeking/</guid>
      <pubDate>Thu, 08 Feb 2024 02:12:05 GMT</pubDate>
    </item>
    <item>
      <title>高效的零动态网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aljipl/efficient_zero_dynamics_network/</link>
      <description><![CDATA[我正在研究高效的零实现，我注意到他们的动态网络通过转换层将输入压缩一个通道 然而，他们然后在残差/身份操作中添加原始 x，但没有最后一个通道 我想知道这是为什么？ 编辑：进一步挖掘，似乎他们将操作附加到状态，因此他们删除了 cnn 之前的操作以获得身份状态值   由   提交/u/proturtle46  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aljipl/efficient_zero_dynamics_network/</guid>
      <pubDate>Thu, 08 Feb 2024 01:20:49 GMT</pubDate>
    </item>
    <item>
      <title>使用 A2C 训练的纸牌游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1al84n0/cards_game_trained_using_a2c/</link>
      <description><![CDATA[查看我如何使用 Tensorflow 和 Openai Gym 使用 A2C 算法训练纸牌游戏代理。 https://youtu.be/Odaa9T6PxkQ?si=qned3bP2n60eBGma   由   提交 /u/mehulgupta7991   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1al84n0/cards_game_trained_using_a2c/</guid>
      <pubDate>Wed, 07 Feb 2024 17:17:48 GMT</pubDate>
    </item>
    </channel>
</rss>