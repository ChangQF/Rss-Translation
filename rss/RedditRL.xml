<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 03 Feb 2025 18:22:04 GMT</lastBuildDate>
    <item>
      <title>éœ€è¦æŒ‡å¯¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ‹¥æœ‰æ•°å­¦å­¦ä½ï¼Œå¹¶é€‰ä¿®äº†å‡ é—¨æœºå™¨å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹  (RL) è¯¾ç¨‹ã€‚ç›®å‰ï¼Œæˆ‘æ­£åœ¨å·¥ä½œï¼Œä½†æˆ‘å¯¹ RL ç ”ç©¶éå¸¸æ„Ÿå…´è¶£ã€‚è™½ç„¶æˆ‘è¿˜æ²¡æœ‰å¤ªå¤šçš„çŸ¥è¯†ï¼Œä½†æˆ‘åœ¨ç©ºé—²æ—¶é—´å­¦ä¹  RLã€‚ æœªæ¥ï¼Œæˆ‘æƒ³ä»äº‹ RL ç ”ç©¶å·¥ä½œï¼Œä½†æˆ‘ä¸çŸ¥é“è¯¥æ€ä¹ˆåšã€‚æˆ‘åº”è¯¥å‡†å¤‡ GATE å¹¶ç”³è¯· IIT/IIScï¼Œè¿˜æ˜¯åº”è¯¥ç›´æ¥ç”³è¯·é¡¶å°–çš„å¤–å›½å¤§å­¦ï¼Œå°½ç®¡æˆ‘æ²¡æœ‰ç ”ç©¶ç»éªŒï¼Ÿ    æäº¤äºº    /u/BigBuddy1276   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igozch/need_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 13:24:45 GMT</pubDate>
    </item>
    <item>
      <title>ç»“æœçš„å¯é‡å¤æ€§</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</link>
      <description><![CDATA[æ‚¨å¥½ï¼æˆ‘æ­£åœ¨å°è¯•æŸ¥æ‰¾æœ¬æ–‡ä¸­æåˆ°çš„åŸºäºæ¨¡å‹çš„ PPO çš„å®ç°ï¼šåŸºäºæ¨¡å‹çš„æ¢ç´¢çš„ç­–ç•¥ä¼˜åŒ–ï¼Œä»¥ä¾¿é‡ç°ç»“æœå¹¶å¯èƒ½åœ¨æˆ‘çš„è®ºæ–‡ä¸­ä½¿ç”¨è¯¥æ¶æ„ã€‚ä½†ä¼¼ä¹ä»»ä½•åœ°æ–¹éƒ½æ²¡æœ‰å®˜æ–¹å®ç°ã€‚æˆ‘å·²ç»ç»™ä½œè€…å‘äº†ç”µå­é‚®ä»¶ï¼Œä½†ä¹Ÿæ²¡æœ‰æ”¶åˆ°ä»»ä½•å›å¤ã€‚ åœ¨åƒ AAAI è¿™æ ·çš„å¤§å‹ä¼šè®®ä¸Šå‘è¡¨çš„è®ºæ–‡æ²¡æœ‰ä»»ä½•å¯é‡ç°çš„å®ç°ï¼Œè¿™æ­£å¸¸å—ï¼Ÿ    æäº¤äºº    /u/GamingOzz   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igouqe/reproducibility_of_results/</guid>
      <pubDate>Mon, 03 Feb 2025 13:18:10 GMT</pubDate>
    </item>
    <item>
      <title>â€œKimi k1.5ï¼šä½¿ç”¨ LLM æ‰©å±•å¼ºåŒ–å­¦ä¹ â€ï¼ŒKimi å›¢é˜Ÿ 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igodrz/kimi_k15_scaling_reinforcement_learning_with_llms/</guid>
      <pubDate>Mon, 03 Feb 2025 12:53:20 GMT</pubDate>
    </item>
    <item>
      <title>å¸®åŠ©æ¶ˆé™¤é”™è¯¯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</link>
      <description><![CDATA[å˜¿ï¼Œæˆ‘ç›®å‰æ­£åœ¨ä»¥æ·±åº¦ q å­¦ä¹ æ¨¡å‹çš„å½¢å¼è®­ç»ƒæˆ‘çš„ç¬¬ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚åœ¨å°è¯•ä½¿ç”¨ Python ä¸­çš„ keras æ—¶ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼Œå¦‚æœæœ‰äººæ„¿æ„å¸®åŠ©æˆ‘å¼„æ¸…æ¥šå¦‚ä½•è§£å†³è¿™äº›é—®é¢˜ï¼Œæˆ‘å°†ä¸èƒœæ„Ÿæ¿€ã€‚ï¼ˆå®ƒä»¬å¯¹äºæˆ‘çš„é¡¹ç›®æ¥è¯´éå¸¸å…·ä½“ï¼Œå› æ­¤å¾ˆéš¾åœ¨ DM ä¹‹å¤–è§£é‡ŠğŸ˜…ï¼‰    æäº¤äºº    /u/at_69_420   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igmp4s/help_squashing_an_error/</guid>
      <pubDate>Mon, 03 Feb 2025 11:08:58 GMT</pubDate>
    </item>
    <item>
      <title>ç¬¬ä¸€å±Š Tinker AI å¤§èµ›ä¼˜èƒœä½œå“ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</link>
      <description><![CDATA[        æäº¤äºº    /u/goncalogordo   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iglqdo/winning_submission_for_the_first_tinker_ai/</guid>
      <pubDate>Mon, 03 Feb 2025 10:00:02 GMT</pubDate>
    </item>
    <item>
      <title>å°è¯•å¤åˆ¶æ™®é€šçš„ k-bandits é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨å°è¯•å®ç° Barto Sutton ä¹¦ä¸­çš„ç¬¬ä¸€ä¸ª k-Bandits æµ‹è¯•å¹³å°ã€‚Python ä»£ç å¯åœ¨ Git ä¸Šæ‰¾åˆ°ï¼Œä½†æˆ‘æ­£å°è¯•ä»å¤´å¼€å§‹ç‹¬ç«‹å®Œæˆã€‚ æˆªè‡³ç›®å‰ï¼Œæˆ‘æ­£åœ¨å°è¯•ç”Ÿæˆå›¾ 2.2 ä¸­çš„å¹³å‡å¥–åŠ±å›¾ã€‚æˆ‘çš„ä»£ç å¯ä»¥å·¥ä½œï¼Œä½†å¹³å‡å¥–åŠ±å›¾è¿‡æ—©ç¨³å®šä¸‹æ¥ï¼Œå¹¶ä¸”ä¿æŒç¨³å®šï¼Œè€Œä¸æ˜¯åƒä¹¦ä¸­/git ä¸­é‚£æ ·å¢åŠ ã€‚æˆ‘æ— æ³•å¼„æ¸…æ¥šæˆ‘å“ªé‡Œåšé”™äº†ã€‚ å¦‚æœæœ‰äººèƒ½çœ‹ä¸€ä¸‹å¹¶åˆ†äº«ä¸€äº›æŠ€å·§ï¼Œé‚£å°†éå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæœ‰äººæƒ³è¿è¡Œ/æµ‹è¯•å®ƒï¼Œä»£ç åº”è¯¥æŒ‰åŸæ ·å·¥ä½œã€‚  éå¸¸æ„Ÿè°¢ï¼ ``` è¯¥ç¨‹åºå®ç°äº† k-bandit é—®é¢˜çš„ n æ¬¡è¿è¡Œ import numpy as np import matplotlib.pyplot as plt bandit_reward_dist_mean = 0 bandit_reward_dist_sigma = 1 k_bandits = 10 bandit_sigma = 1 samples_per_bandit = 1000 epsilon = 0.01 def select_action(): r = np.random.randn() if r &lt; epsilonï¼šaction = np.random.randintï¼ˆ0ï¼Œk_banditsï¼‰elseï¼šaction = np.argmaxï¼ˆq_estimatesï¼‰ è¿”å›æ“ä½œ def update_action_countï¼ˆA_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢å·²é‡‡å–æ¯ä¸ªæ“ä½œçš„æ¬¡æ•°n_action [A_t] + = 1 def update_action_reward_totalï¼ˆA_tï¼ŒR_tï¼‰ï¼š# åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªæ“ä½œçš„æ€»å¥–åŠ±action_rewards [A_t] + = R_t def generate_rewardï¼ˆmeanï¼Œsigmaï¼‰ï¼š# ä»æ­£æ€åˆ†å¸ƒä¸­ä¸ºè¿™ä¸ªç‰¹å®šçš„banditæå–å¥–åŠ±#r = np.random.normalï¼ˆmeanï¼Œsigmaï¼‰r = np.random.randnï¼ˆï¼‰+mean# ç±»ä¼¼äºåœ¨Git repoä¸­æ‰€åšçš„return r def update_qï¼ˆA_tï¼ŒR_tï¼‰ï¼š q_estimates[A_t] += 0.1 * (R_t - q_estimates[A_t]) n_steps = 1000 n_trials = 2000 #æ¯æ¬¡è¯•éªŒä½¿ç”¨ä¸€æ‰¹æ–°çš„è€è™æœºè¿è¡Œ n_steps æ‰€æœ‰è¯•éªŒä¸­æ¯ä¸€æ­¥çš„å¥–åŠ±çŸ©é˜µ - ä»é›¶å¼€å§‹ rewards_episodes_trials = np.zeros((n_trials, n_steps)) for j in range(0, n_trials): #q_true = np.random.normal(bandit_reward_dist_mean, bandit_reward_dist_sigma, k_bandits) q_true = np.random.randn(k_bandits) # å°è¯•å¤åˆ¶ book/git ç»“æœ # æ¯ä¸ªåŠ¨ä½œï¼ˆè€è™æœºï¼‰çš„ Q å€¼ - ä»random q_estimates = np.random.randn(k_bandits) # æ¯ä¸ªåŠ¨ä½œï¼ˆbanditï¼‰çš„æ€»å¥–åŠ± - ä»é›¶å¼€å§‹ action_rewards = np.zeros(k_bandits) # åˆ°ç›®å‰ä¸ºæ­¢æ¯ä¸ªåŠ¨ä½œå·²é‡‡å–çš„æ¬¡æ•° - ä»é›¶å¼€å§‹ n_action = np.zeros(k_bandits) # æ¯ä¸€æ­¥çš„å¥–åŠ± - ä» 0 å¼€å§‹ rewards_episodes = np.zeros(n_steps) for i in range(0, n_steps): A_t = select_action() R_t = generate_reward(q_true[A_t], bandit_sigma) rewards_episodes[i] = R_t  update_action_reward_total(A_t, R_t) update_action_count(A_t) update_q(A_t, R_t) rewards_episodes_trials[j,:] = rewards_episodes  æ‰€æœ‰è¿è¡Œä¸­æ¯æ­¥çš„å¹³å‡å¥–åŠ± average_reward_per_step = np.zeros(n_steps) for i in range(0, n_steps): average_reward_per_step[i] = np.mean(rewards_episodes_trials[:,i]) plt.plot(average_reward_per_step) plt.show() ```    æäº¤äºº    /u/datashri   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igleht/trying_to_replicate_the_vanilla_kbandits_problem/</guid>
      <pubDate>Mon, 03 Feb 2025 09:35:25 GMT</pubDate>
    </item>
    <item>
      <title>Vision RL å¸®åŠ©å’ŒæŒ‡å¯¼ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</link>
      <description><![CDATA[å‘èªæ˜çš„äººä»¬é—®å¥½ã€‚æˆ‘ä¸€ç›´åœ¨æ·±å…¥ç ”ç©¶ RLï¼Œæˆ‘è®¤ä¸ºé‚£ä¸ªç”·äººè·³å…¥æ¸¸æ³³æ± å´æ’åˆ°å†°å—çš„è§†é¢‘é€‚ç”¨äºæˆ‘ã€‚ https://jacomoolman.co.za/reinforcementlearning/ï¼ˆå‘ä¸‹æ»šåŠ¨æˆ–ç›´æ¥æœç´¢â€œvisionâ€ä»¥è·³è¿‡ä¸æˆ‘çš„é—®é¢˜æ— å…³çš„å†…å®¹ï¼‰ è¿™æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢çš„è¿›å±•ã€‚ä»»ä½•ä½¿ç”¨è¿‡è§†è§‰ RL çš„äººå¯èƒ½éƒ½èƒ½çœ‹å‡ºæˆ‘åšé”™äº†ä»€ä¹ˆï¼Ÿæˆ‘å·²ç»å°è¯•äº†å¤§çº¦ 2 ä¸ªæœˆï¼Œè¯•å›¾ä¸ºæ¨¡å‹æä¾›å›¾åƒè€Œä¸æ˜¯å˜é‡ï¼Œä½†æ²¡æœ‰æˆåŠŸã€‚    æäº¤äºº    /u/TheRealMrJm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 07:01:10 GMT</pubDate>
    </item>
    <item>
      <title>è¿™çœ‹èµ·æ¥åƒæ˜¯ç¨³å®šçš„ PPO æ”¶æ•›å—ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</link>
      <description><![CDATA[      è¿™çœ‹èµ·æ¥åƒç¨³å®šçš„ PPO æ”¶æ•›å—ï¼Ÿ    æäº¤äºº    /u/TopSigmaNoCap79970   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</guid>
      <pubDate>Mon, 03 Feb 2025 01:30:55 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ·±åº¦ç ”ç©¶ç®€ä»‹â€ï¼ŒOpenAIï¼ˆåŸºäº o3 çš„ç½‘é¡µæµè§ˆ/ç ”ç©¶ä»£ç†çš„ RL è®­ç»ƒï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</guid>
      <pubDate>Mon, 03 Feb 2025 01:29:39 GMT</pubDate>
    </item>
    <item>
      <title>â€œè‡ªæˆ‘éªŒè¯ï¼Œäººå·¥æ™ºèƒ½çš„å…³é”®â€ï¼ŒSutton 2001ï¼ˆæ˜¯ä»€ä¹ˆè®©æœç´¢å‘æŒ¥ä½œç”¨ï¼‰</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</link>
      <description><![CDATA[  ç”±    /u/gwern  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</guid>
      <pubDate>Sun, 02 Feb 2025 23:55:51 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘å¯ä»¥ä½¿ç”¨ RL æ¥è§£å†³æˆ‘çš„ opt. æ§åˆ¶é—®é¢˜å—</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iga0t5/can_i_use_rl_for_my_opt_control_problem/</link>
      <description><![CDATA[æ‚¨å¥½ï¼Œæˆ‘ç›®å‰æ­£åœ¨ä¼˜åŒ–ä¸€ä¸ªæ§åˆ¶é—®é¢˜ï¼Œè¯•å›¾å°†ç½‘ç»œä»éšæœºçŠ¶æ€è½¬å˜ä¸ºæœŸæœ›çŠ¶æ€ã€‚ å†³ç­–å˜é‡å¦‚ä¸‹ï¼š - ä¸€ä¸ªåŒ…å« n ä¸ªè¿ç»­å˜é‡çš„å‘é‡ - ä¸€ä¸ªåŒ…å« m ä¸ªäºŒè¿›åˆ¶å˜é‡çš„å‘é‡ ç›®å‰ï¼Œæˆ‘å¯¹è¿ç»­å˜é‡ä½¿ç”¨æ ‡å‡†å·®åˆ†è¿›åŒ–ï¼Œå¯¹äºŒè¿›åˆ¶å‘é‡ä½¿ç”¨ç‰¹æ®Šçš„äºŒè¿›åˆ¶å·®åˆ†è¿›åŒ–ç®—æ³•ã€‚æˆ‘æƒ³çŸ¥é“æˆ‘æ˜¯å¦å¯ä»¥ä¸ºè¿™ä¸ªè®¾ç½®å®ç°ä¸€ä¸ª RL æ¨¡å‹ï¼Ÿ æ›´å¤šä¿¡æ¯ï¼šn é€šå¸¸ä¸º 10-20ï¼Œm é€šå¸¸ä¸º 80-100ã€‚æˆ‘ç»è¿‡å¤§çº¦ 10000-20000 æ¬¡ä¸åŒçš„è¿›åŒ–å°è¯•åæ”¶æ•›åˆ°æŸç§æœŸæœ›çŠ¶æ€ï¼Œè¿™éœ€è¦ä¸€äº›æ—¶é—´ã€‚ä¸»è¦æ˜¯å› ä¸ºå¯¹äºæ¯æ¬¡è¿­ä»£ï¼Œæˆ‘éƒ½éœ€è¦è¿›è¡Œä¸€äº›è€—æ—¶çš„ç‰©ç†è®¡ç®—ã€‚å¯¹äºä¸åŒçš„è¿›åŒ–ç®—æ³•ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†è¿™äº›è®¡ç®—ï¼Œä½†æ˜¯ç”±äºè®¸å¯é—®é¢˜ï¼Œéœ€è¦èŠ±è´¹å¾ˆå¤šé’±ã€‚æ‰€ä»¥ç›®å‰ï¼Œæˆ‘æ¯æ¬¡åªèƒ½è¿›è¡Œ 1 æ¬¡è®¡ç®—ï¼Œå¤§çº¦éœ€è¦ 1 åˆ° 1.5 ç§’ã€‚æ€»å…±éœ€è¦ 10000-30000 ç§’æ‰èƒ½å®Œæˆã€‚ æ‚¨è®¤ä¸ºæˆ‘å­¦ä¹  RL å¹¶å°è¯•é€šè¿‡ RL è§£å†³è¿™ä¸ªé—®é¢˜å€¼å¾—å—ï¼Ÿè¯·è®°ä½ï¼ŒRL ç®—æ³•è¿˜éœ€è¦ä¸ºæ¯æ¬¡äº¤äº’ç­‰å¾… 1.5 ç§’ï¼Œå› ä¸ºæˆ‘ä¼šå°†å…¶æ“ä½œå‘é€åˆ°è®¸å¯è½¯ä»¶å¹¶æ¥æ”¶å…¶è¾“å‡ºã€‚ å½“å‰æµç¨‹ï¼š1ï¼‰ä½¿ç”¨åˆå§‹çŒœæµ‹å¯åŠ¨ç®—æ³• 2ï¼‰å°†æ‰€æœ‰åˆå§‹çŒœæµ‹å‘é€åˆ°é»‘ç›’è½¯ä»¶ 1by1 3ï¼‰ä»é»‘ç›’è½¯ä»¶ 1by1 æ¥æ”¶æ‰€æœ‰è¾“å‡º 4ï¼‰å¯åŠ¨å·®åˆ†è¿›åŒ–å¾ªç¯ 5ï¼‰åˆ›å»ºæ–°çš„çŒœæµ‹ 1by1 å°†å®ƒä»¬å‘é€åˆ°é»‘ç›’è·å–è¾“å‡ºå¹¶å¯¹å…¶è¿›è¡Œè¯„åˆ† 6ï¼‰å¦‚æœå¾—åˆ†æ›´é«˜ï¼Œåˆ™ç”¨æ–°çŒœæµ‹æ›¿æ¢æ—§çŒœæµ‹ 7ï¼‰é‡å¤ 5-6 æ¬¡ï¼Œå…± 10000 æ¬¡å¹¶è·å¾—æœ€ä½³å¾—åˆ†çŒœæµ‹    submitted by    /u/Icedkk   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iga0t5/can_i_use_rl_for_my_opt_control_problem/</guid>
      <pubDate>Sun, 02 Feb 2025 22:46:09 GMT</pubDate>
    </item>
    <item>
      <title>2025 å¹´ç§‹å­£ç¡•å£«/åšå£«ç”³è¯·</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼ éšç€æ‹›ç”Ÿå‘¨æœŸå…¨é¢å±•å¼€ï¼Œæˆ‘ç¥æ„¿æœ¬å‘¨æœŸç”³è¯·çš„æ‰€æœ‰äººå¥½è¿ï¼æˆ‘æ­£åœ¨ç”³è¯·ï¼Œè¿«ä¸åŠå¾…åœ°æƒ³å»ç ”ç©¶ç”Ÿé™¢åš RL ç ”ç©¶ï¼ˆåœ¨æˆ‘çš„å›½å®¶è¿™å¾ˆå°‘è§ï¼‰ã€‚ åœ¨è¯„è®ºä¸­å†™ä¸‹ä½ ç”³è¯·çš„åœ°æ–¹ä»¥åŠä½ æœ€æƒ³è¿›å…¥çš„åœ°æ–¹ã€‚ä¹Ÿè®¸å®‡å®™ä¼šå¬åˆ°ï¼Œæœºä¼šå°±ä¼šå¯¹ä½ æœ‰åˆ©ï¼    æäº¤äºº    /u/issyonibba   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</guid>
      <pubDate>Sun, 02 Feb 2025 19:47:59 GMT</pubDate>
    </item>
    <item>
      <title>GRPO ä¸­çš„ä»£å¸çº§ä¼˜åŠ¿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig0a3h/tokenlevel_advantages_in_grpo/</link>
      <description><![CDATA[åœ¨ GRPO æŸå¤±å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°æ¯ä¸ªè¾“å‡º (o_i) éƒ½æœ‰ä¸€ä¸ªå•ç‹¬çš„ä¼˜åŠ¿ï¼Œè¿™æ˜¯å¯ä»¥é¢„æ–™çš„ï¼Œæ¯ä¸ª token t ä¹Ÿæœ‰ä¸€ä¸ªå•ç‹¬çš„ä¼˜åŠ¿ã€‚æˆ‘æœ‰ä¸¤ä¸ªé—®é¢˜ï¼š  ä¸ºä»€ä¹ˆéœ€è¦ token çº§ä¼˜åŠ¿ï¼Ÿä¸ºä»€ä¹ˆä¸ç»™è¾“å‡ºä¸­çš„æ‰€æœ‰ token ç›¸åŒçš„ä¼˜åŠ¿ï¼Ÿ å¦‚ä½•è®¡ç®—è¿™ä¸ª token çº§ä¼˜åŠ¿ï¼Ÿ  æˆ‘è¿™é‡Œé—æ¼äº†ä»€ä¹ˆå—ï¼Ÿä» Hugginface TRL çš„å®ç°æ¥çœ‹ï¼Œä»–ä»¬ä¼¼ä¹æ²¡æœ‰æ‰§è¡Œä»¤ç‰Œçº§åˆ«çš„ä¼˜åŠ¿ï¼š https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py#L507    æäº¤äºº    /u/AdministrativeRub484   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig0a3h/tokenlevel_advantages_in_grpo/</guid>
      <pubDate>Sun, 02 Feb 2025 15:57:43 GMT</pubDate>
    </item>
    <item>
      <title>â€œè¿ˆå‘é€šç”¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ â€ï¼ŒFujimoto ç­‰äººï¼Œ2025 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</guid>
      <pubDate>Sun, 02 Feb 2025 12:02:03 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘å¯¹å­¦ä¹  RL çš„å»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</link>
      <description><![CDATA[æˆ‘è¯»è¿‡ Sutton å’Œ Barto çš„ä¹¦ï¼Œæœ‰æ—¶æˆ‘å‘ç°å¾ˆéš¾ç†è§£å…¶ä¸­çš„ä¸€äº›æ¦‚å¿µã€‚ç„¶åï¼Œæˆ‘å¼€å§‹æ¢ç´¢è¿™ä¸ªèµ„æºã€‚ç°åœ¨ï¼Œæˆ‘çœŸæ­£ç†è§£äº†ä»·å€¼è¿­ä»£å’Œå…¶ä»–åŸºæœ¬æ¦‚å¿µèƒŒåçš„å«ä¹‰ã€‚æˆ‘è®¤ä¸ºè¿™æœ¬ä¹¦åº”è¯¥åœ¨ Sutton å’Œ Barto çš„ä¹¦ä¹‹å‰æˆ–åŒæ—¶é˜…è¯»ã€‚è¿™çœŸæ˜¯ä¸€æœ¬å¾ˆæ£’çš„ä¹¦ï¼    æäº¤äºº    /u/demirbey05   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</guid>
      <pubDate>Sun, 02 Feb 2025 07:59:42 GMT</pubDate>
    </item>
    </channel>
</rss>