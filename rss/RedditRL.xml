<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 26 Jun 2024 06:21:55 GMT</lastBuildDate>
    <item>
      <title>如何解决运行 MuZero 常规时的错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dom7kr/how_to_resolve_errors_when_running_muzero_general/</link>
      <description><![CDATA[我是日本人，所以我使用翻译器。 当我尝试运行 MuZero general 时，出现以下错误消息 我想知道如何解决这个问题。 (base) C:\Users\kooou\study\muzero-general-master&gt;python muzero.py 欢迎使用 MuZero！以下是游戏列表： 0. atari 1. breakout 2. cartpole 3. connect4 4. gomoku 5. gridworld 6. lunarlander 7. simple_grid 8. spiel 9. tictactoe 10. twentyone 输入一个数字来选择游戏：9 2024-06-26 09:24:16,548 INFO worker.py:1770 -- 启动了本地 Ray 实例。回溯（最近一次调用最后一次）：文件“C:\Users\kooou\study\muzero-general-master\muzero.py”，第 650 行，在&lt;module&gt; muzero = MuZero(game_name) ^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\muzero.py&quot;, 第 122 行, 在 __init__ self.checkpoint[&quot;weights&quot;], self.summary = copy.deepcopy(ray.get(cpu_weights)) ^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\auto_init_hook.py&quot;, 第 21 行, 在 auto_init_wrapper 中 return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\client_mode_hook.py&quot;, 第 103 行, 在包装器中返回 func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\worker.py&quot;, 第 2630 行, 在获取值中, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\worker.py&quot;, 第863，在 get_objects 中引发 value.as_instanceof_cause() ray.exceptions.RayTaskError(IndexError): ray::CPUActor.get_initial_weights() (pid=24336, ip=127.0.0.1, actor_id=93a6ec0dc4612ea8190775ec01000000, repr=&lt;muzero.CPUActor object at 0x0000021E07EC1FD0&gt;) 文件 &quot;python\ray\_raylet.pyx&quot;，第 1893 行，在 ray._raylet.execute_task 文件 &quot;python\ray\_raylet.pyx&quot;，第 1834 行，在 ray._raylet.execute_task.function_executor 文件中&quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\function_manager.py&quot;，第 691 行，在 actor_method_executor 中返回方法（__ray_actor，*args，**kwargs） ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\util\tracing\tracing_helper.py&quot;，第 467 行，在 _resume_span 中返回方法（self，*_args，**_kwargs） ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\study\muzero-general-master\muzero.py&quot;, 第 489 行, 在 get_initial_weights model = models.MuZeroNetwork(config) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\models.py&quot;, 第 23 行, 在 __new__ return MuZeroResidualNetwork( ^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\models.py&quot;, 第 487 行, 在 __init__ self.representation_network = torch.nn.DataParallel( ^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\Anaconda\Lib\site-packages\torch\nn\parallel\data_parallel.py&quot;，第 150 行，在 __init__ output_device = device_ids[0] ~~~~~~~~~~^^^ IndexError: 列表索引超出范围  谢谢。    提交人    /u/Sufficient-Fly-4040   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dom7kr/how_to_resolve_errors_when_running_muzero_general/</guid>
      <pubDate>Wed, 26 Jun 2024 01:10:24 GMT</pubDate>
    </item>
    <item>
      <title>muzero 如何构建他们的 MCTS？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dogoda/how_does_muzero_build_their_mcts/</link>
      <description><![CDATA[在 Muzero 中，他们同时在各种不同的游戏环境（围棋、atari 等）上训练他们的网络。  在训练期间，MuZero 网络展开 K 个假设步骤，并与从 MCTS 参与者生成的轨迹中采样的序列对齐。通过从重放缓冲区中的任何游戏中采样一个状态来选择序列，然后从该状态展开 K 个步骤。  我无法理解 MCTS 树是如何构建的。每个游戏环境都有一棵树吗？ 是否假设每个环境的初始状态都是恒定的？（不知道这是否适用于所有 atari 游戏）    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dogoda/how_does_muzero_build_their_mcts/</guid>
      <pubDate>Tue, 25 Jun 2024 21:00:03 GMT</pubDate>
    </item>
    <item>
      <title>ValueError：需要解包的值太多（预期为 5 个）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dofdn3/valueerror_too_many_values_to_unpack_expected_5/</link>
      <description><![CDATA[不知道为什么会这样，我猜附加的东西不太好！！ 代码 -&gt; replay_buffer.py -&gt; https://www.pythonmorsels.com/p/2umqw/ train.py -&gt; https://www.pythonmorsels.com/p/2heh6/    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dofdn3/valueerror_too_many_values_to_unpack_expected_5/</guid>
      <pubDate>Tue, 25 Jun 2024 20:06:12 GMT</pubDate>
    </item>
    <item>
      <title>CNN 足以支持 Connect 4 自玩吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do7vbk/is_cnn_enough_for_connect_4_self_play/</link>
      <description><![CDATA[  由    /u/Professional_Card176  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do7vbk/is_cnn_enough_for_connect_4_self_play/</guid>
      <pubDate>Tue, 25 Jun 2024 14:50:24 GMT</pubDate>
    </item>
    <item>
      <title>RL 动作的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do7o32/problem_with_rl_actions/</link>
      <description><![CDATA[大家好，我有一个包含 24 个元素的目标数组，RL 将每个元素分开处理，并从一个函数（更像一个黑匣子）获取反馈，奖励是目标期望值与实际值（当然是负值）之间的差异。 所以我的问题是，有没有办法让模型知道当前正在处理哪个元素（索引）？ 我如何定义这个代理的状态？ 抱歉，我是 RL 新手，所以请原谅我的理解:) 注意：我在 python 上使用稳定基线 3，请随时询问更多信息，谢谢！    提交人    /u/Tiger-2001   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do7o32/problem_with_rl_actions/</guid>
      <pubDate>Tue, 25 Jun 2024 14:41:52 GMT</pubDate>
    </item>
    <item>
      <title>请帮助我理解常数α蒙特卡洛方法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do7cpp/please_help_me_understand_constantα_monte_carlo/</link>
      <description><![CDATA[      这是我迄今为止对用于近似价值函数的蒙特卡罗方法的理解： 蒙特卡罗方法不使用递归贝尔曼方程和环境动力学知识，而是使用统计数据来评估价值函数。对于给定的策略π和起始状态S_t，会产生多个事件。计算每个事件的回报并取平均值。如果样本数量足够大，则计算值会收敛到预期回报。  但根据常数 α 蒙特卡罗，价值函数使用以下更新规则进行评估： https://preview.redd.it/3ct62f2z9q8d1.png?width=452&amp;format=png&amp;auto=webp&amp;s=b450e965cf90765ea3de19f0b51258064f0702ac 其中 V(S_t) 是状态 S_t 的价值函数的当前估计，G_t 是时间步长 t 之后的回报。我不明白这与平均方法有何关联。     提交人    /u/Dead_as_Duck   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do7cpp/please_help_me_understand_constantα_monte_carlo/</guid>
      <pubDate>Tue, 25 Jun 2024 14:28:24 GMT</pubDate>
    </item>
    <item>
      <title>CFR 实际上是一种强化学习算法吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do6zdg/is_cfr_actually_a_reinforcement_learning_algorithm/</link>
      <description><![CDATA[尽管 CFR 与“传统”强化学习算法一样，依赖于从经验中学习并在顺序决策中找到最佳策略，但 CFR 并不依赖于价值函数或策略。那么 CFR 是不是 RL 算法？如果我的一些说法有误，请原谅我，在这个分类中有点迷失了方向    提交人    /u/Bubbly-Stranger-1175   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do6zdg/is_cfr_actually_a_reinforcement_learning_algorithm/</guid>
      <pubDate>Tue, 25 Jun 2024 14:12:11 GMT</pubDate>
    </item>
    <item>
      <title>理解和诊断深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</link>
      <description><![CDATA[发表于 ICML 2024 https://openreview.net/pdf?id=s9RKqT7jVM    由   提交  /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</guid>
      <pubDate>Tue, 25 Jun 2024 09:31:31 GMT</pubDate>
    </item>
    <item>
      <title>HPC 集群中超参数搜索和训练速度缓慢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1cim/slow_hyperparameter_search_and_training_in_hpc/</link>
      <description><![CDATA[大家好， 我目前正在使用 PPO 训练自主决策卫星星座，最多可进行 86400 个模拟步骤（每秒 1 步）。尽管使用了强大的硬件，包括 4 个 NVIDIA A100 GPU 和 11 个 CPU，但训练过程仍然非常缓慢。以下是我的设置和配置的摘要： **软件和库版本：** * **容器基础：**NVIDIA PyTorch 22.09 py3 * **库：** numpy==1.23.5 gymnasium==0.28.1 matplotlib==3.7.1 pandas==1.5.3 ray==2.10.0 ray[tune]==2.10.0 typer==0.7.0 dm_tree tree scikit-image lz4 gputil==1.4.0 pyarrow **环境配置：** * num_targets：10 * num_observers：10 * time_step：1 秒 * 持续时间：86400 秒 - 24 小时 **训练配置：** * batch_mode：“complete_episodes” * rollout_fragment_length：“auto” * num_rollout_workers：10 * num_envs_per_worker：1 * num_cpus_per_worker：1 * num_gpus_per_worker：0 * num_learner_workers：4 * num_cpus_per_learner_worker: 1 * num_gpus_per_learner_worker: 1 **搜索空间配置：** * fcnet_hiddens: [[64, 64], [128, 128], [256, 256], [64, 64, 64]] * num_sgd_iter: [10, 30, 50] * lr: [1e-5, 1e-3] * gamma: [0.9, 0.99] * lambda: [0.9, 1.0] * train_batch_size: [512, 1024, 2048, 4096] * sgd_minibatch_size：[32, 64, 128, 512] 训练过程非常缓慢，单次迭代 1 小时都无法完成（30 个样本的超参数搜索，20 次迭代将花费一个多月）。我期望使用 4 个 A100 GPU 进行更快的训练。以下是我尝试和观察到的几件事： * 大部分时间都花在训练上，而不是环境模拟上。使用 10 个推出的工作人员、每个工作人员 1 个环境和每个工作人员 1 个 CPU，大约需要 5 分钟来模拟环境。 * 将每个工作人员的 CPU 数量增加到 7 个（70 个 CPU）实际上减慢了该过程，需要 15 分钟左右才能完成模拟。 * 我可以在我的 MacBook Pro M2（大约 20 次迭代）以及 NVIDIA Jetson AGX Orin 上进行训练，但当然每次迭代大约需要 90 分钟，而且它们的批次大小更小并且参数经过调整。因此，我希望使用新硬件进行更快的训练。 * 我不完全理解以下示例中的这一部分：0.0/1.0 accelerater_type:A100 这里有一个例子： 试用状态：1 正在运行 | 3 PENDING 当前时间：2024-06-25 10:33:42。总运行时间：1小时1分6秒 逻辑资源使用情况：11.0/80 CPU、4.0/4 GPU（0.0/1.0 accelerater_type:A100） ╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮ │ 试验名称状态 gamma lr train_batch_size sgd_minibatch_size num_sgd_iter lambda model/fcnet_hiddens │ ═──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤ │ PPO_FSS_env-v0_17a5c_00000 正在运行 0.928133 1.38519e-05 2048 128 30 0.905579 [128, 128] │ │ PPO_FSS_env-v0_17a5c_00001 正在处理 0.972168 8.7419e-05 1024 128 10 0.957066 [256, 256] │ │ PPO_FSS_env-v0_17a5c_00002 待定 0.906353 2.12536e-05 2048 512 50 0.968513 [64, 64] │ │ PPO_FSS_env-v0_17a5c_00003 待定 0.948159 0.000202029 512 64 30 0.997584 [64, 64] │ ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ 如有任何关于进一步优化训练过程的建议或见解，我们将不胜感激！    提交人    /u/CLEMENMAN   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1cim/slow_hyperparameter_search_and_training_in_hpc/</guid>
      <pubDate>Tue, 25 Jun 2024 08:51:31 GMT</pubDate>
    </item>
    <item>
      <title>“探索大型语言模型中上下文学习的决策边界”，Zhao 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnud65/probing_the_decision_boundaries_of_incontext/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnud65/probing_the_decision_boundaries_of_incontext/</guid>
      <pubDate>Tue, 25 Jun 2024 01:41:05 GMT</pubDate>
    </item>
    <item>
      <title>“主题：人工智能反馈中的内在动机”，Klissarov 等人 2023 {FB}（Nethack 法学硕士的标签表明这是一种学习奖励）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntush/motif_intrinsic_motivation_from_artificial/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntush/motif_intrinsic_motivation_from_artificial/</guid>
      <pubDate>Tue, 25 Jun 2024 01:14:09 GMT</pubDate>
    </item>
    <item>
      <title>“神经语言代理的差异历史”，Piterbarg 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dntmy1/diff_history_for_neural_language_agents_piterbarg/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dntmy1/diff_history_for_neural_language_agents_piterbarg/</guid>
      <pubDate>Tue, 25 Jun 2024 01:02:47 GMT</pubDate>
    </item>
    <item>
      <title>强化学习库在超参数搜索和优化方面的现状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnromj/the_current_state_of_rl_libraries_in_terms_of/</link>
      <description><![CDATA[目前有许多库和框架为 RL 提供基线和即插即用算法。  由于众所周知 RL 对超参数非常敏感，我想问 RL 社区，在您使用的库和框架中，它们在系统超参数搜索和优化方面的经验排名如何？哪一个为其提供了更好的界面和/或效率更高？或者即使您使用过任何第三方库。     提交人    /u/Human_Professional94   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnromj/the_current_state_of_rl_libraries_in_terms_of/</guid>
      <pubDate>Mon, 24 Jun 2024 23:28:15 GMT</pubDate>
    </item>
    <item>
      <title>这难道不是《IMPLEMENTATION MATTERS IN DEEP POLICY GRADIENTS: A CASE STUDY ON PPO AND TRPO》这篇论文中的问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</link>
      <description><![CDATA[      我正在阅读这篇论文：“深度强化学习中的实施问题：PPO 和 TRPO 案例研究” [pdf 链接]。 我认为我对这篇论文的信息有疑问。看看这个表格： https://preview.redd.it/uaw20jf6fk8d1.png?width=1056&amp;format=png&amp;auto=webp&amp;s=e3c698529ec45dc4ad71b807f587572db2988dba 根据这个表格，作者认为 TRPO+ 即 TRPO 加上 PPO 的代码级优化优于 PPO。因此，这表明代码级优化比算法更重要。我的问题是，他们说他们对 TRPO+ 中打开和关闭代码级优化的所有可能组合进行网格搜索，而对于 PPO，则是将所有优化都打开。  我的问题是，通过进行网格搜索，他们给了 TRPO+ 更多的机会来获得一次良好的运行。我知道他们使用种子，但有 10 个种子。根据 Henderson 的说法，这还不够，因为即使我们做 10 个随机种子，将它们分组为两个 5 个种子并绘制奖励和标准差，我们也会得到完全分离的图，这表明方差太高，无法被 5 个种子或我猜甚至 10 个种子捕获。  因此，我不知道他们的论点在他们正在进行的网格搜索下如何成立。至少，他们也应该对 PPO 进行网格搜索。  我遗漏了什么？   由    /u/miladink  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnl78g/isnt_this_a_problem_in_the_implementation_matters/</guid>
      <pubDate>Mon, 24 Jun 2024 18:53:12 GMT</pubDate>
    </item>
    <item>
      <title>无模型 Stewart 平台</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</link>
      <description><![CDATA[        由    /u/FriendlyStandard5985   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dnkrvg/modelfree_stewart_platform/</guid>
      <pubDate>Mon, 24 Jun 2024 18:35:11 GMT</pubDate>
    </item>
    </channel>
</rss>