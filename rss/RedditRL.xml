<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 31 Aug 2024 06:20:31 GMT</lastBuildDate>
    <item>
      <title>Flappy Bird DQN 的损失曲线增加，但模型正在学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f59ayo/loss_curve_for_flappy_bird_dqn_increases_but_the/</link>
      <description><![CDATA[      我使用深度 Q 网络训练了一个 Flappy Bird 代理。第一个图中的得分大约等于小鸟穿过的管道数量（我使用的是第三方 openai gym，Flappy Bird Gymnasium），因此模型肯定学会了玩游戏，但损失一直在增加。我对强化学习还很陌生，但在我之前的机器学习经验中从未见过这种行为。根据您的经验，这意味着什么？此外，我意识到我应该训练更长时间，以便在训练期间充分利用模型。这是我的第一次运行，我计划继续运行保存的 DQN。 https://preview.redd.it/av03fnkbxvld1.png?width=1200&amp;format=png&amp;auto=webp&amp;s=0b3684c3dd56dee7c56a8af44e8deeed22fde027    提交人    /u/Goober329   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f59ayo/loss_curve_for_flappy_bird_dqn_increases_but_the/</guid>
      <pubDate>Fri, 30 Aug 2024 23:25:34 GMT</pubDate>
    </item>
    <item>
      <title>对于多智能体纸牌游戏我应该尝试什么算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f53rnj/what_algorithm_should_i_try_for_a_multiagent_card/</link>
      <description><![CDATA[我目前正在学习 RL 和 AI，并在 PyTorch 中尝试了 DQN 和 Double DQN。 我想尝试实现一个多智能体纸牌游戏，我想听听你们的意见，了解最好的入门方法。由于这是一个不完全信息游戏，我认为更简单的方法不适用于此。 我为此建立了一个概念证明，其中 4 个 Double DQN 智能体相互对抗，每个模型都会从环境中分配一个玩家，然后进行游戏。作为状态，我给了它表中所有类型卡片的数量（13 个值）、手中所有类型卡片的数量（13 个值）以及有关游戏的更多信息。 不幸的是，正如预期的那样，它无法学到太多东西，所以我正在寻找其他选择。 这是游戏：https://bicyclecards.com/how-to-play/presidents 此外，我遇到的一个问题是模型在选择有效的动作时遇到了问题（它通常没有足够的所选类型的卡片，或者它与桌面上的卡片不匹配）。    提交人    /u/Neither_Butterfly_51   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f53rnj/what_algorithm_should_i_try_for_a_multiagent_card/</guid>
      <pubDate>Fri, 30 Aug 2024 19:25:06 GMT</pubDate>
    </item>
    <item>
      <title>强化学习调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4si41/reinforcement_learning_survey/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz/generalization-reinforcement-learning    由   提交  /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4si41/reinforcement_learning_survey/</guid>
      <pubDate>Fri, 30 Aug 2024 11:06:15 GMT</pubDate>
    </item>
    <item>
      <title>从自定义环境构建环境张量</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4rxvb/building_environment_tensor_from_custom_env/</link>
      <description><![CDATA[我正在构建一个自定义扑克环境来训练 deepQLearning 代理进行游戏。我目前使用的基因环境张量如下所示： 大小：1,14 [玩家卡牌、玩家卡牌、桌子卡牌、桌子卡牌、桌子卡牌、桌子卡牌、桌子卡牌，] 其中每张牌由 2 个变量 hand_value 和 suit 表示。 但是当前的代理很垃圾，因为我没有将足够的信息从环境传递给算法。就像他们目前只将玩家牌映射到最终手牌值的奖励输出（如果这有意义的话） 但是我在扑克环境中存储了太多信息，我想将这些信息传递到深度 Q 学习算法中： 其他玩家的数量每个玩家的资金每个玩家之前的操作： 但我不确定应该如何构建环境张量，以便它保存有关所有这些提到的变量的信息，同时还优先考虑“players_cards”（因为实际上这些将决定玩家是否应该加注/弃牌/可以采取的任何其他操作） 是否有人知道任何资源或技巧来构建这个环境张量以传递到前馈网络来执行深度 Q 学习    提交人    /u/Amazing_Track4881   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4rxvb/building_environment_tensor_from_custom_env/</guid>
      <pubDate>Fri, 30 Aug 2024 10:31:24 GMT</pubDate>
    </item>
    <item>
      <title>如何训练模型在基于网格的环境中导航到固定目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4q982/how_do_i_train_a_model_to_navigate_to_a_fixed/</link>
      <description><![CDATA[      我怎么也想不通，这个问题困扰了我好几个月。我最初以为让环境基于网格会简化训练，但我仍然在努力获得我想要的结果。我迫不及待地想完成这个项目，然后更直接地使用 PyTorch 或 Keras 等框架，而不必过多依赖 Gymnasium 或 Stable Baselines。 这是我的代码：https://codeshare.io/Q8A4VW。 当前的奖励函数很简单：  达到目标：+100（游戏结束） 撞到障碍物：-100（游戏结束） 每次移动：-2（以鼓励最佳寻路）  我已经尝试调整熵系数并修改奖励函数，但似乎没有任何效果。任何建议都将不胜感激！ PS：抱歉，如果这不是提问的正确地方，请告诉我。    提交人    /u/Z-A-F-A-R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4q982/how_do_i_train_a_model_to_navigate_to_a_fixed/</guid>
      <pubDate>Fri, 30 Aug 2024 08:36:05 GMT</pubDate>
    </item>
    <item>
      <title>如何训练模型在基于网格的环境中导航到固定目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6z4/how_do_i_train_a_model_to_navigate_to_a_fixed/</link>
      <description><![CDATA[      我怎么也想不通，这个问题困扰了我好几个月。我最初以为让环境基于网格会简化训练，但我仍然难以获得我想要的结果。我渴望完成这个项目，并更直接地使用 PyTorch 或 Keras 等框架，而不必过多依赖 Gymnasium 或 Stable Baselines。  这是我的代码：https://codeshare.io/Q8A4VW。 当前的奖励函数很简单：  达到目标：+100（游戏结束） 撞到障碍物：-100（游戏结束） 每次移动：-2（以鼓励最佳寻路）  我已经尝试调整熵系数并修改奖励函数，但似乎没有任何效果。任何建议都将不胜感激！ PS：抱歉，如果这不是提问的正确地方，请告诉我。    提交人    /u/Z-A-F-A-R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6z4/how_do_i_train_a_model_to_navigate_to_a_fixed/</guid>
      <pubDate>Fri, 30 Aug 2024 08:31:32 GMT</pubDate>
    </item>
    <item>
      <title>如何训练模型在基于网格的环境中导航到固定目标？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6jg/how_do_i_train_a_model_to_navigate_to_a_fixed/</link>
      <description><![CDATA[      我怎么也想不通，这个问题困扰了我好几个月。我最初以为让环境基于网格会简化训练，但我仍然在努力获得我想要的结果。我迫不及待地想完成这个项目，然后更直接地使用 PyTorch 或 Keras 等框架，而不必过多依赖 Gymnasium 或 Stable Baselines。 这是我的代码：https://codeshare.io/Q8A4VW。 当前的奖励函数很简单：  达到目标：+100（游戏结束） 撞到障碍物：-100（游戏结束） 每次移动：-2（以鼓励最佳寻路）  我已经尝试调整熵系数并修改奖励函数，但似乎没有任何效果。任何建议都将不胜感激！ PS：抱歉，如果这不是提问的正确地方，请告诉我。    提交人    /u/Z-A-F-A-R   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4q6jg/how_do_i_train_a_model_to_navigate_to_a_fixed/</guid>
      <pubDate>Fri, 30 Aug 2024 08:30:41 GMT</pubDate>
    </item>
    <item>
      <title>关于 RL 中的符号的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4p34u/questions_about_notation_in_rl/</link>
      <description><![CDATA[        提交人    /u/jthat92   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4p34u/questions_about_notation_in_rl/</guid>
      <pubDate>Fri, 30 Aug 2024 07:11:02 GMT</pubDate>
    </item>
    <item>
      <title>棋盘游戏的策略梯度方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4foyf/policy_gradient_methods_for_board_games/</link>
      <description><![CDATA[棋盘游戏和离散动作空间问题的所有方法似乎都是基于价值的。这有什么特别的原因吗？ 为什么我们不能用像 PPO 这样对大多数其他问题更稳健、更高效的基于策略的算法取代 AlphaZero？    提交人    /u/gepeto97   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4foyf/policy_gradient_methods_for_board_games/</guid>
      <pubDate>Thu, 29 Aug 2024 22:39:26 GMT</pubDate>
    </item>
    <item>
      <title>我正在寻找研究人员和人工智能开发团队成员参与用户研究，以支持我的研究</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f4d4vp/im_looking_for_researchers_and_members_of_ai/</link>
      <description><![CDATA[我们正在寻找年满 18 岁且在软件开发领域拥有 2 年以上经验的研究人员和 AI 开发团队成员，以参与一项匿名调查，支持我在缅因大学的研究。这可能需要 20-30 分钟，并将调查您对您所在行业未来发展 AI 系统所带来的挑战的看法。如果您想参与，请在继续调查之前阅读以下招聘页面。完成调查后，您可以参加抽奖，赢取价值 25 美元的亚马逊礼品卡。 https://docs.google.com/document/d/1Jsry_aQXIkz5ImF-Xq_QZtYRKX3YsY1_AJwVTSA9fsA/edit    提交人    /u/wildercb   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f4d4vp/im_looking_for_researchers_and_members_of_ai/</guid>
      <pubDate>Thu, 29 Aug 2024 20:49:20 GMT</pubDate>
    </item>
    <item>
      <title>用于 PPO 的 ResNet 迁移学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f47fuc/resnet_transfer_learning_for_ppo/</link>
      <description><![CDATA[我目前正在尝试在像素输入上训练 PPO，但是训练速度非常慢。与我提供代理环境信息时相比，对像素输入进行训练至少需要 10 倍的样本数量（可能接近 25 倍），并且反向传播所需的时间要长得多。  我曾考虑使用 Resnet18 作为特征提取器来加快速度，但该模型有很多 BatchNorm 层，我听说这对 RL 不利。如果我删除 BatchNorm 层，该模型是否仍然有效，或者是否存在专门针对 RL 的预训练模型？    提交人    /u/AUser213   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f47fuc/resnet_transfer_learning_for_ppo/</guid>
      <pubDate>Thu, 29 Aug 2024 16:55:47 GMT</pubDate>
    </item>
    <item>
      <title>CartPole v0 中的 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f425dn/dqn_in_cartpole_v0/</link>
      <description><![CDATA[我正在尝试实现一个具有重放缓冲区、epsilon 衰减、目标 q 网络定期更新的 DQN。我不知道问题是什么，但网络似乎根本没有进行训练。我尝试更改超参数。似乎仍然不起作用。 # %% from collections import deque import random import torch from torch import nn import torch.nn. functional as F import gymnasium as gym import matplotlib.pyplot as plt import numpy as np import imageio # %% env = gym.make(&#39;CartPole-v1&#39;, render_mode = &#39;rgb_array&#39;) # %% def save_frames_as_gif(frames, path=&#39;./&#39;, filename=&#39;gym_animationq.gif&#39;): &quot;&quot;&quot;将帧另存为 GIF。&quot;&quot;&quot; imageio.mimsave(路径 + 文件名, 帧, fps=30) # %% pos_space = np.linspace(-2.4, 2.4, 10) vel_space = np.linspace(-4, 4, 10) ang_space = np.linspace(-.2095, .2095, 10) ang_vel_space = np.linspace(-4, 4, 10) # %% x = np.linspace(-1, 1, 10) x[9] # %% def digitize(state): state_p = np.digitize(state[0], pos_space) state_v = np.digitize(state[1], vel_space) state_a = np.digitize(state[2], ang_space) state_av = np.digitize(state[3], ang_vel_space) return torch.FloatTensor([state_p, state_v, state_a, state_av]) # %% class DQN(nn.Module): def __init__(self, in_states, h1_nodes, out_actions): super().__init__() self.fc1 = nn.Linear(in_states, h1_nodes) self.out = nn.Linear(h1_nodes, out_actions) def forward(self, x): x = F.relu(self.fc1(x)) x = self.out(x) return x # 为 Experience Replay 定义内存 class ReplayMemory(): def __init__(self, maxlen): self.memory = deque([], maxlen=maxlen) def append(self, transition): self.memory.append(transition) def sample(self, sample_size): return random.sample(self.memory, sample_size) def __len__(self): 返回 len(self.memory) # %% discount_factor = 0.9 epsilon = 1 epsilon_decay_rate = 0.0001 frames = [] rewards_per_episode = [] i = 0 memory_size = 100000 replay_size = 128 network_sync_rate = 50 memory = ReplayMemory(memory_size) policy_dqn = DQN(in_states=4, h1_nodes=10, out_actions=2) target_dqn = DQN(in_states=4, h1_nodes=10, out_actions=2) target_dqn.load_state_dict(policy_dqn.state_dict()) optimizer = torch.optim.Adam(policy_dqn.parameters(), lr= 1e-4) loss_fn = nn.HuberLoss() # %% def optimize(mini_batch, policy_dqn, target_dqn): # print(&quot;optimizing&quot;) current_q_list = [] target_q_list = [] for state, action, new_state, reward, terminated in mini_batch: if terminated: target = torch.FloatTensor([reward]) else: with torch.no_grad(): target = torch.FloatTensor( reward + discount_factor * target_dqn(digitize(state)).max()) current_q = policy_dqn(digitize(state)) current_q_list.append(current_q) # 获取目标Q值集合 target_q = target_dqn(digitize(state)) # 将具体的action调整为刚刚计算出的目标 target_q[action] = target target_q_list.append(target_q) loss = loss_fn(torch.stack(current_q_list), torch.stack(target_q_list)) optimizer.zero_grad() loss.backward() optimizer.step() # %% while(True): i +=1 state = env.reset()[0] # 起始位置，起始速度始终为 0 deterministic = False rewards = 0 while (未终止且奖励 &lt; 10000): if np.random.uniform() &lt;= epsilon: action = env.action_space.sample() else: with torch.no_grad(): action = policy_dqn(digitize(state)).argmax().item() new_state, r_t, termined,_,_ = env.step(action) rewards += r_t memory.append((state, action, new_state, r_t, termined)) # print(memory.__len__()) if memory.__len__() &gt; replay_size：#打印（“hello”）minibatch = memory.sample（replay_size）优化（minibatch，policy_dqn，target_dqn）epsilon = max（epsilon - epsilon_decay_rate，0）#打印（epsilon，epsilon_decay_rate）如果（i + 1）％network_sync_rate == 0：target_dqn.load_state_dict（policy_dqn.state_dict（））state = new_staterewards_per_episode.append（rewards）mean_rewards = np.mean（rewards_per_episode [len（rewards_per_episode）-100：]）如果i％10 == 0：打印（f&#39;Episode：{i} {rewards} Epsilon：{epsilon：0.2f}平均奖励{mean_rewards：0.1f}&#39;）如果mean_rewards &gt; 50: break # %% env = gym.make(&#39;CartPole-v1&#39;, render_mode = &#39;rgb_array&#39;) import time frames = [] state = env.reset()[0] terminology = False while not deterd: action = policy_dqn(digitize(state)).argmax().item() new_state, reward, terminology, _, _ = env.step(action) frame = env.render() time.sleep(0.1) frames.append(frame) # 调整睡眠时间来控制渲染速度 state = new_state env.close() # %% save_frames_as_gif(frames) # %%     submitted by    /u/Caveman2k23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f425dn/dqn_in_cartpole_v0/</guid>
      <pubDate>Thu, 29 Aug 2024 13:16:18 GMT</pubDate>
    </item>
    <item>
      <title>机器人深度强化学习：现实世界的成功调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3twmr/deep_reinforcement_learning_for_robotics_a_survey/</link>
      <description><![CDATA[随着我们接近 DQN 诞生 10 周年的里程碑，现在正是反思深度强化学习在机器人技术领域发展历程的好时机。哪些现实世界的机器人挑战因深度强化学习而取得了最重大的进步？我们还有哪些工作要做？ 我们最近发布了一项调查，探讨了这些问题。本文对深度强化学习在各种机器人能力中在现实世界中的成功进行了分类，用新的问题定义和解决方法分类法分析了潜在趋势，并强调了机器人专家和强化学习研究人员面临的关键挑战以及未来工作的主要途径。 如果您感兴趣，可以在此处查看完整论文：arxiv 链接。这篇调查论文将发表在 2025 年《控制、机器人和自主系统年度评论》中。    提交人    /u/Best-Pension-8837   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3twmr/deep_reinforcement_learning_for_robotics_a_survey/</guid>
      <pubDate>Thu, 29 Aug 2024 04:33:16 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的模拟器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3l0uu/simulators_in_pytorch/</link>
      <description><![CDATA[最近，有在 JAX 中构建模拟器的趋势：jumanji、TORAX、pgx、minimax，全部根据 Anakin 架构 构建。 为什么不在 PyTorch 中？    提交人    /u/gepeto97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3l0uu/simulators_in_pytorch/</guid>
      <pubDate>Wed, 28 Aug 2024 21:30:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的低计算研究领域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1f3c848/low_compute_research_areas_in_rl/</link>
      <description><![CDATA[我现在是本科四年级，必须为我的论文选择一个研究主题。我之前上过 ML/DL/RL 的课程，所以我有基础知识。 问题是我在这里无法访问适当的 GPU 资源。（当然，云是存在的，但价格昂贵。）我们大学只有一个简单的消费级 GPU（RTX 3090）和一台始终供不应求的 HPC 服务器，我的笔记本电脑中有 GTX 1650Ti。 所以，我正在寻找 RL 中需要相对较少计算的研究领域。我对理论和实践主题都持开放态度，但理想情况下，我想研究一些可以在我可用的硬件上实现和测试的东西。 我研究过的一些领域是迁移学习、元 RL、安全 RL 和逆 RL。我认为我的硬件很难处理 MARL。 您可以推荐研究领域、应用领域，甚至是可能有趣的特定论文。 此外，任何关于如何最大限度提高我的硬件在 RL 实验中的效率的建议都将不胜感激。 谢谢！！    提交人    /u/Abominable_Liar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1f3c848/low_compute_research_areas_in_rl/</guid>
      <pubDate>Wed, 28 Aug 2024 14:52:53 GMT</pubDate>
    </item>
    </channel>
</rss>