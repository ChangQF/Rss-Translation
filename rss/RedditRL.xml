<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 02 Aug 2024 01:06:36 GMT</lastBuildDate>
    <item>
      <title>CORL 和 D4RL 分数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</link>
      <description><![CDATA[我提前为一个可能微不足道的问题道歉： 在评估离线强化学习算法时，我们应该关心哪些分数？它们是 D4RL 标准化分数吗？还有其他需要考虑的吗？ 提前致谢。    提交人    /u/Constant_Koala_7744   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehsc4j/corl_and_d4rl_scores/</guid>
      <pubDate>Thu, 01 Aug 2024 21:26:26 GMT</pubDate>
    </item>
    <item>
      <title>RL 模型内部的 RL 模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</link>
      <description><![CDATA[大家好。 我正在为纸牌游戏制作强化学习算法。我可以采取的可能动作取决于我手中的牌。如果我打出一张牌，我可以执行特定的操作。您可以假设这是一场 1v1 游戏。如果我的对手打出一张特定的牌，那么我必须决定给对手一张牌。 本质上，我希望有一个 RL 模型来决定打出哪张牌，然后另一个 RL 模型用于我必须给对手一张牌的特定情况。第一个 RL 模型将在每个回合激活，而第二个 RL 模型仅在我的对手打出迫使我给他一张自己的牌的特定牌时激活。 我想通过先与随机模型对战，然后再与自己对战来训练模型。 这可以在体育馆内完成吗？如果是这样，该怎么办？    提交人    /u/Practical-Resort7278   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ehfgj5/rl_model_inside_of_rl_model/</guid>
      <pubDate>Thu, 01 Aug 2024 12:30:12 GMT</pubDate>
    </item>
    <item>
      <title>真人秀 PS1 游戏 - 铁拳 3</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</link>
      <description><![CDATA[我是 RL 领域的新手。我尝试过使用 atari 游戏的复古模拟作为 ENV 的 openai gym，但我想下一步。 我想在 PS1 上为格斗游戏 Tekken 3 实现 RL 模型。 据我所知，我必须使用 PS1 模拟器实现自己的交互层才能正确获取状态，例如健康、位置等。 我该如何正确处理？  尝试对 PS1 游戏进行逆向工程，并找出具有必要值的内存地址（通过 python 获取）。 使用 OpenCV 进行相同的检测，但从帧中  当涉及到创建自定义环境时，您将如何处理？    提交人    /u/Comprehensive_Cod331   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh8cxg/rl_ps1_game_tekken3/</guid>
      <pubDate>Thu, 01 Aug 2024 04:58:31 GMT</pubDate>
    </item>
    <item>
      <title>既然离线 RL 与环境无关，为什么很多论文实现仍然基于 gym？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</link>
      <description><![CDATA[谢谢。    由   提交  /u/Desperate_List4312   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh378j/since_offline_rl_is_environmentindependent_why/</guid>
      <pubDate>Thu, 01 Aug 2024 00:33:33 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助为不同的游戏选择不同的 RL 算法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</link>
      <description><![CDATA[我是荷兰 6 VWO 的一名 16 岁学生，目前正在参与一个关于强化学习 (RL) 在各种电脑游戏中的应用的学校研究项目。我的主要研究问题是：电脑游戏的具体特征如何影响不同 RL 算法的有效性？ 子问题：  不同类型的电脑游戏有哪些具体特征？ 有哪些 RL 算法可用，它们的特点是什么？ 游戏特征如何影响 RL 算法的性能？  实践部分：我计划将不同的 RL 算法应用于各种游戏。我正在考虑的游戏是：  超级马里奥兄弟 贪吃蛇 国际象棋 赛车  算法标准：  具有显著差异的算法。 最好是新算法。  反馈问题：  考虑到这些游戏的独特特点，您能否推荐适合这些游戏的特定 RL 算法？ 您认为我选择的游戏适合研究不同 RL 算法的有效性吗？如果不适合，您会建议什么游戏？     提交人    /u/matmoet   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eh1oyj/need_help_choosing_different_rl_algorithms_for/</guid>
      <pubDate>Wed, 31 Jul 2024 23:23:28 GMT</pubDate>
    </item>
    <item>
      <title>“彩虹团队：开放式生成多样化对抗提示”，Samvelyan 等人 2024 {FB}（用于质量多样性搜索的 MAP-Elites）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1egb71a/rainbow_teaming_openended_generation_of_diverse/</guid>
      <pubDate>Wed, 31 Jul 2024 01:48:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么在 epsilon-greedy 算法的概率部分中包含贪婪动作？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</link>
      <description><![CDATA[      新手在这里...我正在阅读 Sutton 和 Barto 关于强化学习的常年书籍。在他们对 epsilon-greedy 策略的描述中，他们认为在给定状态下选择一个动作应该以较小的 epsilon 概率发生；其余时间则选择贪婪动作。我理解这一点，因为你想鼓励探索，这反过来又允许人们满足先决条件，即在无限的时间轴下，最终将为给定状态选择所有动作，以收敛到接近最优的策略。 但是，我不明白即使在随机操作时也允许代理选择贪婪动作的原因。书中给出的这个公式就是一个例子，其中选择贪婪的概率是 1 减去选择任何动作（甚至是贪婪的动作）的概率 epsilon 加上随机选择贪婪动作的概率： https://preview.redd.it/vz26lao3vqfd1.png?width=142&amp;format=png&amp;auto=webp&amp;s=e3c7c0f6d48f1f349f9260f4a4e8897f438e2b42 当然，代理选择贪婪动作的次数会比选择其他非贪婪动作的次数多得多。因此，当整个重点是防止代理以小概率 epsilon 利用此操作时，为什么代理会在随机阶段选择最佳操作？ 谢谢， 一位充满激情的 RL 学习者。    提交人    /u/Soft-Establishment96   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eg8x8f/why_include_the_greedy_action_in_the/</guid>
      <pubDate>Wed, 31 Jul 2024 00:02:51 GMT</pubDate>
    </item>
    <item>
      <title>“Auto Evol-Instruct：大型语言模型的自动指令进化”，Zeng 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efy7aj/auto_evolinstruct_automatic_instruction_evolving/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efy7aj/auto_evolinstruct_automatic_instruction_evolving/</guid>
      <pubDate>Tue, 30 Jul 2024 16:45:14 GMT</pubDate>
    </item>
    <item>
      <title>你在PPO算法中遇到过这个问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</link>
      <description><![CDATA[我正在创建一个用于任务路由的深度强化学习环境，使用基于策略的 DRL 方法 PPO。在我的环境中，想法是每次任务到达一个节点时，都会生成一个概率，最终找到目的地并结束游戏。因此，每次任务到达一个节点时，都会生成相邻节点的概率，形成到达终点的连续路径。但是，存在一个问题：为每个状态生成的概率正在收敛。理论上，它们应该不同。例如，在选择一个节点后，应该生成像 s1[0.2,0.5,0.3] 和 s2[0.4,0.1,0.5] 这样的概率，但目前，每个状态都有相同的概率，例如 s1[0.2,0.5,0.3] 和 s2[0.2,0.5,0.3]。 我怀疑我的奖励设置可能有问题。我给每一步都设置了与时间参数相关的负奖励，完成的奖励为10，遇到路由循环就得到-10的负奖励，难道我的设计有问题？这让我很纳闷。    submitted by    /u/VermicelliBrave1931   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1efn2bx/do_u_meet_this_issue_in_ppo_algorithm/</guid>
      <pubDate>Tue, 30 Jul 2024 07:11:58 GMT</pubDate>
    </item>
    <item>
      <title>“对受试者进行反馈的序贯实验分析”，Diaconis & Graham 1981</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef9zdf/the_analysis_of_sequential_experiments_with/</guid>
      <pubDate>Mon, 29 Jul 2024 20:30:26 GMT</pubDate>
    </item>
    <item>
      <title>平均情节奖励差异，为什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</link>
      <description><![CDATA[      嗨， 我有一个简单的环境，我使用 SB3 中的各种算法进行训练作为练习。这是 DDPG 和 SAC 的 tensorboard 情节平均奖励（针对完全相同的环境）。在学习完成并保存模型时，报告的奖励约为 6。但是，当我在保存的模型上使用 SB3 assesse_policy (with n_eval_episodes=10) 时，我看到以下内容： 对于 SAC：平均奖励：1.5123082560196053 +/- 0.0008870645563937467 对于 DDPG：平均奖励：0.6197831666923037 +/- 0.000696591922367452 我预计平均奖励约为 6。这种预期是错误的吗？ https://preview.redd.it/xxag55fswgfd1.png?width=765&amp;format=png&amp;auto=webp&amp;s=e6903abf29863ad230d84451e83b162e265eb101    submitted by    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ef1183/mean_episode_reward_difference_why/</guid>
      <pubDate>Mon, 29 Jul 2024 14:32:31 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线-3 工人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</link>
      <description><![CDATA[我正在研究 stable-baselines3 库 的一些算法的实现。 具体来说，我看到一些算法（A2C、PPO）的文档提到该实现使用了多个工作器。  对于其他算法，它没有提及任何内容，所以我的问题是：  我可以假设其他算法只使用一个学习者吗？ 为了进一步解答我的疑问，在所有算法的摘要表中显示，每个算法都支持“多处理”。但是，如果我理解正确的话，那仅指使用矢量化环境，尽管我不会打赌它。 最后说明：我尝试通过查看A2C（其中提到使用工作者）和SAC（其中没有提到工作者）的代码来回答我的问题，但这没有帮助，所以我在这里   由    /u/Frank-the-hank  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eethy9/stablebaselines3_workers/</guid>
      <pubDate>Mon, 29 Jul 2024 07:27:37 GMT</pubDate>
    </item>
    <item>
      <title>创建博弈论论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</link>
      <description><![CDATA[在我国，对于原创博弈，制定博弈论与策略优化实施大纲的传统方式是什么？    提交人    /u/Former_Ad_4221   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eeskau/creating_a_game_theory_paper/</guid>
      <pubDate>Mon, 29 Jul 2024 06:24:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 PPO 算法没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</guid>
      <pubDate>Mon, 29 Jul 2024 05:00:03 GMT</pubDate>
    </item>
    <item>
      <title>用于构建 RL 项目的简单可视化工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</link>
      <description><![CDATA[      我计划制作这个用于 RL 开发的简单工具。这个想法是快速构建和训练 RL 代理，无需代码。这对于快速开始新项目或轻松进行实验以调试 RL 代理非常有用。 目前设计中有 3 个选项卡：环境、网络和代理。我计划添加第四个选项卡，称为“实验”，用户可以在其中定义超参数实验并直观地查看每个实验的结果，以便调整代理。这个设计是一个非常早期的原型，可能会随着时间的推移而改变。 你们觉得怎么样？ https://preview.redd.it/sb5awqjys8fd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d1046c3b7e195dba0b7779ee55f11c9330ec3d12    提交人    /u/Charming-Quiet-2617   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</guid>
      <pubDate>Sun, 28 Jul 2024 11:12:20 GMT</pubDate>
    </item>
    </channel>
</rss>