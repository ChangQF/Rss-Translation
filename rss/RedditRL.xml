<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 02 Dec 2024 21:16:40 GMT</lastBuildDate>
    <item>
      <title>帮助证明修改后的策略迭代在有限时间内收敛到最优策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4tkra/help_with_proof_of_modified_policy_iteration/</link>
      <description><![CDATA[大家好， 我正在学习&quot;人工智能：规划和决策&quot;课程，并且一直在研究有关修改后的策略迭代 (MPI)的问题。原始问题不是英文的，因此这是我能做的最好的翻译（如有不准确之处，敬请原谅）： 问题： &quot;写一个证明，证明存在 δ &gt; 0，对于该值，修改后的策略迭代算法在有限时间内收敛到最优策略，其中（折扣因子）γ&lt;1，并且在修改后的策略评估步骤的每次迭代中，对于当前策略 π 评估步骤中的最后一个值函数 vπ(i+1)： ∣vπ(i)−vπ(i+1)∣∞&lt;δ 在证明中，您可以在修改后的策略评估步骤中指定准确度 δ，并假设对于每个 ϵ，对于已知且可计算的函数 f，存在 δ=f(ϵ)，使得： 如果 ∣vπ(i)−vπ(i+1)∣∞&lt;δ 则∣vπ(i)−vπ∣∞&lt;ϵ（第 i 次迭代的值与第 i+1 次迭代的值之间的最大差异小于 δ，则第 i 次迭代的值与真实值之间的差异小于 ϵ。）” 我想确认我是否正确理解了这一点： 在修改后的策略迭代中，算法在策略评估和策略改进之间交替。目标是确保算法在有限步之后收敛到最优策略。挑战在于确定策略评估步骤中 δ 的正确值，一旦值函数的连续近似值之间的差异小于 δ，我们就会停止迭代。 我们知道 MDP 中的策略数量是有限的，因此需要有限次 MPI 迭代才能找到最优策略。我们需要做的就是证明，对于合适的 δ 选择，策略评估将在有限步后终止，并且错误将足够小，以便策略改进能够找到改进的策略（如果有的话）。 我遗漏了什么吗？ 如果您能就此证明中选择或表达 δ 提供任何指导，或者提供任何相关提示，我将不胜感激！ 非常感谢您的帮助！    提交人    /u/AnnieLunaMoore   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4tkra/help_with_proof_of_modified_policy_iteration/</guid>
      <pubDate>Mon, 02 Dec 2024 12:12:10 GMT</pubDate>
    </item>
    <item>
      <title>强化学习（离线或逆向）训练 VRP 的过去历史路线（纬度、经度）？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4rqjy/reinforcement_learning_offline_or_inverse/</link>
      <description><![CDATA[使用历史路线的车辆路径问题 (VRP) 离线强化学习 背景：我有一年以上的历史数据，包括：  使用 Google OR-Tools 生成的计划路线。 实际行驶路径（车辆实际行驶的路线的真实数据）。  我想使用这些历史数据训练一个模型，以便在训练后，它可以根据更新的权重和位置，预测第二天新订单和新位置的优化路线。 目标：我旨在应用离线强化学习 (RL)（或可能是逆 RL）来使用计划路线和实际路线的历史数据来训练模型。目标是修改我目前的方法，即实时生成动态路线，转而利用基于过去经验（历史数据）的离线学习。这样，模型可以从车辆遵循的计划路线和实际路径中学习，而不是仅仅依赖于实时生成的数据。 问题：  VRP 的离线 RL：我发现大多数研究都集中在 VRP 的在线 RL（https://github.com/OptMLGroup/VRP-RL）上，但我对离线 RL很感兴趣。是否有任何专门针对 VRP 的现有算法或方法支持使用历史数据进行离线训练？ 算法建议：鉴于 VRP 的离线 RL 仍是一个新兴领域，哪些 RL 算法或方法可能适合这个问题？例如，逆 RL 是否适用于这种情况下，我想从观察到的数据（实际路径）中学习奖励结构并使用它来优化未来的路线？ 数据集成：如何在训练过程中最好地整合计划路线（由 OR-Tools 生成）和实际路线（来自真实世界数据）？有哪些策略可以确保模型从两个来源有效地学习？ 实用建议：是否有人知道任何类似的项目或研究已经在 VRP 背景下解决了离线 RL，如果是，那么挑战和解决方案是什么？  机器学习专家可以回答问题吗？如何为 RL 实现我的历史数据？    提交人    /u/ComfortableVehicle83   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4rqjy/reinforcement_learning_offline_or_inverse/</guid>
      <pubDate>Mon, 02 Dec 2024 10:08:03 GMT</pubDate>
    </item>
    <item>
      <title>我从 DQN 获得的操作问题有时超出范围。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4m96d/problem_with_action_that_i_get_from_dqn_is/</link>
      <description><![CDATA[大家好， 我是强化学习的初学者。目前正在研究 DQN，并遇到了 DQN 预测动作的问题。我的问题如下：  Atari 游戏有 16 个一般动作空间，但每个游戏都有来自这个一般空间的自己的动作子集。 假设我正在探索 5 个不同的游戏，每个游戏都有不同数量的有效动作。 在进化过程中，一个游戏只有 3 个有效动作，但您的 DQN 返回的是这个有效空间之外的动作。  一种解决方案是用动作 0（NOOP）替换现在有效的动作，这意味着什么也不做。 还有其他方法可以有效地处理这种情况吗？ 提前谢谢您    提交人    /u/Grasmit_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4m96d/problem_with_action_that_i_get_from_dqn_is/</guid>
      <pubDate>Mon, 02 Dec 2024 04:00:39 GMT</pubDate>
    </item>
    <item>
      <title>为什么 DreamerV3 的炒作程度不如 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</link>
      <description><![CDATA[据我所知，PPO 通常是强化学习任务的首选算法。为什么不改用 DreamerV3？它似乎更稳定，并且需要更少的超参数调整。    提交人    /u/AUser213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4j81o/why_is_there_less_hype_around_dreamerv3_than_ppo/</guid>
      <pubDate>Mon, 02 Dec 2024 01:22:39 GMT</pubDate>
    </item>
    <item>
      <title>图注意力对于旋转变换是否不变？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4h1lj/do_graph_attention_is_invariant_to_rotational/</link>
      <description><![CDATA[我真的不确定我是否错了，但就等变和不变图而言，神经网络可以有效地处理这个问题，但旋转等变除外。 当我听到这个词时，我仍然认为 GAT 是否应该添加更多步骤以成为旋转变换不变，使输出 3D 特征不变天气位置不同或不在 3D 图上的 PCQM4Mv2 回归预测任务中。  我的管道： 节点特征 = 从数据集中提取的特征 边缘特征 = 3D 欧几里得     提交人    /u/Ill_Strawberry8459   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4h1lj/do_graph_attention_is_invariant_to_rotational/</guid>
      <pubDate>Sun, 01 Dec 2024 23:39:17 GMT</pubDate>
    </item>
    <item>
      <title>寻找合作者 - 强化学习运筹学问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4fwym/looking_for_collaborators_rl_for_operations/</link>
      <description><![CDATA[大家好 :) 我目前正在攻读硕士学位，一直专注于强化学习。我现在已经在这个领域完成了几个项目，并且我目前还在与一些合作者合作撰写一篇关于在不确定的随机环境中的强化学习的出版物，这些环境可以建模为图形。我在做本科论文时注意到了运筹学领域，我将一个随机组合问题构建为强化学习问题，并使用图神经网络解决了它。在那段时间以及从那时起，我阅读了一些最近的出版物，这些出版物试图用现代强化学习方法解决传统的 OR 问题（例如 https://arxiv.org/abs/2312.15658）。 我认为这总体上仍未得到充分探索，但同时也非常有趣。更现代的神经网络架构（例如 GNN）似乎非常适合与 RL 结合解决许多 OR 问题。因此，我也想专注于图形机器学习方法（例如 GNN），但我也对任何其他建模方法持开放态度。此外，还有一个 OR gym 存储库（https://github.com/hubbs5/or-gym），我想探索并用它来做一些新方法的实验。 因此，我正在寻找一些愿意加入我并共同努力用更现代的基于 RL 的方法解决其中一些问题的人。我还没有想过每周要花多少时间在这些项目上，因为从逻辑上讲，我还有很多事情要做（大学、工作、出版）。因此，如果有兴趣，我们可以联系并找到适合我们所有人的良好设置:)  我个人在设计、构建和流水线大规模神经网络方面拥有丰富的经验，并且非常乐意与来自不同背景的人合作。 喜欢收到您的来信！    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4fwym/looking_for_collaborators_rl_for_operations/</guid>
      <pubDate>Sun, 01 Dec 2024 22:48:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的顺序动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</link>
      <description><![CDATA[我有一个代理，其操作必须遵循顺序：A、B、C。但是，环境不稳定或不可预测。代理根据当前情况采取行动，完成其序列（A、B、C）。完成序列后，代理将根据工作完成情况获得一些奖励。它等待环境并分析下一个情况，然后再决定并执行下一组操作。我们如何在这个场景中使用 RL？我们如何训练模型以具有适当的意识来采取行动。每个动作对于获得良好的动作都同样重要。  总之，环境是不可预测的，但我们必须找到一些隐藏的模式来采取这个动作序列。  提前谢谢您！    提交人    /u/laxuu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h4boz9/sequential_action_in_rl/</guid>
      <pubDate>Sun, 01 Dec 2024 19:47:34 GMT</pubDate>
    </item>
    <item>
      <title>“通过语言游戏实现无边界的苏格拉底式学习”，Schaul 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h48l24/boundless_socratic_learning_with_language_games/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h48l24/boundless_socratic_learning_with_language_games/</guid>
      <pubDate>Sun, 01 Dec 2024 17:36:34 GMT</pubDate>
    </item>
    <item>
      <title>在离策略 PPO 中训练 Ant 时超参数是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h45ovi/what_are_the_hyperparameters_when_training_ant_in/</link>
      <description><![CDATA[我是强化学习的新手。使用 torchrl 作为库，我已按照以下教程创建了 PPO 代码，并确认 InvertedDoublePendulum 可以进行离线策略训练。 接下来我想尝试使用 Ant-v4 进行学习，因此我将环境名称更改为 Ant-v4 并开始学习。它似乎学得不太好，所以我将 `frame_skip` 设置为 5，`frames_per_batch` 设置为 `50_000 // frame_skip`，`total_frames` 设置为 `60_000_000 // frame_skip`，`sub_batch_size` 设置为 2500 来增加训练量（其他超参数与上一个教程中的相同）。 然而，Ant 的奖励来来去去，水平很低，并没有达到预期效果，如下面的视频所示。鉴于增加学习量不起作用，我认为这是超参数设置的问题，但我没有从谷歌获得任何好的见解。 我的代码中缺少什么？ https://reddit.com/link/1h45ovi/video/ewie7pmk994e1/player    提交人    /u/Novel-Resolve-1424   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h45ovi/what_are_the_hyperparameters_when_training_ant_in/</guid>
      <pubDate>Sun, 01 Dec 2024 15:28:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的 Q_Learning 算法不能正常学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/</link>
      <description><![CDATA[嗨，我目前正在编写一个 AI，该 AI 应该使用 Q-Learning 学习井字游戏。我的问题是，该模型在开始时学习了一点，但随后变得越来越糟，并没有变得更好。我正在使用  old_qvalue + self.alpha * (reward + self.gamma * max_qvalue_nextstate - old_qvalue) 更新 QValues，其中 alpha 为 0.3，gamma 为 0.9。我还使用 Epsilon Greedy 策略和衰减的 Epsilon，从 0.9 开始，每回合减少 0.0005，在 0.1 时停止减少。对手是一个 Minimax 算法。我没有发现代码中的任何缺陷，Chat GPT 也没有，我想知道我做错了什么。如果有人有任何提示，我将不胜感激。不幸的是，代码是德文的，我现在没有设置 Github 帐户。    提交人    /u/_waterstar_   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h3eq6h/why_is_my_q_learning_algorithm_not_learning/</guid>
      <pubDate>Sat, 30 Nov 2024 15:19:12 GMT</pubDate>
    </item>
    <item>
      <title>SAC（Soft Actor Critc）无法解决某些任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h3bey8/sac_soft_actor_critc_cannot_solve_some_tasks/</link>
      <description><![CDATA[我编写了一个软演员评论家算法，我想稍后将其用于 carla 模拟器的自动驾驶。我的代码管理器可以解决简单的任务，但是当我在 carla 上尝试它时，即使我的奖励基于硕士论文，我也会得到糟糕的表现。硕士论文获得了更好的表现。如果有人可以检查我的代码是否存在数学或编程错误，那就太好了。你可以在 github 上找到我的代码：https://github.com/b-gtr/Soft-Actor-Critic    提交人    /u/Fair_Device_4961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h3bey8/sac_soft_actor_critc_cannot_solve_some_tasks/</guid>
      <pubDate>Sat, 30 Nov 2024 12:19:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么当我训练 PPO 时，标准差会增加？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h39upr/why_is_std_increasing_when_i_train_ppo/</link>
      <description><![CDATA[我正在演员-评论家设置中，对具有连续动作（高斯分布）的简单任务测试 PPO。演员网络正在快速学习平均值的最优值，但标准差不断增加（最优解是确定性的，标准差越高，回报越差）。发生这种情况的原因可能是什么？我没有使用任何奖励进行探索。标准差与状态无关。    提交人    /u/Ok_Amoeba_9527   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h39upr/why_is_std_increasing_when_i_train_ppo/</guid>
      <pubDate>Sat, 30 Nov 2024 10:29:01 GMT</pubDate>
    </item>
    <item>
      <title>我无法让这个 dqn 在网格世界中收敛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h374em/i_cannot_get_this_dqn_to_converge_on_grid_world/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h374em/i_cannot_get_this_dqn_to_converge_on_grid_world/</guid>
      <pubDate>Sat, 30 Nov 2024 07:09:35 GMT</pubDate>
    </item>
    <item>
      <title>确定性策略的预期收益公式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ud32/expected_return_formula_of_deterministic_policy/</link>
      <description><![CDATA[      我有一个关于确定性策略的预期回报如何写出的问题。我发现在某些情况下使用 Q 函数，如表达式 5 所示。但是，我不完全理解与随机策略相反地获得它的步骤。获得表达式 5 的步骤或理由是什么？ https://preview.redd.it/q6ykgkjzbw3e1.png?width=711&amp;format=png&amp;auto=webp&amp;s=b4ad5f9bbd75430a83d6b240395f52431fed3486    提交人    /u/Street-Vegetable-117   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ud32/expected_return_formula_of_deterministic_policy/</guid>
      <pubDate>Fri, 29 Nov 2024 19:58:19 GMT</pubDate>
    </item>
    <item>
      <title>如何知道 SAC 方法是否过度拟合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</link>
      <description><![CDATA[      https://preview.redd.it/r1tizdsxbt3e1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=0ba0880704571a2868c12a02d3780bb3244d34aa 我是强化学习的初学者，正在使用软演员-评论家（SAC）方法对电动汽车的智能充电进行优化。目标是优化多个电动汽车在离散时间段内的充电计划，以最大限度地降低成本，同时满足电池和电网约束。我已经实现了一个带有优先采样的重放缓冲区，并添加了优先级衰减和动态采样等技术来增强训练稳定性并解决潜在的过度拟合问题。但是，我不确定是否发生了过度拟合，以及如何根据训练和评估奖励之间的差距确定合适的停止标准。我希望得到有关改进模型学习和确保更好的泛化的指导。    提交人    /u/Ok_Efficiency_1318   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h2ilfr/how_to_know_if_sac_method_is_overfitting/</guid>
      <pubDate>Fri, 29 Nov 2024 10:02:52 GMT</pubDate>
    </item>
    </channel>
</rss>