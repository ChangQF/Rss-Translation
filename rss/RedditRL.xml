<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 10 Mar 2025 12:35:35 GMT</lastBuildDate>
    <item>
      <title>让SAC在大型并行模拟器上工作（第一部分）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</link>
      <description><![CDATA[&quot;As researchers, we tend to publish only positive results, but I think a lot of valuable insights are lost in our unpublished failures.&quot; This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with并行模拟了数千个机器人。如果您遵循旅程，您将了解可能对性能产生重大影响的任务设计和算法实现的细节。 扰流板警报：链接： https://araffin.github.io/post/post/sac-massive-sim/   提交由＆＃32; /u/u/araffin2     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</guid>
      <pubDate>Mon, 10 Mar 2025 08:27:24 GMT</pubDate>
    </item>
    <item>
      <title>复制DeepSeek-R1 RL所需的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi rl社区，我想继续复制DeepSeek R1的RL训练管道中的小数据集。我对培训语言模型感到满意，但对培训RL代理不满意。我对深度RL的经典RL和中等理论理解有体面的理论理解。  我认为我需要逐步加强困难，以训练推理语言模型。因此，最近，我开始培训PPO实施方法来解决一些更轻松的健身环境，这确实在努力... 1周，我仍然无法再现低保真性，尽管基本上抬起了稳定的 - 贝赛3。&gt; 我想了解我的最终目标是否正确。一方面，如果我不能RL训练简单的代理商，我将如何使用RL训练语言模型。另一方面，我与我的朋友进行了限制RL经验的交谈，他提到，由于RL培训语言模型的代码已经在那里，而且挑战是正确的...    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/u/complect-media-8074      [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j7t5j4/advice_needed_needed_reproducing_deepseekr1_rl/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</guid>
      <pubDate>Mon, 10 Mar 2025 07:25:05 GMT</pubDate>
    </item>
    <item>
      <title>VINTIX：通过文化强化学习的动作模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我们刚刚发布了我们在离线范围内的初步努力（诸如Laskin等人，2022年的算法蒸馏）时，将其放到离线范围内的范围。虽然我们正在从经典的元元素意义上寻求概括，但初步结果令人鼓舞，表现出对参数变化的适度概括，而仅在总共87个任务下接受了87个任务。 我们的关键要点在此工作： （1）数据策划ICLR的数据curation for Iclr hard tweak at tweaking是一定的。希望所述的数据收集方法将有所帮助。而且我们还发布了数据集（约200mln元组）。 （2）即使在不同的数据集下，也可能对适度参数变化的概括。这是令人鼓舞的。但是，即使在类似jat的建筑中，也不是那么可怕（但很近）。   nb：随着我们进一步努力扩展并使状态和动作空间不变 - 也许您有一些有趣的环境/域名/元元学习基准，您希望在即将到来的工作中看到吗？ href =“ https://github.com/dunnolab/vintix”&gt; https://github.com/dunnolab/vintix    如果您传播词：https://x.com/vladkurenkov/status/1898823752995033299   提交由＆＃32; /u/u/vkurenkov     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</guid>
      <pubDate>Sun, 09 Mar 2025 21:03:08 GMT</pubDate>
    </item>
    <item>
      <title>关于多目标增强学习的环境的概括</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</link>
      <description><![CDATA[      现实世界中的顺序决策任务通常涉及平衡相互矛盾的目标之间的权衡并在各种环境中概括。尽管它很重要，但尚未有一项工作在本文中研究多目标环境中的环境概括！ 我们在多目标增强学习（MORL）中正式化概括以及如何评估。我们还介绍了 Morl Generalization 基准测试，具有各种多样性域具有带有参数化的环境配置，以促进该领域的研究。 我们对当前最新的最新莫尔尔算法的基线评估2关键洞察力：  li li a li a li a li a li a li a li a li a li algorith 与单目标增强学习相比，莫尔（Morl）表现出更大的学习适应性行为的潜力。事后看来，这是可以预期的，因为多目标奖励结构更具表现力，并允许学习更多的行为！ 😲  我们坚信，在未来几年中，开发能够跨多种环境和目标概括的代理将成为一个至关重要的研究方向。有许多有希望的途径用于进一步的探索和研究，尤其是在单一目标RL概括研究中的适应技术和见解中，以解决这个更严重的问题设置！我期待着与有兴趣推进这一新的研究领域的任何人交往！ 🔗纸： https://arxiv.org/arxiv.org/abs/2503.00799999999999999       https://github.com/jaydenteoh         &lt;！ -  SC_ON-&gt;＆＃32;不同环境概括      &lt;！提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j7bdr3/on_generalization_carers_environments_in/”&gt; [link]   ＆＃32;   [注释]    ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</guid>
      <pubDate>Sun, 09 Mar 2025 16:31:56 GMT</pubDate>
    </item>
    <item>
      <title>Python和Unity的RL环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我想训练AI使用Python玩游戏，并以Unity（C＃）形象可视化游戏。目前，我需要在Python中创建环境，以学习实际游戏玩法。有没有办法创建我可以在Python和Unity中使用的环境？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tot-chance9372     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</guid>
      <pubDate>Sun, 09 Mar 2025 14:37:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的模型不能学会在连续的网格世界中玩游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好。由于我正在研究深度Q学习算法，因此我正在尝试从头开始实施它。我创建了一个在网格世界中玩的简单游戏，我的目标是开发一个玩此游戏的代理商。在我的游戏中，状态空间是连续的，但是动作空间是离散的。这就是为什么我认为DQN算法应该起作用的原因。我的游戏具有3种不同的角色类型：主角（代理），目标和球。目标是在不与球相撞的情况下到达目标，而球线性移动。我的动作值剩下，右上，向下和什么都不是，总共进行了5个离散操作。 我使用Pygame Rect在Python中编码了游戏，用于目标，角色和球。我奖励代理如下：   +5，与角色相撞  -5，与球相撞，以  +0.7   +0.7，以靠近目标（使用Manhattan距离）   -1远离目标（使用Manhattan距离移动）（使用Manhattan距离远处）。我尝试了不同的状态表示形式，但是在最好的情况下，我的经纪人只学会避免球并达到目标。在大多数情况下，代理根本不会避免球，或者有时它会连续向左和向右进入摇摆的运动，而不是达到目标。&lt; /p&gt; 我给出了状态表示，如下所示：&lt; /p&gt;  agent.rect.rect.lect.lect.lect.lect.lect.rect.rect.rect.rect.rect.right.rect.right， agent.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.lect--rect-rect--rect- exent， target.rect.bottom， agent.rect.bottom--  Agent.Rect.bottom-  ball.Rect.top ， ball_direction_in_x，ball_dircection_in_in_in_in_in_iin_y_y_y  partive（part）这描述了对经纪人的比赛状态，提供了球和目标的相对位置以及球的方向。但是，我的模型的表现令人惊讶地差。相反，我将状态分类如下：  如果目标在左边，则为-1。 如果目标在右边为+1。游戏中很少或没有球），模型的性能大大提高。当我从游戏中删除球时，分类的状态代表学得很好。但是，当出现球时，即使表示形式连续，该模型也非常慢，最终它过度拟合。 我不想屏幕截图游戏屏幕并将其馈入CNN。我想使用密集的层将游戏的信息直接提供给模型，然后学习。为什么我的模型不学习？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j786hr/1j786hr/why_cant_my_my_my_model_model_lealed_to_to_play_in_continuul/”&gt; [link]   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</guid>
      <pubDate>Sun, 09 Mar 2025 13:58:09 GMT</pubDate>
    </item>
    <item>
      <title>机器人技术定制体育馆环境设计。包装纸还是阶级继承？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为水下机器人构建自定义环境。我已经尝试使用一个快速且脏的整体环境，但是如果我尝试修改环境以添加更多传感器，转换输出，重用代码，以完成另一个任务等，我现在会遇到问题。 ，我想重构代码并必须做出一些设计选择：我应该使用一个不适用的spass类并在每个任务中使用训练和训练的范围，或者我要训练劳动范围，或者我要训练劳动范围，或者我要求职。我只有一个基类，并将其他所有内容添加为包装器（包括传感器配置，任务奖励 +逻辑等）？ 如果您知道环境创建的良好资源，这将不胜感激）  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/equiald-diver     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</guid>
      <pubDate>Sun, 09 Mar 2025 09:18:42 GMT</pubDate>
    </item>
    <item>
      <title>Han等人“一般推理需要学习从一开始推理”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</guid>
      <pubDate>Sun, 09 Mar 2025 08:05:53 GMT</pubDate>
    </item>
    <item>
      <title>软动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6t9rx/soft_action_masking/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有一个想法“软动作遮罩”？对于那些对强化学习的原始数学贴心的人，我将提前道歉。我的想法还没有正式的数学。 让我以一个例子来说明我的想法。想象一个具有以下约束的环境：   - 代理商的可用动作之一是“什么都不做”。   - 每秒发送太多动作是一件坏事。但是，这里不知道具体数字。也许我们有一些数据，即每秒大约10个动作是最大的。有时13/秒是可以的，有时8/秒是不希望的。 防止代理在给定时间范围内采取太多操作的一种方法是使用动作掩蔽。如果最大的动作率是一个明确定义的数量，例如，在最后一秒钟内，代理已经采取了10个操作，则代理将被迫“无所事事”。通过动作面具。一旦最后一秒钟的动作数量降至10以下，我们将不再使用面具并让代理自由选择。 现在，现在考虑到我们的模糊要求，我们是否可以逐渐强迫我们的代理商选择“无所事事”。动作越来越接近极限？我故意不会在数学上正式描述这个想法，因为我认为这在很大程度上取决于您使用的算法类型。相反，我会尝试描述直觉。如上所述，在环境限制中，我们的速率限制每秒约为8-13个动作。如果代理商在最后一秒钟已经采取了10次措施，并且非常有信心它希望采取其他措施，也许我们应该允许它。但是，如果它在篱笆上，只有与什么无所作为相比，只有稍微倾向于采取其他动作，也许我们应该稍微推动它，以便它选择什么都不做。随着动作数量的增加，这个“ nuding”变得越来越强。一旦达到13，在此示例中，我们本质上使用了上述典型的动作掩盖方法，并迫使代理什么都不做，无论其偏好如何。 在策略梯度算法中，这种方法在我看来更有意义。我可以想象，仅将灰心的动作偏好乘以（0,1）中的值。传统的动作掩蔽可能会完全乘以0。我还没有考虑到基于价值的算法。 你们都在想什么？这似乎是有用的吗？我在一个自己的项目和大脑冲击解决方案中大致遇到了这个问题。我可以实现的另一个解决方案是奖励函数，它不鼓励超过限制，但是直到代理商实际学习奖励功能的这一方面，它可能会大大超过限制，并且我需要在任何方面实施一些艰难的动作掩盖。另外，这样的奖励功能似乎很棘手，因为利率限制奖励可能与我实际想学习的奖励是正交的。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6t9rx/soft_action_masking/</guid>
      <pubDate>Sat, 08 Mar 2025 22:52:00 GMT</pubDate>
    </item>
    <item>
      <title>学习ISAAC SIM / ISAAC实验室的最快方法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6rkdf/fastest_way_to_learn_isaac_sim_isaac_lab/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 机电一体化工程师在这里具有ROS/凉亭的经验和表面水平Pybullet + Pybullet + Gymnasium体验。我正在训练RL代理在某个任务上进行训练，我需要进行一些域随机化，因此将其并行化将有很大的帮助。最快的最低工作示例最快的最快是什么。学习ISAAC SIM/ISAAC实验室框架的方法或来源用于模拟RL代理的培训？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1j6rkdf/fastest_way_way_to_to_lealed_isaac_sim_isaac_isaac_lab/&gt; [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6rkdf/fastest_way_to_learn_isaac_sim_isaac_lab/</guid>
      <pubDate>Sat, 08 Mar 2025 21:33:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么功能近似会导致折扣RL的问题，但不会平均奖励RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6mr4u/why_does_function_approximation_cause_issues_in/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在 强化学习简介（第10.3章）中，萨顿介绍了平均奖励设置，没有打折的地方，并且代理值延迟延迟延迟的重新延迟，以立即奖励。他提到功能近似可能会导致折扣设置的问题，这是使用平均奖励的原因之一。 我了解平均奖励设置如何工作，但我不完全了解为什么功能近似值在折扣下差异。有人可以解释问题吗？ 为什么在证据中，萨顿实际上表明，折现设置在数学上等同于未估计的设置（成比例为1/（1 -γ）1/（1  -  \ gamma）1/（1  -  \ gamma）1/（1-γ）1/（1-γ）），所以我不明确地构成折扣的问题。具有政策改进定理，它可以确保提高一个国家的价值会导致整体政策改善。但是据我所知，这个问题适用于持续和情节任务，所以我仍然不明白为什么平均奖励是一个更好的选择。 有人可以在这里阐明动机吗？  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/nigilt_hippo1724     [link]   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6mr4u/why_does_function_approximation_cause_issues_in/</guid>
      <pubDate>Sat, 08 Mar 2025 17:56:37 GMT</pubDate>
    </item>
    <item>
      <title>有关培训基于RL的发电机的建议，具有改变奖励功能的高维物理模拟的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6lwli/advice_on_training_a_rlbased_generator_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我对机器学习和增强学习相对较新，我将其用于另一个领域的研究。我正在培训MLP，以生成一组高维参数（〜500–1000），以运行与物理相关的模拟。目的是生成两者的参数集：   满足必要的条件（条件x）  - 这与特征值有关，并且需要模拟运行。     产生一个模拟结果，可以匹配实验性数据的范围，但如果可以满足的范围，则可以 挑战是模拟本身是非常昂贵的，因此我想避免在无效的参数上浪费计算，并且想法是，该生成器应该能够生成大量有效的参数集。 我目前的计划是： 我的计划：  ph st phl strong &lt;强&gt; &lt;强&gt;  第1阶段：训练发电机生成定期满足条件X的参数集（例如他所有生成的集合的80％）。    阶段2：一旦模型擅长满足条件x，就可以从模拟的结果中引入奖励信号，以提高与实验性数据的匹配。 我还没有发现有关切换奖励功能中期训练的文献 - 这是RL中的已知/标准方法吗？是否有支持这种类型的奖励优化的论文或框架？ 这种两阶段方法对我的情况是否合理？ 我目前正在使用 Evolution策略（ES）进行优化吗？我是否应该将优化技术从第1阶段切换到2阶段？ 我知道奖励功能的重要性吗？只需添加tp tp tp tp the阶段1奖励2阶段的模拟的奖励？ 从第1阶段i i i i i lir lir lig s of erse sot sod of erse sod able of erse a of erse a of erse a of erse a of e彼此在空间（仍然尊重x exep x），所以我可以对2 expe of seper of separe 2我求解。这是否仅通过在Pahse 1中给予探索的奖励（例如，如果它产生尊重的条件X彼此远离的情况，就可以给予奖励）？  预先感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sangalewata     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6lwli/advice_on_training_a_rlbased_generator_with/</guid>
      <pubDate>Sat, 08 Mar 2025 17:18:32 GMT</pubDate>
    </item>
    <item>
      <title>兼容的RL算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6euod/compatible_rl_algorythims/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在启动我在计算机科学方面的硕士论文。我的目标是训练ISAAC实验室中的四倍机器人，并比较不同算法如何学习和对环境变化的反应。我计划使用SKRL库，其中具有以下算法可用：   对抗运动priors （ amp  ）      cross-entropy方法（ cem  ）     深层确定性策略梯度（ ddpg ）    &lt;&gt;  double q-network （ ddqn ） href =“ https://skrl.readthedocs.io/en/latest/api/api/agents/dqn.html”&gt; deep q-network （ dqn ） href =“ https://skrl.readthedocs.io/en/latest/api/apen/agents/ppo.html”&gt;近端策略优化（ ppo  ）      q-learning （ q-learning ）））   强大的策略优化（ rpo  ）      soft actor-critic （ sac   ）     国家行动奖励国家行动（ sarsa  ）      twin-delayed ddpg （ td3 ） href =“ https://skrl.readthedocs.io/en/latest/api/apent/trpo.html”&gt;信任区域策略优化（ trpo ））                &#39;我是否可以在iSAAC中实现所有效果。我还试图找到哪种算法更有趣，因为我无法使用所有算法。我认为3-4是最佳选择。任何帮助都将不胜感激，我在这个领域很新。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/solodres123     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6euod/compatible_rl_algorythims/</guid>
      <pubDate>Sat, 08 Mar 2025 11:01:46 GMT</pubDate>
    </item>
    <item>
      <title>GRPO在体育馆</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6eezp/grpo_in_gymnasium/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在适应 grpo 算法（最初是为LLM提出的），以在体育馆中使用Mujoco使用Mujoco进行连续的加固学习问题。  g 相应的奖励以计算相对优势。 对于连续行动任务，我的解释是，在每个时间步长，我需要：  示例 g 与策略分布相比。  使用这些奖励来计算相对优势，因此，在各自的环境中的行动。在连续行动环境中更有效？任何提高效率的建议或建议将不胜感激！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/totokk55     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6eezp/grpo_in_gymnasium/</guid>
      <pubDate>Sat, 08 Mar 2025 10:29:17 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>