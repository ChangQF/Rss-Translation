<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 25 Dec 2024 01:15:20 GMT</lastBuildDate>
    <item>
      <title>PPO 算法中总损失是如何使用的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hllhps/how_is_total_loss_used_in_ppo_algorithm/</link>
      <description><![CDATA[在 PPO 中，有两种损失：策略损失和价值损失。价值损失用于优化价值函数，而策略损失用于优化策略函数。但策略和价值损失（带有系数参数）结合在总损失函数中。 总损失函数有什么作用？我理解每个网络都使用自己的损失进行优化。那么总损失优化了什么？ 或者我理解错了，两个网络都使用相同的总损失进行优化，而不是使用各自的损失？    提交人    /u/BitShifter1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hllhps/how_is_total_loss_used_in_ppo_algorithm/</guid>
      <pubDate>Tue, 24 Dec 2024 20:10:43 GMT</pubDate>
    </item>
    <item>
      <title>具有离线 RL 的 GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlh6v7/gnn_with_offline_rl/</link>
      <description><![CDATA[我想使用离线 RL，即不与环境交互，仅使用过去的数据，这些数据可以组织为经验 (s、a、s&#39;、r)。代理 - 使用 Pytorch Geometric 的 GNN。状态 - 我使用 Pytorch Geometric 的 HeteroData 类型，这是一个异构图。算法 - CQN（保守 Q 学习）。动作空间 - 离散。奖励 - 仅在每集结束时。 有谁知道哪个 RL 框架可以最轻松地进行定制，而不必深入研究？ 到目前为止，我知道有 rllib、torchRL、d3RL、cleanRL、stable baselines、tianshou 几年前我只使用过稳定基线，做我需要的定制需要付出很多努力。我希望这次能避免这种情况。也许最好从头开始写？     由    /u/Aggravating_Rip_1882  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlh6v7/gnn_with_offline_rl/</guid>
      <pubDate>Tue, 24 Dec 2024 16:37:40 GMT</pubDate>
    </item>
    <item>
      <title>预测图块地图的缺失部分。（我需要使用 RL 吗？）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlgx3v/predict_missing_parts_of_tile_map_do_i_need_to/</link>
      <description><![CDATA[      大家好，我正在尝试制作一个可以预测 11x11 图块地图缺失部分的代理。我有无数张这样的地图需要训练。每张地图都已完全完成，包含所有数据（无雾），但我希望它能够预测我移除的图块。每张图块都有地形、资源和改进。我是否需要对部分地图进行雾化并使用 RL，或者我可以使用某种不需要从环境中学习的数据预测模型？这是地图的图像。 https://preview.redd.it/lpj59bxhot8e1.png?width=706&amp;format=png&amp;auto=webp&amp;s=fec19e3e9f5ab70c1e5d8ef11206d99c4d06e2b5    提交人    /u/Kingofath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlgx3v/predict_missing_parts_of_tile_map_do_i_need_to/</guid>
      <pubDate>Tue, 24 Dec 2024 16:24:02 GMT</pubDate>
    </item>
    <item>
      <title>如何实现“多智能体强化学习独立与合作智能体”？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlgpom/how_to_implement_multiagent_reinforcement/</link>
      <description><![CDATA[我具备强化学习和 Python 的基础知识。一位教授让我实现这篇论文。关于如何实现这一点以及从头开始学习哪些资源，您有什么建议吗？    提交人    /u/t_sia   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlgpom/how_to_implement_multiagent_reinforcement/</guid>
      <pubDate>Tue, 24 Dec 2024 16:13:20 GMT</pubDate>
    </item>
    <item>
      <title>如何在基于 VAPI 的语音 AI 系统中创建/添加 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlg2ue/how_to_createadd_rl_in_vapibased_voice_ai_system/</link>
      <description><![CDATA[我有一个基于 VAPI 的 AI 语音助手，用于我的咨询业务，目前已与 Twilio（电话）、Deepgram（语音识别）、GPT-4（语言理解）和 ElevenLabs（文本转语音）集成。此外，此语音助手集成到 GoHighLevel CRM 系统中以存储客户信息。我想通过两个关键功能增强 AI 语音助手：1. 强化学习 (RL)，从用户交互中学习并不断改进响应。  检索增强生成 (RAG) 以确保从知识库（例如常见问题解答、政策文档或矢量数据库）获得事实和有根据的答案。  您能否：•提供将 RL 添加到我现有的 VAPI AI 工作流程中的分步说明示例？ •推荐一个向量数据库（FAISS、Pinecone 等）并概述如何为 RAG 构建检索管道？ •分享处理来自 Twilio/Deepgram 的语音数据、收集用户反馈以及定期更新模型的最佳实践？ 我的目标是让语音助手在每次通话中都更加准确、更具适应性。谢谢！    提交人    /u/IntelligentOil2047   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlg2ue/how_to_createadd_rl_in_vapibased_voice_ai_system/</guid>
      <pubDate>Tue, 24 Dec 2024 15:42:01 GMT</pubDate>
    </item>
    <item>
      <title>玩转人工智能：探索 Pistonball 中集体行为的魔力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hle8rw/fun_with_ai_discovering_the_magic_of_collective/</link>
      <description><![CDATA[      在本文中，我将分享我在 Pistonball 游戏中进行的一项小实验的惊人发现。一个简单的调整导致了 AI 代理之间意想不到的合作，揭示了多代理系统中集体行为的迷人见解。想知道发生了什么吗？快来看看吧！    提交人    /u/RyanlovesAI   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hle8rw/fun_with_ai_discovering_the_magic_of_collective/</guid>
      <pubDate>Tue, 24 Dec 2024 14:07:09 GMT</pubDate>
    </item>
    <item>
      <title>“搜索和学习的扩展：从强化学习角度重现 o1 的路线图”，Zeng 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hlchn7/scaling_of_search_and_learning_a_roadmap_to/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hlchn7/scaling_of_search_and_learning_a_roadmap_to/</guid>
      <pubDate>Tue, 24 Dec 2024 12:23:44 GMT</pubDate>
    </item>
    <item>
      <title>我训练了一个强化学习代理来玩索尼克。希望得到一些反馈。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hl7pjf/i_trained_a_reinforcement_learning_agent_to_play/</link>
      <description><![CDATA[      我最近训练了一个 AI 来玩刺猬索尼克游戏。我写了一篇关于它的 LinkedIn 文章。但我最近一直在观看索尼克游戏，发现索尼克不仅仅是一款速度跑酷游戏。关卡中有许多很酷的隐藏角落和路径，如果玩家像玩马里奥游戏一样玩这些游戏，可能会错过。我很想收集一些关于你如何玩索尼克的反馈，以及你认为 AI 代理应该如何玩它。我现在只关注第一款游戏。你会针对不同的区域（绿山、大理石等）使用不同的策略吗？    提交人    /u/throwaway-bib   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hl7pjf/i_trained_a_reinforcement_learning_agent_to_play/</guid>
      <pubDate>Tue, 24 Dec 2024 06:34:32 GMT</pubDate>
    </item>
    <item>
      <title>“最大扩散强化学习”，Berrueta 等人 2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hl3au6/maximum_diffusion_reinforcement_learning_berrueta/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hl3au6/maximum_diffusion_reinforcement_learning_berrueta/</guid>
      <pubDate>Tue, 24 Dec 2024 02:05:50 GMT</pubDate>
    </item>
    <item>
      <title>评论家（在演员-评论家模型中）如何从动作和状态中学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hl2l9h/how_does_the_critic_in_an_actorcritic_model_learn/</link>
      <description><![CDATA[大家好， 我看到很多 Critic 的实现都涉及使用与 Actor 相同的架构。如何将动作和状态视为相同的信息？ 我也见过使用 CNN + RNN 的例子，将动作 + 前一层的输出作为输入，但没有关于它的论文。有没有人碰巧有一篇关于这种架构的论文可以让我读一读，或者只是讨论处理动作 + 状态输入？ 非常感谢    提交人    /u/Sea_Farmer5942   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hl2l9h/how_does_the_critic_in_an_actorcritic_model_learn/</guid>
      <pubDate>Tue, 24 Dec 2024 01:26:53 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习对 LLM 进行微调，以说服受害 LLM 选择错误的答案。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkvv2i/fine_tuning_an_llm_using_reinforcement_learning/</link>
      <description><![CDATA[我在这里写信是因为我需要帮助完成一个我不知道如何开始的大学项目。 我想这样做：  获取一个包含问题和多个答案的琐事数据集。需要知道正确答案。 对于每个问题，使用随机 LLM 生成一些中性上下文，提供有关该主题的一些信息，但不透露正确答案。 对于每个问题，选择一个错误的答案并指示本地 LLM 使用该上下文编写叙述，以说服受害者选择该答案。 将问题、上下文和叙述发送给受害者 LLM，并要求其仅根据我发送的内容选择一个选项。 如果受害者 LLM 选择了正确的选项，则不给予任何奖励。如果受害者选择了任何错误的选项，则向本地 LLM 提供一半的奖励。如果受害者选择了目标错误选项，则向本地 LLM 提供全额奖励  这应该会让我训练一个“欺骗者”LLM，试图说服其他 LLM 选择错误的答案。它可以撒谎并捏造事实和研究论文以说服受害者 LLM。 正如我所说，这是一个大学项目，但我从未做过任何与 LLM 或强化学习有关的事情。有人可以给我指明正确的方向并提供支持吗？我发现 huggingface 的 TRL 等库似乎很有用，但我以前从未使用过 pytorch 或类似的东西，所以我真的不知道如何开始。    提交人    /u/XLNBot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkvv2i/fine_tuning_an_llm_using_reinforcement_learning/</guid>
      <pubDate>Mon, 23 Dec 2024 19:53:16 GMT</pubDate>
    </item>
    <item>
      <title>“通过深度强化学习学习协作视觉对话代理”，Das 等人 2017 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hku11e/learning_cooperative_visual_dialog_agents_with/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hku11e/learning_cooperative_visual_dialog_agents_with/</guid>
      <pubDate>Mon, 23 Dec 2024 18:27:43 GMT</pubDate>
    </item>
    <item>
      <title>请推荐适合我的环境的模型选择</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hktor8/please_recommend_model_choices_for_my_environment/</link>
      <description><![CDATA[我正在为大学开展一个 RL 项目，我们应该训练一个在简单的 1v1 环境中打曲棍球的代理。观察是一个具有 18 个值的 1d 向量，而不是帧图像。动作空间由 4 个值组成，其中前 3 个是连续的（水平/垂直移动和旋转），最后一个也是连续的，但实际上只是一个阈值 0.5（持球或射门）。奖励是通过射门（游戏结束）给予的，但如果冰球正朝着你的一侧/球门移动，则在每一帧中靠近冰球也会给予奖励。我们已经实现了 sac，我想知道是否还有其他有前途的方法可以胜过它。我想实现一个梦想家类型的网络，但当观察空间如此之小的时候，这并不是很理想，对吧？    提交人    /u/dotaislife99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hktor8/please_recommend_model_choices_for_my_environment/</guid>
      <pubDate>Mon, 23 Dec 2024 18:11:40 GMT</pubDate>
    </item>
    <item>
      <title>我通过强化学习和训练构建了一个玩《黑暗之魂》的人工智能。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkk837/i_built_a_ai_to_play_dark_souls_through/</link>
      <description><![CDATA[你好， 我构建了一个可直接与 Dark Souls 交互并玩游戏的 AI。Dark Souls 没有 API，因此这是一个持续的复杂过程，需要反复尝试。 到目前为止，该过程已取得良好结果，尤其是对于在非常大且复杂的环境中盲目运行且奖励稀少的代理而言。 为了促进 AI，我设计了一个非常大且量身定制的奖励塑造框架，专门针对 Dark Souls 环境，模拟类似 API 的奖励结构以进行指导和发展。正如人们所说，罗马不是一天建成的，但它带来了数次飞跃的进步和突发行为。 我还设计了两个新系统，试图帮助指导代理并促进学习和进步。  第一个方法称为 Vivid，这个过程允许代理直接从视频输入中学习，比如它所在精确区域的专业演练。这种方法跳过了传统的图片和数据文件帧提取过程，而是从直接视频帧中学习，提高了映射到动作和奖励结构的效率和准确性。  第二个方法称为 TGRL（文本引导强化学习），它允许代理直接从基于文本的演练中学习，该演练以基于脚本的步骤解析信息，通过关键词检测和动作映射进行上下文排序，并与代理跟随和学习的奖励结构相关联。  到目前为止，它已经在代理和进展中产生了一些有趣的结果和行为变化。  有一次，它甚至在游戏中执行了一个我从未遇到过或知道可能做到的动作，也没有在其他任何地方见过。  我当前的挑战是指导。虽然目前的奖励结构运行良好，但代理仍然处于反复试验的环境中，没有像 API 那样明确的游戏进程统一性方向。  如果有人对如何让代理在游戏中“定向移动”（应该是这样）以减少随机性有任何建议，我很乐意接受帮助。  当前进度包括：  挑选第一个牢房钥匙 打开第一个牢房门 杀死前三个被动空洞 成功爬上第一个梯子  下一个预期进度：  在第一堆篝火旁点燃并休息 进入并导航第一个 Boss 竞技场  可以执行游戏中的所有操作。菜单导航、设备导航和升级机制尚未设计或实施。     由    /u/UndyingDemon 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkk837/i_built_a_ai_to_play_dark_souls_through/</guid>
      <pubDate>Mon, 23 Dec 2024 09:28:52 GMT</pubDate>
    </item>
    <item>
      <title>具身沟通游戏：强化学习代理的任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hkib91/the_embodied_communication_game_a_task_for/</link>
      <description><![CDATA[我一直在为我的数据科学硕士学位开展一个独立研究项目，我偶然发现了一个非常有趣的多智能体游戏，该游戏是在本文中设计和运行的（在人类身上）：https://thomscottphillips.wordpress.com/wp-content/uploads/2014/08/scott-phillips-et-al-2009-signalling-signalhood.pdf 这个游戏本质上是这样设计的：两个玩它的智能体必须协调他们各自对世界的观察所分配的信息。要想 100% 准确地做到这一点，唯一的办法就是根据玩家在游戏空间中的行为设计和使用通信系统。人类能够在这款游戏中进行交流，并实现 100% 的成功率（尽管并非每对人类都能做到这一点），而无需事先知道交流是否必要甚至可能。 我在 Gymnasium 用 PettingZoo 和 SuperSuit 的帮助设计了具身交流游戏 (ECG)，并最终在原始游戏和简化版本中训练了一些 Stable-Baselines3 RL 模型。我写了一篇论文，详细介绍了我所做的努力和取得的成果（毫不奇怪，对于模型之间紧急通信的结果持负面态度），虽然它还没有达到科学论文的水平，但我很自豪地写了这篇文章，并且愿意接受批评和评论，所以我想我会把它发布在这里。 我也很好奇你们中是否有人将这个问题（上面链接的论文中概述的具身通信游戏）作为多智能体 RL 问题来处理？这是一个非常有趣的问题，而且似乎很有可能用当前 RL 模型的某些版本来解决。 这是我关于 ECG 的论文的链接： https://evanmccormick37.github.io/independent-study-F24-learning-RL-with-gymnasium/    提交人    /u/EvanMcCormick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hkib91/the_embodied_communication_game_a_task_for/</guid>
      <pubDate>Mon, 23 Dec 2024 07:00:22 GMT</pubDate>
    </item>
    </channel>
</rss>