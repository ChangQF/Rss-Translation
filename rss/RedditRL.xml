<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 05 Mar 2024 00:56:24 GMT</lastBuildDate>
    <item>
      <title>更多关于体育馆环境的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6m208/more_questions_about_the_gymnasium_environment/</link>
      <description><![CDATA[这是构建自定义健身房环境的结构 (https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#make-your-own-custom-environment）。我正在寻找一些说明 -  gym-examples/ README.md setup.pygym_examples/ __init__.py envs/ __init__.py grid_world.pywrappers/ __init__.pyrelative_position.pyreacher_weighted_reward。 py离散_action.py Clip_reward.py  既然有`setup.py`，我们就可以导入`gym_examples`。我想了解导入过程后会发生什么 -   我相信当我们导入包时，最顶层的 __init__.py 就会被调用。该模块由以下代码组成 -  ​ fromgymnasium.envs.registration import register register( id=&quot;gym_examples/GridWorld -v0&quot;, entry_point=&quot;gym_examples.envs:GridWorldEnv&quot;, max_episode_steps=300, )   我不确定 `__init__.py` 是如何对应的子包`envs`被调用。 为什么register函数中有一个`entry_point`？我知道在 setup.py 中有这个，然后在命令行中调用entry_point。但作者在这里将其放置在函数中。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1b6m208/more_questions_about_the_gymnasium_environment/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6m208/more_questions_about_the_gymnasium_environment/</guid>
      <pubDate>Mon, 04 Mar 2024 21:00:57 GMT</pubDate>
    </item>
    <item>
      <title>关于机器人强化学习的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6ii26/question_regarding_reinforcement_learning_in/</link>
      <description><![CDATA[我是 FRC（第一届机器人竞赛）团队的一名高中生，正在研究在我们的机器人中使用强化学习。我在传统机器学习方面有一些经验，我们在 onshape 中有机器人的 CAD。我非常感谢有关机器人模拟等后续步骤的帮助... 编辑：顺便说一句，我们无法支付任何订阅费用。  &amp;# 32；由   提交/u/Cellini_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6ii26/question_regarding_reinforcement_learning_in/</guid>
      <pubDate>Mon, 04 Mar 2024 18:38:09 GMT</pubDate>
    </item>
    <item>
      <title>关于创建定制健身房环境的疑问</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6dh5x/doubts_regarding_creating_a_custom_gymnasium/</link>
      <description><![CDATA[ 为什么在开发自己的自定义健身房环境时需要创建包  我是阅读此处提供的文档 - https://gymnasium.farama.org/ Tutorials/gymnasium_basics/environment_creation/#make-your-own-custom-environment 它具有以下信息 - 最后一步是将我们的代码构建为Python 包。这涉及配置gym-examples/setup.py。如何执行此操作的最小示例如下： from setuptools import setup setup( name=&quot;gym_examples&quot;, version=&quot;0.0.1&quot;, install_requires=[&quot;gymnasium ==0.26.0&quot;, &quot;pygame==2.1.0&quot;], )  我想知道为什么需要创建这个包？令人惊讶的是，如果我不执行此步骤，我会收到错误 - `gymnasium.error.NamespaceNotFound：未找到命名空间gym_examples。您是否为gym_examples安装了正确的软件包？  注册环境意味着什么？我检查了代码，它似乎是一个初始化变量的函数。这就是它的全部作用吗？    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/1b6dh5x/doubts_regarding_creating_a_custom_gymnasium/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6dh5x/doubts_regarding_creating_a_custom_gymnasium/</guid>
      <pubDate>Mon, 04 Mar 2024 15:20:18 GMT</pubDate>
    </item>
    <item>
      <title>对项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b6a6uf/advice_for_a_project/</link>
      <description><![CDATA[我正在为我的最后一年项目寻求紧急建议。我想与某人进行一对一的交谈，最好是博士学者。我有很多想法，我想讨论它们以了解您的意见并获得宝贵的反馈。我想了解这些算法的实现思路以及可能出现的挑战。我非常寻找可以联系的人，因为我迫切需要帮助。    由   提交/u/fa_anony__mous   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b6a6uf/advice_for_a_project/</guid>
      <pubDate>Mon, 04 Mar 2024 12:52:59 GMT</pubDate>
    </item>
    <item>
      <title>自动强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b69vnc/automated_reinforcement_learning/</link>
      <description><![CDATA[我有一个想法，对于给定的游戏，我的代理必须从所有深度 RL 算法中选择可用的最佳算法。所以，我首先想训练一个神经网络，它可以找出最适合给定游戏的算法，例如 DQN、DDPG、A2C、A3C、A3C 等。我想知道这个想法是如何以及如何实现的它。    由   提交/u/fa_anony__mous   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b69vnc/automated_reinforcement_learning/</guid>
      <pubDate>Mon, 04 Mar 2024 12:36:21 GMT</pubDate>
    </item>
    <item>
      <title>项目主题帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b68x6x/help_for_project_topic/</link>
      <description><![CDATA[我这学期的科目是强化学习，我的期末评估是一个基于强化学习的项目。我计划在国际象棋中应用强化学习，但我的导师拒绝了这个主题，因为它已经被很多人完成了，并且与我的主题不“相关”。问题显然。现在我必须选择一个项目主题，我到处研究过，但找不到一个符合他期望的项目主题。我需要一个项目主题来工作，但很多人都没有做过，或者最糟糕的是举例一个可以进行对比分析的项目。请帮忙   由   提交/u/Karthic_2811   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b68x6x/help_for_project_topic/</guid>
      <pubDate>Mon, 04 Mar 2024 11:43:19 GMT</pubDate>
    </item>
    <item>
      <title>ml 药剂中的 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b672v5/ppo_in_ml_agents/</link>
      <description><![CDATA[如果我有一个多代理问题，所有代理都按照相同的模型（同质代理）行事 - 进行训练，如果我使用 PPO，我会得到像 POCA 那样的多智能体倾斜？  我的意思是，在这种情况下，PPO 会利用联合状态行动空间吗？   由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b672v5/ppo_in_ml_agents/</guid>
      <pubDate>Mon, 04 Mar 2024 09:47:18 GMT</pubDate>
    </item>
    <item>
      <title>UniROS 简介：基于 ROS 的机器人强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5xepa/introducing_uniros_rosbased_reinforcement/</link>
      <description><![CDATA[大家好！  我很高兴与大家分享 UniROS，这是我开发的一个基于 ROS 的强化学习框架，旨在弥合模拟与现实世界机器人之间的差距。该框架包含两个关键包：  MultiROS：非常适合使用 ROS 和 Gazebo 创建并发 RL 模拟环境。 RealROS&lt; /strong&gt;：专为在真实的机器人环境中应用 ROS 而设计。  UniROS 的与众不同之处在于它可以轻松地从模拟过渡到现实世界的应用程序，使强化学习对于机器人专家来说更容易使用和有效. 我还为一些低级 ROS 功能添加了额外的 Python 绑定，从而增强了 RL 工作流程之外的可用性。 我很乐意获得您对这些的反馈和想法工具。让我们讨论如何应用和改进它们！ 在 GitHub 上查看它们：  UniROS： github.com/ncbdrck/UniROS RealROS：github.com/ncbdrck/realros MultiROS：  github.com/ncbdrck/multiros  ​   由   提交 /u/ncbdrck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5xepa/introducing_uniros_rosbased_reinforcement/</guid>
      <pubDate>Mon, 04 Mar 2024 00:48:51 GMT</pubDate>
    </item>
    <item>
      <title>适用于低维观测环境的基于模型的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5x7i3/modelbased_rl_for_environments_with_low/</link>
      <description><![CDATA[在阅读有关 MBRL 的论文时，我意识到所有这些方法都是通过基于像素的观察来评估其算法在环境中的性能。然而，很多时候，尤其是在机器人技术中，人们可以访问结构化特征，如 x 位置、y 位置、z 位置、旋转等。 创建一个模型是否有意义？这里的环境适合规划吗？因为即使可以访问结构化信息，模拟的计算成本仍然很高。因此，我认为 MBRL 在这里是有意义的，但我还没有找到关于该特定利基的任何工作。 如果有任何论文推荐，我将不胜感激。    由   提交/u/ijustwanttostudy123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5x7i3/modelbased_rl_for_environments_with_low/</guid>
      <pubDate>Mon, 04 Mar 2024 00:39:34 GMT</pubDate>
    </item>
    <item>
      <title>端到端 JAX 库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5uep5/endtoend_jax_library/</link>
      <description><![CDATA[大家好，我想我应该在这里发个帖子来宣布我最近发布的一个项目。  该项目是一个流行算法实现库，但使用 DeepMind anakin 训练范例。这可能被某些人称为端到端强化学习，其中算法的每个方面都在硬件加速器上运行。对于那些了解 PureJaxRL 的人来说，它是该库的根本构建基础。我的目标是创建一个研究框架（随着时间的推移进行维护和改进），它提供 4 个主要功能： 1. 高效、快速且可扩展的 RL 算法实现，以允许快速迭代研究。 2. 一个易于破解的代码库作为研究项目的起点。 3.标准化基线的高性能算法实现。 4. 教育价值。 紧密遵循 cleanRL 的理念，每种算法的大部分代码都在单个文件中提供，但某些元素已标准化以便重用，例如网络架构、评估和日志记录。该代码库还提供了有用的功能，例如检查点。这个项目还比较新，我只在空闲时间做它。对于任何反馈，我们都表示感谢。如果您有任何疑问，请告诉我。该项目可以在这里找到： https://github.com/EdanToledo/Stoix   由   提交/u/WorkingManTech  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5uep5/endtoend_jax_library/</guid>
      <pubDate>Sun, 03 Mar 2024 22:40:22 GMT</pubDate>
    </item>
    <item>
      <title>离线政策学习的深度生成模型：教程、调查和对未来方向的展望</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5tq4v/deep_generative_models_for_offline_policy/</link>
      <description><![CDATA[ 由   提交/u/Ahamed-Put-2344   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5tq4v/deep_generative_models_for_offline_policy/</guid>
      <pubDate>Sun, 03 Mar 2024 22:13:17 GMT</pubDate>
    </item>
    <item>
      <title>在 Haskell 中使用值迭代</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5qpbe/playing_with_value_iteration_in_haskell/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5qpbe/playing_with_value_iteration_in_haskell/</guid>
      <pubDate>Sun, 03 Mar 2024 20:12:37 GMT</pubDate>
    </item>
    <item>
      <title>持续强化学习和元强化学习研究社区</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5gwsi/continualrl_and_metarl_research_communities/</link>
      <description><![CDATA[我对 RL（连续 RL、元 RL、变压器）对超参数的敏感性和大量的训练时间越来越感到沮丧（我讨厌 RL 之后5年博士研究）。这在元强化学习连续强化学习中尤其成问题，其中一些基准要求长达 100 小时的训练。这使得优化超参数或快速验证新想法的空间很小。考虑到这些挑战以及我准备更深入地探索数学理论，包括学习所有可用的在线数学课程，采用基于证明的方法，以避免无休止的等待和训练循环，我对 2024 年密切相关的人工智能研究领域趋势感到好奇强化学习，但最多只需要 3 个小时的训练时间。有什么建议吗？   由   提交 /u/Noprocr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5gwsi/continualrl_and_metarl_research_communities/</guid>
      <pubDate>Sun, 03 Mar 2024 13:14:49 GMT</pubDate>
    </item>
    <item>
      <title>我应该编写自己的深度强化学习算法还是寻找开源项目？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b5bqth/should_i_write_my_own_deep_reinforcement_learning/</link>
      <description><![CDATA[我是一名研究多智能体深度强化学习的研究生。最近在复现算法方面陷入了自我否定。 正如标题所说，我是应该使用star多的开源GitHub项目框架，还是按照教学博客手动实现算法或者教程？ 在此之前，我一直在根据论文、教学博客、教程等（如YOUTUBE、spin up（openai）等资源）复现算法。我已经成功复现了DQN、DDPG、SAC、MADDPG算法，并且我可以保证没有问题，因为所有的结果都收敛到了一定的良好效果。 这两天遇到的一个问题是当我训练自己的多智能体环境时，我发现我写的maddpg算法训练时间很长，大约需要3天才能收敛。使用的学习率为0.00001（只有使用这么小的学习率才能更好的收敛），有人推荐我使用github上很多star的深度强化学习算法框架，否定了我重新发明轮子的行为。 &lt;当然，我也承认我重新发明轮子的行为很可笑，但是从我自己的学习经历来看，强化学习是非常复杂的。我用于研究的环境是我自己搭建的，不是健身房。数据的处理贯穿强化学习。所以我想听听你的建议或经验。 最后，我找到了为什么我的代码训练这么慢。这是因为我的主循环处于一个纪元中。每执行一步后，神经网络梯度就会更新。我改为先填充经验池，然后多次训练神经网络，直到大约遍历整个经验池，然后停止训练，然后与环境交互来填充经验池。经过这个操作，我发现我可以将训练速度提高10倍。 （nvidia gpu使用cuda有启动时间吗，这个很有趣）   由   提交 /u/yuemingyue   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b5bqth/should_i_write_my_own_deep_reinforcement_learning/</guid>
      <pubDate>Sun, 03 Mar 2024 07:51:56 GMT</pubDate>
    </item>
    <item>
      <title>只是分享我关于 Gran Turismo 的 RL 论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1b4klxv/just_sharing_my_dissertation_in_rl_for_gran/</link>
      <description><![CDATA[https://youtu.be/zpz3Dbmx1SI   由   提交/u/NDR008  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1b4klxv/just_sharing_my_dissertation_in_rl_for_gran/</guid>
      <pubDate>Sat, 02 Mar 2024 09:44:28 GMT</pubDate>
    </item>
    </channel>
</rss>