<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 04 Jan 2025 03:20:04 GMT</lastBuildDate>
    <item>
      <title>随机线性老虎机的最优算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ht4t9n/optimal_algorithms_for_stochastic_linear_bandits/</link>
      <description><![CDATA[我想知道是否有人见过一篇论文，其中匹配了以下设置的遗憾下限。一个线性老虎机，其中在每个时间步骤中根据 1、...、n 上的分布随机选择 n 个损失函数之一。学习者在 d 维连续空间中选择一个动作。我假设老虎机反馈；学习者每次只观察每个动作的损失。 据我所知，下限为 O( d \sqrt{T} ) 已被证明；但是，我认为尚未出现任何算法将预期遗憾的上限设置为 O( d \sqrt{T} )。我见过的最佳算法（尽可能广泛地看待）保证 O( d \sqrt{T log T} ) 预期遗憾。如果有一种算法保证 O( d \sqrt{T} )，我希望这篇论文很容易找到。  是否有人知道一种算法，可以在老虎机反馈下削减对数因子，从而保证 O( d \sqrt{T} )？     提交人    /u/Few_Art1572   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ht4t9n/optimal_algorithms_for_stochastic_linear_bandits/</guid>
      <pubDate>Sat, 04 Jan 2025 03:04:30 GMT</pubDate>
    </item>
    <item>
      <title>强化学习课程建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsv489/reinforcement_learning_course_suggestion/</link>
      <description><![CDATA[我是来自印度的 CS 硕士生，正在准备担任 AI 工程师的职位，我对 ML 和 DL 概念有很好的理解，已经完成了 CS 229（斯坦福 ML）、MIT 6.S191（DL）和其他课程。我对 RL 有基本的了解（作为我的 DL 课程的一部分）并且我想深入研究 RL 概念和实际实施，您能给我推荐免费的在线资源吗？我一直在寻找 YouTube 讲座，然后我遇到了以下内容：1. 斯坦福的 CS 234，最新课程于 2 个月前上传，但没有笔记并且仍然受到限制。（2019 年课程笔记可在线获取）2. 加州大学伯克利分校的 CS 285 Deep R，讲座和幻灯片均可用。 3. David Silver 的 RL 课程（2015 年课程，旧了吗） 4. 阿尔伯塔大学在 Coursea 上的强化学习专业化（我有高级订阅）。 我应该从哪门课程开始，我应该如何进行？ 非常感谢您的建议🙏    提交人    /u/Radiant_Number9202   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsv489/reinforcement_learning_course_suggestion/</guid>
      <pubDate>Fri, 03 Jan 2025 19:44:02 GMT</pubDate>
    </item>
    <item>
      <title>简单的双足机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsu179/simple_bipedal_robot/</link>
      <description><![CDATA[Johnny Five 还活着，而且他现在有腿了！ 双足机器人行走 使用 PPO 训练。您可以在此处试用（需要支持 SharedArrayBuffer 的浏览器）：https://play.prototwin.com/?model=BipedalRobot 单击并拖动以进行交互。右键单击以旋转相机。鼠标中键可平移相机。重置按钮位于屏幕的右上角。 训练脚本可用：https://github.com/prototwin/RLExamples/blob/main/bipedal/bipedal.py 可以在Onshape上找到机器人的 CAD。 如果您有 VR 耳机，则可以单击 VR 按钮。   由    /u/kareem_pt  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsu179/simple_bipedal_robot/</guid>
      <pubDate>Fri, 03 Jan 2025 18:59:48 GMT</pubDate>
    </item>
    <item>
      <title>Advantage Actor-Critic 无法正常工作。（OpenAI gym Cart Pole + Pytorch）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsqmrf/advantage_actorcritic_not_working_properly_openai/</link>
      <description><![CDATA[我试图实现 Advantage Actor Critic 算法来训练 OpenAI 的健身房环境中的 Cart Pole Agent。但即使经过大量参数调整，我也无法产生良好的训练结果。我相信我已经正确实现了它，尽管我看到了同一算法的几个略有不同的变体。 我在这里附上代码。要运行它，您需要使用 pytorch、pygame、gymnasium 和 gymnasium[classic-control] 包。代码已标记且可读。  https://github.com/Utsab-2010/RL-Tests/blob/main/Cart_Pole_A2C-Copy1.ipynb 如果有人能指出到底发生了什么，或者提供一些值得遵循的好资源，我将不胜感激。    提交人    /u/Otaku_boi1833   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsqmrf/advantage_actorcritic_not_working_properly_openai/</guid>
      <pubDate>Fri, 03 Jan 2025 16:40:31 GMT</pubDate>
    </item>
    <item>
      <title>这个 gpu 可以做到</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsparn/cab_this_gpu_do_it/</link>
      <description><![CDATA[所以，我有 nvidia qudro p2000 它具有一个带有 1024 个 CUDA 核心的 Pascal GPU、5 GB GDDR5 板载内存和驱动能力。训练 gpt 1 大小（1.17 亿）或 bert small 大小（400 万）的模型是否足够？    提交人    /u/notanhumanonlyai25   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsparn/cab_this_gpu_do_it/</guid>
      <pubDate>Fri, 03 Jan 2025 15:42:44 GMT</pubDate>
    </item>
    <item>
      <title>面向智能能源社区的多智能体 DQN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsj5z6/multiagent_dqn_for_a_smar_energy_community/</link>
      <description><![CDATA[大家好，祝整个 RL 社区新年快乐！ 我正在研究一个智能能源社区问题。我已经实施了 DQN 来解决社区问题，使用单个 Q 函数将每栋房子的状态作为每栋房子的输入和输出操作，结果还不错。我认为，如果我使用多智能体 RL 方法为每个房子训练智能体，结果会更好。 每栋房子（智能体）都需要通过与主电网或公用电网交易电力来最大限度地降低运营成本，每栋房子都有自己的太阳能、电池和电力消耗。 现在，我正在寻找多智能体 DQN，你们能分享一下你们对这些可能性的看法吗？ 如果您需要有关我的环境或问题的更多详细信息，请提出要求。 提前致谢！    提交人    /u/Dry-Image8120   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsj5z6/multiagent_dqn_for_a_smar_energy_community/</guid>
      <pubDate>Fri, 03 Jan 2025 10:01:40 GMT</pubDate>
    </item>
    <item>
      <title>PPO 不断学习在网格世界环境中什么也不做</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hsj08n/ppo_constantly_learns_to_do_nothing_in_a_gridworl/</link>
      <description><![CDATA[您好！ 我目前正在尝试使用 PPO 解决自定义网格世界环境。网格为 5x5，其中一个单元格是仓库，代理从这里开始。随着时间的推移，物品会随机出现在网格上，并在那里停留 15 个时间步骤，然后再次消失。代理的目标是收集尽可能多的物品并将它们带到仓库。代理的容量为 1（即，他一次只能携带一件物品），并且可以决定向上、向下、向右、向左或什么都不做（请注意，拾取或放下不是单独的操作，因为这些事情在物品单元格或仓库上时会自动发生）。对于成功的物品拾取和放下，他将获得 +15 的总奖励（分为拾取 +7.5 和放下物品 +7.5）。每一步没有拾取或放下都会产生 -1，除非选择的操作是什么都不做，在这种情况下他会得到 0 的奖励。 我选择用大小为 25 的向量表示代理或物品位置等指标（即，每个单元格的一个条目，如果有物品/代理，则为 1 或任何其他相关数字，否则为 0）。因此，我的观察空间包括以下内容：可用容量、代理位置、物品位置、剩余时间、目标位置、到物品和目标的曼哈顿距离、到最近物品的剩余时间和距离、从最近物品到目标的距离、到墙壁的距离。 演员和评论家网络都由 4 个隐藏层组成，有 128 个神经元和 ReLU 激活函数。至于我的超参数，我选择如下： learning_rate：0.0001 gamma：0.99 lam：0.95 clip_ratio：0.2 value_coef：0.5 entropy_coef：0.5 num_trajectories：5 num_epochs：4 num_minibatches：4 max_grad_norm：0.5  现在，在运行此程序时，PPO 无法学习任何可用的策略，最终什么也不做。虽然这是一个合理的学习策略，因为它至少不会产生负的总奖励，但显然不是理想的/最佳的。 由于这有点像 PPO 陷入的局部最优，我认为这可能是一个超参数/探索问题。因此，我增加了熵系数以增加探索，并同样增加了每个策略推出的轨迹数量，以便代理在更新网络时拥有更多可用经验。但是，我尝试的方法似乎都没有奏效。我甚至运行了 WandB 扫描，但该扫描的所有 100 次运行都没有获得超过 0 的总奖励。观察到这一点后，我认为代码中一定存在某种错误，这就是为什么我一遍又一遍地检查代码以试图找出问题所在。但是，我无法发现实施中的任何错误（这并不是说存在错误，我只是在检查了 x 次代码后没有发现任何错误）。 有人知道是什么阻止 PPO 学习好的策略吗？显然，代理在连接拿起物品和放下物品的动作时存在问题。但是，我不明白为什么会这样，因为由于取货和送货的奖励是分开的，代理应该很容易就能弄清楚，在有空闲容量的情况下，他应该去任何物品单元，而在满负荷的情况下，他应该去仓库。 如果需要或感兴趣，您可以通过 pastebin 在这里找到整个代码：https://pastebin.com/zuRprVWR。 我希望有人能对我还能尝试什么来解决这个问题提供一些意见。您认为问题实际上源于实施/逻辑错误，还是还有其他原因？或者 PPO 毕竟无法解决这个问题，而其他算法可能是更好的选择？ 感谢您的任何见解！    提交人    /u/Jakoebly   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hsj08n/ppo_constantly_learns_to_do_nothing_in_a_gridworl/</guid>
      <pubDate>Fri, 03 Jan 2025 09:50:31 GMT</pubDate>
    </item>
    <item>
      <title>有人申请 2025 年秋季 RL 专业的博士课程吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hs6nun/anyone_applying_for_fall_2025_phd_program_with_rl/</link>
      <description><![CDATA[我是一名博士候选人。我只是想知道录取通知是否已经发出。    提交人    /u/searchForApocalypse   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hs6nun/anyone_applying_for_fall_2025_phd_program_with_rl/</guid>
      <pubDate>Thu, 02 Jan 2025 22:37:38 GMT</pubDate>
    </item>
    <item>
      <title>Felix Hill 已去世 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hrzgdg/felix_hill_has_died_dm/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hrzgdg/felix_hill_has_died_dm/</guid>
      <pubDate>Thu, 02 Jan 2025 17:43:09 GMT</pubDate>
    </item>
    <item>
      <title>Sutton 书中的练习 3.27</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hrsjue/exercise_327_in_suttons_book/</link>
      <description><![CDATA[嗨，关于标题中的练习（根据 q_star 给出 pi_star 的方程式）。 我的直觉答案是做一些顺利的事情，例如： pi_star(a|s) = q_star(s,a) / sum_over_a_prime(q_star(s,a_prime)) 但是在互联网上看到一个 1-0 解决方案： 如果 a 是 argmax_over_a(q_star(s,a))，则 pi_star(a|s) = 1，否则为 0。 希望获得外部反馈，看看我的答案在某些情况下是否正确，或者是否完全错误    提交人    /u/Potential_Hippo1724   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hrsjue/exercise_327_in_suttons_book/</guid>
      <pubDate>Thu, 02 Jan 2025 12:24:56 GMT</pubDate>
    </item>
    <item>
      <title>🚀 使用大型语言模型增强数学问题解决能力：分而治之的方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hr4dvz/enhancing_mathematical_problem_solving_with_large/</link>
      <description><![CDATA[大家好！ 我很高兴与大家分享我们的最新项目：使用大型语言模型 (LLM) 增强数学问题解决能力。我们的团队开发了一种新颖的方法，利用分而治之的策略来提高 LLM 在数学应用中的准确性。 主要亮点：  专注于计算挑战，而不是基于证明的问题。 在各种测试中实现最先进的性能。 开源代码可供任何人探索和贡献！  在此处查看我们的 GitHub 存储库：DaC-LLM 我们正在寻找有兴趣推进该领域研究的反馈和潜在合作者。如有任何疑问，请随时联系我们或发表评论！ 感谢您的支持！    提交人    /u/jasonhon2013   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hr4dvz/enhancing_mathematical_problem_solving_with_large/</guid>
      <pubDate>Wed, 01 Jan 2025 14:53:09 GMT</pubDate>
    </item>
    <item>
      <title>grokking 的书好看吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hr03om/is_the_grokkings_book_any_good/</link>
      <description><![CDATA[我正在寻找优秀的 RL 书籍。我知道 Sutton 和 Barto 的书是标准，但我发现它的 pdf 有点吓人。我正在寻找可以帮助我快速学习概念的书籍，最好数学内容较少。另一本书是 Grokkings 书，想知道它是否值得购买（在我的国家它非常昂贵）。如果您有其他推荐的书籍，请告诉我。谢谢    提交人    /u/insightfuleffect   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hr03om/is_the_grokkings_book_any_good/</guid>
      <pubDate>Wed, 01 Jan 2025 10:02:45 GMT</pubDate>
    </item>
    <item>
      <title>关于博士录取</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqvckw/regarding_phd_admissions/</link>
      <description><![CDATA[我想攻读 RL、ML（主要是理论部分）博士学位，我是一名机械工程本科生。不想攻读硕士学位。但我想知道 GPA 在获得博士学位录取时有多重要，我知道它非常重要，但如果 GPA 很差，比如 6/10，会考不上吗？研究论文能否弥补由于 GPA 造成的这种差距？假设某人在医学应用机器学习方面有一篇第一作者方法论文。还有另一篇关于神经符号 AI 的机器人技术论文，以及一些关于机器人策略的经验，以及不错的 ML、RL 课程项目背景等    提交人    /u/vyknot4wongs   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqvckw/regarding_phd_admissions/</guid>
      <pubDate>Wed, 01 Jan 2025 04:11:27 GMT</pubDate>
    </item>
    <item>
      <title>RL 博客？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqsyw2/rl_blogs/</link>
      <description><![CDATA[你们听说过 TLDR AI 吗？它让我对 AI 的发展有了很好的了解。我是一名初学者，想跟上 RL 的步伐。我可以阅读哪些关于 RL 的博客/文章？    提交人    /u/Cereal_killer09   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqsyw2/rl_blogs/</guid>
      <pubDate>Wed, 01 Jan 2025 01:39:15 GMT</pubDate>
    </item>
    <item>
      <title>探索性的定义（Barton 和 Sutton 问题）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hqn7o1/definition_of_exploratory_barton_and_sutton/</link>
      <description><![CDATA[我正在阅读 Barto 和 Sutton 的《强化学习导论》第二版，有一个基本问题。 练习 2.1 &quot;在 e-贪婪动作选择中，对于两个动作且 e = 0.5 的情况，选择贪婪动作的概率是多少？&quot; 我的解决方案是 0.75，因为有 50% 的机会选择随机选择，而此后，只有 50% 的机会选择非贪婪动作。但其他几个在线资源表明是 0.50。 作为参考，此文本包含在书中。 “一个简单的替代方法是大多数时间都贪婪地行事，但偶尔，以小概率从所有具有相同概率的动作中随机选择，独立于动作值估计。” 所以要么我误解了这一点，要么探索故意省略了贪婪动作，要么我错过了一个微妙的语义问题。我也有可能是对的：） 任何帮助都将不胜感激。这是一个非常沉重的文本，我想确保我理解了。    提交人    /u/EricTheNerd2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hqn7o1/definition_of_exploratory_barton_and_sutton/</guid>
      <pubDate>Tue, 31 Dec 2024 20:24:44 GMT</pubDate>
    </item>
    </channel>
</rss>