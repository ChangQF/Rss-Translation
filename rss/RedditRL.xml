<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 03 May 2024 03:16:00 GMT</lastBuildDate>
    <item>
      <title>为什么我无法创建矢量化环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cixsw1/why_cant_i_create_a_vectorized_environment/</link>
      <description><![CDATA[我正在尝试使用以下代码创建矢量化环境 -  from stable_baselines3 import DQN from stable_baselines3.common .env_util import make_vec_env from stable_baselines3.common.vec_env import VecEnvWrapper from stable_baselines3.common.monitor import Monitor fromgym.wrappers import TransformReward importgym import numpy as np # 创建 Gym 环境 # 定义自定义奖励转换函数 defreward_transform(reward): return return *0.0 # 将奖励限制在 [-1, 1] 范围内 # 将 TransformReward 包装器应用到 Gym 环境 # 定义一个返回包装环境的函数 def make_wrapped_env(**env_kwargs): return TransformReward(gym.make( &quot;CartPole-v1&quot;, **env_kwargs),reward_transform) env_kwargs = {} env_kwargs[&quot;eval_env&quot;] = False # 创建一个包含 4 个并行的包装 Gym 环境实例的矢量化环境 vec_env = make_vec_env(make_wrapped_env, n_envs=4 , env_kwargs=env_kwargs) # 现在你可以使用 vec_env 来训练你的 RL 代理 # 创建并训练 DQN 模型 model = DQN(&quot;MlpPolicy&quot;, vec_env, verbose=1) model.learn(total_timesteps=100000, log_interval=4) model.save(“dqn_cartpole”)  但是，我收到以下错误 -  TypeError: __init__() 获得意外的关键字参数 &#39;eval_env &#39;  将“kwargs”添加到我的环境构造函数中会引发错误，这非常令人惊讶。 RLZoo 在这里做了同样的事情，但他们不这样做没有收到错误。   由   提交/u/Academic-Rent7800  /u/Academic-Rent7800  reddit.com/r/reinforcementlearning/comments/1cixsw1/why_cant_i_create_a_vectorized_environment/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cixsw1/why_cant_i_create_a_vectorized_environment/</guid>
      <pubDate>Fri, 03 May 2024 02:32:09 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助使用 RL 解决 3D Bin Packing 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cifzuy/need_help_with_solving_3d_bin_packing_problem/</link>
      <description><![CDATA[大家好！昨天在工作中，我接到的任务是为我们的一位客户创建一种算法来处理 3D 装箱问题的修改版本。如果一切顺利，我将有一些时间来温习我的技能并制定解决方案。 事情是这样的：我们需要找出向送货卡车装载尽可能多货物的最佳方法包尽可能。每个包裹都带有几个关键细节：  与客户的距离（需要走多远）。 材料类型（由什么制成）。   li&gt;  从这些属性中可以看出三个限制  必须运输较远的包裹应包装在运输较短的包裹下面。 易碎包裹应包装在运输距离较短的包裹下面。坐在不太脆弱的上面。 解决方案必须在物理上稳定。  此外，我被要求使用强化学习来解决这个问题，因为我获得的知识将对未来的项目有用（我目前缺乏强化学习方面的知识，因此我将有大约一个月的时间在这方面进行自我培训）。 我一直在研究强化学习，以解决BPP。以下是高度总结的细节：  RL 最近在组合问题上取得了成功，比传统方法具有更好的性能。 RL 求解 BPP 的假设是这些框是先验未知的（在此问题中没有必要，因为所有框都是已知的）。  此 资源确实很有帮助。我认为我们可以在强化学习中利用这个关于盒子顺序的假设，例如我们可以先给智能体一系列非易碎盒子，然后再给智能体一系列易碎盒子，这样智能体就会把非易碎盒子放在底部。 你觉得怎么样？通过强化学习来解决这个问题是否可行，或者我应该向我的老板建议传统的启发式方法可能更合适？感谢您的任何意见！ 谢谢！   由   提交/u/dylannalex01  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cifzuy/need_help_with_solving_3d_bin_packing_problem/</guid>
      <pubDate>Thu, 02 May 2024 13:42:26 GMT</pubDate>
    </item>
    <item>
      <title>意外的观察形状</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cia1gx/unexpected_observation_shape/</link>
      <description><![CDATA[我有一个 boid 集群环境。我制作了一个可以与 3 个 boids 配合使用的模型。我必须将 1D 数组动作 obs 重塑为 2D 才能通过，除此之外还可以。 现在我正在训练 10 个 boids。训练进行得很顺利，但是在测试时我收到了这个错误，尽管似乎没有什么问题。我也重塑它以传递 10 个操作。 ValueError: 错误: Box 环境出现意外的观察形状 (40,)，请使用 (12,) 或 (n_env, 12) 进行观察形状。 我在 OpenAI Gym 中使用 SB3 的 PPO。 动作和观察空间： min_action = np.array ([-5, -5] * len(self.agents), dtype=np.float32) max_action = np.array([5, 5] * len(self.agents), dtype=np.float32) self.action_space = space.Box(low=min_action, high=max_action, dtype=np.float32) min_obs = np.array([-np.inf, -np.inf, -2.5, -2.5] * len(self.agents), dtype=np.float32) max_obs = np.array([np.inf, np.inf, 2.5, 2.5] * len(self.agents), dtype=np.float32) self.observation_space = space.Box(low=min_obs , high=max_obs, dtype=np.float32)    由   提交 /u/Hooooman101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cia1gx/unexpected_observation_shape/</guid>
      <pubDate>Thu, 02 May 2024 07:55:38 GMT</pubDate>
    </item>
    <item>
      <title>kanrl,用于强化学习的 Kolmogorov-Arnold 网络，初始实验,下载kanrl的源码_GitHub_帮酷</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cia0r3/github_riiswakanrl_kolmogorovarnold_network_for/</link>
      <description><![CDATA[       ,下载kanrl的源码_GitHub_帮酷由   提交/u/riiswa   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cia0r3/github_riiswakanrl_kolmogorovarnold_network_for/</guid>
      <pubDate>Thu, 02 May 2024 07:54:15 GMT</pubDate>
    </item>
    <item>
      <title>如何将健身裹布与 SB3 结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/</link>
      <description><![CDATA[我正在尝试结合健身房包装器“TransformReward&quot;与SB3。这是我的代码 -  from stable_baselines3 import DQN from stable_baselines3.common.env_util import make_vec_env fromgym.wrappers import TransformReward defreward_transform(reward): # 奖励转换函数示例 returnreward * 0 # Scale将奖励加0以检查函数是否正常工作 # 创建向量化环境 env = make_vec_env(&quot;CartPole-v1&quot;, n_envs=1,wrapper_class=TransformReward,wrapper_kwargs={&quot;f&quot;:reward_transform}) # 创建并训练DQN 模型 model = DQN(&quot;MlpPolicy&quot;, env, verbose=1) model.learn(total_timesteps=100000, log_interval=4) model.save(&quot;dqn_cartpole&quot;)  As为了确保我的代码正常工作，我尝试将奖励缩放为 0。 但是，我看到代理正在学习并获得奖励 -  ``` &lt;前&gt;&lt;代码&gt;-------------------------------- |推出/ | | | ep_len_mean | 83.6 | 83.6 | ep_rew_mean | 83.6 | 83.6 |探索率 | 0.05 | 0.05 |时间/ | | |剧集 | 3548 | 3548 |帧率 | 3401 | 3401 |已用时间 | 29 | 29 |总时间步数 | 99751 | |火车/ | | |学习率 | 0.0001 | 0.0001 |损失| 2.29e-06 | | n_更新 | 12437 | 12437 ----------------------------------  ``` 有人可以告诉我的代码有什么问题吗？谢谢！   由   提交/u/Academic-Rent7800  /u/Academic-Rent7800  reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chxy67/how_to_combine_a_gym_wrapper_with_sb3/</guid>
      <pubDate>Wed, 01 May 2024 21:34:05 GMT</pubDate>
    </item>
    <item>
      <title>在我的电脑中设置airSim项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chvlkj/set_up_airsim_project_in_my_computer/</link>
      <description><![CDATA[大家好，我需要一些帮助。我一直在尝试在我的计算机上设置这个项目（它是一个准备使用的项目）并遵循提到的所有步骤，但我似乎无法管理它。当我到达“运行first.py以控制无人机”的步骤时，我找不到名为“first.py”的文件。我不确定这里缺少什么。如果已经尝试过的人可以帮助我，那将非常有帮助。 https://github。 com/lap98/RL-Drone-Stabilization   由   提交/u/gintooki_23  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chvlkj/set_up_airsim_project_in_my_computer/</guid>
      <pubDate>Wed, 01 May 2024 19:57:59 GMT</pubDate>
    </item>
    <item>
      <title>如何运用和运用模仿学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chr4lq/how_to_apply_and_use_imitation_learning/</link>
      <description><![CDATA[嗨 我想实现一个人形机器人执行某些动作的模拟。我计划使用 cmu mocap 数据集用于训练目的，并使用 dm_control 给出的 cmu_ humanoid 。 我应该使用什么以及我应该如何处理已经写了一些论文，它们似乎遵循编码器解码器计划来实现这一目标。然而，他们不提供代码实现，我对如何实现这一点感到非常困惑。 这是我引用的论文用于人形控制的神经概率运动原语。 也许我应该参考他们之前使用编码器-解码器来使用此类东西的代码（不需要跟踪 cmu使用人形机器人进行运动），但我似乎找不到太多代码。 您能否向我介绍一些代码实现。 谢谢    由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chr4lq/how_to_apply_and_use_imitation_learning/</guid>
      <pubDate>Wed, 01 May 2024 16:57:08 GMT</pubDate>
    </item>
    <item>
      <title>为什么在这么多论文中重新解释RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chq0w2/why_is_rl_reexplained_in_so_many_papers/</link>
      <description><![CDATA[我遇到的许多论文都用第二章的整个开头来重新解释 RL 的基础知识（状态/动作值函数、贝尔曼方程、q 学习），就好像它是一些没有人听说过的新发现一样，同时又如此快速地讲解了如此多的细节，如果你不熟悉这些概念，你最好在其他地方学习。 其他领域不会这样做，你不会看到每篇物理学论文都重新解释广义相对论的工作原理。假设如果你正在阅读那篇论文，那么你对该领域有一定的了解。 我很好奇，RL 中是否有这样做的原因？我能想到的唯一一件事是，如果每个人对 RL 的理解都略有不同，他们会希望读者在同一页面上，但每次几乎都是如此。    提交人    /u/giorgiocav123   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chq0w2/why_is_rl_reexplained_in_so_many_papers/</guid>
      <pubDate>Wed, 01 May 2024 16:11:44 GMT</pubDate>
    </item>
    <item>
      <title>模型外部的不确定性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chobrx/uncertainty_outside_of_model/</link>
      <description><![CDATA[对于不可能进行完整建模的环境，假设操作 a_k 导致状态 S_k 位于我的模型之外（这也意味着马尔可夫属性不再成立）。 我应该怎么做才能使 a_k+1 和 a_k+2 继续在控制范围内？ 递归贝叶斯估计可以用于“更新”策略吗？    由   提交 /u/Alive-Opportunity-23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chobrx/uncertainty_outside_of_model/</guid>
      <pubDate>Wed, 01 May 2024 15:01:13 GMT</pubDate>
    </item>
    <item>
      <title>无需从头开始训练，如何应对新的动作维度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cho457/how_to_deal_with_new_action_dimensions_without/</link>
      <description><![CDATA[例如，RL 代理首先基于 N 维的动作空间进行训练。现在，有 2 个新的动作维度，使动作空间成为 N+2 维。是否有任何可能的方法可以在 N+2 维动作空间上训练预训练的智能体，而无需从头开始重新训练智能体？  （假设无法知道是否会有新的动作维度/在发生之前会有多少个新的动作维度。因此不可能将动作空间设置为 N+2 维）   由   提交 /u/YuDerrickZ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cho457/how_to_deal_with_new_action_dimensions_without/</guid>
      <pubDate>Wed, 01 May 2024 14:52:19 GMT</pubDate>
    </item>
    <item>
      <title>tf.js 在强化学习方面有多稳定</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chjfal/how_stable_is_tfjs_for_doing_reinforcement/</link>
      <description><![CDATA[嘿，我想在浏览器中做点什么，而不是用 Python。使用 Python 的主要挑战是我必须设置一些东西来演示输出或录制视频，如果我可以在 js 中做到这一点，那么我就可以进行现场网络演示……这是我在 js 中做这件事的目的，我也擅长使用三个 js 和其他可视化 js 库，如果你有其他在 Python 中做到这一点的方法，请告诉我。 回到主要问题，我看到在 tf.js 中我们可以使用  // 执行梯度下降 optimizer.minimize(() =&gt; loss); 我们可以定义一个神经网络并在 js 中进行反向传播和其他操作吗，现在够好了吗？ ​   由    /u/yellowsprinklee  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chjfal/how_stable_is_tfjs_for_doing_reinforcement/</guid>
      <pubDate>Wed, 01 May 2024 11:06:14 GMT</pubDate>
    </item>
    <item>
      <title>dm_control 的替代方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1chhx6w/alternatives_to_dm_control/</link>
      <description><![CDATA[嗨 我知道 dm_control 在很多研究工作中使用，我也想使用它。事实证明，它没有很好的记录，很难导航，而且最糟糕的是，维护者没有正确回答问题，有时甚至完全忽略这些问题。这让我很愤怒，但我无能为力，我不会因此责怪开发人员，他们可能将时间投入到其他一些作品上，并且在任何情况下都没有义务回答我们。  ​ 话虽这么说，我真的很希望看到该领域开发出一些替代方案，以便减少闯入该领域的人并做出更多贡献。  ​ 您是否知道一些正在朝这个方向发展的作品？  &amp; #32；由   提交/u/rak109  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1chhx6w/alternatives_to_dm_control/</guid>
      <pubDate>Wed, 01 May 2024 09:32:07 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习的四轴飞行器控制</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ch4h1d/quadcopter_control_with_reinforcement_learning/</link>
      <description><![CDATA[您好，我是一名学生，正在研究一个有关使用深度强化学习的四轴飞行器控制器的项目。不幸的是，我的项目演示将于下周进行，而我的电脑缺乏从头开始构建它的能力。您知道我可以参考的任何现有资源或项目吗？    由   提交/u/taha_sbh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ch4h1d/quadcopter_control_with_reinforcement_learning/</guid>
      <pubDate>Tue, 30 Apr 2024 21:32:30 GMT</pubDate>
    </item>
    <item>
      <title>具有不确定性的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgx7uz/environments_with_uncertainty/</link>
      <description><![CDATA[有人知道有哪些环境可能表现出一些不确定性吗？例如，让我们想象环境中的一个部分，如果代理进入其中，不确定性为： ​  采取所需操作的概率会降低并且更加随机 奖励在该区域中变得随机 还是其他东西？  ​ 我想要它，这样我就可以研究一些预先存在的不确定性强化学习技术。最好环境与健身房兼容，我不介意离散或连续，对两者都很满意:) ​ 提前致谢！ &lt; /div&gt;  由   提交/u/Glum_Significance140   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgx7uz/environments_with_uncertainty/</guid>
      <pubDate>Tue, 30 Apr 2024 16:33:08 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习目前的最新技术是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cgobyl/what_is_the_current_state_of_the_art_in_multi/</link>
      <description><![CDATA[自 2019 年以来我就没怎么关注过 MARL，我很好奇从那以后发生了什么。论文链接将不胜感激！    由   提交/u/geargi_steed  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cgobyl/what_is_the_current_state_of_the_art_in_multi/</guid>
      <pubDate>Tue, 30 Apr 2024 09:07:35 GMT</pubDate>
    </item>
    </channel>
</rss>