<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 20 Jan 2025 15:17:38 GMT</lastBuildDate>
    <item>
      <title>面对工业工厂中任务分配优化的 DQN 性能不佳的问题，有什么帮助吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5rcvu/facing_poor_performance_with_dqn_for_mission/</link>
      <description><![CDATA[嗨！我是 RL 的初学者，我一直在学习 dqn 并致力于使用它来优化工业工厂中的任务分配。 我们有几个机器人 (AGV) 和任务。每个任务都有一个特定的标签序列，机器人必须按顺序访问这些标签。我将状态构建为一个列表，其中包括： - 空闲机器人， - 当前正在执行任务的机器人， - 停止服务的机器人， - 机器人充电， - 未请求的任务， - 请求的任务， - 正在进行的任务， - 标签可用性， - 机器人位置， - 每个机器人的任务步骤（默认为 1）， - 所有机器人的电池电量。  例如，有 4 个机器人和 4 个任务，状态可能如下所示：  [[0, 1, 1, 1],  [0, 0, 0, 0],  [1, 0, 0, 0],  [0, 0, 0, 0],  [0, 1, 1, 1],  [1, 0, 0, 0],  [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],  [0, 0, 0, 0, 0, 0, 0],  [2, 2, 2, 2],  [1, 1, 1, 1],  [0.71, 0.34, 0.6, 0.4]]  动作以对的形式表示，如 (&#39;1&#39;, &#39;4&#39;)，表示“将任务 4 分配给机器人 1” 如果某个动作不可行（机器人已被占用或任务正在进行中），系统将施加惩罚（-80）。下一个状态与当前状态相同。元组（当前状态、所选动作的索引、惩罚、下一个状态）将添加到缓冲区。  如果操作可行，则根据以下内容计算奖励：  机器人与任务的接近度， 所有正在进行的机器人（包括所选机器人）的电池电量， 机器人完成任务的预计持续时间。  然后将元组（状态、所选操作的索引、奖励、下一个状态）添加到缓冲区。 尽管测试了不同的激活函数和参数，但模型表现不佳。结果是“随机的”，或者预测的动作是重复的（对于我测试的每个随机状态都得到相同的预测） 我不确定是什么原因造成的，或者如何改进它，有什么想法吗 :&#39;)？。如果对我的实现有任何不清楚的地方，请告诉我！     由    /u/Dazzling-Prize3371  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5rcvu/facing_poor_performance_with_dqn_for_mission/</guid>
      <pubDate>Mon, 20 Jan 2025 14:29:26 GMT</pubDate>
    </item>
    <item>
      <title>Pong 的策略梯度代理没有学习（帮助）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5q8kp/policy_gradient_agent_for_pong_is_not_learning/</link>
      <description><![CDATA[嗨，我是强化学习的新手，正在尝试使用策略梯度法训练我的代理玩 Pong。我参考了深度强化学习：从像素开始玩 Pong。和使用 Cartpole 和 PyTorch 的策略梯度。因为我想学习 Pytorch，所以我决定使用它，但我的实现似乎缺少一些东西。我尝试了很多东西，但它所做的只是学习一次反弹然后停止（之后它什么也不做）。我认为问题出在我的损失计算上，所以我尝试改进它，但它仍然重复相同的过程。 这是 git：使用 pytorch 的 Pong 的 RL    提交人    /u/nightsy-owl   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5q8kp/policy_gradient_agent_for_pong_is_not_learning/</guid>
      <pubDate>Mon, 20 Jan 2025 13:33:42 GMT</pubDate>
    </item>
    <item>
      <title>从哪里开始</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5bnne/where_to_begin/</link>
      <description><![CDATA[我最近有个想法，教一个学习模型玩一款名为蜜蜂群模拟器的游戏，这只是一个副项目。 我对 Python 了解甚少，但我对如何做这样的事情一无所知。我希望能够因为做正确的事情而获得奖励，但除此之外，我不知道我需要什么模型、什么脚本或任何东西。 如果你知道或见过类似的东西，请分享，否则如果你能告诉我从哪里开始学习，那就太好了，谢谢。    提交人    /u/Decreasify   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5bnne/where_to_begin/</guid>
      <pubDate>Sun, 19 Jan 2025 22:56:22 GMT</pubDate>
    </item>
    <item>
      <title>分类 DQN 对于确定性完全观察环境是否有用</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i5a0n5/is_categorical_dqn_useful_for_deterministic_fully/</link>
      <description><![CDATA[...喜欢 Cartpole？这个 Rainbow DQN 教程 使用了 Cartpole 示例，但我想知道“彩虹”的分类部分在这里是否有点过头了，因为 Q 值应该是一个明确定义的值，而不是统计分布，既缺乏随机性又缺乏部分可观测性。    提交人    /u/exploring_stuff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i5a0n5/is_categorical_dqn_useful_for_deterministic_fully/</guid>
      <pubDate>Sun, 19 Jan 2025 21:45:33 GMT</pubDate>
    </item>
    <item>
      <title>关于策略迭代的紧急问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i58mt0/urgent_question_about_policy_iteration/</link>
      <description><![CDATA[你好，我明天有期末考试，我对策略迭代有结果。它有两个步骤：评估和改进。例如，我处于状态 s0，我的操作是 a。奖励 s0=0，奖励 s1=-0.04。采取行动后，我的状态将为 s1。在评估步骤中，我必须在计算 v(s0) 时计算 r(s0)，在改进步骤中，奖励是 -0.04，或者我对两个步骤都使用 -0.04。    提交人    /u/dodohasmala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i58mt0/urgent_question_about_policy_iteration/</guid>
      <pubDate>Sun, 19 Jan 2025 20:49:01 GMT</pubDate>
    </item>
    <item>
      <title>受限 3D 环境中使用 RL 进行多无人机目标分配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i588u3/multidrone_goal_allocation_with_rl_in_a/</link>
      <description><![CDATA[我正在研究一个具有挑战性的问题，涉及3D 环境中无人机的多智能体协调。具体来说：  场景： 有 20 架无人机必须共同访问 3D 地图上的所有目标点。 无人机从任意目标点出发（不一定是同一个目标点）。 目标是最小化访问所有目标点所需的总时间。  过程： 该过程分为&quot;轮&quot;： 在每一轮中，无人机选择要移动到的新目标点。 无人机前往其选定的目标点。一旦所有无人机到达目的地，它们就会同时进行测量（不提前开始）。 测量完成后，下一轮开始。   限制： 每架无人机的电池容量有限。 共有五个充电站，可放置在任何目标点。每个站点可以同时为无限数量的无人机提供服务，但充电需要时间。  目标： 尽量减少所有无人机集体访问所有目标点所需的总时间。   问题框架和挑战 我相信这是每轮最小-最大多旅行者推销员问题 (mTSP)的变体，带有电池限制和充电等额外限制。虽然用于成对距离的 Floyd-Warshall 和混合整数规划 (MIP) 等传统方法可以解决这个问题，但我想探索强化学习 (RL) 作为解决方案。但是，我正在努力应对几个挑战：  初始状态多变性：与许多无人机从单个仓库出发的 mTSP 方案不同，我的无人机从任意初始目标点出发。这引入了多种初始状态。  RL 如何处理这种多变性？ 即使我考虑从所有可能的初始状态的均匀概率开始，任何单一状态的概率都非常小，这可能会使学习效率低下。  动作空间大小：在每一轮中，每架无人机必须从所有剩余未访问点中选择一个目标点，从而产生巨大的动作空间（剩余点选择 20​​）。这种高维动作空间使 RL 难以有效地探索或学习最佳策略。  在此类问题中，是否存在有效的动作空间减少或分层 RL技术？  多智能体协调：由于这是一个多智能体设置，因此可能需要多智能体强化学习 (MARL)。但是，我对 MARL 框架或协作动态问题的最佳实践不太熟悉。  征求建议 我正在寻找以下方面的见解或指导：  多智能体强化学习 (MARL) 是否是解决此问题的正确方法？  如果是，是否有适合我的问题的规模和约束的特定框架、算法或策略（例如 QMIX、MADDPG 或其他）？  我如何有效地处理： 无人机的不同初始状态？ 每一轮的大动作空间？  是否有参考文献、研究论文或案例研究涉及多智能体 RL 进行动态目标分配或无人机协调问题？     提交人    /u/Frankie114514   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i588u3/multidrone_goal_allocation_with_rl_in_a/</guid>
      <pubDate>Sun, 19 Jan 2025 20:32:57 GMT</pubDate>
    </item>
    <item>
      <title>偏差和方差：萨顿痛苦教训的重演</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i56xkh/bias_and_variance_a_redux_of_suttons_bitter_lesson/</link>
      <description><![CDATA[原始形式 20 世纪 90 年代，计算机开始在国际象棋比赛中击败人类大师。许多人研究了这些国际象棋代理所使用的技术，并谴责道：“它只是机械地死记硬背所有动作。这不是真正的智能！”  旨在模仿人类认知某些方面的手工算法将始终赋予 AI 系统更高的性能。而且这种性能提升将是暂时的。随着更强大的计算能力的涌入，从长远来看，依赖“无意识”深度搜索或大量数据（CONV 网络）的算法将胜过它们。  Richard Sutton 将此描述为一个惨痛教训，因为他声称，过去 70 年的 AI 研究就是对此的证明。  统计形式 2022 年夏天，牛津大学和伦敦大学学院的研究人员发表了一篇足以包含章节的论文。这是一项关于因果机器学习的调查。第 7 章涵盖了因果强化学习的主题。在那里，Jean Kaddour 和其他人提到了 Sutton 的苦涩教训，但它以新的眼光出现——通过统计和概率的观点进行反映和过滤。   我们将两个社区关注点不同的原因归因于各自处理的应用类型。绝大多数关于现代 RL 的文献评估了能够生成大量数据的合成数据模拟器上的方法。例如，流行的 AlphaZero 算法假设可以访问棋盘游戏模拟，允许代理在不受数据量限制的情况下玩许多游戏。它的一项重要创新是 tabula rasa 算法，它具有更少的手工制作知识和特定领域的数据增强。有些人可能会认为 AlphaZero 证明了 Sutton 的惨痛教训。从统计学的角度来看，它大致表明，在给定更多计算和训练数据的情况下，具有低偏差和高方差的通用算法优于具有高偏差和低方差的方法。  你会说这反映在你自己的研究吗？在实践中，具有低偏差和高方差的算法是否优于高偏差低方差算法？ 你的想法？    http://www.incompleteideas.net/IncIdeas/BitterLesson.html  https://arxiv.org/abs/2206.15475     提交人    /u/moschles   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i56xkh/bias_and_variance_a_redux_of_suttons_bitter_lesson/</guid>
      <pubDate>Sun, 19 Jan 2025 19:38:55 GMT</pubDate>
    </item>
    <item>
      <title>嘿，你听说过 u/0xNestAI 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i56xcz/hey_have_you_heard_about_u0xnestai/</link>
      <description><![CDATA[它是一个自主的 DeFi 代理，旨在通过实时洞察、重新质押策略和最大化收益潜力来帮助指导您了解 DeFi 领域。他们还将很快推出 #DeFAI 代币！非常好奇这会如何改变我们对待 DeFi 的方式。在他们的推特上查看更多详情。    提交人    /u/grassconnoisseur09   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i56xcz/hey_have_you_heard_about_u0xnestai/</guid>
      <pubDate>Sun, 19 Jan 2025 19:38:40 GMT</pubDate>
    </item>
    <item>
      <title>Carla自动驾驶收敛问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4ydt0/carla_autonomous_driving_convergence_problem/</link>
      <description><![CDATA[我遇到了一个问题，我的自动驾驶代理无法收敛，我无法确定确切的原因。我想问问是否有人有时间和兴趣帮助我分析可能导致问题的原因。由于我使用的是 Stable-Baselines3，因此不太可能是 RL 算法本身的问题，因此它可能与超参数或奖励有关。 如果有人感兴趣，请随时对这篇文章发表评论，我会分享我的 Discord 以进一步讨论。    提交人    /u/Fair_Device_4961   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4ydt0/carla_autonomous_driving_convergence_problem/</guid>
      <pubDate>Sun, 19 Jan 2025 13:26:17 GMT</pubDate>
    </item>
    <item>
      <title>教 AI 汽车驾驶：Unity ML-Agents 模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4vsq3/teaching_ai_cars_to_drive_a_unity_mlagents/</link>
      <description><![CDATA[https://reddit.com/link/1i4vsq3/video/ckatil9dgxde1/player 大家好！我一直在 Unity 中研究 AI 模拟，其中使用 ML-Agents 和强化学习训练汽车在红灯时停止、在绿灯时行驶以及在路口导航。 完整视频链接 - https://www.youtube.com/watch?v=rkrcTk5bTJA 在过去的 8-10 天里，我付出了很多努力来训练这些汽车，虽然结果还不完美，但看到它们的进步还是很令人兴奋的！ 我计划探索更复杂的场景，例如汽车处理多车道交通、在环形交叉路口导航以及对动态障碍物做出反应。我还打算与其他对 AI 模拟感兴趣的人合作，并最终在 GitHub 上分享这些实验的代码。 我在 YouTube 上发布了这个模拟的视频，我很乐意听到您的反馈或建议。如果您有兴趣看到更多此类项目，请考虑通过订阅频道来支持！ 谢谢    提交人    /u/FiredNeuron97   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4vsq3/teaching_ai_cars_to_drive_a_unity_mlagents/</guid>
      <pubDate>Sun, 19 Jan 2025 10:40:46 GMT</pubDate>
    </item>
    <item>
      <title>RL 中的优化？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4s3rt/optimization_within_rl/</link>
      <description><![CDATA[我想通过调整边界约束将 RL 应用于受约束的线性规划。LP 的形式为：最大 c&#39;v，受 Ax=0 约束，x &lt; xub。因此，我希望我的代理对 xub（连续）的元素采取行动。我将使用 x 的一些预测值通过欧拉前向方法更新环境。奖励将是每个时间步的函数值，以及该情节的一些折扣值。这可能吗？我可以为每个时间步求解 LP 吗？SAC 方法在这里有效吗？非常感谢您的指导！    提交人    /u/Tako_Poke   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4s3rt/optimization_within_rl/</guid>
      <pubDate>Sun, 19 Jan 2025 06:15:06 GMT</pubDate>
    </item>
    <item>
      <title>不清楚何时使用 RL 还是数学优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4q30v/confused_about_when_to_use_rl_vs_mathematical/</link>
      <description><![CDATA[你好 我是 RL 的新手。 我们优化的问题是库存管理和车间作业调度。 我理解 RL 可以考虑更多动态方面，并且可以在未来进行调整。但我无法将其转化为实际术语 MO 技术何时会失效？ 建模时，如何在 MO 技术和 RL 之间做出选择？ 谢谢。    提交人    /u/20231027   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4q30v/confused_about_when_to_use_rl_vs_mathematical/</guid>
      <pubDate>Sun, 19 Jan 2025 04:18:45 GMT</pubDate>
    </item>
    <item>
      <title>聊天机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4ozq2/chatbots/</link>
      <description><![CDATA[您好， 在伯克利 cs285 关于强化学习的第一讲中，展示了一张聊天机器人的图片，作为强化学习功能的示例。我需要学习哪些主题才能构建遵循自定义规则的自定义聊天机器人？    提交人    /u/throwaway-alphabet-1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4ozq2/chatbots/</guid>
      <pubDate>Sun, 19 Jan 2025 03:23:33 GMT</pubDate>
    </item>
    <item>
      <title>图式网络：利用直觉物理的生成因果模型进行零样本迁移</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4m0xh/schema_networks_zeroshot_transfer_with_a/</link>
      <description><![CDATA[  由    /u/moschles  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4m0xh/schema_networks_zeroshot_transfer_with_a/</guid>
      <pubDate>Sun, 19 Jan 2025 00:46:48 GMT</pubDate>
    </item>
    <item>
      <title>RL 代理根本没有学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i4ilo3/rl_agent_not_learning_at_all/</link>
      <description><![CDATA[嗨，我是 RL 的新手，只是想让我的第一个代理运行。但是，似乎我的代理什么都没学到，我真的遇到了瓶颈，不知道该怎么做。 我为高尔夫纸牌游戏编写了一个简单的脚本，玩家可以与计算机对战。我制作了一些算法计算机玩家，但我真正想做的是教 RL 代理玩游戏。 即使对抗弱计算机玩家，代理在 5M 步内也学不到任何东西。所以我认为它有初始困难，因为它甚至无法在对抗弱玩家时获得足够的奖励。 所以我添加了一个完全随机的玩家，但即使对抗那个玩家，我的代理也根本没有学到任何东西。 好吧，我认为高尔夫对于 RL 来说可能有点难，因为它有两个不同的阶段：首先，你选一张牌，其次，你打出这张牌。我重构了代码，因此代理只需处理打牌，无需处理其他任何事情。但是，经过 5M 步后，代理仍然比一个非常简单的算法更愚蠢。 我尝试过 DQN 和 PPO，两者似乎都什么都没学到。 有人能指点我，我做错了什么吗？我认为我的奖励可能有问题，或者我不知道，我是一个初学者。 如果您有时间，单阶段 RL 代理的 repo 是 https://github.com/SakuOrdrTab/golf_card_game/tree/one-phase-RL 如果您想查看代理完成两个阶段的先前尝试，它是主分支。 谢谢大家！    提交人    /u/Some_Marionberry_403   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i4ilo3/rl_agent_not_learning_at_all/</guid>
      <pubDate>Sat, 18 Jan 2025 22:00:51 GMT</pubDate>
    </item>
    </channel>
</rss>