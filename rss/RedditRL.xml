<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯æ—¨åœ¨æ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå’Œå­¦ä¹ å¦‚ä½•æœ€ä½³è·å¾—å¥–åŠ±çš„AI/ç»Ÿè®¡æ•°æ®çš„å­åœºã€‚ä¾‹å¦‚Alphagoï¼Œä¸´åºŠè¯•éªŒå’ŒA/Bæµ‹è¯•ä»¥åŠAtariæ¸¸æˆã€‚</description>
    <lastBuildDate>Sun, 02 Mar 2025 09:16:52 GMT</lastBuildDate>
    <item>
      <title>ä½¿ç”¨DQNå¸®åŠ©è§£å†³å±±è½¦é—®é¢˜ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</link>
      <description><![CDATA[       &lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼Œ åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æƒ³é“æ­‰ï¼Œé—®é—®è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºæˆ‘çŒœæƒ³è¿™ä¸ªé—®é¢˜å¯èƒ½å·²ç»è¢«é—®åˆ°å¾ˆå¤šæ¬¡äº†ã€‚æˆ‘è¯•å›¾æ•™è‡ªå·±å¢å¼ºå­¦ä¹ ï¼Œå¹¶ä¸”æ­£åœ¨ç ”ç©¶è¿™æ¬¾MountrainCarè¿·ä½ é¡¹ç›®ã€‚  æˆ‘çš„æ¨¡å‹ä¼¼ä¹æ ¹æœ¬æ²¡æœ‰èåˆã€‚æˆ‘ä½¿ç”¨æƒ…èŠ‚æŒç»­æ—¶é—´ä¸æƒ…èŠ‚ç¼–å·çš„æƒ…èŠ‚æ¥æ£€æŸ¥/åˆ†ææ€§èƒ½ã€‚æˆ‘æ³¨æ„åˆ°çš„æ˜¯ï¼Œæœ‰æ—¶ï¼Œå¯¹äºæˆ‘å°è¯•è¿‡çš„æ‰€æœ‰æ¶æ„ï¼Œæƒ…èŠ‚æŒç»­æ—¶é—´éƒ½ä¼šæœ‰æ‰€å‡å°‘ï¼Œç„¶åå†æ¬¡å¢åŠ ã€‚  I have tried doing the following things:  Changing the architecture of the Fully Connected Neural network. Changing the learning rate Changing the epsilon value, and the epsilon decay values.  For neither of these changes, I got a model that seems to converge during training.æˆ‘å¹³å‡åŸ¹è®­äº†1500ä¸ªæŒç»­æ—¶é—´ã€‚è¿™å°±æ˜¯æ¯ä¸ªæ¨¡å‹é€šå¸¸çœ‹èµ·æ¥çš„å›¾ï¼š   æœ‰æ²¡æœ‰é€‚åˆæ­¤ç‰¹å®šé—®é¢˜çš„æŠ€å·§ï¼Œç‰¹å®šçš„DQNä½“ç³»ç»“æ„å’Œè¶…å‚æ•°èŒƒå›´ï¼Ÿè¿˜æœ‰ä¸€ç»„æŒ‡å—ï¼Œåº”è¯¥ç‰¢è®°å¹¶ç”¨æ¥åˆ›å»ºè¿™äº›DQNæ¨¡å‹ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/u/lowkeysuicidal14     [link]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1ifd8/help_with_the_mountain_car_problem_using_dqn/</guid>
      <pubDate>Sun, 02 Mar 2025 04:17:05 GMT</pubDate>
    </item>
    <item>
      <title>å¸®åŠ©2Då³°æœç´¢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j1fdsa/help_with_2d_peak_search/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘ä½¿ç”¨ä¸åŒçš„ä½“è‚²é¦†ç¯å¢ƒæœ‰å¾ˆå¤šRLç»éªŒï¼Œä½¿ç”¨SB3ï¼ŒCleanRlä»¥åŠæˆ‘è‡ªå·±å®æ–½çš„ç®—æ³•ï¼Œè·å¾—äº†ç›¸å½“ä¸é”™çš„æ€§èƒ½ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘å¯¹ä¸€ä¸ªäº‹å®æ„Ÿåˆ°æ¼ç«çš„æ˜¯ï¼Œæˆ‘ä¼¼ä¹æ— æ³•åœ¨ç©å…·é—®é¢˜ä¸Šå–å¾—ä»»ä½•è¿›å±•ï¼Œæˆ‘å·²ç»ä¸ºæˆ‘çš„å·¥ç¨‹é¢†åŸŸä¸­çš„æŸäº›ä¼˜åŒ–ä»»åŠ¡å®ç°äº†RLã€‚ è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œåœ¨è¯¥é—®é¢˜ä¸­ï¼Œè¯¥ä»£ç†çš„ä»»åŠ¡æ˜¯åœ¨2Dç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³çš„å¯åŠ¨å‚æ•°ï¼ˆå¯¹äºå¯åŠ¨å™¨ä¸­çš„æœ€ä½³å‚æ•°ï¼‰ï¼Œä»¥ä¾¿ä»¥å¯åŠ¨ä»¥è¿›è¡Œå¯åŠ¨ï¼Œä»¥ä¾›æŸäº›å‚æ•°ï¼Œä»¥ä¾›ä¸€äº›å‚æ•°ï¼Œä»¥ä¾›ä¸€äº›å‚æ•°ï¼Œä»¥ä¾›æŸäº›å‚æ•°ï¼Œä»¥ä¾¿ä»¥7ä¸ªè®¿é—®é‡ã€‚åœ¨æ‰€ä½¿ç”¨çš„ä¸€ç»„å‚æ•°ä¸Šçš„åˆ†å¸ƒæ˜¯å€¼çš„ï¼Œæœ‰äº›ä¸è¿ç»­æ€§ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘åˆ¶ä½œäº†ä¸€ä¸ªç©å…·ç¯å¢ƒçš„åŸå› ï¼Œåœ¨æ¯ä¸ªæƒ…èŠ‚ä¸­ï¼Œéƒ½ä¼šäº§ç”Ÿæµ‹é‡å€¼çš„é«˜æ–¯åˆ†å¸ƒï¼Œå…·æœ‰ä¸åŒçš„å‡å€¼å’Œåæ–¹å·®ã€‚è¯¥ä»£ç†çš„ä»»åŠ¡æ˜¯é€‰æ‹©ä¸€ç»„å€¼ï¼ŒèŒƒå›´ä»0-36ï¼Œä½¿ç”¨CNNç­–ç•¥ä½¿SB3å®ç°æ›´ç®€å•ï¼Œç„¶åä»¥è¯¥é›†çš„å‚æ•°é›†ä»¥åˆ†å¸ƒå€¼çš„å½¢å¼æ¥æ”¶åé¦ˆã€‚çŠ¶æ€ç©ºé—´æ˜¯æµ‹é‡å€¼çš„2Då›¾åƒï¼Œæ‰€æœ‰åˆå§‹å€¼å‡è®¾ç½®ä¸º0ï¼Œéšç€ä»£ç†æ¢ç´¢çš„å¡«å……ã€‚æˆ‘æ­£åœ¨ä½¿ç”¨çš„åŠ¨ä½œç©ºé—´æ˜¯ä¸€ä¸ªå¤šdiscreteç©ºé—´ï¼Œ[0-36ï¼Œ0-36ï¼Œ0-1]ï¼Œæœ€åä¸€ä¸ªåŠ¨ä½œæ˜¯ä»£ç†æ˜¯å¦è®¤ä¸ºè¿™å¥—å‚æ•°æ˜¯æœ€ä½³çš„ã€‚æˆ‘å°è¯•ä½¿ç”¨PPOå’ŒA2Cï¼Œè€Œæ€§èƒ½å·®å¼‚å¾ˆå°ã€‚ ç°åœ¨ï¼Œé—®é¢˜æ˜¯å–å†³äºæˆ‘å¦‚ä½•æ„é€ å¥–åŠ±çš„æ–¹å¼ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°æœ€ä½³çš„å‚æ•°é›†ã€‚æä¾›1ä¸ªä»¥æ‰¾åˆ°æ­£ç¡®å‚æ•°çš„åé¦ˆçš„å¤©çœŸæ–¹æ³•é€šå¸¸ä¼šå¤±è´¥ï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨æ­¤ç¯å¢ƒä¸­çš„éšæœºç­–ç•¥çš„ç›¸å½“ç¨€ç–çš„å¥–åŠ±æ¥è§£é‡Šã€‚å› æ­¤ï¼Œæˆ‘è¯•å›¾ä¸ºæ¯é¡¹åŠ¨ä½œæä¾›å¢é‡å¥–åŠ±ï¼Œè¯¥åŠ¨ä½œä¼šæ ¹æ®ä¸Šæ¬¡åŠ¨ä½œè¿›è¡Œæ”¹è¿›ï¼Œè¿™å–å†³äºåˆ†å¸ƒçš„å€¼æˆ–è·ç¦»åˆ°æœ€ä½³çš„è·ç¦»ï¼Œå¦‚æœå®é™…ä¸Šæ‰¾åˆ°äº†å³°å€¼ï¼Œåˆ™å…·æœ‰å¾ˆå¤§çš„å¥–åŠ±ã€‚è¿™æœ‰ç‚¹å¥½ï¼Œä½†æ˜¯ä»£ç†å•†æ€»æ˜¯ä¸ºäº†ä¸€é¡¹æ”¿ç­–è€Œå®šï¼Œå³å®ƒåœ¨å±±ä¸Šä¸­é€”èµ°äº†ä¸€åŠï¼Œç„¶åå°±è§£å†³äº†ï¼Œå†ä¹Ÿæ²¡æœ‰æ‰¾åˆ°å®é™…çš„å³°å€¼ã€‚æˆ‘æ²¡æœ‰å¯¹è¿›è¡Œå¤§é‡æµ‹é‡çš„ä»»ä½•æƒ©ç½šï¼ˆç°åœ¨ï¼‰ï¼Œä»¥ä¾¿ä»£ç†å•†å¯ä»¥è¿›è¡Œè¯¦å°½çš„æœç´¢ï¼Œä½†è¿™æ°¸è¿œä¸ä¼šåšåˆ°è¿™ä¸€ç‚¹ã€‚  åœ¨æˆ‘å¦‚ä½•è®¾ç½®ç¯å¢ƒæˆ–ç»“æ„å¥–åŠ±çš„æ–¹å¼ä¸­ï¼Œæˆ‘æ˜¯å¦ç¼ºå°‘ä»€ä¹ˆï¼Ÿæˆ‘æ˜¯å¦å¯ä»¥ç ”ç©¶ç±»ä¼¼çš„é¡¹ç›®æˆ–çº¸å¼ ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1j1fdsa/help_with_with_2d_peak_search/â€&gt; [link]   ï¼†ï¼ƒ32;   [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j1fdsa/help_with_2d_peak_search/</guid>
      <pubDate>Sun, 02 Mar 2025 01:30:54 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•å°†RLä¸åˆšä½“æœºå™¨äººä¸æµä½“ç›¸äº’ä½œç”¨çš„åˆšæ€§æœºå™¨äººé›†æˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j17hl3/how_to_integrate_rl_with_rigid_body_robots/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘æƒ³ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ¥æ•™2-3ä¸ªé“¾æ¥æœºå™¨äººé±¼æ¸¸æ³³ã€‚æœºå™¨äººé±¼æ˜¯ä¸€ä¸ªä¸‰ç»´å›ºä½“ç‰©ä½“ï¼Œå®ƒå°†æ„Ÿè§‰åˆ°å„ä¸ªæ–¹é¢çš„æ°´åŠ›ã€‚ä»€ä¹ˆæ¨¡æ‹Ÿå™¨å°†æœ‰ç”¨ï¼Œä»¥ä¾¿æˆ‘å¯ä»¥å¯¹åˆšä½“æœºå™¨äººå’Œå‘¨å›´çš„æµä½“åŠ›ä¹‹é—´çš„ç›¸äº’ä½œç”¨è¿›è¡Œå»ºæ¨¡ï¼Ÿ  æˆ‘éœ€è¦å®ƒèƒ½å¤Ÿå°†RLé›†æˆåˆ°å…¶ä¸­ã€‚ä¸åŸºäºCFDçš„æ¨¡æ‹Ÿï¼ˆCOMSOLï¼ŒANSYSï¼ŒåŸºäºFEMç­‰ï¼‰ä¸åŒï¼Œå®ƒä¹Ÿåº”è¯¥å¾ˆå¿«å‘ˆç°ç‰©ç†å­¦ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/kingalvez     [link]   [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j17hl3/how_to_integrate_rl_with_rigid_body_robots/</guid>
      <pubDate>Sat, 01 Mar 2025 19:25:09 GMT</pubDate>
    </item>
    <item>
      <title>å¸®åŠ©äº¤æ˜“çš„Qå­¦ä¹ æ¨¡å‹ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j14faj/help_with_qlearning_model_for_trading/</link>
      <description><![CDATA[       &lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼Œ æˆ‘å·²ç»ä½¿ç”¨å¥èº«æˆ¿ç¯å¢ƒå®ç°äº†ä¸€ä¸ªQå­¦ä¹ çš„äº¤æ˜“æœºå™¨äººï¼Œä½†æˆ‘æ³¨æ„åˆ°ä¸€äº›å¥‡æ€ªçš„ï¼ˆè‡³å°‘å¯¹æˆ‘æ¥è¯´ï¼‰ç»“æœã€‚åœ¨è®­ç»ƒäº†1500é›†çš„Qæ¡Œå­ä¹‹åï¼Œç‰¹å®šè‚¡ç¥¨çš„å¸‚åœºå›æŠ¥ç‡ä¸º156ï¼…ï¼Œè€ŒæŠ•èµ„ç»„åˆæ”¶ç›Šï¼ˆç”±Q-tableç­–ç•¥ç”Ÿæˆï¼‰æ˜¯æé«˜çš„ 76,445.94ï¼…ï¼Œè¿™å¯¹æˆ‘æ¥è¯´ä¼¼ä¹æ˜¯ä¸ç°å®çš„ã€‚ Could this be a case of overfitting or another issue? When testing, the results are:  Market Return: 33.87% Portfolio Return: 31.61%  æˆ‘è¿˜æ‹¥æœ‰æ¯é›†æ€»å¥–åŠ±çš„æƒ…èŠ‚ï¼Œå¹¶åœ¨æƒ…èŠ‚ä¸­ç´¯ç§¯äº†å¥–åŠ±ï¼š å¦‚æœ‰å¿…è¦ï¼Œæˆ‘å¯ä»¥å…±äº«æˆ‘çš„ä»£ç ï¼Œä»¥ä¾¿æœ‰äººå¯ä»¥å¸®åŠ©æˆ‘è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è°¢è°¢ï¼  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/ligabo69     [link]       [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j14faj/help_with_qlearning_model_for_trading/</guid>
      <pubDate>Sat, 01 Mar 2025 17:14:34 GMT</pubDate>
    </item>
    <item>
      <title>LLMå¾®è°ƒçš„åˆ†å¸ƒå¼RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0ze7a/distributed_rl_for_llm_finetuning/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘ä¸€ç›´åœ¨ä¸ºä½¿ç”¨ ray  and  unsploth  è¿›è¡Œ  çš„ä»ç„¶æ˜¯ä¸€ä¸ªè¿›æ­¥çš„å·¥ä½œï¼Œä½†æ˜¯æˆ‘å¾ˆé«˜å…´   forng&gt; strong&gt; strong&gt; strong&gt; pros for fork it fors fors fors form formeï¼Œå¦‚æœæ‚¨æœ‰å…´è¶£ï¼Œè¯·æŸ¥çœ‹ï¼æäº¤ç”±ï¼†ï¼ƒ32; /u/sedidrl     [link]        [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0ze7a/distributed_rl_for_llm_finetuning/</guid>
      <pubDate>Sat, 01 Mar 2025 13:24:55 GMT</pubDate>
    </item>
    <item>
      <title>ç¦»çº¿RLç®—æ³•å¯¹10^-6çš„å¥–åŠ±æ•æ„Ÿï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0wyol/offline_rl_algorithm_sensitive_to_perturbations/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼Œæˆ‘æ­£åœ¨åœ¨D4RLåŸºå‡†æµ‹è¯•å°ä¸Šæ•°æ®é›†ï¼ˆç‰¹åˆ«æ˜¯Hopperé‡æ’­æ•°æ®â€‹â€‹é›†ï¼‰ä¸Šè¿è¡Œç¦»çº¿RLç®—æ³•ï¼ˆç‰¹åˆ«æ˜¯éšå¼Qå­¦ä¹ ï¼‰ã€‚æˆ‘çœ‹åˆ°ï¼Œå¥–åŠ±çš„å°æ‰°åŠ¨ï¼Œä»¥10^-6çš„é¡ºåºå¯¼è‡´è®­ç»ƒç»“æœæˆªç„¶ä¸åŒã€‚å½“ç„¶ï¼Œè¿™æ˜¯æ‰€æœ‰ä¸œè¥¿ä¸Šçš„å›ºå®šç§å­ã€‚  æˆ‘çŸ¥é“RLåœ¨è®¸å¤šæ–¹é¢ï¼ˆè¶…å‚æ•°ï¼Œæ¨¡å‹ä½“ç³»ç»“æ„ï¼Œå¥–åŠ±ç­‰ï¼‰ä¸­çš„å°æ‰°åŠ¨å¯èƒ½éå¸¸æ•æ„Ÿã€‚ä½†æ˜¯ï¼Œè¿™å¯¹æˆ‘çš„å¥–åŠ±çš„å˜åŒ–å¾ˆæ•æ„Ÿï¼Œè¿™ä½¿æˆ‘æ„Ÿåˆ°æƒŠè®¶ã€‚å¯¹äºé‚£äº›å…·æœ‰æ›´å¤šå®æ–½è¿™äº›ç®—æ³•çš„ç»éªŒçš„äººï¼Œæ‚¨è®¤ä¸ºè¿™æ˜¯æœŸæœ›çš„å—ï¼Ÿè¿˜æ˜¯ä¼šæš—ç¤ºç®—æ³•å®ç°å‡ºäº†é—®é¢˜ï¼Ÿ å¦‚æœæœ‰äº›é¢„æœŸï¼Œè¿™æ˜¯å¦ä¼šå¼•èµ·ç–‘é—®ï¼Œè¿™åœ¨ç¦»çº¿RLä¸­å·²å‘è¡¨çš„è®¸å¤šå·²å‘è¡¨çš„å·¥ä½œå—ï¼Ÿä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥ä¿®å¤ç§å­å’Œè¶…çº§å‚æ•°ï¼Œä½†æ˜¯éšååœ¨CUDAä¸CPUä¸Šè¿è¡Œå¥–åŠ±æ¨¡å‹å¯ä»¥å¯¼è‡´å¥–åŠ±å€¼çš„å·®å¼‚ä¸º10^-6    &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/used-eagle-9302     [link]    [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0wyol/offline_rl_algorithm_sensitive_to_perturbations/</guid>
      <pubDate>Sat, 01 Mar 2025 10:53:49 GMT</pubDate>
    </item>
    <item>
      <title>æé«˜æ ·å“æ•ˆç‡çš„æœ€æœ‰å¸Œæœ›çš„æŠ€æœ¯</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0rgu4/most_promising_techniques_to_improve_sample/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘çŸ¥é“çš„å‡ ä¸ªæ˜¯mbrlï¼Œæ¨¡ä»¿å­¦ä¹ ï¼ˆé€†RLï¼‰ã€‚è¿˜æœ‰å…¶ä»–è‰¯å¥½çš„ç ”ç©¶é¢†åŸŸé‡ç‚¹æ˜¯è§£å†³æ ·æœ¬æ•ˆç‡çš„æé«˜å—ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/aliaslight     [link]       [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0rgu4/most_promising_techniques_to_improve_sample/</guid>
      <pubDate>Sat, 01 Mar 2025 04:41:26 GMT</pubDate>
    </item>
    <item>
      <title>rllamağŸ¦™-æ•™å­¦è¯­è¨€æ¨¡å‹ï¼Œå¸¦æœ‰å†…å­˜çš„RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j0hgm1/rllama_teaching_language_models_with/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å˜¿ï¼Œå¤§å®¶ï¼Œ æˆ‘æƒ³åˆ†äº«ä¸€ä¸ªä¸LLMå¾®è°ƒå®éªŒä¸­å‡ºç°çš„é¡¹ç›®ã€‚åœ¨ä½¿ç”¨[ llamagym ]å¹¶é‡åˆ°äº†ä¸€äº›å†…å­˜ç®¡ç†æŒ‘æˆ˜åï¼Œæˆ‘å¼€å‘äº†   rllama      ï¼ˆ[[[[[[[ æ ¸å¿ƒæ€æƒ³æ˜¯æ”¹å–„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¦‚ä½•ä¿ç•™å’Œåˆ©ç”¨ç»éªŒã€‚å¯é…ç½®çš„å‹ç¼©ç­–ç•¥ å¿«é€Ÿå¯åŠ¨ğŸ˜¼ğŸ¦™  python3ï¼špipå®‰è£…rllama  æˆ‘ç‰¹åˆ«æ„Ÿå…´è¶£çš„æ˜¯å¬åˆ°æœ‰å…³ï¼š   - æ›¿ä»£è®°å¿†ä½“ç³»ç»“æ„     - æ½œåœ¨çš„      - æ½œåœ¨çš„  - æ½œåœ¨çš„  -  p&gt;  -  p&gt;  -  pervormation-performance optimions    pocitiond  sode  sodeed sode and soper and sope and andï¼ˆIndeï¼‰ã€‚è¯·éšæ—¶è´¡çŒ®æˆ–å»ºè®®æ”¹è¿› - æ¬¢è¿PRå’Œé—®é¢˜ï¼  [è¯„è®ºä¸­æœ‰å…´è¶£çš„äººçš„å®æ–½è¯¦ç»†ä¿¡æ¯]   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/cheenchann     [link]        [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j0hgm1/rllama_teaching_language_models_with/</guid>
      <pubDate>Fri, 28 Feb 2025 20:29:19 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚æœæˆ‘æœ‰ä¸€ä¸ªå·¨å¤§çš„æ•°æ®é›†ï¼Œæˆ‘åº”è¯¥é€‰æ‹©ä»€ä¹ˆé€‰æ‹©é‡æ’­ç¼“å†²åŒºï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_i_go_for_if_i/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨å®ç°è‡ªåŠ¨ç¼“å­˜å†…å­˜ç®¡ç†çš„RLæ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘çš„æ•°æ®é›†çš„ç¤ºä¾‹å¤„äºä»¥ä¸‹å½¢å¼ï¼ˆçŠ¶æ€ï¼Œæ“ä½œï¼Œå¥–åŠ±ï¼‰ã€‚æˆ‘çš„æ•°æ®é›†ç›¸å½“åºå¤§ï¼ˆæˆ‘ä»¬æ­£åœ¨è°ˆè®ºæ•°ä¸‡äº¿ä¸ªæ•°æ®ç¤ºä¾‹ï¼‰ã€‚ä»æˆ‘çš„æœªçŸ¥æ•°ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æ•°æ®é›†æ´—å‡€ï¼Œç„¶åå°†å…¶åŠ è½½åˆ°é‡æ’­ç¼“å†²åŒºï¼ˆè¿™æ˜¯æ•°æ®é›†å¤§å°åˆç†çš„æƒ…å†µï¼‰ã€‚  å¯¹äºæˆ‘çš„æƒ…å†µï¼Œæˆ‘æ­£åœ¨ä½¿ç”¨iterabledataSetå’Œpytorchçš„æ•°æ®åŠ è½½å™¨ï¼ˆhttps://pytorch.org/tutorials/beginner/basics/data\_tutorial.html) and basically it treats my data as a large stream of info so it&#39;s not loaded into memoryç«‹å³å¯¼è‡´å¼€é”€ã€‚æˆ‘çš„é—®é¢˜æ˜¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°é‡æ’­ç¼“å†²æ¶²ä¸­å¹¶ä¸å¯è¡Œï¼Œé‚£ä¹ˆè¿™é‡Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿè€Œä¸”æœ‰å¾ˆå¤šç±»å‹çš„é‡æ’­ç¼“å†²åŒºï¼Œæ‰€ä»¥å“ªä¸€ç§æ˜¯æˆ‘çš„æƒ…å†µæœ€é€‚åˆä½¿ç”¨çš„ï¼Ÿæäº¤ç”±ï¼†ï¼ƒ32; /u/u/saffarini9     [links]      &lt;a href =â€œ https://www.reddit.com/r/reinforevectionlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_should_should_go_go_for_if_if_if_i/â€]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_i_go_for_if_i/</guid>
      <pubDate>Fri, 28 Feb 2025 13:18:51 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•è®¡ç®—L_CLIPçš„æ¢¯åº¦ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j055j8/how_to_compute_the_gradient_of_l_clip/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼æˆ‘æœ€è¿‘é˜…è¯»äº†æœ‰å…³PPOçš„ä¿¡æ¯ï¼Œä½†æ˜¯æˆ‘è¿˜æ²¡æœ‰ç†è§£å¦‚ä½•å¾—å‡ºæ¢¯åº¦ï¼Œå› ä¸ºåœ¨ç®—æ³•ä¸­ï¼Œå‰ªè¾‘è¡Œä¸ºå–å†³äºR_Tï¼ˆThetaï¼‰ï¼ˆthetaï¼‰ï¼Œè¿™æ˜¯æœªçŸ¥çš„ã€‚æœ€å¥½çš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘å¬è¯´æŸç§è¿­ä»£å¯ä»¥å®æ–½ï¼Œä½†æˆ‘å°šæœªç†è§£ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/u/purplebumblebee5620     [link]    [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j055j8/how_to_compute_the_gradient_of_l_clip/</guid>
      <pubDate>Fri, 28 Feb 2025 11:02:00 GMT</pubDate>
    </item>
    <item>
      <title>PPOé‡ç½®æ¯ä¸ªæ—¶é—´æ­¥</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j00jqi/ppo_resets_every_timestep/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  ç¼–è¾‘ï¼šå·²è§£å†³ - é—®é¢˜æ˜¯ä»æˆ‘ç”¨æ¥ç”Ÿæˆè§‚æµ‹å€¼çš„è½¯ä»¶åŒ…ä¸­è¿”å›çš„æˆªæ–­å˜é‡ä¸­çš„é—®é¢˜ã€‚ åŸå§‹å¸–å­ï¼š æ˜¯ä»€ä¹ˆå¯ä»¥å®ç°è¿™ä¸€ç›®æ ‡ï¼Ÿæˆ‘æ˜¯RLçš„æ–°æ‰‹ï¼Œä½†æ˜¯æˆ‘å·²ç»åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸå·¥ä½œäº†å‡ å¹´ï¼Œæ‰€ä»¥æˆ‘å¸Œæœ›æˆ‘åªæ˜¯ç¼ºå°‘ç®€å•çš„ä¸œè¥¿ã€‚ æˆ‘æ­£åœ¨ä½¿ç”¨MultiiinputPolicyè¿è¡Œå•ä¸ªENVã€‚ä½¿ç”¨.learnï¼ˆï¼‰ï¼ŒEnvåœ¨å¼€å§‹æ—¶é‡ç½®ï¼Œä¸€æ­¥ï¼Œå†æ¬¡é‡ç½®ï¼Œç„¶åç»§ç»­æ­¤å‘¨æœŸï¼Œç›´åˆ°å®Œæˆæ—¶é—´æ®µã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1j00jqi/ppo_resets_every_timestep/â€&gt; [link]    32;   [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j00jqi/ppo_resets_every_timestep/</guid>
      <pubDate>Fri, 28 Feb 2025 05:32:22 GMT</pubDate>
    </item>
    <item>
      <title>ä»RL Newbieåˆ°é‡æ–°å®ç°PPOï¼šæˆ‘çš„å­¦ä¹ å†’é™©</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izv5zs/from_rl_newbie_to_reimplementing_ppo_my_learning/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼æˆ‘æ˜¯ä¸€åCSå­¦ç”Ÿï¼Œå¤§çº¦ä¸€å¹´å‰å¼€å§‹æ½œå…¥MLå’ŒDLã€‚ç›´åˆ°æœ€è¿‘ï¼ŒRLæ‰æ˜¯æˆ‘è¿˜æ²¡æœ‰æ¢ç´¢å¤ªå¤šçš„ä¸œè¥¿ã€‚æˆ‘å”¯ä¸€çš„ç»éªŒæ˜¯åœ¨æ‹¥æŠ±Faceçš„TRLå®ç°ä¸­ï¼Œä»¥å°†RLåº”ç”¨äºLLMï¼Œä½†è€å®è¯´ï¼Œæˆ‘å½“æ—¶ä¸çŸ¥é“è‡ªå·±åœ¨åšä»€ä¹ˆã€‚ å¾ˆé•¿ä¸€æ®µæ—¶é—´ä»¥æ¥ï¼Œæˆ‘è®¤ä¸ºRLä¸€ç›´åœ¨æå“ - å°±åƒè¿™æ˜¯æ·±åº¦å­¦ä¹ çš„æœ€ç»ˆé«˜å³°ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œæ‰€æœ‰æœ€é…·çš„çªç ´ï¼Œä¾‹å¦‚Alphagoï¼ŒAlphazeroå’ŒRoboticsï¼Œä¼¼ä¹éƒ½ä¸RLæ¯æ¯ç›¸å…³ï¼Œè¿™ä½¿å®ƒæ„Ÿåˆ°é¥ä¸å¯åŠã€‚ä½†æ˜¯éšåDeepSeekå‘è¡Œäº†GRPOï¼Œæˆ‘çœŸçš„å¾ˆæƒ³äº†è§£å®ƒå¦‚ä½•è¿ä½œå’Œè·Ÿéšè®ºæ–‡ã€‚è¿™å¼•å‘äº†ä¸€ä¸ªæƒ³æ³•ï¼šä¸¤å‘¨å‰ï¼Œæˆ‘å†³å®šé€šè¿‡é‡æ–°è¿›åŒ–ä¸€äº›æ ¸å¿ƒRLç®—æ³•æ¥å¯åŠ¨ä¸€ä¸ªé¡¹ç›®ï¼Œä»¥ä»å¤´å¼€å§‹å»ºç«‹æˆ‘çš„RLçŸ¥è¯†ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘å·²ç»è§£å†³äº†ä¸€äº›ã€‚æˆ‘ä»DQNå¼€å§‹ï¼Œè¿™æ˜¯æˆ‘è¿„ä»Šä¸ºæ­¢é‡æ–°å®ç°çš„å”¯ä¸€åŸºäºä»·å€¼çš„æ–¹æ³•ã€‚ç„¶åï¼Œæˆ‘ç»§ç»­è¿›è¡Œç­–ç•¥æ¢¯åº¦æ–¹æ³•ã€‚æˆ‘çš„ç¬¬ä¸€ä¸ªå°è¯•æ˜¯ä½¿ç”¨å¥–åŠ±å‰è¿›çš„åŸºæœ¬å¢å¼ºç®—æ³•çš„é¦™è‰æ”¿ç­–æ¢¯åº¦ã€‚æˆ‘è¿˜ä¸ºæ­¤æ·»åŠ äº†è¯„è®ºå®¶ï¼Œå› ä¸ºæˆ‘çœ‹åˆ°ä¸¤ç§æ–¹æ³•éƒ½æ˜¯å¯èƒ½çš„ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘æ¥å—äº†TRPOï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€éš¾å®æ–½çš„ã€‚ä½†æ˜¯ï¼Œé€šè¿‡å®ƒä¸ºæˆ‘å¸¦æ¥äº†ä¸€ä¸ªçœŸæ­£çš„â€œå°¤é‡Œå¡â€æ—¶åˆ» - æˆ‘ç»ˆäºæŒæ¡äº†ç›‘ç£å­¦ä¹ ä¸RLçš„ä¼˜åŒ–ä¹‹é—´çš„æ ¹æœ¬å·®å¼‚ã€‚å³ä½¿ç”±äºäºŒé˜¶æ–¹æ³•çš„æˆæœ¬ï¼ŒTRPOä¸å†è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†æˆ‘å¼ºçƒˆå»ºè®®å°†å…¶é‡æ–°å®ç°ä¸ºä»»ä½•å­¦ä¹ RLçš„äººã€‚è¿™æ˜¯æ„å»ºç›´è§‰çš„å¥½æ–¹æ³•ã€‚ ç°åœ¨ï¼Œæˆ‘åˆšåˆšå®Œæˆäº†PPOçš„é‡æ–°è¿›åŒ–ï¼Œè¿™æ˜¯é‚£é‡Œæœ€å—æ¬¢è¿çš„ç®—æ³•ä¹‹ä¸€ã€‚æˆ‘é€‰æ‹©äº†å‰ªè¾‘ç‰ˆæœ¬ï¼Œå°½ç®¡åœ¨TRPOä¹‹åï¼ŒKL-Divergenceç‰ˆæœ¬å¯¹æˆ‘æ¥è¯´æ›´ä¸ºç›´è§‚ã€‚æˆ‘ä¸€ç›´åœ¨ç®€å•çš„æ§åˆ¶ç¯å¢ƒä¸Šæµ‹è¯•è¿™äº›ç®—æ³•ã€‚æˆ‘çŸ¥é“æˆ‘å¯èƒ½åº”è¯¥å°è¯•ä¸€äº›æ›´å¤æ‚çš„äº‹æƒ…ï¼Œä½†æ˜¯è¿™äº›å€¾å‘äºè®­ç»ƒã€‚ è€å®è¯´ï¼Œè¿™ä¸ªé¡¹ç›®ä½¿æˆ‘æ„è¯†åˆ°RLç”šè‡³æœ‰æ•ˆã€‚ä»¥ä¹’ä¹“çƒä¸ºä¾‹ï¼šåœ¨åŸ¹è®­çš„æ—©æœŸï¼Œæ‚¨çš„æ”¿ç­–å¾ˆç³Ÿç³•ï¼Œæ¯æ¬¡éƒ½ä¼šå¤±å»ã€‚å®ƒéœ€è¦20ä¸ªæ­¥éª¤ï¼ˆ4å¸§è·³è¿‡ï¼‰ï¼Œåªæ˜¯å°†çƒä»ä¸€ä¾§è½¬åˆ°å¦ä¸€ä¾§ã€‚åœ¨è¿™20ä¸ªæ­¥éª¤ä¸­ï¼Œæ‚¨å°†è·å¾—19ä¸ªé›¶ï¼Œä¹Ÿè®¸æ˜¯+1æˆ–-1å¥–åŠ±ã€‚è¿™ç§ç¨€ç–æ€§æ˜¯ç–¯ç‹‚çš„ï¼Œä»¤äººéœ‡æƒŠçš„æ˜¯ï¼Œå®ƒæœ€ç»ˆä¼šå¼„æ¸…æ¥šäº‹æƒ…ã€‚ æ¥ä¸‹æ¥ï¼Œæˆ‘æ‰“ç®—åœ¨å°†é‡ç‚¹è½¬ç§»åˆ°è¿ç»­çš„åŠ¨ä½œç©ºé—´ä¹‹å‰å®æ–½GRPOï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘åªä¸ç¦»æ•£çš„åŠ¨ä½œåˆä½œï¼Œæ‰€ä»¥æˆ‘å¾ˆé«˜å…´èƒ½æ¢ç´¢è¿™ä¸€ç‚¹ã€‚æˆ‘è¿˜åšæŒåŸºæœ¬çš„MLPå’ŒConvnetsçš„æ”¿ç­–å’Œä»·å€¼åŠŸèƒ½ï¼Œä½†æ˜¯æˆ‘æ­£åœ¨è€ƒè™‘å°è¯•ä¸ºè¿ç»­åŠ¨ä½œç©ºé—´è¿›è¡Œæ‰©æ•£æ¨¡å‹ã€‚ä»–ä»¬çœ‹èµ·æ¥å¾ˆè‡ªç„¶ã€‚å±•æœ›æœªæ¥ï¼Œæˆ‘å¾ˆæƒ³åœ¨æˆ‘å¾ˆå¿«å®Œæˆä¸Šå­¦åå°è¯•ä¸€äº›æœºå™¨äººé¡¹ç›®ï¼Œå¹¶ä¸ºè¿™æ ·çš„é™„å¸¦é¡¹ç›®æä¾›æ›´å¤šç©ºé—²æ—¶é—´ã€‚ æˆ‘çš„å¤§å¤–å–ï¼Ÿ RLå¹¶ä¸åƒæˆ‘æƒ³è±¡çš„é‚£ä¹ˆææ€–ã€‚å¤§å¤šæ•°ä¸»è¦çš„ç®—æ³•å¯ä»¥å¾ˆå¿«åœ°åœ¨å•ä¸ªæ–‡ä»¶ä¸­é‡æ–°å®Œæˆã€‚å°±æ˜¯è¯´ï¼ŒåŸ¹è®­æ˜¯ä¸€ä¸ªå®Œå…¨ä¸åŒçš„æ•…äº‹ - ç”±äºRLé“²çƒçš„æ€§è´¨ï¼Œå®ƒå¯èƒ½ä»¤äººæ²®ä¸§å’Œæå“ã€‚å¯¹äºè¿™ä¸ªé¡¹ç›®ï¼Œæˆ‘ä¾é Openaiçš„æ—‹è½¬æŒ‡å—å’Œæ¯ç§ç®—æ³•çš„åŸå§‹è®ºæ–‡ï¼Œè¿™éå¸¸æœ‰å¸®åŠ©ã€‚å¦‚æœæ‚¨å¾ˆå¥½å¥‡ï¼Œæˆ‘ä¸€ç›´åœ¨ç”¨å›è´­å·¥ä½œï¼Œç§°ä¸ºâ€œ rl-arenaâ€ã€‚å»ï¼  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/u/awkward-can-8933      [link]   [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izv5zs/from_rl_newbie_to_reimplementing_ppo_my_learning/</guid>
      <pubDate>Fri, 28 Feb 2025 00:38:30 GMT</pubDate>
    </item>
    <item>
      <title>â€œåŸ¹è®­è¯­è¨€æ¨¡å‹ï¼Œç”¨äºé€šè¿‡å¤šæœºæ„å¢å¼ºå­¦ä¹ å­¦ä¹ â€ï¼ŒSarkarç­‰2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</link>
      <description><![CDATA[ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/gwern       [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</guid>
      <pubDate>Thu, 27 Feb 2025 16:59:12 GMT</pubDate>
    </item>
    <item>
      <title>å›½é™…è±¡æ£‹æ ·å“æ•ˆç‡äººä¸sota rl</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</link>
      <description><![CDATA[From what I know, SOTA chess RL like AlphaZero reached GM level after training on many more games than a human GM played throughout their lives before becoming GM Even if u include solved puzzles, incomplete games, and everything in between, humans reached GM with much lesser games than SOTA RL did (pls correct me if I&#39;m wrong aboutè¿™ï¼‰ã€‚ æ˜¯å¦æœ‰æ¯”äººç±»æ•ˆç‡è¾ƒä½çš„ç‰¹å®šåŸå› /éšœç¢ï¼Ÿå¯¹äºæé«˜å›½é™…è±¡æ£‹SOTA RLæ ·æœ¬æ•ˆç‡çš„ç ”ç©¶æ˜¯å¦æœ‰å¸Œæœ›ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/aliaslight     [link]       [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:40:39 GMT</pubDate>
    </item>
    <item>
      <title>ç¦»çº¿RLçš„åŠ¨ä½œå°†æ˜¯ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æ‰€ä»¥ï¼Œæˆ‘æ˜¯RLçš„æ–°æ‰‹ï¼Œæˆ‘å¿…é¡»å®ç°ç¦»çº¿RLæ¨¡å‹ï¼Œç„¶ååœ¨åœ¨çº¿RLé˜¶æ®µä¸­å¾®è°ƒå®ƒã€‚ä»æˆ‘çš„æ‰¿è¯ºä¸­ï¼Œç¦»çº¿å­¦ä¹ é˜¶æ®µæœ€åˆçš„ç­–ç•¥å’Œåœ¨çº¿å­¦ä¹ é˜¶æ®µå°†ä½¿ç”¨å®æ—¶åé¦ˆæ¥å®Œå–„æ”¿ç­–ã€‚å¯¹äºç¦»çº¿å­¦ä¹ é˜¶æ®µï¼Œæˆ‘å°†æœ‰ä¸€ä¸ªæ•°æ®é›†d = {ï¼ˆsiï¼Œaiï¼Œriï¼‰}ã€‚æ•°æ®é›†ä¸­æ¯ä¸ªç¤ºä¾‹çš„æ“ä½œæ˜¯å¦æ˜¯æ”¶é›†æ•°æ®æ—¶é‡‡å–çš„åŠ¨ä½œï¼ˆå³ä¸“å®¶è¡ŒåŠ¨ï¼‰ï¼Ÿè¿˜æ˜¯æ‰€æœ‰å¯èƒ½çš„åŠ¨ä½œï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/saffarini9     [link]    ï¼†ï¼ƒ32;   [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:03:21 GMT</pubDate>
    </item>
    </channel>
</rss>