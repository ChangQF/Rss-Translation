<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 01 Jan 2024 15:13:23 GMT</lastBuildDate>
    <item>
      <title>哪个 OpenAI Gym 版本最好/最常用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vtoyi/which_openai_gym_version_is_bestmost_used/</link>
      <description><![CDATA[大家好， 我最近开始研究健身平台，更具体地说是 BipedalWalker。我最初使用的是最新版本（现在称为 Gymnasium，而不是 Gym），但是 99% 的在线教程和代码都使用旧版本的 Gym。 由于我正在从事的项目非常复杂，并且没有以前在这个环境中完成过，我需要尽可能多地从其他人那里获得工作代码。我看到版本21到26有变化，Gymnasium现在也有差异。 我可以看到很多3年前的教程、视频和代码。但在过去几年里，它似乎已经失去了吸引力。 所以我的问题是，哪个版本的库最适合我工作，以便拥有实际工作的代码？    由   提交 /u/DocMenios   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vtoyi/which_openai_gym_version_is_bestmost_used/</guid>
      <pubDate>Mon, 01 Jan 2024 09:59:21 GMT</pubDate>
    </item>
    <item>
      <title>PPO 与地方政策的融合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vsfzg/ppo_convergence_to_local_policy/</link>
      <description><![CDATA[      我正在使用 PPO 算法，我的算法在训练期间获得的最大奖励为 212，但经过几次（80-100 epi）后它收敛到 176，就像我尝试降低学习率和其他修补一样超级参数，但仍然没有用。 任何帮助表示赞赏。 （下面是训练图。）提前致谢！！！！ https://preview.redd.it/5buwjkzqgs9c1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=6687d40641ac6747acdf42a27b6f2e80 65532c97   由   提交 /u/Wide-Chef-7011   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vsfzg/ppo_convergence_to_local_policy/</guid>
      <pubDate>Mon, 01 Jan 2024 08:25:24 GMT</pubDate>
    </item>
    <item>
      <title>Connect-4 - Q-Learning 与 Actor-Critic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vd577/connect4_qlearning_vs_actorcritic/</link>
      <description><![CDATA[我实现了 Connect-4 的两个版本，一个基于 Q-Learning，另一个基于 REINFORCE（Actor-Critic 方法）。手动调整学习参数后，很容易让 Actor-Critic 版本达到合理的学习进度。然而，我在 Q-Learning 版本上没有取得成功。对于为什么 REINFORCE 更适合这个问题，有什么理由/解释吗？   由   提交 /u/m_jochim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vd577/connect4_qlearning_vs_actorcritic/</guid>
      <pubDate>Sun, 31 Dec 2023 17:56:07 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v8twl/advices_for_reinforcement_learning/</link>
      <description><![CDATA[我想深入了解什么是向量化环境，请给我一些书籍或视频。   由   提交 /u/BryanDeveloper   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v8twl/advices_for_reinforcement_learning/</guid>
      <pubDate>Sun, 31 Dec 2023 14:27:08 GMT</pubDate>
    </item>
    <item>
      <title>网格世界中的 Q 学习 - 贝尔曼方程可视化 [评论中的链接] :)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v6jwi/q_learning_on_a_grid_world_bellman_equation/</link>
      <description><![CDATA[       由   提交/u/prajwalsouza  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v6jwi/q_learning_on_a_grid_world_bellman_equation/</guid>
      <pubDate>Sun, 31 Dec 2023 12:12:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 编写自定义矢量化健身房环境的约定？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v50ai/conventions_to_write_a_custom_vectorized_gym/</link>
      <description><![CDATA[在torchrl中，您只需将一批操作传递给step函数，然后让pytorch处理矢量化。来源：https://pytorch.org/rl/tutorials/pendulum.html#batching-computations .不过，我想使用大多数与 Gym 兼容的其他 RL 库。 在gym 中，您可以使用 vector.make() 或AsyncVectorEnv。如果您的环境实现只是 Pytorch，这不是大材小用吗？有开源的例子吗？或者也许是健身房的替代品？ 注意：我只是 RL 新手几天。任何建议都会有帮助   由   提交/u/hunterh0  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v50ai/conventions_to_write_a_custom_vectorized_gym/</guid>
      <pubDate>Sun, 31 Dec 2023 10:23:40 GMT</pubDate>
    </item>
    <item>
      <title>《利用部分动力学知识进行高效强化学习的样本》2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</link>
      <description><![CDATA[ 由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</guid>
      <pubDate>Sat, 30 Dec 2023 22:12:11 GMT</pubDate>
    </item>
    <item>
      <title>环境生成器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</link>
      <description><![CDATA[嘿，RL 爱好者， 我想知道，当我们做 RL 实验时，你们是否都被开发 RL 所需的开销所困扰？预先环境。我发现这非常烦人，因为我总是需要构建适合我的用例的东西。  据我们所知，我们只有 Farma 基金会（https://farama.org/）提供的十几个高质量环境。 org/)  欢迎任何想法！    由   提交 /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</guid>
      <pubDate>Sat, 30 Dec 2023 18:59:28 GMT</pubDate>
    </item>
    <item>
      <title>多目标场景的最佳强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</link>
      <description><![CDATA[你好， 我正在尝试训练室内无人机代理离开房间。无人机必须逃离日益严重的火势并到达 4 个出口中的任何一个。 我尝试过 DQN、A2C、PPO。这些算法的问题在于，一旦智能体学会了出口门，它总是尝试从那里退出，而其他门则未被探索。 我想知道哪种 RL 算法最适合这种情况，当更多没有一个进球。 谢谢！   由   提交/u/shahmirkhan21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</guid>
      <pubDate>Sat, 30 Dec 2023 16:24:36 GMT</pubDate>
    </item>
    <item>
      <title>Pangu-Agent：具有结构化推理的可微调多面手智能体</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18u2bym/panguagent_a_finetunable_generalist_agent_with/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.14878 摘要：  创建人工智能（AI）代理的关键方法是强化学习（RL）。然而，构建一个将感知直接映射到行动的独立强化学习策略会遇到严重的问题，其中最主要的是它缺乏跨多个任务的通用性以及需要大量的训练数据。主要原因是在制定政策时无法有效地将先验信息融入到感知-行动循环中。大型语言模型（LLM）作为将跨领域知识融入人工智能代理的基本方式而出现，但缺乏针对特定决策问题的关键学习和适应。本文提出了一个将结构化推理集成和学习到人工智能代理策略中的通用框架模型。我们的方法论受到人脑模块化的启发。该框架利用内在和外在函数的构造来添加先前对推理结构的理解。它还提供了学习每个模块或功能内部模型的自适应能力，与认知过程的模块化结构一致。我们深入描述了该框架，并将其与其他人工智能管道和现有框架进行了比较。本文探讨了实际应用，包括证明我们方法有效性的实验。我们的结果表明，当嵌入有组织的推理和先验知识时，人工智能代理的表现和适应能力要好得多。这为更具弹性和通用的人工智能代理系统打开了大门。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18u2bym/panguagent_a_finetunable_generalist_agent_with/</guid>
      <pubDate>Sat, 30 Dec 2023 00:41:00 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的哈密顿路径/循环</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18tvfgf/hamiltonian_pathcycle_with_rl/</link>
      <description><![CDATA[是否可以使用 PPO 和 5x5 矩阵中的 4 种可能的移动方式，通过常规方法求解哈密顿路径/循环？ &lt; /div&gt;  由   提交/u/marques576  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18tvfgf/hamiltonian_pathcycle_with_rl/</guid>
      <pubDate>Fri, 29 Dec 2023 19:39:05 GMT</pubDate>
    </item>
    <item>
      <title>感谢帮助 - 用于温度控制的 DQL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18tslxw/help_appreciated_dql_for_temperature_control/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18tslxw/help_appreciated_dql_for_temperature_control/</guid>
      <pubDate>Fri, 29 Dec 2023 17:37:55 GMT</pubDate>
    </item>
    <item>
      <title>平均奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18trd23/average_reward/</link>
      <description><![CDATA[       大家好，我是 DRL 新手。当我使用Carla自动驾驶车辆模拟器训练DQN模型时，我发现平均奖励曲线是这样的（有波动），解决方案是什么？ ​ https://preview.redd.it/knu4kkqzi99c1.png ?width=583&amp;format=png&amp;auto=webp&amp;s=b516976d2b65d61d610a0726301f23262962fa48   由   提交 /u/Chetioui_PHD   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18trd23/average_reward/</guid>
      <pubDate>Fri, 29 Dec 2023 16:43:49 GMT</pubDate>
    </item>
    <item>
      <title>Boid环境下未实现植绒</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18t82zx/not_achieveing_flocking_in_boid_environment/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18t82zx/not_achieveing_flocking_in_boid_environment/</guid>
      <pubDate>Thu, 28 Dec 2023 23:35:14 GMT</pubDate>
    </item>
    <item>
      <title>具有策略切换功能的指挥官风格强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18t6vkf/commander_style_rl_with_policy_switching/</link>
      <description><![CDATA[现在我面前有一个特别有趣的问题，我有一个想法。但我不确定这个想法以前是否已经实现过... 我的环境基本上会有一名经理和 4 名工人。可以为工人分配 3 种不同任务中的一种（比如做饭、上菜和洗碗）。每个任务都有自己的训练策略。这里有一个问题 - 经理可能会在剧集中间指示工作人员切换任务（例如：洗碗机可能需要在高峰期间帮助厨师，或者服务器可能需要在打烊时帮助洗碗） ）。这里本质上有两个主要的学习层次：每项任务的原始动作（烹饪的最佳方式、服务的最佳方式和洗碗的最佳方式）和资源分配任务（最好地利用 4 名工人来完成任务）。经营一家餐厅的最终目标）。 我对如何做到这一点有一些想法（但我不完全确定什么是可能的或合乎逻辑的）：  &lt; li&gt;将此视为某种分层学习问题 - 确实如此。经理每隔多个时间步设置任务分配，而工作人员则在每个时间步执行操作来完成这些任务。这是相当可行的，但我不确定如何完成策略切换。根据我的经验，每个多代理设置都有自己的训练策略，因此尝试创建 3 个通用策略并动态分配它们进行训练似乎......很棘手。 创建奖励函数而不是策略切换根据工人的分配奖励不同的结果。该分配可以是其观察空间的一部分（例如：当观察到工人是洗碗机时，奖励函数会奖励干净的盘子，但如果观察到工人是服务员，则奖励满水，等等）。将创建一项所有员工都可以共享的单一政策，并且他们的行为可能会根据他们观察到的任务而改变。 这似乎是最不合逻辑的，但也许我错过了一些东西......我可以分别对这 3 个策略中的每一个进行预训练，并将它们设为静态策略，供经理尝试将任务分配给工作人员。但是A）我认为这消除了策略之间的一些上下文依赖性（例如：在晚餐高峰期间，如果我只将厨师训练为单个实体，然后分配了两名厨师，则该行为可能不会接近最佳，因为以前从未见过）B）我仍然存在实际执行策略切换的问题。  ​ 所以我的问题由此可知，有两个方面。  一次事件期间的动态策略切换实际上可能吗？谁能给我举一些例子吗？ 其中哪一个实际上最有意义？  ​ TIA 提供任何建议并帮助！   由   提交/u/Cheap_Leather_6432   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18t6vkf/commander_style_rl_with_policy_switching/</guid>
      <pubDate>Thu, 28 Dec 2023 22:43:35 GMT</pubDate>
    </item>
    </channel>
</rss>