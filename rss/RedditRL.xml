<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 12 Dec 2024 15:19:17 GMT</lastBuildDate>
    <item>
      <title>在整个轨迹中改变观察空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</link>
      <description><![CDATA[嗨， 有人知道关于在轨迹过程中代理的观察空间的场景的任何先前工作吗？ 例如，如果具有多个传感器的机器人决定在轨迹期间转动其中一个传感器（可能出于能量考虑）。  据我所知，最常用的算法没有考虑到轨迹过程中观察空间的变化。 很想听听大家的想法    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcm3og/changing_observation_space_throughout_a_trajectory/</guid>
      <pubDate>Thu, 12 Dec 2024 14:20:01 GMT</pubDate>
    </item>
    <item>
      <title>可视化环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcknsm/visualizing_environments/</link>
      <description><![CDATA[如何将体育馆环境可视化？有哪些好的做法？    提交人    /u/Main-Bit-6404   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcknsm/visualizing_environments/</guid>
      <pubDate>Thu, 12 Dec 2024 13:04:59 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 MATD3 和 MADDPG 的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</link>
      <description><![CDATA[问候， 我需要在某些环境中运行这两个算法（无所谓）来证明多智能体学习确实有效！（是的，这非常简单，但很难！） 问题就在这里。找不到一个单一的框架来在环境中植入算法（现在基本上是宠物动物园 mpe）， 我做了一些研究：  Marllib 没有很好的文档记录。最后我还是搞不懂。 agileRL 很棒但是有一个 bug 我无法解决，（如果您能解决这个 bug 请告诉我）。 Thianshou ，我必须植入算法！！ CleanRL，嗯...我没搞懂。我的意思是我应该在我的主脚本中使用这些算法 .py 文件吗？  好吧请帮忙.......... 带着爱    提交人    /u/matin1099   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcjelg/need_help_about_matd3_and_maddpg/</guid>
      <pubDate>Thu, 12 Dec 2024 11:50:04 GMT</pubDate>
    </item>
    <item>
      <title>针对机械臂 RL 项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</link>
      <description><![CDATA[嗨，我正在寻找一些论文或想法，我可以在硬件机器人操纵器上尝试，以获得 RL 控制和规划方面的经验。     提交人    /u/Stunning-Stable-8553   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</guid>
      <pubDate>Thu, 12 Dec 2024 02:57:44 GMT</pubDate>
    </item>
    <item>
      <title>体育馆/mujoco 教程需要四足机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</link>
      <description><![CDATA[大家好，我正在做一个关于四足机器狗的项目。我正在尝试使用 gymnasium 和 MuJoCo，但在 gymnasium 上设置自定义环境真的很令人困惑。我正在寻找一个教程，这样我就可以学习如何设置它，或者是否有人建议我应该切换我正在使用的工具。    提交人    /u/PrincipleDistinct425   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</guid>
      <pubDate>Wed, 11 Dec 2024 21:48:47 GMT</pubDate>
    </item>
    <item>
      <title>Rl 用于自主导航</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</link>
      <description><![CDATA[大家好，我打算在我的最后一年项目中使用 ubuntu 22.04 和 ros2 hum，该项目包括使用 DRL 技术在 Gazebo 中为 turtlebot3 机器人进行自主导航，但我真的很担心我的笔记本电脑的容量，我真的没有合适的笔记本电脑，我有一台第 8 代英特尔 i5，8 核，我真的会发现学习问题吗？    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</guid>
      <pubDate>Wed, 11 Dec 2024 19:10:28 GMT</pubDate>
    </item>
    <item>
      <title>彼得·墨菲 (Peter Murphy) 的最新强化学习书籍有多好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</link>
      <description><![CDATA[编辑：应该是 Kevin Murphy。 我的一位同事推荐了 https://arxiv.org/pdf/2412.05265。 我发现它有点像洗衣清单，就像其他强化学习调查的情况一样。不同的想法感觉就像反复试验。我自己过去也曾在 TensorFlow 中编写过强化学习。但很难真正感受到它的威力。我有数学背景，我只是不确定是否值得花时间阅读这么厚的书，因为我知道我可能记不住很多东西，除非所有不同的概念形成连贯的意识流。换句话说，我发现这个主题没有足够的基础来轻松消化第一原理。 我对其他人对这个问题的看法很好奇，特别是从第一原理的角度来看。    提交人    /u/Crazy_Suspect_9512   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</guid>
      <pubDate>Wed, 11 Dec 2024 18:43:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Stable Baselines 3 中的训练期间动态修改超参数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/</link>
      <description><![CDATA[我正在使用 Stable Baselines 3，并尝试实施一个训练过程，在该过程中我在训练的不同阶段动态地改变超参数。具体来说，我正在使用 PPO 并想要更改 gamma 参数。 这是我正在尝试执行的操作的简化版本： ```py from stable_baselines3 import PPO # 初始训练 model = PPO(&quot;MlpPolicy&quot;, &quot;CartPole-v1&quot;, gamma=0.99) model.learn(total_timesteps=10000) print(f&quot;初始 gamma：{model.gamma}&quot;) print(f&quot;初始推出缓冲区 gamma：{model.rollout_buffer.gamma}&quot;) # 尝试更改 gamma model.gamma = 0.95 model.learn(total_timesteps=10000) print(f&quot;更改后 - 模型 gamma：{model.gamma}&quot;) print(f&quot;更改后 -推出缓冲区 gamma：{model.rollout_buffer.gamma}&quot;)  ``` 输出： ```py 初始 gamma：0.99 初始推出缓冲区 gamma：0.99 更改后 - 模型 gamma：0.95 更改后 - 推出缓冲区 gamma：0.99  ``` 我们可以看到，更改 model.gamma 不会更新所有必要的内部状态。 model.rollout_buffer.gamma 保持不变，这可能导致不一致的行为。 我考虑过使用新参数保存并重新加载模型： ```py model.save(&quot;temp_model&quot;) model = PPO.load(&quot;temp_model&quot;, gamma=0.95) model.learn(total_timesteps=10000) print(f&quot;重新加载后 - 模型 gamma：{model.gamma}&quot;) print(f&quot;重新加载后 - 滚动缓冲区 gamma：{model.rollout_buffer.gamma}&quot;)  ``` 输出： ```py 重新加载后 - 模型 gamma：0.95 重新加载后 - 滚动缓冲区 gamma：0.95  ``` 这种方法有效但似乎效率低下，特别是如果我想在训练期间频繁更改参数时。 在 Stable Baselines 3 中，是否有一种适当的方法可以在训练期间动态更新像 gamma 这样的超参数？理想情况下，我想要一个解决方案，确保所有相关的内部状态都得到一致更新，而无需保存和重新加载模型。 对于这种情况的任何见解或最佳实践都将不胜感激。    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/</guid>
      <pubDate>Wed, 11 Dec 2024 13:13:23 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 在我的用例中存在问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</link>
      <description><![CDATA[大家好， 这是我第一次从事 RL 项目，我正在构建一个可以与特定 DLT 一起使用的模型。具体来说，我希望它选择最佳数量的块来通过特定的 DLT 发送消息。我尝试了不同的算法，但由于它必须自主选择动作并且不受限制，所以我选择了 DDPG 方法。 然而，让我很困惑的是，对于我构建的特定奖励系统，对于单次训练运行（不更新模型），模型有时会学习，有时不会。这意味着对于大多数运行，模型不会探索选项，它将坚持发送消息所需的最少块数。而在较少的情况下，它似乎在学习，但仅此而已。下次我运行代码时，它可能会回到选择最小数量的块。 不确定这是否是奖励系统、Actor - Critic 网络的架构或算法本身的问题。但我很感激一些指导。非常感谢！    提交人    /u/LionTheAlpha   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</guid>
      <pubDate>Wed, 11 Dec 2024 08:57:42 GMT</pubDate>
    </item>
    <item>
      <title>协助定期 PPO 代理优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbdgqq/assistance_with_recurrent_ppo_agent_optimization/</link>
      <description><![CDATA[我正在训练我的循环 PPO 代理执行优化任务，代理的基于 token 的操作会输入到单独的数值优化器中。然而，在初始训练步骤之后，代理始终卡在其连续动作空间的上限和下限，并且奖励保持不变。您能否提供一些解决此问题的指导？     提交人    /u/YasinRL   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbdgqq/assistance_with_recurrent_ppo_agent_optimization/</guid>
      <pubDate>Tue, 10 Dec 2024 21:56:03 GMT</pubDate>
    </item>
    <item>
      <title>AI Sphere 平衡球：虚幻引擎的强化学习插件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbak2t/ai_sphere_balancing_a_ball_a_reinforcement/</link>
      <description><![CDATA[大家好！👋 我一直在研究一个 AI 球体，它可以学习使用强化学习来平衡球，由虚幻引擎的自定义插件提供支持。该插件是 Unity ML-Agents 的端口，为 Unreal 用户带来 RL 工具。 该视频使用二次奖励系统演示了 Sphere - 如果将球保持在中心附近，则会获得更多积分，如果球掉落，则会失去积分。 🎥 https://youtu.be/6lhCa72TGNk?si=qcIKof09R1Yl0SJw 该插件将很快推出，Sphere 项目将作为示例包含在内。如果您有兴趣，请加入我的 Discord 以获取更新和早期访问：https://discord.gg/Y8MDexY4。 很想听听您的想法或创意 - 感谢您的关注！😊    提交人    /u/Cyber​​Eng   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbak2t/ai_sphere_balancing_a_ball_a_reinforcement/</guid>
      <pubDate>Tue, 10 Dec 2024 19:53:06 GMT</pubDate>
    </item>
    <item>
      <title>GameMaker 8.0 游戏的 Gym 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb7hvd/gym_environment_for_a_gamemaker_80_game/</link>
      <description><![CDATA[我最近决定尝试强化学习，以此来构建一个很酷的投资组合项目。我决定为 Spelunky Classic 制作一个深度 Q 学习算法，因为我在 YouTube 视频 中找到了七年前有人做过同样的事情，而且我是 Spelunky 的狂热粉丝，这让它更具吸引力。 我开始在我的计算机上设置所有适当的 Python 内容，经过几个小时的工作，这个 PyTorch 教程 开始运行。然而，此时我遇到了一个障碍——我了解 DQN 的工作原理，并且我有 Spelunky Classic 的源代码，所以我绝对可以为 DQN 修改它，但我对如何实现它感到困惑。我意识到下一步可能是为 Spelunky Classic 创建一个健身房环境，但我不知道如何将奖励返回给 Python 程序，因为游戏是用 GameMaker 8.0 编写的，这是一个弃用的引擎，在 OpenAI 热潮期间没有相关支持。 所有这些都是为了问，我现在应该如何让它工作？我是否必须制作某种自定义包装器才能将游戏变成健身房环境？有什么简单的方法可以将游戏转换为健身房环境？是否存在我尚未意识到的困难？ 任何帮助或建议都值得感激，我只是有点不知道下一步该怎么做。    提交人    /u/GarrettBotProgrammer   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb7hvd/gym_environment_for_a_gamemaker_80_game/</guid>
      <pubDate>Tue, 10 Dec 2024 17:45:18 GMT</pubDate>
    </item>
    <item>
      <title>“哥德尔代理：用于递归自我改进的自参考代理框架”，Yin 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb5zds/gödel_agent_a_selfreferential_agent_framework_for/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb5zds/gödel_agent_a_selfreferential_agent_framework_for/</guid>
      <pubDate>Tue, 10 Dec 2024 16:41:39 GMT</pubDate>
    </item>
    <item>
      <title>将强化学习应用于投资组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb4hsy/applying_rl_to_portfolio/</link>
      <description><![CDATA[我是一名加密和机器学习爱好者，正在完成一个用于算法交易的回测系统（信不信由你，只是为了好玩）。我正在考虑测试一些用于投资组合优化的强化学习方法。 我有大量的历史数据可以使用，但我对建立训练方案的最佳方法以及模型容量的选择有些困惑。 我目前的想法是采用基于与投资组合价值相关的奖励函数的参与者/评论家设置。 哪个时间步骤最有意义？ 我是否应该预先训练一个模型来简单地预测均值和方差（这样我就可以使用历史数据而无需进行演练）？ 或者我应该专门通过演练进行训练？如果是这样，我应该将它们并行化吗？    提交人    /u/brabbly   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb4hsy/applying_rl_to_portfolio/</guid>
      <pubDate>Tue, 10 Dec 2024 15:37:16 GMT</pubDate>
    </item>
    <item>
      <title>2 个 AI 代理玩捉迷藏。经过 150 万次模拟，代理学会了偷看、搜索和切换方向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hawyj5/2_ai_agents_playing_hide_and_seek_after_15/</link>
      <description><![CDATA[    /u/stokaty   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hawyj5/2_ai_agents_playing_hide_and_seek_after_15/</guid>
      <pubDate>Tue, 10 Dec 2024 08:03:36 GMT</pubDate>
    </item>
    </channel>
</rss>