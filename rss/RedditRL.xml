<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 08 Jun 2024 12:24:55 GMT</lastBuildDate>
    <item>
      <title>我的动作空间有 90% 被掩盖了，我希望从中获得计算上的好处</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</link>
      <description><![CDATA[我的 q-learning 任务中有一个包含 5000 个动作的大型动作空间，但在任何时候都只有几百个合法动作，因此我会屏蔽它们，但是，我只是在模型预测步骤之后进行屏蔽。有没有办法事先屏蔽它并获得大幅加速？  def get_predictions(self, state, legal_mask): state = np.reshape(state, [1, self.state_size) act_values = [act_values[i] if legal_mask[i] == 1 else -np.inf for i in range(len(act_values))] return act_values     submitted by    /u/Breck_Emert   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dawzuq/my_action_space_is_90_masked_and_i_want_to/</guid>
      <pubDate>Sat, 08 Jun 2024 07:03:17 GMT</pubDate>
    </item>
    <item>
      <title>[CfP] 与 RealAIGym 合作举办第二届人工智能奥运会：IROS 2024 机器人竞赛——立即加入！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dahea9/cfp_2nd_ai_olympics_with_realaigym_robotics/</link>
      <description><![CDATA[        提交人    /u/Dense-Positive6651   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dahea9/cfp_2nd_ai_olympics_with_realaigym_robotics/</guid>
      <pubDate>Fri, 07 Jun 2024 18:04:52 GMT</pubDate>
    </item>
    <item>
      <title>计算两个 Q 学习策略之间的 KL 散度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dacrpc/calculating_kldivergence_between_two_qlearning/</link>
      <description><![CDATA[大家好， 我想计算使用 Q-learning 训练的两个策略之间的 KL 散度。由于 Q-learning 根据最高 Q 值选择动作，而不是生成概率分布，这些策略是否应该表示为独热向量？如果是这样，考虑到独热向量中概率为零的问题，我们如何计算 KL 散度？    提交人    /u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dacrpc/calculating_kldivergence_between_two_qlearning/</guid>
      <pubDate>Fri, 07 Jun 2024 14:55:45 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中可以从事哪些工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dacmz0/which_jobs_in_reinforcement_learning/</link>
      <description><![CDATA[嘿， 由于我对强化学习越来越感兴趣，我试图了解如何寻找使用它的工作。 大多数数据科学家、mlops、数据工程师职位似乎都在使用监督机器学习，也许在营销等方面使用无监督学习。 除了机器人公司，我没有看到任何使用 RL 的职位描述。 你如何寻找可以应用 RL 的工作？ 此外，RL 在游戏和机器人背后的应用是什么？ 我将列出几个我感兴趣的主题，但我不确定需要应用哪种类型的机器学习来解决这些问题：  交通优化， 政府预算优化 拥有更“完整”视频游戏体验 - 即与 NPC 合作，如何与他们互动等。  模拟环境（不知道是否存在，但假设您尝试模拟某项新法律将对人口产生什么影响？） 机器人技术     提交人    /u/BoxingBytes   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dacmz0/which_jobs_in_reinforcement_learning/</guid>
      <pubDate>Fri, 07 Jun 2024 14:50:11 GMT</pubDate>
    </item>
    <item>
      <title>规则手册——用于编写高效输入密集型环境的 DSL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dabtbu/rulebook_a_dsl_to_write_efficient_input_intensive/</link>
      <description><![CDATA[嗨，我是 rulebook 的主要开发人员。 Rulebook 语言旨在简化复杂机器学习环境的开发、维护和可重用性，无论是在训练期间还是训练之后。 从概念上讲，您可以将此工具视为&quot;您编写所需的 gym 环境的非常高级的伪代码，然后 `rulebook` 返回一个用 C 编写的优化 gym 环境，并包装在 python 中&quot; 例如，我们正在用作测试的真实世界现成的棋盘游戏，其中包括动态列表并具有 1000 多个动作（在当然，一个完整的项目（例如，一个组）可以用~1000 行代码（不包括测试）来编写。该游戏不包含一行机器学习专用代码，但既可用于机器学习，也可用于其他目的（例如，将其部署在游戏引擎中）。 在该游戏中添加和删除动作就像在可以执行此类动作的代码点添加和删除以下代码一样简单。  act toggle_door(UnitArgType unit_id) { # action board.unit_id_is_valid(unit_id.value), # precondition board.units.get(unit_id.value).action_points != 0, board.is_facing_door(board.units.get(unit_id.value)) } ref unit = board.units.get(unit_id.value) # effect unit.action_points = unit.action_points - 1 board.toggle_door(unit)  您可以阅读有关语言原理这里 。 您可以在这里阅读有关更大的项目原理。 您可以在这里阅读我们如何在现成的游戏中使用它。 您可以在这里尝试它（如果您已经知道什么是机器学习，请跳过介绍并转到关于本文档部分）。您需要一台装有 python3.8 的 linux x64 机器才能执行此操作。 本教程刚刚创建，我们在这里展示它以收集有关它和项目的反馈。因此欢迎任何类型的反馈。    提交人    /u/drblallo   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dabtbu/rulebook_a_dsl_to_write_efficient_input_intensive/</guid>
      <pubDate>Fri, 07 Jun 2024 14:15:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 Rllib 自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1daar1d/custom_env_with_rllib/</link>
      <description><![CDATA[大家好， 我是 RL 社区的新手，我正在尝试学习如何使用 Rllib，到目前为止，这是一次不幸的旅程 我做了一个简单的游戏，但我在这里添加并简化了它，以防你们中的一些人想帮忙，屏幕上随机生成一个圆圈，然后你必须点击它里面才能赢，你最多有 10 次机会点击它里面，否则你就输了，环境会被重置（圆圈是随机重新生成的） 我正在努力定义环境并使用 PPO 来训练它，我只是无法毫无错误地实现它，即使它看起来很简单，你们能帮忙如何实现它并将它连接到游戏吗？ 我只希望它能工作，之后我会根据我的需要调整它， 如果你们有有关如何使用 rllib 制作自定义简单游戏的示例，我很想看看它，因为我还没能找到最近的东西。非常感谢  import cv2 import numpy as np import random class CircleGame: def __init__(self): self.img = None self.center = None self.radius = None self.attempts = 0 self.done = False self.action = None self.reset() def draw_random_circle(self): height, width, _ = self.img.shape radius = random.randint(20, 50) center = (random.randint(radius, width - radius), random.randint(radius, height - radius)) color = (0, 255, 0) thick = 2 cv2.circle(self.img, center, radius, color, thick) return center, radius def is_inside_circle(self, point): return (point[0] - self.center[0]) ** 2 + (point[1] - self.center[1]) ** 2 &lt;= self.radius ** 2 def reset(self): self.img = np.ones((600, 800, 3), dtype=&quot;uint8&quot;) * 255 self.center, self.radius = self.draw_random_circle() self.attempts = 10 self.done = False self.action = None return self.get_state() def step(self, action): if self.done: raise ValueError(&quot;游戏已完成。请重置环境。&quot;) point = action if self.is_inside_circle(point): reward = 1.0 self.done = True print(&quot;你赢了！&quot;) else: self.attempts -= 1 reward = -0.1 color = (0, 0, 255) cv2.circle(self.img, point, 5, color, -1) # 在猜的点处画一个小圆圈 if self.attempts == 0: self.done = True reward = -1.0 打印（“你输了！重新开始游戏。”) return self.get_state(), reward, self.done, {} def get_state(self): return self.img.copy(), self.attempts def render(self): cv2.imshow(&#39;Game&#39;, self.img) cv2.setMouseCallback(&#39;Game&#39;, self.mouse_callback) cv2.waitKey(1) def mouse_callback(self, event, x, y, flags, param): if event == cv2.EVENT_LBUTTONDOWN: self.action = (x, y) if not self.done: state, reward, done, _ = self.step(self.action) print(f&quot;Reward: {reward}, Done: {done}&quot;) if done: self.reset() def close(self): cv2.destroyAllWindows() if __name__ == &quot;__main__&quot;: game = CircleGame() game.reset() while True: game.render() if game.done: game.reset() key = cv2.waitKey(1) &amp; 0xFF if key == 27: # 按 ESC 退出游戏 break game.close()     提交人    /u/Snoo37129   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1daar1d/custom_env_with_rllib/</guid>
      <pubDate>Fri, 07 Jun 2024 13:31:07 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线训练步骤与预期存在差异</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da8fq9/discrepency_in_stable_baselines_training_steps_vs/</link>
      <description><![CDATA[您好， 我是 RL 新手，创建了一个自定义连续 2D 迷宫环境。我想训练一个代理来导航迷宫并到达目标。我的环境 max_timesteps 设置为 500。我正在使用稳定基线 PPO 进行训练。我的参数是： # config.yaml ppo: policy: &#39;MlpPolicy&#39; n_steps: 250 learning_rate: 0.0003 batch_size: 10 n_epochs: 10 gamma: 0.99 gae_lambda: 0.95 clip_range: 0.2 ent_coef: 0.01 vf_coef: 0.5 max_grad_norm: 0.5 use_sde: true sde_sample_freq: -1 tensorboard_log: &quot;./ppo_tensorboard/&quot; seed: 123 device: &quot;cpu&quot;  问题是，无论我告诉 model.learn 训练多少步，我都会这样做。例如，我训练了 model.learn(total_timesteps=10000)。对于每次迭代最多 500 步的 10,000 个时间步，我预计会有 20 次迭代，但此回调指示 4 次迭代和 12000 个时间步。知道是什么原因造成的吗？ ----------------------------------------------------- | rollout/ | | | ep_len_mean | 480 | | ep_rew_mean | -2.77e+03 | | time/ | | | fps | 83 | | iterations | 4 | | time_elapsed | 143 | | total_timesteps | 12000 | | train/ | | | approx_kl | 0.021791738 | | clip_range | 0.2 | | entropy_loss | -9.48 | | explained_variance | 0.0275 | | learning_rate | 0.0003 | | loss | 4.38e+04 | | n_updates | 30 | | policy_gradient_loss | -0.109 | | std | 0.998 | | value_loss | 3.06e+04 | -----------------------------------------     提交人    /u/lujan-002   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da8fq9/discrepency_in_stable_baselines_training_steps_vs/</guid>
      <pubDate>Fri, 07 Jun 2024 11:34:28 GMT</pubDate>
    </item>
    <item>
      <title>Unity ML-Agents 与 Unreal 学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da7j72/unity_mlagents_vs_unreal_learning_agents/</link>
      <description><![CDATA[对于我即将到来的学士项目，我和我的团队决定开发一款带有 RL 代理的游戏。游戏概念尚未 100% 确定。我们现在的主要问题是决定我们应该使用哪种游戏引擎。你们当中有谁同时使用过 Unity ML-Agents 和 Unreal 的学习代理，可以对这两个环境进行简单的比较吗？ 就我个人而言，我更喜欢 Unity 的框架，因为我担心 Unreal 的学习代理还“不够好”。    提交人    /u/Cuuuubee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da7j72/unity_mlagents_vs_unreal_learning_agents/</guid>
      <pubDate>Fri, 07 Jun 2024 10:38:50 GMT</pubDate>
    </item>
    <item>
      <title>在 RL 中可视化和比较 PPO 与 REINFORCE 的状态和动作的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1da5lig/best_ways_to_visualize_and_compare_states_and/</link>
      <description><![CDATA[大家好， 我正在使用 LunarLanderContinuous-v2 环境比较 PPO 和 REINFORCE。在训练完两种算法之后，我希望有效地可视化和比较它们所采取的状态和操作。 具体来说，我正在寻找以下方面的建议：  如何可视化每个算法访问的状态分布。 如何表示每个算法所采取的操作中的多样性和模式。 任何可以帮助使这些比较清晰且富有洞察力的工具或技术。  我已经在训练期间记录了状态和动作，但我正在努力如何以有意义的方式呈现这些数据，至少我真的没有一个清晰的想法。 我还可以展示除累积奖励之外的其他差异，但我真的不知道是什么。 谢谢！    提交人    /u/An4rcyst   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1da5lig/best_ways_to_visualize_and_compare_states_and/</guid>
      <pubDate>Fri, 07 Jun 2024 08:19:00 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何在机器人腿平衡任务中实现奖励/重置系统？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9znxi/how_should_i_implement_the_rewardsreset_system_in/</link>
      <description><![CDATA[大家好。我设计了一个机器人腿，希望将来能教它如何走路（显然是 2），但现在我正尝试使用 Omniverse Isaac Gym 和稳定基线 3 来教一条腿如何在脚上保持平衡，只是为了了解 RL 并看看我的关节是否足够强壮以保持腿的抬起。我想说的是，我是一个初学者，除了训练推车杆、人形机器人和蚂蚁的例子外，没有太多经验。 我的任务有点奏效了，但我认为在如何重置腿以及如何计算奖励和惩罚方面存在一些明显的问题。但我还没有走得足够远来能够告诉 我的问题是，经过几次重置后，机器人就会飞起来，整个模拟基本上就崩溃了......我的重置方法是，一旦机器人掉落（脚上的接触传感器达到某个值以下），代码就会将机器人重置/惩罚到它的起始世界位置，关节设置为 0。也许这不是正确的方法？也许我应该让它保持在跌倒的地方，然后试着让它学会如何重新站起来？我不知道.. 我从人形训练任务中观察到，机器人每次掉落时都会重置并从原来的地方开始，但也许我不明白到底发生了什么？ 任何帮助都将不胜感激.. 我还想提一下，我确实尝试过 pybullet，但它无法正确加载我的 URDF..不知道为什么🤷‍♂️。   由    /u/StoryReader90  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9znxi/how_should_i_implement_the_rewardsreset_system_in/</guid>
      <pubDate>Fri, 07 Jun 2024 02:11:56 GMT</pubDate>
    </item>
    <item>
      <title>深度学习项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9wxf7/deep_learning_projects/</link>
      <description><![CDATA[我正在攻读数据科学和人工智能理科硕士学位。我将于 2025 年 4 月毕业。我正在寻找深度学习项目的想法。1) 为 LLM 实施的深度学习 2) 为 CVision 实施的深度学习 我在网上查了一下，但大多数都是非常标准的项目。来自 Kaggle 的数据集是通用的。我大约有 12 个月的时间，我想做一些好的研究级项目，可能在 NeuraIPS 上发表它。我的优势是我擅长解决问题，一旦确定了问题，但我不擅长识别和构建问题..目前，我正在尝试判断什么是一个好的研究领域？    提交人    /u/Rogue260   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9wxf7/deep_learning_projects/</guid>
      <pubDate>Thu, 06 Jun 2024 23:53:15 GMT</pubDate>
    </item>
    <item>
      <title>“大型语言模型中对齐的基本限制”，Wolf 等人 2023（提示对动作不安全后验的先验）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9le1u/fundamental_limitations_of_alignment_in_large/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9le1u/fundamental_limitations_of_alignment_in_large/</guid>
      <pubDate>Thu, 06 Jun 2024 15:47:15 GMT</pubDate>
    </item>
    <item>
      <title>风险：统治全球 体育馆观景空间设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kzfp/risk_global_domination_gymnasium_observation/</link>
      <description><![CDATA[大家好 我一直在构建一个 Risk: 全球统治环境。我的目标是稍后使用 RL 和 DQN 来构建机器人。 我是 RL 新手。 我目前处于一个阶段，我在本地运行一个简单版本的游戏，确定性机器人相互对战。有 4 个领地和 2 个玩家 我现在想要完成的是将其转换为 Gymnasium 环境，并实施强化学习，专注于单一行动可能性：选择攻击哪个领地。所有其他行动都将确定性地处理。这是为了简化实施，我希望这是一个明智的举措。 我现在需要定义观察空间。对于那些不熟悉 Risk 游戏的人来说，1v1 标准游戏的重要信息包括：  哪个领土由哪个玩家控制，每个领土上有多少军队 游戏的历史（虽然我想我现在会跳过这一个） 领土如何定位（哪个连接到哪个，以预测路径） 大陆 - 包括哪些领土以及它们奖励多少军队（但我现在也会跳过它）  所以，我想要做以下事情： # 谁控制着每个领土。只有两个玩家，所以值为 0 或 1 membership_space = rooms.Box(low=0, high=1, shape=(4,), dtype=int) # 理论上最大部队数量可以无限，所以让我们将体育馆记录的限制设置为整数：2**63 - 2 soldiers_space = rooms.Box(low=0, high=2**63-2, shape=(4,), dtype=int) # 大小为 (num_territories x num_territories) 的邻接矩阵 connections_space = rooms.Box(low=0, high=1, shape=(4, 4), dtype=int) # 知道机器人是哪个玩家 player_id_space = rooms.Box(low=0, high=1, shape=(), dtype=int) self.observation_space = gym.spaces.Dict({ &quot;ownership&quot;: membership_space, &quot;troops&quot;: soldiers_space, &quot;connectivity&quot;: connection_space, &quot;player_id&quot;: player_id_space })  我使用 Boxes 来表示我的向量，这是最好的选择吗？ 我认为让机器人知道它是什么玩家很重要？ 此外，我认为有更好的方法来表示领土连通性。 再次，我对 RL 非常陌生，对实现它的所有选项有点迷茫。 谢谢你的帮助 编辑：我实际上需要一个观察变量来指定我从哪个领土攻击，因为在我的简化版本的 DQN 机器人中，机器人唯一可以选择的就是攻击哪个领土。    提交人    /u/BoxingBytes   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kzfp/risk_global_domination_gymnasium_observation/</guid>
      <pubDate>Thu, 06 Jun 2024 15:29:48 GMT</pubDate>
    </item>
    <item>
      <title>反过来想，在 RL 中“使用采样命令进行探索”有什么意义呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</link>
      <description><![CDATA[在行为函数更新后，RL upside down 选择最佳的前 k 个近期情节并汇总来自它们的信息以提出目标命令，这些命令是这些精英情节的奖励和范围之和的平均值。然后这些命令用于生成更多情节以添加到重放缓冲区中。但我注意到在生成情节时，初始状态是重置状态，这意味着前进的轨迹可能与这些命令完全无关，因为那些新看到的状态可能不会导致那些提出的命令，从而导致随机采样动作。在这方面，RL upside down 是否依靠神经网络的泛化来获得更高的奖励轨迹？否则我不明白它是如何工作的。    提交人    /u/OutOfCharm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d9kfp0/whats_the_point_of_explore_using_sampled_commands/</guid>
      <pubDate>Thu, 06 Jun 2024 15:06:44 GMT</pubDate>
    </item>
    <item>
      <title>从这往哪儿走？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</link>
      <description><![CDATA[我有一个需要 RL 的项目，我学习了 Sutton 撰写的 RL 简介的前 200 页，掌握了基础知识和所有基本理论信息。你们有什么建议可以开始实际实施我的 RL 项目想法，比如从 OpenAI Gym 中的基本想法开始，或者我不知道我是新手，你们能给我一些建议，告诉我如何在实践方面做得更好吗？ 更新：谢谢大家，我会检查所有这些建议，这个 subreddit 很棒！    提交人    /u/Signal-Ad3628   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d96l3l/where_to_go_from_here/</guid>
      <pubDate>Thu, 06 Jun 2024 01:32:05 GMT</pubDate>
    </item>
    </channel>
</rss>