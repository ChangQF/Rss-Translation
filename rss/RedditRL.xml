<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 24 Jul 2024 03:17:46 GMT</lastBuildDate>
    <item>
      <title>以下公式正确吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ear52l/is_the_following_formula_correct/</link>
      <description><![CDATA[我正在阅读https://stackoverflow.com/questions/46422845/what-is-the-way-to-understand-proximal-policy-optimization-algorithm-in-rl#comment89167955_50663200，只是为了再次检查我对 PPO 的直觉，因为我已经有一段时间没有研究它了，但我偶然发现了这一点，这是作者解释 REINFORCE 算法的一部分：https://i.sstatic.net/5VZRT.png 这是 REINFORCE 的正确损失吗？不应该有对数项吗？我认为对数是由损失的微分产生的，而不是损失本身。当然，我明白，如果你最大化这个，它就等于最大化初始损失，但这并不容易微分（或者至少不容易用参与者 NN 中参数的导数来表达）对吧？    提交人    /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ear52l/is_the_following_formula_correct/</guid>
      <pubDate>Wed, 24 Jul 2024 03:11:29 GMT</pubDate>
    </item>
    <item>
      <title>“通过扭曲序贯蒙特卡罗进行语言模型中的概率推理”，Zhao 等人 2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eaq9tl/probabilistic_inference_in_language_models_via/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eaq9tl/probabilistic_inference_in_language_models_via/</guid>
      <pubDate>Wed, 24 Jul 2024 02:27:29 GMT</pubDate>
    </item>
    <item>
      <title>有任何关于观察 3D 数据的 RL 研究吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eapb34/any_rl_study_about_observing_3d_data/</link>
      <description><![CDATA[嗨，有没有使用 3D 空间数据来观察状态的研究？ 我正在做一个 RL 项目，它的观察空间是 3D。特别是病人的 CT 扫描。 3D 扫描的尺寸非常大（缩小到 128*128*64），所以我使用 3D CNN 编码器来减小尺寸。 除了深度学习方法之外，我没有研究过太多 RL，而且似乎构建网络架构与深度学习非常不同（例如，RL 网络要小得多，大多数层只是 MLP，据我在 atari 教程中看到的那样，CNN 编码器中没有规范化）。 有人可以分享任何使用 3D 编码器对状态进行编码的论文或代码吗？    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eapb34/any_rl_study_about_observing_3d_data/</guid>
      <pubDate>Wed, 24 Jul 2024 01:40:19 GMT</pubDate>
    </item>
    <item>
      <title>启动图书馆学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eaeng7/spinnig_up_library_learning_sessions/</link>
      <description><![CDATA[大家好，我是一名二年级博士生，研究方向为强化学习的计算机科学。我有一些强化学习的背景（观看了 David Silver 的 YouTube 讲座，阅读了 DeepMind 的几篇论文，如 AlphaGo 和 Muzero，也熟悉主要的 RL/DRL 算法），并希望在实施方面更深入、更好地理解强化学习。华盛顿特区、马里兰州或弗吉尼亚州地区是否有人愿意安排学习课程来阅读 Spinningup 图书馆文档并观看其 YouTube 讲座？    提交人    /u/atb1399   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eaeng7/spinnig_up_library_learning_sessions/</guid>
      <pubDate>Tue, 23 Jul 2024 18:07:15 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习：与无模型强化学习的区别令人困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eachu8/modelbased_rl_confused_about_the_differences/</link>
      <description><![CDATA[在互联网上，人们可以找到许多帖子来解释 MBRL 和 MFRL 之间的区别。即使在 Reddit 上，也有一个很好的直观帖子。那么，为什么还要问一个关于同一主题的无聊问题呢？ 因为当我读到类似这样的定义时： 基于模型的强化学习 (MBRL) 是一个用于在部分理解的环境中解决任务的迭代框架。有一个代理反复尝试解决问题，积累状态和操作数据。利用这些数据，代理创建一个结构化的学习工具——动态模型——来推理世界。利用动态模型，代理通过预测未来来决定如何行动。通过这些操作，代理可以收集更多数据，改进所述模型，并有望改进未来的操作。 (source)。 那么对我来说，MBRL 和 MFRL 之间只有一个区别：在模型自由的情况下，您将问题视为黑匣子。然后你实际上运行双或数百万个步骤来了解黑匣子的工作原理。但这里的问题是：与 MBRL 有什么区别？ 另一个问题是，当我读到时，您不需要 MBRL 的模拟器，因为算法在训练阶段可以理解动态。好的。这对我来说很清楚…… 但是假设您有一辆正在行驶的汽车（没有摄像头，只有汽车在跑道上行驶的形状）并且您想要应用 MBRL，那么您需要一个汽车模拟器，因为模拟器会生成代理所需的图片，以便代理能够真正看到汽车是否在路上。  所以即使我认为，我理解了两者之间的理论区别，但当我试图弄清楚何时需要模拟器，何时不需要模拟器时，我仍然停滞不前。 从字面上讲：即使我在 Gymnasium 中为 Cartpole 环境训练一个简单的代理（并使用无模型方法），我也需要一个模拟器。 但是，如果我想使用 GPS（基于模型），那么无论如何我都需要那个环境。  如果您能帮助我理解，我真的很感激。 谢谢   由    /u/WilhelmRedemption  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eachu8/modelbased_rl_confused_about_the_differences/</guid>
      <pubDate>Tue, 23 Jul 2024 16:40:21 GMT</pubDate>
    </item>
    <item>
      <title>预训练自动编码器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ea4310/pretrained_autoencoders/</link>
      <description><![CDATA[我过去曾尝试过使用预训练自动编码器来玩 Atari 等游戏，但我发现预训练并没有太大帮助。我也对变分自动编码器进行了一些尝试，发现它们也没有加快训练速度。我通常使用 MSE 重构误差进行预训练，然后冻结编码器权重并附加策略网络或 Q 函数。我知道也有一些研究声称对比自动编码器效果更好，但这依赖于正/负对。 我很好奇是否有其他人遇到过类似的自动编码器问题，或者您是否有任何用于 RL 预训练自动编码器的实用技巧。    提交人    /u/smorad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ea4310/pretrained_autoencoders/</guid>
      <pubDate>Tue, 23 Jul 2024 10:05:06 GMT</pubDate>
    </item>
    <item>
      <title>cleanrl ppo 中 num_step 的含义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9ydvf/meaning_of_num_step_in_cleanrl_ppo/</link>
      <description><![CDATA[cleanrl github 嗨，我对推出时步骤数（repo 中的 num_steps）的含义有疑问。 我以为步骤数是一集的最大长度（例如，在我的环境中，游戏的最大长度是 25 步）。 所以我认为当一集终止时，步骤中的迭代应该关闭，但在 repo 中，迭代仍在继续。即使环境被重置了。 有什么我误解的吗？    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9ydvf/meaning_of_num_step_in_cleanrl_ppo/</guid>
      <pubDate>Tue, 23 Jul 2024 03:58:25 GMT</pubDate>
    </item>
    <item>
      <title>d4rl maze2d 最小和最大分数计算？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9u3z5/d4rl_maze2d_minimum_and_maximum_scores_calculation/</link>
      <description><![CDATA[抱歉，这个问题可能有点琐碎；我想了解为什么 d4rl &#39;maze2d-umaze-v1&#39; 随机代理得分为 23.85，而专家得分为 161.86。 编辑：一集的总奖励似乎是可变的。maze2d 在完成之前是否会为一集返回多个非零奖励？    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9u3z5/d4rl_maze2d_minimum_and_maximum_scores_calculation/</guid>
      <pubDate>Tue, 23 Jul 2024 00:25:30 GMT</pubDate>
    </item>
    <item>
      <title>目前 RL 领域最顶尖的研究人员有哪些？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9mxbr/what_are_the_top_researchers_in_rl_right_now/</link>
      <description><![CDATA[我病了，接下来的几天会待在家里，我想我应该补上读书。 有人对顶级研究人员有什么建议吗，这样我就可以查阅他们的论文了？谢谢！    提交人    /u/phantomBlurrr   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9mxbr/what_are_the_top_researchers_in_rl_right_now/</guid>
      <pubDate>Mon, 22 Jul 2024 19:24:52 GMT</pubDate>
    </item>
    <item>
      <title>我已经训练机器人与 RL 进行战斗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9l5q3/ive_trained_robots_to_fight_with_rl/</link>
      <description><![CDATA[大家好， 我想回答动漫中一个永恒的问题：“哪种武术风格更胜一筹？”😁 这是我最近尝试的视频：https://www.youtube.com/watch?v=7AnJAlDFTN0 一些背景知识：一切都由神经网络控制，包括关节和高级策略。我决定使用 Unity ML-Agents，因为我非常喜欢在 Unity 中开发。训练需要很长时间，但绝对值得😁 我想添加更多具有各种身体结构和尺寸的机器人，当然还有更多的战斗风格。    提交人    /u/bmind7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9l5q3/ive_trained_robots_to_fight_with_rl/</guid>
      <pubDate>Mon, 22 Jul 2024 18:13:59 GMT</pubDate>
    </item>
    <item>
      <title>强化学习，您可以在奖励函数空间中同时进行优化。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9kmdo/reinforcement_learning_where_you_simultaneously/</link>
      <description><![CDATA[考虑一个问题，其中您有一个固定的环境，其中状态为 s，动作为 a。我们还有一个奖励函数 R(s; v)，除了依赖于状态 s 之外，还具有可调参数 v。例如，假设 R(s;v) 是一个神经网络，它接受 s 并产生奖励，而 v 是它的权重。 给定一个固定的 v，训练一个在 R(v,s) 上表现良好的代理是一个标准的 RL 问题。 但是，我还有第二个目标 O(v) = E_s ( F[ R(v,s) ] ) ，其中 E_s 是针对该奖励进行训练的代理的最佳表现的期望值，我希望优化以获得最佳的 v。 简单地说，这个问题的解决方法是逐一运行所有 v，为该问题训练一个代理，计算 O(v)，然后重复。 但是，似乎很明显应该可以实现某种迭代算法，在其中可以同时改进奖励函数和代理，直到两者都收敛。 有人能帮我找到这种方法的名称或一些参考文献吗？    提交人    /u/Acceptable_Trainer53   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9kmdo/reinforcement_learning_where_you_simultaneously/</guid>
      <pubDate>Mon, 22 Jul 2024 17:52:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么 PPO 可以学习而 SAC 却会失败呢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9h49d/why_would_sac_fail_where_ppo_can_learn/</link>
      <description><![CDATA[      大家好， 我有这个我编写的超级简单的 Env。我已经设法用 SB3 PPO 训练了一个代理，但仍然无法达到情节长度 120 步。此外，奖励低于理论最大值 0.37。 我决定尝试 SAC，并使用默认学习参数从 PPO 更改为 SAC。我是 RL 的初学者，因此当我的尝试失败时我并不感到非常惊讶，但我想了解以下内容表示什么。这是从 SAC 中学习到的，平均奖励和情节长度下降并卡在某个水平。 显然，由于我使用默认学习参数并且是新手，也许我不应该期望 SAC 开箱即用，我想了解的是这种学习告诉我什么？ PPO vs SAC。相同环境。    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9h49d/why_would_sac_fail_where_ppo_can_learn/</guid>
      <pubDate>Mon, 22 Jul 2024 15:28:44 GMT</pubDate>
    </item>
    <item>
      <title>用于强化学习的可视化节点编程工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e9agso/visual_nodes_programming_tool_for_reinforcement/</link>
      <description><![CDATA[目前存在用于机器学习的可视化编程工具，如 Visual Blocks。但是我还没有看到任何专门用于强化学习的工具。在我看来，现有的工具（如 Visual Blocks）对强化学习来说并不是很好。 拥有一个用于强化学习的可视化编程工具可能会很有用，因为它可以让开发人员快速制作原型和调试强化学习模型。 我正在考虑制作这样一个工具，它将支持现有的强化学习库，如 Tensorforce、Stable Baselines、RL_Coach 和 OpenAI Gym。 你们觉得这个想法怎么样？你知道这是否已经存在，它是否对你的职业或业余项目有用？    提交人    /u/Charming-Quiet-2617   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e9agso/visual_nodes_programming_tool_for_reinforcement/</guid>
      <pubDate>Mon, 22 Jul 2024 10:00:57 GMT</pubDate>
    </item>
    <item>
      <title>我找不到在 Twitter 上看到的有关新算法的帖子 (x)。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e99tmb/i_cant_find_a_post_about_the_new_algorithm_that_i/</link>
      <description><![CDATA[昨天我看到了一篇帖子，但没有保存（是的，那是一个错误），它写了一种新算法，并且成功地在 2 分钟内训练了智能体运行。我认为它是 Mujoco 环境，其中有一个类人机器人、一只蜘蛛和所有可以行走的智能体。找到它会很不错。如果您看到这篇帖子，请发送链接。 “2 分钟”肯定写在那里。 而且它也可能“近端”“扩散”“优化”。 下面是视频结果，有智能体在跑来跑去。 谢谢。    提交人    /u/imitagent   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e99tmb/i_cant_find_a_post_about_the_new_algorithm_that_i/</guid>
      <pubDate>Mon, 22 Jul 2024 09:17:47 GMT</pubDate>
    </item>
    <item>
      <title>使用自定义 Python 和 Unity 引擎完成的虚拟 AI 实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</link>
      <description><![CDATA[       由    /u/Inexperienced-Me  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1e8ongm/virtual_ai_lab_done_with_custom_python_and_unity/</guid>
      <pubDate>Sun, 21 Jul 2024 15:19:07 GMT</pubDate>
    </item>
    </channel>
</rss>