<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Fri, 24 Jan 2025 03:19:00 GMT</lastBuildDate>
    <item>
      <title>为什么要对卷出缓冲区数据进行混排？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</link>
      <description><![CDATA[在 SB3 的循环缓冲区文件 (https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/recurrent/buffers.py) 中，第 182 行表示要在保留序列的同时对数据进行混洗，代码会在随机点拆分数据，交换每个拆分，然后将其重新连接在一起。  我的问题是，为什么这对于混洗来说已经足够好了，但我们为什么要首先对推出的数据进行混洗呢？   由    /u/AUser213  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</guid>
      <pubDate>Thu, 23 Jan 2025 23:31:30 GMT</pubDate>
    </item>
    <item>
      <title>IsaacSim 人形机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</link>
      <description><![CDATA[我需要一些帮助在 IsaacSim 中构建人形机器人演示，但除了开箱即用的人形机器人 (H1) 之外没有其他可用的东西，有人对 Neo、Sanctuary 等机器人的人形机器人政策有任何线索吗    提交人    /u/sohaib_01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</guid>
      <pubDate>Thu, 23 Jan 2025 21:18:29 GMT</pubDate>
    </item>
    <item>
      <title>aiXplain 的 Evolver：通过自主优化彻底改变代理 AI 系统 🚀</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i899m4/aixplains_evolver_revolutionizing_agentic_ai/</link>
      <description><![CDATA[嗨，RL 社区！👋 我们都知道 Agentic AI 系统在自动化流程和增强各行业决策方面具有多么大的变革性。但问题是：代理角色、任务和工作流程的手动微调一直是一个主要障碍。aiXplain 的 Evolver – 我们正在申请专利的完全自主框架，旨在改变游戏规则。 💡 aiXplain 的 Evolver 是一款下一代工具，它：  🔄 自主优化工作流程：通过自动微调 Agentic AI 系统，无需人工干预。 📈 利用 LLM 驱动的反馈循环：使用高级语言模型评估输出、提供反馈并推动持续改进。 🚀 提高效率和可扩展性：比以往更快地实现 AI 系统的最佳配置。  🌟 为什么重要 我们已在多个领域应用了 Evolver，并看到了令人惊叹的结果。以下是一些亮点： 1️⃣ 市场研究：市场分析师等专业角色提高了准确性并使策略与趋势保持一致。 2️⃣ 医疗保健 AI：提高了法规遵从性和可解释性，以更好地吸引患者。 3️⃣ 职业转型：帮助软件工程师以明确的目标和量身定制的专业知识转向 AI 角色。 4️⃣ 供应链外展：通过高级分析优化电子商务解决方案的外展策略。 5️⃣ LinkedIn 内容创建：创建以受众为中心的帖子，推动对 AI 趋势的参与。 6️⃣ 药物发现：为制药公司提供与利益相关者一致的见解。 7️⃣ EdTech 潜在客户生成：通过个性化的学习见解提高潜在客户质量。 每个案例研究都展示了由 Evolver 提供支持的专业角色和持续改进如何带来更高的评估分数和更好的结果。 📚 对技术细节感到好奇吗？在 Arxiv 上查看：通过迭代细化和 LLM 驱动的反馈循环自主优化代理 AI 解决方案的多 AI 代理系统 🔍 你怎么看？ 你如何看待这样的工具塑造 AI 工作流程的未来？你认为 Evolver 可以在哪些行业或特定用例中发挥巨大作用？期待听到你的想法。😊    提交人    /u/k_yuksel   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i899m4/aixplains_evolver_revolutionizing_agentic_ai/</guid>
      <pubDate>Thu, 23 Jan 2025 17:50:02 GMT</pubDate>
    </item>
    <item>
      <title>关于井字游戏中的贝尔曼方程。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i86uq1/about_bellman_equation_in_tic_tac_toe_game/</link>
      <description><![CDATA[一般来说，贝尔曼方程是 target_Q = Q(state, action) + gamma * Q(next_state, action) 但是，我很好奇我们是否应该使用 -gamma 而不是 gamma，因为下一个玩家是对手。如果我们添加其最大 q 值，我认为这没有意义，因为我们将对手的最大 q 值添加到此回合的 q 值中。  但我在互联网上找到了很多代码，他们会使用 target_Q = Q(state, action) + gamma * Q(next_state, action) 而不是 target_Q = Q(state, action) - gamma * Q(next_state, action)。为什么？    提交人    /u/Upstairs-Lead-2601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i86uq1/about_bellman_equation_in_tic_tac_toe_game/</guid>
      <pubDate>Thu, 23 Jan 2025 16:09:51 GMT</pubDate>
    </item>
    <item>
      <title>需要无人机模拟环境方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i820dg/need_some_help_with_simulation_environments_for/</link>
      <description><![CDATA[大家好，我目前正在模拟基于视觉的 SLAM 设置，用于在 GPS 拒绝环境中模拟 UAV。这意味着我计划使用仅接受两个传感器输入的 SLAM 算法：相机和 IMU。我需要帮助为这个项目选择正确的模拟环境。环境必须具有适用于相机和 IMU 的良好传感器模型，并且 3D 世界必须尽可能接近现实。我排除了带有 UE4 设置的 Airsim，因为 Microsoft 已存档 Airsim，并且不支持 UE5。当我尝试 UE4 时，我找不到要导入的 3D 世界，因为 UE 已升级其市场。 任何有关模拟环境的建议以及教程链接都将非常有帮助！此外，如果有人知道如何让 UE4 适用于这种应用程序，即使是这样也欢迎！    提交人    /u/techgeek1216   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i820dg/need_some_help_with_simulation_environments_for/</guid>
      <pubDate>Thu, 23 Jan 2025 12:15:38 GMT</pubDate>
    </item>
    <item>
      <title>民意调查：视频游戏 RL 的最佳框架？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7zxga/poll_best_frameworks_for_video_game_rl/</link>
      <description><![CDATA[各位强化学习老师们，大家好！您知道或使用哪些工具在现代闭源视频游戏上进行强化学习？我说的强化学习纯粹是从视频帧开始的，无法访问内部游戏状态。您是否使用任何特定的策略和算法来解决昂贵且缓慢的数据收集问题？是否有任何特定的技术适用于 FPS、ARPG 等游戏类型？如何使用导航菜单处理级别之间的视觉差异？用于模拟游戏手柄和键盘的库？ 我认为这对于业余项目来说是一个非常有趣的话题，我已经看到了一些相关的帖子。对这些方法非常好奇。    提交人    /u/mjolk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7zxga/poll_best_frameworks_for_video_game_rl/</guid>
      <pubDate>Thu, 23 Jan 2025 09:51:08 GMT</pubDate>
    </item>
    <item>
      <title>乐观的初始值如何鼓励探索？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7xig0/how_do_optimistic_initial_values_encourage/</link>
      <description><![CDATA[我正在研究（更新的）Sutton&amp;Barto 的书。 在 2.6 中，它说初始估计 +5 过于乐观。但这种乐观鼓励行动价值方法进行探索....即使一直选择贪婪动作，系统也会进行大量探索 这本书只讨论了一个常数 epsilon，其中以恒定概率选择随机动作。 所以，我不太明白乐观的 Q1 值和探索之间的关系。有人可以用简单的术语解释一下吗？    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7xig0/how_do_optimistic_initial_values_encourage/</guid>
      <pubDate>Thu, 23 Jan 2025 06:44:25 GMT</pubDate>
    </item>
    <item>
      <title>对于嘈杂观察环境有什么建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7rlhz/suggestions_for_noisy_observation_environments/</link>
      <description><![CDATA[嗨，我正在探索具有噪声观测的 RL。我在 OpenAI Gym Atari 中向像素添加了高斯噪声，但感觉太简单了。 对环境或更逼真的噪声模型有什么建议吗？有关高级噪声（例如遮挡、结构化噪声）或相关基准的提示将不胜感激。谢谢！    提交人    /u/AdministrativeCar545   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7rlhz/suggestions_for_noisy_observation_environments/</guid>
      <pubDate>Thu, 23 Jan 2025 01:14:05 GMT</pubDate>
    </item>
    <item>
      <title>这就是“糟糕”的奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</link>
      <description><![CDATA[        由    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</guid>
      <pubDate>Wed, 22 Jan 2025 19:48:44 GMT</pubDate>
    </item>
    <item>
      <title>关于 RL 代理控制其他 RL 代理的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7j9ck/question_about_rl_agents_controlling_other_rl/</link>
      <description><![CDATA[      嗨，我是强化学习领域的初学者，目前对基于物理的运动控制感兴趣。 当我查看各种众所周知的环境（例如机械臂）时，我想到了一个问题，即如何尝试在基于物理的环境中表现良好，涉及控制此类模型以实现比简单地到达某个目的地更抽象的复杂任务。具体来说，该问题出现在这篇论文中，问题场景的图像如下所示。 https://preview.redd.it/wrvz16y7flee1.png?width=612&amp;format=png&amp;auto=webp&amp;s=5c24cb87a696247929aff41e8775833c617b9218 例如，假设我要创建一个物理模拟环境，其中机械臂旨在在线 3D 装箱问题场景中表现良好，其中机械臂从传送带上抓取各种尺寸的盒子并将它们放置在指定位置，尝试在受限空间内容纳尽可能多的盒子。（我想我可以将奖励建模为与放置的盒子凸包的体积相关？） 我可以想象，采用不同代理的多层方法可能会充分发挥作用，一个用于解决 3D-BPP 问题，一个用于控制机械臂的各个电机以将盒子移动到某个位置，以便 3D-BPP 求解器的输出可以作为机械臂控制器代理的输入。但是，我无法想象这两个代理会完全解耦，因为 3D-BPP 求解器的某些命令可能在物理上不适合机械臂的移动，而不会破坏先前放置的盒子。 在这种情况下，我想知道通常的方法是什么：  使用单个代理能够独自控制这些看似不同的任务（求解 3d-bpp 和控制机械臂）？ 实际上使用两个代理并在训练序列中引入一些复杂性，以便求解器可以考虑机械臂控制器的运动？  如果这是一个微不足道的问题，任何我可以阅读的适合初学者的文献链接都将不胜感激！    提交人    /u/RulerOfCakes   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7j9ck/question_about_rl_agents_controlling_other_rl/</guid>
      <pubDate>Wed, 22 Jan 2025 19:15:47 GMT</pubDate>
    </item>
    <item>
      <title>强化学习算法的问题/解决方案参考指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7hpnm/a_problemsolution_reference_guide_for_rl/</link>
      <description><![CDATA[在学习 RL 课程时，我为几种算法创建了一个参考，并简要描述了它们解决了哪些限制。示例： 问题：SARSA 将 q 值推向当前策略，但理想情况下，我们想要的是最优值。 解决方案：在 TD 目标计算中使用最佳操作 -&gt; Q 学习 也许其他人会发现它很有用！可在 https://jakubhalmes.substack.com/p/reinforcement-learning-a-reference 获得    提交人    /u/jac08_h   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7hpnm/a_problemsolution_reference_guide_for_rl/</guid>
      <pubDate>Wed, 22 Jan 2025 18:14:36 GMT</pubDate>
    </item>
    <item>
      <title>缩短 REINFORCE 的期限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7h25q/shortening_the_horizon_in_reinforce/</link>
      <description><![CDATA[大家好。我正在研究对具有动态状态（生成的状态是针对先前状态采取行动的结果）的建筑物进行强化学习，并且我正在使用纯 REINFORCE 算法并存储（s、a、r）转换。如果我想将一个时期分成几个情节，比如 10 个情节，（先前：一次运行 4000 个时间步，然后参数更新 --&gt;现在：400 个时间步，更新，另一个 400 个时间步，更新...），除了更改存储转换操作和学习函数的位置外，我还应该注意哪些事情才能正确进行此更改？您能告诉我可以学习的任何来源吗？谢谢。（我的 NN 框架在 Tensorflow 1.10 中）。    提交人    /u/Araf_fml   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7h25q/shortening_the_horizon_in_reinforce/</guid>
      <pubDate>Wed, 22 Jan 2025 17:48:30 GMT</pubDate>
    </item>
    <item>
      <title>硕士学位决定</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7eiqz/masters_degree_decision/</link>
      <description><![CDATA[如果我有兴趣深入了解强化学习，有人能告诉我在欧洲哪里攻读硕士学位会更有益吗？    提交人    /u/Ok-Engineering4612   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7eiqz/masters_degree_decision/</guid>
      <pubDate>Wed, 22 Jan 2025 16:04:45 GMT</pubDate>
    </item>
    <item>
      <title>TD3 奖励不会随着时间的推移而增加</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7cxi6/td3_reward_not_increasing_over_time/</link>
      <description><![CDATA[      嘿，对于一个大学项目，我已经实现了 td3，并尝试在使用分配的环境之前在 pendulum v1 上对其进行测试。 这是我的超参数列表：  &quot;actor_lr&quot;: 0.0001, &quot;critic_lr&quot;: 0.0001, &quot;discount&quot;: 0.95, &quot;tau&quot;: 0.005, &quot;batch_size&quot;: 128, &quot;hidden_​​dim_critic&quot;: [256, 256], &quot;hidden_​​dim_actor&quot;: [256, 256], &quot;noise&quot;: &quot;Gaussian&quot;, &quot;noise_clip&quot;: 0.3, &quot;noise_std&quot;: 0.2, &quot;policy_update_freq&quot;: 2, &quot;buffer_size&quot;: int(1e6),  我面临的问题是奖励随着时间的推移不断减少，并且达到饱和在一些剧集之后，大约在 -1450 左右。有人知道我的问题可能出在哪里吗？ 如果需要，我还可以提供您怀疑可能有错误的任何代码 随着时间的流逝奖励 提前感谢您的帮助！    提交人    /u/bela_u   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7cxi6/td3_reward_not_increasing_over_time/</guid>
      <pubDate>Wed, 22 Jan 2025 14:57:18 GMT</pubDate>
    </item>
    <item>
      <title>推送任务未学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7717b/pusher_task_not_learning/</link>
      <description><![CDATA[我正在尝试在 mujoco 推送器环境中训练一个模型，但它不起作用。基本上，我从 mujoco github repo 获得了推送器类并做了一些小改动。我试图实现的是让推送器将 3 个对象推送到 3 个不同的目标。这些对象一次出现一个，所以当第一个对象被推送到目标时，第二个对象就会出现，依此类推。所以我对 mujoco 提供的类做的唯一修改是我添加了在视图中更改要推送对象的机制。我尝试了 PPO 和 SAC，时间步长为 100 万，奖励仍然为负。这看起来像是一项简单的任务，但它不起作用    提交人    /u/Latinotech   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7717b/pusher_task_not_learning/</guid>
      <pubDate>Wed, 22 Jan 2025 09:01:57 GMT</pubDate>
    </item>
    </channel>
</rss>