<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 26 Nov 2024 15:19:34 GMT</lastBuildDate>
    <item>
      <title>管理 Databricks 中 AI 工作负载的 GPU 资源是一场噩梦！还有其他人吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0d30b/managing_gpu_resources_for_ai_workloads_in/</link>
      <description><![CDATA[我不知道你们怎么想，但管理 Databricks 中机器学习工作负载的 GPU 资源对我来说简直就是地狱。 😤 我是一家电子商务公司的 DevOps 团队的一员，在不浪费钱在空闲的 GPU 上和不让性能在高峰期崩溃之间不断取得平衡让我抓狂。 情况如下： 机器学习工作负载是不可预测的。有一天，你的需求很低，GPU 闲置在那里无所事事，成本不断增加。  然后 BAM 💥 – 第二天，工作量激增，而您的配置不足，突然每个人的模型都在爬行，因为我们没有足够的资源来跟上，顺便说一句，这种情况发生在黑色星期五。 那么我们该怎么办？显然，我们会手动调整集群大小。 但是我不能把每个小时都花在照看集群指标上，并猜测工作量高峰何时到来，顺便说一句，这很无聊。 要么我们在闲置资源上浪费钱，要么我们在争先恐后地扩大规模，把性能抛到九霄云外。这是一个双输的局面。 让我震惊的是，对于真正适用于 AI 工作负载的 GPU 资源，没有真正的自动扩展解决方案。 CPU 扩展很好，但 GPU？不行。  您只能靠自己了。在没有实际工具帮助的情况下提前预测需求就像试图猜测一周后的天气一样。 我见过一些解决方案，但大多数要么太复杂，要么没有完全解决问题。 我只想要一些简单的东西：自动化、实时扩展，不会破坏我们的预算或工作量时间表。 这个要求过分吗？！ 还有其他人经历过同样的痛苦吗？ 您如何在不花费 24/7 调整集群的情况下管理这个问题？ 如果有人找到了更好的方法（或者至少您也经历了同样的挣扎），我很想知道。    提交人    /u/Firass-belhous   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0d30b/managing_gpu_resources_for_ai_workloads_in/</guid>
      <pubDate>Tue, 26 Nov 2024 14:26:08 GMT</pubDate>
    </item>
    <item>
      <title>有人能够使用支持 GPU 的 PyTorch 和 Carla 模拟器训练 RL 模型吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h0aldv/has_someone_been_able_to_use_gpuenabled_pytorch/</link>
      <description><![CDATA[Carla 支持的最新 Python 版本是 3.8.0，对于 PyTorch GPU 加速来说太旧了。我曾尝试将我的 Carla 代码打包到服务器中，但速度太慢了。有什么建议吗？    提交人    /u/AdhesivenessSmall333   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h0aldv/has_someone_been_able_to_use_gpuenabled_pytorch/</guid>
      <pubDate>Tue, 26 Nov 2024 12:22:12 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 演员在评估期间总是采取相同的动作。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1h07zyb/ddpg_actor_always_taking_same_action_during/</link>
      <description><![CDATA[我正在使用自定义环境。其中状态表示为 (x1, x2)，动作为 (delta_x1, delta_x2)，下一个状态为 (x1+delta_x1, x2+ delta_x2)。有奖励。在训练期间，参与者也会多次到达状态空间的边界。我知道很多人都遇到过同样的问题，比如在 DDPG 中，参与者总是采取相同的动作。您的实现中存在什么问题，您是如何解决的？此外，任何其他帮助也非常感谢。提前致谢。     提交人    /u/Adventurous_Fly_5564   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1h07zyb/ddpg_actor_always_taking_same_action_during/</guid>
      <pubDate>Tue, 26 Nov 2024 09:30:42 GMT</pubDate>
    </item>
    <item>
      <title>星际争霸母巢之战</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzvetr/starcraft_broodwar/</link>
      <description><![CDATA[Hello RL World! 我是星际争霸 Broodwar（来自韩国）的超级粉丝，自从它在 90 年代末首次推出以来，当时我还是个孩子。快进 24 年，在获得计算机科学学士学位后，我在不同的公司从事后端领域的工作，主要从事分布式系统/数据库工作 10 年。而现在，我仍然在观看 Broodwar 职业联赛。 9 年前我在韩国偶然发现了 AlphaGo（时间过得真快），当时我对人工智能产生了兴趣，但围棋不是我感兴趣的东西，所以兴趣逐渐消退，直到 AlphaStar 出现征服了星际争霸 II。然而现在我发现，在 Broodwar 中，我并没有看到太多在 APM 方面与人类相似的人工智能系统，这些人工智能系统经过训练可以挑战 Broodwar 传奇人物（比如 Flash、Bisu、Stork 等），所以我至少想了解一下为什么它还没有浮出水面挑战这些传奇人物。是训练模型的成本吗？还是 Broodwar API 上的挑战？ 我做后端工程师已经 10 年了，但我目前对 RL 还是新手，所以我刚从亚马逊买了本书《Grokking the Deep Reinforcement Learning (Morales)》并开始阅读（这是一个好的开始吗）？    提交人    /u/fsw0422   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzvetr/starcraft_broodwar/</guid>
      <pubDate>Mon, 25 Nov 2024 22:06:41 GMT</pubDate>
    </item>
    <item>
      <title>单步演员评论算法（RL 书）在 Cartpole 环境中无法按预期运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzmrry/onestep_actorcritic_algorithm_rl_book_not_working/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzmrry/onestep_actorcritic_algorithm_rl_book_not_working/</guid>
      <pubDate>Mon, 25 Nov 2024 16:24:25 GMT</pubDate>
    </item>
    <item>
      <title>Unity MLAgents 在一个简单的益智游戏上艰难地进行训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzffaj/unity_mlagents_struggle_to_train_on_a_simple/</link>
      <description><![CDATA[      https://preview.redd.it/uva3kh8zp03e1.png?width=677&amp;format=png&amp;auto=webp&amp;s=9f838885a8d433c50e324c68c681a006855448ad 我正在尝试在我的 Unity 益智游戏项目上训练一个代理，游戏的工作原理如下； 您需要发送与当前总线匹配的颜色。您只能扮演路径未被阻挡的角色。您有 5 个位置可以为后面的角色或错误的玩法腾出空间。 到目前为止我尝试过的； 我已经研究了大约一个月，但到目前为止没有成功。 我从矢量观察开始，并输入了瓷砖颜色、状态、当前总线颜色等。但是没有用。这太复杂了。每次失败时，我都会简化观察状态和设置。有一次，我只给了代理 1 和 0，这是它应该学会玩的部分，只有 1 值可以玩，因为我正在检查可玩状态以及颜色是否匹配。我也使用动作蒙版。我无法在这种简单的设置上训练它，这是一种战斗和挫败感。我甚至简化到了当它犯错时结束剧集的地步，我会给它负面奖励并结束剧集。我希望它选择正确的棋子，而不关心玩关卡和制定策略。但它在训练有素的关卡上玩得很好，但它过度拟合，记住了它们。在测试级别，即使是简单的也无法正确执行。  我已经开始深入研究应该如何处理它，并查看 Unity MLAgents 示例中的 match-3 示例。我了解到对于类似网格的结构，我需要使用 CNN，并且我已经创建了自定义传感器，现在进行视觉观察，例如在 20x20 的网格上放置 40 层信息。11 个颜色层 + 11 个总线颜色层 + 可以移动层 + 不能移动层等。我尝试了简单的视觉编码和 match3，但仍然无法对其进行一些训练。  我的问题是；在 RL 上训练这种益智游戏难吗？因为在 Unity 示例中有很多复杂的游戏玩法，即使给代理的帮助较少，它也能快速学习。还是我在核心方法中做错了什么？ 这是我目前使用的配置，但我已经尝试了很多东西，我已经改变并尝试了几乎每一种方法； ``` behaviors：AIAgentBehavior：trainer_type：ppo hyperparameters：batch_size：256 buffer_size：2560 #buffer_size = batch_size * 8 learning_rate：0.0003 beta：0.005 epsilon：0.2 lambd：0.95 num_epoch：3 shared_critic：False learning_rate_schedule：linear beta_schedule：linear epsilon_schedule：linear network_settings：normalize：True hidden_​​units：256 num_layers：3 vis_encode_type：match3 #conv_layers：#-filters： 32 # kernel_size: 3 # 步幅：1 # - 过滤器：64 # kernel_size: 3 # 步幅：1 # - 过滤器：128 # kernel_size: 3 # 步幅：1 确定性：False reward_signals： 外部： gamma：0.99 强度：1.0 # network_settings： # normalize：True # hidden_​​units：256 # num_layers：3 # # memory：None # deterministic：False # init_path：None keep_checkpoints：5 checkpoint_interval：50000 max_steps：200000 time_horizo​​n：32 summary_freq：1000 threaded：False  ```    提交人    /u/menelaus35   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzffaj/unity_mlagents_struggle_to_train_on_a_simple/</guid>
      <pubDate>Mon, 25 Nov 2024 09:57:37 GMT</pubDate>
    </item>
    <item>
      <title>请帮助我理解强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gze5c6/please_help_me_understand_reinforcement_learning/</link>
      <description><![CDATA[我不太了解强化学习以及它与无监督学习的区别，我见过的所有使用强化学习的例子在我看来都可以使用无监督学习来完成。从某种意义上说，强化学习不是也在寻找合作伙伴吗？你能解释一下你会在哪些地方使用强化而不能使用其他任何东西吗？另外，在我的课程笔记中，它说强化学习使用监督作为随着时间的推移的奖励，我不明白监督怎么会成为奖励。谢谢！     提交人    /u/snow_ice_storm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gze5c6/please_help_me_understand_reinforcement_learning/</guid>
      <pubDate>Mon, 25 Nov 2024 08:19:58 GMT</pubDate>
    </item>
    <item>
      <title>网络中可以有比批评家更多的人吗？那会怎样？（见描述）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gzcmfk/can_we_have_more_than_critics_in_networks_how/</link>
      <description><![CDATA[只要考虑一下理论上无限的计算成本，是否存在像多个 Q 学习算法这样效果更好的算法？或者多个评论家网络效果更好？    提交人    /u/palavi_10   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gzcmfk/can_we_have_more_than_critics_in_networks_how/</guid>
      <pubDate>Mon, 25 Nov 2024 06:31:32 GMT</pubDate>
    </item>
    <item>
      <title>“无需经验回放、目标网络或批量更新的深度强化学习”，Elsayed 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gz67aa/deep_reinforcement_learning_without_experience/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gz67aa/deep_reinforcement_learning_without_experience/</guid>
      <pubDate>Mon, 25 Nov 2024 00:36:29 GMT</pubDate>
    </item>
    <item>
      <title>关于可靠性分析中非 Atari 基准的规范化方法的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gygc0w/question_on_normalization_methods_for_nonatari/</link>
      <description><![CDATA[大家好， 我目前正在使用 Rliablehttps://github.com/google-research/rliable 分析 DeepMind Control Suite (DMC) 和 PyBullet 等环境中的强化学习结果。与 Atari 基准测试不同，这些环境没有人工归一化的分数来标准化算法之间的比较。例如，我正在使用 SARC 等最新算法，缺乏这样的基线使得确保公平和一致的评估变得具有挑战性。 我正在考虑使用 Z 分数归一化和百分位数归一化作为比较不同 RL 算法的潜在解决方案，但我不确定这些方法是否理想或是否符合 Rliable 倡导的统计严谨性。 有人有这方面的经验或在这种情况下的最佳实践建议吗？如果您能提供其他适用于此情况的强大规范化方法的见解或建议，我将不胜感激。 感谢您的时间和想法！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gygc0w/question_on_normalization_methods_for_nonatari/</guid>
      <pubDate>Sun, 24 Nov 2024 02:14:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的经验设计</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gy8tdy/empirical_design_in_reinforcement_learning/</link>
      <description><![CDATA[  由    /u/bulgakovML  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gy8tdy/empirical_design_in_reinforcement_learning/</guid>
      <pubDate>Sat, 23 Nov 2024 20:19:42 GMT</pubDate>
    </item>
    <item>
      <title>[R] 模拟引理的最佳紧密度界限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gy4g4a/r_an_optimal_tightness_bound_for_the_simulation/</link>
      <description><![CDATA[https://arxiv.org/abs/2406.16249（也在 RLC 上提出） 模拟引理是强化学习中广泛使用的基础结果，它限制了相对于模型错误指定的值估计误差。但正如许多人所注意到的，它提供的界限非常宽松，尤其是对于较大的错误指定或高折扣（见图 2）。直到现在！ 关键思想是，每次你对最终结果的判断错误时，你未来出错的概率就会降低。传统的模拟引理证明没有考虑到这一点，因此假设你可以永远在每个时间步中错误指定相同的 epsilon 概率质量（这就是为什么它对于长视野或较大的错误指定很宽松的原因）。利用这一观察，我们可以得到一个最佳的紧密界限。 我们的界限取决于与原始模拟引理相同的数量，因此应该能够插入当前使用原始引理的任何地方。希望大家喜欢！    提交人    /u/asdfwaevc   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gy4g4a/r_an_optimal_tightness_bound_for_the_simulation/</guid>
      <pubDate>Sat, 23 Nov 2024 17:09:35 GMT</pubDate>
    </item>
    <item>
      <title>我应该关注什么基于四轴飞行器的 RL 模拟环境？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gy3544/what_simulation_environment_should_i_be_looking/</link>
      <description><![CDATA[我将列出我考虑过的那些以及它们的局限性（据我所知）  Flightmare：似乎是总体上最好的选择，灵活的渲染和物理特性可以真正使用所有选项。但不幸的是，它似乎不再受支持，并且他们的 repo 充满了未解决的问题。 Isaac Sim/Pegasus：运行成本极高，因为它建立在 nvidia omniverse 之上。 Gazebo：缓慢且渲染设置有限 AirSim：不再受支持。 Mujoco：渲染极其有限，没有对传感器的原生支持，但速度非常快。  请让我知道您的想法，以及这个问题是否不适合该子版块。我也非常希望得到关于如何将 rl 算法集成到无人机的 ROS 包中的任何提示，因为我对机器人技术和模拟完全陌生。    提交人    /u/LowStatistician11   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gy3544/what_simulation_environment_should_i_be_looking/</guid>
      <pubDate>Sat, 23 Nov 2024 16:12:55 GMT</pubDate>
    </item>
    <item>
      <title>帮我创建一个关于如何选择强化学习算法的决策树</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxwwfd/help_me_create_a_decision_tree_about_how_to/</link>
      <description><![CDATA[      嘿！我是一名大学教授，我想在未来几年创建一个强化学习专业化课程。 我设法理解了各种经典算法，但我真的不知道在什么时候使用哪一种。我正在尝试在 chatgpt 的帮助下创建一个决策树。我可以得到您的一些评论和更正吗？    提交人    /u/Dougdaddyboy_off   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxwwfd/help_me_create_a_decision_tree_about_how_to/</guid>
      <pubDate>Sat, 23 Nov 2024 10:41:10 GMT</pubDate>
    </item>
    <item>
      <title>最近有任何关于基础 RL 改进的研究吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1gxudhp/any_research_regarding_the_fundamental_rl/</link>
      <description><![CDATA[我一直在 Google Scholar 上关注几位最负盛名的 RL 研究人员，我注意到他们中的许多人近年来已将重点转移到与 LLM 相关的研究上。 哪篇论文最引人注目，推动了 RL 的根本性改进？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1gxudhp/any_research_regarding_the_fundamental_rl/</guid>
      <pubDate>Sat, 23 Nov 2024 07:40:34 GMT</pubDate>
    </item>
    </channel>
</rss>