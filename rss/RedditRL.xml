<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 19 Dec 2024 12:34:38 GMT</lastBuildDate>
    <item>
      <title>“MaxInfoRL：通过信息增益最大化促进强化学习中的探索”，Sukhija 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhms96/maxinforl_boosting_exploration_in_reinforcement/</guid>
      <pubDate>Thu, 19 Dec 2024 06:30:27 GMT</pubDate>
    </item>
    <item>
      <title>SAC 代理在我们的自定义视频游戏环境中没有学到任何东西 - 救命！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hhbkza/sac_agent_not_learning_anything_inside_our_custom/</guid>
      <pubDate>Wed, 18 Dec 2024 20:57:41 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 和线下强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</link>
      <description><![CDATA[由于 LLM 模型以某种方式进行训练，例如行为克隆，那么使用离线 RL 进行训练怎么样？ 我知道奖励设计将是一个重大挑战和可扩展性等。 你怎么看？    提交人    /u/Blasphemer666   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh6r1h/llm_offlinerl/</guid>
      <pubDate>Wed, 18 Dec 2024 17:29:39 GMT</pubDate>
    </item>
    <item>
      <title>Isaac健身房没有联系方式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh3rgi/no_contact_information_in_isaac_gym/</link>
      <description><![CDATA[有没有人有使用 Isaac gym 的经验，我正在使用 physx 引擎来获取两个刚体的接触信息，但无法获取。当我使用 flex 引擎并将软体与相同的刚体一起使用时，我确实会得到软接触。如果有人能分享他们对此的想法，那真的很有帮助???    提交人    /u/Horror_Photo8119   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh3rgi/no_contact_information_in_isaac_gym/</guid>
      <pubDate>Wed, 18 Dec 2024 15:17:17 GMT</pubDate>
    </item>
    <item>
      <title>努力训练用于路线优化的 Dueling DQN 模型 – 需要有关学习和计算要求的建议 😢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</link>
      <description><![CDATA[我正在使用 Dueling DQN 在自定义道路网络环境中进行路线优化项目，该环境具有许多节点和不同的动作空间。但是，该模型无法正确学习 - 训练结果不一致，并且代理难以找到最佳路径。 有人有兴趣贡献吗     提交人    /u/ProfessionalType9800   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hh39qs/struggling_to_train_a_dueling_dqn_model_for_route/</guid>
      <pubDate>Wed, 18 Dec 2024 14:54:11 GMT</pubDate>
    </item>
    <item>
      <title>强化学习前提条件方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgzrvg/help_on_prerequisites_for_reinforcement_learning/</link>
      <description><![CDATA[大家好！ 我已经完成了控制系统硕士学位，并将于 2025 年夏季开始攻读博士学位。由于我对控制系统中的 ML/数据驱动方法感兴趣，我的研究主管要求我在正式开始攻读博士学位之前研究强化学习（作为有前途的研究领域之一）。  根据我的理解，理解强化学习的先决条件是概率和统计、微积分和线性代数（如果我错了，请随时纠正我）。我对微积分和线性代数有很好的了解，但我在本科或硕士阶段没有学过任何概率和统计课程。（请随意添加除上述先决条件之外的任何其他先决条件和学习这些先决条件的良好资源。） 有很多可用于学习概率和统计的资源，但我不知道从工程的角度来看，其中哪些对理解强化学习真正有帮助。因此，如果您能推荐任何资源（视频讲座和/或书籍等）来帮助我了解概率和统计的概念，我将不胜感激。在我开始学习强化学习之前，请告诉我是否有任何我需要了解的概率和统计具体主题。     提交人    /u/reddit_agg   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgzrvg/help_on_prerequisites_for_reinforcement_learning/</guid>
      <pubDate>Wed, 18 Dec 2024 11:41:30 GMT</pubDate>
    </item>
    <item>
      <title>使用 DQN 训练代理进行棋盘游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgy9g3/training_agent_with_dqn_for_board_game/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgy9g3/training_agent_with_dqn_for_board_game/</guid>
      <pubDate>Wed, 18 Dec 2024 09:47:16 GMT</pubDate>
    </item>
    <item>
      <title>David Silver 示例考试问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</link>
      <description><![CDATA[      大家好， 我正在查看 David Silver 网站上的练习考试，但似乎无法理解本页最后一个问题的解决方案。对于状态一的 lambda 返回，它不应该是 0.5**2 x 1 而不是 0.5 x 1。之后，我完全不知道状态 2 和 3 的返回值了。    提交人    /u/LostBandard   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgrl82/david_silver_example_exam_question/</guid>
      <pubDate>Wed, 18 Dec 2024 02:29:26 GMT</pubDate>
    </item>
    <item>
      <title>强化学习代理趋向于不做任何事/负奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgq3z8/rl_agent_converging_on_doing_nothing_negative/</link>
      <description><![CDATA[大家好 - 我正在使用 gymnasium、stable baselines 3 和 pyboy 创建一个代理来玩 NES/GBC 游戏 1942。我在训练中遇到了一个问题，我的代理不断收敛到暂停游戏并坐在那里什么也不做的策略。我尝试过放大正奖励、使负奖励极端化、使用帧缓冲区分配负奖励、生存奖励、负生存信号，但我似乎无法理解是什么导致了这种行为。以前有人见过这样的事情吗？  我的代码在这里：https://github.com/lukerenchik/NineteenFourtyTwoRL 行为可视化在这里：https://www.youtube.com/watch?v=Aaisc4rbD5A    提交人    /u/LukeRenchik   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgq3z8/rl_agent_converging_on_doing_nothing_negative/</guid>
      <pubDate>Wed, 18 Dec 2024 01:12:36 GMT</pubDate>
    </item>
    <item>
      <title>强化学习工作原理的示例</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</link>
      <description><![CDATA[  由    /u/A-Sexy-Name  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgkxif/example_of_how_reinforcement_learning_works/</guid>
      <pubDate>Tue, 17 Dec 2024 21:11:18 GMT</pubDate>
    </item>
    <item>
      <title>对 Gt 和 Rt 的条件期望的使用感到困惑。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hggz4f/confused_over_usage_of_conditional_expectation/</link>
      <description><![CDATA[      来自&quot;强化学习：简介&quot;我明白了 https://preview.redd.it/0nbi4o798g7e1.png?width=549&amp;format=png&amp;auto=webp&amp;s=ca68ef2e18e1c301c942b1031170c57380a60d9d 我理解，根据多重条件期望的公式，上述内容是正确的。 但是，当我像下面这样对 St-1、At-1 和 St 取 Gt 的期望时，两个项都是相等。 E[Gt | St-1=s, At-1=a, St=s`] = E[Gt | St = s`]。因为我可以利用马尔可夫特性，所以 Gt 取决于 St 而不是之前的状态。这个技巧是推导状态值函数的贝尔曼方程所必需的。 我的问题为什么 Gt 取决于当前状态而不是 Rt？ 谢谢    提交人    /u/mono1110   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hggz4f/confused_over_usage_of_conditional_expectation/</guid>
      <pubDate>Tue, 17 Dec 2024 18:17:25 GMT</pubDate>
    </item>
    <item>
      <title>p(s`, r | s, a) 与 p(s` | s, a) 相同吗？？？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgeuly/is_ps_r_s_a_same_as_ps_s_a/</link>
      <description><![CDATA[   目前正在阅读&quot;强化学习：简介&quot;作者：Barto 和 Sutton。 给定一个状态和动作，下一个状态的概率和与下一个状态相关的奖励应该相同。这就是我的理解。 我的理解是两者应该相同，但这本书似乎对此有不同的处理。例如在下面的等式中（第 49 页） https://preview.redd.it/36v3pegmtf7e1.png?width=549&amp;format=png&amp;auto=webp&amp;s=440ffff0dc5594d52d2c6cda67757b15b8e5173e 根据条件概率规则，上述等式是正确的。我的疑问是这两个概率是如何不同的。 我在这里遗漏了什么？ 谢谢    提交人    /u/mono1110   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgeuly/is_ps_r_s_a_same_as_ps_s_a/</guid>
      <pubDate>Tue, 17 Dec 2024 16:47:15 GMT</pubDate>
    </item>
    <item>
      <title>学习代理 | Unreal Fest 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgdja2/learning_agents_unreal_fest_2024/</link>
      <description><![CDATA[       由    /u/Deathcalibur  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgdja2/learning_agents_unreal_fest_2024/</guid>
      <pubDate>Tue, 17 Dec 2024 15:49:26 GMT</pubDate>
    </item>
    <item>
      <title>尚未探索的救援方法是否具有人工智能增强的潜力？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hgct66/unexplored_rescue_methods_with_potential_for/</link>
      <description><![CDATA[我目前正在考虑高中毕业设计要做什么，我想做一些涉及强化控制无人机（与环境交互的人工智能）的事情。然而，我一直在努力寻找任何可以轻松实现人工智能无人机的应用。我正在寻找可以从自动无人机中获益的救援行动，比如消防，但一直遇到问题，比如火灾中无人机的热损伤。人工智能无人机在危险的救援行动中可能优于人类，或者在大面积或无人机飞行员有限的地方，如日本地震区或人类的辐射限制，优于人类的远程控制。它也应该是一些尚未探索的东西，比如无人机稳定地使用水管，而不是更常见的事情，比如使用计算机视觉进行监控或救援搜索。我正在尝试寻找一些物理上可行的无人机，但尚未被探索过。 你们对我可以在物理模拟中做的实现有什么想法吗？在这种模拟中，可以训练 AI 无人机在生命攸关的情况下完成对人类来说太危险或太忙碌的任务？ 我将不胜感激任何答案，希望找到一些可以在我的强化学习项目的训练环境中实现的东西。    提交人    /u/Specific_Bad8641   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hgct66/unexplored_rescue_methods_with_potential_for/</guid>
      <pubDate>Tue, 17 Dec 2024 15:16:26 GMT</pubDate>
    </item>
    <item>
      <title>辩论统计评估（样本效率曲线）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hg9062/debating_statistical_evaluation_sample_efficiency/</link>
      <description><![CDATA[      嗨各位， 我提交的一篇论文已经进入期刊接受的后期阶段。但是，关于评估协议仍然存在持续的冲突。我很乐意听听大家对统计指标和汇总的意见。 假设我在 5 个随机种子（重复）上训练了一个算法，并在给定不同时间步长的几个情节中对其进行了评估。包含情节回报的 numpy 数组可能如下所示： (5, 101, 50) Dim 0：运行次数 Dim 1：时间步长 Dim 2：评估情节次数 您是先对运行次数求平均值，然后计算平均值和标准差，还是将运行次数和情节维度合并为 (101, 250)，然后取平均值和标准差？ 我认为这在研究论文中通常不清楚。在我的特定情况下，首先进行聚合会导致非常严格的标准差和置信区间。因此，我更喜欢对所有原始事件回报取平均值和标准差。 通常，我遵循 Rliable 协议。对于样本效率曲线，建议使用四分位均值和分层引导置信区间。在目前的审核过程中，Rliable 被认为不适合仅运行 5 次。 如果能听到一些意见就太好了！ 运行次数与集数    提交人    /u/LilHairdy   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hg9062/debating_statistical_evaluation_sample_efficiency/</guid>
      <pubDate>Tue, 17 Dec 2024 11:53:59 GMT</pubDate>
    </item>
    </channel>
</rss>