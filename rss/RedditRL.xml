<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Fri, 28 Feb 2025 18:24:44 GMT</lastBuildDate>
    <item>
      <title>如果我有一个巨大的数据集，我应该选择什么选择重播缓冲区？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_i_go_for_if_i/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在实现自动缓存内存管理的RL模型，并且我的数据集的示例处于以下形式（状态，操作，奖励）。我的数据集相当庞大（我们正在谈论数万亿个数据示例）。从我的未知数中，我们首先将数据集洗净，然后将其加载到重播缓冲区（这是数据集大小合理的情况）。  对于我的情况，我正在使用iterabledataSet和pytorch的数据加载器（https://pytorch.org/tutorials/beginner/basics/data\_tutorial.html) and basically it treats my data as a large stream of info so it&#39;s not loaded into memory立即导致开销。我的问题是，在这种情况下，将整个数据集加载到重播缓冲液中并不可行，那么这里最好的方法是什么？而且有很多类型的重播缓冲区，所以哪一种是我的情况最适合使用的？提交由＆＃32; /u/u/saffarini9     [links]      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_should_should_go_go_for_if_if_if_i/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j07gl2/what_choice_of_replay_buffer_should_i_go_for_if_i/</guid>
      <pubDate>Fri, 28 Feb 2025 13:18:51 GMT</pubDate>
    </item>
    <item>
      <title>如何计算L_CLIP的梯度？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j055j8/how_to_compute_the_gradient_of_l_clip/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我最近阅读了有关PPO的信息，但是我还没有理解如何得出梯度，因为在算法中，剪辑行为取决于R_T（Theta）（theta），这是未知的。最好的方法是什么？我听说某种迭代可以实施，但我尚未理解。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/purplebumblebee5620     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j055j8/how_to_compute_the_gradient_of_l_clip/</guid>
      <pubDate>Fri, 28 Feb 2025 11:02:00 GMT</pubDate>
    </item>
    <item>
      <title>PPO重置每个时间步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j00jqi/ppo_resets_every_timestep/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  是什么可以实现的？我是RL的新手，但是我已经在数据科学领域工作了几年，所以我希望我只是缺少简单的东西。 我正在使用MultiiinputPolicy运行单个ENV。使用.learn（），Env在开始时重置，一步，再次重置，然后继续此周期，直到完成时间段。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j00jqi/ppo_resets_every_timestep/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j00jqi/ppo_resets_every_timestep/</guid>
      <pubDate>Fri, 28 Feb 2025 05:32:22 GMT</pubDate>
    </item>
    <item>
      <title>从RL Newbie到重新实现PPO：我的学习冒险</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izv5zs/from_rl_newbie_to_reimplementing_ppo_my_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我是一名CS学生，大约一年前开始潜入ML和DL。直到最近，RL才是我还没有探索太多的东西。我唯一的经验是在拥抱Face的TRL实现中，以将RL应用于LLM，但老实说，我当时不知道自己在做什么。 很长一段时间以来，我认为RL一直在恐吓 - 就像这是深度学习的最终高峰。对我来说，所有最酷的突破，例如Alphago，Alphazero和Robotics，似乎都与RL息息相关，这使它感到遥不可及。但是随后DeepSeek发行了GRPO，我真的很想了解它如何运作和跟随论文。这引发了一个想法：两周前，我决定通过重新进化一些核心RL算法来启动一个项目，以从头开始建立我的RL知识。 到目前为止，我已经解决了一些。我从DQN开始，这是我迄今为止重新实现的唯一基于价值的方法。然后，我继续进行策略梯度方法。我的第一个尝试是使用奖励前进的基本增强算法的香草政策梯度。我还为此添加了评论家，因为我看到两种方法都是可能的。接下来，我接受了TRPO，这是迄今为止最难实施的。但是，通过它为我带来了一个真正的“尤里卡”时刻 - 我终于掌握了监督学习与RL的优化之间的根本差异。即使由于二阶方法的成本，TRPO不再被广泛使用，但我强烈建议将其重新实现为任何学习RL的人。这是构建直觉的好方法。 现在，我刚刚完成了PPO的重新进化，这是那里最受欢迎的算法之一。我选择了剪辑版本，尽管在TRPO之后，KL-Divergence版本对我来说更为直观。我一直在简单的控制环境上测试这些算法。我知道我可能应该尝试一些更复杂的事情，但是这些倾向于训练。 老实说，这个项目使我意识到RL甚至有效。以乒乓球为例：在培训的早期，您的政策很糟糕，每次都会失去。它需要20个步骤（4帧跳过），只是将球从一侧转到另一侧。在这20个步骤中，您将获得19个零，也许是+1或-1奖励。这种稀疏性是疯狂的，令人震惊的是，它最终会弄清楚事情。 接下来，我打算在将重点转移到连续的动作空间之前实施GRPO，到目前为止，我只与离散的动作合作，所以我很高兴能探索这一点。我还坚持基本的MLP和Convnets的政策和价值功能，但是我正在考虑尝试为连续动作空间进行扩散模型。他们看起来很自然。展望未来，我很想在我很快完成上学后尝试一些机器人项目，并为这样的附带项目提供更多空闲时间。 我的大外卖？ RL并不像我想象的那么恐怖。大多数主要的算法可以很快地在单个文件中重新完成。就是说，培训是一个完全不同的故事 - 由于RL铲球的性质，它可能令人沮丧和恐吓。对于这个项目，我依靠Openai的旋转指南和每种算法的原始论文，这非常有帮助。如果您很好奇，我一直在用回购工作，称为“ rl-arena”。去！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/awkward-can-8933      [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izv5zs/from_rl_newbie_to_reimplementing_ppo_my_learning/</guid>
      <pubDate>Fri, 28 Feb 2025 00:38:30 GMT</pubDate>
    </item>
    <item>
      <title>“培训语言模型，用于通过多机构增强学习学习”，Sarkar等2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/gwern       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izkjoi/training_language_models_for_social_deduction/</guid>
      <pubDate>Thu, 27 Feb 2025 16:59:12 GMT</pubDate>
    </item>
    <item>
      <title>国际象棋样品效率人与sota rl</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</link>
      <description><![CDATA[From what I know, SOTA chess RL like AlphaZero reached GM level after training on many more games than a human GM played throughout their lives before becoming GM Even if u include solved puzzles, incomplete games, and everything in between, humans reached GM with much lesser games than SOTA RL did (pls correct me if I&#39;m wrong about这）。 是否有比人类效率较低的特定原因/障碍？对于提高国际象棋SOTA RL样本效率的研究是否有希望？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izimy7/chess_sample_efficiency_humans_vs_sota_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:40:39 GMT</pubDate>
    </item>
    <item>
      <title>离线RL的动作将是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  所以，我是RL的新手，我必须实现离线RL模型，然后在在线RL阶段中微调它。从我的承诺中，离线学习阶段最初的策略和在线学习阶段将使用实时反馈来完善政策。对于离线学习阶段，我将有一个数据集d = {（si，ai，ri）}。数据集中每个示例的操作是否是收集数据时采取的动作（即专家行动）？还是所有可能的动作？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/saffarini9     [link]    ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izhryd/what_will_the_action_be_in_offline_rl/</guid>
      <pubDate>Thu, 27 Feb 2025 15:03:21 GMT</pubDate>
    </item>
    <item>
      <title>[付费]寻找有人在RL NASH差异游戏上编写快速Python程序</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izgmws/paid_looking_for_someone_to_write_a_quick_python/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，我正在寻找一个为零和非零和nash nash差异游戏编写python程序的python程序。  对于熟练的人来说，这应该是一个小时的工作。愿意为演出支付50美元。给我发消息以获取更多详细信息。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/used_chapter007     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izgmws/paid_looking_for_someone_to_write_a_quick_python/</guid>
      <pubDate>Thu, 27 Feb 2025 14:10:05 GMT</pubDate>
    </item>
    <item>
      <title>我被困在瓶颈上，有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1izdx0j/i_am_stuck_at_a_bottleneck_any_suggestions_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在使用称为Rware的RL环境。它给出了RGB数组，但仅在渲染窗口后才提供。因此，我的培训需要大量时间。是否有任何想法绕过或跳过渲染？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/decter_prune_9756      [link]  &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1izdx0j/i_am_astuck_at_a_bottleneck_any_suggestions_to//]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1izdx0j/i_am_stuck_at_a_bottleneck_any_suggestions_to/</guid>
      <pubDate>Thu, 27 Feb 2025 11:48:05 GMT</pubDate>
    </item>
    <item>
      <title>跨领域的凉爽自我校正机制？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  从控制理论的反馈循环和卡尔曼过滤到自然选择，DNA修复，多数投票和引导 - 无数的方式系统自我校正错误，尤其是当地面真理未知时！想知道您遇到的有趣的自我纠正机制是什么，无论是自然界，哲学，工程还是以后？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/neat_comparison_2726      [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iywf0w/cool_selfcorrecting_mechanisms_across_fields/</guid>
      <pubDate>Wed, 26 Feb 2025 19:48:15 GMT</pubDate>
    </item>
    <item>
      <title>现在，您可以使用GRPO（5GB VRAM最小值）训练自己的推理模型。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，很棒的人！第一篇文章在这里！今天，我很高兴地宣布，您现在可以使用grpo +使用Grpo +我们的开放式项目unsploth使用5GB VRAM训练自己的推理模型： https：&gt; https：&gt;是DeepSeek-R1背后的算法以及如何受过训练。它比PPO更有效，我们设法将VRAM使用降低了90％。您需要一个大约500行，答案对和奖励功能的数据集，然后可以启动整个过程！ 这允许将任何开放的LLM（如Llame，Misstral，Phi等）等开放，可以将其转换为具有链链过程的推理模型。关于GRPO的最好的部分是，与更大的型号相比，与更大的训练时间相比，与较大的训练时间相比，与较大的型号相比，训练小型型号与较大的型号无关紧要，因此最终结果将非常相似！您也可以在执行其他操作的同时，在PC的背景下进行GRPO培训！  由于我们新添加的有效的GRPO算法，这使得 10x更长的上下文长度长度  90％使用 90％的vram  vram/strong&gt; vraM/strong&gt; lora/qlora/qula li&gt; li afteraive  li&gt; field afteraiment &lt;0&gt; 标准GRPO设置，Llama 3.1（8b）20K上下文长度的培训需要510.8GB的VRAM。但是，Unsploth的90％VRAM减少的要求使同一设置中的需求仅为54.3GB。 我们利用我们的渐变”检查 algorithm，我们发布了一个aLgorithm。它可以巧妙地将中间激活卸载到系统RAM异步，同时仅慢1％。此剃须372GB VRAM ，因为我们需要num \ _ generations = 8。我们可以通过中间梯度累积进一步减少此内存使用。 使用Google的免费上下文使用我们的GRPO Notebook，使用Google的免费gpus： href =“ https://colab.research.google.com/github/unslothai/notebooks/blob/blob/main/nb/llama3.1_(8B”&gt; llama 3.1（8b）on colab  -grpo.ipynb）以及更多： align =“ left”&gt; metric   unsploth   trl + fa2           training Moregre Cost（GB） align =“左”&gt; 414GB      grpo内存成本（gb）   9.8gb    78.3gb  78.3gb  78.3gb    0gb   16gb      推理20K上下文（GB）   2.5GB  2.5gb  总内存使用   54.3GB（少90％）       510.8GB              我们在所有方面都花了很多时间（pboty&gt;  ：d   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/yoracale     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyw9ly/you_can_now_train_your_own_reasoning_model_using/</guid>
      <pubDate>Wed, 26 Feb 2025 19:41:48 GMT</pubDate>
    </item>
    <item>
      <title>策划可塑性损失的论文清单</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨， 我已经创建了一个存储库，其中包含有关可塑性损失的论文列表。重点是深度RL，但是那里也有一些持续的学习。  https：//github.com/github.com/github.com/probabilistic--interactive-mlaw---interactive-mlaw yourplastive yourplastive plapery plapery plapery  我们还在撰写有关该主题的调查，但仍处于早期阶段：很多牵引力，我希望这可以帮助人们加快速度：）  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/timo_kk     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyrtge/curated_list_of_papers_on_plasticity_loss/</guid>
      <pubDate>Wed, 26 Feb 2025 16:40:39 GMT</pubDate>
    </item>
    <item>
      <title>以非常折扣价的困惑pro</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  任何有兴趣以50％折扣价获得困惑Pro的人，请与我联系  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/beast_of_iit    href =“ https://www.reddit.com/r/reinforevercylearning/comments/1iykcbt/perplexity_pro_at_a_very_very_very_very_very_very_very_discounted_price/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iykcbt/perplexity_pro_at_a_very_discounted_price/</guid>
      <pubDate>Wed, 26 Feb 2025 10:15:47 GMT</pubDate>
    </item>
    <item>
      <title>RL代理当前在不激励特定行为的情况下最佳执行的最复杂环境是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我很想知道sota在环境复杂性方面，在不需要任何中级奖励的情况下，RL代理执行的性能 - 只是+1 +1 for“ win”和-1为“损失”   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/aliaslight     [link]     32;   [注释]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iyi6ev/what_is_the_most_complex_environment_in_which_rl/</guid>
      <pubDate>Wed, 26 Feb 2025 07:34:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么某些环境（例如Minecraft）太困难了，而另一些环境（例如Openai's Hide N See Seek）是可行的？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   tldr：是什么让hide n寻求可解决的环境，但是我很难解决的我的minecraft或简化的Minecraft环境？ 我没有遇到任何RL代理在Minecraft中成功生存的任何RL代理。理想情况下，如果根据代理商的活力来给予奖励，它至少应该为食物建立庇护所和农场。 ， ，Openai的hide n of from 5年前从5年前开始寻求视频，从划痕中，在那个环境中学到了很多东西，甚至没有激励任何行为。为什么不适用于Minecraft？有一个更容易的环境称为手工艺者，但即使是这样的奖励似乎是这样设计的，以至于最佳行为被激励，而不仅仅是基于生存的奖励，而最佳绩效（Dreamer）仍然没有与人类的绩效相比。 是什么让hide n寻求可解决的环境，但可以解决，但可以解决，但是是如此，但是如此难以解决的或简化的Minecraft环境，以求解Minecraft solve？提交由＆＃32; /u/aliaslight     [link]   [注释] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iygakk/why_are_some_environments_like_minecraft_too/</guid>
      <pubDate>Wed, 26 Feb 2025 05:29:58 GMT</pubDate>
    </item>
    </channel>
</rss>