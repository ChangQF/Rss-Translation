<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 01 Jan 2024 21:12:20 GMT</lastBuildDate>
    <item>
      <title>COOM：持续强化学习的游戏基准</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w24o8/coom_a_game_benchmark_for_continual_reinforcement/</link>
      <description><![CDATA[论文：https： //openreview.net/forum?id=qmCxdPkNsa 代码：https ://github.com/hyintell/COOM 视频：https://www.youtube.com/watch?v=FUm2B8MZ6d0 摘要：  进步持续强化学习（RL）一直面临着各种障碍，包括标准化的指标和评估协议、苛刻的计算要求以及缺乏广泛接受的标准基准。为了应对这些挑战，我们提出了COOM（Continual DOOM），这是一个为基于像素的强化学习量身定制的连续强化学习基准。 COOM 提供了一套在视觉上不同的 3D 环境中精心设计的任务序列，作为一个强大的评估框架来评估持续强化学习的关键方面，例如灾难性遗忘、知识转移和样本高效学习。在对流行的持续学习（CL）方法进行深入的实证评估后，我们查明了它们的局限性，提供了对基准的宝贵见解，并强调了独特的算法挑战。这使得我们的工作成为第一个在具有具体感知的 3D 环境中对基于图像的 CRL 进行基准测试的工作。 COOM 基准的主要目标是为研究界提供有价值且具有成本效益的挑战。它旨在加深我们对强化学习环境中当前和即将推出的 CL 方法的功能和局限性的理解。代码和环境是开源的，可以在 GitHub 上访问。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w24o8/coom_a_game_benchmark_for_continual_reinforcement/</guid>
      <pubDate>Mon, 01 Jan 2024 17:53:01 GMT</pubDate>
    </item>
    <item>
      <title>关闭策略策略梯度定理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/</link>
      <description><![CDATA[嗨，我真的很想逐行理解离策略策略梯度算法。 本文由 Degris 撰写， T.、怀特、M. 和萨顿，R.S. (2012).论文链接：(https://arxiv.org/pdf/1205.4839.pdf) 因此，在论文的 2.2 节中，作者指出，在离策略 pg 中，我们通过省略全梯度公式中的附加项来使用真实 pg 的近似值。  现在，在附录 A 中，作者首先尝试在各国共享一个参数化政策的向量 u 的一般情况下证明这一点。  我理解第一点，如果我们使用在不同状态和动作对评估的加性梯度来更新我们的参数，新参数最终将为我们提供更高的目标函数。在此目标中，状态和动作对的价值函数保持不变，但是具有较高价值的 $Q{\pi_u, \gamma}(s,a)$ 在 $\ pi_{u&#39;, \gamma}$。  但是，我无法完全理解，而且我正在努力以一种非常数学上稳健的方式看待它，为什么如果我们开始使用$\pi_{u&#39;, \gamma}$ 依次。  本质上让我困惑的是证明中的政策改进部分（参见附图2）。   由   提交 /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/</guid>
      <pubDate>Mon, 01 Jan 2024 17:22:29 GMT</pubDate>
    </item>
    <item>
      <title>不太模糊的奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w09rn/less_ambiguous_reward_function/</link>
      <description><![CDATA[我的奖励函数中有三个独立的组件，我认为这不会让我的 MARL 自定义环境学习。如何有效地将更改传输给我的代理，即让他更容易理解是什么操作导致了更改。 供参考： Boid（我的代理） 算法： StableBaselines3-PPO 奖励组件：CohesionAlignmentCollision Penalty 另外我如何确定我使用的是CTDE还是DTDE？   由   提交/u/Sadboi1010   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w09rn/less_ambiguous_reward_function/</guid>
      <pubDate>Mon, 01 Jan 2024 16:29:50 GMT</pubDate>
    </item>
    <item>
      <title>使用 DRL 研究项目进行股票交易</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vz6gy/stock_trading_using_drl_research_project/</link>
      <description><![CDATA[我是计算机科学学士学位最后一年的学生。我的 FYP 标题是“使用 DRL 进行股票交易”。这个称号是我的导师给的，对我来说真的是一个很难的称号。该主题是关于优化交易策略，而不是价格预测或投资组合管理。 这些是我的项目目标： 1. 在正常股票市场条件下使用 DRL 来提高股票交易模型的性能，其指标包括：累积回报（CR）。 2. 在熊市股市条件下使用 DRL 以及夏普比率 (SR) 等指标来提高股票交易模型的性能。 3. 通过对 DRL 模型使用的技术指标应用特征选择技术来提高股票交易模型的性能。 我打算使用 DQN，但我对此完全陌生并且陷入困境。我已经完成了介绍和文献综述，但理论框架章节让我很受不了。我还没有开始编码部分，因为那是下学期的事情。现在正在写论文，不知道怎么写，要不要用MDP，怎么用等等，真的很挣扎，压力很大。有人可以帮我解决这个问题并给我一些建议吗？   由   提交/u/cookiesandcream30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vz6gy/stock_trading_using_drl_research_project/</guid>
      <pubDate>Mon, 01 Jan 2024 15:39:11 GMT</pubDate>
    </item>
    <item>
      <title>哪个 OpenAI Gym 版本最好/最常用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vtoyi/which_openai_gym_version_is_bestmost_used/</link>
      <description><![CDATA[大家好， 我最近开始研究健身平台，更具体地说是 BipedalWalker。我最初使用的是最新版本（现在称为 Gymnasium，而不是 Gym），但是 99% 的在线教程和代码都使用旧版本的 Gym。 由于我正在从事的项目非常复杂，并且没有以前在这个环境中完成过，我需要尽可能多地从其他人那里获得工作代码。我看到版本21到26有变化，Gymnasium现在也有差异。 我可以看到很多3年前的教程、视频和代码。但在过去几年里，它似乎已经失去了吸引力。 所以我的问题是，哪个版本的库最适合我工作，以便拥有实际工作的代码？    由   提交 /u/DocMenios   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vtoyi/which_openai_gym_version_is_bestmost_used/</guid>
      <pubDate>Mon, 01 Jan 2024 09:59:21 GMT</pubDate>
    </item>
    <item>
      <title>PPO 与地方政策的融合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vsfzg/ppo_convergence_to_local_policy/</link>
      <description><![CDATA[      我正在使用 PPO 算法，我的算法在训练期间获得的最大奖励为 212，但经过几次（80-100 epi）后它收敛到 176，就像我尝试降低学习率和其他修补一样超级参数，但仍然没有用。 任何帮助表示赞赏。 （下面是训练图。）提前致谢！！！！ https://preview.redd.it/5buwjkzqgs9c1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=6687d40641ac6747acdf42a27b6f2e80 65532c97   由   提交 /u/Wide-Chef-7011   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vsfzg/ppo_convergence_to_local_policy/</guid>
      <pubDate>Mon, 01 Jan 2024 08:25:24 GMT</pubDate>
    </item>
    <item>
      <title>Connect-4 - Q-Learning 与 Actor-Critic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vd577/connect4_qlearning_vs_actorcritic/</link>
      <description><![CDATA[我实现了 Connect-4 的两个版本，一个基于 Q-Learning，另一个基于 REINFORCE（Actor-Critic 方法）。手动调整学习参数后，很容易让 Actor-Critic 版本达到合理的学习进度。然而，我的 Q-Learning 版本并没有取得成功。对于为什么 REINFORCE 更适合这个问题，有什么理由/解释吗？   由   提交 /u/m_jochim   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vd577/connect4_qlearning_vs_actorcritic/</guid>
      <pubDate>Sun, 31 Dec 2023 17:56:07 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v8twl/advices_for_reinforcement_learning/</link>
      <description><![CDATA[我想深入了解什么是向量化环境，请给我一些书籍或视频。   由   提交 /u/BryanDeveloper   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v8twl/advices_for_reinforcement_learning/</guid>
      <pubDate>Sun, 31 Dec 2023 14:27:08 GMT</pubDate>
    </item>
    <item>
      <title>网格世界中的 Q 学习 - 贝尔曼方程可视化 [评论中的链接] :)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v6jwi/q_learning_on_a_grid_world_bellman_equation/</link>
      <description><![CDATA[       由   提交/u/prajwalsouza  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v6jwi/q_learning_on_a_grid_world_bellman_equation/</guid>
      <pubDate>Sun, 31 Dec 2023 12:12:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 pytorch 编写自定义矢量化健身房环境的约定？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v50ai/conventions_to_write_a_custom_vectorized_gym/</link>
      <description><![CDATA[在torchrl中，您只需将一批操作传递给step函数，然后让pytorch处理矢量化。来源：https://pytorch.org/rl/tutorials/pendulum.html#batching-computations .不过，我想使用大多数与 Gym 兼容的其他 RL 库。 在gym 中，您可以使用 vector.make() 或AsyncVectorEnv。如果您的环境实现只是 Pytorch，这会不会太过分了？有开源的例子吗？或者也许是健身房的替代品？ 注意：我只是 RL 新手几天。任何建议都会有帮助   由   提交/u/hunterh0  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v50ai/conventions_to_write_a_custom_vectorized_gym/</guid>
      <pubDate>Sun, 31 Dec 2023 10:23:40 GMT</pubDate>
    </item>
    <item>
      <title>《利用部分动力学知识进行高效强化学习的样本》2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</link>
      <description><![CDATA[ 由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</guid>
      <pubDate>Sat, 30 Dec 2023 22:12:11 GMT</pubDate>
    </item>
    <item>
      <title>环境生成器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</link>
      <description><![CDATA[嘿，RL 爱好者， 我想知道，当我们做 RL 实验时，你们是否都被开发 RL 所需的开销所困扰？预先环境。我发现这非常烦人，因为我总是需要构建适合我的用例的东西。  据我们所知，我们只有 Farma 基金会（https://farama.org/）提供的十几个高质量环境。 org/)  欢迎任何想法！    由   提交 /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</guid>
      <pubDate>Sat, 30 Dec 2023 18:59:28 GMT</pubDate>
    </item>
    <item>
      <title>多目标场景的最佳强化学习算法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</link>
      <description><![CDATA[你好， 我正在尝试训练室内无人机代理离开房间。无人机必须逃离日益严重的火势并到达 4 个出口中的任何一个。 我尝试过 DQN、A2C、PPO。这些算法的问题在于，一旦智能体学会了出口门，它总是尝试从那里退出，而其他门则未被探索。 我想知道哪种 RL 算法最适合这种情况，当更多没有一个进球。 谢谢！   由   提交/u/shahmirkhan21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</guid>
      <pubDate>Sat, 30 Dec 2023 16:24:36 GMT</pubDate>
    </item>
    <item>
      <title>Pangu-Agent：具有结构化推理的可微调多面手智能体</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18u2bym/panguagent_a_finetunable_generalist_agent_with/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2312.14878 摘要：  创建人工智能（AI）代理的关键方法是强化学习（RL）。然而，构建一个将感知直接映射到行动的独立强化学习策略会遇到严重的问题，其中最主要的是它缺乏跨多个任务的通用性以及需要大量的训练数据。主要原因是在制定政策时无法有效地将先验信息融入到感知-行动循环中。大型语言模型（LLM）作为将跨领域知识融入人工智能代理的基本方式而出现，但缺乏针对特定决策问题的关键学习和适应。本文提出了一个将结构化推理集成和学习到人工智能代理策略中的通用框架模型。我们的方法论受到人脑模块化的启发。该框架利用内在和外在函数的构造来添加先前对推理结构的理解。它还提供了学习每个模块或功能内部模型的自适应能力，与认知过程的模块化结构一致。我们深入描述了该框架，并将其与其他人工智能管道和现有框架进行了比较。本文探讨了实际应用，包括证明我们方法有效性的实验。我们的结果表明，当嵌入有组织的推理和先验知识时，人工智能代理的表现和适应能力要好得多。这为更具弹性和通用的人工智能代理系统打开了大门。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18u2bym/panguagent_a_finetunable_generalist_agent_with/</guid>
      <pubDate>Sat, 30 Dec 2023 00:41:00 GMT</pubDate>
    </item>
    <item>
      <title>平均奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18trd23/average_reward/</link>
      <description><![CDATA[       大家好，我是 DRL 新手。当我使用Carla自动驾驶车辆模拟器训练DQN模型时，我发现平均奖励曲线是这样的（有波动），解决方案是什么？ ​ https://preview.redd.it/knu4kkqzi99c1.png ?width=583&amp;format=png&amp;auto=webp&amp;s=b516976d2b65d61d610a0726301f23262962fa48   由   提交 /u/Chetioui_PHD   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18trd23/average_reward/</guid>
      <pubDate>Fri, 29 Dec 2023 16:43:49 GMT</pubDate>
    </item>
    </channel>
</rss>