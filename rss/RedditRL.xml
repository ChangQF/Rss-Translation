<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 12 Dec 2024 03:34:33 GMT</lastBuildDate>
    <item>
      <title>针对机械臂 RL 项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</link>
      <description><![CDATA[嗨，我正在寻找一些论文或想法，我可以在硬件机器人操纵器上尝试，以获得 RL 控制和规划方面的经验。     提交人    /u/Stunning-Stable-8553   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hcbi6t/recommendations_for_robot_arm_rl_projects/</guid>
      <pubDate>Thu, 12 Dec 2024 02:57:44 GMT</pubDate>
    </item>
    <item>
      <title>体育馆/mujoco 教程需要四足机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</link>
      <description><![CDATA[大家好，我正在做一个关于四足机器狗的项目。我正在尝试使用 gymnasium 和 MuJoCo，但在 gymnasium 上设置自定义环境真的很令人困惑。我正在寻找一个教程，这样我就可以学习如何设置它，或者是否有人建议我应该切换我正在使用的工具。    提交人    /u/PrincipleDistinct425   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc52ho/gymnasiummujoco_tutorial_needed_quadruped_robot/</guid>
      <pubDate>Wed, 11 Dec 2024 21:48:47 GMT</pubDate>
    </item>
    <item>
      <title>Rl 用于自主导航</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</link>
      <description><![CDATA[大家好，我打算在我的最后一年项目中使用 ubuntu 22.04 和 ros2 hum，该项目包括使用 DRL 技术在 Gazebo 中为 turtlebot3 机器人进行自主导航，但我真的很担心我的笔记本电脑的容量，我真的没有合适的笔记本电脑，我有一台第 8 代英特尔 i5，8 核，我真的会发现学习问题吗？    提交人    /u/DueStill7268   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc1b8g/rl_for_autonomous_navigation/</guid>
      <pubDate>Wed, 11 Dec 2024 19:10:28 GMT</pubDate>
    </item>
    <item>
      <title>彼得·墨菲 (Peter Murphy) 的最新强化学习书籍有多好？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</link>
      <description><![CDATA[编辑：应该是 Kevin Murphy。 我的一位同事推荐了 https://arxiv.org/pdf/2412.05265。 我发现它有点像洗衣清单，就像其他强化学习调查的情况一样。不同的想法感觉就像反复试验。我自己过去也曾在 TensorFlow 中编写过强化学习。但很难真正感受到它的威力。我有数学背景，我只是不确定是否值得花时间阅读这么厚的书，因为我知道我可能记不住很多东西，除非所有不同的概念形成连贯的意识流。换句话说，我发现这个主题没有足够的基础来轻松消化第一原理。 我对其他人对这个问题的看法很好奇，特别是从第一原理的角度来看。    提交人    /u/Crazy_Suspect_9512   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hc0nk7/how_good_is_peter_murphys_latest_reinforcement/</guid>
      <pubDate>Wed, 11 Dec 2024 18:43:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Stable Baselines 3 中的训练期间动态修改超参数？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/</link>
      <description><![CDATA[我正在使用 Stable Baselines 3，并尝试实施一个训练过程，在该过程中我在训练的不同阶段动态地改变超参数。具体来说，我正在使用 PPO 并想要更改 gamma 参数。 这是我正在尝试执行的操作的简化版本： ```py from stable_baselines3 import PPO # 初始训练 model = PPO(&quot;MlpPolicy&quot;, &quot;CartPole-v1&quot;, gamma=0.99) model.learn(total_timesteps=10000) print(f&quot;初始 gamma：{model.gamma}&quot;) print(f&quot;初始推出缓冲区 gamma：{model.rollout_buffer.gamma}&quot;) # 尝试更改 gamma model.gamma = 0.95 model.learn(total_timesteps=10000) print(f&quot;更改后 - 模型 gamma：{model.gamma}&quot;) print(f&quot;更改后 -推出缓冲区 gamma：{model.rollout_buffer.gamma}&quot;)  ``` 输出： ```py 初始 gamma：0.99 初始推出缓冲区 gamma：0.99 更改后 - 模型 gamma：0.95 更改后 - 推出缓冲区 gamma：0.99  ``` 我们可以看到，更改 model.gamma 不会更新所有必要的内部状态。 model.rollout_buffer.gamma 保持不变，这可能导致不一致的行为。 我考虑过使用新参数保存并重新加载模型： ```py model.save(&quot;temp_model&quot;) model = PPO.load(&quot;temp_model&quot;, gamma=0.95) model.learn(total_timesteps=10000) print(f&quot;重新加载后 - 模型 gamma：{model.gamma}&quot;) print(f&quot;重新加载后 - 滚动缓冲区 gamma：{model.rollout_buffer.gamma}&quot;)  ``` 输出： ```py 重新加载后 - 模型 gamma：0.95 重新加载后 - 滚动缓冲区 gamma：0.95  ``` 这种方法有效但似乎效率低下，特别是如果我想在训练期间频繁更改参数时。 在 Stable Baselines 3 中，是否有一种适当的方法可以在训练期间动态更新像 gamma 这样的超参数？理想情况下，我想要一个解决方案，确保所有相关的内部状态都得到一致更新，而无需保存和重新加载模型。 对于这种情况的任何见解或最佳实践都将不胜感激。    提交人    /u/Academic-Rent7800   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbt64c/how_to_dynamically_modify_hyperparameters_during/</guid>
      <pubDate>Wed, 11 Dec 2024 13:13:23 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 在我的用例中存在问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</link>
      <description><![CDATA[大家好， 这是我第一次从事 RL 项目，我正在构建一个可以与特定 DLT 一起使用的模型。具体来说，我希望它选择最佳数量的块来通过特定的 DLT 发送消息。我尝试了不同的算法，但由于它必须自主选择动作并且不受限制，所以我选择了 DDPG 方法。 然而，让我很困惑的是，对于我构建的特定奖励系统，对于单次训练运行（不更新模型），模型有时会学习，有时不会。这意味着对于大多数运行，模型不会探索选项，它将坚持发送消息所需的最少块数。而在较少的情况下，它似乎在学习，但仅此而已。下次我运行代码时，它可能会返回到选择最小数量的块。 不确定这是否与奖励系统、Actor - Critic 网络的架构或算法本身有关。但我很感激一些指导。非常感谢！    提交人    /u/LionTheAlpha   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbpe06/trouble_with_ddpg_for_my_use_case/</guid>
      <pubDate>Wed, 11 Dec 2024 08:57:42 GMT</pubDate>
    </item>
    <item>
      <title>协助定期 PPO 代理优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbdgqq/assistance_with_recurrent_ppo_agent_optimization/</link>
      <description><![CDATA[我正在训练我的循环 PPO 代理执行优化任务，代理的基于 token 的操作会输入到单独的数值优化器中。然而，在初始训练步骤之后，代理始终卡在其连续动作空间的上限和下限，并且奖励保持不变。您能否提供一些解决此问题的指导？     提交人    /u/YasinRL   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbdgqq/assistance_with_recurrent_ppo_agent_optimization/</guid>
      <pubDate>Tue, 10 Dec 2024 21:56:03 GMT</pubDate>
    </item>
    <item>
      <title>AI Sphere 平衡球：虚幻引擎的强化学习插件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hbak2t/ai_sphere_balancing_a_ball_a_reinforcement/</link>
      <description><![CDATA[大家好！👋 我一直在研究一个 AI 球体，它可以学习使用强化学习来平衡球，由虚幻引擎的自定义插件提供支持。该插件是 Unity ML-Agents 的端口，为 Unreal 用户带来 RL 工具。 该视频使用二次奖励系统演示了 Sphere - 如果将球保持在中心附近，则会获得更多积分，如果球掉落，则会失去积分。 🎥 https://youtu.be/6lhCa72TGNk?si=qcIKof09R1Yl0SJw 该插件将很快推出，Sphere 项目将作为示例包含在内。如果您有兴趣，请加入我的 Discord 以获取更新和早期访问：https://discord.gg/Y8MDexY4。 很想听听您的想法或创意 - 感谢您的关注！😊    提交人    /u/Cyber​​Eng   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hbak2t/ai_sphere_balancing_a_ball_a_reinforcement/</guid>
      <pubDate>Tue, 10 Dec 2024 19:53:06 GMT</pubDate>
    </item>
    <item>
      <title>GameMaker 8.0 游戏的 Gym 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb7hvd/gym_environment_for_a_gamemaker_80_game/</link>
      <description><![CDATA[我最近决定尝试强化学习，以此来构建一个很酷的投资组合项目。我决定为 Spelunky Classic 制作一个深度 Q 学习算法，因为我在 YouTube 视频 中找到了七年前有人做过同样的事情，而且我是 Spelunky 的狂热粉丝，这让它更具吸引力。 我开始在我的计算机上设置所有适当的 Python 内容，经过几个小时的工作，这个 PyTorch 教程 开始运行。然而，此时我遇到了一个障碍——我了解 DQN 的工作原理，并且我有 Spelunky Classic 的源代码，所以我绝对可以为 DQN 修改它，但我对如何实现它感到困惑。我意识到下一步可能是为 Spelunky Classic 创建一个健身房环境，但我不知道如何将奖励返回给 Python 程序，因为游戏是用 GameMaker 8.0 编写的，这是一个弃用的引擎，在 OpenAI 热潮期间没有相关支持。 所有这些都是为了问，我现在应该如何让它工作？我是否必须制作某种自定义包装器才能将游戏变成健身房环境？有什么简单的方法可以将游戏转换为健身房环境？是否存在我尚未意识到的困难？ 任何帮助或建议都值得感激，我只是有点不知道下一步该怎么做。    提交人    /u/GarrettBotProgrammer   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb7hvd/gym_environment_for_a_gamemaker_80_game/</guid>
      <pubDate>Tue, 10 Dec 2024 17:45:18 GMT</pubDate>
    </item>
    <item>
      <title>“哥德尔代理：用于递归自我改进的自参考代理框架”，Yin 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb5zds/gödel_agent_a_selfreferential_agent_framework_for/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb5zds/gödel_agent_a_selfreferential_agent_framework_for/</guid>
      <pubDate>Tue, 10 Dec 2024 16:41:39 GMT</pubDate>
    </item>
    <item>
      <title>将强化学习应用于投资组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hb4hsy/applying_rl_to_portfolio/</link>
      <description><![CDATA[我是一名加密和机器学习爱好者，正在完成一个用于算法交易的回测系统（信不信由你，只是为了好玩）。我正在考虑测试一些用于投资组合优化的强化学习方法。 我有大量的历史数据可以使用，但我对建立训练方案的最佳方法以及模型容量的选择有些困惑。 我目前的想法是采用基于与投资组合价值相关的奖励函数的参与者/评论家设置。 哪个时间步骤最有意义？ 我是否应该预先训练一个模型来简单地预测均值和方差（这样我就可以使用历史数据而无需进行演练）？ 或者我应该专门通过演练进行训练？如果是这样，我应该将它们并行化吗？    提交人    /u/brabbly   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hb4hsy/applying_rl_to_portfolio/</guid>
      <pubDate>Tue, 10 Dec 2024 15:37:16 GMT</pubDate>
    </item>
    <item>
      <title>2 个 AI 代理玩捉迷藏。经过 150 万次模拟，代理学会了偷看、搜索和切换方向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hawyj5/2_ai_agents_playing_hide_and_seek_after_15/</link>
      <description><![CDATA[    /u/stokaty   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hawyj5/2_ai_agents_playing_hide_and_seek_after_15/</guid>
      <pubDate>Tue, 10 Dec 2024 08:03:36 GMT</pubDate>
    </item>
    <item>
      <title>帮助 LLM 调整 json 格式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1havv63/help_in_llm_tuning_json_format/</link>
      <description><![CDATA[我正在尝试微调 LLM，并且我正在使用 json 格式的数据文件。但是我无法训练 gpt2 我在这里呆了 3 天，看了很多地方，但都没有用。请查看附件并提供反馈。我的 json 格式错误吗？ {&quot;dataset&quot;: [ {&quot;prompt&quot;: &quot;你对电动汽车有什么看法？&quot;, &quot;responses&quot;: [&quot;电动汽车正在彻底改变交通运输。它们环保、经济高效，并提供平稳、安静的驾驶体验。每个人都应该考虑改用电动汽车，以帮助保护环境并减少对化石燃料的依赖。&quot;, &quot;电动汽车是未来！随着充电网络的迅速扩张和电池的长寿命化，它们比以往任何时候都更加方便和实惠。政府应该鼓励人们采用电动汽车，以创造一个可持续发展的地球。&quot;, &quot;拥有电动汽车不仅可以省钱，还可以减少空气污染。新款电动汽车造型时尚，配备了先进技术。这对消费者和地球来说是双赢的。&quot; ] }, { &quot;prompt&quot;: &quot;为什么电动汽车比燃气汽车更好？&quot;, &quot;responses&quot;: [ &quot;电动汽车不排放有害气体，与燃气汽车相比，对空气质量更有利。它们的运动部件也更少，大大降低了维护成本。&quot;, &quot;燃气汽车依赖不可再生能源并导致全球变暖，而电动汽车可以使用可再生能源。这使得电动汽车成为有环保意识的消费者的明确选择。&quot; ] }, { &quot;prompt&quot;: &quot;政府是否应该加大对电动汽车基础设施的投资？&quot;, &quot;responses&quot;: [ &quot;当然！投资电动汽车基础设施将加速向可持续交通的转变。它还将创造就业机会、减少污染并改善公共健康。&quot;, &quot;是的，优先考虑电动汽车基础设施对于减少温室气体排放至关重要。强大的充电网络将鼓励更多人改用电动汽车，使长途旅行更加轻松。&quot; ] } ] }     提交人    /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1havv63/help_in_llm_tuning_json_format/</guid>
      <pubDate>Tue, 10 Dec 2024 06:45:12 GMT</pubDate>
    </item>
    <item>
      <title>推荐与 Gymnasium 兼容的 Safe-RL 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hamadk/recommended_gymnasium_compatible_saferl/</link>
      <description><![CDATA[大家好， 希望这个问题适合这里。 我正在使用 CMDP（约束马尔可夫决策过程），并且正在寻找支持 CMDP 的环境，但主要是实现成本感知 RL 算法，如 PPO-Langrangian。 有人可以推荐一个具有此类实现的优秀 github repo 吗？ 或者也许是可以自己扩展的最佳 repo？    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hamadk/recommended_gymnasium_compatible_saferl/</guid>
      <pubDate>Mon, 09 Dec 2024 22:25:34 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 的自定义环境的奖励没有改善</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1haezxl/reward_not_improving_for_a_custom_environment/</link>
      <description><![CDATA[      我一直在尝试在我使用 gym 实现的自定义环境中训练代理，其目标是通过调整每个节点的有功功率（负载）来解决电网中的电压违规问题。我主要尝试了两种算法，通过 stable-baselines3、PPO 和 DDPG。但是，这两种方法都给我带来了一些非常糟糕的结果（比如奖励随着时间的推移而减少），我希望有人能帮助我找到更好的方向。 因此，代理进行包含每个节点的电压值和一些其他连续值的观察，然后执行能够改变电网每个节点负载的操作（因此是一个具有 24 个连续值的数组），然后执行功率流以确定新的电压值，然后根据这些新的电压值计算奖励。 我希望我的代理尽可能少地采取行动，并在最短的时间步内解决违规问题。因此，我按以下方式构建了奖励函数：  如果没有违规，我会给它 10 的奖励，并且情节终止 在每个步骤中，如果有违规，我会施加一个基本惩罚，并且我会添加与调整幅度成比例的额外惩罚 如果代理所做的调整过于极端，以至于我的功率流算法无法收敛并停止工作，我会施加 -10 的惩罚并结束情节。  情节设置：情节从包含违规的初始观察开始。当一个情节结束时，下一个情节将从另一个具有电压的观察开始（其中一些具有违规行为）。 我的 PPO 模型具有以下参数： model = PPO(&quot;MlpPolicy&quot;, env, verbose=1, n_steps=256, tensorboard_log=&quot;C:\\Users\\antonio\\Downloads\\RL&quot;, ent_coef=0.01, gamma=0.9) 我选择了较低的伽马，因为代理需要优先快速解决违规行为。 以下是 10k 步 PPO 尝试的指标： https://preview.redd.it/hqxnmef5wu5e1.png?width=1518&amp;format=png&amp;auto=webp&amp;s=bf4cfb3757835ae7729133f646648680a37d4cd7 就是这样，抱歉发了这么长的帖子。无论如何，您有什么建议可以给我吗？    提交者    /u/Sasukespc   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1haezxl/reward_not_improving_for_a_custom_environment/</guid>
      <pubDate>Mon, 09 Dec 2024 17:24:40 GMT</pubDate>
    </item>
    </channel>
</rss>