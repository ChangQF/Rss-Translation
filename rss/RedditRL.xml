<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 04 Jul 2024 21:13:47 GMT</lastBuildDate>
    <item>
      <title>[META] 有学习 RL 的小组吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvdjcr/meta_group_for_learning_rl/</link>
      <description><![CDATA[嗨，正在寻找想要以自上而下的方式（先了解大局，然后了解细节）开始学习 RL 的人，例如 fast.ai 课程。谁都感兴趣？    提交人    /u/Designer-Air8060   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvdjcr/meta_group_for_learning_rl/</guid>
      <pubDate>Thu, 04 Jul 2024 18:17:30 GMT</pubDate>
    </item>
    <item>
      <title>在 Q-Learning 算法中加入专家知识</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvbgts/including_expert_knowledge_in_qlearning_algorithms/</link>
      <description><![CDATA[大家好 :) 您是否有任何经验或知道一些将专家知识纳入深度 q 学习算法的最佳实践方法？ 目前，我考虑先验地在重放缓冲区中包含一些状态、动作和奖励对。这是一种合适的方法吗？    提交人    /u/No_Individual_7831   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvbgts/including_expert_knowledge_in_qlearning_algorithms/</guid>
      <pubDate>Thu, 04 Jul 2024 16:48:23 GMT</pubDate>
    </item>
    <item>
      <title>“在 HATETRIS 中创下世界纪录”，Dave & Filipe 2022（AlphaZero 失败后高度优化的光束搜索）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</link>
      <description><![CDATA[        提交人    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaowg/getting_the_world_record_in_hatetris_dave_filipe/</guid>
      <pubDate>Thu, 04 Jul 2024 16:14:25 GMT</pubDate>
    </item>
    <item>
      <title>“AlphaZero 的蒙特卡洛图搜索”，Czech 等人 2020 年（将树转换为 DAG 以节省空间）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dvaed1/montecarlo_graph_search_for_alphazero_czech_et_al/</guid>
      <pubDate>Thu, 04 Jul 2024 16:01:49 GMT</pubDate>
    </item>
    <item>
      <title>“《赛道狂飙》中的机器学习历史”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dva3yv/the_history_of_machine_learning_in_trackmania/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dva3yv/the_history_of_machine_learning_in_trackmania/</guid>
      <pubDate>Thu, 04 Jul 2024 15:49:37 GMT</pubDate>
    </item>
    <item>
      <title>非平稳环境中的策略梯度算法收敛。嗨，我正在尝试阅读有关非平稳环境中的 PG。在这样的环境中如何保证收敛到局部最优。在这方面有没有好的教程或论文。谢谢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duyxd7/policy_gradient_algorithm_convergence_in_non/</link>
      <description><![CDATA[在非平稳环境中，还需要哪些其他因素来处理。     提交人    /u/aabra__ka__daabra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duyxd7/policy_gradient_algorithm_convergence_in_non/</guid>
      <pubDate>Thu, 04 Jul 2024 05:14:48 GMT</pubDate>
    </item>
    <item>
      <title>帮助 NEAT-python - 贪吃蛇游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duy1dz/help_with_neatpython_snake_game/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duy1dz/help_with_neatpython_snake_game/</guid>
      <pubDate>Thu, 04 Jul 2024 04:21:28 GMT</pubDate>
    </item>
    <item>
      <title>如果重量极化稳定下来，是不是很糟糕？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duxsdl/is_weight_polarization_bad_if_it_stabilizes/</link>
      <description><![CDATA[        提交人    /u/Breck_Emert   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duxsdl/is_weight_polarization_bad_if_it_stabilizes/</guid>
      <pubDate>Thu, 04 Jul 2024 04:06:53 GMT</pubDate>
    </item>
    <item>
      <title>扩散器/决策扩散器在什么样的视野上进行训练和生成？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1durt7n/what_horizon_does_diffuserdecision_diffuser_train/</link>
      <description><![CDATA[这里有人用过 Janner 的扩散器或 Ajay 的决策扩散器吗？ 我想知道他们为 d4rl 任务训练扩散模型的范围（即序列长度）是否与他们生成的计划的范围（序列长度）相同。  根据论文或代码库配置，目前尚不清楚；但直觉上我会想象，为了完成任务，生成的计划的序列长度应该比他们训练的序列长度更长，特别是如果训练序列最终没有达到目标或只是达到目标的序列的子集。    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1durt7n/what_horizon_does_diffuserdecision_diffuser_train/</guid>
      <pubDate>Wed, 03 Jul 2024 22:57:49 GMT</pubDate>
    </item>
    <item>
      <title>无法决定使用异步还是同步中间件来与 TensorFlow 模型交互自定义模拟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/</link>
      <description><![CDATA[大家好。 我目前正在 MARL n 上进行一个项目，我将在一个 Docker 容器中运行 Python 模拟，在另一个容器中运行 Tensorflow 模型。我知道通过使用 docker compose，我将创建一个包含这些容器的本地网络，但无法真正决定是使用消息队列来通信代理和模型 [他们将询问模型下一步要做什么操作，并且他们还将为其提供数据（无需任何响应）来训练它]；还是使用 Rest API + 模型从中消耗的缓冲区。 您能推荐我任何涉及该主题的参考资料吗？提前谢谢您，祝您有美好的一天     提交人    /u/Miss_Bat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dur1ml/cant_decide_between_async_or_sync_middleware_to/</guid>
      <pubDate>Wed, 03 Jul 2024 22:22:48 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch 与 Jax 2024 在 RL 环境/代理方面的比较</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dupwpu/pytorch_vs_jax_2024_for_rl_environmentsagents/</link>
      <description><![CDATA[只是为了澄清一下。我正在编写一个自定义环境。RL 算法设置为在 JAX 中运行最快（例如稳定基线），因此即使在 Pytorch/JAX 中运行环境的速度一样快，使用 JAX 更明智，因为您可以直接传递数据，或者从 pytorch 到 cpu 再到 jax（用于训练代理）的数据传输速度如此之快，在增加的时间方面是微不足道的？ 或者 pytorch 生态系统是否足够强大，它与 jax 实现一样快    提交人    /u/paswut   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dupwpu/pytorch_vs_jax_2024_for_rl_environmentsagents/</guid>
      <pubDate>Wed, 03 Jul 2024 21:33:18 GMT</pubDate>
    </item>
    <item>
      <title>梦想家 V3 - Rllib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duphip/dreamer_v3_rllib/</link>
      <description><![CDATA[您好， 我正在尝试将 Dreamer V3 实现到 cartpole。我的代码应该可以工作，但我收到此错误；  &quot;ValueError: 注册优化器中的一个参数 (&lt;KerasVariable shape=(4, 256), dtype=float32, path=dreamer\_model/vector\_encoder/dense/kernel&gt;) 不是 tf.Variable！&quot;  我认为这与 DreamerV3Config 有关。该错误是由 DreamerV3Config 和 TensorFlow 之间的关系引起的。但这似乎很奇怪。 DreamerV3Config 由 rllib 提供，因此这不应包含错误。 您对如何解决这个问题有什么想法吗？ 这是我的代码； 来自 ray.rllib.algorithms.dreamerv3.dreamerv3 导入 DreamerV3Config 来自 ray.tune.logger 导入 pretty_print  config = ( DreamerV3Config() .environment(&quot;CartPole-v1&quot;) .training( model_size=&quot;XS&quot;, training_ratio=1024, ) )  algo = config.build()  for i in range(10): result = algo.train() print(pretty_print(result))  if i % 5 == 0: checkpoint_dir = algo.save().checkpoint.path print(f&quot;检查点保存在目录 {checkpoint_dir}&quot;)    提交人    /u/Plenty-Context-8935   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duphip/dreamer_v3_rllib/</guid>
      <pubDate>Wed, 03 Jul 2024 21:15:13 GMT</pubDate>
    </item>
    <item>
      <title>最简单的 RL 环境无法通过 SB3 PPO 进行学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duh0d2/simplest_rl_environment_not_learning_with_sb3_ppo/</link>
      <description><![CDATA[      大家好， 我是 RL 的初学者。我已经构建了一个非常简单的 Env，我想训练一个代理将其用作课程培训的第一步。Env 随机选择一个高度值 (desiredAlt)。env 将动作映射到高度 (选定高度)，误差就是差值。我使用误差的负绝对值作为奖励，希望奖励随着时间的推移会变小。  class Env(gym.Env): metadata = {&#39;render_modes&#39;: [&#39;human&#39;],&#39;render_fps&#39;: 30} MAX_ALT = 5000 def __init__(self, render_mode): super(Env, self).__init__() self.action_space = space.Box(low=-0, high=1, shape=(1,), dtype=np.float32) self.observation_space = space.Box(low=-np.inf, high=np.inf, shape=(2,), dtype=np.float32) self.render_mode = render_mode self.reset() def reset(self, seed=None, options=None): self.done = False self.current_step = 0 self.desiredAlt = np.random.random() * self.MAX_ALT self.altError = 0 返回self._get_obs(), {} def _get_obs(self): return np.array([ self.desiredAlt, self.altError, ], dtype=np.float32) def step(self, action): assert self.action_space.contains(action), &quot;无效操作&quot; self.altitude = action[0] * self.MAX_ALT self.altError = self.desiredAlt - self.altitude self.reward = -np.absolute(self.altError) truncated = False self.done = self.altError &lt; 1 或 self.current_step &gt;= 100 self.current_step += 1 return self._get_obs(), self.reward, self.done, truncated, {} def render(self, mode=&#39;human&#39;): if self.render_mode == &#39;human&#39;: print( f&quot;{self.current_step} | &quot; f&quot;Desired Alt: {self.desiredAlt:.2f}&quot; f&quot; Alt Error after action:{self.altError:.2f}&quot; f&quot; Reward:{self.reward:.6f}&quot; )  我正在使用 SB3 PPO。这是我的 Trainer.py 中的相关部分   class CustomCallback(BaseCallback): def __init__(self, verbose=0): super(CustomCallback, self).__init__(verbose) def _on_step(self) -&gt; bool: return True vec_env = make_vec_env(lambda: env, n_envs=1) model = PPO(&#39;MlpPolicy&#39;, vec_env, verbose=1, tensorboard_log=&quot;./tensorboard/&quot;) model.learn(total_timesteps=500 * 1000, callback=CustomCallback())  这个环境有意义吗？代理是否会知道它只需从给定的奖励（在本例中为错误）中选择正确的高度值？ 我的 ep_rew_mean 如下。它似乎不会比某个值更好，然后在某些时间步骤中也会变得明显更糟。这是为什么呢？ https://preview.redd.it/4d9b4n0fmbad1.png?width=1289&amp;format=png&amp;auto=webp&amp;s=1d05970fe010c9b173742977ba73c0b219e9d9d9    提交人    /u/RamenKomplex   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duh0d2/simplest_rl_environment_not_learning_with_sb3_ppo/</guid>
      <pubDate>Wed, 03 Jul 2024 15:21:33 GMT</pubDate>
    </item>
    <item>
      <title>DRL 算法是否有可能像监督学习一样面临过度拟合问题？如果是这样，那么我们如何检查和缓解它？我将感谢任何与此相关的信息。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dud2i0/does_there_is_possibility_to_drl_algorithm_to/</link>
      <description><![CDATA[  由    /u/Correct-Jaguar-339  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dud2i0/does_there_is_possibility_to_drl_algorithm_to/</guid>
      <pubDate>Wed, 03 Jul 2024 12:25:33 GMT</pubDate>
    </item>
    <item>
      <title>寻求 RL 研究和实验出版标准的指导</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1duc2dc/seeking_guidance_on_rl_research_and_experiment/</link>
      <description><![CDATA[大家好， 我是强化学习 (RL) 研究领域的新手，有几个问题。如果您能提供任何帮助，我将不胜感激。请原谅我的英语不好。  强化学习研究中的实验结果标准：  我正在努力改进 Soft Actor-Critic (SAC) 算法。在知名的机器学习期刊上发表实验结果的惯例或标准是什么？例如，在 DeepMind Control Suite 或 OpenAI Gym 等知名基准上，平均总奖励提高 5%-10% 就足够了吗？  实验步骤和重复：  我注意到一些 RL 研究论文进行了 200 万步的实验，并重复每个实验五次。如果我进行 100 万步实验并重复每个实验三次，结果是否仍然足以令人信服，使我的论文被顶级 ML 期刊接受？产生令人信服的结果所需的最少步骤和重复次数是多少？  RL 训练的硬件建议：  我应该投资 M2 Ultra 或配备 i9-14900K 和 RTX 4090 的 PC 来训练我的模型吗？在 M2 Ultra 上使用 Docker 运行实验是否可行，结果是否足以令人信服地发表？ 提前感谢您的指导！    提交人    /u/Tonight223   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1duc2dc/seeking_guidance_on_rl_research_and_experiment/</guid>
      <pubDate>Wed, 03 Jul 2024 11:30:51 GMT</pubDate>
    </item>
    </channel>
</rss>