<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 27 Jun 2024 06:21:29 GMT</lastBuildDate>
    <item>
      <title>关于 ddpg 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpk45g/question_about_ddpg/</link>
      <description><![CDATA[我正在模拟训练一个带有移动底座和 9 个关节的机械臂来打乒乓球，每一步，状态还包含球拍中心的坐标和精确计算出的击球点的坐标，这样 AI 所需要做的就是将球拍移动到该点。我正在使用 DDPG，想知道在 1.2m 步之后仍然没有学会将球拍移动到精确的位置是否正常。    提交人    /u/MaxiHP   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpk45g/question_about_ddpg/</guid>
      <pubDate>Thu, 27 Jun 2024 05:57:00 GMT</pubDate>
    </item>
    <item>
      <title>TSP 的决斗 QN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpjobo/dueling_qns_for_tsp/</link>
      <description><![CDATA[您好， 我在弄清楚如何为类似 TSP 的问题实现决斗 DQN 时遇到了问题。 我有 N 个地方可以分配资源，并且在一个给定的地方要分配 4 种类型的资源。我的环境会根据分配的资源和当时选择的地方给我奖励。该地方只能选择一次。我正在尝试找出最佳顺序。 所以我使用了决斗 DQN。现在我有一个近似 Q (s,a) = V(s) + A(s,a) 的 CNN。由于 A(s,a) 为您提供了状态和动作的好坏值。然后我认为我可以使用 argmax(A(s,a)) 来选择下一个分配资源的最佳位置，然后到那个地方使用我的 Q(s,a) 来执行此操作。例如，如果 argmax(A(s,a)) 告诉我这是位置 2，那么我将获得 Q(2,a) 的 argmax。  但是，我正在查看文献，我发现没有人使用它来解决这类问题。我发现更接近的是策略优势迭代。这也使用了使用优势函数的 argmax 的概念，但它使用了策略网络。虽然我知道我可以改用这种方法，但为什么不遵循我的初始程序？我哪里错了？  谢谢！     提交人    /u/GreenAppleRL   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpjobo/dueling_qns_for_tsp/</guid>
      <pubDate>Thu, 27 Jun 2024 05:28:33 GMT</pubDate>
    </item>
    <item>
      <title>如何正确理解范畴批评的实施过程？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpfjp5/how_to_understand_the_process_of_implementing/</link>
      <description><![CDATA[嗨！ 我尝试将标量批评家值转换为分类分布，如 C51。但是，我对分布式 RL 还不熟悉，无法理解这个过程。例如，以下代码片段完成了分类过程。  假设原子分布在二维 x 轴上，value 是否应该分布在 y 轴上？ limit 是什么意思？ 为什么 lower 由 value.floor().long() + limit 得出？可以用 value.floor().long() 替换吗？  ​ def to_categorical(value, limit=300): value = value.float() # 避免任何 fp16 恶作剧 value = value.clamp(-limit, limit) distribution = torch.zeros(value.shape[0], (limit*2+1), device=value.device) lower = value.floor().long() + limit upper = value.ceil().long() + limit upper_weight = value % 1 lower_weight = 1 - upper_weight distribution.scatter_add_(-1, lower.unsqueeze(-1), lower_weight.unsqueeze(-1)) distribution.scatter_add_(-1, upper.unsqueeze(-1), upper_weight.unsqueeze(-1)) return distribution  谢谢提前为您的关注和帮助！    提交人    /u/UpperSearch4172   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpfjp5/how_to_understand_the_process_of_implementing/</guid>
      <pubDate>Thu, 27 Jun 2024 01:42:18 GMT</pubDate>
    </item>
    <item>
      <title>“通过联合示例选择进行数据管理进一步加速了多模态学习”，Evans 等人 2024 {DM} (CLIP)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dpeba1/data_curation_via_joint_example_selection_further/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dpeba1/data_curation_via_joint_example_selection_further/</guid>
      <pubDate>Thu, 27 Jun 2024 00:39:40 GMT</pubDate>
    </item>
    <item>
      <title>关于 SB3 的 PPO 问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dp768e/questions_about_ppo_from_sb3/</link>
      <description><![CDATA[1) SB3 的 PPO 是否使用经验缓冲区重放？ 2) 它是多智能体吗？如果不是，我该如何实现（任何指针）？想要转换 PPO 环境，以便所有智能体都有自己的决策网络。    提交人    /u/OccupyFood101   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dp768e/questions_about_ppo_from_sb3/</guid>
      <pubDate>Wed, 26 Jun 2024 19:26:15 GMT</pubDate>
    </item>
    <item>
      <title>q-learning：如何处理模型被锁定在动作序列中？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dp6rgb/qlearning_how_to_handle_the_model_being_locked/</link>
      <description><![CDATA[有很多场景会发生这种情况 - 最常见的可能是 QTE（快速时间事件），您会触发过场动画并必须按下按钮来响应。我遇到的问题是，一旦我的模型决定购买某样东西，它就会被锁定在交易步骤的序列中。我担心一些事情  它是否只有在整个序列之后才能获得购买的物品（在状态空间中）？因为当您决定进去购买它时获得它更直观（迫使您在那时购买它） 它是否只有在整个序列之后才能获得奖励？  我倾向于的方法 - 没有来源，也不是我胡乱想的 - 是它应该立即在其状态空间中获得卡，至少，可能还有奖励。然后，当它在交易中花钱时，它只会稍微更新状态 - 只是稍微改变玩家的资源。但这样一来，之后的举动就完全没用了——它们是有保证的，不会对任何事情产生影响，所以我担心这会完全破坏 q 函数。    提交人    /u/Breck_Emert   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dp6rgb/qlearning_how_to_handle_the_model_being_locked/</guid>
      <pubDate>Wed, 26 Jun 2024 19:09:36 GMT</pubDate>
    </item>
    <item>
      <title>寻求强化学习和 GAN 的课程推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dovbp2/seeking_course_recommendations_for_reinforcement/</link>
      <description><![CDATA[大家好， 我目前正在做我的毕业设计，我想深入研究强化学习 (RL) 和生成对抗网络 (GAN)。我对机器学习和深度学习概念有基本的了解，但对这些特定领域还比较陌生。 您能否推荐一些适合 RL 和 GAN 初学者的好课程或资源？我对免费和付费选项都持开放态度。任何关于书籍或在线教程的建议也欢迎。 提前感谢您的帮助！    提交人    /u/Gemyy48812   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dovbp2/seeking_course_recommendations_for_reinforcement/</guid>
      <pubDate>Wed, 26 Jun 2024 10:29:31 GMT</pubDate>
    </item>
    <item>
      <title>如何解决运行 MuZero 常规时的错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dom7kr/how_to_resolve_errors_when_running_muzero_general/</link>
      <description><![CDATA[我是日本人，所以我使用翻译器。 当我尝试运行 MuZero general 时，出现以下错误消息 我想知道如何解决这个问题。 (base) C:\Users\kooou\study\muzero-general-master&gt;python muzero.py 欢迎使用 MuZero！以下是游戏列表： 0. atari 1. breakout 2. cartpole 3. connect4 4. gomoku 5. gridworld 6. lunarlander 7. simple_grid 8. spiel 9. tictactoe 10. twentyone 输入一个数字来选择游戏：9 2024-06-26 09:24:16,548 INFO worker.py:1770 -- 启动了本地 Ray 实例。回溯（最近一次调用最后一次）：文件“C:\Users\kooou\study\muzero-general-master\muzero.py”，第 650 行，在&lt;module&gt; muzero = MuZero(game_name) ^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\muzero.py&quot;, 第 122 行, 在 __init__ self.checkpoint[&quot;weights&quot;], self.summary = copy.deepcopy(ray.get(cpu_weights)) ^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\auto_init_hook.py&quot;, 第 21 行, 在 auto_init_wrapper 中 return fn(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\client_mode_hook.py&quot;, 第 103 行, 在包装器中返回 func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\worker.py&quot;, 第 2630 行, 在获取值中, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\worker.py&quot;, 第863，在 get_objects 中引发 value.as_instanceof_cause() ray.exceptions.RayTaskError(IndexError): ray::CPUActor.get_initial_weights() (pid=24336, ip=127.0.0.1, actor_id=93a6ec0dc4612ea8190775ec01000000, repr=&lt;muzero.CPUActor object at 0x0000021E07EC1FD0&gt;) 文件 &quot;python\ray\_raylet.pyx&quot;，第 1893 行，在 ray._raylet.execute_task 文件 &quot;python\ray\_raylet.pyx&quot;，第 1834 行，在 ray._raylet.execute_task.function_executor 文件中&quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\_private\function_manager.py&quot;，第 691 行，在 actor_method_executor 中返回方法（__ray_actor，*args，**kwargs） ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\Anaconda\Lib\site-packages\ray\util\tracing\tracing_helper.py&quot;，第 467 行，在 _resume_span 中返回方法（self，*_args，**_kwargs） ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\study\muzero-general-master\muzero.py&quot;, 第 489 行, 在 get_initial_weights model = models.MuZeroNetwork(config) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\models.py&quot;, 第 23 行, 在 __new__ return MuZeroResidualNetwork( ^^^^^^^^^^^^^^^^^^^^^^^^^ 文件 &quot;C:\Users\kooou\study\muzero-general-master\models.py&quot;, 第 487 行, 在 __init__ self.representation_network = torch.nn.DataParallel( ^^^^^^^^^^^^^^^^^^^^^^^^^ 文件&quot;C:\Users\kooou\Anaconda\Lib\site-packages\torch\nn\parallel\data_parallel.py&quot;，第 150 行，在 __init__ output_device = device_ids[0] ~~~~~~~~~~^^^ IndexError: 列表索引超出范围  谢谢。    提交人    /u/Sufficient-Fly-4040   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dom7kr/how_to_resolve_errors_when_running_muzero_general/</guid>
      <pubDate>Wed, 26 Jun 2024 01:10:24 GMT</pubDate>
    </item>
    <item>
      <title>muzero 如何构建他们的 MCTS？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dogoda/how_does_muzero_build_their_mcts/</link>
      <description><![CDATA[在 Muzero 中，他们同时在各种不同的游戏环境（围棋、atari 等）上训练他们的网络。  在训练期间，MuZero 网络展开 K 个假设步骤，并与从 MCTS 参与者生成的轨迹中采样的序列对齐。通过从重放缓冲区中的任何游戏中采样一个状态来选择序列，然后从该状态展开 K 个步骤。  我无法理解 MCTS 树是如何构建的。每个游戏环境都有一棵树吗？ 是否假设每个环境的初始状态都是恒定的？（不知道这是否适用于所有 atari 游戏）    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dogoda/how_does_muzero_build_their_mcts/</guid>
      <pubDate>Tue, 25 Jun 2024 21:00:03 GMT</pubDate>
    </item>
    <item>
      <title>CNN 足以支持 Connect 4 自玩吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do7vbk/is_cnn_enough_for_connect_4_self_play/</link>
      <description><![CDATA[  由    /u/Professional_Card176  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do7vbk/is_cnn_enough_for_connect_4_self_play/</guid>
      <pubDate>Tue, 25 Jun 2024 14:50:24 GMT</pubDate>
    </item>
    <item>
      <title>RL 动作的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do7o32/problem_with_rl_actions/</link>
      <description><![CDATA[大家好，我有一个包含 24 个元素的目标数组，RL 将每个元素分开处理，并从一个函数（更像一个黑匣子）获取反馈，奖励是目标期望值与实际值（当然是负值）之间的差异。 所以我的问题是，有没有办法让模型知道当前正在处理哪个元素（索引）？ 我如何定义这个代理的状态？ 抱歉，我是 RL 新手，所以请原谅我的理解:) 注意：我在 python 上使用稳定基线 3，请随时询问更多信息，谢谢！    提交人    /u/Tiger-2001   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do7o32/problem_with_rl_actions/</guid>
      <pubDate>Tue, 25 Jun 2024 14:41:52 GMT</pubDate>
    </item>
    <item>
      <title>请帮助我理解常数α蒙特卡洛方法。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do7cpp/please_help_me_understand_constantα_monte_carlo/</link>
      <description><![CDATA[      这是我迄今为止对用于近似价值函数的蒙特卡罗方法的理解： 蒙特卡罗方法不使用递归贝尔曼方程和环境动力学知识，而是使用统计数据来评估价值函数。对于给定的策略π和起始状态S_t，会产生多个事件。计算每个事件的回报并取平均值。如果样本数量足够大，则计算值会收敛到预期回报。  但根据常数 α 蒙特卡罗，价值函数使用以下更新规则进行评估： https://preview.redd.it/3ct62f2z9q8d1.png?width=452&amp;format=png&amp;auto=webp&amp;s=b450e965cf90765ea3de19f0b51258064f0702ac 其中 V(S_t) 是状态 S_t 的价值函数的当前估计，G_t 是时间步长 t 之后的回报。我不明白这与平均方法有何关联。     提交人    /u/Dead_as_Duck   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do7cpp/please_help_me_understand_constantα_monte_carlo/</guid>
      <pubDate>Tue, 25 Jun 2024 14:28:24 GMT</pubDate>
    </item>
    <item>
      <title>CFR 实际上是一种强化学习算法吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do6zdg/is_cfr_actually_a_reinforcement_learning_algorithm/</link>
      <description><![CDATA[尽管 CFR 与“传统”强化学习算法一样，依赖于从经验中学习并在顺序决策中找到最佳策略，但 CFR 并不依赖于价值函数或策略。那么 CFR 是不是 RL 算法？如果我的一些说法有误，请原谅我，在这个分类中有点迷失了方向    提交人    /u/Bubbly-Stranger-1175   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do6zdg/is_cfr_actually_a_reinforcement_learning_algorithm/</guid>
      <pubDate>Tue, 25 Jun 2024 14:12:11 GMT</pubDate>
    </item>
    <item>
      <title>理解和诊断深度强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</link>
      <description><![CDATA[发表于 ICML 2024 https://openreview.net/pdf?id=s9RKqT7jVM    由   提交  /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1wtj/understanding_and_diagnosing_deep_reinforcement/</guid>
      <pubDate>Tue, 25 Jun 2024 09:31:31 GMT</pubDate>
    </item>
    <item>
      <title>HPC 集群中超参数搜索和训练速度缓慢</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1do1cim/slow_hyperparameter_search_and_training_in_hpc/</link>
      <description><![CDATA[大家好， 我目前正在使用 PPO 训练自主决策卫星星座，最多可进行 86400 个模拟步骤（每秒 1 步）。尽管使用了强大的硬件，包括 4 个 NVIDIA A100 GPU 和 11 个 CPU，但训练过程仍然非常缓慢。以下是我的设置和配置的摘要： **软件和库版本：** * **容器基础：**NVIDIA PyTorch 22.09 py3 * **库：** numpy==1.23.5 gymnasium==0.28.1 matplotlib==3.7.1 pandas==1.5.3 ray==2.10.0 ray[tune]==2.10.0 typer==0.7.0 dm_tree tree scikit-image lz4 gputil==1.4.0 pyarrow **环境配置：** * num_targets：10 * num_observers：10 * time_step：1 秒 * 持续时间：86400 秒 - 24 小时 **训练配置：** * batch_mode：“complete_episodes” * rollout_fragment_length：“auto” * num_rollout_workers：10 * num_envs_per_worker：1 * num_cpus_per_worker：1 * num_gpus_per_worker：0 * num_learner_workers：4 * num_cpus_per_learner_worker: 1 * num_gpus_per_learner_worker: 1 **搜索空间配置：** * fcnet_hiddens: [[64, 64], [128, 128], [256, 256], [64, 64, 64]] * num_sgd_iter: [10, 30, 50] * lr: [1e-5, 1e-3] * gamma: [0.9, 0.99] * lambda: [0.9, 1.0] * train_batch_size: [512, 1024, 2048, 4096] * sgd_minibatch_size：[32, 64, 128, 512] 训练过程非常缓慢，单次迭代在 1 小时内无法完成（30 个样本的超参数搜索，20 次迭代将需要一个多月）。我期望使用 4 个 A100 GPU 进行更快的训练。以下是我尝试和观察到的几件事： * 大部分时间都花在了训练上，而不是环境模拟上。使用 10 个推出的工作人员、每个工作人员 1 个环境和每个工作人员 1 个 CPU，大约需要 5 分钟来模拟环境。 * 将每个工作人员的 CPU 数量增加到 7 个（70 个 CPU）实际上减慢了该过程，需要 15 分钟左右才能完成模拟。 * 我可以在我的 MacBook Pro M2（大约 20 次迭代）以及 NVIDIA Jetson AGX Orin 上进行训练，但当然每次迭代大约需要 90 分钟，而且它们的批次大小更小并且参数经过调整。因此，我希望使用新硬件进行更快的训练。 * 我不完全理解以下示例中的这一部分：0.0/1.0 accelerater_type:A100 这里有一个例子： 试用状态：1 正在运行 | 3 PENDING 当前时间：2024-06-25 10:33:42。总运行时间：1小时1分6秒 逻辑资源使用情况：11.0/80 CPU、4.0/4 GPU（0.0/1.0 accelerater_type:A100） ╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮ │ 试验名称状态 gamma lr train_batch_size sgd_minibatch_size num_sgd_iter lambda model/fcnet_hiddens │ ═──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┤ │ PPO_FSS_env-v0_17a5c_00000 正在运行 0.928133 1.38519e-05 2048 128 30 0.905579 [128, 128] │ │ PPO_FSS_env-v0_17a5c_00001 正在处理 0.972168 8.7419e-05 1024 128 10 0.957066 [256, 256] │ │ PPO_FSS_env-v0_17a5c_00002 待定 0.906353 2.12536e-05 2048 512 50 0.968513 [64, 64] │ │ PPO_FSS_env-v0_17a5c_00003 待定 0.948159 0.000202029 512 64 30 0.997584 [64, 64] │ ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯ 如有任何关于进一步优化训练过程的建议或见解，我们将不胜感激！    提交人    /u/CLEMENMAN   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1do1cim/slow_hyperparameter_search_and_training_in_hpc/</guid>
      <pubDate>Tue, 25 Jun 2024 08:51:31 GMT</pubDate>
    </item>
    </channel>
</rss>