<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 02 Apr 2024 01:00:44 GMT</lastBuildDate>
    <item>
      <title>“Deep de Finetti：从大型语言模型中恢复主题分布”，Zhang 等人，2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1btihaf/deep_de_finetti_recovering_topic_distributions/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1btihaf/deep_de_finetti_recovering_topic_distributions/</guid>
      <pubDate>Mon, 01 Apr 2024 22:53:04 GMT</pubDate>
    </item>
    <item>
      <title>政策梯度——未来回报</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1btd7im/policy_gradients_future_looking_returns/</link>
      <description><![CDATA[        由   提交/u/Both_Ebb_327   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1btd7im/policy_gradients_future_looking_returns/</guid>
      <pubDate>Mon, 01 Apr 2024 19:33:58 GMT</pubDate>
    </item>
    <item>
      <title>openAi 健身房赛车出现问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bt80t6/trouble_with_car_racing_from_openais_gym/</link>
      <description><![CDATA[我是强化学习的初学者，正如标题所说，我在让我的代理在 openAi 的赛车环境中学习时遇到了一些麻烦 我正在使用带经验回放的双 dqn，似乎无论我让它训练多长时间，我将批次或内存设置多大，它似乎都无法学习，为什么？我会如果有人有兴趣查看，请添加 github 链接 https://github.com/OogaBooga21/Car_Racing。 git    由   提交 /u/AnalSpecialist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bt80t6/trouble_with_car_racing_from_openais_gym/</guid>
      <pubDate>Mon, 01 Apr 2024 16:19:22 GMT</pubDate>
    </item>
    <item>
      <title>用于逆强化学习的库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bsx7sa/libraries_for_inverse_reinforcement_learning/</link>
      <description><![CDATA[我想使用逆强化学习来训练熊猫健身房的熊猫拾取和放置机械臂，并且想知道是否有任何库可以实现此目的。我尝试了“imitation”库，在其中尝试使用 AIRL 算法，但作为初学者，我无法做到正确。谁能建议我如何做到这一点？我可以参考任何库或 github 存储库吗？谢谢。   由   提交/u/confused_agent2024  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bsx7sa/libraries_for_inverse_reinforcement_learning/</guid>
      <pubDate>Mon, 01 Apr 2024 07:15:15 GMT</pubDate>
    </item>
    <item>
      <title>为什么对表格案例的分析比对连续案例的分析更难理解？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bsu3dd/why_analysis_on_tabular_case_is_harder_to/</link>
      <description><![CDATA[ 由   提交/u/Professional_Card176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bsu3dd/why_analysis_on_tabular_case_is_harder_to/</guid>
      <pubDate>Mon, 01 Apr 2024 04:02:20 GMT</pubDate>
    </item>
    <item>
      <title>Noob Post - 2024 超级马里奥兄弟</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bspvoy/noob_post_2024_super_mario_bros/</link>
      <description><![CDATA[我只是想发布此内容，因为我花了太长时间试图让马里奥兄弟使用 PPO 设置，而我只是一直拥有模块由于健身房更新而出现问题。 这是最终使它对我有用的视频：https:/ /youtu.be/_gmQZToTMac?si=t2C8uY1eJrKf32NJ 通常我只是关注 Nicholas Renotte 的视频 (https ://youtu.be/2eeYqJ0uBKE?si=z70WzwUyJjcpwjUy）但是对gym的更新，其中done现在终止并截断了我的命。我弃用了这些模块，我觉得我尝试了新学习者能做的一切，但一直失败。 我知道我在 Reddit、Stack Overflow 和 YouTube 上搜索了解决方案，但没有成功。我将其发布给其他学习者，以便他们能够跳过我的痛苦。   由   提交 /u/AcrobaticAmoeba8158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bspvoy/noob_post_2024_super_mario_bros/</guid>
      <pubDate>Mon, 01 Apr 2024 00:34:14 GMT</pubDate>
    </item>
    <item>
      <title>普通的 DDPG、PPO 或 SAC 可以解决 AntMaze/PointMaze 环境吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bs9hsm/can_vanilla_ddpgppo_or_sac_solve_antmazepointmaze/</link>
      <description><![CDATA[大家好。我是GCRL的菜鸟。我正在尝试使用普通算法在gymnasium-robotics 的 PointMaze 环境中执行 GCRL。 我尝试了 DDPG+HER 和普通 PPO。然而，经过 120 万步训练后，智能体的评估成功率仍然小于 10%。 DDPG+HER 模型在 FetchReach 和 FetchPick&amp;Place 中运行良好。所以我认为这可能不是编码问题。 我想环境对于这些普通算法来说太复杂了...... 顺便说一句，我发现desired_goal不会是即使智能体已经到达，也会立即改变大约 10 步。这是环境的一个特征吗？ ​   由   提交 /u/Chips-HalfPrice   /u/Chips-HalfPrice reddit.com/r/reinforcementlearning/comments/1bs9hsm/can_vanilla_ddpgppo_or_sac_solve_antmazepointmaze/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bs9hsm/can_vanilla_ddpgppo_or_sac_solve_antmazepointmaze/</guid>
      <pubDate>Sun, 31 Mar 2024 12:40:13 GMT</pubDate>
    </item>
    <item>
      <title>RL 课程和 torchRL 指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bs196p/guidance_for_rl_course_torchrl/</link>
      <description><![CDATA[我对 pytorch、torchvision 和 torchtext 非常熟悉。 现在，我正在尝试使用 torchRL &amp;成为一名自信的强化学习工程师。我当前的路线图是： ​ - [Sergey Levine 的 CS-285 课程](https://rail.eecs.berkeley.edu/deeprlcourse/) - Sutton &amp; 出版社的强化学习书籍Barto - 然后是 torchRL 文档。 ​ 这是一个好方法吗？请提供您的反馈。您的小指导可能会节省我几周的时间。   由   提交/u/Dark-Matter79  /u/Dark-Matter79 reddit.com/r/reinforcementlearning/comments/1bs196p/guidance_for_rl_course_torchrl/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bs196p/guidance_for_rl_course_torchrl/</guid>
      <pubDate>Sun, 31 Mar 2024 03:59:23 GMT</pubDate>
    </item>
    <item>
      <title>行动空间尺寸缩小？非常数动作空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bs13e8/action_space_size_reduction_nonconstant_action/</link>
      <description><![CDATA[大家好， 我有 CV/NLP 背景，我正在尝试使用 RL 来解决这个问题，但我在 RL 方面的经验非常有限；老实说，我不知道哪种方法适合哪种情况（如果有人对此有一个好的方向，我真的很感激）。  不过，具体来说，我的问题涉及对图表进行操作。我想用强化学习重新连接一个图，我认为动作空间将是：删除边 i,j；添加边 i,j； [可选地，可以将 i,j 重新连接到 i,k 的单独操作]。有一个明显的问题：输入是一张图，输出也是一张图；如果输入集中并非所有图都具有相同数量的节点，则操作空间的大小将不是恒定的。有没有办法解决这个问题？ 此外，有没有办法重新表述这个问题，使任何大小的图形的动作空间大小恒定。  最后，我应该考虑哪些算法？我有一种感觉，如果我心中的行动空间确实是我想要的，那么它就会变得笨拙/太大。  我知道这是一个很大的问题，但我希望你们能在这方面有任何方向。    由   提交 /u/preordains   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bs13e8/action_space_size_reduction_nonconstant_action/</guid>
      <pubDate>Sun, 31 Mar 2024 03:50:50 GMT</pubDate>
    </item>
    <item>
      <title>必须学rl还是sb3就够了</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bro2n6/do_you_have_to_learn_rl_or_is_sb3_enough/</link>
      <description><![CDATA[我知道普通策略梯度和 dqn 然后发现了 sb3 并且刚刚使用这些算法，但没有学习它们是如何工作的，学习它们仍然有帮助吗他们工作或不   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bro2n6/do_you_have_to_learn_rl_or_is_sb3_enough/</guid>
      <pubDate>Sat, 30 Mar 2024 18:03:03 GMT</pubDate>
    </item>
    <item>
      <title>PPO 实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1brj1r2/ppo_implementation/</link>
      <description><![CDATA[      社区您好， 我需要您的帮助来了解PPO实现的更新功能是否正确。我的场景涉及基于图形的状态（动态大小），以及具有选择器层来选择选项的参与者。然后，相应的选项层选择一个动作。我有三个选项，因此有三个选项层。两个选项层具有离散输出，一个选项层具有多头输出。你能帮我纠正这个问题吗？  select_action和update函数如下： 参与者网络快照 Select_action函数 ​ 更新函数 - 第 1 部分 ​ 更新函数 - 第 2 部分  ​ 更新函数 -第三部分   由   提交/u/GuavaAgreeable208  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1brj1r2/ppo_implementation/</guid>
      <pubDate>Sat, 30 Mar 2024 14:23:18 GMT</pubDate>
    </item>
    <item>
      <title>我想训练一个名为 halfcheetah 的 mujoco 模型，有人可以给我一些关于如何处理无限观察范围的建议吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1brbprs/i_want_to_train_a_mujoco_model_known_as/</link>
      <description><![CDATA[ 由   提交 /u/pratyaksh__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1brbprs/i_want_to_train_a_mujoco_model_known_as/</guid>
      <pubDate>Sat, 30 Mar 2024 07:04:10 GMT</pubDate>
    </item>
    <item>
      <title>“TextCrraftor：您的文本编码器可以成为图像质量控制器”，Li 等人 2024 {Snapchat}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1br4p9j/textcraftor_your_text_encoder_can_be_image/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1br4p9j/textcraftor_your_text_encoder_can_be_image/</guid>
      <pubDate>Sat, 30 Mar 2024 00:48:49 GMT</pubDate>
    </item>
    <item>
      <title>DIAMBRA Arena 环境用于让 OpenAI 和 MistralAI 法学硕士在旧金山 MistralAI 黑客马拉松的《街头霸王 III》中相互对战，由两家 YC 初创公司提供</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bqpf2b/diambra_arena_environments_used_to_make_openai/</link>
      <description><![CDATA[       由   提交/u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bqpf2b/diambra_arena_environments_used_to_make_openai/</guid>
      <pubDate>Fri, 29 Mar 2024 13:34:00 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>