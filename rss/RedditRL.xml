<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 29 Dec 2024 09:15:16 GMT</lastBuildDate>
    <item>
      <title>遗憾值为 O( \sqrt(log K T ) ) 的 K 臂随机赌博机算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</link>
      <description><![CDATA[我想知道是否有任何 K 臂随机老虎机算法可以实现 $O(\sqrt(T))$ 遗憾，且因子为常数 $\sqrt{ log K }$。  我知道 exp3 可以实现 O(\sqrt(T)) 遗憾，因子为 sqrt(k log K )，而 UCB 可以实现 \tilde{O}( sqrt(T) ) 遗憾，因子为 sqrt(k)？  是否有一种算法，其臂数与 sqrt( log K ) 类似？或者，是否有更严格的 exp3 或 UCB 分析，可以在臂数方面实现更好的因子？  我正在研究一个问题，其中臂的数量为 K^{a}，其中 a 是某个参数，并且我想将我的因子归结为类似 a * poly(K) - （poly(K) 表示关于 K 的多项式）的东西。    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</guid>
      <pubDate>Sun, 29 Dec 2024 05:59:36 GMT</pubDate>
    </item>
    <item>
      <title>似乎无法理解如何使用 NEAT-Python 结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hokvmb/cant_seem_to_understand_how_to_work_with/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hokvmb/cant_seem_to_understand_how_to_work_with/</guid>
      <pubDate>Sun, 29 Dec 2024 01:33:15 GMT</pubDate>
    </item>
    <item>
      <title>RL“包裹” 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</link>
      <description><![CDATA[我通常会在假期的最后几天努力赶上进度（事实证明这些天不可能）并回顾学术和工业发展方面的主要亮点。请在此处添加您今年的顶级 RL 作品     提交人    /u/blitzkreig3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</guid>
      <pubDate>Sat, 28 Dec 2024 21:08:08 GMT</pubDate>
    </item>
    <item>
      <title>山地车项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hobshj/mountain_car_project/</link>
      <description><![CDATA[      我正在尝试使用 Q 学习、DQN 和 Soft Actor Critic 解决山地车问题。 我设法在离散空间中使用 Q 学习解决了该问题，但在调整 DQN 时，我发现训练图并不像 Q 学习那样收敛。相反，它相当不稳定。但是，当我使用情节长度和回报来评估策略时，我发现大多数种子情节都很短并且奖励更高。这是否意味着我解决了它？ 参数是： {&#39;env&#39;: &lt;gymnax.environments.classic_control.mountain_car.MountainCar at 0x7b368faf7ee0&gt;, &#39;env_params&#39;: {&#39;max_steps_in_episode&#39;: 200, &#39;min_position&#39;: -1.2, &#39;max_position&#39;: 0.6, &#39;max_speed&#39;: 0.07, &#39;goal_position&#39;: 0.5, &#39;goal_velocity&#39;: 0.0, &#39;force&#39;: 0.001, &#39;gravity&#39;: 0.0025}, &#39;eval_callback&#39;: &lt;function RLinJAX.algos.algorithm.Algorithm.create。&lt;locals&gt;.eval_callback(algo, ts, rng)&gt;，&#39;eval_freq&#39;：5000，&#39;skip_initial_evaluation&#39;：False，&#39;total_timesteps&#39;：1000000，&#39;learning_rate&#39;：0.0003，&#39;gamma&#39;：0.99，&#39;max_grad_norm&#39;：inf，&#39;normalize_observations&#39;：False，&#39;target_update_freq&#39;：800，&#39;polyak&#39;：0.98，&#39;num_envs&#39;：10，&#39;buffer_size&#39;：250000，&#39;fill_buffer&#39;：1000，&#39;batch_size&#39;：256， &#39;eps_start&#39;：1，&#39;eps_end&#39;：0.05，&#39;exploration_fraction&#39;：0.6，&#39;agent&#39;：{&#39;hidden_​​layer_sizes&#39;：（64,64），&#39;activation&#39;：&lt;PjitFunction&gt;，&#39;action_dim&#39;：3，&#39;parent&#39;：None，&#39;name&#39;：None}，&#39;num_epochs&#39;：5，&#39;ddqn&#39;：True}  对学习到的政策的评估 编辑：我打印了短剧集百分比和高奖励剧集百分比： 短剧集百分比 99.718 高奖励百分比 99.718    提交人    /u/Ordinary_Reveal8842   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hobshj/mountain_car_project/</guid>
      <pubDate>Sat, 28 Dec 2024 18:26:30 GMT</pubDate>
    </item>
    <item>
      <title>你喜欢观看 RL 内容视频吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ho23ty/do_you_love_watching_rl_content_videos/</link>
      <description><![CDATA[基本上就是标题，但更具体地说： 我喜欢观看机器人学习某些东西（在模拟中）的内容，这比教育意义更有趣。 尤其是来自“AI Warehouse”这样的频道，其中创建者对 AI（立方体/假人）提出挑战，并描述所玩的“游戏”，并附加有趣的评论。 由于这个 subreddit 更倾向于教育而不是娱乐，我想知道你们中有多少人像我（男，27 岁）一样喜欢这些？ （+ 年龄/性别）？    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ho23ty/do_you_love_watching_rl_content_videos/</guid>
      <pubDate>Sat, 28 Dec 2024 09:15:53 GMT</pubDate>
    </item>
    <item>
      <title>RL 是否用于训练游戏《黎明杀机》中的机器人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnsh5d/was_rl_used_to_train_the_bots_in_the_game_dead_by/</link>
      <description><![CDATA[在这个帖子和这个中有很多讨论，但似乎没人知道 - 开发人员对此没有发表任何评论。我也在游戏的 discord 服务器中询问过，但也没人知道。 他们可以使用强化学习来训练它们吗？我对强化学习的了解非常基础，我现在正在尝试研究它（我刚刚开始了解深度 Q 学习）。这似乎是可能的，因为我知道 RL 已经用于很多游戏（尽管我见过的例子都是老式游戏）。    提交人    /u/gitgud_x   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnsh5d/was_rl_used_to_train_the_bots_in_the_game_dead_by/</guid>
      <pubDate>Fri, 27 Dec 2024 23:46:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的第一步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnqx9l/first_step_in_rl/</link>
      <description><![CDATA[如何开始学习/做 RL？ - 用什么方法学习？ - 了解哪些 hello world 项目？ - 学习 RL 的步骤是什么？ - 如果我想从零开始成为 RL 的英雄，我该怎么做？    提交人    /u/Soft_Awareness6826   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnqx9l/first_step_in_rl/</guid>
      <pubDate>Fri, 27 Dec 2024 22:34:09 GMT</pubDate>
    </item>
    <item>
      <title>AAAI 2025 教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</link>
      <description><![CDATA[AI 安全：从强化学习到基础模型 链接：https://aaai.org/conference/aaai/aaai-25/tutorial-and-lab-list/#TQ10     提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</guid>
      <pubDate>Fri, 27 Dec 2024 13:07:15 GMT</pubDate>
    </item>
    <item>
      <title>TD3 算法第二次更新时策略严重偏向</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hndzen/heavily_biased_policy_on_the_second_update_for/</link>
      <description><![CDATA[大家好， 我为演员和评论家的架构实现了一个带有 CNN 和线性层的 TD3 算法。它被用于解决 Open AI Gym 的 CarRacing 环境。动作空间是连续的，其中最大动作为 [1, 1, 1]，最小动作为 [-1, 0, 0]，分别用于转向、加油和刹车。策略更新是在情节结束时完成的，即环境中的步骤处于完成状态时。环境中发生的时间步数用于在情节结束后多次更新策略。重放缓冲区也使用 1000 个时间步的随机动作初始化。 为什么在第一集之后，演员会选择立即开始向左大力转向（-1）的动作？转向几乎始终为 1 且高于 0.9，并且只发生在第二集中。第一集中选择的动作与随机采样动作本质上相同。 非常感谢    提交人    /u/Sea_Farmer5942   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hndzen/heavily_biased_policy_on_the_second_update_for/</guid>
      <pubDate>Fri, 27 Dec 2024 12:37:05 GMT</pubDate>
    </item>
    <item>
      <title>O(sqrt(T)) 遗憾比 O(sqrt(T \log T)) 遗憾更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hn2me2/is_osqrtt_regret_better_than_osqrtt_log_t_regret/</link>
      <description><![CDATA[从数学上讲，sqrt(T) 优于 sqrt(T \log T)，但如果我提交论文，sqrt(T) 遗憾算法会被认为比 sqrt(T \log T) 遗憾算法更好吗？我正在阅读一篇论文，作者声称他们的算法是 \tildeO( \sqrt(T) )；尽管在论文正文中报告遗憾值为 O( \sqrt(T log T) )。我有点困惑，因为我认为 \tilde 应该是“忽略常量/模型参数”，但 log T 不是 T 方面的常数。他们还提到了一个特殊情况，其中遗憾是 O( \sqrt(T) )。我还查看了高概率遗憾与预期遗憾，他们似乎在说预期遗憾的上限为 O( sqrt (T log T ) )。 O( \sqrt(T) ) 是否被认为比 O( \sqrt(T \log T) ) 更好，或者差异可以忽略不计？    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hn2me2/is_osqrtt_regret_better_than_osqrtt_log_t_regret/</guid>
      <pubDate>Fri, 27 Dec 2024 00:41:34 GMT</pubDate>
    </item>
    <item>
      <title>在哪里可以学习价值函数近似，包括例子？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmzjhm/where_can_i_learn_value_function_approximation/</link>
      <description><![CDATA[我正在学习 David Silver 的强化学习课程，我学到了第 6 讲，内容是关于价值函数近似的。我理解了这堂课之前的所有内容，但这堂课的内容对我来说毫无意义，我认为这是因为原来的班级的学生似乎有机器学习的背景，所以他跳过了很多基础知识。有没有什么地方可以让我正确地从头开始学习，最好是有很多例子的地方？    提交人    /u/Hekkowow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmzjhm/where_can_i_learn_value_function_approximation/</guid>
      <pubDate>Thu, 26 Dec 2024 22:14:13 GMT</pubDate>
    </item>
    <item>
      <title>DQN 中的训练图</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmxh3f/training_plot_in_dqn/</link>
      <description><![CDATA[      大家好， 圣诞快乐，节日快乐！ 我在阅读 DQN 代理的训练图时遇到了麻烦，因为它似乎没有太大的改进，但如果我将它与随机代理进行比较，它的结果会更好。 此外，它还有很多噪音，我认为这不是一个好的事情。 我见过一些人在验证剧集中监控奖励图 对于剧集 = 2000： （在 4096 步上进行训练，然后在其中一集上进行验证并使用其奖励进行绘图） 剧集++ 另外，我已经阅读了有关奖励标准化的内容，我应该尝试一下吗？ returns =（returns - returns.mean()）/（returns.std() + eps） 期待任何见解和训练情节已附上。 提前致谢 https://preview.redd.it/wu6j4o9h9n9e1.png?width=884&amp;format=png&amp;auto=webp&amp;s=435c1d881b29412020c355308543eb911b35db94    提交人    /u/Dry-Image8120   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmxh3f/training_plot_in_dqn/</guid>
      <pubDate>Thu, 26 Dec 2024 20:40:51 GMT</pubDate>
    </item>
    <item>
      <title>强化问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmk9pm/reinforcement_problem/</link>
      <description><![CDATA[我忍不住将我 8 个月大的宝宝当作强化学习问题。设计适当的环境和奖励。只需要研究一种算法……    由    /u/tedthemouse 提交   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmk9pm/reinforcement_problem/</guid>
      <pubDate>Thu, 26 Dec 2024 08:33:09 GMT</pubDate>
    </item>
    <item>
      <title>GAE 和 Actor Critic 方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hmepx8/gae_and_actor_critic_methods/</link>
      <description><![CDATA[      我使用单独的参与者和评论家网络实现了相当经典的 GAE 方法。在 CartPole 任务上测试，使用的批处理大小为 8。看起来只有 GAE(lambda=1) 或接近 1 的某个 lambda 才能使参与者模型起作用。这相当于使用经验奖励来计算 td 误差（我对此进行了单独的实现，结果看起来几乎相同）。  任何较小的 lambda 值基本上都不起作用。预期的情节长度（达到步骤的批次平均值）要么永远不会大于 40；要么显示非常坎坷的曲线（在达到相当大的步骤数后迅速变得更糟）；或者只是收敛到一个非常小的值，例如低于 10。  我试图了解这是否是“预期的”。我理解我们不希望策略损失保持/收敛到 0（无论其质量如何都成为确定性策略）。这实际上发生在较小的 lambda 值上。  这纯粹是由于偏差-方差权衡吗？对于较大的（或 1.0）lambda 值，我们期望偏差较低但方差较高。从 Sergey Levine 的课程来看，我们似乎希望总体上避免这种情况？然而，这种“经验蒙特卡罗”方法似乎是唯一适合我的情况的方法。 此外，我们应该监控策略梯度方法的哪些指标？从我目前的观察来看，策略网络的损失或评论模型损失几乎毫无用处……唯一重要的事情似乎是预期的总回报？  分享一些我的 tensorboard 的截图： https://preview.redd.it/x7bcpud9s39e1.png?width=1572&amp;format=png&amp;auto=webp&amp;s=8dec61d8a3f0f0f1a4798a2da7fd15c5d0e7a23a    提交人    /u/encoreway2020   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hmepx8/gae_and_actor_critic_methods/</guid>
      <pubDate>Thu, 26 Dec 2024 02:22:35 GMT</pubDate>
    </item>
    <item>
      <title>任何结合 RL 和 LLM 的工作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hly9nh/any_work_present_combining_rl_llms/</link>
      <description><![CDATA[有人知道将 RL 和 LLM 结合起来的一些工作吗？我已经看到了一些可以使用的提议方法，但到目前为止还没有实际应用。     提交人    /u/Wide-Chef-7011   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hly9nh/any_work_present_combining_rl_llms/</guid>
      <pubDate>Wed, 25 Dec 2024 10:21:53 GMT</pubDate>
    </item>
    </channel>
</rss>