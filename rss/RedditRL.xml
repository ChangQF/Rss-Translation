<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 03 Feb 2025 09:18:52 GMT</lastBuildDate>
    <item>
      <title>Vision RL 帮助和指导。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</link>
      <description><![CDATA[向聪明的人们问好。我一直在深入研究 RL，我认为那个男人跳入游泳池却撞到冰块的视频适用于我。 https://jacomoolman.co.za/reinforcementlearning/（向下滚动或直接搜索“vision”以跳过与我的问题无关的内容） 这是我迄今为止的进展。任何使用过视觉 RL 的人可能都能看出我做错了什么？我已经尝试了大约 2 个月，试图为模型提供图像而不是变量，但没有成功。    提交人    /u/TheRealMrJm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igjcrn/vision_rl_help_and_guidance/</guid>
      <pubDate>Mon, 03 Feb 2025 07:01:10 GMT</pubDate>
    </item>
    <item>
      <title>这看起来像是稳定的 PPO 收敛吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</link>
      <description><![CDATA[      这看起来像稳定的 PPO 收敛吗？    提交人    /u/TopSigmaNoCap79970   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdi7f/does_this_look_like_stable_ppo_convergence/</guid>
      <pubDate>Mon, 03 Feb 2025 01:30:55 GMT</pubDate>
    </item>
    <item>
      <title>“深度研究简介”，OpenAI（基于 o3 的网页浏览/研究代理的 RL 训练）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igdh9o/introducing_deep_research_openai_rl_training_of/</guid>
      <pubDate>Mon, 03 Feb 2025 01:29:39 GMT</pubDate>
    </item>
    <item>
      <title>“自我验证，人工智能的关键”，Sutton 2001（是什么让搜索发挥作用）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1igbjwn/selfverification_the_key_to_ai_sutton_2001_what/</guid>
      <pubDate>Sun, 02 Feb 2025 23:55:51 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 RL 来解决我的 opt. 控制问题吗</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iga0t5/can_i_use_rl_for_my_opt_control_problem/</link>
      <description><![CDATA[您好，我目前正在优化一个控制问题，试图将网络从随机状态转变为期望状态。 决策变量如下： - 一个包含 n 个连续变量的向量 - 一个包含 m 个二进制变量的向量 目前，我对连续变量使用标准差分进化，对二进制向量使用特殊的二进制差分进化算法。我想知道我是否可以为这个设置实现一个 RL 模型？ 更多信息：n 通常为 10-20，m 通常为 80-100。我经过大约 10000-20000 次不同的进化尝试后收敛到某种期望状态，这需要一些时间。主要是因为对于每次迭代，我都需要进行一些耗时的物理计算。对于不同的进化算法，可以并行处理这些计算，但是由于许可问题，需要花费很多钱。所以目前，我每次只能进行 1 次计算，大约需要 1 到 1.5 秒。总共需要 10000-30000 秒才能完成。 您认为我学习 RL 并尝试通过 RL 解决这个问题值得吗？请记住，RL 算法还需要为每次交互等待 1.5 秒，因为我会将其操作发送到许可软件并接收其输出。 当前流程：1）使用初始猜测启动算法 2）将所有初始猜测发送到黑盒软件 1by1 3）从黑盒软件 1by1 接收所有输出 4）启动差分进化循环 5）创建新的猜测 1by1 将它们发送到黑盒获取输出并对其进行评分 6）如果得分更高，则用新猜测替换旧猜测 7）重复 5-6 次，共 10000 次并获得最佳得分猜测    submitted by    /u/Icedkk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iga0t5/can_i_use_rl_for_my_opt_control_problem/</guid>
      <pubDate>Sun, 02 Feb 2025 22:46:09 GMT</pubDate>
    </item>
    <item>
      <title>2025 年秋季硕士/博士申请</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</link>
      <description><![CDATA[大家好！ 随着招生周期全面展开，我祝愿本周期申请的所有人好运！我正在申请，迫不及待地想去研究生院做 RL 研究（在我的国家这很少见）。 在评论中写下你申请了哪里以及你想进入哪里。也许宇宙会眷顾你，机会也会眷顾你！    提交人    /u/issyonibba   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig5swy/fall_2025_msphd_applications/</guid>
      <pubDate>Sun, 02 Feb 2025 19:47:59 GMT</pubDate>
    </item>
    <item>
      <title>混合动作空间实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig2tlk/hybrid_action_space_implementations/</link>
      <description><![CDATA[大家好！ 我正在开展一个项目，其中既有连续动作也有离散动作，并且偶然发现了称为混合动作空间的研究领域。 我已经看过多篇关于混合动作空间的论文（例如 https://arxiv.org/pdf/1903.01344），但我没有找到任何 GitHub 存储库。 有人知道什么可以推荐吗？    提交人    /u/Plastic-Bus-7003   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig2tlk/hybrid_action_space_implementations/</guid>
      <pubDate>Sun, 02 Feb 2025 17:45:07 GMT</pubDate>
    </item>
    <item>
      <title>使用 sb3 的 DQN 和 RNN？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig18dz/dqn_with_rnn_using_sb3/</link>
      <description><![CDATA[我有一个部分可观察的环境，我需要在其中使用 LSTM 和 CNN 实现 DQN。有什么帮助吗？我无法创建存储隐藏状态的新重放缓冲区    提交人    /u/Prize-Ad8714   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig18dz/dqn_with_rnn_using_sb3/</guid>
      <pubDate>Sun, 02 Feb 2025 16:38:45 GMT</pubDate>
    </item>
    <item>
      <title>GRPO 中的代币级优势</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ig0a3h/tokenlevel_advantages_in_grpo/</link>
      <description><![CDATA[在 GRPO 损失函数中，我们看到每个输出 (o_i) 都有一个单独的优势，这是可以预料的，每个 token t 也有一个单独的优势。我有两个问题：  为什么需要 token 级优势？为什么不给输出中的所有 token 相同的优势？ 如何计算这个 token 级优势？  我这里遗漏了什么吗？从 Hugginface TRL 的实现来看，他们似乎没有执行令牌级别的优势： https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py#L507    提交人    /u/AdministrativeRub484   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ig0a3h/tokenlevel_advantages_in_grpo/</guid>
      <pubDate>Sun, 02 Feb 2025 15:57:43 GMT</pubDate>
    </item>
    <item>
      <title>“迈向通用无模型强化学习”，Fujimoto 等人，2025 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</link>
      <description><![CDATA[ [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifvsu8/towards_generalpurpose_modelfree_reinforcement/</guid>
      <pubDate>Sun, 02 Feb 2025 12:02:03 GMT</pubDate>
    </item>
    <item>
      <title>我对学习 RL 的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</link>
      <description><![CDATA[我读过 Sutton 和 Barto 的书，有时我发现很难理解其中的一些概念。然后，我开始探索这个资源。现在，我真正理解了价值迭代和其他基本概念背后的含义。我认为这本书应该在 Sutton 和 Barto 的书之前或同时阅读。这真是一本很棒的书！    提交人    /u/demirbey05   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifsgpo/my_recommendation_for_learning_rl/</guid>
      <pubDate>Sun, 02 Feb 2025 07:59:42 GMT</pubDate>
    </item>
    <item>
      <title>信息抽象扑克解决方案中阻塞效应的解释</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifp5fk/accounting_for_blocker_effects_in_an_information/</link>
      <description><![CDATA[我目前正在学习扑克解算器的机制，并尝试使用 MCCFR 实现一个。 到目前为止，我已经成功解决了 Leduc Hold&#39;em 扑克问题，并正在尝试转向 NL Hold&#39;em。由于我正在创建一个解算器而不是扑克机器人，因此默认情况下，我将把行动空间抽象为两个玩家的几个可能的典型行动（例如 33%、50 美元、75%、底池、150% 和全押）。 目前，我也不会尝试解决翻牌前的问题，而只会使用已经存在的范围图表，这些图表将到达翻牌的某个位置（或让用户手动创建它们）。翻牌前的动作将被抽象到这个范围后面。翻牌将根据解决方案给出，所以我需要考虑的只是每个玩家的底牌、转牌和河牌。  我的问题是，在这些限制下，是否有可能在合理的时间内使用 MCCFR 解决它，而无需任何信息/聚类抽象，或者我是否仍然需要使用信息空间抽象，例如将相似的信息集分成一个策略。如果我这样做，我不太确定如何根据它们独特的阻断效果在一个桶中为不同的手单独创建策略。例如，众所周知，在完成转牌或河牌的同花上拥有花色的 A 是虚张声势的好时机，但是无论如何，它会被放在与任何其他 A 高手相同的桶中，尤其是在河牌上。  我读到我可以通过根据当前手牌强度和未来街道移动到不同桶的概率选择桶来部分缓和翻牌和转牌上的这种情况，但这仍然不能真正解决桶何时想要采取混合策略的问题，以及决定哪些手应该做哪些动作。    提交者    /u/lddzz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifp5fk/accounting_for_blocker_effects_in_an_information/</guid>
      <pubDate>Sun, 02 Feb 2025 04:25:39 GMT</pubDate>
    </item>
    <item>
      <title>“DivPO：多样化偏好优化”，Lanchantin 等人 2025（通过设置最小新颖性阈值来对抗 RLHF 模式崩溃）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifl1bp/divpo_diverse_preference_optimization_lanchantin/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifl1bp/divpo_diverse_preference_optimization_lanchantin/</guid>
      <pubDate>Sun, 02 Feb 2025 00:43:05 GMT</pubDate>
    </item>
    <item>
      <title>对于非新手项目，从哪里开始使用 GPU？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ifjcbn/where_to_start_with_gpus_for_notsonovice_projects/</link>
      <description><![CDATA[经验丰富的软件工程师，希望涉足一些硬件 - 我想探索一些 AI/模拟支线任务。我完全知道 GPU 和（如果是 NVIDIA，那么就是 CUDA）对于这次旅程必不可少。但是，我不知道从哪里开始。 我是一个典型的 Mac 用户，所以组装一台 PC 或将多个 GPU 联网在一起的想法不是我做过的事情（但我可以学会）。我真的不知道要搜索什么或从哪里开始寻找。 对于如何开始熟悉构建和编程用于自托管目的的 GPU 集群，有什么建议吗？我熟悉一般的网络和相关的分布式编程（需要 VPC、Proxmox、Kubernetes 等），但不熟悉 GPU 方面。 我完全清楚我不知道我还不知道什么，我在寻求方向感。每个人都是从某个地方开始的。 如果有帮助的话，我感兴趣的两个项目是在集群中运行一些本地 Llama 模型，以及为一些机器人项目（Isaac / gym / 等）运行一些大规模并行深度强化学习过程。 如果有 A) 更适合“开发套件之后的步骤”的实用选项，和 B) 可以让我更全面地进入硬件生态系统并真正“理解”发生了什么的选项，我不会在 Jetson 开发套件上花钱。 有什么建议可以帮助迷失的灵魂吗？硬件、课程、YouTube 频道、博客 - 任何能帮助我直观地了解超越 devkit 级别交互的东西。    提交人    /u/NewEnergy21   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ifjcbn/where_to_start_with_gpus_for_notsonovice_projects/</guid>
      <pubDate>Sat, 01 Feb 2025 23:20:48 GMT</pubDate>
    </item>
    <item>
      <title>RL 中有哪些类型的职业？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1if3x11/what_type_of_careers_are_available_in_rl/</link>
      <description><![CDATA[我一直认为，我不可能进入一个完整的机器学习职业（只是机会或经验不足，或者我不够聪明），但最近我被伯克利的 Sergey Levine 实验室录取为本科生。现在，我正在权衡如何利用我将在他的实验室获得的 3.5 年 RL 研究经验（我现在只是一名大一新生）。 一方面，我可以攻读博士学位；我真的不喜欢额外的 5 年和它需要的所有承诺（也包括看到我所有的朋友毕业并开始赚钱），但这可能是在 RAIL 进行研究后进入机器学习职业的最可靠方法。我也觉得这是从做这么多本科研究中获得最大价值的选择（虽然可能是沉没成本谬论哈哈）。但我担心，等我毕业时，人工智能的炒作会冷却下来，或者强化学习可能不是一个适合攻读博士学位的领域。（要清楚的是，我想从事行业研究，而不是学术界） 另一方面，我可以从事某种标准的机器学习工程师职位。我担心的是，我更喜欢研发类的工作，而不是工程类的工作。我也觉得，我的研究经验对于招聘这些工作毫无用处（一些随机的招聘人员真的会关心研究吗？），所以这有点浪费了。但我进入职场的时间要早​​得多，而且不必忍受攻读博士学位的痛苦。 我觉得我想要介于这两个选项之间的某个职位，但不确定这个职位到底是什么。 除了任何与上述问题相关的建议外，我还有两个主要问题：  工程和研发之间的工作范围到底是什么？我听说过一些工作，比如研究工程师，它们介于两者之间，但这些工作似乎相当不常见。此外，没有博士学位的情况下，获得 ML 研发工作有多普遍（假设你已经在本科阶段拥有丰富的研究经验）？ RL 行业总体情况如何？我看到对 CV 和 NLP 专家的需求很大，但除了在 LLM 中的使用之外，我从未听说过太多关于 RL 的信息。RL 专业化是行业真正需要的吗？  谢谢！ - 一名困惑的学生    提交人    /u/dmann1945   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1if3x11/what_type_of_careers_are_available_in_rl/</guid>
      <pubDate>Sat, 01 Feb 2025 10:55:17 GMT</pubDate>
    </item>
    </channel>
</rss>