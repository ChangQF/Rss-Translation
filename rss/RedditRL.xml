<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 14 May 2024 03:16:40 GMT</lastBuildDate>
    <item>
      <title>异构GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crfg0g/heterogeneous_gnn/</link>
      <description><![CDATA[社区您好， 我目前正在将异构 GNN 应用于涉及作业和机器且作业和机器之间有连接的场景。然而，我观察到，即使工作的特征值不同，工作嵌入也包含所有工作的相同张量。聊天 GPT 告诉我，问题是因为作业节点连接到同一台机器。但特征值不同，所以我不知道如何解决这个问题，以便模型可以学习为免费机器选择作业。 你能帮我吗？    由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crfg0g/heterogeneous_gnn/</guid>
      <pubDate>Tue, 14 May 2024 00:49:28 GMT</pubDate>
    </item>
    <item>
      <title>一个时间步内执行多个操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crd71b/multiple_actions_in_one_timestep/</link>
      <description><![CDATA[我正在处理一个问题，其中代理操作是选择打开或关闭多个二进制开关（您可以想象输出是一个二进制数像 01011...）。我见过的大多数文献都涉及每个时间步一个动作，是否有一种简单的方法来实现我所缺少的这个动作空间？将每个可能的组合作为神经网络中的终端节点会很快失控，为 N 个开关提供 2N 个输出节点。 我目前的想法是使用像 A2C 这样的值方法，演员有 N 个输出节点，每个节点对应于打开开关的采样概率，但是，我不确定这会如何改变损失函数。 是否有一个简单的实现我失踪了？任何指示将不胜感激。   由   提交/u/Chewden_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crd71b/multiple_actions_in_one_timestep/</guid>
      <pubDate>Mon, 13 May 2024 23:02:18 GMT</pubDate>
    </item>
    <item>
      <title>CleanRL PPO 不学习简单的双积分器环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cr1jze/cleanrl_ppo_not_learning_a_simple_double/</link>
      <description><![CDATA[我有一个代表双积分器的自定义环境。一开始将环境位置和速度都设置为0，然后选择一个目标值，目标是尽快减小位置与目标之间的差异。代理观察错误和速度。 我尝试使用 CleanRL 的 PPO 实现，但该算法似乎无法学习如何解决环境问题，每集的平均回报随机从 -1k 跳到更大价值观。对我来说，这看起来是一个相当简单的环境，但我不知道为什么它不起作用，有人有任何解释吗？ class DoubleIntegrator(gym.Env): def __init__(self , render_mode=None): super(DoubleIntegrator, self).__init__() self.pos = 0 self.vel = 0 self.target = 0 self.curr_step = 0 self.max_steps = 300 self.terminate = False self.truncated =假 self.action_space =gym.spaces.Box(low=-1, high=1, shape=(1,)) self.observation_space =gym.spaces.Box(low=-5, high=5, shape=(2 ,)) def 步骤(self, 动作): 奖励 = -10 * (self.pos - self.target) vel = self.vel + 0.1 * 动作 pos = self.pos + 0.1 * self.vel self.vel = vel self.pos = pos self.curr_step += 1 如果 self.curr_step &gt; self.max_steps: self.terminate = True self.truncated = True 返回 self._get_obs(),reward,self.termerated,self.truncated,self._get_info() def reset(self,seed=None,options=None): self.pos = 0 self.vel = 0 self.target = np.random.uniform() * 10 - 5 self.curr_step = 0 self.terminate = False self.truncated = False 返回 self._get_obs(), self._get_info () def _get_obs(self): return np.array([self.pos - self.target, self.vel], dtype=np.float32) def _get_info(self): return {&#39;target&#39;: self.target, &#39; pos&#39;: self.pos}    由   提交/u/sauro97  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cr1jze/cleanrl_ppo_not_learning_a_simple_double/</guid>
      <pubDate>Mon, 13 May 2024 15:05:17 GMT</pubDate>
    </item>
    <item>
      <title>QL 如何解决而 DQN 无法解决？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqvb9d/how_ql_solves_but_dqn_cannot/</link>
      <description><![CDATA[我有一个可以通过 QL 成功解决的自定义环境，但无法使用 DQN 解决相同的环境（具有离散状态 - 离散操作）。  QL 可以解决问题，但 DQN 根本无法解决问题，这可能是什么原因？ DQN 是 QL 的神经网络改编版本，不是可以处理连续空间吗？当我尝试在相同环境下使用连续状态空间时，DQN 再次陷入困境。  这怎么可能？ 我的操作是 0 和 1，而代理始终只选择 1。这就像坚持同一个动作，这是完全糟糕的。   由   提交/u/OpenToAdvices96   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqvb9d/how_ql_solves_but_dqn_cannot/</guid>
      <pubDate>Mon, 13 May 2024 09:43:38 GMT</pubDate>
    </item>
    <item>
      <title>确定国际象棋代理的 DQN 架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqm9kd/determining_dqn_architecture_for_a_chess_agent/</link>
      <description><![CDATA[嗨，我正在训练 RL 代理下棋 - 我正在使用 open_spiel 库，所以它更具连接性预制棋子（即国际象棋环境和 RL 模型已创建）。但是，我想知道如何设置模型超参数，特别是关于隐藏层的数量和每个隐藏层的节点数量。我应该如何解决这个问题？ TIA   由   提交/u/shadowknife392   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqm9kd/determining_dqn_architecture_for_a_chess_agent/</guid>
      <pubDate>Mon, 13 May 2024 00:26:06 GMT</pubDate>
    </item>
    <item>
      <title>Stockfish 和 Lc0，在不同的部署次数下进行测试</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqemmm/stockfish_and_lc0_tested_at_different_number_of/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqemmm/stockfish_and_lc0_tested_at_different_number_of/</guid>
      <pubDate>Sun, 12 May 2024 18:36:56 GMT</pubDate>
    </item>
    <item>
      <title>“SOPHON: Non-Fine-Tunable Learning to Retrain Task Transferability For Pre-trained Models”，Deng et al 2024（MAML 在微调时会导致目标任务的灾难性遗忘）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</guid>
      <pubDate>Sun, 12 May 2024 18:05:11 GMT</pubDate>
    </item>
    <item>
      <title>尝试寻找有关 q 学习的学习率和伽玛设置的论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cq7dkk/trying_to_find_papers_on_learningrate_and_gamma/</link>
      <description><![CDATA[大家好。 我正在写关于 Q-learning 的学校期末论文。简而言之，我的项目基于 Netlogo 中的一个 99x99 环境，该环境具有包含七种类型地面类型（人行道、草地等）的网格。每种地面类型都有不同的奖励。该特工被设置为跨越 16 个不同的地点，并在连续 10 集稳定时收敛。我对 q-learning 参数的设置是learning-rate = 0,9 和 gamma = 1.0。 qlearning 收敛在 6500-8000 集左右。情节的定义是到达目标位置或击中建筑物/障碍物并开始新的情节。当代理收敛并找到最佳路线时，它会更新我尝试过这些值（0-30）的该路径的奖励。因此，当下一个代理启动时，一些补丁已从上一个代理更新。我对 100 个代理运行此程序以找到最佳路径。当所有 100 个智能体都找到最佳路径时，我会为路径着色，并与现实生活中的环境足迹观察结果进行比较来评估路径。环境基于真实位置，并且该项目基于收集这些足迹值的先前工作。 如果我没记错的话，当我与老师交谈时，这些高参数设置的原因是因为算法搜索的空间很大。 但是我需要一个关于为什么我选择这些设置的来源，你们有任何论文或可以推荐这些设置的东西吗？ 感谢您的帮助   由   提交 /u/TheAmitySloth   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cq7dkk/trying_to_find_papers_on_learningrate_and_gamma/</guid>
      <pubDate>Sun, 12 May 2024 13:02:14 GMT</pubDate>
    </item>
    <item>
      <title>多轿厢电梯（Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cq6uaf/multi_car_elevator_python/</link>
      <description><![CDATA[大家好， 我正在尝试查找最新的模拟器（Python 版本）和/或论文关于多轿厢电梯系统（基本上一个电梯井中有多辆轿厢）。有谁知道模拟器吗？或者有什么想法可以搜索吗？   由   提交/u/emreyavas20   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cq6uaf/multi_car_elevator_python/</guid>
      <pubDate>Sun, 12 May 2024 12:33:03 GMT</pubDate>
    </item>
    <item>
      <title>蛇 DQN 绕圈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpzz6e/snake_dqn_going_in_circles/</link>
      <description><![CDATA[我正在尝试实现 DQN 来玩贪吃蛇。目前，智能体学习了一些知识，例如如何避免撞到墙上，并且通常能够吃一两个水果，但随后就会陷入原地打转。我将网络状态表示为两个堆叠的框架，每个框架都是 20x20 的网格，其中 -1 标记蛇的头部，-0.5 标记身体，1 标记水果位置。使用两个连续帧背后的想法是确保代理知道蛇要去哪个方向。 奖励结构如下：  -1 表示击中蛇墙壁或撞到自己的身体 吃水果为1 每离开水果一步-0.05（试图阻止它绕圈） &gt; 朝着水果每走一步 0.025  我还限制了剧集长度，以避免蛇最终绕圈的情况。 这是我的神经网络的结构： class Net(nn.Module): def __init__(self, dim, action_size): super(Net, self).__init__() self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, 偏差=False) self.conv2 = nn.Conv2d(4, 16, kernel_size=8, stride=4) self.flatten = 16 * 1 * 1 self.fc1 = nn.Linear(self.flatten, 512) self.fc2 = nn.Linear(512, action_size) defforward(self, x): x = x.type(torch.FloatTensor) x = self.conv1( x) x = F.relu(x) x = self.conv2(x) x = F.relu(x) x = x.view(-1, self.flatten) x = self.fc1(x) x = F .relu(x) x = self.fc2(x) return x  这是我的训练代码： epsilon_start = 0.99 epsilon_final = 0.01 epsilon_decay = 0.0001 lr = 1e-4 batch_size = 32 max_timesteps = int(1e8) learn_every = 4 update_every = 2 Episode_reward = 0 curr_episode = 0 save_every = 10 moving_average_length = 25 max_episode_length = 800 game = Game() Brain = Brain(dim = game.dim , action_size = 4) prev_frame = torch.tensor(game.get_frame()) game.step(game.DOWN) curr_frame = torch.tensor(game.get_frame()) def get_epsilon(step): return epsilon_final + (epsilon_start - epsilon_final ) * np.exp(-epsilon_decay * step) moving_average = deque(maxlen = moving_average_length)starting_timestep = 0 for timestep in range(max_timesteps): state = torch.stack([prev_frame, curr_frame]) action = 0 epsilon = get_epsilon(timestep) ) if(ε &lt; random.uniform(0, 1)): action = Brain.pick_action(state) else: action = random.randint(0, 3) (奖励，终端) = game.step(action) game.render() Episode_reward +=奖励 next_frame = torch.tensor(game.get_frame()) next_state = torch.stack([curr_frame, next_frame]) Brain.add_to_memory(状态, 动作, 奖励, next_state, 终端) prev_frame = curr_frame curr_frame = next_frame if(timestep % learn_every == 0): Brain.learn() if(timestep % update_every == 0): Brain.update() if(terminal or timestep -starting_timestep &gt;= max_episode_length): moving_average.append(len(game.snake)) debug = &#39;剧集&#39; + str(curr_episode) + &#39; 蛇长度 &#39; + str(len(game.snake)) if(len(moving_average) &gt;= moving_average_length): debug += &#39; 移动平均 &#39; + str(float(sum( moving_average)) / float(len(moving_average))) print(debug) curr_episode += 1 if(curr_episode % save_every == 0): Brain.save_model(os.path.join(os.path.dirname(__file__), &#39; model&#39;)) Episode_reward = 0 game = Game() prev_frame = torch.tensor(game.get_frame()) game.step(game.DOWN) curr_frame = torch.tensor(game.get_frame())starting_timestep = timestep + 1 Brain .save_model(os.path.join(os.path.dirname(__file__), &#39;model&#39;))  如有任何建议，我们将不胜感激！   由   提交/u/xxgetrektxx2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpzz6e/snake_dqn_going_in_circles/</guid>
      <pubDate>Sun, 12 May 2024 04:50:08 GMT</pubDate>
    </item>
    <item>
      <title>“记忆的持久性和短暂性”，Richards & Frankland 2017</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpxen3/the_persistence_and_transience_of_memory_richards/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpxen3/the_persistence_and_transience_of_memory_richards/</guid>
      <pubDate>Sun, 12 May 2024 02:18:47 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗方法解决赌徒问题没有给出最优解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpwtxe/monte_carlo_method_for_gamblers_problem_not/</link>
      <description><![CDATA[     &lt; /td&gt; 大家好，我最近开始学习强化学习，并尝试用赌徒问题实现一个简单的蒙特卡罗学习算法。我用 python 编写了一些代码，但经过 100000000 次迭代后得出的解决方案是这样的： 从 0 到 99 的各个资本级别的最佳下注大小 我的代码是这样的： M = 100000000 alpha = 0.1 # 假设 H = 1, T = 0 def Flip_coin() -&gt; int: return random.choices([0, 1], Weights=[0.6, 0.4], k=1)[0] def policy(Q, epsilon, Capital : int) -&gt; int: &#39;&#39;&#39; :param Capital: 玩家目前拥有的资本等级 :return: 赌注大小 &#39;&#39;&#39; gredy_action = Q[ Capital, : ].argmax() do_greedy = random.choices([0, 1], Weights= [epsilon, 1 - epsilon], k=1)[0] if do_greedy: 返回greedy_action else: return random.randint(0,capital) def play_game(Q, epsilon): &#39;&#39;&#39; :return: 包含状态的列表列表、行动（投注）和最终奖励（100 或 0） &#39;&#39;&#39; 历史 = [] 资本 = 50 而资本 != 0 且资本 &lt; 100：断言资本&lt; 100 断言资本 != 0 赌注 = 策略(Q, epsilon, 资本) coin = Flip_coin() History.append([资本,赌注]) if coin == 1: Capital += bet elif coin == 0: Capital -=下注 如果大写 == 0: 返回 [历史记录, 大写] 如果大写 &gt;= 100: 返回 [历史记录, 100] def mc_control(track_history=True): &#39;&#39;&#39; :param track_history: :return: Q, Q_hist &#39;&#39;&#39; Q = np.zeros((100,100)) # 索引 = 资本，列 = 下注大小 Q_hist = [] t = 0 for m in tqdm(range(M + 1)): t += 1 epsilon = max(0.1, 1 - 1e-5 * t) # 线性衰减 epsilon 历史，reward = play_game(Q,epsilon) if track_history: Q_hist.append(Q) for state_action in History: Capital = state_action[0] bet = state_action[1] Q[capital , 赌注] = Q[资本, 赌注] + alpha * (奖励 - Q[资本, 赌注]) 返回 Q, Q_hist     ;由   提交 /u/yuriIsLifeFuckYou   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpwtxe/monte_carlo_method_for_gamblers_problem_not/</guid>
      <pubDate>Sun, 12 May 2024 01:46:38 GMT</pubDate>
    </item>
    <item>
      <title>SAC 代码帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cptxaz/sac_code_help/</link>
      <description><![CDATA[我一直在尝试实现 SAC 以在 openAI 的体育馆中使用。我有一个适用于钟摆环境的 DDPG 算法，但我的 SAC 实现永远不会学习。我怀疑问题出在我的某个地方的逻辑上。从打印的大部分步骤来看，我的 q 末尾充满了 nan，但我不明白为什么。如果有人可以提供帮助，我们将不胜感激。 编辑：删除代码块。添加github链接。 https://github.com/jhunter533/Machine-Learning/blob/main/TorchSAC.py&lt; /a&gt; 该项目位于一个更大的存储库中，但这是唯一适用的文件。也可以随意忽略测试、保存、加载函数，自从几次迭代之前我就没有修复它们，它们不是问题我正在尝试修复 rn。   由   提交 /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cptxaz/sac_code_help/</guid>
      <pubDate>Sat, 11 May 2024 23:15:24 GMT</pubDate>
    </item>
    <item>
      <title>tmrl 的多代理实现（Track Mania RL）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpnxg2/multi_agent_implementation_for_tmrl_track_mania_rl/</link>
      <description><![CDATA[我有兴趣为 TrackMania RL (TMRL) 开发多代理系统 (https://github.com/trackmania-rl/tmrl）。有谁知道与 TMRL 相关的现有多代理实现或文档吗？ Git 存储库将不胜感激。如果没有，您能否分享一些关于如何启动此过程的见解或资源，特别是修改多个代理的 TMRL 及其交互策略？谢谢！   由   提交 /u/msquaresproperty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpnxg2/multi_agent_implementation_for_tmrl_track_mania_rl/</guid>
      <pubDate>Sat, 11 May 2024 18:31:25 GMT</pubDate>
    </item>
    <item>
      <title>对 RL 库/项目的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</link>
      <description><![CDATA[嗨， 我目前正在休学一年，致力于自学和强化学习项目。我将于 9 月份开始攻读硕士学位，目标是准备 RL 博士学位。 由于我还有 4 个月的时间，我正在寻找其他项目来改善我即将到来的简历博士申请。 我考虑过开发一个可用于研究的库，但很难选择一个特定的主题。以下是我目前的指导方针：  框架：最好是 JAX，因为它最近越来越受欢迎，而且生态系统仍在扩展。我认为有机会从更成熟的框架（例如 Pytorch）中调整现有库，或者为 JAX 中缺乏合适软件的特定 RL 问题提供便捷的解决方案。 -  领域：我对开放性（课程学习、无监督环境设计）和多代理设置特别感兴趣。然而，已经有一些流行的库涵盖了这些领域的大量用例（Minimax、JaxMARL、JaxUED...）。我最终想在未来更深入地研究元和进化强化学习等其他领域。据我所知，进化强化学习也有一些不错的库（evojax、evosax 等），但我对元强化学习不太确定。 时间范围 ：大约 5 个月  我对社区对以下问题的见解很感兴趣：  是否有特定的 RL 领域缺乏有用的库贾克斯？ （在我引用的或其他人中） 如果没有，为现有库做出贡献会是更好的主意吗？ 是否更需要高效的环境和基准？   感谢您的帮助！   由   提交 /u/OptimalBandicoot1671   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cpjn7p/suggestion_for_rl_libraryproject/</guid>
      <pubDate>Sat, 11 May 2024 15:11:46 GMT</pubDate>
    </item>
    </channel>
</rss>