<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯æ—¨åœ¨æ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå’Œå­¦ä¹ å¦‚ä½•æœ€ä½³è·å¾—å¥–åŠ±çš„AI/ç»Ÿè®¡æ•°æ®çš„å­åœºã€‚ä¾‹å¦‚Alphagoï¼Œä¸´åºŠè¯•éªŒå’ŒA/Bæµ‹è¯•ä»¥åŠAtariæ¸¸æˆã€‚</description>
    <lastBuildDate>Mon, 24 Feb 2025 18:23:58 GMT</lastBuildDate>
    <item>
      <title>ä¸RLä¸€èµ·ä½¿ç”¨çš„æœ€ä½³æœºå™¨äººæ¨¡æ‹Ÿå™¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å—¨ï¼Œæˆ‘æ­£åœ¨å°è¯•æ¨¡æ‹Ÿä¸€ä¸ªç¯å¢ƒï¼Œåœ¨è¯¥ç¯å¢ƒä¸­ï¼Œæˆ‘çš„æœºå™¨äººå¿…é¡»ä¸è¿æ¥åˆ°æœ«ç«¯æ•ˆåº”å™¨çš„ä¼ æ„Ÿå™¨è®¾å¤‡è¿›è¡Œäº¤äº’ï¼Œå¹¶ä½¿ç”¨RLè¿›è¡Œè¯»æ•°ã€‚æˆ‘å¸Œæœ›ç„¶ååœ¨å®é™…çš„ç¡¬ä»¶ä¸Šä½¿ç”¨è¿™ä¸ªè®­ç»ƒæœ‰ç´ çš„ä»£ç†ã€‚æ‚¨ä¼šæ¨èä»€ä¹ˆæ¨¡æ‹Ÿå™¨ï¼Ÿæˆ‘çœ‹è¿‡Pybulletå’ŒGuazeboã€‚ä½†æ˜¯æˆ‘ä¸ç¡®å®šå“ªä¸ªä¼¼ä¹æ˜¯æœ€ç®€å•ï¼Œæœ€ä½³çš„æ–¹æ³•ï¼Œå› ä¸ºæˆ‘åœ¨æ¨¡æ‹Ÿæ–¹é¢å‡ ä¹æ²¡æœ‰ç»éªŒã€‚   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/bananaoramama   href =â€œ https://www.reddit.com/r/reinforevericeslearning/comments/1ix8eux/best_robotic_simulator_to_to_use_with_with_rl/â€&gt; [link]       [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix8eux/best_robotic_simulator_to_use_with_rl/</guid>
      <pubDate>Mon, 24 Feb 2025 18:01:00 GMT</pubDate>
    </item>
    <item>
      <title>å¥–åŠ±æˆå‹æƒ³æ³•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘æœ‰ä¸€ä¸ªå¥–åŠ±å¡‘é€ å½¢å¼çš„æƒ³æ³•ï¼Œæƒ³çŸ¥é“å¤§å®¶éƒ½åœ¨è€ƒè™‘å®ƒã€‚ æƒ³è±¡ä½ æœ‰ä¸€ä¸ªè¶…çº§ç¨€ç–çš„å¥–åŠ±åŠŸèƒ½ï¼Œä¾‹å¦‚+1èµ¢å¾—èƒœåˆ©å’Œ-1çš„æŸå¤±ï¼Œæƒ…èŠ‚å¾ˆé•¿ã€‚è¿™ä¸ªå¥–åŠ±åŠŸèƒ½æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚  å½“ç„¶ï¼Œæˆ‘ä»¬éƒ½çŸ¥é“ç¨€ç–çš„å¥–åŠ±åŠŸèƒ½å¾ˆéš¾å­¦ä¹ ã€‚å› æ­¤ï¼Œå¼•å…¥å¯†é›†çš„å¥–åŠ±åŠŸèƒ½ä¼¼ä¹å¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªå‡½æ•°ï¼Œè¡¨æ˜æˆ‘ä»¬çš„ä»£ç†å•†æ­£æœæ­£ç¡®æˆ–é”™è¯¯çš„æ–¹å‘è¡Œé©¶ã€‚å®šä¹‰è¿™æ ·çš„å¥–åŠ±å‡½æ•°é€šå¸¸éå¸¸æ£˜æ‰‹ï¼Œä»¥ä¸æˆ‘ä»¬çš„çœŸå®å¥–åŠ±åŠŸèƒ½å®Œå…¨åŒ¹é…ï¼Œå› æ­¤æˆ‘è®¤ä¸ºæš‚æ—¶ä½¿ç”¨æ­¤å¥–åŠ±åŠŸèƒ½ä»¥æœ€åˆä½¿æˆ‘ä»¬çš„ä»£ç†åœ¨æ”¿ç­–ç©ºé—´ä¸­å¤§è‡´é€‚å½“çš„é¢†åŸŸæ˜¯æœ‰æ„ä¹‰çš„ã€‚ ä½œä¸ºå…è´£å£°æ˜ï¼Œæˆ‘å¿…é¡»è¯´æˆ‘æ²¡æœ‰é˜…è¯»ä»»ä½•æœ‰å…³å¥–åŠ±æˆå‹çš„ç ”ç©¶ï¼Œæ‰€ä»¥å¦‚æœæˆ‘çš„æƒ³æ³•å¾ˆæ„šè ¢ï¼Œè¯·åŸè°…æˆ‘ã€‚ æˆ‘è¿‡å»ç”¨DQNåšçš„ä¸€ä»¶äº‹ - åƒç®—æ³•ä¸€æ ·åœ¨åŸ¹è®­è¿‡ç¨‹ä¸­ï¼Œé€æ¸ä»ä¸€ä¸ªå¥–åŠ±åŠŸèƒ½è½¬ç§»åˆ°å¦ä¸€ä¸ªå¥–åŠ±åŠŸèƒ½ã€‚ä¸€å¼€å§‹ï¼Œæˆ‘ä½¿ç”¨äº†100ï¼…çš„è‡´å¯†å¥–åŠ±åŠŸèƒ½å’Œç¨€ç–çš„0ï¼…ã€‚ä¸€æ®µæ—¶é—´åï¼Œæˆ‘å¼€å§‹é€æ¸â€œé€€ç«â€ã€‚è¿™ä¸ªæ¯”ç‡ç›´åˆ°æˆ‘åªä½¿ç”¨çœŸæ­£çš„ç¨€ç–å¥–åŠ±åŠŸèƒ½ã€‚æˆ‘çœ‹å¾—å¾ˆå¥½ã€‚ æˆ‘è¿™æ ·åšçš„åŸå› æ˜¯â€œé€€ç«â€ã€‚æ˜¯å› ä¸ºæˆ‘è®¤ä¸ºQå­¦ä¹ ç®—æ³•å¾ˆéš¾é€‚åº”å®Œå…¨ä¸åŒçš„å¥–åŠ±åŠŸèƒ½ã€‚ä½†æ˜¯æˆ‘ç¡®å®æƒ³çŸ¥é“é€€ç«ç‡æµªè´¹äº†å¤šå°‘æ—¶é—´ã€‚æˆ‘ä¹Ÿä¸å–œæ¬¢é€€ç«ç‡æ˜¯å¦ä¸€ä¸ªè¶…å‚æ•°ã€‚ æˆ‘çš„æƒ³æ³•æ˜¯å°†å¥–åŠ±å‡½æ•°çš„ç¡¬è½¬æ¢åº”ç”¨äºæ¼”å‘˜æ‰¹è¯„ç®—æ³•ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬å°†æ¨¡å‹è®­ç»ƒåœ¨å¯†é›†çš„å¥–åŠ±åŠŸèƒ½ä¸Šã€‚æˆ‘ä»¬å‡è®¾æˆ‘ä»¬å¾—å‡ºäº†ä¸€é¡¹ä½“é¢çš„æ”¿ç­–ï¼Œä¹Ÿæ˜¯è¯„è®ºå®¶çš„ä½“é¢ä»·å€¼ä¼°è®¡ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å†»ç»“æ¼”å‘˜ï¼Œç¡¬å‡»å¥–åŠ±åŠŸèƒ½ï¼Œå¹¶é‡æ–°å®¡æŸ¥è¯„è®ºå®¶ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥æ¶ˆé™¤é«˜å‚æ•°ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬å¯ä»¥è®­ç»ƒï¼Œç›´åˆ°è¯„è®ºå®¶çš„é”™è¯¯è¾¾åˆ°ä¸€å®šçš„é—¨æ§›ä¸ºæ­¢ã€‚æˆ‘æƒ³è¿™æ˜¯ä¸€ä¸ªæ–°çš„è¶…å‚æ•°ã€‚æ— è®ºå¦‚ä½•ï¼Œæˆ‘ä»¬ä¼šè§£å¼€æ¼”å‘˜å¹¶æ¢å¤æ­£å¸¸çš„åŸ¹è®­ã€‚ æˆ‘è®¤ä¸ºè¿™åœ¨å®è·µä¸­åº”è¯¥å¾ˆå¥½ã€‚æˆ‘è¿˜æ²¡æœ‰æœºä¼šå°è¯•ã€‚ä½ ä»¬éƒ½å¯¹è¿™ä¸ªæƒ³æ³•æœ‰ä½•çœ‹æ³•ï¼Ÿæœ‰ä»€ä¹ˆç†ç”±æœŸæœ›å®ƒè¡Œä¸é€šå—ï¼Ÿæˆ‘ä¸æ˜¯æ¼”å‘˜ - æ‰¹è¯„ç®—æ³•çš„ä¸“å®¶ï¼Œæ‰€ä»¥è¿™ä¸ªæƒ³æ³•ç”šè‡³æ²¡æœ‰æ„ä¹‰ã€‚ è®©æˆ‘çŸ¥é“ï¼è°¢è°¢ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/sandsnip3r     [link]   ï¼†ï¼ƒ32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix4a85/reward_shaping_idea/</guid>
      <pubDate>Mon, 24 Feb 2025 15:12:25 GMT</pubDate>
    </item>
    <item>
      <title>200 for LLM FINETUNTININGçš„200ä¸ªç»„åˆèº«ä»½å’Œå®šç†æ•°æ®é›†</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix488a/200_combinatorial_identities_and_theorems_dataset/</link>
      <description><![CDATA[       ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/databaebee     [link]  ï¼†ï¼ƒ32;   [æ³¨é‡Š]    /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix488a/200_combinatorial_identities_and_theorems_dataset/</guid>
      <pubDate>Mon, 24 Feb 2025 15:09:58 GMT</pubDate>
    </item>
    <item>
      <title>Simbav2ï¼šå¯æ‰©å±•æ·±åº¦å¢å¼ºå­¦ä¹ çš„è¶…é€æ˜æ ‡å‡†åŒ–</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</link>
      <description><![CDATA[     å¼•å…¥ simbav2ï¼   ğŸ“„é¡¹ç›®é¡µé¢ï¼š https://dojeon-ai.github.io/simbav2/  ğŸ“„çº¸ï¼š https://arxiv.org/abs/2502.15280   ğŸ”—ä»£ç ï¼š https://github.com/dojeon-ai/simbav2     simbav2æ˜¯ä¸€ç§ç®€å•ï¼Œå¯æ‰©å±•çš„RLä½“ç³»ç»“æ„é€šè¿‡ç®€å•åœ°ç”¨Simbav2æ›¿æ¢MLPï¼Œ&lt; /strong&gt;ã€‚&lt; /strong&gt;ã€‚æ¼”å‘˜è¯„è®ºå®¶åœ¨57ä¸ªè¿ç»­çš„æ§åˆ¶ä»»åŠ¡ï¼ˆMujocoï¼Œdmcontrolï¼ŒMyosuiteï¼Œhumyoid-Benchï¼‰ä¸­å®ç°äº†æœ€å…ˆè¿›çš„è¡¨ç°ï¼ˆSOTAï¼‰ã€‚  å®ƒä¸ä½“è‚²é¦†1.0.0 API   - å°è¯•ä¸€ä¸‹ï¼ ï¼Œå¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·éšæ—¶ä¸ä¹‹ä¼¸å‡ºæ´æ‰‹ï¼šï¼‰ &lt; /div&gt; &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/joonleesky     [link]     [æ³¨é‡Š]  /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ix04ur/simbav2_hyperspherical_normalization_for_scalable/</guid>
      <pubDate>Mon, 24 Feb 2025 11:43:23 GMT</pubDate>
    </item>
    <item>
      <title>å†™äº†æˆ‘å…³äºç”Ÿé”ˆçš„å¼ºåŒ–å­¦ä¹ çš„è®ºæ–‡</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwyveb/wrote_my_thesis_on_reinforcement_learning_in_rust/</link>
      <description><![CDATA[ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/dashdeckers     [link]       [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwyveb/wrote_my_thesis_on_reinforcement_learning_in_rust/</guid>
      <pubDate>Mon, 24 Feb 2025 10:18:10 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘çš„å¼ æ¿çš„ä¸»è¦é—®é¢˜ï¼è¯·å¸®åŠ©æˆ‘</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwyefh/major_issue_with_my_tensorboard_pls_help_me/</link>
      <description><![CDATA[      i Am Amè®­ç»ƒRLç®—æ³•ï¼Œå¹¶åœ¨å¼ æ¿ä¸­è®°å½•ç»“æœã€‚æˆ‘æ˜¯Tensorboardçš„æ–°æ‰‹ã€‚å½“æˆ‘ä»…è®°å½•æ•°æ®æ—¶ï¼Œæˆ‘ä¸çŸ¥é“æ—¶ï¼Œæˆ‘ä¸çŸ¥é“æ—¶æƒ…èŠ‚å›æŠ¥å’Œé•¿åº¦æ˜¯æ•…éšœæˆ–é”™è¯¯ã€‚é—®é¢˜åœ¨äºï¼Œæ—¥å¿—ä»¥0ä¸ªæ­¥éª¤å¼€å§‹ï¼Œå›¾è¡¨å¯ä»¥åœ¨100ä¸‡æ­¥ä¸­å¯ä»¥ä½¿ç”¨ï¼Œä¹‹åçš„å¥–åŠ±å´ä»¥100ä¸‡ä¸ªIEçš„å·®è·ç§»åŠ¨ã€‚æœ€åä¸€ä¸ªæ•°æ®ä»…å…·æœ‰20mè‡³21mçš„å›¾å½¢ã€‚  æˆ‘ä¸çŸ¥é“æˆ‘æ˜¯ä»€ä¹ˆé”™è¯¯çš„äº‹æƒ…ï¼Œä½ ä»¬å¯ä»¥æŒ‡å¯¼æˆ‘å—ï¼Ÿ  å¯¼å…¥ç™»å½•ä»torch.utils.utils.tensorboard import import import import oså¯¼å…¥imports imports imports import import inct loggerï¼šdef __init __ï¼ˆselfï¼Œrun_nameï¼Œargsï¼‰ï¼šself.log_name = f&#39;logs/{run_name}&#39;self.start_time = time.timeï¼ˆï¼‰self.n_eps = 0 os.makedirsï¼ˆ&#39;logs&#39;ï¼Œstef_ok = trueï¼‰os.makedirsï¼ˆ&#39;models&#39;ï¼Œequent_ok = trueï¼‰self.writer = summaryWriterï¼ˆself.log_nameï¼‰logging.basicconfigï¼ˆlevel = = logging.debugï¼Œæ ¼å¼=&#39;ï¼…ï¼ˆasctimeï¼‰sï¼…ï¼ˆæ¶ˆæ¯ï¼‰s&#39;ï¼Œhandlers = [ logging.streamhandlerï¼ˆï¼‰ï¼Œlogging.filehandlerï¼ˆf&#39;{self.log_name} .log&#39;ï¼Œï¼†quot&#39;aï¼†quot;ï¼†quot;ï¼†quot;ï¼†quort;]ï¼Œ]ï¼Œdatefmt =&#39;ï¼…y/ï¼…m/ï¼…m/ï¼…dï¼…iï¼šï¼…mï¼šï¼…mï¼šï¼…sï¼…sï¼…p p p &#39;ï¼‰logging.infoï¼ˆargsï¼‰def log_scalarsï¼ˆselfï¼Œscalar_dictï¼Œstepï¼‰ï¼šå¯¹äºé”®ï¼Œval in scalar_dict.itemsï¼ˆï¼‰ï¼šself.writer.add_scalarï¼ˆkeyï¼Œvalï¼Œstepï¼‰def log_episodeï¼ˆselfï¼Œinfoï¼Œstepï¼‰ï¼šrewards = info = info [ï¼†quort returns/epoindic_rewardï¼†quorts;] length = infor = info = info = info; ï¼ƒä½¿ç”¨é•¿åº¦è€Œä¸æ˜¯å¥–åŠ±å®Œæˆçš„trackæƒ…èŠ‚=é•¿åº¦=é•¿åº¦ï¼†gt; 0å¯¹äºiåœ¨èŒƒå›´å†…ï¼ˆlenï¼ˆrewardsï¼‰ï¼‰ï¼‰ï¼šå¦‚æœå®Œæˆäº†_episodes [i]ï¼šself.n_eps += 1 powers_data = {ï¼†querts; returns/ependodic_rewardï¼†quotï¼†quotï¼†quotï¼†quotï¼†quot; } self.log_scalarsï¼ˆemotive_dataï¼Œstepï¼‰time_expired =ï¼ˆtime.timeï¼ˆï¼‰ - self.start_timeï¼‰ / 60 /60 logging.infoï¼ˆfï¼†quot; gt; ep = {self.n_eps} |æ€»æ­¥éª¤= {step}ï¼†quet; fï¼†quets; fï¼†quest; | reward = {rewards [rewards [i]} |é•¿åº¦}ï¼†quot;æˆ‘ç”¨æ¥è¿™æ ·åšçš„ä»£ç ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/decter_prune_9756      [link]   /table&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwyefh/major_issue_with_my_tensorboard_pls_help_me/</guid>
      <pubDate>Mon, 24 Feb 2025 09:46:11 GMT</pubDate>
    </item>
    <item>
      <title>å¦‚ä½•æŒæ¡å¼ºåŒ–å­¦ä¹ çš„å¯èƒ½æ€§ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwwpmo/how_to_master_probability_for_reinforcement/</link>
      <description><![CDATA[åœ¨æˆ‘çš„æ¦‚ç‡æŠ€èƒ½ä¸æ˜¯ä»–ä»¬éœ€è¦çš„åœ°æ–¹ã€‚æˆ‘åœ¨æœ¬ç§‘æœŸé—´å‚åŠ äº†ä¸€ä¸ªæ¦‚ç‡è¯¾ç¨‹ï¼Œä½†æ˜¯æˆ‘å¿˜è®°äº†å¤§éƒ¨åˆ†å†…å®¹ã€‚ æˆ‘ä¸ä»…æƒ³åˆ·æ–°æˆ‘çš„è®°å¿†ï¼Œæˆ‘æƒ³å˜å¾—æ“…é•¿äºæ¦‚ç‡ï¼Œç›´åˆ°æˆ‘å¯ä»¥ç›´è§‚åœ°å°†å…¶åº”ç”¨äºRLå’Œæœºå™¨å­¦ä¹ çš„å…¶ä»–é¢†åŸŸã€‚ å¯¹äºé‚£äº›æŒæ¡æ¦‚ç‡çš„äººæ¥è¯´ï¼Œæœ€é€‚åˆæ‚¨çš„äººï¼Ÿæœ‰ä»€ä¹ˆä¹¦ç±ï¼Œè¯¾ç¨‹ï¼Œé—®é¢˜é›†æˆ–æ¯æ—¥ä¹ æƒ¯ä¼šå¸¦æ¥å¾ˆå¤§çš„ä¸åŒå—ï¼Ÿ å¾ˆæƒ³å¬å¬æ‚¨çš„å»ºè®®ï¼  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32 ;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/hudhuddz     [link]        [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwwpmo/how_to_master_probability_for_reinforcement/</guid>
      <pubDate>Mon, 24 Feb 2025 07:43:38 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘åº”è¯¥é€‰æ‹©ä»€ä¹ˆç ”ç©¶é—®é¢˜ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrokr/what_research_problem_should_i_pick/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘æ˜¯RLçš„æ–°æ‰‹ï¼Œä½†æ˜¯æˆ‘å¤„äºéœ€è¦ç«‹å³ä¸ºç ”ç©¶çš„æƒ…å†µä¸‹é€‰æ‹©ä¸€ä¸ªå¥½çš„é—®é¢˜é™ˆè¿°ã€‚æˆ‘è¯•å›¾æµè§ˆä¼šè®®çš„è®ºæ–‡ä»¥å¿«é€Ÿé€‰æ‹©ä¸€äº›ä¸œè¥¿ã€‚æ˜¯å¦å¯ä»¥ç ”ç©¶ä»»ä½•ç‰¹å®šçš„é—®é¢˜é™ˆè¿°ï¼Ÿæˆ‘åªæ˜¯åœ¨å¯»æ‰¾ç»éªŒä¸°å¯Œçš„äººçš„çº¿ç´¢ã€‚è°¢è°¢  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/aliaslight     [link]   ï¼†ï¼ƒ32;   [æ³¨é‡Š]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrokr/what_research_problem_should_i_pick/</guid>
      <pubDate>Mon, 24 Feb 2025 02:41:32 GMT</pubDate>
    </item>
    <item>
      <title>ç›®å‰æœ€ç´§è¿«çš„é—®é¢˜ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrkev/most_pressing_problems_currently/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  ç›®å‰RLä¸­æœ€ç´§è¿«çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯å¦æœ‰ä»»ä½•æ–¹æ³•å¯ä»¥æ˜¾ç¤ºå‡ºè‰¯å¥½çš„è§£å†³è¿™äº›æ–¹æ³•ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/uiaslight     [link]   ï¼†ï¼ƒ32;   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrkev/most_pressing_problems_currently/</guid>
      <pubDate>Mon, 24 Feb 2025 02:35:40 GMT</pubDate>
    </item>
    <item>
      <title>RLå¯¹äºAGIï¼Œé‡ç‚¹åº”è¯¥æ”¾åœ¨ä»€ä¹ˆï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  é‚£äº›è®¤ä¸ºRLæ˜¯é€šå¾€AGIçš„å¯è¡Œé€”å¾„çš„äººï¼Œå½“å‰éœ€è¦ä¸“æ³¨äºåœ¨RLä¸­æ±‚è§£çš„å½“å‰å±€é™æ€§æ˜¯ä»€ä¹ˆï¼Ÿäººä»¬å¯ä»¥é€‰æ‹©ä¸ºæ­¤åšå‡ºå“ªäº›ç ”ç©¶é—®é¢˜ï¼Ÿ  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/aliaslight     [link]     [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwrip5/rl_for_agi_what_should_the_focus_be_on/</guid>
      <pubDate>Mon, 24 Feb 2025 02:33:15 GMT</pubDate>
    </item>
    <item>
      <title>å¸®åŠ©å°è¯•äº†è§£SARSAåŠæ¢¯åº¦</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwqh6f/help_on_trying_to_understand_sarsa_semi_gradient/</link>
      <description><![CDATA[       &lt;ï¼ -  sc_off-&gt;  å˜¿ï¼Œå¤§å®¶ï¼Œ æˆ‘æ˜¯ML/AIçˆ±å¥½è€…ï¼ŒRLä¸€ç›´æ˜¯æˆ‘å¿½ç•¥çš„ä¸€å‘¨ã€‚æˆ‘å‘ç°è¯¥ç®—æ³•å¾ˆéš¾è§£å¯†ï¼Œä½†æ˜¯åœ¨é˜…è¯»äº†LLM ArchitectureèƒŒåçš„è®ºæ–‡åï¼Œæˆ‘æ³¨æ„åˆ°å…¶ä¸­å¾ˆå¤šå€¾å‘äºç»å¸¸ä½¿ç”¨RLæ¦‚å¿µã€‚è¿™è®©æˆ‘æ„è¯†åˆ°è¿™æ˜¯ä¸€ä¸ªæˆ‘ä¸èƒ½çœŸæ­£å¿½ç•¥çš„é¢†åŸŸã€‚ ä¸ºæ­¤ï¼Œæˆ‘ä¸€ç›´åœ¨æ…¢æ…¢åœ°é€šè¿‡Bartoå’ŒSuttonçš„ä¹¦è¿›è¡Œä»”ç»†è§‚å¯Ÿï¼Œæˆ‘å¯ä»¥å…è´¹åœ¨çº¿æ‰¾åˆ°è¿™äº›ä¹¦ç±ã€‚ç›®å‰ï¼Œæˆ‘æ­£åœ¨ç¬¬10ç« ä¸­ï¼Œæˆ‘å¸Œæœ›åœ¨ç»“æŸæ—¶æˆ‘åº”è¯¥èƒ½å¤Ÿåˆ©ç”¨æˆ‘çš„å…¶ä»–AI/MLé¡¹ç›®çš„ç»éªŒæ¥åˆ¶ä½œä¸€äº›AIæ¥ç©ä¸€äº›å°šæœªæœ‰AIé¡¹ç›®çš„æ¸¸æˆï¼Œä¾‹å¦‚Spelunkyæˆ–PVZè‹±é›„ã€‚ å½“æˆ‘é˜…è¯»æ¯ä¸ªéƒ¨åˆ†æ—¶ï¼Œä¸ºäº†ç¡®ä¿æˆ‘äº†è§£ç®—æ³•å’ŒåŠ¨åŠ›ï¼Œæˆ‘å°è¯•ä½¿ç”¨è¿™æœ¬ä¹¦å»ºè®®çš„ç®—æ³•å¯¹å©´å„¿é—®é¢˜è¿›è¡Œç¼–ç ã€‚æˆ‘é‡åˆ°çš„è¿‘æœŸæ˜¯SARSAåŠæ¢¯åº¦ã€‚    æˆ‘åˆ¶ä½œäº†ä¸€ä¸ªéå¸¸ç®€å•çš„æ¸¸æˆï¼Œçµæ„Ÿæ¥è‡ªOpenai Mountain Caræ¸¸æˆï¼Œç›¸åï¼Œæ‚¨å®é™…ä¸Šåªéœ€è¦ASCIIå³å¯ä»£è¡¨å·å’Œåœ°å½¢ã€‚ä»£ç†å•†ä»å·¦ä¾§çš„Aç‚¹å¼€å§‹ï¼Œç›®æ ‡æ˜¯åˆ°è¾¾Bç‚¹ï¼Œè¿™ä¸€ç›´åœ¨å³ä¾§ã€‚åœ¨è·¯å¾„ä¸­ï¼Œä»£ç†å¯èƒ½ä¼šé‡åˆ°å‘å‰ï¼ˆ/ï¼‰æˆ–å‘åï¼ˆ\ï¼‰çš„æ–œç‡ã€‚è¿™äº›å¯ä»¥å…è®¸ä»£ç†åˆ†åˆ«è·å¾—æˆ–å¤±å»åŠ¨åŠ›ã€‚è¿˜åº”æ³¨æ„ï¼Œä»£ç†å•†çš„æ±½è½¦çš„å‘åŠ¨æœºéå¸¸è–„ã€‚èµ°ä¸‹å¡è·¯ï¼Œæ±½è½¦å¯ä»¥åŠ é€Ÿè¿›ä¸€æ­¥çš„åŠ¨åŠ›ï¼Œä½†æ˜¯å¦‚æœä¸Šå¡ï¼Œå‘åŠ¨æœºçš„åŠŸç‡ä¸ºé›¶ã€‚ ç›®æ ‡æ˜¯ä»¥é›¶åŠ¨é‡åˆ°è¾¾Bç‚¹ï¼Œä»¥è·å¾—ç§¯æçš„å¥–åŠ±å’Œæœ€ç»ˆçŠ¶æ€ã€‚å…¶ä»–ç»ˆç«¯çŠ¶æ€åŒ…æ‹¬è¿‡æ—©è¾¾åˆ°é›¶åŠ¨é‡æˆ–æ’å‡»åœ°å½¢æœ«ç«¯ã€‚è¿™è¾†è½¦è¿˜å› è¯•å›¾ä¿æŒä½ä½è€Œè·å¾—å¥–åŠ±ã€‚ æˆ‘çš„å®æ–½å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°ï¼š rl_concepts/rollingcar.ipynbåœ¨mainÂ· JJ8428/rl_concepts   æˆ‘å‘å¸ƒçš„åŸå› æ˜¯æˆ‘çš„ä»£ç†äººå¹¶æ²¡æœ‰çœŸæ­£å­¦ä¹ å¦‚ä½•è§£å†³æ¸¸æˆã€‚æˆ‘ä¸ç¡®å®šè¿™æ˜¯æ¸¸æˆè®¾è®¡ä¸ä½³çš„æƒ…å†µï¼Œæ¸¸æˆæ˜¯å¦å¤ªå¤æ‚è€Œæ— æ³•ç”¨ä¸€å±‚æƒé‡è§£å†³ï¼Œæˆ–è€…æˆ‘å¯¹ç®—æ³•çš„å®ç°æ˜¯å¦é”™è¯¯ã€‚ä»ç½‘ä¸Šæµè§ˆä¸­ï¼Œæˆ‘çœ‹åˆ°äººä»¬å·²ç»è§£å†³äº†sarsa semi gradçš„OpenAi MountainCaré—®é¢˜ï¼Œåˆ°ç›®å‰ä¸ºæ­¢å°šæ— næ­¥ï¼Œæ‰€ä»¥æˆ‘æœ‰ä¿¡å¿ƒä¹Ÿå¯ä»¥è§£å†³è¿™ä¸ªæ¸¸æˆã€‚ å¯ä»¥è§£å†³æœ‰äººè¯·å»çœ‹æˆ‘çš„ä»£ç ï¼Œå‘Šè¯‰æˆ‘æ˜¯å¦ä¸çŸ¥æ‰€æªï¼Ÿæˆ‘çš„ä»£ç ä¸é•¿ï¼Œä»»ä½•å¸®åŠ©æˆ–æŒ‡é’ˆéƒ½å°†ä¸èƒœæ„Ÿæ¿€ã€‚å¦‚æœæˆ‘çš„ä»£ç è¶…çº§å‡Œä¹±ä¸”ä¸å¯è¯»ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚å¯æ‚²çš„æ˜¯ï¼Œè‡ªä»æˆ‘åœ¨Pythoné‡æ–°è®¿é—®OOPä»¥æ¥å·²ç»å¾ˆä¹…äº†ã€‚  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32; /u/u/apricotslight9728     [links]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwqh6f/help_on_trying_to_understand_sarsa_semi_gradient/</guid>
      <pubDate>Mon, 24 Feb 2025 01:40:10 GMT</pubDate>
    </item>
    <item>
      <title>åŸºäºæ¨¡å‹çš„RLï¼šå¼€ç¯æ§åˆ¶æ˜¯äºšæœ€ä½³é€‰æ‹©ï¼Œå› ä¸º..ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘ç›®å‰æ­£åœ¨è§‚çœ‹Sergei Levineçš„è®²åº§ã€‚ä»–æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ„æºï¼›å°†å­¦ä¹ ç†è®ºçš„è”ç³»èµ·æ¥å¾ˆå¤šã€‚ä½¿ç”¨æ•°å­¦æµ‹è¯•çš„ç±»æ¯”ï¼Œé€šè¿‡å¼€ç¯æ§åˆ¶é€šè¿‡å¼€ç¯æ§åˆ¶æ˜¯äºšæœ€ä½³çš„ã€‚æˆ‘æƒ³è±¡è¿™ä¸ªç±»æ¯”å°±åƒæœç´¢æ ‘ä¸€æ ·ä¾‹å¦‚ï¼Œä½†å³ä½¿å¦‚æ­¤ï¼Œå®ƒä¹Ÿæœ‰ç‚¹æ¸…é™¤ã€‚ä½†æ˜¯ï¼Œè¦ä¸æŠ½è±¡çš„ä¾‹å­ä¿æŒåœ¨ä¸€èµ·ï¼Œä¸ºä»€ä¹ˆè¯¥æ¨¡å‹ä¸ä¼šåŸºäºä»¥å‰ä¸ç¯å¢ƒäº’åŠ¨çš„ç»éªŒäº§ç”Ÿå¯èƒ½æ€§ï¼Ÿ Sergeiæåˆ°ï¼Œå¦‚æœæˆ‘ä»¬é€‰æ‹©æµ‹è¯•ï¼Œæˆ‘ä»¬å°†å¾—åˆ°æ­£ç¡®çš„ç­”æ¡ˆï¼Œä½†ä¹Ÿæ„å‘³ç€æ²¡æœ‰åŠæ³•å°†è¿™äº›ä¿¡æ¯ä¼ é€’ç»™æ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»£ç†å•†çš„å†³ç­–è€…ï¼‰ã€‚æ„Ÿè§‰ä»ç°å®ä¸­æ¶ˆé™¤äº†ï¼Œå³å¦‚æœå¯èƒ½çš„æµ‹è¯•å°ºå¯¸è¶³å¤Ÿå¤§ï¼Œé‚£ä¹ˆæœ€ä½³çš„åŠ¨ä½œå°±æ˜¯å›å®¶ã€‚å¦‚æœæ‚¨å¯¹å‚åŠ æµ‹è¯•çš„èƒ½åŠ›æœ‰ä»»ä½•ä¿¡å¿ƒï¼ˆä¾‹å¦‚ä»¥å‰çš„æ¨å‡ºç»éªŒï¼‰ï¼Œé‚£ä¹ˆæ‚¨çš„æœ€ä½³ç­–ç•¥ä¼šæ›´æ”¹ï¼Œä½†è¿™æ˜¯ä¿¡æ¯ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¸ä»¥å‰çš„ç¤ºä¾‹ç›¸åŒçš„åˆ†å¸ƒæ¥ç†è§£ã€‚  ä¹Ÿè®¸æˆ‘ç¼ºå°‘æ ‡è®°ã€‚ä¸ºä»€ä¹ˆå¼€æ”¾ç¯æ§åˆ¶æ¬¡ä¼˜ï¼Ÿ   &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1iwhd9t/model_based_rl_rl_openloop_iscontrol_is_suboptimal/â€&gt; [link]  &lt;a href =â€œ https://www.reddit.com/r/reinforevectionlearning/comments/1iwhd9t/model_based_rl_rl_openloop_control_is_is_suboptimal/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwhd9t/model_based_rl_openloop_control_is_suboptimal/</guid>
      <pubDate>Sun, 23 Feb 2025 18:52:18 GMT</pubDate>
    </item>
    <item>
      <title>æˆ‘çš„ä»£ç æœªæ˜¾ç¤ºDyna-Qå’ŒDyna-Q+ç®—æ³•ä¹‹é—´çš„å·®å¼‚ã€‚è¯·å¸®åŠ©ä¿®å¤å®ƒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iwdj4k/difference_between_dynaq_and_dynaq_algorithm_not/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  é¦–å…ˆï¼Œæˆ‘åœ¨æ­¤env www.github.com/VachanVY/Reinforcement-Learning/blob/main/images/shortcut_maze_before_Dyna-Q_with_25_planning_steps.gifåˆ°è¾¾ç›®æ ‡çš„è·¯çº¿æ›´é•¿ã€‚ ç„¶åï¼Œæˆ‘ä»è¿™é‡Œä¹˜åQé˜€æ¥è®­ç»ƒdyna-q + algoåœ¨ä¿®æ”¹åçš„envä¸Šï¼Œå…¶ä¸­åŒ…å«é€šå¾€ç›®æ ‡çš„è¾ƒçŸ­é€”å¾„è¯æ˜å½“envå‘ç”Ÿå˜åŒ–æ—¶ï¼Œdyna-q + æ›´å¥½ï¼Œä½†æ˜¯åœ¨ä»¥ä¸‹ä»£ç åº”ç”¨åï¼Œæˆ‘è®¤ä¸ºåº”ç”¨Dyna-Q+ algoæ²¡æœ‰åŒºåˆ«ï¼Œåº”è¯¥é‡‡å–è¾ƒçŸ­çš„è·¯å¾„ã€‚  www.github.com/vachanvy/reinforeccation-learning/blob/main/images/shortcut_maze_after_dyna-q+_with_with_25_25_planning_steps.gifs.gif    æˆ‘çœ‹ä¸åˆ°å®ƒæ‰€é‡‡å–çš„è·¯çº¿çš„ä»»ä½•å˜åŒ–ï¼Œå°±åƒå¼ºåŒ–å­¦ä¹ Suttonå’ŒBarto  çš„ä»‹ç»æ—¶æ‰€è¯´çš„é‚£æ ·ï¼Œ``````````````&#39; ï¼Œæ—¥å¿—ï¼šbool = falseï¼Œq_values = noneï¼Œepsilon = epsilonï¼‰ï¼šplan = trueå¦‚æœ num_planning_steps&gt;0 else False if not plan: assert not dyna_q_plus q_values = init_q_vals(NUM_STATES, NUM_ACTIONS) if q_values is None else q_values env_model = init_env_model(NUM_STATES, NUM_ACTIONS) if plan else None last_visited_time_step = init_last_visited_timesï¼ˆnum_statesï¼Œnum_actionsï¼‰    sum_rewards_episodes = []; timeStep_episodes = [] total_step = 0 for Epistionï¼ˆ1ï¼Œnum_episodes+1ï¼‰ï¼šçŠ¶æ€ï¼Œinfo = env.resetï¼ˆï¼‰; sum_rewards = floatï¼ˆ0ï¼‰åœ¨è®¡æ•°ï¼ˆ1ï¼‰ä¸­çš„TSTEPï¼štotal_step += 1 action = sample_actionï¼ˆq_values [state]ï¼Œepsilonï¼‰next_stateï¼Œå¥–åŠ±ï¼Œå®Œæˆï¼Œæˆªæ–­ï¼Œnofe = ency.stepï¼ˆactionï¼‰; sum_rewards +=å¥–åŠ±q_values [state] [action] += alpha *ï¼ˆå¥–åŠ± +gamma * maxï¼ˆq_values [next_state]ï¼‰ï¼‰ -  q_values [state] [action] [action]ï¼‰last_visited_time_step [state] ï¼šenv_model [state] [action] =ï¼ˆå¥–åŠ±ï¼Œnext_stateï¼‰ï¼ƒï¼ˆå¥–åŠ±ï¼Œnext_stateï¼‰å¦‚æœå®Œæˆæˆ–æˆªæ–­ï¼šbreakçŠ¶æ€= next_state sum_rewards_episodes.appendï¼ˆsum_rewardsï¼‰timestep_episodes.append.appendï¼ˆtstepï¼‰å¦‚æœlogï¼šprintï¼šprintï¼ˆf&#39;epsisodeï¼š&#39;epsisodeï¼š{ponvision} ||å¥–åŠ±æ€»å’Œ{sum_rewards} ï¼†quotâ€ï¼ƒè®¡åˆ’æ˜¯å¦è®¡åˆ’ï¼šç”¨äºPlanning_step in rangeï¼ˆnum_planning_stepsï¼‰ï¼šplanning_state = rando_prev_observed_stateï¼ˆlast_visited_time_stepï¼‰ï¼ƒéšæœºè§‚å¯Ÿåˆ°çš„çŠ¶æ€planning_action_action_action_action_action_action_action_action_action_action_for_for_stateï¼ˆenv_model [plance_state] env_model [planning_state] [planning_action]å¦‚æœdyna_q_plusï¼šï¼ƒä¸ºäº†é¼“åŠ±æµ‹è¯•ï¼ƒé•¿é€”åŠ¨ä½œçš„è¡Œä¸ºï¼Œåˆ™åœ¨æ¶‰åŠï¼ƒè¿™äº›åŠ¨ä½œçš„æ¨¡æ‹Ÿä½“éªŒä¸Šç»™å‡ºäº†ç‰¹æ®Šçš„â€œå¥–åŠ±å¥–åŠ±â€ã€‚ç‰¹åˆ«æ˜¯ï¼Œå¦‚æœå¯¹è¿‡æ¸¡çš„å»ºæ¨¡å¥–åŠ±ä¸ºRï¼Œå¹¶ä¸”åœ¨Ï„æ—¶é—´æ­¥éª¤ä¸­å°šæœªå°è¯•è¿‡è¿‡æ¸¡ï¼ƒï¼Œåˆ™**è®¡åˆ’æ›´æ–°**å°±åƒè¯¥è¿‡æ¸¡ï¼ƒäº§ç”Ÿäº†R +Îº*çš„å¥–åŠ±ä¸€æ ·ï¼ˆÏ„ï¼ˆÏ„ï¼‰ ï¼‰^0.5ï¼Œå¯¹äºä¸€äº›å°Îºã€‚è¿™é¼“åŠ±ä»£ç†å•†ç»§ç»­æµ‹è¯•æ‰€æœ‰å¯è®¿é—®çš„çŠ¶æ€è¿‡æ¸¡ï¼Œç”šè‡³å¯ä»¥æ‰¾åˆ°é•¿é•¿çš„æ“ä½œåºåˆ—ä»¥è¿›è¡Œï¼ƒè¿›è¡Œæ­¤ç±»æµ‹è¯•ã€‚ ï¼ƒå½“å‰æ­¥éª¤ - ä¸Šæ¬¡è®¿é—®plance_reward += kappa * nath.sqrtï¼ˆtotal_visited_time_time_step [plance_state] [plance_actate] [plancy_action]ï¼‰q_values [planning_state] [planning_state] [planning_action] += alpha * [planning_state] [planning_action]ï¼‰æ‰“å°ï¼ˆæ€»æ­¥éª¤ï¼šï¼†quot; total_stepï¼‰è¿”å›q_valuesï¼Œsum_rewards_episodesï¼ŒtimeStep_episodes    ``` 32;æäº¤ç”±ï¼†ï¼ƒ32; /u/vvy_     [link]  &lt;a href =â€œ https://www.reddit.com/r/reinforeccationlearning/comments/1iwdj4k/difference_between_dynaq_and_ynaq_and_dynaq_algorithm_not/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iwdj4k/difference_between_dynaq_and_dynaq_algorithm_not/</guid>
      <pubDate>Sun, 23 Feb 2025 16:09:40 GMT</pubDate>
    </item>
    <item>
      <title>å­¦ä¹ æ”¿ç­–ä»¥æœ€å¤§åŒ–æ»¡è¶³B</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  æˆ‘æ­£åœ¨å°è¯•å­¦ä¹ ä¸€ä¸ªæ§åˆ¶ç­–ç•¥ï¼Œè¯¥ç­–ç•¥åœ¨ç¡®ä¿æ¡ä»¶Bæ—¶æœ€å¤§åŒ–å˜é‡ã€‚ä¾‹å¦‚ï¼Œæœºå™¨äººåœ¨å°†é€Ÿåº¦ä¿æŒåœ¨ç»™å®šèŒƒå›´å†…ï¼ˆbï¼‰çš„åŒæ—¶æœ€å¤§åŒ–èƒ½æºæ•ˆç‡ï¼ˆaï¼‰ã€‚ æˆ‘çš„æƒ³æ³•ï¼šå°†å¥–åŠ±å®šä¹‰ä¸ºa *ï¼ˆbï¼‰ã€‚å½“Bè¢«æ»¡è¶³Bæ—¶ï¼Œå¥–åŠ±å°†ä¸º= Aï¼Œå¹¶ä¸”åœ¨è¿åBæ—¶ä¸º= 0ã€‚ä½†æ˜¯ï¼Œè¿™å¯èƒ½ä¼šåœ¨åŸ¹è®­çš„æ—©æœŸå¼•èµ·ç¨€ç–çš„å›æŠ¥ã€‚æˆ‘å¯èƒ½ä¼šä½¿ç”¨æ¨¡ä»¿å­¦ä¹ æ¥åˆå§‹åŒ–ç­–ç•¥æ¥å¸®åŠ©è§£å†³æ­¤é—®é¢˜ã€‚ æ˜¯å¦æœ‰é€‚åˆæ­¤ç±»é—®é¢˜çš„ç°æœ‰æ¡†æ¶æˆ–æŠ€æœ¯ï¼Ÿæˆ‘éå¸¸æ„Ÿè°¢ä»»ä½•æ–¹å‘æˆ–ç›¸å…³å…³é”®å­—ï¼  &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforevercylearning/comments/1iw3ijl/learning_policy_to_to_maximize_a_a_a_a_while_satisfying_b/â€&gt; [link]   [æ³¨é‡Š]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iw3ijl/learning_policy_to_maximize_a_while_satisfying_b/</guid>
      <pubDate>Sun, 23 Feb 2025 06:05:48 GMT</pubDate>
    </item>
    <item>
      <title>åšå®¢ï¼šè¡¡é‡æ”¿ç­–æ¢¯åº¦çš„ç†è®ºè§‚ç‚¹</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</link>
      <description><![CDATA[&lt;ï¼ -  sc_off-&gt;  å¤§å®¶å¥½ï¼æˆ‘åœ¨è¿™é‡Œå¾ˆæ–°ï¼Œå¾ˆæŠ±æ­‰ï¼Œå¦‚æœå®ƒä¸ç¬¦åˆè§„åˆ™ï¼ˆæˆ‘æ‰¾ä¸åˆ°ä»»ä½•è§„åˆ™ï¼‰ï¼Œä½†æ˜¯æˆ‘æƒ³ä¸æ‚¨åˆ†äº«æˆ‘çš„åšå®¢ï¼Œä»¥è¡¡é‡æ”¿ç­–æ¢¯åº¦çš„ç†è®ºè§‚ç‚¹ï¼Œæˆ‘ä»‹ç»äº†æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨Raadon-Nikodym derivativeä¸ä»…å¯ä»¥å¾—å‡ºæ ‡å‡†å¢å¼ºï¼Œè¿˜å¯ä»¥å¾—å‡ºä¸€äº›ä»¥åçš„ç‰ˆæœ¬ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨å ç”¨åº¦é‡ä½œä¸ºè½¨è¿¹é‡‡æ ·çš„å€’å…¥æ›¿ä»£å“ã€‚å¸Œæœ›æ‚¨èƒ½äº«å—å¹¶ç»™æˆ‘ä¸€äº›åé¦ˆï¼Œå› ä¸ºæˆ‘å–œæ¬¢åœ¨RL  ä¸­åˆ†äº«ç›´è§‰çš„é‡å¤§è§£é‡Šï¼Œè¿™æ˜¯é“¾æ¥ï¼š httpsï¼š//myxik.github.io/posts/measuretheoretic-view/    &lt;ï¼ -  sc_on-&gt;ï¼†ï¼ƒ32;æäº¤ç”±ï¼†ï¼ƒ32;æ€href =â€œ https://www.reddit.com/r/reinforeverctionlearning/comments/1ivwzw9/blog_measure_theoretod_theoretic_theoretic_on_policy_gradients/â€&gt; [link]      [æ³¨é‡Š]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ivwzw9/blog_measure_theoretic_view_on_policy_gradients/</guid>
      <pubDate>Sun, 23 Feb 2025 00:08:21 GMT</pubDate>
    </item>
    </channel>
</rss>