<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 19 Feb 2024 12:24:18 GMT</lastBuildDate>
    <item>
      <title>无限循环</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aujcu9/infinity_loop/</link>
      <description><![CDATA[你好， 我训练了一个代理来玩类似俄罗斯方块的益智游戏。在每一步中，智能体都可以决定将棋子放置在棋盘上的可能位置或随机获得一个新棋子（6 种可能性）。我设置了奖励，以便首选具有尽可能少的部分的解决方案。尽管如此，他仍然可能达到这样一种状态：每个可能的随机块都以决定更喜欢获得新的随机块而告终。这将创造一个无限的游戏。我怎样才能避免这种行为？ 我的意思是代理训练有素，因此不会在每场比赛中都会发生，但如果发生会发生什么？我不能接受一个永无止境的游戏，游戏在解决之前不可能停止。总有一个可能的解决方案，因为即使是 1x1 大小的碎片也存在。 示例：https ://en-wiki.metin2.gameforge.com/index.php/Fishing_Jigsaw 我会感谢每一个想法和支持。从您的经验中了解您将如何设置奖励以及您将训练多少交互也将很有趣。您会选择什么复杂度的 dqn 神经网络？   由   提交/u/Reasonable_Cry8854  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aujcu9/infinity_loop/</guid>
      <pubDate>Mon, 19 Feb 2024 09:30:06 GMT</pubDate>
    </item>
    <item>
      <title>围棋游戏的无模型强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auh6v7/modelfree_rl_for_game_of_go/</link>
      <description><![CDATA[是否有任何无模型强化学习算法（没有 AlphaGO 系列中的 MCTS）可以在围棋比赛中超越人类专家？  如果是这样，它的性能与基于模型的方法相比如何？    由   提交 /u/RebornHugo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auh6v7/modelfree_rl_for_game_of_go/</guid>
      <pubDate>Mon, 19 Feb 2024 07:05:19 GMT</pubDate>
    </item>
    <item>
      <title>在 Pendulum v1 的 PPO 中使用熵正则化和 TanhNormal</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1au1hcd/using_entropy_regularization_with_tanhnormal_in/</link>
      <description><![CDATA[我正在 Pendulum v1 上使用 PPO，对动作空间使用 TanhNormal 分布。在尝试计算熵正则化的熵时，我遇到了负值，据我所知，由于分布在某些区域的高度集中，微分熵可能会发生负值。这给我提出了几个问题： 在像 Pendulum v1 这样的连续动作空间中，TanhNormal 分布得到负微分熵是否很常见？ 在处理问题时使用熵正则化仍然有意义吗？具有微分熵，尤其是当它可以为负时？ 在这种情况下如何通过 MC 采样准确计算熵？我一直在使用： x = new_dist.rsample(sample_shape=torch.Size([10000])) entropy = -torch.mean(new_dist.log_prob(x))  new_dist 是来自 torchrl 的 TanhNormal 分布，根据动作空间用特定的最小和最大边界定义。 我很好奇其他人如何处理类似的熵计算和正则化场景。熵正则化的方法是否会随着负熵值的可能性而改变？任何见解或参考将不胜感激！   由   提交 /u/hc7Loh21BptjaT79EG   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1au1hcd/using_entropy_regularization_with_tanhnormal_in/</guid>
      <pubDate>Sun, 18 Feb 2024 18:50:26 GMT</pubDate>
    </item>
    <item>
      <title>算法收敛于单一行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1atqur7/algorithm_converge_to_single_behaviour/</link>
      <description><![CDATA[嘿 我有一艘飞船需要躲避导弹。 奖励是导弹经过的最短距离宇宙飞船。我随机化船舶偏航角度（方向），我使用 td3，但有些角度更容易解决，这会导致所有角度的单一机动，因为奖励传播到更具挑战性的角度。 （这个动作可能对这些角度来说是最佳的）这是一个连续的问题，我尝试使用常规噪声和奥恩斯坦噪声。 （我宁愿不使用SAC） TL;DR飞船躲避导弹类型，有些角度更容易学习，所有角度都收敛到更容易的角度学习的行为 &lt; !-- SC_ON --&gt;  由   提交 /u/What_Did_It_Cost_E_T   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1atqur7/algorithm_converge_to_single_behaviour/</guid>
      <pubDate>Sun, 18 Feb 2024 10:08:51 GMT</pubDate>
    </item>
    <item>
      <title>有人可以帮助我了解如何进行策略迭代吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1atafso/could_anyone_please_help_me_understand_how_to_do/</link>
      <description><![CDATA[      我观看了很多视频，但我很难理解政策的流程迭代。有人可以提供使用所附示例图像实现最佳策略的分步指南吗？ ​ https://preview.redd.it/zzloc9ey87jc1.png?width=1102&amp;format=png&amp;auto= webp&amp;s=3399d830ce0107b5ff48637b3949f1e35a17849a 在此场景中，转移概率如下：A = 0.61、B = 0.39、C = 0.47、D = 0.53、E = 0.84 和 F = 0.16。将连续迭代之间的最大误差 (ε) 视为 0.01，将折扣因子 (γ) 视为 0.2。利用策略迭代方法，“Standing”状态的值是多少？ 提前谢谢。   由   提交 /u/thesmudgelord   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1atafso/could_anyone_please_help_me_understand_how_to_do/</guid>
      <pubDate>Sat, 17 Feb 2024 19:45:12 GMT</pubDate>
    </item>
    <item>
      <title>从头开始在 Unity 中训练 FlappyBird：5 分钟内 10k 个管道！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1asl0ob/training_flappybird_in_unity_from_scratch_10k/</link>
      <description><![CDATA[   /u/imitagent  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1asl0ob/training_flappybird_in_unity_from_scratch_10k/</guid>
      <pubDate>Fri, 16 Feb 2024 22:04:19 GMT</pubDate>
    </item>
    <item>
      <title>专家的混合解锁深度强化学习的参数缩放</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ashob2/mixtures_of_experts_unlock_parameter_scaling_for/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2402.08609 摘要：  （自）监督学习模型最近的快速进展是很大一部分是通过经验缩放定律预测的：模型的性能与其大小成正比。然而，对于强化学习领域来说，类似的缩放定律仍然难以捉摸，增加模型的参数数量通常会损害其最终性能。在本文中，我们证明了将专家混合 (MoE) 模块，特别是软 MoE（Puigcerver 等人，2023）纳入基于价值的网络会产生更多参数可扩展的模型，性能的显着提高就证明了这一点跨越各种训练制度和模型大小。因此，这项工作为制定强化学习的缩放法则提供了强有力的经验证据。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ashob2/mixtures_of_experts_unlock_parameter_scaling_for/</guid>
      <pubDate>Fri, 16 Feb 2024 19:46:19 GMT</pubDate>
    </item>
    <item>
      <title>RL 目前有什么用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1arnyvq/what_is_rl_good_for_currently/</link>
      <description><![CDATA[ 由   提交 /u/BadMeditator   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1arnyvq/what_is_rl_good_for_currently/</guid>
      <pubDate>Thu, 15 Feb 2024 19:34:51 GMT</pubDate>
    </item>
    <item>
      <title>帮助解决 PPO 导航问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/</link>
      <description><![CDATA[      我正在尝试使用 PPO 算法来解决一个简单的机器人导航问题。这是环境截图。 ​ https://preview.redd.it/ja7tq5v5uqic1.png?width=577&amp;format=png&amp;auto=webp&amp;s=e20b72e7f0c29b51c9e890b 3c33cec686d2327f1 &lt; p&gt;机器人（纯蓝色）必须导航到目标配置（空蓝色圆圈）。 演员网络设置为将单个灰度图像作为输入并输出下一个代理动作。  批评者网络将图像和当前时间步作为输入，并输出预期回报。 动作集为  wait&lt; /li&gt; 向前移动 向后移动 旋转 30 度 旋转 -30 度  每一步的奖励由以下公式给出： -0.1 + (dist_prev - dist_curr) + 100（如果达到目标）- 10（如果撞墙） 我使用的网络模型与 Atari DQN Nature 论文中使用的网络模型大致相同。 我面临的困难是，智能体在几千集之后似乎没有学到任何东西。 这些是我的 PPO 超参数：  GAMMA = 0.95 TRAJECTORIES_PER_LEARNING_STEP = 10 UPDATES_PER_LEARNING_STEP = 10 MAX_STEPS_PER_EPISODE = 100 ENTROPY_LOSS_COEF = 0 V_LOSS_CEOF = 0.5 CLIP = 0.2 LR = 3e-4  ​ 这是每集奖励的平滑图，它似乎只表现出随机行为。 ​ https://preview.redd.it/ 1yg4c1acwqic1.png?width=1906&amp;format=png&amp;auto=webp&amp;s=cd55d2c2b4b55bf66dce593b0934a6bc60f24987 问题：  为什么不起作用？&lt; /li&gt; 它应该有效吗？ 您希望它需要多少集？  如果需要，我很乐意分享代码。预先感谢您的评论！ ​   由   提交/u/david-wb  /u/david-wb  reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1areqrx/help_with_ppo_navigation_problem/</guid>
      <pubDate>Thu, 15 Feb 2024 12:52:14 GMT</pubDate>
    </item>
    <item>
      <title>帮助自定义环境 Pettingzoo</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</link>
      <description><![CDATA[我有一个自定义环境，我将代理设置为  self.agents = [&quot;EV_&quot; + str(r) for r in range(num_agents)]  当以以下形式测试环境时，我收到下一个错误： from EVenv import DepotEnv from pettingzoo.test import parallel_api_test from pettingzoo.test import api_test if __name__ == &quot;__main__&quot;: env = DepotEnv(num_agents=3) parallel_api_test(env, num_cycles=1000)  ，第 7 行，在  parallel_api_test(env, num_cycles=1000) 文件“C:\Users\” luisb\EVCHARGING\EVenv\lib\site-packages\pettingzoo\test\parallel_test.py”，第 122 行，parallel_api_test  assert ( AssertionError: [&#39;EV_0&#39;, &#39; EV_1&#39;, &#39;EV_2&#39;] != set()  我到处都找不到解决方案，我尝试设置代理，但它也不起作用，有什么想法吗？     提交者   /u/Barbajan22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ar3dxm/help_with_custom_environment_pettingzoo/</guid>
      <pubDate>Thu, 15 Feb 2024 01:11:18 GMT</pubDate>
    </item>
    <item>
      <title>自然语言强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqwxf0/natural_language_reinforcement_learning/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2402.07157 OpenReview：https:// /openreview.net/forum?id=0VzU2H13qj 摘要：  强化学习（RL）在以下方面表现出了非凡的能力：学习决策任务的策略。然而，强化学习常常受到样本效率低、缺乏可解释性和监督信号稀疏等问题的阻碍。为了解决这些限制，我们从人类学习过程中汲取灵感，引入了自然语言强化学习 (NLRL)，它创新地将强化学习原理与自然语言表示相结合。具体来说，NLRL 重新定义了自然语言空间中的任务目标、策略、价值函数、贝尔曼方程和策略迭代等 RL 概念。我们介绍如何利用 GPT-4 等大型语言模型 (LLM) 的最新进展来实际实施 NLRL。对表格 MDP 的初步实验证明了 NLRL 框架的有效性、效率和可解释性。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqwxf0/natural_language_reinforcement_learning/</guid>
      <pubDate>Wed, 14 Feb 2024 20:28:52 GMT</pubDate>
    </item>
    <item>
      <title>为机器人技术贡献提出重要的强化学习建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqq0df/suggest_important_rl_for_robotics_contributions/</link>
      <description><![CDATA[我多年来一直在研究应用强化学习，并且很幸运能够获得博士学位。人形机器人强化学习候选者。很兴奋！ :) 你能提示我一些 RL + 机器人领域必读的文献吗？   由   提交 /u/seawee1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqq0df/suggest_important_rl_for_robotics_contributions/</guid>
      <pubDate>Wed, 14 Feb 2024 15:47:49 GMT</pubDate>
    </item>
    <item>
      <title>帮助确定为什么我的 PPO 的 JAX 实现比 PyTorch 实现慢？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aqeiyo/help_determining_why_my_jax_implementation_of_ppo/</link>
      <description><![CDATA[大家好！我正在学习 JAX，作为学习的一部分，我尝试重新创建一个简单的离散操作版本的 PPO（最初基于 cleanRL JAX PPO 和 cleanRL PyTorch PPO）。然而，我发现它比本质上非常相似的代码的 PyTorch 版本要慢得多。谁能告诉我我在 JAX 实现中可能做错了什么？我在这里故意避免使用 envpool，只是为了坚持更简单的 Gymnasium 设置。 这是我的 JAX 脚本（全部在一个文件中，并且可以在一个文件中运行，如果您有必要的话，只需复制并粘贴即可）包）：https://pastes.io/kronipluiy 这是等效的 PyTorch 脚本：https://pastes.io/u5oz948e27   由   提交 /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aqeiyo/help_determining_why_my_jax_implementation_of_ppo/</guid>
      <pubDate>Wed, 14 Feb 2024 04:43:06 GMT</pubDate>
    </item>
    <item>
      <title>quilterai 筹集 1000 万美元，构建 RL 支持的硬件编译器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</link>
      <description><![CDATA[      强化学习最令人兴奋的行业应用之一即将规模化！    由   提交 /u/mccrearyd   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzzeu/quilterai_raises_10m_building_rlpowered_hardware/</guid>
      <pubDate>Tue, 13 Feb 2024 18:03:12 GMT</pubDate>
    </item>
    <item>
      <title>如何在整个阅读过程中应用萨顿和巴托的概念</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</link>
      <description><![CDATA[目前正在通过阅读这本书自学强化学习萨顿巴托：从头到尾介绍强化学习。我已经读了 4 章半了，感觉被它所强加的所有理论淹没了，有没有关于如何应用这些概念的随附材料或指南，以便我可以放慢一点的速度，并真正内化这些概念我正在阅读的内容？理想情况下，这些将应用于编程环境。 如果有人有时间提供一些建议，我将非常感激！  &amp;# 32；由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1apzyd5/how_to_apply_concepts_from_sutton_barto/</guid>
      <pubDate>Tue, 13 Feb 2024 18:02:05 GMT</pubDate>
    </item>
    </channel>
</rss>