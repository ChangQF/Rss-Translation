<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 23 Jan 2024 21:12:34 GMT</lastBuildDate>
    <item>
      <title>强化学习能否有效地生成具有很多约束的布局？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dwhxk/is_reinforcement_learning_efficient_to_generate/</link>
      <description><![CDATA[      你好， 对于一个学校项目，我想尝试使用强化学习生成平面图，并将其与用于此问题的现有方法（例如进化算法和监督机器学习）进行比较。我希望有一些 RL 经验的人对该项目进行一些评论。 输入：房间列表、房间邻接矩阵、计划占地面积、每个房间的一些空间限制，例如最小/最大面积或比率（宽度/长度）。迭代从随机设置房间的原始布局开始（也许我会启动具有不同开始布局的多个强化学习系统）。 操作：交换 2 个房间、推动房间墙壁、划分房间墙壁（没有矩形形状），合并房间墙 奖励：尊重房间邻接矩阵，尊重空间约束，可以访问所有房间。 使用进化算法，我发现的最相似问题的文章： https://www.researchgate.net/publication/312263676_Evolutionary_approach_for_spatial_architecture_layout _design_enhanced_by_an_agent-based_topology_finding_system 使用强化学习，我发现了与问题最相似的论文：“用于快速芯片设计的图形放置方法” https://www.nature.com/文章/s41586-021-03544-w.epdf?sharing_token=tYaxh2mR5EozfsSL0WHZLdRgN0jAjWel9jnR3ZoTv0PW0K0NmVrRsFPaMa9Y5We9O4Hqf_liatg-lvhiVcYpHL_YQpqkurA31sxqtmA-E1yNUWVM MVSBxWSp7ZFFIWawYQYnEXoBE4esRDSWqubhDFWUPyI5wK_5B_YIO-D_kS8%3D 目标是强化学习过程学习“如何设计住宅平面图”能够适应像这样的新足迹： https://preview.redd.it/clnijy4to8ec1.png?width=1460&amp;format=png&amp;auto=webp&amp;s=8d50de4c4348237b29218c39c963dd7ddf6eaad7   由   提交/u/Geralt2477  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dwhxk/is_reinforcement_learning_efficient_to_generate/</guid>
      <pubDate>Tue, 23 Jan 2024 19:22:28 GMT</pubDate>
    </item>
    <item>
      <title>第一个项目：蛇</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</link>
      <description><![CDATA[     &lt; td&gt; 算法是某种类型的强化（虽然不确定，我只是从课程中获取了 nn 更新部分），我有一个神经网络69m 参数。网络的输入是 3 个网格：苹果位置、蛇位置和地图外区域。我还根据蛇的旋转来旋转输入，因此它始终朝上   由   提交/u/thebrownfrog  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19duakt/first_project_snake/</guid>
      <pubDate>Tue, 23 Jan 2024 17:53:18 GMT</pubDate>
    </item>
    <item>
      <title>头脑风暴：多智能体的强化学习系统</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dr25j/brainstorming_rl_system_for_multiple_agents/</link>
      <description><![CDATA[我正在寻求有关如何构建多个智能体追逐目标的强化学习系统的建议。目标是让所有智能体接近目标，但不要太接近。同时，我希望代理均匀分布在目标周围。 在 2D 中，想象理想的解决方案是代理沿着目标周围的圆均匀分布。  (1) 我能否期望使用 PPO 训练每个代理实例会产生良好的组性能？或者我是否需要研究像 POCA 这样的多代理方法？ (2) 关于如何创建平衡这些同时目标的奖励函数有什么建议吗？    由   提交/u/CuriousDolphin1  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dr25j/brainstorming_rl_system_for_multiple_agents/</guid>
      <pubDate>Tue, 23 Jan 2024 15:34:05 GMT</pubDate>
    </item>
    <item>
      <title>仅考虑剧集奖励的 PPO 应用程序</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dmyvg/ppo_applications_which_consider_only_episode/</link>
      <description><![CDATA[这里有人遇到过 PPO 文献或应用程序，其中我们正在训练代理，然后只考虑最佳训练集（具有最大奖励的集）生成策略？ 主要问题是我可以在我的应用程序中执行此操作，因为无论我尝试什么，我的算法都会收敛到局部次最优解，所以我在想是否可以选出最好的执行情节来构建我的最终策略？   由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dmyvg/ppo_applications_which_consider_only_episode/</guid>
      <pubDate>Tue, 23 Jan 2024 12:07:14 GMT</pubDate>
    </item>
    <item>
      <title>一些 PPO 超参数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19dk2ep/some_of_ppo_hyperparams/</link>
      <description><![CDATA[仅设置并行环境数量 = 物理核心数量和每次更新的总时间步数 = 内存中适合的内容是标准程序吗？我以前有过不好的经历，但我不确定我是否只是运气不好，如果我不这样做，我觉得我只是在浪费机器的潜力。 其他超参数会当然也取决于这些，所以我想如果我在以前研究过的环境中工作，那么找到新的学习率、剪辑等是另一个问题，我可以从其他人发现工作正常的任何内容开始   由   提交 /u/victorsevero   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19dk2ep/some_of_ppo_hyperparams/</guid>
      <pubDate>Tue, 23 Jan 2024 08:50:10 GMT</pubDate>
    </item>
    <item>
      <title>迷宫游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d9a63/maze_game/</link>
      <description><![CDATA[Q-learning 项目，智能体自行学习找到迷宫内的出口。该项目是作为基于关卡的游戏实现的。 ​ https ://github.com/F-a-b-r-i-z-i-o/maze-game   由   提交/u/Stunning_Ad_1539   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d9a63/maze_game/</guid>
      <pubDate>Mon, 22 Jan 2024 23:04:55 GMT</pubDate>
    </item>
    <item>
      <title>有人知道斯坦福强化学习 XCS234 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d7mey/does_anyone_know_about_stanford_reinforcement/</link>
      <description><![CDATA[大家好， 我正在考虑这个在线课程。不过，我有一份全职工作。我的工作日程非常灵活，但这并不意味着我可以忽略所有的会议。  我看到描述说它不是学生节奏而是教师节奏。那么这是否意味着一旦我错过了课程，我就错过了？那么完成硬件并拿到证书会不会很麻烦？  这里有人以前上过课吗？有评论吗？谢谢   由   提交/u/sunson29  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d7mey/does_anyone_know_about_stanford_reinforcement/</guid>
      <pubDate>Mon, 22 Jan 2024 21:56:45 GMT</pubDate>
    </item>
    <item>
      <title>Cogment Lab 简介 - 用于人机循环 RL 的开发人员工具包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d6owp/introducing_cogment_lab_a_developers_toolkit_for/</link>
      <description><![CDATA[      你好你好，我&#39;我很高兴终于能分享我过去几个月在 AI-R 致力于的开源项目：Cogment Lab！ ​ tl ;博士，如果您想在有人参与的情况下运行 Gymnasium 或 PettingZoo 环境，现在就可以了。 ​ 您可以执行的操作的非详尽列表使用 Cogment Lab 轻松完成： 在 Gymnasium/PZ 中收集人类演示以进行模仿学习 观察学习代理并覆盖其行为 运行实验PettingZoo 环境中的混合人类与人工智能团队（与您的 RL 代理合作，或在竞争性游戏中击败它） 根据人类干预设置奖励 训练基于奖励的 RL 交织在一起实时行为克隆 ​ 该库仍在开发中，但应该完全可用。绝对欢迎任何建议、错误报告和贡献。 ​ Repo 链接：https://github.com/cogment/cogment-lab 教程：https://github.com/cogment/cogment-lab/tree/develop/ ​ ​ &gt; https://preview.redd.it/ 8t8ec4b162ec1.png?width=1081&amp;format=png&amp;auto=webp&amp;s=7ec357da5ac1e318d18ec4c7bde566be39c9c03b ​ PS 我很确定我的老板还在没有注意到这个标志，但它一直保持这种状态，直到有人强迫我让它变得更专业，并与公司在垂直业务或其他方面的协同作用保持一致  &amp;# 32；由   提交 /u/RedTachyon   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d6owp/introducing_cogment_lab_a_developers_toolkit_for/</guid>
      <pubDate>Mon, 22 Jan 2024 21:19:37 GMT</pubDate>
    </item>
    <item>
      <title>帮助表示感谢！尝试让代理在 Unity ML Agents 中投篮</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d2fm0/help_appreciated_trying_to_get_an_agent_to_shoot/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d2fm0/help_appreciated_trying_to_get_an_agent_to_shoot/</guid>
      <pubDate>Mon, 22 Jan 2024 18:24:19 GMT</pubDate>
    </item>
    <item>
      <title>最大化探索：融合估计、规划和探索的一个目标函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d210i/maximize_to_explore_one_objective_function_fusing/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2305.18258 OpenReview：https:// /openreview.net/forum?id=A57UMlUJdc 代码：https： //github.com/agentification/MEX 摘要：  在线强化学习（在线RL）中，平衡探索开发对于以样本有效的方式找到最优策略至关重要。为了实现这一目标，现有的样本高效在线强化学习算法通常由三个部分组成：估计、规划和探索。然而，为了应对通用函数逼近器，它们中的大多数都涉及不切实际的算法组件来激励探索，例如数据相关水平集内的优化或复杂的采样程序。为了应对这一挑战，我们提出了一种易于实现的强化学习框架，称为最大化探索 (MEX)，它只需要优化 &lt; em&gt;不受约束一个单一目标，集成了估计和规划组件，同时自动平衡勘探和开发。理论上，我们证明 MEX 通过马尔可夫决策过程（MDP）的一般函数逼近实现了亚线性遗憾，并且可以进一步扩展到两人零和马尔可夫游戏（MG）。同时，我们采用深度 RL 基线以无模型和基于模型的方式设计 MEX 的实用版本，在各种奖励稀疏的 MuJoCo 环境中，其性能可以稳定地优于基线。与现有的具有一般函数逼近的样本高效在线强化学习算法相比，MEX 实现了相似的样本效率，同时具有更低的计算成本，并且与现代深度强化学习方法更加兼容。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d210i/maximize_to_explore_one_objective_function_fusing/</guid>
      <pubDate>Mon, 22 Jan 2024 18:08:16 GMT</pubDate>
    </item>
    <item>
      <title>Mistral.AI 的 Mistral 7B - 完整白皮书概述</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19d1bdy/mistral_7b_from_mistralai_full_whitepaper_overview/</link>
      <description><![CDATA[       由   提交 /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19d1bdy/mistral_7b_from_mistralai_full_whitepaper_overview/</guid>
      <pubDate>Mon, 22 Jan 2024 17:39:47 GMT</pubDate>
    </item>
    <item>
      <title>内在奖励快速收敛的随机网络蒸馏。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cwg4h/random_network_distillation_for_intrinsic_reward/</link>
      <description><![CDATA[      你好， TLDR：随机网络蒸馏发生得如此之快没有进行任何探索。 我一直在尝试将随机网络蒸馏应用于问题以鼓励探索。虽然原则上一切正常，但我遇到了随机固定网络的问题快速地提取到我的探索网络中，即随机嵌入和预测嵌入之间的距离减小得如此之快，以至于在进行任何探索之前损失几乎为零。因此，历元的损失曲线和内在奖励看起来像这样：  https://preview.redd.it/3afnzfvo00ec1.png?width=696&amp;format=png&amp;auto=webp&amp;s=9bdcf4838d922ab324f0481f53bb1f44ac89d1ff  我猜这是因为状态表示相对简单（想想由 CNN 编码的代理、墙壁、物体等位置传递的几个布尔掩码），但不幸的是，这是我的文献中此环境的标准表示因此我无法改变它。顺便说一句，这并不会让环境变得容易。 RND 将以这种方式运行，而我的代理无法观察到任何外在奖励（简单空间中所需的复杂动作序列）。  关于如何使其更具挑战性有什么想法吗？我尝试扩大和缩小探索网络的网络架构，但遗憾的是没有成功。  谢谢！   由   提交 /u/Arconer   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cwg4h/random_network_distillation_for_intrinsic_reward/</guid>
      <pubDate>Mon, 22 Jan 2024 14:09:05 GMT</pubDate>
    </item>
    <item>
      <title>珍珠 vs 火炬RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cr8ih/pearl_vs_torchrl/</link>
      <description><![CDATA[这里有人使用过这两个框架，或者对这两个框架有足够的了解吗？   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cr8ih/pearl_vs_torchrl/</guid>
      <pubDate>Mon, 22 Jan 2024 08:44:00 GMT</pubDate>
    </item>
    <item>
      <title>我用 3D 动画教这个机器人自己行走</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</link>
      <description><![CDATA[       由   提交/u/djessimb   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cpwip/i_teach_this_robot_to_walk_by_itself_with_3d/</guid>
      <pubDate>Mon, 22 Jan 2024 07:08:18 GMT</pubDate>
    </item>
    <item>
      <title>编程…</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</link>
      <description><![CDATA[       由   提交/u/Throwawaybutlove  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19cjpiz/programming/</guid>
      <pubDate>Mon, 22 Jan 2024 01:27:44 GMT</pubDate>
    </item>
    </channel>
</rss>