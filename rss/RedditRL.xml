<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 08 Aug 2024 09:16:16 GMT</lastBuildDate>
    <item>
      <title>决策转换器：关于</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1en17o0/decision_transformer_a_basic_question_about/</link>
      <description><![CDATA[几个月前，我对 Decision Transformers 产生了浓厚的兴趣，尤其是在阅读了这里这篇文章之后。 因此，我阅读了这篇论文，观看了 YT 上的一些视频，并开始寻找真实的例子。 但即使阅读了这个 reddit 帖子，这个一个，这个一个和最后一个一个，有一个问题，我不是 100% 确定。问题是：由于 DT 需要大量离线数据进行训练，它们基本上就像过滤器一样，它们不会生成任何新数据，但它们会在最佳策略中“寻找”，哪个/什么动作会带来最高奖励。基本上它们不与任何环境交互。  这更有意义，因为 NLP 中的 Transformers 不是通过发明新词来生成新语言，而是使用字典中的现有单词通过一个接一个地输出单词来生成新句子。 因此，即使我真的对在 RL 中应用 Transformers 感兴趣，它们对于一般的 RL 问题也没有多大意义。 我发现了一篇关于 Online-DT 的论文。它看起来很有前途也很有趣，但基本上它只是为现有数据集添加了新的轨迹。 经过这么长的演讲：我是否忽略了关于 DT 的一个重要方面？ 非常感谢    提交人    /u/WilhelmRedemption   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1en17o0/decision_transformer_a_basic_question_about/</guid>
      <pubDate>Thu, 08 Aug 2024 09:06:23 GMT</pubDate>
    </item>
    <item>
      <title>[D] 关于政策和价值净损失的解释及系数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1en17ij/d_explanation_regarding_the_policy_and_value_net/</link>
      <description><![CDATA[大家好， 我正在实现 PPO，对某些人在包含系数和熵时实现损失的方式感到困惑。  这是我的“train_agent”代码的第一部分： action_dists, critical_values = self.model(batch_encoding, batch_positional, batch_decoder, batch_action_masks) training_probs = action_dists.log_prob(batch_actions) entropy = action_dists.entropy() ratio = torch.exp(training_probs - batch_memory_logprobs) min_advantage = torch.clamp(ratio, 1-self.clip_val, 1+self.clip_val) * batch_advantages policy_net_loss = -torch.minimum(ratio * batch_advantages, min_advantage).mean() entropy_mean = entropy.mean() value_loss = torch.mean((batch_returns - critical_values) ** 2)  但是，我注意到人们对如何整合最终损失的看法不同：我看到了以下内容： final_loss_one = policy_net_loss + self.policy_entropy_constant * entropy_mean + 0.5 * value_loss final_loss_two = policy_net_loss + 0.5 * value_loss - self.policy_entropy_constant * entropy_mean  这些产生了两个不同的损失值 - 但是，我不明白其中的等价性。如果有人可以向我解释，为什么它们是等价的，或者为什么一个是错的，哪一个是正确的，我将不胜感激。 PPO 论文表明第二个是正确的......也就是。剪辑策略损失 + 价值损失，然后是熵奖励...... 谢谢！   由    /u/amjass12  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1en17ij/d_explanation_regarding_the_policy_and_value_net/</guid>
      <pubDate>Thu, 08 Aug 2024 09:06:07 GMT</pubDate>
    </item>
    <item>
      <title>RL 创建用于训练神经网络的优化器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1en0y54/rl_to_create_optimizers_for_training_neural_nets/</link>
      <description><![CDATA[大家好， 我最近读了一篇论文 **VeLO：通过扩展训练多功能学习优化器** (https://arxiv.org/abs/2211.09760)，该论文训练了一个灵活的神经网络优化器来优化多个不同的模型架构，而无需进行超参数调整。 因此，我直觉地认为，理论上，您可以将超参数搜索框定为 MDP，其中奖励是模型在 n 个时期后的负损失。我的意思是，对于较大的网络来说，这可能不可扩展，并且参数必须在训练过程中进行调整。 但是，您知道该领域有什么工作吗？使用 RL 动态高效地调整训练神经网络的超参数？ 此致敬意！    提交人    /u/No_Individual_7831   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1en0y54/rl_to_create_optimizers_for_training_neural_nets/</guid>
      <pubDate>Thu, 08 Aug 2024 08:48:43 GMT</pubDate>
    </item>
    <item>
      <title>RL 任务，不知道如何处理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1emvtcs/rl_task_dont_know_how_to_approach/</link>
      <description><![CDATA[免责声明：从未实际做过 RL，只读过理论... 我有一个数据集，其参数为长度、周长、功率，均为浮点值。我还有另一个参数 SER。SER 不使用长度、周长或功率计算。 给定一个数据集，我可以使用 RL 找到长度、周长和功率的最佳设置以最小化 SER 吗？SER 值是对数刻度，范围从 -12 到 -1。有人可以指出我可以从哪些论文/github 获得灵感吗？非常感谢大家！    提交人    /u/Upset_Push_8412   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1emvtcs/rl_task_dont_know_how_to_approach/</guid>
      <pubDate>Thu, 08 Aug 2024 03:29:15 GMT</pubDate>
    </item>
    <item>
      <title>TD3 仍然是确定性策略梯度的最新技术吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1emozx3/is_td3_still_the_stateoftheart_for_deterministic/</link>
      <description><![CDATA[大家好，我希望重新开始研究强化学习，甚至可能再次尝试发表论文。然而，距离我发表最后一篇论文并毕业已经过去了 2 年，我却没有跟上任何最新技术的进步。我隐约知道一些新的研究方向，比如一次性学习和离线强化学习，但我最感兴趣的是&quot;vanilla&quot; 强化学习问题，其约束是环境不能重新启动或复制（评估目的和生成统计运行除外）。在这些约束下，我可以限制我的研究范围，而不必完全考虑一些新方法，如 ERL 或 A3C。  TD3 仍然是确定性策略梯度的最新技术吗？ 您有关于确定性策略梯度的调查或 SOTA 论文的资源可以给我吗？  是否有任何值得注意的新“技巧”可以应用于 TD3 以提高其性能或扩展其用例？  为什么我还没有看到任何方法采用简单的路线，只在最多 N 个步骤中的每个步骤中使用不同的 Q 网络，以完全避免 Q 网络自参考更新的问题？（我知道双 Q 和冻结 Q 网络的好处。这似乎可能是一个更简单的解决方案，可以让我们将注意力转移到策略梯度的其他方面，以回答特定的研究问题，并隔离 Q 网络训练方式的影响。）  我很感激您能与我分享的一切。谢谢！:)    提交人    /u/Omnifect   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1emozx3/is_td3_still_the_stateoftheart_for_deterministic/</guid>
      <pubDate>Wed, 07 Aug 2024 22:11:54 GMT</pubDate>
    </item>
    <item>
      <title>RL 论文工作代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1emkgpr/rl_agents_for_thesis_work/</link>
      <description><![CDATA[大家好！我是新来的，对强化学习也不是很熟悉。我目前是一名研究生，我想写一篇关于视频游戏开发中强化学习技术的评估的论文。为了进行我自己的实验，我认为寻求那些在游戏中对强化学习算法有丰富经验的人的帮助是个好主意。我想知道是否已经有项目可以在这项研究中提供帮助。 我的实验将包括真人与几个不同类型的短游戏互动，其中敌人是使用强化学习算法的人工智能。然后，我们将分析用户在那种氛围中与所述人工智能对战的感受，然后切换算法。我知道有很多游戏使用强化学习只是为了研究它们，但我希望能够让玩家对抗这种算法，看看这会如何改变体验。我正在寻找的类型如下：  格斗游戏 街机射击游戏 策略游戏  如果已经有任何工具或项目可以帮助解决这个问题，请告诉我！当然，如果在研究中使用，将给予信用。谢谢大家！    提交人    /u/Juno_02   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1emkgpr/rl_agents_for_thesis_work/</guid>
      <pubDate>Wed, 07 Aug 2024 19:09:06 GMT</pubDate>
    </item>
    <item>
      <title>非常缓慢的环境——我应该转向离线 RL 吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eme41i/very_slow_environment_should_i_pivot_to_offline_rl/</link>
      <description><![CDATA[我的目标是创建一个在高度复杂的生产环境中智能运行的代理。但我并不是从零开始：  我可以使用一个缓慢而复杂的软件，该软件能够相当好地模拟生产系统。 给定一个代理（手工制作或通过其他方式制作），我可以让它在这个模拟中自由发挥，记录其行为并计算性能指标。这意味着我有一个相当好的评估机制。  在这个模拟软件之上构建一个高性能健身房并进行在线强化学习是非常不切实际的。因此，我选择通过仅设计与手头问题最相关的功能来构建此模拟系统的简化版本。简化版本对于在线强化学习来说已经足够快了，但是正如您所猜测的，经过训练的策略在简化模拟中表现良好，而在原始模拟中表现较差。 我已经设法通过改进简化模拟来缓解这个问题，但这种方法已经失去动力，我正在寻找备用计划。你们认为离线强化学习是个好主意吗？我的理解是，它只适用于您无法访问模拟环境，但您拥有来自相当好的代理（可能来自生产环境）的历史观察-动作对的情况。如您所见，我的情况并没有那么糟糕 - 我可以访问模拟环境，因此我可以使用它为离线强化学习生成大量训练数据。我可以随意改变代理和模拟配置，这样我就可以生成丰富多样的训练数据。    提交人    /u/NoNeighborhood9302   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eme41i/very_slow_environment_should_i_pivot_to_offline_rl/</guid>
      <pubDate>Wed, 07 Aug 2024 15:03:38 GMT</pubDate>
    </item>
    <item>
      <title>LLM 中的 RLHF：可变作用空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1embf92/rlhf_in_llms_variable_action_space/</link>
      <description><![CDATA[嗨， 我有一个关于 PPO 参与 LLM 的 RLHF 的问题。由于目标是优化模型的答案，即一系列标记，那么动作空间是什么样的？ 标记序列的长度总是不同的，一个标记等于一个动作。因此，模型在每个步骤必须同时输出的动作数量会有所不同。我从未在 RL 中遇到过这样的情况，即每个步骤的动作数量都会有所不同。 所以我的问题是，这是否是正确的直觉，即如何构建动作，如果是，如何处理可变数量的动作？    提交人    /u/No_Individual_7831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1embf92/rlhf_in_llms_variable_action_space/</guid>
      <pubDate>Wed, 07 Aug 2024 13:12:18 GMT</pubDate>
    </item>
    <item>
      <title>从机器学习转向强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1em57qs/switching_academic_careerpath_from_ml_to_rl/</link>
      <description><![CDATA[我即将完成硕士学位，我专攻机器学习，即非常注重 NLP/ASR 和一些零散的简历。 我确实选修了一些认知系统的基础课程，这些课程涉及了强化学习的基础知识，但老实说，除了 A* 或 Q-learning 等一些流行词之外，大部分内容已经被遗忘了。 我正在考虑攻读计算机科学博士学位，但我想转到强化学习。这种转变现实吗，即，机构是否会聘请这样的博士职位，同时允许一个人过渡到强化学习？ 如果这很重要的话，我只考虑德国、瑞士的大学。我在攻读硕士学位期间的成绩一直很优秀，但我怀疑这是否重要。    提交人    /u/SenecaEnjoyer69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1em57qs/switching_academic_careerpath_from_ml_to_rl/</guid>
      <pubDate>Wed, 07 Aug 2024 06:59:20 GMT</pubDate>
    </item>
    <item>
      <title>使用强化学习 (RL) 寻找创业想法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1elwqag/looking_for_startup_ideas_using_reinforcement/</link>
      <description><![CDATA[我最近在加州大学完成了我的机器人学博士学位，在研究中我广泛使用了强化学习 (RL)。我正在探索潜在的创业想法，其中 RL 可以应用于解决实际问题。RL 可以在哪些创新领域或行业产生重大影响？欢迎任何可以带来令人信服的商业案例的想法或建议。     提交人    /u/Odd_Dig_5012   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1elwqag/looking_for_startup_ideas_using_reinforcement/</guid>
      <pubDate>Tue, 06 Aug 2024 23:33:01 GMT</pubDate>
    </item>
    <item>
      <title>“通过模拟寻找纳什均衡”（用 Python 可视化博弈论的收益和结果）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1elr14u/finding_nash_equilibria_through_simulation/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1elr14u/finding_nash_equilibria_through_simulation/</guid>
      <pubDate>Tue, 06 Aug 2024 19:38:30 GMT</pubDate>
    </item>
    <item>
      <title>分享我基于 JAX 的 RL 算法库 - 包括 BBF 和 TD7 实现</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1elikrd/sharing_my_jaxbased_rl_algorithms_repository/</link>
      <description><![CDATA[你好 r/reinforcementlearning！ 我很高兴与大家分享一个我一直在维护的存储库，用于我的实验、论文实现和基于 JAX 的 RL 优化：https://github.com/tinker495/jax-baseline 主要特点：  使用 JAX 实现高效的 RL 优化 可灵活选择 Flax 和 DeepMind Haiku 进行网络实现 包括最近的近 SOTA 算法：BBF（Big-Better-Faster）和 TD7  亮点：BBF 和 TD7 的实现已证明与其他算法相比，性能更高，与各自论文中提出的结果一致。如果您对这些算法感兴趣，此存储库可以作为有价值的参考。 重点是实现通用算法，而不是特定领域的优化。以下是基于 DQN 的内容：  DQN C51 QRDQN IQN FQF Ape-X 变体 SPR（带有 SR-SPR 选项） BBF  基于 A2C：  A2C PPO TPPO（真正的 PPO） IMPALA IMPALA-PPO（参考 rllib 实现）  基于 DDPG：  DDPG TD3 SAC TQC TD7  虽然它可能不是开创性的，但我希望这个存储库可以为那些使用或学习这些 RL 算法的人提供有用的资源。 请随意探索，欢迎任何反馈！    提交人    /u/New_East832   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1elikrd/sharing_my_jaxbased_rl_algorithms_repository/</guid>
      <pubDate>Tue, 06 Aug 2024 14:01:44 GMT</pubDate>
    </item>
    <item>
      <title>一切都很好，直到这个变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1el959m/everything_was_fine_until_this_change/</link>
      <description><![CDATA[      您好， 我有一个环境，与我分享给您的链接相同，该链接每集有 3500 步。我只是将步长从 3500 增加到 4000，延长了道路，代理开始努力寻找最佳解决方案集。 我的环境有一个最简单的奖励系统，每一步 +1，仅此而已。与 CartPole-v1 相同。如果代理撞到线上，情节就结束。 我使用 N-Step Dueling Double DQN 代理来解决它，它已经达到了最佳奖励，即 3500。延长道路后，代理一直停留在 [2800, 3200] 之间的奖励。 为了增强我的代理，我应该怎么做？如果情节长度使得代理在某个点之后陷入挣扎，你会怎么做？ 添加优先经验重放对我来说不起作用，或者它的参数 alpha=0.6、Beta=0.4 不是一个好的选择？  新算法？ 新的超参数？（增加批次大小、增加经验缓冲区大小、增加隐藏层、增​​加 N_Step、增加 LR 等等？） 用 LSTM 代替简单的神经元？  我想听听您的宝贵建议！ 谢谢……    提交人    /u/OpenToAdvices96   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1el959m/everything_was_fine_until_this_change/</guid>
      <pubDate>Tue, 06 Aug 2024 04:53:26 GMT</pubDate>
    </item>
    <item>
      <title>John Schulman（PPO、OA 联合创始人、后训练/RLHF）离开 OpenAI 加盟 Anthropic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1el3td4/john_schulman_ppo_oa_cofounder_posttrainingrlhf/</link>
      <description><![CDATA[    /u/gwern   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1el3td4/john_schulman_ppo_oa_cofounder_posttrainingrlhf/</guid>
      <pubDate>Tue, 06 Aug 2024 00:23:31 GMT</pubDate>
    </item>
    <item>
      <title>对于应开展什么项目，有何建议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ekug4s/advice_on_what_to_work_on_as_a_project/</link>
      <description><![CDATA[嘿，我有点为难了。我真的不知道我的下一个项目/目标是什么。我觉得我的兴趣很广泛，我真的需要一些建议。 今年秋天我将进入第四年，最近我一直在考虑强化学习方面的研究前景，并希望了解它的理论，也许可以做一些该领域的项目等。然而，我也有点喜欢创业公司/微型软件即服务的想法，并构建产品并将其作为副业进行扩展。毕业后，也可以选择参加 leetcode 以获得更好的全职工作。 我觉得我的兴趣无处不在，因为我看到这些领域有很多很酷的项目/研究，尽管我目前正在做一些事情，但我仍然对下一步该做什么感到困惑。 你们是如何决定要做什么的？感谢你们对此的见解。    提交人    /u/MysteriousAppl3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ekug4s/advice_on_what_to_work_on_as_a_project/</guid>
      <pubDate>Mon, 05 Aug 2024 17:58:22 GMT</pubDate>
    </item>
    </channel>
</rss>