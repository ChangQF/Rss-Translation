<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是旨在探索/理解复杂环境和学习如何最佳获得奖励的AI/统计数据的子场。例如Alphago，临床试验和A/B测试以及Atari游戏。</description>
    <lastBuildDate>Mon, 10 Mar 2025 21:17:00 GMT</lastBuildDate>
    <item>
      <title>使用RL探索电力市场竞标中的NASH平衡 - 寻求反馈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j88v0y/exploring_nash_equilibria_in_electricity_market/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我正在研究一个研究项目，我们旨在在使用加强学习的电力市场竞标中探索 nash equilibria 。核心问题是：  “在竞争性的电力市场中，代理人自然会像古典经济理论那样竞标其生产成本？ Or does strategic behavior emerge, leading to a different market equilibrium?&quot; Approach  Baseline Model (Perfect Competition &amp; Social Welfare Maximization):  We first model the electricity market using Pyomo, solving an optimization problem where all agents (generators and consumers) bid their  true成本。 这会导致最佳调度，最大化社会福利并用作基准。           rlib）允许代理人学习他们的最佳招标策略。 每个代理商都会提交出价，通过 pyomo 进行竞标，并根据利润分配奖励。平衡没有代理可以单方面改进。     比较＆amp;见解：  我们比较了基于 rl的NASH平衡与完美竞争基准  这使我们能够评估战略性竞标市场是市场操纵或效率           将模型扩展到多处拍卖，代理商会随着时间的流逝学习最佳策略。 探索混合竞争性合件设置，当地社区中的代理商在当地社区中的代理商进行了合作，但与其他社区竞争。竞标。  寻找反馈！  您是否在市场模拟之前多代理RL ？   在这种情况下，在这种情况下进行建模在这种情况下进行建模的建议？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/iminderent-milk5530     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j88v0y/exploring_nash_equilibria_in_electricity_market/</guid>
      <pubDate>Mon, 10 Mar 2025 20:42:57 GMT</pubDate>
    </item>
    <item>
      <title>为什么在SARSA算法的非政策n-Step版本中，重要性采样率不仅会使整个错误乘以整个错误？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j85tzj/why_in_the_offpolicy_nstep_version_of_sarsa/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  对我的理解，我们使用重要的采样比&#39;rho;加重在遵循行为政策时观察到的回报。根据目标策略“ pi”观察相同轨迹的概率。然后，如果我们考虑对许多回报的期望以及行为政策给出的概率的期望，我们将获得相同的价值，就好像我们接受了相同的回报的期望，但使用目标策略的概率，直观地，我认为这就像考虑加权收益rho•g一样，就像考虑到目标策略的目标，但在这种情况下，更新规则是Q＆lt; q q＆lt; q＆lt;  - 写为Q＆lt;  -  q + alpha•rho•（g-q）我们如何获得该表格？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/samas69420     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j85tzj/why_in_the_offpolicy_nstep_version_of_sarsa/</guid>
      <pubDate>Mon, 10 Mar 2025 18:36:10 GMT</pubDate>
    </item>
    <item>
      <title>LLM可以学会看吗？微调QWEN 0.5B用于SFT + GRPO的视觉任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j815kg/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/</link>
      <description><![CDATA[     &lt;！ -  sc_off-&gt;  嘿大家！ 我刚刚发布了一个博客，分解了一个博客，分解了数学 群体相对策略优化    grpo，depsseek r1 r1 步骤！   有趣的实验包括： i微调 qwen 2.5 0.5b ，a  folly&gt;语言 -  模型，没有事先视觉训练，使用 sft + sft + grpo    &lt;强&gt; href =“ https://preview.redd.it/ujxzkscccrvne1.png?width = 927＆amp; format = png＆amp; amp; amp; amp; amp; am https://preview.redd.it/ujxzksccrvne1.png?width=927＆amp; format = png＆amp; amp; amp; amp; auto = webppumppumpp；  完整博客&gt; href =“ https://github.com/jacksoncakes/vision-r1”&gt; github    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforeverctionlearning/comments/1j815kg/can_an_an_llm_llm_learn_learn_to_to_see_fine_fine_tuning_qwen_05b_for//”&gt; [link]      &lt;a href =“ https://www.reddit.com/r/reinforevectionlearning/comments/1j815kg/can_an_an_llm_llm_llm_learn_to_to_to_fine_fine_tuning_qwen_05b_05b_for/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j815kg/can_an_llm_learn_to_see_fine_tuning_qwen_05b_for/</guid>
      <pubDate>Mon, 10 Mar 2025 15:24:11 GMT</pubDate>
    </item>
    <item>
      <title>让SAC在大型并行模拟器上工作（第一部分）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</link>
      <description><![CDATA[&quot;As researchers, we tend to publish only positive results, but I think a lot of valuable insights are lost in our unpublished failures.&quot; This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with并行模拟了数千个机器人。如果您遵循旅程，您将了解可能对性能产生重大影响的任务设计和算法实现的细节。 扰流板警报：链接： https://araffin.github.io/post/post/sac-massive-sim/   提交由＆＃32; /u/u/araffin2     [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7ty3c/getting_sac_to_work_on_a_massive_parallel/</guid>
      <pubDate>Mon, 10 Mar 2025 08:27:24 GMT</pubDate>
    </item>
    <item>
      <title>复制DeepSeek-R1 RL所需的建议</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   hi rl社区，我想继续复制DeepSeek R1的RL训练管道中的小数据集。我对培训语言模型感到满意，但对培训RL代理不满意。我对深度RL的经典RL和中等理论理解有体面的理论理解。  我认为我需要逐步加强困难，以训练推理语言模型。因此，最近，我开始培训PPO实施方法来解决一些更轻松的健身环境，这确实在努力... 1周，我仍然无法再现低保真性，尽管基本上抬起了稳定的 - 贝赛3。&gt; 我想了解我的最终目标是否正确。一方面，如果我不能RL训练简单的代理商，我将如何使用RL训练语言模型。另一方面，我与我的朋友进行了限制RL经验的交谈，他提到，由于RL培训语言模型的代码已经在那里，而且挑战是正确的...    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/u/complect-media-8074      &lt;a href =“ https://www.reddit.com/r/reinforecctionlearning/comments/1j7t5j4/advice_needed_needed_reproducing_deepseekr1_rl/]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7t5j4/advice_needed_on_reproducing_deepseekr1_rl/</guid>
      <pubDate>Mon, 10 Mar 2025 07:25:05 GMT</pubDate>
    </item>
    <item>
      <title>VINTIX：通过文化强化学习的动作模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我们刚刚发布了我们在离线范围内的初步努力（诸如Laskin等人，2022年的算法蒸馏）时，将其放到离线范围内的范围。虽然我们正在从经典的元元素意义上寻求概括，但初步结果令人鼓舞，表现出对参数变化的适度概括，而仅在总共87个任务下接受了87个任务。 我们的关键要点在它上工作：（1）数据策划ICLR的数据curation for Iclr hard tweaking是一定的。希望所述的数据收集方法将有所帮助。而且我们还发布了数据集（约200mln元组）。 （2）即使在不同的数据集下，也可能对适度参数变化的概括。这是令人鼓舞的。但是，即使在类似jat的建筑中，也不是那么可怕（但很近）。   nb：随着我们进一步努力扩展并使状态和动作空间不变 - 也许您有一些有趣的环境/域名/元元学习基准，您希望在即将到来的工作中看到吗？ href =“ https://github.com/dunnolab/vintix”&gt; https://github.com/dunnolab/vintix    如果您传播词：https://x.com/vladkurenkov/status/1898823752995033299   提交由＆＃32; /u/u/vkurenkov     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7hm84/vintix_action_model_via_incontext_reinforcement/</guid>
      <pubDate>Sun, 09 Mar 2025 21:03:08 GMT</pubDate>
    </item>
    <item>
      <title>关于多目标增强学习的环境的概括</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</link>
      <description><![CDATA[      现实世界中的顺序决策任务通常涉及平衡相互矛盾的目标之间的权衡并在各种环境中概括。尽管它很重要，但尚未有一项工作在本文中研究多目标环境中的环境概括！ 我们在多目标增强学习（MORL）中正式化概括以及如何评估。我们还介绍了 Morl Generalization 基准测试，具有各种多样性域具有带有参数化的环境配置，以促进该领域的研究。 我们对当前最新的最新莫尔尔算法的基线评估2关键洞察力：  li li a li a li a li a li a li a li a li a li algorith 与单目标增强学习相比，莫尔（Morl）表现出更大的学习适应性行为的潜力。事后看来，这是可以预期的，因为多目标奖励结构更具表现力，并允许学习更多的行为！ 😲  我们坚信，在未来几年中，开发能够跨多种环境和目标概括的代理将成为一个至关重要的研究方向。有许多有希望的途径用于进一步的探索和研究，尤其是在单一目标RL概括研究中的适应技术和见解中，以解决这个更严重的问题设置！我期待着与有兴趣推进这一新的研究领域的任何人交往！ 🔗纸： https://arxiv.org/arxiv.org/abs/2503.00799999999999999       https://github.com/jaydenteoh         &lt;！ -  SC_ON-&gt;＆＃32;不同环境概括      &lt;！提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j7bdr3/on_generalization_carers_environments_in/”&gt; [link]   ＆＃32;   [注释]           ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j7bdr3/on_generalization_across_environments_in/</guid>
      <pubDate>Sun, 09 Mar 2025 16:31:56 GMT</pubDate>
    </item>
    <item>
      <title>Python和Unity的RL环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨，我想训练AI使用Python玩游戏，并以Unity（C＃）形象可视化游戏。目前，我需要在Python中创建环境，以学习实际游戏玩法。有没有办法创建我可以在Python和Unity中使用的环境？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/tot-chance9372     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j78xyg/rl_environment_in_python_and_unity/</guid>
      <pubDate>Sun, 09 Mar 2025 14:37:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的模型不能学会在连续的网格世界中玩游戏？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好。由于我正在研究深度Q学习算法，因此我正在尝试从头开始实施它。我创建了一个在网格世界中玩的简单游戏，我的目标是开发一个玩此游戏的代理商。在我的游戏中，状态空间是连续的，但是动作空间是离散的。这就是为什么我认为DQN算法应该起作用的原因。我的游戏具有3种不同的角色类型：主角（代理），目标和球。目标是在不与球相撞的情况下到达目标，而球线性移动。我的动作值剩下，右上，向下和什么都不是，总共进行了5个离散操作。 我使用Pygame Rect在Python中编码了游戏，用于目标，角色和球。我奖励代理如下：   +5，与角色相撞  -5，与球相撞，以  +0.7   +0.7，以靠近目标（使用Manhattan距离）   -1远离目标（使用Manhattan距离移动）（使用Manhattan距离远处）。我尝试了不同的状态表示形式，但是在最好的情况下，我的经纪人只学会避免球并达到目标。在大多数情况下，代理根本不会避免球，或者有时它会连续向左和向右进入摇摆的运动，而不是达到目标。&lt; /p&gt; 我给出了状态表示，如下所示：&lt; /p&gt;  agent.rect.rect.lect.lect.lect.lect.lect.rect.rect.rect.rect.rect.right.rect.right， agent.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.rect.lect--rect-rect--rect- exent， target.rect.bottom， agent.rect.bottom--  Agent.Rect.bottom-  ball.Rect.top ， ball_direction_in_x，ball_dircection_in_in_in_in_in_iin_y_y_y  partive（part）这描述了对经纪人的比赛状态，提供了球和目标的相对位置以及球的方向。但是，我的模型的表现令人惊讶地差。相反，我将状态分类如下：  如果目标在左边，则为-1。 如果目标在右边为+1。游戏中很少或没有球），模型的性能大大提高。当我从游戏中删除球时，分类的状态代表学得很好。但是，当出现球时，即使表示形式连续，该模型也非常慢，最终它过度拟合。 我不想屏幕截图游戏屏幕并将其馈入CNN。我想使用密集的层将游戏的信息直接提供给模型，然后学习。为什么我的模型不学习？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevercylearning/comments/1j786hr/1j786hr/why_cant_my_my_my_model_model_lealed_to_to_play_in_continuul/”&gt; [link]   [注释]      ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j786hr/why_cant_my_model_learn_to_play_in_continuous/</guid>
      <pubDate>Sun, 09 Mar 2025 13:58:09 GMT</pubDate>
    </item>
    <item>
      <title>机器人技术定制体育馆环境设计。包装纸还是阶级继承？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在为水下机器人构建自定义环境。我已经尝试使用一个快速且脏的整体环境，但是如果我尝试修改环境以添加更多传感器，转换输出，重用代码，以完成另一个任务等，我现在会遇到问题。 ，我想重构代码并必须做出一些设计选择：我应该使用一个不适用的spass类并在每个任务中使用训练和训练的范围，或者我要训练劳动范围，或者我要训练劳动范围，或者我要求职。我只有一个基类，并将其他所有内容添加为包装器（包括传感器配置，任务奖励 +逻辑等）？ 如果您知道环境创建的良好资源，这将不胜感激）  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/equiald-diver     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j73wup/custom_gymnasium_environment_design_for_robotics/</guid>
      <pubDate>Sun, 09 Mar 2025 09:18:42 GMT</pubDate>
    </item>
    <item>
      <title>Han等人“一般推理需要学习从一开始推理”。 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</link>
      <description><![CDATA[   [link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j72yhl/general_reasoning_requires_learning_to_reason/</guid>
      <pubDate>Sun, 09 Mar 2025 08:05:53 GMT</pubDate>
    </item>
    <item>
      <title>软动作掩蔽</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6t9rx/soft_action_masking/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  有一个想法“软动作遮罩”？对于那些对强化学习的原始数学贴心的人，我将提前道歉。我的想法还没有正式的数学。 让我以一个例子来说明我的想法。想象一个具有以下约束的环境：   - 代理商的可用动作之一是“什么都不做”。   - 每秒发送太多动作是一件坏事。但是，这里不知道具体数字。也许我们有一些数据，即每秒大约10个动作是最大的。有时13/秒是可以的，有时8/秒是不希望的。 防止代理在给定时间范围内采取太多操作的一种方法是使用动作掩蔽。如果最大的动作率是一个明确定义的数量，例如，在最后一秒钟内，代理已经采取了10个操作，则代理将被迫“无所事事”。通过动作面具。一旦最后一秒钟的动作数量降至10以下，我们将不再使用面具并让代理自由选择。 现在，现在考虑到我们的模糊要求，我们是否可以逐渐强迫我们的代理商选择“无所事事”。动作越来越接近极限？我故意不会在数学上正式描述这个想法，因为我认为这在很大程度上取决于您使用的算法类型。相反，我会尝试描述直觉。如上所述，在环境限制中，我们的速率限制约为每秒8-13个动作。如果代理商在最后一秒钟已经采取了10次措施，并且非常有信心它希望采取其他措施，也许我们应该允许它。但是，如果它在篱笆上，只有与什么无所作为相比，只有稍微倾向于采取其他动作，也许我们应该稍微推动它，以便它选择什么都不做。随着动作数量的增加，这个“ nuding”变得越来越强。一旦达到13，在此示例中，我们本质上使用了上述典型的动作掩盖方法，并迫使代理什么都不做，无论其偏好如何。 在策略梯度算法中，这种方法在我看来更有意义。我可以想象，仅将灰心的动作偏好乘以（0,1）中的值。传统的动作掩蔽可能会完全乘以0。我还没有考虑到基于价值的算法。 你们都在想什么？这似乎是有用的吗？我在一个自己的项目和大脑冲击解决方案中大致遇到了这个问题。我可以实现的另一个解决方案是奖励函数，它不鼓励超过限制，但是直到代理商实际学习奖励功能的这一方面，它可能会大大超过限制，并且我需要在任何方面实施一些艰难的动作掩盖。另外，这样的奖励功能似乎很棘手，因为利率限制奖励可能与我实际想学习的奖励是正交的。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sandsnip3r     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6t9rx/soft_action_masking/</guid>
      <pubDate>Sat, 08 Mar 2025 22:52:00 GMT</pubDate>
    </item>
    <item>
      <title>学习ISAAC SIM / ISAAC实验室的最快方法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6rkdf/fastest_way_to_learn_isaac_sim_isaac_lab/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 机电一体化工程师在这里具有ROS/凉亭的经验和表面水平Pybullet + Pybullet + Gymnasium体验。我正在训练RL代理在某个任务上进行训练，我需要进行一些域随机化，因此将其并行化将有很大的帮助。最快的最低工作示例最快的最快是什么。学习ISAAC SIM/ISAAC实验室框架的方法或来源用于模拟RL代理的培训？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/reinforevericeslearning/comments/1j6rkdf/fastest_way_way_to_to_lealed_isaac_sim_isaac_isaac_lab/&gt; [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6rkdf/fastest_way_to_learn_isaac_sim_isaac_lab/</guid>
      <pubDate>Sat, 08 Mar 2025 21:33:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么功能近似会导致折扣RL的问题，但不会平均奖励RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j6mr4u/why_does_function_approximation_cause_issues_in/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  在 强化学习简介（第10.3章）中，萨顿介绍了平均奖励设置，没有打折的地方，并且代理值延迟延迟延迟的重新延迟，以立即奖励。他提到功能近似可能会导致折扣设置的问题，这是使用平均奖励的原因之一。 我了解平均奖励设置如何工作，但我不完全了解为什么功能近似值在折扣下差异。有人可以解释问题吗？ 为什么在证据中，萨顿实际上表明，折现设置在数学上等同于未估计的设置（成比例为1/（1 -γ）1/（1  -  \ gamma）1/（1  -  \ gamma）1/（1-γ）1/（1-γ）），所以我不明确地构成折扣的问题。具有政策改进定理，它可以确保提高一个国家的价值会导致整体政策改善。但是据我所知，这个问题适用于持续和情节任务，所以我仍然不明白为什么平均奖励是一个更好的选择。 有人可以在这里阐明动机吗？  &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/unity_hippo1724     [link]   [commist]   ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j6mr4u/why_does_function_approximation_cause_issues_in/</guid>
      <pubDate>Sat, 08 Mar 2025 17:56:37 GMT</pubDate>
    </item>
    <item>
      <title>Andrew G. Barto和Richard S. Sutton被任命为2024 ACM A.M.图灵奖</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</link>
      <description><![CDATA[   /u/u/meepinator     &lt;a href =“ https://www.reddit.com/r/reinforeccationlearning/comments/comments/1j472l7/andrew_g_barto_and_richard_richard_richard_s_s_sutton_neamed_as/”]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1j472l7/andrew_g_barto_and_richard_s_sutton_named_as/</guid>
      <pubDate>Wed, 05 Mar 2025 16:29:27 GMT</pubDate>
    </item>
    </channel>
</rss>