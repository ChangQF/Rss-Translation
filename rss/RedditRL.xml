<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 24 Apr 2024 06:21:02 GMT</lastBuildDate>
    <item>
      <title>规范观察、奖励和价值目标的标准方法是什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbpj9q/what_is_the_standard_way_of_normalizing/</link>
      <description><![CDATA[我正在观看 John Schulman 的深度强化学习实验的具体细节https://www.youtube.com/watch?v=8EcdaCk9KaQ&amp;t=687s&amp;ab_channel=AIPrism 他提到你应该规范奖励、观察，价值目标。我想知道这是否真的完成了，因为我还没有在 RL 代码库中看到它。你能分享一些建议吗？   由   提交 /u/miladink   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbpj9q/what_is_the_standard_way_of_normalizing/</guid>
      <pubDate>Wed, 24 Apr 2024 04:35:02 GMT</pubDate>
    </item>
    <item>
      <title>S 实时卡牌策略游戏使用的模型类型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cblyzz/type_of_model_to_use_for_s_a_realtime_card/</link>
      <description><![CDATA[我非常熟悉监督和无监督，但我以前从未使用过强化。在大学上过一门关于它的课程，所以我确实了解了一些理论，但我很难找到一个适合这种情况的好模型，所以我想我在这里要求指出正确的方向。 在该游戏的动作空间由您手中的牌与 x-y 网格上放置牌的离散位置组合而成。然而，你的手牌是你的牌组的子集，而你的牌组是更大的 128 张牌的子集。在游戏中，你将参与一场 1v1 的即时战略游戏，用你的军队击败对手。因此，在任何给定点，您只能打出手上可用的牌。 所以我陷入困境，因为这并没有真正遵循传统的强化动作模型，其中动作很简单，例如（移动向左，向右移动……）并且您可以在任何状态下执行这些操作。就像你在教程中看到的那样。 我觉得我必须抽象一点，将动作空间变成这样的东西：攻击、防御、什么也不做……但即便如此，我也不知道如何选择使用哪张卡以及去哪里。  任何提示都有帮助，谢谢。    由   提交 /u/AskedSuperior   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cblyzz/type_of_model_to_use_for_s_a_realtime_card/</guid>
      <pubDate>Wed, 24 Apr 2024 01:30:09 GMT</pubDate>
    </item>
    <item>
      <title>ReBeL 贝叶斯更新</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbgpck/rebel_bayesian_updating/</link>
      <description><![CDATA[我已阅读ReBeL 论文，感觉我已经很好地掌握了 CFR 的结构以及围绕训练的主要思想，并且对使用 Kuhn 扑克进行公众信仰状态的 CFR 工作版本（尽管我知道有时简单版本中不会出现错误）。  我的问题实际上是在论文中基本上是一次性的一行：  当这个游戏开始时，每个玩家对其私人卡牌的信念分布是均匀随机的。 但是，在裁判的每次动作之后，玩家可以通过贝叶斯规则更新他们对自己持有哪张牌的信念分布。同样，玩家可以通过相同的操作更新他们对对手的私人牌的信念分布。因此，在这个游戏中，每个玩家持有每张私人牌的概率是所有玩家在任何时候都知道的。  这是有道理的：主要思想是，如果你有一个分布玩家 a (p_a) 的状态，并且您知道玩家 a 的策略 (policy_a)，那么您可以使用贝叶斯更新玩家 a 对自己状态的信念。该策略明确为 P(action | p_a)。  要更新玩家 b 的状态，您需要导出 P(action | p_b)。您可以通过边缘化玩家 a 的持股并应用相互排他性来做到这一点。  我所困扰的是，是否有一些技巧可以使这种计算在大动作空间和多个玩家的情况下变得易于处理。如果您进行了一场完整的环形游戏，那么您将获得约 1326 ^ 9 个状态的联合分布。我知道使用不兼容矩阵可能有一些技巧，但据我所知，这些技巧中的大多数都会具体化在多人游戏设置中非常大的联合矩阵。 def update_players_states(state_matrix, actor_index ,policy_matrix,action): num_players,num_states = state_matrix.shape new_state = np.zeros_like(state_matrix) p_action = np.dot(state_matrix[actor_index, :],policy_matrix[:,action]) # 获取 np 中索引的所有可能状态.ndindex((num_states,) * num_players): if len(set(indices)) == num_players: # 作为快捷方式，只需使两个状态不兼容，只有当它们是完全相同的状态时 prob = 1.0 available = list(range(num_states)) # 仍可供选择的状态 for i, state in enumerate(indices): state_probability = state_matrix[i, state] / sum(state_matrix[i, available]) # 可用状态的条件 prob * = state_probability available.remove(state) action_prob = policy_matrix[indices[actor_index], action] joint_action_prob = prob * action_prob p_joint_given_action = joint_action_prob # 累加枚举(indices)中的player_idx、state_idx: new_state[player_idx, state_idx] += p_joint_given_action new_state / = p_action return new_state  为了摆脱 ReBeL 的困扰，这里有一个更简单的问题版本。 你正在玩一个有 N 个玩家和 N 张牌的游戏。所有牌最初面朝下并随机分配（每个人持有 N 张牌中任意一张的概率为 1/N）。  裁判说“玩家 1 有 20% 的机会拿到 2”。根据该声明，您如何（有效地）更新每个人的信念？   由   提交 /u/Dhdjskk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbgpck/rebel_bayesian_updating/</guid>
      <pubDate>Tue, 23 Apr 2024 21:35:37 GMT</pubDate>
    </item>
    <item>
      <title>从什么框架开始学习强化学习？艾萨克·西姆？穆乔科RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbfraz/what_framework_to_start_learning_rl_with_isaacsim/</link>
      <description><![CDATA[嗨，我刚刚开始研究强化学习并尝试各种 cartpole 实现。 哪个框架被认为是在 pytorch 中训练强化学习算法的“最佳”以及其优点/缺点是什么IsaacSim、openAI Gym、基于 Mujoco 的框架？我什至不确定哪些是最突出的，我需要在这里进行一些冷启动。谢谢。   由   提交/u/Specialist_Ice_5715   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbfraz/what_framework_to_start_learning_rl_with_isaacsim/</guid>
      <pubDate>Tue, 23 Apr 2024 20:58:38 GMT</pubDate>
    </item>
    <item>
      <title>稳定的基线3 DQN、A2C 和 PPO 在 Pong 上成绩不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cbch1y/stable_baselines3_dqn_a2c_and_ppo_getting_bad/</link>
      <description><![CDATA[     &lt; td&gt; 你好。  我更改超参数已经有一段时间了，但没有迹象表明可以得到更好的结果。 https://preview.redd.it/ffcwkk2mx9wc1.png?width=930&amp;format=png&amp;auto=webp&amp;s=af5d9e10bf70a9fe9 5ce5ae3b26730be96990e1a ~DQN~ vec_env: VecEnv = make_vec_env(&quot;ALE/Pong-ram-v5&quot; ;, n_envs=4) vec_env = VecFrameStack(vec_env, n_stack=4)policy_kwargs = dict(activation_fn=torch.nn.ReLU, net_arch=[64, 32]) model = DQN(“MlpPolicy”, vec_env,learning_rate=0.0001 ，target_update_interval = 1000，train_freq = 4，buffer_size = 10000，learning_starts = 5000，batch_size = 32，exploration_fraction = 0.1，exploration_final_eps = 0.01，gamma = 0.99，gamma = 0.99，polition_kwargs = polition_kwargs = polition_kwargs = kwargs = qualtepe = kwargs = qualteps = retimest（reality） /pre&gt; ~A2C~ vec_env: VecEnv = make_vec_env(&quot;ALE/Pong-ram-v5&quot;, n_envs = 4）vec_env = VecFrameStack（vec_env，n_stack = 4）policy_kwargs = dict（activation_fn = torch.nn.ReLU，net_arch = [dict（pi = [32, 16]，vf = [32, 16]）]，ortho_init =真）模型= A2C（“MlpPolicy”，vec_env，learning_rate=1.4e-5，n_steps=512，gamma=0.983，gae_lambda=0.95，max_grad_norm = 0.36，ent_coef=0.01，policy_kwargs=policy_kwargs）model.learn（total_timesteps） =2000000)  ~PPO~ vec_env: VecEnv = make_vec_env(&quot;ALE /Pong-ram-v5”，n_envs = 4） vec_env = VecFrameStack（vec_env，n_stack = 4）policy_kwargs = dict（activation_fn = torch.nn.ReLU，net_arch = dict（pi = [32, 32]，vf = [32） ，32]））模型= PPO（“MlpPolicy”，vec_env，learning_rate=2.5e-4，n_steps=128，batch_size=256，n_epochs=4，gamma=0.99，gae_lambda=0.95，clip_range=0.3，ent_coef=0.1 ，vf_coef=0.5，policy_kwargs=policy_kwargs，device=“cpu”）model.learn（total_timesteps=2000000） 此外，通过这个确切的设置，我使用了不同的体系结构，例如{[64, 32, 16] [64, 32, 16]}，{[32, 16]，[32, 16]} 等等。我还尝试过使用游戏作为图像和 CnnPolicy。   由   提交/u/ufoludek3000   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cbch1y/stable_baselines3_dqn_a2c_and_ppo_getting_bad/</guid>
      <pubDate>Tue, 23 Apr 2024 18:47:45 GMT</pubDate>
    </item>
    <item>
      <title>使用 DreamerV3 训练鱼游任务</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cb0o62/training_a_fishswim_task_using_dreamerv3/</link>
      <description><![CDATA[大家好！我目前正在尝试使用 Hafner 等人的 DreamerV3（此处链接到论文）。我能够在我的计算机中对其进行设置，并能够运行他们在论文中所做的默认示例。  但是，我想要训练策略的主要任务是 Deepmind Control Suite 中的 fish-swim 任务，特别是使用 dmc_vision设置。我能够让它运行，但它训练的策略很差。即使我训练它 100 万步（默认设置），我得到的奖励也非常低。  你对我可以尝试什么有什么建议吗？以下是我到目前为止所做的：  这是我正在使用的 DreamerV3 的实现：https://github.com/NM512/dreamerv3-torch  我尝试使用dmc_proprio&lt;为fish-swim任务训练策略/code&gt; 我能够训练出良好的策略并获得良好的回报。但是，我想使用视觉输入让它工作。  我尝试使用状态空间和图像输入来增强编码器和解码器，但生成的策略仍然很差（与 dmc_vision 性能类似）。   我对强化学习也比较陌生，所以如果我没有正确使用术语，我深表歉意。如果我的问题中有任何需要澄清的地方，请告诉我。谢谢你！    由   提交/u/Learning-Robot-137   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cb0o62/training_a_fishswim_task_using_dreamerv3/</guid>
      <pubDate>Tue, 23 Apr 2024 09:56:07 GMT</pubDate>
    </item>
    <item>
      <title>DDQN 代理通过 keras 和 flappybird 收敛到次优策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cazr28/ddqn_agent_converges_to_a_suboptimal_policy_with/</link>
      <description><![CDATA[大家好，我正在尝试训练一个智能体来玩 flappybird，但是无论我如何调整我的超参数，我都会遇到收敛问题。我认为某处可能有错误，但我无法发现它，也许多一双眼睛可以提供帮助。这是我的实现： LINK 该实现支持多步骤，但目前我正在测试没有。 该算法与 atari 非常相似，只是修改了预期 Q 值的计算，该值使用了 H. van Hasselt 提出的优化。   由   提交 /u/Overall-Ask-3858   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cazr28/ddqn_agent_converges_to_a_suboptimal_policy_with/</guid>
      <pubDate>Tue, 23 Apr 2024 08:52:55 GMT</pubDate>
    </item>
    <item>
      <title>重要性抽样的书籍参考</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caxkyn/book_reference_for_importance_sampling/</link>
      <description><![CDATA[嗨，我正在寻找一些有关重要性采样数学的资料。我觉得所有课程都很好地介绍了它，但我想要我的论文有一个书源。我在一些概率书籍上查找过，但找不到任何参考资料。   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caxkyn/book_reference_for_importance_sampling/</guid>
      <pubDate>Tue, 23 Apr 2024 06:22:57 GMT</pubDate>
    </item>
    <item>
      <title>Isaacgym 中 Franka 的自定义动作空间</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1camkc3/custom_action_space_for_franka_in_isaacgym/</link>
      <description><![CDATA[大家好， 我正在尝试与 Franka 一起制作自定义 RL 环境。我想制作一个自定义的离散动作空间，基本上是全局坐标/姿势供机器人选择。有什么想法吗？   由   提交/u/Natural-Ad-6073   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1camkc3/custom_action_space_for_franka_in_isaacgym/</guid>
      <pubDate>Mon, 22 Apr 2024 21:27:36 GMT</pubDate>
    </item>
    <item>
      <title>稳定基线问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caljp2/issue_with_stable_baselines/</link>
      <description><![CDATA[使用 pybullet 创建自定义环境。环境运行良好，最高可达 10000 步左右。获得意想不到的观察形状。不知道当模拟运行 10k 步正常时会发生什么。   由   提交 /u/Open-Chemical-7930   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caljp2/issue_with_stable_baselines/</guid>
      <pubDate>Mon, 22 Apr 2024 20:47:13 GMT</pubDate>
    </item>
    <item>
      <title>在专用服务器上运行的并行环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caj70y/parallelized_environments_running_on_a_dedicated/</link>
      <description><![CDATA[在 RL 中最大化 GPU 利用率通常相当困难，即使在并行环境中也是如此，因此我正在考虑在单独的仅包含 CPU 的服务器上运行并行模拟器使用最大可能数量的 CPU，并在任何云中的 GPU 节点上进行实际训练（在同一放置组内 + 所有可能的网络优化）。 这是否可行，或者额外的网络延迟是否太大而无法实现这可行吗？   由   提交/u/Ok-Entertainment-286   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caj70y/parallelized_environments_running_on_a_dedicated/</guid>
      <pubDate>Mon, 22 Apr 2024 19:05:50 GMT</pubDate>
    </item>
    <item>
      <title>动态奖励问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cag07l/dynamic_reward_problem/</link>
      <description><![CDATA[大家好！  我前几天在这里发帖，但我的问题仍然存在问题，但今天的问题完全不同。  我正在使用 DRL 方法（在本例中为 DQN）优化网络服务的位置。  但我有一个大问题：奖励函数。我正在尝试以非启发式的方式找到解决方案。因此，我决定使用为用户提供的吞吐量。  我的问题是，在研究的这些早期步骤中，我基本上试图让路由器位于 (0,0) 位置，并将用户集群置于 (1000,1000) （以米为单位测量） 。所以我的吞吐量很差，0.11 Mbps，而且要好，它至少必须到达集群附近 200 米，所以在大多数迭代中，它的奖励很低，永远不会学会到达那里。  此外，我想对位置初始化靠近集群的情况保持奖励。因此，不可能达到满足所有这些情况的奖励函数。有什么想法吗？ ​   由   提交 /u/RikoteMasterrrr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cag07l/dynamic_reward_problem/</guid>
      <pubDate>Mon, 22 Apr 2024 17:00:36 GMT</pubDate>
    </item>
    <item>
      <title>使用 rllib 训练时，episode_reward_max 始终为 0。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cafdtr/when_training_with_rllib_episode_reward_max_is/</link>
      <description><![CDATA[      我正在尝试训练一名单挑无限注德州扑克代理使用 rllib 的 PPO，环境为 PettingZoo 的 texas_holdem_no_limit。在训练期间，episode_reward_max 保持为 0，而 Episode_reward_min 保持为 -1。不过，根据我的理解，每轮结束时，玩家筹码数的变化应该作为奖励，并且这个值的最大值应该大于0。这种情况是否异常，还是我的理解不正确？ 这是训练代码的一部分： ray.init(num_gpus=8) env_name = &quot;poker&quot;; register_env(env_name, lambda _: PettingZooEnv( texas_holdem_no_limit.env() )) ModelCatalog.register_custom_model(“BaselineModel”, CNNModelV2) config = ( PPOConfig() .environment(env=env_name, Clip_actions=True,disable_env_checking=True) .rollouts (num_rollout_workers=4，rollout_fragment_length=128) .resources(num_gpus=8) .framework(framework=“torch”) .debugging(log_level=“ERROR”) .rl_module(_enable_rl_module_api=False) .training( _enable_learner_api=False, train_batch_size = 512，lr = 1e-4，gamma = 0.99，lambda_ = 0.9，use_gae = True，clip_param = 0.4，grad_clip =无，entropy_coeff = 0.1，vf_loss_coeff = 0.25，sgd_minibatch_size = 64，num_sgd_iter = 10，模型= {&quot; ;custom_model&quot;: &quot;BaselineModel&quot; } ) ) tune.Tuner( &quot;PPO&quot;, run_config=train.RunConfig( checkpoint_config=train.CheckpointConfig( checkpoint_Frequency=10, ), stop={&quot;timesteps_total&quot;: 10000000 如果不是操作系统.environ.get(&quot;CI&quot;) else 50000}, ), param_space=config, ).fit()  训练结果： https://preview.redd.it/371owm4d62wc1.png?width=2458&amp; format=png&amp;auto=webp&amp;s=06fd69d8789ca2a67b0551d50908437b34dfe785   由   提交 /u/euxcet   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cafdtr/when_training_with_rllib_episode_reward_max_is/</guid>
      <pubDate>Mon, 22 Apr 2024 16:35:50 GMT</pubDate>
    </item>
    <item>
      <title>循环 PPO（LSTM）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1caboyh/recurrent_ppo_lstm/</link>
      <description><![CDATA[大家好， 我目前正在尝试使用 LSTM 将当前的 PPO 实现“更新”为循环 PPO 实现对于我的演员和评论家网络。我目前不确定如何做到这一点，以及应该对正常的 PPO 实施进行哪些更改以纳入循环网络。只需更改网络实现中的几层即可完成工作，还是我必须做得更进一步？ 感谢有关此事的任何帮助，提前致谢！    由   提交/u/blrigo99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1caboyh/recurrent_ppo_lstm/</guid>
      <pubDate>Mon, 22 Apr 2024 14:06:42 GMT</pubDate>
    </item>
    <item>
      <title>用于知识图路径查找/排名的最先进的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ca4f03/state_of_the_art_rl_for_knowledge_graph_path/</link>
      <description><![CDATA[大家好， 我正在寻找一种最先进的 RL 算法来找到两个节点之间最有趣的路径知识图。 我发现 DeepPath 适合我的任务。但它相当“旧”（2017 年），并且仅在相当小的 KG（15.000 个节点，310.000 个三元组）上进行了测试，而我的 KG 有 680 万个节点，大约有 680 万个节点。 50.000.000 个三元组。 我的问题：RL 适合这个规模吗？如果是，您会建议哪种算法？ 提前非常感谢！   由   提交 /u/Zaaesar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ca4f03/state_of_the_art_rl_for_knowledge_graph_path/</guid>
      <pubDate>Mon, 22 Apr 2024 07:10:35 GMT</pubDate>
    </item>
    </channel>
</rss>