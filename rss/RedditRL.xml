<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sat, 20 Jan 2024 01:01:06 GMT</lastBuildDate>
    <item>
      <title>试验自定义游戏环境算法的最佳实践</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19az7f4/best_practice_for_experimenting_with_algorithms/</link>
      <description><![CDATA[我是个 RL 菜鸟。我的目标是创建一个信息不完善的多人棋盘游戏环境，并训练代理在其中玩游戏。 我应该遵循哪些最佳实践？我应该从头开始实现所有逻辑吗？我可以实现哪些库和接口以获得更连贯的体验并学习使用 RL 中使用的规范包？   由   提交/u/fool126  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19az7f4/best_practice_for_experimenting_with_algorithms/</guid>
      <pubDate>Sat, 20 Jan 2024 00:59:28 GMT</pubDate>
    </item>
    <item>
      <title>对我的项目概述的反馈</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19aokgw/feedback_on_my_project_overview/</link>
      <description><![CDATA[我正在编写项目的高级描述。我不需要深入思考技术实现，但我需要对有意义的实现有一个概述。请指导我应该添加、修改或更改哪些内容。  我也不知道哪种学习方法最适合这里。  这是我到目前为止所写的内容： \section{强化学习} \subsection{概述} 强化学习从根本上围绕代理通过接收以下形式的反馈进行学习：奖励或惩罚，在环境中做出决策以实现预定目标。就我们而言，我们的目标是在整个城市战略性地放置自行车共享站。为了实现强化学习模型，我们需要从状态、动作和奖励的角度来阐述我们的问题。代表我们模型的代理将学习决策，而环境就是城市。放置站点的最佳位置的特点是人口密度高和站点之间有足够的间距等因素；这两个限制都意味着将车站放置在对公众最有利的地方。 \subsection{潜在的实现} 城市，我们的环境，被描述为由单元组成的网格地图布局。每个单元格将： \begin{enumerate} \item 为空 \item 为自行车站 \item 其他占用空间（建筑物、住宅等） \item 为道路 \end{enumerate} 单元格类型 1-3 将有一个相关值代表附近的人口密度，而单元格类型 4 表示附近是否有共享单车站点。代理的动作空间由三个离散的动作组成：放置新的自行车站、删除现有的自行车站或选择不放置。这些动作可以表示为元组（x，y，放置/删除/传递），其中x和y表示所选位置的坐标。奖励结构必须准确考虑不同的情况，才能有效地学习。根据附近位置的高人气、靠近兴趣点（企业或景点）以及靠近自行车道，放置站点会获得积极的奖励。如果站点与其他站点距离太近，则会受到处罚。每个行动都会附带一个小的负面奖励，以阻止过度不必要的行动。地图将随着模型交互而修改，模拟放置自行车站后的变化。  ​   由   提交 /u/obvslynot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19aokgw/feedback_on_my_project_overview/</guid>
      <pubDate>Fri, 19 Jan 2024 17:31:16 GMT</pubDate>
    </item>
    <item>
      <title>我想知道是否有一个考虑时间维度的策略/价值函数？例如，在时间 t 处于状态 s 的价值</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19amupa/i_am_wondering_if_there_is_a_policyvalue_function/</link>
      <description><![CDATA[ 由   提交/u/Imo-Ad-6158   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19amupa/i_am_wondering_if_there_is_a_policyvalue_function/</guid>
      <pubDate>Fri, 19 Jan 2024 16:20:40 GMT</pubDate>
    </item>
    <item>
      <title>“受行为塑造启发的课程学习训练神经网络采用类似动物的决策策略”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19am4sp/curriculum_learning_inspired_by_behavioral/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2024.01.12.575461 摘要：  经常性神经网络 (RNN) 在神经科学中广泛使用，用于捕获神经动力学和生命系统的行为。然而，当涉及复杂的认知任务时，传统的 RNN 训练方法可能无法捕捉动物行为的关键方面。为了应对这一挑战，我们利用了实验神经科学家工具包中常用的（尽管很少受到赞赏）的方法：行为塑造。以之前在老鼠身上研究的时间投注任务为目标，我们设计了一个包含更简单认知任务的预训练课程，这是良好执行该任务的先决条件。这些预训练任务不是时间投注任务的简化版本，而是反映了相关的子计算。我们表明，这种方法对于 RNN 来说是采取与大鼠类似的策略所必需的，包括对潜在状态进行长时间尺度的推断，而传统的预训练方法无法捕获这些策略。从机制上讲，我们的预训练支持实现推理和基于价值的决策所需的关键动力系统功能的开发。总的来说，我们的方法通过结合动物的归纳偏差来解决神经网络模型训练中的差距，这在对依赖于从过去经验获得的计算能力进行复杂行为建模时非常重要。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19am4sp/curriculum_learning_inspired_by_behavioral/</guid>
      <pubDate>Fri, 19 Jan 2024 15:50:45 GMT</pubDate>
    </item>
    <item>
      <title>本文如何将两项政策结合起来？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</link>
      <description><![CDATA[      我正在阅读这篇论文，我对算法 1 中的一些细节有点迷失 -  ​ ​ https://preview.redd.it/ f0hhmaye8edc1.png?width=845&amp;format=png&amp;auto=webp&amp;s=a99793f4a5bc099c1a165145aeb44e4db441d95c 在第 3 行中，它们似乎通过 $h 组合了 $h$ 和 $f$ (s) = \pi(s) + f(s)$。我不明白这是怎么发生的。  他们在本节中称 $h$ 为混合策略，但我不明白 -  ​ https://preview.redd.it/4flfgn1x8edc1.png?width=851&amp;format =png&amp;auto=webp&amp;s=a2108806ba92d46e9afcd7668dc9c78ee58120e9 ​ 如果需要任何说明，请告诉我。    由   提交/u/Academic-Rent7800   reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19aietu/how_is_this_paper_combining_two_policies/</guid>
      <pubDate>Fri, 19 Jan 2024 12:52:43 GMT</pubDate>
    </item>
    <item>
      <title>[需要建议/反馈]第2部分：决斗双倍DQN奖励大多在0和2之间波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19af4ag/need_advicefeedback_part_2_dueling_double_dqn/</guid>
      <pubDate>Fri, 19 Jan 2024 09:19:09 GMT</pubDate>
    </item>
    <item>
      <title>RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a9nli/rl/</link>
      <description><![CDATA[大家好 我想你会发现这篇文章很愚蠢。但我没有NN专业背景。我了解反向传播的基础知识以及有关神经网络的其他简单知识，我编写简单的神经网络进行函数预测。我在基本级别上也了解 RL 应该如何工作。为了好玩，我尝试使用 chatGPT 与 RL 代理创建基本游戏，该游戏使用输入作为整个屏幕。 我的游戏是两个 2d 玩家（圆圈）可以旋转、前进并发射射弹。寻找良好的奖励函数有点困难。因为如果我写下仅通过命中和惩罚来获得奖励，当射弹击中我时，我会错过特工所做的很多动作，而且我不知道如何奖励它。我想我错过了一些重要的事情。也许我应该记录几个动作并将它们奖励为 1 个动作。当射弹击中我时的惩罚是过去几次行动的结果，所以仅仅惩罚最后一个行动看起来是不正确的。  我的项目中有3个文件，如果你愿意的话可以查看ofc。但一般建议也会非常有帮助。  main.py（主游戏循环逻辑） https://pastebin.com/cLm8DC5g&lt; /p&gt; ​ DRLAgent.py。给我写信的逻辑是ChatGPT。我理解代码的概念，但我不明白什么对我的任务有好处，什么对我的任务没有好处。 https://pastebin.com /ST3Pfkk8 ​ DQNCNN.py。也是由 chatGPT 编写的。我完全不明白这段代码。我只知道这是我的屏幕输入的调用层，但我不知道转换层是如何工作的。 https://pastebin.com/ RXfB0meN ​ 最后一个愚蠢的问题。我有两个由同一个代理控制的玩家，我给他们屏幕框架和两个额外的功能，具体取决于哪个玩家 RL 尝试预测更好的动作 [1.0, 0.0] 或 [0.0, 1.0]。我希望他能找到这 2 个功能和屏幕上的颜色之间的依赖关系（因为 2 个玩家有不同的颜色），但我也认为这是一个糟糕的解决方案。 我应该给 RL 额外的信息，比如屏幕上的玩家位置和玩家回转？  非常感谢您的关注。 ​ ​ ​ ​   由   提交/u/Specialist_Soup_4994   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a9nli/rl/</guid>
      <pubDate>Fri, 19 Jan 2024 03:42:26 GMT</pubDate>
    </item>
    <item>
      <title>专门在密室中培训 MiniGrid 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a7mfw/training_minigrid_agents_specifically_the/</link>
      <description><![CDATA[大家好， 我目前正在尝试专门解决 MiniGrid 环境： https://minigrid.farama.org/environments/minigrid/LockedRoomEnv/  这个环境非常稀疏，我一直在尝试用 PPO 解决这个问题，尝试了不同的网络和超参数调整，但没有成功。 是否有人已经解决了这个问题或者知道如何解决它？ p&gt; 另外我想知道是否有学习实用深度强化学习的资源（理论资源很多，但实用性不高）。 谢谢！   由   提交/u/Key_Lie_7975   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a7mfw/training_minigrid_agents_specifically_the/</guid>
      <pubDate>Fri, 19 Jan 2024 02:01:49 GMT</pubDate>
    </item>
    <item>
      <title>MARL 逐帧持续学习（格斗游戏研究）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/19a19ck/frame_by_frame_continuous_learning_for_marl/</link>
      <description><![CDATA[你好！ ​ 我和我的朋友正在研究如何使用 MARL格斗游戏的上下文，其中演员/代理同时提交输入，然后由格斗游戏物理引擎解析。有许多论文在格斗游戏的背景下讨论 DL / RL / 一些 MARL，但值得注意的是，它们不包含源代码或实际上谈论其方法，而是谈论普遍的发现/见解。 ​ 现在我们正在考虑使用 Pytorch（在 CUDA 上运行以提高训练速度）和 Petting Zoo（MARL 体育馆的扩展），特别是使用 AgileRL 库进行超参数优化。我们很清楚，超参数如此之多，当我们试图改进问题时，知道要改变什么是很棘手的。我们设想，我们有 8 个左右的研究游戏引擎实例（我有 10 个核心 CPU）连接到 10 个宠物动物园（可能是敏捷 RL 修改版）训练环境的实例，其中输入/输出在引擎和训练环境，来回。 ​ 我想我是在寻求一些关于我们正在使用的工具的一般建议/提示和反馈。如果您知道解决类似问题的特定教科书、GitHub 存储库的研究论文，那可能会非常有帮助。我们有一些关于超参数优化的资源以及一些关于如何摆弄设置的想法，但是仅仅为了进行人工智能学习而设计的项目初始结构/启动代码有点棘手。我们确实有一个 MARL 工作的 Connect 4 训练示例，由 AgileRL 提供。但我们正在寻求将其从轮流输入提交调整为同时输入提交（这当然是可能的，MARL 用于 MOBA 等实时游戏中）。 ​ ​ p&gt; 您可以提供给我们的任何信息都是一种祝福并且很有帮助。非常感谢您抽出时间。    由   提交/u/stardoge42  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/19a19ck/frame_by_frame_continuous_learning_for_marl/</guid>
      <pubDate>Thu, 18 Jan 2024 21:22:30 GMT</pubDate>
    </item>
    <item>
      <title>TMRL 和 vgamepad 现在可以在 Windows 和 Linux 上运行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199ujyt/tmrl_and_vgamepad_now_work_on_both_windows_and/</link>
      <description><![CDATA[亲爱的社区，您好， 你们中的一些人要求我使这些库与 Linux 兼容，并在我们的帮助下我们刚刚做了伟大的贡献者。 对于那些不熟悉的人，tmrl 是一个开源项目面向机器人专家的强化学习框架，因为它支持对数据管道的实时控制和细粒度控制，在自动驾驶社区中因其在 TrackMania2020 视频游戏中基于视觉的管道而闻名。另一方面，vgamepad 是支持此应用程序中游戏手柄模拟的开源库，它可以模拟 Xbox 360 和 PS4 Python 中的游戏手柄适合您的应用程序。 Linux 支持刚刚推出，我真的很想找到测试人员和新的贡献者来改进它，特别是对于“vgamepad”，它并不支持 Windows 版本的所有功能在 Linux 中还没有。如果您有兴趣贡献...请加入:)   由   提交 /u/yannbouteiller   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199ujyt/tmrl_and_vgamepad_now_work_on_both_windows_and/</guid>
      <pubDate>Thu, 18 Jan 2024 16:48:54 GMT</pubDate>
    </item>
    <item>
      <title>如果你是 DL 研究人员，自学深度 RL 会不会很困难？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199ik9w/is_it_hard_to_selfstudy_deep_rl_if_youre_a/</link>
      <description><![CDATA[作为一名法学硕士研究员，自学 Deep-RL 很难吗？我希望能够理解来自 DeepMind 的 RL 论文，但它似乎与常规 ML 非常不同，以至于我在自学这方面遇到困难。   由   提交/u/DoubleAd9650   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199ik9w/is_it_hard_to_selfstudy_deep_rl_if_youre_a/</guid>
      <pubDate>Thu, 18 Jan 2024 05:25:32 GMT</pubDate>
    </item>
    <item>
      <title>“通过离散扩散学习自动驾驶的无监督世界模型”，Zhang 等人 2023（MAE 规划）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/199awka/learning_unsupervised_world_models_for_autonomous/</guid>
      <pubDate>Wed, 17 Jan 2024 23:15:49 GMT</pubDate>
    </item>
    <item>
      <title>在决斗深度 Q 网络 (DQN) 训练中更改张量维度</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</link>
      <description><![CDATA[我目前正在使用 PyTorch 实现决斗深度 Q 网络 (DQN)，以在 Gym 的 Ms. Pacman 环境中训练代理。训练似乎开始顺利，但经过几集（特别是大约 100 集之后），我意识到模型的输入张量的尺寸开始发生变化，导致训练不稳定。 I&#39; m 使用包含 ObservationBuffer、ExperienceBuffer、FrameSkippingAgent、DuelingDQN 和 Agent 等类的代码结构。该模型是使用 Double DQN 方法进行训练的，我很难在张量维度变化中识别此问题的根源。 一些重要的观察结果：我正在使用 GPU (CUDA) 来加速训练。 Dueling DQN 模型的输入观察维度一开始是正确的，但在超过 200 个回合后开始发生变化 问题：训练期间张量维度的这些变化可能是什么原因导致的？ 预先感谢您提供任何可能有助于解决此问题的指导或建议。如果需要，我很乐意提供更多详细信息。 完整代码：https://stackoverflow.com/questions/77830358/changing-tensor-dimensions-in-dueling-deep-q-network-dqn-training 第 481 集，总奖励：300.0 第 482 集，总奖励：770.0 第 483 集，总奖励：210.0 第 484 集，总奖励：200.0 第 485 集，总奖励：280.0 运行时错误：给定组=1，大小权重 [ 64, 4, 8, 8]，预期输入[1, 84, 84, 1]有4个通道，但得到了84个通道   由   提交 /u/sigma_ks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1997oox/changing_tensor_dimensions_in_dueling_deep/</guid>
      <pubDate>Wed, 17 Jan 2024 21:03:19 GMT</pubDate>
    </item>
    <item>
      <title>关于动作分支论文的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</link>
      <description><![CDATA[大家好， 我正在尝试改编 这篇论文适合我的应用，但在实现过程中，论文的方法有些不清楚。 ​ 在第 4 页上，方程 5 和 6 讨论了如何将 d 目标值（每个分支 1 个目标值）减少到一个目标 y。  ​ 但是，方程 7 将损失描述为 Q 值与目标之间的平方差之和，分别针对所有分支！  ​ 那么，他们是否将目标聚合为一个目标值，从而形成 y - Q(s, a) 形式的损失方程，或者不聚合？我试图深入研究代码，但这并没有让我变得更明智。  ​ 如果您理解这一点，请告诉我，这将会有巨大的帮助！  &amp; #32；由   提交 /u/Abilitytofart   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1991wvq/question_about_the_action_branching_paper/</guid>
      <pubDate>Wed, 17 Jan 2024 17:16:34 GMT</pubDate>
    </item>
    <item>
      <title>分析强化学习泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /泛化强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/198t0tw/analyzing_reinforcement_learning_generalization/</guid>
      <pubDate>Wed, 17 Jan 2024 09:54:16 GMT</pubDate>
    </item>
    </channel>
</rss>