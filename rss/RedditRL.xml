<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 09 Jan 2025 03:37:48 GMT</lastBuildDate>
    <item>
      <title>当 epsilon 达到最小值时，CleanRL 中的损失停止减少。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwpmog/loss_stops_decreasing_in_cleanrl_when_epsilon/</link>
      <description><![CDATA[嗨， 我正在使用 CleanRL 的 DQN。我对所看到的内容有点困惑，并且不太了解其中的奥秘。 附件是我运行 10M 步的损失图表。随着 epsilon 在 5M 步达到最小值 (0.05)，损失停止减少并趋于平稳。 损失图 我发现有趣的是，这在任何数量的步骤（50k、100k、1M、5M、10M）中都是持久的。 我知道 epsilon 何时达到最小探索停止点。那么，损失是否严格来说只是因为代理不再真正探索，而是在 95% 的时间内执行最佳操作而趋于平稳？ 任何阅读或建议都将不胜感激。    提交人    /u/chysallis   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwpmog/loss_stops_decreasing_in_cleanrl_when_epsilon/</guid>
      <pubDate>Wed, 08 Jan 2025 17:26:51 GMT</pubDate>
    </item>
    <item>
      <title>ROCm (amd) 上的 pytorch？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwkp00/pytorch_on_rocm_amd/</link>
      <description><![CDATA[我使用的是 Linux，Nvidia 很麻烦。我正在考虑换回 AMD GPU，而且我已经看到了 ROCm。由于我只将 Pytorch 的东西（例如 Unity 中的 ml-agents）用作业余爱好，因此性能差异可能并不明显？ 有什么经验可以分享吗？    提交人    /u/sendbootypics_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwkp00/pytorch_on_rocm_amd/</guid>
      <pubDate>Wed, 08 Jan 2025 13:51:38 GMT</pubDate>
    </item>
    <item>
      <title>关于如何克服自我游戏 RL 中的推理速度瓶颈，有什么建议吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwj10e/any_advice_on_how_to_overcome_the_inferencespeed/</link>
      <description><![CDATA[大家好！ 我一直在为棋盘游戏开发一个 MCTS 风格的 RL 项目，这是一个业余项目。没有什么特别的，类似于 alpha zero。使用网络进行树搜索，该网络将获取当前状态并输出价值判断和下一个可能动作的先验分布。 我的问题是，考虑到连续运行推理步骤的成本，我不明白如何在自我游戏中生成足够多的游戏。具体来说，假设我想每一步查看大约 1000 个位置。相当适中……但对于玩游戏的单个代理来说，这仍然是连续 1000 个推理步骤。使用合理大小的模型，比如像样的 resnet 大小，以及良好的 GPU，我估计我每秒可以获得大约 200 个状态评估。所以单个动作需要 1000/200 = 5 秒？？然后假设我的游戏平均持续 50 步。我们称其为自玩游戏的整整 5 分钟。真扫兴。 如果我想要游戏多样性，并且每个训练周期的重播缓冲区长度合理，比如 5000 场游戏，并且我可以并行运行代理，那么我可以同时运行 100 个代理，并批量处理到 GPU（这是乐观的 - 我在这方面很糟糕），这可以连续进行 50 场游戏，所以 250 分钟 = 4 小时，对于单代游戏。我需要其中几代才能让我的网络学到任何东西…… 我是否遗漏了什么，或者这个问题的解决方案仅仅是“更多资源，一切并行”，以便从自游戏中生成足够的样本？我是否在上述近似值中犯了一些严重错误？任何帮助或建议都非常感谢！   由    /u/Fd46692  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwj10e/any_advice_on_how_to_overcome_the_inferencespeed/</guid>
      <pubDate>Wed, 08 Jan 2025 12:24:58 GMT</pubDate>
    </item>
    <item>
      <title>建立强化学习直觉的最佳统计和概率书籍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwdbsr/best_statistics_and_probability_books_for/</link>
      <description><![CDATA[我是数学专业的。所以数学不是问题。Python 也不错。我只是需要对统计数据更加直观，并且如果需要任何高级概念，则需要特别关注 RL 的概率。请推荐一些好书。    提交人    /u/brahmawadi   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwdbsr/best_statistics_and_probability_books_for/</guid>
      <pubDate>Wed, 08 Jan 2025 06:19:48 GMT</pubDate>
    </item>
    <item>
      <title>使用 Q 学习制作无与伦比的井字游戏 AI 的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwcnrk/problem_with_making_unbeatable_tictactoe_ai_using/</link>
      <description><![CDATA[我正在尝试使用 q-learning 制作井字游戏 ai。但是，它根本不是不可战胜的。我试图在阻挡时给它更多的奖励，但它仍然没有阻挡对手。我真的不知道我哪里写错了代码。 下面的链接是我在 Google Colab 中的项目的链接。你可能会注意到我使用了 ChatGPT 的一些帮助，但我认为我真的清楚地理解了它们 Google Colab 链接 非常感谢。    提交人    /u/FrostFireThunderGlow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwcnrk/problem_with_making_unbeatable_tictactoe_ai_using/</guid>
      <pubDate>Wed, 08 Jan 2025 05:41:19 GMT</pubDate>
    </item>
    <item>
      <title>从课程到实施</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwbyjq/from_courses_to_implementation/</link>
      <description><![CDATA[我是 RL 新手，希望将我的职业转向 RL，一直在学习理解数学和建立直觉的东西，但无法转向实际模拟。还想知道是否有一个真正好的课程可以学习深度 RL 方法和基于 mujoco 的基本机器人实现，我可以在学习主题后继续学习。到目前为止，我已经了解了大部分基础知识，直到 q 学习。  任何帮助都将不胜感激。    提交人    /u/Awkward_Bid7858   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwbyjq/from_courses_to_implementation/</guid>
      <pubDate>Wed, 08 Jan 2025 04:59:59 GMT</pubDate>
    </item>
    <item>
      <title>使用连续 PPO 进行重新缩放操作时，裁剪与压缩 tanh 有何区别？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwau8q/clipping_vs_squashed_tanh_for_rescaling_actions/</link>
      <description><![CDATA[当我们有连续 PPO 时，它通常会从具有无界均值和标准差的高斯中采样动作。我已经看到 tanh 激活通常用于网络的中间激活，以便这些均值等不会太失控。 但是，当我实际从这个高斯中采样动作时，它们不在我的环境限制内（0 到 1）。确保从高斯中采样的动作最终在我的环境限制内的最佳方法是什么？在初始化我的高斯分布之前，最好在均值上添加一个 tanh 层，然后重新调整从该分布中采样的动作？还是直接剪辑高斯的原始输出使其介于 0 和 1 之间更好？    提交人    /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwau8q/clipping_vs_squashed_tanh_for_rescaling_actions/</guid>
      <pubDate>Wed, 08 Jan 2025 03:57:05 GMT</pubDate>
    </item>
    <item>
      <title>RLHF PPO 训练的更密集奖励</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hwadqv/denser_reward_for_rlhf_ppo_training/</link>
      <description><![CDATA[我很高兴与大家分享我们最近的成果“分割文本并学习其奖励以改进语言模型中的 RLHF”！ 在本文中，我们研究了 RLHF PPO 训练中动作空间的粒度，仅假设二元偏好标签。我们的建议是为每个语义完整的文本段分配奖励，而不是每个标记（可能过于精细）或强盗奖励（稀疏）。我们进一步设计了技术来确保在更密集的{segment, token}级奖励下 RLHF PPO 训练的有效性和稳定性。 我们的 Segment 级 RLHF PPO 及其 Token 级 PPO 变体在各种主干 LLM 下的 AlpacaEval 2、Arena-Hard 和 MT-Bench 基准测试中优于 bandit PPO。  论文：https://arxiv.org/pdf/2501.02790 代码：https://github.com/yinyueqin/DenseRewardRLHF-PPO RLHF 的 token 级奖励模型的前期工作：https://arxiv.org/abs/2306.00398     由   提交  /u/Leading-Contract7979   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hwadqv/denser_reward_for_rlhf_ppo_training/</guid>
      <pubDate>Wed, 08 Jan 2025 03:32:33 GMT</pubDate>
    </item>
    <item>
      <title>赛车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hw8tyf/auto_racing/</link>
      <description><![CDATA[我目前正在开展一个模仿强化学习项目，使用 DDPG 训练自动驾驶赛车代理。我使用 CarSim 进行车辆动力学模拟，因为我需要高保真物理和灵活的驾驶条件。我已经弄清楚了如何运行 CarSim 模拟并获得实时结果。 但是，我遇到了一些问题 - 当我尝试在 CarSim 中训练 DDPG 代理在我的自定义赛道上行驶时，它几乎立即失败并且似乎没有学到任何有意义的东西。我最初的猜测是任务太复杂，动作空间太大，代理无法找到好的学习方向。 为了解决这个问题，我收集了 5 组我自己的赛车数据（转向角、油门、刹车）并训练了一个神经网络来模仿我的驾驶行为。然后我尝试使用这个网络作为 DDPG 中的初始参与者模型进行进一步训练。但是，结果仍然是一样的——快速失败。 我想知道我的方法是否有缺陷。有没有人做过类似的项目，或者有更好的方法建议？非常感谢任何意见！    提交人    /u/Fun_Package_1786   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hw8tyf/auto_racing/</guid>
      <pubDate>Wed, 08 Jan 2025 02:12:30 GMT</pubDate>
    </item>
    <item>
      <title>GNN+DEEPRL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvtu9k/gnndeeprl/</link>
      <description><![CDATA[大家好，我在使用端到端架构时遇到了一些麻烦：GNN（获取嵌入）然后是 Actor Critic 架构。 与使用原始特征相比，使用 gnn 嵌入的性能确实很差。我认为这是因为我得到的初始嵌入很差。 有什么想法可以改进吗？谢谢。    提交人    /u/TeamTop4542   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvtu9k/gnndeeprl/</guid>
      <pubDate>Tue, 07 Jan 2025 15:25:56 GMT</pubDate>
    </item>
    <item>
      <title>我的 DQN 存在一些问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvqcdv/i_have_some_problems_with_my_dqn/</link>
      <description><![CDATA[      我尝试在类似国际象棋的环境中创建 DQN 代理（带有 lambda 目标），奖励总和为零。 我的params: optimizer=Adam lr=0.00005 loss=SmoothL1Loss rewards = [-1,0,+1] (loose, draw/max_game_length, win therefore) 我还使用衰减 epsilon 从 0.6 到 0.01 这是灾难性遗忘（或其他问题）的问题吗？如果是，我该如何解决？ reward_fn 或 decay_lr 能帮上忙吗？ 最近用这个参数测试： https://preview.redd.it/xcz7gkvkekbe1.png?width=1297&amp;format=png&amp;auto=webp&amp;s=95ed4c2dda38896507598b1dee4e785b34f5cdc7 平滑： https://preview.redd.it/zg7m9wurekbe1.png?width=1285&amp;format=png&amp;auto=webp&amp;s=de2def6873ac19c1017122008b2ce4e5a7ba3c1b    提交人    /u/No-Eggplant154   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvqcdv/i_have_some_problems_with_my_dqn/</guid>
      <pubDate>Tue, 07 Jan 2025 12:28:33 GMT</pubDate>
    </item>
    <item>
      <title>寻求衡量供应链管理强化学习模型效率和性能的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvolro/seeking_metrics_to_evaluate_efficiency_and/</link>
      <description><![CDATA[大家好， 我正在开发一种强化学习 (RL) 模型来帮助一家公司的自行车供应链。 RL 代理旨在通过制定战略决策来最大限度地减少生产延迟和管理相关风险，包括：  行动：  不采取任何行动：让生产自行进行，无需干预。 加快：加速组件的交付，以一定成本缩短其交付周期（例如 2 天）。 延迟生产：推迟特定自行车型号的生产以适应组件短缺或降低风险。  状态空间包括：  风险评分：基于特定于组件的风险的每个生产订单的汇总评分。 工厂产能（未来日期）：有关未来时期生产能力的信息。 采购订单：关键产品的预计到达日期组件。  奖励函数：  平衡过度延迟的惩罚与加快行动的成本，鼓励高效利用资源和及时生产。   我正在考虑使用 PPO 算法来训练代理，并且正在寻找有效的指标来衡量该 RL 模型的效率和整体性能。具体来说，我想评估代理在供应链模拟中管理延迟和降低风险的能力。 问题：  在这种情况下，您会推荐哪些指标来评估 RL 代理的效率？ 如何有效衡量代理在最小化延迟和管理风险方面的决策总体表现和成功率？ 在供应链 RL 应用中，是否有我应该考虑的最佳实践或标准评估方法？  如能提供任何建议、见解或相关文献参考，我们将不胜感激！ 提前感谢您的帮助！    提交人    /u/Euphoric-You-8437   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvolro/seeking_metrics_to_evaluate_efficiency_and/</guid>
      <pubDate>Tue, 07 Jan 2025 10:32:55 GMT</pubDate>
    </item>
    <item>
      <title>多人回合制 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hvge53/multiplayer_turn_based_rl/</link>
      <description><![CDATA[我正在开发一个可以玩 Hansa Teutonica（3-5 人游戏）的 AI。 游戏逻辑很复杂，而且已经接近完成，但我无法理解为游戏结束时分配奖励。 在游戏中，游戏有 3 种结束方式，并且只能在一个人的回合中结束。 从理论上讲，游戏中的某些动作可能会导致僵局 - 类似于国际象棋中黑白棋的骑士来回移动（忽略 3 次重复）。 我现在的写法是，如果代理执行了好的操作，则分配一个次要的+奖励。而对于中立操作（或强制操作）则分配接近 0 的奖励。确定不良行为是未来的目标。 我真正感到困惑的是分配游戏结束时的奖励。 如果活跃玩家采取行动结束游戏，并获得第一名，那么直接奖励相当多的奖励就足够了。但是如果是 5 名中的第 2 名/第 3 名呢？ 我将如何奖励其他代理？代理的最后一个动作并没有直接导致他们的最终排名。 第三名玩家可以结束游戏，而第四名玩家可能很长时间没有采取行动。 我正在使用 PyTorch，并在执行操作后分配奖励。 如果不是活跃玩家的回合，为他们的最后一个动作分配奖励似乎不正确。 游戏中的另一个小问题是，当游戏接近尾声并且轮到你时，你可以 A) 结束游戏，获得第二名，或者 B) 放弃这一回合，也许让你的对手接管你的一些积分，将你推向更糟糕的排名。 我希望这足够有意义，因为我肯定很挣扎并且需要一些指导。    提交人    /u/Gozuk99   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hvge53/multiplayer_turn_based_rl/</guid>
      <pubDate>Tue, 07 Jan 2025 01:54:32 GMT</pubDate>
    </item>
    <item>
      <title>塞尔达传说 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1huzxec/the_legend_of_zelda_rl/</link>
      <description><![CDATA[我目前正在训练一个代理来“通关”《塞尔达传说：林克的觉醒》，但我面临一个问题：我无法想出一个可以让林克通过初始房间的奖励系统。 目前，我使用的唯一正向奖励是林克获得新物品时的 +1。我正在考虑对停留在同一地方太久的情况实施负向奖励（以阻止代理在同一个房间内转圈）。 你们觉得怎么样？关于如何改进奖励系统并解决这个问题有什么想法或建议吗？    提交人    /u/SlipFrosty2342   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1huzxec/the_legend_of_zelda_rl/</guid>
      <pubDate>Mon, 06 Jan 2025 14:13:15 GMT</pubDate>
    </item>
    <item>
      <title>评论？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hue0ox/comments/</link>
      <description><![CDATA[        提交人    /u/ValueSeekerAgent   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hue0ox/comments/</guid>
      <pubDate>Sun, 05 Jan 2025 18:48:22 GMT</pubDate>
    </item>
    </channel>
</rss>