<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 26 Jan 2025 03:18:36 GMT</lastBuildDate>
    <item>
      <title>特征选择/状态抽象方法</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</link>
      <description><![CDATA[大家好，有谁知道有什么论文/作品，其中代理具有非常高维的状态空间，并且可以通过某种方式减小尺寸？有没有什么常用的方法可以为代理选择最佳特征？    提交人    /u/Plastic-Bus-7003   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ia03pl/feature_selectionstate_abstraction_methods/</guid>
      <pubDate>Sat, 25 Jan 2025 23:25:36 GMT</pubDate>
    </item>
    <item>
      <title>“DeepSeek-R1：通过强化学习激励法学硕士中的推理能力”，Guo 等人 2025 {DeepSeek}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9zeb3/deepseekr1_incentivizing_reasoning_capability_in/</guid>
      <pubDate>Sat, 25 Jan 2025 22:52:46 GMT</pubDate>
    </item>
    <item>
      <title>无法在 Collab 中安装 MARLlib</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</link>
      <description><![CDATA[我按照说明在 Collab 中安装 MARLib： https://marllib.readthedocs.io/en/latest/ conda create -n marllib python=3.8 conda activate marllib git clone cd MARLlib pip install --upgrade pip pip install -r requirements.txt # 我们推荐 gym 版本在 0.20.0~0.22.0 之间。 pip install gym&gt;=0.20.0,&lt;0.22.0 # 将补丁文件添加到 MARLlib python patch/add_patch.py​​ -yhttps://github.com/Replicable-MARL/MARLlib.git  要求安装到 ray 1.8.0，找不到该版本（我也尝试过 1.13 但找不到）。 删除版本会导致更多错误和更多不兼容性。总是显示相同的消息： 错误：subprocess-exited-with-error 当安装没有特定版本的所有内容时，调用 marl.algos.mappo 时，它会抛出： ModuleNotFoundError：没有名为“ray.rllib.agents”的模块 有人可以为我提供安装 MARLlib 的更新说明并且没有不兼容性吗？    提交人    /u/BitShifter1   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9yyun/cant_install_marllib_in_collab/</guid>
      <pubDate>Sat, 25 Jan 2025 22:33:04 GMT</pubDate>
    </item>
    <item>
      <title>文本推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</link>
      <description><![CDATA[大家好，我想知道你们有没有推荐的教科书，无论是在线的还是数字的，这些教科书可以从高层次深入到强化学习领域。就背景而言，我拥有电气硕士学位，并且做过相当多的机器学习工作，但我在强化学习方面做得最先进的是 cuda 中的批量 Q 学习。我甚至从未实现过自己的深度 Q 学习算法。希望能找到一些数学密集型的问题。主要关注机器人技术和寻路，但愿意看任何东西。    提交人    /u/Tassadon   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i9cb1n/text_recommendation/</guid>
      <pubDate>Sat, 25 Jan 2025 01:57:22 GMT</pubDate>
    </item>
    <item>
      <title>对 5090 / GTC 2025 的思考</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</link>
      <description><![CDATA[有人对培训代理的 5090 感到兴奋吗？有什么特别的理由吗？ 此外，如果有人要去，廉价的边疆航班将让我今年第二次参加 GTC。很想喝点东西。去年我玩得很开心，将在周日参加其中一次培训，然后周二离开。    提交人    /u/ParamedicFabulous345   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i96pmy/thoughts_on_5090_gtc_2025/</guid>
      <pubDate>Fri, 24 Jan 2025 21:36:59 GMT</pubDate>
    </item>
    <item>
      <title>如何确定扑克锦标赛中的最佳经纪人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</link>
      <description><![CDATA[我目前正在开展一个项目，该项目旨在确定哪种深度强化学习算法最适合复杂环境，例如无限注德州扑克。我正在使用 Tianshou 制作代理和 PettingZoo 环境。我已经完成了项目的这一部分，现在我必须确定哪个代理是最好的。我让每个代理相互对战了 30,000 场，并收集了大量数据。 起初，我认为赢得最多筹码的玩家应该是赢家，但这并不公平，因为一名玩家在与最弱的玩家之一的比赛中赢得了很多筹码，并输给了所有其他玩家，但这仍然使他成为赢得最多筹码的赢家。然后我考虑了 ELO 评级，但这也行不通，因为如果玩家赢得的钱很少，那么获胜就不重要了。 在其他游戏中最常用的 2 种情况的组合是 chip_won_by_A / (chips_won_by_A + chip_won_by_B)，但这种组合也行不通，因为这是一个零和游戏环境，chips_won_by_A = -chips_won_by_B，结果除以零。你对这类问题还有其他解决方案吗？我认为使用他们本可以赢得的筹码数量中赢得的筹码百分比可能是个好主意？欢迎提供任何帮助！    提交人    /u/komensalizam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i915uw/how_to_determine_the_best_agent_in_a_poker/</guid>
      <pubDate>Fri, 24 Jan 2025 17:42:05 GMT</pubDate>
    </item>
    <item>
      <title>策略迭代中的策略评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</link>
      <description><![CDATA[      在 Sutton 的书中，策略评估 (4.5) 是 pi(s,a) * q(s,a) 的总和。但是，当我们在策略迭代过程中使用策略评估时（图 4.3），为什么我们不需要对所有动作进行求和，而只需要对 pi(s) 进行评估呢？ https://preview.redd.it/5vo75evilyee1.png?width=1030&amp;format=png&amp;auto=webp&amp;s=77af1304d549008b8c2e24c9cd8dff034519acae    submitted by    /u/lalalagay   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xtnv/policy_evaluation_in_policy_iteration/</guid>
      <pubDate>Fri, 24 Jan 2025 15:22:14 GMT</pubDate>
    </item>
    <item>
      <title>仍然不太漂亮，但奖励函数略好一些</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</link>
      <description><![CDATA[       由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8xns0/still_not_pretty_but_slightly_better_reward/</guid>
      <pubDate>Fri, 24 Jan 2025 15:15:20 GMT</pubDate>
    </item>
    <item>
      <title>帮助 Shadow Dextrous 的手在 pybullet 中抓取 3D 杯子模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</link>
      <description><![CDATA[你好。我正在尝试使用 PyBullet 来模拟假手抓握。我使用影子手 urdf 作为我的手，一个杯子的 3d 模型。我正在努力实现影子手抓取杯子。 我希望最终使用强化学习来优化对不同尺寸杯子的抓取，但我需要先运行没有任何 AI 的 Python 脚本，这样我才能有一个基线来与 RL 模型进行比较。有人知道可以帮助我的资源吗？提前谢谢了。    提交人    /u/Flamesword200   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8un2w/help_with_shadow_dextrous_hand_grabbing_a_3d_cup/</guid>
      <pubDate>Fri, 24 Jan 2025 12:50:18 GMT</pubDate>
    </item>
    <item>
      <title>关于强盗贪婪策略的新手问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8n57l/noob_question_about_greedy_strategy_on_bandits/</link>
      <description><![CDATA[考虑 10 臂老虎机问题，从每个动作的初始奖励估计 0 开始。假设代理尝试的第一个动作的奖励为正。该动作的平均奖励的真实值也为正。还假设此特定动作的奖励的“正态分布”几乎完全为正（因此，从此动作获得负奖励的可能性非常低）。 贪婪策略是否会探索任何其他动作？    提交人    /u/datashri   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8n57l/noob_question_about_greedy_strategy_on_bandits/</guid>
      <pubDate>Fri, 24 Jan 2025 04:20:02 GMT</pubDate>
    </item>
    <item>
      <title>为什么要对卷出缓冲区数据进行随机排序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</link>
      <description><![CDATA[在 SB3 的循环缓冲区文件 (https://github.com/Stable-Baselines-Team/stable-baselines3-contrib/blob/master/sb3_contrib/common/recurrent/buffers.py) 中，第 182 行表示要在保留序列的同时对数据进行混洗，代码会在随机点拆分数据，交换每个拆分，然后将其重新连接在一起。  我的问题是，为什么这对于混洗来说已经足够好了，但我们为什么要首先对推出的数据进行混洗呢？   由    /u/AUser213  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8he0u/why_shuffle_rollout_buffer_data/</guid>
      <pubDate>Thu, 23 Jan 2025 23:31:30 GMT</pubDate>
    </item>
    <item>
      <title>IsaacSim 人形机器人</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</link>
      <description><![CDATA[我需要一些帮助在 IsaacSim 中构建人形机器人演示，但除了开箱即用的人形机器人 (H1) 之外没有其他可用的东西，有人对 Neo、Sanctuary 等机器人的人形机器人政策有任何线索吗    提交人    /u/sohaib_01   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i8ec1e/isaacsim_humanoids/</guid>
      <pubDate>Thu, 23 Jan 2025 21:18:29 GMT</pubDate>
    </item>
    <item>
      <title>关于井字游戏中的贝尔曼方程。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i86uq1/about_bellman_equation_in_tic_tac_toe_game/</link>
      <description><![CDATA[一般来说，贝尔曼方程是 target_Q = Q(state, action) + gamma * Q(next_state, action) 但是，我很好奇我们是否应该使用 -gamma 而不是 gamma，因为下一个玩家是对手。如果我们添加其最大 q 值，我认为这没有意义，因为我们将对手的最大 q 值添加到此回合的 q 值中。  但我在互联网上找到了很多代码，他们会使用 target_Q = Q(state, action) + gamma * Q(next_state, action) 而不是 target_Q = Q(state, action) - gamma * Q(next_state, action)。为什么？    提交人    /u/Upstairs-Lead-2601   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i86uq1/about_bellman_equation_in_tic_tac_toe_game/</guid>
      <pubDate>Thu, 23 Jan 2025 16:09:51 GMT</pubDate>
    </item>
    <item>
      <title>需要无人机模拟环境方面的帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i820dg/need_some_help_with_simulation_environments_for/</link>
      <description><![CDATA[大家好，我目前正在模拟基于视觉的 SLAM 设置，用于在 GPS 拒绝环境中模拟 UAV。这意味着我计划使用仅接受两个传感器输入的 SLAM 算法：相机和 IMU。我需要帮助为这个项目选择正确的模拟环境。环境必须具有适用于相机和 IMU 的良好传感器模型，并且 3D 世界必须尽可能接近现实。我排除了带有 UE4 设置的 Airsim，因为 Microsoft 已存档 Airsim，并且不支持 UE5。当我尝试 UE4 时，我找不到要导入的 3D 世界，因为 UE 已升级其市场。 任何有关模拟环境的建议以及教程链接都将非常有帮助！此外，如果有人知道如何让 UE4 适用于这种应用程序，即使是这样也欢迎！    提交人    /u/techgeek1216   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i820dg/need_some_help_with_simulation_environments_for/</guid>
      <pubDate>Thu, 23 Jan 2025 12:15:38 GMT</pubDate>
    </item>
    <item>
      <title>这就是“糟糕”的奖励函数</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</link>
      <description><![CDATA[        由    /u/goncalogordo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1i7k3c9/this_is_what_a_bad_reward_function_looks_like/</guid>
      <pubDate>Wed, 22 Jan 2025 19:48:44 GMT</pubDate>
    </item>
    </channel>
</rss>