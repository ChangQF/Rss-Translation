<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•ä»¥æœ€ä½³æ–¹å¼è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Thu, 06 Feb 2025 09:17:49 GMT</lastBuildDate>
    <item>
      <title>éœ€è¦æœ‰å…³é«˜çº§ RL èµ„æºçš„å»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘å·²ç»æ·±å…¥ç ”ç©¶å¼ºåŒ–å­¦ä¹ ä¸€æ®µæ—¶é—´äº†ï¼Œä½†æˆ‘é‡åˆ°äº†ç“¶é¢ˆã€‚æˆ‘å‘ç°çš„å‡ ä¹æ¯é—¨è¯¾ç¨‹æˆ–èµ„æºéƒ½æ¶µç›–äº†ç›¸åŒçš„å†…å®¹ â€” PPOã€SACã€DDPG ç­‰ã€‚å®ƒä»¬å¯¹äºç†è§£åŸºç¡€çŸ¥è¯†éå¸¸æœ‰ç”¨ï¼Œä½†æˆ‘æ„Ÿè§‰é™·å…¥äº†å›°å¢ƒã€‚å°±å¥½åƒæˆ‘åªæ˜¯åœ¨ç›¸åŒçš„ç®—æ³•ä¸Šæ‰“è½¬ï¼Œè€Œæ²¡æœ‰çœŸæ­£å–å¾—è¿›å±•ã€‚ æˆ‘æ­£åœ¨å°è¯•å¼„æ¸…æ¥šå¦‚ä½•çªç ´è¿™ä¸€ç‚¹ï¼Œå¹¶è¿›å…¥æ›´é«˜çº§æˆ–æœ€æ–°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚è¯¸å¦‚é—æ†¾æœ€å°åŒ–ã€åŸºäºæ¨¡å‹çš„å¼ºåŒ–å­¦ä¹ ï¼Œç”šè‡³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œ HRL ä¹‹ç±»çš„ä¸œè¥¿å¬èµ·æ¥å¾ˆä»¤äººå…´å¥‹ï¼Œä½†æˆ‘ä¸çŸ¥é“ä»å“ªé‡Œå¼€å§‹ã€‚ æœ‰æ²¡æœ‰äººæœ‰è¿™ç§æ„Ÿè§‰ï¼Ÿå¦‚æœä½ å·²ç»æˆåŠŸçªç ´äº†è¿™ä¸ªç“¶é¢ˆï¼Œä½ æ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Ÿä»»ä½•è¯¾ç¨‹ã€è®ºæ–‡ç”šè‡³ä¸ªäººå»ºè®®éƒ½ä¼šéå¸¸æœ‰å¸®åŠ©ã€‚ æå‰è‡´è°¢ï¼    æäº¤äºº    /u/Helpful-Number1288   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iixqgs/need_advice_on_advanced_rl_resources/</guid>
      <pubDate>Thu, 06 Feb 2025 08:15:09 GMT</pubDate>
    </item>
    <item>
      <title>å¯¹å¼ºåŒ–å­¦ä¹ ä¸­çš„æ•°å­¦ç¬¦å·æ„Ÿåˆ°å›°æƒ‘</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iivcd2/confused_about_math_notations_in_rl/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘ä¸€ç›´åœ¨å­¦ä¹ å¼ºåŒ–å­¦ä¹ ï¼Œä½†æ˜¯æˆ‘å¯¹ä¸€äº›æ•°å­¦ç¬¦å·ï¼Œå°¤å…¶æ˜¯æœŸæœ›ç¬¦å·æ„Ÿåˆ°å¾ˆå›°æƒ‘ã€‚ä¾‹å¦‚ï¼Œä»·å€¼å‡½æ•°é€šå¸¸å†™æˆï¼š V^Ï€(s) = E_Ï€ [ R_t | s_t = s ] = E_Ï€ [ âˆ‘_{k=0}^{âˆ} Î³^k r_{t+k+1} | s_t = s ] ä¸‹æ ‡ E_Ï€ åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæˆ‘çš„ç†è§£æ˜¯ï¼Œä¸‹æ ‡åº”è¯¥è¡¨ç¤ºæ¦‚ç‡åˆ†å¸ƒæˆ–éšæœºå˜é‡ï¼Œä½†Ï€æ˜¯ä¸€ç§ç­–ç•¥ï¼ˆå‡½æ•°ï¼‰ï¼Œè€Œä¸æ˜¯é€šå¸¸æ„ä¹‰ä¸Šçš„åˆ†å¸ƒã€‚ è¿™ç§æ··æ·†ä¹Ÿå‡ºç°åœ¨è½¨è¿¹æ¦‚ç‡å®šä¹‰ä¸­ï¼Œä¾‹å¦‚ï¼š P(Ï„ | Ï€) = Ï_0(s_0) âˆ_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) Ï€(a_t | s_t) Ï€ æ˜¯ä¸€ä¸ªè¾“å‡ºåŠ¨ä½œçš„å‡½æ•°ã€‚è™½ç„¶åŠ¨ä½œæ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œä½† Ï€ æœ¬èº«ä¸æ˜¯ï¼ˆå¦‚æœæˆ‘é”™äº†ï¼Œè¯·çº æ­£æˆ‘ï¼‰ã€‚ åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼Œæƒ…å†µç”šè‡³æ›´ç³Ÿï¼ˆæ¥è‡ªhttps://spinningup.openai.com/en/latest/spinningup/rl\_intro.htmlï¼‰ V^\pi(s)=\mathbb{E}_{\tau \sim \pi}\left[R(\tau) \mid s_0=s\right] ä½œè€…åœ¨è¿™é‡Œå†™äº† $\tau \sim \pi}$ï¼Œä½†è½¨è¿¹ \tau ä¸æ˜¯ä»ç­–ç•¥ \pi ä¸­é‡‡æ ·çš„ï¼Œå› ä¸º \tau è¿˜åŒ…æ‹¬ç”±ç¯å¢ƒç”Ÿæˆçš„çŠ¶æ€ã€‚ ç±»ä¼¼åœ°ï¼Œåƒ E_Ï€ [ R(Ï„) | s_0 = s, a_0 = a ] æ„Ÿè§‰å¾ˆç›´è§‚ï¼Œä½†æˆ‘å‘ç°å®ƒä»¬åœ¨æ•°å­¦ä¸Šå¹¶ä¸ä¸¥æ ¼ï¼Œå› ä¸ºæœŸæœ›é€šå¸¸å–è‡ªæ˜ç¡®å®šä¹‰çš„æ¦‚ç‡åˆ†å¸ƒã€‚ æ›´æ–°ï¼š æˆ‘æ›´æ‹…å¿ƒçš„æ˜¯ï¼Œåƒ $E_\pi$ è¿™æ ·çš„ç¬¦å·å®é™…ä¸Šæ˜¯æ–°çš„æ•°å­¦è¿ç®—ï¼Œä¸åŒäºä¼ ç»Ÿçš„æœŸæœ›è¿ç®—ã€‚ æˆ‘çŸ¥é“å¯¹äºå¤§å¤šæ•° RL è¿™æ ·çš„ç®€å•æƒ…å†µï¼Œå®ƒä»¬ä¸å¤ªå¯èƒ½æ— æ•ˆæˆ–ä¸å®Œæ•´ã€‚ä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¯æ˜æ¥è¡¨æ˜å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚ ç”µæ°”å·¥ç¨‹å¸ˆä½¿ç”¨ Dx è¡¨ç¤º dx/dtï¼Œä½¿ç”¨ 1/Dx è¡¨ç¤º \integral x dtã€‚æˆ‘ä¸çŸ¥é“æ˜¯å¦æœ‰è¯æ®è¯æ˜è¿™ä¸€ç‚¹ï¼Œä½†å¾®åˆ†ç®—å­å…·æœ‰éå¸¸æ˜ç¡®çš„å«ä¹‰ï¼Œè€Œ E_\pi åˆ™ä»¤äººå›°æƒ‘ã€‚ ä»»ä½•è§è§£éƒ½å°†ä¸èƒœæ„Ÿæ¿€ï¼    æäº¤äºº    /u/AdministrativeCar545   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iivcd2/confused_about_math_notations_in_rl/</guid>
      <pubDate>Thu, 06 Feb 2025 05:30:02 GMT</pubDate>
    </item>
    <item>
      <title>RL ä¸é€‚ç”¨äºè¿åŠ¨æ§åˆ¶å’Œå­¦ä¹ ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</link>
      <description><![CDATA[æˆ‘æƒ³çŸ¥é“æ˜¯å¦æœ‰äººçŸ¥é“ä½¿ç”¨ RL è¿›è¡Œè¿åŠ¨å­¦ä¹ çš„ç ”ç©¶ï¼Ÿæˆ‘å¬è¯´å®ƒä»æ¥éƒ½æ— æ³•ç”¨äºå»ºæ¨¡æˆ–æ§åˆ¶ç°å®ä¸–ç•Œä¸­çš„è¿åŠ¨ã€‚è¿™æ˜¯çœŸçš„å—ï¼Ÿ    æäº¤äºº    /u/BerkeleyYears   [link] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiur88/rl_does_not_work_for_motor_control_and_learning/</guid>
      <pubDate>Thu, 06 Feb 2025 04:55:56 GMT</pubDate>
    </item>
    <item>
      <title>äººå½¢æœºå™¨äººçš„å¼ºåŒ–å­¦ä¹ æ§åˆ¶</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</link>
      <description><![CDATA[å—¨ï¼Œ æˆ‘å¯¹åŸºäº RL çš„äººå½¢æ§åˆ¶å™¨å¾ˆæ„Ÿå…´è¶£ã€‚å¦‚æœæ‚¨èƒ½åˆ—å‡ºä¸€äº›å¾ˆæ£’çš„èµ„æºä½œä¸ºèµ·ç‚¹ï¼Œæˆ‘å°†ä¸èƒœæ„Ÿæ¿€ã€‚è°¢è°¢    æäº¤äºº    /u/rua0ra1   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iiuiqf/rl_control_for_humanoids/</guid>
      <pubDate>Thu, 06 Feb 2025 04:42:30 GMT</pubDate>
    </item>
    <item>
      <title>ç®—æ³•çš„è®¾è®¡æ—¨åœ¨å°†â€œä¹è¶£â€çš„æ¦‚å¿µå……åˆ†çŒè¾“åˆ°äººå·¥æ™ºèƒ½ä¸­ã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iihtux/algorithm_designed_to_instill_the_concept_of_fun/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iihtux/algorithm_designed_to_instill_the_concept_of_fun/</guid>
      <pubDate>Wed, 05 Feb 2025 19:03:00 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ å’Œæ¨¡å‹é¢„æµ‹æ§åˆ¶è°ƒæŸ¥ 2025</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1iidp8i/reinforcement_learning_and_model_predictive/</link>
      <description><![CDATA[  ç”±    /u/tmms_  æäº¤  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1iidp8i/reinforcement_learning_and_model_predictive/</guid>
      <pubDate>Wed, 05 Feb 2025 16:15:55 GMT</pubDate>
    </item>
    <item>
      <title>â€œæ”¹è¿› Transformer ä¸–ç•Œæ¨¡å‹ä»¥å®ç°æ•°æ®é«˜æ•ˆçš„å¼ºåŒ–å­¦ä¹ â€ï¼ŒDedieu ç­‰äººã€‚ 2025å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ii9r1s/improving_transformer_world_models_for/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ii9r1s/improving_transformer_world_models_for/</guid>
      <pubDate>Wed, 05 Feb 2025 13:19:49 GMT</pubDate>
    </item>
    <item>
      <title>éœ€è¦å¸®åŠ©ï¼ï¼</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ii9oxu/need_help/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨å°è¯•åˆ›å»ºä¸€ä¸ªäººå·¥æ™ºèƒ½ï¼Œé€šè¿‡è®©å®ƒä¸åƒ stockfish è¿™æ ·è®­ç»ƒæœ‰ç´ çš„äººå·¥æ™ºèƒ½å¯¹æˆ˜æ¥å­¦ä¹ ä¸‹æ£‹ã€‚ æˆ‘è®¡åˆ’ç”¨ python æ¥åšè¿™ä¸ªï¼Œå› ä¸º python å·²ç»æœ‰äº† python-chessï¼Œä½¿ç”¨ stockfish æ›´å®¹æ˜“ æˆ‘è¿˜è®¡åˆ’åœ¨æ­¤æœŸé—´å­¦ä¹ è¿™äº›äººå·¥æ™ºèƒ½æ˜¯å¦‚ä½•å·¥ä½œçš„ å¯¹äºè®­ç»ƒéƒ¨åˆ†ï¼Œæˆ‘è®¡åˆ’ä½¿ç”¨ Stable-Baseline3ã€‚ æˆ‘å¯¹äººå·¥æ™ºèƒ½å’Œå¦‚ä½•è®­ç»ƒä»£ç†æœ‰ä¸€äº›åŸºæœ¬çš„äº†è§£ï¼Œä½†æˆ‘å·²ç»ä½¿ç”¨ ml-agents åœ¨ unity ä¸­è®­ç»ƒäº†ä»£ç†ï¼Œæ‰€ä»¥æˆ‘ä¸çŸ¥é“è¿™æœ‰å¤šéš¾ï¼Ÿ æˆ‘åº”è¯¥åšä»€ä¹ˆï¼Œæˆ‘åº”è¯¥æ€ä¹ˆåšï¼Ÿ è°¢è°¢ã€‚    æäº¤äºº    /u/kungfuaryan   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ii9oxu/need_help/</guid>
      <pubDate>Wed, 05 Feb 2025 13:16:31 GMT</pubDate>
    </item>
    <item>
      <title>æ€è€ƒæ•°æ®ï¼šæˆ‘æƒ³çŸ¥é“æˆ‘çš„æƒ³æ³•æ˜¯å¦å¯è¡Œã€‚</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</link>
      <description><![CDATA[ä½ å¥½ã€‚æˆ‘å¾ˆå¿«å°±è¦å¼€å§‹å­¦ä¹ è®¡ç®—æœºç§‘å­¦äº†ï¼ˆä»Šå¹´ç§‹å¤©æˆ–æ˜å¹´ç§‹å¤©ï¼Œå–å†³äºæˆ‘çš„å¤§å­¦ä½•æ—¶å…è®¸æˆ‘é€‰æ‹©å¹¶ä¸“æ³¨äºä¸€ä¸ªä¸“ä¸šï¼‰ï¼Œä½†æˆ‘æƒ³åœ¨äººå·¥æ™ºèƒ½æœ€è¿·äººçš„éƒ¨åˆ†ä¹‹ä¸€ï¼šå¼ºåŒ–å­¦ä¹ æ–¹é¢å–å¾—çªç ´ã€‚ æˆ‘çš„è®¡åˆ’ï¼šåˆ¶ä½œå¤šä¸ªå¯ä»¥å­¦ä¹ ç©æ¸¸æˆçš„äººå·¥æ™ºèƒ½ï¼Œç„¶åå°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ï¼Œè¿™æ ·æ„Ÿè§‰å°±åƒä¸€ä¸ªäººå·¥æ™ºèƒ½ã€‚ä½†è¿™è¿˜ä¸æ˜¯å…¨éƒ¨ã€‚é¦–å…ˆï¼Œå®ƒä¼šä»ä¸€ä¸ªæ¸¸æˆå¼€å§‹ï¼Œç„¶åæˆ‘å°†å†…å­˜å¤åˆ¶å¹¶ç²˜è´´ï¼ˆå¹¶æœ€æœ‰å¯èƒ½å¯¹å…¶è¿›è¡Œä¸€äº›ä¿®æ”¹ï¼‰åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå®ƒå°†åœ¨å…¶ä¸­ç©å¦ä¸€ä¸ªæ¸¸æˆï¼Œè¿™æ ·å®ƒå°±å¯ä»¥åœ¨å·²ç»çŸ¥é“åŸºæœ¬æ§åˆ¶çš„æƒ…å†µä¸‹å¿«é€Ÿå¯åŠ¨ã€‚ä¸€æ®µæ—¶é—´åï¼Œæˆ‘ä¼šè®©å®ƒç©æ›´é«˜çº§çš„æ¸¸æˆï¼Œå¸Œæœ›å®ƒçŸ¥é“å¤§å¤šæ•°æ¸¸æˆéƒ½æœ‰ç±»ä¼¼çš„æ§åˆ¶ç»“æ„ã€‚ æœ€ç»ˆç›®æ ‡ï¼šæ‹¥æœ‰ä¸€ä¸ªå¯ä»¥ç©å¤šä¸ªæ¸¸æˆçš„å¤šç”¨é€”äººå·¥æ™ºèƒ½ï¼Œäº†è§£æ¸¸æˆæ— éšœç¢æŒ‡å—ï¼Œç„¶ååœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­æ‹†åˆ†æ— éšœç¢å®¡æŸ¥ã€‚å“¦ï¼Œæ˜¯çš„ï¼Œä¹Ÿè®¸å¯ä»¥ä½¿ç”¨è¯­è¨€æ¨¡å‹ä¸æˆ‘èŠå¤©ã€‚ åœ¨ç†æƒ³ä¸–ç•Œä¸­ï¼Œæˆ‘ä¼šä½¿ç”¨ç°æœ‰çš„ RL ä»£ç†ï¼ˆå½“ç„¶æ˜¯åœ¨å¼€å‘äººå‘˜çš„è®¸å¯ä¸‹ï¼‰æ¥å¸®åŠ©åŠ å¿«è¯¥è¿‡ç¨‹ï¼Œå¹¶ä½¿ç”¨ LLM ä¸å…¶èŠå¤©å¹¶è·å–ä»…ç©æ¸¸æˆçš„ AI æ— æ³•æä¾›çš„ä¿¡æ¯ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘æœ‰ä¸€å° MSI GF75 Thinï¼Œé…å¤‡ Intel i5-10300hã€NVIDIA GTX 1650ï¼ˆé…å¤‡ 4gh VRAMï¼‰å’Œ 32gb Ramã€‚æˆ‘è®¤ä¸ºå¾ˆå¤šéƒ½å¾ˆå¥½ï¼Œé™¤äº†æ˜¾å¡ï¼ˆæˆ‘è§‰å¾—å³ä½¿æ²¡æœ‰å°è¯•åˆ¶ä½œ AI ä¹Ÿç¼ºä¹æ˜¾å¡ï¼‰ï¼Œæ‰€ä»¥æˆ‘æ— æ³•ç”¨æˆ‘å½“å‰çš„è®¾ç½®åšå¾ˆå¤šäº‹æƒ…ã€‚ä½†è¿™æ˜¯æˆ‘æƒ³é•¿æœŸè€ƒè™‘çš„äº‹æƒ…ï¼Œå› ä¸ºæœ‰ä¸€å¤©å°†æˆ‘çš„æƒ³æ³•ä»˜è¯¸å®è·µçœŸçš„å¾ˆé…·ã€‚    æäº¤äºº    /u/Octo_Chara   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihy2o9/data_for_thought_i_wonder_if_my_idea_is_possible/</guid>
      <pubDate>Wed, 05 Feb 2025 01:11:40 GMT</pubDate>
    </item>
    <item>
      <title>ç ”ç©¶å®ä¹ ç”Ÿ - æ¬§æ´²</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihsdnf/research_intern_europe/</link>
      <description><![CDATA[ä¸ç¡®å®šè¿™æ˜¯å¦æ˜¯ä¸€ä¸ªæ­£ç¡®çš„å­ä¸»é¢˜ï¼Œä½†æˆ‘æƒ³çŸ¥é“å¦‚ä½•æ‰èƒ½åœ¨æ¬§æ´²æ‰¾åˆ° RL çš„ç ”ç©¶å®ä¹ ç”ŸèŒä½ã€‚æœ€å¥½æ˜¯å¾·å›½ã€‚æˆ‘ä¸ç¡®å®šå¦‚ä½•æ‰èƒ½æ‰¾åˆ°è¿™æ ·çš„èŒä½ï¼Œå› ä¸ºä»–ä»¬ä¸»è¦å®£ä¼ ä¸ºåšå£«èŒä½ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚æˆ‘çš„èƒŒæ™¯å¹¶ä¸å®Œå…¨åŒ¹é…ï¼Œæ‰€ä»¥æˆ‘å®æ„¿å…ˆåšå®ä¹ ç”Ÿï¼Œç„¶åå†è½¬ä¸ºåšå£«ã€‚ä½†æˆ‘åº”è¯¥åœ¨å“ªé‡Œå¯»æ‰¾ï¼Ÿæˆ‘å¿…é¡»ç»™å®éªŒå®¤å‘å†·é‚®ä»¶å—ï¼Ÿæˆ‘å¾ˆå°‘çœ‹åˆ°ä»»ä½•å…¬å¼€å®£å¸ƒçš„èŒä½ã€‚æˆ‘å¾ˆæ„Ÿæ¿€ä»»ä½•å»ºè®®ã€‚     æäº¤äºº    /u/ProfileSad6040   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihsdnf/research_intern_europe/</guid>
      <pubDate>Tue, 04 Feb 2025 21:01:13 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨ RLã€å†³ç­–æ™ºèƒ½åº”ç”¨æ–¹é¢æœ‰æ²¡æœ‰åšå£«å­¦ä½æœºä¼šï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</link>
      <description><![CDATA[æˆ‘æ˜¯ä¸€åå¤§å››æœ¬ç§‘ç”Ÿï¼Œæƒ³ç”³è¯· RL æˆ–å†³ç­–æ™ºèƒ½åº”ç”¨é¢†åŸŸçš„ç›´æ¥æ”»è¯»åšå£«å­¦ä½æœºä¼šã€‚  è™½ç„¶æˆ‘å·²ç»ç”³è¯·äº†ä¸€äº›å¤§å­¦ï¼Œä½†æˆ‘è§‰å¾—æˆ‘çš„æœºä¼šå¾ˆå°ã€‚æˆ‘å·²ç»åæ‚”å¾ˆä¹…äº†ï¼Œå› ä¸ºæˆ‘å»å¹´æ²¡æœ‰è·Ÿè¸ªç”³è¯·æˆ–çœ‹æ¸…æœºä¼šã€‚å¦‚æœä½ ä»¬å½“ä¸­æœ‰è°å¯¹ 2025 å¹´ä»å¼€æ”¾çš„ç›´æ¥æ”»è¯»åšå£«å­¦ä½è¯¾ç¨‹æœ‰æ‰€äº†è§£ï¼Œè¯·åœ¨è¿™ä¸ª subreddit ä¸­å‘Šè¯‰æˆ‘ğŸ™    æäº¤äºº    /u/Miserable_Ad2265   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihjji2/any_phd_opportunities_in_rl_decision_intelligence/</guid>
      <pubDate>Tue, 04 Feb 2025 14:59:07 GMT</pubDate>
    </item>
    <item>
      <title>PPO é™·å…¥å±€éƒ¨æœ€ä¼˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgtec/ppo_stuck_in_local_optima/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æ­£åœ¨åšä¸€ä¸ªå¾®ç”µç½‘é—®é¢˜ï¼Œæˆ‘ä¹‹å‰ç”¨ DQN å®Œæˆäº†è¿™ä¸ªé—®é¢˜ï¼Œç»“æœè¿˜ä¸é”™ã€‚ ç°åœ¨æˆ‘æ­£åœ¨ç”¨ PPO è§£å†³ç›¸åŒçš„ç¯å¢ƒï¼Œä½†ç»“æœæ¯” DQN é—®é¢˜å·®ï¼ˆåŸºçº¿æ¨¡å‹æ˜¯ MILPï¼‰ã€‚ PPO ä»£ç†æ­£åœ¨å­¦ä¹ ï¼Œä½†è¿˜ä¸å¤Ÿå¥½ï¼Œæˆ‘æ­£åœ¨åˆ†äº«è®­ç»ƒçš„å›¾ç‰‡ã€‚ https://imgur.com/a/GHHYmow MG é—®é¢˜æ˜¯åœ¨ä¸»ç”µç½‘ä»·æ ¼ä½æ—¶å¯¹ç”µæ± å……ç”µï¼Œåœ¨ä»·æ ¼ä½æ—¶æ”¾ç”µã€‚ åŠ¨ä½œç©ºé—´æ˜¯ 4 ä¸ªç”µæ± çš„å……ç”µ/æ”¾ç”µï¼ˆæˆ‘ç¨ååœ¨ç”µæ± ä¸­å°†å…¶ä½œä¸ºæ ‡å‡†åŒ–å½¢å¼ï¼Œæˆ‘å°†ä¹˜ä»¥ 2.5ï¼Œå³æœ€å¤§ ch/dischï¼‰æˆ–è€…æˆ‘åº”è¯¥åˆå§‹åŒ– -2.5 åˆ° 2.5 å¦‚æœæœ‰å¸®åŠ©ï¼Ÿ self.action_space = space.Box(low=-1, high=1, dtype=np.float32, shape=(4,))  ä¸ºäº†å°†å…¶ä¿æŒåœ¨ -1 å’Œ 1 ä¹‹é—´ï¼Œæˆ‘æ­£åœ¨é™åˆ¶ NN çš„å¹³å‡å€¼ï¼Œç„¶ååœ¨ -1 åˆ° 1 ä¹‹é—´å¯¹æ“ä½œè¿›è¡Œé‡‡æ ·ï¼Œä»¥ç¡®ä¿ç”µæ± å……ç”µ/æ”¾ç”µä¸ä¼šè¶…å‡ºå®ƒï¼Œä½¿ç”¨ä¸‹é¢åˆ†äº«çš„è¿™ç§æ–¹å¼ã€‚ mean = torch.tanh(mean) action = dist.sample()  action = torch.clip(action, -1, 1) è¿˜æœ‰ä¸€ä»¶äº‹ï¼Œæˆ‘å¯¹ä¸‹é¢åˆ†äº«çš„ M æ­£æ€åˆ†å¸ƒä½¿ç”¨å›ºå®šåæ–¹å·®ï¼Œæ‰€æœ‰æ“ä½œçš„åæ–¹å·®ä¸º 0.5ã€‚ dist = MultivariateNormal(mean, self.cov_mat) è¯·åˆ†äº«æ‚¨çš„å»ºè®®ï¼Œæˆ‘ä»¬å°†éå¸¸æ„Ÿè°¢å¹¶è€ƒè™‘ã€‚ å¦‚æœæ‚¨éœ€è¦æ›´å¤šèƒŒæ™¯ä¿¡æ¯ï¼Œè¯·è¯¢é—®ã€‚    ç”±   æäº¤  /u/Dry-Image8120   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgtec/ppo_stuck_in_local_optima/</guid>
      <pubDate>Tue, 04 Feb 2025 12:45:10 GMT</pubDate>
    </item>
    <item>
      <title>å…³äº TRPO è®ºæ–‡çš„é—®é¢˜</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨ç ”ç©¶ TRPO è®ºæ–‡ï¼Œæˆ‘æœ‰ä¸€ä¸ªå…³äºå¦‚ä½•åœ¨ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜ä¸­è®¡ç®—æ–°ç­–ç•¥çš„é—®é¢˜ï¼š https://preview.redd.it/l8fndz5ra4he1.png?width=940&amp;format=png&amp;auto=webp&amp;s=f49f53bedb23a9a6d04f6fbeaf79a643bde0052b è¿™ä¸ªæ–¹ç¨‹ç”¨äºæ›´æ–°å’Œå¯»æ‰¾æ–°ç­–ç•¥ï¼Œä½†æˆ‘æƒ³çŸ¥é“ Ï€_Î¸(a|s) æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Œå› ä¸ºå®ƒå±äºæˆ‘ä»¬è¯•å›¾ä¼˜åŒ–çš„ç­–ç•¥æœ¬èº«â€”â€”å°±åƒå…ˆæœ‰é¸¡è¿˜æ˜¯å…ˆæœ‰è›‹çš„é—®é¢˜ä¸€æ ·ã€‚ è®ºæ–‡æåˆ°ï¼Œæ ·æœ¬ç”¨äºè®¡ç®—è¿™ä¸ªè¡¨è¾¾å¼ï¼š  1. ä½¿ç”¨å•è·¯å¾„æˆ–è—¤è”“ç¨‹åºæ”¶é›†ä¸€ç»„çŠ¶æ€-åŠ¨ä½œå¯¹ä»¥åŠå®ƒä»¬çš„ Q å€¼çš„è’™ç‰¹å¡æ´›ä¼°è®¡ã€‚ 2. é€šè¿‡å¯¹æ ·æœ¬å–å¹³å‡å€¼ï¼Œæ„é€ å…¬å¼ (14) ä¸­çš„ä¼°è®¡ç›®æ ‡å’Œçº¦æŸã€‚ 3. è¿‘ä¼¼è§£å†³è¿™ä¸ªçº¦æŸä¼˜åŒ–é—®é¢˜ä»¥æ›´æ–°ç­–ç•¥çš„å‚æ•°å‘é‡ã€‚æˆ‘ä»¬ä½¿ç”¨å…±è½­æ¢¯åº¦ç®—æ³•ï¼Œç„¶åè¿›è¡Œçº¿æœç´¢ï¼Œè¿™åªæ¯”è®¡ç®—æ¢¯åº¦æœ¬èº«ç¨å¾®è´µä¸€ç‚¹ã€‚è¯¦æƒ…è¯·å‚é˜…é™„å½• Cã€‚     æäº¤äºº    /u/audi_etron   [é“¾æ¥] [è¯„è®º] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihgpb2/question_about_the_trpo_paper/</guid>
      <pubDate>Tue, 04 Feb 2025 12:38:38 GMT</pubDate>
    </item>
    <item>
      <title>åœ¨å•å°æœºå™¨ä¸Šè¿è¡Œ Ray Tune çš„å¹¶è¡Œå®éªŒ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ Ray çš„æ–°æ‰‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæµè¡Œçš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œå°¤å…¶é€‚ç”¨äº MLï¼Œæˆ‘ä¸€ç›´è‡´åŠ›äºå……åˆ†åˆ©ç”¨æˆ‘æœ‰é™çš„ä¸ªäººè®¡ç®—èµ„æºã€‚è¿™å¯èƒ½æ˜¯æˆ‘æƒ³è¦äº†è§£ Ray åŠå…¶åº“çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚å—¯ï¼Œæˆ‘ç›¸ä¿¡è®¸å¤šå­¦ç”Ÿå’Œä¸ªäººç ”ç©¶äººå‘˜éƒ½æœ‰ç›¸åŒçš„åŠ¨æœºã€‚åœ¨ä½¿ç”¨ Ray Tuneï¼ˆå…¨éƒ¨åŸºäº Pythonï¼‰è¿›è¡Œä¸€äº›å®éªŒåï¼Œæˆ‘å¼€å§‹æ„Ÿåˆ°ç–‘æƒ‘å¹¶æƒ³å¯»æ±‚å¸®åŠ©ã€‚ä»»ä½•å¸®åŠ©éƒ½å°†ä¸èƒœæ„Ÿæ¿€ï¼ ğŸ™ğŸ™ğŸ™ï¼š  Ray åœ¨å•å°æœºå™¨ä¸Šä»ç„¶æœ‰æ•ˆä¸”é«˜æ•ˆå—ï¼Ÿ æ˜¯å¦å¯ä»¥ä½¿ç”¨ Ray åœ¨å•å°æœºå™¨ä¸Šè¿è¡Œå¹¶è¡Œå®éªŒï¼ˆåœ¨æˆ‘çš„æƒ…å†µä¸‹æ˜¯ Tuneï¼‰ï¼Ÿ æˆ‘çš„è„šæœ¬æ˜¯å¦ä¸ºæ­¤ç›®çš„æ­£ç¡®è®¾ç½®ï¼Ÿ æˆ‘é—æ¼äº†ä»€ä¹ˆå—ï¼Ÿ  æ•…äº‹ï¼š* æˆ‘çš„è®¡ç®—èµ„æºéå¸¸æœ‰é™ï¼šä¸€å°é…å¤‡ 12 æ ¸ CPU å’Œ RTX 3080 Ti GPU ä»¥åŠ 12GB å†…å­˜çš„æœºå™¨ã€‚ * æˆ‘çš„ç©å…·å®éªŒæ²¡æœ‰å……åˆ†åˆ©ç”¨å¯ç”¨èµ„æºï¼šå•æ¬¡æ‰§è¡Œè€—è´¹ 11% çš„ GPU Util å’Œ 300MiB /11019MiBã€‚ * ä»ç†è®ºä¸Šè®²ï¼Œåº”è¯¥å¯ä»¥åœ¨è¿™æ ·çš„æœºå™¨ä¸ŠåŒæ—¶è¿›è¡Œ 8-9 ä¸ªè¿™æ ·çš„ç©å…·å®éªŒä¸€å°æœºå™¨ã€‚ * è‡ªç„¶è€Œç„¶ï¼Œæˆ‘æ±‚åŠ©äº Rayï¼Œå¸Œæœ›å®ƒèƒ½å¸®åŠ©ç®¡ç†å’Œè¿è¡Œå…·æœ‰ä¸åŒè¶…å‚æ•°ç»„çš„å¹¶è¡Œå®éªŒã€‚ * ä½†æ˜¯ï¼Œæ ¹æ®ä¸‹é¢çš„è„šæœ¬ï¼Œæˆ‘æ²¡æœ‰çœ‹åˆ°ä»»ä½•å¹¶è¡Œæ‰§è¡Œï¼Œå³ä½¿æˆ‘åœ¨tune.run()ä¸­è®¾ç½®äº†max_concurrent_trialsã€‚æ ¹æ®æˆ‘çš„è§‚å¯Ÿï¼Œæ‰€æœ‰å®éªŒä¼¼ä¹ä¸€ä¸ªæ¥ä¸€ä¸ªè¿è¡Œã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä¸çŸ¥é“å¦‚ä½•ä¿®å¤æˆ‘çš„ä»£ç ä»¥å®ç°é€‚å½“çš„å¹¶è¡Œæ€§ã€‚ ğŸ˜­ğŸ˜­ğŸ˜­ï¼š* ä»¥ä¸‹æ˜¯æˆ‘çš„ ray tune è„šæœ¬ï¼ˆray_experiment.pyï¼‰ ```python import os import ray from ray import tune from ray.tune import CLIReporter from ray.tune.schedulers import ASHAScheduler from Simulation import run_simulations # Ray Tune ä¸­çš„å¯è®­ç»ƒå¯¹è±¡ from utils.trial_name_generator import trial_name_generator if name == &#39;maâ€‹â€‹in&#39;: ray.init() # è°ƒè¯•æ¨¡å¼ï¼šray.init(local_mode=True) # ray.init(num_cpus=12, num_gpus=1) print(ray.available_resources()) current_dir = os.path.abspath(os.getcwd()) # å½“å‰ç›®å½•çš„ç»å¯¹è·¯å¾„params_groups = { &#39;exp_name&#39;: &#39;Ray_Tune&#39;, # æœç´¢ç©ºé—´ &#39;lr&#39;: tune.choice([1e-7, 1e-4]), &#39;simLength&#39;: tune.choice([400, 800]), } reporter = CLIReporter( metric_columns=[&quot;exp_progress&quot;, &quot;eval_episodes&quot;, &quot;best_r&quot;, &quot;current_r&quot;], print_intermediate_tables=True, ) analysis = tune.run( run_simulations, name=params_groups[&#39;exp_name&#39;], mode=&quot;max&quot;, config=params_groups, resources_per_trial={&quot;gpu&quot;: 0.25}, max_concurrent_trials=8, # scheduler=scheduler, storage_path=f&#39;{current_dir}/logs/&#39;, # ä¿å­˜æ—¥å¿—çš„ç›®å½• trial_dirname_creator=trial_name_generator, trial_name_creator=trial_name_generator, # resume=&quot;AUTO&quot; ) print(&quot;Best config:&quot;, analysis.get_best_config(metric=&quot;best_r&quot;, mode=&quot;max&quot;)) ray.shutdown()  ```    submitted by    /u/yxwmm   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihfnzn/parallel_experiments_with_ray_tune_running_on_a/</guid>
      <pubDate>Tue, 04 Feb 2025 11:35:04 GMT</pubDate>
    </item>
    <item>
      <title>â€œé€šè¿‡éšæ€§å¥–åŠ±å¼ºåŒ–è¿‡ç¨‹â€ï¼ŒCui ç­‰äºº 2025 å¹´</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</link>
      <description><![CDATA[ [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ihbepp/process_reinforcement_through_implicit_rewards/</guid>
      <pubDate>Tue, 04 Feb 2025 06:21:10 GMT</pubDate>
    </item>
    </channel>
</rss>