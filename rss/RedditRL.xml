<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 21 Feb 2024 09:15:34 GMT</lastBuildDate>
    <item>
      <title>失去动力</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw6mn4/losing_motivation/</link>
      <description><![CDATA[也许我只是反应过度，或者我太弱了。但请听我说完。在过去的几个月里，我一直在尝试自学强化学习。我正在学习 Coursera RL 专业课程的第二门课程，萨顿 &amp; 课程的第 8 章。巴托。起初我的一些朋友也想和我一起学习，但他们都不再这样做了。我还在做一份全职工作，这几乎耗尽了我所有的精力。我寻找导师，但没有找到。如今每个人都热衷于计算机视觉或 NLP。此外，很多人都说强化学习没有未来。所有这些加在一起真是太累了。 我真的不知道我在这里要找什么。如果你能分享你的旅程，那将会有帮助。另外，如果您能指导我（即使是一点点时间），我将永远感激不已。   由   提交/u/Casio991es  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw6mn4/losing_motivation/</guid>
      <pubDate>Wed, 21 Feb 2024 07:55:27 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助理解研究论文</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aw1ce3/need_help_with_understanding_a_research_paper/</link>
      <description><![CDATA[我需要帮助来理解 DeepMind 在 2020 年发表的一篇论文，该论文的名称是，“通过考虑未来的任务来避免副作用”。  我很难理解本文中讨论的几个概念。  如果有人愿意帮助我，请与我联系，提前致谢。   由   提交 /u/Donald-the-dramaduck   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aw1ce3/need_help_with_understanding_a_research_paper/</guid>
      <pubDate>Wed, 21 Feb 2024 03:06:19 GMT</pubDate>
    </item>
    <item>
      <title>训练 MuZero</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avzdmq/training_muzero/</link>
      <description><![CDATA[是否有人尝试使用此代码 - https: //pypi.org/project/muzero-baseline/ ? 我将其安装在桌面上并尝试训练它玩井字游戏 muzero = MuZero(tictactoe.Game, tictactoe.MuZeroConfig()) muzero.train() muzero .test(render=True) 运行2天后没有产生任何结果。 我注意到CPU的使用率很低，GPU的使用率也很低。  ​ ​   由   提交 /u/Competitive-Elk4319    reddit.com/r/reinforcementlearning/comments/1avzdmq/training_muzero/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avzdmq/training_muzero/</guid>
      <pubDate>Wed, 21 Feb 2024 01:34:55 GMT</pubDate>
    </item>
    <item>
      <title>强化学习和统计的交叉点在哪里？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avr3kx/where_do_rl_and_statistics_intersect/</link>
      <description><![CDATA[我是一名统计研究生，对 RL/DRL 感兴趣，我希望最大限度地从学位中获益并专注于统计主题（和数值方法）如果可能的话，这将使我为 RL/DL 的博士课程做好准备，我需要特别注意哪些主题？   由   提交 /u/al3arabcoreleone   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avr3kx/where_do_rl_and_statistics_intersect/</guid>
      <pubDate>Tue, 20 Feb 2024 19:57:12 GMT</pubDate>
    </item>
    <item>
      <title>强化学习再训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avnd8b/rl_retraining/</link>
      <description><![CDATA[我在 Unity 中有一个真实的机器人环境。我制定了一个更简单的版本（数学模型）并训练了一个 TD3 代理，结果非常好。 我直接将这个策略应用到实际环境中来查看行为，它实际上是在做好的轨迹跟踪。 然后，我使用经过训练的策略在实际环境中进一步训练和微调，实际环境稍微复杂一些。 一开始，学习看起来不错，但是3-4 集之后，该政策只是产生超级奇怪的行为（距离目标还很远）。 我不太确定我做错了什么。 我保持第二次训练中的学习率和探索噪声低于第一次训练，以便策略在更复杂的环境中顺利过渡。    ;由   提交/u/fx619  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avnd8b/rl_retraining/</guid>
      <pubDate>Tue, 20 Feb 2024 17:29:40 GMT</pubDate>
    </item>
    <item>
      <title>对于任何流行的游戏引擎，有关于两足动物训练的工作吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avktvc/any_work_around_biped_training_used_for_any_of/</link>
      <description><![CDATA[过去有 GTA5 中使用的自然运动内啡肽。然后我看到一篇 2 分钟的关于 2 Bipeds 拳击的论文，真是太棒了。 . 很多年过去了，但我找不到任何我想象中的到 2024 年会出现的东西，用于使用经过训练的两足动物模型进行游戏。我了解 Unity 的 MLAGENTS，他们有一个生物经过训练且移动不良的示例。 对于该主题还有其他资源吗？我觉得很有趣   由   提交/u/punkouter23  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avktvc/any_work_around_biped_training_used_for_any_of/</guid>
      <pubDate>Tue, 20 Feb 2024 15:49:26 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习中的观察标准化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1avks7q/observation_normalization_in_multiagent_rl/</link>
      <description><![CDATA[研究人员大家好，我有一个关于 MARL 中观察标准化的问题。当在集中训练分散执行（CTDE）环境中训练智能体时，应该如何进行观察标准化？我想到了三种方法来做到这一点：分别标准化每个代理的观察结果（在这种方法中，评论家的输入将具有不同的均值和标准差）。对一个代理的观察结果进行归一化，然后使用平均值和标准差对其他代理的观察结果进行归一化，或者对所有连接的观察结果的张量进行归一化，然后对每个相应代理的单独观察结果进行归一化。预先感谢您   由   提交 /u/Many_Reception_4921   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1avks7q/observation_normalization_in_multiagent_rl/</guid>
      <pubDate>Tue, 20 Feb 2024 15:47:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 PPO 训练山地连续车</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aviclz/training_mountaincontinuouscar_with_ppo/</link>
      <description><![CDATA[      嗨！ 我目前正在尝试使用 PPO 训练 MountrainContinouslyCar。这是我获得的平均回报的图表类型。 &lt; a href=&quot;https://preview.redd.it/1f83xrxbzqjc1.png?width=514&amp;format=png&amp;auto=webp&amp;s=28a3afb294f072506aad9a34711fa59be7cd0da5&quot;&gt;https://preview.redd.it/1f83xrxbzqjc1.png?width =514&amp;format=png&amp;auto=webp&amp;s=28a3afb294f072506aad9a34711fa59be7cd0da5 ​ 有人对如何使其工作有建议吗？ 这是我的配置顺便说一句： action_space: Continuousframe_skip: 1 truncate_env_steps_at: 200 use_truncation_as_termination: true num_envs: 1 Total_env_steps: 819200 agent_steps_per_env: 256 is_episodic: true agent_steps_per_batch: 256 Total_agent_steps: 819200 num_batches：3200 num_training_phases：3200 actor：激活：Tanh Linear_layers：num_layers：2 Layer_size：64 评论家：激活：Tanh Linear_layers：num_layers：2 Layer_size：64 minibatch_size：256 num_epochs：10 算法：Adam max_grad_norm：5 lr：7.77e-05 num_minibatches_per_epoch: 1 max_gradient_steps_per_rollout: 10 max_total_gradient_steps: 32000 gamma: 0.9999 lmbda: 0.9    由   提交 /u/hc7Loh21BptjaT79EG   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aviclz/training_mountaincontinuouscar_with_ppo/</guid>
      <pubDate>Tue, 20 Feb 2024 14:02:10 GMT</pubDate>
    </item>
    <item>
      <title>通过 PPO 进行迟滞作用，这有意义吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auwexp/hysteresis_as_action_via_ppo_does_it_make_sense/</link>
      <description><![CDATA[我有一个 PPO 代理和持续观察和行动的空间。如果某物处于正确/不正确的位置，则一个操作应该提供信息。但为了避免切换问题，位置应该是稳定值。我们可以先说约后。如果物体处于正确/不正确的位置，则需要 20 个步骤。这意味着一旦物体处于正确位置并更改为错误位置，操作应从 1 降至 -1，反之亦然。  我使用高斯函数作为奖励函数，但在执行此操作之前，我将操作转换为范围 0...1。  奖励 = 1 * math.exp(-(math.pow((action - Expected_action), 2)) / (2 * math.pow(0.3, 2))) 观察有20行历史记录： Box(low=-1, high=1, shape=(20, 6), dtype=np.float64) 我的问题是的，PPO无法解决这个问题，我尝试通过rl-baselines3-zoo优化超参数，但没有好的结果。我的意思是剧集长度约为 160 步，我达到最大奖励 120，然后甚至奖励也在下降。我已停用其他操作的奖励计算，以便仅针对滞后计算检查代理的行为和性能。  我知道，这可以通过监督学习来解决，但我还有更多的行动。  长话短说，我做错了什么？ ​    ;由   提交/u/Inevitable_Engineer5   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auwexp/hysteresis_as_action_via_ppo_does_it_make_sense/</guid>
      <pubDate>Mon, 19 Feb 2024 19:40:06 GMT</pubDate>
    </item>
    <item>
      <title>是否有与 JAX 的 vmap 兼容的实验日志框架？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auqhfb/is_there_an_experiment_logging_framework/</link>
      <description><![CDATA[大家好！随着我对 JAX 越来越熟悉，一件非常酷的事情是我可以 vmap 整个训练循环来测试多个种子： # Seeding key = jax.random。 PRNGKey(args.seed) keywords = jax.random.split(key, 5) # 编译训练函数 train_vjit = jax.jit(jax.vmap(make_train(args))) # 运行训练 train_output = jax.block_until_ready(train_vjit(keys) ))  但是，当我使用 Wandb 时，我认为我只能记录其中一次运行，因为我不确定如何为每个 vmapped 实验设置 wandb 初始化。有没有办法做到这一点，或者是否有一些替代框架支持 vmapped 训练函数并单独记录每个 vmapped 种子而不是覆盖？  &amp;# 32；由   提交 /u/1cedrake   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auqhfb/is_there_an_experiment_logging_framework/</guid>
      <pubDate>Mon, 19 Feb 2024 15:50:43 GMT</pubDate>
    </item>
    <item>
      <title>也有助于制定阶跃函数和环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auq8w5/help_in_formulating_a_step_function_and/</link>
      <description><![CDATA[我必须接受大约 100~200 的多个输入（取决于测试用例），然后将其传递到方程中，然后输出 i get 是我必须最小化的。输入都有自己的范围，并且输入的不同子集也有自己应等于的值。我需要指导如何描述环境、观察和行动空间。如果有人可以帮助我理解，必须如何实现步骤函数才能最小化它？我现在打算使用 A2C。 我不知道这是问这个问题的合适地方，但任何指导都会有所帮助。谢谢。   由   提交 /u/MysteryManav   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auq8w5/help_in_formulating_a_step_function_and/</guid>
      <pubDate>Mon, 19 Feb 2024 15:40:59 GMT</pubDate>
    </item>
    <item>
      <title>用于收集卷展的独立库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auq20n/standalone_library_for_collecting_rollouts/</link>
      <description><![CDATA[我发现自己不断地重新实现标准的 rollout 收集器循环 (env.reset(seed), env.step （政策（州）））。是否有任何库提供一个模块，该模块使用策略步进环境，并返回非嵌套的输出字典。  python results = env.rollout(policy) print(results[&#39;reward&#39;], results[&#39;action&#39;] ...)   TorchRL 中的一个很接近，但要求您的策略是 TensorDictModule，创建一堆 TorchSpecs，并且有大量依赖项。我宁愿不依赖任何东西，除了 numpy （或 jax，可以简单地转换为 numpy）。   由   提交 /u/smorad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auq20n/standalone_library_for_collecting_rollouts/</guid>
      <pubDate>Mon, 19 Feb 2024 15:33:16 GMT</pubDate>
    </item>
    <item>
      <title>Alpha Zero 适用于每回合包含两个动作的游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aumn42/alpha_zero_for_games_where_each_turn_consists_of/</link>
      <description><![CDATA[嘿， 我正在修改一个 alpha 零存储库以用于名为 野永。两名玩家各有一个由 19 个方块和 3 个令牌组成的六边形棋盘。 在这个游戏中，每一回合都包括将三个令牌之一移动到六个不同方向之一。之后，您可以通过将 19 个方块中的一个移动到另一个位置来修改实际的棋盘。 我已经实现了定义合法移动和游戏结束条件的逻辑。然而，我很难对这些动作进行编码。据我对国际象棋的理解，每个可能的 8x8x73 (=4672) 个动作都由神经网络中的输出神经元表示。 为了简单起见，我们假设 Nonaga 的最大字段大小为 8x8 (实际上它可以变得更大）。为了沿六个方向之一移动令牌之一，我考虑将它们编码为 8 x 8 x 6 (= 384) 个神经元。 对于理论上的图块，有 8 x 8 起始位置和 8 x 8 目标其中大多数位置无效，具体取决于单独导致 4096 个操作的场景。如果我现在尝试将 384 个令牌操作与 4096 个图块操作结合起来，我最终会得到大约 150 万个操作，这似乎不可行。 我怎样才能避免这种情况？我想到了神经网络的两个政策负责人。这行得通吗？提前致谢！   由   提交 /u/LuckerNo1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aumn42/alpha_zero_for_games_where_each_turn_consists_of/</guid>
      <pubDate>Mon, 19 Feb 2024 12:56:07 GMT</pubDate>
    </item>
    <item>
      <title>无限循环</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1aujcu9/infinity_loop/</link>
      <description><![CDATA[你好， 我训练了一个代理来玩类似俄罗斯方块的益智游戏。在每一步中，智能体都可以决定将棋子放置在棋盘上的可能位置或随机获得一个新棋子（6 种可能性）。我设置了奖励，以便首选具有尽可能少的部分的解决方案。尽管如此，他仍然可能达到这样一种状态：每个可能的随机块都以决定更喜欢获得新的随机块而告终。这将创造一个无限的游戏。我怎样才能避免这种行为？ 我的意思是代理训练有素，因此不会在每场比赛中都会发生，但如果发生会发生什么？我不能接受一个永无止境的游戏，游戏在解决之前不可能停止。总有一个可能的解决方案，因为即使是 1x1 大小的碎片也存在。 示例：https ://en-wiki.metin2.gameforge.com/index.php/Fishing_Jigsaw 我会感谢每一个想法和支持。从您的经验中了解您将如何设置奖励以及您将训练多少交互也将很有趣。您会选择什么复杂度的 dqn 神经网络？   由   提交/u/Reasonable_Cry8854  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1aujcu9/infinity_loop/</guid>
      <pubDate>Mon, 19 Feb 2024 09:30:06 GMT</pubDate>
    </item>
    <item>
      <title>围棋游戏的无模型强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1auh6v7/modelfree_rl_for_game_of_go/</link>
      <description><![CDATA[是否有任何无模型强化学习算法（没有 AlphaGO 系列中的 MCTS）可以在围棋比赛中超越人类专家？  如果是这样，它的性能与基于模型的方法相比如何？    由   提交 /u/RebornHugo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1auh6v7/modelfree_rl_for_game_of_go/</guid>
      <pubDate>Mon, 19 Feb 2024 07:05:19 GMT</pubDate>
    </item>
    </channel>
</rss>