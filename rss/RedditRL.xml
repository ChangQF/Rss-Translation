<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 06 Jan 2025 15:17:33 GMT</lastBuildDate>
    <item>
      <title>塞尔达传说 RL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1huzxec/the_legend_of_zelda_rl/</link>
      <description><![CDATA[我目前正在训练一个代理来“通关”《塞尔达传说：林克的觉醒》，但我面临一个问题：我无法想出一个可以让林克通过初始房间的奖励系统。 目前，我使用的唯一正向奖励是林克获得新物品时的 +1。我正在考虑对停留在同一地方太久的情况实施负向奖励（以阻止代理在同一个房间内转圈）。 你们觉得怎么样？关于如何改进奖励系统并解决这个问题有什么想法或建议吗？    提交人    /u/SlipFrosty2342   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1huzxec/the_legend_of_zelda_rl/</guid>
      <pubDate>Mon, 06 Jan 2025 14:13:15 GMT</pubDate>
    </item>
    <item>
      <title>Github 仓库</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hue4bh/github_repo/</link>
      <description><![CDATA[抱歉，我对主题有疑问，但是有没有办法让 chatGPT 通过 github repo。     提交人    /u/Wide-Chef-7011   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hue4bh/github_repo/</guid>
      <pubDate>Sun, 05 Jan 2025 18:52:36 GMT</pubDate>
    </item>
    <item>
      <title>评论？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hue0ox/comments/</link>
      <description><![CDATA[        提交人    /u/ValueSeekerAgent   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hue0ox/comments/</guid>
      <pubDate>Sun, 05 Jan 2025 18:48:22 GMT</pubDate>
    </item>
    <item>
      <title>具有奖励（和价值）分布的分布式强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hudvfl/distributional_rl_with_reward_and_value/</link>
      <description><![CDATA[大多数分布式 RL 方法在训练值/q 值网络分布时使用标量即时奖励（尤其是：C51 和 QR 系列网络）。在这种情况下，奖励只是转移目标分布。 我很好奇是否有人遇到过任何学习即时奖励分布的工作（即随机奖励）。    提交人    /u/Losthero_12   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hudvfl/distributional_rl_with_reward_and_value/</guid>
      <pubDate>Sun, 05 Jan 2025 18:42:31 GMT</pubDate>
    </item>
    <item>
      <title>教 PPO“绘画”有困难</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hu4s6t/trouble_teaching_ppo_to_draw/</link>
      <description><![CDATA[我正尝试教神经网络在 colab 中“绘制”。这个想法是，给定一个输入画布和一个参考图像，网络需要输出两个 x 和 y 坐标以及一个 rgba 值，并在输入画布顶部绘制一个具有 rgba 颜色的矩形。画布上带有矩形，然后是新状态。然后重复该过程。 我正在使用 PPO 训练这个网络。据我所知，这是一种用于连续动作的良好 DRL 算法。 奖励是放置矩形之前和之后与参考图像相比的 mse 差异。此外，对于完全相同或非常接近的坐标，还会受到惩罚。通常，初始网络会吐出非常接近的坐标，导致绘制矩形时没有奖励。 一开始，损失似乎在下降，但过了一段时间就停滞了，我想找出我做错了什么。 我最后一次使用强化学习是在 2019 年，现在我有点生疏了。我已经订购了 Grokking DRL 书，10 天后到货。同时，我有几个问题： - PPO 是这个问题的正确算法选择吗？ - 我的 PPO 实现看起来正确吗？ - 您是否发现我的奖励函数有任何问题？ - 网络是否足够大以学习这个问题？（小得多的 CPPN 能够做得不错，但它们是符号网络） - 您认为我的网络也可以从参考图像作为输入中受益吗？即第二个 CNN 输入流，用于参考图像，我将其输出展平并将其连接到线性层的另一个输入流。    提交人    /u/matigekunst   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hu4s6t/trouble_teaching_ppo_to_draw/</guid>
      <pubDate>Sun, 05 Jan 2025 11:10:46 GMT</pubDate>
    </item>
    <item>
      <title>强化学习Flappy Bird代理失败！！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htzwke/reinforcement_learning_flappy_bird_agent_failing/</link>
      <description><![CDATA[我尝试使用 DQN 为 Flappy Bird 创建强化学习代理，但代理根本没有学习。它一直与管道和地面发生碰撞，我不知道哪里出了问题。我不确定问题出在奖励系统、神经网络还是我实现的游戏机制上。有人能帮我吗？我会分享我的 GitHub 存储库链接以供参考。 GitHub 链接    提交人    /u/uddith   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htzwke/reinforcement_learning_flappy_bird_agent_failing/</guid>
      <pubDate>Sun, 05 Jan 2025 06:50:18 GMT</pubDate>
    </item>
    <item>
      <title>训练期间目标 Q 值告诉我什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htzpk6/what_does_the_target_qvalue_tell_me_during/</link>
      <description><![CDATA[大家好， 我正在训练一个 TD3 代理，想知道目标 Q 值能告诉我关于训练的什么信息。 我知道最基本的知识，即如果我们遵循某些最佳策略，它是预期的折扣奖励。那么如果它开始收敛到某个值，然后稍微减少，然后一遍又一遍地增加（有点像在 2 个点之间反弹），它是否学到了一些次优策略？还是训练还没有完成？对于奖励稀疏的环境来说，这尤其令人困惑，那么它是否可以成为一个有用的指标，表明在训练的哪个点它会达到最优策略？我之所以问这个问题，是因为在连续 5 个左右的场景中，环境会得到解决，然后出现不利的表现。这让我想到了以下问题： 如果动作中总是添加噪音，那么目标 Q 值是否有助于告诉我噪音是否妨碍了训练？至于具体细节，我确实将噪声衰减到了 0.1，这意味着添加的随机噪声是从标准差为 0.1 的正态分布中采样的。我觉得这可能会影响一些目标 Q 值？ 我觉得这是一个开放式的问题，所以我很乐意详细说明任何事情。 非常感谢！    提交人    /u/Sea_Farmer5942   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htzpk6/what_does_the_target_qvalue_tell_me_during/</guid>
      <pubDate>Sun, 05 Jan 2025 06:37:15 GMT</pubDate>
    </item>
    <item>
      <title>“无流程标签的免费流程奖励”，Yuan 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1httwti/free_process_rewards_without_process_labels_yuan/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1httwti/free_process_rewards_without_process_labels_yuan/</guid>
      <pubDate>Sun, 05 Jan 2025 01:17:53 GMT</pubDate>
    </item>
    <item>
      <title>“Aviary：训练语言代理完成具有挑战性的科学任务”，Narayanan 等人 2024 {Futurehouse}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htsw86/aviary_training_language_agents_on_challenging/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htsw86/aviary_training_language_agents_on_challenging/</guid>
      <pubDate>Sun, 05 Jan 2025 00:28:52 GMT</pubDate>
    </item>
    <item>
      <title>需要一点帮助来验证项目想法 - gym-super-mario-bros</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htr6ve/need_a_little_help_verifying_an_idea_for_a/</link>
      <description><![CDATA[大家好！ 我正在开始一个新项目，这也是我对强化学习的介绍。我当时想创建一个学习玩超级马里奥兄弟的人工智能模型（我知道，这很有创意 :)）。但问题是，我想实现一个系统，在这个系统中，模型在一定数量的帧内不能切换他选择的动作。例如，如果他的动作是按下跳跃按钮，他必须按住跳跃按钮几帧。这个想法是，用户可以输入他的反应时间（假设为 200 毫秒），然后根据该值，我们得到他不能“改变”输入的帧数（游戏以 60 帧/1000 毫秒的速度运行，因此在这个例子中，人工智能必须坚持相同的动作至少 12 帧）。  背后的原因是我想根据用户的反应时间创建“个性化”半速通指南。然后添加覆盖层，显示“在给定时刻按下哪个按钮”。  话虽如此，我不知道使用 gym ai 是否可能实现这种事情（？）。是否有更有经验的人愿意验证我的想法是否可行？我打算在这个项目中使用 gym-super-mario-bros 7.4.0。  干杯 :)     提交人    /u/Broad-Ball-1131   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htr6ve/need_a_little_help_verifying_an_idea_for_a/</guid>
      <pubDate>Sat, 04 Jan 2025 23:10:53 GMT</pubDate>
    </item>
    <item>
      <title>梦想家架构中行动空间的变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htoxs7/changing_action_spaces_in_dreamer_architecture/</link>
      <description><![CDATA[您好 r/reinforcementlearning， 因此，我正在设计一个模型来完成某种类型的复杂工作。 本质上，我设计环境的方式涉及在不同的动作空间上工作。 我认为，为了创建不同的动作空间，我只需更改 Agent 的动作空间即可；但是我检查了代码，似乎 。空间的数量非常有限（大约 30 个不同的动作空间），但它们是不同的 - 有时它只是一个从 1 到 3 的 uint，有时它是（3 个 float32 选择、一个 bool 选择、另一个但不同的 3 个 float32 选择）；或者有时它是一个包含 127 个布尔值的向量，模型应该选择真/假。 这绝对比使用单个 action 参数更复杂。 有人处理过这个问题吗？怎么做？ 干杯。 &gt; 我担心的一件事是不同的 dtypes。从技术上讲，我可以有 3 个输出，分别为布尔值、整数和浮点数，并惩罚不必要的操作，但是……我已经将所有环境编码为静态操作，此外，我很确定在这个环境中较少的循环是好的 - 我已经完成了数千个离散步骤才能实现它。    提交人    /u/JustZed32   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htoxs7/changing_action_spaces_in_dreamer_architecture/</guid>
      <pubDate>Sat, 04 Jan 2025 21:29:13 GMT</pubDate>
    </item>
    <item>
      <title>从基于模型到无模型的强化学习：转换我的旋转倒立摆解决方案</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htma2j/from_modelbased_to_modelfree_rl_transitioning_my/</link>
      <description><![CDATA[嗨，RL 爱好者们！我最近为旋转倒立摆问题实现了一个基于模型的强化学习解决方案，现在我希望迈出无模型领域的下一步。我正在寻求有关实现这一转变的最佳方法的建议。 当前设置  问题：旋转倒立摆 方法：基于模型的 RL 状态：成功实施并正在运行  目标 我的目标是：  过渡到无模型 RL 方法 保持或提高性能 深入了解基于模型和无模型方法之间的差异  问题  对于这个特定问题，您会推荐哪种无模型算法？ （例如 DQN、DDPG、SAC） 对于旋转倒立摆，从基于模型的 RL 转向无模型 RL 时，我应该预见到哪些主要挑战？ 我应该考虑哪些特定的修改或技术来使我当前的解决方案适应无模型框架？ 如何有效地将我当前基于模型的解决方案的性能与新的无模型方法进行比较？  如果您能分享任何见解、资源或个人经历，我将不胜感激。提前感谢你的帮助！    提交人    /u/Fit-Orange5911   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htma2j/from_modelbased_to_modelfree_rl_transitioning_my/</guid>
      <pubDate>Sat, 04 Jan 2025 19:32:52 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助选择研究主题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1htdek0/need_help_picking_research_topic/</link>
      <description><![CDATA[我最近开始攻读强化学习博士学位，说实话，我有点迷茫。我应该从强化学习领域中选择一个研究问题。我真的知道如何找到研究差距以及要寻找什么或如何寻找它？我真的很感激任何帮助/指导（找到这个特定主题或研究差距的程序以及任何想法）。    提交人    /u/No-Ranger-2702   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1htdek0/need_help_picking_research_topic/</guid>
      <pubDate>Sat, 04 Jan 2025 12:29:32 GMT</pubDate>
    </item>
    <item>
      <title>截断和终止之间的区别有多重要？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ht65uu/how_important_is_the_difference_between/</link>
      <description><![CDATA[我最近一直在查看多个 RL 环境框架，并注意到许多（据我所知）环境/gym API 没有为终止和截断提供单独的标志/返回值。许多 API 只是报告“完成”或“终端” Farama 的人们已经更新了他们的 Gynasium API，以在环境 step() 函数中返回来自终止和截断的单独值。 他们在 2023 年 10 月发布的关于这一重大 API 更改的帖子似乎非常引人注目：https://farama.org/Gymnasium-Terminated-Truncated-Step-API 将终止和截断视为相同的 RL 框架列表： - brax - JaxMARL - Gymnax - jym 具有单独终止和截断值的 RL 框架列表： - Farama Gymnasium - PGX - StableBaselines3 - Jumanji 所以我的问题是，为什么没有更多的 RL 框架采用类似的能力来辨别截断和终止？终止和截断之间的区别是否没有我想象的那么重要？我感觉我错过了其他人已经弄清楚的东西。 难道当使用端到端 Jax 进行环境和训练时，大规模并行环境的速度提升会完全消除因不对终止和截断进行不同处理而导致的低效率？ 编辑：将 StableBaselines3 添加到具有单独终止 + 截断的框架列表中；至少在我从其 repo 链接的特定代码示例中。将 Jumanji 移至具有单独截断和终止的列表。    提交人    /u/aloecar   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ht65uu/how_important_is_the_difference_between/</guid>
      <pubDate>Sat, 04 Jan 2025 04:17:10 GMT</pubDate>
    </item>
    <item>
      <title>“使用计算高效传感器的战术射击类人机器人”，Justesen 等人 2025 年（Valorant / Riot Games）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ht5lwv/humanlike_bots_for_tactical_shooters_using/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ht5lwv/humanlike_bots_for_tactical_shooters_using/</guid>
      <pubDate>Sat, 04 Jan 2025 03:47:09 GMT</pubDate>
    </item>
    </channel>
</rss>