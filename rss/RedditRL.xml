<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 19 Jun 2024 06:21:24 GMT</lastBuildDate>
    <item>
      <title>将代理最后选择的离散动作（int）包含在观察空间中可以吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1djay0a/is_it_ok_to_include_agents_last_chosen_discrete/</link>
      <description><![CDATA[  由    /u/against_all_odds_  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1djay0a/is_it_ok_to_include_agents_last_chosen_discrete/</guid>
      <pubDate>Wed, 19 Jun 2024 04:52:56 GMT</pubDate>
    </item>
    <item>
      <title>DQN 损失</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dj9hha/dqn_loss/</link>
      <description><![CDATA[      嗨， 我正在使用自定义环境运行 DQN 算法。如果我每 20 集训练一次我的环境，或者我每集每一步训练我的环境，我的奖励就会收敛到相同的值。但是，我的损失看起来非常不同。您能告诉我损失对 DQN 的意义是什么吗？为什么在这种情况下它们会有所不同，我应该选择哪一个更接近我们对 DQN 的期望？ 结果 这些是我调整超参数后的结果。我试图更好地理解损失结果。它们有问题吗？ 谢谢。    提交人    /u/hifzak   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dj9hha/dqn_loss/</guid>
      <pubDate>Wed, 19 Jun 2024 03:27:36 GMT</pubDate>
    </item>
    <item>
      <title>扩散规划器（Janner 的扩散器）如何开环执行计划？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dj0q1k/how_does_diffuser_by_janner_diffusion_planner/</link>
      <description><![CDATA[扩散器在连续空间中生成 (s,a) 元组的计划，论文称它们开环执行生成的计划。 但扩散模型在连续空间中的动态方面肯定存在轻微的不准确性，即从 s_t 采取动作 a_t 并不能完美地产生计划中生成的 s_{t+1}。那么模型如何开环执行计划，它是否只是在动作 a_t 引起的实际 s_{t+1} 状态下作用 a_{t+1}？ 此外，有人玩过具有离散设置的扩散规划器吗？例如，使用扩散规划器玩棋盘游戏，直观地看，生成的计划会违反很多动态（即不能将棋子移动到那里）？    提交人    /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dj0q1k/how_does_diffuser_by_janner_diffusion_planner/</guid>
      <pubDate>Tue, 18 Jun 2024 20:31:53 GMT</pubDate>
    </item>
    <item>
      <title>PPO 有问题，我很着急 :(</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1diukhg/issue_with_ppo_im_in_a_hurry/</link>
      <description><![CDATA[      您好， 我正在使用 torchrl 库实现 PPO，并遵循官方实现（链接）。我的项目涉及移动通信问题，其中代理（无人机）必须动态地将自己定位在用户附近。但是，我遇到了一个问题，我将用一个简单的例子来解释：用户是静态的，代理在每个情节开始时都初始化在相同的位置。 在训练和初始评估期间一切似乎都很好，看起来代理正在学习。然而，在训练的某个时刻，发生了以下情况： 环境设置如下：  观察空间：代理的位置、信号到达的角度以及与用户的距离。 奖励：标准化的通信吞吐量，如果代理达到最佳位置，则额外获得 +10 奖励。 动作：移动的角度和幅度。  https://preview.redd.it/nhqx0mk3uc7d1.png?width=1727&amp;format=png&amp;auto=webp&amp;s=e73985eaf786ede319115410038c87b75d4d1b01 在所附的图表中，您可以看到代理和用户在某一情节中的平均距离。右侧的图表表示训练阶段，可以清楚地看到，随着距离的减小，代理正在学习。问题出在左侧的图表上，它表示评估阶段。最初，代理会进行学习，距离也会减小，但经过一定次数的迭代后，它似乎会完全忘记所学的内容，并开始持续表现不佳。尽管在训练期间，代理会继续正确执行，但这种情况仍然会发生。在评估期间，性能比训练期间更差，这毫无道理，尤其是因为我在评估期间使用的是操作的平均值，如官方实施中所示。 我非常感谢任何帮助，因为我需要在四天内提交我的学士论文，但我尚未取得一致的结果。非常感谢。    提交人    /u/RikoteMasterrrr   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1diukhg/issue_with_ppo_im_in_a_hurry/</guid>
      <pubDate>Tue, 18 Jun 2024 16:15:49 GMT</pubDate>
    </item>
    <item>
      <title>刚刚拿到了我的 Copilot + Surface，想在上面测试我的机器学习工作负载</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dijzee/just_got_my_copilot_surface_and_wanting_to_test/</link>
      <description><![CDATA[我使用常见的 RL 库在 Python 中构建了我的大多数项目，在稳定基线或其他基线中是否已经支持 DirectML？    提交人    /u/SnooDoughnuts476   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dijzee/just_got_my_copilot_surface_and_wanting_to_test/</guid>
      <pubDate>Tue, 18 Jun 2024 06:28:53 GMT</pubDate>
    </item>
    <item>
      <title>“从阿谀奉承到诡计多端：调查大型语言模型中的奖励篡改”，Denison 等人 2024 年 {Anthropic}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1die9r7/sycophancy_to_subterfuge_investigating/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1die9r7/sycophancy_to_subterfuge_investigating/</guid>
      <pubDate>Tue, 18 Jun 2024 01:05:23 GMT</pubDate>
    </item>
    <item>
      <title>模型似乎没有学到任何东西</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di8da4/model_doesnt_seem_to_be_learning_anything/</link>
      <description><![CDATA[我目前正在创建一个期权交易环境。由于在任何给定时间有多少期权合约，我只关注那些距离执行价格和到期日在一定距离内的合约。例如，如果 SPY 的当前价格为 300，它将包含 295-304 之间以及距离到期日 5 天之间的每个执行价格数据。因此，操作空间为 2（看涨/看跌）x 股票代码数量（仅查看 spy）x strike_size x expiration_size。我目前正在处理的数据集是 2 x 1 x 10 x 5。我尝试过不同的模型，如 PPO 和 SAC，但回报似乎没有增加。是因为我的状态和操作空间太大吗？可能是因为我定义环境的方式？我实施了一个系统，其中所拥有的合约根据日期是否发生变化或当前价格是否移动到新的执行范围而转移到新的位置。    提交人    /u/newjeison   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di8da4/model_doesnt_seem_to_be_learning_anything/</guid>
      <pubDate>Mon, 17 Jun 2024 20:39:52 GMT</pubDate>
    </item>
    <item>
      <title>Pong 游戏的自定义环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di6q6v/custom_environment_for_pong_game/</link>
      <description><![CDATA[因此，我为 Pong 游戏创建了一个自定义环境，但遇到了一些问题（reddit 新手），因为我已经在这个问题上卡了好几个小时了 因此，基本机制是，有一个击球手和一个球 如果球击中顶部、底部和右侧墙壁，它应该被反射，如果它击中左侧墙壁，这意味着击球手无法接住球，那么游戏就结束了，但是，如果击球手和球相撞会怎样？我被困在这个逻辑中，我该如何实现这一切呢    提交人    /u/Cautious-Plan-9491   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di6q6v/custom_environment_for_pong_game/</guid>
      <pubDate>Mon, 17 Jun 2024 19:31:48 GMT</pubDate>
    </item>
    <item>
      <title>完全隐藏对手的游戏 AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di1sr3/game_ai_with_completely_hidden_opponent/</link>
      <description><![CDATA[我有一个场景，我正在尝试构建一个对抗性 RL 解决方案。有一个代理（代理 X）从点 A 开始，需要遍历到点 B，然后返回点 A。有一个对手（代理 Y），其工作是使用上方的摄像头搜索代理 X。代理 Y 可以在成功识别代理 X 10 步后发起攻击。代理 X 永远不会知道代理 Y 是否真正看到了他们，直到他们被袭击杀死。  我的问题是......如果代理 X 实际上不知道他们是否被代理 Y 观察，那么塑造代理 X 的奖励函数以帮助他们隐秘行动的好方法是什么。或者他们会学会“随机”行动，以至于不会被追踪，仅仅因为被杀的巨大负面奖励？    提交人    /u/Cheap_Leather_6432   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di1sr3/game_ai_with_completely_hidden_opponent/</guid>
      <pubDate>Mon, 17 Jun 2024 16:07:11 GMT</pubDate>
    </item>
    <item>
      <title>梯度尺寸差异 PPO</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1di13c6/gradient_size_difference_ppo/</link>
      <description><![CDATA[大家好，在我的 PPO 无法学习并继续打印网络中梯度的范数后，我关注了这个帖子。我发现价值函数范数比策略的范数大得多，甚至大 1000 倍。这导致我的表示模型的范数很大。 尽管随着训练的继续，优势确实会变小，但范数仍然很大。 有人知道可能是什么问题吗？ 附言：我正在运行具有多个级别的分层代理。    提交人    /u/sagivborn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1di13c6/gradient_size_difference_ppo/</guid>
      <pubDate>Mon, 17 Jun 2024 15:38:10 GMT</pubDate>
    </item>
    <item>
      <title>集成模型的强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhrtf2/reinforcement_learning_for_ensemble_models/</link>
      <description><![CDATA[大家好，据我所知，RLHF 一直与 Gen AI 任务相关联。原因是，由于 gen AI 是随机的并且可以生成多个响应（基于提示、温度等参数的细微变化），因此它是对齐的良好候选者。这是通过给它足够的成对样本（首选，不太首选）来实现的，然后它最小化由训练不足的策略生成的分布与首选选项之间的距离。这是通过批次完成的，并在一个时期内取平均值。到目前为止一切都好吗？ 我有一个集成模型（比如说 BERT 和 LayoutLM），其工作是从文档中提取键值对。由于这两个模型都是端到端联合训练的，因此它能够为相同的底层输入生成不同的选项（因为输入是通过 OCR 生成的文本，并且 OCR 本身可以根据文档的质量产生细微的差异）。我可以使用人工代理（通过 UX 生成的输出进行校正）生成的反馈来训练策略吗？我对这种方法感到困惑的原因是我无法说服自己为什么需要奖励函数 + 策略训练，因为我可以非常简单地使用人工操作员所做的校正来微调模型。如果只向模型传递正确答案，而不给它提供了解正确答案和错误答案之间区别的选项，我会错过什么吗？     提交人    /u/immortanslow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhrtf2/reinforcement_learning_for_ensemble_models/</guid>
      <pubDate>Mon, 17 Jun 2024 06:51:00 GMT</pubDate>
    </item>
    <item>
      <title>为什么相关样本在基于策略的方法或更新 Actor 中不会出现问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhra6a/why_correlated_samples_are_not_problematic_in/</link>
      <description><![CDATA[在 DQN 中，我们说 s、s&#39; 是相关的，我们使用重放缓冲区来打破这种关系。  在 PPO 中，当我们更新价值网络时，相同的相关性问题不会影响更新吗？    提交人    /u/seatedrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhra6a/why_correlated_samples_are_not_problematic_in/</guid>
      <pubDate>Mon, 17 Jun 2024 06:13:32 GMT</pubDate>
    </item>
    <item>
      <title>为什么不使用 Actor-Critic 和 PPO 中的目标网络来更新价值网络？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhr94u/why_not_use_a_target_network_in_actorcritic_ppo/</link>
      <description><![CDATA[在 DQN 中，我们使用目标网络来阻止目标移动。但是我们不对 PPO 或 Actor-Critic 方法使用相同的技巧。为什么移动目标对于这些方法的价值网络来说不是问题？在这两种方法中，目标 = r(s,a) + V(s&#39;)。因此更新 V(s) 将改变 V(s&#39;)，这不会影响稳定性吗？    提交人    /u/seatedrow   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhr94u/why_not_use_a_target_network_in_actorcritic_ppo/</guid>
      <pubDate>Mon, 17 Jun 2024 06:11:26 GMT</pubDate>
    </item>
    <item>
      <title>Isaac Gym 什么时候被弃用了？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhqx8y/when_did_isaac_gym_get_deprecated/</link>
      <description><![CDATA[我刚刚在下载网站上看到它被标记为“现已弃用”，并且将不再受支持，而应该考虑使用 Isaac Lab。 这到底是什么时候发生的？有人知道吗？ 我仍然需要研究 Isaac Lab，但有点担心是否会有很多新东西需要学习。我还是这个行业的新手。    提交人    /u/stop_stalking_me_plz   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhqx8y/when_did_isaac_gym_get_deprecated/</guid>
      <pubDate>Mon, 17 Jun 2024 05:48:37 GMT</pubDate>
    </item>
    <item>
      <title>“创造力已远离聊天：消除语言模型偏见的代价”，Mohammedi 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1dhkn9o/creativity_has_left_the_chat_the_price_of/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1dhkn9o/creativity_has_left_the_chat_the_price_of/</guid>
      <pubDate>Sun, 16 Jun 2024 23:45:38 GMT</pubDate>
    </item>
    </channel>
</rss>