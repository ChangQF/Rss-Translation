<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 22 May 2024 09:16:23 GMT</lastBuildDate>
    <item>
      <title>健身房环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</link>
      <description><![CDATA[我有一个实时系统的数据集，我可以使用数据表为强化学习算法创建基于深度神经网络的环境吗？ 请尽快回复。紧急，项目工作陷入困境    提交人    /u/Past-News-1373   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxvwqr/gym_environment/</guid>
      <pubDate>Wed, 22 May 2024 09:14:03 GMT</pubDate>
    </item>
    <item>
      <title>很难阅读大多数 RL 算法的 CS 符号</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</link>
      <description><![CDATA[在观看带有示例的视频后，我理解了贝尔曼方程，但是有没有更直观的方法来理解任何论文中的任何政策制定。并不是所有的论文都能很好地打破他们的方程的作用，而是让比我智力更高的人尝试从论文中挖掘它 &lt;！-- SC_ON - -&gt;  由   提交 /u/baboolasiquala   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxrkih/struggle_reading_cs_notations_with_respect_to/</guid>
      <pubDate>Wed, 22 May 2024 04:16:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么是零和？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</link>
      <description><![CDATA[在很多论文中，你都可以看到“因为游戏是零和的”、“我们以零和的方式解决问题”之类的短语博弈”等，但为什么零和很重要？零和的哪些属性使训练变得更容易？ 例如，在 2P 设置中，很容易看出无论玩家 A 赚到什么，玩家 B 都会输。多人（&gt;2）游戏怎么样？ 再举一个例子，想象一个玩家拥有能力的游戏。玩家 A 有能力为自己生产 10 块金币。 B 从另一位选择的玩家那里偷走了 5 个金币。 C使所有其他玩家失去5金。这显然不是零和游戏，但是你不能按照正常方式训练代理吗？   由   提交 /u/HyogoKita19C   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxo1x7/why_zerosum/</guid>
      <pubDate>Wed, 22 May 2024 01:07:16 GMT</pubDate>
    </item>
    <item>
      <title>棋盘游戏神经网络架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</link>
      <description><![CDATA[有人有过在棋盘游戏中尝试不同神经网络架构的经验吗？ 目前使用 PPO 进行数独 - 输入 I我正在考虑只是一个展平的板向量，因此神经网络是一个简单的 MLP。但我没有得到很好的结果——想知道 MLP 架构是否是问题所在？  AlphaGo 论文使用了 CNN，很想知道你们尝试过什么。感谢任何建议    由   提交 /u/goexploration   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxll8g/board_games_nn_architecture/</guid>
      <pubDate>Tue, 21 May 2024 23:07:06 GMT</pubDate>
    </item>
    <item>
      <title>将 MAB 集成到 Web 应用程序中</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxl655/integrate_mab_into_a_web_application/</link>
      <description><![CDATA[大家好，我正在寻求将 MAB（多武装强盗）RL 集成到网络或移动应用程序中，该项目是为了我的硕士论文和它是关于使用主动学习和强化学习的自适应推荐系统，所以我已经在 Kaggle 中运行了所有内容（我正在做离线评估并使用重播），并且由于我是 RL 的新手，我想知道我应该如何解决整合这项工作进入网络或移动应用程序（我可以在网络/应用程序开发中管理自己）。谢谢大家。我选择使用 movielens Ml 1m 数据集。如果您需要任何其他信息，请告诉我。   由   提交 /u/Fredybec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxl655/integrate_mab_into_a_web_application/</guid>
      <pubDate>Tue, 21 May 2024 22:47:58 GMT</pubDate>
    </item>
    <item>
      <title>这个项目创意好不好 - 自驾代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</link>
      <description><![CDATA[我正在为自动驾驶代理构建一个项目，我计划包括改变状态的动态交通灯和在网格中随机移动的动态行人，独立于中介。包含动态行人有多困难，并且可以在网格中随机生成灯光和行人的位置吗？还使用 q-learning 来实现 我怎样才能使这个项目更加随机？我正在考虑引入天气影响，在每次模拟之前使用概率分布，并且根据天气预报，这将影响车辆的燃油消耗，惩罚会导致燃油消耗大幅增加。本质上，代理处于生存模式，以确保不发生碰撞、遵守交通法规并且不达到最大燃油容量。   由   提交/u/amulli21  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cxbaah/is_this_project_idea_good_self_driving_agent/</guid>
      <pubDate>Tue, 21 May 2024 15:56:21 GMT</pubDate>
    </item>
    <item>
      <title>如何使用人类演示来预训练 RL 代理？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</link>
      <description><![CDATA[我想要一个代码示例，以便了解我该如何做到这一点。有人能帮帮我吗？    提交人    /u/SebyR   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwmb1f/how_do_i_pretrain_a_rl_agent_using_human/</guid>
      <pubDate>Mon, 20 May 2024 18:16:01 GMT</pubDate>
    </item>
    <item>
      <title>“Mobile ALOHA：通过低成本全身远程操作学习双手移动操作”，Fu 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwlbl4/mobile_aloha_learning_bimanual_mobile/</guid>
      <pubDate>Mon, 20 May 2024 17:35:15 GMT</pubDate>
    </item>
    <item>
      <title>强化学习中的对抗性攻击和对抗性训练</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</link>
      <description><![CDATA[https://github.com/EzgiKorkmaz /对抗性强化学习   由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwl8lb/adversarial_attacks_and_adversarial_training_in/</guid>
      <pubDate>Mon, 20 May 2024 17:31:42 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助：解决强化学习 PyTorch 模型中的“mat1 和 mat2 形状无法相乘”错误</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwj968/help_needed_solving_mat1_and_mat2_shapes_cannot/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwj968/help_needed_solving_mat1_and_mat2_shapes_cannot/</guid>
      <pubDate>Mon, 20 May 2024 16:08:17 GMT</pubDate>
    </item>
    <item>
      <title>q 学习和井字游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</link>
      <description><![CDATA[tttrl - Pastebin.com KDT85/Tic-Tac-Toe-RL：使用强化学习的 Tic-Tac-Toe 游戏 (github.com)&lt; /a&gt; 上面是我使用强化学习的井字棋游戏的代码，你们觉得怎么样。  它似乎可以工作，但 q 表没有填满，这正常吗？其训练次数为 20000000 集。  GitHub 中包含 q 表作为 Excel 文件。  谢谢！    由   提交 /u/chinfuk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cwhxzz/q_learning_and_tic_tac_toe/</guid>
      <pubDate>Mon, 20 May 2024 15:12:36 GMT</pubDate>
    </item>
    <item>
      <title>致力于为 Azul Board Game 开发成功的强化学习模型</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</link>
      <description><![CDATA[大家好， 我目前正在做一个项目，旨在训练一个模型精通桌游 Azul 。我用 Python 编写了自己的游戏引擎，并且使用 SB3 PPO 实现。我的观察是一个由大约 700 位组成的二进制向量（这是整个游戏状态的二进制表示，如工厂、玩家板等），我的动作空间是 300 大小的向量，每个位对应于某个动作（在 azul 游戏中，每回合你必须做出 3 个决定：选择哪个工厂、什么颜色以及将其放置在哪里，并且你有 10 个工厂（我的模型正在玩 4 人游戏）、5 种颜色和 6 条图案线，因此你有 300 种可能我已经多次运行它，使用不同的激活函数、层大小、层数、学习率、伽马等，我尝试使用分数作为奖励，也尝试使用一些任意奖励，例如进行有效的移动等。 ，但我从未达到令人满意的结果，该模型从平均 -90 分开始，逐渐达到 -30/-20，但这仍然是非常糟糕的结果，因为 azul 中的负分意味着它做了很多废话平均球员每场得分超过 40/50 分，所以我与我的模型还相去甚远。  azul 环境的巨大问题是，在 300 个动作中，通常只有 40 个有效动作，因此这意味着只有大约 10-15% 的动作是有效的。首先，我的模型将以某种方式工作，它采取具有最高神经元值的操作，如果有效，则执行此操作，如果建议的操作无效（例如，模型想要从工厂 2 获取蓝色瓷砖，并将其放在模式行3中，但该工厂没有蓝色瓷砖），它将采取第二高神经元值动作并检查它是否有效等等，直到达到有效动作。  我最近的想法是对模型输出应用一些屏蔽，这样当模型观察棋盘并返回动作向量时，它会过滤所有无效动作，将它们的值设置为 0，然后然后我们只有有效动作的动作向量，第二个变化是我不选择最高值，而是将 softmax 应用于这些值并使用计算的概率随机选择它们。  不幸的是，我没有得到任何结果，该模型根本没有学习。你们对我可以在哪里改进我的算法、为什么我的模型无法学习等有什么建议吗？我将非常感谢所有的帮助，我也愿意为那些感兴趣的人分享代码，所以请告诉我，  请帮助我（：    提交者    /u/Different-Leave8202   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cweh20/stuck_in_developing_successful_rl_model_for_azul/</guid>
      <pubDate>Mon, 20 May 2024 12:34:04 GMT</pubDate>
    </item>
    <item>
      <title>有人真的部署了一个模型来用于推理吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</link>
      <description><![CDATA[我只是好奇有人实际上使用 RL 训练的策略来帮助解决哪些应用程序或控制问题，您用什么 Algo 进行训练？在此过程中遇到的主要挑战是什么？    由   提交 /u/Aggressive-Reach1657    reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw9d6s/has_anyone_actually_deployed_a_model_to_use_for/</guid>
      <pubDate>Mon, 20 May 2024 07:01:20 GMT</pubDate>
    </item>
    <item>
      <title>“认识一下 Shakey：第一个电子人——拥有自己思想的机器的迷人而可怕的现实”，Darrach 1970</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cw41oe/meet_shakey_the_first_electronic_personthe/</guid>
      <pubDate>Mon, 20 May 2024 01:39:13 GMT</pubDate>
    </item>
    <item>
      <title>RL 导师/专家</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</link>
      <description><![CDATA[我是一名本科生，目前正在完成论文。我接手了一个项目，该项目使用 RL 进行连续控制，通过 6d 位姿估计器来控制机器人。我看得很远，但强化学习机器人技术在我们国家可能还不够饱和。我试图寻找结构化的方法来学习这一点，就像使用 OpenAI 旋转强化学习以及 Sutton &amp; 的理论背景一样。巴托的书。我真的很渴望在明年之前完成这个项目，但我没有导师。甚至我们大学的教授也很快就会采用强化学习机器人技术。我从以前的帖子里看到，在这里请教导师是可以的，所以请原谅。如果我无法正确地提出问题，我深表歉意。 我想要实现这些目标： - 充分掌握 RL 基础知识，特别是在连续动作空间控制方面。 - 熟悉艾萨克·西姆。 - 了解如何为强化学习建模物理系统 - 将经过训练的模型部署到物理机器人 - 通过项目慢慢积累知识，最终引导我完成项目 - 寻找指导我完成整个工作流程的导师 &lt; p&gt;我所知道的： - 深度学习的背景 - RL 的基本原理（直到 MDP 和 TD） - RL 算法的背景 - DQN、DDPG、TD3 如何在高级抽象中工作 - 在高级抽象中体验重播缓冲区和 HER - ROS 2 基础知识 我想知道什么： - 我需要学习所有数学知识吗？或者我可以只参考现有的实现吗？ - 考虑到我的资源限制，我只能实现一个算法（我在第三世界国家），我应该使用它来实现完成项目的最大可能性。目前，我正在关注TD3。 - 一个本科生团队有可能完成这样的项目吗？ - 鉴于资源限制，我们应该使用哪个 Jetson 板来运行策略？ - 我们的目标是针对易碎处理进行优化，我们如何限制研究？ 我的努力我目前正在研究更多并建立关于算法和强化学习的直觉。最近，我迁移到 Ubuntu 并设置了模拟所需的所有软件和环境 (Isaac Sim)。 挫败感 在没有人交谈的情况下继续这个项目非常具有挑战性，因为每个人都几乎不感兴趣与RL。每个资源都有一个非常陡峭的学习曲线，当我认为我知道某些资源时，某些资源指向了我不知道的其他东西。我必须在明年之前完成这个工作，尽管我正在尽我所能地学习，但仍有很多我不知道的事情。    由   提交 /u/echialas22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cvycbp/mentorexpert_in_rl/</guid>
      <pubDate>Sun, 19 May 2024 21:04:03 GMT</pubDate>
    </item>
    </channel>
</rss>