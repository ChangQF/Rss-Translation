<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 12 Sep 2024 18:22:45 GMT</lastBuildDate>
    <item>
      <title>大家好，我是 RL 的一个应用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff9j09/an_application_of_rl_everyone/</link>
      <description><![CDATA[  由    /u/nimageran  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff9j09/an_application_of_rl_everyone/</guid>
      <pubDate>Thu, 12 Sep 2024 18:22:15 GMT</pubDate>
    </item>
    <item>
      <title>离线 RL 模型 OPE 的 Python 包</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff6xqs/python_package_for_ope_of_offline_rl_models/</link>
      <description><![CDATA[嘿！我已经开发了一个 Python 包，用于对现实世界数据执行离线 RL 模型的离策略评估！https://github.com/joshuaspear/offline_rl_ope 它经过单元测试，并且具有张量维度的运行时类型检查，因此祝愿它易于使用！使用示例可在 repo 的示例文件夹中找到 - 非常欢迎反馈！ 干杯    提交人    /u/bean_the_great   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff6xqs/python_package_for_ope_of_offline_rl_models/</guid>
      <pubDate>Thu, 12 Sep 2024 16:35:08 GMT</pubDate>
    </item>
    <item>
      <title>最好的软件提交会议？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ff6dvr/best_conference_for_software_submission/</link>
      <description><![CDATA[大家好 - 有人推荐一个好的软件提交会议吗？JMLR 需要坚实的用户基础 - 我还没有！ 谢谢    提交人    /u/bean_the_great   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ff6dvr/best_conference_for_software_submission/</guid>
      <pubDate>Thu, 12 Sep 2024 16:12:12 GMT</pubDate>
    </item>
    <item>
      <title>你好呀！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fewq1s/hi_there/</link>
      <description><![CDATA[我正在构建一个 AI 框架，该框架利用两个领域的强化学习来自动清理杂乱的数据以增强数据管理。 背景：基本上，来自科学研究人员的可能包含缺失值或异常的原始数据被提交到数据存储库，在那里对这些提交的数据执行手动数据管理。提交的数据多种多样（例如：地球数据、生物数据、化学数据……）。我计划专注于两个不同的领域，并创建一个框架，可以自动清理或自动检测提交数据中的错误。对于自动清理或自动检测错误/异常值/缺失值，我必须计划使用强化学习，它可以了解该领域并在数据提交者提交数据之前建议或突出显示数据错误。 我想知道：  这是一个广泛的话题吗？ 我在概念上是否走在正确的道路上。 对研究论文有什么建议。我发现关于这方面的论文很少。  我承认我是个菜鸟。任何建议都会有帮助！    提交人    /u/Dazzling_Rose_0708   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fewq1s/hi_there/</guid>
      <pubDate>Thu, 12 Sep 2024 07:22:37 GMT</pubDate>
    </item>
    <item>
      <title>PPO 中的参与者损失和价值损失的合理值是多少？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fes456/what_are_good_values_for_actor_loss_and_value/</link>
      <description><![CDATA[我理解参与者损失和价值损失取决于奖励结构和环境。关于良好范围的经验法则是什么？这样我就可以相应地重新调整和规范奖励。我发现的一个来源说 0.1-0.5 适合参与者损失，价值损失低于 1。我怎么知道什么时候停止训练和调试？只有奖励才是好的指标？我们能从参与者和价值损失中学到一些东西吗？谢谢。    提交人    /u/L16H7   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fes456/what_are_good_values_for_actor_loss_and_value/</guid>
      <pubDate>Thu, 12 Sep 2024 02:34:29 GMT</pubDate>
    </item>
    <item>
      <title>“SEAL：价值一致的系统误差分析”，Revel 等人 2024（偏好学习数据集中的错误和偏差）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fercxt/seal_systematic_error_analysis_for_value/</link>
      <description><![CDATA[  由    /u/gwern  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fercxt/seal_systematic_error_analysis_for_value/</guid>
      <pubDate>Thu, 12 Sep 2024 01:55:56 GMT</pubDate>
    </item>
    <item>
      <title>关于 RPG 沙盒的 BOSS 战技巧（最好使用 Python）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fec8bs/tips_on_a_boss_fight_rpg_sandbox_preferrably_with/</link>
      <description><![CDATA[大家好， 我正在为我的计算机工程学位做毕业论文，我将使用 RL 探索 AI 在类似灵魂的 Boss 上的应用，所以我真的只需要一个有 2 个实体的区域，1 个是 Boss，1 个是玩家。 你们认为最简单的方法是什么？这意味着我的大部分注意力将集中在 AI 模型上。 我需要的几乎是一个带有两个实体的沙盒，问题是它需要是 3D 的，我在 youtube 上寻找教程但找不到，而且我不知道哪个库或框架更适合我的问题。 任何建议都会有很大的帮助。 谢谢！    提交人    /u/lordgvp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fec8bs/tips_on_a_boss_fight_rpg_sandbox_preferrably_with/</guid>
      <pubDate>Wed, 11 Sep 2024 14:59:03 GMT</pubDate>
    </item>
    <item>
      <title>模拟推荐系统环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1feamwu/stimulating_a_recommder_system_environment/</link>
      <description><![CDATA[嗨，我是 python 开发人员。有人知道像 RecSim 这样的软件包可以模拟推荐系统来测试 RL 模型吗？RecSim 似乎不受支持且可能无法使用（不知道 tensorflow 1.13.1 是否还能使用）。 非常感谢！    提交人    /u/pot8o118   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1feamwu/stimulating_a_recommder_system_environment/</guid>
      <pubDate>Wed, 11 Sep 2024 13:51:14 GMT</pubDate>
    </item>
    <item>
      <title>Tetris Gymnasium：可定制的俄罗斯方块强化学习环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fea6bs/tetris_gymnasium_a_customizable_reinforcement/</link>
      <description><![CDATA[今天，Tetris Gymnasium 的第一个版本发布了，对于从事强化学习相关工作或想要涉足该领域的人来说，这可能很有趣。 这是什么？Tetris Gymnasium 是 Tetris 作为强化学习环境的简洁实现，并与 Gymnasium 集成。它可以自定义（例如棋盘尺寸、重力等），并包含许多有关如何使用它的示例，如训练脚本。 为什么是俄罗斯方块？尽管许多 Atari 游戏在强化学习方面取得了重大进展，但俄罗斯方块仍然是 AI 面临的一个挑战。它结合了 NP 难复杂性、随机元素和长期规划需求，使其成为强化学习研究中一个持续存在的未解问题。到目前为止，还没有出版物可以在不使用手工制作的特征向量或其他简化的情况下很好地与游戏配合使用。 我能用它做什么？请不要犹豫，尝试一下该环境以进入强化学习。好处是俄罗斯方块很容易理解，你可以观看代理玩并清楚地看到它所犯的错误。如果您已经进入 RL，您可以将其用作可自定义的环境，可以很好地与其他框架（如 Gymnasium 和 W&amp;B）集成。 GitHub：https://github.com/Max-We/Tetris-Gymnasium 在存储库中，您还可以找到我们的短文“逐个步骤：为俄罗斯方块组装模块化强化学习环境”的预印本其中更详细地解释了背景、实施情况以及学生和研究人员的机会。 如果您尝试该环境，欢迎您留下星星或打开一个问题！    提交人    /u/Npoes   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fea6bs/tetris_gymnasium_a_customizable_reinforcement/</guid>
      <pubDate>Wed, 11 Sep 2024 13:30:43 GMT</pubDate>
    </item>
    <item>
      <title>我想将强化学习应用到机械臂上。寻求建议！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fe73oa/i_want_to_apply_reinforcement_learning_to_a/</link>
      <description><![CDATA[您好， 我是一名正在学习强化学习的大学生。我正尝试首次将强化学习应用于机械手，感觉有点不知所措，因此，如果您能提供任何建议，我将不胜感激。  模拟器推荐：我不确定哪种模拟器最适合将强化学习应用于机械手。我听说过 PyBullet、MuJoCo、Gazebo 和其他几种模拟器。哪种模拟器使用最广泛，推荐度最高？ 论文推荐：如果您知道任何关于将强化学习应用于机械手的重要论文或评论文章，我将非常感激您的推荐。尤其是作为初学者，我想知道我应该从哪些论文开始。 推荐的学习资源：如果有任何网站或资源提供该领域的组织良好的学习材料，我将不胜感激。或者，如果您有将 RL 应用于操纵器的建议课程或学习路径，那将非常有帮助。  任何针对初学者的资源或建议都将不胜感激！提前谢谢大家。    提交人    /u/DRLC_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fe73oa/i_want_to_apply_reinforcement_learning_to_a/</guid>
      <pubDate>Wed, 11 Sep 2024 10:46:27 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的自学研究方法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdz4jl/selflearning_research_methods_for_rl/</link>
      <description><![CDATA[你好！ 我想攻读 RL 博士学位。但是，我的潜在导师回复说我没有研究经验。 我在 PhD SubReddit 上发帖询问如何自学研究。我注意到似乎有更适合 CS 的研究方法。 我想我可以向这里的集体智慧寻求更具体的建议。我在哪里可以开始自学如何为 RL 空间进行研究和学术写作？ 有人知道好的资源吗？ 天方夜谭：如果这里有博士生导师并且愿意直接留言，我很想知道向你这样的人推销申请的最佳知识和经验基础是什么。 附注：我住在澳大利亚。   由    /u/jeroku  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdz4jl/selflearning_research_methods_for_rl/</guid>
      <pubDate>Wed, 11 Sep 2024 02:04:10 GMT</pubDate>
    </item>
    <item>
      <title>学习一个价值函数，然后通过最小化相应的 Q 函数来学习一个策略，最后使用该策略来热启动最优控制求解器。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdybbo/learning_a_value_function_then_learning_a_policy/</link>
      <description><![CDATA[任何在优化和最优控制、强化学习编程方面有专业知识的人可以自由联系我 [tesfayedmu@gmail.com](mailto:tesfayedmu@gmail.com)    提交人    /u/AsleepCreme5489   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdybbo/learning_a_value_function_then_learning_a_policy/</guid>
      <pubDate>Wed, 11 Sep 2024 01:19:49 GMT</pubDate>
    </item>
    <item>
      <title>没有任何经验并且只有一个强化学习项目的学生可以在 RL 领域找到工作吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdxgry/can_students_with_no_experience_and_one_project/</link>
      <description><![CDATA[没有发表过论文但有一个 RL 大项目的硕士生能在 RL 领域找到工作吗？    提交人    /u/optimum_point   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdxgry/can_students_with_no_experience_and_one_project/</guid>
      <pubDate>Wed, 11 Sep 2024 00:35:55 GMT</pubDate>
    </item>
    <item>
      <title>分享我的副项目 raice - 现实生活中的特工在 F1 赛道上的赛车比赛</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdtpn8/sharing_my_side_project_raice_a_racing/</link>
      <description><![CDATA[      嗨！  让我向您展示 https://github.com/Fer14/raice，这是使用不同算法训练的 RL 代理之间的赛车比赛。 不确定如何发布此内容，但自从学习 RL 以来，我认为让所有这些不同的算法以某种方式相互竞争会很有趣。因此，我有了这个想法，我从 YouTube 视频中获取了一个简洁的代理在自定义赛道上训练并实施更多算法（可能还会有更多算法）以查看谁在 F1 赛道上表现最佳。我不是 F1 的狂热爱好者，但我认为添加一条真正的赛道并举办一整场 F1 比赛会很有趣，所以这就是我现在正在做的事情，我认为分享会很有趣。 我不指望它能完美运行，一旦一切完成，我想做一些调整，但现在我认为它很酷！    提交人    /u/Fer14x   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdtpn8/sharing_my_side_project_raice_a_racing/</guid>
      <pubDate>Tue, 10 Sep 2024 21:40:20 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 无法学习简单的环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1fdr8zu/ddpg_fails_to_learn_simple_environment/</link>
      <description><![CDATA[我在 https://github.com/JijaProGamer/Car-Racer-AI &gt; src/GPU/model.js 上有一个非常简单的 DDPG 代码 并且大部分代码都是从 https://keras.io/examples/rl/ddpg_pendulum/ 复制而来，只是用 TFJS（以及扩展的 JS）重写。 这里有一些内容：我使用与 keras 示例相同的超参数，我相信我得到了噪声类工作，并且没有错误，只是一些静默问题。 DQN 也有效（对于离散动作空间），所以我知道这不是环境或内存问题，而是 DDPG 特定代码的问题。 我不确定哪里出了问题，因为一切都看起来像 keras 中的 99%，并且演员损失不断上升，而批评家不断降低（趋于负值）。    提交人    /u/ZazaGaza213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1fdr8zu/ddpg_fails_to_learn_simple_environment/</guid>
      <pubDate>Tue, 10 Sep 2024 19:56:54 GMT</pubDate>
    </item>
    </channel>
</rss>