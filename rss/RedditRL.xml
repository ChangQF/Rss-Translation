<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 13 Oct 2024 18:23:17 GMT</lastBuildDate>
    <item>
      <title>演员评论家、探索问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2vcbm/actor_critic_exploration_issue/</link>
      <description><![CDATA[我正在训练一个简单的环境，其中输入是 144 个 1D 扁平数据（4x12 网格 x 3 个特征）。  我有两个问题：1. 如果我想要最优动作和替代动作，哪种探索更好？使用 E-greedy，状态的最优动作具有最高动作概率，而其他所有动作都接近 0。  如果我在 softmax 之前使用 Boltzman 温度进行平滑，则探索性不够。   128-64-32（三个隐藏层）的 nn 设置有效，但为什么 256-128-64 不起作用？或者只是一层的 128 个单位不起作用？或者任何其他配置。      提交人    /u/doctor_fhk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2vcbm/actor_critic_exploration_issue/</guid>
      <pubDate>Sun, 13 Oct 2024 17:47:09 GMT</pubDate>
    </item>
    <item>
      <title>Q 学习（及变体）评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2qqcg/q_learning_and_variations_evaluation/</link>
      <description><![CDATA[大家好， 我有一个非常基本/快速的问题，关于 RL 框架（特别是 Q 学习）的评估/测试。假设我们使用简单/经典的 Q 学习算法。我们让它训练一些情节来学习环境并构建 Q 表。那又怎么样？准备好 Q 表意味着训练后我们将遵循以下状态： action = np.argmax(q_table[state])  ？ 我的问题是：准备好 Q 表后，我们只需遵循它吗？我们构建了 Q 表然后我们遵循它，对吗？ 提前谢谢大家！！    提交人    /u/Interesting_Pea_4605   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2qqcg/q_learning_and_variations_evaluation/</guid>
      <pubDate>Sun, 13 Oct 2024 14:21:42 GMT</pubDate>
    </item>
    <item>
      <title>之前谁想过简单的 Actor-Critic Transformers 设置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2op1r/who_thought_about_simple_actorcritic_transformers/</link>
      <description><![CDATA[在上一篇文章中，我描述了这种廉价的前馈变压器（没有自我注意块）：https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/ 如果我们应用状态序列 S1、S2、S3 ... SN，并输出动作序列 A1、A2、A3 ... AN，称其为 Actor 将它们联合起来并应用于 [S1,A1]、[S2,A2]、[S3,A3] ... [SN,AN] 以创建 Q1、Q2 序列， Q3...QN。称其为评论家 我们可以通过它进行反向传播并改进动作序列吗？ 例如 N 步时间差异。 当然，这种设置将输给 Full Transformer，或者输给学习状态、动作和奖励（彼此平行）之间互连的决策变压器 但是这种设置可以被视为 2 个可以首先从推出中学习的长距离 LSTM。（如果我们有一些来自远程控制机器人的数据） 如果奖励函数得到适当调整，那么它可以在虚拟环境中得到潜在改进。相比之下，决策变压器的 TD 改进可能性较小。    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2op1r/who_thought_about_simple_actorcritic_transformers/</guid>
      <pubDate>Sun, 13 Oct 2024 12:37:09 GMT</pubDate>
    </item>
    <item>
      <title>如何通过控制和学习算法解决电动汽车充电问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2n5c2/how_to_solve_ev_charging_problem_by_control_and/</link>
      <description><![CDATA[下午好， 我计划实施文章中指定的 EV 充电算法：https://www.researchgate.net/publication/353031955_Learning-Based_Predictive_Control_via_Real-Time_Aggregate_Flexibility **问题描述** 我正在尝试思考如何实施这种基于控制和学习的算法。该算法解决了 EV 充电问题，确保 EV 充电成本最小化，同时满足基础设施约束（容量）和 EV 约束（满足所需的能源需求）。为了解决这个问题，我们需要实时协调聚合器和系统运营商。在每个时间步，系统操作员都会向聚合器提供可用电力。聚合器接收此电力并使用简单的调度算法（如 LLF）为电动汽车充电。聚合器向系统操作员发送（通过 RL 算法）学习到的最大熵反馈/灵活性（=电动汽车约束摘要），系统操作员据此选择下一个时间步的可用电力。这个循环重复到最后一个时间步（=直到一天结束）。 **RL 环境描述** 基本上，我们在时间步 t 的状态空间包含有关在时间步 t 连接到 EVSE 的每个电动汽车的信息（=剩余充电时间、剩余充电能量）。状态空间将是一个维度为 EVSE*2 + 1 的向量（也许包括时间步也是值得的）。 动作空间将是大小为 U 的概率向量（=灵活性）（其中 U 是不同的功率级）。根据这个概率向量，我们选择每个时间步的电动汽车充电功率水平（=基础设施容量）。 RL 算法将在每个充电日后终止。 **问题：**  学习是离线的究竟意味着什么？RL 代理是否具有有关系统未来成本和约束的信息？如果是，如何在离线学习期间为 RL 代理提供有关未来的信息，而无需扩大状态空间和动作空间（具有与文章中相似/相同的动作空间）？ 每个时间步的奖励函数包含所有时间步的充电决策（奖励函数中的第 3 项），但充电决策取决于给定动作生成的信号。基本上，奖励会考虑代理的未来行动，那么如何获得它们呢？如何设计用于在线测试的奖励函数？ 我们也可以在此问题中进行离线测试或在线训练/学习吗？  如何在我们的环境中为这个问题设计重置函数？我是否应该从给定的训练/测试数据集中随机选择不同的充电日并保持其他超参数不变？     提交人    /u/Superb-Carry6469   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2n5c2/how_to_solve_ev_charging_problem_by_control_and/</guid>
      <pubDate>Sun, 13 Oct 2024 11:00:46 GMT</pubDate>
    </item>
    <item>
      <title>如何申请并破解Google Summer of Code？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2ml2i/how_to_apply_and_crack_google_summer_of_code/</link>
      <description><![CDATA[        提交人    /u/External_Ad_11   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2ml2i/how_to_apply_and_crack_google_summer_of_code/</guid>
      <pubDate>Sun, 13 Oct 2024 10:20:27 GMT</pubDate>
    </item>
    <item>
      <title>解释 SB3 日志</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2jz2l/interpreting_sb3_logs/</link>
      <description><![CDATA[      我知道有几种来源试图解释 SB-3s PPO 的日志输出。但是，我只是没有完全掌握它。 从正常的 ML 中，我了解到损失是正的，应该约为 0。但是，在我当前的 SB3 PPO 设置中，我的负损失接近于 0 这是正确的行为吗（即代理是否学习）？如果是，有没有人有好的资料可以解释这一点？ https://preview.redd.it/t32ed10g2hud1.png?width=467&amp;format=png&amp;auto=webp&amp;s=1e0c5ca156a173546aeb039456250247edd7657f    提交人    /u/luigi1603   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2jz2l/interpreting_sb3_logs/</guid>
      <pubDate>Sun, 13 Oct 2024 07:02:05 GMT</pubDate>
    </item>
    <item>
      <title>面向初学者的奖励函数发现简单教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</guid>
      <pubDate>Sat, 12 Oct 2024 13:28:33 GMT</pubDate>
    </item>
    <item>
      <title>关于 ALE 论文和超参数调优的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1zrgr/question_regarding_ale_paper_and_hyper_parameter/</link>
      <description><![CDATA[我一直在阅读此链接上的论文“Arcade 学习环境：通用代理的评估平台”：https://arxiv.org/abs/1207.4708，我不确定他们如何进行超参数调整。  据我所知，他们在 5 个不同的环境中优化超参数，然后使用这些超参数对其余环境进行训练和评估。  那么我的问题是，这是如何工作的？我很难理解。如何同时在多个环境中优化超参数？  我假设所有环境都有相同的观察和动作空间，但如何同时在不同环境中进行训练和评估？   由    /u/IAmNotMarcus  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1zrgr/question_regarding_ale_paper_and_hyper_parameter/</guid>
      <pubDate>Sat, 12 Oct 2024 13:08:55 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium - 股票交易环境的终止状态与截断状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1yi0c/gymnasium_terminated_vs_truncated_state_for_stock/</link>
      <description><![CDATA[嗨， 所以我读了一些关于 gym.Env 中终止和截断之间的区别。根据我的理解： terminated = True -&gt; 表示达到 MDP 定义下的终止状态（因此取决于您如何定义底层 MDP） truncated = True -&gt; 由于 MDP 中未明确定义的条件，情节结束。例如（来自 Gym Docu），代理在物理上超出界限或达到时间限制。 虽然这对于机器人任务来说是有意义的，但当涉及到我正在处理的问题（用于交易/管理金融资产的代理）时，我缺少一些部分。我主要有两个问题：  代理以数据框的形式在给定长度 T 的一系列状态（每天一个）上进行训练。一旦到达情节的结尾，我会设置 done = True（在 gym 0.26 之前的版本下）。现在我必须设置 determinant = True 或 truncated = True。这里什么才有意义？请记住，代理的目标是最大化利润，因此没有“明确的目标条件”表明代理成功或失败了特定任务（就像在机器人技术中一样）。 假设我正在使用 StableBaselines 之类的框架。代理对 terminated = True 和 truncated = True 的解释是否不同？     提交人    /u/Intelligent-Put1607   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1yi0c/gymnasium_terminated_vs_truncated_state_for_stock/</guid>
      <pubDate>Sat, 12 Oct 2024 11:56:49 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关攻读 RL 和机器人学博士学位的建议，同时处理签证问题和职业变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1wgby/seeking_advice_on_pursuing_a_phd_in_rl_and/</link>
      <description><![CDATA[我的职业生涯正处于十字路口，需要一些建议。我目前在海外一家大型科技公司担任高级 SDE，但我感觉并不充实。我计划转向 RL 和机器人技术，目标是未来获得博士学位。↳ 我的情况如下：↳  我将在一所优秀大学的机器人实验室担任研究助理，一直工作到 2025 年 6 月。 为了这个机会，我愿意大幅减薪（从每月 1x k+ 到 2k）。 我今年将申请硕士和博士课程，但我不确定我是否能进入一个好的课程。 我有大约 12 万美元的存款，但我担心在过渡期间的财务问题。 作为中国公民，签证问题令人担忧。即使有工作机会，对于拥有学士学位的中国女性来说，H1B 抽签的成功率也只有 20%。 我也在考虑个人生活方面的问题，比如约会和结婚，这可能会对签证问题有所帮助。我有点漂亮，这对我来说约会容易一些，但保持美丽需要花费大量的时间和精力。  我的主要目标是专注于 RL 和机器人技术，发表优秀的论文，并在该领域产生影响。但是，我担心会因财务问题和签证问题而分心。↳ 有人遇到过类似的情况吗？或者对平衡职业目标和实际问题有什么建议吗？我如何在处理这些其他因素的同时专注于我的研究？↳ 任何见解或经验都将不胜感激！    提交人    /u/FaithlessnessFree554   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1wgby/seeking_advice_on_pursuing_a_phd_in_rl_and/</guid>
      <pubDate>Sat, 12 Oct 2024 09:31:02 GMT</pubDate>
    </item>
    <item>
      <title>“更大、更规则、更乐观：计算和样本高效连续控制的扩展”，Nauman 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1il8n/bigger_regularized_optimistic_scaling_for_compute/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2405.16158 摘要：  强化学习 (RL) 中的样本效率传统上由算法增强驱动。在这项工作中，我们证明扩展也可以带来显着的改进。我们对扩展模型容量和特定领域的 RL 增强之间的相互作用进行了彻底的研究。这些实证发现为我们提出的 BRO（更大，正则化，乐观）算法的设计选择提供了信息。BRO 背后的关键创新是强正则化允许有效扩展评论家网络，这与乐观探索相结合，可带来卓越的性能。 BRO 取得了最先进的成果，在 DeepMind Control、MetaWorld 和 MyoSuite 基准的 40 个复杂任务中，其表现显著优于领先的基于模型和无模型的算法。BRO 是第一个在极具挑战性的狗和人形任务中实现近乎最优策略的无模型算法。    [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1il8n/bigger_regularized_optimistic_scaling_for_compute/</guid>
      <pubDate>Fri, 11 Oct 2024 19:57:17 GMT</pubDate>
    </item>
    <item>
      <title>“奖励进步：扩展 LLM 推理的自动化流程验证器”，Setlur 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1igfb/rewarding_progress_scaling_automated_process/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2410.08146 摘要：  一种用于改进大型语言模型推理的有前途的方法是使用过程奖励模型 (PRM)。PRM 在多步推理跟踪的每个步骤中提供反馈，与仅在最后一步提供反馈的结果奖励模型 (ORM) 相比，可能改善信用分配。但是，收集密集的每步人工标签是不可扩展的，并且迄今为止，从自动标记的数据训练 PRM 带来的收益有限。为了通过针对 PRM 运行搜索或将其用作强化学习 (RL) 的密集奖励来改进基础策略，我们问：“我们应该如何设计过程奖励？”。我们的关键见解是，为了有效，步骤的过程奖励应该衡量进度：在采取该步骤之前和之后，未来产生正确响应的可能性的变化，与 RL 中的步骤级优势概念相对应。至关重要的是，应该在不同于基础策略的证明者策略下衡量这一进展。我们从理论上描述了一组好的证明者，我们的结果表明，优化这些证明者的过程奖励可以改善测试时搜索和在线 RL 期间的探索。事实上，我们的描述表明，弱证明者策略可以显著改善更强大的基础策略，我们也通过经验观察到了这一点。我们通过训练过程优势验证器 (PAV)来预测此类证明器下的进展，从而验证了我们的说法，并表明与 ORM 相比，针对 PAV 的测试时搜索准确率高出 8% 以上，计算效率高出 1.5-5 倍。通过 PAV 密集奖励的在线 RL 实现了 首批结果之一，与 ORM 相比，样本效率提高了 5-6 倍，准确率提高了 6% 以上。    [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1igfb/rewarding_progress_scaling_automated_process/</guid>
      <pubDate>Fri, 11 Oct 2024 19:51:19 GMT</pubDate>
    </item>
    <item>
      <title>AlphaZero MCTS 的搜索深度有多深？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1iavg/how_deep_does_alphazero_mcts_search/</link>
      <description><![CDATA[我在 A0 的论文中看到，他们在训练期间运行了 1600 次 MCTS 迭代，与单独使用策略网络相比，搜索结果可获得 1000+ elo 增益。但是，假设在围棋中，每个状态有 5 种合理的走法，那么经过 5 步深度后，5^5 = 3125 &gt; 1600。显然，这是一个粗略的估计，可能由于剪枝能力而被低估，但直观地看，1600 感觉很少。有没有发布 alphazero 搜索深度的信息，或者 alphago lee 在与李世石对弈时搜索的深度？    提交人    /u/DumplingLife7584   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1iavg/how_deep_does_alphazero_mcts_search/</guid>
      <pubDate>Fri, 11 Oct 2024 19:44:19 GMT</pubDate>
    </item>
    <item>
      <title>按顺序运行数据点或选择随机点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1df5n/running_though_the_datapoints_sequentially_or/</link>
      <description><![CDATA[我正在使用 sb3 训练交易环境，我的数据集由 250k 个数据点组成。我不确定我是否应该让环境始终从数据集的开头开始并按顺序运行直到 250k 结束以计算奖励，或者我应该使用固定的情节长度（例如 50k）并让它从每个情节的随机点开始。哪个可以让训练效果更好？    提交人    /u/Acceptable_Egg6552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1df5n/running_though_the_datapoints_sequentially_or/</guid>
      <pubDate>Fri, 11 Oct 2024 16:08:57 GMT</pubDate>
    </item>
    <item>
      <title>基于模型的强化学习和脉冲神经网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g15gw3/model_based_reinforcement_learning_and_spiking/</link>
      <description><![CDATA[有人知道是否有基于模型的强化学习和脉冲神经网络的相关论文吗？或者只是关于带有 snn 的模型的相关论文？    提交人    /u/Embri21   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g15gw3/model_based_reinforcement_learning_and_spiking/</guid>
      <pubDate>Fri, 11 Oct 2024 09:01:31 GMT</pubDate>
    </item>
    </channel>
</rss>