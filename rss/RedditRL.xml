<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 29 Jul 2024 06:22:31 GMT</lastBuildDate>
    <item>
      <title>为什么我的 PPO 算法没有学习？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</link>
      <description><![CDATA[我尝试了代码中几乎所有的超级参数，但奖励只停留在 8-11 之间，从未超出这个范围。我让它运行了 10-20 分钟，它仍然只在这两个值之间波动。我很困惑我做错了什么，请帮帮我，我花了一整天的时间调试，我打印出了演员的 nn.parameters，它们正在发生变化。 import torch from torch import nn from torchrl.envs import Compose, ObservationNorm, DoubleToFloat, StepCounter, TransformedEnv from torchrl.envs.libs.gym import GymEnv from torchrl.envs.utils import check_env_specs, set_exploration_type, ExplorationType from torchrl.modules import ProbabilisticActor, OneHotCategorical, ValueOperator from torchrl.collectors import SyncDataCollector from torchrl.data.replay_buffers import ReplayBuffer from torchrl.data.replay_buffers.storages import LazyTensorStorage from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement 从 torchrl.objectives.value 导入 GAE 从 torchrl.objectives 导入 ClipPPOLoss 从 tensordict.nn 导入 TensorDictModule torch.set_printoptions(threshold=16384) device=&quot;cuda&quot; base_env = GymEnv(&#39;CartPole-v0&#39;, device=device) env = TransformedEnv( base_env, Compose( ObservationNorm(in_keys=[&quot;observation&quot;]), DoubleToFloat(), StepCounter() ) ) env.transform[0].init_stats(1024) check_env_specs(env) actor_net = nn.Sequential( nn.Linear(env.observation_spec[&quot;observation&quot;].shape[-1], 256, device=device), nn.Sigmoid(), nn.Linear(256, 256, device=device), nn.Sigmoid(), nn.Linear(256, 256, device=device), nn.Sigmoid(), nn.Linear(256, env.action_spec.shape[-1],设备=设备））actor_module = TensorDictModule（actor_net，in_keys = [“observation”]，out_keys = [“logits”]）actor = ProbabilisticActor（module = actor_module，spec = env.action_spec，in_keys = [“logits”]，distribution_class = OneHotCategorical，return_log_prob = True）value_net = nn.Sequential（nn.Linear（env.observation_spec [“observation”]。shape [-1]，16，设备=设备），nn.Sigmoid（），nn.Linear（16，16，设备=设备），nn.Sigmoid（），nn.Linear（16，16，设备=设备），nn.Sigmoid（），nn.Linear（16，1，设备=设备） ）value_module = ValueOperator（module = value_net，in_keys = [“observation”]）frames_per_batch = 1024 total_frames = 1048576收集器 = SyncDataCollector（env，actor，frames_per_batch = frames_per_batch，total_frames = total_frames，split_trajs = True，reset_at_each_iter = True，设备=设备）replay_buffer = ReplayBuffer（存储 = LazyTensorStorage（max_size = frame_per_batch），采样器 = SamplerWithoutReplacement（））advantage_module = GAE（gamma = 0.99，lmbda = 0.95，value_network = value_module，average_gae = True）entropy_eps = 1e-4 loss_module = ClipPPOLoss（actor_network = actor，critic_network = value_module，clip_epsilon = 0.2，entropy_bonus = bool（entropy_eps），entropy_coef = entropy_eps）optim = torch.optim.Adam（loss_module.parameters（），lr = 1e-4）scheduler = torch.optim.lr_scheduler.CosineAnnealingLR（optim，total_frames // frames_per_batch）sub_batch_size = 64 for i，tensordict_data in enumerate（collector）：for _ in range（8）：advantage_module（tensordict_data）replay_buffer.extend（tensordict_data.reshape（-1）.cpu（））for _ in range（frames_per_batch // sub_batch_size）：data = replay_buffer.sample（sub_batch_size）loss = loss_module（data.to（device））loss_value = loss[“loss_objective”] + loss[&quot;loss_critic&quot;] + loss[&quot;loss_entropy&quot;] loss_value.backward() optim.step() optim.zero_grad() scheduler.step() if i % 16 == 0: with set_exploration_type(ExplorationType.MEAN), torch.no_grad(): rollout = env.rollout(1024, actor) print(rollout[&quot;next&quot;,&quot;reward&quot;].sum()) del rollout     提交人    /u/Unusual_Guidance2095   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1eer8iv/why_is_my_ppo_algorithm_not_learning/</guid>
      <pubDate>Mon, 29 Jul 2024 05:00:03 GMT</pubDate>
    </item>
    <item>
      <title>用于构建 RL 项目的简单可视化工具</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</link>
      <description><![CDATA[      我计划制作这个用于 RL 开发的简单工具。这个想法是快速构建和训练 RL 代理，无需代码。这对于快速开始新项目或轻松进行实验以调试 RL 代理非常有用。 目前设计中有 3 个选项卡：环境、网络和代理。我计划添加第四个选项卡，称为“实验”，用户可以在其中定义超参数实验并直观地查看每个实验的结果，以便调整代理。这个设计是一个非常早期的原型，可能会随着时间的推移而改变。 你们觉得怎么样？ https://preview.redd.it/sb5awqjys8fd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=d1046c3b7e195dba0b7779ee55f11c9330ec3d12    提交人    /u/Charming-Quiet-2617   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee525x/simple_visual_tool_for_building_rl_projects/</guid>
      <pubDate>Sun, 28 Jul 2024 11:12:20 GMT</pubDate>
    </item>
    <item>
      <title>关于 Stable Baselines3 和 AgileRL 的其他问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</link>
      <description><![CDATA[我尝试使用 SB3 实现我的 RL 算法，但是我对该框架中使用的术语感到很困惑。 我所有的旧 RL 算法都有一个这样的结构： untill episode &lt; max_episode_numbers: for step in max_steps_number: s&#39;, r, d, info = env.step 因此，基本上，我定义了最大 episode 数和最大 step 数，然后让算法运行。 现在在 SB3 中（以 PPO 算法 为例），有许多不同的术语，例如： n_epochs (int) – 优化替代损失时的 epoch 数 total_timesteps (int) – 要训练的总样本数（env 步骤） 据我所知，n_epochs 并不等同于我的 max_episode_numbers上面的示例中，但它是调用代理损失的优化的内部属性。 但如何定义要执行的最大情节数？即使他们论坛上有趣的线程也没有太大帮助。 我个人想出了类似以下的东西，但我不确定它是否有意义（我的想法来自这里）： for episode &lt; max_number_episode： model.learn(total_timesteps = 10_000) model.save(path_model) 更糟糕的是，我试图将该框架与 AgileRL 框架进行比较，后者看起来非常有前途。 在 AgileRL 中，明确定义了最大剧集数量的参数： &quot;EPISODES&quot;: 1000, # 要训练的剧集数量EPISODES&quot;: 1000, # 要训练的剧集数量 甚至是一集中的 max_number 步数： &quot;MAX_STEPS&quot;: 500, # 代理在环境中采取的最大步数 但是对于外部循环，它不采用情节数，而是采用步数。 # 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []# 训练循环 print(&quot;Training...&quot;) pbar = trange(INIT_HP[&quot;MAX_STEPS&quot;], unit=&quot;step&quot;) while np.less([agent.steps[-1] for agent in pop], INIT_HP[&quot;MAX_STEPS&quot;]).all(): pop_episode_scores = []  现在我完全对术语、它们的含义和功能感到困惑    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ee4clc/miscellaneous_questions_about_stable_baselines3/</guid>
      <pubDate>Sun, 28 Jul 2024 10:24:11 GMT</pubDate>
    </item>
    <item>
      <title>使用多臂老虎机的推荐系统参考资料</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</link>
      <description><![CDATA[我正在尝试学习如何将多臂老虎机应用到推荐系统中，但我不知道如何将原始的老虎机问题转换为有数据的场景。例如，这个库 https://github.com/fidelity/mab2rec 声称他们使用多臂老虎机，但他们没有解释如何使用。我试着阅读源代码，但我不太明白。 有人可以推荐一些资源来学习这种特定的应用程序吗？    提交人    /u/VanBloot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edsrq8/references_on_recommender_system_using_multiarmed/</guid>
      <pubDate>Sat, 27 Jul 2024 22:44:09 GMT</pubDate>
    </item>
    <item>
      <title>MADDPG 未学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1edo44n/maddpg_not_learning/</link>
      <description><![CDATA[大家好，我最近在简单的对手环境中训练了一个 MADDPG。模型开始运行，但结果很糟糕，代理没有学到任何东西。我已经尝试调试了好几个星期，但一直没有成功。 我知道提供的信息有限，很多事情都可能出错，但如果你感兴趣并帮助查看我的代码（在 Google Collab 上集成到一个页面中），我将不胜感激。提前致谢。 代码：https://colab.research.google.com/drive/1bRV803GR2vnjX0jy7bkTEy3A8VLYFPB6?usp=sharing    提交人    /u/TransportationOk2251   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1edo44n/maddpg_not_learning/</guid>
      <pubDate>Sat, 27 Jul 2024 19:14:40 GMT</pubDate>
    </item>
    <item>
      <title>用于构建和测试 RL 算法的软件</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ed62l3/software_used_for_building_and_testing_rl/</link>
      <description><![CDATA[在观看了 AI 击败各种游戏的视频后，我最近对强化学习 (RL) 着迷。我计划在接下来的几个月内完成一些深度学习项目后深入研究 RL。 我偶然看到了一段视频，其中 AI 在 Trackmania 中打破了多项世界纪录 ，我很好奇这些视频中用于设计汽车和赛道元素的软件。有人知道在 Trackmania 中构建这些环境和测试 RL 算法可能会使用什么工具或软件吗？ 提前感谢您的帮助！    提交人    /u/iam_raito   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ed62l3/software_used_for_building_and_testing_rl/</guid>
      <pubDate>Sat, 27 Jul 2024 02:46:56 GMT</pubDate>
    </item>
    <item>
      <title>如何管理巨大的行动空间？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ed0642/how_to_manage_huge_action_spaces/</link>
      <description><![CDATA[我对深度强化学习还很陌生。我正在尝试解决一个问题，其中代理学习在 NxN 网格中绘制矩形。这需要代理选择两个坐标点，每个坐标点都是 2 个数字的元组。动作空间多项式 N4。我目前使用 DQN 算法处理 N=4 的情况。在此算法中，神经网络输出动作的 N4 个 q 值。对于 20x20 网格，我需要一个具有 160,000 个输出的神经网络，这太荒谬了。我应该如何处理这种动作空间巨大的问题？参考论文也将不胜感激。    提交人    /u/medwatt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ed0642/how_to_manage_huge_action_spaces/</guid>
      <pubDate>Fri, 26 Jul 2024 21:58:57 GMT</pubDate>
    </item>
    <item>
      <title>利用机器的传感器来定义观察空间是否（总是）有意义？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecw0i3/does_make_always_sense_to_take_the_sensors_of_a/</link>
      <description><![CDATA[在许多机械臂示例和框架中，观察几乎是一本包含以下项目的字典： obs = {&#39;observation&#39;: [x, y, z], &#39;achieved_goal&#39;: [x, y, z], &#39;desired_goal&#39;: [x, y, z]&gt; 所以基本上末端执行器和目标的空间坐标被传递给网络。 但我的观点是：不太可能观察末端执行器在空间中的 3D 位置。是的，您可以使用相机并使用一些三角测量方法来检索末端执行器的位置。 但获取传感器读取的关节值不是更好吗？更有可能有一个编码器可以给我关节的旋转。这会更加现实，也更精确。 我为什么要问这个问题？因为理论上它应该可以简化很多问题，因为我观察到的空间会更小（一个能给我 +/- 120° 之间角度的编码器肯定比空间中的 3D 坐标更好）。    提交人    /u/WilhelmRedemption   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecw0i3/does_make_always_sense_to_take_the_sensors_of_a/</guid>
      <pubDate>Fri, 26 Jul 2024 18:59:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么在使用动作之前要将其与 action_scale 相乘？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</link>
      <description><![CDATA[我发现在很多 RL 例子中，动作都乘以了某些动作比例值。 这是为了从策略中调整动作的“影响力”吗？    提交人    /u/Open-Safety-1585   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecmdxl/why_are_actions_multiplied_with_action_scale/</guid>
      <pubDate>Fri, 26 Jul 2024 12:04:12 GMT</pubDate>
    </item>
    <item>
      <title>参加空气曲棍球挑战！构建并训练可以玩空气曲棍球的代理。击败您的竞争对手，赢取 3000 美元，并有机会在真正的机器人设置上试用您的代理。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</link>
      <description><![CDATA[        提交人    /u/elizabeth_duhh04   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecloyl/participate_in_the_air_hockey_challenge_build_and/</guid>
      <pubDate>Fri, 26 Jul 2024 11:26:31 GMT</pubDate>
    </item>
    <item>
      <title>如何实现风摩擦力作用于猎豹模型？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecjura/how_to_realize_wind_frictions_acting_on_the/</link>
      <description><![CDATA[嗨， 我是一名学生，我正在尝试使用 MuJoCo 在不同环境中训练基于模型的强化学习代理。主要目标是扩展基于模型的强化学习方法以处理非平稳环境，例如动态（例如改变质量、重力、增加风摩擦）和/或奖励（例如改变目标速度）随时间变化的环境。目前，我专注于 [Gymnasium](https://gymnasium.farama.org/environments/mujoco/) 中基于 MuJoCo 的环境 我正在寻求一些帮助来定义作用于 [Cheetah 模型](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/half\_cheetah\_v5.py) 的风摩擦。我目前的想法是添加额外的执行器，作用于模型移动方向上或相反方向的某些关节。 下面你只能看到我修改的 Cheetah 模型的一部分，即执行器部分。所有带齿轮参数设置的电机均来自原始 Cheetah 模型。此外，我定义了一个执行器连接到特定关节，其控制范围限制在 -50 到 50 之间。我将额外的执行器从代理的动作空间中排除，以便代理无法控制它们（请检查下面的函数），我在运行时明确设置它们的值。 我有两个问题：  我不确定是否需要为每个附加执行器设置齿轮参数？ 此外，根据文档，多个执行器作用于单个关节没有问题，但一切对我来说都很新，不知道我是否以正确的方式更改模式？  这是修改后的执行器部分： &lt;details&gt; ``` &lt;actuator&gt; &lt;motor gear=&quot;120&quot;关节=&quot;bthigh&quot; name=&quot;bthigh&quot;/&gt; &lt;马达齿轮=&quot;90&quot; 关节=&quot;bshin&quot; name=&quot;bshin&quot;/&gt; &lt;马达齿轮=&quot;60&quot; 关节=&quot;bfoot&quot; name=&quot;bfoot&quot;/&gt; &lt;马达齿轮=&quot;120&quot; 关节=&quot;fthigh&quot; name=&quot;fthigh&quot;/&gt; &lt;马达齿轮=&quot;60&quot; 关节=&quot;fshin&quot; name=&quot;fshin&quot;/&gt; &lt;马达齿轮=&quot;30&quot; 关节=&quot;ffoot&quot; name=&quot;ffoot&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;rootx&quot; name=&quot;frictionrootx&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bthigh&quot; name=&quot;frictionbthigh&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bshin&quot; name=&quot;frictionbshin&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;bfoot&quot; name=&quot;frictionbfoot&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;fthigh&quot; name=&quot;frictionfthigh&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-50 50&quot; joint=&quot;fshin&quot; name=&quot;frictionfshin&quot;/&gt; &lt;motor ctrllimited=&quot;true&quot; ctrlrange=&quot;-30 30&quot; joint=&quot;ffoot&quot; name=&quot;frictionffoot&quot;/&gt; &lt;/actuator&gt; ``` def exclude_wind_friction_from_action_space(self): &quot;&quot;&quot;用来实现风摩擦的额外执行器不应该是代理动作空间的一部分！&quot;&quot;&quot; bounds = self.model.actuator_ctrlrange.copy().astype(np.float32)[:-7] low, high = bounds.T self.action_space = Box(low=low, high=high, dtype=np.float32) &lt;/details&gt; 欢迎任何反馈/建议:)    由    /u/CertainLoad1589  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecjura/how_to_realize_wind_frictions_acting_on_the/</guid>
      <pubDate>Fri, 26 Jul 2024 09:30:06 GMT</pubDate>
    </item>
    <item>
      <title>如何设置 CORL 和 D4RL 数据集</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ecdsgd/how_to_setup_corl_and_d4rl_datasets/</link>
      <description><![CDATA[我正在尝试使用从 D4RL 下载的 maze2d-umaze-v1 数据集运行 sac_n.py。我在 sac_n.py 上使用 github 的 CORL 实现。  我是一名新手，正在尝试弄清楚如何使用下载的数据集（当前位于我的 ~/.d4rl/datasets 文件夹中）作为 sac_n.py 文件的输入。  我目前在 VScode 文件夹中将两个文件并排放在一起，但正在努力寻找有意义的突破，以便使用数据集下载文件作为 ORL 算法文件的输入。 提前感谢您的时间和考虑。    提交人    /u/Constant_Koala_7744   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ecdsgd/how_to_setup_corl_and_d4rl_datasets/</guid>
      <pubDate>Fri, 26 Jul 2024 03:07:12 GMT</pubDate>
    </item>
    <item>
      <title>使用纯强化学习制作国际象棋引擎的可行性</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ec76vw/feasibility_of_using_pure_rl_to_make_a_chess/</link>
      <description><![CDATA[今年夏天，我正在与当地一所大学合作进行强化学习研究，现在应该开始着手我的最终项目（2.5 周后完成）。我想知道仅使用强化学习来训练国际象棋引擎是否可行，无论是使用自对弈还是在某些随机游戏数据集上进行离线学习。我担心训练强化学习国际象棋引擎需要多少能力，而且我认为我无法使用与训练 AlphaZero、Leela Chess Zero 等相同类型的东西。同时，我也不会追求它们所展示的深度水平。训练如此大型的游戏是否可行？或者我应该尝试更简单的游戏（Connect 4、跳棋等）？    提交人    /u/dmann1945   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ec76vw/feasibility_of_using_pure_rl_to_make_a_chess/</guid>
      <pubDate>Thu, 25 Jul 2024 21:55:50 GMT</pubDate>
    </item>
    <item>
      <title>稳态误差补偿</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ebrr0k/steady_state_error_compensation/</link>
      <description><![CDATA[您好， 我正在使用 RL 和 DDPG 来控制无人机。总体而言，训练效果非常好，准确度高，误差很小（RMSE 为 0.008）。但是，稳态误差很小，为 0.5%。任务是将无人机从 5 米降落到 0 米，这个稳态误差很明显。 最初，我尝试减少步进时间，但问题几乎相同。然后，​​我读了一篇名为“基于 RL 的控制的稳态误差补偿”的论文。在这篇论文中，他们提出了一种扩展方法，可将稳态误差降低 52%。他们使用的方法涉及将积分组件合并到奖励函数中，这有助于通过惩罚代理不随时间减少稳态误差来最小化稳态误差。 我尝试了这种方法，但没有看到很大的改进。  所以我的问题是：1.强化学习能否实现0稳态误差，还是不可能？2.如果可以，我该怎么做才能实现？ 请参阅下面的奖励函数： ```matlab函数[reward，integralError] = rewardFunction（z，zt，integralError）％参数scaling_factor = 300;％积分分量的缩放因子integral_limit = 0.2;％限制积分以避免结束 ％计算距离（误差）dist = abs（z - zt）;％更新误差的积分integralError = integrationError + dist;％限制积分以避免结束integralError = max（min（integralError，integral_limit），-integral_limit）;％使用更平滑的梯度减少距离的基本奖励reward = 5 - 0.5 * dist; % 非常接近目标的额外奖励 if dist &lt; 1 奖励 = 奖励 + 35; end if dist &lt; 0.4 奖励 = 奖励 + 45; end if dist &lt; 0.01 奖励 = 奖励 + 55; end % 将积分部分合并到奖励中 奖励 = 奖励 - scaling_factor * integrationError;  end ``` 提前致谢！    提交人    /u/OkFig243   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ebrr0k/steady_state_error_compensation/</guid>
      <pubDate>Thu, 25 Jul 2024 10:43:34 GMT</pubDate>
    </item>
    <item>
      <title>我怎样才能在不同的状态下训练好 PPO？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ebmjnj/how_can_i_train_ppo_well_in_diverse_state/</link>
      <description><![CDATA[假设山地车环境是用随机生成的斜率和随机定位的目标初始化的。 在这种情况下，训练 PPO 有什么技巧？ 我目前正在做一个项目，它是一个优化问题（优化放射治疗计划中的辐射光束角度）。 我想训练代理来调整随机设置的光束角度。 而更困难的问题是，我想让代理优化光束，而不管任何患者的解剖几何形状如何。 所以我设置了自定义环境，在每个 env.reset() 中，环境都会设置不同的患者（当然是不同的肿瘤位置和大小）。 是否有可能训练 DRL 算法以在那种多样化的患者观察中很好地工作？ 如果可能的话，有什么技巧可以解决它？ 我在想的是为了在这些多样化的观察中表现良好，批量大小应该足够大以覆盖许多不同的患者几何形状。所以我目前将 n_steps（horizo​​n）设置为很大，但它还不起作用.. 我将保留我之前在 reddit 上讨论过的链接供您参考。 https://www.reddit.com/r/reinforcementlearning/comments/1eapb34/any_rl_study_about_observing_3d_data/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button    提交人    /u/MediocreAgency6070   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ebmjnj/how_can_i_train_ppo_well_in_diverse_state/</guid>
      <pubDate>Thu, 25 Jul 2024 05:00:15 GMT</pubDate>
    </item>
    </channel>
</rss>