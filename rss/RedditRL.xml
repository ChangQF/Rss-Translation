<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Wed, 15 May 2024 18:18:08 GMT</lastBuildDate>
    <item>
      <title>概率论书籍？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cspq7r/books_on_probability_theory/</link>
      <description><![CDATA[我对概率论在强化学习中的应用有足够直观的理解，我能理解数学，但这些并不那么容易，而且我缺乏很多问题练习，这可能有助于我更好地理解概念，现在我可以理解数学，但我无法自己重新推导或证明这些界限或引理，所以如果你对有关书籍有任何建议概率论，将不胜感激您的反馈。 （另外，我懒得学习经典概率论〜纯数学，因为如果我想探索工程中应用概率的任何其他领域，它会派上用场或物理学或其他应用概率部分）所以任何可以为我提供该领域强大的基础和稳健多样性的书。谢谢！   由   提交/u/vyknot4wongs  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cspq7r/books_on_probability_theory/</guid>
      <pubDate>Wed, 15 May 2024 16:57:00 GMT</pubDate>
    </item>
    <item>
      <title>TorchRL 的概率参与者没有进行概率采样......</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csjw4k/torchrls_probabilistic_actor_not_sampling/</link>
      <description><![CDATA[作为标题，概率参与者总是返回具有最大概率的动作... 代码： actor_net = nn.Sequential( nn.LazyConv2d(out_channels = 64, kernel_size = 5, stride=2, device=device), nn.Tanh() ， nn.LazyConv2d(out_channels = 32，kernel_size = 5，stride=2，device=device)， nn.Tanh()， nn .Flatten(start_dim=-3), nn.LazyLinear(num_cells, device=device), nn.Tanh(), nn.LazyLinear (env.action_spec.shape[-1], device=device), nn.Softmax(), ) policy_module = TensorDictModule( actor_net, in_keys=[“观察”], out_keys=[“概率”] ) policy_module = ProbabilisticActor(  module=policy_module， spec=env.action_spec， in_keys=[“probs”]， distribution_class=OneHotCategorical， return_log_prob=True, ) 我已经检查了概率是否具有足够高的熵（应该采取哪个操作几乎是随机的），当我我尝试通过代码跟踪它，但它包含在如此多的转换中，因此很难找到实际的采样器在哪里。我将进一步深入研究代码，但与此同时还有其他人遇到过这种情况吗？   由   提交/u/dagangsta2012   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csjw4k/torchrls_probabilistic_actor_not_sampling/</guid>
      <pubDate>Wed, 15 May 2024 12:43:53 GMT</pubDate>
    </item>
    <item>
      <title>Lstm/gru 与过去的观察结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csjvwx/lstmgru_vs_past_observations/</link>
      <description><![CDATA[我有一个 pomdp 控制问题，我使用 TD3 算法和 gru 或 lstm 解决了这个问题（两者都有效，但 Lstm 稍好一些）。 是否可以不使用循环神经网络，而是再次向代理提供过去的观察结果，以获得系统动态的知识？  我搜索了使用过去的观察而不是 RNN 来解决 pomdp 控制问题的论文，但我找到的所有论文都使用了 RNN。 我希望我的问题是可以理解的。   由   提交/u/1qaym0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csjvwx/lstmgru_vs_past_observations/</guid>
      <pubDate>Wed, 15 May 2024 12:43:36 GMT</pubDate>
    </item>
    <item>
      <title>Q 和价值函数有何不同？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csdwxu/how_does_q_and_value_function_differ/</link>
      <description><![CDATA[      https://preview.redd.it/vq1vjyg27j0d1.png?width=1641&amp;format=png&amp;auto=webp&amp;s=c793e6352 ec192fed1f8b0351ec03f57c74c468f 我知道价值函数会在不考虑行动的情况下给出预期的奖励总和，但我真的不明白它是如何工作的  &amp;# 32；由   提交/u/Ordinary_Big_8726   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csdwxu/how_does_q_and_value_function_differ/</guid>
      <pubDate>Wed, 15 May 2024 06:07:17 GMT</pubDate>
    </item>
    <item>
      <title>机器人 MARL</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1csdj5k/marl_for_robots/</link>
      <description><![CDATA[我正在寻找模拟移动机器人的竞争性 MARL。有相同的 3D 模拟器吗？   由   提交 /u/TopSimilar6673   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1csdj5k/marl_for_robots/</guid>
      <pubDate>Wed, 15 May 2024 05:41:26 GMT</pubDate>
    </item>
    <item>
      <title>零样本强化学习 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs85oi/zero_shot_reinforcement_learning_r/</link>
      <description><![CDATA[有人熟悉零样本强化学习的概念吗？Ahmed Touati 最近进行的一项名为“零样本强化吗？”的调查学习存在吗？今年推出   由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs85oi/zero_shot_reinforcement_learning_r/</guid>
      <pubDate>Wed, 15 May 2024 00:48:36 GMT</pubDate>
    </item>
    <item>
      <title>零样本强化学习 [R]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs84ou/zero_shot_reinforcement_learning_r/</link>
      <description><![CDATA[ 由   提交/u/Sea-Collection-8844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs84ou/zero_shot_reinforcement_learning_r/</guid>
      <pubDate>Wed, 15 May 2024 00:47:11 GMT</pubDate>
    </item>
    <item>
      <title>“鲁棒智能体学习因果世界模型”，Richens & Everitt 2024 {DM}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cs5xai/robust_agents_learn_causal_world_models_richens/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cs5xai/robust_agents_learn_causal_world_models_richens/</guid>
      <pubDate>Tue, 14 May 2024 23:03:34 GMT</pubDate>
    </item>
    <item>
      <title>强化学习来识别最大化公式的特征组合</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crtcl0/reinforcement_learning_to_identify_combinations/</link>
      <description><![CDATA[我的特征空间非常大，我想知道如何使用强化学习来优化公式。我对机器学习这个领域非常陌生，因此感谢所有建议和帮助。谢谢   由   提交/u/Naive_Yoghurt4959   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crtcl0/reinforcement_learning_to_identify_combinations/</guid>
      <pubDate>Tue, 14 May 2024 14:20:16 GMT</pubDate>
    </item>
    <item>
      <title>异构GNN</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crfg0g/heterogeneous_gnn/</link>
      <description><![CDATA[社区您好， 我目前正在将异构 GNN 应用于涉及作业和机器且作业和机器之间有连接的场景。然而，我观察到，即使工作的特征值不同，工作嵌入也包含所有工作的相同张量。聊天 GPT 告诉我，问题是因为作业节点连接到同一台机器。但特征值不同，所以我不知道如何解决这个问题，以便模型可以学习为免费机器选择作业。 你能帮我吗？    由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crfg0g/heterogeneous_gnn/</guid>
      <pubDate>Tue, 14 May 2024 00:49:28 GMT</pubDate>
    </item>
    <item>
      <title>一个时间步内执行多个操作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1crd71b/multiple_actions_in_one_timestep/</link>
      <description><![CDATA[我正在处理一个问题，其中代理操作是选择打开或关闭多个二进制开关（您可以想象输出是一个二进制数像 01011...）。我见过的大多数文献都涉及每个时间步一个动作，是否有一种简单的方法来实现我所缺少的这个动作空间？将每个可能的组合作为神经网络中的终端节点会很快失控，为 N 个开关提供 2N 个输出节点。 我目前的想法是使用像 A2C 这样的值方法，演员有 N 个输出节点，每个节点对应于打开开关的采样概率，但是，我不确定这会如何改变损失函数。 是否有一个简单的实现我失踪了？任何指示将不胜感激。   由   提交/u/Chewden_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1crd71b/multiple_actions_in_one_timestep/</guid>
      <pubDate>Mon, 13 May 2024 23:02:18 GMT</pubDate>
    </item>
    <item>
      <title>CleanRL PPO 不学习简单的双积分器环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cr1jze/cleanrl_ppo_not_learning_a_simple_double/</link>
      <description><![CDATA[我有一个代表双积分器的自定义环境。一开始将环境位置和速度都设置为0，然后选择一个目标值，目标是尽快减小位置与目标之间的差异。代理观察错误和速度。 我尝试使用 CleanRL 的 PPO 实现，但算法似乎无法学习如何解决环境问题，每集的平均回报随机从 -1k 跳到更大价值观。对我来说，这看起来是一个相当简单的环境，但我不知道为什么它不起作用，有人有任何解释吗？ class DoubleIntegrator(gym.Env): def __init__(self , render_mode=None): super(DoubleIntegrator, self).__init__() self.pos = 0 self.vel = 0 self.target = 0 self.curr_step = 0 self.max_steps = 300 self.terminate = False self.truncated =假 self.action_space =gym.spaces.Box(low=-1, high=1, shape=(1,)) self.observation_space =gym.spaces.Box(low=-5, high=5, shape=(2 ,)) def 步骤(self, 动作): 奖励 = -10 * (self.pos - self.target) vel = self.vel + 0.1 * 动作 pos = self.pos + 0.1 * self.vel self.vel = vel self.pos = pos self.curr_step += 1 如果 self.curr_step &gt; self.max_steps: self.terminate = True self.truncated = True 返回 self._get_obs(),reward,self.termerated,self.truncated,self._get_info() def reset(self,seed=None,options=None): self.pos = 0 self.vel = 0 self.target = np.random.uniform() * 10 - 5 self.curr_step = 0 self.terminate = False self.truncated = False 返回 self._get_obs(), self._get_info () def _get_obs(self): return np.array([self.pos - self.target, self.vel], dtype=np.float32) def _get_info(self): return {&#39;target&#39;: self.target, &#39; pos&#39;: self.pos}    由   提交/u/sauro97  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cr1jze/cleanrl_ppo_not_learning_a_simple_double/</guid>
      <pubDate>Mon, 13 May 2024 15:05:17 GMT</pubDate>
    </item>
    <item>
      <title>QL 如何解决而 DQN 无法解决？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqvb9d/how_ql_solves_but_dqn_cannot/</link>
      <description><![CDATA[我有一个可以通过 QL 成功解决的自定义环境，但无法使用 DQN 解决相同的环境（具有离散状态 - 离散操作）。  QL 可以解决问题，但 DQN 根本无法解决问题，这可能是什么原因？ DQN 是 QL 的神经网络改编版本，不是可以处理连续空间吗？当我尝试在相同环境下使用连续状态空间时，DQN 再次陷入困境。  这怎么可能？ 我的操作是 0 和 1，而代理始终只选择 1。这就像坚持同一个动作，这是完全糟糕的。   由   提交/u/OpenToAdvices96   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqvb9d/how_ql_solves_but_dqn_cannot/</guid>
      <pubDate>Mon, 13 May 2024 09:43:38 GMT</pubDate>
    </item>
    <item>
      <title>确定国际象棋代理的 DQN 架构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqm9kd/determining_dqn_architecture_for_a_chess_agent/</link>
      <description><![CDATA[嗨，我正在训练 RL 代理下棋 - 我正在使用 open_spiel 库，所以它更具连接性预制棋子（即国际象棋环境和 RL 模型已创建）。但是，我想知道如何设置模型超参数，特别是关于隐藏层的数量和每个隐藏层的节点数量。我应该如何解决这个问题？ TIA   由   提交/u/shadowknife392   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqm9kd/determining_dqn_architecture_for_a_chess_agent/</guid>
      <pubDate>Mon, 13 May 2024 00:26:06 GMT</pubDate>
    </item>
    <item>
      <title>“SOPHON: Non-Fine-Tunable Learning to Retrain Task Transferability For Pre-trained Models”，Deng et al 2024（MAML 在微调时会导致目标任务的灾难性遗忘）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cqdwn8/sophon_nonfinetunable_learning_to_restrain_task/</guid>
      <pubDate>Sun, 12 May 2024 18:05:11 GMT</pubDate>
    </item>
    </channel>
</rss>