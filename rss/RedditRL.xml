<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Tue, 12 Mar 2024 15:13:25 GMT</lastBuildDate>
    <item>
      <title>将矩形放置在区域（2D）中，同时优化多个功能并避免定义区域</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcy873/placing_rectangles_in_area_2d_while_optimizing/</link>
      <description><![CDATA[您好， 我们有一个可以放置对象（来自定义的对象列表）的区域。该区域由点定义（为简单起见，假设连接的四个点显示一个正方形）。在这个区域中，我们还可以有一些区域，其中不能放置对象（也不能重叠）。这让人想起二维装箱问题之一。然而，我们放置的物品也有价格和其他因素。我们希望以优化所有这些因素（低价等...）的方式放置对象。我过去做过很多机器学习，但从未做过强化学习。首先：这是一个 RL 是最佳解决方案的问题吗？对我来说似乎是这样，但我不确定...... 您对解决这个问题有什么建议或技巧吗？我正在考虑一种算法，该算法首先随机放置对象（在可以放置对象的区域内），同时因价格和其他可以从输出中推迟的事情而受到惩罚，以便它学习... &lt; p&gt;​ 你们觉得怎么样？ 提前谢谢！   由   提交 /u/Lucky_Funny_3259   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcy873/placing_rectangles_in_area_2d_while_optimizing/</guid>
      <pubDate>Tue, 12 Mar 2024 14:13:01 GMT</pubDate>
    </item>
    <item>
      <title>在isaacgymenv中，如何指定state和action中关节的顺序？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bctil2/in_isaacgymenv_how_to_specify_the_order_of_the/</link>
      <description><![CDATA[嗨，我是 RL 新手，刚刚开始使用 isaacgymenv 和 isaacgym。 在控制 CartPole 或 Franka 等机器人时，如何确定返回的观察或提供的操作中的哪个元素对应于 URDF 文件中定义的关节？  例如，在IsaacgymEnvs/isaacgymenvs/tasks/cartpole.py中 （如图所示），&#39;self.obs_buf&#39;中的各个元素是如何实现的 &lt; p&gt;对应于cartpole.urdf中定义的关节。 谢谢！ ​  &amp; #32；由   提交/u/Slight_Rip_516   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bctil2/in_isaacgymenv_how_to_specify_the_order_of_the/</guid>
      <pubDate>Tue, 12 Mar 2024 09:58:43 GMT</pubDate>
    </item>
    <item>
      <title>卡牌游戏的提示和指示</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bct4re/tips_and_pointers_for_card_game/</link>
      <description><![CDATA[嗨，我打算制作一个 RL 模型来玩澳门。这是我的第一个项目，所以我将不胜感激任何可以帮助我设计东西的指示和技巧。也许您会发现一些类似的设计视频。可能也可以使用一些文献，但很难找到能够涵盖我实际上认为有用的内容的文献。 有 Uno（澳门的简化版本，非常随机，玩家的选择并不重要）在澳门有这样的影响）模型已经在网上完成，但它太简单了。我需要更复杂的一张，我什至不知道是否可以做到...我认为对于澳门，我需要为代理制作一张桌子来收集每张牌的状态信息，例如：红心之王[是否在手，如果在手现在可以玩吗，如果不能直接玩可以批量玩吗（澳门允许一次玩多张相同值的牌），被对手持有的概率，在栈顶，使用并在堆栈中（重新洗牌后会重置）]我对如何设计该部分有一定的了解，但完全不知道如何管理代理所做的选择...而在 Uno 中，就像选择了一张可玩的牌或抽了一张牌在澳门打牌更像是抽一张牌（不管手上是否有可以打的牌（有时这比打出一张你能打的牌更好）），打运动员并设定价值要求，打A并设定颜色要求，玩战斗卡（就像uno中的通配符，但可以被反击）。 我越想它就越复杂，一个月内有最后期限，需要一些帮助🙏 &lt; /div&gt;  由   提交/u/Comfortable_Sleep988  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bct4re/tips_and_pointers_for_card_game/</guid>
      <pubDate>Tue, 12 Mar 2024 09:32:40 GMT</pubDate>
    </item>
    <item>
      <title>Sutton 的《强化学习：简介》第二版更好吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcrpib/is_the_2nd_edition_of_reinforcement_learning_an/</link>
      <description><![CDATA[嗨，我想阅读强化学习：萨顿和巴托的简介。然而，较新的第二版内容相当广泛。有谁知道第二版和第一版有没有很大的区别？   由   提交/u/d-eighties  /u/d-eighties  reddit.com/r/reinforcementlearning/comments/1bcrpib/is_the_2nd_edition_of_reinforcement_learning_an/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcrpib/is_the_2nd_edition_of_reinforcement_learning_an/</guid>
      <pubDate>Tue, 12 Mar 2024 07:52:46 GMT</pubDate>
    </item>
    <item>
      <title>如果你使用像 alphaGo 这样的东西作为 LLM 的奖励模型，会发生什么？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcr2sw/what_happens_if_you_use_something_like_alphago_as/</link>
      <description><![CDATA[这可能不是第一次有人问这个问题，但你能在 LLM 上进行强化学习并使用类似于奖励模型的 AlphaGo 吗？任何不是有效移动的令牌都会受到惩罚   由   提交 /u/rdyazdi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcr2sw/what_happens_if_you_use_something_like_alphago_as/</guid>
      <pubDate>Tue, 12 Mar 2024 07:08:31 GMT</pubDate>
    </item>
    <item>
      <title>“良好的表示足以实现样本高效强化学习吗？”，Du et al 2020</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcl32v/is_a_good_representation_sufficient_for_sample/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcl32v/is_a_good_representation_sufficient_for_sample/</guid>
      <pubDate>Tue, 12 Mar 2024 01:44:00 GMT</pubDate>
    </item>
    <item>
      <title>DQN 性能先上升后趋平</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcj5fm/dqn_performance_spikes_then_flatlines/</link>
      <description><![CDATA[      &lt; div class=&quot;md&quot;&gt;我正在根据本教程使用 DQN 代理。当在 cartpole 上训练它时，性能似乎一直在提高，直到平均总奖励达到 500（这是 cartpole 环境的最大值）。紧接着，在其余的训练中，性能崩溃并以非常一致的 90-100 平均奖励稳定下来。 我认为这很有趣，并且想知道是否有人有一个直观的原因为什么会出现这种情况发生了。谢谢！ 我的超参数： self.learning_rate = 0.0005 self.gamma = 0.99 self.batch_size = 128 p&gt; self.start_epsilon = .9 self.end_epsilon = .1 self.epsilon_decay = 40000 self.tau = 0.005 p&gt; self.clip_grad_value = 100 self.loss_criterion = nn.SmoothL1Loss() self.optimizer = AdamW(self.policy_net.parameters(),lr=self .learning_rate,amsgrad=True) ​ 这是训练期间评估数据的屏幕截图： https://preview.redd.it/kc9ic6dtpsnc1.png?width=1162&amp;format=png&amp;auto =webp&amp;s=c03202628c95595e2e86dc4bd89082a3c4421f61   由   提交 /u/PainindaAsh   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcj5fm/dqn_performance_spikes_then_flatlines/</guid>
      <pubDate>Tue, 12 Mar 2024 00:16:59 GMT</pubDate>
    </item>
    <item>
      <title>还有其他人在 Colab 中安装 torchrl 时突然遇到问题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcduht/anyone_else_suddenly_having_a_problem_with/</link>
      <description><![CDATA[我正在使用 torchrl 库在 Google Colab 中试验一些强化学习环境。几个月前，运行 !pip install torchrl 运行良好。现在，运行此命令会导致以下错误： 安装收集的软件包：triton、nvidia-nvtx-cu12、nvidia-nvjitlink-cu12、nvidia-nccl-cu12、nvidia-curand-cu12、 nvidia-cufft-cu12、nvidia-cuda-runtime-cu12、nvidia-cuda-nvrtc-cu12、nvidia-cuda-cupti-cu12、nvidia-cublas-cu12、nvidia-cusparse-cu12、nvidia-cudnn-cu12、nvidia- cusolver-cu12、torch、tensordict、torchrl 尝试卸载：triton 找到现有安装：triton 2.1.0 正在卸载 triton-2.1.0：成功卸载 triton-2.1.0 尝试卸载：torch 找到现有安装：torch 2.1.0+cu121 正在卸载torch-2.1.0+cu121：已成功卸载 torch-2.1.0+cu121 错误：pip 的依赖项解析器当前未考虑所有已安装的软件包。此行为是以下依赖性冲突的根源。 torchaudio 2.1.0+cu121 需要 torch==2.1.0，但您有 torch 2.2.1，这是不兼容的。 torchdata 0.7.0 需要 torch==2.1.0，但您拥有不兼容的 torch 2.2.1。 torchtext 0.16.0 需要 torch==2.1.0，但您有 torch 2.2.1，这是不兼容的。 torchvision 0.16.0+cu121 需要 torch==2.1.0，但您有 torch 2.2.1，这是不兼容的。  我尝试过 !pip install torch==2.1.0，但这不起作用，因为 torchrl 安装似乎卸载 torch 2.1.0 以支持 torch 2.2.1。我还尝试运行 !pip install torchrl 作为新笔记本和新运行时中的唯一命令，这会产生相同的结果。我怀疑这个问题与最近发布的 torch 2.2.1 有关，但我不确定如何解决它。有人遇到同样的问题，或者在新版本发布后能够在 Colab 中使用 torchrl 吗？   由   提交/u/brantacanadensis906   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcduht/anyone_else_suddenly_having_a_problem_with/</guid>
      <pubDate>Mon, 11 Mar 2024 20:45:26 GMT</pubDate>
    </item>
    <item>
      <title>[2403.04642]通过强化学习教授大型语言模型进行推理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bcbs6l/240304642_teaching_large_language_models_to/</link>
      <description><![CDATA[ 由   提交/u/LushousLightfoot  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bcbs6l/240304642_teaching_large_language_models_to/</guid>
      <pubDate>Mon, 11 Mar 2024 19:26:00 GMT</pubDate>
    </item>
    <item>
      <title>啤酒厂的人工智能和持续优化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bc7zes/ai_continuous_optimization_for_brewery/</link>
      <description><![CDATA[我将在一家啤酒厂的瓶颈装瓶和灌装生产线上启动一个优化项目。目标是在最短的停机时间和减速情况下生产最大数量的罐装和瓶装啤酒。我的经理希望这个项目能够结合人工智能+优化来完成，据他们说，这个人工智能系统应该根据来自MES、物联网、ERP的数据不断更新其决策和计划，并在不离开的情况下动态优化流程本身对规划者和运营者的优化。我猜经典的优化和机器学习方法在这里行不通，但除了强化学习之外我想不出任何其他方法。我正在等待您的建议。   由   提交 /u/robustersr2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bc7zes/ai_continuous_optimization_for_brewery/</guid>
      <pubDate>Mon, 11 Mar 2024 16:55:05 GMT</pubDate>
    </item>
    <item>
      <title>Tensorboard：“当前数据集没有活动的仪表板。”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bc3rdq/tensorboard_no_dashboards_are_active_for_the/</link>
      <description><![CDATA[大家好， 几天前我完成了我的第一个 RL 代理的设置，并直接开始训练它。我将日志文件保存到我的 anaconda 环境文件夹中，并使用 logdir 打开它们以查看张量板。一开始我遇到了问题，没有显示任何内容，但经过两个小时的重新启动和耐心之后，它突然起作用了... 现在，我将文件移动到云存储中，现在我想从中打开我的张量板会话。由于我移动了文件，因此出现了与以前相同的错误。没有显示任何数据，并显示消息“当前数据集没有活动的仪表板。” 如果你们中的任何人之前遇到过类似的问题，我将不胜感激。非常感谢！   由   提交 /u/QuiGon-GinTonic   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bc3rdq/tensorboard_no_dashboards_are_active_for_the/</guid>
      <pubDate>Mon, 11 Mar 2024 13:55:19 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bc0xf8/graph_neural_network/</link>
      <description><![CDATA[您好， 我正在研究灵活作业车间的生产调度（即，任务是安排操作，以便每个操作可以在许多机器上处理）。我通过将状态建模为基于动态图的结构来利用图神经网络（节点数量在每个决策时间根据可用作业和空闲机器而变化）。 我有三个主要问题： A.我应该选择哪个选项？ 1- 将状态建模为析取图（在许多学术论文中使用） 2- 将状态建模为图模型 (nx.DiGraph() ）并将状态作为 Data(x=, edge_index=) 馈送到 GNN B.哪一个更好？ 1- 训练 GAT 进行特征嵌入提取，然后训练 DRL 代理进行动作选择 2- 将 GAT 集成到 DRL 代理中，直接根据状态选择动作表示为数据，然后训练整个网络 C.我在图中有三个节点类别，如果输入表示为数据，GNN是否应该分别处理图中的三个节点类别中的每一个（这样我每次都有一个机器节点（多代理设置），几个作业节点, ...)?   由   提交/u/GuavaAgreeable208  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bc0xf8/graph_neural_network/</guid>
      <pubDate>Mon, 11 Mar 2024 11:25:49 GMT</pubDate>
    </item>
    <item>
      <title>监控 RL 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbzkai/monitoring_rl_agents/</link>
      <description><![CDATA[您好，我有两个一般性问题： 在训练和探索时监控 RL 代理性能的方法是什么？我知道奖励函数行为反映了 RL 的性能，但是，我们是否可以跟踪任何其他标准来了解代理的学习情况？ 最先进的安全方法是什么？除了用例特定方法之外的探索技术？   由   提交/u/alysavalan  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbzkai/monitoring_rl_agents/</guid>
      <pubDate>Mon, 11 Mar 2024 09:56:51 GMT</pubDate>
    </item>
    <item>
      <title>监控 RL 代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1bbzgth/monitoring_the_rl_agent/</link>
      <description><![CDATA[您好，我有两个一般性问题： 在训练和探索时监控 RL 代理性能的方法是什么？我知道奖励函数行为反映了 RL 的性能，但是，我们是否可以跟踪任何其他标准来了解代理的学习情况？ 最先进的安全方法是什么？除了用例特定方法之外的探索技术？   由   提交/u/raminhashemi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1bbzgth/monitoring_the_rl_agent/</guid>
      <pubDate>Mon, 11 Mar 2024 09:49:31 GMT</pubDate>
    </item>
    <item>
      <title>“无需搜索的大师级国际象棋”，Ruoss 等人 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1baz8hl/grandmasterlevel_chess_without_search_ruoss_et_al/</guid>
      <pubDate>Sun, 10 Mar 2024 02:25:15 GMT</pubDate>
    </item>
    </channel>
</rss>