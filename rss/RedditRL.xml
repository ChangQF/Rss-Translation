<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 29 Apr 2024 18:17:32 GMT</lastBuildDate>
    <item>
      <title>“初创公司 [Swaayatt、Minus Zero、RoshAI] 表示印度是测试自动驾驶汽车的理想之地”</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cg4cu9/startups_swaayatt_minus_zero_roshai_say_india_is/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cg4cu9/startups_swaayatt_minus_zero_roshai_say_india_is/</guid>
      <pubDate>Mon, 29 Apr 2024 17:04:44 GMT</pubDate>
    </item>
    <item>
      <title>具有优先体验重放的双 DQN 中的 TD 错误分配</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cg1y0l/td_error_assignment_in_double_dqn_with/</link>
      <description><![CDATA[这将如何工作？每个网络的观测值的 TD 误差会有所不同。我应该为每个观测值设置两个 TD 误差值并使用当前网络的一个，还是使用最小 TD 误差值并希望更准确的网络最终“教导”网络。另一个？   由   提交/u/AUser213  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cg1y0l/td_error_assignment_in_double_dqn_with/</guid>
      <pubDate>Mon, 29 Apr 2024 15:27:31 GMT</pubDate>
    </item>
    <item>
      <title>ep_rew_mean 不断减小</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cfojsr/the_ep_rew_mean_keeps_decreasing/</link>
      <description><![CDATA[      大家好，我定制了一个环境， 基本上，奖励函数是 1. 得分 2. 一些软约束以避免设计违规 3. 违规次数的加权和之前的状态到当前状态， 但看起来代理只学会了降低分数（如下所示） https://preview.redd.it/0tbygps61cxc1.png?width=1746&amp;format=png&amp;auto=webp &amp;s =cbdc97b3d0a014063f6a222a2a0fe313a6a797f7 由于某种原因，ep_rew_mean不断减小，如下所示。如果我没有误解的话，ep_rew_mean是每个时期的累积奖励的平均值， https://preview.redd.it/etzlprwr0cxc1.png?width=492&amp;format=png&amp;auto=webp&amp;s=b389c8a36b5fe81d81a90f1780 d246d5df6e83ee 其他训练图好像都正常？对吗？ https://preview .redd.it/5e0vf9bq0cxc1.png?width=1841&amp;format=png&amp;auto=webp&amp;s=b7b3a712413618ed92e72d6a51d4683b03c2deef 谢谢大家！   由   提交 /u/Nice_Charge8971   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cfojsr/the_ep_rew_mean_keeps_decreasing/</guid>
      <pubDate>Mon, 29 Apr 2024 02:52:55 GMT</pubDate>
    </item>
    <item>
      <title>随机 MuZero Chance 结果训练值问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cfju0c/stochastic_muzero_chance_outcomes_training_values/</link>
      <description><![CDATA[我最近偶然发现了随机 MuZero 论文。我真的很喜欢这篇论文的结果和整个过程。我了解网络的推论和MCTS规划。然而，我不明白机会结果的训练。有人可以解释一下吗？在 MCTS 中，西格玛变量表示该状态下机会结果的分布。这个分布是针对什么进行训练的？他们在论文中提到它是针对某些编码器进行训练的？网络中是否有额外的编码器用于此目的，或者他们如何知道实际发生了哪种机会结果？   由   提交 /u/_Hardric   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cfju0c/stochastic_muzero_chance_outcomes_training_values/</guid>
      <pubDate>Sun, 28 Apr 2024 23:01:48 GMT</pubDate>
    </item>
    <item>
      <title>(Crafter + NetHack) 在 JAX 中，速度快 15 倍，ascii 和像素模式</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cfjkmm/crafter_nethack_in_jax_15x_faster_ascii_and_pixel/</link>
      <description><![CDATA[       由   提交 /u/CellWithoutCulture   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cfjkmm/crafter_nethack_in_jax_15x_faster_ascii_and_pixel/</guid>
      <pubDate>Sun, 28 Apr 2024 22:50:07 GMT</pubDate>
    </item>
    <item>
      <title>在（国际象棋或将棋）中实现 alpha 0 是一个好的论文主题吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cf4dob/is_implementation_of_alpha_zero_in_chess_or_shogi/</link>
      <description><![CDATA[我应该写一篇关于数据科学/机器学习的论文，我选择了在（国际象棋或将棋）中实现 alphazero（尚未选择游戏） ）但是这是一个有效的论文主题吗？如果是的话，它是一个好的主题吗？如果您有任何主题想法，请提出任何建议谢谢   由   提交 /u/General_Arm_7352   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cf4dob/is_implementation_of_alpha_zero_in_chess_or_shogi/</guid>
      <pubDate>Sun, 28 Apr 2024 11:39:02 GMT</pubDate>
    </item>
    <item>
      <title>动作在 100 步后重复状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cf41pk/action_repeats_states_after_100_steps/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cf41pk/action_repeats_states_after_100_steps/</guid>
      <pubDate>Sun, 28 Apr 2024 11:18:46 GMT</pubDate>
    </item>
    <item>
      <title>“超越 A*：通过搜索动态 Bootstrapping 使用 Transformer 进行更好的规划”，Lehnert 等人 2024 {FB}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cerhkh/beyond_a_better_planning_with_transformers_via/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cerhkh/beyond_a_better_planning_with_transformers_via/</guid>
      <pubDate>Sat, 27 Apr 2024 22:58:33 GMT</pubDate>
    </item>
    <item>
      <title>在使用 RL 的火箭着陆环境中需要帮助</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ced23q/need_help_in_rocket_landing_environment_using_rl/</link>
      <description><![CDATA[嘿伙计们，希望你们一切顺利。我是 RL 新手，正在开发一个使用 pyflyt 火箭着陆健身房 的项目，我目前正在与 PPO 代理一起进行培训，但我没有得到很好的结果。一件好事是火箭至少正在非常接近着陆台的地方坠落。你们能帮我提供一些我可以尝试的想法和我可以使用的算法吗？预先感谢！ 我当前的 PPO 超参数是这些 policy_kwargs = { &quot;net_arch&quot;: [256, 256, 128], # 神经网络架构 } ppo_params = {“tensorboard_log”：“./”，“policy_kwargs”：policy_kwargs，“learning_rate”：0.0003，“clip_range”：0.2，“batch_size”：4096，“n_steps”：4096，“ gamma”: 0.99, “gae_lambda”: 0.95, “n_epochs”: 10, “ent_coef”: 0.01, “vf_coef”: 0.5, “max_grad_norm”: 0.5, }  &lt; /div&gt;  由   提交/u/Successful_Bug_5098   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ced23q/need_help_in_rocket_landing_environment_using_rl/</guid>
      <pubDate>Sat, 27 Apr 2024 12:10:27 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习约束</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cebn3g/deep_rl_constraints/</link>
      <description><![CDATA[有没有办法对 TD3 和 SAC 等与奖励函数无关的深度 RL 方法应用约束（即，除了惩罚违反行为的代理）约束）？   由   提交/u/Key-Scientist-3980   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cebn3g/deep_rl_constraints/</guid>
      <pubDate>Sat, 27 Apr 2024 10:45:49 GMT</pubDate>
    </item>
    <item>
      <title>DDPG 能解决高维环境吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ce8rwg/can_ddpg_solve_high_dimensional_environments/</link>
      <description><![CDATA[所以，我正在试验我的 DDPG 代码，发现它在低维状态动作空间（猎豹和漏斗）的环境中效果很好，但得到在高维空间上更糟（ant：111 + 8）。有没有人之前观察到类似的结果或者我的实现有问题？   由   提交/u/Interesting-Weeb-699   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ce8rwg/can_ddpg_solve_high_dimensional_environments/</guid>
      <pubDate>Sat, 27 Apr 2024 07:33:20 GMT</pubDate>
    </item>
    <item>
      <title>让智能体在不确定状态下继续概率转移的策略</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ce3z1p/policy_letting_the_agent_continue_the_probability/</link>
      <description><![CDATA[我研究机器人技术，是强化学习的初学者。基本上，我正在构建一系列用于应对不确定性的功能，并且我正在尝试弄清楚策略如何让代理在与故障、错误和故障相关的内部和外部状态中保持转换。简而言之，我想了解的是政策如何帮助应对不确定性，目前基本概念对我来说已经足够了。   由   提交 /u/Alive-Opportunity-23   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ce3z1p/policy_letting_the_agent_continue_the_probability/</guid>
      <pubDate>Sat, 27 Apr 2024 02:48:43 GMT</pubDate>
    </item>
    <item>
      <title>为什么损失如此之高，而我的模型似乎没有在二维空间的强化模型上学到任何东西</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cdl8zw/why_is_the_loss_so_high_and_my_model_is_seemingly/</link>
      <description><![CDATA[​ 我创建了一个根据此视频改编的模型： &lt; a href=&quot;https://www.youtube.com/watch?v=L8ypSXwyBds&amp;t=3720s&amp;pp=ygUecHl0b3JjaCByZWluZm9yY2VtZW50IGxlYXJuaW5n&quot;&gt;https://www.youtube.com/watch?v=L8ypSXwyBds&amp;t=3720s&amp;pp =ygUecHl0b3JjaCByZWluZm9yY2VtZW50IGxlYXJuaW5n 模型工作的环境是一个带有坦克式控件的可控球，一个可以推动的球和一个作为目标的方框 模型会因玩家触球而获得奖励，并根据迭代结束时球与球门的距离而获得奖励。 ​ 此外，模型似乎没有学习任何东西 ​ 我尝试离散化数据， 我&#39;我尝试过调整学习率， 我已经调整了奖励（尽管我可能调整得不够） ​ 仍然没有似乎没有学到任何东西， ​ 我不确定我的模型是否存在深刻的缺陷，或者模型是否不太适合这个领域因为它最初是为了在类似网格的空间中运行蛇而设计的，而这个游戏并不是在网格中构建的 ​ 链接到代码库：  https://github.com/jamiemitch121/RLNN-2D ​   由   提交/u/jam1211  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cdl8zw/why_is_the_loss_so_high_and_my_model_is_seemingly/</guid>
      <pubDate>Fri, 26 Apr 2024 13:20:29 GMT</pubDate>
    </item>
    <item>
      <title>是否有将棋的 MuZero 实现？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cdghrz/is_there_a_muzero_implementation_of_shogi/</link>
      <description><![CDATA[我想为将棋实现 MuZero 我寻找了将棋的 MuZero 实现，但没有找到任何理论，但没有实际实现本身。有人知道将棋 MuZero 实现的资源或指导吗？ 谢谢    提交人    /u/General_Arm_7352   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cdghrz/is_there_a_muzero_implementation_of_shogi/</guid>
      <pubDate>Fri, 26 Apr 2024 08:51:08 GMT</pubDate>
    </item>
    <item>
      <title>“法学硕士的偏好微调应该利用次优的、符合政策的数据”，Tajwar 等人，2024 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1cd8rxr/preference_finetuning_of_llms_should_leverage/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1cd8rxr/preference_finetuning_of_llms_should_leverage/</guid>
      <pubDate>Fri, 26 Apr 2024 01:25:53 GMT</pubDate>
    </item>
    </channel>
</rss>