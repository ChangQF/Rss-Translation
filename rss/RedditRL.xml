<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Sun, 10 Dec 2023 03:14:45 GMT</lastBuildDate>
    <item>
      <title>RL 比赛/世界纪录？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18em2dp/rl_competitions_world_records/</link>
      <description><![CDATA[嗨， 有一个致力于快速运行各种视频游戏的大型社区。我想知道强化学习是否有类似的东西，人们可以跟踪每场比赛的最佳机器人。人工智能可以像人类速通变体一样衡量速度，也可以衡量目前国际象棋中已经存在的竞争强度。  这样的社区存在吗？您认为现有的人工智能（例如 AlphaZero）会占主导地位吗？或者您是否认为现有的人工智能不会给业余程序员留下任何乐趣？  让我知道你的想法   由   提交/u/Aggravating_Lack_454   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18em2dp/rl_competitions_world_records/</guid>
      <pubDate>Sat, 09 Dec 2023 20:17:33 GMT</pubDate>
    </item>
    <item>
      <title>截断分位数批评 (TQC) 和 n 步学习算法的问题。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ekb5x/problem_with_truncated_quantile_critics_tqc_and/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ekb5x/problem_with_truncated_quantile_critics_tqc_and/</guid>
      <pubDate>Sat, 09 Dec 2023 18:55:43 GMT</pubDate>
    </item>
    <item>
      <title>自定义 pettingzoo 环境</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ego4t/custom_pettingzoo_environments/</link>
      <description><![CDATA[我正在疯狂地尝试使用我的自定义 pettingzoo （并行 API）环境。这是一个 Mario 64 环境，我花了很多时间来允许多个 Mario 并为每个人提供图像，但我真的无法让它与任何软件包一起工作。我尝试过 Agilerl、sb3、rllib 甚至 cleanRL。我知道 pettingzoo 不像体育馆那么成熟，但这太荒谬了。如果你喜欢 Mario 64，你可以尝试一下，如果你让它工作，请告诉我最新信息 https://github.com/ Gumbo64/sm64-AI Agilerl 可以运行，但只能运行 4 名或更少的玩家，我不知道它是否能正常学习，尽管我现在要把它留着过夜。由于某种原因，Ray rllib 在推出后冻结 我所做的随机动作测试工作得很好，不过，重置或动作/观察空间或其他什么都不应该成为问题   由   提交/u/Gumbo64  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ego4t/custom_pettingzoo_environments/</guid>
      <pubDate>Sat, 09 Dec 2023 16:04:56 GMT</pubDate>
    </item>
    <item>
      <title>如何通过仅在剧集结束时给出的奖励来升级AI</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18eeutp/how_to_upgrade_ai_with_rewards_only_given_at_the/</link>
      <description><![CDATA[例如，当我们只在剧集结束时给 AI 奖励，而不是针对它所做的每一个动作时，我们该怎么办AI 赢得了比赛，获得+1 奖励，如果输了，获得-1 奖励，这种情况下我们该怎么办？   由   提交/u/OneCommonMan123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18eeutp/how_to_upgrade_ai_with_rewards_only_given_at_the/</guid>
      <pubDate>Sat, 09 Dec 2023 14:35:19 GMT</pubDate>
    </item>
    <item>
      <title>PPO 的低奖励波动</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ebijk/low_reward_oscillations_in_ppo/</link>
      <description><![CDATA[我正在尝试实现 PPO 算法，其中每个情节只有 1 个步骤，采样时间为 800 秒（意味着每个情节长度为 800 秒）我获得的奖励在从低到高的奖励之间波动。 （参考图片）如何在训练时解决这个问题。    由   提交 /u/Wide-Chef-7011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ebijk/low_reward_oscillations_in_ppo/</guid>
      <pubDate>Sat, 09 Dec 2023 11:17:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 PPO 训练 LSTM 策略？伴随着复杂的动作</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18eb69c/how_to_train_a_lstm_policy_with_ppo_with_complex/</link>
      <description><![CDATA[您好， 我正在尝试了解如何实现具有多个操作作为输出的训练循环和策略（这是自回归预测的），例如LSTM（Seq2Seq，输入序列经过 N 个观察，输出序列复杂的动作类似于 Openai Five 中的做法 https://openai.com/研究/openai-5）。我所说的复杂动作是指定义了 N 种类型的动作，例如具有多个类别（例如，LSTM 输出序列可能用于移动动作，例如 hide_0 -&gt; move_action -&gt; offset_x -&gt; offset_y）。  让我担心的一件事是如何将这些操作映射到相应的对数概率并执行反向传播步骤，另一个问题是探索，因为单个操作空间显着爆炸，我假设需要的时间由于探索，训练这种策略要高得多。   由   提交/u/basic_r_user  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18eb69c/how_to_train_a_lstm_policy_with_ppo_with_complex/</guid>
      <pubDate>Sat, 09 Dec 2023 10:56:08 GMT</pubDate>
    </item>
    <item>
      <title>“Eureka：通过编码大型语言模型进行人性化奖励设计”，Ma 等人 2023 {Nvidia}</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dycua/eureka_humanlevel_reward_design_via_coding_large/</link>
      <description><![CDATA[ 由   提交/u/gwern  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dycua/eureka_humanlevel_reward_design_via_coding_large/</guid>
      <pubDate>Fri, 08 Dec 2023 22:22:22 GMT</pubDate>
    </item>
    <item>
      <title>开放世界中的学习课程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18du95j/learning_curricula_in_openended_worlds/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2312.03126 后记中列出的后续工作（第 7 章）。 代码&lt; /strong&gt;： https://github.com/facebookresearch/level-replay https://github.com/facebookresearch/dcd ACCEL 演示：https://accelagent.github.io/ 摘要：  深度强化学习（RL）为训练最佳顺序决策代理提供了强大的方法。由于收集现实世界的交互可能会带来额外的成本和安全风险，sim2real 的常见范例是在模拟器中进行训练，然后进行现实世界的部署。不幸的是，强化学习代理很容易过度适应模拟训练环境的选择，更糟糕的是，当代理掌握了一组特定的模拟环境时，学习就结束了。相比之下，现实世界是高度开放的，具有不断变化的环境和挑战，使得这种强化学习方法不适合。简单地对模拟环境进行随机化是不够的，因为它需要做出任意的分布假设，并且组合地采样对学习有用的特定环境实例的可能性较小。理想的学习过程应该自动适应训练环境，以在匹配或超越现实世界复杂性的开放式任务空间上最大限度地发挥智能体的学习潜力。本论文开发了一类名为无监督环境设计（UED）的方法，旨在产生这种开放式过程。给定环境设计空间，UED 在学习代理能力的前沿自动生成无限序列或训练环境课程。通过基于极小最大遗憾决策理论和博弈论的广泛实证研究和理论论证，本论文的研究结果表明，UED 自动课程可以产生 RL 代理，该代理对以前未见过的环境实例表现出显着提高的鲁棒性和泛化能力。这样的自动课程是通向开放式学习系统的有希望的途径，该系统通过不断生成和掌握自己设计的额外挑战来实现更通用的智能。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18du95j/learning_curricula_in_openended_worlds/</guid>
      <pubDate>Fri, 08 Dec 2023 19:16:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的力量：看看这个 DeepRL Sektor 模型如何在 DIAMBRA 竞赛平台上提交的视频中为《终极真人快打 3》找到一个智能、超酷的漏洞利用！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dpb57/the_power_of_reinforcement_learning_look_how_this/</link>
      <description><![CDATA[   /u/DIAMBRA_AIArena   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dpb57/the_power_of_reinforcement_learning_look_how_this/</guid>
      <pubDate>Fri, 08 Dec 2023 15:31:26 GMT</pubDate>
    </item>
    <item>
      <title>[D] 寻找基于语言的机器人/嵌入式人工智能方向的实验室</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dmos1/d_looking_for_labs_working_in_the_direction_of/</link>
      <description><![CDATA[大家好，我来自欧洲一所知名大学，攻读机器人学硕士学位。我对在机器人技术中使用视觉语言模型感兴趣，目前正在研究使用 CLIP 嵌入进行基于对象的映射和导航。 一些论文可能会让您更多地了解我所说的基于语言的机器人技术的含义：- 去任何地方，Gervet 等人。 - 牧场上的奶牛：语言驱动的零样本对象导航的基线和基准，Gadre 等人。 - 用于机器人导航的视觉语言地图，Huang 等人。 我正在尝试寻找该领域的博士职位，并寻找从事此工作的实验室。根据我的研究，我发现这个方向的大多数论文都来自美国的实验室（Sergey Levine、Dhruv Batra、Jitendra Malik、Pieter Abeel 等教授） 我很惊讶地发现只有欧洲有一个实验室在这一领域开展工作，该实验室位于德国弗莱堡。 （Wolfram Burgard） 如果有人知道在该领域工作的实验室，可以为我提供更多提示，我将非常感激。我主要对欧洲感兴趣，但如果您知道我的列表中没有的美国实验室，请随时发表评论！   由   提交/u/Bluebird705  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dmos1/d_looking_for_labs_working_in_the_direction_of/</guid>
      <pubDate>Fri, 08 Dec 2023 13:21:03 GMT</pubDate>
    </item>
    <item>
      <title>在几秒钟内学会飞行</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18df2os/learning_to_fly_in_seconds/</link>
      <description><![CDATA[       由   提交/u/jonas-eschmann   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18df2os/learning_to_fly_in_seconds/</guid>
      <pubDate>Fri, 08 Dec 2023 04:52:42 GMT</pubDate>
    </item>
    <item>
      <title>通过 4 个开创性项目向初学者介绍强化学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dbmy6/a_beginners_intro_to_rl_through_4_seminal_projects/</link>
      <description><![CDATA[   /u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dbmy6/a_beginners_intro_to_rl_through_4_seminal_projects/</guid>
      <pubDate>Fri, 08 Dec 2023 01:49:29 GMT</pubDate>
    </item>
    <item>
      <title>“利用基于优势的离线策略梯度改进语言模型”，Baheti 等人，2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18dagry/improving_language_models_with_advantagebased/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18dagry/improving_language_models_with_advantagebased/</guid>
      <pubDate>Fri, 08 Dec 2023 00:49:32 GMT</pubDate>
    </item>
    <item>
      <title>我的定制体育馆环境似乎根本没有学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18d8tkc/my_custom_gymnasium_env_seems_not_learning_at_all/</link>
      <description><![CDATA[训练了一整天似乎没有学到是强化学习还是环境问题 https://github.com/jonnytracker/Flappy-Bird-RL &lt;!-- SC_ON - -&gt;  由   提交/u/jonnytracker2020   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18d8tkc/my_custom_gymnasium_env_seems_not_learning_at_all/</guid>
      <pubDate>Thu, 07 Dec 2023 23:29:03 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Gumbel-softmax 与 TD3 一起使用？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18d8ib1/why_does_gumbelsoftmax_work_with_td3/</link>
      <description><![CDATA[我的假设是“在单臂老虎机设置中，选择正确的箱子将返回奖励 1，返回函数将成为一块”明智的常数函数。因此，critic的梯度几乎到处都是0”。 但是，我尝试了。 TD3 以 Gumbel-softmax 作为输出层。它会学习！我不知道为什么它会学习。  此外，设置温度太低无法学习&#39; 谁能解释一下发生了什么以及我错过了什么？谢谢   由   提交/u/Lopside_Hall_9750   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18d8ib1/why_does_gumbelsoftmax_work_with_td3/</guid>
      <pubDate>Thu, 07 Dec 2023 23:14:25 GMT</pubDate>
    </item>
    </channel>
</rss>