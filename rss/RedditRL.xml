<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 27 May 2024 09:16:07 GMT</lastBuildDate>
    <item>
      <title>如何确定重播缓冲区的最小大小</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</link>
      <description><![CDATA[我正在统一训练一个 dqn，它使用 [偏航、横向矢量和前向矢量] 作为我的动作空间来控制游戏对象。我读到人们在开始训练之前获得了重放缓冲区的最低经验数（在某些帖子中，读到该最小值高达 10000）。我发现我的模型基本上对每个推理都给出了相同的动作，因为它现在完全未经训练。在我看来，在开始训练之前收集大量此类数据基本上是多余的，因为它并没有真正尝试各种操作。如果我的批量大小为 64，如果我想使用包含（假设为 256）个经验的重播缓冲区开始训练，有什么需要注意的吗？我假设这将帮助我在早期增加训练数据的多样性。 我是一个菜鸟，所以我非常愿意参考可能与此相关的论文，提前谢谢。  p&gt;   由   提交 /u/ProtectionFrosty5393   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1jsx5/how_do_i_determine_the_minimum_size_of_replay/</guid>
      <pubDate>Mon, 27 May 2024 04:55:29 GMT</pubDate>
    </item>
    <item>
      <title>我可以在电子商务网站中使用强化学习进行产品推荐吗？有关于这个主题的可用资源吗？或任何其他最佳选择。</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</link>
      <description><![CDATA[嗨，我是一家餐饮/电子商务服务提供商的实习生，我被指派为 The F&amp;B/E-commerce 服务提供商创建推荐引擎。 B和电子商务网站，建议我一些关于基于RL的推荐系统的好的指导材料或任何其他效果好的方法，注意：数据有限哪个可能是最好的？   由   提交/u/Educational-Town-710   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d1hst1/can_i_use_reinforcement_learning_for_product/</guid>
      <pubDate>Mon, 27 May 2024 02:53:59 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关通过电池和电网交互构建能源管理 RL 环境的建议和见解</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</link>
      <description><![CDATA[大家好！ 我正在参与一个项目，重点是使用强化学习来模拟和优化能源管理系统。目标是管理电网和可再生能源的能源存储和消耗，并密切关注实时定价和能源需求。 这种设置代表了我在给定情况下提出的最佳方法。数据集和项目的紧急情况。（我愿意接受任何想法） 系统概述： 状态空间包括：  两个电池的充电状态 (SoC)。 当前电网电价。 能源使用的历史和预测数据。 &lt; li&gt;未来 6 小时的可再生能源发电预测。 一天中的时间和一周中的日期指示器。  行动空间涉及：  确定每个电池充电的电量。 （2 个连续值） 安排充电操作的开始时间（选项范围从立即到延迟 6 小时）。  数据和迭代： 我已经转换了我的数据集，以表示每小时的所有相关信息，例如负载、价格和可再生能源预测。环境中的每个模拟步骤对应一小时的实时时间，其中模型根据当前状态预测开始充电的最佳时间以及充电量。 主要挑战：&lt; /strong&gt; 延迟操作： 例如，如果 2024 年 1 月 1 日 00:00:00 决定在 5 小时后开始充电，我应该如何：  考虑操作对系统的延迟影响来计算奖励？ 更新模型的预测： 针对几小时后影响系统的操作，确定最佳方案模型重新评估和做出新预测的时间令人困惑。 学习的数据迭代： 假设环境的每个步骤处理来自转换后的数据集的一小时的数据：&lt; /p&gt; 如何确保 RL 模型有效地迭代数据集以实现最大程度的学习？ 建议采用哪些策略来处理这种每小时数据的连续流，特别是在集成操作的延迟效果时模型的学习过程？ 我正在寻找有关在强化学习环境中管理延迟操作和有效数据迭代的见解、建议或资源。 感谢您的指导和时间!   由   提交 /u/Nnarruqt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d118db/seeking_advice_and_insights_on_building_an_rl/</guid>
      <pubDate>Sun, 26 May 2024 13:20:52 GMT</pubDate>
    </item>
    <item>
      <title>如何改进深度 RL 交易设置，使其在 1 小时时间范围内效果良好，但在 100 万时间范围内效果不佳？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d112xj/how_to_improve_a_deep_rl_setup_for_trading_that/</link>
      <description><![CDATA[嗨， 几个月来，我一直在研究如何设置 RL 模型进行交易。 无需详细了解设置本身（本质上我能够轻松配置我想要测试的所有参数），我有 RL 模型，我可以将其提供给经过处理的时间序列并让它们执行操作。 &lt; p&gt;到目前为止，我一直在针对 BTCUSDT 进行测试，主要是在 1 小时时间范围内，假设复利，我可以以 2 倍左右的速度击败 HODL（所以我的测试数据是 2024 年 1 月至 4 月，其中 HODL 似乎得到大约 41,000 美元，而我的模型可以达到 &gt; 81,000 美元）。 这还假设每次买卖都会产生 %0.1 的费用（以模拟经纪商当前的 SPOT 费用）。 大多数模型的交易都没有错误（每笔交易都盈利）。 现在，这一切看起来都很有希望，但有两个问题：  1) 大多数模型在这 4 个月内进行大约 60-90 笔交易，这意味着有时每 2 天只有一笔交易。对于在现实生活中与经纪人进行测试来说，这是一个问题，因为我必须等待很长时间才能看到任何操作。 2) 我尝试在 1m 时间范围上训练相同的精确设置，但结果远不及 1 小时。我尝试了许多配置（例如显示 1m + 1h 或 1m + 1h + 1d 时间范围），但似乎要处理的数据量增加，大大降低了模型学习方式的影响（事实上，在很多情况下模型执行 0 个动作）。使用学习率会有所帮助 - 但我似乎永远无法达到 1 小时帧得到的结果。 2 个问题： 1）有人有关于如何进行的任何提示吗？处理如此高频的数据，为什么与 1 小时的结果相比会有如此大的差异？ （我们甚至不讨论 1 秒的时间范围 :) ）  2）看来我开发的奖励系统运行良好，我很高兴讨论它，但也许有人知道如何做激励 RL 模型进行更多交易？在大多数情况下，这些模型似乎倾向于更大/更安全的波动，而不是更频繁地交易，这将显示复利的力量。我最近读到了有关多重奖励系统（矢量化奖励）的内容，但没有一个可用的库支持它（线性“近似”它基本上就是我现在正在做的事情，但实际上并不是同一件事）。 感谢您就此事提供的任何意见或讨论。 PS。我还为经纪人配置了自动交易设置，我目前正在其上运行 1 小时模拟（在他们的测试环境中），但该环境不是最好的（由于那里处理交易的方式），所以我只是可能必须在那里上线并进行测试。   由   提交 /u/cloudjubei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d112xj/how_to_improve_a_deep_rl_setup_for_trading_that/</guid>
      <pubDate>Sun, 26 May 2024 13:12:53 GMT</pubDate>
    </item>
    <item>
      <title>学士论文游戏</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</link>
      <description><![CDATA[嘿，我想为我的计算机科学学士学位论文训练一个人工智能来玩强化学习游戏。 我还没有强化学习的经验。 我可以选择哪些当时可行的游戏？   由   提交/u/TMG_Indi   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0x25q/games_for_bachelor_thesis/</guid>
      <pubDate>Sun, 26 May 2024 08:40:15 GMT</pubDate>
    </item>
    <item>
      <title>环境复杂性与最优策略收敛的关系</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</link>
      <description><![CDATA[大家好，是否有一些关于环境复杂性与学习到的最优策略本身之间关系的文献？例如，如果一个环境是由“世界模型”中的VAE生成的，那么环境复杂度和策略之间的关系是什么？   由   提交/u/Main_Pressure271   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0voo9/relation_between_environment_complexity_and/</guid>
      <pubDate>Sun, 26 May 2024 06:58:00 GMT</pubDate>
    </item>
    <item>
      <title>经常性 SAC 指南</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</link>
      <description><![CDATA[我一直在尝试了解有关 LSTM 在 POMDP 强化学习中如何发挥作用的更多信息。我专门尝试与 SAC 合作，想知道是否有一些关于该主题的好资源。    由   提交 /u/Spiritual_Basket8332   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0vhmu/recurrent_sac_guidance/</guid>
      <pubDate>Sun, 26 May 2024 06:43:50 GMT</pubDate>
    </item>
    <item>
      <title>最优随机策略是否存在？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</link>
      <description><![CDATA[我知道在 MDP 中总是存在唯一的最优确定性策略。对于最优随机策略也存在这样的说法吗？是否总是存在唯一的最优随机策略？它能比最优确定性策略更好吗？我想我不太明白。 谢谢！   由   提交 /u/jthat92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0uz9x/existence_of_optimal_stochastic_policy/</guid>
      <pubDate>Sun, 26 May 2024 06:07:26 GMT</pubDate>
    </item>
    <item>
      <title>观察空间中的矩阵（强化学习）</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</link>
      <description><![CDATA[如果我希望代理显示 4 个空间，其中每个空间有 10 个组件，每个组件有 3 个变量。为观察空间定义一个矩阵是不是更好？因为这告诉我要做 ChatGPT    由   提交/u/Gullible_Capital_146   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0t40k/matrices_in_observation_space_reinforcement/</guid>
      <pubDate>Sun, 26 May 2024 04:02:53 GMT</pubDate>
    </item>
    <item>
      <title>“静息大脑标签记忆中的电‘涟漪’用于存储”：体验重播机制和选择睡眠期间优先重播的点</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</link>
      <description><![CDATA[   /u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0mfgg/electric_ripples_in_the_resting_brain_tag/</guid>
      <pubDate>Sat, 25 May 2024 21:48:03 GMT</pubDate>
    </item>
    <item>
      <title>部分循环观察空间的最佳库？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</link>
      <description><![CDATA[假设我有一个环境，其中代理必须在 2D 平面上移动来收集硬币，硬币的数量各不相同。代理可以在四个基本方向中的任何一个方向上加速，并在收集硬币时获得 1 的奖励，然后将其从环境中移除。观察空间如下所示： Agent x Agent y Agent vx Agent vy [Coin x, Coin y] * 硬币数量 我的假设是处理这个问题的方法是使用变压器 - 使用前馈网络对固定组件的表示进行编码，然后是每个非固定组件之一（我还可以使用变压器来生成每种类型的固定长度编码对象并将其与固定组件连接起来，但直观地将所有内容放入同一个变压器中应该效果更好，因为上下文使变压器变得有用）。本质上，这意味着编写一个自定义状态空间编码器，我认为两个大库（Stable Baselines 和 Rllib）都支持。伪代码如下所示： defencode(obs): generic, coin = obs # a 1 x 4 np array and a k x 2 np array coin_emb = self.coin_embed(c) # a前馈层映射 2 到嵌入维度 gen_emb = self.general_embed(general) # 4 --&gt; emb_dim编码= self.encoder(torch.stack(gen_emb,coins_emb))#torch.nn.TransformerEncoder，接受Nxemb_dim输入，产生Nxh_dim输出encoded=encoded[0]#BERT将输出用于特殊的[CLS] ] 标记作为其固定长度输出。在这里，我们使用输出作为开始标记。 return generated def policy(obs): return self.policy_net(encode(obs)) def value(obs): return self.value_net(encode(obs))  我记得OpenAI的隐藏并寻求环境使用池化（特征编码器之后的 IIRC 最大池化）而不是变压器，但那是不久前的事了。无论如何，我的问题的核心是是否有人对实现这种自定义观察网络时使用的最佳堆栈有建议。如果有人见过这样的项目（越新越好 - 语法似乎像季节一样变化），我也非常感谢 github 链接。 谢谢！   由   提交 /u/Dry-Sock7131    reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0g01x/best_library_for_partiallyrecurrent_observation/</guid>
      <pubDate>Sat, 25 May 2024 16:41:35 GMT</pubDate>
    </item>
    <item>
      <title>教人形机器人用头部弹球的教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</link>
      <description><![CDATA[您好，刚刚在 github 上发布了一个新教程 - https ://github.com/goncalog/ai-robotics。您的反馈会很棒！   由   提交/u/goncalogordo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0fw35/tutorial_on_teaching_a_humanoid_to_bounce_a_ball/</guid>
      <pubDate>Sat, 25 May 2024 16:36:23 GMT</pubDate>
    </item>
    <item>
      <title>强化学习自定义环境引擎</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</link>
      <description><![CDATA[我目前正在尝试使用 Isaac sim 构建用于强化学习的自定义环境。我已经构建了模型，但我不知道如何将其导入 VS Code 以便我真正对环境进行编程，而且我找不到任何相关教程。我也想知道我是否应该使用 mojuco 代替？但我真的不知道如何用它创建模型。   由   提交/u/Teaser_404  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a4jo/reinforcement_learning_custom_environment_engine/</guid>
      <pubDate>Sat, 25 May 2024 11:39:40 GMT</pubDate>
    </item>
    <item>
      <title>多智能体强化学习结果不佳</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1d0a08b/multi_agent_rl_bad_results/</guid>
      <pubDate>Sat, 25 May 2024 11:32:13 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND（扩散作为环境梦想的模型）是在扩散世界模型中训练的强化学习代理</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</link>
      <description><![CDATA[   /u/clumma  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1czxw85/diamond_diffusion_as_a_model_of_environment/</guid>
      <pubDate>Fri, 24 May 2024 23:02:06 GMT</pubDate>
    </item>
    </channel>
</rss>