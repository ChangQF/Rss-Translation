<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 30 Dec 2024 15:17:09 GMT</lastBuildDate>
    <item>
      <title>触发环境重置的明确奖励[体育馆和稳定基线3]</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpnubr/explicit_reward_for_triggering_env_reset/</link>
      <description><![CDATA[大家好， 提前感谢大家的帮助！ 当我的代理导致环境重置（低于阈值）时，我想应用特定惩罚。我不明白的是，我可以正确触发重置，但惩罚没有应用，奖励是按常规计算的。如果您能指出我在某处误解了结构的话，那就太好了：) step() 伪代码： #action 提取 #action 处理 #updating 值 #reward 计算 # penalty 检查 if value1 &lt;= Threshold: Terminated = True self.reward = -200 # Override reward with penalty observer = self._get_observation() return observer, self.reward, Terminated, Triuncated, {}     submitted by    /u/Much_Razzmatazz_6641   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpnubr/explicit_reward_for_triggering_env_reset/</guid>
      <pubDate>Mon, 30 Dec 2024 14:02:13 GMT</pubDate>
    </item>
    <item>
      <title>无限武装老虎机问题的遗憾下限</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hpalfe/lower_bound_on_regret_in_infinitelyarmed_bandit/</link>
      <description><![CDATA[只要臂空间是连续/无限的，或者我们不对拉动每个臂的效用做任何假设，遗憾的下限应该是 Omega(T)，对吗（假设奖励在 [0, 1] 之间）？ 众所周知，在 K 臂老虎机问题中，T 轮累积遗憾的下限是 Omega(sqrt(KT))。如果没有对效用做任何假设，只是给定一个臂，则该臂的奖励是独立且相同分布的。在无限多臂老虎机中，我们有 K 趋向于无穷大，因此 Omega(sqrt(KT)) 下限变为无界，并且我们知道对于老虎机问题，遗憾最多为 T，因此最坏情况遗憾的下限应该是 Omega(T)。  我没见过任何地方有这样的说法，但也许很明显没有这样说。不过，我明白这并不意味着在每个连续体或无限多的武装匪徒问题中遗憾都是 Omega(T)，因为我们可以对导致更严格遗憾界限的效用做出假设。    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hpalfe/lower_bound_on_regret_in_infinitelyarmed_bandit/</guid>
      <pubDate>Mon, 30 Dec 2024 00:42:27 GMT</pubDate>
    </item>
    <item>
      <title>迷宫/最短路径环境的奖励结构</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hp8ns5/reward_structure_for_maze_shortest_path/</link>
      <description><![CDATA[嗨， 我正在构建一个环境，玩家必须穿过一个随机生成的迷宫，激活迷宫随机部分的开关，然后导航到出口。玩家的移动是连续的（即不是基于网格的）。 我一直在研究一种信息丰富的、有形状的奖励结构，以鼓励学习最短路径。 目前，我的奖励结构如下： - 每帧减 0.01 - 根据玩家与目标（出口开关或出口门，如果开关被激活）之间距离与前一步的差异，每帧给予小幅（&lt;0.5）奖励或惩罚 - 到达出口给予大额奖励（10） 在训练之前，我将奖励标准化为 0 到 1 之间。但是，这里似乎可能存在一些冗余，我想问问大家的想法，以及是否有更好的方式来构建奖励。 作为参考，这是一个模拟游戏 N++ 的环境。 谢谢大家的帮助！    由    /u/Tetramputechture  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hp8ns5/reward_structure_for_maze_shortest_path/</guid>
      <pubDate>Sun, 29 Dec 2024 23:10:51 GMT</pubDate>
    </item>
    <item>
      <title>RL 书籍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hp4nb7/rl_books/</link>
      <description><![CDATA[我开始学习 RL。这个领域最好的书籍或文章是什么。    提交人    /u/fg-dev   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hp4nb7/rl_books/</guid>
      <pubDate>Sun, 29 Dec 2024 20:12:58 GMT</pubDate>
    </item>
    <item>
      <title>我的 DQN 代理怎么会这么弱智？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hova0i/how_my_dqn_agent_can_be_so_rtarded/</link>
      <description><![CDATA[很抱歉出现这个标题，但我真的很沮丧。我真的请求一些帮助，并弄清楚我遗漏了什么...... 我正在尝试教我的 DQN 代理学习最简单的控制器问题，遵循所需的值。 我正在模拟一个只有 1 个状态和 3 个动作的淋浴环境。  目标 = 达到所需的温度范围。 状态 = 当前温度 动作 = 增加（+1），Noop（0），减少（-1） 如果温度为 [36, 38]，则奖励 = +1，否则 -1 重置 = 20 + random.randint(-5, 5)  我的 DQN 代理实际上无法学习世界上最简单的问题。 这怎么可能？ Q-Learning 可以学习这个。DQN 算法有什么不同？ DQN 不是试图近似最佳 Q 函数吗？换句话说，试图模仿正确的 Q 表，但使用函数而不是查找表？ 我的干净代码在这里。我想了解到底发生了什么，以及为什么我的代理无法学到任何东西！ 谢谢！ 代码： from stable_baselines3.common.callbacks import BaseCallback from stable_baselines3 import DQN import numpy as np import gym import random from gym import space from gym.spaces import Box class ShowerEnv(gym.Env): def __init__(self): super(ShowerEnv, self).__init__() # 动作空间：减少、保持、增加 self.action_space = space.Discrete(3) # 观察空间：温度 self.observation_space = Box(low=np.array([0], dtype=np.float32), high=np.array([100.0], dtype=np.float32)) # 设置起始温度 self.state = 20 + random.randint(-5, 5) # 设置淋浴长度 self.shower_length = 100 def step(self, action): # 应用操作 ---&gt; [-1, 0, 1] self.state += action - 1 # 将淋浴长度减少 1 秒 self.shower_length -= 1 # 保护边界状态条件 if self.state &lt; 0: self.state = 0 reward = -1 # 保护边界状态条件 elif self.state &gt; 100：self.state = 100 reward = -1 # 如果状态在边界状态条件内 else：# 温度条件的期望范围 if 36 &lt;= self.state &lt;= 38：reward = 1 # 温度条件的非期望范围 else：reward = -1 # 检查情节是否结束 if self.shower_length &lt;= 0：done = True else：done = False info = {} return np.array([self.state]), reward, done, {} def render(self, action=None): pass def reset(self): self.state = 20 + random.randint(-50, 50) self.shower_length = 100 return np.array([self.state]) class SaveOnEpisodeEndCallback(BaseCallback): def __init__(self, save_freq_episodes, save_path, verbose=1):超级（SaveOnEpisodeEndCallback，self）。__init__（verbose）self.save_freq_episodes = save_freq_episodes self.save_path = save_path self.episode_count = 0 def _on_step（self）-&gt; bool：如果self.locals [&#39;dones&#39;] [0]：self.episode_count + = 1如果self.episode_count％self.save_freq_episodes == 0：save_path_full = f＆quot; {self.save_path} _ep_ {self.episode_count}＆quot; self.model.save（save_path_full）如果self.verbose＆gt; 0：print（f&quot;模型在剧集 {self.episode_count}&quot;）如果 __name__ ==&quot;__main__&quot;则返回 True：env = ShowerEnv() save_callback = SaveOnEpisodeEndCallback(save_freq_episodes=25，save_path=&#39;./models_00/dqn_model&#39;) logdir =&quot;logs&quot;模型 = DQN(policy=&#39;MlpPolicy&#39;, env=env, batch_size=32, buffer_size=10000, exploration_final_eps=0.005, exploration_fraction=0.01, gamma=0.99, gradient_steps=32, learning_rate=0.001, learning_starts=200, policy_kwargs=dict(net_arch=[16, 16]), target_update_interval=20, train_freq=64, verbose=1, tensorboard_log=logdir) 模型.学习(total_timesteps=int(1000000.0), reset_num_timesteps=False, callback=save_callback, tb_log_name=&quot;DQN&quot;)     提交人    /u/OpenToAdvices96   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hova0i/how_my_dqn_agent_can_be_so_rtarded/</guid>
      <pubDate>Sun, 29 Dec 2024 12:42:39 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle 和 Colab 上可用的 GPU 是否足以学习 Deep RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1houfkt/will_gpu_available_on_kaggle_and_colab_be_enough/</link>
      <description><![CDATA[大家好， 我正在考虑深入研究深度强化学习。我无法在本地使用强大的 GPU。 所以我有这个问题，Kaggle 和 Colab 上可用的 GPU 是否有助于学习和探索所有不同的算法。深度强化学习的样本效率还不够高。 我见过人们训练 2M+ 或更多步骤才能获得结果。 谢谢。    提交人    /u/mono1110   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1houfkt/will_gpu_available_on_kaggle_and_colab_be_enough/</guid>
      <pubDate>Sun, 29 Dec 2024 11:45:56 GMT</pubDate>
    </item>
    <item>
      <title>我如何使用 carla 进行 RL？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</link>
      <description><![CDATA[我的毕业设计用carla完成强化学习，能推荐一些在线课程吗？    submitted by    /u/Clean_Tip3272   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hotgct/how_can_i_use_carla_to_rl/</guid>
      <pubDate>Sun, 29 Dec 2024 10:36:18 GMT</pubDate>
    </item>
    <item>
      <title>遗憾值为 O( \sqrt(log K T ) ) 的 K 臂随机赌博机算法？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</link>
      <description><![CDATA[我想知道是否有任何 K 臂随机老虎机算法可以实现 $O(\sqrt(T))$ 遗憾，且因子为常数 $\sqrt{ log K }$。  我知道 exp3 可以实现 O(\sqrt(T)) 遗憾，因子为 sqrt(k log K )，而 UCB 可以实现 \tilde{O}( sqrt(T) ) 遗憾，因子为 sqrt(k)？  是否有一种算法在臂数方面具有像 sqrt( log K ) 这样的因子？或者是否有更严格的 exp3 或 UCB 分析，可以在臂数方面实现更好的因子？  我正在研究一个问题，其中臂的数量为 K^{a}，其中 a 是某个参数，并且我想将我的因子归结为类似 a * poly(K) - （poly(K) 表示关于 K 的多项式）的东西。    提交人    /u/Anxious_Positive3998   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hoplm5/karmed_stochastic_bandit_algorithms_with_o/</guid>
      <pubDate>Sun, 29 Dec 2024 05:59:36 GMT</pubDate>
    </item>
    <item>
      <title>似乎无法理解如何使用 NEAT-Python 结果</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hokvmb/cant_seem_to_understand_how_to_work_with/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hokvmb/cant_seem_to_understand_how_to_work_with/</guid>
      <pubDate>Sun, 29 Dec 2024 01:33:15 GMT</pubDate>
    </item>
    <item>
      <title>RL“包裹” 2024</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</link>
      <description><![CDATA[我通常会在假期的最后几天努力赶上进度（事实证明这些天不可能）并回顾学术和工业发展方面的主要亮点。请在此处添加您今年的顶级 RL 作品     提交人    /u/blitzkreig3   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hofcye/rl_wrapped_2024/</guid>
      <pubDate>Sat, 28 Dec 2024 21:08:08 GMT</pubDate>
    </item>
    <item>
      <title>山地车项目</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hobshj/mountain_car_project/</link>
      <description><![CDATA[      我正在尝试使用 Q 学习、DQN 和 Soft Actor Critic 解决山地车问题。 我设法在离散空间中使用 Q 学习解决了该问题，但在调整 DQN 时，我发现训练图并不像 Q 学习那样收敛。相反，它相当不稳定。但是，当我使用情节长度和回报来评估策略时，我发现大多数种子情节都很短并且奖励更高。这是否意味着我解决了它？ 参数是： {&#39;env&#39;: &lt;gymnax.environments.classic_control.mountain_car.MountainCar at 0x7b368faf7ee0&gt;, &#39;env_params&#39;: {&#39;max_steps_in_episode&#39;: 200, &#39;min_position&#39;: -1.2, &#39;max_position&#39;: 0.6, &#39;max_speed&#39;: 0.07, &#39;goal_position&#39;: 0.5, &#39;goal_velocity&#39;: 0.0, &#39;force&#39;: 0.001, &#39;gravity&#39;: 0.0025}, &#39;eval_callback&#39;: &lt;function RLinJAX.algos.algorithm.Algorithm.create。&lt;locals&gt;.eval_callback(algo, ts, rng)&gt;，&#39;eval_freq&#39;：5000，&#39;skip_initial_evaluation&#39;：False，&#39;total_timesteps&#39;：1000000，&#39;learning_rate&#39;：0.0003，&#39;gamma&#39;：0.99，&#39;max_grad_norm&#39;：inf，&#39;normalize_observations&#39;：False，&#39;target_update_freq&#39;：800，&#39;polyak&#39;：0.98，&#39;num_envs&#39;：10，&#39;buffer_size&#39;：250000，&#39;fill_buffer&#39;：1000，&#39;batch_size&#39;：256， &#39;eps_start&#39;：1，&#39;eps_end&#39;：0.05，&#39;exploration_fraction&#39;：0.6，&#39;agent&#39;：{&#39;hidden_​​layer_sizes&#39;：（64,64），&#39;activation&#39;：&lt;PjitFunction&gt;，&#39;action_dim&#39;：3，&#39;parent&#39;：None，&#39;name&#39;：None}，&#39;num_epochs&#39;：5，&#39;ddqn&#39;：True}  对学习到的政策的评估 编辑：我打印了短剧集百分比和高奖励剧集百分比： 短剧集百分比 99.718 高奖励百分比 99.718    提交人    /u/Ordinary_Reveal8842   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hobshj/mountain_car_project/</guid>
      <pubDate>Sat, 28 Dec 2024 18:26:30 GMT</pubDate>
    </item>
    <item>
      <title>你喜欢观看 RL 内容视频吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1ho23ty/do_you_love_watching_rl_content_videos/</link>
      <description><![CDATA[基本上就是标题，但更具体地说： 我喜欢观看机器人学习某些东西（在模拟中）的内容，这比教育意义更有趣。 尤其是来自“AI Warehouse”这样的频道，其中创建者对 AI（立方体/假人）提出挑战，并描述所玩的“游戏”，并附加有趣的评论。 由于这个 subreddit 更倾向于教育而不是娱乐，我想知道你们中有多少人像我（男，27 岁）一样喜欢这些？ （+ 年龄/性别）？    提交人    /u/LoveYouChee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1ho23ty/do_you_love_watching_rl_content_videos/</guid>
      <pubDate>Sat, 28 Dec 2024 09:15:53 GMT</pubDate>
    </item>
    <item>
      <title>RL 是否用于训练游戏《黎明杀机》中的机器人？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnsh5d/was_rl_used_to_train_the_bots_in_the_game_dead_by/</link>
      <description><![CDATA[在这个帖子和这个中有很多讨论，但似乎没人知道 - 开发人员对此没有发表任何评论。我也在游戏的 discord 服务器中询问过，但也没人知道。 他们可以使用强化学习来训练它们吗？我对强化学习的了解非常基础，我现在正在尝试研究它（我刚刚开始了解深度 Q 学习）。这似乎是可能的，因为我知道 RL 已经用于很多游戏（尽管我见过的例子都是老式游戏）。    提交人    /u/gitgud_x   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnsh5d/was_rl_used_to_train_the_bots_in_the_game_dead_by/</guid>
      <pubDate>Fri, 27 Dec 2024 23:46:39 GMT</pubDate>
    </item>
    <item>
      <title>强化学习的第一步</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnqx9l/first_step_in_rl/</link>
      <description><![CDATA[如何开始学习/做 RL？ - 用什么方法学习？ - 了解哪些 hello world 项目？ - 学习 RL 的步骤是什么？ - 如果我想从零开始成为 RL 的英雄，我该怎么做？    提交人    /u/Soft_Awareness6826   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnqx9l/first_step_in_rl/</guid>
      <pubDate>Fri, 27 Dec 2024 22:34:09 GMT</pubDate>
    </item>
    <item>
      <title>AAAI 2025 教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</link>
      <description><![CDATA[AI 安全：从强化学习到基础模型 链接：https://aaai.org/conference/aaai/aaai-25/tutorial-and-lab-list/#TQ10     提交人    /u/ml_dnn   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1hnehkr/aaai_2025_tutorial/</guid>
      <pubDate>Fri, 27 Dec 2024 13:07:15 GMT</pubDate>
    </item>
    </channel>
</rss>