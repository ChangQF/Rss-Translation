<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>å¼ºåŒ–å­¦ä¹ </title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>å¼ºåŒ–å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½/ç»Ÿè®¡å­¦çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºæ¢ç´¢/ç†è§£å¤æ‚çš„ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä½³åœ°è·å¾—å¥–åŠ±ã€‚ä¾‹å¦‚ AlphaGoã€ä¸´åºŠè¯•éªŒå’Œ A/B æµ‹è¯•ä»¥åŠ Atari æ¸¸æˆã€‚</description>
    <lastBuildDate>Tue, 02 Jan 2024 15:15:29 GMT</lastBuildDate>
    <item>
      <title>[R] å¤§è¯­è¨€æ¨¡å‹ä¸–ç•Œå›½é™…è±¡æ£‹é”¦æ ‡èµ›ğŸ†â™Ÿï¸ (GPT-4 > Gemini-Pro)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18wez9h/r_large_language_models_world_chess_championship/</link>
      <description><![CDATA[ ç”±   æäº¤ /u/gwern   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18wez9h/r_large_language_models_world_chess_championship/</guid>
      <pubDate>Tue, 02 Jan 2024 03:11:03 GMT</pubDate>
    </item>
    <item>
      <title>é¢å¤–è®­ç»ƒ RL ç®—æ³•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w7vf0/additional_training_a_rl_algorithm/</link>
      <description><![CDATA[æˆ‘æ­£åœ¨è®­ç»ƒ RL æ¨¡å‹ã€‚æˆ‘æƒ³çŸ¥é“æ˜¯å¦å¯ä»¥è®©æ¨¡å‹ä¸€æ¬¡ä»…ä½¿ç”¨ä¸€ä¸ªå¥–åŠ±å‡½æ•°è¿›è¡Œå­¦ä¹ ï¼Œç„¶åå–æ¶ˆæ³¨é‡Šå¹¶ä½¿å…¶ä½¿ç”¨å¦ä¸€ä¸ªäº’æ–¥çš„å‡½æ•°è¿›è¡Œå­¦ä¹ ï¼Ÿç†è®ºä¸Šå¯è¡Œå—ï¼Œæˆ‘å¦‚ä½•åœ¨ä»£ç ä¸­å®ç°å®ƒã€‚  æˆ‘æ˜¯æ–°æ‰‹ã€‚ æˆ‘çš„è®­ç»ƒåˆ¶åº¦ï¼š Name = rf&#39;Agents_Allignment{SimulationVariables[&quot;SimAgents&quot;]}_PPO_{SimulationVariables [â€œLearningTimeStepsâ€]}&#39; env = DummyVecEnv([lambda: FlockingEnv()]) model = PPO(â€œMlpPolicyâ€, env,tensorboard_log=â€œ./ppo_Agents_tensorboard/â€, verbose=1) model.learn( Total_timesteps=SimulationVariables[â€œLearningTimeStepsâ€]) # è°ƒæ•´ä¹˜æ•° # ä¿å­˜æ¨¡å‹ model.save(Name) env.close() # åŠ è½½æ¨¡å‹ env = FlockingEnv() model = PPO.load(Name) # è¿è¡Œ 10èŒƒå›´å†…å‰§é›†çš„å‰§é›†ï¼ˆ1ï¼ŒRLVariables[&#39;Episodes&#39;]ï¼‰ï¼šobs = env.resetï¼ˆï¼‰å®Œæˆ=é”™è¯¯å¥–åŠ±= 0 Position_dict = {iï¼š[] for i in rangeï¼ˆlenï¼ˆenv.agentsï¼‰ï¼‰}æ—¶é—´æ­¥é•¿= 0reward_log=[] print(â€œEpisodeâ€,episode) # å®Œæˆæ¡ä»¶ while((timestep &lt;=SimulationVariables[â€œEvalTimeStepsâ€]) and (not did)): action, state = model.predict(obs) obs,reward,done,info = env.step(action) ######### env.step() #æ·»åŠ ç¢°æ’é€€å‡ºæ¡ä»¶reward_log.append(reward) print(reward) for i, agent in enumerate(env.agents): Positions_dict[i].append(agent.position.tolist()) with open(rf&#39;{Results[&quot;EpRewards&quot;]}_Allignment_{episode}.json&#39;, &#39;w&#39;) as f : json.dump(reward_log, f, indent=4) timestep = timestep + 1 # print(reward_log) with open(rf&#39;agent_positionsTestAllignment_{episode}.json&#39;, &#39;w&#39;) as f: #æ·»åŠ åˆ°å‚æ•°æ–‡ä»¶ json. dump(positions_dict, f, indent=4) env.close()  â€‹   ç”±   æäº¤/u/Sadboi1010   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w7vf0/additional_training_a_rl_algorithm/</guid>
      <pubDate>Mon, 01 Jan 2024 21:53:51 GMT</pubDate>
    </item>
    <item>
      <title>COOMï¼šæŒç»­å¼ºåŒ–å­¦ä¹ çš„æ¸¸æˆåŸºå‡†</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w24o8/coom_a_game_benchmark_for_continual_reinforcement/</link>
      <description><![CDATA[è®ºæ–‡ï¼šhttpsï¼š //openreview.net/forum?id=qmCxdPkNsa ä»£ç ï¼šhttps ://github.com/hyintell/COOM è§†é¢‘ï¼šhttps://www.youtube.com/watch?v=FUm2B8MZ6d0 æ‘˜è¦ï¼š  è¿›æ­¥æŒç»­å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸€ç›´é¢ä¸´ç€å„ç§éšœç¢ï¼ŒåŒ…æ‹¬æ ‡å‡†åŒ–çš„æŒ‡æ ‡å’Œè¯„ä¼°åè®®ã€è‹›åˆ»çš„è®¡ç®—è¦æ±‚ä»¥åŠç¼ºä¹å¹¿æ³›æ¥å—çš„æ ‡å‡†åŸºå‡†ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†COOMï¼ˆContinual DOOMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸ºåŸºäºåƒç´ çš„å¼ºåŒ–å­¦ä¹ é‡èº«å®šåˆ¶çš„è¿ç»­å¼ºåŒ–å­¦ä¹ åŸºå‡†ã€‚ COOM æä¾›äº†ä¸€å¥—åœ¨è§†è§‰ä¸Šä¸åŒçš„ 3D ç¯å¢ƒä¸­ç²¾å¿ƒè®¾è®¡çš„ä»»åŠ¡åºåˆ—ï¼Œä½œä¸ºä¸€ä¸ªå¼ºå¤§çš„è¯„ä¼°æ¡†æ¶æ¥è¯„ä¼°æŒç»­å¼ºåŒ–å­¦ä¹ çš„å…³é”®æ–¹é¢ï¼Œä¾‹å¦‚ç¾éš¾æ€§é—å¿˜ã€çŸ¥è¯†è½¬ç§»å’Œæ ·æœ¬é«˜æ•ˆå­¦ä¹ ã€‚åœ¨å¯¹æµè¡Œçš„æŒç»­å­¦ä¹ ï¼ˆCLï¼‰æ–¹æ³•è¿›è¡Œæ·±å…¥çš„å®è¯è¯„ä¼°åï¼Œæˆ‘ä»¬æŸ¥æ˜äº†å®ƒä»¬çš„å±€é™æ€§ï¼Œæä¾›äº†å¯¹åŸºå‡†çš„å®è´µè§è§£ï¼Œå¹¶å¼ºè°ƒäº†ç‹¬ç‰¹çš„ç®—æ³•æŒ‘æˆ˜ã€‚è¿™ä½¿å¾—æˆ‘ä»¬çš„å·¥ä½œæˆä¸ºç¬¬ä¸€ä¸ªåœ¨å…·æœ‰å…·ä½“æ„ŸçŸ¥çš„ 3D ç¯å¢ƒä¸­å¯¹åŸºäºå›¾åƒçš„ CRL è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„å·¥ä½œã€‚ COOM åŸºå‡†çš„ä¸»è¦ç›®æ ‡æ˜¯ä¸ºç ”ç©¶ç•Œæä¾›æœ‰ä»·å€¼ä¸”å…·æœ‰æˆæœ¬æ•ˆç›Šçš„æŒ‘æˆ˜ã€‚å®ƒæ—¨åœ¨åŠ æ·±æˆ‘ä»¬å¯¹å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­å½“å‰å’Œå³å°†æ¨å‡ºçš„ CL æ–¹æ³•çš„åŠŸèƒ½å’Œå±€é™æ€§çš„ç†è§£ã€‚ä»£ç å’Œç¯å¢ƒæ˜¯å¼€æºçš„ï¼Œå¯ä»¥åœ¨ GitHub ä¸Šè®¿é—®ã€‚    ç”±   æäº¤ /u/APaperADay   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w24o8/coom_a_game_benchmark_for_continual_reinforcement/</guid>
      <pubDate>Mon, 01 Jan 2024 17:53:01 GMT</pubDate>
    </item>
    <item>
      <title>å…³é—­ç­–ç•¥ç­–ç•¥æ¢¯åº¦å®šç†</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/</link>
      <description><![CDATA[å—¨ï¼Œæˆ‘çœŸçš„å¾ˆæƒ³é€è¡Œç†è§£ç¦»ç­–ç•¥ç­–ç•¥æ¢¯åº¦ç®—æ³•ã€‚ æœ¬æ–‡ç”± Degris æ’°å†™ï¼Œ T.ã€æ€€ç‰¹ã€M. å’Œè¨é¡¿ï¼ŒR.S. (2012).è®ºæ–‡é“¾æ¥ï¼š(https://arxiv.org/pdf/1205.4839.pdf) å› æ­¤ï¼Œåœ¨è®ºæ–‡çš„ 2.2 èŠ‚ä¸­ï¼Œä½œè€…æŒ‡å‡ºï¼Œåœ¨ç¦»ç­–ç•¥ pg ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡çœç•¥å…¨æ¢¯åº¦å…¬å¼ä¸­çš„é™„åŠ é¡¹æ¥ä½¿ç”¨çœŸå® pg çš„è¿‘ä¼¼å€¼ã€‚  ç°åœ¨ï¼Œåœ¨é™„å½• A ä¸­ï¼Œä½œè€…è¯•å›¾é¦–å…ˆåœ¨å„å›½å…±äº«ä¸€ä¸ªå‚æ•°åŒ–æ”¿ç­–çš„å‘é‡ u çš„ä¸€èˆ¬æƒ…å†µä¸‹è¯æ˜è¿™ä¸€ç‚¹ã€‚  æˆ‘ç†è§£ç¬¬ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨åœ¨ä¸åŒçŠ¶æ€å’ŒåŠ¨ä½œå¯¹è¯„ä¼°çš„åŠ æ€§æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ï¼Œæ–°å‚æ•°æœ€ç»ˆå°†ä¸ºæˆ‘ä»¬æä¾›æ›´é«˜çš„ç›®æ ‡å‡½æ•°ã€‚åœ¨æ­¤ç›®æ ‡ä¸­ï¼ŒçŠ¶æ€å’ŒåŠ¨ä½œå¯¹çš„ä»·å€¼å‡½æ•°ä¿æŒä¸å˜ï¼Œä½†æ˜¯å…·æœ‰è¾ƒé«˜ä»·å€¼çš„ $Q{\pi_u, \gamma}(s,a)$ åœ¨ $\ pi_{u&#39;, \gamma}$ã€‚  ä½†æ˜¯ï¼Œæˆ‘æ— æ³•å®Œå…¨ç†è§£ï¼Œå¹¶ä¸”æˆ‘æ­£åœ¨åŠªåŠ›ä»¥ä¸€ç§éå¸¸æ•°å­¦ä¸Šç¨³å¥çš„æ–¹å¼çœ‹å¾…å®ƒï¼Œä¸ºä»€ä¹ˆå¦‚æœæˆ‘ä»¬å¼€å§‹ä½¿ç”¨$\pi_{u&#39;, \gamma}$ ä¾æ¬¡ã€‚  æœ¬è´¨ä¸Šè®©æˆ‘å›°æƒ‘çš„æ˜¯è¯æ˜ä¸­çš„æ”¿ç­–æ”¹è¿›éƒ¨åˆ†ï¼ˆå‚è§é™„å›¾2ï¼‰ã€‚   ç”±   æäº¤ /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/&quot;&gt;[é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w1fvv/off_policy_policy_gradient_theorem/</guid>
      <pubDate>Mon, 01 Jan 2024 17:22:29 GMT</pubDate>
    </item>
    <item>
      <title>ä¸å¤ªæ¨¡ç³Šçš„å¥–åŠ±å‡½æ•°</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18w09rn/less_ambiguous_reward_function/</link>
      <description><![CDATA[æˆ‘çš„å¥–åŠ±å‡½æ•°ä¸­æœ‰ä¸‰ä¸ªç‹¬ç«‹çš„ç»„ä»¶ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸ä¼šè®©æˆ‘çš„ MARL è‡ªå®šä¹‰ç¯å¢ƒå­¦ä¹ ã€‚å¦‚ä½•æœ‰æ•ˆåœ°å°†æ›´æ”¹ä¼ è¾“ç»™æˆ‘çš„ä»£ç†ï¼Œå³è®©ä»–æ›´å®¹æ˜“ç†è§£æ˜¯ä»€ä¹ˆæ“ä½œå¯¼è‡´äº†æ›´æ”¹ã€‚ ä¾›å‚è€ƒï¼š Boidï¼ˆæˆ‘çš„ä»£ç†ï¼‰ ç®—æ³•ï¼š StableBaselines3-PPO å¥–åŠ±ç»„ä»¶ï¼šCohesionAlignmentCollision Penalty å¦å¤–æˆ‘å¦‚ä½•ç¡®å®šæˆ‘ä½¿ç”¨çš„æ˜¯CTDEè¿˜æ˜¯DTDEï¼Ÿ   ç”±   æäº¤/u/Sadboi1010   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18w09rn/less_ambiguous_reward_function/</guid>
      <pubDate>Mon, 01 Jan 2024 16:29:50 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ DRL ç ”ç©¶é¡¹ç›®è¿›è¡Œè‚¡ç¥¨äº¤æ˜“</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vz6gy/stock_trading_using_drl_research_project/</link>
      <description><![CDATA[æˆ‘æ˜¯è®¡ç®—æœºç§‘å­¦å­¦å£«å­¦ä½æœ€åä¸€å¹´çš„å­¦ç”Ÿã€‚æˆ‘çš„ FYP æ ‡é¢˜æ˜¯â€œä½¿ç”¨ DRL è¿›è¡Œè‚¡ç¥¨äº¤æ˜“â€ã€‚è¿™ä¸ªç§°å·æ˜¯æˆ‘çš„å¯¼å¸ˆç»™çš„ï¼Œå¯¹æˆ‘æ¥è¯´çœŸçš„æ˜¯ä¸€ä¸ªå¾ˆéš¾çš„ç§°å·ã€‚è¯¥ä¸»é¢˜æ˜¯å…³äºä¼˜åŒ–äº¤æ˜“ç­–ç•¥ï¼Œè€Œä¸æ˜¯ä»·æ ¼é¢„æµ‹æˆ–æŠ•èµ„ç»„åˆç®¡ç†ã€‚ è¿™äº›æ˜¯æˆ‘çš„é¡¹ç›®ç›®æ ‡ï¼š 1. åœ¨æ­£å¸¸è‚¡ç¥¨å¸‚åœºæ¡ä»¶ä¸‹ä½¿ç”¨ DRL æ¥æé«˜è‚¡ç¥¨äº¤æ˜“æ¨¡å‹çš„æ€§èƒ½ï¼Œå…¶æŒ‡æ ‡åŒ…æ‹¬ï¼šç´¯ç§¯å›æŠ¥ï¼ˆCRï¼‰ã€‚ 2. åœ¨ç†Šå¸‚è‚¡å¸‚æ¡ä»¶ä¸‹ä½¿ç”¨ DRL ä»¥åŠå¤æ™®æ¯”ç‡ (SR) ç­‰æŒ‡æ ‡æ¥æé«˜è‚¡ç¥¨äº¤æ˜“æ¨¡å‹çš„æ€§èƒ½ã€‚ 3. é€šè¿‡å¯¹ DRL æ¨¡å‹ä½¿ç”¨çš„æŠ€æœ¯æŒ‡æ ‡åº”ç”¨ç‰¹å¾é€‰æ‹©æŠ€æœ¯æ¥æé«˜è‚¡ç¥¨äº¤æ˜“æ¨¡å‹çš„æ€§èƒ½ã€‚ æˆ‘æ‰“ç®—ä½¿ç”¨ DQNï¼Œä½†æˆ‘å¯¹æ­¤å®Œå…¨é™Œç”Ÿå¹¶ä¸”é™·å…¥å›°å¢ƒã€‚æˆ‘å·²ç»å®Œæˆäº†ä»‹ç»å’Œæ–‡çŒ®ç»¼è¿°ï¼Œä½†ç†è®ºæ¡†æ¶ç« èŠ‚è®©æˆ‘å¾ˆå—ä¸äº†ã€‚æˆ‘è¿˜æ²¡æœ‰å¼€å§‹ç¼–ç éƒ¨åˆ†ï¼Œå› ä¸ºé‚£æ˜¯ä¸‹å­¦æœŸçš„äº‹æƒ…ã€‚ç°åœ¨æ­£åœ¨å†™è®ºæ–‡ï¼Œä¸çŸ¥é“æ€ä¹ˆå†™ï¼Œè¦ä¸è¦ç”¨MDPï¼Œæ€ä¹ˆç”¨ç­‰ç­‰ï¼ŒçœŸçš„å¾ˆæŒ£æ‰ï¼Œå‹åŠ›å¾ˆå¤§ã€‚æœ‰äººå¯ä»¥å¸®æˆ‘è§£å†³è¿™ä¸ªé—®é¢˜å¹¶ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ   ç”±   æäº¤/u/cookiesandcream30   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vz6gy/stock_trading_using_drl_research_project/</guid>
      <pubDate>Mon, 01 Jan 2024 15:39:11 GMT</pubDate>
    </item>
    <item>
      <title>å“ªä¸ª OpenAI Gym ç‰ˆæœ¬æœ€å¥½/æœ€å¸¸ç”¨ï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vtoyi/which_openai_gym_version_is_bestmost_used/</link>
      <description><![CDATA[å¤§å®¶å¥½ï¼Œ æˆ‘æœ€è¿‘å¼€å§‹ç ”ç©¶å¥èº«å¹³å°ï¼Œæ›´å…·ä½“åœ°è¯´æ˜¯ BipedalWalkerã€‚æˆ‘æœ€åˆä½¿ç”¨çš„æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼ˆç°åœ¨ç§°ä¸º Gymnasiumï¼Œè€Œä¸æ˜¯ Gymï¼‰ï¼Œä½†æ˜¯ 99% çš„åœ¨çº¿æ•™ç¨‹å’Œä»£ç éƒ½ä½¿ç”¨æ—§ç‰ˆæœ¬çš„ Gymã€‚ ç”±äºæˆ‘æ­£åœ¨ä»äº‹çš„é¡¹ç›®éå¸¸å¤æ‚ï¼Œå¹¶ä¸”æ²¡æœ‰ä»¥å‰åœ¨è¿™ä¸ªç¯å¢ƒä¸­å®Œæˆè¿‡ï¼Œæˆ‘éœ€è¦å°½å¯èƒ½å¤šåœ°ä»å…¶ä»–äººé‚£é‡Œè·å¾—å·¥ä½œä»£ç ã€‚æˆ‘çœ‹åˆ°ç‰ˆæœ¬21åˆ°26æœ‰å˜åŒ–ï¼ŒGymnasiumç°åœ¨ä¹Ÿæœ‰å·®å¼‚ã€‚ æˆ‘å¯ä»¥çœ‹åˆ°å¾ˆå¤š3å¹´å‰çš„æ•™ç¨‹ã€è§†é¢‘å’Œä»£ç ã€‚ä½†åœ¨è¿‡å»å‡ å¹´é‡Œï¼Œå®ƒä¼¼ä¹å·²ç»å¤±å»äº†å¸å¼•åŠ›ã€‚ æ‰€ä»¥æˆ‘çš„é—®é¢˜æ˜¯ï¼Œå“ªä¸ªç‰ˆæœ¬çš„åº“æœ€é€‚åˆæˆ‘å·¥ä½œï¼Œä»¥ä¾¿æ‹¥æœ‰å®é™…å·¥ä½œçš„ä»£ç ï¼Ÿ    ç”±   æäº¤ /u/DocMenios   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vtoyi/which_openai_gym_version_is_bestmost_used/</guid>
      <pubDate>Mon, 01 Jan 2024 09:59:21 GMT</pubDate>
    </item>
    <item>
      <title>PPO ä¸åœ°æ–¹æ”¿ç­–çš„èåˆ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vsfzg/ppo_convergence_to_local_policy/</link>
      <description><![CDATA[      æˆ‘æ­£åœ¨ä½¿ç”¨ PPO ç®—æ³•ï¼Œæˆ‘çš„ç®—æ³•åœ¨è®­ç»ƒæœŸé—´è·å¾—çš„æœ€å¤§å¥–åŠ±ä¸º 212ï¼Œä½†ç»è¿‡å‡ æ¬¡ï¼ˆ80-100 epiï¼‰åå®ƒæ”¶æ•›åˆ° 176ï¼Œå°±åƒæˆ‘å°è¯•é™ä½å­¦ä¹ ç‡å’Œå…¶ä»–ä¿®è¡¥ä¸€æ ·è¶…çº§å‚æ•°ï¼Œä½†ä»ç„¶æ²¡æœ‰ç”¨ã€‚ ä»»ä½•å¸®åŠ©è¡¨ç¤ºèµèµã€‚ ï¼ˆä¸‹é¢æ˜¯è®­ç»ƒå›¾ã€‚ï¼‰æå‰è‡´è°¢ï¼ï¼ï¼ï¼ https://preview.redd.it/5buwjkzqgs9c1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=6687d40641ac6747acdf42a27b6f2e80 65532c97   ç”±   æäº¤ /u/Wide-Chef-7011   [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vsfzg/ppo_convergence_to_local_policy/</guid>
      <pubDate>Mon, 01 Jan 2024 08:25:24 GMT</pubDate>
    </item>
    <item>
      <title>Connect-4 - Q-Learning ä¸ Actor-Critic</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18vd577/connect4_qlearning_vs_actorcritic/</link>
      <description><![CDATA[æˆ‘å®ç°äº† Connect-4 çš„ä¸¤ä¸ªç‰ˆæœ¬ï¼Œä¸€ä¸ªåŸºäº Q-Learningï¼Œå¦ä¸€ä¸ªåŸºäº REINFORCEï¼ˆActor-Critic æ–¹æ³•ï¼‰ã€‚æ‰‹åŠ¨è°ƒæ•´å­¦ä¹ å‚æ•°åï¼Œå¾ˆå®¹æ˜“è®© Actor-Critic ç‰ˆæœ¬è¾¾åˆ°åˆç†çš„å­¦ä¹ è¿›åº¦ã€‚ç„¶è€Œï¼Œæˆ‘åœ¨ Q-Learning ç‰ˆæœ¬ä¸Šæ²¡æœ‰å–å¾—æˆåŠŸã€‚å¯¹äºä¸ºä»€ä¹ˆ REINFORCE æ›´é€‚åˆè¿™ä¸ªé—®é¢˜ï¼Œæœ‰ä»€ä¹ˆç†ç”±/è§£é‡Šå—ï¼Ÿ   ç”±   æäº¤ /u/m_jochim   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18vd577/connect4_qlearning_vs_actorcritic/</guid>
      <pubDate>Sun, 31 Dec 2023 17:56:07 GMT</pubDate>
    </item>
    <item>
      <title>å¼ºåŒ–å­¦ä¹ çš„å»ºè®®</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v8twl/advices_for_reinforcement_learning/</link>
      <description><![CDATA[æˆ‘æƒ³æ·±å…¥äº†è§£ä»€ä¹ˆæ˜¯å‘é‡åŒ–ç¯å¢ƒï¼Œè¯·ç»™æˆ‘ä¸€äº›ä¹¦ç±æˆ–è§†é¢‘ã€‚   ç”±   æäº¤ /u/BryanDeveloper   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v8twl/advices_for_reinforcement_learning/</guid>
      <pubDate>Sun, 31 Dec 2023 14:27:08 GMT</pubDate>
    </item>
    <item>
      <title>ç½‘æ ¼ä¸–ç•Œä¸­çš„ Q å­¦ä¹  - è´å°”æ›¼æ–¹ç¨‹å¯è§†åŒ– [è¯„è®ºä¸­çš„é“¾æ¥] :)</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v6jwi/q_learning_on_a_grid_world_bellman_equation/</link>
      <description><![CDATA[       ç”±   æäº¤/u/prajwalsouza  [é“¾æ¥] [è¯„è®º] &lt; /è¡¨&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v6jwi/q_learning_on_a_grid_world_bellman_equation/</guid>
      <pubDate>Sun, 31 Dec 2023 12:12:04 GMT</pubDate>
    </item>
    <item>
      <title>ä½¿ç”¨ pytorch ç¼–å†™è‡ªå®šä¹‰çŸ¢é‡åŒ–å¥èº«æˆ¿ç¯å¢ƒçš„çº¦å®šï¼Ÿ</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18v50ai/conventions_to_write_a_custom_vectorized_gym/</link>
      <description><![CDATA[åœ¨torchrlä¸­ï¼Œæ‚¨åªéœ€å°†ä¸€æ‰¹æ“ä½œä¼ é€’ç»™stepå‡½æ•°ï¼Œç„¶åè®©pytorchå¤„ç†çŸ¢é‡åŒ–ã€‚æ¥æºï¼šhttps://pytorch.org/rl/tutorials/pendulum.html#batching-computations .ä¸è¿‡ï¼Œæˆ‘æƒ³ä½¿ç”¨ä¸Gymå¤§éƒ¨åˆ†å…¼å®¹çš„å…¶ä»–å¼ºåŒ–å­¦ä¹ åº“ã€‚ åœ¨gymä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨vector.make()æˆ–AsyncVectorEnvã€‚å¦‚æœæ‚¨çš„ç¯å¢ƒå®ç°åªæ˜¯ Pytorchï¼Œè¿™ä¸æ˜¯å¤§æå°ç”¨å—ï¼Ÿæœ‰å¼€æºçš„ä¾‹å­å—ï¼Ÿæˆ–è€…ä¹Ÿè®¸æ˜¯å¥èº«æˆ¿çš„æ›¿ä»£å“ï¼Ÿ æ³¨æ„ï¼šæˆ‘åªæ˜¯ RL æ–°æ‰‹å‡ å¤©ã€‚ä»»ä½•å»ºè®®éƒ½ä¼šæœ‰å¸®åŠ©   ç”±   æäº¤/u/hunterh0  [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18v50ai/conventions_to_write_a_custom_vectorized_gym/</guid>
      <pubDate>Sun, 31 Dec 2023 10:23:40 GMT</pubDate>
    </item>
    <item>
      <title>ã€Šåˆ©ç”¨éƒ¨åˆ†åŠ¨åŠ›å­¦çŸ¥è¯†è¿›è¡Œé«˜æ•ˆå¼ºåŒ–å­¦ä¹ çš„æ ·æœ¬ã€‹2023</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</link>
      <description><![CDATA[ ç”±   æäº¤ /u/APaperADay   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ur9pg/sample_efficient_reinforcement_learning_with/</guid>
      <pubDate>Sat, 30 Dec 2023 22:12:11 GMT</pubDate>
    </item>
    <item>
      <title>ç¯å¢ƒç”Ÿæˆå™¨</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</link>
      <description><![CDATA[å˜¿ï¼ŒRL çˆ±å¥½è€…ï¼Œ æˆ‘æƒ³çŸ¥é“ï¼Œå½“æˆ‘ä»¬åš RL å®éªŒæ—¶ï¼Œä½ ä»¬æ˜¯å¦éƒ½è¢«å¼€å‘ RL æ‰€éœ€çš„å¼€é”€æ‰€å›°æ‰°ï¼Ÿé¢„å…ˆç¯å¢ƒã€‚æˆ‘å‘ç°è¿™éå¸¸çƒ¦äººï¼Œå› ä¸ºæˆ‘æ€»æ˜¯éœ€è¦æ„å»ºé€‚åˆæˆ‘çš„ç”¨ä¾‹çš„ä¸œè¥¿ã€‚  æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬åªæœ‰ Farma åŸºé‡‘ä¼šï¼ˆhttps://farama.org/ï¼‰æä¾›çš„åå‡ ä¸ªé«˜è´¨é‡ç¯å¢ƒã€‚ org/)  æ¬¢è¿ä»»ä½•æƒ³æ³•ï¼    ç”±   æäº¤ /u/Illustrious-Drop5872    reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/&quot;&gt;[é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18umt9x/an_environment_generator/</guid>
      <pubDate>Sat, 30 Dec 2023 18:59:28 GMT</pubDate>
    </item>
    <item>
      <title>å¤šç›®æ ‡åœºæ™¯çš„æœ€ä½³å¼ºåŒ–å­¦ä¹ ç®—æ³•</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</link>
      <description><![CDATA[ä½ å¥½ï¼Œ æˆ‘æ­£åœ¨å°è¯•è®­ç»ƒå®¤å†…æ— äººæœºä»£ç†ç¦»å¼€æˆ¿é—´ã€‚æ— äººæœºå¿…é¡»é€ƒç¦»æ—¥ç›Šä¸¥é‡çš„ç«åŠ¿å¹¶åˆ°è¾¾ 4 ä¸ªå‡ºå£ä¸­çš„ä»»ä½•ä¸€ä¸ªã€‚ æˆ‘å°è¯•è¿‡ DQNã€A2Cã€PPOã€‚è¿™äº›ç®—æ³•çš„é—®é¢˜åœ¨äºï¼Œä¸€æ—¦æ™ºèƒ½ä½“å­¦ä¼šäº†å‡ºå£é—¨ï¼Œå®ƒæ€»æ˜¯å°è¯•ä»é‚£é‡Œé€€å‡ºï¼Œè€Œå…¶ä»–é—¨åˆ™æœªè¢«æ¢ç´¢ã€‚ æˆ‘æƒ³çŸ¥é“å“ªç§ RL ç®—æ³•æœ€é€‚åˆè¿™ç§æƒ…å†µï¼Œå½“æ›´å¤šæ²¡æœ‰ä¸€ä¸ªè¿›çƒã€‚ è°¢è°¢ï¼   ç”±   æäº¤/u/shahmirkhan21   [é“¾æ¥] [è¯„è®º]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18uja4a/best_rl_algorithm_for_multigoal_scenario/</guid>
      <pubDate>Sat, 30 Dec 2023 16:24:36 GMT</pubDate>
    </item>
    </channel>
</rss>