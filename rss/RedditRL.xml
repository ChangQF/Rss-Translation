<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂的环境并学习如何最佳地获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Thu, 07 Dec 2023 15:14:54 GMT</lastBuildDate>
    <item>
      <title>规划的循环网络模型解释了海马体重放和人类行为</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cu0wa/a_recurrent_network_model_of_planning_explains/</link>
      <description><![CDATA[论文：https://www.biorxiv.org/content/10.1101/2023.01.16.523429v2 代码：https://github.com/KrisJensen/planning_code 摘要：  当面对新的情况时，人类常常会花大量时间思考可能的未来。为了使这种计划变得合理，行为的好处必须补偿花费在思考上的时间。在这里，我们通过开发神经网络模型来捕获人类行为的这些特征，其中计划本身由前额叶皮层控制。该模型由一个元强化学习代理组成，该代理具有通过从其自己的策略中采样想象的动作序列来进行计划的能力，我们称之为“推出”。当计划有益时，代理会学习计划，解释人类思维时代的经验变化。此外，人工智能体所采用的策略推出模式与最近在空间导航过程中记录的啮齿动物海马回放模式非常相似。我们的工作提供了一种新理论，说明大脑如何通过前额叶-海马体相互作用来实施规划，其中海马体重放是由前额叶动态触发并适应性影响的。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cu0wa/a_recurrent_network_model_of_planning_explains/</guid>
      <pubDate>Thu, 07 Dec 2023 11:55:55 GMT</pubDate>
    </item>
    <item>
      <title>强化学习“最方便”的期刊俱乐部</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cphho/most_convenient_journal_club_for_reinforcement/</link>
      <description><![CDATA[大家好！我正在为 RL 社区中的一些人解决问题，并希望得到您的意见/想法。 问题：  没有时间阅读最酷、最新的论文 单独处理论文并不那么有趣，而且通常很困难。 无法承诺参加每周的会议，需要完全的灵活性 &lt; /ol&gt; 为了解决这些问题，我建立了一个名为 DenseLayers.com 的期刊俱乐部网站。 帮助我需要的是反馈！如果可能，请至少回答其中一个问题。 :)   您目前阅读论文的频率如何，以上 3 个问题中的哪一个与您产生共鸣？ 您目前正在采取哪些措施来解决该问题？什么可以完全消除这个问题？ 您喜欢阅读和理解哪些流行的强化学习论文（如果您有时间的话）？  谢谢！   由   提交 /u/mngrwl   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cphho/most_convenient_journal_club_for_reinforcement/</guid>
      <pubDate>Thu, 07 Dec 2023 06:30:13 GMT</pubDate>
    </item>
    <item>
      <title>对于 MAPPO，批评者网络应该如何处理全局状态输入？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cmzi9/for_mappo_how_should_the_critic_network_handle/</link>
      <description><![CDATA[我正在使用MAPPO算法来解决多无人机协作导航问题。其中，无人机代理的观测信息是图像和自身速度。我想知道批评者网络如何以最合适的方式处理输入的全局信息？我目前想到两种方法。第一种方法是将所有无人机图像堆叠起来，并使用大型CNN网络对其进行处理以获得特征向量。第二种方法是使用小型CNN网络分别处理每个无人机图像以获得特征向量，然后将这些向量拼接在一起。哪种方法更好？或者有更好的方法来解决这个问题吗？   由   提交/u/Cute-Heron-1709   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cmzi9/for_mappo_how_should_the_critic_network_handle/</guid>
      <pubDate>Thu, 07 Dec 2023 04:04:40 GMT</pubDate>
    </item>
    <item>
      <title>强化学习找到的解离最优解有多远？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18cfm38/how_far_is_the_solution_found_by_reinforcement/</link>
      <description><![CDATA[例如，在像西洋跳棋这样完全解决的游戏中（肯定会以双方完美的平局结束），是否有任何研究比较深度学习与最优策略？像 AlphaZero 这样的深度强化学习方法能否在有限的训练时间内以最优策略实现 99% 的绘图概率？   由   提交 /u/ZealousidealRub8250   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18cfm38/how_far_is_the_solution_found_by_reinforcement/</guid>
      <pubDate>Wed, 06 Dec 2023 22:04:50 GMT</pubDate>
    </item>
    <item>
      <title>节目介绍</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18c3gm4/program_induction/</link>
      <description><![CDATA[有人可以解释一下程序归纳方法如何在元学习机器学习模型上发挥作用   由   提交/u/NoShoe6803  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18c3gm4/program_induction/</guid>
      <pubDate>Wed, 06 Dec 2023 13:00:19 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡罗树搜索</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bu5wh/monte_carlo_tree_search/</link>
      <description><![CDATA[MCTS可以用于多代理问题吗？   由   提交/u/Meta_Sage_247   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bu5wh/monte_carlo_tree_search/</guid>
      <pubDate>Wed, 06 Dec 2023 03:01:02 GMT</pubDate>
    </item>
    <item>
      <title>“越野自动驾驶汽车的多模态动力学建模”，Tremblay 等人，2020</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bo4rd/multimodal_dynamics_modeling_for_offroad/</link>
      <description><![CDATA[       由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bo4rd/multimodal_dynamics_modeling_for_offroad/</guid>
      <pubDate>Tue, 05 Dec 2023 22:19:56 GMT</pubDate>
    </item>
    <item>
      <title>深度强化学习中时间信用分配的调查</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18bai3f/a_survey_of_temporal_credit_assignment_in_deep/</link>
      <description><![CDATA[https://arxiv.org/abs/2312.01072   由   提交/u/Conscious_Heron_9133   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18bai3f/a_survey_of_temporal_credit_assignment_in_deep/</guid>
      <pubDate>Tue, 05 Dec 2023 12:02:23 GMT</pubDate>
    </item>
    <item>
      <title>持续学习：应用和前进之路</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18b8ial/continual_learning_applications_and_the_road/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2311.11908 OpenReview：https:// /openreview.net/forum?id=axBIMcGZn9 Dagstuhl 研讨会 23122 总结：https://drops.dagstuhl.de/entities/document/10.4230/DagRep.13.3.74 摘要&lt; /strong&gt;:  持续学习是机器学习的一个子领域，其目的是让机器学习模型能够不断地学习新数据，通过积累知识而不忘记过去学到的东西。在这项工作中，我们退一步问：“为什么人们首先应该关心持续学习？”。我们通过调查最近在三个主要机器学习会议上发表的持续学习论文来奠定基础，并表明内存受限的设置在该领域占据主导地位。然后，我们讨论了机器学习中的五个开放问题，尽管乍一看它们似乎与持续学习无关，但我们表明持续学习将不可避免地成为其解决方案的一部分。这些问题包括模型编辑、个性化、设备上学习、更快（重新）训练和强化学习。最后，通过比较这些未解决问题的需求和当前持续学习的假设，我们强调并讨论了持续学习研究的四个未来方向。我们希望这项工作为持续学习的未来提供一个有趣的视角，同时展示其潜在价值以及我们为了使其成功而必须追求的路径。这项工作是作者在 2023 年 3 月的 Dagstuhl 深度持续学习研讨会上进行多次讨论的结果。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18b8ial/continual_learning_applications_and_the_road/</guid>
      <pubDate>Tue, 05 Dec 2023 09:44:59 GMT</pubDate>
    </item>
    <item>
      <title>拼图解算器</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18b4p6c/nonogram_solver/</link>
      <description><![CDATA[您好！我在 ML 和神经网络方面有一些经验，但这是我第一次涉足 RL。我希望能够解决非图（数字难题），并且我已经找到了一种将约束数据预处理为 10x10 浮点网格（0-1（含）之间）的方法。我希望输出是二进制值或布尔值的 10x10 网格，但是我不太确定如何构造它。我觉得我遇到的问题是如何维持状态。使用预处理网格作为“游戏板”并让代理在其顶部放置 0 和 1（在使浮动范围不包含之后）是一个好主意吗，即使这会删除该网格中的先前信息正方形？有没有一种方法可以为约束信息设置一个单独的网格，并为游戏板设置一个单独的网格来保持空间数据的相关性？欢迎任何想法，谢谢！   由   提交/u/FissioN47   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18b4p6c/nonogram_solver/</guid>
      <pubDate>Tue, 05 Dec 2023 05:22:30 GMT</pubDate>
    </item>
    <item>
      <title>“强化学习（不是）用于自然语言处理：自然语言策略优化的基准、基线和构建模块”，Ramamurthy 等人，2023 年</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18az4ey/is_reinforcement_learning_not_for_natural/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/gwern  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18az4ey/is_reinforcement_learning_not_for_natural/</guid>
      <pubDate>Tue, 05 Dec 2023 00:36:09 GMT</pubDate>
    </item>
    <item>
      <title>即使损失下降，PPO 中的代理也不会学习</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18ax96z/agent_in_ppo_not_learning_even_when_loss_is/</link>
      <description><![CDATA[嗨，我正在尝试直接从原始论文实现 PPO，所以我不想复制别人的代码。不幸的是，我遇到了一个问题，即演员和评论家的损失正在下降，但剧集分数却没有改善。我正在 pendulum-v1 上测试 PPO，平均剧集分数约为 -1200。如果有人能告诉我我做错了什么，我将不胜感激。 这是我正在运行的代码：https://github.com/stanislawraczk/PPO/tree/main 提前感谢您的帮助:)   由   提交 /u/stachursky   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18ax96z/agent_in_ppo_not_learning_even_when_loss_is/</guid>
      <pubDate>Mon, 04 Dec 2023 23:12:22 GMT</pubDate>
    </item>
    <item>
      <title>解释 RLlib 中 PPO 的指标</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18a930l/explaining_the_metrics_for_ppo_in_rllib/</link>
      <description><![CDATA[       您能否解释一下 Ray Rllib 图中的policy_loss、vf_loss 和total_loss 的预期趋势？目前，我的保单损失为负。它先下降然后增加，我认为这是由于高学习率造成的。 Total_loss 和 vf_loss 均为正且递减。  除了损失和平均奖励之外，我还应该关注哪些值来确定我的策略是否经过训练并表现良好？ ​  https://preview.redd.it/pcrssmq0m64c1.png？ width=354&amp;format=png&amp;auto=webp&amp;s=6f5b8150ad5fa7872168e6a2a2eef13647a4fdfd   由   提交 /u/Beautiful_Basis8441   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18a930l/explaining_the_metrics_for_ppo_in_rllib/</guid>
      <pubDate>Mon, 04 Dec 2023 01:34:53 GMT</pubDate>
    </item>
    <item>
      <title>帮我解决依赖问题！</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependencies_issue/</link>
      <description><![CDATA[尝试在 google colab 上运行 DQN pong。  我使用了这行代码作为依赖项： !pip installgym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate ==3.1.* ​ 我收到此错误消息： 收集gym[box2d]==0.17.* 使用缓存的gym -0.17.3-py3-none-any.whl 已满足要求：/usr/local/lib/python3.10/dist-packages 中的 pyvirtualdisplay==0.2.* (0.2.5) 已满足要求：PyOpenGL==3.1 .* 在 /usr/local/lib/python3.10/dist-packages (3.1.7) 已满足要求： PyOpenGL-accelerate==3.1.* 在 /usr/local/lib/python3.10/dist-packages ( 3.1.7）已满足要求：/usr/local/lib/python3.10/dist-packages中的scipy（来自gym[box2d]==0.17.*）（1.11.4）已满足要求：numpy&gt;=1.10。 4 /usr/local/lib/python3.10/dist-packages (来自gym[box2d]==0.17.*) (1.23.5) 收集 pyglet&lt;=1.5.0,&gt;=1.4.0 (来自gym [box2d]==0.17.*) 使用缓存的 pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB) 收集 cloudpickle&lt;1.7.0,&gt;=1.2.0 (来自gym[box2d]= =0.17.*) 使用缓存的cloudpickle-1.6.0-py3-none-any.whl (23 kB) 收集box2d-py~=2.3.5 (来自gym[box2d]==0.17.*) 使用缓存的box2d-py -2.3.8.tar.gz (374 kB) 正在准备元数据 (setup.py) ... 已完成 要求已满足： /usr/local/lib/python3.10/dist-packages 中的 EasyProcess （来自 pyvirtualdisplay==0.2。 *）（1.1）已满足要求：未来在 /usr/local/lib/python3.10/dist-packages 中（来自 pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym[box2d]==0.17 .*) (0.18.3) 为收集的包构建轮子：box2d-py 错误：subprocess-exited-with-error × python setup.py bdist_wheel 未成功运行。 │ 退出码：1 ╰─&gt;请参阅上面的输出。  注意：此错误源自子进程，并且可能不是 pip 的问题。为 box2d-py 构建轮子（setup.py）...错误错误：为 box2d-py 构建轮子失败为 box2d-py 运行 setup.py clean 无法构建 box2d-py 错误：无法为 box2d-py 构建轮子，这是安装基于 pyproject.toml 的项目所必需的 ​ 任何帮助将不胜感激！   由   提交 /u/Opening-Ocelot1748    reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependency_issue/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/18a6oed/help_me_for_the_dependencies_issue/</guid>
      <pubDate>Sun, 03 Dec 2023 23:34:08 GMT</pubDate>
    </item>
    <item>
      <title>在规划中结合空间和时间抽象以实现更好的泛化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/189r06i/combining_spatial_and_temporal_abstraction_in/</link>
      <description><![CDATA[arXiv: https:// arxiv.org/abs/2310.00229 OpenReview：https:// /openreview.net/forum?id=eo9dHwtTFt 代码： https://github.com/mila-iqia/skipper 博客文章：http://mingde.world/combining-spatial-and-temporal-abstraction-in-planning/ 摘要:  受人类意识规划的启发，我们提出了Skipper，这是一种基于模型的强化学习代理，它利用空间和时间抽象来概括所学技能新颖的情况。它自动将手头的任务分解为更小规模、更易于管理的子任务，从而实现稀疏决策并将其计算集中在环境的相关部分。这依赖于表示为有向图的高级代理问题的定义，其中使用事后知识端到端地学习顶点和边。我们的理论分析在适当的假设下提供了性能保证，并确定了我们的方法预计会有所帮助的地方。与现有最先进的分层规划方法相比，以泛化为中心的实验验证了 Skipper 在零样本泛化方面的显着优势。   &amp;# 32；由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/189r06i/combining_spatial_and_temporal_abstraction_in/</guid>
      <pubDate>Sun, 03 Dec 2023 10:41:44 GMT</pubDate>
    </item>
    </channel>
</rss>