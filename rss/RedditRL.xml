<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>强化学习</title>
    <link>https://www.reddit.com/r/reinforcementlearning/?format=xml.rss</link>
    <description>强化学习是人工智能/统计学的一个子领域，专注于探索/理解复杂环境并学习如何以最佳方式获得奖励。例如 AlphaGo、临床试验和 A/B 测试以及 Atari 游戏。</description>
    <lastBuildDate>Mon, 14 Oct 2024 06:25:31 GMT</lastBuildDate>
    <item>
      <title>关于演员-评论家和在线学习中的策略梯度的困惑</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g378u7/confusion_about_policy_gradient_in_actorcritic/</link>
      <description><![CDATA[      你好！我试图了解策略梯度是如何得出的，并在Actor-Critic 方法中使用，以及它如何与在线学习保持一致。 下图显示了 REINFORCE 和 Actor-Critic 的不同策略梯度公式： https://preview.redd.it/9i2nkp936nud1.png?width=1432&amp;format=png&amp;auto=webp&amp;s=8759231eeacc0d9a655807f87890345b800c02f4  在图片中的 Actor-Critic 方程中，期望中的和在最后一行消失了。然而，在其他材料中，我看到和仍然在期望中。如果我们从第一行推导出梯度（就像在 REINFORCE 中一样），那么总和似乎应该留在里面。我说得对吗？ 如果总和应该保留，那么 Actor-Critic 方法如何在在线学习中处理这个问题？我认为 Actor-Critic 方法可以在每一步（在线）更新策略，但我不确定在这些在线更新过程中如何处理所有步骤的总和。  如能对此作出任何澄清，我们将不胜感激！提前谢谢您。    提交人    /u/DRLC_   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g378u7/confusion_about_policy_gradient_in_actorcritic/</guid>
      <pubDate>Mon, 14 Oct 2024 03:35:36 GMT</pubDate>
    </item>
    <item>
      <title>不同的 RL 算法真的有很大影响吗？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</link>
      <description><![CDATA[我现在正在进行 RL 项目来解决组合优化问题，由于复杂的约束，这个问题很难用数学来表达。我正在使用 A2C 训练我的代理，这是最简单的入门方法。 我只是想知道其他算法（如 TRPO、PPO）在实践中是否真的效果更好，而不是像在基准测试中那样。 有没有人尝试过 SOTA 算法（论文中声称）并真的看到了差异？ 我觉得设计奖励比算法本身重要得多。    提交人    /u/Electronic_Estate854   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g35fsg/do_different_rl_algorithms_really_affect_much/</guid>
      <pubDate>Mon, 14 Oct 2024 01:53:08 GMT</pubDate>
    </item>
    <item>
      <title>DIAMOND：世界建模的扩散</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</link>
      <description><![CDATA[DIAMOND 💎 世界建模的扩散：Atari 中的视觉细节很重要 项目网页：https://diamond-wm.github.io/ 代码、代理和可玩世界模型：https://github.com/eloialonso/diamond 论文：https://arxiv.org/pdf/2405.12399 摘要  RL 代理是由 REINFORCE 训练的演员-评论家。  演员和评论家网络除最后一层外共享权重。这些共享层由一个卷积“主干”和一个 LSTM 单元组成。卷积主干有四个带有 2x2 最大池化的残差块。 每次训练运行需要 500 万帧，在一台 Nvidia RTX 4090 上持续 12 天。  世界模型是一个带有 U-Net 2D 的 2D 扩散模型。它不是潜在扩散模型。它直接从视频游戏中生成帧。 该模型将最后 4 帧和动作以及扩散噪声水平作为条件。 在 RTX 3090 上以 ~10 FPS 运行。 他们使用 EDM 采样器从扩散模型中采样，即使每帧只有 1 个扩散步骤，它仍然可以很好地训练 RL 代理。     由    /u/furrypony2718  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g34qgx/diamond_diffusion_for_world_modeling/</guid>
      <pubDate>Mon, 14 Oct 2024 01:13:23 GMT</pubDate>
    </item>
    <item>
      <title>资源推荐</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g30ici/resource_recommendation/</link>
      <description><![CDATA[嗨！我对 RL 还很陌生，对于我的课程项目，我希望在多智能体系统中做一些事情来监视和跟踪目标。假设已知环境，我想最大化群体覆盖的区域。 我真的想为此做一个好的可视化。我希望在任何类型的模拟器上运行它。 有人可以推荐任何类似的项目/资源来参考吗？    提交人    /u/whatsinthaname   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g30ici/resource_recommendation/</guid>
      <pubDate>Sun, 13 Oct 2024 21:37:05 GMT</pubDate>
    </item>
    <item>
      <title>为什么尽管使用了 RLZoo3 的最佳超参数，我的 SB3 DQN 代理仍无法学习 CartPole-v1？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2xpq9/why_is_my_sb3_dqn_agent_unable_to_learn/</link>
      <description><![CDATA[我从 [RLZoo3][1] 获得了用于训练 CartPole-v1 的最佳超参数。我创建了一个最小示例来展示我的 CartPole 代理的性能。根据官方 [文档][2]，代理应获得 500 分才能成功完成一集。不幸的是，分数没有超过 300。 这是我的代码 - https://pastecode.io/s/3j1c1xb0 这是最终结果 -  [![在此处输入图像描述][3]][3] 如果需要更多信息，请告诉我。非常感谢。 [1]: https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/dqn.yml [2]: https://gymnasium.farama.org/environments/classic_control/cart_pole/ [3]: https://i.sstatic.net/wc00LfY8.png   由    /u/Academic-Rent7800  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2xpq9/why_is_my_sb3_dqn_agent_unable_to_learn/</guid>
      <pubDate>Sun, 13 Oct 2024 19:30:26 GMT</pubDate>
    </item>
    <item>
      <title>演员评论家、探索问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2vcbm/actor_critic_exploration_issue/</link>
      <description><![CDATA[我正在训练一个简单的环境，其中输入是 144 个 1D 扁平数据（4x12 网格 x 3 个特征）。  我有两个问题：1. 如果我想要最优动作和替代动作，哪种探索更好？使用 E-greedy，状态的最优动作具有最高动作概率，而其他所有动作都接近 0。  如果我在 softmax 之前使用 Boltzman 温度进行平滑，则探索性不够。   128-64-32（三个隐藏层）的 nn 设置有效，但为什么 256-128-64 不起作用？或者只是一层的 128 个单位不起作用？或者任何其他配置。      提交人    /u/doctor_fhk   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2vcbm/actor_critic_exploration_issue/</guid>
      <pubDate>Sun, 13 Oct 2024 17:47:09 GMT</pubDate>
    </item>
    <item>
      <title>Q 学习（及变体）评估</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2qqcg/q_learning_and_variations_evaluation/</link>
      <description><![CDATA[大家好， 我有一个非常基本/快速的问题，关于 RL 框架（特别是 Q 学习）的评估/测试。假设我们使用简单/经典的 Q 学习算法。我们让它训练一些情节来学习环境并构建 Q 表。那又怎么样？准备好 Q 表意味着训练后我们将遵循以下状态： action = np.argmax(q_table[state])  ？ 我的问题是：准备好 Q 表后，我们只需遵循它吗？我们构建了 Q 表然后我们遵循它，对吗？ 提前谢谢大家！！    提交人    /u/Interesting_Pea_4605   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2qqcg/q_learning_and_variations_evaluation/</guid>
      <pubDate>Sun, 13 Oct 2024 14:21:42 GMT</pubDate>
    </item>
    <item>
      <title>之前谁想过简单的 Actor-Critic Transformers 设置？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2op1r/who_thought_about_simple_actorcritic_transformers/</link>
      <description><![CDATA[在上一篇文章中，我描述了这种廉价的前馈变压器（没有自我注意块）：https://www.reddit.com/r/reinforcementlearning/comments/1fvj4gs/repeat_feed_forward_without_selfattention_can/ 如果我们应用状态序列 S1、S2、S3 ... SN，并输出动作序列 A1、A2、A3 ... AN，称其为 Actor 将它们联合起来并应用于 [S1,A1]、[S2,A2]、[S3,A3] ... [SN,AN] 以创建 Q1、Q2 序列， Q3...QN。称其为评论家 我们可以通过它进行反向传播并改进动作序列吗？ 例如 N 步时间差异。 当然，这种设置将输给 Full Transformer，或者输给学习状态、动作和奖励（彼此平行）之间互连的决策变压器 但是这种设置可以被视为 2 个可以首先从推出中学习的长距离 LSTM。（如果我们有一些来自远程控制机器人的数据） 如果奖励函数得到适当调整，那么它可以在虚拟环境中得到潜在改进。相比之下，决策变压器的 TD 改进可能性较小。    提交人    /u/Timur_1988   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2op1r/who_thought_about_simple_actorcritic_transformers/</guid>
      <pubDate>Sun, 13 Oct 2024 12:37:09 GMT</pubDate>
    </item>
    <item>
      <title>如何通过控制和学习算法解决电动汽车充电问题？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2n5c2/how_to_solve_ev_charging_problem_by_control_and/</link>
      <description><![CDATA[下午好， 我计划实施文章中指定的 EV 充电算法：https://www.researchgate.net/publication/353031955_Learning-Based_Predictive_Control_via_Real-Time_Aggregate_Flexibility **问题描述** 我正在尝试思考如何实施这种基于控制和学习的算法。该算法解决了 EV 充电问题，确保 EV 充电成本最小化，同时满足基础设施约束（容量）和 EV 约束（满足所需的能源需求）。为了解决这个问题，我们需要实时协调聚合器和系统运营商。在每个时间步，系统操作员都会向聚合器提供可用电力。聚合器接收此电力并使用简单的调度算法（如 LLF）为电动汽车充电。聚合器向系统操作员发送（通过 RL 算法）学习到的最大熵反馈/灵活性（=电动汽车约束摘要），系统操作员据此选择下一个时间步的可用电力。这个循环重复到最后一个时间步（=直到一天结束）。 **RL 环境描述** 基本上，我们在时间步 t 的状态空间包含有关在时间步 t 连接到 EVSE 的每个电动汽车的信息（=剩余充电时间、剩余充电能量）。状态空间将是一个维度为 EVSE*2 + 1 的向量（也许包括时间步也是值得的）。 动作空间将是大小为 U 的概率向量（=灵活性）（其中 U 是不同的功率级）。根据这个概率向量，我们选择每个时间步的电动汽车充电功率水平（=基础设施容量）。 RL 算法将在每个充电日后终止。 **问题：**  学习是离线的究竟意味着什么？RL 代理是否具有有关系统未来成本和约束的信息？如果是，如何在离线学习期间为 RL 代理提供有关未来的信息，而无需扩大状态空间和动作空间（具有与文章中相似/相同的动作空间）？ 每个时间步的奖励函数包含所有时间步的充电决策（奖励函数中的第 3 项），但充电决策取决于给定动作生成的信号。基本上，奖励会考虑代理的未来行动，那么如何获得它们呢？如何设计用于在线测试的奖励函数？ 我们也可以在此问题中进行离线测试或在线训练/学习吗？  如何在我们的环境中为这个问题设计重置函数？我是否应该从给定的训练/测试数据集中随机选择不同的充电日并保持其他超参数不变？     提交人    /u/Superb-Carry6469   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2n5c2/how_to_solve_ev_charging_problem_by_control_and/</guid>
      <pubDate>Sun, 13 Oct 2024 11:00:46 GMT</pubDate>
    </item>
    <item>
      <title>如何申请并破解Google Summer of Code？</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2ml2i/how_to_apply_and_crack_google_summer_of_code/</link>
      <description><![CDATA[        提交人    /u/External_Ad_11   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2ml2i/how_to_apply_and_crack_google_summer_of_code/</guid>
      <pubDate>Sun, 13 Oct 2024 10:20:27 GMT</pubDate>
    </item>
    <item>
      <title>解释 SB3 日志</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g2jz2l/interpreting_sb3_logs/</link>
      <description><![CDATA[      我知道有几种来源试图解释 SB-3s PPO 的日志输出。但是，我只是没有完全掌握它。 从正常的 ML 中，我了解到损失是正的，应该近似于 0。但是，在我当前的 SB3 PPO 设置中，我的负损失接近于 0 这是正确的行为吗（即代理是否学习）？如果是，有没有人有好的资料可以解释这一点？ https://preview.redd.it/t32ed10g2hud1.png?width=467&amp;format=png&amp;auto=webp&amp;s=1e0c5ca156a173546aeb039456250247edd7657f    提交人    /u/luigi1603   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g2jz2l/interpreting_sb3_logs/</guid>
      <pubDate>Sun, 13 Oct 2024 07:02:05 GMT</pubDate>
    </item>
    <item>
      <title>面向初学者的奖励函数发现简单教程</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</link>
      <description><![CDATA[        由    /u/goncalogordo  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g204v3/simple_tutorial_for_beginners_on_reward_function/</guid>
      <pubDate>Sat, 12 Oct 2024 13:28:33 GMT</pubDate>
    </item>
    <item>
      <title>关于 ALE 论文和超参数调优的问题</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1zrgr/question_regarding_ale_paper_and_hyper_parameter/</link>
      <description><![CDATA[我一直在阅读此链接上的论文“Arcade 学习环境：通用代理的评估平台”：https://arxiv.org/abs/1207.4708，我不确定他们如何进行超参数调整。  据我所知，他们在 5 个不同的环境中优化超参数，然后使用这些超参数对其余环境进行训练和评估。  那么我的问题是，这是如何工作的？我很难理解。如何同时在多个环境中优化超参数？  我假设所有环境都有相同的观察和动作空间，但如何同时在不同环境中进行训练和评估？   由    /u/IAmNotMarcus  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1zrgr/question_regarding_ale_paper_and_hyper_parameter/</guid>
      <pubDate>Sat, 12 Oct 2024 13:08:55 GMT</pubDate>
    </item>
    <item>
      <title>Gymnasium - 股票交易环境的终止状态与截断状态</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1yi0c/gymnasium_terminated_vs_truncated_state_for_stock/</link>
      <description><![CDATA[嗨， 所以我读了一些关于 gym.Env 中终止和截断之间的区别。根据我的理解： terminated = True -&gt; 表示达到 MDP 定义下的终止状态（因此取决于您如何定义底层 MDP） truncated = True -&gt; 由于 MDP 中未明确定义的条件，情节结束。例如（来自 Gym Docu），代理在物理上超出界限或达到时间限制。 虽然这对于机器人任务来说是有意义的，但当涉及到我正在处理的问题（用于交易/管理金融资产的代理）时，我缺少一些部分。我主要有两个问题：  代理以数据框的形式在给定长度 T 的一系列状态（每天一个）上进行训练。一旦到达情节的结尾，我会设置 done = True（在 gym 0.26 之前的版本下）。现在我必须设置 determinant = True 或 truncated = True。这里什么才有意义？请记住，代理的目标是最大化利润，因此没有“明确的目标条件”表明代理成功或失败了特定任务（就像在机器人技术中一样）。 假设我正在使用 StableBaselines 之类的框架。代理对 terminated = True 和 truncated = True 的解释是否不同？     提交人    /u/Intelligent-Put1607   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1yi0c/gymnasium_terminated_vs_truncated_state_for_stock/</guid>
      <pubDate>Sat, 12 Oct 2024 11:56:49 GMT</pubDate>
    </item>
    <item>
      <title>寻求有关攻读 RL 和机器人学博士学位的建议，同时处理签证问题和职业变化</title>
      <link>https://www.reddit.com/r/reinforcementlearning/comments/1g1wgby/seeking_advice_on_pursuing_a_phd_in_rl_and/</link>
      <description><![CDATA[我的职业生涯正处于十字路口，需要一些建议。我目前在海外一家大型科技公司担任高级 SDE，但我感觉并不充实。我计划转向 RL 和机器人技术，目标是未来获得博士学位。↳ 我的情况如下：↳  我将在一所优秀大学的机器人实验室担任研究助理，一直工作到 2025 年 6 月。 为了这个机会，我愿意大幅减薪（从每月 1x k+ 到 2k）。 我今年将申请硕士和博士课程，但我不确定我是否能进入一个好的课程。 我有大约 12 万美元的存款，但我担心在过渡期间的财务问题。 作为中国公民，签证问题令人担忧。即使有工作机会，对于拥有学士学位的中国女性来说，H1B 抽签的成功率也只有 20%。 我也在考虑个人生活方面的问题，比如约会和结婚，这可能会对签证问题有所帮助。我有点漂亮，这对我来说约会容易一些，但保持美丽需要花费大量的时间和精力。  我的主要目标是专注于 RL 和机器人技术，发表优秀的论文，并在该领域产生影响。但是，我担心会因财务问题和签证问题而分心。↳ 有人遇到过类似的情况吗？或者对平衡职业目标和实际问题有什么建议吗？我如何在处理这些其他因素的同时专注于我的研究？↳ 任何见解或经验都将不胜感激！    提交人    /u/FaithlessnessFree554   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/reinforcementlearning/comments/1g1wgby/seeking_advice_on_pursuing_a_phd_in_rl_and/</guid>
      <pubDate>Sat, 12 Oct 2024 09:31:02 GMT</pubDate>
    </item>
    </channel>
</rss>