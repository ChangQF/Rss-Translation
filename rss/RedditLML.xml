<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Fri, 07 Jun 2024 09:15:42 GMT</lastBuildDate>
    <item>
      <title>哪个更适合在公共场合学习机器学习？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1da69km/which_is_better_for_learning_machine_learning_in/</link>
      <description><![CDATA[我计划从头开始重新学习机器学习。我已经找工作大约 2 年了，甚至更多。但即使有了 20 年的经验，我仍然失业。我想我错过了一些东西。所以，与其浪费时间坐在家里，我计划从头开始重新学习一些东西，这次免费与大家分享，这样他们如果有兴趣也可以和我一起学习。但是，我有几个问题，由于 r/learnmachinelearning 过去对我很有帮助，还有什么地方比在这里问他们更好呢。所以，开始吧：  41 岁从头开始重新学习是个好主意吗？ 公开分享我正在学习的东西是个好主意吗？ 有什么地方比分享东西更好呢？ Notion 还是 Discord？  我愿意接受建议和批评。现在我甚至愿意接受吐槽。    提交人    /u/UnemployedTechie2021   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1da69km/which_is_better_for_learning_machine_learning_in/</guid>
      <pubDate>Fri, 07 Jun 2024 09:09:52 GMT</pubDate>
    </item>
    <item>
      <title>你最好的法学硕士学习路线图</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1da66jo/your_best_llms_learning_roadmap/</link>
      <description><![CDATA[对于机器学习工程师（不是初学者）来说，学习 LLM 的最佳路线图是什么。     提交人    /u/FreePudding8143   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1da66jo/your_best_llms_learning_roadmap/</guid>
      <pubDate>Fri, 07 Jun 2024 09:03:31 GMT</pubDate>
    </item>
    <item>
      <title>出租车网络数据的混合密度网络</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1da5l0f/mixture_density_network_on_taxi_network_data/</link>
      <description><![CDATA[您认为出租车数据是多模式的吗？我是从出租车需求（例如）取决于时间（一种模式）和空间/区域（第二种模式）以及其他模式（如天气、区域类型（住宅/商务/工作））的角度来看待这个问题的。这些都被视为不同的模式吗？问这个问题是因为我看到当数据是多模式时 MDN 很好。  谢谢    提交人    /u/Bobsthejob   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1da5l0f/mixture_density_network_on_taxi_network_data/</guid>
      <pubDate>Fri, 07 Jun 2024 08:18:00 GMT</pubDate>
    </item>
    <item>
      <title>除了机器学习之外，还有其他分支或专业包含如此多的数学直觉吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1da4zjq/is_there_any_branch_or_specialisation_other_than/</link>
      <description><![CDATA[  由    /u/WaveAdministrative36  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1da4zjq/is_there_any_branch_or_specialisation_other_than/</guid>
      <pubDate>Fri, 07 Jun 2024 07:33:27 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 从头构建 ResNet</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9xk76/building_resnets_from_scratch_using_pytorch/</link>
      <description><![CDATA[      使用 PyTorch 从头开始​​构建 ResNets https://debuggercafe.com/building-resnets-from-scratch-using-pytorch/ https://preview.redd.it/oxv8yt7vm15d1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=5e4e57211b9582d8e6bd3ae2045c0cb8c4506f37    提交人    /u/sovit-123   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9xk76/building_resnets_from_scratch_using_pytorch/</guid>
      <pubDate>Fri, 07 Jun 2024 00:23:30 GMT</pubDate>
    </item>
    <item>
      <title>深度学习项目</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9wwhw/deep_learning_projects/</link>
      <description><![CDATA[我正在攻读数据科学和人工智能理科硕士学位。我将于 2025 年 4 月毕业。我正在寻找深度学习项目的想法。1) 为 LLM 实施的深度学习 2) 为 CVision 实施的深度学习 我在网上查了一下，但大多数都是非常标准的项目。来自 Kaggle 的数据集是通用的。我大约有 12 个月的时间，我想做一些好的研究级项目，可能在 NeuraIPS 上发表它。我的优势是我擅长解决问题，一旦确定了问题，但我不擅长识别和构建问题..目前，我正在尝试判断什么是一个好的研究领域？    提交人    /u/Rogue260   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9wwhw/deep_learning_projects/</guid>
      <pubDate>Thu, 06 Jun 2024 23:52:00 GMT</pubDate>
    </item>
    <item>
      <title>输出层之前的层和批归一化会毁掉卷积余弦预测模型。LSTM 和 Dense 不会发生这种情况</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9wszk/layer_and_batch_normalization_before_output_layer/</link>
      <description><![CDATA[这是一个简单的模型 Conv1d（leaky relu 或 tanh）或 LSTM 或 Dense（leaky relu） 批量或层规范化 Dense（tanh） 适用于每种组合，除了使用批量规范化的卷积层，这会显着降低预测效果，也没有使用层规范化来破坏它们，因此它总是在每一步中输出相同的数字。    提交人    /u/indexator69   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9wszk/layer_and_batch_normalization_before_output_layer/</guid>
      <pubDate>Thu, 06 Jun 2024 23:47:23 GMT</pubDate>
    </item>
    <item>
      <title>在 6 月 22 日至 23 日举行的加州大学伯克利分校人工智能黑客马拉松上了解最新的人工智能/机器学习技术和创新。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9vzog/learn_about_the_latest_aiml_technologies_and/</link>
      <description><![CDATA[        由    /u/nikita-1298  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9vzog/learn_about_the_latest_aiml_technologies_and/</guid>
      <pubDate>Thu, 06 Jun 2024 23:09:32 GMT</pubDate>
    </item>
    <item>
      <title>超越炒作：LLM 和嵌入简介（使用一切开源技术）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9vuu9/beyond_the_hype_intro_to_llms_embeddings_using/</link>
      <description><![CDATA[        由    /u/kushalgoenka 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9vuu9/beyond_the_hype_intro_to_llms_embeddings_using/</guid>
      <pubDate>Thu, 06 Jun 2024 23:03:22 GMT</pubDate>
    </item>
    <item>
      <title>训练误差持续下降，但测试误差却没有下降，即使测试数据集是训练数据集的子集</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9u3ff/train_error_decreases_consistently_but_test_error/</link>
      <description><![CDATA[我的数据包含来自传感器的 6 个特征。我正在用这些数据训练一个 LSTM 网络来预测三个值。在训练期间，我的训练损失随着每个时期而持续减少，但测试损失在几个时期后并没有减少多少。这是训练和测试数据之间没有重叠的情况。所以我尝试使用训练数据的子集作为测试数据。但是，仍然是同样的行为，测试损失仍然没有减少。 以下是 LSTM 模型和训练器的代码。  class LSTMModel(nn.Module): def __init__(self, in_dim=6, hidden_​​size=200, num_layers=1, output_size=3): super(LSTMModel, self).__init__() self.lstm_1 = nn.LSTM(in_dim, hidden_​​size, num_layers, batch_first=True) self.lstm_2 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True) self.lstm_3 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True) self.lstm_4 = nn.LSTM(hidden_​​size, hidden_​​size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_​​size, output_size) def forward(self, x): x, _ = self.lstm_1(x) x, _ = self.lstm_2(x) x, _ = self.lstm_3(x) x, _ = self.lstm_4(x) output = self.fc(x[:, -1, :]) 返回输出类 SimpleModelTrainer：def __init__(self，model，train_dataset，test_dataset，batch_size=1024，epochs=100，lr=0.005）：# window_size=200，do_windowing=True，patience=5，pad_testing_data = False self.model = model self.optimizer = AdamW(params=self.model.parameters()，lr=lr) self.lr = lr self.epochs = epochs self.batch_size = batch_size self.loss_fn = nn.L1Loss() self.train_data = train_dataset self.test_data = test_dataset def train(self): self.train_dataloader = torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device=device)) self.test_dataloader = torch.utils.data.DataLoader(self.test_data, batch_size=self.batch_size, shuffle=True, generator=torch.Generator(device=device)) total_samples = 0 for epoch in tqdm(range(self.epochs), desc=&quot;epoch&quot;): self.model.train() total_loss = 0 for train_data in tqdm(self.train_dataloader, desc=&quot;train&quot;): X = train_data[0] Y = train_data[1] if X.shape[0] != self.batch_size: continue # 避免 RuntimeError: shape &#39;[16, 1, 256]&#39; 对于大小为 3328 的输入无效 total_samples += self.batch_size y_hat = self.model(X) loss = self.loss_fn(y_hat, Y) self.optimizer.zero_grad() loss.backward() self.optimizer.step() total_loss += loss.item() av​​g_train_loss = total_loss / total_samples val_loss = self.test(self.test_dataloader) print(f&quot;Epoch {epoch} - Train loss:{avg_train_loss:.10f}, Val loss:{val_loss:.10f}&quot;) def test(self, dataloader): self.model.eval() with torch.no_grad(): total_loss = 0 total_samples = 0 for test_data in tqdm(dataloader, desc=&quot;test&quot;): X = test_data[0] Y = test_data[1] if X.shape[0] != self.batch_size: continue # 避免 RuntimeError: 对于大小为 Z 的输入，形状 &#39;[Y, 200, 6]&#39; 无效 total_samples += self.batch_size y_hat = self.model(X) loss = self.loss_fn(y_hat, Y) total_loss += loss.item() val_loss = total_loss/total_samples return val_loss  我用随机生成的虚拟数据集尝试了这个。它给出了与上面完全相同的行为！您可以在这个 colab 笔记本中查看它。 正如您在笔记本中看到的，自第一个时期以来，验证损失一直停留在 0.00048。但是训练损失随着每个时期持续减少，从第 28 个时期的 0.00048 减少到 0.000016。（我写这个问题的时候它还在训练。）测试数据集是训练数据集的子集： train_dataset = CustomDataset(windowed_input_data, windowed_target_data) test_dataset = CustomDataset(windowed_input_data[:20000], windowed_target_data[:20000])  因此，我相信验证损失应该得到类似的行为，验证损失也应该达到约 0.00001。我想我在代码中犯了一些愚蠢的错误（错误的 pytorch API 调用？）而且我的眼睛根本无法帮助我。有人可以帮帮我吗？我是否遗漏了概念上的什么？    提交人    /u/Tiny-Entertainer-346   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9u3ff/train_error_decreases_consistently_but_test_error/</guid>
      <pubDate>Thu, 06 Jun 2024 21:45:50 GMT</pubDate>
    </item>
    <item>
      <title>我对现在应该学什么感到很困惑</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9r85k/i_am_too_confused_on_what_should_i_learn_now/</link>
      <description><![CDATA[看到很多数据角色要求使用 R 和 sas......但我还必须涵盖 DL 和 NLP 🥲    提交人    /u/Assalamwhileicum   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9r85k/i_am_too_confused_on_what_should_i_learn_now/</guid>
      <pubDate>Thu, 06 Jun 2024 19:49:53 GMT</pubDate>
    </item>
    <item>
      <title>抱歉，如果这太初级了，但是</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9pbo5/sorry_if_this_is_too_beginner_but/</link>
      <description><![CDATA[我对向量和张量感到困惑，有人能给我解释一下吗，我好迷茫    提交人    /u/Fluid_Structure_1506   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9pbo5/sorry_if_this_is_too_beginner_but/</guid>
      <pubDate>Thu, 06 Jun 2024 18:31:25 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用什么书？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9ja9y/what_book_can_i_use/</link>
      <description><![CDATA[大家好，你们推荐哪些书籍来进入机器学习的世界。我是一名工程本科生，所以我有微积分线性代数和统计学方面的经验。我在网上找到了 Bishop 模式识别和机器学习，但我还想有一个在 Python 上实现代码的实用指南。    提交人    /u/AshamedRecover1786   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9ja9y/what_book_can_i_use/</guid>
      <pubDate>Thu, 06 Jun 2024 14:17:47 GMT</pubDate>
    </item>
    <item>
      <title>使用 CPU 上的 LLM 嵌入实现闪电般的文本分类</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d9gozo/lightningfast_text_classification_with_llm/</link>
      <description><![CDATA[      https://preview.redd.it/br9q3raj0y4d1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=bf803ff40788e515488c2dc8aa8b07c11fe16e19 我很高兴介绍fastc，这是一个不起眼的 Python 库，旨在使文本分类高效而直接，尤其是在 CPU 环境中。无论您从事的是情绪分析、垃圾邮件检测还是其他文本分类任务，fastc 都面向小型模型，避免了微调，非常适合资源受限的环境。尽管方法简单，但性能却相当不错。 主要特点  专注于 CPU 执行：使用 deepset/tinyroberta-6l-768d 等高效模型生成嵌入。 余弦相似度分类：无需微调，而是使用类嵌入质心和文本嵌入之间的余弦相似度对文本进行分类。 高效的多分类器执行：在使用同一模型进行嵌入时，无需额外开销即可运行多个分类器。 使用 HuggingFace 轻松导出和加载：可以轻松地将模型导出到 HuggingFace 并从中加载。与微调不同，只需要在内存中加载一个嵌入模型即可为任意数量的分类器提供服务。  https://github.com/EveripediaNetwork/fastc    提交人    /u/brunneis   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d9gozo/lightningfast_text_classification_with_llm/</guid>
      <pubDate>Thu, 06 Jun 2024 12:12:54 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>