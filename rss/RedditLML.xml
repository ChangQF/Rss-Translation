<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Fri, 14 Jun 2024 18:20:10 GMT</lastBuildDate>
    <item>
      <title>ML 证书建议</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfuthd/ml_certificate_recommendations/</link>
      <description><![CDATA[我是计算机科学应用博士学位，主修大数据分析。我将于今年 10 月完成该课程，但考虑到我的背景是机械工程和故障分析，我仍然觉得自己缺乏真正过渡到 ML/AI 世界的资格。你建议参加哪些认证或课程？    提交人    /u/West-Big-9944   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfuthd/ml_certificate_recommendations/</guid>
      <pubDate>Fri, 14 Jun 2024 16:22:34 GMT</pubDate>
    </item>
    <item>
      <title>了解 LoRA：低秩近似的视觉指南，用于有效微调 LLM。🧠</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfur54/understanding_lora_a_visual_guide_to_lowrank/</link>
      <description><![CDATA[      TL;DR：LoRA 是参数高效微调 (PEFT) 方法。它通过使用低秩自适应解决了以前的微调技术的缺点，低秩自适应侧重于高效近似权重更新。这显著减少了微调中涉及的参数数量 10,000 倍，并且仍然收敛到完全微调模型的性能。 这使得它在成本、时间、数据和 GPU 方面高效，而不会损失性能。 为什么 LoRA 对于模型微调至关重要：视觉指南。 https://preview.redd.it/0tpmgjajbk6d1.png?width=1456&amp;format=png&amp;auto=webp&amp;s=1692dc7a967a41f89e9133334163e5c8be5a0178    提交人    /u/ml_a_day   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfur54/understanding_lora_a_visual_guide_to_lowrank/</guid>
      <pubDate>Fri, 14 Jun 2024 16:19:49 GMT</pubDate>
    </item>
    <item>
      <title>顺序模型无法编译（Tensorflow）[项目]</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfuag4/sequential_model_failing_to_compile_tensorflow/</link>
      <description><![CDATA[大家好。我正在尝试使用 tensorflow 在 python 中构建一个用于训练数据的模型，但构建失败。 def create_model(num_words, embedding_dim, lstm1_dim, lstm2_dim, num_categories): tf.random.set_seed(200) model = Sequential([layers.Dense(num_categories,activation=&#39;softmax&#39;),layers.Embedding(num_words, embedding_dim),layers.Bidirectional(layers.LSTM(lstm1_dim, return_sequences=True)),layers.Bidirectional(layers.LSTM(lstm2_dim))]) model.compile(loss=&#39;sparse_categorical_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;]) return model model = create_model(NUM_WORDS, EMBEDDING_DIM，32，16，5）print（model） 每当我 print（model）时，它都会显示类似 sequential_model_built=False 的内容。有人发现任何问题吗？    提交人    /u/lordbendtner11   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfuag4/sequential_model_failing_to_compile_tensorflow/</guid>
      <pubDate>Fri, 14 Jun 2024 16:00:01 GMT</pubDate>
    </item>
    <item>
      <title>新手寻求工具推荐？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dftp21/newbie_looking_for_tools_recommendations/</link>
      <description><![CDATA[尝试深入学习 ML，研究结构化数据，因此经典 ML 是重点关注 XGboost 的领域。现在是否有可用的 GUI 工具可以让我学习包括 XGboost 在内的 ML 建模基础知识？    提交人    /u/AMGraduate564   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dftp21/newbie_looking_for_tools_recommendations/</guid>
      <pubDate>Fri, 14 Jun 2024 15:34:02 GMT</pubDate>
    </item>
    <item>
      <title>如何从零编码知识开始学习任何语言以成为数据分析师？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dftdvs/how_to_learn_any_language_to_become_a_data/</link>
      <description><![CDATA[大家好， 希望你们都一切顺利！ 我之所以联系你，是因为我真的很想成为一名数据分析师，但我完全没有编码背景。我一直在阅读各种编程语言，如 Python、R 和 SQL，这些语言对于数据分析至关重要，我感到有点不知所措。 说实话，在学习方面，我很难克服懒惰。很多时候，我决定开始，但很快就会失去动力，尤其是当我在 YouTube 上一遍又一遍地观看相同的介绍视频时。 我想从头开始，打下坚实的基础。以下是我特别想要的：  结构化学习路径：我应该遵循哪些步骤才能从零开始精通数据分析所需的语言？有什么推荐的资源、课程或书籍吗？ 免费或赞助的训练营：是否有任何信誉良好的免费或赞助的训练营专注于数据分析并教您所需的编程技能？个人经历或成功案例会很棒！ 与团队一起学习：我认为与他人一起学习会非常有益。有没有什么社区或平台可以让我加入学习小组或找到学习伙伴？ 实践经验：如何在学习的同时获得实践经验？是否有任何项目或平台允许初学者练习真实世界的数据分析？     提交人    /u/Ri_chka   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dftdvs/how_to_learn_any_language_to_become_a_data/</guid>
      <pubDate>Fri, 14 Jun 2024 15:21:08 GMT</pubDate>
    </item>
    <item>
      <title>我拥有数据领域的硕士学位，但没有相关经验</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfsqdj/i_have_my_masters_in_data_field_but_i_dont_have/</link>
      <description><![CDATA[相反，我在本科期间做了两个月的运营实习生。从那时起，我就没有机会实习了，因为我们不被允许实习（很奇怪，但确实如此） 我如何在简历中展示这一点？我已将简历提交给多个简历审查网站。他们看到的只是我在运营实习生方面的技能……  尽管我在简历中提到了我的项目和技能    提交人    /u/BeyondAmbitious2039   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfsqdj/i_have_my_masters_in_data_field_but_i_dont_have/</guid>
      <pubDate>Fri, 14 Jun 2024 14:53:19 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch empty() 函数出错，我大概知道原因，但无法修复</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfseqv/pytorch_empty_function_erroring_out_i_have_some/</link>
      <description><![CDATA[我正在编写一个基于变分递归自动编码器的模型来预测股票收益，但就在实例化该模型之前，遇到了一个障碍。抛出的错误是：  empty()：参数“size”无法解压位置 2 处的对象，错误为“类型必须是 int 的元组，但得到了元组”  Encoder 模型构造函数是： def convert_to_dotdict(d): if isinstance(d, dict): return DotDict({k: convert_to_dotdict(v) for k, v in d.items()}) elif isinstance(d, list): return [convert_to_dotdict(i) for i in d] else: return d class Encoder(nn.Module): def __init__(self, params: DotDict): super(Encoder, self).__init__() self.params = params self.batch_size = self.params.batch_size self.num_stocks = int(self.params.num_stocks) self.num_factors = self.params.num_factors self.num_layers = self.params.num_layers self.hidden_​​size = self.params.hidden_​​size self.num_lags = self.params.num_lags self.dropout = self.params.dropout self.rnn = nn.LSTM( input_size=self.num_stocks, hidden_​​size=self.hidden_​​size, num_layers=self.num_layers, dropout=self.dropout, batch_first=True ) print(&quot;Hi&quot;) # 健全性检查 self.relu = nn.ReLU() = nn.Linear(self.hidden_​​size, self.num_factors) self.mu_activation = nn.ReLU() self.log_sigma = nn.Linear(self.hidden_​​size, self.num_factors) self.log_sigma_activation = nn.ReLU()self.mu  解码器模型构造函数非常相似，除了可能在这里或那里有一个完全连接的层或投影矩阵。 RVAE 模块是： class RVAE(nn.Module): def __init__(self,coder_params:dict,coder_params:dict): super(RVAE, self).__init__() self.encoder = Encoder(encoder_params) self.decoder = Decoder(decoder_params) self.weight_matrix = nn.Parameter(torch.randn(hyperparams.FACTOR_NETWORK.NUM_FACTORS, hyperparams.DATA.NUM_STOCKS), require_grad=True) = None self.logvar = None def sample(mu : list, logvar : list): assert len(mu) == len(logvar) std = torch.exp(0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def forward(self, x): mu, logvar = self.encoder(x) = mu self.logvar = logvar z = self.sample(mu, logvar) ft = self.decoder(z) return np.matmul(self.weight_matrix, ft)self.muself.mu  当我运行 model = RVAE(encoder_params,coder_params) 时，出现上述错误。请注意，DotDict 只是一个将字典转换为点可访问形式的实用程序类。 我尝试搜索有关 torch.empty() 的文档，并尝试对该函数进行各种输入组合以试图破坏它。有趣的是，`torch.empty(2, 2)`，`torch.empty((2, 2))`，`torch.empty([2, 2])` 似乎都可以工作，但 `torch.empty((2, (2, 2))` 给出了完全相同的错误，根据错误消息的内容，这是合理的。 此后，我尝试搜索 torch.empty() 的源代码，但找不到任何可靠的东西，而且我找到的任何内容都没有提到引发此错误。 但我的问题是，我已经非常明确地将 self.num_stocks 转换为 int()，但问题仍然存在。您的帮助对我很有帮助。谢谢！ 注意：`hyperparams` 是一个外部 JSON 文件，我正在读取它并将其转换为 DotDict。    由    /u/browbruh 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfseqv/pytorch_empty_function_erroring_out_i_have_some/</guid>
      <pubDate>Fri, 14 Jun 2024 14:39:09 GMT</pubDate>
    </item>
    <item>
      <title>我是否应该先运行元优化来排除不成功的模型？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfqwux/should_i_run_metaoptimization_to_rule_out/</link>
      <description><![CDATA[我正在尝试在大型图像数据集（100000+ 张图像）上训练识别器。由于我是在个人电脑上执行此操作，因此需要花费大量时间，并且我想避免运行导致死胡同或不能很好地收敛的模型架构和参数的所有训练周期。因此，我正在寻找更快排除性能不佳的模型的方法。 我的一个想法是首先尝试一堆超参数集：批量大小、优化器和模型架构，但周期数较少。然后对性能最佳的模型运行完整的（200+ 个周期）训练。 我预见到的一个问题是我的模型通常不会快速收敛 - 损失仅在 60-70 个周期左右后才开始显着下降。这是使用较小的数据集和较少多样化图片的情况 - 我认为包含多样化图片的大型数据集将需要更多时间才能开始收敛。 我还能做些什么来减少训练时间？我也可以尝试较小的数据集，但重点是训练模型来识别可能看起来非常不同的图片，而且我不确定小数据集上的性能是否是较大数据集上性能的良好指标。 任何建议都值得赞赏！    提交人    /u/smthamazing   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfqwux/should_i_run_metaoptimization_to_rule_out/</guid>
      <pubDate>Fri, 14 Jun 2024 13:31:33 GMT</pubDate>
    </item>
    <item>
      <title>网络中的网络 | 使用 Pytorch 解释深度神经网络</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfqjni/network_in_network_deep_neural_network_explained/</link>
      <description><![CDATA[        提交人    /u/research_pie   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfqjni/network_in_network_deep_neural_network_explained/</guid>
      <pubDate>Fri, 14 Jun 2024 13:14:15 GMT</pubDate>
    </item>
    <item>
      <title>[项目] 将十大 LMSYS 模型与通用 LLM API 库进行比较</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfpvdj/project_comparing_top_10_lmsys_models_with_a/</link>
      <description><![CDATA[      我们构建了一个开源 AI 网关，可让您访问和比较来自多个提供商的 200 多个 LLM。在基准问题上运行了前 10 个 LMSYS 排行榜模型： https://preview.redd.it/3f8nob1p8j6d1.png?width=549&amp;format=png&amp;auto=webp&amp;s=1e8cd9cdf96654ffe8e8ab0b208d556d38397ba4 结果？ GPT-4o 是唯一一个做对了的。 https://preview.redd.it/t0v1a7zp8j6d1.png?width=553&amp;format=png&amp;auto=webp&amp;s=7bf274357880c435bf7e9ba6e35926aca5c6505a 代码和演示：https://github.com/Portkey-AI/gateway/blob/main/cookbook/use-cases/LMSYS%20Series/comparing-top10-LMSYS-models-with-Portkey.ipynb 有什么想法吗？    提交人    /u/EscapedLaughter   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfpvdj/project_comparing_top_10_lmsys_models_with_a/</guid>
      <pubDate>Fri, 14 Jun 2024 12:40:56 GMT</pubDate>
    </item>
    <item>
      <title>我是唯一一个对人工智能/机器学习职业发展轨迹感到沮丧的人吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfls47/am_i_the_only_one_feeling_discouraged_at_the/</link>
      <description><![CDATA[大家好， 我很好奇其他人是否与此相关，如果是，你们中有人如何处理这个问题。  最近，我对担任 AI/ML 工程师感到非常沮丧、没有动力，也不太兴奋。这主要源于我的观察，这些观察表明，这类工程师的工作至少与整个 AI/ML 行业一样发生了变化。也就是说，变化很大，而且速度非常快。  我最喜欢的这个领域的一个方面是从头开始设计和开发个性化的定制模型。然而，除非我们从事严格的研究角色或学术工作（我主要指大学工作），否则我们似乎越来越无法利用这项技能谋生。  最近，似乎更重要的是如何使用模型，而不是创建模型，因为网上有太多开源模型可供获取并用于任何你想要的东西。我知道“如何使用它们一直很重要”，但说实话，与自己创建并自己或团队设计解决方案相比，整理已经为您预先打包的 Azure 模型感觉真的很无聊。不幸的是，预打包解决方案带来的易用性和部署速度才是最终赚钱的关键。 TL;DR：感到沮丧，因为我最喜欢的 AI/ML 东西开始感觉与行业无关，除非你只满足于严格的研究。还有谁能理解？    提交人    /u/1Motinator1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfls47/am_i_the_only_one_feeling_discouraged_at_the/</guid>
      <pubDate>Fri, 14 Jun 2024 08:13:44 GMT</pubDate>
    </item>
    <item>
      <title>小语言模型作为评估者</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfh5gk/small_language_models_as_evaluators/</link>
      <description><![CDATA[GroundedAI 正在开创更小、更高效的语言模型，专门用于评估 LLM 应用程序。 （即幻觉、毒性等） 如果您对当前的 LLM 应用评估方法感到失望，请加入我们的 Discord 以了解我们的创新解决方案。 模型：https://huggingface.co/grounded-ai Discord：https://discord.gg/V5Jme28u    提交人    /u/Jl_btdipsbro   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfh5gk/small_language_models_as_evaluators/</guid>
      <pubDate>Fri, 14 Jun 2024 03:13:16 GMT</pubDate>
    </item>
    <item>
      <title>Julia 中的神经网络</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1dfg7ig/neural_nets_from_scratch_in_julia/</link>
      <description><![CDATA[我最近创建了一个名为 SimpleGrad 的新 Julia 包。这里是文档站点和这里是 GitHub 存储库。 它是用于基本机器学习应用程序的梯度跟踪工具。但与其他 ML 包不同，SimpleGrad 的主要目标是教育性。其理念是，源代码应该足够易于阅读、理解和编辑/自定义，并且该包应该既可用于基本应用程序，也对学习 Julia 或 ML（或两者）的人有帮助。  文档还包括一个“引擎盖下”部分，解释了一切的工作原理，以便您可以根据需要从头开始重新创建它。我的目标是把它写成教科书章节，适合那些喜欢从第一原理了解事物工作原理的人。 此外，这是一个正在进行的项目，我会继续添加内容，所以如果有人有任何反馈、批评或功能请求，请告诉我！    提交人    /u/mike20731   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1dfg7ig/neural_nets_from_scratch_in_julia/</guid>
      <pubDate>Fri, 14 Jun 2024 02:21:30 GMT</pubDate>
    </item>
    <item>
      <title>Apple Silicon 会很快成为预算内 ML 的最佳选择吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1df7kxt/will_apple_silicon_be_the_best_choice_for_ml_on_a/</link>
      <description><![CDATA[我知道之前已经有人问过类似的问题，但随着 Apple 的 WWDC 公告，Mac 似乎可能成为机器学习领域 Nvidia 的真正强大替代品。Apple 正在使用 Apple Silicon 构建他们的计算云，我注意到现在有多少 Youtube 视频是关于在 Apple Silicon Mac 上本地运行 LLM 的。 无论如何，我真正的问题是，就投资具有足够内存的硬件以供非常大的 LLM 在本地运行和训练（而不是每个人都使用的 7B 参数小模型）而言，最明智的做法是什么...二手 Nvidia RTX 8000（48gb）还是几乎满配 RAM 的 Mac Studio？如果选择 RTX 8000，理想情况下我需要两个 96gb 的 Nvlinked，但它们不需要同时购买。  我知道 Nvidia 拥有更多的支持，但 Apple Silicon 似乎每天都在获得更多的支持，而且我认为这不会改变，尤其是在 Apple 完全进入 AI 领域的情况下。    提交人    /u/Serqetry7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1df7kxt/will_apple_silicon_be_the_best_choice_for_ml_on_a/</guid>
      <pubDate>Thu, 13 Jun 2024 19:34:23 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>