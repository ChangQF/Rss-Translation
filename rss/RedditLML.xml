<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Mon, 19 Aug 2024 09:17:02 GMT</lastBuildDate>
    <item>
      <title>机器学习中的 Kmeans 聚类 [从头开始编码]</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evwjql/kmeans_clustering_in_machine_learning_coded_from/</link>
      <description><![CDATA[很高兴分享我的机器学习系列的第七篇文章！ 您将学到什么：  Kmeans 背后的数学直觉 如何从头开始实现算法 Kmeans 闪耀的真实场景  链接：https://cckeh.hashnode.dev/a-step-by-step-guide-to-kmeans-clustering-in-machine-learning    提交人    /u/CC-KEH   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evwjql/kmeans_clustering_in_machine_learning_coded_from/</guid>
      <pubDate>Mon, 19 Aug 2024 08:22:40 GMT</pubDate>
    </item>
    <item>
      <title>“MindSearch：模仿人类思维进行更深入的人工智能搜索”</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evwhdj/mindsearch_mimicking_human_mind_for_deeper_ai/</link>
      <description><![CDATA[      Google 的搜索引擎可以以网页链接的形式为您提供最佳搜索结果。但不能直接回答您的查询。如果将搜索引擎与 GPT4o 等强大的现代 LLM 相结合，您可以获得对复杂查询的非常准确的响应。 但是，即使是搜索引擎 + LLM 组合也有局限性。 1) 网页搜索结果包含太多信息和噪音。这很容易超过 LLM 的上下文长度。 2) 如果查询很复杂，搜索引擎无法有效地进行搜索 人类查找信息的过程非常不同。我们不会试图直接回答复杂的问题。我们将问题模块化为更小、更易于管理的块。 想象一下你想买一辆车。你不会直接试图回答你的问题“我希望买一辆预算在 120 万卢比以下的汽车用于日常使用。我想要 XYZ 功能、里程和 CC”。 相反，你会从不同角度看待这个问题。你的品牌偏好、功能列表、转售价值、座位数、预计总使用年限/公里数、最高预算等。然后你会找到一份汽车清单。其中一些会符合你的一些要求。你会权衡你的选择，然后打电话。这是一个相当反复的过程。 如何将这种复杂的迭代决策融入人工智能辅助互联网搜索中？ 这篇题为“MindSearch”的论文于 3 周前在 arXiv 上发表，有一个很有趣的想法。模块化搜索查询并将其表示为具有节点和边的图形神经网络。部署 LLM 代理以通过搜索引擎 API 规划网络搜索。进行并行搜索查询来回答查询的不同部分。这称为细粒度搜索。 图将是动态的。根据 LLM 代理的初始网络搜索结果，更新节点和边。图中的节点数取决于搜索查询的复杂性。  我发布了一个 30 分钟的讲座视频来回顾这篇论文，以便来自各种背景的观众都能理解这篇论文。观看视频：https://www.youtube.com/watch?v=DQH6CM5sj7o 根据论文作者进行的研究，MindSearch 毫无疑问击败了现有的开放式和封闭式问题的 AI 辅助搜索解决方案。他们的 GitHub repo 已经有 3.7k 个星：https://github.com/InternLM/MindSearch 这是论文的链接：https://arxiv.org/abs/2407.20183 LLM 搜索要取代传统的 Google 搜索还有很长的路要走。MindSearch 的商业用途将归结为成本、准确性、幻觉率、速度以及 LLM 代理响应的实际效用。 无论如何，我认为现在批判性地评论这篇论文还为时过早。我只是喜欢他们写它的方式。有影响力的研究，易于理解，完全开放。这正是科学应该有的样子。向作者致敬。 https://preview.redd.it/5aaqxon2ykjd1.jpg?width=800&amp;format=pjpg&amp;auto=webp&amp;s=d91d41628640491671e1c6b0159f65c54b7bc236    提交人    /u/thesreedath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evwhdj/mindsearch_mimicking_human_mind_for_deeper_ai/</guid>
      <pubDate>Mon, 19 Aug 2024 08:17:58 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 边界框</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evw0rh/yolo_bounding_boxes/</link>
      <description><![CDATA[      嗨， 我想了解边界框在 YOLO 中为何以及如何工作。我读过一些帖子和解释，但我仍然不确定我是否真的理解了这个概念。 如何为大于网格的物体分配中心？我的意思是，如果网格看不到整个物体，它怎么能分辨出它是什么呢？  https://preview.redd.it/4zqgvdk8rkjd1.png?width=1446&amp;format=png&amp;auto=webp&amp;s=c2348485643f0b9ba3e95e9a12a71f32e988a601 这里的狗比 S x S 网格的每个单元格都大得多。例如，如果细胞只看到爪子，它怎么知道它检测到了狗？细胞是否以某种方式相互了解？我读到过，每个细胞（即其神经元）的接受域都大于其自身，但从架构上如何理解这一点？  https://preview.redd.it/sjjbnnnqrkjd1.png?width=2764&amp;format=png&amp;auto=webp&amp;s=7c1e8eac03b55b386387aba030ee6068010d97cf 我理解卷积不是在每个单元上独立运行的，但是如果最后的层都是 7 x 7，它们就比输入网格小得多，因此“分辨率”可能不允许我们制作准确的粘合框？  我不知道我是否很好地解释了我的观点，我真的很感激任何帮助:)谢谢！    提交人    /u/Advanced-Platform-97   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evw0rh/yolo_bounding_boxes/</guid>
      <pubDate>Mon, 19 Aug 2024 07:44:51 GMT</pubDate>
    </item>
    <item>
      <title>我现在正在进行 ml 专业化，之后我将学习深度学习课程的入门知识，但我怀疑我已经看到还有许多其他经典 ML 相关的东西，如贝叶斯线性注册、贝叶斯决策理论、内核、GDA 等。所以我什么时候会这样做？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evulbx/i_am_now_doing_the_ml_specialization_one_and/</link>
      <description><![CDATA[标题    提交人    /u/Agitated-Bowl7487   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evulbx/i_am_now_doing_the_ml_specialization_one_and/</guid>
      <pubDate>Mon, 19 Aug 2024 06:04:29 GMT</pubDate>
    </item>
    <item>
      <title>了解线性回归参数估计的方差</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evubvh/understand_variance_of_linear_regression/</link>
      <description><![CDATA[      因此，我正在阅读 ESL 并尝试更好地理解基础知识。在线性回归部分（第 3 章）中，讨论了估计 beta-hat 的方差（方程 3.8）。有人能解释一下这个方程是如何（逻辑上）得到的吗？我将尽力解释我对这个方程的困惑。 最小二乘回归的参数对于来自总体的单个样本是确定性的。这里单个样本代表一个数据集。因此，如果我们从这个总体（即更​​多数据集）中随机抽取样本，我们的 beta hat 估计值应该有一定的方差。但我不明白该方程的概念含义，它有两个分量，  X 转置和 X 的矩阵乘积的逆  现在，我知道矩阵的逆（用线性代数术语来说）是试图逆转由原始矩阵引起的变换。所以我的第一个问题是，X-transpose 和 X 之间的矩阵乘积代表什么变换？  在平方 sigma-hat 计算中，我期望分母为 1/N，而不是 1/(N-p-1)，其中 p 表示维数，额外的 1 表示偏差。当我联系一位朋友时，他告诉我这与自由度有关。我在这里感到困惑的是 - 假设我们有 100 个数据点和 2 个特征。(100 - 2 -1) = 97 如何成为自由度。有人可以用更简单的术语解释这个概念吗？以便更好地理解它。  以下是方程式的图片，供参考 https://preview.redd.it/il22jiu87kjd1.png?width=1144&amp;format=png&amp;auto=webp&amp;s=237863f9c016cd792faa39f0002253ed73fb930c 提前致谢！    由   提交  /u/PsychologicalRide127   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evubvh/understand_variance_of_linear_regression/</guid>
      <pubDate>Mon, 19 Aug 2024 05:47:46 GMT</pubDate>
    </item>
    <item>
      <title>除了梯度下降之外，还有哪些其他拟合模型的方法？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evsev6/what_are_some_other_methods_of_fitting_a_model/</link>
      <description><![CDATA[我正在做一个简单的项目，从头开始实现各种机器学习模型，我想提供使用不同方法拟合模型的灵活性。 例如，我希望用户能够选择他们使用的模型（例如线性回归/ SVM / KNN），还可以选择他们想要如何拟合模型（例如梯度下降，随机梯度下降等） 我注意到很多教程和项目都使用梯度下降作为拟合许多回归模型的默认方法。我想知道其他模型是否使用其他方法。 我只想要一个我要实现的各种方法的列表。 此外，除了网格搜索之外，还有其他超参数调整方法吗？    提交人    /u/leemanjoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evsev6/what_are_some_other_methods_of_fitting_a_model/</guid>
      <pubDate>Mon, 19 Aug 2024 03:49:55 GMT</pubDate>
    </item>
    <item>
      <title>缺失值 MNAR</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evpbh0/missing_values_mnar/</link>
      <description><![CDATA[大家好，“我正在研究一个机器学习项目，该项目有两列：‘一个人开始吸烟的年龄’和‘醒来和吸第一支烟之间的间隔时间’。这些列中的缺失值对应于非吸烟者（已经有了一个二元列表示吸烟者或非吸烟者）。我需要一个解决方案来处理这些缺失值，这些缺失值不会扭曲进一步的分析或模型，因为我不能使用平均值（非随机缺失值）和使用占位符，如 -1 或 999 idk。有什么建议吗？”    提交人    /u/medaziz777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evpbh0/missing_values_mnar/</guid>
      <pubDate>Mon, 19 Aug 2024 01:10:14 GMT</pubDate>
    </item>
    <item>
      <title>我为 Pokémon BDSP 创造了终极自动闪光猎人</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evp4yl/i_created_the_definitive_automatic_shiny_hunter/</link>
      <description><![CDATA[      大家好！我是 Dinones！我编写了一个使用对象检测的 Python 程序，让我的电脑在我睡觉时在我的实体 Nintendo Switch 上捕捉闪光神奇宝贝。到目前为止，我已经在 Pokémon BDSP 中自动捕捉了闪光宝可梦，如 Giratina、Dialga 或 Azelf、Rotom、Drifloon、所有三种初始宝可梦等等。想知道它是如何工作的吗？快来看看吧！该程序对所有人开放！显然是免费的；我只是一个喜欢在空闲时间编写这些程序的学生 :) 游戏在 Nintendo Switch（不是模拟的，是真实的）上运行。该程序使用捕获卡获取输出图像，然后对其进行处理以检测宝可梦是否闪光（OpenCV）。最后，它使用蓝牙（NXBT）模拟 joycons 并控制 Nintendo。 也可以在 Raspberry Pi 上使用！ 我不会用这个赚钱，我只是觉得我的项目会让很多人感兴趣。 📽️ Youtube：https://www.youtube.com/watch?v=84czUOAvNyk 🤖 Github：https://github.com/Dinones/Nintendo-Switch-Pokemon-Shiny-Hunter https://preview.redd.it/1pckbzv5sijd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=6c497fced41ad71dea07e7876edbabc9e63c3c9a https://preview.redd.it/lyz2e0x5sijd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=48b094bbf486382dfa685080b602ff4d0afdb554    提交人    /u/Dinones   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evp4yl/i_created_the_definitive_automatic_shiny_hunter/</guid>
      <pubDate>Mon, 19 Aug 2024 01:01:19 GMT</pubDate>
    </item>
    <item>
      <title>您有兴趣了解生物工程的数据科学吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evn75y/are_you_interested_in_learning_about_data_science/</link>
      <description><![CDATA[        提交人    /u/Faisal-CS   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evn75y/are_you_interested_in_learning_about_data_science/</guid>
      <pubDate>Sun, 18 Aug 2024 23:28:06 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器入门指南</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evl3rn/autoencoders_for_dummies/</link>
      <description><![CDATA[大家好！ 我的名字是 Eros，我是一名人工智能硕士生。 这是我试图让自动编码器的主题变得容易理解的尝试。 欢迎评论！ https://theelandor.github.io/prova/autoencoders.pdf    提交人    /u/Grand-Sale-2343   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evl3rn/autoencoders_for_dummies/</guid>
      <pubDate>Sun, 18 Aug 2024 21:54:04 GMT</pubDate>
    </item>
    <item>
      <title>对法学硕士 (LLM) 工作原理的直观解释</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evdab7/an_intuitive_explanation_of_how_llms_work/</link>
      <description><![CDATA[      博客文章中的一张图显示了 LLM 的示例输出。这些概率加起来为 1（或 100%） 嗨！ 我写了一篇博文文章，以非常直观的方式解释了 LLM 的工作原理。我们从高层次的抽象开始，其中 LLM 被视为个人助理，然后深入研究并涵盖标记化、采样和嵌入等概念。 我添加了一些图形以直观的方式说明一些概念。 我还解决了当前 LLM 的一些局限性，例如无法计算“strawberry”中的 R 以及反转字符串“copenhagen”。 我希望你觉得它有用！ 如果您有任何反馈或疑问，请告诉我。 https://amgadhasan.substack.com/p/explaining-how-llms-work-in-7-levels   由    /u/Amgadoz  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evdab7/an_intuitive_explanation_of_how_llms_work/</guid>
      <pubDate>Sun, 18 Aug 2024 16:24:31 GMT</pubDate>
    </item>
    <item>
      <title>数据科学路线图及免费资源 [分步指南]</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ev9hfm/data_science_roadmap_with_free_resources/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ev9hfm/data_science_roadmap_with_free_resources/</guid>
      <pubDate>Sun, 18 Aug 2024 13:39:34 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用线性回归模型，在末尾和开头是否有垂直散点线</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ev81n2/i_am_using_a_linear_regression_model_are_are/</link>
      <description><![CDATA[        提交人    /u/Beyond_Birthday_13   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ev81n2/i_am_using_a_linear_regression_model_are_are/</guid>
      <pubDate>Sun, 18 Aug 2024 12:27:11 GMT</pubDate>
    </item>
    <item>
      <title>当人们说代码不是“Pythonic”时他们的意思是什么</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ev5v09/what_do_people_mean_when_they_say_the_code_is_not/</link>
      <description><![CDATA[就 ML 而言，什么是 Python 代码？有什么基准需要遵循吗？ 谢谢。    提交人    /u/Infinite-Dragonfruit   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ev5v09/what_do_people_mean_when_they_say_the_code_is_not/</guid>
      <pubDate>Sun, 18 Aug 2024 10:12:23 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>