<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sat, 23 Mar 2024 06:20:27 GMT</lastBuildDate>
    <item>
      <title>微调LLM以完成的正确方法</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blleib/corret_way_of_finetuning_llm_for_completions/</link>
      <description><![CDATA[``` import torch from Transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments from peft import LoraConfig, get_peft_model /&gt; from datasets import Dataset from trl import SFTTrainer, DataCollat​​orForCompletionOnlyLM model_id = &quot;TinyLlama/TinyLlama-1.1B-Chat-v1.0&quot;; response_template = &quot;\n&lt;|助理|&gt;\n&quot; max_sequence_length = 2048 bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=&quot;nf4&quot;, bnb_4bit_compute_dtype=torch.bfloat16 ) tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={&quot;&quot;:0}, low_cpu_mem_usage=True) sharegpt_template = tokenizer.chat_template sharegpt_template = sharegpt_template.replace(&quot;角色&quot;, &quot;来自&quot;) sharegpt_template = sharegpt_template.replace(&quot;内容&quot;, &quot;值&quot;) sharegpt_template = sharegpt_template.replace(&quot;&#39;用户&#39;&quot;, &quot;&#39;人类&#39;&quot;) sharegpt_template = sharegpt_template.replace(&quot; ;&#39;assistant&#39;&#39;, &#39;&#39;gpt&#39;&#39;) tokenizer.chat_template = sharegpt_template config = LoraConfig( r=32, lora_alpha=32, # target_modules=[&quot;query_key_value&quot;], lora_dropout ＝0.05，偏差＝“无”，task_type＝“CAUSAL_LM” ) model = get_peft_model(model, config) print(model.print_trainable_parameters()) dataset = Dataset.from_json(“emoji_conversations.jsonl”) dataset = dataset.shuffle( ）数据集= dataset.train_test_split（test_size = 0.1，seed = 3407，）数据集 defformatting_prompts_func（示例）：convos =示例[“对话”]文本= [tokenizer.apply_chat_template（convo，tokenize =） False, add_ Generation_prompt = False) for convo in convos] return { “text”; : texts, } pass dataset = dataset.map(formatting_prompts_func, batched = True,) collat​​or = DataCollat​​orForCompletionOnlyLM(response_template, tokenizer=tokenizer) 训练器= SFTTrainer( model = model, tokenizer = tokenizer, train_dataset = dataset[&#39;train&#39;], eval_dataset= dataset[&#39;test&#39;], dataset_text_field = &quot;text&quot;, max_seq_length = max_sequence_length, dataset_num_proc = 2, Packing = False, data_collat​​or= coltator，args = triagnArguments（adam_beta1 = 0.9，adam_beta2 = 0.95，per_device_train_batch_size = 1，gradient_accumulation_steps = 1，hamperup_ratio = 0.01，num_train_epochs = 1，num_train_epochs = 1，Learning_rate = 4e-4e-4，fpppa = 4e-4 e-4，fppppa， BF16 =火炬.cuda.is_bf16_supported()、logging_steps = 1、optim =“adamw_bnb_8bit”、weight_decay = 0.1、lr_scheduler_type =“线性”、seed = 3407、output_dir =“输出”、#evaluation_strategy=“steps”、#save_strategy =“步数”, # save_total_limit=3, # eval_steps=100, # load_best_model_at_end=True, # metric_for_best_model=“eval_loss”, #greater_is_better=False, neftune_noise_alpha=5, max_steps=20, ), ) &lt; p&gt;trainer.train() ``` 这是使用 QLoRA 完成 LLM 微调的正确方法吗？    ;由   提交 /u/Suspiciously_Yours   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blleib/corret_way_of_finetuning_llm_for_completions/</guid>
      <pubDate>Sat, 23 Mar 2024 06:15:48 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow模型加载错误</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blkllz/tensorflow_model_loading_error/</link>
      <description><![CDATA[https://pastebin.com/ybhbVAN7 这是终端输出 这是代码：``` model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(256,activation = tf.nn.relu)) model.add(tf.keras.layers.Dense(self.NbrActions,activation = tf.nn.softmax)) model.compile(loss =“mse”,optimizer =“adam”)  model.save(&#39;model.keras&#39;) model1 = load_model(&#39;model.keras&#39;)   由   提交/u/Invicto_50  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blkllz/tensorflow_model_loading_error/</guid>
      <pubDate>Sat, 23 Mar 2024 05:24:07 GMT</pubDate>
    </item>
    <item>
      <title>利用 sCompute 的质量数据平台提高 ML 模型的准确性</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blk3ze/elevating_ml_model_accuracy_with_scomputes/</link>
      <description><![CDATA[嘿 learnmachinelearning 社区！ 我最近发现了一篇富有洞察力的文章，深入探讨了高质量数据的重要性构建强大的机器学习模型。对于我们这些一直在努力解决数据质量问题的人来说，您知道它对于任何 ML 项目的成功有多重要。 本文介绍了 sCompute，这是一个旨在为 ML 从业者提供访问数据的平台。他们的模型的最高质量数据。它概述了 sCompute 复杂的数据管理和验证流程如何极大地提高模型的准确性和可靠性。 关键要点：  sCompute 对高质量数据的承诺可确保更少的噪音和更高的准确性ML 模型。 该平台采用先进技术来收集、清理和准备可供使用的数据。 它提供了一种独特的方法来获取和验证数据集，使其与其他数据集区分开来。传统方法。  我相信，对于我们许多希望提高机器学习项目效率的人来说，这可能会改变游戏规则。本文深入探讨了 sCompute 的方法以及对 ML 社区的潜在影响。 你们中有人有使用 sCompute 或类似平台的经验吗？您对数据质量在您自己的机器学习工作中的重要性有何看法？ 期待热烈的讨论！ 这是为感兴趣的人准备的文章。 保持好奇心并不断学习！ &gt;   由   提交 /u/AnkushSantra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blk3ze/elevating_ml_model_accuracy_with_scomputes/</guid>
      <pubDate>Sat, 23 Mar 2024 04:54:49 GMT</pubDate>
    </item>
    <item>
      <title>在继续之前我应该​​阅读拉塞尔和诺维格的整本《人工智能》吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blhgw0/should_i_read_the_whole_book_of_artificial/</link>
      <description><![CDATA[我现在在读本科生。我的大学刚刚完成这本书，但他们只教了我们书中的一些主题。我有两个问题。  我应该在继续下一件事之前完成整本书吗？因为在这本1000页的书中，她只教了我们250-300页，并告诉我们这些就足够了。 读完这本书我应该做什么？我知道 Andrew NG ML 课程。我应该这样做吗？或任何其他机器学习书籍？  任何帮助将不胜感激。   由   提交 /u/itsmekalisyn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blhgw0/should_i_read_the_whole_book_of_artificial/</guid>
      <pubDate>Sat, 23 Mar 2024 02:33:22 GMT</pubDate>
    </item>
    <item>
      <title>Web 开发到 ML</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blfpqr/web_dev_to_ml/</link>
      <description><![CDATA[您好，我担任软件工程师已有大约 6 年了。我的大部分经验都是全栈 Web 开发。随着年龄的增长，我意识到我不再喜欢全栈 Web 开发，而是想尝试其他一些事情。我将机器学习视为一个有潜力的研究领域，因为我发现它很有趣，并且感觉这是一个值得了解的好领域。我的问题是，根据我的编程经验，你们会如何建议我开始学习这个主题？我应该遵循什么好的路线图吗？预先感谢您！   由   提交 /u/NoNeutralNed   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blfpqr/web_dev_to_ml/</guid>
      <pubDate>Sat, 23 Mar 2024 01:07:42 GMT</pubDate>
    </item>
    <item>
      <title>井字游戏</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blex79/tictactoe/</link>
      <description><![CDATA[我知道这对你们所有人来说可能是一个非常基本的问题，但我只是在介绍机器学习，我写了一个 python 程序随机玩井字游戏，就像它随机选择下一步行动一样，并且每次行动时，它都会跟踪游戏状态和要完成的下一步行动，例如（[1,0,0,0,0,0,0,0, -1], 2) 对应于一种游戏状态，其中在单元格 0 处有一个 X(1) qnd，在单元格 9 处有一个 O，下一步的移动是将 X 放置在单元格 2 中。 神经网络能否从这种格式的数据中学习在给定的特定游戏状态下采取什么最佳行动   由   提交 /u/confusedassff   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blex79/tictactoe/</guid>
      <pubDate>Sat, 23 Mar 2024 00:30:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 pytorch 制作具有多个输出（和多个类）的神经网络？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blekao/how_to_make_a_neural_network_with_multiple/</link>
      <description><![CDATA[例如，我的 train_features_data 的形状为 (4, 6)（即三行/示例和 6 列/特征），我的 train_target_data 的形状为形状 (4, 3)（即 4 行/示例和 3 列/目标）。对于每个目标，我有三个不同的类（-1、0、1）。 我为这个问题定义了一个示例模型架构（和数据），如下所示： 将 pandas 导入为 pd from torch import nn import torch feature_data = { &#39;A&#39;: [1, 2, 3, 4], &#39;B&#39;: [5, 6, 7, 8], &#39;C&#39;: [9, 10, 11, 12], &#39;D&#39;: [13, 14, 15, 16], &#39;E&#39;: [17, 18, 19, 20], &#39;F&#39;: [21, 22, 23, 24] } target_data = { &#39;Col1&#39;: [1, -1, 0, 1], &#39;Col2&#39;: [-1, 0, 1, -1], &#39;Col3&#39;: [-1, 0, 1, 1] } # 创建 DataFrame train_feature_data = pd.DataFrame(feature_data) train_target_data = pd.DataFrame(target_data) device = “cuda” if torch.cuda.is_available() else “cpu” # 创建模型类 MyModel(nn.Module): def __init__(self,inputs=6, l1=12,outputs=3): super().__init__() self.sequence = nn.Sequential( nn.Linear(inputs) , l1), nn.Linear(l1, 输出), nn.Softmax(dim=1) ) defforward(self, x): x = self.sequence(x) return x x_train = torch.tensor(train_feature_data.to_numpy( )).type(torch.float) model = MyModel(inputs = 6, l1 = 12,outputs = 3).to(device) model(x_train.to(device=device))  &lt; p&gt;当我将训练数据传递到模型中时（即，当我调用 model(x_train.to(device=device)) 时），我会返回一个形状数组 (4, 3)。 通过在此之后（资源&lt; /a&gt; （参见 链接）），我的期望是我会得到类似（4,3,3）的东西，其中4是我的特征和目标数据中的示例数量，中间的3（即第二个轴）代表每个示例的 logits（或者在本例中，因为我有一个 softmax 函数，这将是预测概率）（这将是 3，因为我有三个类），而第三个轴（或形状中最右边的 3 个值）表示我的 train_target_data 中的输出/列数。 有人可以提供一些指导，说明我在这里做错了什么（如果我的方法是错误的）以及如何修复它。我也会感谢有关此类问题的任何好的资源（例如多任务学习教程、YouTube 视频等）。谢谢。   由   提交/u/Individual_Ad_1214   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blekao/how_to_make_a_neural_network_with_multiple/</guid>
      <pubDate>Sat, 23 Mar 2024 00:14:08 GMT</pubDate>
    </item>
    <item>
      <title>KerasCV YOLOv8 COCO 预设有问题，但 VOC 没有问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blcwih/trouble_with_kerascv_yolov8_coco_presets_but_not/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blcwih/trouble_with_kerascv_yolov8_coco_presets_but_not/</guid>
      <pubDate>Fri, 22 Mar 2024 23:01:50 GMT</pubDate>
    </item>
    <item>
      <title>学习机器学习真的是商业“必须”吗？如果您创办一家企业是否有助于提高排名并增加销售额？ （我认为这是另一个领域吧）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blciwn/is_it_true_that_learning_machine_learning_is_a/</link>
      <description><![CDATA[所以我受到了影响，在抖音上看到这个，我只是想知道 1) 这是真的 2) 相关性是什么？谢谢:)   由   提交/u/redditprofile00   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blciwn/is_it_true_that_learning_machine_learning_is_a/</guid>
      <pubDate>Fri, 22 Mar 2024 22:45:51 GMT</pubDate>
    </item>
    <item>
      <title>为什么不会摸索？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1blb8h4/why_wont_it_grok/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1blb8h4/why_wont_it_grok/</guid>
      <pubDate>Fri, 22 Mar 2024 21:51:44 GMT</pubDate>
    </item>
    <item>
      <title>在注册 Andrew NG 机器学习课程之前我应该​​学习什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bl8v38/what_should_i_learn_before_enrolling_in_andrew_ng/</link>
      <description><![CDATA[我知道这是一种利基市场，但我知道这个子会有答案，哈哈。我只是一名二年级计算机科学学生，但我真的希望在夏天学习这门课程。我从我的第一门 compsci 课程中只了解基本的 python，因为我的大部分编程都是用 java 进行的。如果我打算花钱订阅这门课程，那么在学习之前我应该​​学习什么？有什么我应该习惯的特殊主题吗？我在编程世界里还是个婴儿哈哈。我学了很多数学和统计学知识，但是了解一些利基数学也很好吗？   由   提交/u/Efficient_Cress_6831   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bl8v38/what_should_i_learn_before_enrolling_in_andrew_ng/</guid>
      <pubDate>Fri, 22 Mar 2024 20:12:22 GMT</pubDate>
    </item>
    <item>
      <title>视频 LLAVA - 管道设置</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bl5vjx/video_llava_pipeline_setup/</link>
      <description><![CDATA[大家好，  我对 Hugging Face 和法学硕士还比较陌生。我目前正在尝试通过 Hugging Face 管道运行 Video LLAVA。我已经在 HF 代码的帮助下安装了模型和标记器，但之后我不知道如何继续。我想提供一个视频，我会向 Video LLAVA 提问。 如果有人可以帮助我或与我分享教程或笔记本来实现这一点，那将会很有帮助。  拥抱脸部链接：https://huggingface.co/LanguageBind/Video-LLaVA-7B   由   提交 /u/TelephoneParty5934   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bl5vjx/video_llava_pipeline_setup/</guid>
      <pubDate>Fri, 22 Mar 2024 18:10:10 GMT</pubDate>
    </item>
    <item>
      <title>IBM Machine Learning with Python 评论：学生的观点 -</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bl3a2r/ibm_machine_learning_with_python_review_students/</link>
      <description><![CDATA[ 由   提交/u/Sreeravan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bl3a2r/ibm_machine_learning_with_python_review_students/</guid>
      <pubDate>Fri, 22 Mar 2024 16:22:58 GMT</pubDate>
    </item>
    <item>
      <title>我需要一个合作伙伴与我一起处理一些 ML/DL 项目</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bkwmer/i_need_a_partner_to_work_with_me_on_some_mldl/</link>
      <description><![CDATA[您好，我一直在学习 ML/DL，并且几乎完成了 Andrew Ng 的 ML/DL 专业化，我现在正在学习 TensorFlow，以便我可以实现一些项目，我观看了这个关于TensorFlow for JavaScript (Google I/O &#39;18)的视频，我发现他们做到了我喜欢复制一些很酷的项目，这样我就可以学习如何构建自己的项目，我需要一个像我这样的情况的合作伙伴，并希望共同为构建它们做出贡献，我们将互相帮助搜索和建议材料以了解更多信息和代码部分结对编程。有人对这样的事情感兴趣吗？私信我或发表评论，我会私信你！   由   提交 /u/maelghrib   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bkwmer/i_need_a_partner_to_work_with_me_on_some_mldl/</guid>
      <pubDate>Fri, 22 Mar 2024 11:05:46 GMT</pubDate>
    </item>
    <item>
      <title>我应该走哪条通往 ML/AI 的道路？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bku13q/which_road_to_mlai_should_i_take/</link>
      <description><![CDATA[我已经完成了 Rob Tibshirani 和 Chris Manning 的《统计学习简介》课程，并一直在 Chhip Huyen 在她的博客上探索推荐的机器学习课程。然而，我质疑某些课程的相关性，例如斯坦福大学的 CS231N 和 CS231N 课程。 CS224N，因为他们缺乏对变形金刚等最新发展的报道。我没有选择麻省理工学院的线性代数课程，而是选择了 Rachel Thomas 的计算线性代数系列。鉴于我的志向是成为一名自学成才的数据科学家，能够获得一份有价值的职位，我应该采取什么学习途径？此外，我的实践经验包括在工作中合作开展基于 GPT 的项目，这既具有挑战性又具有教育意义。  想法？   由   提交/u/Aish-1992   /u/Aish-1992  reddit.com/r/learnmachinelearning/comments/1bku13q/which_road_to_mlai_should_i_take/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bku13q/which_road_to_mlai_should_i_take/</guid>
      <pubDate>Fri, 22 Mar 2024 08:04:22 GMT</pubDate>
    </item>
    </channel>
</rss>