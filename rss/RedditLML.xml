<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Tue, 28 Nov 2023 12:25:58 GMT</lastBuildDate>
    <item>
      <title>学习老师丑陋的圣诞毛衣 圣诞毛衣作为送给老师的圣诞礼物</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185ufh3/learning_teacher_ugly_christmas_sweater_christmas/</link>
      <description><![CDATA[       由   提交/u/NadineBell5689  [链接]  [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185ufh3/learning_teacher_ugly_christmas_sweater_christmas/</guid>
      <pubDate>Tue, 28 Nov 2023 12:01:42 GMT</pubDate>
    </item>
    <item>
      <title>你什么时候说你真正了解机器学习了？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185u9ht/when_do_you_say_you_actually_know_ml/</link>
      <description><![CDATA[什么时候你可以认为你了解机器学习。我一直在研究机器学习，也做了一些项目。有时我不确定它是否足够好。有太多的 tao ML 没有定义的标准或基准，你可以说你了解 ML。那么如何确定您了解 ML。   由   提交/u/OutsideNetwork3634   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185u9ht/when_do_you_say_you_actually_know_ml/</guid>
      <pubDate>Tue, 28 Nov 2023 11:51:34 GMT</pubDate>
    </item>
    <item>
      <title>如何在内存不足的情况下加载大模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185tyqm/how_to_load_big_model_on_low_memory/</link>
      <description><![CDATA[嘿，我今天在 HuggingFace 上发现了一个模型  https://huggingface.co/microsoft/Orca-2-7b ，我想在本地系统上运行它，但我有 8GB RAM，有什么办法可以运行它吗？ &lt; /div&gt;  由   提交 /u/Downtown-Rice-7560   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185tyqm/how_to_load_big_model_on_low_memory/</guid>
      <pubDate>Tue, 28 Nov 2023 11:32:02 GMT</pubDate>
    </item>
    <item>
      <title>AZURE 人工智能语言</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185tobl/azure_ai_language/</link>
      <description><![CDATA[大家好，我是 Azure AI 新手（因为我一直使用 GCP）。我阅读了文档，但没有找到任何有关基于已处理文档数量的可达性能的信息。 只是为了提供有关我所说内容的线索，Google Document AI 报告了此类类型信息（请参见此处：https://cloud.google.com/document-ai/docs /workbench/training-overview#number_of_documents) 有经验丰富的人可以帮助我吗？我专注于自定义文本分类和数据提取模型   由   提交/u/fedeloscaltro   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185tobl/azure_ai_language/</guid>
      <pubDate>Tue, 28 Nov 2023 11:13:31 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助寻找法学硕士</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185tkzn/need_help_finding_a_llm/</link>
      <description><![CDATA[您好， 我是一名学生，我和我的团队的任务是为我们的大学制作一个聊天机器人。我们需要制作一个聊天机器人来帮助其他学生找到有关他们课程的信息。我们将从多个大学网站的手册中获取数据（pdf 格式）。这些数据将使用 ChatGPT 4 转化为问答数据。 但是，我们正在努力寻找适合我们任务的预训练法学硕士。我们研究了 T5、BERT 和 GPT-2，但我们的老师对我们研究的模型感到惊讶，因为还有更流行和更新的模型。我们的聊天机器人必须是荷兰语，但我们可以进行翻译，因此法学硕士不需要接受荷兰语数据的培训。 LLM不能太大，因为我们没有用于非常大模型的硬件。 我们目前正在考虑openchat和falcon，两者都有7B参数。这些是不错的选择吗？或者有人对更好的法学硕士有任何建议吗？   由   提交 /u/Flo501   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185tkzn/need_help_finding_a_llm/</guid>
      <pubDate>Tue, 28 Nov 2023 11:07:16 GMT</pubDate>
    </item>
    <item>
      <title>在 leetcode 上失败很容易，但我已经在数据科学和分析领域工作了近 5 年！该怎么办？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185sze3/failing_at_leetcode_easy_but_ive_been_working_in/</link>
      <description><![CDATA[这太糟糕了，我什至不感到羞耻，实际上很搞笑。  我使用 python、sql、DAX 已经近 5 年了，最近使用 pyspark 和 Sparksql。 我决定尝试一下 leetcode 简单的 python 问题，问题是与我在职业生涯中遇到的任何事情都不一样。  即使尝试变得优秀，也感觉像是在浪费时间。 但我至少想变得更舒服，以防万一。我正在考虑换工作，可能会遇到一些 leetcode 面试。 有人和我处境相似吗？你在 leetcode 怎么样？ 有什么资源可以帮助你更好地解决这些 leetcode 问题吗？   由   提交 /u/scrotalist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185sze3/failing_at_leetcode_easy_but_ive_been_working_in/</guid>
      <pubDate>Tue, 28 Nov 2023 10:27:42 GMT</pubDate>
    </item>
    <item>
      <title>预订机器学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185scmw/book_for_ml/</link>
      <description><![CDATA[我们有没有一本书可以从头开始学习 ML 及其背后的数学？ ​ ​ ​ #MachineLearning   由   提交 /u/peeyushkmisra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185scmw/book_for_ml/</guid>
      <pubDate>Tue, 28 Nov 2023 09:44:29 GMT</pubDate>
    </item>
    <item>
      <title>我想出了一个合成数据集生成器的想法，这行得通吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185s71w/ive_come_up_withan_idea_for_a_synthetic_dataset/</link>
      <description><![CDATA[   /u/JakeN9  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185s71w/ive_come_up_withan_idea_for_a_synthetic_dataset/</guid>
      <pubDate>Tue, 28 Nov 2023 09:33:32 GMT</pubDate>
    </item>
    <item>
      <title>从哪儿开始？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185qksg/where_to_start/</link>
      <description><![CDATA[我在完全不同的领域拥有 2 年的 IT 经验，对机器学习感兴趣，有什么建议我可以从哪里开始，因为我擅长数学和 Python好吧   由   提交 /u/Rangaul   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185qksg/where_to_start/</guid>
      <pubDate>Tue, 28 Nov 2023 07:39:19 GMT</pubDate>
    </item>
    <item>
      <title>2024 年适合所有人的 12 门 Coursera 免费机器学习课程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185orm7/12_best_coursera_free_courses_machine_learning/</link>
      <description><![CDATA[   /u/Aqsa81  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185orm7/12_best_coursera_free_courses_machine_learning/</guid>
      <pubDate>Tue, 28 Nov 2023 05:40:22 GMT</pubDate>
    </item>
    <item>
      <title>在 Transformer 模型上遇到梯度消失问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185jx2j/running_into_vanishing_gradient_on_transformer/</link>
      <description><![CDATA[大家好！抱歉，如果我在这里遗漏了一些完全明显的东西，但我在使用一个非常简单的变压器模型时遇到了一些问题，该模型正在对推文进行情感分类训练。我这样做只是作为变压器的最小可行示例，以便我可以更好地理解它们。我还使用 PyTorch 内置的 nn.Transformer。 通过阅读一些文献，我发现 Transformer 如此出色的部分原因是因为你可以使它们变得非常深，而不会遇到梯度消失的情况。然而，在我的模型中，如果我使编码器/解码器层的数量大于 1，我就会遇到梯度消失的情况（平均梯度幅度约为 1e-10 到 1e-12，并且损失根本不会减少）。当我只运行 1 层模型时，我倾向于看到平均只有 1e-8 梯度幅度，这是非常小的，但模型似乎仍在学习。 这导致我遇到第二个问题..模型确实学习了，但仅在一两个时期之后，训练和验证损失似乎出现了分歧，表明模型过度拟合。我的模型如此快地过度拟合，这似乎不太正确。 一些特别说明：我正在使用 AMP 优化。我尝试删除它，看看这是否会扰乱我的渐变。事实并非如此。 完全披露：这是一个学校项目，但我的模型的表现与我的成绩不相关。我问这个问题不是为了分数，而是因为我想了解这些东西是如何工作的以及我哪里出错了。 如果有人想查看更详细的细节，这里是代码（我会重点关注train_transformer.py、src/transformer.py和src/utils.py）https://github.com/NoahSchiro/cs448_final。有问题的数据集是 https://www.kaggle.com/code/paoloripamonti/twitter-sentiment -analysis/input 我的猜测是，变压器模型对于手头的任务来说太过强大，这就是为什么我看到非常快的收敛/梯度消失。任何帮助是极大的赞赏！    由   提交 /u/NoahSchiro   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185jx2j/running_into_vanishing_gradient_on_transformer/</guid>
      <pubDate>Tue, 28 Nov 2023 01:27:46 GMT</pubDate>
    </item>
    <item>
      <title>在 PyTorch 中实现软最近邻损失</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185jdzp/implementing_soft_nearest_neighbor_loss_in_pytorch/</link>
      <description><![CDATA[您好！我已经有一段时间没有写博客了。我写了两篇博客，内容如下： - 通过解缠改进 k 均值聚类： https://medium.com/@afagarap/improving-k-means-clustering-with-disentanglement-caf59a8c57bd - 在 PyTorch 中实现软最近邻损失：https://medium.com/@afagarap/implementing-soft-nearest-neighbor-loss-in-pytorch-b9ed2a371760 第一篇文章涵盖了我在 IJCNN 2020 中发表的论文，而第二篇文章是关于实现我们在参考论文中提出和扩展的损失函数的分步教程。 我希望您喜欢阅读它们。谢谢！   由   提交/u/afagarap  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185jdzp/implementing_soft_nearest_neighbor_loss_in_pytorch/</guid>
      <pubDate>Tue, 28 Nov 2023 01:02:45 GMT</pubDate>
    </item>
    <item>
      <title>关于多头LLM自我关注的问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185ibbr/question_regarding_multi_head_llm_self_attention/</link>
      <description><![CDATA[嗨，我刚刚开始学习 Transformer 架构，有一个问题：  当多头时自注意力计算 Q、K 的点积，你会得到一个注意力分数，同一个单词对齐的对角线没有有害影响，因为它是均匀分布的，但这不会与软最大尺度混淆，为什么每个单词都与本身？ （已编辑） 我们难道不想标准化相同单词计算之间的对角线吗，否则注意力分数会放大单词 -&gt;词，这会稀释注意力分数乘积？ GPT 说变压器依靠权重的差异来确定注意力模式，但它值得探索（我想特别是当变压器拾取这个值时）模式本身）？    由   提交 /u/JakeN9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185ibbr/question_regarding_multi_head_llm_self_attention/</guid>
      <pubDate>Tue, 28 Nov 2023 00:14:23 GMT</pubDate>
    </item>
    <item>
      <title>为什么线性回归不能解决分类问题？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185cbk8/why_doesnt_linear_regression_work_on/</link>
      <description><![CDATA[你好。我是机器学习的初学者。我试图真正理解为什么线性回归不适用于分类问题。  我经常给出这样的答案：“它预测连续值”或“找到最佳拟合线”或类似的东西。  这对我来说很难直观地理解，并且我已经尝试弄清楚这一点已经超过三周了。  我正在研究泰坦尼克号数据集并尝试使用线性回归，但我什至不知道如何使其与线性回归一起工作。  我知道这不是 LR 的目的，但我只是想真正看到并理解为什么会这样。  如果可能的话，像个新手一样解释一下。没有复杂或隐性的语言   由   提交 /u/Ok_Tumbleweed8796   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185cbk8/why_doesnt_linear_regression_work_on/</guid>
      <pubDate>Mon, 27 Nov 2023 20:10:35 GMT</pubDate>
    </item>
    <item>
      <title>探索即将到来的人工智能格局</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/185910n/navigating_the_ai_landscape_on_the_horizon/</link>
      <description><![CDATA[我想现在我们大多数人都听过这样一句话：“人工智能不会取代工作，知道如何使用人工智能的人会取代那些会使用人工智能的人”  对于我们许多人来说，他们不是程序员或软件架构师，而是从事营销、会计、项目管理等工作的白领，我们到底应该开始学习什么？  我只能考虑了解一些基本概念（例如，什么是生成人工智能，什么是幻觉等），那里的大玩家及其关注点（例如，谁最擅长做文本）视频、图像生成等），也许还有一些关于提示优化的技巧，但仅此而已。  我没有看到差异化因素，因为所有这些都可以由任何人在一个下午学会，并且在网络上搜索“[我的职业]的人工智能最佳实践”就可以了。或者直接提示人工智能平台听起来也不太复杂。 这是全部还是我遗漏了什么？  &amp;# 32；由   提交 /u/YepYepisalifestyle   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/185910n/navigating_the_ai_landscape_on_the_horizon/</guid>
      <pubDate>Mon, 27 Nov 2023 17:55:22 GMT</pubDate>
    </item>
    </channel>
</rss>