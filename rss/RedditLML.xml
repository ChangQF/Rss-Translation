<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Thu, 28 Nov 2024 09:18:59 GMT</lastBuildDate>
    <item>
      <title>DS/ML 和应用科学面试怎么会比 SWE 面试难这么多呢？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1rsjw/how_can_dsml_and_applied_science_interviews_be/</link>
      <description><![CDATA[我与亚马逊进行了最后 5 轮应用科学面试。 每轮面试内容如下：（每轮 1 小时，单日面试）  ML 广度（所有经典 ML 和 DL，所有内容都将进行一定深度测试，+ 数学推导） ML 深度（深入研究您的一般研究领域/或切线，激烈的质问） 编码（ML Algos 编码 + Leetcode 媒介） 科学应用：ML 系统设计，解决一些广泛的问题 行为：Bar Raiser 就领导原则进行 1.5 小时质问  您需要对 ML 中无限数量的概念有广泛而深入的知识，并且能够回忆和准确地重现它们，包括数学。 这么多本身基本上是不可能实现的（特别是对于像我这样记忆力和回忆能力较差的人来说）。 即使在您的研究领域（这本身就是一个巨大的领域），也可能有大量问题或整个领域您一无所知。 + 您需要与 SWE 2 相同级别的编码。 ______ 而这正是包括亚马逊在内的几乎所有公司中的 SWE 所需要的： - Leetcode 练习。 - 如果是高级，则进行系统设计。 我很擅长 Leetcode - 它是临时思考和解决问题的。即使没有练习，我在编码测试中也做得很好，通过练习，你基本上可以看到大多数问题和模式。 我根本不擅长记住软边缘支持向量机的晦涩理论细节，然后突然跳到为什么 RLHF 有问题，将 LLM 与人类偏好相结合，然后被告知从头开始在 PyTorch 中编写稀疏注意力 ______ 最糟糕的是，在掌握了这么多知识并付出了这么多努力之后，得到的报酬是一样的。即使是工作也要困难 100 倍，因为你可能需要做的事情种类繁多。 与此相反，你通常会作为 SWE 拥有一套专业知识，在某个领域建立明确的能力，并且总是可以毫无问题地跳到任何只需要这些而不需要其他任何东西的工作中。    提交人    /u/anotheraccount97   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1rsjw/how_can_dsml_and_applied_science_interviews_be/</guid>
      <pubDate>Thu, 28 Nov 2024 08:51:21 GMT</pubDate>
    </item>
    <item>
      <title>项目需要指导</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1rmly/guidance_needed_for_project/</link>
      <description><![CDATA[大家好， 我正在开展一个项目，旨在构建一个优化的 GPT 平台，该平台以提示作为输入并对多个 LLM（例如 ChatGPT、Perplexity 等）发出 API 调用。目标是评估这些模型的响应并向用户返回最佳输出。 作为一名学生，我的预算很紧张，我需要您在几个关键点上的指导：  具有成本效益的 API 调用： 如何设计 API 调用以使其不会过于昂贵？ 是否有任何最佳实践或技术可以优化 OpenAI 和 Perplexity 等 API 的令牌使用？  减少延迟： 如何有效处理 API 调用以避免用户长时间等待？ 是否有策略可以并行管理多个 API 调用而不会显着增加响应时间？   此外，我想尝试使用开源模型（如 LLaMA 或 Mistral）构建此系统。但是，我受到硬件的限制：我有一台没有 GPU 的 Dell Inspiron 5000 笔记本电脑。  我可以使用哪些在 CPU 上运行良好的轻量级开源模型？ 是否有基于云的解决方案（最好是免费或低成本的）让我可以尝试运行这些模型？  任何建议、资源或提示都将非常有帮助。    提交人    /u/Dependent_Acadia_433   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1rmly/guidance_needed_for_project/</guid>
      <pubDate>Thu, 28 Nov 2024 08:38:30 GMT</pubDate>
    </item>
    <item>
      <title>Mac GPU 上的随机森林（Metal）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1pz66/random_forest_on_mac_gpu_metal/</link>
      <description><![CDATA[大家好， 有人知道如何使用 Mac 上的 GPU 来运行随机森林或 XGBoost 吗？ 我在网上搜索过，但找不到任何 Python 库来帮助解决这个问题。我通常使用的 Scikit 不兼容 Metal。我找到了一些 CUDA 集成模型的库，但不幸的是我没有 NVIDIA GPU，而且在 CPU 上，我的代码需要很长时间，尽管它非常强大（M2 Ultra）。 谢谢帮助。    提交人    /u/Dr_Superfluid   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1pz66/random_forest_on_mac_gpu_metal/</guid>
      <pubDate>Thu, 28 Nov 2024 06:42:33 GMT</pubDate>
    </item>
    <item>
      <title>最新的 MacBook Pro/MacBook Air 适用于深度学习吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1prl0/latest_macbook_pro_macbook_air_for_deep_learning/</link>
      <description><![CDATA[我有特定要求。我选择 MacBook（而不是基于 Windows 的机器）是因为电池寿命（对我来说非常重要），而且我一生都是 Linux/Mac 用户。但是我对选择哪款 MacBook 感到困惑，是选择最好的还是只选择基本款以节省一些钱。 我的技术栈/主要工作：我主要从事计算机视觉工作，但有时也从事 LLM 工作。做研究。尝试实现论文（主要基于 Linux，有时库/构建至少在我的 Intel Mac 上不起作用，只能在 Linux 上工作）。我也做视频编辑工作（这就是选择 Mac 的原因） MacBook Pro：如果我要使用它，我会选择带有 M4 Pro（14 核 CPU，20 核 GPU）的 MBP。它有 24 GB 的总 RAM。现在，我认为我不会在这台机器上进行直接训练。但我认为最好有这样的计算来进行推理，或者构建/编译项目，或者更好地创建数据（图像、视频）。但我仍然需要 SSH/云 GPU 来训练我的模型（这就是我目前的做法）。 MacBook Air：我会省下所有的钱，只需将这台机器连接到 SSH/云 GPU，然后用省下的钱购买云代币。但我想知道无风扇机器能否维持工作负载，尤其是长时间每天 10-12 小时。另外，本地没有推理（我怀疑）。所以在这种情况下我会完全依赖云。另外，这对于视频编辑来说不是很好（但我没问题，因为我偶尔会这样做）。 我应该选哪一个？你觉得这些机器怎么样？ PS：我知道有一款带风扇和 M4 的基准 MacBook Pro，但我宁愿跳过它，购买具有更快 SSD 和更多 RAM 的基准 M4 pro。     由    /u/DramaticCloud1498 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1prl0/latest_macbook_pro_macbook_air_for_deep_learning/</guid>
      <pubDate>Thu, 28 Nov 2024 06:28:33 GMT</pubDate>
    </item>
    <item>
      <title>实现类似Pnet(Mtcnn)模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1nnmg/implementing_similar_pnetmtcnn_model/</link>
      <description><![CDATA[我正在构建一个受 MTCNN 启发的简化人脸检测模型，旨在将 12x12 裁剪图像分类为包含人脸或不包含人脸（因此只有一个输出 1 或 0）。我使用 Wider Face 数据集对其进行训练，裁剪并调整人脸区域为 12x12，同时还包括偏移人脸（人脸的部分视图）和随机非人脸裁剪。为了进行测试，我使用 OpenCV 在 240x240 摄像头馈送上实现了滑动窗口（12x12，步长为 2）方法。如果窗口检测到人脸，则会突出显示其位置。 结果很差（我的脸经常被忽略，模型主要突出显示背景），可能是因为 12x12 的小输入尺寸丢失了关键信息，使得模型很难区分人脸和非人脸。那么你有什么建议可以解决这个问题吗？谢谢🙏 另外，我还删除了 bbox 输出，因为我认为我可以将所有突出显示的部分提供给另一个模型，以进一步区分面部和非面部。     提交人    /u/GateCodeMark   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1nnmg/implementing_similar_pnetmtcnn_model/</guid>
      <pubDate>Thu, 28 Nov 2024 04:18:49 GMT</pubDate>
    </item>
    <item>
      <title>新推理 LLM：QwQ 在多个基准测试中击败 OpenAI-o1</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1nkck/new_reasoning_llm_qwq_beats_openaio1_on_multiple/</link>
      <description><![CDATA[阿里巴巴最新的推理模型 QwQ 在许多基准测试中击败了 o1-mini、o1-preview、GPT-4o 和 Claude 3.5 Sonnet。该模型只有 32b，并且完全开源。查看如何使用它：https://youtu.be/yy6cLPZrE9k?si=wKAPXuhKibSsC810    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1nkck/new_reasoning_llm_qwq_beats_openaio1_on_multiple/</guid>
      <pubDate>Thu, 28 Nov 2024 04:13:23 GMT</pubDate>
    </item>
    <item>
      <title>𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱𝗶𝗻𝗴 𝗧𝗼𝗸𝗲𝗻𝗶𝘇𝗮𝘁𝗶𝗼𝗻 𝗶𝗻 𝗟𝗮𝗿𝗴𝗲 𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗠𝗼𝗱𝗲𝗹𝘀: 𝗪𝗼𝗿𝗱, 𝗖𝗵𝗮𝗿𝗮𝗰𝘁𝗲𝗿, 𝗮𝗻𝗱 𝗕𝘆𝘁𝗲 𝗣𝗮𝗶𝗿 𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1n1dn/𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱𝗶𝗻𝗴_𝗧𝗼𝗸𝗲𝗻𝗶𝘇𝗮𝘁𝗶𝗼𝗻_𝗶𝗻_𝗟𝗮𝗿𝗴𝗲_𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲/</link>
      <description><![CDATA[      Tokenizer 自然语言处理是自然语言处理的基石（NLP），多年来，已经开发出各种方法来优化它。其中最值得注意的方法是基于字、基于字符和字节对编码 (BPE)。 𝗪𝗼𝗿𝗱-𝗯𝗮𝘀𝗲𝗱：虽然直观，但它需要维护庞大的词汇量——高达 𝟭𝟳𝟬,𝟬𝟬𝟬 𝗰𝘂𝗿𝗿𝗲𝗻𝘁 𝘄𝗼𝗿𝗱𝘀（牛津词典）和 47,000 个过时的单词。尽管如此，它仍在努力应对未知单词标记。 𝗖𝗵𝗮𝗿𝗮𝗰𝘁𝗲𝗿-𝗯𝗮𝘀𝗲𝗱：通过将词汇量减少到英语中仅有 𝟮𝟱𝟲 𝗰𝗵𝗮𝗿𝗮𝗰𝘁𝗲𝗿𝘀，它解决了未知标记问题。但是，它无法有效地保留单词的语义。 𝗕𝘆𝘁𝗲 𝗣𝗮𝗶𝗿 𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴 (𝗕𝗣𝗘)：字节对编码 (BPE) 是一种基于子词的标记器。它的工作原理是将最常见的相邻字符对迭代合并为一个单元，直到达到所需的词汇量。它通过将单词分解为子单词、有效处理未知标记以及与基于单词的编码相比保持词汇量易于管理，达到了完美的平衡。 这种在保持语义连贯性的同时处理看不见的单词的能力使得 BPE 标记器成为大多数 𝗺𝗼𝗱𝗲𝗿𝗻 𝗹𝗮𝗿𝗴𝗲 𝗹𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗺𝗼𝗱𝗲𝗹𝘀 的标准。 标记化创新是我们今天在 NLP 中看到的进步的关键推动因素！  要深入了解 tokenizer，我强烈建议您观看以下视频： • 使用 Python 从 Scratch 编写 LLM Tokenizer：https://youtu.be/rsy5Ragmso8 • GPT Tokenizer：字节对编码：https://youtu.be/fKd8s29e-l4 作者：Raj Abhijit Dandekar 𝘍𝘰𝘳 𝘴𝘪𝘮𝘪𝘭𝘢𝘳谢谢大家的支持：Pritam Kudale 我们的未来之路谢谢你Vizuara！ --- 您可以在此处加入新闻通讯： https://9bfb8b39.sibforms.com/serve/MUIFAJFcOMHmiNnOggw1w5qD7tmpEtKMgA6BKj_WzggssRmgSDHoVWfB1OZOjVAB7uaJYCbWnvH-HG2NpolvOj6qHUOLkEJ5YA_cwnKeEIKulJ_h6NhvVaX9yGKM3ACtCZ5eITK80_zhvdz8uOdHfW46XkLnTiZsZzyX4nfyr6pzGMAumdmlv-UNZcYsNI5YipaBImsHcnpCeibg    提交人    /u/Ambitious-Fix-3376   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1n1dn/𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱𝗶𝗻𝗴_𝗧𝗼𝗸𝗲𝗻𝗶𝘇𝗮𝘁𝗶𝗼𝗻_𝗶𝗻_𝗟𝗮𝗿𝗴𝗲_𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲/</guid>
      <pubDate>Thu, 28 Nov 2024 03:43:17 GMT</pubDate>
    </item>
    <item>
      <title>无需 API 调用的 LangGraph</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1ld8j/langgraph_without_api_calls/</link>
      <description><![CDATA[晚上好， 我正在尝试学习使用 LangGraph 创建多智能体项目，该操作基于 LangGraph Quickstart。我想知道如何使用无 API 的系统和 LangGraph。我尝试使用 Hugging Face 模型，并能够成功使用调用命令。但是，当我将模型作为聊天机器人的一部分调用时（在设置开始、聊天机器人和结束节点之后），我得到了通用的 AttributeError：&#39;str&#39; 对象没有属性 &#39;content&#39;。 我想知道这是否与我选择的模型有关。如有必要，我可以提供特定的代码。此外，如果有必要，我非常愿意以其他方式进行操作。非常感谢！    提交人    /u/Need_More_Learn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1ld8j/langgraph_without_api_calls/</guid>
      <pubDate>Thu, 28 Nov 2024 02:12:09 GMT</pubDate>
    </item>
    <item>
      <title>需要建议：如何以及在哪里学习 ML 模型部署/将 ML 模型部署到生产中？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1h98a/advice_needed_how_and_where_to_learn_ml_model/</link>
      <description><![CDATA[我正在寻找一些关于如何将机器学习模型部署到生产中的指导/资源。我写这篇文章的原因是，在针对不同用例进行部署时，有太多的服务/工具。 以下是我的一些背景：我在机器学习方面有扎实的基础，并围绕 LLM 构建了多个应用程序，但我从未真正部署过模型。    提交人    /u/Rare_Mud7490   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1h98a/advice_needed_how_and_where_to_learn_ml_model/</guid>
      <pubDate>Wed, 27 Nov 2024 22:47:51 GMT</pubDate>
    </item>
    <item>
      <title>理解大型语言模型 (LLM)：全面概述</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1awif/understanding_large_language_models_llms_a/</link>
      <description><![CDATA[      https://reddit.com/link/1h1awif/video/skvim49gjz2e1/player Lar 当你开始学习大型语言模型 (LLM) 时，你可能会对网上可用的大量内容感到不知所措。为了简化这一过程，我整理了一份 LLM 中关键主题的概述，以帮助你以结构化的方式掌握概念。仅仅听说一项新技术可能不足以完全理解它，但将其分解为易于理解的概念并提供资源可能是加深理解的好方法。 在这篇文章中，我将分享重要的资源和主题供您探索，这将有助于您在 LLM 领域打下坚实的基础。如果某个主题引起了您的兴趣，我鼓励您使用提供的链接深入研究它。每个视频都会引导您了解 LLM 的特定方面，从基础知识到更高级的主题。 以下是入门概述： 1. 大型语言模型 (LLM) 简介 从 LLM 的基础知识开始，了解它们是什么以及它们为什么重要。点击此处观看 2.预训练与微调 LLM 了解预训练和微调之间的区别，这是 LLM 开发中的两个关键步骤。 点击此处观看 3. 什么是 Transformer？ Transformer 是许多现代 LLM 的骨干。了解此架构的工作原理。 点击此处观看 4. GPT-3 究竟是如何工作的？ 深入了解最著名的 LLM 之一 — GPT-3 的内部工作原理。 观看此处 5. 从头开始​​构建 LLM 的阶段 探索从头开始构建 LLM 所涉及的步骤。 观看此处 6. 使用 Python 从 Scratch 编写 LLM 标记器 理解和构建 LLM 标记器的实践指南。 观看此处 7. GPT 标记器：字节对编码 了解标记化中使用的关键技术之一：字节对编码 (BPE)。点击此处观看 8. 什么是标记嵌入？ 了解标记嵌入的概念及其在 LLM 中的作用。点击此处观看 9. 位置嵌入的重要性 探索位置嵌入如何帮助 LLM 理解序列中标记的顺序。 点击此处观看 10. LLM 的数据预处理流程 了解支持 LLM 的复杂数据预处理流程。 点击此处观看 通过浏览这些视频，您将更清楚地了解 LLM 的工作原理以及促成其成功的各种组件。我鼓励您按照最适合您的顺序关注这些资源，并深入研究引起您兴趣的主题。 如果您有任何疑问或需要更多资源，请随时提问！祝您学习愉快    提交人    /u/Ambitious-Fix-3376   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1awif/understanding_large_language_models_llms_a/</guid>
      <pubDate>Wed, 27 Nov 2024 18:16:26 GMT</pubDate>
    </item>
    <item>
      <title>卷积解释</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h17ok5/convolutions_explained/</link>
      <description><![CDATA[      大家好！ https://preview.redd.it/bm3eqih0xg3e1.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=d3a55c85cf53693611a5c32d8b3f4ceadb5ff734 我拍摄了我的第一个YouTube视频，这是一个关于卷积的教育视频（数学定义、应用手册内核在计算机视觉中的作用，并解释它们在卷积神经网络中的作用）。 需要您的反馈！  是否足够容易理解？ 长度是否适合处理信息？  谢谢！ 我想制作的下一个视频将更实用（例如如何在 Vertex AI 中设置 ML 管道）    提交人    /u/nepherhotep   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h17ok5/convolutions_explained/</guid>
      <pubDate>Wed, 27 Nov 2024 16:02:24 GMT</pubDate>
    </item>
    <item>
      <title>我快要疯了。我向 MLE 职位投递了 200 份简历，但只有 10 次面试。我做错了什么？我应该补充什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h126tg/im_slowly_losing_my_mind_200_resumes_sent_for_mle/</link>
      <description><![CDATA[        提交人    /u/Bitter-Surprise-7508   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h126tg/im_slowly_losing_my_mind_200_resumes_sent_for_mle/</guid>
      <pubDate>Wed, 27 Nov 2024 11:28:31 GMT</pubDate>
    </item>
    <item>
      <title>线性代数项目，我从头开始实现了一个带动画的 K-Means，效果不错？我们需要添加一个停止条件，即使质心几乎没有变化，它也会继续，有没有什么提示可以说明这个条件是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/</link>
      <description><![CDATA[        提交人    /u/Ang3k_TOH   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/</guid>
      <pubDate>Wed, 27 Nov 2024 07:13:57 GMT</pubDate>
    </item>
    <item>
      <title>有谁完成过 Andrew Ng 的 ML 专业化课程并且目前在 ML 领域工作吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h0x63n/anyone_whos_done_andrew_ngs_ml_specialization_and/</link>
      <description><![CDATA[对于那些从 Andrew Ng 的 ML 专业课程开始学习 ML 并且现在在 ML 领域工作的人来说，你的道路是什么样的？    提交人    /u/lil_leb0wski   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h0x63n/anyone_whos_done_andrew_ngs_ml_specialization_and/</guid>
      <pubDate>Wed, 27 Nov 2024 05:34:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>