<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sat, 16 Mar 2024 03:14:19 GMT</lastBuildDate>
    <item>
      <title>实施主成分分析 (PCA)</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfv3yz/implementing_principal_component_analysis_pca/</link>
      <description><![CDATA[   /u/itsstylepoint  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfv3yz/implementing_principal_component_analysis_pca/</guid>
      <pubDate>Sat, 16 Mar 2024 02:04:59 GMT</pubDate>
    </item>
    <item>
      <title>乐高物体检测数据集</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfun7z/lego_object_detection_data_set/</link>
      <description><![CDATA[       这是我断断续续工作了两年的项目。这是乐高的物体检测数据集。   由   提交 /u/Rojepi   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfun7z/lego_object_detection_data_set/</guid>
      <pubDate>Sat, 16 Mar 2024 01:42:05 GMT</pubDate>
    </item>
    <item>
      <title>学习 ML 需要多少数学知识？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bftden/how_much_math_need_to_learn_ml/</link>
      <description><![CDATA[我想了解 Python 库的数学含义，而不仅仅是使用它们。我想知道数学建模的意义和深度。目前正在学习微积分。我应该使用哪本书来学习什么类型的数学？ kaggle 有帮助吗？   由   提交/u/Informal_Software477   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bftden/how_much_math_need_to_learn_ml/</guid>
      <pubDate>Sat, 16 Mar 2024 00:40:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么“from tensorflow.keras.models import load_model”需要这么长时间？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfszfj/why_does_from_tensorflowkerasmodels_import_load/</link>
      <description><![CDATA[您好， 我有一个问题：我有一个脚本来测试我通过 model.save(&#39;some_model.keras&#39;)。当我现在通过 load_model(&#39;some_model.keras&#39;) 加载该模型时，需要 5 分钟。它只是一个197M的文件。我有一个应用程序，我们加载了使用 pytorch 创建的模型（&#39;somemodel.pt&#39;），大小约为 570 MB，并且立即加载。  有人可以帮我解决这个问题吗？  干杯 Mr_LA   由   提交 /u/Mr_LA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfszfj/why_does_from_tensorflowkerasmodels_import_load/</guid>
      <pubDate>Sat, 16 Mar 2024 00:22:33 GMT</pubDate>
    </item>
    <item>
      <title>nn.ReLU() 是一个层吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfsd3q/is_nnrelu_a_layer/</link>
      <description><![CDATA[考虑以下模型： import torch import torch.nn as nn class MyModel(nn.Module)： def __init__(self，input_size，hidden_​​size，output_size)： super(MyModel，self).__init__() self.model = nn.Sequential( nn.Linear(input_size，hidden_​​size)，nn.BatchNorm1d(hidden_​​size)，nn.ReLU( ) ) self.output_layer = nn.Linear(hidden_​​size, output_size) defforward(self, x): x = self.model(x) x = self.output_layer(x) return x  线性层，然后是批量归一化，然后是 relu 激活。我不明白的是：我认为激活函数本质上是每一层每个神经元的一部分。如果没有激活函数，如何在第一层计算激活？此外，如果 relu 是一个层，那么它会有自己的神经元，有自己的权重、偏差和激活，但这与密集层有什么不同？ 批量规范也是一个层吗？    由   提交 /u/Exciting-Ordinary133    reddit.com/r/learnmachinelearning/comments/1bfsd3q/is_nnrelu_a_layer/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfsd3q/is_nnrelu_a_layer/</guid>
      <pubDate>Fri, 15 Mar 2024 23:54:44 GMT</pubDate>
    </item>
    <item>
      <title>了解曲线拟合中“lm”、“trf”和“dogbox”之间的差异</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfpczt/understanding_the_differences_between_lm_trf_and/</link>
      <description><![CDATA[我目前正在研究 Python 中的曲线拟合，并遇到了三种不同的方法：“lm”、“trf”和“dogbox”。这些方法似乎都有其独特的特征和应用，但我发现掌握它们之间的实际差异具有挑战性。 有人可以清楚地解释“lm”（Levenberg-Marquardt）的原理吗？ ）、“trf”（信任区域反射）和“dogbox”彼此不同吗？具体来说，我有兴趣了解其中一个问题可能比其他问题更受青睐的场景或类型。举例说明每种方法的主要区别和实际应用将非常有帮助。 我正在寻找可以帮助我决定在不同曲线拟合场景中使用哪种方法的见解。我的目标是通过了解每种方法的优点和局限性，实现最适合我的数据。 提前感谢您的时间和帮助！   由   提交 /u/Cyber​​Glint   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfpczt/understanding_the_differences_between_lm_trf_and/</guid>
      <pubDate>Fri, 15 Mar 2024 21:42:50 GMT</pubDate>
    </item>
    <item>
      <title>人工智能和质量保证的免费研讨会</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfp6p7/a_free_workshop_for_ai_and_qas/</link>
      <description><![CDATA[ 由   提交 /u/sometimesispeak1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfp6p7/a_free_workshop_for_ai_and_qas/</guid>
      <pubDate>Fri, 15 Mar 2024 21:35:03 GMT</pubDate>
    </item>
    <item>
      <title>训练模型时如何计算 ram/vram 使用情况？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfoal1/how_do_i_calculate_ramvram_usage_when_training_a/</link>
      <description><![CDATA[      我已经尝试训练扩散模型有一段时间了。我的模型有大约 800 万个参数。当我开始训练循环时，它需要 22-24GB 的 RAM。即使我只训练 1-10 张图像，也需要 13GB RAM。这太多了！根据我在网上找到的有关如何计算训练 llm Loras 的 vram 使用量的粗略计算，我预计 RAM 约为 3-4GB。我尝试分析我的数据加载函数，这似乎不是问题。为什么需要这么多 vram ？我不明白训练循环的哪一部分会占用如此多的 RAM。我附上了我的训练循环的图片以供参考。谢谢。   由   提交 /u/Full-Bell-4323   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfoal1/how_do_i_calculate_ramvram_usage_when_training_a/</guid>
      <pubDate>Fri, 15 Mar 2024 20:56:42 GMT</pubDate>
    </item>
    <item>
      <title>我无法理解这个形状错误</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfn6so/i_cant_wrap_my_head_around_this_shape_error/</link>
      <description><![CDATA[鉴别器中的前向传递函数不断出现输入张量不匹配问题，我不明白为什么。我尝试过插值、删除和保留批次尺寸等 导入火炬 导入torch.nn as nn 导入 torchvision.transforms 作为变换 导入 matplotlib.pyplot 作为 plt 从 PIL 导入图像 &lt; code&gt;# 检查支持 CUDA 的 GPU 是否可用 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) # 定义转换为灰度的变换 transform = Transforms.Compose([transforms.ToTensor()]) &lt; code&gt;# 加载图像 image_path = &#39;grayscale_image.png&#39; # 替换为图像的路径 image = plt.imread(image_path ) # 将图像调整为所需大小（例如 400x400） resized_image = Transforms.Resize((400, 400))(transforms .ToPILImage()(image)) # 将调整大小后的图像转换为张量并添加批量维度 image_tensor = transform(resized_image).unsqueeze (0).to(设备) num_channels = image_tensor.size(1) 类生成器(nn.Module):  def __init__(self, num_shares=2): super(Generator, self).__init__() self .nu​​m_shares = num_shares self.main = nn.Sequential( nn.ConvTranspose2d(100, 512, kernel_size=4, stride=1, padding=0), nn.BatchNorm2d(512), nn.ReLU(True), &lt; code&gt;nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(256), &lt; code&gt;nn.ReLU(True), nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), &lt; code&gt;nn.BatchNorm2d(128), nn.ReLU(True), nn.ConvTranspose2d(128, num_shares, kernel_size=4, stride=2, padding=1), nn.Tanh() ).to(device) defforward(self,noise,real_image):shares = self.main(noise)返回shares&lt; /p&gt; class Discriminator(nn.Module): def __init__(self): super(Discriminator, self ).__init__() self.main = nn.Sequential( nn.Conv2d(2, 64, kernel_size=4, stride=2 , padding=1), # 将输入通道改为2 nn.LeakyReLU(0.2, inplace=True), nn.Conv2d( 64, 128, kernel_size=4, stride=2, padding=1), nn.BatchNorm2d(128), nn.LeakyReLU( 0.2, inplace=True), nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True), nn.Conv2d(256, 1, kernel_size= 4、stride=1，padding=0), nn.Sigmoid() ).to(device) defforward(self, real_image,shares):combined=combine_shares(shares)combined=nn.function.interpolate （组合，size=real_image.shape[2:]，mode=&#39;双线性&#39;，align_corners=True） input_tensor = torch.cat((real_image,组合),dim=1) # 沿通道维度串联 probability = self.main(input_tensor) 返回概率  &lt; code&gt;# 合并共享的函数 def merge_shares(shares): combined = torch.sum(shares, dim=1, keepdim=False ) # 沿着共享维度求和，去掉多余的维度 print(f&quot;组合形状：{combined.shape}&quot;) return组合    由   提交/u/Fast-Life9648  /u/Fast-Life9648 reddit.com/r/learnmachinelearning/comments/1bfn6so/i_cant_wrap_my_head_around_this_shape_error/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfn6so/i_cant_wrap_my_head_around_this_shape_error/</guid>
      <pubDate>Fri, 15 Mar 2024 20:08:44 GMT</pubDate>
    </item>
    <item>
      <title>模型搭建比赛，2k奖励</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfmko1/model_building_competition_2k_reward/</link>
      <description><![CDATA[在 Twitter 上看到这个，认为社区可能会感兴趣。类似 Kaggle 的学术竞赛，构建偏见模型。最佳模型赢得 2k。 更多信息位于此处：[https://hehmanlab.org/competition]   由   提交 /u/Brighteye   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfmko1/model_building_competition_2k_reward/</guid>
      <pubDate>Fri, 15 Mar 2024 19:42:52 GMT</pubDate>
    </item>
    <item>
      <title>有什么比 ISLR（统计学习导论）更简单的吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfk0n6/anything_that_is_even_simpler_than_islr/</link>
      <description><![CDATA[寻找一些比 R 统计学习简介更容易理解的建议（视频或文本）。这是我的讲师建议的：一本更容易阅读的书。    由   提交 /u/StereotypedComrade   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfk0n6/anything_that_is_even_simpler_than_islr/</guid>
      <pubDate>Fri, 15 Mar 2024 17:53:34 GMT</pubDate>
    </item>
    <item>
      <title>描述 Beta：最具描述性的视频视听摘要</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfics0/describe_beta_the_most_descriptive_audiovisual/</link>
      <description><![CDATA[   /u/happybirthday290   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfics0/describe_beta_the_most_descriptive_audiovisual/</guid>
      <pubDate>Fri, 15 Mar 2024 16:43:09 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的验证 acc 和 loss 如此随机？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bfgh46/why_would_my_validation_acc_and_loss_be_so_random/</link>
      <description><![CDATA[       由   提交/u/CuteKittenMittens   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bfgh46/why_would_my_validation_acc_and_loss_be_so_random/</guid>
      <pubDate>Fri, 15 Mar 2024 15:21:36 GMT</pubDate>
    </item>
    <item>
      <title>这些来自大公司的AI/ML工程师如何成长？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1bf2rwm/how_these_aiml_engineers_from_big_companies_grow/</link>
      <description><![CDATA[你好，每一个伟大的人工智能公司，比如 OpenAI、Nvidia、FigureAI、ElevenLabs 等，都在由那些 ML 工程师实现的人工智能方面做出了令人惊叹的事情，我想有一天我会像他们一样，但我不知道他们做了什么或如何成长，我是一名计算机科学学生，我不会等到毕业才能成长，我想从今天开始成长，我已经开始在我的机器学习中学习大一的时候，我从零到精通的 Pytorch 和 ML 课程开始，然后我从 DeepLearning 开始了 MLOps 专业化，我做了我所学课程中教授的项目，我真的很想学习那些大的或资深的 ML/AI 工程师，他们如何思考、工作和成长。我目前确实什么都不做，因为我看到我要创建的 99% 的模型都已完成并可用于拥抱脸部。我不觉得自己在成长，但我想成长，所以当我有了想法时，我立即使用模型，添加一些数据，并在一小时内完成我的项目。任何有关如何成长的建议都将受到高度赞赏。  我真的希望成为某个人或建立自己的人工智能公司，希望每个人都一样。 感谢您阅读本文    由   提交 /u/Genuine_Giraffe   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1bf2rwm/how_these_aiml_engineers_from_big_companies_grow/</guid>
      <pubDate>Fri, 15 Mar 2024 02:02:18 GMT</pubDate>
    </item>
    <item>
      <title>一位软件工程师正在学习 ML，并有一些问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1beui5z/a_software_engineer_is_learning_ml_and_has_some/</link>
      <description><![CDATA[我是一名 10 岁的软件工程师（主要是后端经验，学习计算机科学）。我在数学和统计学方面有很强的背景，并且对传统的机器学习技术（多年前的大学讲座）有足够的了解，但在深度学习、法学硕士或过去 7-8 年发生的任何事情上还是新手。 现在，我正在尝试越来越多地进入这个领域。我已经完成了研究并提出了一些问题。如果您能给我一些有关它们的指导，我将不胜感激。 - 几乎所有事情的核心都是随机开始+计算损失+对梯度下降求导数，重复直到局部最小值。几乎所有的想法都可以简化为这个模板吗？还是我太天真了？即使大型语言模型也会计算 n-gram 的概率密度并做同样的事情，对吗？当然，我知道每个步骤中令人难以置信的细节，但我现在只是想更好地了解事情。 - 关于 Kaggle，看起来大多数解决方案都有一个模式。预处理+xgboost/lightgbm+调整。我认为竞赛青睐具有最佳数据工程技术（EDA、特征工程、清理等）的解决方案。我认为 Kaggle 是一个向其他笔记本学习的好地方，但大多数都与预处理部分相关。我这个想法是愚蠢的，还是我错过了一些东西？ - 怎么突然有这么多人工智能初创公司？每个公司都在开发自己的模式吗？这是一项艰巨的任务，计算成本很高，而且需要数据。初创公司如何找到所有这些？他们是否只使用开源模型（如 LLAMA）、公共数据集并尽可能地对其进行调整？或者在幕后，它们都是 GPT 或者什么？ - 作为一名 10 岁的软件工程师，要进入这个领域，我想我必须接受机器学习工程的大幅减薪，否则我需要切换到数据工程/MLOps，我并不是很喜欢。像我这样的人是否有机会，或者我是否需要在 Kaggle 等上建立一个投资组合？对我来说最好的前进方式是什么？我主要喜欢其中的数学和统计学，但我想对于研究来说有点太晚了。 ​ 谢谢！    由   提交 /u/m1thriller   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1beui5z/a_software_engineer_is_learning_ml_and_has_some/</guid>
      <pubDate>Thu, 14 Mar 2024 20:02:01 GMT</pubDate>
    </item>
    </channel>
</rss>