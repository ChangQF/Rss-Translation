<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Sat, 16 Nov 2024 18:20:46 GMT</lastBuildDate>
    <item>
      <title>网络上的线性回归图表</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gst6ba/linear_regression_charts_on_the_web/</link>
      <description><![CDATA[我一直在使用 statsmodel 和 seaborn 来了解我的数据点之间的线性关系。我想构建一个 Web 应用程序，将这种建模展示给我公司的其他利益相关者。 我做了一些研究，现有的库（如 d3 和 chartjs）不支持绘制像 seaborn 这样的图形。有什么建议吗？    提交人    /u/njinja10   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gst6ba/linear_regression_charts_on_the_web/</guid>
      <pubDate>Sat, 16 Nov 2024 18:10:31 GMT</pubDate>
    </item>
    <item>
      <title>了解 PatchTST 时间序列转换器官方存储库所做的缩放</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gst0nz/understanding_scaling_done_by_official_repository/</link>
      <description><![CDATA[我正在尝试从其官方 github 存储库了解PatchTST 论文实现。它似乎是当前最先进的时间序列转换器。 其 repo 中定义的数据集类有以下几行（第 1-3 行永久链接、第 4-5 行永久链接）： train_data = df_data[border1s[0]:border2s[0]] # 第 1 行 self.scaler.fit(train_data.values) # 第 2 行 data = self.scaler.transform(df_data.values) # 第 3 行 self.data_x = data[border1:border2] # 第 4 行 self.data_y = data[border1:border2] # 第 5 行  让我解释一下：  border1s 数组包含训练、测试和 val 数据分割的起始索引，border12s 数组包含训练、测试和 val 分割的结束索引。因此，border1s[0] 是训练分割的起始索引，border1s[1] 是测试分割的起始索引，border1s[2] 是 val 分割的起始索引。类似地，因此，border2s[0] 是训练分割的结束索引，border2s[1] 是测试分割的结束索引，border2s[2] 是 val 分割的结束索引。 border1 和 border2 是基于上下文的某些特定分割的开始和结束索引。（假设训练分割）  请注意，第 2 行将缩放器拟合到训练数据集分割，第 3 行使用相同的缩放器转换整个数据集。  Q1.为什么不适合整个数据集而只适合训练数据集分割？ 请注意第 4 行和第 5 行，输入特征 data_x 和目标 data_y 是完全相同的值。 Q2.如何使目标均匀缩放才有意义？ （我觉得只有输入特征是标准化的。）这不会迫使模型学习预测缩放目标而不是实际/地面实况目标吗？ 在所有数据集类别中，本文似乎总是设置 data_x与data_y相同。 Q3.（与缩放无关）如果我想要输入特征时间序列与目标时间序列不同怎么办？也就是说我想要预测的值与我想要作为输入特征的值不同？我是否仍应设置 data_x = data_y = 所有列，或者我应该将 data_x 仅设置为输入列，将 data_y 仅设置为目标列？（但请注意，在训练期间，它似乎将目标列从预测值中分离出来以计算损失第 172 行。）    提交人    /u/RajSingh9999   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gst0nz/understanding_scaling_done_by_official_repository/</guid>
      <pubDate>Sat, 16 Nov 2024 18:02:56 GMT</pubDate>
    </item>
    <item>
      <title>LGBM 指标为 0.0</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gssigi/lgbm_metrics_are_00/</link>
      <description><![CDATA[嗨， 我是这个领域的新手，我正在研究一个简单的欺诈检测问题，其类别分布如下：  标签 0：142,900 个样本 标签 1：16,530 个样本  我正在使用 Optuna 训练 LightGBM 模型以进行超参数调整。我进行了第一次试验，但分数（可能是准确度或类似指标）为 0.0。预处理步骤已正确完成，数据似乎很好。 我不确定为什么会发生这种情况。有人遇到过类似的问题吗？关于可能导致此问题的原因或如何进行故障排除，您有什么建议吗？ 提前感谢您的帮助！ def objective(trial, X, y): params = { &#39;objective&#39;: &#39;binary&#39;, &#39;metric&#39;: &#39;binary_logloss&#39;, &#39;boosting_type&#39;: &#39;gbdt&#39;, &#39;verbosity&#39;: -1, &#39;max_depth&#39;: -1, &#39;learning_rate&#39;: trial.suggest_float(&#39;learning_rate&#39;, 0.005, 0.01, log=True), &#39;num_leaves&#39;: trial.suggest_int(&#39;num_leaves&#39;, 400, 500), &#39;feature_fraction&#39;: trial.suggest_float(&#39;feature_fraction&#39;, 0.3, 0.6), &#39;bagging_fraction&#39;: trial.suggest_float(&#39;bagging_fraction&#39;, 0.4, 0.7), &#39;min_child_weight&#39;: trial.suggest_float(&#39;min_child_weight&#39;, 0.01, 0.1, log=True), &#39;min_data_in_leaf&#39;: trial.suggest_int(&#39;min_data_in_leaf&#39;, 50, 150), &#39;reg_alpha&#39;: trial.suggest_float(&#39;reg_alpha&#39;, 0.1, 1.0, log=True), &#39;reg_lambda&#39;: trial.suggest_float(&#39;reg_lambda&#39;, 0.1, 1.0, log=True), &#39;random_state&#39;: RANDOM_STATE } NFOLDS = 5 折叠 = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=RANDOM_STATE) 列 = X.columns splits = folds.split(X, y) y_oof = np.zeros(X.shape[0]) score = 0 for fold_n, (train_index, valid_index) in enumerate(splits): logs.info(f&quot;正在处理折叠 {fold_n + 1} 的 {NFOLDS}。&quot;) X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index] y_train, y_valid = y.iloc[train_index], y.iloc[valid_index] train_data = lgb.Dataset(X_train, label=y_train) valid_data = lgb.Dataset(X_valid, label=y_valid) model = lgb.train(params, train_data, num_boost_round=1000, valid_sets=[valid_data],回调=[early_stopping(stopping_rounds=50)]) y_pred_valid = model.predict(X_valid) y_oof[valid_index] = y_pred_valid fold_f1 = f1_score(y_valid, [1 if pred &gt; 0.2 else 0 for pred in y_pred_valid]) login.info(f“折叠 {fold_n + 1} | F1 分数: {fold_f1}” 得分 += fold_f1 / NFOLDS login.info(f“平均 F1 分数 = {score}”) login.info(f“折叠外 F1 分数 = {f1_score(y, [1 if pred &gt; 0.2 else 0 for pred in y_oof])}”) 如果 __name__ ==“__main__”则返回分数： train_df = pd.read_csv(DATA_PATH, encoding=&#39;utf-8&#39;) X = train_df.drop(columns=[&#39;isFraud&#39;]) y = train_df[&#39;isFraud&#39;] X = preprocess_data(X, MODE, DIR) X_clnd, drop_features = drop_corr_features（X，阈值=0.95） X_scaled = scale_features（X_clnd） X_scaled_df = pd.DataFrame（X_scaled，columns=X_clnd.columns） X_train，X_test，y_train，y_test = train_test_split（X_scaled_df，y，test_size=TEST_SIZE，random_state=RANDOM_STATE，stratify=y） study = optuna.create_study（direction=&#39;maximize&#39;，study_name=&#39;maximize_auc&#39;） study.optimize（lambda trial：objective（trial，X_train，y_train），n_trials=N_TRIALS） best_params = study.best_params logsing.info（f“最佳超参数：{best_params}”） final_model = lgb.LGBMClassifier(**best_params) final_model.fit(X_train，y_train) y_test_pred = final_model.predict(X_test) f1 = f1_score(y_test，y_test_pred) precision = precision_score(y_test，y_test_pred) recall = recall_score(y_test，y_test_pred) roc_auc = roc_auc_score(y_test，final_model.predict_proba(X_test)[:, 1]) cm = confused_matrix(y_test，y_test_pred) logs.info(f“混淆矩阵：\n{cm}”）logging.info(f“F1 分数：{f1}”）logging.info(f“精度：{precision}”）logging.info(f“召回率： {recall}&quot;)logging.info(f&quot;ROC AUC 得分：{roc_auc}&quot;) 2024-11-16 19:27:35,892 - INFO - 混淆矩阵： [[28580 0] [ 3306 0]] 2024-11-16 19:27:35,892 - INFO - F1 得分：0.0 2024-11-16 19:27:35,907 - INFO - 精确度：0.0 2024-11-16 19:27:35,907 - INFO - 召回率：0.0 2024-11-16 19:27:35,907 - INFO - ROC AUC 得分：0.49814946698688517    提交人    /u/DrGenius22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gssigi/lgbm_metrics_are_00/</guid>
      <pubDate>Sat, 16 Nov 2024 17:40:07 GMT</pubDate>
    </item>
    <item>
      <title>为了训练 VAE，KL 散度应该增加还是减少？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsqw63/should_kl_divergence_increase_or_decrease_for/</link>
      <description><![CDATA[我制作了一个 VAE，虽然它的结构看起来很防御（比普通 AE 差，但我会尝试使用鉴别器来修复它），但问题是 KL 损失在训练过程中不断增加，从而产生了不错的图像，但潜在空间仍然很糟糕，例如，改变一个变量会（稍微）改变图像的一些随机部分，与普通 AE 相同，KL 在 10k 个批次（批次大小为 16）中从 1.1 上升到 400。 这是我的 KL 散度公式： kl = mean((zLog + 1 - zMean**2 - exp(zLog) - 1) * -0.5) 或以代码形式： const kl = zLogVar.add(1).sub(zMean.square()).sub(zLogVar.exp()).sum(-1).mul(-0.5).mean().mul(this.beta) （Beta 目前始终为 1） 并使训练损失为 rebuilding.add(kl)    提交人    /u/ZazaGaza213   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsqw63/should_kl_divergence_increase_or_decrease_for/</guid>
      <pubDate>Sat, 16 Nov 2024 16:26:33 GMT</pubDate>
    </item>
    <item>
      <title>知道验证损失下降的原因是什么吗？我没有改变学习率，也没有使用调度程序</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsqt3k/any_idea_what_is_this_drop_in_validation_loss_im/</link>
      <description><![CDATA[       由    /u/Even_Information4853  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsqt3k/any_idea_what_is_this_drop_in_validation_loss_im/</guid>
      <pubDate>Sat, 16 Nov 2024 16:22:33 GMT</pubDate>
    </item>
    <item>
      <title>Udemy 上最好的机器学习课程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gspxnf/best_machine_learning_courses_on_udemy/</link>
      <description><![CDATA[  由    /u/Sreeravan  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gspxnf/best_machine_learning_courses_on_udemy/</guid>
      <pubDate>Sat, 16 Nov 2024 15:42:37 GMT</pubDate>
    </item>
    <item>
      <title>购买用于 AI/ML 和应用程序开发的笔记本电脑。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsp6a8/buying_a_laptop_for_aiml_and_app_dev/</link>
      <description><![CDATA[P：你好，ML 爱好者。我是 ML 新手，我打算买一台新笔记本电脑，最好是惠普的。我想买一台 1000 美元（82000 印度卢比）以下的，但我不知道该选什么处理器。https://www.hp.com/in-en/shop/victus-gaming-laptop-15-fa1415tx-ar2b5pa.html 这是我的选择之一，但我不想以后后悔，所以请帮助我。我懂中级 Python（pandas、numpy），接下来会转到 scikit 学习。我的主要兴趣是 ML、2d 游戏/应用程序开发。抱歉，如果这不是正确的 subreddit，我不知道在哪里问。    提交人    /u/Royal-Client3118   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsp6a8/buying_a_laptop_for_aiml_and_app_dev/</guid>
      <pubDate>Sat, 16 Nov 2024 15:06:14 GMT</pubDate>
    </item>
    <item>
      <title>推荐系统</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsoi0o/recommendation_system/</link>
      <description><![CDATA[我正在开发一个房地产推荐系统，试图模仿抖音的强大算法。一个核心特性是客户端必须获得近乎精确的位置，在这种情况下，我使用 H3 进行空间索引。我意识到 Rtree 也做得很好。该系统由出租的房屋和出售的土地组成。我正在考虑如何优化系统以在几毫秒内返回准确的结果。房屋数据库有位置、卧室、代理、价格等参数，而土地有位置、大小、成本和描述参数。优化这样一个系统的适当方法是什么    提交人    /u/Cute-Opening-2454   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsoi0o/recommendation_system/</guid>
      <pubDate>Sat, 16 Nov 2024 14:33:14 GMT</pubDate>
    </item>
    <item>
      <title>最适合我这种背景的人的机器学习入门网站/资料</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsnjc0/best_beginner_ml_websitematerials_for_someone_of/</link>
      <description><![CDATA[嗨， 所以我一直在寻找应用机器学习，特别是金融市场的时间序列预测。 我有金融/经济背景（本科），对时间序列（ARMA、多元 GARCH 等）有些熟悉。我有兴趣了解更多有关实施机器学习的一些实用方法，例如实时预测宏观变量/金融数据。我最近才听说 LSTM 和 RNN，经过一段时间的自学后，我很想能够通过 Python 实现它们。就时间安排而言，我能够花几个月的时间系统地学习所有 ML 概念，但我不知道从哪里开始。 我看到 machinelearningmastery.com 提供了很多相关内容。坦白说，材料的数量看起来有点让人不知所措。如果有人可以分享他们使用该网站的经验（与我的学习目标相关），那将很有帮助，只要适合我这样的人，我不介意他们的书的价格。 我还看到了一些关于 d2l.ai（免费）的推荐。对我来说，这些内容似乎不符合我的目的，与我相关的材料（例如 chapter_recurrent-modern/lstm ）看起来有点陌生，因为它们似乎更侧重于计算机科学而不是应用金融，如果这说得通的话。 TLDR:  本科金融背景 自学 寻找有效的方法快速掌握 ML 时间序列预测（~3 个月） 理想情况下，材料需要包含以金融/经济为重点而不是以计算机科学为重点的动手编码示例（Python 或 R） 学习成果：能够理解时间序列 ML的高级概念（即，而不仅仅是盲目地应用软件包），并且能够在不同的财务情景下操纵和调整相关模型。 目前更倾向于machinelearningmastery.com  任何意见都值得赞赏！    提交人    /u/Awkward-Action322   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsnjc0/best_beginner_ml_websitematerials_for_someone_of/</guid>
      <pubDate>Sat, 16 Nov 2024 13:43:13 GMT</pubDate>
    </item>
    <item>
      <title>如何系统地学习机器学习领域</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsn9c9/how_to_systematically_learn_in_the_field_of/</link>
      <description><![CDATA[我是机器学习领域的一名大三学习者，目前是计算机科学与技术专业本科二年级的学生。我读过一些深度学习方面的资料，比如李牧的《深入学习》，也读过《Attention Is All You Need》、《Very Deep Convolutional Networks for Large Scale Image Recognition》等经典论文。但是通过这些学习，我发现自己只是了解了一些backbone，知识比较零散，对机器学习领域缺乏系统的了解。我该如何学习才能对机器学习领域有比较全面的了解？有哪些教材或者学习资料推荐呢？    submitted by    /u/JobFew6218   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsn9c9/how_to_systematically_learn_in_the_field_of/</guid>
      <pubDate>Sat, 16 Nov 2024 13:28:09 GMT</pubDate>
    </item>
    <item>
      <title>从 Scratch 到 GPU 的深度学习 - 第 0 至 17 部分</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsm1cv/deep_learning_from_scratch_to_gpu_part_0_to_17/</link>
      <description><![CDATA[  由    /u/dragandj  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsm1cv/deep_learning_from_scratch_to_gpu_part_0_to_17/</guid>
      <pubDate>Sat, 16 Nov 2024 12:14:42 GMT</pubDate>
    </item>
    <item>
      <title>过去 4-5 个月，我一直在德国申请第一份机器学习全职工作，但现在我刚刚毕业，仍然没有收到下一轮的邮件。我非常希望得到对我简历的反馈。我主要申请 CV 或 MLOps 职位，但也申请 ML/AI Eng/Dev 职位</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsk5ld/i_have_been_applying_for_my_first_machine/</link>
      <description><![CDATA[        提交人    /u/M4AZ   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsk5ld/i_have_been_applying_for_my_first_machine/</guid>
      <pubDate>Sat, 16 Nov 2024 09:57:54 GMT</pubDate>
    </item>
    <item>
      <title>创建了一个神经网络库并托管了 bug smash！</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gsaya4/created_a_neural_network_library_and_hosting_bug/</link>
      <description><![CDATA[大家好！我和我的朋友一直在从头开始开发一个神经网络库，只使用 NumPy 进行矩阵运算/矢量化。我们正在举办一场 bug 清理活动，希望社区能够测试我们的库并为我们找出尽可能多的 bug。该库可在 Pypi 上获得：https://pypi.org/project/ncxlib/ 该库支持：  输入/隐藏/输出层 激活函数：Sigmoid、ReLU、Leaky ReLU、Softmax 和 TanH 优化器：Adam、RMS Prop、SGD、带动量的 SGD 损失函数：二元和分类交叉熵、MSE 大量用于图像和原始表格数据的预处理器  有关 bug smash 和我们的库文档的所有信息可在以下位置找到： https://www.ncxlib.com 谢谢！我们希望收到大量改进反馈。    提交人    /u/cwcoogan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gsaya4/created_a_neural_network_library_and_hosting_bug/</guid>
      <pubDate>Sat, 16 Nov 2024 00:21:11 GMT</pubDate>
    </item>
    <item>
      <title>高斯过程很难理解</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gs0ckc/gaussian_processes_are_so_difficult_to_understand/</link>
      <description><![CDATA[大家好。我花了无数个小时阅读和观看有关高斯过程 (GP) 的视频，但一直无法正确理解它们。有人有好的资料可以引导您完成并指导 GP 的每一个元素吗？    提交人    /u/amirdol7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gs0ckc/gaussian_processes_are_so_difficult_to_understand/</guid>
      <pubDate>Fri, 15 Nov 2024 16:32:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>