<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Fri, 26 Apr 2024 15:14:52 GMT</lastBuildDate>
    <item>
      <title>如何用较小的开源模型击败专有法学硕士</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdn9pa/how_to_beat_proprietary_llms_with_smaller_open/</link>
      <description><![CDATA[       由   提交/u/aidantcooper  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdn9pa/how_to_beat_proprietary_llms_with_smaller_open/</guid>
      <pubDate>Fri, 26 Apr 2024 14:46:10 GMT</pubDate>
    </item>
    <item>
      <title>如何减少过拟合？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdmox9/how_to_reduce_overfitting/</link>
      <description><![CDATA[我有这些选项可供选择以减少过度拟合，并想看看社区是否同意。  提高学习率   我说这不会减少过度拟合。 bc 这只会改变在无限长的时间内开始过度拟合的速度  对训练图像应用随机变换（例如翻转、旋转） &lt; /p&gt;  我说这确实有助于减少过度拟合，因为您正在增加训练测试集。   使用较小的网络   我说这确实减少了过度拟合，因为它不会“记住”网络。复杂的模式或任何东西  使用更大的网络  我说这不会减少过度拟合，因为使用更大的网络你开始记住不同的模式，对我来说，这就是过度拟合的定义。  当验证损失趋于平稳时停止训练   对此不太确定。  ​ 有人可以帮助我了解如何减少过度拟合的逻辑吗？  ​ ​   由   提交 /u/Famous-Help-3572   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdmox9/how_to_reduce_overfitting/</guid>
      <pubDate>Fri, 26 Apr 2024 14:22:04 GMT</pubDate>
    </item>
    <item>
      <title>带编码器+GRU的变压器用于时间序列输入</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdm9hh/transformer_with_encoder_gru_for_time_series_input/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdm9hh/transformer_with_encoder_gru_for_time_series_input/</guid>
      <pubDate>Fri, 26 Apr 2024 14:04:30 GMT</pubDate>
    </item>
    <item>
      <title>Ian Goodfellow 的深度学习书籍和 ML 数学</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdlujl/ian_goodfellows_deep_learning_book_and_ml/</link>
      <description><![CDATA[我开始阅读 Ian Goodfellow 的 Deep Laerning 书。但是，我意识到我在大学数学课程中几乎已经学完了书中的所有内容。当然，我并不了解机器学习数学的所有知识，而且我对大学数学也有点生疏了。但是，把所有的事情都记下来还是感觉很累很无聊。而且我似乎并没有纯粹从书本上学到很多东西。 而且我对 ML 的实际方面知之甚少，所以我浏览了 Andrej Karpathy、bycloud、Gary Explains 视频，他们帮助我很多。所以，现在，我正在考虑做更多的实践课程，学习如何实际使用 ML irl。我现在应该浏览一下伊恩的书吗？我觉得无论如何我都会忘记大部分这些东西，当我需要它时，我最好稍后再学习数学。    由   提交/u/open_23  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdlujl/ian_goodfellows_deep_learning_book_and_ml/</guid>
      <pubDate>Fri, 26 Apr 2024 13:46:48 GMT</pubDate>
    </item>
    <item>
      <title>如何处理多模态特征？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdlsar/how_to_handle_multi_modal_feature/</link>
      <description><![CDATA[      嗨！我有一个名为“财务损失”的功能。基本上描述了一个人在诈骗期间损失了多少钱。您如何预处理或处理这种功能？对数或平方根变换有帮助吗？     提交人    /u/Vitoahshik   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdlsar/how_to_handle_multi_modal_feature/</guid>
      <pubDate>Fri, 26 Apr 2024 13:44:07 GMT</pubDate>
    </item>
    <item>
      <title>Matryoshka 嵌入：它好还是 PCA 足够？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdls0y/matryoshka_embeddings_is_it_good_or_is_pca_enough/</link>
      <description><![CDATA[       由   提交/u/csyrup  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdls0y/matryoshka_embeddings_is_it_good_or_is_pca_enough/</guid>
      <pubDate>Fri, 26 Apr 2024 13:43:45 GMT</pubDate>
    </item>
    <item>
      <title>[R] 多头混合专家 (MH-MoE) + 非官方实施</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdkep9/r_multihead_mixture_of_experts_mhmoe_unofficial/</link>
      <description><![CDATA[论文：https://arxiv.org/abs/2404.15045  HuggingFace：https://huggingface.co/papers/2404.15045 非官方实施：https://github.com/lhallee/Multi_Head_Mixture_of_Experts__MH-MOE 摘要：  专家稀疏混合（SMoE）在不显着增加训练和推理成本的情况下扩展了模型容量，但存在以下两个问题：（1）专家激活率低，其中只有一小部分激活专家进行优化。 (2)缺乏对单个token内多个语义概念的细粒度分析能力。我们提出多头专家混合（MH-MoE），它采用多头机制将每个令牌拆分为多个子令牌。然后，这些子代币被分配给一组不同的专家并行处理，并无缝地重新集成到原始代币形式中。多头机制使模型能够共同关注来自不同专家内的各种表示空间的信息，同时显着增强专家激活，从而加深上下文理解并缓解过度拟合。此外，我们的 MH-MoE 易于实现，并且与其他 SMoE 优化方法解耦，从而可以轻松与其他 SMoE 模型集成以增强性能。跨三个任务的广泛实验结果：以英语为中心的语言建模、多语言语言建模和 Masked 多模态建模任务，证明了 MH-MoE 的有效性。  PyTorch 中的 Subtoken 路由实现！请分享并创建问题或提出任何建议！   由   提交 /u/ThrowawayTheReal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdkep9/r_multihead_mixture_of_experts_mhmoe_unofficial/</guid>
      <pubDate>Fri, 26 Apr 2024 12:41:57 GMT</pubDate>
    </item>
    <item>
      <title>2024 年 10 门数据科学数学免费课程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdjeke/10_mathematics_for_data_science_free_courses_in/</link>
      <description><![CDATA[       由   提交/u/Aqsa81  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdjeke/10_mathematics_for_data_science_free_courses_in/</guid>
      <pubDate>Fri, 26 Apr 2024 11:52:21 GMT</pubDate>
    </item>
    <item>
      <title>什么是自动化机器学习？它是如何工作的？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdj8co/what_is_automated_machine_learning_how_does_it/</link>
      <description><![CDATA[   /u/Emily-joe  /u/Emily-joe  artiba.org/blog/what-is-automated-machine-learning-how-does-it-work&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdj8co/what_is_automated_machine_learning_how_does_it/</guid>
      <pubDate>Fri, 26 Apr 2024 11:43:00 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助提出可比较表的技术特征</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdiuld/need_help_coming_up_with_technical/</link>
      <description><![CDATA[您好，我需要帮助为可在线访问的生成图像模型的可比较表提供技术特征，例如：Microsoft Designer、Adobe firefly、ImageFX 、DreamStudio、Craiyon。这是一个工程大学项目，因此特性应该是客观的，请帮忙   由   提交/u/Jealous_Big3405  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdiuld/need_help_coming_up_with_technical/</guid>
      <pubDate>Fri, 26 Apr 2024 11:22:20 GMT</pubDate>
    </item>
    <item>
      <title>LLM：情境学习为何有效？从技术角度来看，具体发生了什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdif8m/llms_why_does_incontext_learning_work_what/</link>
      <description><![CDATA[在我寻找这个问题的答案时，得到的答案只不过是将模型拟人化而已。他们总是提出这样的主张：  如果没有示例，模型必须推断上下文并依靠其知识来推断出预期的结果。这可能会导致误解。 一次性提示通过提供具体示例来减轻这种认知负担，有助于锚定模型的解释并专注于具有更清晰期望的更狭窄的任务。  该示例充当模型的参考或提示，帮助其理解您正在寻求的响应类型并在训练期间触发对类似实例的记忆。&lt; /p&gt; 提供示例允许模型识别要复制的模式或结构。它为模型建立了一个对齐线索，减少了零样本场景中固有的猜测。  顺便说一句，这些是真实的摘录。 但这些模型不“理解”任何东西。他们不“推断”，或“解释”，或“聚焦”，或“记住训练”，或“猜测”，或有字面上的“认知负荷”。它们只是统计令牌生成器。因此，当寻求对上下文学习提高准确性的确切机制的具体理解时，像这样的流行科学解释是毫无意义的。 有人可以提供一个根据实际模型来解释事物的解释吗？架构/机制以及提供额外上下文如何带来更好的输出？我可以“说说而已”，所以请不遗漏任何技术细节。 我可以做出有根据的猜测 - 在输入中包含示例，这些示例使用与您想要的输出类型近似的标记，从而引导注意力机制，并且最终的密集层对更高的令牌进行加权，这些令牌在某种程度上与这些示例相似，从而增加了在每个生成步骤中对这些所需令牌进行采样的几率；就像从根本上讲，我猜测相似性/距离的事情，其中​​明确举例说明我想要的输出会增加获得的输出与其相似的可能性 - 但我更愿意从对这些模型有深入了解的其他人那里听到它   由   提交 /u/synthphreak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdif8m/llms_why_does_incontext_learning_work_what/</guid>
      <pubDate>Fri, 26 Apr 2024 10:59:11 GMT</pubDate>
    </item>
    <item>
      <title>在服务器上部署预训练模型以进行实时图像处理 [D] [R] [P]</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdh85r/deploying_pretrained_model_on_a_server_for/</link>
      <description><![CDATA[我有一个flask应用程序，它使用预训练的ml模型，其主要任务是查找图像的嵌入，一次可能有100s要处理的图像数量，假设 100 个图像处理需要 80 秒才能完成，我应该如何在 AWS 或任何其他云服务上部署应用程序，以便处理 100 个图像只需要 4-5 秒。    由   提交/u/No-Ganache4424  /u/No-Ganache4424 reddit.com/r/learnmachinelearning/comments/1cdh85r/deploying_pretrained_model_on_a_server_for/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdh85r/deploying_pretrained_model_on_a_server_for/</guid>
      <pubDate>Fri, 26 Apr 2024 09:42:38 GMT</pubDate>
    </item>
    <item>
      <title>接下来我该怎么办？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdgwa9/what_do_i_do_next/</link>
      <description><![CDATA[我目前是一名 DS 学士学生，作为 MLE 实习生进行实习，在那里我设计了适合使用公司数据以及其他数据进行回归预测的 DL 模型实习生。经过 4 个月的努力，临近毕业，我今天接到公司顾问的电话，他说我根本不适合这个领域，因为它更注重研究，而我的态度、心态和我的能力与此相差甚远，建议我更多地利用我的技能担任技术经理。这让我思考我未来真正应该做什么，因为我也应用了人工智能嵌入式系统的MS，并真正考虑我是否应该继续在这个领域追求，因为我一直在挣扎并且不是顶尖的所以我真的需要一些关于它的建议   由   提交/u/khang2001  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdgwa9/what_do_i_do_next/</guid>
      <pubDate>Fri, 26 Apr 2024 09:19:23 GMT</pubDate>
    </item>
    <item>
      <title>硕士生，却是个骗子。想要把事情做对。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cdahde/masters_student_but_a_fraud_want_to_make_it_right/</link>
      <description><![CDATA[大家好，我想分享一些我非常没有安全感和羞耻的事情。但我觉得为了以后的改进，需要把它拿出来。我是美国一所非常普通的公立大学的计算机硕士学生，我也是从那里获得了学士学位。在我本科生期间，一开始我表现不错，但随着我进入第三年和第四年，课程变得越来越难，我在课堂上只做了最低限度的事情。这意味着没有副业，没有动力去做任何事情，没有实习，并且在我交作业或完成一个学期的那一刻就忘记了一切。我一直告诉自己，我会“稍后”阅读这个基本概念，但后来却从未实现，而且我对现在正在做的事情的基础非常薄弱。这意味着每当我遇到问题时，我都会严重依赖 ChatGPT，这让我感觉很糟糕和愚蠢，从而导致更多的不良行为。我从来没有完成过令我感到自豪的项目。在攻读硕士学位期间，我接触了 ML 并参加了 NLP 课程，我非常喜欢这门课程，主要是因为教授的帮助，我想在 2024 年秋季在这位教授的指导下进行研究，但我的编程技能，尤其是 Python 技能低于标准水平，而且我对 ML 的了解还不够。是不够的。我有 3.5 个月的时间来打下良好的基础，真正学习 ML 和 NLP，而不是在我不明白的时候就使用 chatGPT。我正在考虑首先学习 Andrew NG 的 ML 专业课程，并通过 Andrej Karpathy 在 YT 上的从零到英雄播放列表进行补充。有没有人有任何建议或建议，或者这是一个好的起点，以及完成这些课程后我应该做什么。我厌倦了无能，我想改变这一点。    由   提交/u/Funny_Professional85   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cdahde/masters_student_but_a_fraud_want_to_make_it_right/</guid>
      <pubDate>Fri, 26 Apr 2024 02:48:01 GMT</pubDate>
    </item>
    <item>
      <title>不受欢迎的观点：除了 ML/DL 专业之外，DeepLearning.AI 的其余课程都非常基础，没有多大帮助。非常抽象的教训。他们也很晚了。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ccrab4/unpopular_opinion_besides_mldl_specializations/</link>
      <description><![CDATA[ 由   提交/u/MBU_NXTDOOR_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ccrab4/unpopular_opinion_besides_mldl_specializations/</guid>
      <pubDate>Thu, 25 Apr 2024 12:58:31 GMT</pubDate>
    </item>
    </channel>
</rss>