<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Thu, 14 Nov 2024 03:22:04 GMT</lastBuildDate>
    <item>
      <title>如何回答这个关于 NLP 的面试问题？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gquxul/how_to_answer_this_interview_question_on_nlp/</link>
      <description><![CDATA[我在一次采访中被问到这个问题。&quot;对于分类任务，你会使用基于编码器还是基于解码器的模型，如果选择，背后的原因是什么？&quot;我只是告诉他们我会使用编码器模型，因为它的注意力机制是双向的，但这仍然不是一个明显的区别。    提交人    /u/madara_x13   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gquxul/how_to_answer_this_interview_question_on_nlp/</guid>
      <pubDate>Thu, 14 Nov 2024 02:48:08 GMT</pubDate>
    </item>
    <item>
      <title>转发：为什么我的随机森林使用我的神经网络忽略的特征？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gquwgc/repost_why_does_my_random_forest_use_features/</link>
      <description><![CDATA[我的神经网络和随机森林在二元分类任务上的准确率大致相同（随机森林略好）。根据神经网络，某些特征的 shapley 值为零，但对于随机森林，其 shapley 值明显大于零。我的领域知识告诉我，这些特征非常有用，但即使在正则化之后也没有被神经网络拾取。怎么会这样？    提交人    /u/learning_proover   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gquwgc/repost_why_does_my_random_forest_use_features/</guid>
      <pubDate>Thu, 14 Nov 2024 02:46:09 GMT</pubDate>
    </item>
    <item>
      <title>如何训练 CNN 模型来标记所有人脸特征点？照片中有 n 张人脸</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gquvo8/how_to_train_a_cnn_model_to_label_all_the_facial/</link>
      <description><![CDATA[因此，训练 CNN 模型来输出（x，y）一张脸的所有面部标志的位置非常容易，但对于照片中未知数量的脸部，我不知道该怎么做。     提交人    /u/GateCodeMark   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gquvo8/how_to_train_a_cnn_model_to_label_all_the_facial/</guid>
      <pubDate>Thu, 14 Nov 2024 02:45:07 GMT</pubDate>
    </item>
    <item>
      <title>对于基于 Twitter 的数据集，文本预处理步骤的顺序是否正确？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqtscz/is_the_order_of_text_preprocessing_steps_correct/</link>
      <description><![CDATA[顺序是否正确，或者是否有任何步骤需要按顺序更改？  仅保留相关列（文本）。 删除 URL。 删除提及和主题标签。 删除多余的空格。 缩写。 俚语。 将表情符号转换为文本。 删除标点符号。 替换领域特定术语（给定其上下文、机场名称等） 小写。 标记化。 拼写更正。 删除停用词。 删除罕见词 词形还原 命名实体识别（NER）。 词性 (POS) 标记。 文本矢量化。     提交人    /u/Creative_Collar_841   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqtscz/is_the_order_of_text_preprocessing_steps_correct/</guid>
      <pubDate>Thu, 14 Nov 2024 01:49:40 GMT</pubDate>
    </item>
    <item>
      <title>我们对 AI 模型无损压缩研究的关键见解</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqtebn/key_insight_from_our_research_on_lossless/</link>
      <description><![CDATA[      📝 论文： https://arxiv.org/abs/2411.05239 💻 代码： https://github.com/zipnn/zipnn/  我们最近发表了一篇预印本，ZipNN：AI 模型的无损压缩，并希望与社区分享我们的一项重要发现。 神经网络参数可能看起来是随机的（例如，[0.1243, -1.2324, -0.3294...]），但它们在计算机中的表示实际上使压缩成为可能。 关键见解：浮点结构实现压缩 用于存储模型参数的浮点数是结构化的如：  符号位（正/负） 指数（范围） 尾数（精度）  有趣的是，虽然符号位和尾数位看起来是随机的，但指数并不覆盖其范围内的所有值，并且其分布是倾斜的。如图所示，该分布在四个不同的模型中都有说明 — — 我们在许多模型中都观察到了这种模式。 指数值直方图 为什么？这是由于模型的训练方式所致（有关详细信息，请参阅论文中的第 3 段）。 ZipNN 库：利用这一见解 这一见解构成了 ZipNN 的基础，这是我们的开源库无损压缩，与 ZSTD 等最先进的方法相比，它提供了更高的压缩比和更快的压缩/解压缩速度。 流行浮点格式的存储节省：  BF16 格式：节省 33% 的空间 FP32 格式：节省 17% 的空间  我们还开发了 Hugging Face 插件，可以快速下载和加载压缩模型。 示例模型：LLama-3.2-11B 使用 ZipNN，只需添加一行代码即可启用压缩。 🔗 GitHub 存储库   由    /u/Candid_Raccoon2102  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqtebn/key_insight_from_our_research_on_lossless/</guid>
      <pubDate>Thu, 14 Nov 2024 01:29:52 GMT</pubDate>
    </item>
    <item>
      <title>我们对 AI 模型无损压缩研究的关键见解</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqtdjb/key_insight_from_our_research_on_lossless/</link>
      <description><![CDATA[📝 论文： https://arxiv.org/abs/2411.05239 💻 代码： https://github.com/zipnn/zipnn/ 我们最近发表了一篇预印本《ZipNN：AI 模型的无损压缩》，并希望与社区分享我们的一项重要发现。 神经网络参数可能看起来是随机的（例如，[0.1243, -1.2324, -0.3294...]），但它们在计算机中的表示实际上使压缩成为可能。 关键见解：浮点结构可实现压缩 用于存储模型参数的浮点数结构如下：  符号位（正/负） 指数（范围） 尾数（精度）  有趣的是，虽然符号位和尾数位看起来是随机的，但指数并不涵盖其范围内的所有值，并且其分布是倾斜的。如图所示，该分布在四个不同的模型中都有说明 — 我们在许多模型中都观察到了这种模式。 为什么？这是由于模型的训练方式所致（有关详细信息，请参阅论文中的第 3 段）。 ZipNN 库：利用这一见解 这一见解构成了 ZipNN 的基础，这是我们用于无损压缩的开源库，与 ZSTD 等最先进的方法相比，它提供了更高的压缩比和更快的压缩/解压缩速度。 流行浮点格式的存储节省：  BF16 格式：节省 33% 的空间 FP32 格式：节省 17% 的空间  我们还开发了 Hugging Face 插件，可以快速下载和加载压缩模型。 示例模型：LLama-3.2-11B 使用 ZipNN，只需添加一行代码即可启用压缩。 🔗 GitHub 存储库    提交人    /u/Candid_Raccoon2102   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqtdjb/key_insight_from_our_research_on_lossless/</guid>
      <pubDate>Thu, 14 Nov 2024 01:28:44 GMT</pubDate>
    </item>
    <item>
      <title>深度学习数学练习解决方案（De Gruyter）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqrcqj/exercise_solutions_for_the_mathematics_of_deep/</link>
      <description><![CDATA[大家好！我目前正在使用标题中提到的书籍（链接此处）学习深度学习的数学基础。我真的很喜欢它，但我注意到它似乎没有包含练习的解决方案 - 至少在我拥有的版本中没有。我试过在线搜索解决方案，但到目前为止还没有运气。  这里有人可以访问解决方案或知道我可以在哪里找到它们吗？提前感谢任何帮助！    提交人    /u/nerec   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqrcqj/exercise_solutions_for_the_mathematics_of_deep/</guid>
      <pubDate>Wed, 13 Nov 2024 23:52:07 GMT</pubDate>
    </item>
    <item>
      <title>如何处理多标签文本分类？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqq6pi/how_to_deal_with_multi_labeled_text_classification/</link>
      <description><![CDATA[我有大量文本数据，这些数据是多标签的，而且极不平衡。任务是将文本分类到各个类别。问题是我必须预处理文本以减少类别的数据不平衡，并选择相关模型（转换器等）对文本进行分类。我想要一些关于如何预处理数据以处理数据不平衡以及使用哪种模型进行多标签分类的建议？我有 AWS g5x2 大型版，训练应该在 1 小时 30 分钟内（时间限制）完成，准确度合理。    提交人    /u/nottITACHI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqq6pi/how_to_deal_with_multi_labeled_text_classification/</guid>
      <pubDate>Wed, 13 Nov 2024 22:59:19 GMT</pubDate>
    </item>
    <item>
      <title>如何微调 ClipSeg？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqpxm8/how_do_i_finetune_clipseg/</link>
      <description><![CDATA[我正在使用零样本 ClipSeg 进行图像分割，并带有文本提示。如何让此模型提供特定于域的分割？ 谢谢！    提交人    /u/tinkerpal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqpxm8/how_do_i_finetune_clipseg/</guid>
      <pubDate>Wed, 13 Nov 2024 22:47:45 GMT</pubDate>
    </item>
    <item>
      <title>GPT 就是这样处理提示的吗???拜托，我明天要考试...</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqn4cv/is_this_how_gpt_handles_the_prompt_please_i_have/</link>
      <description><![CDATA[大家好，这是我第一次在这里发帖，因为我最近才开始学习 ML。目前，我正在准备一个关于 transformer 的测试，不确定我是否理解得正确。所以我将写下我对提示处理和答案生成的理解，如果我错了，请纠正我。 在训练时，GPT 会同时生成所有输出标记，但在使用经过训练的 GPT 时，它会一次生成一个输出标记。 因此，当给出提示时，此提示将传递给与编码器基本相同的机制，以便在提示内部计算注意力。因此，提示被拆分成标记，然后将标记嵌入并传递到应用非掩码注意力的多个编码器层中。最后，我们剩下提示标记的上下文矩阵。 然后，当 GPT 开始生成时，为了生成第一个输出标记，它需要关注最后一个提示标记。这里，需要 Q、K、V 向量来继续执行解码器算法。因此，对于所有提示标记，我们使用上下文矩阵和解码器在训​​练期间学习的 Wq、Wk、Wv 矩阵来计算它们的 K 和 V 向量。因此，前一个提示标记只需要 K 和 V 向量，而最后一个提示标记还需要一个 Q 向量（因为我们专注于它）来生成第一个输出标记。 现在，解码器机制已应用，我们剩下一个维度为 vocabSize 的向量，其中包含所有词汇标记的概率分布，这些词汇标记将成为下一个生成的词汇标记。因此我们将概率最高的一个作为第一个生成的输出标记。 然后，我们通过将其嵌入向量乘以 Wq、Wk、Wv 矩阵来创建其 Q、K、V 向量，然后我们继续生成下一个输出标记，依此类推…… 所以这是我对其工作原理的理解，如果有任何错误，我将不胜感激任何评论和更正（即使只是一个小细节或命名约定，任何事情对我来说都意义重大）。我希望有人能回答我。 谢谢！    提交人    /u/Ok-Reputation5282   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqn4cv/is_this_how_gpt_handles_the_prompt_please_i_have/</guid>
      <pubDate>Wed, 13 Nov 2024 20:47:15 GMT</pubDate>
    </item>
    <item>
      <title>视频游戏人工智能</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqmc9w/video_game_ai/</link>
      <description><![CDATA[是否有可能制作一个可以控制游戏输入的 AI 覆盖层，然后可以玩游戏并学习如何真正擅长该游戏？如果可以，我很乐意从哪里开始。    提交人    /u/Karatetiger7   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqmc9w/video_game_ai/</guid>
      <pubDate>Wed, 13 Nov 2024 20:14:50 GMT</pubDate>
    </item>
    <item>
      <title>对自己在人工智能和数据科学领域的发展道路感到困惑——NLP/LLM 的研究经验真的足以获得机器学习工程实习机会吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gqcfaw/confused_about_my_path_in_ai_data_scienceis/</link>
      <description><![CDATA[我最近看到一位名为The Data Janitor的 YouTuber 谈论 AI 和 ML 的职业道路，这让我对自己的方法产生了怀疑。他提到，ML 工程职位主要针对具有扎实数据背景的人，比如数据工程师和数据科学家。他甚至给出了这个等式：数据 + 建模 = ML 模型。 现在，这让我怀疑只专注于建模是否足够。我目前正在攻读 AI 和数据科学学位课程，我的目标是最终在 AI/ML 领域工作。但问题是：我不确定攀登典型的“数据”层级（比如从数据分析师做起）是否适合我。在我的国家，实际上没有入门级数据分析师的工作，远程工作看起来也没有什么前景，因为市场上充斥着愿意以低工资工作的人们。 现在，我正准备成为我所在大学教授的研究助理。我们的大部分工作将涉及 NLP/LLM 项目，例如针对特定应用微调现有模型，例如识别阿拉伯语手写，我们将在研究论文中发表这些发现。我的问题是：这种研究经验是否会增加我获得非学术职位的机会，尤其是实习机会？ 我的目标是在大一找到实习机会。我一直在研究机器学习入门级实习的要求，其中一些要求似乎太简单了。他们列出了“Python 基础知识”、“了解 ANN 架构”和“对 TensorFlow 有一定的了解”等内容，就足够了。这是真的吗？ 很想得到一些建议，关于以研究为重点的 LLM 和 NLP 经验是否能从长远来看帮助我，或者我是否最好转向不同的方法。提前感谢任何想法！    提交人    /u/R0b0_69   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gqcfaw/confused_about_my_path_in_ai_data_scienceis/</guid>
      <pubDate>Wed, 13 Nov 2024 13:05:36 GMT</pubDate>
    </item>
    <item>
      <title>“机器学习中的点积和线性变换”</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gq9ye3/dot_product_linear_transformation_in_ml/</link>
      <description><![CDATA[      https://preview.redd.it/iias75g6cn0e1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=79b3d053e6e4f51b9fc0889d70ac80c1111deb74 点积和线性变换是机器学习和线性代数的基础。 点积：需要两个向量并输出一个标量，测量一个向量与另一个向量的“对齐”程度 - 就像投射阴影一样。 线性变换：将矩阵与向量相乘以改变其方向、比例或维数，重塑空间同时保留向量关系。 连接： 点积可以看作是将一个向量投影到另一个向量的跨度上。这与使用矩阵的线性投影变换一致。 例如，使用矩阵将二维向量映射到一维线可反映点积的计算。 为什么它在机器学习中很重要： 特征重要性：用于线性回归等算法中计算加权和。 相似性度量：量化聚类或推荐系统中的特征相似性。 高效计算：神经网络利用矩阵向量乘法（点积批次）。 降维：PCA 使用投影来简化数据，同时保留关键模式。 在 Vizuara 的 YouTube 上观看我的讲座了解更多信息：https://www.youtube.com/watch?v=47c2138lFRI&amp;feature=youtu.be    由   提交  /u/thesreedath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gq9ye3/dot_product_linear_transformation_in_ml/</guid>
      <pubDate>Wed, 13 Nov 2024 10:31:05 GMT</pubDate>
    </item>
    <item>
      <title>𝐁𝐮𝐢𝐥𝐝 𝐋𝐋𝐌𝐬 𝐟𝐫𝐨𝐦 𝐬𝐜𝐫𝐚𝐭𝐜𝐡</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gq6jsr/𝐁𝐮𝐢𝐥𝐝_𝐋𝐋𝐌𝐬_𝐟𝐫𝐨𝐦_𝐬𝐜𝐫𝐚𝐭𝐜𝐡/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gq6jsr/𝐁𝐮𝐢𝐥𝐝_𝐋𝐋𝐌𝐬_𝐟𝐫𝐨𝐦_𝐬𝐜𝐫𝐚𝐭𝐜𝐡/</guid>
      <pubDate>Wed, 13 Nov 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>