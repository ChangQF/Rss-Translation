<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Mon, 06 Jan 2025 21:15:20 GMT</lastBuildDate>
    <item>
      <title>从贝叶斯语言模型生成序列？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv8ziq/sequence_generation_from_a_bayesian_language_model/</link>
      <description><![CDATA[我很好奇在贝叶斯语境中，token 生成与 transformer 模型生成token 的方式有何不同。如果我理解正确的话，transformer 的输出是下一个单词的概率向量。（假设的）贝叶斯等价物会是这些向量的后验预测分布吗？    提交人    /u/Murky-Motor9856   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv8ziq/sequence_generation_from_a_bayesian_language_model/</guid>
      <pubDate>Mon, 06 Jan 2025 20:30:45 GMT</pubDate>
    </item>
    <item>
      <title>我如何利用人工智能构建“思维宫殿”（你也可以）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv8ols/how_i_built_a_mind_palace_using_ai_and_you_can_too/</link>
      <description><![CDATA[        由    /u/zeloxolez 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv8ols/how_i_built_a_mind_palace_using_ai_and_you_can_too/</guid>
      <pubDate>Mon, 06 Jan 2025 20:18:51 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型中的多重损失</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv8j3q/multiple_losses_in_keras_model/</link>
      <description><![CDATA[在训练具有多个输出的 Keras 模型时，每个输出在编译步骤中都有自己的损失函数，在训练期间，我在模型的打印输出中只看到 1 个损失值。同样，在调用 model.evaluate() 时，它只返回 1 个损失。我如何才能获得每个输出的单独损失以及总损失？  以下是代码： input_wide = tf.keras.layers.Input(shape=[5], name=&quot;input_wide&quot;) input_deep = tf.keras.layers.Input(shape=[6], name=&quot;input_deep&quot;) norm_layer_wide = tf.keras.layers.Normalization() norm_layer_deep = tf.keras.layers.Normalization() norm_wide = norm_layer_wide(input_wide) norm_deep = norm_layer_deep(input_deep) hidden1 = tf.keras.layers.Dense(30,activation=&quot;relu&quot;)(norm_deep) hidden2 = tf.keras.layers.Dense(30,activation=&quot;relu&quot;)(hidden1) concat = tf.keras.layers.concatenate（[norm_wide，hidden2]） 输出 = tf.keras.layers.Dense（1，name =“output”）（concat） aux_output = tf.keras.layers.Dense（1，name =“aux_output”）（hidden2） 模型 = tf.keras.Model（输入=[input_wide，input_deep]，输出=[output，aux_output]） 优化器 = tf.keras.optimizers.Adam（learning_rate = 1e-3） 模型.编译（损失=（“mse”，“mse”），loss_weights =（0.9，0.1），优化器=优化器，指标=[“RootMeanSquaredError”，“RootMeanSquaredError”]） norm_layer_wide.adapt(X_train_wide) norm_layer_deep.adapt(X_train_deep) history = model.fit( (X_train_wide, X_train_deep), (y_train, y_train), epochs=20, validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)) eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test)) print(eval_results) # [0.33134523034095764, 0.622011661529541, 0.5702391266822815]  如何获得每个输出的单独损失以及总损失？  感谢您的帮助。    由    /u/headmaster_007 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv8j3q/multiple_losses_in_keras_model/</guid>
      <pubDate>Mon, 06 Jan 2025 20:12:41 GMT</pubDate>
    </item>
    <item>
      <title>关于脚本语言的问题；有人怎么能使用 javascript ML/NN？我在网上找到了一个非常适合学习的频道，如果，视觉，并在视频中使用很酷的理论；但是，它使用 javascript......为什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv80t5/question_about_script_language_how_the_hell_can/</link>
      <description><![CDATA[我在 youtube 上找到的这个频道很有趣，并且创建了很酷的视觉效果（孟德尔波特、蒙特卡洛布冯针，使用代码讨论代数变压器，这对于数学生疏或根本没有数学的人来说非常有用，并将代码与理论基础结合起来）它可能也是学习 NN 和 ML 的最佳频道之一。特别是 PC 中的视觉处理，视频是所需的确切时间，揭示了很酷的谜题 但是它使用 Javascript。为什么？！？！？！我被迫使用 Python 进行研究 XD 而且 Javascript 似乎很痛苦。该死的我总是受苦，希望他换一种语言；我宁愿从 C 语言中学习 抱歉，我发牢骚了，我只是哭了，因为我找到了我最喜欢的 3b1b 频道，还有另一个我忘记名字的频道， https://www.youtube.com/watch?v=p7IGZTjC008 只是 https://www.youtube.com/watch?v=alhpH6ECFvQ 他对我来说非常好；只有前几个视频帮助我理解如何将相机与我的电脑集成以及如何将图像数字化；对我来说这是一个谜，他真的是一个坚实的库 为什么是 Javascript？我不明白。我是 java raci st 吗？学习和从事高级编程似乎很痛苦    提交人    /u/Proper_Fig_832   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv80t5/question_about_script_language_how_the_hell_can/</guid>
      <pubDate>Mon, 06 Jan 2025 19:52:45 GMT</pubDate>
    </item>
    <item>
      <title>训练 Block Blast 求解器的最佳方法？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv7oe3/best_way_to_train_a_block_blast_solver/</link>
      <description><![CDATA[嘿，我是神经网络的初学者，我决定制作一个可以玩流行游戏 Block Blast 的人工智能，它类似于俄罗斯方块，除了你可以将方块放在 8x8 的棋盘上（你不能旋转方块。） 我首先重新创建了游戏，然后使用 tensorflow 创建一个模型： 输入：（64 + 25 + 25 + 25 = 139） 网格状态（64 个值）+ 块 1 形状（5x5，因此有 25 个值）+ 块 2 形状（5x5，因此有 25 个值）+ 块 3 形状（5x5，因此有 25 个值） 2 个隐藏层（每层 64 个节点） 输出： (64 + 64 + 64 = 192) 每个位置上方块 1 的概率 (64 个值) + 每个位置上方块 2 的概率 (64 个值) + 每个位置上方块 3 的概率 (64 个值) 由于我没有任何数据集，我使用神经进化并让 AI 玩 15 组游戏来确定其适应性。在我的设置中，当 AI 试图将方块放置在无法放置的位置（因为该位置已被占用）时，它会自动失败。  https://www.hippovideo.io/video/play/J0qGk5BtLu51smEKv2tf46B0mvtlqxQs7qTIXUx_W9w （小视频，当我点击“运行”按钮时，它什么也没做，这是因为 AI 给出了一个错误的移动） 因此，我得到的最好的模型在搞砸之前只做了 3 步。无论我对模型进行多少次变异，我都无法让模型将块放在正确的位置。我的神经网络设置错误吗？我没有像我应该的那样训练神经网络吗？有人可以指导我吗？    提交人    /u/witherbattler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv7oe3/best_way_to_train_a_block_blast_solver/</guid>
      <pubDate>Mon, 06 Jan 2025 19:39:01 GMT</pubDate>
    </item>
    <item>
      <title>我在训练模型时遇到了奇怪但有趣的错误。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv5okn/weird_but_interesting_bug_im_encountering_while/</link>
      <description><![CDATA[因此，我正在训练一个模型（封装在 RNN 层中的 ANN），其输出顺序为 a、b、c、d。训练时，c、d 收敛，而 a、b 不收敛。a、b 随机振荡，甚至看起来不像收敛。 现在有趣的部分是，如果我改变数据的顺序，使得我有 c、d、a、b；a、b 收敛，而 c、d 不收敛，并显示与之前的 c、d 相同的行为。我进行了调试以查看是否存在任何逻辑错误，但一切似乎都是正确的。我不知道为什么前两个神经元无论数据如何都不会收敛。 我不确定应该朝哪个方向来调试此问题。我绘制了每个输出的损失值，对于前两个神经元，它保持振荡，对于后两个神经元，它随着时间的推移而减小。 仅供参考：我使用的是 mse 损失函数。    提交人    /u/Alex_7738   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv5okn/weird_but_interesting_bug_im_encountering_while/</guid>
      <pubDate>Mon, 06 Jan 2025 18:17:34 GMT</pubDate>
    </item>
    <item>
      <title>CLIP 嵌入确定性？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv56wk/clip_embedding_deterministic/</link>
      <description><![CDATA[我的第一个问题是 CLIP 嵌入是否具有确定性，随后的问题是有多少回旋余地？ 我认为将对象或视觉概念作为加密密码可能会很酷。相同的图像但通过诸如 jpeg 压缩之类的方法会很棒，但如果您可以拍摄物理对象的照片并可靠地完成这项工作，那将是最佳选择。当然，如果我可以拍摄一盏非常具体的灯的照片，而不是任何灯或光源，那就太好了。 有没有办法将 clip 用于此目的？这是否有点太过分和不可靠？    提交人    /u/NihilisticAssHat   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv56wk/clip_embedding_deterministic/</guid>
      <pubDate>Mon, 06 Jan 2025 17:57:55 GMT</pubDate>
    </item>
    <item>
      <title>Keras 模型拟合 - 它仍然是增量的吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv3x10/keras_model_fit_is_it_still_incremental/</link>
      <description><![CDATA[我遇到一个问题，我必须逐块训练我的 keras 模型（时间序列数据 - 无监督异常检测问题，数据太大而无法全部保存在内存中）。 我在网上找到了一些帖子（旧帖子，可能已经改变了），它们说使用 fit 方法将继续学习 - 但我不确定，因为文档缺少这些信息。目前我有以下代码： seq_length = 50 batch_size = 64 epochs = 10 partition_size = &quot;50MB&quot;分区 = train_df.repartition(partition_size=partition_size).to_delayed() for partition in partitions: pandas_df = partition.compute() 数据 = pandas_df.to_numpy(dtype=float) 序列 = create_sequences(data, seq_length) autoencoder.fit(sequences, 序列, epochs=epochs, batch_size=batch_size, verbose=1)  fit 方法会逐步训练我的模型吗？如果我想分块进行训练，该怎么做？    提交人    /u/Wikar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv3x10/keras_model_fit_is_it_still_incremental/</guid>
      <pubDate>Mon, 06 Jan 2025 17:06:28 GMT</pubDate>
    </item>
    <item>
      <title>我们创建了足球比赛语义分割数据集</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv1hn0/we_have_created_a_football_match_semantic/</link>
      <description><![CDATA[我很高兴与大家分享我们创建的一个新数据集：足球比赛语义分割数据集。该数据集由从足球比赛视频中手动选择的帧组成，每个帧都带有语义分割标签。标签包括广告、场地、足球、球门栏、守门员、裁判、观众、球队和背景等类别，每个类别都与特定的 RGB 颜色代码相关联。我们相信这个数据集对于从事计算机视觉任务（尤其是在体育分析领域）的人来说是一个宝贵的资源。非常欢迎您的反馈和建议。该数据集可供研究和商业使用。 您可以在此处访问数据集    提交人    /u/hasibhaque07   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv1hn0/we_have_created_a_football_match_semantic/</guid>
      <pubDate>Mon, 06 Jan 2025 15:24:41 GMT</pubDate>
    </item>
    <item>
      <title>计算大数据的 LOF</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hv05hi/calculating_lof_for_big_data/</link>
      <description><![CDATA[您好， 我有一个大数据集（数亿条记录，数十 GB），我想对异常检测问题执行 LOF（出于学术目的测试不同的方法）对该数据集进行训练，然后在较小的标记数据集上进行测试以检查方法的准确性。由于很难一次容纳所有数据，是否有任何实现允许我分批训练它？你会怎么做？    提交人    /u/Wikar   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hv05hi/calculating_lof_for_big_data/</guid>
      <pubDate>Mon, 06 Jan 2025 14:24:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta 的 LCM（大型概念模型）：改进的 LLM，用于输出概念，而不是标记</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1huyyny/metas_lcms_large_concept_models_improved_llms_for/</link>
      <description><![CDATA[因此，Meta 最近发表了一篇关于 LCM 的论文，该论文可以一次输出整个概念，而不仅仅是一个标记。这个想法非常有趣，可以支持任何语言、任何模态。请在此处查看更多详细信息：https://youtu.be/GY-UGAsRF2g    提交人    /u/mehul_gupta1997   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1huyyny/metas_lcms_large_concept_models_improved_llms_for/</guid>
      <pubDate>Mon, 06 Jan 2025 13:27:29 GMT</pubDate>
    </item>
    <item>
      <title>在成为 ML 工程师之前，我是否需要先担任软件工程师？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1huqs27/do_i_need_to_work_as_a_software_engineer_first/</link>
      <description><![CDATA[我正在攻读计算机科学学士学位，基本上我想问的是，在进入 ML 之前，我是否需要先建立传统的软件工程经验？ 然后，我再次看到具有商业或化学/机械/土木工程等背景的人学习机器学习，这怎么可能呢？我认为你至少应该拥有一些计算机相关课程的学士学位？ 无论如何，当谈到编程技能和数据结构和算法时，我是否需要达到 FAANG 有志者的水平，他们基本上是具有丰富数据结构和算法知识的 leetcode 猴子，才能成为一名机器学习工程师？ 我该如何在 DSA / leetcode monkey 方面变得如此强大，然后学习成为一名机器学习工程师？仅 DSA/leetcode FAANG 级别所需的时间和奉献精神就令人难以置信。    提交人    /u/Comfortable-Unit9880   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1huqs27/do_i_need_to_work_as_a_software_engineer_first/</guid>
      <pubDate>Mon, 06 Jan 2025 04:28:13 GMT</pubDate>
    </item>
    <item>
      <title>集成模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1huo0mk/ensemble_model/</link>
      <description><![CDATA[      嗨！我正在尝试使用经典 ML 为 CIFAR 10 数据集创建一个集成模型，作为我的课程项目。我使用 SVM、NKK、随机森林和梯度提升创建了一堆模型，使用数据集的 HOG 特征进行训练，采用不同的提取方法，应用 openCV 的 grabcut 算法并跳过结果中的全黑图像，然后提取其灰度、RGB 中的 HOG 特征并使用增强，总共为集成模型创建了 12 个模型。结果中最好的模型得分为 52%，平均值约为 46%，但集成模型的准确率为 36%。我使用混淆矩阵来计算每个类别的模型准确率，而不是采用每个模型的总体准确率，认为这会有所帮助。有人可以告诉我如何提高集成模型得分吗？    提交人    /u/MakinaDeFuego6942   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1huo0mk/ensemble_model/</guid>
      <pubDate>Mon, 06 Jan 2025 02:05:52 GMT</pubDate>
    </item>
    <item>
      <title>使用 4gb GPU 可以进行 LLM 研究吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1huhmk4/is_it_possible_to_do_llm_research_with_a_4gb_gpu/</link>
      <description><![CDATA[大家好，社区！ 正如标题所示，使用 4GB RTX 3050 Ti、i7 处理器和 16GB RAM 进行 LLM 研究是否可行？ 我目前正在研究 transformer 的工作原理，并希望开始亲自动手进行实验。是否有任何非常轻量级的开源 LLM 可以在这些规格上运行？如果有，您会推荐哪种型号？ 我之所以问这个问题，是因为我想从我拥有的资源开始，并尽可能少地在云计算上花钱。    提交人    /u/angry_gingy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1huhmk4/is_it_possible_to_do_llm_research_with_a_4gb_gpu/</guid>
      <pubDate>Sun, 05 Jan 2025 21:19:31 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>