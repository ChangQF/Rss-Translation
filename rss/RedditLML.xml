<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Mon, 19 Aug 2024 06:22:39 GMT</lastBuildDate>
    <item>
      <title>我现在正在进行 ml 专业化，之后我将学习深度学习课程的入门知识，但我怀疑我已经看到还有许多其他经典 ML 相关的东西，如贝叶斯线性注册、贝叶斯决策理论、内核、GDA 等。所以我什么时候会这样做？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evulbx/i_am_now_doing_the_ml_specialization_one_and/</link>
      <description><![CDATA[标题    提交人    /u/Agitated-Bowl7487   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evulbx/i_am_now_doing_the_ml_specialization_one_and/</guid>
      <pubDate>Mon, 19 Aug 2024 06:04:29 GMT</pubDate>
    </item>
    <item>
      <title>了解线性回归参数估计的方差</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evubvh/understand_variance_of_linear_regression/</link>
      <description><![CDATA[      因此，我正在阅读 ESL 并尝试更好地理解基础知识。在线性回归部分（第 3 章）中，讨论了估计 beta-hat 的方差（方程 3.8）。有人能解释一下这个方程是如何（逻辑上）得到的吗？我将尽力解释我对这个方程的困惑。 最小二乘回归的参数对于来自总体的单个样本是确定性的。这里单个样本代表一个数据集。因此，如果我们从这个总体（即更​​多数据集）中随机抽取样本，我们的 beta hat 估计值应该有一定的方差。但我不明白该方程的概念含义，它有两个分量，  X 转置和 X 的矩阵乘积的逆  现在，我知道矩阵的逆（用线性代数术语来说）是试图逆转由原始矩阵引起的变换。所以我的第一个问题是，X-transpose 和 X 之间的矩阵乘积代表什么变换？  在平方 sigma-hat 计算中，我期望分母为 1/N，而不是 1/(N-p-1)，其中 p 表示维数，额外的 1 表示偏差。当我联系一位朋友时，他告诉我这与自由度有关。我在这里感到困惑的是 - 假设我们有 100 个数据点和 2 个特征。(100 - 2 -1) = 97 如何成为自由度。有人可以用更简单的术语解释这个概念吗？以便更好地理解它。  以下是方程式的图片，供参考 https://preview.redd.it/il22jiu87kjd1.png?width=1144&amp;format=png&amp;auto=webp&amp;s=237863f9c016cd792faa39f0002253ed73fb930c 提前致谢！    由   提交  /u/PsychologicalRide127   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evubvh/understand_variance_of_linear_regression/</guid>
      <pubDate>Mon, 19 Aug 2024 05:47:46 GMT</pubDate>
    </item>
    <item>
      <title>除了梯度下降之外，还有哪些其他拟合模型的方法？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evsev6/what_are_some_other_methods_of_fitting_a_model/</link>
      <description><![CDATA[我正在做一个简单的项目，从头开始实现各种机器学习模型，我想提供使用不同方法拟合模型的灵活性。 例如，我希望用户能够选择他们使用的模型（例如线性回归/ SVM / KNN），还可以选择他们想要如何拟合模型（例如梯度下降，随机梯度下降等） 我注意到很多教程和项目都使用梯度下降作为拟合许多回归模型的默认方法。我想知道其他模型是否使用其他方法。 我只想要一个我要实现的各种方法的列表。 此外，除了网格搜索之外，还有其他超参数调整方法吗？    提交人    /u/leemanjoo   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evsev6/what_are_some_other_methods_of_fitting_a_model/</guid>
      <pubDate>Mon, 19 Aug 2024 03:49:55 GMT</pubDate>
    </item>
    <item>
      <title>关于变压器尺寸的一些问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evs7gw/some_questions_about_dimensions_of_the_transformer/</link>
      <description><![CDATA[以下是我对“注意力就是你所需要的一切”论文中的原始转换器提出的问题（在推理的背景下，而不是训练） 在编码器中，如果输入序列小于最大上下文大小，它只是填充？编码器如何处理这种填充？ 2）编码器/解码器的最大上下文大小是否相同？ 3）对于解码器，初始“输入序列”是 SOS；那么这里没有填充吗？ 就像这样： 初始输入：1 x d_model，然后它进行预测，并将该预测附加到 SOS。 然后是 2 x d_model。 然后：3 x d_model 等等？ 3）解码器如何停止？它能在达到最大上下文大小之前停止吗？    提交人    /u/Mammoth-Radish-4048   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evs7gw/some_questions_about_dimensions_of_the_transformer/</guid>
      <pubDate>Mon, 19 Aug 2024 03:38:28 GMT</pubDate>
    </item>
    <item>
      <title>如何解开潜在空间</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evq3oj/how_disentangle_the_latent_space/</link>
      <description><![CDATA[您好，我正在一个项目中，其中有一个 3D 面部动画。 我的目标是创建该面部的 2D 表示，但带有用户可以编辑的控件。因此，我导出了 6000 张不同面部表情的图像以及每张图片的相关控件。 我有一个运行良好的 VAE，以及一个可以从控件预测潜在空间的 MLP。在 UI 上，用户修改控件并预测潜在空间，然后我使用该潜在空间重建图像。 此设置在许多控件组合上运行良好，但在其他组合上会产生一些噪音。我的直觉是，潜在空间缺乏一些结构，无法实际理解哪个控制实际上负责每个图像部分。 我试图在 VAE 中添加额外的损失，在那里我从潜在空间预测控制值，虽然它有帮助，但有些组合仍然会产生一些噪音。 我能做些什么来帮助解开潜在空间？    提交人    /u/vincentzaraek   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evq3oj/how_disentangle_the_latent_space/</guid>
      <pubDate>Mon, 19 Aug 2024 01:49:50 GMT</pubDate>
    </item>
    <item>
      <title>缺失值 MNAR</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evpbh0/missing_values_mnar/</link>
      <description><![CDATA[大家好，“我正在研究一个机器学习项目，该项目有两列：‘一个人开始吸烟的年龄’和‘醒来和吸第一支烟之间的间隔时间’。这些列中的缺失值对应于非吸烟者（已经有了一个二元列表示吸烟者或非吸烟者）。我需要一个解决方案来处理这些缺失值，这些缺失值不会扭曲进一步的分析或模型，因为我不能使用平均值（非随机缺失值）和使用占位符，如 -1 或 999 idk。有什么建议吗？”    提交人    /u/medaziz777   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evpbh0/missing_values_mnar/</guid>
      <pubDate>Mon, 19 Aug 2024 01:10:14 GMT</pubDate>
    </item>
    <item>
      <title>我为 Pokémon BDSP 创造了终极自动闪光猎人</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evp4yl/i_created_the_definitive_automatic_shiny_hunter/</link>
      <description><![CDATA[      大家好！我是 Dinones！我编写了一个使用对象检测的 Python 程序，让我的电脑在我睡觉时在我的实体 Nintendo Switch 上捕捉闪光神奇宝贝。到目前为止，我已经在 Pokémon BDSP 中自动捕捉了闪光宝可梦，如 Giratina、Dialga 或 Azelf、Rotom、Drifloon、所有三种初始宝可梦等等。想知道它是如何工作的吗？快来看看吧！该程序对所有人开放！显然是免费的；我只是一个喜欢在空闲时间编写这些程序的学生 :) 游戏在 Nintendo Switch（不是模拟的，是真实的）上运行。该程序使用捕获卡获取输出图像，然后对其进行处理以检测宝可梦是否闪光（OpenCV）。最后，它使用蓝牙（NXBT）模拟 joycons 并控制 Nintendo。 也可以在 Raspberry Pi 上使用！ 我不会用这个赚钱，我只是觉得我的项目会让很多人感兴趣。 📽️ Youtube：https://www.youtube.com/watch?v=84czUOAvNyk 🤖 Github：https://github.com/Dinones/Nintendo-Switch-Pokemon-Shiny-Hunter https://preview.redd.it/1pckbzv5sijd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=6c497fced41ad71dea07e7876edbabc9e63c3c9a https://preview.redd.it/lyz2e0x5sijd1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=48b094bbf486382dfa685080b602ff4d0afdb554    提交人    /u/Dinones   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evp4yl/i_created_the_definitive_automatic_shiny_hunter/</guid>
      <pubDate>Mon, 19 Aug 2024 01:01:19 GMT</pubDate>
    </item>
    <item>
      <title>您有兴趣了解生物工程的数据科学吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evn75y/are_you_interested_in_learning_about_data_science/</link>
      <description><![CDATA[        提交人    /u/Faisal-CS   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evn75y/are_you_interested_in_learning_about_data_science/</guid>
      <pubDate>Sun, 18 Aug 2024 23:28:06 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器入门指南</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evl3rn/autoencoders_for_dummies/</link>
      <description><![CDATA[大家好！ 我的名字是 Eros，我是一名人工智能硕士生。 这是我试图让自动编码器的主题变得容易理解的尝试。 欢迎评论！ https://theelandor.github.io/prova/autoencoders.pdf    提交人    /u/Grand-Sale-2343   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evl3rn/autoencoders_for_dummies/</guid>
      <pubDate>Sun, 18 Aug 2024 21:54:04 GMT</pubDate>
    </item>
    <item>
      <title>玻尔兹曼机与神经网络相比如何？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evkgcp/how_do_boltzmann_machines_compare_to_neural/</link>
      <description><![CDATA[  由    /u/WishIWasBronze  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evkgcp/how_do_boltzmann_machines_compare_to_neural/</guid>
      <pubDate>Sun, 18 Aug 2024 21:26:35 GMT</pubDate>
    </item>
    <item>
      <title>对法学硕士 (LLM) 工作原理的直观解释</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1evdab7/an_intuitive_explanation_of_how_llms_work/</link>
      <description><![CDATA[      博客文章中的一张图显示了 LLM 的示例输出。这些概率加起来为 1（或 100%） 嗨！ 我写了一篇博文文章，以非常直观的方式解释了 LLM 的工作原理。我们从高层次的抽象开始，其中 LLM 被视为个人助理，然后深入研究并涵盖标记化、采样和嵌入等概念。 我添加了一些图形以直观的方式说明一些概念。 我还解决了当前 LLM 的一些局限性，例如无法计算“strawberry”中的 R 以及反转字符串“copenhagen”。 我希望你觉得它有用！ 如果您有任何反馈或疑问，请告诉我。 https://amgadhasan.substack.com/p/explaining-how-llms-work-in-7-levels   由    /u/Amgadoz  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1evdab7/an_intuitive_explanation_of_how_llms_work/</guid>
      <pubDate>Sun, 18 Aug 2024 16:24:31 GMT</pubDate>
    </item>
    <item>
      <title>数据科学路线图及免费资源 [分步指南]</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ev9hfm/data_science_roadmap_with_free_resources/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ev9hfm/data_science_roadmap_with_free_resources/</guid>
      <pubDate>Sun, 18 Aug 2024 13:39:34 GMT</pubDate>
    </item>
    <item>
      <title>我正在使用线性回归模型，在末尾和开头是否有垂直散点线</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ev81n2/i_am_using_a_linear_regression_model_are_are/</link>
      <description><![CDATA[        提交人    /u/Beyond_Birthday_13   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ev81n2/i_am_using_a_linear_regression_model_are_are/</guid>
      <pubDate>Sun, 18 Aug 2024 12:27:11 GMT</pubDate>
    </item>
    <item>
      <title>当人们说代码不是“Pythonic”时他们的意思是什么</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ev5v09/what_do_people_mean_when_they_say_the_code_is_not/</link>
      <description><![CDATA[就 ML 而言，什么是 Python 代码？有什么基准需要遵循吗？ 谢谢。    提交人    /u/Infinite-Dragonfruit   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ev5v09/what_do_people_mean_when_they_say_the_code_is_not/</guid>
      <pubDate>Sun, 18 Aug 2024 10:12:23 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>