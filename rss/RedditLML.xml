<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sun, 28 Jan 2024 09:13:23 GMT</lastBuildDate>
    <item>
      <title>如何在信号处理中实现用于基线校正的自定义评分函数？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acygzv/how_to_implement_a_custom_score_function_for/</link>
      <description><![CDATA[我正在尝试通过将其实例化为 SKLearn 自定义估计器并使用 GridSearchCV 来优化用于信号预处理的基线拟合算法。如何定义自定义分数来优化基线拟合，即最大化拟合函数的平滑度和与信号函数的交集？另外，这是定义最佳拟合的合理方法吗？ 最终目标是 SKLearn 管道，在通过模型拟合优化进行反卷积之前具有多个预处理步骤。   由   提交/u/cookiecutter73  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acygzv/how_to_implement_a_custom_score_function_for/</guid>
      <pubDate>Sun, 28 Jan 2024 08:57:00 GMT</pubDate>
    </item>
    <item>
      <title>200k 向量 (30GB) 语义搜索的最佳实践 值得嵌入吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acxy85/best_practices_for_semantic_search_on_200k/</link>
      <description><![CDATA[嗨，我已将一些特定于域的名称向量转换为嵌入，数据集大小为 200k 字。所有嵌入都是使用 OpenAI 的嵌入模型 3 生成的（每个嵌入 3072 个暗淡）。现在我计划实现语义搜索相似度。给定一个域关键字，我想找到前 5 个最相似的匹配项。嵌入所有 280k 个单词后，包含嵌入的 JSON 文件的大小约为 30GB。 我是这个领域的新手，正在评估最佳选项。  我应该这样做吗？使用像 Pinecone 或 Typsense 这样的云矢量数据库，还是在 DigitalOcean 上本地托管？ 如果我选择像 Typsense 这样的云选项，我需要什么配置（RAM 等）才能实现 280k 嵌入（30GB）尺寸）？大概要花多少钱？  过去几天我一直很困惑，找不到有用的资源。我们将非常感谢您提供的任何帮助或建议。   由   提交/u/stoicbats_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acxy85/best_practices_for_semantic_search_on_200k/</guid>
      <pubDate>Sun, 28 Jan 2024 08:21:02 GMT</pubDate>
    </item>
    <item>
      <title>在训练 GAN 时需要帮助……它显示出持续的损失。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acwm74/need_help_in_training_a_gan_it_is_showing/</link>
      <description><![CDATA[      我一直在尝试训练 GAN 模型，但即使在 75 个时期之后，损失仍然是相距很远......我不知道该怎么做...... 我使用了MSEloss，我知道这不是分类的完美方法，但我正在复制一篇论文，必须从其非官方存储库中获得很多帮助......所以我保留了一些东西，因为它是 判别器和生成器损失   由   提交/u/Relevant-Ad9432  /u/Relevant-Ad9432 reddit.com/r/learnmachinelearning/comments/1acwm74/need_help_in_training_a_gan_it_is_showing/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acwm74/need_help_in_training_a_gan_it_is_showing/</guid>
      <pubDate>Sun, 28 Jan 2024 06:52:27 GMT</pubDate>
    </item>
    <item>
      <title>文本分类器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acv0vo/text_classifier/</link>
      <description><![CDATA[大家好，我正在尝试构建一个文本分类器。假设我们有大量的单词，我们想将它们分为更广泛的类别。比如猫、狗都会去找动物。自行车、汽车都会去车辆。我已准备好训练集，其中包含与主题相关的单词。但测试集未标记。这意味着我只有单词，没有它们所属的主题。  我的想法是将单词转换为向量并计算它们的相似度分数。但这种方法将无法找到上下文相似性。任何帮助将不胜感激。预先感谢您！   由   提交/u/TheRizzler2306   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acv0vo/text_classifier/</guid>
      <pubDate>Sun, 28 Jan 2024 05:16:29 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用图像制作脑电图情感识别数据集？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acu5vg/is_it_possible_to_make_your_dataset_for_eeg/</link>
      <description><![CDATA[你好，我是一名学生。我从互联网上看到的大多数用于情感识别的数据集都是通过观看视频和其他东西来获得的。对于我这样一个只会基础编程的学生来说，真的很难。有人可以建议我做点什么吗？我的项目是制作一个脑电图设备，只需查看图像即可识别人类情感   由   提交 /u/Feifeichan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acu5vg/is_it_possible_to_make_your_dataset_for_eeg/</guid>
      <pubDate>Sun, 28 Jan 2024 04:27:21 GMT</pubDate>
    </item>
    <item>
      <title>3Blue1Brown 的背面支撑视频是传统上正确的数学计算方法吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1act005/is_3blue1browns_video_on_back_prop_the/</link>
      <description><![CDATA[我正在写一篇关于机器学习的研究论文，目前，我正在研究与之相关的所有数学知识。我想知道他的视频是否是传统上正确的编写方式，因为在 3-4 个视频中，所有视频都有不同的表示和写入变量等的方式。   由   提交 /u/_Stampy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1act005/is_3blue1browns_video_on_back_prop_the/</guid>
      <pubDate>Sun, 28 Jan 2024 03:24:22 GMT</pubDate>
    </item>
    <item>
      <title>让人工智能唱原创歌词（按照既定的曲调）——有可能吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acstad/getting_ai_to_sing_original_lyrics_to_an/</link>
      <description><![CDATA[我喜欢用人工智能翻唱歌曲。我想尝试使用一些人工智能声音来创作原创歌词。让 RVC 模型“唱歌”的最佳方法是什么？原来的歌词是基于已经唱过那首歌的人的歌词（愚蠢的例子，它说“我想要鱼片”而不是“我想要那样”）？有什么办法可以做到这一点吗？ 最明显的方法是自己唱，但我是一个糟糕的歌手，而且我的亲密朋友也没有一个能保持调子。这对我来说只是一个有趣的项目，所以雇用某人是不可能的。  我对 RVC 有一些经验，包括训练语音模型和创建 AI 歌曲封面的整个过程。    由   提交 /u/Rod-Serling-Lives   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acstad/getting_ai_to_sing_original_lyrics_to_an/</guid>
      <pubDate>Sun, 28 Jan 2024 03:14:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么反向传播不总是与 dropout 正则化一起教授。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acs9yx/why_isnt_backpropagation_always_taught_along_with/</link>
      <description><![CDATA[我最近了解了更多关于神经网络中 dropout 正则化的有效性，我不禁觉得 dropout 应该自然地与整个反向传播算法。有什么理由不这样吗？是否存在 dropout 正则化弊大于利的情况？   由   提交 /u/Traditional_Soil5753   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acs9yx/why_isnt_backpropagation_always_taught_along_with/</guid>
      <pubDate>Sun, 28 Jan 2024 02:46:54 GMT</pubDate>
    </item>
    <item>
      <title>了解 VAE 的资源</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acqyiz/resources_to_understand_vaes/</link>
      <description><![CDATA[我“理解”VAE 是如何工作的，我已经阅读了很多关于它们的内容，但是当我尝试阅读原始论文时，我完全迷失了。我以为我有数学背景，但我很快意识到我没有。  您会推荐阅读哪些课程/书籍来理解该论文（以及其他生成模型论文，一般来说，我对生成模型论文有这个问题。）支撑它的数学超出了我的理解范围。    由   提交 /u/Ok_Seesaw5723   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acqyiz/resources_to_understand_vaes/</guid>
      <pubDate>Sun, 28 Jan 2024 01:39:23 GMT</pubDate>
    </item>
    <item>
      <title>如何抓住 AI 作弊：智胜机器人 - 2024 年版</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/</link>
      <description><![CDATA[&quot;如果您碰巧有任何不使用 ChatGPT 和其他人工智能 (AI) 作弊的学生，是时候了解情况了。根据民主与技术中心最近的一项调查，58% 的学生表示使用生成式人工智能来完成作业。随着人们对这项技术的认识不断提高，这个数字只会增加。与此同时，同一项研究报告称，教育工作者发现自己落后于技术曲线，只有 43% 的教师接受过生成人工智能方面的重要培训。  在本文中，我们将尝试为教育工作者提供所需的信息，以了解学生如何使用这项技术进行作弊以及教师如何检测和应对生成式人工智能。除了检测其使用之外，这项新技术还可能提供利用新的创新教育方式的机会。” https://ai-solutions.pro/tools-to-detect-ai-cheating/  &amp;# 32；由   提交/u/Science-man777  /u/Science-man777 reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/</guid>
      <pubDate>Sat, 27 Jan 2024 23:58:57 GMT</pubDate>
    </item>
    <item>
      <title>为什么线性回归模型的成本函数应用于逻辑回归时会给出非凸图或具有多个局部最小值的图？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acnyri/why_does_the_cost_function_of_a_linear_regression/</link>
      <description><![CDATA[ 由   提交/u/Professional_Path552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acnyri/why_does_the_cost_function_of_a_linear_regression/</guid>
      <pubDate>Sat, 27 Jan 2024 23:18:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习工程师面临的最头疼和烦人的事情是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acmunb/what_are_the_biggest_headaches_and_annoying/</link>
      <description><![CDATA[我是一位非常新的机器学习工程师，我有兴趣了解更多有关机器学习工程师日常解决的问题以及一些问题的信息他们必须解决的最困难的问题，我很乐意听到与任何阶段相关的问题，例如构建模型、训练、部署等。 提前致谢！！ &lt; /div&gt;  由   提交/u/jaym-00   /u/jaym-00  reddit.com/r/learnmachinelearning/comments/1acmunb/what_are_the_biggest_headaches_and_annoying/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acmunb/what_are_the_biggest_headaches_and_annoying/</guid>
      <pubDate>Sat, 27 Jan 2024 22:29:14 GMT</pubDate>
    </item>
    <item>
      <title>关于 Huggingface 中的 GPT-1 的问题。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1aclizm/questions_about_gpt1_in_huggingface/</link>
      <description><![CDATA[大家好，目前我正在学习法学硕士，我有几个菜鸟问题。 首先，让我们从GPT-1 论文：https://cdn.openai.com/research-covers/language- unsupervised/language_understand_paper.pdf 问题 1：嵌入步骤的输入到底是什么形状？ 让我们看一下表达式论文中的块（2）。 根据论文，模型的输入被命名为U。从我到目前为止收集到的信息来看，这些应该是第一个标记化步骤之后的 token_id。然而，我对这里的维度如何运作有点困惑。嵌入矩阵 W_e 应该是一个维度为 N x H 的矩阵，其中 N 是该分词器所有可能的分词 ID 的总数，H 是转换器块中隐藏层的维度。所以基本上 W_e 是一个查找表，其中每一行对应于该令牌 id 的嵌入。 W_e 应该工作的方式是，您采用令牌 id 的单热编码表示并将其相乘以选择适当的行。 所以我有点困惑论文中的乘法是如何运作的。我理解它计算矩阵 U 的方式应该包含每个 token_id 作为 one-hot 编码行。换句话说，矩阵应如下所示： 第一行 = U_{1,:} = [ 0 , 0 , ... 1 , 0 , 0 ...] 第二行 = U_{2,:} = [ 0 , 1 ,0 , .. 0 ] 等等。其中 1 的列索引对应于该令牌的令牌 id 的值。  这是正确的吗？然后我觉得论文中有点不清楚。他们只是写 U= (u1, u2, ...) 就这样了。如果我的理解是正确的，有人可以证实这一点吗？ 问题 2： 这一步发生在 HuggingFace 模型中的哪里？ &lt; p&gt;所以假设之前的解释是正确的，我尝试在 HuggingFace 中调用模型。考虑以下代码片段： from Transformers import OpenAIGPTTokenizer, OpenAIGPTModel import torch tokenizer = OpenAIGPTTokenizer.from_pretrained(“openai-gpt”) model = OpenAIGPTModel.from_pretrained(“openai-gpt”) input = tokenizer(&quot;你好，我的狗很可爱&quot;, return_tensors=&quot;pt&quot;) print(f&quot;输入：{输入}&quot;) 输出 =model(**输入) print(f&quot;输出：{输出}&quot; ;)  它返回： 输入：{&#39;input_ids&#39;：tensor([[3570, 240, 547, 2585, 544, 4957]] ), &#39;attention_mask&#39;: 张量([[1, 1, 1, 1, 1, 1]])} 输出: BaseModelOutput(last_hidden_​​state=tensor([[[ 0.4653, 0.0642, 0.5910, ..., 0.1177, -0.0021 , -1.2262], [-0.3697, -0.0957, 0.6613, ..., -0.0344, -0.2164, 0.1205], [ 0.1700, -0.3252, 0.0407, ..., 0.1589, -0.8057, -0.2830], [- 0.3669, -0.0448, 0.8061, ..., -0.0090, -0.0872, -0.5224], [-0.5047, 0.6522, 0.6932, ..., 0.0811, 0.6475, 0.3190], [-0.2972, 0.0591, 1.2 333、.. ., -0.7394, -0.2600, 0.0863]]], grad_fn=),hidden_​​states=None, Attentions=None)  因此，这里的实际输入似乎是model 只是 token id tensor([[3570, 240, 547, 2585, 544, 4957]]) 的单个张量，尚未采用适当的 one-hot 编码形式 U.任何人都可以确认这是否首先发生在模型内部，然后再乘以嵌入矩阵吗？ 问题 3： 该怎么办与这个“last_hidden_​​state”的输出？ 所以，按照我的理解，这里的输出实际上是最后一个变压器块“h_n”。它的维度为 (1, 6 , 768) - 所以基本上对于每个输入标记，我得到长度为 768 的隐藏最终状态。 那么如果我想用它做某事，现在如何使用这个结果？如果不训练单独的分类器，我对这个最终状态无能为力。 ​   由   提交/u/Invariant_apple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1aclizm/questions_about_gpt1_in_huggingface/</guid>
      <pubDate>Sat, 27 Jan 2024 21:30:06 GMT</pubDate>
    </item>
    <item>
      <title>深度学习小书（François Fleuret，2023 年 6 月 23 日，168 页）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acj92q/the_little_book_of_deep_learning_françois_fleuret/</link>
      <description><![CDATA[      PDF 链接：https://fleuret.org/public/lbdl.pdf 主页：https://fleuret.org/francois/lbdl.html “本书是为具有 STEM 背景的读者提供的关于深度学习的简短介绍，最初旨在在手机屏幕上阅读。它根据非商业知识共享许可证分发，八个月内下载了 500,000 次。 ＆quot; https://preview .redd.it/pspdw9ohe1fc1.jpg?width=1683&amp;format=pjpg&amp;auto=webp&amp;s=e06166fae2656bda007a160f775f7567ac9aaea4   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acj92q/the_little_book_of_deep_learning_françois_fleuret/</guid>
      <pubDate>Sat, 27 Jan 2024 19:49:40 GMT</pubDate>
    </item>
    <item>
      <title>试图让 GA 生成类似于数字 7 的 MNIST 图像，这个想法注定要失败吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1aca14i/trying_to_get_a_ga_to_generate_mnist_like_images/</link>
      <description><![CDATA[大家好，我创建了一个适用于 28x28 二进制矩阵和 MNIST 数字分类器的遗传算法模型，我将 GA 适应度函数调整为依赖于MNIST 分类器给出的矩阵为 #7 的概率。我还没有看到人们在网上做类似的事情，并且由于训练时间很长并且摆弄超参数，算法没有收敛 - 这个想法可行吗？为什么/为什么不呢？  可以提供代码。 （还要注意的是，我应用了高斯模糊和 0-&gt;0、1-&gt;255 的映射来将二进制转换为灰度。）    ;由   提交 /u/Few-Fun3008    reddit.com/r/learnmachinelearning/comments/1aca14i/trying_to_get_a_ga_to_generate_mnist_like_images/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1aca14i/trying_to_get_a_ga_to_generate_mnist_like_images/</guid>
      <pubDate>Sat, 27 Jan 2024 12:42:31 GMT</pubDate>
    </item>
    </channel>
</rss>