<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Sun, 20 Oct 2024 21:14:49 GMT</lastBuildDate>
    <item>
      <title>为什么 DDPM 实现的正弦位置编码与 transformer 不同？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g876j8/why_do_ddpms_implement_a_different_sinusoidal/</link>
      <description><![CDATA[      嗨， 我正在尝试为 DDPM 实现正弦位置编码。我发现了两种解决方案，它们以相同的嵌入维度为相同的位置/时间步长计算不同的嵌入。我想知道其中一个是错的还是两个都是正确的。DDPM 官方源代码不使用 transformers 论文中使用的原始正弦位置编码……为什么？ 1) 来自“Attention is all you need”的原始正弦位置编码论文。 原始正弦位置编码 https://preview.redd.it/569mjvbnqyvd1.png?width=706&amp;format=png&amp;auto=webp&amp;s=c4ef0141668b5c80e835a5bef2631c35a4b57fba 2）DDPM论文官方代码中使用的正弦位置编码 官方 DDPM 代码中使用的正弦位置编码。基于 tensor2tensor。 https://preview.redd.it/m6juj22iqyvd1.png?width=702&amp;format=png&amp;auto=webp&amp;s=5e7f6da6d3a281895366d197a13d81179e7e0c22 为什么 DDPM 的官方代码使用的编码（选项 2）与 transformers 论文中使用的原始正弦位置编码不同？对于 DDPM 来说，第二种选择是否更好？ 我注意到官方 DDPM 代码实现中使用的正弦位置编码是从 tensor2tensor 借用的。实现上的差异甚至在对官方 tensor2tensor 实现的 PR 提交中突出显示。为什么 DDPM 的作者使用这种实现（选项 2）而不是来自 transformers 的原始实现（选项 1）？ ps：如果您想检查代码，请访问这里 https://stackoverflow.com/questions/79103455/should-i-interleave-sin-and-cosine-in-sinusoidal-positional-encoding    提交人    /u/CompSciAI   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g876j8/why_do_ddpms_implement_a_different_sinusoidal/</guid>
      <pubDate>Sun, 20 Oct 2024 19:43:10 GMT</pubDate>
    </item>
    <item>
      <title>我正在尝试在 Colab 中运行一个脚本，希望得到任何帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g86cd4/am_trying_to_run_a_script_in_colab_and_would/</link>
      <description><![CDATA[我正在帮助进行一些研究，并被告知使用 psg-mit/llm-random-number-gen 重现一些结果。我在设置方面遇到了困难，由于一些冲突的软件包，我无法在我的 colab 上运行它，但我不知道如何继续。任何帮助都将不胜感激。    提交人    /u/pavo76   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g86cd4/am_trying_to_run_a_script_in_colab_and_would/</guid>
      <pubDate>Sun, 20 Oct 2024 19:07:07 GMT</pubDate>
    </item>
    <item>
      <title>加入我们的 ML Discord 服务器，与其他机器学习爱好者一起学习、协作和开展项目！</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g838vo/join_our_ml_discord_server_to_learn_collaborate/</link>
      <description><![CDATA[这是链接👇👇 Discord    由   提交  /u/kRiShNa-Kaushik   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g838vo/join_our_ml_discord_server_to_learn_collaborate/</guid>
      <pubDate>Sun, 20 Oct 2024 16:54:25 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用 OpenCV 或计算机视觉来执行哪些过程来增强捕获的手写笔记并使其更清晰？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g82gyh/what_process_can_i_do_using_opencv_or_computer/</link>
      <description><![CDATA[      像这样。 使背景更白并增加笔迹的厚度或对比度的东西......    提交人    /u/AppropriateWork4011   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g82gyh/what_process_can_i_do_using_opencv_or_computer/</guid>
      <pubDate>Sun, 20 Oct 2024 16:21:03 GMT</pubDate>
    </item>
    <item>
      <title>我有一个用于情绪分析的编码器 - 解码器模型，其中有一个我无法弄清楚的错误，这里有没有人有时间可以帮忙？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g82bwo/i_have_an_encoderdecoder_model_for_sentiment/</link>
      <description><![CDATA[我使用 RNN GRU 和电影评论构建了该情绪分析模型。cide 运行良好，但模型训练过程中存在一些错误，k 无法理解它来自哪里。我已经检查了矩阵维度，没问题。所以如果这里有人可以帮助我解决这个问题，我可以给他们笔记本的链接并讨论这个错误    提交人    /u/Emotional-Rhubarb725   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g82bwo/i_have_an_encoderdecoder_model_for_sentiment/</guid>
      <pubDate>Sun, 20 Oct 2024 16:14:57 GMT</pubDate>
    </item>
    <item>
      <title>ML初学者</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g80riz/ml_beginner/</link>
      <description><![CDATA[大家好，ml 社区，我是 ml 和 dl 之旅的新手，我属于 IIT，因此我的数学很好，但是自从我开始 ml 以来，我感到非常失望和孤独，我正在取得非常小的进步，我觉得想放弃它，但是当我看到编码时，它似乎也很难。我缺乏什么，或者这种怀旧的感觉在开始时很常见。 请帮帮我.. 对我来说意义重大     提交人    /u/Longjumping-Cheek101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g80riz/ml_beginner/</guid>
      <pubDate>Sun, 20 Oct 2024 15:05:31 GMT</pubDate>
    </item>
    <item>
      <title>使用计算机学习时，如何整理多个参考文献？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7yy30/how_do_you_organize_multiple_references_while/</link>
      <description><![CDATA[当通过实体书本和笔记学习时，通常会在前面的桌子上摆放 3-4 本书、几张纸/打印件和 1-2 个笔记本。它们都很容易在视觉上区分，因此您知道您从每个来源查找的材料。  当您可以打开许多选项卡/浏览器窗口和 pdf 时，您如何在线学习时组织内容？     提交人    /u/datashri   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7yy30/how_do_you_organize_multiple_references_while/</guid>
      <pubDate>Sun, 20 Oct 2024 13:40:32 GMT</pubDate>
    </item>
    <item>
      <title>在电子商务分类数据集上对 Phi-3.5 进行微调</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7y9dh/finetuning_phi35_on_ecommerce_classification/</link>
      <description><![CDATA[探索微软的新 LLM 系列，并通过在电子商务分类数据集上对其进行微调，将模型的准确率从 65% 提高到 86%。 https://www.datacamp.com/tutorial/fine-tuning-phi-3-5    提交人    /u/kingabzpro   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7y9dh/finetuning_phi35_on_ecommerce_classification/</guid>
      <pubDate>Sun, 20 Oct 2024 13:05:51 GMT</pubDate>
    </item>
    <item>
      <title>哪个 LLM（如 ChatGPT 或 NotebookLM）最适合帮助从已发表的 arXiv 论文中重现深度学习代码？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7xrt9/which_llm_like_chatgpt_or_notebooklm_is_best_for/</link>
      <description><![CDATA[我正在寻找有关最佳大型语言模型 (LLM) 的推荐，例如 ChatGPT、NotebookLM 或其他模型，它们可以帮助在 PyTorch 中重现 arXiv 上发表的研究论文中的深度学习代码。目标是找到一个擅长理解和实现这些论文中的技术细节的 LLM，包括代码片段、数学解释和复杂算法。哪个 LLM 最适合这项任务，为什么？例如，https://arxiv.org/abs/2102.04238。    提交人    /u/Affectionate-Let-246   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7xrt9/which_llm_like_chatgpt_or_notebooklm_is_best_for/</guid>
      <pubDate>Sun, 20 Oct 2024 12:40:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI Swarm 使用 Ollama 实现本地 LLM</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7w4vq/openai_swarm_with_local_llms_using_ollama/</link>
      <description><![CDATA[  由    /u/mehul_gupta1997  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7w4vq/openai_swarm_with_local_llms_using_ollama/</guid>
      <pubDate>Sun, 20 Oct 2024 10:59:01 GMT</pubDate>
    </item>
    <item>
      <title>我的 Loss 和 F1 怎么会相关呢？不是反相关的吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7v9sh/how_can_my_loss_and_f1_be_correlated_not/</link>
      <description><![CDATA[      https://preview.redd.it/1yd04eepvvvd1.png?width=1034&amp;format=png&amp;auto=webp&amp;s=cbd95cbec821e7a7168c198307928fee6eb4f54d 上图是我关于学习率调整的数据，你可以看到，虽然 f1 的差异很小，但是val loss 很大，但是最好的 f1 是 1e-5，但 val loss 最差；而 1e-6 的 f1 最差，但 val loss 最好。在我的另一个数据上可以看到类似的模式，使用 RoBERTa 代替 XLNet。 https://preview.redd.it/zo4vxhhyvvvd1.png?width=1040&amp;format=png&amp;auto=webp&amp;s=c631bf51c4ca2e5602440a9a997bb13c2e239f7e 为了便于理解，这里使用的损失函数是交叉熵，经过 10 次训练，并使用 AdamW 优化器。如果这很重要的话，这是一个具有多类数据而不是多标签的情绪分类任务（我从 GoEmotions 中删除了多标签行，因此数据非常不平衡）。 由于整个过程是我的超参数调整的一部分，我不知道应该使用哪个学习率，应该关注损失还是 f1？ 我的代码中可能存在一些问题导致此问题，或者可能只是方法错误，我对机器学习还很陌生，所以这可能只是我的错误。    提交人    /u/BlehBlah_   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7v9sh/how_can_my_loss_and_f1_be_correlated_not/</guid>
      <pubDate>Sun, 20 Oct 2024 09:57:04 GMT</pubDate>
    </item>
    <item>
      <title>如何同时学习机器学习和数学</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7tpug/how_to_study_ml_along_with_maths/</link>
      <description><![CDATA[先学习数学，然后深入研究机器学习，这非常忙碌。我了解线性代数、微积分和概率的基础知识。但我如何才能学习机器学习以及其中的数学细节。 我如何解决（针对笔试的）包含数学问题但与机器学习相关的问题 推荐一些可以教你这类知识的书籍和资源。如果没有，请建议一种类似这样的研究策略 问题例如：   将以下八个点（以 (x, y) 代表位置）聚类为三个聚类：A1(2, 10)、A2(2, 5)、A3(8, 4)、A4(5, 8)、A5(7, 5)、A6(6, 4)、A7(1, 2)、A8(4, 9) 基于欧几里得距离的 k 均值聚类算法在 100 个点的数据集上运行，k=3。如果点 [1,1] 和 [−1,1] 都是聚类 3 的一部分，那么以下哪一点也必然是聚类 3 的一部分？     提交人    /u/pujitasunnapu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7tpug/how_to_study_ml_along_with_maths/</guid>
      <pubDate>Sun, 20 Oct 2024 07:57:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么不同的架构只需要编码器/解码器或者两者都需要？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7plvb/why_do_different_architectures_only_need_an/</link>
      <description><![CDATA[据我所知，Transformer 最初是为机器翻译而设计的，它使用编码器输入语言，使用解码器输出语言，然后在两者之间有注意层，这不是他们发明的，我也不知道是什么直觉促使他们使用注意层，但它在这项任务上击败了 SotA RNN，并从那里被采用。从那时起，我们有了 BERT，据我所知它只使用编码器并对标记进行去噪，而不是严格预测下一个标记，或者我们有其他 Transformer，例如 GPT，它只使用解码器。我不知道你为什么以及何时使用哪个。 我还对用于其他模态或与其他架构组合的 Transformer 很好奇，例如 GNN 嵌套 Transformer 或 MMDiT。但我觉得编码器/解码器是一个更基本的东西，需要理解。在学校，我学习了 QKV 和注意力操作、多头等，但它们让我的大脑难以理解，我承认，在某个时候，我放弃了理解，纯粹学习了考试所需的知识。我确实想理解。此外，编码器和解码器都是它们自己的神经网络，对吧，那么从理论上讲，它们可以是任何架构吗？从技术上讲，你能训练一个在其编码器和/或解码器中嵌套更多变压器的变压器吗？如果发现与许多其他任务一样，变压器在编码和解码任务中的表现优于传统 SotA，这是一种明智的方法吗？    提交人    /u/w-wg1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7plvb/why_do_different_architectures_only_need_an/</guid>
      <pubDate>Sun, 20 Oct 2024 03:12:20 GMT</pubDate>
    </item>
    <item>
      <title>有人看过这本书吗？有什么想法吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g7f326/anyone_checked_out_this_book_thoughts/</link>
      <description><![CDATA[        提交人    /u/Grouchy_Replacement5   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g7f326/anyone_checked_out_this_book_thoughts/</guid>
      <pubDate>Sat, 19 Oct 2024 18:17:35 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>