<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Sat, 19 Oct 2024 01:14:21 GMT</lastBuildDate>
    <item>
      <title>我该如何处理二进制特征？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6xf7p/how_do_i_deal_with_the_binary_features/</link>
      <description><![CDATA[      我尝试了各种回归算法，但我能得到的最高回归分数是 0.64。我认为偏斜的二进制特征是造成不准确的原因。对它们进行过采样/欠采样有意义吗？    提交人    /u/aryan9596   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6xf7p/how_do_i_deal_with_the_binary_features/</guid>
      <pubDate>Sat, 19 Oct 2024 00:56:28 GMT</pubDate>
    </item>
    <item>
      <title>微调 yolo v8 模型现在需要 wandb.me（权重和偏差软件）的 api 吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6wgn1/fine_tuning_a_yolo_v8_model_now_requires_an_api/</link>
      <description><![CDATA[ie 使用 nc=4 覆盖 model.yaml nc=80使用 nc=4 覆盖 model.yaml nc=80，来自 n 个参数模块参数 0 -1 1 464 ultralytics.nn.modules.conv.Conv [3, 16, 3, 2] 1 -1 1 4672 ultralytics.nn.modules.conv.Conv [16, 32, 3, 2] 2 -1 1 7360 ultralytics.nn.modules.block.C2f [32, 32, 1, True] 3 -1 1 18560 ultralytics.nn.modules.conv.Conv [32, 64, 3, 2] 4 -1 2 49664 ultralytics.nn.modules.block.C2f [64, 64, 2, True] 5 -1 1 73984 ultralytics.nn.modules.conv.Conv [64, 128, 3, 2] 6 -1 2 197632 ultralytics.nn.modules.block.C2f [128, 128, 2, True] 7 -1 1 295424 ultralytics.nn.modules.conv.Conv [128, 256, 3, 2] 8 -1 1 460288 ultralytics.nn.modules.block.C2f [256, 256, 1, True] 9 -1 1 164608 ultralytics.nn.modules.block.SPPF [256, 256, 5] 10 -1 1 0 torch.nn.modules.upsampling.Upsample [无，2，&#39;最近&#39;] 11 [-1，6] 1 0 ultralytics.nn.modules.conv.Concat [1] 12 -1 1 148224 ultralytics.nn.modules.block.C2f [384，128，1] 13 -1 1 0 torch.nn.modules.upsampling.Upsample [无，2，&#39;最近&#39;] 14 [-1，4] 1 0 ultralytics.nn.modules.conv.Concat [1] 15 -1 1 37248 ultralytics.nn.modules.block.C2f [192，64，1] 16 -1 1 36992 ultralytics.nn.modules.conv.Conv [64，64，3，2] 17 [-1，12] 1 0 ultralytics.nn.modules.conv.Concat [1] 18 -1 1 123648 ultralytics.nn.modules.block.C2f [192, 128, 1] 19 -1 1 147712 ultralytics.nn.modules.conv.Conv [128, 128, 3, 2] 20 [-1, 9] 1 0 ultralytics.nn.modules.conv.Concat [1] 21 -1 1 493056 ultralytics.nn.modules.block.C2f [384, 256, 1] 22 [15, 18, 21] 1 752092 ultralytics.nn.modules.head.Detect [4, [64, 128, 256]] 模型摘要：225 层， 3,011,628 个参数，3,011,612 个梯度，8.2 GFLOP 从预训练权重中传输了 319/355 个项目 TensorBoard：从“tensorboard --logdir runs/detect/train4”开始，从 n 个参数模块参数中查看 0 -1 1 464 ultralytics.nn.modules.conv.Conv [3, 16, 3, 2] 1 -1 1 4672 ultralytics.nn.modules.conv.Conv [16, 32, 3, 2] 2 -1 1 7360 ultralytics.nn.modules.block.C2f [32, 32, 1, True] 3 -1 1 18560 ultralytics.nn.modules.conv.Conv [32, 64, 3, 2] 4 -1 2 49664 ultralytics.nn.modules.block.C2f [64, 64, 2, True] 5 -1 1 73984 ultralytics.nn.modules.conv.Conv [64, 128, 3, 2] 6 -1 2 197632 ultralytics.nn.modules.block.C2f [128, 128, 2, True] 7 -1 1 295424 ultralytics.nn.modules.conv.Conv [128, 256, 3, 2] 8 -1 1 460288 ultralytics.nn.modules.block.C2f [256, 256, 1, True] 9 -1 1 164608 ultralytics.nn.modules.block.SPPF [256, 256, 5] 10 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 11 [-1, 6] 1 0 ultralytics.nn.modules.conv.Concat [1] 12 -1 1 148224 ultralytics.nn.modules.block.C2f [384, 128, 1] 13 -1 1 0 torch.nn.modules.upsampling.Upsample [None, 2, &#39;nearest&#39;] 14 [-1, 4] 1 0 ultralytics.nn.modules.conv.Concat [1] 15 -1 1 37248 ultralytics.nn.modules.block.C2f [192, 64, 1] 16 -1 1 36992 ultralytics.nn.modules.conv.Conv [64, 64, 3, 2] 17 [-1, 12] 1 0 ultralytics.nn.modules.conv.Concat [1] 18 -1 1 123648 ultralytics.nn.modules.block.C2f [192, 128, 1] 19 -1 1 147712 ultralytics.nn.modules.conv.Conv [128, 128, 3, 2] 20 [-1, 9] 1 0 ultralytics.nn.modules.conv.Concat [1] 21 -1 1 493056 ultralytics.nn.modules.block.C2f [384, 256, 1] 22 [15, 18, 21] 1 752092 ultralytics.nn.modules.head.Detect [4, [64, 128, 256]] 模型概要：225 层，3,011,628 个参数，3,011,612 个梯度，8.2 GFLOP 从预训练权重中转移了 319/355 个项目 TensorBoard：以“tensorboard --logdir runs/detect/train4”开始，在 http://localhost:6006/http://localhost:6006/ 查看 wandb：使用 wandb-core 作为 SDK 后端。有关更多信息，请参考。https://wandb.me/wandb-core wandb：登录 wandb.ai。 （了解如何在本地部署 W&amp;B 服务器：） wandb：您可以在此处的浏览器中找到您的 API 密钥： wandb：从您的个人资料中粘贴 API 密钥并按 Enter 键，或按 ctrl+c 退出：https://wandb.me/wandb-serverhttps://wandb.ai/authorize --------------------------------------------------------------------------------------- 在 &lt;cell line: 3&gt;() 中中止回溯（最近一次调用最后一次） 1 model = YOLO(&#39;yolov8n.pt&#39;) ----&gt; 2 model.train(data=yaml_output_path, epochs=1000, batch=32, imgsz=720, plots=True,patient = 20, lr0=0.01) &lt;ipython-input-9-e3277e5a8526&gt;    由    /u/punkindrublicyo  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6wgn1/fine_tuning_a_yolo_v8_model_now_requires_an_api/</guid>
      <pubDate>Sat, 19 Oct 2024 00:06:45 GMT</pubDate>
    </item>
    <item>
      <title>在生产中测量模型性能</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6w9fb/measuring_model_performance_in_production/</link>
      <description><![CDATA[假设我在离线评估期间有一个准确率为 95% 的分类模型。如何在生产过程中跟踪模型性能？由于标签要过一段时间才可用，我可以使用哪些指标。    提交人    /u/lalalagay   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6w9fb/measuring_model_performance_in_production/</guid>
      <pubDate>Fri, 18 Oct 2024 23:56:46 GMT</pubDate>
    </item>
    <item>
      <title>我总是遇到基于稀疏图的操作的性能问题，当前的技术堆栈是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6vgj1/i_always_have_performance_issue_with_sparse/</link>
      <description><![CDATA[我对深度学习不是很专业，我的背景主要是传统信号处理（所以我的大部分编码经验都在 MATLAB 中哈哈），所以如果我的问题有任何缺陷，请提前致歉。 最近我开始研究“基于模型的深度学习”，这有点像传统信号处理和现代深度学习的重叠，我们的想法是将迭代的传统 SP 算法展开到前馈神经网络中并通过该 NN 学习一些参数。 首先，我找不到很好的资源来在 PyTorch 或 JAX 中实现这些内容，我的意思是有成千上万的 CNN 或 transformers 或 LLM 资源，但我找不到很多特别是这种“基于模型的 DL”的资源的事情，如果你有任何资源我将不胜感激。 其次，我正在使用图信号处理方案探索这个问题，即将图信号处理算法展开到 NN 中，它在某种意义上有点类似于 GNN，与图拉普拉斯算子和类似的员工相乘。 这里的关键是，与 CNN 或 Transformers 等现代 DL 架构相比，我的代码总是需要这么多内存并且消耗大量时间来训练。 所以我真的不确定现在人们用什么技术堆栈来做这些事情？我在 PyG 上看到他们正在使用 torch_sparse 库来实现稀疏矩阵-向量乘法，但这似乎并没有太大帮助，因为 GPU 在执行稀疏操作方面性能并不好，我也听说过“KeOPS”但还没有尝试过。 我只想问你们，你们是如何处理这些稀疏操作的？特别是与 GNN 和几何 DL 相关的事情，因为它们与我的工作最相似的现代 DL 架构，并且一定要进行稀疏操作吗？我的意思是有时候我觉得稀疏操作的开销几乎与使用密集矩阵执行所有操作相同。    提交人    /u/WerewolfAmbitious131   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6vgj1/i_always_have_performance_issue_with_sparse/</guid>
      <pubDate>Fri, 18 Oct 2024 23:16:30 GMT</pubDate>
    </item>
    <item>
      <title>机器学习协作组</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6vf1j/machine_learning_collaboration_group/</link>
      <description><![CDATA[大家好， 我向所有想要掌握机器学习的人寻求帮助 — — 无论你无法使用 GPU，还是受到免费 Colab 的限制，或者只是相信在小组中工作会帮助你创建很酷的项目并获得个人工作所需的支持。 我正在创建一个 ML 协作小组，以共同构建项目并相互支持。无论是开展小组项目、帮助完成个人任务、准备面试还是仅仅提供建议，目标都是创建一个支持性的学习社区。此外，小组项目在简历上看起来很棒！ 如果这听起来像你感兴趣的东西，请随时给我发私信！    提交人    /u/AdEfficient1804   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6vf1j/machine_learning_collaboration_group/</guid>
      <pubDate>Fri, 18 Oct 2024 23:14:25 GMT</pubDate>
    </item>
    <item>
      <title>你如何跟上？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6u8nq/how_do_you_keep_up/</link>
      <description><![CDATA[大家好，2013-2019 年我从事人工智能领域，我真正学到的最后一件事是 transformer 和 BERT。从那时起，我更专注于软件，对发生的事情失去了深入了解。我想知道你们如何了解最新的工作，以及如何同时保持健康的生活方式。有时，这也很可怕，从那时起出现的论文和技术的数量让人不知所措。最近，我在阅读一篇关于 PEFT 技术的调查论文，已经有 40-50 种方法可以做到这一点。似乎每次我试图了解外面的情况时，都感觉遥不可及。我知道我的观点可能不正确，但我正在寻求任何建议，让我看到更大的图景，而不会感到不知所措并最终放弃。我认为我需要努力变得更加自律，开始学习而不是担心外面的一切，但如果能从该领域的专业人士那里获得有关如何实现这一目标的任何建议，我将不胜感激。  谢谢    提交人    /u/bal-ame   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6u8nq/how_do_you_keep_up/</guid>
      <pubDate>Fri, 18 Oct 2024 22:17:08 GMT</pubDate>
    </item>
    <item>
      <title>Meta 发布了 Spirit LM，这是一款可以根据文本/音频作为输入生成音频和文本的 LLM</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6nm2u/meta_released_spirit_lm_an_llm_that_can_generated/</link>
      <description><![CDATA[Meta 今天发布了许多代码、模型和演示。其中最主要的是 SAM2.1（改进的 SAM2）和 Spirit LM，后者是一种可以同时接受文本和音频作为输入并生成文本或音频的 LLM（演示非常不错）。请在此处查看 Spirit LM 演示：https://youtu.be/7RZrtp268BM?si=dF16c1MNMm8khxZP    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6nm2u/meta_released_spirit_lm_an_llm_that_can_generated/</guid>
      <pubDate>Fri, 18 Oct 2024 17:23:25 GMT</pubDate>
    </item>
    <item>
      <title>如何在分段平坦数据上训练 ML 模型？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6mzfs/how_to_train_an_ml_model_on_piecewise_flat_data/</link>
      <description><![CDATA[我正在尝试训练一个神经网络，使用加速度和陀螺仪传感器来预测没有 GPS 的速度。用于训练网络的目标速度由 GPS 给出。但是，由于 GPS 的频率，速度变化在约 50 行数据中记录一次，从而形成分段平坦曲线： https://imgur.com/a/nZxceGM 我试图训练网络使用该行中的加速度和陀螺仪信息来计算每行数据期间引起的速度变化，但正因为如此，附近行的速度变化大多为零，从而阻止模型学习任何东西。然后我尝试使用移动平均线： https://imgur.com/a/rFAbaE0 问题是速度现在是分段线性的，因此即使每行的加速度和角速度不同，每 50 行彼此接近的数据的速度变化也是相同的。这会使神经网络混乱，有时会导致过度拟合。我应该如何训练网络？    提交人    /u/InsaneWatchingEye   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6mzfs/how_to_train_an_ml_model_on_piecewise_flat_data/</guid>
      <pubDate>Fri, 18 Oct 2024 16:56:38 GMT</pubDate>
    </item>
    <item>
      <title>尝试使用极不平衡的数据集（非欺诈与欺诈实例的比例为 1:47,500）构建有效的欺诈检测模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6jx90/trying_to_build_an_effective_fraud_detection/</link>
      <description><![CDATA[首先我想说的是，我已经用尽了大多数传统的 ML 方法（逻辑回归、XGBoost 和随机森林分类器）来解决此问题。数据集显然极不平衡，因此需要更复杂的方法。 我还想说，我实际上还没有在比例为 1:47.500 的数据集上测试过这些算法。我只在比例为 1:550 的公开数据集上测试过它们。在我开始使用采样方法（SMOTE 系列）之前，指标是公平的。使用这些方法后，结果看起来太好了，我得出结论，这是过度拟合的结果。 无论如何，我有一些想法，但我只是想看看是否有人遇到过这个问题，现在有他们想分享的解决方案。这真的能帮助我避免即将经历的所有压力。我目前的想法是：  在 cGAN 和 SDG-GAN 等合成数据生成方法上使用 MLP 算法。 也在 cGAN 和 SDG-GAN 生成的数据上使用前面提到的传统 ML 方法（用于比较）。 采用基于图的方法（目前我对此了解甚少）。  我真的很感激对此的帮助，因为它已经困扰我有一段时间了。感谢所有回复者。    提交人    /u/Blvckprier   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6jx90/trying_to_build_an_effective_fraud_detection/</guid>
      <pubDate>Fri, 18 Oct 2024 14:52:08 GMT</pubDate>
    </item>
    <item>
      <title>开发人工植物生命模拟是一个好的项目想法吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6hdcf/is_developing_an_simulation_for_artificial_plant/</link>
      <description><![CDATA[我最近在 Youtude 上发现了一系列关于他们开发人工生命的视频，名为 The Bibites，我觉得非常有趣。我想知道这是否也适用于其他生命形式，例如植物生命（植物在不同环境压力下会如何进化）可能是我的一个很好的长期个人项目。请随时提供任何建议/想法/批评。谢谢！     提交人    /u/nookyto   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6hdcf/is_developing_an_simulation_for_artificial_plant/</guid>
      <pubDate>Fri, 18 Oct 2024 12:54:59 GMT</pubDate>
    </item>
    <item>
      <title>决策树的计算复杂性⌛：了解决策树在输入大小增加时的表现。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6fwdz/computational_complexity_of_decision_trees_learn/</link>
      <description><![CDATA[      https://preview.redd.it/i8l2fibx3ivd1.png?width=900&amp;format=png&amp;auto=webp&amp;s=bcde0246193635a2bd8990abbcd3f0704bd73595    提交人    /u/Amitchejara   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6fwdz/computational_complexity_of_decision_trees_learn/</guid>
      <pubDate>Fri, 18 Oct 2024 11:35:18 GMT</pubDate>
    </item>
    <item>
      <title>微软发布适用于 1 位 LLM 的 BitNet.cpp</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6evbc/microsoft_bitnetcpp_for_1_bit_llms_released/</link>
      <description><![CDATA[BitNet.cpp 是运行和加载论文“1 位 LLM 时代”中的 1 位 LLM 的官方框架，即使在 CPU 中也能运行巨大的 LLM。该框架目前支持 3 种模型。您可以在此处查看其他详细信息：https://youtu.be/ojTGcjD5x58?si=K3MVtxhdIgZHHmP7    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6evbc/microsoft_bitnetcpp_for_1_bit_llms_released/</guid>
      <pubDate>Fri, 18 Oct 2024 10:30:05 GMT</pubDate>
    </item>
    <item>
      <title>从事机器学习工作真的需要硕士学位吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6duvh/does_working_in_ml_really_need_master_degree/</link>
      <description><![CDATA[  由    /u/gggsss119  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6duvh/does_working_in_ml_really_need_master_degree/</guid>
      <pubDate>Fri, 18 Oct 2024 09:15:53 GMT</pubDate>
    </item>
    <item>
      <title>8 到 12 个月内成为 AI 工程师的路线图（从零开始）。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g6d4cz/roadmap_to_becoming_an_ai_engineer_in_8_to_12/</link>
      <description><![CDATA[大家好！ 我刚刚开始攻读电子与通信工程 (ECE) 的机械工程/技术硕士学位，我的目标是在未来 8 到 12 个月内转型为人工智能工程师。我从零开始，但每天可以投入 6 到 8 个小时来学习和构建项目。我正在寻找一份详细的路线图，以及在此过程中要构建的项目想法、任何相关的黑客马拉松、实习和其他可以帮助我实现这一目标的机会。 如果有人经历过这段旅程或目前正在走类似的道路，我很想听听你对以下方面的见解：  学习路线图——我应该每个月关注什么？ 项目——我可以构建哪些现实世界的 AI 项目来提高我的技能？ 黑客马拉松——我在哪里可以找到专注于 AI/ML 的黑客马拉松？ 实习/机会——关于在哪里寻找与 AI 相关的实习或兼职机会的任何建议？  非常感谢任何资源、建议或经验分享。提前谢谢！😊   由    /u/Massive-Medium-4174  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g6d4cz/roadmap_to_becoming_an_ai_engineer_in_8_to_12/</guid>
      <pubDate>Fri, 18 Oct 2024 08:18:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>