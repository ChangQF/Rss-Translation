<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Wed, 18 Sep 2024 03:18:45 GMT</lastBuildDate>
    <item>
      <title>脑电图数据增强</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fjha8h/eeg_data_augmentations/</link>
      <description><![CDATA[嗨，我尝试了 10 种不同的 EEG 数据增强方法，对异常/正常 EEG 进行分类。我将训练集大小减少到 0.3，以在低数据范围内测试增强方法。所有增强方法均无效，我运行了很多次，可能超过 500 次实验，但测试准确率从未超过 82%（即使没有增强，准确率也算在内），我感觉自己一次又一次陷入局部最小值。有什么解释/解决方案吗？    提交人    /u/Agreeable_Fig9423   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fjha8h/eeg_data_augmentations/</guid>
      <pubDate>Wed, 18 Sep 2024 01:40:50 GMT</pubDate>
    </item>
    <item>
      <title>CGP 可用性问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fjgwen/cgp_availability_issues/</link>
      <description><![CDATA[过去两天，我无法在 GCP 中启动带有 L4 gpu 的 g2 实例。这只是我的问题？我在欧盟和美国尝试过    提交人    /u/hmater   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fjgwen/cgp_availability_issues/</guid>
      <pubDate>Wed, 18 Sep 2024 01:21:55 GMT</pubDate>
    </item>
    <item>
      <title>图像分割</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fjgpuw/image_segmentation/</link>
      <description><![CDATA[大家好！ 我正在寻找专门针对服装数据集进行微调的预训练图像分割模型，用于商业用途。我在 Hugging Face 上找到了一个使用 NVIDIA 的 SegFormer 的模型，该模型仅限于非商业应用。有人知道任何适合商业用途的替代预训练模型吗？    提交人    /u/tinkerpal   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fjgpuw/image_segmentation/</guid>
      <pubDate>Wed, 18 Sep 2024 01:13:22 GMT</pubDate>
    </item>
    <item>
      <title>寻求技术创业者关于自学机器学习的建议</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fjfzhr/seeking_advice_on_selflearning_machine_learning/</link>
      <description><![CDATA[大家好， 我正在寻找有关如何有效地自学机器学习的建议。我曾经是一名小企业主，但我的背景和专业与机器学习无关。我现在的目标是转型为技术创业，我意识到要达到这一点我还有很多东西要学。 我的目标不是成为一名专业的数据科学家，也不是为机器学习研究的前沿做出贡献。相反，我想对机器学习有扎实的理解，以便将其应用于现实世界的商业机会。 目前，我有一些自学 Python 的经验，并且从基础级大学课程中对微积分、矩阵和统计学有了基本的了解。我还阅读了一些关于神经网络的书籍并完成了一些相关项目。 从我现在的水平开始，您会推荐哪些必要步骤来对机器学习有一个不错的理解？甚至开始这条路都需要硕士学位吗？任何建议或资源都将不胜感激。 提前感谢您的帮助！    提交人    /u/WaffleSQQ   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fjfzhr/seeking_advice_on_selflearning_machine_learning/</guid>
      <pubDate>Wed, 18 Sep 2024 00:39:44 GMT</pubDate>
    </item>
    <item>
      <title>学习是为了更好，不只是为了工作</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fjcmlg/studying_to_be_better_not_just_for_job/</link>
      <description><![CDATA[我想成为更好的 ML 工程师，我曾在 jupyter notebook 中使用过算法和神经网络，但现在我想做的不止这些，我希望能够在 notebook 之外做一些事情。 我学习了 tensorflow 并做了一些作业，然后开始学习 nlp 因为我觉得每个人都知道这个。 当我在谷歌上搜索下一步要学习什么时，有太多的建议，很难知道从哪里开始以及按照什么顺序学习，我希望找到一些 Ai/ML 主题的顺序，以及人们通常如何学习和找到要做的项目。 人们是否也会边全职工作边学习以跟上技术更新？你学习几个小时？    提交人    /u/AdZealousideal7170   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fjcmlg/studying_to_be_better_not_just_for_job/</guid>
      <pubDate>Tue, 17 Sep 2024 22:15:57 GMT</pubDate>
    </item>
    <item>
      <title>了解 DvC 但寻找替代方案</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fja6kf/learning_about_dvc_but_looking_for_alternatives/</link>
      <description><![CDATA[我最近学习了一个关于 DVC 的精彩教程 (https://dvc.org/)。我真的很喜欢它的管道功能，其中每个阶段都由 yaml 文件和相关配置参数定义，并且它会根据生成的图表中的变化自动知道要运行什么。 但是由于各种原因，我不想要与 git 紧密耦合的东西。所以基本上，寻找具有类似功能但不需要 git 的工具。我隐约知道有各种 ML 管道工具，所以我假设这是可能的。谢谢！    提交人    /u/QuasiEvil   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fja6kf/learning_about_dvc_but_looking_for_alternatives/</guid>
      <pubDate>Tue, 17 Sep 2024 20:39:08 GMT</pubDate>
    </item>
    <item>
      <title>健全性检查句子-BERT</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fj8d1m/sanity_check_sentencebert/</link>
      <description><![CDATA[大家好！ 我使用 Sentence-BERT 来查找句子相似度，方法是遍历句子并根据它们与另一篇文档中的句子的接近程度分配数字分数。这会产生一组相似度分数和每个查询对应的最相似句子。最终目标是找出句子的“新颖性”。这一切都非常简单，现在对我来说效果很好。 问题出现在将论文与存储库进行比较时。在原始论文（Reimers, N., &amp; Gurevych, I. (2019, 11). Sentence-bert: 使用暹罗 bert 网络的句子嵌入。在 2019 年自然语言处理经验方法会议论文集。计算语言学协会。取自 https://arxiv.org/abs/1908.10084 ）中，我发现了以下描述（第 3984 页）： 处理 img rvl6a31q0fpd1... 使用 Python 实现和相应的存储库（https://www.sbert.net/examples/applications/cross-encoder/README.html) 我发现了这个例子： 处理 img 9iu2wngc1fpd1... 该 repo 还指出了以下内容：  &quot;Bi-Encoders 为给定的句子生成一个句子嵌入。我们将句子 A 和 B 分别传递给 BERT，从而得到句子嵌入 u 和 v。然后可以使用余弦相似度比较这些句子嵌入&quot;  这对我来说很清楚，但它也表明：  &quot;相反，对于 Cross-Encoder，我们将两个句子同时传递给 Transformer 网络。然后它产生一个介于 0 和 1 之间的输出值，表示输入句子对的相似性：Cross-Encoder 不会产生句子嵌入。此外，我们无法将单个句子传递给 Cross-Encoder。&quot;  这对我来说是违反直觉的。我以为我们正在像论文中的图 1 那样传递句子。对我来说，论文写得很清楚，两种变体都会产生句子嵌入，但 Cross-Encoder 会采用元素差异，然后使用 softmax 来训练权重，以找到句子相似的概率。然后我在实现中使用这些预先训练的权重。Cross-Encoder 不会输出两个句子嵌入，但实际上，它使用句子嵌入在嵌入空间中产生距离度量。 下一个令人困惑的部分是，repo 的右图表明该过程的输出是二进制的。我第一次看到这个时，以为它想说 {0,1}，但事实并非如此。我想要的和我使用它时得到的都是 [0,1] 之间的数字。 我忽略了什么吗？！任何帮助我都会非常感谢。也许我没有理解一些重要的事情，我对此感到不舒服。    提交人    /u/Lazy_Price3593   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fj8d1m/sanity_check_sentencebert/</guid>
      <pubDate>Tue, 17 Sep 2024 19:28:33 GMT</pubDate>
    </item>
    <item>
      <title>这样的学习曲线可能有什么解释吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fj7fp3/possible_explanations_for_a_learning_curve_like/</link>
      <description><![CDATA[        提交人    /u/nvs93   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fj7fp3/possible_explanations_for_a_learning_curve_like/</guid>
      <pubDate>Tue, 17 Sep 2024 18:53:19 GMT</pubDate>
    </item>
    <item>
      <title>如何在 Pytorch 中在所有时间序列上滑动窗口？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fj6paf/how_to_slide_a_window_on_all_timeserie_in_pytorch/</link>
      <description><![CDATA[嗨， 我目前正在尝试了解如何使用 Pytorch 使用 LSTM 进行时间序列预测。我基本上有 50 个长度为 100 的时间序列，每个时间序列都有 10 个输入，我想将它们用于学习。我还想使用长度为 5 的序列，以便使用最后 5 个值来预测下一个值。现在，我基本上以以下方式重塑我的输入张量：  包含所有形状为 (50, 100, 10) 的时间序列的 Numpy 数组 (时间序列、时间步长、输入)-&gt;形状为 (1000, 5, 10) 的张量 (batch_size、sequence_length、input_size)  这种方法似乎可以粗略地工作，但从技术上讲，它仅使用可用数据的一小部分，因为我将时间序列拆分为较小的窗口而不重叠。当使用神经网络评估新的时间序列时，这也会出现问题，因为缺乏重叠会导致预测不连续。 我该如何解决这个问题？我原本想在时间序列上滑动窗口而不是重塑，但我不知道如何进行…… 编辑：拼写错误    提交人    /u/Heimdell_Irsei   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fj6paf/how_to_slide_a_window_on_all_timeserie_in_pytorch/</guid>
      <pubDate>Tue, 17 Sep 2024 18:24:28 GMT</pubDate>
    </item>
    <item>
      <title>适用于加载深度学习模型并运行推理的 Python 应用程序的廉价但性能不错的云托管平台？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fj5q7d/cheap_but_decent_performance_cloud_hosting/</link>
      <description><![CDATA[我是一名大二学生，目前正在从事一个由人工智能驱动的移动应用程序项目。  我的计划是使用 Flutter 或 Kotlin（我最熟悉的两种语言）创建一个前端，然后使用 FastAPI 创建一个 Python 应用程序后端，并将其部署在云托管平台上，以便随时调用。 因此，这个 Python 应用程序加载了一个图像识别模型（大约 200mb）并使用 Google Vision API 进行 OCR（我为该服务付费）。 您建议在哪个云部署平台上部署我的 Python 应用程序，该平台以合理的价格提供不错的性能和速度？ 此外，如果您对我的架构提出建议或意见，我们将不胜感激。谢谢！    提交人    /u/AppropriateWork4011   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fj5q7d/cheap_but_decent_performance_cloud_hosting/</guid>
      <pubDate>Tue, 17 Sep 2024 17:45:37 GMT</pubDate>
    </item>
    <item>
      <title>解释随机森林和xgboost</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fj0azj/explain_random_forest_and_xgboost/</link>
      <description><![CDATA[我知道这些模型被称为 bagging 模型，本质上是将数据分成子集并在这些子集上进行训练。我更想知道它背后的统计数据和现实世界的应用。 听起来你想用不同的参数和不同的子集构建许多这样的模型（例如 100 个），然后多次运行它们（再次运行 100 次），然后对结果进行概率分析。 这听起来对吗，还是我错了？    提交人    /u/Legal-Yam-235   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fj0azj/explain_random_forest_and_xgboost/</guid>
      <pubDate>Tue, 17 Sep 2024 14:12:19 GMT</pubDate>
    </item>
    <item>
      <title>机器学习的微积分变体</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fizh6k/calculus_variation_for_ml/</link>
      <description><![CDATA[      大家好！我正在学习 Bishop 的《深度学习和基础概念》中的机器学习。我看到了这个页面 (51)，其中解释了使用变化计算函数最大熵的示例。不幸的是，尽管我阅读了引用的附录 B，但我还是无法理解。有人能帮助我吗？非常感谢！    提交人    /u/ArlingtonBeech343   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fizh6k/calculus_variation_for_ml/</guid>
      <pubDate>Tue, 17 Sep 2024 13:37:24 GMT</pubDate>
    </item>
    <item>
      <title>在家用电脑上运行 LLM：Llama 3.1 70B，压缩 6.4 倍，大小 22 GB</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fisec0/run_an_llm_on_your_home_pc_llama_31_70b/</link>
      <description><![CDATA[大家好！想分享一些可能有助于您了解和试验 LLM 的内容。最近，我们使用 PV-Tuning 方法成功压缩了 Llama 3.1 70B 和 Llama 3.1 70B Instruct。 结果如下： - 压缩率：6.4 倍（从 141 GB 到 22 GB） - 质量保留：Llama 3.1-70B（MMLU 0.78 -&gt; 0.73），Llama 3.1-70B Instruct（MMLU 0.82 -&gt; 0.78） 我们实际上对 Llama 3.1 8B 模型做了同样的事情。根据[此](https://blacksamorez.substack.com/p/aqlm-executorch-android?r=49hqp1&amp;utm\_campaign=post&amp;utm\_medium=web&amp;triedRedirect=true) 工作证明，它现在可以在 RAM 少于 2.5 GB 的 Android 上运行。因此，您现在可以离线部署它，而无需共享您的数据。  您可以在此处找到结果并下载压缩模型： https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16 https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-8B-AQLM-PV-2Bit-1x16-hf https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-8B-Instruct-AQLM-PV-2Bit-1x16-hf    提交人    /u/azalio   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fisec0/run_an_llm_on_your_home_pc_llama_31_70b/</guid>
      <pubDate>Tue, 17 Sep 2024 06:57:47 GMT</pubDate>
    </item>
    <item>
      <title>学习机器学习的书籍</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fipadd/books_to_learn_machine_learning/</link>
      <description><![CDATA[这篇帖子是我对 reddit 不让我评论某人帖子的报复。他们是一名想要学习机器学习的物理学毕业生。因此，这些建议适用于已经具备扎实数学基础（熟悉并能解决线性代数和概率论问题）的人。 机器学习领域分为许多领域，但最突出的是深度学习、计算机视觉和自然语言处理。如果您想要深入研究某个特定领域，我或其他人肯定可以提供更具体的建议，话虽如此，已经有一些通用书籍出版，旨在涵盖AI（人工智能）的广度，其中最好的两本是-  深度学习（自适应计算和机器学习系列）：Goodfellow，Ian，Bengio，Yoshua，Courville，Aaron：9780262035613：Amazon.com：图书。 （电子书可从此处免费获取） 人工智能：一种现代方法（pearson.com）  这两本书试图涵盖人工智能领域的所有内容。虽然第一本书能让你真正理解和欣赏人工智能创新背后的启发式和直觉思维，但第二本书只会让你意识到人工智能思维的起源，可以追溯到亚里士多德。 现在，以上两本书都不会给你实践课程，我不推荐“实践”书籍。事实上，机器学习算法非常容易实现，只需几行 python/c++（一个算法可能需要 10 行到 100 行代码 - 无论如何都不是很多）。所以，一个好的策略是先学习 python（如果你还没有）-&gt;了解该领域并学习数学（并行），然后在学习 pytorch 的同时实现每个算法。既然你已经了解数学，我建议你阅读深度学习（自适应计算和机器学习系列）：Goodfellow，Ian，Bengio，Yoshua，Courville，Aaron：9780262035613：Amazon.com：图书。 （电子书免费这里）和/或我下面要提到的书籍。 下面的书都不是废话，只是数学和可视化书籍，作为物理学毕业生，你可能很容易理解。  计算智能：方法论介绍 | SpringerLink（我最喜欢的一本介绍神经网络、进化算法和模糊逻辑的书）。 （免费）模式识别和机器学习 (microsoft.com)（广受好评的“统计学习方法”书籍。 （免费）深入学习 — 深入学习 1.0.3 文档 (d2l.ai)（迄今为止学习深度学习的最佳书籍）。它有理论和代码）。  基于相关领域的其他书籍：  计算机视觉：算法和应用，第二版。 （szeliski.org） 计算机视觉：一种现代方法：Forsyth，David，Ponce，Jean：9780136085928：Amazon.com：图书  （注：CV（我认为通过视频课程学习计算机视觉效果更好）  统计自然语言处理基础（stanford.edu） 强化学习（mit.edu）     由   提交  /u/reacher1000   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fipadd/books_to_learn_machine_learning/</guid>
      <pubDate>Tue, 17 Sep 2024 03:56:40 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>