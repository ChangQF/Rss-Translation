<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Thu, 31 Oct 2024 01:16:45 GMT</lastBuildDate>
    <item>
      <title>Xgboost 输出是“分箱的”还是像逻辑回归一样的滑动比例？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gg1ho9/are_xgboost_outputs_binned_or_is_it_a_sliding/</link>
      <description><![CDATA[我想建立一个 Xgboost 二元分类模型，但我希望输出（概率）是像逻辑回归那样的连续滑动“尺度”。Xgboost 输出是这样的吗？还是概率是根据观察结果所属的叶子“分箱”的？    提交人    /u/learning_proover   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gg1ho9/are_xgboost_outputs_binned_or_is_it_a_sliding/</guid>
      <pubDate>Thu, 31 Oct 2024 00:10:55 GMT</pubDate>
    </item>
    <item>
      <title>为什么 2 * relu(x) + sin(x) 优于 ReLU 及其变体？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gg16w3/why_does_2_relux_sinx_outperform_relu_and_its/</link>
      <description><![CDATA[我刚刚尝试使用随机激活函数在 MNIST 上训练神经网络，并随机尝试了 2 * relu(x) + sin(x)，损失下降的速度比 ReLU、GeLU 等快得多。我在其他一些数据集上进行了训练，结果差别并不大，但它仍然比我尝试过的所有 ReLU 变体都要好。这对我来说没有多大意义。为了比较： relu(x) + sin(x) 网络 Epoch [1/20], 损失：0.3950 Epoch [2/20], 损失：0.1650 Epoch [3/20], 损失：0.1207 Epoch [4/20], 损失：0.0970 Epoch [5/20], 损失：0.0801 Epoch [6/20], 损失：0.0679 Epoch [7/20], 损失：0.0574 Epoch [8/20], 损失：0.0497 Epoch [9/20], 损失：0.0433 Epoch [10/20], 损失：0.0369 Epoch [11/20], 损失：0.0321纪元[12/20]，损失：0.0268纪元[13/20]，损失：0.0232纪元[14/20]，损失：0.0199纪元[15/20]，损失：0.0174纪元[16/20]，损失：0.0150纪元[17/20]，损失：0.0123纪元[18/20]，损失：0.0121纪元[19/20]，损失：0.0091纪元[20/20]，损失：0.0069 relu（x）+ sin（x）准确度：97.70％abs网络纪元[1/20]，损失：1.3862纪元[2/20]，损失： 0.3914 纪元 [3/20]，损失：0.2589 纪元 [4/20]，损失：0.1892 纪元 [5/20]，损失：0.1494 纪元 [6/20]，损失：0.1230 纪元 [7/20]，损失：0.1067 纪元 [8/20]，损失：0.0929 纪元 [9/20]，损失：0.0813 纪元 [10/20]，损失：0.0723 纪元 [11/20]，损失：0.0648 纪元 [12/20]，损失：0.0581 纪元 [13/20]，损失：0.0527 纪元 [14/20]，损失： 0.0461 Epoch [15/20]，损失：0.0418 Epoch [16/20]，损失：0.0380 Epoch [17/20]，损失：0.0338 Epoch [18/20]，损失：0.0302 Epoch [19/20]，损失：0.0276 Epoch [20/20]，损失：0.0254 绝对准确率：97.61% ReLU 网络 Epoch [1/20]，损失：2.0522 Epoch [2/20]，损失：0.6355 Epoch [3/20]，损失：0.4122 Epoch [4/20]，损失：0.3446 Epoch [5/20]，损失： 0.2903 纪元 [6/20]，损失：0.2431 纪元 [7/20]，损失：0.2079 纪元 [8/20]，损失：0.1806 纪元 [9/20]，损失：0.1595 纪元 [10/20]，损失：0.1412 纪元 [11/20]，损失：0.1275 纪元 [12/20]，损失：0.1170 纪元 [13/20]，损失：0.1068 纪元 [14/20]，损失：0.0978 纪元 [15/20]，损失：0.0910 纪元 [16/20]，损失：0.0838 纪元 [17/20]，损失：0.0776 Epoch [18/20]，损失：0.0728 Epoch [19/20]，损失：0.0676 Epoch [20/20]，损失：0.0631 ReLU 准确度：97.00％Leaky ReLU 网络 Epoch [1/20]，损失：2.2401 Epoch [2/20]，损失：0.9840 Epoch [3/20]，损失：0.4971 Epoch [4/20]，损失：0.3574 Epoch [5/20]，损失：0.2902 Epoch [6/20]，损失：0.2414 Epoch [7/20]，损失：0.2057 Epoch [8/20]，损失： 0.1788 纪元 [9/20]，损失：0.1576 纪元 [10/20]，损失：0.1406 纪元 [11/20]，损失：0.1261 纪元 [12/20]，损失：0.1153 纪元 [13/20]，损失：0.1058 纪元 [14/20]，损失：0.0976 纪元 [15/20]，损失：0.0901 纪元 [16/20]，损失：0.0833 纪元 [17/20]，损失：0.0771 纪元 [18/20]，损失：0.0722 纪元 [19/20]，损失：0.0687 纪元[20/20]，损失：0.0633 Leaky ReLU 准确度：97.36% GeLU 网络 Epoch [1/20]，损失：2.2979 Epoch [2/20]，损失：1.9896 Epoch [3/20]，损失：0.6773 Epoch [4/20]，损失：0.4036 Epoch [5/20]，损失：0.3152 Epoch [6/20]，损失：0.2631 Epoch [7/20]，损失：0.2269 Epoch [8/20]，损失：0.1966 Epoch [9/20]，损失：0.1757 Epoch [10/20]，损失：0.1580 Epoch [11/20]，损失：0.1437 Epoch [12/20]，损失：0.1319 Epoch [13/20]，损失：0.1199 Epoch [14/20]，损失：0.1118 Epoch [15/20]，损失：0.1030 Epoch [16/20]，损失：0.0957 Epoch [17/20]，损失：0.0892 Epoch [18/20]，损失：0.0837 Epoch [19/20]，损失：0.0787 Epoch [20/20]，损失：0.0736 GeLU 准确率：97.13% SiLU 网络 Epoch [1/20]，损失：2.3007 Epoch [2/20]，损失：2.2903 纪元 [3/20]，损失：1.8074 纪元 [4/20]，损失：0.5913 纪元 [5/20]，损失：0.3984 纪元 [6/20]，损失：0.3400 纪元 [7/20]，损失：0.3060 纪元 [8/20]，损失：0.2805 纪元 [9/20]，损失：0.2568 纪元 [10/20]，损失：0.2344 纪元 [11/20]，损失：0.2152 纪元 [12/20]，损失：0.1980 纪元 [13/20]，损失：0.1816 纪元[14/20]，损失：0.1673 Epoch [15/20]，损失：0.1555 Epoch [16/20]，损失：0.1459 Epoch [17/20]，损失：0.1357 Epoch [18/20]，损失：0.1277 Epoch [19/20]，损失：0.1198 Epoch [20/20]，损失：0.1130 SiLU 准确率：96.30%     提交人    /u/InsaneWatchingEye   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gg16w3/why_does_2_relux_sinx_outperform_relu_and_its/</guid>
      <pubDate>Wed, 30 Oct 2024 23:57:24 GMT</pubDate>
    </item>
    <item>
      <title>2024 年的深度学习：持续洞察与策略 – 第 1 天</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfz20g/deep_learning_in_2024_continued_insights_and/</link>
      <description><![CDATA[  由    /u/Potential_Arrival326  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfz20g/deep_learning_in_2024_continued_insights_and/</guid>
      <pubDate>Wed, 30 Oct 2024 22:21:44 GMT</pubDate>
    </item>
    <item>
      <title>神经元偏差和标量偏差之间的差异</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfyl35/difference_between_neuronal_biases_and_scalar/</link>
      <description><![CDATA[或者 chatgpt 只是编造了这个？在为学校做一个项目，主要是基础的东西，想知道将偏差表示为额外的神经元或仅仅是添加的值之间是否存在区别，如果这有意义的话。有人可以帮帮我吗？    提交人    /u/doodoodoododoo   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfyl35/difference_between_neuronal_biases_and_scalar/</guid>
      <pubDate>Wed, 30 Oct 2024 22:01:04 GMT</pubDate>
    </item>
    <item>
      <title>Mac 统一内存对 AI 推理更有利？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfydjl/macs_unified_memory_is_superior_for_ai_reasoning/</link>
      <description><![CDATA[有人声称 Apple 的统一内存在 AI 推理方面更胜一筹、速度更快，因为它拥有大量内存，可以访问高达 192GB，而即使 RTX 4090 也只能拥有高达 24GB 的 VRAM。我知道 Apple Silicon 芯片的 GPU 性能并不好，带宽实际上也不是那么快，但内存量似乎更胜一筹。 但是有人可以用 M3 Max 和 RTX 4090m 进行 AI 训练和推理测试吗？我很好奇性能差异。    提交人    /u/Neat-Appointment-950   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfydjl/macs_unified_memory_is_superior_for_ai_reasoning/</guid>
      <pubDate>Wed, 30 Oct 2024 21:52:00 GMT</pubDate>
    </item>
    <item>
      <title>训练模型——本地还是云端？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfy94p/training_modelslocal_or_cloud/</link>
      <description><![CDATA[我正在学习机器学习理论，计划将其应用于 Python 和相关库。我还考虑在大约半年后攻读 AI/ML 硕士学位。 对于小数据集和机器学习应用，有没有经济实惠的基于云的选项？我认为没有必要依赖本地环境进行机器学习学习，对吗？    提交人    /u/Fair-Manufacturer456   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfy94p/training_modelslocal_or_cloud/</guid>
      <pubDate>Wed, 30 Oct 2024 21:46:32 GMT</pubDate>
    </item>
    <item>
      <title>这是机器学习课程的课程大纲。有人知道这门课程有多好吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfqdsg/this_is_the_course_outline_of_the_machine/</link>
      <description><![CDATA[      标题：完整的人工智能机器学习和数据科学：从零到精通    提交人    /u/pilo_lo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfqdsg/this_is_the_course_outline_of_the_machine/</guid>
      <pubDate>Wed, 30 Oct 2024 16:15:19 GMT</pubDate>
    </item>
    <item>
      <title>这个在线课程好吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfpsw6/how_good_is_this_online_course/</link>
      <description><![CDATA[        提交人    /u/pilo_lo   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfpsw6/how_good_is_this_online_course/</guid>
      <pubDate>Wed, 30 Oct 2024 15:51:14 GMT</pubDate>
    </item>
    <item>
      <title>人工智能代理和模块化数据堆栈的强大组合：推理型人工智能</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfomtb/the_power_combo_of_ai_agents_and_the_modular_data/</link>
      <description><![CDATA[        由    /u/growth_man 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfomtb/the_power_combo_of_ai_agents_and_the_modular_data/</guid>
      <pubDate>Wed, 30 Oct 2024 15:02:26 GMT</pubDate>
    </item>
    <item>
      <title>您应该使用哪些 AI/ML 模型以及为什么？ - Jozu MLOps</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfmrhr/what_aiml_models_should_you_use_and_why_jozu_mlops/</link>
      <description><![CDATA[        由    /u/iamjessew 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfmrhr/what_aiml_models_should_you_use_and_why_jozu_mlops/</guid>
      <pubDate>Wed, 30 Oct 2024 13:39:36 GMT</pubDate>
    </item>
    <item>
      <title>我构建了一个人工智能来帮助企业直接与其数据交互——以下是我学到的东西</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfjmqd/i_built_an_ai_to_help_businesses_interact/</link>
      <description><![CDATA[大家好！我一直在研究一个名为 Cells AI 的项目，该项目使用 NLP 使企业更容易获取数据。目标是让用户直接从他们的数据中提出问题，例如“上个月我们最畅销的产品是什么？”并立即得到答案 - 无需手动数据分析。 通过这个项目，我一直在尝试各种 NLP 和 ML 技术来实现自然语言查询。这是一次令人难以置信的学习经历，它让我思考如何应用 ML 来弥合复杂数据与可能不具备技术技能的日常业务用户之间的差距。 如果有人感兴趣，我会制作一个演示来展示它的工作原理。 很高兴在评论中分享。  我也很想听听其他从事类似项目或学习 ML 的人的意见——到目前为止，您最有趣的应用程序是什么？    提交人    /u/Maleficent-Tear7949   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfjmqd/i_built_an_ai_to_help_businesses_interact/</guid>
      <pubDate>Wed, 30 Oct 2024 10:48:39 GMT</pubDate>
    </item>
    <item>
      <title>批评我的简历（并提出改进建议）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfim5f/roast_my_resume_and_suggest_improvements/</link>
      <description><![CDATA[        提交人    /u/xayushman   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfim5f/roast_my_resume_and_suggest_improvements/</guid>
      <pubDate>Wed, 30 Oct 2024 09:37:45 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle、项目还是认证？数据科学实习最重要的是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfievm/kaggle_projects_or_certifications_what_matters/</link>
      <description><![CDATA[对于那些有招聘或面试入门级数据科学实习经验的人：候选人的个人资料中真正突出的是什么？我正试图通过平衡几件事来充分利用我有限的时间——建立一个有意义的 Kaggle 个人资料（周到的笔记本、高质量的贡献）、从事个人项目、完成在线课程和追求认证（Hackerrank）。根据您的经验，这些元素中的哪一个给人留下了最深刻的印象？我应该如何优先安排我的时间，以获得获得实习机会的最佳机会？    提交人    /u/ds_reddit1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfievm/kaggle_projects_or_certifications_what_matters/</guid>
      <pubDate>Wed, 30 Oct 2024 09:22:05 GMT</pubDate>
    </item>
    <item>
      <title>Meta FAIR CodeGen 团队的新 RL 实习机会</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gfhs27/new_rl_internship_at_meta_fair_codegen_team/</link>
      <description><![CDATA[        提交人    /u/bulgakovML   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gfhs27/new_rl_internship_at_meta_fair_codegen_team/</guid>
      <pubDate>Wed, 30 Oct 2024 08:31:58 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>