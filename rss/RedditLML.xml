<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Fri, 17 May 2024 21:12:51 GMT</lastBuildDate>
    <item>
      <title>初学者</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cug043/beginner/</link>
      <description><![CDATA[作为人工智能初学者该学什么，最重要的是我数学不好   由   提交 /u/philippesay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cug043/beginner/</guid>
      <pubDate>Fri, 17 May 2024 20:52:54 GMT</pubDate>
    </item>
    <item>
      <title>烤我的简历</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cufjes/roast_my_cv/</link>
      <description><![CDATA[      遭到了很多拒绝，所以思考我错了什么以及为什么我没有资格实习。还想知道 DS 是否值得，或者我应该转向其他东西吗？我也来自波兰 https： //preview.redd.it/lg7yt9pkr11d1.png?width=732&amp;format=png&amp;auto=webp&amp;s=80e62b507ff4256b2524d5f68197d3d23b0e162c    ;由   提交/u/Ok_Produce_9055   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cufjes/roast_my_cv/</guid>
      <pubDate>Fri, 17 May 2024 20:32:51 GMT</pubDate>
    </item>
    <item>
      <title>资源太多了，现在该关注哪个，很困惑</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cudjn9/plethora_of_resources_which_to_follow_now_very/</link>
      <description><![CDATA[让我简单介绍一下我的情况 -我通过 andrew ng 的 ML 入门课程开始学习机器学习 (3系列） -完成了第一门课程，目前正在学习关于神经网络、张量流实现等的第二门课程。 -我遇到了 Aurelien Geron 的 Hands On ML，它非常有趣 -我在 fast.ai ML 课程中了解了实际应用，而这在 Andrew ng 的课程中​​是非常缺失的 我对所有这些资源感到非常不知所措 我需要什么 - 您对现在如何进行的意见，参考什么 - 书籍或快速人工智能课程等。就像我应该首先阅读预订我学到的所有内容以便更好地理解，然后继续进行或同时进行？  编辑 - 我还没有制作任何项目（只是按照 YouTube 视频使用 sklearn 对加州住房数据集实现线性回归） ~Kay  ~Kay div&gt;  由   提交 /u/Weak_Display1131   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cudjn9/plethora_of_resources_which_to_follow_now_very/</guid>
      <pubDate>Fri, 17 May 2024 19:10:29 GMT</pubDate>
    </item>
    <item>
      <title>K 均值聚类算法的视觉指南。 👥</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cua97t/a_visual_guide_to_the_kmeans_clustering_algorithm/</link>
      <description><![CDATA[      TL;DR：K-Means 聚类根据数据点的相似性将数据点分组到聚类中，这使其对于客户细分等应用非常有用，图像分割和文档聚类。 K 均值聚类可视化指南  https://preview.redd.it/92n1nckko01d1.png?width=936&amp;format=png&amp;auto=webp&amp;s=ae4bfeb8fa4ee1399afc0 3447cbf5bc95563d464    由   提交/u/ml_a_day  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cua97t/a_visual_guide_to_the_kmeans_clustering_algorithm/</guid>
      <pubDate>Fri, 17 May 2024 16:54:48 GMT</pubDate>
    </item>
    <item>
      <title>与最新法学硕士的文本相似度</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu7voj/text_similarity_with_latest_llms/</link>
      <description><![CDATA[假设您有两个文本，您想要定量测量它们传达相同含义的程度，并且您关心微妙的细节，例如内在逻辑是否有意义等这样粗略的旧的和更小的 BERT 模型就不行了。  任何人都可以向我指出最近使用最新的法学硕士（例如 Llama3）进行此类操作的参考文献吗？    由   提交/u/Invariant_apple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu7voj/text_similarity_with_latest_llms/</guid>
      <pubDate>Fri, 17 May 2024 15:18:47 GMT</pubDate>
    </item>
    <item>
      <title>在视频（或文本）上训练人工智能以使用人工智能创建新视频的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu7mcz/what_would_be_the_best_way_to_train_ai_on_videos/</link>
      <description><![CDATA[例如： 假设您有一个电视节目已经结束，并且您想使用 AI 来创建新一季。是否有可能根据过去的季节来训练人工智能来制作新剧集？ 此外，有没有办法通过同人小说写作来做到这一点？ 会是什么？最简单的方法？   由   提交 /u/number001   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu7mcz/what_would_be_the_best_way_to_train_ai_on_videos/</guid>
      <pubDate>Fri, 17 May 2024 15:08:28 GMT</pubDate>
    </item>
    <item>
      <title>[D] LoRA 基础知识和低阶微调</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu7h66/d_fundamentals_of_lora_and_lowrank_finetuning/</link>
      <description><![CDATA[  由    /u/Patrick-239  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu7h66/d_fundamentals_of_lora_and_lowrank_finetuning/</guid>
      <pubDate>Fri, 17 May 2024 15:02:45 GMT</pubDate>
    </item>
    <item>
      <title>如何评价降维算法的好坏？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu7d7l/how_do_you_evaluate_how_good_a_dimensionality/</link>
      <description><![CDATA[我一直在尝试寻找人们如何选择要减少到的维度的方法，但到目前为止我还没有任何运气。我基本上找不到关于这个主题的任何内容，尽管这似乎是一个非常明显的问题。人们是否只是选择任意数量的维度，然后继续使用它？ 对于 PCA，有解释方差，而且您可以将逆变换应用于减少的数据，然后计算重建误差。但是，一旦您不使用像 PCA 这样的线性东西（因此没有解释的方差）并且使用没有明确/容易获得的逆变换的东西（例如自动编码器和 PCA）。例如，如果您使用 t-SNE、UMAP、isomaps、稀疏/内核/增量 PCA、ICA 等，您将如何评估/理解是否有足够的维度来完全捕获数据集？ 这如何推广到更高级的方法，如嵌入或流形学习？   由   提交 /u/Amun-Aion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu7d7l/how_do_you_evaluate_how_good_a_dimensionality/</guid>
      <pubDate>Fri, 17 May 2024 14:58:42 GMT</pubDate>
    </item>
    <item>
      <title>在我的无限注意力实现中需要 NaN 的帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu702s/need_help_with_nans_in_my_infiniattention/</link>
      <description><![CDATA[大家好， 我目前正在致力于实现来自 本文，但我一直遇到一个问题，即我的实现不断产生 NaN。在循环的第一次迭代中，_update_memory 的 memory 和 norm_term 输出非常大，到下一次迭代时，一切都变成了NaN。我不确定我的代码中是否存在错误，或者 Infini-Attention 本质上不稳定。 这是我当前的实现： class Attention(nn.Module) : def __init__( self: &quot;Attention&quot;, causal: bool = True, Heads: int = 8, infini: bool = True, segment_len: int = 1024, ) -&gt;无： super().__init__() 断言不是 version.parse(torch.__version__) &lt; version.parse(“2.0.0”),“sdpa 需要 torch&gt;=2.0.0” self.causal = 因果 self.infini = 无限 self.segment_len = segment_len # sdpa 配置 self.cpu_config = _config(True, True, True) if infini: self.gate = nn.Parameter(torch.full((1, 头, 1, 1), -100.0)) 如果不是 torch.cuda.is_available(): return device_properties = torch.cuda.get_device_properties(torch.device(&quot;cuda&quot;)) 如果 device_properties.major == 8 并且 device_properties.minor = = 0: self.cuda_config = _config(True, False, False) else: self.cuda_config = _config(False, True, True) defforward_sdpa( self: “注意力”, q: torch.Tensor, k: torch.Tensor , v: torch.Tensor, ) -&gt; torch.Tensor: is_cuda, dtype = v.is_cuda, v.dtype config = self.cuda_config if is_cuda else self.cpu_config 与 torch.backends.cuda.sdp_kernel(**config._asdict()): q = q.half( ) k = k.half() v = v.half() q, k, v = (t.contigious() for t in (q, k, v)) 比例 = q.shape[-1] ** - 0.5 q = q * 横向扩展 = F.scaled_dot_product_attention( q, k, v, is_causal=self.causal, ) return out.to(dtype) def _retrieve_from_memory( self: &quot;Attention&quot;, q: torch.Tensor, memory:可选[torch.Tensor] = None，norm_term：可选[torch.Tensor] = None，）-&gt; torch.Tensor：如果内存为None或norm_term为None：返回torch.zeros_like（q）q = F.elu（q）+ 1.0内存= torch.matmul（q，内存）norm_term = torch.matmul（q，rearrange（ norm_term, &quot;b 1 1 d -&gt; b 1 d 1&quot;), ) 返回内存 /norm_term def _update_memory( self: &quot;Attention&quot;, k: torch.Tensor, v: torch.Tensor, memory: 可选[torch .Tensor] = None，norm_term：可选[torch.Tensor] = None，）-&gt; torch.Tensor: k = F.elu(k) + 1.0 如果内存不是 None: 内存 = 内存 + torch.matmul(rearrange(k, &quot;b h n d -&gt; b h d n&quot;), v) 否则: 内存 = torch .matmul(rearrange(k, &quot;b h n d -&gt; b h d n&quot;), v) ifnorm_term 不是 None: # noqa: SIM108norm_term =norm_term + k.sum(dim=-2, keepdim=True) else:norm_term = k.sum(dim=-2, keepdim=True) 返回内存，norm_term def front_infini( self: &quot;Attention&quot;, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, ) -&gt; torch.Tensor: n_segments = q.shape[-2] // self.segment_len # 假设序列长度可以被段长度整除 q, k, v = (rearrange(t, &quot;b h (s n) d -&gt; b h s n d&quot; ;, s=n_segments) for t in (q, k, v)) 输出 = [] 内存 = Nonenorm_term = None for idx in range(n_segments): q_segment = q[:, :, idx, :, :] k_segment = k[:, :, idx, :, :] v_segment = v[:, :, idx, :, :] 内存输出 = self._retrieve_from_memory(q_segment, 内存,norm_term) update_memory, Updated_norm_term = self._update_memory( k_segment, v_segment ，内存，norm_term，）内存= Updated_memory.detach（）norm_term = Updated_norm_term.detach（） attn = self.forward_sdpa（q_segment，k_segment，v_segment）combined_output =（F.sigmoid（self.gate）* memory_output）+（1 - F.sigmoid(self.gate)) * attn ports.append(combined_output) out = torch.cat(outputs, dim=-2) return out defforward( self: &quot;Attention&quot;, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, ) -&gt; torch.Tensor: if self.infini: return self.forward_infini(q, k, v) return self.forward_sdpa(q, k, v)  这里有人成功实现了无限注意力吗并让它发挥作用？任何帮助将不胜感激！ 其他上下文：我的数据是一维（某种）时间序列。   由   提交 /u/TheDisturbedBooty   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu702s/need_help_with_nans_in_my_infiniattention/</guid>
      <pubDate>Fri, 17 May 2024 14:44:11 GMT</pubDate>
    </item>
    <item>
      <title>您使用什么软件与本地大语言模型交互？为什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu6dso/what_software_do_you_use_to_interact_with_local/</link>
      <description><![CDATA[您使用 deepchecks、KoboldCpp、LM Studio、PrivateGPT、GPT4All 等吗？ 您喜欢您的解决方案的哪些方面？您使用多个吗？你做RAG吗？您正在做其他人可能认为独特或新颖的事情吗？   由   提交 /u/UpvoteBeast   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu6dso/what_software_do_you_use_to_interact_with_local/</guid>
      <pubDate>Fri, 17 May 2024 14:18:49 GMT</pubDate>
    </item>
    <item>
      <title>自动数据分析Python包要知道</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu4t3o/auto_data_analysis_python_packages_to_know/</link>
      <description><![CDATA[查看此视频教程，探索不同的 AutoEDA python 软件包，如 pandas-profiling、sweetviz、dataprep 等，这些软件包可以在几分钟内轻松实现自动数据分析: https://youtu.be/Z7RgmM4cI2I?si=8GGM50qqlN0lGzry   由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu4t3o/auto_data_analysis_python_packages_to_know/</guid>
      <pubDate>Fri, 17 May 2024 13:11:50 GMT</pubDate>
    </item>
    <item>
      <title>10 门最佳高级机器学习课程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu3r1p/10_best_advanced_machine_learning_courses/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/Aqsa81  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu3r1p/10_best_advanced_machine_learning_courses/</guid>
      <pubDate>Fri, 17 May 2024 12:20:36 GMT</pubDate>
    </item>
    <item>
      <title>有助于可视化神经网络的新平台</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cu20j0/a_new_platform_that_helps_visualising_neural/</link>
      <description><![CDATA[在过去的一年里，我开始致力于这个项目，以帮助简化神经网络的设计和开发。我想在这里分享它，以尽可能帮助新人了解深度学习。您基本上可以直观地设计、构建、训练和部署人工智能网络等等。非常感谢您的反馈和支持！我会让您看一下。 产品搜寻启动：https://www.producthunt.com /posts/neuralhub-beta 网站：https://neuralhub.ai/   由   提交/u/s_n_dev  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cu20j0/a_new_platform_that_helps_visualising_neural/</guid>
      <pubDate>Fri, 17 May 2024 10:38:55 GMT</pubDate>
    </item>
    <item>
      <title>高级 RAG：集成检索器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ctzx3a/advanced_rag_ensemble_retriever/</link>
      <description><![CDATA[嗨， 制作了有关高级 RAG 的视频：Ensemble Retriever。 &lt; p&gt;Ensemble Retriever 同时结合了多种高性能检索技术，使用多数投票和排名来提供强相关段落。 逻辑是：更好的检索段落 == 更好的上下文==更好的一代。 最初来自这篇论文：倒数排名融合优于孔多塞和个人排名学习方法 但是我制作了一个视频来介绍如何将其与带有 GPT-4o 的 Langchain 和 llama Index 一起使用。 希望您发现它有用。 &lt;强&gt;https://youtu.be/s2i4zeWjUtM   由   提交/u/Mosh_98  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ctzx3a/advanced_rag_ensemble_retriever/</guid>
      <pubDate>Fri, 17 May 2024 08:06:18 GMT</pubDate>
    </item>
    <item>
      <title>有没有涵盖这些主题的书籍或课程？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ctuqdo/is_there_any_book_or_courses_that_covers_these/</link>
      <description><![CDATA[       由   提交 /u/itsmekalisyn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ctuqdo/is_there_any_book_or_courses_that_covers_these/</guid>
      <pubDate>Fri, 17 May 2024 02:37:49 GMT</pubDate>
    </item>
    </channel>
</rss>