<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sun, 28 Jan 2024 06:15:40 GMT</lastBuildDate>
    <item>
      <title>文本分类器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acv0vo/text_classifier/</link>
      <description><![CDATA[大家好，我正在尝试构建一个文本分类器。假设我们有大量的单词，我们想将它们分为更广泛的类别。比如猫、狗都会去找动物。自行车、汽车都会去车辆。我已准备好训练集，其中包含与主题相关的单词。但测试集未标记。这意味着我只有单词，没有它们所属的主题。  我的想法是将单词转换为向量并计算它们的相似度分数。但这种方法将无法找到上下文相似性。任何帮助将不胜感激。预先感谢您！   由   提交/u/TheRizzler2306   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acv0vo/text_classifier/</guid>
      <pubDate>Sun, 28 Jan 2024 05:16:29 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用图像制作脑电图情感识别数据集？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acu5vg/is_it_possible_to_make_your_dataset_for_eeg/</link>
      <description><![CDATA[你好，我是一名学生。我从互联网上看到的大多数用于情感识别的数据集都是通过观看视频和其他东西来获得的。对于我这样一个只会基础编程的学生来说，真的很难。有人可以建议我做点什么吗？我的项目是制作一个脑电图设备，只需查看图像即可识别人类情感   由   提交 /u/Feifeichan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acu5vg/is_it_possible_to_make_your_dataset_for_eeg/</guid>
      <pubDate>Sun, 28 Jan 2024 04:27:21 GMT</pubDate>
    </item>
    <item>
      <title>3Blue1Brown 的背面支撑视频是传统上正确的数学计算方法吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1act005/is_3blue1browns_video_on_back_prop_the/</link>
      <description><![CDATA[我正在写一篇关于机器学习的研究论文，目前，我正在研究与之相关的所有数学知识。我想知道他的视频是否是传统上正确的编写方式，因为在 3-4 个视频中，所有视频都有不同的表示和写入变量等的方式。   由   提交 /u/_Stampy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1act005/is_3blue1browns_video_on_back_prop_the/</guid>
      <pubDate>Sun, 28 Jan 2024 03:24:22 GMT</pubDate>
    </item>
    <item>
      <title>让人工智能唱原创歌词（按照既定的曲调）——有可能吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acstad/getting_ai_to_sing_original_lyrics_to_an/</link>
      <description><![CDATA[我喜欢用人工智能翻唱歌曲。我想尝试使用一些人工智能声音来创作原创歌词。让 RVC 模型“唱歌”的最佳方法是什么？原来的歌词是基于已经唱过那首歌的人的歌词（愚蠢的例子，它说“我想要鱼片”而不是“我想要那样”）？有什么办法可以做到这一点吗？ 最明显的方法是自己唱，但我是一个糟糕的歌手，而且我的亲密朋友也没有一个能保持调子。这对我来说只是一个有趣的项目，所以雇用某人是不可能的。  我对 RVC 有一些经验，包括训练语音模型和创建 AI 歌曲封面的整个过程。    由   提交 /u/Rod-Serling-Lives   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acstad/getting_ai_to_sing_original_lyrics_to_an/</guid>
      <pubDate>Sun, 28 Jan 2024 03:14:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么反向传播不总是与 dropout 正则化一起教授。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acs9yx/why_isnt_backpropagation_always_taught_along_with/</link>
      <description><![CDATA[我最近了解了更多关于神经网络中 dropout 正则化的有效性，我不禁觉得 dropout 应该自然地与整个反向传播算法。有什么理由不这样吗？是否存在 dropout 正则化弊大于利的情况？   由   提交 /u/Traditional_Soil5753   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acs9yx/why_isnt_backpropagation_always_taught_along_with/</guid>
      <pubDate>Sun, 28 Jan 2024 02:46:54 GMT</pubDate>
    </item>
    <item>
      <title>了解 VAE 的资源</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acqyiz/resources_to_understand_vaes/</link>
      <description><![CDATA[我“理解”VAE 是如何工作的，我已经阅读了很多关于它们的内容，但是当我尝试阅读原始论文时，我完全迷失了。我以为我有数学背景，但我很快意识到我没有。  您会推荐阅读哪些课程/书籍来理解该论文（以及其他生成模型论文，一般来说，我对生成模型论文有这个问题。）支撑它的数学超出了我的理解范围。    由   提交 /u/Ok_Seesaw5723   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acqyiz/resources_to_understand_vaes/</guid>
      <pubDate>Sun, 28 Jan 2024 01:39:23 GMT</pubDate>
    </item>
    <item>
      <title>如何抓住 AI 作弊：智胜机器人 - 2024 年版</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/</link>
      <description><![CDATA[&quot;如果您碰巧有任何不使用 ChatGPT 和其他人工智能 (AI) 作弊的学生，是时候了解情况了。根据民主与技术中心最近的一项调查，58% 的学生表示使用生成式人工智能来完成作业。随着人们对这项技术的认识不断提高，这个数字只会增加。与此同时，同一项研究报告称，教育工作者发现自己落后于技术曲线，只有 43% 的教师接受过生成人工智能方面的重要培训。  在本文中，我们将尝试为教育工作者提供所需的信息，以了解学生如何使用这项技术进行作弊以及教师如何检测和应对生成式人工智能。除了检测其使用之外，这项新技术还可能提供利用新的创新教育方式的机会。” https://ai-solutions.pro/tools-to-detect-ai-cheating/  &amp;# 32；由   提交/u/Science-man777  /u/Science-man777 reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acov77/how_to_catch_aicheating_outsmart_the_bot_2024/</guid>
      <pubDate>Sat, 27 Jan 2024 23:58:57 GMT</pubDate>
    </item>
    <item>
      <title>为什么线性回归模型的成本函数应用于逻辑回归时会给出非凸图或具有多个局部最小值的图？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acnyri/why_does_the_cost_function_of_a_linear_regression/</link>
      <description><![CDATA[ 由   提交/u/Professional_Path552   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acnyri/why_does_the_cost_function_of_a_linear_regression/</guid>
      <pubDate>Sat, 27 Jan 2024 23:18:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习工程师面临的最头疼和烦人的事情是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acmunb/what_are_the_biggest_headaches_and_annoying/</link>
      <description><![CDATA[我是一位非常新的机器学习工程师，我有兴趣了解更多有关机器学习工程师日常解决的问题以及一些问题的信息他们必须解决的最困难的问题，我很乐意听到与任何阶段相关的问题，例如构建模型、训练、部署等。 提前致谢！！ &lt; /div&gt;  由   提交/u/jaym-00   /u/jaym-00  reddit.com/r/learnmachinelearning/comments/1acmunb/what_are_the_biggest_headaches_and_annoying/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acmunb/what_are_the_biggest_headaches_and_annoying/</guid>
      <pubDate>Sat, 27 Jan 2024 22:29:14 GMT</pubDate>
    </item>
    <item>
      <title>关于 Huggingface 中的 GPT-1 的问题。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1aclizm/questions_about_gpt1_in_huggingface/</link>
      <description><![CDATA[大家好，目前我正在学习法学硕士，我有几个菜鸟问题。 首先，让我们从GPT-1 论文：https://cdn.openai.com/research-covers/language- unsupervised/language_understand_paper.pdf 问题 1：嵌入步骤的输入到底是什么形状？ 让我们看一下表达式论文中的块（2）。 根据论文，模型的输入被命名为U。从我到目前为止收集到的信息来看，这些应该是第一个标记化步骤之后的 token_id。然而，我对这里的维度如何运作有点困惑。嵌入矩阵 W_e 应该是一个维度为 N x H 的矩阵，其中 N 是该分词器所有可能的分词 ID 的总数，H 是转换器块中隐藏层的维度。所以基本上 W_e 是一个查找表，其中每一行对应于该令牌 id 的嵌入。 W_e 应该工作的方式是，您采用令牌 id 的单热编码表示并将其相乘以选择适当的行。 所以我有点困惑论文中的乘法是如何运作的。我理解它计算矩阵 U 的方式应该包含每个 token_id 作为 one-hot 编码行。换句话说，矩阵应如下所示： 第一行 = U_{1,:} = [ 0 , 0 , ... 1 , 0 , 0 ...] 第二行 = U_{2,:} = [ 0 , 1 ,0 , .. 0 ] 等等。其中 1 的列索引对应于该令牌的令牌 id 的值。  这是正确的吗？然后我觉得论文中有点不清楚。他们只是写 U= (u1, u2, ...) 就这样了。如果我的理解是正确的，有人可以证实这一点吗？ 问题 2： 这一步发生在 HuggingFace 模型中的哪里？ &lt; p&gt;所以假设之前的解释是正确的，我尝试在 HuggingFace 中调用模型。考虑以下代码片段： from Transformers import OpenAIGPTTokenizer, OpenAIGPTModel import torch tokenizer = OpenAIGPTTokenizer.from_pretrained(“openai-gpt”) model = OpenAIGPTModel.from_pretrained(“openai-gpt”) input = tokenizer(&quot;你好，我的狗很可爱&quot;, return_tensors=&quot;pt&quot;) print(f&quot;输入：{输入}&quot;) 输出 =model(**输入) print(f&quot;输出：{输出}&quot; ;)  它返回： 输入：{&#39;input_ids&#39;：tensor([[3570, 240, 547, 2585, 544, 4957]] ), &#39;attention_mask&#39;: 张量([[1, 1, 1, 1, 1, 1]])} 输出: BaseModelOutput(last_hidden_​​state=tensor([[[ 0.4653, 0.0642, 0.5910, ..., 0.1177, -0.0021 , -1.2262], [-0.3697, -0.0957, 0.6613, ..., -0.0344, -0.2164, 0.1205], [ 0.1700, -0.3252, 0.0407, ..., 0.1589, -0.8057, -0.2830], [- 0.3669, -0.0448, 0.8061, ..., -0.0090, -0.0872, -0.5224], [-0.5047, 0.6522, 0.6932, ..., 0.0811, 0.6475, 0.3190], [-0.2972, 0.0591, 1.2 333、.. ., -0.7394, -0.2600, 0.0863]]], grad_fn=),hidden_​​states=None, Attentions=None)  因此，这里的实际输入似乎是model 只是 token id tensor([[3570, 240, 547, 2585, 544, 4957]]) 的单个张量，尚未采用适当的 one-hot 编码形式 U.任何人都可以确认这是否首先发生在模型内部，然后再乘以嵌入矩阵吗？ 问题 3： 该怎么办与这个“last_hidden_​​state”的输出？ 所以，按照我的理解，这里的输出实际上是最后一个变压器块“h_n”。它的维度为 (1, 6 , 768) - 所以基本上对于每个输入标记，我得到长度为 768 的隐藏最终状态。 那么如果我想用它做某事，现在如何使用这个结果？如果不训练单独的分类器，我对这个最终状态无能为力。 ​   由   提交/u/Invariant_apple   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1aclizm/questions_about_gpt1_in_huggingface/</guid>
      <pubDate>Sat, 27 Jan 2024 21:30:06 GMT</pubDate>
    </item>
    <item>
      <title>关于实验设计的问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acli33/question_about_experimental_design/</link>
      <description><![CDATA[大家好， 在我的项目中，我评估了 4 个模型（xgboost、随机森林、svm 和 mlp）与 2 个模型源自同一数据集的特征工程数据集。我使用 AUC 指标进行超参数优化。  我想最终报告 4 个模型的平均 AUC 之间的统计显着性，以及模型组之间在所选特征数据集级别上的统计显着性。可能使用 t 检验或 F 检验。  现在由于问题与时间序列问题有关，因此我的测试集已修复，即最近 20% 的数据。因此，我不会通过交叉验证等方式生成分数样本来测试其显着性。我在参数优化期间确实使用交叉验证，但仅使用我的训练集。  我希望收到一些关于这种实验方法的反馈，以及我如何仍然可以测试显着性，而不是简单地报告测试集分数。我感到困惑的原因是我认为简单地报告测试集分数会忽略模型方差，并且我想解释这一点。  也许这是通过训练集上的交叉验证来解释的（？），但我不太确定。    ;由   提交 /u/JanBitesTheDust   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acli33/question_about_experimental_design/</guid>
      <pubDate>Sat, 27 Jan 2024 21:28:26 GMT</pubDate>
    </item>
    <item>
      <title>深度学习小书（François Fleuret，2023 年 6 月 23 日，168 页）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acj92q/the_little_book_of_deep_learning_françois_fleuret/</link>
      <description><![CDATA[      PDF 链接：https://fleuret.org/public/lbdl.pdf 主页：https://fleuret.org/francois/lbdl.html “本书是为具有 STEM 背景的读者提供的关于深度学习的简短介绍，最初旨在在手机屏幕上阅读。它根据非商业知识共享许可证分发，八个月内下载了 500,000 次。 ＆quot; https://preview .redd.it/pspdw9ohe1fc1.jpg?width=1683&amp;format=pjpg&amp;auto=webp&amp;s=e06166fae2656bda007a160f775f7567ac9aaea4   由   提交/u/Singularian2501  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acj92q/the_little_book_of_deep_learning_françois_fleuret/</guid>
      <pubDate>Sat, 27 Jan 2024 19:49:40 GMT</pubDate>
    </item>
    <item>
      <title>免费数学重机器学习/人工智能课程？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1acaxty/free_math_heavy_machine_learningai_courses/</link>
      <description><![CDATA[我已完成 Andrew Ng 的机器学习“专业化”课程。共有三门独立的课程：  监督机器学习 高级学习算法 无监督机器学习  我对他在课程中介绍的数学感到非常惊讶（不同的回归、梯度下降、不同的激活函数、归一化、正则化、算法的变化等），对于那些不熟悉数学的人来说，其中的一部分也是可选的因为喜欢这个话题。完成本专业课程后，我渴望另一门类似的课程，它可能会涉及其他领域，或者相同的领域但更深入？ 如有任何建议，我们将不胜感激：D   由   提交/u/Guava-Java-  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1acaxty/free_math_heavy_machine_learningai_courses/</guid>
      <pubDate>Sat, 27 Jan 2024 13:33:59 GMT</pubDate>
    </item>
    <item>
      <title>试图让 GA 生成类似于数字 7 的 MNIST 图像，这个想法注定要失败吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1aca14i/trying_to_get_a_ga_to_generate_mnist_like_images/</link>
      <description><![CDATA[大家好，我创建了一个适用于 28x28 二进制矩阵和 MNIST 数字分类器的遗传算法模型，我将 GA 适应度函数调整为依赖于MNIST 分类器给出的矩阵为 #7 的概率。我还没有看到人们在网上做类似的事情，并且由于训练时间很长并且摆弄超参数，算法没有收敛 - 这个想法可行吗？为什么/为什么不呢？  可以提供代码。 （还要注意的是，我应用了高斯模糊和 0-&gt;0、1-&gt;255 的映射来将二进制转换为灰度。）    ;由   提交 /u/Few-Fun3008    reddit.com/r/learnmachinelearning/comments/1aca14i/trying_to_get_a_ga_to_generate_mnist_like_images/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1aca14i/trying_to_get_a_ga_to_generate_mnist_like_images/</guid>
      <pubDate>Sat, 27 Jan 2024 12:42:31 GMT</pubDate>
    </item>
    <item>
      <title>为什么人工神经网络在训练时在验证数据上表现得更好？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ac0c6a/why_does_the_ann_perform_so_much_better_on_the/</link>
      <description><![CDATA[   /u/HoleNother  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ac0c6a/why_does_the_ann_perform_so_much_better_on_the/</guid>
      <pubDate>Sat, 27 Jan 2024 02:41:41 GMT</pubDate>
    </item>
    </channel>
</rss>