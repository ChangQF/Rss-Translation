<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>欢迎来到R/LearnMachineLearning-一个学习者和教育工作者对机器学习充满热情的社区！这是您提出问题，共享资源并共同发展的空间，从理解ML概念到从基本原理到高级技术。无论您是写第一个神经网络还是潜入变压器，都可以在这里找到支持者。用于ML研究， /R /MachineLearning用于简历审查， /R /工程介绍ML工程师， /r /mlengineering</description>
    <lastBuildDate>Sat, 08 Mar 2025 09:14:27 GMT</lastBuildDate>
    <item>
      <title>图形数据的矢量化方法（在线ML）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j6csr2/vectorization_method_for_graph_data_online_ml/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您好， 我目前正在从事Android恶意软件检测项目（二进制分类；恶意软件和良性），在其中我分析从我发现的在线数据集中从APK文件中提取的函数呼叫图。但是我是整个“图形数据”部分的新手。 我的项目尤其基于在线学习，这是模型随着新数据到达而不断自行更新的时候，而不是在固定数据集中培训。尽管我想知道我是否应该首先合并部分批处理学习...  我正在使用 示例原始JSON数据，但我打算使用：     {＆lt; lt; lt; lt; dump; lt; lt; dump; lt; lt; lt; dummymymainmathod（void dummymainmethod（java.lang.lang.lang.string [java.lang.string []） ＆quot＆quot＆quor com.ftnpv.speed.mywrapperproxyapplication：void＆lt; init＆gt;（） ＆quot＆quot＆quot android.app.application：void＆lt; init＆gt;（） ＆quot＆quord＆lt; com.wrapper.proxyapplication.wrapperproxyapplication：void onCreate（）＆gt;：＆qut;：{}}}}}}}}}    每个键是函数名称，并且值是其他功能。该结构表示应用程序的控制流。 href =“ https://arxiv.org/pdf/1707.05005”&gt; Graph2vec 兼容性。 ARF, Hoeffding Tree, SDG) using these embeddings.  Based on what I have seen, Graph2vec only captures structural properties of the graph so similar function call patterns between different APKs and variations in function relationships between benign and malware samples. I&#39;m kind of stuck here and I have a couple of问题：   graph2vec是这个问题的正确选择吗？ 有没有基于ol的gnn在那里可以尝试？ 另一种图形嵌入方法（node2vec，gcns或其他东西）可以更好地工作吗？提交由＆＃32; /u/u/i-am-am-just-that-guy       [link]  &lt;a href =“ https://www.reddit.com/r/learnmachinelearning/comments/1j6csr2/vectorization_method_method_for_graph_graph_data_online_mlle/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j6csr2/vectorization_method_for_graph_data_online_ml/</guid>
      <pubDate>Sat, 08 Mar 2025 08:26:55 GMT</pubDate>
    </item>
    <item>
      <title>任何人都可以在NLP领域建议我一些研究想法吗</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j6ahnz/can_anybody_suggest_me_some_research_ideas_in_the/</link>
      <description><![CDATA[＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/learnmachinelearning/comments/1j6ahnz/can_anybody_suggest_me_some_some_some_research_ideas_in_in_in_the/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j6ahnz/can_anybody_suggest_me_some_research_ideas_in_the/</guid>
      <pubDate>Sat, 08 Mar 2025 05:46:55 GMT</pubDate>
    </item>
    <item>
      <title>Gini杂质与熵 - 有什么区别以及何时使用它们？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j6a1l5/gini_impurity_vs_entropy_whats_the_difference_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有一个问题并搜索了它，但是Gini杂质和熵似乎很相似。一个人谈论“杂质”另一个是指“不确定性”。它们到底有什么区别，何时应该使用？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/udbhav96      &lt;a href =“ https://www.reddit.com/r/lealelnmachinelearning/comments/1j6a1l5/gini_impurity_vs_entropy_thats_whats_thats_thats_thates_the_difference_and/”]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j6a1l5/gini_impurity_vs_entropy_whats_the_difference_and/</guid>
      <pubDate>Sat, 08 Mar 2025 05:19:09 GMT</pubDate>
    </item>
    <item>
      <title>在您最喜欢的电影/电视角色项目中，AI驱动的语音帮助？是一个好人吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j69ae8/ai_powered_voice_assistance_in_your_favourite/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  它会成为专业人士吗？对于试图闯入数据科学和相关角色的进入角色？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/javaphile77     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j69ae8/ai_powered_voice_assistance_in_your_favourite/</guid>
      <pubDate>Sat, 08 Mar 2025 04:33:49 GMT</pubDate>
    </item>
    <item>
      <title>从机器学习开始</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j68n0q/starting_on_machine_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，reddit！我一直在考虑学习ML一段时间。大家都建议新手推荐哪些技巧/资源？  对于某些背景，我是100％的机器学习新手。因此，任何建议和技巧都将非常感谢！我想首先从完整的基础知识开始。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/verity_q     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j68n0q/starting_on_machine_learning/</guid>
      <pubDate>Sat, 08 Mar 2025 03:56:08 GMT</pubDate>
    </item>
    <item>
      <title>填补与转移学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j68lw6/finetuning_vs_transfer_learning/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  为什么一个模型在填充过程中会遭受健忘的痛苦，我对OCR模型进行了填充以识别IAM数据集上的手写，而是忘记了其原始用例。以及如何转移学习不同  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/mehdii __    href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j68lw6/finetuning_vs_transfer_learning/”&gt; [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j68lw6/finetuning_vs_transfer_learning/</guid>
      <pubDate>Sat, 08 Mar 2025 03:54:16 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络（CNN）数据流 - 观察数据如何通过层移动！该动画显示了激活如何在CNN中传播。不是花的确切模型，而是数据流的演示。您如何看待AI模型的解释性发展？专注于流程，而不是体系结构。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j66un5/convolutional_neural_network_cnn_data_flow_viz/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/aiwithashwin     &lt;a href =“ https://www.reddit.com/r/lealelnmachinelearning/comments/1j666un5/convolutional_neural_neur_network_cnn_data_data_data_flow_viz/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j66un5/convolutional_neural_network_cnn_data_flow_viz/</guid>
      <pubDate>Sat, 08 Mar 2025 02:17:29 GMT</pubDate>
    </item>
    <item>
      <title>关于在自定义数据集上使用Dreambooth的微调稳定扩散的建议</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j656s4/advice_on_fine_tuning_stable_diffusion_using/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨， 我一直在使用Dreambooth（在我的数据集中使用Dreambooth）进行微调稳定扩散（这是10张人的图像）。它生成的图像与数据集中的主题不同。有时，它会产生不同的产生。我在这方面做我的NVIDIA RTX 4060 GPU，该GPU具有8GB RAM。我已经使用了Xformers和一半的精度。我能否就如何改善稳定的扩散微调提出任何建议。我将其用作我的参考脚本（ colab参考）。  预先感谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/faircut     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j656s4/advice_on_fine_tuning_stable_diffusion_using/</guid>
      <pubDate>Sat, 08 Mar 2025 00:51:24 GMT</pubDate>
    </item>
    <item>
      <title>训练神经网络国际象棋引擎 - 为什么黑色继续获胜？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j63t44/training_a_neural_network_chess_engine_why_does/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在开发一个自学习的国际象棋引擎，该引擎可以通过自我播放，随着时间的推移逐渐结合神经网络评估。 Despite multiple adjustments, Black consistently outperforms White, and I can&#39;t seem to fix it. Current Training Metrics:  Games Played: 2400 White Wins: 30 (1.2%) Black Wins: 368 （15.3％）  绘制： 1155（48.1％）   赢得率： 0.2563      当前的Elo评级： 1200  1200  1200  0.029513   最新MAE： 0.056798    最新结果准确度： 96.62％   我到目前为止尝试了什么我尝试的是：           调整探索参数以平衡随机性。 增加对神经网络评估对材料启发式的依赖。     然而，对黑色保留的偏见。这是自我播放增强学习中的常见问题，还是我的数据收集或评估过程中的某些问题是加强失衡  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/learnmachinelearning/comments/1j63t44/training_a_neural_neurwork_network_chess_chess_engine_engine_why_does/”&gt; [link]  &lt;a href =“ https://www.reddit.com/r/lealenmachinelearning/comments/1j63t44/1j63t44/training_a_neural_neural_network_chess_chess_chess_engine_engine_why_does/”]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j63t44/training_a_neural_network_chess_engine_why_does/</guid>
      <pubDate>Fri, 07 Mar 2025 23:43:53 GMT</pubDate>
    </item>
    <item>
      <title>AI可以预测2025 F1赛季吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j63rdb/can_ai_predict_the_2025_f1_season/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在python中进行了竞赛预测，以预测2025的构造函数性能。 在此处查看代码： https://github.com/connormck33333/f1-race-predictor     href =“ https://medium.com/@connora.mckenzie/can-ai-predict-the-f1-f1-2025-season-6d629e1e56a4”&gt; https://medium.com/@connora.mckenzie/can-ai-predict-the-the-f1-2025-season-6d629e1e56a4    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/connormck333     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j63rdb/can_ai_predict_the_2025_f1_season/</guid>
      <pubDate>Fri, 07 Mar 2025 23:41:33 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA RTX 3050</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j61vxg/nvidia_rtx_3050/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  所以我几个月前用GPU购买了一台笔记本电脑。我主要拿到这台笔记本电脑，因此可以开始为Uni和我的投资组合做一些个人深度学习项目。但是，我最近意识到，出于此目的，GPU根本不是很好（NVIDIA RTX 3050 4GB VRAM）。 VRAM太低。仅依靠这一数量的GPU记忆是可持续的吗？几个月前购买，我现在无法退还笔记本电脑。我必须依靠Google合作吗？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sessaro290     [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j61vxg/nvidia_rtx_3050/</guid>
      <pubDate>Fri, 07 Mar 2025 22:15:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么Openai带来了一个新的，更大的型号，例如4.5？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j5w7pu/why_has_openai_brought_a_new_larger_model_like_45/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我仍然对为什么打开AI带来了4.5之类的模型；可能是其他研究实验室将来会带来同样的东西。但是重点是什么？ LLM的轨迹突然转向推理模型。  如果需要新的，最新的数据，则可以轻松搜索，对吗？  今天我正在使用4.5; 也没有任何差异。而且，我觉得大多数人口甚至都无法利用这些LLM的全部潜力。这些模型在数学编码方面变得如此强大。  另外，如果我说错了，请更正。我仍在研究注意机制。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/learnmachinelearning/comments/1j5w7pu/why_has_has_openai_apenai_a_a_new_new_new_larger_model_model_model_like_like_45/  &lt;a href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j5w7pu/why_has_openai_a_a_new_new_new_larger_model_model_model_larger_model_largike_45/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j5w7pu/why_has_openai_brought_a_new_larger_model_like_45/</guid>
      <pubDate>Fri, 07 Mar 2025 18:35:09 GMT</pubDate>
    </item>
    <item>
      <title>2025年学习Pytorch的最佳资源。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j5trra/best_resources_to_learn_pytorch_in_2025/</link>
      <description><![CDATA[I was confused between TF and Pytorch but looking at earlier discussion in this sub, everyone prefers PyTorch. I looked at various resources for learning it. Here are few options I came across:   fast.ai 实用的深度学习课程     pytorch：懒惰程序员的深度学习和人工智能（  pytorch为深度学习训练训练camp（ link ）       我应该去哪一个？ - &gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j5trra/best_resources_to_learn_learn_pytorch_in_2025/”&gt; [link]   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j5trra/best_resources_to_learn_pytorch_in_2025/</guid>
      <pubDate>Fri, 07 Mar 2025 17:08:19 GMT</pubDate>
    </item>
    <item>
      <title>我制作了我的第一个神经网络，可以识别简单的面孔！</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j52zxi/i_made_my_1st_neural_network_that_can_recognize/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  在图片上有一部分代码和训练+推理数据（我已经绘制了我自己😀）。如果您有兴趣，则该代码在GitHub上。如果您想启动它，尽管可能不需要，但必须对其进行编辑，但终端的图片可以解释所有内容。该程序非常稳定地犯了一个错误，但这没什么大不了的。  https://github.com/ihateandreykrasnokutsky/neural_networks_python/blob/main/9.%201St%20Face%20Recognition%20NN%21.py     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/altruistic-error-262       &lt;a href =“ https://www.reddit.com/r/lealelnmachinelearning/comments/1j52zxi/i_made_my_my_my_my_my_my_my_neal_neur_network_that_that_that_that_that_that_that_that_that_that_that_that_can_recognize/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j52zxi/i_made_my_1st_neural_network_that_can_recognize/</guid>
      <pubDate>Thu, 06 Mar 2025 18:55:50 GMT</pubDate>
    </item>
    <item>
      <title>与机器学习相关的简历评论帖子</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请礼貌地将有关简历评论的任何帖子重定向到这里   对于那些正在寻找简历评论的人，请在 href =“ href =“ https://www.reddit.com/r/resumes”&gt;/r/简历或 r/EngineeringResumes 首先，然后在此处交叉crosspost。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/techrat_reddit     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>