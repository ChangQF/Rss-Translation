<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Sun, 10 Nov 2024 09:14:59 GMT</lastBuildDate>
    <item>
      <title>我不知道下一步该做什么</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnwkyf/i_dunno_what_to_do_next/</link>
      <description><![CDATA[我是一名 clg 学生，希望学习 ml 然后学习 dl。我了解 python、numpy、pandas、matplotlib 和数学。我还参加了 Andrew ng 的 ML 专业课程。 现在我陷入了困境，不知道下一步该怎么做。我应该学习 eda、预处理还是开始学习 ml 算法？如果是这样，我在哪里以及如何学习这些？我需要你们的指导。请帮帮我。提前谢谢了！ （编辑：点赞并将此帖子置顶，因为这对像我这样的人会有所帮助）    提交人    /u/Asta-12   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnwkyf/i_dunno_what_to_do_next/</guid>
      <pubDate>Sun, 10 Nov 2024 08:59:54 GMT</pubDate>
    </item>
    <item>
      <title>只需花费 239 美元一年即可访问 Coursera 的几乎所有机器学习课程。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnwghj/get_access_of_almost_all_machine_learning_courses/</link>
      <description><![CDATA[优惠详情：  优惠日期： 2024 年 11 月 7 日 — 2024 年 12 月 12 日（26 天） 优惠折扣：Coursera Plus 年度订阅 40% 折扣（优惠 160 美元） 限制：不包括印度、德国、讲西班牙语的拉丁美洲  从今天开始，Coursera 将为我们的 Coursera Plus 年度订阅提供 40% 的折扣。您可以无限制地访问 7,000 多门课程，包括来自 Google、Meta、Microsoft、IBM 等顶级行业领导者的专业证书 - 所有这些只需 239 美元（通常为 399 美元）即可享受 12 个月的优惠。 阅读主要文章。    由    /u/eham2017 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnwghj/get_access_of_almost_all_machine_learning_courses/</guid>
      <pubDate>Sun, 10 Nov 2024 08:50:26 GMT</pubDate>
    </item>
    <item>
      <title>练习题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnvird/questions_for_practice/</link>
      <description><![CDATA[我刚刚完成了机器学习数学书，但找不到练习题的资源。有人可以给我推荐一些资源吗？    提交人    /u/badanunu   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnvird/questions_for_practice/</guid>
      <pubDate>Sun, 10 Nov 2024 07:41:40 GMT</pubDate>
    </item>
    <item>
      <title>GAN 训练的周期</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnv3x8/epoch_for_gan_training/</link>
      <description><![CDATA[      嗨，我想尝试学习 GAN。目前，我正在为 126x126 GAN 模型使用大约 10k 个图像数据集。我应该训练我的模型多少个 epoch？我使用 6k epoch 和 4 个批次大小，因为我的笔记本电脑只能处理这么多，并且在 6k epoch 之后，我的生成器仅产生奇怪的像素，fid 分数为 27.9。    提交人    /u/No-Attention9172   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnv3x8/epoch_for_gan_training/</guid>
      <pubDate>Sun, 10 Nov 2024 07:11:42 GMT</pubDate>
    </item>
    <item>
      <title>[需要帮助] 寻找机器学习数据集</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnuwmo/help_needed_looking_for_a_machine_learning_dataset/</link>
      <description><![CDATA[大家好！我是一名从事机器学习项目的学生，我需要一个数据集。理想情况下，我正在寻找一个包含几千个样本和大约 15 个特征的数据集，我可以对其进行预处理，然后将其用于训练 ML 算法。任何关于合适数据集的建议或关于在哪里查找的建议都将不胜感激！ 提前谢谢您！    提交人    /u/Minute-End-1522   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnuwmo/help_needed_looking_for_a_machine_learning_dataset/</guid>
      <pubDate>Sun, 10 Nov 2024 06:57:45 GMT</pubDate>
    </item>
    <item>
      <title>[帮助] LSTM seq2seq 生成相同序列</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnq81z/help_lstm_seq2seq_generating_same_sequence/</link>
      <description><![CDATA[Kaggle Notebook 我正在尝试在 pytorch 中实现 seq2seq 模型来进行翻译。问题是模型生成相同的序列。我的目标是实现 seq2seq 的注意力，然后最终转向 transformers。有人可以看看我的代码吗（还附有 kaggle 笔记本）： class Encoder(nn.Module): def __init__(self,vocab_size,embedding_dim,hidden_​​dim,num_layers): super(Encoder,self).__init__() self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.hidden_​​dim = hidden_​​dim self.num_layers = num_layers self.embedding = nn.Embedding(self.vocab_size,self.embedding_dim) self.lstm = nn.LSTM(self.embedding_dim,self.hidden_​​dim,self.num_layers,batch_first=True) def forward(self,x): x = self.embedding(x) output,(hidden_​​state,cell_state) = self.lstm(x) return输出，hidden_​​state，cell_state 类解码器（nn.Module）： def __init__（self，vocab_size，embedding_dim，hidden_​​dim，num_layers）： super（Decoder，self）。__init__（） self.vocab_size = vocab_size self.embedding_dim = embedding_dim self.hidden_​​dim = hidden_​​dim self.num_layers = num_layers self.embedding = nn.Embedding（self.vocab_size，self.embedding_dim） self.lstm = nn.LSTM（self.embedding_dim，self.hidden_​​dim，self.num_layers，batch_first=True） self.fc = nn.Linear（self.hidden_​​dim，self.vocab_size） def forward（self，x，h，c）： x = self.embedding（x）输出，（hidden_​​state，cell_state）= self.lstm（x）输出=self.fc（输出）返回输出，h，c class Seq2Seq（nn.Module）：def __init__（self，encoder，decoder）：super（Seq2Seq，self）。__init__（）self.encoder =编码器self.decoder =解码器def forward（self，X，Y）：输出，h，c =编码器（X）decoder_input = Y [：，0]。到（torch.int32）输出_tensor = torch.zeros（Y.shape[0]，Y.shape[1]，FR_VOCAB_SIZE）。到（device）#output_tensor [：，0] = Y [：，0]#设置相同的起始标记，即&lt;START&gt;&gt;对于范围内的 i（1，Y.shape [1]）： output_d，h，c = 解码器（decoder_input，h，c）# 输出形状：（b​​atch_size，fr_vocab_size）coder_input = torch.argmax（output_d，dim = 1）# 输出形状：（b​​atch_size，1）output_tensor [：，i] = output_d return output_tensor # 输出形状：（b​​atch_size，seq_length）class Seq2Seq2（nn.Module）：def __init__（self，encoder，decoder）：super（Seq2Seq2，self）。__init__（）self.encoder = 编码器self.decoder = 解码器def forward（self，X，Y）：output，h，c = 编码器（X）decoder_input = Y [：，：-1] 。到（torch.int32）output_tensor，h，c = self.decoder（decoder_input，h，c）返回output_tensor 编码器 = 编码器（ENG_VOCAB_SIZE，32,64,1）.to（设备） 解码器 = 解码器（FR_VOCAB_SIZE，32,64,1）.to（设备） 模型 = Seq2Seq2（编码器，解码器）.to（设备） lr = 0.001 优化器 = torch.optim.Adam（model.parameters（），lr=lr） loss_fn = nn.CrossEntropyLoss（ignore_index=0） epochs = 20 for epoch in range(epochs): running_loss = 0.0 progress_bar = tqdm（train_dataloader，desc=f“Epoch {epoch+1}”，leave=False） for X，Y in progress_bar： Y_pred = 模型（X，Y）# Y = Y[:,1:]# Y_pred = Y_pred[:,:-1,:] Y_pred = Y_pred.reshape(-1, Y_pred.size(-1)) # 展平为 (batch_size * seq_length, vocab_size) Y_true = Y[:,1:] Y_true = Y_true.reshape(-1) # 展平为 (batch_size * seq_length) loss = loss_fn(Y_pred, Y_true) optimizer.zero_grad() loss.backward() optimizer.step() # 更新运行损失并在 tqdm 中显示 running_loss += loss.item() progress_bar.set_postfix(loss=loss.item()) print(f&quot;Epoch {epoch+1}, Loss = {running_loss/len(train_dataloader)}&quot;)     submitted by    /u/Disastrous_Pie9783   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnq81z/help_lstm_seq2seq_generating_same_sequence/</guid>
      <pubDate>Sun, 10 Nov 2024 02:14:57 GMT</pubDate>
    </item>
    <item>
      <title>开始机器学习的最佳资源和建议？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnnjkh/best_resources_advice_for_getting_started_in/</link>
      <description><![CDATA[我打算学习机器学习，但我才刚刚开始攻读计算机科学学位，面对众多选择，我感到有些不知所措。我很想得到一些关于从哪里开始的指导，尤其是因为我想攻读机器学习硕士学位，并且想成为一名出色的申请人。 我有一些问题：  我应该先专注于学习数学，还是深入研究实用的机器学习并边学边学？ 您会推荐哪些在线课程或资源？ 我如何才能提高进入优秀机器学习硕士课程的机会。  提前感谢您的回答 :)    提交人    /u/Kerensky0   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnnjkh/best_resources_advice_for_getting_started_in/</guid>
      <pubDate>Sat, 09 Nov 2024 23:55:41 GMT</pubDate>
    </item>
    <item>
      <title>比较两个角色</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnj6ss/compare_two_roles/</link>
      <description><![CDATA[大家好，我收到了一家小型保险公司的录用通知，我是一名数据科学家，工作内容是预测客户行为并将其输入风险方程（包括部署和监控），但我认为这个职位缺乏工作与生活的平衡。我目前的职位是机器学习工程师，主要使用 GPT 等 genAI 进行研究和概念验证，这是一家工作与生活平衡性很好的大型保险公司。我收到的录用通知比我现在的职位高出约 10%，请分享一些建议，因为我正在努力做出明智的决定    提交人    /u/Putrid_Earth3846   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnj6ss/compare_two_roles/</guid>
      <pubDate>Sat, 09 Nov 2024 20:30:17 GMT</pubDate>
    </item>
    <item>
      <title>训练期间测试准确率不稳定意味着什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gniryc/what_does_a_volatile_test_accuracy_during/</link>
      <description><![CDATA[      在训练分类神经网络时，我一直得到非常不稳定/“跳跃”的测试准确率？这仍然是我对网络进行微调的早期阶段，但我很好奇这是否对模型有任何众所周知的影响？我怎样才能让它在更高的准确度下稳定下来？我很感激任何关于此的反馈或想法。     提交人    /u/learning_proover   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gniryc/what_does_a_volatile_test_accuracy_during/</guid>
      <pubDate>Sat, 09 Nov 2024 20:11:05 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型背后的数学</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnfb6o/math_behind_diffusion_models/</link>
      <description><![CDATA[有没有人有任何好的资源可以清楚地解释扩散模型背后的数学原理？    提交人    /u/amirdol7   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnfb6o/math_behind_diffusion_models/</guid>
      <pubDate>Sat, 09 Nov 2024 17:34:57 GMT</pubDate>
    </item>
    <item>
      <title>新手询问如何为拥有 150 万数据的网站构建 LLM 或生成式 AI</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gnarbb/newbie_asking_how_to_build_an_llm_or_generative/</link>
      <description><![CDATA[我是一名开发人员，但对 AI 来说是新手，这是我发布的第一个关于它的问题。 我们的非营利网站托管着人物数据，例如传记。我正在寻找构建类似 chatgpt 的东西，它可以帮助用户搜索并理解这些数据。 例如，如果有人问“南卡罗来纳州有多少人死于新冠病毒并结婚”，它就能告诉你。 基本上是一个基于我们数据的 AI 驱动的搜索引擎。 我不知道从哪里开始查找或编码。不知何故，我知道我需要一个 llm 模型和数据集来训练 AI。但是我如何找到模型，然后如何安装它，以及我们使用什么 UI 来使用我们的数据训练 AI。我们的网站由 WordPress 提供支持。 基本上，我需要一个关于从哪里开始的指南。 提前致谢！    提交人    /u/tjthomas101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gnarbb/newbie_asking_how_to_build_an_llm_or_generative/</guid>
      <pubDate>Sat, 09 Nov 2024 14:02:43 GMT</pubDate>
    </item>
    <item>
      <title>Daniel Bourke 是倾斜的山羊</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gn9bmc/daniel_bourke_is_the_goat_for_leanin/</link>
      <description><![CDATA[就是这样，他很好地解释了实际概念，andrew ng 也不错，但主要是理论     提交人    /u/jinstronda   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gn9bmc/daniel_bourke_is_the_goat_for_leanin/</guid>
      <pubDate>Sat, 09 Nov 2024 12:45:07 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习打败恐龙游戏 - 详情见评论</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gn9a1p/beating_the_dinosaur_game_with_ml_details_in/</link>
      <description><![CDATA[        由    /u/Mbird1258  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gn9a1p/beating_the_dinosaur_game_with_ml_details_in/</guid>
      <pubDate>Sat, 09 Nov 2024 12:42:31 GMT</pubDate>
    </item>
    <item>
      <title>尽管 numpy 在数据和参数相同的情况下收敛，但 Pytorch 却出现了分歧</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gn80t9/pytorch_diverges_although_numpy_converges_with/</link>
      <description><![CDATA[      我首先在 numpy 中实现了线性回归的基本梯度下降，然后使用 pytorch。但是，在相同的数据、参数初始化和学习率的情况下，一个会收敛（numpy，左），而另一个会发散（pytorch，右） https://preview.redd.it/xz80dqlxzuzd1.png?width=1274&amp;format=png&amp;auto=webp&amp;s=808142943d9ecf323a4a7933fef95b2bb7532de7 以下是每个代码： Numpy： 导入数学 导入 matplotlib.pyplot 作为 plt 导入 numpy 作为 np n = 50 np.random.seed(1) x = np.linspace(0, 2*math.pi, n) y = np.sin(x) y += np.random.normal(scale=0.1, size=len(y)) alpha = 0.15 m = 0 b = 0loss = [] fig, axs = plt.subplots(2) while True: axs[0].plot(x, m*x+b) axs[0].scatter(x, y) axs[1].plot(losses) plt.draw() plt.waitforbuttonpress() for ax in axs: ax.clear() b -= alpha * 1/n * sum(b + m*x[i] - y[i] for i in range(n)) m -= alpha * 1/n * sum((b + m*x[i] - y[i]) * x[i] for i in range(n)) mse = sum((y - (m*x+b))**2)/n loss.append(mse)  Pytorch: import math import matplotlib.pyplot as plt import numpy as np import torch.nn n = 50 np.random.seed(1) x = np.linspace(0, 2*math.pi, n) y = np.sin(x) y += np.random.normal(scale=0.1, size=len(y)) x = torch.from_numpy(x) y = torch.from_numpy(y) x = x.reshape(-1, 1) y = y.reshape(-1, 1) alpha = 0.15 m = torch.zeros(1, require_grad=True) b = torch.zeros(1, require_grad=True) loss_fn = torch.nn.MSELoss() optimizer = torch.optim.SGD([m, b], lr=alpha) loss = [] fig, axs = plt.subplots(2) while True: y_est = m * x + b loss = loss_fn(y_est, y) loss.append(loss.item()) loss.backward() optimizer.step() optimizer.zero_grad() axs[0].plot(x, y_est.detach().numpy()) axs[0].scatter(x, y) axs[1].plot(losses) plt.draw() plt.waitforbuttonpress() for ax in axs: ax.clear()  即使我将 LR 降至 0.1，它们的行为仍然相同，因此我认为这不是一个小的舍入误差或类似错误。    提交人    /u/autorayn   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gn80t9/pytorch_diverges_although_numpy_converges_with/</guid>
      <pubDate>Sat, 09 Nov 2024 11:22:28 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>