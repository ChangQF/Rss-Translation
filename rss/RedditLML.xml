<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Wed, 24 Jan 2024 15:14:51 GMT</lastBuildDate>
    <item>
      <title>需要 pytorch VAE 示例的帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eimpl/need_help_with_pytorch_vae_example/</link>
      <description><![CDATA[亲爱的*， ​ 我可能需要一些帮助来解决我的玩具 VAE问题。我有3个问题： ​  为什么x是encode()中的一个列表？  看起来我必须这样做： x = F.relu(self.fc1(self.embedding(x[0])))  这会影响性能吗? ​ 2) 我无法训练模型，因为我运行时遇到形状问题。 ValueError: 目标尺寸（火炬） .Size([16, 160])) 必须与输入大小相同 (torch.Size([16, 160, 160]))  16 是我的批量大小，160 是输入的长度。所以目标大小代表我的标记化（随机）输入。如何解决此处出现的问题： ​ recon_loss = F.binary_cross_entropy_with_logits(x_recon, x[0],duction=&#39;sum&#39;)  3) 如果我想在某个时刻使用 VAE 进行采样，我该怎么做？有人可以推荐一些基于 pytorch 和 pt-lightning 的好资源吗？ ​ 编辑： 4）我怎样才能真正获得一些潜在的表示，人们通常使用嵌入的地方，例如用于聚类？ ​ ​ 谢谢！ 这是我的代码： 导入torch 导入torch.nn as nn导入torch.optim作为optim从pytorch_lightning导入Trainer从pytorch_lightning.core导入LightningModule从torch.nn导入功能作为F从torch.utils。数据导入 DataLoader, TensorDataset class VAE(nn.Module): def __init__(self, input_dim, hide_dim, Latent_dim): super(VAE, self).__init__() # 编码器 self.embedding = nn.Embedding(input_dim, hide_dim) self .fc1 = nn.Linear(hidden_​​dim, 256) self.fc_mu = nn.Linear(256, Latent_dim) self.fc_logvar = nn.Linear(256, Latent_dim) # 解码器 self.fc2 = nn.Linear(latent_dim, 256) self .fc3 = nn.Linear(256, hide_dim) self.fc4 = nn.Linear(hidden_​​dim, input_dim) self.get_probs = nn.Softmax(dim=1) def 编码(self, x): x = F.relu(self .fc1(self.embedding(x[0]))) mu = self.fc_mu(x) logvar = self.fc_logvar(x) return mu, logvar def reparameterize(self, mu, logvar): std = torch.exp( 0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def 解码(self, z): z = F.relu(self.fc2(z)) z = F.relu(self.fc3(z) )) z = torch.sigmoid(self.fc4(z)) return z defforward(self, x): mu, logvar = self.encode(x) z = self.reparameterize(mu, logvar) x_recon = self.decode (z) x_recon = F.log_softmax(x_recon, dim=-1) 返回 x_recon, mu, logvar 类 VAE_Lightning(LightningModule): def __init__(self, model,learning_rate=1e-3): super(VAE_Lightning, self).__init__ () self.model = 模型 self.learning_rate = Learning_rate def Training_step(self, batch, batch_idx): x = 批次 x_recon, mu, logvar = self.model(x) loss = self.vae_loss(x_recon, x, mu, logvar ) 返回损失 def configure_optimizers(self): return optim.Adam(self.parameters(), lr=self.learning_rate) def vae_loss(self, x_recon, x, mu, logvar): print(x_recon) print(x_recon.shape) print(x) print(x[0].shape) recon_loss = F.binary_cross_entropy_with_logits(x_recon, x[0],duction=&#39;sum&#39;) kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) ) - logvar.exp()) return recon_loss + kl_divergence if __name__ == &#39;__main__&#39;: # 创建 token 的随机张量 length = 160 n_tokens = 30 n_samples = 1000 batch_size = 16 tensor_dataset = torch.randint(0, n_tokens, (n_samples) , length)) dataset = TensorDataset(tensor_dataset) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4) # 初始化 VAE 模型 input_dim = length hide_dim = 64 Latent_dim = 16 # 初始化模型和 LightningModule vae_model = VAE( input_dim, hide_dim, Latent_dim) vae_lightning = VAE_Lightning(vae_model) # 训练模型 trainer = Trainer(max_epochs=10) trainer.fit(vae_lightning, dataloader)  ​   由   提交 /u/onlyrandomthings   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eimpl/need_help_with_pytorch_vae_example/</guid>
      <pubDate>Wed, 24 Jan 2024 14:41:56 GMT</pubDate>
    </item>
    <item>
      <title>Karpathy的课程《神经网络：从零到英雄》是学习NLP的良好开端吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eib9a/is_karpathys_course_neural_networks_zero_to_hero/</link>
      <description><![CDATA[我一直在通过浅层 ML/DL（使用预训练模型进行微调）进行回归、分类和这些任务，并且我有扎实的基础对它的理解。  现在，我有一个宠物项目的想法，但它基本上是一个适合微调语言模型的 NLP 任务。由于我对 NLP 的了解为零，并且希望将其学习到一个不错的（功能）水平，以便对某些应用进行微调，例如HuggingFace 预训练模型很有意义，Karpathy 的课程《从零到英雄》的教学大纲足以满足我的需求吗？ https ://karpathy.ai/zero-to-hero.html 我发现它回归到建立和培训法学硕士，但我不确定这是否是正确的一路上教授所有这些 NLP 基础知识。   由   提交/u/maybenexttime82   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eib9a/is_karpathys_course_neural_networks_zero_to_hero/</guid>
      <pubDate>Wed, 24 Jan 2024 14:27:03 GMT</pubDate>
    </item>
    <item>
      <title>感觉自己什么都不知道</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ei3g2/it_feels_like_i_know_nothing/</link>
      <description><![CDATA[大家好， 几个月前我开始了我的机器学习之旅。这几个月来，我读了两本最受认可的 ML 书籍，看了 3 门课程，全部都是通过编码和项目完成的 但我仍然觉得我什么都不懂，我什至不知道如何开始一个ML 项目 任何关于如何发展我的技能的建议，我对 ML 非常感兴趣，并且想成为该领域的工程师 感谢您的阅读 &lt; /div&gt;  由   提交/u/Herooftime998  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ei3g2/it_feels_like_i_know_nothing/</guid>
      <pubDate>Wed, 24 Jan 2024 14:17:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 LoRA/QLoRA 进行无代码 LLM 微调</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ehtnb/nocode_llm_finetuning_with_loraqlora/</link>
      <description><![CDATA[大家好！ 在 MonsterAPI，我们发布了一个无代码 LLM 微调器，使您能够针对特定领域的任务自定义 LLM使用 LoRA 和 QLoRA，您无需设置任何基础设施或找到合适的批量大小，以避免内存不足等问题。它通过一个界面自动为您完成这一切。 它提供完整的高级设置配置，使您可以完全控制管道，并支持从 Mixtral 8x7B、Llama 2 70B 甚至 SLM 等所有最新型号Tinyllama！ 阅读我们的版本： https://www.linkedin.com/pulse/fine-tune-large-language-model-llm-deploy-monsterapis-rohan-paul-rrzc/  LLM微调入门： https://monsterapi.ai/finetuning 非常感谢您的想法和反馈。欢迎尝试一下，感谢您的支持！   由   提交 /u/gvij   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ehtnb/nocode_llm_finetuning_with_loraqlora/</guid>
      <pubDate>Wed, 24 Jan 2024 14:04:14 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Hugging Face Transformer 构建简单的情感分析器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ehe64/how_to_build_a_simple_sentiment_analyzer_using/</link>
      <description><![CDATA[   /u/manishmanalath  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ehe64/how_to_build_a_simple_sentiment_analyzer_using/</guid>
      <pubDate>Wed, 24 Jan 2024 13:43:50 GMT</pubDate>
    </item>
    <item>
      <title>预训练 VAE MNIST</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eh7gz/pretrained_vae_mnist/</link>
      <description><![CDATA[我很好奇：在哪里可以找到 mnist 数据集上经过预训练的最先进的自动编码器权重？ &lt; !-- SC_ON --&gt;  由   提交/u/amal_ML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eh7gz/pretrained_vae_mnist/</guid>
      <pubDate>Wed, 24 Jan 2024 13:34:23 GMT</pubDate>
    </item>
    <item>
      <title>是否可以使用图像识别从轮辋图像估计车轮规格？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19efgnv/is_it_possible_to_use_image_recognition_to/</link>
      <description><![CDATA[如标题所示。我正在研究构建一个应用程序的可能性，用户可以在其中拍摄汽车轮辋的照片，模型会估计许多变量，包括：PCD、螺母/螺栓长度等。这似乎是一个技术上非常具有挑战性的项目，但我会想知道这是否真的可能。 提前致谢！   由   提交/u/Better_Pollution_114   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19efgnv/is_it_possible_to_use_image_recognition_to/</guid>
      <pubDate>Wed, 24 Jan 2024 11:58:57 GMT</pubDate>
    </item>
    <item>
      <title>Chomsky vs Shannon 的 NLP 和 AI 方法 - Chris Manning 斯坦福大学 OpenNLP 创建者</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ef7ql/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/</link>
      <description><![CDATA[       由   提交 /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ef7ql/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/</guid>
      <pubDate>Wed, 24 Jan 2024 11:43:54 GMT</pubDate>
    </item>
    <item>
      <title>从 ML 论文中学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eel4f/learning_from_ml_papers/</link>
      <description><![CDATA[我一直在努力从机器学习论文中学习并在实践中实施这些概念 有任何具体资源吗？ &gt;   由   提交 /u/sigma_ks   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eel4f/learning_from_ml_papers/</guid>
      <pubDate>Wed, 24 Jan 2024 11:03:38 GMT</pubDate>
    </item>
    <item>
      <title>汉明距离问题。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ee7qw/hamming_distance_problem/</link>
      <description><![CDATA[      ​ A. 4、B. 否，最多 7。 我的看法   由   提交/u/Victor_Paul_  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ee7qw/hamming_distance_problem/</guid>
      <pubDate>Wed, 24 Jan 2024 10:39:28 GMT</pubDate>
    </item>
    <item>
      <title>关于 LSTM 的问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ee4h2/questions_about_lstms/</link>
      <description><![CDATA[所以我观看了 Andrew Ng 的视频并阅读了一些有关 RNN 的 pdf 文件，这样我就掌握了基础知识，但在使用它们时我有一些关于它们的问题在 PyTorch 上。我正在尝试实现自己的自定义 LSTM，所以我只是好奇它是如何在 PyTorch 上实现的。 首先，LSTM 如何批量训练。观察 LSTM 的内部，我发现有一个专门用于输入权重的矩阵（我假设它结合了遗忘门、输入门、控制门和输出门的所有权重）。然而，同样有趣的是，隐藏状态也有一个类似的权重矩阵，但大小与批量大小相关。根据我的推断，这意味着隐藏状态会批量相乘，但隐藏状态并不依赖于它们之前的输入，那么这将如何工作。总的来说，考虑到矩阵大小，我很困惑谁会批量训练 LSTM。  其次，我的输入是二维的，因为它包含序列长度的特征数量，这意味着它需要 n 天的数据作为输入（我的 LSTM 用于时间预测）。我感到困惑的是 LSTM 如何获取这些数据。它会把它压平吗？除了权重矩阵之外，它是否会乘以第二个矩阵，使其变平？我只是不知道。 第三，如何从 PyTorch 中的数据加载器类访问成员？基本上，我试图制作的 LSTM 试图回忆以前的内存值和输入，但当我尝试仅使用传统数组表示法访问数据加载器类中的成员时，我不断收到错误。那么还有哪些其他方法呢？    由   提交/u/Successful-Fee4220   reddit.com/r/learnmachinelearning/comments/19ee4h2/questions_about_lstms/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ee4h2/questions_about_lstms/</guid>
      <pubDate>Wed, 24 Jan 2024 10:33:08 GMT</pubDate>
    </item>
    <item>
      <title>[项目] BELT（较长文本的 BERT）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ee1fb/project_belt_bert_for_longer_texts/</link>
      <description><![CDATA[我们创建了 BELT（BERT For Longer Texts）——一个 Python 包，允许对长度超过 512 个 token 的文本使用类似 BERT 的模型。该方法是 Jacob Devlin 提出的想法的实现，Jacob Devlin 是 评论。您可以在 Medium 上我刚刚发表的两篇文章中阅读有关它的更多详细信息： 第一部分是应用 BERT 分类器的概述： 第 1 部分 第二部分深入介绍我们训练 BELT 模型的方法。 第 2 部分 该存储库已开源： Repo 我知道你在想什么：“等等，bucko，这不是什么新鲜事。每个人都知道有像 BigBird 或 Longformer 这样的模型可以处理更长的文本”。对此我的回答是：“我知道，伙计，但是 BigBird 和 Longformer 不是修改过的 BERT。它们是具有不同架构的模型。因此，它们需要从头开始预训练或下载。 BELT修改模型微调。这带来了 BELT 方法的主要优点 - 它使用任何预先训练的 BERT 或 RoBERTa 模型。快速查看 HuggingFace Hub 可以确认，BERT 的资源比 Longformer 多大约 100 倍。找到适合特定任务或语言的可能会更容易。”享受吧！   由   提交/u/MBrzozowskiML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ee1fb/project_belt_bert_for_longer_texts/</guid>
      <pubDate>Wed, 24 Jan 2024 10:27:40 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们使用准确率而不是二元交叉熵来进行超参数调整？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ebwo7/why_do_we_use_accuracy_instead_of_binary_cross/</link>
      <description><![CDATA[在许多产生结果概率的模型（决策树、随机森林、概率神经网络等）中，我认为准确度是超级的选择指标参数调整（即准确度与历元、准确度与树深度、准确度与minimum_leaf_size等）。二元交叉熵对于这些评估来说也是一个足够的指标吗？我的直觉告诉我确实如此，只是我想多了。大家可以发表一下自己的意见吗？   由   提交/u/Traditional_Soil5753   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ebwo7/why_do_we_use_accuracy_instead_of_binary_cross/</guid>
      <pubDate>Wed, 24 Jan 2024 07:51:11 GMT</pubDate>
    </item>
    <item>
      <title>使用 C++/CUDA 构建的相同模型收敛速度不如使用 Python/NumPy 构建的模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eagxe/identical_model_built_using_ccuda_doesnt_converge/</link>
      <description><![CDATA[我想学习神经网络背后的数学，因此我决定使用 numpy 从头开始​​构建一个 Python 模型来训练 MNIST 数据集。速度非常快，在调整超参数后，在 30 个 epoch 内测试数据集的准确率达到了 98%。作为一个简单的全连接网络，我没想到它会做得更好。所以我决定在 C++ 和 CUDA 中尝试同样的方法。该模型确实可以学习，但收敛速度不那么快。即使经过 1000 个 epoch，其准确性仍然非常可怕。模型架构相同，浮点精度、权重和偏差初始化、小批量大小以及所有超参数都相同。 GPU 在每次内核调用后也会同步。然而该模型的收敛速度似乎并没有那么快。我不知道该怎么做。有人知道可能是什么原因造成的吗？感谢您阅读所有内容。希望您有美好的一天。   由   提交 /u/Hey_DeadGuyHere   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eagxe/identical_model_built_using_ccuda_doesnt_converge/</guid>
      <pubDate>Wed, 24 Jan 2024 06:16:09 GMT</pubDate>
    </item>
    <item>
      <title>这里发生了什么？这只是大规模的过度拟合吗？或者是其他东西？提前致谢。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19e57wk/whats_going_on_here_is_this_just_massive/</link>
      <description><![CDATA[   /u/HoleNother  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19e57wk/whats_going_on_here_is_this_just_massive/</guid>
      <pubDate>Wed, 24 Jan 2024 01:39:25 GMT</pubDate>
    </item>
    </channel>
</rss>