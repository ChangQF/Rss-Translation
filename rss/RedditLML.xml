<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Wed, 24 Jul 2024 06:22:01 GMT</lastBuildDate>
    <item>
      <title>NEURIPS 与 ICML</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eaubfv/neurips_vs_icml/</link>
      <description><![CDATA[      我看了视频片段，但它只涵盖了我们所需解释的一小部分。您是在寻找这个特定的，还是我们应该深入研究另一个？我很高兴收到您的意见并继续前进     提交人    /u/TimeTruthPatience   [link] [comments] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eaubfv/neurips_vs_icml/</guid>
      <pubDate>Wed, 24 Jul 2024 06:20:03 GMT</pubDate>
    </item>
    <item>
      <title>使用 Ollama 的 Llama3.1</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eau89x/llama31_using_ollama/</link>
      <description><![CDATA[  由    /u/mehul_gupta1997  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eau89x/llama31_using_ollama/</guid>
      <pubDate>Wed, 24 Jul 2024 06:14:12 GMT</pubDate>
    </item>
    <item>
      <title>谷歌 Gemini 2.0 和其他未发布的 AI 模型问世</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eau0tl/googles_gemini_20_and_other_unreleased_ai_models/</link>
      <description><![CDATA[        由    /u/Competitive_Luck8606  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eau0tl/googles_gemini_20_and_other_unreleased_ai_models/</guid>
      <pubDate>Wed, 24 Jul 2024 06:00:49 GMT</pubDate>
    </item>
    <item>
      <title>文本分类</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eaqsg5/text_classification/</link>
      <description><![CDATA[目前正在开展一个与文本分类相关的项目。我的数据集不平衡（20% = 1，80% = 0）。我的流程是：1. 数据预处理（例如词干提取、删除停用词），2. 数据建模，3. 预测。 对于数据建模，我运行了多个 ml（例如 SVC、NB、RFC、ADA、GB），其中 SVC、RFC 和 ADA 表现最佳。因此，我对它们进行了相应的调整，并获得了它们的超参数以供调整。调整后，我将它们堆叠起来，并将 ADA 作为元模型。 我甚至尝试了 LSTM、RNN 和 Transformer。但即使准确率达到 95% 以上，我仍然没有得到我想要的预测。 不确定哪里出了问题。并且需要关于从现在开始如何处理这个问题的建议。 我们正在考虑使用 hugging face，但正在考虑它的稳定性。 可以从 HuggingFace 下载模型吗？例如 mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis    提交人    /u/anixouskid   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eaqsg5/text_classification/</guid>
      <pubDate>Wed, 24 Jul 2024 02:53:33 GMT</pubDate>
    </item>
    <item>
      <title>如果我们可以使用python进行数据预处理和清理，为什么还需要学习SQL？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eamt74/why_do_we_need_to_learn_sql_if_we_can_just_do_the/</link>
      <description><![CDATA[  由    /u/BEE_LLO  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eamt74/why_do_we_need_to_learn_sql_if_we_can_just_do_the/</guid>
      <pubDate>Tue, 23 Jul 2024 23:42:22 GMT</pubDate>
    </item>
    <item>
      <title>为什么第一次推论往往比后来的推论长得多？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eal8o9/why_is_first_inference_often_much_longer_than/</link>
      <description><![CDATA[我正在开发一个应用程序，对经典和较新的机器学习和人工智能概念半熟悉，我一直观察到的一个现象是，在加载模型后，我第一次运行推理时所花的时间比后续推理要长得多，但我真的不知道为什么。 例如，第一次 whisper 采样需要 4 秒钟，然后所有后续采样都需要 ~400ms。对于槽填充/意图检测模型也是如此，还有更多。 我知道一些训练序列涉及热身，但我不认为这与此有关。 我对此的直觉是，它在某种程度上与缓存和其他加速技术有关，但我不确定。  我还想知道什么会被视为正确的报告指标，非初始推断还是两者兼而有之，以及在更大规模的应用程序（例如语音助手）中，我应该期望推断是更快还是更慢的时间？ 我相信我在第一次推断之前也初始化了模型。 任何评论或见解都值得赞赏！    提交人    /u/ethan3048   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eal8o9/why_is_first_inference_often_much_longer_than/</guid>
      <pubDate>Tue, 23 Jul 2024 22:34:14 GMT</pubDate>
    </item>
    <item>
      <title>Llama 3.1 现已发布。如何通过 API 访问它？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eak8gb/llama_31_is_here_how_do_i_access_it_through_an_api/</link>
      <description><![CDATA[Meta 刚刚发布了其最新的 LLM 模型 Llama 3.1，这标志着无障碍人工智能迈出了重要一步。以下是公告中的要点：  405B 版本。有一个新的 Llama 3.1 405B 版本。没错，4050 亿个参数。 扩展上下文长度：现在所有 Llama 3.1 模型都提供 128K 标记 的上下文长度，是 Llama 3 之前 8K 上下文长度的 16 倍。这允许更高级的用例，例如长格式文本摘要、多语言对话代理和编码助手 模型评估：Meta 发布的模型评估如下： 405B 模型评估 8B 模型评估  太好了，我该怎么尝试呢？ 有兴趣尝试新 Llama 3.1 模型的用户可以在 Meta 官方网站上免费试用：meta.ai API 调用怎么样？ Meta 已与 25 家云提供商合作，包括 AWS、NVIDIA、Databricks、Groq、Dell、Azure 和 Google Cloud，以供企业使用。但是，如果您正在尝试构建一个基本的应用程序，或者为一家初创公司构建 MVP，awanllm.com 即将宣布推出一个可供使用的 Llama 3.1 API。  来源：https://ai.meta.com/blog/meta-llama-3-1/    提交人    /u/Acanthocephala_Salt   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eak8gb/llama_31_is_here_how_do_i_access_it_through_an_api/</guid>
      <pubDate>Tue, 23 Jul 2024 21:51:58 GMT</pubDate>
    </item>
    <item>
      <title>我想成为一名专门从事计算机视觉的机器人工程师</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eaj7xx/i_want_to_be_a_robotics_engineering_specialized/</link>
      <description><![CDATA[嗨，我想成为一名专门从事计算机视觉的机器人工程师，我需要学习计算机视觉方面的哪些算法、哪些技术，基本上需要有一个路线图才能胜任这个角色，我是刚毕业的学生，​​我对机器学习、生成人工智能和深度学习有很好的了解。我想掌握这个角色所需的技能。请给我建议资源和路线图，谢谢    提交人    /u/Rare-Breed420   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eaj7xx/i_want_to_be_a_robotics_engineering_specialized/</guid>
      <pubDate>Tue, 23 Jul 2024 21:10:51 GMT</pubDate>
    </item>
    <item>
      <title>使用 DataChain 进行多模式数据管理</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eaivff/multimodal_data_curation_with_datachain/</link>
      <description><![CDATA[大家好！今天我们开源了https://github.com/iterative/datachain 它包括一个多模态数据管理教程，教你如何清理和合并图像和文本数据，为使用 PyTorch 进行 ML 训练做好准备。 教程：  Jupyter Notebook：https://github.com/iterative/datachain/blob/main/examples/multimodal/clip_fine_tuning.ipynb Colab：https://colab.research.google.com/github/iterative/datachain/blob/main/examples/multimodal/clip_fine_tuning.ipynb  请分享您的反馈！    由   提交  /u/dmpetrov   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eaivff/multimodal_data_curation_with_datachain/</guid>
      <pubDate>Tue, 23 Jul 2024 20:57:05 GMT</pubDate>
    </item>
    <item>
      <title>即使数据较少，ML 模型也需要花费太多时间进行训练</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eacg1t/ml_model_taking_too_much_time_to_train_even_with/</link>
      <description><![CDATA[我目前正在尝试训练一个使用视频数据的模型作为我最后一年的项目。我已经将原始数据集（10 GB）的大小显著减少到 500 MB 以下。但训练每个时期仍然需要 3 个小时。总共 20 个时期。我应该让我的笔记本电脑开着 60 个小时吗？有什么策略可以用来分割工作吗？让我的笔记本电脑开着太久似乎不是一个好主意，特别是考虑到我在创建包含更多数据的模型之前只想创建一个迷你模型。请帮我解决这个问题。我不能花钱购买 gpu 等额外资源。我只有我的笔记本电脑可以做这件事。    提交人    /u/HeronEfficient7063   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eacg1t/ml_model_taking_too_much_time_to_train_even_with/</guid>
      <pubDate>Tue, 23 Jul 2024 16:38:23 GMT</pubDate>
    </item>
    <item>
      <title>Meta 发布 Llama 3.1</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eaajqb/meta_releases_llama_31/</link>
      <description><![CDATA[      Meta 的最新模型现已发布8B、70B 和 405B 版本    由    /u/aifordevs 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eaajqb/meta_releases_llama_31/</guid>
      <pubDate>Tue, 23 Jul 2024 15:22:30 GMT</pubDate>
    </item>
    <item>
      <title>AI 日报：第 1 周回顾 - 回归和指标</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eaaawe/ai_daily_week_1_recap_regression_and_metrics/</link>
      <description><![CDATA[       由    /u/Avienir  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eaaawe/ai_daily_week_1_recap_regression_and_metrics/</guid>
      <pubDate>Tue, 23 Jul 2024 15:12:59 GMT</pubDate>
    </item>
    <item>
      <title>我想成为一名机器学习研究员</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ea6t8y/i_want_to_become_a_researcher_in_machine_learning/</link>
      <description><![CDATA[所以，我一直想成为一名机器学习研究员。因此，我想问问我是否应该攻读计算机科学或数学学士学位，以走这条路。谢谢！    提交人    /u/MohNiz   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ea6t8y/i_want_to_become_a_researcher_in_machine_learning/</guid>
      <pubDate>Tue, 23 Jul 2024 12:39:35 GMT</pubDate>
    </item>
    <item>
      <title>深入学习感知，用代码示例解释其背后的数学原理</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ea39by/learn_perception_in_deep_level_explained_the_math/</link>
      <description><![CDATA[        提交人    /u/ingoampt-com   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ea39by/learn_perception_in_deep_level_explained_the_math/</guid>
      <pubDate>Tue, 23 Jul 2024 09:10:22 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>