<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Tue, 13 Feb 2024 00:57:26 GMT</lastBuildDate>
    <item>
      <title>重新学习机器学习：寻找导师和指导</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apfwjm/relearning_machine_learning_looking_for_mentors/</link>
      <description><![CDATA[大家好！我目前是一名软件工程师，希望转向应用机器学习。 我学习了优化与优化。我和滑铁卢大学的统计数据希望为开源项目做出贡献，或者与人们合作解决现有问题。 （医疗保健、农业技术、无人机、体育、人工智能安全和可解释性）。  我非常喜欢这个社区的存在，非常感谢每个人让这个社区变得如此易于访问。  我之前曾担任数据科学家，为 CLIF bar 等包装产品的业务成果提供预测。我主要从事传统的机器学习、语言模型（当时是 SoTA 时的 BERT）以及构建模型管道和数据准备/清理。  我目前正在学习 LLM、LLMOps 并重新学习 MLOps。我想学习如何在设备上提供 ML 模型，并希望在明年大幅提高自己的技能。  绝对是在寻找导师！有我可以联系的人吗？  我住在美国东部时间，但也愿意跨时区工作。我可以根据要求提供我的简历。  谢谢大家！   由   提交/u/No-Juggernaut-2920   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apfwjm/relearning_machine_learning_looking_for_mentors/</guid>
      <pubDate>Tue, 13 Feb 2024 00:32:50 GMT</pubDate>
    </item>
    <item>
      <title>我正在训练一个用于异常检测的自动编码器。如何使用自定义损失函数将相似数据合并到训练中？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apfpdp/i_am_training_an_autoencoder_for_anomaly/</link>
      <description><![CDATA[本文建议采用以下算法将异常数据点合并到训练过程中： 算法 1 自动编码二元分类器 在未收敛时执行：样本小批量 {(x1, y1), · · · , (来自数据集的 xK , yK )} 将 θ 的梯度计算为 g0（使用包含 x 标签的自定义损失函数）以 gθ 结尾对 θ 执行 SGD 更新，同时  我有使用 keras 实现的自动编码器的以下代码： class AnomalyDetector(Model): def __init__(self): super(AnomalyDetector, self).__init__() self.encoder = tf.keras.Sequential ([ 层.Dense(32，激活=“relu”)，层.Dense(16，激活=“relu”)，层.Dense(8，激活=“relu”)]) self.decoder = tf .keras.Sequential([layers.Dense(16，activation=“relu”)，layers.Dense(32，activation=“relu”)，layers.Dense(140，activation=“sigmoid”)]) def call（self，x）：encoded = self.encoder（x）decoded = self.decoder（encoded）返回解码的autoencoder = AnomalyDetector（） 我的主要问题是：如何包含如果自动编码器的输入和输出必须是 X，则自定义损失函数中的标签。假设我将标签编码为特征，在推理过程中，标签将不可用，所以我不确定如何实现由论文。   由   提交 /u/Exciting-Ordinary133    reddit.com/r/learnmachinelearning/comments/1apfpdp/i_am_training_an_autoencoder_for_anomaly/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apfpdp/i_am_training_an_autoencoder_for_anomaly/</guid>
      <pubDate>Tue, 13 Feb 2024 00:23:33 GMT</pubDate>
    </item>
    <item>
      <title>在线学习（递归最小二乘）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apey82/online_learning_recursive_least_squares/</link>
      <description><![CDATA[您好， 我正在尝试使用递归最小二乘作为算法的一部分来更新最小二乘回归的系数随着时间的推移，新数据不断出现。然而，当使用简单的实现时，结果在数值上非常不稳定，并且我的系数很快就会爆炸。简单浏览一下Google Scholar，这是递归最小二乘法的一个常见问题。我没有看到任何好的 scikit-learn 实现，而且 statsmodel 实现也不稳定。  有谁知道稳定递归最小二乘法或任何新算法的实现吗？我宁愿不使用随机梯度下降或任何近似方法，我想要一种方法来获得精确的 OLS 解决方案，但具有更好的时间复杂度。   由   提交/u/fyre87  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apey82/online_learning_recursive_least_squares/</guid>
      <pubDate>Mon, 12 Feb 2024 23:49:55 GMT</pubDate>
    </item>
    <item>
      <title>这很可能是一个愚蠢的帖子..但需要一个路线图</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apetpy/this_is_highly_likely_to_be_a_dumb_post_but_need/</link>
      <description><![CDATA[我不是计算机科学学生。我主修艺术。但我学的是前端开发，所以不是第一次接触CS相关的东西。 我想学习与LLM一起工作。我知道我需要定义“工作”在这里，但坦率地说，我自己仍在制定定义。 我的目标是能够在大约 5 年内创建基于 LLM 的 AI 产品。所以我想尽可能多地学习。那么我从哪里开始呢？ 是这样的：https://roadmap.sh/ai-data-scientist&lt; /a&gt; 有什么好的方法吗？ 我了解 Python 和基本的 DSA。   由   提交/u/liberaltilltheend   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apetpy/this_is_highly_likely_to_be_a_dumb_post_but_need/</guid>
      <pubDate>Mon, 12 Feb 2024 23:44:18 GMT</pubDate>
    </item>
    <item>
      <title>如何在论文中“通过示例进行工作”？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apcyoa/how_to_work_through_the_examples_in_a_paper/</link>
      <description><![CDATA[我的任务是编写 Python 代码来完成本文中的示例：https://www.pnas.org/doi/10.1073/pnas.0500334102 问题是我以前从未做过这样的事情，而且我什至不知道论文的哪些部分可以作为我应该完成的示例。如果有人能够快速浏览这篇论文并帮助我识别它们，我将不胜感激。   由   提交/u/tofe_lemon  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apcyoa/how_to_work_through_the_examples_in_a_paper/</guid>
      <pubDate>Mon, 12 Feb 2024 22:25:19 GMT</pubDate>
    </item>
    <item>
      <title>遗传算法 (PyGAD) 使用 16 笔画 + 源代码绘制逼真的蒙娜丽莎</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apbnzg/genetic_algorithm_pygad_paints_realistic_monalisa/</link>
      <description><![CDATA[       由   提交 /u/ahmed26gad   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apbnzg/genetic_algorithm_pygad_paints_realistic_monalisa/</guid>
      <pubDate>Mon, 12 Feb 2024 21:32:26 GMT</pubDate>
    </item>
    <item>
      <title>如何使用多个数据集训练 LSTM？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1apa3a2/how_to_train_lstm_with_multiple_datasets/</link>
      <description><![CDATA[我正在尝试训练 LSTM 以解决时间序列预测的流体动力学问题。我有 4 个不同的数据集，每个数据集都有 2400 个时间步长和 6 个特征。我想训练 lstm 使其能够学习所有 4 个数据集的共同动态（因为它们在相同参数下运行本质上是不同的）。有人可以建议如何解决这个问题吗？ 我正在使用 keras，并且已经能够成功地训练和预测单个数据集以进行多对多预测。   由   提交/u/Born-Plankton2373  /u/Born-Plankton2373 reddit.com/r/learnmachinelearning/comments/1apa3a2/how_to_train_lstm_with_multiple_datasets/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1apa3a2/how_to_train_lstm_with_multiple_datasets/</guid>
      <pubDate>Mon, 12 Feb 2024 20:29:52 GMT</pubDate>
    </item>
    <item>
      <title>字错误率 (WER) 解释</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap9bh7/word_error_rate_wer_explained/</link>
      <description><![CDATA[       由   提交/u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap9bh7/word_error_rate_wer_explained/</guid>
      <pubDate>Mon, 12 Feb 2024 20:00:10 GMT</pubDate>
    </item>
    <item>
      <title>解码后的预测输出始终是标记列表中的空字符串，但预测看起来不错。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap946i/predicted_output_after_decoding_is_always_empty/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap946i/predicted_output_after_decoding_is_always_empty/</guid>
      <pubDate>Mon, 12 Feb 2024 19:52:03 GMT</pubDate>
    </item>
    <item>
      <title>了解梯度下降和损失函数</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap85k0/understanding_gradient_descent_and_loss_function/</link>
      <description><![CDATA[我很难理解梯度下降和损失函数之间的关系。读了这么多文章，我得出的结论是，对于线性回归模型，调整权重的方向是由梯度下降决定的，而根据差异调整权重的多少是由损失函数计算的。    由   提交/u/Ok_Source_9844   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap85k0/understanding_gradient_descent_and_loss_function/</guid>
      <pubDate>Mon, 12 Feb 2024 19:14:14 GMT</pubDate>
    </item>
    <item>
      <title>Kaggle Enfit 竞赛：掌握时间序列 ML 策略的基础知识</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap6k0e/kaggle_enfit_contest_basics_to_grand_master_ml/</link>
      <description><![CDATA[       由   提交/u/BenLebovitz   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap6k0e/kaggle_enfit_contest_basics_to_grand_master_ml/</guid>
      <pubDate>Mon, 12 Feb 2024 18:10:48 GMT</pubDate>
    </item>
    <item>
      <title>教授超过“X 小时机器学习课程”的资源？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap3f29/resources_that_teach_more_than_x_hours_machine/</link>
      <description><![CDATA[我在 youtube 上学习了一些课程，只是为了习惯这些框架。我对神经网络背后的基本概念有了理论上的理解，并且很好地掌握了其背后的数学。 但是，似乎存在一个奇怪的差距，我看到每个人都只是在教授一个非常简单的架构，或者非常复杂的东西。  我尝试实现一篇研究论文（只是为了看看我不知道多少，我拿了一篇旧论文）。我对我以为自己学到的东西感到非常失望。  是否有任何资源可以指导您了解更复杂的架构，并教您解析研究论文及其有时非常令人困惑的符号？  &amp; #32；由   提交 /u/AudienceOpening4531   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap3f29/resources_that_teach_more_than_x_hours_machine/</guid>
      <pubDate>Mon, 12 Feb 2024 16:05:02 GMT</pubDate>
    </item>
    <item>
      <title>LangChain 播放列表，包含 60 个教程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap1uy7/langchain_playlist_with_60_tutorials/</link>
      <description><![CDATA[大家好，看看这个 LangChain（生成式 AI 框架）播放列表，其中包含 60 个从头开始解释一切的教程 https://youtube.com/playlist?list=PLnH2pfPCPZsKJnAIPimrZaKwStQrLSNIQ&amp;si=8bXhqED-NiVITZK9   由   提交/u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap1uy7/langchain_playlist_with_60_tutorials/</guid>
      <pubDate>Mon, 12 Feb 2024 14:56:59 GMT</pubDate>
    </item>
    <item>
      <title>商业数据科学</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ap0ymc/data_science_for_business/</link>
      <description><![CDATA[我正在参加商业数据科学考试，它更侧重于使用 R 进行机器学习，主题包括基于树的方法和移动等超越线性。讲座没有提供任何示例考试问题，但他坚持将分为 1. 多个问题 2. 开放式问题 3. 代码问题 如果您有任何示例问题或网站，我可以用来测试我的考试技能，我会欣赏。   由   提交/u/Warm_Bell23  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ap0ymc/data_science_for_business/</guid>
      <pubDate>Mon, 12 Feb 2024 14:15:12 GMT</pubDate>
    </item>
    <item>
      <title>【研究】《Attention is All You Need》中的多头自注意力公式是否存在误解？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1aoz7hj/research_is_there_a_misunderstanding_in_the/</link>
      <description><![CDATA[   大家好， 希望你们一切顺利。我一直在研究 Transformer 架构，并且在开创性论文“Attention is All You Need”中提出的多头自注意力 (MHSA) 公式方面遇到了障碍。  ​ https:// /preview.redd.it/9ap9c5ieh5ic1.png?width=715&amp;format=png&amp;auto=webp&amp;s=2569326828c63eda509def99f0b5eb08c8ec159e 论文中指出，每个注意力头，表示为head(i) 是使用 QW^Q_i、KW^K_i 和 VW^V_i 计算的。我对 QW^Q_i 一词感到困惑。如果我们已经将 Q 定义为输入嵌入 X 和权重矩阵 W^Q 的乘积（即 Q = XW^Q），那么 QW^Q_i 代表什么？这是符号的简化，还是意味着在我们获得 Q、K 和 V 向量之后、执行注意力计算之前还有额外的可学习参数发挥作用？我们将非常感谢您就此事提供的任何见解或澄清。谢谢！   由   提交/u/amt_42  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1aoz7hj/research_is_there_a_misunderstanding_in_the/</guid>
      <pubDate>Mon, 12 Feb 2024 12:45:49 GMT</pubDate>
    </item>
    </channel>
</rss>