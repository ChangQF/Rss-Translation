<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>欢迎来到R/LearnMachineLearning-一个学习者和教育工作者对机器学习充满热情的社区！这是您提出问题，共享资源并共同发展的空间，从理解ML概念到从基本原理到高级技术。无论您是写第一个神经网络还是潜入变压器，都可以在这里找到支持者。用于ML研究， /R /MachineLearning用于简历审查， /R /工程介绍ML工程师， /r /mlengineering</description>
    <lastBuildDate>Mon, 03 Mar 2025 06:27:03 GMT</lastBuildDate>
    <item>
      <title>我可以实施哪些非琐碎的NLP论文？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j2ccwm/what_are_some_non_trivial_nlp_papers_i_can/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/dark_matter22     [links]    [注释]      ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j2ccwm/what_are_some_non_trivial_nlp_papers_i_can/</guid>
      <pubDate>Mon, 03 Mar 2025 06:22:01 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost模型的ZKML实现</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j2aha4/zkml_implementation_for_xgboost_model/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  你好，我想在模型上使用ezkl来实现ZKP，但是它是XGBRegressor模型不是神经网络，因此EZKL不支持它，我该怎么办？还有其他支持的框架吗？ 谢谢  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/_xd22     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j2aha4/zkml_implementation_for_xgboost_model/</guid>
      <pubDate>Mon, 03 Mar 2025 04:26:27 GMT</pubDate>
    </item>
    <item>
      <title>寻求帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j2a9yb/seeking_some_help/</link>
      <description><![CDATA[         a project i&#39;m seeking help in is working on a dataset to predict the blood pressure abnormality,the data is cleaned and filled in and the features being selected by performing uni,bi and multivariate analysis ,and also feature selection with binary tree,correlation and information value it gives me amazing accuracy ,f1 score but the problem being its not able to classify as it gives the output according to the probablity,the model beimg too accurate seems a bit fishy to me as i&#39;ve tried using二进制分类，洛斯氏回归，随机森林和XG的提升，但似乎给我带来了几乎相同的结果，成为实习生，我的导师建议在其他一些数据集上工作，但尝试在3个不同的数据集上进行工作，这给了相同的结果，而不是在这里获得问题，以寻求一些帮助。 href =“ https://preview.redd.it/wizu1hrdheme1.png?width = 2353＆amp; amp; format = png＆amp; amp; amp; amp; amp; amp; am https://preview.redd.it/wizu1hrdheme1.png?width = 2353＆amp; amp; format = png＆amp; amp; amp; auto = webppumppumpp；  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/ok_chain6866     [link]    32;   [注释]    ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j2a9yb/seeking_some_help/</guid>
      <pubDate>Mon, 03 Mar 2025 04:14:59 GMT</pubDate>
    </item>
    <item>
      <title>预处理图像</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j28ekp/preprocessing_images/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有一小部分.TIFF图像，我想用作培训数据。它们的大小并不相同，有些的分辨率与其他分辨率不同。我只需要帮助它们标准化。 IE。将它们全部转换为具有相同维度和分辨率的图像。当然，我不在乎获得实际的标准化图像，所以我的确的意思是具有代表图像的相同尺寸的数组，并且它们都可以在灰度中表示。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/eefmu     ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j28ekp/preprocessing_images/</guid>
      <pubDate>Mon, 03 Mar 2025 02:32:34 GMT</pubDate>
    </item>
    <item>
      <title>来自信誉良好机构的动手课程/证书</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j28cmo/handson_coursecert_recs_from_reputable/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨， 寻找课程和/或认证的建议，最好少于$ 2000美元，但从知名的机构或可能是事实上的标准或知名课程所视为的东西。大约13年来，我一直在扮演各种不同的角色，其中所有这些都包括一些软件开发人员，想真正学习如何利用ML并更多地了解如今的所有可用内容。 我购买了Géron的动手机器学习书，并从中获得了大约20％的手机学习书，但我怀疑大部分时间，我怀疑它可能是在此之前的大部分。我在过去的24小时内看到了一篇文章，说Keras和TF如今并不常见。 我正在看Caltech ML Bootcamp，但这是$ 8000 ...  ，很高兴能找到一些涵盖某些理论的东西，两者主要集中在实际实施上，这是很高兴的。 “我如何采用这种验证的模型并满足我的需求”。并使您熟悉那里的所有图书馆，工具和LLM。 另外，我也很欣赏这已被要求100000次，但始终有助于获得新鲜的意见，尤其是在不断变化的领域中。   thx。提交由＆＃32;态href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j28cmo/handson_coursecert_recs_from_reputable/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j28cmo/handson_coursecert_recs_from_reputable/</guid>
      <pubDate>Mon, 03 Mar 2025 02:29:47 GMT</pubDate>
    </item>
    <item>
      <title>草稿链：即兴思想链促使</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j272fe/chain_of_drafts_improvised_chain_of_thoughts/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/mehul_gupta1997       [link]   &lt;a href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j272fe/chain_of_drafts_improvise_chain_chain_of_thoughts/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j272fe/chain_of_drafts_improvised_chain_of_thoughts/</guid>
      <pubDate>Mon, 03 Mar 2025 01:23:18 GMT</pubDate>
    </item>
    <item>
      <title>运行LLMS真的很便宜吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j26wxz/is_it_really_this_cheap_to_run_llms/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我目前正在弄清楚哪种大型语言模型对游戏翻译来说是最具成本效益的。我希望将它们用于翻译似乎已经丢失的旧PlayStation游戏。但是，我没有资源可以在本地运行其中一个庞然大物，并选择使用提供商。 我执行了一般计算，但觉得它们总是以便宜的结尾出现？假设对话率约为100,000行，每个对话框约为200个令牌（我知道过分杀了，但最好比以下方面高估了）。我将花费我〜$ 100，最小的价格约为8美元（Llama 3.3），最大值约为350美元（Claude 3.7十四行诗）。  $ 8美元，100,000行对话框！真的？&lt; /p&gt; 这是公司提供的成本&lt; /p&gt;  llama 3.3 70b指令，in：$ 0.12 /m，out：$ 0.30 /m = $ 8.40  gemini 2.0 flash，in：$ 0.15 /m。 OUT：$ 0.60/m = $ 15.00  我是否在某处错过了一些隐藏的费用？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/low-information389    href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j26wxz/is_it_it_really_this_cheap_to_run_llms/”&gt; [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j26wxz/is_it_really_this_cheap_to_run_llms/</guid>
      <pubDate>Mon, 03 Mar 2025 01:15:32 GMT</pubDate>
    </item>
    <item>
      <title>训练错误加权损耗功能优化（批评）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j262sf/training_error_weighted_loss_function/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，所以我正在努力一个想法，我在以前的运行中使用模型的训练错误（即，我将与计算出的损失相乘（1-精度）。对我的问题的快速描述：这是一个多公开的多类分类问题。因此，我训练模型，为每个输出目标获得每键准确性。我使用这种每箱精度来计算每箱“难度”。 （即1-精度）。我将这种难度值用作下一个训练环上损失的每个损失的重量/系数。 ，请使用第一个图像，有15个垃圾箱。中间垃圾箱中红色类的准确性是（0.2，我将使用1-0.2 = 0.8在该箱中的每个值中获得我的损失功能重量，这意味着代表“该箱中的示例的难度”，因此我最终将在下一个训练中的所有示例中乘以0.8的损失，即在下一个训练中的0.8，即在下一步的估算中，我会应用这些值。同样，如果垃圾箱的准确性为0.9，我会得到“我的重量”。使用1-0.9 = 0.1，然后我将该垃圾箱中所有示例的所有计算损失乘以0.1。 该想法的目标是：  相反类的准确性（即降低了中心的准确性均准确的bl blue cember bin的准确性），请&lt;降低绿色曲线的准确性。 (e.g the middle bin in the first image). This is more of an expectation (by members of my team) but I&#39;m not sure if this can be achieved:  Reach a steady state, say iteration j, whereby the plots of each of my output targets at iteration j is similar to the plot at iteration j + 1   Also, I start off the training loop with an  init_weights = 1，weights = init_weights （我的理解是，这类似于设置还原=平均值，在交叉熵损耗函数中类似）。然后在随后的运行中，我应用权重= 0.5 * init_weights + 0.5 *（1-accuracy_per_bin）。我附加了两个输出目标（1C0_I和2AB_I）的图像，显​​示了4个迭代后的改进。 我会感谢人们对这个想法的一些一般批评，基本上是我可以做得更好/可以做的更好或其他事情要尝试。我注意到的一件事是，这会导致对训练集的过度拟合（我还不确定为什么）。        &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/sysy_ad_1214     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j262sf/training_error_weighted_loss_function/</guid>
      <pubDate>Mon, 03 Mar 2025 00:33:23 GMT</pubDate>
    </item>
    <item>
      <title>在训练ESRGAN模型时，对于数据集而言，重要的是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j25eq4/what_is_important_regarding_the_datasets_when/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我想训练自己的高档模型。为什么？首先，最重要的是高档动画节目。 &lt; /p&gt;  13600K  rtx 3090  64GB ram  2tb sn850x &lt; /p&gt;  tl; dr; dr：是否可以改变图像尺寸？或其他东西是其他方面的东西，例如。 512x512p或1024x1024p。 如果我将模型基于20分钟的插曲，我可以将整个插曲渲染到图像上并训练成千上万的框架吗？删除长长的黑色和/或白色图像的长时间的一部分；缺乏信息的图像。 有没有帮助的提示或提示？  I am going to make more than just one model, of course (if I manage to make one again) but if I go for +30K images it&#39;s going to take a while between my attempts. So I am going to experiment with processing the LQ datasets in different ways. Longer text:  I did get the ESRGAN model to train and just did a quick test (only 20000 iterations).尝试并失败的培训是一种较新的模型（RGT），因为据说Esrgan已经过时；但是我无法工作。 &lt; / p&gt; 有人可以从科学语言中翻译出Esrgan做什么 /对数据集很重要的东西吗？  据我了解，训练模型的一种方法是具有高清晰度图像（HQ集），然后您将其降低（LQ集），因此它将“参见”这就是外观（LQ），但看起来像这样（HQ）。这就是我在快速测试中所做的。  我所看到的指南说您应该拥有总部是正方形图像；例如512x512像素，LQ设置为例如256x256像素。只有一个指南以随机比率设置为HQ，然后LQ与HQ对应物成比例地设置为尺寸。 &lt; /p&gt; 图像尺寸 /比率是否重要？我要训练的来源是720x576p。 &lt; /p&gt; 清理总部（去除胶片，灰尘，划痕），但离开LQ设置不受欢迎（但降低尺寸）。我不知道去除灰尘和划痕是否会影响是否会删除其他细节。重复。 .. 的答案如果HQ集有一个绿色的字符，我将角色呈蓝色绘制，如果将那个角色视为蓝色的角色，则将其呈绿色变化，但上下文很重要，但是是多少？蓝色至绿色？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/misaria     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j25eq4/what_is_important_regarding_the_datasets_when/</guid>
      <pubDate>Mon, 03 Mar 2025 00:01:00 GMT</pubDate>
    </item>
    <item>
      <title>需要帮助实习/工作</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j21gre/need_help_internjob/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我需要帮助来获得工作或实习。我已经学习ML和NLP已有一年了，但是我仍然无法找到任何工作。我很难生存。请帮助！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j21gre/need_help_internjob/”&gt; [link]    32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j21gre/need_help_internjob/</guid>
      <pubDate>Sun, 02 Mar 2025 21:07:01 GMT</pubDate>
    </item>
    <item>
      <title>如果有人在人工智能和培训模型方面有一定的知识，请提供帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j1xe4p/if_anyone_has_some_knowledge_in_artificial/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  模型= tf.keras.models.sequesential（[tf.keras.layers.input（shape =（42，）），＃21键键 *（x，x，x，x，y） activation =; relu＆quot;），tf.keras.layers.dense（num_classes，activation =; softmax＆quot; softmax＆quot; softmax＆quort; upture layer]） 您认为该模型是什么。多层perceptron或什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/personalbumblebee754     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j1xe4p/if_anyone_has_some_knowledge_in_artificial/</guid>
      <pubDate>Sun, 02 Mar 2025 18:17:46 GMT</pubDate>
    </item>
    <item>
      <title>你们有没有人从事过这样的工具？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j1tlfs/has_anyone_of_yall_worked_on_a_tool_like_this/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  大家好！因此，我最近一直深入AI，偶然发现了听起来超级酷的东西。这是一个AI工具，而不是根据现有的5分钟视频录制U创建MN AI克隆，它可以像数字双胞胎一样创建自己的AI版本，而IDRA则是Indra Innovativ，但我对创建过程感到好奇。这是关于面部识别和语音映射的全部内容吗？哪些工具可以用来创建这样的克隆？有什么想法吗？我是这个的新手。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/cupicake99      &lt;a href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j1tlfs/has_anyone_of_yall_worked_worked_worked_oon_a_a_a_tool_tool_like_like_this/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j1tlfs/has_anyone_of_yall_worked_on_a_tool_like_this/</guid>
      <pubDate>Sun, 02 Mar 2025 15:39:39 GMT</pubDate>
    </item>
    <item>
      <title>哪个是学习ML的更好来源？ O'Reilly Hands ML Book还是Andrew Ng Coursera课程？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j1qug5/which_is_the_better_source_for_learning_ml/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我个人更喜欢文档而不是视频，但想知道哪个是最好的来源。   &lt;！ -  sc_on- sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/hamstermolester6969      [注释]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j1qug5/which_is_the_better_source_for_learning_ml/</guid>
      <pubDate>Sun, 02 Mar 2025 13:28:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么要softmax引起注意？为什么只有一个标记对只有一个标量？ 2来自好奇初学者的问题。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1j1nq6r/why_softmax_for_attention_why_just_one_scalar_per/</link>
      <description><![CDATA[Hi, I just watched 3Blue1Brown’s transformer series, and I have a couple of questions that are bugging me and chatgpt couldn&#39;t help me :(  Why does attention use softmax instead of something like sigmoid? It seems like words should have their own independent importance rather than competing in a probability distribution. Wouldn&#39;t sigmoid allow for a更重要的是，更重要的是    为什么查询和钥匙只能计算出一个象征性对的单个标量，因为两个令牌并不是一个含义  [link]  &lt;a href =“ https://www.reddit.com/r/learlenmachinelearning/comments/1j1nq6r/1j1nq6r/why_softmax_for_terention_why_just_just_one_one_scalar_scalar_scalar_per/]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1j1nq6r/why_softmax_for_attention_why_just_one_scalar_per/</guid>
      <pubDate>Sun, 02 Mar 2025 10:12:09 GMT</pubDate>
    </item>
    <item>
      <title>与机器学习相关的简历评论帖子</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  请礼貌地将有关简历评论的任何帖子重定向到这里   对于那些正在寻找简历评论的人，请在 href =“ href =“ https://www.reddit.com/r/resumes”&gt;/r/简历或 r/EngineeringResumes 首先，然后在此处交叉crosspost。   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/techrat_reddit     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>