<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sat, 09 Dec 2023 15:12:29 GMT</lastBuildDate>
    <item>
      <title>我不明白 CLAP/CLIP 论文对比损失函数。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18ef8ot/i_dont_understand_clapclip_paper_contrastive_loss/</link>
      <description><![CDATA[      ​ https://preview.redd.it/4caumg5c9a5c1.png?width=1716&amp;format=png&amp; ;auto=webp&amp;s=05638bed653441bc04144f120a0de3d01533f513 有人可以帮我理解这篇对比学习预训练论文吗？此解释来自 https://arxiv.org/pdf/2206.04769.pdf（第 2 页）。据我所知，他们将音频编码器应用于信号，将文本编码器应用于文本（等式1），然后他们通过线性层（等式2）将两个编码器映射到相同的大小，因为两个学习的线性投影具有相同的尺寸，您现在可以计算点积或余弦相似度，就像在方程 (3) 中使用温度因子“Tau”所做的那样。我的问题在这里：到目前为止，C（方程 3）是音频和文本表示之间的 NxN 相似度矩阵。我猜这个矩阵是对称的，对吗？在等式（4）中，他们计算“音频损失”和“文本损失”的平均值，他们说“l_k = 1/N sum ( log (diag (softmax C) ，但我看到 C 是相同的对于 Loss_text 和 Loss_audio ；为什么他们要计算两次？它们相同吗？我听说在对比预训练中，如果考虑到这一点，你的损失应该将每个相似的元素放在一个接近的空间以及彼此远离的不同空间中只有对角矩阵 这实际上是怎么发生的？嗯，有很多问题，但如果您至少能帮助解决其中一个问题，这对我来说是一个巨大的进步。    提交者    /u/Middle_Feeling8257   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18ef8ot/i_dont_understand_clapclip_paper_contrastive_loss/</guid>
      <pubDate>Sat, 09 Dec 2023 14:55:32 GMT</pubDate>
    </item>
    <item>
      <title>是否有 100% 免费的方式来开展检索增强生成项目？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18eexlf/is_there_a_100_free_way_to_work_on_a_retrieval/</link>
      <description><![CDATA[我有一个我非常热衷的爱好项目，它不会赚钱，但我会 100% 帮助一些人 该项目是一个 RAG 系统，我需要一个知识库来保存我的数据，还需要一个法学硕士来根据检索到的文档和用户查询生成响应。我尝试使用 Cohere 和 FAISS 开发一个小型 POC，我对结果很满意，并且希望让这个项目焕发活力。  然而，我破产了，我住在第三世界国家，我在这里甚至没有信用卡，所以我正在寻找一种 100% 免费的方式来实施和放置这样一个系统在制作方面，我一直在到处寻找，但找不到免费的方法。你能帮我么？有什么办法或者我应该找人帮我付款吗？ 闭源LLM是付费的，开源LLM需要付费的云空间来部署。对于矢量数据库，Pinecone 允许一个项目，我相信，我也可以使用 Chroma DB 或 Faiss。 非常感谢您，如果可以的话请帮助我，因为这对我来说意义重大：&#39; )    由   提交 /u/Beneficial-Job-6266   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18eexlf/is_there_a_100_free_way_to_work_on_a_retrieval/</guid>
      <pubDate>Sat, 09 Dec 2023 14:39:20 GMT</pubDate>
    </item>
    <item>
      <title>Balderdash LSTM：Python AI 游戏（仅从头开始 numpy）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18edmlw/balderdash_lstm_python_ai_game_from_scratch_only/</link>
      <description><![CDATA[       由   提交 /u/JTexpo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18edmlw/balderdash_lstm_python_ai_game_from_scratch_only/</guid>
      <pubDate>Sat, 09 Dec 2023 13:29:25 GMT</pubDate>
    </item>
    <item>
      <title>您能想到的 GenAI 最酷的、对人类非常有用的应用程序是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18edjm2/what_is_one_coolest_application_that_you_can/</link>
      <description><![CDATA[标题说明了这一点   由   提交/u/Cool_Bhidu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18edjm2/what_is_one_coolest_application_that_you_can/</guid>
      <pubDate>Sat, 09 Dec 2023 13:24:44 GMT</pubDate>
    </item>
    <item>
      <title>GitHub 上连续 3 天的全球趋势：SuperDuperDB，一个将人工智能与主要数据库集成的框架（使它们成为超级数据库）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18ebzhv/trending_on_github_globally_3_days_in_a_row/</link>
      <description><![CDATA[它用于轻松将人工智能（到您的）应用程序中构建，无需复杂的管道，并且需要将您的数据全部移动到那里 - 并使您的数据库智能化（包括向量搜索），一定要检查一下： https://github.com/SuperDuperDB/superduperdb &lt; !-- SC_ON --&gt;  由   提交 /u/escalize   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18ebzhv/trending_on_github_globally_3_days_in_a_row/</guid>
      <pubDate>Sat, 09 Dec 2023 11:49:16 GMT</pubDate>
    </item>
    <item>
      <title>我真的需要最好的 GPU 来进行训练吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18ebuwj/do_i_really_need_the_best_gpus_for_training/</link>
      <description><![CDATA[嘿！我渴望深入研究大型语言模型 (LLM) 的世界，并为 LLAMA 等项目做出贡献。我也在考虑自己尝试构建一些较小的模型。我想知道硬件要求。我真的需要像 4090 Ti Ultra Super mega 这样配备 128gb vram 的强大处理器，还是像 3060 或 2070 这样的消费类 GPU 就足够了？此外，我听说过 A100 的 VPS 选项 - 这些对于刚开始使用的人来说是必要的吗？或者我可以通过更易于访问的硬件取得进展吗？在这方面的任何建议都会非常有帮助！   由   提交/u/Fantastic-Schedule92  /u/Fantastic-Schedule92 reddit.com/r/learnmachinelearning/comments/18ebuwj/do_i_really_need_the_best_gpus_for_training/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18ebuwj/do_i_really_need_the_best_gpus_for_training/</guid>
      <pubDate>Sat, 09 Dec 2023 11:40:45 GMT</pubDate>
    </item>
    <item>
      <title>进行实践的资源</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18earji/resources_to_get_some_hands_on_practice/</link>
      <description><![CDATA[大家好， 下周我将参加为 ML 爱好者举办的 AI/ML 研讨会（任何对 ML 感兴趣的人都可以参加）加入），我需要资源来进行一些一般性的介绍，并希望在我去之前进行一些实践，这样我在其他开发人员中就不会觉得自己是新手。  我是一名 BI 开发人员，很久以前我就研究过 ML。 活动将于下周六举行。您能给我提供一些我可以在本周查看的资源吗？  提前谢谢您！   由   提交 /u/Judessaa   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18earji/resources_to_get_some_hands_on_practice/</guid>
      <pubDate>Sat, 09 Dec 2023 10:28:18 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的信息论方法</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18eaolm/information_theory_approach_to_neural_networks/</link>
      <description><![CDATA[你好， 我正在寻找结合信息论和神经网络的论文。我对讨论如何在各种神经网络架构中表示信息或表示给定信息的最佳架构是什么的作品感兴趣。另外，如何在网络的不同节点之间交换不同的信息（例如消息传递）。 我问，因为我看到这个领域很有潜力，但这绝对不是主流研究。我想知道是否有人致力于此。   由   提交/u/groman434  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18eaolm/information_theory_approach_to_neural_networks/</guid>
      <pubDate>Sat, 09 Dec 2023 10:22:51 GMT</pubDate>
    </item>
    <item>
      <title>大家都在谈论dropout，但是它真的被使用了吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18ea8ku/everyone_talks_about_dropout_but_is_it_actually/</link>
      <description><![CDATA[上下文： http://mipal.snu.ac.kr/images/1/16/Dropout_ACCV2016.pdf，https://arxiv.org/pdf/1411.4280.pdf, https:/ /arxiv.org/pdf/1207.0580.pdf 像这样的论文和大量博客文章都强调了 dropout 的重要性，即在接受输入的层或初始隐藏层中，因此它很好地概括了更广泛的任务，因为初始层有助于收集一般/表面特征，然后收集任务所需的细节。但是Resnet家族都没有使用dropout，inception和mobilenet在最后一层使用dropout，alexnet也在最后一个分类块中使用，Swin-T家族在最后一层使用了一些地方，但默认p是0。 主要问题： 1. 如何确定放置 dropout 层的位置？ 2. 如何确定dropout的强度（p=？）？ 3. 如何确定我们应该使用哪种类型的dropout；最大丢弃层，随机丢弃，http://mipal.snu.ac.kr/images/1 /16/Dropout_ACCV2016.pdf?   由   提交 /u/MaintenanceNo5993   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18ea8ku/everyone_talks_about_dropout_but_is_it_actually/</guid>
      <pubDate>Sat, 09 Dec 2023 09:53:04 GMT</pubDate>
    </item>
    <item>
      <title>菜鸟问题，标题图片上的标志是什么？我认识 PyTorch、Keras、SkLearn 和 TensorFlow。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18ea76w/noob_question_what_are_the_logos_on_the_header/</link>
      <description><![CDATA[   /u/IOvOI_owl  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18ea76w/noob_question_what_are_the_logos_on_the_header/</guid>
      <pubDate>Sat, 09 Dec 2023 09:50:19 GMT</pubDate>
    </item>
    <item>
      <title>请推荐一个关于类/OOP 的教程，该教程与狗或猫或 25 岁、身高 180 厘米的吉姆无关。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18e9icj/please_recommend_a_tutorial_on_classesoop_that_is/</link>
      <description><![CDATA[每个教程都使用此方法。他们创建了一个猫、狗或汽车的类。  我无法将其推断到我混合数据帧和构建模型的日常工作中。它就是不点击。 这里有人知道包含真实世界数据科学示例的教程吗？ YouTube 上的内容，甚至是文章或网站。   由   提交 /u/scrotalist   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18e9icj/please_recommend_a_tutorial_on_classesoop_that_is/</guid>
      <pubDate>Sat, 09 Dec 2023 09:00:04 GMT</pubDate>
    </item>
    <item>
      <title>我将仅使用开放课程材料来学习机器学习。我应该学习哪些课程？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18e4ula/im_going_to_learn_machine_learning_using_open/</link>
      <description><![CDATA[我的背景是生物化学，我一直对语言学和语言结构着迷。  如果我基本上是根据哈佛、耶鲁、Coursera 等的开放课程材料构建自己的课程，那么哪些课程是必要的？ 看起来像线性代数、统计学、机器学习基础、Python、数学思维/问题解决、语言学 101 可能是一个好的开始？ TIA！   由   提交 /u/Tktpas222   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18e4ula/im_going_to_learn_machine_learning_using_open/</guid>
      <pubDate>Sat, 09 Dec 2023 03:53:46 GMT</pubDate>
    </item>
    <item>
      <title>你们怎么知道这么多</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18dw1a2/how_do_you_guys_know_so_much/</link>
      <description><![CDATA[我是数据科学项目的一年级研究生，本科毕业于计算机科学和数学专业。我已经作为数据科学家工作了几个季度，参加过训练营，获得了证书等等。但每天我都会在 Reddit 上看到有关 ML/DL/“AI”的新东西。我以前不知道。这太荒谬了。我认为我了解得相当多，并且我已经完成了一些有关 CNN、自动编码器和其他类型的 NN 的项目，我正在为我现在所做的之前的 CNN 项目开发 Vision Transformer 方法。但随着我进入这个领域越多，我发现自己知道的就越少。据我估计，这是一个（负）指数增长。感觉在接下来的两年内我不可能学到足够多的知识来匹配目前每个从事这些工作的人所知道的知识。即使现在有了理论知识和经验，仍然感觉每当我做一个项目时，我基本上只是修改和调整其他人的方法和模型，也许混合其他东西，但人们如何发明自己的全新架构等等，这太疯狂了。我什至做梦也想不到我能做到这一点。  那么，怎么样？我和只有硕士学位的人之间的区别，甚至是我见过的一些只有学士学位但从美国顶尖学校的本科 CS/DS/Stats 课程中以优异或不优异成绩毕业的人（制作 Phind 的人是 21 岁或 21 岁） 22 例如，当他们这样做时）感觉就像我和一个甚至还没有写过 Hello World 或上过 Calc/Stats 的高中生之间的区别。这是无法克服的，我只是不明白一个人如何在生活中承担责任、爱好等等，达到这一点。从我作为一年级学生进入大学的那一刻起，我是否只需要每天每一分钟都生活在数学和线性代数中？   由   提交 /u/Traditional_Land3933   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18dw1a2/how_do_you_guys_know_so_much/</guid>
      <pubDate>Fri, 08 Dec 2023 20:36:17 GMT</pubDate>
    </item>
    <item>
      <title>为什么决策树不好？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18dv4ee/why_are_decision_trees_bad/</link>
      <description><![CDATA[我最近迷上了决策树。但任何人都可以解释为什么“轴平行分裂”是这样的吗？被认为是决策树的弱点吗？我一直听说这是一件坏事，但我不明白为什么。   由   提交 /u/Traditional_Soil5753   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18dv4ee/why_are_decision_trees_bad/</guid>
      <pubDate>Fri, 08 Dec 2023 19:56:06 GMT</pubDate>
    </item>
    <item>
      <title>谷歌承认 Gemini AI 演示视频是为了看起来更好而制作的 - 但为什么呢？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18duxsv/google_admits_gemini_ai_demo_video_was_fabricated/</link>
      <description><![CDATA[ 由   提交/u/vadhavaniyafaijan   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18duxsv/google_admits_gemini_ai_demo_video_was_fabricated/</guid>
      <pubDate>Fri, 08 Dec 2023 19:48:02 GMT</pubDate>
    </item>
    </channel>
</rss>