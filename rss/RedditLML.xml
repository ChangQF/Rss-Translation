<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Fri, 26 Jul 2024 15:15:52 GMT</lastBuildDate>
    <item>
      <title>需要机器学习的概率和统计资源</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecpel8/need_probability_and_statistics_resources_for_ml/</link>
      <description><![CDATA[如前所述，我需要优质、简洁、快捷的资源来涵盖上述 ML 主题。 我目前的重点是各个公司的 ML 角色 OA，我看到他们询问与 ML 相关的 MCQ。 此外，如果您对基于一般 ML OA 的其他 ML 概念有类似的主题覆盖范围，那就太好了。 我对两者都有相当的了解，并且已经自己研究过它们，但不是以有组织的方式。 谢谢    提交人    /u/Few-Cardiologist8183   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecpel8/need_probability_and_statistics_resources_for_ml/</guid>
      <pubDate>Fri, 26 Jul 2024 14:25:25 GMT</pubDate>
    </item>
    <item>
      <title>深度学习项目所需指导</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecokme/guidance_required_for_deep_learning_project/</link>
      <description><![CDATA[大家好，我是计算机科学工程专业二年级学生。我想开发一款移动应用，可以录制课堂视频并生成出席学生名单以供点名。我没有机器学习或 git 方面的经验。我发现人脸识别属于深度学习。我对深度学习的工作原理有基本的了解。请指导我如何完成我的项目。我的目的不是建立自己的模型。希望我能找到一个好的模型，可以用我同学的脸部进行训练以获得所需的输出。任何指导都会有所帮助。谢谢。    提交人    /u/sangeehhsach   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecokme/guidance_required_for_deep_learning_project/</guid>
      <pubDate>Fri, 26 Jul 2024 13:49:41 GMT</pubDate>
    </item>
    <item>
      <title>在机器学习领域工作过的人，你们在工作中使用数学吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecn2pz/people_who_work_or_worked_in_the_field_of_ml_do/</link>
      <description><![CDATA[  由    /u/BEE_LLO  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecn2pz/people_who_work_or_worked_in_the_field_of_ml_do/</guid>
      <pubDate>Fri, 26 Jul 2024 12:39:36 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯优化：为什么 AUC-ROC 会出现天花板效应而敏感度却不会？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecm3n8/bayesian_optimisation_why_does_aucroc_show_a/</link>
      <description><![CDATA[      我执行了贝叶斯优化来调整 XGBoost 分类模型的超参数。尽管进行了多次迭代（30、60、100、200），AUC-ROC 指标始终表现出天花板效应，如下图所示。 https://preview.redd.it/qf9l47hxpued1.png?width=794&amp;format=png&amp;auto=webp&amp;s=eb45974488818f7f1ea19d0bda7f9a498828aa3e 有人能解释一下为什么 AUC-ROC 指标显示天花板效应，而敏感度指标没有？    提交人    /u/Dobra_Vila   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecm3n8/bayesian_optimisation_why_does_aucroc_show_a/</guid>
      <pubDate>Fri, 26 Jul 2024 11:49:02 GMT</pubDate>
    </item>
    <item>
      <title>将句子与列表中的另一个句子进行匹配的模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecly6n/a_model_to_match_the_sentence_to_another_from_a/</link>
      <description><![CDATA[您好， 我需要创建一个模型，将一个文件中的句子与另一个文件中的句子进行匹配。例如，如果文件中的句子是“我们需要更换 2 楼的门”。则模型应该能够选择含义最接近的句子，例如“需要更换一扇门”。我需要什么样的模型来执行这样的任务？    提交人    /u/NefariousnessFar7176   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecly6n/a_model_to_match_the_sentence_to_another_from_a/</guid>
      <pubDate>Fri, 26 Jul 2024 11:40:40 GMT</pubDate>
    </item>
    <item>
      <title>[T5] [HuggingFace] 如何控制生成摘要的长度</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecll3g/t5_huggingface_how_to_control_the_lenth_of_the/</link>
      <description><![CDATA[大家好，在对 T5 模型进行微调后，我编写了一个用于推理的脚本： def generate_summary(text, max_length, creativity): input= tokenizer(text= text, return_tensors= &quot;pt&quot;, truncation= True, padding= True, max_length= 1024).to(parameters.device) output= model.generate( **inputs, max_length= max_length,temperature= creativity,num_beams= 4, no_repeat_ngram_size= 2, early_stopping= True, do_sample= True ) summary= tokenizer.decode(outputs[0], skip_special_tokens= True) return summary  在 generate_summary 方法中，我解析了三个参数：用于输入上下文的 text、用于确定性控制的temperature和用于输出长度控制。但是，我注意到，虽然温度运行良好，但 max_length 却不行，当达到最大长度时，它只会切断（截断）文本。 我在这里做错了什么吗？目标是告诉模型在预设长度内生成输出。 仅供参考，在微调过程中，我创建了一个自定义数据集方法： # 加载和处理数据集 class CustomDataset(Dataset): def __init__(self, data, context_max_length, summary_max_length): #self.data = data self.context_max_length= context_max_length self.summary_max_length= summary_max_length self.tokenizer= AutoTokenizer.from_pretrained(pretrained_model_name_or_path= parameters.model_name, cache_dir= parameters.model_cache_dir) self.data= [ item for item in data if len(self.tokenizer.encode(item[&quot;context&quot;]))&lt;= self.context_max_length and len(self.tokenizer.encode(item[&quot;summary&quot;]))&lt;= self.summary_max_length ] def __len__(self): return len(self.data) def __getitem__(self, idx): #context_token= self.tokenizer.encode(self.data[idx][&quot;context&quot;]) #summary_token= self.tokenizer.encode(self.data[idx][&quot;summary&quot;]) context_encodings= self.tokenizer( self.data[idx][&quot;context&quot;], max_length= self.context_max_length, padding= &quot;max_length&quot;, truncation= True, return_tensors= &quot;pt&quot; ) summary_encodings= self.tokenizer( self.data[idx][&quot;summary&quot;], max_length= self.summary_max_length, padding= &quot;max_length&quot;, truncation= True, return_tensors= &quot;pt&quot; ) return { &#39;input_ids&#39;: context_encodings[&quot;input_ids&quot;].squeeze(), &#39;labels&#39;: summary_encodings[&quot;input_ids&quot;].squeeze(), &quot;attention_mask&quot;: context_encodings[&quot;attention_mask&quot;].squeeze() } print(&quot;加载数据集&quot;) train_dataset= load_from_disk(f&quot;{parameters.datasets_cache_dir}/big_sum_dataset/train&quot;) validation_dataset= load_from_disk(f&quot;{parameters.datasets_cache_dir}/big_sum_dataset/validation&quot;) tokenized_train_dataset= CustomDataset(data= train_dataset, context_max_length= 1024, summary_max_length= 512) tokenized_validation_dataset= CustomDataset(data= validation_dataset, context_max_length= 1024, summary_max_length= 512)  这里我对数据集做了一些预处理：  我过滤掉了所有不满足预设长度的实例 我将输入上下文的长度填充为 1024，将输出摘要的长度填充为 512。  我不知道这个自定义数据集是否意​​味着模型不能生成可变长度的输出。 谢谢。    由    /u/Q_H_Chu 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecll3g/t5_huggingface_how_to_control_the_lenth_of_the/</guid>
      <pubDate>Fri, 26 Jul 2024 11:20:30 GMT</pubDate>
    </item>
    <item>
      <title>帮助本地设置 pytorch</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecklkn/help_in_setting_pytorch_locally/</link>
      <description><![CDATA[正如标题所说，我大部分工作都是在 colab 笔记本中完成的，因为我的笔记本电脑没有 GPU。最近，我购买了一台配备 Nvidia GeForce RTX 3050 GPU 的笔记本电脑。 因此，我尝试从预先训练的 hf 模型制作聊天机器人应用程序，然后我首先在 colab 上运行该模型，它运行良好👍。但现在我的下一步是在本地运行它。 经过一番研究，我首先下载了 cuda 12.1，然后下载了 cdn（12.x）并进行了复制粘贴。现在我设置了 conda 环境并安装了我的需求。  pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121 但运行后，我只从第一行收到错误，即 import torch 和错误，它说是操作系统错误 Window 126，错误 fbgemm.dll 或其依赖项丢失。所以我检查了路径文件，这个 dll 就在那里。 我该如何解决这个问题？    提交人    /u/ArugulaCrafty9236   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecklkn/help_in_setting_pytorch_locally/</guid>
      <pubDate>Fri, 26 Jul 2024 10:19:29 GMT</pubDate>
    </item>
    <item>
      <title>“神经网络是通用函数逼近器”</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eck5u8/neural_networks_are_universal_function/</link>
      <description><![CDATA[      当我们使用神经网络时，我们正在构建一个从输入映射到输出的函数。 神经网络的真正威力在于它们充当函数近似的能力。如果你能理解为什么，这意味着你对神经网络和激活函数的必要性了解很多。 通用近似定理指出，具有单个隐藏层和有限数量节点的前馈神经网络可以以任何精度近似任何连续函数。唯一的限制是，对于给定的神经网络权重和偏差，输入值的范围是固定的。 你如何真正理解通用近似定理？ 取一个连续非线性函数 f(x)=x^3-3x^2+2x+5 然后考虑 ReLU 激活函数。 ReLU(x)=max(0,x) 我们可以构造 5 种 ReLU 变体。当我们将 ReLU 或 x 乘以一个常数时，我们会缩放斜率。当我们将一个常数添加到 x 时（例如 ReLU(x+a)），我们会将 ReLU 的转换点移动 -a。  1) y1 =-5ReLU(x) 2) y2 = 5ReLU(x+1) 3) y3 = 5ReLU(x-2) 4) y4 = 15ReLU(x-3)  5) y5 = -20ReLU(-x-1) 现在，如果您将这 5 个函数相加并绘制出来，您将看到 f(x) ~ y1 + y2 + y3 + y4 + y5。 我们可以将这 5 个 ReLU 函数表示为只有一个输入节点和 1 个包含 5 个节点的隐藏层的神经网络。  https://preview.redd.it/lyp5kslt4ued1.png?width=1098&amp;format=png&amp;auto=webp&amp;s=172a9cd84a65ba5091be3181e8c09df62395647e 从输入到隐藏层的权重矩阵为 W1=[1,1,1,1,-1]  到隐藏层节点的偏置矩阵为B1=[0,1,-2,-3,-1] 从隐藏层到输出层的权重矩阵为 W2=[-5,5,5,15,-20] 从隐藏层到输出层的偏差矩阵为 B2=[0,0,0,0,0] 您可能已经注意到，如果我们将函数分为 100 个 ReLU 函数的总和，则隐藏层中将有 100 个节点，并且函数 f(x)=x^3-3x^2+2x+5 将可以通过 ReLU 很好地近似。  这不是通用近似定理的正式证明，而只是一个说明性演示。  我已经发表了一整篇关于此的讲座。我认为您会喜欢它，因为我们将向您展示如何构建这些 ReLU 函数并使用名为 Desmos 的工具绘制它们。在此处查看讲座：https://www.youtube.com/watch?v=SQeZ2FONQEw&amp;feature=youtu.be    提交人    /u/thesreedath   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eck5u8/neural_networks_are_universal_function/</guid>
      <pubDate>Fri, 26 Jul 2024 09:51:10 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归的成本函数不是凸的？并且影响另一个</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eck3p6/cost_function_of_logistic_regression_isnt_convex/</link>
      <description><![CDATA[      为什么他们说逻辑回归是凸的，但似乎不是，它延伸到了其他部分，并相互影响，请参考所附的成本函数图 如果 y 为 1，则成本函数将为 -log(yhat) 如果 y 为 0，则成本函数将为 -log(1-yhat) 两者都会趋向于自己的最小值 一个成本的权重更新不会影响其他成本吗，例如，如果 y=1，它会尝试更新权重，使预测（yhat）接近 1 但是，如果调整权重以使 yhat 接近 1，那么其他成本函数（y=0）不会受到严重惩罚吗，因为它预计 yhat 会接近于零？ 您能解释一下逻辑回归的成本函数吗？ 为什么在那里使用 Sigmoid 函数？为什么他们说如果 S 型函数的输出 &gt;= 阈值 (0.5)，我们就会预测 1？ 提前致谢  图片来源 https://towardsdatascience.com/logistic-regression-and-decision-boundary-eab6e00c1e8    提交人    /u/ajihkrish   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eck3p6/cost_function_of_logistic_regression_isnt_convex/</guid>
      <pubDate>Fri, 26 Jul 2024 09:47:07 GMT</pubDate>
    </item>
    <item>
      <title>双编码器与交叉编码器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecisvl/biencoder_vs_crossencoder/</link>
      <description><![CDATA[大家好， 目前我正在学习编码器，我发现有两种常用类型：双编码器和交叉编码器（https://www.sbert.net/examples/applications/cross-encoder/README.html）。 我仍然有点难以理解。双编码器（基于 BERT）每次仅接收一个句子作为输入，然后计算相应的嵌入。另一方面，交叉编码器总是采用两个句子，然后计算一个公共嵌入，然后是分类层以生成 0 到 1 之间的值。 我读到过 BERT 只能进行自我注意，但我经常读到在交叉编码器设置中，两个输入句子被连接起来，然后使用交叉注意，但 BERT 中不存在交叉注意，因为它是一个仅编码器模型，对吗？ 那么，BERT 究竟如何处理这个问题？BERT 最多可以接受两个输入，然后看起来像这样“[CLS] 句子 1 [SEP] 句子 2”。在这种情况下，这已经被称为“交叉编码器”了吗？那么这后面是否只会跟着一个分类头？ 另一方面，在双编码器设置中，来自 BERT 的输入将如下所示：“[CLS] 句子 [SEP]”？然后将对这个句子应用自注意力多次，最后返回一个嵌入？ 一个输入和两个输入之间到底有什么区别？什么时候 BERT 是双编码器，什么时候是交叉编码器？当 BERT 仅接收一个输入（双编码器）时的行为与同时接收两个输入（交叉编码器）时的行为是否不同？BERT 交叉编码器只是普通的 BERT 模型 + 分类头吗？BERT 在给定两个输入时会执行交叉注意，而在仅给定一个输入时会执行正常的自注意力吗？我很困惑。 我希望你能理解我的问题，并提前非常感谢你！    提交人    /u/Ok-Bedroom2108   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecisvl/biencoder_vs_crossencoder/</guid>
      <pubDate>Fri, 26 Jul 2024 08:15:28 GMT</pubDate>
    </item>
    <item>
      <title>如果您即将购买任何月度订阅，那么有个好消息要告诉您。截止 2024 年 8 月 9 日，Coursera Plus 订阅可享受 50% 的折扣。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecin18/if_you_are_about_to_purchase_any_monthly/</link>
      <description><![CDATA[  由    /u/eham2017  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecin18/if_you_are_about_to_purchase_any_monthly/</guid>
      <pubDate>Fri, 26 Jul 2024 08:04:17 GMT</pubDate>
    </item>
    <item>
      <title>学习机器学习的绝佳资源！</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1eci5aw/great_resource_for_learning_ml/</link>
      <description><![CDATA[我偶然发现了微软为任何对机器学习感兴趣的人提供的免费课程。  描述机器学习的核心概念 识别不同类型的机器学习 描述训练和评估机器学习模型的注意事项 描述深度学习的核心概念 在 Azure 机器学习服务中使用自动化机器学习  它涵盖以下主题，并通过注释和示例详细讲授回归、分类和聚类 非常适合初学者 最后还提供了可共享的证书 在这里查看 https://learn.microsoft.com/training/modules/fundamentals-machine-learning/?wt.mc\_id=studentamb\_395038    由   提交  /u/Strange-Slide-5300   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1eci5aw/great_resource_for_learning_ml/</guid>
      <pubDate>Fri, 26 Jul 2024 07:30:11 GMT</pubDate>
    </item>
    <item>
      <title>我可以成为一名拥有商业学位的数据分析师吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecf8t4/can_i_be_a_data_analyst_with_a_busines_degree/</link>
      <description><![CDATA[我很快就要上大学了。我计划攻读商业分析学士学位。我可以在获得学士学位后继续攻读数据科学或人工智能，或者攻读数据科学或人工智能硕士学位吗？\    提交人    /u/JuliusSeizure75   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecf8t4/can_i_be_a_data_analyst_with_a_busines_degree/</guid>
      <pubDate>Fri, 26 Jul 2024 04:27:13 GMT</pubDate>
    </item>
    <item>
      <title>有没有老师以与阿卜杜勒·巴里教授算法相同的方式教授机器学习概念？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ecdzqk/any_teacher_who_teaches_ml_concepts_in_the_same/</link>
      <description><![CDATA[  由    /u/-AnujMishra  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ecdzqk/any_teacher_who_teaches_ml_concepts_in_the_same/</guid>
      <pubDate>Fri, 26 Jul 2024 03:17:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>