<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sun, 11 Feb 2024 01:01:10 GMT</lastBuildDate>
    <item>
      <title>使用线性回归教程进行股票预测</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anue04/making_stock_predictions_with_linear_regression/</link>
      <description><![CDATA[我最近开始从 youtube 上的 senddex 教程学习 ML，但不明白他如何创建日期并将所有内容添加到新列中数据集。这是代码： import pandas as pd import quandl import math import datetime import numpy as np from sklearn import preprocessing, svm from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression import matplotlib。 pyplot as plt from matplotlib import style style.use(&#39;ggplot&#39;) df = quandl.get(&#39;WIKI/GOOGL&#39;) df = df[[&#39;Adj.打开&#39;，&#39;调整。高&#39;, &#39;调整。低&#39;，&#39;调整。关闭&#39;, &#39;调整。成交量&#39;]] df[&#39;HL_PCT&#39;] = (df[&#39;调整高位&#39;] - df[&#39;调整收盘价&#39;]) * 100 / df[&#39;调整高位&#39;]) * 100 / df[&#39;调整高位&#39;]关闭&#39;] df[&#39;PCT_change&#39;] = (df[&#39;调整关闭&#39;] - df[&#39;调整打开&#39;]) * 100 / df[&#39;调整关闭&#39;]打开&#39;] df = df[[&#39;调整。关闭&#39;、&#39;HL_PCT&#39;、&#39;PCT_change&#39;、&#39;调整。交易量&#39;]] Forecast_col = &#39;调整。关闭&#39; df.fillna(-99999，inplace=True) Forecast_out = int(math.ceil(0.01*len(df))) df[&#39;label&#39;] = df[forecast_col].shift(-forecast_out) X = np. array(df.drop([&#39;label&#39;], axis=1)) X = preprocessing.scale(X) X = X[:-forecast_out] X_lately = X[-forecast_out:] df.dropna(inplace=True) y = np.array(df[&#39;label&#39;]) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) clf = LinearRegression(n_jobs=-1) clf.fit(X_train, y_train) 准确度 = clf .score(X_test, y_test) Forecast_set = clf.predict(X_lately) df[&#39;Forecast&#39;] = np.nan last_date = df.iloc[-1].name last_unix = last_date.timestamp() one_day = 86400 #以秒为单位 next_unix = last_unix + one_day for i in Forecast_set: next_date = datetime.datetime.fromtimestamp(next_unix) next_unix += one_day df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)] + [i] df[&#39;调整。 Close&#39;].plot() df[&#39;Forecast&#39;].plot() print(&#39;预测股票价格 &#39; + str(forecast_out) + &#39; 未来的天数&#39;) print(df, Forecast_set, precision, Forecast_out) plt. legend(loc=4) plt.xlabel(&#39;Date&#39;) plt.ylabel(&#39;Price&#39;) plt.show()  我理解所有代码，直到他创建一个新的数据集中名为“预测”的列填充了 NAN 值。我不明白他如何获取新的预测日期，然后将所有这些添加到数据集中的预测列中。请详细解释每一行，因为我对机器学习完全陌生。非常感谢！ PS：我读到了 pandas 中的 iloc 函数，并理解了它应该做什么，但我不明白如何只输入一个整数而不输入其他内容，甚至不输入一个冒号，将使之发挥作用。是否只选择最后一行以便可以检索数据集中的最后日期？再次感谢！   由   提交/u/Indoraptor0902   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anue04/making_stock_predictions_with_linear_regression/</guid>
      <pubDate>Sun, 11 Feb 2024 00:06:08 GMT</pubDate>
    </item>
    <item>
      <title>随机森林分类器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1antgst/random_forest_classifier/</link>
      <description><![CDATA[大家好， 我正在尝试在 python 中实现以下用于随机森林分类器的 R 代码- &lt; pre&gt;## 训练随机森林 set.seed(5136) a.rf = randomForest(y=a.y, x=a.x,ntree=1000,importance=T, strata=a.y,sampsize=c(76,76) ,do.trace=10, approximation=T)  这是我在 python 中的实现 - ###random Forest class_weights = {0: 76, 1: 76} rf_pipeline = Pipeline(steps=[(&#39;预处理器&#39;, 预处理器), (&#39;模型&#39;,RandomForestClassifier(n_estimators=1000,max_features=28,class_weight=class_weights, random_state=5136, oob_score=True, verbose=1) )]) rf_pipeline.fit(a_x, a_y)  这是Python中的等效实现吗？我对 R 的 strata=a.y 和 sampsize=c(76,76) 感到困惑？这里的等效语法是什么？另外，在 R 代码中实现 OOB 估计错误率：19.41% 和在 python OOB 估计错误率：7.14% 中实现后，这是预期的吗？有人可以澄清一下吗？   由   提交 /u/tinkerpal   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1antgst/random_forest_classifier/</guid>
      <pubDate>Sat, 10 Feb 2024 23:23:08 GMT</pubDate>
    </item>
    <item>
      <title>帮助建立对象检测模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anqs4c/help_with_object_detection_model/</link>
      <description><![CDATA[你好，我决定做一个基于机器学习的小项目。该模型应将用户屏幕（移动/桌面）图像作为输入，并给出该屏幕中使用的用户体验法则。经过一番研究，我得出以下结论： 1. 使用相应的 ui 元素和适用的 ux 法则注释用户屏幕图像（我可以做到这一点） 2. 通过开源对象检测模型（如 YOLO）运行它。 3. 以一定概率考虑以上结果并显示。我的问题是，a）我考虑了两种方法，首先仅注释 ui 元素，然后让模型找到元素并编写在存在某个 ui 元素时适用的规则。其次，直接用ux定律进行注释，让模型直接输出定律。哪种方法更适合我的目的？ b) 有一些规则不限于 ui 元素，而是基于整个屏幕或元素之间的空间。对于那些复杂的法律，最好的方法是什么？提前致谢！   由   提交/u/booberrypie_  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anqs4c/help_with_object_detection_model/</guid>
      <pubDate>Sat, 10 Feb 2024 21:23:05 GMT</pubDate>
    </item>
    <item>
      <title>从哪儿开始？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anqp7k/where_to_start/</link>
      <description><![CDATA[我有兴趣学习 ML，但不知道从哪里开始。我有近 10 年的软件工程经验（如果这也有帮助的话）。  理想情况下，从我需要了解的基础知识开始，了解正在发生的事情及其原因。   由   提交 /u/Forumpy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anqp7k/where_to_start/</guid>
      <pubDate>Sat, 10 Feb 2024 21:19:26 GMT</pubDate>
    </item>
    <item>
      <title>使用一些黑色蒙版数据集进行图像分割的最佳实践</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anplxk/best_practice_for_image_segmentation_with_dataset/</link>
      <description><![CDATA[我有一个训练数据集，我试图在其中分割图像中的肿瘤，但并非所有图像都包含肿瘤。所以这些图像的掩模只是黑色像素。在部署过程中，还会有一些不存在肿瘤的图像，因此我决定在训练数据集中保留没有任何肿瘤的图像。然而，我在网上搜索发现人们会删除不存在要分割的目标对象的图像。我不确定保留这些无肿瘤图像是否有一些价值，因为我认为这会帮助模型了解什么不是肿瘤？  我在想，也许我可以创建一个分类模型来识别肿瘤是否存在，然后对存在肿瘤的图像使用分割模型。或者有更好的方法吗？   由   提交/u/waterstrider123  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anplxk/best_practice_for_image_segmentation_with_dataset/</guid>
      <pubDate>Sat, 10 Feb 2024 20:30:19 GMT</pubDate>
    </item>
    <item>
      <title>使用 Tensorflow 和 keras 进行时间序列分类</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anp4pv/time_series_classification_using_tensorflow_and/</link>
      <description><![CDATA[       我正在尝试创建一个模型来创建一个基本上将每个时间序列数据映射到其特定情况的模型。基本上，我的输入是形状为 (100, 300, 20) 的张量，其中 100 是我从中获取数据的文件数量，每个文件对应于每种情况，300 是行，20 是包括时间在内的特征。我事先对数据进行了标准化。现在我的目标是模型能够识别给定数据集（300 行，20 列）属于这些文件中的哪一个。因此，我的 y_data 是代表每个文件的 1-100 标签数组，形状为 (100,1)。我对数据进行了重组（同时维护每个文件的相应标签）以创建训练、验证和测试集。我的理解是这是一个分类问题，我在输出中使用softmax。模型架构如下： model = Sequential() model.add(Flatten(input_shape=(300, 20))) model.add(Dense(216,activation=&#39;linear&#39;, kernel_regularizer) =l2(0.01))) model.add(BatchNormalization()) model.add(Dropout(0.5)) model.add(Dense(32, 激活=&#39;线性&#39;, kernel_regularizer=l2(0.01))) model.add( BatchNormalization()) model.add(Dense(101,activation=&#39;softmax&#39;)) cp5 = ModelCheckpoint(&#39;model/&#39;, save_best_only=True) model.compile(loss=&#39;SparseCategoricalCrossentropy&#39;, optimizationr=Adam(learning_rate=0.001) ,metrics=[&#39;accuracy&#39;])history=model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=50,callbacks=[cp5]) 现在，我无法获得任何好的结果，而且似乎无论我如何使用这种架构，我总是过度拟合（无论我增加或减少多少层或神经元 - 除非它收敛到无穷大）。 这是我的损失与历元的屏幕截图 ​ ​ https://preview.redd.it/8cmziu3rethc1.png?width=787&amp;format=png&amp;auto=webp&amp;s =0f544456d5604f1ce7b​​081ec0e5d62cea294de38 ​ 编辑：我也尝试过各种激活函数，如&#39;relu&#39;或&#39;tanh&#39;，并且我尝试使用lstm，所有收益更差或相似的结果 ​   由   提交/u/relatively-physicals  /u/relatively-physicals reddit.com/r/learnmachinelearning/comments/1anp4pv/time_series_classification_using_tensorflow_and/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anp4pv/time_series_classification_using_tensorflow_and/</guid>
      <pubDate>Sat, 10 Feb 2024 20:09:10 GMT</pubDate>
    </item>
    <item>
      <title>学习伙伴</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anol93/study_partner/</link>
      <description><![CDATA[你好，我是一名 22 岁的电气工程专业学生，我刚刚开始学习一些机器学习/计算机视觉，我是一个完全的初学者！我每晚学习大约1/2小时，我希望有一个学习伙伴可以互相激励   由   提交 /u/armyy__   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anol93/study_partner/</guid>
      <pubDate>Sat, 10 Feb 2024 19:45:56 GMT</pubDate>
    </item>
    <item>
      <title>用奇异值分解技术实现朴素贝叶斯推荐系统</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anmboz/implement_naive_bayes_with_singular_value/</link>
      <description><![CDATA[目前我正在做一个零售项目，目的是“向用户推荐排名前三的产品”与他们的口味相似。 该技术是协同过滤，基于用户的过滤。 我想到应用奇异值分解（SVD），然后实现朴素贝叶斯来设置阈值。  p&gt; 我是从理论角度来说的，你觉得怎么样？ 使用的数据集：用户ID、用户特征[年龄、性别、位置等]、产品详细信息和评分。 ​ 我正在调查贝叶斯个性化排名，不过感觉朴素贝叶斯更接近我的需求但不确定！   由   提交 /u/The_Simpsons_22   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anmboz/implement_naive_bayes_with_singular_value/</guid>
      <pubDate>Sat, 10 Feb 2024 18:05:43 GMT</pubDate>
    </item>
    <item>
      <title>学习曲线突然下降</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anjxsq/an_abrupt_fall_in_the_learning_curve/</link>
      <description><![CDATA[       这是训练期间F1分数的数字。在第 4 个 epoch 之后，模型突然将每个样本分类为一个特定的类别。可能是什么原因？顺便说一句，这是一个编码器-解码器变换器，它以 N 个连续数据作为输入并产生 M 个连续嵌入作为输出，并且变换器的输出（每个嵌入单独）由线性层映射到 C 个类。 https://preview .redd.it/ehcjs9oh9shc1.png?width=574&amp;format=png&amp;auto=webp&amp;s=2166193a98bb63f047f67528656d8f931078b733   由   提交/u/tandir_boy  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anjxsq/an_abrupt_fall_in_the_learning_curve/</guid>
      <pubDate>Sat, 10 Feb 2024 16:20:28 GMT</pubDate>
    </item>
    <item>
      <title>介于 RAM 上缓存数据集和从硬盘加载每批数据之间</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anjoll/intermediate_between_caching_dataset_on_ram_and/</link>
      <description><![CDATA[我目前的理解是，训练期间最大的瓶颈是从 HDD 加载数据和输入变换（包括增强），而不是前向传递、反向传播或梯度下降。  当前的惯例是将所有数据加载到 RAM 上（如果合适），这会显着加快训练速度。我的问题涉及到内存太大而无法容纳的数据集。我们是否可以说，以这样的方式实现我们的（火炬）数据集和/或数据加载器：一个时期内的部分批次（例如，总共 300 个中的前 30 个）以顺序方式加载到 CPU 上？  我们非常欢迎您指出的任何代码片段或教程。   由   提交 /u/mimivirus2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anjoll/intermediate_between_caching_dataset_on_ram_and/</guid>
      <pubDate>Sat, 10 Feb 2024 16:08:52 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中从头开始的稳定扩散 |第一部分 - 无条件潜在扩散模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anhts5/stable_diffusion_from_scratch_in_pytorch_part_i/</link>
      <description><![CDATA[       由   提交/u/tusharkumar91   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anhts5/stable_diffusion_from_scratch_in_pytorch_part_i/</guid>
      <pubDate>Sat, 10 Feb 2024 14:43:07 GMT</pubDate>
    </item>
    <item>
      <title>关于理论机器学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1angr16/about_theoretical_machine_learning/</link>
      <description><![CDATA[大家好！我研究机器学习已经几个月了，我在我一直使用的资源中注意到了一些东西。他们几乎总是不会详细了解更复杂的事情是如何真正运作的。我们没有看到诸如如何计算基本算法和模型的时空复杂性之类的事情，我想念这一点......我不知道这种情况是否发生在其他人身上，但对我来说，没有看到这一点让我感觉像我使用了某种我并不完全理解的黑魔法。这给我们带来了我的问题：如何学习机器学习的这些更正式的方面？在深入研究之前我应该​​掌握哪些主题？是否有初学者论文可以阅读，以大致了解学术界如何开发此类事物？它是当前研究重点的机器学习的一个分支吗？我应该关注谁的工作以保持该领域的最新动态？提前谢谢您。   由   提交 /u/vmmc2   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1angr16/about_theoretical_machine_learning/</guid>
      <pubDate>Sat, 10 Feb 2024 13:50:10 GMT</pubDate>
    </item>
    <item>
      <title>如果我们引入 ɣ 和 β，批量归一化如何减少协变量偏移</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1andyh1/how_does_batch_normalization_reduce_covariate/</link>
      <description><![CDATA[我对为什么使用批量归一化的理解是，因为当我们通过网络将输入移动得更深时，层输出的平均平均值变得更加不稳定，这使得我们走得越深，这使得后续层无法有效学习，因为它们总是针对输入进行优化，而每当我们更新前面层中的权重时，这些输入就会发生变化。 所以我的理解，批量归一化应该固定这些输出，以便它们的均值为零，并且每次前向传递的标准差为 1，以便后续层的优化更加稳定，并且不会因输入的变化而损失效率。&lt; /p&gt; 但如果是这样的话，那我们为什么还要引入 gamma 和 beta 呢？难道他们不会带回我们最初试图解决的问题，因为他们修改了协变量移位，使得后续层永远不会有准确的优化，因为每次我们执行训练步骤时，它们的输入都会被移位和缩放。&lt; /p&gt; 我从某处读到，如果存在一种情况，如果我们没有规范化后续层可以表现得更好，那么通过引入这些可训练参数可以实现身份批量规范。即没有任何变化并且下一层表现更好（这可以扩展到任何一些移位和缩放表现更好的情况）。但这仍然无法摆脱在协变量偏移中重新引入波动性的问题，并且下一层不会只是针对标准化规模和偏移进行优化吗？ 我们将不胜感激！&lt; /p&gt;   由   提交 /u/DisciplinedPenguin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1andyh1/how_does_batch_normalization_reduce_covariate/</guid>
      <pubDate>Sat, 10 Feb 2024 11:05:49 GMT</pubDate>
    </item>
    <item>
      <title>有关 CUDA 和 GPU 内核的问题</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1anaqce/questions_about_cuda_and_gpu_kernels/</link>
      <description><![CDATA[嗨/r/learnmachinelearning，&lt; /p&gt; 我是人工智能领域的新手，正在尽力学习。我一直遇到的一件事是，NVIDIA 的 GPU 对于 AI 来说如此出色的原因是因为 CUDA。 有人可以为我扩展一下这一点吗？我不完全掌握 CUDA 和 CUDA 内核的概念。是什么阻止 AMD 让工程师为 ROCm 编写构成 CUDA 的相同软件？或者是什么阻止 AMD 为他们的 GPU 编写相同的 CUDA 内核？以下是我的明确问题：  CUDA 的哪些部分是如此令人惊奇，AMD/其他任何人都无法复制？  我知道 CUDA 是一个不同软件的整个范围/套件。 CUDA 中是否存在对 NVIDIA 更重要而 AMD/其他公司无法轻易复制的特定部分？ 例如：CUDA 由库（cuBLAS、cuFFT、cuDNN、cuRAND、NCCL 等）组成，编译器（nvcc、nvc++ 等）、驱动程序、NVIDIA SMI 等  为什么编写 CUDA/GPU 内核是一项如此具有挑战性的任务?  我理解 GPU/CUDA 内核的方式是，它们是 GPU 上几种不同“基本”操作的融合。为什么编写这些内核如此困难？ AMD 的工程师可以查看这些内核并将其复制到他们的 GPU/软件中吗？  是否还有其他与 GPU 相关的中间件（无论是否属于 CUDA）对于 NVIDIA 来说至关重要且竞争对手无法轻易复制？  提前感谢任何人谁能够回答这些问题的任何部分！   由   提交 /u/redditnaked   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1anaqce/questions_about_cuda_and_gpu_kernels/</guid>
      <pubDate>Sat, 10 Feb 2024 07:20:18 GMT</pubDate>
    </item>
    <item>
      <title>寻找学习伙伴</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1an55ms/looking_for_a_study_partner/</link>
      <description><![CDATA[大家好，我是一名 21 岁的学生，正在寻找学习伙伴。 我是一名来自埃及的计算机科学专业高年级学生，我正在寻找学习伙伴。感兴趣的是NLP、深度学习和神经网络。我的最终目标是能够从事涉及音乐、歌词、社交媒体和推荐系统的项目。我目前的学习计划正在学习 Datacamp 的 NLP 和深度学习技能课程，我目前正在学习它们的先决课程，并且正在学习 fastai 课程，到目前为止我已经听过并应用了 2 个讲座。 如果有人的话，我愿意想聊天！谢谢。   由   提交 /u/charliesmusictaste   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1an55ms/looking_for_a_study_partner/</guid>
      <pubDate>Sat, 10 Feb 2024 02:01:55 GMT</pubDate>
    </item>
    </channel>
</rss>