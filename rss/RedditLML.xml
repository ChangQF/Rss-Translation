<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Fri, 01 Nov 2024 12:31:39 GMT</lastBuildDate>
    <item>
      <title>神经网络推理的最快方法？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gh3hhn/quickest_method_of_neural_network_inference/</link>
      <description><![CDATA[我遇到的情况是需要多次从已加载的 NN 模型中进行预测。 请注意，我每次仅对小批量数据调用 .predict()。不确定这是否相关，但可能相关。 我本质上是在运行模拟（~10k 次迭代），每次迭代我都会调用网络进行预测。 我需要大幅加快速度。 就机器而言，我受到 CPU 限制（没有 GPU/TPU）。 我发现的当前最佳解决方案是将我的模型转换为 ONNX 模型，创建一个 InferenceSession 对象，并将此对象传递给我的各个工作器（以在我的 CPU 之间分配工作）。 ONNX 针对推理进行了优化，因此这已经足够好了。我已经修改了操作线程间/内属性等等。 我也尝试过量化我的模型，这有一点帮助，但作用不大。 有没有更好的方法？还有谁知道的其他技术吗？    提交人    /u/BasslineButty   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gh3hhn/quickest_method_of_neural_network_inference/</guid>
      <pubDate>Fri, 01 Nov 2024 11:49:24 GMT</pubDate>
    </item>
    <item>
      <title>Bert 分类器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gh3ahm/bert_classifier/</link>
      <description><![CDATA[有人可以分享使用 BERT 进行多标签分类的博客或教程吗？我将围绕它为黑客马拉松做一个项目    提交人    /u/RandiyOrtonu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gh3ahm/bert_classifier/</guid>
      <pubDate>Fri, 01 Nov 2024 11:37:53 GMT</pubDate>
    </item>
    <item>
      <title>microk8s 与 minikube，哪个更适合 kubeflow？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gh1n08/microk8s_vs_minikube_which_is_better_for_kubeflow/</link>
      <description><![CDATA[最适合学习本地 kubernetes 和运行本地 kubeflow 管道的是哪个 kubernetes 发行版？    提交人    /u/Relative_Rope4234   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gh1n08/microk8s_vs_minikube_which_is_better_for_kubeflow/</guid>
      <pubDate>Fri, 01 Nov 2024 09:45:03 GMT</pubDate>
    </item>
    <item>
      <title>“是什么让 GPU 如此强大？矩阵乘法！”</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggzx8t/what_makes_gpus_so_powerful_matrix_multiplication/</link>
      <description><![CDATA[      https://preview.redd.it/xm1m3w4as8yd1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=f7ddc7e1e90137325b0653d931a7f807f027c70e GPU 已成为当今最必不可少（且最昂贵）的硬件之一。在我发布在 Vizuara 的 YouTube 频道上的最新视频中，我探讨了 GPU 为何如此强大、它们有何不同，以及它们如何从提升游戏图形到改造 AI：https://www.youtube.com/watch?v=wYXARXhSoSs&amp;feature=youtu.be GPU 强大的背后是它们能够快速并行执行大量矩阵乘法。矩阵乘法是 3D 图形渲染和 AI 模型训练的核心。 在游戏中，每个 3D 对象都被分解为顶点和三角形，每次游戏场景刷新时，GPU 都必须使用矩阵数学快速重新计算位置、纹理和光照。高质量的游戏会渲染数百万个顶点和三角形。如果没有 GPU，我们所熟知的游戏根本无法实现。 2008 年，我记得我曾尝试在一台没有显卡的旧电脑上运行《侠盗猎车手：圣安地列斯》。我不得不降低分辨率才能让游戏可玩。与此同时，拥有专用显卡的朋友正在享受无缝的高分辨率游戏体验。那是我第一次真正接触到 GPU 的功能。 当时，NVIDIA 仍然主要专注于增强游戏视觉效果。但是，一旦深度学习出现，GPU 用于图形的矩阵乘法运算最终就非常适合 AI。 事实上，随着 AI 研究人员开始训练更大的神经网络，他们发现用于 3D 场景的相同类型的重复矩阵数学也适用于神经网络计算。NVIDIA 能够顺利地从专注于游戏过渡到走在 AI 硬件的最前沿。当我第一次听说 NVIDIA 在 AI 中的作用日益增强时，我不确定它是否真的会腾飞。现在，随着他们的股价飙升和人工智能需求达到历史最高水平，很明显他们在一个意想不到的市场中发现了金矿。 在当今的人工智能世界中，像谷歌和 OpenAI 这样的公司正在追逐人工智能的进步，就像淘金热中的矿工一样。然而，NVIDIA 提供了“铲子”——使这场人工智能革命成为可能的 GPU。 这些 GPU 的价格从 10,000 美元到高端型号的 40,000 美元不等，可执行大规模训练人工智能模型所需的矩阵乘法和并行计算。由于可用的芯片数量有限且需求不断增长，NVIDIA 已迅速成为科技界最有价值的公司之一。 对于那些想知道 GPU 对游戏和人工智能为何如此重要的人来说，我的视频对此进行了分解。我介绍了 GPU 处理的特定矩阵运算和转换，解释了为什么这些设备对于游戏爱好者和 AI 研究人员来说都物有所值。 GPU 从小众游戏硬件到 AI 强国的历程是一个鼓舞人心的创新、适应和令人惊讶的新应用的故事。如果您对这些突破背后的深层机制感兴趣，请查看我的视频中的完整分解：https://www.youtube.com/watch?v=wYXARXhSoSs&amp;feature=youtu.be    提交人    /u/thesreedath   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggzx8t/what_makes_gpus_so_powerful_matrix_multiplication/</guid>
      <pubDate>Fri, 01 Nov 2024 07:26:28 GMT</pubDate>
    </item>
    <item>
      <title>如果物体是矩形物体，从稍微倾斜的角度拍摄，可以给出顶部和侧面的一部分视图，则如何标记物体</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggzsqi/how_to_label_object_if_the_object_are_rectangular/</link>
      <description><![CDATA[      我是一名正在学习计算机视觉的学生，在我的课堂上，我被分配训练yolo-obb（定向物体检测）使用自定义数据集 当我尝试标记自己的数据集时，我意识到它不是完全平坦的，只能提供侧面的部分视图 我应该标记所有可见部分（图像 1）还是只标记顶部部分（例如图像 2）？ 如果我的英语不好，请原谅 图片 1 图片 2    提交人    /u/MeasurementChoice917   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggzsqi/how_to_label_object_if_the_object_are_rectangular/</guid>
      <pubDate>Fri, 01 Nov 2024 07:15:40 GMT</pubDate>
    </item>
    <item>
      <title>寻找包含以下内容的书籍/课程：训练 VAE、主要机器/深度学习模型概述，最好是在 pytorch 中</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggyct5/looking_for_bookcourses_that_includes_training/</link>
      <description><![CDATA[我目前正在阅读《傻瓜机器学习》，原本计划阅读《傻瓜深度学习》。但这本书太糟糕了。是的，它涵盖了基础知识。但描述和代码在完全不同的章节中。当你读到代码时，你已经忘记了描述。此外，他们几乎没有解释代码。 所以，我正在寻找一本替代书籍/课程来涵盖所有基本的机器/深度学习模型。理想情况下使用 pytorch，因为我计划在未来的项目中使用原生 pytorch FSDP（而不是像加速之类的东西）。并且理想情况下深入介绍训练 VAE，因为这是我真正感兴趣的当前主题。 http://neuralnetworksanddeeplearning.com 看起来有点基础。通过阅读《理解深度学习》的几章，我熟悉了其中的大部分概念。 d2l.ai 看起来不错，但我在大纲中没有看到任何关于 VAE 的内容。 背景：我是一名业余爱好者，计划以后微调 LLM 和文本到图像模型以供工作使用。学习的数学：线性代数、微积分、基础统计/概率。目前正在阅读《学生贝叶斯统计指南》。可以用 Python 编写一些基本的东西。比如，我编写了一个带有扩散器和加速 FSDP 的多 GPU sdxl 微调脚本，带有自定义数据集类和验证函数以及其他自定义内容。目前正在将其移植到 sd3.5 介质，并进行 CLIP 微调（我不喜欢通过 UNET 训练 CLIP 的想法）。 提前致谢！    提交人    /u/Aware_Photograph_585   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggyct5/looking_for_bookcourses_that_includes_training/</guid>
      <pubDate>Fri, 01 Nov 2024 05:24:11 GMT</pubDate>
    </item>
    <item>
      <title>我应该发布关于机器学习的笔记/博客吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggxm3q/should_i_post_my_notes_blog_on_machine_learning/</link>
      <description><![CDATA[大家好， 我是机器学习的硕士生（本科电气和计算机工程专业 + 3 年软件/网络开发经验）。现在，我是一名全日制学生，也是机器学习实验室的研究助理。 事情是这样的：我在机器学习方面完全是个菜鸟。比如，如果你认为使用 API 和人工智能工具意味着你“了解机器学习”，那么我要说这不算数。我对机器学习着迷了一段时间，并试图自学，但大多数课程都非常抽象。 事实证明，机器学习涉及大量数学。当然，有一些很酷的库，但如果你不懂数学，祝你好运改进你的模型。过去几个月，我一直在研究一些复杂的数学——高级线性代数、矩阵方法、信息论——同时在我的实验室从头开始构建一个 transformer 训练管道。这太让人不知所措了。说实话，我因为迷茫而崩溃了好几次。 但事情开始变得明朗起来。我最大的挣扎是不知道为什么以及如何使用我所学的东西。感觉我只是顺其自然，希望它最终会有意义，有时确实如此……但它花费的时间比它应该的要长得多。另外，我有没有提到数学？这不是高中数学；我们说的是研究生水平，甚至是博士水平的数学。而且大多数时候，你必须阅读最近的研究论文并解码这些符号以将它们应用于你的问题。 所以这是我的问题是：我挣扎了很多，也许其他人也一样？也许我只是反应慢。但我一路上都做了笔记，试图简化那些我希望有人能更好地解释的概念。我应该将它们作为博客/子堆栈/网站分享吗？我觉得知识最好是分享的，尤其是在一个想要一起学习的社区里。我很乐意与大家一起学习，一起探索这些很酷的东西。 关于从哪里开始或哪种格式可能最好的想法？    提交人    /u/ziggyboom30   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggxm3q/should_i_post_my_notes_blog_on_machine_learning/</guid>
      <pubDate>Fri, 01 Nov 2024 04:33:48 GMT</pubDate>
    </item>
    <item>
      <title>迷路了！救命</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggxjgf/lost_help/</link>
      <description><![CDATA[所以，我刚刚开始学习机器学习，老实说，我不知道从哪里开始！如果有人有一些可靠的免费资源可以系统地列出所有内容，我将不胜感激。任何关于我应该如何开始的提示也都很棒。    提交人    /u/BowlInternational584   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggxjgf/lost_help/</guid>
      <pubDate>Fri, 01 Nov 2024 04:28:57 GMT</pubDate>
    </item>
    <item>
      <title>ML 工程师 techstack - 2024 年</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggx4yk/ml_engineer_techstack_2024/</link>
      <description><![CDATA[感谢大家的帮助。 对于任何计划从大数据/后端转向 MLE 作为工作角色的人，您会推荐哪种技术堆栈。我知道大多数基本原理保持不变，但肯定有一些偏好是大多数公司使用的工具？ 例如：在数据中，那就是 Spark 和 Airflow 对 MLE 有任何类似的想法吗？    提交人    /u/RobotsMakingDubstep   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggx4yk/ml_engineer_techstack_2024/</guid>
      <pubDate>Fri, 01 Nov 2024 04:03:53 GMT</pubDate>
    </item>
    <item>
      <title>超越原始文本分类？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggu83s/beyond_vanilla_text_classification/</link>
      <description><![CDATA[我经常使用 HuggingFace Transformer 的框架来微调类似 BERT 的文本分类模型。在我的任务上，性能通常相当不错，而且我尝试扩展完全原始的管道来改进它。具体来说，我尝试过堆叠模型、用合成样本扩充我的训练数据集，以及训练自定义 QA 模型以在分类之前从每个文档中分割重要文本。 这一切都有帮助，但仍有改进的空间，而且我的想法已经用完了。对于那些在这个范式中训练文本分类器的人，你们使用了哪些创造性的方法来提高性能，超越了原始的&amp;32; 微调变压器模型&amp;32;？    提交人    /u/empirical-sadboy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggu83s/beyond_vanilla_text_classification/</guid>
      <pubDate>Fri, 01 Nov 2024 01:17:46 GMT</pubDate>
    </item>
    <item>
      <title>首次微调模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggtn3n/first_time_fine_tuning_model/</link>
      <description><![CDATA[我如何微调像 Llama 3 这样的模型以从给定的描述中提取重要信息？另外，我必须手动完成这个过程吗？我希望它提取非常具体的数据并以特殊的方式组织它，所以我想我必须提示它，告诉它输出是否正确，并继续生成我自己的数据。有没有办法自动生成数据，这样我就不必总是手动操作了？ 这是我第一次这样做，所以任何提示和指导都很好。谢谢！    提交人    /u/Droski_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggtn3n/first_time_fine_tuning_model/</guid>
      <pubDate>Fri, 01 Nov 2024 00:47:24 GMT</pubDate>
    </item>
    <item>
      <title>在自定义数据集上训练 PyTorch DeepLabV3</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggtefu/train_pytorch_deeplabv3_on_custom_dataset/</link>
      <description><![CDATA[      在自定义数据集上训练 PyTorch DeepLabV3 https://debuggercafe.com/train-pytorch-deeplabv3-on-custom-dataset/ 语义分割与深度学习结合可以增加巨大的价值。语义分割在医学成像、环境成像和卫星图像领域有多种应用。涉足这些领域中的任何一个并开展项目都可以提供大量知识。在本文中，我们将从单类语义分割开始。我们将在自定义数据集上训练 PyTorch DeepLabV3 模型。该数据集由水体的卫星图像组成。 https://preview.redd.it/46hc1rwxq6yd1.png?width=1000&amp;format=png&amp;auto=webp&amp;s=0633f6446953c86193b1a03e453ea21657370541    提交人    /u/sovit-123   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggtefu/train_pytorch_deeplabv3_on_custom_dataset/</guid>
      <pubDate>Fri, 01 Nov 2024 00:35:01 GMT</pubDate>
    </item>
    <item>
      <title>VAE 中的后验、证据、先验和可能性是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ggp3u5/what_is_the_posterior_evidence_prior_and/</link>
      <description><![CDATA[      嗨， 在变分自动编码器 (VAE) 中，我们尝试学习一些数据的分布。为此，我们有“两个”端到端训练的神经网络。第一个网络，即编码器，对分布 q(z|x) 进行建模，即给定 x 预测 z。第二个网络对后验 q(x|z) 和 p_theta(x|z) 进行近似建模，即，对给定潜在变量 z 的 x 采样分布进行建模。 https://preview.redd.it/ti93kfzsp5yd1.png?width=658&amp;format=png&amp;auto=webp&amp;s=cbe1ff00503ed8dfdd11145ef37fd030e07d0475 阅读文献似乎 VAE 的优化目标是最大化 ELBO。这意味着最大化 p_theta(x)。但是，我想知道 p_theta(x) 不是先验吗？它是证据吗？ 我的疑问只是关于行话。让我解释一下。对于具有两个随机变量 A 和 B 的给定条件概率，我们有： p(B|A) = p(A|B)*p(B)/P(A) - p(B|A) 是后验 - p(A|B) 是似然 - p(B) 是先验 - P(A) 是证据 好吧，对于 VAE，解码器将尝试近似后验 q(x|z)。在 VAE 中，似然是 q(z|x)，这意味着后验是 q(x|z)，证据是 q(z)，先验是 q(x)。好吧，如果 VAE 的目标是最大化 ELBO（证据下限），并且 p_theta(x|z) 是后验 q(x|z) 的近似值，那么鉴于 q(z) 是证据，证据应该是 p_theta(z)，对吗？这就是我不明白的，因为他们说 p_theta(x) 现在是证据……但那是 q 中的先验…… q 和 p_theta 分布是否不同，并且它们具有不同的似然、先验、证据和后验？q 和 p_theta 的似然、先验、证据和后验是什么？ 谢谢！    提交人    /u/CompSciAI   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ggp3u5/what_is_the_posterior_evidence_prior_and/</guid>
      <pubDate>Thu, 31 Oct 2024 21:07:10 GMT</pubDate>
    </item>
    <item>
      <title>重新审视机器学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1gghau0/revisiting_ml/</link>
      <description><![CDATA[我之前于 2013 年在布里斯托尔获得了机器学习硕士学位，但最终从事金融软件工作，因为当时该领域非博士生的工作机会并不多。 显然，深度学习从那时起就一路飙升。 有没有人对我们这些已经离开十年左右的人有什么建议？    提交人    /u/NotSoEnlightenedOne   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1gghau0/revisiting_ml/</guid>
      <pubDate>Thu, 31 Oct 2024 15:30:31 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>