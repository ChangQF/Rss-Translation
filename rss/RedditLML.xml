<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Wed, 16 Oct 2024 12:33:06 GMT</lastBuildDate>
    <item>
      <title>在远程服务器上运行 ML 模型会引发错误，但在本地运行正常</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4xw31/running_ml_model_on_remote_server_throws_error/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4xw31/running_ml_model_on_remote_server_throws_error/</guid>
      <pubDate>Wed, 16 Oct 2024 12:13:21 GMT</pubDate>
    </item>
    <item>
      <title>Unity MLAgents Toolkit 和 Snake 等训练游戏</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4xu8j/unity_mlagents_toolkit_and_training_games_like/</link>
      <description><![CDATA[大家好， 我一直在尝试理解神经网络和游戏 AI 的训练。但我目前在努力玩 Snake。我想“好吧，让我们给它一些射线传感器、一个摄像头传感器，吃食物时给予奖励，与自身或墙壁碰撞时给予负面奖励”。 我想说它学得很好，但并不完美！在 10x10 的游戏场中，它的最高分约为 50，但到目前为止它从未掌握游戏。 有人能给我一些建议或线索，如何更好地处理使用 PPO 进行蛇 AI 训练吗？ 射线传感器可以检测墙壁、蛇本身和食物（3 个不同的传感器，每个传感器有 16 条射线） 摄像头传感器的分辨率为 50x50，也可以看到墙壁、蛇头以及蛇周围的蛇尾。它是一个尺寸为 8 的正交相机，因此它可以看到整个运动场。 首先，我只使用射线传感器进行测试，然后我添加了相机传感器，我可以说的是，它通过相机视觉观察学习得更快，但最后它的最高分大约相同。 我正在并行训练 10 个代理。 网络设置为： 50x50x1 视觉观察输入 大约 100 个射线观察输入 512 个隐藏神经元 2 个隐藏层 4 个离散输出动作 我目前正在尝试使用 buffer_size 为 25000 和 batch_size 为 2500。学习率为 0.0003，Num Epoch 为 3。时间范围设置为 250。 是否有人使用过 Unity 的 ML Agents Toolkit 并能帮助我一点？ 我做错了什么吗？ 我将感谢你们给予我的每一次帮助！ 这里有一个小视频，你可以在其中看到大约第 150 万步的培训： https://streamable.com/tecde6    提交人    /u/Seismoforg   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4xu8j/unity_mlagents_toolkit_and_training_games_like/</guid>
      <pubDate>Wed, 16 Oct 2024 12:10:41 GMT</pubDate>
    </item>
    <item>
      <title>使用树结构生成临床记录</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4xrg3/clinical_note_generation_using_tree_structure/</link>
      <description><![CDATA[大家好。我正在做一个项目，项目要求我输入医生和病人之间的对话，并输出一份临床记录。 最近我读了一篇论文：https://arxiv.org/abs/2401.18059  基本上，在本文中，他们首先根据标记数量将文档切分为块。块及其对应的 Sbert 嵌入形成一个节点。然后他们使用软聚类算法，即高斯混合模型对这些节点进行分组。聚类过程包括全局聚类和局部聚类，如果局部聚类组合上下文超过模型的最大标记输入，则它将再次聚类。一旦聚类成功，它们就会由 LLM 进行汇总。然后再次嵌入这些总结的文本块，并继续嵌入、聚类和总结的循环，直到聚类变得不可行。 根据论文的想法，我想出了这样的流程。首先，我将预处理输入对话，然后将它们切成块。接下来，我将使用 GMM 对它们进行聚类。值得注意的是，我也获得了样本临床笔记，在临床笔记中，有章节。因此，我将通过嵌入提供的临床笔记来启动聚类的质心。接下来，像论文一样，我将进行嵌入、聚类然后总结的循环。之后，我将计算树的叶节点和示例部分之间的余弦模拟，以将它们放入正确的部分。 您对这种方法有何看法？对我来说，它有很多挑战，比如分块可能会丢失上下文，或者重新聚类可能会使质心转移到其他地方，等等。但我真的很喜欢这里的软聚类的想法，因为它可以帮助我看到每个块属于多个部分的概率。    提交人    /u/Ok-Elderberry3966   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4xrg3/clinical_note_generation_using_tree_structure/</guid>
      <pubDate>Wed, 16 Oct 2024 12:06:22 GMT</pubDate>
    </item>
    <item>
      <title>我是如何开始学习机器学习的</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4x299/how_i_started_learning_machine_learning/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4x299/how_i_started_learning_machine_learning/</guid>
      <pubDate>Wed, 16 Oct 2024 11:26:39 GMT</pubDate>
    </item>
    <item>
      <title>音乐和声音嵌入模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4x0zm/music_and_sound_embedding_models/</link>
      <description><![CDATA[大家好， 我目前正在做一个业余项目，使用生成的嵌入对不同的声音和音乐进行分类。我尝试了 MFCC，但结果并不理想。因此，我想探索一种深度学习模型，并遇到了 Wav2Vec。但是，似乎大多数预训练模型都是为语音识别而设计的。您对可以提供给定声音文件（作为 NumPy 数组）的嵌入的即插即用模型有什么建议吗？专注于音乐是理想的，但更通用的模型也是可以接受的。 谢谢！    提交人    /u/DerKaggler   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4x0zm/music_and_sound_embedding_models/</guid>
      <pubDate>Wed, 16 Oct 2024 11:24:27 GMT</pubDate>
    </item>
    <item>
      <title>安装 Espnet2 时出现 Conda 错误</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4w7w0/conda_error_while_installing_espnet2/</link>
      <description><![CDATA[你好！我正在尝试通过 conda（从 gi​​thub 克隆）在 WSL2 上安装 espnet2 环境，就像官方手册中说的一样，但由于某种原因，它失败了。我尝试了从 3.8 到 3.12 的版本，但没有什么好的结果。错误指出：``` 发生了意外错误。Conda 已准备好上述报告。如果您怀疑此错误是由插件故障引起的，请考虑使用 --no-plugins 选项关闭插件。 示例：conda --no-plugins install &lt;package&gt; 或者，您可以在命令行上设置 CONDA_NO_PLUGINS 环境变量以在未启用插件的情况下运行命令。 示例：CONDA_NO_PLUGINS=true conda install &lt;package&gt; ``` 有人说这是因为 Fonda tdqm 已损坏，但我通过 pip 手动安装了它，什么都没有改变。然后重新安装了 conda，仍然没有进展。 Espnet2 存储库：https://github.com/espnet/espnet Espnet2 安装手册：https://espnet.github.io/espnet/notebook/ESPnet2/Course/CMU_SpeechRecognition_Fall2022/recipe_tutorial.html#check-installation    提交人    /u/Ksauxion   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4w7w0/conda_error_while_installing_espnet2/</guid>
      <pubDate>Wed, 16 Oct 2024 10:32:58 GMT</pubDate>
    </item>
    <item>
      <title>为基于机器学习和区块链的项目提供帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4w6jy/help_for_a_project_based_on_ml_and_blockchain/</link>
      <description><![CDATA[目标：构建基于机器学习的异常检测系统，该系统可以有效检测基于区块链的平台上的异常模式或潜在的欺诈性微交易。该系统应该能够处理区块链的独特特征（如分散记录、假名用户和不可变交易）并扩展到各种类型的微交易。 关键考虑因素： 微交易的实时分析。 处理区块链的分散性和假名性质。 结合交易金额、频率、钱包年龄和交易关系等功能。 与区块链 API 集成以提取数据进行训练和验证。 以可理解的方式向用户解释检测到的异常的框架（例如风险评分）。 预期结果： 一种基于 AI 的异常检测系统，能够实时标记可疑的微交易。  考虑的关键属性的摘要，包括交易规模、频率和与钱包相关的信息。  一种可视化标记异常及其随时间变化的风险评分的方法。    提交人    /u/Imaginary-Milk-1238   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4w6jy/help_for_a_project_based_on_ml_and_blockchain/</guid>
      <pubDate>Wed, 16 Oct 2024 10:30:18 GMT</pubDate>
    </item>
    <item>
      <title>从 Colab Notebook 到生产代码</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4v98g/from_colab_notebook_to_production_code/</link>
      <description><![CDATA[我是应用数学专业的大二学生，也是生产环境代码的新手，之前我只使用笔记本文件编写了一些简单的 Torch NLP 项目。我查看了带有代码的论文端到端生产仓库，它让我感到紧张和迷失，但我获得了一些需要学习的材料。  模块化文件  为了实现可重用性和可维护性，我必须将 Pytorch 工作流拆分为 5 个基本 Python 脚本，它们是：utils、data_process_and_loader、trainer、model、evaluation  Linux  生产代码必须在 Linux 和 SSH 远程服务器中运行以进行训练和服务 Linux 环境比 Windows 或 macOS 更安全  日志记录  开发人员必须编写日志来跟踪其性能并使用代码进行动态调试  CI/CD  这有助于在与团队合作时维护和保存编码管道   有没有还有什么材料需要学习吗？我如何构造我的代码以匹配生产风格，是否有任何学习资源/GitHub repos 可以帮助我熟悉生产代码？ 提前致谢    提交人    /u/Disastrous-Sand8882   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4v98g/from_colab_notebook_to_production_code/</guid>
      <pubDate>Wed, 16 Oct 2024 09:24:17 GMT</pubDate>
    </item>
    <item>
      <title>我从哪里开始？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4ulc0/where_do_i_start/</link>
      <description><![CDATA[我是一名 Python 后端开发人员，希望进入生成式人工智能领域。我应该从哪里开始？我应该像数据科学家或机器学习工程师一样从头开始学习机器学习吗？    提交人    /u/d_danso   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4ulc0/where_do_i_start/</guid>
      <pubDate>Wed, 16 Oct 2024 08:33:33 GMT</pubDate>
    </item>
    <item>
      <title>如何在图像标题生成中包含外部元数据（如位置）？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4tu5d/how_to_include_external_metadata_like_location_in/</link>
      <description><![CDATA[我正在训练一个模型来为图像生成标题，我正在使用一个数据集，其中我连接了视觉特征、文本嵌入和一些额外的外部信息，例如来自开源数据的“位置：亚利桑那州”。 目标是让生成的标题包含这些额外信息（例如，“这张照片来自亚利桑那州市”）。但是，这些元数据不包含在训练数据的真实标题中（例如，“这是一张城市的照片”）。 是否可以引导模型将这些外部元数据包含在生成的标题中？如果可以，实现这一目标的最佳方法是什么？任何提示都会有所帮助。    提交人    /u/LePotatoShark   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4tu5d/how_to_include_external_metadata_like_location_in/</guid>
      <pubDate>Wed, 16 Oct 2024 07:34:02 GMT</pubDate>
    </item>
    <item>
      <title>还有人读过 Kevin Murphy 的《概率机器学习——导论》吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4souj/is_anyone_else_reading_kevin_murphys/</link>
      <description><![CDATA[      我想看看是否有其他人正在读这本书，并且可以澄清他在 XGboost 和神经网络部分的一些观点？    提交人    /u/learning_proover   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4souj/is_anyone_else_reading_kevin_murphys/</guid>
      <pubDate>Wed, 16 Oct 2024 06:08:23 GMT</pubDate>
    </item>
    <item>
      <title>F5-TTS：开源音频克隆模型（效果很好）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4s0hb/f5tts_opensourced_audio_cloning_model_results_are/</link>
      <description><![CDATA[F5-TTS 是一种新的音频克隆模型，可以产生高质量的结果，延迟时间短。它甚至可以根据您的脚本在您的音频中生成播客。在此处查看演示：https://youtu.be/YK7Yi043M5Y?si=AhHWZBlsiyuv6IWE    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4s0hb/f5tts_opensourced_audio_cloning_model_results_are/</guid>
      <pubDate>Wed, 16 Oct 2024 05:20:51 GMT</pubDate>
    </item>
    <item>
      <title>Deep Atlas ML Bootcamp - 评测</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4p1rw/deep_atlas_ml_bootcamp_review/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4p1rw/deep_atlas_ml_bootcamp_review/</guid>
      <pubDate>Wed, 16 Oct 2024 02:26:05 GMT</pubDate>
    </item>
    <item>
      <title>使用 LivePortrait 矫正眼神接触</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1g4ar3x/eye_contact_correction_with_liveportrait/</link>
      <description><![CDATA[        提交人    /u/happybirthday290   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1g4ar3x/eye_contact_correction_with_liveportrait/</guid>
      <pubDate>Tue, 15 Oct 2024 15:42:33 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>