<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Fri, 10 May 2024 06:18:54 GMT</lastBuildDate>
    <item>
      <title>曼巴玩具示例</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1coig9j/mamba_toy_examples/</link>
      <description><![CDATA[我已经使用（非预训练的）Mamba 模型一段时间了，但我似乎无法让它工作。代码运行后，损失（MSE；由 ASGD 优化）在训练数据上下降得几乎和我想要的一样低（是的，过度拟合，但现在还好），但它似乎从来没有学习过实际的任务。&lt; /p&gt; 例如，我尝试生成各种分布的输入，在所有情况下正确的标签都是 1.0。应该是一项微不足道的任务，因为它可以通过“模型”来完成。有 1 个参数。而且，如前所述，我得到了非常好的低损失值，但是运行实际张量给出的值根本不接近 1.0 任何关于潜在修复或工作玩具示例的想法，将不胜感激.   由   提交 /u/Rhoderick   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1coig9j/mamba_toy_examples/</guid>
      <pubDate>Fri, 10 May 2024 05:58:28 GMT</pubDate>
    </item>
    <item>
      <title>YOLO v8 中的帧检测</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cohi0z/frame_detection_in_yolo_v8/</link>
      <description><![CDATA[1) 我正在运行 YOLOv8 模型 2) 用于对象检测，我的代码使用其中的 2 个：一个通用版本，用于检测车辆和另一辆专门经过预先训练来检测车牌的车辆 3）我遇到的问题是，有关车架号的输出似乎会永远持续下去，如果我给它设置一个上限，它并不能准确地检测到车牌还没有，它应该将输出保存在按帧号序列化的 csv 文件中，但它没有这样做 4) 我不确定整个帧速率以及这是否是问题   由   提交/u/varun-saha  /u/varun-saha  reddit.com/r/learnmachinelearning/comments/1cohi0z/frame_detection_in_yolo_v8/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cohi0z/frame_detection_in_yolo_v8/</guid>
      <pubDate>Fri, 10 May 2024 04:57:03 GMT</pubDate>
    </item>
    <item>
      <title>在线性回归中，预测时必须满足“无多重共线性”假设？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1coh5u0/in_linear_regression_no_multicollinearity/</link>
      <description><![CDATA[我在某处了解到，当我们进行预测时，假设没有任何意义，但对于推理来说，它很重要......就像当 chatgpt 时我感到困惑一样和其他消息来源说不同...    由   提交 /u/Assalamwhileicum   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1coh5u0/in_linear_regression_no_multicollinearity/</guid>
      <pubDate>Fri, 10 May 2024 04:36:14 GMT</pubDate>
    </item>
    <item>
      <title>您如何找到适合您的研究论文？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cogs39/how_do_you_find_the_research_papers_right_for_you/</link>
      <description><![CDATA[所以我是机器学习的初学者，我观察到很多人在阅读研究论文。他们为什么这么做？他们如何找到适合他们的论文（从某种意义上说，有很多论文适合他们）？读完论文后会发生什么？ ~凯   由   提交 /u/Weak_Display1131   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cogs39/how_do_you_find_the_research_papers_right_for_you/</guid>
      <pubDate>Fri, 10 May 2024 04:13:09 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyTorch 的混合网络简介</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cocpjb/introduction_to_hybridnets_using_pytorch/</link>
      <description><![CDATA[使用 PyTorch 的混合网络简介 https://debuggercafe.com/introduction-to-hybridnets-using-pytorch/   由   提交/u/sovit-123  /u/sovit-123  reddit.com/r/learnmachinelearning/comments/1cocpjb/introduction_to_hybridnets_using_pytorch/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cocpjb/introduction_to_hybridnets_using_pytorch/</guid>
      <pubDate>Fri, 10 May 2024 00:37:36 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士/自然语言处理</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cochi1/llmnlp/</link>
      <description><![CDATA[大家好，  我正在尝试为我的工作学习 nlp/llm 技术。我在不同的领域（物理学），想学习使用 NLP/LLM 模型来实现这些目标。我不太清楚如何去学习这些。当我查阅传统的 NLP 课程时，大多数都是处理词袋、vec2vec 等。但是 Transformer 的引入是否让这些传统方法变得过时了呢？我该如何学习这些？谁能为我提供学习此内容所需遵循的路径指南？归根结底，我想构建一个类似 BERT 的模型来用于预测。如果问题听起来含糊不清，我深表歉意。    由   提交/u/No-Mud4063  /u/No-Mud4063 reddit.com/r/learnmachinelearning/comments/1cochi1/llmnlp/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cochi1/llmnlp/</guid>
      <pubDate>Fri, 10 May 2024 00:26:40 GMT</pubDate>
    </item>
    <item>
      <title>数据科学和机器学习从基础到高级 |免费 Udemy 课程，报名人数有限</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1coaipi/data_science_and_machine_learning_basic_to/</link>
      <description><![CDATA[       由   提交/u/Ordinary_Craft   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1coaipi/data_science_and_machine_learning_basic_to/</guid>
      <pubDate>Thu, 09 May 2024 22:54:29 GMT</pubDate>
    </item>
    <item>
      <title>为什么文字、音频、视频的生成同时进步很快？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1coa7wz/why_has_text_audio_video_generation_all/</link>
      <description><![CDATA[根据我对这个主题的虚拟理解 文本生成是通过依赖于首先编写的转换器的 LLM 完成的关于“注意”就是您所需要的。  图像是通过扩散模型完成的，这与LLM完全不同。  那么，为什么它们似乎同时呈指数级增长，而几年前这对于普通人来说几乎是不可想象的。还有很多其他的例子，比如蛋白质等。  我应该添加逻辑告诉我有一个组件（我认为是注意力机制？）可以用于一切，所以它占据了整个领域向前 ？  我想这是一个完全不同的问题，但是扩散模型可以用来生成文本吗？我再次假设您会生成错误的输出，但是在粒度级别上，您如何告诉模型您想要输出的格式？  对这篇糟糕的帖子表示歉意，但我认为这对像我这样的很多傻瓜来说是一个障碍，无法更好地理解这些东西。    由   提交/u/bigmad99  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1coa7wz/why_has_text_audio_video_generation_all/</guid>
      <pubDate>Thu, 09 May 2024 22:41:15 GMT</pubDate>
    </item>
    <item>
      <title>未毕业指南：柯尔莫哥洛夫-阿诺德网络</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1co9xha/didnt_graduate_guide_to_kolmogorovarnold_networks/</link>
      <description><![CDATA[       由   提交 /u/import_torch-nn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1co9xha/didnt_graduate_guide_to_kolmogorovarnold_networks/</guid>
      <pubDate>Thu, 09 May 2024 22:28:27 GMT</pubDate>
    </item>
    <item>
      <title>使用大型数据集训练大型模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1co8v48/training_a_large_model_with_large_dataset/</link>
      <description><![CDATA[我正在用一个非常大的数据集（大约 100M 数据点）进行 Kaggle 竞赛。我已经完成了对少量数据的研究，并创建/缩小了 30 个特征。我计划训练 keras 模型，但我不知道它应该有多大（层数和层大小）。我还想知道应该如何尽快训练这个模型。我有一个旧的 AMD gpu，所以我必须使用 colab 之类的。我需要一些帮助来了解如何选择模型的大小以及如何尽可能快速、尽可能便宜地使用如此多的数据进行训练。   由   提交/u/古生物学家-Over   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1co8v48/training_a_large_model_with_large_dataset/</guid>
      <pubDate>Thu, 09 May 2024 21:42:16 GMT</pubDate>
    </item>
    <item>
      <title>JSTOR 被发现缺乏</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1co8jn0/jstor_got_caught_lacking/</link>
      <description><![CDATA[       由   提交 /u/blablablabling   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1co8jn0/jstor_got_caught_lacking/</guid>
      <pubDate>Thu, 09 May 2024 21:28:28 GMT</pubDate>
    </item>
    <item>
      <title>了解变形金刚中的注意力机制：5 分钟的视觉指南。 🧠</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1co37tn/understanding_the_attention_mechanism_in/</link>
      <description><![CDATA[      TL;DR：注意力是“可以学习的” ”，键值存储或字典的“模糊”版本。由于主要针对 NLP 和 LLM 改进了序列建模，Transformers 使用注意力并接管了以前的架构 (RNN)。 什么是注意力以及为什么它接管了法学硕士和机器学习：视觉指南   由   提交/u/ml_a_day  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1co37tn/understanding_the_attention_mechanism_in/</guid>
      <pubDate>Thu, 09 May 2024 17:46:47 GMT</pubDate>
    </item>
    <item>
      <title>最大的小丑奖颁发给：</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cntx3p/the_biggest_clown_award_goes_to/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/yphase  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cntx3p/the_biggest_clown_award_goes_to/</guid>
      <pubDate>Thu, 09 May 2024 10:26:36 GMT</pubDate>
    </item>
    <item>
      <title>我觉得这需要出现在机器学习教科书的封面上</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cnp4vj/i_feel_like_this_needs_to_be_on_the_cover_of_an/</link>
      <description><![CDATA[       由   提交 /u/blablablabling   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cnp4vj/i_feel_like_this_needs_to_be_on_the_cover_of_an/</guid>
      <pubDate>Thu, 09 May 2024 04:57:01 GMT</pubDate>
    </item>
    <item>
      <title>还有其他人对人工智能中所有不同类型的数学感到不知所措吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1cn97az/does_anyone_else_get_overwhelmed_by_all_the/</link>
      <description><![CDATA[   如此多的数学分支融合在一起，似乎没有人能真正掌握人工智能。即使是数学家也几乎不可能掌握这门学科。我和一些人交谈过，他们不知道我在说什么。此外，该领域的发展速度非常快 混合线性代数和随机微分方程是魔鬼的工作。   由   提交 /u/blablablabling   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1cn97az/does_anyone_else_get_overwhelmed_by_all_the/</guid>
      <pubDate>Wed, 08 May 2024 16:43:51 GMT</pubDate>
    </item>
    </channel>
</rss>