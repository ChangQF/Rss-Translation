<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Thu, 29 Aug 2024 09:17:44 GMT</lastBuildDate>
    <item>
      <title>有没有办法将文本生成模型的特征空间可视化？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3wke0/is_there_a_way_to_visualize_the_feature_space_of/</link>
      <description><![CDATA[基本上就是标题。 我知道它们是仅解码器模型，但内部存在某种表示。 有没有办法有效地将其可视化？ 我想知道这些模型之间的区别，但似乎无法想出一种合理的方法来做到这一点。 如果之前已经尝试过，有人有结果的链接吗？ 谢谢    提交人    /u/manjimin   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3wke0/is_there_a_way_to_visualize_the_feature_space_of/</guid>
      <pubDate>Thu, 29 Aug 2024 07:33:30 GMT</pubDate>
    </item>
    <item>
      <title>在 NVIDIA A100 上进行模型训练的大部分时间都处于 0%。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3wcmv/model_training_on_nvidia_a100_on_0_most_of_the/</link>
      <description><![CDATA[嗨！我有点困惑，为什么尽管使用了 A100 GPU，我的模型训练仍然很慢。在此之前，我也尝试过 A40，但每个时期处理批次的速度并没有明显差异。我在这里做错了什么吗？谢谢！ 我尝试过的事情：  设置 torch.backends.cudnn.benchmark = True 将 num_workers 设置为 &gt;4 但一旦达到 16 左右，速度就会变慢。 将 pin_memory 设置为 True 明确将我的所有功能和标签从 `.to()` 移动到 `.cuda()`  代码：https://pastebin.com/YGaS8ip5    提交人    /u/AnatoliGaming   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3wcmv/model_training_on_nvidia_a100_on_0_most_of_the/</guid>
      <pubDate>Thu, 29 Aug 2024 07:17:52 GMT</pubDate>
    </item>
    <item>
      <title>几何深度学习的 5 个 G：图形、网格、群组、测地线和仪表</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3w8f6/5_gs_of_geometric_deep_learning_graphs_grids/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3w8f6/5_gs_of_geometric_deep_learning_graphs_grids/</guid>
      <pubDate>Thu, 29 Aug 2024 07:09:34 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity AI PRO 年度优惠券仅需 25 美元</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3vy1p/perplexity_ai_pro_yearly_coupon_available_just/</link>
      <description><![CDATA[我从我的互联网提供商那里获得了 1 年的 Perplexity pro 代金券。它们在全球范围内有效，我可以通过您的电子邮件兑换。 仅接受 Paypal、Crypto、Venmo、UPI。如果您对此不确定，我的个人资料中有很多反馈。 Perplexity.ai 比 ChatGPT 拥有更多的模型。它有 GPT-4o、Claude 3 Opus、Claude 3 Sonnet、Llam 3.1 305B(Meta) 和 Sonar Large 32k。 以及来自图像生成模型：Playground v2.5、DALL-E 3 和 Stable Diffusion XL 发短信给我获取！  编辑：有关证明和反馈：https://www.reddit.com/r/accountsharing/s/A28CNDv77I    提交人    /u/Perplexity_AI_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3vy1p/perplexity_ai_pro_yearly_coupon_available_just/</guid>
      <pubDate>Thu, 29 Aug 2024 06:49:05 GMT</pubDate>
    </item>
    <item>
      <title>什么是适合构建自动编码器的超参数优化包？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3vujt/what_is_the_appropriate_hyperparameter/</link>
      <description><![CDATA[我正在尝试在 Keras Tensor Flow 中构建自动编码器。我是深度学习的新手，不知道应该选择哪个优化包。    提交人    /u/Hefty-Emergency-7222   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3vujt/what_is_the_appropriate_hyperparameter/</guid>
      <pubDate>Thu, 29 Aug 2024 06:42:09 GMT</pubDate>
    </item>
    <item>
      <title>Tensorflow、PyTorch 和 JAX 之间的哲学/概念差异？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3vcon/philosophyconceptual_differences_between/</link>
      <description><![CDATA[我一直在深入研究 JAX，因为我对 ML 硬件和性能很感兴趣。我是一个相当生疏的 ML 程序员（自五年前大学毕业以来就没有接触过太多）。我想了解每个框架背后的哲学是什么，以及为什么有人会选择一个而不是另一个？ 从高层次来看，JAX 似乎比其他框架性能更好？PyTorch 似乎是首选，而 Tensorflow 似乎没有我多年前记得的那么受欢迎了。 我也很好奇像 JAX 这样的东西如何或为什么比其他框架性能更好？    提交人    /u/stereotypical_CS   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3vcon/philosophyconceptual_differences_between/</guid>
      <pubDate>Thu, 29 Aug 2024 06:06:33 GMT</pubDate>
    </item>
    <item>
      <title>无法从头开始创建可工作的神经网络</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3v3ja/unable_to_create_a_working_neural_network_from/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3v3ja/unable_to_create_a_working_neural_network_from/</guid>
      <pubDate>Thu, 29 Aug 2024 05:48:54 GMT</pubDate>
    </item>
    <item>
      <title>已启用 Google 搜索的 RAG</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3trs9/rag_with_google_search_enabled/</link>
      <description><![CDATA[我尝试为我的 RAG 应用程序启用互联网访问，这在很多方面都有帮助，例如 1) 使用互联网验证您的数据 2) 在您的上下文中添加额外信息等。请在此处查看完整教程：https://youtu.be/nOuE_oAWxms    提交人    /u/mehul_gupta1997   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3trs9/rag_with_google_search_enabled/</guid>
      <pubDate>Thu, 29 Aug 2024 04:25:01 GMT</pubDate>
    </item>
    <item>
      <title>如何处理 2 个小数相乘导致结果较小</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3t1jf/how_to_handle_multiplying_2_decimals_resulting_in/</link>
      <description><![CDATA[您好， 我正在研究将特征重要性（表示为 0 到 1 之间的值）乘以该估计的置信度（也介于 0 和 1 之间）的方法。但是，由于要乘以两个小数，因此此计算可能会得出较小的数字。 在机器学习中如何处理这个问题？我知道我可以使用对数，但这是一个昂贵的计算，不是吗？当我们将此计算视为更大算法的一部分时，尤其如此。    提交人    /u/Grand_Comparison2081   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3t1jf/how_to_handle_multiplying_2_decimals_resulting_in/</guid>
      <pubDate>Thu, 29 Aug 2024 03:42:59 GMT</pubDate>
    </item>
    <item>
      <title>最近在一次技术面试中失败了。你在哪里学习 DL 评估、可解释性和失败分析？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3pfht/recently_failed_a_technical_interview_where_do/</link>
      <description><![CDATA[就像标题所说，我在一次面试中失败了，面试内容涉及深度学习模型评估和改进计算机视觉模型的过程。 除了通常的分类指标之外：  精度 召回率 F1 分数 准确度 ROC、AUC 平均精度  您会使用或可以使用哪些其他指标来进行彻底的故障分析？ 我谈到了采样数据和手动检查标签、检查标签分布以及使用 CAM 制作热图。 我可以使用哪些其他指标来评估深度学习模型失败的地方？ TIA！    提交人    /u/SpecCRA   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3pfht/recently_failed_a_technical_interview_where_do/</guid>
      <pubDate>Thu, 29 Aug 2024 00:42:53 GMT</pubDate>
    </item>
    <item>
      <title>在 Tiny ImageNet 上从头开始训练 AlexNet</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3pbi4/training_alexnet_from_scratch_on_tiny_imagenet/</link>
      <description><![CDATA[大家好！我最近在从头开始训练 AlexNet 时遇到了一些问题，于是发了一篇求助帖（为了好玩）。然而，这似乎只是对论文结果的简单误解。非常感谢 u/mineNombies 帮助确认我的结果。 从那时起，我写了一篇简单的博客文章，并稍微整理了一下代码。非常希望得到 ML 社区中善良人士的任何反馈！任何有关代码/博客的可读性和清晰度的信息都将不胜感激。 代码：https://github.com/aandyw/StuffFromScratch/tree/main/AlexNet 博客：https://aandyw.github.io/blog/2024/alexnet/ 再次感谢 :)  另外，稍微插件一下（嘘👎）我将来肯定会做更多类似的有趣实现，所以如果你有兴趣请给我的推特看一看！   由    /u/_aandyw  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3pbi4/training_alexnet_from_scratch_on_tiny_imagenet/</guid>
      <pubDate>Thu, 29 Aug 2024 00:37:22 GMT</pubDate>
    </item>
    <item>
      <title>澄清 VAE 中的“重新参数化技巧”及其原因</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3owae/clarification_on_the_reparameterization_trick_in/</link>
      <description><![CDATA[大家好， 我一直在研究变分自动编码器 (VAE)，并不断遇到术语“重新参数化技巧”。据我所知，该技巧涉及使用公式 (X = 平均值 + 标准偏差 * Z) 从正态分布中抽样，其中 Z 是从标准正态分布中抽取的。此公式似乎是从正态分布中抽样的标准方法 这是我的困惑： 为什么这是一个技巧？ 重新参数化“技巧”通常被强调为一个聪明的技巧，但对我来说，它似乎是转换公式的直接应用。如果 ( X = 平均值 + 标准差 * Z ) 是从正态分布中采样的唯一方法，那么为什么重新参数化技巧被认为特别具有创新性？ 我理解该技巧允许通过采样过程进行反向传播。但是，似乎使用 ( X = 平均值 + 标准差 * Z ) 是从给定 ( 平均值 ) 和 ( 标准差 ) 的正态分布中生成样本的唯一方法。除了确保可区分性之外，这个技巧还有什么特别之处？ 这是我的思维过程：我们从编码器获得平均值和标准差，并从中采样，唯一且最明显的方法是“X = 平均值 + 标准差 * Z”。 有人可以帮忙解释为什么重新参数化技巧被称为“技巧”吗？ 提前感谢您的见解！    提交人    /u/SwaroopMeher   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3owae/clarification_on_the_reparameterization_trick_in/</guid>
      <pubDate>Thu, 29 Aug 2024 00:17:07 GMT</pubDate>
    </item>
    <item>
      <title>有人能解释一下如何改进我的模型吗？详情见描述</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3kr78/can_someone_explain_me_how_can_i_improve_my_model/</link>
      <description><![CDATA[      这是关于一年的每日销售数据。我已经使用 p、q、r（2,1,1）和季节性 s 14（我怀疑每 14 天重复一次）实现了 SARIMA。但我得到了这种预测，无论我做什么，mape 仍为 30%。我尝试使用 auto arima 预测分量 AR、I 和 MA，但结果不起作用。我下一步该怎么做？    提交人    /u/Needy_Panda   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3kr78/can_someone_explain_me_how_can_i_improve_my_model/</guid>
      <pubDate>Wed, 28 Aug 2024 21:20:03 GMT</pubDate>
    </item>
    <item>
      <title>从 2025 年秋季开始在美国攻读人工智能/机器学习硕士学位是否值得</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1f3fjiu/is_worth_taking_master_aiml_in_the_us_from_2025/</link>
      <description><![CDATA[我来自韩国，到目前为止，我有 2 年 10 个月的后端软件开发工程经验。我想搬到美国找份工作并永远生活下去。据我所知，我必须获得 AI、ML 学位才能从事 STEM 工作。 我想知道现在值多少钱，并且可以在 2025 年实习和常规工作中找到工作。我在本科学校学习计算机科学。我从本科开始学习非常基础的大数据课程。我也对这个领域感兴趣。但我不擅长数学。    提交人    /u/Difficult-Present-95   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1f3fjiu/is_worth_taking_master_aiml_in_the_us_from_2025/</guid>
      <pubDate>Wed, 28 Aug 2024 17:06:29 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>