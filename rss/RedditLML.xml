<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Thu, 28 Nov 2024 06:26:15 GMT</lastBuildDate>
    <item>
      <title>实现类似Pnet(Mtcnn)模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1nnmg/implementing_similar_pnetmtcnn_model/</link>
      <description><![CDATA[我正在构建一个受 MTCNN 启发的简化人脸检测模型，旨在将 12x12 裁剪图像分类为包含人脸或不包含人脸（因此只有一个输出 1 或 0）。我使用 Wider Face 数据集对其进行训练，裁剪并调整人脸区域为 12x12，同时还包括偏移人脸（人脸的部分视图）和随机非人脸裁剪。为了进行测试，我使用 OpenCV 在 240x240 摄像头馈送上实现了滑动窗口（12x12，步长为 2）方法。如果窗口检测到人脸，则会突出显示其位置。 结果很差（我的脸经常被忽略，模型主要突出显示背景），可能是因为 12x12 的小输入尺寸丢失了关键信息，使得模型很难区分人脸和非人脸。那么你有什么建议可以解决这个问题吗？谢谢🙏 另外，我还删除了 bbox 输出，因为我认为我可以将所有突出显示的部分提供给另一个模型，以进一步区分面部和非面部。     提交人    /u/GateCodeMark   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1nnmg/implementing_similar_pnetmtcnn_model/</guid>
      <pubDate>Thu, 28 Nov 2024 04:18:49 GMT</pubDate>
    </item>
    <item>
      <title>新推理 LLM：QwQ 在多个基准测试中击败 OpenAI-o1</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1nkck/new_reasoning_llm_qwq_beats_openaio1_on_multiple/</link>
      <description><![CDATA[阿里巴巴最新的推理模型 QwQ 在许多基准测试中击败了 o1-mini、o1-preview、GPT-4o 和 Claude 3.5 Sonnet。该模型只有 32b，并且完全开源。查看如何使用它：https://youtu.be/yy6cLPZrE9k?si=wKAPXuhKibSsC810    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1nkck/new_reasoning_llm_qwq_beats_openaio1_on_multiple/</guid>
      <pubDate>Thu, 28 Nov 2024 04:13:23 GMT</pubDate>
    </item>
    <item>
      <title>𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱𝗶𝗻𝗴 𝗧𝗼𝗸𝗲𝗻𝗶𝘇𝗮𝘁𝗶𝗼𝗻 𝗶𝗻 𝗟𝗮𝗿𝗴𝗲 𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗠𝗼𝗱𝗲𝗹𝘀: 𝗪𝗼𝗿𝗱, 𝗖𝗵𝗮𝗿𝗮𝗰𝘁𝗲𝗿, 𝗮𝗻𝗱 𝗕𝘆𝘁𝗲 𝗣𝗮𝗶𝗿 𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1n1dn/𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱𝗶𝗻𝗴_𝗧𝗼𝗸𝗲𝗻𝗶𝘇𝗮𝘁𝗶𝗼𝗻_𝗶𝗻_𝗟𝗮𝗿𝗴𝗲_𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲/</link>
      <description><![CDATA[      Tokenizer 自然语言处理是自然语言处理的基石（NLP），多年来，已经开发出各种方法来优化它。其中最值得注意的方法是基于字、基于字符和字节对编码 (BPE)。 𝗪𝗼𝗿𝗱-𝗯𝗮𝘀𝗲𝗱：虽然直观，但它需要维护庞大的词汇量——高达 𝟭𝟳𝟬,𝟬𝟬𝟬 𝗰𝘂𝗿𝗿𝗲𝗻𝘁 𝘄𝗼𝗿𝗱𝘀（牛津词典）和 47,000 个过时的单词。尽管如此，它仍在努力应对未知单词标记。 𝗖𝗵𝗮𝗿𝗮𝗰𝘁𝗲𝗿-𝗯𝗮𝘀𝗲𝗱：通过将词汇量减少到英语中仅有 𝟮𝟱𝟲 𝗰𝗵𝗮𝗿𝗮𝗰𝘁𝗲𝗿𝘀，它解决了未知标记问题。但是，它无法有效地保留单词的语义。 𝗕𝘆𝘁𝗲 𝗣𝗮𝗶𝗿 𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴 (𝗕𝗣𝗘)：字节对编码 (BPE) 是一种基于子词的标记器。它的工作原理是将最常见的相邻字符对迭代合并为一个单元，直到达到所需的词汇量。它通过将单词分解为子单词、有效处理未知标记以及与基于单词的编码相比保持词汇量易于管理，达到了完美的平衡。 这种在保持语义连贯性的同时处理看不见的单词的能力使得 BPE 标记器成为大多数 𝗺𝗼𝗱𝗲𝗿𝗻 𝗹𝗮𝗿𝗴𝗲 𝗹𝗮𝗻𝗴𝘂𝗮𝗴𝗲 𝗺𝗼𝗱𝗲𝗹𝘀 的标准。 标记化创新是我们今天在 NLP 中看到的进步的关键推动因素！  要深入了解 tokenizer，我强烈建议您观看以下视频： • 使用 Python 从 Scratch 编写 LLM Tokenizer：https://youtu.be/rsy5Ragmso8 • GPT Tokenizer：字节对编码：https://youtu.be/fKd8s29e-l4 作者：Raj Abhijit Dandekar 𝘍𝘰𝘳 𝘴𝘪𝘮𝘪𝘭𝘢𝘳谢谢大家的支持：Pritam Kudale 我们的未来之路谢谢你Vizuara！ --- 您可以在此处加入新闻通讯： https://9bfb8b39.sibforms.com/serve/MUIFAJFcOMHmiNnOggw1w5qD7tmpEtKMgA6BKj_WzggssRmgSDHoVWfB1OZOjVAB7uaJYCbWnvH-HG2NpolvOj6qHUOLkEJ5YA_cwnKeEIKulJ_h6NhvVaX9yGKM3ACtCZ5eITK80_zhvdz8uOdHfW46XkLnTiZsZzyX4nfyr6pzGMAumdmlv-UNZcYsNI5YipaBImsHcnpCeibg    提交人    /u/Ambitious-Fix-3376   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1n1dn/𝗨𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱𝗶𝗻𝗴_𝗧𝗼𝗸𝗲𝗻𝗶𝘇𝗮𝘁𝗶𝗼𝗻_𝗶𝗻_𝗟𝗮𝗿𝗴𝗲_𝗟𝗮𝗻𝗴𝘂𝗮𝗴𝗲/</guid>
      <pubDate>Thu, 28 Nov 2024 03:43:17 GMT</pubDate>
    </item>
    <item>
      <title>无需 API 调用的 LangGraph</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1ld8j/langgraph_without_api_calls/</link>
      <description><![CDATA[晚上好， 我正在尝试学习使用 LangGraph 创建多智能体项目，该操作基于 LangGraph Quickstart。我想知道如何使用无 API 的系统和 LangGraph。我尝试使用 Hugging Face 模型，并能够成功使用调用命令。但是，当我将模型作为聊天机器人的一部分调用时（在设置开始、聊天机器人和结束节点之后），我得到了通用的 AttributeError：&#39;str&#39; 对象没有属性 &#39;content&#39;。 我想知道这是否与我选择的模型有关。如有必要，我可以提供特定的代码。此外，如果有必要，我非常愿意以其他方式进行操作。非常感谢！    提交人    /u/Need_More_Learn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1ld8j/langgraph_without_api_calls/</guid>
      <pubDate>Thu, 28 Nov 2024 02:12:09 GMT</pubDate>
    </item>
    <item>
      <title>如何更好地推导出关于某些变量的损失函数的简化表达式？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1l7dm/how_to_get_better_at_deriving_simplified/</link>
      <description><![CDATA[在机器学习中，您经常需要得出损失关于某个变量的导数。 是否有地方提供大量导数表达式，让我可以学习和练习如何得出它们的简化表达式？ 谢谢。    提交人    /u/_stracci   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1l7dm/how_to_get_better_at_deriving_simplified/</guid>
      <pubDate>Thu, 28 Nov 2024 02:03:19 GMT</pubDate>
    </item>
    <item>
      <title>需要建议：如何以及在哪里学习 ML 模型部署/将 ML 模型部署到生产中？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1h98a/advice_needed_how_and_where_to_learn_ml_model/</link>
      <description><![CDATA[我正在寻找一些关于如何将机器学习模型部署到生产中的指导/资源。我写这篇文章的原因是，在针对不同用例进行部署时，有太多的服务/工具。 以下是我的一些背景：我在机器学习方面有扎实的基础，并围绕 LLM 构建了多个应用程序，但我从未真正部署过模型。    提交人    /u/Rare_Mud7490   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1h98a/advice_needed_how_and_where_to_learn_ml_model/</guid>
      <pubDate>Wed, 27 Nov 2024 22:47:51 GMT</pubDate>
    </item>
    <item>
      <title>将 CNN 与 DT 相结合</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1b51l/combining_cnns_with_dts/</link>
      <description><![CDATA[因此，我在 AI/ML 课程的期末论文中出现了一个问题。这个问题更像是一个开放式问题，它问的是：如何将 CNN 网络与决策树结合起来？在考试的时候，我突然想到，只需获取卷积基础的平坦层的输出并将其用作决策树的输入特征即可。 我没怎么注意答案。我写下了我想到的第一件事。但现在考试结束后，我想也许这不是一个坏主意。 你们觉得怎么样？以前有人尝试过吗？以前有没有将 CNN 与树结合起来的论文？    提交人    /u/Previous-Scheme-5949   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1b51l/combining_cnns_with_dts/</guid>
      <pubDate>Wed, 27 Nov 2024 18:26:15 GMT</pubDate>
    </item>
    <item>
      <title>理解大型语言模型 (LLM)：全面概述</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h1awif/understanding_large_language_models_llms_a/</link>
      <description><![CDATA[      https://reddit.com/link/1h1awif/video/skvim49gjz2e1/player Lar 当你开始学习大型语言模型 (LLM) 时，你可能会对网上可用的大量内容感到不知所措。为了简化这一过程，我整理了一份 LLM 中关键主题的概述，以帮助你以结构化的方式掌握概念。仅仅听说一项新技术可能不足以完全理解它，但将其分解为易于理解的概念并提供资源可能是加深理解的好方法。 在这篇文章中，我将分享重要的资源和主题供您探索，这将有助于您在 LLM 领域打下坚实的基础。如果某个主题引起了您的兴趣，我鼓励您使用提供的链接深入研究它。每个视频都会引导您了解 LLM 的特定方面，从基础知识到更高级的主题。 以下是入门概述： 1. 大型语言模型 (LLM) 简介 从 LLM 的基础知识开始，了解它们是什么以及它们为什么重要。点击此处观看 2.预训练与微调 LLM 了解预训练和微调之间的区别，这是 LLM 开发中的两个关键步骤。 点击此处观看 3. 什么是 Transformer？ Transformer 是许多现代 LLM 的骨干。了解此架构的工作原理。 点击此处观看 4. GPT-3 究竟是如何工作的？ 深入了解最著名的 LLM 之一 — GPT-3 的内部工作原理。 观看此处 5. 从头开始​​构建 LLM 的阶段 探索从头开始构建 LLM 所涉及的步骤。 观看此处 6. 使用 Python 从 Scratch 编写 LLM 标记器 理解和构建 LLM 标记器的实践指南。 观看此处 7. GPT 标记器：字节对编码 了解标记化中使用的关键技术之一：字节对编码 (BPE)。点击此处观看 8. 什么是标记嵌入？ 了解标记嵌入的概念及其在 LLM 中的作用。点击此处观看 9. 位置嵌入的重要性 探索位置嵌入如何帮助 LLM 理解序列中标记的顺序。 点击此处观看 10. LLM 的数据预处理流程 了解支持 LLM 的复杂数据预处理流程。 点击此处观看 通过浏览这些视频，您将更清楚地了解 LLM 的工作原理以及促成其成功的各种组件。我鼓励您按照最适合您的顺序关注这些资源，并深入研究引起您兴趣的主题。 如果您有任何疑问或需要更多资源，请随时提问！祝您学习愉快    提交人    /u/Ambitious-Fix-3376   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h1awif/understanding_large_language_models_llms_a/</guid>
      <pubDate>Wed, 27 Nov 2024 18:16:26 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么好的网站可以练习机器学习的线性代数、统计学和概率？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h19jqy/any_good_sites_to_practice_linear_algebra/</link>
      <description><![CDATA[大家好！ 我刚刚被一个人工智能硕士项目（授课型）录取，也有点紧张。我目前是一名应用程序开发人员，但我想在开始工作之前为数学方面做好准备。 数学从来都不是我的强项（我的数学一直很一般），看着线性代数的数学让我想起了高中数学，但我确信它比高中数学更复杂。我对即将发生的事情有点紧张，我真的想做好准备，这样当我的项目开始时我就不会不知所措。 我还记得当我试图加入一个机器人人工智能实验室的时候。他们告诉我我只需要“基本运动学”来准备——然后给我布置了机器人手运动学的问题！这真是一个震惊，我不想在开始攻读硕士学位时再经历一次。 我知道他们会在第一学期讲授基础知识，但我真的想提前做好准备。有没有人知道有哪些好的网站或资源可以让我练习线性代数、统计学和机器学习概率？理想情况下，有一些关键的答案或解释，这样我就可以有效地学习而不会感到迷茫。 有人能推荐一些可以帮助我做好准备的网站、工具或策略吗？提前谢谢！🙏    提交人    /u/CogniLord   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h19jqy/any_good_sites_to_practice_linear_algebra/</guid>
      <pubDate>Wed, 27 Nov 2024 17:20:17 GMT</pubDate>
    </item>
    <item>
      <title>谢谢</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h19f8d/thank_you/</link>
      <description><![CDATA[我只想感谢你们对我之前关于我简历的帖子的反馈。 这真是一个警钟。我意识到我作为 ML 从业者的 3 年经验毫无价值。 感谢您有时粗鲁的反馈，我需要它。 我会使用它。 再次感谢您提供如此多有用的回复。    提交人    /u/Bitter-Surprise-7508   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h19f8d/thank_you/</guid>
      <pubDate>Wed, 27 Nov 2024 17:14:59 GMT</pubDate>
    </item>
    <item>
      <title>卷积解释</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h17ok5/convolutions_explained/</link>
      <description><![CDATA[      大家好！ https://preview.redd.it/bm3eqih0xg3e1.jpg?width=640&amp;format=pjpg&amp;auto=webp&amp;s=d3a55c85cf53693611a5c32d8b3f4ceadb5ff734 我拍摄了我的第一个YouTube视频，这是一个关于卷积的教育视频（数学定义、应用手册内核在计算机视觉中的作用，并解释它们在卷积神经网络中的作用）。 需要您的反馈！  是否足够容易理解？ 长度是否适合处理信息？  谢谢！ 我想制作的下一个视频将更实用（例如如何在 Vertex AI 中设置 ML 管道）    提交人    /u/nepherhotep   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h17ok5/convolutions_explained/</guid>
      <pubDate>Wed, 27 Nov 2024 16:02:24 GMT</pubDate>
    </item>
    <item>
      <title>我快要疯了。我向 MLE 职位投递了 200 份简历，但只有 10 次面试。我做错了什么？我应该补充什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h126tg/im_slowly_losing_my_mind_200_resumes_sent_for_mle/</link>
      <description><![CDATA[        提交人    /u/Bitter-Surprise-7508   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h126tg/im_slowly_losing_my_mind_200_resumes_sent_for_mle/</guid>
      <pubDate>Wed, 27 Nov 2024 11:28:31 GMT</pubDate>
    </item>
    <item>
      <title>线性代数项目，我从头开始实现了一个带动画的 K-Means，效果不错？我们需要添加一个停止条件，即使质心几乎没有变化，它也会继续，有没有什么提示可以说明这个条件是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/</link>
      <description><![CDATA[        提交人    /u/Ang3k_TOH   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h0ypc2/linear_algebra_project_i_implemented_a_kmeans/</guid>
      <pubDate>Wed, 27 Nov 2024 07:13:57 GMT</pubDate>
    </item>
    <item>
      <title>有谁完成过 Andrew Ng 的 ML 专业化课程并且目前在 ML 领域工作吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1h0x63n/anyone_whos_done_andrew_ngs_ml_specialization_and/</link>
      <description><![CDATA[对于那些从 Andrew Ng 的 ML 专业课程开始学习 ML 并且现在在 ML 领域工作的人来说，你的道路是什么样的？    提交人    /u/lil_leb0wski   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1h0x63n/anyone_whos_done_andrew_ngs_ml_specialization_and/</guid>
      <pubDate>Wed, 27 Nov 2024 05:34:53 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>