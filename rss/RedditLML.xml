<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Wed, 11 Sep 2024 18:20:57 GMT</lastBuildDate>
    <item>
      <title>寻求帮助：提高数据集不平衡情况下 CNN 模型的皮肤癌检测准确性（作为初学者）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fegqf2/looking_for_help_enhancing_cnn_model_accuracy_for/</link>
      <description><![CDATA[大家好。我使用包含两个文件夹（测试和训练）的数据集创建了一个基本的 CNN 模型，将皮肤癌图像分类为恶性或良性。每个文件夹都有良性和恶性图像的子文件夹。训练集包含 1,440 张良性和 1,197 张恶性图像，因此存在不平衡。我的代码是（由 AI 精炼）： 最初，我得到了一个这样的混淆矩阵：[[192,163],[158,142]]，但模型的性能并不令人满意。应用图像增强后，混淆矩阵变成了 [[0,360],[0,300]]，情况更糟。我尝试了其他几种方法，但都无法提高准确性。 由于我是新手并且正在从事一个项目，有人可以就如何提高模型的准确性提供建议吗？ import tensorflow as tf from tensorflow import keras from keras import Sequential from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Rescaling from sklearn.metrics importfusion_matrix import numpy as np import matplotlib.pyplot as plt import seaborn as sns from keras.preprocessing import image # 加载和预处理数据集 train_ds = keras.utils.image_dataset_from_directory( directory=&quot;/content/train&quot;, labels=&quot;inferred&quot;, label_mode=&quot;binary&quot;, batch_size=32, image_size=(256, 256）） test_ds = keras.utils.image_dataset_from_directory（directory =“/content/test”，labels =“inferred”，label_mode =“binary”，batch_size = 32，image_size =（256,256））def process（image，label）：image = tf.cast（image / 255.0，tf.float32）返回图像，标签def augment（image，label）：image = tf.image.random_flip_left_right（image）image = tf.image.random_flip_up_down（image）image = tf.image.random_rotation（image，0.2）image = tf.image.random_zoom（image，（0.8,1.2））返回图像，标签train_ds = train_ds.map（process）。map（augment） test_ds = test_ds.map(process) # 构建模型 model = Sequential([ Rescaling(1./255, input_shape=(256, 256, 3)), # 标准化图像 Conv2D(32, kernel_size=(3, 3), padding=&#39;valid&#39;, activity=&#39;relu&#39;), MaxPooling2D(pool_size=(2, 2), strides=2, padding=&#39;valid&#39;), Conv2D(64, kernel_size=(3, 3), padding=&#39;valid&#39;, activity=&#39;relu&#39;), MaxPooling2D(pool_size=(2, 2), strides=2, padding=&#39;valid&#39;), Conv2D(128, kernel_size=(3, 3), padding=&#39;valid&#39;, activity=&#39;relu&#39;), MaxPooling2D(pool_size=(2, 2), strides=2, padding=&#39;valid&#39;), Flatten(), Dense(128,activation=&#39;relu&#39;), Dense(64,activation=&#39;relu&#39;), Dense(1,activation=&#39;sigmoid&#39;) ]) # 编译模型 model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;]) # 训练模型 history = model.fit(train_ds,epochs=10,validation_data=test_ds) # 评估模型 test_loss,test_accuracy = model.evaluate(test_ds) print(f&quot;测试损失：{test_loss}&quot;) print(f&quot;测试准确率：{test_accuracy}&quot;) # 为测试数据集生成预测 y_pred = model.predict(test_ds) y_pred_binary = (y_pred &gt; 0.5).astype(int) # 从测试数据集中获取真实标签 y_true = np.concatenate([y for x, y in test_ds], axis=0) # 计算混淆矩阵 cm =fusion_matrix(y_true, y_pred_binary) print(&quot;Confusion Matrix:&quot;) print(cm) # 绘制混淆矩阵 plt.figure(figsize=(5, 5)) sns.heatmap(cm, annot=True, fmt=&#39;d&#39;, cmap=&#39;Blues&#39;, cbar=False) plt.xlabel(&#39;Predicted&#39;) plt.ylabel(&#39;True&#39;) plt.title(&#39;Confusion Matrix&#39;) plt.show() # 绘制训练和验证准确率和损失 plt.figure(figsize=(12, 5)) plt.subplot(1, 2, 1) plt.plot(history.history[&#39;accuracy&#39;], label=&#39;Train Accuracy&#39;) plt.plot(history.history[&#39;val_accuracy&#39;], label=&#39;Validation Accuracy&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.legend() plt.title(&#39;Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(history.history[&#39;loss&#39;], label=&#39;Train Loss&#39;) plt.plot(history.history[&#39;val_loss&#39;], label=&#39;Validation Loss&#39;) plt.xlabel(&#39;Epochs&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() plt.title(&#39;Loss&#39;) plt.show() # 测试单张图片 test_img_path = &#39;/content/test/benign/1191.jpg&#39; test_img = image.load_img(test_img_path, target_size=(256, 256)) test_img = image.img_to_array(test_img) / 255.0 test_input = np.expand_dims(test_img, axis=0) # 进行预测 prediction_prob = model.predict(test_input) if prediction_prob &gt; 0.5: print(&quot;Malignant&quot;) else: print(&quot;Benign&quot;) # 显示测试图像 plt.imshow(test_img) plt.axis(&#39;off&#39;) # 隐藏轴 plt.show()     submitted by    /u/Ekavya_1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fegqf2/looking_for_help_enhancing_cnn_model_accuracy_for/</guid>
      <pubDate>Wed, 11 Sep 2024 18:01:58 GMT</pubDate>
    </item>
    <item>
      <title>用于文本分类任务的大型文档的文本分割</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1feg0m6/text_splitting_of_large_documents_for_the_text/</link>
      <description><![CDATA[我不确定我是否遗漏了某些显而易见的东西，但我想使用微调变压器构建一个文本分类器。我拥有的数据是文本文件，其中有 2 个目标类的人工标记部分，比如说 A 和 B。未标记的文本比如说标签 C。这些标记部分有时只有一个句子，有时有 5 个句子长，我该如何将大型文本文件拆分为数据以进行微调？ 我的第一个想法是将文本拆分为数组，每行是一个句子，目标标签为 A 或 B 或 C。但在那种情况下，我会丢失一些可能定义原因的其他句子的上下文。 我的第二个想法是对文本中的每个单词进行标记分类，然后根据每个句子的平均结果构建部分。 您将如何解决这个问题？    提交人    /u/aljaz0101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1feg0m6/text_splitting_of_large_documents_for_the_text/</guid>
      <pubDate>Wed, 11 Sep 2024 17:32:48 GMT</pubDate>
    </item>
    <item>
      <title>数据融合</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fefj6l/data_fusion/</link>
      <description><![CDATA[我非常紧张，想要找到一个可以解决目前未解答问题的现代项目。有没有什么关于数据融合的新想法的建议？请随时给我发邮件。非常感谢    提交人    /u/Immediate_Rutabaga_3   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fefj6l/data_fusion/</guid>
      <pubDate>Wed, 11 Sep 2024 17:12:56 GMT</pubDate>
    </item>
    <item>
      <title>用于深度学习 (tensorflow) 的 Radeon RX GPU</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fef7aq/radeon_rx_gpu_for_deeplearningtensorflow/</link>
      <description><![CDATA[大家好， 有人用 Radeon RX GPU 通过 tensorflow 进行深度学习吗？我在网上看到的都是用 CUDA 设置 NVIDIA GPU。如果有人能指导我完成设置步骤或分享任何有用的资源，我将不胜感激。 谢谢    提交人    /u/Primary_Paramedic_40   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fef7aq/radeon_rx_gpu_for_deeplearningtensorflow/</guid>
      <pubDate>Wed, 11 Sep 2024 16:59:49 GMT</pubDate>
    </item>
    <item>
      <title>HOG 并未按我预期的方式发挥作用</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fef3bb/hog_doesnt_work_the_way_i_expect/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fef3bb/hog_doesnt_work_the_way_i_expect/</guid>
      <pubDate>Wed, 11 Sep 2024 16:55:20 GMT</pubDate>
    </item>
    <item>
      <title>您能否想出一个神经网络回归器，可以将此图像映射到 1000 个 (x,y) 位置并进行过度拟合？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fee0u9/can_you_come_up_with_a_neural_network_regressor/</link>
      <description><![CDATA[我有一张 360p 图像：https://imgur.com/a/u3uDFj9  我有一个对应的人体蒙版和一个 (1000,2) 输出，即 1000 个二维点：https://file.io/Vc4hA1LYd5NO 我的数据集实际上有大量这些图像和点的配对。我们的想法是让神经网络从输入图像/蒙版中学习点初始化。每个图像对应的所有点都在蒙版内，因此神经网络肯定有要学习的结构。 我最初的问题是所有点都输出在图像的正中央，即平均值。 此 stackoverflow 帖子 讨论了此问题，但那里针对我的问题所述的任何内容均无法帮助到我。  我也尝试过让模型在一张图像上过度拟合，但似乎不起作用。我尝试了不同的损失函数，以及不同的模型输出预处理和后处理。   我主要使用 Resnet50 模型进行此实验，并在最后使用修改后的全连接层  我使用 pytorch 文档中针对 resnet50 所述的相同均值和标准差对输入图像进行了归一化  由于基本事实中的所有点都在 (-1,1) 之间，因此我尝试在输出层使用 tanh 激活。我还尝试使用 Sigmoid，以防 tanh 出现梯度消失问题，并在计算损失之前将该模型输出标准化回 [-1,1] 范围。 我尝试在优化期间冻结和解冻所有 resnet50 层。  这些似乎都不适合我。我想看看是否有人可以让这个在我包含的单个数据点上运行。    提交人    /u/ChaosAdm   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fee0u9/can_you_come_up_with_a_neural_network_regressor/</guid>
      <pubDate>Wed, 11 Sep 2024 16:11:54 GMT</pubDate>
    </item>
    <item>
      <title>你推荐什么书？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1feauam/what_book_do_you_recommend/</link>
      <description><![CDATA[我想买一本书来尽可能好地学习 ML，我心里有两本书，但我不知道选哪一本。  使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习实践：构建智能系统的概念、工具和技术，第 3 版。 使用 PyTorch 和 Scikit-Learn 进行机器学习：使用 Python 第 1 版开发机器学习和深度学习模型。  您推荐其中哪一本？    提交人    /u/Cute_Talk_126   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1feauam/what_book_do_you_recommend/</guid>
      <pubDate>Wed, 11 Sep 2024 14:00:28 GMT</pubDate>
    </item>
    <item>
      <title>大规模多时间序列预测</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1feabo7/largescale_multiple_time_series_forecasting/</link>
      <description><![CDATA[大家好， 我正在做一个个人/学校项目，为美国某个州不同家庭（约 1000 个）的电力消耗时间序列数据集创建日前预测。因此，我有 1000 个时间序列需要预测，并且正在尝试开发能够对所有时间序列做出准确预测的方法。 以下是我到目前为止尝试过的方法：  **移动平均值**：使用过去 7 天的移动平均值预测第二天。 **LightGBM 模型**：从日期时间和原始时间序列中提取日期时间特征（小时、星期几、月份中的某天）和历史特征（滞后 1 天、滞后 1 周、7 天的移动平均值）。然后我拟合了一个默认的 LightGBM 回归器并进行了预测。  但是，就测试集中所有家庭的平均 MAPE 而言，移动平均仍然是最好的模型。通过分析移动平均 MAPE 并可视化一些时间序列，我发现只有 20-30% 非常重复，而大多数波动很大，因此很难预测。我也尝试了 SARIMA，但训练一个模型需要太长时间，更不用说 1000 个模型并对其进行回测了。 我认为一定有一些方法可以击败这种简单的移动平均方法，但我现在陷入困境。所以我正在寻找建议，如何像业界一样以一种良好而标准的方式解决这个问题。 公司通常如何处理这样的大规模预测？他们是对所有时间序列使用单一模型，还是为每个时间序列开发特定模型？如果他们为每个时间序列都有一个模型，那么他们如何在如此大规模下管理所有模型（重新训练、部署、监控等）？ 我尝试过在线搜索并使用 ChatGPT，但没有找到太多关于如何解决这个大规模多时间序列问题的方法。 我对 MLOps 和 MLE 也很感兴趣，所以我正在尝试更深入地研究这个问题并学习如何正确地做到这一点。 任何建议或资源都会非常有帮助！ 谢谢！# 大规模多时间序列预测    提交人    /u/Dry-Shoulder-8574   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1feabo7/largescale_multiple_time_series_forecasting/</guid>
      <pubDate>Wed, 11 Sep 2024 13:37:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 Argilla 和 AutoTrain 对合法数据的标记分类模型进行微调</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fe7sj2/finetuning_a_token_classification_model_for_legal/</link>
      <description><![CDATA[        由    /u/abhi1thakur  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fe7sj2/finetuning_a_token_classification_model_for_legal/</guid>
      <pubDate>Wed, 11 Sep 2024 11:28:58 GMT</pubDate>
    </item>
    <item>
      <title>RAG 管道的最佳实践</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fe6pbl/best_practices_for_rag_pipeline/</link>
      <description><![CDATA[      ☕️ Coffee Break Concepts&#39; Vol.11 ——RAG 管道的最佳实践 在过去的几年中，RAG 已经成熟，并且已经进行了多项研究来了解可以降低成本且提高准确性的模式和行为。其中一项研究是《寻找检索增强生成的最佳实践》，已发表为一篇论文。了解 RAG 组件以及哪些有效、哪些无效？ 本文档深入探讨：  典型的 RAG 工作流程（所有模块） 查询分类 分块 嵌入模型 矢量数据库 检索 重新排名 重新打包 摘要 生成器微调 寻找最佳 RAG 实践 摘要     提交人    /u/masteringllm   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fe6pbl/best_practices_for_rag_pipeline/</guid>
      <pubDate>Wed, 11 Sep 2024 10:20:06 GMT</pubDate>
    </item>
    <item>
      <title>简化机器学习：用日常类比解释 10 种算法</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fe44du/simplifying_machine_learning_10_algorithms/</link>
      <description><![CDATA[       由    /u/emelian1917  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fe44du/simplifying_machine_learning_10_algorithms/</guid>
      <pubDate>Wed, 11 Sep 2024 07:07:00 GMT</pubDate>
    </item>
    <item>
      <title>ML 职业有趣吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fdxe6i/is_ml_career_fun/</link>
      <description><![CDATA[我正在用 ML 做我的论文，但一直很挣扎。我知道 CERN 的科学家做了很多理论，但他们几乎不知道足够的代码来运行他们的实验并分析他们的结果。我觉得我现在做的就是这个样子。 我有一个数据集，我正在努力提高 f1score。如果我找不到改进它的方法，我会回去阅读模型，思考数据和提取什么特征。我觉得我做的 90% 是理论，10% 是实践，实践只是对我的理论的案例测试。我觉得自己更像一个科学家，而不是软件开发人员 如果我的工作基于事实，我会感到很开心。如果我正在开发 VR 耳机，或者我正在研究一种制作耳机的方法，就像在刀剑神域中一样，我们最终可以将感觉发送到大脑，这样在 VR 中我们就可以感受到周围的环境。好吧，我很激动。而且从现实的角度来说，所有这些 ML 在现实技术中的应用都很酷。例如人脸识别、手势控制电脑、视频生成、深度伪造等等。我很激动，我想创造一些这样的东西，因为我是一个基于项目的人 但是我的论文水平很低。我看到的只是 f1 分数上下波动。继续阅读模型文档等等。 所以我不知道我是否想走这条职业道路 对于这个领域的专家。你每天的工作是什么？更实际的是为消费者创造一些最终产品，还是更低水平的、理论水平的，比如我的论文，你试图改进一些结果？    提交人    /u/mosenco   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fdxe6i/is_ml_career_fun/</guid>
      <pubDate>Wed, 11 Sep 2024 00:32:15 GMT</pubDate>
    </item>
    <item>
      <title>构建了一个棋子检测器，以便在 VR 头盔中渲染最佳走法的叠加层</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fdtrd3/built_a_chess_piece_detector_in_order_to_render/</link>
      <description><![CDATA[  由    /u/AreaInternational565  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fdtrd3/built_a_chess_piece_detector_in_order_to_render/</guid>
      <pubDate>Tue, 10 Sep 2024 21:42:19 GMT</pubDate>
    </item>
    <item>
      <title>Reddit-Nemesis：自动进行愤怒诱饵的 AI Reddit 机器人。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1fdt671/redditnemesis_ai_reddit_bot_that_automatizes/</link>
      <description><![CDATA[还有什么比在互联网上争论更人性化吗？还有什么比激烈的在线辩论更好？没错，就是自动化的激烈在线辩论。这就是 Reddit-Nemesis 发挥作用的地方。我一直在研究这个新的人工智能项目，我想与大家分享。这是一个人工智能机器人，它可以抓取 Reddit 并反对它发现的任何观点。它仍在进行中，但我很想听听你的想法，并得到任何反馈或改进建议。看看这里 :) 编辑：只是提醒一下，对项目的贡献是免费的，欢迎 :)    提交人    /u/NotPepus   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1fdt671/redditnemesis_ai_reddit_bot_that_automatizes/</guid>
      <pubDate>Tue, 10 Sep 2024 21:17:01 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>