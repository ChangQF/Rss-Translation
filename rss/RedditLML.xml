<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Mon, 30 Dec 2024 06:24:36 GMT</lastBuildDate>
    <item>
      <title>机器学习的入门文本？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hpf1tv/introductory_texts_for_ml/</link>
      <description><![CDATA[我开始学习机器学习，正在寻找一些好的理论读物。我已经读过《共产党宣言》，但我不确定列宁的哪部作品是一个很好的起点—— 等等，这不是马克思列宁主义的 subreddit    由   提交  /u/qscgy_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hpf1tv/introductory_texts_for_ml/</guid>
      <pubDate>Mon, 30 Dec 2024 04:31:38 GMT</pubDate>
    </item>
    <item>
      <title>𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴 𝗡𝗼𝗺𝗶𝗻𝗮𝗹 𝗖𝗮𝘁𝗲𝗴𝗼𝗿𝗶𝗰𝗮𝗹 𝗗𝗮𝘁𝗮 𝗶𝗻 𝗠𝗮𝗰𝗵𝗶𝗻𝗲 𝗟𝗲𝗮𝗿𝗻𝗶𝗻𝗴</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hpevf7/𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴_𝗡𝗼𝗺𝗶𝗻𝗮𝗹_𝗖𝗮𝘁𝗲𝗴𝗼𝗿𝗶𝗰𝗮𝗹_𝗗𝗮𝘁𝗮_𝗶𝗻_𝗠𝗮𝗰𝗵𝗶𝗻𝗲/</link>
      <description><![CDATA[      独热编码 将分类数据编码为数值格式是大多数机器学习算法的关键预处理步骤。由于许多模型都需要数字输入，因此编码技术的选择会显著影响性能。精心选择的编码策略可提高准确性，而非最优方法则会导致信息丢失和模型性能下降。 独热编码是一种处理分类变量的常用技术。它将每个类别转换为单独的列，并在相应类别所在的位置分配 1 的值。但是，独热编码可能会引入 𝗺𝘂𝗹𝘁𝗶𝗰𝗼𝗹𝗹𝗶𝗻𝗲𝗮𝗿𝗶𝘁𝘆，其中一个类别可以根据其他类别进行预测，违反了独立变量中不存在多重共线性的假设（尤其是在线性回归中）。这就是所谓的𝗱𝘂𝗺𝗺𝘆 𝘃𝗮𝗿𝗶𝗮𝗯𝗹𝗲 𝘁𝗿𝗮𝗽。 𝗛𝗼𝘄 𝘁𝗼 𝗔𝘃𝗼𝗶𝗱 𝘁𝗵𝗲 𝗗𝘂𝗺𝗺𝘆 𝗩𝗮𝗿𝗶𝗮𝗯𝗹𝗲 𝗧𝗿𝗮𝗽？ 👉 简单来说𝗱𝗿𝗼𝗽 𝗼𝗻𝗲 𝗮𝗿𝗯𝗶𝘁𝗿𝗮𝗿𝘆 𝗳𝗲𝗮𝘁𝘂𝗿𝗲 来自独热编码类别的数据。 这通过打破特征之间的线性依赖性消除了多重共线性，确保模型遵循基本假设并实现最佳性能。 𝗪𝗵𝗲𝗻 𝗦𝗵𝗼𝘂𝗹𝗱 𝗬𝗼𝘂 𝗨𝘀𝗲 𝗢𝗻𝗲-𝗛𝗼𝘁 𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴？ ✅ 𝗨𝘀𝗲 𝗶𝘁 𝗳𝗼𝗿 𝗻𝗼𝗺𝗶𝗻𝗮𝗹 𝗱𝗮𝘁𝗮（没有固有顺序的类别）。 ❌ 𝗔𝘃𝗼𝗶𝗱 𝗶𝘁 𝘄𝗵𝗲𝗻 𝘁𝗵𝗲 𝗻𝘂𝗺𝗯𝗲𝗿 𝗼𝗳非常昂贵，因为它可能导致具有过多列的稀疏数据。这会降低模型性能并导致过度拟合，尤其是在数据有限的情况下——这一挑战通常被称为“瓶颈”。 📰 结论，关于我们的 Tigran Hamasyan 博士：https://www.vizuaranewsletter.com?r=502twn 📹 𝗗𝗶𝘃𝗲 𝗱𝗲𝗲𝗽：简化分类数据编码 | Ohe-Hot 编码 | 标签编码 | 目标编码。 |https://youtu.be/IOtsuDz1Fb4?si=XXt62mCLN3tNGpul&amp;t=385 by Pritam Kudale 了解何时以及如何使用独热编码对于设计稳健高效的机器学习模型至关重要。明智选择以获得更好的结果！ 💡 #MachineLearning #DataScience #EncodingTechniques #OneHotEncoding #DummyVariableTrap #CurseOfDimensionality #AI    提交人    /u/Ambitious-Fix-3376   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hpevf7/𝗘𝗻𝗰𝗼𝗱𝗶𝗻𝗴_𝗡𝗼𝗺𝗶𝗻𝗮𝗹_𝗖𝗮𝘁𝗲𝗴𝗼𝗿𝗶𝗰𝗮𝗹_𝗗𝗮𝘁𝗮_𝗶𝗻_𝗠𝗮𝗰𝗵𝗶𝗻𝗲/</guid>
      <pubDate>Mon, 30 Dec 2024 04:21:42 GMT</pubDate>
    </item>
    <item>
      <title>我制作了 ComiQ，一种用于处理疯狂文本的混合 OCR 方法</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hpd7z3/i_made_comiq_a_hybrid_ocr_approach_for_crazy_texts/</link>
      <description><![CDATA[      OCR 程序无法识别高度风格化或手写单词，但提供良好的边界框。 MLLM（Gemini-flash-1.5）是一款出色的文本识别器，但缺乏为文本提供正确的边界框 这个项目，ComiQ，旨在通过创建一个混合系统来解决具有正确边界框位置的高度风格化文本的识别问题，该系统将发挥 MLLM 和 OCR 程序的优势。 GitHub 存储库：Comiq    提交人    /u/StoneSteel_1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hpd7z3/i_made_comiq_a_hybrid_ocr_approach_for_crazy_texts/</guid>
      <pubDate>Mon, 30 Dec 2024 02:53:45 GMT</pubDate>
    </item>
    <item>
      <title>还有其他新手想一起在 GitHub 上合作吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hpcm6n/any_other_newbies_want_to_collaborate_on_github/</link>
      <description><![CDATA[我在 GitHub 上的名字是 JPIPPEN89。我现在是大二的后半段，我开始学习 ML 中比较简单的算法，因为我想在完成硕士学位时精通这些算法。我对 Pandas 和可视化的掌握正在提高，但我还有很多东西要学。 任何想要合作和相互学习的人，请添加我！！ 谢谢    提交人    /u/Better-Subject-2468   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hpcm6n/any_other_newbies_want_to_collaborate_on_github/</guid>
      <pubDate>Mon, 30 Dec 2024 02:22:06 GMT</pubDate>
    </item>
    <item>
      <title>字节潜伏变压器（BLT）深度探究：🥪 你好字节，再见代币👋🤖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hpc5au/byte_latent_transformer_blt_deep_dive_hello_bytes/</link>
      <description><![CDATA[    /u/meowkittykitty510   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hpc5au/byte_latent_transformer_blt_deep_dive_hello_bytes/</guid>
      <pubDate>Mon, 30 Dec 2024 01:58:42 GMT</pubDate>
    </item>
    <item>
      <title>PyReason 入门教程：宠物商店示例</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp95m0/intro_pyreason_tutorial_pet_store_example/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp95m0/intro_pyreason_tutorial_pet_store_example/</guid>
      <pubDate>Sun, 29 Dec 2024 23:34:12 GMT</pubDate>
    </item>
    <item>
      <title>机器学习课程结束后我遇到的第一个回归问题和疑问</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp8cbf/issues_in_my_first_regression_problem_after_ml/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp8cbf/issues_in_my_first_regression_problem_after_ml/</guid>
      <pubDate>Sun, 29 Dec 2024 22:56:50 GMT</pubDate>
    </item>
    <item>
      <title>为什么 L1 正则化会促使系数缩小至零？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp674d/why_does_l1_regularization_encourage_coefficients/</link>
      <description><![CDATA[  由    /u/madiyar  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp674d/why_does_l1_regularization_encourage_coefficients/</guid>
      <pubDate>Sun, 29 Dec 2024 21:21:40 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个用于使用和创建模仿学习数据集的包</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp2jek/ive_created_a_package_for_using_and_creating/</link>
      <description><![CDATA[      嘿，我想你们中的一些人可能会喜欢这个个人项目！ 我的项目做什么： 我已经从事模仿学习一段时间了，一直困扰我的是它有多难它是找到好的专家权重以及运行基线需要多长时间，因为每个工作都使用他们的数据集。因此，我创建了这个项目，努力让研究人员更容易使用来自 HuggingFace 的专家创建数据集并共享他们的数据。它是轻量级的，我正在（慢慢地）发布不同模仿学习方法的基准。目前，我们有 MuJoCo 和经典控制数据集，我正在用多种方法测试它们以确保它们能正常工作。数据集有 1,000 集长，我正在考虑把它们做得更大。 目标受众： 使用模仿学习或任何基于代理的学习进行研究的人需要数据。 比较： 我认为没有其他项目试图让数据易于访问。如果有，我很想知道它们。 https://preview.redd.it/2yhfp9lh1u9e1.png?width=1773&amp;format=png&amp;auto=webp&amp;s=3ae51ad0567d8c12fd968b03494e68e6ec3ebd09 存储库： https://github.com/NathanGavenski/IL-Datasets    由   提交  /u/NightmareOx   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp2jek/ive_created_a_package_for_using_and_creating/</guid>
      <pubDate>Sun, 29 Dec 2024 18:39:58 GMT</pubDate>
    </item>
    <item>
      <title>我应该学习多少机器学习方面的统计学知识？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp20pb/how_much_of_statistics_should_i_learn_for_ml/</link>
      <description><![CDATA[我是一名自学者，最近一直在研究机器学习算法。我只阅读了学习机器学习算法时需要应用的统计学概念。我觉得有必要以结构化的方式学习统计学，但我不想陷入教程地狱。你们能列出必要的主题吗？我一直在参考 ISLP，但我对某些主题不熟悉，例如假设检验。他们在书中对此进行了简要解释，但我应该深入研究这些主题还是书中给出的理论就足够了？    提交人    /u/Critical-Mix-1116   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp20pb/how_much_of_statistics_should_i_learn_for_ml/</guid>
      <pubDate>Sun, 29 Dec 2024 18:17:05 GMT</pubDate>
    </item>
    <item>
      <title>Noema：简化 Python 代理创建！</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp0nk1/noema_simplify_python_agent_creation/</link>
      <description><![CDATA[        提交人    /u/Super_Dependent_2978   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp0nk1/noema_simplify_python_agent_creation/</guid>
      <pubDate>Sun, 29 Dec 2024 17:17:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么是 ml？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hp0k50/why_ml/</link>
      <description><![CDATA[我看到很多帖子说，没有任何定量背景的人试图学习机器学习，他们相信自己能找到工作。你为什么要这样做？机器学习是对数学要求最高的领域之一。以下是一些示例主题：我不懂编码，可以学习机器学习吗？我讨厌数学，可以学习机器学习吗？本子版块 90% 的帖子都是这类主题。如果你数学不好，就去找另一份工作吧。你无法通过观看 YouTube 视频或 Coursera 上的一些随机课程来打败 ChatGPT。你想真正擅长机器学习吗？去获得应用数学、机器学习等方面的硕士学位。 编辑：看完评论后，天哪……我不敢相信很多人甚至不知道梯度下降是什么。另外，你为什么认为它是守门人？好吧，那我想当一名医生，但是我讨厌生物学，而且我记性不好，哦，我也不想上医学院。 编辑 2：我看到很多人说入门级微积分足以学习机器学习。我认为这还不够。一些非常基本的例子：如果不学习线性代数，你怎么学习 PCA？如果不学习对偶，你怎么能理解 SVM？如果你不知道如何计算梯度，你怎么能学习优化算法？如果你不了解优化，你怎么能学习神经网络？或者，你不会学习这些，而是​​通过从 Coursera 获得证书假装你知道机器学习。哈哈。你没有学到任何关于机器学习的东西。你只是学会了使用一些库，但你对黑匣子里发生了什么一无所知。    提交人    /u/Formal_Ad_9415   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hp0k50/why_ml/</guid>
      <pubDate>Sun, 29 Dec 2024 17:13:48 GMT</pubDate>
    </item>
    <item>
      <title>如何沉迷于机器学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hou8ku/how_to_get_addicted_to_machine_learning/</link>
      <description><![CDATA[        提交人    /u/kingabzpro   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hou8ku/how_to_get_addicted_to_machine_learning/</guid>
      <pubDate>Sun, 29 Dec 2024 11:32:06 GMT</pubDate>
    </item>
    <item>
      <title>有哪些好的 Youtube 频道发布相对频繁、质量较高的机器学习视频（类似于 3B1B）？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hoftwn/what_are_good_youtube_channels_that_post/</link>
      <description><![CDATA[不一定是讲座视频，而是解决机器学习中的概念的视频，这些概念非常准确且解释得很好。 我在考虑类似 3Blue1Brown 这样的频道，这些频道在为试图理解这些主题基础知识的人们澄清问题方面非常出色，但我想知道是否还有其他人们认为质量不错的频道。 感谢您的任何建议。    提交人    /u/DontSayIMean   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hoftwn/what_are_good_youtube_channels_that_post/</guid>
      <pubDate>Sat, 28 Dec 2024 21:29:58 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>