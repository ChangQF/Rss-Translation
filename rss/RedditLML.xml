<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>一个致力于学习机器学习的 subreddit</description>
    <lastBuildDate>Mon, 09 Dec 2024 18:25:19 GMT</lastBuildDate>
    <item>
      <title>#机器学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hagccw/machinelearning/</link>
      <description><![CDATA[有人可以指导我如何学习机器学习以成为一名量化工程师吗？    提交人    /u/dabloo_1598   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hagccw/machinelearning/</guid>
      <pubDate>Mon, 09 Dec 2024 18:19:15 GMT</pubDate>
    </item>
    <item>
      <title>为机器学习任务构建的 PC</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hafhwg/pc_build_for_ml_tasks/</link>
      <description><![CDATA[我处理时间序列数据。我主要执行以下计算密集型代码：- - 扩展卡尔曼滤波器 - Pytorch CNN 模型 - 元参数调整 - 使用 sklearn 经典模型进行快速原型设计  我想我会在不久的将来从事语音处理和 LLM 工作，并且希望为我的目的构建一个具有成本效益的设备。 请浏览我的构建并提出一些更改，因为这是我第一次构建 PC，我想从慢开始。    提交人    /u/Ok-Mistake-9308   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hafhwg/pc_build_for_ml_tasks/</guid>
      <pubDate>Mon, 09 Dec 2024 17:45:01 GMT</pubDate>
    </item>
    <item>
      <title>训练自定义 NER 模型的最佳方法</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1haf04l/best_approach_to_train_custom_ner_model/</link>
      <description><![CDATA[我正在为一个自定义工作而努力使用 spacy 的 NER，我很好奇我可以采取什么方法来获得更适合我的特定用例的模型，而不是通用的 NER 用例。  作为起点，我有一个包含几千个实体名称示例的数据库，还有几百个文本示例，这些示例通常只包含其中一个。这项任务的最终目标是过滤掉我感兴趣的单个实体。 到目前为止，我一直在尝试基本的字符串匹配技术来建立链接，但它最终变成了一个缓慢的循环过程，似乎不太稳定，所以我想知道还有什么替代方法可以尝试。  欢迎任何意见。     提交人    /u/RDA92   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1haf04l/best_approach_to_train_custom_ner_model/</guid>
      <pubDate>Mon, 09 Dec 2024 17:24:54 GMT</pubDate>
    </item>
    <item>
      <title>如果我对一个准确度较低的二元分类器进行逆运算会怎么样？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1haehrx/what_if_i_inverse_a_binary_classifier_with_low/</link>
      <description><![CDATA[抱歉，如果这只是愚蠢的，假设一个准确率较低（30%）的二元分类器，如果我反转所有结果，我的准确率会是 70%？    提交人    /u/Eshan2703   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1haehrx/what_if_i_inverse_a_binary_classifier_with_low/</guid>
      <pubDate>Mon, 09 Dec 2024 17:04:06 GMT</pubDate>
    </item>
    <item>
      <title>AI 特工后台模拟器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hae7u2/ai_agent_backroom_simulator/</link>
      <description><![CDATA[制作了一个简单的 llm 后台模拟器。为 AI 代理赋予名称和个性，然后看着他们迷失在彼此的交谈中。 这很有趣。你可以设置随机两个人之间的说唱比赛，让甘道夫和终结者辩论生命的意义。等等。在你的角色细节中描述一下。给出一些关于你希望它如何响应的示例消息等。给出非常严格的做与不做的事情。目前它带上你自己的钥匙。 查看：https://simulator.rnikhil.com/    提交人    /u/Excellent-Effect237   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hae7u2/ai_agent_backroom_simulator/</guid>
      <pubDate>Mon, 09 Dec 2024 16:53:12 GMT</pubDate>
    </item>
    <item>
      <title>Pytorch Lightning：总步数、GPU 数量和批次大小之间的关系</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hae2en/pytorch_lightning_relationship_between_total/</link>
      <description><![CDATA[因此，我最近使用以下设置训练了一个模型： GPU：1 总步数：100,000 批次大小：16 策略：DDP 现在，我想使用不同的超参数训练几个其他模型。但是，我现在可以使用 2x GPU（相同类型）。但我想在这里仔细检查我的理解。使用 DDP 作为策略，如果我只是保持其他参数不变，是否能有效地产生 200,00 步？例如它将以每个批次大小 16 的速度在 100,000 步上训练 2x GPU？ 那么接下来的问题是，如果是这样，如果我将 n 减半，我是否可以有效地复制训练运行？但是，这是否可以确保大致相同百分比的训练数据也被覆盖？以前，100k 步基本上只产生了一个 epoch。    提交人    /u/leoholt   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hae2en/pytorch_lightning_relationship_between_total/</guid>
      <pubDate>Mon, 09 Dec 2024 16:46:48 GMT</pubDate>
    </item>
    <item>
      <title>过采样到底有多好？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1hadear/how_good_is_oversampling_really/</link>
      <description><![CDATA[大家好， 我正在开展一个机器学习项目，我们试图预测抑郁症，但我们的数据集存在很大的不平衡——一大群健康患者和一小群抑郁症患者。我的同事建议使用 SMOTE 等过采样方法来“平衡”数据。 问题是——我们俩都没有过采样方面的扎实背景，老实说，我对此持怀疑态度。生成人工样本如何改善训练过程？我知道它可以帮助模型在训练期间“看到”更多不同的样本，但在对真实数据进行验证和测试时，我并不信服。我们不是只是在欺骗模型，让它认为数据分布与实际情况不同吗？ 我有几个具体的问题： 1.过采样（尤其是 SMOTE）真的有助于提高模型性能吗？7   我如何选择正确的过采样“量”？比如，我是否应该将抑郁症患者的数量增加一倍，还是应该将健康患者和抑郁症患者的比例定为 1:1？  我担心使用过多的人工数据会破坏模型的通用性。提前致谢！🙏    提交人    /u/Standing_Appa8   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1hadear/how_good_is_oversampling_really/</guid>
      <pubDate>Mon, 09 Dec 2024 16:18:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 LangChain、LangGraph、Gemini 和 MongoDB 开发内存感知聊天机器人。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1had1rd/developing_memory_aware_chatbots_with_langchain/</link>
      <description><![CDATA[      在本分步指南中，您将了解：  如何使用 LangChain、Gemini 创建聊天机器人。 使用 LangGraph 和 MongoDB 处理聊天记录。     提交人    /u/CC-KEH   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1had1rd/developing_memory_aware_chatbots_with_langchain/</guid>
      <pubDate>Mon, 09 Dec 2024 16:03:44 GMT</pubDate>
    </item>
    <item>
      <title>在代码的可读性和效率之间取得平衡（换句话说，是否要矢量化？）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1haci8n/balancing_between_readability_and_efficiency_of/</link>
      <description><![CDATA[当我们在代码中编写算法时，对于每一行代码，我们都会做出影响代码可读性和效率的决定。通常，当我们使一行代码更高效时，我们会使其可读性降低。例如，假设我们要计算向量矩阵（又称张量）中每个向量与其自身的外积。我们可以使用嵌套的 for 循环，选择每个向量，然后简单地使用 np.outer(v, v)，这非常易读，因为我们了解矩阵中每个条目是如何处理的。但是，以下代码效率更高，因为它是向量化的（术语来自数组编程）。 outer_products = np.einsum(&#39;nik, nij -&gt; nkj&#39;, tensor_A, tensor_A) 如果您不熟悉 np.einsum()，它被称为“爱因斯坦求和约定”。它基本上是说“选择第 (n,i) 个向量并执行外积”，并对“tensor_A”的每个条目执行此操作。  这是我对更有经验的科学程序员的问题 - “当不仅仅是一行代码变得更高效时（有许多行代码可以通过这种方式矢量化），哪种方法（方法是（1）可读性（2）效率）更好？“    提交人    /u/reacher1000   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1haci8n/balancing_between_readability_and_efficiency_of/</guid>
      <pubDate>Mon, 09 Dec 2024 15:40:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么在某些论文中位置/法线嵌入只是可学习的参数？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1habnwz/why_are_positionalnormal_embeddings_just/</link>
      <description><![CDATA[我知道原始 Transformer 架构，它们将位置嵌入和正常嵌入（文本）传递给 Transformer。但是现在处理一些论文（非文本）时，我发现这些嵌入只是可学习的参数。具体来说，正常嵌入的线性层（如果我们需要将某些东西投影到更高维度，这是有意义的）和位置嵌入初始化为 nn.Parameter(torch.zeros())。我不明白为什么我们不将位置嵌入传递给模型，并将它们保留为可学习的参数。模型将如何理解位置？ 我无法理解这个概念。我知道这取决于数据和手头的任务。但我想让这个问题保持一般性。模型将如何学习这种嵌入，我们不是应该从我们的角度告诉模型吗？ PS：我正在处理非文本数据。但对于文本数据来说，这也许也是正确的。我对此并不了解。    提交者    /u/DramaticCloud1498   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1habnwz/why_are_positionalnormal_embeddings_just/</guid>
      <pubDate>Mon, 09 Dec 2024 15:02:30 GMT</pubDate>
    </item>
    <item>
      <title>新基准评估法学硕士的大学数学水平</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1haatmg/new_benchmark_assessing_llms_on_university_level/</link>
      <description><![CDATA[      Toloka 团队推出了两个新的基准 U-MATH 和 μ-MATH，用于测试 LLM 大学水平的数学。这是市场上唯一具有这种规模和复杂性的基准，它包括视觉输入问题。检查HuggingFace 以获取数据集来测试你的模型。 https://preview.redd.it/3ac6fn4b3u5e1.png?width=2400&amp;format=png&amp;auto=webp&amp;s=933fa452dc5ae7d71fa86037d7a3cee6e21f1617    由   提交  /u/Alla-il   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1haatmg/new_benchmark_assessing_llms_on_university_level/</guid>
      <pubDate>Mon, 09 Dec 2024 14:23:48 GMT</pubDate>
    </item>
    <item>
      <title>开始我的 ML 学习，因此寻找同行......</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ha85l9/starting_my_ml_learning_so_looking_for_peers/</link>
      <description><![CDATA[我即将开始我的机器学习之旅，并希望与同行建立联系。如果您也在同一条轨道上，那么让我们携手并进，打破障碍。如果感兴趣，请直接留言……    由   提交  /u/redxtremee   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ha85l9/starting_my_ml_learning_so_looking_for_peers/</guid>
      <pubDate>Mon, 09 Dec 2024 12:01:05 GMT</pubDate>
    </item>
    <item>
      <title>我构建了一个由 RAG 提供支持的 AI 工具搜索引擎（免费）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ha5mmv/i_built_a_ragpowered_search_engine_for_ai_tools/</link>
      <description><![CDATA[        由    /u/dhj9817 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ha5mmv/i_built_a_ragpowered_search_engine_for_ai_tools/</guid>
      <pubDate>Mon, 09 Dec 2024 08:55:59 GMT</pubDate>
    </item>
    <item>
      <title>最佳变形金刚速成课程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1ha21wi/best_transformers_crash_course/</link>
      <description><![CDATA[通过学校和一些工作相关的项目，在 ML 和基本神经网络 (MLP 和 RNN) 方面具有一些背景知识和实践经验。我正在寻找一个关于 Transformers 的优秀速成课程，以便快速开始我的学习和使用。 我的目标是能够构建和使用 Transformers 来完成各种 NLP、生成式 AI 和代理式 AI 任务。因此，主要是利用和微调现有模型，但要有足够深入的理解来完成更复杂的任务。 这是最终目标，并不期望在一门课程中实现这一目标，但以一个良好的 2-3 周选项作为开端，在数学和概念理解与实践编码经验之间取得平衡将是理想的。 Udemy、Coursera、Udacity、datacamp 和每个人的妈妈现在都有课程，所以我试图确定在上述限制条件下最好的选择是什么。    提交人    /u/SizePunch   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1ha21wi/best_transformers_crash_course/</guid>
      <pubDate>Mon, 09 Dec 2024 04:50:55 GMT</pubDate>
    </item>
    <item>
      <title>机器学习相关的简历审查帖</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</link>
      <description><![CDATA[请礼貌地将任何关于简历审查的帖子重定向到这里 对于那些正在寻找简历审查的人，请先在 imgur.com 上发布它们，然后将链接作为评论发布，或者甚至先在 /r/resumes 或 r/EngineeringResumes 上发布，然后在此处交叉发布。     提交人    /u/techrat_reddit   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/1d8odje/machinelearningrelated_resume_review_post/</guid>
      <pubDate>Wed, 05 Jun 2024 12:11:43 GMT</pubDate>
    </item>
    </channel>
</rss>