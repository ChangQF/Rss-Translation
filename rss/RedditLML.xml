<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Sat, 06 Jan 2024 00:59:36 GMT</lastBuildDate>
    <item>
      <title>机器学习训练营</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zmmuz/machine_learning_bootcamp/</link>
      <description><![CDATA[关于训练营或一些按您的节奏付费学习的培训的任何建议，我可以利用它们来开始我在加利福尼亚州的机器学习之旅？我是 python 的新手，正在学习它，有 .Net 背景，一直担任数据工程师，所以只是想把学习机器学习作为一种兴趣，而不是为了一份工作。 如果有人接受了这个，有任何反馈吗？ https://pg- p.ctme.caltech.edu/ai-machine-learning-bootcamp-online-certification-course   由   提交/u/No_Cranberry_8107   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zmmuz/machine_learning_bootcamp/</guid>
      <pubDate>Sat, 06 Jan 2024 00:37:46 GMT</pubDate>
    </item>
    <item>
      <title>情感分析</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zk7sl/sentiment_analysis/</link>
      <description><![CDATA[大家好，我刚刚开始学习 ML，我想为我的简历制作一个项目，可以接收一些文本数据和进行情绪分析。但更重要的是，我知道有很多库可以为你做到这一点，但我想从头开始。谁能给我一些关于我具体需要学习什么的见解。到目前为止，我刚刚参加斯坦福机器学习课程。   由   提交 /u/Miserable-Try8393    reddit.com/r/learnmachinelearning/comments/18zk7sl/sentiment_analysis/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zk7sl/sentiment_analysis/</guid>
      <pubDate>Fri, 05 Jan 2024 22:55:48 GMT</pubDate>
    </item>
    <item>
      <title>需要 SBert 领域适应方面的帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zieai/need_help_with_domain_adaption_for_sbert/</link>
      <description><![CDATA[我的下游任务涉及高度特定领域文档中的信息检索或语义相似性。目标是识别每个文档的 n 个最相关/相似的文档。 到目前为止，我已经采用 Word2Vec 和词袋方法来生成文档向量。然后使用 Annoy 对这些向量进行索引，并与欧几里得距离进行比较。 我有几百个文档的验证数据和一个指示每个文档的最近邻居的列表。  W2V，我的 F1 分数约为 0.33。 我为我的任务找到的最佳 SBert 模型是 intfloat/multilingual-e5-base，它开箱即用时的性能为 0.34，精细后提高到 0.38 - 调整（只有 600 个标记文档对）。 微调的挑战在于创建标记数据的稀缺性和难度。然而，我确实有大约 250 万个未标记的特定领域句子（源自 700,000 个文档）。 因此，我探索了 TSDAE 作为 intfloat/multilingual-e5-base 模型之上的额外预训练。我尝试了各种参数和学习率（在 4090 上计算长达 6 天），但最终该模型在我的验证数据上的表现要差得多。我认为这是由于缺乏继续培训的检查点造成的。下面是我用于预训练的代码： pre_model = &#39;intfloat/multilingual-e5-base&#39; word_embedding_model = models.Transformer(pre_model) pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension() ，&#39;cls&#39;）模型= sendencetransformer（模块= [Word_embedding_model，boming_model]））train_dataset = datasets.denoisingautoencoderdataset（train_sentences） aloader = dataloader（train_dataset，batch_size = pre_batch，shuffle =True）对于范围内的历元（pre_epochs）：model.fit（train_objectives = [（train_dataloader，train_loss）]，epochs = 1，weight_decay = 0，scheduler =&#39;consantlr&#39;，optimizer_params = {&#39;lr&#39;：pre_lr}，show_progress_bar =True ) epoch_losses = train_loss.epoch_lossesaverage_epoch_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else 0.0streamlit_logger((f&quot;Epoche {epoch + 1}: {average_epoch_loss}&quot;)) model.save(pretrain_save_path)  我收到以下警告，然后是所有层的大列表，我猜这些层已被“丢弃”？  XLMRobertaForCausalLM 的某些权重不是从 intfloat/multilingual-e5-base 的模型检查点初始化并重新初始化：   我的目标是尽可能获得最有意义/相关的嵌入来执行越来越多的下游任务跟他们。由于我有很多未标记的域数据，我想以某种方式使用它来制作一个可靠的基础模型。  有人在 SBert 上使用未标记数据进行域适应方面取得了一些成功吗？   由   提交 /u/CaptainSnackbar   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zieai/need_help_with_domain_adaption_for_sbert/</guid>
      <pubDate>Fri, 05 Jan 2024 21:39:45 GMT</pubDate>
    </item>
    <item>
      <title>高质量的开源Python文本转语音模型我们可以在本地下载和使用吗？或者免费的API？抱歉，这篇文章已被 r/learn 的版主删除</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zhwaw/good_quality_open_source_python_text_to_speech/</link>
      <description><![CDATA[我需要将大约20万个字符转录成语音。 大家推荐elevenlabs.io 我测试了他们的 API，效果很好，但他们的订阅模式是骗人的。 20 万个字符的价格为 40 美元。而在 ChatGPT 中，这需要大约 4 美元才能生成。 我开玩笑地可能会以这个价格雇人阅读，或者自己做。但这不是本次练习的重点 我想要一个本地模型，它将通过 ML 进行高质量的文本到语音转换。 如果此类模型不可用，或者如果它们采用占用太多空间，我不介意在线的，只要它不哄抬物价。 为此使用的最佳库是什么？   由   提交/u/Sharp-Cat2319  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zhwaw/good_quality_open_source_python_text_to_speech/</guid>
      <pubDate>Fri, 05 Jan 2024 21:18:41 GMT</pubDate>
    </item>
    <item>
      <title>囚徒困境的实施</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zhvsk/an_implementation_of_prisoner_dilemma/</link>
      <description><![CDATA[嗨 是否有一个简单的在线囚徒困境的机器学习实现可供我查看？ 我试图从 gpt 获得关于在 JavaScript 中使用张量流实现的帮助（是的，我知道我应该切换到 python），但它确实表现得不太好。我编写了一堆传统策略，如果 ML 策略最终能进入整体前 5 名，我会非常高兴。 我首先尝试使用 3 个密集层，然后我尝试更改并发送最新的移动首先（反向时间轴），然后我添加了全 -1 的输入以表示新比赛的开始。然后我尝试给予最近的动作更多的权重，gpt 建议使用一个 lstm 层作为输入，以强调最近的动作，并为输出提供密集。它建议 returnSequences true 这让我有点困惑，直到我明白它的作用并将其关闭。在所有这些不同的情况下，我尝试了 3 到 5 个先前动作的历史记录。使用 lstm，我尝试发送等于上一场比赛或我发送的所有比赛总和的奖励。 一般情况下，它很早就知道合作是好的，然后永远不会切换到即使对手使用了缺陷，并且每场比赛的得分为零，也会有缺陷。 如果有必要，我可以切换到 python，但我只想建立一个稍微不错的玩家。额外的好处是理解为什么所有这些尝试都没有成功。 编辑添加：除非所有可能的参数都得到完美调整（历史长度、按时间顺序与逆时间顺序排列的数据、数量，这是否正常？层数、探索/利用的 epsilon 等），该模型真的很糟糕吗？我并不期望它是完美的，但我认为前 5 名会很“容易”。  如此多的独立参数和如此多的组合，专家如何找到最佳解决方案？   由   提交 /u/my_n3w_account   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zhvsk/an_implementation_of_prisoner_dilemma/</guid>
      <pubDate>Fri, 05 Jan 2024 21:18:04 GMT</pubDate>
    </item>
    <item>
      <title>先运行 ML 模型还是先进行预处理？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zf761/running_ml_model_first_or_preprocessing/</link>
      <description><![CDATA[Heya 我目前正在开发一个项目，使用 MediaEval 2015 数据集中的推文对真实或虚假媒体进行分类。 I我是个初学者，我有点困惑是否应该先运行模型，然后通过处理数据（例如删除表情符号、语言翻译）或其他方式来提高性能。我不确定删除这些是否会对模型的性能产生负面影响。 如果有人可以提供一些指导，我们将非常感激，谢谢！ &lt; !-- SC_ON --&gt;  由   提交/u/Immortalpancakes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zf761/running_ml_model_first_or_preprocessing/</guid>
      <pubDate>Fri, 05 Jan 2024 19:27:41 GMT</pubDate>
    </item>
    <item>
      <title>TPU编程。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zezgu/tpu_programming/</link>
      <description><![CDATA[它似乎对很多事情都非常有帮助，但我不知道从哪里开始学习它使用的语言。&lt; /p&gt; 除了 FPGA 之外，我对 ASIC 几乎没有任何经验，而且我对 FPGA 从来都不太擅长。 我在互联网上查了几天，尽我所能。 find 是一堆 Medium 文章，这些文章似乎大多是由那些不知道自己在说什么的人串在一起的流行语。 目前，我只使用免费套餐的 Colab，因此，我并不是在寻找比简单的编程教程更复杂的东西，它可以让我了解要学习什么语言，以及它是像 Python 这样的高级语言还是像 Assembly 这样的低级语言。   由   提交 /u/TrustworthyCthulu   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zezgu/tpu_programming/</guid>
      <pubDate>Fri, 05 Jan 2024 19:18:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 ML/DL 的翻译器。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zed6j/translator_using_mldl/</link>
      <description><![CDATA[我目前正在完成我的期末项目（电气工程师学士学位），并且我计划使用 ML/DL 制作一个翻译器。我对这个主题进行了基本的诅咒，这样我就可以理解如何做到这一点......原则上。然而，我并没有从事这么大的事情的认真经验。所以我的主要问题是，这个项目对于以前经验很少的人来说是否可行？这样做时要考虑哪些因素？这个主题是否有足够的文档供我自己完成？我需要多长时间才能做到这一点？ 另一个可能有用的信息，我想将玛雅语言（最流行的）翻译成西班牙语。  [英语不是我的母语，抱歉犯了错误]   由   提交/u/fmoralesh  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zed6j/translator_using_mldl/</guid>
      <pubDate>Fri, 05 Jan 2024 18:52:42 GMT</pubDate>
    </item>
    <item>
      <title>有人在他们的模型中实际使用 float16 混合精度吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zccyh/anyone_actually_using_float16_mixed_precision_in/</link>
      <description><![CDATA[标题几乎说明了一切。我最近尝试在我的模型中使用 float16 权重，这让我想，我从未见过有人说他们使用混合精度。这是不是经常使用，还是我只是缺少使用它的项目？ 此外，您什么时候会考虑使用混合精度而不是标准单精度？   由   提交/u/Rajivrocks  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zccyh/anyone_actually_using_float16_mixed_precision_in/</guid>
      <pubDate>Fri, 05 Jan 2024 17:29:31 GMT</pubDate>
    </item>
    <item>
      <title>ML 或 Web 开发</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zcayw/ml_or_web_dev/</link>
      <description><![CDATA[我的 Python 水平处于中级水平，希望在机器学习 (ML) 领域从事职业生涯。然而，我的一位精通 Web 开发的朋友建议我不要坚持使用 Python。相反，他们建议学习其他语言，如 Java 或 C++，因为它们可以提供对内存管理的更深入的理解。此外，他们建议我考虑转向网络开发，并为初学者提供了更多的范围和机会。请提供指导   由   提交/u/Lazy_Explanation_239   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zcayw/ml_or_web_dev/</guid>
      <pubDate>Fri, 05 Jan 2024 17:27:08 GMT</pubDate>
    </item>
    <item>
      <title>soft-max 创建的概率是否代表模型感受到的概率？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18zar0j/do_the_probabilities_created_by_softmax_represent/</link>
      <description><![CDATA[我不太确定如何表达这个问题，但是 softmax 的输出是否给出了模型相信的概率？  从技术上讲，输出是概率分布，但这并不一定意味着它是模型“相信”的概率。  就像如果我们有猫或非猫分类器，并且 softmax 之后的输出是 0.7、0.3，那么可以准确地说模型认为图像有 70% 的机会是一只猫？或者说softmax仅在关联大小时有用，即在上面的例子中，我们只能说模型更有信心图像是猫而不是不是猫。  我是否对模型进行了太多的拟人化？   由   提交/u/Rit2Strong  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18zar0j/do_the_probabilities_created_by_softmax_represent/</guid>
      <pubDate>Fri, 05 Jan 2024 16:22:22 GMT</pubDate>
    </item>
    <item>
      <title>本周人工智能主要进展简述（2023 年 12 月第 4 周 + 2024 年 1 月第 1 周）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18z95ko/this_weeks_major_ai_developments_in_a_nutshell/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18z95ko/this_weeks_major_ai_developments_in_a_nutshell/</guid>
      <pubDate>Fri, 05 Jan 2024 15:14:07 GMT</pubDate>
    </item>
    <item>
      <title>为什么这个感知器不学习？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18z92te/why_doesnt_this_perceptron_learn/</link>
      <description><![CDATA[import pandas as pd import numpy as np train_set = np.array(pd.read_csv(&#39;/kaggle/input/mnist-in- csv/mnist_train.csv&#39;))[:1000] test_set = np.array(pd.read_csv(&#39;/kaggle/input/mnist-in-csv/mnist_test.csv&#39;))[2000:3000] train_labels = train_set[: , 0].T test_labels = test_set[:, 0].T X_train = train_set[:, 1:].T X_test = test_set[:, 1:].T print(&quot;X_train 形状：&quot;, X_train.shape ) print(&quot;X_label shape: &quot;, train_labels.shape) print(&quot;训练标签 shape: &quot;, train_labels.shape) print(&quot;测试标签 shape: &quot;, test_labels.shape) # 初始化参数 def init_params (): w1 = np.random.rand(784, 10) b1 = np.random.rand(10, 1) w2 = np.random.rand(10, 10) b2 = np.random.rand(10, 1 ) return w1, b1, w2, b2 # 在此定义激活函数 def ReLU(x): return np.maximum(0, x) def Softmax(x): exp_x = np.exp(x - np.max(x)) return exp_x / exp_x.sum() # 定义所用激活的导数 def der_ReLU(x): return np.where(x &lt;= 0, 0, 1) def der_Softmax(z): exps = np.exp(z ) sum_exps = np.sum(exps, axis=1, keepdims=True) softmax = exps / sum_exps return softmax * (1 - softmax) def one_hot_encode(Y): return np.eye(10)[Y].reshape(- 1, 1) print(one_hot_encode(5)) # 定义损失函数 # 标签必须是一个热编码 def cost(output, label): epsilon = 1e-10 # 小常数以避免 log(0) return -np .sum(label * np.log(output + epsilon)) / label.shape[1] def cost_derivative(output, label): return 2 * (output - label) # 前向传递 defforward(x, w1, b1, w2) , b2): Z1 = w1.T.dot(x) + b1 A1 = ReLU(Z1) Z2 = w2.dot(A1) + b2 A2 = Softmax(Z2) return Z1, A1, Z2, A2 # 向后传递 def向后（X，Z2，A2，A1，Z1，W2，标签）：dz2 = Z2 - 标签 dw2 = dz2.dot（A1.T） db2 = np.sum（dz2，axis = 1，keepdims = True）# z1 的导数 dz1 = W2.T.dot(dz2) * der_ReLU(A1) dw1 = dz1.dot(X.T) db1 = np.sum(dz1, axis=1, keepdims=True) 返回 dw1, db1, dw2, db2 # 更新参数 def update_params(w1, b1, w2, b2, dw1, db1, dw2, db2, alpha): w1 = w1 - (alpha * dw1.T) w2 = w2 - (alpha * dw2) b1 = b1 - ( alpha * db1) b2 = b2 - (alpha * db2) return w1, b1, w2, b2 defgradient_descent(X, Y, num_epochs, alpha): &quot;&quot;&quot;在指定的纪元数内执行随机梯度下降。 ”“” w1, b1, w2, b2 = init_params() for epoch in range(num_epochs): loss = 0.0 for i in range(X.shape[1]): # 迭代样本sample_X = X[:, i].reshape( -1, 1) # 重塑为列向量sample_Y = Y[i] sample_Y = one_hot_encode(sample_Y) # 对此样本执行前向传递、后向传递和更新 Z1, A1, Z2, A2 =forward(sample_X, w1, b1) 、w2、b2) dw1、db1、dw2、db2 = 向后(sample_X、Z2、A2、A1、Z1、w2、sample_Y) w1、b1、w2、b2 = update_params(w1、b1、w2、b2、dw1、db1 , dw2, db2, alpha) loss += cost(A2, Sample_Y) # 定期打印损失 if epoch % 50 == 0: # 每 50 个样本打印损失 print(&quot;Epoch:&quot;, epoch, &quot;Loss:&quot; ;, loss) return w1, b1, w2, b2 w1, b1, w2, b2 =gradient_descent(X_train, train_labels, 500, 0.01)  以上是我的代码。损失基本保持不变。我不认为这是学习率问题。很可能是我造成了一些结构混乱。  任何帮助将不胜感激！   由   提交 /u/GraphicsMonster   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18z92te/why_doesnt_this_perceptron_learn/</guid>
      <pubDate>Fri, 05 Jan 2024 15:10:41 GMT</pubDate>
    </item>
    <item>
      <title>如何通过冷电子邮件获得优秀教授的研究实习机会？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18z40z6/how_to_get_research_internships_under_great_profs/</link>
      <description><![CDATA[我是一名数据科学专业的本科生，对 ML、DL 和 Python 有相当的了解。 我一直希望在一些独特且有创意的东西上做大量的工作。但我得到的大多数想法都有教程。我意识到，通过分步教程做一些事情不会给我的个人资料增加任何价值。 主要问题是我没有一个闪亮的作品集，其中有许多成就等，有人会立即雇用它我就像实习生一样。我只有一个关于简单 ML 的小项目和一个非常标准的神经风格迁移项目，尽管我可能拥有比这些项目所展示的更多的 ML/DL 知识。我也读过一些论文和博客。 我的大学没有在这个领域工作的优秀教授，所以我想在 CMU 等大学的教授（排名前 100 的大学）的教授指导下工作crankings AI）。 我如何受聘为一名本科生研究实习生并与优秀教授一起撰写研究论文，而且我如何找到愿意接受我的人？    ;由   提交/u/clever-machine  /u/clever-machine  reddit.com/r/learnmachinelearning/comments/18z40z6/how_to_get_research_internships_under_great_profs/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18z40z6/how_to_get_research_internships_under_great_profs/</guid>
      <pubDate>Fri, 05 Jan 2024 10:42:57 GMT</pubDate>
    </item>
    <item>
      <title>学习机器学习的最佳方法是什么？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/18yzvek/what_is_the_best_way_to_learn_machine_learning/</link>
      <description><![CDATA[我想开始学习机器学习。但我看到很多人说了很多东西...有人说最好购买课程，有人说最好通过做项目开始学习... 我有一点编程背景。 ……我的目标是从事数据科学/机器学习领域的职业。但我不知道如何开始。因为我喜欢以结构化的方式和类似于学术的环境学习，所以我想我将从 mit ocw 课程开始学习。但是你看，有些人真的只是先开始学习库（比如 pandas、numpy 等）。  我想将机器学习学习到完全掌握基础知识的水平。 那么，你们认为开始学习 ML 的最佳方法是什么以及需要多长时间你们认为我需要掌握基础知识。 提前感谢您的宝贵时间..😁   由   提交 /u/juspassingby___-_-   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/18yzvek/what_is_the_best_way_to_learn_machine_learning/</guid>
      <pubDate>Fri, 05 Jan 2024 06:11:47 GMT</pubDate>
    </item>
    </channel>
</rss>