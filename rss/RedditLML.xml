<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>学习机器学习</title>
    <link>https://www.reddit.com/r/learnmachinelearning/</link>
    <description>致力于学习机器学习的 Reddit 子版块</description>
    <lastBuildDate>Wed, 24 Jan 2024 18:16:38 GMT</lastBuildDate>
    <item>
      <title>从头开始学习</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19emys9/learning_from_the_beginning/</link>
      <description><![CDATA[您好，我是一名计算机科学专业二年级学生，去年夏天我开始学习数据科学和机器学习，并练习了一些机器学习技术和数据可视化使用python等等，但我觉得我不太明白我在做什么，我也有几个月没有练习了，所以我想重新开始我的学习过程，做项目等等 我擅长Python，我学过微积分1,2和线性代数、概率和统计 所以我认为我缺少的部分是学习机器学习以及如何实际解决问题和使用库像 pandas 和 matplotlib，所以任何关于如何开始的建议都会很棒！提前感谢大家   由   提交/u/Extension-Group2131    reddit.com/r/learnmachinelearning/comments/19emys9/learning_from_the_beginning/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19emys9/learning_from_the_beginning/</guid>
      <pubDate>Wed, 24 Jan 2024 18:12:49 GMT</pubDate>
    </item>
    <item>
      <title>需要有关 Pandas 和 Cloud API 的建议</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19emxt8/need_advice_regarding_pandas_and_cloud_api/</link>
      <description><![CDATA[大家好，我刚刚完成了 Corey Schafer 的 pandas 播放列表，并开始处理 Stack Overflow 工作调查的真实数据集。但我想提高我的技能，以便更熟悉 Pandas。那么有人可以建议一些我可以练习技能的资源/数据集吗？就像某种笔记本，里面有关于我们正在搜索什么的问题，就像我们练习 SQL 一样。 此外，我对使用 Cloud API 进行 AI/ML 感兴趣，所以您能否建议我如何开始也与他们一起。   由   提交/u/Ok_Law1259   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19emxt8/need_advice_regarding_pandas_and_cloud_api/</guid>
      <pubDate>Wed, 24 Jan 2024 18:11:43 GMT</pubDate>
    </item>
    <item>
      <title>困惑如何处理缺失值:(</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19emrty/confused_as_what_to_do_with_missing_values/</link>
      <description><![CDATA[所以我目前正在研究道路事故数据集的事故严重程度预测任务。 看起来像是一个简单的机器学习问题。然而它有 46 个列特征，其中 17 个存在缺失值。我还想提一下 df.shape = (7728394, 46) 其中 12 个的缺失值百分比小于 5%（我可以相应地删除/估算），另一个下面提到了一些。 End_Lat：缺失 44.6% End_Lng：44.6% -- Wind_Chill(F)：23.8% -- 风速（英里/小时）：5.1% -- 降水量（英寸）：27.5% -- 有关如何进行此操作的任何提示？   由   提交 /u/Stunning_Marzipan105   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19emrty/confused_as_what_to_do_with_missing_values/</guid>
      <pubDate>Wed, 24 Jan 2024 17:53:49 GMT</pubDate>
    </item>
    <item>
      <title>机器学习课程</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19emq5b/machine_learning_courses/</link>
      <description><![CDATA[嘿伙计们！我想开始学习机器学习课程，这将帮助我涉足水中并推进我在该领域的工作。我正在四处寻找课程，想知道你们认为什么是最好的机器学习课程。    由   提交 /u/mootyhoot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19emq5b/machine_learning_courses/</guid>
      <pubDate>Wed, 24 Jan 2024 17:45:34 GMT</pubDate>
    </item>
    <item>
      <title>为什么我的成绩是零</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19emgug/why_my_grad_is_zero/</link>
      <description><![CDATA[当我陷入这个困境时，请帮助我做一些事情，我的成绩总是为零 这里是整个代码的链接：https://colab.research.google.com/drive/1Y2pkfloBMu5divG9CK642Hyyup9rMC2F?usp=sharing   由   提交/u/narendra7799  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19emgug/why_my_grad_is_zero/</guid>
      <pubDate>Wed, 24 Jan 2024 17:26:18 GMT</pubDate>
    </item>
    <item>
      <title>“渐变”的多重含义</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19em8fj/the_multiple_meanings_of_gradient/</link>
      <description><![CDATA[（ML 新手，作为一名编码员/我的头脑围绕着数学） 我试图理解术语“梯度”的使用因为它涉及机器学习相关的数学以及深度神经网络的操作。我现在的。不完整的心智模型如下。  “梯度”这个概念有两种表达方式。出现：  “渐变”下降：模型计算和执行单个节点/函数的参数应如何更新的过程，以便产生更接近于建模内容的输出。  “渐变” /“grad”：为每个节点保存的单独值，用于评估该节点的执行对最后一层的输出值的影响程度。   这是正确的吗？  正如你可能猜测的那样，我的理解很混乱，我想要么崩溃，要么完全理清上面的两点。想法之间有什么关系？我是否应该放弃这个思路并以不同的方式处理它？  注意：为了清楚起见，我忽略了“渐变”的使用。因为它与梯度提升有关，但如果它有助于将其纳入两个桶中（或者可能是第三个桶），我很想听听！  提前谢谢您！   由   提交 /u/jpranay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19em8fj/the_multiple_meanings_of_gradient/</guid>
      <pubDate>Wed, 24 Jan 2024 17:16:50 GMT</pubDate>
    </item>
    <item>
      <title>如何在较小的数据集上训练 ViT？ Ik ViT 不适用于小数据集和低分辨率。但是您是否曾经在 CIFAR10/100 上使用 ViT 达到传统 CNN 的精度</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19elsan/how_to_train_vit_on_smaller_datasets_ik_vits/</link>
      <description><![CDATA[我一直在 CIFAR10 和 100 上使用 ViT。但我无法在 CIFAR10 上获得超过 75% 的准确率。 &lt; p&gt;我已经尝试过这些架构配置：  补丁大小：4 和 8 维度：512 和 786 深度/变压器块：8 和 10 个注意力头：8、10 和 12 mlp 维度：512、2048 和 3072 &lt; /code&gt; 我分别尝试了具有不同学习率 (0.1, 0.01) 和 (0.01, 0.001) 的 SGD 和 Adam，学习率在 100 和 175 epoch 后逐渐减小。 还使用 1e-4 和 1e-5 的权重衰减以及一些图像增强，例如随机翻转、随机旋转、最小颜色抖动和随机仿射。 任何提高验证准确性的建议以达到〜 90%？   由   提交/u/V1bicycle  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19elsan/how_to_train_vit_on_smaller_datasets_ik_vits/</guid>
      <pubDate>Wed, 24 Jan 2024 16:58:27 GMT</pubDate>
    </item>
    <item>
      <title>从头开始构建基于 LLM 的分类器</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ejd2v/building_llmbased_classifiers_from_scratch/</link>
      <description><![CDATA[大家好，我录制了一个简短的编码会话，向您展示如何从头开始构建基于 LLM 的分类器。 对于法学硕士来说，这是一个被高度低估的用例。它允许您回答是/否问题或大规模解决其他分类任务。 示例应用程序包括自动分类文章、电子邮件、帖子等，甚至视频（基于其文字记录）。 示例应用程序包括自动分类文章、电子邮件、帖子等，甚至视频（基于其文字记录）。 p&gt; https://www.youtube.com/watch?v=l7NPMiyuh1M我希望你觉得它有用:)   由   提交/u/alongub  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ejd2v/building_llmbased_classifiers_from_scratch/</guid>
      <pubDate>Wed, 24 Jan 2024 15:14:51 GMT</pubDate>
    </item>
    <item>
      <title>需要 pytorch VAE 示例的帮助</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eimpl/need_help_with_pytorch_vae_example/</link>
      <description><![CDATA[亲爱的*， ​ 我可能需要一些帮助来解决我的玩具 VAE问题。我有3个问题： ​  为什么x是encode()中的一个列表？  看起来我必须这样做： x = F.relu(self.fc1(self.embedding(x[0])))  这会影响性能吗? ​ 2) 我无法训练模型，因为我运行时遇到形状问题。 ValueError: 目标尺寸（火炬） .Size([16, 160])) 必须与输入大小相同 (torch.Size([16, 160, 160]))  16 是我的批量大小，160 是输入的长度。所以目标大小代表我的标记化（随机）输入。如何解决此处出现的问题： ​ recon_loss = F.binary_cross_entropy_with_logits(x_recon, x[0],duction=&#39;sum&#39;)  3) 如果我想在某个时刻使用 VAE 进行采样，我该怎么做？有人可以推荐一些基于 pytorch 和 pt-lightning 的好资源吗？ ​ 编辑： 4）我怎样才能真正获得一些潜在的表示，人们通常使用嵌入的地方，例如用于聚类？ ​ ​ 谢谢！ 这是我的代码： 导入torch 导入torch.nn as nn导入torch.optim作为optim从pytorch_lightning导入Trainer从pytorch_lightning.core导入LightningModule从torch.nn导入功能作为F从torch.utils。数据导入 DataLoader, TensorDataset class VAE(nn.Module): def __init__(self, input_dim, hide_dim, Latent_dim): super(VAE, self).__init__() # 编码器 self.embedding = nn.Embedding(input_dim, hide_dim) self .fc1 = nn.Linear(hidden_​​dim, 256) self.fc_mu = nn.Linear(256, Latent_dim) self.fc_logvar = nn.Linear(256, Latent_dim) # 解码器 self.fc2 = nn.Linear(latent_dim, 256) self .fc3 = nn.Linear(256, hide_dim) self.fc4 = nn.Linear(hidden_​​dim, input_dim) self.get_probs = nn.Softmax(dim=1) def 编码(self, x): x = F.relu(self .fc1(self.embedding(x[0]))) mu = self.fc_mu(x) logvar = self.fc_logvar(x) return mu, logvar def reparameterize(self, mu, logvar): std = torch.exp( 0.5 * logvar) eps = torch.randn_like(std) return mu + eps * std def 解码(self, z): z = F.relu(self.fc2(z)) z = F.relu(self.fc3(z) )) z = torch.sigmoid(self.fc4(z)) return z defforward(self, x): mu, logvar = self.encode(x) z = self.reparameterize(mu, logvar) x_recon = self.decode (z) x_recon = F.log_softmax(x_recon, dim=-1) 返回 x_recon, mu, logvar 类 VAE_Lightning(LightningModule): def __init__(self, model,learning_rate=1e-3): super(VAE_Lightning, self).__init__ () self.model = 模型 self.learning_rate = Learning_rate def Training_step(self, batch, batch_idx): x = 批次 x_recon, mu, logvar = self.model(x) loss = self.vae_loss(x_recon, x, mu, logvar ) 返回损失 def configure_optimizers(self): return optim.Adam(self.parameters(), lr=self.learning_rate) def vae_loss(self, x_recon, x, mu, logvar): print(x_recon) print(x_recon.shape) print(x) print(x[0].shape) recon_loss = F.binary_cross_entropy_with_logits(x_recon, x[0],duction=&#39;sum&#39;) kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) ) - logvar.exp()) return recon_loss + kl_divergence if __name__ == &#39;__main__&#39;: # 创建 token 的随机张量 length = 160 n_tokens = 30 n_samples = 1000 batch_size = 16 tensor_dataset = torch.randint(0, n_tokens, (n_samples) , length)) dataset = TensorDataset(tensor_dataset) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4) # 初始化 VAE 模型 input_dim = length hide_dim = 64 Latent_dim = 16 # 初始化模型和 LightningModule vae_model = VAE( input_dim, hide_dim, Latent_dim) vae_lightning = VAE_Lightning(vae_model) # 训练模型 trainer = Trainer(max_epochs=10) trainer.fit(vae_lightning, dataloader)  ​   由   提交 /u/onlyrandomthings   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eimpl/need_help_with_pytorch_vae_example/</guid>
      <pubDate>Wed, 24 Jan 2024 14:41:56 GMT</pubDate>
    </item>
    <item>
      <title>Karpathy的课程《神经网络：从零到英雄》是学习NLP的良好开端吗？</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eib9a/is_karpathys_course_neural_networks_zero_to_hero/</link>
      <description><![CDATA[我一直在通过浅层 ML/DL（使用预训练模型进行微调）进行回归、分类和这些任务，并且我有扎实的基础对它的理解。  现在，我有一个宠物项目的想法，但它基本上是一个适合微调语言模型的 NLP 任务。由于我对 NLP 的了解为零，并且希望将其学习到一个不错的（功能）水平，以便对某些应用进行微调，例如HuggingFace 预训练模型很有意义，Karpathy 的课程《从零到英雄》的教学大纲足以满足我的需求吗？ https ://karpathy.ai/zero-to-hero.html 我发现它回归到建立和培训法学硕士，但我不确定这是否是正确的一路上教授所有这些 NLP 基础知识。   由   提交/u/maybenexttime82   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eib9a/is_karpathys_course_neural_networks_zero_to_hero/</guid>
      <pubDate>Wed, 24 Jan 2024 14:27:03 GMT</pubDate>
    </item>
    <item>
      <title>感觉自己什么都不知道</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ei3g2/it_feels_like_i_know_nothing/</link>
      <description><![CDATA[大家好， 几个月前我开始了我的机器学习之旅。这几个月来，我读了两本最受认可的 ML 书籍，看了 3 门课程，全部都是通过编码和项目完成的 但我仍然觉得我什么都不懂，我什至不知道如何开始一个ML 项目 任何关于如何发展我的技能的建议，我对 ML 非常感兴趣，并且想成为该领域的工程师 感谢您的阅读 &lt; /div&gt;  由   提交/u/Herooftime998  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ei3g2/it_feels_like_i_know_nothing/</guid>
      <pubDate>Wed, 24 Jan 2024 14:17:11 GMT</pubDate>
    </item>
    <item>
      <title>Chomsky vs Shannon 的 NLP 和 AI 方法 - Chris Manning 斯坦福大学 OpenNLP 创建者</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ef7ql/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/</link>
      <description><![CDATA[       由   提交 /u/fancypigollo   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ef7ql/chomsky_vs_shannon_approaches_to_nlp_and_ai_chris/</guid>
      <pubDate>Wed, 24 Jan 2024 11:43:54 GMT</pubDate>
    </item>
    <item>
      <title>[项目] BELT（较长文本的 BERT）</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19ee1fb/project_belt_bert_for_longer_texts/</link>
      <description><![CDATA[我们创建了 BELT（BERT For Longer Texts）——一个 Python 包，允许对长度超过 512 个 token 的文本使用类似 BERT 的模型。该方法是 Jacob Devlin 提出的想法的实现，Jacob Devlin 是 评论。您可以在 Medium 上我刚刚发表的两篇文章中阅读有关它的更多详细信息： 第一部分是应用 BERT 分类器的概述： 第 1 部分 第二部分深入介绍我们训练 BELT 模型的方法。 第 2 部分 该存储库已开源： Repo 我知道你在想什么：“等等，bucko，这不是什么新鲜事。每个人都知道有像 BigBird 或 Longformer 这样的模型可以处理更长的文本”。对此我的回答是：“我知道，伙计，但是 BigBird 和 Longformer 不是修改过的 BERT。它们是具有不同架构的模型。因此，它们需要从头开始预训练或下载。 BELT修改模型微调。这带来了 BELT 方法的主要优点 - 它使用任何预先训练的 BERT 或 RoBERTa 模型。快速查看 HuggingFace Hub 可以确认，BERT 的资源比 Longformer 多大约 100 倍。找到适合特定任务或语言的可能会更容易。”享受吧！   由   提交/u/MBrzozowskiML   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19ee1fb/project_belt_bert_for_longer_texts/</guid>
      <pubDate>Wed, 24 Jan 2024 10:27:40 GMT</pubDate>
    </item>
    <item>
      <title>使用 C++/CUDA 构建的相同模型收敛速度不如使用 Python/NumPy 构建的模型</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19eagxe/identical_model_built_using_ccuda_doesnt_converge/</link>
      <description><![CDATA[我想学习神经网络背后的数学，因此我决定使用 numpy 从头开始​​构建一个 Python 模型来训练 MNIST 数据集。速度非常快，在调整超参数后，在 30 个 epoch 内测试数据集的准确率达到了 98%。作为一个简单的全连接网络，我没想到它会做得更好。所以我决定在 C++ 和 CUDA 中尝试同样的方法。该模型确实可以学习，但收敛速度不那么快。即使经过 1000 个 epoch，其准确性仍然非常可怕。模型架构相同，浮点精度、权重和偏差初始化、小批量大小以及所有超参数都相同。 GPU 在每次内核调用后也会同步。然而该模型的收敛速度似乎并没有那么快。我不知道该怎么做。有人知道可能是什么原因造成的吗？感谢您阅读所有内容。希望您有美好的一天。   由   提交 /u/Hey_DeadGuyHere   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19eagxe/identical_model_built_using_ccuda_doesnt_converge/</guid>
      <pubDate>Wed, 24 Jan 2024 06:16:09 GMT</pubDate>
    </item>
    <item>
      <title>这里发生了什么？这只是大规模的过度拟合吗？或者是其他东西？提前致谢。</title>
      <link>https://www.reddit.com/r/learnmachinelearning/comments/19e57wk/whats_going_on_here_is_this_just_massive/</link>
      <description><![CDATA[   /u/HoleNother  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/learnmachinelearning/comments/19e57wk/whats_going_on_here_is_this_just_massive/</guid>
      <pubDate>Wed, 24 Jan 2024 01:39:25 GMT</pubDate>
    </item>
    </channel>
</rss>