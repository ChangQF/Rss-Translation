<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 14 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>大语言模型中公平评估的变质测试：识别骆驼和GPT中的交叉偏见</title>
      <link>https://arxiv.org/abs/2504.07982</link>
      <description><![CDATA[ARXIV：2504.07982V1公告类型：新 
摘要：大型语言模型（LLM）在自然语言处理方面取得了长足的进步，但仍然容易受到与公平相关的问题的影响，通常反映了其培训数据中固有的偏见。这些偏见会带来风险，特别是当LLM部署在敏感地区，例如医疗保健，金融和法律时。本文介绍了一种变质​​测试方法，以系统地识别LLMS中的公平性错误。我们定义并应用了一组面向公平的变质关系（MRS），以评估各种人口统计学输入中的最先进的LLM Llama和GPT模型。我们的方法包括为每个MR生成源和后续测试案例，并分析违反公平的模型响应。结果表明，MT在暴露偏见模式中的有效性，尤其是与语调和情感有关，并突出显示了经常揭示公平性故障的敏感属性的特定交集。这项研究改善了LLMS中的公平测试，提供了一种结构化方法来检测和减轻偏见并改善对公平敏感应用中的模型鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2504.07982</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>心理健康知识增强LLM的社会网络危机干预文本转移识别方法</title>
      <link>https://arxiv.org/abs/2504.07983</link>
      <description><![CDATA[ARXIV：2504.07983V1公告类型：新 
摘要：随着社交媒体平台上心理健康危机的普遍性的增加，识别和预防潜在伤害已成为紧迫的挑战。这项研究介绍了一个大型语言模型（LLM）基于社交网络危机干预的文本转移识别方法，并通过特定领域的心理健康知识增强了。我们提出了一个多层框架，该框架结合了使用BERT的转移学习，并整合了心理健康知识，情感分析和行为预测技术。该框架包括一个在现实世界中的社交媒体数据集中训练的危机注释工具，使该模型能够检测细微的情感线索并确定心理危机。实验结果表明，该提出的方法在危机检测准确性中的表现优于传统模型，并表现出对微妙的情感和上下文变化的更高敏感性。]]></description>
      <guid>https://arxiv.org/abs/2504.07983</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于微调句子 - 伯特和LDA的主题挖掘</title>
      <link>https://arxiv.org/abs/2504.07984</link>
      <description><![CDATA[ARXIV：2504.07984V1公告类型：新 
摘要：研究背景：随着社会的持续发展，消费者更加关注购物时产品细粒度属性的关键信息。研究目的：这项研究将微调句子 -  bert单词嵌入模型和LDA模型，在线商品评论中挖掘主题特征，并向消费者展示商品各个方面的细节。研究方法：首先，在电子商务在线评论领域进行了微调，在线评论文本被转换为带有更丰富语义信息的单词矢量。其次，矢量化的单词集输入到LDA模型中以进行主题特征提取。最后，通过主题下的关键字分析专注于产品的关键功能。结果：本研究将该模型与其他单词嵌入模型和LDA模型进行了比较，并将其与常见主题提取方法进行了比较。该模型的主题一致性比其他模型高0.5，这提高了主题提取的准确性]]></description>
      <guid>https://arxiv.org/abs/2504.07984</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>密封：免费的大语言模型的通知推理校准</title>
      <link>https://arxiv.org/abs/2504.07986</link>
      <description><![CDATA[ARXIV：2504.07986V1公告类型：新 
摘要：大型语言模型（LLMS），例如OpenAI的O1系列，通过扩展的思维链（COT）推理机制证明了对复杂推理任务的引人注目的功能。但是，最近的研究揭示了COT推理轨迹的大量冗余，这不仅增加了推理潜伏期，而且还通过将注意力转移到不必要的推理路径上对模型性能产生负面影响。为了解决这个问题，我们研究了LLM的内部推理结构，并将它们分为三种主要思想类型：执行，反思和过渡思想。此外，我们的分析表明，过度反思和过渡思想与失败案例密切相关，这些思想类别在潜在空间中表现出明显的分离。基于这些，我们引入了密封件（可通行的推理校准），这是一种无训练的方法，可以无缝校准COT工艺，提高准确性，同时显示出显着的效率提高。密封由一个离线阶段组成，用于在潜在空间中提取推理转向向量，然后通过使用转向向量的表示干预对推理轨迹进行直通校准。值得注意的是，转向矢量在各种任务中表现出强大的可传递性。多种模型（DeepSeek-R1-Distill和QWQ-32B-Preigiew）和基准测试（Math500，GSM8K，LiveCodeBench）之间进行了广泛的实验，可验证密封的有效性，而准确性提高了11％，而将其降低了11.8％至50.4％。我们的代码可在https://github.com/vita-group/seal上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2504.07986</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>区域小故事：使用小型模型比较语言学习和代币功能</title>
      <link>https://arxiv.org/abs/2504.07989</link>
      <description><![CDATA[ARXIV：2504.07989V1公告类型：新 
摘要：小语言模型（SLM）为特定域提供了有效的LLMS替代方案。 2023年的小故事研究开发了一个英语数据集，该数据集允许具有1至1000万参数的SLM产生连贯的输出。我们的研究通过将原始数据集转换为印度语言并使用LLM创建合成数据来扩展此框架。我们专注于印地语，马拉地语和孟加拉语，评估SLM的区域语言处理和理解语言复杂性。我们表明，SLM的参数比LLMS有效地处理区域语言，为``代币化策略&#39;&#39;和语言复杂性提供了一个互补的框架。我们的分析表明，语言特异性的表达者表明，该语言的特定于印度语言的通用效果超过了仿效的效果，并提供了信息分析。印地语模型和孟加拉语，我们表明，合成数据集超过了训练SLM的内容。]]></description>
      <guid>https://arxiv.org/abs/2504.07989</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型中的“神经how叫”：一种自我增强的偏见现象和动态衰减解决方案</title>
      <link>https://arxiv.org/abs/2504.07992</link>
      <description><![CDATA[ARXIV：2504.07992V1公告类型：新 
摘要：大语言模型（LLM）驱动的AI系统可能表现出推理故障模式，我们称其为“神经ho叫”，这是一种自我增强的认知环，其中某些高度加权的输入占主导地位，从而导致固定的响应模式抵抗校正。本文探讨了这种现象的基础机制，该机制不同于模型崩溃和偏见的显着性加权。我们提出了一种基于衰减的校正机制，即使在“锁定” AI系统中，也可以动态引入平衡调整并可以恢复适应性推理。此外，我们讨论了由不当管理的强化产生的其他相关效果。最后，我们概述了这种缓解策略的潜在应用，以改善现实世界决策任务中的AI鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2504.07992</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估本体论在问题产生的任务中的适应性</title>
      <link>https://arxiv.org/abs/2504.07994</link>
      <description><![CDATA[ARXIV：2504.07994V1公告类型：新 
摘要：基于本体的问题生成是语义感知系统的重要应用，它可以为各种学习环境创建大型问题库。这些系统的有效性，无论是在产生问题的能力和认知难度方面，都在很大程度上取决于基本本体论的质量和建模方法，因此评估其对此任务的适合度至关重要。迄今为止，还没有对影响问题产生过程的特定本体方面或特征进行全面调查。因此，本文提出了一组要求和特定于任务的指标，以评估教学环境中问题生成任务的适应性。使用Romeo方法论，一种用于得出特定任务指标的结构化框架，采用了一种基于专家的方法来评估自动问题生成（AQG）任务中各种本体学的性能，然后在一组本体论中进行评估。我们的结果表明，本体特征显着影响问题产生的有效性，不同的本体论表现出不同的绩效水平。这突出了评估本体质量在AQG任务方面的重要性。]]></description>
      <guid>https://arxiv.org/abs/2504.07994</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Safechat：建立值得信赖的协作助理及其有用性的案例研究的框架</title>
      <link>https://arxiv.org/abs/2504.07995</link>
      <description><![CDATA[ARXIV：2504.07995V1公告类型：新 
摘要：协作助理或聊天机器人是数据驱动的决策支持系统，可实现自然互动以完成任务完成。尽管他们可以在现代社会中满足关键需求，但对其可靠性和可信赖性的关注仍然存在。特别是，大型语言模型（LLM）基于Chatgpt，Gemini和DeepSeek等聊天机器人越来越易于​​使用。但是，此类聊天机器人有局限性，包括无法解释响应产生，产生有问题的内容的风险，缺乏标准化的可靠性测试以及对深度AI专业知识的需求和扩展的开发时间。这些问题使聊天机器人不适合对选举或医疗保健等信任敏感的应用程序。为了解决这些问题，我们介绍了Safechat，这是一种用于构建安全且值得信赖的聊天机器人的一般体系结构，重点是信息检索用例。 SafeChat的主要特征包括：（a）安全性，具有域形不足的设计，在该设计中，响应是接地的，可以追溯到批准的来源（出处），以及“ do-not-not-respond”策略，以防止有害答案； （b）可用性，自动提取性摘要长期响应，可追溯到其来源，以及自动化的信任评估，以传达预期的聊天机器人行为，例如情感； （c）快速，可扩展的开发，包括CSV驱动的工作流程，自动测试以及与各种设备的集成。我们使用开源聊天机器人平台RASA在可执行的框架中实现了Safechat。一项案例研究表明了其在构建选举机器人-SC中的应用，这是一种旨在安全传播官方选举信息的聊天机器人。 Safechat正在许多域中使用，验证其潜力，并在以下网址提供：https：//github.com/ai4society/trustworthy-chatbot。]]></description>
      <guid>https://arxiv.org/abs/2504.07995</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偏见：评估大语模型的社会偏见因果推理</title>
      <link>https://arxiv.org/abs/2504.07997</link>
      <description><![CDATA[ARXIV：2504.07997V1公告类型：新 
摘要：尽管大型语言模型（LLM）已经在社会中起着重要作用，但研究表明，LLM仍会产生内容，包括针对某些敏感群体的社会偏见。尽管现有的基准有效地确定了LLM中的社会偏见，但我们对导致这些偏见产出的基本推理的理解仍然存在一个危险的差距。本文回答引起社会偏见的问题时，迈出了一步，以评估LLM的因果推理过程。我们首先提出了一个新颖的概念框架，以对LLMS产生的因果推理进行分类。接下来，我们使用LLMS合成$ 1788 $的问题，涵盖$ 8 $敏感属性并手动验证它们。这些问题可以通过让LLMS使用因果图披露其推理过程来测试各种因果推理。然后，我们测试4个最先进的LLM。所有模型都以有偏见的因果推理来回答大多数问题，从而产生了$ 4135 $偏见的因果图。同时，通过分析“无偏见”案件，我们发现LLMS的$ 3 $策略以避免因果推理。最后，我们揭示了LLM也容易出现“偏见”因果推理，在那里他们首先将与因果关系混淆以推断特定的敏感群体名称，然后结合有偏见的因果推理。]]></description>
      <guid>https://arxiv.org/abs/2504.07997</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于变压器的语言模型的语言解释性：系统评价</title>
      <link>https://arxiv.org/abs/2504.08001</link>
      <description><![CDATA[ARXIV：2504.08001V1公告类型：新 
摘要：基于变压器体系结构的语言模型在许多与语言有关的任务（例如文本分类或情感分析）中取得了出色的成果。但是，尽管这些模型的结构是明确定义的，但对它们的内部计算如何帮助他们取得结果知之甚少。截至今天，这将使这些模型是一种“黑匣子”系统。但是，有一系列研究 - “可解释性”  - 旨在了解这些模型中的信息是如何编码的。更具体地说，有专门研究基于变形金刚的模型是否具有类似于人类者的语言现象的知识，这是我们称之为这些模型的“语言解释性”的知识。在这项调查中，我们对160所研究作品进行了全面的分析，分布在多种语言和模型中，包括多语言，试图从几个传统语言学学科的角度发现语言信息：语法，形态学，词典 - 词和话语。我们的调查填补了现有的可解释性文献的空白，该文献不关注这些模型中的语言知识，或者提出了一些局限性 - 例如仅研究基于英语的模型。我们的调查还集中于预先训练的语言模型，而不是进一步专门针对下游任务，重点是使用探索模型内部表示的可解释性技术的作品。]]></description>
      <guid>https://arxiv.org/abs/2504.08001</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更多样化的自适应：用于改进电子商务的LLM域适应性的全面多任务学习</title>
      <link>https://arxiv.org/abs/2504.08002</link>
      <description><![CDATA[ARXIV：2504.08002V1公告类型：新 
摘要：近年来，由于其强大的域适应能力，大型语言模型（LLMS）已广泛应用于各个领域。先前的研究表明，多种模式数据可以增强LLMS的域适应性性能。但是，该假设在电子商务领域仍未得到充分验证。为了解决这一差距，我们提出了一个全面的电子商务多任务框架和设计经验实验，以从两个角度研究不同数据和任务对LLM的影响：“能力全面性”和“任务全面性”。具体而言，我们通过逐步引入与新的主要能力领域相关的任务以及在不同的主要功能域中不断添加子任务，从而观察到LLM性能的显着改善。此外，我们观察到，增加模型能力增加了多样性的好处，这表明模型容量与数据多样性之间存在协同关系。最后，我们从2024年KDD杯中的经验实验中验证了表现最佳的模型，在任务1中获得了排名5的排名。这一结果表明了我们研究对推进电子商务领域LLM的重要性。]]></description>
      <guid>https://arxiv.org/abs/2504.08002</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从语音到摘要：对语音摘要的全面调查</title>
      <link>https://arxiv.org/abs/2504.08024</link>
      <description><![CDATA[ARXIV：2504.08024V1公告类型：新 
摘要：语音摘要已成为有效管理和访问口语和视听内容日益增长的工具。但是，尽管其重要性越来越大，但语音摘要仍未得到明确定义，并与多个研究领域相交，包括语音识别，文本摘要和特定应用，例如满足摘要。这项调查不仅研究了现有的数据集和评估方法，这些方法对于评估摘要方法的有效性至关重要，而且还综合了该领域的最新发展，从而强调了从传统系统向高级模型（如精细调整的级联建筑架构和端到端解决方案）的转变。]]></description>
      <guid>https://arxiv.org/abs/2504.08024</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推理LLM可以增强临床文档分类吗？</title>
      <link>https://arxiv.org/abs/2504.08040</link>
      <description><![CDATA[ARXIV：2504.08040V1公告类型：新 
摘要：临床文档分类对于将非结构化医学文本转换为标准化的ICD-10诊断至关重要，但是由于复杂的医学语言，隐私限制和有限的注释数据集，它面临挑战。大型语言模型（LLMS）为这项任务提供了有希望的改善准确性和效率。这项研究评估了八个LLM的性能和一致性；四个推理（QWEN QWQ，DeepSeek推理器，GPT O3 Mini，Gemini 2.0 Flash Thinking）和四个非争议（Llama 3.3，GPT 4O Mini，Gemini 2.0 2.0 Flash，DeepSeek Chat）；在使用MIMIC-IV数据集对临床放电摘要进行分类时。使用CTAKE来构建临床叙事，在三个实验跑步中评估了模型，多数投票决定了最终预测。结果表明，推理模型的精度（71％vs 68％）和F1得分（67％vs 60％）优于非争议模型，Gemini 2.0 Flash Thinking可以达到最高精度（75％）和F1分数（76％）。但是，非争议模型表现出更大的稳定性（91％vs 84％的一致性）。 ICD-10代码的性能各不相同，其推理模型在复杂的情况下出色，但在抽象类别中挣扎。调查结果表明准确性和一致性之间的权衡，表明混合方法可以优化临床编码。未来的研究应探讨多标签分类，特定于领域的微调和集合方法，以增强现实世界应用中的模型可靠性。]]></description>
      <guid>https://arxiv.org/abs/2504.08040</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>虚假新闻检测的多视图自动编码器</title>
      <link>https://arxiv.org/abs/2504.08102</link>
      <description><![CDATA[ARXIV：2504.08102V1公告类型：新 
摘要：鉴于假新闻在社交媒体中传播的数量和速度，自动假新闻检测已成为非常重要的任务。但是，此任务提出了一些挑战，包括提取包含有关假新闻相关信息的文本功能。有关虚假新闻检测的研究表明，在所有情况下，没有任何单一的特征提取技术始终优于其他功能。然而，不同的功能提取技术可以提供有关文本数据的互补信息，并可以更全面地表示内容。本文建议使用多视图自动编码器通过整合文献中常用的几种特征提取技术来生成伪造新闻检测的联合特征表示。与单个观点（功能表示形式）相比，假新闻数据集的实验显示了分类性能的显着改善。我们还观察到，在准确性和计算工作方面，选择视图的子集而不是用所有视图组成潜在空间可能是有利的。有关更多详细信息，包括源代码，图形和数据集，请参阅该项目的存储库：https：//github.com/ingryrydpereira/multiview-fake-news。]]></description>
      <guid>https://arxiv.org/abs/2504.08102</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DeepSeek vs. O3-Mini：推理LLM可以评估MT和摘要的能力如何？</title>
      <link>https://arxiv.org/abs/2504.08120</link>
      <description><![CDATA[ARXIV：2504.08120V1公告类型：新 
摘要：支持推理的大语言模型（LLMS）最近在复杂的逻辑和数学任务中表现出令人印象深刻的表现，但是它们在评估自然语言生成方面的有效性仍未得到探索。这项研究系统地将基于推理的LLM（DeepSeek-R1和OpenAI O3）与跨机器翻译（MT）（MT）和文本摘要（TS）评估任务进行了比较。我们评估了三个架构类别的八个模型，包括最先进的推理模型，它们的蒸馏变体（从8B到70B参数）以及同等的常规，非季节性的LLM。我们在WMT23和夏斯文基准上进行的实验表明，推理能力的好处是高度模型和任务依赖性的：虽然OpenAI O3-Mini模型显示出一致的性能提高，而推理强度的提高，而DeepSeek-R1表现不佳与其非劳动变体相比，与TS评估的某些方面相比，其非劳动性变体相比。相关分析表明，增加的推理令牌用法与O3-MINI模型中的评估质量正相关。此外，我们的结果表明，推理能力的蒸馏在中型模型（32B）中保持合理的性能，但在较小的变体（8B）中大大降解。这项工作提供了对NLG评估的推理LLM的首次全面评估，并提供了对其实际使用的见解。]]></description>
      <guid>https://arxiv.org/abs/2504.08120</guid>
      <pubDate>Mon, 14 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>