<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上进行更新。</description>
    <lastBuildDate>Fri, 01 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过事实模板分解减少实体抽象概括中的幻觉</title>
      <link>https://arxiv.org/abs/2402.18873</link>
      <description><![CDATA[arXiv:2402.18873v1 公告类型：新
摘要：实体摘要摘要旨在基于一组相关的互联网文档生成给定实体的连贯描述。预训练语言模型（PLM）在这项任务中取得了巨大的成功，但它们可能会产生幻觉，即生成有关实体的非事实信息。为了解决这个问题，我们将摘要分解为两个部分： 表示有关给定实体的事实信息的事实，PLM 很容易伪造这些信息；包含通用内容和指定事实槽的模板，PLM 可以胜任生成这些内容。基于事实模板分解，我们提出了 SlotSum，一个可解释的实体抽象摘要框架。 SlotSum 首先创建模板，然后根据输入文档预测每个模板槽的事实。受益于我们的事实模板分解，SlotSum 可以轻松定位错误，并利用外部知识进一步纠正幻觉预测。我们构建了一个新的数据集 WikiFactSum 来评估 SlotSum 的性能。实验结果表明，SlotSum 可以通过可靠的外部知识生成更加真实的摘要。]]></description>
      <guid>https://arxiv.org/abs/2402.18873</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>主成分分析作为贝叶斯系统语言重建的健全性检查</title>
      <link>https://arxiv.org/abs/2402.18877</link>
      <description><![CDATA[arXiv:2402.18877v1 公告类型：新
摘要：重建语言进化史的贝叶斯方法依赖于树模型，该模型假设这些语言源自共同的祖先，并随着时间的推移而经历了修改。然而，由于接触等因素，这一假设可能会在不同程度上被违反。了解这一假设被违反的程度对于验证系统语言推理的准确性至关重要。在本文中，我们提出了一种简单的健全性检查：将重建的树投影到主成分分析生成的空间上。通过使用合成数据和真实数据，我们证明了我们的方法可以有效地可视化异常，特别是慢跑形式的异常。]]></description>
      <guid>https://arxiv.org/abs/2402.18877</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>用非结构化事实更新语言模型：走向实用知识编辑</title>
      <link>https://arxiv.org/abs/2402.18909</link>
      <description><![CDATA[arXiv:2402.18909v1 公告类型：新
摘要：知识编辑旨在将知识更新注入语言模型中，以保持其正确性和最新性。然而，其当前的评估策略明显不切实际：它们仅使用精心策划的结构化事实（具有主题、关系和对象的三元组）进行更新，而现实世界的知识更新通常出现在新闻文章等非结构化文本中。在本文中，我们提出了一个新的基准：非结构化知识编辑（UKE）。它直接使用非结构化文本作为知识更新（称为非结构化事实）来评估编辑性能。因此，UKE避免了结构化事实的费力构建，并实现了高效且响应迅速的知识编辑，成为更实用的基准。我们对新建的数据集进行了广泛的实验，并证明 UKE 对最先进的知识编辑方法提出了重大挑战，导致其关键性能下降。我们进一步表明，即使我们提取三元组作为结构化事实，这一挑战仍然存在。我们的分析揭示了推动 UKE 未来研究进行更实用的知识编辑的关键见解。]]></description>
      <guid>https://arxiv.org/abs/2402.18909</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:48 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型如何处理多语言？</title>
      <link>https://arxiv.org/abs/2402.18815</link>
      <description><![CDATA[arXiv:2402.18815v1 公告类型：新
摘要：大型语言模型（LLM）在多种语言中表现出了卓越的性能。在这项工作中，我们深入研究了这样一个问题：法学硕士如何处理多语言？我们引入了一个描述法学硕士对多语言输入的处理的框架：在前几层中，法学硕士理解问题，将多语言输入转换为英语，以促进任务解决阶段。在中间层，法学硕士分别利用自注意力和前馈结构，通过用英语思考并结合多语言知识来获取事实内容来解决问题。在最后几层中，法学硕士生成与查询的原始语言一致的响应。此外，我们还研究了处理某种语言时语言特异性神经元的存在。为了检测输入语言激活的神经元，即使没有标签，我们创新地设计了一种并行语言特定神经元检测（$\texttt{PLND}$）方法，该方法可以有效测量处理多语言输入时神经元的重要性。通过停用不同层和结构的神经元进行综合消融分析，我们验证了我们提出的框架。此外，我们证明我们可以利用这样的框架以更少的训练量有效地增强多语言能力。]]></description>
      <guid>https://arxiv.org/abs/2402.18815</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>利用局部层次结构和对抗性训练进行层次文本分类</title>
      <link>https://arxiv.org/abs/2402.18825</link>
      <description><![CDATA[arXiv:2402.18825v1 公告类型：新
摘要：层次文本分类（HTC）由于其复杂的分类结构，是多标签分类的一个具有挑战性的子任务。几乎所有最近的 HTC 工作都关注标签的结构，但忽略了根据每个输入文本的真实标签的子结构，其中包含丰富的标签共现信息。在这项工作中，我们引入了带有对抗性框架的本地层次结构。我们提出了一个 HiAdv 框架，它可以适应几乎所有的 HTC 模型，并以局部层次结构作为辅助信息对其进行优化。我们对两种典型的 HTC 模型进行了测试，发现 HiAdv 在所有场景下都有效，并且擅长处理复杂的分类层次结构。进一步的实验表明，我们框架的提升确实来自局部层次结构，并且局部层次结构对于训练数据不足的稀有类是有益的。]]></description>
      <guid>https://arxiv.org/abs/2402.18825</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>词序什么时候重要，什么时候不重要？</title>
      <link>https://arxiv.org/abs/2402.18838</link>
      <description><![CDATA[arXiv:2402.18838v1 公告类型：新
摘要：语言模型（LM）可能对自然语言理解（NLU）任务中的词序变化不敏感。在本文中，我们提出语言冗余可以解释这种现象，即词序和其他语言线索（例如格标记）提供了重叠的冗余信息。我们的假设是，当顺序提供冗余信息时，模型表现出对词序不敏感，并且不敏感的程度因任务而异。我们使用未打乱的句子和打乱的句子之间的互信息 (MI) 来量化词序的信息量。我们的结果表明，词序信息越少，模型的预测在未打乱的句子和打乱的句子之间越一致。我们还发现，不同任务的效果各不相同：对于某些任务，如 SST-2，即使 Pointwise-MI (PMI) 发生变化，LM 的预测也几乎总是与原始任务一致，而对于其他任务，如 RTE，一致性当 PMI 较低时，接近随机，即词序非常重要。]]></description>
      <guid>https://arxiv.org/abs/2402.18838</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:47 GMT</pubDate>
    </item>
    <item>
      <title>使用开放解码器 Gerv\'asio PT* 推进葡萄牙语生成人工智能</title>
      <link>https://arxiv.org/abs/2402.18766</link>
      <description><![CDATA[arXiv:2402.18766v1 公告类型：新
摘要：为了推进葡萄牙语的神经解码，在本文中，我们提出了一种完全开放的基于 Transformer 的指令调整解码器模型，该模型在这方面树立了新的技术水平。为了开发这个解码器（我们将其命名为 Gerv\&#39;asio PT*），我们使用强大的 LLaMA~2 7B 模型作为起点，并通过对语言资源（包括准备的新葡萄牙语指令数据集）进行额外训练来进一步改进它为此目的，本文也做出了贡献。 Gerv\&#39;asio 的所有版本都是开源的，并在开放许可下免费分发，包括用于研究或商业用途，并且可以在消费级硬件上运行，从而寻求为语言研究和创新的进步做出贡献葡萄牙语技术。]]></description>
      <guid>https://arxiv.org/abs/2402.18766</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>论大语言模型角色扮演中的决策能力</title>
      <link>https://arxiv.org/abs/2402.18807</link>
      <description><![CDATA[arXiv:2402.18807v1 公告类型：新
摘要：大型语言模型（LLM）现在越来越多地用于角色扮演任务，特别是主要通过角色扮演提示来模仿特定领域的专家。在现实场景中进行交互时，角色的决策能力会显着影响其行为模式。在本文中，我们集中评估法学硕士在角色扮演后的决策能力，从而验证角色扮演的有效性。我们的目标是提供衡量标准和指导，以增强法学硕士在角色扮演任务中的决策能力。具体来说，我们首先使用 LLM 生成与代表人群细分的迈尔斯-布里格斯类型指标（缩写为 MBTI）的 16 种性格类型相对应的虚拟角色描述。然后，我们设计了具体的量化操作，从适应性、探索性、探索性权衡能力、推理能力和安全性四个方面来评估LLM角色扮演后的决策能力。最后，我们通过GPT-4分析决策绩效与相应MBTI类型之间的关联。大量实验表明，不同角色决策能力的四个方面存在稳定差异，这表明决策能力与法学硕士所模拟的角色之间存在强大的相关性。这些结果强调，法学硕士可以有效地扮演不同的角色，同时体现其真正的社会学特征。]]></description>
      <guid>https://arxiv.org/abs/2402.18807</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:46 GMT</pubDate>
    </item>
    <item>
      <title>学习以自然语言格式压缩提示</title>
      <link>https://arxiv.org/abs/2402.18700</link>
      <description><![CDATA[arXiv:2402.18700v1 公告类型：新
摘要： 大型语言模型（LLM）擅长处理多种自然语言处理任务，但其能力受到长上下文、推理速度慢和计算结果成本高等性能较差的限制。部署具有精确且信息丰富的上下文的法学硕士可以帮助用户更有效、更经济地处理大规模数据集。现有的工作依赖于将长提示上下文压缩为软提示。然而，软提示压缩在不同 LLM 之间的可转移性方面遇到了限制，尤其是基于 API 的 LLM。为此，这项工作旨在以具有LLM可转移性的自然语言形式压缩冗长的提示。这带来了两个挑战：(i) 自然语言 (NL) 提示与反向传播不兼容，(ii) NL 提示在施加长度约束方面缺乏灵活性。在这项工作中，我们提出了一种自然语言提示封装（Nano-Capsulator）框架，将原始提示压缩为NL格式的胶囊提示，同时保持提示的实用性和可转移性。具体来说，为了解决第一个挑战，纳米胶囊通过奖励函数进行优化，该函数与所提出的保留损失的语义相互作用。为了解决第二个问题，纳米胶囊通过具有长度约束的奖励函数进行优化。实验结果表明，Capsule Prompt 可以减少原始长度的 81.4%，将推理延迟降低高达 4.5 倍，并节省 80.1% 的预算开销，同时提供跨不同法学硕士和不同数据集的可转移性。]]></description>
      <guid>https://arxiv.org/abs/2402.18700</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>微调机器翻译指标在未知领域的挣扎</title>
      <link>https://arxiv.org/abs/2402.18747</link>
      <description><![CDATA[arXiv:2402.18747v1 公告类型：新
摘要：我们引入了一个新的、广泛的多维质量度量（MQM）注释数据集，涵盖生物医学领域的 11 个语言对。我们使用该数据集来研究根据人类生成的 MT 质量判断进行微调的机器翻译 (MT) 指标对于训练和推理之间的领域转换是否具有鲁棒性。我们发现，相对于依赖于表面形式的指标以及未根据 MT 质量判断进行微调的预训练指标，经过微调的指标在看不见的领域场景中表现出显着的性能下降。]]></description>
      <guid>https://arxiv.org/abs/2402.18747</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>比较摘要模型需要多少注释？</title>
      <link>https://arxiv.org/abs/2402.18756</link>
      <description><![CDATA[arXiv:2402.18756v1 公告类型：新
摘要：现代指令调整模型在摘要等文本生成任务中已经变得非常有能力，并且预计将稳步发布。在实践中，人们现在可能希望在应用于新领域或目的时，以最小的努力自信地选择性能最佳的摘要模型。在这项工作中，我们根据经验研究了在新闻摘要背景下选择首选模型所需的测试样本量。实证结果表明，自动评估和人工评估的比较评估快速收敛，并且对 100 个以下示例中出现的系统有明显的偏好。人类偏好数据使我们能够量化自动评分在各种下游汇总任务中重现偏好排名的效果。我们发现，虽然自动指标在较小的样本量下保持稳定，但只有一些自动指标能够根据人类偏好适度预测模型获胜率。]]></description>
      <guid>https://arxiv.org/abs/2402.18756</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:45 GMT</pubDate>
    </item>
    <item>
      <title>FOFO：评估法学硕士格式遵循能力的基准</title>
      <link>https://arxiv.org/abs/2402.18667</link>
      <description><![CDATA[arXiv:2402.18667v1 公告类型：新
摘要：本文介绍了 FoFo，这是一个用于评估大型语言模型 (LLM) 遵循复杂的、特定领域格式的能力的开创性基准，对于其作为人工智能代理的应用而言，这是一项至关重要但尚未得到充分检验的能力。尽管法学硕士取得了进步，但现有的基准未能充分评估他们遵循格式的熟练程度。 FoFo 通过人工智能与人类协作方法开发的各种真实世界格式和指令填补了这一空白。我们对开源（例如，Llama 2、WizardLM）和闭源（例如，GPT-4、PALM2、Gemini）法学硕士的评估强调了三个关键发现：开源模型在格式遵守方面明显落后于闭源模型;法学硕士的格式遵循性能与其内容生成质量无关；法学硕士的格式熟练程度因不同领域而异。这些见解表明需要对格式遵循技能进行专门调整，并强调 FoFo 在指导选择特定领域 AI 代理方面的作用。 FoFo 发布于 https://github.com/SalesforceAIResearch/FoFo。]]></description>
      <guid>https://arxiv.org/abs/2402.18667</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>简单的线性注意力语言模型平衡召回率与吞吐量的权衡</title>
      <link>https://arxiv.org/abs/2402.18668</link>
      <description><![CDATA[arXiv:2402.18668v1 公告类型：新
摘要：最近的工作表明，基于注意力的语言模型在召回方面表现出色，能够根据先前在上下文中看到的标记来生成代词。然而，基于注意力的模型的效率在推理过程中因 KV 缓存的大量内存消耗而受到瓶颈。在这项工作中，我们探索是否可以在不影响召回率的情况下提高语言模型的效率（例如通过减少内存消耗）。通过将实验和理论应用于广泛的架构，我们确定了模型状态大小和召回能力之间的关键权衡。我们证明了注意力的有效替代方案（例如 H3、Mamba、RWKV）可以维持固定大小的循环状态，但在回忆方面却很困难。我们提出了一种结合线性和滑动窗口注意力的简单架构。通过改变 BASED 窗口大小和线性注意力特征维度，我们可以调整状态大小并遍历回忆-记忆权衡曲线的帕累托前沿，在一端恢复注意力的全部质量，在一端恢复注意力替代品的小状态大小其他。我们训练了高达 1.3b 个参数的语言模型，并表明 BASED 与困惑度方面最强的次二次模型（例如 Mamba）相匹配，并且在现实世界的回忆密集型任务中比它们高出 6.22 个准确度点。线性注意力的实现通常比优化的标准注意力实现效率低。为了使 BASED 具有竞争力，我们开发了 IO 感知算法，当使用 1.3b 参数模型生成 1024 个令牌时，该算法的语言生成吞吐量比 FlashAttention-2 高 24 倍。这项工作的代码位于：https://github.com/HazyResearch/based。]]></description>
      <guid>https://arxiv.org/abs/2402.18668</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>RORA：稳健的自由文本基本原理评估</title>
      <link>https://arxiv.org/abs/2402.18678</link>
      <description><![CDATA[arXiv:2402.18678v1 公告类型：新
摘要：自由文本原理在可解释的 NLP 中发挥着关键作用，弥合了模型决策背后的知识和推理差距。然而，由于潜在推理路径的多样性以及相应缺乏明确的基本事实，它们的评估仍然是一个挑战。现有的评估指标依赖于基本原理支持目标标签的程度，但我们发现这些指标在评估无意中泄漏标签的基本原理方面存在不足。为了解决这个问题，我们提出了 RORA，一种针对标签泄漏的鲁棒自由文本基本原理评估。 RORA 量化了理由所提供的新信息，以证明标签的合理性。这是通过使用一个预测族来评估条件 V 信息 \citep{hewitt-etal-2021-conditional} 来实现的，该预测族对小模型可以利用的泄漏特征具有鲁棒性。 RORA 在评估人工编写、合成或模型生成的基本原理方面始终优于现有方法，特别是证明了针对标签泄漏的稳健性。我们还表明，RORA 与人类判断非常一致，可以跨不同的自由文本原理提供更可靠、更准确的测量。]]></description>
      <guid>https://arxiv.org/abs/2402.18678</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:44 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型和游戏：调查和路线图</title>
      <link>https://arxiv.org/abs/2402.18659</link>
      <description><![CDATA[arXiv:2402.18659v1 公告类型：新
摘要：近年来，大型语言模型（LLM）的研究呈爆炸式增长，公众对该主题的参与也随之增加。虽然法学硕士最初是自然语言处理中的一个利基领域，但它在包括游戏在内的广泛应用和领域中表现出了巨大的潜力。本文调查了法学硕士在游戏中的各种应用的最新技术水平，并确定了法学硕士在游戏中可以扮演的不同角色。重要的是，我们讨论了未充分探索的领域和未来在游戏中使用法学硕士的有希望的方向，并协调了法学硕士在游戏领域的潜力和局限性。作为法学硕士和游戏交叉领域的第一份全面调查和路线图，我们希望本文能够成为这个令人兴奋的新领域突破性研究和创新的基础。]]></description>
      <guid>https://arxiv.org/abs/2402.18659</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:43 GMT</pubDate>
    </item>
    </channel>
</rss>