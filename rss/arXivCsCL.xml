<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 19 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>增量序列标记：两个转变的故事</title>
      <link>https://arxiv.org/abs/2402.10447</link>
      <description><![CDATA[arXiv:2402.10447v1 公告类型：新
摘要：增量序列标记任务涉及随着时间的推移不断学习新类别，同时保留先前类别的知识。我们的调查发现了两个重要的语义转变：E2O（模型将旧实体错误标记为非实体）和 O2E（模型将非实体或旧实体标记为新实体）。以往的研究主要集中在解决E2O问题，忽视了O2E问题。这种疏忽导致模型在学习过程中倾向于将新数据样本分类为属于新类别。为了应对这些挑战，我们提出了一个新颖的框架，即无语义转换的增量顺序标签（IS3）。受已识别的语义转变（E2O 和 O2E）的推动，IS3 旨在减轻模型中的灾难性遗忘。对于E2O问题，我们使用知识蒸馏来保持模型对旧实体的判别能力。同时，为了解决 O2E 问题，我们通过去偏损失和优化级别来减轻模型对新实体的偏差。我们在具有不同增量设置的三个数据集上进行的实验评估表明，与之前最先进的方法相比，IS3 具有显着的优越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.10447</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:15 GMT</pubDate>
    </item>
    <item>
      <title>较小的语言模型能够为较大的语言模型选择指令调整训练数据</title>
      <link>https://arxiv.org/abs/2402.10430</link>
      <description><![CDATA[arXiv:2402.10430v1 公告类型：新
摘要：指令调优语言模型已成为使其通用的关键步骤。通常，此过程涉及对大型数据集的大量训练，从而产生高昂的训练成本。在本文中，我们介绍了一种基于样本学习百分比的新型训练数据选择。我们断言，当前的语言模型具有自主选择高质量训练数据的能力，与整个数据集上的训练相比，可以获得可比或更高的性能。我们的实验涵盖了不同尺寸的模型，结果表明该特征适用于尺寸从 1B（小）到 13B（大）的模型。此外，我们证明了一个有趣的发现，即数据硬度在模型大小之间转移，较小的 350M 模型可以有效地为较大的 13B 模型提供具有硬样本的高质量训练数据，从而产生与在完整数据集上进行训练。我们的论文利用大小高达 13B 的开源 OPT 和 Llama-2 模型、两个公开可用的指令调整训练数据集并由自动指标和人工进行评估，介绍了一种新的训练数据选择方法，展示了一种更有效的替代方案。]]></description>
      <guid>https://arxiv.org/abs/2402.10430</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:14 GMT</pubDate>
    </item>
    <item>
      <title>我不是他们：大型语言模型中的流动身份和持续的外群体偏见</title>
      <link>https://arxiv.org/abs/2402.10436</link>
      <description><![CDATA[arXiv:2402.10436v1 公告类型：新
摘要：我们探讨了 ChatGPT 中三种西方语言（即英语、德语和法语）和三种东方语言（即汉语、日语和韩语）中的文化偏见——个人主义与集体主义。当ChatGPT采用西方语言中的个人主义角色时，其集体主义得分（即外群体价值观）表现出更加消极的趋势，超过了其对个人主义（即内群体价值观）的积极取向。相反，当在东方语言中将集体主义角色分配给 ChatGPT 时，会出现类似的模式，与集体主义（即内群体价值观）相比，对个人主义（即外群体价值观）有更多负面反应。结果表明，当充满特定的社会身份时，ChatGPT 能够辨别内群体和外群体，拥抱内群体价值观，同时避开外群体价值观。值得注意的是，对外群体的消极情绪超过了对内群体的积极情绪，从而产生了偏见和歧视。该实验在政治领域得到了重复，结果仍然一致。此外，这种复制揭示了大型语言模型（LLM）中固有的民主偏见，与早期的发现相一致，并为通过即时工程减轻这种偏见提供了完整的见解。使用不同的超参数和角色设置方法（有或没有社会身份标签）在其他流行的语言模型中进行了广泛的稳健性检查。]]></description>
      <guid>https://arxiv.org/abs/2402.10436</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:14 GMT</pubDate>
    </item>
    <item>
      <title>使用 Pelican Soup 框架理解情境学习</title>
      <link>https://arxiv.org/abs/2402.10424</link>
      <description><![CDATA[arXiv:2402.10424v1 公告类型：新
摘要：许多现有的自然语言处理上下文学习理论分析都是基于潜变量模型，这在理论与实践之间留下了差距。我们的目标是通过提出一个理论框架——鹈鹕汤框架来缩小这些差距。在这个框架中，我们引入了（1）常识知识库的概念，（2）自然语言分类任务的一般形式主义，以及（3）意义关联的概念。在此框架下，我们可以为上下文学习建立 $\mathcal{O}(1/T)$ 损失界限，其中 $T$ 是演示中示例标签对的数量。与以前的作品相比，我们的界限反映了言语器选择的效果和指令调整的效果。 \textit{原子概念} 的附加概念使我们的框架能够解释语言模型训练数据中未见的任务的泛化。最后，我们提出了一个玩具设置 Calcutec 和一个数字加法任务，该任务模仿模型执行上下文学习所需克服的分布变化类型。我们还在现实世界的 NLP 任务中尝试使用 GPT2-Large。我们的实证结果证明了我们的框架在解释情境学习方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.10424</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>DELL：为基于 LLM 的错误信息检测生成反应和解释</title>
      <link>https://arxiv.org/abs/2402.10426</link>
      <description><![CDATA[arXiv:2402.10426v1 公告类型：新
摘要：大型语言模型受到事实性和幻觉方面的挑战，无法直接用于判断新闻文章的真实性，而事实准确性至关重要。在这项工作中，我们建议 DELL 确定错误信息检测的三个关键阶段，其中法学硕士可以作为管道的一部分纳入其中：1）法学硕士可以 \emph{生成新闻反应} 来代表不同的观点并模拟用户新闻交互网络； 2）法学硕士可以为代理任务（例如情绪、立场）\emph{生成解释}，以丰富新闻文章的上下文，并培养专门从事新闻理解各个方面的专家； 3）法学硕士可以\emph{合并特定于任务的专家}并通过合并不同专家的预测和置信度分数来提供总体预测。对三个法学硕士的七个数据集进行的广泛实验表明，DELL 在宏观 f1 分数方面比最先进的基线高出 16.8%。进一步的分析表明，生成的反应和解释对于错误信息检测非常有帮助，而我们提出的法学硕士指导的专家合并有助于产生更好校准的预测。]]></description>
      <guid>https://arxiv.org/abs/2402.10426</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>评估和改进口语理解的持续学习</title>
      <link>https://arxiv.org/abs/2402.10427</link>
      <description><![CDATA[arXiv:2402.10427v1 公告类型：新
摘要：持续学习已成为各种任务中日益重要的挑战，包括口语理解（SLU）。 SLU 的目标是有效处理新概念的出现和不断变化的环境。持续学习算法的评估通常涉及评估模型的稳定性、可塑性和通用性，作为标准的基本方面。然而，现有的持续学习指标主要只关注其中的一两个属性。他们忽略了所有任务的整体性能，并且没有充分理清模型内可塑性与稳定性/泛化性的权衡。在这项工作中，我们提出了一种评估方法，对持续学习中的稳定性、可塑性和泛化性提供统一的评估。通过采用所提出的指标，我们演示了如何引入各种知识蒸馏来改进 SLU 模型这三个属性的不同方面。我们进一步表明，我们提出的指标在捕获持续学习中任务排序的影响方面更加敏感，使其更适合实际用例场景。]]></description>
      <guid>https://arxiv.org/abs/2402.10427</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>通过图表示学习了解有关大型语言模型的调查论文分类</title>
      <link>https://arxiv.org/abs/2402.10409</link>
      <description><![CDATA[arXiv:2402.10409v1 公告类型：新
摘要：随着大型语言模型（LLM）的新研究不断进行，很难跟上新的研究和模型的步伐。为了帮助研究人员综合新研究，许多人撰写了调查论文，但即便如此，这些论文也变得数量众多。在本文中，我们开发了一种自动将调查论文分配给分类的方法。我们收集了 144 篇 LLM 调查论文的元数据，并探索了三种范式来对分类法中的论文进行分类。我们的工作表明，利用同类别图上的图结构信息可以在两种范式中显着优于语言模型；使用 LLM 进行预训练语言模型的微调和零样本/少样本分类。我们发现我们的模型超越了人类的平均识别水平，并且使用较小模型（例如本研究中的 GCN）生成的弱标签来微调 LLM 可能比使用地面实况标签更有效，从而揭示了弱标签的潜力-分类学分类任务中的强泛化。]]></description>
      <guid>https://arxiv.org/abs/2402.10409</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>通过专业知识加权测量和减少没有金标准答案的法学硕士幻觉</title>
      <link>https://arxiv.org/abs/2402.10412</link>
      <description><![CDATA[arXiv:2402.10412v1 公告类型：新
摘要：法学硕士幻觉，即产生事实上不正确但看似令人信服的答案，目前是法学硕士可信度和可靠性的主要威胁。解决这个复杂问题的第一步是对其进行测量。然而，现有的幻觉指标需要有一个包含黄金标准答案的基准数据集，即由人类编写的“最佳”或“正确”答案。这种要求使得幻觉测量成本高昂并且容易出现人为错误。在这项工作中，我们提出了通过加权法学硕士（FEWL）进行事实性评估，这是第一个专门针对缺乏黄金标准答案的情况而设计的幻觉指标。 FEWL 利用现成的法学硕士的答案作为黄金标准答案的代理。关键的挑战是如何巧妙地量化参考法学硕士的专业知识。我们证明 FEWL 具有一定的理论保证，并通过经验证明它比单纯使用参考法学硕士能提供更准确的幻觉测量。我们还展示了如何利用 FEWL 通过上下文学习和监督微调来减少幻觉。最后，我们构建了一个大规模的基准数据集来促进 LLM 幻觉研究。]]></description>
      <guid>https://arxiv.org/abs/2402.10412</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>突破零样本端到端语音翻译的极限</title>
      <link>https://arxiv.org/abs/2402.10422</link>
      <description><![CDATA[arXiv:2402.10422v1 公告类型：新
摘要：数据稀缺以及语音和文本模态之间的模态差距是端到端语音翻译（ST）系统的两个主要障碍，从而阻碍了其性能。之前的工作试图通过利用外部机器翻译数据和优化距离指标来缓解这些挑战，从而使语音文本表示更接近。然而，要获得有竞争力的结果通常需要一些 ST 数据。因此，我们引入了 ZeroSwot，这是一种零样本 ST 方法，可以在没有任何配对 ST 数据的情况下弥补模态差距。利用新颖的 CTC 压缩和最佳传输，我们仅使用 ASR 数据训练语音编码器，以与大规模多语言 MT 模型的表示空间保持一致。语音编码器在推理时与 MT 模型无缝集成，可在 MT 模型支持的所有语言中直接将语音翻译为文本。我们的实验表明，我们可以在没有 ST 数据的情况下有效地缩小模态差距，而我们在 MuST-C 和 CoVoST 上的结果证明我们的方法不仅优于以前的零样本模型，而且优于监督模型，实现了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.10422</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>DataDreamer：用于合成数据生成和可重复的 LLM 工作流程的工具</title>
      <link>https://arxiv.org/abs/2402.10379</link>
      <description><![CDATA[arXiv:2402.10379v1 公告类型：新
摘要：大型语言模型（LLM）已成为 NLP 研究人员在各种任务中的主导且重要的工具。如今，许多研究人员在合成数据生成、任务评估、微调、蒸馏和其他模型在环研究工作流程中使用法学硕士。然而，使用这些模型时会出现一些挑战，这些挑战源于其规模、闭源性质以及这些新兴工作流程缺乏标准化工具。这些模型的迅速崛起和这些独特的挑战对开放科学和使用它们的工作的可重复性产生了直接的不利影响。在本文中，我们介绍了 DataDreamer，这是一个开源 Python 库，允许研究人员编写简单的代码来实现强大的 LLM 工作流程。 DataDreamer 还帮助研究人员遵循我们建议的最佳实践，以鼓励开放科学和可重复性。该库和文档可从 https://github.com/datadreamer-dev/DataDreamer 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.10379</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>逻辑链：大型语言模型的基于规则的推理</title>
      <link>https://arxiv.org/abs/2402.10400</link>
      <description><![CDATA[arXiv:2402.10400v1 公告类型：新
摘要：基于规则的推理是法律推理的基本类型，使我们能够通过准确地将规则应用于一组事实来得出结论。我们探索因果语言模型作为基于规则的推理器，特别是关于组合规则 - 由形成复杂逻辑表达式的多个元素组成的规则。关于组合规则的推理具有挑战性，因为它需要多个推理步骤，并关注元素之间的逻辑关系。我们引入了一种新的提示方法，即逻辑链，它通过分解（将元素作为独立的逻辑线程来解决）和重组（重新组合这些子答案来解析底层逻辑表达式）来引发基于规则的推理。该方法的灵感来自 IRAC（问题、规则、应用、结论）框架，这是律师使用的顺序推理方法。我们评估了八个基于规则的推理任务的逻辑链，涉及来自 LegalBench 基准的三个不同的组成规则，并证明它始终优于其他提示方法，包括使用开源和商业语言模型的思维链和自我询问。]]></description>
      <guid>https://arxiv.org/abs/2402.10400</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>名词短语中中心词的最佳位置。指示词、数词、形容词和名词的情况</title>
      <link>https://arxiv.org/abs/2402.10311</link>
      <description><![CDATA[arXiv:2402.10311v1 公告类型：新
摘要：句子的词序是由多种原则决定的。句法依存距离最小化原则与单头句法依存结构中的意外最小化（或可预测性最大化）原则相冲突：前者预测头应该放置在线性排列的中心，而后者预测：头部应放置在一端（第一个或最后一个）。一个关键问题是，意外最小化（或可预测性最大化）何时应超过句法依赖距离最小化。在单头结构的背景下，据预测，当满足两个条件时，即（a）涉及的单词较少和（b）单词较短时，这种情况更有可能发生。在这里，我们测试对由指示词、数字、形容词和名词组成的名词短语的预测。我们发现，在语言的首选顺序中，名词往往位于末尾之一，这证实了理论预测。我们还展示了反局部性效应的证据：首选顺序中的句法依赖距离比偶然预期的要长。]]></description>
      <guid>https://arxiv.org/abs/2402.10311</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>基于提示的偏差校准可实现更好的语言模型零/少样本学习</title>
      <link>https://arxiv.org/abs/2402.10353</link>
      <description><![CDATA[arXiv:2402.10353v1 公告类型：新
摘要：即时学习很容易受到预训练语言模型（LM）中存在的内在偏差的影响，导致基于即时的零/少样本学习的性能不佳。在这项工作中，我们提出了一种空输入提示方法来校准预训练 LM 中编码的内在偏差。与之前主要为了社会公平而解决内在偏差且通常涉及过多计算成本的努力不同，我们的目标是探索增强 LM 在下游零/少样本学习中的性能，同时强调内在偏差校准的效率。具体来说，我们利用 GPT-4 生成的一组不同的自动选择的无效输入来提示预先训练的 LM 进行内在偏差探测。利用偏差反映的概率分布，我们制定了偏差校准的分布差异损失，其中我们专门将 LM 的偏差参数（总参数的 $0.1\%$）更新为相等的概率分布。实验结果表明，校准促进了语言模型的公平起点，同时保留了语言建模能力。在包括情感分析和主题分类在内的广泛数据集上，我们的方法显着提高了 LM 在上下文学习和基于提示的微调方面的零/少样本学习性能（平均 $9\%$ 和 $2\分别是%$）。]]></description>
      <guid>https://arxiv.org/abs/2402.10353</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>BioMistral：医学领域开源预训练大型语言模型的集合</title>
      <link>https://arxiv.org/abs/2402.10373</link>
      <description><![CDATA[arXiv:2402.10373v1 公告类型：新
摘要：近年来，大型语言模型（LLM）表现出了显着的多功能性，在医疗保健和医学等专业领域提供了潜在的应用。尽管有各种针对健康环境量身定制的开源法学硕士，但将通用法学硕士适应医疗领域却面临着巨大的挑战。在本文中，我们介绍了 BioMistral，这是一个专为生物医学领域量身定制的开源法学硕士，利用 Mistral 作为其基础模型，并在 PubMed Central 上进行了进一步的预训练。我们根据由 10 项既定的英语医学问答 (QA) 任务组成的基准对 BioMistral 进行了全面评估。我们还探索通过量化和模型合并方法获得的轻量级模型。我们的结果证明了 BioMistral 与现有开源医疗模型相比具有卓越的性能，并且与专有模型相比具有竞争优势。最后，为了解决英语以外的数据有限的问题，并评估医学法学硕士的多语言泛化能力，我们自动将该基准翻译和评估为 7 种其他语言。这标志着医学领域法学硕士的首次大规模多语言评估。数据集、多语言评估基准、脚本以及我们实验中获得的所有模型都是免费发布的。]]></description>
      <guid>https://arxiv.org/abs/2402.10373</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>如何辨别重要紧急消息？</title>
      <link>https://arxiv.org/abs/2402.10302</link>
      <description><![CDATA[arXiv:2402.10302v1 公告类型：新
摘要：我们发现新闻聚类数据集中聚类的一个简单属性与法学硕士评估的新闻重要性和紧迫性 (IUN) 密切相关。我们在不同的新闻数据集、数据集大小、聚类算法和嵌入中验证了我们的发现。发现的相关性应该允许使用聚类（作为 LLM 的替代方案）来识别最重要的紧急新闻，或过滤掉不重要的文章。]]></description>
      <guid>https://arxiv.org/abs/2402.10302</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:09 GMT</pubDate>
    </item>
    </channel>
</rss>