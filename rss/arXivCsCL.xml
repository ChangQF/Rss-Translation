<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Thu, 10 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>减少正式上下文提取：大型公司的新提出的框架</title>
      <link>https://arxiv.org/abs/2504.06285</link>
      <description><![CDATA[ARXIV：2504.06285V1公告类型：新 
摘要：自动从自由文本中提取概念层次结构是有利的，因为手动生成通常是劳动力和资源密集型的。免费结果，概念层次结构从自由文本中学习的整个过程需要几个阶段，包括句子级文本处理，句子分裂和令牌化。 Lemmatization是在形式上下文分析（FCA）之后以得出配对的。然而，在正式背景下可能会有一些无趣和不正确的配对。可能需要一段时间才能产生正式背景；因此，缩小规模的形式环境对于消除无关紧要和不正确的配对以更快地提取概念晶格和层次结构是必需的。这项研究旨在提出一个框架，以减少从自由文本提取概念层次结构以减少正式背景的歧义的框架。我们通过使用基于WordNet方法的混合体和基于频率的技术来减少正式上下文的大小来实现这一目标。使用来自Wikipedia语料库的385个样本和建议的框架，进行了测试以检查正式上下文的减小尺寸，从而导致概念晶格和概念层次结构。在概念晶格不变的帮助下，将产生的正式上下文晶格与普通晶格进行了比较。与基本的晶格之间的同态保留了生成概念层次结构的质量的98％，而降低的概念晶格则获得了标准构件的结构连接。此外，将新框架与五种基线技术进行比较，以计算具有各种密度的随机数据集上的运行时间。研究结果表明，在各种填充比中，该方法的混合方法优于其他指示的概念晶格性能的竞争策略。]]></description>
      <guid>https://arxiv.org/abs/2504.06285</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于LLM的对话信息寻求的查询理解</title>
      <link>https://arxiv.org/abs/2504.06356</link>
      <description><![CDATA[ARXIV：2504.06356V1公告类型：新 
摘要：在会话信息寻求（CI）中的查询理解涉及通过上下文感知的互动准确解释用户意图。这包括解决歧义，精炼查询以及适应不断发展的信息需求。大型语言模型（LLM）通过解释细微的语言并动态调整来增强此过程，从而实时提高搜索的相关性和精度。在本教程中，我们探索了先进的技术，以增强基于LLM的CIS系统中的查询理解。我们深入研究了以LLM驱动的方法来开发可靠的评估指标，以评估查询多转交互作用中的查询质量，建立更多交互式系统的策略以及主动查询管理和诸如主动查询管理和查询重新重新重新重新重新重新重新制定。我们还讨论了整合LLM的关键挑战，以在对话搜索系统中查询理解并概述未来的研究方向。我们的目标是加深听众对基于LLM的对话查询理解的理解，并激发讨论以推动该领域正在进行的进步。]]></description>
      <guid>https://arxiv.org/abs/2504.06356</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>零身体问题：探测LLM感官语言的使用</title>
      <link>https://arxiv.org/abs/2504.06393</link>
      <description><![CDATA[ARXIV：2504.06393V1公告类型：新 
摘要：感官语言表达体现的体验，从口味，声音到兴奋和胃痛。该语言吸引了来自包括机器人技术，叙事学，语言学和认知科学在内的各种领域的学者。在这项工作中，我们探讨了没有体现的语言模型是否可以近似人类对体现语言的使用。我们将现有的平行人类和模型响应扩展到短篇小说提示，并通过18个流行模型产生的另外18,000个故事。我们发现，所有模型都会产生与人类对感官语言使用显着差异的故事，但是这些差异的方向在模型家族之间差异很大。也就是说，双子座模型比大多数轴的人类使用更多的感觉语言，而其余五个家庭的大多数模型的使用量要少得多。线性探针在五个型号上运行，表明它们能够识别感觉语言。但是，我们发现初步证据表明，教学调整可能会阻止使用感官语言。最后，为了支持进一步的工作，我们发布了扩展的故事数据集。]]></description>
      <guid>https://arxiv.org/abs/2504.06393</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>S'More：LLM微调剩余专家的结构混合物</title>
      <link>https://arxiv.org/abs/2504.06426</link>
      <description><![CDATA[ARXIV：2504.06426V1公告类型：新 
摘要：预先训练的大型语言模型（LLMS）提出了平衡参数效率和模型容量的双重挑战。现有的方法（例如低级适应）（LORA）是有效的，但缺乏灵活性，而专家（MOE）体系结构的混合物以更多和未充分利用的参数为代价增强了模型容量。为了解决这些局限性，我们提出了剩余专家（S&#39;More）的结构混合物，该结构混合物是一个新颖的框架，无缝地将Lora的效率整合到MOE的灵活性。具体而言，S&#39;More采用了专家权重的层次低级分解，产生了在多层结构中相互联系的不同订单的残差。通过通过残差的子树来路由输入令牌，S&#39;More通过仅实例化和组装少数低级别的矩阵来模仿许多专家的能力。我们制作了S&#39;More残留物作为一种特殊类型的图形神经网络（GNN）的层间传播，并证明在相似的参数预算下，S&#39;More通过指数顺序提高了传统MOE（或Lora混合物）的“结构性灵活性”。全面的理论分析和经验结果表明，S&#39;More实现了出色的微调性能，为有效的LLM适应提供了变革性的方法。]]></description>
      <guid>https://arxiv.org/abs/2504.06426</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AI中依赖语言的政治偏见：Chatgpt和Gemini的研究</title>
      <link>https://arxiv.org/abs/2504.06436</link>
      <description><![CDATA[ARXIV：2504.06436V1公告类型：新 
摘要：作为大型语言模型的主要例子，Chatgpt和Gemini声称提供准确，公正的信息，强调他们对政治中立性的承诺和避免对个人偏见的承诺。这项研究调查了大语言模型的政治趋势以及根据查询语言的分化的存在。为此，使用14种不同的语言对Chatgpt和Gemini进行了政治轴心测试。研究结果表明，这些大语言模型确实表现出政治倾向，这两个模型都表明了自由主义者和左派偏见。比较分析表明，与Chatgpt相比，双子座表现出更为明显的自由主义和左翼趋势。研究还发现，这些政治偏见因用于查询的语言而异。该研究深入研究了构成政治倾向和语言差异化的因素，探索了教育数据的来源和范围的差异，语言的结构和语法特征，文化和政治环境以及模型对语言特征的反应。从这个角度来看，是一种道德的角度，有人提出人工智能工具应避免断言缺乏政治倾向和中立，而是努力争取政治中立，并通过结合这些趋势来执行用户查询。]]></description>
      <guid>https://arxiv.org/abs/2504.06436</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>不要让它幻觉：通过检索提示的逻辑推理前提验证</title>
      <link>https://arxiv.org/abs/2504.06438</link>
      <description><![CDATA[ARXIV：2504.06438V1公告类型：新 
摘要：大型语言模型（LLM）显示出产生流利，上下文适当的响应的大量能力。但是，它们可以产生幻觉的输出，尤其是当用户查询包含与确定事实相矛盾的一个或多个虚假场所所述时。这样的场所可能会误导LLM，以提供伪造或误导性的细节。现有的方法包括训练，微调和推理时间技术，这些技术通常依赖于对逻辑或地址幻觉发生后的访问。这些方法往往在计算上很昂贵，需要广泛的培训数据，或者缺乏积极的机制来防止在生成前幻觉，从而限制了它们在实时应用中的效率。我们提出了一个基于检索的框架，该框架可以识别和解决生成前的错误前提。我们的方法首先将用户的查询转换为逻辑表示形式，然后应用检索功能增强的生成（RAG）来评估使用事实来源的每个前提的有效性。最后，我们将验证结果纳入LLM的提示中，以保持最终输出中的事实一致性。实验表明，这种方法有效地降低了幻觉，提高了事实的准确性，并且不需要访问模型逻辑或大规模微调。]]></description>
      <guid>https://arxiv.org/abs/2504.06438</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM可以以相反的性能模拟角色吗？反事实指令的基准</title>
      <link>https://arxiv.org/abs/2504.06460</link>
      <description><![CDATA[ARXIV：2504.06460V1公告类型：新 
摘要：大型语言模型（LLM）现在越来越广泛地用于模拟虚拟环境中的角色，利用其指导跟随能力。但是，我们发现，即使是最先进的LLM也无法以相反的表现模拟角色（例如，在教育环境中熟练水平较低的学生角色），这会损害模拟多样性并限制模拟环境的实际应用。在这项工作中，使用数学推理作为代表性方案，我们提出了第一个基准数据集，以评估LLMS以相反的性能模拟角色，这是我们认为“以下的反事实指令”的功能。我们在此任务上评估了开放式和封闭源LLMS，并发现包括OpenAi O1推理模型在内的LLM都难以按照反事实指令进行模拟反向执行角色。交叉点模拟角色的表现水平和种族人口，使效果进一步加剧。这些结果突出了反事实指导的挑战，以及需要进一步研究的必要性。]]></description>
      <guid>https://arxiv.org/abs/2504.06460</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用Distilbert和机器学习分析考生评论，以确保考试内容中的质量控制</title>
      <link>https://arxiv.org/abs/2504.06465</link>
      <description><![CDATA[ARXIV：2504.06465V1公告类型：新 
摘要：本研究使用自然语言处理（NLP）探讨以分析候选评论以识别有问题的测试项目。我们开发了验证和验证的机器学习模型，这些模型自动识别相关的负面反馈，评估合并心理测量功能的方法增强了模型性能，并将NLP-FLAGAG的项目与传统标记的项目进行了比较。结果表明，候选反馈为统计方法提供了有价值的补充信息，从而有可能提高测试有效性，同时减轻了手动审查负担。这项研究为测试组织提供了一种有效的机制，可以将直接候选经验纳入质量保证过程中。]]></description>
      <guid>https://arxiv.org/abs/2504.06465</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CDER：文档级关系提取的协作证据检索</title>
      <link>https://arxiv.org/abs/2504.06529</link>
      <description><![CDATA[ARXIV：2504.06529V1公告类型：新 
摘要：文档级关系提取（DOCRE）涉及识别文档中多个句子之间的实体之间的关系。证据句子，对于精确的实体对关系识别至关重要，增强了对基本文本细分的关注，从而提高了DOCRE的性能。但是，现有的证据检索系统通常会忽略同一文档中语义相似的实体对之间的协作性质，从而阻碍了证据检索任务的有效性。为了解决这个问题，我们提出了一个新颖的证据检索框架，即CDR。 CDER采用基于图形的注意的架构来捕获协作模式，并结合了一个动态的子结构，以在证据检索中进行额外的鲁棒性。基准DOCRE数据集的实验结果表明，CDER不仅在证据检索任务中出色，而且还提高了现有DOCRE系统的整体性能。]]></description>
      <guid>https://arxiv.org/abs/2504.06529</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Lugha-llama：调整非洲语言的大型语言模型</title>
      <link>https://arxiv.org/abs/2504.06536</link>
      <description><![CDATA[ARXIV：2504.06536V1公告类型：新 
摘要：大型语言模型（LLM）在广泛的自然语言应用中取得了令人印象深刻的结果。但是，他们经常努力识别低资源语言，特别是非洲语言，这些语言在大型培训语料库中没有很好地代表。在本文中，我们考虑如何使LLM适应低资源的非洲语言。我们发现，将非洲语言的精选数据与高质量的英语教育文本相结合，从而实现了培训组合，从而大大提高了模型在这些语言上的表现。在具有挑战性的irokobench数据集上，我们的模型始终在大小相似的基准中取得最佳性能，尤其是在知识密集的多项选择问题上（Afrimmlu）。此外，在回答基准AFRIQA的跨语性问题上，我们的模型的表现优于基本模型超过10％。为了更好地了解培训期间英语数据的作用，我们将200m令牌的子集转化为斯瓦希里语语言，并进行分析，该分析表明这些数据的内容主要负责强大的性能。我们发布我们的模型和数据，以鼓励对非洲语言的未来研究。]]></description>
      <guid>https://arxiv.org/abs/2504.06536</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针线素：探索大语模型的长期文化能力</title>
      <link>https://arxiv.org/abs/2504.06560</link>
      <description><![CDATA[ARXIV：2504.06560V1公告类型：新 
摘要：处理结构化的表格数据，尤其是冗长的表格，构成了大型语言模型（LLMS）的基本而又具有挑战性的任务。但是，现有的长篇小写基准主要集中于非结构化文本，忽略了长长而复杂的结构化表的挑战。为了解决这一差距，我们引入了针刺（NIAT），这是一个新的任务，将每个表单元视为“针”，并要求模型在不同的查询下提取目标细胞。主流LLM在此基准测试中的评估结果表明，它们缺乏强大的长桌理解，通常依靠表面相关性或快捷方式来理解复杂的桌子理解任务，从而揭示了处理复杂的表格数据的显着限制。为此，我们提出了一种数据综合方法，以增强模型的长桌理解能力。实验结果表明，我们的合成训练数据显着提高了LLM在NIAT任务上的性能，表现优于长篇文化LLM和长桌子的方法。这项工作推进了LLMS真正的长期结构桌子理解能力的评估，并为长篇文章和表格理解应用程序的进步铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2504.06560</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Fuserl：非均质模型融合的密集偏好优化</title>
      <link>https://arxiv.org/abs/2504.06562</link>
      <description><![CDATA[ARXIV：2504.06562V1公告类型：新 
摘要：异质模型融合通过整合多种结构多样性模型的知识和能力来增强LLM的性能。但是，现有方法通常仅依赖于从源模型中选择每个提示的最佳输出，由于源知识有限并导致稀疏优化信号，从源模型中的每个提示都不足。为了解决这一限制，我们提出了Fuserl，这是一种新型的两阶段框架，包括Feseft和FusePo，以最大程度地利用源LLM。 Feseft通过在每个提示的各种输出上通过加权监督微调（SFT）整合异质源模型的强度来建立强大的初始化。 FUSEPO根据多个源模型的输出来优化加权偏好，以实现出色的对齐性能。广泛的实验证明了我们在包括Rloo，DPO和Simpo在内的各种偏好比对方法中的框架的有效性。我们使用Llama-3.1-8B教学作为目标模型，我们的方法在Alpacaeval-2和Arena-Hard-Hard-Hard基准测试中实现了8B LLM的最新性能。进一步的分析表明，Feseft将训练过程正规化以减少过度拟合，而Fusepo引入了密集和多样的信号以进行优化。]]></description>
      <guid>https://arxiv.org/abs/2504.06562</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>推理模型是否显示出更好的语言校准？</title>
      <link>https://arxiv.org/abs/2504.06564</link>
      <description><![CDATA[ARXIV：2504.06564V1公告类型：新 
摘要：大型推理模型（LRMS）最近通过利用增加的测试时间计算并表现出类似于类似人类的审议的行为，在复杂的推理方面表现出了令人印象深刻的能力。尽管有这些进步，但与指导调节的对应物相比，LRM是否得到更好的校准（尤其是其口头上的信心）仍然是一个悬而未决的问题。在本文中，我们研究了通过在长期推理轨迹（此后SFT推理模型）和基于结果的推理学习（此后为RL推理模型）跨不同领域培训的LRMS的校准特性。我们的发现表明，在精确性和置信度校准中，LRMS在复杂的推理任务上的表现明显优于指令调节的模型。相比之下，我们发现了尤其是事实领域的令人惊讶的趋势。在事实任务上，虽然DeepSeek-R1显示出强烈的校准行为，但较小的QWQ-32B对指导模型没有任何改进。此外，与指导模型相比，SFT推理模型显示出更差的校准（更大的自信）。我们的结果提供了证据，证明了面向推理的RL培训在提高LLMS产生值得信赖的自我意识产出能力方面的潜在关键作用。]]></description>
      <guid>https://arxiv.org/abs/2504.06564</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用幽默绕过LLMS中的安全护栏</title>
      <link>https://arxiv.org/abs/2504.06577</link>
      <description><![CDATA[ARXIV：2504.06577V1公告类型：新 
摘要：在本文中，我们表明可以通过幽默的提示（包括不安全的请求）绕过大语言模型（LLMS）的安全护栏。特别是，我们的方法不会编辑不安全的请求并遵循固定的模板 - 实现很容易，并且不需要其他LLM来制作提示。广泛的实验显示了我们在不同LLM的方法的有效性。我们还表明，消除和增加更多幽默的方法都可以降低其有效性 - 过度的幽默可能会使LLM不满意其不安全的要求。因此，我们认为，当关注不安全的请求与幽默的存在之间存在适当的平衡时，LLM越狱就会发生。]]></description>
      <guid>https://arxiv.org/abs/2504.06577</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自动化业务流程分析：基于LLM的价值评估方法</title>
      <link>https://arxiv.org/abs/2504.06600</link>
      <description><![CDATA[ARXIV：2504.06600V1公告类型：新 
摘要：业务流程是组织运营的基础，但是由于手动过程分析的时间耗尽性质，它们的优化仍然具有挑战性。我们的论文利用大型语言模型（LLMS）来自动化增值分析，这是一种定性过程分析技术，旨在确定流程中无法提供价值的步骤。迄今为止，这项技术主要是手动，耗时和主观的。我们的方法提供了一种更具原则性的方法，该方法分为两个阶段：首先将高级活动分解为详细的步骤，以实现颗粒状分析，其次，进行增值分析以根据精益原则对每个步骤进行分类。这种方法可以系统地识别废物，同时保持定性分析所需的语义理解。我们使用50个业务流程模型来开发我们的方法，并为其收集和发布手动基础真实标签。我们的评估，将零击基线与更结构化的提示进行比较，揭示了（a）结构化提示和（b）这两个任务的有希望的表现的一致好处。我们讨论了LLM在定性过程分析中增强人类专业知识的潜力，同时减少手动方法固有的时间和主观性。]]></description>
      <guid>https://arxiv.org/abs/2504.06600</guid>
      <pubDate>Thu, 10 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>