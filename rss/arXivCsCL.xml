<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的计算机科学 - 计算和语言 (cs.CL) 更新</description>
    <lastBuildDate>Mon, 08 Jan 2024 06:18:47 GMT</lastBuildDate>
    <item>
      <title>通过单词混淆网络的上下文学习实现 ASR 强大的口语理解。 （arXiv：2401.02921v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02921</link>
      <description><![CDATA[在口语理解 (SLU) 领域，许多自然语言
语言理解（NLU）方法已经通过提供大量的
使用转录语音而不是传统书面语言模型（LLM）
文本。在现实场景中，在输入法学硕士之前，自动语音
识别（ASR）系统生成输出转录本假设，其中
固有错误可能会降低后续 SLU 任务的性能。这里介绍一个方法
利用 ASR 系统的晶格输出，而不是仅仅依赖于
顶级假设，旨在封装语音歧义并增强 SLU
结果。我们的情境学习实验，涵盖口语问题
回答和意图分类，强调了法学硕士对噪音的适应能力
借助来自格子的单词混淆网络的语音记录，
弥合使用顶级 ASR 假设和使用 ASR 假设之间的 SLU 性能差距
甲骨文上限。此外，我们还深入研究了法学硕士对各种变化的稳健性
ASR 表现条件并仔细审视情境学习的各个方面
事实证明这是最有影响力的。
]]></description>
      <guid>http://arxiv.org/abs/2401.02921</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:47 GMT</pubDate>
    </item>
    <item>
      <title>Bode 简介：针对葡萄牙语提示任务的微调大型语言模型。 （arXiv：2401.02909v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02909</link>
      <description><![CDATA[大型语言模型 (LLM) 越来越多地为自然带来进步
语言处理。然而，资源匮乏的语言，缺乏广泛的
各种 NLP 任务的数据集中或现有数据集的突出位置
不那么重要，例如葡萄牙语，已经获得了一些好处
法学硕士，但程度不同。接受过多语言数据集培训的法学硕士
通常很难令人满意地回答葡萄牙语的提示，
例如，在他们的回答中呈现语码转换。这项工作提出
一个基于 LLaMA 2 的微调葡萄牙语提示模型，名为 Bode in Two
版本：7B 和 13B。我们评估该模型的性能
使用零样本方法和上下文学习进行分类任务，以及
与其他法学硕士进行比较。我们的主要贡献是带来法学硕士
葡萄牙语成绩令人满意，并提供了一个模型
免费用于研究或商业目的。
]]></description>
      <guid>http://arxiv.org/abs/2401.02909</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:46 GMT</pubDate>
    </item>
    <item>
      <title>Pheme：高效的对话式语音生成。 （arXiv：2401.02839v1 [eess.AS]）</title>
      <link>http://arxiv.org/abs/2401.02839</link>
      <description><![CDATA[近年来，语音生成取得了显着的进步，现在
实现通常虚拟的一次性生成能力
与真实的人声没有区别。将这些进步融入
使用大型语言模型生成语音可能会彻底改变广泛的领域
应用程序。然而，某些应用，例如辅助
会话系统，需要自然的会话语音生成
实时高效运行的工具。目前最先进的
VALL-E 和 SoundStorm 等模型由分层神经音频编解码器提供支持，
需要大量的神经组件和大量的训练数据才能正常工作。在
相比之下，MQTTS 旨在构建更紧凑的会话 TTS 模型，同时
利用较小规模的现实生活对话语音数据。然而，
其自回归性质会产生较高的推理延迟，从而限制了其
实时使用。为了减轻当前的限制
在这项工作中，我们利用了最先进的 TTS 模型，同时利用了它们的优势
我们推出 Pheme 型号系列，1) 提供紧凑而高性能的
模型，2) 允许并行语音生成 3) 自然会话
语音，4）它可以在较小规模的会话上进行有效的训练
数据，将数据需求减少了 10 倍以上，但仍然符合
自回归 TTS 模型。我们还通过简单的师生关系展示了
蒸馏我们可以满足语音质量的显着改进
在预训练 Pheme 检查点之上的单扬声器设置，仅依赖于
由更大的教师模型生成的合成语音。音频样本和
预训练模型可在线获取。
]]></description>
      <guid>http://arxiv.org/abs/2401.02839</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>AFSPP：使用大型语言模型塑造偏好和个性的代理框架。 （arXiv：2401.02870v1 [cs.MA]）</title>
      <link>http://arxiv.org/abs/2401.02870</link>
      <description><![CDATA[大型语言模型 (LLM) 的发展引入了新的范式
用于研究人类行为仿真。最近的研究采用了
基于法学硕士的代理创建一个社会学研究环境，其中代理
表现出基于大语言未经过滤的特征的行为
楷模。然而，这些研究忽视了迭代开发
类人的环境 - 人类的偏好和个性是复杂的，由
各种因素，并会因环境和环境而不断变化
主观影响。根据这一观察，我们提出了 Agent 框架
塑造偏好和个性（AFSPP），探索多方面的
社交网络和主观意识对法学硕士代理人的影响
偏好和个性形成。通过 AFSPP，我们第一次拥有：
成功复制了人类性格的几个关键发现
实验。其他基于 AFSPP 的实验结果表明，计划
带有主观信息的制作、感官知觉和社交网络，
对偏好塑造产生最显着的影响。 AFSPP 可以
显着提高心理实验的效率和范围，
同时为值得信赖的人工智能提供有价值的见解
研究防止不良偏好和个性的策略
发展。
]]></description>
      <guid>http://arxiv.org/abs/2401.02870</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>MLLM-Protector：确保 MLLM 的安全而不影响性能。 （arXiv：2401.02906v1 [cs.CR]）</title>
      <link>http://arxiv.org/abs/2401.02906</link>
      <description><![CDATA[多模态大语言模型（MLLM）的部署带来了
独特的漏洞：容易受到视觉恶意攻击
输入。我们深入研究了保护 MLLM 免受此类威胁的新挑战
攻击。我们发现图像作为一种“外语”而不是
在对齐过程中考虑，这可能会使 MLLM 容易产生有害的
回应。不幸的是，与基于文本的离散标记不同
法学硕士，图像信号的连续性呈现出显着的一致性
挑战，这给彻底覆盖可能的场景带来了困难。
由于开源 MLLM 是
主要对有限的图像-文本对进行微调，该图像-文本对远小于
广泛的基于文本的预训练语料库，这使得 MLLM 更容易
在明确的对齐过程中灾难性地忘记了他们最初的能力
调整。为了应对这些挑战，我们推出了 MLLM-Protector，
即插即用策略结合了轻量级危害检测器和响应
解毒剂。危害检测器的作用是识别潜在有害的输出
来自 MLLM，而解毒器会纠正这些输出以确保
响应规定了安全标准。这种方法有效
减轻恶意视觉输入带来的风险，而不影响
模型的整体性能。我们的结果表明 MLLM-Protector 提供
针对 MLLM 安全性之前未解决的问题提供了一个强大的解决方案。
]]></description>
      <guid>http://arxiv.org/abs/2401.02906</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>PeFoMed：用于医学视觉问答的多模态大语言模型的参数高效微调。 （arXiv：2401.02797v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02797</link>
      <description><![CDATA[多模态大语言模型 (MLLM) 代表了一种进化扩展
传统大型语言模型的功能，使它们能够
解决超出纯文本应用程序范围的挑战。它
利用先前在这些语言模型中编码的知识，
从而增强了它们在统治时期的适用性和功能性
多模式环境。最近的工作研究了 MLLM 的适应
预测自由形式答案作为解决医学视觉问题的生成任务
回答（Med-VQA）任务。在本文中，我们提出了一种参数有效的
用于微调专门针对 Med-VQA 应用程序的 MLLM 的框架，
并在公共基准数据集上进行实证验证。为了准确地
衡量绩效，我们采用人工评估，结果表明
我们的模型总体准确率达到 81.9%，优于 GPT-4v
模型在封闭式上的绝对准确度显着提高 26%
问题。代码可在此处获取：https://github.com/jinlHe/PeFoMed。
]]></description>
      <guid>http://arxiv.org/abs/2401.02797</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>DocGraphLM：用于信息提取的文档图语言模型。 （arXiv：2401.02823v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02823</link>
      <description><![CDATA[视觉丰富文档理解 (VrDU) 方面的进步已启用
对复杂文档进行信息提取和问答
布局。出现了两种架构——基于变压器的模型
受到法学硕士和图神经网络的启发。在本文中，我们介绍
DocGraphLM，一种新颖的框架，它将预训练的语言模型与
图语义。为了实现这一点，我们提出 1）联合编码器架构
表示文档，2）一种新颖的链接预测方法来重建
文档图表。 DocGraphLM 预测两个方向和之间的距离
使用优先考虑邻域的收敛联合损失函数的节点
恢复并降低远程节点检测的权重。我们的实验分为三个
SotA 数据集显示，随着采用，IE 和 QA 任务得到了持续改进
的图形特征。此外，我们报告采用图形特征
加速训练期间学习过程的收敛，尽管
仅通过链接预测构建。
]]></description>
      <guid>http://arxiv.org/abs/2401.02823</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>从法学硕士到对话代理：具有大型语言模型微调的记忆增强架构。 （arXiv：2401.02777v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02777</link>
      <description><![CDATA[本文介绍了 RAISE（通过 Scratchpad 进行推理和行动）
示例），一种增强大语言集成的先进架构
模型（法学硕士）喜欢 GPT-4 进入对话代理。 RAISE，增强
ReAct 框架，包含双组件内存系统、镜像
人类的短期和长期记忆，以保持上下文和连续性
对话。它需要一个全面的代理构建场景，
包括对话选择、场景提取、CoT 完成等阶段，
场景增强，进入法学硕士培训阶段。这种方法
似乎可以增强智能体在复杂情况下的可控性和适应性，
多轮对话。我们对房地产销售的初步评估
上下文表明RAISE比传统代理有一些优势，
表明其具有更广泛应用的潜力。这项工作有助于
人工智能领域通过提供一个强大的框架来开发更多的上下文感知和
多功能会话代理。
]]></description>
      <guid>http://arxiv.org/abs/2401.02777</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>植物生物学中的大型语言模型。 （arXiv：2401.02789v1 [q-bio.GN]）</title>
      <link>http://arxiv.org/abs/2401.02789</link>
      <description><![CDATA[大型语言模型 (LLM)，例如 ChatGPT，已经风靡全球
并通过了某些形式的图灵测试。然而，LLM 并不限于
人类语言并分析序列数据，例如 DNA、蛋白质和基因
表达。由此产生的基础模型可以重新调整用途来识别
数据中的复杂模式，产生强大的、多用途的
能够解释细胞系统的预测工具。这篇评论概述了
不同类型的法学硕士并展示它们最近在生物学中的用途。自从法学硕士以来
尚未被植物界所接受，我们还介绍了这些如何
可以为植物王国部署模型。
]]></description>
      <guid>http://arxiv.org/abs/2401.02789</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>用于更快的最小贝叶斯风险解码的无超参数方法。 （arXiv：2401.02749v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.02749</link>
      <description><![CDATA[最小贝叶斯风险 (MBR) 解码被证明是一个强大的替代方案
适用于各种文本生成任务的波束搜索解码。然而，MBR
需要大量时间进行推理来计算 MBR 目标，
这使得该方法在响应时间很长的许多情况下不可行
批判的。基于置信度的剪枝（CBP）（Cheng 和 Vlachos，2023）最近
被提出来减少机器翻译任务中的推理时间。
虽然它被证明可以显着减少计算量，但
需要使用开发集进行超参数调整才能有效。对此
最后，我们提出近似最小贝叶斯风险（AMBR）解码，
无超参数方法来近似运行 MBR 解码。 AMBR 衍生
根据观察，计算基于样本的 MBR 问题
目标是中心点识别问题。 AMBR 使用相关
顺序减半（CSH）算法（Baharav 和 Tse，2019），最好的
迄今为止用于中心点识别问题的近似算法，
计算基于样本的 MBR 目标。我们在机器上评估 AMBR
翻译、文本摘要和图像字幕任务。结果显示
AMBR 与 CBP 达到了同等水平，CBP 通过以下方式选择超参数
每个给定的计算预算都有一个 Oracle。
]]></description>
      <guid>http://arxiv.org/abs/2401.02749</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>自然语言的复杂系统方法。 （arXiv：2401.02772v1 [physical.soc-ph]）</title>
      <link>http://arxiv.org/abs/2401.02772</link>
      <description><![CDATA[该评论总结了研究中使用的主要方法论概念
从复杂性科学的角度分析自然语言并记录它们
适用于识别通用特征和系统特定特征
语言的书面表达。三个主要的复杂性相关研究
涵盖了定量语言学的趋势。第一部分讨论
文本中的词频问题并证明采用标点符号
考虑到恢复缩放，其违反齐普夫定律的情况经常发生
观察最常见的单词。第二部分介绍方法
受到时间序列分析的启发，用于研究各种
书面文本中的相关性。相关的时间序列是在
连续之间的文本分割成句子或短语的基础
标点符号。事实证明，这些系列开发的功能经常被发现
在复杂系统生成的信号中，例如远程相关性或
（多重）分形结构。此外，似乎之间的距离
标点符号符合威布尔分布的离散变体。
第三部分，网络形式主义在自然语言中的应用
被审查，特别是在所谓的单词邻接的背景下
网络。表征此类网络拓扑的参数可用于
例如，从文体测量的角度对文本进行分类。网络
方法也可以应用于表示单词的组织
协会。单词联想网络的结构是
与随机网络中观察到的显着不同，揭示
语言的真实属性。最后，标点符号似乎有一个
不仅对语言的信息承载能力产生重大影响
还取决于其关键统计特性，因此建议考虑
标点符号与单词同等。
]]></description>
      <guid>http://arxiv.org/abs/2401.02772</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>德语文本嵌入聚类基准。 （arXiv：2401.02709v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02709</link>
      <description><![CDATA[这项工作引入了评估集群性能的基准
不同领域中的德语文本嵌入。该基准是由
在需要的任务中增加使用聚类神经文本嵌入
文本分组（例如主题建模）以及对德语资源的需求
现有的基准。我们为一系列预训练的模型提供初步分析
根据不同聚类的结果评估单语言和多语言模型
算法。结果包括表现强劲的单语言和多语言模型。
减少嵌入的维度可以进一步改善聚类。
此外，我们还针对德语进行了持续预训练的实验
BERT 模型来估计这种额外训练的好处。我们的
实验表明，可以显着提高性能
短文本。所有代码和数据集都是公开的。
]]></description>
      <guid>http://arxiv.org/abs/2401.02709</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>MAMI：用于长序列神经元描述的多注意互信息。 （arXiv：2401.02744v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.02744</link>
      <description><![CDATA[神经元标记是一种可视化行为和响应的方法
某些神经元以某种模式激活该神经元。神经元标记
提取有关深层某些神经元捕获的特征的信息
神经网络，其中之一使用编码器-解码器图像描述
方法。使用的编码器可以是预训练的基于 CNN 的模型，解码器可以是
是一种基于 RNN 的文本生成模型。之前的作品，即米兰（Mutual
神经元的信息引导语言注释），试图将
使用修改后的显示、参加和讲述 (SAT) 模型的神经元行为
编码器，LSTM 在解码器中添加了 Bahdanau 注意力。米兰可以展示
在短序列神经元描述上取得了很好的结果，但它并没有表现出很好的效果
长序列神经元描述的结果，所以在这项工作中，我们想要
通过利用不同类型进一步提高 MILAN 的性能
注意力机制并另外将多个注意力结果添加到一个中，
为了结合多种注意力机制的所有优点。使用
我们的复合数据集，我们在我们提出的方案上获得了更高的 BLEU 和 F1-Score
模型，分别达到 17.742 和 0.4811。在某个时刻，模型
在峰值处收敛，我们的模型获得了 21.2262 的 BLEU 和 BERTScore
F1 得分为 0.4870。
]]></description>
      <guid>http://arxiv.org/abs/2401.02744</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>记忆、意识和大语言模型。 （arXiv：2401.02509v1 [q-bio.NC]）</title>
      <link>http://arxiv.org/abs/2401.02509</link>
      <description><![CDATA[随着认知科学和大语言模型（LLM）的发展，
这两个不同领域之间的联系日益增多。
基于这些联系，我们提出一个猜想：
法学硕士和图尔文的记忆理论之间存在二元性。我们确定
图尔文协同效应模型 (SEM) 之间的潜在对应关系
检索和在法学硕士中观察到的新兴能力，作为支持
为我们的猜想提供证据。此外，我们推测意识可能
被认为是基于这种二元性的突现能力的一种形式。我们还讨论
其他意识理论如何与我们的研究相交叉。
]]></description>
      <guid>http://arxiv.org/abs/2401.02509</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:40 GMT</pubDate>
    </item>
    <item>
      <title>用于对比学习的无监督硬负增强。 （arXiv：2401.02594v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.02594</link>
      <description><![CDATA[我们提出了无监督硬负增强（UNA），这是一种方法
基于术语频率逆生成合成负实例
文档频率（TF-IDF）检索模型。 UNA 使用 TF-IDF 分数来
确定句子中术语的感知重要性，然后生成
通过替换相关项来获得负样本。我们的实验
证明使用 UNA 训练的模型提高了以下方面的整体表现
语义文本相似性任务。获得额外的性能提升
将 UNA 与释义增强相结合时。进一步的结果显示
我们的方法与不同的骨干模型兼容。消融研究
还支持选择对负值进行 TF-IDF 驱动控制
增强。
]]></description>
      <guid>http://arxiv.org/abs/2401.02594</guid>
      <pubDate>Mon, 08 Jan 2024 06:18:40 GMT</pubDate>
    </item>
    </channel>
</rss>