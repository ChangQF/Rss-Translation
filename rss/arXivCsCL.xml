<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 21 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>标准化：使语言模型与专家定义的内容生成标准保持一致</title>
      <link>https://arxiv.org/abs/2402.12593</link>
      <description><![CDATA[arXiv:2402.12593v1 公告类型：新
摘要：工程、医疗保健和教育领域的专家遵循严格的标准来制作技术手册、用药说明和儿童阅读材料等优质内容。然而，目前可控文本生成方面的工作尚未探索使用这些标准作为控制的参考。为此，我们引入了 Standardize，这是一种基于检索式上下文学习的框架，用于指导大型语言模型与专家定义的标准保持一致。以教育领域的英语语言标准为用例，我们考虑使用欧洲共同语言参考框架（CEFR）和共同核心标准（CCS）来完成开放式内容生成的任务。我们的研究结果表明，模型对于 Llama2 和 GPT-4 的精确度可以分别提高 40% 到 100%，这表明使用从标准中提取的知识工件并将其集成到生成过程中可以有效地指导模型生成更好的标准- 对齐的内容。]]></description>
      <guid>https://arxiv.org/abs/2402.12593</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>什么是词？</title>
      <link>https://arxiv.org/abs/2402.12605</link>
      <description><![CDATA[arXiv:2402.12605v1 公告类型：新
摘要：为了设计隔离词汇访问和语义的强大范例，我们需要知道单词是什么。令人惊讶的是，尽管单词基本上影响着人类生活的各个方面，但很少有语言学家和哲学家对单词是什么有清晰的模型。定期发表有关语言的学术论文的研究人员常常依赖于过时的或不准确的关于词语的假设。这份简短的教学文件概述了该词典最肯定不是什么（尽管经常被错误地认为是什么）、它可能是什么（基于当前良好的理论）以及对实验设计的一些影响。]]></description>
      <guid>https://arxiv.org/abs/2402.12605</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>StyleDubber：迈向电影配音的多尺度风格学习</title>
      <link>https://arxiv.org/abs/2402.12636</link>
      <description><![CDATA[arXiv:2402.12636v1 公告类型：新
摘要：给定一个脚本，电影配音（视觉语音克隆，V2C）中的挑战是根据参考音轨的音调生成在时间和情感上与视频完美契合的语音。现有最先进的V2C模型根据视频帧之间的划分来打破脚本中的音素，这解决了时间对齐问题，但导致音素发音不完整且身份稳定性差。为了解决这个问题，我们提出了StyleDubber，它将配音学习从帧级别切换到音素级别。它包含三个主要组件：（1）在音素级别运行的多模式风格适配器，从参考音频中学习发音风格，并根据视频中呈现的面部情感生成中间表示； （2）话语级风格学习模块，指导梅尔谱图解码和中间嵌入的精炼过程，以改善整体风格表达； (3) 音素引导唇形矫正器以保持唇形同步。对 V2C 和 Grid 这两个主要基准进行的大量实验证明了所提出的方法与当前最先进的方法相比具有良好的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.12636</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>信心很重要：重新审视大型语言模型的内在自我纠正能力</title>
      <link>https://arxiv.org/abs/2402.12563</link>
      <description><![CDATA[arXiv:2402.12563v1 公告类型：新
摘要：大型语言模型（LLM）最近的成功引发了人们对其自我纠正能力的日益浓厚的兴趣。本文对法学硕士内在的自我修正进行了全面的调查，试图解决有关其可行性的持续争论。我们的研究发现了自我修正过程中一个重要的潜在因素——法学硕士的“信心”。忽视这个因素可能会导致模型过度批评自己，从而导致关于自我纠正功效的不可靠结论。我们通过实验观察到法学硕士有能力了解自己的回答的“信心”。它激励我们开发一个“如果-否则”（IoE）提示框架，旨在指导法学硕士评估自己的“信心”，促进内在的自我纠正。我们进行了广泛的实验，并证明我们基于 IoE 的提示可以在自我纠正答案的准确性方面比初始答案实现持续改进。我们的研究不仅揭示了影响法学硕士自我纠错的潜在因素，还引入了一个实用的框架，利用万物互联激励原理，以“信心”有效地提高自我纠错能力。该代码可在 \url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git} 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.12563</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>GenAudit：用证据修复语言模型输出中的事实错误</title>
      <link>https://arxiv.org/abs/2402.12566</link>
      <description><![CDATA[arXiv:2402.12566v1 公告类型：新
摘要：即使提供参考文档的访问权限，法学硕士也可能生成事实上不正确的陈述。此类错误在高风险应用程序中可能会很危险（例如，医疗保健或金融领域基于文档的 QA）。我们推出 GenAudit——一种旨在协助对基于文档的任务的 LLM 响应进行事实检查的工具。 GenAudit 建议通过修改或删除参考文件不支持的主张来编辑 LLM 回复，并提供参考文件中似乎有支持的事实的证据。我们训练模型来执行这些任务，并设计一个交互式界面来向用户呈现建议的编辑和证据。人工评估员的综合评估表明，GenAudit 在汇总来自不同领域的文档时可以检测 8 种不同的 LLM 输出中的错误。为了确保系统标记大多数错误，我们提出了一种方法，可以提高错误召回率，同时最大限度地减少对精度的影响。我们将发布我们的工具（GenAudit）和事实检查模型供公众使用。]]></description>
      <guid>https://arxiv.org/abs/2402.12566</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>发展人工智能集体以增强人类多样性并实现自我监管</title>
      <link>https://arxiv.org/abs/2402.12590</link>
      <description><![CDATA[arXiv:2402.12590v1 公告类型：新
摘要：大型语言模型根据其他语言模型生成的文本来引导其行为。这种能力及其在网络环境中的日益普及预示着它们将有意无意地相互“编程”并形成新兴的人工智能主体性、关系和集体。在这里，我们呼吁研究界研究交互人工智能的这些“类社会”特性，以增加其回报并降低其对人类社会和在线环境健康的风险。我们使用一个简单的模型及其输出来说明这种新兴的、去中心化的人工智能集体如何扩大人类多样性的界限并降低在线有毒、反社会行为的风险。最后，我们讨论人工智能自我调节的机会，并解决与创建和维护去中心化人工智能集体相关的道德问题和设计挑战。]]></description>
      <guid>https://arxiv.org/abs/2402.12590</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>Archer：具有算术、常识和假设推理的人工标记文本到 SQL 数据集</title>
      <link>https://arxiv.org/abs/2402.12554</link>
      <description><![CDATA[arXiv:2402.12554v1 公告类型：新
摘要：我们提出了 Archer，这是一个具有挑战性的双语文本到 SQL 数据集，专门针对复杂推理，包括算术、常识和假设推理。它包含 1,042 个英语问题和 1,042 个中文问题，以及 521 个独特的 SQL 查询，涵盖 20 个领域的 20 个英语数据库。值得注意的是，与现有的公开数据集相比，该数据集的复杂性明显更高。我们的评估表明，Archer 挑战了当前最先进模型的能力，Spider 排行榜上排名靠前的模型在 Archer 测试集上的执行准确度仅为 6.73%。因此，Archer 对该领域的未来研究提出了重大挑战。]]></description>
      <guid>https://arxiv.org/abs/2402.12554</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>使用法学硕士创建细粒度实体类型分类法</title>
      <link>https://arxiv.org/abs/2402.12557</link>
      <description><![CDATA[arXiv:2402.12557v1 公告类型：新
摘要：在这项研究中，我们研究了 GPT-4 及其高级迭代 GPT-4 Turbo 在自主开发详细实体类型分类法方面的潜力。我们的目标是构建一个全面的分类法，从实体类型的广泛分类开始——包括对象、时间、地点、组织、事件、行动和主题——类似于现有的手动分类法。然后，利用 GPT-4 的内部知识库，通过迭代提示技术逐步完善该分类。其结果是一个广泛的分类，包含 5000 多个细致入微的实体类型，在主观评估中表现出卓越的质量。
  我们采用了简单而有效的提示策略，使分类能够动态扩展。这种详细分类法的实际应用是多样且重要的。它有助于通过基于模式的组合创建新的、更复杂的分支，并显着增强信息提取任务，例如关系提取和事件参数提取。我们的方法不仅引入了分类法创建的创新方法，而且还为在各种计算语言学和人工智能相关领域应用此类分类法开辟了新途径。]]></description>
      <guid>https://arxiv.org/abs/2402.12557</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>CausalGym：对语言任务的因果可解释性方法进行基准测试</title>
      <link>https://arxiv.org/abs/2402.12560</link>
      <description><![CDATA[arXiv:2402.12560v1 公告类型：新
摘要：语言模型（LM）已被证明是心理语言学研究的强大工具，但大多数先前的工作都集中在纯粹的行为测量（例如，意外比较）上。与此同时，模型可解释性的研究已经开始阐明塑造 LM 行为的抽象因果机制。为了帮助将这些研究领域更加紧密地结合在一起，我们推出了 CausalGym。我们调整并扩展了 SyntaxGym 任务套件，以对可解释性方法因果影响模型行为的能力进行基准测试。为了说明如何使用 CausalGym，我们研究了 pythia 模型 (14M--6.9B) 并评估了各种可解释性方法的因果功效，包括线性探测和分布式对齐搜索 (DAS)。我们发现 DAS 优于其他方法，因此我们用它来研究 pythia-1b 中两种困难语言现象的学习轨迹：负极性项目许可和填充间隙依赖性。我们的分析表明，实现这两项任务的机制是在离散阶段学习的，而不是逐渐学习的。]]></description>
      <guid>https://arxiv.org/abs/2402.12560</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>预训练数据中的并行结构产生上下文学习</title>
      <link>https://arxiv.org/abs/2402.12530</link>
      <description><![CDATA[arXiv:2402.12530v1 公告类型：新
摘要：预训练语言模型（LM）能够进行上下文学习（ICL）：它们可以适应提示中仅给出几个示例的任务，而无需任何参数更新。然而，目前尚不清楚这种能力从何而来，因为预训练文本和 ICL 提示之间存在明显的分布变化。在这项工作中，我们研究了预训练数据的哪些模式对 ICL 有贡献。我们发现 LM 的 ICL 能力取决于预训练数据中的 $\textit{parallel Structure}$ ——同一上下文窗口中遵循相似模板的短语对。具体来说，我们通过检查一个短语的训练是否改善了另一个短语的预测来检测并行结构，并进行消融实验来研究它们对 ICL 的影响。我们表明，删除预训练数据中的并行结构会使 LM 的 ICL 准确度降低 51%（随机消融则降低 2%）。即使排除 n 元重复和远程依赖等常见模式，这种下降仍然存在，显示了并行结构的多样性和通用性。仔细观察检测到的并行结构表明它们涵盖了不同的语言任务并且跨越了数据的长距离。]]></description>
      <guid>https://arxiv.org/abs/2402.12530</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>TrustScore：LLM 响应可信度的无参考评估</title>
      <link>https://arxiv.org/abs/2402.12545</link>
      <description><![CDATA[arXiv:2402.12545v1 公告类型：新
摘要：大型语言模型（LLM）在各个领域都展示了令人印象深刻的功能，促进了其实际应用的激增。然而，人们对法学硕士输出的可信度产生了担忧，特别是在闭卷问答任务中，由于缺乏上下文或真实信息，非专家可能很难识别不准确之处。本文介绍了 TrustScore，这是一个基于行为一致性概念的框架，用于评估法学硕士的回答是否与其内在知识相符。此外，TrustScore 可以与事实检查方法无缝集成，从而评估与外部知识源的一致性。实验结果表明，TrustScore 与人类判断实现了很强的相关性，超越了现有的无参考指标，并取得了与基于参考的指标相当的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.12545</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>文物还是绑架：法学硕士如何在没有问题的情况下回答多项选择题？</title>
      <link>https://arxiv.org/abs/2402.12483</link>
      <description><![CDATA[arXiv:2402.12483v1 公告类型：新
摘要：多项选择题回答（MCQA）通常用于评估大型语言模型（LLM）。为了了解 MCQA 是否按预期评估 LLM，我们探讨了 LLM 是否可以在仅选择提示的情况下执行 MCQA，其中模型必须仅从选项中选择正确的答案。在三个 MCQA 数据集和四个法学硕士中，此提示在 11/12 案例中优于大多数基线，准确度增益高达 0.33。为了帮助解释这种行为，我们对记忆、选择动态和问题推理进行了深入的黑盒分析。我们的主要发现有三个方面。首先，我们没有发现任何证据表明仅选择的准确性仅源于记忆。其次，个人选择的先验并不能完全解释仅选择的准确性，这暗示法学硕士使用选择的群体动态。第三，法学硕士有一定的能力从选择中推断出相关问题，令人惊讶的是有时甚至可以匹配原始问题。我们希望推动在 MCQA 基准测试中使用更强的基线、设计稳健的 MCQA 数据集，并进一步努力解释 LLM 决策。]]></description>
      <guid>https://arxiv.org/abs/2402.12483</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>预先训练的语言模型是否可以检测和理解语义不规范？问尘埃！</title>
      <link>https://arxiv.org/abs/2402.12486</link>
      <description><![CDATA[arXiv:2402.12486v1 公告类型：新
摘要： 在日常语言使用中，说话者经常说出和解释语义上不明确的句子，即其内容不足以充分传达其信息或单义地解释它们。例如，要解释未指定的句子“不要花太多”，这会隐含什么（不）花什么，需要额外的语言背景或外部知识。在这项工作中，我们提出了一种新颖的按类型分组的语义未指定句子数据集（DUST），并用它来研究预训练的语言模型（LM）是否正确识别和解释未指定的句子。我们发现，当明确提示时，较新的 LM 能够合理地识别未指定的句子。然而，对于任何语言模型来说，正确解释它们要困难得多。我们的实验表明，在解释未指定的句子时，语言模型几乎没有表现出不确定性，这与未指定的理论解释所预测的相反。总的来说，我们的研究揭示了当前模型处理句子语义的局限性，并强调了在评估 LM 语言能力时使用自然数据和交流场景的重要性。]]></description>
      <guid>https://arxiv.org/abs/2402.12486</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>您的视觉语言模型本身就是一个强大的过滤器：通过数据选择实现高质量的指令调整</title>
      <link>https://arxiv.org/abs/2402.12501</link>
      <description><![CDATA[arXiv:2402.12501v1 公告类型：新
摘要：指令调优中的数据选择是获取高质量数据和训练遵循指令的大语言模型（LLM）的关键过程，但它仍然是视觉语言模型（VLM）的一个新的、未经探索的研究领域。法学硕士现有的数据选择方法要么依赖于单个不可靠的分数，要么使用下游任务进行选择，这非常耗时，并且可能导致对所选评估数据集的潜在过度拟合。为了应对这一挑战，我们引入了一种新颖的数据集选择方法，即自过滤器，它利用 VLM 本身作为过滤器。这种方法的灵感来自于这样的观察：VLM 受益于最具挑战性指令的训练。自过滤器分两个阶段运行。在第一阶段，我们设计了一个评分网络来评估训练指令的难度，该网络与 VLM 共同训练。在第二阶段，我们使用训练后的得分网来衡量每条指令的难度，选择最具挑战性的样本，并惩罚相似的样本以鼓励多样性。在 LLaVA 和 MiniGPT-4 上的综合实验表明，与仅使用约 15% 样本的全数据设置相比，Self-Filter 可以达到更好的结果，并且可以实现相对于竞争基线的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2402.12501</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>理解科学发现报告中的细粒度扭曲</title>
      <link>https://arxiv.org/abs/2402.12431</link>
      <description><![CDATA[arXiv:2402.12431v1 公告类型：新
摘要：扭曲的科学传播会损害个人和社会，因为它可能导致不健康的行为改变并降低对科学机构的信任。鉴于近年来科学传播量的迅速增加，深入了解如何向公众报告科学出版物的发现以及自动检测原始作品中的扭曲的方法至关重要。之前的工作主要集中在扭曲的各个方面或使用不成对的数据。在这项工作中，我们为解决这个问题做出了三项基础性贡献：（1）注释了学术论文中的 1,600 个科学发现实例，并与新闻文章和推文中报道的相应发现配对。四个特征：因果性、确定性、普遍性、煽情性； (2) 建立自动检测这些特征的基线； (3) 分析人工注释数据和大规模未标记数据中这些特征变化的普遍程度。我们的结果表明，科学发现在报道时经常会发生微妙的扭曲。推文比科学新闻报道更容易歪曲研究结果。自动检测细粒度的扭曲是一项具有挑战性的任务。在我们的实验中，经过微调的特定任务模型始终优于少数 LLM 提示。]]></description>
      <guid>https://arxiv.org/abs/2402.12431</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:11 GMT</pubDate>
    </item>
    </channel>
</rss>