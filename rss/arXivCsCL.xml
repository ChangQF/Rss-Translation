<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 27 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>迈入大门：通过认知心理学理解大语言模型越狱</title>
      <link>https://arxiv.org/abs/2402.15690</link>
      <description><![CDATA[arXiv:2402.15690v1 公告类型：新
摘要：大型语言模型（LLM）逐渐成为人们获取新知识的门户。然而，攻击者可以突破模型的安全保护（“监狱”）来访问受限信息，这就是所谓的“越狱”。之前的研究已经表明，当前的法学硕士在面对此类越狱攻击时存在弱点。然而，在收到越狱提示后，法学硕士内部决策机制的理解明显缺乏。我们的研究为越狱提示提供了心理学解释。借鉴认知一致性理论，我们认为越狱的关键是引导法学硕士朝着错误的方向实现认知协调。此外，我们提出了一种基于Foot-in-the-Door（FITD）技术的自动黑盒越狱方法。该方法通过多步骤增量提示逐步引导模型回答有害问题。我们实例化了一个原型系统来评估 8 名高级法学硕士的越狱效果，平均成功率为 83.9%。本研究从心理学角度来解释法学硕士内在决策逻辑。]]></description>
      <guid>https://arxiv.org/abs/2402.15690</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    <item>
      <title>通过解码大脑信号语义来增强查询</title>
      <link>https://arxiv.org/abs/2402.15708</link>
      <description><![CDATA[arXiv:2402.15708v1 公告类型：新
摘要：查询增强是细化语义不精确查询的关键技术。传统上，查询增强依赖于从最初检索的潜在相关文档中提取信息。如果最初检索到的文档质量较低，那么查询增强的有效性也会受到限制。我们提出了 Brain-Aug，它通过合并从大脑信号解码的语义信息来增强查询。 BrainAug 使用由大脑信号信息和面向排名的推理方法构建的提示生成原始查询的延续。 fMRI（功能磁共振成像）数据集的实验结果表明，Brain-Aug 生成语义上更准确的查询，从而提高文档排名性能。大脑信号带来的这种改善对于模糊查询尤其显着。]]></description>
      <guid>https://arxiv.org/abs/2402.15708</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    <item>
      <title>解决因果语言模型中上下文演示示例的顺序敏感性</title>
      <link>https://arxiv.org/abs/2402.15637</link>
      <description><![CDATA[arXiv:2402.15637v1 公告类型：新
摘要：上下文学习已成为自然语言处理中的流行范例。然而，它的性能可能会受到上下文演示示例的顺序的显着影响。在本文中，我们发现与前缀语言模型（PrefixLM）相比，因果语言模型（CausalLM）对此顺序更敏感。我们将这种现象归因于 CausalLM 中的自回归注意掩码，它限制每个 token 访问后续 token 的信息。这导致不同位置的样本具有不同的感受野，从而导致不同位置的表示差异。为了应对这一挑战，我们引入了一种无监督的微调方法，称为信息增强和一致性增强方法。这种方法利用对比学习来对齐不同位置的上下文示例的表示，并引入一致性损失以确保具有不同排列的输入的相似表示。这增强了模型在排列之间的预测一致性。四个基准的实验结果表明，我们提出的方法可以降低对上下文中示例顺序的敏感性，并表现出强大的通用性，特别是当演示来自于与训练阶段使用的池不同的池时，或者当示例的数量- 上下文示例与训练期间使用的示例不同。]]></description>
      <guid>https://arxiv.org/abs/2402.15637</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>探索物理动力学多模态推理中的失败案例</title>
      <link>https://arxiv.org/abs/2402.15654</link>
      <description><![CDATA[arXiv:2402.15654v1 公告类型：新
摘要：在本文中，我们探索了法学硕士在情境环境中通过物理推理解决问题的能力。我们构建了一个简单的模拟环境，并演示了一些示例，在零样本设置中，文本和多模态 LLM 都显示有关各种对象的原子世界知识，但无法将这些知识组合成对象操作和放置任务的正确解决方案。我们还使用 BLIP（一种经过更复杂的跨模式注意力训练的视觉语言模型）来识别与该模型无法支持的对象物理属性相关的案例。最后，我们提出了一个发现环境中对象的相关属性的过程，并提出了一种将这些知识提炼回法学硕士的方法。]]></description>
      <guid>https://arxiv.org/abs/2402.15654</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>利用 ChatGPT 进行药物警戒事件提取：一项实证研究</title>
      <link>https://arxiv.org/abs/2402.15663</link>
      <description><![CDATA[arXiv:2402.15663v1 公告类型：新
摘要：随着大语言模型（LLM）的出现，人们对探索其医学应用潜力越来越感兴趣。本研究旨在调查法学硕士（特别是 ChatGPT）在药物警戒事件提取背景下的能力，其主要目标是从文本医学来源中识别和提取不良事件或潜在的治疗事件。我们进行了大量的实验，采用各种提示和演示选择策略来评估 ChatGPT 在药物警戒事件提取任务中的性能。研究结果表明，虽然 ChatGPT 通过适当的演示选择策略表现出了合理的性能，但与完全微调的小型模型相比，它仍然存在不足。此外，我们还探索了利用 ChatGPT 进行数据增强的潜力。然而，我们的调查表明，将合成数据纳入微调可能会导致性能下降，这可能归因于 ChatGPT 生成的标签中的噪声。为了缓解这个问题，我们探索了不同的过滤策略，并发现，通过适当的方法，可以实现更稳定的性能，尽管持续的改进仍然难以实现。]]></description>
      <guid>https://arxiv.org/abs/2402.15663</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>选择性“选择性预测”：减少视觉语言推理中不必要的放弃</title>
      <link>https://arxiv.org/abs/2402.15610</link>
      <description><![CDATA[arXiv:2402.15610v1 公告类型：新
摘要：先前关于选择性预测的工作通过允许视觉语言模型（VLM）在不确定时放弃回答来最大限度地减少视觉语言模型（VLM）的错误预测。然而，当部署对不准确预测的容忍度较低的视觉语言系统时，选择性预测可能会过于谨慎并且过于频繁地放弃，即使对于许多正确的预测也是如此。我们引入了 ReCoVERR，一种推理时间算法，可减少选择性视觉语言系统的过度放弃，而不会降低预测准确性。当 VLM 做出低置信度预测时，ReCoVERR 不会放弃，而是尝试在图像中找到相关线索，为预测提供额外的证据。 ReCoVERR 使用 LLM 向 VLM 提出相关问题，收集高置信度证据，如果有足够的证据证实预测，系统就会做出预测而不是弃权。 ReCoVERR 使两个 VLM（BLIP2 和 InstructBLIP）能够比普通选择性预测多回答 20% 的 A-OKVQA 任务问题，而不会降低系统精度，从而提高整体系统可靠性。]]></description>
      <guid>https://arxiv.org/abs/2402.15610</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>基于语言的用户配置文件进行推荐</title>
      <link>https://arxiv.org/abs/2402.15623</link>
      <description><![CDATA[arXiv:2402.15623v1 公告类型：新
摘要：大多数传统推荐方法（例如矩阵分解）将用户配置文件表示为高维向量。不幸的是，这些向量缺乏可解释性和可操纵性，并且在冷启动环境中通常表现不佳。为了解决这些缺点，我们探索使用以人类可读文本表示的用户配置文件。我们提出了基于语言的分解模型（LFM），它本质上是一个编码器/解码器模型，其中编码器和解码器都是大型语言模型（LLM）。编码器 LLM 根据用户的评级历史生成用户兴趣的紧凑自然语言配置文件。解码器 LLM 使用此摘要配置文件来完成预测下游任务。我们在 MovieLens 数据集上评估我们的 LFM 方法，将其与矩阵分解和直接根据用户评分历史进行预测的 LLM 模型进行比较。在冷启动设置中，我们发现我们的方法比矩阵分解具有更高的精度。此外，我们发现生成紧凑且人类可读的摘要通常与直接 LLM 预测相当或更好，同时具有更好的可解释性和更短的模型输入长度。我们的结果激发了许多未来的研究方向和潜在的改进。]]></description>
      <guid>https://arxiv.org/abs/2402.15623</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>细粒度的自我认可提高了事实性和推理能力</title>
      <link>https://arxiv.org/abs/2402.15631</link>
      <description><![CDATA[arXiv:2402.15631v1 公告类型：新
摘要：这项工作研究通过减轻事实冲突的幻觉来改进推理时的大语言模型（LLM）生成。特别是，我们提出了一个自我认可框架，该框架利用多个采样响应之间的细粒度事实水平比较。与之前执行响应级别选择的集成方法（Wang et al., 2022;Chen et al., 2023））相比，我们的方法可以更好地减轻幻觉，特别是对于长格式生成任务。我们的方法可以让规模较小的开源法学硕士广泛受益，因为它主要进行简单的基于内容的比较。传记上的实验表明，我们的方法可以在不同规模的法学硕士中通过简单直观的提示有效提高世代的真实性。此外，对 TriviaQA 和 GSM8K 的综合分析证明了自我认可具有更广泛应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.15631</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>促使法学硕士根据学术手稿的同行评审叙述撰写元评审草稿</title>
      <link>https://arxiv.org/abs/2402.15589</link>
      <description><![CDATA[arXiv:2402.15589v1 公告类型：新
摘要：学术同行评审过程中最重要但最繁重的任务之一是撰写元评审，这涉及根据多位专家的同行评审叙述来了解学术稿件的核心贡献、优点和缺点，然后总结将多位专家的观点转化为简洁的整体概述。鉴于生成式人工智能的最新重大发展，特别是大型语言模型（LLM），严格研究 LLM 在学术同行评审环境中生成此类元评审的效用非常引人注目。在本文中，我们对三种流行的法学硕士（即 GPT-3.5、LLaMA2 和 PaLM2）进行了案例研究，根据最近提出的 TELeR 分类法，通过不同类型/级别的提示来自动生成元评论。最后，我们对法学硕士生成的元评论进行了详细的定性研究，并总结了我们的发现和建议，以促使法学硕士完成这项复杂的任务。]]></description>
      <guid>https://arxiv.org/abs/2402.15589</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:48 GMT</pubDate>
    </item>
    <item>
      <title>混合模型的交替弱三音素/BPE 对准监督可改善端到端 ASR</title>
      <link>https://arxiv.org/abs/2402.15594</link>
      <description><![CDATA[arXiv:2402.15594v1 公告类型：新
摘要：本文提出了交替弱三音素/BPE 对齐监督来改进端到端模型训练。为此，使用预先存在的混合 ASR 系统提取三音素和 BPE 对齐。然后，通过在用于三音素对齐的编码器的中间层表示和用于BPE对齐的编码器处的此类对齐上计算的基于交叉熵的中间辅助损失来获得正则化效果。弱监督是通过参数为 0.5 的强标签平滑来实现的。 TED-LIUM 2 上的实验结果表明，基于弱监督的三音素或 BPE 对齐比标准 CTC 辅助损失提高了 ASR 性能。此外，它们的组合进一步降低了错误率。我们还研究了模型训练期间两个辅助任务的交替，并观察到额外的性能增益。总体而言，所提出的技术比 CTC 正则化基线系统的相对错误率降低了 10% 以上。]]></description>
      <guid>https://arxiv.org/abs/2402.15594</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:48 GMT</pubDate>
    </item>
    <item>
      <title>PCA-Bench：评估感知-认知-行动链中的多模态大语言模型</title>
      <link>https://arxiv.org/abs/2402.15527</link>
      <description><![CDATA[arXiv:2402.15527v1 公告类型：新
摘要：我们提出了 PCA-Bench，这是一种用于评估多模态大型语言模型（MLLM）集成能力的多模态决策基准。与之前专注于简单任务和个体模型能力的基准测试不同，PCA-Bench引入了三个复杂的场景：自动驾驶、家用机器人和开放世界游戏。给定任务指令和不同的上下文，模型需要将感知、认知和行动的多种能力无缝​​集成在推理链中，以做出准确的决策。此外，PCA-Bench 还具有错误定位功能，可仔细检查感知、知识或推理等领域的模型不准确性。这增强了部署 MLLM 的可靠性。为了平衡评估的准确性和效率，我们提出了 PCA-Eval（一种自动评估协议），并评估了 10 个流行的 MLLM。结果揭示了开源模型和强大的专有模型（例如 GPT-4 Vision）之间存在显着的性能差异。为了解决这个问题，我们引入了 Embodied-Instruction-Evolution (EIE)，这是一个用于在多模式体现环境中合成指令调优示例的自动框架。 EIE 在 PCA-Bench 中生成了 7,510 个训练样本，并增强了开源 MLLM 的性能，偶尔超越 GPT-4 Vision（决策精度+3%），从而验证了 EIE 的有效性。我们的研究结果表明，像 GPT4-Vision 这样强大的 MLLM 在实体智能体的决策中表现出了希望，为 MLLM 研究开辟了新的途径。]]></description>
      <guid>https://arxiv.org/abs/2402.15527</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:47 GMT</pubDate>
    </item>
    <item>
      <title>评估 ChatGPT 垃圾邮件检测的性能</title>
      <link>https://arxiv.org/abs/2402.15537</link>
      <description><![CDATA[arXiv:2402.15537v1 公告类型：新
摘要：电子邮件仍然是专业和商业领域中关键且广泛使用的通信媒介。尽管如此，垃圾邮件的盛行给用户带来了重大挑战，扰乱了他们的日常生活并降低了生产力。因此，根据内容准确识别和过滤垃圾邮件对于网络安全至关重要。自然语言处理的最新进展，特别是像 ChatGPT 这样的大型语言模型，在问答和文本生成等任务中表现出了卓越的性能。然而，其在垃圾邮件识别方面的潜力仍未得到充分开发。为了填补这一空白，本研究尝试评估 ChatGPT 在英文和中文电子邮件数据集中识别垃圾邮件的能力。我们使用 ChatGPT 通过上下文学习来检测垃圾邮件，这需要及时的指导和一些演示。我们还研究了训练示例大小如何影响 ChatGPT 的性能。为了进行比较，我们还实现了五种流行的基准方法，包括朴素贝叶斯、支持向量机 (SVM)、逻辑回归 (LR)、前馈密集神经网络 (DNN) 和 BERT 分类器。经过大量实验，ChatGPT 在大型英语数据集上的性能明显比深度监督学习方法差，但在资源匮乏的中文数据集上表现出优越的性能，甚至在这种情况下优于 BERT。]]></description>
      <guid>https://arxiv.org/abs/2402.15537</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:47 GMT</pubDate>
    </item>
    <item>
      <title>大规模生成人工智能文本应用于体育和音乐</title>
      <link>https://arxiv.org/abs/2402.15514</link>
      <description><![CDATA[arXiv:2402.15514v1 公告类型：新
摘要：我们解决了扩大媒体内容制作的问题，包括全球大型体育和音乐活动的评论和个性化新闻报道。我们的方法依靠生成式人工智能模型将大量多模式数据（例如视频、文章、实时评分源、统计数据和情况说明书）转换为连贯流畅的文本。基于这种方法，我们首次引入了人工智能解说系统，该系统被部署用于为 2023 年美国网球公开赛、温布尔登网球公开赛和大师赛的精彩赛事生成自动解说。同样，我们的解决方案也得到了扩展，可以为 ESPN Fantasy Football 创建个性化内容，并为格莱美奖创建有关音乐艺术家的故事。这些应用程序使用通用软件架构构建，速度提高了 15 倍，平均 Rouge-L 为 82.00，困惑度为 6.6。我们的工作在上述活动中得到了成功部署，为全球 9000 万粉丝提供了 80 亿次页面浏览量，不断突破体育、娱乐和人工智能交叉领域的界限。]]></description>
      <guid>https://arxiv.org/abs/2402.15514</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>当心单词：评估会话大型语言模型的词汇丰富度</title>
      <link>https://arxiv.org/abs/2402.15518</link>
      <description><![CDATA[arXiv:2402.15518v1 公告类型：新
摘要：目前正在对许多不同任务（从逻辑推理或数学到回答无数主题的问题）评估会话大型语言模型（LLM）的性能，特别是 ChatGPT 的性能。相反，对这些法学硕士生成的文本的语言特征研究的关注要少得多。这是令人惊讶的，因为法学硕士是语言的模型，了解他们如何使用语言非常重要。事实上，会话法学硕士有望对语言的演变产生重大影响，因为它们最终可能主导新文本的创建。这意味着，例如，如果会话法学硕士不使用某个单词，它可能会变得越来越不频繁，并最终完全停止使用。因此，评估它们生成的文本的语言特征以及这些特征如何依赖于模型参数是了解会话式法学硕士对语言进化的潜在影响的第一步。在本文中，我们考虑了对法学硕士生成的文本的词汇丰富度的评估以及它如何依赖于模型参数。以 ChatGPT 作为案例研究，提出并使用一种方法对词汇丰富度进行综合评估。结果显示词汇丰富度如何取决于 ChatGPT 的版本及其一些参数（例如存在惩罚）或分配给模型的角色。我们分析中使用的数据集和工具是在开放许可下发布的，目的是引起对法学硕士生成文本的语言特征评估的急需关注。]]></description>
      <guid>https://arxiv.org/abs/2402.15518</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:46 GMT</pubDate>
    </item>
    <item>
      <title>通过框架理论检测错误信息：基于框架元素的模型</title>
      <link>https://arxiv.org/abs/2402.15525</link>
      <description><![CDATA[arXiv:2402.15525v1 公告类型：新
摘要：在本文中，我们深入研究了错误信息检测快速发展的挑战，特别关注叙事框架的微妙操纵——这是人工智能社区中尚未探索的领域。生成式人工智能模型产生误导性叙述的潜力凸显了这个问题的紧迫性。根据沟通和框架理论，我们认为准确信息的呈现或“框架”可以极大地改变其解释，可能导致错误信息。我们通过现实世界的例子来强调这个问题，展示叙事框架的转变如何将基于事实的信息转化为错误信息。为了应对这一挑战，我们提出了一种创新方法，利用预先训练的大型语言模型和深度神经网络的力量来检测源自不同框架下描绘的准确事实的错误信息。这些先进的人工智能技术提供了前所未有的能力，可以识别非结构化数据中的复杂模式，这对于检查叙事框架的微妙之处至关重要。本文的目的是弥合人工智能领域的重大研究空白，为解决框架引起的错误信息提供有价值的见解和方法，从而促进负责任和值得信赖的人工智能技术的进步。集中进行了多项实验，实验结果明确证明了框架理论要素的各种影响，证明了应用框架理论来提高错误信息检测性能的基本原理。]]></description>
      <guid>https://arxiv.org/abs/2402.15525</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:46 GMT</pubDate>
    </item>
    </channel>
</rss>