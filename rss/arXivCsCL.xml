<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 09 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>不平等的机会：通过大语言模型检查地理建议中的偏见</title>
      <link>https://arxiv.org/abs/2504.05325</link>
      <description><![CDATA[ARXIV：2504.05325V1公告类型：新 
摘要：大语言模型（LLMS）的最新进展使它们成为最终用户中流行的信息寻求工具。但是，LLMS的统计培训方法引起了人们对代表不足的主题的担忧，可能导致可能影响现实世界的决策和机遇的偏见。随着LLM变得更加普遍，无论是通过直接互动（例如，当用户与聊天机器人或自动化助手互动时），还是通过将其整合到第三方应用程序（作为代理商）中，这些偏见可能会产生重大的经济，社会和文化影响，就像通过用户互动时，模型会影响决策制定过程和幕后的决策过程。我们的研究研究了美国城市和城镇跨三个领域的LLMS建议中存在的偏见：搬迁，旅游业和开展业务。我们探讨了两个关键的研究问题：（i）LLMS的回答的相似性，以及（ii）这种相似性如何有利于具有某些特征的领域，而不是其他特征，从而引入了偏见。我们专注于LLMS响应的一致性及其倾向过多或代表性不足的特定位置的趋势。我们的发现表明，在这些建议中，人口偏见一致，这可能会使``丰富的富裕人&#39;&#39;效果永久化，从而扩大了现有的经济差异。]]></description>
      <guid>https://arxiv.org/abs/2504.05325</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从具有自适应加权拒绝采样的语言模型中快速控制的生成</title>
      <link>https://arxiv.org/abs/2504.05410</link>
      <description><![CDATA[ARXIV：2504.05410V1公告类型：新 
摘要：从受到某种约束的语言模型生成的主要方法是本地限制的解码（LCD），在每个时间步骤中逐步采样令牌，因此永远不会违反约束。通常，这是通过令牌掩盖来实现的：在词汇上循环并排除不合格令牌。这种方法有两个重要的问题。 （i）评估每个令牌上的约束可能非常昂贵 -  LM词汇量通常超过$ 100,000 $的代币。 （ii）LCD可以在字符串上扭曲全球分布，即使它们导致死端路径下降，也只能基于本地信息进行采样令牌。这项工作引入了一种解决这两个问题的新算法。首先，为避免在生成的每个步骤中评估对完整词汇的约束，我们提出了一种自适应拒绝采样算法，该算法通常需要减少约束评估的数量级。其次，我们展示了如何扩展该算法以产生低变化的，无偏见的重要性权重的估计值 - 额外的成本很小 - 可以在先前建议的顺序蒙特卡洛算法中大量使用，以纠正局部约束执法的近视行为。通过在文本到SQL，分子综合，目标推理，模式匹配和JSON领域的广泛经验评估中，我们表明我们的方法优于最新的基线，支持更广泛的约束并提高运行时和性能。其他理论和经验分析表明，我们的方法的运行时效率是由其动态使用计算的驱动，并随着未约束和受约束的LM之间的差异而进行缩放，因此，对于更好的模型而言，运行时的改进更大。]]></description>
      <guid>https://arxiv.org/abs/2504.05410</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更少但更好：大型语言模型的参数效率微调用于人格检测</title>
      <link>https://arxiv.org/abs/2504.05411</link>
      <description><![CDATA[ARXIV：2504.05411V1公告类型：新 
摘要：个性检测会自动从各种数据源（例如社交媒体文本）中识别个人的个性。但是，随着语言模型的参数量表的不断增长，计算成本变得越来越难以管理。微调也变得更加复杂，因此很难证明努力并可靠地预测结果。我们介绍了一个新颖的参数效率微调框架Persllm，以应对这些挑战。在Persllm中，大型语言模型（LLM）从原始数据中提取高维表示，并将其存储在动态内存层中。然后，Persllm使用可更换的输出网络更新下游图层，从而可以灵活地适应各种人格检测方案。通过将功能存储在内存层中，我们消除了LLM重复进行复杂计算的需求。同时，轻型输出网络是评估框架总体有效性的代理，从而提高了结果的可预测性。 Kaggle和Pandora等关键基准数据集的实验结果表明，Persllm大大降低了计算成本，同时保持竞争性能和强大的适应性。]]></description>
      <guid>https://arxiv.org/abs/2504.05411</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Presumm：预测汇总性能而无需汇总</title>
      <link>https://arxiv.org/abs/2504.05420</link>
      <description><![CDATA[ARXIV：2504.05420V1公告类型：新 
摘要：尽管自动汇总的最新进展，但最先进的模型并不能很好地总结所有文档，从而提出了一个问题：为什么？尽管先前的研究已广泛分析了摘要模型，但很少关注文档特征在影响汇总性能中的作用。在这项工作中，我们探讨了两个关键的研究问题。首先，文档是否在多个系统上表现出一致的摘要质量？如果是这样，我们可以预测文档的摘要性能而不会产生摘要吗？我们肯定地回答了这两个问题，并介绍了Presumm，这是一个新颖的任务，其中系统仅根据源文档预测汇总性能。我们的分析阐明了具有低预期分数的文档的常见特性，表明它们通常遭受连贯性问题，复杂的内容或缺乏明确主题的困扰。此外，我们在两个关键应用程序中演示了Presumm的实用性：通过确定需要手动摘要的文档来改善混合摘要工作流程，并通过过滤异常值和嘈杂的文档来提高数据集质量。总体而言，我们的发现突出了文档属性在总结性能中的关键作用，并提供了有关当前系统局限性的见解，这些系统可以作为未来改进的基础。]]></description>
      <guid>https://arxiv.org/abs/2504.05420</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于大语模型时代科学发现的假设产生的调查</title>
      <link>https://arxiv.org/abs/2504.05496</link>
      <description><![CDATA[ARXIV：2504.05496V1公告类型：新 
摘要：假设产生是科学发现的基本步骤，但由于信息过载和纪律处分，它越来越受到挑战。大型语言模型（LLM）的最新进展引起了人们对增强和自动化这一过程的潜力的日益兴趣。本文通过（i）审查现有方法，从简单提示技术到更复杂的框架以及提出对这些方法进行分类的分类法，对LLM的假设产生进行了全面调查； （ii）分析改善假设质量的技术，例如提高新颖性和结构化推理； （iii）提供评估策略的概述； （iv）讨论关键挑战和未来的方向，包括多模式整合和人类合作。我们的调查旨在作为研究人员的参考，探索LLM的假设产生。]]></description>
      <guid>https://arxiv.org/abs/2504.05496</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ChartQapro：图表问题回答的更多样化，更具挑战性的基准</title>
      <link>https://arxiv.org/abs/2504.05506</link>
      <description><![CDATA[ARXIV：2504.05506V1公告类型：新 
摘要：图表无处不在，因为人们经常使用它们来分析数据，回答问题并发现关键见解。但是，使用图表执行复杂的分析任务需要重大的感知和认知工作。图表问题回答（CQA）系统通过使模型可以通过数据的可视化表示和推理来自动化此过程。但是，如ChartQa这样的现有基准缺乏现实世界的多样性，并且最近通过现代大型视觉模型（LVLM）显示了性能饱和。为了解决这些限制，我们介绍了ChartQapro，这是一种新的基准测试，其中包括1341个来自157种不同来源的图表，涵盖了各种图表类型，包括信息图表和仪表板，并在各种类型的各种类型的问题上提供了1,948个问题，例如多选择，对话，假设，假设和无可​​争议的问题，以更好地反映现实的挑战。我们使用21个模型的评估表明，ChartQapro上LVLM的性能下降；例如，Claude Sonnet 3.5在ChartQA上得分为90.5％，但ChartQapro仅占55.81％，强调了图表推理的复杂性。我们通过详细的错误分析和消融研究来补充我们的发现，确定了在图表理解和推理中推进LVLM的关键挑战和机会。我们在https://github.com/vis-nlp/chartqapro发布ChartQapro。]]></description>
      <guid>https://arxiv.org/abs/2504.05506</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>历时语言变化的语言模型发现</title>
      <link>https://arxiv.org/abs/2504.05523</link>
      <description><![CDATA[ARXIV：2504.05523V2公告类型：新 
摘要：大型语言模型（LLM）显示出作为科学发现的工具的潜力。这引起了他们在人文学科中的使用，例如历史语言学和文学研究。这些领域通常会根据诸如流派或更灵活的时间段的描述构建论点。尽管已经努力通过微调或模型编辑限制推断特定领域，但我们认为唯一真正的保证是域限制性预处理 - 通常是数据和账单昂贵的命题。
  我们表明，有效的预审进技术可以使与Corpora的有用模型更大，无法轻松检查，但对于“典型” LLM方法而言太小。我们采用一种新颖的日期归因管道，以获取5千万个单词切片的时间细分数据集。我们在这些语料库的细分市场上训练两个相应的五模型电池，有效的预处理和Llama3-8B参数有效地进行了易键调。
  我们发现，经过预告片的模型比易月的基线更快，并且更好地尊重我们语料库的历史分裂。强调速度和精确性在A史综合性上，可以在我们的目标领域中进行许多新颖的假设发现和测试方法。我们将历时语言学作为测试床，表明我们的方法可以检测各种现象，包括共同的词汇变化，非障碍（语法和形态学）变化以及单词感官介绍/过时/过时。我们提供一条现成的管道，该管道允许将我们的方法扩展到其他目标字段，仅适应最少。]]></description>
      <guid>https://arxiv.org/abs/2504.05523</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>与LLM驱动的对话代理商桥接工业专业知识和XR</title>
      <link>https://arxiv.org/abs/2504.05527</link>
      <description><![CDATA[ARXIV：2504.05527V1公告类型：新 
摘要：本文介绍了带有扩展现实（XR）技术的检索增强生成（RAG）增强的大型语言模型（LLM）的新颖集成，以应对工业环境中的知识转移挑战。所提出的系统通过自然语言界面将特定领域的工业知识嵌入XR环境中，从而为工人提供了免提的，可感知的专家指南。我们介绍了提出的系统的体系结构，该系统由LLM聊天引擎组成，其动态工具编排和具有语音驱动交互的XR应用程序。对各种构成策略，嵌入模型和向量数据库的性能评估表明，语义块，平衡的嵌入模型和有效的矢量商店为工业知识检索提供了最佳的性能。该系统的潜力通过在多种工业用例中的早期实施来证明，包括机器人组装，智能基础设施维护和航空航天组件维修。结果表明，在与工业5.0的工业发展中以人为中心和弹性的方式保持一致的培训效率，远程援助能力以及运营指导的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.05527</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>COIG-P：高质量和大规模的中国偏好数据集，以与人类价值保持一致</title>
      <link>https://arxiv.org/abs/2504.05535</link>
      <description><![CDATA[ARXIV：2504.05535V1公告类型：新 
摘要：将大语言模型（LLM）与人类偏好保持一致。但是，现有的中国偏好数据集受小规模，狭窄的域覆盖范围以及缺乏严格的数据验证的限制。另外，对人类注释的指导和响应标签的依赖显着限制了人类偏好数据集的可扩展性。为了应对这些挑战，我们设计了一个基于LLM的中国偏好数据集注释管道，没有人类干预。具体而言，我们爬行并仔细地过滤了92K高质量的中国查询，并采用了15个主流LLM，以生成和评分选择的拒绝响应对。基于它，我们介绍了高质量的中国偏好数据集（中国开放指导通才 - 偏好），包括1,009k的中国偏好对，跨越了6个不同的领域：聊天，代码，数学，逻辑，新颖和角色。在Coig-P的基础上，为了减少使用LLM进行评分的开销，我们训练了一个8B大小的中国奖励模型（CRM），并精心构建了中国奖励基准（CRBENCH）。基于AlignBench \ citep {liu2024AlignBenchbenchMarkingChineSealtigementment}的评估结果表明，GOIG-P显着胜过其他中国偏好数据集，并且在QWEN2/2.5和Infinstruct-Inflincition-Inflation-Inflocter-Inflation-infinstruct-3M-0625M-0625模型中，它的性能改善范围从2％到12％，相应地相应。 CRBENCH的结果表明，我们的CRM具有强大而强大的分数能力。我们将其应用于在COIG-P的测试拆分中过滤所选的响应对，我们的实验表明，在识别低质量样本的同时，在保持效率和成本效益的同时，它与GPT-4O相当。我们的代码和数据以https://github.com/multimodal-art-proctive/coig-p发布。]]></description>
      <guid>https://arxiv.org/abs/2504.05535</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型可以匹配辅导系统适应性吗？一项基准研究</title>
      <link>https://arxiv.org/abs/2504.05570</link>
      <description><![CDATA[ARXIV：2504.05570V1公告类型：新 
摘要：大型语言模型（LLMS）作为动态教学辅助工具有望。然而，尚不清楚LLM是否可以复制智能辅导系统（ITS）的适应性，在该系统中明确建模学生知识和教学策略。我们提出了一个迅速的变体框架，以评估LLM生成的教学动作的适应性和教学声音，从ITS的75个现实世界的辅导场景中。我们从提示中系统地删除关键上下文组件（例如，学生错误和知识组件），以创建每种情况的变化。三个代表性LLM（Llama3-8B，Llama3-70B和GPT-4O）产生1,350个教学动作。我们使用文本嵌入和随机测试来衡量每个上下文特征的遗漏如何影响LLMS的输出（适应性）和经过验证的导师培训分类器来评估响应质量（教学声音）。令人惊讶的是，即使是表现最佳的模型，也只能略微模仿其适应性。具体而言，Llama3-70B表现出对学生错误的统计学显着适应性。尽管Llama3-8B的建议获得的教学声音得分比其他模型更高，但它与指导跟随行为（包括输出格式）斗争。相比之下，GPT-4O可靠地遵守说明，但倾向于提供过度直接的反馈，这些反馈与有效的辅导不同，促使学习者有开放式问题来衡量知识。鉴于这些结果，我们讨论了当前基于LLM的辅导如何产生学习益处可与已知的辅导媲美。通过我们的开源基准测试代码，我们为评估LLMS的教学适应性和忠诚度提供了可再现的方法。]]></description>
      <guid>https://arxiv.org/abs/2504.05570</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>知识教学：使用说明有效地从有限数据中进行有效的持续预训练</title>
      <link>https://arxiv.org/abs/2504.05571</link>
      <description><![CDATA[ARXIV：2504.05571V1公告类型：新 
摘要：虽然大型语言模型（LLMS）在预训练期间获得了广泛的知识，但它们通常缺乏特定领域的，新或利基的信息。持续的预训练（CPT）试图解决这一差距，但在低数据表格中遭受了灾难性的遗忘和效率低下的困扰。我们介绍了知识教学，这是一种新颖的方法，可以通过纯粹的指导进行有限的语料库注入知识。通过生成信息密集的综合指导数据，它可以有效地整合新知识，同时保留一般的推理和跟随能力。知识教学证明了卓越的事实记忆，最大程度地减少了灾难性的遗忘，并通过利用相对较小的语言模型的合成数据来扩展。此外，它增强了上下文理解，包括复杂的多跳推理，促进与检索系统的整合。我们验证了包括公司在内的各种基准的有效性，包括公司，我们发布了一个新数据集，以衡量知识注入能力。]]></description>
      <guid>https://arxiv.org/abs/2504.05571</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DEL：上下文感知的动态出口层，以进行有效的自我指码</title>
      <link>https://arxiv.org/abs/2504.05598</link>
      <description><![CDATA[ARXIV：2504.05598V1公告类型：新 
摘要：投机解码（SD）是一种广泛使用的方法，可以加速大型语言模型（LLM）而不降低发电质量。它首先使用紧凑型模型来有效地草拟多个令牌，然后使用目标llm进行并行验证。与自动回归解码相比，这种方法导致推断更快。尽管有多种创建草稿模型的方法，但一种有前途的方法是使用早期外观方法。这些方法通过使用主要模型的一层并应用剩余层进行验证，从而草拟了候选令牌，从而使单个模型可以同时处理起草和验证。尽管此技术降低了内存的使用和计算成本，但其性能依赖于用于制图的出口层的选择以及在每个SD回合中起草的标记（投机长度）的数量。先前的工作使用超参数探索来静态选择这些值。但是，我们的评估表明，这些超参数值是特定于任务的，即使在任务中，它们都取决于当前的序列上下文。我们介绍了DEL，这是一种插件方法，可以自适应地选择推理期间的出口层和投机长度。如果在LLM的每一层起草令牌，并使用该知识来启发最佳出口层和投机长度，则DEL会动态跟踪令牌的接受率。我们在各种型号和下游任务上进行的实验表明，DEL可实现$ 2.16 \ times $$ $$ \ sim $$ 2.50 \ times $ 2.50 \ times $ a自动回归解码的总体加速，并根据最先进的SD方法提高了最高$ 0.27 \ times $ \ times $ $。]]></description>
      <guid>https://arxiv.org/abs/2504.05598</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于语言细微差别对具有大型语言模型的情感分析的影响：释义，讽刺和表情符号</title>
      <link>https://arxiv.org/abs/2504.05603</link>
      <description><![CDATA[ARXIV：2504.05603V1公告类型：新 
摘要：大型语言模型（LLMS）在包括情感分析在内的各种任务中都表现出了令人印象深刻的表现。但是，数据质量 - 尤其是从社交媒体中采购时 - 可以显着影响其准确性。这项研究探讨了包括表情符号和讽刺在内的文本细微差别如何影响情绪分析，特别着重于通过文本释义技术提高数据质量。为了解决缺乏标记的讽刺数据，作者创建了一个由5929个推文的人体标记的数据集，该数据集可在各种讽刺环境中评估LLM。结果表明，当特定于主题的数据集（例如与核能相关的数据集）被用来填补LLMS时，这些模型无法在存在讽刺的情况下，由于文本较少而无法理解准确的情感，需要诸如讽刺删除之类的外部干预措施以提高模型的准确性。讽刺的去除导致情绪准确性提高了21％，因为在与核电相关的内容中训练了与讽刺推文斗争的LLM，仅达到了30％的精度。相比之下，在一般推文数据集上训练的LLM涵盖了更广泛的主题，在预测讽刺推文的情感方面有很大改善（准确性60％），这表明合并一般文本数据可以增强讽刺检测。这项研究还利用了对抗文本的增强，表明通过进行较小的变化来显着提高模型的鲁棒性和讽刺推文的准确性（约85％）来创建合成文本变体。此外，具有零散语言的推文的文本释义将大约40％的带有低信心标签的推文转化为高信心，将LLMS情感分析精度提高了6％。]]></description>
      <guid>https://arxiv.org/abs/2504.05603</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FACTGUARD：利用多代理系统生成可回答和无法回答的问题，以增强长篇文化LLM提取</title>
      <link>https://arxiv.org/abs/2504.05607</link>
      <description><![CDATA[ARXIV：2504.05607V1公告类型：新 
摘要：提取性阅读理解系统旨在在给定文本中找到对问题的正确答案。但是，持续的挑战在于确保这些模型在回答问题时保持高度准确性，同时可靠地识别无法回答的查询。尽管大型语言模型（LLM）的阅读理解有重大进步，但此问题仍然至关重要，尤其是随着支持环境的长度不断扩大。为了应对这一挑战，我们提出了一种基于多机构协作框架的创新数据增强方法。与传统方法不同，例如诸如小队2.0之类的数据集所需的昂贵人类注释过程，我们的方法自动生成基于证据的问题回答和系统地构建无法回答的问题。使用此方法，我们开发了Factguard Bench数据集，该数据集包含25,220个示例，这些示例既可以回答和无法回答的问题情景，上下文长度从8K到128K不等。在七个流行的LLM上进行的实验评估表明，即使最先进的模型也只能达到61.79％的总体准确性。此外，我们强调了模型推理无法回答的问题的能力的重要性，以避免产生合理但不正确的答案。通过在多代理协作框架内实施有效的数据选择和生成，我们的方法大大降低了与手动注释相关的传统上高成本，并为LLM的培训和优化提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2504.05607</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>两个中间翻译比一个中间翻译要好：用于文档级翻译改进的微调LLM</title>
      <link>https://arxiv.org/abs/2504.05614</link>
      <description><![CDATA[ARXIV：2504.05614V1公告类型：新 
摘要：最近的研究表明，大型语言模型（LLMS）可以通过自我进行提高翻译质量。在本文中，我们通过将细化从句子级别扩展到文档级翻译，特别是专门针对文档对文档（doc2doc）翻译的改进，从而将其扩展到了这一想法。由于句子到句子（已发送2sent）和doc2oc翻译地址转换过程的不同方面，因此我们建议使用两个中间翻译的微调llms进行翻译改进，并结合了send2sent和doc2doc的优势。此外，认识到中间翻译的质量各不相同，我们引入了一种增强的微调方法，具有质量意识，将较低的权重分配给更容易的翻译和更高的权重，从而使模型能够专注于具有挑战性的翻译案例。通过Llama-3-8B教学和Mistral-Nemo-Instruction进行的十项翻译任务的实验结果证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2504.05614</guid>
      <pubDate>Wed, 09 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>