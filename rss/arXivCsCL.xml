<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 26 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>SRMIR：基于内省推理LLM对齐的阴影奖励模型</title>
      <link>https://arxiv.org/abs/2503.18991</link>
      <description><![CDATA[ARXIV：2503.18991V1公告类型：新 
摘要：将大语言模型（LLM）与人类的偏好和价值保持一致，对于应用至关重要。但是，当前的一致性方法面临三个主要局限性：（1）依赖昂贵的人类注释； （2）对齐税； （3）容易受到越狱攻击的浅对齐。此外，当前的对齐数据集经常遭受分布不平衡的困扰，从而导致某些主题和忽视其他主题过多。为了解决这些问题，我们提出了SRMIR（基于内省推理的阴影奖励模型），灵感来自成员推理攻击中的影子模型。我们首先在$ 7 $的有害类型中构建了一个平衡的草稿数据集（COD）数据集，并通过结构化及时利用LLM的内省推理能力，然后训练一组专业奖励模型，以通过小组相对政策优化（GRPO）来指导政策优化。我们采用两种策略，即线性组合和分类方法，以整合影子奖励模型以进行策略优化。相比之下，我们发现尽管计算成本较高，后者仍达到了较高的一致性。几个LLMS的实验证明了SRMIR明显优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2503.18991</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LookAhead调整：通过部分答案预览的更安全的语言模型</title>
      <link>https://arxiv.org/abs/2503.19041</link>
      <description><![CDATA[ARXIV：2503.19041V1公告类型：新 
摘要：微调使大型语言模型（LLMS）适应特定的领域，但经常破坏其先前确定的安全对准。为了减轻微调过程中模型安全性的降解，我们介绍了LookAhead Tuning，其中包括两种简单，低资源且有效的数据驱动方法，这些方法通过预测部分答案前缀来修改培训数据。这两种方法旨在通过最大程度地减少对初始令牌分布的扰动来保护模型的固有安全机制。全面的实验表明，LookAhead调整可以有效地维护模型安全性，而无需牺牲下游任务的稳健性能。我们的发现位置lookahead调整是可靠有效的解决方案，以实现LLM的安全有效适应。代码在https://github.com/zjunlp/lookaheadtuning上发布。]]></description>
      <guid>https://arxiv.org/abs/2503.19041</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于LLM的Insight提取，用于联络中心分析和成本效益的部署</title>
      <link>https://arxiv.org/abs/2503.19090</link>
      <description><![CDATA[ARXIV：2503.19090V1公告类型：新 
摘要：大型语言模型已经改变了联络中心行业，以增强的自助工具，简化的管理流程和增强的代理生产率表现出来。本文描述了我们的系统，该系统可以自动化呼叫驱动程序的生成，该系统是主题建模，传入呼叫分类，趋势检测和FAQ生成等任务的基础，为联系中心代理和管理员提供了可行的见解。我们提出了经济高效的LLM系统设计，1）对专有，开放权重和微调模型的全面评估以及2）具有成本效益的策略，以及3）在生产环境中部署时相应的成本分析。]]></description>
      <guid>https://arxiv.org/abs/2503.19090</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面具和模仿：对作者身份验证的战略混淆和模仿攻击</title>
      <link>https://arxiv.org/abs/2503.19099</link>
      <description><![CDATA[ARXIV：2503.19099V1公告类型：新 
摘要：人工智能（AI）技术的使用日益增加，例如大语言模型（LLMS）导致了各种任务的非平地改进，包括对文档的准确作者身份识别。但是，尽管LLMS改进了这种防御技术，但它们也为恶意演员提供了推出新攻击媒介的工具。为了应对这种安全风险，我们将作者身份模型（特别是作者验证模型）的对抗性鲁棒性评估为有效的基于LLM的攻击。这些攻击包括未靶向的方法 -  \ textIt {authorship obfuscation}和目标方法 -  \ textit {authorship Impersonation}。对于这两种攻击，目的是掩盖或模仿作者的写作风格，同时分别保留原始文本的语义。因此，我们扰动精确的作者身份验证模型，并分别为混淆和假冒攻击达到92 \％和78 \％的最大攻击成功率。]]></description>
      <guid>https://arxiv.org/abs/2503.19099</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解和改善在LLM的迅速压缩中的信息保存</title>
      <link>https://arxiv.org/abs/2503.19114</link>
      <description><![CDATA[ARXIV：2503.19114V1公告类型：新 
摘要：大语言模型（LLMS）的最新进展使他们成功地应用了广泛的任务。但是，在信息密集型任务中，及时长度可以快速增长，从而增加计算要求，性能下降以及无关或冗余信息引起的偏见。最近，已经引入了各种及时的压缩技术，以优化减少输入长度和保持性能之间的权衡。我们提出了一个整体评估框架，可以深入分析及时压缩方法。除压缩比以外，我们专注于三个关键方面：（i）下游任务性能，（ii）在输入上下文中接地，以及（iii）信息保存。通过此框架，我们研究了最先进的软压缩方法，表明它们很难从原始提示中保留关键细节，从而将其绩效限制在复杂的任务上。我们证明，修改软提示方法以更好地控制压缩信息的粒度可以显着提高其有效性 - 在下游任务性能中，高达+23 \％，地接地中的+8 Bertscore点超过8倍，并且在压缩中保留了2.7倍的实体。]]></description>
      <guid>https://arxiv.org/abs/2503.19114</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>这是从哪里来的？在评估文档VQA模型中进行基础计数</title>
      <link>https://arxiv.org/abs/2503.19120</link>
      <description><![CDATA[ARXIV：2503.19120V1公告类型：新 
摘要：文档视觉问题回答（VQA）模型在过去几年中以令人印象深刻的速度发展，在某些基准测试中接近或匹配人类的性能。我们认为，流行基准使用的常见评估指标并不能说明模型输出的语义和多模式基础。结果，幻觉和重大语义错误的处理方式与良好的输出相同，评估得分并不能反映模型的推理能力。作为回应，我们提出了一种新的评估方法，该方法解释了预测的基础，这些方法与输出的语义特征以及输出文档中输出的多模式放置有关。我们提出的方法的参数化是用户可以根据其喜好配置分数的方式。我们使用人类判断来验证我们的评分方法，并显示其对现有流行排行榜的潜在影响。通过广泛的分析，我们证明我们提出的方法产生的分数可以更好地表明模型的鲁棒性，并且倾向于给出更高的奖励，以获得更好地校准的答案。]]></description>
      <guid>https://arxiv.org/abs/2503.19120</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>克服词汇不匹配：词汇 - 敏捷教师指导语言建模</title>
      <link>https://arxiv.org/abs/2503.19123</link>
      <description><![CDATA[ARXIV：2503.19123V1公告类型：新 
摘要：使用大型教师模型指导较小的学生模型的培训已成为高效学习的主要范式。但是，教师和学生语言模型之间的词汇不匹配在语言建模中构成了重大挑战，从而导致令牌序列和输出分布不同。 To overcome these limitations, we propose Vocabulary-agnostic Teacher Guided Language Modeling (VocAgnoLM), a novel approach that bridges the gap caused by vocabulary mismatch through two key methods: (1) Token-level Lexical Alignment, which aligns token sequences across mismatched vocabularies, and (2) Teacher Guided Loss, which leverages the loss of teacher model to guide effective student training.我们使用具有不同词汇的各种7B教师模型通过1B学生模型来证明其在语言建模中的有效性。值得注意的是，随着QWEN2.5-MATH-INSTRUCT，教师模型与Tinyllama只有6％的词汇量，与持续预处理相比，Vocagnolm的性能提高了46％。此外，我们证明了Vocagnolm始终从更强的教师模型中受益，从而为语言建模中的词汇不匹配提供了强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2503.19123</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>海市rage：多模式的沉浸式推理和红线越狱攻击的指导探索</title>
      <link>https://arxiv.org/abs/2503.19134</link>
      <description><![CDATA[ARXIV：2503.19134V1公告类型：新 
摘要：尽管安全机制在过滤有害文本输入方面取得了显着发展，但MLLM仍然容易受到利用其跨模式推理能力的多模式越狱的攻击。我们展示了Mirage，这是一种新型的多式联运越狱框架，它利用了叙事驱动的环境和浸入角色，以规避多模式大语言模型（MLLM）的安全机制。通过系统地将有毒的查询分解为环境，角色和动作三胞胎，Mirage使用稳定的扩散构建了图像和文本的多转视觉故事序列，从而通过引人入胜的侦探叙事来指导目标模型。这个过程逐渐降低了模型的防御能力，并通过结构化的上下文提示巧妙地指导其推理，最终引起了有害的反应。在具有六个主流MLLM的选定数据集上的广泛实验中，Mirage实现了最先进的性能，将攻击成功率提高了最佳基线的17.5％。此外，我们证明了浸入和结构化的语义重建可以激活固有的模型偏见，从而促进模型自发违反道德保障。这些结果突出了当前多模式安全机制中的关键弱点，并强调了迫切需要对跨模式威胁进行更强大的防御能力。]]></description>
      <guid>https://arxiv.org/abs/2503.19134</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语言模型不确定性量化与注意链</title>
      <link>https://arxiv.org/abs/2503.19168</link>
      <description><![CDATA[ARXIV：2503.19168V1公告类型：新 
摘要：准确地量化大型语言模型（LLM）的预测不确定性对于判断其答案的可靠性至关重要。尽管大多数现有的研究都集中在带有封闭形式输出（例如多项选择）的简短，可直接回答的问题上，涉及LLM响应中的中间推理步骤的问题越来越重要。这种增加的复杂性使不确定性定量（UQ）复杂化，因为分配给代币的概率是在推理令牌之前的巨大空间上进行的。直接边缘化是不可行的，依赖性膨胀的概率估计值，导致UQ中的过度自信。为了解决这个问题，我们提出了UQAC，这是一种有效的方法，它将推理空间缩小到边缘化的可拖动大小。 UQAC迭代地构建了通过回溯过程认为最终答案的令牌的“注意链”。从答案令牌开始，它使用注意力权重识别最有影响力的前辈，然后迭代此过程，直到达到输入令牌。相似性滤波和概率阈值进一步完善了所得链，使我们能够近似答案令牌的边际概率，这些链条作为LLM的置信度。我们在具有高级开源LLM的多个推理基准上验证了UQAC，这表明它始终以高计算效率提供可靠的UQ估计值。]]></description>
      <guid>https://arxiv.org/abs/2503.19168</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估LLM中的偏见以进行职务匹配：性别，种族和教育</title>
      <link>https://arxiv.org/abs/2503.19182</link>
      <description><![CDATA[ARXIV：2503.19182V1公告类型：新 
摘要：大型语言模型（LLMS）提供了通过将职位描述与候选简历相匹配，简化招聘流程和降低运营成本来自动化招聘的潜力。但是，这些模型固有的偏见可能导致不公平的招聘实践，加强社会偏见并破坏工作场所的多样性。这项研究检查了LLM在英语和美国背景下的工作库匹配任务中的表现和公平性。它评估了性别，种族和教育背景等因素如何影响模型的决策，从而为人力资源应用程序中LLM的公平性和可靠性提供了重要的见解。我们的发现表明，尽管最近的模型减少了与性别和种族等明确属性有关的偏见，但有关教育背景的隐性偏见仍然很大。这些结果凸显了进行持续评估的必要性以及在行业环境中使用LLM时确保公平招聘实践的高级偏见策略的发展。]]></description>
      <guid>https://arxiv.org/abs/2503.19182</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>蛋白质结构功能关系：一种用于反应坐标识别的内核PCA方法</title>
      <link>https://arxiv.org/abs/2503.19186</link>
      <description><![CDATA[ARXIV：2503.19186V1公告类型：新 
摘要：在这项研究中，我们提出了一个旨在捕获蛋白质中结构功能关系的内核PCA模型。该模型还可以根据反应坐标对蛋白质特性的影响进行排名。通过利用机器学习技术，包括内核和主成分分析（PCA），我们的模型发现了从分子动力学（MD）模拟获得的高维蛋白数据中有意义的模式。我们的模型在准确识别反应坐标方面的有效性已通过应用于G蛋白偶联受体的应用。此外，该模型利用基于网络的方法来发现与特定蛋白质特性相关的残基的动态行为相关性。这些发现强调了我们模型作为蛋白质结构 - 功能分析和可视化的强大工具的潜力。]]></description>
      <guid>https://arxiv.org/abs/2503.19186</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>过度训练的语言模型更难微调</title>
      <link>https://arxiv.org/abs/2503.19206</link>
      <description><![CDATA[ARXIV：2503.19206V1公告类型：新 
摘要：大型语言模型已在不断增长的代币预算上进行了预先培训，因为假设更好的预训练性能转化为改进的下游模型。在这项工作中，我们挑战了这一假设，并表明扩展的预训练可以使模型更难微调，从而导致最终性能降低。我们称这种现象的灾难性过度训练。例如，在3T代币上预先训练的指令调整的OLMO-1B模型导致多个标准LLM基准测试的性能比其2.3T代币对应物差2％。通过对照实验和理论分析，我们表明灾难性过度训练是由于预训练参数对修改的广泛灵敏度的系统性提高，包括但不限于微调。我们的发现要求对预训练设计的重新评估，以考虑模型的下游适应性。]]></description>
      <guid>https://arxiv.org/abs/2503.19206</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>建立阿拉伯语的术语管理自动化</title>
      <link>https://arxiv.org/abs/2503.19211</link>
      <description><![CDATA[ARXIV：2503.19211V1公告类型：新 
摘要：本文介绍了一种用于自动化阿拉伯语术语管理的方法和支持工具。这些工具从外语中提取了并行术语匹配术语的列表，这些术语与字段特定文本中的阿拉伯语对应物。这具有重大含义，因为它可用于改善专业的阿拉伯学术书籍中一致的翻译和用语的使用，并为增强跨语言文本处理提供了自动辅助。术语管理的这种自动化旨在减少处理时间，并确保使用一致和正确的术语。提取利用自然发生的术语翻译。它考虑了几个不同长度的候选短语，这些短语是外国术语旁边同时发生的。然后，它计算几个相似性指标，包括词典，语音，形态和语义的指标来决定问题。我们通过后处理方法尝试了启发式，机器学习和ML。本文报告了一个针对该任务的新型策划数据集，现有的专家审查了行业并行语料库以及三种方法的性能。最佳方法达到了94.9％的精度和92.4％的召回。]]></description>
      <guid>https://arxiv.org/abs/2503.19211</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型代理的调查回答</title>
      <link>https://arxiv.org/abs/2503.19213</link>
      <description><![CDATA[ARXIV：2503.19213V1公告类型：新 
摘要：本文调查了基于大语言模型（LLM）的代理以进行问题答案（QA）。传统代理人面临重大局限性，包括大量数据要求和对新环境的概括。基于LLM的代理商通过利用LLM作为其核心推理引擎来应对这些挑战。与传统的QA管道和Naive LLM QA系统相比，这些代理通过与外部环境相互作用而获得了质量质量的结果。我们会系统地回顾质量检查任务的设计LLM代理的设计，在关键阶段组织我们的讨论：计划，问题理解，信息检索和回答生成。此外，本文确定了正在进行的挑战，并探讨了未来的研究方向，以增强LLM Agent QA系统的性能。]]></description>
      <guid>https://arxiv.org/abs/2503.19213</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Sci-Idea：使用令牌和句子嵌入的上下文感知科学意识</title>
      <link>https://arxiv.org/abs/2503.19257</link>
      <description><![CDATA[ARXIV：2503.19257V1公告类型：新 
摘要：每个科学发现都始于一个受到先前工作，跨学科概念和新兴挑战的想法。对科学语料库进行培训的大型语言模型（LLM）的最新进展引起了人们对AI支持的想法产生的兴趣。但是，产生背景感，高质量和创新思想仍然具有挑战性。我们介绍了Sci-Idea，该框架使用LLM提示策略和AHA时刻检测进行迭代思想的完善。 Sci-Idea提取了研究出版物的基本方面，评估了有关新颖，兴奋，可行性和有效性的产生的想法。全面的实验验证了Sci-IDEA的有效性，平均得分分别在新颖，兴奋，可行性和有效性的范围内分别达到6.84、6.86、6.89和6.84（以1-10的比例）。使用GPT-4O，GPT-4.5，DeepSeek-32B（每个下方的提示下）和DeepSeek-70B（3-shot提示）采用的评估，并带有用于AHA力矩检测的令牌级嵌入。同样，它的得分达到6.87、6.86、6.83和6.87，使用GPT-4O在5次提示下使用GPT-4O，GPT-4.5在3次及3次提示下，DeepSeek-32B，零射链链链的提示下的DeepSeek-32b，在零链链链中，DeepSeek-70b在5次的句子下方，以句子级别的固定剂量提示。我们还解决了诸如智力信用，潜在滥用和平衡人类创造力与AI驱动的构想之类的道德考虑因素。我们的结果凸显了Sci-Idea促进对情境感知的科学思想的结构化和灵活探索的潜力，同时支持创新，同时保持道德标准。]]></description>
      <guid>https://arxiv.org/abs/2503.19257</guid>
      <pubDate>Wed, 26 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>