<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上进行更新。</description>
    <lastBuildDate>Wed, 27 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>$n$-gram 平滑在神经网络时代的作用</title>
      <link>https://arxiv.org/abs/2403.17240</link>
      <description><![CDATA[arXiv:2403.17240v1 公告类型：新
摘要：近三十年来，从 $n$-gram 假设导出的语言模型一直保持着该任务的最先进水平。他们成功的关键在于应用各种平滑技术来对抗过度拟合。然而，当神经语言模型取代 $n$-gram 模型成为表现最佳的模型时，$n$-gram 平滑技术就变得不那么重要了。事实上，毫不夸张地说，对 $n$-gram 平滑技术的研究已经处于休眠状态。本文重新阐述了经典的 $n$-gram 平滑技术在神经语言模型时代可能发挥的作用。首先，我们在标签平滑（一种流行的神经语言模型正则化技术）和 add-$\lambda$ 平滑之间建立了形式上的等价关系。其次，我们推导了一个通用框架，用于将 \emph{any} $n$-gram 平滑技术转换为与神经语言模型兼容的正则化器。我们的实证结果发现，我们的新型正则化器在语言建模和机器翻译方面与标签平滑相当，甚至有时优于标签平滑。]]></description>
      <guid>https://arxiv.org/abs/2403.17240</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>SPLICE：用于共指解析的单例增强管道</title>
      <link>https://arxiv.org/abs/2403.17245</link>
      <description><![CDATA[arXiv:2403.17245v1 公告类型：新
摘要：单例提及，即在文本中仅提及一次的实体，对于人类如何从理论角度理解话语非常重要。然而，之前将其检测纳入英语端到端神经共指解析的尝试因 OntoNotes 基准中缺乏单例提及范围而受到阻碍。本文通过结合现有嵌套 NER 系统的预测提及和源自 OntoNotes 语法树的功能来解决此限制。通过这种方法，我们创建了包含所有单例提及的 OntoNotes 数据集的近似值，在黄金单例样本上实现了约 94% 的召回率。然后，我们提出了一个名为 SPLICE 的两步神经提及和共指解析系统，并在两种场景下将其性能与端到端方法进行比较：OntoNotes 测试集和域外 (OOD) OntoGUM 语料库。结果表明，重建的单例训练产生的结果与 OntoNotes 的端到端系统相当，同时提高了 OOD 稳定性（+1.1 平均 F1）。我们对提及检测进行了错误分析，并深入研究了其对共指聚类的影响，结果表明，在解决共指链方面，精度的提高比召回率的增加带来了更实质性的好处。]]></description>
      <guid>https://arxiv.org/abs/2403.17245</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>使用自然语言推理和概念嵌入的本体完成：分析</title>
      <link>https://arxiv.org/abs/2403.17216</link>
      <description><![CDATA[arXiv:2403.17216v1 公告类型：新
摘要：我们将寻找给定本体中缺失的合理知识的问题视为经过充分研究的分类扩展任务的概括。其中一项工作将此任务视为自然语言推理（NLI）问题，从而依靠语言模型捕获的知识来识别缺失的知识。另一项工作使用概念嵌入来识别不同概念的共同点，从基于类别的归纳的认知模型中获取灵感。这两种方法直观上是互补的，但其有效性尚未进行比较。在本文中，我们介绍了评估本体完成方法的基准，并彻底分析了两种方法的优缺点。我们发现这两种方法确实是互补的，混合策略可以获得最佳的总体结果。我们还发现，即使在微调之后，这项任务对于大型语言模型来说也极具挑战性。]]></description>
      <guid>https://arxiv.org/abs/2403.17216</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:20 GMT</pubDate>
    </item>
    <item>
      <title>使句子嵌入对用户生成的内容具有鲁棒性</title>
      <link>https://arxiv.org/abs/2403.17220</link>
      <description><![CDATA[arXiv:2403.17220v1 公告类型：新
摘要：众所周知，NLP 模型在用户生成内容（UGC）上表现不佳，主要是因为它呈现出大量词汇变化，并且偏离了大多数模型所训练的标准文本。在这项工作中，我们关注 LASER（一种句子嵌入模型）对 UGC 数据的鲁棒性。我们通过 LASER 表示非标准句子及其在嵌入空间中彼此接近的标准对应句子的能力来评估这种鲁棒性。受到之前将 LASER 扩展到其他语言和模式的工作的启发，我们提出了 RoLASER，这是一种强大的英语编码器，使用师生方法进行训练，以减少标准句子和 UGC 句子表示之间的距离。我们表明，通过仅对标准和合成的 UGC 数据进行训练，RoLASER 显着提高了 LASER 对自然和人工 UGC 数据的鲁棒性，得分提高了 2 倍和 11 倍。我们还对人工 UGC 数据进行了细粒度分析，发现我们的模型在最具挑战性的 UGC 现象（例如键盘拼写错误和社交媒体缩写）上远远优于 LASER。对下游任务的评估表明，RoLASER 在标准数据上的表现与 LASER 相当或更好，而在 UGC 数据上始终优于 LASER。]]></description>
      <guid>https://arxiv.org/abs/2403.17220</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:20 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4 至少能像人类一样理解话语</title>
      <link>https://arxiv.org/abs/2403.17196</link>
      <description><![CDATA[arXiv:2403.17196v1 公告类型：新
摘要：我们使用标准化的话语理解测试来测试领先的人工智能系统 GPT-4 是否能像人类一样理解话语。参与者会看到简短的故事，然后回答八个是/否问题，探究他们对故事的理解。这些问题的格式旨在评估直接性（明示与暗示）和显着性（主要思想与细节）的单独影响。鉴于人类的表现水平非常高，GPT-4 的表现略好于人类，但在统计上并不显着。 GPT-4 和人类都表现出强大的能力，可以对故事中未明确陈述的信息进行推断，这是对理解力的关键考验。]]></description>
      <guid>https://arxiv.org/abs/2403.17196</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>从临床精神病学笔记中提取社会支持和社会隔离信息：比较基于规则的 NLP 系统和大型语言模型</title>
      <link>https://arxiv.org/abs/2403.17199</link>
      <description><![CDATA[arXiv:2403.17199v1 公告类型：新
摘要：背景：社会支持（SS）和社会隔离（SI）是与精神病结果相关的健康社会决定因素（SDOH）。在电子健康记录 (EHR) 中，个人级别的 SS/SI 通常记录为叙述性临床记录，而不是结构化编码数据。自然语言处理 (NLP) 算法可以自动化原本劳动密集型的数据提取过程。
  数据和方法：对西奈山卫生系统 (MSHS，n=300) 和威尔康奈尔医学 (WCM，n=225) 的精神病治疗笔记进行了注释并建立了金标准语料库。使用 FLAN-T5-XL 开发了一个涉及词典和大型语言模型 (LLM) 的基于规则的系统 (RBS)，用于识别 SS 和 SI 及其子类别（例如社交网络、工具支持和孤独）的提及。
  结果：对于提取 SS/SI，RBS 在 MSHS（0.89 vs. 0.65）和 WCM（0.85 vs. 0.82）方面均获得了比 LLM 更高的宏观平均 f 分数。在提取子类别方面，RBS 在 MSHS（0.90 vs. 0.62）和 WCM（0.82 vs. 0.81）方面也优于 LLM。
  讨论和结论：出乎意料的是，苏格兰皇家商学院在所有指标上都优于法学硕士。深入审查表明，这一发现是由于苏格兰皇家银行和法学硕士采取的不同方法造成的。 RBS 的设计和改进遵循与黄金标准注释相同的特定规则。相反，法学硕士在分类上更具包容性，并且符合通用的英语语言理解。这两种方法都有优势，并且可以开源以供将来测试。]]></description>
      <guid>https://arxiv.org/abs/2403.17199</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>用于基于插入的后门攻击的任务无关检测器</title>
      <link>https://arxiv.org/abs/2403.17155</link>
      <description><![CDATA[arXiv:2403.17155v1 公告类型：新
摘要：文本后门攻击构成重大安全威胁。当前的检测方法通常依赖于中间特征表示或重建潜在的触发因素，是特定于任务的，并且除了句子分类之外效率较低，难以应对问答和命名实体识别等任务。我们介绍 TABDet（任务无关后门检测器），这是一种开创性的任务无关后门检测方法。 TABDet 利用最后一层 logits 与高效池化技术相结合，实现了三个重要 NLP 任务的统一 logit 表示。 TABDet 可以共同学习不同的特定任务模型，表现出比传统特定任务方法更优越的检测效果。]]></description>
      <guid>https://arxiv.org/abs/2403.17155</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>反映男性目光：量化 19 世纪和 20 世纪小说中的女性客体化</title>
      <link>https://arxiv.org/abs/2403.17158</link>
      <description><![CDATA[arXiv:2403.17158v1 公告类型：新
摘要：受文学和媒体研究中男性凝视概念（Mulvey，1975）的启发，本文提出了一个分析女性客体化方面的性别偏见的框架：文本将女性个体描绘为视觉愉悦对象的程度。我们的框架沿着两个轴衡量女性物化。首先，我们计算代理偏差分数，该分数表明男性实体是否比女性实体更有可能作为语法代理出现在文本中。接下来，通过分析文本引起的词嵌入空间（Caliskan et al., 2017），我们计算了一个外观偏差分数，该分数表明女性实体是否比男性实体与外观相关单词的关联更紧密。将我们的框架应用到 19 世纪和 20 世纪的小说中，揭示了文学中女性客体化的证据：我们发现从男性角度写的小说系统地客体化了女性角色，而从女性角度写的小说并没有表现出统计上显着的任何性别的客体化。]]></description>
      <guid>https://arxiv.org/abs/2403.17158</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>NUMTEMP：使用统计和时间表达式验证声明的现实世界基准</title>
      <link>https://arxiv.org/abs/2403.17169</link>
      <description><![CDATA[arXiv:2403.17169v1 公告类型：新
摘要：为了解决数字时代日益增长的错误信息，自动事实检查引起了人们的极大兴趣。现有系统主要关注维基百科上的综合声明，在现实世界的声明上也取得了显着的进展。在这项工作中，我们发布了 Numtemp，这是一个多样化的多领域数据集，专门关注数字主张，涵盖时间、统计和多个方面，具有细粒度的元数据和无泄漏的证据收集。这解决了验证现实世界数字声明的挑战，这些数字声明很复杂，而且往往缺乏精确的信息，主要关注合成声明的现有工作没有解决这一问题。我们评估并量化现有解决方案在验证数值声明任务方面的局限性。我们还评估了基于权利要求分解的方法、基于数值理解的模型，我们的最佳基线达到了 58.32 的宏观 F1。这表明 Numtemp 是用于数值声明验证的具有挑战性的评估集。]]></description>
      <guid>https://arxiv.org/abs/2403.17169</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>多语言关系抽取数据的引导远程监督：适应新语言</title>
      <link>https://arxiv.org/abs/2403.17143</link>
      <description><![CDATA[arXiv:2403.17143v1 公告类型：新
摘要：关系提取对于数字人文及相关学科背景下提取和理解传记信息至关重要。社区对构建能够训练机器学习模型以提取关系的数据集越来越感兴趣。然而，除了仅限于英语之外，对此类数据集进行注释可能既昂贵又耗时。本文应用引导式远程监督来创建一个大型的德语传记关系提取数据集。我们的数据集由九种关系类型的 80,000 多个实例组成，是最大的德国传记关系提取数据集。我们还创建了一个包含 2000 个实例的手动注释数据集来评估模型，并将其与使用引导远程监督编译的数据集一起发布。我们在自动创建的数据集上训练并发布了几种最先进的机器学习模型。此外，我们还进行了多语言和跨语言实验，这可以使许多资源匮乏的语言受益。]]></description>
      <guid>https://arxiv.org/abs/2403.17143</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>用于反击仇恨言论的结果受限的大型语言模型</title>
      <link>https://arxiv.org/abs/2403.17146</link>
      <description><![CDATA[arXiv:2403.17146v1 公告类型：新
摘要：挑战或回应仇恨言论的反言论已被视为减轻仇恨言论负面影响和促进富有成效的在线交流的替代方案。研究工作的重点是使用语言模型自动生成反驳言论，以协助打击网络仇恨。现有的研究重点是生成具有某些语言属性的反言语，例如礼貌、信息丰富和意图驱动。然而，目前尚不清楚反言论在网络环境中可能产生什么影响。我们首先探索利用大语言模型（LLM）来生成受潜在对话结果限制的反言语的方法。我们构建了两个对话结果分类器，用于预测使用 Reddit 数据回复仇恨后的不文明程度和仇恨者再进入行为，然后提出四种方法将期望的结果（即低对话不文明性和非仇恨性仇恨者再进入）纳入文本生成中流程，包括提示提示、提示和选择、LLM 微调和 LLM 变压器强化学习 (TRL)。评估结果显示了生成结果受限的反语音的有效策略以及不同方法生成的文本的语言特征。]]></description>
      <guid>https://arxiv.org/abs/2403.17146</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中先验知识的强大吸引力及其对情绪识别的影响</title>
      <link>https://arxiv.org/abs/2403.17125</link>
      <description><![CDATA[arXiv:2403.17125v1 公告类型：新
摘要：与传统的基于梯度的微调相比，上下文学习（ICL）已成为使用大型语言模型（LLM）执行自然语言任务而无需更新模型参数的强大范例。 ICL 的承诺是，法学硕士可以适应以具有竞争力或最先进的水平以一小部分成本执行当前任务。法学硕士以这种少量方式执行任务的能力依赖于他们对任务（或任务先验）的背景知识。然而，最近的研究发现，与传统学习不同，法学硕士无法完全整合来自对比任务先验的演示的信息。这可能会导致性能饱和在次优水平，特别是对于情感识别等主观任务，其中由于人类注释的可变性，从文本到情感的映射可能存在很大差异。在这项工作中，我们设计了实验并提出了测量方法，以明确量化法学硕士先验代理的一致性及其对后验的影响。我们表明，法学硕士在情感识别方面具有强大但不一致的先验，这使他们的预测变得僵化。我们还发现模型越大，这些效应就越强。我们的结果表明，在将 ICL 与较大的法学硕士一起用于预训练领域之外的以情感为中心的任务以及解释 ICL 结果时需要谨慎。]]></description>
      <guid>https://arxiv.org/abs/2403.17125</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>探索跨疾病的癌症临床试验资格分类器的泛化</title>
      <link>https://arxiv.org/abs/2403.17135</link>
      <description><![CDATA[arXiv:2403.17135v1 公告类型：新
摘要：临床试验在医学研究中至关重要，NLP 在招募中的应用可以提高其成功率。本研究旨在评估资格分类在广泛的临床试验中的普遍性。从 3 期癌症试验开始，注释有七项资格排除，然后确定模型可以在多大程度上推广到非癌症和非 3 期试验。为了评估这一点，我们编制了五类试验的资格标准数据：(1) 额外的 3 期癌症试验，(2) 1 期和 2 期癌症试验，(3) 心脏病试验，(4) 2 型糖尿病试验， (5) 针对任何疾病的观察性试验，包括七种排除类型的 2,490 项带注释的资格标准。我们的结果表明，在广泛的癌症数据集上训练的模型可以有效地处理非癌症试验中常见的标准，例如自身免疫性疾病。然而，他们在癌症试验中与不成比例地普遍存在的标准作斗争，比如既往恶性肿瘤。我们还尝试了少样本学习，证明有限数量的特定疾病示例可以部分克服这种性能差距。我们正在发布这个带有注释的资格声明的新数据集，以促进临床试验分类中跨疾病泛化的发展。]]></description>
      <guid>https://arxiv.org/abs/2403.17135</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>MetaAligner：语言模型可泛化多目标对齐的条件弱到强校正</title>
      <link>https://arxiv.org/abs/2403.17141</link>
      <description><![CDATA[arXiv:2403.17141v1 公告类型：新
摘要：大型语言模型（LLM）的最新进展旨在通过多目标偏好调整来解决异质的人类期望和价值观。然而，现有方法与策略模型的参数一致，导致两个关键限制：（1）针对每个新目标模型重复对齐算法的成本很高； (2) 由于其静态对齐目标，它们无法扩展到看不见的目标。在这项工作中，我们提出了元目标对齐器（MetaAligner），这是一种对弱响应进行条件弱到强校正以接近强响应的模型。 MetaAligner 是第一个与策略无关且可推广的多目标偏好对齐方法，它通过将参数更新与策略模型解耦来实现即插即用对齐，并通过上下文学习促进未见目标的零样本偏好对齐。实验结果表明，MetaAligner 在 11 个策​​略模型上实现了多目标对齐的显着且均衡的改进，参数增加了 63 倍，并且优于之前的对齐方法，计算资源减少了 22.27 倍。该模型还准确地与看不见的目标对齐，标志着迈向可推广的多目标偏好对齐的第一步。]]></description>
      <guid>https://arxiv.org/abs/2403.17141</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>先属性，后生成：本地可归因的接地文本生成</title>
      <link>https://arxiv.org/abs/2403.17104</link>
      <description><![CDATA[arXiv:2403.17104v1 公告类型：新
摘要：最近解决大型语言模型（LLM）中的幻觉的努力主要集中在归因文本生成上，它通过引用支持来源来补充生成的文本，以进行生成后的事实检查和更正。然而，这些引文通常指向整个文档或段落，给用户带来大量验证工作的负担。在本文中，我们介绍了一种本地归因文本生成方法，优先考虑简洁的归因。我们的方法名为“先属性，然后生成”，将传统的端到端生成过程分解为三个直观的步骤：内容选择、句子规划和顺序句子生成。通过最初识别相关的源片段（“首先选择”），然后调整它们的生成过程（“然后生成”），我们确保这些片段也充当输出的细粒度属性（“选择”）成为“属性”）。经过多文档摘要和长格式问答测试，我们的方法不仅产生比基线更简洁的引文，而且还保持（在某些情况下增强）生成质量和归因准确性。此外，它还大大减少了人类评估员进行事实验证所需的时间。]]></description>
      <guid>https://arxiv.org/abs/2403.17104</guid>
      <pubDate>Wed, 27 Mar 2024 06:17:15 GMT</pubDate>
    </item>
    </channel>
</rss>