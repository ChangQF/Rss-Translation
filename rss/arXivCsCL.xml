<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://arxiv.org/</link>
    <description>arXiv.org 电子打印档案上的计算机科学 - 计算和语言 (cs.CL) 更新</description>
    <lastBuildDate>Mon, 29 Jan 2024 03:14:29 GMT</lastBuildDate>
    <item>
      <title>迈向实用的自动语音识别和后处理：呼吁可解释的错误基准指南。 （arXiv：2401.14625v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14625</link>
      <description><![CDATA[自动语音识别 (ASR) 结果作为下游的输入
任务，极大地影响最终用户的满意度。因此，
诊断和增强ASR模型中存在的漏洞
具有重要意义。然而，传统的 ASR 评估方法
系统生成单一的、复合的定量指标，该指标无法
提供对特定漏洞的全面洞察。这种缺乏
细节延伸到后处理阶段，导致进一步混淆
潜在的弱点。尽管 ASR 模型能够识别话语
准确地说，低于标准的可读性会对用户满意度产生负面影响，从而
需要在识别准确性和用户友好性之间进行权衡。到
有效地解决这个问题，必须同时考虑语音层面，
对于识别准确性至关重要，而文本级别对于
用户友好性。因此，我们建议开发一个错误
可解释基准（EEB）数据集。该数据集同时考虑了两者
语音和文本级别，可以更精细地理解模型的
缺点。我们的主张提供了一个结构化的途径，以实现更多
“以现实世界为中心”的评估，从抽象的、
传统方法，允许检测和纠正细微差别
系统弱点，最终目的是改善用户体验。
]]></description>
      <guid>http://arxiv.org/abs/2401.14625</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:29 GMT</pubDate>
    </item>
    <item>
      <title>通过多代理对话提高诊断准确性：使用大型语言模型减轻认知偏差。 （arXiv：2401.14589v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14589</link>
      <description><![CDATA[背景：临床决策中的认知偏差显着
导致诊断错误和患者结果不佳。寻址
这些偏见给医学领域带来了巨大的挑战。这项研究
探讨大型语言模型 (LLM) 在减轻这些偏见方面的作用
通过利用多代理框架。我们模拟临床
通过多智能体对话进行决策过程并评估其
提高诊断准确性的功效。方法：共有16种已发表的方法
未发表的认知偏差导致误诊的病例报告
是从文献中鉴定出来的。在多智能体系统中，我们利用
GPT-4 Turbo 促进四个模拟代理之间的交互以进行复制
临床团队动态。每个代理都有不同的角色： 1) 进行初始
以及考虑讨论后的最终诊断，2）魔鬼代言人
以及正确的确认和锚定偏差，3）导师和促进者
减少过早关闭偏差的讨论，以及 4) 记录和总结
调查结果。总共评估了 80 次模拟的准确性
初步诊断、顶级鉴别诊断和最终二级鉴别诊断
诊断。结果：总共 80 份回复评估了初始和
最终诊断的准确性为 0% (0/80)，但是
经过多主体讨论后，顶部微分的准确性
诊断率提高至 71.3% (57/80)，最后两项鉴别诊断
诊断率达到 80.0% (64/80)。该系统展示了重新评估的能力
并纠正误解，即使在最初具有误导性的情况下
调查。解读：LLM驱动的多智能体对话系统
在诊断具有挑战性的情况下显示出提高诊断准确性的希望
医疗场景。
]]></description>
      <guid>http://arxiv.org/abs/2401.14589</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>另类言语：反叙事的补充方法以获得更好的话语。 （arXiv：2401.14616v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14616</link>
      <description><![CDATA[我们引入了“另类语音”的概念，作为直接
打击仇恨言论并补充反叙事的局限性。一个
另类言论为现实世界中的仇恨言论提供了实用的替代方案
通过向说话者提供语音级别校正，同时考虑
周围环境并促进演讲者改革。此外，一个
另类言论可以与反叙事一起对抗仇恨言论，
提供有用的工具来解决种族歧视等社会问题
和性别不平等。我们提出新的概念并提供详细的
构建必要数据集的指南。通过讨论，我们
证明将替代性言论和反叙事相结合可以是一种
通过补充特异性来打击仇恨言论的更有效策略
和反叙事的引导能力。本文提出了另一种
处理仇恨言论的观点，提供可行的补救措施
补充当前减轻有害偏见方法的限制。
]]></description>
      <guid>http://arxiv.org/abs/2401.14616</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>CC 查询：从公共语料库中挖掘大规模领域特定知识。 （arXiv：2401.14624v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14624</link>
      <description><![CDATA[大型语言模型在各个领域都展现出了巨大的潜力
然而，开源模型和任务仍然严重缺乏
特定领域的数据。以前的作品主要集中在手动
指定资源并收集特定领域的高质量数据，
这极大地消耗了时间和精力。为了解决这个限制，我们
提出一种基于大数据的高效数据采集方法~\textit{CC的查询}
语言模型。该方法通过大量引导种子信息
语言模型并从公共语料库中检索相关数据。它不仅
收集特定领域的知识相关数据，但挖掘数据
潜在的推理过程。通过应用该方法，我们有
策划了一个名为~\textsc{知识堆}的高质量数据集，包括
四大领域，包括干科学和人文科学等。
实验结果表明~\textsc{知识堆}显着
提高大型语言模型在数学和数学方面的性能
与知识相关的推理能力测试。为了方便学术分享，我们
开源我们的数据集和代码，为学术界提供宝贵的支持
社区。
]]></description>
      <guid>http://arxiv.org/abs/2401.14624</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:28 GMT</pubDate>
    </item>
    <item>
      <title>不要（总是）看起来正确：研究基于解码器的大型语言模型的序列标记功能。 （arXiv：2401.14556v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14556</link>
      <description><![CDATA[基于掩码语言建模 (MLM) 目标的预训练语言模型
擅长自然语言理解 (NLU) 任务。虽然基于传销进行了微调
编码器始终优于因果语言建模解码器
类似的大小，将解码器模型扩展到数十亿的最新趋势
参数导致大型语言模型（LLM），使其具有竞争力
使用基于 MLM 的编码器。尽管规模扩大了他们在 NLU 任务中的能力，
法学硕士在信息提取 (IE) 任务中达不到 SOTA 结果，许多
框架为序列标记（SL）。然而，这是否是一个内在的
LLM 的局限性或他们的 SL 表现是否可以提高仍然存在
不清楚。为了解决这个问题，我们探索了增强 SL 性能的策略
关于 IE 任务的“开放”法学硕士（Llama2 和 Mistral）。我们调查双向
解码器块组内的信息流，应用逐层去除
或在 LLM 微调期间强制执行因果掩模 (CM)。这种方法
产生与 SOTA SL 型号竞争的性能增益，匹配或
优于从所有块中去除 CM 的结果。我们的研究结果适用于
多样化的 SL 任务，证明“开放”的 LLM 具有层相关的 CM 去除功能
优于基于 MLM 的强大编码器和指令调整的 LLM。然而，我们
观察到小规模去除 CM 时没有影响
等效模型大小、预训练步骤以及预训练和微调
数据。
]]></description>
      <guid>http://arxiv.org/abs/2401.14556</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>自适应机器翻译的语言建模方法。 （arXiv：2401.14559v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14559</link>
      <description><![CDATA[一致性是高质量翻译的关键要求。这是
尤其重要的是遵守预先批准的术语并适应
更正了特定领域项目中的翻译。机器翻译（MT）
在领域适应领域取得了重大进展。然而，
领域内数据稀缺在翻译环境中很常见，因为缺乏
专业数据集和术语，或不一致和不准确
可用的域内翻译。在这种资源不足的场景下
域内数据来微调机器翻译模型，生成的翻译
与相关背景保持一致是具有挑战性的。在实时适应的同时
可以利用少量的域内数据来改进翻译
苍蝇，由于支持的上下文限制，它仍然具有挑战性
效率限制。大型语言模型（LLM）最近表明
情境学习的有趣能力，他们学会复制
某些输入输出文本生成模式，无需进一步微调。
这些功能为特定领域的数据开辟了新的视野
增强和实时自适应机器翻译。这项工作试图解决两个主要问题
相关问题：1）在涉及人机交互和连续的场景中
反馈，我们能否利用语言模型来提高自适应机器翻译的质量
在推理时？ 2）在缺乏足够的域内数据的情况下，我们能否
使用预训练的大规模语言模型来改进MT领域的流程
适应？
]]></description>
      <guid>http://arxiv.org/abs/2401.14559</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>通过将语言识别与傅里叶分析相结合来检测历史文献中的结构化语言变化。 （arXiv：2401.14569v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14569</link>
      <description><![CDATA[在这项研究中，我们提出了一个通用的工作流程来识别文档
具有非标准语言和文字组合的历史语言，
阿尔梅诺-土耳其语。我们介绍了检测不同模式的任务
基于结构化语言交替频率的多语言能力
在一个文档内。
]]></description>
      <guid>http://arxiv.org/abs/2401.14569</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:27 GMT</pubDate>
    </item>
    <item>
      <title>PET 的 MED：潜在委婉术语的多语言委婉语消歧。 （arXiv：2401.14526v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14526</link>
      <description><![CDATA[这项研究调查了委婉语的计算处理，
跨多种语言的普遍语言现象。我们训练一个
多语言 Transformer 模型 (XLM-RoBERTa) 可以消除潜在的歧义
多语言和跨语言环境中的委婉术语 (PET)。排队
根据当前趋势，我们证明了跨语言的零样本学习
发生。我们还展示了多语言模型在
与单语模型相比，任务具有统计学上显着的优势，
表明多语言数据为模型提供了额外的机会
了解委婉语的跨语言、计算特性。在一个
后续分析，我们关注普遍委婉的“类别”，例如
死亡和身体机能等。我们测试一下是否跨语言
同一领域的数据比其他领域的语言内数据更重要
域以进一步了解跨语言迁移的本质。
]]></description>
      <guid>http://arxiv.org/abs/2401.14526</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的相对值偏差。 （arXiv：2401.14530v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14530</link>
      <description><![CDATA[对人类和动物的强化学习研究表明
偏好过去产生相对更好结果的选项，
即使这些选项与较低的绝对奖励相关。现在的
研究测试了大型语言模型是否会表现出类似的偏差。我们有
gpt-4-1106-preview (GPT-4 Turbo) 和 Llama-2-70B 之间反复选择
以收益最大化为目标的选项对。完整记录
每个提示中都包含以前的结果。两种模型均表现出相对
价值决策偏差与在人类和动物中观察到的类似。制作
结果之间的相对比较更明确地放大了偏差，而
促使模型估计预期结果导致了偏差
消失。这些结果对潜在机制有影响
有助于人类代理人的上下文相关选择。
]]></description>
      <guid>http://arxiv.org/abs/2401.14530</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:26 GMT</pubDate>
    </item>
    <item>
      <title>同理心和成为例外的权利：法学硕士能做什么和不能做什么。 （arXiv：2401.14523v1 [cs.CY]）</title>
      <link>http://arxiv.org/abs/2401.14523</link>
      <description><![CDATA[大型语言模型 (LLM) 性能的进步已经导致一些
研究人员提出心智理论（ToM）在人工智能中的出现
智能（AI）。法学硕士可以归因于信念、愿望、意图和
情绪，他们的准确性就会提高。而不是采用
这是人类特有的同理心方法，他们学会将心理归因于
通过识别数据集中通常不会出现的语言模式来表示
包括那个人。我们询问法学硕士缺乏同理心是否会妨碍他们
他们不尊重个人作为例外的权利，即
对性格进行评估并预测行为，以反映
对一个人的个性的适当敏感性。 LLM可以认真吗
考虑个人的主张，即他们的情况根据内部情况有所不同
诸如信念、欲望和意图之类的精神状态，或者它们仅限于
根据与其他案例的相似性来判断该案例？我们建议
移情方法对于尊重成为人的权利具有特殊意义
与预测准确性的值不同的例外，其中法学硕士
出类拔萃。最后，我们考虑是否使用同理心来考虑例外情况
案例具有内在的或仅仅是实用的价值，我们引入概念和
推进这项调查的实证途径。
]]></description>
      <guid>http://arxiv.org/abs/2401.14523</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>评估 GPT-3.5 对具有共享主题的欧洲宪法文本的认知和总结能力。 （arXiv：2401.14524v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14524</link>
      <description><![CDATA[宪法是支撑政府运作的基础性法律文件
和社会结构。因此，它们是一个民族文化的反映。
和社会独特性，也有助于建立普遍的话题
重要性，如公民的权利和义务 (RD)。在这项工作中，使用
著名的 GPT-3.5，我们利用生成式大语言模型来理解
超越国界的宪法段落。关键贡献
我们研究的重点是引入抽象的新颖应用
对多来源宪法文本收集的总结
重点关注欧洲国家宪法中与RD主题相关的段落。我们的
结果表明 GPT-3.5 在产生信息丰富、连贯和
忠实地总结了欧洲各国的研发主题。
]]></description>
      <guid>http://arxiv.org/abs/2401.14524</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:25 GMT</pubDate>
    </item>
    <item>
      <title>Wordflow：大型语言模型的社交提示工程。 （arXiv：2401.14447v1 [cs.HC]）</title>
      <link>http://arxiv.org/abs/2401.14447</link>
      <description><![CDATA[大型语言模型 (LLM) 需要精心设计的提示才能有效使用。
提示工程，设计提示的过程，是具有挑战性的，
特别是对于不太熟悉人工智能技术的非专家来说。尽管
研究人员提出了技术和工具来帮助法学硕士用户及时
设计，这些作品主要针对人工智能应用程序开发人员，而不是
非专家。为了解决这一研究空白，我们提出社会提示
工程，一种利用社交计算技术的新颖范式
促进协作提示设计。调查社交提示
工程方面，我们引入了 Wordflow，一个开源的社交文本编辑器，
使日常用户能够轻松创建、运行、共享和发现 LLM 提示。
此外，通过利用现代网络技术，Wordflow 允许用户
在浏览器中本地私下运行法学硕士。两个使用场景凸显
社交提示工程和我们的工具如何增强外行人的互动
与法学硕士。 Wordflow 可在以下位置公开访问：
https://poloclub.github.io/wordflow。
]]></description>
      <guid>http://arxiv.org/abs/2401.14447</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>LongHealth：具有长临床文档的问答基准。 （arXiv：2401.14490v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14490</link>
      <description><![CDATA[背景：大型语言模型 (LLM) 的最新进展提供了
医疗保健方面的潜在好处，特别是在处理大量患者方面
记录。然而，现有的基准并没有充分评估法学硕士的能力
处理真实世界的冗长临床数据。

方法：我们提出了 LongHealth 基准，包括 20 个详细的指标
虚构的各种疾病的患者病例，每个病例都包含
5,090 到 6,754 个字。该基准测试对法学硕士提出了 400 项多项选择题的挑战
问题分为三类：信息提取、否定和排序，
挑战法学硕士从大型临床中提取和解释信息
文件。

结果：我们使用至少 16,000 个代币评估了 9 个开源 LLM
还包括 OpenAI 专有且经济高效的 GPT-3.5 Turbo，用于
比较。 Mixtral-8x7B-Instruct-v0.1 的准确度最高，
特别是在专注于从单个和多个信息检索的任务中
患者文件。然而，所有模型在任务中都表现得很挣扎
要求识别缺失信息，突出关键信息
临床数据解释方面需要改进的领域。

结论：虽然法学硕士在处理长
临床文件，其目前的准确性水平不足以可靠
临床使用，尤其是需要识别缺失的场景
信息。 LongHealth 基准提供了更现实的评估
医疗保健环境中的法学硕士并强调需要进一步的模型
完善安全有效的临床应用。

我们公开提供基准测试和评估代码。
]]></description>
      <guid>http://arxiv.org/abs/2401.14490</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>K-QA：真实世界的医学问答基准。 （arXiv：2401.14493v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14493</link>
      <description><![CDATA[确保大型语言模型 (LLM) 提供的响应的准确性
至关重要，特别是在可能存在错误信息的临床环境中
直接影响患者健康。为了应对这一挑战，我们构建了 K-QA，
包含来自现实世界的 1,212 个患者问题的数据集
在 K Health（人工智能驱动的临床平台）上进行的对话。我们雇佣了一名
由内部医生组成的小组负责回答并手动分解 K-QA 的子集
成独立的陈述。此外，我们还制定了两个基于 NLI 的
近似召回率和精确率的评估指标：（1）全面性，
测量生成的信息中基本临床信息的百分比
答案和（2）幻觉率，衡量来自答案的陈述数量
医生策划的回答与法学硕士的回答相矛盾。最后，我们使用K-QA
与这些指标一起评估几个最先进的模型，以及
作为情境学习和面向医学的增强检索的效果
作者制定的方案。我们的研究结果表明，在上下文中
学习提高了模型的综合性，增强了检索能力
能有效减少幻觉。我们将 K-QA 提供给
社区促进对医学准确的 NLP 应用的研究。
]]></description>
      <guid>http://arxiv.org/abs/2401.14493</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:24 GMT</pubDate>
    </item>
    <item>
      <title>语义敏感性和不一致的预测：衡量 NLI 模型的脆弱性。 （arXiv：2401.14440v1 [cs.CL]）</title>
      <link>http://arxiv.org/abs/2401.14440</link>
      <description><![CDATA[最近对基于变压器的自然能力的研究
语言理解（NLU）模型表明它们有一个
理解词汇和组合语义。我们提供证据表明
建议对这些说法持保留态度：我们发现
最先进的自然语言推理（NLI）模型对
较小的语义保留了表面形式的变化，这导致了相当大的
推理过程中模型决策不一致。值得注意的是，这种行为有所不同
然而，从对组合语义的有效和深入的理解来看，
在标准基准上评估模型准确性时或在
探索句法、单调和逻辑上稳健的推理。我们提出一个
衡量语义敏感性程度的新颖框架。为此，我们
在包含次要内容的对抗性生成的示例上评估 NLI 模型
保留语义的表面形式输入噪声。这是通过使用实现的
条件文本生成，明确条件是 NLI 模型
预测原始输入和对抗性输入之间的关系
对称等价蕴涵。我们系统地研究了影响
\emph{in-} 和 \emph{out-of} 域设置的 NLI 模型中存在这种现象。
我们的实验表明语义敏感性会导致性能下降
\emph{in-} 和 \emph{out-of-} 域的平均值为 $12.92\%$ 和 $23.71\%$
分别设置。我们进一步进行消融研究，分析这一点
跨模型、数据集和推理变化的现象，并表明
语义敏感性可能导致模型预测出现严重不一致。
]]></description>
      <guid>http://arxiv.org/abs/2401.14440</guid>
      <pubDate>Mon, 29 Jan 2024 03:14:23 GMT</pubDate>
    </item>
    </channel>
</rss>