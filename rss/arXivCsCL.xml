<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 05 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>在推文之间阅读：解读相互关联的混合意识形态社区的意识形态立场</title>
      <link>https://arxiv.org/abs/2402.01091</link>
      <description><![CDATA[NLP 的最新进展提高了我们理解在线社区微妙世界观的能力。现有的专注于探究意识形态立场的研究将自由派和保守派视为不同的群体。然而，这未能解释有机形成的在线社区的微妙观点以及它们之间的联系。在本文中，我们研究了 Twitter 上关于 2020 年美国大选的讨论，以确定复杂的互动社区。利用这种相互关联性，我们引入了一种新颖的方法，该方法在微调语言模型（LM）时利用消息传递来探究这些社区的微妙意识形态。通过比较语言模型产生的响应和现实世界的调查结果，我们的方法显示出比现有基线更高的一致性，突显了使用语言模型在揭示相互关联的混合意识形态社区内部和之间的复杂意识形态方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.01091</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>我们来谈判吧！谈判对话系统综述</title>
      <link>https://arxiv.org/abs/2402.01097</link>
      <description><![CDATA[谈判是人类沟通中至关重要的能力。最近，人们对谈判对话系统的研究兴趣重新兴起，其目标是创建可以帮助人们解决冲突或达成协议的智能代理。尽管人们对谈判对话系统进行了许多探索，但迄今为止尚未对该任务进行系统回顾。我们的目标是通过调查谈判对话系统领域的最新研究并涵盖文献中的基准、评估和方法来填补这一空白。我们还讨论了未来潜在的方向，包括多模式、多方和跨文化谈判场景。我们的目标是为社区提供谈判对话系统的系统概述并激发未来的研究。]]></description>
      <guid>https://arxiv.org/abs/2402.01097</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>多智能体系统的推理能力：局限性、挑战和以人为本的解决方案</title>
      <link>https://arxiv.org/abs/2402.01108</link>
      <description><![CDATA[大型语言模型 (LLM) 在各种任务中的出色表现为在生产环境中使用它们带来了许多机遇和挑战。为了实际采用法学硕士，多代理系统有望在更大的企业平台环境中增强、集成和编排法学硕士，这些平台使用现有的专有数据和模型来处理复杂的现实世界任务。尽管这些系统取得了巨大成功，但当前的方法依赖于狭窄的、单一焦点的目标来进行优化和评估，往往忽视了现实场景中的潜在限制，包括有限的预算、资源和时间。此外，解释、分析和调试这些系统需要对不同的组件进行相互关联的评估。这种需求目前用现有的方法是不可行的。在本文中，我们引入推理能力的概念作为统一标准，以便在优化过程中整合约束并在系统内不同组件之间建立联系，这也使得能够采用更全面、更全面的评估方法。我们提出了推理能力的正式定义，并说明了其在识别系统每个组件的局限性方面的效用。然后，我们讨论如何通过自我反思过程来解决这些限制，其中使用人类反馈来减轻推理中的缺陷并增强系统的整体一致性。]]></description>
      <guid>https://arxiv.org/abs/2402.01108</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:54 GMT</pubDate>
    </item>
    <item>
      <title>充分利用分词器进行预训练和领域适应</title>
      <link>https://arxiv.org/abs/2402.01035</link>
      <description><![CDATA[代币化是现代法学硕士中一个未被充分研究且经常被忽视的组成部分。大多数已发表的作品都使用单个标记器进行所有实验，通常是从另一个模型借用的，而不执行消融或分析来优化标记化。此外，在微调基本模型时，分词器通常保持不变。在本文中，我们表明分词器的大小、预分词正则表达式和训练数据可以显着影响模型的生成速度、有效上下文大小、内存使用和下游性能。我们训练专门的字节对编码代码分词器，并针对分词器设计对用于代码生成任务（例如 HumanEval 和 MBPP）的 LLM 性能的影响进行广泛的消融，并为分词器超参数选择和切换分词器提供建议。预训练的法学硕士。我们对从头开始训练的模型和预先训练的模型进行实验，验证它们对广泛用例的适用性。我们发现，当对超过 500 亿个 token 进行微调时，我们可以专门化预训练 LLM 的 tokenizer，以获得生成速度和有效上下文大小的巨大增益。]]></description>
      <guid>https://arxiv.org/abs/2402.01035</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:53 GMT</pubDate>
    </item>
    <item>
      <title>使用基础语言模型生成、提炼和评估动机访谈式反思</title>
      <link>https://arxiv.org/abs/2402.01051</link>
      <description><![CDATA[大型基础语言模型能够在高级别上执行许多任务，但由于其规模和专有所有权而难以在许多应用程序中部署。许多人将有动力将基础模型的特定功能提炼成可以拥有和控制的较小模型。在开发治疗性聊天机器人的过程中，我们希望提炼出一种称为反思性聆听的能力，其中治疗师可以对客户的言语进行反思。这些反思要么重述客户所说的内容，要么将所说的内容与相关的观察、想法或猜测联系起来，鼓励和引导客户继续思考。在本文中，我们提出了一种将基础语言模型（GPT-4）的反射生成提取为更小的模型的方法。我们首先证明，使用零样本提示的 GPT-4 可以以接近 100% 的成功率生成反射，优于之前的所有方法。使用 GPT-4 生成的反射，我们微调 GPT-2 系列的不同大小。 GPT-2-small 模型在保留测试集上取得了 83% 的成功，GPT-2 XL 取得了 90% 的成功。我们还表明，GPT-4 可以帮助完成评估蒸馏模型质量的劳动密集型任务，将其用作零样本分类器。使用三人评审作为指导，分类器实现了 0.66 的 Cohen-Kappa，这是一个重要的评估者间可靠性数字。]]></description>
      <guid>https://arxiv.org/abs/2402.01051</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:53 GMT</pubDate>
    </item>
    <item>
      <title>用于双目标对话设置的基于计划的大型语言模型</title>
      <link>https://arxiv.org/abs/2402.01053</link>
      <description><![CDATA[事实证明，训练大型语言模型 (LLM) 遵循用户指令可以为 LLM 提供足够的能力，使其在与人类保持一致的同时能够流利地交谈。然而，尚不完全清楚法学硕士如何在混合主动设置中引导基于计划的对话，其中指令在对话的两个方向流动，即法学硕士和用户都向彼此提供指令。在本文中，我们解决了双重目标混合主动对话设置，其中法学硕士不仅将对话建立在任意计划上，而且还寻求满足程序计划和用户指令。然后，法学硕士负责指导用户完成计划，同时适应新情况、回答问题并在需要时激活安全护栏。我们提出了一种新颖的法学硕士，它将对话建立在程序计划的基础上，可以采取对话主动性，并对系统行为实施护栏，同时还改进了法学硕士对意外用户行为的响应。在受控设置和真实用户中进行的实验表明，性能最佳的模型（我们称之为 PlanLLM）比强大的基线提高了 2.1 倍。此外，实验还显示出对未见领域的良好泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2402.01053</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:53 GMT</pubDate>
    </item>
    <item>
      <title>多语言文档问答大语言模型评估方法</title>
      <link>https://arxiv.org/abs/2402.01065</link>
      <description><![CDATA[随着大型语言模型（LLM）的广泛采用，在本文中我们研究了这些模型的多语言能力。我们的初步结果表明，将母语上下文、问题和答案翻译成高资源语言产生了最佳结果。]]></description>
      <guid>https://arxiv.org/abs/2402.01065</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:53 GMT</pubDate>
    </item>
    <item>
      <title>与领域无关的欺骗：新的分类法和语言分析</title>
      <link>https://arxiv.org/abs/2402.01019</link>
      <description><![CDATA[基于互联网的经济和社会正陷入欺骗性攻击之中。这些攻击有多种形式，例如假新闻、网络钓鱼和工作诈骗，我们称之为“欺骗领域”。机器学习和自然语言处理研究人员一直试图通过设计领域来改善这种不稳定的情况。特定探测器。只有少数最近的作品考虑了与领域无关的欺骗。我们收集这些不同的研究线索并调查与领域无关的欺骗。首先，我们提供了欺骗的新计算定义，并将欺骗分解为新的分类法。然后，我们分析了关于欺骗的语言线索的争论，并为系统评价提供了指导。最后，我们研究了共同的语言特征，并为跨不同形式的欺骗的知识转移提供了证据。]]></description>
      <guid>https://arxiv.org/abs/2402.01019</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:52 GMT</pubDate>
    </item>
    <item>
      <title>基于图的聚类用于检测随时间和语言变化的语义变化</title>
      <link>https://arxiv.org/abs/2402.01025</link>
      <description><![CDATA[尽管上下文嵌入在 NLP 中占据主导地位，但依赖这些嵌入和聚类方法来检测语义变化的方法的性能不如基于静态词嵌入的更简单的对应方法。这是因为产生词义簇的聚类方法质量较差，难以捕捉词义，尤其是那些频率较低的词义。这个问题阻碍了下一步研究一种语言的词义变化如何影响另一种语言。为了解决这个问题，我们提出了一种基于图的聚类方法来捕获高频和低频词义在时间和语言上的细微变化，包括随着时间的推移这些含义的获得和丢失。我们的实验结果表明，我们的方法在 SemEval2020 跨四种语言的二元分类任务中大大超越了之前的方法。此外，我们展示了我们的方法作为多功能可视化工具的能力，可以检测语言内和语言间设置中的语义变化。我们公开我们的代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2402.01025</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:52 GMT</pubDate>
    </item>
    <item>
      <title>可执行代码操作引出更好的 LLM 代理</title>
      <link>https://arxiv.org/abs/2402.01030</link>
      <description><![CDATA[大型语言模型（LLM）代理能够执行广泛的操作，例如调用工具和控制机器人，在应对现实世界的挑战方面显示出巨大的潜力。通常会提示 LLM 代理通过生成预定义格式的 JSON 或文本来生成操作，这通常受到操作空间（例如预定义工具的范围）和灵活性的限制（例如无法组合多个工具）的限制）。这项工作建议使用可执行的Python代码将LLM代理的操作整合到统一的操作空间（CodeAct）中。 CodeAct 与 Python 解释器集成，可以执行代码操作并动态修改先前的操作或通过多轮交互根据新观察发出新操作。我们对 API-Bank 上 17 个法学硕士的广泛分析和新制定的基准表明，CodeAct 的性能优于广泛使用的替代方案（成功率高出 20%）。 CodeAct 令人鼓舞的性能激励我们构建一个开源 LLM 代理，它通过执行可解释的代码与环境交互，并使用自然语言与用户协作。为此，我们收集了一个指令调优数据集 CodeActInstruct，其中包含使用 CodeAct 的 7k 多轮交互。我们证明，它可以与现有数据一起使用，以改进面向代理的任务中的模型，而不会影响其一般能力。 CodeActAgent 经过 Llama2 和 Mistral 的微调，与 Python 解释器集成，经过专门定制，可使用现有库和自主自调试来执行复杂的任务（例如模型训练）。]]></description>
      <guid>https://arxiv.org/abs/2402.01030</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:52 GMT</pubDate>
    </item>
    <item>
      <title>使用用于 KG 问答的实体预训练 GPT 生成 SPARQL</title>
      <link>https://arxiv.org/abs/2402.00969</link>
      <description><![CDATA[知识图谱的受欢迎程度在过去几年中迅速增长。所有这些知识都可供人们通过互联网上的许多在线数据库查询。不过，如果非程序员用户能够访问他们想知道的任何信息，那将是一个巨大的成就。为了使用自然语言处理工具和通过许多挑战鼓励创造力来解决这项任务，人们付出了很多努力。我们的方法侧重于假设链接到自然语言问题的正确实体，并训练 GPT 模型以从中创建 SPARQL 查询。我们成功地隔离了任务的哪些属性在少量或零样本情况下最难解决，并且我们建议对所有实体（在 CWA 下）进行预训练以提高性能。我们在 3 次测试中获得了 62.703% 的 SPARQL 精确匹配准确率，实体链接挑战的 F1 为 0.809，问答挑战的 F1 为 0.009。]]></description>
      <guid>https://arxiv.org/abs/2402.00969</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:51 GMT</pubDate>
    </item>
    <item>
      <title>分析 NLP 分类任务的信息论方法</title>
      <link>https://arxiv.org/abs/2402.00978</link>
      <description><![CDATA[了解输入对输出的重要性对于许多任务都很有用。这项工作提供了一个信息论框架来分析输入对文本分类任务的影响。自然语言处理 (NLP) 任务采用单个元素输入或多个元素输入来预测输出变量，其中元素是文本块。每个文本元素都有两个组成部分：关联的语义和语言实现。选择多项选择阅读理解（MCRC）和情感分类（SC）来展示该框架。对于 MCRC，我们发现，在更具挑战性的数据集上，与问题影响相比，上下文对输出的影响会减少。特别是，更具挑战性的环境允许问题的复杂性有更大的变化。因此，测试创建者在设计评估多项选择题时需要仔细考虑上下文的​​选择。对于 SC，我们发现在确定情感时，与其语言实现相比，输入文本的语义占主导地位（对于所有考虑的数据集，其语义超过 80%）。该框架位于：https://github.com/WangLuran/nlp-element-influence]]></description>
      <guid>https://arxiv.org/abs/2402.00978</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:51 GMT</pubDate>
    </item>
    <item>
      <title>HR-MultiWOZ：HR LLM 代理的面向任务的对话 (TOD) 数据集</title>
      <link>https://arxiv.org/abs/2402.01018</link>
      <description><![CDATA[大型语言模型 (LLM) 的最新进展正在重塑多个领域的自然语言处理 (NLP) 任务。它们在人力资源 (HR) 领域的应用仍有扩展空间，并且可能有益于完成一些耗时的任务。休假申请、医疗索赔申请和访问请求等例子值得注意，但它们绝不是唯一的例子。然而，上述发展必须应对构建高质量训练数据集的关键挑战。一方面，大多数对话数据集是为客户而不是员工解决问题。另一方面，收集与人力资源部门的对话可能会引起隐私问题。为了解决这个问题，我们引入了 HR-Multiwoz，这是一个包含 10 个 HR 领域的 550 个对话的完全标记数据集，用于评估 LLM Agent。我们的工作有以下贡献：（1）它是人力资源领域第一个用于 NLP 研究的标记开源对话数据集。 (2) 它提供了数据生成过程以及数据分析和人工评估的详细方法。数据生成管道是可转移的，并且可以轻松地适应其他领域中的标记对话数据生成。 (3) 所提出的数据收集管道主要基于法学硕士，注释方面的人工参与最少，既省时又经济。]]></description>
      <guid>https://arxiv.org/abs/2402.01018</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:51 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的安全和隐私挑战：调查</title>
      <link>https://arxiv.org/abs/2402.00888</link>
      <description><![CDATA[大型语言模型（LLM）展示了非凡的能力，并对多个领域做出了贡献，例如生成和总结文本、语言翻译和问答。如今，法学硕士正在成为计算机语言处理任务中非常流行的工具，能够分析复杂的语言模式并根据上下文提供相关且适当的响应。这些模型在提供显着优势的同时，也容易受到安全和隐私攻击，例如越狱攻击、数据中毒攻击和个人身份信息（PII）泄漏攻击。这项调查全面审查了法学硕士对培训数据和用户的安全和隐私挑战，以及交通、教育和医疗保健等各个领域基于应用程序的风险。我们评估法学硕士漏洞的程度，调查法学硕士新兴的安全和隐私攻击，并审查潜在的防御机制。此外，该调查概述了该领域现有的研究空白，并强调了未来的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2402.00888</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:50 GMT</pubDate>
    </item>
    <item>
      <title>探索大型语言和视觉模型中的空间模式直觉</title>
      <link>https://arxiv.org/abs/2402.00956</link>
      <description><![CDATA[尽管大型语言模型（LLM）在人工智能研究中无处不在，但 LLM 中的体现问题仍未得到充分探索，这将它们与机器人中的体现系统区分开来，在机器人中，感官知觉直接通知身体动作。我们的调查探索了一个有趣的领域：法学硕士尽管具有非具身性，但能否有效地捕捉人类对语言的基本空间构建块的隐含直觉。我们利用通过早期感觉运动经验发展​​起来的空间认知基础的见解，通过三个心理语言学实验的再现来指导我们的探索。令人惊讶的是，模型输出和人类反应之间出现了相关性，揭示了与具体经验没有实际联系的适应性。显着的区别包括极化的语言模型响应和视觉语言模型的相关性降低。这项研究有助于深入理解语言、空间体验和大型语言模型计算之间的相互作用。更多信息请访问 https://cisnlp.github.io/Spatial_Schemas/]]></description>
      <guid>https://arxiv.org/abs/2402.00956</guid>
      <pubDate>Tue, 06 Feb 2024 03:14:50 GMT</pubDate>
    </item>
    </channel>
</rss>