<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>https://arxiv.org/rss/</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 31 Jan 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>基于梯度的语言模型红队</title>
      <link>https://arxiv.org/abs/2401.16656</link>
      <description><![CDATA[红队是识别生成语言模型 (LM) 弱点的常见策略，其中会产生对抗性提示，触发 LM 生成不安全的响应。红队对于模型对齐和评估都很有帮助，但如果由人类完成，则属于劳动密集型且难以扩展。在本文中，我们提出了基于梯度的红队（GBRT），这是一种红队方法，用于自动生成可能导致 LM 输出不安全响应的各种提示。 GBRT 是提示学习的一种形式，通过使用安全分类器对 LM 响应进行评分进行训练，然后通过冻结的安全分类器和 LM 进行反向传播以更新提示。为了提高输入提示的连贯性，我们引入了两种变体，它们增加了真实性损失，并对预训练模型进行微调以生成提示，而不是直接学习提示。我们的实验表明，与基于强化学习的红队方法相比，GBRT 在寻找触发 LM 生成不安全响应的提示方面更有效，并且即使 LM 已被微调以产生更安全的输出，GBRT 也能取得成功。]]></description>
      <guid>https://arxiv.org/abs/2401.16656</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:22 GMT</pubDate>
    </item>
    <item>
      <title>OWSM v3.1：基于 E-Branchformer 更好更快的开放式 Whisper 式语音模型</title>
      <link>https://arxiv.org/abs/2401.16658</link>
      <description><![CDATA[最近的研究提倡完全开放的基金会模型，以促进透明度和开放科学。作为第一步，开放式 Whisper 式语音模型 (OWSM) 使用公开数据和开源工具包重现了 OpenAI 的 Whisper。为了重现 Whisper，之前的 OWSM v1 到 v3 模型仍然基于 Transformer，这可能会导致性能低于其他最先进的语音编码器。在这项工作中，我们的目标是在不需要额外训练数据的情况下提高 OWSM 的性能和效率。我们提出了两种尺度的基于 E-Branchformer 的 OWSM v3.1 模型，即 100M 和 1B。 1B 模型是已公开的最大的基于 E-Branchformer 的语音模型。它在绝大多数评估基准中都优于之前的 OWSM v3，同时推理速度提高了 25%。我们公开发布数据准备脚本、预训练模型和训练日志。]]></description>
      <guid>https://arxiv.org/abs/2401.16658</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:22 GMT</pubDate>
    </item>
    <item>
      <title>打破自由 Transformer 模型：特定任务的上下文归因有望提高通用性，而无需微调预训练的 LLM</title>
      <link>https://arxiv.org/abs/2401.16638</link>
      <description><![CDATA[在特定数据集上微调大型预训练语言模型 (LLM) 是自然语言处理 (NLP) 分类任务中常用的策略。然而，这种方法通常会导致模型普遍性的损失。在本文中，我们提出了一个框架，可以保持通用性，并通过利用特定于任务的上下文归因来增强下游任务的性能。我们证明，使用特定于任务的概念运算符对任何转换器模型的文本表示进行线性转换会导致投影到潜在概念空间，在本文中称为上下文归因。在监督学习阶段通过新颖的损失函数优化特定概念算子。所提出的框架表明，每个任务目标的文本表示的上下文归因可以提高鉴别器函数的能力，从而实现更好的分类任务性能。在 HateXplain、IMDB 评论和社交媒体归因这三个数据集上的实验结果表明，所提出的模型具有卓越的准确性和泛化性。具体来说，对于 HateXplain 数据集上的非微调 BERT，我们观察到准确度提高了 8%，F1 分数提高了 10%。而对于 IMDB 数据集，经过微调的最先进的 XLNet 在准确度和 F1 分数方面均优于 1%。此外，在域外跨数据集测试中，DistilBERT 结合所提出的模型对 IMDB 数据集进行微调，将 HateXplain 数据集上的 F1 分数提高了 7%。对于 YouTube 评论的社交媒体归因数据集，我们观察到 F1 指标增加了 5.2%。所提出的框架是用 PyTorch 实现的，并在 GitHub 上提供开源。]]></description>
      <guid>https://arxiv.org/abs/2401.16638</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:21 GMT</pubDate>
    </item>
    <item>
      <title>TeenyTinyLlama：用巴西葡萄牙语训练的开源微型语言模型</title>
      <link>https://arxiv.org/abs/2401.16640</link>
      <description><![CDATA[大型语言模型（LLM）在自然语言处理方面取得了显着的进步，但其进步在不同语言中尚未达到同等水平。虽然大多数法学硕士都接受过英语等高资源语言的培训，但多语言模型的表现通常不如单语言模型。此外，其多语言基础的某些方面有时会限制它们产生的副产品，例如计算需求和许可制度。在这项研究中，我们记录了专为资源匮乏环境而设计的开放基础模型的开发、其局限性和好处。这是 TeenyTinyLlama 对：用于巴西葡萄牙语文本生成的两个紧凑模型。我们根据 Apache 2.0 许可在 GitHub 和 Hugging Face 上发布它们，以供社区使用和进一步开发。请参阅 https://github.com/Nkluge-correa/TeenyTinyLlama]]></description>
      <guid>https://arxiv.org/abs/2401.16640</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:21 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中不连贯的概率判断</title>
      <link>https://arxiv.org/abs/2401.16646</link>
      <description><![CDATA[经过训练用于下一个单词预测的自回归大型语言模型 (LLM) 在生成连贯文本方面表现出了非凡的能力。但他们同样擅长形成连贯的概率判断吗？我们使用概率恒等式和重复判断来评估法学硕士做出的概率判断的一致性。我们的结果表明，这些模型产生的判断往往是不连贯的，表现出与概率论规则类似的人类系统性偏差。此外，当被提示对同一事件进行判断时，法学硕士产生的概率判断的均值-方差关系呈现出与人类相似的倒U形关系。我们提出，这些与理性的偏差可以通过将自回归法学硕士与隐式贝叶斯推理联系起来并与人类概率判断的贝叶斯采样器模型进行比较来解释。]]></description>
      <guid>https://arxiv.org/abs/2401.16646</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:21 GMT</pubDate>
    </item>
    <item>
      <title>利用专业放射科医生的专业知识来增强法学硕士对放射学报告的评估</title>
      <link>https://arxiv.org/abs/2401.16578</link>
      <description><![CDATA[在放射学领域，人工智能 (AI) 显着改进了报告生成，但自动评估这些人工智能生成的报告仍然具有挑战性。当前的指标，例如传统自然语言生成 (NLG) 和临床疗效 (CE)，通常无法捕捉临床背景的语义复杂性或过分强调临床细节，从而损害了报告的清晰度。为了克服这些问题，我们提出的方法将专业放射科医生的专业知识与大型语言模型 (LLM)（例如 GPT-3.5 和 GPT-4）相结合 1. 利用上下文指令学习 (ICIL) 和思想链 (CoT) 推理，我们的方法使法学硕士评估与放射科医生标准保持一致，从而能够对人类和人工智能生成的报告进行详细比较。聚合句子评估分数的回归模型进一步增强了这一点。实验结果表明，我们的“Detailed GPT-4（5-shot）”模型获得了 0.48 分，比 METEOR 指标高出 0.19，而我们的“Regressed GPT-4”模型显示出与专家评估更加一致，超出现有最佳指标 0.35。此外，我们的解释的稳健性已通过彻底的迭代策略得到验证。我们计划公开发布放射学专家的注释，为未来评估的准确性设立新标准。这强调了我们的方法在增强人工智能驱动的医疗报告的质量评估方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2401.16578</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:20 GMT</pubDate>
    </item>
    <item>
      <title>针对资源匮乏语言的大规模多语言文本翻译</title>
      <link>https://arxiv.org/abs/2401.16582</link>
      <description><![CDATA[翻译成资源严重匮乏的语言既具有拯救和复兴这些语言的文化目标，也具有满足当地社区日常需求的人道主义目标，而最近的 COVID-19 大流行加速了这种需求。在许多人道主义工作中，翻译成资源严重匮乏的语言通常不需要通用翻译引擎，而是专用的特定于文本的翻译引擎。例如，医疗记录、卫生程序、政府通讯、紧急程序和宗教文本都是有限文本。虽然不存在适用于所有语言的通用翻译引擎，但将多语言已知的有限文本翻译成新的、资源匮乏的语言是可能的，并减少人工翻译工作。我们尝试利用资源丰富的语言的翻译资源，以一种新的资源匮乏的语言，有效地为众所周知的文本提供最佳的翻译质量，这些文本有多种语言版本。为了实现这一目标，我们认为，在将封闭文本翻译成资源匮乏的语言时，不一定需要泛化到域外文本，但泛化到新语言是必要的。性能增益来自于大规模源并行性，通过仔细选择相近的语系、同一语言中风格一致的语料库级释义以及将现有的大型预训练多语言模型首先适应领域，然后适应语言。这种性能提升使得机器翻译系统可以与人工翻译人员协作，以加快翻译成新的、资源匮乏的语言的过程。]]></description>
      <guid>https://arxiv.org/abs/2401.16582</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:20 GMT</pubDate>
    </item>
    <item>
      <title>人类和 ChatGPT 生成的对话之间的语言比较</title>
      <link>https://arxiv.org/abs/2401.16587</link>
      <description><![CDATA[本研究使用 ChatGPT-3.5 生成的 19,500 个对话作为 EmpathicDialogues 数据集的伴侣，探讨了人类和 LLM 生成的对话之间的语言差异。该研究采用语言查询和字数统计 (LIWC) 分析，将 ChatGPT 生成的对话与 118 个语言类别的人类对话进行比较。结果显示，人类对话具有更大的可变性和真实性，但 ChatGPT 在社交过程、分析风格、认知、注意力集中和积极情绪基调等方面表现出色，这强化了法学硕士“比人类更人性化”的最新发现。然而，ChatGPT 和人类对话之间的积极或消极影响没有发现显着差异。对话嵌入的分类器分析表明，尽管对话中没有明确提及情感，但情感效价的隐式编码。该研究还贡献了一个由 ChatGPT 生成的新颖的两个独立聊天机器人之间的对话数据集，该数据集旨在复制可供开放访问的人类对话语料库，并广泛用于语言建模的人工智能研究。我们的研究结果增进了对 ChatGPT 语言能力的理解，并为区分人类和法学硕士生成的文本的持续努力提供了信息，这对于检测人工智能生成的假货、错误信息和虚假信息至关重要。]]></description>
      <guid>https://arxiv.org/abs/2401.16587</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:20 GMT</pubDate>
    </item>
    <item>
      <title>ToPro：跨语言序列标记任务的标记级提示分解</title>
      <link>https://arxiv.org/abs/2401.16589</link>
      <description><![CDATA[基于提示的方法已成功应用于多语言预训练语言模型，以实现零样本跨语言理解。然而，之前的大多数研究主要集中在句子级分类任务，只有少数考虑了标记级标记任务，例如命名实体识别（NER）和词性（POS）标记。在本文中，我们提出了令牌级提示分解（ToPro），它有助于基于提示的方法进行令牌级序列标记任务。 ToPro 方法将输入句子分解为单个标记，并对每个标记应用一个提示模板。我们对多语言 NER 和 POS 标记数据集的实验表明，在零样本跨语言迁移中，基于 ToPro 的微调优于 Vanilla 微调和 Prompt-Tuning，特别是对于与源语言英语类型不同的语言。当与 mT5 模型一起使用时，我们的方法也获得了最先进的性能。此外，我们对多语言大语言模型的探索性研究表明，ToPro 的性能比当前的上下文学习方法要好得多。总体而言，性能改进表明 ToPro 有可能成为序列标记任务的一种新颖且简单的基准测试方法。]]></description>
      <guid>https://arxiv.org/abs/2401.16589</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:20 GMT</pubDate>
    </item>
    <item>
      <title>印地文梵文脚本中的多类遗憾检测</title>
      <link>https://arxiv.org/abs/2401.16561</link>
      <description><![CDATA[近年来，社交媒体上说印地语的人数急剧增加。遗憾是我们日常生活中常见的情感体验。许多演讲者在社交媒体上定期分享他们令人遗憾的经历和观点。它可能会导致一个人重新评估自己的选择，并希望有机会做出不同的选择。因此，了解后悔的根源对于调查其对行为和决策的影响至关重要。这项研究的重点是遗憾及其在各种社交媒体平台上的表达方式，特别是印地语。在我们的研究中，我们提出了来自三个不同来源的新颖数据集，其中每个句子都被手动分类为“因行动而后悔”、“因不作为而后悔”和“不后悔”三个类别之一。接下来，我们使用该数据集来研究印地语文本中遗憾的语言表达，并确定与遗憾最常相关的文本域。我们的研究结果表明，社交媒体平台上的个人经常对过去的不作为和行为表示遗憾，特别是在人际关系领域。我们使用预先训练的 BERT 模型为印地语数据集生成词嵌入，并将深度学习模型与传统机器学习模型进行比较，以证明准确性。我们的结果表明，嵌入 CNN 的 BERT 始终优于其他模型。这描述了 BERT 在表达遗憾领域的上下文和单词含义方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2401.16561</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:19 GMT</pubDate>
    </item>
    <item>
      <title>超越图像文本匹配：使用引导掩码的多模态转换器中的动词理解</title>
      <link>https://arxiv.org/abs/2401.16575</link>
      <description><![CDATA[主要的探测方法依赖于图像文本匹配任务的零样本性能，以获得对最近多模态图像语言转换器模型学习的表示的更细粒度的理解。评估是在精心策划的数据集上进行的，重点关注计数、关系、属性等。这项工作引入了一种称为引导掩蔽的替代探测策略。所提出的方法使用掩蔽消除不同的模态，并评估模型高精度预测掩蔽词的能力。我们专注于研究多模态模型，该模型将目标检测器获得的感兴趣区域（ROI）特征视为输入标记。我们在 ViLBERT、LXMERT、UNTER 和 VisualBERT 上使用引导掩蔽来探索对动词的理解，并表明这些模型可以高精度地预测正确的动词。这与之前从图像文本匹配探测技术得出的结论形成鲜明对比，这些技术在需要动词理解的情况下经常失败。所有实验的代码将公开 https://github.com/ivana-13/guided_masking。]]></description>
      <guid>https://arxiv.org/abs/2401.16575</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:19 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士作为按需定制服务</title>
      <link>https://arxiv.org/abs/2401.16577</link>
      <description><![CDATA[大型语言模型 (LLM) 已展现出卓越的语言理解和生成能力。然而，训练、部署和访问这些模型带来了显着的挑战，包括资源密集型需求、延长的训练持续时间和可扩展性问题。为了解决这些问题，我们引入了分层、分布式 LLM 架构的概念，旨在增强 LLM 跨异构计算平台的可访问性和可部署性，包括通用计算机（例如笔记本电脑）和物联网式设备（例如嵌入式系统） ）。通过引入“分层”方法，所提出的架构可以按需访问法学硕士作为可定制的服务。这种方法还确保了可用计算资源和用户应用程序需求之间的最佳权衡。我们设想，分层法学硕士的概念将使广泛的众包用户群能够利用法学硕士的能力，从而总体上促进人工智能技术的进步。]]></description>
      <guid>https://arxiv.org/abs/2401.16577</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:19 GMT</pubDate>
    </item>
    <item>
      <title>InfoLossQA：描述和恢复文本简化中的信息丢失</title>
      <link>https://arxiv.org/abs/2401.16475</link>
      <description><![CDATA[文本简化旨在使技术文本更容易被外行人理解，但通常会导致信息删除和模糊。这项工作提出了 InfoLossQA，这是一个以问答（QA）对的形式描述和恢复简化引起的信息丢失的框架。问答对以“讨论中的问题”理论为基础，旨在帮助读者加深对文本的了解。我们用这个框架进行了一系列的实验。首先，我们收集了 1,000 个由语言学家策划的 QA 对的数据集，这些 QA 对源自 104 个法学硕士对医学研究科学摘要的简化。我们对这些数据的分析表明，信息丢失经常发生，并且 QA 对对丢失的信息进行了高级概述。其次，我们为这项任务设计了两种方法：开源和商业语言模型的端到端提示，以及自然语言推理管道。通过考虑 QA 对的正确性及其语言适用性的新颖评估框架，我们的专家评估表明，模型很难可靠地识别信息丢失，并在构成信息丢失的情况上应用与人类类似的标准。]]></description>
      <guid>https://arxiv.org/abs/2401.16475</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:18 GMT</pubDate>
    </item>
    <item>
      <title>GuReT：区分内疚和遗憾相关文本</title>
      <link>https://arxiv.org/abs/2401.16541</link>
      <description><![CDATA[人类决策与情绪（尤其是内疚和后悔）之间错综复杂的关系，对行为和幸福感具有重大影响。然而，这些情绪的微妙区别和相互作用在计算模型中经常被忽视。本文介绍了一个专门用于剖析内疚和遗憾及其独特文本标记之间关系的数据集，填补了情感计算研究中的一个显着空白。我们的方法将内疚和遗憾识别视为二元分类任务，并采用三种机器学习和六种基于变压器的深度学习技术来对新创建的数据集进行基准测试。该研究进一步实施了思想链和思想树等创新推理方法来评估模型的解释逻辑。结果表明，基于 Transformer 的模型具有明显的性能优势，宏观 F1 得分为 90.4%，而最佳机器学习分类器的得分为 85.3%，这证明了它们在区分复杂情绪状态方面的卓越能力。]]></description>
      <guid>https://arxiv.org/abs/2401.16541</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:18 GMT</pubDate>
    </item>
    <item>
      <title>SelectLLM：LLM 可以选择重要说明进行注释吗？</title>
      <link>https://arxiv.org/abs/2401.16553</link>
      <description><![CDATA[使用大型且多样化的指令数据集训练大型语言模型 (LLM)，使模型能够理解和遵循人类指令。最近的研究表明，使用一小组高质量指令的性能优于使用大量但噪声更大的指令。由于指令是未标记的，并且它们的响应是自然文本，因此具有模型置信度的传统主动学习方案不能直接应用于未标记指令的选择。在这项工作中，我们提出了一种新的指令选择方法，称为 SelectLLM，它利用 LLM 来选择高质量指令。我们的高级想法是使用法学硕士通过提示来估计每条指令的有用性和影响力，而无需相应的标签（即响应）。 SelectLLM涉及两个步骤：使用聚类算法（例如CoreSet）将未标记的指令划分为多个簇，然后提示LLM在每个簇内选择高质量的指令。与最近最先进的选择方法相比，SelectLLM 在流行的指令基准测试中表现出相当或略好的性能。所有代码和数据都是公开的（https://github.com/minnesotanlp/select-llm）。]]></description>
      <guid>https://arxiv.org/abs/2401.16553</guid>
      <pubDate>Wed, 31 Jan 2024 18:13:18 GMT</pubDate>
    </item>
    </channel>
</rss>