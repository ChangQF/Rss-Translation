<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arxiv.org上的cs.cl更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.cl在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>重复并不是全部：不同的机制在语言模型中维持重复</title>
      <link>https://arxiv.org/abs/2504.01100</link>
      <description><![CDATA[ARXIV：2504.01100V1公告类型：新 
摘要：语言模型（LMS）生成的文本可以降低为重复的周期，其中相同的单词序列持续重复一个。先前的研究通常将重复视为统一现象。但是，重复序列在不同的任务和上下文下出现，从而增加了可能由多个基本因素驱动的可能性。在这里，我们通过实验探讨了以下假设：LMS中的重复可能是由不同的机制引起的，反映了模型使用的不同文本生成策略。我们检查了LMS在两个迅速重复的条件下的内部工作：一个重复的序列在人写的文本后自然出现，而另一种是通过内在学习（ICL）设置明确诱导的重复序列。我们的分析揭示了这两种条件之间的关键差异：该模型表现出不同水平的置信度，依赖于不同的注意力头，并且在响应受控扰动的响应中显示出不同的变化曲折。这些发现表明，不同的内部机制可以相互作用以驱动重复，这对其解释和缓解策略产生了影响。更广泛地说，我们的结果强调，LMS中相同的表面行为可能由不同的基础过程来维持，独立或结合起来。]]></description>
      <guid>https://arxiv.org/abs/2504.01100</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM可以掌握隐式文化价值吗？基准LLMS的元认知文化智能与CQ Bench</title>
      <link>https://arxiv.org/abs/2504.01127</link>
      <description><![CDATA[ARXIV：2504.01127V1公告类型：新 
摘要：文化智能（CQ）是指理解陌生文化背景的能力 - 大型语言模型（LLMS）有效地与全球多样的用户互动的重要技能。尽管现有的研究通常集中在明确指定的文化规范上，但这种方法未能捕捉到现实世界对话的微妙而隐含的价值观。为了解决这一差距，我们介绍了CQ-Bench，这是一种专门旨在评估LLMS从自然对话环境中推断出隐式文化价值的能力的基准。我们使用世界价值调查和全球范围数据集的价值观生成了一个基于对话的故事数据集，其中包括道德，宗教，社会和政治。我们的数据集施工管道包括严格的验证程序 - 使用GPT-4O，一致性和内在性检查，在最终验证中，人类模型一致性为98.2％。我们的基准包括增加复杂性的三个任务：态度检测，价值选择和价值提取。我们发现，尽管O1和DeepSeek-R1模型在价值选择方面达到了人级的性能（0.809和0.814），但它们的态度检测仍然不足，F1得分分别为0.622和0.635。在价值提取任务中，GPT-4O-Mini和O3-Mini得分为0.602和0.598，突出了开放式文化推理的困难。值得注意的是，只有500个文化丰富的例子的微调较小的模型（例如，Llama-3.2-3b）提高了10％以上的性能，在某些情况下甚至超过了更强大的基线（O3-MINI）。使用CQ Bench，我们提供了有关LLMS CQ研究中当前挑战的见解，并提出了增强LLMS跨文化推理能力的实用途径。]]></description>
      <guid>https://arxiv.org/abs/2504.01127</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>顶部还在旋转吗？评估叙事理解中的主观性</title>
      <link>https://arxiv.org/abs/2504.01132</link>
      <description><![CDATA[ARXIV：2504.01132V1公告类型：新 
摘要：确定对源文件的忠诚是许多领域的重要问题。该任务通常被视为二进制判断，即对索赔是否受到支持或不支持来源的判断。但是，在许多情况下，是否支持索赔可能是模棱两可的。例如，这可能取决于从给定证据中提出的推论，而不同的人可以根据他们与这些推论的一致性合理地将主张解释为受支持或不支持的说法。强迫二进制标签对此类主张降低评估的可靠性。在这项工作中，我们重新构架了管理与歧义主张的事实判断所涉及的主观性的任务。我们介绍了摘要的LLM生成的编辑，以提供对索赔的细微评估的一种方法：需要编辑多少摘要才能明确？索赔是否被重写以及更改的程度可以用作自动评估指标，歧义重写度量标准（ARM），其反馈信号比忠实的二进制判断更为丰富。我们专注于叙事总结的领域，因为它特别充满了歧义和主观解释。我们表明，ARM在主张忠诚方面的注释人协议中产生21％的绝对改善，表明主观性降低了。]]></description>
      <guid>https://arxiv.org/abs/2504.01132</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>遵循流程：在文本到图像模型中跨文本令牌的信息流有关</title>
      <link>https://arxiv.org/abs/2504.01137</link>
      <description><![CDATA[ARXIV：2504.01137V1公告类型：新 
摘要：文本对图像（T2I）模型通常遭受语义泄漏，不正确的特征绑定以及生成图像中关键概念的遗漏。这项工作通过研究文本令牌表示之间的信息流的作用来研究这些现象。为此，我们通过在给定的提示中应用于上下文令牌表示子集上的扩散组件来生成图像，并观察几个有趣的现象。首先，在许多情况下，单词或多词表达式由一个或两个令牌完全表示，而其他令牌则是冗余的。例如，在“旧金山的金门大桥”中，单独的“大门”捕捉了完整的表达。我们通过在文本编码和从结果表示形式中生成图像后将其删除来证明这些令牌的冗余。令人惊讶的是，我们发现此过程不仅保持图像生成性能，而且与标准生成相比，错误还可以将错误降低21 \％。然后，我们表明信息也可以在句子中的不同表达式之间流动，这通常会导致语义泄漏。基于此观察结果，我们提出了一种简单，无训练的方法来减轻语义泄漏：用文本编码以其不受义言的表示，替换泄漏的项目的表示形式。值得注意的是，这种简单的方法可将语义泄漏减少85 \％。总体而言，我们的工作对T2I模型中文本令牌之间的信息流进行了全面分析，从而提供了新颖的见解和实际好处。]]></description>
      <guid>https://arxiv.org/abs/2504.01137</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$ \ mu $ ke：matryoshka大语言模型的非结构化知识编辑</title>
      <link>https://arxiv.org/abs/2504.01196</link>
      <description><![CDATA[ARXIV：2504.01196V1公告类型：新 
摘要：大型语言模型（LLM）已成为强大的知识库，但受到静态培训数据的限制，导致幻觉和安全风险等问题。通过定位和编辑范式编辑模型的内部知识已证明是一种具有成本效益的替代方法，尽管当前的非结构化方法，尤其是基于窗口的自动回应方法，但通常会破坏早期内存更新和后来的输出令牌之间的因果关系。在这项工作中，我们首先对这些限制进行了分析，然后引入Matryoshka非结构化知识编辑（$ \ MU $ KE），这是一种新型的内存更新机制，可通过Matryoshka式的目标和自适应损耗系数保留此类依赖性。对四个基准测试模型的两个模型的经验评估表明，$ \ mu $ ke比最先进的方法提高了高达12.33％的编辑功效，并且在应用于多样化的格式编辑中时保持强大的效果，从而实现了其在LLMS中有效的非结构化知识编辑的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.01196</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>医学大语言模型很容易分散注意力</title>
      <link>https://arxiv.org/abs/2504.01201</link>
      <description><![CDATA[ARXIV：2504.01201V1公告类型：新 
摘要：大型语言模型（LLMS）有可能改变医学的可能性，但是现实世界中的临床场景包含可以阻碍性能的无关信息。辅助技术（如环境命令）的兴起，自动产生了活着的患者遇到的票据，它有可能引入额外的噪音，从而使评估LLM的能力过滤相关数据的能力至关重要。为了调查这一点，我们开发了Meddistractqa，这是一种使用USMLE风格的问题的基准，并嵌入了模拟现实世界的分心。我们的发现表明，分散注意力的陈述（具有非临床环境中使用的临床含义或对无关健康状况的临床含义的多义单词）可以将LLM准确性降低高达17.9％。通常提出的解决方案来改善模型性能，例如检索功能增强的生成（RAG）和医学微调并没有改变这种效果，在某些情况下，引入了自己的混杂因素并进一步退化了性能。我们的发现表明，LLM在本地缺乏将相关性与无关的临床信息区分开所必需的逻辑机制，从而对现实世界应用构成了挑战。 Meddistractqa和我们的结果凸显了需要强大的缓解策略来增强LLM对无关信息的弹性。]]></description>
      <guid>https://arxiv.org/abs/2504.01201</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在临床访谈中检测PTSD：NLP方法和大语言模型的比较分析</title>
      <link>https://arxiv.org/abs/2504.01216</link>
      <description><![CDATA[ARXIV：2504.01216V1公告类型：新 
摘要：创伤后应激障碍（PTSD）在临床环境中仍未诊断，为自动检测提供了识别患者的机会。这项研究评估了自然语言处理方法，用于从临床访谈笔录中检测PTSD。我们使用DAIC-WOZ数据集比较了一般和心理健康特异性变压器模型（BERT/ROBERTA），基于嵌入的方法（Senterbert/Llama）和大型语言模型促使策略（零射击/少数射击/链链）使用DAIC-WOZ数据集。域特异性模型显着超过了一般模型（精神 - 罗伯塔F1 = 0.643 vs. Roberta-base 0.485）。具有神经网络的Llama嵌入性能达到最高的性能（F1 = 0.700）。使用DSM-5标准零射击提示在没有训练数据的情况下得出竞争结果（F1 = 0.657）。在症状严重程度和合并症状态下的性能差异很大，严重的PTSD病例和合并症患者的准确性更高。我们的发现突出了针对域适应的嵌入和LLM的潜力进行可扩展筛选的潜力，同时强调需要改善细微差异的检测，并为开发用于PTSD评估的临床上可行的AI工具提供见解。]]></description>
      <guid>https://arxiv.org/abs/2504.01216</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>粒状单词评估和夹杆质量估算的不确定性校准的共形风险控制框架</title>
      <link>https://arxiv.org/abs/2504.01225</link>
      <description><![CDATA[ARXIV：2504.01225V1公告类型：新 
摘要：这项研究探讨了学到的图像字幕评估指标的当前局限性，特别是缺乏字幕中单个单词不对对准的颗粒状评估，以及依赖单点质量估计值而不考虑不确定性。为了解决这些限制，我们提出了一种简单而有效的策略，用于生成和校准夹克分布。利用模型 - 不合稳定风险控制框架，我们校准了特定于特定于任务的控制变量的夹克值，以应对上述两个限制。实验结果表明，与更复杂的方法相比，使用简单方法（例如输入遮罩）产生的分布可以实现竞争性能。我们的方法有效地检测到未对准的单词，同时提供正式保证与所需的风险水平保持一致，并改善了不确定性估计和预测错误之间的相关性，从而提高了字幕评估指标的整体可靠性。]]></description>
      <guid>https://arxiv.org/abs/2504.01225</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMS中的灾难性遗忘：跨语言任务的比较分析</title>
      <link>https://arxiv.org/abs/2504.01241</link>
      <description><![CDATA[ARXIV：2504.01241V1公告类型：新 
摘要：大型语言模型（LLM）具有明显的高级自然语言处理（NLP），尤其是在自然语言理解（NLU）任务中。随着我们朝着基于LLM的代理人自主处理专业任务的代理世界的发展，对于这些模型而言，在不忘记以前学习的信息的情况下适应新任务至关重要，这是一种被称为灾难性遗忘的挑战。这项研究评估了来自胶水基准的关键NLU任务（包括SST-2，MRPC，COLA和MNLI）的关键NLU任务上的各种开源LLM的持续微调（特别是在100亿个参数以下的模型）。通过采用迅速的工程和特定于任务的调整，我们可以评估和比较模型在学习新任务时保留先验知识的能力。我们的结果表明，诸如PHI-3.5-MINI之类的模型在保持强大的学习能力的同时表现出最小的遗忘，使其适合持续学习环境。此外，诸如Orca-2-7b和Qwen2.5-7B之类的模型表现出令人印象深刻的学习能力和微调后的整体表现。这项工作有助于理解LLM中的灾难性遗忘，并突出显示促使工程技术优化持续学习场景的模型性能。]]></description>
      <guid>https://arxiv.org/abs/2504.01241</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大语言模型的车载对话系统的自动事实基准测试</title>
      <link>https://arxiv.org/abs/2504.01248</link>
      <description><![CDATA[ARXIV：2504.01248V1公告类型：新 
摘要：车载对话系统带来了改善车载用户体验的希望。现代对话系统基于大型语言模型（LLM），这使它们容易遇到诸如幻觉，即不准确，虚构的，因此实际上不正确的信息。在本文中，我们提出了一种基于LLM的方法，用于自动的车载对话系统基准。我们使用五种基于LLM的方法实例化方法，利用结合技术和多样化的人物来增强一致性并最大程度地减少幻觉。我们使用我们的方法来评估Carexpert，这是一种在车内检索的对话问题答案系统，就车辆手册的事实正确性而言。我们制作了一个专门为车载域创建的新型数据集，并针对专家评估测试了我们的方法。我们的结果表明，GPT-4与输入输出的组合促使事实正确的一致性率与专家评估达到了90％以上，而不是最有效的方法，其平均响应时间为4.5s。我们的发现表明，基于LLM的测试构成了验证对话系统有关其事实正确性的可行方法。]]></description>
      <guid>https://arxiv.org/abs/2504.01248</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>年级卫队：简短答案自动分级的智能系统</title>
      <link>https://arxiv.org/abs/2504.01253</link>
      <description><![CDATA[ARXIV：2504.01253V1公告类型：新 
摘要：教育领域中大型语言模型（LLMS）的出现已经提供了自动化评分简短答案问题的动力。 LLMS使评估简短答案非常有效，从而解决了员工短缺等问题。但是，在自动简短答案分级（ASAG）的任务中，LLM响应受培训数据集中的不同观点的影响，导致评估细微或部分正确答案的不准确性。为了应对这一挑战，我们提出了一个新颖的框架，年级后卫。
  1。为了增强LLM的基于任务的专业化，使用均方根误差（RMSE）对温度参数进行了微调。
  2。与传统的方法不同，年级后卫的LLM和等级一起计算优柔寡断的评分（IS），以反映预测等级的不确定性。
  3。引入了信心损失（CAL），以产生优化的优柔寡断评分（IS）。
  4。为了提高可靠性，已经将基于优化的自我反射引入了框架中，从而使人类重新评估以最大程度地减少不正确的成绩分配。
  我们的实验表明，最佳级别卫队在台阶太阳能专业人士中的表现优于传统方法，在台阶太阳能Mini中，RMSE胜过19.16％的RMSE，在Gemini 1.5闪存中的4.00％RMSE和GPT 4-O MINI中的RMSE和10.20％的RMSE。未来的工作包括通过为成绩产生理由来提高准确性来提高可解释性。扩展基准数据集并用特定领域的细微差别注释它们将提高评分精度。最后，分析反馈以增强对预测等级的信心，减少偏见，优化评分标准和个性化学习，同时支持多语言分级系统，这将使解决方案更加准确，适应性，公平，公平和包容。]]></description>
      <guid>https://arxiv.org/abs/2504.01253</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迅速不一致的矛盾：LLM超越生成随机性和及时释义</title>
      <link>https://arxiv.org/abs/2504.01282</link>
      <description><![CDATA[ARXIV：2504.01282V1公告类型：新 
摘要：虽然LLM的不一致不是一个新的主题，但先前的研究主要解决了两种类型的生成不一致之处：i）随机性不一致：进行相同的LLM多次试验，产生不同的响应； ii）释义不一致：释义提示会导致与同一LLM的不同响应。随机性不一致源于生成模型中随机采样引起的固有随机性，而释义不一致是语言建模目标的结果，其中释义会改变词汇逻辑的分布。这项研究发现了迅速的逆转不一致（PRIN），这是LLM自我矛盾的一种新形式：一个问题和几个LLM生成的答案候选者，LLM在提示“哪个是正确的答案？”时经常会有冲突的答案。和“哪些不正确答案？”。普林（Prin）在破坏了LLM-AS-A-A-Gudge的信誉时提出了一个很大的关注，并提出了LLMS遵守基本逻辑规则的挑战。我们进行了一系列实验来研究PRIN，研究了不同LLM的PRIN的程度，减轻它的方法，潜在的应用以及与随机性不一致和释义不一致的关系。作为探索PRIN的首次研究，我们的发现为LLM的内部运作提供了宝贵的见解，并有助于推进值得信赖的AI。]]></description>
      <guid>https://arxiv.org/abs/2504.01282</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ThinkPrune：通过增强学习来修剪LLM的长期思考</title>
      <link>https://arxiv.org/abs/2504.01296</link>
      <description><![CDATA[ARXIV：2504.01296V1公告类型：新 
摘要：我们提出了ThinkPrune，这是一种简单而有效的方法，用于修剪长期思考LLM的思维长度，这通常会产生效率低下且冗余的思维过程。现有的初步探索减少思维长度主要集中于迫使思维过程提早退出，而不是改编LLM以优化和巩固思维过程，因此到目前为止观察到的长度 - 绩效权衡是次优的。为了填补这一空白，ThinkPrune提供了一个简单的解决方案，该解决方案通过增强学习（RL）不断地训练长期构想的LLM，并增加了令牌限制，除此之外，任何未完成的思想和答案都将被丢弃，从而获得零奖励。为了进一步保留模型性能，我们引入了一种迭代长度修剪方法，其中进行了多个RL，每个RL都具有越来越严格的令牌极限。我们观察到，ThinkPrune会导致出色的性能长度折衷 - 在AIME24数据集上，DeepSeek-R1-Distill-Qwen-1.5b的推理长度可以减少一半，而性能下降只有2％。我们还观察到，修剪后，LLM可以绕过不必要的步骤，同时保持核心推理过程完成。代码可从https://github.com/ucsb-nlp-chang/thinkprune获得。]]></description>
      <guid>https://arxiv.org/abs/2504.01296</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生物医学问题通过当地知识图上的多层次摘要回答</title>
      <link>https://arxiv.org/abs/2504.01309</link>
      <description><![CDATA[ARXIV：2504.01309V1公告类型：新 
摘要：在有问题的回答（QA）中，检索增强发电（RAG）彻底改变了各个领域的性能。但是，如何有效捕获多文件关系，尤其是生物医学任务至关重要的问题，仍然是一个悬而未决的问题。在这项工作中，我们提出了一种新的方法，该方法利用命题主张从检索到的文档中构建本地知识图。然后，摘要是通过从知识图来通过layerwise摘要得出的，以将小型语言模型上下文化以执行质量检查。在几个生物医学QA基准上，我们的方法超过了抹布基线，我们的方法具有可比性或卓越的性能。我们还评估了我们方法的每个单独的步骤，以证明其有效性。]]></description>
      <guid>https://arxiv.org/abs/2504.01309</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于测试时间计算缩放的自适应矫正抽样</title>
      <link>https://arxiv.org/abs/2504.01317</link>
      <description><![CDATA[ARXIV：2504.01317V1公告类型：新 
摘要：新发布的OpenAI-O1和DeepSeek-R1证明，测试时间缩放可以显着改善模型性能，尤其是在诸如逻辑推理之类的复杂任务中。常见的测试时间缩放方法涉及产生更多的思想链（COT）或更长的COTS进行自我纠正。但是，尽管自我校正可以提高性能，但如果推理步骤已经正确，它可能会导致大量的令牌废物并降低婴儿床的可读性。为了证明大型语言模型（LLMS）可以在更细粒度的水平上纠正错误，我们提出了自适应矫正抽样（AR-SMPLING），这可以指导LLMS在适当的步骤中进行自我纠正。 AR采样利用过程监督奖励模型（PRM）作为验证者和构造的触发句子，以自适应级别的重新思考指导该模型。通过在GSM8K和MATH500上的实验，它表明我们的方法使模型能够以更细粒度的水平重新考虑，从而提高溶液的准确性，同时产生合理数量的其他标记。]]></description>
      <guid>https://arxiv.org/abs/2504.01317</guid>
      <pubDate>Thu, 03 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>