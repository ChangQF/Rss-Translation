<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CL 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CL</link>
    <description>cs.CL 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Fri, 16 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>AI医院：法学硕士作为临床诊断实习医生的互动评估与协作</title>
      <link>https://arxiv.org/abs/2402.09742</link>
      <description><![CDATA[arXiv:2402.09742v1 公告类型：新
摘要：大型语言模型（LLM）在医疗保健领域的应用标志着一个重大进步。然而，该应用程序主要局限于判别性和问答任务，没有充分发挥其交互潜力。为了解决这个限制，我们的论文提出了 AI Hospital，一个旨在构建实时交互式诊断环境的框架。为了模拟该过程，我们收集高质量的医疗记录来创建患者、检查员和医疗主任代理。然后利用人工智能医院对法学硕士进行交互式评估和协作。最初，我们创建了一个多视图医学评估（MVME）基准，其中各种法学硕士作为实习医生进行交互式诊断。随后，为了提高诊断准确性，我们引入了一种协作机制，该机制涉及在医疗主任的监督下进行迭代讨论和争议解决流程。在我们的实验中，我们验证了人工智能医院的可靠性。结果不仅探讨了法学硕士应用于临床会诊的可行性，而且证实了以争议解决为中心的协作方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.09742</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>出席前对齐：对齐视觉和文本特征以进行多模式仇恨内容检测</title>
      <link>https://arxiv.org/abs/2402.09738</link>
      <description><![CDATA[arXiv:2402.09738v1 公告类型：新
摘要：多模式仇恨内容检测是一项具有挑战性的任务，需要跨视觉和文本模式的复杂推理。因此，创建有意义的多模态表示，通过中间融合有效捕获视觉和文本特征之间的相互作用至关重要。传统的融合技术无法有效地关注特定于模态的特征。此外，大多数研究只集中在英语上，而忽视了其他资源匮乏的语言。本文提出了一种用于多模式仇恨内容检测的上下文感知注意框架，并针对英语和非英语语言对其进行了评估。所提出的方法结合了注意力层来有意义地对齐视觉和文本特征。这种对齐可以在融合之前选择性地关注特定于模态的特征。我们在两个基准仇恨模因数据集上评估了所提出的方法，即。 MUTE（孟加拉语代码混合）和 MultiOFF（英语）。评估结果证明了我们提出的方法的有效性，MUTE 和 MultiOFF 数据集的 F1 分数分别为 69.7$% 和 70.3$%。得分显示，在这些数据集上，与最先进的系统相比，性能提高了大约 2.5$% 和 $3.2$%。我们的实现可以在 https://github.com/eftekhar-hossain/Bengali-Hateful-Memes 上找到。]]></description>
      <guid>https://arxiv.org/abs/2402.09738</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>QuRating：选择高质量数据来训练语言模型</title>
      <link>https://arxiv.org/abs/2402.09739</link>
      <description><![CDATA[arXiv:2402.09739v1 公告类型：新
摘要：选择高质量的预训练数据对于创建强大的语言模型非常重要，但现有方法依赖于简单的启发式方法。我们引入了 QuRating，一种选择预训练数据的方法，可以捕获人类直观感知的文本的抽象品质。在本文中，我们研究了四种品质——写作风格、所需的专业知识、事实和琐事以及教育价值。我们发现法学硕士能够辨别这些品质，并观察到他们更擅长对文本进行成对判断，而不是直接评估文本的质量。我们训练 QuRater 模型来从成对判断中学习标量评分，并使用它来注释 260B 训练语料库，其中包含四个标准中每一个标准的质量评分。在我们的实验中，我们根据不同的质量评分选择 30B 个 token，并在所选数据上训练 1.3B 参数语言模型。我们发现平衡质量和多样性非常重要，因为仅选择评分最高的文档会导致结果不佳。当我们使用质量评级作为文档的逻辑进行采样时，我们的模型比基线实现了更低的困惑度和更强的上下文学习性能。除了数据选择之外，我们还使用质量评级来构建培训课程，在不更改培训数据集的情况下提高性能。我们广泛分析质量评级并讨论其特征、偏见和更广泛的影响。]]></description>
      <guid>https://arxiv.org/abs/2402.09739</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>通过错误暴露和一致性正则化改进非自回归机器翻译</title>
      <link>https://arxiv.org/abs/2402.09725</link>
      <description><![CDATA[arXiv:2402.09725v1 公告类型：新
摘要：作为 IR-NAT（基于迭代细化的 NAT）框架之一，条件屏蔽语言模型（CMLM）采用屏蔽预测范式来重新预测屏蔽的低置信度令牌。然而，CMLM 受到训练和推理之间数据分布差异的影响，其中观察到的标记在两种情况下生成不同。在本文中，我们通过错误暴露和一致性正则化（EECR）的训练方法来解决这个问题。我们在训练期间基于模型预测构建混合序列，并提出在不完善的观察条件下对屏蔽标记进行优化。我们还设计了一种一致性学习方法来约束不同观察情况下屏蔽标记的数据分布，以缩小训练和推理之间的差距。与基础模型相比，在五个翻译基准上的实验分别获得了 0.68 和 0.40 BLEU 分数的平均改进，并且我们的 CMLMC-EECR 在与 Transformer 相当的翻译质量下实现了最佳性能。实验结果证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.09725</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>具有长上下文要点记忆的受人类启发的阅读代理</title>
      <link>https://arxiv.org/abs/2402.09727</link>
      <description><![CDATA[arXiv:2402.09727v1 公告类型：新
摘要：当前的大型语言模型（LLM）不仅限于某些最大上下文长度，而且无法稳健地消耗长输入。为了解决这些限制，我们提出了 ReadAgent，这是一种 LLM 代理系统，在我们的实验中将有效上下文长度增加了 20 倍。受人类交互阅读长文档方式的启发，我们将 ReadAgent 实现为一个简单的提示系统，该系统使用法学硕士的高级语言功能来 (1) 决定在记忆片段中一起存储哪些内容，(2) 将这些记忆片段压缩为短片段（3）如果 ReadAgent 需要提醒自己相关细节来完成任务，则采取行动查找原文中的段落。我们使用检索方法、原始长上下文和要点记忆来根据基线评估 ReadAgent。这些评估是针对三个长文档阅读理解任务进行的：QuALITY、NarrativeQA 和 QMSum。 ReadAgent 在所有三项任务上均优于基线，同时将有效上下文窗口扩展了 3-20 倍。]]></description>
      <guid>https://arxiv.org/abs/2402.09727</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士了解幻觉吗？ LLM隐藏状态的实证研究</title>
      <link>https://arxiv.org/abs/2402.09733</link>
      <description><![CDATA[arXiv:2402.09733v1 公告类型：新
摘要：大型语言模型（LLM）可以编造不真实的答案，这被称为幻觉。这项研究旨在了解法学硕士是否、如何以及在多大程度上意识到幻觉。更具体地说，我们检查法学硕士在正确回答问题时和出现幻觉时在其隐藏状态下的反应是否以及如何不同。为此，我们引入了一个实验框架，允许检查法学硕士在不同幻觉情况下的隐藏状态。在此框架的基础上，我们对 LLaMA 家族中的语言模型进行了一系列实验（Touvron 等人，2023）。我们的实证研究结果表明，法学硕士在处理真实的回应与捏造的回应时的反应不同。然后，我们应用各种模型解释技术来帮助更好地理解和解释研究结果。此外，根据经验观察，我们显示出利用法学硕士隐藏表示空间的指导来减轻幻觉的巨大潜力。我们相信这项工作提供了关于法学硕士如何产生幻觉答案以及如何减少它们出现频率的见解。]]></description>
      <guid>https://arxiv.org/abs/2402.09733</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>PAL：针对大型语言模型的代理引导黑盒攻击</title>
      <link>https://arxiv.org/abs/2402.09674</link>
      <description><![CDATA[arXiv:2402.09674v1 公告类型：新
摘要：近几个月来，大型语言模型 (LLM) 的受欢迎程度激增，但它们已被证明具有在被操纵时生成有害内容的能力。虽然安全微调等技术旨在最大限度地减少有害使用，但最近的研究表明，法学硕士仍然容易受到引起毒性反应的攻击。在这项工作中，我们介绍了对 LLM 的代理引导攻击 (PAL)，这是在黑盒仅查询设置中针对 LLM 的第一个基于优化的攻击。特别是，它依赖于代理模型来指导优化以及为现实世界的 LLM API 设计的复杂损失。我们的攻击在 GPT-3.5-Turbo 上实现了 84% 的攻击成功率 (ASR)，在 Llama-2-7B 上实现了 48% 的攻击成功率 (ASR)，而当前技术水平为 4%。我们还提出了 GCG++，这是对 GCG 攻击的改进，在白盒 Llama-2-7B 上达到 94% ASR，以及 LLM 随机搜索攻击 (RAL)，这是基于查询的攻击的强大但简单的基线。我们相信，这项工作中提出的技术将使法学硕士能够进行更全面的安全测试，并从长远来看，开发出更好的安全护栏。代码可以在 https://github.com/chawins/pal 找到。]]></description>
      <guid>https://arxiv.org/abs/2402.09674</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>世界语语言频率分析及纠错</title>
      <link>https://arxiv.org/abs/2402.09696</link>
      <description><![CDATA[arXiv:2402.09696v1 公告类型：新
摘要：当前的语法错误纠正 (GEC) 计划往往侧重于主要语言，而较少关注世界语等资源匮乏的语言。在本文中，我们首先使用专门为此目的创建的 Eo-GP 数据集进行全面的频率分析，以此来弥补这一差距。然后，我们介绍 Eo-GEC 数据集，该数据集源自真实的用户案例，并用细粒度的语言细节进行注释以进行错误识别。利用 GPT-3.5 和 GPT-4，我们的实验表明，GPT-4 在自动评估和人工评估中均优于 GPT-3.5，突出了其在解决世界语语法特性方面的功效，并说明了高级语言模型在增强不太常见的 GEC 策略方面的潜力。学习过语言。]]></description>
      <guid>https://arxiv.org/abs/2402.09696</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>EntailE：在常识知识图补全中引入文本蕴涵</title>
      <link>https://arxiv.org/abs/2402.09666</link>
      <description><![CDATA[arXiv:2402.09666v1 公告类型：新
摘要：常识知识图谱补全是常识知识图谱构建和应用的新挑战。与 Freebase 和 YAGO 等事实知识图相比，常识知识图（CSKG；例如 ConceptNet）利用自由格式文本来表示命名实体、短语和事件作为其节点。这种松散的结构导致 CSKG 较大且稀疏，这使得这些节点的语义理解对于学习丰富的常识知识图嵌入更加关键。虽然当前的方法利用语义相似性来增加图密度，但节点及其关系的语义合理性尚未得到充分探索。以前的工作采用概念抽象来提高建模（事件）合理性的一致性，但它们的可扩展性不够，并且仍然受到数据稀疏的影响。在本文中，我们建议采用文本蕴涵来寻找 CSKG 节点之间的隐含蕴涵关系，以有效地密集化同一概念类内连接节点的子图，这表明了相似的合理性水平。 CSKG 中的每个节点使用自然语言推理 (NLI) 任务上的微调变压器来找到其顶部蕴涵节点，该任务充分捕获文本蕴涵信号。这些节点之间的蕴涵关系进一步用于：1）在源三元组和蕴涵节点之间建立新的连接以致密稀疏的CSKG； 2）通过将节点嵌入与对比损失进行比较来丰富节点表示的泛化能力。对两个标准 CSKG 的实验表明，我们提出的框架 EntailE 可以在传导和归纳设置下提高 CSKG 完成任务的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.09666</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>API Pack：用于 API 调用生成的海量多语言数据集</title>
      <link>https://arxiv.org/abs/2402.09615</link>
      <description><![CDATA[arXiv:2402.09615v1 公告类型：新
摘要：我们介绍了 API Pack，这是一个多语言数据集，具有超过一百万个指令 API 调用对，旨在提高大型语言模型的 API 调用生成能力。通过实验，我们证明了 API Pack 在增强此专门任务的模型方面的功效，同时保持了一般编码的整体熟练程度。仅在 20,000 个 Python 实例上对 CodeLlama-13B 进行微调，在生成未见过的 API 调用方面，准确率分别比 GPT-3.5 和 GPT-4 高 10% 和 5% 以上。扩展到 100k 个示例可以提高对训练期间未见过的新 API 的泛化能力。此外，无需每种语言的大量数据即可实现跨语言 API 调用生成。数据集、微调模型和整体代码库可在 https://github.com/anonymous_url 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2402.09615</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>答案就是您所需要的：通过回答问题来遵循指令嵌入文本</title>
      <link>https://arxiv.org/abs/2402.09642</link>
      <description><![CDATA[arXiv:2402.09642v1 公告类型：新
摘要：这项工作旨在构建一个文本嵌入器，可以捕获用户指令指定的文本特征。尽管它在部署面向用户的嵌入方面具有巨大的潜力，但以前的方法都没有为其提供具体的解决方案。本文提供了一种新的观点，它将指令视为关于输入文本的问题，并对预期答案进行编码以获得相应的表示。直观上，具有相同（隐式）语义的文本将在指令后共享相似的答案，从而导致更相似的嵌入。具体来说，我们建议 InBedder 通过仅在抽象问答任务上微调语言模型来实例化这种通过回答嵌入的想法。根据我们提出的指令意识测试和指令稳健性测试，当应用于大型语言模型 (LLM)（例如 llama-2-7b）和较小的基于编码器的 LM（例如 roberta-）时，InBedder 展示了显着改进的指令跟踪能力。大的）。此外，我们通过对同一语料库应用不同的指令来实现对聚类结果的定性分析，证明了高度的可解释性。]]></description>
      <guid>https://arxiv.org/abs/2402.09642</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>LogicPrpBank：逻辑蕴涵和等价语料库</title>
      <link>https://arxiv.org/abs/2402.09609</link>
      <description><![CDATA[arXiv:2402.09609v1 公告类型：新
摘要： 逻辑推理在解决问题和决策中至关重要。尽管语言模型（LM）已经证明了处理多种推理任务（例如常识推理）的能力，但它们推理复杂数学问题（特别是命题逻辑）的能力在很大程度上仍未得到充分开发。缺乏探索可归因于注释语料库的可用性有限。在这里，我们提出了一个标记良好的命题逻辑语料库 LogicPrpBank，其中包含跨越六个数学学科的 7093 个命题逻辑语句（PLS），用于研究逻辑蕴涵和等价推理的全新任务。我们使用广泛使用的语言模型对 LogicPrpBank 进行基准测试，以表明我们的语料库为这项具有挑战性的任务提供了有用的资源，并且模型还有足够的改进空间。]]></description>
      <guid>https://arxiv.org/abs/2402.09609</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>大规模实现隐私意识手语翻译</title>
      <link>https://arxiv.org/abs/2402.09611</link>
      <description><![CDATA[arXiv:2402.09611v1 公告类型：新
摘要：手语翻译（SLT）发展的一个主要障碍是数据稀缺。由于缺乏对齐的字幕，目前网络上可用的许多手语数据无法用于训练监督模型。此外，由于生物识别信息的存在，使用大规模网络抓取数据集扩展 SLT 会带来隐私风险，SLT 技术的负责任开发应该考虑到这一点。在这项工作中，我们提出了一个大规模隐私感知 SLT 的两阶段框架，可以解决这两个问题。我们引入了 SSVP-SLT，它利用对匿名和未注释视频的自监督视频预训练，然后对策划的并行数据集进行监督 SLT 微调。 SSVP-SLT 在 How2Sign 数据集上实现了最先进的微调和零样本无光泽 SLT 性能，比最强的相应基线高出 3 BLEU-4 以上。基于对照实验，我们进一步讨论了 SLT 的自监督预训练和通过面部混淆进行匿名化的优点和局限性。]]></description>
      <guid>https://arxiv.org/abs/2402.09611</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>生成大型语言模型中的概率推理</title>
      <link>https://arxiv.org/abs/2402.09614</link>
      <description><![CDATA[arXiv:2402.09614v1 公告类型：新
摘要：本文考虑了大型语言模型 (LLM) 在对包含通过概率值明确量化的不确定性的信息进行推理时所面临的挑战。这种类型的推理与从日常对话到医疗决策等各种环境相关。尽管法学硕士的数学推理能力有所提高，但在概率推理方面他们仍然表现出很大的困难。为了解决这个问题，我们首先引入贝叶斯语言推理数据集（BLInD），这是一个专门为测试法学硕士概率推理能力而设计的新数据集。然后，我们利用这个新数据集彻底说明法学硕士对于涉及概率推理的任务的具体限制，并提出几种将问题映射到不同形式表示的策略，包括Python代码、概率推理算法和概率逻辑编程。最后，我们对我们的 BLInD 方法和因果推理问答数据集的改编进行了评估，这进一步表明了它们的实际有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.09614</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>合理性成绩单：评估大型语言模型的经济合理性</title>
      <link>https://arxiv.org/abs/2402.09552</link>
      <description><![CDATA[arXiv:2402.09552v1 公告类型：新
摘要：人们越来越有兴趣使用法学硕士作为决策“代理人”。这样做包括许多自由度：应该使用哪种模型；应该如何提示；是否应该要求其进行反省、进行链式推理等？解决这些问题——更广泛地说，确定法学硕士代理人是否足够可靠值得信任——需要一种评估此类代理人经济合理性的方法。在本文中，我们提供了一个。我们首先调查有关理性决策的经济文献，对代理应表现出的大量细粒度“元素”以及它们之间的依赖关系进行分类。然后，我们提出一个基准分布，对法学硕士在这些要素上的表现进行定量评分，并结合用户提供的评分标准，生成“理性成绩单”。最后，我们描述了 14 个不同 LLM 的大规模实证实验的结果，描述了当前的技术水平以及不同模型大小对模型表现出理性行为的能力的影响。]]></description>
      <guid>https://arxiv.org/abs/2402.09552</guid>
      <pubDate>Fri, 16 Feb 2024 06:17:04 GMT</pubDate>
    </item>
    </channel>
</rss>