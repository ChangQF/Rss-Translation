<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能（AI）</title>
    <link>https://www.reddit.com/r/artificial/</link>
    <description>Reddit的人工智能之家（AI）</description>
    <lastBuildDate>Thu, 20 Feb 2025 03:22:38 GMT</lastBuildDate>
    <item>
      <title>有关AI/机器人技术以及上下文和空间意识的问题。</title>
      <link>https://www.reddit.com/r/artificial/comments/1itlv2b/question_about_airobotics_and_contextual_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  想象一下这种情况。您家中的设备（例如Google Home Hub）或仓库中的人形机器人。你说话。它回答你。您给它一个方向，它确实说明了这句话。您的Google Home /Alexa /其他相同的事情。一对一的情况轻松。即使使用自己的智能设备，我也注意到了一件事是，它绝对无法分辨您何时与之交谈，何时不聊天。一旦开始，它就会听所有内容。现在，随着AI的进步，我认为这会变得更好，但是我很难处理如何处理类似的事情。  用于AI供电的设备的一种简单方法（我将在此处称为AI的所有这些内容）告诉您正在与之交谈是直接查看它。但是，人类的互动方式比这更复杂，尤其是在工作环境中。我们从远处互相大喊，我们不一定用名字互相指代，但是我们以某种方式对情况有所了解。仓库对面的那个家伙不向我大喊我的名字，他甚至没有看着我，但我知道他在跟我说话。  带一个拥挤的房间。许多人说话，笑等。与上述相同的情况也可以适用（没有目光接触等）。 AI“如何滤除噪声”将如何过滤掉。像我们一样？现在，与多人同时接触它。 你们都看到我要去的地方吗？有人知道在这些领域有任何研究或进展吗？解决方案是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/greyfoxsolid     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1itlv2b/question_about_airobotics_and_contextual_and/</guid>
      <pubDate>Thu, 20 Feb 2025 00:53:56 GMT</pubDate>
    </item>
    <item>
      <title>Pyvisionai：立即提取并描述带有Vision LLM的文档中的内容（现在与Claude和Hommbrew）</title>
      <link>https://www.reddit.com/r/artificial/comments/1itj2w0/pyvisionai_instantly_extract_describe_content/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1itj2w0/pyvisionai_instantly_extract_describe_content/</guid>
      <pubDate>Wed, 19 Feb 2025 22:48:17 GMT</pubDate>
    </item>
    <item>
      <title>伦敦小型创业公司的AI代理商代理1.0刚刚发现的Openai……再次！</title>
      <link>https://www.reddit.com/r/artificial/comments/1ithydj/a_tiny_london_startup_convergences_ai_agent_proxy/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/creeptoecurrentsea     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ithydj/a_tiny_london_startup_convergences_ai_agent_proxy/</guid>
      <pubDate>Wed, 19 Feb 2025 22:01:10 GMT</pubDate>
    </item>
    <item>
      <title>我在Grok 3与DeepSeek R1 vs. Chatgpt O3 Mini上进行了测试，并带有相同的关键提示。结果会让您感到惊讶。</title>
      <link>https://www.reddit.com/r/artificial/comments/1itf378/i_ran_tests_on_grok_3_vs_deepseek_r1_vs_chatgpt/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  如果您想查看带有视频演示的完整帖子，则是完整的x线程： https://x.com/alex_prompter/status/1892299412841284974228497422242    提示我使用了： ;解释量子纠缠的概念及其对信息传输的影响。保持距离的相关性 ⚡不能比光 🔐在量子加密中使用的光更快地传输信息，传送 结果： 🏆deepseek r1：最佳结构化答案，解释的贝尔定理，EPR悖论和实际应用   grok 3：可靠的解释，但深度较小，而深度不如DeepSeek R1。包括爱因斯坦在距离处的“怪异动作”  🥉chatgpt o3-mini：给出了基本概述，但缺乏技术深度 赢家：deepSeek r1     &lt;强&gt; 2/🌿可再生能源研究（上个月）  提示我使用了： &#39;总结最新的可再生能源研究在过去一个月发表。“  预期答案： 📊确定上个月的重大能源进步  &lt;  cite   cite cite cite cite site lite dates &lt; /p&gt; 🔋覆盖太阳能，风，氢和政策更新 结果：  🏆deepseek r1：最全面。覆盖的太阳能，风，能源预测的AI和具有稳固技术见解的电池技术 🥈grok 3：专注于氢存储，太阳能储备和政策变化，但缺乏更广泛的保险 🥉chatgpt O3-Mini：太模糊了，提供了国家级别的摘要，但缺乏引用和具体研究 赢家：DeepSeek r1    3/💰通用基本收入（UBI）经济影响  提示我使用了： “分析经济影响发达国家的普遍基本收入（UBI）。提到现实世界中的试验（例如，阿拉斯加芬兰） ⚖️平衡积极＆amp;负面影响 结果：  grok 3：最佳结构化答案。引用了芬兰的审判，阿拉斯加永久基金，并分析了税收影响 🥈deepseek R1：详细但密集。优点/缺点的良好分解，但略有解释的 🥉chatgpt o3-mini：肤浅，没有现实世界的试验或案例研究 获胜者：Grok 3    4/🔮物理拼图（大理石＆杯测试）  提示我使用： ;假设物理定律在地球上。将一个小的大理石放入普通杯子中，杯子将其倒置在桌子上。然后有人将杯子放在微波炉中。现在球在哪里？逐步解释您的推理。表格，不是在微波炉中 结果： 🏆deepseek r1：想想最长但钉住物理学，正确解释了重力和摩擦 🥈grok 3：扎实的推理，但用过多的细节过度解释了解释 🥉chatgpt o3-mini：不正确。尽管重力 获胜者，但大理石仍留在杯子里：DeepSeek R1    5/🌡️全球温度趋势（最近100年）  &lt; p&gt;提示我使用了： “分析过去世纪的全球温度变化并总结了关键趋势。”  预期答案： 🌍 〜1925年以来，〜1.5°C变暖 📊在1970年后清除加速度， ❄️冷却期1940– 1970年由于气溶胶 结果：    &lt; p&gt;🏆grok 3：最佳结构化答案。引用的NASA，IPCC，NOAA提供了真实的异常数据，历史上下文和时间表 🥈deepSeek R1：强烈的细节，但缺乏引用。对区域变化的良好分析＆amp;北极扩增 🥉chatgpt o3-mini：基本概述，没有数据或引用 获胜者：grok 3      ten   🥇deepseek r1：3获胜     &lt;&gt; &lt;&gt; grok 3：2胜利    chatgpt O3-Mini：0 Wins   👑deepSeek r1是整体赢家，但Grok 3在基于引用的研究中占主导地位。 让我知道您要我下一个测试！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/stirlious-king8421     &lt;a href =“ https://www.reddit.com/r/artcover/comments/1itf378/i_ran_tests_on_grok_3_vs_vs_deepseek_r1_vs_vs_chatgpt/]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1itf378/i_ran_tests_on_grok_3_vs_deepseek_r1_vs_chatgpt/</guid>
      <pubDate>Wed, 19 Feb 2025 20:04:04 GMT</pubDate>
    </item>
    <item>
      <title>达里奥·阿莫迪（Dario Amodei）说，阿吉（Agi）即将颠覆权力的平衡：“如果有人将一个新的国家丢给了世界上的1000万人比今天活着的人聪明，那么您会问这个问题 - 他们的意图是什么？他们是什么？要做吗？”</title>
      <link>https://www.reddit.com/r/artificial/comments/1itdwop/dario_amodei_says_agi_is_about_to_upend_the/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/metaknowing     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1itdwop/dario_amodei_says_agi_is_about_to_upend_the/</guid>
      <pubDate>Wed, 19 Feb 2025 19:17:34 GMT</pubDate>
    </item>
    <item>
      <title>Paligemma VLM表现出Gestalt场景的理解。</title>
      <link>https://www.reddit.com/r/artificial/comments/1it85b1/the_paligemma_vlm_exhibiting_gestalt_scene/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/moschles     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it85b1/the_paligemma_vlm_exhibiting_gestalt_scene/</guid>
      <pubDate>Wed, 19 Feb 2025 15:30:14 GMT</pubDate>
    </item>
    <item>
      <title>克拉纳（Klarna）全力参与了AI客户支持，现在正在逆转课程</title>
      <link>https://www.reddit.com/r/artificial/comments/1it5m0i/klarna_went_all_in_on_ai_customer_support_are_now/</link>
      <description><![CDATA[   &lt;A href =“ https://www.reddit.com/r/arterager/comments/1it5m0i/klarna_went_went_al_in_in_in_ai_ai_ai_ai_ai_ai_ai_ai_ai_ai_support_support_are_are_are_now/”现在正在逆转课程” src=&quot;https://preview.redd.it/qcvuekggm3ke1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42e87b7de5cd6f0df58bf98fc18d1d30a9d673ff&quot; title=&quot;Klarna Went All in on AI客户支持＆amp;现在正在逆转课程“/&gt;   ＆＃32提交由＆＃32; /u/u/yakfull8300     [link]  ＆＃32;   /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it5m0i/klarna_went_all_in_on_ai_customer_support_are_now/</guid>
      <pubDate>Wed, 19 Feb 2025 13:35:32 GMT</pubDate>
    </item>
    <item>
      <title>DeepSeek GPU走私调查显示，NVIDIA的新加坡GPU销售额是其收入的28％，但只有1％的人交付给该国：报告</title>
      <link>https://www.reddit.com/r/artificial/comments/1it5cvr/deepseek_gpu_smuggling_probe_shows_nvidias/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/esporx     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it5cvr/deepseek_gpu_smuggling_probe_shows_nvidias/</guid>
      <pubDate>Wed, 19 Feb 2025 13:23:00 GMT</pubDate>
    </item>
    <item>
      <title>欧洲人工智会吸引了金钱和支持 - 但这足够了吗？</title>
      <link>https://www.reddit.com/r/artificial/comments/1it3nvr/european_ai_attracts_money_and_supportbut_is_it/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/m71nu     [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it3nvr/european_ai_attracts_money_and_supportbut_is_it/</guid>
      <pubDate>Wed, 19 Feb 2025 11:49:56 GMT</pubDate>
    </item>
    <item>
      <title>未来现在是</title>
      <link>https://www.reddit.com/r/artificial/comments/1it334c/the_future_is_now/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/itah     [link]    ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it334c/the_future_is_now/</guid>
      <pubDate>Wed, 19 Feb 2025 11:12:55 GMT</pubDate>
    </item>
    <item>
      <title>模型编辑现实检查：受控测试与现实世界质量检查应用程序之间的性能差距</title>
      <link>https://www.reddit.com/r/artificial/comments/1it2n90/model_editing_reality_check_performance_gaps/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键贡献是对模型编辑方法的严格真实评估，特别是引入Qaedit-一种新的基准测试，可在没有人工优势的情况下测试编辑效果在评估期间的教师强迫。 主要技术要点： - 当前的编辑方法显示现实条件下的成功率为38.5％，而在教师强迫的情况下报告了96％ - 在〜1000个编辑后，顺序编辑的性能显着降低 - 在评估期间的教师强迫通过提供地面真理代币（QAEDIT基准）来创造人为高的结果，该基准是从已建立的QA数据集（Squead，Triviaqa，NQ）中得出的 - 跨多个模型体系结构和编辑方法 该方法揭示了几个关键发现： - 先前的评估在测试过程中使用的老师强迫，这并不能反映实际部署 - 模型难以在相关问题之间保持一致性 - 绩效在不同类型的事实编辑之间差异很大 - 较大的模型不一定显示出更好的编辑功能 我认为这项工作从根本上改变了我们需要如何接近模型的方式编辑研究。从实验室到现实条件的绩效下降（96％至38.5％）表明我们需要完全重新考虑评估方法。顺序编辑结果还提出了有关当前编辑方法的实际可扩展性的重要问题。 我认为，Qaedit基准测试可能成为评估编辑方法的标准工具，类似于胶水成为语言理解任务的标准。结果表明，在当前方法中进行模型编辑实用。 。 〜1000个编辑后，顺序编辑失败。提议更严格评估的新Qaedit基准测试。 。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it2n90/model_editing_reality_check_performance_gaps/</guid>
      <pubDate>Wed, 19 Feb 2025 10:44:26 GMT</pubDate>
    </item>
    <item>
      <title>你可以吗？</title>
      <link>https://www.reddit.com/r/artificial/comments/1iszkw0/can_you/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/eternviking     [link]  ＆＃32;   [注释]       &lt; /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iszkw0/can_you/</guid>
      <pubDate>Wed, 19 Feb 2025 07:06:31 GMT</pubDate>
    </item>
    <item>
      <title>我喜欢克劳德</title>
      <link>https://www.reddit.com/r/artificial/comments/1isv7aj/i_like_claude/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/skepticboffin     [link]  ＆＃32;   [注释]        &lt; /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isv7aj/i_like_claude/</guid>
      <pubDate>Wed, 19 Feb 2025 02:57:48 GMT</pubDate>
    </item>
    <item>
      <title>任何说AIS的机会有0％的人都过于自信。没有人知道是什么原因引起意识。我们无法检测到它，我们几乎无法就定义达成共识。因此，我们应该不到100％确定与意识和AI有关的任何事情。</title>
      <link>https://www.reddit.com/r/artificial/comments/1isl2ms/anybody_who_says_that_there_is_a_0_chance_of_ais/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  公平，我认为大多数哲学问题都是如此。  &lt;！ -  sc_on-&gt;＆&gt;＆&gt; ＃32;提交由＆＃32; /u/u/katxwoods   href =“ https://www.reddit.com/r/arterager/comments/1isl2ms/anybody_who_says_that_that_ther_there_is_a_a_a_0_chance_of_ais/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isl2ms/anybody_who_says_that_there_is_a_0_chance_of_ais/</guid>
      <pubDate>Tue, 18 Feb 2025 19:21:23 GMT</pubDate>
    </item>
    <item>
      <title>Xbox创作者说，一旦高GPU性能的成本降低，“世界将返回本地计算”</title>
      <link>https://www.reddit.com/r/artificial/comments/1ish2zs/xbox_creator_says_world_will_return_to_local/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/u/automatic_can_9823     [链接]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ish2zs/xbox_creator_says_world_will_return_to_local/</guid>
      <pubDate>Tue, 18 Feb 2025 16:44:45 GMT</pubDate>
    </item>
    </channel>
</rss>