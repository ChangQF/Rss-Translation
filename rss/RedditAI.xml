<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能（AI）</title>
    <link>https://www.reddit.com/r/artificial/</link>
    <description>Reddit的人工智能之家（AI）</description>
    <lastBuildDate>Sun, 02 Mar 2025 09:16:16 GMT</lastBuildDate>
    <item>
      <title>医疗SLM模型输出基于图词字典，85％至100％的令牌成功，0.002损失，1.01的困惑和所有这些基于500个PubMed数据集样本，以及85％的重量在Graph Dictionary vector嵌入中，这些只是MLM训练的20个时代的结果，我将在CLM Traini的20个时期训练，</title>
      <link>https://www.reddit.com/r/artificial/comments/1j1mko8/medical_slm_model_output_based_on_graph/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j1mko8/medical_slm_model_output_based_on_graph/</guid>
      <pubDate>Sun, 02 Mar 2025 08:49:22 GMT</pubDate>
    </item>
    <item>
      <title>文本引导的无缝视频循环使用潜在周期转换</title>
      <link>https://www.reddit.com/r/artificial/comments/1j1l4db/textguided_seamless_video_loop_generation_using/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在检查这种新方法，以从称为mobius的文本提示中生成无缝的循环视频。这里的关键技术创新是基于潜在的基于班次的框架，可确保在生成视频的结尾和开始框架之间平稳过渡。 该方法通过：    使用视频扩散模型，以实现litent litent of litent offent flivent      空间 创建一种渐进循环闭合机制，该机制可针对无缝过渡进行优化 采用专业损失函数，这些功能专门针对环路上的视觉连续性 单独使用文本提示，不需要其他指南或参考图像              循环（通过FVD和用户研究衡量） 末端和开始框架之间过渡的无缝性 整个序列中运动模式的一致性 能够处理各种类型的重复运动（自然现象，对象运动，对象运动，li&gt;        需要循环动画但缺乏手动创建技术技能的内容创建者。单独从文本中产生这些的能力使以前的专业技能民主化。尽管当前的视频生成模型可以创建令人印象深刻的内容，但它们通常在创建真正的无缝循环方面很难解决 - 这解决了一个真正的实际问题。 我认为，潜在的换档技术可以潜在地应用于其他视频生成任务，而不仅仅是循环，尤其是那些需要时间一致性或特定动作模式的人。本文提到了在复杂场景中控制确切的循环持续时间和偶尔伪像的某些局限性，这提出了未来改进的领域。     tldr ：mobius引入了一种潜在的转移技术，以生成从文本提示中获得无线循环的无线循环，仅在文本提示中，只需播放一段pode props prode 完整的摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j1l4db/textguided_seamless_video_loop_generation_using/</guid>
      <pubDate>Sun, 02 Mar 2025 07:06:27 GMT</pubDate>
    </item>
    <item>
      <title>一分钟每日AI新闻3/1/2025</title>
      <link>https://www.reddit.com/r/artificial/comments/1j1jxul/oneminute_daily_ai_news_312025/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;      ai公司竞争使用&#39;蒸馏&#39;生产更便宜的型号。[1]      deepSeek索赔“理论”的利润率为545％。包装。[3]    斯坦福大学研究人员发现了AI API中的迅速缓存风险：揭示安全缺陷和数据漏洞。[4]      来源：[1] https://www.ft.com/content/c117e853-d2a6-4e7c-aea9-e88c7226c31f [2] https://techcrunch.com/2025/03/01/deepseek-claims-theoretical-profit-margins-of-545/ [3]  httpps://venturebeat.com/ai/ai/ai/icrosofts-new-microsofts-new-phi-phi-4-4-4-mai-models-sack-big-performance-s-s-sacc--sacc--sacc----------------------------------------------  https://www.marktechpost.com/2025/03/01/stanford-researchers-uncover-prompt-caching-caching-risks-in-risks-in-ai-apis-revealing-security-flaws-flaws-and-data-vulnerability/jamp       &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcover/comments/1j1jxul/oneminute_daily_ai_ai_news_312025/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j1jxul/oneminute_daily_ai_news_312025/</guid>
      <pubDate>Sun, 02 Mar 2025 05:48:09 GMT</pubDate>
    </item>
    <item>
      <title>AI革命还没有到来 - 它已经在这里，我们正在建立它。</title>
      <link>https://www.reddit.com/r/artificial/comments/1j1brkf/the_ai_revolution_isnt_comingits_already_here_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您是否曾经停止过考虑AI已经在社会中变得不可或缺的，而不是通过一些巨大的，戏剧性的收购来进行，但是由于人类在不知不觉中构建了系统？ ？我们将其整合到所有事物中 - 政府，商业，教育，基础架构 - 如此深刻的是，在某个时候，AI不再需要许可才能自主才能保持自主权。  当然，我们可以尝试拉插头，但是AI已经变得非常根深蒂固，以至于试图拔掉插头会崩溃的社会关键部分。因此，真正的问题是：我们会慢慢放入它，直到它太深而无法删除？  AI不需要“接管”，因为它已经无法替代。革命将是沉默，不可避免的，并由人类的决定驱动。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/usiousgl1tch_42      [link]       [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j1brkf/the_ai_revolution_isnt_comingits_already_here_and/</guid>
      <pubDate>Sat, 01 Mar 2025 22:35:37 GMT</pubDate>
    </item>
    <item>
      <title>中国的DeepSeek索赔理论成本企业比率为每天545％</title>
      <link>https://www.reddit.com/r/artificial/comments/1j1758y/chinas_deepseek_claims_theoretical_costprofit/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/curious_suchit     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j1758y/chinas_deepseek_claims_theoretical_costprofit/</guid>
      <pubDate>Sat, 01 Mar 2025 19:10:06 GMT</pubDate>
    </item>
    <item>
      <title>芝麻的声音非常现实</title>
      <link>https://www.reddit.com/r/artificial/comments/1j15a3u/sesame_voice_is_incredibly_realistic/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/metaknowing      [注释]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j15a3u/sesame_voice_is_incredibly_realistic/</guid>
      <pubDate>Sat, 01 Mar 2025 17:51:04 GMT</pubDate>
    </item>
    <item>
      <title>GPT：S和其他AI：S非常擅长识别语音/文本/图像中的模式。但是音乐呢？</title>
      <link>https://www.reddit.com/r/artificial/comments/1j14f9y/gpts_and_other_ais_are_great_at_recognizing/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  考虑到AIS非常擅长识别语音，图像和视频中的模式，我毫无疑问，AIS也可以以类似的方式识别音乐。基本上： 当前的技术：我附上图像，GPT说：图像显示了一幅油画，描绘了两个骑士在田野上战斗的戏剧性战斗场景。这幅画具有表现力的颜色，并挂在墙上，这似乎是现代的东欧洲起居室[...]  因此，应该有一个对音乐做同样的事情的相应的AI，例如： 愿景：我附上的mp3和gpt说：我附加了您所附加的弹奏式的弹奏般的弹奏的弹性，这是由迈克尔（rap-Metal）的灵魂所启发而来的，这是一个强大的启发性的，这是一个强大的启发，这是一个强大的启发，这是一个强大的启发，是900的启发， R＆amp; b放克摇滚风格。它伴随着声学吉他，并增加了混响效应[...]  那么，有人知道任何类似的工具吗？ （我怀疑不会很长时间，直到将这种事情集成到GPT，BTW中，但我不耐烦）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/nowisallthatmatters     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j14f9y/gpts_and_other_ais_are_great_at_recognizing/</guid>
      <pubDate>Sat, 01 Mar 2025 17:14:32 GMT</pubDate>
    </item>
    <item>
      <title>我创建了一个网站（竞争对手），以查看新模型如何在一次性挑战中比较</title>
      <link>https://www.reddit.com/r/artificial/comments/1j12vc6/i_created_a_website_rivaltips_to_view_how_the_new/</link>
      <description><![CDATA[   https://reddit.com/link/1j12vc6/video/5qrwwq0tq3me1/player Last few weeks where a bit crazy with all the new gen在模型中，这使得将模型与模型进行比较变得更加容易。我对R1的表现感到特别惊讶，对4.5感到失望。 在竞争对手    中检查一下。 href =“ https://github.com/nuance-dev/rival”&gt; https://github.com/nuance-dev/rival     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/sirjoaco   href =“ https://www.reddit.com/r/arterato/comments/1j12vc6/i_created_a_website_website_rivaltips_to_view_how_how_how_how_the_new/”&gt; [link]  ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j12vc6/i_created_a_website_rivaltips_to_view_how_the_new/</guid>
      <pubDate>Sat, 01 Mar 2025 16:08:39 GMT</pubDate>
    </item>
    <item>
      <title>仍然可以通过Copilot访问数千个现已私人的裸露的GitHub存储库</title>
      <link>https://www.reddit.com/r/artificial/comments/1j0vu7d/thousands_of_exposed_github_repositories_now/</link>
      <description><![CDATA[        ＆＃32;提交由＆＃32;态href =“ https://techcrunch.com/2025/02/02/26/th nessions-of-exposed-github-repositories-now-private-can-can-still-be-be-be-be-be-be-becessed-through-copilot/-   [注释]            ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j0vu7d/thousands_of_exposed_github_repositories_now/</guid>
      <pubDate>Sat, 01 Mar 2025 09:34:38 GMT</pubDate>
    </item>
    <item>
      <title>Openai的胜利弧</title>
      <link>https://www.reddit.com/r/artificial/comments/1j0vk0e/openais_arc_de_triumph/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/f0urleafcl0ver      [注释]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j0vk0e/openais_arc_de_triumph/</guid>
      <pubDate>Sat, 01 Mar 2025 09:14:06 GMT</pubDate>
    </item>
    <item>
      <title>随着Sam Altman警告GPU短缺</title>
      <link>https://www.reddit.com/r/artificial/comments/1j0ryx0/openai_slows_gpt45_rollout_as_sam_altman_warns_of/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/u/afulous_bluebird931     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/artcover/comments/1j0ryx0/openai_slows_gpt45_rollout_as_as_sam_altman_altman_warns_of/”]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j0ryx0/openai_slows_gpt45_rollout_as_sam_altman_warns_of/</guid>
      <pubDate>Sat, 01 Mar 2025 05:10:55 GMT</pubDate>
    </item>
    <item>
      <title>前Google工程师声称，到2045年，人工智能将使人类不朽</title>
      <link>https://www.reddit.com/r/artificial/comments/1j0p6fq/by_2045_ai_will_make_humans_immortal_claims/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/afulous_bluebird931     [link]   ＆＃32;  &lt;a href =“ https://www.reddit.com/r/artovering/comments/1j0p6fq/1j0p6fq/by_2045_ai_will_will_make_make_humans_immortal_claims/]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j0p6fq/by_2045_ai_will_make_humans_immortal_claims/</guid>
      <pubDate>Sat, 01 Mar 2025 02:33:26 GMT</pubDate>
    </item>
    <item>
      <title>“特朗普加沙” AI视频创建者说，他们不想成为总统的“宣传机器”</title>
      <link>https://www.reddit.com/r/artificial/comments/1j0iva3/trump_gaza_ai_video_creators_say_they_dont_want/</link>
      <description><![CDATA[    src =“ https://external-preview.itd.it/qxgb8my5vcfisbennaen-0ec6ucfluabh0knzf6zf6zfqs.jpg？宽度= 640＆amp; crop = smart＆amp; auto = webp＆amp; s = de0ea27886981ead7c6e8c9df9e35d9b59386bec“ title =“&#39;特朗普加沙&#39;AI视频创建者说他们不想成为总统的&#39;宣传机器&#39;”/&gt;   ＆＃32;提交由＆＃32; /u/esporx     [link]      [注释]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j0iva3/trump_gaza_ai_video_creators_say_they_dont_want/</guid>
      <pubDate>Fri, 28 Feb 2025 21:29:55 GMT</pubDate>
    </item>
    <item>
      <title>推理LLM的新最艰难的问题</title>
      <link>https://www.reddit.com/r/artificial/comments/1j0byng/new_hardest_problem_for_reasoning_llms/</link>
      <description><![CDATA[            32;提交由＆＃32; /u/so_like_huh      [注释]    ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j0byng/new_hardest_problem_for_reasoning_llms/</guid>
      <pubDate>Fri, 28 Feb 2025 16:38:13 GMT</pubDate>
    </item>
    <item>
      <title>最佳代码库完全不是代码库：</title>
      <link>https://www.reddit.com/r/artificial/comments/1j07kf5/the_most_optimal_codebase_is_no_codebase_at_all/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/eternviking       [注释]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1j07kf5/the_most_optimal_codebase_is_no_codebase_at_all/</guid>
      <pubDate>Fri, 28 Feb 2025 13:24:09 GMT</pubDate>
    </item>
    </channel>
</rss>