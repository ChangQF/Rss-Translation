<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能（AI）</title>
    <link>https://www.reddit.com/r/artificial/</link>
    <description>Reddit 的人工智能之家</description>
    <lastBuildDate>Fri, 07 Feb 2025 01:15:11 GMT</lastBuildDate>
    <item>
      <title>OpenAI 使用此 subreddit 测试 AI 说服力 | TechCrunch</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijfuxi/openai_used_this_subreddit_to_test_ai_persuasion/</link>
      <description><![CDATA[        由    /u/A-Dog22  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijfuxi/openai_used_this_subreddit_to_test_ai_persuasion/</guid>
      <pubDate>Thu, 06 Feb 2025 22:46:47 GMT</pubDate>
    </item>
    <item>
      <title>人工智能作弊悖论 - 人工智能模型是否会越来越多地误导用户对其准确性的判断？对新旧法学硕士进行小实验。</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijfqms/the_ai_cheating_paradox_do_ai_models_increasingly/</link>
      <description><![CDATA[  由    /u/sdac-  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijfqms/the_ai_cheating_paradox_do_ai_models_increasingly/</guid>
      <pubDate>Thu, 06 Feb 2025 22:41:39 GMT</pubDate>
    </item>
    <item>
      <title>作者称 Meta 下载了超过 81.7TB 的盗版书籍用于训练 AI</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijf3rz/meta_torrented_over_817tb_of_pirated_books_to/</link>
      <description><![CDATA[        由    /u/esporx 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijf3rz/meta_torrented_over_817tb_of_pirated_books_to/</guid>
      <pubDate>Thu, 06 Feb 2025 22:14:14 GMT</pubDate>
    </item>
    <item>
      <title>研究人员将 DeepSeek 轰动一时的聊天机器人与被禁止在美国开展业务的中国电信联系起来</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijeh4b/researchers_link_deepseeks_blockbuster_chatbot_to/</link>
      <description><![CDATA[        由    /u/F0urLeafCl0ver 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijeh4b/researchers_link_deepseeks_blockbuster_chatbot_to/</guid>
      <pubDate>Thu, 06 Feb 2025 21:48:06 GMT</pubDate>
    </item>
    <item>
      <title>是时候重温一个新时代了，哈利波特风格的感人肖像</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijcrmw/time_to_relive_a_new_era_harry_potter_style/</link>
      <description><![CDATA[        由    /u/ml_guy1  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijcrmw/time_to_relive_a_new_era_harry_potter_style/</guid>
      <pubDate>Thu, 06 Feb 2025 20:39:09 GMT</pubDate>
    </item>
    <item>
      <title>[N] Deepseek 如何训练他们的 R1 模型，以及当今前沿 LLM 是如何训练的</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijbe4k/n_how_deepseek_trained_their_r1_models_and_how/</link>
      <description><![CDATA[https://www.youtube.com/watch?v=aAfanTeRn84 Lex Friedman 最近发布了一篇名为“DeepSeek 的 GPU 优化技巧”的采访。这是一次很棒的幕后观察，展示了 Deepseek 在没有像美国同行那样多的 GPU 的情况下如何训练他们的最新模型。 需要是发明之母，Deepseek 做了几件事-  他们的专家混合配置非常创新，其中他们具有非常高的稀疏因子，8/256 专家激活。这比其他模型高得多，其他模型中 8 位专家中有​​ 2 位被激活。 训练这个模型可能很难，因为只有少数专家真正学习并被激活，这使得模型很弱。他们引入了辅助损失，以确保所有专家都用于所有任务，从而形成一个强大的模型。 专家混合模型的一个挑战是，如果只有少数专家激活，那么只有少数 GPU 可能会计算过载，而其余 GPU 则处于闲置状态。辅助损失也可以防止这种情况发生。 他们走得更远，实现了他们自己版本的 Nvidia NCCL 通信库，并使用更接近汇编级的 PTX 指令来管理 GPU 中的 SM 如何为每个操作进行调度。这种低级优化使他们的模型在有限的硬件上具有非常高的性能。  他们还讨论了研究人员如何使用新的模型架构和数据工程步骤进行实验。他们说，在训练过程中，损失曲线会出现一些峰值，很难确切知道原因。有时训练后它会消失，但有时 ML 工程师必须从较早的检查点重新开始训练。 他们还提到了 YOLO 运行，研究人员将所有可用的硬件和预算都用于尝试获得前沿模型。他们可能会得到一个非常好的模型，也可能会在这个过程中浪费数亿美元。 这次采访实际上是对当今训练前沿 LLM 的幕后情况的一次非常好的深入观察。我很喜欢它，我建议你也去看看！    提交人    /u/ml_guy1   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijbe4k/n_how_deepseek_trained_their_r1_models_and_how/</guid>
      <pubDate>Thu, 06 Feb 2025 19:43:40 GMT</pubDate>
    </item>
    <item>
      <title>分享您最喜欢的基准，这是我的。</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijb6qt/share_your_favorite_benchmarks_here_are_mine/</link>
      <description><![CDATA[我最喜欢的整体基准测试是 livebench。如果您点击显示语言平均值的子类别，您将能够通过 plot_unscrambling 进行排名，对我来说，这是写作最重要的基准： https://livebench.ai/ Vals 对于税务和法律情报很有用： https://www.vals.ai/models 其余的也很有趣： https://github.com/vectara/hallucination-leaderboard https://artificialanalysis.ai/ https://simple-bench.com/ https://agi.safe.ai/ https://aider.chat/docs/leaderboards/ https://eqbench.com/creative_writing.html https://github.com/lechmazur/writing 也请分享您最喜欢的基准！我很想看到一些长上下文基准。    提交人    /u/Mr-Barack-Obama   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijb6qt/share_your_favorite_benchmarks_here_are_mine/</guid>
      <pubDate>Thu, 06 Feb 2025 19:35:05 GMT</pubDate>
    </item>
    <item>
      <title>如何提示 DeepSeek-R1 模型</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijas54/how_to_prompt_the_deepseekr1_model/</link>
      <description><![CDATA[      https://preview.redd.it/niibnvu9kkhe1.png?width=680&amp;format=png&amp;auto=webp&amp;s=d1fce2f1ab39e5be8293a4827fc7cbbae7861821 这其实没什么好奇怪的。像 o1 这样的模型往往能很好地响应直接的指示，而不是分步指南或详细的思路。你只需要清晰地构造输入并使用演示或相关示例来提供上下文，而不是长篇解释。我还没有尝试过使用DeepSeek-R1进行小样本提示，但我怀疑它实际上可能会降低o1的性能。 我的个人发现： - 在 RL 训练中加入多种语言可能会导致混淆 - 地理位置是政治驱动的，所以避免将地理边界设为提示，因为它们非常敏感 - 由于混合了专家，零样本提示结果非常好。    提交人    /u/ml_guy1   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijas54/how_to_prompt_the_deepseekr1_model/</guid>
      <pubDate>Thu, 06 Feb 2025 19:18:03 GMT</pubDate>
    </item>
    <item>
      <title>英国人希望禁止“比人类更聪明”的人工智能</title>
      <link>https://www.reddit.com/r/artificial/comments/1ijaiya/brits_want_to_ban_smarter_than_human_ai/</link>
      <description><![CDATA[        提交人    /u/MetaKnowing   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ijaiya/brits_want_to_ban_smarter_than_human_ai/</guid>
      <pubDate>Thu, 06 Feb 2025 19:07:55 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT 是否比医生更能判断概率？ - 讨论案例研究与 RCT 作为疗效的可靠指标 - 数据点少但疗效高的案例研究能否胜过结果不佳的“黄金标准”大型 RCT？</title>
      <link>https://www.reddit.com/r/artificial/comments/1ij9d53/is_chatgpt_a_better_judge_of_probability_than/</link>
      <description><![CDATA[        提交人    /u/stereomatch   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ij9d53/is_chatgpt_a_better_judge_of_probability_than/</guid>
      <pubDate>Thu, 06 Feb 2025 18:20:54 GMT</pubDate>
    </item>
    <item>
      <title>您如何应对不确定性？</title>
      <link>https://www.reddit.com/r/artificial/comments/1ij7ksh/how_do_you_deal_with_uncertainty/</link>
      <description><![CDATA[我认为生活从来没有像现在这样不确定。未来几年 AGI 的变化和预见性不断增加，这意味着很难适应。没有人确切知道世界将如何变化，作为一个年轻人，我不知道现在该如何度过我的一生。    由   提交  /u/PianistWinter8293   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ij7ksh/how_do_you_deal_with_uncertainty/</guid>
      <pubDate>Thu, 06 Feb 2025 17:08:41 GMT</pubDate>
    </item>
    <item>
      <title>《纽约时报》的《不会飞的飞行器》（1903 年 10 月 9 日）：预测载人飞行需要 100 万至 1000 万年。69 天后，莱特兄弟于 1903 年 12 月 17 日揭穿了这一谎言！</title>
      <link>https://www.reddit.com/r/artificial/comments/1iiok3a/nyts_flying_machines_which_do_not_fly_october_9/</link>
      <description><![CDATA[        提交人    /u/subwaycooler   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iiok3a/nyts_flying_machines_which_do_not_fly_october_9/</guid>
      <pubDate>Wed, 05 Feb 2025 23:42:49 GMT</pubDate>
    </item>
    <item>
      <title>2019 年，预测者认为通用人工智能还需要 80 年才能实现</title>
      <link>https://www.reddit.com/r/artificial/comments/1iiiuuk/in_2019_forecasters_thought_agi_was_80_years_away/</link>
      <description><![CDATA[        提交人    /u/MetaKnowing   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iiiuuk/in_2019_forecasters_thought_agi_was_80_years_away/</guid>
      <pubDate>Wed, 05 Feb 2025 19:44:30 GMT</pubDate>
    </item>
    <item>
      <title>经济学家泰勒·科文 (Tyler Cowen) 表示，深度研究“相当于拥有一名优秀的博士级研究助理，并派他们出去一两周完成一项任务”</title>
      <link>https://www.reddit.com/r/artificial/comments/1iiiqi5/economist_tyler_cowen_says_deep_research_is/</link>
      <description><![CDATA[        由    /u/MetaKnowing 提交   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iiiqi5/economist_tyler_cowen_says_deep_research_is/</guid>
      <pubDate>Wed, 05 Feb 2025 19:39:30 GMT</pubDate>
    </item>
    <item>
      <title>事情很快就升级了</title>
      <link>https://www.reddit.com/r/artificial/comments/1iidv6l/well_that_escalated_quickly/</link>
      <description><![CDATA[        提交人    /u/MetaKnowing   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iidv6l/well_that_escalated_quickly/</guid>
      <pubDate>Wed, 05 Feb 2025 16:22:32 GMT</pubDate>
    </item>
    </channel>
</rss>