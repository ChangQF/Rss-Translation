<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>人工智能（AI）</title>
    <link>https://www.reddit.com/r/artificial/</link>
    <description>Reddit的人工智能之家（AI）</description>
    <lastBuildDate>Wed, 19 Feb 2025 12:30:59 GMT</lastBuildDate>
    <item>
      <title>欧洲人工智会吸引了金钱和支持 - 但这足够了吗？</title>
      <link>https://www.reddit.com/r/artificial/comments/1it3nvr/european_ai_attracts_money_and_supportbut_is_it/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/m71nu     [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it3nvr/european_ai_attracts_money_and_supportbut_is_it/</guid>
      <pubDate>Wed, 19 Feb 2025 11:49:56 GMT</pubDate>
    </item>
    <item>
      <title>未来现在是</title>
      <link>https://www.reddit.com/r/artificial/comments/1it334c/the_future_is_now/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/itah     [link]    ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it334c/the_future_is_now/</guid>
      <pubDate>Wed, 19 Feb 2025 11:12:55 GMT</pubDate>
    </item>
    <item>
      <title>模型编辑现实检查：受控测试与现实世界质量检查应用程序之间的性能差距</title>
      <link>https://www.reddit.com/r/artificial/comments/1it2n90/model_editing_reality_check_performance_gaps/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键贡献是对模型编辑方法的严格真实评估，特别是引入Qaedit-一种新的基准测试，可在没有人工优势的情况下测试编辑效果在评估期间的教师强迫。 主要技术要点： - 当前的编辑方法显示现实条件下的成功率为38.5％，而在教师强迫的情况下报告了96％ - 在〜1000个编辑后，顺序编辑的性能显着降低 - 在评估期间的教师强迫通过提供地面真理代币（QAEDIT基准）来创造人为高的结果，该基准是从已建立的QA数据集（Squead，Triviaqa，NQ）中得出的 - 跨多个模型体系结构和编辑方法 该方法揭示了几个关键发现： - 先前的评估在测试过程中使用的老师强迫，这并不能反映实际部署 - 模型难以在相关问题之间保持一致性 - 绩效在不同类型的事实编辑之间差异很大 - 较大的模型不一定显示出更好的编辑功能 我认为这项工作从根本上改变了我们需要如何接近模型的方式编辑研究。从实验室到现实条件的绩效下降（96％至38.5％）表明我们需要完全重新考虑评估方法。顺序编辑结果还提出了有关当前编辑方法的实际可扩展性的重要问题。 我认为，Qaedit基准测试可能成为评估编辑方法的标准工具，类似于胶水成为语言理解任务的标准。结果表明，在当前方法中进行模型编辑实用。 。 〜1000个编辑后，顺序编辑失败。提议更严格评估的新Qaedit基准测试。 。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it2n90/model_editing_reality_check_performance_gaps/</guid>
      <pubDate>Wed, 19 Feb 2025 10:44:26 GMT</pubDate>
    </item>
    <item>
      <title>AI的Guernica</title>
      <link>https://www.reddit.com/r/artificial/comments/1it0sjh/the_guernica_of_ai/</link>
      <description><![CDATA[        &lt;！ -  sc_off-&gt;   思想？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/riddleofsteel     [link]  ＆＃32;   [注释]        &lt; /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1it0sjh/the_guernica_of_ai/</guid>
      <pubDate>Wed, 19 Feb 2025 08:33:11 GMT</pubDate>
    </item>
    <item>
      <title>您是否尝试过Avante为Neovim？</title>
      <link>https://www.reddit.com/r/artificial/comments/1iszm51/have_you_tried_avante_for_neovim/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我听说过很多好的光标，但我真的不想放开我的珍贵neovim。因此，我听说Avante的目标是复制Neovim的光标体验，但我想知道它有多好？你们中的任何一个都尝试过吗？它可以使用哪种型号来发挥最佳作用？我听说有些人被他们的副驾驶订阅被踢了，因为Avante提出了太多要求。，这笔钱呢？您需要支付模型的溢价还是免费计划以使Avante工作？  是的，如果您有任何经验或任何见解，如果您愿意在这里分享  &lt;！ -  sc_on-&gt;＆＃，我将非常感激。 32;提交由＆＃32; /u/u/osmium999    href =“ https://www.reddit.com/r/artcover/comments/1iszm51/have_you_tried_tried_avante_for_neovim/”&gt; [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iszm51/have_you_tried_avante_for_neovim/</guid>
      <pubDate>Wed, 19 Feb 2025 07:08:50 GMT</pubDate>
    </item>
    <item>
      <title>你可以吗？</title>
      <link>https://www.reddit.com/r/artificial/comments/1iszkw0/can_you/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/eternviking     [link]  ＆＃32;   [注释]       &lt; /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1iszkw0/can_you/</guid>
      <pubDate>Wed, 19 Feb 2025 07:06:31 GMT</pubDate>
    </item>
    <item>
      <title>一分钟每日AI新闻2/18/2025</title>
      <link>https://www.reddit.com/r/artificial/comments/1isyfmr/oneminute_daily_ai_news_2182025/</link>
      <description><![CDATA[在&gt;  Humane&#39;s  AI PIN已死，HP以1.66亿美元的价格购买了创业公司的资产。[2]  随着以色列在战争中使用美国制造的AI模型，因此出现了关注关于技术在谁生活和死亡中的角色。[3]   万事达卡和feedzai团队与AI驱动的骗局作战。[4]     &lt; P&gt;来源：  [1]  https://www.thereverge.com/news/news/news/614742/google-google--google--google--google--google--满足gemini-ai-note-take-taking-action-items    [2]  https://techcrunch.com/2025/02/18/humanes-ai-pin-is-is-dead-as-hp-buys-startups-astartups-assets-for-116m/    [3]  https://apnews.com/article/israel-palestinians-ai-technology-737bc17b03e98c29c29cec4e15d0f108    [4]  https://www.pymnts.com/fraud-prevention/2025/mastercard-and-feedzai-team-team-to-fight-ai-power-scams/    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcover/comments/1isyfmr/oneminute_daily_ai_ai_news_2182025/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isyfmr/oneminute_daily_ai_news_2182025/</guid>
      <pubDate>Wed, 19 Feb 2025 05:53:27 GMT</pubDate>
    </item>
    <item>
      <title>最好的AI API用于处理实时数据？</title>
      <link>https://www.reddit.com/r/artificial/comments/1isvs9e/best_ai_api_for_handling_live_data/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我正在寻找有关处理和分析实时数据的最佳AI API的建议 -  DATA  -  data data持续进行实时更新。理想情况下，API应处理：   流数据输入（WebSockets，kafka等）   快速推理速度   可伸缩性用于高频更新   支持各种AI模型（例如，NLP，图像处理，预测分析）  易于集成与云平台（aws，gcp，azure等）   Openai，Google Vertex AI和AWS Bedrock，但是我很好奇是否有更好的选择，尤其是对于实时分析。 您对这些或其他AI API的经验是什么？我应该考虑任何优点和缺点？ 预先感谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/artcover/comments/1isvs9e/best_ai_api_for_handling_handling_live_data/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isvs9e/best_ai_api_for_handling_live_data/</guid>
      <pubDate>Wed, 19 Feb 2025 03:26:32 GMT</pubDate>
    </item>
    <item>
      <title>我喜欢克劳德</title>
      <link>https://www.reddit.com/r/artificial/comments/1isv7aj/i_like_claude/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/skepticboffin     [link]  ＆＃32;   [注释]        &lt; /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isv7aj/i_like_claude/</guid>
      <pubDate>Wed, 19 Feb 2025 02:57:48 GMT</pubDate>
    </item>
    <item>
      <title>任何说AIS的机会有0％的人都过于自信。没有人知道是什么原因引起意识。我们无法检测到它，我们几乎无法就定义达成共识。因此，我们应该不到100％确定与意识和AI有关的任何事情。</title>
      <link>https://www.reddit.com/r/artificial/comments/1isl2ms/anybody_who_says_that_there_is_a_0_chance_of_ais/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  公平，我认为大多数哲学问题都是如此。  &lt;！ -  sc_on-&gt;＆&gt;＆&gt; ＃32;提交由＆＃32; /u/u/katxwoods   href =“ https://www.reddit.com/r/arterager/comments/1isl2ms/anybody_who_says_that_that_ther_there_is_a_a_a_0_chance_of_ais/”&gt; [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isl2ms/anybody_who_says_that_there_is_a_0_chance_of_ais/</guid>
      <pubDate>Tue, 18 Feb 2025 19:21:23 GMT</pubDate>
    </item>
    <item>
      <title>Xbox创作者说，一旦高GPU性能的成本降低，“世界将返回本地计算”</title>
      <link>https://www.reddit.com/r/artificial/comments/1ish2zs/xbox_creator_says_world_will_return_to_local/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/u/automatic_can_9823     [链接]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1ish2zs/xbox_creator_says_world_will_return_to_local/</guid>
      <pubDate>Tue, 18 Feb 2025 16:44:45 GMT</pubDate>
    </item>
    <item>
      <title>这将任何噪音变成了SFXS</title>
      <link>https://www.reddit.com/r/artificial/comments/1isgrn7/this_turns_any_noise_into_sfxs/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  刚刚遇到了一个视频，一个人将他的声音变成效果，例如Adobe预览的Sketch2sound Thing。我觉得这将是改变电影制片厂创建内容的方式之一。   https://reddit.com/link/link/link/1isgrn7/video/phlwbndocxje1/player  &gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/meadless-investment1     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isgrn7/this_turns_any_noise_into_sfxs/</guid>
      <pubDate>Tue, 18 Feb 2025 16:31:41 GMT</pubDate>
    </item>
    <item>
      <title>探索非算法计算模式：自然和人工计算的框架</title>
      <link>https://www.reddit.com/r/artificial/comments/1isbs4c/exploring_nonalgorithmic_modes_of_computing_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文通过表示和解释的镜头研究人工和生物计算系统之间的基本差异。关键的技术贡献是一个正式的分析框架，将机器和生物如何处理信息进行对比。 关键技术点： - 人造系统依赖于明确的符号表示与固定的解释规则 - 生物系统使用动态，依赖于上下文依赖于上下文信息的解释 - 神经网络和当前的AI方法试图弥合这一差距，但以关键方式缩小 - 本文提供了比较算法与生物学信息的数学模型处理 结果显示了当前AI方法的几个关键局限性： - 模式识别能力不会转化为真实理解 - 固定代表方案限制了灵活性 - 缺乏上下文感知解释 - 数据处理和有意义的有意义的差距理解 我认为这种分析可能会影响我们如何构建与生物计算更好一致的AI系统。我们可能需要从根本上进行动态解释和上下文处理。 我认为最大的挑战是我们还没有好的，而不是试图将类似于生物的行为施加到传统的计算框架中。生物系统如何实现灵活解释的正式模型。虽然本文提供了一个理论框架，但将其转化为实用的AI系统仍然是一个开放的挑战。  tldr：详细分析当前AI系统为何在其表示和解释信息方面与生物学计算有根本不同。建议可能需要新的方法来弥合这一差距。   。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1isbs4c/exploring_nonalgorithmic_modes_of_computing_a/</guid>
      <pubDate>Tue, 18 Feb 2025 12:41:21 GMT</pubDate>
    </item>
    <item>
      <title>哪些很棒的YouTube频道可以解释AI中最新的发展？</title>
      <link>https://www.reddit.com/r/artificial/comments/1is2mgl/what_are_some_great_youtube_channels_that_explain/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  您最喜欢的是什么？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/bearhunter429     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1is2mgl/what_are_some_great_youtube_channels_that_explain/</guid>
      <pubDate>Tue, 18 Feb 2025 02:57:26 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA计算每10个月加倍</title>
      <link>https://www.reddit.com/r/artificial/comments/1irm1wx/nvidia_compute_is_doubling_every_10_months/</link>
      <description><![CDATA[   ”   ＆＃32;提交由＆＃32; /u/metaknowing     [link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/artificial/comments/1irm1wx/nvidia_compute_is_doubling_every_10_months/</guid>
      <pubDate>Mon, 17 Feb 2025 15:13:14 GMT</pubDate>
    </item>
    </channel>
</rss>