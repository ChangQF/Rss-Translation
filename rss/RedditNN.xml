<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的子版块。</description>
    <lastBuildDate>Sun, 05 Jan 2025 06:22:12 GMT</lastBuildDate>
    <item>
      <title>训练神经网络进行手部运动识别</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1htk2iv/training_a_neural_network_for_hand_movement/</link>
      <description><![CDATA[我正在使用自己的数据集训练神经网络来识别特定的手部动作。由于我自己创建了数据集，因此它仅包含有限数量的图像，并且我已经应用了数据增强来增加数据集大小。 但是，我在某些类别上仍然得到较差的结果。鉴于我的数据集很小并且由主体执行手势的图像组成，我想知道：  我是否应该裁剪图像中的手以专注于手势，还是最好将整个主体包含在图像中？ 您能否推荐一些轻量级的预训练模型（大小为几 MB）供我用于此任务？     提交人    /u/Wonderful-Beat3355   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1htk2iv/training_a_neural_network_for_hand_movement/</guid>
      <pubDate>Sat, 04 Jan 2025 17:57:25 GMT</pubDate>
    </item>
    <item>
      <title>一些神经元真的能够被两种不同的模式激活吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hthbkk/is_it_true_that_some_neurons_can_be_activated_by/</link>
      <description><![CDATA[我记得我曾经看过一个视频，解释神经网络如何对图像进行分类。在这个视频中，他们展示了前几层如何关注简单的模式，比如边缘或点，但是随着我们向上看，我们看到激活某些神经元的模式，我们开始识别眼睛或手之类的东西，最终我们可以看到蛇、飞机等等。但是在这个视频中，他们还展示了一些神经元可以被两个看似不相关的概念激活，比如猫和汽车，或者狐狸和汽车，或者诸如此类的东西。他们解释说这是有道理的，神经元必须能够执行多任务，毕竟模式比神经元多，所以它们当然必须识别不止一种东西，然后其他神经元可以通过寻找其他模式来细化结果，比如眼睛或轮子。我记得很清楚，但我找不到视频。但是我不需要视频，我只需要确保这是真的，那么，是吗？单个神经元能被两种不同的模式激活吗？    提交人    /u/Frigorifico   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hthbkk/is_it_true_that_some_neurons_can_be_activated_by/</guid>
      <pubDate>Sat, 04 Jan 2025 15:56:03 GMT</pubDate>
    </item>
    <item>
      <title>无法正确预测</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1htgjyo/not_able_to_predoct_properly/</link>
      <description><![CDATA[免责声明：这是我第一次创建神经网络 因此，我创建了一个神经网络来预测手写数字（参见 Samson zhangs 视频）现在在 mnsit 数据集上训练它的准确率约为 88%，但每当我提供自己的输入（用 paint 制作的 28x28 绘图）时，它都无法正确预测 sm1 可以帮忙吗（我正在为我的学校项目尝试这样做）    提交人    /u/SorbetMajor7934   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1htgjyo/not_able_to_predoct_properly/</guid>
      <pubDate>Sat, 04 Jan 2025 15:20:40 GMT</pubDate>
    </item>
    <item>
      <title>过度拟合和欠拟合 - 简单解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1htgf1u/overfitting_and_underfitting_simply_explained/</link>
      <description><![CDATA[        提交人    /u/Personal-Trainer-541   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1htgf1u/overfitting_and_underfitting_simply_explained/</guid>
      <pubDate>Sat, 04 Jan 2025 15:14:06 GMT</pubDate>
    </item>
    <item>
      <title>有人能帮我做这个吗？哪怕只是第一步？RapidMiner 问题。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hteor6/can_someone_help_me_how_to_do_this_even_just_the/</link>
      <description><![CDATA[      对于这 3 个任务我是否使用数据集 3 次？并且每个任务单独执行？还是它们都应该从任务 1 连接到任务 3？ 比如将数据集用于任务 1 然后使用另一个数据集用于任务 2 然后使用另一个数据集用于任务 3？    提交人    /u/Pristine_Tell_2450   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hteor6/can_someone_help_me_how_to_do_this_even_just_the/</guid>
      <pubDate>Sat, 04 Jan 2025 13:46:29 GMT</pubDate>
    </item>
    <item>
      <title>我的神经网络无法识别使用 HTML Canvas 绘制的 PNG 中的数字，但它可以识别在其他应用程序中绘制的 PNG 中的数字。有人能帮我找出原因吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hsp5l9/my_neural_network_cant_recognise_digits_from_pngs/</link>
      <description><![CDATA[我用 Python 创建了一个神经网络，并使用来自 MNIST 数据集的 100 张图像对其进行了训练。它可以以相对较高的准确度识别我在 Figma 等应用程序中创建的 28x28 PNG 中的数字，但似乎无法识别我使用 HTML Canvas 绘制的 28x28 图像。 这是我的 Python 代码，它使用 imageio 库加载 PNG： print (&quot;loading ... my_own_images/2828_my_own_image.png&quot;) img_array = imageio.v3.imread(&#39;my_own_images/2828_my_own_image.png&#39;, mode=&#39;F&#39;)  # 从 28x28 重塑为 784 个值的列表，反转值 img_data = 255.0 - img_array.reshape(784)  # 将数据缩放到 0.01 到 1.0 的范围 img_data = (img_data / 255.0 * 0.99) + 0.01 如果有人有任何建议，我将不胜感激 - 尽管我为 HTML 画布提供的 React.js 代码很长，但我很乐意在必要时提供任何进一步的代码。    提交人    /u/RedGiraffeElephant   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hsp5l9/my_neural_network_cant_recognise_digits_from_pngs/</guid>
      <pubDate>Fri, 03 Jan 2025 15:36:12 GMT</pubDate>
    </item>
    <item>
      <title>制作一个国际象棋引擎可视化 GUI，让你了解基于神经网络的国际象棋引擎是如何思考的</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hs7x0s/making_a_chess_engine_visualization_gui_that_lets/</link>
      <description><![CDATA[      大家好，我是一名高中生，正在为一个使用 lc0 的学校项目开发这款国际象棋可视化工具，该工具通过详细输出模式和引擎分析制作了神经网络评估热图。您可以与引擎对战，也可以将其用作分析工具，以了解基于 NN 的引擎如何“思考”。链接到 youtube 预览：https://www.youtube.com/watch?v=7nbWr8TR6nA 预览 github：https://github.com/jay63683/BlackBox-Chess-a-XAI-leela-chess-GUI 需要 Processing 才能运行。或者，如果您不想下载 Processing，您可以直接观看视频教程。计划将引擎切换到 ONNX 以进行未来的更新，这样我就可以使用 ONNX 工具更深入地解释流程。非常感谢您的反馈。    由   提交  /u/Lower_Junket_222   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hs7x0s/making_a_chess_engine_visualization_gui_that_lets/</guid>
      <pubDate>Thu, 02 Jan 2025 23:31:17 GMT</pubDate>
    </item>
    <item>
      <title>PyReason 入门教程：宠物商店示例</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hp9654/intro_pyreason_tutorial_pet_store_example/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hp9654/intro_pyreason_tutorial_pet_store_example/</guid>
      <pubDate>Sun, 29 Dec 2024 23:34:54 GMT</pubDate>
    </item>
    <item>
      <title>可视化神经网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hoyzue/visualizing_neural_networks/</link>
      <description><![CDATA[大家好，我正在尝试为我的论文制作一些漂亮的神经网络可视化，但我觉得它们都很糟糕。是否有可视化神经网络的标准化或某种人工智能工具可以做到这一点？ 我有两个网络，一个只有 LSTM 和一个输出，另一个编码器解码器框架也使用 LSTM。真的很想为它们做一个漂亮的可视化。    提交人    /u/Packman-2022   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hoyzue/visualizing_neural_networks/</guid>
      <pubDate>Sun, 29 Dec 2024 16:03:24 GMT</pubDate>
    </item>
    <item>
      <title>提高物理神经网络的学习能力</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hoshuk/improve_learning_for_physics_informed_neural/</link>
      <description><![CDATA[      大家好， 我目前正在使用 DeepXDE 库研究 PINN，用于热传输方程的逆参数估计。虽然 PINN 总体运行良好，但我在学习过程中遇到了一个问题：最初，训练进展顺利，但在某个点之后，损失函数开始波动（见图）。 我使用了 Adam 优化器和 L-BFGS-B 算法的组合。尽管尝试了各种设置，我仍然无法解决这个问题。 有人有什么技巧或建议来改善学习过程并稳定损失函数吗？ 提前谢谢您！ https://preview.redd.it/yrr9g628ar9e1.png?width=640&amp;format=png&amp;auto=webp&amp;s=a013a6e099cec3b4c9b74a98db2eb470f56e2fce    由   提交  /u/ConsistentDimension9   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hoshuk/improve_learning_for_physics_informed_neural/</guid>
      <pubDate>Sun, 29 Dec 2024 09:24:21 GMT</pubDate>
    </item>
    <item>
      <title>Meta 发布 Byte Latent Transformer：一种改进的 Transformer 架构</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ho4d97/meta_released_byte_latent_transformer_an_improved/</link>
      <description><![CDATA[Byte Latent Transformer 是 Meta 推出的一种新型改进型 Transformer 架构，它不使用标记化，可以直接处理原始字节。它引入了基于熵的补丁概念。通过此处的示例了解完整架构及其工作原理：https://youtu.be/iWmsYztkdSg    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ho4d97/meta_released_byte_latent_transformer_an_improved/</guid>
      <pubDate>Sat, 28 Dec 2024 12:05:07 GMT</pubDate>
    </item>
    <item>
      <title>AQLM-rs：如何在浏览器中运行 llama 3.1 8B</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ho37yd/aqlmrs_how_to_run_llama_31_8b_in_browser/</link>
      <description><![CDATA[今年 5 月，Yandex Research 的一个团队与 ISTA 和 KAUST 合作，发布了一种名为 PV-tuning 的新型 SOTA 量化方法。 其中一位作者的这个项目使用 PV-tuning 压缩在任何现代浏览器中运行类似 Llama 3.1 8B 的模型。 演示 代码    提交人    /u/azalio   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ho37yd/aqlmrs_how_to_run_llama_31_8b_in_browser/</guid>
      <pubDate>Sat, 28 Dec 2024 10:40:16 GMT</pubDate>
    </item>
    <item>
      <title>新的 AR 方法：更快的图像生成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hncu5e/new_ar_approach_faster_image_generation/</link>
      <description><![CDATA[一种有趣的方法：AR 模型现在按比例工作，将生成速度提高了 7 倍。激活稳定以确保可靠性。  https://huggingface.co/papers/2412.01819    提交人    /u/ActualInternet3277   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hncu5e/new_ar_approach_faster_image_generation/</guid>
      <pubDate>Fri, 27 Dec 2024 11:19:40 GMT</pubDate>
    </item>
    <item>
      <title>转换后的训练集数据最终存储于 NN/CNN 的哪里？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hlqcae/where_does_the_converted_training_set_data_ends/</link>
      <description><![CDATA[      因此，进行训练，训练结束后，以类似的方式开始探测，数据通过网络运行以获得概率。假设我有 100 张图像要训练我的 CNN 网络。 这里的想法是，这 100 张图像最终会出现在网络中的什么位置，它们会以什么形式存储？......以及在网络内部的什么位置，它们最终会出现在网络中的什么位置。 因此，有 100 张图像，它们的值最终会出现在哪里，我的意思是，网络如何存储这么多图像，必须有一个地方存放它们，它们在反复反向传播后会驻留在整个网络中？ 我很难理解它们（训练集）的存储方式和位置，它们是作为网络中的权重还是神经元值存储的？ 例如，当您探测网络并在图像卷积后进行前向传递时，这些训练集不会被前向传递后分配给神经元的新值覆盖吗？ 所以我的问题是： 训练集是为了帮助您在训练模型后预测您需要什么正在使用单幅图像进行探测，以使其更准确？我如何使用一张图片对分布在网络中哪个位置的训练集进行探测？以及训练集图像值变成什么样。 我理解探测和步骤（从损失函数级别进行前向传递和反向传播）我不理解使用多幅图像作为集合的训练部分，如 - 数据转换为什么，神经元值，权重？ - 这些转换后的数据最终在网络中的什么位置，它存储在哪里（训练集） 没有关于训练集以及它们最终在哪里或转换为什么以及它们在网络中的位置的教程详细信息，我的意思是我还没有设法找到它 编辑：制作了一个图表。 https://preview.redd.it/iwp0m0ojc09e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=a5a0c34c62b8765085a33e8cf2a5079ddf458033 https://preview.redd.it/p3zga7ojc09e1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=55fcb7e73ac3670676adb9771db56ad1e6613a4c .    提交人    /u/No-Earth-374   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hlqcae/where_does_the_converted_training_set_data_ends/</guid>
      <pubDate>Wed, 25 Dec 2024 00:43:58 GMT</pubDate>
    </item>
    <item>
      <title>分析 DAI 稳定币机制和稳定性的形式逻辑框架</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1hlgo0m/formal_logic_framework_for_analyzing_dai/</link>
      <description><![CDATA[本文提出了一个基于形式逻辑的框架，使用 Prolog 分析 DAI 稳定币系统。关键创新是将 DAI 的复杂机制转化为可以模拟和验证其稳定性属性的程序模型。 关键技术方面： - 在 Prolog 的声明式逻辑编程范式中实现 DAI 的核心机制 - 抵押品要求、清算程序和价格反馈的形式化表示 - 模拟市场场景和压力测试稳定性机制的能力 - 用于分析稳定币设计的开源框架 主要结果： - 成功建模了 DAI 的主要稳定机制 - 展示了加密抵押如何与算法方法相结合 - 确定了系统对各种市场条件的响应 - 创建了可重复使用的稳定币分析框架 我认为这项工作为分析其他稳定币设计和 DeFi 协议开辟了重要的可能性。正式框架可以帮助开发人员在部署之前识别潜在的漏洞，并帮助监管机构了解这些系统。 我认为简化的市场行为建模的局限性很大——现实世界的动态比纯逻辑编程所能捕捉到的要复杂得多。然而，这里奠定的基础可以通过更复杂的市场模型进行扩展。 TLDR：研究人员创建了一个基于 Prolog 的形式框架来分析 DAI 的稳定机制，提供了一种系统的方法来理解和验证稳定币设计。 完整摘要在这里。论文这里。    提交人    /u/Successful-Western27   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1hlgo0m/formal_logic_framework_for_analyzing_dai/</guid>
      <pubDate>Tue, 24 Dec 2024 16:11:03 GMT</pubDate>
    </item>
    </channel>
</rss>