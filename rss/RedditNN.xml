<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Wed, 20 Mar 2024 18:16:14 GMT</lastBuildDate>
    <item>
      <title>自然语言指令诱导神经元网络中的成分泛化</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1biry2e/natural_language_instructions_induce/</link>
      <description><![CDATA[   /u/nickb  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1biry2e/natural_language_instructions_induce/</guid>
      <pubDate>Tue, 19 Mar 2024 18:50:46 GMT</pubDate>
    </item>
    <item>
      <title>像素完美：工程师的新方法使图像成为焦点</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bilj9u/pixel_perfect_engineers_new_approach_brings/</link>
      <description><![CDATA[   /u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bilj9u/pixel_perfect_engineers_new_approach_brings/</guid>
      <pubDate>Tue, 19 Mar 2024 14:26:06 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习？数学公式解释了它们如何检测相关模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bid6t5/how_do_neural_networks_learn_a_mathematical/</link>
      <description><![CDATA[       由   提交/u/nickb  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bid6t5/how_do_neural_networks_learn_a_mathematical/</guid>
      <pubDate>Tue, 19 Mar 2024 05:52:47 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的时间之箭</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bhnp5i/arrows_of_time_for_large_language_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.17505 摘要：  我们通过以下方法研究自回归大型语言模型执行的概率建模：时间方向性的角度。我们凭经验发现此类模型在模拟自然语言的能力方面表现出时间不对称性：尝试预测下一个标记与尝试预测前一个标记时的平均对数困惑度存在差异。这种差异同时是微妙的，并且在各种模式（语言、模型大小、训练时间……）中非常一致。从理论上讲，这是令人惊讶的：从信息论的角度来看，不应该存在这样的差异。我们提供了一个理论框架来解释这种不对称性是如何从稀疏性和计算复杂性考虑中出现的，并概述了我们的结果所带来的一些观点。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bhnp5i/arrows_of_time_for_large_language_models/</guid>
      <pubDate>Mon, 18 Mar 2024 10:43:41 GMT</pubDate>
    </item>
    <item>
      <title>对反向传播算法的疑问。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bhcz4b/doubt_about_backpropagration_algorithm/</link>
      <description><![CDATA[大家好，我是人工智能方面的新手，也是工程专业的学生。因此，我正在尝试编写一个具有两个隐藏层和一个输出层神经元的多层感知器，以在没有任何库的情况下仅使用线性代数来执行回归任务。我认为我理解反向传播背后的数学，但在编码时我有疑问。 ​ 在反向传播期间，我写了类似的东西这个（只是为了知道，我在编码之前在三星笔记中编写代码）： ​ % 第二个隐藏层 dE_dY = grad3*w3 dY_dI = g(L2_input,derivative = &quot;True&quot;) grad2 = dI_dy*dI_dy dI_dW = L1_output &lt; p&gt;​ W2 = W2 -(learning_rate)*(-grad2*dI_dW) ​ 这部分是我的代码正在更新第二个隐藏层的权重。我的问题是尺寸。因为当我计算 grad2 时，在我的例子中是相同维度的 2 个列向量的乘法，例如 (1xn)*(1xn)，如你所知，我无法将其相乘。  ​ 由于我知道权重矩阵的维度，例如 MxN，我知道 --&gt; (learning_rate)*(-grad2*dI_dW) &lt;--- 需要具有相同的维度。这样，实现维度 MxN 的唯一可能性是如果我实现操作 dI_dy.*dI_dy，其中运算符“.*”是将 dI_dy 的每个元素乘以 dI_dy 的每个元素，得到 (1xn)。 ​ 我的疑问是：在数学中，- -&gt;dI_dy*dI_dy&lt;--- 只是一个乘法，但是当我编码时，似乎我需要使用“.*”，但我不知道这是否正确。&lt; /p&gt; ​ 只是想知道，我正在 MATLAB 中编程。 很抱歉，文字很长，如果我说得不够清楚，请告诉我（我的母语不是英语）。 ​   由   提交 /u/Jotavebechis   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bhcz4b/doubt_about_backpropagration_algorithm/</guid>
      <pubDate>Mon, 18 Mar 2024 00:04:27 GMT</pubDate>
    </item>
    <item>
      <title>LLM 应用程序的评估指标摘要（RAG、聊天机器人、摘要）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bh82wf/summary_of_eval_metrics_for_llm_apps_rag_chatbot/</link>
      <description><![CDATA[       由   提交/u/jdogbro12  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bh82wf/summary_of_eval_metrics_for_llm_apps_rag_chatbot/</guid>
      <pubDate>Sun, 17 Mar 2024 20:43:50 GMT</pubDate>
    </item>
    <item>
      <title>验证链 (COVE) 解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bh5xv3/chainofverification_cove_explained/</link>
      <description><![CDATA[   /u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bh5xv3/chainofverification_cove_explained/</guid>
      <pubDate>Sun, 17 Mar 2024 19:18:42 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习进行脑肿瘤分类</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bh2lna/brain_tumor_classification_using_deep_learning/</link>
      <description><![CDATA[      https://preview .redd.it/yapyr9cmexoc1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=355d17b34d2478a44d9427cd80b11f6916c440c5 欢迎来到脑肿瘤初学者教程，在这里我们深入探讨 CNN 的世界（卷积神经网络）及其在图像分类和脑肿瘤检测中的突破性应用。 这是一个简单的卷积神经网络教程，演示如何在图像数据集中检测脑肿瘤。 我们将使用 CNN 构建和训练模型，并查看模型的准确性和准确性。损失，然后我们将使用新图像测试和预测肿瘤。 以下是视频链接：https:/ /youtu.be/-147KGbGI3g 享受 Eran #cnnforimageclassification #cnnmachinelearningmodel #cnnml #deeplearningbraintumorclassification #aiDetectbraintumor   由   提交 /u/Feitgemel   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bh2lna/brain_tumor_classification_using_deep_learning/</guid>
      <pubDate>Sun, 17 Mar 2024 17:04:01 GMT</pubDate>
    </item>
    <item>
      <title>马达琳网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bgtmx6/madaline_network/</link>
      <description><![CDATA[是否有一个正式的证据来说明为什么要设置更新规则，我知道与简单的反向传播相比，这是一个愚蠢的模型，我很容易理解（链式法则和错误道具）但是 Madaline 规则感觉很奇怪，只是想到而不是派生的东西    由   提交/u/borisshootspancakes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bgtmx6/madaline_network/</guid>
      <pubDate>Sun, 17 Mar 2024 09:32:34 GMT</pubDate>
    </item>
    <item>
      <title>关于 RNN 的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bg3gm8/question_about_rnns/</link>
      <description><![CDATA[在 RNN 中，神经元的输出是否会：  直接返回到与输入相同的神经元？  或  回到上一层，然后再次触发同一个神经元，同时也触发其他神经元？   如果这两种类型的 RNN 都存在，它们的名字是什么？   由   提交/u/BePoliter  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bg3gm8/question_about_rnns/</guid>
      <pubDate>Sat, 16 Mar 2024 10:55:40 GMT</pubDate>
    </item>
    <item>
      <title>这是什么程序？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bfxoag/what_program_is_this/</link>
      <description><![CDATA[      我正在 YouTube 上学习神经网络的基础知识，老师使用了下面的工具解释材料的某些部分。不过他没有提到名字，有谁知道并可以告诉我名字吗？ https://preview.redd.it/cma43faqhmoc1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=57fc2d9cd55b07f815f5bb336a3a 07cdb8e4aadf &lt; /div&gt;  由   提交 /u/ConfectionPuzzled271   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bfxoag/what_program_is_this/</guid>
      <pubDate>Sat, 16 Mar 2024 04:18:00 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的自组织映射邻域实现</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bevbd9/selforganizing_map_neighborhood_implementation_in/</link>
      <description><![CDATA[我正在尝试实现一个自组织映射，其中对于给定的输入样本，根据（例如）选择最佳匹配单元/获胜单元SOM 和输入之间的 L2 范数距离。获胜单元/BMU (som[x, y]) 与给定输入 (z) 的 L2 距离最小： # 输入批次：batch-size = 512, input-dim = 84- z = torch.randn(512, 84) # SOM 形状：（高度、宽度、输入-dim)- som = torch.randn(40, 40, 84) print(f&quot;BMU 行, col 形状; row = {row.shape} &amp; col = {col.shape}&quot;) # BMU row, col 形状; row = torch.Size([512]) &amp; col = torch.Size([512]) 为了清楚起见，对于批次“z[0]”中的第一个输入样本，获胜单位是“som[row” [0]，col[0]]”- z[0].shape，som[row[0]，col[0]].shape # (torch.Size([84]), torch.Size([84])) torch.norm((z[0]) - som[row[0], col[0]])) 是 z[0] 与除 row[0] 和 col[0] 之外的所有其他 som 单位之间的最小 L2 距离。 &lt; p&gt;# 定义初始邻域半径和学习率- neighb_rad = torch.tensor(2.0)  lr = 0.5 # 更新第一个输入“z[0]”的权重及其对应的 BMU“som[row[0], col[0]]”- for r in range(som.shape[0]): for c in range(som.shape[1]): neigh_dist = torch.exp(-torch.norm(输入 = (som[r, c] - som[row[0], col[0]])) / (2.0 * torch.pow(neighb_rad, 2))) &lt; code&gt;som[r, c] = som[r, c] + (lr * neigh_dist * (z[0] - som[r, c])) 如何实现代码：  更新每个 BMU 周围所有单元的权重，无需 2 个 for 循环（并且） 对所有输入“z”执行此操作（这里，z有512个样本）    由   提交/u/grid_world  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bevbd9/selforganizing_map_neighborhood_implementation_in/</guid>
      <pubDate>Thu, 14 Mar 2024 20:35:43 GMT</pubDate>
    </item>
    <item>
      <title>使用CNN进行黑白矩阵识别</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bekofr/black_and_white_matrix_identification_using_cnn/</link>
      <description><![CDATA[        由   提交/u/LightFounder  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bekofr/black_and_white_matrix_identification_using_cnn/</guid>
      <pubDate>Thu, 14 Mar 2024 13:04:02 GMT</pubDate>
    </item>
    <item>
      <title>您想知道您的神经网络需要多长时间才能得到充分训练？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1behfja/do_you_want_to_know_the_time_it_will_take_your/</link>
      <description><![CDATA[查看投票 &lt; /div&gt;  由   提交 /u/Red_Pudding_pie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1behfja/do_you_want_to_know_the_time_it_will_take_your/</guid>
      <pubDate>Thu, 14 Mar 2024 09:46:44 GMT</pubDate>
    </item>
    <item>
      <title>1 位法学硕士时代 - 论文解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1beel34/the_era_of_1bit_llms_paper_explained/</link>
      <description><![CDATA[       由   提交/u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1beel34/the_era_of_1bit_llms_paper_explained/</guid>
      <pubDate>Thu, 14 Mar 2024 06:20:55 GMT</pubDate>
    </item>
    </channel>
</rss>