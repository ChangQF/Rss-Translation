<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Wed, 31 Jan 2024 15:13:06 GMT</lastBuildDate>
    <item>
      <title>顶级脑机接口（史努比狗狗风格）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1afijvw/top_brain_computer_interface_in_the_style_of/</link>
      <description><![CDATA[       由   提交/u/SnooWoofers7789   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1afijvw/top_brain_computer_interface_in_the_style_of/</guid>
      <pubDate>Wed, 31 Jan 2024 14:03:15 GMT</pubDate>
    </item>
    <item>
      <title>需要资源来练习制作神经网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1afiie7/need_resources_to_practice_making_neural_networks/</link>
      <description><![CDATA[有谁知道有什么好的资源可以提供练习编写神经网络的练习，例如 nn 的目标、相应的数据集和解决方案，按难度排序？就像神经网络的 Leetcode 一样。如果存在与此完全相同或类似的东西，那就太好了   由   提交 /u/AryAimshot   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1afiie7/need_resources_to_practice_making_neural_networks/</guid>
      <pubDate>Wed, 31 Jan 2024 14:01:21 GMT</pubDate>
    </item>
    <item>
      <title>为复杂的游戏制作简单的神经网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1afhj21/making_a_simple_neural_network_for_a_complex_game/</link>
      <description><![CDATA[嘿， 因此，作为一个家庭项目，我决定尝试制作一个玩游戏的神经网络（一个特定的游戏）游戏）。 我已经对游戏进行了修改，为我提供了大量写入磁盘的数据（玩家状态、3 个最近的敌人、5 个最近的交互对象），并制作了一个可以操纵这些数据的解析器 问题在于学习 - 我无法运行多个游戏实例，因此经典的强化学习将其解决，所以我尝试制作一个模仿模型。我玩游戏很不错，所以作为 POC，我想制作一个可以和我玩得一样好的模型。 我目前的做法是记录我玩每一帧的游戏状态，并且将其输入到模型中，在给定当前状态和“期望”的情况下，该模型状态，我的下一帧的输入是什么。 在摆弄不同的数据、参数和模型（尝试过 Linear、Transformer 和 LSTM（我认为））之后，我到达了一个我不知道的地步不知道该怎么做，模型只是向右移动（或向数据集中最常见的输入方向移动） 这里有人可以提供任何建议/帮助吗？ 谢谢！   由   提交 /u/MidnightCardFight   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1afhj21/making_a_simple_neural_network_for_a_complex_game/</guid>
      <pubDate>Wed, 31 Jan 2024 13:12:06 GMT</pubDate>
    </item>
    <item>
      <title>使用 GFPGAN 增强您的图像：低分辨率照片恢复教程 📸</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1aeulvc/enhance_your_images_with_gfpgan_lowresolution/</link>
      <description><![CDATA[      https://preview.redd.it/am7izsau8mfc1.png?width=1280&amp;format=png&amp;auto=webp&amp; ;s=8ef7be7f9889d6a792a118162714d0fe4a103ead 🚀 在我们最新的视频教程中，我们将介绍使用 GFPGAN 进行照片修复！非常酷的 Python 库。 教程分为四个部分： 🖼️ 第 1 部分：设置 Conda 环境以进行无缝开发并安装必要的 Python 库。 &lt; p&gt;🧠 第 2 部分：克隆包含代码和资源的 GitHub 存储库。 🚀 第 3 部分：将模型应用到您自己的图像上 您可以在此处找到说明：https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/ GFPGAN 视频链接：https://youtu.be/nPnQm7HFWJs 享受 Eran ​ #python #GFPGAN #increaseimageresolution #Enhancephoto   由   提交 /u/Feitgemel   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1aeulvc/enhance_your_images_with_gfpgan_lowresolution/</guid>
      <pubDate>Tue, 30 Jan 2024 17:55:01 GMT</pubDate>
    </item>
    <item>
      <title>数据集</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1aej89s/datasets/</link>
      <description><![CDATA[除了kaggle，您还从哪里获取数据集？    ;由   提交/u/joab_kc   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1aej89s/datasets/</guid>
      <pubDate>Tue, 30 Jan 2024 07:57:44 GMT</pubDate>
    </item>
    <item>
      <title>C/C++ 代码中的反向传播算法错误（梯度下降对我不起作用）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1adt56m/backpropagation_algorithm_error_in_cc_code/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1adt56m/backpropagation_algorithm_error_in_cc_code/</guid>
      <pubDate>Mon, 29 Jan 2024 11:27:24 GMT</pubDate>
    </item>
    <item>
      <title>中途</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1adew3t/midjourney/</link>
      <description><![CDATA[谁已经在使用 Midjourney？我需要什么工具才能开始，请帮助我   由   提交/u/Simple-Bookkeeper947  /u/Simple-Bookkeeper947 reddit.com/r/neuralnetworks/comments/1adew3t/midjourney/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1adew3t/midjourney/</guid>
      <pubDate>Sun, 28 Jan 2024 22:20:14 GMT</pubDate>
    </item>
    <item>
      <title>模型选择和对初始随机种子的敏感性。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1acsul8/model_selection_and_sensitivity_to_initial_random/</link>
      <description><![CDATA[各位聪明人， 我已经自学了机器学习几年，并且正在尝试神经网络。 专注于回归问题，我有一些关于神经网络选择的基本问题。  我正在尝试预测具有高度随机性的硬回归问题。通过算法、激活函数、插补和缩放所有修复，我注意到回归结果和准确性可能会根据不同的初始随机猜测而变化，即一切都相同，但每次运行都会产生不同的准确性。 经过几次之后运行，有一个特定的运行，我对性能感到满意，所以我节省了重量和偏差，并转向生产。  我感觉不对的是，这个特定的运行是由于特定的随机启动而起作用的。在我看来，这很容易出现过度拟合。  抱歉，非常基本，我可能会错过一些东西或完全错误，如果愚蠢的话请道歉。 干杯 尼尔森    由   提交/u/Nelson_Chow  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1acsul8/model_selection_and_sensitivity_to_initial_random/</guid>
      <pubDate>Sun, 28 Jan 2024 03:16:32 GMT</pubDate>
    </item>
    <item>
      <title>计算可比较的嵌入：两座塔、连体网络和三元组损失</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1abg6q1/compute_comparable_embeddings_two_towers_siamese/</link>
      <description><![CDATA[       由   提交/u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1abg6q1/compute_comparable_embeddings_two_towers_siamese/</guid>
      <pubDate>Fri, 26 Jan 2024 11:26:28 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 揭秘：清晰指南</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19fip2c/yolo_unraveled_a_clear_guide/</link>
      <description><![CDATA[   ​ 此处 &lt; !-- SC_ON --&gt;  由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/neuralnetworks/comments/19fip2c/yolo_unraveled_a_clear_guide/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19fip2c/yolo_unraveled_a_clear_guide/</guid>
      <pubDate>Thu, 25 Jan 2024 20:16:48 GMT</pubDate>
    </item>
    <item>
      <title>我这几天发表的每一篇博文在谷歌上的排名都在第五位以内。这一切都归功于 Junia.ai 的博客文章工作流程。以下是自动链接的预览，它将进一步提高您网站的搜索引擎优化：</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19exhkp/every_one_of_the_blog_post_i_published_in_the/</link>
      <description><![CDATA[       由   提交/u/Lunaopty  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19exhkp/every_one_of_the_blog_post_i_published_in_the/</guid>
      <pubDate>Thu, 25 Jan 2024 01:34:22 GMT</pubDate>
    </item>
    <item>
      <title>与我的人工智能伙伴 Synthia 就神经网络的复杂性进行了一场精彩的对话。 🤖✨ 查看我最新文章中的见解和问答环节。让我们一起来揭开AI的奥秘吧！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19dgjxy/engaging_in_a_fascinating_conversation_with/</link>
      <description><![CDATA[   本文采用了一种独特的方法，通过想象的神经网络进行问答环节网络。我们不会通过传统的视角深入研究错综复杂的技术，而是将神经网络拟人化，邀请它阐明其内部工作原理，揭开其决策过程的神秘面纱，并揭示其存在的细微差别。通过这种富有想象力的对话，我们的目标是以一种令人耳目一新的独特方式揭开神经网络的秘密，为读者提供一个深入而平易近人的视角来了解人工智能的迷人世界。 &lt;!-- SC_ON - -&gt;  由   提交/u/ardesai1907  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19dgjxy/engaging_in_a_fascinating_conversation_with/</guid>
      <pubDate>Tue, 23 Jan 2024 05:00:15 GMT</pubDate>
    </item>
    <item>
      <title>突然validation_loss下降到零</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19deubj/suddenly_validation_loss_drops_to_zero/</link>
      <description><![CDATA[      有人见过这样的val_dice曲线吗？ 真的不合理，max_epoch=100learning_reate=8e-4，不涉及lr_scheduler。 除了验证，训练过程也是这样，train_loss突然激增。 大家有什么想法或者建议吗？谢谢大家。 https://preview.redd.it/4nl5qakly3ec1.png?width=576&amp;format=png&amp;auto=webp&amp;s=43307a87e91072394dcc369b1dbe2f2308fdad7c ​ &lt; p&gt;https://preview.redd.it/7wbvlnu3z3ec1。 png?width=567&amp;format=png&amp;auto=webp&amp;s=b93e45020116da1dd26140559796e7abeda79346   由   提交/u/No-Supermarket-2567   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19deubj/suddenly_validation_loss_drops_to_zero/</guid>
      <pubDate>Tue, 23 Jan 2024 03:27:05 GMT</pubDate>
    </item>
    <item>
      <title>采访麻省理工学院林肯实验室的 Zack Serlin：正式方法......</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</guid>
      <pubDate>Sat, 20 Jan 2024 20:00:19 GMT</pubDate>
    </item>
    <item>
      <title>人工神经网络中的类脑学习：综述</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.11252 摘要：  人工神经网络（ANN）已成为以下领域的重要工具：机器学习，在图像和语音生成、游戏和机器人等多个领域取得了显着的成功。然而，人工神经网络的运行机制与生物大脑的运行机制之间存在根本差异，特别是在学习过程方面。本文对当前人工神经网络中的类脑学习表示进行了全面回顾。我们研究了更多生物学上合理的机制的整合，例如突触可塑性，以增强这些网络的能力。此外，我们深入研究了这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究找到了有希望的途径，这可以让我们更接近理解智能的本质。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</guid>
      <pubDate>Fri, 19 Jan 2024 16:03:08 GMT</pubDate>
    </item>
    </channel>
</rss>