<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络，深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的subreddit。</description>
    <lastBuildDate>Sun, 06 Apr 2025 06:25:40 GMT</lastBuildDate>
    <item>
      <title>交互式AI演示 - 可视化图像内生长的合成大脑（独立研究）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jsju6y/interactive_ai_demo_visualizing_a_synthetic_brain/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我是一名独立的AI研究人员，从事两个独立但相关的实验项目。我想分享一个实时的WebGL演示，以获得反馈和好奇心。这不是商业化，不是为了游戏，而只是纯认知AI实验。  项目：神经像素AI系统 这个WebGL项目编码PNG图像中的人造大脑。目的是随着神经元从像素信息增长而来的结构和活动的出现。 每个像素编码突触或符号数据。 神经元在视觉上自我组织会随着时间的推移而自动组织。   整个系统都是确定性的，但由pseudo-eviludo-evi            href =“ https://www.dfgamesstudio.com/neural-pixel-ai-system/”&gt; https://www.dfgamesstudio.com/neural-pixel-ai-system/ 符号/认知AI架构旨在通过梦想合成，记忆衰减，情绪调节和通过一个称为“ADNσ”的系统的符号演变来模拟模块化意识。我知道这是非常规的，但我相信具有视觉逻辑的混合符号/神经系统值得探索。 谢谢！ - &gt;＆＃32;提交由＆＃32; /u/u/conanfredleseul     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jsju6y/interactive_ai_demo_visualizing_a_synthetic_brain/</guid>
      <pubDate>Sun, 06 Apr 2025 02:19:38 GMT</pubDate>
    </item>
    <item>
      <title>如何训练多视图注意模型以结合NGram和Biobert嵌入</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1js9auj/how_to_train_a_multiview_attention_model_to/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我希望您做得很好，我正在努力构建一个多视图模型，该模型使用注意机制结合了两种类型的功能：Ngram嵌入式和Biobert Embeddings    目标是通过使用这些不同的视图来创建富裕的代表，并使用这些不同的视图来创建富裕的表示。但是，我不确定如何构建训练过程，以便注意力机制学会从每个视图中有意义地对齐功能。我的意思是，我不能直接直接在标签上训练它，因为这就像在分类任务上训练常规的MLP有没有人从事类似方向的工作？ 我还没有尝试过任何具体的事情，因为我仍然对如何接近培训这种基于注意力的多视觉模型感到困惑。我不确定目标应该是什么以及如何使其学习有意义的注意力。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/connect-courage6458     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1js9auj/how_to_train_a_multiview_attention_model_to/</guid>
      <pubDate>Sat, 05 Apr 2025 17:54:58 GMT</pubDate>
    </item>
    <item>
      <title>有人可以回答吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1js5r4n/anyone_can_answer_that/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   如果有3个输入，我有3个隐藏的层，例如，一个神经元将取得所有3个输入，但增加了2个输入的权重，而不是第三个输入，而第二个神经元则关注第二和第三个输入的重量和第二个输入的重量和重新量的重量，也是第二个NEROR的重量。这是正确的吗？或神经网络中的神经元。例如，如果您有3个输入（x1，x2，x3），则一个感知器可能会集中在第一个和第三个输入（x1和x3）上（x1和x3），并给予它们高权重（例如，0.9和0.8），而将第二个输入（x2）给出了非常小的重量或零重量（例如，0.1或0）。同时，另一个感知者可能会专注于第二和第三个输入（x2和x3），从而使它们具有很高的权重（例如，0.7和0.9），并减少了第一个输入（x1）的重量（x1），以接近零。提交由＆＃32; /u/u/u/zestyclose-produce17     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1js5r4n/anyone_can_answer_that/</guid>
      <pubDate>Sat, 05 Apr 2025 15:17:31 GMT</pubDate>
    </item>
    <item>
      <title>有人真的知道NLP是如何工作的吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1js1igr/does_anyone_actually_know_how_nlp_works/</link>
      <description><![CDATA[    src =“ https://preview.itd.it/89rqqg5n60te1.png？ NLP有效????? /&gt;    &lt;！ -  sc_off-&gt;   01   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/poopo-hitshit       [注释] ] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1js1igr/does_anyone_actually_know_how_nlp_works/</guid>
      <pubDate>Sat, 05 Apr 2025 11:38:58 GMT</pubDate>
    </item>
    <item>
      <title>用于增强扩散模型控制的频率指导缩放</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jrxv14/frequencydecomposed_guidance_scaling_for_enhanced/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   fresca是一种开创性的方法，可以通过作者所说的“缩放空间”来理解和操纵扩散模型。通过分析扩散模型如何自然地在deno的过程中在各个时间步处缩放不同的特征，他们发现了一种固有的结构，可以在不进行其他培训的情况下进行精确的图像编辑。   的关键技术贡献包括：   发现，在整个范围中 实现了与任何预验证的扩散模型一起使用的实施，而无需进行微调或其他网络 在多个图像操作任务中进行最新的结果结果，包括颜色调整，样式传输和本地编辑，包括   extripers offiel offief offief interfusion   extife extirals extirals extrips extress（类似于范围），这些方法是不同的，这些方法是差异的，这些绘制了范围不同的图像（范围差异）（ - something that&#39;s been present but untapped in these models until now. The results are impressive across various manipulation tasks: * Color manipulation: Changing color schemes while preserving textures and object identities * Style transfer: Applying styles to specific objects without affecting others * Local editing: Making precise changes to targeted areas while keeping the rest of the image intact * 一致的优势：在保持有针对性的更改的同时，在保存图像身份方面胜过现有技术   技术实施涉及计算每个时间段的模型输出与输入之间的比率与识别缩放因素，然后将目标调整应用于这些因素以修改特定属性。弗雷斯卡（Fresca）没有将它们视为黑匣子，而是揭示了它们具有内部结构，可以反映人类如何层次处理视觉信息。这可能会导致图像生成和编辑工具中更直观，更精确的控制。 我认为最令人兴奋的方面是，这种功能始终存在于扩散模型中，但只需要正确理解和利用。 It suggests there may be other untapped capabilities in these models we haven&#39;t yet discovered. The limitations around model dependency and the somewhat empirical process for identifying optimal timesteps for specific manipulations will need to be addressed in future work. TLDR: FreSca discovers and manipulates an inherent &quot;scaling space&quot;在扩散模型中，在不同的时间段上处理不同的图像特征，在没有其他培训的情况下进行精确的图像编辑。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jrxv14/frequencydecomposed_guidance_scaling_for_enhanced/</guid>
      <pubDate>Sat, 05 Apr 2025 07:14:55 GMT</pubDate>
    </item>
    <item>
      <title>努力在医学成像中选择正确的CNN XAI方法</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jqyfov/struggling_to_pick_the_right_xai_method_for_cnn/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   嘿，大家！&lt; /strong&gt; 我正在撰写论文，以使用可解释的AI（XAI）与CNN进行肺炎检测。 The goal is to make model predictions more transparent and trustworthy—especially for clinicians—by showing why a chest X-ray is classified as pneumonia or not. I’m currently exploring different XAI methods like Grad-CAM, LIME, and SHAP, but I’m struggling to decide which one best explains my model’s decisions. Would love to hear your thoughts or experiences with XAI in医学成像。任何建议或见解都将非常有帮助！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/divedent-Ad914     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jqyfov/struggling_to_pick_the_right_xai_method_for_cnn/</guid>
      <pubDate>Fri, 04 Apr 2025 00:07:10 GMT</pubDate>
    </item>
    <item>
      <title>ROR BENCH：评估语言模型对朗诵的敏感性与基本问题的推理</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jqfevt/rorbench_evaluating_language_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这项新研究引入了 ror bench （对推理基准的朗诵），旨在测试语言模型是通过问题还是简单地背诵记忆的模式。研究人员创建了1,500个小学数学问题，具有测试相同概念但可以简单的模式匹配的变化。 关键发现： * GPT-4，Claude 3 Opus和Gemini 1.5 Pro在标准问题上表现出明显更好的性能，而在标准问题上，与在相同的概念上进行了相同的概念相比，与78.5％的质量相比，均为7.5％compliatiation * 1％，但仅在78.5％上均可遵循7.5％的质量。不同的数学操作和模型类型 *经过思考的链条促使性能提高了，但并没有消除与“反事实”变化最大的推理差距 *模型。 - 看起来类似于培训示例但需要不同推理的问题 我认为这项研究突出了当前LLM的基本限制，在典型评估中很容易错过。解决标准问题和变化之间的差距表明这些模型不是发展真正的数学理解，而是利用模式识别。这可以解释为什么在实际推理任务中部署LLM经常会导致意外的失败 - 他们缺乏人类发展的灵活推理能力。 我认为这对我们如何处理AI安全和能力研究有影响。如果即使是小学的数学问题也揭示了推理中的这种脆弱性，我们应该对声称单独扩展会产生强大的推理能力非常谨慎。更多地关注专门设计用于建立真正理解的新型体系结构或训练方法似乎是必要的。  tldr：领先的LLM（GPT-4，Claude，gemini）在标准数学问题上表现良好，但在测试相同概念的变化方面，表明他们依靠记忆，而不是真实的推理。 href =“ https://aimodels.fyi/papers/arxiv/recitation-erecitation-over-reasoning-how-cutting-denguage”&gt;完整的摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jqfevt/rorbench_evaluating_language_models/</guid>
      <pubDate>Thu, 03 Apr 2025 11:00:17 GMT</pubDate>
    </item>
    <item>
      <title>通过注意力图训练4D场景重建</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jpmdcu/trainingfree_4d_scene_reconstruction_via/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我最近读了一篇论文，该论文介绍了一种从无需任何培训的情况下从视频中提取3D运动的方法。该方法称为EASI3R，建立在DUST3R（一种从图像对创建3D场景结构的模型），并添加后处理以将相机运动与对象运动分开。 关键见解使用几何约束，而不是从数据中学习。这是通过分析框架之间的点对应关系和使用RANSAC来确定属于静态背景与移动对象的点的对应关系的。    主要技术贡献：   使用dust3r使用dust3r来提取3D点对应关系在框架之间提取框架    li&gt; li li li li fire n olim li fimit  轨迹在多个框架上跟踪时间一致性的点 簇按运动模式通过运动模式来处理多个移动对象 需要在运动数据集中进行零培训或进行微调。 benchmarks Works on complex real-world scenes with multiple independent objects Functions with as few as two frames but improves with longer sequences Shows robustness to challenges like occlusions and lighting changes Maintains DUSt3R&#39;s capabilities while adding motion analysis  I think this approach could be particularly valuable for robotics and在没有广泛培训数据的情况下，需要在新环境中了解运动的自主系统。区分与摄像机运动的能力是导航和互动的基础。 我也认为这是对“大量数据上的“火车”的有趣反击”。趋势，表明几何理解在计算机视觉中仍然具有重要位置。它表明将几何约束与学习的特征结合在一起的混合方法可能是一个富有成果的方向。    tldr： easi3r提取物通过在dust3r上构建和使用几何约束来从视频中进行3D运动，并使用几何约束来将摄像机运动与对象运动分开 - 没有任何训练。 href =“ https://aimodels.fyi/papers/arxiv/arxiv/easi3r-estimating-disentangled-motion-motion-from-dust3r-without”&gt;完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jpmdcu/trainingfree_4d_scene_reconstruction_via/</guid>
      <pubDate>Wed, 02 Apr 2025 11:35:14 GMT</pubDate>
    </item>
    <item>
      <title>机械学检查神经网络 - 我的第一个视频，我会喜欢反馈！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jpj6j2/mechanistically_examining_neural_networks_my/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/aneesh6214      [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jpj6j2/mechanistically_examining_neural_networks_my/</guid>
      <pubDate>Wed, 02 Apr 2025 07:47:55 GMT</pubDate>
    </item>
    <item>
      <title>解开梯度下降：窥视AI的学习方式（以有趣的类比！）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jpb56j/unpacking_gradient_descent_a_peek_into_how_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好！我最近一直在深入研究AI，并想分享一种很酷的方式来思考梯度下降，这是机器学习的无名英雄之一。想象一下，您是山上蒙住眼睛的寻宝者，试图找到最低的山谷。你唯一的线索？脚下的斜率。您下坡采取很小的台阶，感觉到朝下。简而言之，这是梯度下降 -  ai的方式通过一点一点地调整参数来“感觉”更好地预测。 我从我一直在研究的一个项目中汲取了类比（AI概念的小指南），并陷入了我的困扰。这是它如何使用数学的快速片段：您从a = 1，b = 1和学习率alpha = 0.1之类的参数开始。然后，您计算出损失（例如，预测表中的1.591），并根据梯度进行调整。太大的一步，你超越了；太小了，您永远卡住了！ 对于任何好奇的人来说，我还忽略了这与神经网络如何联系在一起，就像一个感知者如何学习AN和大门，或者像Adam这样的优化者如何平滑旅程。您最喜欢解释梯度下降的方法是什么？或者，一旦找到正确的类比，还是其他任何AI概念，它们会为您点击吗？很想听听您的想法！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/msahmad     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jpb56j/unpacking_gradient_descent_a_peek_into_how_ai/</guid>
      <pubDate>Wed, 02 Apr 2025 00:12:50 GMT</pubDate>
    </item>
    <item>
      <title>这就是为什么大型语言模型可以理解世界的原因</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1joe7na/this_is_why_large_language_models_can_understand/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/keghn       [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1joe7na/this_is_why_large_language_models_can_understand/</guid>
      <pubDate>Mon, 31 Mar 2025 21:07:46 GMT</pubDate>
    </item>
    <item>
      <title>探索沉浸式神经接口</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jnho3z/exploring_immersive_neural_interfacing/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我们目前正在研究一个旨在开发一个完全沉浸式技术平台的项目，该平台与人类的思想无缝集成。该概念涉及使用神经界面来创造引人入胜的体验 - 从治疗应用和认知培训到游戏甚至军事模拟。   核心思想是开发一个从用户学习，适应用户，动态响应的系统，提供个性化和变革的体验。想象一个环境，可以将记忆，思想和情感可视化和与人类意识之间的差距进行可视化和互动。 任何想法都受到欢迎。开放对话。 编辑******时，在尝试解释这样的事情时，很容易听起来有些“ butiness-y”。我绝对不会在这里出售任何东西😅只是想进行真实的对话并收集那些技术人员的意见。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/okincident3886     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jnho3z/exploring_immersive_neural_interfacing/</guid>
      <pubDate>Sun, 30 Mar 2025 17:31:12 GMT</pubDate>
    </item>
    <item>
      <title>层次运动扩散模型启用具有同步头和身体运动的实时风格化肖像视频生成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jn6yh6/hierarchical_motion_diffusion_model_enables/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   chatanyone引入了层次运动扩散模型，该模型可以从单个图像和音频输入中创建实时说话的肖像视频。 The model decomposes facial motion into three levels (global, mid-level, and local) to capture the complex relationships in human facial movement during speech. Key technical points: * Real-time performance: Generates videos at 25 FPS on a single GPU, significantly faster than previous methods * Hierarchical motion representation: Separates facial movements into global (head position), mid-level （表达式）和局部（唇部运动）用于更自然的动画 * 级联扩散模型 ：每个运动条件级别都会影响下一个，确保面部运动 * 风格控制的渲染 ：保留对参考图像的认同和独特的特征。嘴唇同步准确性和整体质量 我认为这种方法通过建模人类运动的实际运作方式来解决问题的基本问题 - 我们的头部不是孤立地移动，而是在协调的运动层次结构中移动。这种层次结构方法使动画看起来更自然，更少。比以前的方法。 我认为实时功能对于实际应用特别重要。在单个GPU上的25 fps时，该技术可以集成到视频会议，内容创建工具或虚拟助手的情况下，而无需专门的硬件。从单个图像中生成个性化的会说话的头视频的能力为定制的教育内容，可访问性应用程序和更身临其境的数字互动打开了可能性。 我认为我们也应该考虑道德含义。随着肖像动画变得更加现实和易于访问，我们需要更好地保障来防止可能滥用创建误导性内容。本文提到了道德考虑，但没有提出特定的检测方法或身份验证机制。     tldr ：chatanyone通过使用层次的面部运动来实时从单个图像中实时产生真实的说话头视频，从而实现了更好的视觉质量和唇彩，并保留了与以前的方法相同的完整的摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jn6yh6/hierarchical_motion_diffusion_model_enables/</guid>
      <pubDate>Sun, 30 Mar 2025 07:04:31 GMT</pubDate>
    </item>
    <item>
      <title>任何人都可以帮助我未遇到反向传播概念。您的解释，建议对我有帮助</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jmxhp3/anyone_help_me_undestanding_backpropagation/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/nonympus746     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jmxhp3/anyone_help_me_undestanding_backpropagation/</guid>
      <pubDate>Sat, 29 Mar 2025 22:04:04 GMT</pubDate>
    </item>
    <item>
      <title>视频生成中的身体认知：从视觉现实主义到身体一致性</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jmgzpt/physical_cognition_in_video_generation_from/</link>
      <description><![CDATA[This paper presents a systematic survey of how physics cognition has evolved in video generation models from 2017 to early 2024. The researchers introduce VideoPhysCOG, a comprehensive benchmark for evaluating different levels of physical understanding in these models, and track the development through three distinct stages. Key technical contributions: * Taxonomy of physics cognition等级：作者将身体理解分为四个渐进水平 - 从基本运动感知（L1）到抽象的物理知识（L4） *  videophyscog基准：一个结构化评估框架，专门设计用于测试所有四个级别的物理认知 * 开发阶段分类：differ&gt; niffer&gt; distrifife ：distrifie  202-202-202-202-202-202-202，202，202，202，202和Advanced 2023-inwards）具有不同的建筑方法和能力 主要发现： *早期模型（2017-2021）使用gan，vaes和自动回归方法可以处理基本运动，但与相干的物理学 *相干 *过渡期（2021-2023）通过散布模型和范围的模型进行了艰难的模型 *在复杂的推理中理解但仍然失败 *当前模型在L1（运动感知）和L2（基本物理学）的一部分（基本物理）的一部分中表现出色，但与L3（复杂的相互作用）和L4（摘要物理学） *体系结构在直接空间建模到实体模型相互启动的范围          人类水平的身体推理。虽然视觉保真度已大大改善，但真正的身体理解仍然有限。 Videophyscog基准提供了一种结构化的方法来评估和比较不仅视觉质量的模型，这可以帮助集中未来的研究工作。 我认为分类法和发展阶段框架将对现场的新进展特别有用。复杂的物理互动中确定的限制指向特定领域，其中合并了明确的物理模型或专业体系结构可能会产生改进。  tldr：这项调查跟踪视频生成模型如何在对物理学的理解中发展，引入视频杂货基准评估，并在复杂的物理理由中识别未来的研究，应将未来的研究限制为      。 href =“ https://aimodels.fyi/papers/arxiv/exploring-evolution-physics-physics-cognition-video-generation-survey”&gt;完整的摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jmgzpt/physical_cognition_in_video_generation_from/</guid>
      <pubDate>Sat, 29 Mar 2025 07:24:15 GMT</pubDate>
    </item>
    </channel>
</rss>