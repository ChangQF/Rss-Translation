<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络，深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的subreddit。</description>
    <lastBuildDate>Sat, 22 Feb 2025 06:24:03 GMT</lastBuildDate>
    <item>
      <title>通过对比度学习从时间序列数据中学习内在的神经表示</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iunfqx/learning_intrinsic_neural_representations_from/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员提出了一种对比度学习方法，将神经活动动态映射到几何表示，提取他们所说的“柏拉图式”人群级神经记录的形状。该方法将时间嵌入与几何约束结合在一起，以揭示基本的组织原理。 关键技术方面： - 在神经时间序列数据上使用对比度学习来学习低维的嵌入 - 应用拓扑结构来强制执行几何结构 - 验证 - 验证 - 验证 - 验证在来自不同物种的多个神经记录数据集中 - 显示了基本几何模式的一致出现（球体，托里等） - 证明了不同神经种群和大脑区域之间的鲁棒性 结果证明了： - 神经种群自然组织成几何歧管 - 这些几何模式在不同的时间范围内保留了这些几何模式 - 在任务和自发活动中始终出现表示的表示 - 方法工作在数十个到数千个神经元的种群上 - 几何结构与行为和认知变量相关 我认为这种方法可以为了解神经种群的编码和过程信息提供新的框架。几何视角可能有助于弥合单神经元和人口水平分析之间的差距。 我认为，如果我们可以可靠地将神经活动映射到神经假体和脑部计算机界面中，则最有趣的潜在影响是一致的几何表示，它可以使解码神经信号更加稳健。  tldr：新方法使用对比度学习如何显示神经种群将信息组织成几何形状，为神经计算提供潜在的通用原理。  完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iunfqx/learning_intrinsic_neural_representations_from/</guid>
      <pubDate>Fri, 21 Feb 2025 09:59:51 GMT</pubDate>
    </item>
    <item>
      <title>接近神经网络和机器学习理论的在线课程。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iu02so/online_courses_that_approach_neural_networks_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是一名电气工程师，我想开始学习有关A.I.基础知识及其在嵌入式系统上的实现。但是，有关这些主题的大多数在线课程似乎都提供了更多的“ pratical”。通过向学生扔Python和Matlab包裹，而无需教授神经网络的实际工作方式。如果有人能够向我推荐一门课程（免费或付费），我将不胜感激，以了解神经网络和机器学习的基础，包括神经元的模型和Network的培训。   &lt;！ -  sc_on-- &gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/neuralnetworks/comments/comments/1IU02SO/Online_courses_that_that_that_ apphact_neural_networks_and/”&gt; [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iu02so/online_courses_that_approach_neural_networks_and/</guid>
      <pubDate>Thu, 20 Feb 2025 14:44:29 GMT</pubDate>
    </item>
    <item>
      <title>基于内存的视觉基础模型，用于3D膝盖MRI分割的混合动力降温</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1itt241/memorybased_visual_foundation_model_with_hybrid/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文引入了一种基于内存的视觉模型，称为SAMRI-2用于3D医疗图像分割，特别专注于MRI扫描中的膝盖软骨和半月板。关键创新是将记忆机制与混合洗牌策略结合在一起，以更好地处理3D空间关系，同时保持计算效率。 主要技术点： - 使用带有变压器的基于变压器的体系结构来处理3D卷 - 实施一种新颖的“混合洗牌策略”在有助于保持空间一致性的训练过程中 - 每次扫描仅需要3次点击作为提示 - 接受了270次患者扫描培训，对57例外部病例进行了测试 - 与3D -VNET和其他Transformer Baselines进行了比较 结果： -  &lt;强&gt;骰子得分比以前的方法提高了5％ - 胫骨软骨分割精度提高了12％ -  厚度测量显示3倍更好的精度 - 在不同的MRI机/协议上保持性能 - 每次扫描的处理时间约为30秒 我认为这种方法对于临床部署可能特别有价值，因为它与最小用户输入之间的自动化平衡。基于内存的设计似乎比以前的方法更有效地处理医学扫描的3D性质。 我认为混合洗牌策略是一种有趣的技术贡献，可以适用于其他3D视觉任务。只需单击3个点击即可维持准确性的能力。  tldr：基于膝关节MRI分析的新型内存模型，将强度的精度与最小的用户输入相结合（3次点击）。使用混合洗牌策略有效地处理3D数据。 完整摘要在这里。 Paper 在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1itt241/memorybased_visual_foundation_model_with_hybrid/</guid>
      <pubDate>Thu, 20 Feb 2025 07:34:20 GMT</pubDate>
    </item>
    <item>
      <title>引入CNN学习工具</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1it64cd/introducing_cnn_learning_tool/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  使用我的新互动应用程序探索卷积神经网络（CNN）的内部工作。观看每一层如何处理您的草图，对行动中的深度学习有更清晰的了解。 （这也很有趣） 链接： Applepear.streamlit.app    &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/foreltert2597     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1it64cd/introducing_cnn_learning_tool/</guid>
      <pubDate>Wed, 19 Feb 2025 14:00:06 GMT</pubDate>
    </item>
    <item>
      <title>硬件优化的本地稀疏注意力，以进行有效的长篇下说建模</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1it2oyq/hardwareoptimized_native_sparse_attention_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键贡献是一种新的稀疏注意方法，它与可训练的端到端训练相一致。本机稀疏注意（NSA）不使用复杂的预处理或动态稀疏模式，而是使用与GPU内存访问模式相匹配的块 -  sparse模式。 主要技术点： - 引入固定但可学习的稀疏模式，这些模式与硬件保持一致 - 在正常训练期间学习模式，而无需预处理 - 使用针对GPU内存访问优化的块 -  sparse结构 - 与密集的关注相比，达到2-3倍的速度 - 在使用50-75％的计算 之间保持准确性 跨不同设置的结果： - 语言建模：匹配密集的注意力困惑 - 机器翻译：可比的BLEU分数 - 图像分类：与密集的注意力相似的准确性 - 量表很好随着序列长度的增加 - 在不同模型大小 之间有效地工作，我认为这种方法可以使变压器模型在资源约束环境中更实用。硬件对齐方式意味着理论效率的提高实际上转化为现实世界的性能改进，与许多现有的稀疏注意方法不同。 我认为块 -  sparse模式在某些情况下可能限制了块，代表了一种良好的交易 - 灵活性和效率之间的依据。在训练过程中学习这些模式的能力尤其重要，因为它允许模型使稀疏性适应任务。  tldr：新的稀疏注意方法，与硬件约束相一致并在训练，训练，学习稀疏模式时，达到2-3倍的速度而没有准确的损失。  完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1it2oyq/hardwareoptimized_native_sparse_attention_for/</guid>
      <pubDate>Wed, 19 Feb 2025 10:47:35 GMT</pubDate>
    </item>
    <item>
      <title>从多类变为多标签训练</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1isbwra/going_from_multiclass_to_multilabel_training/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我有一个神经网络，其中有1个输入层2隐藏层和1个输出层。现在，我将其用作多类分类器，这意味着输出是0到15之间的值（总计16个可能的和相互排斥的类）。但是，作为下一步，我想培训一个具有7个类的多标签分类器，并且每个班级都有多达6个子类，因此我希望每个类都有标签。  与多类培训相比，这有何不同？我想主要区别在于输入（例如标签）和输出层？到目前为止，我一直在输出层中使用SoftMax作为激活函数。  感谢任何见解！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/rda92     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1isbwra/going_from_multiclass_to_multilabel_training/</guid>
      <pubDate>Tue, 18 Feb 2025 12:48:25 GMT</pubDate>
    </item>
    <item>
      <title>具有高精度肌肉和脂肪指标的人体组成分析的自动多组织CT分割模型</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1isbu5z/automated_multitissue_ct_segmentation_model_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文提出了一种自动深度学习系统，用于分割和量化CT扫描的肌肉和脂肪组织。关键的技术创新是将修改后的U -NET体系结构与自定义损失功能中编码的解剖约束相结合。 关键技术点： - 修改了U -NET体系结构，对500个手动标记的CT扫描进行了培训 - 通过纳入扫描 - 解剖学验证 - 通过结合了解剖学。损失损失功能，以惩罚不可能的组织布置 - 生成不同组织类型的3D体积测量 - 每次扫描的处理时间为2-3分钟VS小时手动分析 结果： - 肌肉组织分割的精度为96％ - 皮下脂肪的精度为95％的精度 - 内脏脂肪的精度为94％ - 对3位专家放射科医生的测量结果进行了验证 - 跨不同身体的测量值类型 我认为这可能会通过将人体组成分析所需的时间从小时减少到几分钟来显着影响临床工作流程。高精度和解剖学意识的方法表明，它可能足够可靠用于临床使用。尽管需要更多的验证，尤其是对于边缘病例和极端身体组成，该系统显示出有望改善肿瘤学，营养和运动医学的治疗计划。 我认为解剖学约束的整合特别聪明 - 它有助于防止纯粹的深度学习方法可能产生的身体上不可能的细分。这种领域知识的整合对于其他医学成像任务可能很有价值。  tldr：自动化的CT扫描分析系统将深度学习与解剖学规则结合在一起，以2--的2--; 94％的精度来测量肌肉和脂肪组织。 3分钟。显示出临床使用的希望，但需要更广泛的验证。摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1isbu5z/automated_multitissue_ct_segmentation_model_for/</guid>
      <pubDate>Tue, 18 Feb 2025 12:44:32 GMT</pubDate>
    </item>
    <item>
      <title>物理知情的神经网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1irnodl/physics_informed_neural_networks/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/nickb     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1irnodl/physics_informed_neural_networks/</guid>
      <pubDate>Mon, 17 Feb 2025 16:22:09 GMT</pubDate>
    </item>
    <item>
      <title>如何使用U-NET和TensorFlow分割X射线肺</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1irlh6f/how_to_segment_xray_lungs_using_unet_and/</link>
      <description><![CDATA[    &lt;！ -  sc_off-&gt;   本教程提供了有关如何使用TensorFlow/keras实现和训练X射线肺部分段的U-NET模型的分步指南。 🔍     构建UNET模型：学习如何使用TensorFlow和Keras构建模型。   模型培训：我们将指导您完成培训过程，优化模型以生成肺部位置的口罩  测试和评估：运行预训练的模型在新的新鲜图像上，并在预测的蒙版旁边可视化测试图像。   您可以在博客中找到代码的链接： https://eranfeit.net.net/how-to-to-segment-x -ray-rungs-using-u-net and-tensorFlow/  中等用户的完整代码描述： https://medium.com/@feitgemel/how-to-segment-x-ray-lungs-using-u-net-and-ant-tensorflow-59b5a99a893f   您可以找到更多教程，然后在此处加入我的新闻通讯： https://eranfeit.net/        &lt;强&gt;在此处查看我们的教程： [ https://youtu.be/-aejmcdeoom＆amp;list=uulftifftiwjjhah6bviswkljum9sg]（％20Https：/youtu.be/-aejmcdeoom; 享受  eran      #python #opencv＃tensorflow #tepeeplearning #imagesegentration #imagesementation #unet＃resunet #machinelearningproject #machinelearningproject #segentation #segentation #sementation       div&gt; &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/feitgemel     [link]        [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1irlh6f/how_to_segment_xray_lungs_using_unet_and/</guid>
      <pubDate>Mon, 17 Feb 2025 14:48:24 GMT</pubDate>
    </item>
    <item>
      <title>多语言语音模型的缩放定律：培训0.25b-18b参数模型的见解150种语言</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1irfuvs/scaling_laws_for_multilingual_speech_models/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员在多语言语音识别和通过培训模型翻译中系统地研究缩放行为，跨不同尺寸（300m至1B参数）和数据数量（1K至10K）每种语言小时）。他们根据计算，数据和模型量表为性能开发预测方程。 关键技术方面： - 确定模型大小，培训数据和绩效之间的幂律关系 - 发现添加语言可以改善性能在降低回报之前，至〜8-10语言 - 开发的“猫头鹰得分”量化多语言转移效率的度量 - 证明较大的模型在3个模型体系结构和2种训练方法中显示出更好的跨语性转移 - 验证缩放定律 结果显示： - 错误率遵循指数-0.32的功率定律缩放率-0.32对于模型尺寸 - 跨语言转移通过log（n）改进，其中n是语言数 - 高资源语言从缩放中受益于低资源的缩放量 -  compute-optimal培训需要平衡模型大小和数据数量 - 体系结构的选择小于规模和数据数量 我认为这项工作将有助于组织对多语言模型的资源分配做出更好的决定。缩放定律可以指导有关模型大小，语言选择和数据收集的选择。但是，对高​​资源语言的关注意味着我们仍然需要对真正低资源场景的更多研究。  tldr：系统研究揭示了多语言语音AI的可预测缩放模式语言数量。结果为构建更好的系统提供了实用的指导。 。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1irfuvs/scaling_laws_for_multilingual_speech_models/</guid>
      <pubDate>Mon, 17 Feb 2025 09:12:48 GMT</pubDate>
    </item>
    <item>
      <title>桥接2d-3d域间隙，具有通信感知潜在的辐射场</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iqmoj8/bridging_2d3d_domain_gap_with_correspondenceaware/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员提出了一种新颖的方法，将潜在的辐射场与3D感知的2D图像表示结合在一起，有效地弥合了2D图像操纵和3D一致性之间的差距。关键创新是一个通信感知自动编码框架，该框架在跨不同观点的几何一致性的同时保持有效的编辑。 主要技术方面： - 双分支架构：一个用于2D功能提取，另一个用于3d-warreweaweave to 3d-warreweaweave。处理 - 新颖的对应损失，可确保跨视图的空间一致性 - 有效的本地和全局编辑优化 - 与现有基于NERF的集成体系结构同时减少计算间接费用 结果显示： - 视图合成基准上的最新性能 - 提高了编辑功能，同时保持3D一致性 - 与完整的3D方法相比，记忆要求较低 - 更好地处理复杂的复杂方法照明方案 我认为这种方法可能会严重影响3D一致性至关重要的内容创建工作流。计算需求的减少同时保持质量使其与现实世界应用特别相关。该框架能够处理本地和全球编辑的能力，同时保留3D一致性，可以使其对于虚拟生产和增强现实应用程序都有价值。 我认为，最有趣的方面是他们如何结合了将其结合起来的好处。具有3D意识的2D图像操纵，而无需显式3D建模。这可能会为熟悉2D工作流但需要3D一致性的内容创建者提供更直观的工具。  tldr：新方法将潜在的辐射字段与3D-Aware 2D表示形式相结合，以启用高质量的视图合成和合成。在保持3D一致性的同时进行编辑。   。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iqmoj8/bridging_2d3d_domain_gap_with_correspondenceaware/</guid>
      <pubDate>Sun, 16 Feb 2025 07:05:21 GMT</pubDate>
    </item>
    <item>
      <title>自学CNN，RNN，LSTM用于学位级别应用</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ipye5a/selflearning_cnn_rnn_lstm_for_degree_level/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是最后一年的生物医学工程专业的学生，​​他们对在医疗保健领域的应用有浓厚的兴趣，例如，促进疾病的早期发现使用CNN左右。我的大部分软技能来自MATLAB或C ++，我已经接触了可能与NN有关的信号处理或医学成像等课程。 我的目标很简单，我想应用于NN喜欢CNN通过图像分割进行疾病检测，甚至使用RNN进行生理信号相关分析。我的主要问题是，我应该从哪里开始？社区的任何渠道，书籍甚至文章建议吗？那些在我的问题上有经验的人有什么快速提示吗？甚至更具体地与生物医学领域有关。非常感谢任何相关建议。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/grimgrix     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ipye5a/selflearning_cnn_rnn_lstm_for_degree_level/</guid>
      <pubDate>Sat, 15 Feb 2025 10:10:14 GMT</pubDate>
    </item>
    <item>
      <title>自我引用：通过自我监督的上下文消融改善LLM引文产生</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ipwbr9/selfcite_improving_llm_citation_generation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   selfcite引入了一种自我监督的方法，用于教授LLM，以将信息正确归因于文本生成期间的源文档。关键的创新是使用对比度学习来帮助模型确定应引用输入上下文的哪些部分，而无需手动引用标签。 主要技术点： - 片段输入文档输入到连贯的块中进行引用匹配 - 使用注意 - 基于链接生成的文本与源的链接的上下文归因 - 在真实和随机文档对之间实现对比度学习 - 训练模型以自动区分值得引用的内容 - 实现提高引文精度的同时保持发电质量 关键结果： - 与基线​​模型相比，多种模型尺寸（在7B -70B参数模型上测试）的引文精度提高了 - 降低的幻觉速率 - 维持或改善了rouge的生成分数质量 - 对学术和通用领域文本有效 - 随着模型大小的增加而缩放 我认为这种方法可以显着提高AI生成的可靠性内容通过提供内置源归因。自我监督的性质意味着它可以在没有昂贵的手动标签的情况下广泛应用。对于研究和技术写作应用程序，这可以帮助自动化文献综述，同时保持严格的引用标准。 我看到了学术写作援助和新闻业的特殊价值，而准确的源归因至关重要。该方法还可以通过使索赔更容易追溯到原始来源来帮助进行事实检查。  tldr：自我监督的方法教LLM在没有手动标签的情况下在文本生成期间准确引用来源，从而提高归属准确性同时保持发电质量。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ipwbr9/selfcite_improving_llm_citation_generation/</guid>
      <pubDate>Sat, 15 Feb 2025 07:30:51 GMT</pubDate>
    </item>
    <item>
      <title>新的学作为人类概念交流的桥梁</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ip7iik/neologisms_as_a_bridge_for_humanai_conceptual/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文研究了我们当前的词汇和概念框架如何限制我们正确理解和讨论AI系统的能力。核心论点是，我们需要专门为描述AI行为和能力而开发的新术语，而不是从人类认知中借用拟人化术语。 关键技术点： - 对ML研究中常用的术语分析（学习，学习，学习，理解，智力）及其如何创建错误的类比 - 研究神经网络如何通过人类认知没有直接相似的数学转换处理信息 - 展示如何当前语言导致对AI功能的系统误解 - 开发新的AI特定技术词汇 主要发现： - 人类认知术语不能准确地映射到ML模型操作 - 当前的术语会产生关于AI功能的虚假期望 - 缺乏精确的词汇助攻技术讨论 - 神经网络信息处理与人类认知 我认为这项工作从根本上有所不同强调了人工智能研究和沟通中的一个关键问题。没有准确的术语，我们冒着高估和低估AI功能的风险。 AI特异性词汇的发展可以帮助弥合技术现实和公众理解之间的差距，尽管广泛采用新术语将是具有挑战性的。 我认为本文本可以为提出的新的新典范提供更多具体的例子术语和特定用例。开发新词汇的框架是牢固的，但是实际实施指导是有限的。  tldr：我们需要专门设计用于描述AI系统的新词汇，而不是使用人类的认知术语，因为当前的语言会造成误解并吸引技术技术。理解。  完整摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ip7iik/neologisms_as_a_bridge_for_humanai_conceptual/</guid>
      <pubDate>Fri, 14 Feb 2025 10:15:30 GMT</pubDate>
    </item>
    <item>
      <title>一定步骤后模型损失爆炸</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ip60lf/model_loss_explodes_after_a_certain_steps/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ip60lf/model_loss_explodes_after_a_certain_steps/</guid>
      <pubDate>Fri, 14 Feb 2025 08:22:18 GMT</pubDate>
    </item>
    </channel>
</rss>