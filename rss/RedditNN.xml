<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络，深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的subreddit。</description>
    <lastBuildDate>Sat, 15 Feb 2025 18:20:47 GMT</lastBuildDate>
    <item>
      <title>自学CNN，RNN，LSTM用于学位级别应用</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ipye5a/selflearning_cnn_rnn_lstm_for_degree_level/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是最后一年的生物医学工程专业的学生，​​他们对在医疗保健领域的应用有浓厚的兴趣，例如，促进疾病的早期发现使用CNN左右。我的大部分软技能来自MATLAB或C ++，我已经接触了可能与NN有关的信号处理或医学成像等课程。 我的目标很简单，我想应用于NN喜欢CNN通过图像分割进行疾病检测，甚至使用RNN进行生理信号相关分析。我的主要问题是，我应该从哪里开始？社区的任何渠道，书籍甚至文章建议吗？那些在我的问题上有经验的人有什么快速提示吗？甚至更具体地与生物医学领域有关。非常感谢任何相关建议。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/grimgrix     [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ipye5a/selflearning_cnn_rnn_lstm_for_degree_level/</guid>
      <pubDate>Sat, 15 Feb 2025 10:10:14 GMT</pubDate>
    </item>
    <item>
      <title>自我引用：通过自我监督的上下文消融改善LLM引文产生</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ipwbr9/selfcite_improving_llm_citation_generation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   selfcite引入了一种自我监督的方法，用于教授LLM，以将信息正确归因于文本生成期间的源文档。关键的创新是使用对比度学习来帮助模型确定应引用输入上下文的哪些部分，而无需手动引用标签。 主要技术点： - 片段输入文档输入到连贯的块中进行引用匹配 - 使用注意 - 基于链接生成的文本与源的链接的上下文归因 - 在真实和随机文档对之间实现对比度学习 - 训练模型以自动区分引用的内容 - 提高了引文准确性，同时保持发电质量 关键结果： - 引用精度提高了多个模型尺寸（在7B -70B参数模型上测试） - 与基线​​模型相比降低了幻觉速率 - 在学术和通用域文本中保持或改善了发电质量的Rouge分数 - 随着模型大小的增加而有效地扩展。 /p&gt; 我认为这种方法可以通过提供内置源归因来显着提高AI生成内容的可靠性。自我监督的性质意味着它可以在没有昂贵的手动标签的情况下广泛应用。对于研究和技术写作应用程序，这可以帮助自动化文献综述，同时保持严格的引用标准。 我看到了学术写作援助和新闻业的特殊价值，而准确的源归因至关重要。该方法还可以通过使索赔更容易追溯到原始来源来帮助进行事实检查。  tldr：自我监督的方法教LLM在没有手动标签的情况下在文本生成期间准确引用来源，从而提高归属准确性在保持发电质量的同时。 完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ipwbr9/selfcite_improving_llm_citation_generation/</guid>
      <pubDate>Sat, 15 Feb 2025 07:30:51 GMT</pubDate>
    </item>
    <item>
      <title>新的学作为人类概念交流的桥梁</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ip7iik/neologisms_as_a_bridge_for_humanai_conceptual/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文研究了我们当前的词汇和概念框架如何限制我们正确理解和讨论AI系统的能力。核心论点是，我们需要专门为描述AI行为和能力而开发的新术语，而不是从人类认知中借用拟人化术语。 关键技术点： - 对ML研究中常用的术语分析（学习，学习，学习，理解，智力）及其如何创建错误的类比 - 研究神经网络如何通过人类认知没有直接相似的数学转换来处理信息 - 展示当前语言如何导致对AI功能的系统性误解 - 开发新的AI特定特异性的框架技术词汇 主要发现： - 人类认知术语不能准确地映射到ML模型操作 - 当前的术语会产生对AI功能的错误期望 - 缺乏精确的词汇障碍者的技术讨论技术讨论 - 神经网络信息处理基本上是不同的不同从人类认知 我认为这项工作突出了AI研究和交流中的一个关键问题。没有准确的术语，我们冒着高估和低估AI功能的风险。 AI特异性词汇的发展可以帮助弥合技术现实和公众理解之间的差距，尽管广泛采用新术语将是具有挑战性的。 我认为本文本可以为提出的新的新典范提供更多具体的例子术语和特定用例。开发新词汇的框架是牢固的，但是实际实施指导是有限的。  tldr：我们需要专门设计用于描述AI系统的新词汇，而不是使用人类的认知术语，因为当前的语言会造成误解并吸引技术技术。理解。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ip7iik/neologisms_as_a_bridge_for_humanai_conceptual/</guid>
      <pubDate>Fri, 14 Feb 2025 10:15:30 GMT</pubDate>
    </item>
    <item>
      <title>一定步骤后模型损失爆炸</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ip60lf/model_loss_explodes_after_a_certain_steps/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ip60lf/model_loss_explodes_after_a_certain_steps/</guid>
      <pubDate>Fri, 14 Feb 2025 08:22:18 GMT</pubDate>
    </item>
    <item>
      <title>Matryoshka量化：一种用于具有嵌套精度级别的单个模型的多尺度训练方法</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iofoh9/matryoshka_quantization_a_multiscale_training/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员提出了一种嵌套的量化方法，其中单个模型可以通过权重的层次表示在多个位宽度上运行。关键的想法是构建量化，以使较高的精度表示包含较低精度版本所需的所有信息 - 类似于嵌套的Matryoshka娃娃的工作方式。 关键技术点： - 权重分解为嵌套的组件，这些组件可以合并以达到不同的精度水平 - 使用专门的损失函数同时在多个位宽度上进行优化 - 与训练后量化和量化感知培训兼容 - 在高达7B参数的视觉和语言模型上证明了0.5％的精度在大多数情况下，单位远程基准的结果 结果显示： -  8位→4位嵌套模型的性能类似于单独量化的版本 - 与单精制模型相比，存储开销仅为12.5％ - 动态切换在没有重新加载的精确度之间 - 使用现有的量化方法，例如GPTQ和AWQ  ，我认为这可能对边缘部署方案特别有影响力，在这些方案中，相同模型需要在具有不同计算功能的设备上运行。在不存储多个版本的情况下动态调整精度的能力可以使大型模型在资源约束环境中更实用。 我认为下一个有趣的方向是： - 在较大型号（30b+）上测试 - 硬件特定于硬件优化 - 与其他压缩技术集成（例如修剪） - 探索甚至较低的位宽度表示  tldr：新颖的量化方法，该方法使单个模型可以通过嵌套权重表示以多个精确的方式运行。在启用灵活的部署时保持准确性。  完整摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iofoh9/matryoshka_quantization_a_multiscale_training/</guid>
      <pubDate>Thu, 13 Feb 2025 09:52:56 GMT</pubDate>
    </item>
    <item>
      <title>除了变压器之外，是否有模型体系结构可以使用小数据集，一些GPU和“几个”参数生成良好的文本？这足以生成连贯的英语文本作为简短的答案。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1inx3ys/is_there_a_model_architecture_beyond_transformer/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/challenger_official     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1inx3ys/is_there_a_model_architecture_beyond_transformer/</guid>
      <pubDate>Wed, 12 Feb 2025 17:57:25 GMT</pubDate>
    </item>
    <item>
      <title>在我的NN学习旅程中应该是什么。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1inr3bn/what_should_be_next_in_my_nn_learning_journey/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，一段时间前，我开始学习神经网络纯粹是出于对它们背后的数学的兴趣。从那以后，我写了四篇中等文章总结了我学到的知识：   手动从头开始构建神经网络：初学者的Hello World    爬下坡度：神经网络的梯度下降   神经网络中的归一化和管理梯度下降挑战   Convolutional Neural Networks from刮擦   我开始时对神经网络的了解为零。现在，我的下一个目标是在Pytorch中实施CNN，然后深入研究经常性的神经网络。 但是，我已经收到了一些杂乱无章，人们认为了解NNS在数学上和内部工作（即使我的不是我的）该深度）根本不是有价值的，因为大多数ML/AI Engnieers都使用预制模型，而无需了解在引擎盖下构建它们的内容。这些反馈中有一些让我停下来，让我质疑我是否应该继续使用RNN。 您认为，是否有实际价值可以理解详细信息神经网络在场景后面的工作方式？还是对ML/AI中的人几乎没有任何后果？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/flaky_profession_619      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1inr3bn/what_should_be_next_in_my_nn_learning_journey/</guid>
      <pubDate>Wed, 12 Feb 2025 13:43:57 GMT</pubDate>
    </item>
    <item>
      <title>有效多语言LLM安全检测的两人增强器学习框架</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1innz0u/twoplayer_reinforcement_learning_framework_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文介绍了一种两者增强的学习方法，用于在多语言LLMS中实现护栏。核心创新是使用Markov游戏框架，其中两个RL代理共同使用 - 一个专注于安全措施，另一个侧重于保持对话质量。 关键技术点： - 仅使用2个参数效率微调微调基本模型参数的百分比 - 平衡内容安全性和响应实用程序的自定义奖励功能 - 两个RL播放器之间的交替优化 - 用于多语言理解和文化适应的专业模块 - 实时适度能力，最小的延迟范围 结果显示： - 有害/不适当内容的降低27％ - 有用响应的保存92％，而不是修改的基准 - 跨8种语言有效 - 与以前的方法相比，计算成本降低 - 成功处理明确和细微的安全违规  我认为这种方法可能对在安全和性能重要的生产环境中部署LLM尤其有影响。参数效率意味着可以将其集成到没有大量计算开销的情况下。随着人工智能部署变得更加全局，多语言功能尤其重要。 但是，我认为需要考虑一些局限性。各种语言的各种表现都表明，文化适应需要更多的工作。含糊不清的情况下的保守方法也可能需要调整不同的用例。  tldr：LLM护栏的两播放器RL RL框架可实现27％的有害内容降低，同时维持92％的帮助响应，使用参数 - 跨多种语言起作用的有效的微调。   full摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1innz0u/twoplayer_reinforcement_learning_framework_for/</guid>
      <pubDate>Wed, 12 Feb 2025 10:27:08 GMT</pubDate>
    </item>
    <item>
      <title>评估LLM作为会议代表：跨不同模型和参与策略的绩效分析</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1imwq43/evaluating_llms_as_meeting_delegates_a/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文介绍了一个系统的评估框架，用于测试LLMS作为会议代表，并具有新颖的两阶段体系结构，用于满足理解和摘要。关键的技术贡献是100个带注释的会议记录的基准数据集，并配对，其重点是信息提取和上下文理解的评估方法。 主要技术要点： - 两阶段架构：上下文理解模块，然后是响应模块。 - 跨4个关键指标进行评估：信息提取，摘要连贯性，动作项跟踪和上下文保留 - 单转交换和多转交互之间的比较 - 对包括GPT-4，Claude，Claude等多个LLM体系结构的测试 关键结果：-GPT -4在关键点识别上达到了82％的精度 - 多转弯相互作用显示摘要质量提高了15％ - 在技术讨论中的性能显着降低（30-40％） - 模型显示出不同的性能不一致。会议类型和文化背景 我认为这项工作为自动会议文档提供了实际应用，特别是在常规的商务会议上。多转弯的改进表明，交互式完善可能是这些系统的重要途径。 我认为，技术讨论和跨文化交流的局限性突出了全球组织部署的重要挑战。结果表明，在广泛采用之前，我们需要在领域的适应和文化背景理解方面进行更多工作。  tldr：LLMS作为会议代表的新基准和评估框架的新基准和评估框架，显示出基本的会议理解的有希望的结果，但仍有重大挑战，但仍在技术和跨文化背景。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1imwq43/evaluating_llms_as_meeting_delegates_a/</guid>
      <pubDate>Tue, 11 Feb 2025 11:45:22 GMT</pubDate>
    </item>
    <item>
      <title>PT II：彼得·萨特（Peter Sutor）（访谈）的高维计算（HDC）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1imc73h/pt_ii_hyperdimensional_computing_hdc_with_peter/</link>
      <description><![CDATA[   &lt;A href =“ https://www.reddit.com/r/nealurnetworks/comments/comments/1imc73h/1imc73h/pt_ii_hyperdimensional_computing_hdc_hdc_hdc_hdc_with_peter/” Peter Sutor (Interview)&quot; src=&quot;https://external-preview.redd.it/lx2N8bw5fgUZ7wxweHkDfARaTmwmpwu09YibKeDjiG4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d62805405d4a1c2550036039821add9e6562f36f&quot; title=&quot;Pt II: Hyperdimensional Computing ( HDC）与Peter Sutor（访谈）“/&gt;   ＆＃32;提交由＆＃32; /u/u/u/neurosymbolic     [link]  ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1imc73h/pt_ii_hyperdimensional_computing_hdc_with_peter/</guid>
      <pubDate>Mon, 10 Feb 2025 17:50:58 GMT</pubDate>
    </item>
    <item>
      <title>邀请合作者获得可区分的几何损失功能库</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ilzops/inviting_collaborators_for_a_differentiable/</link>
      <description><![CDATA[在一个用于在pytorch中创建可区分几何损失函数库的项目。 我在这里放置了一些初始提交的项目，以了解可能的外观： github repo        &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/atharvaaalok1     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ilzops/inviting_collaborators_for_a_differentiable/</guid>
      <pubDate>Mon, 10 Feb 2025 06:19:02 GMT</pubDate>
    </item>
    <item>
      <title>我在Java实施了整洁的（预言拓扑的神经进化）！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ilpgk3/i_made_an_implementation_of_neat_neuroevolution/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   heya， 我最近在Java中实现了Neat（Adefing Tupologies的Neurovorloute）！我试图使其与原始纸张和源代码一样真实。我看到还没有足够的实现，所以我在Java中完成了它，目前我也在JavaScript版本上工作！   https://github.com/joshuadam/neat-java   任何反馈和批评都非常欢迎！这是我的第一个大型项目之一，我从制作中学到了很多东西，我为此感到非常自豪！ 谢谢你  &lt;！ -  sc_on-&gt;＆＃ 32;提交由＆＃32; /u/u/u/joshua_damian     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ilpgk3/i_made_an_implementation_of_neat_neuroevolution/</guid>
      <pubDate>Sun, 09 Feb 2025 21:16:41 GMT</pubDate>
    </item>
    <item>
      <title>在部署中挣扎：在一日XGBoost预测中处理动态特征的重要性</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iliiwo/struggling_with_deployment_handling_dynamic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在训练和测试期间使用XGBoost使用XGBoost使用XGBoost创建一个时间序列预测模型。该模型仅在一天前预测能源使用情况，因为我认为这将是最准确的。我们的培训和测试表明，我非常有希望，我在部署方面挣扎。问题在于，最重要的功能是前几天的使用情况，它可能与第二天相关。由于我几乎每天都使用一个滚动窗口，因此到那天有些独特而超大，但非常擅长预测。在部署期间，我不能具有最新功能的重要性，因为我需要与它相对应的目标，这是我要预测的确切值。因此，我可以将目标转移并每天直到前一天进行训练，并且仍然使用末日功能，但是与培训和测试相比，这最终会很糟糕。例如：我有  1月1日  1月2日 试图预测Jan 3rd（无数据）  Jan 1sts 1sts目标（能源使用）在1月2日非常依赖，因此我们可以在所有数据上进行训练，直到第1个数据，因为它具有可用于计算特征重要性的最佳“增益”的目标。我可以包括1月2日的功能，但不会具有正确的功能。看来我几乎正在尝试预测此时的特征重要性。 这很重要，因为如果前一天的能量使用倒转，第二天的温度会大大下降，没有人会再使用AC来例如，前一天从积极到负相关。  我已经构建了一些K表示模型的聚类，但是即使如此，仍然存在一些差异，如果我试图预测下一个K群集，我只会达到同一问题吗？趋势存在很长时间，然后可能会突然下降，下一个K群集将具有不准确的预测。   tldr  如何预测高度可变特征的重要性，这很大程度上依赖于前一天  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/eleastbreath6062     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iliiwo/struggling_with_deployment_handling_dynamic/</guid>
      <pubDate>Sun, 09 Feb 2025 16:27:52 GMT</pubDate>
    </item>
    <item>
      <title>有关选择研究生学论文项目的建议</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ild9cz/advice_on_choosing_a_grad_school_dissertation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家建议。我的目标是在机器人技术领域（希望在AI）领域确保一项出色的工作。这是我要考虑的项目的选项： 传感器选项（或）：视觉触觉传感器 算法选项：尖峰图形神经网络（SGNNS）神经体系结构搜索（ nas）尖峰卷积神经网络（SCNNS） 你们认为哪些选择会在我的简历上留下很大的印记，并帮助将来在行业中确保工作？优点和缺点将不胜感激。  谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/purpleConversation8     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ild9cz/advice_on_choosing_a_grad_school_dissertation/</guid>
      <pubDate>Sun, 09 Feb 2025 12:02:07 GMT</pubDate>
    </item>
    <item>
      <title>多语言互动可以实现更有效的LLM越狱攻击</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1il94av/multistep_multilingual_interactions_enable_more/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员通过自然对话互动介绍了一种系统的测试LLM安全性方法，证明了简单的对话模式如何可靠地绕过内容过滤。他们没有使用复杂的提示或代币操作，而是表明，通过多转交谈的逐步社会工程达到了很高的成功率。 关键技术要点： - 开发的可再现方法来测试对话越狱 - 针对GPT -4测试，Claude和Llama模型变体 - 绕过安全措施的成功率达到了92％的成功率 - 事实证明，多转向对话更有效 - 创建了有害输出类别的分类法 - 跨多个对话模式和主题的验证结果 结果细分： - 安全搭桥的成功因模型而异（GPT -4：92％，克劳德：88％） - 自然语言模式比显式提示更有效 - 逐渐的操纵表现出比直接请求更高的成功 - 效果持续了多个对话。回合 - 在不同的有害内容类型中，成功率保持稳定 我认为这项工作暴露于当前LLM安全机制中的弱点。这些技术的简单性和可靠性表明，我们需要重新考虑如何实施AI安全护栏。当前的方法似乎容易受到基本社会工程的影响，这可能是有问题的，因为这些模型会看到更广泛的部署。 我认为该方法为系统安全测试提供了宝贵的框架，尽管我担心可能滥用这些发现的可能性。跨领先模型的高成功率表明，这不是特定实现的孤立问题。  tldr：简单的对话技术可以可靠地绕开LLM的安全措施，最高92％的成功率，这表明了AI的当前方法安全需要大量改进。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     fink]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1il94av/multistep_multilingual_interactions_enable_more/</guid>
      <pubDate>Sun, 09 Feb 2025 07:05:21 GMT</pubDate>
    </item>
    </channel>
</rss>