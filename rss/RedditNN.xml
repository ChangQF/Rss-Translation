<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Mon, 22 Jan 2024 09:15:35 GMT</lastBuildDate>
    <item>
      <title>Comfi 用户界面帮助（</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19cpeb2/comfi_ui_help/</link>
      <description><![CDATA[您好reddit，这是我第一次发帖，所以我希望知情者能够注意到。请告诉我几件事，1 是将对象添加到已生成的图片中：例如，我有一张图片，我想在上面画一些东西/绘制或更改等，然后将其投入处理，以便这些零件发生了变化，但同时保留了大致的形状。 （我在网上没有找到任何信息，每个人都在谈论inpaint，但要么我做错了什么，要么神经元不理解我，没有任何作用）2-如何在不改变图片的情况下对图片进行更改从根本上改变它？我通过ipadapter尝试过，但最终图像与原始图像越来越远（我减少了去噪，但在这种情况下结果是一团糟）我将非常感谢那些做出回应和与我一起的人可以讨论 comfiui &lt; !-- SC_ON --&gt;  由   提交 /u/Wahiho   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19cpeb2/comfi_ui_help/</guid>
      <pubDate>Mon, 22 Jan 2024 06:35:00 GMT</pubDate>
    </item>
    <item>
      <title>任何人都可以解释（简单地）在有关人工神经网络中的多模态神经元的页面上看到的图像吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19bzp25/can_anyone_explain_in_simple_terms_the_images/</link>
      <description><![CDATA[我了解神经网络的基础知识 - 输入/输出层、隐藏层、权重、偏差等。基本了解。这是一个令人着迷的主题，所以我一直在尝试阅读更多内容，我发现这些页面非常有趣，但我无法理解它们所描述的内容： 人工神经网络中的多模态神经元 https://distill.pub/2021/multimodal-neurons/ &lt; a href=&quot;https://openai.com/research/multimodal-neurons&quot;&gt;https://openai.com/research/multimodal-neurons 这些页面上有图像（对我来说）就像 LSD 幻觉和迷幻艺术。 任何人都可以解释一下（简单地说）：  什么是多模态神经元？ （在这种情况下，“神经元”是什么意思，等等） 这些页面上的奇怪图像到底向我们展示了什么？我不明白那些奇怪的图像想告诉我们什么。    由   提交 /u/papa_libra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19bzp25/can_anyone_explain_in_simple_terms_the_images/</guid>
      <pubDate>Sun, 21 Jan 2024 09:28:23 GMT</pubDate>
    </item>
    <item>
      <title>采访麻省理工学院林肯实验室的 Zack Serlin：正式方法......</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</guid>
      <pubDate>Sat, 20 Jan 2024 20:00:19 GMT</pubDate>
    </item>
    <item>
      <title>人工神经网络中的类脑学习：综述</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.11252 摘要：  人工神经网络（ANN）已成为以下领域的重要工具：机器学习，在图像和语音生成、游戏和机器人等多个领域取得了显着的成功。然而，人工神经网络的运行机制与生物大脑的运行机制之间存在根本差异，特别是在学习过程方面。本文对当前人工神经网络中的类脑学习表示进行了全面回顾。我们研究了更多生物学上合理的机制的整合，例如突触可塑性，以增强这些网络的能力。此外，我们深入研究了这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究找到了有希望的途径，这可以让我们更接近理解智能的本质。     ;由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</guid>
      <pubDate>Fri, 19 Jan 2024 16:03:08 GMT</pubDate>
    </item>
    <item>
      <title>温度、Top-k 和 Top-p 解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19ahr5a/temperature_topk_and_topp_explained/</link>
      <description><![CDATA[   /u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19ahr5a/temperature_topk_and_topp_explained/</guid>
      <pubDate>Fri, 19 Jan 2024 12:14:04 GMT</pubDate>
    </item>
    <item>
      <title>全连接层。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1997h71/full_connected_layers/</link>
      <description><![CDATA[我正在尝试学习神经网络，但目前不了解很多数学知识，所以我有一个问题要问那些知道自己的知识的人关于.我在 python 中使用的神经网络是完全连接的，显然这很好，但它违背了我的直觉，也违背了我认为神经网络的好处。当然，拥有许多不同类型的连接可以存储更复杂的信息。   由   提交/u/Unlucky_Culture_6996   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1997h71/full_connected_layers/</guid>
      <pubDate>Wed, 17 Jan 2024 20:55:24 GMT</pubDate>
    </item>
    <item>
      <title>输入复杂度和所需的隐藏层之间到底有什么关系？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1995sxc/what_exactly_is_the_relationship_between_input/</link>
      <description><![CDATA[ 由   提交/u/swampshark19  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1995sxc/what_exactly_is_the_relationship_between_input/</guid>
      <pubDate>Wed, 17 Jan 2024 19:49:17 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 闪电模型只需很少的代码行即可完成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/</link>
      <description><![CDATA[      我刚刚建立了一个Python库来帮助构建PyTorch闪电模型只需很少的代码行。我很想听听您的想法！ https://github.com/brianrisk/lightning_factory&lt; /p&gt; 闪电工厂概览&lt; /p&gt; ​ ​   由   提交/u/qwaver-io  /u/qwaver-io  reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/</guid>
      <pubDate>Wed, 17 Jan 2024 16:30:56 GMT</pubDate>
    </item>
    <item>
      <title>大脑连接性突破：在不同物种中发现相似的神经网络模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/198zhok/brain_connectivity_breakthrough_similar_neural/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/198zhok/brain_connectivity_breakthrough_similar_neural/</guid>
      <pubDate>Wed, 17 Jan 2024 15:42:08 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/198dh3l/transformers_are_multistate_rnns/</link>
      <description><![CDATA[ 由   提交/u/nickb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/198dh3l/transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 20:48:21 GMT</pubDate>
    </item>
    <item>
      <title>人工智能和计算机视觉解决方案预算实用指南 |第 1 部分 硬件</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/</link>
      <description><![CDATA[      ​&lt; /p&gt; https://preview.redd。 it/efs28iav6ucc1.jpg?width=2800&amp;format=pjpg&amp;auto=webp&amp;s=0ae07562cdd592cbb203f11df5ccbd78abf83213 关于计算机视觉定价的好文章。希望您能顺利找到它。 简短描述： 2024 年，随着越来越多的公司融入人工智能，许多企业主面临着挑战。借助 OpenCV.ai 的专家见解，了解将 AI 集成到您的业务中的基本注意事项。从为计算机视觉解决方案选择合适的相机到导航不同的计算平台，文章提供了实用指导。探索网络和电源优化的细微差别，迈出人工智能驱动成功的第一步。在本系列文章中，我们将指导您了解从硬件和软件选择到人工智能法律方面的所有要点。让我们从第 1 部分开始 |硬件。   由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/</guid>
      <pubDate>Tue, 16 Jan 2024 17:26:04 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1986kc1/transformers_are_multistate_rnns/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.06104 代码：https ://github.com/schwartz-lab-NLP/TOVA 摘要：  Transformers 被认为在概念上与到上一代最先进的 NLP 模型 - 循环神经网络 (RNN)。在这项工作中，我们证明了仅解码器 Transformer 实际上可以被概念化为无限多状态 RNN——一种具有无限隐藏状态大小的 RNN 变体。我们进一步证明，通过固定隐藏状态的大小，预训练的 Transformer 可以转换为有限多状态 RNN。我们观察到一些现有的转换器缓存压缩技术可以被构建为这样的转换策略，并引入了一种新的策略，TOVA，它比这些策略更简单。我们对多个远程任务进行的实验表明，TOVA 优于所有其他基线策略，同时几乎与完整（无限）模型相当，并且在某些情况下仅使用原始缓存大小的 1/8。我们的结果表明，变压器解码器 LLM 在实践中通常表现为 RNN。他们还提出了缓解最痛苦的计算瓶颈之一——缓存大小的选项。我们在 此 https URL 公开发布我们的代码。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1986kc1/transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 16:12:45 GMT</pubDate>
    </item>
    <item>
      <title>遗传算法的前馈网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196s6wq/feedfoward_network_with_genetic_algorithm/</link>
      <description><![CDATA[大家好！我正在尝试创建一个神经网络来在赛道上行驶，但它太难了，比我想象的要难。我使用遗传算法和具有 2 个隐藏层的神经网络，每个隐藏层有 10 个隐藏神经元，为了选择最好的，我使用波前算法。我在 Unity 中编码，这是我的 github 存储库。如果您能帮助我，我将不胜感激： https://github.com/lucasramosdev/self-driven -汽车  ​   由   提交 /u/Proscrite   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196s6wq/feedfoward_network_with_genetic_algorithm/</guid>
      <pubDate>Sun, 14 Jan 2024 22:26:07 GMT</pubDate>
    </item>
    <item>
      <title>科学家展示了大脑使用的浅层学习机制如何与深度学习竞争</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196j4du/scientists_show_how_shallow_learning_mechanism/</link>
      <description><![CDATA[   /u/SparklySpencer  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196j4du/scientists_show_how_shallow_learning_mechanism/</guid>
      <pubDate>Sun, 14 Jan 2024 16:03:09 GMT</pubDate>
    </item>
    <item>
      <title>KL 散度数学解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196d7qh/kl_divergence_mathematics_explained/</link>
      <description><![CDATA[您好， 我创建了一个视频 这里我解释了KL散度背后的数学直觉。 我希望它对你们中的一些人有用。非常欢迎反馈！ :)   由   提交/u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196d7qh/kl_divergence_mathematics_explained/</guid>
      <pubDate>Sun, 14 Jan 2024 10:44:32 GMT</pubDate>
    </item>
    </channel>
</rss>