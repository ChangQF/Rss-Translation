<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Mon, 15 Jan 2024 18:17:18 GMT</lastBuildDate>
    <item>
      <title>强化学习调查</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/197f617/reinforcement_learning_survey/</link>
      <description><![CDATA[https://twitter.com/EzgiKorkmazAI/status /1744434469107335628    由   提交 /u/ml_dnn   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/197f617/reinforcement_learning_survey/</guid>
      <pubDate>Mon, 15 Jan 2024 18:01:28 GMT</pubDate>
    </item>
    <item>
      <title>遗传算法的前馈网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196s6wq/feedfoward_network_with_genetic_algorithm/</link>
      <description><![CDATA[大家好！我正在尝试创建一个神经网络来在赛道上行驶，但它太难了，比我想象的要难。我使用遗传算法和具有 2 个隐藏层的神经网络，每个隐藏层有 10 个隐藏神经元，为了选择最好的，我使用波前算法。我在 Unity 中编码，这是我的 github 存储库。如果您能帮助我，我将不胜感激： https://github.com/lucasramosdev/self-driven -汽车  ​   由   提交 /u/Proscrite   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196s6wq/feedfoward_network_with_genetic_algorithm/</guid>
      <pubDate>Sun, 14 Jan 2024 22:26:07 GMT</pubDate>
    </item>
    <item>
      <title>损失被限制在我的 GCN 模型中 [P]</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196lpjc/loss_is_getting_clamped_in_my_gcn_model_p/</link>
      <description><![CDATA[​ 我已经使用 pytorch 在具有相同边缘索引的图上训练了以下模型（任务是图分类）电子健康记录，其中每个图代表患者数据，节点向量是从组合知识图导出的） class mdl(torch.nn.Module): def init(self, input_size, hidden_​​size，output_size，dropout_rate）： super（GCNClassifier，self）.init（） self.conv1 = GCNConv（input_size，hidden_​​size） self.conv2 = GCNConv（hidden_​​size，output_size） self.dropout = torch.nn.Dropout（dropout_rate）def向前（self，x，edge_index）：x = self.conv1（x，edge_index）x = F.relu（x）x = self.dropout（x）x = self.conv2（x，edge_index）x = torch.mean (x, dim=0, keepdim=True) return x  问题是损失被限制在特定值 我尝试了各种学习率值并尝试了各种技术，如动量和学习率调度，但损失仍然保持不变 我尝试使用以下循环训练上述模型 #training (graphVec) 800 个图（每个图形状为 [5,20]) #y_train 是形状为 [800,1] 的 0 和 1 的张量，用于二元分类 num_epochs = 100（对于 epoch） range(num_epochs): model.train() for i in range(len(graphVec)): # 在每次迭代中将每个图传递给模型 output = model(graphVec[i], edge_index) loss = criteria(output, y_train[ i])loss.backward()optimizer.step()optimizer.zero_grad()#StepLR调度器步骤scheduler.step()print(output)#打印每个epoch的损失和学习率current_lr=optimizer.param_groups[0][&#39;lr &#39;] print(f&#39;Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Learning Rate: {current_lr}&#39;)  但是我我的损失严重受限（损失并没有随着时代的推移而减少）我该怎么办？   由   提交/u/Willing-Cell1790   /u/Willing-Cell1790 reddit.com/r/neuralnetworks/comments/196lpjc/loss_is_getting_clamped_in_my_gcn_model_p/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196lpjc/loss_is_getting_clamped_in_my_gcn_model_p/</guid>
      <pubDate>Sun, 14 Jan 2024 17:54:53 GMT</pubDate>
    </item>
    <item>
      <title>科学家展示了大脑使用的浅层学习机制如何与深度学习竞争</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196j4du/scientists_show_how_shallow_learning_mechanism/</link>
      <description><![CDATA[   /u/SparklySpencer  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196j4du/scientists_show_how_shallow_learning_mechanism/</guid>
      <pubDate>Sun, 14 Jan 2024 16:03:09 GMT</pubDate>
    </item>
    <item>
      <title>KL 散度数学解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196d7qh/kl_divergence_mathematics_explained/</link>
      <description><![CDATA[您好， 我创建了一个视频 这里我解释了KL散度背后的数学直觉。 我希望它对你们中的一些人有用。非常欢迎反馈！ :)   由   提交/u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196d7qh/kl_divergence_mathematics_explained/</guid>
      <pubDate>Sun, 14 Jan 2024 10:44:32 GMT</pubDate>
    </item>
    <item>
      <title>🎨 使用 Tensorflow 和 Python 的神经风格迁移教程</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/195195z/neural_style_transfer_tutorial_with_tensorflow/</link>
      <description><![CDATA[      https://preview.redd.it/k8xazw7ps1cc1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=8e3d290829f6 01169dfc1014ad18824bb3844115 🚀 在本视频教程中，我们将使用艺术 Python 库生成图像 探索神经风格迁移的迷人领域，并学习如何将图像与您选择的风格合并  p&gt; 以下是您将学到的内容： 🔍 从 TensorFlow 模型中心下载模型：探索使用 TensorFlow 模型中心预训练模型的便利性。  我们将引导您完成为您的艺术事业找到完美模型的步骤。  🖼️ 预处理图像以进行神经风格迁移：优化图像以实现风格迁移成功！  学习基本的预处理步骤，从调整大小到标准化，确保您的结果非常出色。  🎭 应用和可视化风格迁移：深入研究“风格迁移质量” GitHub 存储库。跟随我们应用神经网络来区分风格和生成的图像特征。  观看您的图像以比以往更高的质量进行转换。 您可以在此处找到代码：https://github.com/feitgemel/Python-Code-Cool-Stuff/tree/master/style-transfer 视频链接：https://youtu.be/QgEg61WyTe0 欣赏 Eran ​ #python #styletransferquality #tensorflow #NeuralStyleTransfer #PythonAI #ArtTech   &amp;# 32；由   提交 /u/Feitgemel   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/195195z/neural_style_transfer_tutorial_with_tensorflow/</guid>
      <pubDate>Fri, 12 Jan 2024 17:56:28 GMT</pubDate>
    </item>
    <item>
      <title>推理捷径</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/194fhgs/reasoning_shortcuts/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/194fhgs/reasoning_shortcuts/</guid>
      <pubDate>Thu, 11 Jan 2024 23:09:10 GMT</pubDate>
    </item>
    <item>
      <title>在尖峰神经网络中学习长序列</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1945g0p/learning_long_sequences_in_spiking_neural_networks/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.00955 摘要：  尖峰神经网络（SNN）从大脑中汲取灵感，使节能计算。自从 Transformer 出现以来，SNN 一直在努力与现代序列任务上的人工网络竞争，因为它们继承了循环神经网络 (RNN) 的局限性，并且增加了使用不可微二元尖峰激活进行训练的挑战。然而，最近人们对 Transformer 的高效替代方案重新产生了兴趣，催生了名为状态空间模型 (SSM) 的最先进的循环架构。这项工作首次系统地研究了最先进的 SSM 与用于远程序列建模的 SNN 的交叉点。结果表明，基于 SSM 的 SNN 在成熟的远程序列建模基准的所有任务上都可以优于 Transformer。研究还表明，基于 SSM 的 SNN 在顺序图像分类方面可以使用更少的参数优于当前最先进的 SNN。最后，引入了一种新颖的特征混合层，提高了 SNN 的准确性，同时挑战了有关二元激活在 SNN 中的作用的假设。这项工作为将强大的基于 SSM 的架构（例如大型语言模型）部署到神经形态硬件以实现节能的远程序列建模铺平了道路。  &lt;!-- SC_ON - -&gt;  由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1945g0p/learning_long_sequences_in_spiking_neural_networks/</guid>
      <pubDate>Thu, 11 Jan 2024 16:17:26 GMT</pubDate>
    </item>
    <item>
      <title>我们是否可以说 Siamese 网络使用的 GPU RAM 是基准模型的两倍？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19342b2/can_we_say_that_the_siamese_network_uses_twice_as/</link>
      <description><![CDATA[如标题所示，可以吗？   由   提交 /u/JohnTheWeak   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19342b2/can_we_say_that_the_siamese_network_uses_twice_as/</guid>
      <pubDate>Wed, 10 Jan 2024 09:04:50 GMT</pubDate>
    </item>
    <item>
      <title>MoE-Mamba：专家混合的高效选择性状态空间模型</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/192z0iv/moemamba_efficient_selective_state_space_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.04081 代码：https ://github.com/llm-random/llm-random 摘要：  状态空间模型（SSM）已成为顺序建模领域的有力竞争者，挑战 Transformers 的统治地位。与此同时，Mixture of Experts (MoE) 显着改进了基于 Transformer 的法学硕士，包括最近最先进的开源模型。我们建议，为了释放 SSM 的扩展潜力，它们应该与 MoE 结合起来。我们在 Mamba 上展示了这一点，这是一个最近基于 SSM 的模型，它实现了类似 Transformer 的卓越性能。我们的模型 MoE-Mamba 的性能优于 Mamba 和 Transformer-MoE。特别是，MoE-Mamba 以减少 2.2 倍的训练步骤达到与 Mamba 相同的性能，同时保留了 Mamba 针对 Transformer 的推理性能增益。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/192z0iv/moemamba_efficient_selective_state_space_models/</guid>
      <pubDate>Wed, 10 Jan 2024 04:00:49 GMT</pubDate>
    </item>
    <item>
      <title>完全自动化的 GPT 博客案例研究</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1926v40/completely_automated_gpt_blog_case_study/</link>
      <description><![CDATA[ 由   提交 /u/PikeMerry   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1926v40/completely_automated_gpt_blog_case_study/</guid>
      <pubDate>Tue, 09 Jan 2024 05:13:17 GMT</pubDate>
    </item>
    <item>
      <title>LSTM 网络的帮助/建议</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/191izew/helpadvice_with_lstmnetworks/</link>
      <description><![CDATA[       ​ https://preview.redd.it/8ekj0m6u97bc1.png?width=2596&amp;format=png&amp;auto=webp&amp;s=f69367e2579fcdd4b9720660fa4d56d8 3255dd91 我是rnn lstm 中的新功能。我用这是基本的 lstm 模型来解释我的问题。 ​ https://preview.redd.it/hbhef6kv97bc1.png?width=931&amp;format=png&amp;auto=webp&amp;s=b532db72a9abad77202de4ecc7 fe4ad2b33f3233&lt; /p&gt; 如何实现这样的 lstm 模型。我想按顺序将预编码和固定代码发送到 lstm 模型中，并预测它的错误错误或重构名称。 ​ ​ &lt; /div&gt;  由   提交 /u/Surprise_Nearby   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/191izew/helpadvice_with_lstmnetworks/</guid>
      <pubDate>Mon, 08 Jan 2024 11:18:12 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶汽车中的计算机视觉</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/191h0s0/computer_vision_in_selfdriving_cars/</link>
      <description><![CDATA[        由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/neuralnetworks/comments/191h0s0/computer_vision_in_selfdriven_cars/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/191h0s0/computer_vision_in_selfdriving_cars/</guid>
      <pubDate>Mon, 08 Jan 2024 09:06:38 GMT</pubDate>
    </item>
    <item>
      <title>从头开始的 NEAT 算法（很难）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/190x2nu/neat_algorithm_from_scratch_it_was_hard/</link>
      <description><![CDATA[   /u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/190x2nu/neat_algorithm_from_scratch_it_was_hard/</guid>
      <pubDate>Sun, 07 Jan 2024 17:19:38 GMT</pubDate>
    </item>
    <item>
      <title>NIST 确定了操纵人工智能系统行为的网络攻击类型</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/190kncb/nist_identifies_types_of_cyberattacks_that/</link>
      <description><![CDATA[   /u/nickb  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/190kncb/nist_identifies_types_of_cyberattacks_that/</guid>
      <pubDate>Sun, 07 Jan 2024 05:23:10 GMT</pubDate>
    </item>
    </channel>
</rss>