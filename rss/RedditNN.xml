<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的子版块。</description>
    <lastBuildDate>Sat, 28 Sep 2024 12:29:55 GMT</lastBuildDate>
    <item>
      <title>如何计算神经网络架构的步幅和填充？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fr4o5m/how_to_calculate_stride_and_padding_for_neural/</link>
      <description><![CDATA[        提交人    /u/varundate98 ​​  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fr4o5m/how_to_calculate_stride_and_padding_for_neural/</guid>
      <pubDate>Sat, 28 Sep 2024 02:41:38 GMT</pubDate>
    </item>
    <item>
      <title>智囊团税务信息[D]</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fqwpzw/braintrust_tax_information_d/</link>
      <description><![CDATA[我来自埃及，我不知道该提供什么有关 Braintrust 税务信息，有人可以帮我吗？    提交人    /u/Asser404   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fqwpzw/braintrust_tax_information_d/</guid>
      <pubDate>Fri, 27 Sep 2024 20:02:50 GMT</pubDate>
    </item>
    <item>
      <title>平均只是神经科学的一个虚构</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fo5rbp/averaging_is_a_convenient_fiction_of_neuroscience/</link>
      <description><![CDATA[        提交人    /u/nickb   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fo5rbp/averaging_is_a_convenient_fiction_of_neuroscience/</guid>
      <pubDate>Tue, 24 Sep 2024 06:08:27 GMT</pubDate>
    </item>
    <item>
      <title>探索101ai.net：面向大学生和爱好者的全新 AI 学习工具！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fnbeub/exploring_101ainet_a_new_ai_learning_tool_for/</link>
      <description><![CDATA[大家好！👋 我一直在做一个个人项目，101ai.net，这是一个旨在让学习 AI 和 ML 变得有趣和互动的平台。它提供了一个带有可视化工具、教程、视频和动手 Python 代码示例的游乐场。我很想听听你对如何改进或扩展它的想法。如果你想以简化的方式试验和学习 AI 概念，那就来看看吧！你的反馈将非常宝贵。    提交人    /u/msahmad   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fnbeub/exploring_101ainet_a_new_ai_learning_tool_for/</guid>
      <pubDate>Mon, 23 Sep 2024 03:34:42 GMT</pubDate>
    </item>
    <item>
      <title>使用根尖 X 射线检测堵塞根管内断裂/分离的器械 [D]</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1flk804/detection_of_fracturedseperated_instruments_in/</link>
      <description><![CDATA[是否有任何开源数据集可供我进行根尖周 x 射线图像的断裂或分离器械的物体检测？    提交人    /u/Asser404   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1flk804/detection_of_fracturedseperated_instruments_in/</guid>
      <pubDate>Fri, 20 Sep 2024 19:22:30 GMT</pubDate>
    </item>
    <item>
      <title>复杂、混乱、非线性序列中的下一个数字</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1flana1/next_number_in_a_complex_chaotic_and_nonlinear/</link>
      <description><![CDATA[      我正在尝试预测复杂、混乱和非线性序列中的下一个数字。即使我无法预测确切的下一个数字，我也想发现影响序列中值的模式。我认为可能发生的情况如下：多种因素在起作用：序列中的每个数字可能受到几种不同属性或因素的影响，并且这些因素可以以复杂的方式相互作用。 可能存在噪音：序列可能包含噪音或异常，这意味着可能会存在混淆事物的误报或漏报。 属性之间的相互作用：某些属性可能不会产生孤立的影响，而是可以与其他属性（来自序列中的不同位置）相互作用以影响数字的值。 模式发现和预测：最终，我的目标是识别任何潜在模式（如果存在）。即使它是一个混乱的系统，我也希望预测序列中下一个数字的成功率至少达到 60%。 训练数据的困难：鉴于系统非常混乱，我不确定是否可以通过传统方式在一组测试数据上训练系统。 我需要帮助： 使用神经网络是最佳方法吗？是否有任何现有程序或产品可用于解决此问题？如有任何线索或建议，我们将不胜感激。 https://preview.redd.it/mvqep3woiypd1.jpg?width=3009&amp;format=pjpg&amp;auto=webp&amp;s=2db7c377cbf2b53abe11456f35e57f45ced5055f    提交人    /u/Putrid_Pea_6699   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1flana1/next_number_in_a_complex_chaotic_and_nonlinear/</guid>
      <pubDate>Fri, 20 Sep 2024 12:20:56 GMT</pubDate>
    </item>
    <item>
      <title>我们读过的一些研究论文的摘要！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fl4al2/summaries_of_some_research_papers_we_read/</link>
      <description><![CDATA[印度理工学院鲁尔基分校的视觉语言小组整理了 2016 年至 2024 年期间来自 NeurIPS、CVPR、ICCV、ICML 等顶级会议的深度学习研究论文的综合摘要库。这些摘要旨在让您简明扼要地了解计算机视觉、自然语言处理和机器学习等领域的有影响力的论文。该库不断扩充，并经常添加新的摘要。以下是一些值得注意的例子：  DreamBooth：针对主题驱动生成对文本到图像扩散模型进行微调，CVPR&#39;23 DreamBooth 摘要 Segment Anything，ICCV&#39;23 Segment Anything 摘要 一图胜千言：使用文本反转个性化文本到图像生成，ICCV&#39;23 文本反转总结 具有深度语言理解的逼真文本到图像扩散模型，NIPS&#39;22 逼真扩散总结 一张图片胜过 16x16 个单词：用于大规模图像识别的 Transformers，ICLR&#39;21 Vision Transformer 总结 Big Bird：用于更长序列的 Transformers，NIPS&#39;20 Big Bird Transformers 摘要  该存储库邀请社区做出贡献。如果您发现这些摘要有用，我们鼓励您提交自己的研究论文摘要。该团队旨在定期更新该集合，其中包含即将召开的会议的论文摘要以及深度学习和人工智能的关键主题。 您可以在此处访问完整的存储库并做出贡献： 视觉语言组论文摘要 通过贡献，您将帮助使该领域的初学者和专家更容易获得高级研究。    提交人    /u/vlg_iitr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fl4al2/summaries_of_some_research_papers_we_read/</guid>
      <pubDate>Fri, 20 Sep 2024 04:57:28 GMT</pubDate>
    </item>
    <item>
      <title>从头开始构建神经网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fkn6cc/build_the_neural_network_from_scratch/</link>
      <description><![CDATA[大家好， 我们刚刚为那些想要了解如何从头开始构建神经网络（包括所有数学知识）的人提供了一个 github 存储库和 medium 博客。 GitHub：https://github.com/SorawitChok/Neural-Network-from-scratch-in-Cpp Medium：https://medium.com/@sirawitchokphantavee/build-a-neural-network-from-scratch-in-c-to-deeply-understand-how-it-works-not-just-how-to-use-008426212f57 希望这可能有用    提交人    /u/Crucial-Manatee   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fkn6cc/build_the_neural_network_from_scratch/</guid>
      <pubDate>Thu, 19 Sep 2024 15:09:23 GMT</pubDate>
    </item>
    <item>
      <title>使用神经网络解决随机规划（或任何数学规划）问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fkgrf4/solving_stochastic_programming_or_any/</link>
      <description><![CDATA[有人知道文献或 github 中有人使用神经网络解决线性规划或随机规划等数学规划问题的例子吗？ 大约一年前，我在这个 subreddit 上发表了一篇关于 NN 及其解决线性规划等数学规划问题的能力的帖子。大多数回复指出，经典数学规划算法优于 NN，但如果问题公式中存在高度随机性，NN 可能会优于经典算法。 我一直在寻找任何关于使用 NN 解决随机规划/模糊规划甚至线性规划的材料，但发现很难找到任何东西。 有人知道关于这方面的参考资料吗？    提交人    /u/vniversvs_   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fkgrf4/solving_stochastic_programming_or_any/</guid>
      <pubDate>Thu, 19 Sep 2024 09:31:18 GMT</pubDate>
    </item>
    <item>
      <title>需要有关创建用于聚类的神经网络（不使用 K 均值）的指导</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fk63qe/need_guidance_on_creating_a_neural_network_for/</link>
      <description><![CDATA[我目前正在从事一个对我来说非常重要的项目，我需要一些有神经网络经验的人的指导。我面临的挑战是使用随机生成的 2D 数据点为 K=n 簇创建一个神经网络，准确率超过 95%。但是，我需要在不使用 K 均值算法的情况下实现这一点。 如果您能提出任何解决此问题的建议、资源或方法，我将不胜感激。我知道这个社区有很多专家，您的见解将意义重大！    提交人    /u/According_Lynx_3571   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fk63qe/need_guidance_on_creating_a_neural_network_for/</guid>
      <pubDate>Wed, 18 Sep 2024 22:56:19 GMT</pubDate>
    </item>
    <item>
      <title>FDA 批准 Neuralink Blindsight</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fjyu6j/fda_approves_neuralink_blindsight/</link>
      <description><![CDATA[        提交人    /u/keghn   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fjyu6j/fda_approves_neuralink_blindsight/</guid>
      <pubDate>Wed, 18 Sep 2024 17:42:37 GMT</pubDate>
    </item>
    <item>
      <title>死亡的 relu 神经元</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fj3jj0/dead_relu_neurons/</link>
      <description><![CDATA[如果网络早期部分的输出发生变化，即使神经元前面的权重保持不变，死亡的 relu 神经元能否恢复？    提交人    /u/Outrageous-Key-4838   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fj3jj0/dead_relu_neurons/</guid>
      <pubDate>Tue, 17 Sep 2024 16:22:31 GMT</pubDate>
    </item>
    <item>
      <title>尝试用 Python 编写自己的神经网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fiuen9/trying_to_program_my_own_neural_network_in_python/</link>
      <description><![CDATA[您有任何可以帮助我开始的视频或文档吗？谢谢！    提交人    /u/MatFSouza   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fiuen9/trying_to_program_my_own_neural_network_in_python/</guid>
      <pubDate>Tue, 17 Sep 2024 09:12:54 GMT</pubDate>
    </item>
    <item>
      <title>Llama 3.1 70B 和 Llama 3.1 70B Instruct 压缩了 6.4 倍，现在重 22 GB</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fiseqq/llama_31_70b_and_llama_31_70b_instruct_compressed/</link>
      <description><![CDATA[我们使用与 IST Austria 和 KAUST 共同开发的 PV-Tuning 方法压缩了 Llama 3.1 70B 和 Llama 3.1 70B Instruct。  该模型现在小了 6.4 倍（141 GB --&gt; 22 GB）。  您将需要 3090 GPU 来运行模型，但您可以在自己的电脑上执行此操作。  您可以在此处下载压缩模型： https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-AQLM-PV-2Bit-1x16 https://huggingface.co/ISTA-DASLab/Meta-Llama-3.1-70B-Instruct-AQLM-PV-2Bit-1x16/tree/main   由    /u/azalio  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fiseqq/llama_31_70b_and_llama_31_70b_instruct_compressed/</guid>
      <pubDate>Tue, 17 Sep 2024 06:58:31 GMT</pubDate>
    </item>
    <item>
      <title>元认知 AI：通过查找 ML 错误来恢复约束</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1fhwh3g/metacognitive_ai_recovering_constraints_by/</link>
      <description><![CDATA[        由    /u/Neurosymbolic  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1fhwh3g/metacognitive_ai_recovering_constraints_by/</guid>
      <pubDate>Mon, 16 Sep 2024 04:46:21 GMT</pubDate>
    </item>
    </channel>
</rss>