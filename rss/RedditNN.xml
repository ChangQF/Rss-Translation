<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Sun, 31 Mar 2024 21:11:01 GMT</lastBuildDate>
    <item>
      <title>黑匣子内部：卷积神经网络可视化！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bshff9/inside_the_black_box_convolutional_neural_nets/</link>
      <description><![CDATA[   /u/AvvYaa  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bshff9/inside_the_black_box_convolutional_neural_nets/</guid>
      <pubDate>Sun, 31 Mar 2024 18:38:14 GMT</pubDate>
    </item>
    <item>
      <title>我们将如何实现 1 万亿晶体管 GPU</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bsdqfm/how_well_reach_a_1_trillion_transistor_gpu/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bsdqfm/how_well_reach_a_1_trillion_transistor_gpu/</guid>
      <pubDate>Sun, 31 Mar 2024 15:58:14 GMT</pubDate>
    </item>
    <item>
      <title>我在一个函数上训练了 LSTM，使其在输入前 50 个数据点后预测下一个数据点！每一帧都是另一个训练纪元！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bs8obr/i_trained_an_lstm_on_a_function_such_that_it/</link>
      <description><![CDATA[   /u/frinnedfodern   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bs8obr/i_trained_an_lstm_on_a_function_such_that_it/</guid>
      <pubDate>Sun, 31 Mar 2024 11:55:14 GMT</pubDate>
    </item>
    <item>
      <title>批量归一化</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bs7olg/batch_normalization/</link>
      <description><![CDATA[      &amp;# 32；由   提交/u/UpvoteBeast  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bs7olg/batch_normalization/</guid>
      <pubDate>Sun, 31 Mar 2024 10:58:12 GMT</pubDate>
    </item>
    <item>
      <title>TensorFlow 迁移学习：使用 Mobilenet 和 Python 对图像进行分类</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1brddq9/tensorflow_transfer_learning_classify_images_with/</link>
      <description><![CDATA[      https://preview.redd.it/zyusaigsrfrc1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=b6b7442176ceca4d52bef 6dba7571da7af8e6b91 在本视频中，我们将向您展示如何使用 TensorFlow 和 Mobilenet 通过迁移学习来训练图像分类模型。  我们将指导您完成图像数据预处理、微调预训练 Mobilenet 模型以及使用验证数据评估其性能的过程。  视频教程的链接在这里：https://youtu.be/xsBm_DTSbB0 &lt; p&gt;我还在视频说明中分享了 Python 代码。 享受吧， Eran #TensorFlow #Mobilenet #ImageClassification #TransferLearning #Python #DeepLearning #机器学习 #ArtificialIntelligence #PretrainedModels #ImageRecognition #OpenCV #ComputerVision #Cnn   由   提交 /u/Feitgemel   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1brddq9/tensorflow_transfer_learning_classify_images_with/</guid>
      <pubDate>Sat, 30 Mar 2024 08:58:14 GMT</pubDate>
    </item>
    <item>
      <title>受大脑启发的混沌尖峰反向传播</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bquik4/braininspired_chaotic_spiking_backpropagation/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bquik4/braininspired_chaotic_spiking_backpropagation/</guid>
      <pubDate>Fri, 29 Mar 2024 17:10:30 GMT</pubDate>
    </item>
    <item>
      <title>BART 模型解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bqptwh/bart_model_explained/</link>
      <description><![CDATA[您好， 我创建了一个视频 这里我解释了BART模型的架构以及它是如何预训练的。 我希望它对你们中的一些人有用。非常欢迎反馈！ :)   由   提交/u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bqptwh/bart_model_explained/</guid>
      <pubDate>Fri, 29 Mar 2024 13:53:04 GMT</pubDate>
    </item>
    <item>
      <title>负责任的法学硕士（陈天龙，麻省理工学院 - Metacog AI）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bpdgxg/accountable_llms_tianlong_chen_mit_metacog_ai/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bpdgxg/accountable_llms_tianlong_chen_mit_metacog_ai/</guid>
      <pubDate>Wed, 27 Mar 2024 21:23:53 GMT</pubDate>
    </item>
    <item>
      <title>探索用于生成音乐可解释人工智能的变分自动编码器架构、配置和数据集</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bpb1ko/exploring_variational_autoencoder_architectures/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bpb1ko/exploring_variational_autoencoder_architectures/</guid>
      <pubDate>Wed, 27 Mar 2024 19:47:02 GMT</pubDate>
    </item>
    <item>
      <title>人工神经网络的基本深度学习算法</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bo7wno/essential_deep_learning_algorithms_for_artificial/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/Emily-joe  /u/Emily-joe  artiba.org/blog/essential-deep-learning-algorithms-for-artificial-neural-networks&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bo7wno/essential_deep_learning_algorithms_for_artificial/</guid>
      <pubDate>Tue, 26 Mar 2024 13:28:24 GMT</pubDate>
    </item>
    <item>
      <title>如何使用更少的 GPU 内存训练神经网络：可逆残差网络回顾</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bn93sm/how_to_train_a_neural_network_with_less_gpu/</link>
      <description><![CDATA[探索可逆残差网络的有趣方法。 OpenCV.ai 团队的新文章回顾了一种减少 GPU 内存需求的方法在神经网络训练期间。您将发现可逆残差网络在神经网络训练期间如何节省 GPU 内存。该技术在“可逆残差网络：无需存储激活的反向传播”中详细描述。通过不存储反向传播的激活，可以有效地训练更大的模型。了解其在降低硬件要求方面的应用，同时保持 CIFAR 和 ImageNet 分类等任务的准确性。   由   提交/u/Human_Statistician48   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bn93sm/how_to_train_a_neural_network_with_less_gpu/</guid>
      <pubDate>Mon, 25 Mar 2024 09:05:45 GMT</pubDate>
    </item>
    <item>
      <title>FeatUp：一种机器学习算法，可升级深度神经网络的分辨率以提高计算机视觉任务的性能</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bn76uj/featup_a_machine_learning_algorithm_that_upgrades/</link>
      <description><![CDATA[       由   提交/u/UpvoteBeast  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bn76uj/featup_a_machine_learning_algorithm_that_upgrades/</guid>
      <pubDate>Mon, 25 Mar 2024 06:46:48 GMT</pubDate>
    </item>
    <item>
      <title>我需要有关 SoftMax 函数的说明</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bmsq2f/i_need_clarification_on_the_softmax_function/</link>
      <description><![CDATA[我最近一直在开发一个深度学习库，以更好地理解机器学习概念，现在我正在实现一个软最大激活函数，一切似乎都是正确的，因为我检查了现有 softmax 计算器上的输出和导数的值并且它们匹配起来，但是当我让我的网络进行训练时，它没有学到任何东西，输出要么是 [0.5, 0.4] ish，要么是它们不考虑目标，随机变为 0.9。  当我的数据集中有多个样本（例如图像分类器）时，各个样本的所有输出加起来为 1，这令人困惑。我知道 softmax 应该将 1 个样本的输出分配为“概率”但我不明白为什么它发生在我的数据集的样本之间 我很困惑，softmax 是否只是一个常规激活函数，或者它是否像一个特殊情况。我看到一个视频，他们将 softmax 视为一层而不是激活函数，这让我怀疑与其他激活函数相比，softmax 函数是否具有不同的反向传播步骤。当我将最后一个激活函数更改为 sigmoid、tanH 或 ReLU 时，一切正常。 非常感谢任何反馈或评论，因为我花了几个小时在这上面。   由   提交 /u/chjammy   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bmsq2f/i_need_clarification_on_the_softmax_function/</guid>
      <pubDate>Sun, 24 Mar 2024 19:13:17 GMT</pubDate>
    </item>
    <item>
      <title>回归网络中使用什么样的激活函数？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bmentm/what_kind_of_activation_functions_are_used_in/</link>
      <description><![CDATA[我正在从头开始编码一个神经网络，并且想知道哪种激活函数最适合回归问题（我正在尝试让它评估输入）。我的权重和偏差应该在什么样的范围内，它们应该是负数还是正数。训练过程与分类网络有何不同？感谢任何帮助:)   由   提交 /u/Crisps_Beluker   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bmentm/what_kind_of_activation_functions_are_used_in/</guid>
      <pubDate>Sun, 24 Mar 2024 07:02:21 GMT</pubDate>
    </item>
    <item>
      <title>大学人工智能项目帮助</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bmd4aw/college_ai_project_help/</link>
      <description><![CDATA[我正在做一个项目，我们利用神经网络模型来评估描述性答卷。目前我们只有大约 120 篇论文来训练它，即 2 套试卷。这够了吗？ bert 是这个项目的好模型吗？   由   提交/u/Similar-Ranger4226   reddit.com/r/neuralnetworks/comments/1bmd4aw/college_ai_project_help/&quot;&gt;[链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bmd4aw/college_ai_project_help/</guid>
      <pubDate>Sun, 24 Mar 2024 05:20:09 GMT</pubDate>
    </item>
    </channel>
</rss>