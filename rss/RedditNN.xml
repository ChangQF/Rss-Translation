<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络，深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的subreddit。</description>
    <lastBuildDate>Tue, 11 Feb 2025 03:20:57 GMT</lastBuildDate>
    <item>
      <title>PT II：彼得·萨特（Peter Sutor）（访谈）的高维计算（HDC）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1imc73h/pt_ii_hyperdimensional_computing_hdc_with_peter/</link>
      <description><![CDATA[   &lt;A href =“ https://www.reddit.com/r/nealurnetworks/comments/comments/1imc73h/1imc73h/pt_ii_hyperdimensional_computing_hdc_hdc_hdc_hdc_with_peter/” Peter Sutor (Interview)&quot; src=&quot;https://external-preview.redd.it/lx2N8bw5fgUZ7wxweHkDfARaTmwmpwu09YibKeDjiG4.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d62805405d4a1c2550036039821add9e6562f36f&quot; title=&quot;Pt II: Hyperdimensional Computing ( HDC）与Peter Sutor（访谈）“/&gt;   ＆＃32;提交由＆＃32; /u/u/u/neurosymbolic     [link]  ＆＃32;  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1imc73h/pt_ii_hyperdimensional_computing_hdc_with_peter/</guid>
      <pubDate>Mon, 10 Feb 2025 17:50:58 GMT</pubDate>
    </item>
    <item>
      <title>邀请合作者获得可区分的几何损失功能库</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ilzops/inviting_collaborators_for_a_differentiable/</link>
      <description><![CDATA[在一个用于在pytorch中创建可区分几何损失函数库的项目。 我在这里放置了一些初始提交的项目，以了解可能的外观： github repo        &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/atharvaaalok1     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ilzops/inviting_collaborators_for_a_differentiable/</guid>
      <pubDate>Mon, 10 Feb 2025 06:19:02 GMT</pubDate>
    </item>
    <item>
      <title>我在Java实施了整洁的（预言拓扑的神经进化）！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ilpgk3/i_made_an_implementation_of_neat_neuroevolution/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   heya， 我最近在Java中实现了Neat（Adefing Tupologies的Neurovorloute）！我试图使其与原始纸张和源代码一样真实。我看到还没有足够的实现，所以我在Java中完成了它，目前我也在JavaScript版本上工作！   https://github.com/joshuadam/neat-java   任何反馈和批评都非常欢迎！这是我的第一个大型项目之一，我从制作中学到了很多东西，我为此感到非常自豪！ 谢谢你  &lt;！ -  sc_on-&gt;＆＃ 32;提交由＆＃32; /u/u/u/joshua_damian     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ilpgk3/i_made_an_implementation_of_neat_neuroevolution/</guid>
      <pubDate>Sun, 09 Feb 2025 21:16:41 GMT</pubDate>
    </item>
    <item>
      <title>在部署中挣扎：在一日XGBoost预测中处理动态特征的重要性</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iliiwo/struggling_with_deployment_handling_dynamic/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我在训练和测试期间使用XGBoost使用XGBoost使用XGBoost创建一个时间序列预测模型。该模型仅在一天前预测能源使用情况，因为我认为这将是最准确的。我们的培训和测试表明，我非常有希望，我在部署方面挣扎。问题在于，最重要的功能是前几天的使用情况，它可能与第二天相关。由于我几乎每天都使用一个滚动窗口，因此到那天有些独特而超大，但非常擅长预测。在部署期间，我不能具有最新功能的重要性，因为我需要与它相对应的目标，这是我要预测的确切值。因此，我可以将目标转移并每天直到前一天进行训练，并且仍然使用末日功能，但是与培训和测试相比，这最终会很糟糕。例如：我有  1月1日  1月2日 试图预测Jan 3rd（无数据）  Jan 1sts 1sts目标（能源使用）在1月2日非常依赖，因此我们可以在所有数据上进行训练，直到第1个数据，因为它具有可用于计算特征重要性的最佳“增益”的目标。我可以包括1月2日的功能，但不会具有正确的功能。看来我几乎正在尝试预测此时的特征重要性。 这很重要，因为如果前一天的能量使用倒转，第二天的温度会大大下降，没有人会再使用AC来例如，前一天从积极到负相关。  我已经构建了一些K表示模型的聚类，但是即使如此，仍然存在一些差异，如果我试图预测下一个K群集，我只会达到同一问题吗？趋势存在很长一段时间，然后可能会突然下降，下一个K群集将具有不准确的预测。   tldr  如何预测高度可变特征的重要性，这很大程度上依赖于前一天  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/eleastbreath6062     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iliiwo/struggling_with_deployment_handling_dynamic/</guid>
      <pubDate>Sun, 09 Feb 2025 16:27:52 GMT</pubDate>
    </item>
    <item>
      <title>有关选择研究生论文项目的建议</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ild9cz/advice_on_choosing_a_grad_school_dissertation/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嘿，大家建议。我的目标是在机器人技术领域（希望在AI）领域确保一项出色的工作。这是我要考虑的项目的选项： 传感器选项（或）：视觉触觉传感器 算法选项：尖峰图形神经网络（SGNNS）神经体系结构搜索（ nas）尖峰卷积神经网络（SCNNS） 你们认为哪些选择会在我的简历上留下很大的印记，并帮助将来在行业中确保工作？优点和缺点将不胜感激。  谢谢！  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/purpleConversation8     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ild9cz/advice_on_choosing_a_grad_school_dissertation/</guid>
      <pubDate>Sun, 09 Feb 2025 12:02:07 GMT</pubDate>
    </item>
    <item>
      <title>多语言互动可以实现更有效的LLM越狱攻击</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1il94av/multistep_multilingual_interactions_enable_more/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员通过自然对话互动介绍了一种系统的测试LLM安全性方法，证明了简单的对话模式如何可靠地绕过内容过滤。他们没有使用复杂的提示或代币操作，而是表明，通过多转交谈的逐步社会工程达到了很高的成功率。 关键技术要点： - 开发的可再现方法来测试对话越狱 - 针对GPT -4测试，Claude和Llama模型变体 - 绕过安全措施的成功率达到了92％的成功率 - 事实证明，多转向对话更有效 - 创建了有害输出类别的分类法 - 跨多个对话模式和主题的验证结果 结果细分： - 安全搭桥的成功因模型而异（GPT -4：92％，克劳德：88％） - 自然语言模式比显式提示更有效 - 逐渐的操纵表现出比直接请求更高的成功 - 效果持续了多个对话。回合 - 在不同的有害内容类型中，成功率保持稳定 我认为这项工作暴露于当前LLM安全机制中的弱点。这些技术的简单性和可靠性表明，我们需要重新考虑如何实施AI安全护栏。当前的方法似乎容易受到基本社会工程的影响，这可能是有问题的，因为这些模型会看到更广泛的部署。 我认为该方法为系统安全测试提供了宝贵的框架，尽管我担心可能滥用这些发现的可能性。跨领先模型的高成功率表明，这不是特定实现的孤立问题。  tldr：简单的对话技术可以可靠地绕开LLM的安全措施，最高92％的成功率，这表明了AI的当前方法安全需要大量改进。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     fink]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1il94av/multistep_multilingual_interactions_enable_more/</guid>
      <pubDate>Sun, 09 Feb 2025 07:05:21 GMT</pubDate>
    </item>
    <item>
      <title>在没有模型蒸馏的语言模型中，引导性长链经过思考推理</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ikhom6/bootstrap_long_chainofthought_reasoning_in/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  螺栓引入了一种新的方法，可以改善语言模型推理而无需模型蒸馏或其他训练。关键思想是使用自举迭代的思想链，使模型可以通过自我审查和改进来改善自己的推理过程。 关键技术点： - 介绍一个多阶段的推理过程，其中模型是模型生成，审查和完善其自己的思想链 - 使用精心设计的提示来指导模型通过推理改进的不同方面 - 通过结构化的引导方法保持连贯性，该方法可以保留有效的推理，同时纠正错误时 - 与现有模型一起使用，而无需其他培训或从较大模型进行蒸馏 结果： - 与标准的经过想象的提示相比，与模型大小相比，有效地提高了多个推理基准的性能 - 更可靠的推理链 - 更好地处理复杂多步骤问题 我认为这种方法可以改变我们对提高语言模型功能的看法。与其总是需要更大的模型或更多培训，我们也许可以通过巧妙的提示和迭代策略获得更好的性能。引导技术可能可能应用于推理以外的其他类型的任务。 我认为，计算成本和提高性能之间的权衡对于实际应用而言至关重要。螺栓的迭代性质意味着更长的推理时间，但是在不进行重新训练的情况下提高推理的能力可能使许多用例中的值得。  tldr：新方法通过让他们审查和改进他们的语言模型来更好地帮助语言模型推理。自己的思想链推理。无需额外的培训，只需巧妙的提示和迭代即完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ikhom6/bootstrap_long_chainofthought_reasoning_in/</guid>
      <pubDate>Sat, 08 Feb 2025 07:13:15 GMT</pubDate>
    </item>
    <item>
      <title>使用不同的传感器数据频率，可以将Convolutuonal神经网络用于天气预测吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ik80cm/can_convolutuonal_neural_networks_be_used_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  假设有传感器以不同的间隔为1分钟，5分钟15分钟20分钟以不同的间隔为气象输入。可以训练CNN从所有这些传感器中获取数据并预测未来1小时的降雨概率吗？当新数据被用不同的传感器馈送时，它是否能够使概率更加准确？   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/peneaple_throw_105      [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ik80cm/can_convolutuonal_neural_networks_be_used_for/</guid>
      <pubDate>Fri, 07 Feb 2025 22:44:32 GMT</pubDate>
    </item>
    <item>
      <title>基于内容的推荐系统 - 解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ik113q/contentbased_recommender_systems_explained/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/personal-trainer-541      link]  ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ik113q/contentbased_recommender_systems_explained/</guid>
      <pubDate>Fri, 07 Feb 2025 17:51:30 GMT</pubDate>
    </item>
    <item>
      <title>得分流：通过基于连续得分的偏好学习优化LLM代理工作流程</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ijrfu7/scoreflow_optimizing_llm_agent_workflows_through/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文介绍了ScoreFlow，这是一种使用连续优化和定量反馈来优化语言模型代理工作流程的新方法。关键创新是Score-DPO，它扩展了直接偏好优化以处理数值得分，而不仅仅是二进制偏好。 关键的技术方面： - 使用基于得分的梯度-Score-dpo在策略空间中连续优化包含定量反馈的损失函数 - 多代理工作流优化框架 - 基于梯度的学习，用于平滑策略更新 主要结果： - 多种任务类型的基线方法提高了8.2％ - 使用Score Flow的较小模型超过较大的较大模型基线模型 - 有效地回答，编程和数学推理任务 - 在多代理协调方案 中显示了好处 我认为这种方法对于我们需要优化复杂的代理工作流程的实用应用程序可能特别影响。使用定量反馈而不仅仅是二进制偏好的能力打开了更细微的训练信号。对于具有资源限制的部署场景而言，较小的模型可以胜过较大的模型。 我认为，连续优化方法对于代理工作流程很有意义 - 离散优化可以导致生意，无法预测的行为更改。流畅的策略更新应导致更稳定和可靠的代理行为。 我看到的主要限制是，本文没有用大量代理或潜在的不稳定性完全解决具有冲突反馈信号的潜在不稳定性。这些将是跟进工作的重要领域。  tldr：ScoreFlow使用基于连续得分的优化优化LLM代理工作流程，实现比基线更好的性能，同时使较小的型号能够胜过较大的模型。 。  完整摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ijrfu7/scoreflow_optimizing_llm_agent_workflows_through/</guid>
      <pubDate>Fri, 07 Feb 2025 09:47:51 GMT</pubDate>
    </item>
    <item>
      <title>通过Cauchy损失和最佳运输，强大的潜在一致性训练</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iiysw8/robust_latent_consistency_training_via_cauchy/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  潜在一致性模型（LCMS）的新训练方法修改了噪声时间表，以达到更好的图像质量，同时保持LCMS吸引人的快速推理速度。关键创新是在训练过程中引入其他中间步骤，同时保留推理时间的有效抽样过程。 主要技术点： - 修改后的噪声时间表包含训练期间更多的颗粒状步骤 - 动态加权方案调整了不同噪声的重要性级别 - 优化的采样策略平衡质量和速度 - 无需建筑变化或所需的其他参数 - 维护原始4-8步骤推理过程 结果： - 标准图像质量指标的改善15-20％ - 更好地保存精细的细节和纹理 - 与基线​​LCM的可比推理速度 - 在诸如面部之类的复杂功能上的提高性能 - 对多个标准基准测试 我认为这种方法对于质量和速度重要的实际应用可能特别有价值。在推理时间，提高输出质量而无需计算开销的能力表明，我们可能会看到生产系统中采用的这一技术。该方法也可能适用于图像生成以外的其他类型的一致性模型。 我认为关键限制是改进是随着训练复杂性的提高带来的。虽然推理仍然很快，但额外的培训步骤可能会使初始模型开发更加重要。培训期间的修改噪声调度而不是体系结构变化。 完整摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iiysw8/robust_latent_consistency_training_via_cauchy/</guid>
      <pubDate>Thu, 06 Feb 2025 09:36:26 GMT</pubDate>
    </item>
    <item>
      <title>用于改进视力语言的特定于实例的负面挖掘，以促进分割任务的迅速生成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ii6oh9/instancespecific_negative_mining_for_improved/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文引入了一种新方法，以使用实例特定的负面挖掘来改善跨多个任务的基于及时的分割。核心想法是挖掘每个实例特定的负面示例，以学习更好的判别特征。 关键技术点： - 使用两阶段的体系结构：提示生成，然后是负面挖掘 - 来自相似 - 在同一图像中查看实例 - 在没有特定任务培训的情况下学习实例特定的歧视功能 - 与现有的骨干网络（如Sam and Shem）集成 - 使用对比度学习，以最大程度地在正面和负面特征和负面特征 结果中最大化。改进标准基准测试（可可，ADE20K）的基线方法 - 跨多个任务的工作 - 显示更好地处理相似实例和重叠对象 - 尽管采矿步骤进行了额外的采矿步骤 - 在基于及时的细分任务上达到SOTA，但仍保持竞争性推理速度 我认为，这种方法对于我们需要可以处理多个任务的灵活分割系统的现实应用程序可能会产生影响。特定于实例的负面挖掘似乎是一种自然的方法，可以帮助模型学习更多强大的功能，尤其是在具有相似对象的情况下。在部署方案中，它在没有特定任务的培训的情况下起作用的事实特别有趣。 我看到的主要限制是采矿过程中的计算开销，尽管作者报告了影响易于管理。我很想知道如何使用许多类似对象的对象来扩展到非常大的场景。  tldr：使用实例特定的负面挖掘的新实例分割方法，可以提高未经特定于任务培训的多个任务的准确性。通过学习的判别特征可以更好地处理类似对象。  完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]  ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ii6oh9/instancespecific_negative_mining_for_improved/</guid>
      <pubDate>Wed, 05 Feb 2025 09:57:06 GMT</pubDate>
    </item>
    <item>
      <title>凸优化理论预测了大语言模型的最佳学习率时间表</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ihgfao/convex_optimization_theory_predicts_optimal/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文在现代深度学习中使用的经典凸优化理论与经验成功的学习率计划之间建立了关键联系。研究人员得出了数学证据，表明余弦学习率衰减自然来自优化范围。 主要技术要点： - 开发的理论框架与深度学习计划连接经典优化 - 证明余弦衰减时间表可最大程度地减少convex的融合界限问题 - 显示线性热身通过优化镜头具有理论上的理由 - 在ImageNet，语言模型和其他标准基准上进行了验证的结果 - 发现使用理论上最佳的时间表 发现最终模型性能提高了10-15％为主要通过反复试验开发的实践提供了宝贵的数学基础。虽然分析侧重于凸病例，但与经验结果的一致性表明，见解很好地转移到了深度学习中。这些证明可以帮助开发更好的自动调度方法。 我认为可以扩展框架以分析其他训练组件，例如动量和重量衰减。与经典优化理论的联系为利用数十年的理论工作开辟了可能性。  tldr：研究证明了流行的学习速率时间表（余弦衰减，线性热身）在理论上是凸优化的最佳选择，与经验的发现相匹配。结果验证了当前的实践并为改进培训方法提供了基础。 &gt;完整的摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     link]  ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ihgfao/convex_optimization_theory_predicts_optimal/</guid>
      <pubDate>Tue, 04 Feb 2025 12:22:37 GMT</pubDate>
    </item>
    <item>
      <title>彼得·萨特（Peter Sutor）第1部分（访谈）的高维计算（HDC）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1igtag7/hyperdimensional_computing_hdc_with_peter_sutor/</link>
      <description><![CDATA[   &lt;A href =“ https://www.reddit.com/r/nealurnetworks/comments/comments/1igtag7/hyperdimensional_computing_hdc_with_with_with_peter_peter_sutor_sutor_sutor_sutor/” 1 (Interview)&quot; src=&quot;https://external-preview.redd.it/4BIXZ9D2AObAHxxh29oFMhMercVoKSpeXfUs7Tw4rQ8.jpg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=386e944cf63091dcf8f48a1bc874cdd82fec00f9&quot; title=&quot;Hyperdimensional Computing (HDC) with Peter辅助第1部分（访谈）“/&gt;   ＆＃32;提交由＆＃32; /u/u/u/neurosymbolic     [link]   ＆＃32;   [注释] /table&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1igtag7/hyperdimensional_computing_hdc_with_peter_sutor/</guid>
      <pubDate>Mon, 03 Feb 2025 16:37:26 GMT</pubDate>
    </item>
    <item>
      <title>计算隐藏层的批处理规范</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1igiluz/calculating_batch_norm_for_hidden_layers/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我试图理解隐藏层的批处理规范的详细信息。我知道，对于给定的神经元，例如x  l ，我们需要在所有迷你批次样本上计算平均值和差异，以在将其激活中进行标准化，然后再将其喂入下一层。&lt; 我想了解上述计算是如何完成的。一种方法可能是处理迷你批次的每个元素并在L层中收集神经元的统计数据，并忽略后续层。一旦计算了L层中所有元素的均值和差异，请再次处理L+1层的小批量元素，依此类推。这似乎很浪费。这是正确的吗？ 如果没有，请分享所执行的确切计算的描述。我混淆的根源是，L层中的标准化会影响为L+1层的值。因此，除非我们知道L层L层的均值和差异，否则我们如何将下一层标准化。预先感谢您。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/far-cantaloupe4144      [link]  ＆＃32;   [commist]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1igiluz/calculating_batch_norm_for_hidden_layers/</guid>
      <pubDate>Mon, 03 Feb 2025 06:09:34 GMT</pubDate>
    </item>
    </channel>
</rss>