<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络，深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的subreddit。</description>
    <lastBuildDate>Fri, 28 Mar 2025 12:35:10 GMT</lastBuildDate>
    <item>
      <title>从文章长度文本中布局引导的商业信息图表</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jlsrhb/layoutguided_generation_of_business_infographics/</link>
      <description><![CDATA[BizGen presents an impressive approach to generating infographics from full articles through a three-level text understanding architecture and a specialized Visual Text Rendering (VTR) component. The key technical contributions include:  Three-level text understanding that processes content at article, section, and sentence levels同时   bizvtr（视觉文本渲染）组件专门设计用于处理信息图表中的排行榜挑战     26K配对数据文章的培训和专业设计的培训和评估的信息图表           使Bizgen与众不同的是其保持层次信息连贯性的能力，同时将复杂的文章转换为视觉上吸引人的信息图表。 Previous approaches typically worked only at the sentence level, but BizGen&#39;s multi-level approach preserves the logical structure of the original content. Results show: * Significant improvements over existing methods in both automatic metrics and human evaluations * The BizVTR component provides the most substantial improvement in visual quality * Ablation studies confirm each component&#39;s contribution to overall performance I think this work could be particularly impactful for content creators and没有专用设计资源的企业。从现有内容中自动产生高质量的信息图表的能力可以显着降低创建有效的视觉通信的障碍。  我对这种方法如何扩展到业务内容以外的其他领域特别感兴趣。科学论文，教育材料和新闻文章都可以从保持信息完整性的同时增强视觉吸引力的自动可视化工具中受益。   说，我对计算要求以及它处理非常技术内容的能力感到好奇。本文提到了一些非常长或技术文章的局限性。  tldr：Bizgen引入了三级文本理解方法和专业的视觉文本渲染，以产生完整文章的高质量信息图表，并显着胜过以前的方法。   完整的摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jlsrhb/layoutguided_generation_of_business_infographics/</guid>
      <pubDate>Fri, 28 Mar 2025 11:21:52 GMT</pubDate>
    </item>
    <item>
      <title>关于神经形态计算和深度学习的书籍建议</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jl14oo/book_recommendations_for_neuromorphic_computing/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好， 我希望进入神经形态计算，尤其是基于AFMTJ的系统及其与尖峰神经网络（SNNS）的关系。我是一名软件工程师，但是我在AI和机器学习方面的背景为零。 由于我是该领域的新手，所以我知道我需要从基础知识开始。因此，我希望找到一些初学者友好的书籍和资源，这些书籍和资源可以帮助我在潜入神经形态计算，基于MTJ的系统和AFMTJS。 之前，都非常感谢！     &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/express_count9489     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jl14oo/book_recommendations_for_neuromorphic_computing/</guid>
      <pubDate>Thu, 27 Mar 2025 10:52:14 GMT</pubDate>
    </item>
    <item>
      <title>带有视觉模型的标签传播，用于零拍语义分段</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jl0wjg/label_propagation_with_vision_models_for_zeroshot/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在研究新的LPOSS体系结构，该体系结构可以处理开放式摄影量的语义细分，而无需进行额外的培训。该方法利用现有的视觉语言模型并通过巧妙的标签传播技术来增强其细分功能。 该方法通过：  使用 patch and pixel cats and Pixel level&gt;使用标签传播 以完善分割蒙版 &lt;强&gt; &lt;强&gt; &lt;强&gt; &lt;强&gt; VM（VM）将VLM本身用于此任务） 处理整个图像同时，而不是使用可能错过全局上下文的基于窗口的方法 而无需在细分数据集上进行任何额外的培训    使用patch patch patch patch plate prection * clip flip with a vallevel grount a vlive the vlive（ *在类似斑块上传播标签以完善初始预测 *在像素级别上进一步完善，以提高边界精度 *，同时保持从基本VLM  继承的开放式摄影能力 我认为，这种方法标志着在不需要专业培训的情况下使高级计算机视觉能力更易于访问。通过仅审计的模型进行高质量分割的能力在稀缺的注释分段数据中可能特别有价值。 对我来说，对我而言引人入胜的是他们如何识别和解决VLM的限制，即VLMS已针对交叉模态校准而不是模式内的相似性进行了优化。关于使用单独的视觉模型进行斑块相似性测量的这种见解似乎是显而易见的，但在其结果中产生了重大影响。      tldr ：lposs+在无训练方法中实现无训练的方法，以在不需要开放式语义序列的情况下使用两阶段的传播vlms vlms的训练方法，以实现训练方法。培训。  完整摘要在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jl0wjg/label_propagation_with_vision_models_for_zeroshot/</guid>
      <pubDate>Thu, 27 Mar 2025 10:36:40 GMT</pubDate>
    </item>
    <item>
      <title>LLM代理通过在共享预印型服务器上的协作研究实现更好的性能</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jk9hki/llm_agents_achieve_better_performance_through/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   AgentRxiv介绍了一个自主科学研究的框架，使用25种专业的LLM通过定义的角色和沟通协议进行协作，以生成完整的研究论文，而无需人工干预，以生成完整的研究论文。   主要的技术方面： *多人技术研究： *多人研究工作，研究五个研究，研究范围（研究五个研究领域）。构想，计划，实验，分析，写作 *标准化的消息系统实现协作决策 *基于Python的实施工具，用于编码，调试和执行实验 *代理专业化 *允许在系统 之间进行专业知识分布 该系统已证明了遵循四个完整的研究论文（ *独立的研究），这是一个完整的研究（成功的科学范围）（成功的科学标准），归一化改善神经网络培训） *生成和评估多种研究方法，以选择有希望的方向 *通过调试和适应机制来处理故障 *进行计算实验，并通过代码生成和执行 进行计算实验 ，我认为这代表了AI研究助理的重要一步，可以通过标准化的实验性程序来解决科学危机的AI研究助理。重复已知发现和提出新方向的能力可以加速某些类型的研究，尤其是在计算领域。 我认为主要局限性很明显：这些系统受到知识截止，缺乏物理实验室的能力的限制，并且还没有证明他们能够使他们能够真正发展出促进科学领域的新颖发现。关于研究伦理，偏见和适当归因的重要问题，需要进一步探索。  tldr：AgentRxiv展示了一个多机构系统，其中25个具有专业角色的LLM代理人自主，成功地生产完整的论文并复制已知科学发现。这表明了加速研究的希望，尽管仍然存在新颖性和物理实验的局限性。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jk9hki/llm_agents_achieve_better_performance_through/</guid>
      <pubDate>Wed, 26 Mar 2025 11:26:29 GMT</pubDate>
    </item>
    <item>
      <title>您知道哪些好YouTube博客作者是谁拍摄有关神经网络的拍摄？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jjpswu/what_good_youtube_bloggers_do_you_know_who_shoot/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   youtube Blogger什么好，您知道谁拍摄了有关神经网络的拍摄？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/aphandered-fix-636     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jjpswu/what_good_youtube_bloggers_do_you_know_who_shoot/</guid>
      <pubDate>Tue, 25 Mar 2025 18:02:14 GMT</pubDate>
    </item>
    <item>
      <title>如何在2D Platformer中修复网络？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jj4wpo/how_to_fix_network_in_2d_platformer/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  我正在尝试创建一个可以完成简单平台级别的神经网络，但是由于错误系统，它只是直接朝目标迈进，即使它们是走得更远的路，也拒绝走其他方式。有什么方法可以调整错误或突变值以使其更多地探索？还是我只需要更耐心？  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/u/helicopterfun9030       [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jj4wpo/how_to_fix_network_in_2d_platformer/</guid>
      <pubDate>Mon, 24 Mar 2025 23:08:46 GMT</pubDate>
    </item>
    <item>
      <title>刚刚与Langchain和Ollama建立了一个交互式AI驱动的Crewai文档助理</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jispjt/just_built_an_interactive_aipowered_crewai/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/u/malefent-penalty50      [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jispjt/just_built_an_interactive_aipowered_crewai/</guid>
      <pubDate>Mon, 24 Mar 2025 15:00:40 GMT</pubDate>
    </item>
    <item>
      <title>探索神经：神经网络的下一代DSL和调试解决方案</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jhsazw/explore_neural_the_nextgeneration_dsl_and/</link>
      <description><![CDATA[＆＃32;提交由＆＃32;态href =“ https://neurallang.hashnode.dev/explore-neural-neural-next-next-gener-dsl-and-and-debugging-solution-solution-for-neural-networks”&gt; [link]    ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jhsazw/explore_neural_the_nextgeneration_dsl_and/</guid>
      <pubDate>Sun, 23 Mar 2025 05:35:32 GMT</pubDate>
    </item>
    <item>
      <title>基于DIT的身份保留的图像生成和多阶段训练</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jh32q3/ditbased_identitypreserved_image_generation_with/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;       控制身份保存 我一直在研究这种用于个性化图像生成的新方法，该方法可以解决基本的折衷：保持基本的折衷：维持同时允许灵活的编辑。在扩散过程中，它专门隔离并保留身份特征。 This allows the model to maintain a person&#39;s likeness across different scenarios, styles, and contexts. Main technical points: * Works with just 3-5 reference photos of a person * Modifies the cross-attention mechanism of diffusion models to give higher weight to identity-related features * Creates specialized identity tokens that capture appearance essence * Implements a zero-shot approach that requires no per-person fine-tuning * Demonstrated quantitatively superior identity preservation compared to DreamBooth, Custom Diffusion, and IP-Adapter The results are quite strong in several dimensions: * Maintains identity across different ages, expressions, lighting conditions * Preserves identity even with significant background and style changes * Achieves higher CLIP-based identity similarity scores than previous methods * Performs well on challenging scenarios like unusual poses or dramatic lighting I think this approach could对个性化内容创建具有变革性。零拍的性质使其对于从虚拟尝试到个性化营销的应用程序立即实用。在没有专门培训的情况下维持身份的能力消除了采用的主要障碍。 我特别感兴趣的是他们如何从编辑问题中分解了身份保存问题 - 以前的方法正在努力。这种对注意机制的模块化方法可能会应用于我们需要维持某些属性同时允许其他人变化的其他领域。 极端姿势和偶尔的伪像的局限性表明仍有工作要做，但基本方法似乎很合理。我很好奇这可能如何扩展到视频生成或实时应用程序。   tldr ：InfiniteYou引入了身份增强的跨注意，该跨意识可以保留一个人在生成的图像中的外观，同时允许灵活编辑。它胜过现有方法，而无需每人培训，并且仅从几张参考照片中工作。  full&#39;&gt; full ofter ofer ofer Shore in there   。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jh32q3/ditbased_identitypreserved_image_generation_with/</guid>
      <pubDate>Sat, 22 Mar 2025 07:26:42 GMT</pubDate>
    </item>
    <item>
      <title>非常需要建议！完整的初学者。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jgp1ln/advice_greatly_needed_complete_beginner/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  嗨！我是一名A级计算机科学专业的学生，​​对于所有不知道这是英国通常是16-18岁的年龄范围的人。我必须在最后一年级的20％进行编码项目，并且我正在考虑创建一个神经网络，我需要一些建议。 首先，我应该使用什么语言？我具有基本的JavaScript编码技能，但主要用于使用P5 Play Library的游戏，因为这是我们老师想要做的。对我来说，另一个选择是Python，因为我已经读到，这通常是最常见的初学者友好选择。  其次，我需要有什么硬件的明智之举，我知道最低限度是16GB RAM和GPU，但是有什么具体的吗？我想训练的模型是为了虹膜识别，该系统几乎像指纹ID一样工作。 第三，您的个人建议是什么？我感谢所有评论，建议和意见，因此请随时说出想到的一切！ 感谢您阅读所有这些内容，祝您有美好的一天！   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/code-block     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jgp1ln/advice_greatly_needed_complete_beginner/</guid>
      <pubDate>Fri, 21 Mar 2025 19:19:21 GMT</pubDate>
    </item>
    <item>
      <title>AI在浏览器中学习脆弱的鸟：整洁的算法</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jgkn0e/ai_learns_flappy_bird_in_the_browser_neat/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/joshuaamdamian      [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jgkn0e/ai_learns_flappy_bird_in_the_browser_neat/</guid>
      <pubDate>Fri, 21 Mar 2025 16:16:38 GMT</pubDate>
    </item>
    <item>
      <title>通量流：通过时间扩展改善视频生成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jgf7v8/fluxflow_improving_video_generation_through/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我一直在探索视频扩散模型的时间正则化，而且令人惊讶地简单而有效。这种方法在推理过程中实现帧之间的一致性，而无需进行任何模型进行训练或架构变化。 关键见解是在deno的过程中连续框架之间的约束以确保自然运动模式，从而确保自然运动模式，从而大大减少闪烁和抖动，从而通过许多当前的视频生成模型来延伸。 during the denoising process that penalizes large changes between consecutive frames * Compatible with both 2D diffusion models (generating all frames simultaneously) and 3D diffusion models (with built-in temporal dimensions) * No model retraining required - applies during the inference process only * Achieves 13.2% improvement on UCF-101 and 18.2% on SkyTimelapse datasets * Most effective when applied during middle denoising steps * Includes an可调节的正则强度参数以平衡时间一致性与多样性 我认为这是我们如何进行视频生成改进的重要转变。与其不断追求新的体系结构或广泛的重新培训，不如关注目标领域（时间连贯性）的基本特性会带来可观的好处。实施的简单性意味着，研究人员和开发人员可以立即采用与现有视频生成模型合作的开发人员。 本文中强调的一致性和多样性之间的权衡特别有趣 - 过多的正则化可能导致“运动冷冻”，”虽然太少并不能解决闪烁的问题。    tldr：在推断过程中添加时间正则化显着提高了视频的产生质量而无需模型再培训，因此发现该最佳点对于不同的应用而言至关重要。它在不同的模型体系结构上工作，并始终减少闪烁/抖动的同时保持内容富裕。   full ofere 。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jgf7v8/fluxflow_improving_video_generation_through/</guid>
      <pubDate>Fri, 21 Mar 2025 12:04:05 GMT</pubDate>
    </item>
    <item>
      <title>DeepMesh：高质量自动回归3D网格的增强学习</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jfmuhu/deepmesh_reinforcement_learning_for_highquality/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;   deepMesh使用具有自动回归过程的强化学习引入了一种新颖的3D网格生成方法。与现有在一次拍摄或使用隐式表示形式中产生网格的方法不同，DeepMesh一次通过一次添加一张面孔来构建网络构建。 -  图形神经网络编码器使用图像或文本提示中的剪辑嵌入 -  多模式调节期间的架拓扑 -  多模式调节  -  三相训练：模仿从艺术家网格，RL优化的                   &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;&gt; &lt;更好的形状质量 - 在多种质量指标上胜过火星和Edgerunner-创建具有更均匀的三角形分布的网格，使其更适合动画 - 可以有效地使用单视图像和文本到3D生成任务 我认为，我认为这种方法可以解决AI生成3D内容之间的基本脱节。当前的方法通常会产生在生产管道中可用之前需要大量清理的网格。通过学习与三角效率的面对面构建网格，DeepMesh可以大大减少3D艺术家的后处理时间。 我认为最大的影响可能是对游戏开发和动画的影响，在这种开发和动画中，有效的网眼构造直接影响性能。这最终可以使资产创造更快，同时保持这些行业所需的质量标准。文本到3D功能还暗示了从概念描述中快速进行原型制作的潜力。 也就是说，当前使用复杂结构（如脸和手）的局限性意味着这不会很快替代角色艺术家。顺序生成过程还可能对实时应用提出绩效挑战。  tldr：DeepMesh使用强化学习，像人类艺术家一样，一次构建一个面孔，导致三角形的高质量模型比以前的方法少43％。与图像和文本输入一起使用。  在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jfmuhu/deepmesh_reinforcement_learning_for_highquality/</guid>
      <pubDate>Thu, 20 Mar 2025 11:37:41 GMT</pubDate>
    </item>
    <item>
      <title>100个在单个GPU上运行的神经AMP建模器音频插件的实例</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jfkx9q/100_instances_of_the_neural_amp_modeler_audio/</link>
      <description><![CDATA[    src =“ https://external-preview.redd.it/diihsiw4wjhcig6lzoscjjjfbwhe4ltct7yi56cjigqw.jpg？宽度= 320＆amp; crop = smart＆amp; auto = webp＆amp; s = B9911831F18D840DBF555555555CA3919F2633AD73CBC7“ title =“在单个GPU上运行的神经AMP建模器音频插件的100个实例”/&gt;   ＆＃32;提交由＆＃32; /u/u/midierror      [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jfkx9q/100_instances_of_the_neural_amp_modeler_audio/</guid>
      <pubDate>Thu, 20 Mar 2025 09:26:26 GMT</pubDate>
    </item>
    <item>
      <title>通过混合AI的元认知的概率基础</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1jf69th/probabilistic_foundations_of_metacognition_via/</link>
      <description><![CDATA[      ＆＃32;提交由＆＃32; /u/u/u/neurosymbolic      [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1jf69th/probabilistic_foundations_of_metacognition_via/</guid>
      <pubDate>Wed, 19 Mar 2025 20:01:48 GMT</pubDate>
    </item>
    </channel>
</rss>