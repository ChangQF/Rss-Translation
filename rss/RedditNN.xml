<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Sun, 28 Jan 2024 03:13:31 GMT</lastBuildDate>
    <item>
      <title>计算可比较的嵌入：两座塔、连体网络和三元组损失</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1abg6q1/compute_comparable_embeddings_two_towers_siamese/</link>
      <description><![CDATA[       由   提交/u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1abg6q1/compute_comparable_embeddings_two_towers_siamese/</guid>
      <pubDate>Fri, 26 Jan 2024 11:26:28 GMT</pubDate>
    </item>
    <item>
      <title>YOLO 揭秘：清晰指南</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19fip2c/yolo_unraveled_a_clear_guide/</link>
      <description><![CDATA[   ​ 此处 &lt; !-- SC_ON --&gt;  由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/neuralnetworks/comments/19fip2c/yolo_unraveled_a_clear_guide/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19fip2c/yolo_unraveled_a_clear_guide/</guid>
      <pubDate>Thu, 25 Jan 2024 20:16:48 GMT</pubDate>
    </item>
    <item>
      <title>我这几天发表的每一篇博文在谷歌上的排名都在第五位以内。这一切都归功于 Junia.ai 的博客文章工作流程。以下是自动链接的预览，它将进一步提高您网站的搜索引擎优化：</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19exhkp/every_one_of_the_blog_post_i_published_in_the/</link>
      <description><![CDATA[       由   提交/u/Lunaopty  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19exhkp/every_one_of_the_blog_post_i_published_in_the/</guid>
      <pubDate>Thu, 25 Jan 2024 01:34:22 GMT</pubDate>
    </item>
    <item>
      <title>与我的人工智能伙伴 Synthia 就神经网络的复杂性进行了一场精彩的对话。 🤖✨ 查看我最新文章中的见解和问答环节。让我们一起来揭开AI的奥秘吧！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19dgjxy/engaging_in_a_fascinating_conversation_with/</link>
      <description><![CDATA[   本文采用了一种独特的方法，通过想象的神经网络进行问答环节网络。我们不会通过传统的视角深入研究错综复杂的技术，而是将神经网络拟人化，邀请它阐明其内部工作原理，揭开其决策过程的神秘面纱，并揭示其存在的细微差别。通过这种富有想象力的对话，我们的目标是以一种令人耳目一新的独特方式揭开神经网络的秘密，为读者提供一个深入而平易近人的视角来了解人工智能的迷人世界。 &lt;!-- SC_ON - -&gt;  由   提交/u/ardesai1907  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19dgjxy/engaging_in_a_fascinating_conversation_with/</guid>
      <pubDate>Tue, 23 Jan 2024 05:00:15 GMT</pubDate>
    </item>
    <item>
      <title>突然validation_loss下降到零</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19deubj/suddenly_validation_loss_drops_to_zero/</link>
      <description><![CDATA[      有人见过这样的val_dice曲线吗？ 真的不合理，max_epoch=100learning_reate=8e-4，不涉及lr_scheduler。 除了验证，训练过程也是这样，train_loss突然激增。 大家有什么想法或者建议吗？谢谢大家。 https://preview.redd.it/4nl5qakly3ec1.png?width=576&amp;format=png&amp;auto=webp&amp;s=43307a87e91072394dcc369b1dbe2f2308fdad7c ​ &lt; p&gt;https://preview.redd.it/7wbvlnu3z3ec1。 png?width=567&amp;format=png&amp;auto=webp&amp;s=b93e45020116da1dd26140559796e7abeda79346   由   提交/u/No-Supermarket-2567   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19deubj/suddenly_validation_loss_drops_to_zero/</guid>
      <pubDate>Tue, 23 Jan 2024 03:27:05 GMT</pubDate>
    </item>
    <item>
      <title>采访麻省理工学院林肯实验室的 Zack Serlin：正式方法......</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</guid>
      <pubDate>Sat, 20 Jan 2024 20:00:19 GMT</pubDate>
    </item>
    <item>
      <title>人工神经网络中的类脑学习：综述</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.11252 摘要：  人工神经网络（ANN）已成为以下领域的重要工具：机器学习，在图像和语音生成、游戏和机器人等多个领域取得了显着的成功。然而，人工神经网络的运行机制与生物大脑的运行机制之间存在根本差异，特别是在学习过程方面。本文对当前人工神经网络中的类脑学习表示进行了全面回顾。我们研究了更多生物学上合理的机制的整合，例如突触可塑性，以增强这些网络的能力。此外，我们深入研究了这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究找到了有希望的途径，这可以让我们更接近理解智能的本质。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</guid>
      <pubDate>Fri, 19 Jan 2024 16:03:08 GMT</pubDate>
    </item>
    <item>
      <title>温度、Top-k 和 Top-p 解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19ahr5a/temperature_topk_and_topp_explained/</link>
      <description><![CDATA[   /u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19ahr5a/temperature_topk_and_topp_explained/</guid>
      <pubDate>Fri, 19 Jan 2024 12:14:04 GMT</pubDate>
    </item>
    <item>
      <title>全连接层。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1997h71/full_connected_layers/</link>
      <description><![CDATA[我正在尝试学习神经网络，但目前不了解很多数学知识，所以我有一个问题要问那些知道自己的知识的人关于.我在 python 中使用的神经网络是完全连接的，显然这很好，但它违背了我的直觉，也违背了我认为神经网络的好处。当然，拥有许多不同类型的连接可以存储更复杂的信息。   由   提交/u/Unlucky_Culture_6996   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1997h71/full_connected_layers/</guid>
      <pubDate>Wed, 17 Jan 2024 20:55:24 GMT</pubDate>
    </item>
    <item>
      <title>输入复杂度和所需的隐藏层之间到底有什么关系？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1995sxc/what_exactly_is_the_relationship_between_input/</link>
      <description><![CDATA[ 由   提交/u/swampshark19  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1995sxc/what_exactly_is_the_relationship_between_input/</guid>
      <pubDate>Wed, 17 Jan 2024 19:49:17 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 闪电模型只需很少的代码行即可完成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/</link>
      <description><![CDATA[      我刚刚建立了一个Python库来帮助构建PyTorch闪电模型只需很少的代码行。我很想听听您的想法！ https://github.com/brianrisk/lightning_factory&lt; /p&gt; 闪电工厂概览&lt; /p&gt; ​ ​   由   提交/u/qwaver-io  /u/qwaver-io  reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/</guid>
      <pubDate>Wed, 17 Jan 2024 16:30:56 GMT</pubDate>
    </item>
    <item>
      <title>大脑连接性突破：在不同物种中发现相似的神经网络模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/198zhok/brain_connectivity_breakthrough_similar_neural/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/198zhok/brain_connectivity_breakthrough_similar_neural/</guid>
      <pubDate>Wed, 17 Jan 2024 15:42:08 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/198dh3l/transformers_are_multistate_rnns/</link>
      <description><![CDATA[ 由   提交/u/nickb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/198dh3l/transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 20:48:21 GMT</pubDate>
    </item>
    <item>
      <title>人工智能和计算机视觉解决方案预算实用指南 |第 1 部分 硬件</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/</link>
      <description><![CDATA[      ​&lt; /p&gt; https://preview.redd。 it/efs28iav6ucc1.jpg?width=2800&amp;format=pjpg&amp;auto=webp&amp;s=0ae07562cdd592cbb203f11df5ccbd78abf83213 关于计算机视觉定价的好文章。希望您能顺利找到它。 简短描述： 2024 年，随着越来越多的公司融入人工智能，许多企业主面临着挑战。借助 OpenCV.ai 的专家见解，了解将 AI 集成到您的业务中的基本注意事项。从为计算机视觉解决方案选择合适的相机到导航不同的计算平台，文章提供了实用指导。探索网络和电源优化的细微差别，迈出人工智能驱动成功的第一步。在本系列文章中，我们将指导您了解从硬件和软件选择到人工智能法律方面的所有要点。让我们从第 1 部分开始 |硬件。   由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/</guid>
      <pubDate>Tue, 16 Jan 2024 17:26:04 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1986kc1/transformers_are_multistate_rnns/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.06104 代码：https ://github.com/schwartz-lab-NLP/TOVA 摘要：  Transformers 被认为在概念上与到上一代最先进的 NLP 模型 - 循环神经网络 (RNN)。在这项工作中，我们证明了仅解码器 Transformer 实际上可以被概念化为无限多状态 RNN——一种具有无限隐藏状态大小的 RNN 变体。我们进一步证明，通过固定隐藏状态的大小，预训练的 Transformer 可以转换为有限多状态 RNN。我们观察到一些现有的转换器缓存压缩技术可以被构建为这样的转换策略，并引入了一种新的策略，TOVA，它比这些策略更简单。我们对多个远程任务进行的实验表明，TOVA 优于所有其他基线策略，同时几乎与完整（无限）模型相当，并且在某些情况下仅使用原始缓存大小的 1/8。我们的结果表明，变压器解码器 LLM 在实践中通常表现为 RNN。他们还提出了缓解最痛苦的计算瓶颈之一——缓存大小的选项。我们在 此 https URL 公开发布我们的代码。    [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1986kc1/transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 16:12:45 GMT</pubDate>
    </item>
    </channel>
</rss>