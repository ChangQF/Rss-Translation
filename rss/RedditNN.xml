<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Sun, 07 Jan 2024 01:06:01 GMT</lastBuildDate>
    <item>
      <title>我从头开始制作了教育自动毕业</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18z7dbg/i_made_an_educational_autograd_from_scratch/</link>
      <description><![CDATA[学习机器学习，我一直对 PyTorch 及其 Autograd 引擎感兴趣。&lt; /p&gt; 在这个项目中，我尝试重新实现 PyTorch 的大部分内容（包括 Autograd）以记录完善、经过单元测试且可解释的方式从头开始。它对我来说非常有用，我希望它也能帮助您更好地了解 Autograd！  希望您喜欢！  GitHub 存储库此处！   由   提交 /u/suspicious_beam   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18z7dbg/i_made_an_educational_autograd_from_scratch/</guid>
      <pubDate>Fri, 05 Jan 2024 13:52:28 GMT</pubDate>
    </item>
    <item>
      <title>我用 Python 创建了一个神经网络，可以在虚幻引擎中按程序生成这些关卡。最终图像是我创建的，并提供给神经网络进行学习:]</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18z1fob/i_created_a_neural_network_in_python_that/</link>
      <description><![CDATA[       由   提交/u/atomiclollypop  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18z1fob/i_created_a_neural_network_in_python_that/</guid>
      <pubDate>Fri, 05 Jan 2024 07:48:48 GMT</pubDate>
    </item>
    <item>
      <title>2024 年 Neuro Symbolic 频道值得关注的 5 件事</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18yg3xz/5_things_to_watch_for_in_2024_on_the_neuro/</link>
      <description><![CDATA[   /u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18yg3xz/5_things_to_watch_for_in_2024_on_the_neuro/</guid>
      <pubDate>Thu, 04 Jan 2024 15:44:04 GMT</pubDate>
    </item>
    <item>
      <title>随机变压器：通过揭开变压器背后的所有数学原理来了解变压器的工作原理</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18y28q4/the_random_transformer_understand_how/</link>
      <description><![CDATA[       由   提交/u/nickb  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18y28q4/the_random_transformer_understand_how/</guid>
      <pubDate>Thu, 04 Jan 2024 02:50:59 GMT</pubDate>
    </item>
    <item>
      <title>帮助提高模型效率</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18xwheq/help_with_model_efficiency_improvement/</link>
      <description><![CDATA[        由   提交/u/Gold-Ad4040   /u/Gold-Ad4040  reddit.com/r/neuralnetworks/comments/18xwheq/help_with_model_efficiency_improvement/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18xwheq/help_with_model_efficiency_improvement/</guid>
      <pubDate>Wed, 03 Jan 2024 22:42:38 GMT</pubDate>
    </item>
    <item>
      <title>关于人工神经网络的博客</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18xbxfj/blog_on_artificial_neural_network/</link>
      <description><![CDATA[      https://bhargavoza.com/blogs/Artificial%20Neural%20Network  https://preview.redd.it /zwvcphhqy5ac1.png?width=1007&amp;format=png&amp;auto=webp&amp;s=ea7cd84820900c92b298e26a8e5308faa24852d2 嘿，我在人工神经网络上写了这篇博客。它是如何工作的以及它背后的每个数学方程。另外，我在 numpy 的帮助下用 Python 从头开始​​开发了一个完整的训练周期。 我恳请您访问我的博客并提供一些反馈。   由   提交/u/Troniq777  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18xbxfj/blog_on_artificial_neural_network/</guid>
      <pubDate>Wed, 03 Jan 2024 05:49:31 GMT</pubDate>
    </item>
    <item>
      <title>预测器（回归）性能面临的挑战：MAE 持续为 0.26 且二元向量预测不准确</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18xbg5b/challenges_with_predictor_regression_performance/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18xbg5b/challenges_with_predictor_regression_performance/</guid>
      <pubDate>Wed, 03 Jan 2024 05:23:37 GMT</pubDate>
    </item>
    <item>
      <title>多头/多查询/分组查询注意力解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18wlxga/multiheadmultiquerygroupedquery_attentions/</link>
      <description><![CDATA[您好， 我创建了一个视频 这里我解释了多头注意力（MHA）、多查询注意力（MQA）和分组查询注意力（GQA）如何工作，以及使用它们中的每一种的优缺点&lt; /p&gt; 我希望它对你们中的一些人有用。非常欢迎反馈！ :)   由   提交/u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18wlxga/multiheadmultiquerygroupedquery_attentions/</guid>
      <pubDate>Tue, 02 Jan 2024 10:00:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么大家都说单层感知器无法解决异或问题？那这个呢？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18v926p/why_is_everybody_saying_singlelayer_perceptron/</link>
      <description><![CDATA[def activate_function(number): if number%2: return 1 return 0 权重 = [1, 1] for x in range(2 ): for y in range(2): print(f&quot;{x}, {y} = {activation_function(weights[0] * x + Weights[1] * y)}&quot;)  输出： 0, 0 = 0 | 0, 1 = 1 | 1, 0 = 1 | 1, 1 = 0 |   由   提交/u/jaroslavtavgen  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18v926p/why_is_everybody_saying_singlelayer_perceptron/</guid>
      <pubDate>Sun, 31 Dec 2023 14:40:10 GMT</pubDate>
    </item>
    <item>
      <title>未来的计算机将截然不同（模拟计算）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18uku2k/future_computers_will_be_radically_different/</link>
      <description><![CDATA[       由   提交/u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18uku2k/future_computers_will_be_radically_different/</guid>
      <pubDate>Sat, 30 Dec 2023 17:32:58 GMT</pubDate>
    </item>
    <item>
      <title>《注意力》、《变形金刚》、神经网络《大语言模型》</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18qexbz/attention_transformers_in_neural_network_large/</link>
      <description><![CDATA[ 由   提交/u/nickb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18qexbz/attention_transformers_in_neural_network_large/</guid>
      <pubDate>Mon, 25 Dec 2023 08:47:41 GMT</pubDate>
    </item>
    <item>
      <title>带洗牌标签的训练的意义。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n8ppb/significance_of_training_with_shuffled_labels/</link>
      <description><![CDATA[我刚刚观看了 3Blue1Brown 的这段视频，其中他提到了 Lisha Li 的研究，她使用随机排列的数据集标签来训练网络。他说，这是为了识别“最小化成本函数是否对应于图像中的任何结构，或者只是记忆”。 （正如 Lisha 所说，“记住正确分类的整个数据集”） https://youtu.be/IHZwWFHWa-w?si=aDGIG1zVMHtFlYk7&amp;t=1064 我的问题是如何通过随机打乱标签来解决这个问题？也就是说，仅仅因为汽车被称为狗和狗被称为拖拉机，这有什么区别？在理解标签洗牌实际上意味着什么时，我是否缺少一些隐含的知识？ P.S：我是一名开发人员，但对神经网络来说是个新手。   由   提交 /u/OhDearAI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n8ppb/significance_of_training_with_shuffled_labels/</guid>
      <pubDate>Wed, 20 Dec 2023 23:40:24 GMT</pubDate>
    </item>
    <item>
      <title>关于 LLM 评估的帖子：解码策略及其对遵循 IFEval 基准的指令的影响</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n1y22/a_post_on_llm_evaluation_decoding_strategies_and/</link>
      <description><![CDATA[嘿！我刚刚写了一篇关于大型语言模型 (LLM) 细微差别的博文，我想您会感兴趣的。  在其中，我讨论： - DeciLM-7B 和 Mistral-7B-v0.1 的详细比较。 - 不同的文本生成策略如何影响法学硕士。 - 用于法学硕士评估的新指令遵循基准 (IFEval)。 我相信这里的社区会对这些主题有宝贵的见解。  看看，我们来详细讨论一下！ [在此处阅读博客](https://deci.ai/ blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/)。 ​   由   提交 /u/datascienceharp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n1y22/a_post_on_llm_evaluation_decoding_strategies_and/</guid>
      <pubDate>Wed, 20 Dec 2023 18:45:46 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习说话 | ChatGPT：30 年历史</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n0r0f/how_neural_networks_learned_to_talk_chatgpt_a_30/</link>
      <description><![CDATA[       由   提交/u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n0r0f/how_neural_networks_learned_to_talk_chatgpt_a_30/</guid>
      <pubDate>Wed, 20 Dec 2023 17:57:32 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 自注意力的关键（上下文敏感连接）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n05nq/key_to_transformer_self_attention_context/</link>
      <description><![CDATA[   /u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n05nq/key_to_transformer_self_attention_context/</guid>
      <pubDate>Wed, 20 Dec 2023 17:33:07 GMT</pubDate>
    </item>
    </channel>
</rss>