<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Wed, 03 Jan 2024 12:25:31 GMT</lastBuildDate>
    <item>
      <title>关于人工神经网络的博客</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18xbxfj/blog_on_artificial_neural_network/</link>
      <description><![CDATA[      https://bhargavoza.com/blogs/Artificial%20Neural%20Network  https://preview.redd.it /zwvcphhqy5ac1.png?width=1007&amp;format=png&amp;auto=webp&amp;s=ea7cd84820900c92b298e26a8e5308faa24852d2 嘿，我在人工神经网络上写了这篇博客。它是如何工作的以及它背后的每个数学方程。另外，我在 numpy 的帮助下用 Python 从头开始​​开发了一个完整的训练周期。 我恳请您访问我的博客并提供一些反馈。   由   提交/u/Troniq777  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18xbxfj/blog_on_artificial_neural_network/</guid>
      <pubDate>Wed, 03 Jan 2024 05:49:31 GMT</pubDate>
    </item>
    <item>
      <title>预测器（回归）性能面临的挑战：MAE 持续为 0.26 且二元向量预测不准确</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18xbg5b/challenges_with_predictor_regression_performance/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18xbg5b/challenges_with_predictor_regression_performance/</guid>
      <pubDate>Wed, 03 Jan 2024 05:23:37 GMT</pubDate>
    </item>
    <item>
      <title>多头/多查询/分组查询注意力解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18wlxga/multiheadmultiquerygroupedquery_attentions/</link>
      <description><![CDATA[您好， 我创建了一个视频 这里我解释了多头注意力（MHA）、多查询注意力（MQA）和分组查询注意力（GQA）如何工作，以及使用它们中的每一种的优缺点&lt; /p&gt; 我希望它对你们中的一些人有用。非常欢迎反馈！ :)   由   提交/u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18wlxga/multiheadmultiquerygroupedquery_attentions/</guid>
      <pubDate>Tue, 02 Jan 2024 10:00:25 GMT</pubDate>
    </item>
    <item>
      <title>为什么大家都说单层感知器无法解决异或问题？那这个呢？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18v926p/why_is_everybody_saying_singlelayer_perceptron/</link>
      <description><![CDATA[def activate_function(number): if number%2: return 1 return 0 权重 = [1, 1] for x in range(2 ): for y in range(2): print(f&quot;{x}, {y} = {activation_function(weights[0] * x + Weights[1] * y)}&quot;)  输出： 0, 0 = 0 | 0, 1 = 1 | 1, 0 = 1 | 1, 1 = 0 |   由   提交/u/jaroslavtavgen  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18v926p/why_is_everybody_saying_singlelayer_perceptron/</guid>
      <pubDate>Sun, 31 Dec 2023 14:40:10 GMT</pubDate>
    </item>
    <item>
      <title>未来的计算机将截然不同（模拟计算）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18uku2k/future_computers_will_be_radically_different/</link>
      <description><![CDATA[       由   提交/u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18uku2k/future_computers_will_be_radically_different/</guid>
      <pubDate>Sat, 30 Dec 2023 17:32:58 GMT</pubDate>
    </item>
    <item>
      <title>《注意力》、《变形金刚》、神经网络《大语言模型》</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18qexbz/attention_transformers_in_neural_network_large/</link>
      <description><![CDATA[ 由   提交/u/nickb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18qexbz/attention_transformers_in_neural_network_large/</guid>
      <pubDate>Mon, 25 Dec 2023 08:47:41 GMT</pubDate>
    </item>
    <item>
      <title>使用洗牌标签进行训练的意义。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n8ppb/significance_of_training_with_shuffled_labels/</link>
      <description><![CDATA[我刚刚观看了 3Blue1Brown 的这段视频，其中他提到了 Lisha Li 的研究，她使用随机排列的数据集标签来训练网络。他说，这是为了识别“最小化成本函数是否对应于图像中的任何结构，或者只是记忆”。 （正如 Lisha 所说，“记住正确分类的整个数据集”） https://youtu.be/IHZwWFHWa-w?si=aDGIG1zVMHtFlYk7&amp;t=1064 我的问题是如何通过随机打乱标签来解决这个问题？也就是说，仅仅因为汽车被称为狗和狗被称为拖拉机，这有什么区别？在理解标签洗牌实际上意味着什么时，我是否缺少一些隐含的知识？ P.S：我是一名开发人员，但对神经网络来说是个新手。   由   提交 /u/OhDearAI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n8ppb/significance_of_training_with_shuffled_labels/</guid>
      <pubDate>Wed, 20 Dec 2023 23:40:24 GMT</pubDate>
    </item>
    <item>
      <title>关于 LLM 评估的帖子：解码策略及其对遵循 IFEval 基准的指令的影响</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n1y22/a_post_on_llm_evaluation_decoding_strategies_and/</link>
      <description><![CDATA[嘿！我刚刚写了一篇关于大型语言模型 (LLM) 细微差别的博客文章，我想您会感兴趣的。  在其中，我讨论： - DeciLM-7B 和 Mistral-7B-v0.1 的详细比较。 - 不同的文本生成策略如何影响法学硕士。 - 用于法学硕士评估的新指令遵循基准 (IFEval)。 我相信这里的社区会对这些主题有宝贵的见解。  看看，我们来详细讨论一下！ [在此处阅读博客](https://deci.ai/ blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/)。 ​   由   提交 /u/datascienceharp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n1y22/a_post_on_llm_evaluation_decoding_strategies_and/</guid>
      <pubDate>Wed, 20 Dec 2023 18:45:46 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习说话 | ChatGPT：30 年历史</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n0r0f/how_neural_networks_learned_to_talk_chatgpt_a_30/</link>
      <description><![CDATA[       由   提交/u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n0r0f/how_neural_networks_learned_to_talk_chatgpt_a_30/</guid>
      <pubDate>Wed, 20 Dec 2023 17:57:32 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 自注意力的关键（上下文相关连接）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n05nq/key_to_transformer_self_attention_context/</link>
      <description><![CDATA[   /u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n05nq/key_to_transformer_self_attention_context/</guid>
      <pubDate>Wed, 20 Dec 2023 17:33:07 GMT</pubDate>
    </item>
    <item>
      <title>关于实现具有交叉熵损失的 Softmax 输出层的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18muew7/question_about_implementing_a_softmax_output/</link>
      <description><![CDATA[嗨，NN 大师， 我正在使用这个存储库 (https://github.com/SnailWalkerYC/LeNet-5_Speed_Up）并尝试学习神经网络细节。该仓库用 C 和 CUDA 实现了 LeNet5。我现在关注的是 CPU 部分及其在 seq/ 中的代码。我迷失的一个特别地方是 seq/lenet.c 中的这个函数 ​ static inline void softmax(double input[OUTPUT], double loss [输出], int 标签, int 计数){ 双内部 = 0; for (int i = 0; i &lt; count; ++i){ double res = 0; for (int j = 0; j &lt; count; ++j){ res += exp(input[j] - input[i]); }损失[i] = 1. / res;内部 -= 损失[i] * 损失[i]； } 内部 += 损失[标签]; for (int i = 0; i &lt; count; ++i){ loss[i] *= (i == label) - loss[i] - 内部; } }  ​ 因为没有注释，所以我花了一些时间来理解这个函数。最后我发现它正在计算 MSE 损失函数相对于 softmax 层输入的导数。 然后我尝试将交叉熵损失函数与 softmax 一起使用，所以我出来了使用以下函数替换上面的函数。 static inline void softmax(double input[OUTPUT], double loss[OUTPUT], int label, int count) { double inside = 0;双最大输入=-INFINITY； // 找到最大输入值以防止数值不稳定 for (int i = 0; i &lt; count; ++i) { if (input[i] &gt; max_input) max_input = input[i]; } // 计算softmax和交叉熵损失 double sum_exp = 0; for (int i = 0; i &lt; count; ++i) { double exp_val = exp(input[i] - max_input); } sum_exp += exp_val;损失[i] = exp_val;双softmax_output[输出]; for (int i = 0; i &lt; count; ++i) { loss[i] /= sum_exp; softmax_output[i] = 损失[i]; } // 计算交叉熵损失和导数 inside = -log(softmax_output[label]); for (int i = 0; i &lt; count; ++i) { loss[i] = softmax_output[i] - (i == label); } } }  ​ 但是，使用我的 softmax() 函数版本，MNIST 识别不起作用。原始版本的准确率达到了 96% 以上。我的交叉熵损失代码有什么问题？ 请帮忙。 ​ 谢谢 ​ ;   由   提交 /u/bssrdf   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18muew7/question_about_implementing_a_softmax_output/</guid>
      <pubDate>Wed, 20 Dec 2023 13:19:17 GMT</pubDate>
    </item>
    <item>
      <title>寻找交易人工智能项目的合作实验室</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18mhkir/looking_for_colab_on_a_trading_ai_project/</link>
      <description><![CDATA[嗨，我正在开始一个基于 Python 的人工智能交易项目，目标是看看我们能创造出一个专注于股票交易的人工智能有多棒。这是一个开源项目，所以它更像是一个有趣的项目，我需要帮助。 谢谢托尼。    ;由   提交/u/Tonyhauf  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18mhkir/looking_for_colab_on_a_trading_ai_project/</guid>
      <pubDate>Wed, 20 Dec 2023 00:52:04 GMT</pubDate>
    </item>
    <item>
      <title>对于具有生物学背景的人来说，如何从头开始学习神经网络。请提供资源和建议</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18koa00/how_to_start_learning_neural_networks_from/</link>
      <description><![CDATA[我是一名物理治疗师 (24M)，想攻读运动科学硕士学位。未来，我渴望攻读神经生物学博士学位，并最终转向人工智能和神经网络。我知道这是一个非常艰难的领域，如果你想开始理解神经网络，你需要学习很多东西。我知道这是一个漫长的过程，需要很多年，但我想在学习和工作的同时开始这段旅程，这样我就能在未来的 5-7 年内做好准备。  请为我提供资源和链接以及路线图来追求我的这一兴趣。还可以向我提供任何替代建议或解决方案，或者任何你们认为可以帮助我更好地实现这一目标和我的职业生涯的建议   由   提交 /u/biocosmosian   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18koa00/how_to_start_learning_neural_networks_from/</guid>
      <pubDate>Sun, 17 Dec 2023 18:54:59 GMT</pubDate>
    </item>
    <item>
      <title>这种 Momentum GD 方法不起作用。帮助</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18jr0sc/this_momentum_gd_method_aint_working_help/</link>
      <description><![CDATA[       由   提交/u/imvedant04  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18jr0sc/this_momentum_gd_method_aint_working_help/</guid>
      <pubDate>Sat, 16 Dec 2023 13:16:52 GMT</pubDate>
    </item>
    <item>
      <title>Udemy 上值得考虑的最佳神经网络课程</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18hx0hy/best_neural_networks_courses_on_udemy_to_consider/</link>
      <description><![CDATA[       [链接]&lt; /a&gt; [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18hx0hy/best_neural_networks_courses_on_udemy_to_consider/</guid>
      <pubDate>Thu, 14 Dec 2023 01:29:44 GMT</pubDate>
    </item>
    </channel>
</rss>