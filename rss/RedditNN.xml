<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的子版块。</description>
    <lastBuildDate>Mon, 10 Jun 2024 09:17:04 GMT</lastBuildDate>
    <item>
      <title>ML Ops 简介</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dc3yfi/introduction_to_ml_ops/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dc3yfi/introduction_to_ml_ops/</guid>
      <pubDate>Sun, 09 Jun 2024 20:53:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么会有偏见？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dc34vf/why_biases/</link>
      <description><![CDATA[      神经网络中偏差的用途是什么？ChatGPT 告诉我它会“转移”激活函数（这意味着如果添加偏差，它不会通过原点）。 这对我来说没有意义，因为如果偏差只是将自己添加到加权和中，函数本身没有理由转移。 同样，在所有条件相同的情况下，为什么不直接使用更强的权重而不是添加偏差，这似乎是毫无理由的额外工作。 更新：我已经发现偏差如何“转移”（使用这个词来描述正在发生的事情的非常误导的方式）激活函数。没有发生字面上的“转移”；发生的情况是，偏差只是增加了加权和（根据偏差），因此使其等同于如果函数实际上在 x 轴上移动（通过偏差）并且只将加权和作为其输入时激活函数所返回的值。如有疑问，请继续阅读。 这里有一个图片示例： 注意：加权和 = 2 且偏差 = 2。因此，当两者相加时，您将得到 4（呵呵哈哈……） 蓝线表示如果我们将加权和 (w1 * inp1) + 等... + 偏差输入到 S 型函数中。红线表示如果我们只输入加权和而不在 S 型函数中添加偏差，然后修改函数，使其在 x 轴上移动 2。 如您所见，当 x = 4 时，蓝线显示 y 为 .982 同样，当 x = 2 时，红线显示 y 为 .982 如果您认为我错了，请随时发表评论。    提交人    /u/mistr_bean   [link] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dc34vf/why_biases/</guid>
      <pubDate>Sun, 09 Jun 2024 20:18:45 GMT</pubDate>
    </item>
    <item>
      <title>AI 阅读清单 - 第 2 部分</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dc00tg/ai_reading_list_part_2/</link>
      <description><![CDATA[大家好， 我在这里创建了一个新系列，我们将探讨前 OpenAI 首席科学家 Ilya Sutskever 给 John Carmack 的阅读材料中的以下 6 项。Ilya 接着说：“如果你真的学会了所有这些，你就会知道今天重要的事情的 90%”。 我希望这对你们中的一些人有用。非常欢迎反馈！:)    提交人    /u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dc00tg/ai_reading_list_part_2/</guid>
      <pubDate>Sun, 09 Jun 2024 18:06:50 GMT</pubDate>
    </item>
    <item>
      <title>体素化三维物体的特征识别</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dbcvgg/feature_recognition_for_voxelized_3d_objects/</link>
      <description><![CDATA[嗨。我见过人们如何使用神经网络来识别 2D 图像上的特征或对图像上描绘的物体进行分类。例如，检测人脸或分辨照片上的动物种类。但是 3D 物体的特征识别呢？理论上，我可以将 3D 模型从网格格式转换为体素格式，并使用几乎相同的算法进行特征识别，以分辨哪些体素与手、头、眼睛等相关。 是否有任何现有的模型可以完成这种任务？如果我想构建这样的东西，我会遇到什么挑战？    提交人    /u/zergon321   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dbcvgg/feature_recognition_for_voxelized_3d_objects/</guid>
      <pubDate>Sat, 08 Jun 2024 21:08:47 GMT</pubDate>
    </item>
    <item>
      <title>人工智能阅读清单</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dayhe6/ai_reading_list/</link>
      <description><![CDATA[大家好， 我在这里创建了一个新系列，我们将在其中探讨前 OpenAI 首席科学家 Ilya Sutskever 给 John Carmack 的阅读材料中的前 5 项。Ilya 接着说：“如果你真的学会了所有这些，你就会知道今天重要的事情的 90%”。 我希望这对你们中的一些人有用。非常欢迎反馈！:)    提交人    /u/Personal-Trainer-541   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dayhe6/ai_reading_list/</guid>
      <pubDate>Sat, 08 Jun 2024 08:47:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 dILP 进行归纳逻辑编程</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dabwgl/inductive_logic_programming_with_dilp/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dabwgl/inductive_logic_programming_with_dilp/</guid>
      <pubDate>Fri, 07 Jun 2024 14:19:19 GMT</pubDate>
    </item>
    <item>
      <title>选择手写单词</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1da7d7g/select_handwritten_word/</link>
      <description><![CDATA[      大家好， 我目前正在做一个项目，需要一些指导。如何从二值化图像中检测和选择手写单词？我一直在努力解决这个问题，如果您能分享任何建议或资源，我将不胜感激。 https://preview.redd.it/kcbfv9dsm45d1.png?width=990&amp;format=png&amp;auto=webp&amp;s=b80ae1c68d25fa66729b74a1787f829f73ddf70d 提前感谢您的帮助！    提交人    /u/Dependent-Ad914   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1da7d7g/select_handwritten_word/</guid>
      <pubDate>Fri, 07 Jun 2024 10:28:15 GMT</pubDate>
    </item>
    <item>
      <title>回归任务的批量标准化</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1da5f1q/batch_normalization_for_a_regression_task/</link>
      <description><![CDATA[我是机器学习的新手，正在研究用于回归任务的神经网络。我有 788 个输入神经元和一个输出神经元，以及大量的训练输入/正确输出。我正在使用 MSE 作为成本函数。我一直在阅读有关批量标准化如何改善这些网络的文章，这涉及标准化每个层的输入。我的 788 个输入中的每一个都是布尔值，所以我不确定标准化它们是否有意义。如果我理解过程正确，我应该为批次（激活函数之前）取我的整个 z 值集，对其进行标准化，然后乘以 gamma 并添加 beta，并且 gamma/beta 应该与权重一起在每个节点上单独训练。很多问题：  我最初的理解是它正在标准化批次中的每个输入。因此，如果我有 788 个输入神经元并且正在进行 256 个样本的批次，我将对神经元 1 取 256 个样本的均值/方差，对神经元 2 取 256 个样本的均值/方差，等等。现在我在网上看到的东西看起来好像是在寻找 788 个输入的均值/方差，如下所示：https://i.ytimg.com/vi/BZh1ltr5Rkg/hqdefault.jpg 我还期望每个神经元都有自己的 gamma/beta（输出层除外，因为它不使用批量标准化）。这是正确的吗？ 在实际用例中，不会有批次。在没有批次且标准化没有意义的实际情况下，这如何工作？     提交人    /u/tic-tac135   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1da5f1q/batch_normalization_for_a_regression_task/</guid>
      <pubDate>Fri, 07 Jun 2024 08:05:14 GMT</pubDate>
    </item>
    <item>
      <title>问题：Python 生成狭缝光衍射图案以及真实图像的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d9gpcl/question_python_generated_light_diffraction/</link>
      <description><![CDATA[嗨。我是一名学生，正在尝试制作一个神经网络，该神经网络可以对使用 Python 创建的光衍射图像进行分类。问题是，当在新的 PythonG 生成的图像上进行测试时，该模型在准确度和分类报告/混淆矩阵方面取得了非常高的分数，但当我给它提供从互联网上找到的真实图像时，它的表现非常差。我怀疑这与真实图像的噪声不太理想有关，现在我试图将噪声引入我的训练数据集。有什么建议吗？    提交人    /u/AncientGearAI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d9gpcl/question_python_generated_light_diffraction/</guid>
      <pubDate>Thu, 06 Jun 2024 12:13:29 GMT</pubDate>
    </item>
    <item>
      <title>有人能解释一下吗？（概率神经网络）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d9fx56/can_anyone_explain_probabilistic_neural_networks/</link>
      <description><![CDATA[      我的老师给了我们这个他解决的例子，但我不明白。特别是多变量函数求和部分。有人能给我解释一下吗？    提交人    /u/Ikossys   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d9fx56/can_anyone_explain_probabilistic_neural_networks/</guid>
      <pubDate>Thu, 06 Jun 2024 11:29:48 GMT</pubDate>
    </item>
    <item>
      <title>不确定我的项目</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d98u9u/unsure_about_my_project/</link>
      <description><![CDATA[我是一名计算机科学专业的最后一年学生，我还没有完成我的 FYP 项目。我和我的团队已经研究了很长一段时间，我们想到了一个想法，有时感觉它具有革命性，有时感觉我们可能在浪费时间，更不用说对它可实现性的怀疑了。 我们的想法是设计一个 SNN 模型，使用某种同态加密对加密数据进行计算。我们为此决定的应用程序是欺诈检测，银行和金融机构可以使用它来检测欺诈交易，而无需解密数据进行处理。 我很感激关于这个想法的反馈和想法，以及任何与项目相关的建议或推荐。    提交人    /u/Other-Community9534   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d98u9u/unsure_about_my_project/</guid>
      <pubDate>Thu, 06 Jun 2024 03:32:33 GMT</pubDate>
    </item>
    <item>
      <title>我利用人工智能让 8 岁的阿尔伯特·爱因斯坦 (1879-1955) 复活。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d8qgde/i_resurrected_albert_einstein_18791955_at_age_of/</link>
      <description><![CDATA[       由    /u/languagelack  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d8qgde/i_resurrected_albert_einstein_18791955_at_age_of/</guid>
      <pubDate>Wed, 05 Jun 2024 13:51:41 GMT</pubDate>
    </item>
    <item>
      <title>“人工智能概述”中的谷歌与幻觉</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d7z2ut/google_vs_hallucinations_in_ai_overviews/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d7z2ut/google_vs_hallucinations_in_ai_overviews/</guid>
      <pubDate>Tue, 04 Jun 2024 14:51:27 GMT</pubDate>
    </item>
    <item>
      <title>关于理解前馈网络的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d7pqyu/question_on_understanding_the_feed_forward_network/</link>
      <description><![CDATA[我现在可以从头开始创建自己的前馈网络（推导出所有反向传播方程），但我很难说服自己这些东西有效。显然它在实践中有效，但更具体地说，我几乎可以说服自己，具有多个层的单输入 NN（使用 relu 激活）可以基于样本数据对任何函数进行建模。但是，我很难说服自己这真的适用于多维输入（尤其是超过 2 维）。你们有什么直觉吗？此外，我觉得梯度下降不会起作用，因为它只会找到局部最小值，而不能真正精确地近似随机函数。我想直观地了解为什么这实际上不仅在 2D 中有效，而且在 3D 及更高版本中也有效。任何直觉和/或帮助都将不胜感激！    提交人    /u/Gullible_Big5193   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d7pqyu/question_on_understanding_the_feed_forward_network/</guid>
      <pubDate>Tue, 04 Jun 2024 05:38:53 GMT</pubDate>
    </item>
    <item>
      <title>卷积神经网络 | 深度学习动画</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1d772lh/convolutional_neural_networks_deep_learning/</link>
      <description><![CDATA[        提交人    /u/keghn   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1d772lh/convolutional_neural_networks_deep_learning/</guid>
      <pubDate>Mon, 03 Jun 2024 15:22:51 GMT</pubDate>
    </item>
    </channel>
</rss>