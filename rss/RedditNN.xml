<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的子版块。</description>
    <lastBuildDate>Sun, 03 Nov 2024 15:15:54 GMT</lastBuildDate>
    <item>
      <title>神经语言模型中嵌入层的目的</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gikyzx/purpose_of_embedding_layer_in_neural_language/</link>
      <description><![CDATA[所以我正在参加一个关于机器学习的讲座，我们正在学习 3blue1brown 在他的视频系列中关于大型语言模型的内容，即给定一组单词，如何预测下一个单词。 让我困惑的是关于词嵌入的解释和词嵌入的实现不合逻辑。 词嵌入的解释：它是一些高维空间，其中单词被映射到向量，这些向量在高维空间中仍然共享一些意义概念。讲座中的解释显示了彼此相关的单词在集群中更接近的图形。 3blue1brown 的解释是这样的，男人和女人的向量之间的差异类似于国王和王后的向量，所以你可以做一些看起来像数学的事情来得到：王后 = 国王 - 男人 + 女人。 换句话说，词嵌入的解释声称它们捕获了语义关系！ 但是，当你真正实现嵌入时，而不是真正实现而只是使用 torch.nn.Embedding，所有这些都变得没用，嵌入层的用途对我来说变得模糊。 nn.Embedding 基本上给你一种将整数映射到随机初始化的向量的可能性。因此，您应该自己定义标记和整数之间的映射，可以是 Python 字典，也可以只使用文本并从 0 到文本末尾索引单词，然后声明相同的索引在嵌入中。 nn.Embedding 本质上只是根据您的 vocab_size（标记数）和您想要的嵌入向量的更高维度创建一个矩阵。然后，您可以通过传入带有这些索引的张量来访问该矩阵的每一行。 换句话说，单词嵌入的实现只会给您随机的、未经训练的向量。映射由您完成。 它可能朝着您必须训练嵌入的方向发展，以使其变成所教的内容以及您在谷歌搜索“词嵌入”时通常会找到的内容。 但如果这是真的，我们究竟想通过在神经网络中使用嵌入层来实现什么？ 因为如果嵌入层未经训练，它实际上不会为网络本身带来任何好处。它只是将文本映射到向量的一种方式，仅此而已。嵌入层的权重会以某种方式进行调整，以便我尝试使用网络完成的任何工作都可以正常工作。这并不意味着嵌入层会变成这个词向量空间，其中相似的单词彼此更接近。或者这仍然会作为副作用发生？    提交人    /u/elm1ra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gikyzx/purpose_of_embedding_layer_in_neural_language/</guid>
      <pubDate>Sun, 03 Nov 2024 11:42:05 GMT</pubDate>
    </item>
    <item>
      <title>Oasis：基于扩散变压器的模型，用于生成可玩的视频游戏</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ght148/oasis_diffusion_transformer_based_model_to/</link>
      <description><![CDATA[decart 和 etched 开发的 Oasis 已经发布，它可以输出可玩的视频游戏，用户可以执行移动、跳跃、检查库存等操作。这不像谷歌的 GameNGen，它只能输出游戏视频（但不能播放）。在此处查看演示和其他详细信息：https://youtu.be/INsEs1sve9k    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ght148/oasis_diffusion_transformer_based_model_to/</guid>
      <pubDate>Sat, 02 Nov 2024 09:55:08 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的偏差</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ggctva/bias_in_nn/</link>
      <description><![CDATA[大家好，我最近开始研究神经网络。让我有些困惑的概念是偏差。我理解神经网络中偏差的用途，但我仍然不明白两件事：  各个隐藏层中的每个单元是否都有自己的偏差，或者每个隐藏层的所有单元是否有共同的偏差？ 我不明白为什么在某些情况下偏差是通过一个单元来表示的，并附加自己的权重。它不应该是一个参数，因此不应该作为一个单元出现吗？     提交人    /u/Annual_Inflation_235   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ggctva/bias_in_nn/</guid>
      <pubDate>Thu, 31 Oct 2024 12:03:28 GMT</pubDate>
    </item>
    <item>
      <title>运行此代码需要多少普通内存</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gfpjyx/how_much_normal_ram_would_i_need_to_just_run_this/</link>
      <description><![CDATA[import torch 导入 torch.nn 作为 nn class TransformerBlock(nn.Module): def __init__(self, embed_size, heads, dropout, forward_expansion): super(TransformerBlock, self).__init__() self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads) self.norm1 = nn.LayerNorm(embed_size) self.norm2 = nn.LayerNorm(embed_size) self.feed_forward = nn.Sequential( nn.Linear(embed_size, forward_expansion * embed_size), nn.ReLU(), nn.Linear(forward_expansion * embed_size, embed_size) ) self.dropout1 = nn.Dropout(dropout) self.dropout2 = nn.Dropout(dropout) def forward(self, x):tention = self.attention(x, x, x)[0] x = self.dropout1(self.norm1(attention + x)) forward = self.feed_forward(x) out = self.dropout2(self.norm2(forward + x)) return out class ChatGPT(nn.Module): def __init__(self, embed_size, num_heads, num_layers, vocab_size, max_length, forward_expansion, dropout): super(ChatGPT, self).__init__() self.embed_size = embed_size self.word_embedding = nn.Embedding(vocab_size, embed_size) self.position_embedding = nn.Embedding(max_length, embed_size) self.transformer_blocks = nn.ModuleList( [TransformerBlock(embed_size, num_heads, dropout, forward_expansion) for _ in range(num_layers)] ) self.fc_out = nn.Linear(embed_size, vocab_size) self.dropout = nn.Dropout(dropout) def forward(self, x): N, seq_length = x.shape positions = torch.arange(0, seq_length).expand(N, seq_length).to(x.device) out = self.dropout(self.word_embedding(x) + self.position_embedding(positions)) for transformer in self.transformer_blocks: out = transformer(out) out = self.fc_out(out) return out # 大型模型的模型超参数（类似于 GPT-3） embed_size = 12288 # 大型模型的嵌入大小 num_heads = 96 # 注意力头的数量 num_layers = 96 # 数量Transformer 块的数量 vocab_size = 50257 # 词汇表的大小（GPT-3 使用更大的词汇表） max_length = 2048 # 输入序列的最大长度 forward_expansion = 4 # 前馈层的扩展因子 dropout = 0.1 # 丢失率 # 初始化模型 model_0 = ChatGPT(embed_size, num_heads, num_layers, vocab_size, max_length, forward_expansion, dropout)  ```     submitted by    /u/Budget-Relief1307   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gfpjyx/how_much_normal_ram_would_i_need_to_just_run_this/</guid>
      <pubDate>Wed, 30 Oct 2024 15:41:01 GMT</pubDate>
    </item>
    <item>
      <title>🌟 游戏开发的人工智能：改变游戏世界的未来！🌟</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gew3ly/ai_for_game_development_transforming_the_future/</link>
      <description><![CDATA[正在寻找加速角色、位置和纹理创建的方法？想看看 AI 如何加速开发并激发新想法吗？ 🎮 欢迎参加 AI 重塑游戏开发的演示！使用 ControlNet、ChatGPT、Stable Diffusion 等示例，我将展示人工智能如何显著增强和优化游戏创作过程。 🚀 您会发现什么？ - 如何使用 AI 在几秒钟内创建姿势和场景 - 轻松为特定项目训练模型 - 将手绘与神经网络集成的示例 不要错过获得灵感并从全新视角看待游戏开发的机会！ 👉 观看演示    提交人    /u/Bozhenart   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gew3ly/ai_for_game_development_transforming_the_future/</guid>
      <pubDate>Tue, 29 Oct 2024 14:34:21 GMT</pubDate>
    </item>
    <item>
      <title>机器学习与知识的集成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1geleye/machine_learning_integration_with_knowledge/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1geleye/machine_learning_integration_with_knowledge/</guid>
      <pubDate>Tue, 29 Oct 2024 03:29:55 GMT</pubDate>
    </item>
    <item>
      <title>FSF 致力于实现机器学习应用的自由</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ge3cqh/fsf_is_working_on_freedom_in_machine_learning/</link>
      <description><![CDATA[  由    /u/nickb  提交  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ge3cqh/fsf_is_working_on_freedom_in_machine_learning/</guid>
      <pubDate>Mon, 28 Oct 2024 14:25:38 GMT</pubDate>
    </item>
    <item>
      <title>组合 DQN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ge11g0/combining_dqns/</link>
      <description><![CDATA[将 3 个 DQN 组合成一个 DQN 的最佳方法是什么。每个 DQN 都有类似的参数，就像它们在不同的任务上工作但仍然相似。例如，假设我们有一个有敌人和状态的游戏。首先，您可以使用 3 个动作。 1) 使用剑 2) 使用弓 3) 使用魔法 如果您使用剑，您可以使用 2 种不同的动作，如轻攻击或重攻击。如果您使用弓，您可以用它击中敌人的近战，或者使用箭（如果有）等。 我不想创建一个可以决定第一个动作（将使用什么样的武器）的 DQN，然后为每种武器决定将采取什么样的动作，而是想为每种武器创建一个 DQN，它确切知道如何使用一种武器，然后将它们组合成 1。最终的网络应该从状态中了解将使用哪种武器以及这些武器将采取什么动作。    提交人    /u/volvol7   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ge11g0/combining_dqns/</guid>
      <pubDate>Mon, 28 Oct 2024 12:42:23 GMT</pubDate>
    </item>
    <item>
      <title>寻求针对 CVPR、ICML 等会议正在进行的完整论文的合作。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gd64zq/looking_for_collaborations_on_ongoing/</link>
      <description><![CDATA[大家好， 我们小组，印度理工学院鲁尔基分校视觉与语言小组，最近有三篇研讨会论文被 NeurIPS 研讨会接受！🚀 我们还建立了一个网站 👉 VLG，展示我们参与过的其他出版物，因此我们的团队正在稳步建立 ML 和 AI 研究组合。目前，我们正在合作撰写几篇正在进行的论文，目的是向 CVPR 和 ICML 等顶级会议提交全文。 话虽如此，我们还有更多让我们兴奋的想法。尽管如此，我们的主要限制之一是无法获得适当的指导和 GPU 和 API 的资金，这对于试验和扩展我们的一些概念至关重要。如果您或您的实验室有兴趣一起工作，我们很乐意探索我们感兴趣领域的交集以及您可能带来的任何新想法！ 如果您有可用资源或有兴趣讨论潜在的合作，请随时联系我们！期待着建立联系并共同建立有影响力的东西！这是我们的 Open Slack 的链接👉 Open Slack    提交人    /u/vlg_iitr   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gd64zq/looking_for_collaborations_on_ongoing/</guid>
      <pubDate>Sun, 27 Oct 2024 08:09:34 GMT</pubDate>
    </item>
    <item>
      <title>你们用什么椅子来编码？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gc78dz/what_chairs_are_you_guys_using_to_code_with/</link>
      <description><![CDATA[我需要一把椅子放在我的办公桌上。你对哪些椅子感到满意？    由    /u/EleTriCTNT  提交  [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gc78dz/what_chairs_are_you_guys_using_to_code_with/</guid>
      <pubDate>Fri, 25 Oct 2024 23:01:43 GMT</pubDate>
    </item>
    <item>
      <title>神经网络使其具有适应性？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gb7sus/neural_networks_making_it_adaptive/</link>
      <description><![CDATA[ 我目前是一名学习 ANN 概念的初学者，有人可以为我的新研究提供意见吗？ 键的形成：  形成标准：如果它导致损失函数显着减少（提高性能），则两个神经元之间会形成新的连接。 实施：定期评估当前未连接的神经元之间的潜在连接。如果在神经元 iii 和神经元 jjj 之间添加连接使损失降低超过阈值 ϵadd\epsilon_{\text{add}}ϵadd​，则我们添加该连接。  键断裂：  断裂标准：如果现有连接对网络性能贡献不大，或者移除它不会显著增加损失函数，则将其移除。 实施：监控现有连接的权重。如果权重 wijw_{ij}wij​ 的绝对值低于阈值 ϵremove\epsilon_{\text{remove}}ϵremove​，或者连接对性能的贡献很小，则我们移除该连接。      提交人    /u/South-Ad-1977   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gb7sus/neural_networks_making_it_adaptive/</guid>
      <pubDate>Thu, 24 Oct 2024 17:10:31 GMT</pubDate>
    </item>
    <item>
      <title>请给我推荐一门课程，让我可以学习如何用特定语言训练由聚合参数组成的神经网络......</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1gb11h3/please_suggest_me_a_course_that_i_can_follow_to/</link>
      <description><![CDATA[大家好。 我认为现在是时候开始研究神经网络如何帮助系统管理员了，具体来说就是 FreeBSD 系统管理员，因为我喜欢使用 FreeBSD 胜过任何其他操作系统；比 Linux 更喜欢。但这不是重点。 基本上，我想尝试使用一组 bhyve 命令来训练神经网络，以便它能够预测和理解您想要从输入文本中执行的操作。我从小就喜欢虚拟化。我非常喜欢 bhyve。 在这 3 年的努力中，我尝试学习如何管理 FreeBSD，在 bhyve 上花费了大量时间。 了解 bhyve 是让我使用 FreeBSD 的第一个原因。我也很想研究深度学习和神经网络，因为我认为在不久的将来，这些技术将在较低水平上集成到操作系统中.... 话虽如此，我希望您能为我指明正确的方向，因为我想学习如何使用 bhyve 管理程序所需的特殊语言来训练神经网络，以便网络可以预测用户想要做什么，他/她想要“给” bhyve 哪些 bhyve 命令和参数。 非常感谢。    提交人    /u/loziomario   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1gb11h3/please_suggest_me_a_course_that_i_can_follow_to/</guid>
      <pubDate>Thu, 24 Oct 2024 12:06:39 GMT</pubDate>
    </item>
    <item>
      <title>为 VQA 推导场景图中的域关系</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1g9gzgo/abducing_domain_relationships_in_scene_graphs_for/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1g9gzgo/abducing_domain_relationships_in_scene_graphs_for/</guid>
      <pubDate>Tue, 22 Oct 2024 12:31:42 GMT</pubDate>
    </item>
    <item>
      <title>数据清理：清理 ML 数据集的 9 种方法</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1g9cdwz/data_cleaning_9_ways_to_clean_your_ml_datasets/</link>
      <description><![CDATA[       由    /u/codingdecently  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1g9cdwz/data_cleaning_9_ways_to_clean_your_ml_datasets/</guid>
      <pubDate>Tue, 22 Oct 2024 07:20:21 GMT</pubDate>
    </item>
    <item>
      <title>解释基本文本分类神经网络背后的理论（构建垃圾邮件检测器）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1g8t5tt/theory_behind_basic_text_classification_neural/</link>
      <description><![CDATA[        由    /u/maksyche  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1g8t5tt/theory_behind_basic_text_classification_neural/</guid>
      <pubDate>Mon, 21 Oct 2024 15:58:00 GMT</pubDate>
    </item>
    </channel>
</rss>