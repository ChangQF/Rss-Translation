<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Tue, 19 Mar 2024 15:14:37 GMT</lastBuildDate>
    <item>
      <title>像素完美：工程师的新方法使图像成为焦点</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bilj9u/pixel_perfect_engineers_new_approach_brings/</link>
      <description><![CDATA[   /u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bilj9u/pixel_perfect_engineers_new_approach_brings/</guid>
      <pubDate>Tue, 19 Mar 2024 14:26:06 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习？数学公式解释了它们如何检测相关模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bid6t5/how_do_neural_networks_learn_a_mathematical/</link>
      <description><![CDATA[       由   提交/u/nickb  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bid6t5/how_do_neural_networks_learn_a_mathematical/</guid>
      <pubDate>Tue, 19 Mar 2024 05:52:47 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型的时间之箭</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bhnp5i/arrows_of_time_for_large_language_models/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.17505 摘要：  我们通过以下方法研究自回归大型语言模型执行的概率建模：时间方向性的角度。我们凭经验发现此类模型在模拟自然语言的能力方面表现出时间不对称性：尝试预测下一个标记与尝试预测前一个标记时的平均对数困惑度存在差异。这种差异同时是微妙的，并且在各种模式（语言、模型大小、训练时间……）中非常一致。从理论上讲，这是令人惊讶的：从信息论的角度来看，不应该存在这样的差异。我们提供了一个理论框架来解释这种不对称性是如何从稀疏性和计算复杂性考虑中出现的，并概述了我们的结果所带来的一些观点。    由   提交 /u/SunsetOneSix   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bhnp5i/arrows_of_time_for_large_language_models/</guid>
      <pubDate>Mon, 18 Mar 2024 10:43:41 GMT</pubDate>
    </item>
    <item>
      <title>验证链 (COVE) 解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bh5xv3/chainofverification_cove_explained/</link>
      <description><![CDATA[   /u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bh5xv3/chainofverification_cove_explained/</guid>
      <pubDate>Sun, 17 Mar 2024 19:18:42 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习进行脑肿瘤分类</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bh2lna/brain_tumor_classification_using_deep_learning/</link>
      <description><![CDATA[      https://preview .redd.it/yapyr9cmexoc1.png?width=1280&amp;format=png&amp;auto=webp&amp;s=355d17b34d2478a44d9427cd80b11f6916c440c5 欢迎来到脑肿瘤初学者教程，在这里我们深入探讨 CNN 的世界（卷积神经网络）及其在图像分类和脑肿瘤检测中的突破性应用。 这是一个简单的卷积神经网络教程，演示如何在图像数据集中检测脑肿瘤。 我们将使用 CNN 构建和训练模型，并查看模型的准确性和准确性。损失，然后我们将使用新图像测试和预测肿瘤。 以下是视频链接：https:/ /youtu.be/-147KGbGI3g 享受 Eran #cnnforimageclassification #cnnmachinelearningmodel #cnnml #deeplearningbraintumorclassification #aiDetectbraintumor   由   提交 /u/Feitgemel   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bh2lna/brain_tumor_classification_using_deep_learning/</guid>
      <pubDate>Sun, 17 Mar 2024 17:04:01 GMT</pubDate>
    </item>
    <item>
      <title>马达琳网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bgtmx6/madaline_network/</link>
      <description><![CDATA[是否有一个正式的证据来说明为什么要设置更新规则，我知道与简单的反向传播相比，这是一个愚蠢的模型，我很容易理解（链式法则和错误道具）但是 Madaline 规则感觉很奇怪，只是想到而不是派生的东西    由   提交/u/borisshootspancakes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bgtmx6/madaline_network/</guid>
      <pubDate>Sun, 17 Mar 2024 09:32:34 GMT</pubDate>
    </item>
    <item>
      <title>关于 RNN 的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bg3gm8/question_about_rnns/</link>
      <description><![CDATA[在 RNN 中，神经元的输出是否会：  直接返回到与输入相同的神经元？  或  回到上一层，然后再次触发同一个神经元，同时也触发其他神经元？   如果这两种类型的 RNN 都存在，它们的名字是什么？   由   提交/u/BePoliter  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bg3gm8/question_about_rnns/</guid>
      <pubDate>Sat, 16 Mar 2024 10:55:40 GMT</pubDate>
    </item>
    <item>
      <title>这是什么程序？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bfxoag/what_program_is_this/</link>
      <description><![CDATA[      我正在 YouTube 上学习神经网络的基础知识，老师使用了下面的工具解释材料的某些部分。不过他没有提到名字，有谁知道并可以告诉我名字吗？ https://preview.redd.it/cma43faqhmoc1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=57fc2d9cd55b07f815f5bb336a3a 07cdb8e4aadf &lt; /div&gt;  由   提交 /u/ConfectionPuzzled271   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bfxoag/what_program_is_this/</guid>
      <pubDate>Sat, 16 Mar 2024 04:18:00 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的自组织映射邻域实现</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bevbd9/selforganizing_map_neighborhood_implementation_in/</link>
      <description><![CDATA[我正在尝试实现一个自组织映射，其中对于给定的输入样本，根据（例如）选择最佳匹配单元/获胜单元SOM 和输入之间的 L2 范数距离。获胜单元/BMU (som[x, y]) 与给定输入 (z) 的 L2 距离最小： # 输入批次：batch-size = 512, input-dim = 84- z = torch.randn(512, 84) # SOM 形状：（高度、宽度、输入-dim)- som = torch.randn(40, 40, 84) print(f&quot;BMU 行, col 形状; row = {row.shape} &amp; col = {col.shape}&quot;) # BMU row, col 形状; row = torch.Size([512]) &amp; col = torch.Size([512]) 为了清楚起见，对于批次“z[0]”中的第一个输入样本，获胜单位是“som[row” [0]，col[0]]”- z[0].shape，som[row[0]，col[0]].shape # (torch.Size([84]), torch.Size([84])) torch.norm((z[0]) - som[row[0], col[0]])) 是 z[0] 与除 row[0] 和 col[0] 之外的所有其他 som 单位之间的最小 L2 距离。 &lt; p&gt;# 定义初始邻域半径和学习率- neighb_rad = torch.tensor(2.0)  lr = 0.5 # 更新第一个输入“z[0]”的权重及其对应的 BMU“som[row[0], col[0]]”- for r in range(som.shape[0]): for c in range(som.shape[1]): neigh_dist = torch.exp(-torch.norm(输入 = (som[r, c] - som[row[0], col[0]])) / (2.0 * torch.pow(neighb_rad, 2))) &lt; code&gt;som[r, c] = som[r, c] + (lr * neigh_dist * (z[0] - som[r, c])) 如何实现代码：  更新每个 BMU 周围所有单元的权重，无需 2 个 for 循环（并且） 对所有输入“z”执行此操作（这里，z有512个样本）    由   提交/u/grid_world  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bevbd9/selforganizing_map_neighborhood_implementation_in/</guid>
      <pubDate>Thu, 14 Mar 2024 20:35:43 GMT</pubDate>
    </item>
    <item>
      <title>使用CNN进行黑白矩阵识别</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bekofr/black_and_white_matrix_identification_using_cnn/</link>
      <description><![CDATA[        由   提交/u/LightFounder  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bekofr/black_and_white_matrix_identification_using_cnn/</guid>
      <pubDate>Thu, 14 Mar 2024 13:04:02 GMT</pubDate>
    </item>
    <item>
      <title>您想知道您的神经网络需要多长时间才能得到充分训练？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1behfja/do_you_want_to_know_the_time_it_will_take_your/</link>
      <description><![CDATA[查看投票 &lt; /div&gt;  由   提交 /u/Red_Pudding_pie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1behfja/do_you_want_to_know_the_time_it_will_take_your/</guid>
      <pubDate>Thu, 14 Mar 2024 09:46:44 GMT</pubDate>
    </item>
    <item>
      <title>1 位法学硕士时代 - 论文解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1beel34/the_era_of_1bit_llms_paper_explained/</link>
      <description><![CDATA[       由   提交/u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1beel34/the_era_of_1bit_llms_paper_explained/</guid>
      <pubDate>Thu, 14 Mar 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习？数学公式解释了它们如何检测相关模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bde520/how_do_neural_networks_learn_a_mathematical/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bde520/how_do_neural_networks_learn_a_mathematical/</guid>
      <pubDate>Wed, 13 Mar 2024 00:55:16 GMT</pubDate>
    </item>
    <item>
      <title>复制理论表明深度神经网络的想法是相似的</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bddxi0/replica_theory_shows_deep_neural_networks_think/</link>
      <description><![CDATA[   /u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bddxi0/replica_theory_shows_deep_neural_networks_think/</guid>
      <pubDate>Wed, 13 Mar 2024 00:45:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习工程的最佳实践是什么？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bd4wsm/what_are_best_practices_for_machine_learning/</link>
      <description><![CDATA[   /u/beluis3d  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bd4wsm/what_are_best_practices_for_machine_learning/</guid>
      <pubDate>Tue, 12 Mar 2024 18:41:45 GMT</pubDate>
    </item>
    </channel>
</rss>