<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Sat, 30 Dec 2023 12:23:05 GMT</lastBuildDate>
    <item>
      <title>我终于接触到了 Pika AI，所以我制作了一些视频</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18s4ctz/i_finally_got_access_to_pika_ai_so_i_made_little/</link>
      <description><![CDATA[       由   提交/u/Snoo_8366  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18s4ctz/i_finally_got_access_to_pika_ai_so_i_made_little/</guid>
      <pubDate>Wed, 27 Dec 2023 16:18:28 GMT</pubDate>
    </item>
    <item>
      <title>NeuralByte 每周人工智能概要 - 12 月 23 日</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18r71dc/neuralbytes_weekly_ai_rundown_23th_december/</link>
      <description><![CDATA[    &lt; /a&gt;   由   提交/u/Snoo_8366  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18r71dc/neuralbytes_weekly_ai_rundown_23th_december/</guid>
      <pubDate>Tue, 26 Dec 2023 12:23:46 GMT</pubDate>
    </item>
    <item>
      <title>《注意力》、《变形金刚》、神经网络《大语言模型》</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18qexbz/attention_transformers_in_neural_network_large/</link>
      <description><![CDATA[ 由   提交/u/nickb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18qexbz/attention_transformers_in_neural_network_large/</guid>
      <pubDate>Mon, 25 Dec 2023 08:47:41 GMT</pubDate>
    </item>
    <item>
      <title>带洗牌标签的训练的意义。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n8ppb/significance_of_training_with_shuffled_labels/</link>
      <description><![CDATA[我刚刚观看了 3Blue1Brown 的这段视频，其中他提到了 Lisha Li 的研究，她使用随机排列的数据集标签来训练网络。他说，这是为了识别“最小化成本函数是否对应于图像中的任何结构，或者只是记忆”。 （正如 Lisha 所说，“记住正确分类的整个数据集”） https://youtu.be/IHZwWFHWa-w?si=aDGIG1zVMHtFlYk7&amp;t=1064 我的问题是如何通过随机打乱标签来解决这个问题？也就是说，仅仅因为汽车被称为狗和狗被称为拖拉机，这有什么区别？在理解标签洗牌的实际含义时，我是否缺少一些隐含的知识？ P.S：我是一名开发人员，但对神经网络来说是个新手。   由   提交 /u/OhDearAI   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n8ppb/significance_of_training_with_shuffled_labels/</guid>
      <pubDate>Wed, 20 Dec 2023 23:40:24 GMT</pubDate>
    </item>
    <item>
      <title>关于 LLM 评估的帖子：解码策略及其对遵循 IFEval 基准的指令的影响</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n1y22/a_post_on_llm_evaluation_decoding_strategies_and/</link>
      <description><![CDATA[嘿！我刚刚写了一篇关于大型语言模型 (LLM) 细微差别的博文，我想您会感兴趣的。  在其中，我讨论： - DeciLM-7B 和 Mistral-7B-v0.1 的详细比较。 - 不同的文本生成策略如何影响法学硕士。 - 用于法学硕士评估的新指令遵循基准 (IFEval)。 我相信这里的社区会对这些主题有宝贵的见解。  看看，我们来详细讨论一下！ [在此处阅读博客](https://deci.ai/ blog/llm-evaluation-and-how-decoding-strategies-impact-instruction-following/)。 ​   由   提交 /u/datascienceharp   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n1y22/a_post_on_llm_evaluation_decoding_strategies_and/</guid>
      <pubDate>Wed, 20 Dec 2023 18:45:46 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习说话 | ChatGPT：30 年历史</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n0r0f/how_neural_networks_learned_to_talk_chatgpt_a_30/</link>
      <description><![CDATA[       由   提交/u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n0r0f/how_neural_networks_learned_to_talk_chatgpt_a_30/</guid>
      <pubDate>Wed, 20 Dec 2023 17:57:32 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 自注意力的关键（上下文相关连接）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18n05nq/key_to_transformer_self_attention_context/</link>
      <description><![CDATA[   /u/keghn   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18n05nq/key_to_transformer_self_attention_context/</guid>
      <pubDate>Wed, 20 Dec 2023 17:33:07 GMT</pubDate>
    </item>
    <item>
      <title>关于实现具有交叉熵损失的 Softmax 输出层的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18muew7/question_about_implementing_a_softmax_output/</link>
      <description><![CDATA[嗨，NN 大师， 我正在使用这个存储库 (https://github.com/SnailWalkerYC/LeNet-5_Speed_Up）并尝试学习神经网络细节。该仓库用 C 和 CUDA 实现了 LeNet5。我现在关注的是 CPU 部分及其在 seq/ 中的代码。我迷失的一个特别地方是 seq/lenet.c 中的这个函数 ​ static inline void softmax(double input[OUTPUT], double loss [输出], int 标签, int 计数){ 双内部 = 0; for (int i = 0; i &lt; count; ++i){ double res = 0; for (int j = 0; j &lt; count; ++j){ res += exp(input[j] - input[i]); }损失[i] = 1. / res;内部 -= 损失[i] * 损失[i]； } 内部 += 损失[标签]; for (int i = 0; i &lt; count; ++i){ loss[i] *= (i == label) - loss[i] - 内部; } }  ​ 因为没有注释，所以我花了一些时间来理解这个函数。最后我发现它正在计算MSE损失函数相对于softmax层输入的导数。 然后我尝试将交叉熵损失函数与softmax一起使用，所以我出来了使用以下函数替换上面的函数。 static inline void softmax(double input[OUTPUT], double loss[OUTPUT], int label, int count) { double inside = 0;双最大输入=-INFINITY； // 找到最大输入值以防止数值不稳定 for (int i = 0; i &lt; count; ++i) { if (input[i] &gt; max_input) max_input = input[i]; } // 计算softmax和交叉熵损失 double sum_exp = 0; for (int i = 0; i &lt; count; ++i) { double exp_val = exp(input[i] - max_input); } sum_exp += exp_val;损失[i] = exp_val;双softmax_output[输出]; for (int i = 0; i &lt; count; ++i) { loss[i] /= sum_exp; softmax_output[i] = 损失[i]; } // 计算交叉熵损失和导数 inside = -log(softmax_output[label]); for (int i = 0; i &lt; count; ++i) { loss[i] = softmax_output[i] - (i == label); } } }  ​ 但是，使用我的 softmax() 函数版本，MNIST 识别不起作用。原始版本的准确率达到了 96% 以上。我的交叉熵损失代码有什么问题？ 请帮忙。 ​ 谢谢 ​ ;   由   提交 /u/bssrdf   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18muew7/question_about_implementing_a_softmax_output/</guid>
      <pubDate>Wed, 20 Dec 2023 13:19:17 GMT</pubDate>
    </item>
    <item>
      <title>寻找交易人工智能项目的合作实验室</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18mhkir/looking_for_colab_on_a_trading_ai_project/</link>
      <description><![CDATA[嗨，我正在开始一个基于 Python 的人工智能交易项目，目标是看看我们能创造出一个专注于股票交易的人工智能有多棒。这是一个开源项目，所以它更像是一个有趣的项目，我需要帮助。 谢谢托尼。    ;由   提交/u/Tonyhauf  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18mhkir/looking_for_colab_on_a_trading_ai_project/</guid>
      <pubDate>Wed, 20 Dec 2023 00:52:04 GMT</pubDate>
    </item>
    <item>
      <title>在本周的人工智能新闻中，字节跳动正在使用 OpenAI 的技术开发自己的竞争对手。 Mistral 推出了新的开源 AI 模型 Mixtral 8x7B 等</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18kqtk5/in_this_weeks_ai_news_bytedance_is_developing_its/</link>
      <description><![CDATA[https://open.substack.com/pub/neuralbyte/p/neuralbytes-weekly-ai-rundown-304?r=33qj5t&amp;utm_campaign=post&amp; ;utm_medium=web&amp;showWelcome=true   由   提交/u/Snoo_8366   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18kqtk5/in_this_weeks_ai_news_bytedance_is_developing_its/</guid>
      <pubDate>Sun, 17 Dec 2023 20:46:29 GMT</pubDate>
    </item>
    <item>
      <title>对于具有生物学背景的人来说，如何从头开始学习神经网络。请提供资源和建议</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18koa00/how_to_start_learning_neural_networks_from/</link>
      <description><![CDATA[我是一名物理治疗师 (24M)，想攻读运动科学硕士学位。未来，我渴望攻读神经生物学博士学位，并最终转向人工智能和神经网络。我知道这是一个非常艰难的领域，如果你想开始理解神经网络，你需要学习很多东西。我知道这是一个漫长的过程，需要很多年，但我想在学习和工作的同时开始这段旅程，这样我就能在未来的 5-7 年内做好准备。  请为我提供资源和链接以及路线图来追求我的这一兴趣。还可以向我提供任何替代建议或解决方案，或者任何你们认为可以帮助我更好地实现这一目标和我的职业生涯的建议   由   提交 /u/biocosmosian   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18koa00/how_to_start_learning_neural_networks_from/</guid>
      <pubDate>Sun, 17 Dec 2023 18:54:59 GMT</pubDate>
    </item>
    <item>
      <title>这种 Momentum GD 方法不起作用。帮助</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18jr0sc/this_momentum_gd_method_aint_working_help/</link>
      <description><![CDATA[       由   提交/u/imvedant04  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18jr0sc/this_momentum_gd_method_aint_working_help/</guid>
      <pubDate>Sat, 16 Dec 2023 13:16:52 GMT</pubDate>
    </item>
    <item>
      <title>Udemy 上值得考虑的最佳神经网络课程</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18hx0hy/best_neural_networks_courses_on_udemy_to_consider/</link>
      <description><![CDATA[       [链接]&lt; /a&gt; [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18hx0hy/best_neural_networks_courses_on_udemy_to_consider/</guid>
      <pubDate>Thu, 14 Dec 2023 01:29:44 GMT</pubDate>
    </item>
    <item>
      <title>公告：在塞浦路斯举办 HybridAIMS 研讨会</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18hiru0/announcement_hybridaims_workshop_in_cyprus/</link>
      <description><![CDATA[     &lt; /td&gt;  由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18hiru0/announcement_hybridaims_workshop_in_cyprus/</guid>
      <pubDate>Wed, 13 Dec 2023 15:04:05 GMT</pubDate>
    </item>
    <item>
      <title>隆重推出 DeciLM 7B：迄今为止最快、最准确的 7B LLM</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/18gq9wg/introducing_decilm_7b_the_fastest_and_most/</link>
      <description><![CDATA[   /u/nickb  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/18gq9wg/introducing_decilm_7b_the_fastest_and_most/</guid>
      <pubDate>Tue, 12 Dec 2023 16:46:48 GMT</pubDate>
    </item>
    </channel>
</rss>