<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Wed, 24 Jan 2024 09:14:12 GMT</lastBuildDate>
    <item>
      <title>与我的人工智能伙伴 Synthia 就神经网络的复杂性进行了一场精彩的对话。 🤖✨ 查看我最新文章中的见解和问答环节。让我们一起来揭开AI的奥秘吧！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19dgjxy/engaging_in_a_fascinating_conversation_with/</link>
      <description><![CDATA[   本文采用了一种独特的方法，通过想象的神经网络进行问答环节网络。我们不会通过传统的视角深入研究错综复杂的技术，而是将神经网络拟人化，邀请它阐明其内部工作原理，揭开其决策过程的神秘面纱，并揭示其存在的细微差别。通过这种富有想象力的对话，我们的目标是以一种令人耳目一新的独特方式揭开神经网络的秘密，为读者提供一个深入而平易近人的视角来了解人工智能的迷人世界。 &lt;!-- SC_ON - -&gt;  由   提交/u/ardesai1907  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19dgjxy/engaging_in_a_fascinating_conversation_with/</guid>
      <pubDate>Tue, 23 Jan 2024 05:00:15 GMT</pubDate>
    </item>
    <item>
      <title>突然validation_loss下降到零</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19deubj/suddenly_validation_loss_drops_to_zero/</link>
      <description><![CDATA[      有人见过这样的val_dice曲线吗？ 真的不合理，max_epoch=100learning_reate=8e-4，不涉及lr_scheduler。 除了验证，训练过程也是这样，train_loss突然激增。 大家有什么想法或者建议吗？谢谢大家。 https://preview.redd.it/4nl5qakly3ec1.png?width=576&amp;format=png&amp;auto=webp&amp;s=43307a87e91072394dcc369b1dbe2f2308fdad7c ​ &lt; p&gt;https://preview.redd.it/7wbvlnu3z3ec1。 png?width=567&amp;format=png&amp;auto=webp&amp;s=b93e45020116da1dd26140559796e7abeda79346   由   提交/u/No-Supermarket-2567   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19deubj/suddenly_validation_loss_drops_to_zero/</guid>
      <pubDate>Tue, 23 Jan 2024 03:27:05 GMT</pubDate>
    </item>
    <item>
      <title>任何人都可以解释（简单地）在有关人工神经网络中的多模态神经元的页面上看到的图像吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19bzp25/can_anyone_explain_in_simple_terms_the_images/</link>
      <description><![CDATA[我了解神经网络的基础知识 - 输入/输出层、隐藏层、权重、偏差等。基本了解。这是一个令人着迷的主题，所以我一直在尝试阅读更多内容，我发现这些页面非常有趣，但我无法理解它们所描述的内容： 人工神经网络中的多模态神经元 https://distill.pub/2021/multimodal-neurons/ &lt; a href=&quot;https://openai.com/research/multimodal-neurons&quot;&gt;https://openai.com/research/multimodal-neurons 这些页面上有图像（在我看来）就像 LSD 幻象和迷幻艺术。 任何人都可以解释一下（用简单的术语）：  什么是多模态神经元？ （在这种情况下，“神经元”是什么意思，等等） 这些页面上的奇怪图像到底向我们展示了什么？我不明白那些奇怪的图像想告诉我们什么。    由   提交 /u/papa_libra   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19bzp25/can_anyone_explain_in_simple_terms_the_images/</guid>
      <pubDate>Sun, 21 Jan 2024 09:28:23 GMT</pubDate>
    </item>
    <item>
      <title>采访麻省理工学院林肯实验室的 Zack Serlin：正式方法......</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</link>
      <description><![CDATA[       由   提交/u/Neurosymbolic  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19bke9d/interview_with_zack_serlin_mit_lincoln/</guid>
      <pubDate>Sat, 20 Jan 2024 20:00:19 GMT</pubDate>
    </item>
    <item>
      <title>人工神经网络中的类脑学习：综述</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2305.11252 摘要：  人工神经网络（ANN）已成为以下领域的重要工具：机器学习，在图像和语音生成、游戏和机器人等多个领域取得了显着的成功。然而，人工神经网络的运行机制与生物大脑的运行机制之间存在根本差异，特别是在学习过程方面。本文对当前人工神经网络中的类脑学习表示进行了全面回顾。我们研究了更多生物学上合理的机制的整合，例如突触可塑性，以增强这些网络的能力。此外，我们深入研究了这种方法的潜在优势和挑战。最终，我们为这个快速发展的领域的未来研究找到了有希望的途径，这可以让我们更接近理解智能的本质。     ;由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19amfdl/braininspired_learning_in_artificial_neural/</guid>
      <pubDate>Fri, 19 Jan 2024 16:03:08 GMT</pubDate>
    </item>
    <item>
      <title>温度、Top-k 和 Top-p 解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/19ahr5a/temperature_topk_and_topp_explained/</link>
      <description><![CDATA[   /u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/19ahr5a/temperature_topk_and_topp_explained/</guid>
      <pubDate>Fri, 19 Jan 2024 12:14:04 GMT</pubDate>
    </item>
    <item>
      <title>全连接层。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1997h71/full_connected_layers/</link>
      <description><![CDATA[我正在尝试学习神经网络，但目前不了解很多数学知识，所以我有一个问题要问那些知道自己的知识的人关于.我在 python 中使用的神经网络是完全连接的，显然这很好，但它违背了我的直觉，也违背了我认为神经网络的好处。当然，拥有许多不同类型的连接可以存储更复杂的信息。   由   提交/u/Unlucky_Culture_6996   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1997h71/full_connected_layers/</guid>
      <pubDate>Wed, 17 Jan 2024 20:55:24 GMT</pubDate>
    </item>
    <item>
      <title>输入复杂度和所需的隐藏层之间到底有什么关系？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1995sxc/what_exactly_is_the_relationship_between_input/</link>
      <description><![CDATA[ 由   提交/u/swampshark19  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1995sxc/what_exactly_is_the_relationship_between_input/</guid>
      <pubDate>Wed, 17 Jan 2024 19:49:17 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 闪电模型只需很少的代码行即可完成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/</link>
      <description><![CDATA[      我刚刚建立了一个Python库来帮助构建PyTorch闪电模型只需很少的代码行。我很想听听您的想法！ https://github.com/brianrisk/lightning_factory&lt; /p&gt; 闪电工厂概览&lt; /p&gt; ​ ​   由   提交/u/qwaver-io  /u/qwaver-io  reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1990pgj/pytorch_lightning_models_made_with_very_few_lines/</guid>
      <pubDate>Wed, 17 Jan 2024 16:30:56 GMT</pubDate>
    </item>
    <item>
      <title>大脑连接性突破：在不同物种中发现相似的神经网络模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/198zhok/brain_connectivity_breakthrough_similar_neural/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/198zhok/brain_connectivity_breakthrough_similar_neural/</guid>
      <pubDate>Wed, 17 Jan 2024 15:42:08 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/198dh3l/transformers_are_multistate_rnns/</link>
      <description><![CDATA[ 由   提交/u/nickb  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/198dh3l/transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 20:48:21 GMT</pubDate>
    </item>
    <item>
      <title>人工智能和计算机视觉解决方案预算实用指南 |第 1 部分 硬件</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/</link>
      <description><![CDATA[      ​&lt; /p&gt; https://preview.redd。 it/efs28iav6ucc1.jpg?width=2800&amp;format=pjpg&amp;auto=webp&amp;s=0ae07562cdd592cbb203f11df5ccbd78abf83213 关于计算机视觉定价的好文章。希望您能顺利找到它。 简短描述： 2024年，随着越来越多的公司融入人工智能，许多企业主面临着挑战。借助 OpenCV.ai 的专家见解，了解将 AI 集成到您的业务中的基本注意事项。从为计算机视觉解决方案选择合适的相机到导航不同的计算平台，文章提供了实用指导。探索网络和电源优化的细微差别，迈出人工智能驱动成功的第一步。在本系列文章中，我们将指导您了解从硬件和软件选择到人工智能法律方面的所有要点。让我们从第 1 部分开始 |硬件。   由   提交/u/No-Independence5880   /u/No-Independence5880 reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/&quot;&gt;[链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1988ehe/practical_guides_to_budget_your_ai_and_computer/</guid>
      <pubDate>Tue, 16 Jan 2024 17:26:04 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 是多状态 RNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1986kc1/transformers_are_multistate_rnns/</link>
      <description><![CDATA[论文：https:// arxiv.org/abs/2401.06104 代码：https ://github.com/schwartz-lab-NLP/TOVA 摘要：  Transformers 被认为在概念上与到上一代最先进的 NLP 模型 - 循环神经网络 (RNN)。在这项工作中，我们证明了仅解码器 Transformer 实际上可以被概念化为无限多状态 RNN——一种具有无限隐藏状态大小的 RNN 变体。我们进一步证明，通过固定隐藏状态的大小，预训练的 Transformer 可以转换为有限多状态 RNN。我们观察到一些现有的转换器缓存压缩技术可以被构建为这样的转换策略，并引入了一种新的策略，TOVA，它比这些策略更简单。我们对多个远程任务进行的实验表明，TOVA 优于所有其他基线策略，同时几乎与完整（无限）模型相当，并且在某些情况下仅使用原始缓存大小的 1/8。我们的结果表明，变压器解码器 LLM 在实践中通常表现为 RNN。他们还提出了缓解最痛苦的计算瓶颈之一——缓存大小的选项。我们在 此 https URL 公开发布我们的代码。    由   提交 /u/APaperADay   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1986kc1/transformers_are_multistate_rnns/</guid>
      <pubDate>Tue, 16 Jan 2024 16:12:45 GMT</pubDate>
    </item>
    <item>
      <title>遗传算法的前馈网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196s6wq/feedfoward_network_with_genetic_algorithm/</link>
      <description><![CDATA[大家好！我正在尝试创建一个神经网络来在赛道上行驶，但它太难了，比我想象的要难。我使用遗传算法和具有 2 个隐藏层的神经网络，每个隐藏层有 10 个隐藏神经元，为了选择最好的，我使用波前算法。我在 Unity 中编码，这是我的 github 存储库。如果您能帮助我，我将不胜感激： https://github.com/lucasramosdev/self-driven -汽车  ​   由   提交 /u/Proscrite   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196s6wq/feedfoward_network_with_genetic_algorithm/</guid>
      <pubDate>Sun, 14 Jan 2024 22:26:07 GMT</pubDate>
    </item>
    <item>
      <title>科学家展示了大脑使用的浅层学习机制如何与深度学习竞争</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/196j4du/scientists_show_how_shallow_learning_mechanism/</link>
      <description><![CDATA[   /u/SparklySpencer  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/196j4du/scientists_show_how_shallow_learning_mechanism/</guid>
      <pubDate>Sun, 14 Jan 2024 16:03:09 GMT</pubDate>
    </item>
    </channel>
</rss>