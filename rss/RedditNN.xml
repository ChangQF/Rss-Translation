<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的子版块。</description>
    <lastBuildDate>Sat, 07 Sep 2024 01:09:49 GMT</lastBuildDate>
    <item>
      <title>您知道我可以在哪里找到详尽的人工智能研究术语列表吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f9j6k7/any_idea_on_where_i_could_find_an_exhaustive_list/</link>
      <description><![CDATA[我希望找到一份详尽的术语列表。包括人工智能研究中提到的术语。    提交人    /u/EconomyPumpkin2050   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f9j6k7/any_idea_on_where_i_could_find_an_exhaustive_list/</guid>
      <pubDate>Thu, 05 Sep 2024 10:34:09 GMT</pubDate>
    </item>
    <item>
      <title>GameNGen：谷歌基于神经网络的游戏引擎</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f7uejh/gamengen_googles_neural_network_based_gaming/</link>
      <description><![CDATA[Google 刚刚发布了 GameNGen，这是一种基于神经网络的架构，用于生成游戏模拟，目前正在对 DOOM 进行训练。请在此处查看其详细信息：https://youtu.be/n-4zb8FdptQ?si=IiPNaCJBX_Y1_4ZH    提交人    /u/mehul_gupta1997   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f7uejh/gamengen_googles_neural_network_based_gaming/</guid>
      <pubDate>Tue, 03 Sep 2024 08:00:54 GMT</pubDate>
    </item>
    <item>
      <title>每个神经元是否与前一层的所有神经元相连？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f7aa3z/is_each_neuron_connected_to_all_the_neurons_in/</link>
      <description><![CDATA[例如，每个神经元的值是否来自 (权重 1 * 前一个神经元 1) + (权重 2 * 前一个神经元 2) + (权重 3 * 前一个神经元 3) + ... + 偏差？    提交人    /u/_licketysplit_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f7aa3z/is_each_neuron_connected_to_all_the_neurons_in/</guid>
      <pubDate>Mon, 02 Sep 2024 15:58:08 GMT</pubDate>
    </item>
    <item>
      <title>机器学习和编程爱好者，需要见解！</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f5xn76/ml_and_programming_enthusiasts_insights_needed/</link>
      <description><![CDATA[大家好， 我正在寻求帮助，完成我的硕士论文研究，该研究探讨了模型预测控制 (MPC) 在 HVAC 系统中的作用。如果您从事 HVAC、工程或建筑工作，您的意见将非常有帮助。 我整理了一份简短的调查问卷 - 总共 15 个问题 - 只需几分钟即可完成。 https://qualtricsxmnpnnrmzlm.qualtrics.com/jfe/form/SV_ddnRqNzH4FNH7Su 无论您对 MPC 非常熟悉还是仅有一般了解，您的回答都很有价值。非常感谢您提供的任何意见。    提交人    /u/Less_Organization190   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f5xn76/ml_and_programming_enthusiasts_insights_needed/</guid>
      <pubDate>Sat, 31 Aug 2024 21:10:09 GMT</pubDate>
    </item>
    <item>
      <title>Tri-Gram 神经网络故障排除</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f4nqy6/trigram_neural_network_troubleshooting/</link>
      <description><![CDATA[]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f4nqy6/trigram_neural_network_troubleshooting/</guid>
      <pubDate>Fri, 30 Aug 2024 05:39:15 GMT</pubDate>
    </item>
    <item>
      <title>谷歌，你这是在搞怪吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f3uhgh/google_are_you_trolling/</link>
      <description><![CDATA[        提交人    /u/Historical-Dog-2550   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f3uhgh/google_are_you_trolling/</guid>
      <pubDate>Thu, 29 Aug 2024 05:08:53 GMT</pubDate>
    </item>
    <item>
      <title>使用均值中心化还是最小最大中心化来规范用户评分？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f3e42a/mean_centering_or_minmax_centering_for/</link>
      <description><![CDATA[我遇到了两种规范化项目用户评分的方法，如果不进行面对面的比较，我真的不知道如何比较它们。这两种方法是均值中心化和最小-最大中心化。 你有答案吗？或者如果你知道更好的或其他行之有效的方法，可以和我分享吗？ 谢谢！    提交人    /u/kotvic_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f3e42a/mean_centering_or_minmax_centering_for/</guid>
      <pubDate>Wed, 28 Aug 2024 16:08:31 GMT</pubDate>
    </item>
    <item>
      <title>关于学习神经网络的更多信息，有没有什么书籍/资源推荐？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f3e1w5/bookresource_recommendations_for_learning_more/</link>
      <description><![CDATA[大家好， 我一直在尝试自学更多关于神经网络的知识，我正在寻找有关该主题的综合指南或书籍。没有“傻瓜神经网络”指南，亚马逊上的每本书都是关于如何构建自己的网络。我一直在阅读一些 ML 论文，并且知道我需要更多地了解神经网络。如果你们中的任何人可以推荐任何资源，我将不胜感激！！！！ 谢谢大家。 TLDR；请推荐任何全面的资源来帮助我了解神经网络 - 这将对更好地理解 ML 论文非常有帮助。    提交人    /u/Adept_Investigator_9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f3e1w5/bookresource_recommendations_for_learning_more/</guid>
      <pubDate>Wed, 28 Aug 2024 16:06:09 GMT</pubDate>
    </item>
    <item>
      <title>帮助 LSTM 进行批处理</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f28uv1/help_with_batching_for_an_lstm/</link>
      <description><![CDATA[嗨，我是深度学习的新手，我想学习如何为 LSTM 批量处理数据。我的问题是我有多个数据集，具体来说是 10 个，每个数据集都是来自同一实验的不同试验的数据。每个数据集的长度为 2880 x 5（4 个输入，1 个输出）。我怎样才能让 LSTM 知道每个序列都是不同的试验？训练数据和测试数据的分离过程是怎样的？如果您需要更多信息，请告诉我。提前谢谢您    提交人    /u/CarelessJellyfish9   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f28uv1/help_with_batching_for_an_lstm/</guid>
      <pubDate>Tue, 27 Aug 2024 04:51:18 GMT</pubDate>
    </item>
    <item>
      <title>哪一个主观上看起来最有趣</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f28t2k/which_one_subjectively_looks_the_most_interesting/</link>
      <description><![CDATA[        提交人    /u/DesmosGrapher314   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f28t2k/which_one_subjectively_looks_the_most_interesting/</guid>
      <pubDate>Tue, 27 Aug 2024 04:48:15 GMT</pubDate>
    </item>
    <item>
      <title>受限玻尔兹曼机 RBM 1</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1f24o3b/restricted_boltzmann_machines_rbm_1/</link>
      <description><![CDATA[        提交人    /u/keghn   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1f24o3b/restricted_boltzmann_machines_rbm_1/</guid>
      <pubDate>Tue, 27 Aug 2024 01:09:40 GMT</pubDate>
    </item>
    <item>
      <title>寻找深度学习资源来掌握 CNN</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ezxmmz/looking_for_deep_learning_resources_to_master_cnns/</link>
      <description><![CDATA[大家好， 我是一名博士生，拥有分析学硕士学位，专注于计算数据科学，在数学和统计学方面有很强的背景。 目前，我正在深入研究 CNN，这是我自学的一部分，同时准备选择一个论文主题。我对神经网络有相当的了解，目前正在研究流行的 CNN 架构，如 AlexNet 和 GoogleNet，对它们进行编码以了解它们的工作原理，并了解某些架构优于其他架构的原因。 我主要在寻找深入研究 CNN 的研究论文，但如果有一本非常好的书，我也会接受。任何关于下一步该看什么的建议都很棒。    提交人    /u/Joergyll   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ezxmmz/looking_for_deep_learning_resources_to_master_cnns/</guid>
      <pubDate>Sat, 24 Aug 2024 05:11:52 GMT</pubDate>
    </item>
    <item>
      <title>类似这样的问题是如何解决的？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1eznyeh/how_are_problems_like_this_solved/</link>
      <description><![CDATA[该神经网络的准确率从未超过 0.667。这类问题通常是如何解决的？ from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential import numpy as np inputs = [ [1], [2], [3], ] outputs = [ [0], [1], [0] ] x_train = np.array(inputs) y_train = np.array(outputs) model = Sequential() model.add(Dense(1000, &quot;sigmoid&quot;)) model.add(Dense(1000, &quot;sigmoid&quot;)) model.add(Dense(1, &quot;sigmoid&quot;)) model.compile(&quot;adam&quot;, &quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;]) history = model.fit(x_train, y_train, epochs=1000) 我认为发生这种情况是因为输入和输出的性质（输入：1、2、3，而输出是 0,1,0)，结果相互矛盾。但这在构建神经网络时非常常见，所以我想知道这个问题通常是如何解决的。    提交人    /u/jaroslavtavgen   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1eznyeh/how_are_problems_like_this_solved/</guid>
      <pubDate>Fri, 23 Aug 2024 21:12:27 GMT</pubDate>
    </item>
    <item>
      <title>torch.argmin() 非可微性解决方法</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ez2zi8/torchargmin_nondifferentiability_workaround/</link>
      <description><![CDATA[我正在实现一个基于地形约束的神经网络层。该层可以被认为类似于 2D 网格图，或者基于深度学习的自组织图。它由 4 个参数组成，即高度、宽度、潜在维数和 p 范数（用于距离计算）。每个单元/神经元的维数等于潜在维数。此类的最简代码如下： class Topography(nn.Module): def __init__( self, latent_dim:int = 128, height:int = 20, width:int = 20, p_norm:int = 2 ): super().__init__() self.latent_dim = latent_dim self.height = height self.width = width self.p_norm = p_norm # 创建包含索引 2D 坐标的 2D 张量 locs = np.array(list(np.array([i, j]) for i in range(self.height) for j in range(self.width))) self.locations = torch.from_numpy(locs).to(torch.float32) del locs # 线性层的可训练权重- self.lin_wts = nn.Parameter(data = torch.empty(self.height * self.width, self.latent_dim), require_grad = True) # 高斯初始化，平均值 = 0 且 std-dev = 1 / sqrt(d)- self.lin_wts.data.normal_(mean = 0.0, std = 1 / np.sqrt(self.latent_dim)) def forward(self, z): # L2 标准化 &#39;z&#39; 将其转换为单位向量- z = F.normalize(z, p = self.p_norm, dim = 1) # 每个输入到所有 SOM 单元的成对平方 L2 距离（L2 范数距离）- pairwise_squaredl2dist = torch.square( torch.cdist( x1 = z, # 还将所有 lin_wts 转换为单位向量- x2 = F.normalize(input = self.lin_wts, p = self.p_norm, dim = 1), p = self.p_norm ) ) # 对于每个输入 zi，计算“lin_wts”中的最近单元 - nearest_indices = torch.argmin(pairwise_squaredl2dist, dim = 1) # 获取 2D 坐标索引 - nearest_2d_indices = self.locations[closest_indices] # 计算最近单元和其他每个单元之间的 L2 距离 - l2_dist_squared_topo_neighb = torch.square(torch.cdist(x1 = nearest_2d_indices.to(torch.float32), x2 = self.locations, p = self.p_norm)) del nearest_indices, nearest_2d_indices return l2_dist_squared_topo_neighb, pairwise_squaredl2dist  对于给定的输入“z”（比如编码器的输出） ViT/CNN），它计算距离最近的单元，然后使用径向基函数核/高斯（逆）函数在该最近单元周围创建地形结构——在下面的&quot;topo_neighb&quot; 张量中完成。 由于&quot;torch.argmin()&quot;给出类似于独热编码向量的索引，这些向量根据定义是不可微的，我正在尝试创建一个解决方法： # 2D 单元数 - height = 20 width = 20 # 每个单元的维数指定为 - latent_dim = 128 # 使用 L2-norm 进行距离计算 - p_norm = 2 topo_layer = Topography(latent_dim = latent_dim, height = height, width = width, p_norm = p_norm) optimizer = torch.optim.SGD(params = topo_layer.parameters(), lr = 0.001, influence = 0.9) batch_size = 1024 # 创建一个输入向量 - z = torch.rand(batch_size, latent_dim) l2_dist_squared_topo_neighb, pairwise_squaredl2dist = topo_layer(z) # l2_dist_squared_topo_neighb.size(), pairwise_squaredl2dist.size() # (torch.Size([1024, 400]), torch.Size([1024, 400])) curr_sigma = torch.tensor(5.0) # 计算相对于最近单元的高斯拓扑邻域结构- topo_neighb = torch.exp(torch.div(torch.neg(l2_dist_squared_topo_neighb), ((2.0 * torch.square(curr_sigma)) + 1e-5))) # 计算地形损失- loss_topo = (topo_neighb * pairwise_squaredl2dist).sum(dim = 1).mean() loss_topo.backward() optimizer.step()  现在，成本函数的值发生了变化，减少。此外，作为健全性检查，我正在记录“topo_layer.lin_wts”的 L2 范数，以反映其权重正在使用梯度进行更新。 这是正确的实现，还是我遗漏了什么？    提交人    /u/grid_world   [link] [comments]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ez2zi8/torchargmin_nondifferentiability_workaround/</guid>
      <pubDate>Fri, 23 Aug 2024 03:37:43 GMT</pubDate>
    </item>
    <item>
      <title>神经网络初始化 - 随机 x 结构化</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1evw88a/neural_network_initialization_random_x_structured/</link>
      <description><![CDATA[我在 ANN 领域还没有那么多经验，所以我希望这个问题没有完全偏离图表 :) 我发现神经网络用随机值初始化其权重和偏差，以确保这些值不会在相同或对称的值上初始化。 我完全理解为什么它们不能相同 - 除了一个节点之外的所有节点都是多余的。 我无法理解的是为什么它们不能是对称的。我在 YouTube 上没有找到关于它的一个视频，当我一直问为什么不这样做时，GPT 低调地告诉我，如果你有一系列相关权重（假设为 -10 到 10），那么实际上最好将它们初始化得尽可能远，而不是使用其中一种随机算法。 GPT 提到的唯一问题是完全分离的节点的交付。 谁能向我解释为什么每个人都使用随机初始化？    提交人    /u/kotvic_   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1evw88a/neural_network_initialization_random_x_structured/</guid>
      <pubDate>Mon, 19 Aug 2024 08:00:12 GMT</pubDate>
    </item>
    </channel>
</rss>