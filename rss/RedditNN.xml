<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络，深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的subreddit。</description>
    <lastBuildDate>Fri, 28 Feb 2025 09:18:28 GMT</lastBuildDate>
    <item>
      <title>如何使用卷积神经网络对疟疾细胞进行分类</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1izmrzi/how_to_classify_malaria_cells_using_convolutional/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;    本教程提供了一份逐步指南，介绍了如何使用TensorFlow和Keras实施和培训CNN模型进行疟疾细胞分类。                      数据准备中的数据准备，您可以在此部分进行培训，并在此部分下载数据，并准备该数据供应数据，并准备该数据供您进行数据准备。这涉及到诸如准备数据，分类为训练和测试集，以及必要时进行数据扩展。       cnn模型构建和培训 -  在第二部分中，您将专注于建立卷积神经网络（CNN）模型，用于用于疟疾细胞的二元分类。这包括模型自定义，定义层和使用准备好的数据训练模型。     模型测试和预测 -  最终部分涉及使用以前从未见过的新图像测试训练的模型。您将加载已保存的模型并使用它来对此新图像进行预测以确定是否感染。       您可以在博客中找到代码的链接：      href =“ https://medium.com/@feitgemel/how-to-complasify-malaria-cells-using-convolutional-neur-network-c00859bc6b46”&gt; https://medium.com/@feitgemel/how-to-classify-malaria-cells-using-convolutional-neurner-network-c00859bc6b46     您可以找到更多教程，并在此处加入我的新闻通讯： https://eranfeit.net/                 href =“ https://youtu.be/wlpuw3ggpqo＆amp;list=uulftiwjjhah6bviswkljum9sg”&gt; https://youtu.be/youtu.be/wlpuw3ggppqo&amp;/ wlppuppqo&amp;list=ulist=uulfftiiwjjjjjjjhah6bvis     在提交由＆＃32; /u/feitgemel     [link]   [注释]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1izmrzi/how_to_classify_malaria_cells_using_convolutional/</guid>
      <pubDate>Thu, 27 Feb 2025 18:30:29 GMT</pubDate>
    </item>
    <item>
      <title>稳定垃圾邮件：增强梯度归一化，以进行更高效的4位LLM训练</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1izcdva/stablespam_enhanced_gradient_normalization_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  一种新方法结合了尖峰感动的动量和优化的4位量化，以启用比16位亚当的稳定训练，同时使用明显更少的内存。关键的创新是通过仔细的梯度监控在低度训练期间检测和预防优化的不稳定性。 主要技术点： - 引入尖峰 - 尖峰 - 施加 spike -avare动量重置，以梯度统计量，以调查潜在的不稳定性 - 检测潜在的不稳定性 - 使用动态调整量表的量表量尺度，以量表为基础量表，以4位的量度为基础量表 - 该量度的量表 - 以4位的量度为基础量表 - 该量表 - 以4位的量级为基础量表 - 该量表 - 以4位的量级为基础量表 - 保持重量和梯度量化量表的单独跟踪 - 与现有的优化器和体系结构兼容 关键结果： - 匹配或超过16位的ADAM性能，同时使用75％的记忆力 - 成功地训练Bert -large到4位精确的4位精确培训 - 从1E -4到1E -4的学习率稳定培训 - 在1e -4的范围内显示出稳定的培训 - 参数 我认为这对于使ML研究民主化可能会产生影响。培训大型模型目前需要大量的GPU资源，并且能够以4位精确的精度进行培训，而无需牺牲稳定性或准确性，这可能会使研究预算有限的实验室更容易获得研究。 我认为，Spike-Aware Momentum Reset Reset Technique    可以证明，不仅可以证明一种不仅能够改善最优先级的其他情况&lt;其他情况&gt;其他情况&lt;其他情况&gt; &lt;其他情况&lt;&lt;其他情况。方法可以通过仔细的动量管理和优化的量化来实现稳定的4位模型训练，将16位性能与少75％的内存使用量匹配。   full unshore   。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1izcdva/stablespam_enhanced_gradient_normalization_for/</guid>
      <pubDate>Thu, 27 Feb 2025 10:03:19 GMT</pubDate>
    </item>
    <item>
      <title>Anyon可以推荐一些最佳初学者友好的卷积神经网络教程，这将导致智能照明系统</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iyoe8b/can_anyon_recommend_some_of_the_best/</link>
      <description><![CDATA[＆＃32;提交由＆＃32; /u/u/arnoldpaclarinjr     [link]       [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iyoe8b/can_anyon_recommend_some_of_the_best/</guid>
      <pubDate>Wed, 26 Feb 2025 14:11:46 GMT</pubDate>
    </item>
    <item>
      <title>偏爱感知的LLM框架，用于事实基础营销内容生成</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iyk47b/preferenceaware_llm_framework_for_factgrounded/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员提出了一个新框架，用于生成营销内容，以保持说服力和事实准确性之间保持平衡。 The core innovation is a two-stage architecture combining a retrieval module for product specifications with a controlled generation approach. Key technical components: - Grounded generation module that references source product specifications during content creation - Persuasion scoring mechanism measuring effectiveness across multiple marketing dimensions - Fact alignment checker comparing generated content against source material - Novel将50,000个产品描述与相应的营销材料结合的数据集 结果显示：-23％的说服力提高了基线模型（通过人类评估测量）-91％的事实准确性在合并产品规格时保持了实际的事实准确性 - 合并产品规格的大量降低幻觉降低了幻觉产品的幻觉效果，与标准LLM的销售方法相比，可以自动销售自动流动 - 在自动销售中保持自然的影响 我认为，最有趣的技术方面是它们如何处理创意营销语言和事实约束之间的权衡。检索功能的方法可能有可能应用于需要创造力和准确性的其他领域。  tldr：AI营销内容生成的新框架，可保持事实准确性，同时优化说服力，显示出23％的提高有效性，同时保持91％的事实准确性。     完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]     [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iyk47b/preferenceaware_llm_framework_for_factgrounded/</guid>
      <pubDate>Wed, 26 Feb 2025 09:59:41 GMT</pubDate>
    </item>
    <item>
      <title>测试时间缩放方法在数学推理任务中显示出有限的多语言概括</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ixtf5q/testtime_scaling_methods_show_limited/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键洞察力是使用测试时间缩放来改善跨多种语言的数学推理，而无需重新训练模型。 The researchers apply this technique to competition-level mathematics problems that go well beyond basic arithmetic. Main technical points: - Test-time scaling involves generating multiple solution attempts (5-25) and selecting the most consistent answer - Problems were carefully translated to preserve mathematical meaning while allowing natural language variation - Evaluation used competition-level problems including algebra, geometry, and proofs - Performance gains were consistent across all tested语言 - 特别注意维持数学符号一致性 关键结果： - 测试时间缩放在所有问题类型和语言中的准确性提高了准确性 - 改进在多步推理问题中最为明显的效果 - 性能提高 - 缩放的缩放相似，无论源语言的质量如何，对数学推理能力                                I think the methodological contribution here - showing that test-time scaling works consistently across languages - is particularly valuable for developing multilingual mathematical AI systems. The limitations around cultural mathematical contexts and translation edge cases suggest interesting directions for future work. TLDR: Test-time scaling improves mathematical在竞争级别的问题上证明，跨语言始终如一地推理。  完整的总结在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]      [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ixtf5q/testtime_scaling_methods_show_limited/</guid>
      <pubDate>Tue, 25 Feb 2025 12:05:01 GMT</pubDate>
    </item>
    <item>
      <title>负责人AI的课程材料</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iwely0/course_materials_for_responsible_ai/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  大家好，我目前正在设计负责人AI 的课程，我想寻求帮助，以找到课程内容，任何大学课程或您认为相关的任何大学课程或研究的良好免费材料，请分享。提交由＆＃32; /u/u/over_reward9875    href =“ https://www.reddit.com/r/neuralnetworks/comments/comments/1iwely0/course_materials_for_responsible_ai/”&gt; [link]   ＆＃32;   [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iwely0/course_materials_for_responsible_ai/</guid>
      <pubDate>Sun, 23 Feb 2025 16:56:17 GMT</pubDate>
    </item>
    <item>
      <title>辍学解释了</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iwcdm8/dropout_explained/</link>
      <description><![CDATA[       ＆＃32;提交由＆＃32; /u/u/u/sersion-trainer-541       [注释]         ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iwcdm8/dropout_explained/</guid>
      <pubDate>Sun, 23 Feb 2025 15:18:28 GMT</pubDate>
    </item>
    <item>
      <title>CNN和Tensorboard的新手</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iw5mib/new_to_cnns_and_tensorboard/</link>
      <description><![CDATA[       &lt;！ -  sc_off-&gt;  开始学习如何训练CNN，好奇，如果val_accranacy中的初始尖峰是正常的，或者如果峰值是正常的，那么如果Spike drop Drop表示某种过度贴身或某种东西？我本来可以肯定的是，如果Val_accuracy保持较低，但随着模型继续训练，似乎会逐渐增加。这也可以是过度拟合验证数据的模型吗？我正在使用每班大约1500张图像的数据集。谢谢！ 〜一个试图学习cnns   &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/stkoopchoop      [注释]           ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iw5mib/new_to_cnns_and_tensorboard/</guid>
      <pubDate>Sun, 23 Feb 2025 08:32:09 GMT</pubDate>
    </item>
    <item>
      <title>多模式奖励基地：评估视觉模型奖励功能的综合基准</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iw4bnl/multimodal_rewardbench_a_comprehensive_benchmark/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文介绍了多模式奖励贝恩，这是视觉奖励模型的全面评估框架。框架测试使用超过2,000个测试用例，包括准确性，偏差检测，安全考虑和鲁棒性，包括 关键技术要点： - 使用标准化指标跨越多个功能：响应质量，事实准确，偏见，跨模态理解方法 - 进行多种功能 - 评估6个显着的奖励模型 - 评估6个突出的奖励模型 - 用于评估6个突出的奖励模型 -  performance - Identifies specific failure modes in current models Main results: - Models show strong performance (&gt;80%) on basic text evaluation - Cross-modal understanding scores drop significantly (~40-60%) - High variance in safety/bias detection (30-70% range) - Inconsistent performance across different content types - Most models struggle with complex reasoning tasks involving both modalities I think this work highlights当前奖励模型功能的关键差距，尤其是在处理多模式内容方面。基准可以帮助我们标准化我们如何评估这些模型并推动安全和偏见检测等领域的改进。 我认为，最有价值的贡献是揭露特定的故障模式 - 准确地表明当前模型的位置短缺有助于集中于未来的研究工作。结果表明，我们需要从根本上使用新的方法来处理奖励模型中的跨模式内容。  tldr：新的基准测试标准揭示了视觉奖励模型处理复杂多模式任务的能力，尤其是在安全性和偏见检测中。提供了改进的清晰指标。  在这里。纸在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iw4bnl/multimodal_rewardbench_a_comprehensive_benchmark/</guid>
      <pubDate>Sun, 23 Feb 2025 07:00:22 GMT</pubDate>
    </item>
    <item>
      <title>Chase：使用LLMS自动生成硬评估问题的框架</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1ivd5gj/chase_a_framework_for_automated_generation_of/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  使LLMS生成挑战性问题的新框架研究如何系统地创建高质量的测试问题。核心方法论通过明确提示策略使用迭代性自我测试和有针对性的难度校准。 关键的技术组成部分： - 具有中间验证的多阶段生成过程 - 自我评估循环，LLM批评其自身的输出 - 通过参数提示 - 交叉 -  valify -pp  我认为这项工作为开发更好的评估数据集提供了实用的基础。产生校准难度水平的能力可以更精确地帮助基准模型功能。尽管当前的实施使用GPT-4，但原理应扩展到其他LLM。 系统产生问题的系统方法似乎是迈向更严格的测试方法的重要一步。但是，我看到了一些关于将其扩展到非常大的数据集并确保不同领域的质量一致的问题。  tldr：新方法演示了如何通过自我测试和迭代的改进来使LLMS产生更好的测试问题，并具有可衡量的问题质量和难度校准。 href =“ https://aimodels.fyi/papers/arxiv/how-to-to-get-get-your-llm-to-generate”&gt;完整的摘要在这里。纸在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1ivd5gj/chase_a_framework_for_automated_generation_of/</guid>
      <pubDate>Sat, 22 Feb 2025 07:12:27 GMT</pubDate>
    </item>
    <item>
      <title>通过对比度学习从时间序列数据中学习内在的神经表示</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iunfqx/learning_intrinsic_neural_representations_from/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  研究人员提出了一种对比度学习方法，将神经活动动态映射到几何表示，提取他们所说的“柏拉图式”人群级神经记录的形状。 The method combines temporal embedding with geometric constraints to reveal fundamental organizational principles. Key technical aspects: - Uses contrastive learning on neural time series data to learn low-dimensional embeddings - Applies topological constraints to enforce geometric structure - Validates across multiple neural recording datasets from different species - Shows consistent emergence of basic geometric patterns (spheres, tori, etc.) - Demonstrates robustness across different neural population sizes and brain regions Results demonstrate: - Neural populations naturally organize into geometric manifolds - These geometric patterns are preserved across different timescales - The representations emerge consistently in both task and spontaneous activity - Method works on populations ranging from dozens to thousands of neurons - Geometric structure correlates with behavioral and cognitive variables 我认为这种方法可以为了解神经种群的编码和过程信息提供新的框架。 The geometric perspective might help bridge the gap between single-neuron and population-level analyses. I think the most interesting potential impact is in neural prosthetics and brain-computer interfaces - if we can reliably map neural activity to consistent geometric representations, it could make decoding neural signals more robust. TLDR: New method uses contrastive learning to show how neural populations将信息组织成几何形状，为神经计算提供潜在的通用原理。  完整shere”&gt;完整摘要。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iunfqx/learning_intrinsic_neural_representations_from/</guid>
      <pubDate>Fri, 21 Feb 2025 09:59:51 GMT</pubDate>
    </item>
    <item>
      <title>接近神经网络和机器学习理论的在线课程。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1iu02so/online_courses_that_approach_neural_networks_and/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  我是一名电气工程师，我想开始学习有关A.I.基础知识及其在嵌入式系统上的实现。但是，有关这些主题的大多数在线课程似乎都提供了更多的“ pratical”。通过向学生扔Python和Matlab包裹，而无需教授神经网络的实际工作方式。如果有人能够向我推荐一门课程（免费或付费），该课程（包括神经元的模型和Network的培训，包括神经网络和机器学习的基础）。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32;态href =“ https://www.reddit.com/r/neuralnetworks/comments/comments/1IU02SO/Online_courses_that_that_that_ apphact_neural_networks_and/”&gt; [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1iu02so/online_courses_that_approach_neural_networks_and/</guid>
      <pubDate>Thu, 20 Feb 2025 14:44:29 GMT</pubDate>
    </item>
    <item>
      <title>基于内存的视觉基础模型，用于3D膝盖MRI分割的混合动力降温</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1itt241/memorybased_visual_foundation_model_with_hybrid/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  本文引入了一种基于内存的视觉模型，称为SAMRI-2用于3D医疗图像分割，特别专注于MRI扫描中的膝盖软骨和半月板。 The key innovation is combining a memory mechanism with a hybrid shuffling strategy to better handle 3D spatial relationships while maintaining computational efficiency. Main technical points: - Uses a transformer-based architecture with memory tokens to process 3D volumes - Implements a novel &quot;Hybrid Shuffling Strategy&quot; during training that helps maintain spatial consistency - Requires only 3 user clicks per scan as prompts - Trained on 270 patient scans, tested on 57 external cases - Compared against 3D-VNet and other transformer baselines Results: - Dice scores improved by 5% over previous methods - Tibial cartilage segmentation accuracy increased by 12% - Thickness measurements showed 3倍更好的精度 - 在不同的MRI机/协议上保持性能 - 每次扫描的处理时间约为30秒 我认为这种方法对于临床部署可能特别有价值，因为它与最小用户输入之间的自动化平衡。基于内存的设计似乎比以前的方法更有效地处理医学扫描的3D性质。 我认为混合洗牌策略是一种有趣的技术贡献，可以适用于其他3D视觉任务。只需单击3个点击即可维持准确性的能力。  tldr：基于膝关节MRI分析的新型内存模型，将强度的精度与最小的用户输入相结合（3次点击）。使用混合改组策略有效地处理3D数据。 在这里。 Paper 在这里。  &lt;！&lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]    [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1itt241/memorybased_visual_foundation_model_with_hybrid/</guid>
      <pubDate>Thu, 20 Feb 2025 07:34:20 GMT</pubDate>
    </item>
    <item>
      <title>引入CNN学习工具</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1it64cd/introducing_cnn_learning_tool/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  使用我的新互动应用程序探索卷积神经网络（CNN）的内部工作。观看每一层如何处理您的草图，对行动中的深度学习有更清晰的了解。 （这也很有趣） 链接：提交由＆＃32; /u/u/foreltert2597     [link]   ＆＃32;   [注释]  ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1it64cd/introducing_cnn_learning_tool/</guid>
      <pubDate>Wed, 19 Feb 2025 14:00:06 GMT</pubDate>
    </item>
    <item>
      <title>硬件优化的本地稀疏注意力，以进行有效的长篇下说建模</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1it2oyq/hardwareoptimized_native_sparse_attention_for/</link>
      <description><![CDATA[&lt;！ -  sc_off-&gt;  这里的关键贡献是一种新的稀疏注意方法，它与可训练的端到端训练相一致。 Instead of using complex preprocessing or dynamic sparsity patterns, Native Sparse Attention (NSA) uses block-sparse patterns that match GPU memory access patterns. Main technical points: - Introduces fixed but learnable sparsity patterns that align with hardware - Patterns are learned during normal training without preprocessing - Uses block-sparse structure optimized for GPU memory access - Achieves 2-3x speedup compared to dense attention - Maintains accuracy while using 50-75% less computation Results across different settings: - Language modeling: Matches dense attention perplexity - Machine translation: Comparable BLEU scores - Image classification: Similar accuracy to dense attention - Scales well with increasing sequence lengths - Works effectively across different model sizes I think this approach could make transformer models more practical in resource-constrained environments.硬件对齐方式意味着理论效率的提高实际上转化为现实世界的性能改进，与许多现有的稀疏注意方法不同。 我认为，在某些情况下可能限制了块 -  sparse模式，而在某些情况下可能限制了灵活性和效率之间的良好权衡。 The ability to learn these patterns during training is particularly important, as it allows the model to adapt the sparsity to the task. TLDR: New sparse attention method that aligns with hardware constraints and learns sparsity patterns during training, achieving 2-3x speedup without accuracy loss. 完整摘要在这里。 Paper 在这里。  &lt;！ -  sc_on-&gt;＆＃32;提交由＆＃32; /u/u/us sucke-western27     [link]        [注释]   ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1it2oyq/hardwareoptimized_native_sparse_attention_for/</guid>
      <pubDate>Wed, 19 Feb 2025 10:47:35 GMT</pubDate>
    </item>
    </channel>
</rss>