<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络、深度学习和机器学习的 Reddit 子版块。</description>
    <lastBuildDate>Sun, 17 Mar 2024 15:13:40 GMT</lastBuildDate>
    <item>
      <title>马达琳网络</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bgtmx6/madaline_network/</link>
      <description><![CDATA[是否有一个正式的证据来说明为什么要设置更新规则，我知道与简单的反向传播相比，这是一个愚蠢的模型，我很容易理解（链式法则和错误属性）但是 Madaline 规则感觉很奇怪，只是想到而不是派生的东西    由   提交/u/borisshootspancakes  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bgtmx6/madaline_network/</guid>
      <pubDate>Sun, 17 Mar 2024 09:32:34 GMT</pubDate>
    </item>
    <item>
      <title>关于 RNN 的问题</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bg3gm8/question_about_rnns/</link>
      <description><![CDATA[在 RNN 中，神经元的输出是否会：  直接返回到与输入相同的神经元？  或  回到上一层，然后再次触发同一个神经元，同时也触发其他神经元？   如果这两种类型的 RNN 都存在，它们的名字是什么？   由   提交/u/BePoliter  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bg3gm8/question_about_rnns/</guid>
      <pubDate>Sat, 16 Mar 2024 10:55:40 GMT</pubDate>
    </item>
    <item>
      <title>这是什么程序？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bfxoag/what_program_is_this/</link>
      <description><![CDATA[      我正在 YouTube 上学习神经网络的基础知识，老师使用了下面的工具解释材料的某些部分。不过他没有提到名字，有谁知道并可以告诉我名字吗？ https://preview.redd.it/cma43faqhmoc1.png?width=1920&amp;format=png&amp;auto=webp&amp;s=57fc2d9cd55b07f815f5bb336a3a 07cdb8e4aadf &lt; /div&gt;  由   提交 /u/ConfectionPuzzled271   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bfxoag/what_program_is_this/</guid>
      <pubDate>Sat, 16 Mar 2024 04:18:00 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的自组织映射邻域实现</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bevbd9/selforganizing_map_neighborhood_implementation_in/</link>
      <description><![CDATA[我正在尝试实现一个自组织映射，其中对于给定的输入样本，根据（例如）选择最佳匹配单元/获胜单元SOM 和输入之间的 L2 范数距离。获胜单元/BMU (som[x, y]) 与给定输入 (z) 的 L2 距离最小： # 输入批次：batch-size = 512, input-dim = 84- z = torch.randn(512, 84) # SOM 形状：（高度、宽度、输入-dim)- som = torch.randn(40, 40, 84) print(f&quot;BMU 行, col 形状; row = {row.shape} &amp; col = {col.shape}&quot;) # BMU row, col 形状; row = torch.Size([512]) &amp; col = torch.Size([512]) 为了清楚起见，对于批次“z[0]”中的第一个输入样本，获胜单位是“som[row” [0]，col[0]]”- z[0].shape，som[row[0]，col[0]].shape # (torch.Size([84]), torch.Size([84])) torch.norm((z[0]) - som[row[0], col[0]])) 是 z[0] 与除 row[0] 和 col[0] 之外的所有其他 som 单位之间的最小 L2 距离。 &lt; p&gt;# 定义初始邻域半径和学习率- neighb_rad = torch.tensor(2.0)  lr = 0.5 # 更新第一个输入“z[0]”的权重及其对应的 BMU“som[row[0], col[0]]”- for r in range(som.shape[0]): for c in range(som.shape[1]): neigh_dist = torch.exp(-torch.norm(输入 = (som[r, c] - som[row[0], col[0]])) / (2.0 * torch.pow(neighb_rad, 2))) &lt; code&gt;som[r, c] = som[r, c] + (lr * neigh_dist * (z[0] - som[r, c])) 如何实现代码：  更新每个 BMU 周围所有单元的权重，无需 2 个 for 循环（并且） 对所有输入“z”执行此操作（这里，z有512个样本）    由   提交/u/grid_world  [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bevbd9/selforganizing_map_neighborhood_implementation_in/</guid>
      <pubDate>Thu, 14 Mar 2024 20:35:43 GMT</pubDate>
    </item>
    <item>
      <title>使用CNN进行黑白矩阵识别</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bekofr/black_and_white_matrix_identification_using_cnn/</link>
      <description><![CDATA[        由   提交/u/LightFounder  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bekofr/black_and_white_matrix_identification_using_cnn/</guid>
      <pubDate>Thu, 14 Mar 2024 13:04:02 GMT</pubDate>
    </item>
    <item>
      <title>您想知道您的神经网络需要多长时间才能得到充分训练？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1behfja/do_you_want_to_know_the_time_it_will_take_your/</link>
      <description><![CDATA[查看投票 &lt; /div&gt;  由   提交 /u/Red_Pudding_pie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1behfja/do_you_want_to_know_the_time_it_will_take_your/</guid>
      <pubDate>Thu, 14 Mar 2024 09:46:44 GMT</pubDate>
    </item>
    <item>
      <title>1 位法学硕士时代 - 论文解释</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1beel34/the_era_of_1bit_llms_paper_explained/</link>
      <description><![CDATA[       由   提交/u/Personal-Trainer-541   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1beel34/the_era_of_1bit_llms_paper_explained/</guid>
      <pubDate>Thu, 14 Mar 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>神经网络如何学习？数学公式解释了它们如何检测相关模式</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bde520/how_do_neural_networks_learn_a_mathematical/</link>
      <description><![CDATA[       由   提交/u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bde520/how_do_neural_networks_learn_a_mathematical/</guid>
      <pubDate>Wed, 13 Mar 2024 00:55:16 GMT</pubDate>
    </item>
    <item>
      <title>复制理论表明深度神经网络的想法是相似的</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bddxi0/replica_theory_shows_deep_neural_networks_think/</link>
      <description><![CDATA[   /u/keghn  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bddxi0/replica_theory_shows_deep_neural_networks_think/</guid>
      <pubDate>Wed, 13 Mar 2024 00:45:47 GMT</pubDate>
    </item>
    <item>
      <title>机器学习工程的最佳实践是什么？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bd4wsm/what_are_best_practices_for_machine_learning/</link>
      <description><![CDATA[   /u/beluis3d  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bd4wsm/what_are_best_practices_for_machine_learning/</guid>
      <pubDate>Tue, 12 Mar 2024 18:41:45 GMT</pubDate>
    </item>
    <item>
      <title>挑战：这些 Neural Network Playground 设置（噪声 = 20 的螺旋数据）能否获得最低的测试损失</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bckpo1/challenge_can_you_get_lowest_test_loss_for_these/</link>
      <description><![CDATA[       由   提交 /u/LatestLurkingHandle   [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bckpo1/challenge_can_you_get_lowest_test_loss_for_these/</guid>
      <pubDate>Tue, 12 Mar 2024 01:27:39 GMT</pubDate>
    </item>
    <item>
      <title>如果我希望感知器输出 n 到 m 之间的数字，而不是 -1 或 1，该怎么办</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bcj18v/what_if_i_want_my_perceptron_to_output_a_number/</link>
      <description><![CDATA[嗨， 我是神经网络的初学者。我正在尝试构建一个感知器或神经网络中的任何神经元，可以输出一系列数字，比如说-7到+7之间，包括0；这可能吗，因为，我只看过感知器神经元的视频，可以输出-1和1，如果你愿意的话还可以输出0，但对我来说这有点有限。或者有什么我还没有看到的东西，我想对这个主题进行基本的细分，这样我就可以掌握我所缺少的内容。   由   提交 /u/CapMustang101   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bcj18v/what_if_i_want_my_perceptron_to_output_a_number/</guid>
      <pubDate>Tue, 12 Mar 2024 00:11:47 GMT</pubDate>
    </item>
    <item>
      <title>神经网络的算力安排？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bceuun/arrangement_of_computing_power_for_neural_networks/</link>
      <description><![CDATA[计算能力是神经网络训练的支柱，但并不是每个人都有钱买得起。企业如何应对这一挑战？  查看民意调查  &amp; #32；由   提交 /u/Red_Pudding_pie   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bceuun/arrangement_of_computing_power_for_neural_networks/</guid>
      <pubDate>Mon, 11 Mar 2024 21:24:56 GMT</pubDate>
    </item>
    <item>
      <title>The Dode Abides - 神经网络模拟中的 24 小时人工进化</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bc5k99/the_dode_abides_24_hours_of_artificial_evolution/</link>
      <description><![CDATA[       由   提交/u/urocyon_dev  [链接] [评论] &lt; /表&gt;]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bc5k99/the_dode_abides_24_hours_of_artificial_evolution/</guid>
      <pubDate>Mon, 11 Mar 2024 15:15:43 GMT</pubDate>
    </item>
    <item>
      <title>菜鸟问题：我可以制作一个可以快速训练的神经网络，以比普通的基于多边形的渲染更快地渲染一个特定对象吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1bc5erp/noob_question_could_i_make_a_nn_that_can_be/</link>
      <description><![CDATA[警告：我只是知道神经网络是什么以及它们如何训练的大致流程，所以我可能会说一些非常愚蠢的事情，我的想法可能很荒谬。无论如何，我问这个问题是因为我想至少确切地知道我有哪些错误和误解，并且万一我正在考虑的事情实际上是可能的。 这是上下文： 我已经成为一名游戏程序员三年了，当我第一次听说 3D Gaussian Splatting 时，我错误地认为这是一种可以训练神经网络来渲染 3D 模型的方法从任何角度来看，无需实际读取网格数据，我立即开始想象如何利用它来降低游戏图形的性能负担。 从那以后我了解到 3DGS 的作用恰恰相反，并且从渲染构建模型，但想法仍然存在：就是这样。 如果我想要渲染特定模型，则将其用作训练数据渲染，相机始终直接朝向其中心，并且一个点光源-来源。渲染将使用处于各种位置的相机和光源来进行，并且将具有三种类型：1)彩色图像的经典渲染2)“深度图”，其中每个像素的值是3）法线贴图，其中每个像素的颜色值是该像素采样的物体表面点的法线的分量 神经网络应该采用作为输入相机和光源相对于对象的位置，并将这三个渲染吐出作为输出。 如果这样的东西甚至可以工作，它是否可以：a）足够快，足够频繁与普通渲染相比值得使用它吗？ b) 足够快地训练可以为游戏中的每个资产制作不同的版本，而不需要每个资产花费数周的时间？ 我知道我的问题很模糊，但我不&#39;我知道得不够具体，而且我认为如果我错了太，您乍一看就会知道这是不可行的。   由   提交 /u/StrixLiterata   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1bc5erp/noob_question_could_i_make_a_nn_that_can_be/</guid>
      <pubDate>Mon, 11 Mar 2024 15:09:01 GMT</pubDate>
    </item>
    </channel>
</rss>