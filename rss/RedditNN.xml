<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>神经网络、深度学习和机器学习</title>
    <link>https://www.reddit.com/r/neuralnetworks/?format=xml.rss</link>
    <description>关于人工神经网络，深度学习和机器学习的子版块。</description>
    <lastBuildDate>Wed, 26 Jun 2024 12:29:16 GMT</lastBuildDate>
    <item>
      <title>使用二维矩阵作为 LSTM / RNN 模型的特征输入</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1do9nmj/using_a_2d_matrix_as_a_feature_input_to_lstm_rnn/</link>
      <description><![CDATA[我正在构建一个 LSTM 模型来预测每天在商店层面销售的商品组合。请注意，这是一个探索性模型，我对不同类型的 SKU / 产品之间的相关性有一个很好的了解。输入特征将包括每个 SKU 的不同特征作为矩阵的行（因此列将是特征，行将是 SKU ID）。该模型的输出将是大小为 N 的一维向量（其中 N 是 SKU 的数量），标签（GT）将提供每日销售的百分比细分。现在我也明白，使用 softmax 激活的输出并不能直接转化为百分比，但我需要的只是一个大概的估计（我也可以使用 KL 散度损失，因为我们需要的只是销售分布以匹配预测） 所以主要问题是如何将这个二维矩阵转换为一维特征向量？我的愚蠢想法是使用相同的顺序将其展平（例如 SKU1-SKU2- 等 ..这当然会出现特定日期的销售缺失问题，并且将是一个 0 的向量），并且因为在推理过程中我知道这个顺序，所以我将使用相同的顺序。每当引入新的 SKU 时，我只需使用新顺序从头开始重新训练模型即可。 就像我说的，以上只是第一次通过，因此任何意见、指针都将受到高度赞赏（跨越所有时间步骤 :P）    提交人    /u/immortanslow   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1do9nmj/using_a_2d_matrix_as_a_feature_input_to_lstm_rnn/</guid>
      <pubDate>Tue, 25 Jun 2024 16:05:51 GMT</pubDate>
    </item>
    <item>
      <title>我利用 Strava 活动训练了一个神经网络，以预测我的比赛时间</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dna5fg/i_trained_a_neural_network_with_my_strava/</link>
      <description><![CDATA[https://github.com/nst/StravaNeuralNetwork 我仍然发现这些预测非常不精确，希望得到评论和建议。    提交人    /u/Dull_Replacement8890   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dna5fg/i_trained_a_neural_network_with_my_strava/</guid>
      <pubDate>Mon, 24 Jun 2024 10:21:46 GMT</pubDate>
    </item>
    <item>
      <title>我已经训练了一个神经网络来合并我的世界皮肤。</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dn47gy/ive_trained_a_neural_network_to_merge_minecraft/</link>
      <description><![CDATA[        提交人    /u/Cfgodndje28   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dn47gy/ive_trained_a_neural_network_to_merge_minecraft/</guid>
      <pubDate>Mon, 24 Jun 2024 03:40:21 GMT</pubDate>
    </item>
    <item>
      <title>构建一个 Python 库来快速为 RAG 创建+搜索知识图谱——想要做出贡献吗？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dmm1e5/building_a_python_library_to_quickly_createsearch/</link>
      <description><![CDATA[如果您的文档包含相互关联的概念，知识图谱可以提高您的 RAG 准确性。 并且，您可以使用最新版本的 knowledge-graph-rag 库自动为现有文档创建 + 搜索 KG。 只需 3 行代码即可完成所有操作。 在此示例中，我使用医疗文档。该库的工作原理如下：  从语料库中提取实体（例如器官、疾病、疗法等） 提取它们之间的关系（例如疗法的缓解效果、斑块的积累等） 使用 LLM 从这些表示中创建知识图谱。 当用户发送查询时，将其分解为要搜索的实体。 搜索 KG 并在 LLM 调用的上下文中使用结果。  这是 repo：https://github.com/sarthakrastogi/graph-rag 如果您想贡献或对功能有建议，请在 Github 上提出。    由    /u/sarthakai 提交   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dmm1e5/building_a_python_library_to_quickly_createsearch/</guid>
      <pubDate>Sun, 23 Jun 2024 13:22:19 GMT</pubDate>
    </item>
    <item>
      <title>LinkedIn 使用 Graph RAG 将他们的工单解决时间从 40 小时缩短至 15 小时。让我们创建一个库，让每个人都可以使用它？</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dlwchd/linkedin_used_graph_rag_to_cut_down_their_ticket/</link>
      <description><![CDATA[首先，我了解一下他们是如何做到的： 他们通过将客户支持工单解析为结构化的树形表示来创建 KG，并保留其内部关系。 工单基于上下文相似性、依赖性和引用进行链接 - 所有这些都构成了一个综合图。 KG 中的每个节点都是嵌入的，因此它们可以进行语义搜索和检索。 RAG QA 系统通过遍历和按语义相似性搜索来识别相关子图。 然后，它从 KG 中生成上下文感知答案，并通过 MRR 进行评估，结果显着改善。 论文：https://arxiv.org/pdf/2404.17723 如果您也想实现 Graph RAG，我正在创建一个 Python 库，它可以自动为您的 vectordb 中的文档创建此图表。它还使您可以轻松检索与最佳匹配相关的相关文档。 如果您有兴趣做出贡献或有任何建议，请在 Github 上提出。 这是该库的 repo：https://github.com/sarthakrastogi/graph-rag/tree/main    提交人    /u/sarthakai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dlwchd/linkedin_used_graph_rag_to_cut_down_their_ticket/</guid>
      <pubDate>Sat, 22 Jun 2024 14:01:14 GMT</pubDate>
    </item>
    <item>
      <title>AI 阅读清单 - 第 5 部分</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dlqcc8/ai_reading_list_part_5/</link>
      <description><![CDATA[        提交人    /u/Personal-Trainer-541   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dlqcc8/ai_reading_list_part_5/</guid>
      <pubDate>Sat, 22 Jun 2024 07:41:23 GMT</pubDate>
    </item>
    <item>
      <title>自动编码器 | 深度学习动画</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dlgrr3/autoencoders_deep_learning_animated/</link>
      <description><![CDATA[        提交人    /u/keghn   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dlgrr3/autoencoders_deep_learning_animated/</guid>
      <pubDate>Fri, 21 Jun 2024 22:40:25 GMT</pubDate>
    </item>
    <item>
      <title>简单解释 LoRA 的实际工作原理 (ELI5)</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dl55h7/simply_explaining_how_lora_actually_works_eli5/</link>
      <description><![CDATA[假设在您的 LLM 中，您有维度为 d x k 的原始权重矩阵 W。 您的传统训练过程会直接更新 W - 如果 d x k 很大，那么参数数量将非常庞大，需要大量计算。 因此，我们在权重更新之前使用低秩分解对其进行分解。方法如下 - 我们将权重更新 (Delta W) 表示为两个低秩矩阵 A 和 B 的乘积，使得 Delta W = BA。 这里，A 是维度为 r x k 的矩阵，B 是维度为 d x r 的矩阵。这里，r（秩）比 d 和 k 小得多。 现在，矩阵 A 用一些随机高斯值初始化，矩阵 B 用零初始化。 为什么？因此最初 Delta W = BA 可以为 0。 现在开始训练过程： 在权重更新期间，仅更新较小的矩阵 A 和 B - 这大大减少了需要调整的参数数量。 对原始权重矩阵 W 的有效更新是 Delta W = BA，它使用更少的参数来近似 W 的变化。 让我们比较一下 LoRA 之前和之后要更新的参数： 之前，要更新的参数是 d x k（记住 W 的尺寸）。 但是现在，参数数量减少到 (d x r) + (r x k)。这个值要小得多，因为秩 r 被认为比 d 和 k 小得多。 这就是低秩近似如何通过这种紧凑的表示为您提供有效的微调。 训练速度更快，需要的计算和内存更少，同时仍然可以从微调数据集中捕获重要信息。 我还使用 Artifacts 制作了一个快速动画来解释（大约花了 10 秒）： https://www.linkedin.com/posts/sarthakrastogi_simply-explaining-how-lora-actually-works-activity-7209893533011333120-RSsz    提交人    /u/sarthakai   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dl55h7/simply_explaining_how_lora_actually_works_eli5/</guid>
      <pubDate>Fri, 21 Jun 2024 14:19:55 GMT</pubDate>
    </item>
    <item>
      <title>案例研究：人工智能和计算机视觉——在麦克风后面和舞台上</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dl2r07/case_study_artificial_intelligence_and_computer/</link>
      <description><![CDATA[在案例研究回顾中，您将了解机器人如何创作交响乐。神经网络创造热门歌曲，3D 投影在舞台上表演，音乐服务根据声谱图对曲目进行评分。音乐文化是不断发展的技术的完美游乐场 完整文章位于 OpenCV.ai 博客中。链接此处。    提交人    /u/Computer_Vision4883   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dl2r07/case_study_artificial_intelligence_and_computer/</guid>
      <pubDate>Fri, 21 Jun 2024 12:24:53 GMT</pubDate>
    </item>
    <item>
      <title>概率电路（YooJung Choi，亚利桑那州立大学）</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dkfzjg/probabilistic_circuits_yoojung_choi_asu/</link>
      <description><![CDATA[        由    /u/Neurosymbolic  提交  [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dkfzjg/probabilistic_circuits_yoojung_choi_asu/</guid>
      <pubDate>Thu, 20 Jun 2024 16:32:32 GMT</pubDate>
    </item>
    <item>
      <title>高维函数上的神经网络基准测试</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1dk9zc3/benchmark_neural_networks_on_highdimensional/</link>
      <description><![CDATA[对于个人项目，我有兴趣在高维函数逼近的背景下对某些神经网络架构进行基准测试。具体来说，我对 R^d 中在 [0,1]^d 上定义的连续、平滑、Hölder 和 Sobolev 函数感兴趣。  是否有人知道文献中是否通常使用 *标准* 高维函数列表来对此类模型进行基准测试？例如，在优化文献中，有一个标准函数列表，例如此处找到的函数，用于对各种优化算法进行基准测试。 如果没有这样的列表，应该如何构建一个有代表性的函数列表？这种选择会在问题中引入归纳偏差，因此我想确保列表尽可能具有代表性。  谢谢！    提交人    /u/JM753   [链接] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1dk9zc3/benchmark_neural_networks_on_highdimensional/</guid>
      <pubDate>Thu, 20 Jun 2024 12:06:13 GMT</pubDate>
    </item>
    <item>
      <title>人工智能和政治可以共存——但新技术不应该掩盖选举往往获胜的领域——在实地</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1djixkv/ai_and_politics_can_coexist_but_new_technology/</link>
      <description><![CDATA[    /u/CWang   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1djixkv/ai_and_politics_can_coexist_but_new_technology/</guid>
      <pubDate>Wed, 19 Jun 2024 13:16:26 GMT</pubDate>
    </item>
    <item>
      <title>需要编码伙伴</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1djbrna/need_coding_partner/</link>
      <description><![CDATA[我是编码新手，对 PyTorch 一无所知。我需要一个像我这样的合作伙伴，这样我就不会对开发 nn 和 ai 失去兴趣，也不必孤军奋战。年龄大概在 17-18 岁左右，这样我的智力就不会无人能及    提交人    /u/Purple-Meaning-6306   [link] [评论]]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1djbrna/need_coding_partner/</guid>
      <pubDate>Wed, 19 Jun 2024 05:46:08 GMT</pubDate>
    </item>
    <item>
      <title>AI 阅读清单 - 第 4 部分</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1diob1c/ai_reading_list_part_4/</link>
      <description><![CDATA[        提交人    /u/Personal-Trainer-541   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1diob1c/ai_reading_list_part_4/</guid>
      <pubDate>Tue, 18 Jun 2024 11:25:26 GMT</pubDate>
    </item>
    <item>
      <title>溯因学习</title>
      <link>https://www.reddit.com/r/neuralnetworks/comments/1di3kjq/abductive_learning/</link>
      <description><![CDATA[        提交人    /u/Neurosymbolic   [链接] [评论] ]]></description>
      <guid>https://www.reddit.com/r/neuralnetworks/comments/1di3kjq/abductive_learning/</guid>
      <pubDate>Mon, 17 Jun 2024 17:20:23 GMT</pubDate>
    </item>
    </channel>
</rss>