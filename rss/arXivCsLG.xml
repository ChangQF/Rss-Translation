<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.lg arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.lg arxiv.org e-print档案中的更新。</description>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>Mobillm：通过服务器辅助侧调整在移动设备上启用LLM微调</title>
      <link>https://arxiv.org/abs/2502.20421</link>
      <description><![CDATA[ARXIV：2502.20421V1公告类型：新 
摘要：移动设备上的大型语言模型（LLM）及其潜在应用永远不会着迷。但是，由于非常高的记忆需求和缓慢的训练速度，因此在设备上的LLM微调构成了巨大的挑战。即使使用参数有效的微调（PEFT）方法仅更新一小部分参数，资源受限的移动设备也无法负担。在本文中，我们建议Mobillm通过服务器辅助的侧调启用移动设备上的记忆有效变压器LLM进行微调。特别是，Mobillm允许资源受限的移动设备仅保留一个冷冻的骨干模型，同时卸载可训练的侧网网络的内存和计算密集型反向传播到高性能服务器。与现有的微调方法在冷冻骨架内保持可训练的参数不同，Mobillm将一组并行适配器从骨架分开以创建反向传播旁路，仅涉及从移动设备到服务器的单向激活传输，在前传播过程中具有低宽度量化。通过这种方式，数据永远不会离开移动设备，而设备可以通过本地骨干模型删除反向传播，并且可以使用服务器端执行来瘫痪其正向传播。因此，Mobillm保留了数据隐私，同时显着减少了LLM微调的内存和计算负担。通过广泛的实验，我们证明了Mobillm可以使资源受限的移动设备（甚至仅使用CPU）能够微调LLM，并显着减少收敛时间和内存使用情况。]]></description>
      <guid>https://arxiv.org/abs/2502.20421</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>统一模型预测路径积分控制，增强学习和扩散模型，用于最佳控制和计划</title>
      <link>https://arxiv.org/abs/2502.20476</link>
      <description><![CDATA[ARXIV：2502.20476V1公告类型：新 
摘要：模型预测路径积分（MPPI）控制，增强学习（RL）和扩散模型各自在轨迹优化，决策和运动计划中表现出强烈的性能。但是，这些方法传统上被视为具有独立优化框架的不同方法。在这项工作中，我们建立了一个统一的观点，该视角通过基于梯度的Gibbs测量来连接MPPI，RL和扩散模型。我们首先表明MPPI可以解释为在平滑的能量函数上执行梯度上升。然后，我们证明，在将策略参数视为固定初始状态下的控制变量时，策略梯度方法将其减少到MPPI。此外，我们确定扩散模型中的反向采样过程遵循与MPPI相同的更新规则。]]></description>
      <guid>https://arxiv.org/abs/2502.20476</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>统一内核分离的旋转卷积操作</title>
      <link>https://arxiv.org/abs/2502.20493</link>
      <description><![CDATA[ARXIV：2502.20493V1公告类型：新 
摘要：通过内核隔离机制实现了用于深度学习应用的转置卷积层的优化。但是，内核分离存在缺点，例如计算额外的元素以在启动线程时获得具有奇数的输出特征映射。为了减轻此问题，我们引入了一种统一的内核隔离方法，该方法通过使用一个统一的内核执行四个子内核来限制内存和计算资源的使用。研究结果表明，当使用RTX 2070 GPU（Intel Xeon CPU）测试时，建议的方法达到了2.03倍（3.89倍）的平均计算速度。消融研究表明，评估来自众所周知的生成对抗网络（GAN）的转置卷积层时的平均计算速度为3.5倍。 EB-GAN模型中的旋转卷积层的提议方法的实现表明，最高35 MB的内存节省。]]></description>
      <guid>https://arxiv.org/abs/2502.20493</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数据分布性能作为系统概括的电感偏差</title>
      <link>https://arxiv.org/abs/2502.20499</link>
      <description><![CDATA[ARXIV：2502.20499V1公告类型：新 
摘要：深度神经网络（DNNS）在系统概括（SG）方面的斗争。几项研究评估了通过新的结构，损失功能或训练方法提出的提议来促进SG的可能性。然而，很少有研究集中在培训数据特性在促进SG中的作用。在这项工作中，我们研究了某些数据分布属性的影响，作为多模式语言模型SG能力的归纳偏见。为此，我们研究了三种不同的属性。首先，数据多样性是将训练分布中潜在特性的可能值提高而实例化的。其次，爆发性，我们可能会在训练过程中限制特定输入的潜在因素的可能值数量。第三，潜在干预，其中特定的潜在因素在训练过程中随机改变。我们发现，这三个因素都显着增强了SG，多样性导致受影响最大的财产的准确性的绝对准确性增长了89％。通过一系列实验，我们测试了各种假设，以了解这些特性为何促进SG。最后，我们发现训练分布中的潜在属性之间的归一化互信息（NMI）强烈预测分布式概括。我们发现，较低NMI诱导SG的机制是表示形式的几何形状。特别是，我们发现NMI在模型的神经表示（即，在平行神经向量中编码的输入特征）中诱导更多的并行性，这是一种与类比的推理能力相关的属性。]]></description>
      <guid>https://arxiv.org/abs/2502.20499</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过相关的高斯流程表示，重新访问内核注意力</title>
      <link>https://arxiv.org/abs/2502.20525</link>
      <description><![CDATA[ARXIV：2502.20525V1公告类型：新 
摘要：变形金刚越来越多地成为用最新性能建模顺序数据的事实方法。由于其广泛使用，能够估计和校准其建模不确定性对于理解和设计强大的变压器模型很重要。为了实现这一目标，以前的作品使用高斯工艺（GPS）来对变压器的注意力单位进行不确定性校准，并取得了显着的成功。但是，这种方法必须将变压器局限于对称关注的空间，以确保其GP内核规范的必要对称要求，从而降低了模型的表示能力。为了减轻这种限制，我们提出了相关的高斯过程变压器（CGPT），这是一种新的变压器，其自我发项单元的建模为两个相关的GPS（CGP）之间的跨交联。这允许注意力不对称，并可以增强基于GP的变压器的表示能力。我们还得出了CGP稀疏近似值，以使其比例更好。我们的实证研究表明，基于CGP的基于CGP和基于CGP的稀疏变压器在各种基准任务上都比基于GP的最先进的GP变压器获得了更好的性能。我们的实验代码可从https://github.com/minhlong210/cgp-transformers获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20525</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SOS1：O1和R1般的推理LLM是平方的求解器</title>
      <link>https://arxiv.org/abs/2502.20545</link>
      <description><![CDATA[ARXIV：2502.20545V1公告类型：新 
摘要：大型语言模型（LLMS）已在各种任务中实现了人类水平的水平，但是他们执行严格的数学问题解决的能力仍然是一个开放的挑战。在这项工作中，我们研究了一个基本但计算上棘手的问题：确定给定的多元多项式是否不负。这个问题与希尔伯特的第十七个问题密切相关，在全球多项式优化中起着至关重要的作用，并且在各个领域都有应用。首先，我们介绍了SOS-1K，这是一个精心策划的大约1,000个多项式的数据集，以及基于五个逐渐挑战的标准的专家设计的推理说明。评估多个最先进的LLMS，我们发现没有结构化指导，所有模型的表现仅略高于随机猜测基线50％。但是，高质量的推理说明可显着提高准确性，从而提高性能高达81％。此外，我们的7B型号SOS-7B在SOS-1K上进行了微调仅4个小时，其准确性优于671B DeepSeek-V3和GPT-4O-Mini，而仅需要字母所需的1.8％和5％的计算时间。我们的发现突出了LLM的潜力，即推动数学推理的界限并解决NP-HARD问题。]]></description>
      <guid>https://arxiv.org/abs/2502.20545</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>$ q \夏普$：可证明是最佳的LLM训练后培训的最佳分销RL</title>
      <link>https://arxiv.org/abs/2502.20548</link>
      <description><![CDATA[ARXIV：2502.20548V1公告类型：新 
摘要：训练后的强化学习（RL）对于LLM的一致性和推理至关重要，但是现有的基于策略的方法（例如PPO和DPO）可能无法固定从预训练中继承的快捷方式。在这项工作中，我们引入了$ q \ sharp $，这是一种基于价值的KL登记RL的算法，该算法使用最佳正则化$ Q $函数来指导参考策略。我们建议在汇总的在线数据集中使用Distributal RL学习最佳$ Q $函数。与先前的基于价值的基线不同，可以使用未注册的$ Q $值指导模型，我们的方法在理论上是原则性的，可以证明对KL登记的RL问题的最佳策略。从经验上讲，$ q \ sharp $在数学推理基准中优于先前的基线，同时保持较小的吉隆坡差异与参考策略。从理论上讲，我们将从KL登记的RL降低到无需在线学习，仅在仅在可实现的情况下为确定性MDP提供了第一范围。多亏了分布RL，我们的界限也依赖于方差，并且当参考策略的差异较小时，收敛速度更快。总而言之，我们的结果突出显示了$ Q \ sharp $是用于培训后LLM的有效方法，既可以提高性能和理论保证。可以在https://github.com/jinpz/q_sharp上找到该代码。]]></description>
      <guid>https://arxiv.org/abs/2502.20548</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向大型视力模型的统计事实保证</title>
      <link>https://arxiv.org/abs/2502.20560</link>
      <description><![CDATA[ARXIV：2502.20560V1公告类型：新 
摘要：大型视觉模型（LVLM）的进步在涉及图像条件的自由形式文本生成的各种视觉语言任务中表现出了令人鼓舞的表现。但是，对LVLMS中幻觉的越来越关注，因为生成的文本与视觉上下文不一致，它已成为将这些模型部署在需要保证可靠性的应用程序中的主要障碍。在本文中，我们介绍了一个框架来应对这一挑战，即ConflvLM，该挑战基于共形预测，以实现LVLM输出事实的有限样本分配的无统计保证。该框架将LVLM视为假设发生器，其中每个生成的文本细节（或主张）被视为单个假设。然后，它应用了统计假设测试程序，以使用有效的启发式不确定性度量来验证每个索赔，以在返回对用户的任何响应之前过滤不可靠的索赔。我们进行了广泛的实验，涵盖了三个代表性应用领域，包括一般场景理解，医学放射学报告生成和文档理解。值得注意的是，通过以95.3 \％真实的正率来滤除错误的索赔，Conflvlm将场景描述的索赔错误率从87.8 \％\％降低到10.0 \％。我们的结果进一步表明，ConflvLM具有很高的灵活性，并且可以应用于任何黑盒LVLMS与任何不确定性措施，用于任何图像条件条件的自由形式文本生成任务，同时为控制幻觉风险提供严格的保证。]]></description>
      <guid>https://arxiv.org/abs/2502.20560</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DPZV：用于差异私有VFL的资源有效ZO优化</title>
      <link>https://arxiv.org/abs/2502.20565</link>
      <description><![CDATA[ARXIV：2502.20565V1公告类型：新 
摘要：垂直联合学习（VFL）可以在特征分区的数据上进行协作模型培训，但在扩展到大型模型时会面临明显的隐私风险和效率低下。 We propose DPZV, a memory-efficient Zeroth-Order(ZO) optimization framework that integrates differential privacy (DP) with vertical federated learning, addressing three critical challenges: (1) privacy vulnerabilities from gradient leakage, (2) high computation/communication costs of first-order methods, and (3) excessive memory footprint in conventional zeroth-order approaches.我们的框架通过两点梯度估计消除了反向传播，与一阶相比，在启用异步通信的同时，将客户存储器的使用量减少了90 \％。通过在服务器上策略性地注入高斯噪声，DPZV实现了严格的$（\ epsilon，\ delta）$  -  DP保证没有第三方信任假设。理论分析建立了在非凸目标下匹配集中案例的收敛速率。关于图像和NLP基准测试的广泛实验表明，DPZV在提供强大的隐私保证（$ \ epsilon \ leq 10 $）的同时，超过了所有基准，并且需要更少的计算资源，并确定了新的最先进的最先进的隐私权 - 利用可用于资源限制的VFL Exployments。]]></description>
      <guid>https://arxiv.org/abs/2502.20565</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM培训的随机舍入：理论与实践</title>
      <link>https://arxiv.org/abs/2502.20566</link>
      <description><![CDATA[ARXIV：2502.20566V1公告类型：新 
摘要：随着大语言模型（LLM）的参数已扩展到数百亿美元，对有效训练方法的需求 - 平衡更快的计算和减少的内存使用而不牺牲准确性 - 比以往任何时候都更为关键。近年来，已经提出了各种混合精度策略，这些策略涉及不同的精度水平以进行优化组成部分，以提高训练速度，而准确性降低最小。但是，这些策略通常需要手动调整，并且缺乏理论上的理由。在这项工作中，我们利用随机舍入（SR）来解决训练的数值错误，并以低精度表示。当使用SR时，我们提供了ADAM优化器下隐式正则化和收敛性的理论分析。通过这些分析的见解，我们扩展了以前的BF16 + SR策略，以用于分布式设置，从而增强了大规模训练的稳定性和性能。首次具有高达6.7b参数的预训练模型的经验结果表明，我们的BF16具有SR策略的表现优于（BF16，FP32），混合精度策略，实现更好的验证困惑，高达$ 1.54 \ $ 1.54 \ timper $ timper $ aighter $ thims $ ai Meigh thims $ 30 \％\％\％\％\％\％$ $ $ $ $。]]></description>
      <guid>https://arxiv.org/abs/2502.20566</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>pfformer：一种无位置变压器变种，用于极端自适应多元时间序列预测</title>
      <link>https://arxiv.org/abs/2502.20571</link>
      <description><![CDATA[ARXIV：2502.20571V1公告类型：新 
摘要：多元时间序列（MTS）预测在天气，能源和金融等领域至关重要。然而，尽管有深入的学习进步，但传统的基于变压器的模型通常会通过奇异的令牌嵌入并难以有效地捕获变量之间的复杂依赖性，尤其是在具有罕见或极端事件的数据集中，从而减少了至关重要的间变化关系的影响。这些事件引起了重大的失衡，并导致高偏度，使准确的预测工作变得复杂。这项研究介绍了Pfformer，这是一种基于无位置变压器的模型，旨在单目标MTS预测，专门用于以极端变异性为特征的挑战数据集。 Pfformer集成了两种新颖的嵌入策略：增强基于功能的嵌入（EFE）和基于自动编码器的嵌入（AEE）。 EFE通过将相关的序列子集映射到没有位置约束的高维空间，从而有效地编码可变量的依赖关系，从而增强了编码器的功能。 Pfformer显示出卓越的预测精度，而没有传统的MTS建模局限性局限性。我们评估了四个具有挑战性的数据集中的Pfformer，重点介绍了两个关键的预测方案：前3天的长序列预测，每四个小时进行一次滚动预测，以反映水管理中的实时决策过程。与最先进的模型相比，Pfformer显示出从20％到60％的显着改善。]]></description>
      <guid>https://arxiv.org/abs/2502.20571</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>培训具有低维错误反馈的大型神经网络</title>
      <link>https://arxiv.org/abs/2502.20580</link>
      <description><![CDATA[ARXIV：2502.20580V1公告类型：新 
摘要：训练深神网络通常依赖于向后传播高维误差标志着计算密集型过程，几乎没有证据支持其在大脑中的实施。但是，由于大多数任务涉及低维输出，因此我们建议低维误差信号可能足以有效学习。为了检验这一假设，我们基于反馈对齐方式介绍了一项新的本地学习规则，该规则利用间接的，低维的错误反馈来训练大型网络。我们的方法将向后传递与前向通行证解剖，从而可以精确控制误差信号维度，同时保持高维​​表示。我们从线性网络的详细理论派生开始，该线性网络构成了我们学习框架的基础，并将我们的方法扩展到非线性，卷积和变压器体系结构。值得注意的是，我们证明，即使在任务维度顺序上的最小误差维度也可以实现传统反向传播的性能匹配。此外，我们的规则可以有效地对卷积网络进行有效的培训，这些卷积网络以前对反馈对准方法具有最小的误差。这一突破不仅为更加精确的学习模型铺平了道路，而且还挑战了在神经网络培训中对高维梯度信号的常规依赖。我们的发现表明，低维误差信号可能与高维误差一样有效，从而促使对高维系统中基于梯度的学习进行重新评估。最终，我们的工作为神经网络优化提供了新的视角，并有助于理解人工和生物系统中的学习机制。]]></description>
      <guid>https://arxiv.org/abs/2502.20580</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Liteasr：有效的自动语音识别，近似近似</title>
      <link>https://arxiv.org/abs/2502.20583</link>
      <description><![CDATA[ARXIV：2502.20583V1公告类型：新 
摘要：现代自动语音识别（ASR）模型，例如Openai的耳语，依靠深层编码器架构，并且它们的编码器是由于高计算强度而进行有效部署的关键瓶颈。我们介绍了LITEASR，这是一种针对ASR编码器的低级压缩方案，可显着降低推理成本，同时保持转录精度。我们的方法利用了中间激活中观察到的强级低级别性能：通过使用较小的校准数据集应用主成分分析（PCA），我们以一系列低级别矩阵乘数近似线性变换，并进一步优化自我主张以在减小的尺寸中起作用。评估结果表明，我们的方法可以将Whisper大V3的编码器大小压缩到50％以上，以更好的转录精度与Whisper Medium的大小相匹配，从而建立了新的帕累托优势效率和性能的前沿。 Liteasr的代码可从https://github.com/efeslab/liteasr获得。]]></description>
      <guid>https://arxiv.org/abs/2502.20583</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用MXFP4培训LLM</title>
      <link>https://arxiv.org/abs/2502.20586</link>
      <description><![CDATA[ARXIV：2502.20586V1公告类型：新 
摘要：MXFP4等低精度（LP）数据类型可以加速矩阵乘法（GEMM）并降低培训成本。但是，在训练过程中直接使用MXFP4代替BF16会显着降低模型质量。在这项工作中，我们提出了第一个使用MXFP4 GEMM的近乎无情的培训食谱，该食谱的价格比支持硬件的FP8快2美元。我们的关键见解是用随机舍入（SR）计算无偏梯度估计，从而导致更准确的模型更新。但是，将SR直接应用于MXFP4可能会导致块级离群值的较高差异，从而损害收敛性。为了克服这一点，我们使用随机的Hadamard Tranform来理论上结合SR的方差。我们训练GPT模型高达6.7b参数，发现我们的方法在混合精液BF16培训中诱导最小的降解。我们的食谱计算$&gt; 1/2 $ MXFP4中的训练拖失lop，使估计的速度超过了FP8 $&gt; 1.3 \ times $，而在反向propapagation期间，BF16 $&gt; 1.7 \ times $。]]></description>
      <guid>https://arxiv.org/abs/2502.20586</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>经过思考：具有成本效益的视觉语言模型推理的大师批准框架</title>
      <link>https://arxiv.org/abs/2502.20587</link>
      <description><![CDATA[ARXIV：2502.20587V1公告类型：新 
摘要：视觉语言模型（VLM）在增加复杂性和尺度的广泛视力应用中取得了显着的成功，但是选择正确的VLM模型大小涉及响应质量和成本之间的权衡。虽然较小的VLM可以运行便宜，但它们通常比在MMMU等基准上的随机猜测要好得多。
  在本文中，我们提出了思想的缓存（COT），这是一个硕士学徒框架，用于大型和小VLM之间的协作推断。 COT管理高质量查询是由缓存中的大VLM（主）产生的，然后通过新型的多模态检索和秘密学习来选择该问题，以帮助小型VLM（学徒）的表现。我们对各种公认和具有挑战性的一般VQA基准进行了广泛评估COT，并表明COT在相同的预算下将总体VQA绩效提高了高达7.7％，并特别使学徒VLMS的绩效提高了36.6％。]]></description>
      <guid>https://arxiv.org/abs/2502.20587</guid>
      <pubDate>Mon, 03 Mar 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>