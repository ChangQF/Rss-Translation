<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Fri, 09 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>通过有序加权平均值的可微优化来学习公平排名策略</title>
      <link>https://arxiv.org/abs/2402.05252</link>
      <description><![CDATA[Learning to Rank (LTR) 是最广泛使用的机器学习应用程序之一。它是具有深远社会影响的平台的关键组成部分，包括求职、医疗信息检索和社交媒体内容提要。传统的 LTR 模型已被证明会产生偏差结果，从而引发了关于如何解决仅优先考虑用户相关性的排名系统所带来的差异的讨论。然而，虽然已经提出了几种公平学习排名模型，但它们在准确性或效率方面存在缺陷，从而限制了它们在现实世界排名平台中的适用性。本文展示了如何将基于有序加权平均 (OWA) 函数优化的可有效解决的公平排名模型集成到 LTR 模型的训练循环中，以实现公平性、用户效用和运行时效率之间的良好平衡。特别是，本文首次展示了如何通过 OWA 目标的约束优化进行反向传播，从而使其能够在集成预测和决策模型中使用。]]></description>
      <guid>https://arxiv.org/abs/2402.05252</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>通用神经泛函</title>
      <link>https://arxiv.org/abs/2402.05232</link>
      <description><![CDATA[许多现代机器学习任务中的一个具有挑战性的问题是处理权重空间特征，即从神经网络的权重和梯度中转换或提取信息。最近的工作已经开发出了有前景的权重空间模型，这些模型与简单前馈网络的排列对称性等价。然而，它们不适用于一般架构，因为权重空间的排列对称性可能因递归或残差连接而变得复杂。这项工作提出了一种针对任何权重空间自动构建排列等变模型的算法，我们将其称为通用神经函数（UNF）。在其他应用中，我们演示了如何将 UNF 替换到现有的学习优化器设计中，并在优化小型图像分类器和语言模型时发现比现有方法有希望的改进。我们的结果表明，学习优化器可以从考虑其优化的权重空间的（对称）结构中受益。我们在 https://github.com/AllanYangZhou/universal_neural_function 上开源了用于构建 UNF 的库。]]></description>
      <guid>https://arxiv.org/abs/2402.05232</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>QGFN：具有行动价值的可控贪婪</title>
      <link>https://arxiv.org/abs/2402.05234</link>
      <description><![CDATA[生成流网络（GFlowNets；GFN）是一系列基于奖励/能量的组合对象生成方法，能够生成多样化且高实用性的样本。然而，让 GFN 偏向于生成高效用样本并非易事。在这项工作中，我们利用 GFN 和强化学习 (RL) 之间的联系，并建议将 GFN 策略与动作值估计 $Q$ 相结合，以创建可以通过混合参数控制的贪婪采样策略。我们证明了所提出的方法 QGFN 的几种变体能够在不牺牲多样性的情况下提高各种任务中生成的高奖励样本的数量。]]></description>
      <guid>https://arxiv.org/abs/2402.05234</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>神经缩放法则的资源模型</title>
      <link>https://arxiv.org/abs/2402.05164</link>
      <description><![CDATA[神经缩放定律描述了模型性能如何随着模型尺寸的扩大而提高。受经验观察的启发，我们引入了神经缩放的资源模型。一个任务通常是复合的，因此可以分解为许多子任务，这些子任务会争夺资源（通过分配给子任务的神经元数量来衡量）。在玩具问题上，我们凭经验发现：（1）子任务的损失与其分配的神经元成反比。 (2)当复合任务中存在多个子任务时，每个子任务获取的资源随着模型变大均匀增长，保持获取资源的比例恒定。我们假设这些发现总体上是正确的，并构建了一个模型来预测一般复合任务的神经缩放定律，该模型成功复制了 arXiv:2203.15556 中报告的 Chinchilla 模型的神经缩放定律。我们相信本文中使用的资源概念将成为表征和诊断神经网络的有用工具。]]></description>
      <guid>https://arxiv.org/abs/2402.05164</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>理解变压器中的感应偏置：无穷大的视角</title>
      <link>https://arxiv.org/abs/2402.05173</link>
      <description><![CDATA[我们在无限超参数化高斯过程极限中研究 Transformer 中的归纳偏差，并认为 Transformer 倾向于偏向于序列空间中更多的排列对称函数。我们证明，当数据集与标记之间的排列对称时，对称群的表示理论可用于给出定量分析预测。我们提出了一个简化的变压器块，并在极限下求解模型，包括对学习曲线和网络输出的准确预测。我们表明，在常见的设置中，我们可以以缩放定律的形式导出可学习性作为上下文长度的函数的严格界限。最后，我们认为维基文本数据集确实具有一定程度的排列对称性。]]></description>
      <guid>https://arxiv.org/abs/2402.05173</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>贝尔曼保形推理：校准时间序列的预测区间</title>
      <link>https://arxiv.org/abs/2402.05203</link>
      <description><![CDATA[我们引入贝尔曼保形推理 (BCI)，这是一个包含任何时间序列预测模型并提供校准预测区间的框架。与现有方法不同，BCI 能够利用多步提前预测，并通过在每个时间步解决一维随机控制问题 (SCP) 来显式优化平均间隔长度。特别是，我们使用动态规划算法来寻找 SCP 的最优策略。我们证明，即使多步提前预测不佳，BCI 也能在任意分布变化和时间依赖性下实现长期覆盖。我们根据经验发现，与现有方法相比，BCI 避免了无限长度的无信息区间，并在波动性预测问题上产生了更短的预测区间。]]></description>
      <guid>https://arxiv.org/abs/2402.05203</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>根据路网开放数据和始发地流量数据估算道路交通碳排放</title>
      <link>https://arxiv.org/abs/2402.05153</link>
      <description><![CDATA[道路交通碳排放量占碳排放总量的20%以上，其精确估算对于碳排放监测和有效的减排政策制定至关重要。然而，现有的估算方法通常依赖于难以收集的车辆行驶里程的个体统计数据来计算排放，因此数据收集难度较高。为了利用人工智能强大的模式识别能力来缓解这个问题，我们结合了代表交通需求和容量因素的两个开放数据源，即起点-目的地（OD）流量数据和路网数据，构建了分层异构道路碳排放估算的图学习方法（HENCE）。具体来说，构建由道路网络级别、社区级别和区域级别组成的层次图，以对空间区域之间基于多尺度道路网络的连通性和出行连接进行建模。在社区层面和区域层面进一步构建由OD链接和空间链接组成的异构图，以捕获出行需求和路网可达性之间的内在相互作用。在两个大规模真实数据集上进行的大量实验证明了HENCE的有效性和优越性，R平方超过0.75，平均优于基线9.60%，验证了其在利用人工智能赋能碳排放管理和可持续发展方面的成功。实现代码可通过以下链接获取：https://github.com/tsinghua-fib-lab/HENCE。]]></description>
      <guid>https://arxiv.org/abs/2402.05153</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>通过修剪和低阶修改评估安全对齐的脆弱性</title>
      <link>https://arxiv.org/abs/2402.05162</link>
      <description><![CDATA[大型语言模型（LLM）在其安全机制中表现出固有的脆弱性，这一点可以从它们对越狱甚至非恶意微调的敏感性中得到证明。本研究通过利用剪枝和低阶修改来探讨安全对齐的脆弱性。我们开发方法来识别对安全护栏至关重要的关键区域，并且在神经元和等级级别上与实用程序相关区域分离。令人惊讶的是，我们发现的孤立区域非常稀疏，在参数级别大约包含 $3\%$，在排名级别包含 $2.5\%$。删除这些区域会损害安全性，但不会显着影响实用性，这证实了模型安全机制固有的脆弱性。此外，我们表明，即使对安全关键区域的修改受到限制，法学硕士仍然容易受到低成本微调攻击。这些发现强调了法学硕士迫切需要更强有力的安全策略。]]></description>
      <guid>https://arxiv.org/abs/2402.05162</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>设计用于驾驶员意图识别的深度神经网络</title>
      <link>https://arxiv.org/abs/2402.05150</link>
      <description><![CDATA[驾驶员意图识别研究越来越依赖深度神经网络。深度神经网络已经在许多不同的任务中实现了顶级性能，但显式分析网络架构的复杂性和性能并不常见。因此，本文应用神经架构搜索来研究深度神经网络架构对计算能力有限的现实世界安全关键应用的影响。我们探索了三种能够处理顺序数据的深度神经网络层类型（长期短期记忆、时间卷积和时间序列变换层）的预定义搜索空间，以及不同数据融合策略的影响驾驶员意图识别性能。针对两个驾驶员意图识别数据集评估一组八个搜索策略。对于这两个数据集，我们观察到没有明确采样更好的深度神经网络架构的搜索策略。然而，与原始手动设计的网络相比，执行架构搜索确实提高了模型性能。此外，我们观察到模型复杂性的增加和驾驶员意图识别性能的提高之间没有关系。结果表明，无论深度神经网络层类型或融合策略如何，多种架构都会产生相似的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.05150</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>CrashFormer：预测碰撞风险的多模式架构</title>
      <link>https://arxiv.org/abs/2402.05151</link>
      <description><![CDATA[减少交通事故是全球公共安全的一个重要问题。事故预测是改善交通安全、在事故发生前采取主动措施以及为安全政策、法规和有针对性的干预措施提供信息的关键。尽管过去几十年来对事故预测进行了大量研究，但由于输入数据或问题表述，许多研究在普遍性、再现性或实际使用的可行性方面存在局限性。为了解决现有的缺点，我们提出了 CrashFormer，这是一种多模式架构，它利用全面（但相对容易获取）的输入，例如事故历史、天气信息、地图图像和人口统计信息。该模型以合理可接受的节奏（即每六个小时）预测 5.161 平方公里地理位置的未来事故风险。 CrashFormer 由五个组件组成：一个利用历史事故和天气数据的顺序编码器、一个使用地图图像数据的图像编码器、一个利用人口统计信息的原始数据编码器、一个用于聚合编码特征的特征融合模块，以及一个分类器接受汇总数据并相应地做出预测。在美国 10 个主要城市进行的广泛现实世界实验的结果表明，在使用“稀疏”输入数据时，CrashFormer 的 F1 分数平均优于最先进的序列和非序列模型 1.8%。]]></description>
      <guid>https://arxiv.org/abs/2402.05151</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>ApiQ：2 位量化大语言模型的微调</title>
      <link>https://arxiv.org/abs/2402.05147</link>
      <description><![CDATA[随着 LLM 规模的不断增大，大型语言模型 (LLM) 的内存高效微调最近引起了人们的广泛关注，这主要是由于 GPU 内存限制以及这些方法在完全微调的情况下的可比结果所带来的限制。尽管取得了进步，但当前的内存高效微调策略（例如 QLoRA）在不同的位宽量化和多方面任务中表现出不一致的性能。这种不一致很大程度上源于量化过程对保存的知识的有害影响，导致灾难性的遗忘并破坏了预训练模型用于微调的用途。在这项工作中，我们引入了一种名为 ApiQ 的新型量化框架，旨在通过同时初始化 LoRA 组件和量化 LLM 的权重来恢复量化中丢失的信息。这种方法确保维持原始 LLM 的激活精度，同时减轻从浅层到更深层的误差传播。通过使用各种模型对一系列语言任务进行综合评估，ApiQ 明显最小化了量化过程中的激活误差。因此，它能够在各种量化位宽上始终实现卓越的微调结果。]]></description>
      <guid>https://arxiv.org/abs/2402.05147</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:35 GMT</pubDate>
    </item>
    <item>
      <title>FlowPG：具有标准化流量的行动约束策略梯度</title>
      <link>https://arxiv.org/abs/2402.05149</link>
      <description><![CDATA[行动约束强化学习（ACRL）是解决安全关键和资源分配相关决策问题的流行方法。 ACRL 的一个主要挑战是确保智能体采取有效的行动，满足每个 RL 步骤中的约束。在策略网络之上使用投影层的常用方法需要解决优化程序，这可能导致训练时间更长、收敛速度慢和零梯度问题。为了解决这个问题，首先我们使用归一化流模型来学习可行动作空间与潜在变量（例如高斯）上简单分布的支持之间的可逆、可微映射。其次，学习流模型需要从可行的动作空间中进行采样，这也具有挑战性。我们基于哈密顿蒙特卡罗和概率句子决策图开发了多种方法，用于凸和非凸约束的此类动作采样。第三，我们将学习到的归一化流程与 DDPG 算法相结合。根据设计，训练有素的标准化流程将策略输出转换为有效的操作，而不需要优化求解器。根据经验，我们的方法可以显着减少约束违规（在多个实例中达到一个数量级），并且在各种连续控制任务上速度提高数倍。]]></description>
      <guid>https://arxiv.org/abs/2402.05149</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:35 GMT</pubDate>
    </item>
    <item>
      <title>Tag-LLM：将通用 LLM 重新用于专业领域</title>
      <link>https://arxiv.org/abs/2402.05140</link>
      <description><![CDATA[大型语言模型（LLM）在理解和生成自然语言方面表现出了卓越的能力。然而，它们的能力在预训练语料库中代表性不足的高度专业化领域（例如物理和生物医学科学）中减弱。这项工作探讨了如何将通用法学硕士重新利用为专门领域的有效任务解决器。我们引入了一种新颖的、与模型无关的框架，用于学习自定义输入标签，这些标签被参数化为附加到 LLM 嵌入层的连续向量，以调节 LLM。我们设计了两种类型的输入标签：域标签用于界定专门的表示形式（例如化学式）并提供与域相关的上下文；函数标签用于表示特定函数（例如，预测分子特性）并压缩函数求解指令。我们开发了一个三阶段协议来使用辅助数据和领域知识来学习这些标签。通过明确地将任务域与任务函数分离，我们的方法可以通过输入标签的不同组合对未见过的问题进行零样本泛化。它还提高了法学硕士在各个专业领域的表现，例如预测蛋白质或化学性质以及建模药物-靶点相互作用，优于为这些任务量身定制的专家模型。]]></description>
      <guid>https://arxiv.org/abs/2402.05140</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>生存分析的在线学习方法</title>
      <link>https://arxiv.org/abs/2402.05145</link>
      <description><![CDATA[我们引入了用于生存分析的在线数学框架，允许实时适应动态环境和审查数据。该框架能够通过最佳二阶在线凸优化算法——在线牛顿步（ONS）来估计事件时间分布。这种以前未被探索过的方法具有显着的优势，包括具有非渐近收敛保证的显式算法。此外，我们分析了 ONS 超参数的选择，它取决于 exp-concavity 属性，并对遗憾界限有显着影响。我们提出了一种随机方法，保证 ONS 的对数随机遗憾。此外，我们引入了一种自适应聚合方法，可确保超参数选择的鲁棒性，同时保持快速后悔边界。本文的研究结果可以扩展到生存分析领域之外，并且适用于任何以差的指数凹性和不稳定的 ONS 为特征的案例。最后，通过模拟实验来说明这些主张。]]></description>
      <guid>https://arxiv.org/abs/2402.05145</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>使用动态结构化剪枝方法压缩深度强化学习网络以实现自动驾驶</title>
      <link>https://arxiv.org/abs/2402.05146</link>
      <description><![CDATA[深度强化学习（DRL）在复杂的自动驾驶场景中取得了显着的成功。然而，DRL模型不可避免地带来较高的内存消耗和计算量，这阻碍了其在资源有限的自动驾驶设备中的广泛部署。结构化剪枝已被认为是压缩和加速 DRL 模型的有用方法，但估计参数（即神经元）对 DRL 模型的贡献仍然具有挑战性。在本文中，我们介绍了一种新颖的动态结构化剪枝方法，该方法在训练阶段逐渐删除 DRL 模型的不重要神经元。我们的方法由两个步骤组成，即使用组稀疏正则器训练 DRL 模型，并使用动态剪枝阈值去除不重要的神经元。为了使用少量重要神经元有效训练 DRL 模型，我们采用神经元重要性组稀疏正则化器。与传统的正则化器相比，该正则化器对冗余神经元组施加惩罚，而这些神经元组不会显着影响 DRL 模型的输出。此外，我们设计了一种新颖的结构化剪枝策略来动态确定剪枝阈值，并使用二值掩码逐渐去除不重要的神经元。因此，我们的方法不仅可以去除 DRL 模型中冗余的神经元组，而且可以实现高性能和鲁棒性。实验结果表明，该方法在离散控制环境（即 CartPole-v1 和 LunarLander-v2）和 MuJoCo 连续环境（即 Hopper-v3 和 Walker2D-v3）上与现有的 DRL 剪枝方法具有竞争力。具体来说，我们的方法在四个具有挑战性的 DRL 环境中有效地压缩了 DRL 模型的 $93\%$ 神经元和 $96\%$ 权重，但精度略有下降。]]></description>
      <guid>https://arxiv.org/abs/2402.05146</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:34 GMT</pubDate>
    </item>
    </channel>
</rss>