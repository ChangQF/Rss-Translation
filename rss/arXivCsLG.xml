<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.lg arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.lg arxiv.org e-print档案中的更新。</description>
    <lastBuildDate>Fri, 04 Apr 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>当推理达到压缩时：基准在复杂的推理任务上进行大型推理模型</title>
      <link>https://arxiv.org/abs/2504.02010</link>
      <description><![CDATA[ARXIV：2504.02010V1公告类型：新 
摘要：最近的开源大型推理模型（LRMS）在复杂的推理任务上表现出很强的性能，但是它们的较大参数计数使它们对个人的昂贵。大型语言模型（LLM）的压缩提供了一种有效的解决方案，以降低计算资源的成本。但是，缺乏对压缩LLM在复杂推理任务（尤其是针对LRMS）中的性能的系统研究。大多数在量化和修剪专注于保持语言建模绩效方面的工作，而现有的蒸馏作品并未基于推理难度或压缩对知识和推理的影响进行基于基准的学生模型。在本文中，我们在四个不同的推理数据集（Aime 2024，对开本，大台式硬台和Musique的时间序列）上对DeepSeek-R1模型进行了基于压缩DeepSeek-R1模型，从数学到多跃化推理，使用量化，蒸馏，蒸馏和修剪方法。我们基准测试了采用动态量化的2.51-，1.73-和1.58位R1模型。我们还基于基于Llama或Qwen的蒸馏型R1型号，并在其上运行稀疏设备以获得各种稀疏​​水平。研究压缩LRM的性能和行为，我们报告了他们的性能得分和测试时间计算（在每个问题上花费的令牌数）。值得注意的是，使用Musique，我们发现参数计数对LRMS的知识记忆的影响要比其推理能力更大，这可以为压缩技术的选择提供信息。通过我们对测试时间计算的经验分析，我们发现，对于R1及其压缩变体的几个基准测试的模型输出通常比更长的模型输出更好，这突出了需要更简洁的推理链。]]></description>
      <guid>https://arxiv.org/abs/2504.02010</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机调节与数据有效扩散模型压缩的随机调节</title>
      <link>https://arxiv.org/abs/2504.02011</link>
      <description><![CDATA[ARXIV：2504.02011V1公告类型：新 
摘要：扩散模型通过渐进式denoing产生高质量的图像，但由于模型尺寸和重复采样而在计算密集程度上产生了密集型图像。知识蒸馏将知识从复杂的教师转移到更简单的学生模型，已被广泛研究以识别任务，尤其是在学生培训期间不见了的概念。但是，它在扩散模型中的应用仍未得到充满激光，尤其是在使学生模型能够生成训练图像不涵盖的概念时。在这项工作中，我们提出了随机调节，这是一种新型方法，该方法与随机选择的文本条件配对噪声图像，以实现有效的，无图像的知识蒸馏。通过利用这项技术，我们表明学生可以在培训图像中产生看不见的概念。当应用于条件扩散模型蒸馏时，我们的方法允许学生探索条件空间而无需产生特定条件的图像，从而显着提高了发电质量和效率。这促进了生成扩散模型的资源有效部署，从而扩大了其对研究和现实世界应用的可访问性。代码，模型和数据集可在https://dohyun-as.github.io/random-conditioning上找到。]]></description>
      <guid>https://arxiv.org/abs/2504.02011</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>指导引导的自回归神经网络参数生成</title>
      <link>https://arxiv.org/abs/2504.02012</link>
      <description><![CDATA[ARXIV：2504.02012V1公告类型：新 
摘要：学习以任务描述和体系结构规范为条件的神经网络参数是推进模型适应性和转移学习的关键。现有的方法尤其是基于扩散模型的方法，对大型架构的可扩展性有限，处理不同的网络深度的刚度以及破坏了层间相干性的分离参数生成。在这项工作中，我们提出了IGPG（指导引导参数生成），这是一个自动回归框架，统一跨不同任务和体系结构的参数合成。 IGPG利用VQ-VAE和自回归模型生成神经网络参数，以任务指令，数据集和体系结构详细信息为条件。通过自动加工生成神经网络权重的令牌，IGPG确保了层间相干性并在模型和数据集之间实现有效的适应性。 IGPG在代币级别运行，有效地捕获了从广泛的预验证模型汇总的复杂参数分布。在多个视觉数据集上进行的广泛实验表明，IGPG将各种预处理的模型合并为一个灵活的生成框架。合成的参数相对于最先进的方法具有竞争性或卓越的性能，尤其是在应用于大型体系结构时的可扩展性和效率方面。这些结果强调了ICPG的潜力作为预审预检测，模型选择和快速特定于任务的微调的强大工具。]]></description>
      <guid>https://arxiv.org/abs/2504.02012</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>注意Mamba：具有自适应合并加速度和接受场增强的时间序列建模</title>
      <link>https://arxiv.org/abs/2504.02013</link>
      <description><![CDATA[ARXIV：2504.02013V1公告类型：新 
摘要：“这项工作已提交给Leee以获取可能的出版物。版权可以在没有以下明显的情况下转移，而该版本可能无法再访问。”时间序列建模是现实世界应用的基石，例如天气预报和运输管理。最近，曼巴（Mamba）已成为一个有前途的模型，它将近线性计算复杂性与时间序列建模的高预测准确性相结合，同时面临诸如注意力依赖性的挑战，以及卷积引起的非线性依赖性模型不足。为了克服这些局限性，本文介绍了一个创新的框架，注意Mamba，其具有新颖的自适应池块，该块可以加速注意力计算并结合了全球信息，有效地克服了有限的接受场的约束。此外，注意Mamba整合了双向Mamba块，有效地捕获长短特征并将输入转换为注意机制的价值表示。在各种数据集上进行的广泛实验强调了注意Mamba在提取非线性依赖性和增强接受场的有效性，从而在领先的对应物中建立了出色的表现。我们的代码将在GitHub上提供。]]></description>
      <guid>https://arxiv.org/abs/2504.02013</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>卫星异常检测的实际NVP归一流流量模型的故障注射分析</title>
      <link>https://arxiv.org/abs/2504.02015</link>
      <description><![CDATA[ARXIV：2504.02015V1公告类型：新 
摘要：卫星用于多种应用，包括通信，地球观察和太空科学。现在，神经网络和深度学习方法代表了提高这些任务的性能和效率的最新方法。鉴于卫星容易受到各种断层的影响，因此人工智能（AI）的一种关键应用是故障检测。但是，尽管神经网络具有优势，但这些系统容易受到辐射错误的影响，这可能会对它们的可靠性产生重大影响。确保这些解决方案的可靠性需要广泛的测试和验证，尤其是使用故障注入方法。这项研究分析了物理知识（PI）实价的非体积保存（实际NVP）标准化流程模型，用于空间系统中的故障检测，重点是对单事件的抗气（SEUS）的弹性。我们在张量流中提出了一个定制的故障注入框架，以评估神经网络的弹性。通过两种主要方法应用了故障注射：层状态注入，靶向内部网络组件，例如权重和偏见，以及层输出注入，从而修改了各种激活的层输出。故障类型包括零，随机值和位流动操作，在不同的级别和不同的网络层上应用。我们的发现揭示了一些关键见解，例如临界点中位叶片错误的重要性，这可能导致大量的性能降级甚至系统故障。通过这项工作，我们旨在详尽地研究实际NVP模型的弹性，以防止由于辐射引起的错误，从而提供了一种指导实施可容忍度措施的方法。]]></description>
      <guid>https://arxiv.org/abs/2504.02015</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>傅立叶功能归因：一种新的效率归因方法</title>
      <link>https://arxiv.org/abs/2504.02016</link>
      <description><![CDATA[ARXIV：2504.02016V1公告类型：新 
摘要：从傅立叶特征的角度研究神经网络引起了极大的关注。尽管现有的分析研究表明，神经网络倾向于学习低频特征，但一种明确的归因方法来识别特定的学习傅立叶特征仍然难以捉摸。为了弥合这一差距，我们提出了一种基于信号分解理论的新型傅立叶特征归因方法。此外，我们分析了傅立叶和空间域特征的游戏理论归因指标之间的差异，这表明游戏理论评估指标更适合基于傅立叶的功能属性。
  我们的实验表明，与空间域归因方法相比，傅立叶特征归因具有出色的特征选择功能。例如，对于Imagenet数据集中的视觉变压器（VIT），仅需$ 8 \％$的傅立叶功能即可维持原始预测，以$ 80 \％的样本$。此外，我们将方法与传统的空间域归因方法进行比较。结果表明，傅立叶特征表现出更高的类内浓度和阶层间的独特性，表明它们具有更有效的分类和可解释的AI算法的潜力。]]></description>
      <guid>https://arxiv.org/abs/2504.02016</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>嵌入空间中的几何推理</title>
      <link>https://arxiv.org/abs/2504.02018</link>
      <description><![CDATA[ARXIV：2504.02018V1公告类型：新 
摘要：在此贡献中，我们证明了图形神经网络和变形金刚可以学会推理几何约束。我们训练它们以从一组唯一描述包含这些点的隐藏数字的约束中预测分离2D网格中点的空间位置。这两个模型都能够预测点的位置，有趣的是，它们形成了推理过程中嵌入空间中输入约束所描述的隐藏数字。我们的分析表明，两个模型在训练过程中恢复了网格结构，以便与网格中点相对应的嵌入在2D子空间中组织起来，并反映网格的邻域结构。我们还表明，我们为任务设计的图形神经网络的性能明显优于变压器，并且更易于扩展。]]></description>
      <guid>https://arxiv.org/abs/2504.02018</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Top-K Shapley识别的对比抽样</title>
      <link>https://arxiv.org/abs/2504.02019</link>
      <description><![CDATA[ARXIV：2504.02019V1公告类型：新 
摘要：加性功能说明主要依赖于游戏理论概念，例如通过将功能视为合作玩家，例如Shapley值。 Shapley价值在可解释的AI中的知名度源于其公理唯一性。但是，其计算复杂性严重限制了可行性。大多数作品都研究了所有功能的莎普利值的均匀近似，不必要地消费样品，以实现微不足道的特征。相比之下，确定$ k $最重要的功能已经足够有见地了，并产生了利用与多军强盗领域相关的算法机会的潜力。我们提出了可比的边际贡献采样（CMC），这是利用相关观察结果利用新的抽样方案的顶部$ K $识别问题的方法。我们进行实验以展示与竞争基线相比，我们的方法的功效。我们的经验发现表明，大约所有问题的估计质量不一定转移到顶部$ K $识别，反之亦然。]]></description>
      <guid>https://arxiv.org/abs/2504.02019</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>最佳运输的纽顿截断方法</title>
      <link>https://arxiv.org/abs/2504.02067</link>
      <description><![CDATA[ARXIV：2504.02067V1公告类型：新 
摘要：开发当代最佳运输（OT）求解器需要在几个关键要求之间进行权衡：GPU并行化，对高维问题的可扩展性，理论融合保证，在精确度与运行时的经验绩效以及在实践中的数值稳定性。考虑到这些挑战，我们引入了一种专门的截短的牛顿算法，用于熵限制的OT。除了在不假设Lipschitz Hessian的情况下证明局部二次收敛是可能的，我们还提供了最大程度地利用实践中局部收敛率的高率的策略。我们的GPU  - 平行算法表现出异常有利的运行时性能，比许多现有替代方案都能达到高精度的数量级。在24个问题集（12个数据集$ \ tims $ 2的成本功能）上进行了墙上锁定时间实验证明了这一点。该算法的可伸缩性在$ N \ 10^6 $的一个非常大的OT问题上显示，在弱企业正则化下大约解决。]]></description>
      <guid>https://arxiv.org/abs/2504.02067</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测量数据</title>
      <link>https://arxiv.org/abs/2504.02083</link>
      <description><![CDATA[ARXIV：2504.02083V1公告类型：新 
摘要：通过分析数据测量数据中的大数据中的固有歧管。首先，最佳传输在每个数据点上生成切线空间，从中揭示了固有维度。然后，Koopman维度降低过程从数据到固有歧管的非线性转换。在这里介绍了测量数据程序，并获得了令人鼓舞的结果。]]></description>
      <guid>https://arxiv.org/abs/2504.02083</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对基于自动编码器的深群集的介绍性调查 - 将聚类与深度学习相结合的沙盒</title>
      <link>https://arxiv.org/abs/2504.02087</link>
      <description><![CDATA[ARXIV：2504.02087V1公告类型：新 
摘要：自动编码器提供了一种从没有标签的数据中学习低维，非线性表示的一般方法。这是可以实现的，而无需对数据类型或其他领域知识做出任何特定的假设。一般性和领域不可知论与它们的简单性结合起来，使自动编码器成为研究和开发新颖（深）聚类算法的理想沙箱。聚类方法基于相似性组数据，该任务从自动编码器学到的较低维表示，从而减轻维度的诅咒。具体而言，深度学习与聚类（称为深群集）的组合使能够学习针对特定聚类任务的表示形式，从而导致高质量的结果。这项调查介绍了基于基于自动编码器的基本深层聚类算法，这些算法是许多现代方法的基础。]]></description>
      <guid>https://arxiv.org/abs/2504.02087</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FlowDistill：可扩展的交通流通过LLMS蒸馏预测</title>
      <link>https://arxiv.org/abs/2504.02094</link>
      <description><![CDATA[ARXIV：2504.02094V1公告类型：新 
摘要：准确的交通流量预测对于优化城市流动性至关重要，但是由于复杂的时空依赖性和有限的高质量数据，许多城市仍然很难。尽管基于图形的深度模型表现出强大的预测能力，但它们的性能通常以高计算开销和实质性培训数据要求为代价，这使得它们在资源受限或数据筛选环境中部署不切实际。我们提出了Flowdistill，这是一个基于大型语言模型（LLMS）知识蒸馏的轻巧且可扩展的交通预测框架。在这个教师学生的设置中，一个微调的LLM指导了紧凑的多层感知器（MLP）学生模型，使用信息瓶颈原理和教师结合的回归损失的新型组合，确保蒸馏模型保留仅保留必不可少的知识和可转移的知识。明确编码空间和时间相关性，以增强模型在各种城市环境中的概括。尽管它很简单，但Flowdistill在预测准确性方面始终胜过最先进的模型，同时需要较少的训练数据，并实现较低的内存使用和推理潜伏期，从而突出了其对现实世界，可扩展部署的效率和适合性。]]></description>
      <guid>https://arxiv.org/abs/2504.02094</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TIC-LM：时间限制性LLM预处理的网络尺度基准测试</title>
      <link>https://arxiv.org/abs/2504.02107</link>
      <description><![CDATA[ARXIV：2504.02107V1公告类型：新 
摘要：对历史网络数据培训的大型语言模型（LLM）不可避免地过时。随着新数据的可用，我们调查了LLMS的评估策略和更新方法。我们引入了一个网络尺度数据集，以用于从114个常见爬网（CC）垃圾场得出的LLMS的时间预处理 - 比以前的持续语言建模基准测试的数量级。我们还设计了一般CC数据和特定领域（Wikipedia，stackexchange和Code文档）的时间分层评估，以评估各种持续学习方法在保留过去知识的同时适应新数据的程度。我们的发现表明，在一般CC数据上，自回归的元时间间隔与固定比例重播的旧数据相结合可以实现与从头开始的重新训练的可比持有损失，同时需要较少的计算（2.6倍）。但是，合并新数据和重播旧数据之间的最佳平衡是重播至关重要的，对于避免忘记通用的Web数据而言，但在特定域上却较少。]]></description>
      <guid>https://arxiv.org/abs/2504.02107</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>polyg：具有自适应图遍历的有效有效的图形</title>
      <link>https://arxiv.org/abs/2504.02112</link>
      <description><![CDATA[ARXIV：2504.02112V1公告类型：新 
摘要：GraphRag通过从外部知识图中检索相关事实来增强大型语言模型（LLMS），从而为用户问题生成质量答案。现有的GraphRag方法采用固定的图形遍历策略进行事实检索，但我们观察到用户问题以不同的类型出现，需要不同的图形遍历策略。因此，现有的GraphRag方法的有效性有限（即生成的答案的质量）和/或效率（即响应时间或二手代币的数量）。在本文中，我们建议根据完整的四级分类法对问题进行分类，并适应每种类型的问题的适当的图形遍历策略。我们的系统Polyg本质上是GraphRag的查询计划者，可以通过统一的界面和执行引擎来处理各种问题。与SOTA GraphRag方法相比，Polyg的发电质量的总胜率为75％，响应时间的加速速度高达4倍。]]></description>
      <guid>https://arxiv.org/abs/2504.02112</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMPI：优化在Raspberry Pi上高通量的LLM</title>
      <link>https://arxiv.org/abs/2504.02118</link>
      <description><![CDATA[ARXIV：2504.02118V1公告类型：新 
摘要：在资源受限的边缘设备（例如Raspberry Pi）上部署大型语言模型（LLM）在计算效率，功耗和响应延迟方面提出了挑战。本文探讨了基于量化的优化技术，以实现低功率嵌入式系统上LLM的高通量，节能执行。我们的方法利用K量化的方法，一种专为不同的位宽度设计的训练后量化（PTQ）方法，可实现有效的2位，4位，6位和8位权重量化。此外，我们使用量化感知训练（QAT）对比特网模型采用三元量化，从而可以更有效地适应较低位的表示，同时保持准确性。
  我们的发现突出了在边缘设备上量化实时对话AI的LLM的潜力，为在移动和嵌入式应用程序中的低功率，高效率AI部署铺平了道路。这项研究表明，积极的量化策略可以显着降低能源消耗，同时保持推理质量，从而使LLM适用于资源有限的环境。]]></description>
      <guid>https://arxiv.org/abs/2504.02118</guid>
      <pubDate>Fri, 04 Apr 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>