<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Thu, 09 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>当前瞻剪枝遇上零阶优化：低内存设备的高效联邦学习</title>
      <link>https://arxiv.org/abs/2405.04765</link>
      <description><![CDATA[arXiv:2405.04765v1 公告类型：新
摘要：虽然联邦学习（FL）可以在人工智能物联网（AIoT）设计中实现协作学习，但由于其内存使用量较大，无法在低内存 AIoT 设备上工作。为了解决这个问题，提出了各种联合剪枝方法来减少推理过程中的内存使用。然而，很少有人能够在修剪和训练过程中显着减轻记忆负担。作为替代方案，零阶或无反向传播（BP-Free）方法可以部分减轻内存消耗，但它们会受到扩展和大量计算开销的影响，因为梯度估计误差和浮点运算（FLOP）随着模型参数的维数增加。在本文中，我们提出了一种基于神经正切核（NTK）的联邦前瞻剪枝方法，该方法可以与联邦无BP训练框架无缝集成。我们通过使用本地 NTK 矩阵提出了联合 NTK 计算的近似值。此外，我们证明了我们的方法的无数据特性可以大大减少极端数据异构情况下的近似误差。由于我们的方法以更少的 FLOP 提高了普通 BP-Free 方法的性能，并真正减轻了训练和推理过程中的内存压力，因此它使 FL 对低内存设备更加友好。从基于模拟和真实测试平台的综合实验结果表明，我们的联合前瞻剪枝方法不仅保留了密集模型的能力，内存减少高达 9 倍，而且还提高了普通 BP-Free 的性能FLOP 次数显着减少的方法。]]></description>
      <guid>https://arxiv.org/abs/2405.04765</guid>
      <pubDate>Thu, 09 May 2024 06:18:47 GMT</pubDate>
    </item>
    <item>
      <title>旅行推销员问题的测试时间增加</title>
      <link>https://arxiv.org/abs/2405.04767</link>
      <description><![CDATA[arXiv:2405.04767v1 公告类型：新
摘要：我们提出测试时间增强（TTA）作为解决组合优化问题（包括旅行商问题）的有效技术。一般来说，深度学习模型具有不变性，无论节点索引如何，输出都是唯一确定的，已被提出来有效地学习图结构。相反，我们将节点索引的排列（交换距离矩阵的元素）解释为 TTA 方案。结果表明，我们的方法能够获得比最新模型更短的解决方案。此外，我们还表明，找到更接近精确解的解的概率会随着增强大小的增加而增加。]]></description>
      <guid>https://arxiv.org/abs/2405.04767</guid>
      <pubDate>Thu, 09 May 2024 06:18:47 GMT</pubDate>
    </item>
    <item>
      <title>解开套索：变分惩罚目标的次梯度跟随</title>
      <link>https://arxiv.org/abs/2405.04710</link>
      <description><![CDATA[arXiv:2405.04710v1 公告类型：新
摘要：我们描述了一种新颖的次梯度跟随装置，用于计算具有变分惩罚的凸问题的最优值。在此设置中，我们接收序列 $y_i,\ldots,y_n$ 并寻求平滑序列 $x_1,\ldots,x_n$。平滑序列通过一般形式 $\sum_i g_i(x_{i+1}-x_i)$ 的加性变分惩罚获得输入序列的最小 Bregman 散度。作为我们装置的特例，我们推导了融合套索和等渗回归的已知算法。我们的方法还促进了新的变分惩罚，例如非光滑障碍函数。接下来我们推导并分析多变量问题，其中 $\mathbf{x}_i,\mathbf{y}_i\in\mathbb{R}^d$ 以及取决于 $\|\mathbf{x}_{i 的变分惩罚+1}-\mathbf{x}_i\|$。我们考虑的规范是 $\ell_2$ 和 $\ell_\infty$ ，它们促进了群体稀疏性。最后但并非最不重要的一点是，我们推导出一个基于格的次梯度，用于通过任意卷积滤波器的输出表征的变分惩罚。该范式为需要稀疏高阶离散导数（例如加速度和加加速度）的问题提供了有效的求解器。]]></description>
      <guid>https://arxiv.org/abs/2405.04710</guid>
      <pubDate>Thu, 09 May 2024 06:18:46 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络的条件局部特征编码</title>
      <link>https://arxiv.org/abs/2405.04755</link>
      <description><![CDATA[arXiv:2405.04755v1 公告类型：新
摘要：图神经网络（GNN）在基于图的数据学习方面取得了巨大成功。当前 GNN 的关键机制是消息传递，其中节点的特征根据从其本地邻居传递的信息进行更新。这种机制的局限性在于，随着我们使用更多轮的消息传递，节点特征越来越受到从邻居聚合的信息的支配。因此，随着 GNN 层数变深，相邻节点特征趋于相似，使得 GNN 更难以区分相邻节点，从而限制了 GNN 的性能。在本文中，我们提出了条件局部特征编码（CLFE）来帮助防止节点特征被局部邻域信息支配的问题。我们的方法的思想是从消息传递过程中提取节点隐藏状态嵌入并将其与前一阶段的节点特征连接起来，然后我们利用线性变换形成基于连接向量的CLFE。 CLFE将形成层输出以更好地保留节点特定信息，从而有助于提高GNN模型的性能。为了验证我们方法的可行性，我们在七个基准数据集上针对四个图域任务进行了广泛的实验：超像素图分类、节点分类、链接预测和图回归。实验结果一致表明，我们的方法提高了所有四个任务的各种基线 GNN 模型的模型性能。]]></description>
      <guid>https://arxiv.org/abs/2405.04755</guid>
      <pubDate>Thu, 09 May 2024 06:18:46 GMT</pubDate>
    </item>
    <item>
      <title>可解释的张量融合</title>
      <link>https://arxiv.org/abs/2405.04671</link>
      <description><![CDATA[arXiv:2405.04671v1 公告类型：新
摘要：传统的机器学习方法主要设计用于基于单一数据类型预测结果。然而，实际应用可能包含多种类型的数据，例如文本、图像和音频。我们引入了可解释张量融合（InTense），这是一种多模态学习方法，用于训练神经网络以同时学习多模态数据表示及其可解释融合。 InTense 可以分别捕获不同数据类型的线性组合和乘法交互，从而将高阶交互与每种模态的单独影响分开。 InTense 通过为模式及其关联分配相关性分数来提供开箱即用的可解释性。该方法具有理论依据，并在多个合成和现实数据集上产生有意义的相关性分数。对六个真实世界数据集的实验表明，InTense 在准确性和可解释性方面优于现有最先进的多模态可解释方法。]]></description>
      <guid>https://arxiv.org/abs/2405.04671</guid>
      <pubDate>Thu, 09 May 2024 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>基于边缘的内存计算架构上检索增强生成的稳健实现</title>
      <link>https://arxiv.org/abs/2405.04700</link>
      <description><![CDATA[arXiv:2405.04700v1 公告类型：新
摘要：部署在边缘设备上的大型语言模型（LLM）通过微调和更新其参数的特定部分来学习。尽管可以优化此类学习方法以减少资源利用率，但所需的总体资源仍然是边缘设备的沉重负担。相反，检索增强生成（RAG）是一种资源高效的LLM学习方法，可以在不更新模型参数的情况下提高LLM生成内容的质量。然而，基于 RAG 的 LLM 可能会涉及在每次用户与 LLM 交互中对个人资料数据的重复搜索。这种搜索可能会导致显着的延迟以及用户数据的积累。减少延迟的传统方法会限制保存的用户数据的大小，从而随着用户数据的不断增长而降低 RAG 的可扩展性。这仍然是一个悬而未决的问题：如何使 RAG 摆脱边缘设备延迟和可扩展性的限制？在本文中，我们提出了一种通过内存计算 (CiM) 架构来加速 RAG 的新颖框架。它通过在内存内执行原位计算来加速矩阵乘法，同时避免计算单元和内存之间昂贵的数据传输。我们的框架 Robust CiM-backed RAG (RoCR) 利用新颖的基于对比学习的训练方法和噪声感知训练，可以使 RAG 能够使用 CiM 有效地搜索配置文件数据。据我们所知，这是第一个利用 CiM 加速 RAG 的工作。]]></description>
      <guid>https://arxiv.org/abs/2405.04700</guid>
      <pubDate>Thu, 09 May 2024 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>通过自适应探索进行近端策略优化</title>
      <link>https://arxiv.org/abs/2405.04664</link>
      <description><![CDATA[arXiv:2405.04664v1 公告类型：新
摘要：自适应探索的近端策略优化（axPPO）是一种新颖的学习算法。本文研究了强化学习背景下的探索与利用权衡，旨在为强化学习算法设计提供新的见解。所提出的自适应探索框架根据代理的近期表现动态调整训练期间的探索幅度。我们提出的方法在学习效率方面优于标准 PPO 算法，特别是当学习过程开始时需要大量探索行为时。]]></description>
      <guid>https://arxiv.org/abs/2405.04664</guid>
      <pubDate>Thu, 09 May 2024 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>通过训练动力从理论上理解“逆转诅咒”</title>
      <link>https://arxiv.org/abs/2405.04669</link>
      <description><![CDATA[arXiv:2405.04669v1 公告类型：新
摘要：自回归大语言模型（LLM）显示出令人印象深刻的能力，可以解决许多复杂的推理任务，同时还可以处理一些简单的逻辑推理任务，例如逆向搜索：当训练“A is B”时，LLM 无法直接得出“A is B”的结论。在推理过程中“B 是 A”，这被称为“逆转诅咒”（Berglund et al., 2023）。在本文中，我们通过两个自回归模型的（随机）梯度下降的训练动力学从理论上分析了反转诅咒：（1）双线性模型，可以视为单层变压器的简化； (2) 使用 Tian 等人的框架的一层变压器。 （2023a）。我们的分析揭示了逆转诅咒发生的一个核心原因：两个自回归模型的（有效）权重都表现出不对称性，即训练期间从令牌$A$到令牌$B$的权重增加并不一定会导致权重从 $B$ 增加到 $A$。此外，我们的分析可以自然地应用于其他逻辑推理任务，例如思想链（COT）（Wei et al., 2022b）。我们证明了 COT 的必要性，即在 &#39;&#39;$A \to B$&#39;&#39; 和 &#39;&#39;$B \to C$&#39;&#39; 上训练的模型无法在没有 COT 的情况下直接得出 &#39;&#39;$A \to C$&#39;&#39; （Allen-Zhu 和 Li (2023) 也根据经验观察到），通过动态训练来实现单层 Transformer，这提供了与之前关注表达性的工作 (Feng et al., 2024) 不同的新视角。最后，我们还进行了实验来验证我们在不同设置下的多层变压器的理论。]]></description>
      <guid>https://arxiv.org/abs/2405.04669</guid>
      <pubDate>Thu, 09 May 2024 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>数据驱动的误差估计：没有技术债务的上限多重误差</title>
      <link>https://arxiv.org/abs/2405.04636</link>
      <description><![CDATA[arXiv:2405.04636v1 公告类型：新
摘要：我们将构建多个同时有效的置信区间（CI）的问题表述为估计一类/一组估计-估计-误差元组的最大误差的高概率上限，并将其称为误差估计问题。对于单个这样的元组，数据驱动的置信区间通常可用于限制我们估计中的误差。然而，对于一类估计-估计-误差元组，最大误差的非平凡高概率上限通常需要类复杂性作为输入——限制了此类方法的实用性，并且常常导致宽松的边界。我们提出了一种完全数据驱动的方法来估计最大误差的上限，而不是推导基于理论类复杂性的界限。我们针对这一基本挑战的解决方案的简单性和通用性使其适合多种应用，包括：多重 CI 构建、多重假设检验、估计任何训练/微调算法的超额风险界限（机器学习中不确定性的基本度量），并实现上下文老虎机管道的开发，该管道可以利用任何奖励模型估计程序作为输入（无需额外的数学分析）。]]></description>
      <guid>https://arxiv.org/abs/2405.04636</guid>
      <pubDate>Thu, 09 May 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>ACEGEN：用于药物发现的生成化学试剂的强化学习</title>
      <link>https://arxiv.org/abs/2405.04657</link>
      <description><![CDATA[arXiv:2405.04657v1 公告类型：新
摘要：近年来，强化学习（RL）已成为药物设计中的一种有价值的工具，为提出和优化具有所需特性的分子提供了潜力。然而，由于高级 RL 算法的复杂性以及对专用代码的严重依赖，在功能、灵活性和可靠性之间取得平衡仍然具有挑战性。在这项工作中，我们介绍了 ACEGEN，这是一个专为生成药物设计而定制的全面且简化的工具包，使用 TorchRL 构建，TorchRL 是一个现代决策库，提供高效且经过彻底测试的可重复使用组件。 ACEGEN 为分子设计提供了强大、灵活且高效的平台。我们通过对各种算法进行基准测试并进行多个药物发现案例研究来验证其有效性。 ACEGEN 可通过 https://github.com/acellera/acegen-open 访问。]]></description>
      <guid>https://arxiv.org/abs/2405.04657</guid>
      <pubDate>Thu, 09 May 2024 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>集成知识引导的符号回归和基于模型的实验设计以自动化流程图开发</title>
      <link>https://arxiv.org/abs/2405.04592</link>
      <description><![CDATA[arXiv:2405.04592v1 公告类型：新 
摘要：必须快速配制新产品才能在全球配方产品市场上取得成功；然而，关键产品指标 (KPI) 可能是复杂的、不太了解的化学成分和加工历史函数。因此，目前必须进行昂贵的反复试验才能扩大规模。为了加速流程图 (PFD) 优化和知识发现，这项工作提出了一种新颖的数字框架，通过将符号回归 (SR) 集成到基于模型的实验设计 (MBDoE) 中来自动量化过程机制。每次迭代，SR 都会提出一个可解释的机械表达式的帕累托前沿，然后 MBDoE 设计一个新的实验来区分它们，同时平衡 PFD 优化。为了研究该框架的性能，构建了一个能够模拟一般配方产品合成的新过程模型，以生成不同案例研究的计算机数据。该框架可以在几次迭代内有效地发现真实的过程机制，表明其在一般化学工业的数字化制造和产品创新方面具有巨大的潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.04592</guid>
      <pubDate>Thu, 09 May 2024 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>多边际损失：推荐系统中的提议和应用</title>
      <link>https://arxiv.org/abs/2405.04614</link>
      <description><![CDATA[arXiv:2405.04614v1 公告类型：新
摘要：推荐系统通过根据用户预测的偏好推荐项目来引导用户浏览大量信息。基于协作过滤的深度学习技术因其简单性、仅依赖于用户-项目交互而重新流行起来。通常，这些系统由三个主要组件组成：交互模块、损失函数和负采样策略。最初，研究人员专注于通过开发复杂的交互模块来提高性能。然而，最近出现了向细化损失函数和负采样策略的转变。这种转变导致人们对对比学习的兴趣增加，对比学习可以使相似的配对更加接近，同时将不同的配对推开。对比学习涉及大量数据增强、大批量和硬负采样等关键实践，但这些也带来了高内存需求和某些负样本利用不足等挑战。提出的多边距损失（MML）通过引入多个边距和改变负样本的权重来解决这些挑战。这使得 MML 不仅能够有效地利用最难的负例，而且还能够利用其他重要的负例，提供一种更简单但有效的损失函数，其性能优于更复杂的方法，尤其是在资源有限的情况下。在两个著名数据集上进行的实验表明，当使用较少数量的负样本时，与基线对比损失函数相比，MML 的性能提高了 20%。]]></description>
      <guid>https://arxiv.org/abs/2405.04614</guid>
      <pubDate>Thu, 09 May 2024 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>快速分散梯度跟踪，用于具有本地更新的联合极小极大优化</title>
      <link>https://arxiv.org/abs/2405.04566</link>
      <description><![CDATA[arXiv:2405.04566v1 公告类型：新
摘要：用于极小极大优化的联邦学习（FL）已成为跨分布式节点/客户端训练模型的强大范例，同时保护数据隐私和模型对数据异构性的鲁棒性。在这项工作中，我们通过提出 \texttt{K-GT-Minimax} 来深入研究联邦极小极大优化的去中心化实现，这是一种结合了局部更新和梯度跟踪技术的新型去中心化极小极大优化算法。我们的分析展示了该算法的通信效率和非凸强凹 (NC-SC) 极小极大优化的收敛速度，与现有方法相比，展示了卓越的收敛速度。 \texttt{K-GT-Minimax} 处理数据异构性并确保稳健性的能力强调了其在推进联邦学习研究和应用方面的重要性。]]></description>
      <guid>https://arxiv.org/abs/2405.04566</guid>
      <pubDate>Thu, 09 May 2024 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>对地下水位深度估计的批判性评估：机器学习中的挑战和机遇</title>
      <link>https://arxiv.org/abs/2405.04579</link>
      <description><![CDATA[arXiv:2405.04579v1 公告类型：新
摘要：地下水位深度（WTD）的精细分辨率空间模式可以为依赖地下水的系统（包括生态、水文和人为系统）的动态提供信息。一般来说，静态 WTD 的大尺度（例如大陆或全球）空间地图可以使用基于物理 (PB) 或基于机器学习 (ML) 的模型进行模拟。我们使用 XGBoost 算法以及美国和加拿大超过 2000 万个 WTD 真实和代理观测数据构建了 WTD 的三个高分辨率 (500 m) ML 模拟。这三个 ML 模型使用 WTD 驾驶员和 WTD 之间已知的物理关系进行约束，并通过顺序添加 WTD 的真实观察和代理观察进行训练。我们解释了物理约束机器学习模型的黑匣子，并将其与地下水水文学的现有文献进行了比较。通过广泛的（逐像素）评估，我们证明，与三种可用的 WTD PB 模拟相比，我们的模型可以更准确地预测北美大部分生态区域中未见的 WTD 真实和代理观测结果。然而，我们仍然认为大规模 WTD 估计还远未成为一个已解决的问题。我们的理由是，由于观测数据有偏差且不可信、基于物理的方程的错误指定以及机器学习模型的过度灵活性，我们的社区对 WTD 的 ML 或 PB 模拟的信心太高，并且可验证的 WTD 准确模拟确实存在问题。文献中尚不存在，特别是在干旱的高海拔地区。最终，我们深入讨论了未来的方向，这些方向可能有助于水文地质学家决定如何进行 WTD 估计，特别关注机器学习的应用。]]></description>
      <guid>https://arxiv.org/abs/2405.04579</guid>
      <pubDate>Thu, 09 May 2024 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>通过标签特征学习极限多标签分类中的标签-标签相关性</title>
      <link>https://arxiv.org/abs/2405.04545</link>
      <description><![CDATA[arXiv:2405.04545v1 公告类型：新
摘要：极端多标签文本分类（XMC）涉及学习一个分类器，该分类器可以为输入分配数百万个标签选择中最相关标签的子集。该领域最近的工作越来越关注对称问题设置，其中输入实例和标签特征本质上都是短文本。具有标签功能的短文本 XMC 在搜索广告中的查询与广告词组匹配、基于标题的产品推荐、相关搜索预测等领域得到了广泛的应用。在本文中，我们提出了 Gandalf，这是一种利用标签共现图来利用标签特征作为附加数据点来补充训练分布的新颖方法。通过利用短文本XMC问题的特点，它利用标签特征构建有效的训练实例，并使用标签图生成相应的软标签目标，从而有效地捕获标签与标签的相关性。令人惊讶的是，在这些新训练实例上训练的模型虽然不到原始数据集的一半，但可以优于在原始数据集上训练的模型，特别是在尾部标签的 PSP@k 指标上。有了这种洞察力，我们的目标是在原始训练实例和新训练实例上训练现有的 XMC 算法，从而使 4 个包含多达 130 万个标签的基准数据集的 6 种最先进算法平均相对改进 5% 。 Gandalf 可以以即插即用的方式应用于各种方法，从而推动该领域的最先进技术，而不会产生任何额外的计算开销。]]></description>
      <guid>https://arxiv.org/abs/2405.04545</guid>
      <pubDate>Thu, 09 May 2024 06:18:40 GMT</pubDate>
    </item>
    </channel>
</rss>