<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 09 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>重新思考深度学习：无需反向传播和统计优化即可在神经网络中传播信息</title>
      <link>https://arxiv.org/abs/2409.03760</link>
      <description><![CDATA[arXiv:2409.03760v1 公告类型：新
摘要：发展强人工智能标志着技术奇点的到来，对推动人类文明和解决社会问题做出了巨大贡献。神经网络 (NN) 和利用 NN 的深度学习由于其生物神经系统模仿结构而有望实现强人工智能。然而，常用的统计权重优化技术，如误差反向传播和损失函数，可能会阻碍神经系统的模仿。本研究通过解决修改后的国家标准与技术研究所 (MNIST) 数据库中的手写字符识别问题，讨论了 NN 作为神经系统模仿结构的信息传播能力和潜在的实际应用，而无需使用误差反向传播等统计权重优化技术。在本研究中，NN 架构包括使用阶跃函数作为激活函数的全连接层，具有 0-15 个隐藏层，并且没有权重更新。准确度是通过比较每个标签的训练数据的平均输出向量与测试数据的输出向量（基于向量相似性）来计算的。结果表明，最大准确度约为 80%。这表明 NN 无需使用统计权重优化即可正确传播信息。此外，准确度随着隐藏层数量的增加而降低。这是因为随着隐藏层数量的增加，输出向量的方差会降低，这表明输出数据变得平滑。本研究的 NN 和准确度计算方法很简单，还有各种改进空间。此外，创建一个反复循环“输入 -&gt; 处理 -&gt; 输出 -&gt; 环境响应 -&gt; 输入 -&gt; ...”的前馈 NN 可以为实际的软件应用铺平道路。]]></description>
      <guid>https://arxiv.org/abs/2409.03760</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 Lipschitz 优化加速基于神经子空间的可变形模拟降阶求解器</title>
      <link>https://arxiv.org/abs/2409.03807</link>
      <description><![CDATA[arXiv:2409.03807v1 公告类型：新
摘要：降阶模拟是一种加速高自由度物理模拟的新兴方法，最近开发的基于非线性子空间的神经网络方法已被证明在各种应用中都是有效的，因为可以检测到更简洁的子空间。然而，子空间内模拟目标的复杂性和景观尚未得到优化，这为提高收敛速度留下了空间。这项工作着重于这一点，提出了一种寻找优化子空间映射的通用方法，从而进一步加速神经降阶模拟，同时捕获配置流形的全面表示。我们通过优化模拟目标中弹性项的 Lipschitz 能量并将体积近似纳入训练过程来实现这一点，以管理与优化新引入的能量相关的高内存和时间需求。我们的方法是通用的，适用于有监督和无监督设置，以优化配置流形的参数化。我们通过准静态和动态模拟中的一般情况证明了我们方法的有效性。我们的方法实现了高达 6.83 的加速因子，同时在各种情况下始终保持相当的模拟精度，包括使用碰撞处理的大扭曲、弯曲和旋转变形。这种新颖的方法为加速物理模拟​​提供了巨大的潜力，并且可以很好地补充现有的基于神经网络的解决方案，以对复杂的可变形物体进行建模。]]></description>
      <guid>https://arxiv.org/abs/2409.03807</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经熵</title>
      <link>https://arxiv.org/abs/2409.03817</link>
      <description><![CDATA[arXiv:2409.03817v1 公告类型：新
摘要：我们通过扩散模型范式研究深度学习与信息论之间的联系。利用非平衡热力学中成熟的原理，我们可以描述逆转扩散过程所需的信息量。神经网络存储这些信息，并在生成阶段以类似麦克斯韦妖的方式运行。我们使用一种称为熵匹配模型的新型扩散方案来说明这个循环，其中在训练期间传递给网络的信息恰好对应于逆转期间必须消除的熵。我们证明这种熵可用于分析网络的编码效率和存储容量。这个概念图融合了随机最优控制、热力学、信息论和最优传输的元素，并提出了将扩散模型作为测试平台来理解神经网络的前景。]]></description>
      <guid>https://arxiv.org/abs/2409.03817</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于潜在空间能量的神经微分方程</title>
      <link>https://arxiv.org/abs/2409.03845</link>
      <description><![CDATA[arXiv:2409.03845v1 公告类型：新
摘要：本文介绍了一种用于表示连续时间序列数据的新型深度动态模型系列。该模型系列通过神经发射模型生成时间序列中的每个数据点，该模型是潜在状态向量的非线性变换。潜在状态的轨迹由神经常微分方程 (ODE) 隐式描述，初始状态遵循由基于能量的模型参数化的信息先验分布。此外，我们可以扩展此模型以将动态状态与潜在的静态变化因素区分开来，这些因素表示为潜在空间中的时间不变变量。我们使用马尔可夫链蒙特卡罗 (MCMC) 的最大似然估计以端到端的方式训练模型，而无需额外的辅助组件（例如推理网络）。我们对振荡系统、视频和真实世界状态序列（MuJoCo）进行的实验表明，具有可学习的基于能量的先验的 ODE 优于现有的 ODE，并且可以推广到新的动态参数化，从而实现长期预测。]]></description>
      <guid>https://arxiv.org/abs/2409.03845</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我们可以从理论上量化局部更新对联邦学习泛化性能的影响吗？</title>
      <link>https://arxiv.org/abs/2409.03863</link>
      <description><![CDATA[arXiv:2409.03863v1 公告类型：新
摘要：联邦学习 (FL) 因其在跨不同站点训练机器学习模型而无需直接数据共享方面的有效性而广受欢迎。虽然各种算法及其优化分析表明，具有局部更新的 FL 是一种通信效率高的分布式学习框架，但具有局部更新的 FL 的泛化性能却受到的关注相对较少。这种缺乏调查可以归因于数据异质性和 FL 框架内局部更新导致的不频繁通信之间的复杂相互作用。这促使我们研究 FL 中的一个基本问题：我们能否量化数据异质性和局部更新对 FL 泛化性能的影响，因为学习过程不断发展？为此，我们使用线性模型作为第一步，对 FL 的泛化性能进行了全面的理论研究，其中考虑了平稳和在线/非平稳情况的数据异质性。通过提供模型误差的闭式表达式，我们严格量化了三种设置（$K=1$、$K&lt;\infty$ 和 $K=\infty$）下局部更新数量（表示为 $K$）的影响，并展示了泛化性能如何随轮数 $t$ 而变化。我们的研究还全面了解了不同配置（包括模型参数数量 $p$ 和训练样本数量 $n$）如何影响整体泛化性能，从而为在网络上实施 FL 提供了新的见解（例如良性过度拟合）。]]></description>
      <guid>https://arxiv.org/abs/2409.03863</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高斯核无脊回归的过度拟合行为：变化的带宽或维数</title>
      <link>https://arxiv.org/abs/2409.03891</link>
      <description><![CDATA[arXiv:2409.03891v1 公告类型：新
摘要：我们考虑了高斯核岭回归（即核无岭回归）的最小范数插值解的过度拟合行为，当带宽或输入维度随样本大小变化时。对于固定维度，我们表明即使带宽变化或调整，无岭解决方案也永远不会一致，并且至少在噪声足够大的情况下，总是比零预测器更差。对于增加维度，我们给出了对样本大小的任何维度缩放的过度拟合行为的一般表征。我们使用它来提供使用具有亚多项式缩放维度的高斯核的良性过度拟合的第一个例子。我们所有的结果都是在高斯普遍性假设和核特征结构的（非严格）风险预测下得出的。]]></description>
      <guid>https://arxiv.org/abs/2409.03891</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>理解推荐系统的公平性：医疗保健视角</title>
      <link>https://arxiv.org/abs/2409.03893</link>
      <description><![CDATA[arXiv:2409.03893v2 公告类型：新
摘要：人工智能驱动的决策系统中的公平性已成为一个关键问题，尤其是当这些系统直接影响人类生活时。本文探讨了公众对医疗保健建议公平性的理解。我们进行了一项调查，参与者从四个公平指标（人口均等性、平等准确度、均等几率和阳性预测值）中选择了不同的医疗保健场景，以评估他们对这些概念的理解。我们的研究结果表明，公平是一个复杂且经常被误解的概念，公众对推荐系统中公平性指标的理解程度普遍较低。这项研究强调需要加强对算法公平性的信息和教育，以支持在使用这些系统时做出明智的决策。此外，结果表明，一刀切的公平方法可能不够，指出了情境敏感设计在开发公平的人工智能系统中的重要性。]]></description>
      <guid>https://arxiv.org/abs/2409.03893</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>跨异构环境的联邦 Q 学习收敛速度</title>
      <link>https://arxiv.org/abs/2409.03897</link>
      <description><![CDATA[arXiv:2409.03897v1 公告类型：新
摘要：大规模多智能体系统通常部署在广阔的地理区域，其中智能体与异构环境交互。人们对理解异质性在经典强化学习算法的联合版本性能中的作用产生了浓厚的兴趣。在本文中，我们研究了同步联合 Q 学习，旨在通过让 $K$ 个智能体在每 $E$ 次迭代中平均其局部 Q 估计来学习最佳 Q 函数。我们观察到 $K$ 和 $E$ 收敛速度的一个有趣现象。与同质环境设置类似，在减少由采样随机性引起的误差方面，$K$ 有一个线性加速。然而，与同质设置形成鲜明对比的是，$E&gt;1$ 会导致性能显著下降。具体而言，我们提供了在存在环境异质性的情况下误差演变的细粒度表征，随着迭代次数 $T$ 的增加，误差衰减为零。事实证明，$E&gt;1$ 的收敛速度缓慢是根本原因，而非我们分析的产物。我们证明，对于各种步长，误差的 $\ell_{\infty}$ 范数不能比 $\Theta (E/T)$ 衰减得更快。此外，我们的实验表明，收敛表现出一种有趣的两相现象。对于任何给定的步长，收敛都会出现急剧的相变：误差在开始时迅速衰减，但随后反弹并稳定下来。只要可以估计相变时间，为两个阶段选择不同的步长可以加快整体收敛速度。]]></description>
      <guid>https://arxiv.org/abs/2409.03897</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WaterMAS：神经网络水印的清晰度感知最大化</title>
      <link>https://arxiv.org/abs/2409.03902</link>
      <description><![CDATA[arXiv:2409.03902v1 公告类型：新
摘要：如今，深度神经网络被用于解决几个关键应用中的复杂任务，保护它们的完整性和知识产权 (IPR) 已变得至关重要。为此，我们提出了 WaterMAS，这是一种替代性的白盒神经网络水印方法，它改善了鲁棒性、不可感知性和计算复杂度之间的权衡，同时为增加数据有效载荷和安全性做出了准备。WasterMAS 插入保持水印权重不变，同时锐化其底层梯度空间。因此，通过限制攻击的强度来确保鲁棒性：即使对水印权重进行微小的改变也会影响模型的性能。通过在训练过程中插入水印来确保不可感知性。讨论了 WaterMAS 数据有效载荷、不可感知性和鲁棒性属性之间的关系。密钥由传递水印的权重的位置表示，这些位置是通过模型的多个层随机选择的。通过调查攻击者拦截密钥的情况来评估安全性。实验验证考虑了 5 个模型和 2 个任务（VGG16、ResNet18、MobileNetV3、用于 CIFAR10 图像分类的 SwinT 和用于 Cityscapes 图像分割的 DeepLabV3）以及 4 种类型的攻击（高斯噪声添加、修剪、微调和量化）。代码将在文章被接受后开源发布。]]></description>
      <guid>https://arxiv.org/abs/2409.03902</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>异步随机近似和平均奖励强化学习</title>
      <link>https://arxiv.org/abs/2409.03915</link>
      <description><![CDATA[arXiv:2409.03915v1 公告类型：新
摘要：本文研究异步随机近似 (SA) 算法及其在具有平均奖励标准的半马尔可夫决策过程 (SMDP) 中的强化学习中的应用。我们首先扩展了 Borkar 和 Meyn 的稳定性证明方法以适应更一般的噪声条件，从而为异步 SA 算法提供更广泛的收敛保证。利用这些结果，我们建立了 Schweitzer 经典相对值迭代算法 RVI Q 学习的异步 SA 类似物的收敛性，用于有限空间、弱通信 SMDP。此外，为了在此应用中充分利用 SA 结果，我们引入了新的单调性条件来估计 RVI Q 学习中的最佳奖励率。这些条件大大扩展了之前考虑的算法框架，我们在 RVI Q 学习的稳定性和收敛分析中使用新颖的证明论证来解决它们。]]></description>
      <guid>https://arxiv.org/abs/2409.03915</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用神经切线核处理认知不确定性和观测噪声</title>
      <link>https://arxiv.org/abs/2409.03953</link>
      <description><![CDATA[arXiv:2409.03953v1 公告类型：新
摘要：最近的研究表明，使用梯度下降训练宽神经网络在形式上等同于计算高斯过程 (GP) 中后验分布的均值，其中神经切线核 (NTK) 作为先验协方差和零随机噪声 \parencite{jacot2018neural}。在本文中，我们以两种方式扩展了这个框架。首先，我们展示了如何处理非零随机噪声。其次，我们推导出后验协方差的估计量，让我们能够处理认知不确定性。我们提出的方法与标准训练流程无缝集成，因为它涉及使用梯度下降在均方误差损失上训练少量额外的预测因子。我们通过对合成回归的实证评估证明了我们方法的概念验证。]]></description>
      <guid>https://arxiv.org/abs/2409.03953</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种高效、可推广的时间序列符号回归分析方法</title>
      <link>https://arxiv.org/abs/2409.03986</link>
      <description><![CDATA[arXiv:2409.03986v1 公告类型：新
摘要：时间序列分析和预测方法目前在定量分析方面表现出色，可以提供准确的未来预测和多样化的统计指标，但通常无法阐明时间序列的潜在演变模式。为了获得更全面的理解并提供有见地的解释，我们利用符号回归技术来推导时间序列变量演变中非线性动态的显式表达式。然而，这些技术在计算效率和跨各种现实世界时间序列数据的通用性方面面临挑战。为了克服这些挑战，我们提出了针对时间序列的\textbf{N}eural-\textbf{E}nhanced \textbf{Mo}nte-Carlo \textbf{T}ree \textbf{S}earch (NEMoTS)。NEMoTS 利用蒙特卡洛树搜索 (MCTS) 的探索-利用平衡，显着减少了符号回归中的搜索空间并提高了表达质量。此外，通过将神经网络与 MCTS 相结合，NEMoTS 不仅可以利用其出色的拟合能力在搜索空间缩减后集中精力进行更相关的操作，还可以取代复杂且耗时的模拟过程，从而大大提高时间序列分析的计算效率和通用性。NEMoTS 为时间序列分析提供了一种高效而全面的方法。使用三个真实世界数据集的实验证明了 NEMoTS 在性能、效率、可靠性和可解释性方面的显著优势，使其非常适合大规模真实世界时间序列数据。]]></description>
      <guid>https://arxiv.org/abs/2409.03986</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过有效的子目标指导从非专家观察中进行目标达成策略学习</title>
      <link>https://arxiv.org/abs/2409.03996</link>
      <description><![CDATA[arXiv:2409.03996v1 公告类型：新
摘要：在这项工作中，我们解决了从非专家、无行动观察数据中进行长期目标达成策略学习的挑战性问题。与完全标记的专家数据不同，我们的数据更易于访问，并且避免了昂贵的行动标记过程。此外，与通常涉及漫无目的的探索的在线学习相比，我们的数据为更有效的探索提供了有用的指导。为了实现我们的目标，我们提出了一种新颖的子目标指导学习策略。该策略背后的动机是，长期目标为有效探索和准确的状态转换提供了有限的指导。我们开发了一种基于扩散策略的高级策略来生成合理的子目标作为路径点，优先选择更容易导致最终目标的状态。此外，我们学习状态目标值函数以鼓励有效的子目标达成。这两个组件自然地集成到离策略演员-评论家框架中，从而通过信息探索实现有效的目标实现。我们在复杂的机器人导航和操作任务上评估了我们的方法，结果表明该方法比现有方法具有显著的性能优势。我们的消融研究进一步表明，我们的方法对于存在各种损坏的观测数据具有很强的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2409.03996</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>过度参数化的回归方法及其在半监督学习中的应用</title>
      <link>https://arxiv.org/abs/2409.04001</link>
      <description><![CDATA[arXiv:2409.04001v1 公告类型：新
摘要：最小范数最小二乘法是一种在过度参数化情况下的估计策略，在机器学习中，它被认为是理解深度学习本质的有用工具。在本文中，为了将其应用于非参数回归问题，我们建立了几种基于 SVD（奇异值分解）分量阈值的方法，称为 SVD 回归方法。我们考虑了几种方法，即基于奇异值的阈值、交叉验证的硬阈值、通用阈值和桥阈值。第一种方法不使用输出样本的信息，而其他方法则使用这些信息。然后，我们将它们应用于半监督学习，其中未标记的输入样本被纳入回归器的核函数中。真实数据的实验结果表明，根据数据集，SVD 回归方法优于朴素岭回归方法。不幸的是，利用输出样本信息的方法没有明显的优势。此外，对于依赖数据集，将未标记的输入样本合并到内核中被发现具有一定的优势。]]></description>
      <guid>https://arxiv.org/abs/2409.04001</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>统计信息指导的极不平衡分类</title>
      <link>https://arxiv.org/abs/2409.04101</link>
      <description><![CDATA[arXiv:2409.04101v1 公告类型：新
摘要：在现实世界的分类任务中经常遇到不平衡数据。以前关于不平衡学习的研究主要集中在少数类样本较少的学习上。然而，不平衡的概念也适用于少数类包含大量样本的情况，这通常是金融风险管理领域的欺诈检测等工业应用的情况。在本文中，我们采取了一种人口级的方法来解决不平衡学习问题，提出了一种称为 \emph{超不平衡分类} (UIC) 的新公式。在 UIC 下，即使有无限数量的训练样本，损失函数的行为也会有所不同。为了理解 UIC 问题的内在难度，我们借鉴了信息论的思想，并建立了一个框架，通过统计信息的视角来比较不同的损失函数。开发了一种称为可调增强损失的新型学习目标，该目标可证明能够抵抗 UIC 下的数据不平衡，并且通过对公共和工业数据集的大量实验研究验证了其经验有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.04101</guid>
      <pubDate>Mon, 09 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>