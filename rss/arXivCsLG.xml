<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 18 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>生成奖励模型</title>
      <link>https://arxiv.org/abs/2410.12832</link>
      <description><![CDATA[arXiv:2410.12832v1 公告类型：新
摘要：强化学习从人类反馈 (RLHF) 极大地提高了现代大型语言模型 (LLM) 的性能。RLHF 过程资源密集且技术挑战性大，通常需要大量人类偏好标签而不是模型生成的输出。强化学习从人工智能反馈 (RLAIF) 通过利用 LLM 生成的合成偏好来解决这一数据收集挑战。然而，最近的研究表明，合成偏好标签可能与人类偏好判断不太吻合。为了解决这个问题，我们提出了一种统一 RLHF 和 RLAIF 方法的混合方法。我们引入了 GenRM，这是一种迭代算法，它在自生成的推理轨迹上训练 LLM，从而产生与人类偏好判断相匹配的合成偏好标签。从经验上看，我们表明，与 Bradley-Terry 奖励模型相比，基于零样本 LLM 的判断在分布内任务上表现不佳（9-36% 之间）。相比之下，GenRM 实现了与 Bradley-Terry 模型相当的分布内准确度，同时在分布外任务上的表现明显优于后者（10-45%）。此外，GenRM 在分布内（9-31%）和分布外任务（2-6%）上的表现都超过了使用 LLM 作为判断者的表现。我们的结果表明，结合 RLHF 和 RLAIF 的优势，为提高合成偏好标签的质量提供了一种有前途的方法。]]></description>
      <guid>https://arxiv.org/abs/2410.12832</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>公平聚类数据汇总：改进的近似算法和复杂性洞察</title>
      <link>https://arxiv.org/abs/2410.12913</link>
      <description><![CDATA[arXiv:2410.12913v1 公告类型：新
摘要：数据汇总任务通常被建模为 $k$ 聚类问题，其目标是通过最小化聚类目标来选择最能代表数据集的 $k$ 个数据点（称为聚类中心）。一个流行的目标是最小化任何数据点与其最近中心之间的最大距离，这被形式化为 $k$ 中心问题。虽然在某些应用中可以选择所有数据点作为中心，但在一般情况下，必须从预定义的点子集（称为设施或供应商）中选择中心；这被称为 $k$ 供应商问题。在这项工作中，我们专注于公平数据汇总，建模为公平 $k$ 供应商问题，其中数据由多个组组成，并且必须从每个组中选择最少数量的中心，同时最小化 $k$ 供应商目标。这些组可以是不相交的或重叠的，从而导致两个不同的问题变体，每个变体具有不同的计算复杂度。
我们为这两种变体提出了 3 近似算法，从而改进了之前已知的 5 因子。对于不相交组，我们的算法在多项式时间内运行，而对于重叠组，我们提出了一种固定参数易处理算法，其中指数运行时间仅取决于组和中心的数量。我们表明，这些近似因子与理论下限相匹配，假设标准复杂性理论猜想。最后，使用开源实现，我们展示了我们的算法在大型合成数据集上的可扩展性，并评估了现实世界数据的公平性价格，比较了有和没有公平约束的解决方案质量。]]></description>
      <guid>https://arxiv.org/abs/2410.12913</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SoK：利用深度模型合并技术在损失景观中寻找共同点</title>
      <link>https://arxiv.org/abs/2410.12927</link>
      <description><![CDATA[arXiv:2410.12927v1 公告类型：新
摘要：了解神经网络对于创建可靠且值得信赖的深度学习模型至关重要。大多数当代可解释性研究通过因果干预或激活分析一次仅分析一个模型。然而，尽管取得了成功，但这些方法在我们对神经网络的训练行为、其内部表示如何出现以及如何可预测地将模型组件与特定于任务的行为相关联的理解方面仍然存在重大差距。为了从相关领域的工作中寻求新的见解，我们在此调查了模型合并领域的文献，该领域旨在通过合并它们的参数并在过程中识别特定于任务的模型组件来结合各种神经网络的能力。我们通过损失景观几何的视角分析模型合并文献，这种方法使我们能够将可解释性、安全性、模型合并和损失景观分析的实证研究的观察结果与控制神经网络训练及其内部表示出现的现象联系起来。为了系统化该领域的知识，我们提出了一种按核心算法原理组织的模型合并技术新分类法。此外，我们从这些领域的文献中提炼出反复的经验观察，以表征损失景观几何的四个主要方面：模式凸性、确定性、有向性和连通性。我们认为，通过提高我们对模型合并和损失景观几何原理的理解，这项工作有助于确保机器学习在实践中安全可靠。]]></description>
      <guid>https://arxiv.org/abs/2410.12927</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于局部离网天气预报的多模态图神经网络</title>
      <link>https://arxiv.org/abs/2410.12938</link>
      <description><![CDATA[arXiv:2410.12938v1 公告类型：新
摘要：野火管理和可再生能源发电等紧急应用需要精确、局部化的地球表面附近天气预报。然而，机器学习或数值天气模型的天气预报产品目前是在全球规则网格上生成的，简单的插值无法准确反映地面附近的细粒度天气模式。在这项工作中，我们端到端训练异构图神经网络 (GNN)，以将网格化预测缩小到感兴趣的离网位置。这种多模态 GNN 利用当地历史天气观测（例如风、温度）来校正不同提前期的网格化天气预报，以实现局部精确的预报。每个数据模态都被建模为图中不同类型的节点。使用消息传递，预测位置的节点会从其异构邻居节点聚合信息。使用美国东北部各地气象站进行的实验表明，我们的模型优于一系列数据驱动和非数据驱动的离网预测方法。我们的方法展示了如何弥合全球大规模天气模型与本地准确预测之间的差距，从而为本地决策提供信息。]]></description>
      <guid>https://arxiv.org/abs/2410.12938</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>机械化忘却：通过机械化定位实现稳健的知识忘却和编辑</title>
      <link>https://arxiv.org/abs/2410.12949</link>
      <description><![CDATA[arXiv:2410.12949v1 公告类型：新
摘要：大型语言模型中的知识编辑和反学习方法寻求在不影响一般语言建模性能的情况下编辑或删除不良知识或能力。这项工作研究了机械可解释性（部分旨在识别与构成模型能力的特定可解释机制相关的模型组件（电路））如何提高编辑和反学习的精度和有效性。我们发现，当训练通过不同方法本地化的组件时，反学习和编辑稳健性存在明显差异。我们强调了主要基于保留输出来定位组件的方法与寻找具有可预测中间状态的高级机制的方法之间的重要区别。具体来说，将编辑/反学习定位到与事实回忆查找表机制相关的组件 1) 可实现跨不同输入/输出格式的更稳健的编辑/反学习，2) 可抵御重新学习不需要的信息的尝试，同时与基线相比，还可减少意外的副作用，无论是在体育事实数据集还是跨多个模型的 CounterFact 数据集上。我们还发现，某些局部编辑比任何其他基线更能破坏模型中的潜在知识，从而使反学习对各种攻击更具鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2410.12949</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用侧扫声纳进行水下雷状物体检测的 Syn2Real 领域泛化</title>
      <link>https://arxiv.org/abs/2410.12953</link>
      <description><![CDATA[arXiv:2410.12953v1 公告类型：新
摘要：由于现实世界数据的稀缺，使用深度学习进行水下地雷探测受到限制。
这种稀缺性导致过度拟合，模型在训练数据上表现良好，但在看不见的数据上表现不佳。本文提出了一种使用扩散模型的 Syn2Real（合成到真实）领域泛化方法来应对这一挑战。我们证明，即使不是完全逼真的，DDPM 和 DDIM 模型生成的带噪声合成数据也可以有效地增强现实世界样本以进行训练。最终采样图像中的残余噪声提高了模型推广到具有固有噪声和高变化的现实世界数据的能力。当在合成和原始训练数据集的组合上进行训练时，基线 Mask-RCNN 模型的平均精度 (AP) 比仅在原始训练数据上进行训练时提高了约 60%。这一显着的改进凸显了 Syn2Real 领域泛化在水下地雷探测任务中的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.12953</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于 Shumailov 等人 (2024) 的注释：“使用递归生成的数据进行训练时，AI 模型会崩溃”</title>
      <link>https://arxiv.org/abs/2410.12954</link>
      <description><![CDATA[arXiv:2410.12954v1 公告类型：新
摘要：Shumailov 等人 (2024) 进行的研究表明，反复在合成数据上训练生成模型会导致模型崩溃。这一发现引起了人们的极大兴趣和争论，特别是考虑到当前的模型几乎已经耗尽了可用的数据。在这项工作中，我们研究了将分布（通过核密度估计或 KDE）或模型拟合到数据，然后从中重复采样的效果。我们的目标是对 Shumailov 等人 (2024) 观察到的现象进行理论理解。我们的结果表明，报告的结果是一种统计现象，可能是不可避免的。]]></description>
      <guid>https://arxiv.org/abs/2410.12954</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>闪存推理：针对长卷积序列模型及其他模型的近线性时间推理</title>
      <link>https://arxiv.org/abs/2410.12982</link>
      <description><![CDATA[arXiv:2410.12982v1 公告类型：新
摘要：虽然 Transformer 一直是序列生成模型最新进展的核心，但它们的计算成本在序列长度上仍然是二次的。已经提出了几种次二次架构来解决这个计算问题。其中一些，包括长卷积序列模型 (LCSM)，如 Hyena，在训练时解决了这个问题，但在推理过程中仍然是二次的。我们提出了一种将 LCSM 的精确推理加速到准线性 $O(L\log^2L)$ 时间的方法，确定了实现这一点的关键属性，并提出了一个利用这些属性的通用框架。我们的方法受到以前关于放松多项式插值的研究的启发，基于一种有助于减少内存移动和共享计算的平铺。它还有一个额外的好处，就是允许在架构的位置混合部分的各层之间几乎完全并行化。从经验上讲，我们为 Hyena 提供了概念验证实现，通过在位置混合部分中提高 50\times$，它比标准推理获得了高达 1.6\times$ 的端到端改进。]]></description>
      <guid>https://arxiv.org/abs/2410.12982</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于状态的连续控制的欧几里得数据增强强化学习</title>
      <link>https://arxiv.org/abs/2410.12983</link>
      <description><![CDATA[arXiv:2410.12983v1 公告类型：新
摘要：数据增强通过转换原始数据点来创建新的数据点，以供强化学习 (RL) 代理学习，这已被证明对于提高 RL 的连续控制数据效率非常有效。 之前针对这一目标的研究主要局限于基于扰动的数据增强，即通过扰动原始数据点来创建新的数据点，这对于 RL 代理将控制状态观察为带有扰动（包括随机裁剪、移位等）的图像的任务非常有效。 这项工作侧重于基于状态的控制，其中 RL 代理可以直接观察原始运动学和任务特征，并考虑基于旋转等变换下的欧几里得对称性对这些特征应用替代数据增强。 我们表明，基于关节配置的现有基准任务中使用的默认状态特征不适合欧几里得变换。因此，我们主张使用基于肢体配置（即关节连接的刚体）的状态特征，这些特征可以在欧几里得变换下提供丰富的增强数据。通过最少的超参数调整，我们证明了这种新的欧几里得数据增强策略可以显著提高强化学习在各种连续控制任务上的数据效率和渐近性能。]]></description>
      <guid>https://arxiv.org/abs/2410.12983</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>双贝叶斯学习</title>
      <link>https://arxiv.org/abs/2410.12984</link>
      <description><![CDATA[arXiv:2410.12984v1 公告类型：新
摘要：当代机器学习方法将尝试接近贝叶斯误差，因为它是任何模型可以实现的最低误差。本文假设任何决策都由两个贝叶斯决策组成，因此决策是一个双贝叶斯过程。本文展示了这种二元性如何暗示决策中的内在不确定性，以及它如何融入可解释性。所提出的方法认为，贝叶斯学习相当于为测量不确定性的对数函数找到一个基数，其解是固定点。此外，按照这种方法，黄金比例描述了满足贝叶斯定理的可能解。双贝叶斯框架建议使用与文献中用于训练随机梯度下降神经网络的值相似的学习率和动量权重。]]></description>
      <guid>https://arxiv.org/abs/2410.12984</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SSET：情感检测中时间序列分类器的交换滑动解释</title>
      <link>https://arxiv.org/abs/2410.12996</link>
      <description><![CDATA[arXiv:2410.12996v1 公告类型：新
摘要：机器学习 (ML) 模型的局部解释最近受到了广泛关注，因为它能够减少关于模型做出特定决策的原因的歧义。人们投入了大量精力来解决不同数据类型（尤其是图像）的可解释性问题。然而，对多变量时间序列数据的研究有限。一个可能的原因是，时间序列数据中时间和其他变量的混合可能会导致生成的解释对人类来说难以理解。此外，一些时间序列方面的努力未能提供准确的解释，因为它们要么忽略了时间域中的上下文，要么对 ML 模型施加了可微分性要求。这些限制阻碍了它们在实际应用和不可微分的 ML 设置中提供有效解释的能力。在本文中，我们提出了一种用于多变量时间序列分类器的交换滑动决策解释，称为 SSET。该提案包括交换和滑动阶段，其中导致预测分数显着下降的显着子序列将作为解释呈现。在前一个阶段，通过将感兴趣的序列与来自目标类别的接近训练数据交换来检测重要变量。在后一个阶段，通过在每个时间步骤上滑动窗口来探索这些变量的显着观察。此外，该模型以一种新颖的方式衡量不同变量随时间的重要性，这种方式以多种因素为特征。我们在情感检测领域利用 SSET，其中对两个真实世界的生理时间序列数据集 WESAD 和 MAHNOB-HCI 以及深度卷积分类器 CN-Waterfall 进行评估。该分类器在检测人类情感状态方面表现出优于先前模型的性能。将 SSET 与包括 LIME、积分梯度和 Dynamask 在内的几个基准进行比较，我们发现……]]></description>
      <guid>https://arxiv.org/abs/2410.12996</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于可扩展且准确的数据注释的 LLM 链集成</title>
      <link>https://arxiv.org/abs/2410.13006</link>
      <description><![CDATA[arXiv:2410.13006v1 公告类型：新
摘要：大型语言模型 (LLM) 执行零样本分类的能力使其成为快速发展领域中数据注释的可行解决方案，在这些领域中，优质标记数据通常稀缺且获取成本高昂。然而，大规模部署 LLM 的成本可能高得令人望而却步。本文介绍了一种 LLM 链集成方法，该方法将多个 LLM 按顺序对齐，并根据分类不确定性将数据子集路由到后续模型。这种方法利用了更广泛系统中各个 LLM 的优势，允许每个模型处理其表现出最高置信度的数据点，同时将更复杂的情况转发给可能更稳健的模型。我们的结果表明，链集成方法通常超过链中最佳单个模型的性能并实现显着的成本节省，使 LLM 链集成成为解决大规模数据注释挑战的实用而有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2410.13006</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用 CLIP 的 HiPS 攻击从图像中删除目标对象</title>
      <link>https://arxiv.org/abs/2410.13010</link>
      <description><![CDATA[arXiv:2410.13010v1 公告类型：新
摘要：众所周知，机器学习模型容易受到对抗性攻击，但传统攻击主要集中在单模态上。随着 CLIP 等结合了视觉和语言功能的大型多模态模型 (LMM) 的兴起，新的漏洞也随之出现。然而，之前在多模态定向攻击方面的工作旨在将模型的输出完全改变为对手想要的。在许多现实场景中，对手可能只会对输出进行细微的修改，以便下游模型甚至人类都不会注意到这些变化。我们引入了“隐藏在普通视线中” (HiPS) 攻击，这是一种新型的对抗性攻击，它通过有选择地隐藏目标对象来巧妙地修改模型预测，就好像目标对象不在场景中一样。我们提出了两种 HiPS 攻击变体，HiPS-cls 和 HiPS-cap，并证明了它们在转移到下游图像字幕模型（如 CLIP-Cap）方面的有效性，以便从图像字幕中删除有针对性的对象。]]></description>
      <guid>https://arxiv.org/abs/2410.13010</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>样本压缩方案减少</title>
      <link>https://arxiv.org/abs/2410.13012</link>
      <description><![CDATA[arXiv:2410.13012v1 公告类型：新
摘要：我们提出了从多类分类、回归和对抗性鲁棒学习设置中的样本压缩方案到二进制样本压缩方案的新方法。假设我们有一个大小为 $f(d_\mathrm{VC})$ 的二进制类压缩方案，其中 $d_\mathrm{VC}$ 是 VC 维度，那么我们得到以下结果：（1）如果二进制压缩方案是多数投票或稳定压缩方案，则存在一个大小为 $O(f(d_\mathrm{G}))$ 的多类压缩方案，其中 $d_\mathrm{G}$ 是图维度。此外，对于一般的二进制压缩方案，我们获得大小为 $O(f(d_\mathrm{G})\log|Y|)$ 的压缩方案，其中 $Y$ 是标签空间。 (2) 如果二进制压缩方案是多数票或稳定压缩方案，则存在一个 $\epsilon$ 近似压缩方案，用于大小为 $O(f(d_\mathrm{P}))$ 的 $[0,1]$ 值函数的回归，其中 $d_\mathrm{P}$ 是伪维数。对于一般的二进制压缩方案，我们获得大小为 $O(f(d_\mathrm{P})\log(1/\epsilon))$ 的压缩方案。如果样本压缩猜想得到解决（Littlestone 和 Warmuth，1986 年；Floyd 和 Warmuth，1995 年；Warmuth，2003 年），这些结果将具有重要意义。我们的结果将立即将猜想的证明扩展到其他设置。我们为对抗性鲁棒学习建立了类似的结果，并且还提供了一个具有鲁棒性但没有有界大小压缩方案的概念类的示例，证明了可学习性并不等同于具有独立于样本大小的压缩方案，这与二元分类不同，其中可以实现大小为 $2^{O(d_\mathrm{VC})}$ 的压缩（Moran and Yehudayoff，2016）。]]></description>
      <guid>https://arxiv.org/abs/2410.13012</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FedGTST：通过统计调整提高联邦模型的全局可转移性</title>
      <link>https://arxiv.org/abs/2410.13045</link>
      <description><![CDATA[arXiv:2410.13045v1 公告类型：新
摘要：迁移学习 (TL) 的性能在很大程度上依赖于有效的预训练，这需要大量数据集和大量计算资源。因此，执行 TL 对单个模型开发人员来说通常具有挑战性。联邦学习 (FL) 通过促进客户端之间的协作、间接扩展数据集、分配计算成本和保护隐私来解决这些问题。然而，关键挑战仍未解决。首先，现有的 FL 方法倾向于仅在本地域内优化可迁移性，而忽略了全局学习域。其次，大多数方法依赖于间接可迁移性指标，这些指标不能准确反映最终目标损失或真正的可迁移程度。为了解决这些差距，我们提出了对 FL 的两项增强。首先，我们引入了一种客户端-服务器交换协议，该协议利用跨客户端雅可比（梯度）范数来提高可迁移性。其次，我们在服务器上增加跨客户端的平均雅可比范数，并将其用作本地正则化器以减少跨客户端雅可比方差。我们的可转移联合算法称为 FedGTST（通过统计调整实现联合全局可转移性），表明增加平均雅可比矩阵并减少其方差可以更严格地控​​制目标损失。这导致目标损失在源损失和源-目标域差异方面的上限。在 MNIST 到 MNIST-M 和 CIFAR10 到 SVHN 等数据集上进行的大量实验表明，FedGTST 的表现优于相关基线，包括 FedSR。在第二对数据集上，当使用 LeNet 作为主干时，FedGTST 的准确率比 FedSR 提高了 9.8%，比 FedIIR 提高了 7.6%。]]></description>
      <guid>https://arxiv.org/abs/2410.13045</guid>
      <pubDate>Fri, 18 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>