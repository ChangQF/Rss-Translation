<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 23 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于主动学习的下游借口领域知识回溯</title>
      <link>https://arxiv.org/abs/2407.14720</link>
      <description><![CDATA[arXiv:2407.14720v1 公告类型：新
摘要：主动学习（AL）旨在通过迭代选择最具信息量的样本来构建高质量的标记数据集。这种采样严重依赖于数据表示，而最近预训练在鲁棒特征学习中很受欢迎。然而，由于预训练利用缺乏注释的低级借口任务，直接使用 AL 中的预训练表示不足以确定采样分数。为了解决这个问题，我们提出了一种下游借口领域知识回溯（DOKT）方法，该方法追踪下游知识和预训练指导的数据交互，以在决策边界附近选择多样化和有指导意义的样本。DOKT 由回溯多样性指标和基于领域的不确定性估计器组成。多样性指标基于预训练借口模型和来自注释的下游知识构建两个特征空间，通过它们在借口空间中定位来自下游空间的未标记数据的邻居以探索样本的相互作用。通过这种机制，DOKT 统一了低级和高级表示的数据关系，以估计回溯多样性。接下来，在不确定性估计器中，域混合旨在对借口空间中具有相似视觉块的未标记样本强制感知扰动。然后测量扰动样本的发散度以估计域不确定性。因此，DOKT 基于这两个模块选择了最多样化和最重要的样本。在十个数据集上进行的实验表明，我们的模型优于其他最先进的方法，并且可以很好地推广到各种应用场景，例如语义分割和图像字幕。]]></description>
      <guid>https://arxiv.org/abs/2407.14720</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:08 GMT</pubDate>
    </item>
    <item>
      <title>结合 R 和 Python 代码进行数据科学、机器学习和强化学习的综合指南</title>
      <link>https://arxiv.org/abs/2407.14695</link>
      <description><![CDATA[arXiv:2407.14695v1 公告类型：新
摘要：Python 因其有效性和丰富的库而在机器学习、人工智能和数据工程领域广受欢迎。而 R 仍然是统计分析和可视化的主要语言。然而，某些库已经过时，限制了它们的功能和性能。通过结合这两种编程语言，用户可以将 Python 的高级机器学习和人工智能功能与 R 强大的统计包一起使用。本文探讨了如何使用 R 的 reticulate 包从 R 调用 Python，提供实际示例并重点介绍这种集成可以提高生产力和分析能力的场景。通过一些 hello-world 代码片段，我们演示了如何运行 Python 的 scikit-learn、pytorch 和 OpenAI gym 库，轻松构建机器学习、深度学习和强化学习项目。]]></description>
      <guid>https://arxiv.org/abs/2407.14695</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:07 GMT</pubDate>
    </item>
    <item>
      <title>通用协调联邦学习差分隐私机制：提高准确性和收敛性</title>
      <link>https://arxiv.org/abs/2407.14710</link>
      <description><![CDATA[arXiv:2407.14710v1 公告类型：新 
摘要：差异隐私联邦学习 (DP-FL) 是一种很有前途的协作模型训练技术，同时可确保客户的可证明隐私。然而，优化隐私和准确性之间的权衡仍然是一个关键挑战。据我们所知，我们提出了第一个 DP-FL 框架（即 UDP-FL），它普遍协调任何随机化机制（例如，最佳机制）与高斯矩会计（即 DP-SGD），以显着提高准确性和收敛性。具体而言，UDP-FL 通过减轻对高斯噪声的依赖来展示增强的模型性能。这种转变中的关键中介变量是 R\&#39;enyi 差异隐私概念，它被谨慎地用于协调隐私预算。我们还提出了一种创新方法，基于模式连通性分析从理论上分析 DP-FL（包括我们的 UDP-FL）的收敛性。此外，我们通过大量实验对 UDP-FL 进行了评估，这些实验以最先进的 (SOTA) 方法为基准，结果显示 UDP-FL 在隐私保证和模型性能方面均表现出色。值得注意的是，UDP-FL 对不同的推理攻击表现出了很强的抵御能力，这表明在联邦学习环境中保护敏感数据方面取得了重大进展。]]></description>
      <guid>https://arxiv.org/abs/2407.14710</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:07 GMT</pubDate>
    </item>
    <item>
      <title>具有可证明保证的交叉注意差分隐私</title>
      <link>https://arxiv.org/abs/2407.14717</link>
      <description><![CDATA[arXiv:2407.14717v1 公告类型：新
摘要：交叉注意如今已成为许多重要人工智能应用的基本模块，例如检索增强生成 (RAG)、系统提示、引导稳定扩散等等。确保交叉注意隐私至关重要且迫切需要，因为它的键和值矩阵可能包含有关公司及其用户的敏感信息，其中许多公司仅从其系统提示或 RAG 数据中获利。在这项工作中，我们设计了一种新颖的差分隐私 (DP) 数据结构，以理论保证的方式解决交叉注意的隐私安全问题。具体来说，设 $n$ 为系统提示/RAG 数据的输入标记长度，$d$ 为特征维度，$0 &lt; \alpha \le 1$ 为相对误差参数，$R$ 为查询和键矩阵的最大值，$R_w$ 为值矩阵的最大值，$r,s,\epsilon_s$ 为多项式核方法的参数。然后，我们的数据结构需要 $\widetilde{O}(ndr^2)$ 内存消耗，初始化时间复杂度为 $\widetilde{O}(nr^2)$，单个标记查询的查询时间复杂度为 $\widetilde{O}(\alpha^{-1} r^2)$。此外，我们的数据结构可以保证用户查询是 $(\epsilon, \delta)$-DP，其中我们的输出与真实答案之间存在 $\widetilde{O}(n^{-1} \epsilon^{-1} \alpha^{-1/2} R^{2s} R_w r^2)$ 加性误差和 $n^{-1} (\alpha + \epsilon_s)$ 相对误差。此外，我们的结果对于自适应查询具有鲁棒性，在自适应查询中，用户可以故意攻击交叉注意系统。据我们所知，这是第一项为交叉注意提供 DP 的工作。我们相信它可以启发大型生成模型 (LGM) 中更多的隐私算法设计。]]></description>
      <guid>https://arxiv.org/abs/2407.14717</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:07 GMT</pubDate>
    </item>
    <item>
      <title>OASIS：离线安全强化学习的条件分布整形</title>
      <link>https://arxiv.org/abs/2407.14653</link>
      <description><![CDATA[arXiv:2407.14653v1 公告类型：新
摘要：离线安全强化学习 (RL) 旨在使用预先收集的数据集训练满足约束的策略。大多数当前方法都在努力解决不完美演示与期望的安全和奖励性能之间的不匹配问题。在本文中，我们介绍了 OASIS（条件分布塑造），这是离线安全 RL 中的一种新范式，旨在克服这些关键限制。OASIS 利用条件扩散模型来合成离线数据集，从而将数据分布塑造为有益的目标域。我们的方法通过有效的数据利用和正则化技术来遵守安全约束，从而使离线安全 RL 训练受益。对公共基准和不同数据集的综合评估展示了 OASIS 在帮助离线安全 RL 代理实现高奖励行为的同时满足安全约束方面的优势，优于既定的基线。此外，OASIS 表现出较高的数据效率和稳健性，使其适用于实际应用，特别是在安全性至关重要且高质量演示稀缺的任务中。]]></description>
      <guid>https://arxiv.org/abs/2407.14653</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:06 GMT</pubDate>
    </item>
    <item>
      <title>对于网络安全模型来说，$F_1$ 分数是否不够理想？引入 $C_{score}$，一种成本感知型模型评估替代方案</title>
      <link>https://arxiv.org/abs/2407.14664</link>
      <description><![CDATA[arXiv:2407.14664v1 公告类型：新
摘要：与机器学习分类器相关的错误成本，即假阳性和假阴性，并不相等，并且取决于应用程序。例如，在网络安全应用中，未检测到攻击的成本与将良性活动标记为攻击的成本非常不同。机器学习模型构建过程中的各种设计选择，例如超参数调整和模型选择，允许数据科学家在这两个错误之间进行权衡。然而，大多数常用的评估模型质量的指标，例如以模型精度和召回率定义的$F_1$分数，对这两个错误一视同仁，这使得用户难以针对这些错误的实际成本进行优化。在本文中，我们提出了一种新的成本感知指标，即基于精度和召回率的$C_{score}$，它可以替代$F_1$分数进行模型评估和选择。它包括一个成本比，该成本比考虑了处理假阳性和假阴性的不同成本。我们推导并描述了新的成本指标，并将其与 $F_1$ 分数进行比较。此外，我们使用此指标对五个网络安全相关数据集的多个成本比率进行模型阈值设定。结果显示平均成本节省了 49%。]]></description>
      <guid>https://arxiv.org/abs/2407.14664</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:06 GMT</pubDate>
    </item>
    <item>
      <title>价值内化：从社会奖励中学习和概括</title>
      <link>https://arxiv.org/abs/2407.14681</link>
      <description><![CDATA[arXiv:2407.14681v1 公告类型：新
摘要：社会奖励塑造人类行为。在发展过程中，照顾者引导学习者的行为朝着文化一致的目标和价值观发展。当照顾者不再在场，而学习者必须自主继续时，这些行为如何持续和概括？在这里，我们提出了一个价值内化的模型，其中社会反馈训练了一个内部社会奖励 (ISR) 模型，该模型在没有社会奖励时产生内部奖励。通过实证模拟，我们表明 ISR 模型可以防止代理忘记社会化行为，并能够在分布外的任务中进行概括。我们描述了不完全内化的含义，类似于 ISR 上的“奖励黑客”。此外，我们表明我们的模型在多代理环境中内化了亲社会行为。我们的工作为理解人类如何获得和概括价值观奠定了基础，并为将人工智能与人类价值观相结合提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2407.14681</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:06 GMT</pubDate>
    </item>
    <item>
      <title>数据中毒：对电网弹性的被忽视的威胁</title>
      <link>https://arxiv.org/abs/2407.14684</link>
      <description><![CDATA[arXiv:2407.14684v1 公告类型：新
摘要：随着动态数据驱动应用系统的复杂性增加，保持其弹性变得更具挑战性。例如，由于随机变量（如可再生能源输出）的数量不断增加以及极端天气事件给电网增加了不确定性，维持电网弹性变得越来越复杂。当前的优化方法难以适应这种复杂性的上升。这引发了人们对用于运营电网的数据驱动方法日益增长的兴趣，导致更容易受到网络攻击。一种经常讨论的此类破坏是对抗性破坏，入侵者试图在输入数据中添加一个小的扰动以“操纵”系统运行。在过去的几年里，对抗性训练和电力系统破坏的研究越来越受欢迎。在本文中，我们将首先回顾这些应用，特别是最常见的对抗性破坏类型：逃避和毒害破坏。通过这篇评论，我们强调了在应用于电网时，数据中毒和规避研究之间的差距。这是由于模型训练是安全的这一基本假设，导致规避中断成为研究的主要中断类型。最后，我们将研究数据中毒干预的影响，并展示它们如何危及电网的弹性。]]></description>
      <guid>https://arxiv.org/abs/2407.14684</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:06 GMT</pubDate>
    </item>
    <item>
      <title>交易魔鬼决赛：通过股票市场和贝叶斯优化进行后门攻击</title>
      <link>https://arxiv.org/abs/2407.14573</link>
      <description><![CDATA[arXiv:2407.14573v1 公告类型：新
摘要：自从生成人工智能问世以来，每家公司和研究人员都在争相开发自己的生成模型，无论是否商业化。鉴于这些强大的新工具的用户数量众多，目前还没有本质上可验证的方法从头开始解释 LLM（大型语言模型）学习时会发生什么。例如，那些基于自动语音识别系统的系统，它们必须依靠从网络各处收集的庞大而天文数字的数据来产生快速有效的结果，在本文中，我们开发了一种名为 MarketBackFinal 2.0 的后门攻击，基于声学数据中毒，MarketBackFinal 2.0 主要基于现代股票市场模型。为了展示可能依赖 LLM 的基于语音的 Transformer 的可能漏洞。]]></description>
      <guid>https://arxiv.org/abs/2407.14573</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>将语言模型评估为风险评分</title>
      <link>https://arxiv.org/abs/2407.14614</link>
      <description><![CDATA[arXiv:2407.14614v1 公告类型：新
摘要：当前问答基准主要关注可实现预测任务的准确性。以问题和答案为条件，最可能的标记是否与基本事实相匹配？这样的基准必然无法评估语言模型量化结果不确定性的能力。在这项工作中，我们专注于将语言模型用作不可实现预测任务的风险评分。我们引入了 folktexts，这是一个软件包，用于使用大型语言模型系统地生成风险评分，并根据基准预测任务对其进行评估。具体而言，该软件包从美国人口普查数据产品中得出自然语言任务，灵感来自流行的表格数据基准。灵活的 API 允许使用 28 个人口普查特征构建任何任务，这些特征的值映射到提示完成对。我们通过对 16 个最近的大型语言模型进行一系列实证分析，检查风险评分、校准曲线和各种评估指标，展示了 folktexts 的实用性。我们发现，零样本风险模型虽然具有很高的预测信号，但校准误差很大：基础模型高估了结果的不确定性，而指令调整模型低估了不确定性并产生了过于自信的风险分数。]]></description>
      <guid>https://arxiv.org/abs/2407.14614</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>BOND：将 LLM 与 Best-of-N 提炼相结合</title>
      <link>https://arxiv.org/abs/2407.14622</link>
      <description><![CDATA[arXiv:2407.14622v1 公告类型：新
摘要：从人类反馈中进行强化学习 (RLHF) 是最先进的大型语言模型中质量和安全性的关键驱动因素。然而，一个令人惊讶的简单而强大的推理时间策略是 Best-of-N 采样，它在 N 个候选中选择最佳代。在本文中，我们提出了 Best-of-N Distillation (BOND)，这是一种新颖的 RLHF 算法，旨在模拟 Best-of-N，但在推理时没有其显著的计算开销。具体而言，BOND 是一种分布匹配算法，它迫使策略中的代分布更接近 Best-of-N 分布。我们使用 Jeffreys 散度（前向和后向 KL 的线性组合）来平衡模式覆盖和模式寻求行为，并推导出一个利用移动锚点提高效率的迭代公式。我们通过对抽象总结和 Gemma 模型的实验证明了我们的方法和几种设计选择的有效性。将 Gemma 策略与 BOND 相结合可改善多个基准测试的结果，从而优于其他 RLHF 算法。]]></description>
      <guid>https://arxiv.org/abs/2407.14622</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>基于学习启发式技术的两种乳腺癌预测新特征选择方法：综合分析</title>
      <link>https://arxiv.org/abs/2407.14631</link>
      <description><![CDATA[arXiv:2407.14631v1 公告类型：新
摘要：乳腺癌无法预防，因为其病因不明。然而，早期诊断可以增加患者的康复机会。机器学习 (ML) 可用于改善医疗保健操作中的治疗结果，同时降低成本和时间。在这项研究中，我们提出了两种基于帝国主义竞争算法 (ICA) 和蝙蝠算法 (BA) 及其与 ML 算法结合的新型特征选择 (FS) 方法。本研究旨在提高诊断模型的效率，并提供全面的分析，以帮助临床医生做出比以前更精确、更可靠的决策。K-最近邻、支持向量机、决策树、朴素贝叶斯、AdaBoost、线性判别分析、随机森林、逻辑回归和人工神经网络是所采用的一些方法。本文分别使用基于 ICA (WFSIC) 和 BA (WFSB) 的包装器特征选择，应用了评估措施和 ML 算法的独特集成。我们比较了两种提出的方​​法的分类器性能。此外，我们将我们的最佳诊断模型与文献调查中报道的先前研究进行了比较。在威斯康星州乳腺癌诊断数据集上进行了实验。结果表明，使用 BA 的所提出的框架的准确率为 99.12%，超过了使用 ICA 的框架和大多数先前的研究。此外，基于 BA 的 FS 方法中的 RF 分类器成为最佳模型，并且在其标准方面优于其他模型。此外，结果说明了我们的技术在将数据集维度减少高达 90% 并将诊断模型的性能提高 99% 以上方面的作用。此外，结果表明，与大多数 ML 模型选择的所提出的 FS 方法获得的最佳数据集相比，还有更多关键特征。]]></description>
      <guid>https://arxiv.org/abs/2407.14631</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:05 GMT</pubDate>
    </item>
    <item>
      <title>足球的基础模型</title>
      <link>https://arxiv.org/abs/2407.14558</link>
      <description><![CDATA[arXiv:2407.14558v1 公告类型：新
摘要：我们提出了一个足球基础模型，该模型能够根据给定的输入动作序列预测足球比赛中的后续动作。作为概念验证，我们利用来自职业足球联赛的三个赛季的数据训练了一个 Transformer 架构。我们在定量和定性上将该 Transformer 架构的性能与两个基线模型进行了比较：马尔可夫模型和多层感知器。此外，我们还讨论了我们模型的潜在应用。我们在 https://github.com/danielhocevar/Foundation-Model-for-Soccer 上提供了我们方法的开源实现。]]></description>
      <guid>https://arxiv.org/abs/2407.14558</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>神经网络和 ASIC 的自动化和整体协同设计，实现像素内智能</title>
      <link>https://arxiv.org/abs/2407.14560</link>
      <description><![CDATA[arXiv:2407.14560v1 公告类型：新
摘要：极端边缘 AI 系统（例如用于辐射检测的读出 ASIC 中的系统）必须在严格的硬件约束（例如微米级尺寸、亚毫瓦功率和纳秒级速度）下运行，同时提供比传统架构明显的精度优势。寻找理想的解决方案意味着从在这些领域合并过程中爆炸式扩展的设计空间中识别最佳的 AI 和 ASIC 设计选择，从而创建非平凡的耦合，这些耦合随着约束的收紧共同作用于一小组解决方案。即使在小规模问题中，手动确定超过数十亿种可能性中的理想选择也是不切实际的，甚至是不可能的。现有的弥合这一差距的方法利用对硬件的理论理解来进行架构搜索。然而，在计算这些理论指标时所做的假设过于理想化，无法在艰难的实际实施搜索中提供足够的指导。同时，许多其他关键指标（如延迟）的理论估计甚至不存在，并且同样是可变的，取决于工艺设计套件 (PDK) 的参数。为了应对这些挑战，我们提出了一项研究，该研究采用多目标贝叶斯优化进行智能搜索，将神经网络搜索和 ASIC 综合集成到循环中。这种方法提供了有关所有跨域设计选择的集体影响的可靠反馈。我们通过找到有效且高效的神经网络的几个帕累托最优设计选择来展示我们方法的有效性，这些神经网络可以从读出 ASIC 的各个像素内的输入脉冲中执行实时特征提取。]]></description>
      <guid>https://arxiv.org/abs/2407.14560</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:04 GMT</pubDate>
    </item>
    <item>
      <title>NNsight 和 NDIF：民主化访问基础模型内部</title>
      <link>https://arxiv.org/abs/2407.14561</link>
      <description><![CDATA[arXiv:2407.14561v1 公告类型：新
摘要：最先进的基础模型规模庞大，限制了科学家对它们的使用，因为大模型的定制实验需要昂贵的硬件和复杂的工程，这对大多数研究人员来说是不切实际的。为了缓解这些问题，我们引入了 NNsight，这是一个开源 Python 包，具有​​简单、灵活的 API，可以通过构建计算图来表达对任何 PyTorch 模型的干预。我们还介绍了 NDIF，这是一个协作研究平台，研究人员可以通过 NNsight API 访问基础规模的 LLM。代码、文档和教程可在 https://www.nnsight.net 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.14561</guid>
      <pubDate>Wed, 24 Jul 2024 03:17:04 GMT</pubDate>
    </item>
    </channel>
</rss>