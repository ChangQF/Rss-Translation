<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 22 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>重新思考用于图像分类的 VLM 和 LLM</title>
      <link>https://arxiv.org/abs/2410.14690</link>
      <description><![CDATA[arXiv:2410.14690v1 公告类型：新
摘要：视觉语言模型 (VLM) 现在越来越多地与大型语言模型 (LLM) 合并，以实现新功能，特别是在改进交互性和开放式响应性方面。虽然这些都是非凡的功能，但 LLM 对增强长期存在的关键问题（即在一系列选择中对图像进行分类）的贡献仍不清楚。通过涉及七个模型、十个视觉理解数据集和每个数据集的多个提示变化的大量实验，我们发现，对于对象和场景识别，不利用 LLM 的 VLM 可以比利用 LLM 的 VLM 实现更好的性能。但与此同时，利用 LLM 可以提高需要推理和外部知识的任务的性能。为了应对这些挑战，我们提出了一个务实的解决方案：一个轻量级的修复，涉及一个相对较小的 LLM，它可以有效地将视觉任务路由到最适合该任务的模型。LLM 路由器使用由超过 250 万个视觉任务和模型准确度对示例构成的数据集进行训练。我们的结果表明，这种轻量级修复方法超越或匹配了包括 GPT-4V 和 HuggingGPT 在内的最先进替代方案的准确性，同时提高了成本效益。]]></description>
      <guid>https://arxiv.org/abs/2410.14690</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>QuAILoRA：LoRA 的量化感知初始化</title>
      <link>https://arxiv.org/abs/2410.14713</link>
      <description><![CDATA[arXiv:2410.14713v1 公告类型：新 
摘要：QLoRA 通过量化基础 LLM 降低了使用 LoRA 对大型语言模型 (LLM) 进行微调的内存成本。然而，量化会引入量化误差，这会对微调后的模型性能产生负面影响。在本文中，我们介绍了 QuAILoRA，这是一种用于 LoRA 的量化感知初始化，它通过减少初始化时的量化误差来减轻这种负面影响。我们的方法花费少量的计算开销来计算这种量化感知初始化，而不会增加微调的内存成本。我们使用几种不同的模型大小和系列在几个因果语言建模和下游评估任务上评估了我们的方法。我们观察到，几乎所有使用 QuAILoRA 微调的 LLM 都实现了更好的验证困惑度。在下游任务上进行评估时，我们发现 QuAILoRA 产生的改进与量化误差的负面影响成正比。平均而言，将 QuAILoRA 应用于 4 位 QLoRA 模型，可使验证困惑度降低 75%，下游任务准确度提高 86%，因为量化精度增加了一倍至 8 位，并且在微调期间不会增加 GPU 内存利用率。]]></description>
      <guid>https://arxiv.org/abs/2410.14713</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>算法设计的大型语言模型系统综述</title>
      <link>https://arxiv.org/abs/2410.14716</link>
      <description><![CDATA[arXiv:2410.14716v1 公告类型：新
摘要：算法设计 (AD) 对于有效解决各个领域的问题至关重要。大型语言模型 (LLM) 的出现显著增强了该领域的自动化和创新，提供了新的视角和更好的解决方案。在过去三年中，LLM 与 AD 的集成 (LLM4AD) 取得了重大进展，并在优化、机器学习、数学推理和科学探索等不同领域得到应用。鉴于该领域的快速发展和范围不断扩大，系统综述既及时又必不可少。本文对 LLM4AD 的著作进行了系统综述。首先，我们对现有研究进行了概述和总结。然后，我们从 LLM 的作用、搜索技术、提示策略和应用领域等四个维度对现有作品进行了系统分类和回顾。我们还讨论了每个领域的成就和挑战以及 LLM4AD 解决这些问题的能力。最后，我们探讨当前的局限性并提出了几个悬而未决的问题和未来研究的有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2410.14716</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SGLP：用于压缩大型深度模型的相似性引导快速层分区修剪</title>
      <link>https://arxiv.org/abs/2410.14720</link>
      <description><![CDATA[arXiv:2410.14720v1 公告类型：新
摘要：由于对计算和参数的要求很高，在资源受限的设备上部署基于深度神经网络 (DNN) 的网络仍然是一项重大挑战。为了解决这个问题，层修剪已成为一种有效的方法，可以减少网络规模并提高计算效率。然而，现有的层修剪方法大多忽略了复杂深度神经网络中不同层之间的内在联系和相互依赖性。这种疏忽可能导致修剪后的模型不能像预期的那样有效地保留预训练网络的基本特征。为了解决这一限制，我们提出了一种用于压缩大型深度模型的相似性引导快速层分区修剪 (SGLP)，该方法专注于从通过表示相似性划分的网络段中修剪层。具体而言，我们提出的方法首先利用中心核对齐 (CKA) 来指示预训练网络层之间的内部表示，这为我们提供了层修剪的有力基础。基于 CKA 得出的相似度矩阵，我们采用 Fisher 最优分割将网络划分为多个段，这为逐段移除层提供了基础。此外，我们的方法创新地采用 GradNorm 进行逐段层重要性评估，无需进行大量微调，最后修剪不重要的层以获得紧凑的网络。图像分类和大型语言模型 (LLM) 的实验结果表明，我们提出的 SGLP 在准确性和计算效率方面均优于最先进的方法，为在资源有限的平台上部署 DNN 提供了更有效的解决方案。我们的代码可在 https://github.com/itsnotacie/information-fusion-SGLP 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.14720</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>物理信号的现象学人工智能基础模型</title>
      <link>https://arxiv.org/abs/2410.14724</link>
      <description><![CDATA[arXiv:2410.14724v1 公告类型：新
摘要：这项工作的目的是开发一个物理信号的人工智能基础模型，该模型可以推广到各种现象、领域、应用和传感设备。我们提出了一种现象学方法和框架来创建和验证这样的人工智能基础模型。基于这个框架，我们开发并训练了一个模型，该模型基于 5.9 亿个跨模态传感器测量样本，从电流到流体流动再到光学传感器。值得注意的是，模型中没有引入任何物理定律或归纳偏差的先验知识。通过几次现实世界的实验，我们证明一个基础模型可以有效地编码和预测物理行为，例如机械运动和热力学，包括训练中未见过的现象。该模型还可以扩展到不同复杂程度的物理过程，从跟踪简单弹簧质量系统的轨迹到预测大型电网动态。这项工作凸显了为各种物理世界过程构建统一的人工智能基础模型的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.14724</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新思考状态空间模型的标记减少</title>
      <link>https://arxiv.org/abs/2410.14725</link>
      <description><![CDATA[arXiv:2410.14725v1 公告类型：新
摘要：状态空间模型 (SSM) 的最新进展引起了人们的极大兴趣，尤其是在针对并行训练和处理长距离依赖关系进行优化的模型中。像 Mamba 这样的架构已经通过选择性 SSM 扩展到数十亿个参数。为了促进使用 Mamba 的更广泛应用，探索其效率至关重要。虽然 token 减少技术提供了一种简单的后训练策略，但我们发现将现有方法直接应用于 SSM 会导致性能大幅下降。通过深入的分析，我们确定了这种失败的原因和当前技术的局限性。作为回应，我们提出了一种针对 SSM 的定制、统一的后训练 token 减少方法。我们的方法整合了 token 重要性和相似性，从而利用了修剪和合并，来设计一种细粒度的层内 token 减少策略。大量实验表明，与现有方法相比，我们的方法在 Mamba-2 的六个基准上将平均准确率提高了 5.7% 至 13.1%，同时显著降低了计算需求和内存要求。]]></description>
      <guid>https://arxiv.org/abs/2410.14725</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>结合长期数据训练短期交通预测模型</title>
      <link>https://arxiv.org/abs/2410.14726</link>
      <description><![CDATA[arXiv:2410.14726v1 公告类型：新
摘要：短期交通量预测对于智能交通系统至关重要，许多研究都集中在这一领域。然而，现有的大多数研究都集中在完善模型架构上，而忽略了训练数据的数量。因此，在彻底探索增强数据集的效果方面仍然存在明显的差距，尤其是训练中大量的历史数据。在本研究中，使用了两个包含纽约八年多出租车和自行车使用情况的数据集来测试这种效果。进行了实验以评估使用最近 12、24、48 和 96 个月的数据训练的模型的精度。结果发现，涵盖 96 个月的训练集有时会导致准确性降低，这可能是由于历史交通模式与当前交通模式之间的差异造成的。随后进行了分析以辨别不一致模式的潜在来源，其中可能包括协变量转移和概念转移。为了解决这些转变，我们提出了一种创新方法，使用加权方案对齐协变量分布以管理协变量转变，并结合环境感知学习方法来解决概念转变。基于真实数据集的实验证明了我们方法的有效性，它可以显著减少测试错误并确保在使用大规模历史数据进行训练时提高准确性。据我们所知，这项工作是首次尝试评估连续扩展训练数据集对交通预测模型准确性的影响。此外，我们的训练方法能够纳入大多数现有的短期交通预测模型，使其更适合长期历史训练数据集。]]></description>
      <guid>https://arxiv.org/abs/2410.14726</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用时段内和时段间特征增强地铁站客流预测</title>
      <link>https://arxiv.org/abs/2410.14727</link>
      <description><![CDATA[arXiv:2410.14727v1 公告类型：新
摘要：准确的地铁站短期客流预测对于使地铁站工作人员能够主动应对客流变化至关重要。尽管该领域已有文献，但缺乏有效整合不同时期特征（特别是期内和跨期特征）进行地铁站客流预测的研究。在本文中，我们提出了一种名为 \textbf{M}uti \textbf{P}eriod \textbf{S}patial \textbf{T}emporal \textbf{N}etwork \textbf{MPSTN} 的新模型，该模型通过将一维时间序列数据转换为基于时期的二维矩阵来利用不同时期的特征。折叠矩阵表现出与图像相似的结构特征，使得能够利用图像处理技术，特别是卷积神经网络 (CNN) 来整合不同时期的特征。因此，我们的 MPSTN 模型结合了 CNN 模块来提取不同时期的时间信息，以及图神经网络 (GNN) 模块来整合不同站点的空间信息。我们使用公开可用的数据集将我们的方法与各种最先进的时空数据预测方法进行了比较，并实现了最小的预测误差。我们的模型代码可在以下存储库中公开获取：https://github.com/xiannanhuang/MPSTN]]></description>
      <guid>https://arxiv.org/abs/2410.14727</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>论线性扩散与幂迭代的关系</title>
      <link>https://arxiv.org/abs/2410.14730</link>
      <description><![CDATA[arXiv:2410.14730v1 公告类型：新
摘要：最近，扩散模型因其令人印象深刻的生成能力而广受欢迎。这些模型学习训练数据集给出的隐式分布，并通过逆过程变换随机噪声来采样新数据，这可以被认为是逐步去噪。在这项工作中，我们将生成过程视为“相关机”，其中随机噪声与隐式给定分布相关地反复增强。为此，我们探索了线性情况，其中 MSE 意义上的最佳去噪器已知是 PCA 投影。这使我们能够将扩散模型理论与尖峰协方差模型联系起来，其中在秩为 1 的情况下，去噪器对噪声水平和训练数据量的依赖关系可以用分析表示。在一系列数值实验中，我们将这一结果扩展到一般低秩数据，并表明低频在生成过程中出现得更早，其中去噪基向量与真实数据的对齐程度更高，其对齐速率取决于其特征值。该模型使我们能够证明线性扩散模型的均值收敛到底层数据的首特征向量，类似于流行的幂迭代方法。最后，我们通过经验证明了我们的研究结果在线性情况之外的适用性，即在用于一般图像生成任务的深度非线性去噪器的雅可比矩阵中。]]></description>
      <guid>https://arxiv.org/abs/2410.14730</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MatryoshkaKV：通过可训练正交投影实现自适应 KV 压缩</title>
      <link>https://arxiv.org/abs/2410.14731</link>
      <description><![CDATA[arXiv:2410.14731v1 公告类型：新 
摘要：KV 缓存已成为大型语言模型 (LLM) 推理的事实上的技术，其中引入了形状（层数、头数、序列长度、特征维度）的张量来缓存历史信息以供自注意。随着模型和数据的大小增长，KV 缓存很快就会成为系统在存储和内存传输方面的瓶颈。为了解决这个问题，以前的研究通常集中于缓存张量的前三个轴进行压缩。本文对它们进行了补充，重点关注特征维度轴，利用低秩投影矩阵将缓存特征转换为维度降低的空间。我们首先研究通过主成分分析 (PCA) 进行数据压缩的规范正交投影方法。我们观察到 PCA 投影的问题，在低压缩率下性能会显著下降。为了弥补这一差距，我们建议使用精心设计的 Matryoshka 训练策略直接调整具有蒸馏目标的正交投影矩阵。训练后，我们会根据不同的压缩预算自适应地搜索各个层和头的最佳压缩率。与之前的研究相比，我们的方法可以轻松包含预先训练的 LLM，并在性能和压缩率之间保持平衡。我们通过经验见证了我们的训练过程的高数据效率，并发现对于 LLaMA2-7B-base 和 Mistral-7B-v0.3-base 等流行 LLM，我们的方法可以维持 90% 以上的性能，平均 KV 缓存压缩率为 60%（在某些极端情况下最高可达 75%）。]]></description>
      <guid>https://arxiv.org/abs/2410.14731</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SIFM：多粒度北极海冰预报的基础模型</title>
      <link>https://arxiv.org/abs/2410.14732</link>
      <description><![CDATA[arXiv:2410.14732v1 公告类型：新
摘要：北极海冰在全球气候中发挥着至关重要的作用，对极地生态系统和沿海社区都有着至关重要的影响。在过去的几年里，多种基于深度学习的泛北极海冰浓度 (SIC) 预测方法应运而生，并展示了优于基于物理的动力学模型的性能。然而，以前的方法以固定的时间粒度（例如亚季节或季节）预测 SIC，因此只利用粒度间信息，而忽略了丰富的粒度间相关性。不同时间粒度的 SIC 表现出累积效应，并且具有自然一致性，短期波动可能会影响长期趋势，而长期趋势为促进北极海冰的短期预测提供了有效的提示。因此，在本研究中，我们建议培养从北极海冰再分析数据中自然得出的时间多粒度，并通过我们的海冰基础模型为 SIC 建模提供统一的视角。 SIFM 经过精心设计，可利用粒度内和粒度间信息来捕获粒度一致的表示，从而提高预测能力。我们进行了广泛的实验，结果表明，SIFM 在特定时间粒度方面的表现优于现成的深度学习模型。]]></description>
      <guid>https://arxiv.org/abs/2410.14732</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>知识图谱嵌入：捕获关系属性的综合调查</title>
      <link>https://arxiv.org/abs/2410.14733</link>
      <description><![CDATA[arXiv:2410.14733v1 公告类型：新
摘要：知识图谱嵌入 (KGE) 技术在将符号知识图谱 (KG) 转换为数字表示中起着关键作用，从而增强了各种深度学习模型以用于知识增强应用。与实体不同，KG 中的关系是语义意义的载体，它们的准确建模对于 KGE 模型的性能至关重要。首先，我们解决关系中固有的复杂映射属性，例如一对一、一对多、多对一和多对多映射。我们全面总结了基于关系感知映射的模型、利用特定表示空间的模型、基于张量分解的模型和基于神经网络的模型。接下来，我们将重点关注捕捉各种关系模式，如对称、不对称、反转和组合，回顾采用改进的张量分解的模型、基于改进的关系感知映射的模型以及利用旋转操作的模型。随后，考虑到实体之间的隐式层次关系，我们介绍了包含辅助信息的模型、基于双曲空间的模型以及利用极坐标系的模型。最后，针对稀疏和动态 KGE 等更复杂的场景，本文讨论了未来的潜在研究方向。我们探索了一些创新思路，例如将多模态信息集成到 KGE 中、使用规则增强关系模式建模以及开发模型以捕获动态 KGE 设置中的关系特征。]]></description>
      <guid>https://arxiv.org/abs/2410.14733</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>心脏病预测的进展：用于早期检测和风险评估的机器学习方法</title>
      <link>https://arxiv.org/abs/2410.14738</link>
      <description><![CDATA[arXiv:2410.14738v1 公告类型：新
摘要：本文的主要目的是理解、评估和分析机器学习模型在使用临床数据预测心脏病风险中的作用、相关性和效率。虽然心脏病风险预测的重要性怎么强调也不为过，但机器学习 (ML) 在识别和评估各种特征对有无心脏病患者分类的影响以及生成可靠的临床数据集方面的应用同样重要。本研究主要依赖横断面临床数据。ML 方法旨在加强对心脏病预后过程中各种临床特征的考虑。一些特征成为强有力的预测因子，增加了重要的价值。本文评估了七个 ML 分类器：逻辑回归、随机森林、决策树、朴素贝叶斯、k-最近邻、神经网络和支持向量机 (SVM)。根据准确性指标评估每个模型的性能。值得注意的是，支持向量机 (SVM) 的准确率最高，为 91.51%，证实了其在预测能力方面在评估的模型中的优势。这项研究的总体结果凸显了先进的计算方法在评估、预测、改善和管理心血管风险方面的优势。换句话说，SVM 模型的强大性能说明了其在临床环境中的适用性和价值，为个性化医疗和医疗保健的进一步发展铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2410.14738</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用 DRAM 和 SSD 实现可持续且可访问的 LLM 推理，并具备混合精度和多级缓存功能</title>
      <link>https://arxiv.org/abs/2410.14740</link>
      <description><![CDATA[arXiv:2410.14740v1 公告类型：新
摘要：尽管大型语言模型 (LLM) 已经展示了卓越的能力，但它们庞大的参数数量和相关的大量计算使 LLM 的部署成为当今 AI 应用碳排放的主要部分。与 H$100$ 等现代 GPU 相比，如果我们能够利用 M$40$ 等老式 GPU（如图~\ref{fig:tisser} 所示，M$40$ 的碳排放量仅为 H$100$ 的三分之一）来提供 LLM 服务，那么它将具有显著的碳可持续性。然而，由于模型大小和中间激活数据巨大，此类 GPU 上可用的有限高带宽内存 (HBM) 通常无法支持 LLM 的加载，这使得它们的服务具有挑战性。例如，具有 $70$B 参数的 LLaMA2 模型通常需要 $128$GB 进行推理，这大大超过了 $3090$ GPU 中的 $24$GB HBM，即使考虑到额外的 $64$GB DRAM 仍然不可行。为了应对这一挑战，本文提出了一种混合精度和模型模块化算法，以便在资源受限的过时硬件上实现 LLM 推理。（精度表示数值精度，如 FP16、INT8、INT4）和多级缓存（M2Cache）。）
具体来说，我们的 M2Cache 首先对 LLM 中的神经元进行模块化并创建它们的重要性排名。然后，它在权重空间中采用动态稀疏混合精度量化机制，以减少每个解码步骤的计算需求和通信开销。它共同降低了与 LLM 推理相关的运营碳排放。此外，M2Cache 引入了 HBM、DRAM 和 SSD 的三级缓存管理系统，以补充动态稀疏混合精度推理。为了提高通信效率，M2Cache 在 HBM 中维护神经元级混合精度 LRU 缓存，在 DRAM 中维护更大的层感知缓存，并在 SSD 中维护完整模型。]]></description>
      <guid>https://arxiv.org/abs/2410.14740</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CAKD：基于解耦 Kullback-Leibler 散度的关联感知知识蒸馏框架</title>
      <link>https://arxiv.org/abs/2410.14741</link>
      <description><![CDATA[arXiv:2410.14741v1 公告类型：新
摘要：在知识蒸馏中，主要关注的是转换和平衡多个蒸馏组件。在这项工作中，我们强调彻底检查每个蒸馏组件的重要性，因为我们观察到并非所有元素都同样重要。从这个角度来看，我们将 Kullback-Leibler (KL) 散度分解为三个独特元素：二元分类散度 (BCD)、强相关散度 (SCD) 和弱相关散度 (WCD)。每个元素都呈现出不同程度的影响。利用这些见解，我们提出了相关感知知识蒸馏 (CAKD) 框架。CAKD 旨在优先考虑对预测影响最大的蒸馏组件方面，从而优化从教师到学生模型的知识转移。我们的实验表明，调整每个元素的影响可以提高知识转换的有效性。此外，有证据表明，我们新颖的 CAKD 框架在各种模型和数据集中的表现始终优于基线。我们的工作进一步凸显了密切检查蒸馏过程不同部分的影响的重要性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.14741</guid>
      <pubDate>Tue, 22 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>