<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Thu, 21 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过凸优化进行约束的概率电路</title>
      <link>https://arxiv.org/abs/2403.13125</link>
      <description><![CDATA[arXiv:2403.13125v1 公告类型：新
摘要：这项工作致力于将概率命题逻辑约束集成到概率电路（PC）编码的分布中。 PC 是一类易于处理的模型，可以进行高效计算（例如条件概率和边际概率），同时在某些领域实现最先进的性能。所提出的方法将 PC 和约束作为输入，并输出满足约束的新 PC。这是通过凸优化有效完成的，无需重新训练整个模型。实证评估表明，约束和 PC 的组合可以有多种用例，包括在稀缺或不完整数据下提高模型性能，以及在不影响模型适应性的情况下将机器学习公平性措施强制执行到模型中。我们相信，这些想法将为涉及逻辑和深度概率模型组合的多种其他应用带来可能性。]]></description>
      <guid>https://arxiv.org/abs/2403.13125</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>AdaFish：使用二阶信息进行快速低秩参数高效微调</title>
      <link>https://arxiv.org/abs/2403.13128</link>
      <description><![CDATA[arXiv:2403.13128v1 公告类型：新
摘要：大规模预训练模型的最新进展显着提高了自然语言处理和计算机视觉领域各种任务的性能。然而，这些模型中的大量参数需要大量的内存和计算资源来进行全面训练。为了使这些模型适应下游任务或特定的面向应用的数据集，利用预训练参数的参数高效微调方法已经引起了相当大的关注。然而，由于大量参数和纪元，它仍然可能非常耗时。在这项工作中，我们介绍了 AdaFish，这是一种高效的二阶算法，旨在加快基于低秩分解的微调框架内的训练过程。我们的主要观察结果是，相关的广义费舍尔信息矩阵要么是低秩的，要么是规模极小的。这样的广义 Fisher 信息矩阵被证明与 Hessian 矩阵等效。此外，我们证明了 AdaFish 的全局收敛性及其迭代/预言的复杂性。数值实验表明，我们的算法与最先进的 AdamW 方法相当有竞争力。]]></description>
      <guid>https://arxiv.org/abs/2403.13128</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>对抗性训练下的鲁棒 NAS：基准、理论及其他</title>
      <link>https://arxiv.org/abs/2403.13134</link>
      <description><![CDATA[arXiv:2403.13134v1 公告类型：新
摘要：神经架构搜索（NAS）的最新发展强调了考虑针对恶意数据的鲁棒架构的重要性。然而，搜索这些鲁棒架构的基准评估和理论保证明显缺乏，特别是在考虑对抗性训练时。在这项工作中，我们的目标是解决这两个挑战，做出双重贡献。首先，我们发布了一个全面的数据集，其中包含图像数据集上 NAS-Bench-201 搜索空间中大量经过对抗训练的网络的干净准确性和鲁棒准确性。然后，利用深度学习理论中的神经正切核（NTK）工具，我们建立了一种在多目标对抗训练下在清洁精度和鲁棒精度方面搜索架构的泛化理论。我们坚信，我们的基准测试和理论见解将通过可靠的可重复性、高效的评估和理论基础，使 NAS 社区受益匪浅，特别是在追求稳健的架构方面。]]></description>
      <guid>https://arxiv.org/abs/2403.13134</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>了解非线性：Shapley 交互揭示数据的底层结构</title>
      <link>https://arxiv.org/abs/2403.13106</link>
      <description><![CDATA[arXiv:2403.13106v1 公告类型：新
摘要：测量非线性特征交互是理解许多模型中复杂归因模式的既定方法。在本文中，我们使用 Shapley Taylor 交互指数（STII）来分析底层数据结构对各种模式、任务和架构中模型表示的影响。考虑到掩码和自回归语言模型（MLM 和 ALM）中的语言结构，我们发现 STII 在惯用表达中增加，并且 MLM 通过句法距离缩放 STII，比 ALM 更依赖其非线性结构中的语法。我们的语音模型研究结果反映了语音原理，即口腔的开放程度决定了音素根据其上下文而变化的程度。最后，我们研究图像分类器并说明特征交互直观地反映了对象边界。我们广泛的结果说明了跨学科工作和可解释性研究领域专业知识的好处。]]></description>
      <guid>https://arxiv.org/abs/2403.13106</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>分析部分共享对在线联邦学习抵御模型中毒攻击的弹性的影响</title>
      <link>https://arxiv.org/abs/2403.13108</link>
      <description><![CDATA[arXiv:2403.13108v1 公告类型：新
摘要：我们仔细研究了部分共享在线联邦学习（PSO-Fed）算法针对模型中毒攻击的恢复能力。 PSO-Fed 使客户端能够在每个更新轮次中仅与服务器交换模型估计的一小部分，从而减少了通信负载。模型估计的部分共享还增强了算法针对模型中毒攻击的鲁棒性。为了更好地了解这种现象，我们分析了 PSO-Fed 算法在存在拜占庭客户端的情况下的性能，这些恶意行为者可能会在与服务器共享本地模型之前通过添加噪声来巧妙地篡改本地模型。通过我们的分析，我们证明即使在模型中毒攻击的压力下，PSO-Fed 在均值和均方意义上也保持收敛。我们进一步推导了 PSO-Fed 的理论均方误差（MSE），并将其与步长、攻击概率、拜占庭客户端数量、客户端参与率、部分共享比率和噪声方差等各种参数联系起来。我们还表明，当 PSO-Fed 面临模型中毒攻击时，存在一个不平凡的最佳步长。我们广泛的数值实验的结果证实了我们的理论主张，并强调了 PSO-Fed 对抗拜占庭攻击的卓越能力，优于其他相关的领先算法。]]></description>
      <guid>https://arxiv.org/abs/2403.13108</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>医学预测问题中带有噪声标签的深度学习：范围界定审查</title>
      <link>https://arxiv.org/abs/2403.13111</link>
      <description><![CDATA[arXiv:2403.13111v1 公告类型：新
摘要：目的：医学研究面临着由于专家间差异和机器提取标签等因素造成的噪声标签的巨大挑战。尽管如此，标签噪声管理的采用仍然有限，并且标签噪声在很大程度上被忽略。为此，迫切需要针对问题空间进行范围界定审查。本次范围审查旨在全面审查基于深度学习的医学预测问题中的标签噪声管理，包括标签噪声检测、标签噪声处理和评估。还包括涉及标签不确定性的研究。
  方法：我们的范围界定审查遵循系统审查和荟萃分析的首选报告项目 (PRISMA) 指南。我们检索了 4 个数据库，包括 PubMed、IEEE Xplore、Google Scholar 和 Semantic Sc​​holar。我们的搜索词包括“噪声标签与医疗/保健/临床”、“不确定性与医疗/保健/临床”和“噪声与医疗/保健/临床”。
  结果：2016年至2023年间共有60篇论文符合纳入标准。调查了医学研究中的一系列实际问题。其中包括标签噪声的来源、标签噪声的影响、标签噪声的检测、标签噪声处理技术及其评估。提供了标签噪声检测方法和处理技术的分类。
  讨论：从方法论的角度来看，我们观察到医学界已经与更广泛的深度学习社区保持同步，因为大多数技术都是根据医学数据进行评估的。我们建议将标签噪声视为医学研究中的标准元素，即使它不是专门用于处理噪声标签。初始实验可以从易于实现的方法开始，例如抗噪声损失函数、加权和课程学习。]]></description>
      <guid>https://arxiv.org/abs/2403.13111</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>JaxUED：Jax 中的一个简单且可用的 UED 库</title>
      <link>https://arxiv.org/abs/2403.13091</link>
      <description><![CDATA[arXiv:2403.13091v1 公告类型：新
摘要：我们提出 JaxUED，一个开源库，在 Jax 中提供现代无监督环境设计 (UED) 算法的最小依赖实现。与之前基于 CPU 的实现相比，JaxUED 利用硬件加速获得约 100 倍的加速。受 CleanRL 的启发，我们提供快速、清晰、易于理解且易于修改的实现，旨在加速 UED 的研究。本文描述了我们的库并包含基线结果。代码可以在 https://github.com/DramaCow/jaxued 找到。]]></description>
      <guid>https://arxiv.org/abs/2403.13091</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>离线强化学习的简单要素</title>
      <link>https://arxiv.org/abs/2403.13097</link>
      <description><![CDATA[arXiv:2403.13097v1 公告类型：新
摘要：离线强化学习算法已被证明对与目标下游任务高度相关的数据集有效。然而，利用一个新的测试平台（MOOD），其中轨迹来自异构源，我们表明现有的方法在处理不同的数据时遇到了困难：当为相关但不同的任务收集的数据被简单地添加到离线缓冲区时，它们的性能会大大恶化。鉴于这一发现，我们进行了一项大型实证研究，提出并测试了几个假设来解释这一失败。令人惊讶的是，我们发现规模比算法考虑因素更重要，是影响性能的关键因素。我们证明，像 AWAC 和 IQL 这样的简单方法随着网络规模的增加，克服了 MOOD 中包含额外数据所带来的自相矛盾的故障模式，并且在规范的 D4RL 基准上明显优于先前最先进的算法。]]></description>
      <guid>https://arxiv.org/abs/2403.13097</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>AdaptSFL：资源受限边缘网络中的自适应分割联邦学习</title>
      <link>https://arxiv.org/abs/2403.13101</link>
      <description><![CDATA[arXiv:2403.13101v1 公告类型：新
摘要：深度神经网络日益复杂，为将其民主化到资源有限的边缘设备带来了重大障碍。为了应对这一挑战，分割联邦学习 (SFL) 已成为一种有前途的解决方案，它通过模型分区将主要​​训练工作负载转移到服务器，同时实现边缘设备之间的并行训练。然而，尽管系统优化极大地影响了资源受限系统下 SFL 的性能，但该问题在很大程度上仍然未知。在本文中，我们提供了 SFL 的收敛分析，量化了模型分裂（MS）和客户端模型聚合（MA）对学习性能的影响，作为理论基础。然后，我们提出了 AdaptSFL，一种新颖的资源自适应 SFL 框架，以在资源受限的边缘计算系统下加速 SFL。具体来说，AdaptSFL 自适应地控制客户端 MA 和 MS，以平衡通信计算延迟和训练收敛。跨各种数据集的广泛模拟验证了我们提出的 AdaptSFL 框架实现目标精度所需的时间比基准要少得多，这证明了所提出策略的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.13101</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:56 GMT</pubDate>
    </item>
    <item>
      <title>深度学习的基本组成部分：类别理论方法</title>
      <link>https://arxiv.org/abs/2403.13001</link>
      <description><![CDATA[arXiv:2403.13001v1 公告类型：新
摘要：深度学习尽管取得了令人瞩目的成就，但仍然是一个年轻的领域。与许多科学学科的早期阶段一样，它的特点是新现象的发现、临时设计决策以及缺乏统一和组合的数学基础。从错综复杂的反向传播实现，到不断增长的神经网络架构，再到双下降、缩放定律或上下文学习等新的、人们知之甚少的现象，深度学习中几乎没有统一的原则。本论文基于范畴论语言为深度学习奠定了新颖的数学基础。我们开发了一个新的框架，它是a）端到端的，b）不形式的，c）不仅是描述性的，而且是规范性的，这意味着它可以用具有足够功能的编程语言直接实现。我们还将许多现有的方法系统化，将文献中的许多现有的结构和概念置于同一保护伞下。在第一部分中，我们通过扩展先前定义的分类和 Para 结构来研究前者，并定义加权光学来研究后者，从而识别和建模深度学习系统参数化和双向性的两个主要属性。将它们结合起来可以产生参数加权光学、人工神经网络的分类模型等等。第二部分证明了第一部分中的抽象，并将它们应用于模型反向传播、架构和监督学习。我们提供微分的透镜理论公理化，不仅涵盖平滑空间，还涵盖布尔电路的离散设置。我们调查现有的神经网络架构，并开发新的分类模型。我们形式化了优化器的概念，最后将所有现有概念结合在一起，为监督学习提供了一个统一的组合框架。]]></description>
      <guid>https://arxiv.org/abs/2403.13001</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>用于监控工业批处理过程的混合无监督学习策略</title>
      <link>https://arxiv.org/abs/2403.13032</link>
      <description><![CDATA[arXiv:2403.13032v1 公告类型：新
摘要：工业生产过程，特别是制药行业，是复杂的系统，需要持续监控以确保效率、产品质量和安全。本文提出了一种用于监控复杂工业过程的混合无监督学习策略（HULS）。针对传统自组织映射 (SOM) 的局限性，特别是在数据集不平衡和过程变量高度相关的场景中，HULS 结合现有的无监督学习技术来应对这些挑战。为了评估 HULS 概念的性能，基于实验室批次进行了比较实验]]></description>
      <guid>https://arxiv.org/abs/2403.13032</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>BiLoRA：用于大型预训练模型的过拟合弹性低阶适应的双层优化框架</title>
      <link>https://arxiv.org/abs/2403.13037</link>
      <description><![CDATA[arXiv:2403.13037v1 公告类型：新
摘要：低秩适应（LoRA）是一种流行的方法，通过学习低秩增量矩阵来微调下游任务中的大规模预训练模型。尽管与完整的微调方法相比，LoRA 及其变体有效地减少了可训练参数的数量，但它们经常过度拟合训练数据，导致测试数据的泛化效果不佳。为了解决这个问题，我们引入了 BiLoRA，一种基于双层优化（BLO）的过度拟合微调方法。 BiLoRA 采用伪奇异值分解来参数化低秩增量矩阵，并将伪奇异向量和值的训练划分为两个不同的训练数据子集。这种划分嵌入到 BLO 框架的不同级别中，可以降低过度拟合单个数据集的风险。 BiLoRA 在涵盖自然语言理解和生成任务的 10 个数据集上进行了测试，并应用于各种众所周知的大型预训练模型，在可训练参数数量相似的情况下，其性能显着优于 LoRA 方法和其他微调方法。]]></description>
      <guid>https://arxiv.org/abs/2403.13037</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>基于多层感知器的简单全谱相关k分布模型</title>
      <link>https://arxiv.org/abs/2403.12993</link>
      <description><![CDATA[arXiv:2403.12993v1 公告类型：新
摘要：虽然神经网络已成功应用于大范围热力学的全谱 k 分布（FSCK）方法，并通过训练的多层感知器（MLP）模型预测 k 值，但所需的 a 值仍然需要是即时计算的，理论上这会降低 FSCK 方法的性能并可能导致错误。另一方面，当前MLP模型结构过于复杂，不可避免地降低了计算效率。因此，为了在精度、效率和存储之间进行补偿，开发了基于 FSCK 方法的性质设计的简单 MLP，即简单 FSCK MLP (SFM) 模型，从中可以得出相关的 k 值和相应的 ka 值有效获得。已经进行了几个测试用例来比较开发的 SFM 模型和其他 FSCK 工具，包括查找表和传统 FSCK MLP (TFM) 模型。结果表明，SFM 模型能够以远低于 TFM 模型的微小计算成本实现甚至优于查找表的优异精度。考虑到准确性、效率和可移植性，SFM模型不仅是光谱特性预测的优秀工具，而且还提供了一种减少非线性效应带来的误差的方法。]]></description>
      <guid>https://arxiv.org/abs/2403.12993</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>Duwak：大型语言模型中的双水印</title>
      <link>https://arxiv.org/abs/2403.13000</link>
      <description><![CDATA[arXiv:2403.13000v1 公告类型：新
摘要：随着大型语言模型（LLM）越来越多地用于文本生成任务，审计其使用情况、管理其应用程序并减轻其潜在危害至关重要。现有的水印技术在嵌入单个人类不可感知和机器可检测的模式方面被证明是有效的，而不会显着影响生成的文本质量和语义。然而，检测水印的效率，即断言检测具有重要性和针对后期编辑的鲁棒性所需的最小标记数量，仍然存在争议。在本文中，Duwak 提出，通过在令牌概率分布和采样方案中嵌入双重秘密模式，从根本上提高水印的效率和质量。为了减轻因偏向某些标记而导致的表达退化，我们设计了一种对比搜索来对采样方案加水印，从而最大限度地减少标记重复并增强多样性。我们从理论上解释了 Duwak 中两个水印的相互依赖性。我们针对四种最先进的水印技术及其组合，在各种后期编辑攻击下对 Llama2 上的 Duwak 进行了广泛的评估。我们的结果表明，Duwak 标记的文本以最低的检测所需标记数量实现了最高的水印文本质量，比现有方法减少了 70% 的标记，尤其是在后释义情况下。]]></description>
      <guid>https://arxiv.org/abs/2403.13000</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>将机器学习与量子张量网络结合起来</title>
      <link>https://arxiv.org/abs/2403.12969</link>
      <description><![CDATA[arXiv:2403.12969v1 公告类型：新
摘要：本文研究了张量网络在语言建模中的使用，它可以有效地表示高维量子态。它是（van der Poel，2023）中所做工作的提炼和延续。为此，我们将问题抽象为 Motzkin 自旋链建模，它表现出长程相关性，让人想起语言中的相关性。矩阵积状态 (MPS) 也称为张量序列，其键维数随着其建模序列的长度而缩放。为了解决这个问题，我们使用因子核心 MPS，其键维次线性缩放。我们发现张量模型达到了近乎完美的分类能力，并且随着有效训练样本数量的减少而保持稳定的性能水平。]]></description>
      <guid>https://arxiv.org/abs/2403.12969</guid>
      <pubDate>Thu, 21 Mar 2024 06:16:53 GMT</pubDate>
    </item>
    </channel>
</rss>