<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>SAFL：通过客户端特定聚类和 SCSI 引导模型修剪实现结构感知个性化联合学习</title>
      <link>https://arxiv.org/abs/2501.18659</link>
      <description><![CDATA[arXiv:2501.18659v1 公告类型：新
摘要：联邦学习 (FL) 使客户端能够协作训练机器学习模型而无需共享本地数据，从而在不同环境中保护隐私。虽然传统的 FL 方法可以保护隐私，但它们通常会面临高计算和通信开销的问题。为了解决这些问题，引入了模型修剪作为简化计算的策略。然而，现有的修剪方法在仅基于本地数据应用时，通常会由于数据不足而产生不能充分反映客户端特定任务的子模型。为了克服这些挑战，本文介绍了 SAFL（结构感知联邦学习），这是一种新颖的框架，可通过特定于客户端的聚类和相似客户端结构信息 (SCSI) 指导的模型修剪来增强个性化联邦学习。SAFL 采用两阶段过程：首先，它根据数据相似性对客户端进行分组，并使用聚合修剪标准来指导修剪过程，从而有助于识别最佳子模型。随后，客户端训练这些修剪后的模型并进行基于服务器的聚合，确保为每个客户端提供量身定制的高效模型。这种方法显著减少了计算开销，同时提高了推理准确性。大量实验表明，SAFL 显著减小了模型大小并提高了性能，使其在以异构数据为特征的联合环境中非常有效。]]></description>
      <guid>https://arxiv.org/abs/2501.18659</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BARNN：贝叶斯自回归和循环神经网络</title>
      <link>https://arxiv.org/abs/2501.18665</link>
      <description><![CDATA[arXiv:2501.18665v1 公告类型：新
摘要：自回归和循环网络在从天气预报到分子生成和大型语言模型等各个领域都取得了显著进展。尽管这些模型具有强大的预测能力，但它们缺乏解决不确定性的严格框架，而不确定性是 PDE 求解、分子生成和机器学习力场等科学应用的关键。为了解决这一缺点，我们提出了 BARNN：一种变分贝叶斯自回归和循环神经网络。BARNN 旨在提供一种将任何自回归或循环模型转变为其贝叶斯版本的原则性方法。BARNN 基于变分 dropout 方法，允许将其应用于大型循环神经网络。我们还引入了“后验变分混合”先验（tVAMP-prior）的时间版本，以使贝叶斯推理高效且校准良好。对 PDE 建模和分子生成的大量实验表明，BARNN 不仅实现了与现有方法相当或更优异的精度，而且在不确定性量化和建模长程依赖性方面表现出色。]]></description>
      <guid>https://arxiv.org/abs/2501.18665</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>列表排序 Transformer 中的结构开发</title>
      <link>https://arxiv.org/abs/2501.18666</link>
      <description><![CDATA[arXiv:2501.18666v1 公告类型：新
摘要：我们研究单层仅注意的 Transformer 在学习对数字列表进行排序时如何开发相关结构。在训练结束时，该模型将其注意力头组织成两种主要模式，我们称之为词汇拆分和复制抑制。这两种模式都比多个头处理重叠的数字范围更简单。有趣的是，无论我们是否使用权重衰减（一种常见的正则化技术，被认为可以推动简化）都会存在词汇拆分，这支持了神经网络自然喜欢更简单解决方案的论点。我们将复制抑制与 GPT-2 中的一种机制联系起来，并研究其在我们的模型中的功能作用。在对模型的发展分析的指导下，我们确定了训练数据中驱动模型最终获得解决方案的特征。这提供了一个具体的例子，说明训练数据如何塑造 Transformer 的内部组织，为未来的研究铺平了道路，这些研究可以帮助我们更好地理解 LLM 如何开发其内部结构。]]></description>
      <guid>https://arxiv.org/abs/2501.18666</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用可穿戴传感器数据进行帕金森震颤分类的机器学习策略</title>
      <link>https://arxiv.org/abs/2501.18671</link>
      <description><![CDATA[arXiv:2501.18671v1 公告类型：新
摘要：帕金森病 (PD) 是一种神经系统疾病，需要早期准确诊断才能有效治疗。机器学习 (ML) 已成为增强 PD 分类和诊断准确性的强大工具，尤其是通过利用可穿戴传感器数据。本调查全面回顾了当前用于分类帕金森病震颤的 ML 方法，评估了各种震颤数据采集方法、信号预处理技术和跨时间和频域的特征选择方法，重点介绍了震颤分类的实用方法。本调查探讨了现有研究中使用的 ML 模型，从支持向量机 (SVM) 和随机森林等传统方法到卷积神经网络 (CNN) 和长短期记忆网络 (LSTM) 等高级深度学习架构。我们评估了这些模型在分类与 PD 相关的震颤模式方面的有效性，并考虑了它们的优势和局限性。此外，我们讨论了当前研究中的挑战和差异，以及使用可穿戴传感器数据将 ML 应用于 PD 诊断的更广泛的挑战。我们还概述了未来的研究方向，以推进 ML 在 PD 诊断中的应用，为研究人员和从业者提供见解。]]></description>
      <guid>https://arxiv.org/abs/2501.18671</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>张量网络 Born 机的正则化二阶优化</title>
      <link>https://arxiv.org/abs/2501.18691</link>
      <description><![CDATA[arXiv:2501.18691v1 公告类型：新
摘要：张量网络生成机 (TNBM) 是受量子启发的生成模型，用于学习数据分布。使用张量网络收缩和优化技术，该模型可以学习目标分布的有效表示，能够通过紧凑的参数化捕获复杂的相关性。尽管前景光明，但 TNBM 的优化仍面临一些挑战。TNBM 的一个关键瓶颈是通常用于此问题的损失函数的对数性质。单张量对数优化问题无法通过分析解决，因此需要采用迭代方法，这会减慢收敛速度并增加陷入许多非最优局部最小值之一的风险。在本文中，我们提出了一种改进的二阶优化技术用于 TNBM 训练，这显着提高了收敛速度和优化模型的质量。我们的方法在归一化状态的流形上采用了改进的牛顿法，并结合了损失景观的正则化来缓解局部最小值问题。我们通过在离散和连续数据集上训练一维矩阵乘积状态 (MPS) 来证明我们方法的有效性，展示了其在稳定性、效率和泛化方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2501.18691</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>STAN：平滑过渡自回归网络</title>
      <link>https://arxiv.org/abs/2501.18699</link>
      <description><![CDATA[arXiv:2501.18699v1 公告类型：新
摘要：传统的平滑过渡自回归 (STAR) 模型提供了一种有效的方法来通过基于特定过渡变量的平滑状态变化来建模这些动态。在本文中，我们通过将 STAR 模型与多层神经网络架构进行类比，提出了一种新方法。我们提出的神经网络架构模仿了 STAR 框架，采用多层来模拟状态之间的平滑过渡并捕捉复杂的非线性关系。网络的隐藏层和激活函数被构造为复制 STAR 模型典型的逐渐切换行为，从而允许更灵活和可扩展的方式进行依赖于状态的建模。这项研究表明，神经网络可以为 STAR 模型提供强大的替代方案，并有可能提高经济和金融预测的预测准确性。]]></description>
      <guid>https://arxiv.org/abs/2501.18699</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>隐形痕迹：使用混合指纹识别 GenAI 应用程序中的底层 LLM</title>
      <link>https://arxiv.org/abs/2501.18712</link>
      <description><![CDATA[arXiv:2501.18712v1 公告类型：新
摘要：指纹识别是指通过分析人工智能系统（例如大型语言模型 (LLM)）的独特特征或模式来识别其底层机器学习 (ML) 模型的过程，就像人类的指纹一样。大型语言模型 (LLM) 的指纹识别对于确保人工智能集成应用程序的安全性和透明度至关重要。虽然现有方法主要依赖于访问与应用程序的直接交互来推断模型身份，但它们在涉及多智能体系统、频繁的模型更新和对模型内部的受限访问的现实场景中往往会失败。在本文中，我们介绍了一种新颖的指纹识别框架，旨在通过集成静态和动态指纹识别技术来应对这些挑战。我们的方法可以识别架构特征和行为特征，从而实现动态环境中准确而强大的 LLM 指纹识别。我们还重点介绍了传统指纹识别方法无效的新威胁场景，弥合了理论技术与实际应用之间的差距。为了验证我们的框架，我们提出了一个广泛的评估设置，模拟现实世界的条件，并证明我们的方法在识别和监控 Gen-AI 应用中的 LLM 方面的有效性。我们的结果突出了该框架对多样化和不断发展的部署环境的适应性。]]></description>
      <guid>https://arxiv.org/abs/2501.18712</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>chebgreen：从数据中学习和插入连续经验格林函数</title>
      <link>https://arxiv.org/abs/2501.18715</link>
      <description><![CDATA[arXiv:2501.18715v1 公告类型：新
摘要：在这项工作中，我们提出了一个独立于网格的数据驱动库 chebgreen，用于对具有相关控制参数且其控制偏微分方程未知的一维系统进行数学建模。所提出的方法以有理神经网络的形式学习相关但隐藏的边界值问题的经验格林函数，然后我们根据该函数在切比雪夫基础上构建双变量表示。我们通过在合适的库中插入左和右奇异函数来揭示看不见的控制参数值的格林​​函数，这些库表示为准矩阵流形上的点，而相关的奇异值则用拉格朗日多项式进行插值。]]></description>
      <guid>https://arxiv.org/abs/2501.18715</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估口语作为认知障碍自动筛查的生物标记物</title>
      <link>https://arxiv.org/abs/2501.18731</link>
      <description><![CDATA[arXiv:2501.18731v1 公告类型：新
摘要：及时准确地评估认知障碍是高危人群的主要未满足需求。言语和语言的改变可以作为阿尔茨海默病和相关痴呆症 (ADRD) 的早期预测指标，早于神经退行性疾病的临床症状出现。语音生物标记为自动筛查提供了一种可扩展且非侵入性的解决方案。然而，机器学习 (ML) 的临床适用性仍然受到通用性、可解释性和获取患者数据以训练临床适用预测模型方面的挑战的限制。使用 DementiaBank 录音（N=291，64% 为女性），我们评估了 ML 技术用于 ADRD 筛查和从口语预测严重程度。我们通过从老年人（N=22，59% 为女性）居住地收集的试点数据验证了模型的通用性。风险分层和语言特征重要性分析增强了预测的可解释性和临床效用。对于 ADRD 分类，应用于词汇特征的随机森林实现了 69.4% 的平均敏感度（95% 置信区间 (CI) = 66.4-72.5）和 83.3% (78.0-88.7) 的特异性。在现实世界的试点数据上，该模型实现了 70.0% (58.0-82.0) 的平均敏感度和 52.5% (39.3-65.7) 的特异性。对于使用简易精神状态检查 (MMSE) 分数进行严重程度预测，随机森林回归器实现了 3.7 (3.7-3.8) 的平均绝对 MMSE 误差，在试点数据上的表现为 3.3 (3.1-3.5)。与较高 ADRD 风险相关的语言特征包括代词和副词的使用增加、不流畅性增加、分析性思维减少、词汇多样性降低以及反映心理完成状态的词语减少。我们的可解释预测模型提供了一种新方法，可以将对话式人工智能融入家庭中以监测认知健康并分类高风险个体，从而实现更早的发现和干预。]]></description>
      <guid>https://arxiv.org/abs/2501.18731</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经图模式机</title>
      <link>https://arxiv.org/abs/2501.18739</link>
      <description><![CDATA[arXiv:2501.18739v1 公告类型：新
摘要：图形学习任务需要模型理解与下游任务相关的基本子结构模式，例如社交网络中的三元闭包和分子图中的苯环。由于图的非欧几里得性质，现有的图神经网络 (GNN) 依赖于消息传递来迭代地聚合来自局部邻域的信息。尽管消息传递在经验上取得了成功，但它难以识别基本子结构，例如三角形，从而限制了其表达能力。为了克服这一限制，我们提出了神经图模式机 (GPM)，这是一个旨在直接从图形模式中学习的框架。GPM 有效地提取和编码子结构，同时识别与下游任务最相关的子结构。我们还证明，与消息传递相比，GPM 提供了卓越的表达能力和改进的远程信息建模。对节点分类、链接预测、图分类和回归的经验评估表明，GPM 优于最先进的基线。进一步的分析表明，GPM 具有理想的分布式稳健性、可扩展性和可解释性。我们认为 GPM 是超越消息传递的一步。]]></description>
      <guid>https://arxiv.org/abs/2501.18739</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于增强小样本的合成数据生成</title>
      <link>https://arxiv.org/abs/2501.18741</link>
      <description><![CDATA[arXiv:2501.18741v1 公告类型：新
摘要：小型数据集在健康研究中很常见。然而，当训练数据集较小时，机器学习模型的泛化性能并不理想。为了解决这个问题，数据增强是一种解决方案。增强增加了样本量，被视为一种正则化形式，增加了小型数据集的多样性，从而使它们在看不见的数据上表现更好。我们发现增强可以提高以下数据集的预后性能：观察次数较少、基线 AUC 较小、基数分类变量较高、结果变量更平衡。没有特定的生成模型始终优于其他模型。我们开发了一个决策支持模型，可用于告知分析师增强是否有用。对于七个小型应用数据集，增强现有数据可使 AUC 增加 4.31%（AUC 从 0.71 增加到 0.75）和 ​​43.23%（AUC 从 0.51 增加到 0.73），平均相对改善 15.55%，表明增强对小型数据集的影响非同小可（p=0.0078）。增强 AUC 高于仅重采样 AUC（p=0.016）。增强数据集的多样性高于重采样数据集的多样性（p=0.046）。]]></description>
      <guid>https://arxiv.org/abs/2501.18741</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>二氧化碳羽流监测的概率联合恢复方法</title>
      <link>https://arxiv.org/abs/2501.18761</link>
      <description><![CDATA[arXiv:2501.18761v1 公告类型：新
摘要：减少二氧化碳排放对于缓解气候变化至关重要。碳捕获和储存 (CCS) 是少数能够实现二氧化碳净负排放的技术之一。然而，由于二氧化碳羽流动力学和储层特性的不确定性，预测 CCS 中的流体流动模式仍然具有挑战性。基于现有的地震成像方法（如缺乏不确定性量化的联合恢复方法 (JRM)），我们提出了概率联合恢复方法 (pJRM)。通过使用共享生成模型估计调查中的后验分布，pJRM 提供不确定性信息以改进 CCS 项目中的风险评估。]]></description>
      <guid>https://arxiv.org/abs/2501.18761</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>设计多样性：利用分布匹配进行基于离线模型的优化</title>
      <link>https://arxiv.org/abs/2501.18768</link>
      <description><![CDATA[arXiv:2501.18768v1 公告类型：新
摘要：离线基于模型的优化 (MBO) 的目标是提出新的设计，在仅给定离线数据集的情况下最大化奖励函数。然而，一个重要的需求是还要提出一组多样化的最终候选集，以捕获许多最佳和接近最佳的设计配置。我们提出了基于对抗模型的优化中的多样性 (DynAMO) 作为一种新方法，将设计多样性作为明确目标引入任何 MBO 问题。我们的主要见解是将多样性表述为分布匹配问题，其中生成的设计的分布捕获了离线数据集中包含的固有多样性。跨越多个科学领域的大量实验表明，DynAMO 可以与常见的优化方法一起使用，以显着提高所提设计的多样性，同时仍能发现高质量的候选者。]]></description>
      <guid>https://arxiv.org/abs/2501.18768</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过图形生成模型探索香味空间并预测气味</title>
      <link>https://arxiv.org/abs/2501.18777</link>
      <description><![CDATA[arXiv:2501.18777v1 公告类型：新
摘要：我们探索了一套生成建模技术，以有效地导航和探索复杂的气味景观和更广泛的化学空间。与传统方法不同，我们不仅生成分子，还预测气味可能性，ROC AUC 得分为 0.97，并分配可能的气味标签。我们使用机器学习技术将气味可能性与分子的物理化学特征相关联，并利用 SHAP（SHapley Additive exPlanations）来证明该功能的可解释性。整个过程涉及四个关键阶段：分子生成、严格的分子有效性消毒检查、香味可能性筛选和生成分子的气味预测。通过公开我们的代码和训练模型，我们旨在促进我们的研究在香味发现和嗅觉研究应用中的更广泛采用。]]></description>
      <guid>https://arxiv.org/abs/2501.18777</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用已知观测模型在平均奖励 POMDP 中实现 $\widetilde{\mathcal{O}}(\sqrt{T})$ 遗憾</title>
      <link>https://arxiv.org/abs/2501.18790</link>
      <description><![CDATA[arXiv:2501.18790v1 公告类型：新
摘要：我们使用未知的转换模型但已知的观察模型来解决平均奖励无限期 POMDP，这种设置以前曾以两种限制性方式解决：（i）依赖于次优随机策略的频率方法，选择每个动作的概率最小，以及（ii）采用最优策略类但需要对所用估计量的一致性做出强假设的贝叶斯方法。我们的工作通过为转换模型提供方便的估计保证并引入一种利用确定性信念策略的最佳类的乐观算法来消除这些限制。我们对现有的估计技术进行了修改，为每个估计的动作转换矩阵分别提供理论保证。与无法使用来自不同策略的样本的现有估计方法不同，我们提出了一种新颖而简单的估计量来克服这一障碍。这种新的数据高效技术与所提出的 \emph{Action-wise OAS-UCRL} 算法和更严格的理论分析相结合，使得第一种方法与最优策略相比具有 $\mathcal{O}(\sqrt{T \,\log T})$ 阶的遗憾保证，从而优于最先进的技术。最后，通过数值模拟验证了理论结果，表明我们的方法相对于基线方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2501.18790</guid>
      <pubDate>Mon, 03 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>