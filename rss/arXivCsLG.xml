<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 11 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>斐波那契网络：位置编码的简单替代方案</title>
      <link>https://arxiv.org/abs/2411.05052</link>
      <description><![CDATA[arXiv:2411.05052v1 公告类型：新
摘要：众所周知，基于坐标的多层感知器 (MLP) 难以重建训练数据的高频。解决此问题的一个常见方法是位置编码 (PE)，它已经变得非常流行。然而，PE 有缺点。它具有高频伪影并添加了另一个超超参数，就像批量标准化和 dropout 一样。我们认为在某些情况下 PE 是不必要的，更智能的网络架构构建和智能训练方法足以实现类似的结果。在本文中，我们表明，当给定半频和四分之一频率的输入时，非常简单的 MLP 可以很容易地输出频率。利用这一点，我们设计了一个分块的网络架构，其中每个块的输入是前两个块的输出以及原始输入。我们称之为{\it 斐波那契网络}。通过在信号的相应频率上训练每个块，我们表明斐波那契网络可以重建任意高频率。]]></description>
      <guid>https://arxiv.org/abs/2411.05052</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过语言模型对语言模型进行水印</title>
      <link>https://arxiv.org/abs/2411.05091</link>
      <description><![CDATA[arXiv:2411.05091v1 公告类型：新
摘要：本文提出了一种通过语言模型生成的提示为语言模型添加水印的新框架。所提出的方法利用多模型设置，结合提示语言模型来生成水印指令，标记语言模型来在生成的内容中嵌入水印，以及检测语言模型来验证这些水印的存在。实验使用 ChatGPT 和 Mistral 作为提示和标记语言模型进行，并使用预训练的分类器模型评估检测准确率。结果表明，所提出的框架在各种配置中均实现了高分类准确率，ChatGPT 的准确率为 95%，Mistral 的准确率为 88.79%。这些发现验证了所提出的水印策略在不同语言模型架构中的适应性。因此，所提出的框架有望应用于内容归属、版权保护和模型认证。]]></description>
      <guid>https://arxiv.org/abs/2411.05091</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>层次凝聚聚类中平均链接的凝聚性和可分离性</title>
      <link>https://arxiv.org/abs/2411.05097</link>
      <description><![CDATA[arXiv:2411.05097v1 公告类型：新
摘要：平均链接被广泛认为是构建层次聚集聚类的最流行和最有效的方法之一。现有的理论分析表明，对于 Dasgupta 成本函数的变体，该方法比其他流行的启发式方法（如单链接和完全链接）具有更好的近似值 [STOC 2016]。然而，这些分析并没有将平均链接与随机层次结构分开，而且它们对度量空间没有吸引力，因为每个层次聚类对于用于相异性度量的 Dasgupta 函数的变体都有 1/2 的近似值 [Moseley and Yang 2020]。在本文中，我们全面研究了平均链接在度量空间中的表现，研究了几个自然标准，这些标准可以捕捉可分离性和凝聚性，并且比 Dasgupta 的成本函数及其变体更具解释性。我们还展示了使用真实数据集的实验结果，这些结果与我们的理论分析相结合，表明当凝聚性和可分离性都是重要目标时，平均链接是比其他相关方法更好的选择。]]></description>
      <guid>https://arxiv.org/abs/2411.05097</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用图神经网络挖掘两个图的结构</title>
      <link>https://arxiv.org/abs/2411.05119</link>
      <description><![CDATA[arXiv:2411.05119v1 公告类型：新
摘要：图神经网络 (GNN) 已成为处理非结构化数据的有前途的解决方案，其表现优于传统的深度学习架构。然而，目前大多数 GNN 模型都是为使用单个图而设计的，这限制了它们在许多可能涉及多个图的现实场景中的适用性。为了解决这一限制，我们提出了一种基于图的新型深度学习架构来处理存在两组信号的任务，每组信号都定义在不同的图上。首先，我们考虑这样的设置：输入表示为一个图（输入图）顶部的信号，输出是在另一个图（输出图）上定义的图信号。对于这种设置，我们提出了一个三块架构，其中我们首先使用在输入图上运行的 GNN 处理输入数据，然后应用在潜在空间中运行的转换函数并将信号从输入映射到输出图，最后实现在输出图上运行的第二个 GNN。我们的目标不是为这三个模块中的每一个提出一个具体的定义，而是提供一种灵活的方法来解决涉及在两个图上定义的数据的任务。本文的第二部分讨论了自监督设置，其中的重点不是输出空间而是底层潜在空间，并且受到典型相关分析的启发，我们寻求可用于解决下游任务的数据信息表示。通过利用来自多个图的信息，所提出的架构可以捕获数据中不同实体之间更复杂的关系。我们使用合成和真实世界数据集在几个实验设置中对此进行了测试，并观察到所提出的架构比传统的深度学习架构效果更好，展示了利用两个图的信息的重要性。]]></description>
      <guid>https://arxiv.org/abs/2411.05119</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EPIC：通过迭代协作增强隐私</title>
      <link>https://arxiv.org/abs/2411.05167</link>
      <description><![CDATA[arXiv:2411.05167v1 公告类型：新
摘要：基因组学技术的进步导致病毒（例如 SARS-CoV-2）序列数据量不断增加，从而导致机器学习 (ML) 在生物信息学中的使用增加。传统的 ML 技术需要集中式数据收集和处理，这对现实的医疗保健场景提出了挑战。此外，将医疗数据汇集到集中式存储中以训练强大的深度学习 (DL) 模型时，还存在隐私、所有权和严格的监管问题。联邦学习 (FL) 方法通过设置中央聚合器服务器和共享的全局模型克服了这些问题。它还通过提取知识来促进数据隐私，同时保持实际数据的私密性。这项工作提出了一种通过迭代协作 (EPIC) 架构实现的尖端隐私增强。网络在本地和集中式服务器之间划分和分布。我们展示了 EPIC 方法来解决监督分类问题，以估计 SARS-CoV-2 基因组序列数据谱系，而无需明确传输原始序列数据。我们的目标是创建一个通用的去中心化优化框架，允许各种数据持有者共同合作并收敛到单个预测模型。研究结果表明，隐私保护策略可以成功地与聚合方法一起使用，而不会实质性地改变学习收敛程度。最后，我们重点介绍了基于 FL 的医疗保健应用方法中的一些潜在问题和研究前景。]]></description>
      <guid>https://arxiv.org/abs/2411.05167</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DWFL：通过动态加权平均增强联邦学习</title>
      <link>https://arxiv.org/abs/2411.05173</link>
      <description><![CDATA[arXiv:2411.05173v1 公告类型：新
摘要：联邦学习 (FL) 是一种分布式学习技术，它通过使用分布式大数据为机器学习模型提供分散式训练方法来维护数据隐私。这种有前途的联邦学习方法在生物信息学中也越来越受欢迎，生物医学数据的隐私非常重要，尤其是涉及患者数据时。尽管联邦学习在生物序列分析中成功实施，但仍需要严格考虑以提高准确性，以免损害数据隐私。此外，联邦学习的最佳整合，特别是在蛋白质序列分析中，尚未得到充分探索。我们提出了一种基于深度前馈神经网络的增强联邦学习方法，用于蛋白质序列分类，以克服这些挑战。我们的方法引入了新的增强功能来提高分类准确性。我们引入了动态加权联邦学习 (DWFL)，这是一种基于联邦学习的方法，其中局部模型权重根据其性能指标使用加权平均进行调整。通过为表现良好的模型分配更高的权重，我们旨在为联邦学习过程创建更有效的初始全局模型，从而提高准确性。我们使用现实世界的蛋白质序列数据集进行实验，以评估 DWFL 的有效性。使用我们提出的方法获得的结果表明模型准确性显著提高，使联邦学习成为协作机器学习任务的首选、更强大且隐私保护的方法。]]></description>
      <guid>https://arxiv.org/abs/2411.05173</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>逆向过渡学习：从演示中学习动态</title>
      <link>https://arxiv.org/abs/2411.05174</link>
      <description><![CDATA[arXiv:2411.05174v1 公告类型：新
摘要：我们考虑在离线基于模型的强化学习背景下从接近最优的专家轨迹估计转换动态 $T^*$ 的问题。我们开发了一种新颖的基于约束的方法，即逆向转换学习，将专家轨迹的有限覆盖范围视为 \emph{feature}：我们利用专家接近最优的事实来告知我们对 $T^*$ 的估计。我们将约束条件整合到贝叶斯方法中。在合成环境和真实医疗场景（如重症监护病房 (ICU) 低血压患者管理）中，我们不仅展示了决策方面的显着改善，而且我们的后验可以告知何时转移会成功。]]></description>
      <guid>https://arxiv.org/abs/2411.05174</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 中上下文学习的线性回归对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2411.05189</link>
      <description><![CDATA[arXiv:2411.05189v1 公告类型：新
摘要：Transformers 已在包括统计学习任务在内的各个领域展示了卓越的上下文学习能力。虽然先前的研究表明，Transformers 可以实现常见的学习算法，但这些学习算法的对抗鲁棒性仍未被探索。这项工作调查了 Transformers 中的上下文学习对 \textit{劫持攻击} 的脆弱性，重点关注线性回归任务的设置。劫持攻击是提示操纵攻击，其中对手的目标是操纵提示以迫使 Transformer 生成特定输出。我们首先证明，已知在上下文中实现梯度下降的单层线性 Transformers 是不稳健的，可以通过扰乱上下文训练集中的单个示例来操纵以输出任意预测。虽然我们的实验表明这些攻击在线性 Transformers 上成功了，但我们发现它们不会转移到具有 GPT-2 架构的更复杂的 Transformers 上。尽管如此，我们表明这些 Transformer 可以使用基于梯度的对抗性攻击进行劫持。然后，我们证明对抗性训练可以增强 Transformer 对劫持攻击的鲁棒性，即使只是在微调期间应用也是如此。此外，我们发现在某些情况下，针对较弱攻击模型的对抗性训练可以提高对较强攻击模型的鲁棒性。最后，我们研究了劫持攻击在不同规模和初始化种子的 Transformer 之间以及 Transformer 和普通最小二乘法 (OLS) 之间的可转移性。我们发现，虽然攻击可以在小规模 Transformer 之间有效转移，但在其他场景中（从小到大、从大到大以及 Transformer 和 OLS 之间）表现出较差的可转移性。]]></description>
      <guid>https://arxiv.org/abs/2411.05189</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Q-SFT：通过监督微调进行语言模型的 Q 学习</title>
      <link>https://arxiv.org/abs/2411.05193</link>
      <description><![CDATA[arXiv:2411.05193v1 公告类型：新
摘要：基于价值的强化学习 (RL) 原则上可以学习各种多轮问题的有效策略，从游戏到对话再到机器人控制，包括通过离线 RL 从静态先前收集的数据集中学习。然而，尽管广泛使用策略梯度方法来训练用于单轮任务（例如问答）的大型语言模型，但事实证明，在离线策略或离线环境中进行多轮 RL 的基于价值的方法很难扩展到大型语言模型的设置。这种设置需要有效地利用预训练、扩展到具有数十亿个参数的大型架构以及在大型数据集上进行训练，所有这些都代表了当前基于价值的 RL 方法面临的重大挑战。在这项工作中，我们提出了一种解决这些缺点的新型离线 RL 算法，将 Q 学习视为一种改进的监督微调 (SFT) 问题，其中标记的概率直接转换为 Q 值。通过这种方式，我们获得了一种算法，该算法可以平稳地从预训练期间最大化数据的似然值过渡到微调期间学习接近最优的 Q 函数。我们的算法具有强大的理论基础，其性能界限与最先进的 Q 学习方法相似，同时在实践中利用的目标与 SFT 非常相似。正因为如此，我们的方法可以充分利用语言模型预训练的优势，而无需在 RL 微调之前重新初始化任何权重，也无需初始化新的主管来预测值或优势。从经验上讲，我们在预训练的 LLM 和 VLM 上评估了我们的方法，评估了各种任务，包括自然语言对话以及机器人操作和图像导航。]]></description>
      <guid>https://arxiv.org/abs/2411.05193</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过强化学习实现事后再生的交互式对话代理</title>
      <link>https://arxiv.org/abs/2411.05194</link>
      <description><![CDATA[arXiv:2411.05194v1 公告类型：新
摘要：大型语言模型 (LLM) 的最新进展使对话代理能够生成高度自然和可信的文本。然而，当前的 LLM 语言生成侧重于通过单一有效的响应准确响应问题和请求。实际上，许多真实对话都是交互式的，这意味着代理的话语会影响他们的对话伙伴、获取信息或改变他们的观点。解释代理如何有效地引导对话是许多对话任务（从医疗保健到偏好引出）中的关键能力。现有的微调对话代理以完成此类任务的方法将依赖于整理一定数量的专家数据。然而，这样做通常需要了解对话伙伴的底层认知过程，这是人类和接受人类数据训练的 LLM 都无法可靠做到的技能。我们的主要见解是，虽然 LLM 可能不擅长先验地或在正在进行的对话中找出引导对话的有效策略，但他们可以在事后或事后看到对话伙伴如何回应后做到这一点。我们利用这一事实重写和增强现有的次优数据，并通过离线强化学习 (RL) 训练一个代理，该代理的表现优于提示和从未改变的人类演示中学习。我们将我们的方法应用于两个需要了解人类心理状态、智能交互和说服的领域：心理健康支持和募集慈善捐款。我们对真实人类进行的用户研究的结果表明，我们的方法远远优于现有的最先进的对话代理。]]></description>
      <guid>https://arxiv.org/abs/2411.05194</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>关于 CLIP 图像嵌入的错误一致性</title>
      <link>https://arxiv.org/abs/2411.05195</link>
      <description><![CDATA[arXiv:2411.05195v1 公告类型：新
摘要：最近的研究表明，视觉语言模型 (VLM) 在视觉推理方面的失败通常源于错误的协议——当 CLIP 图像编码器将语义上不同的图像模糊地编码为具有高余弦相似度的嵌入时。在本文中，我们表明错误的协议并不总是罪魁祸首，因为多模态大型语言模型 (MLLM) 仍然可以从中提取不同的信息。例如，在 What&#39;sUp 基准中区分左侧和右侧的对象时，左右对的 CLIP 图像嵌入具有平均余弦相似度 $&gt;0.99$，并且 CLIP 的表现是随机的；但使用相同 CLIP 图像编码器的 LLaVA-1.5-7B 实现了近 $100\%$ 的准确率。我们发现 CLIP 图像嵌入中可提取的信息可能因 CLIP 视觉语言对齐不足而变得模糊：对比目标学习到的匹配分数可能无法捕捉到所有不同的图像文本对应关系。我们还研究了 MMVP 基准，先前的研究表明 LLaVA-1.5 无法区分具有高余弦相似度的图像对。我们观察到通过替代解码算法更多地关注视觉输入可以带来性能提升。此外，如果模型可以将两幅图像作为输入以强调它们的细微差别，准确率会显著提高。这两个发现都表明 LLaVA-1.5 没有充分利用提取的视觉信息。总之，我们的研究结果表明，虽然改进图像编码器可以使 VLM 受益，但仍有空间通过应用更好的策略来提取和利用视觉信息来增强具有固定图像编码器的模型。]]></description>
      <guid>https://arxiv.org/abs/2411.05195</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>硬件和软件平台推理</title>
      <link>https://arxiv.org/abs/2411.05197</link>
      <description><![CDATA[arXiv:2411.05197v1 公告类型：新
摘要：由于前期硬件基础设施和能源成本高昂，购买大型语言模型 (LLM) 推理的访问权限而不是自行托管现在是一种常见的商业做法。但是，作为买家，没有机制来验证所宣传服务的真实性，包括服务硬件平台，例如它实际上是使用 NVIDIA H100 提供服务的。此外，有报告表明，模型提供商可能会提供与宣传略有不同的模型，通常是为了使它们在较便宜的硬件上运行。这样，客户为更昂贵的硬件上的能力模型访问支付了额外费用，但最终却由更便宜的硬件上（可能能力较差的）更便宜的模型提供服务。在本文中，我们介绍了 \textit{\textbf{硬件和软件平台推理 (HSPI)}}——一种仅基于其输入输出行为识别（黑盒）机器学习模型的底层 \GPU{} 架构和软件堆栈的方法。我们的方法利用各种 \GPU{} 架构和编译器的固有差异来区分不同的 \GPU{} 类型和软件堆栈。通过分析模型输出中的数值模式，我们提出了一个分类框架，能够准确识别用于模型推理的 \GPU{} 以及底层软件配置。我们的研究结果表明，从黑盒模型推断 \GPU{} 类型是可行的。我们针对在不同真实硬件上运行的模型评估了 HSPI，发现在白盒设置中，我们可以区分不同的 \GPU{}，准确率在 $83.9\%$ 到 $100\%$ 之间。即使在黑盒设置中，我们也能够获得比随机猜测准确率高出三倍的结果。]]></description>
      <guid>https://arxiv.org/abs/2411.05197</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机鞍点和变分不等式的私有算法：超越欧几里得几何</title>
      <link>https://arxiv.org/abs/2411.05198</link>
      <description><![CDATA[arXiv:2411.05198v1 公告类型：新
摘要：在这项工作中，我们在欧几里得和非欧几里得设置中对 $(\epsilon,\delta)$-差分隐私 (DP) 约束下的随机鞍点问题 (SSP) 和随机变分不等式 (SVI) 进行了系统研究。我们首先考虑 $\ell_p/\ell_q$ 设置中的 Lipschitz 凸凹 SSP，$p,q\in[1,2]$。在这里，我们在强 SP-gap 上获得了 $\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ 的界限，其中 $n$ 是样本数，$d$ 是维度。对于任何 $p,q\in[1,2]$，这个速率几乎是最优的。在没有额外假设（例如平滑度或线性要求）的情况下，先前的 DP 工作仅在 $p=q=2$ 时（即仅在欧几里得设置中）获得此速率。此外，现有算法仅在特定的 $p$ 和 $q$ 设置下以及在损失和可行集的某些假设下才有效，而我们为 $p,q\in[1,2]$ 时的 DP SSP 提供了一种通用算法。我们的结果是通过对递归正则化算法的新颖分析获得的。特别是，我们开发了用于分析泛化的新工具，这可能具有独立的兴趣。接下来，我们将注意力转向具有单调、有界和 Lipschitz 算子的 SVI，并考虑 $\ell_p$ 设置，$p\in[1,2]$。这里，我们提供了第一个分析，该分析获得了强 VI 间隙 $\tilde{O}\big(\frac{1}{\sqrt{n}} + \frac{\sqrt{d}}{n\epsilon}\big)$ 的界限。对于 $p-1=\Omega(1)$，由于存在下限，该速率接近最优。为了获得此结果，我们开发了递归正则化的修改版本。我们的分析基于我们为 SSP 开发的技术，并采用了其他新组件来处理将递归正则化框架适应 SVI 时出现的困难。]]></description>
      <guid>https://arxiv.org/abs/2411.05198</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用替代损失求解隐藏的单调变分不等式</title>
      <link>https://arxiv.org/abs/2411.05228</link>
      <description><![CDATA[arXiv:2411.05228v1 公告类型：新
摘要：深度学习已被证明可有效解决各种损失最小化问题。然而，许多感兴趣的应用，如最小化投影贝尔曼误差和最小-最大优化，不能建模为最小化标量损失函数，而是对应于解决变分不等式 (VI) 问题。这种设置上的差异已经引起了许多实际挑战，因为监督学习中基于梯度的简单方法在 VI 情况下往往会发散和循环。在这项工作中，我们提出了一种与深度学习兼容的基于原则的替代方法来解决 VI。我们表明，我们的基于替代的方法有三个主要优点：(1) 在实践中现实的假设下（当存在隐藏的单调结构、插值和对替代的充分优化时），它保证收敛，(2) 它提供了现有方法的统一视角，(3) 适用于现有的深度学习优化器，如 ADAM。通过实验，我们证明了基于代理的方法在最小-最大优化和最小化投影贝尔曼误差方面是有效的。此外，在深度强化学习案例中，我们提出了一种新的 TD(0) 变体，它具有更高的计算和样本效率。]]></description>
      <guid>https://arxiv.org/abs/2411.05228</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用线性马尔可夫决策过程进行表演强化学习</title>
      <link>https://arxiv.org/abs/2411.05234</link>
      <description><![CDATA[arXiv:2411.05234v1 公告类型：新
摘要：我们研究 \emph{执行强化学习} 的设置，其中部署的策略会影响奖励和底层马尔可夫决策过程的转变。先前的工作 ~\parencite{MTR23} 已经在表格设置下解决了这个问题，并建立了重复再训练的最后迭代收敛，迭代复杂度明确取决于状态数。在这项工作中，我们将结果推广到 \emph{线性马尔可夫决策过程}，这是大规模 MDP 的主要理论模型。线性 MDP 的主要挑战是正则化目标不再是强凸的，我们希望有一个与特征维度一起扩展的界限，而不是可以无限的状态。我们的第一个结果表明，反复优化正则化目标会收敛到 \emph{执行稳定的策略}。在没有强凸性的情况下，我们的分析利用了一种新的递归关系，该关系使用最佳对偶解的特定线性组合来证明收敛性。然后，我们处理有限样本设置，其中学习者可以访问从当前策略中提取的一组轨迹。我们考虑原始问题的重新参数化版本，并构建一个经验拉格朗日量，该量将从样本中进行优化。我们表明，在 \emph{有界覆盖} 条件下，反复求解此经验拉格朗日量的鞍点会收敛到性能稳定的解，并且还构建了一个有效求解经验拉格朗日量的原始对偶算法。最后，我们展示了包括多智能体系统在内的性能强化学习一般框架的几种应用。]]></description>
      <guid>https://arxiv.org/abs/2411.05234</guid>
      <pubDate>Mon, 11 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>