<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 28 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>让 LLM 成为更好的零样本推理器：面向结构的自主推理</title>
      <link>https://arxiv.org/abs/2410.19000</link>
      <description><![CDATA[arXiv:2410.19000v1 公告类型：新
摘要：使用大型语言模型 (LLM) 的零样本推理方法具有显着的优势，包括对新任务的广泛泛化和对人工示例的减少依赖。然而，当前的零样本方法在复杂任务中仍然存在局限性，例如回答需要多步推理的问题。在本文中，我们通过引入一种新颖的面向结构的分析方法来解决这一限制，以帮助 LLM 更好地理解问题并指导 LLM 的解决问题过程。我们首先演示现有的推理策略 Chain-of-Thought 和 ReAct 如何从我们的面向结构的分析中受益。除了实证研究之外，我们还利用概率图模型从理论上解释了为什么我们的面向结构的分析可以改进 LLM 推理过程。为了进一步提高复杂问答任务的可靠性，我们提出了一种多智能体推理系统，即面向结构的自主推理智能体 (SARA)，该系统可以通过改进技术更好地按照面向结构的分析执行推理过程，并配备外部知识检索功能以减少事实错误。大量实验验证了所提出的推理系统的有效性。令人惊讶的是，在某些情况下，该系统甚至超越了少样本方法。最后，该系统不仅提高了复杂任务的推理准确性，而且还表现出对破坏推理过程的潜在攻击的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2410.19000</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>偏见何去何从，我亦将追随：算法偏见缓解的综合系统评价</title>
      <link>https://arxiv.org/abs/2410.19003</link>
      <description><![CDATA[arXiv:2410.19003v1 公告类型：新
摘要：机器学习 (ML) 模型越来越多地用于人员评估和选拔（例如，简历筛选器、自动评分面试）。然而，整个社会都担心 ML 评估可能会存在偏见并延续或加剧不平等。尽管组织研究人员已经开始从传统的心理测量和法律角度研究 ML 评估，但需要从计算机科学、数据科学和组织研究文献中理解、澄清和整合公平操作化和算法偏见缓解方法。我们提出了一个开发 ML 评估和应用偏见缓解方法的四阶段模型，包括 1) 生成训练数据、2) 训练模型、3) 测试模型和 4) 部署模型。在介绍四阶段模型时，我们描述了每个阶段潜在的偏见和不公平来源。然后，我们系统地回顾了算法偏见的定义和操作化、美国和欧洲关于人员选拔的法律要求以及跨多个领域的算法偏见缓解研究，并将这些发现整合到我们的框架中。我们的回顾通过阐明算法偏见的可能机制，同时确定哪些偏见缓解方法是合法和有效的，为研究和实践提供了见解。这个综合框架还揭示了算法偏见缓解知识方面的差距，组织研究人员、计算机科学家和数据科学家之间的未来合作研究应该解决这些差距。我们为开发和部署机器学习评估提供了建议，并为未来对算法偏见和公平性的研究提供了建议。]]></description>
      <guid>https://arxiv.org/abs/2410.19003</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GAN 的对偶空间训练：通往高效、创新生成模型的途径</title>
      <link>https://arxiv.org/abs/2410.19009</link>
      <description><![CDATA[arXiv:2410.19009v1 公告类型：新
摘要：生成对抗网络 (GAN) 在生成建模方面取得了显著进步；然而，它们的训练通常是资源密集型的，需要大量的计算时间和数十万个时期。本文提出了一种新颖的优化方法，该方法通过使用可逆映射（特别是自动编码器）在初始数据的对偶空间内进行操作来改变训练过程。通过在对偶空间中的编码表示上训练 GAN，这些表示封装了数据的最显着特征，生成过程变得更加高效，并且可能揭示出超出人类识别范围的潜在模式。这种方法不仅提高了训练速度和资源利用率，而且还探讨了一个哲学问题：模型是否可以产生超越人类智能的见解，同时又受到人类生成的数据的限制。]]></description>
      <guid>https://arxiv.org/abs/2410.19009</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>异构随机森林</title>
      <link>https://arxiv.org/abs/2410.19022</link>
      <description><![CDATA[arXiv:2410.19022v1 公告类型：新
摘要：随机森林 (RF) 是分类问题中备受青睐的机器学习方法。RF 的有效性取决于两个关键因素：单个树的准确性和它们之间的多样性。在本研究中，我们介绍了一种称为异构 RF (HRF) 的新方法，旨在以有意义的方式增强树的多样性。这种多样化是通过在树构建过程中故意引入异质性来实现的。具体而言，在构建后续树的特征子空间时，用于在先前树的根节点附近进行分裂的特征被分配较低的权重。因此，先前树中的主要特征不太可能在下一次迭代中使用，从而导致节点处的分裂特征更加多样化。通过模拟研究，证实了 HRF 方法有效地减轻了集合中树的选择偏差，增加了集合的多样性，并在噪声特征较少的数据集上表现出优异的性能。为了评估 HRF 与其他广泛采用的集成方法的比较性能，我们对 52 个数据集进行了测试，包括真实世界数据和合成数据。在大多数数据集的准确率方面，HRF 始终优于其他集成方法。]]></description>
      <guid>https://arxiv.org/abs/2410.19022</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>金融时间序列预测中的金融援助大型语言模型</title>
      <link>https://arxiv.org/abs/2410.19025</link>
      <description><![CDATA[arXiv:2410.19025v1 公告类型：新
摘要：考虑到财务援助中财务时间序列预测的难度，目前的大部分研究都集中在利用大数据分析来预测金融服务。一种现代方法是利用“预测分析”，类似于预测金融趋势。然而，由于历史数据集有限和财务信息维数高，财务援助 (FA) 中的许多时间序列数据都带来了独特的挑战，这阻碍了开发有效的预测模型，以平衡准确性与高效的运行时间和内存使用。预先训练的基础模型用于解决这些具有挑战性的任务。我们使用最先进的时间序列模型，包括预先训练的 LLM（GPT-2 作为骨干）、Transformers 和线性模型，以证明它们能够超越传统方法，即使在最少（“少量”）或没有微调（“零次”）的情况下也是如此。我们的基准研究包括财务援助和其他七个时间序列任务，展示了将 LLM 用于稀缺财务数据集的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.19025</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>鹦鹉的混合：专家提高记忆力多于推理能力</title>
      <link>https://arxiv.org/abs/2410.19034</link>
      <description><![CDATA[arXiv:2410.19034v1 公告类型：新
摘要：混合专家 (MoE) 架构能够以最小的计算开销显著增加模型参数的总数。然而，目前尚不清楚 MoE 和标准密集变压器之间存在哪些性能权衡（如果有的话）。在本文中，我们表明，随着我们增加专家的数量（同时固定活动参数的数量），记忆性能会持续提高，而推理能力会饱和。我们首先分析 MoE 在推理方面的理论局限性。我们证明存在一些图问题，这些图问题无法由一定宽度的任何数量的专家解决；然而，同样的任务可以通过宽度稍大的密集模型轻松解决。另一方面，我们发现在内存密集型任务中，MoE 可以有效地利用少量的活动参数和大量专家来记忆数据。我们在合成图问题和内存密集型闭卷检索任务上通过实证验证了这些发现。最后，我们预训练了一系列 MoE 和密集转换器，并根据数学和自然语言中常用的基准对它们进行了评估。我们发现增加专家数量有助于解决知识密集型任务，但无法为推理任务带来同样的好处。]]></description>
      <guid>https://arxiv.org/abs/2410.19034</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>牛顿损失：利用曲率信息进行可微分算法学习</title>
      <link>https://arxiv.org/abs/2410.19055</link>
      <description><![CDATA[arXiv:2410.19055v1 公告类型：新
摘要：当使用自定义目标（例如排名损失和最短路径损失）训练神经网络时，一个常见的问题是它们本身是不可微的。一种流行的方法是不断放宽目标以提供梯度，从而实现学习。然而，这种可微分的放松通常是非凸的，并且可能表现出消失和爆炸的梯度，使得它们（已经孤立地）难以优化。在这里，损失函数是训练深度神经网络的瓶颈。我们提出了牛顿损失，这是一种通过利用经验 Fisher 和 Hessian 矩阵的二阶信息来提高现有难以优化损失性能的方法。我们不使用二阶技术训练神经网络，而是仅利用损失函数的二阶信息将其替换为牛顿损失，同时使用梯度下降训练网络。这使得我们的方法具有计算效率。我们将牛顿损失应用于八种可微分排序和最短路径算法，对于优化程度较低的可微分算法取得了显著的改进，对于优化程度较高的可微分算法也取得了持续的改进。]]></description>
      <guid>https://arxiv.org/abs/2410.19055</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于 VAE 的数据增强来提高机器学习预测准确度和减少不确定性的研究</title>
      <link>https://arxiv.org/abs/2410.19063</link>
      <description><![CDATA[arXiv:2410.19063v1 公告类型：新
摘要：超快计算机与大内存的融合、机器学习 (ML) 算法的快速发展以及大数据集的可用性使多个工程领域处于巨大进步的门槛上。然而，核工程的一个独特挑战是数据稀缺，因为核系统的实验通常比大多数其他学科更昂贵且耗时。解决数据稀缺问题的一种潜在方法是深度生成学习，它使用某些 ML 模型来学习现有数据的底层分布并生成类似于真实数据的合成样本。通过这种方式，可以显着扩展数据集以训练更准确的预测 ML 模型。在这项研究中，我们的目标是评估使用基于变分自动编码器 (VAE) 的深度生成模型进行数据增强的有效性。我们研究了数据增强是否会提高使用增强数据训练的深度神经网络 (DNN) 模型的预测准确性。此外，使用贝叶斯神经网络 (BNN) 和共形预测 (CP) 量化 DNN 预测不确定性，以评估对预测不确定性减少的影响。为了测试所提出的方法，我们使用基于 NUPEC 沸水反应堆全尺寸细网格束测试 (BFBT) 基准的稳态空隙率数据 TRACE 模拟。我们发现使用 VAE 扩充训练数据集提高了 DNN 模型的预测准确性，提高了预测置信区间，并降低了预测不确定性。]]></description>
      <guid>https://arxiv.org/abs/2410.19063</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>目标奇异度：一种新颖的共形预测难度估计器</title>
      <link>https://arxiv.org/abs/2410.19077</link>
      <description><![CDATA[arXiv:2410.19077v1 公告类型：新
摘要：本文介绍了目标奇异性，这是一种用于共形预测 (CP) 的新型难度估计器，它提供了一种标准化预测间隔 (PI) 的替代方法。通过评估预测在其最近邻居的目标分布范围内的异常程度，目标奇异性可以超越当前最先进的性能。在几个共形回归实验中，将这种新型难度估计器与其他估计器进行了评估。]]></description>
      <guid>https://arxiv.org/abs/2410.19077</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FastSurvival：训练 Cox 比例风险模型中隐藏的计算优势</title>
      <link>https://arxiv.org/abs/2410.19081</link>
      <description><![CDATA[arXiv:2410.19081v1 公告类型：新
摘要：生存分析是一个重要的研究课题，可应用于医疗保健、商业和制造业。该领域的一个重要工具是 Cox 比例风险 (CPH) 模型，该模型因其可解释性、灵活性和预测性能而被广泛使用。然而，对于现代数据科学挑战，例如高维性（$n$ 和 $p$）和高特征相关性，当前训练 CPH 模型的算法存在缺陷，阻止我们充分利用 CPH 模型。根本原因是基于牛顿法的当前算法在最小化器的局部区域之外由于二阶导数消失而难以收敛。为了解决这个问题，我们提出了新的优化方法，通过构建和最小化利用 CPH 模型隐藏数学结构的代理函数。我们的新方法易于实现，并确保单调损失减少和全局收敛。我们通过实证研究验证了我们方法的计算效率。作为直接应用，我们展示了如何使用我们的优化方法来解决基数受限的 CPH 问题，从而生成以前无法构建的非常稀疏的高质量模型。我们列出了我们的突破所带来的几个扩展，包括优化机会、CPH 数学结构的理论问题以及其他与 CPH 相关的应用。]]></description>
      <guid>https://arxiv.org/abs/2410.19081</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可证明的最小网络和典型网络的缓和过拟合</title>
      <link>https://arxiv.org/abs/2410.19092</link>
      <description><![CDATA[arXiv:2410.19092v1 公告类型：新
摘要：我们研究完全连接的深度神经网络 (NN) 的过度拟合行为，该网络的二元权重可以完美地对嘈杂的训练集进行分类。我们考虑使用最小 NN（具有最小权重数）和随机插值 NN 进行插值。对于这两种学习规则，我们证明过度拟合是缓和的。我们的分析基于与部分函数一致的阈值电路大小的新界限。据我们所知，我们是关于良性或缓和过度拟合的第一个理论结果：（1）适用于深度 NN，并且（2）不需要非常高或非常低的输入维度。]]></description>
      <guid>https://arxiv.org/abs/2410.19092</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TesseraQ：具有块重建的超低位 LLM 后训练量化</title>
      <link>https://arxiv.org/abs/2410.19103</link>
      <description><![CDATA[arXiv:2410.19103v1 公告类型：新
摘要：大型语言模型 (LLM) 彻底改变了自然语言处理，尽管代价是巨大的内存和计算需求。训练后量化 (PTQ) 正在成为减少内存占用和提高 LLM 推理吞吐量的事实上的方法。在这项工作中，我们旨在通过使用块重构技术（以前的视觉模型中的主要方法）优化权重舍入参数来突破 LLM PTQ 的上限。我们提出了一种新的最先进的 PTQ 技术 TesseraQ，将 LLM 的权重量化为超低位。为了有效优化 LLM 中的舍入并稳定重构过程，我们引入了渐进式自适应舍入。该方法在重构过程中迭代地将软舍入变量转换为硬变量。此外，我们优化了反量化尺度参数以充分利用块重构技术。我们证明 TesseraQ 可以与现有的基于缩放或裁剪的 PTQ 算法（如 AWQ 和 OmniQuant）无缝集成，从而显著提高其性能并建立新的领先地位。例如，与 AWQ 相比，TesseraQ 使用 LLaMA-2-7B 的 2 位权重量化将 wikitext2 困惑度从 14.65 提高到 6.82，并将平均下游准确度从 50.52 提高到 59.27。在包括 W2A16、W3A16、W3A3 和 W4A4 在内的一系列量化方案中，TesseraQ 始终表现出卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.19103</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Bio2Token：使用 Mamba 对任何生物分子结构进行全原子标记</title>
      <link>https://arxiv.org/abs/2410.19110</link>
      <description><![CDATA[arXiv:2410.19110v1 公告类型：新
摘要：对大型 3D 分子结构进行高保真度的高效编码和表示对于生物分子设计应用至关重要。尽管如此，许多表示学习方法仅限于对较小的系统进行建模或使用系统的粗粒度近似，例如以氨基酸残基的分辨率而不是单个原子的水平对蛋白质进行建模。为了解决这个问题，我们开发了量化自动编码器，可以学习完整蛋白质、RNA 和小分子结构的原子级标记，重建精度低于 1 埃左右。我们证明所采用的 Mamba 状态空间模型架构相对高效，只需要一小部分训练数据、参数和计算即可达到具有竞争力的精度，并且可以扩展到具有近 100,000 个原子的系统。bio2token 学习到的结构标记将来可以作为全原子语言模型的输入。]]></description>
      <guid>https://arxiv.org/abs/2410.19110</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LanFL：使用合成样本的大型语言模型的差异隐私联邦学习</title>
      <link>https://arxiv.org/abs/2410.19114</link>
      <description><![CDATA[arXiv:2410.19114v1 公告类型：新
摘要：联邦学习 (FL) 是一种协作的、隐私保护的机器学习框架，可让多个参与者训练单个全局模型。然而，最近出现的具有数百到数千亿个参数的强大大型语言模型 (LLM) 使得传统的 FL 方法在 LLM 中的应用变得不切实际，因为计算和通信成本很高。此外，LLM 的最终用户通常无法访问模型的完整架构和权重，这使得参与者无法直接微调这些模型。本文介绍了一种用于 LLM 的新型 FL 方案，名为 LanFL，它完全基于提示，并将底层 LLM 视为黑匣子。我们开发了一种差异隐私合成样本生成机制，以促进参与者之间的知识共享，以及一种能够从合成样本中学习的提示优化方案。我们大量的实验表明，LanFL 成功地促进了参与者之间的学习，同时在各种任务中保护了本地数据集的隐私。]]></description>
      <guid>https://arxiv.org/abs/2410.19114</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型跨云联邦训练关键技术研究</title>
      <link>https://arxiv.org/abs/2410.19130</link>
      <description><![CDATA[arXiv:2410.19130v1 Announce Type: new 
摘要：随着自然语言处理技术的快速发展，大型语言模型在各种应用场景中表现出色。然而，训练这些模型需要大量的计算资源和数据处理能力。跨云联邦训练为解决单一云平台的资源瓶颈问题提供了一种新方法，使多个云的计算资源能够协同完成大型模型的训练任务。本研究分析了跨云联邦训练的关键技术，包括数据划分与分发、通信优化、模型聚合算法、异构云平台兼容性等。此外，研究了跨云训练中的数据安全和隐私保护策略，特别是数据加密和差分隐私技术的应用。通过实验验证，提出的技术框架提高了训练效率，保证了数据安全，降低了训练成本，彰显了跨云联邦训练的广阔应用前景。]]></description>
      <guid>https://arxiv.org/abs/2410.19130</guid>
      <pubDate>Mon, 28 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>