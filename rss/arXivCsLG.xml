<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 18 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>5G NR 毫米波系统中的早期切换准备</title>
      <link>https://arxiv.org/abs/2411.09720</link>
      <description><![CDATA[arXiv:2411.09720v1 公告类型：新 
摘要：切换 (HO) 过程是蜂窝网络中最重要的功能之一，由服务小区和相邻小区的用户信道测量驱动。整个 HO 过程的成功率受到准备阶段的显著影响。由于具有大型天线阵列的大规模多输入多输出 (MIMO) 系统可以解析信道行为的更精细细节，我们研究如何将机器学习应用于第五代 (5G) 新无线电 (NR) 系统中波束测量的时间序列数据以改进 HO 过程。本文介绍了早期计划的切换准备方案，旨在增强 HO 过程的稳健性和效率，特别是在涉及高移动性和密集小型蜂窝部署的场景中。早期计划的切换准备侧重于通过利用机器学习技术来预测 HO 事件的最早触发点，从而优化 HO 准备阶段的时间。我们确定了 HO 准备的早期触发机制，并展示了它如何有效地减少 HO 执行所需的时间，从而降低信道质量下降。这些见解促成了一种新的 HO 准备方案，该方案在结合移动性的 MIMO 场景中提供了一种新颖、用户感知且主动的 HO 决策。]]></description>
      <guid>https://arxiv.org/abs/2411.09720</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过安全多样化模型策略搜索进行迭代批量强化学习</title>
      <link>https://arxiv.org/abs/2411.09722</link>
      <description><![CDATA[arXiv:2411.09722v1 公告类型：新
摘要：批量强化学习使策略学习在训练期间无需与环境直接交互，而仅依赖于先前收集的交互集。因此，这种方法非常适合高风险和成本密集型应用，例如工业控制。学习到的策略通常被限制为以与批处理中观察到的类似方式运行。在现实世界中，学习到的策略部署在工业系统中，不可避免地导致收集新数据，这些数据随后可以添加到现有记录中。因此，学习和部署的过程可以在系统的整个生命周期内多次发生。在这项工作中，我们建议利用应用离线强化学习的这种迭代性质来引导学习到的策略在部署期间实现高效和信息丰富的数据收集，从而不断改进学习到的策略，同时仍在收集的数据的支持范围内。我们提出了一种基于集成模型的策略搜索的迭代批量强化学习算法，并增强了安全性，更重要的是，增强了多样性标准。]]></description>
      <guid>https://arxiv.org/abs/2411.09722</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于数值模型误差近似和超分辨率的物理信息神经网络 (PINN)</title>
      <link>https://arxiv.org/abs/2411.09728</link>
      <description><![CDATA[arXiv:2411.09728v1 公告类型：新
摘要：有限元分析中不可避免地会出现数值建模误差。模型误差的存在本质上反映了模型的准确性和不确定性。迄今为止，很少有方法可以明确量化关注点（例如有限元节点）处的误差。随着机器学习 (ML) 的出现，最近解决了缺乏显式模型误差近似器的问题，机器学习闭合了数值模型特征/解决方案和显式模型误差近似之间的循环。在本文中，我们提出了物理信息神经网络 (PINN)，用于同时进行数值模型误差近似和超分辨率。为了测试我们的方法，使用有限元模拟在具有中心开口的二维弹性板上生成数值数据。离散化中使用四节点和八节点四边形元素分别表示降阶和高阶模型。我们发现，开发的 PINN 能够有效预测 x 和 y 位移场中的模型误差，预测值与地面实况之间的差异很小。我们的研究结果表明，集成基于物理的损失函数使神经网络 (NN) 能够超越纯数据驱动的近似模型误差方法。]]></description>
      <guid>https://arxiv.org/abs/2411.09728</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SureMap：单任务和多任务分解评估的同步均值估计</title>
      <link>https://arxiv.org/abs/2411.09730</link>
      <description><![CDATA[arXiv:2411.09730v1 公告类型：新
摘要：分解评估——评估机器学习模型在不同子群体上的表现——是评估人工智能系统的性能和群体公平性时的核心任务。一个关键的挑战是评估数据稀缺，而由属性（例如种族、性别、年龄）交叉而产生的子群体通常很小。如今，多个客户从模型开发人员那里采购相同的人工智能模型是很常见的，每个客户都单独面临分解评估的任务。这就产生了我们所说的多任务分解评估问题，其中多个客户试图在他们自己的数据设置（任务）中对给定模型进行分解评估。在这项工作中，我们开发了一种称为 SureMap 的分解评估方法，该方法对黑盒模型的多任务和单任务分解评估都具有很高的估计精度。 SureMap 的效率提升来自于 (1) 将问题转化为结构化同步高斯均值估计和 (2) 整合外部数据，例如来自 AI 系统创建者或其他客户的数据。我们的方法结合了使用精心选择的先验的最大后验 (MAP) 估计和通过 Stein 的无偏风险估计 (SURE) 进行的无交叉验证调整。我们在多个领域的分类评估任务上对 SureMap 进行了评估，发现其准确率比几个强大的竞争对手有显著提高。]]></description>
      <guid>https://arxiv.org/abs/2411.09730</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>引导还是推出？最佳自适应插值</title>
      <link>https://arxiv.org/abs/2411.09731</link>
      <description><![CDATA[arXiv:2411.09731v1 公告类型：新
摘要：引导和推出是强化学习 (RL) 中价值函数估计的两个基本原则。我们引入了一类新的贝尔曼算子，称为子图贝尔曼算子，它在引导和推出方法之间进行插值。我们的估计器是通过求解经验子图贝尔曼算子的不动点得出的，结合了基于引导的时间差 (TD) 估计器和基于推出的蒙特卡洛 (MC) 方法的优势。具体而言，我们的估计器的误差上限接近 TD 实现的最佳方差，附加项取决于状态空间选定子集的退出概率。同时，估计器表现出 MC 的有限样本自适应性，样本复杂度仅取决于该子集的占用率度量。我们用信息论下限补充上限，表明在合理的样本量下，附加项是不可避免的。总之，这些结果确立了子图贝尔曼估计量作为在策略评估中协调 TD 和 MC 方法的最佳自适应框架。]]></description>
      <guid>https://arxiv.org/abs/2411.09731</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用积分微分方程对 AdaGrad、RMSProp 和 Adam 进行建模</title>
      <link>https://arxiv.org/abs/2411.09734</link>
      <description><![CDATA[arXiv:2411.09734v1 公告类型：新
摘要：在本文中，我们通过将 AdaGrad、RMSProp 和 Adam 优化算法建模为一阶积分微分方程，提出了一种连续时间公式。我们对这些方程进行数值模拟，以证明它们作为原始算法的精确近似的有效性。我们的结果表明，连续时间模型的行为与离散实现之间存在很强的一致性，从而为自适应优化方法的理论理解提供了新的视角。]]></description>
      <guid>https://arxiv.org/abs/2411.09734</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用可区分渲染的对抗性攻击：一项调查</title>
      <link>https://arxiv.org/abs/2411.09749</link>
      <description><![CDATA[arXiv:2411.09749v1 公告类型：新
摘要：可微分渲染方法已成为一种有前途的手段，通过操纵可以欺骗深度神经网络 (DNN) 的 3D 对象和场景来生成照片般逼真且物理上合理的对抗性攻击。最近，可微分渲染功能已显著发展为多样化的库，例如 Mitsuba、PyTorch3D，以及神经辐射场和 3D 高斯溅射等方法，用于解决逆渲染问题，这些方法在概念上具有与攻击 DNN 常用的相似属性，例如反向传播和优化。然而，对抗性机器学习研究界尚未充分探索或理解此类生成攻击的能力。一些关键原因是研究人员通常有不同的攻击目标，例如错误分类或错误检测，并使用不同的任务通过操纵场景中的不同表示（例如对象的网格或纹理）来实现这些目标。本综述采用面向任务的统一框架，系统地总结了常见任务，例如操纵纹理、改变照明和修改 3D 网格以利用 DNN 中的漏洞。我们的框架可以轻松比较现有工作，揭示研究差距并聚焦这一快速发展领域中令人兴奋的未来研究方向。通过关注这些任务如何对各种 DNN 进行攻击，例如图像分类、面部识别、物体检测、光流和深度估计，我们的综述有助于研究人员和从业者更好地了解计算机视觉系统在面对可能威胁现实世界应用的照片级对抗攻击时的漏洞。]]></description>
      <guid>https://arxiv.org/abs/2411.09749</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人体分解建模：贝叶斯方法</title>
      <link>https://arxiv.org/abs/2411.09802</link>
      <description><![CDATA[arXiv:2411.09802v1 公告类型：新
摘要：环境和个体变量以复杂的方式影响人体腐烂的速度。这些影响使基于观察到的腐烂特征估计死亡间隔 (PMI) 变得复杂。在这项工作中，我们开发了一个基于 PMI 和各种环境和个体变量的腐烂人体遗骸的生成概率模型。该模型明确表示了每个变量（包括 PMI）对每个腐烂特征出现的影响，允许直接解释模型效应并允许使用该模型进行 PMI 推断和最佳实验设计。此外，该模型的概率性质允许以先验分布的形式整合专家知识。我们将该模型与来自 GeoFOR 数据集的 2,529 个不同案例进行拟合。我们证明该模型准确预测了 24 个腐烂特征，ROC AUC 得分为 0.85。使用贝叶斯推理技术，我们反转分解模型，根据观察到的分解特征以及环境和个体变量预测 PMI，得出 R 平方值为 71%。最后，我们演示了如何使用拟合模型设计未来的实验，以使用预期信息增益形式最大化有关分解机制的预期新信息量。]]></description>
      <guid>https://arxiv.org/abs/2411.09802</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>弱耦合马尔可夫决策过程中的公平资源分配</title>
      <link>https://arxiv.org/abs/2411.09804</link>
      <description><![CDATA[arXiv:2411.09804v1 公告类型：新
摘要：我们考虑在顺序决策环境中公平的资源分配，该环境被建模为弱耦合马尔可夫决策过程，其中资源约束将原本独立运行的 $N$ 个子马尔可夫决策过程 (sub-MDP) 的动作空间耦合起来。我们采用广义基尼函数而不是传统的功利主义 (总和) 目标来定义公平性。在引入基于线性规划的通用但计算量大的解决方案方案后，我们专注于所有子 MDP 都相同的同质情况。对于这种情况，我们首次表明问题归结为在“置换不变”策略类上优化功利主义目标。这个结果特别有用，因为我们可以在不安分的匪徒设置中利用 Whittle 指数策略，而对于更一般的设置，我们引入了一种基于计数比例的深度强化学习方法。最后，我们通过全面的实验验证了我们的理论发现，证实了我们提出的方法在实现公平方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2411.09804</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从拓扑角度评估损失状况</title>
      <link>https://arxiv.org/abs/2411.09807</link>
      <description><![CDATA[arXiv:2411.09807v1 公告类型：新
摘要：根据模型参数描述神经网络的损失，即损失景观，可以提供对该模型属性的宝贵见解。已经提出了各种可视化损失景观的方法，但较少强调量化和从这些复杂表示中提取可操作和可重复的见解。受到拓扑数据分析 (TDA) 用于总结高维数据结构的强大工具的启发，我们在这里描述了损失景观的底层形状（或拓扑），量化拓扑以揭示有关神经网络的新见解。为了将我们的发现与机器学习 (ML) 文献联系起来，我们计算简单的性能指标（例如准确度、误差），并使用基于 Hessian 的指标（例如最大特征值、迹、特征值谱密度）来描述损失景观的局部结构。按照这种方法，我们研究了图像模式识别（例如 ResNets）和科学 ML（例如物理信息神经网络）中已建立的模型，并展示了如何量化损失景观的形状可以为模型性能和学习动态提供新的见解。]]></description>
      <guid>https://arxiv.org/abs/2411.09807</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用张量分解和稀疏性学习参数共享</title>
      <link>https://arxiv.org/abs/2411.09816</link>
      <description><![CDATA[arXiv:2411.09816v1 公告类型：新
摘要：大型神经网络实现了卓越的性能，但其规模阻碍了在资源受限的设备上部署。虽然存在各种压缩技术，但参数共享仍然相对未被探索。本文介绍了细粒度参数共享 (FiPS)，这是一种利用参数共享、张量分解和稀疏性之间的关系来有效压缩大型视觉变换模型的新算法。FiPS 采用共享基和稀疏因​​子来表示跨多层感知 (MLP) 模块的共享神经元。共享参数化通过奇异值分解 (SVD) 初始化，并通过最小化块重建误差进行优化。实验表明，FiPS 将 DeiT-B 和 Swin-L MLP 压缩到其原始参数数量的 25-40%，同时保持原始模型 1 个百分点内的准确度。]]></description>
      <guid>https://arxiv.org/abs/2411.09816</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WelQrate：定义小分子药物发现基准测试的黄金标准</title>
      <link>https://arxiv.org/abs/2411.09820</link>
      <description><![CDATA[arXiv:2411.09820v1 公告类型：新 
摘要：虽然深度学习彻底改变了计算机辅助药物发现，但人工智能社区主要关注模型创新，而较少重视建立最佳基准测试实践。我们认为，如果没有完善的模型评估框架，人工智能社区的努力就无法充分发挥其潜力，从而减缓创新向现实世界药物发现的进展和转移。因此，在本文中，我们寻求建立小分子药物发现基准测试的新黄金标准 WelQrate。具体来说，我们的贡献有三方面：WelQrate 数据集集合——我们引入了精心策划的 9 个数据集集合，涵盖 5 个治疗目标类别。我们由药物发现专家设计的分层管理流程超越了主要的高通量筛选，利用额外的确认和计数器筛选以及严格的领域驱动预处理，例如泛测定干扰化合物 (PAINS) 过滤，以确保数据集中的高质量数据； WelQrate 评估框架 - 我们提出了一个标准化的模型评估框架，该框架考虑了高质量数据集、特征化、3D 构象生成、评估指标和数据分割，为进行真实世界虚拟筛选的药物发现专家提供了可靠的基准测试；基准测试 - 我们使用 WelQrate 数据集集合通过各种研究问题评估模型性能，探索不同模型、数据集质量、特征化方法和数据分割策略对结果的影响。总之，我们建议采用我们提出的 WelQrate 作为小分子药物发现基准测试的黄金标准。WelQrate 数据集集合以及策展代码和实验脚本均可在 WelQrate.org 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2411.09820</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>新生儿一般运动的自动分类</title>
      <link>https://arxiv.org/abs/2411.09821</link>
      <description><![CDATA[arXiv:2411.09821v1 公告类型：新
摘要：一般运动 (GM) 是婴儿自发的、协调的身体运动，可为发育中的神经系统提供宝贵的见解。通过 Prechtl GM 评估 (GMA) 进行评估，GM 是神经发育障碍的可靠预测指标。然而，GMA 需要经过专门培训的临床医生，而这些医生的数量有限。为了扩大新生儿筛查的规模，需要一种能够自动对婴儿视频记录中的 GM 进行分类的算法。这些数据带来了挑战，包括记录长度、设备类型和设置的多样性，每个视频都粗略地标注了整体运动质量。在这项工作中，我们介绍了一种从这些记录中提取特征的工具，并探索了用于自动 GM 分类的各种机器学习技术。]]></description>
      <guid>https://arxiv.org/abs/2411.09821</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>良好、高效和归纳偏差：通过使用归纳偏差探索深度学习的效率</title>
      <link>https://arxiv.org/abs/2411.09827</link>
      <description><![CDATA[arXiv:2411.09827v1 公告类型：新
摘要：深度学习的出现标志着机器学习的深刻转变，其推动力是近年来取得的众多突破。然而，随着深度学习在日常工具和应用中越来越普遍，越来越需要解决与其效率和可持续性相关的未解决的挑战。这篇论文深入探讨了归纳偏差（特别是连续建模和对称性保存）作为提高深度学习效率的策略的作用。它分为两个主要部分。
第一部分研究连续建模作为提高深度学习算法效率的工具。连续建模涉及在连续空间中参数化神经操作的想法。这里介绍的研究证明了（i）计算效率（时间和内存）、（ii）参数效率和（iii）设计效率（为新数据集和任务设计神经架构的复杂性）的实质性好处。
第二部分重点关注对称性保存对深度学习效率的作用。对称性保存涉及设计与数据固有对称性相一致的神经操作。本部分介绍的研究强调了通过使用对称性保存，数据和参数效率都得到了显著提高。然而，它也承认了由此带来的计算成本增加的权衡。
论文最后对这些发现进行了批判性评估，公开讨论了它们的局限性并提出了解决这些局限性的策略，这些策略是基于文献和作者的见解。最后，它确定了在探索归纳偏差以提高效率方面有希望的未来研究途径，以及它们对深度学习的更广泛影响。]]></description>
      <guid>https://arxiv.org/abs/2411.09827</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实时自适应路由 (RAR)：通过分层基础模型驱动的软件持续学习提高效率</title>
      <link>https://arxiv.org/abs/2411.09837</link>
      <description><![CDATA[arXiv:2411.09837v1 公告类型：新 
摘要：为了平衡基础模型（FM，例如大型语言模型 (LLM)）驱动的软件的质量和推理成本，人们通常选择训练一个路由模型，将请求路由到具有不同大小和功能的 FM。现有的路由模型依赖于从精心策划的数据中学习最佳路由决策，需要复杂的计算来更新，并且不考虑较弱 FM 的潜在演变。在本文中，我们提出了实时自适应路由 (RAR)，这是一种不断调整 FM 路由决策的方法，同时使用引导式上下文学习来增强较弱 FM 的功能。目标是减少对更强大、更昂贵的 FM 的依赖。我们在流行的 MMLU 基准的不同子集上评估了我们的方法。随着时间的推移，我们的方法将 50.2% 的请求路由到计算成本高昂的模型，同时保持了大约 90.5% 的一般响应质量。此外，与使用独立较弱 FM 的等效方法相比，由更强大模型生成的指南已显示出域内泛化能力并带来了更好的响应质量。]]></description>
      <guid>https://arxiv.org/abs/2411.09837</guid>
      <pubDate>Mon, 18 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>