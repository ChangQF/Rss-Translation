<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 31 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>RLeXplore：加速内在动机强化学习的研究</title>
      <link>https://arxiv.org/abs/2405.19548</link>
      <description><![CDATA[arXiv:2405.19548v1 公告类型：新
摘要：外部奖励可以有效地指导强化学习 (RL) 代理完成特定任务。然而，由于外部奖励的设计和注释需要大量的人力，因此在复杂的环境中，外部奖励经常会失效。这一限制强调了内在奖励的必要性，它提供辅助和密集信号，可以使代理以无监督的方式学习。虽然已经提出了各种内在奖励公式，但它们的实施和优化细节尚未得到充分探索，缺乏标准化，从而阻碍了研究进展。为了解决这一差距，我们引入了 RLeXplore，这是一个统一、高度模块化、即插即用的框架，提供八种最先进的内在奖励算法的可靠实现。此外，我们进行了深入研究，确定了关键的实施细节，并在内在动机的 RL 中建立了合理的标准实践。RLeXplore 的源代码可在 https://github.com/RLE-Foundation/RLeXplore 上找到。]]></description>
      <guid>https://arxiv.org/abs/2405.19548</guid>
      <pubDate>Fri, 31 May 2024 06:20:18 GMT</pubDate>
    </item>
    <item>
      <title>偏好学习算法不学习偏好排序</title>
      <link>https://arxiv.org/abs/2405.19534</link>
      <description><![CDATA[arXiv:2405.19534v1 公告类型：新
摘要：偏好学习算法（例如 RLHF 和 DPO）经常用于引导 LLM 产生更受人类青睐的代，但我们对其内部工作原理的理解仍然有限。在这项工作中，我们研究了偏好学习训练模型将更高的可能性分配给更受欢迎的输出而不是不太受欢迎的输出的传统观点，通过 $\textit{排名准确度}$ 来衡量。令人惊讶的是，我们发现大多数最先进的偏好调整模型在常见偏好数据集上的排名准确度不到 60%。此外，我们推导出如果偏好调整的 LLM 完美优化 DPO 或 RLHF 目标，它将实现的 $\textit{理想排名准确度}$。我们证明现有模型表现出显着的 $\textit{对齐差距}$——$\textit{即}$，观察到的和理想的排名准确度之间的差距。我们将这种差异归因于 DPO 目标，从经验和理论上讲，该目标不适合修复参考模型中哪怕是轻微的排名错误，并推导出一个简单而有效的公式来量化学习给定偏好数据点的难度。最后，我们证明，当模型接近目标中使用的参考模型时，排名准确度与经验上流行的胜率指标密切相关，这进一步阐明了在线策略（例如 RLHF）和离线策略（例如 DPO）偏好学习算法之间的差异。]]></description>
      <guid>https://arxiv.org/abs/2405.19534</guid>
      <pubDate>Fri, 31 May 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>用于多模态对比学习的 CLIPLoss 和基于范数的数据选择方法</title>
      <link>https://arxiv.org/abs/2405.19547</link>
      <description><![CDATA[arXiv:2405.19547v1 公告类型：新
摘要：数据选择已成为大规模视觉语言模型保留（例如 CLIP）的核心问题，尤其是在嘈杂的网络策划数据集中。三种主要的数据选择方法是：（1）利用外部非 CLIP 模型来帮助数据选择，（2）训练新的 CLIP 样式嵌入模型，这些模型在选择高质量数据方面比原始 OpenAI CLIP 模型更有效，以及（3）设计更好的指标或策略，普遍适用于任何 CLIP 嵌入，而无需特定的模型属性（例如，CLIPScore 是一种流行的指标）。虽然前两种方法已经得到广泛研究，但第三种方法仍未得到充分探索。在本文中，我们通过提出两种新方法来推进第三种方法。首先，我们引入了 negCLIPLoss，这是一种受 CLIP 损失启发的方法，它添加了一个样本与其对比对之间的对齐作为额外的规范化项，以实现更好的质量测量，而不是使用仅考虑单个样本中两种模态之间对齐的传统 CLIP 分数。其次，当下游任务已知时，我们提出了一种新的基于规范的度量 NormSim，以衡量预训练数据和目标数据之间的相似性。我们在数据选择基准 DataComp~\cite{gadre2023datacomp} 上测试了我们的方法。与仅使用 OpenAI 的 CLIP-L/14 的最佳基线相比，我们的方法在 ImageNet-1k 上实现了 5.3\% 的改进，在 38 个下游评估任务上实现了 2.8\% 的改进。此外，negCLIPLoss 和 NormSim 都与现有技术兼容。通过将我们的方法与当前最好的方法 DFN~\cite{fang2023data} 和 HYPE~\cite{kim2024hype} 相结合，我们可以将下游任务的平均性能提高 0.9\%，达到新的最佳水平。]]></description>
      <guid>https://arxiv.org/abs/2405.19547</guid>
      <pubDate>Fri, 31 May 2024 06:20:17 GMT</pubDate>
    </item>
    <item>
      <title>困难众包：异构项目的贝叶斯评级模型</title>
      <link>https://arxiv.org/abs/2405.19521</link>
      <description><![CDATA[arXiv:2405.19521v1 公告类型：新
摘要：在应用统计和机器学习中，用于训练的“黄金标准”往往存在偏差，而且几乎总是嘈杂的。Dawid 和 Skene 广受欢迎的众包模型调整了评分者（编码员、注释者）的敏感性和特异性，但未能捕捉为训练而收集的评分数据的分布特性，这反过来又使训练产生偏差。在本研究中，我们引入了一个通用的测量误差模型，通过添加难度、判别性和可猜测性的项目级效应，我们可以推断出共识类别。我们进一步展示了如何限制这些模型的双峰后验，以避免（或在必要时允许）对抗性评分者。我们用后验预测检查来验证我们模型的拟合优度，即 $\chi^2$ 检验的贝叶斯模拟。Dawid 和 Skene 的模型被拟合优度检验拒绝，而我们针对项目异质性进行调整的新模型并没有被拒绝。我们用两个经过充分研究的数据集来说明我们的新模型，即牙科 X 射线中龋齿的二进制评级数据和自然语言中的含义。]]></description>
      <guid>https://arxiv.org/abs/2405.19521</guid>
      <pubDate>Fri, 31 May 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>将多重表征与多重边缘匹配差距进行对比</title>
      <link>https://arxiv.org/abs/2405.19532</link>
      <description><![CDATA[arXiv:2405.19532v1 公告类型：新
摘要：学习可以通过多个 ($k\geq 3$) 视图或模态看到的复杂对象的有意义的表示是机器学习的核心任务。现有方法使用最初用于成对视图的损失，并将其扩展到 $k$ 个视图，方法是实例化 $\tfrac12k(k-1)$ 个损失对，或者通过使用减少的嵌入，遵循 \textit{one vs. average-of-rest} 策略。我们提出了多边际匹配间隙 (M3G)，这是一种借用多边际最优传输 (MM-OT) 理论的工具来同时合并所有 $k$ 个视图的损失。给定一批 $n$ 个点，每个点被视为一个 $k$ 元组视图，随后转换为 $k$ 个嵌入，我们的损失对比了匹配这些 $n$ 个真实 $k$ 元组的成本与 MM-OT 多匹配成本，后者寻求在这些 $n\times k$ 个向量中选择 $n$ 个最佳排列的 $k$ 元组。虽然 MM-OT 问题的指数复杂度 $O(n^k$) 似乎令人生畏，但我们在实验中表明，该问题的 Sinkhorn 算法的适当泛化可以扩展到例如 $k=3\sim 6$ 个视图，使用大小为 $64~\sim128$ 的小批量。我们的实验表明，对于自监督和多模态任务，与成对损失的多视图扩展相比，性能有所提高。]]></description>
      <guid>https://arxiv.org/abs/2405.19532</guid>
      <pubDate>Fri, 31 May 2024 06:20:16 GMT</pubDate>
    </item>
    <item>
      <title>具有任意延迟的时变网络中的分散优化</title>
      <link>https://arxiv.org/abs/2405.19513</link>
      <description><![CDATA[arXiv:2405.19513v1 公告类型：新
摘要：我们考虑受通信延迟影响的网络的分散优化问题。此类网络的示例包括协作机器学习、传感器网络和多智能体系统。为了模拟通信延迟，我们向网络中添加虚拟的非计算节点，从而产生有向图。这促使我们研究有向图上的分散优化解决方案。现有解决方案假设节点知道其出度，导致适用性有限。为了克服这一限制，我们引入了一种新的基于八卦的算法，称为 DT-GO，它不需要知道出度。该算法适用于一般有向网络，例如具有延迟或有限确认能力的网络。我们推导出凸和非凸目标的收敛速度，表明我们的算法实现了与集中随机梯度下降相同的复杂度阶数。换句话说，图拓扑和延迟的影响仅限于高阶项。此外，我们扩展了分析以适应随时间变化的网络拓扑。我们提供了数值模拟来支持我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2405.19513</guid>
      <pubDate>Fri, 31 May 2024 06:20:15 GMT</pubDate>
    </item>
    <item>
      <title>机器学习中的数据最小化原则</title>
      <link>https://arxiv.org/abs/2405.19471</link>
      <description><![CDATA[arXiv:2405.19471v1 公告类型：新
摘要：数据最小化原则旨在减少收集、处理或保留的数据量，以最大限度地降低滥用、未经授权的访问或数据泄露的可能性。数据最小化植根于隐私设计原则，已得到各种全球数据保护法规的认可。然而，由于缺乏严格的表述，其实际实施仍然是一个挑战。本文解决了这一差距，并根据法律定义介绍了一个数据最小化的优化框架。然后，它采用几种优化算法来执行数据最小化，并根据它们是否符合最小化目标以及对用户隐私的影响进行全面评估。我们的分析强调了数据最小化的隐私期望与实际隐私利益之间的不匹配，强调需要采取能够考虑现实世界隐私风险的多个方面的方法。]]></description>
      <guid>https://arxiv.org/abs/2405.19471</guid>
      <pubDate>Fri, 31 May 2024 06:20:14 GMT</pubDate>
    </item>
    <item>
      <title>胜利的势头：跨异构环境的协作联邦强化学习</title>
      <link>https://arxiv.org/abs/2405.19499</link>
      <description><![CDATA[arXiv:2405.19499v1 公告类型：新 
摘要：我们探索联合强化学习 (FRL) 问题，其中 $N$ 个代理协作学习共同策略而不共享其轨迹数据。到目前为止，现有的 FRL 工作主要集中在相同或“相似”环境中运行的代理。相比之下，我们的问题设置允许任意大程度的环境异质性。为了获得最大化所有可能完全不同的环境中的平均性能的最佳策略，我们提出了两种算法：FedSVRPG-M 和 FedHAPG-M。与现有结果相比，我们证明 FedSVRPG-M 和 FedHAPG-M 都利用动量机制，无论环境异质性的程度如何，都可以精确收敛到平均性能函数的驻点。此外，通过结合方差减少技术或 Hessian 近似的优势，这两种算法都实现了最先进的收敛结果，其特征是样本复杂度为 $\mathcal{O}\left(\epsilon^{-\frac{3}{2}}/N\right)$。值得注意的是，我们的算法在代理数量方面享有线性收敛加速，突出了代理之间协作在寻找共同策略方面的好处。]]></description>
      <guid>https://arxiv.org/abs/2405.19499</guid>
      <pubDate>Fri, 31 May 2024 06:20:14 GMT</pubDate>
    </item>
    <item>
      <title>基于聚类的域泛化验证分割</title>
      <link>https://arxiv.org/abs/2405.19461</link>
      <description><![CDATA[arXiv:2405.19461v1 公告类型：新
摘要：本文考虑了域转移下的模型选择问题。在这种情况下，提出训练集和验证集之间的高最大均值差异 (MMD) 会增加所选模型的通用性。提出了一种基于核 k 均值聚类的数据分割算法，该算法可以最大化此目标。该算法利用线性规划来控制分割的大小、标签和（可选）组分布，并保证收敛。对于域泛化 (DG) 和无监督域自适应 (UDA) 任务，该技术在一系列数据集和训练算法中始终优于替代分割策略。分析还表明，训练集和验证集之间的 MMD 与测试域准确度具有很强的等级相关性（\rho=0.63），进一步证实了该方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.19461</guid>
      <pubDate>Fri, 31 May 2024 06:20:13 GMT</pubDate>
    </item>
    <item>
      <title>通过自回归生成进行后验采样</title>
      <link>https://arxiv.org/abs/2405.19466</link>
      <description><![CDATA[arXiv:2405.19466v1 公告类型：新
摘要：现实世界的决策需要应对环境变化带来的数据缺失问题；智能代理必须理解不确定性并积极收集信息来解决它。我们提出了一个从海量历史数据中学习强盗算法的新框架，并在冷启动推荐问题中进行了演示。首先，我们使用历史数据对自回归模型进行预训练，以预测一系列重复的反馈/奖励（例如，对不同用户随时间推移展示的新闻文章的回应）。在学习做出准确预测的过程中，该模型会根据丰富的动作特征（例如，文章标题）隐式地学习知情先验，以及如何在收集更多奖励时（例如，推荐每篇文章时的点击次数）强化信念。在决策时，我们自回归地为每个动作采样（估算）一个想象的奖励序列，并选择具有最大平均估算奖励的动作。我们的方法远非启发式方法，而是汤普森抽样（具有先验学习）的实现，汤普森抽样是一种著名的主动探索算法。我们证明了我们的预训练损失直接控制在线决策性能，并在新闻推荐任务上展示了我们的框架，在该任务中，我们集成了预训练语言模型的端到端微调来处理新闻文章标题文本，以提高性能。]]></description>
      <guid>https://arxiv.org/abs/2405.19466</guid>
      <pubDate>Fri, 31 May 2024 06:20:13 GMT</pubDate>
    </item>
    <item>
      <title>广义光滑条件下多目标优化的收敛性</title>
      <link>https://arxiv.org/abs/2405.19440</link>
      <description><![CDATA[arXiv:2405.19440v1 公告类型：新
摘要：多目标优化（MOO）在多任务学习等各个领域受到越来越多的关注。最近的研究提供了一些有效的算法和理论分析，但它们受到标准$L$平滑或有界梯度假设的限制，这些假设通常不适用于神经网络，例如循环神经网络（RNN）和变压器。在本文中，我们研究了一类更通用和更现实的$\ell$平滑损失函数，其中$\ell$是梯度范数的一般非减函数。我们为$\ell$平滑MOO问题开发了两种新的单循环算法，广义平滑多目标梯度下降（GSMGrad）及其随机变体随机广义平滑多目标梯度下降（SGSMGrad），它们近似于最大化目标间最小改进的冲突避免（CA）方向。我们对这两种算法进行了全面的收敛分析，并表明它们在所有迭代中收敛到一个 $\epsilon$ 精确的 Pareto 驻点，并保证 $\epsilon$ 级平均 CA 距离（即更新方向和 CA 方向之间的差距），其中确定性和随机性设置分别需要总共 $\mathcal{O}(\epsilon^{-2})$ 和 $\mathcal{O}(\epsilon^{-4})$ 个样本。我们的算法还可以使用更多样本在每次迭代中保证更紧密的 $\epsilon$ 级 CA 距离。此外，我们提出了一种 GSMGrad 的实用变体，名为 GSMGrad-FA，仅使用恒定级别的时间和空间，同时实现与 GSMGrad 相同的性能保证。我们的实验验证了我们的理论并证明了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.19440</guid>
      <pubDate>Fri, 31 May 2024 06:20:12 GMT</pubDate>
    </item>
    <item>
      <title>深度探索：深度神经网络能否更好地泛化？</title>
      <link>https://arxiv.org/abs/2405.19454</link>
      <description><![CDATA[arXiv:2405.19454v1 公告类型：新
摘要：最近对 grokking 现象的研究阐明了神经网络训练动态及其泛化行为的复杂性。Grokking 是指网络在测试集上的泛化准确率急剧上升，这发生在延长的过拟合阶段之后很长时间，在此期间网络完美地拟合训练集。虽然现有研究主要集中在浅层网络（例如 2 层 MLP 和 1 层 Transformer）上，但我们探索了深度网络（例如 12 层 MLP）上的 grokking。我们通过经验复制了这一现象，发现深度神经网络比其较浅的同类网络更容易受到 grokking 的影响。同时，我们观察到一个有趣的多阶段泛化现象，当增加 MLP 模型的深度时，测试准确率会出现二次激增，这在浅层模型中很少见。我们进一步揭示了特征等级的下降与 grokking 过程中从过拟合到泛化阶段的阶段转变之间的令人信服的对应关系。此外，我们发现多阶段泛化现象通常与特征等级的双下降模式一致。这些观察结果表明，与权重范数相比，内部特征等级可以作为模型泛化行为的更有希望的指标。我们相信我们的工作是第一个深入研究深度神经网络并研究特征等级与泛化性能之间关系的工作。]]></description>
      <guid>https://arxiv.org/abs/2405.19454</guid>
      <pubDate>Fri, 31 May 2024 06:20:12 GMT</pubDate>
    </item>
    <item>
      <title>通过许可实现安全：构建快速安全的强化学习盾牌</title>
      <link>https://arxiv.org/abs/2405.19414</link>
      <description><![CDATA[arXiv:2405.19414v1 公告类型：新
摘要：为实际问题设计强化学习 (RL) 解决方案仍然是一项重大挑战。一个主要关注的领域是安全性。“屏蔽”是一种流行的技术，通过将用户定义的安全规范转化为安全代理行为来强制执行 RL 中的安全性。然而，这些方法要么遭受极端的学习延迟，要么需要大量人力来设计模型和问题中的安全域，要么需要预先计算。在本文中，我们提出了一种新的基于许可的框架来处理安全性和屏蔽构造。许可最初是为了消除不会导致最佳解决方案的（非许可）行为而设计的，以提高 RL 训练效率。本文表明，安全性可以自然地纳入该框架，即将许可扩展到包括安全性，从而我们可以同时实现安全性和提高效率。使用三个标准 RL 应用程序的实验评估表明了该方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.19414</guid>
      <pubDate>Fri, 31 May 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>使用对比学习和生成相似性来学习捕捉人类归纳偏差的空间</title>
      <link>https://arxiv.org/abs/2405.19420</link>
      <description><![CDATA[arXiv:2405.19420v1 公告类型：新
摘要：人类依靠强大的归纳偏差从少数例子中学习并从感官数据中抽象出有用的信息。在机器学习模型中灌输这种偏差已被证明可以提高它们在各种基准上的表现，包括少量学习、稳健性和对齐。然而，找到有效的训练程序来实现这一目标可能具有挑战性，因为心理丰富的训练数据（例如人类相似性判断）的扩展成本很高，而人类归纳偏差的贝叶斯模型对于复杂、现实的领域通常是难以处理的。在这里，我们通过引入贝叶斯生成相似性概念来解决这一挑战，即如果两个数据点很可能是从同一分布中采样的，则认为它们是相似的。这种度量可以应用于复杂的生成过程，包括概率程序。我们表明，即使生成相似性的确切形式难以处理，也可以使用生成相似性来定义对比学习目标，从而能够学习表达特定归纳偏差的空间嵌入。我们通过展示如何使用它来捕捉人类对几何形状的归纳偏见，并更好地区分由概率程序参数化的不同抽象绘图风格，证明了我们方法的实用性。]]></description>
      <guid>https://arxiv.org/abs/2405.19420</guid>
      <pubDate>Fri, 31 May 2024 06:20:11 GMT</pubDate>
    </item>
    <item>
      <title>PureEBM：通过基于能量的模型的中期运行动力学实现通用毒物净化</title>
      <link>https://arxiv.org/abs/2405.19376</link>
      <description><![CDATA[arXiv:2405.19376v1 公告类型：新
摘要：数据中毒攻击通过在训练期间注入对抗性示例导致目标分布测试数据的错误分类，对机器学习模型的完整性构成重大威胁。现有的最先进 (SoTA) 防御方法受到各种限制，例如泛化性能显著降低、特定攻击类型和分类器的特殊性以及训练期间的大量开销，使其不切实际或限制于实际应用。为了应对这一挑战，我们引入了一种通用数据净化方法，该方法通过应用通用随机预处理步骤 $\Psi_{T}(x)$ 来保护自然训练的分类器免受恶意白盒、灰盒和黑盒图像毒药的侵害，该步骤通过对以图像 $x$ 初始化的收敛能量基模型 (EBM) 进行迭代朗之万采样来实现。$\Psi_{T}(x)$ 的中期运行动态净化毒药信息，对分类器网络泛化的重要特征的影响最小。我们表明，EBM 的对比学习过程使它们即使在存在有毒的 EBM 训练数据的情况下也能保持通用净化器，并实现对领先的触发毒药 Narcissus 和无触发毒药 Gradient Matching 和 Bullseye Polytope 的 SoTA 防御。这项工作是 PureGen 中引入的更大框架的一个子集，更详细地关注 EBM 净化和毒药防御。]]></description>
      <guid>https://arxiv.org/abs/2405.19376</guid>
      <pubDate>Fri, 31 May 2024 06:20:10 GMT</pubDate>
    </item>
    </channel>
</rss>