<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 11 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>利用神经网络进行相似性学习</title>
      <link>https://arxiv.org/abs/2410.07214</link>
      <description><![CDATA[arXiv:2410.07214v1 公告类型：新
摘要：在这项工作中，我们引入了一种神经网络算法，旨在自动从数据中识别相似关系。通过揭示这些相似关系，我们的网络近似于将无量纲量与其无量纲变量和系数联系起来的底层物理定律。此外，我们开发了一个线性代数框架，并附带代码，以推导与这些相似关系相关的对称群。虽然我们的方法很通用，但我们通过流体力学中的例子来说明它的应用，包括光滑管道中的层流牛顿和非牛顿流，以及光滑和粗糙管道中的湍流。选择这些例子是为了突出该框架处理简单和复杂情况的能力，并进一步验证其从数据中发现底层物理定律的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.07214</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>记忆增强型 Transformer 可以实现线性一阶优化方法</title>
      <link>https://arxiv.org/abs/2410.07263</link>
      <description><![CDATA[arXiv:2410.07263v1 公告类型：新
摘要：我们表明，记忆增强的 Transformers (Memformers) 可以实现线性一阶优化方法，例如共轭梯度下降、动量方法，以及更一般地线性组合过去梯度的方法。在展示 Transformers 如何模拟预条件梯度下降的先前工作的基础上，我们提供了理论和经验证据，证明 Memformers 可以学习更高级的优化算法。具体来说，我们分析了 Memformers 中的内存寄存器如何存储合适的中间注意值，从而使它们能够实现共轭梯度等算法。我们的结果表明，Memformers 可以通过对随机线性回归任务进行训练来有效地学习这些方法，甚至可以学习优于共轭梯度的方法。这项工作扩展了我们对 Transformers 算法能力的认识，展示了它们如何学习复杂的优化方法。]]></description>
      <guid>https://arxiv.org/abs/2410.07263</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过 Catalyst 加速提高去中心化联邦学习的性能</title>
      <link>https://arxiv.org/abs/2410.07272</link>
      <description><![CDATA[arXiv:2410.07272v1 公告类型：新 
摘要：分散式联邦学习因其训练速度更快、隐私保护和减少通信开销而成为集中式架构的替代方案。在分散式通信中，集中式联邦学习中的服务器聚合阶段转移到客户端，这意味着客户端以对等的方式相互连接。然而，与集中式模式相比，分散式联邦学习中的数据异构性将导致聚合模型之间的差异较大，从而导致训练收敛速度慢、测试泛化性能差。为了解决这些问题，我们引入了 Catalyst Acceleration 并提出了一种加速分散式联邦学习算法 DFedCata。它由两个主要组件组成：Moreau 包络函数，主要解决数据异构性导致的客户端之间参数不一致问题，以及 Nesterov 的外推步骤，加速聚合阶段。理论上，我们证明了算法的优化误差界和泛化误差界，从而进一步理解算法的本质和超参数选择的理论观点。从经验上，我们证明了所提算法在 CIFAR10/100 上对各种非独立同分布数据在收敛速度和泛化性能方面的优势。此外，我们还通过实验验证了 DFedCata 的理论性质。]]></description>
      <guid>https://arxiv.org/abs/2410.07272</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种用于分析点击流序列的效用挖掘驱动的主动学习方法</title>
      <link>https://arxiv.org/abs/2410.07282</link>
      <description><![CDATA[arXiv:2410.07282v1 公告类型：新
摘要：在快速发展的电子商务行业中，选择高质量数据进行模型训练的能力至关重要。本研究介绍了使用 SHAP 值的高效用序列模式挖掘 (HUSPM-SHAP) 模型，这是一种基于效用挖掘的主动学习策略，旨在应对这一挑战。我们发现正负 SHAP 值的参数设置会影响模型的挖掘结果，从而为主动学习框架引入了一个关键考虑因素。通过大量旨在预测是否会导致购买的行为的实验，设计的 HUSPM-SHAP 模型展示了其在不同场景中的优势。该模型在保持高预测性能的同时减轻标签需求的能力得到了强调。我们的研究结果表明，该模型能够改进电子商务数据处理，从而实现更精简、更具成本效益的预测建模。]]></description>
      <guid>https://arxiv.org/abs/2410.07282</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>个性化联邦学习的基准数据异质性评估方法</title>
      <link>https://arxiv.org/abs/2410.07286</link>
      <description><![CDATA[arXiv:2410.07286v1 公告类型：新
摘要：越来越多的研究开始关注测量客户端本地数据集的统计异质性。此类测量用于估计个性化联邦学习 (PFL) 模型的协作训练的适用性。目前，这些研究工作都是在孤岛中进行的，缺乏统一的基准来公平、方便地比较常见环境中的各种方法。我们旨在在本文中弥合这一重要差距。提出的基准测试框架目前包括六种代表性方法。已经进行了广泛的实验来比较五种标准非 IID FL 设置下的这些方法，从而提供了哪些方法在哪些设置下具有优势的急需见解。提出的框架为 FL 系统中各种数据发散度量的适用性提供了有用的指导。它有利于在以下方面保持相关研究活动的正确轨道：(1) 设计 PFL 方案，(2) 为特定的 FL 应用场景选择合适的数据异质性评估方法，以及 (3) 解决协作模型训练中的公平性问题。代码可以在https://github.com/Xiaoni-61/DH-Benchmark上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.07286</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>主正交潜在成分分析（POLCA Net）</title>
      <link>https://arxiv.org/abs/2410.07289</link>
      <description><![CDATA[arXiv:2410.07289v1 公告类型：新
摘要：表征学习是机器学习领域的关键领域，专注于开发从原始数据中自动发现给定任务所需的表征或特征的方法。与需要手工制作特征的传统特征工程不同，表征学习旨在学习对分类、预测和聚类等任务更有用和更相关的特征。我们引入了主正交潜在成分分析网络 (POLCA Net)，这是一种模拟和扩展 PCA 和 LDA 功能到非线性域的方法。POLCA Net 将自动编码器框架与一组专门的损失函数相结合，以实现有效的降维、正交性、基于方差的特征排序、高保真重建，此外，当与分类标签一起使用时，潜在表示非常适合线性分类器和类分布的低维可视化。]]></description>
      <guid>https://arxiv.org/abs/2410.07289</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向跨领域的通用时间序列理解</title>
      <link>https://arxiv.org/abs/2410.07299</link>
      <description><![CDATA[arXiv:2410.07299v1 公告类型：新
摘要：在自然语言处理和计算机视觉中，对大型数据集进行自监督预训练可解锁跨领域和任务的基础模型功能。然而，这种潜力尚未在时间序列分析中实现，现有方法忽略了时间序列特征的异构性。时间序列在许多领域都很普遍，包括医学、工程、自然科学和金融，但它们的特征在变量计数、变量间关系、时间动态和采样频率方面差异很大。这种跨领域固有的异质性阻碍了对大型时间序列语料库进行有效的预训练。为了解决这个问题，我们引入了 OTiS，这是一个用于通用时间序列分析的开放模型，专门用于处理多领域异质性。我们提出了一种新颖的预训练范式，其中包括具有可学习领域特定签名的标记器、用于捕获时间因果关系的双掩蔽策略以及用于建模长距离依赖关系的正则化互相关损失。我们的模型在包含 640,187 个样本和 110 亿个时间点的大型语料库上进行了预训练，这些样本和时间点涵盖 8 个不同的领域，使其能够分析任何（未见）领域的时间序列。在 15 种不同应用（包括分类、回归和预测）的综合实验中，OTiS 展示了其准确捕获领域特定数据特征的能力，并展示了其与最先进基线的竞争力。我们的代码和预训练权重可在 https://github.com/oetu/otis 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2410.07299</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MoE++：利用零计算专家加速混合专家方法</title>
      <link>https://arxiv.org/abs/2410.07348</link>
      <description><![CDATA[arXiv:2410.07348v1 公告类型：新
摘要：在这项工作中，我们旨在同时提高混合专家 (MoE) 方法的有效性和效率。为了实现这一目标，我们提出了 MoE++，这是一个通用的异构 MoE 框架，它集成了前馈网络 (FFN) 和零计算专家。具体来说，我们引入了三种类型的零计算专家：零专家、复制专家和常量专家，分别对应丢弃、跳过和替换操作。这种设计有三个关键优势：(i) 低计算开销：与 vanilla MoE 中所有 token 的统一混合机制不同，MoE++ 允许每个 token 与动态数量的 FFN 交互，通过常量向量进行调整，甚至完全跳过 MoE 层。(ii) 高性能：通过使简单 token 利用更少的 FFN 专家，MoE++ 允许更多专家专注于具有挑战性的 token，从而释放比 vanilla MoE 更大的性能潜力。 (iii) 易于部署：鉴于零计算专家的参数可以忽略不计，我们可以在每个 GPU 上部署所有零计算专家，从而消除了分布在不同 GPU 上的 FFN 专家所带来的大量通信开销和专家负载不平衡。此外，我们利用门控残差，使每个 token 在选择合适的专家时都能考虑前一层所采用的路径。大量实验结果表明，与相同大小的 vanilla MoE 模型相比，MoE++ 实现了更好的性能，同时提供了 1.1-2.1 倍的专家前向吞吐量，这为开发先进而高效的 MoE 相关模型奠定了坚实的基础。]]></description>
      <guid>https://arxiv.org/abs/2410.07348</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在神经空间交互模型中生成起点-终点矩阵</title>
      <link>https://arxiv.org/abs/2410.07352</link>
      <description><![CDATA[arXiv:2410.07352v1 公告类型：新
摘要：基于代理的模型 (ABM) 正在作为决策工具在交通、经济和流行病学等政策领域中不断涌现。在这些模型中，一个核心关注对象是离散的起点-目的地矩阵，它捕获位置之间的空间交互和代理行程计数。现有方法依赖于对该矩阵的连续近似和随后的临时离散化，以执行 ABM 模拟和校准。这阻碍了对部分观察到的汇总统计数据的条件化，无法探索离散组合支持上的多模态矩阵分布，并导致离散化误差。为了应对这些挑战，我们引入了一个计算效率高的框架，该框架与起点-目的地对的数量成线性比例，直接在离散组合空间上运行，并通过嵌入空间相互作用的神经微分方程学习代理的行程强度。我们的方法在重建误差和地面实况矩阵覆盖方面优于现有技术，而计算成本仅为其一小部分。我们在英国剑桥和美国华盛顿特区的大型空间移动 ABM 中展示了这些优势。]]></description>
      <guid>https://arxiv.org/abs/2410.07352</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM 嵌入改进了对表格 $Y|X$ 移位的测试时间适应性</title>
      <link>https://arxiv.org/abs/2410.07395</link>
      <description><![CDATA[arXiv:2410.07395v1 公告类型：新
摘要：对于表格数据集，由于缺少变量（又称混杂因素），标签和协变量（$Y|X$-shifts）之间的关系发生变化很常见。由于不可能推广到完全新的未知领域，我们研究即使只有少量标记示例也易于适应目标领域的模型。我们专注于构建可以减轻 $Y|X$-shifts 的表格数据更具信息量的表示，并建议通过序列化（写下来）表格数据对其进行编码来利用 LLM 中的先验世界知识。我们发现 LLM 嵌入本身在鲁棒性方面提供了不一致的改进，但即使使用 32 个标记观察值，在其上训练的模型也可以很好地适应/微调到目标域。我们的发现基于一项全面而系统的研究，该研究包括 7650 个源目标对，并以 22 种算法训练的 261,000 个模型配置为基准。我们的观察在缩小可访问目标数据的大小和采用不同的适应策略时成立。代码可在 https://github.com/namkoong-lab/LLM-Tabular-Shifts 获得。]]></description>
      <guid>https://arxiv.org/abs/2410.07395</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>将人工智能驱动的发现与人类直觉相结合</title>
      <link>https://arxiv.org/abs/2410.07397</link>
      <description><![CDATA[arXiv:2410.07397v1 公告类型：新
摘要：随着数据驱动的物理动力系统建模变得越来越普遍，一个新的挑战正在出现：使这些模型与现有的人类知识更加兼容和一致。人工智能驱动的科学建模过程通常从识别隐藏的状态变量开始，然后推导控制方程，然后预测和分析未来行为。识别一组合适的状态变量的关键初始步骤仍然具有挑战性，原因有二。首先，找到一组紧凑的有意义的预测变量在数学上很困难，而且定义不明确。第二个原因是发现的变量通常缺乏物理意义，因此人类科学家很难解释。我们提出了一个新的一般原则，用于提炼自然更符合人类直觉的表示，而不依赖于先前的物理知识。我们在许多实验和模拟系统上展示了我们的方法，其中人工智能生成的变量与人类科学家独立选择的变量非常相似。我们认为这一原则可以帮助人机协作更加富有成效，并揭示人类如何做出科学建模选择。]]></description>
      <guid>https://arxiv.org/abs/2410.07397</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CAFEEN：一种利用多智能体强化学习实现节能 NoC 的合作方法</title>
      <link>https://arxiv.org/abs/2410.07426</link>
      <description><![CDATA[arXiv:2410.07426v1 公告类型：新
摘要：在新兴的高性能片上网络 (NoC) 架构中，高效的电源管理对于最大限度地降低能耗至关重要。我们提出了一个名为 CAFEEN 的新框架，该框架采用基于启发式的细粒度和基于机器学习的粗粒度电源门控来实现节能的 NoC。CAFEEN 使用细粒度方法在较低的网络负载期间仅激活必要的 NoC 缓冲区。它在峰值负载下切换到粗粒度方法，以使用多智能体强化学习最大限度地减少复合唤醒开销。结果表明，与最先进的 NoC 电源门控框架相比，CAFEEN 自适应地平衡了电源效率和性能，将单个应用程序工作负载的总能耗降低了 2.60 倍，将多应用程序工作负载的总能耗降低了 4.37 倍。]]></description>
      <guid>https://arxiv.org/abs/2410.07426</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>隐式网络家族的泛化界限</title>
      <link>https://arxiv.org/abs/2410.07427</link>
      <description><![CDATA[arXiv:2410.07427v1 公告类型：新
摘要：隐式网络是一类神经网络，其输出由参数化运算符的不动点定义。它们在许多应用中都取得了成功，包括自然语言处理、图像处理和许多其他应用。虽然它们已经取得了大量经验上的成功，但其泛化的理论研究仍未得到充分探索。在这项工作中，我们考虑了一大类隐式网络定义的参数化收缩不动点运算符。我们根据这些架构的 Rademacher 复杂度的覆盖数参数展示了此类的泛化界限。]]></description>
      <guid>https://arxiv.org/abs/2410.07427</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EventFlow：使用流匹配预测连续时间事件数据</title>
      <link>https://arxiv.org/abs/2410.07430</link>
      <description><![CDATA[arXiv:2410.07430v1 公告类型：新
摘要：连续时间事件序列（其中事件以不规则的间隔发生）在广泛的工业和科学领域中无处不在。当代建模范式是将此类数据视为时间点过程的实现，在机器学习中，通常使用神经网络以自回归方式对时间点过程进行建模。虽然自回归模型可以成功预测单个后续事件的时间，但由于级联误差，它们在预测更长的时间范围内的性能可能不令人满意。我们提出了 EventFlow，这是一种用于时间点过程的非自回归生成模型。我们的模型建立在流匹配框架之上，以便直接学习事件时间的联合分布，从而避开自回归过程。 EventFlow 是无似然的，易于实施和采样，并且在一系列标准基准的无条件和条件生成任务中达到或超越了最先进模型的性能]]></description>
      <guid>https://arxiv.org/abs/2410.07430</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 能逻辑推理吗？SAT 解答中的一项研究</title>
      <link>https://arxiv.org/abs/2410.07432</link>
      <description><![CDATA[arXiv:2410.07432v1 公告类型：新
摘要：我们在布尔可满足性 (SAT) 问题的背景下从理论和实证上研究了 LLM 的逻辑推理能力。首先，我们构建了一个仅解码器的 Transformer，它可以通过思想链 (CoT) 使用回溯和推理来解决 SAT。我们通过展示与众所周知的 DPLL SAT 解决算法的轨迹等价性来证明其正确性。其次，为了支持这种抽象构造的实现，我们设计了一个编译器 $\texttt{PARAT}$，它将程序规范作为输入并输出实现此规范的转换器模型。第三，我们不是 $\textit{编程}$ 转换器进行推理，而是通过直接从 DPLL 算法的算法轨迹（“推理路径”）中学习来实证评估它是否能够 $\textit{训练}$ 进行推理。]]></description>
      <guid>https://arxiv.org/abs/2410.07432</guid>
      <pubDate>Fri, 11 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>