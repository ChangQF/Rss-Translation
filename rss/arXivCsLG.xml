<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Tue, 26 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>数据净化的群体福利实例选择</title>
      <link>https://arxiv.org/abs/2403.15694</link>
      <description><![CDATA[arXiv:2403.15694v1 公告类型：新
摘要：手动注释数据集以训练深度模型非常耗费人力和时间。为了克服这种劣势，直接利用网络图像来进行训练数据成为自然的选择。然而，网络数据中标签噪声的存在通常会降低模型性能。现有的对抗标签噪声的方法通常是在合成噪声数据集上设计和测试的。然而，他们往往无法在现实世界的噪声数据集上取得令人满意的结果。为此，我们提出了一种名为 GRIP 的方法来缓解合成数据集和真实数据集的噪声标签问题。具体来说，GRIP 利用组正则化策略来估计类软标签，以提高噪声鲁棒性。软标签监督减少了对噪声标签的过度拟合，并学习类间相似性以有利于分类。此外，实例净化操作通过测量每个训练样本与其类软标签之间的差异来全局识别噪声标签。通过组和实例级别的操作，我们的方法集成了噪声鲁棒和噪声清除方法的优点，并显着减轻了噪声标签引起的性能下降。对合成数据集和真实数据集的综合实验结果证明了 GRIP 相对于现有最先进方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2403.15694</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:41 GMT</pubDate>
    </item>
    <item>
      <title>通过核化斯坦因差异进行以数据为中心的预测解释</title>
      <link>https://arxiv.org/abs/2403.15576</link>
      <description><![CDATA[arXiv:2403.15576v1 公告类型：新
摘要：现有的基于示例的预测解释方法通常通过模型的参数或潜在表示来桥接测试和训练数据点。虽然这些方法为模型预测的原因提供了线索，但它们通常表现出先天的缺点，例如产生大量的计算开销或产生粗粒度的解释。本文提出了一种高精度且以数据为中心的解释（HD-Explain），这是一种利用核斯坦因差异（KSD）特性的简单预测解释方法。具体来说，KSD 为训练模型独特地定义了参数化核函数，该模型对模型相关的数据相关性进行编码。通过利用核函数，我们可以识别能够有效地为测试点提供最佳预测支持的训练样本。我们在多个分类领域进行了彻底的分析和实验，结果表明 HD-Explain 从各个方面都优于现有方法，包括 1）精确性（细粒度解释）、2）一致性和 3）计算效率，从而产生了令人惊讶的结果简单、有效、稳健的预测解释解决方案。]]></description>
      <guid>https://arxiv.org/abs/2403.15576</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>使用注意力和卷积的参数编码可减轻神经偏微分方程求解器的谱偏差</title>
      <link>https://arxiv.org/abs/2403.15652</link>
      <description><![CDATA[arXiv:2403.15652v1 公告类型：新
摘要：深度神经网络（DNN）越来越多地用于求解在对各种系统和物理现象进行建模时自然出现的偏微分方程（PDE）。然而，此类 DNN 的准确性会随着偏微分方程复杂度的增加而降低，并且由于它们倾向于学习低频解特性，因此它们还会受到谱偏差的影响。为了解决这些问题，我们引入了参数网格卷积注意网络（PGCAN），它可以在不利用域中任何标记数据的情况下解决偏微分方程系统。 PGCAN 的主要思想是使用基于网格的编码器对输入空间进行参数化，该编码器的参数通过 DNN 解码器连接到输出，该解码器利用注意力来优先考虑特征训练。我们的编码器提供局部学习能力，并使用卷积层来避免过度拟合并提高从域边界到内部的信息传播速率。我们在各种偏微分方程系统上测试了 PGCAN 的性能，结果表明，与竞争方法相比，它可以有效解决谱偏差并提供更准确的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2403.15652</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>数据异构下局部更新对去中心化学习的有效性</title>
      <link>https://arxiv.org/abs/2403.15654</link>
      <description><![CDATA[arXiv:2403.15654v1 公告类型：新
摘要：我们重新审视两种基本的去中心化优化方法：去中心化梯度跟踪（DGT）和去中心化梯度下降（DGD），并具有多个本地更新。我们考虑两种设置，并证明合并 $K &gt; 1$ 本地更新步骤可以降低通信复杂性。具体来说，对于$\mu$-强凸和$L$-平滑损失函数，我们证明了本地DGT实现了通信复杂性$\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big) $，其中$\rho$衡量网络连接性，$\delta$衡量局部损失的二阶异质性。我们的结果揭示了通信和计算之间的权衡，并表明当数据异构性较低且网络连接良好时，增加 $K$ 可以有效降低通信成本。然后，我们考虑局部损失具有相同最小值的过度参数化机制，我们证明，即使没有梯度校正，在 DGD 中使用局部更新也可以在降低通信复杂性方面产生与 DGT 类似的效果。数值实验验证了我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2403.15654</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:40 GMT</pubDate>
    </item>
    <item>
      <title>物联网入侵检测系统的多输入自动编码器引导特征选择</title>
      <link>https://arxiv.org/abs/2403.15511</link>
      <description><![CDATA[arXiv:2403.15511v1 公告类型：新
摘要：虽然入侵检测系统（IDS）受益于物联网数据特征的多样性和泛化性，但数据多样性（例如数据的异构性和高维度）也使得在物联网 IDS 中训练有效的机器学习模型变得困难。这还会导致潜在的冗余/噪声特征，从而可能降低 IDS 中检测引擎的准确性。本文首先介绍了一种称为多输入自动编码器（MIAE）的新型神经网络架构。 MIAE 由多个子编码器组成，可以处理来自具有不同特征的不同来源的输入。 MIAE 模型以无监督学习模式进行训练，将异构输入转换为低维表示，这有助于分类器区分正常行为和不同类型的攻击。为了在训练过程中提取和保留更多相关特征，同时删除不太重要/冗余的特征，我们进一步设计并在 MIAE 表示层之后嵌入一个特征选择层，从而形成一个名为 MIAEFS 的新模型。该层学习表示向量中特征的重要性，有助于从表示向量中选择信息丰富的特征。三个 IDS 数据集（即 NSLKDD、UNSW-NB15 和 IDS2017）上的结果表明，与其他方法（例如传统分类器、降维模型、不同输入维度的无监督表示学习方法）相比，MIAE 和 MIAEFS 具有优越的性能。无监督特征选择模型。此外，MIAE 和 MIAEFS 与随机森林 (RF) 分类器相结合，在检测复杂攻击（例如 Slowloris）方面实现了 96.5% 的准确率。使用以 MIAE 和 MIAEFS 表示的 RF 检测攻击样本的平均运行时间约为 1.7E-6 秒，而模型大小小于 1 MB。]]></description>
      <guid>https://arxiv.org/abs/2403.15511</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>通过增加表示等级和特征丰富度来提高类增量学习的前向兼容性</title>
      <link>https://arxiv.org/abs/2403.15517</link>
      <description><![CDATA[arXiv:2403.15517v1 公告类型：新
摘要：类增量学习（CIL）是持续学习中的一个关键子领域，旨在使模型能够逐步学习新的分类任务，同时保留从先前任务中获得的知识。尽管之前的研究主要集中在向后兼容的方法上以减轻灾难性遗忘，但最近的研究引入了前向兼容的方法来提高新任务的性能并补充现有的向后兼容的方法。在本研究中，我们引入了一种基于有效排名的特征丰富度增强（RFR）方法，旨在提高前向兼容性。具体来说，该方法增加了基本会话期间表示的有效排名，从而促进与未见过的新颖任务相关的更多信息特征的结合。因此，RFR 实现了向后和向前兼容性的双重目标：分别最小化特征提取器修改和增强新任务性能。为了验证我们方法的有效性，我们在有效排名和表示的香农熵之间建立了理论联系。随后，我们将 RFR 集成到 11 种著名的 CIL 方法中进行了全面的实验。我们的结果证明了我们的方法在提高新任务表现同时减少灾难性遗忘方面的有效性。此外，我们的方法显着提高了所有十一个检查案例的平均增量准确性。]]></description>
      <guid>https://arxiv.org/abs/2403.15517</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>GTC：用于自监督异构图表示的 GNN-Transformer 联合对比学习</title>
      <link>https://arxiv.org/abs/2403.15520</link>
      <description><![CDATA[arXiv:2403.15520v1 公告类型：新
摘要：由于消息传递机制强大的本地信息聚合能力，图神经网络（GNN）已成为各种图任务最强大的武器。然而，过度平滑始终阻碍 GNN 更深入并捕获多跳邻居。与 GNN 不同，Transformer 可以通过多头自注意力对全局信息和多跳交互进行建模，并且适当的 Transformer 结构可以对过度平滑问题表现出更强的免疫力。那么，我们能否提出一种新的框架来结合 GNN 和 Transformer，整合 GNN 的局部信息聚合和 Transformer 的全局信息建模能力来消除过度平滑问题？为了实现这一点，本文提出了一种 GNN-Transformer 的协作学习方案并构建了 GTC 架构。 GTC利用GNN和Transformer分支分别对不同视图的节点信息进行编码，并基于编码的跨视图信息建立对比学习任务，实现自监督的异构图表示。对于 Transformer 分支，我们提出了 Metapath-aware Hop2Token 和 CG-Hetphormer，它们可以与 GNN 合作，仔细编码不同级别的邻域信息。据我们所知，这是图表示学习领域首次尝试利用GNN和Transformer协同捕获不同视图信息并进行跨视图对比学习。在真实数据集上的实验表明，与最先进的方法相比，GTC 表现出优越的性能。代码可在 https://github.com/PHD-lanyu/GTC 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.15520</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>不要相信你所相信的：半监督学习中的错误校准</title>
      <link>https://arxiv.org/abs/2403.15567</link>
      <description><![CDATA[arXiv:2403.15567v1 公告类型：新
摘要：最先进的半监督学习（SSL）方法依赖于高度置信的预测作为伪标签来指导未标记样本的训练。该策略的固有缺点源于不确定性估计的质量，因为仅根据其不确定性程度来过滤伪标签，而不管其预测的正确性如何。因此，评估和增强网络预测的不确定性在伪标记过程中至关重要。在这项工作中，我们凭经验证明基于伪标签的 SSL 方法明显错误校准，并正式证明最小熵（香农熵的下限）的最小化是错误校准的潜在原因。为了缓解这个问题，我们集成了一个简单的惩罚项，它强制未标记样本的预测的对数距离保持较低，从而防止网络预测变得过度自信。对各种 SSL 图像分类基准的综合实验表明，所提出的解决方案系统地提高了相关 SSL 模型的校准性能，同时还增强了它们的判别能力，是解决 SSL 任务的一个有吸引力的补充。]]></description>
      <guid>https://arxiv.org/abs/2403.15567</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>国际象棋语言模型中的涌现世界模型和潜变量估计</title>
      <link>https://arxiv.org/abs/2403.15498</link>
      <description><![CDATA[arXiv:2403.15498v1 公告类型：新
摘要：语言模型表现出了前所未有的能力，引发了对其性能来源的争论。它仅仅是学习句法模式和表面统计数据的结果，还是它们从文本中提取语义和世界模型？ Li 等人之前的工作。通过在合成的、随机生成的黑白棋游戏上训练 GPT 模型对此进行了研究，发现该模型学习了棋盘状态的内部表示。我们将这项工作扩展到更复杂的国际象棋领域，对真实游戏进行训练，并使用线性探针和对比激活来研究我们模型的内部表示。该模型没有获得游戏的先验知识，并且仅针对下一个角色预测进行训练，但我们发现了棋盘状态内部表示的证据。我们通过使用这些内部表示对模型的激活进行干预并编辑其内部棋盘状态来验证这些内部表示。与 Li 等人之前的合成数据集方法不同，我们的分析发现该模型还学习估计潜在变量，例如玩家技能，以更好地预测下一个角色。我们导出了玩家技能向量并将其添加到模型中，将模型的胜率提高了 2.6 倍。]]></description>
      <guid>https://arxiv.org/abs/2403.15498</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>通过机器学习驱动的 Metalearners 对电力市场二氧化碳减排策略进行因果分析</title>
      <link>https://arxiv.org/abs/2403.15499</link>
      <description><![CDATA[arXiv:2403.15499v1 公告类型：新
摘要：本研究采用因果机器学习（CausalML）统计方法来分析电价政策对家庭部门二氧化碳（CO2）水平的影响。通过调查潜在结果和治疗效果之间的因果关系，其中定价政策的变化是治疗方法，我们的分析挑战了围绕激励性电价的传统观点。研究结果表明，采取此类政策可能会无意中增加二氧化碳排放强度。此外，我们还集成了基于机器学习的元算法，反映了当代统计方法，以增强因果分析的深度。该研究对学习者 X、T、S 和 R 进行比较分析，以确定基于已定义问题的指定目标和上下文细微差别的最佳方法。这项研究为正在进行的可持续发展实践对话提供了宝贵的见解，强调了在政策制定中考虑意外后果的重要性。]]></description>
      <guid>https://arxiv.org/abs/2403.15499</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>教育背景下集成强先验模块和数据重叠估计的三相 SFT 混合模型</title>
      <link>https://arxiv.org/abs/2403.15426</link>
      <description><![CDATA[arXiv:2403.15426v1 公告类型：新
摘要：在本文中，我们提出了一种基于端到端先验的三相监督微调模型，事实证明该模型比传统的微调方法更具竞争力。更具体地说，我们的模型实现了教育知识的结构拆解和增量引导输出。为此，我们通过采样器和重叠估计神经网络对三种类型的数据分类进行鲁棒化，并将预处理数据集分三批注入到预训练模型中以进行 LORA 微调。然后，我们设计了一个耦合系统提示、向量数据库和抽象语法树任务分割的优先模块。最后，将压缩方法和正则化约束应用于基于先验的微调模型，然后在输出端进行文本过滤以获得增量引导结果。我们的模型代表了第一个真正体现导师角色的研究成果，具有丰富的教育知识、逐步增量的引导输出和不公开答案的特点。大量实验表明，与开源模型相比，我们的模型在代码能力方面也达到了最先进的水平，在 HumanEval (@pass 1) 基准测试中达到了令人印象深刻的 75.10%。此外，我们的模型保持了强大的对话能力，13B 量化版本在 MMLU、C-Eval 和 AGIEval（5 shot）对话评估基准上分别获得了 56.34、50.60 和 45.27 的分数。]]></description>
      <guid>https://arxiv.org/abs/2403.15426</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>基于剪枝和恢复的联邦学习</title>
      <link>https://arxiv.org/abs/2403.15439</link>
      <description><![CDATA[arXiv:2403.15439v1 公告类型：新
摘要：考虑到现实环境中客户端的不同网络速度，提出了一种新颖的异构环境联邦学习训练框架。该框架融合了异步学习算法和剪枝技术，有效解决了传统联邦学习算法在异构设备场景下效率低下的问题，以及异步算法的陈旧问题和某些客户端训练不足的问题。通过在训练期间增量恢复模型大小，该框架可以加快模型训练速度，同时保持模型准确性。此外，还引入了联邦学习聚合过程的增强功能，结合了缓冲机制，使异步联邦学习能够类似于同步学习进行操作。此外，服务器向客户端传输全局模型过程中的优化减少了通信开销。我们在各种数据集上进行的实验表明：(i) 与传统的异步 FL 和 HeteroFL 相比，训练时间显着减少，收敛精度提高； (ii)我们的方法的优势在具有异构客户端和非独立同分布客户端数据的场景中更加明显。]]></description>
      <guid>https://arxiv.org/abs/2403.15439</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>使用 Rollout 算法生成 $n$-Gram、Transformers、HMM 和马尔可夫链的最有可能的序列</title>
      <link>https://arxiv.org/abs/2403.15465</link>
      <description><![CDATA[arXiv:2403.15465v1 公告类型：新
摘要：在本文中，我们考虑一种具有 $n$-gram 结构的转换器，例如底层的 ChatGPT。转换器提供下一个单词的概率，可用于生成单词序列。我们考虑基于这些概率来计算极有可能的单词序列的方法。计算从给定初始状态开始的最优（即最有可能）单词序列是一个棘手的问题，因此我们提出了计算时间上最可能的 $N$ 个单词序列的方法，该序列是 $N$ 中的低阶多项式，并且在$n$-gram 的词汇量大小。这些方法基于近似动态规划的推出方法，这是一种单策略迭代的形式，可以提高任何给定启发式策略的性能。在我们的例子中，我们使用贪婪启发式生成概率最高的单词作为下一个单词。我们通过分析、示例和计算实验表明，我们的方法能够生成极有可能的序列，并且与贪婪启发式相比，计算量略有增加。虽然我们的分析和实验集中于 Transformer 和类似 ChatGPT 模型中出现的马尔可夫链，但我们的方法适用于一般有限状态马尔可夫链，以及隐马尔可夫模型 (HMM) 的相关推理应用，其中使用维特比解码广泛地。]]></description>
      <guid>https://arxiv.org/abs/2403.15465</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>二阶优化中的模糊超参数更新</title>
      <link>https://arxiv.org/abs/2403.15416</link>
      <description><![CDATA[arXiv:2403.15416v1 公告类型：新
摘要：本研究将提出一种加速二阶优化收敛的混合方法。将引入对角 Hessian 矩阵的在线有限差分近似，以及几个超参数的模糊推理。已取得具有竞争力的成果]]></description>
      <guid>https://arxiv.org/abs/2403.15416</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:36 GMT</pubDate>
    </item>
    <item>
      <title>提升图卷积神经网络所需的全部就是注意力</title>
      <link>https://arxiv.org/abs/2403.15419</link>
      <description><![CDATA[arXiv:2403.15419v1 公告类型：新
摘要：图卷积神经网络（GCN）具有强大的非网格域图数据处理能力。他们可以捕获图中的拓扑逻辑结构和节点特征，并将它们集成到节点的最终表示中。 GCN 已在推荐系统、社交网络和蛋白质分子结构等各个领域得到广泛研究。随着图神经网络应用的不断增加，研究重点是在压缩其尺寸的同时提高其性能。在这项工作中，提出了一个名为图知识增强和蒸馏模块（GKEDM）的插件模块。 GKEDM 可以通过多头注意力机制提取和聚合图信息来增强节点表示并提高 GCN 的性能。此外，GKEDM可以作为知识蒸馏的辅助传递者。通过专门设计的注意力蒸馏方法，GKEDM 可以将大型教师模型的知识蒸馏为高性能且紧凑的学生模型。在多个数据集上的实验表明，GKEDM 可以以最小的开销显着提高各种 GCN 的性能。此外，它可以通过注意力蒸馏有效地将蒸馏知识从大型教师网络转移到小型学生网络。]]></description>
      <guid>https://arxiv.org/abs/2403.15419</guid>
      <pubDate>Tue, 26 Mar 2024 06:17:36 GMT</pubDate>
    </item>
    </channel>
</rss>