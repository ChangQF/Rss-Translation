<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Tue, 27 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>在线客服通用模型</title>
      <link>https://arxiv.org/abs/2402.15666</link>
      <description><![CDATA[arXiv:2402.15666v1 公告类型：新
摘要：构建机器学习模型可能是一个耗时的过程，在典型的业务场景中通常需要几个月的时间才能实现。为了确保一致的模型性能并考虑数据分布的变化，定期重新训练是必要的。本文介绍了一种改进电子商务在线客户服务的解决方案，提出了一种基于客户问题预测标签的通用模型，无需培训。我们的新颖方法涉及使用机器学习技术在记录中标记客户问题并创建问题和相应标签的存储库。当客户请求帮助时，信息检索模型会在存储库中搜索类似问题，并使用统计分析来预测相应的标签。通过消除单个模型训练和维护的需要，我们的方法减少了模型开发周期和成本。存储库只需要定期更新即可保持准确性。]]></description>
      <guid>https://arxiv.org/abs/2402.15666</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:38 GMT</pubDate>
    </item>
    <item>
      <title>客户服务中的联系人复杂性</title>
      <link>https://arxiv.org/abs/2402.15655</link>
      <description><![CDATA[arXiv:2402.15655v1 公告类型：新
摘要：寻求客户服务支持的客户可能会面临一系列复杂程度各异的问题。将高复杂性联系转给初级客服人员可能会导致多次转接或重复联系，而将低复杂性联系转给高级客服人员可能会限制他们为需要专业帮助的客户提供帮助的能力。为了解决这个问题，非常需要一种能够准确预测客户问题复杂性的机器学习模型。然而，定义接触的复杂性是一项艰巨的任务，因为它是一个高度抽象的概念。虽然由经验丰富的代理进行基于共识的数据注释是一种可能的解决方案，但它既耗时又昂贵。为了克服这些挑战，我们开发了一种新颖的机器学习方法来定义接触复杂性。我们没有依赖人工注释，而是训练了一个人工智能专家模型来模仿代理的行为，并根据人工智能专家的响应方式评估每个联系人的复杂性。如果人工智能专家不确定或缺乏理解联系记录的技能，则被视为高复杂性联系。根据收集的数据，我们的方法已被证明是可靠的、可扩展的且具有成本效益的。]]></description>
      <guid>https://arxiv.org/abs/2402.15655</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>学习半线性神经算子：用于预测和数据同化的统一递归框架</title>
      <link>https://arxiv.org/abs/2402.15656</link>
      <description><![CDATA[arXiv:2402.15656v1 公告类型：新
摘要：神经算子 (NO) 理论的最新进展使得能够快速、准确地计算偏微分方程 (PDE) 描述的复杂系统的解。尽管取得了巨大成功，但当前基于 NO 的解决方案在处理长时间尺度的时空偏微分方程时面临着重大挑战。具体来说，当前的 NO 理论并没有提出一个系统框架来执行数据同化，并根据稀疏采样的噪声测量有效地纠正 PDE 解随时间的演变。在本文中，我们提出了一种基于学习的状态空间方法来计算无限维半线性偏微分方程的解算子。利用半线性偏微分方程的结构和函数空间中的非线性观测器理论，我们开发了一种灵活的递归方法，该方法允许通过结合预测和校正操作来进行预测和数据同化。所提出的框架能够在长时间范围内产生快速、准确的预测，处理不规则采样的噪声测量以纠正解决方案，并受益于此类偏微分方程的空间和时间动态之间的解耦。我们通过对 Kuramoto-Sivashinsky、Navier-Stokes 和 Korteweg-de Vries 方程的实验表明，所提出的模型对噪声具有鲁棒性，并且可以利用任意数量的测量来校正长时间范围内的预测，而计算开销很小。]]></description>
      <guid>https://arxiv.org/abs/2402.15656</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>智能路由复杂性师生学习</title>
      <link>https://arxiv.org/abs/2402.15665</link>
      <description><![CDATA[arXiv:2402.15665v1 公告类型：新
摘要：客户服务往往是电子商务网站最耗时的环节，每次联系通常需要 10-15 分钟。因此，在不进行转接的情况下有效地将客户路由到适当的代理对于电子商务的成功至关重要。为此，我们开发了一个机器学习框架，可以预测客户联系人的复杂性，并相应地将其路由给适当的代理。该框架由两部分组成。首先，我们训练一个教师模型，根据接触后的记录对接触的复杂性进行评分。然后，我们使用教师模型作为数据注释器来提供标签来训练学生模型，该模型仅根据接触前的数据来预测复杂性。我们的实验表明，这样的框架是成功的，可以显着改善客户体验。我们还提出了一个称为复杂性 AUC 的有用指标，用于在统计层面评估客户服务的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.15665</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:37 GMT</pubDate>
    </item>
    <item>
      <title>通过加加速度正则化实现算子学习中的平滑和稀疏潜在动态</title>
      <link>https://arxiv.org/abs/2402.15636</link>
      <description><![CDATA[arXiv:2402.15636v1 公告类型：新
摘要：时空建模对于理解跨不同科学和工程学科的复杂系统至关重要，但由于系统固有的复杂性，控制方程通常不完全已知或难以计算。数据驱动的降阶模型 (ROM) 为通过压缩潜在空间中的计算解决方案进行快速准确的时空预测提供了一种有前景的方法。然而，这些模型在构建潜在空间时常常忽略连续快照之间的时间相关性，从而导致次优压缩、锯齿状潜在轨迹以及随时间推移的外推能力有限。为了解决这些问题，本文引入了一种连续算子学习框架，该框架将加加速度正则化纳入压缩潜在空间的学习中。这种加加速度正则化促进了潜在空间动力学的平滑性和稀疏性，这不仅提高了准确性和收敛速度，而且还有助于识别内在的潜在空间坐标。该框架由基于隐式神经表示 (INR) 的自动编码器和神经 ODE 潜在动力学模型组成，允许以任何所需的空间或时间分辨率进行推理。该框架的有效性通过纳维-斯托克斯方程控制的二维非定常流问题得到了证明，突显了其在各种科学和工程应用中加快高保真模拟的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.15636</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>多任务学习中的公平资源分配</title>
      <link>https://arxiv.org/abs/2402.15638</link>
      <description><![CDATA[arXiv:2402.15638v1 公告类型：新
摘要：通过联合学习多个任务，多任务学习（MTL）可以利用跨任务的共享知识，从而提高数据效率和泛化性能。然而，MTL 的一个主要挑战在于存在冲突的梯度，这可能会阻碍某些任务的公平优化，从而阻碍 MTL 实现更好的整体性能的能力。受通信网络中公平资源分配的启发，我们将 MTL 的优化表述为效用最大化问题，其中跨任务的损失减少在不同的公平性测量下最大化。为了解决这个问题，我们提出了 FairGrad，一种新颖的 MTL 优化方法。 FairGrad不仅能够灵活地侧重于某些任务，而且还实现了理论上的收敛保证。大量的实验表明，我们的方法可以在监督学习和强化学习的一系列多任务基准上实现梯度操作方法中最先进的性能。此外，我们将 $\alpha$-公平性的思想融入到各种 MTL 方法的损失函数中。广泛的实证研究表明，它们的性能可以显着提高。代码位于 \url{https://github.com/OptMN-Lab/fairgrad}。]]></description>
      <guid>https://arxiv.org/abs/2402.15638</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>具有目标抑制功能的多约束安全强化学习适用于安全关键型应用</title>
      <link>https://arxiv.org/abs/2402.15650</link>
      <description><![CDATA[arXiv:2402.15650v1 公告类型：新
摘要：尽管在现实世界中很常见，但具有多重约束的安全强化学习任务仍然是一个具有挑战性的领域。为了应对这一挑战，我们提出了目标抑制，这是一种根据安全批评家的说法，自适应抑制任务奖励最大化目标的新方法。我们在两个多约束安全领域对目标抑制进行基准测试，其中包括自动驾驶领域，任何不正确的行为都可能导致灾难性后果。根据经验，我们证明了我们提出的方法与现有的安全强化学习算法相结合，可以与我们的基线实现的任务奖励相匹配，并且约束违规显着减少。]]></description>
      <guid>https://arxiv.org/abs/2402.15650</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:36 GMT</pubDate>
    </item>
    <item>
      <title>通过预训练表示实现 NLP 中的高效主动学习</title>
      <link>https://arxiv.org/abs/2402.15613</link>
      <description><![CDATA[arXiv:2402.15613v1 公告类型：新
摘要：微调大型语言模型（LLM）现在是广泛应用中文本分类的常用方法。当标记文档稀缺时，主动学习有助于节省注释工作，但需要在每次采集迭代时重新训练大量模型。我们通过在主动学习循环中使用 LLM 的预训练表示来极大地加快这一过程，并且一旦获取了所需数量的标记数据，就可以对该标记数据进行微调，甚至对此标记数据进行不同的预训练 LLM 以实现最佳性能。正如在以预训练 BERT 和 RoBERTa 作为骨干的常见文本分类基准上所验证的那样，我们的策略在整个主动学习循环中产生了与微调类似的性能，但计算成本却低了几个数量级。通过我们的程序获取的数据可以推广到预训练的网络中，从而可以灵活地选择最终模型或在更新版本发布时对其进行更新。]]></description>
      <guid>https://arxiv.org/abs/2402.15613</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:35 GMT</pubDate>
    </item>
    <item>
      <title>MegaScale：将大型语言模型训练扩展到超过 10,000 个 GPU</title>
      <link>https://arxiv.org/abs/2402.15627</link>
      <description><![CDATA[arXiv:2402.15627v1 公告类型：新
摘要：我们介绍了构建和部署 MegaScale 的设计、实现和工程经验，MegaScale 是一个用于训练超过 10,000 个 GPU 规模的大型语言模型 (LLM) 的生产系统。如此规模的法学硕士培训给培训效率和稳定性带来了前所未有的挑战。我们采用全栈方法，跨模型块和优化器设计、计算和通信重叠、操作员优化、数据管道和网络性能调整来共同设计算法和系统组件。鉴于LLM培训工作的范围很长，在整个培训过程中保持高效率（即稳定性）是生产中的一个重要考虑因素。许多硬稳定性问题只会大规模出现，而深入的可观测性是解决这些问题的关键。我们开发了一套诊断工具来监控堆栈深处的系统组件和事件，识别根本原因，并得出有效的技术来实现容错并减少掉队的情况。在 12,288 个 GPU 上训练 175B LLM 模型时，MegaScale 实现了 55.2% 的模型 FLOP 利用率 (MFU)，与 Megatron-LM 相比，MFU 提高了 1.34 倍。我们分享在识别和修复故障和落后者方面的运营经验。我们希望通过从系统角度阐述问题并分享我们的经验，这项工作能够启发未来的法学硕士系统研究。]]></description>
      <guid>https://arxiv.org/abs/2402.15627</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:35 GMT</pubDate>
    </item>
    <item>
      <title>差分隐私公平二元分类</title>
      <link>https://arxiv.org/abs/2402.15603</link>
      <description><![CDATA[arXiv:2402.15603v1 公告类型：新
摘要：在这项工作中，我们研究了差分隐私和公平性约束下的二元分类。我们首先提出一种基于解耦技术的算法，用于学习仅具有公平性保证的分类器。该算法采用针对不同人口群体进行训练的分类器，并生成满足统计奇偶性的单个分类器。然后，我们改进该算法以纳入差异隐私。最终算法的性能在隐私、公平性和效用保证方面经过严格检验。对成人和信用卡数据集进行的实证评估表明，我们的算法在公平保证方面优于最先进的算法，同时保持相同水平的隐私和实用性。]]></description>
      <guid>https://arxiv.org/abs/2402.15603</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>训练非线性变压器以实现高效的情境学习：理论学习和泛化分析</title>
      <link>https://arxiv.org/abs/2402.15607</link>
      <description><![CDATA[arXiv:2402.15607v1 公告类型：新
摘要：基于 Transformer 的大型语言模型表现出了令人印象深刻的上下文学习能力，其中预训练的模型可以通过简单地使用该任务中的一些输入输出示例来增强查询来处理新任务，而无需进行微调。尽管在实证上取得了成功，但由于分析 Transformer 中的非线性自注意力和非线性激活所产生的非凸训练问题的技术挑战，如何训练 Transformer 实现 ICL 和相应的 ICL 能力的机制大多难以捉摸。据我们所知，本文首次对具有非线性自注意力和非线性 MLP 的 Transformer 训练动态进行了理论分析，以及所得模型的 ICL 泛化能力。专注于一组二元分类任务，我们使用这些任务子集的数据来训练 Transformer，并量化各种因素对 ICL 泛化性能对剩余未见任务（有或没有数据分布变化）的影响。我们还分析了学习到的 Transformer 中的不同组件如何影响 ICL 性能。此外，我们首次对模型剪枝如何影响 ICL 性能进行了理论分析，并证明适当的基于幅度的剪枝可以对 ICL 产生最小的影响，同时降低推理成本。这些理论发现通过数值实验得到了证实。]]></description>
      <guid>https://arxiv.org/abs/2402.15607</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的完井排序用于油井性能优化</title>
      <link>https://arxiv.org/abs/2402.15608</link>
      <description><![CDATA[arXiv:2402.15608v1 公告类型：新
摘要：由于油井开发的复杂性以及估计长期油井产量的不确定性，建立准确的油田开发参数以优化长期石油产量需要时间和精力。传统上，石油和天然气公司使用模拟软件来预测产量，其计算成本本来就很高。因此，机器学习方法最近在文献中被用作通过增强完井条件来优化油井开发的有效替代方案。该项目的主要目标是开发有效的机器学习模型，该模型可以整合多维预测变量（即完工条件）的影响，以准确预测 12 个月累计产量。
  实施了三种预测回归机器学习模型来预测 12 个月的累积石油产量：随机森林、梯度提升和长短期记忆模型。所有三个模型都得出了累积产量预测，其均方根误差 (RMSE) 值范围为 7.35 至 20,01 千桶石油。尽管我们假设所有模型都会产生准确的预测，但结果表明迫切需要进一步完善以在地下创建可靠且合理的预测工具。虽然这项研究没有产生用于完成测序以最大化长期生产的最佳模型，但我们确定仅靠机器学习模型并不能自给自足地解决这种性质的问题。因此，有可能进行重大改进，包括综合特征工程，并建议探索使用混合或代理模型（即耦合物理简化模型和机器学习模型），以确定对完成测序工作流程的进展做出重大贡献。]]></description>
      <guid>https://arxiv.org/abs/2402.15608</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:34 GMT</pubDate>
    </item>
    <item>
      <title>深度网络总是让人摸不着头脑，原因如下</title>
      <link>https://arxiv.org/abs/2402.15555</link>
      <description><![CDATA[arXiv:2402.15555v1 公告类型：新
摘要：Grokking 或延迟泛化是深度神经网络 (DNN) 中的泛化在实现接近零训练误差很久之后才发生的现象。先前的研究报告了在特定受控设置中发生的 grokking，例如使用大范数参数初始化的 DNN 或在算法数据集上训练的变压器。我们证明，grokking 实际上更为广泛，并且在各种实际环境中得以实现，例如在 CIFAR10 上训练卷积神经网络 (CNN) 或在 Imagenette 上训练 Resnet。我们引入了延迟鲁棒性的新概念，即 DNN 深入研究对抗性示例并在插值和/或泛化后很长时间内变得鲁棒。我们基于 DNN 输入输出映射局部复杂性的新度量，对延迟泛化和延迟鲁棒性的出现进行了分析解释。我们的局部复杂性测量所谓的“线性区域”（又名样条分区区域）的密度，这些区域平铺 DNN 输入空间，并作为训练的实用进度度量。我们提供了第一个证据，表明对于分类问题，线性区域在训练期间经历相变，然后它们从训练样本迁移（使那里的 DNN 映射更平滑）并移向决策边界（使那里的 DNN 映射不太平滑）。 Grokking 发生在相变后，由于训练点周围的 DNN 映射的线性化，输入空间的稳健划分出现了。网站：https://bit.ly/grok-adversarial]]></description>
      <guid>https://arxiv.org/abs/2402.15555</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:33 GMT</pubDate>
    </item>
    <item>
      <title>公平多元自适应回归样条，确保公平和透明度</title>
      <link>https://arxiv.org/abs/2402.15561</link>
      <description><![CDATA[arXiv:2402.15561v1 公告类型：新
摘要：预测分析广泛应用于包括教育在内的各个领域，为决策提供信息并改善结果。然而，许多预测模型是专有的，研究人员和从业者无法评估或修改，限制了他们的责任和道德设计。此外，预测模型对于使用它们的官员来说通常是不透明和难以理解的，从而降低了他们的信任和效用。此外，预测模型可能会引入或加剧偏见和不平等，正如它们在社会许多部门所做的那样。因此，需要透明、可解释和公平的预测模型，以便不同利益相关者轻松采用和调整。在本文中，我们提出了一种基于多元自适应回归样条（MARS）的公平预测模型，该模型在学习过程中结合了公平性措施。 MARS 是一种非参数回归模型，它执行特征选择、处理非线性关系、生成可解释的决策规则，并得出变量的最佳分割标准。具体来说，我们将公平性集成到结优化算法中，并提供理论和经验证据来证明它如何实现公平的结放置。我们将 fairMARS 模型应用于现实世界的数据，并证明其在准确性和公平性方面的有效性。我们的论文有助于推进负责任和道德的预测分析，造福社会。]]></description>
      <guid>https://arxiv.org/abs/2402.15561</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:33 GMT</pubDate>
    </item>
    <item>
      <title>希尔伯特代表的基金会政策</title>
      <link>https://arxiv.org/abs/2402.15567</link>
      <description><![CDATA[arXiv:2402.15567v1 公告类型：新
摘要：无监督和自监督目标，例如下一个令牌预测，使得能够从大量未标记数据中预训练通才模型。然而，在强化学习（RL）中，从离线数据中找到一个真正通用且可扩展的无监督预训练目标来实现通用策略仍然是一个主要的悬而未决的问题。虽然基于目标条件强化学习、行为克隆和无监督技能学习等原则，已经提出了许多方法来实现通用的自监督强化学习，但这些方法在所发现行为的多样性、需要高质量的示范数据，或者下游任务缺乏明确的提示或适应机制。在这项工作中，我们提出了一种新颖的无监督框架来预训练通才策略，这些策略从未标记的离线数据中捕获多样化的、最优的、长期的行为，以便它们能够以零样本的方式快速适应任何任意的新任务。我们的主要见解是学习一种保留底层环境的时间结构的结构化表示，然后通过定向运动跨越这个学习的潜在空间，从而为下游任务提供各种零样本策略“提示”方案。通过我们对模拟机器人运动和操作基准的实验，我们表明，我们的无监督策略可以以零样本的方式解决目标条件和一般强化学习任务，甚至常常优于专门为每种设置设计的先前方法。我们的代码和视频可在 https://seohong.me/projects/hilp/ 获取]]></description>
      <guid>https://arxiv.org/abs/2402.15567</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:33 GMT</pubDate>
    </item>
    </channel>
</rss>