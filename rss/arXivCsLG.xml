<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 07 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>具有去相关反向传播的高效深度学习</title>
      <link>https://arxiv.org/abs/2405.02385</link>
      <description><![CDATA[arXiv:2405.02385v1 公告类型：新
摘要：反向传播算法仍然是训练深度神经网络（DNN）的主导且最成功的方法。与此同时，大规模训练 DNN 需要大量的计算成本，因此碳足迹也很高。越来越多的证据表明，输入去相关可能会加速深度学习。然而，迄今为止，这尚未转化为大规模 DNN 训练效率的实质性提高。这主要是由于执行快速稳定的网络范围去相关的挑战造成的。在这里，我们首次证明使用去相关反向传播对非常深的神经网络进行更有效的训练是可行的。为了实现这一目标，我们使用了一种新颖的算法，该算法使用最小的计算开销引发网络范围的输入去相关。通过将该算法与仔细的优化相结合，在训练 18 层深度残差网络时，与反向传播相比，我们获得了两倍以上的加速比和更高的测试精度。这表明去相关为大规模高效深度学习提供了令人兴奋的前景。]]></description>
      <guid>https://arxiv.org/abs/2405.02385</guid>
      <pubDate>Tue, 07 May 2024 06:19:19 GMT</pubDate>
    </item>
    <item>
      <title>稀疏 Tsetlin 机：具有活动文字的稀疏表示</title>
      <link>https://arxiv.org/abs/2405.02375</link>
      <description><![CDATA[arXiv:2405.02375v1 公告类型：新
摘要：本文介绍了稀疏 Tsetlin 机（STM），这是一种有效处理稀疏数据的新型 Tsetlin 机（TM）。传统上，TM 不考虑稀疏性等数据特征，这在 NLP 应用程序和其他基于词袋的表示中常见。因此，TM 必须初始化、存储和处理大量的零值，从而导致过多的内存使用和计算时间。之前创建稀疏 TM 的尝试主要是不成功的，主要是因为他们无法识别哪些文字足以进行 TM 训练。通过引入主动文字 (AL)，STM 可以专门关注对当前数据表示有积极贡献的文字，显着减少内存占用和计算时间，同时展示有竞争力的分类性能。]]></description>
      <guid>https://arxiv.org/abs/2405.02375</guid>
      <pubDate>Tue, 07 May 2024 06:19:18 GMT</pubDate>
    </item>
    <item>
      <title>去中心化学习对节点的鲁棒性和数据中断</title>
      <link>https://arxiv.org/abs/2405.02377</link>
      <description><![CDATA[arXiv:2405.02377v1 公告类型：新
摘要：在充满活力的人工智能研究领域，去中心化学习正在蓬勃发展。去中心化学习允许各个节点将数据保存在生成​​数据的本地，并通过协作细化的交互过程在它们之间共享从本地数据中提取的知识。该范例支持以下场景：由于隐私或主权原因，或者由于模型与必须执行推理的位置的接近性的实时限制，数据无法离开本地节点。去中心化学习的分布式性质意味着中心化学习面临着重大的新研究挑战。其中，在本文中，我们重点关注鲁棒性问题。具体来说，我们研究节点中断对集体学习过程的影响。假设给定百分比的“中心”节点从网络中消失，我们关注不同的情况，其特征是（i）节点之间的数据分布不同，以及（ii）协作学习任务开始时发生中断的不同时间。通过这些配置，我们能够展示网络连接节点的属性之间的重要相互作用、在中断或缺乏知识之前集体获取的知识的持久性，以及中断前后数据可用性的影响。我们的结果表明，去中心化学习过程对于网络中断具有非常强大的鲁棒性。只要网络中某个地方保留了最少量的数据，学习过程就能够从中断中恢复并实现显着的分类准确性。这显然取决于中断后剩余的连接性，但我们表明，即使是完全隔离的节点也可以保留中断前获得的重要知识。]]></description>
      <guid>https://arxiv.org/abs/2405.02377</guid>
      <pubDate>Tue, 07 May 2024 06:19:18 GMT</pubDate>
    </item>
    <item>
      <title>垂直联邦学习中的贡献评估研究综述</title>
      <link>https://arxiv.org/abs/2405.02364</link>
      <description><![CDATA[arXiv:2405.02364v1 公告类型：新
摘要：垂直联合学习（VFL）已成为机器学习中的一种关键方法，用于解决与集中式数据存储和处理相关的隐私问题。 VFL 促进同一用户群上具有不同特征集的多个实体之间的协作，从而无需直接共享数据即可联合训练预测模型。 VFL 的一个关键方面是公平、准确地评估每个实体对学习过程的贡献。这对于维持参与实体之间的信任、确保公平的资源共享以及培育可持续的合作框架至关重要。本文对 VFL 中的贡献评估进行了全面回顾。我们按照 VFL 生命周期、评估粒度、隐私考虑和核心计算方法对大量贡献评估技术进行分类。我们还探索了 VFL 中涉及贡献评估的各种任务，并分析了它们所需的评估属性以及与 VFL 生命周期阶段的关系。最后，我们对 VFL 贡献评估的未来挑战提出了愿景。通过对当前形势和潜在进展进行结构化分析，本文旨在指导研究人员和从业者设计和实施更有效、高效和以隐私为中心的 VFL 解决方案。相关文献和开源资源已在 GitHub 存储库中编译并持续更新：\url{https://github.com/cuiyuebing/VFL_CE}。]]></description>
      <guid>https://arxiv.org/abs/2405.02364</guid>
      <pubDate>Tue, 07 May 2024 06:19:17 GMT</pubDate>
    </item>
    <item>
      <title>通过视觉内容增强社交媒体帖子受欢迎程度预测</title>
      <link>https://arxiv.org/abs/2405.02367</link>
      <description><![CDATA[arXiv:2405.02367v1 公告类型：新
摘要：我们的研究提出了一个预测基于图像的社交媒体内容流行度的框架，该框架专注于解决复杂的图像信息和分层数据结构。我们利用 Google Cloud Vision API 有效地从用户的帖子中提取关键图像和颜色信息，与单独使用非图像协变量相比，准确率提高了 6.8%。对于预测，我们探索了广泛的预测模型，包括线性混合模型、支持向量回归、多层感知器、随机森林和 XGBoost，以线性回归为基准。我们的比较研究表明，能够捕捉协变量之间潜在非线性相互作用的模型优于其他方法。]]></description>
      <guid>https://arxiv.org/abs/2405.02367</guid>
      <pubDate>Tue, 07 May 2024 06:19:17 GMT</pubDate>
    </item>
    <item>
      <title>CVTGAD：具有交叉视图注意力的简化变压器，用于无监督图级异常检测</title>
      <link>https://arxiv.org/abs/2405.02359</link>
      <description><![CDATA[arXiv:2405.02359v1 公告类型：新
摘要：无监督图级异常检测（UGAD）在化学分析和生物信息学等各个关键学科中都取得了显著的表现。现有的UGAD范式通常采用数据增强技术来构建多个视图，然后采用不同的策略从不同视图获取表示以联合进行UGAD。然而，大多数以前的工作仅从有限的接受域考虑节点/图之间的关系，导致一些关键的结构模式和特征信息被忽略。此外，大多数现有方法以并行的方式分别考虑不同的视图，无法直接探索不同视图之间的相互关系。因此，需要一种具有更大接受域的方法，可以直接探索不同视图之间的相互关系。在本文中，我们提出了一种用于无监督图级异常检测的新型简化Transformer，即CVTGAD。为了增加感受野，我们构建了一个简化的基于 Transformer 的模块，从图内和图间两个角度利用节点/图之间的关系。此外，我们设计了一个跨视图注意机制，直接利用不同视图之间的视图共现，弥合节点级和图级的视图间差距。据我们所知，这是首次将 Transformer 和交叉注意应用于 UGAD 的工作，实现了图神经网络和 Transformer 的协同工作。在 3 个领域的 15 个真实数据集上进行的大量实验证明了 CVTGAD 在 UGAD 任务上的优势。代码可在 \url{https://github.com/jindongli-Ai/CVTGAD} 获得。]]></description>
      <guid>https://arxiv.org/abs/2405.02359</guid>
      <pubDate>Tue, 07 May 2024 06:19:16 GMT</pubDate>
    </item>
    <item>
      <title>整体评估指标：联邦学习的用例敏感评估指标</title>
      <link>https://arxiv.org/abs/2405.02360</link>
      <description><![CDATA[arXiv:2405.02360v1 公告类型：新
摘要：针对不同的应用和不同的角度，人们提出了大量的联邦学习（FL）算法。然而，对此类方法的评估通常依赖于单一指标（例如准确性）。这种做法无法考虑不同用例的独特需求和多样化要求。因此，如何综合评估 FL 算法并为指定用例确定最合适的候选算法仍然是一个悬而未决的问题。为了缩小这一研究差距，我们在这项工作中引入了 FL 的整体评估指标 (HEM)。具体来说，我们共同关注三个主要用例，即物联网 (IoT)、智能设备和机构。评估指标涵盖准确性、收敛性、计算效率、公平性和个性化等各个方面。然后，我们为每个用例分配各自的重要性向量，反映其不同的性能要求和优先级。通过将这些度量分量与其各自的重要性向量集成，最终生成 HEM 指数。通过评估这三个流行用例中的不同 FL 算法，我们的实验结果表明，HEM 可以有效评估和识别最适合特定场景的 FL 算法。我们预计这项工作将有助于揭示实用 FL 算法在实际应用中的评估过程。]]></description>
      <guid>https://arxiv.org/abs/2405.02360</guid>
      <pubDate>Tue, 07 May 2024 06:19:16 GMT</pubDate>
    </item>
    <item>
      <title>交通系统移动性的大型语言模型：预测任务调查</title>
      <link>https://arxiv.org/abs/2405.02357</link>
      <description><![CDATA[arXiv:2405.02357v1 公告类型：新
摘要： 移动性分析是交通系统研究领域的关键要素。预测交通信息为解决日益增长的交通需求与交通基础设施的局限性之间的冲突提供了可行的解决方案。预测人类出行对于协助各种交通和城市管理任务（例如出租车调度和城市规划）具有重要意义。机器学习和深度学习方法因其灵活性和准确性而受到青睐。如今，随着大型语言模型（LLM）的出现，许多研究人员将这些模型与先前的技术相结合或应用LLM来直接预测未来的交通信息和人类出行行为。然而，目前缺乏关于法学硕士如何为这一领域做出贡献的全面研究。这项调查探讨了使用法学硕士解决流动性预测问题的现有方法。我们提供了有关交通系统中预测应用的文献综述，阐明了研究人员如何利用法学硕士，展示了最新的最新进展，并确定了在该领域充分利用法学硕士必须克服的挑战。]]></description>
      <guid>https://arxiv.org/abs/2405.02357</guid>
      <pubDate>Tue, 07 May 2024 06:19:15 GMT</pubDate>
    </item>
    <item>
      <title>时间序列基础模型综述：用大语言模式推广时间序列表示</title>
      <link>https://arxiv.org/abs/2405.02358</link>
      <description><![CDATA[arXiv:2405.02358v1 公告类型：新
摘要：时间序列数据在各个领域中普遍存在，使得时间序列分析变得至关重要。传统的时间序列模型是特定于任务的，具有单一的功能和有限的泛化能力。最近，大型语言基础模型展示了其在跨任务可迁移性、零样本/少样本学习和决策可解释性方面的卓越能力。这一成功激发了人们对探索同时解决多个时间序列挑战的基础模型的兴趣。主要有两个研究方向，即\textbf{从头开始预训练时间序列基础模型}和\textbf{调整时间序列的大型语言基础模型}。它们都有助于开发一个高度通用、通用且易于理解的时间序列分析统一模型。该调查提供了一个 3E 分析框架，用于综合检验相关研究。具体来说，我们从三个维度来审视现有的作品，即 \textbf{Effectiveness}、\textbf{Efficiency} 和 \textbf{Explainability}。在每个维度中，我们重点讨论相关工作如何通过考虑时间序列领域的独特挑战来设计量身定制的解决方案。此外，我们提供了领域分类法，以帮助关注者跟上特定领域的进展。此外，我们引入了广泛的资源来促进该领域的发展，包括数据集、开源、时间序列库。还维护 GitHub 存储库以进行资源更新 (https://github.com/start2020/Awesome-TimeSeries-LLM-FM)。]]></description>
      <guid>https://arxiv.org/abs/2405.02358</guid>
      <pubDate>Tue, 07 May 2024 06:19:15 GMT</pubDate>
    </item>
    <item>
      <title>用于 LncRNA 疾病关联预测的异构网络和图注意自动编码器</title>
      <link>https://arxiv.org/abs/2405.02354</link>
      <description><![CDATA[arXiv:2405.02354v1 公告类型：新
摘要：新兴研究表明，lncRNA 与一系列复杂的人类疾病相关。然而，大多数现有方法在识别非线性lncRNA-疾病关联（LDA）方面存在局限性，并且预测新的LDA仍然是一个巨大的挑战。因此，LDA的准确识别对于疾病的预警和治疗非常重要。这项工作充分利用多种来源的生物医学数据来构建lncRNA和疾病的特征，并且有效地整合了线性和非线性特征。此外，还提出了一种基于图注意力自动编码器的新型深度学习模型，称为HGATELDA。首先，lncRNA 和疾病的线性特征是由 miRNA-lncRNA 相互作用矩阵和 miRNA-疾病相互作用矩阵创建的。随后，使用图注意力自动编码器提取疾病和lncRNA的非线性特征，该编码器很大程度上保留了关键信息并有效聚合了节点的邻域信息。最终，可以通过融合疾病和lncRNA的线性和非线性特征来预测LDA。当使用 5 倍交叉验证进行评估时，HGATELDA 模型达到了令人印象深刻的 AUC 值 0.9692，这表明与最近的几个预测模型相比，其具有优越的性能。同时，案例研究进一步证明了HGATELDA在识别新型LDA方面的有效性。 HGATELDA 模型似乎是预测 LDA 的可行计算模型。]]></description>
      <guid>https://arxiv.org/abs/2405.02354</guid>
      <pubDate>Tue, 07 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>随机多元通用基有限状态机：一个理论和实践上优雅的非线性函数逼近器</title>
      <link>https://arxiv.org/abs/2405.02356</link>
      <description><![CDATA[arXiv:2405.02356v1 公告类型：新
摘要：非线性对于捕获复杂的输入输出关系至关重要，尤其是在深度神经网络中。然而，非线性函数通常会产生各种硬件和计算开销。与此同时，随机计算（SC）已经成为解决这一挑战的一种有前景的方法，它通过牺牲输出精度来换取硬件的简单性。为此，本文提出了一种史无前例的随机多元通用基有限状态机（SMURF），它利用 SC 来高精度地生成硬件简单的多元非线性函数。我们提出了 SMURF 的有限状态机 (FSM) 架构，以及用于精确逼近通用非线性函数的采样门系数的分析推导。实验证明了SMURF的优越性，仅需要泰勒级数近似方案的16.07%面积和14.45%功耗，以及查找表(LUT)方案的2.22%面积。]]></description>
      <guid>https://arxiv.org/abs/2405.02356</guid>
      <pubDate>Tue, 07 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>是什么让模型具有组合性？理论观点：有补充</title>
      <link>https://arxiv.org/abs/2405.02350</link>
      <description><![CDATA[arXiv:2405.02350v1 公告类型：新
摘要：组合性被认为是语言的关键组成部分，并且已经开发了各种组合基准来实证探索现有序列处理模型的组合泛化。这些基准测试通常会强调现有模型的失败，但尚不清楚为什么这些模型会以这种方式失败。在本文中，我们试图从理论上理解模型的组成结构在这些失败中所扮演的角色，以及这种结构如何与它们的表达性和样本复杂性相关。我们提出了组合函数及其组合复杂性的一般神经符号定义。然后，我们展示各种现有的通用和专用序列处理模型（例如循环模型、卷积模型和基于注意力的模型）如何符合这个定义，并用它来分析它们的组合复杂性。最后，我们为组合模型的表达性和系统概括提供了理论保证，这些模型明确依赖于我们提出的定义，并强调了导致实证表现不佳的因素。]]></description>
      <guid>https://arxiv.org/abs/2405.02350</guid>
      <pubDate>Tue, 07 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>使用专用神经加速器实现通用神经代理求解器</title>
      <link>https://arxiv.org/abs/2405.02351</link>
      <description><![CDATA[arXiv:2405.02351v1 公告类型：新
摘要：基于代理神经网络的偏微分方程（PDE）求解器具有以加速方式求解 PDE 的潜力，但它们在很大程度上仅限于具有固定域大小、几何布局和边界条件的系统。我们提出了专门的神经加速器驱动的域分解方法（SNAP-DDM），这是一种基于 DDM 的 PDE 求解方法，其中使用专门的神经算子集合准确地解决包含任意边界条件和几何参数的子域问题。我们针对 2D 电磁学和流体流动问题定制 SNAP-DDM，并展示网络架构和损失函数工程的创新如何能够产生具有接近单位精度的专用代理子域求解器。我们利用这些求解器和标准 DDM 算法来准确解决具有各种域大小的自由形式电磁和流体问题。]]></description>
      <guid>https://arxiv.org/abs/2405.02351</guid>
      <pubDate>Tue, 07 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>COPAL：大型语言生成模型中的持续剪枝</title>
      <link>https://arxiv.org/abs/2405.02347</link>
      <description><![CDATA[arXiv:2405.02347v1 公告类型：新
摘要：将预训练的大语言模型适应自然语言处理中的不同领域需要两个关键考虑因素：高计算需求和模型无法持续适应。为了同时解决这两个问题，本文提出了 COPAL（自适应语言设置中的持续修剪），这是一种为在连续模型自适应设置下修剪大型语言生成模型而开发的算法。在避免大量资源的微调或再训练的同时，我们的修剪过程以建议的敏感性分析为指导。灵敏度有效地衡量模型承受新数据集引入的扰动的能力，并找到与所有遇到的数据集相关的模型权重。因此，COPAL 允许模型无缝适应新领域，同时提高资源效率。我们对各种规模的法学硕士的实证评估表明，COPAL 优于基线模型，证明了其在效率和适应性方面的功效。]]></description>
      <guid>https://arxiv.org/abs/2405.02347</guid>
      <pubDate>Tue, 07 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>MBTI 类型的可解释多标签分类</title>
      <link>https://arxiv.org/abs/2405.02349</link>
      <description><![CDATA[arXiv:2405.02349v1 公告类型：新
摘要：在这项研究中，我们的目标是确定最有效的机器学习模型，用于从 Reddit 帖子和 Kaggle 数据集中准确分类 Myers-Briggs 类型指标 (MBTI) 类型。我们使用二元相关性方法应用多标签分类。我们使用可解释的人工智能（XAI）方法来强调过程和结果的透明度和可理解性。为了实现这一目标，我们尝试了玻璃盒学习模型，即为简单性、透明性和可解释性而设计的模型。我们为玻璃盒模型选择了 k 最近邻、多项式朴素贝叶斯和逻辑回归。我们表明，如果排除具有观察者 (S) 特征的类，多项式朴素贝叶斯和 k-近邻会表现得更好，而当所有类都有 &gt; 550 个条目时，逻辑回归会获得最佳结果。]]></description>
      <guid>https://arxiv.org/abs/2405.02349</guid>
      <pubDate>Tue, 07 May 2024 06:19:12 GMT</pubDate>
    </item>
    </channel>
</rss>