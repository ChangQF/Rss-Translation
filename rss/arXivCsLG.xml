<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.lg arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.lg arxiv.org e-print档案中的更新。</description>
    <lastBuildDate>Mon, 24 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>P2W：从功率轨迹到权重矩阵 - 一种非常规的转移学习方法</title>
      <link>https://arxiv.org/abs/2502.14968</link>
      <description><![CDATA[ARXIV：2502.14968V1公告类型：新 
摘要：芯片上嵌入式系统中部署机器学习（ML）模型的快速增长导致医疗保健和自动驾驶汽车等领域的变化转变。培训此类嵌入式ML模型的主要挑战之一是缺乏公开可用的高质量培训数据。转移学习方法通​​过利用现有ML模型中封装的知识作为培训新的ML模型的起点，以应对这一挑战。但是，现有的转移学习方法需要直接访问并不总是可行的现有模型，尤其是对于部署在嵌入式SOC上的ML模型。因此，在本文中，我们引入了一种新型的非常规的传递学习方法，通过从嵌入式SOC上运行的现有ML模型中提取和使用权重，而无需访问SOC中的模型，以训练新的ML模型。我们的方法在执行ML模型时捕获了SOC的功耗测量值，并将其转换为用于初始化新ML模型的近似权重矩阵。这提高了新模型的学习效率和预测性能，尤其是在可用于训练该模型的数据有限的情况下。与使用相同数量的有限培训数据相比，我们的新型方法可以有效地提高新ML模型的准确性3倍。]]></description>
      <guid>https://arxiv.org/abs/2502.14968</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>特征柴菲尔德：通过随机矩阵理论的因果子空间过滤，以实现对抗性强大的视觉模型</title>
      <link>https://arxiv.org/abs/2502.14976</link>
      <description><![CDATA[ARXIV：2502.14976V1公告类型：新 
摘要：视觉模型（VLMS）继承了大语言模型（LLMS）的对抗性漏洞，这些漏洞被其多模式的性质进一步加剧。现有的防御措施，包括对抗性训练，输入转换和启发式检测，在计算上是昂贵，依赖建筑的且针对自适应攻击的脆弱的。我们介绍了特征性的特征，这是一种推理时间防御，利用随机矩阵理论来量化高维VLM表示中的对抗性破坏。与依赖经验启发式方法的先前方法不同，特征希尔德采用尖刺的协方差模型来检测结构的光谱偏差。使用基于鲁棒性的非符号评分（RBN）和基于分位数的阈值，它将因果特征向量分开，该因素特征向量将语义信息与对对抗性伪影易感的相关特征向量进行编码。通过将嵌入到因果子空间上，特征柴场可以过滤对抗噪声，而无需修改模型参数或需要对抗训练。这种与建筑无关的，攻击无形的方法大大降低了攻击成功率，建立了光谱分析作为常规防御的原则替代方案。我们的结果表明，特征希尔德始终优于所有现有防御，包括对抗性训练，Uniguard和苹果酒。]]></description>
      <guid>https://arxiv.org/abs/2502.14976</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大规模对个体行为的生成建模</title>
      <link>https://arxiv.org/abs/2502.14998</link>
      <description><![CDATA[ARXIV：2502.14998V1公告类型：新 
摘要：使用AI对人类行为进行建模，尤其是在人类与该技术相互作用的领域中，人们越来越感兴趣。尽管大多数现有的工作都以总级别的人类行为进行模型，但我们的目标是在个人层面上建模行为。行为风格的最新方法 - 或仅从其行为中识别某人的任务 - 在国际象棋等领域中显示了承诺，但是这些方法是不可扩展的（例如，对每个人的单独模型进行微调）生成性，因为它们无法产生动作。我们通过将行为风格计量学作为多任务学习问题来解决这些限制 - 每个任务代表一个独特的人 - 并使用参数有效的微调（PEFT）方法来学习每个人的明确样式矢量。样式向量是生成的：它们有选择地激活共享的“技能”参数，以以每个人的样式生成动作。它们还诱导了我们可以通过算法来解释和操纵的潜在空间。特别是，我们开发了一种针对样式转向的一般技术，使我们可以将玩家的样式向量引导到所需的属性。我们将方法应用于前所未有的两场截然不同的比赛：国际象棋（47,864名球员）和火箭联赛（2,000名球员）。我们还通过将我们的方法应用于图像生成，在其中展示了一般性，我们在其中学习了10,177名名人的风格向量，并使用这些向量来引导其图像。]]></description>
      <guid>https://arxiv.org/abs/2502.14998</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度提取器的数字实现本质上是信息丰富的</title>
      <link>https://arxiv.org/abs/2502.15004</link>
      <description><![CDATA[ARXIV：2502.15004V1公告类型：新 
摘要：深度特征提取器中的快速信息（能量）传播对于平衡计算复杂性与表达性作为输入的表示至关重要。我们证明了统一框架中能量传播速度的上限，该框架涵盖了欧几里得和非欧几里得领域的不同神经网络模型。有关信号域的其他结构信息可用于明确确定或改善衰减速率。为了说明这一点，我们显示了1）具有离散域输入信号的特征提取器的全局指数衰减，以及2）通过在局部紧凑的Abelian（LCA）组上散射卷积神经网络（CNN）。]]></description>
      <guid>https://arxiv.org/abs/2502.15004</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解指示设置中链接预测的设计原理</title>
      <link>https://arxiv.org/abs/2502.15008</link>
      <description><![CDATA[ARXIV：2502.15008V1公告类型：新 
摘要：链接预测是用于建模关系数据的图表学习（GRL）中广泛研究的任务。 GRL中的早期理论基于对称邻接矩阵的假设，反映了无方向的设置。结果，即使现实世界中的数据通常涉及通过关系方向传达的关键信息，但以下许多最新研究都在这种对称性假设下继续运行。这种疏忽限制了这些模型充分捕获定向相互作用的复杂性的能力。在本文中，我们通过评估在无向环境中成功的关键启发式方法来关注定向链接预测的挑战。我们提出了这些启发式方法的简单但有效的适应定向，并表明这些修饰与最初为无向图设计的领先图形神经网络（GNNS）相比，这些修改会产生竞争性能。通过一系列广泛的实验，我们得出了洞察力，这些见解为有向链接预测的新型框架开发而成，该预测不仅超过了基线方法，而且在多个基准上都优于最先进的GNN。]]></description>
      <guid>https://arxiv.org/abs/2502.15008</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迈向物理引导的基础模型</title>
      <link>https://arxiv.org/abs/2502.15013</link>
      <description><![CDATA[ARXIV：2502.15013V1公告类型：新 
摘要：传统的基础模型已在广泛的数据集中进行了预训练，以减少训练资源（例如时间，能量，标记的样本），以微调各种下游任务。但是，传统的基础模型在分布外的预测方面努力努力，并且可以产生不现实且身体上不可行的输出。我们提出了物理指导基础模型（PGFM）的符号，即与广泛或一般领域（例如科学）物理知识集成的基础模型，适用于广泛的下游任务。]]></description>
      <guid>https://arxiv.org/abs/2502.15013</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>加速神经网络培训：对algoperf竞争的分析</title>
      <link>https://arxiv.org/abs/2502.15015</link>
      <description><![CDATA[ARXIV：2502.15015V1公告类型：新 
摘要：算法的目标：培训算法竞赛是评估仅通过改善基础培训算法实现的神经网络培训的实际加速度。在外部调整规则集中，提交必须提供工作负载不合时宜的超参数搜索空间，而在自我调整规则集中，它们必须完全不含超级参数。在这两个规则集中，在多个深度学习工作负载的时间表上进行了比较，对固定硬件进行了培训。本文介绍了首届Algoperf竞赛的成果，该竞赛的结果吸引了10个团队的18种不同的意见。我们的调查揭示了几个关键发现：（1）使用分布式洗发水在外部调整规则集中的获胜提交，即使在墙壁通行时间运行时进行了比较，也证明了非违规预处理对亚当等流行方法的有效性。 （2）根据时间表免费ADAMW算法，自我调整规则集中的获胜提交为完全无效的培训算法展示了新的有效性。 （3）最高得分的提交对工作负载的变化令人惊讶。我们还讨论了在确保不同培训算法之间进行公平比较时遇到的工程挑战。这些结果突出了迄今为止的重大进展，也凸显了进一步改进的余地。]]></description>
      <guid>https://arxiv.org/abs/2502.15015</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时蒂尔：通过跨架结构蒸馏使用MLP进行有效的长期时间序列预测</title>
      <link>https://arxiv.org/abs/2502.15016</link>
      <description><![CDATA[ARXIV：2502.15016V1公告类型：新 
摘要：基于变压器和基于CNN的方法在长期序列预测中表现出强大的性能。但是，它们的高计算和存储要求可能会阻碍大规模的部署。为了解决此限制，我们建议使用知识蒸馏（KD）将轻量级MLP与高级体系结构集成。我们的初步研究揭示了不同的模型可以捕获互补模式，尤其是时间域和频域中的多尺度和多周期模式。基于此观察结果，我们介绍了TimeDistill，这是一个跨架构的KD框架，将这些模式从教师模型（例如Transformers，CNNS）转移到MLP。此外，我们提供了理论分析，表明我们的KD方法可以解释为混合数据增强的一种专业形式。 TimeDistill将MLP的性能提高了多达18.6％，超过了八个数据集的教师模型。它的推断速度最多还高达7倍，并且参数少130倍。此外，我们进行了广泛的评估，以突出计时性的多功能性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2502.15016</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用具有增强性的体系结构来解释对抗性攻击和防御</title>
      <link>https://arxiv.org/abs/2502.15017</link>
      <description><![CDATA[ARXIV：2502.15017V1公告类型：新 
摘要：深度学习中的对抗性攻击代表了对机器学习模型的完整性和可靠性的重大威胁。对抗性训练一直是针对这些对抗性攻击的一种流行的防御技术。在这项工作中，我们利用了网络体系结构，即线性封闭式网络（DLGN），该网络具有比常规深层网络体系结构更好的解释功能。使用此体系结构，我们解释了使用PGD对抗训练训练的强大模型，并将其与标准培训进行比较。 DLGN中的功能网络充当特征提取器，使其成为对手可以攻击模型的唯一媒介。我们将DLGN的特征网络与完全连接的层相对于属性，例如超平面的比对，与PCA的超平面关系以及类别之间的子网络重叠，并在强大模型和标准模型之间进行比较。我们还考虑了具有CNN层的这种体系结构，其中我们定性（使用可视化）和稳健模型和标准模型之间的定量对比门控模式。我们发现了类似于PGD-AT和STD-TR模型中主成分的超级成分的超级平面的见解，PGD-AT超级平面与数据点相距较远。我们使用路径活动分析来表明PGD-AT模型在各个类中创建了多样化的非重叠的活动子网，从而阻止了攻击引起的门控重叠。我们的可视化思想显示了PGD-AT和STD-TR模型所学的表示的性质。]]></description>
      <guid>https://arxiv.org/abs/2502.15017</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>地球聚集器：用于地理空间表格数据的有效变压器模型</title>
      <link>https://arxiv.org/abs/2502.15032</link>
      <description><![CDATA[ARXIV：2502.15032V1公告类型：新 
摘要：用深度学习对地理空间表格数据进行建模已成为传统统计和机器学习方法的有前途的替代方法。但是，随着数据集的增长，现有的深度学习模型通常面临与可伸缩性和灵活性有关的挑战。为此，本文介绍了GeoAggregator，这是一种基于专门用于地理空间表格数据建模的变压器体系结构的高效且轻巧的算法。地球分子通过高斯偏见的当地注意力和全球位置意识明确地解释了空间自相关和空间异质性。此外，我们引入了一种新的注意机制，该机制使用笛卡尔产品来管理模型的大小，同时保持强大的表现力。我们使用合成和经验地理空间数据集对空间统计模型，XGBoost和几种最先进的地理空间深度学习方法进行基准测试。结果表明，与几乎所有数据集中的竞争对手相比，地球种子取得了最佳或第二好的表现。地球种子的效率降低了模型尺寸，使其既可扩展又轻量级。此外，消融实验提供了有关高斯偏见和笛卡尔注意机制有效性的见解，为进一步优化地球种子的性能提供了建议。]]></description>
      <guid>https://arxiv.org/abs/2502.15032</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过消失的理想来近似神经网络中的潜伏</title>
      <link>https://arxiv.org/abs/2502.15051</link>
      <description><![CDATA[ARXIV：2502.15051V1公告类型：新 
摘要：深层神经网络通过学习强大的潜在表示，从而与歧管假设保持一致，从而改造了现代机器学习：高维数据位于较低维的流形上。在本文中，我们通过证明理想如何表征深网的潜在流形，建立了多种学习和计算代数之间的联系。为此，我们提出了一种新的神经体系结构，（i）在中间层上截断了一个预处理的网络，（ii）通过消失的理想的多项式发电机近似近似于每个类别，（iii）将所得的潜在空间转化为线性分离的空间通过单个多项式层的特征。所得模型的层明显少于其预处理的基线，同时保持了可比的准确性，达到更高的吞吐量并使用较少的参数。此外，在借助光谱复杂性分析的情况下，我们得出了概括的概括性保证，表明我们的方法原则上可以比标准深网提供更紧密的界限。数值实验证实了所提出方法的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2502.15051</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GIGL：Snapchat的大型图形神经网络</title>
      <link>https://arxiv.org/abs/2502.15054</link>
      <description><![CDATA[ARXIV：2502.15054V1公告类型：新 
摘要：随着图形神经网络（GNNS）的引入，最新的图形机器学习进展（ML）引起了人们对将这些方法扩展应用于业务应用程序的广泛兴趣。 GNNS启用​​了给定的图形结构的模型参数的可区分端到端（E2E），该学习可以优化流行节点，边缘（链接）和图形级任务。尽管新的GNN层和培训策略的研究创新是迅速的，但GNN的工业采用和实用性却大大落后，这是由于大型图形ML问题所带来的独特规模挑战。在这项工作中，我们分享了Snapchat中GNN的培训，推理和利用方法。为此，我们介绍了GIGL（巨大的图形学习），这是一个开源库，旨在使大规模分布式图ML获得研究人员，ML工程师和从业人员的好处。我们在Snapchat内部使用GIGL来管理GNN工作流程的繁重提升，包括从关系DBS，子图抽样，分布式培训，推理和编排的图形数据预处理。 GIGL的设计旨在与Pytorch几何（PYG）等学术界突出的开源GNN建模库干净地接口，同时处理缩放和生产挑战，使内部实践者更容易专注于建模。 GIGL用于多个生产环境中，在过去2年中，在朋友推荐，内容推荐和广告的背景下，在多个业务领域中启动了超过35个发射。这项工作详细介绍了图书馆提供的高级设计和工具，具有行业规模图的不同业务环境中的规模属性，案例研究，以及在大规模使用大型社交数据的Graph ML时学到的几个关键课程。 Gigl在https://github.com/snap-research/gigl上开放。]]></description>
      <guid>https://arxiv.org/abs/2502.15054</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可视化机器学习模型，以增强财务决策和风险管理</title>
      <link>https://arxiv.org/abs/2502.15073</link>
      <description><![CDATA[ARXIV：2502.15073V1公告类型：新 
摘要：这项研究强调了可视化机器学习模型，尤其是对于银行业，以改善高利益财务环境中的预测是多么重要。视觉工具通过提供对算法决策过程的关键见解，可以改善绩效并支持创建创新的财务模型。在金融机器学习框架内，研究使用视觉指导的实验来制作重要的概念，此类风险评估和投资组合分配，更容易理解。该研究还研究了交易策略的差异及其与风险食欲的关系，得出的结论是，投资组合重新平衡的频率与风险承受能力有负相关。通过可视化，找到这些想法的很大程度上是可能的。该研究结束了一种新的局部随机资产称重方法，其中可视化促进了数据提取和验证。这突出了这些方法在促进金融机器学习研究领域的有用性。]]></description>
      <guid>https://arxiv.org/abs/2502.15073</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>更多的键，少量值：自适应KV缓存量化</title>
      <link>https://arxiv.org/abs/2502.15075</link>
      <description><![CDATA[ARXIV：2502.15075V1公告类型：新 
摘要：本文介绍了一个信息感知的量化框架，该框架在大语言模型（LLMS）中自适应压缩键值（KV）缓存。尽管先前的工作强调了推断过程中密钥和价值缓存的独特作用，但我们的系统分析（检查奇异的价值分布，光谱规范和弗罗贝尼乌斯规范）首次揭示了关键矩阵始终表现出较高的规范值和对量化比值矩阵更敏感。此外，我们的理论分析表明，具有较高频谱规范的矩阵更明显地扩大了量化误差。在这些见解的推动下，我们提出了一种混合精确量化策略，即KV-Dadaquant，该策略为键分配了更多的位宽度，并且值较少，因为键矩阵具有较高的规范值。凭借相同的KV位预算，这种方法有效地减轻了跨变压器层的错误传播，同时可节省大量内存。我们在多个LLM（1B--70B）上进行的广泛实验表明，即使在侵略性压缩下，我们的混合精确量化方案也能保持高模型精度。例如，使用4位用于钥匙和2位的价值达到75.2％的精度，而反转分配（键的2位和值的4位）仅产生54.7％的精度。该代码可从https://tinyurl.com/kv-adadaquant获得]]></description>
      <guid>https://arxiv.org/abs/2502.15075</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>上游：均衡的核心核心选择</title>
      <link>https://arxiv.org/abs/2502.15082</link>
      <description><![CDATA[ARXIV：2502.15082V1公告类型：新 
摘要：用户规格或法律框架通常需要从验证的模型（包括大语言模型（LLM））中删除信息。这需要从已经训练的模型中删除或“忘记”一组数据点，该模型通常会在其他数据点上降低其性能。因此，必须在删除信息和保持模型的其他能力完好无损之间达到平衡，而无法平衡这一权衡取舍，从而导致删除差或无法使用的模型。为此，我们提出了Upcore（可提供公用事业的核心选择），这是一种方法不合时宜的数据选择框架，用于减轻未学习过程中的侧支损害。发现模型损害与忘记集合中模型表示的方差相关，我们有选择地修剪忘记集以删除异常值，从而最大程度地减少了模型退化后的降级。我们评估了三种标准未学习方法的上行，始终达到删除功效的竞争目标和模型保存之间的卓越平衡。为了更好地评估这一权衡，我们引入了一个新的指标，测量了标准指标范围内的面积（AUC）。我们发现，上核可以改善标准指标和AUC，从而受益于核心和修剪点之间的积极转移，同时减少了从忘记集合到其以外的点的负转移。]]></description>
      <guid>https://arxiv.org/abs/2502.15082</guid>
      <pubDate>Mon, 24 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>