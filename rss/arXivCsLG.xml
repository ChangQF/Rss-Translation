<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Thu, 15 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>关系函数和注意力机制的近似</title>
      <link>https://arxiv.org/abs/2402.08856</link>
      <description><![CDATA[arXiv:2402.08856v1 公告类型：新
摘要：神经网络特征图的内积作为输入之间关系建模的方法出现在各种机器学习框架中。这项工作研究神经网络内积的近似性质。结果表明，多层感知器与其自身的内积是对称正定关系函数的通用逼近器。在不对称关系函数的情况下，表明两个不同多层感知器的内积是通用逼近器。在这两种情况下，都获得了实现给定近似精度所需的神经元数量的界限。在对称情况下，函数类可以用再生核希尔伯特空间的核来识别，而在非对称情况下，函数类可以用再生核巴拿赫空间的核来识别。最后，将这些近似结果应用于分析 Transformer 底层的注意力机制，表明任何由抽象预序定义的检索机制都可以通过其内积关系由注意力来近似。这个结果利用了经济学中的德布鲁表示定理，用效用函数来表示偏好关系。]]></description>
      <guid>https://arxiv.org/abs/2402.08856</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>立场文件：拓扑深度学习的挑战和机遇</title>
      <link>https://arxiv.org/abs/2402.08871</link>
      <description><![CDATA[arXiv:2402.08871v1 公告类型：新
摘要：拓扑深度学习（TDL）是一个快速发展的领域，它利用拓扑特征来理解和设计深度学习模型。本文认为，TDL 可以通过结合拓扑概念来补充图表示学习和几何深度学习，从而为各种机器学习设置提供自然的选择。为此，本文讨论了 TDL 中的开放问题，从实际效益到理论基础。对于每个问题，它概述了潜在的解决方案和未来的研究机会。同时，本文也邀请科学界积极参与TDL研究，以释放这一新兴领域的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.08871</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:24 GMT</pubDate>
    </item>
    <item>
      <title>通过双阶段扰动测试进行因果解释的必要性和充分性特征归因</title>
      <link>https://arxiv.org/abs/2402.08845</link>
      <description><![CDATA[arXiv:2402.08845v1 公告类型：新
摘要：我们研究了机器学习中的可解释性问题。为了解决这个问题，特征归因方法（FAM）通过扰动测试来衡量每个特征的贡献，其中比较不同扰动下预测的差异。然而，这种扰动测试当扰动后预测变化相同时，可能无法准确区分不同特征的贡献。为了增强 FAM 在这种具有挑战性的环境中区分不同特征贡献的能力，我们建议利用概率（PNS）作为特征重要性的衡量标准，扰动特征是预测发生变化的必要且充分的原因。我们的方法，具有必要性和充分性的特征归因（FANS），通过涉及两个阶段（事实和干预）的扰动测试来计算 PNS。在实践中，为了生成反事实样本，我们对观察到的样本使用基于重采样的方法来近似所需的条件分布。最后，我们结合 FANS 和基于梯度的优化来提取具有最大 PNS 的子集。我们证明 FANS 优于六个基准上的现有特征归因方法。]]></description>
      <guid>https://arxiv.org/abs/2402.08845</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>混合逆强化学习</title>
      <link>https://arxiv.org/abs/2402.08848</link>
      <description><![CDATA[arXiv:2402.08848v1 公告类型：新
摘要：模仿学习的逆强化学习方法是一把双刃剑。一方面，它可以从较少数量的专家演示中进行学习，并且比行为克隆方法对错误复合具有更强的鲁棒性。另一方面，它要求学习者重复解决计算成本高昂的强化学习（RL）问题。通常，大部分计算都浪费在搜索与专家的策略非常不同的策略上。在这项工作中，我们建议使用混合强化学习（在线数据和专家数据的混合训练）来减少不必要的探索。直观上，专家数据使学习者在训练期间专注于良好状态，这减少了计算强大策略所需的探索量。值得注意的是，这种方法不需要将学习器重置为环境中的任意状态的能力，而这是高效逆强化学习先前工作的要求。更正式地说，我们将逆强化学习简化为专家竞争强化学习（而不是全局最优强化学习），这使我们能够大幅减少内部策略搜索循环期间的交互，同时保持 IRL 方法的优势。这使我们能够导出具有强大策略性能保证的无模型和基于模型的混合逆强化学习算法。根据经验，我们发现我们的方法在一系列连续控制任务上比标准逆强化学习和其他几个基线的样本效率要高得多。]]></description>
      <guid>https://arxiv.org/abs/2402.08848</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:23 GMT</pubDate>
    </item>
    <item>
      <title>使用图神经网络消除歧义的节点分类</title>
      <link>https://arxiv.org/abs/2402.08824</link>
      <description><![CDATA[arXiv:2402.08824v1 公告类型：新
摘要：图神经网络（GNN）在从各个领域的图结构数据学习方面取得了巨大的成功。尽管它们取得了巨大的成功，但现有的工作经常忽视一个关键的挑战，即学习可以有效地推广到代表性不足的图区域的消息传播。这些少数地区经常表现出不规则的同质/异质模式和不同的邻域类别分布，从而导致歧义。在这项工作中，我们研究了 GNN 中的歧义问题、其对表示学习的影响，以及开发更丰富的监督信号来解决这个问题。我们对 GNN 进行了细粒度的评估，分析了不同图区域中是否存在歧义及其与节点位置的关系。为了消除节点嵌入的歧义，我们提出了一种新方法，{\method}，它利用额外的优化指导来增强表示学习，特别是对于模糊区域中的节点。 {\method} 基于预测的时间不一致来识别模糊节点，并通过以拓扑感知的方式采用对比学习来引入消歧正则化。 {\method} 提高了节点表示的辨别性，可以减轻消息传播引起的语义混合，有效解决歧义问题。实证结果验证了 {\method} 的效率，并强调了其在代表性不足的图区域中提高 GNN 性能的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.08824</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>考虑N$_2$O排放和不确定性气候变化的智能农业管理</title>
      <link>https://arxiv.org/abs/2402.08832</link>
      <description><![CDATA[arXiv:2402.08832v1 公告类型：新
摘要：本研究探讨了人工智能（AI），特别是强化学习（RL）如何应用于农业，以提高作物产量、微调氮肥使用和浇水、减少硝酸盐径流和温室气体，重点关注一氧化二氮（ N$_2$O) 土壤排放。面对气候变化和有限的农业知识，我们使用部分可观察马尔可夫决策过程（POMDP）和作物模拟器来模拟人工智能代理与农业环境的相互作用。我们应用深度 Q 学习和基于循环神经网络 (RNN) 的 Q 网络来训练代理的最佳动作。此外，我们还开发了机器学习 (ML) 模型来预测 N$_2$O 排放量，并将这些预测集成到模拟器中。我们的研究通过概率机器学习方法解决了 N$_2$O 排放估算的不确定性，并通过随机天气模型解决了气候变化，提供了一系列排放结果，以提高预测可靠性和决策。通过纳入气候变化的影响，我们增强了代理商的气候适应能力，旨在实现有弹性的农业实践。结果表明，这些药剂可以通过惩罚 N$_2$O 排放来使作物生产力与环境问题保持一致，有效地适应气候变化，如气温升高和降雨减少。该战略改善了气候变化下的农场管理，凸显了人工智能在可持续农业中的作用。]]></description>
      <guid>https://arxiv.org/abs/2402.08832</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:22 GMT</pubDate>
    </item>
    <item>
      <title>具有时变约束的无投影在线凸优化</title>
      <link>https://arxiv.org/abs/2402.08799</link>
      <description><![CDATA[arXiv:2402.08799v1 公告类型：新
摘要：我们考虑设置具有对抗性时变约束的在线凸优化，其中动作必须是可行的。一个固定的约束集，并且平均还需要近似满足附加的时变约束。受固定可行集（硬约束）难以投影的场景的启发，我们考虑仅通过线性优化预言机（LOO）访问该集的无投影算法。我们提出了一种算法，在长度为 $T$ 的序列上并使用对 LOO 的总体 $T$ 调用，保证 $\tilde{O}(T^{3/4})$ 后悔。损失和 $O(T^{7/8})$ 约束违规（忽略除 $T$ 之外的所有数量）。特别是，这些界限成立。序列的任意间隔。我们还提出了一种更有效的算法，该算法仅需要一阶预言机访问软约束并实现类似的边界。整个序列。我们将后者扩展到强盗反馈的设置，并在期望中获得类似的界限（作为 $T$ 的函数）。]]></description>
      <guid>https://arxiv.org/abs/2402.08799</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>范数有界无限宽度神经网络中的深度分离</title>
      <link>https://arxiv.org/abs/2402.08808</link>
      <description><![CDATA[arXiv:2402.08808v1 公告类型：新
摘要：我们研究无限宽度神经网络中的深度分离，其中复杂性由权重的总体平方 $\ell_2$-范数（网络中所有权重的平方和）控制。虽然之前的深度分离结果集中在宽度方面的分离，但这样的结果并没有深入了解深度是否决定是否有可能学习一个即使在网络宽度无界时也能很好地泛化的网络。在这里，我们根据可学习性所需的样本复杂性来研究分离。具体来说，我们表明，有些函数可以通过范数控制的深度 3 ReLU 网络在输入维度上以样本复杂度多项式学习，但不能通过范数控制的深度 2 ReLU 网络以次指数样本复杂度学习（使用任何标准值）。我们还表明，相反方向上的类似陈述是不可能的：任何可以通过具有无限宽度的范数控制的深度 2 ReLU 网络以多项式样本复杂性学习的函数也可以通过范数控制的深度 3 以多项式样本复杂性学习ReLU 网络。]]></description>
      <guid>https://arxiv.org/abs/2402.08808</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:21 GMT</pubDate>
    </item>
    <item>
      <title>重新思考大型语言模型的机器遗忘</title>
      <link>https://arxiv.org/abs/2402.08787</link>
      <description><![CDATA[arXiv:2402.08787v1 公告类型：新
摘要：我们在大型语言模型（LLM）领域探索机器取消学习（MU），称为 LLM 取消学习。该举措旨在消除不良数据影响（例如敏感或非法信息）和相关模型功能，同时保持基本知识生成的完整性并且不影响因果无关的信息。我们预计法学硕士的忘却学习将成为法学硕士生命周期管理的关键要素，有可能成为开发生成式人工智能的重要基础，这种人工智能不仅安全可靠、值得信赖，而且无需全面再培训即可实现资源高效利用。我们从概念表述、方法论、指标和应用中探索法学硕士中的遗忘景观。我们特别强调了现有法学硕士遗忘研究中经常被忽视的方面，例如遗忘范围、数据模型交互和多方面的效能评估。我们还建立了 LLM 反学习与模型编辑、影响函数、模型解释、对抗性训练和强化学习等相关领域之间的联系。此外，我们概述了法学硕士遗忘的有效评估框架，并探索其在版权和隐私保护以及减少社会技术危害方面的应用。]]></description>
      <guid>https://arxiv.org/abs/2402.08787</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:20 GMT</pubDate>
    </item>
    <item>
      <title>通过知识增强的生成模型改进分子生成和药物发现</title>
      <link>https://arxiv.org/abs/2402.08790</link>
      <description><![CDATA[arXiv:2402.08790v1 公告类型：新
摘要：生成模型的最新进展为生成分子和新型候选药物建立了最先进的基准。尽管取得了这些成功，生成模​​型与广泛的生物医学知识的利用之间仍然存在巨大差距，这些知识通常在知识图中系统化，其告知和增强生成过程的潜力尚未实现。在本文中，我们提出了一种新颖的方法，通过开发称为 K-DReAM 的知识增强生成模型框架来弥合这一鸿沟。我们开发了一种可扩展的方法来扩展知识图的功能，同时保留语义完整性，并将这些上下文信息合并到生成框架中以指导基于扩散的模型。知识图嵌入与我们的生成模型的集成提供了一种强大的机制，用于生产具有特定特征的新候选药物，同时确保有效性和可合成性。 K-DReAM 在无条件和目标生成任务上均优于最先进的生成模型。]]></description>
      <guid>https://arxiv.org/abs/2402.08790</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:20 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯策略分类</title>
      <link>https://arxiv.org/abs/2402.08758</link>
      <description><![CDATA[arXiv:2402.08758v1 公告类型：新
摘要：在策略分类中，智能体以一定的代价修改其特征，以理想地从学习者的分类器中获得积极的分类。学习者的典型反应是仔细修改他们的分类器，使其对这种策略行为具有鲁棒性。在推理代理操作时，大多数研究策略分类的论文都依赖于以下强有力的假设：代理完全知道学习者部署的分类器的确切参数。在现实世界的预测任务中使用复杂或专有的机器学习技术时，这通常是一个不切实际的假设。
  我们发起了策略分类中学习者部分信息发布的研究。我们放弃了代理完全了解分类器的传统假设。相反，我们认为代理在学习者使用的分类器上具有共同的分布先验。我们模型中的学习器可以向代理揭示有关已部署分类器的真实但不一定完整的信息。学习器的目标是发布有关分类器的足够信息以最大限度地提高准确性。我们展示了这种部分信息发布如何与直觉相反，有利于学习者的准确性，尽管增加了代理的操纵能力。
  我们表明，虽然在一般情况下计算代理的最佳响应很困难，但是当学习者的假设类是线性分类器类时，或者当代理的成本函数满足我们定义的子模块性的自然概念。然后，我们将注意力转向学习器的优化问题，并提供关于学习器应该发布多少有关分类器的信息以最大化其预期准确性的算法问题的正面和负面结果。]]></description>
      <guid>https://arxiv.org/abs/2402.08758</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>FLASH：跨同时异构的联邦学习</title>
      <link>https://arxiv.org/abs/2402.08769</link>
      <description><![CDATA[arXiv:2402.08769v1 公告类型：新
摘要：联邦学习 (FL) 的关键前提是在不同的数据所有者（客户端）集合中训练 ML 模型，而不交换本地数据。迄今为止的首要挑战是客户端异构性，这不仅可能源于数据分布的变化，还可能源于数据质量以及计算/通信延迟。对这些不同且同时存在的异质性来源的综合看法至关重要；例如，低延迟客户端的数据质量可能较差，反之亦然。在这项工作中，我们提出了 FLASH（跨同时异构的联邦学习），这是一种轻量级且灵活的客户端选择算法，通过权衡与客户端相关的统计信息，该算法在广泛的异构源下优于最先进的 FL 框架。数据质量、数据分布和延迟。据我们所知，FLASH 是第一种以统一方式处理所有这些异构性的方法。为此，FLASH 通过上下文多臂老虎机 (CMAB) 对学习动态进行建模，并动态选择最有前途的客户。通过大量的实验，我们证明 FLASH 凭借其统一的方法，相对于最先进的基线实现了实质性且一致的改进——绝对精度高达 10%。重要的是，FLASH 的性能还优于旨在处理高度异构设置的联合聚合方法，甚至在与它们集成时还能获得性能提升。]]></description>
      <guid>https://arxiv.org/abs/2402.08769</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:19 GMT</pubDate>
    </item>
    <item>
      <title>PRDP：扩散模型大规模奖励微调的近端奖励差异预测</title>
      <link>https://arxiv.org/abs/2402.08714</link>
      <description><![CDATA[arXiv:2402.08714v1 公告类型：新
摘要：奖励微调已成为使基础模型与下游目标保持一致的一种有前景的方法。通过使用强化学习 (RL) 来最大化反映人类偏好的奖励，在语言领域取得了显着的成功。然而，在视觉领域，现有的基于强化学习的奖励微调方法因其在大规模训练中的不稳定性而受到限制，导致它们无法泛化到复杂的、看不见的提示。在本文中，我们提出了近端奖励差异预测（PRDP），首次在具有超过 100K 提示的大规模提示数据集上实现了扩散模型的稳定黑盒奖励微调。我们的关键创新是奖励差异预测（RDP）目标，它具有与 RL 目标相同的最优解，同时具有更好的训练稳定性。具体来说，RDP 目标是一个监督回归目标，它要求扩散模型根据去噪轨迹预测生成的图像对的奖励差异。我们从理论上证明，获得完美奖励差异预测的扩散模型正是 RL 目标的最大化者。我们进一步开发了一种具有最近更新的在线算法，以稳定地优化 RDP 目标。在实验中，我们证明 PRDP 可以在小规模训练中与基于强化学习的成熟方法的奖励最大化能力相匹配。此外，通过对来自人类偏好数据集 v2 和 Pick-a-Pic v1 数据集的文本提示进行大规模训练，PRDP 在各种复杂的、看不见的提示上实现了卓越的生成质量，而基于 RL 的方法完全失败。]]></description>
      <guid>https://arxiv.org/abs/2402.08714</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>专家不会作弊：通过预测配对来了解你不知道的东西</title>
      <link>https://arxiv.org/abs/2402.08733</link>
      <description><![CDATA[arXiv:2402.08733v1 公告类型：新
摘要：确定模型 ${\widehat{p}}_{\theta}(Y|X)$ 对它所训练的随机现实世界过程 $p(Y|X)$ 了解多少对于确保它可以避免产生错误或“幻觉”的答案或采取不安全的行动。但这对于生成模型来说很困难，因为概率预测无法区分每次响应的噪声（任意不确定性）和缺乏对过程的了解（认知不确定性），并且当模型拟合不足时，现有的认知不确定性量化技术往往会过于自信。我们提出了一种通用策略，用于教授模型近似 $p(Y|X)$ 并估计 ${\widehat{p}}_{\theta}(Y|X)$ 和 $p( Y|X)$：训练它预测从真实条件分布中得出的一对独立响应，允许它通过观察一个响应同时预测另一个响应来“作弊”，然后测量它作弊的程度。值得注意的是，我们证明善于作弊（即只要能提高你的预测就进行作弊）相当于进行二阶校准，这是普通校准的原则性扩展，使我们能够为 $p(Y| 构建可证明正确的频率论置信区间。 X)$ 并以高概率检测到不正确的响应。我们凭经验证明，我们的方法可以准确估计模型在模糊图像分类、（合成）语言建模和部分可观察的导航任务中不知道的程度，优于现有技术。]]></description>
      <guid>https://arxiv.org/abs/2402.08733</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>BECoTTA：依赖于输入的专家在线混合，以实现持续的测试时间适应</title>
      <link>https://arxiv.org/abs/2402.08712</link>
      <description><![CDATA[arXiv:2402.08712v1 公告类型：新
摘要：需要持续测试时间适应（CTTA）来有效地适应连续的看不见的领域，同时保留以前学到的知识。然而，尽管 CTTA 取得了进展，遗忘适应权衡和效率仍未得到探索。此外，当前的 CTTA 场景仅假设不相交的情况，即使现实世界的域是无缝改变的。为了应对这些挑战，本文提出了 BECoTTA，这是一种依赖输入但高效的 CTTA 框架。我们提出域混合低阶专家（MoDE），它包含两个核心组件：i）域自适应路由，有助于通过多个域路由器有选择地捕获域自适应知识，以及（ii）域专家协同最大化每个领域和专家之间的依赖关系的损失。我们验证了我们的方法优于多种 CTTA 场景，包括不相交和渐进的域狗屎，同时只需要约 98% 的可训练参数。我们还提供了对我们的方法的分析，包括专家的构建、领域自适应专家的效果和可视化。]]></description>
      <guid>https://arxiv.org/abs/2402.08712</guid>
      <pubDate>Thu, 15 Feb 2024 06:17:17 GMT</pubDate>
    </item>
    </channel>
</rss>