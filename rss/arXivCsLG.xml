<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Wed, 21 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>诱导模型匹配：受限模型如何帮助更大的模型</title>
      <link>https://arxiv.org/abs/2402.12513</link>
      <description><![CDATA[arXiv:2402.12513v1 公告类型：新
摘要：我们考虑这样的场景：在训练更大的全功能模型时，可以使用受限特征的非常准确的预测模型。该受限模型可以被认为是“辅助信息”，通过强制限制从辅助详尽数据集或同一数据集导出。受限模型如何对完整模型有用？我们提出了一种通过将完整模型的上下文受限性能与受限模型的性能对齐来将受限模型的知识转移到完整模型的方法。我们将这种方法称为诱导模型匹配 (IMM)，并首先通过使用逻辑回归作为玩具示例来说明其普遍适用性。然后，我们探索 IMM 在语言建模中的使用、最初启发它的应用程序，以及与噪声等技术中限制模型的隐式使用相比，它提供了明确的基础。我们使用 $N$-grams 作为限制模型，在 LSTM 和 Transformer 完整模型上演示了该方法。为了进一步说明该原则的潜力，只要收集受限信息比收集完整信息便宜得多，我们以一个简单的 RL 示例作为结论，其中 POMDP 策略可以通过 IMM 改进学习的 MDP 策略。]]></description>
      <guid>https://arxiv.org/abs/2402.12513</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>迈向跨领域持续学习</title>
      <link>https://arxiv.org/abs/2402.12490</link>
      <description><![CDATA[arXiv:2402.12490v1 公告类型：新
摘要：持续学习是一个过程，涉及训练学习代理顺序掌握一系列任务或课程，而无需重新访问过去的数据。挑战在于如何利用以前获得的知识来有效地学习新任务，同时避免灾难性的遗忘。现有方法主要关注单个领域，限制了它们对特定问题的适用性。
  在这项工作中，我们引入了一种称为跨域持续学习（CDCL）的新颖方法，该方法解决了仅限于单个监督域的局限性。我们的方法在紧凑的卷积网络中结合了任务间和任务内的交叉注意机制。这种集成使模型能够与先前任务的特征保持一致，从而延迟任务之间可能发生的数据漂移，同时在相关域之间执行无监督跨域（UDA）。通过利用特定于任务内的伪标记方法，我们确保标记和未标记样本的准确输入对，从而增强学习过程。为了验证我们的方法，我们对公共 UDA 数据集进行了广泛的实验，展示了其在跨领域持续学习挑战中的积极表现。此外，我们的工作引入了有助于该领域进步的增量想法。
  我们提供代码和模型，以鼓励进一步探索和复制我们的结果：\url{https://github.com/Ivsucram/CDCL}]]></description>
      <guid>https://arxiv.org/abs/2402.12490</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>PARCv2：用于时空动力学建模的物理感知递归卷积神经网络</title>
      <link>https://arxiv.org/abs/2402.12503</link>
      <description><![CDATA[arXiv:2402.12503v1 公告类型：新
摘要：对不稳定、快速瞬态和平流主导的物理问题进行建模是物理感知深度学习（PADL）面临的紧迫挑战。复杂系统的物理过程受大型偏微分方程 (PDE) 系统和具有非线性结构的辅助本构模型以及表现出尖锐梯度和快速变形材料界面的演化状态场的控制。在这里，我们研究了一种通用且可推广的归纳偏置方法，可以模拟一般的非线性场演化问题。我们的研究重点是最近的物理感知循环卷积（PARC），它结合了微分器-积分器架构，可以对通用物理系统的时空动力学进行归纳建模。我们扩展了 PARC 的功能，以模拟不稳定、瞬态和平流主导系统。扩展模型称为 PARCv2，配备微分算子来模拟平流-反应-扩散方程，以及用于稳定、长时间预测的混合积分求解器。 PARCv2 在流体动力学的两个标准基准问题（即伯格斯方程和纳维-斯托克斯方程）上进行了测试，然后应用于含能材料中更复杂的冲击诱发反应问题。我们将 PARCv2 的行为与其他基于物理的模型和学习偏差模型进行比较，并证明其模拟非定常和平流主导动力学状态的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.12503</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>用于极小极大优化的 SDE</title>
      <link>https://arxiv.org/abs/2402.12508</link>
      <description><![CDATA[arXiv:2402.12508v1 公告类型：新
摘要：极小极大优化问题在过去几年中引起了广泛关注，其应用范围从经济学到机器学习。虽然针对此类问题存在先进的优化方法，但在随机场景中表征其动态仍然非常具有挑战性。在本文中，我们开创性地使用随机微分方程 (SDE) 来分析和比较 Minimax 优化器。我们的随机梯度下降-上升、随机外梯度和随机哈密顿梯度下降的 SDE 模型是其算法对应模型的可证明近似值，清楚地展示了超参数、隐式正则化和隐式曲率引起的噪声之间的相互作用。这种观点还允许基于微积分原理的统一和简化的分析策略。最后，我们的方法有助于推导简化设置中动力学的收敛条件和封闭式解决方案，从而进一步深入了解不同优化器的行为。]]></description>
      <guid>https://arxiv.org/abs/2402.12508</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>具有连续自组织映射的神经模拟无任务无监督在线学习</title>
      <link>https://arxiv.org/abs/2402.12465</link>
      <description><![CDATA[arXiv:2402.12465v1 公告类型：新
摘要：能够持续学习的智能系统是一种能够从潜在无限长的模式向量流中处理和提取知识的系统。构建这样一个系统的主要挑战被称为灾难性遗忘——代理，例如基于人工神经网络（ANN）的代理，在从新样本中学习时很难保留以前获得的知识。此外，当输入没有补充任务边界信息时，确保保留先前任务的知识变得更具挑战性。尽管人工神经网络背景下的遗忘已经被广泛研究，但在无监督架构方面研究它的工作仍然少得多，例如古老的自组织映射（SOM），一种经常用于聚类和降维的神经模型。虽然 SOM 的内部机制原则上可以产生稀疏表示来提高内存保留，但我们观察到，当固定大小的 SOM 处理连续数据流时，它会经历概念漂移。鉴于此，我们提出了 SOM 的泛化，即连续 SOM（CSOM），它能够在低内存预算下进行在线无监督学习。我们在 MNIST、Kuzushiji-MNIST 和 Fashion-MNIST 等基准测试中的结果表明，准确性几乎提高了两倍，而 CIFAR-10 在（在线）无监督类增量学习测试中展示了最先进的结果环境。]]></description>
      <guid>https://arxiv.org/abs/2402.12465</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>在深度强化学习中，剪枝后的网络就是好的网络</title>
      <link>https://arxiv.org/abs/2402.12479</link>
      <description><![CDATA[arXiv:2402.12479v1 公告类型：新
摘要：最近的工作表明深度强化学习代理很难有效地使用其网络参数。我们利用先前对稀疏训练技术优势的见解，并证明渐进式剪枝使智能体能够最大限度地提高参数有效性。这使得网络比传统网络产生显着的性能改进，并表现出一种“缩放定律”，仅使用完整网络参数的一小部分。]]></description>
      <guid>https://arxiv.org/abs/2402.12479</guid>
      <pubDate>Wed, 21 Feb 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>EBFT：稀疏法学硕士的有效和块级微调</title>
      <link>https://arxiv.org/abs/2402.12419</link>
      <description><![CDATA[arXiv:2402.12419v1 公告类型：新
摘要：现有的稀疏法学硕士微调方法通常面临资源密集型要求和高昂的再培训成本。此外，许多微调方法通常依赖于近似或启发式优化策略，这可能会导致次优解决方案。为了解决这些问题，我们提出了一个高效、快速的框架，用于基于最小化重建误差来微调稀疏 LLM。我们的方法包括对小数据集进行采样以进行校准，并利用反向传播以逐块为基础迭代优化逐块重建误差，旨在获得最佳解决方案。对各种基准的广泛实验一致证明了我们的方法相对于其他基准的优越性。例如，在 LlamaV1-7B 稀疏度为 70% 的 Wikitext2 数据集上，我们提出的 EBFT 实现了 16.88 的困惑度，超过了最先进的 DSnoT 的 75.14 的困惑度。此外，在结构化稀疏率为 26% 的情况下，EBFT 的困惑度达到 16.27，优于 LoRA（困惑度 16.44）。此外，LlamaV1-7B的EBFT微调过程仅需大约30分钟，整个框架可以在单个16GB GPU上执行。源代码可在 https://github.com/sunggo/EBFT 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.12419</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>表格作为图像？探索法学硕士在表格数据多模态表示方面的优势和局限性</title>
      <link>https://arxiv.org/abs/2402.12424</link>
      <description><![CDATA[arXiv:2402.12424v1 公告类型：新
摘要：在本文中，我们研究了各种法学硕士通过不同的提示策略和数据格式解释表格数据的有效性。我们的分析涵盖了与表格相关的任务（例如问答和事实核查）的六个基准。我们首次引入了法学硕士在基于图像的表格表示方面的表现评估。具体来说，我们比较了五种基于文本的表格表示和三种基于图像的表格表示，展示了表示和提示对法学硕士表现的影响。我们的研究提供了如何有效利用法学硕士来完成与表格相关的任务的见解。]]></description>
      <guid>https://arxiv.org/abs/2402.12424</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>基于车辆分组的高速公路碰撞风险形成与传播分析</title>
      <link>https://arxiv.org/abs/2402.12415</link>
      <description><![CDATA[arXiv:2402.12415v1 公告类型：新
摘要： 以往预测碰撞风险的研究主要是将路段发生碰撞的次数或可能性与该路段的交通参数或几何特征相关联，通常忽略了车辆连续运动以及与附近车辆相互作用的影响。通信技术的进步使得能够从周围车辆收集驾驶信息，从而能够研究基于群体的碰撞风险。本研究基于高分辨率车辆轨迹数据，以车辆群体为分析对象，结合车辆群体和路段特征，探讨风险形成和传播机制。确定了导致碰撞风险的几个关键因素，包括过去的高风险车辆组状态、复杂的车辆行为、大型车辆的高比例、车辆组内频繁变换车道以及特定的道路几何形状。开发了多项逻辑回归模型来分析空间风险传播模式，并根据车辆组内高风险发生的趋势进行分类。结果表明，高风险状态持续时间延长、车队规模增加以及频繁变换车道与不良风险传播模式相关。相反，更顺畅的交通流和较高的初始碰撞风险值与风险消散有关。此外，研究还对不同类型的分类器、预测时间间隔和自适应 TTC 阈值进行了敏感性分析。车辆组风险预测的最高 AUC 值超过 0.93。研究结果为研究人员和从业者理解和预测车辆组安全提供了宝贵的见解，最终改善了联网和自动驾驶车辆的主动交通安全管理和运营。]]></description>
      <guid>https://arxiv.org/abs/2402.12415</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>利用卡车司机跨公司的安全氛围感知来预测卡车运输事故：迁移学习方法</title>
      <link>https://arxiv.org/abs/2402.12417</link>
      <description><![CDATA[arXiv:2402.12417v1 公告类型：新
摘要：人们对使用人工智能 (AI) 支持的安全分析来预测卡车运输行业的事故越来越感兴趣。然而，公司可能面临实际挑战，即没有足够的数据来开发良好的安全分析模型。尽管预训练模型可能为此类公司提供解决方案，但现有的使用迁移学习的安全研究主要集中在计算机视觉和自然语言处理，而不是事故分析。为了填补上述空白，我们提出了一种预训练然后微调的迁移学习方法，帮助任何公司利用其他公司的数据来开发人工智能模型，以更准确地预测事故风险。我们还开发了 SafeNet，这是一种适用于事故预测的分类任务的深度神经网络算法。使用来自七家不同数据大小的货运公司的安全氛围调查数据，我们表明，与仅使用目标公司的数据从头开始训练模型相比，我们提出的方法可以带来更好的模型性能。我们还表明，为了使迁移学习模型有效，应使用来自不同来源的更大数据集来开发预训练模型。因此，卡车运输行业可以考虑汇集来自众多公司的安全分析数据来开发预训练模型并在行业内共享它们，以实现更好的知识和资源转移。上述贡献表明先进安全分析有望使行业更安全、更可持续。]]></description>
      <guid>https://arxiv.org/abs/2402.12417</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>超越统一缩放：探索神经架构中的深度异质性</title>
      <link>https://arxiv.org/abs/2402.12418</link>
      <description><![CDATA[arXiv:2402.12418v1 公告类型：新
摘要：神经网络的传统缩放通常涉及设计一个基础网络并通过一些预定义的缩放因子来增长不同的维度，如宽度、深度等。我们引入了一种利用二阶损失景观信息的自动扩展方法。我们的方法对于跳跃连接是灵活的，这是现代视觉变换器的支柱。我们的训练感知方法联合缩放和训练 Transformer，无需额外的训练迭代。出于并非所有神经元都需要统一深度复杂性的假设，我们的方法包含了深度异质性。对使用 ImageNet100 的 DeiT-S 进行的广泛评估显示，与传统缩放相比，精度提高了 2.5%，参数效率提高了 10%。规模化网络在从头开始训练小规模数据集时表现出卓越的性能。我们为视觉转换器引入了第一个完整的缩放机制，这是迈向高效模型缩放的一步。]]></description>
      <guid>https://arxiv.org/abs/2402.12418</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:58 GMT</pubDate>
    </item>
    <item>
      <title>变废为宝：整改教育部Top-$k$路由器</title>
      <link>https://arxiv.org/abs/2402.12399</link>
      <description><![CDATA[arXiv:2402.12399v1 公告类型：新
摘要：稀疏专家混合（MoE）模型由于其计算效率而在训练大型语言模型中很受欢迎。然而，常用的top-$k$路由机制由于不平衡路由而遭受冗余计算和内存成本的困扰。有些专家是溢出的，超出的代币会被丢弃。虽然一些专家是空缺的，但用零填充，这会对模型性能产生负面影响。为了解决丢弃的标记和填充问题，我们提出了 Rectify-Router，包括 GPU 内校正和填充校正。 GPU 内纠正处理丢弃的令牌，有效地将它们路由到它们所在的 GPU 内的专家，以避免 GPU 间通信。填充纠正通过用具有高路由分数的令牌替换填充令牌来解决填充问题。我们的实验结果表明，GPU 内校正和填充校正分别有效地处理丢弃的标记和填充。此外，它们的组合实现了卓越的性能，比普通 top-1 路由器的精度高出 4.7%。]]></description>
      <guid>https://arxiv.org/abs/2402.12399</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>教师作为宽容的专家：与教师无关的无数据知识蒸馏</title>
      <link>https://arxiv.org/abs/2402.12406</link>
      <description><![CDATA[arXiv:2402.12406v1 公告类型：新
摘要：无数据知识蒸馏（DFKD）旨在在生成器的帮助下，在不使用原始数据的情况下将预训练的知识蒸馏为学生模型。在这种无数据的场景中，由于验证数据的不可用，实现 DFKD 的稳定性能至关重要。不幸的是，本文发现现有的 DFKD 方法对不同的教师模型非常敏感，即使使用训练有素的教师模型，偶尔也会出现灾难性的蒸馏失败。我们的观察是，DFKD 中的生成器并不总是保证使用最小化类先验损失和对抗性损失的现有代表性策略生成精确但多样化的样本。通过我们的实证研究，我们注意到类先验不仅降低了生成样本的多样性，而且不能完全解决依赖于教师模型生成意外低质量样本的问题。在本文中，我们提出了与教师无关的无数据知识蒸馏（TA-DFKD）方法，其目标是无论教师模型如何，都能获得更鲁棒和稳定的性能。我们的基本想法是为教师模型分配一个宽松的专家角色来评估样本，而不是一个严格的监督者，在生成器上强制执行其类优先。具体来说，我们设计了一种样本选择方法，仅采用经过教师模型验证的干净样本，而不对生成不同样本的能力施加限制。通过大量的实验，我们表明我们的方法成功地实现了各种教师模型的鲁棒性和训练稳定性，同时优于现有的 DFKD 方法。]]></description>
      <guid>https://arxiv.org/abs/2402.12406</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>ModelGPT：释放 LLM 生成定制模型的能力</title>
      <link>https://arxiv.org/abs/2402.12408</link>
      <description><![CDATA[arXiv:2402.12408v1 公告类型：新
摘要：大型语言模型（LLM）的快速发展通过自动化日常任务彻底改变了各个领域，标志着朝着实现通用人工智能（AGI）迈出了一步。然而，他们仍然难以满足用户多样化和特定的需求，并简化普通用户对人工智能模型的使用。为此，我们提出了 ModelGPT，这是一种新颖的框架，旨在利用法学硕士的能力，确定和生成专门针对用户提供的数据或任务描述定制的人工智能模型。根据用户需求，ModelGPT 能够提供比以前的范例（例如全参数或 LoRA 微调）最多快 270 倍的定制模型。对 NLP、CV 和表格数据集的综合实验证明了我们的框架在使 AI 模型更易于访问和用户友好方面的有效性。我们的代码可在 https://github.com/IshiKura-a/ModelGPT 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.12408</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:57 GMT</pubDate>
    </item>
    <item>
      <title>主次因素一致性作为领域知识指导在线评估中的幸福计算</title>
      <link>https://arxiv.org/abs/2402.12398</link>
      <description><![CDATA[arXiv:2402.12398v1 公告类型：新
摘要：基于大规模在线网络数据和机器学习方法的幸福计算是一个新兴的研究课题，支撑着从个人成长到社会稳定等一系列问题。许多带有解释的先进机器学习（ML）模型用于计算幸福感在线评估，同时保持结果的高精度。然而，这些模型缺乏领域知识约束，例如幸福因素的主次关系，这限制了计算结果与其发生的正确原因之间的关联。本文试图从实证研究的角度对解释一致性提供新的见解。然后我们研究如何表示和引入领域知识约束以使机器学习模型更值得信赖。我们通过以下方式实现这一目标：（1）证明具有加性因子归因的多个预测模型将具有主次关系一致性的理想属性，以及（2）表明因子与数量的关系可以表示为编码领域知识的重要性分布。因素解释差异受到计算模型之间基于 Kullback-Leibler 散度的损失的惩罚。使用两个在线网络数据集的实验结果表明，存在稳定因子关系的领域知识。利用这些知识不仅可以提高幸福计算的准确性，还可以揭示更重要的幸福因素，从而更好地辅助决策。]]></description>
      <guid>https://arxiv.org/abs/2402.12398</guid>
      <pubDate>Wed, 21 Feb 2024 06:16:56 GMT</pubDate>
    </item>
    </channel>
</rss>