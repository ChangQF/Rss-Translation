<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 27 Aug 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>因果感知时空多图卷积网络，实现准确可靠的交通预测</title>
      <link>https://arxiv.org/abs/2408.13293</link>
      <description><![CDATA[arXiv:2408.13293v1 公告类型：新
摘要：准确可靠的预测对广泛的应用具有深远的影响。在本研究中，我们专注于时空学习问题的一个实例——交通预测——以展示一种为进行准确可靠的预测而开发的先进深度学习模型。尽管交通预测取得了重大进展，但有限的研究同时结合了显性和隐性交通模式来提高预测性能。同时，交通状态的多变性需要以统计原则的方式量化模型预测的不确定性；然而，现有的研究无法证明置信区间在反映其包含基本事实的实际可能性方面的统计有效性。在本文中，我们提出了一个端到端的交通预测框架，该框架利用三个主要组件来生成准确可靠的交通预测：动态因果结构学习，用于从海量交通数据中发现隐含的交通模式，因果感知时空多图卷积网络（CASTMGCN）用于学习时空依赖性，以及共形预测用于不确定性量化。CASTMGCN 融合了几个表征交通网络不同重要方面的图和一个辅助图，用于捕捉外生因素对道路网络的影响。在此基础上，进一步开发了一种针对时空数据的共形预测方法，用于量化不同预测范围内节点式交通预测的不确定性。在两个真实交通数据集上的实验结果表明，所提出的方法在预测精度方面优于几种最先进的模型；此外，它比其他方法生成更有效的预测区域，同时严格满足覆盖范围的统计有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.13293</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索偏差和预测指标以表征机器学习在以公平为中心的公共卫生决策中的公平性：叙述性评论</title>
      <link>https://arxiv.org/abs/2408.13295</link>
      <description><![CDATA[arXiv:2408.13295v1 公告类型：新
摘要：背景：机器学习 (ML) 的快速发展为加强公共卫生研究、监测和决策提供了新的机会。然而，人们对算法偏差（预测人口健康结果中的系统性错误）缺乏全面的了解，而算法偏差是由 ML 在公共卫生中的应用导致的。这篇叙述性评论的目的是探索 ML 产生的偏见类型以及评估这些偏见的定量指标。
方法：我们在 PubMed、MEDLINE、IEEE（电气和电子工程师协会）、ACM（计算机协会）数字图书馆、Science Direct 和 Springer Nature 上进行了搜索。我们使用关键词来识别描述偏见类型的研究和衡量这些偏见的指标，这些研究在 2008 年至 2023 年期间以英文发表在 ML 和公共及人口健康领域。
结果：共有 72 篇文章符合纳入标准。我们的审查确定了通常描述的偏见类型和从公平角度评估这些偏见的定量指标。
结论：此次审查将有助于从公平的角度正式确立机器学习对公共卫生的评估框架。]]></description>
      <guid>https://arxiv.org/abs/2408.13295</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从基础到突破的法学硕士微调终极指南：对技术、研究、最佳实践、应用研究挑战和机遇的详尽回顾</title>
      <link>https://arxiv.org/abs/2408.13296</link>
      <description><![CDATA[arXiv:2408.13296v1 公告类型：新
摘要：本报告研究了大型语言模型 (LLM) 的微调，将理论见解与实际应用相结合。它概述了 LLM 从传统自然语言处理 (NLP) 模型到其在 AI 中的关键作用的历史演变。微调方法的比较，包括监督、无监督和基于指令的方法，突出了它们对不同任务的适用性。该报告介绍了一种结构化的七阶段管道，用于微调 LLM，涵盖数据准备、模型初始化、超参数调整和模型部署。重点放在管理不平衡数据集和优化技术上。探索了低秩自适应 (LoRA) 和半微调等参数高效方法，以平衡计算效率和性能。讨论了内存微调、专家混合 (MoE) 和代理混合 (MoA) 等先进技术，以利用专用网络和多代理协作。该报告还研究了近端策略优化 (PPO) 和直接偏好优化 (DPO) 等新方法，这些方法使 LLM 与人类偏好保持一致，同时通过修剪和路由优化来提高效率。其他部分涵盖验证框架、部署后监控和推理优化，重点关注在分布式和基于云的平台上部署 LLM。报告还讨论了多模态 LLM、音频和语音微调等新兴领域，以及与可扩展性、隐私和问责制相关的挑战。本报告为在不断变化的环境中探索 LLM 微调的研究人员和从业者提供了切实可行的见解。]]></description>
      <guid>https://arxiv.org/abs/2408.13296</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用分段线性空间衰减进行局部观察抽象，用于战斗模拟中的强化学习</title>
      <link>https://arxiv.org/abs/2408.13328</link>
      <description><![CDATA[arXiv:2408.13328v1 公告类型：新
摘要：在战斗模拟领域，由于此类环境的动态和复杂性质，深度强化学习 (RL) 代理的训练和部署仍然面临巨大挑战。不幸的是，随着场景和可用信息的复杂性增加，达到一定性能阈值所需的训练时间不仅会增加，而且通常会呈指数级增长。这种关系强调了复杂性对训练 RL 代理的深远影响。本文介绍了一种新方法，解决了使用 RL 训练人工智能 (AI) 代理的这一限制。由于现实世界的计算限制和已知的 RL 样本效率低下挑战，传统的 RL 方法已被证明在这些高维动态环境中举步维艰。为了克服这些限制，我们提出了一种使用分段线性空间衰减的局部观察抽象方法。该技术简化了状态空间，减少了计算需求，同时仍保留了基本信息，从而提高了动态环境中的 AI 训练效率，其中空间关系通常至关重要。我们的分析表明，这种局部观察方法在场景复杂度不断提高的情况下始终优于更传统的全局观察方法。本文推进了强化学习观察抽象的研究，说明了具有分段线性空间衰减的局部观察如何为动态环境中的大型状态表示挑战提供有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2408.13328</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>掌握数字战争艺术：使用分层强化学习开发用于战争游戏的智能战斗模拟代理</title>
      <link>https://arxiv.org/abs/2408.13333</link>
      <description><![CDATA[arXiv:2408.13333v1 公告类型：新
摘要：在当今快速发展的军事格局中，推进人工智能 (AI) 以支持战争游戏变得至关重要。尽管强化学习 (RL) 显示出开发智能代理的前景，但传统 RL 在处理战斗模拟固有的复杂性方面面临局限性。本论文提出了一种全面的方法，包括有针对性的观察抽象、多模型集成、混合 AI 框架和总体分层强化学习 (HRL) 框架。我们使用分段线性空间衰减的局部观察抽象简化了 RL 问题，提高了计算效率并显示出优于传统全局观察方法的功效。我们的多模型框架结合了各种 AI 方法，优化了性能，同时仍支持使用多样化、专门的个人行为模型。我们的混合 AI 框架将 RL 与脚本代理协同作用，利用 RL 进行高级决策，利用脚本代理执行低级任务，从而增强适应性、可靠性和性能。我们的 HRL 架构和训练框架将复杂问题分解为可管理的子问题，与军事决策结构保持一致。虽然初步测试并未显示性能有所改善，但我们获得了改进未来迭代的见解。这项研究强调了 AI 彻底改变战争游戏的潜力，强调了继续研究该领域的必要性。]]></description>
      <guid>https://arxiv.org/abs/2408.13333</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NeurCAM：通过附加模型进行可解释的神经聚类</title>
      <link>https://arxiv.org/abs/2408.13361</link>
      <description><![CDATA[arXiv:2408.13361v1 公告类型：新
摘要：可解释的聚类算法旨在对相似的数据点进行分组，同时解释所获得的组以支持知识发现和模式识别任务。虽然大多数可解释聚类方法都使用决策树构建聚类，但在需要大树的复杂问题上，树的可解释性往往会下降。在这项工作中，我们引入了神经聚类加法模型 (NeurCAM)，这是一种解决可解释聚类问题的新方法，它利用神经广义加法模型为获得的聚类提供模糊聚类成员资格和加法解释。为了提高我们模型解释的稀疏性，我们引入了选择门，明确限制了所利用的特征和成对交互的数量。此外，我们展示了我们的模型执行文本聚类的能力，该聚类考虑了文本的上下文表示，同时根据单词或双词术语为获得的聚类提供解释。大量实验表明，NeurCAM 在表格数据集上实现了与黑盒方法相当的性能，同时保持了可解释性。此外，在对文本数据进行聚类时，我们的方法明显优于其他可解释的聚类方法。]]></description>
      <guid>https://arxiv.org/abs/2408.13361</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLaVaOLMoBitnet1B：三元 LLM 走向多模式！</title>
      <link>https://arxiv.org/abs/2408.13402</link>
      <description><![CDATA[arXiv:2408.13402v1 公告类型：新
摘要：多模态大型语言模型 (MM-LLM) 在过去一年中取得了重大进展，在各个任务中表现出色。然而，要真正实现人工智能的民主化，模型必须表现出强大的能力，并能够在大多数人可访问的小型计算空间上高效运行。作为这项任务的一部分，我们引入了 LLaVaOLMoBitnet1B - 第一个能够接受图像+文本输入以产生连贯文本响应的三元多模态 LLM。该模型与训练脚本一起完全开源，以鼓励在该领域进一步研究。随附的技术报告重点介绍了训练过程、评估细节、与三元模型相关的挑战以及未来的机会。模型链接：https://huggingface.co/IntelLabs/LlavaOLMoBitnet1B]]></description>
      <guid>https://arxiv.org/abs/2408.13402</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>潜在数据增强的最佳层选择</title>
      <link>https://arxiv.org/abs/2408.13426</link>
      <description><![CDATA[arXiv:2408.13426v1 公告类型：新
摘要：虽然数据增强 (DA) 通常应用于输入数据，但一些研究报告称，将 DA 应用于神经网络中的隐藏层，即特征增强，可以提高性能。然而，在以前的研究中，应用 DA 的层并没有得到仔细考虑，通常被随机和均匀地应用，或者只应用于特定层，留下了任意性的空间。因此，在本研究中，我们研究了在各种实验配置中应用 DA 的合适层的趋势，例如从头开始训练、迁移学习、各种数据集设置和不同的模型。此外，为了自动调整适合 DA 的层，我们提出了自适应层选择 (AdaLASE) 方法，该方法在训练期间根据梯度下降法更新对每个层执行 DA 的比例。在几个图像分类数据集上获得的实验结果表明，提出的 AdaLASE 方法按预期改变了比例，并实现了较高的整体测试准确率。]]></description>
      <guid>https://arxiv.org/abs/2408.13426</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型中的下一个标记预测定律</title>
      <link>https://arxiv.org/abs/2408.13442</link>
      <description><![CDATA[arXiv:2408.13442v1 公告类型：新
摘要：大型语言模型 (LLM) 已广泛应用于各种应用领域，但它们的黑箱性质对理解这些模型如何在内部处理输入数据以进行预测提出了重大挑战。在本文中，我们引入了一个精确的定量定律，该定律控制着通过预训练 LLM 中的中间层学习上下文化标记嵌入以进行下一个标记预测。我们的研究结果表明，从最低层到最高层，每一层都对提高预测准确性做出了同等的贡献——这是在基于 Transformer、RWKV 和 Mamba 等架构构建的各种开源 LLM 中观察到的普遍现象。我们证明该定律提供了新的视角和见解，以指导和指导 LLM 开发和应用的实践，包括模型扩展、预训练任务和信息流。总体而言，我们的定律通过审查其内部数据处理机制，使 LLM 的设计、训练和解释方法更加细粒度。]]></description>
      <guid>https://arxiv.org/abs/2408.13442</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无非循环约束的高效强化 DAG 学习</title>
      <link>https://arxiv.org/abs/2408.13448</link>
      <description><![CDATA[arXiv:2408.13448v1 公告类型：新
摘要：揭示仅凭观察数据所嵌入的因果结构具有重大的科学意义，因为可以从此类结构中受益的知识非常丰富。最近，强化学习 (RL) 已成为传统技术的增强，用于以有向无环图 (DAG) 的形式搜索最可能的因果解释。然而，由于候选者数量众多且无环性约束复杂，有效探索 DAG 空间具有挑战性。在本研究中，我们提出了 REACT（无无环性约束的强化 DAG 学习）——一种由 RL 机制推动的新型因果发现方法，具有有效的 DAG 生成策略。通过 DAG 的新型参数化，允许在单个步骤中直接将实值向量映射到表示有效 DAG 的邻接矩阵，而无需强制执行任何无环性约束，我们能够使用策略梯度方法更有效地导航搜索空间。此外，我们对多种合成和真实数据进行的全面数值评估证实了我们的方法与最先进基线相比的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.13448</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对抗梯度情景记忆进行持续强化学习的数据增强</title>
      <link>https://arxiv.org/abs/2408.13452</link>
      <description><![CDATA[arXiv:2408.13452v1 公告类型：新
摘要：学习的数据效率在强化学习 (RL) 训练过程中起着关键作用，在具有顺序环境的持续 RL 中变得更加重要。在持续 RL 中，学习者与非平稳、顺序任务交互，并且需要在不忘记先前知识的情况下学习新任务。然而，在为持续 RL 实施数据增强方面几乎没有工作。在本文中，我们研究了数据增强对持续 RL 的有效性。具体来说，我们通过 (1) 总结现有的数据增强方法和 (2) 包括一种用于持续 RL 的新增强方法：具有梯度情景记忆的对抗性增强 (Adv-GEM)，为持续 RL 提供基准数据增强。大量实验表明，数据增强（例如随机幅度缩放、状态切换、混合、对抗性增强和 Adv-GEM）可以在机器人控制任务中改善现有连续强化学习算法的平均性能、灾难性遗忘和前向迁移。所有数据增强方法都以插件模块的形式实现，以便轻松集成到连续强化学习方法中。]]></description>
      <guid>https://arxiv.org/abs/2408.13452</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DOPPLER：具有低通滤波器的差分隐私优化器，用于降低隐私噪声</title>
      <link>https://arxiv.org/abs/2408.13460</link>
      <description><![CDATA[arXiv:2408.13460v1 公告类型：新 
摘要：隐私是现代深度学习系统和应用中日益关注的问题。差分隐私 (DP) 训练可防止从训练的机器学习模型中泄露收集的训练数据中的敏感信息。DP 优化器，包括 DP 随机梯度下降 (DPSGD) 及其变体，通过梯度剪裁和 DP 噪声注入将训练过程私有化。然而，在实践中，使用 DPSGD 及其变体训练的 DP 模型通常会遭受严重的模型性能下降。这种性能下降阻碍了 DP 优化在许多关键任务中的应用，例如基础模型预训练。在本文中，我们为 DP 优化器的设计和分析提供了一个新颖的信号处理视角。我们表明，一种称为低通滤波的“频域”操作可用于有效降低 DP 噪声的影响。更具体地说，通过为梯度和差分隐私 (DP) 噪声定义“频域”，我们开发了一个名为 DOPPLER 的新组件。该组件专为 DP 算法设计，其工作原理是有效地放大梯度，同时抑制该频域内的 DP 噪声。因此，它保持了隐私保证并提高了 DP 保护模型的质量。我们的实验表明，在各种模型和数据集上，具有低通滤波器的 DP 优化器在测试准确率上比没有滤波器的优化器高出 3%-10%。理论和实践证据都表明，DOPPLER 可以有效缩小 DP 和非 DP 训练之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2408.13460</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LlamaDuo：LLMOps 流程，实现从服务 LLM 到小规模本地 LLM 的无缝迁移</title>
      <link>https://arxiv.org/abs/2408.13467</link>
      <description><![CDATA[arXiv:2408.13467v1 公告类型：新
摘要：基于云的专有大型语言模型 (LLM) 的广泛采用带来了重大挑战，包括操作依赖性、隐私问题以及持续互联网连接的必要性。在这项工作中，我们引入了一个 LLMOps 管道“LlamaDuo”，用于将知识和能力从面向服务的 LLM 无缝迁移到更小的、本地可管理的模型。此管道对于确保在出现操作故障、严格的隐私政策或离线要求的情况下的服务连续性至关重要。我们的 LlamaDuo 涉及使用后者生成的合成数据集针对服务 LLM 对小型语言模型进行微调。如果微调模型的性能未达到预期，则可以通过进一步微调服务 LLM 创建的其他类似数据来增强它。这个迭代过程保证了较小的模型最终可以在特定的下游任务中匹配甚至超越服务 LLM 的能力，为在受限环境中管理 AI 部署提供了实用且可扩展的解决方案。我们利用前沿的 LLM 进行了广泛的实验，以证明 LlamaDuo 在各种下游任务中的有效性、适应性和可负担性。我们的管道实现可在 https://github.com/deep-diver/llamaduo 上找到。]]></description>
      <guid>https://arxiv.org/abs/2408.13467</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解开生成图表征学习</title>
      <link>https://arxiv.org/abs/2408.13471</link>
      <description><![CDATA[arXiv:2408.13471v1 公告类型：新
摘要：最近，生成图模型在通过自监督方法学习图表示方面表现出了良好的效果。然而，大多数现有的生成图表示学习 (GRL) 方法依赖于整个图中的随机掩蔽，这忽略了学习表示的纠缠。这种疏忽导致不稳健性和缺乏可解释性。此外，解开学习到的表示仍然是一项重大挑战，在 GRL 研究中尚未得到充分探索。基于这些见解，本文介绍了一种自监督学习框架 DiGGR（解缠生成图表示学习）。DiGGR 旨在学习潜在的解缠因子并利用它们来指导图掩码建模，从而增强学习表示的解缠并实现端到端联合学习。针对两种不同的图学习任务在 11 个公共数据集上进行的大量实验表明，DiGGR 的表现始终优于许多以前的自监督方法，验证了所提出方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2408.13471</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MPruner：使用基于 CKA 的互信息剪枝优化神经网络大小</title>
      <link>https://arxiv.org/abs/2408.13482</link>
      <description><![CDATA[arXiv:2408.13482v1 公告类型：新
摘要：确定神经网络的最佳大小至关重要，因为它直接影响运行时性能和内存使用情况。修剪是一种成熟的模型压缩技术，可在数学上保证准确性的同时减小神经网络的大小。然而，许多最近的修剪方法忽略了各个模型组件的全局贡献，因此很难确保修剪后的模型满足所需的数据集和性能要求。为了应对这些挑战，我们开发了一种新的修剪算法 MPruner，该算法通过向量相似性利用互信息。MPruner 利用层聚类和中心核对齐 (CKA) 相似性度量，使我们能够整合来自神经网络的全局信息，从而实现更精确、更高效的分层修剪。我们在各种架构和配置中评估了 MPruner，展示了它的多功能性并提供了实用指南。 MPruner 将 CNN 和基于 Transformer 的模型的参数和内存使用量减少了 50%，而准确度几乎没有损失。]]></description>
      <guid>https://arxiv.org/abs/2408.13482</guid>
      <pubDate>Tue, 27 Aug 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>