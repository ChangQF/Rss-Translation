<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 27 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>连续时间强化学习的随机测量方法</title>
      <link>https://arxiv.org/abs/2409.17200</link>
      <description><![CDATA[arXiv:2409.17200v1 公告类型：新
摘要：我们提出了一种随机测量方法，用于建模探索，即在具有受控扩散和跳跃的连续时间强化学习 (RL) 中执行测量值控制。首先，我们考虑在离散时间网格上对连续时间中的随机控制进行采样的情况，并将得到的随机微分方程 (SDE) 重新表述为由合适的随机测量驱动的方程。这些随机测量的构造利用了布朗运动和泊松随机测度（它们是原始模型动力学中的噪声源）以及在网格上采样以执行控制的附加随机变量。然后，我们证明了当采样网格的网格大小变为零时这些随机测度的极限定理，这导致了由白噪声随机测度和泊松随机测度共同驱动的网格采样极限 SDE。我们还认为，网格采样极限 SDE 可以替代最近的连续时间 RL 文献中的探索性 SDE 和样本 SDE，即它可以应用于探索性控制问题的理论分析和学习算法的推导。]]></description>
      <guid>https://arxiv.org/abs/2409.17200</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Mnemosyne：高效处理数百万上下文长度 LLM 推理请求的并行化策略（无需近似）</title>
      <link>https://arxiv.org/abs/2409.17264</link>
      <description><![CDATA[arXiv:2409.17264v1 公告类型：新
摘要：随着大型语言模型 (LLM) 不断发展以处理越来越长的上下文，为数百万个标记范围内的上下文长度提供推理请求带来了独特的挑战。虽然现有技术对于训练很有效，但它们无法解决推理的独特挑战，例如不同的预填充和解码阶段及其相关的延迟约束 - 如第一个标记的时间 (TTFT) 和标记之间的时间 (TBT)。此外，目前没有允许批处理请求以提高硬件利用率的长上下文推理解决方案。
在本文中，我们提出了三项关键创新，以实现高效的交互式长上下文 LLM 推理，而无需诉诸任何近似值：自适应分块以减少混合批处理中的预填充开销，序列管道并行性 (SPP) 以降低 TTFT，以及 KV 缓存并行性 (KVP) 以最小化 TBT。这些贡献被整合成一个 3D 并行策略，使 Mnemosyne 能够将交互式推理扩展到至少 1000 万个 token 的上下文长度，并通过批处理实现高吞吐量。据我们所知，Mnemosyne 是第一个能够高效支持 1000 万个长上下文推理的平台，同时在 1000 万个及以下的上下文中满足 TBT（30 毫秒）的生产级 SLO。]]></description>
      <guid>https://arxiv.org/abs/2409.17264</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于生物体特异性和密码子最优逆折叠的 CodonMPNN</title>
      <link>https://arxiv.org/abs/2409.17265</link>
      <description><![CDATA[arXiv:2409.17265v1 公告类型：新
摘要：生成以蛋白质结构为条件的蛋白质序列是蛋白质工程的一项重要技术。合成工程蛋白质时，它们通常被翻译成 DNA 并在酵母等生物体中表达。此过程中的一个困难是，由于在宿主生物体中表达蛋白质的密码子序列不理想，表达率可能较低。我们提出了 CodonMPNN，它生成以蛋白质主链结构和生物体标签为条件的密码子序列。如果自然发生的 DNA 序列接近密码子最优性，CodonMPNN 可以学习生成比启发式密码子选择具有更高表达产量的密码子序列，用于生成的氨基酸序列。实验表明，CodonMPNN 保留了以前的逆折叠方法的性能，并且比基线更频繁地恢复野生型密码子。此外，对于相同的蛋白质序列，CodonMPNN 生成高适应度密码子序列的可能性高于低适应度密码子序列。代码可在https://github.com/HannesStark/CodonMPNN获得。]]></description>
      <guid>https://arxiv.org/abs/2409.17265</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模型聚合：最小化经验方差优于最小化经验误差</title>
      <link>https://arxiv.org/abs/2409.17267</link>
      <description><![CDATA[arXiv:2409.17267v1 公告类型：新
摘要：无论是确定性还是随机性，模型都可以被视为旨在近似特定感兴趣量的函数。我们提出了一个数据驱动的框架，将来自不同模型的预测聚合成一个更准确的输出。这种聚合方法利用每个模型的优势来提高整体准确性。它是非侵入式的 - 将模型视为黑盒函数 - 与模型无关，需要最少的假设，并且可以组合来自各种模型的输出，包括来自机器学习和数值求解器的输出。我们认为聚合过程应该是逐点线性的，并提出了两种方法来找到最佳聚合：最小误差聚合（MEA），它可以最小化聚合的预测误差，以及最小方差聚合（MVA），它可以最小化其方差。当模型与目标量之间的相关性完全已知时，MEA 本质上会更准确，但当必须根据数据估计这些相关性时，最小经验方差聚合 (MEVA)（MVA 的经验版本）的表现始终优于最小经验误差聚合 (MEEA)（MEA 的经验对应物）。关键区别在于，MEVA 通过估计模型误差来构建聚合，而 MEEA 将模型视为直接插值感兴趣量的特征。这使得 MEEA 更容易出现过度拟合和泛化不良，在测试期间，聚合的表现可能不如单个模型。我们展示了我们的框架在数据科学和偏微分方程等各种应用领域的多功能性和有效性，展示了它如何成功地将传统求解器与机器学习模型相结合，以提高稳健性和准确性。]]></description>
      <guid>https://arxiv.org/abs/2409.17267</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>记忆网络：迈向完全生物学上可信的学习</title>
      <link>https://arxiv.org/abs/2409.17282</link>
      <description><![CDATA[arXiv:2409.17282v1 公告类型：新
摘要：人工智能领域在实现生物合理性和计算效率方面面临重大挑战，特别是在视觉学习任务中。当前的人工神经网络（例如卷积神经网络）依赖于反向传播和权重共享等技术，这些技术与大脑的自然信息处理方法不一致。为了解决这些问题，我们提出了记忆网络，这是一种受生物学原理启发的模型，它避免了反向传播和卷积，并且一次操作即可完成。这种方法可以实现快速高效的学习，模仿大脑在接触最少数据的情况下快速适应的能力。我们的实验表明，记忆网络实现了高效且生物合理的学习，在 MNIST 等简单数据集上表现出色。然而，需要进一步改进模型以处理更复杂的数据集（例如 CIFAR10），这凸显了开发与生物过程紧密结合的新算法和技术的必要性，同时保持计算效率。]]></description>
      <guid>https://arxiv.org/abs/2409.17282</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>神经网络可塑性和损失锐度</title>
      <link>https://arxiv.org/abs/2409.17300</link>
      <description><![CDATA[arXiv:2409.17300v1 公告类型：新
摘要：近年来，持续学习（一种问题环境可能随时间演变的预测设置）已成为一个越来越受欢迎的研究领域，因为该框架面向复杂、非平稳的目标。学习这样的目标需要可塑性，或者神经网络将其预测调整到不同任务的能力。最近的研究结果表明，新任务的可塑性损失与非平稳 RL 框架中的损失景观锐度高度相关。我们探索了锐度正则化技术的使用，该技术寻找平滑的最小值，并因其在原始预测设置中的泛化能力而受到吹捧，以努力对抗可塑性损失。我们的研究结果表明，这种技术对减少可塑性损失没有显着影响。]]></description>
      <guid>https://arxiv.org/abs/2409.17300</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数据核透视空间中生成模型表示的一致性估计</title>
      <link>https://arxiv.org/abs/2409.17308</link>
      <description><![CDATA[arXiv:2409.17308v1 公告类型：新
摘要：生成模型（例如大型语言模型和文本到图像的扩散模型）在提出查询时会产生相关信息。不同的模型在提出相同的查询时可能会产生不同的信息。随着生成模型格局的发展，开发技术来研究和分析模型行为的差异非常重要。在本文中，我们为一组查询中的生成模型的基于嵌入的表示提供了新颖的理论结果。我们建立了在查询集和模型数量增长的情况下对模型嵌入进行一致估计的充分条件。]]></description>
      <guid>https://arxiv.org/abs/2409.17308</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KIPPS：隐私保护合成数据生成中的知识注入</title>
      <link>https://arxiv.org/abs/2409.17315</link>
      <description><![CDATA[arXiv:2409.17315v1 公告类型：新 
摘要：隐私措施的整合，包括差异隐私技术，确保了合成数据的可证明隐私保证。然而，当生成真实数据时，生成深度学习模型面临着挑战，特别是在网络安全和医疗保健等关键领域。针对连续数据优化的生成模型难以对具有领域约束的离散和非高斯特征进行建模。当训练数据集有限且不多样化时，挑战会增加。在这种情况下，生成模型会创建重复敏感特征的合成数据，这是一种隐私风险。此外，生成模型在理解专业领域中的属性约束方面面临困难。这导致生成不切实际的数据，影响下游准确性。为了解决这些问题，本文提出了一个新模型 KIPPS，将知识图谱中的领域和监管知识注入生成深度学习模型，以增强隐私保护合成数据生成。该新框架通过补充属性值上下文来增强生成模型的训练，并在训练期间强制执行领域约束。这种附加指导增强了模型生成真实且符合领域的合成数据的能力。所提出的模型在现实世界的数据集上进行了评估，特别是在网络安全和医疗保健领域，其中领域约束和规则增加了数据的复杂性。我们的实验根据基准方法评估了模型的隐私弹性和下游准确性，证明了其在解决复杂领域中隐私保护和数据准确性之间的平衡方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2409.17315</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>维度的毒害</title>
      <link>https://arxiv.org/abs/2409.17328</link>
      <description><![CDATA[arXiv:2409.17328v1 公告类型：新
摘要：本文推进了对机器学习模型的大小如何影响其易受毒害性的理解，尽管有最先进的防御措施。给定各向同性的随机诚实特征向量和几何中值（或截断均值）作为稳健梯度聚合规则，我们基本上证明了，也许令人惊讶的是，具有 $D \geq 169 H^2/P^2$ 参数的线性和逻辑回归容易受到毒害者的任意模型操纵，其中 $H$ 和 $P$ 是用于训练的诚实标记和毒害数据点的数量。我们的实验继续揭示在合成数据以及具有随机特征的线性分类器的 MNIST 和 FashionMNIST 数据上增强模型表达能力和增加毒害者攻击面之间的基本权衡。我们还讨论了对基于源的学习和神经网络的潜在影响。]]></description>
      <guid>https://arxiv.org/abs/2409.17328</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>训练 Transformer 进行下一个 token 预测的非渐近收敛</title>
      <link>https://arxiv.org/abs/2409.17335</link>
      <description><![CDATA[arXiv:2409.17335v1 公告类型：新
摘要：Transformer 因其出色的处理顺序数据的能力而在现代机器学习中取得了非凡的成功，尤其是在下一个标记预测 (NTP) 任务中。然而，对其在 NTP 中性能的理论理解有限，现有研究主要集中于渐近性能。本文对由自注意模块和前馈层组成的单层 Transformer 的训练动态进行了细粒度的非渐近分析。我们首先使用基于偏序的数学框架来描述 NTP 训练数据集的基本结构特性。然后，我们设计了一个两阶段训练算法，其中训练前馈层的预处理阶段和训练注意层的主要阶段表现出快速收敛性能。具体而言，这两个层都以亚线性方式收敛到其相应最大边际解的方向。我们还表明交叉熵损失具有线性收敛速度。此外，我们表明经过训练的 Transformer 在数据集移位时表现出非平凡的预测能力，这揭示了 Transformer 出色的泛化性能。我们的分析技术涉及开发注意力梯度的新属性，并进一步深入分析这些属性如何有助于训练过程的收敛。我们的实验进一步验证了我们的理论发现。]]></description>
      <guid>https://arxiv.org/abs/2409.17335</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从马尔可夫决策过程的演示中学习效用</title>
      <link>https://arxiv.org/abs/2409.17355</link>
      <description><![CDATA[arXiv:2409.17355v1 公告类型：新
摘要：我们的目标是从顺序决策问题中的行为演示中提取有用的知识。尽管众所周知，人类通常在随机性的情况下从事风险敏感行为，但大多数逆向强化学习 (IRL) 模型都假设代理是风险中性的。除了引入模型错误指定之外，这些模型不会直接捕捉观察到的代理的风险态度，这在许多应用中至关重要。在本文中，我们提出了一种马尔可夫决策过程 (MDP) 中的新型行为模型，该模型通过效用函数明确表示代理的风险态度。然后，我们将效用学习 (UL) 问题定义为从 MDP 中的演示中推断通过效用函数编码的观察到的代理的风险态度的任务，并分析代理效用的部分可识别性。此外，我们设计了两种在有限数据范围内可证明有效的 UL 算法，并分析了它们的样本复杂性。我们最后通过概念验证实验来实证验证我们的模型和算法。]]></description>
      <guid>https://arxiv.org/abs/2409.17355</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>重新审视 Hessian 向量逆积以计算影响函数</title>
      <link>https://arxiv.org/abs/2409.17357</link>
      <description><![CDATA[arXiv:2409.17357v1 公告类型：新
摘要：影响函数是一种将模型输出归因于训练数据的流行工具。传统方法依赖于计算逆 Hessian-向量积 (iHVP)，但经典求解器“线性时间随机二阶算法”(LiSSA，Agarwal 等人 (2017)) 通常被认为不适用于大型模型，因为计算成本高昂且需要进行超参数调整。我们表明，可以根据 Hessian 的谱特性（特别是其迹和最大特征值）来选择三个超参数——缩放因子、批量大小和步数。通过随机草图评估（Swartworth 和 Woodruff，2023），我们发现批量大小必须足够大才能使 LiSSA 收敛；但是，对于我们考虑的所有模型，要求都很温和。我们通过与近端 Bregman 再训练函数 (PBRF，Bae 等人 (2022)) 进行比较，从经验上证实了我们的发现。最后，我们讨论了逆 Hessian 在计算影响力方面起什么作用。]]></description>
      <guid>https://arxiv.org/abs/2409.17357</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于同时减少和连续重建多高度气候数据的隐式神经表征</title>
      <link>https://arxiv.org/abs/2409.17367</link>
      <description><![CDATA[arXiv:2409.17367v1 公告类型：新
摘要：世界正在转向清洁和可再生能源，例如风能，以减少导致全球变暖的温室气体排放。为了增强风数据的分析和存储，我们引入了一个深度学习框架，旨在同时实现有效的降维和离散观测的多高度风数据的连续表示。该框架由三个关键部分组成：降维、跨模态预测和超分辨率。我们的目标是：（1）提高不同气候条件下的数据分辨率以恢复高分辨率细节；（2）降低数据维度以更有效地存储大型气候数据集；（3）实现在不同高度测量的风数据之间的交叉预测。全面的测试证实，我们的方法在超分辨率质量和压缩效率方面都超越了现有方法。]]></description>
      <guid>https://arxiv.org/abs/2409.17367</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过核心集选择实现数据高效的轨迹预测</title>
      <link>https://arxiv.org/abs/2409.17385</link>
      <description><![CDATA[arXiv:2409.17385v1 公告类型：新
摘要：现代车辆配备了多种信息收集设备，例如传感器和摄像头，不断生成大量原始数据。准确预测邻近车辆的轨迹是理解复杂驾驶环境的重要组成部分。然而，训练轨迹预测模型在两个方面具有挑战性。处理大规模数据需要大量计算。此外，简单-中等驾驶场景往往占据数据集的主导地位，而密集交通等具有挑战性的驾驶场景则代表性不足。例如，在 Argoverse 运动预测数据集中，只有极少数实例具有 $\ge 50$ 个代理，而具有 $10 \thicksim 20$ 个代理的场景则更为常见。在本文中，为了减轻过度代表的驾驶场景中的数据冗余并减少由于复杂驾驶场景的数据稀缺而导致的偏差，我们提出了一种基于核心集选择的新型数据高效训练方法。该方法策略性地选择了一个较小但具有代表性的数据子集，同时平衡了不同场景难度的比例。据我们所知，我们是第一个引入能够有效压缩大规模轨迹数据集的方法，同时实现最先进的压缩率。值得注意的是，即使仅使用 50% 的 Argoverse 数据集，也可以在性能几乎不下降的情况下训练模型。此外，所选的核心集保持了出色的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2409.17385</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>超越冗余：信息感知的无监督多路复用图结构学习</title>
      <link>https://arxiv.org/abs/2409.17386</link>
      <description><![CDATA[arXiv:2409.17386v1 公告类型：新
摘要：无监督多路复用图学习（UMGL）旨在学习各种边缘类型上的节点表示而无需手动标记。然而，现有的研究忽略了一个关键因素：图结构的可靠性。现实世界的数据通常表现出复杂的性质，并包含大量与任务无关的噪声，严重损害了 UMGL 的性能。此外，现有方法主要依靠对比学习来最大化不同图之间的相互信息，将它们限制在多路复用图冗余场景中，并且无法捕获视图唯一的任务相关信息。在本文中，我们专注于一个更现实和更具挑战性的任务：从多个图中无监督地学习一个融合图，该图保留了足够的任务相关信息，同时消除了与任务无关的噪声。具体来说，我们提出的信息感知无监督多路复用图融合框架 (InfoMGF) 使用图结构细化来消除无关噪声，同时最大化视图共享和视图唯一任务相关信息，从而解决非冗余多路复用图的前沿问题。理论分析进一步保证了 InfoMGF 的有效性。针对不同下游任务的各种基线的综合实验证明了其卓越的性能和鲁棒性。令人惊讶的是，我们的无监督方法甚至胜过复杂的监督方法。源代码和数据集可在 https://github.com/zxlearningdeep/InfoMGF 获得。]]></description>
      <guid>https://arxiv.org/abs/2409.17386</guid>
      <pubDate>Fri, 27 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>