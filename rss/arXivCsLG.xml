<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 26 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>采用输入扭曲高斯过程的日前风力发电概率时空建模</title>
      <link>https://arxiv.org/abs/2409.16308</link>
      <description><![CDATA[arXiv:2409.16308v1 公告类型：新
摘要：我们设计了一个高斯过程 (GP) 时空模型来捕捉日前风力发电预测的特征。我们研究了数百个风电场位置的小时级日前预测，主要目的是构建一个跨空间和一天中各个小时的完全概率联合模型。为此，我们设计了一个可分离的时空核，实现了时间和空间输入扭曲，以捕捉风力发电协方差中的非平稳性。我们进行了综合实验来验证我们对空间核的选择，并证明了扭曲在解决非平稳性方面的有效性。本文的后半部分致力于使用代表德克萨斯州 ERCOT 地区风电场的真实、完全校准的数据集进行详细的案例研究。]]></description>
      <guid>https://arxiv.org/abs/2409.16308</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于负荷预测的自动时空天气建模</title>
      <link>https://arxiv.org/abs/2409.16326</link>
      <description><![CDATA[arXiv:2409.16326v1 公告类型：新
摘要：除非成本高昂，否则电力很难储存，因此必须始终保持发电量和负荷之间的平衡。传统上，电力的管理方式是预测需求和间歇性生产（风能、太阳能）以及匹配灵活生产（水力、核能、煤炭和天然气）。因此，准确预测电力负荷和可再生能源生产对于确保电网性能和稳定性至关重要。两者都高度依赖于气象变量（温度、风、阳光）。这些依赖关系很复杂，难以建模。一方面，空间变化的影响并不统一，因为人口、工业以及风能和太阳能发电场在整个领土上的分布并不均匀。另一方面，时间变化可能会对负荷产生延迟影响（由于建筑物的热惯性）。通过获取来自不同气象站的观测数据和来自气象模型的模拟数据，我们相信这两种现象可以一起建模。在当今最先进的负荷预测模型中，天气的时空建模是固定的。在这项工作中，我们旨在利用深度神经网络的自动表示和时空特征提取功能来改进用于负荷预测的时空天气建模。我们将基于深度学习的方法与法国国家负荷的最新方法进行了比较。该方法也可以完全适应预测可再生能源生产。]]></description>
      <guid>https://arxiv.org/abs/2409.16326</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>质量至关重要：评估使用工具的 LLM 的合成数据</title>
      <link>https://arxiv.org/abs/2409.16341</link>
      <description><![CDATA[arXiv:2409.16341v1 公告类型：新
摘要：为外部工具使用而训练大型语言模型 (LLM) 是一个快速发展的领域，最近的研究重点是生成合成数据以解决可用数据短缺的问题。然而，缺乏系统的数据质量检查给正确训练和测试模型带来了麻烦。为此，我们提出了两种方法来评估训练 LLM 以使用外部工具的数据的可靠性。第一种方法使用直观的、人为定义的正确性标准。第二种方法使用模型驱动的评估和上下文评估。我们根据两个流行的基准对数据质量进行了全面的评估，然后进行了外部评估，展示了数据质量对模型性能的影响。我们的结果表明，即使在使用较少量的数据进行训练时，使用高质量数据训练的模型也优于使用未经验证的数据训练的模型。这些发现从经验上支持了评估和确保使用工具的 LLM 的训练数据可靠性的重要性。]]></description>
      <guid>https://arxiv.org/abs/2409.16341</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于块的对比学习和记忆巩固，实现在线无监督持续学习</title>
      <link>https://arxiv.org/abs/2409.16391</link>
      <description><![CDATA[arXiv:2409.16391v1 公告类型：新
摘要：我们专注于一种相对未开发的学习范式，称为“在线无监督持续学习”（O-UCL），其中代理接收非平稳、未标记的数据流并逐步学习识别越来越多的类别。该范式旨在模拟现实世界中的应用，在这些应用中遇到新奇事物是常态，例如探索具有多个未知和随时间变化的实体的地形。与无监督、持续或在线学习中的先前工作不同，O-UCL 将这三个领域结合成一个具有挑战性和现实性的学习范式。在这种情况下，代理经常受到评估，并且必须力求在数据流的任何点（而不是在预先指定的离线任务结束时）保持最佳表示。所提出的方法称为基于 \textbf{P} 的 \textbf{对比学习和 \textbf{M} 记忆 \textbf{C} 整合 (PCMC)，通过识别和聚类补丁级特征来构建对数据的组合理解。这些补丁级特征的嵌入由通过基于补丁的对比学习训练的编码器提取。PCMC 将新数据纳入其分布，同时避免灾难性遗忘，并在“睡眠”期间巩固记忆示例。我们评估了 PCMC 在从 ImageNet 和 Places365 数据集创建的流上的性能。此外，我们探索了 PCMC 算法的各种版本，并将其性能与几种现有方法和简单基线进行比较。]]></description>
      <guid>https://arxiv.org/abs/2409.16391</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>现代 Hopfield 网络与编码神经表征相遇——解决实际问题</title>
      <link>https://arxiv.org/abs/2409.16408</link>
      <description><![CDATA[arXiv:2409.16408v1 公告类型：新
摘要：内容可寻址存储器（例如现代霍普菲尔德网络 (MHN)）已被研究为人类陈述性记忆中自动联想和存储/检索的数学模型，但它们在大规模内容存储中的实际应用面临挑战。其中最主要的是亚稳态的出现，特别是在处理大量高维内容时。本文介绍了霍普菲尔德编码网络 (HEN)，这是一个将编码神经表征集成到 MHN 中的框架，以提高模式可分离性并减少亚稳态。我们表明，HEN 还可用于在图像与自然语言查询的异质关联环境中进行检索，从而消除了需要访问同一域中的部分内容的限制。实验结果表明，亚稳态显著减少，存储容量增加，同时仍能完美回忆大量输入，从而提高了联想记忆网络在实际任务中的实用性。]]></description>
      <guid>https://arxiv.org/abs/2409.16408</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>评估实体匹配中的阻塞偏差</title>
      <link>https://arxiv.org/abs/2409.16410</link>
      <description><![CDATA[arXiv:2409.16410v1 公告类型：新
摘要：实体匹配 (EM) 对于识别不同来源的等效数据实体至关重要，随着数据的增长和异构性，这项任务变得越来越具有挑战性。阻止技术可以降低 EM 的计算复杂性，在使该过程可扩展方面发挥着至关重要的作用。尽管阻止方法取得了进步，但公平性问题（阻止可能会无意中偏向某些人口群体）在很大程度上被忽视了。本研究扩展了传统的阻止指标以纳入公平性，为评估阻止技术中的偏见提供了一个框架。通过实验分析，我们评估了各种阻止方法的有效性和公平性，并深入了解了它们的潜在偏见。我们的研究结果强调了在 EM 中考虑公平性的重要性，特别是在阻止阶段，以确保数据集成任务的公平结果。]]></description>
      <guid>https://arxiv.org/abs/2409.16410</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>所有学习都是（自然的）梯度下降吗？</title>
      <link>https://arxiv.org/abs/2409.16422</link>
      <description><![CDATA[arXiv:2409.16422v1 公告类型：新
摘要：本文表明，一类广泛的有效学习规则（那些在给定时间窗口内改善标量性能度量的规则）可以重写为关于适当定义的损失函数和度量的自然梯度下降。具体而言，我们表明，此类学习规则中的参数更新可以表示为对称正定矩阵（即度量）与损失函数负梯度的乘积。我们还证明这些指标具有规范形式并确定了几个最佳指标，包括实现最小可能条件数的指标。主要结果的证明很简单，仅依赖于初等线性代数和微积分，适用于连续时间、离散时间、随机和高阶学习规则，以及明确依赖于时间的损失函数。]]></description>
      <guid>https://arxiv.org/abs/2409.16422</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从视觉识别中参数高效迁移学习 (PETL) 的统一实证研究中得到的经验教训</title>
      <link>https://arxiv.org/abs/2409.16434</link>
      <description><![CDATA[arXiv:2409.16434v1 公告类型：新
摘要：参数高效迁移学习 (PETL) 最近引起了广泛关注，这是由于预训练模型的规模不断增加，并且需要对其进行微调 (FT) 以获得卓越的下游性能。这种全社区的热情引发了大量新方法。然而，缺乏系统的研究来了解它们的性能和合适的应用场景，导致何时应用 PETL 以及使用哪种方法等问题在很大程度上没有得到解答。在本文中，我们在 Vision Transformers 的背景下对代表性 PETL 方法进行了统一的实证研究。我们系统地调整它们的超参数，以公平地比较它们在下游任务上的准确性。我们的研究不仅提供了有价值的用户指南，而且还揭示了一些新的见解。首先，如果仔细调整，不同的 PETL 方法可以在低样本基准 VTAB-1K 中获得相当相似的准确度。这包括像 FT 这样的简单方法，这些方法的偏差项被报告为较差。其次，尽管准确率相似，但我们发现 PETL 方法会犯不同的错误和高置信度预测，这可能是由于它们不同的归纳偏差。这种不一致性（或互补性）为集成方法提供了机会，我们对此进行了初步尝试。第三，除了常用的低样本任务之外，我们发现 PETL 在多样本情况下也很有用——它使用更少的可学习参数实现了与完整 FT 相当甚至更好的准确率。最后但并非最不重要的是，我们研究了 PETL 保持预训练模型对分布变化（例如 CLIP 主干）的鲁棒性的能力。也许并不奇怪，PETL 方法的表现优于单独的完整 FT。但是，通过权重空间集成，完整 FT 模型可以在下游和分布外性能之间实现更好的平衡，这为 PETL 的未来研究方向提供了参考。]]></description>
      <guid>https://arxiv.org/abs/2409.16434</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用零阶优化技术的通信和节能联邦学习</title>
      <link>https://arxiv.org/abs/2409.16456</link>
      <description><![CDATA[arXiv:2409.16456v1 公告类型：新
摘要：联邦学习 (FL) 是一种流行的机器学习技术，它使多个用户能够协作训练模型，同时保持用户数据隐私。FL 的一个重大挑战是上传方向的通信瓶颈，以及由于模型/梯度的大小增加而导致的设备相应能耗。在本文中，我们通过提出一种零阶 (ZO) 优化方法来解决这个问题，该方法要求每个设备每次迭代上传一个量化的单个标量，而不是整个梯度向量。我们证明了它的理论收敛性，并在非凸设置中找到其收敛速度的上限，并讨论了它在实际场景中的实现。我们的 FL 方法和相应的收敛分析考虑了由于无线错误导致的量化和丢包的影响。与标准的基于梯度的 FL 方法相比，我们还展示了我们的方法在通信开销和能耗方面的优越性。]]></description>
      <guid>https://arxiv.org/abs/2409.16456</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成式人工智能驱动的石油产量预测</title>
      <link>https://arxiv.org/abs/2409.16482</link>
      <description><![CDATA[arXiv:2409.16482v1 公告类型：新
摘要：预测多井油田的石油产量是石油和地热能开采以及能源储存技术中的一个重要问题。石油预测的准确性是经济预测、碳氢化合物储量估计、流体处理设施建设和能源价格波动的关键决定因素。利用生成式人工智能技术，我们对四个多井站点四十年来的石油和水产量的时间序列预测进行建模。我们的目标是有效地模拟不确定性并做出精确的预测，以指导现场规模的决策过程。我们使用一种称为 TimeGrad 的自回归模型和一种名为 Informer 的变压器架构变体，专门用于预测长序列时间序列数据。TimeGrad 和 Informer 的预测与地面真实数据紧密相关。Informer 的整体表现非常突出，与 TimeGrad 相比，它在预测所有站点的石油产量方面表现出更高的效率。]]></description>
      <guid>https://arxiv.org/abs/2409.16482</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Flight：基于 FaaS 的复杂分层联邦学习框架</title>
      <link>https://arxiv.org/abs/2409.16495</link>
      <description><![CDATA[arXiv:2409.16495v1 公告类型：新
摘要：联邦学习 (FL) 是一种分散的机器学习范式，其中模型在分布式设备上进行训练并聚合在中央服务器上。现有的 FL 框架假设简单的两层网络拓扑，其中终端设备直接连接到聚合服务器。虽然这是一个实用的思维模型，但它并没有利用物联网等现实世界分布式系统的固有拓扑。我们提出了 Flight，这是一种新颖的 FL 框架，它支持复杂的分层多层拓扑、异步聚合，并将控制平面与数据平面分离。我们将 Flight 的性能与最先进的 FL 框架 Flower 进行了比较。我们的结果表明，Flight 的规模超越了 Flower，支持多达 2048 个同时设备，并缩短了多个模型的 FL 完成时间。最后，我们表明 Flight 的分层 FL 模型可以将通信开销减少 60% 以上。]]></description>
      <guid>https://arxiv.org/abs/2409.16495</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>从双线性观测中学习线性动力学</title>
      <link>https://arxiv.org/abs/2409.16499</link>
      <description><![CDATA[arXiv:2409.16499v1 公告类型：新
摘要：我们考虑学习具有线性状态转换和双线性观测的部分观测动态系统的实现的问题。在对过程和测量噪声做出非常温和的假设的情况下，我们提供了学习未知动态矩阵（最多相似变换）的有限时间分析。我们的分析涉及具有重尾和依赖数据的回归问题。此外，我们的设计矩阵的每一行都包含当前输入与输入历史的克罗内克积，因此很难保证激励的持久性。我们克服了这些挑战，首先为任意但固定的输入提供数据相关的高概率误差界限。然后，我们为根据简单随机设计选择的输入得出数据独立的误差界限。我们的主要结果提供了从双线性观测的单个有限轨迹学习未知动态矩阵的统计错误率和样本复杂度的上限。]]></description>
      <guid>https://arxiv.org/abs/2409.16499</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于NOMA的边缘智能QoE感知分割推理加速算法</title>
      <link>https://arxiv.org/abs/2409.16537</link>
      <description><![CDATA[arXiv:2409.16537v1 Announce Type: new 
摘要：尽管人工智能已经得到广泛应用并极大地改变了我们的生活，但将大型人工智能模型直接部署在资源有限的边缘设备上并不合适。因此，提出了模型分割推理来提高边缘智能的性能，其中人工智能模型被分成不同的子模型，资源密集型的子模型通过无线方式卸载到边缘服务器，以减少资源需求和推理延迟。然而，以前的工作主要集中在改善和优化系统QoS，忽略了QoE的影响，QoE是除了QoS之外对用户的另一个关键项目。即使QoE已经在EC中得到广泛的学习，考虑到EC中的任务卸载和EI中的分割推理之间的差异，以及EC和EI中仍未解决的QoE的具体问题，这些算法在边缘分割推理场景中无法有效工作。为此，本文提出了一种有效的资源分配算法，用于加速EI中的分裂推理，并实现推理延迟、QoE和资源消耗之间的权衡，简称ERA。具体而言，ERA综合考虑资源消耗、QoE和推理延迟，找到最优的模型分裂策略和资源分配策略。由于最小推理延迟和资源消耗与最大QoE无法同时满足，因此采用基于梯度下降的算法在它们之间找到最优权衡。此外，提出了循环迭代GD方法来降低参数离散化导致的GD算法的复杂度。此外，还研究了所提算法的收敛性、复杂度和逼近误差等性质。实验结果表明，ERA的性能远远优于以前的研究。]]></description>
      <guid>https://arxiv.org/abs/2409.16537</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Monge-Kantorovich 符合 Sobolev 的预算</title>
      <link>https://arxiv.org/abs/2409.16541</link>
      <description><![CDATA[arXiv:2409.16541v1 公告类型：新 
摘要：我们考虑使用测度 $\nu$ 来寻找 $n$ 维概率测度 $\rho$ 的“最佳”近似值的问题，该测度的支持度由 $f : \mathbb{R}^m \to \mathbb{R}^n$ 参数化，其中 $m &lt; n$。我们用 Monge-Kantorovich $p$ 成本（也称为 Wasserstein $p$ 成本）$\mathbb{W}_p^p(\rho, \nu)$ 量化近似的性能，并通过限制 $f$ 的 $W^{k,q}$ Sobolev 范数来限制近似的复杂性，该范数充当“预算”。然后，我们可以将问题重新表述为在 Sobolev 预算约束下最小化函数 $\mathscr{J}_p(f)$。
我们将一般 $k \geq 1$ 视为 Sobolev 可微阶（尽管选择 $q, m$ 来将 $W^{k,q}$ 限制在超临界状态 $k q &gt; m$ 以保证优化器的存在）。该问题与（但不同于）当 $m=1, k = 1$ 时具有长度约束的主曲线和当 $k &gt; 1$ 时具有平滑样条曲线的主曲线密切相关。高阶可微性条件带来了新的方面和挑战。
我们研究了 $\mathscr{J}_p$ 的梯度，它由沿 $f$ 的矢量场给出，我们称之为重心场。我们用它来构造给定 $f$ 的改进，这给出了函数 $\mathscr{J}_p$ 和 Sobolev 预算之间的非平凡（几乎）严格单调关系。我们还提供了一种自然的离散化方案并建立了其一致性。我们使用这个方案来模拟生成学习任务；特别是，我们证明添加像我们这样的约束作为软惩罚可以显着改善训练 GAN 以生成手写数字图像，其性能可与权重衰减相媲美。]]></description>
      <guid>https://arxiv.org/abs/2409.16541</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AlignedKV：通过精确对齐量化减少 KV-Cache 的内存访问</title>
      <link>https://arxiv.org/abs/2409.16546</link>
      <description><![CDATA[arXiv:2409.16546v1 公告类型：新
摘要：模型量化已成为解决 LLM 内存消耗大、推理时间长问题的关键技术。混合精度量化区分重要参数和不重要参数，在众多量化方案中脱颖而出，实现了精度和压缩率之间的平衡。然而，现有的方法只能通过定性分析和人工实验来识别重要参数，而无法定量分析如何确定它们的重要性。我们提出了一个新的标准，即所谓的“精度对齐”，以建立一个定量框架来全面评估混合精度量化中参数的重要性。我们在各种实际场景下对浮点加法的观察表明，两个加数应该具有相同的精度，否则更高精度数字中的信息将被浪费。这样的观察为确定矩阵乘法运算中每个参数的精度提供了一个基本原则。作为将上述发现应用于大型模型推理的第一步，我们开发了一种动态 KV-Cache 量化技术，以有效减少内存访问延迟。与专注于节省内存的现有量化方法不同，这项工作直接旨在通过量化浮点数来加速 LLM 推理。所提出的技术可节省 25% 的内存访问，并在 LLM 解码阶段的注意力计算中将速度提高 1.3 倍，而几乎没有精度损失。]]></description>
      <guid>https://arxiv.org/abs/2409.16546</guid>
      <pubDate>Thu, 26 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>