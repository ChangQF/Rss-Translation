<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Mon, 19 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>当您的离散优化是神经网络的大小时该怎么办？</title>
      <link>https://arxiv.org/abs/2402.10339</link>
      <description><![CDATA[arXiv:2402.10339v1 公告类型：新
摘要：通常，使用神经网络的机器学习应用涉及解决离散优化问题，例如剪枝、基于参数隔离的持续学习和二元网络的训练。尽管如此，这些离散问题本质上是组合问题，也不适合基于梯度的优化。此外，离散环境中使用的经典方法不能很好地扩展到大型神经网络，迫使科学家和经验主义者依赖替代方法。其中，自上而下信息的两个主要不同来源可用于引导模型获得良好的解决方案：（1）从解决方案集外部的点推断梯度信息（2）比较有效解决方案子集的成员之间的评估。我们用连续路径（CP）方法来表示纯粹使用前者，用蒙特卡罗（MC）方法来表示后者，同时也注意到一些混合方法将两者结合起来。这项工作的主要目标是比较这两种方法。为此，我们首先概述这两个类，同时分析地讨论它们的一些缺点。然后，在实验部分，我们比较它们的性能，从较小的微观世界实验开始，这允许对问题变量进行更细粒度的控制，并逐渐转向更大的问题，包括用于图像分类的神经网络回归和神经网络剪枝，其中我们另外还与基于幅度的修剪进行比较。]]></description>
      <guid>https://arxiv.org/abs/2402.10339</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>RLHF 中探索驱动的政策优化：高效数据利用的理论见解</title>
      <link>https://arxiv.org/abs/2402.10342</link>
      <description><![CDATA[arXiv:2402.10342v1 公告类型：新
摘要：基于人类反馈的强化学习（RLHF）在依赖少量人类反馈的同时取得了令人印象深刻的经验成功。然而，这种现象的理论依据有限。此外，尽管基于策略的算法最近取得了经验上的成功，但大多数最近的研究都集中在基于价值的算法上。在这项工作中，我们考虑基于策略优化的 RLHF 算法（PO-RLHF）。该算法基于流行的策略覆盖策略梯度（PC-PG）算法，该算法假设了解奖励函数。在 PO-RLHF 中，不假设奖励函数的知识，算法依赖于基于轨迹的比较反馈来推断奖励函数。我们为 PO-RLHF 提供了低查询复杂性的性能界限，这让我们深入了解为什么少量的人类反馈可能足以通过 RLHF 获得良好的性能。一个关键的新颖之处是我们的轨迹级椭圆势分析技术，用于在使用比较查询而不是奖励观察时推断奖励函数参数。我们提供并分析两种设置的算法：线性和神经函数逼近，分别是 PG-RLHF 和 NN-PG-RLHF。]]></description>
      <guid>https://arxiv.org/abs/2402.10342</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>离散概率推理作为多路径环境中的控制</title>
      <link>https://arxiv.org/abs/2402.10309</link>
      <description><![CDATA[arXiv:2402.10309v1 公告类型：新
摘要：我们将从离散和结构化分布中采样的问题视为顺序决策问题，其目标是找到一种随机策略，以便在此顺序过程结束时按与某些预定义奖励的比例对对象进行采样。虽然我们可以使用最大熵强化学习（MaxEnt RL）来解决某些分布的这个问题，但事实证明，一般来说，在有多种方式生成最优策略的情况下，由最优策略引起的状态分布可能会出现偏差。同一个对象。为了解决这个问题，生成流网络（GFlowNets）学习一种随机策略，通过在整个马尔可夫决策过程（MDP）中近似强制执行流守恒，对对象进行与其奖励成比例的采样。在本文中，我们扩展了最近修正奖励的方法，以保证最优 MaxEnt RL 策略引起的边际分布与原始奖励成正比，无论底层 MDP 的结构如何。我们还证明，GFlowNet 文献中发现的一些流匹配目标实际上相当于具有修正奖励的完善的 MaxEnt RL 算法。最后，我们实证研究了多种 MaxEnt RL 和 GFlowNet 算法在涉及离散分布采样的多个问题上的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.10309</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释的生成对抗性模仿学习</title>
      <link>https://arxiv.org/abs/2402.10310</link>
      <description><![CDATA[arXiv:2402.10310v1 公告类型：新
摘要：模仿学习方法通​​过专家演示在教授自主系统复杂任务方面取得了相当大的成功。然而，这些方法的局限性在于缺乏可解释性，特别是在理解学习代理想要完成的特定任务方面。在本文中，我们提出了一种新颖的模仿学习方法，该方法结合了信号时序逻辑（STL）推理和控制合成，从而能够将任务显式表示为 STL 公式。这种方法不仅提供了对任务的清晰理解，而且还允许通过手动调整 STL 公式来融入人类知识并适应新场景。此外，我们采用生成对抗网络（GAN）启发的训练方法来进行推理和控制策略，有效缩小了专家策略和学习策略之间的差距。通过两个案例研究证明了我们算法的有效性，展示了其实际适用性和适应性。]]></description>
      <guid>https://arxiv.org/abs/2402.10310</guid>
      <pubDate>Mon, 19 Feb 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>具有中介反馈的强盗的信息能力遗憾界限</title>
      <link>https://arxiv.org/abs/2402.10282</link>
      <description><![CDATA[arXiv:2402.10282v1 公告类型：新
摘要：这项工作解决了中介反馈问题，这是一种老虎机游戏，其中决策集由许多策略组成，每个策略都与公共结果空间上的概率分布相关。选择策略后，学习者观察从其分布中采样的结果，并在本轮中承担分配给该结果的损失。我们引入策略集容量作为策略集复杂性的信息论度量。采用经典的 EXP4 算法，我们根据对抗性和随机设置中的策略集容量提供新的后悔边界。对于一系列策略集系列的选择，我们证明了几乎匹配的下限，并随容量进行类似的扩展。我们还考虑了策略分布在轮次之间可能变化的情况，从而通过专家建议问题解决了相关的老虎机问题，我们对此进行了改进。此外，我们证明了一个下限，表明在线性老虎机反馈下，利用策略之间的相似性通常是不可能的。最后，对于全信息变体，我们提供了与策略集的信息半径相关的遗憾界限缩放。]]></description>
      <guid>https://arxiv.org/abs/2402.10282</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>针对一类顺序异常检测模型的后门攻击</title>
      <link>https://arxiv.org/abs/2402.10283</link>
      <description><![CDATA[arXiv:2402.10283v1 公告类型：新
摘要：序列数据的深度异常检测由于广泛的应用场景而受到广泛关注。然而，基于深度学习的模型面临着严重的安全威胁——它们容易受到后门攻击。在本文中，我们通过提出一种新颖的后门攻击策略来探索妥协的深度顺序异常检测模型。攻击方法包括两个主要步骤：触发器生成和后门注入。触发生成是通过从良性正常数据中制作扰动样本来导出难以察觉的触发，其中扰动样本仍然是正常的。后门注入是为有触发器的样本正确注入后门触发器来组成模型。实验结果证明了我们提出的通过在两个完善的一类异常检测模型上注入后门的攻击策略的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.10283</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>使用 KCUSUM 实时自适应采样变点检测算法的评估</title>
      <link>https://arxiv.org/abs/2402.10291</link>
      <description><![CDATA[arXiv:2402.10291v1 公告类型：新
摘要：从科学模拟中检测实时数据流的突变是一项具有挑战性的任务，需要部署准确而高效的算法。识别实时数据流中的变化点涉及对传入观察结果的统计特征偏差进行持续检查，特别是在大容量数据场景中。在突然变化检测和最大限度地减少误报之间保持平衡至关重要。用于此目的的许多现有算法依赖于已知的概率分布，限制了它们的可行性。在本研究中，我们介绍了基于核的累积和（KCUSUM）算法，这是传统累积和（CUSUM）方法的非参数扩展，该算法因其在限制较少的条件下在线变化点检测的有效性而受到关注。 KCUSUM 通过直接将传入样本与参考样本进行比较来分裂自身，并计算基于最大平均差异 (MMD) 非参数框架的统计量。这种方法将 KCUSUM 的相关性扩展到只有参考样本的场景，例如真空中蛋白质的原子轨迹，从而有助于在事先不了解数据基本分布的情况下检测与参考样本的偏差。此外，通过利用 MMD 固有的随机游走结构，我们可以从理论上分析 KCUSUM 在各种用例中的性能，包括预期延迟和误报平均运行时间等指标。最后，我们讨论 NWChem CODAR 和蛋白质折叠数据等科学模拟中的实际用例，证明 KCUSUM 在在线变化点检测方面的实际有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.10291</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>对空越狱的强烈拒绝</title>
      <link>https://arxiv.org/abs/2402.10260</link>
      <description><![CDATA[arXiv:2402.10260v1 公告类型：新
摘要：大型语言模型（LLM）的兴起引起了人们对模型被恶意使用的“越狱”的存在的关注。然而，没有衡量越狱严重程度的标准基准，因此越狱论文的作者必须创建自己的基准。我们表明，这些基准通常包括模糊或无法回答的问题，并使用偏向于高估低质量模型响应的误用潜力的评分标准。一些越狱技术甚至在良性问题上也会降低模型响应的质量，从而使问题变得更糟：我们表明，几种越狱技术大大降低了 GPT-4 在 MMLU 上的零样本性能。越狱还可能使“未经审查”的开源模型更难引发有害反应。我们提出了一个新的基准 StrongREJECT，它通过使用更高质量的问题集和更准确的响应评分算法来更好地区分有效和无效的越狱。我们表明，我们的新评分方案更符合人类对响应质量和整体越狱有效性的判断，特别是对于在现有基准上高估越狱性能的低质量响应。我们在 https://github.com/alexandrasouly/strongreject 发布了我们的代码和数据。]]></description>
      <guid>https://arxiv.org/abs/2402.10260</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:58 GMT</pubDate>
    </item>
    <item>
      <title>SusFL：可持续智能农场的基于能源感知的联合学习监控</title>
      <link>https://arxiv.org/abs/2402.10280</link>
      <description><![CDATA[arXiv:2402.10280v1 公告类型：新
摘要：我们提出了一种新型的基于能量感知联合学习（FL）的系统，即 SusFL，用于可持续智能农业，以解决由于太阳能传感器能量水平波动而导致的健康监测不一致的挑战。该系统为牛等动物配​​备了具有计算能力的太阳能传感器（包括 Raspberry Pi），以根据健康数据训练本地深度学习模型。这些传感器定期更新远程（LoRa）网关，形成无线传感器网络（WSN）来检测乳腺炎等疾病。我们提出的 SusFL 系统采用了机制设计（一种博弈论概念），用于智能客户端选择，以优化监控质量，同时最大限度地减少能源使用。该策略确保系统的可持续性和抵御对抗性攻击的能力，包括数据中毒和隐私威胁，这些攻击可能会破坏 FL 的运行。通过使用实时数据集进行广泛的比较分析，我们证明了基于 FL 的监测系统在预测准确性、运行效率、系统可靠性（即平均故障间隔时间或 MTBF）以及通过该机制实现社会福利最大化方面显着优于现有方法设计师。我们的研究结果验证了我们的系统在智能农场中有效和可持续的动物健康监测的优越性。实验结果表明，SusFL 显着提高了系统性能，包括能耗减少 $10\%$、社会福利增加 $15\%$、平均故障间隔时间 (MTBF) 增加 $34\%$，同时全局模型的预测精度略有提高。]]></description>
      <guid>https://arxiv.org/abs/2402.10280</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:58 GMT</pubDate>
    </item>
    <item>
      <title>一种数据驱动的监督机器学习方法，用于估计全球环境空气污染浓度以及相关的预测区间</title>
      <link>https://arxiv.org/abs/2402.10248</link>
      <description><![CDATA[arXiv:2402.10248v1 公告类型：新
摘要：全球环境空气污染是一项跨界挑战，通常通过依赖空间稀疏且分布不均的监测站数据的干预措施来解决。由于断电等问题，这些站经常会遇到时间数据缺口。作为回应，我们开发了一个可扩展的、数据驱动的、有监督的机器学习框架。该模型旨在估算缺失的时间和空间测量值，从而生成污染物的综合数据集，包括 NO$_2$、O$_3$、PM$_{10}$、PM$_{2.5}$ 和 SO$_2 $。该数据集的细粒度为每小时 0.25$^{\circ}$，并附有每次估计的预测间隔，满足了依赖室外空气污染数据进行下游评估的广泛利益相关者的需求。这使得更详细的研究成为可能。此外，还检查了模型在不同地理位置的性能，为未来监测站的战略布局提供见解和建议，以进一步提高模型的准确性。]]></description>
      <guid>https://arxiv.org/abs/2402.10248</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:57 GMT</pubDate>
    </item>
    <item>
      <title>统计异质性的个性化联邦学习</title>
      <link>https://arxiv.org/abs/2402.10254</link>
      <description><![CDATA[arXiv:2402.10254v1 公告类型：新
摘要：联邦学习（FL）的受欢迎程度正在上升，同时人们对人工智能应用中数据隐私的担忧也日益增加。 FL 促进协作多方模型学习，同时确保数据机密性的保存。然而，由于客户数据分布不同而导致的统计异质性问题带来了一定的挑战，例如个性化不足和收敛缓慢。为了解决上述问题，本文对当前个性化联邦学习（PFL）领域的研究进展进行了简要总结。它概述了 PFL 概念，检查了相关技术，并强调了当前的努力。此外，本文还讨论了与 PFL 相关的潜在进一步研究和障碍。]]></description>
      <guid>https://arxiv.org/abs/2402.10254</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:57 GMT</pubDate>
    </item>
    <item>
      <title>HyperAgent：适用于复杂环境的简单、可扩展、高效且可证明的强化学习框架</title>
      <link>https://arxiv.org/abs/2402.10228</link>
      <description><![CDATA[arXiv:2402.10228v1 公告类型：新
摘要：为了解决资源限制下的复杂任务，强化学习（RL）代理需要简单、高效且可扩展，具有（1）大的状态空间和（2）不断积累的交互数据。我们提出了 HyperAgent，这是一种具有超模型、索引采样方案和增量更新机制的 RL 框架，能够在超越共轭的一般值函数近似下实现计算高效的顺序后验逼近和数据高效的动作选择。 \HyperAgent 的实现很简单，只在 DDQN 的基础上添加了一个模块和一行代码。实际上，HyperAgent 在大规模深度 RL 基准测试中展示了其强大的性能，并在数据和计算方面显着提高了效率。理论上，在实际可扩展的算法中，HyperAgent 是第一个在表格强化学习下实现可证明可扩展的每步计算复杂度以及亚线性遗憾的方法。我们理论分析的核心是序贯后验近似论证，它是通过序贯随机投影的第一个分析工具（约翰逊-林登斯特劳斯引理的非平凡鞅扩展）而成为可能的。这项工作弥合了 RL 的理论和实践领域，为 RL 算法设计建立了新的基准。]]></description>
      <guid>https://arxiv.org/abs/2402.10228</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:56 GMT</pubDate>
    </item>
    <item>
      <title>不稳定火焰演化的时间推进算子参数化学习</title>
      <link>https://arxiv.org/abs/2402.10238</link>
      <description><![CDATA[arXiv:2402.10238v1 公告类型：新
摘要：本研究研究了机器学习的应用，特别是傅里叶神经算子（FNO）和卷积神经网络（CNN），以学习参数偏微分方程（PDE）的时间推进算子。我们的重点是扩展现有的算子学习方法来处理表示偏微分方程参数的附加输入。目标是创建一种统一的学习方法，能够准确预测短期解决方案，并在不同参数条件下提供可靠的长期统计数据，从而促进计算成本的节省并加速工程模拟的开发。我们开发并比较了基于 FNO 和 CNN 的参数学习方法，评估了它们在学习一维偏微分方程的参数相关解时间推进算子以及从纳维-斯托克斯方程直接数值模拟获得的真实火焰锋演化数据方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.10238</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:56 GMT</pubDate>
    </item>
    <item>
      <title>为什么问题的动态观点</title>
      <link>https://arxiv.org/abs/2402.10240</link>
      <description><![CDATA[arXiv:2402.10240v1 公告类型：新
摘要：我们解决随机过程生成的多元时间序列数据中的因果推理。现有的方法很大程度上局限于静态设置，忽略了随时间变化的连续性和发射。相反，我们提出了一种学习范式，可以直接建立时间过程中事件之间的因果关系。我们提出了两个关键引理来计算因果贡献并将其构建为强化学习问题。我们的方法提供了形式和计算工具，用于揭示和量化扩散过程中的因果关系，包含各种重要设置，例如离散时间马尔可夫决策过程。最后，在相当复杂的实验中，通过纯粹的学习，我们的框架揭示并量化了因果联系，否则这些联系似乎难以解释。]]></description>
      <guid>https://arxiv.org/abs/2402.10240</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:56 GMT</pubDate>
    </item>
    <item>
      <title>相关拉格朗日 Schr\"odinger 桥：具有群体水平正则化的学习动态</title>
      <link>https://arxiv.org/abs/2402.10227</link>
      <description><![CDATA[arXiv:2402.10227v1 公告类型：新
摘要：系统动力学的精确建模在包括细胞动力学和流体力学在内的广泛科学领域中具有令人着迷的潜力。当（i）观察仅限于横截面样本（其中个体轨迹无法用于学习），并且（ii）个体粒子的行为是异质的（特别是由于生物多样性而在生物系统中）时，这项任务通常会带来重大挑战。为了解决这些问题，我们引入了一种称为相关拉格朗日薛定格桥（CLSB）的新颖框架，旨在寻求横截面观察之间的进化“桥接”，同时针对最小群体“成本”进行正则化。方法依赖于所有粒子 \textit{同质} 的 \textit{individual} 级正则化器（例如限制个体运动），CLSB 在群体水平上运行，承认异质性本质，从而在实践中产生更通用的建模。为此，我们的贡献包括（1）一类新的群体正则化器，捕获多元关系中的时间变化，并导出易于处理的公式，（2）基于遗传共表达稳定性的三个领域信息实例化，以及（3）群体的整合将正则化器引入数据驱动的生成模型中作为约束优化和数值解，并进一步扩展到条件生成模型。根据经验，我们证明了 CLSB 在单细胞测序数据分析中的优越性，例如模拟细胞随时间的发育和预测细胞反应不同剂量的药物。]]></description>
      <guid>https://arxiv.org/abs/2402.10227</guid>
      <pubDate>Mon, 19 Feb 2024 06:17:55 GMT</pubDate>
    </item>
    </channel>
</rss>