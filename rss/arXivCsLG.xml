<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>医疗保健应用中的图结构数据的自监督学习：全面回顾</title>
      <link>https://arxiv.org/abs/2412.05312</link>
      <description><![CDATA[arXiv:2412.05312v1 公告类型：新
摘要：大量复杂且相互关联的医疗保健数据为改善预测、诊断和治疗提供了大量机会。图形结构数据（包括实体及其关系）非常适合捕获复杂的连接。有效利用这些数据通常需要强大而高效的学习算法，尤其是在处理有限的标记数据时。对于各个领域的下游任务来说，利用自监督学习 (SSL) 作为学习和优化未标记数据的有效表示的范例变得越来越重要。在本文中，我们彻底回顾了专门为医疗保健应用中的图形结构数据设计的 SSL 方法。我们探讨了与医疗保健数据相关的挑战和机遇，并评估了 SSL 技术在现实世界医疗保健应用中的有效性。我们的讨论涵盖了各种医疗保健环境，例如疾病预测、医学图像分析和药物发现。我们严格评估了不同 SSL 方法在这些任务中的表现，强调了它们的优势、局限性和潜在的未来研究方向。最终，这篇评论旨在成为希望将 SSL 用于医疗保健领域图形结构数据的研究人员和从业人员的宝贵资源，为改善这一关键领域的成果和见解铺平道路。据我们所知，这项工作是首次全面回顾将 SSL 应用于医疗保健领域图形数据的文献。]]></description>
      <guid>https://arxiv.org/abs/2412.05312</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络绘制海底各层地图</title>
      <link>https://arxiv.org/abs/2412.05329</link>
      <description><![CDATA[arXiv:2412.05329v1 公告类型：新
摘要：海底层的测绘是石油工业当前面临的挑战。现有的解决方法包括通过地震方法和波反演进行测绘，这些方法复杂且计算成本高昂。引入人工神经网络（特别是 UNet）来预测基于海底反射的地震波的速度模型，有望优化这一过程。在本研究中，验证了两种神经网络架构的速度模型反演，并在稳定性指标（例如损失函数和相似系数）以及预测模型与实际模型之间的差异方面进行了比较。事实上，神经网络被证明是解决这一挑战的有希望的解决方案，其 S{\o}rensen-Dice 系数值超过 70%。]]></description>
      <guid>https://arxiv.org/abs/2412.05329</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>标记化流量模型的闭环监督微调</title>
      <link>https://arxiv.org/abs/2412.05334</link>
      <description><![CDATA[arXiv:2412.05334v1 公告类型：新
摘要：交通模拟旨在学习交通代理的策略，当在闭环中展开时，该策略可以忠实地恢复现实世界中观察到的轨迹的联合分布。受大型语言模型的启发，标记化的多代理策略最近已成为交通模拟的最新技术。然而，它们通常是通过开环行为克隆进行训练的，因此在模拟期间以闭环执行时会受到协变量偏移的影响。在这项工作中，我们提出了 Top-K 中最近 (CAT-K) 推出，这是一种简单而有效的闭环微调策略，可以减轻协变量偏移。CAT-K 微调只需要现有的轨迹数据，而无需强化学习或生成对抗模仿。具体来说，CAT-K 微调使一个 7M 参数的小型标记化交通模拟策略能够超越同一模型系列中的 102M 参数模型，并在提交时登上 Waymo Sim Agent Challenge 排行榜榜首。代码可在 https://github.com/NVlabs/catk 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.05334</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用张量收缩层和变换器生成表格数据</title>
      <link>https://arxiv.org/abs/2412.05390</link>
      <description><![CDATA[arXiv:2412.05390v1 公告类型：新
摘要：表格数据的生成模型最近在深度学习领域引起了广泛关注。其目标是估计数据的底层分布。然而，估计表格数据的底层分布有其独特的挑战。具体来说，这种数据模态由混合类型的特征组成，这使得模型学习它们之间的内部关系并非易事。解决混合问题的一种方法是通过标记化将每个特征嵌入到连续矩阵中，而捕获变量之间内部关系的解决方案是通过变压器架构。在这项工作中，我们实证研究了在表格数据生成中使用嵌入表示的潜力，利用张量收缩层和变压器在变分自动编码器中对表格数据的底层分布进行建模。具体来说，我们比较了四种架构方法：一个基线 VAE 模型、两个分别关注张量收缩层和变压器的变体，以及一个集成两种技术的混合模型。我们针对 OpenML CC18 套件中的多个数据集进行了实证研究，比较了模型的密度估计和机器学习效率指标。从我们的研究结果中得出的主要结论是，在张量收缩层的帮助下利用嵌入表示可以改善密度估计指标，同时在机器学习效率方面保持竞争力。]]></description>
      <guid>https://arxiv.org/abs/2412.05390</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HiVeGen——基于 LLM 的分层 Verilog 生成，适用于可扩展芯片设计</title>
      <link>https://arxiv.org/abs/2412.05393</link>
      <description><![CDATA[arXiv:2412.05393v1 公告类型：新
摘要：大型语言模型 (LLM) 最近在代码生成方面表现出令人印象深刻的能力，有望将其能力扩展到硬件描述语言 (HDL)。然而，LLM 倾向于为硬件设计生成单个 HDL 代码块而不是分层结构，这会导致幻觉，尤其是在领域特定加速器 (DSA) 等复杂设计中。为了解决这个问题，我们提出了 HiVeGen，这是一个基于分层 LLM 的 Verilog 生成框架，它将生成任务分解为 LLM 可管理的分层子模块。HiVeGen 通过将自动设计空间探索 (DSE) 集成到层次感知提示生成中，引入基于权重的检索以增强代码重用，并实现实时人机交互以降低纠错成本，从而进一步利用这种分层结构的优势，显着提高生成的设计质量。]]></description>
      <guid>https://arxiv.org/abs/2412.05393</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机特征组合没有免费午餐</title>
      <link>https://arxiv.org/abs/2412.05418</link>
      <description><![CDATA[arXiv:2412.05418v1 公告类型：新
摘要：给定总模型大小的预算，必须决定是训练单个大型神经网络还是组合许多较小网络的预测。我们研究了随机特征岭回归模型集合的这种权衡。我们证明，当固定数量的可训练参数在 $K$ 个独立训练的模型之间进行分配时，只要岭参数经过最佳调整，$K=1$ 即可实现最佳性能。然后，我们推导出缩放定律，描述回归模型集合的测试风险如何随其总大小而衰减。我们确定了内核和任务特征结构的条件，在这些条件下，集合可以实现近乎最优的缩放定律。通过在 CIFAR-10 上训练深度卷积神经网络集合并在 C4 上训练变换器架构，我们发现，只要将权重衰减和特征学习强度调整到最佳值，单个大型网络的表现就会优于任何具有相同总参数数的网络集合。]]></description>
      <guid>https://arxiv.org/abs/2412.05418</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>KEDformer：知识提取季节性趋势分解用于长期序列预测</title>
      <link>https://arxiv.org/abs/2412.05421</link>
      <description><![CDATA[arXiv:2412.05421v1 公告类型：新
摘要：时间序列预测是能源、金融和气象等领域的一项关键任务，其中准确的长期预测至关重要。虽然基于 Transformer 的模型在捕获时间依赖性方面表现出色，但它们在扩展序列中的应用受到计算效率低下和泛化能力有限的限制。在本研究中，我们提出了 KEDformer，这是一个知识提取驱动的框架，它集成了季节性趋势分解来应对这些挑战。KEDformer 利用知识提取方法，专注于自注意力机制中最具信息量的权重，以减少计算开销。此外，提出的 KEDformer 框架将时间序列分解为季节性和趋势成分。这种分解增强了模型捕捉短期波动和长期模式的能力。在能源、交通和天气领域的五个公共数据集上进行的大量实验证明了 KEDformer 的有效性和竞争力，为长期时间序列预测提供了有效的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2412.05421</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DART-Eval：针对监管 DNA 的综合 DNA 语言模型评估基准</title>
      <link>https://arxiv.org/abs/2412.05430</link>
      <description><![CDATA[arXiv:2412.05430v1 公告类型：新
摘要：自然语言、视觉和蛋白质序列的自监督模型的最新进展激发了大型基因组 DNA 语言模型 (DNALM) 的发展。这些模型旨在学习各种 DNA 元素的可泛化表示，从而可能实现各种基因组预测、解释和设计任务。尽管它们具有潜力，但现有的基准测试并不能充分评估 DNALM 在涉及对调节基因活动至关重要的一类重要非编码 DNA 元素的关键下游应用上的能力。在本研究中，我们引入了 DART-Eval，这是一套专门针对调节 DNA 的代表性基准测试，用于评估模型在零样本、探测和微调场景中的表现，以当代从头算模型为基线。我们的基准测试针对具有生物学意义的下游任务，例如功能序列特征发现、预测细胞类型特异性调节活动以及对遗传变异影响的反事实预测。我们发现，目前的 DNALM 表现出不一致的性能，并且在大多数任务中并没有提供比其他基线模型更显著的收益，同时需要更多的计算资源。我们讨论了下一代 DNALM 的潜在有前景的建模、数据管理和评估策略。我们的代码可在 https://github.com/kundajelab/DART-Eval 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.05430</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>颗粒球K类孪生支持向量分类器</title>
      <link>https://arxiv.org/abs/2412.05438</link>
      <description><![CDATA[arXiv:2412.05438v1 公告类型：新
摘要：本文介绍了粒球 K 类双支持向量分类器 (GB-TWKSVC)，这是一种将双支持向量机 (TWSVM) 与粒球计算相结合的新型多类分类框架。所提出的方法通过利用粒球表示来提高噪声鲁棒性，解决了多类分类中的关键挑战，而 TWSVM 的非并行超平面架构解决了两个较小的二次规划问题，提高了效率。我们的方法引入了一种可以有效处理多类场景的新公式，从而改进了传统的二元分类方法。对各种基准数据集的实验评估表明，GB-TWKSVC 在准确性和计算性能方面都明显优于当前最先进的分类器。通过全面的统计测试和复杂性分析验证了该方法的有效性。我们的工作通过提供一个数学上合理的框架来改进分类算法，该框架可以满足现代机器学习应用程序的可扩展性和鲁棒性需求。结果表明，GB-TWKSVC 在模式识别、故障诊断和大规模数据分析等领域具有广泛的适用性，成为分类算法领域的宝贵补充。]]></description>
      <guid>https://arxiv.org/abs/2412.05438</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于图形的对话式人工智能驱动个人记忆捕获和检索方法在实际应用中的应用</title>
      <link>https://arxiv.org/abs/2412.05447</link>
      <description><![CDATA[arXiv:2412.05447v1 公告类型：新
摘要：TOBU 是一款新颖的移动应用程序，它以用户参与的 AI 引导对话方式捕获和检索“个人记忆”（图片/视频以及围绕这些时刻的故事和背景）。我们的初始原型表明，现有的检索技术（例如检索增强生成 (RAG) 系统）由于在理解记忆关系方面的局限性而存在不足，导致回忆率低、幻觉和不令人满意的用户体验。我们设计了一种新颖的基于图的检索方法 TOBUGraph。在捕获过程中，TOBUGraph 利用大型语言模型 (LLM) 自动创建记忆的动态知识图，建立这些记忆的上下文和关系。在检索过程中，TOBUGraph 将 LLM 与记忆图相结合，通过图遍历实现全面回忆。我们使用真实用户数据进行的评估表明，TOBUGraph 在准确率和召回率方面均优于多种 RAG 实现，通过提高检索准确性和减少幻觉显著改善了用户体验。]]></description>
      <guid>https://arxiv.org/abs/2412.05447</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于优化合成数据训练的多臂老虎机方法</title>
      <link>https://arxiv.org/abs/2412.05466</link>
      <description><![CDATA[arXiv:2412.05466v1 公告类型：新
摘要：监督机器学习方法需要大规模训练数据集才能在实践中表现良好。合成数据最近取得了长足的进步，并已被用作真实数据的补充。然而，人们仍然迫切需要评估合成数据的可用性。为此，我们提出了一种基于 UCB 的新型训练程序，并结合了动态可用性指标。我们提出的指标整合了来自合成图像及其相应的真实和合成数据集的低级和高级信息，超越了现有的传统指标。通过利用基于 UCB 的动态方法，可确保模型学习的持续增强。与其他方法不同，我们的方法可以有效地适应机器学习模型状态的变化，并考虑训练过程中训练样本的不断演变的效用。我们表明，我们的指标是一种根据可用性对合成图像进行排名的有效方法。此外，我们提出了一种新的属性感知强盗管道，用于通过将大型语言模型与稳定扩散相结合来生成合成数据。定量结果表明，我们的方法可以提高各种监督分类器的性能。值得注意的是，与传统方法相比，我们观察到分类准确率提高了 10%，证明了我们方法的有效性。我们的源代码、数据集和其他材料可在 https://github.com/A-Kerim/Synthetic-Data-Usability-2024 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2412.05466</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于 Web 代理研究的 BrowserGym 生态系统</title>
      <link>https://arxiv.org/abs/2412.05467</link>
      <description><![CDATA[arXiv:2412.05467v1 公告类型：新
摘要：BrowserGym 生态系统满足了对 Web 代理进行高效评估和基准测试日益增长的需求，特别是那些利用自动化和大型语言模型 (LLM) 进行 Web 交互任务的代理。许多现有的基准测试都存在碎片化和评估方法不一致的问题，因此很难实现可靠的比较和可重复的结果。BrowserGym 旨在通过提供一个统一的、类似健身房的环境来解决这一问题，该环境具有明确的观察和行动空间，有助于跨不同基准测试进行标准化评估。结合 AgentLab（一种有助于代理创建、测试和分析的补充框架），BrowserGym 可以灵活地集成新的基准测试，同时确保一致的评估和全面的实验管理。这种标准化方法旨在减少开发 Web 代理的时间和复杂性，支持更可靠的比较并促进对代理行为的深入分析，并可能产生更具适应性、更有能力的代理，最终加速 LLM 驱动的自动化创新。作为支持证据，我们进行了首次大规模、多基准 Web 代理实验，并比较了 6 个最先进的 LLM 在 BrowserGym 中目前所有基准上的性能。除其他发现外，我们的结果还凸显了 OpenAI 和 Anthropic 最新模型之间的巨大差异，Claude-3.5-Sonnet 在几乎所有基准上都处于领先地位，但在视觉相关任务上，GPT-4o 表现更佳。尽管取得了这些进步，但我们的结果强调，由于现实世界 Web 环境固有的复杂性和当前模型的局限性，构建强大而高效的 Web 代理仍然是一项重大挑战。]]></description>
      <guid>https://arxiv.org/abs/2412.05467</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过超容量最大化实现大型语言模型的多目标对齐</title>
      <link>https://arxiv.org/abs/2412.05469</link>
      <description><![CDATA[arXiv:2412.05469v1 公告类型：新
摘要：大型语言模型 (LLM) 中的人类反馈多目标对齐 (MOAHF) 是一个具有挑战性的问题，因为人类的偏好复杂、多面且经常相互冲突。最近关于 MOAHF 的研究考虑了先验多目标优化 (MOO)，其中人类偏好在训练或推理时已知。相反，当人类偏好未知或难以量化时，一种自然的方法是通过多种不同的解决方案覆盖帕累托前沿。我们提出了一种算法 HaM，用于学习最大化其超容量的多样化 LLM 策略。这是后验 MOO 首次应用于 MOAHF。HaM 在计算和空间上都具有较高的效率，并且在各种数据集上在无害、乐于助人、幽默、忠诚和幻觉等目标方面具有经验优势。]]></description>
      <guid>https://arxiv.org/abs/2412.05469</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的海洋数字孪生：利用深度集成进行实时波高预测的可靠不确定性量化</title>
      <link>https://arxiv.org/abs/2412.05475</link>
      <description><![CDATA[arXiv:2412.05475v1 公告类型：新
摘要：环境污染和化石燃料的枯竭促使人们需要基于可再生能源的环保发电方法。然而，由于能量密度低和非平稳性，可再生能源在提供稳定电力方面往往面临挑战。波浪能转换器 (WEC) 尤其需要可靠的实时波高预测来解决由不规则波浪模式引起的这些问题，这些问题可能导致 WEC 运行效率低下和不稳定。在本研究中，我们提出了一种由人工智能驱动的可靠实时波高预测模型，旨在实现高预测精度和可靠的不确定性量化 (UQ)。所提出的架构 LSTM-DE 将用于时间预测的长短期记忆 (LSTM) 网络与用于稳健 UQ 的深度集成 (DE) 相结合，实现了波高预测的准确性和可靠性。为了进一步提高预测模型的可靠性，应用了不确定性校准，事实证明这可以显着提高量化不确定性的质量。基于从韩国济州岛的振荡水柱-波浪能转换器 (OWC-WEC) 系统获得的实际运行数据，我们证明了所提出的 LSTM-DE 模型架构实现了显著的预测精度 (R2 &gt; 0.9)，同时通过简单的校准技术将不确定性质量提高了 50% 以上。此外，我们还进行了全面的参数研究，以探索关键模型超参数的影响，为波长、振幅和周期不同的各种运行场景提供有价值的指导。研究结果表明，所提出的方法可以提供稳健可靠的实时波高预测，从而促进海洋数字孪生。]]></description>
      <guid>https://arxiv.org/abs/2412.05475</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Flex Attention：一种用于生成优化注意力核的编程模型</title>
      <link>https://arxiv.org/abs/2412.05496</link>
      <description><![CDATA[arXiv:2412.05496v1 公告类型：新
摘要：在过去的 7 年里，注意力机制已成为深度学习中最重要的原语之一。优化注意力机制的主要方法是 FlashAttention，它将操作融合在一起，大大改善了运行时间和内存消耗。然而，FlashAttention 的重要性与其单片特性相结合，给那些试图尝试新注意力机制变体的研究人员带来了一个问题——“软件彩票”。这个问题因编写高效的融合注意力内核的难度而加剧，抵制了传统的基于编译器的方法。我们引入了 FlexAttention，这是一种新颖的编译器驱动编程模型，允许在几行惯用的 PyTorch 代码中实现大多数注意力机制变体。我们证明许多现有的注意力机制变体（例如 Alibi、Document Masking、PagedAttention 等）都可以通过 FlexAttention 实现，并且与这些手写内核相比，我们实现了具有竞争力的性能。最后，我们演示了 FlexAttention 如何轻松组合注意力变体，解决注意力变体的组合爆炸问题。]]></description>
      <guid>https://arxiv.org/abs/2412.05496</guid>
      <pubDate>Tue, 10 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>