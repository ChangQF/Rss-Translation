<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 09 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>FedDW：通过异构联邦学习中的一致性优化提取权重</title>
      <link>https://arxiv.org/abs/2412.04521</link>
      <description><![CDATA[arXiv:2412.04521v1 公告类型：新
摘要：联邦学习 (FL) 是一种创新的分布式机器学习范式，它能够在不集中数据的情况下跨设备进行神经网络训练。虽然这解决了信息共享和数据隐私的问题，但客户端之间的数据异构性和网络规模的增加带来了挑战，从而影响了模型性能和训练效率。先前的研究表明，在 IID 环境中，模型的参数结构应遵循某些特定的一致性原则。因此，识别和规范这些一致性可以缓解异构数据的问题。我们发现，知识蒸馏得出的软标签和分类器头参数矩阵在乘以它们自己的转置时，都捕获了数据类之间的内在关系。这些共享关系表明了固有的一致性。因此，本文的工作确定了两者之间的一致性并利用它来规范训练，为我们提出的 FedDW 框架奠定了基础。实验结果表明，FedDW 的表现优于 10 种最先进的 FL 方法，在高度异构的环境中平均将准确率提高了 3%。此外，我们还提供了理论证明，证明 FedDW 效率更高，而反向传播带来的额外计算负荷可以忽略不计。代码可在 https://github.com/liuvvvvv1/FedDW 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.04521</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用多模态蛋白质表征预测蛋白质熔化温度</title>
      <link>https://arxiv.org/abs/2412.04526</link>
      <description><![CDATA[arXiv:2412.04526v1 公告类型：新
摘要：准确预测蛋白质熔化温度变化（Delta Tm）对于评估蛋白质稳定性和指导蛋白质工程至关重要。利用多模态蛋白质表征在捕捉蛋白质序列、结构和功能之间的复杂关系方面显示出巨大的前景。在本研究中，我们基于强大的蛋白质语言模型（包括 ESM-2、ESM-3、SaProt 和 AlphaFold）开发模型，使用各种特征提取方法来提高预测准确性。通过利用 ESM-3 模型，我们在 s571 测试数据集上实现了新的最先进性能，获得了 0.50 的皮尔逊相关系数 (PCC)。此外，我们进行了公平评估，以比较不同蛋白质语言模型在 Delta Tm 预测任务中的表现。我们的结果表明，整合多模态蛋白质表征可以促进蛋白质熔化温度的预测。]]></description>
      <guid>https://arxiv.org/abs/2412.04526</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>WinTSR：用于解释时间序列深度学习模型的窗口时间显著性重缩放方法</title>
      <link>https://arxiv.org/abs/2412.04532</link>
      <description><![CDATA[arXiv:2412.04532v1 公告类型：新
摘要：由于时间步骤之间的时间依赖性和输入特征随时间变化的动态相关性，解释复杂的时间序列预测模型具有挑战性。现有的解释方法主要侧重于分类任务，使用自定义基线模型而不是最新的时间序列模型进行评估，使用简单的合成数据集，并且需要训练另一个模型。我们引入了一种称为窗口时间显着性重新缩放（WinTSR）的新型解释方法来解决这些限制。WinTSR 明确捕获过去时间步骤之间的时间依赖性，并利用此时间重要性有效地扩展特征重要性。我们使用 5 种不同架构的最先进的深度学习模型（包括时间序列基础模型）将 WinTSR 与 10 种最新解释技术进行基准测试。我们使用 3 个真实世界数据集进行时间序列分类和回归。我们的综合分析表明，WinTSR 在整体性能上明显优于其他局部解释方法。最后，我们提供了一个新颖的开源框架来解释最新的时间序列变换器和基础模型。]]></description>
      <guid>https://arxiv.org/abs/2412.04532</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无控制变量的分布式学习通信压缩</title>
      <link>https://arxiv.org/abs/2412.04538</link>
      <description><![CDATA[arXiv:2412.04538v1 公告类型：新
摘要：分布式学习算法，例如联邦学习 (FL) 中采用的算法，需要通信压缩来降低客户端上传的成本。实践中使用的压缩方法往往有偏差，当压缩程度较高时，需要错误反馈才能实现收敛。反过来，错误反馈需要特定于客户端的控制变量，这直接与隐私保护原则相矛盾，并且需要有状态的客户端。在本文中，我们提出了压缩聚合反馈 (CAFe)，这是一种新颖的分布式学习框架，它通过利用过去的聚合更新来实现高度可压缩的客户端更新，并且不需要控制变量。我们将分布式梯度下降 (DGD) 视为一种代表性算法，并提供了 CAFe 优于分布式压缩梯度下降 (DCGD) 的理论证明，该算法在非平滑状态下具有有界梯度相异性的有偏压缩。实验结果证实，CAFe 的表现始终优于直接压缩的分布式学习，并凸显了 CAFe 客户端更新的可压缩性。]]></description>
      <guid>https://arxiv.org/abs/2412.04538</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用摊销无似然推理解决噪声和不完整数据中的高维逆问题</title>
      <link>https://arxiv.org/abs/2412.04565</link>
      <description><![CDATA[arXiv:2412.04565v1 公告类型：新
摘要：我们提出了一种基于高维逆问题正则化流的新型无似然概率反演方法。所提出的方法由两个互补网络组成：用于数据压缩的摘要网络和用于参数估计的推理网络。摘要网络将原始观测值编码为固定大小的摘要统计数据向量，而推理网络根据这些摘要统计数据生成模型参数近似后验分布的样本。后验样本以深度生成的方式生成，方法是从潜在高斯分布中采样并对这些样本进行可逆变换。我们通过顺序交替条件可逆神经网络 (cINN) 和条件神经样条流 (cNSF) 层来构建这种可逆变换。摘要和推理网络同时训练。我们将所提出的方法应用于地下水水文学中的反演问题，以估计系统对数电导率场的后验分布，该分布以系统水头响应的空间稀疏时间序列观测为条件。在所考虑的问题中，电导率场以 706 个自由度表示。与基于似然的迭代集合平滑器 PEST-IES 方法的比较表明，所提出的方法在 PEST-IES 推理时间的一小部分内准确地估计了参数后验分布和观测的预测后验分布。]]></description>
      <guid>https://arxiv.org/abs/2412.04565</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于预测金属 3D 打印变形的数据驱动、参数化降阶模型</title>
      <link>https://arxiv.org/abs/2412.04577</link>
      <description><![CDATA[arXiv:2412.04577v1 公告类型：新
摘要：在激光粉末床熔合 (LPBF) 中，施加的激光能量会产生高热梯度，导致最终部件出现不可接受的变形。准确的变形预测对于优化 3D 打印过程和制造符合几何精度要求的部件至关重要。本研究引入了数据驱动的参数化降阶模型 (ROM) 来预测各种机器工艺设置中 LPBF 中的变形。我们提出了一个结合适当正交分解 (POD) 和高斯过程回归 (GPR) 的 ROM 框架，并将其性能与基于深度学习的参数化图卷积自动编码器 (GCA) 进行比较。POD-GPR 模型表现出高精度，预测 $\pm0.001mm$ 内的变形，并将计算速度提高约 1800 倍。]]></description>
      <guid>https://arxiv.org/abs/2412.04577</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Koopman 自动编码器的损失项和算子形式</title>
      <link>https://arxiv.org/abs/2412.04578</link>
      <description><![CDATA[arXiv:2412.04578v1 公告类型：新
摘要：Koopman 自动编码器是运算符学习中流行的架构。但是，文献中的损失函数和运算符形式差异很大。本文对这些选项进行了公正而系统的研究。此外，它还引入了新的损失项。]]></description>
      <guid>https://arxiv.org/abs/2412.04578</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过双随机张量的权重共享来学习对称性</title>
      <link>https://arxiv.org/abs/2412.04594</link>
      <description><![CDATA[arXiv:2412.04594v1 公告类型：新
摘要：组等方差已成为深度学习中一种有价值的归纳偏差，可增强泛化、数据效率和鲁棒性。传统上，组等方差方法要求事先知道感兴趣的组，这对于现实世界的数据来说可能并不现实。此外，固定组等方差可能会对模型架构施加过于严格的约束。这凸显了对能够动态发现和应用对称性作为软约束的方法的需求。对于神经网络架构，等方差通常是通过规范权重张量的组变换来实现的，从而导致给定组 $G$ 上的权重共享。在这项工作中，我们建议通过定义一组可学习的双随机矩阵来学习这种权重共享方案，这些矩阵充当规范权重张量的软置换矩阵，可以将常规组表示作为特例。这产生了可学习的核变换，这些变换与下游任务联合优化。我们表明，当数据集表现出强对称性时，置换矩阵将收敛到正则群表示，并且我们的权重共享网络实际上变成了正则群卷积。此外，该方法的灵活性使其能够有效地拾取部分对称性。]]></description>
      <guid>https://arxiv.org/abs/2412.04594</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用能量最小化和 MLP 进行非线性算子学习</title>
      <link>https://arxiv.org/abs/2412.04596</link>
      <description><![CDATA[arXiv:2412.04596v1 公告类型：新
摘要：我们开发并评估了一种用于学习由偏微分方程控制的非线性问题的解算子的方法。该方法基于有限元离散化，旨在通过以潜在变量为输入的 MLP 来表示解算子。潜在变量通常对应于输入数据参数化中的参数，例如边界条件、系数和右侧。损失函数通常是能量函数，我们根据在每个元素上局部组装能量来制定有效的可并行训练算法。对于大问题，每次迭代中仅使用网格中随机选择的一小部分元素可以使学习过程更有效率。该方法在几个相关的测试案例上进行了评估，与传统数值方法相比，学习解算子被证明是有益的。]]></description>
      <guid>https://arxiv.org/abs/2412.04596</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在预训练中学习到的提取结构能够对经过微调的事实进行泛化</title>
      <link>https://arxiv.org/abs/2412.04614</link>
      <description><![CDATA[arXiv:2412.04614v1 公告类型：新 
摘要：预训练语言模型 (LM) 可以推广到经过微调的事实含义。例如，如果对“John Doe 住在东京”进行微调，LM 可以正确地用“日语”回答“John Doe 所在城市的人说什么语言？”。然而，人们对实现这种泛化的机制或它们在预训练过程中是如何学习的知之甚少。我们引入了提取结构作为框架，用于描述 LM 中的组件（例如 MLP 或注意力头）如何协调以实现这种泛化。这些结构由将训练事实存储为权重变化的信息组件以及查询和处理存储信息以产生正确含义的上游和下游提取组件组成。我们假设提取结构是在预训练过程中遇到先前已知事实的含义时学习的。这产生了两个预测：数据排序效应，其中只有事实先于其含义才能学习提取结构；权重嫁接效应，其中提取结构可以转移以预测反事实含义。我们在 OLMo-7b、Llama 3-8b、Gemma 2-9b 和 Qwen 2-7b 模型中通过实证证明了这些现象。值得一提的是，我们的结果还表明，事实学习可以发生在早期和晚期层，从而导致不同形式的泛化。]]></description>
      <guid>https://arxiv.org/abs/2412.04614</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有时我是一棵树：数据驱动不稳定的层次概括</title>
      <link>https://arxiv.org/abs/2412.04619</link>
      <description><![CDATA[arXiv:2412.04619v1 公告类型：新
摘要：神经网络通常倾向于基于表面模式的快捷启发式方法。例如，语言模型 (LM) 在训练初期表现得像 n-gram 模型。但是，为了正确应用语法规则，LM 必须依赖分层句法表示而不是 n-gram。在这项工作中，我们使用英语语法案例研究来探索训练数据中的潜在结构如何推动模型实现更好的分布外 (OOD) 泛化。然后，我们研究数据组成如何导致随机种子之间的 OOD 行为不一致以及训练动态不稳定。我们的结果表明，只有当模型完全遵循表面级线性规则或分层规则时，它们的 OOD 行为才会稳定。此外，分层规则是由具有深度嵌入结构的语法复杂序列引起的，而线性规则是由更简单的序列引起的。当数据包含简单和复杂示例的混合时，潜在规则会发生竞争；每次独立的训练运行要么通过遵循单一规则而稳定下来，要么在其 OOD 行为中保持不稳定。这些条件导致“稳定种子”聚集在简单规则周围，在种子之间形成双峰性能分布。我们还发现了稳定性和泛化之间关系的一个例外：记忆低多样性训练数据模式的模型可能会稳定地过度拟合，记忆和未记忆模式的规则不同。我们的研究结果强调了训练数据在塑造泛化模式方面的关键作用，以及数据子集之间的竞争如何导致随机种子之间的泛化结果不一致。代码可在 https://github.com/sunnytqin/concept_comp.git 上找到。]]></description>
      <guid>https://arxiv.org/abs/2412.04619</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BigDocs：一个开放且许可的数据集，用于在文档和代码任务上训练多模式模型</title>
      <link>https://arxiv.org/abs/2412.04626</link>
      <description><![CDATA[arXiv:2412.04626v1 公告类型：新
摘要：多模态 AI 有可能显著增强文档理解任务，例如处理收据、理解工作流、从文档中提取数据和总结报告。需要长结构化输出的代码生成任务也可以通过多模态得到增强。尽管如此，由于对训练数据的访问有限和限制性许可阻碍了开放访问，它们在商业应用中的使用往往受到限制。为了解决这些限制，我们推出了 BigDocs-7.5M，这是一个高质量的开放访问数据集，包含 30 个任务中的 750 万份多模态文档。我们使用高效的数据管理流程来确保我们的数据是高质量和许可许可的。我们的流程通过过滤规则、可追溯的元数据和仔细的内容分析强调问责制、责任制和透明度。此外，我们还推出了 BigDocs-Bench，这是一个包含 10 个新任务的基准测试套件，我们在其中创建的数据集反映了现实世界的用例，包括通过图形用户界面 (GUI) 进行推理和从图像生成代码。我们的实验表明，使用 BigDocs-Bench 进行训练在文档推理和结构化输出任务（例如 Screenshot2HTML 或 Image2Latex 生成）中的平均性能比闭源 GPT-4o 提高了 25.8%。最后，人类评估显示，人们更喜欢在 BigDocs 上训练的模型的输出，而不是 GPT-4o。这表明 BigDocs 可以帮助学术界和开源社区利用和改进 AI 工具来增强多模式能力和文档推理。该项目托管在 https://bigdocs.github.io 。]]></description>
      <guid>https://arxiv.org/abs/2412.04626</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SWEPO：群体对比对齐的同步加权偏好优化</title>
      <link>https://arxiv.org/abs/2412.04628</link>
      <description><![CDATA[arXiv:2412.04628v1 公告类型：新
摘要：我们引入了同步加权偏好优化 (SWEPO)，这是直接偏好优化 (DPO) 的一种新扩展，旨在适应每个查询的多个动态选择的正面和负面响应。SWEPO 采用加权组对比损失，根据响应与平均奖励分数的偏差为响应分配权重。这种方法有效地优先考虑明显优于或差于平均值的响应，从而增强优化。我们的理论分析表明，同时考虑多个偏好可以减少对齐偏差，从而实现更稳健的对齐。此外，我们还深入了解了我们的损失函数和相关函数 InfoNCA 的训练动态。对 UltraFeedback 数据集的实证验证表明，SWEPO 是最先进的，在使用 AlpacaEval 数据集的下游评估中具有卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2412.04628</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用工具进行因果推理的解缠表征学习</title>
      <link>https://arxiv.org/abs/2412.04641</link>
      <description><![CDATA[arXiv:2412.04641v1 公告类型：新
摘要：潜在混杂因素是从观察数据推断因果关系的一个基本挑战。工具变量 (IV) 方法是应对这一挑战的一种实用方法。现有的基于 IV 的估计量需要已知的 IV 或其他强假设，例如系统中存在两个或多个 IV，这限制了 IV 方法的应用。在本文中，我们考虑了一个宽松的要求，即假设系统中有一个 IV 代理，而不知道哪个变量是代理。我们提出了一种基于变分自动编码器 (VAE) 的解缠表示学习方法，从具有潜在混杂因素的数据集中学习 IV 表示，然后利用 IV 表示从数据中获得因果关系的无偏估计。对合成数据和真实数据进行的大量实验表明，所提出的算法优于现有的基于 IV 的估计量和基于 VAE 的估计量。]]></description>
      <guid>https://arxiv.org/abs/2412.04641</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过上下文学习提高表格数据的 LLM 组公平性</title>
      <link>https://arxiv.org/abs/2412.04642</link>
      <description><![CDATA[arXiv:2412.04642v1 公告类型：新
摘要：大型语言模型 (LLM) 已被证明在低数据环境下对表格预测任务有效，可利用其内部知识和从指令和示例中学习的能力。然而，LLM 可能无法生成满足群体公平性的预测，即在群体之间产生公平的结果。至关重要的是，传统的自然语言任务去偏方法并不能直接转化为减轻表格环境中的群体不公平。在这项工作中，我们系统地研究了四种经验方法来提高表格数据集上 LLM 预测的群体公平性，包括公平提示优化、软提示调整、少数样本示例的战略选择以及通过思维链推理自我完善预测。通过使用开源和专有 LLM 在四个表格数据集上进行的实验，我们展示了这些方法在增强人口平等的同时保持高整体性能的有效性。我们的分析为从业者提供了可行的见解，帮助他们根据特定的要求和限制选择最合适的方法。]]></description>
      <guid>https://arxiv.org/abs/2412.04642</guid>
      <pubDate>Mon, 09 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>