<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>回到未来：面向行动的未来预报混合 Transformer-XGBoost 模型</title>
      <link>https://arxiv.org/abs/2412.19832</link>
      <description><![CDATA[arXiv:2412.19832v1 公告类型：新
摘要：受标志性电影《回到未来》的启发，本文探讨了一种创新的自适应即时预测方法，该方法重新构想了当前行动与未来结果之间的关系。在电影中，角色穿越时空来操纵过去的​​事件，旨在创造更美好的未来。类似地，我们的框架采用对未来的预测见解来通知和调整当前状况。这个双阶段模型将 Transformers（未来远见者）的预测能力与 XGBoost（决策者）的可解释性和效率相结合，实现了未来预测和当前适应的无缝循环。通过对气象数据集的实验，我们展示了该框架在实现更准确预测的同时为实时应用指导可操作干预措施方面的优势。]]></description>
      <guid>https://arxiv.org/abs/2412.19832</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>降阶模型和条件期望</title>
      <link>https://arxiv.org/abs/2412.19836</link>
      <description><![CDATA[arXiv:2412.19836v1 公告类型：新
摘要：系统可能依赖于人们可以控制的参数，或者用于优化系统的参数，或者外部强加的参数，或者它们可能是不确定的。最后一种情况被视为以下内容的“主旋律”。降阶模型是通过某种投影到相对低维流形或子空间而从全阶模型生成的。参数依赖的减少过程会产生流形中参数的函数。现在，人们想要检查所有可能的感兴趣的参数值的全状态和减少状态之间的关系。类似地，在机器学习领域，机器学习模型的图像空间中的参数集的函数也是在样本的训练集上学习的，通常最小化均方误差。该集合可以看作是来自某个概率分布的样本，因此训练是对期望的近似计算，给出条件期望的近似值，这是贝叶斯更新的一个特例，其中贝叶斯损失函数是均方误差。这提供了综合研究这些方法的可能性，并且还引入了更通用的损失函数。]]></description>
      <guid>https://arxiv.org/abs/2412.19836</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>极小极大最优多智能体鲁棒强化学习</title>
      <link>https://arxiv.org/abs/2412.19873</link>
      <description><![CDATA[arXiv:2412.19873v1 公告类型：新
摘要：多智能体稳健强化学习，也称为多人稳健马尔可夫博弈 (RMG)，是环境不确定性下建模竞争互动的重要框架，在多智能体系统中具有广泛的应用。然而，RMG 中样本复杂性的现有结果至少受到以下三个障碍之一的影响：不确定性水平或准确性的限制范围、多智能体的诅咒以及长期障碍，所有这些都导致现有结果大大超出信息论下限。为了弥补这一差距，我们将 Q-FTRL 算法 \citep{li2022minimax} 扩展到有限视界设置中的 RMG，假设可以访问生成模型。我们证明，所提出的算法实现了 $\varepsilon$ 稳健粗相关均衡 (CCE)，其样本复杂度（最多为对数因子）为 $\widetilde{O}\left(H^3S\sum_{i=1}^mA_i\min\left\{H,1/R\right\}/\varepsilon^2\right)$，其中 $S$ 表示状态数，$A_i$ 表示第 $i$ 个代理的动作数，$H$ 表示有限视界长度，$R$ 表示不确定性水平。我们还通过结合信息论下限证明了此样本复杂度是极小极大最优的。此外，在两人零和 RMG 的特殊情况下，该算法实现了具有相同样本复杂度的 $\varepsilon$ 稳健纳什均衡 (NE)。]]></description>
      <guid>https://arxiv.org/abs/2412.19873</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应裁剪的 DP-SGD 收敛性</title>
      <link>https://arxiv.org/abs/2412.19916</link>
      <description><![CDATA[arXiv:2412.19916v1 公告类型：新
摘要：带梯度裁剪的随机梯度下降 (SGD) 是一种实现差分隐私优化的强大技术。尽管之前的研究广泛研究了具有恒定阈值的裁剪，但隐私训练仍然对阈值选择高度敏感，这可能代价高昂甚至无法调整。这种敏感性促使开发自适应方法，例如分位数裁剪，这些方法已证明在经验上取得了成功，但缺乏扎实的理论理解。本文首次全面分析了带分位数裁剪的 SGD (QC-SGD)。我们证明 QC-SGD 存在与恒定阈值裁剪 SGD 类似的偏差问题，但展示了如何通过精心设计的分位数和步长计划来缓解这一问题。我们的分析揭示了分位数选择、步长和收敛行为之间的关键关系，为参数选择提供了实用指南。我们将这些结果扩展到差分隐私优化，为 DP-QC-SGD 建立了第一个理论保证。我们的研究结果为广泛使用的自适应裁剪启发式算法提供了理论基础，并指明了未来研究的开放途径。]]></description>
      <guid>https://arxiv.org/abs/2412.19916</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>标准差启发式正则化可提高对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2412.19947</link>
      <description><![CDATA[arXiv:2412.19947v1 公告类型：新
摘要：对抗训练 (AT) 已被证明可以提高深度神经网络 (DNN) 对抗攻击的鲁棒性。AT 是一种最小-最大优化程序，其中生成对抗性示例以训练更强大的 DNN。AT 的内部最大化步骤增加了输入相对于其实际类别的损失。外部最小化涉及最小化从内部最大化获得的对抗性示例的损失。这项工作提出了一个标准偏差启发 (SDI) 正则化项来提高对抗鲁棒性和泛化能力。我们认为 AT 中的内部最大化类似于最小化模型输出概率的修改标准偏差。此外，我们建议最大化这个修改后的标准偏差可以补充 AT 框架的外部最小化。为了支持我们的论点，我们通过实验表明 SDI 度量可用于制作对抗性示例。此外，我们证明将 SDI 正则化项与现有的 AT 变体相结合可以增强 DNN 抵御更强大的攻击（例如 CW 和自动攻击）的鲁棒性，并提高泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2412.19947</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于过程集成的单传感器方法的数据驱动铣削刀具磨损预测</title>
      <link>https://arxiv.org/abs/2412.19950</link>
      <description><![CDATA[arXiv:2412.19950v1 公告类型：新
摘要：准确的刀具磨损预测对于保持生产率和最大限度地降低加工成本至关重要。然而，刀具磨损过程的复杂性对实现可靠的预测提出了重大挑战。本研究探讨了用于刀具磨损预测的数据驱动方法，特别是深度学习。传统的数据驱动方法通常专注于单一过程，依赖于多传感器设置和大量数据生成，这限制了对新设置的推广。此外，多传感器集成在工业环境中通常是不切实际的。为了解决这些限制，本研究使用最少的训练数据调查了预测模型的可转移性，并在两个过程中进行了验证。此外，它使用带有单个加速度传感器的简单设置来建立一种低成本的数据生成方法，该方法有助于通过迁移学习将模型推广到其他过程。该研究评估了几种机器学习模型，包括卷积神经网络 (CNN)、长短期记忆网络 (LSTM)、支持向量机 (SVM) 和决策树，这些模型在不同输入格式（例如特征向量和短时傅里叶变换 (STFT)）上进行训练。在不同数量的训练数据上评估了模型的性能，包括数据集显著减少的场景，从而深入了解了它们在受限数据条件下的有效性。结果证明了特定模型和配置在有效预测刀具磨损方面的潜力，有助于开发更具适应性和效率的机械加工预测性维护策略。值得注意的是，ConvNeXt 模型具有出色的性能，仅使用四把铣刀运行至磨损的数据，即可实现 99.1% 的刀具磨损识别准确率。]]></description>
      <guid>https://arxiv.org/abs/2412.19950</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MobileNetV2：一种用于家庭睡眠呼吸暂停筛查的轻量级分类模型</title>
      <link>https://arxiv.org/abs/2412.19967</link>
      <description><![CDATA[arXiv:2412.19967v1 公告类型：新
摘要：本研究提出了一种新型轻量级神经网络模型，利用从心电图 (ECG) 和呼吸信号中提取的特征进行早期 OSA 筛查。ECG 信号用于生成特征频谱图以预测睡眠阶段，而呼吸信号用于检测与睡眠相关的呼吸异常。通过整合这些预测，该方法可以更准确地计算呼吸暂停低通气指数 (AHI)，从而有助于精确诊断 OSA。
该方法在三个公开的睡眠呼吸暂停数据库上得到了验证：Apnea-ECG 数据库、UCDDB 数据集和 MIT-BIH 多导睡眠图数据库。结果显示 OSA 的整体检测准确率为 0.978，突显了该模型的稳健性。呼吸事件分类的准确率为 0.969，受试者工作特征曲线下面积 (ROC-AUC) 为 0.98。对于睡眠阶段分类，在 UCDDB 数据集中，所有阶段的 ROC-AUC 均超过 0.85，睡眠的回忆率达到 0.906，REM 和清醒状态的特异性分别为 0.956 和 0.937。
这项研究强调了将轻量级神经网络与多信号分析相结合以实现准确、便携且经济高效的 OSA 筛查的潜力，为更广泛地应用于家庭和可穿戴健康监测系统铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2412.19967</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释语义联邦学习助力消防监控工业边缘网络</title>
      <link>https://arxiv.org/abs/2412.19979</link>
      <description><![CDATA[arXiv:2412.19979v1 公告类型：新 
摘要：在消防监控中，工业物联网 (IIoT) 设备需要频繁传输大量监控数据，这导致频谱资源的巨大消耗。因此，我们提出了一种工业边缘语义网络 (IESN)，允许 IIoT 设备通过语义通信 (SC) 发送警告。因此，我们应该考虑 (1) 数据隐私和安全性。 (2) 异构设备的 SC 模型适配。 (3) 语义的可解释性。因此，首先，我们提出了一种可解释的语义联邦学习 (XSFL) 来训练 SC 模型，从而确保数据的隐私和安全。然后，我们提出了一种自适应客户端训练 (ACT) 策略，根据其 Fisher 信息矩阵为每个设备提供特定的 SC 模型，从而克服异构性。接下来，设计了一种可解释的 SC (ESC) 机制，该机制引入了基于 leakyReLU 的激活映射来解释提取的语义与监控数据之间的关系。最后，仿真结果证明了XSFL的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.19979</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>第五届国际神经网络验证大赛 (VNN-COMP 2024)：总结与结果</title>
      <link>https://arxiv.org/abs/2412.19985</link>
      <description><![CDATA[arXiv:2412.19985v1 公告类型：新
摘要：本报告总结了第五届国际神经网络验证竞赛 (VNN-COMP 2024)，该竞赛是第七届国际人工智能验证研讨会 (SAIV) 的一部分，与第 36 届国际计算机辅助验证会议 (CAV) 同期举办。VNN-COMP 每年举办一次，旨在促进对最先进的神经网络验证工具进行公平客观的比较，鼓励工具接口的标准化，并汇集神经网络验证社区。为此，定义了网络 (ONNX) 和规范 (VNN-LIB) 的标准化格式，在同等成本的硬件上评估工具（使用基于 AWS 实例的自动评估管道），并在最终测试集公开之前由参与者选择工具参数。在 2024 年的迭代中，8 支队伍参加了 12 个常规基准和 8 个扩展基准的多样化测试。本报告总结了本次竞赛的规则、基准、参与工具、结果和经验教训。]]></description>
      <guid>https://arxiv.org/abs/2412.19985</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于联邦学习的延迟随机部分梯度平均</title>
      <link>https://arxiv.org/abs/2412.19987</link>
      <description><![CDATA[arXiv:2412.19987v1 公告类型：新
摘要：联邦学习 (FL) 是一种分布式机器学习范例，它使多个客户端能够协作训练共享模型，同时保护隐私。然而，现实世界中 FL 系统的扩展通常受到两个通信瓶颈的限制：(a) 虽然边缘设备的计算能力不断提高，使得大规模深度神经网络 (DNN) 的部署成为可能，但有限的带宽限制了大型 DNN 上的频繁传输；(b) 高延迟成本大大降低了 FL 的性能。鉴于这些瓶颈，我们提出了一种延迟随机部分梯度平均 (DPGA) 来增强 FL。在 DPGA 下，客户端仅与服务器共享部分本地模型梯度。本地模型中共享部分的大小由更新率决定，更新率被粗略初始化，然后在时间维度上进行细化。此外，DPGA 通过实现与通信并行的计算，大大缩短了系统运行时间。我们对非独立同分布 CIFAR-10/100 进行了实验，以证明我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2412.19987</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Caesar：一种用于高效联邦学习的低偏差压缩方法</title>
      <link>https://arxiv.org/abs/2412.19989</link>
      <description><![CDATA[arXiv:2412.19989v1 公告类型：新
摘要：压缩是减轻联邦学习 (FL) 系统巨大通信开销的有效方法。然而，对于现有的工作，压缩下的信息丢失将导致 FL 训练出现意外的模型/梯度偏差，从而显著降低训练性能，尤其是在数据异构性和模型过时的挑战下。为了在模型精度和流量成本之间取得微妙的平衡，我们提出了 Caesar，一种具有低偏差压缩方法的新型 FL 框架。对于全局模型下载，我们设计了一种贪婪方法，根据本地模型的陈旧程度优化每个设备的压缩率，确保本地训练的精确初始模型。对于局部梯度上传，我们利用设备的本地数据属性（即样本量和标签分布）来量化其局部梯度的重要性，然后指导梯度压缩率的确定。此外，通过细粒度的批次大小优化，Caesar 可以显著减少同步屏障下设备的空闲等待时间。我们在两个物理平台上实现了 Caesar，包括 40 部智能手机和 80 台 NVIDIA Jetson 设备。大量结果表明，与具有相同目标精度的基于压缩的基线相比，Caesar 可以将流量成本降低约 25.54%$\thicksim$37.88%，而与全精度通信相比，最终测试精度仅降低 0.68%。]]></description>
      <guid>https://arxiv.org/abs/2412.19989</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>适用于大规模不可靠设备的强大联邦学习框架</title>
      <link>https://arxiv.org/abs/2412.19991</link>
      <description><![CDATA[arXiv:2412.19991v1 公告类型：新 
摘要：在联邦学习 (FL) 系统中，许多设备（例如智能手机）在训练期间通常是不可靠的（例如，经常与 WiFi 断开连接）。现有的 FL 框架始终假设可靠的环境并将不可靠的设备排除在训练之外，导致模型性能不佳和资源浪费。在本文中，我们提出了 FLUDE 来有效处理不可靠的环境。首先，FLUDE 根据设备历史行为的概率分布（例如，成功完成训练的可能性）来评估设备的可靠性。基于此评估，FLUDE 自适应地选择可靠性高的设备进行训练。为了减少训练阶段的资源浪费，FLUDE 在每个设备上维护一个模型缓存，旨在保留最新的训练状态以供以后使用，以防不可靠设备上的本地训练中断。此外，FLUDE 提出了一种过时感知策略，可以明智地将全局模型分发到设备子集，从而在保持模型性能的同时显著减少资源浪费。我们在两个物理平台上实现了 FLUDE，包括 120 部智能手机和 NVIDIA Jetson 设备。大量实验结果表明，FLUDE 可以有效提高不可靠环境中 FL 训练的模型性能和资源效率。]]></description>
      <guid>https://arxiv.org/abs/2412.19991</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>离散曲率图信息瓶颈</title>
      <link>https://arxiv.org/abs/2412.19993</link>
      <description><![CDATA[arXiv:2412.19993v1 公告类型：新
摘要：图神经网络（GNN）已被证明取决于节点有效信息是否充分传递。离散曲率（Ricci 曲率）用于以几何视角研究图连通性和信息传播效率，近年来被提出来探索 GNN 的有效消息传递结构。然而，大多数实证研究都是基于直接观察到的图结构或启发式拓扑假设，缺乏对下游任务底层最优信息传输结构的深入探索。我们认为，对于具有更丰富的消息传递特征和更好的信息传输可解释性的图结构，图曲率优化比直接重新布线或学习更深入和必要。从图几何和信息论的角度，我们提出了新颖的离散曲率图信息瓶颈（CurvGIB）框架，以优化信息传输结构并同时学习更好的节点表示。 CurvGIB 推进了 Ricci 曲率优化的变分信息瓶颈 (VIB) 原理，以学习特定下游任务的最佳信息传输模式。学习到的 Ricci 曲率用于细化图的最佳传输结构，并充分有效地学习节点表示。此外，针对 Ricci 曲率微分的计算复杂性，我们结合 Ricci 流和 VIB 推导出曲率优化近似，以形成可处理的 IB 目标函数。在各种数据集上进行的大量实验证明了 CurvGIB 的卓越有效性和可解释性。]]></description>
      <guid>https://arxiv.org/abs/2412.19993</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ProtCLIP：基于功能的蛋白质多模态学习</title>
      <link>https://arxiv.org/abs/2412.20014</link>
      <description><![CDATA[arXiv:2412.20014v1 公告类型：新
摘要：将蛋白质序列与生物描述进行比对的多模态预训练范式已经学习到了一般的蛋白质表示，并在各种下游应用中取得了良好的效果。然而，由于对齐的蛋白质-文本配对数据的使用效率低下以及缺乏有效的功能知情预训练范式，这些工作仍然无法复制语言监督的视觉基础模型的非凡成功。针对这些问题，本文利用属性驱动的采样策略整理了一个名为ProtAnno的大规模蛋白质-文本配对数据集，并介绍了一种新颖的功能知情蛋白质预训练范式。具体而言，采样策略根据样本置信度和属性覆盖率确定选择概率，在面对大规模噪声数据时平衡数据质量和数据数量。此外，受蛋白质特定功能机制重要性的启发，所提出的范式通过两个逐段预训练目标明确地模拟蛋白质静态和动态功能段，以功能知情的方式注入细粒度信息。利用所有这些创新，我们开发了ProtCLIP，这是一个全面表示功能感知蛋白质嵌入的多模态基础模型。在蛋白质功能分类、突变效应预测、跨模态转换、语义相似性推断和蛋白质-蛋白质相互作用预测等5类22个不同蛋白质基准测试中，我们的ProtCLIP始终如一地实现SOTA性能，在五个跨模态转换基准测试中平均提高了75%，在GO-CC中提高了59.9%，在GO-BP蛋白质功能预测中提高了39.7%。实验结果验证了ProtCLIP作为蛋白质多模态基础模型的非凡潜力。]]></description>
      <guid>https://arxiv.org/abs/2412.20014</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>无界光滑条件下随机双层优化的近似最优单循环算法</title>
      <link>https://arxiv.org/abs/2412.20017</link>
      <description><![CDATA[arXiv:2412.20017v1 公告类型：新
摘要：本文研究随机双层优化问题，其中上层函数是非凸的，具有潜在无界的平滑度，而下层函数是强凸的。该问题的动机是应用于序列数据的元学习，例如使用循环神经网络进行文本分类，其中上层损失函数的平滑度常数与梯度范数线性相关，并且可能无界。现有算法关键依赖于嵌套循环设计，这需要大量的调整工作并且不切实际。在本文中，我们通过提出单循环双层优化器（SLIP）来解决这个问题。所提出的算法首先通过几步随机梯度下降更新下层变量，然后同时通过具有动量的正则化随机梯度下降更新上层变量，并通过随机梯度下降更新下层变量。在标准假设下，我们表明我们的算法可以在随机梯度或 Hessian 向量积的 $\widetilde{O}(1/\epsilon^4)$\footnote{此处 $\widetilde{O}(\cdot)$ 压缩对数因子 $1/\epsilon$ 和 $1/\delta$，其中 $\delta\in(0,1)$ 表示失败概率。} 次 oracle 调用中找到一个 $\epsilon$ 驻点，期望值和概率值均满足。此复杂度结果在对数因子附近接近最优，而无需随机梯度 oracle 的均方平滑度。我们的证明依赖于 (i) 对低级变量的精细表征和控制，以及 (ii) 在分布漂移下建立双层优化与随机优化之间的新联系。我们在各种任务上的实验表明，我们的算法在双层优化中的表现明显优于强基线。]]></description>
      <guid>https://arxiv.org/abs/2412.20017</guid>
      <pubDate>Tue, 31 Dec 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>