<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Fri, 01 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>编译器大型语言模型的优先采样</title>
      <link>https://arxiv.org/abs/2402.18734</link>
      <description><![CDATA[arXiv:2402.18734v1 公告类型：新
摘要：大型语言模型在生成和优化代码方面显示出巨大的潜力。广泛使用的采样方法（例如核采样）增加了生成的多样性，但通常会产生低温重复样本和高温不相干样本。此外，必须针对每个任务调整温度系数，限制了其可用性。我们提出了优先采样，这是一种简单且确定性的采样技术，可生成按模型置信度排序的独特样本。每个新样本都会在增强搜索树中以最高概率扩展未扩展的标记。此外，优先采样支持基于正则表达式的生成，提供可控且结构化的探索过程。对于任意数量的样本，优先采样都优于核采样，将原始模型的性能从 -Oz 的 2.87% 提高到 5%。此外，它的性能优于用于在仅 30 个样本中训练原始模型时生成标签的自动调谐器。]]></description>
      <guid>https://arxiv.org/abs/2402.18734</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:36 GMT</pubDate>
    </item>
    <item>
      <title>量化社交和导航网络上的人类先验</title>
      <link>https://arxiv.org/abs/2402.18651</link>
      <description><![CDATA[arXiv:2402.18651v1 公告类型：新
摘要：人类知识在很大程度上是隐性的和相关的——我们有共同的朋友吗？我可以从这里步行到那里吗？在这项工作中，我们利用图的组合结构来量化人类对此类关系数据的先验。我们的实验重点关注在进化时间尺度上持续相关的两个领域：社交互动和空间导航。我们发现推断的先验的一些特征非常一致，例如稀疏性作为图大小函数的趋势。其他特征是特定于领域的，例如社交互动中三元闭合的倾向。更广泛地说，我们的工作展示了如何使用间接行为实验的非经典统计分析来有效地模拟数据中的潜在偏差。]]></description>
      <guid>https://arxiv.org/abs/2402.18651</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>VOROS：将 ROC 曲线提升至 3D</title>
      <link>https://arxiv.org/abs/2402.18689</link>
      <description><![CDATA[arXiv:2402.18689v1 公告类型：新
摘要：ROC 曲线下面积是一种常见的度量方法，常用于对不同二元分类器的相对性能进行排名。然而，正如之前所指出的，当两个类别之间的真实类别值或误分类成本高度不平衡时，这种衡量方法可能无法捕捉到不同分类器的好处。我们引入第三个维度来捕获这些成本，并以自然的方式将 ROC 曲线提升到 ROC 曲面。我们研究了这两个曲面，并引入了 VOROS（该 ROC 曲面上的体积），作为 ROC 曲线下 2D 面积的 3D 概括。对于仅对预期成本或类别不平衡有限制的问题，我们限制考虑 ROC 表面的适当子区域的体积。我们展示了 VOROS 如何在经典和现代示例数据集上更好地捕获不同分类器的成本。]]></description>
      <guid>https://arxiv.org/abs/2402.18689</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>通过梯度下降学习联想记忆</title>
      <link>https://arxiv.org/abs/2402.18724</link>
      <description><![CDATA[arXiv:2402.18724v1 公告类型：新
摘要：这项工作的重点是存储令牌嵌入外积的一个关联记忆模块的训练动态。我们将这个问题简化为对粒子系统的研究，粒​​子系统根据数据分布的属性和嵌入之间的相关性进行交互。通过理论和实验，我们提供了一些见解。在过度参数化的情况下，我们获得了“分类裕度”的对数增长。然而，我们表明，由于相关嵌入导致的令牌频率和内存干扰的不平衡会导致振荡的瞬时状态。大步长时振荡更加明显，这可能会产生良性损失尖峰，尽管这些学习速率会加速动态并加速渐近收敛。在参数化不足的情况下，我们说明了交叉熵损失如何导致次优的记忆方案。最后，我们评估了小型 Transformer 模型研究结果的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.18724</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>揭示隐私、记忆和输入曲率链接</title>
      <link>https://arxiv.org/abs/2402.18726</link>
      <description><![CDATA[arXiv:2402.18726v1 公告类型：新
摘要：深度神经网络（DNN）已成为解决许多新问题的普遍工具。然而，他们往往会过度适应并记住训练集。记忆之所以引起人们的浓厚兴趣，是因为它与泛化、噪声学习和隐私等几个概念密切相关。为了研究记忆，Feldman（2019）提出了正式的分数，但其计算要求限制了其实际使用。最近的研究显示了将输入损失曲率（通过损失 Hessian w.r.t 输入的迹来测量）与记忆联系起来的经验证据。结果表明，它比计算记忆分数的效率高出约 3 个数量级。然而，缺乏将记忆与输入损失曲率联系起来的理论理解。在本文中，我们不仅研究了这种联系，还扩展了我们的分析，以建立差异隐私、记忆和输入损失曲率之间的理论联系。首先，我们推导出以差分隐私和输入损失曲率为特征的记忆上限。其次，我们提出了一种新颖的见解，表明输入损失曲率的上限受到差分隐私参数的限制。我们的理论研究结果通过 CIFAR 和 ImageNet 数据集上的深度模型得到了进一步的实证验证，表明我们的理论预测与实践中观察到的结果之间存在很强的相关性。]]></description>
      <guid>https://arxiv.org/abs/2402.18726</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>使用固定随机分类器训练的深度神经网络模型可以更好地跨域迁移</title>
      <link>https://arxiv.org/abs/2402.18614</link>
      <description><![CDATA[arXiv:2402.18614v1 公告类型：新
摘要：最近发现的神经崩溃（NC）现象表明，深度神经网络（DNN）的最后一层权重在训练的最后阶段收敛到所谓的等角紧框架（ETF）单纯形。该 ETF 几何结构相当于最后一层激活的类内变异性消失。受 NC 特性的启发，我们在本文中探讨了根据 ETF 固定最后一层权重训练的 DNN 模型的可迁移性。这通过消除类协方差信息来强制类分离，有效地提供隐式正则化。我们表明，使用此类固定分类器训练的 DNN 模型可显着提高传输性能，尤其是在域外数据集上。在广泛的细粒度图像分类数据集上，我们的方法优于 i) 不执行任何协方差正则化的基线方法（高达 22%），以及 ii）在整个训练过程中显式白化激活协方差的方法（高达19%）。我们的研究结果表明，使用固定 ETF 分类器训练的 DNN 为改进跨领域的迁移学习提供了强大的机制。]]></description>
      <guid>https://arxiv.org/abs/2402.18614</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:34 GMT</pubDate>
    </item>
    <item>
      <title>使用成本函数调节多边定位和图神经网络进行 GNSS 定位</title>
      <link>https://arxiv.org/abs/2402.18630</link>
      <description><![CDATA[arXiv:2402.18630v1 公告类型：新
摘要： 在城市环境中，GNSS 卫星的视距信号经常被高层物体遮挡，GNSS 接收器在测量卫星范围时会出现较大误差。启发式方法通常用于估计这些误差并减少噪声测量对定位精度的影响。在我们的工作中，我们用基于图神经网络的深度学习模型取代了这些错误估计启发式。此外，通过分析多边定位过程的成本函数，我们得出了利用估计误差的最佳方法。我们的方法保证随着误差估计精度的提高，多点定位收敛到接收器的位置。我们在包含超过 10 万个 GNSS 纪元的现实数据集上评估我们的解决方案，这些数据集是从具有不同特征的多个城市收集的。实证结果表明，与最近的深度学习基线以及经典定位方法相比，水平定位误差从 40% 提高到 80%。]]></description>
      <guid>https://arxiv.org/abs/2402.18630</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:34 GMT</pubDate>
    </item>
    <item>
      <title>探索共享扩散模型中的隐私和公平风险：对抗性视角</title>
      <link>https://arxiv.org/abs/2402.18607</link>
      <description><![CDATA[arXiv:2402.18607v1 公告类型：新
摘要：扩散模型最近在学术界和工业界引起了广泛关注，因为它们在采样质量和分布覆盖范围方面具有令人印象深刻的生成性能。因此，提出了在不同组织之间共享预先训练的扩散模型的建议，作为提高数据利用率的一种方式，同时通过避免直接共享私人数据来增强隐私保护。然而，与这种方法相关的潜在风险尚未得到全面检查。
  在本文中，我们采取对抗性的视角来调查与扩散模型共享相关的潜在隐私和公平风险。具体来说，我们调查了一方（共享者）使用私有数据训练扩散模型并为另一方（接收者）提供对下游任务的预训练模型的黑盒访问的情况。我们证明共享者可以通过操纵扩散模型的训练数据分布来执行公平中毒攻击来破坏接收者的下游模型。同时，接收者可以执行属性推断攻击，以揭示共享者数据集中敏感特征的分布。我们在现实世界数据集上进行的实验表明，不同类型的扩散模型具有出色的攻击性能，这凸显了稳健的数据审计和隐私保护协议在相关应用中的至关重要性。]]></description>
      <guid>https://arxiv.org/abs/2402.18607</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>ICE-SEARCH：语言模型驱动的特征选择方法</title>
      <link>https://arxiv.org/abs/2402.18609</link>
      <description><![CDATA[arXiv:2402.18609v1 公告类型：新
摘要：本研究揭示了上下文进化搜索（ICE-SEARCH）方法，这是第一个将语言模型（LM）与特征选择（FS）任务的进化算法相融合的工作，并证明了其在医学预测分析（MPA）应用中的有效性。 ICE-SEARCH 在进化框架内利用 LM 固有的交叉和变异能力，通过模型全面的世界知识及其对各种角色的适应性显着提高 FS。我们对该方法的评估涵盖三个关键的 MPA 任务：中风、心血管疾病和糖尿病，其中 ICE-SEARCH 在确定医疗应用的基本特征方面优于传统的 FS 方法。 ICE-SEARCH 在中风预测和糖尿病预测方面实现了最先进 (SOTA) 的性能；决策随机 ICE-SEARCH 在心血管疾病预测方面排名 SOTA。我们的结果不仅证明了 ICE-SEARCH 在医疗 FS 中的功效，而且还强调了在 FS 任务中集成 LM 的多功能性、效率和可扩展性。该研究强调了整合特定领域见解的关键作用，说明了 ICE-SEARCH 的稳健性、普遍性和快速收敛性。这为进一步研究全面而复杂的金融服务领域开辟了道路，标志着人工智能在医学预测分析中的应用迈出了重大一步。]]></description>
      <guid>https://arxiv.org/abs/2402.18609</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们只需要注意力图：利用 LeukoGraph 对血液细胞群进行开创性的层次分类</title>
      <link>https://arxiv.org/abs/2402.18610</link>
      <description><![CDATA[arXiv:2402.18610v1 公告类型：新
摘要：在外周血或骨髓等复杂的血液样本中，细胞分类将不同的群体划分为层次结构，提出了深刻的挑战。这项研究提出了 LeukoGraph，这是一个最近开发的框架，专门为此目的而设计，采用图注意力网络（GAT）来导航层次分类（HC）复杂性。值得注意的是，LeukoGraph 是一项开创性的工作，标志着图神经网络 (GNN) 在图的层次推理中的应用，可容纳多达一百万个节点和数百万条边，所有这些都源自流式细胞术数据。 LeukoGraph 错综复杂地解决了一种分类范式，例如，四种不同的细胞群进行平面分类，而第五个细胞群则分为两个不同的子分支，举例说明了复杂数据集中固有的微妙层次结构。该技术比这个例子更通用。 LeukoGraph 的标志性成就是其 F 分数高达 98%，显着超过了流行的最先进方法。至关重要的是，LeukoGraph 的实力超越了理论创新，在预测来自 30 名不同患者的流式细胞术数据集中的扁平和分层细胞类型方面表现出了非凡的精度。尽管分层分类带来了固有的挑战，但 LeukoGraph 保持正确标签比率的能力进一步强调了这种精度。]]></description>
      <guid>https://arxiv.org/abs/2402.18610</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>FORML：一种具有正交约束的元学习的黎曼无 Hessian 方法</title>
      <link>https://arxiv.org/abs/2402.18605</link>
      <description><![CDATA[arXiv:2402.18605v1 公告类型：新
摘要：元学习问题通常被表述为双层优化，其中任务特定参数和元参数分别在优化的内循环和外循环中更新。然而，在参数和元参数位于黎曼流形上的黎曼空间中执行优化是计算密集型的。与欧几里得方法不同，黎曼反向传播需要计算二阶导数，其中包括通过黎曼算子（例如回缩和正交投影）进行反向计算。本文介绍了一种无 Hessian 方法，该方法使用 Stiefel 流形上导数的一阶近似。我们的方法显着减少了计算负载和内存占用。我们展示了如何使用 Stiefel 全连接层作为主干网络的头部，对最后一个分类层的参数强制正交约束，从而增强基于梯度的元学习方法的表示重用。我们在各种少样本学习数据集上的实验结果证明了我们提出的方法与最先进的方法相比的优越性，特别是 MAML（其欧几里德对应物）。]]></description>
      <guid>https://arxiv.org/abs/2402.18605</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>网络拓扑对去中心化联邦学习性能的影响</title>
      <link>https://arxiv.org/abs/2402.18606</link>
      <description><![CDATA[arXiv:2402.18606v1 公告类型：新
摘要：完全去中心化的学习在互联网边缘训练人工智能模型、解决基础设施挑战和隐私问题方面正在获得动力。在去中心化机器学习系统中，数据分布在多个节点上，每个节点根据各自的数据集训练本地模型。然后共享并组合局部模型，形成能够对新数据进行准确预测的全局模型。我们的探索重点是不同类型的网络结构如何影响知识的传播，即节点将从网络中其他节点上可用数据的学习模式中获得的见解纳入其中的过程。具体来说，本研究使用三种网络拓扑和六种数据分布方法研究网络结构和学习性能之间复杂的相互作用。这些方法考虑不同的顶点属性，包括度中心性、介数中心性和聚类系数，以及节点是否表现出这些指标的高值或低值。我们的研究结果强调了全局中心性指标（程度、介数）与学习表现相关的重要性，而局部聚类的预测能力较差。我们强调了将知识从外围节点转移到中心节点的挑战，这归因于模型聚合过程中的稀释效应。此外，我们观察到中心节点发挥拉力效应，促进知识的传播。在检查学位分布时，Barabasi-Albert 网络中的中心对中心节点的学习产生积极影响，但当知识源自外围节点时会加剧稀释。最后，我们展示了隔离社区之外知识流通的巨大挑战。]]></description>
      <guid>https://arxiv.org/abs/2402.18606</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>具有图形反馈的随机上下文老虎机：从独立数到 MAS 数</title>
      <link>https://arxiv.org/abs/2402.18591</link>
      <description><![CDATA[arXiv:2402.18591v1 公告类型：新
摘要：我们考虑带有图反馈的上下文强盗，这是一类比普通上下文强盗具有更丰富结构的交互式学习问题，其中采取一个动作会揭示所有上下文下反馈图中所有相邻动作的奖励。与多臂强盗设置不同的是，越来越多的文献对图形反馈进行了近乎完整的理解，而上下文强盗设置中还有很多尚未探索的内容。在本文中，我们通过建立遗憾下限$\Omega(\sqrt{\beta_M(G) T})$来深入研究这一问题，其中$M$是上下文的数量，$G$是反馈图，$\beta_M(G)$ 是我们提出的图论量，它表征了此类问题的基本学习极限。有趣的是，$\beta_M(G)$在$\alpha(G)$（图的独立数）和$\mathsf{m}(G)$（图的最大非循环子图（MAS）数）之间进行插值随着上下文 $M$ 数量的变化。我们还提供了对重要类别的上下文序列和/或反馈图实现近乎最佳遗憾的算法，例如在拍卖和库存控制中找到应用的传递闭合图。特别是，在许多上下文中，我们的结果表明，MAS 数完全表征了上下文老虎机的统计复杂性，而不是多臂老虎机中的独立数。]]></description>
      <guid>https://arxiv.org/abs/2402.18591</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>元任务：元学习正则化的另一种观点</title>
      <link>https://arxiv.org/abs/2402.18599</link>
      <description><![CDATA[arXiv:2402.18599v1 公告类型：新
摘要：由于标记数据的稀缺，小样本学习（FSL）是一个具有挑战性的机器学习问题。有效概括新颖任务和训练任务的能力是 FSL 的一个重大障碍。本文提出了一种新颖的解决方案，可以推广到训练和新任务，同时还可以利用未标记的样本。该方法在使用无监督技术作为“元任务”更新外循环之前完善嵌入模型。实验结果表明，我们提出的方法在新任务和训练任务上表现良好，具有更快更好的收敛性、更低的泛化性和标准差误差，表明其在 FSL 中的实际应用潜力。实验结果表明，该方法比原型网络性能提高了 3.9%。]]></description>
      <guid>https://arxiv.org/abs/2402.18599</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:31 GMT</pubDate>
    </item>
    <item>
      <title>MMSR：符号回归是一项多模态任务</title>
      <link>https://arxiv.org/abs/2402.18603</link>
      <description><![CDATA[arXiv:2402.18603v1 公告类型：新
摘要：数学公式是人类几千年来探索自然规律智慧的结晶。用简洁的数学公式描述复杂的自然规律，是科学家不断的追求，也是对人工智能的巨大挑战。该领域称为符号回归。符号回归最初被表述为组合优化问题，并使用 GP 和强化学习算法来解决它。然而GP对超参数比较敏感，这两类算法效率较低。为了解决这个问题，研究人员将数据到表达式的映射视为翻译问题。并介绍了相应的大规模预训练模型。然而，数据和表达骨架并不像两种语言那样具有非常清晰的单词对应关系。相反，它们更像是两种模式（例如图像和文本）。因此，在本文中，我们提出了MMSR。将SR问题作为纯多模态问题来解决，并且在模态对齐的训练过程中还引入了对比学习，以利于后面的模态特征融合。值得注意的是，为了更好地促进模态特征融合，我们采用了对比学习损失和其他损失同时训练的策略，只需要一步训练，而不是先训练对比学习损失再训练其他损失。因为我们的实验证明一起训练可以让特征提取模块和特征融合模块磨合得更好。实验结果表明，与多个大规模预训练基线相比，MMSR在包括SRBench在内的多个主流数据集上取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.18603</guid>
      <pubDate>Fri, 01 Mar 2024 06:17:31 GMT</pubDate>
    </item>
    </channel>
</rss>