<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 13 Jan 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>PAC 框架中的单调学习：一种新视角</title>
      <link>https://arxiv.org/abs/2501.05493</link>
      <description><![CDATA[arXiv:2501.05493v1 公告类型：新
摘要：单调学习是指随着引入更多训练数据，预期性能持续提高的学习过程。机器学习的非单调行为一直是最近一系列研究的主题，各种提案都通过在学习算法上应用转换或包装来确保单调性。在这项工作中，我们从不同的角度，在可能近似正确 (PAC) 学习理论的框架内探讨单调学习的主题。遵循估计 PAC 可学习问题样本复杂度的机制，我们推导出该问题的性能下限，并证明该下限随着样本量增加而具有单调性。通过计算下限分布，我们能够证明，给定一个假设空间为有限大小或有限 VC 维的 PAC 可学习问题，如果训练样本是独立且相同分布的 (i.i.d.)，则任何基于经验风险最小化 (ERM) 的学习算法都是单调的。进一步对两个具体的机器学习问题进行了实验，其中一个问题具有有限的假设集，另一个问题具有有限的VC维数，并将经验风险分布的实验数据与估计的理论边界进行了比较。比较结果证实了这两个PAC可学习问题的学习单调性。]]></description>
      <guid>https://arxiv.org/abs/2501.05493</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数学建模和机器学习用于预测热应激下奶牛的避荫行为</title>
      <link>https://arxiv.org/abs/2501.05494</link>
      <description><![CDATA[arXiv:2501.05494v1 公告类型：新
摘要：在本文中，我们开发了一个结合机器学习技术的数学模型，以预测暴露于热应激的奶牛的避荫行为。该方法集成了先进的数学特征，例如时间平均热指数和累积热应激指标，这些特征是通过对 2023 年夏季收集的 Titaguas（西班牙瓦伦西亚）农场的数据进行数学分析获得的。比较了两种预测模型，即随机森林和神经网络，以了解其准确性、稳健性和可解释性。随机森林模型因其在精度和可解释性之间的平衡而备受关注，实现了 RMSE 为 $14.97$。该方法还采用 $5-$ 倍交叉验证以确保在现实条件下的稳健性。这项工作不仅推进了动物行为的数学建模，而且还为通过数据驱动工具减轻牲畜的热应激提供了有用的见解。]]></description>
      <guid>https://arxiv.org/abs/2501.05494</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LSEBMCL：基于潜在空间能量的持续学习模型</title>
      <link>https://arxiv.org/abs/2501.05495</link>
      <description><![CDATA[arXiv:2501.05495v1 公告类型：新
摘要：持续学习已成为许多实际应用中必不可少的部分，例如在线新闻摘要和产品分类。主要挑战被称为灾难性遗忘，即模型在接受新任务训练时无意中丢弃先前学习的知识的现象。现有的解决方案包括存储以前类别的样本、在微调过程中规范参数或为每个任务分配不同的模型参数。这项工作中提出的解决方案 LSEBMCL（基于潜在空间能量的持续学习模型）是使用基于能量的模型 (EBM) 来防止灾难性遗忘，方法是在对新任务进行训练时从先前的任务中采样数据点。EBM 是一种将能量值与每个输入数据点相关联的机器学习模型。所提出的方法使用 EBM 层作为 NLP 任务持续学习框架中的外部生成器。该研究证明了 EBM 在 NLP 任务中的有效性，在所有实验中都取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2501.05495</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FedSA：基于原型的联邦学习中通过语义锚点进行的统一表示学习</title>
      <link>https://arxiv.org/abs/2501.05496</link>
      <description><![CDATA[arXiv:2501.05496v1 公告类型：新 
摘要：基于原型的联邦学习已成为一种有前途的方法，它共享轻量级原型，以与模型无关的方式在具有数据异构性的客户端之间传递知识。然而，现有的方法通常直接从本地模型收集原型，由于客户端之间的数据分布有偏差和模型架构不同，这不可避免地会在表示学习中引入不一致。在本文中，我们发现统计和模型异质性都会造成表示不一致、分类器发散和原型对齐偏差的恶性循环，从而对客户端的性能产生负面影响。为了打破这种恶性循环，我们提出了一种名为通过语义锚点进行联邦学习 (FedSA) 的新框架，将原型的生成与本地表示学习分离开来。我们介绍了一种新颖的视角，使用简单但有效的语义锚点作为原型来指导本地模型学习一致的表示。通过整合语义锚点，我们进一步提出了基于锚点的正则化和边缘增强对比学习以及基于锚点的分类器校准，以校正特征提取器并校准跨客户端的分类器，实现原型的类内紧凑性和类间可分性，同时确保一致的决策边界。然后，我们使用这些一致且有判别力的原型更新语义锚点，这些原型迭代地鼓励客户端协作学习具有稳健泛化的统一数据表示。在统计和模型异质性设置下进行的大量实验表明，FedSA 在各种分类任务上的表现明显优于现有的基于原型的 FL 方法。]]></description>
      <guid>https://arxiv.org/abs/2501.05496</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成流网络：结构学习的理论和应用</title>
      <link>https://arxiv.org/abs/2501.05498</link>
      <description><![CDATA[arXiv:2501.05498v1 公告类型：新
摘要：在不假设数据生成的情况下，多个因果模型可能同样很好地解释我们的观察结果。因此，为了避免选择一个任意的模型，如果该模型与现实不符，可能会导致不安全的决策，因此必须保持对可能候选者的认知不确定性。本论文从贝叶斯的角度研究结构学习问题，在给定数据的情况下近似因果模型结构的后验分布，该模型表示为有向无环图 (DAG)。它引入了生成流网络 (GFlowNets)，这是一类新型概率模型，旨在对离散和组合对象（例如图）上的分布进行建模。他们将生成视为一个顺序决策问题，逐个构建定义为归一化常数的目标分布样本。在本论文的第一部分中，我们介绍了 GFlowNets 的数学基础、它们与现有机器学习和统计学领域的联系（例如变分推理和强化学习），以及它们在离散问题之外的扩展。在本论文的第二部分中，我们展示了 GFlowNets 如何在给定观察数据和实验数据的情况下近似因果贝叶斯网络 DAG 结构的后验分布及其因果机制的参数。]]></description>
      <guid>https://arxiv.org/abs/2501.05498</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用傅里叶神经算子对不同风向和城市的城市风环境进行概括</title>
      <link>https://arxiv.org/abs/2501.05499</link>
      <description><![CDATA[arXiv:2501.05499v1 公告类型：新 
摘要：城市风环境模拟对于城市规划、污染控制和可再生能源利用至关重要。然而，高保真计算流体动力学 (CFD) 方法的计算要求使其不适用于现实城市。为了解决这些限制，本研究调查了傅里叶神经算子 (FNO) 模型在预测不同风向和城市布局下的流场方面的有效性。在本研究中，我们调查了傅里叶神经算子 (FNO) 模型在预测不同风向和城市布局下的城市风况方面的有效性。通过在来自大涡模拟数据的速度数据上训练模型，我们评估了该模型在不同城市配置和风况下的性能。结果表明，FNO 模型可以提供准确的预测，同时显着减少 99% 的计算时间。我们将风场划分为更小的空间块进行训练的创新方法提高了 FNO 模型有效捕捉风频特征的能力。 SDF 数据还提供了重要的空间建筑信息，增强了模型识别物理边界和生成更现实预测的能力。所提出的 FNO 方法增强了 AI 模型对不同风向和城市布局的通用性。]]></description>
      <guid>https://arxiv.org/abs/2501.05499</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>缩小最长：利用对称几何改善潜在空间各向同性</title>
      <link>https://arxiv.org/abs/2501.05502</link>
      <description><![CDATA[arXiv:2501.05502v1 公告类型：新
摘要：尽管基于 Transformer 的模型一直主导着深度学习领域，但对其嵌入空间的各种研究表明，它们存在“表示退化问题”：嵌入往往分布在一个狭窄的锥体中，使潜在空间高度各向异性。增加各向同性已被证明可以提高静态和上下文语言模型中下游任务的性能。然而，大多数方法要么增加了推理开销，要么需要大量数据进行模型重新参数化。我们提出了一种基于单纯几何的新型正则化技术来提高潜在表示的各向同性。我们方法的核心思想是基于最大化使用 Vietoris-Rips 过滤从底层潜在空间中的上下文嵌入获得的条形码的持久熵。我们证明，该方法通过利用现有的几何结构而不是重新参数化，可以提高下游性能，同时显著降低微调过程中的各向异性。]]></description>
      <guid>https://arxiv.org/abs/2501.05502</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>面向快速物理应用的神经架构协同设计</title>
      <link>https://arxiv.org/abs/2501.05515</link>
      <description><![CDATA[arXiv:2501.05515v1 公告类型：新
摘要：我们开发了一个流水线来简化物理应用的神经架构协同设计，以减少在为新任务设计模型时对 ML 专业知识的需求。我们的方法采用两阶段方法的神经架构搜索和网络压缩来发现硬件高效模型。该方法包括一个全局搜索阶段，该阶段在考虑硬件约束的同时探索各种架构，然后是一个局部搜索阶段，该阶段对最有希望的候选者进行微调和压缩。我们在各种任务上都超越了性能，并通过量化感知训练和神经网络修剪等模型压缩技术展示了进一步的加速。我们使用 hls4ml 库将最佳模型合成为用于 FPGA 部署的高级综合代码。此外，我们的分层搜索空间在优化方面提供了更大的灵活性，可以轻松扩展到其他任务和领域。我们通过两个案例研究证明了这一点：材料科学中的布拉格峰发现和高能物理中的喷流分类，实现了相对于基线模型具有更高准确性、更小延迟或更低资源利用率的模型。]]></description>
      <guid>https://arxiv.org/abs/2501.05515</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NSChat：一个统治一切的聊天机器人系统</title>
      <link>https://arxiv.org/abs/2501.05541</link>
      <description><![CDATA[arXiv:2501.05541v1 公告类型：新
摘要：人工智能的快速发展导致了大型语言模型 (LLM) 的出现，它能够生成与人类交流非常相似的文本。这些模型已无缝集成到各种应用程序中，从而实现了跨多个平台的交互式和响应式通信。聊天机器人的潜在效用超越了这些传统应用程序，特别是在研究环境中，它们可以提供有价值的见解并促进创新实验的设计。在本研究中，我们介绍了 NSChat，这是一个基于网络的聊天机器人系统，旨在协助神经科学研究。该系统经过精心设计，可用作实验工具而不是传统的聊天机器人，需要用户在访问时输入用户名和实验代码。这种设置有助于精确的数据交叉引用，从而增强了为研究目的而收集的数据的完整性和适用性。它可以根据需要轻松扩展以适应新的基本事件；并且它允许研究人员集成自己的日志记录事件，而无需实施单独的日志记录机制。值得注意的是，我们的系统主要用于协助神经科学研究，但不仅限于此，它可以轻松地适应以协助信息检索研究或与聊天机器人代理的交互。]]></description>
      <guid>https://arxiv.org/abs/2501.05541</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络中的新兴权重形态</title>
      <link>https://arxiv.org/abs/2501.05550</link>
      <description><![CDATA[arXiv:2501.05550v1 公告类型：新
摘要：深度神经网络是否能够表现出突发行为不仅与理解深度学习的工作原理有关，而且对于评估日益强大的人工智能系统的潜在安全风险也至关重要。在这里，我们表明，训练深度神经网络会产生独立于训练数据的突发权重形态。具体而言，类似于凝聚态物理学，我们推导出一种理论，该理论预测深度神经网络的均匀状态是不稳定的，从而导致周期性通道结构的出现。我们通过对各种数据集进行数值实验来验证这些结构。我们的工作展示了深度神经网络训练中的突发性，这影响了深度神经网络的可实现性能。]]></description>
      <guid>https://arxiv.org/abs/2501.05550</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>外带汤：通过模型平均减轻持续学习过程中的遗忘</title>
      <link>https://arxiv.org/abs/2501.05559</link>
      <description><![CDATA[arXiv:2501.05559v1 公告类型：新
摘要：在持续学习中，任务数据按顺序到达，对后续任务进行微调通常会导致早期任务的性能下降。当这些任务来自不同领域时，这种情况尤其明显。在这种情况下，我们如何才能减轻对早期任务的灾难性遗忘，并以最小的计算成本保留模型所学到的内容？受到其他合并方法和 L2 回归的启发，我们提出了带平均的顺序微调 (SFA)，这是一种在训练过程中将当前训练模型与早期检查点合并的方法。SOTA 方法通常维护过去任务的数据缓冲区或在每个梯度步骤施加惩罚。相比之下，我们的方法无需存储过去的数据或每个梯度步骤的多个参数副本即可实现类似的结果。此外，我们的方法优于常见的合并技术，例如任务算术、TIES 合并和 WiSE-FT，以及其他惩罚方法，例如 L2 和弹性权重合并。反过来，我们的方法让我们了解到在图像和语言领域进行训练时合并部分训练模型的好处。]]></description>
      <guid>https://arxiv.org/abs/2501.05559</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>模拟贝叶斯神经网络对权重分布的形状不敏感</title>
      <link>https://arxiv.org/abs/2501.05564</link>
      <description><![CDATA[arXiv:2501.05564v1 公告类型：新
摘要：最近的研究表明，用均值场变分推理 (MFVI) 训练的贝叶斯神经网络 (BNN) 可以在模拟硬件中实现，与标准数字实现相比，有望节省大量能源。然而，虽然高斯通常用作 MFVI 中的变分分布，但很难精确控制采样模拟设备产生的噪声分布的形状。本文介绍了一种使用真实设备噪声作为变分分布进行 MFVI 训练的方法。此外，我们通过经验证明，无论变分分布的形状如何，具有相同权重均值和方差的 BNN 的预测分布都会收敛到相同的分布。这个结果表明，模拟设备设计人员在硬件实现执行 MFVI 的 BNN 时不需要考虑设备噪声分布的形状。]]></description>
      <guid>https://arxiv.org/abs/2501.05564</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过对输入参数相关性的对抗性攻击来强化基本关系</title>
      <link>https://arxiv.org/abs/2501.05588</link>
      <description><![CDATA[arXiv:2501.05588v1 公告类型：新
摘要：输入参数之间的相关性在许多科学分类任务中起着至关重要的作用，因为这些任务通常与自然的基本定律有关。例如，在高能物理中，深度学习的常见用例之一是粒子碰撞中信号和背景过程的分类。在许多这样的情况下，可观测量之间相关性的基本原理通常比可观测量本身的实际分布更容易理解。在这项工作中，我们提出了一种称为随机分布洗牌攻击 (RDSA) 的新对抗攻击算法，强调网络中可观测量之间的相关性，而不是单个特征特征。在对抗训练中使用生成的对手时，正确应用所提出的新攻击可以显着提高分类性能 - 特别是在数据增强的背景下。鉴于输入特征之间的相关性在许多其他学科中也至关重要。我们在六个分类任务中证明了 RDSA 的有效性，包括两个粒子碰撞挑战（使用 CERN 开放数据）、手写数字识别（MNIST784）、人类活动识别（HAR）、天气预报（澳大利亚的降雨）和 ICU 患者死亡率（MIMIC-IV），展示了这种新型对抗性攻击算法超越基础物理的一般用例。]]></description>
      <guid>https://arxiv.org/abs/2501.05588</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用离线稳健强化学习进行会话级动态广告负载优化</title>
      <link>https://arxiv.org/abs/2501.05591</link>
      <description><![CDATA[arXiv:2501.05591v1 公告类型：新
摘要：会话级动态广告负载优化旨在通过动态平衡用户体验质量和广告货币​​化，在用户在线会话期间实时个性化投放广告的密度和类型。传统的基于因果学习的方法面临着关键的技术挑战，特别是在处理混杂偏差和分布变化方面。在本文中，我们开发了一个基于离线深度 Q 网络 (DQN) 的框架，该框架有效地减轻了动态系统中的混杂偏差，并且与最佳的基于因果学习的生产基线相比，离线收益超过 80%。此外，为了提高框架对意外分布变化的鲁棒性，我们通过一种新颖的离线稳健决斗 DQN 方法进一步增强了我们的框架。随着扰动的增加，这种方法在多个 OpenAI-Gym 数据集上获得了更稳定的奖励，并在现实世界的广告投放数据上提供了额外的 5% 的离线收益。
我们的方法已部署到多个生产系统中，取得了巨大的收入增长。发布后的在线 A/B 测试显示，参与度与广告得分的权衡效率实现了两位数的提升，大大增强了我们平台为消费者和广告商提供服务的能力。]]></description>
      <guid>https://arxiv.org/abs/2501.05591</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过创新领域知识知情的基于注意力的知识追踪方法推进个性化学习分析</title>
      <link>https://arxiv.org/abs/2501.05605</link>
      <description><![CDATA[arXiv:2501.05605v1 公告类型：新 
摘要：新兴的知识追踪 (KT) 模型，尤其是深度学习和基于注意力的知识追踪，通过基于学生过去的互动预测学生未来的表现，在实现个性化学习分析方面显示出巨大的潜力。现有的方法主要关注直接的过去互动或个人概念，而不考虑知识概念之间的依赖关系，即知识概念路线，这对于促进对学生学习成果的理解至关重要。为了解决这个问题，在本文中，我们提出了一种创新的基于注意力的方法，通过有效地将知识概念路线的领域知识融入给定的课程中。此外，我们利用 XES3G5M 数据集（一个具有丰富知识概念路线辅助信息的基准数据集）来评估和比较我们提出的方法与七种最先进的 (SOTA) 深度学习模型的性能。]]></description>
      <guid>https://arxiv.org/abs/2501.05605</guid>
      <pubDate>Mon, 13 Jan 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>