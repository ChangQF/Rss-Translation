<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Wed, 14 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>微调预训练模型时要演练哪些预训练样本？</title>
      <link>https://arxiv.org/abs/2402.08096</link>
      <description><![CDATA[针对特定任务微调预训练的基础模型现在已成为文本和视觉任务的事实上的方法。这种方法的一个已知陷阱是在微调过程中忘记预训练知识。从预训练数据集中随机排练样本是减轻这种遗忘的常见方法。然而，我们发现随机混合无意中包含了模型尚未忘记或无法学习的样本。我们提出了一种新颖的采样方案 mix-cd，它可以识别并优先考虑实际面临遗忘的样本，我们称之为附带损害。由于直接识别附带损害样本的计算成本很高，因此我们提出了一种通过跟踪微调样本的统计数据来估计此类样本分布的程序。我们的方法是轻量级的，易于实现，并且可以无缝集成到现有模型中，提供了一种保留预训练性能的有效方法，而无需额外的计算成本。]]></description>
      <guid>https://arxiv.org/abs/2402.08096</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>学习具有拉普拉斯约束的笛卡尔积图</title>
      <link>https://arxiv.org/abs/2402.08105</link>
      <description><![CDATA[图拉普拉斯学习，也称为网络拓扑推理，是多个社区非常感兴趣的问题。在高斯图模型（GM）中，图学习相当于赋予拉普拉斯结构协方差选择。在图信号处理（GSP）中，从过滤系统的输出推断未观察到的图至关重要。在本文中，我们研究了拉普拉斯约束下学习笛卡尔积图的问题。笛卡尔图积是对高阶条件依赖进行建模的自然方法，也是将 GSP 推广到多路张量的关键。我们为笛卡尔乘积拉普拉斯算子的惩罚最大似然估计（MLE）建立了统计一致性，并提出了一种有效的算法来解决该问题。我们还扩展了我们的方法，以在存在结构缺失值的情况下进行有效的联合图学习和插补。对合成数据集和真实世界数据集的实验表明，我们的方法优于以前的 GSP 和 GM 方法。]]></description>
      <guid>https://arxiv.org/abs/2402.08105</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>microRTS 中获奖的深度强化学习代理</title>
      <link>https://arxiv.org/abs/2402.08112</link>
      <description><![CDATA[脚本代理在 CIG 和 CoG 主办的 IEEE microRTS ($\mu$RTS) 竞赛的前五届比赛中主要获胜。尽管深度强化学习（DRL）算法在实时策略（RTS）游戏中取得了重大进展，但由于需要大量的训练资源以及创建和调试此类代理所固有的复杂性，它们在主要学术竞赛中的采用受到限制。 RAISocketAI 是第一个赢得 IEEE microRTS 竞赛的 DRL 代理。在没有性能限制的基准测试中，RAISocketAI 经常击败之前的两个竞赛获胜者。第一个获奖的 DRL 提交可以成为未来 microRTS 竞赛的基准和未来 DRL 研究的起点。迭代地微调基本策略并将学习迁移到特定地图对于 RAISocketAI 的获胜表现至关重要。这些策略可用于经济地训练未来的 DRL 智能体。使用行为克隆进行模仿学习并使用 DRL 微调这些模型的进一步工作已被证明是一种很有前景的有效方法，可以引导具有已证明的竞争行为的模型。]]></description>
      <guid>https://arxiv.org/abs/2402.08112</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:39 GMT</pubDate>
    </item>
    <item>
      <title>BASE TTS：基于 10 万小时数据构建十亿参数文本转语音模型的经验教训</title>
      <link>https://arxiv.org/abs/2402.08093</link>
      <description><![CDATA[我们引入了一种名为 BASE TTS 的文本转语音 (TTS) 模型，它代表 $\textbf{B}$ig $\textbf{A}$daptive $\textbf{S}$treamable TTS with $\textbf{E }$合并能力。 BASE TTS 是迄今为止最大的 TTS 模型，经过 10 万小时的公共领域语音数据训练，在语音自然度方面达到了新的最先进水平。它部署了一个 10 亿参数的自回归 Transformer，将原始文本转换为离散代码（“语音代码”），然后是基于卷积的解码器，以增量、可流式传输的方式将这些语音代码转换为波形。此外，我们的语音代码是使用一种新颖的语音标记化技术构建的，该技术具有说话人 ID 解开和字节对编码压缩的功能。与广泛报道的大型语言模型在不断增加的数据量上训练时的“涌现能力”相呼应，我们表明，用 10K 多个小时和 5 亿多个参数构建的 BASE TTS 变体开始在文本复杂的句子上展示自然韵律。我们设计并共享一个专门的数据集来衡量这些文本转语音的新兴能力。我们通过对照基线进行评估，展示 BASE TTS 最先进的自然性，其中包括公开可用的大型文本转语音系统：YourTTS、Bark 和 TortoiseTTS。可以在 https://amazon-ltts-paper.com/ 上听到该模型生成的音频样本。]]></description>
      <guid>https://arxiv.org/abs/2402.08093</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:38 GMT</pubDate>
    </item>
    <item>
      <title>以文本为中心的多模态学习对齐</title>
      <link>https://arxiv.org/abs/2402.08086</link>
      <description><![CDATA[本研究论文解决了多模态学习中模态不匹配的挑战，其中推理过程中可用的模态与训练中可用的模态不同。我们提出了以文本为中心的多模态学习对齐（TAMML）方法，这是一种创新方法，利用具有上下文学习和基础模型的大型语言模型（LLM）来增强这些条件下多模态系统的通用性。通过利用文本作为统一语义空间的独特属性，TAMML 在处理看不见的、多样化的和不可预测的模态组合方面展示了显着的改进。 TAMML 不仅适应不同的模态，而且保持稳健的性能，展示了基础模型在克服传统固定模态框架在嵌入表示方面的局限性的潜力。这项研究为模态可用性动态且不确定的现实应用提供了灵活、有效的解决方案，为该领域做出了贡献。]]></description>
      <guid>https://arxiv.org/abs/2402.08086</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>学习神经契约动力学：扩展线性化和全局保证</title>
      <link>https://arxiv.org/abs/2402.08090</link>
      <description><![CDATA[学习动态系统中的全局稳定性和鲁棒性保证对于确保系统在面对不确定性时表现良好至关重要。我们提出了扩展线性收缩动力学（ELCD），这是第一个基于神经网络的动力系统，具有任意度量的全局收缩性保证。 ELCD 的主要特点是非线性矢量场扩展线性化的参数化。在最基本的形式中，ELCD 保证 (i) 全局指数稳定，(ii) 均衡收缩，以及 (iii) 就某些指标而言全局收缩。为了允许数据空间中更通用的度量的收缩，我们训练数据空间和潜在空间之间的微分同态，并在潜在空间中强制收缩，这确保了数据空间中的全局收缩。我们在 $2$D、$4$D 和 $8$D LASA 数据集上演示了 ELCD 的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.08090</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:37 GMT</pubDate>
    </item>
    <item>
      <title>使用输入输出规范为数据科学代码生成奠定基础</title>
      <link>https://arxiv.org/abs/2402.08073</link>
      <description><![CDATA[大型语言模型 (LLM) 最近展示了根据自然语言 (NL) 提示生成代码的卓越能力。然而，在现实世界中，NL 通常过于模糊，无法捕捉编程问题背后的真实意图，需要额外的输入输出 (I/O) 规范。不幸的是，法学硕士很难将其输出与 NL 提示和 I/O 规范保持一致。在本文中，我们提供了一种在数据科学编程背景下缓解此问题的方法，其中任务需要明确的 I/O 规范才能清晰。具体来说，我们提出了 GIFT4Code，这是一种根据 I/O 规范对 LLM 进行指令微调的新颖方法。我们的方法利用法学硕士本身产生的合成数据，并利用执行衍生的反馈作为关键的学习信号。这种反馈以程序 I/O 规范的形式提供给法学硕士，以促进指令微调。我们在两个具有挑战性的数据科学基准 Arcade 和 DS-1000 上评估了我们的方法。结果表明，法学硕士生成代码的能力得到了显着提高，这些代码不仅可以执行，而且可以准确地符合用户规范，从而大大提高了复杂数据科学任务的代码生成质量。]]></description>
      <guid>https://arxiv.org/abs/2402.08073</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:36 GMT</pubDate>
    </item>
    <item>
      <title>消息绕行：表达图学习的简单而有效的循环表示</title>
      <link>https://arxiv.org/abs/2402.08085</link>
      <description><![CDATA[图学习在生物信息学、社交网络和化学领域至关重要。尽管高阶图元（例如循环）对于实现节点分类、边缘预测和图识别的信息图表示至关重要，但对高阶拓扑特征进行建模带来了巨大的计算挑战，限制了其在机器学习中的广泛应用。为了解决这个限制，我们引入了 \textit{message detouring} 的概念来分层描述整个图中的循环表示，它利用与每个图节点相关的一系列局部拓扑内的最短和最长路径之间的对比。从我们的消息绕行景观中得出的拓扑特征表示表现出与高阶 \textit{Weisfeiler-Lehman} (WL) 测试相当的表达能力，但计算需求要少得多。除了与图内核和消息传递神经网络的集成之外，我们还提出了一种新颖的消息绕行神经网络，它使用 Transformer 主干来集成跨节点和边缘的循环表示。除了理论结果之外，表达性、图分类和节点分类的实验结果表明，消息绕行可以在各种基准数据集上显着优于当前的对应方法。]]></description>
      <guid>https://arxiv.org/abs/2402.08085</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:36 GMT</pubDate>
    </item>
    <item>
      <title>MIML 库：用于多实例多标签学习的模块化且灵活的库</title>
      <link>https://arxiv.org/abs/2402.08056</link>
      <description><![CDATA[MIML 库是一个 Java 软件工具，用于开发、测试和比较多实例多标签 (MIML) 学习的分类算法。该库包含 43 种算法，并提供用于数据管理和分区、保留和交叉验证方法、性能评估标准指标以及报告生成的特定格式和设施。另外，算法可以通过$xml$配置文件执行，无需编程。它独立于平台、可扩展、免费、开源，并可根据 GNU 通用公共许可证在 GitHub 上获取。]]></description>
      <guid>https://arxiv.org/abs/2402.08056</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>通过寻求帮助避免连续空间中的灾难</title>
      <link>https://arxiv.org/abs/2402.08062</link>
      <description><![CDATA[大多数带有正式后悔保证的强化学习算法都假设所有错误都是可逆的，并且基本上依赖于尝试所有可能的选项。当某些错误无法弥补甚至是灾难性的时，这种方法会导致不良结果。我们提出了上下文强盗问题的一种变体，其目标是最大限度地减少灾难的可能性。具体来说，我们假设每轮的收益代表了该轮避免灾难的机会，并尝试最大化收益的乘积（避免灾难的总体机会）。为了给代理一些成功的机会，我们允许向导师提出有限数量的查询，并假设利普希茨连续支付函数。我们提出了一种算法，假设连续的一维状态空间和相对“简单”的收益函数，随着时间范围的增长，遗憾和查询导师的比率都接近 0。我们还提供了一个匹配的下限：没有简单的假设：任何算法要么不断地寻求帮助，要么几乎肯定会导致灾难。最后，我们确定了将我们的算法推广到多维状态空间的关键障碍。]]></description>
      <guid>https://arxiv.org/abs/2402.08062</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:35 GMT</pubDate>
    </item>
    <item>
      <title>UGMAE：图屏蔽自动编码器的统一框架</title>
      <link>https://arxiv.org/abs/2402.08023</link>
      <description><![CDATA[图上的生成式自监督学习，特别是图屏蔽自动编码器，已成为一种流行的学习范式，并证明了其在处理非欧几里得数据方面的功效。然而，剩下的几个问题限制了现有方法的能力：1）在掩码中忽视了不均匀的节点重要性，2）整体图信息的利用不足，3）由于仅使用重建而忽略了表示空间中的语义知识输出空间的损失，4）大量屏蔽内容导致的不稳定重建。鉴于此，我们提出了UGMAE，一种图屏蔽自动编码器的统一框架，从适应性、完整性、互补性和一致性的角度解决这些问题。具体来说，我们首先开发一个自适应特征掩码生成器，以考虑节点和样本信息掩码的独特重要性（自适应性）。然后，我们设计了一个基于排名的结构重建目标，与特征重建相结合，以捕获整体图信息并强调邻居之间的拓扑邻近性（完整性）。之后，我们提出了一个基于引导的相似性模块来编码表示空间中的高级语义知识，与输出空间中的低级重建互补（互补性）。最后，我们构建了一个一致性保证模块，为重构目标提供额外稳定的一致性目标（一致性）。大量实验表明，UGMAE 在跨多个数据集的多项任务上均优于对比和生成最先进的基线。]]></description>
      <guid>https://arxiv.org/abs/2402.08023</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:34 GMT</pubDate>
    </item>
    <item>
      <title>扩散生成模型的最近邻分数估计器</title>
      <link>https://arxiv.org/abs/2402.08018</link>
      <description><![CDATA[评分函数估计是扩散生成模型训练和采样的基石。尽管如此，最常用的估计器要么是有偏差的神经网络近似值，要么是基于条件分数的高方差蒙特卡罗估计器。我们引入了一种新颖的最近邻得分函数估计器，它利用训练集中的多个样本来显着降低估计器方差。我们在两个引人注目的应用程序中利用了我们的低方差估计器。使用我们的估计器训练一致性模型，我们报告收敛速度和样本质量都有显着提高。在扩散模型中，我们表明我们的估计器可以取代学习网络进行概率流 ODE 集成，为未来研究开辟了有希望的新途径。]]></description>
      <guid>https://arxiv.org/abs/2402.08018</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>利用数字表兄弟在大规模无线网络中进行集成 Q 学习</title>
      <link>https://arxiv.org/abs/2402.08022</link>
      <description><![CDATA[优化大规模无线网络，包括优化资源管理、功率分配和吞吐量最大化，由于其不可观测的系统动态以及异构和复杂的性质，本质上具有挑战性。本文提出了一种新颖的集成 Q 学习算法，该算法解决了优化无线网络的传统 Q 学习算法的性能和复杂性挑战。使用合成马尔可夫决策过程的集成学习通过新模型针对无线网络进行定制，用于近似大型状态空间可观察无线网络。特别是，数字孪生被提出作为传统数字孪生概念的扩展，其中多个合成马尔可夫环境上的多个 Q 学习算法并行运行，并且它们的输出被融合到单个 Q 函数中。提供了关键统计数据和 Q 函数的收敛分析以及估计偏差和方差上限的推导。各种现实世界无线网络的数值结果表明，与最先进的强化学习算法相比，所提出的算法可以将平均策略错误降低多达 50%，运行时复杂性降低多达 40%。研究还表明，理论结果可以正确预测实验结果的趋势。]]></description>
      <guid>https://arxiv.org/abs/2402.08022</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:33 GMT</pubDate>
    </item>
    <item>
      <title>NetInfoF 框架：测量和利用网络可用信息</title>
      <link>https://arxiv.org/abs/2402.07999</link>
      <description><![CDATA[给定一个节点属性图和一个图任务（链接预​​测或节点分类），我们能否判断图神经网络（GNN）是否会表现良好？更具体地说，图结构和节点特征是否为任务携带了足够的可用信息？我们的目标是（1）开发一种快速工具来测量图结构和节点特征中有多少信息，以及（2）如果信息足够，则利用这些信息来解决任务。我们提出了 NetInfoF，一个包括 NetInfoF_Probe 和 NetInfoF_Act 的框架，分别用于网络可用信息（NUI）的测量和开发。给定图数据，NetInfoF_Probe 在没有任何模型训练的情况下测量 NUI，NetInfoF_Act 解决链路预测和节点分类，而两个模块共享相同的主干网。综上所述，NetInfoF具有以下显着优点： (a)通用性，同时处理链路预测和节点分类； (b) 有原则的，有理论保证和封闭式解； (c) 由于拟议的节点相似性调整，有效； (d) 可扩展，随输入大小线性缩放。在我们精心设计的合成数据集中，NetInfoF 正确识别了 NUI 的基本事实，并且是唯一对所有图形场景都稳健的方法。应用于现实数据集时，与一般 GNN 基线相比，NetInfoF 在链接预测的 12 次中赢得了 11 次。]]></description>
      <guid>https://arxiv.org/abs/2402.07999</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:32 GMT</pubDate>
    </item>
    <item>
      <title>CNN 需要哪些频率？特征学习中出现的瓶颈结构</title>
      <link>https://arxiv.org/abs/2402.08010</link>
      <description><![CDATA[我们描述了 CNN 中卷积瓶颈 (CBN) 结构的出现，其中网络使用其前几层将输入表示转换为仅沿少数频率和通道支持的表示，然后使用最后几层进行映射回到输出。我们定义了 CBN 等级，它描述了瓶颈内保留的频率的数量和类型，并部分证明了表示函数 $f$ 所需的参数范数随着深度乘以 CBN 等级 $f$ 而变化。我们还表明，参数范数在下一阶上取决于 $f$ 的规律性。我们证明，任何具有几乎最佳参数范数的网络都会在权重和激活中表现出 CBN 结构（假设网络在大学习率下稳定），这激发了下采样的常见实践；我们验证了 CBN 结果在下采样时仍然成立。最后，我们使用 CBN 结构来解释 CNN 在许多任务中学习到的函数。]]></description>
      <guid>https://arxiv.org/abs/2402.08010</guid>
      <pubDate>Wed, 14 Feb 2024 06:17:32 GMT</pubDate>
    </item>
    </channel>
</rss>