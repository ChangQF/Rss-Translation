<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>船舶设计中的生成式人工智能</title>
      <link>https://arxiv.org/abs/2408.16798</link>
      <description><![CDATA[arXiv:2408.16798v1 公告类型：新
摘要：船舶设计过程错综复杂，受船体形状的影响很大，船体形状约占总成本的 70%。传统方法依赖于基于船舶设计原理和工程分析的人为迭代过程。相比之下，生成式人工智能提出了一种新方法，利用植根于机器学习和人工智能的计算算法来优化船体设计。本报告概述了为此目的系统地创建生成式人工智能，涉及数据集收集、模型架构选择、训练和验证等步骤。该报告利用由 30,000 个船体形状组成的“SHIP-D”数据集，采用高斯混合模型 (GMM) 作为生成模型架构。GMM 提供了一个统计框架来分析数据分布，这对于高效生成创新船舶设计至关重要。总体而言，这种方法有望通过探索更广阔的设计空间并有效整合多学科优化目标来彻底改变船舶设计。]]></description>
      <guid>https://arxiv.org/abs/2408.16798</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>HLogformer：用于表示日志数据的分层转换器</title>
      <link>https://arxiv.org/abs/2408.16803</link>
      <description><![CDATA[arXiv:2408.16803v1 公告类型：新
摘要：Transformer 因其在处理各种数据结构方面的多功能性而广受赞誉，但它们在日志数据中的应用仍未得到充分探索。日志数据的特点是其分层的、类似字典的结构，在使用传统 Transformer 模型处理时会带来独特的挑战。传统方法通常依赖于手工制作的模板来解析日志，这个过程劳动密集型且缺乏通用性。此外，标准 Transformer 对日志序列的线性处理忽略了日志条目内丰富的嵌套关系，导致表示不理想和内存使用过多。
为了解决这些问题，我们引入了 HLogformer，这是一种专为日志数据设计的新型分层 Transformer 框架。HLogformer 利用日志条目的层次结构显着降低内存成本并增强表示学习。与将日志数据视为平面序列的传统模型不同，我们的框架以尊重其固有层次结构组织的方式处理日志条目。这种方法可确保对细粒度细节和更广泛的上下文关系进行全面编码。
我们的贡献有三点：首先，HLogformer 是第一个设计动态分层转换器的框架，专门用于类似字典的日志数据。其次，它大大降低了处理大量日志序列所需的内存成本。第三，全面的实验表明，HLogformer 可以更有效地编码分层上下文信息，这证明它对于合成异常检测和产品推荐等下游任务非常有效。]]></description>
      <guid>https://arxiv.org/abs/2408.16803</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>物理信息神经网络及其扩展</title>
      <link>https://arxiv.org/abs/2408.16806</link>
      <description><![CDATA[arXiv:2408.16806v1 公告类型：新 
摘要：在本文中，我们回顾了已成为科学机器学习主要支柱的新方法物理信息神经网络 (PINN)，我们介绍了最近的实际扩展，并提供了数据驱动的控制微分方程发现中的具体示例。]]></description>
      <guid>https://arxiv.org/abs/2408.16806</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于机器学习的青少年在线教育适应性研究</title>
      <link>https://arxiv.org/abs/2408.16849</link>
      <description><![CDATA[arXiv:2408.16849v1 公告类型：new 
摘要：随着互联网技术的飞速发展，青少年在线学习适应性问题成为教育界关注的焦点，但学术界对青少年在线学习适应性的预测模型还需要进一步完善和拓展。本研究利用2014年至2016年“中国青少年在线教育调查”数据，采用逻辑回归、K近邻、随机森林、XGBoost、CatBoost五种机器学习算法，分析影响青少年在线学习适应性的因素，并确定最适合预测的模型。研究发现，课程时长、家庭经济状况、年龄是影响学生在线学习适应性的主要因素，此外，年龄对学​​生的适应能力有显著影响。在预测模型中，随机森林、XGBoost、CatBoost算法表现出了较强的预测能力，其中随机森林模型尤其擅长捕捉学生的适应性特征。]]></description>
      <guid>https://arxiv.org/abs/2408.16849</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于评论家的正则化学习的星形几何</title>
      <link>https://arxiv.org/abs/2408.16852</link>
      <description><![CDATA[arXiv:2408.16852v1 公告类型：新
摘要：变分正则化是一种解决统计推断任务和逆问题的经典技术，现代数据驱动方法通过深度神经网络参数化正则化器，展示了令人印象深刻的经验性能。最近的研究沿着这个思路学习任务相关的正则化器。这是通过在无监督的、基于批评的损失函数中集成有关测量和地面真实数据的信息来实现的，其中正则化器将低值归因于可能的数据，将高值归因于不可能的数据。然而，关于通过这个过程学习到的正则化器的结构以及它与两个数据分布的关系的理论很少。为了在这一挑战上取得进展，我们发起了一项研究，优化基于批评的损失函数，以学习特定正则化器系列的正则化器：星形体的量规（或明可夫斯基函数）。这个家族包含实践中常用的正则化器，并与深度神经网络参数化的正则化器共享属性。我们专门研究了基于批评的损失，这些损失源自概率度量之间的统计距离的变分表示。通过利用星形几何和对偶 Brunn-Minkowski 理论的工具，我们说明了这些损失如何被解释为依赖于数据分布的对偶混合体积。这使我们能够在某些情况下推导出最佳正则化器的精确表达式。最后，我们确定了哪些神经网络架构会产生这样的星体量规，以及何时这样的正则化器具有有利于优化的特性。更广泛地说，这项工作强调了星形几何工具如何帮助理解无监督正则化学习的几何形状。]]></description>
      <guid>https://arxiv.org/abs/2408.16852</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GSTAM：利用结构注意力匹配实现高效图形蒸馏</title>
      <link>https://arxiv.org/abs/2408.16871</link>
      <description><![CDATA[arXiv:2408.16871v1 公告类型：新
摘要：图蒸馏已成为将大型图数据集缩减为更小、更易于管理和信息量更大的数据集的解决方案。现有方法主要针对节点分类，涉及计算密集型过程，并且无法捕获完整图数据集的真实分布。为了解决这些问题，我们引入了具有结构注意力匹配的图蒸馏 (GSTAM)，这是一种用于压缩图分类数据集的新方法。GSTAM 利用 GNN 的注意力图将原始数据集中的结构信息提炼到合成图中。结构注意力匹配机制利用 GNN 优先分类的输入图区域，有效地将这些信息提炼到合成图中并提高整体蒸馏性能。综合实验证明了 GSTAM 优于现有方法，在极端浓缩率下实现了 0.45% 至 6.5% 的更好性能，凸显了其在推进图形分类任务的蒸馏方面的潜在用途（代码可在 https://github.com/arashrasti96/GSTAM 获得）。]]></description>
      <guid>https://arxiv.org/abs/2408.16871</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用扩散解码器修改多模态 VAE</title>
      <link>https://arxiv.org/abs/2408.16883</link>
      <description><![CDATA[arXiv:2408.16883v1 公告类型：新
摘要：多模态 VAE 通常难以生成高质量输出，这一挑战超出了 VAE 框架的固有限制。核心问题在于潜在空间的联合表示受限，尤其是当涉及图像等复杂模态时。通常用于这些复杂模态的前馈解码器会无意中限制联合潜在空间，从而导致其他模态的质量下降。尽管最近的研究表明，通过引入特定于模态的表示可以取得改善，但问题仍然很严重。在这项工作中，我们证明，专门针对图像模态加入灵活的扩散解码器不仅可以提高图像的生成质量，还可以对依赖前馈解码器的其他模态的性能产生积极影响。这种方法解决了传统联合表示带来的限制，并为使用多模态 VAE 框架改进多模态生成任务开辟了新的可能性。与不同数据集中的其他多模态 VAE 相比，我们的模型提供了最先进的结果，生成的模态具有更高的连贯性和卓越的质量]]></description>
      <guid>https://arxiv.org/abs/2408.16883</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DLFormer：使用分布式滞后嵌入增强多元时间序列预测的可解释性</title>
      <link>https://arxiv.org/abs/2408.16896</link>
      <description><![CDATA[arXiv:2408.16896v1 公告类型：新
摘要：。大多数现实世界变量都是受过去值和解释因素影响的多变量时间序列。因此，使用人工智能预测这些时间序列数据正在进行中。特别是在医疗保健和金融等领域，可靠性至关重要，对预测有可理解的解释至关重要。然而，事实证明，在高预测精度和直观可解释性之间取得平衡具有挑战性。虽然基于注意力的模型在表示每个变量的个体影响方面存在局限性，但这些模型可以影响时间序列预测中的时间依赖性和单个变量影响的大小。为了解决这个问题，本研究引入了 DLFormer，这是一种基于注意力的架构，集成了分布式滞后嵌入，以时间嵌入单个变量并捕获它们的时间影响。通过对各种现实世界数据集的验证，DLFormer 与现有的基于注意力的高性能模型相比，展示了卓越的性能改进。此外，比较变量之间的关系增强了可解释性的可靠性。]]></description>
      <guid>https://arxiv.org/abs/2408.16896</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过机器学习中的梯度分析推理隐私风险</title>
      <link>https://arxiv.org/abs/2408.16913</link>
      <description><![CDATA[arXiv:2408.16913v1 公告类型：新
摘要：在分布式学习设置中，模型使用从潜在敏感用户数据计算出的共享梯度进行迭代更新。虽然以前的工作已经研究了共享梯度的各种隐私风险，但我们的论文旨在提供一种系统的方法来分析梯度中泄露的私人信息。我们提出了一个统一的基于游戏的框架，涵盖了广泛的攻击，包括属性、属性、分布和用户披露。我们通过对不同数据模态的五个数据集进行大量实验，研究了对手的不同不确定性如何影响他们的推理能力。我们的结果表明，在分布式学习中，仅依靠数据聚合来实现对推理攻击的隐私是无效的。我们进一步评估了五种类型的防御，即梯度修剪、有符号梯度下降、对抗性扰动、变分信息瓶颈和差异隐私，包括静态和自适应对手设置。我们提供了一个信息论观点来分析这些防御梯度推理的有效性。最后，我们介绍了一种审计属性推理隐私的方法，通过制作对抗性金丝雀记录来改进对最坏情况隐私的经验估计。]]></description>
      <guid>https://arxiv.org/abs/2408.16913</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>多任务和基于重放的持续学习中过度参数化模型的理论见解</title>
      <link>https://arxiv.org/abs/2408.16939</link>
      <description><![CDATA[arXiv:2408.16939v1 公告类型：新
摘要：多任务学习 (MTL) 是一种机器学习范式，旨在通过同时对多个相关任务进行训练来提高模型在这些任务上的泛化性能。与 MTL 不同，模型可以即时访问所有任务的训练数据，而持续学习 (CL) 涉及随着时间的推移适应新的顺序到达的任务，而不会忘记先前获得的知识。尽管 CL 和 MTL 被广泛采用，并且这两个领域都有大量文献，但当与深度神经网络等过度参数化的模型一起使用时，对这些方法的理论理解仍然存在差距。本文研究了过度参数化的线性模型作为更复杂模型的代理。我们开发了理论结果，描述了各种系统参数对 MTL 设置中模型性能的影响。具体而言，我们研究了模型大小、数据集大小和任务相似性对泛化误差和知识传递的影响。此外，我们提出了理论结果来表征基于重放的 CL 模型的性能。我们的结果揭示了缓冲区大小和模型容量对 CL 设置中的遗忘率的影响，并有助于阐明一些最先进的 CL 方法。最后，通过大量的实证评估，我们证明了我们的理论发现也适用于深度神经网络，为实践中设计 MTL 和 CL 模型提供了宝贵的指导。]]></description>
      <guid>https://arxiv.org/abs/2408.16939</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>迁移尺度律的实证研究</title>
      <link>https://arxiv.org/abs/2408.16947</link>
      <description><![CDATA[arXiv:2408.16947v1 公告类型：新
摘要：我们对 Transformer 模型中迁移学习的缩放定律进行了有限的实证研究。更具体地说，我们研究了一种包含“转移差距”项的缩放定律，该定律表明在优化另一个分布上的下游性能时，在一个分布上进行预训练的有效性。当转移差距较小时，预训练是一种提高下游性能的经济有效的策略。相反，当差距较大时，收集高质量的微调数据会变得相对更具成本效益。将缩放定律拟合到来自不同数据集的实验中，可以发现不同分布之间的转移差距存在显著差异。理论上，缩放定律可以为最佳数据分配策略提供信息，并强调下游数据的稀缺性如何影响性能。我们的研究结果有助于制定一种衡量迁移学习效率的原则性方法，并了解数据可用性如何影响能力。]]></description>
      <guid>https://arxiv.org/abs/2408.16947</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用强化学习发现频率控制器上的虚假数据注入方案</title>
      <link>https://arxiv.org/abs/2408.16958</link>
      <description><![CDATA[arXiv:2408.16958v1 公告类型：新
摘要：虽然基于逆变器的分布式能源 (DER) 在将可再生能源整合到电力系统中发挥着至关重要的作用，但它们同时也削弱了电网的系统惯性，增加了频率不稳定的风险。此外，如果不认真管理，通过通信网络连接的智能逆变器可能会受到网络威胁。为了主动加强电网抵御复杂的网络攻击，我们建议采用强化学习 (RL) 来识别潜在威胁和系统漏洞。本研究集中于分析虚假数据注入的对抗策略，特别是针对涉及一次频率控制的智能逆变器。我们的研究结果表明，RL 代理可以熟练地辨别最佳的虚假数据注入方法来操纵逆变器设置，从而可能导致灾难性的后果。]]></description>
      <guid>https://arxiv.org/abs/2408.16958</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>UserSumBench：评估用户摘要方法的基准框架</title>
      <link>https://arxiv.org/abs/2408.16966</link>
      <description><![CDATA[arXiv:2408.16966v1 公告类型：新 
摘要：大型语言模型 (LLM) 已显示出从大量原始用户活动数据生成用户摘要的卓越能力。这些摘要捕获了基本用户信息，例如偏好和兴趣，因此对于基于 LLM 的个性化应用程序（例如可解释的推荐系统）非常有价值。然而，缺乏真实标签、用户摘要固有的主观性以及通常成本高昂且耗时的人工评估阻碍了新摘要技术的开发。为了应对这些挑战，我们引入了 \UserSumBench，这是一个基准框架，旨在促进基于 LLM 的摘要方法的迭代开发。该框架提供两个关键组件：(1) 无参考摘要质量指标。我们表明，该指标在三个不同的数据集（MovieLens、Yelp 和 Amazon Review）中有效且与人类偏好一致。 (2) 一种新颖的稳健摘要方法，利用时间分层摘要器和自我批评验证器来生成高质量的摘要，同时消除幻觉。该方法为摘要技术的进一步创新奠定了坚实的基础。]]></description>
      <guid>https://arxiv.org/abs/2408.16966</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>点神经元学习：一种新的物理信息神经网络架构</title>
      <link>https://arxiv.org/abs/2408.16969</link>
      <description><![CDATA[arXiv:2408.16969v1 公告类型：新
摘要：机器学习和神经网络已经推动了许多研究领域的发展，但诸如大量训练数据需求和模型性能不一致等挑战阻碍了它们在某些科学问题中的应用。为了克服这些挑战，研究人员研究了将物理原理集成到机器学习模型中，主要通过：（i）物理引导的损失函数，通常称为物理信息神经网络，以及（ii）物理引导的架构设计。虽然这两种方法都在多个科学学科中取得了成功，但它们存在局限性，包括陷入局部最小值、可解释性差和通用性受限。本文提出了一种新的物理信息神经网络 (PINN) 架构，通过将波动方程的基本解嵌入到网络架构中，结合了两种方法的优势，使学习到的模型严格满足波动方程。所提出的点神经元学习方法可以根据麦克风观察对任意声场进行建模，而无需任何数据集。与其他 PINN 方法相比，我们的方法直接处理复数，具有更好的可解释性和通用性。我们通过混响环境中的声场重建问题来评估所提架构的多功能性。结果表明，点神经元方法优于两种竞争方法，并且可以有效处理具有稀疏麦克风观测的嘈杂环境。]]></description>
      <guid>https://arxiv.org/abs/2408.16969</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>联邦 Q 学习中的样本通信复杂性权衡</title>
      <link>https://arxiv.org/abs/2408.16981</link>
      <description><![CDATA[arXiv:2408.16981v1 公告类型：新
摘要：我们考虑联邦 Q 学习的问题，其中 $M$ 个代理旨在协作学习具有有限状态和动作空间的未知无限期马尔可夫决策过程的最优 Q 函数。我们研究了广泛使用的间歇通信算法类的样本和通信复杂性之间的权衡。我们首先建立逆结果，其中表明，联邦 Q 学习算法在每个代理样本复杂度中提供相对于代理数量的任何加速，需要产生至少为 $\frac{1}{1-\gamma}$ 阶的通信成本，最高可达对数因子，其中 $\gamma$ 是折扣因子。我们还提出了一种新算法，称为 Fed-DVR-Q，这是第一个同时实现阶数最优样本和通信复杂性的联邦 Q 学习算法。因此，这些结果共同提供了联邦 Q 学习中样本通信复杂性权衡的完整表征。]]></description>
      <guid>https://arxiv.org/abs/2408.16981</guid>
      <pubDate>Mon, 02 Sep 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>