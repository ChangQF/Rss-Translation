<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>训练过程中神经网络参数演变的信息几何</title>
      <link>https://arxiv.org/abs/2406.05295</link>
      <description><![CDATA[arXiv:2406.05295v1 公告类型：新
摘要：人工神经网络 (ANN) 是一种强大的工具，能够近似任何任意数学函数，但它们的可解释性仍然有限，使它们成为黑箱模型。为了解决这个问题，已经提出了许多方法来增强 ANN 的可解释性和可解释性。在本研究中，我们介绍了信息几何框架的应用，以研究 ANN 训练过程中的相变类行为，并将这些转变与某些模型中的过度拟合联系起来。
通过查看其参数的概率分布来研究 ANN 在训练过程中的演变。信息几何利用微分几何的原理，通过将概率密度函数视为黎曼流形上的点，为概率和统计提供了独特的视角。我们使用基于 Fisher 信息的度量来创建此流形以定义距离和速度。通过使用训练步骤参数化此距离和速度，我们研究了 ANN 如何随着训练的进展而演变。利用 MNIST、FMNIST 和 CIFAR-10 等标准数据集，我们在训练 ANN 时观察到流形上的运动转变，并且这种转变在所考虑的 ANN 模型中被确定为过度拟合。观察到的信息几何转变在数学上与物理学中的相变相似。还提供了显示有限尺寸缩放行为的初步结果。这项工作有助于开发用于提高 ANN 的可解释性和可解释性的强大工具，帮助我们理解这些复杂模型在训练过程中表现出的参数的变化。]]></description>
      <guid>https://arxiv.org/abs/2406.05295</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>用于在线多组学习的分组预言机高效算法</title>
      <link>https://arxiv.org/abs/2406.05287</link>
      <description><![CDATA[arXiv:2406.05287v1 公告类型：新
摘要：我们研究在线多组学习问题，这是一种学习模型，其中在线学习者必须同时对对应于一组组的大集合（可能重叠）子序列实现较小的预测遗憾。组是上下文空间的子集，在公平应用中，它们可能对应于由人口统计属性的表达函数定义的子种群。与之前关于这种学习模型的工作相比，我们考虑了组群太大而无法明确枚举的情况，因此我们寻求仅通过优化预言机访问组的算法。在本文中，我们在各种设置下设计了具有亚线性遗憾的这种预言机高效算法，包括：（i）i.i.d. 设置，（ii）具有平滑上下文分布的对抗设置，以及（iii）对抗传导设置。]]></description>
      <guid>https://arxiv.org/abs/2406.05287</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>利用极值化微调物理信息神经网络来解决边界值问题</title>
      <link>https://arxiv.org/abs/2406.05290</link>
      <description><![CDATA[arXiv:2406.05290v1 公告类型：新
摘要：我们提出了一种快速准确地训练物理信息神经网络 (PINN) 的新方法，以找到边界值问题 (BVP) 和初始边界值问题 (IBVP) 的解决方案。通过结合训练深度神经网络 (DNN) 和极限学习机 (ELM) 的方法，我们开发了一个具有 DNN 表达能力和 ELM 微调能力的模型。我们通过求解几个 BVP 和 IBVP 展示了我们提出的方法的优越性，其中包括线性和非线性常微分方程 (ODE)、偏微分方程 (PDE) 和耦合 PDE。我们考虑的例子包括传统数值方法失效的刚性耦合 ODE 系统、3+1D 非线性 PDE、Kovasznay 流和不可压缩 Navier-Stokes 方程的 Taylor-Green 涡旋解以及 1+1 D 可压缩欧拉方程的纯平流解。
函数连接理论 (TFC) 用于将 (I)BVP 的初始和边界条件 (IBC) 精确地施加到 PINN 上。我们提出了一种对 TFC 框架的修改，称为简化 TFC，与使用 TFC 施加的 IBC 相比，PINN 的训练和推理时间显著改善。此外，简化 TFC 被证明能够推广到更复杂的边界几何，而 TFC 则无法做到这一点。我们还介绍了一种在无穷远处为 BVP 应用边界条件的方法，并使用这些边界条件对 1+1 D 欧拉方程中的纯对流进行数值求解。]]></description>
      <guid>https://arxiv.org/abs/2406.05290</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>机器学习分类器的自动可信度测试</title>
      <link>https://arxiv.org/abs/2406.05251</link>
      <description><![CDATA[arXiv:2406.05251v1 公告类型：新
摘要：机器学习 (ML) 已成为我们社会不可或缺的一部分，常用于金融、医疗保健和交通等关键领域。因此，不仅要评估 ML 模型是否做出正确的预测，还要评估它们是否出于正确的原因这样做，以确保我们对看不见的数据的信任。这个概念在 ML 中被称为可信度。最近，可解释的技术（例如 LIME、SHAP）已经开发出来解释 ML 模型的决策过程，为它们的预测提供解释（例如，输入中对预测影响最大的单词）。评估这些解释的合理性可以增强我们对模型可信度的信心。然而，目前的方法通常依赖于人类判断来确定这些解释的合理性。
本文提出了 TOWER，这是第一种自动创建可信度预言的技术，可以确定文本分类器预测是否可信。它利用词嵌入根据解释技术的输出自动评估与模型无关的文本分类器的可信度。我们的假设是，如果解释中的单词在语义上与预测的类别相关，则预测是可信的。
我们使用从噪声数据中获得的不可信模型进行无监督学习，以找到 TOWER 的最佳配置。然后，我们在我们创建的人工标记的可信度数据集上对 TOWER 进行了评估。结果表明，TOWER 可以检测到随着噪声增加而导致的可信度下降，但在针对人工标记的数据集进行评估时效果不佳。我们的初步实验表明我们的假设是有效的和有希望的，但需要进一步研究以更好地理解解释和可信度问题之间的关系。]]></description>
      <guid>https://arxiv.org/abs/2406.05251</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>扩散模型的有效差异隐私微调</title>
      <link>https://arxiv.org/abs/2406.05257</link>
      <description><![CDATA[arXiv:2406.05257v1 公告类型：新
摘要：扩散模型 (DM) 的最新发展使得生成质量惊人的合成样本成为可能。最近的研究表明，扩散模型生成的合成样本可以在训练下游分类器的同时实现良好的隐私效用权衡，该模型在公共数据上进行了预训练，并在私有数据上进行了差分隐私完全微调。但是，使用 DP-SGD 对这种大型扩散模型进行完全微调在内存使用和计算方面可能非常耗费资源。在这项工作中，我们研究了使用低维自适应 (LoDA) 和差分隐私的扩散模型的参数高效微调 (PEFT)。我们使用 MNIST 和 CIFAR-10 数据集评估了所提出的方法，并证明这种高效的微调也可以生成有用的合成样本来训练下游分类器，同时保证微调数据的隐私保护。我们的源代码将在 GitHub 上提供。]]></description>
      <guid>https://arxiv.org/abs/2406.05257</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>VTrans：利用基于变分信息瓶颈的剪枝加速 Transformer 压缩</title>
      <link>https://arxiv.org/abs/2406.05276</link>
      <description><![CDATA[arXiv:2406.05276v1 公告类型：新
摘要：近年来，人们越来越重视为资源受限的设备压缩大型预训练的 Transformer 模型。然而，传统的修剪方法通常不触及嵌入层，导致模型过度参数化。此外，它们需要大量的压缩时间和大数据集来保持修剪模型的性能。为了应对这些挑战，我们提出了 VTrans，这是一个由变分信息瓶颈 (VIB) 原理指导的迭代修剪框架。我们的方法使用 VIB 训练的掩码压缩所有结构组件，包括嵌入、注意头和层。这种方法只保留每层中的必要权重，确保符合指定的模型大小或计算约束。值得注意的是，我们的方法比之前最先进的方法（无论是任务无关的还是任务特定的）实现了高达 70% 的压缩率。我们还提出了我们方法的更快变体：Fast-VTrans 仅利用 3% 的数据，而 Faster-VTrans 是一种时间高效的替代方案，它涉及对 VIB 掩码的独家微调，与以前的方法相比，压缩速度提高了 25 倍，而性能损失最小。在 BERT、ROBERTa 和 GPT-2 模型上进行的大量实验证实了我们方法的有效性。此外，我们的方法在压缩大型模型（例如 LLaMA-2-7B）方面表现出了可扩展性，与以前的剪枝方法相比实现了卓越的性能。此外，我们使用基于注意力的探测来定性评估模型冗余并解释我们方法的效率。值得注意的是，我们的方法将未剪枝模型中对特殊和当前标记具有高度关注的头部视为剪枝的首要候选，而保留的头部则更多地关注任务关键型关键字。]]></description>
      <guid>https://arxiv.org/abs/2406.05276</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>混合曲率决策树和随机森林</title>
      <link>https://arxiv.org/abs/2406.05227</link>
      <description><![CDATA[arXiv:2406.05227v1 公告类型：新
摘要：我们将决策树和随机森林算法扩展到混合曲率乘积空间。此类空间定义为欧几里得、超球面和双曲流形的笛卡尔积，通常可以以比单个流形低得多的失真度嵌入成对距离的点。到目前为止，所有用于乘积空间的分类器都适合单个线性决策边界，并且没有描述回归器。我们的方法通过在乘积流形中实现简单、富有表现力的分类和回归来克服这些限制。我们证明了我们的工具与在环境空间中针对覆盖广泛曲率的组件流形以及所选乘积流形运行的欧几里得方法相比具有更高的准确性。]]></description>
      <guid>https://arxiv.org/abs/2406.05227</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:34 GMT</pubDate>
    </item>
    <item>
      <title>具有稀疏通信的联合 LoRA</title>
      <link>https://arxiv.org/abs/2406.05233</link>
      <description><![CDATA[arXiv:2406.05233v1 公告类型：新
摘要：低秩自适应 (LoRA) 是一种在通信受限的机器学习设置（例如跨设备联邦学习）中进行微调的自然方法。在联邦学习背景下研究 LoRA 的先前工作侧重于提高 LoRA 对异质性和隐私的鲁棒性。在这项工作中，我们考虑进一步提高联邦 LoRA 通信效率的技术。不幸的是，我们表明，通过非结构化修剪来提高 LoRA 效率的集中式 ML 方法不能很好地转移到联邦设置。我们研究了一种简单的方法 \textbf{FLASC}，它在通信过程中将稀疏性应用于 LoRA，同时允许客户端本地微调整个 LoRA 模块。在四个常见的联邦学习任务中，我们证明该方法的性能与密集 LoRA 相当，但通信量减少了 10\times$。此外，尽管该方法主要针对通信而设计，但我们发现，相对于针对这些特定问题量身定制的现有方法，该方法在异质性和隐私方面具有优势。总体而言，我们的工作强调了在开发通信效率高的微调方法时考虑系统特定​​约束的重要性，并为未来的联邦微调工作提供了简单且具有竞争力的基准。]]></description>
      <guid>https://arxiv.org/abs/2406.05233</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:34 GMT</pubDate>
    </item>
    <item>
      <title>CorDA：面向上下文的大型语言模型分解适配</title>
      <link>https://arxiv.org/abs/2406.05223</link>
      <description><![CDATA[arXiv:2406.05223v1 公告类型：新
摘要：当前的参数高效微调 (PEFT) 方法在构建适配器时没有考虑要学习的下游任务的上下文或要维护的重要知识的上下文。因此，与全参数微调相比，通常存在性能差距，同时微调后的模型会遭受预先训练的世界知识的灾难性遗忘。在本文中，我们提出了 CorDA，一种面向上下文的分解自适应方法，该方法从面向下游任务或世界知识的上下文的权重分解构建可学习的适配器。具体来说，我们收集一些数据样本，并使用这些样本对预训练的 LLM 的每个线性层执行奇异值分解，并乘以输入激活的协方差矩阵。通过这样做，通过决定分解方向来捕获代表性样本的上下文。我们的方法支持两种选择，即知识保留的自适应和指令预览的自适应。对于前者，我们使用问答样本来获得协方差矩阵，并使用具有最小$r$奇异值的分解分量来初始化可学习的适配器，同时冻结其他分量以更好地保留世界知识。对于后者，我们使用来自微调任务的指令数据（例如数学或编码）来定向分解并训练最大的$r$个组件，以捕捉要学习的任务的主要特征。我们对数学、代码和指令遵循任务进行了广泛的实验。我们的知识保存自适应不仅在微调任务上取得了比LoRA更好的性能，而且还减轻了世界知识的遗忘。我们的指令预览自适应能够进一步提高微调性能，超越全参数微调和最先进的PEFT方法。]]></description>
      <guid>https://arxiv.org/abs/2406.05223</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:33 GMT</pubDate>
    </item>
    <item>
      <title>图神经网络统计泛化的多方面视角</title>
      <link>https://arxiv.org/abs/2406.05225</link>
      <description><![CDATA[arXiv:2406.05225v1 公告类型：新
摘要：卷积神经网络已成功扩展到图操作，从而产生了图神经网络 (GNN)。GNN 通过连续应用图卷积将来自相邻节点的信息组合起来。GNN 已成功应用于各种学习任务，而对其泛化能力的理论理解仍在进行中。在本文中，我们利用流形理论来分析在流形采样点上构建的图上运行的 GNN 的统计泛化差距。我们研究了 GNN 在节点级和图级任务上的泛化差距。我们表明，泛化差距随着训练图中节点数量的增加而减小，这保证了 GNN 可以泛化到流形上看不见的点。我们在多个真实世界数据集中验证了我们的理论结果。]]></description>
      <guid>https://arxiv.org/abs/2406.05225</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:33 GMT</pubDate>
    </item>
    <item>
      <title>TabPFGen——使用 TabPFN 生成表格数据</title>
      <link>https://arxiv.org/abs/2406.05216</link>
      <description><![CDATA[arXiv:2406.05216v1 公告类型：新
摘要：深度生成模型的进展未能很好地转化为表格数据。我们认为这是由于流行的生成模型和表格数据的判别模型在结构上不匹配造成的。因此，我们设计了一种技术，将 TabPFN（一种最初为上下文判别表格任务设计的高性能转换器）转变为基于能量的生成模型，我们将其称为 TabPFGen。这个新颖的框架利用预先训练的 TabPFN 作为能量函数的一部分，不需要任何额外的训练或超参数调整，从而继承了 TabPFN 的上下文学习能力。我们可以从 TabPFGen 中采样，类似于其他基于能量的模型。我们在标准生成建模任务（包括数据增强、类平衡和归纳）上展示了强大的结果，开辟了表格数据生成的新领域。]]></description>
      <guid>https://arxiv.org/abs/2406.05216</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:32 GMT</pubDate>
    </item>
    <item>
      <title>通过连续梯度协调实现可解释的深度局部学习</title>
      <link>https://arxiv.org/abs/2406.05222</link>
      <description><![CDATA[arXiv:2406.05222v1 公告类型：新
摘要：由于 BP 在生物学上的不合理性和巨大的内存消耗，减轻神经网络训练对全局反向传播 (BP) 的依赖已成为一个值得关注的研究课题。在现有的解决方案中，局部学习优化了具有局部误差的神经网络的梯度隔离模块，并且已被证明即使在大规模数据集上也是有效的。然而，局部误差之间的协调从未被研究过。在本文中，我们首先从理论上研究了非贪婪的逐层训练，并表明当模块中相对于其输入的局部梯度与前一个模块相对于其输出的局部梯度不一致时，无法保证收敛。受理论结果的启发，我们进一步提出了一种局部训练策略，该策略在不破坏梯度隔离或引入任何可学习参数的情况下连续规范相邻模块之间的梯度协调。我们的方法可以集成到局部 BP 和无 BP 设置中。在实验中，我们与以前的方法相比实现了显著的性能提升。特别是，我们在 ImageNet 上针对 CNN 和 Transformer 架构的方法能够获得与全局 BP 相媲美的性能，节省了 40% 以上的内存消耗。]]></description>
      <guid>https://arxiv.org/abs/2406.05222</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:32 GMT</pubDate>
    </item>
    <item>
      <title>评估资源匮乏环境下数据增强对情绪分类的有效性</title>
      <link>https://arxiv.org/abs/2406.05190</link>
      <description><![CDATA[arXiv:2406.05190v1 公告类型：新
摘要：数据增强有可能通过增加可用的训练数据量来提高机器学习模型的性能。在这项研究中，我们使用低资源数据集评估了不同数据增强技术对多标签情绪分类任务的有效性。我们的结果表明，反向翻译优于基于自动编码器的方法，并且每个训练实例生成多个示例可以进一步提高性能。此外，我们发现反向翻译生成了最多样化的一组单元词和三元词。这些发现证明了反向翻译在资源有限的情况下提高情绪分类模型性能的实用性。]]></description>
      <guid>https://arxiv.org/abs/2406.05190</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:31 GMT</pubDate>
    </item>
    <item>
      <title>上下文表格模型的检索和微调</title>
      <link>https://arxiv.org/abs/2406.05207</link>
      <description><![CDATA[arXiv:2406.05207v1 公告类型：新
摘要：表格数据是一种普遍存在的模式，涵盖了广泛的领域，其固有的多样性对深度学习提出了相当大的挑战。使用基于转换器的上下文学习的最新进展已经在较小和不太复杂的数据集上显示出希望，但难以扩展到更大和更复杂的数据集。为了解决这一限制，我们提出了检索和微调的组合：我们可以通过收集最近的邻居将转换器适应数据的局部子集，然后在上下文中使用这组检索到的邻居执行特定于任务的微调。使用 TabPFN 作为基础模型（目前最好的表格上下文学习器）并在顶部应用我们的检索和微调方案，得到我们所谓的局部校准 PFN 或 LoCalPFN。我们对 TabZilla 从 OpenML 中整理的 95 个数据集进行了广泛的评估，在此基础上，我们使用 LoCalPFN 建立了新的最先进水平——甚至与经过调整的基于树的模型相比也是如此。值得注意的是，与基础上下文模型相比，我们的性能有了显著提升，证明了我们方法的有效性，并推动了表格数据深度学习的前沿发展。]]></description>
      <guid>https://arxiv.org/abs/2406.05207</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:31 GMT</pubDate>
    </item>
    <item>
      <title>因式分解诅咒：你预测的哪些标记是逆转诅咒的基础以及更多</title>
      <link>https://arxiv.org/abs/2406.05183</link>
      <description><![CDATA[arXiv:2406.05183v1 公告类型：新
摘要：当今最好的语言模型仍然在与幻觉作斗争：事实上不正确的生成，这阻碍了它们可靠地检索训练期间看到的信息的能力。逆转诅咒，即当以与训练期间遇到的顺序不同的顺序探测信息时，模型无法回忆起信息，这在信息检索中就是一个例子。我们将逆转诅咒重新定义为分解诅咒 - 模型无法在不同的分解下学习相同的联合分布。通过一系列越来越真实的受控实验，包括 WikiReversal，这是我们引入的一种设置，用于紧密模拟知识密集型微调任务，我们发现分解诅咒是流行的大型语言模型中使用的下一个标记预测目标的固有失败。此外，我们证明可靠的信息检索无法通过规模、反转标记甚至简单的双向注意力训练来解决。因此，除非模型已经看到了正确的标记序列，否则对专门数据进行微调的各种方法必然会在下游任务中产生混合结果。在五个不同复杂程度的任务中，我们的结果揭示了一条有希望的前进道路：与分解无关的目标可以显著缓解逆转诅咒，并暗示知识存储和规划能力的提高。]]></description>
      <guid>https://arxiv.org/abs/2406.05183</guid>
      <pubDate>Wed, 12 Jun 2024 03:16:30 GMT</pubDate>
    </item>
    </channel>
</rss>