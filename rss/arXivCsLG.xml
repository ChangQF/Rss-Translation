<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 25 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>高效自适应联邦优化</title>
      <link>https://arxiv.org/abs/2410.18117</link>
      <description><![CDATA[arXiv:2410.18117v1 公告类型：新
摘要：自适应优化在联邦学习中起着关键作用，其中同时进行的服务器和客户端自适应已被证明对于最大化其性能至关重要。然而，联合自适应系统的可扩展性通常受到通信和内存资源有限的限制。在本文中，我们介绍了一类高效的自适应算法，名为 $FedAda^2$，专为大规模跨设备联邦环境而设计。$FedAda^2$ 通过避免在服务器和客户端之间传输预处理器来优化通信效率。同时，它利用客户端的内存高效自适应优化器来减少设备上的内存消耗。从理论上讲，我们证明 $FedAda^2$ 对于一般的非凸目标实现了与直接集成联合自适应性的资源密集型算法相同的收敛速度。从实证角度来看，我们展示了联合自适应性的优势以及 $FedAda^2$ 在图像和文本数据集上的有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.18117</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过聚类细化负采样进行图对比学习，实现半监督文本分类</title>
      <link>https://arxiv.org/abs/2410.18130</link>
      <description><![CDATA[arXiv:2410.18130v1 公告类型：新
摘要：图对比学习（GCL）因其能够从未标记数据中生成自监督信号从而促进模型训练而被广泛应用于文本分类任务。然而，现有的基于 GCL 的文本分类方法经常受到负采样偏差的影响，其中相似的节点被错误地配对为负对。这可能导致过度聚类，其中同一类的实例被分成不同的簇。为了解决过度聚类问题，我们提出了一种基于 GCL 的创新图对比学习方法，通过聚类细化负采样进行半监督文本分类，即 ClusterText。首先，我们将预训练模型 Bert 与图神经网络相结合以学习文本表示。其次，我们引入了一种聚类细化策略，该策略对学习到的文本表示进行聚类以获得伪标签。对于每个文本节点，其负样本集来自不同的簇。此外，我们提出了一种自我修正机制来减轻由于聚类不一致而导致的真负样本的丢失。通过计算每个文本节点与同一簇内其他节点之间的欧几里得距离，距离较远的节点仍被选为负样本。我们提出的 ClusterText 具有良好的可扩展计算能力，因为它可以从大量数据中有效地提取重要信息。实验结果证明了 ClusterText 在文本分类任务中的优越性。]]></description>
      <guid>https://arxiv.org/abs/2410.18130</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MEC-IP：通过整数规划有效发现马尔可夫等价类</title>
      <link>https://arxiv.org/abs/2410.18147</link>
      <description><![CDATA[arXiv:2410.18147v1 公告类型：新
摘要：本文提出了一种新颖的整数规划 (IP) 方法，用于通过观测数据发现贝叶斯网络 (BN) 的马尔可夫等价类 (MEC)。MEC-IP 算法利用独特的团聚焦策略和扩展最大生成图 (EMSG) 来简化对 MEC 的搜索，从而克服了其他现有算法固有的计算限制。我们的数值结果表明，我们的算法不仅显著减少了计算时间，而且在不同数据集中也看到了因果发现准确性的提高。这些发现强调了这种新算法作为因果发现和 BNSL 研究人员和从业人员的强大工具的潜力，为高效准确地分析复杂数据结构提供了重大飞跃。]]></description>
      <guid>https://arxiv.org/abs/2410.18147</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>具有类 SVD 收敛性和平坦最小值的深度自动编码器</title>
      <link>https://arxiv.org/abs/2410.18148</link>
      <description><![CDATA[arXiv:2410.18148v1 公告类型：新
摘要：高维复杂物理系统的表示学习旨在识别低维固有潜在空间，这对于降阶建模和模态分析至关重要。为了克服众所周知的 Kolmogorov 障碍，近年来引入了深度自动编码器 (AE)，但随着潜在空间的秩增加，它们通常会出现较差的收敛行为。为了解决这个问题，我们提出了可学习加权混合自动编码器，这是一种通过可学习加权框架将奇异值分解 (SVD) 与深度自动编码器的优势相结合的混合方法。我们发现引入可学习的加权参数是必不可少的——如果没有它们，生成的模型要么会崩溃为标准 POD，要么无法表现出所需的收敛行为。此外，我们通过经验发现，与其他模型相比，我们训练的模型的锐度要小数千倍。我们对经典混沌 PDE 系统（包括 1D Kuramoto-Sivashinsky 和强制各向同性湍流数据集）进行的实验表明，与几种竞争方法相比，我们的方法显著提高了泛化性能，为高维复杂物理系统的稳健表示学习铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2410.18148</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梦想学习</title>
      <link>https://arxiv.org/abs/2410.18156</link>
      <description><![CDATA[arXiv:2410.18156v1 公告类型：新
摘要：将新颖性融入深度学习系统仍然是一个具有挑战性的问题。向机器学习系统引入新信息可能会干扰先前存储的数据并可能改变全局模型范式，尤其是在处理非平稳源时。在这种情况下，基于验证误差最小化的传统方法提供的优势有限。为了解决这个问题，我们提出了一种受 Stuart Kauffman 的相邻可能概念启发的训练算法。这种新颖的训练方法在学习阶段探索新的数据空间。它使神经网络倾向于顺利接受和整合具有与预期不同的统计特征的数据序列。与这种包含兼容的最大距离取决于特定参数：本方法探索阶段使用的采样温度。这种称为梦想学习的算法可以预测随时间推移的潜在状态转变，从而增强神经网络对改变统计特性的非平稳事件的响应能力。为了评估此方法的优势，我们将此方法应用于马尔可夫链中的意外统计变化和文本序列中的非平稳动态。我们证明了在马尔可夫链发生范式转变的情况下，它能够将生成的文本序列的自相关性提高 $\sim 29\%$，并将损失收敛速度提高 $\sim 100\%$。]]></description>
      <guid>https://arxiv.org/abs/2410.18156</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TabDPT：扩展表格基础模型</title>
      <link>https://arxiv.org/abs/2410.18164</link>
      <description><![CDATA[arXiv:2410.18164v1 公告类型：新
摘要：神经网络在表格数据上面临的挑战是有据可查的，并且阻碍了表格基础模型的发展。利用上下文学习 (ICL) 的技术在这里显示出希望，可以动态适应看不见的数据。ICL 可以为全新的数据集提供预测，而无需进一步训练或超参数调整，因此在遇到新任务时提供非常快速的推理。然而，扩展表格数据的 ICL 仍然是一个问题：基于大型语言模型的方法无法有效地处理数字表，而表格特定的技术无法有效地利用真实数据的力量来提高性能和泛化能力。我们能够通过在真实数据上训练表格特定的基于 ICL 的架构并使用自监督学习和检索来克服这些挑战，结合两全其美。我们最终的模型——表格判别式预训练 Transformer (TabDPT)——在无需针对特定任务进行微调的情况下，在 CC18（分类）和 CTR23（回归）基准上实现了最佳性能，展示了模型预训练后 ICL 的适应性和速度。随着模型大小和可用数据量的增加，TabDPT 还展示了强大的扩展能力，这表明未来只需通过整理更大的表格预训练数据集和训练更大的模型即可实现改进。]]></description>
      <guid>https://arxiv.org/abs/2410.18164</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ZIP-FIT：通过基于压缩的对齐进行无嵌入数据选择</title>
      <link>https://arxiv.org/abs/2410.18194</link>
      <description><![CDATA[arXiv:2410.18194v1 公告类型：新
摘要：数据选择对于优化特定任务上的语言模型 (LM) 性能至关重要，但大多数现有方法未能有效考虑目标任务分布。
当前的方法要么完全忽略特定任务的要求，要么依赖于无法捕捉自动形式化或代码生成等任务所需的细微模式的近似值。
考虑目标分布的方法通常依赖于简单、有时嘈杂的表示，如散列 n-gram 特征，这可能导致冲突并引入噪音。
我们引入了 ZIP-FIT，这是一个数据选择框架，它使用 gzip 压缩来直接测量潜在训练数据与目标任务分布之间的一致性。
在对自动形式化和 Python 代码生成的广泛评估中，ZIP-FIT 的表现明显优于 DSIR 和 D4 等领先基线。
在 ZIP-FIT 选择的数据上训练的模型实现了最低交叉熵损失，比基线快 85.1\%，表明更好的任务对齐可以提高学习效率。
此外，ZIP-FIT 的选择速度比 DSIR 快 65.8\%，比 D4 快两个数量级。
值得注意的是，ZIP-FIT 表明，较小、对齐良好的数据集通常优于较大但针对性较差的数据集，这表明少量高质量数据优于大量低质量数据。
我们的结果表明，任务感知数据选择对于有效的域适应至关重要，而压缩提供了一种衡量任务对齐的原则性方法。
通过表明有针对性的数据选择可以显著提高特定任务的性能，我们的工作为数据质量、任务对齐和模型学习效率之间的关系提供了新的见解。]]></description>
      <guid>https://arxiv.org/abs/2410.18194</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>增强大型语言模型的有效推理</title>
      <link>https://arxiv.org/abs/2410.18248</link>
      <description><![CDATA[arXiv:2410.18248v1 公告类型：新
摘要：增强型大型语言模型 (LLM) 通过 API 调用集成外部数据源，增强了独立 LLM 的功能。在交互式 LLM 应用程序中，高效的调度对于保持较低的请求完成时间至关重要，直接影响用户参与度。然而，由于需要管理缓存信息 (KV 缓存) 的有限内存，这些增强带来了调度挑战。因此，传统的基于大小的调度算法，例如最短作业优先 (SJF)，在最小化完成时间方面变得不那么有效。现有的工作仅关注通过保留、丢弃或交换内存来处理 API 调用期间的请求，而不考虑如何使用 API 调用来调度请求。在本文中，我们提出了 LAMPS，一种用于增强型 LLM 的新型 LLM 推理框架。LAMPS 通过统一的调度方法来最小化请求完成时间，该方法考虑了请求的总长度及其在 API 调用期间的处理策略。认识到 LLM 推理受内存限制，我们的方法根据请求在一段时间内的内存消耗对请求进行排序，这取决于输出大小以及请求在其 API 调用期间的管理方式。为了实现我们的调度，LAMPS 预测了在请求在其 API 调用期间最小化内存浪费的策略，与现有方法保持一致但有所改进。我们还提出了预防饥饿的技术和优化，以减轻我们的调度开销。我们在 vLLM 之上实现了 LAMPS，并根据基线 LLM 推理系统评估了其性能，与现有的增强型 LLM 系统相比，端到端延迟提高了 27%-85%，TTFT 降低了 4%-96%，甚至比 vLLM 的收益更大。]]></description>
      <guid>https://arxiv.org/abs/2410.18248</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>异步 RLHF：更快、更高效的语言模型离线策略 RL</title>
      <link>https://arxiv.org/abs/2410.18252</link>
      <description><![CDATA[arXiv:2410.18252v1 公告类型：新
摘要：RLHF 的主导范式是在线和基于策略的 RL：从大型语言模型 (LLM) 策略同步生成，使用奖励模型进行标记，并使用 LLM 自身输出的反馈进行学习。虽然这种范式性能良好，但计算效率低下。受经典深度 RL 文献的启发，我们提出在 RLHF 中将生成和学习分开。这使得异步生成新样本的同时对旧样本进行训练成为可能，从而加快训练速度并实现更优化的计算扩展。然而，异步训练依赖于一种未被充分探索的机制，即在线但基于策略的 RLHF：从我们模型的先前迭代中学习样本。为了了解这种机制中的挑战，我们研究了一个基本问题：我们可以容忍多少基于策略的异步训练以加快学习速度但保持性能？在我们测试的几种 RLHF 算法中，我们发现在线 DPO 对离线策略数据最具有鲁棒性，并且鲁棒性会随着策略模型的规模而增加。我们进一步研究了异步 RLHF 的计算优化，但发现它们会以性能为代价，从而产生权衡。最后，我们通过在指令跟踪任务上训练 LLaMA 3.1 8B 来验证异步 RLHF 的可扩展性，该任务比同步运行快 40%，同时与最终性能相匹配。]]></description>
      <guid>https://arxiv.org/abs/2410.18252</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>辛神经积分器的哈密顿匹配</title>
      <link>https://arxiv.org/abs/2410.18262</link>
      <description><![CDATA[arXiv:2410.18262v1 公告类型：新
摘要：汉密尔顿运动方程构成了物理学各个分支的基本框架，包括天文学、量子力学、粒子物理学和气候科学。通常使用经典数值求解器来计算这些系统的时间演化。但是，当系统跨越多个空间和时间尺度时，数值误差会累积，从而导致精度降低。为了应对在长时间尺度上发展此类系统的挑战，我们提出了一种基于神经网络的新型辛积分器 SympFlow，它是由一系列参数化时间相关汉密尔顿函数的精确流图组成的。该架构允许进行后向误差分析：我们可以识别架构的底层汉密尔顿函数并使用它来定义汉密尔顿匹配目标函数，我们将其用于训练。在数值实验中，我们表明 SympFlow 表现出有希望的结果，其定性能量守恒行为类似于时间步进辛积分器。]]></description>
      <guid>https://arxiv.org/abs/2410.18262</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用矢量量化变分自动编码器增强训练数据以对射频信号进行分类</title>
      <link>https://arxiv.org/abs/2410.18283</link>
      <description><![CDATA[arXiv:2410.18283v1 公告类型：新
摘要：几十年来，射频 (RF) 通信一直是民用和军用通信的重要组成部分。随着无线环境的日益复杂以及共享频谱的设备数量的不断增加，有效管理和分类这些频率上的信号变得至关重要。在这种情况下，准确分类无线信号对于有效的频谱管理、信号拦截和干扰缓解至关重要。然而，由于标记训练数据的可用性有限，无线射频信号的分类通常面临挑战，尤其是在低信噪比 (SNR) 条件下。为了应对这些挑战，本文提出使用矢量量化变分自动编码器 (VQ-VAE) 来增强训练数据，从而提高基线无线分类器的性能。VQ-VAE 模型生成高保真合成射频信号，通过捕获射频通信信号固有的复杂变化来增加训练数据集的多样性和保真度。我们的实验结果表明，结合 VQ-VAE 生成的数据可显著提高基线模型的分类准确度，尤其是在低 SNR 条件下。这种增强可提高分类器的泛化能力和鲁棒性，从而克服有限的真实数据带来的限制。通过改进 RF 信号分类，所提出的方法可提高民用和战术环境中无线通信的效率，确保可靠和安全的运营。这一进步支持在通信保真度至关重要的环境中做出关键决策和做好作战准备。]]></description>
      <guid>https://arxiv.org/abs/2410.18283</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用基于元音的集成学习方法从语音中进行稳健且可解释的抑郁症识别</title>
      <link>https://arxiv.org/abs/2410.18298</link>
      <description><![CDATA[arXiv:2410.18298v1 公告类型：新
摘要：本研究调查了可解释的机器学习算法，用于从语音中识别抑郁症。基于语音产生的证据，抑郁症会影响运动控制和元音生成，使用预先训练的基于元音的嵌入，整合语义上有意义的语言单元。随后，集成学习方法将问题分解为具有特定抑郁症状和严重程度的组成部分。探索了两种方法：一种是“自下而上”的方法，其中 8 个模型预测单个患者健康问卷-8 (PHQ-8) 项目分数，另一种是“自上而下”的方法，使用混合专家 (MoE) 和路由器模块来评估抑郁症严重程度。这两种方法都表现出与最先进的基线相当的性能，表现出稳健性和对数据集平均值/中值的降低敏感性。讨论了系统可解释性的好处，强调了它们在协助临床医生进行抑郁症诊断和筛查方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.18298</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>CoreInfer：利用语义启发的自适应稀疏激活加速大型语言模型推理</title>
      <link>https://arxiv.org/abs/2410.18311</link>
      <description><![CDATA[arXiv:2410.18311v1 公告类型：新
摘要：具有数十亿个参数的大型语言模型 (LLM) 引发了新一波令人兴奋的 AI 应用。然而，它们在推理过程中的高计算成本和内存需求带来了重大挑战。自适应稀疏激活推理为每个 token 仅激活少量神经元，提供了一种在不降低性能的情况下加速模型推理的新方法，显示出对资源受限的硬件设备的巨大潜力。然而，现有方法基于带有附加 MLP 的单个 token 来预测激活的神经元，这涉及激活图和资源调用的频繁变化，限制了稀疏激活的加速优势。在本文中，我们介绍了 CoreInfer，一种基于句子级预测的无 MLP 自适应稀疏激活推理方法。具体而言，我们提出了逐句核心神经元的概念，它指的是给定句子最关键的神经元子集，并通过经验证明了其有效性。为了确定核心神经元，我们探索了核心神经元与句子语义之间的相关性。值得注意的是，我们发现核心神经元与句子语义既有稳定性又有相似性——这是以前的研究忽视的见解。基于这一发现，我们进一步设计了两种基于语义的方法来预测核心神经元以适应不同的输入场景。在 CoreInfer 中，核心神经元在预填充阶段确定，在编码阶段固定，从而实现零成本稀疏推理。我们在各种模型和任务中评估了 CoreInfer 的模型泛化和任务泛化。值得注意的是，在 NVIDIA TITAN XP GPU 上，与 Huggingface 实现和 PowerInfer 相比，CoreInfer 分别实现了 10.33 倍和 2.72 倍的加速。]]></description>
      <guid>https://arxiv.org/abs/2410.18311</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>时间序列的自监督学习：对 FITS 的回顾与批判</title>
      <link>https://arxiv.org/abs/2410.18318</link>
      <description><![CDATA[arXiv:2410.18318v1 公告类型：新
摘要：准确的时间序列预测是一项非常有价值的工作，可应用于许多行业。尽管最近深度学习取得了进展，模型复杂性增加，模型规模更大，但许多最先进的模型通常表现更差或与简单模型相当。其中一个案例是最近提出的模型 FITS，它声称具有竞争力的性能，并且参数数量显着减少。通过在复杂的频域中训练单层神经网络，我们能够复制这些结果。我们在广泛的现实世界数据集上进行的实验进一步表明，FITS 特别擅长捕捉周期性和季节性模式，但在趋势、非周期性或随机相似行为方面却举步维艰。我们尝试通过两种新颖的混合方法，将 FITS 与 DLinear 相结合来弥补其弱点，在多元回归方面取得了任何已知开源模型的最佳结果，并在价格数据集的多元/线性回归方面取得了良好的结果，同时还大大提高了 FITS 作为独立模型所取得的成就。]]></description>
      <guid>https://arxiv.org/abs/2410.18318</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用欧几里得距离校准深度神经网络</title>
      <link>https://arxiv.org/abs/2410.18321</link>
      <description><![CDATA[arXiv:2410.18321v1 公告类型：新
摘要：不确定性是现实世界场景的一个基本方面，其中很少有完美的信息。人类自然会开发复杂的内部模型来处理不完整的数据并有效应对不可预见或部分观察到的事件。在机器学习中，焦点损失通常用于通过强调难以分类的样本来降低错误分类率。然而，它不能保证校准良好的预测概率，并且可能导致模型过度自信或信心不足。高校准误差表示预测概率与实际结果不一致，从而影响模型可靠性。这项研究引入了一种称为焦点校准损失（FCL）的新型损失函数，旨在改进概率校准，同时保留焦点损失在处理困难样本方面的优势。通过严格适当的损失最小化欧几里得范数，FCL 惩罚实例校准误差并限制界限。我们对所提出的方法进行了理论验证，并将其应用于校准 CheXNet，以便在基于网络的医疗保健系统中进行部署。对各种模型和数据集的广泛评估表明，我们的方法在校准和准确度指标方面都达到了 SOTA 性能。]]></description>
      <guid>https://arxiv.org/abs/2410.18321</guid>
      <pubDate>Fri, 25 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>