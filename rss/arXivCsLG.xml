<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.lg arxiv.org上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.lg arxiv.org e-print档案中的更新。</description>
    <lastBuildDate>Thu, 13 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>卫星观察结果以任意分辨率的准确气象状态引导扩散模型</title>
      <link>https://arxiv.org/abs/2502.07814</link>
      <description><![CDATA[ARXIV：2502.07814V1公告类型：新 
摘要：在任意位置准确获取表面气象条件对于天气预测和气候模拟至关重要。由于通常以低分辨率网格场的形式提供了从卫星观测到的气象状态，因此与实际观察结果相比，空间插值直接应用空间插值以获得特定位置的气象状态。以较高分辨率获取气象状态信息的现有降尺度方法通常忽略与卫星观察的相关性。为了弥合差距，我们提出了卫星观察引导扩散模型（SGD），这是一种条件扩散模型，在ERA5重新分析数据中以卫星观测（GridSat）为条件，作为条件，用于通过对通过零点进行零量的微观学态来取样的条件指导采样策略和基于补丁的方法。在训练过程中，我们建议通过注意机制将来自Gridsat卫星观测值的信息融合到ERA5地图中，从而使SGD能够生成更准确地与实际条件保持一致的大气状态。在抽样中，我们采用了可优化的卷积内核来模拟高档过程，从而使用低分辨率ERA5地图以及从气象站的观察来生成高分辨率ERA5地图作为指导。此外，我们设计的基于补丁的方法促进了SGD，以在任意决议下生成气象状态。实验证明SGD实现了准确的气象状态，缩小到6.25公里。]]></description>
      <guid>https://arxiv.org/abs/2502.07814</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>隐式语言模型是RNN：平衡并行化和表现力</title>
      <link>https://arxiv.org/abs/2502.07827</link>
      <description><![CDATA[ARXIV：2502.07827V1公告类型：新 
摘要：状态空间模型（SSM）和变形金刚主导着语言建模格局。但是，与经典的复发性神经网络（RNN）相比，它们被限制为较低的计算复杂性，从而限制了它们的表现力。相比之下，RNN在训练过程中缺乏并行化，从而提出了有关并行化和表现力之间的权衡的基本问题。我们提出了隐式SSM，该隐式SSM迭代转换直至收敛到固定点。从理论上讲，我们表明隐式SSM实现了RNN的非线性状态过渡。从经验上讲，我们发现只有近似固定点的收敛就足够了，从而实现了可扩展的训练课程的设计，该课程在很大程度上保留了并行化，仅需一小部分令牌即可完全收敛。我们的方法表明，对普通语言的卓越国家跟踪功能超越了变形金刚和SSM。我们进一步将隐式SSM扩展到自然语言推理任务和大规模语言模型的预读，最高为207b代币的1.3b参数 - 据我们所知，这是迄今为止训练有素的最大隐式模型。值得注意的是，我们的隐式模型在标准基准上的明确表现优于其明确的同类。]]></description>
      <guid>https://arxiv.org/abs/2502.07827</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>尖锐：通过与恢复参数共享相邻层来加速语言模型推断</title>
      <link>https://arxiv.org/abs/2502.07832</link>
      <description><![CDATA[ARXIV：2502.07832V1公告类型：新 
摘要：尽管大型语言模型（LLM）具有高级的自然语言处理任务，但它们不断增长的计算和内存需求使得在资源受限的设备（如移动电话）上部署越来越具有挑战性。在本文中，我们提出了Sharp（与恢复参数共享相邻层），这是一种通过在相邻层上共享参数来加速LLM推断的新方法，从而减少了存储器负载的开销，同时引入了低排名的恢复参数以维持性能。灵感来自观察到连续层具有相似输出的启发，Sharp采用了两个阶段的恢复过程：单层热身（SLW）和监督的微调（SFT）。 SLW阶段使用L_2损失对齐共享层的输出，为以下SFT阶段提供了良好的初始化，以进一步恢复模型性能。广泛的实验表明，Sharp可以使用不超过50k的微调数据来恢复模型在各种分配任务上的困惑，同时将存储的MLP参数数量减少38％至65％。我们还进行了几项尖锐的消融研究，并表明将图层取代朝向模型的后期部分可以更好地保留性能，并且在匹配参数计数时，不同的恢复参数的性能相似。此外，与原始的移动设备上的原始Llama2-7b型号相比，Sharp可节省42.8％的模型存储，并将总推断时间降低42.2％。我们的结果强调了敏锐的解决方案，是减少在部署LLM的推理成本的有效解决方案，而无需预处理规模的资源。]]></description>
      <guid>https://arxiv.org/abs/2502.07832</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>情绪性脑电图分类使用高尺度的连接矩阵</title>
      <link>https://arxiv.org/abs/2502.07843</link>
      <description><![CDATA[ARXIV：2502.07843V1公告类型：新 
摘要：在有关情绪脑电图分类的最新研究中，连通性矩阵已成功用作卷积神经网络（CNN）的输入，这些卷积神经网络（CNN）可以有效地考虑EEG中的区域间相互作用模式。但是，我们发现这种方法有一个限制，即在CNN的卷积操作过程中，连通性矩阵中的重要模式可能会丢失。为了解决这个问题，我们提出并验证一个想法，以提高连接矩阵以加强当地模式。实验结果表明，这个简单的想法可以显着提高分类性能。]]></description>
      <guid>https://arxiv.org/abs/2502.07843</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>了解无分类器指导：高维理论和非线性概括</title>
      <link>https://arxiv.org/abs/2502.07849</link>
      <description><![CDATA[ARXIV：2502.07849V1公告类型：新 
摘要：最近的研究引起了人们对无分类器指导（CFG）有效性的担忧，表明在低维度中，它可能导致目标分布过度并降低样品多样性。在这项工作中，我们证明，在无限和足够的高维环境中，CFG有效地再现了目标分布，从而揭示了差异的结果。此外，我们探索有限维效应，精确地表征了过冲和降低方差。基于我们的分析，我们引入了CFG的非线性概括。通过对高斯混合物和班级条件和文本形象扩散模型实验的数值模拟，我们验证了我们的分析，并表明我们的非线性CFG提供了提高的灵活性和发电质量，而无需额外的计算成本。]]></description>
      <guid>https://arxiv.org/abs/2502.07849</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过注意机制提高热量需求的预测：机遇和挑战</title>
      <link>https://arxiv.org/abs/2502.07854</link>
      <description><![CDATA[ARXIV：2502.07854V1公告类型：新 
摘要：全球领导人和政策制定者在他们对脱碳式努力方面的明确承诺中统一了支持零零协议。地区供暖系统（DHS）虽然由于持续依赖化石燃料用于热量生产而导致碳排放量，但尽管具有某种脆弱感，但仍在采用更可持续的实践，因为它可能会限制其适应动态需求和生产方案的能力。随着人口需求的增长，可再生能源成为脱碳供暖部门的核心策略，对准确的需求预测的需求加剧了。数字化的进步为基于机器学习的解决方案铺平了道路，以成为建模复杂时间序列模式的行业标准。在本文中，我们专注于建立一个深度学习（DL）模型，该模型使用了独立变量和因变量的解构组件，这些变量会影响热需求，作为对头部需求进行多步骤预测的功能。该模型表示时间频空间中的输入特征，并使用注意机制来生成准确的预测。在现实世界数据集上评估了所提出的方法，并根据LSTM和CNN的预测模型评估了预测性能。在不同的供应区域中，基于注意力的模型在定量和定性上优于基准，平均绝对误差（MAE）为0.105，标准偏差为0.06kW H，平均绝对百分比误差（MAPE）为5.4％，标准差为5.4％偏差为2.8％，相比之下，第二最佳模型为0.10，标准偏差为0.06kW H，MAPE为5.6％，标准偏差为3％。]]></description>
      <guid>https://arxiv.org/abs/2502.07854</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MAAT：MAMBA自适应异常变压器与时间序列的关联差异</title>
      <link>https://arxiv.org/abs/2502.07858</link>
      <description><![CDATA[ARXIV：2502.07858V1公告类型：新 
摘要：时间序列中的异常检测对于工业监测和环境传感至关重要，但是将异常与复杂模式区分开来仍然具有挑战性。现有的方法（例如异常变压器和DCDETECTOR）已经取得了进展，但是它们面临着诸如对短期环境的敏感性以及嘈杂，非平稳环境的效率低下的限制。
  为了克服这些问题，我们介绍了MAAT，这是一种改进的体系结构，可增强关联差异建模和重建质量。 MAAT具有稀疏的注意力，通过关注相关时间步骤有效地捕获长期依赖性，从而降低计算冗余。此外，使用跳过连接和门控注意来改善异常定位和检测性能，将曼巴省选择性状态空间模型纳入重建模块。
  广泛的实验表明，MAAT显着胜过以前的方法，在各种时间序列应用中实现了更好的异常性和概括性，在现实世界中为无监督时间序列异常检测设定了新的标准。]]></description>
      <guid>https://arxiv.org/abs/2502.07858</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>BalanceKV：通过差异理论压缩KV缓存压缩</title>
      <link>https://arxiv.org/abs/2502.07861</link>
      <description><![CDATA[ARXIV：2502.07861V1公告类型：新 
摘要：大型语言模型（LLMS）取得了令人印象深刻的成功，但是他们的高内存需求对长篇文字代币产生了挑战。长篇小说LLM的内存复杂性主要是由于需要将键值（KV）嵌入在其KV缓存中。我们提出了BalanceKv，这是一种基于Banaszczyk的矢量平衡理论的几何抽样过程的KV缓存压缩方法，该过程介绍了由密钥和价值代币的几何形状所告知的依赖性，并提高了精度。 BalanceKV对现有方法提供了理论上证明和经验验证的绩效改进。]]></description>
      <guid>https://arxiv.org/abs/2502.07861</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ADMN：层面的自适应多模式网络，用于动态输入噪声和计算资源</title>
      <link>https://arxiv.org/abs/2502.07862</link>
      <description><![CDATA[ARXIV：2502.07862V1公告类型：新 
摘要：由于多种感应方式提供的鲁棒性，多模式深度学习系统被部署在动态场景中。然而，他们在不同的计算资源可用性（由于多租户，设备异质性等）以及输入质量的波动（来自传感器饲料饲料腐败，环境噪声等）方面挣扎。当前的多模式系统采用静态资源提供，并且当计算资源随时间变化时，无法轻易适应。此外，他们对使用固定特征提取器处理传感器数据的依赖不足以处理方式质量的变化。因此，诸如噪音高的非信息模式（例如那些具有高噪音的方式）可以更好地分配资源，以分配给其他方式。我们提出了ADMN，这是一个能够应对这两个挑战的层次自适应深度多模式网络 - 它调整了所有模式中的活动层的总数以满足计算资源限制，并根据其模态质量不断地跨输入方式重新分层。我们的评估展示了ADMN可以匹配最先进的网络的准确性，同时降低了其浮点操作的75％。]]></description>
      <guid>https://arxiv.org/abs/2502.07862</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Transmla：您需要的全部潜在关注</title>
      <link>https://arxiv.org/abs/2502.07864</link>
      <description><![CDATA[ARXIV：2502.07864V2公告类型：新 
摘要：现代大型语言模型（LLMS）经常在当前硬件上遇到通信瓶颈，而不是纯粹的计算约束。多头潜在注意力（MLA）通过在钥匙值（KV）层中使用低级矩阵来应对这一挑战，从而使压缩潜在的KV状态被缓存。相对于传统的多头关注，这种方法大大降低了KV缓存的大小，从而更快地推断了推断。此外，MLA采用了上注矩阵来提高表现力，以减少通信开销的额外计算。尽管MLA在DeepSeek V2/V3/R1中表现出效率和有效性，但许多主要的模型提供商仍然依靠小组查询关注（GQA），并且尚未宣布任何采用MLA的计划。在本文中，我们表明GQA始终可以由MLA表示，同时保持相同的KV缓存开销，但相反的情况不存在。为了鼓励更广泛地使用MLA，我们介绍了TransMLA，TransMLA是一种训练后方法，该方法将广泛使用的基于GQA的模型（例如Llama，Qwen，Mixtral）转换为基于MLA的模型。转换后，该模型可以进行额外的训练以提高表现力而不增加KV缓存大小。此外，我们计划开发MLA特异性的推理加速技术，以保持转换模型中的潜伏期低，从而可以对DeepSeek R1进行更有效的蒸馏。]]></description>
      <guid>https://arxiv.org/abs/2502.07864</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过离线数据积极优势与在线增强学习</title>
      <link>https://arxiv.org/abs/2502.07937</link>
      <description><![CDATA[ARXIV：2502.07937V1公告类型：新 
摘要：在线加强学习（RL）通过与环境的直接互动来增强政策，但面临与样本效率相关的挑战。相比之下，离线RL利用大量的预先收集的数据来学习政策，但由于数据覆盖率有限，因此通常会产生次优的结果。最近的努力试图整合离线和在线RL，以利用这两种方法的优势。但是，由于包括灾难性遗忘，缺乏稳健性和样本效率在内的问题，有效地结合在线和离线RL仍然具有挑战性。为了应对这些挑战，我们引入了A3 RL，这是一种新颖的方法，该方法积极从在线和离线资源中选择数据以优化策略改进。我们提供理论保证，以验证我们的主动抽样策略的有效性，并进行彻底的经验实验，以表明我们的方法优于使用离线数据的现有最新在线RL技术。我们的代码将在以下网址公开获取：https：//github.com/xuefeng-cs/a3rl。]]></description>
      <guid>https://arxiv.org/abs/2502.07937</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>VSC-RL：使用各种子观念的强化学习来推进自主视力语言代理</title>
      <link>https://arxiv.org/abs/2502.07949</link>
      <description><![CDATA[ARXIV：2502.07949V1公告类型：新 
摘要：最新的（SOTA）强化学习（RL）方法使视觉语言代理可以在没有人类监督的情况下从与环境的互动中学习。但是，他们在学习效率低下时遇到困难，以应对现实世界中复杂的顺序决策任务，尤其是在稀疏的奖励信号和长马依赖性的情况下。为了有效解决该问题，我们引入了各种子观念的RL（VSC-RL），该问题将视觉语言的顺序决策任务重新制定为变分目标调节的RL问题，从而使我们能够利用先进的优化方法来提高学习效率。具体而言，VSC-RL优化了子观念下限（SGC-ELBO），该证据包括（a）通过RL最大化亚属条件的回报，（b）最大程度地减少参考策略的子观念差异。从理论上讲，我们证明了SGC-Elbo等同于原始优化目标，从而在不牺牲绩效保证的情况下确保了提高学习效率。此外，对于现实世界中复杂的决策任务，VSC-RL利用视觉模型将目标自主分解为可行的子目标，从而实现了有效的学习。在各种基准中，包括挑战现实世界的移动设备控制任务，VSC-RL显着超过了SOTA视觉语言代理，从而实现了卓越的性能和显着的学习效率。]]></description>
      <guid>https://arxiv.org/abs/2502.07949</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ESPFORMER：带有预期切片运输计划的双重关注</title>
      <link>https://arxiv.org/abs/2502.07962</link>
      <description><![CDATA[ARXIV：2502.07962V1公告类型：新 
摘要：虽然自我注意力在变压器的成功中发挥了作用，但在训练过程中可能会导致一些令牌对一些令牌的重点，从而导致次优信息流。已经显示，在注意矩阵中强制执行双重构成约束可以改善注意力分布的结构和平衡。但是，现有的方法依赖于迭代的沉没归一化，这在计算上是昂贵的。在本文中，我们介绍了一种基于切片的最佳运输，利用预期的切片运输计划（ESP）的新型，完全可行的双重性注意机制。与先前的方法不同，我们的方法在没有迭代sindhorn归一化的情况下实现了双随机性，从而显着提高了效率。为了确保不同的性能，我们结合了一种基于温度的软分排序技术，将无缝集成到深度学习模型中。跨多个基准数据集的实验，包括图像分类，点云分类，情感分析和神经机器翻译，表明我们的增强注意力正规化始终提高不同应用程序的性能。]]></description>
      <guid>https://arxiv.org/abs/2502.07962</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>比较肿瘤生长的经典和神经ODE模型的新工具</title>
      <link>https://arxiv.org/abs/2502.07964</link>
      <description><![CDATA[ARXIV：2502.07964V1公告类型：新 
摘要：引入了一种用于建模肿瘤生长的新计算工具肿瘤生长。该工具允许将标准教科书模型（例如Bertalanffy和Gompertz）与一些较新的型号进行比较，包括首次使用神经ODE模型。作为应用，我们在接受两种不同治疗选择的患者中重新审视非小细胞肺癌和膀胱癌病变的人类元研究，以确定先前报道的性能差异是否具有统计学意义，并且更复杂的模型是否更新，是否更复杂更好。在至少四个时间体积测量的示例中，可以进行校准，平均约为6.3，我们的主要结论是，普通的Bertalanffy模型平均具有较高的性能。但是，如果有更多测量值可用，我们认为更复杂的模型，能够捕获反弹和复发行为，可能是更好的选择。]]></description>
      <guid>https://arxiv.org/abs/2502.07964</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>在图表上分布概括的生成风险最小化</title>
      <link>https://arxiv.org/abs/2502.07968</link>
      <description><![CDATA[ARXIV：2502.07968V1公告类型：新 
摘要：图表上的分布（OOD）概括旨在处理测试图分布与训练图分布不同的方案。与I.I.D.相比图像之类的数据，由于非i.i.d，图形结构数据上的OOD泛化问题仍然具有挑战性。图形上的属性和复杂的结构信息。最近，关于图形OOD概括的几项作品探索了提取不变子图，这些子图在不同分布中共享关键的分类信息。然而，这样的策略对于完全捕获不变的信息可能是最理想的，因为离散结构的提取可能会导致不变信息的丢失或涉及虚假信息。在本文中，我们提出了一个名为“生成风险最小化”（GRM）的创新框架，旨在为要分类的每个输入图生成一个不变的子图，而不是提取。为了在没有最佳不变子图（即地面真理）的情况下应对优化的挑战，我们通过引入潜在的因果变量来得出所提出的GRM目标的可拖动形式，其有效性通过我们的理论分析来验证。我们进一步对节点级别和图形OOD泛化的各种现实图形数据集进行了广泛的实验，结果证明了我们框架GRM的优势。]]></description>
      <guid>https://arxiv.org/abs/2502.07968</guid>
      <pubDate>Thu, 13 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>