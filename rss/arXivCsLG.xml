<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Thu, 08 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>使用真实数据和替代数据进行学习的缩放法则</title>
      <link>https://arxiv.org/abs/2402.04376</link>
      <description><![CDATA[收集大量高质量数据通常成本高昂或不切实际，并且是机器学习的关键瓶颈。相反，我们可以使用来自更容易访问的来源（例如公共数据集、在不同情况下收集的数据或由生成模型合成的数据）的数据来扩充目标分布中的一小组 $n$ 数据点。为了模糊区别，我们将此类数据称为“替代数据”。
  我们定义了一个将代理数据集成到训练中的简单方案，并使用理论模型和实证研究来探索其行为。我们的主要发现是： $(i)$ 整合替代数据可以显着减少原始分布的测试误差； $(ii)$ 为了获得这种好处，使用最优加权经验风险最小化至关重要； $(iii)$ 在真实数据和替代数据的混合上训练的模型的测试误差可以通过比例定律很好地描述。这可用于预测最佳权重和替代数据的增益。]]></description>
      <guid>https://arxiv.org/abs/2402.04376</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:17 GMT</pubDate>
    </item>
    <item>
      <title>$\texttt{NeRCC}$：弹性分布式预测服务系统的嵌套回归编码计算</title>
      <link>https://arxiv.org/abs/2402.04377</link>
      <description><![CDATA[针对落后者的弹性是预测服务系统的关键要素，其任务是对预先训练的机器学习模型的输入数据执行推理。在本文中，我们提出 NeRCC，作为近似编码计算的通用抗掉队框架。 NeRCC 包括三层：(1) 编码回归和采样，生成编码数据点，作为原始数据点的组合；(2) 计算，其中一组工作人员对编码数据点进行推理；(3) 解码回归和采样，从编码数据点的可用预测中近似恢复原始数据点的预测。我们认为该框架的总体目标揭示了编码层和解码层中两个回归模型之间的潜在互连。我们通过总结嵌套回归问题对联合优化的两个正则化项的依赖性，提出了嵌套回归问题的解决方案。我们对不同数据集和各种机器学习模型（包括 LeNet5、RepVGG 和 Vision Transformer (ViT)）进行的广泛实验表明，NeRCC 在各种落后者中准确地逼近原始预测，比最先进的模型高出至 23%。]]></description>
      <guid>https://arxiv.org/abs/2402.04377</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:17 GMT</pubDate>
    </item>
    <item>
      <title>微调语言模型生成稳定的无机材料作为文本</title>
      <link>https://arxiv.org/abs/2402.04379</link>
      <description><![CDATA[我们建议微调大型语言模型以生成稳定的材料。虽然非正统，但在文本编码的原子数据上微调大型语言模型实现起来很简单而且可靠，大约 90% 的采样结构遵守原子位置和电荷的物理约束。使用来自学习的 ML 势和黄金标准 DFT 计算的船体能量计算，我们表明我们最强大的模型（微调的 LLaMA-2 70B）可以以大约两倍的速度生成预计亚稳态的材料（49% vs 28%） ）CDVAE，一种竞争扩散模型。由于文本提示固有的灵活性，我们的模型可以同时用于稳定材料的无条件生成、部分结构的填充和文本条件生成。最后，我们表明语言模型捕获晶体结构关键对称性的能力随着模型规模的增加而提高，这表明预训练的 LLM 的偏差非常适合原子数据。]]></description>
      <guid>https://arxiv.org/abs/2402.04379</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:17 GMT</pubDate>
    </item>
    <item>
      <title>神经网络学习复杂性不断增加的统计数据</title>
      <link>https://arxiv.org/abs/2402.04362</link>
      <description><![CDATA[分布简单性偏差 (DSB) 假设神经网络首先学习数据分布的低阶矩，然后再学习高阶相关性。在这项工作中，我们为 DSB 提供了令人信服的新证据，表明网络会自动学习在最大熵分布上表现良好，其低阶统计数据在训练早期与训练集的统计数据相匹配，然后失去这种能力。我们还通过证明 token $n$-gram 频率和嵌入向量矩之间的等价性，并通过寻找 LLM 偏差的经验证据，将 DSB 扩展到离散域。最后，我们使用最佳传输方法对一个类别的低阶统计数据进行外科手术编辑，以匹配另一类别的低阶统计数据，并表明早期训练网络将编辑后的样本视为从目标类别中提取的样本。代码可在 https://github.com/EleutherAI/features-across-time 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.04362</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:16 GMT</pubDate>
    </item>
    <item>
      <title>限制在边际保留、差分隐私、合成数据上训练的线性模型的过度风险</title>
      <link>https://arxiv.org/abs/2402.04375</link>
      <description><![CDATA[机器学习 (ML) 的使用日益广泛，引发了人们的担忧，即 ML 模型可能会泄露对训练数据集做出贡献的个人的私人信息。为了防止敏感数据泄露，我们考虑使用差分隐私（DP）合成训练数据而不是真实训练数据来训练机器学习模型。合成数据的一个关键的理想特性是它能够保留原始分布的低阶边际。我们的主要贡献包括针对连续损失函数和 Lipschitz 损失函数在此类合成数据上训练的线性模型的超额经验风险的新上限和下限。我们根据理论结果进行了大量的实验。]]></description>
      <guid>https://arxiv.org/abs/2402.04375</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:16 GMT</pubDate>
    </item>
    <item>
      <title>置信度校准有助于保形预测吗？</title>
      <link>https://arxiv.org/abs/2402.04344</link>
      <description><![CDATA[保形预测作为一种新兴的不确定性限定技术，构建保证以高概率包含真实标签的预测集。以前的工作通常采用温度缩放来校准分类器，假设置信校准可以有利于保形预测。在这项工作中，我们首先表明，事后校准方法通过改进的校准令人惊讶地导致更大的预测集，而对小温度的过度自信反而有利于共形预测性能。从理论上讲，我们证明高置信度会降低在预测集中添加新类的概率。受分析的启发，我们提出了一种新方法，$\textbf{Conformal Thermal Scaling}$ (ConfTS)，它通过阈值与地面真实标签的不合格分数之间的差距来纠正目标。这样，ConfTS的新目标将优化温度值，使其达到满足$\textit{边缘覆盖}$的最佳集合。实验表明，我们的方法可以有效改进广泛使用的共形预测方法。]]></description>
      <guid>https://arxiv.org/abs/2402.04344</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:15 GMT</pubDate>
    </item>
    <item>
      <title>刺猬和豪猪：使用 Softmax Mimicry 表达线性注意力</title>
      <link>https://arxiv.org/abs/2402.04347</link>
      <description><![CDATA[线性注意力已经显示出提高 Transformer 效率的潜力，将注意力的二次复杂度降低到序列长度的线性。这为以下方面带来了令人兴奋的前景：(1) 从头开始​​训练线性 Transformer，(2) 将特定于任务的 Transformer“微调转换”为可恢复任务性能的线性版本，以及 (3) Transformer（例如大型语言）的“预训练转换”将模型转换为可在下游任务上微调的线性版本。然而，线性注意力在质量上常常低于标准的 softmax 注意力。为了缩小这种性能差距，我们发现先前的线性注意力缺乏与良好性能相关的 softmax 注意力的关键属性：低熵（或“尖峰”）权重和点积单调性。我们进一步观察到令人惊讶的简单特征图，它们保留了这些属性并匹配 softmax 性能，但在线性注意力中计算效率低下。因此，我们提出了 Hedgehog，一种可学习的线性注意力，它保留了 softmax 注意力的尖峰和单调特性，同时保持了线性复杂性。 Hedgehog 使用简单的可训练 MLP 来生成模仿 softmax 注意力的注意力权重。实验表明，Hedgehog 在从头开始训练和微调转换设置中恢复了 99% 以上的标准 Transformer 质量，在具有因果 GPT 的 WikiText-103 上优于先前的线性注意力，最多 6 个困惑点，在微调双向上最多 8.7 个 GLUE 得分点BERT。 Hedgehog 还支持预训练转换。将预训练的 GPT-2 转换为线性注意变体，可​​在 WikiText-103 上针对 125M 次二次解码器模型实现最先进的 16.7 困惑度。我们最终将预训练的 Llama-2 7B 转变为可行的线性注意力 Llama。通过低秩适应，Hedgehog-Llama2 7B 比基本标准注意力模型获得了 28.1 点更高的 ROUGE-1 点，而先前的线性注意力导致 16.5 点下降。]]></description>
      <guid>https://arxiv.org/abs/2402.04347</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:15 GMT</pubDate>
    </item>
    <item>
      <title>自适应推理：理论限制和未探索的机会</title>
      <link>https://arxiv.org/abs/2402.04359</link>
      <description><![CDATA[本文介绍了第一个用于量化自适应推理算法的效率和性能增益机会大小的理论框架。我们为可实现的效率和性能增益提供了新的近似和精确界限，并得到经验证据的支持，证明计算机视觉和自然语言处理任务的效率有可能提高 10-100 倍，而不会造成任何性能损失。此外，我们还提供了通过自适应推理状态空间的最佳选择和设计来提高可实现的效率增益的见解。]]></description>
      <guid>https://arxiv.org/abs/2402.04359</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:15 GMT</pubDate>
    </item>
    <item>
      <title>多视图符号回归</title>
      <link>https://arxiv.org/abs/2402.04298</link>
      <description><![CDATA[符号回归 (SR) 搜索表示一组解释变量和响应变量之间关系的分析表达式。当前的 SR 方法假设从单个实验中提取单个数据集。然而，研究人员经常面临通过不同设置进行的实验获得的多组结果。传统的 SR 方法可能无法找到潜在的表达式，因为每个实验的参数可能不同。在这项工作中，我们提出了多视图符号回归（MvSR），它同时考虑多个数据集，模仿实验环境，并输出通用参数解决方案。这种方法将评估的表达式拟合到每个独立的数据集，并同时返回能够准确拟合所有数据集的参数函数族 f(x; θ)。我们使用已知表达式生成的数据以及来自天文学、化学和经济的现实世界数据（无法使用先验分析表达式）来证明 MvSR 的有效性。结果表明，MvSR 更频繁地获得正确的表达，并且对超参数变化具有鲁棒性。在现实世界的数据中，它能够掌握群体行为，从文献中恢复已知的表达以及有希望的替代方案，从而使 SR 能够用于大范围的实验场景。]]></description>
      <guid>https://arxiv.org/abs/2402.04298</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:14 GMT</pubDate>
    </item>
    <item>
      <title>通过向非必要神经元注入噪声来增强 DNN 对抗鲁棒性和效率</title>
      <link>https://arxiv.org/abs/2402.04325</link>
      <description><![CDATA[深度神经网络 (DNN) 通过提供无与伦比的数据分析和决策功能，彻底改变了从医疗保健、金融到汽车等众多行业。尽管 DNN 具有变革性的影响，但它仍面临两个关键挑战：容易受到对抗性攻击，以及与更复杂和更大的模型相关的计算成本不断增加。在本文中，我们介绍了一种有效的方法，旨在同时增强对抗鲁棒性和执行效率。与之前通过均匀注入噪声来增强鲁棒性的研究不同，我们引入了一种非均匀噪声注入算法，策略性地应用于每个 DNN 层，以破坏攻击中引入的对抗性扰动。通过采用近似技术，我们的方法识别并保护必要神经元，同时策略性地将噪声引入非必要神经元。我们的实验结果表明，我们的方法成功地增强了多种攻击场景、模型架构和数据集的鲁棒性和效率。]]></description>
      <guid>https://arxiv.org/abs/2402.04325</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:14 GMT</pubDate>
    </item>
    <item>
      <title>BiLLM：突破法学硕士训练后量化的极限</title>
      <link>https://arxiv.org/abs/2402.04291</link>
      <description><![CDATA[预训练的大型语言模型 (LLM) 表现出卓越的通用语言处理能力，但对内存和计算资源有很高的要求。作为一种强大的压缩技术，二值化可以将模型权重极大地降低至仅 1 位，从而降低了昂贵的计算和内存需求。然而，现有的量化技术无法在超低位宽下保持 LLM 性能。为了应对这一挑战，我们提出了 BiLLM，这是一种专为预训练的 LLM 量身定制的突破性 1 位训练后量化方案。基于LLM的权重分布，BiLLM首先识别并结构性地选择显着权重，并通过有效的二元残差逼近策略最小化压缩损失。此外，考虑到非显着权重的钟形分布，我们提出了一种最佳分割搜​​索来准确地对它们进行分组和二值化。 BiLLM 首次在各种 LLM 系列和评估指标中以 1.08 位权重实现高精度推理（例如 LLaMA2-70B 上的困惑度为 8.41），显着优于 LLM 的 SOTA 量化方法。此外，BiLLM在单GPU上能够在0.5小时内完成70亿个权重的LLM二值化过程，表现出令人满意的时间效率。]]></description>
      <guid>https://arxiv.org/abs/2402.04291</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:13 GMT</pubDate>
    </item>
    <item>
      <title>AdaFlow：具有方差自适应流策略的模仿学习</title>
      <link>https://arxiv.org/abs/2402.04292</link>
      <description><![CDATA[基于扩散的模仿学习改进了多模态决策的行为克隆（BC），但由于扩散过程中的递归，其代价是推理速度明显减慢。它敦促我们设计高效的政策生成器，同时保持制定多样化行动的能力。为了应对这一挑战，我们提出了 AdaFlow，一种基于流生成建模的模仿学习框架。 AdaFlow 表示具有状态条件常微分方程 (ODE) 的策略，称为概率流。我们揭示了训练损失的条件方差与 ODE 的离散化误差之间的有趣联系。有了这种见解，我们提出了一种方差自适应 ODE 求解器，可以在推理阶段调整其步长，使 AdaFlow 成为自适应决策者，在不牺牲多样性的情况下提供快速推理。有趣的是，当动作分布是单峰时，它会自动简化为单步生成器。我们全面的实证评估表明，AdaFlow 在所有维度上都实现了高性能，包括成功率、行为多样性和推理速度。代码可在 https://github.com/hxixixh/AdaFlow 获取]]></description>
      <guid>https://arxiv.org/abs/2402.04292</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:13 GMT</pubDate>
    </item>
    <item>
      <title>LightHGNN：将超图神经网络提炼为 MLP，推理速度加快 100 美元\倍</title>
      <link>https://arxiv.org/abs/2402.04296</link>
      <description><![CDATA[超图神经网络（HGNN）由于其在高阶相关建模方面的优越性，最近引起了广泛的关注并表现出了令人满意的性能。然而，人们注意到超图的高阶建模能力也带来了计算复杂度的增加，这阻碍了其实际的工业部署。在实践中，我们发现高效部署 HGNN 的一个关键障碍是推理过程中的高阶结构依赖性。在本文中，我们建议弥合 HGNN 和推理高效的多层感知器（MLP）之间的差距，以消除 HGNN 的超图依赖性，从而降低计算复杂性并提高推理速度。具体来说，我们引入了 LightHGNN 和 LightHGNN$^+$ 以实现低复杂度的快速推理。 LightHGNN 通过软标签直接将教师 HGNN 的知识提炼到学生 MLP，并且 LightHGNN$^+$ 进一步明确地将可靠的高阶相关性注入到学生 MLP 中，以实现拓扑感知的提炼和抗过度平滑。对八个超图数据集的实验表明，即使没有超图依赖性，所提出的 LightHGNN 仍然可以实现比 HGNN 有竞争力甚至更好的性能，并且平均比普通 MLP 多 16.3 美元。对三个图数据集的广泛实验进一步表明，与所有其他方法相比，我们的 LightHGNN 具有平均最佳性能。对具有 5.5w 个顶点的合成超图进行的实验表明，LightHGNN 的运行速度比 HGNN 快 100 倍，展示了其对延迟敏感的部署的能力。]]></description>
      <guid>https://arxiv.org/abs/2402.04296</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:13 GMT</pubDate>
    </item>
    <item>
      <title>PRES：迈向可扩展的基于内存的动态图神经网络</title>
      <link>https://arxiv.org/abs/2402.04284</link>
      <description><![CDATA[基于内存的动态图神经网络 (MDGNN) 是一系列动态图神经网络，它利用内存模块来提取、提取和记忆长期时间依赖性，与无内存网络相比，具有卓越的性能。然而，训练 MDGNN 面临着处理纠缠的时间和结构依赖性的挑战，需要对数据序列进行顺序和时间顺序处理以捕获准确的时间模式。在批量训练期间，同一批次内的时间数据点将被并行处理，而它们的时间依赖性被忽略。这个问题被称为时间不连续性，它限制了有效的时间批量大小，限制了数据并行性并降低了 MDGNN 在工业应用中的灵活性。本文研究了大规模 MDGNN 的有效训练，重点关注训练大时间批量大小的 MDGNN 时的时间不连续性。我们首先对时间批量大小对 MDGNN 训练收敛性的影响进行了理论研究。基于分析，我们提出了 PRES，一种迭代预测校正方案，与记忆一致性学习目标相结合，以减轻时间不连续性的影响，使 MDGNN 能够在不牺牲泛化性能的情况下使用更大的时间批次进行训练。实验结果表明，我们的方法在 MDGNN 训练期间可实现高达 4 倍大的时间批量（3.4 倍加速）。]]></description>
      <guid>https://arxiv.org/abs/2402.04284</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:12 GMT</pubDate>
    </item>
    <item>
      <title>CasCast：通过级联建模进行熟练的高分辨率降水临近预报</title>
      <link>https://arxiv.org/abs/2402.04290</link>
      <description><![CDATA[基于雷达数据的降水临近预报在极端天气预测中发挥着至关重要的作用，并对灾害管理具有广泛的影响。尽管基于深度学习取得了进展，但降水临近预报的两个关键挑战尚未得到很好的解决：（i）不同尺度的复杂降水系统演化建模，以及（ii）极端降水的准确预报。在这项工作中，我们提出了 CasCast，这是一个由确定性部分和概率部分组成的级联框架，用于解耦中尺度降水分布和小尺度模式的预测。然后，我们探索在高分辨率下训练级联框架，并使用逐帧引导扩散变换器在低维潜在空间中进行概率建模，以增强极端事件的优化，同时降低计算成本。对三个基准雷达降水数据集的大量实验表明，CasCast 实现了具有竞争力的性能。尤其是，CasCast 显着超过了区域极端降水临近预报的基线（高达 +91.8%）。]]></description>
      <guid>https://arxiv.org/abs/2402.04290</guid>
      <pubDate>Thu, 08 Feb 2024 18:16:12 GMT</pubDate>
    </item>
    </channel>
</rss>