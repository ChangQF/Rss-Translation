<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过对比替代目标进行去偏图毒害攻击</title>
      <link>https://arxiv.org/abs/2407.19155</link>
      <description><![CDATA[arXiv:2407.19155v1 公告类型：新
摘要：图神经网络 (GNN) 容易受到对抗性攻击，这种攻击旨在通过图上的不可察觉的变化来降低 GNN 的性能。然而，我们发现，事实上，流行的基于元梯度的攻击利用了邻接矩阵的损失梯度，偏向于训练节点。也就是说，它们的元梯度由代理模型的训练过程决定，该模型仅在训练节点上进行训练。这种偏差表现为不均匀的扰动，当两个节点中至少有一个是标记节点（即训练节点）时，它们会连接两个节点，而不太可能连接两个未标记节点。然而，这些有偏见的攻击方法是次优的，因为它们根本不考虑两个未标记节点之间的翻转边。这意味着它们错过了未标记节点之间潜在的受攻击边，而这些边会显著改变节点的表示。在本文中，我们研究了元梯度，以揭示现有攻击扰动不均匀的根本原因。基于我们的分析，我们提出了一种使用对比替代目标 (Metacon) 的基于元梯度的攻击方法，该方法使用新的替代损失来缓解元梯度中的偏差。我们进行了广泛的实验，以表明 Metacon 通过基准数据集优于现有的基于元梯度的攻击方法，同时表明缓解对训练节点的偏差对于攻击图结构是有效的。]]></description>
      <guid>https://arxiv.org/abs/2407.19155</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>使用图神经网络分解异构动力系统</title>
      <link>https://arxiv.org/abs/2407.19160</link>
      <description><![CDATA[arXiv:2407.19160v1 公告类型：新
摘要：自然物理、化学和生物动力系统通常很复杂，异构组件以多种方式相互作用。我们表明，图神经网络可以设计为仅从数据中联合学习交互规则和异质性结构。学习到的潜在结构和动态可用于虚拟分解复杂系统，这对于参数化和推断底层控制方程是必要的。我们通过相互作用的移动粒子和矢量场的模拟实验测试了该方法。虽然我们目前的目标是更好地理解和验证模拟数据的方法，但我们预计它将成为一种普遍适用的工具，以揭示自然界中观察到的复杂动力学背后的控制规则。]]></description>
      <guid>https://arxiv.org/abs/2407.19160</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>图记忆学习：模仿大脑网络的终身记忆和遗忘</title>
      <link>https://arxiv.org/abs/2407.19183</link>
      <description><![CDATA[arXiv:2407.19183v1 公告类型：新 
摘要：现实场景中的图数据经历着快速而频繁的变化，现有的图模型很难有效处理不断涌入的新数据并适应数据提取请求。频繁重新训练图模型的方法资源密集且不切实际。为了解决这一紧迫的挑战，本文引入了一种新的图记忆学习概念。其核心思想是让图模型有选择地记住新知识，但忘记旧知识。在此基础上，本文提出了一种新颖的图记忆学习框架——脑启发式图记忆学习（BGML），其灵感来自大脑网络动力学和功能结构耦合策略。BGML 结合了一种基于特征图粒度学习的多粒度分层渐进学习机制，以缓解图记忆学习中记忆与遗忘之间的潜在冲突。该机制允许对不断发展的图中的局部细节进行全面和多层次的感知。此外，针对新增增量信息结构不可靠的问题，本文引入了信息自评估所有权机制，该机制不仅有利于增量信息在模型内部的传播，还能有效保留过去经验的完整性。我们设计了五种类型的图记忆学习任务：常规、记忆、反学习、数据增量和类增量来评估 BGML。通过在多个真实世界节点分类数据集上的大量实验证实了其优异的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.19183</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>联邦学习中缓解成员推理攻击的准确性-隐私权衡</title>
      <link>https://arxiv.org/abs/2407.19119</link>
      <description><![CDATA[arXiv:2407.19119v1 公告类型：新
摘要：在过去几年中，联邦学习 (FL) 已成为机器学习中一种重要的方法，它强调隐私保护，允许多个客户端协作构建模型，同时保持其训练数据的私密性。尽管注重隐私，但 FL 模型容易受到各种攻击，包括成员推理攻击 (MIA)，对数据机密性构成严重威胁。在最近的一项研究中，Rezaei \textit{et al.} 揭示了深度集成中存在准确性-隐私权衡，并提出了一些融合策略来克服它。在本文中，我们旨在探索深度集成和 FL 之间的关系。具体而言，我们研究从深度集成中得出的基于置信度的指标是否适用于 FL，以及 FL 中是否存在关于 MIA 的准确性和隐私之间的权衡。实证研究表明，客户端数量与准确性-隐私权衡之间缺乏非单调相关性。通过尝试不同数量的联合客户端、数据集和基于置信度度量的融合策略，我们识别并通过分析证明了准确性-隐私权衡的明确存在。]]></description>
      <guid>https://arxiv.org/abs/2407.19119</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>基于像素的分层策略对任务泛化的好处</title>
      <link>https://arxiv.org/abs/2407.19142</link>
      <description><![CDATA[arXiv:2407.19142v1 公告类型：新
摘要：强化学习从业者经常避免使用分层策略，尤其是在基于图像的观察空间中。通常，与平面策略相比，单任务性能的提高并不能证明实施层次结构所带来的额外复杂性是合理的。然而，通过引入多个决策层，分层策略可以组成较低级别的策略，以更有效地在任务之间进行泛化，从而凸显了多任务评估的必要性。我们通过从像素模拟多任务机器人控制实验来分析层次结构的好处。我们的结果表明，使用任务条件训练的分层策略可以 (1) 提高训练任务的性能，(2) 提高类似任务中的奖励和状态空间泛化，以及 (3) 降低解决新任务所需的微调复杂性。因此，我们认为在构建能够在任务之间泛化的强化学习架构时应该考虑分层策略。]]></description>
      <guid>https://arxiv.org/abs/2407.19142</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>FedAR：通过局部更新近似和校正解决联邦学习中的客户端不可用问题</title>
      <link>https://arxiv.org/abs/2407.19103</link>
      <description><![CDATA[arXiv:2407.19103v1 公告类型：新 
摘要：联邦学习 (FL) 使客户端能够在服务器的协调下以隐私保护的方式协作训练机器学习模型。FL 的主要挑战之一是，由于客户端资源限制和间歇性网络连接，服务器可能无法在每一轮中从每个客户端接收本地更新。不可用客户端的存在严重降低了整体 FL 性能。在本文中，我们提出了一种新颖的客户端更新近似和校正算法来解决客户端不可用问题。FedAR 可以让所有客户端参与全局模型更新，以在服务器上实现高质量的全局模型，这也为每个客户端提供准确的预测。为此，服务器使用来自每个客户端的最新更新作为其当前更新的替代。然后，它为每个客户端的替代更新分配不同的权重以得出全局模型，以保证来自可用和不可用客户端的贡献。我们的理论分析证明，FedAR 在非 IID 数据集上实现了凸和非凸平滑损失函数的最佳收敛速度。大量的实证研究表明，FedAR 在训练损失、测试准确率和偏差缓解方面全面超越了最先进的 FL 基线，包括 FedAvg、MIFA、FedVARP 和 Scaffold。此外，FedAR 在存在大量客户端且客户端严重不可用的情况下也表现出色。]]></description>
      <guid>https://arxiv.org/abs/2407.19103</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>你属于哪个参考类别？使用规范模型测量参考类别的种族公平性</title>
      <link>https://arxiv.org/abs/2407.19114</link>
      <description><![CDATA[arXiv:2407.19114v1 公告类型：新
摘要：医疗保健中的参考类别建立了健康规范，例如身高和体重的儿科生长图表，并用于绘制代表潜在临床风险的这些规范的偏差。参考类别的人口统计数据如何影响偏差的临床解释尚不清楚。使用规范建模（一种构建参考类别的方法），我们评估了广泛应用于精神病学和神经病学的结构性脑图像参考模型中的公平性（种族偏见）。我们测试在模型中包含种族是否会创建更公平的模型。我们使用来自三个不同参考类别规范模型的偏差分数来预测自我报告的种族，以便更好地从综合、多变量的角度理解偏见。在所有这些任务中，我们发现了现有数据或常用建模技术不易解决的种族差异。我们的工作表明，偏离规范可能是由于与参考类别的人口统计不匹配造成的，并且应谨慎地为这些偏差赋予临床意义。我们的方法还表明，获取更具代表性的样本是一项紧迫的研究重点。]]></description>
      <guid>https://arxiv.org/abs/2407.19114</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>实现非线性 RNN 的可扩展且稳定的并行化</title>
      <link>https://arxiv.org/abs/2407.19115</link>
      <description><![CDATA[arXiv:2407.19115v1 公告类型：新
摘要：传统的非线性 RNN 不能自然地在整个序列长度上并行化，而 Transformer 和线性 RNN 则可以。因此，Lim 等人 [2024] 将非线性 RNN 的并行化评估视为一个不动点问题，并用牛顿法解决。通过推导和应用牛顿法的并行化形式，它们比顺序评估实现了巨大的加速。然而，他们的方法继承了立方计算复杂度和数值不稳定性。我们解决了这些弱点。为了降低计算复杂度，我们应用了拟牛顿近似，并表明它们与全牛顿法相当地收敛，占用更少的内存，而且速度更快。为了稳定牛顿法，我们利用了信任区域阻尼牛顿法和卡尔曼平滑之间的联系。这种联系使我们能够根据信任区域稳定牛顿法，同时使用高效的并行卡尔曼算法来保持性能。我们根据经验比较了这些方法，并重点介绍了每种算法的优势用例。]]></description>
      <guid>https://arxiv.org/abs/2407.19115</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>用于错误感知场景表示网络的正则化多解码器集成</title>
      <link>https://arxiv.org/abs/2407.19082</link>
      <description><![CDATA[arXiv:2407.19082v1 公告类型：新
摘要：特征网格场景表示网络 (SRN) 已应用于科学数据，作为分析和可视化的紧凑功能替代。由于 SRN 是黑盒有损数据表示，因此评估预测质量对于科学可视化应用至关重要，以确保科学家可以信任可视化的信息。目前，现有架构不支持推理时间重建质量评估，因为在没有地面真实数据的情况下无法评估坐标级误差。我们提出了一种参数高效的多解码器 SRN (MDSRN) 集成架构，由一个共享特征网格和多个轻量级多层感知器解码器组成。MDSRN 可以为给定的输入坐标生成一组合理的预测，以计算均值作为多解码器集成的预测，方差作为置信度分数。坐标级方差可以与数据一起呈现以告知重建质量，或集成到不确定性感知体积可视化算法中。为了防止量化方差与预测质量不一致，我们提出了一种用于集成学习的新型方差正则化损失，以促进正则化多解码器 SRN (RMDSRN) 获得与真实模型误差密切相关的更可靠的方差。我们全面评估了蒙特卡洛 Dropout、均值场变分推理、深度集成和预测方差与所提出的 MDSRN 和 RMDSRN 在不同标量场数据集上的方差量化和数据重构质量。我们证明，在相同的神经网络参数预算下，RMDSRN 在不确定的 SRN 中实现了最准确的数据重构和具有竞争力的方差-误差相关性。]]></description>
      <guid>https://arxiv.org/abs/2407.19082</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>增强广义正态分布：将机器学习与运维知识相结合</title>
      <link>https://arxiv.org/abs/2407.19092</link>
      <description><![CDATA[arXiv:2407.19092v1 公告类型：新
摘要：机器学习 (ML) 技术在操作环境中的应用通常面临两个挑战：i) ML 方法主要提供点预测，而许多操作问题需要分布信息；ii) 它们通常不包含操作文献中的大量知识，特别是表征特定分布的理论和实证发现。我们引入了一种新颖而严格的方法，即增强广义正态分布 ($b$GND)，以应对这些挑战。广义正态分布 (GND) 涵盖了操作中常见的各种参数分布，$b$GND 利用树学习器的梯度提升来灵活地估计 GND 的参数作为协变量的函数。我们建立了 $b$GND 的统计一致性，从而将这一关键属性扩展到 ML 文献中研究的缺乏此类保证的特殊情况。我们使用来自美国一家大型学术急诊科的数据，表明通过利用医疗保健运营文献中的发现，可以显著改善患者等待和服务时间的分布预测。具体而言，$b$GND 的表现分别比用于预测等待和服务时间的分布无关 ML 基准高出 6% 和 9%。进一步的分析表明，这些改进意味着患者满意度提高 9%，心肌梗死患者死亡率降低 4%。我们的工作强调了将 ML 与运营知识相结合以增强分布预测的重要性。]]></description>
      <guid>https://arxiv.org/abs/2407.19092</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>通过促进出现的初始化方案提高神经网络性能</title>
      <link>https://arxiv.org/abs/2407.19044</link>
      <description><![CDATA[arXiv:2407.19044v1 公告类型：新
摘要：我们引入了一种新颖而直接的神经网络初始化方案，该方案修改了 Xavier 和 Kaiming 初始化等传统方法。受涌现概念的启发并利用 Li (2023) 提出的涌现度量，我们的方法调整了逐层权重缩放因子以实现更高的涌现值。这种增强功能易于实现，与 GradInit 相比，初始化不需要额外的优化步骤。我们在各种架构中评估了我们的方法，包括用于图像识别的 MLP 和卷积架构，以及用于机器翻译的转换器。我们在有和没有批量归一化的情况下展示了模型准确性和训练速度的显着提高。我们的方法的简单性、理论创新性和可证明的经验优势使其成为神经网络初始化实践的有力增强。这些结果表明，利用涌现来改进神经网络训练方法是一个有希望的方向。代码可在以下位置获得：https://github.com/johnnyjingzeli/EmergenceInit。]]></description>
      <guid>https://arxiv.org/abs/2407.19044</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:34 GMT</pubDate>
    </item>
    <item>
      <title>Uber 使用因果机器学习进行实际市场优化</title>
      <link>https://arxiv.org/abs/2407.19078</link>
      <description><![CDATA[arXiv:2407.19078v1 公告类型：新 
摘要：市场杠杆的预算分配，例如对司机的激励和对乘客的促销，长期以来一直是 Uber 的技术和业务挑战；了解杠杆预算变化的影响并估计成本效率以实现预定义预算至关重要，目标是实现最大化商业价值的最佳分配；我们引入了一种端到端机器学习和优化程序，依靠特征存储、模型训练和服务、优化器和回测来自动化城市的预算决策；提出基于 S-Learner 和新型张量 B 样条回归模型的最先进的深度学习 (DL) 估计器，我们使用 ADMM 和原始对偶内点凸优化解决高维优化问题，大大提高了 Uber 的资源配置效率。]]></description>
      <guid>https://arxiv.org/abs/2407.19078</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:34 GMT</pubDate>
    </item>
    <item>
      <title>使用双向 LSTM 对 SMILES 数据进行加速药物安全性评估</title>
      <link>https://arxiv.org/abs/2407.18919</link>
      <description><![CDATA[arXiv:2407.18919v1 公告类型：新
摘要：计算方法有助于加快药物发现的速度。药物发现包含几个步骤，例如目标识别和验证、先导化合物发现和先导化合物优化等。在先导化合物优化阶段，将评估先导化合物的吸收、分布、代谢、排泄和毒性特性。为了解决预测先导化合物的毒性和溶解度的问题，以简化分子输入线输入系统 (SMILES) 符号表示。在处理 SMILES 数据的不同方法中，所提出的模型是使用基于序列的方法构建的。所提出的双向长短期记忆 (BiLSTM) 是循环神经网络 (RNN) 的一种变体，它处理输入的分子序列，以从正向和反向全面检查分子的结构特征。这项研究旨在了解 SMILES 字符串中编码的序列模式，然后利用这些模式预测分子的毒性。在 ClinTox 数据集上，所提出的模型超越了 Trimnet 和预训练图神经网络 (GNN) 等以前的方法，实现了 0.96 的 ROC 准确度。在 FreeSolv 数据集上，BiLSTM 的表现优于以前的模型，在溶解度预测中，RMSE 值低至 1.22。]]></description>
      <guid>https://arxiv.org/abs/2407.18919</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:33 GMT</pubDate>
    </item>
    <item>
      <title>敬请期待：超参数对 LLM 调优在实际应用中的影响的实证研究</title>
      <link>https://arxiv.org/abs/2407.18990</link>
      <description><![CDATA[arXiv:2407.18990v1 公告类型：新
摘要：微调大型语言模型 (LLM) 是提高其在下游任务上性能的有效方法。但是，选择适当的超参数 (HP) 调整设置是一个劳动密集型且计算成本高昂的过程。在这里，我们为实际用例提供了推荐的 HP 配置，这些配置代表了从业者的更好起点，同时考虑了两个 SOTA LLM 和两种常用的调整方法。我们描述了基于覆盖范围的搜索 (CBS)，这是一种基于离线广泛网格搜索对 HP 配置进行排名的过程，这样排名靠前的配置共同为广泛的数据集和领域提供了实用的稳健建议。我们的实验重点是 Llama-3-8B 和 Mistral-7B，以及完全微调和 LoRa，总共进行了 10,000 多次调整实验。我们的结果表明，一般来说，如果可能的话，应该优先考虑 Llama-3-8B 和 LoRA。此外，我们表明，对于模型和调整方法，只需探索我们分析所推荐的少数 HP 配置，就可以在实践中提供出色的结果，这使得这项工作成为从业者的宝贵资源。]]></description>
      <guid>https://arxiv.org/abs/2407.18990</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:33 GMT</pubDate>
    </item>
    <item>
      <title>GraphBPE：分子图与字节对编码的结合</title>
      <link>https://arxiv.org/abs/2407.19039</link>
      <description><![CDATA[arXiv:2407.19039v1 公告类型：新 
摘要：随着对分子机器学习的关注度不断提高，人们在设计更好的模型或提出更全面的基准方面做出了各种创新。然而，对分子图的数据预处理计划的研究较少，而对分子图的不同看法可能会提高模型的性能。受字节对编码 (BPE) 算法（一种在自然语言处理中广泛采用的子词标记方法）的启发，我们提出了 GraphBPE，它将分子图标记为不同的子结构，并充当独立于模型架构的预处理计划。我们在 3 个图级分类和 3 个图级回归数据集上进行的实验表明，数据预处理可以提高分子图模型的性能，并且 GraphBPE 对小型分类数据集有效，并且它在不同模型架构中的表现与其他标记方法相当。]]></description>
      <guid>https://arxiv.org/abs/2407.19039</guid>
      <pubDate>Wed, 31 Jul 2024 03:16:33 GMT</pubDate>
    </item>
    </channel>
</rss>