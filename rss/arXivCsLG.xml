<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 14 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>使用机器学习对患有学习障碍和多种长期疾病的患者的住院时间进行公平预测</title>
      <link>https://arxiv.org/abs/2411.08048</link>
      <description><![CDATA[arXiv:2411.08048v1 公告类型：新
摘要：英国和其他国家发表的研究报告显示，与普通公众相比，学习障碍患者的死亡率和过早死亡率更高。本研究使用来自 SAIL 数据库的电子健康记录 (EHR) 数据源，分析了威尔士人口中 9,618 名被确诊为学习障碍和长期疾病的患者的住院情况。我们描述了研究队列的人口统计特征、长期疾病的患病率、用药史、医院就诊和生活方式史，并应用机器学习模型来预测该队列的住院时间。随机森林 (RF) 模型的曲线下面积 (AUC) 为 0.759（男性）和 0.756（女性），假阴性率为 0.224（男性）和 0.229（女性），平衡准确度为 0.690（男性）和 0.689（女性）。在检查了不同种族群体的模型性能后，应用了两种偏差缓解算法（阈值优化和使用指数梯度的缩减算法）来最大限度地减少性能差异。阈值优化算法优于缩减算法，在各个种族的男性群体中实现了较低的假阳性率范围和平衡的准确度。这项研究展示了在 EHR 数据源上应用具有有效偏差缓解方法的机器学习模型的潜力，通过解决跨组数据不平衡问题，实现公平的住院时间预测。]]></description>
      <guid>https://arxiv.org/abs/2411.08048</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度学习 2.0：重要的人工神经元——拒绝相关性，拥抱正交性</title>
      <link>https://arxiv.org/abs/2411.08085</link>
      <description><![CDATA[arXiv:2411.08085v1 公告类型：新
摘要：我们引入了一个由 yat-product 驱动的神经网络，即神经物质网络 (NMN)，这是深度学习的一项突破，它无需激活函数即可实现非线性模式识别。我们的关键创新依赖于 yat-product 和 yat-product，它们通过将输入投射到伪度量空间中自然地诱导非线性，消除了对传统激活函数的需求，同时仅保留一个用于最终类概率分布的 softmax 层。这种方法简化了网络架构，并为网络的决策过程提供了前所未有的透明度。我们对不同数据集的全面实证评估表明，NMN 的表现始终优于传统 MLP。结果挑战了单独的激活函数对于有效的深度学习模型是必要的假设。这项工作的意义不仅限于直接的架构优势，通过消除中间激活函数同时保留非线性功能，yat-MLP 为神经网络设计建立了一种结合了简单性和有效性的新范式。最重要的是，我们的方法对神经网络传统上不透明的“黑箱”性质提供了前所未有的洞察，从而更清楚地了解这些模型如何处理和分类信息。]]></description>
      <guid>https://arxiv.org/abs/2411.08085</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>全精度模型上具有影响力的位翻转搜索</title>
      <link>https://arxiv.org/abs/2411.08133</link>
      <description><![CDATA[arXiv:2411.08133v1 公告类型：新
摘要：神经网络在各种任务中都表现出色，但它们仍然容易受到输入或模型参数细微变化的影响。一个特别有影响力的漏洞是通过位翻转攻击 (BFA) 产生的，其中翻转模型参数中的少数关键位会严重降低其性能。在 DRAM 中引发位翻转的一种常见技术是 Row-Hammer 攻击，它利用频繁的未缓存内存访问来更改数据。识别易受影响的位可以通过详尽搜索或逐层分析来实现，尤其是在量化网络中。在这项工作中，我们引入了有影响力的位翻转搜索 (IBS)，这是一种在全精度网络中有效定位和翻转关键位的新方法。此外，我们提出了一种 Weight-Stealth 技术，该技术以一种将浮点值保持在原始分布内的方式策略性地修改模型的参数，从而绕过篡改检测中经常使用的简单范围检查。]]></description>
      <guid>https://arxiv.org/abs/2411.08133</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>EAPCR：一种无需明确特征关系模式的科学数据通用特征提取器</title>
      <link>https://arxiv.org/abs/2411.08164</link>
      <description><![CDATA[arXiv:2411.08164v1 公告类型：新
摘要：传统方法，包括基于决策树 (DT) 的方法，在科学任务中非常有效，例如非图像医学诊断、系统异常检测和无机催化效率预测。然而，大多数深度学习技术都难以超越甚至达到传统机器学习方法的成功水平。主要原因是这些应用涉及多源异构数据，其中特征缺乏明确的关系。这与图像数据形成对比，其中像素表现出空间关系；文本数据，其中单词具有顺序依赖性；以及图形数据，其中节点通过已建立的关联连接。缺乏明确的特征关系模式 (FRP) 对非基于图像、文本和图形的科学应用中的深度学习技术提出了重大挑战。在本文中，我们介绍了 EAPCR，这是一种专为没有明确 FRP 的数据设计的通用特征提取器。经过各种科学任务的测试，EAPCR 的表现始终优于传统方法，并弥补了深度学习模型的不足。为了进一步证明其稳健性，我们合成了一个没有显式 FRP 的数据集。虽然 Kolmogorov-Arnold 网络 (KAN) 和卷积神经网络 (CNN)、图卷积网络 (GCN) 和 Transformer 等特征提取器表现不佳，但 EAPCR 表现出色，证明了其在没有 FRP 的科学任务中的稳健性和卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2411.08164</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>利用神经元嵌入解决多义性问题</title>
      <link>https://arxiv.org/abs/2411.08166</link>
      <description><![CDATA[arXiv:2411.08166v1 公告类型：新
摘要：我们提出了神经元嵌入，这是一种可用于解决多义性的表示，通过识别神经元特征数据集示例中的不同语义行为，使下游手动或自动解释变得更加容易。我们将我们的方法应用于 GPT2-small，并提供用于探索结果的 UI。神经元嵌入是使用模型的内部表示和权重计算的，使它们与领域和架构无关，并消除了引入可能无法反映模型实际计算的外部结构的风险。我们描述了如何使用神经元嵌入来测量神经元多义性，这可以用于更好地评估稀疏自动编码器 (SAE) 的功效。]]></description>
      <guid>https://arxiv.org/abs/2411.08166</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>抵御对抗性腐败的多智能体随机匪徒</title>
      <link>https://arxiv.org/abs/2411.08167</link>
      <description><![CDATA[arXiv:2411.08167v1 公告类型：新
摘要：我们研究了在异构环境中具有对抗性腐败的多智能体多臂老虎机问题，其中每个智能体访问一组子集。对手可以破坏所有智能体的奖励观察。智能体彼此分享这些被破坏的奖励，目标是最大化所有智能体的累积总奖励（并且不被对手误导）。我们提出了一种对对抗性腐败具有鲁棒性的多智能体合作学习算法。对于这种新设计的算法，我们证明，在非腐败环境中，具有未知腐败预算 $C$ 的对手只会对模型的标准遗憾产生附加的 $O((L / L_{\min}) C)$ 项，其中 $L$ 是智能体的总数，$L_{\min}$ 是可以相互访问一个臂的智能体的最小数量。作为副产品，我们的算法在减少单智能体和同质多智能体场景时还改进了最先进的遗憾界限，分别收紧了乘法 $K$（臂的数量）和 $L$（智能体的数量）因子。]]></description>
      <guid>https://arxiv.org/abs/2411.08167</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>PERFT：混合专家模型的参数高效路由微调</title>
      <link>https://arxiv.org/abs/2411.08212</link>
      <description><![CDATA[arXiv:2411.08212v1 公告类型：新
摘要：混合专家 (MoE) 范式已成为一种强大的方法，可用于扩展变压器并提高资源利用率。然而，有效微调 MoE 模型仍然在很大程度上尚未得到充分探索。受最近关于参数高效微调 (PEFT) 的研究的启发，我们提出了一个统一的框架，用于将 PEFT 模块直接集成到 MoE 机制中。与 MoE 的核心原则和架构保持一致，我们的框架包含一组设计维度，包括各种功能和组合策略。通过结合我们框架内的设计选择，我们引入了参数高效路由微调 (PERFT)，作为针对 MoE 模型量身定制的灵活且可扩展的 PEFT 策略系列。针对常识和算术推理任务对 OLMoE-1B-7B 和 Mixtral-8$\times$7B 进行了大量实验，证明了 PERFT 的有效性、可扩展性和有趣的动态性。此外，我们为每个特定的设计选择提供了实证结果，以促进更好地应用 MoE 和 PEFT。]]></description>
      <guid>https://arxiv.org/abs/2411.08212</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>持续学习中的联合扩散模型</title>
      <link>https://arxiv.org/abs/2411.08224</link>
      <description><![CDATA[arXiv:2411.08224v1 公告类型：新
摘要：在这项工作中，我们介绍了 JDCL——一种基于联合扩散模型的生成排练持续学习的新方法。神经网络遭受灾难性遗忘，即当使用来自不同分布的额外数据进行重新训练时，模型性能会突然下降。基于生成重放的持续学习方法试图通过使用从生成模型中采样的新数据和排练数据的组合来重新训练模型来缓解这个问题。在这项工作中，我们建议通过将持续训练的分类器与基于扩散的生成模型组合成一个联合优化的神经网络来扩展这个想法。我们表明，这种共享参数化与知识蒸馏技术相结合，可以稳定地适应新任务而不会发生灾难性遗忘。我们在几个基准上评估了我们的方法，结果发现它优于最近最先进的生成重放技术。此外，我们将我们的方法扩展到半监督的持续学习设置，它优于竞争的基于缓冲区的重放技术，并以自我监督的方式评估训练表示的质量。]]></description>
      <guid>https://arxiv.org/abs/2411.08224</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过观察进行模仿学习：一种自回归混合专家方法</title>
      <link>https://arxiv.org/abs/2411.08232</link>
      <description><![CDATA[arXiv:2411.08232v1 公告类型：新
摘要：本文提出了一种从观察中进行模仿学习的新方法，其中部署了自回归混合专家模型来适应底层策略。模型的参数通过两阶段框架学习。通过利用现有的动态知识，框架的第一阶段估计控制输入序列，从而降低问题的复杂性。在第二阶段，通过使用估计的控制输入序列解决正则化的最大似然估计问题来学习策略。我们通过结合 Lyapunov 稳定性约束来进一步扩展学习过程，以确保所识别模型的渐近稳定性，从而实现准确的多步预测。使用从人类演示中收集的两个自动驾驶数据集验证了所提出的框架的有效性，证明了其在建模复杂非线性动力学中的实际适用性。]]></description>
      <guid>https://arxiv.org/abs/2411.08232</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>NVCiM-PT：一种用于边缘 LLM 的 NVCiM 辅助快速调优框架</title>
      <link>https://arxiv.org/abs/2411.08244</link>
      <description><![CDATA[arXiv:2411.08244v1 公告类型：新 
摘要：部署在边缘设备上的大型语言模型 (LLM) 称为边缘 LLM，需要在有限的资源约束下根据用户生成的数据不断微调其模型参数。然而，大多数现有的学习方法并不适用于边缘 LLM，因为它们依赖高资源和低学习能力。快速调整 (PT) 最近通过仅修改一小部分 LLM 参数而成为一种有效的边缘 LLM 微调方法，但它会受到用户域转移的影响，导致重复训练并降低资源效率。解决域转移问题的传统技术通常涉及复杂的神经网络和复杂的训练，这与边缘 LLM 的 PT 不兼容。因此，一个悬而未决的研究问题是如何在资源有限的情况下解决边缘 LLM 的域转移问题。在本文中，我们提出了一个用于边缘 LLM 的快速调整框架，利用非易失性内存计算 (NVCiM) 架构提供的优势。我们引入了一种新颖的 NVCiM 辅助 PT 框架，其中我们将核心操作缩小到矩阵乘法，然后可以通过在 NVCiM 上执行现场计算来加速。据我们所知，这是第一项使用 NVCiM 来提高边缘 LLM PT 性能的工作。]]></description>
      <guid>https://arxiv.org/abs/2411.08244</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>检索增强时间序列预测</title>
      <link>https://arxiv.org/abs/2411.08249</link>
      <description><![CDATA[arXiv:2411.08249v1 公告类型：新
摘要：检索增强生成 (RAG) 是现代 LLM 系统的核心组件，特别是在最新信息对于准确响应用户查询或查询超出训练数据范围至关重要的情况下。时间序列基础模型 (TSFM)（例如 Chronos）的出现以及对跨各种时间序列域的有效零样本预测性能的需求引发了一个问题：RAG 的好处是否同样可以延续到时间序列预测中？在本文中，我们认为时间序列数据的动态和事件驱动性质使 RAG 成为 TSFM 的重要组成部分，并引入了一个用于时间序列预测的原则性 RAG 框架，称为检索增强预测 (RAF)。在 RAF 中，我们开发了有效的策略来检索相关的时间序列示例并将其纳入预测中。通过实验和机制研究，我们证明 RAF 确实提高了跨不同时间序列域的预测准确性，并且对于较大的 TSFM 规模，这种改进更为显著。]]></description>
      <guid>https://arxiv.org/abs/2411.08249</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>GPTree：通过 LLM 驱动的决策树实现可解释的决策</title>
      <link>https://arxiv.org/abs/2411.08257</link>
      <description><![CDATA[arXiv:2411.08257v1 公告类型：新
摘要：传统的决策树算法是可解释的，但在处理非线性、高维数据时会遇到困难，这限制了其在复杂决策中的适用性。神经网络擅长捕捉复杂模式，但在此过程中牺牲了可解释性。在这项工作中，我们提出了 GPTree，这是一个将决策树的可解释性与 LLM 的高级推理能力相结合的新型框架。GPTree 消除了对特征工程和提示链的需求，只需要特定于任务的提示并利用基于树的结构动态分割样本。我们还引入了一种专家在环反馈机制，通过允许人工干预来改进和重建决策路径，从而进一步提高性能，强调人类专业知识与机器智能之间的和谐。我们的决策树在初创公司的初始阶段识别“独角兽”初创公司的准确率达到了 7.8%，超过了使用少量学习的 GPT-4O 以及最好的人类决策者（3.1% 到 5.6%）。]]></description>
      <guid>https://arxiv.org/abs/2411.08257</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>二次卷积神经网络的最小二乘训练及其在系统理论中的应用</title>
      <link>https://arxiv.org/abs/2411.08267</link>
      <description><![CDATA[arXiv:2411.08267v1 公告类型：新
摘要：本文提供了一种最小二乘公式，用于使用二次激活函数、2 范数损失函数和无正则化项来训练 2 层卷积神经网络。使用此方法，可以获得全局最优权重的解析表达式以及网络的二次输入输出方程。这些属性使网络成为系统理论中的可行工具，因为它可以进行进一步分析，例如输出对输入扰动的敏感性，这对于飞机或自动驾驶汽车等安全关键系统至关重要。将最小二乘法与先前提出的训练二次网络的策略以及反向传播训练的 ReLU 网络进行了比较。所提出的方法应用于系统识别问题和 GPS 位置估计问题。结果表明，最小二乘网络的训练时间显著缩短，对预测准确性的影响最小，同时还具有解析输入输出方程的优势。虽然这些结果仅适用于 2 层网络，但本文激发了在系统理论背景下对更深的二次网络的探索。]]></description>
      <guid>https://arxiv.org/abs/2411.08267</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>蛋白质结构相似性搜索的哈希算法</title>
      <link>https://arxiv.org/abs/2411.08286</link>
      <description><![CDATA[arXiv:2411.08286v1 公告类型：新摘要：蛋白质结构相似性搜索（PSSS）试图搜索具有相似结构的蛋白质，在从药物设计到蛋白质功能预测和分子进化等不同领域中发挥着至关重要的作用。传统的基于比对的 PSSS 方法直接计算蛋白质结构的比对，非常耗时且内存成本高。最近，提出了用于 PSSS 的无比对方法，将蛋白质结构表示为固定长度的实值向量。虽然这些方法比基于比对的方法具有更低的时间和内存成本，但对于大规模 PSSS 来说，它们的时间和内存成本仍然太高，而且其准确性不令人满意。在本文中，我们为 PSSS 提出了一种称为 $\underline{\text{p}}$r$\underline{\text{o}}$tein $\underline{\text{s}}$structure $\underline{\text{h}}$ashing (POSH) 的新方法。 POSH 为每个蛋白质结构学习一个二进制向量表示，与基于实值向量表示的方法相比，这可以大大减少 PSSS 的时间和内存成本。此外，在 POSH 中，我们还提出了富有表现力的手工制作特征和结构编码器，以很好地模拟蛋白质中的节点和边缘相互作用。在真实数据集上的实验结果表明，POSH 可以胜过其他方法，实现最先进的准确率。此外，与其他方法相比，POSH 实现了六倍以上的内存节省和四倍以上的速度提升。]]></description>
      <guid>https://arxiv.org/abs/2411.08286</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>TowerDebias：一种基于 Tower 属性的新型去偏方法</title>
      <link>https://arxiv.org/abs/2411.08297</link>
      <description><![CDATA[arXiv:2411.08297v1 公告类型：新
摘要：决策过程越来越依赖于复杂的机器学习工具，这引发了人们对其预测对任何敏感群体的公平性的担忧。商业黑盒机器学习模型的广泛使用需要仔细考虑其对消费者的法律和道德影响。在用户可以访问这些“黑盒”模型的情况下，出现了一个关键问题：我们如何减轻或消除种族或性别等敏感属性的影响？我们提出了 towerDebias (tDB)，这是一种旨在减少敏感变量对黑盒模型预测影响的新方法。使用概率论中的 Tower 属性，tDB 旨在以符合公平效用权衡的方式在后处理阶段提高预测公平性。该方法高度灵活，不需要事先了解原始模型的内部结构，并且可以扩展到一系列不同的应用程序。我们为 tDB 提供了一个正式的改进定理，并证明了其在回归和分类任务中的有效性，强调了其对公平性效用权衡的影响。]]></description>
      <guid>https://arxiv.org/abs/2411.08297</guid>
      <pubDate>Thu, 14 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>