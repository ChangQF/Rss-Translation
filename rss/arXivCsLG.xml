<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Fri, 16 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>基于机器学习的无线定位的复杂性降低：最低描述特征</title>
      <link>https://arxiv.org/abs/2402.09580</link>
      <description><![CDATA[arXiv:2402.09580v1 公告类型：新
摘要：最近的一项研究一直在研究无线定位（WP）的深度学习方法。尽管这些 WP 算法在不同的信道条件下表现出了高精度和稳健的性能，但它们也有一个主要缺点：它们需要处理高维特征，这对于移动应用程序来说可能是令人望而却步的。在这项工作中，我们设计了一种定位神经网络（P-NN），通过精心设计的最小描述特征，大大降低了基于深度学习的 WP 的复杂性。我们的特征选择基于最大功率测量及其时间位置，以传达进行 WP 所需的信息。我们还开发了一种自适应选择特征空间大小的新颖方法，该方法优化了有用信息的预期数量和分类能力的平衡，并使用信号仓选择的信息论测量进行量化。数值结果表明，与利用全功率延迟分布 (PDP) 的深度学习基线相比，P-NN 在性能与复杂性权衡方面取得了显着优势。]]></description>
      <guid>https://arxiv.org/abs/2402.09580</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>WERank：利用权重正则化实现自我监督学习的排名降级预防</title>
      <link>https://arxiv.org/abs/2402.09586</link>
      <description><![CDATA[arXiv:2402.09586v1 公告类型：新
摘要：自监督学习（SSL）中限制表示质量的常见现象是维度崩溃（也称为秩退化），其中学习到的表示被映射到表示空间的低维子空间。最先进的 SSL 方法已被证明会遭受维度崩溃并落后于维持满排名。最近提出了使用对比损失、正则化技术或架构技巧来防止此问题的方法。我们提出了 WERank，一种关于网络权重参数的新正则化器，以防止网络不同层的排名退化。我们提供经验证据和数学论证来证明所提出的正则化方法在防止维度崩溃方面的有效性。我们验证了 WERank 对图 SSL 的影响，其中由于缺乏适当的数据增强，维度崩溃更加明显。我们凭经验证明，WERank 可以有效帮助 BYOL 在 SSL 预训练期间获得更高的排名，从而在评估探测期间获得更高的下游准确性。消融研究和实验分析揭示了所提出的方法性能提升背后的潜在因素。]]></description>
      <guid>https://arxiv.org/abs/2402.09586</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:55 GMT</pubDate>
    </item>
    <item>
      <title>Neyman-Pearson 分类中的无分配利率</title>
      <link>https://arxiv.org/abs/2402.09560</link>
      <description><![CDATA[arXiv:2402.09560v1 公告类型：新
摘要：我们考虑 Neyman-Pearson 分类的问题，该分类对不平衡的分类设置进行建模，其中误差 w.r.t.分布 $\mu_1$ 将在误差较低的情况下最小化。不同的分布$\mu_0$。给定要最小化的固定 VC 类 $\mathcal{H}$ 分类器，我们提供了可能的无分布率的完整特征，即所有对 $(\mu_0, \mu_1)$ 空间上的极小最大率。这些比率涉及困难类和简单类 $\mathcal{H}$ 之间的二分法，其特征在于简单的几何条件、三点分离条件，与 VC 维度松散相关。]]></description>
      <guid>https://arxiv.org/abs/2402.09560</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>蝴蝶的变化：用群水库变压器进行有远见的预测</title>
      <link>https://arxiv.org/abs/2402.09573</link>
      <description><![CDATA[arXiv:2402.09573v1 公告类型：新
摘要：在混沌中，两个初始条件之间的微小差异会随着时间的推移呈现指数放大，导致遥远的结果，称为蝴蝶效应。因此，遥远的未来充满了不确定性，难以预测。我们引入 Group Reservoir Transformer，通过克服混沌中的两个挑战，更准确、更稳健地预测长期事件：(1) 广泛的历史序列和 (2) 对初始条件的敏感性。储层附加到 Transformer 上，以有效处理任意长的历史长度，并扩展一组储层以减少由于初始化变化而导致的不确定性。我们的架构在多元时间序列上始终优于最先进的 DNN 模型，包括 NLinear、Pyformer、Informer、Autoformer 和基线 Transformer，在 ETTh、 ETTm 和空气质量，证明了蝴蝶学习的集合，可以将预测改进为更充分和确定的预测，尽管到达未知的未来需要时间。]]></description>
      <guid>https://arxiv.org/abs/2402.09573</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:54 GMT</pubDate>
    </item>
    <item>
      <title>分层近端重放：一种在线持续学习的近点方法</title>
      <link>https://arxiv.org/abs/2402.09542</link>
      <description><![CDATA[arXiv:2402.09542v1 公告类型：新
摘要：在在线持续学习中，神经网络从非独立同分布的模型中增量学习。数据流。几乎所有在线持续学习方法都采用经验回放来同时防止灾难性遗忘和对过去数据的欠拟合。我们的工作证明了这种方法的局限性：通过经验重播训练的网络往往具有不稳定的优化轨迹，从而阻碍了其整体准确性。令人惊讶的是，即使重放缓冲区存储了所有以前的训练示例，这些不稳定性仍然存在，这表明这个问题与灾难性遗忘是正交的。我们通过对优化几何结构的简单修改来最大限度地减少这些不稳定性。我们的解决方案，分层近端重放（LPR），平衡了新数据和重放数据的学习，同时只允许过去数据的隐藏激活逐渐变化。我们证明，无论可用的重放内存量有多大，LPR 都能在多个问题设置中持续改进基于重放的在线持续学习方法。]]></description>
      <guid>https://arxiv.org/abs/2402.09542</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>用于改进离线策略学习的数据集聚类</title>
      <link>https://arxiv.org/abs/2402.09550</link>
      <description><![CDATA[arXiv:2402.09550v1 公告类型：新
摘要：离线策略学习旨在从先前收集的数据集中发现决策策略，而无需与环境进行额外的在线交互。由于训练数据集是固定的，其质量成为学习策略性能的关键决定因素。本文研究了我们称为多行为的数据集特征，表明数据集是使用表现出不同行为的多个策略来收集的。相比之下，单一行为数据集将仅使用一种策略来收集。我们观察到，从单行为数据集学习的策略通常优于从多行为数据集学习的策略，尽管单行为数据集的示例较少且多样性较低。因此，我们提出了一种行为感知深度聚类方法，将多行为数据集划分为多个单行为子集，从而有利于下游策略学习。我们的方法灵活有效；它可以自适应地估计聚类数量，同时表现出较高的聚类精度，在各种连续控制任务数据集上实现平均调整 Rand 指数 0.987。最后，我们提出使用数据集聚类改进的政策学习示例，并讨论我们的方法可能使离线政策学习社区受益的几种潜在场景。]]></description>
      <guid>https://arxiv.org/abs/2402.09550</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:53 GMT</pubDate>
    </item>
    <item>
      <title>PMGDA：基于偏好的多重梯度下降算法</title>
      <link>https://arxiv.org/abs/2402.09492</link>
      <description><![CDATA[arXiv:2402.09492v1 公告类型：新
摘要：在许多多目标机器学习应用中，例如多任务学习和多目标强化学习，需要找到一个能够完全匹配决策者给定偏好的帕累托最优解。这些问题通常是大规模的，具有可用的梯度信息，但现有算法无法很好地处理。为了解决这个关键问题，本文提出了一种新颖的预测和纠正框架，用于定位决策者所需的精确帕累托最优解。在所提出的框架中，在搜索过程中引入了约束函数，以使解决方案与用户特定的偏好保持一致，可以使用多个目标函数同时进行优化。实验结果表明，我们提出的方法可以有效地为具有数千个决策变量的标准基准、多任务和多目标强化学习问题找到精确的帕累托最优解。
  代码位于：\url{https://github.com/xzhang2523/pmgda}。]]></description>
      <guid>https://arxiv.org/abs/2402.09492</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>流形密度函数：验证流形学习的内在方法</title>
      <link>https://arxiv.org/abs/2402.09529</link>
      <description><![CDATA[arXiv:2402.09529v1 公告类型：新
摘要：我们引入了流形密度函数，它是验证流形学习技术的内在方法。我们的方法适应并扩展了 Ripley 的 $K$ 函数，并在无监督的设置中对流形学习算法的输出捕获潜在流形结构的程度进行分类。我们的流形密度函数可推广到广泛的黎曼流形。特别是，我们使用高斯-邦内定理将流形密度函数扩展到一般的二流形，并证明超曲面的流形密度函数可以使用第一拉普拉斯特征值很好地近似。我们证明了理想的收敛性和鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2402.09529</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:52 GMT</pubDate>
    </item>
    <item>
      <title>通过列生成进行一对多反事实解释</title>
      <link>https://arxiv.org/abs/2402.09473</link>
      <description><![CDATA[arXiv:2402.09473v1 公告类型：新
摘要：在本文中，我们考虑使用一对多分配规则为一组实例生成一组反事实解释的问题，其中一个解释被分配给实例的子组。我们第一次解决了最小化解释所有实例所需的解释数量的问题，同时通过限制每个解释中允许集体更改的特征数量来考虑稀疏性。开发了一种新颖的列生成框架来有效地搜索解释。我们的框架可以应用于任何黑盒分类器，例如神经网络。与文献中混合整数规划公式的简单改编相比，列生成框架在可扩展性、计算性能和解决方案的质量方面占据主导地位。]]></description>
      <guid>https://arxiv.org/abs/2402.09473</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    <item>
      <title>UMOEA/D：基于分解的均匀帕累托目标的多目标进化算法</title>
      <link>https://arxiv.org/abs/2402.09486</link>
      <description><![CDATA[arXiv:2402.09486v1 公告类型：新
摘要：多目标优化（MOO）在许多应用中普遍存在，其中构建帕累托前沿（PF）来显示各种偏好下的最优值。以前的方法通常利用帕累托目标集（PF 上的粒子）来表示整个 PF。然而，帕累托目标在PF上的经验分布很少被研究，这隐含地阻碍了先前方法中多样化和代表性帕累托目标的生成。为了弥补这一差距，我们建议在本文中构建 PF 上的 \emph{均匀分布} Pareto 目标，以缓解先前 MOO 方法中发现的有限多样性。我们是第一个正式定义 MOO 问题“均匀性”概念的人。我们使用神经网络优化 Pareto 前沿上的最大最小距离，从而产生渐近和非渐近均匀 Pareto 目标。我们提出的方法是通过对现实世界和合成问题的实验进行了验证，这证明了生成高质量统一帕累托目标的有效性以及超越现有最先进方法的令人鼓舞的性能。
  详细的模型实现和代码计划在发布后开源。]]></description>
      <guid>https://arxiv.org/abs/2402.09486</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:51 GMT</pubDate>
    </item>
    <item>
      <title>滚动扩散模型</title>
      <link>https://arxiv.org/abs/2402.09470</link>
      <description><![CDATA[arXiv:2402.09470v1 公告类型：新
摘要：扩散模型最近越来越多地应用于时态数据，例如视频、流体力学模拟或气候数据。这些方法通常对于扩散过程中的噪声量同等对待后续帧。本文探讨了滚动扩散：一种使用滑动窗口去噪过程的新方法。它通过向序列中较晚出现的帧分配更多噪声，确保扩散过程随着时间的推移逐渐恶化，反映出随着生成过程的展开，未来的不确定性更大。根据经验，我们表明，当时间动态复杂时，滚动扩散优于标准扩散。特别是，这个结果在使用 Kinetics-600 视频数据集的视频预测任务和混沌流体动力学预测实验中得到了证明。]]></description>
      <guid>https://arxiv.org/abs/2402.09470</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>随机参数化的机器学习</title>
      <link>https://arxiv.org/abs/2402.09471</link>
      <description><![CDATA[arXiv:2402.09471v1 公告类型：新
摘要：用于天气和气候预测的大气模型传统上是以确定性方式制定的。换句话说，给定已解析尺度变量的特定状态，估计来自子网格尺度过程的最可能的强迫并用于预测大规模流的演化。然而，大气中缺乏尺度分离意味着这种方法是预测误差的一大来源。近年来，出现了另一种范式：使用随机技术来表征小规模过程中的不确定性。这些技术现在广泛应用于天气、次季节、季节性和气候时间尺度。与此同时，近年来在使用机器学习 (ML) 替代参数化方案方面也取得了重大进展。这有可能加速和改进我们的数值模型。然而，迄今为止的焦点主要集中在确定性方法上。在这篇立场文件中，我们将这两个关键进展结合在一起，并讨论了数据驱动的随机参数化方法的潜力。我们重点介绍该领域的早期研究，并提请人们注意仍然存在的新挑战。]]></description>
      <guid>https://arxiv.org/abs/2402.09471</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:50 GMT</pubDate>
    </item>
    <item>
      <title>使用可持续深度径向函数对智慧城市中的交通智能进行增强分析</title>
      <link>https://arxiv.org/abs/2402.09432</link>
      <description><![CDATA[arXiv:2402.09432v1 公告类型：新
摘要：智慧城市通过结合先进技术来优化城市基础设施的各个方面（例如交通系统），彻底改变了城市生活。有效的交通管理是智慧城市的重要组成部分，因为它直接影响居民和游客的生活质量。本文利用深度径向基函数（RBF）网络，描述了一种增强智慧城市交通智能的新策略。传统的交通分析方法经常依赖于简单化的模型，这些模型无法捕捉城市交通系统的复杂模式和动态。深度学习技术（例如深度 RBF 网络）有潜力从流量数据中提取有价值的见解，并实现更精确的预测和决策。在本文中，我们提出了一种基于 RBF 的方法来增强智慧城市交通智能。深度RBF网络将深度学习的适应性和泛化能力与径向基函数的判别能力结合起来。该方法可以利用深度神经网络的层次结构有效地学习交通数据中的复杂关系和非线性模式。深度 RBF 模型可以学习预测交通状况、识别拥堵模式，并通过结合这些丰富多样的数据为优化交通管理策略提出明智的建议。开展了智慧城市环境建设。在预测精度和效率方面，结果表明基于深度 RBF 的方法优于传统的流量分析方法。模型捕获非线性关系和管理大规模数据集的能力增强了智慧城市交通智能。]]></description>
      <guid>https://arxiv.org/abs/2402.09432</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>未知游戏中无悔学习的乐观汤普森采样</title>
      <link>https://arxiv.org/abs/2402.09456</link>
      <description><![CDATA[arXiv:2402.09456v1 公告类型：新
摘要：许多涉及多个决策者的现实世界问题可以建模为以部分观察为特征的未知博弈。为了解决部分信息带来的挑战和多机构的诅咒，我们开发了汤普森采样型算法，利用有关对手行动和奖励结构的信息。我们的方法显着减少了实验预算，与交通路由和雷达传感等实际应用中的基准算法相比，减少了十倍以上。我们证明，在奖励结构的某些假设下，遗憾界限仅表现出对总行动空间大小的对数依赖性，有效地减轻了多代理的诅咒。此外，这项研究还引入了 Optimism-then-NoRegret 框架，这是一项新颖的贡献，集成了我们提出的方法和该领域现有的算法。]]></description>
      <guid>https://arxiv.org/abs/2402.09456</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中的傅立叶电路：释放大型语言模型在数学推理和模块化算术中的潜力</title>
      <link>https://arxiv.org/abs/2402.09469</link>
      <description><![CDATA[arXiv:2402.09469v1 公告类型：新
摘要：在不断发展的机器学习领域，一个关键的挑战在于破译神经网络和 Transformers 利用的内部表示。基于最近在理解网络如何执行不同目标功能方面取得的进展，我们的研究开始探索网络采用特定计算策略背后的根本原因。我们将重点放在涉及 $k$ 输入的模加法的复杂代数学习任务上。我们的研究对程式化单隐藏层神经网络和单层 Transformer 在解决这一任务时所学到的特征进行了全面的分析表征。
  我们理论框架的基石是阐明边缘最大化原理如何塑造单隐藏层神经网络所采用的特征。令$p$表示模数，$D_p$表示具有$k$输入的模算术数据集，$m$表示网络宽度。我们证明神经元数量为 $ m \geq 2^{2k-2} \cdot (p-1) $，这些网络在数据集 $ D_p $ 上达到最大 $ L_{2,k+1} $-margin 。此外，我们确定每个隐藏层神经元都与特定的傅里叶谱对齐，这是解决模加法问题的组成部分。
  通过将我们的发现与类似研究的实证观察联系起来，我们有助于更深入地理解神经网络的内在计算机制。此外，我们在 Transformer 的注意力矩阵中观察到类似的计算机制。这项研究在解决其运算复杂性方面迈出了重大一步，特别是在复杂代数任务领域。]]></description>
      <guid>https://arxiv.org/abs/2402.09469</guid>
      <pubDate>Fri, 16 Feb 2024 06:16:49 GMT</pubDate>
    </item>
    </channel>
</rss>