<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Thu, 17 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>基于模拟的散射表征推理：散射就是你所需要的</title>
      <link>https://arxiv.org/abs/2410.11883</link>
      <description><![CDATA[arXiv:2410.11883v1 公告类型：新
摘要：我们展示了首次成功使用散射表示而无需进一步压缩来进行基于模拟的图像推理 (SBI)（即场级），并以宇宙学案例研究为例。散射表示为后续学习任务提供了高效的表示空间，尽管更高维度的压缩空间带来了挑战。我们通过空间平均以及更具表现力的密度估计器克服了这些问题。与其他方法相比，这种方法不需要额外的模拟来训练或计算导数，是可解释的，并且能够抵御协变量偏移。正如预期的那样，我们表明，仅散射方法比传统的二阶汇总统计数据提取了更多信息。]]></description>
      <guid>https://arxiv.org/abs/2410.11883</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用图注意网络和 LSTM 进行时空轴承故障检测</title>
      <link>https://arxiv.org/abs/2410.11923</link>
      <description><![CDATA[arXiv:2410.11923v1 公告类型：新 
摘要：目的：本文旨在通过介绍一种结合图形注意网络 (GAT) 和长短期记忆 (LSTM) 网络的新方法来增强工业机械中的轴承故障诊断。这种方法可以捕获传感器数据中的空间和时间依赖关系，从而提高各种条件下轴承故障检测的准确性。方法：所提出的方法将时间序列传感器数据转换为图形表示。GAT 捕获组件之间的空间关系，而 LSTM 模拟时间模式。该模型使用凯斯西储大学 (CWRU) 轴承数据集进行验证，该数据集包括不同马力水平以及正常和故障条件下的数据。将其性能与 K-最近邻 (KNN)、局部离群因子 (LOF)、孤立森林 (IForest) 和基于 GNN 的轴承故障检测方法 (GNNBFD) 等方法进行了比较。发现：该模型取得了出色的结果，在各种测试条件下，准确率、召回率和 F1 分数均达到 100%。它不仅可以准确识别故障，还可以在不同的操作场景中有效推广，优于传统方法。原创性：这项研究提出了一种独特的 GAT 和 LSTM 组合用于故障检测，通过捕捉复杂的时空依赖关系克服了传统时间序列方法的局限性。其卓越的性能表明其在工业应用中具有巨大的预测性维护潜力。]]></description>
      <guid>https://arxiv.org/abs/2410.11923</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于全国核辐射预报的提示式时空变换模型</title>
      <link>https://arxiv.org/abs/2410.11924</link>
      <description><![CDATA[arXiv:2410.11924v1 公告类型：新 
摘要：核辐射（NR）是指原子核在衰变过程中释放的能量，对人类健康和环境安全构成重大风险。准确预测核辐射水平对于个人和政府的明智决策至关重要。然而，由于监测站在广阔的空间范围内分布不平衡，以及辐射变化模式不稳定，这项任务具有挑战性。在本研究中，我们介绍了 NRFormer，这是一个专为全国范围内预测核辐射变化而量身定制的创新框架。通过整合非平稳时间注意模块、不平衡感知空间注意模块和辐射传播提示模块，NRFormer 共同捕捉核辐射复杂的时空动态。在两个真实世界数据集上进行的大量实验证明了我们提出的框架相对于七个基线的优越性。该研究不仅提高了核辐射预报的准确性和可靠性，而且有助于推进应急响应策略和监测系统，从而保障环境和公众健康。]]></description>
      <guid>https://arxiv.org/abs/2410.11924</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用精炼信息和模式交互选择对 KL 误差进行完整分解</title>
      <link>https://arxiv.org/abs/2410.11964</link>
      <description><![CDATA[arXiv:2410.11964v1 公告类型：新
摘要：对数线性模型在过去几十年中获得了大量的理论关注，并且仍然是学习离散变量概率分布的基本工具。尽管它在统计力学和高维统计中非常流行，但绝大多数此类基于能量的建模方法仅关注双变量关系，例如玻尔兹曼机和马尔可夫图模型。虽然这些方法具有更易于解决的结构学习问题和更易于优化的参数分布，但它们往往忽略了存在于不同变量之间高阶相互作用中的丰富结构。使用信息几何领域的最新工具，我们重新审视对数线性模型的经典公式，重点关注高阶模式相互作用，超越了独立分布的 1 体模式和玻尔兹曼分布的 2 体模式。这种观点使我们能够定义 KL 误差的完整分解。这促使我们在可能的模式交互集上制定稀疏选择问题。正如稀疏图选择可以实现更好的泛化一样，我们发现我们学习到的分布能够更有效地利用实践中可用的有限数据量。在合成数据集和现实世界数据集上，我们展示了我们的算法在最大化生成任务的对数似然方面的有效性，以及对分类判别任务的易适应性。]]></description>
      <guid>https://arxiv.org/abs/2410.11964</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>DDIL：通过模仿学习改进扩散蒸馏</title>
      <link>https://arxiv.org/abs/2410.11971</link>
      <description><![CDATA[arXiv:2410.11971v1 公告类型：新
摘要：扩散模型在生成建模（例如文本到图像）方面表现出色，但采样需要多次去噪网络传递，限制了实用性。渐进式蒸馏或一致性蒸馏等努力已显示出希望，因为它们以牺牲生成样本的质量为代价减少了传递次数。在这项工作中，我们将协变量偏移确定为多步蒸馏模型性能不佳的原因之一，因为它在推理时会产生复合误差。为了解决协变量偏移问题，我们在模仿学习 (DDIL) 框架内制定了扩散蒸馏，并增强了数据分布（前向扩散）和学生诱导分布（后向扩散）上的蒸馏扩散模型的训练分布。通过保留边际数据分布，对数据分布进行训练有助于实现世代多样化，而对学生分布进行训练则通过纠正协变量偏移来解决复合误差问题。此外，我们采用反射扩散公式进行蒸馏，并展示了不同蒸馏方法的性能改进和稳定的训练。我们表明，DDIL 一致性在渐进式蒸馏 (PD)、潜在一致性模型 (LCM) 和分布匹配蒸馏 (DMD2) 的基线算法上得到了改进。]]></description>
      <guid>https://arxiv.org/abs/2410.11971</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>随机接入信道上联邦学习的梯度年龄更新</title>
      <link>https://arxiv.org/abs/2410.11986</link>
      <description><![CDATA[arXiv:2410.11986v1 公告类型：新
摘要：本文研究了在计算机网络、无线网络和蜂窝系统等随机接入信道 (RACH) 上联合训练深度神经网络 (DNN) 的问题。更准确地说，一组远程用户在参数服务器 (PS) 的协调下使用 SGD 参与训练集中式 DNN 模型。本地模型更新通过时隙 ALOHA 协议从远程用户通过 RACH 传输到 PS。PS 收集来自远程用户的更新，累积它们，并定期将中央模型更新发送给用户。我们将此设置称为 RACH-FL 设置。RACH-FL 设置主要解决联合设计 (i) 客户端选择和 (ii) 梯度压缩策略的问题，该策略解决了通过 RACH 进行传输时远程用户和 PS 之间的通信约束。对于 RACH-FL 设置，我们提出了一种策略，我们称之为“梯度年龄”（AoG）策略，其中 (i) 使用 top-K 稀疏化执行梯度稀疏化，(ii) 使用记忆累积执行错误校正，以及 (iii) 通过将当前本地记忆幅度减去梯度更新幅度与阈值进行比较来获得时隙传输概率。直观地说，记忆状态“新鲜度”的 AoG 度量让人想起通信理论背景下的信息年龄 (AoI) 的概念，并为该策略提供了相当自然的解释。数值模拟表明，与其他 RACH-FL 策略相比，AoG 策略具有更优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.11986</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大型语言模型之间的偏差相似性</title>
      <link>https://arxiv.org/abs/2410.12010</link>
      <description><![CDATA[arXiv:2410.12010v1 公告类型：新
摘要：机器学习模型中的偏见一直是一个长期存在的问题，尤其是当这些模型影响人类社会的决策时。在生成式人工智能中，例如大型语言模型，与分类模型相比，偏见的影响更为深远。LLM 会产生逼真且类似人类的内容，用户可能会无意识地信任这些内容，这可能会给不受控制的公众带来有害的刻板印象。当它用于新闻或教育时，尤其令人担忧。虽然先前的研究已经探索并量化了单个人工智能模型中的偏见，但还没有研究比较不同 LLM 之间的偏见相似性。为了填补这一空白，我们全面研究了四个模型系列中的十个开源和闭源 LLM，通过输出分布评估偏见的程度。使用两个数据集（一个包含 4k 个问题，另一个包含四个偏见维度中的每一个的一百万个问题）我们测量功能相似性以了解偏见在模型中的表现方式。我们的研究结果表明：1）微调不会显著改变输出分布，这会限制其减轻偏差的能力；2）同一家族树中的 LLM 不会产生类似的输出分布，这意味着解决一个模型中的偏差对同一家族中的其他模型的影响可能有限；3）存在训练数据信息泄露的风险，引发了对隐私和数据安全的担忧。我们的分析深入了解了 LLM 行为，并强调了实际部署中的潜在风险。]]></description>
      <guid>https://arxiv.org/abs/2410.12010</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度网络的几何归纳偏差：数据和架构的作用</title>
      <link>https://arxiv.org/abs/2410.12025</link>
      <description><![CDATA[arXiv:2410.12025v1 公告类型：新
摘要：在本文中，我们提出了$\textit{几何不变性假设 (GIH)}$，该假设认为在训练神经网络时，输入空间曲率在由其架构确定的某些方向上的变换下保持不变。从一个简单的非线性二元分类问题开始，该问题位于高维空间中的平面上，我们观察到虽然 MLP 可以解决这个问题，而不管平面的方向如何，但 ResNet 并非如此。受此示例的启发，我们定义了两个映射，它们提供了神经网络的输入空间几何形状及其在训练过程中的演变的紧凑的$\textit{架构相关}$摘要，我们分别将其称为$\textbf{平均几何}$和$\textbf{平均几何演变}$。通过研究初始化时的平均几何演变，我们发现神经网络的几何形状根据数据协方差在平均几何形状上的投影而演变。因此，在平均几何结构为低秩的情况下（例如在 ResNet 中），几何结构仅在输入空间的子集中发生变化。这会导致输入空间曲率中出现与架构相关的不变性，我们将其称为 GIH。最后，我们展示了大量实验结果，以观察 GIH 的后果以及它与神经网络泛化的关系。]]></description>
      <guid>https://arxiv.org/abs/2410.12025</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度表格学习综述</title>
      <link>https://arxiv.org/abs/2410.12034</link>
      <description><![CDATA[arXiv:2410.12034v1 公告类型：新
摘要：表格数据广泛应用于医疗保健、金融和交通等行业，由于其异构性和缺乏空间结构，给深度学习带来了独特的挑战。本综述回顾了表格数据深度学习模型的演变，从早期的全连接网络 (FCN) 到 TabNet、SAINT、TabTranSELU 和 MambaNet 等高级架构。这些模型结合了注意力机制、特征嵌入和混合架构来解决表格数据的复杂性。TabNet 使用顺序注意力进行实例特征选择，提高了可解释性，而 SAINT 结合了自注意力和样本间注意力来捕获特征和数据点之间的复杂交互，既提高了可扩展性又降低了计算开销。TabTransformer 和 FT-Transformer 等混合架构将注意力机制与多层感知器 (MLP) 集成在一起，以处理分类和数值数据，FT-Transformer 则将变压器适配到表格数据集。研究仍在继续，以平衡大型数据集的性能和效率。基于图形的模型（如 GNN4TDL 和 GANDALF）将神经网络与决策树或图形结构相结合，通过高级正则化技术增强特征表示并减轻小数据集中的过度拟合。基于扩散的模型（如表格去噪扩散概率模型 (TabDDPM)）生成合成数据以解决数据稀缺问题，从而提高模型的鲁棒性。同样，TabPFN 和 Ptab 等模型利用预先训练的语言模型，将迁移学习和自监督技术融入表格任务中。本综述重点介绍了关键进展，并概述了各种表格数据应用程序中可扩展性、泛化和可解释性的未来研究方向。]]></description>
      <guid>https://arxiv.org/abs/2410.12034</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>测试因果解释：了解干预措施对慢性肾脏病的影响的案例研究</title>
      <link>https://arxiv.org/abs/2410.12047</link>
      <description><![CDATA[arXiv:2410.12047v1 公告类型：新
摘要：随机对照试验 (RCT) 是评估临床干预效果的标准。为了解决 RCT 对现实世界人群的局限性，我们开发了一种使用大型观察性电子健康记录 (EHR) 数据集的方法。回归不连续性 (rd) 原理用于导出随机数据子集，以使用动态贝叶斯网络 (DBN) 执行操作来测试专家驱动的干预措施。这种组合方法应用于超过 200 万个体的慢性肾病 (CKD) 队列，并用于了解 CKD 变量与估计肾小球滤过率 (eGFR) 下降 &gt;=40% 的替代结果之间的关联和因果关系。关联和因果分析在两个独立医疗系统的 DBN 中描绘了类似的发现。关联分析显示，最具影响力的变量是 eGFR、尿白蛋白与肌酐比和脉压，而因果分析显示，eGFR 是最具影响力的变量，其次是可改变的因素，例如可能随着时间的推移影响肾功能的药物。这种方法论展示了如何使用现实世界的 EHR 数据来提供人口层面的见解，从而为改善医疗保健服务提供信息。]]></description>
      <guid>https://arxiv.org/abs/2410.12047</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>摘下玫瑰色眼镜：从逃避攻击的角度批判性地看待对抗性机器学习</title>
      <link>https://arxiv.org/abs/2410.12076</link>
      <description><![CDATA[arXiv:2410.12076v1 公告类型：新
摘要：过去十年，机器学习模型在对抗场景中的脆弱性引起了学术界的极大关注，导致了无数的攻击和防御。然而，尽管社区似乎在设计新的攻击以应对新的环境方面取得了明显的成功，但防御的发展却停滞不前。经过十年的研究，我们似乎并没有在额外的训练之外更进一步保护人工智能应用。尽管缺乏有效的缓解措施，但随着生成式人工智能和大型语言模型的兴起，人工智能的发展及其与现有系统的结合正在全速推进。我们在开发对抗威胁解决方案方面的无效性是否会进一步延伸到这些新技术？
在本文中，我们认为过于宽容的攻击和过于严格的防御威胁模型阻碍了机器学习领域的防御发展。通过对抗性规避攻击神经网络的视角，我们严格审查了常见的攻击假设，例如绕过模型中未明确内置的任何防御的能力。我们认为，这些有缺陷的假设（社区根据论文的接受程度认为这些假设是合理的）鼓励了对抗性攻击的发展，而这些攻击与现实世界的情况不太吻合。反过来，针对这些攻击进行评估的新防御措施被无意中要求几乎完美无缺，并被纳入模型的一部分。但他们需要这样做吗？在实践中，机器学习模型被部署为大型系统的一个小组件。我们从系统安全角度而不是人工智能角度分析对抗性机器学习及其对新兴人工智能范式的影响。]]></description>
      <guid>https://arxiv.org/abs/2410.12076</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>协作老虎机算法的比较性能：稀疏性和探索强度的影响</title>
      <link>https://arxiv.org/abs/2410.12086</link>
      <description><![CDATA[arXiv:2410.12086v1 公告类型：新
摘要：本文对协作式老虎机算法进行了全面分析，并对其性能进行了彻底的比较。协作式老虎机旨在通过引入各臂（或项目）之间的关系来提高情境式老虎机的性能，从而有效地传播信息。各臂之间的协作允许将通过单个用户（项目）获得的反馈共享给相关用户（项目）。引入协作还可以缓解冷用户（项目）问题，即当新用户（项目）到达平台时，没有先前的交互记录时缺乏历史信息。在对各臂（项目）之间的关系进行建模的背景下，主要有两种方法：硬聚类和软聚类。我们将以 \textit{absolute} 方式对各臂之间关系进行建模的方法称为硬聚类，即关系是二元的。软聚类放宽了成员资格约束，允许 \textit{fuzzy} 分配。重点关注后者，我们对最先进的协作上下文强盗算法进行了广泛的实验，并研究了稀疏性的影响以及探索强度如何充当校正机制。我们的数值实验表明，控制协作中的稀疏性可以提高数据效率和性能，因为它可以更好地指导学习。同时，增加探索强度可以起到校正作用，因为它可以有效减少由于用户之间可能错误指定的关系而导致的方差。我们观察到，通过引入潜在因素，从而增加强盗参数的维度，可以进一步纠正这种错误指定。]]></description>
      <guid>https://arxiv.org/abs/2410.12086</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>连接大型语言模型和图结构学习模型以实现稳健的表征学习</title>
      <link>https://arxiv.org/abs/2410.12096</link>
      <description><![CDATA[arXiv:2410.12096v1 公告类型：新
摘要：图表示学习涉及节点特征和图结构，对于实际应用至关重要，但经常会遇到普遍存在的噪声。最先进的方法通常通过分别关注大型语言模型 (LLM) 的节点特征和图结构学习模型 (GSLM) 的图结构来解决噪声问题。在本文中，我们介绍了一个强大的框架 LangGSL，它集成了预训练语言模型和 GSLM 的互补优势，共同增强了节点特征和图结构学习。在 LangGSL 中，我们首先利用 LLM 过滤原始数据中的噪声并提取有价值的清洁信息作为特征，从而增强下游模型的协同作用。在 LangGSL 的相互学习阶段，核心思想是利用相对较小的语言模型 (LM) 来处理局部属性并生成可靠的伪标签和信息丰富的节点嵌入，然后将其集成到 GSLM 的预测阶段。这种方法丰富了全局背景并提高了整体性能。同时，GSLM 改进了从 LM 的输出构建的不断演化的图结构，将更新的标签作为额外的指导反馈给 LM，从而促进更有效的相互学习过程。LM 和 GSLM 协同工作，在变分信息最大化框架内互补优势、抵消劣势，从而增强节点特征并实现更稳健的图结构。在不同规模和不同任务场景的各种图数据集上进行的大量实验证明了所提方法的可扩展性和有效性。]]></description>
      <guid>https://arxiv.org/abs/2410.12096</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>波斯地毯：利用大规模对称性解决叠加玩具模型</title>
      <link>https://arxiv.org/abs/2410.12101</link>
      <description><![CDATA[arXiv:2410.12101v1 公告类型：新
摘要：我们给出了在输入维度较大的情况下，由最小非线性稀疏数据自动编码器学习的算法的完整机制描述。该模型最初在 arXiv:2209.10652 中提出，通过线性层压缩稀疏数据向量，然后使用另一个线性层解压缩，然后进行 ReLU 激活。我们注意到，当数据是置换对称的（没有输入特征是特权的）时，大型模型可以可靠地学习一种仅通过大规模统计数据对单个权重敏感的算法。对于这些模型，损失函数变得易于分析。利用这种理解，我们给出了高稀疏度下损失的明确缩放比例，并表明该模型在最近提出的架构中接近最优。特别是，更改或向激活函数添加任何元素或过滤操作最多可以将模型的性能提高一个常数倍。最后，我们对具有必要对称性的模型进行正向工程，并表明其损失与训练模型的损失完全匹配。与训练模型权重不同，人工权重的低随机性产生了类似于波斯地毯的神奇分形结构，而算法对此却视而不见。我们的工作通过引入理解自动编码器结构的技术，为神经网络的可解释性做出了贡献。可以在 https://github.com/KfirD/PersianRug 找到用于重现我们结果的代码。]]></description>
      <guid>https://arxiv.org/abs/2410.12101</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>训练后量化大型语言模型的缩放定律</title>
      <link>https://arxiv.org/abs/2410.12119</link>
      <description><![CDATA[arXiv:2410.12119v1 公告类型：新
摘要：众所周知，训练良好的大型语言模型 (LLM) 的泛化能力可以作为模型大小的函数进行可预测的扩展。与管理预训练的实际扩展规律相比，训练后压缩后的 LLM 质量仍然非常难以预测，通常需要在实践中逐案验证。在这项工作中，我们试图通过对使用流行的权重量化技术量化为多种低精度张量数据类型的多个 LLM 系列进行系统的实证研究，以缩小 LLM 训练后权重量化的这一差距。我们确定了与局部损失景观特征有关的关键缩放因子，在此基础上，可以通过统计模型合理地预测量化后的 LLM 的性能。]]></description>
      <guid>https://arxiv.org/abs/2410.12119</guid>
      <pubDate>Thu, 17 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>