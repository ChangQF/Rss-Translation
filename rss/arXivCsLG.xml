<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新了 arXiv.org 电子打印档案。</description>
    <lastBuildDate>Mon, 27 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Polyak 遇见无参数截断梯度下降</title>
      <link>https://arxiv.org/abs/2405.15010</link>
      <description><![CDATA[arXiv:2405.15010v1 公告类型：新
摘要：梯度下降及其变体是训练机器学习模型的事实标准算法。由于梯度下降对其超参数很敏感，我们需要使用网格搜索仔细调整超参数，但这很耗时，尤其是在存在多个超参数时。最近，已经研究了动态调整超参数的无参数方法。然而，现有的工作只研究了步长的无参数方法，而其他超参数的无参数方法尚未探索。例如，除了步长之外，梯度剪裁阈值也是一个重要的超参数，可以防止梯度爆炸问题，但现有的研究都没有研究剪裁梯度下降的无参数方法。在本文中，我们研究了剪裁梯度下降的无参数方法。具体来说，我们提出了不精确 Polyak 步长，它无需任何超参数调整即可收敛到最优解，并且其收敛速度在 L 平滑和 $(L_0, L_1)$ 平滑假设下与 L 渐近无关，其中损失函数为具有良好调整的超参数的截断梯度下降。我们使用合成函数对收敛结果进行了数值验证，并使用 LSTM、Nano-GPT 和 T5 证明了我们提出的方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.15010</guid>
      <pubDate>Mon, 27 May 2024 06:20:51 GMT</pubDate>
    </item>
    <item>
      <title>通过数据相关充分统计扰动实现隐私回归</title>
      <link>https://arxiv.org/abs/2405.15002</link>
      <description><![CDATA[arXiv:2405.15002v1 公告类型：新 
摘要：充分统计扰动（SSP）是一种广泛使用的差分隐私线性回归方法。 SSP 采用与数据无关的方法，将来自简单分布的隐私噪声添加到足够的统计数据中。然而，足够的统计信息通常可以表示为线性查询，并通过数据相关机制更好地近似。在本文中，我们引入了基于后处理私人发布的边际数据的线性回归的数据相关 SSP，并发现它优于最先进的数据无关 SSP。我们通过开发一个可以用足够的统计数据来表达的近似目标，将此结果扩展到逻辑回归，从而产生一种新颖且极具竞争力的逻辑回归 SSP 方法。我们还与机器学习的合成数据建立了联系：对于具有足够统计数据的模型，合成数据的训练对应于数据相关的 SSP，整体效用取决于该机制回答这些线性查询的程度。]]></description>
      <guid>https://arxiv.org/abs/2405.15002</guid>
      <pubDate>Mon, 27 May 2024 06:20:50 GMT</pubDate>
    </item>
    <item>
      <title>路径度量、修剪和泛化</title>
      <link>https://arxiv.org/abs/2405.15006</link>
      <description><![CDATA[arXiv:2405.15006v1 公告类型：新 
摘要：分析 ReLU 神经网络的行为通常取决于理解其参数与其实现的功能之间的关系。本文根据所谓的参数路径度量证明了函数距离的新界限。由于该边界对于网络的缩放对称性本质上是不变的，因此它锐化了先前已知的边界。据我们所知，这也是第一个广泛适用于现代网络（例如 ResNets、VGG、U-net 等）的界限。在网络修剪和量化等上下文中，仅使用两次前向传递就可以有效地计算所提出的路径度量。除了其内在的理论兴趣之外，该界限不仅产生了新颖的理论泛化界限，而且还为重新缩放不变剪枝提供了有希望的概念证明。]]></description>
      <guid>https://arxiv.org/abs/2405.15006</guid>
      <pubDate>Mon, 27 May 2024 06:20:50 GMT</pubDate>
    </item>
    <item>
      <title>MaSS：从信息论角度进行保效数据转换的多属性选择性抑制</title>
      <link>https://arxiv.org/abs/2405.14981</link>
      <description><![CDATA[arXiv:2405.14981v1 公告类型：新 
摘要：大规模数据集的日益丰富对于推动机器学习技术的快速发展和广泛采用至关重要。然而，由于无意的不当处理或恶意利用，数据的大量收集和使用给人们的私人和敏感信息带来了越来越大的风险。除了立法解决方案外，还提出了许多数据隐私保护的技术方法。然而，它们具有各种局限性，例如导致数据可用性和实用性下降，或者依赖启发式方法并缺乏坚实的理论基础。为了克服这些限制，我们为这种保留效用的隐私保护问题提出了正式的信息论定义，并设计了一个数据驱动的可学习数据转换框架，该框架能够选择性地抑制目标数据集中的敏感属性，同时保留其他有用的属性，无论它们是否事先已知或明确注释以进行保存。我们对框架的操作范围提供严格的理论分析，并使用各种模式的数据集（包括面部图像、语音音频片段和人类活动运动传感器信号）进行全面的实验评估。结果证明了我们的方法在多种任务的各种配置下的有效性和通用性。]]></description>
      <guid>https://arxiv.org/abs/2405.14981</guid>
      <pubDate>Mon, 27 May 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>上下文时间序列预测器</title>
      <link>https://arxiv.org/abs/2405.14982</link>
      <description><![CDATA[arXiv:2405.14982v1 公告类型：新
摘要：最近基于 Transformer 的大型语言模型 (LLM) 展示了上下文学习能力，可以仅根据提供的上下文执行各种功能，而无需更新模型参数。为了充分利用时间序列预测 (TSF) 问题中的上下文功能，与以前基于 Transformer 或基于 LLM 的时间序列预测方法不同，我们将“时间序列预测任务”重新表述为输入标记，方法是在标记内构建一系列 (回顾、未来) 对。该方法与固有的上下文机制更加一致，并且参数效率更高，无需使用预先训练的 LLM 参数。此外，它解决了现有基于 Transformer 的 TSF 模型中的过拟合等问题，与以前的架构相比，在全数据、小样本和零样本设置中始终实现更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.14982</guid>
      <pubDate>Mon, 27 May 2024 06:20:49 GMT</pubDate>
    </item>
    <item>
      <title>了解神经网络中频率偏差的动态</title>
      <link>https://arxiv.org/abs/2405.14957</link>
      <description><![CDATA[arXiv:2405.14957v1 公告类型：新 
摘要：最近的研究表明，传统的神经网络（NN）架构在学习过程中表现出明显的频率偏差。也就是说，神经网络首先学习低频特征，然后再学习高频特征。在这项研究中，我们严格开发了一个偏微分方程 (PDE)，它揭示了神经正切核体系中 2 层神经网络的误差频率动态。此外，利用这种见解，我们明确证明了初始化权重分布的适当选择如何可以消除或控制频率偏差。我们的研究重点是傅里叶特征模型，这是一种神经网络，其中第一层具有正弦和余弦激活函数，频率从规定的分布中采样。在此设置中，我们通过实验验证了我们的理论结果，并将神经网络动力学与使用有限元方法的偏微分方程的解进行了比较。最后，我们凭经验证明，同样的原理也适用于多层神经网络。]]></description>
      <guid>https://arxiv.org/abs/2405.14957</guid>
      <pubDate>Mon, 27 May 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>不确定性下顺序决策的两阶段 ML 引导决策规则</title>
      <link>https://arxiv.org/abs/2405.14973</link>
      <description><![CDATA[arXiv:2405.14973v1 公告类型：新 
摘要：不确定性下的顺序决策（SDMU）在能源、金融和供应链等许多领域普遍存在。一些 SDMU 应用程序自然地建模为多级随机优化问题 (MSP)，但从计算的角度来看，由此产生的优化极具挑战性。在凸性和不确定性的阶段独立性的假设下，可以使用随机对偶动态规划 (SDDP) 有效地求解所得优化。两阶段线性决策规则 (TS-LDR) 已被提出来解决 MSP，而无需阶段独立性假设。 TS-LDR 在计算上易于处理，但使用过去观测值的线性函数的策略通常不适合出现的非凸环境，例如能源系统中的非凸环境。本文介绍了一种新颖的方法，即两阶段通用决策规则（TS-GDR），以概括线性函数之外的策略空间，使其适用于非凸环境。 TS-GDR是一种自监督学习算法，使用随机梯度下降（SGD）来训练非线性决策规则；其前向传递解决了策略执行优化问题，后向传递利用对偶理论获得封闭式梯度。 TS-GDR 的有效性通过使用名为两阶段深度决策规则 (TS-DDR) 的深度循环神经网络的实例化来证明。该方法继承了深度学习方法的灵活性和计算性能，可以解决通常通过大规模优化技术解决的 SDMU 问题。使用玻利维亚的实际电力系统数据将TS-DDR应用于长期热液调度（LTHD）问题，不仅提高了解决方案质量，而且还显着减少了几个数量级的计算时间。]]></description>
      <guid>https://arxiv.org/abs/2405.14973</guid>
      <pubDate>Mon, 27 May 2024 06:20:48 GMT</pubDate>
    </item>
    <item>
      <title>在天体粒子物理学中使用自动微分和神经传输进行快速推理</title>
      <link>https://arxiv.org/abs/2405.14932</link>
      <description><![CDATA[arXiv:2405.14932v1 公告类型：新 
摘要：在试图捕捉新现象的天体粒子物理理论中经常遇到多维参数空间。然而，它们通常具有复杂的后部几何形状，使用该社区的传统技术遍历这些几何形状的成本很高。对这些空间进行有效采样对于弥合实验与理论之间的差距至关重要。最近的几项创新才刚刚开始进入这一领域，使得导航如此复杂的后验成为可能。其中包括 GPU 加速、自动微分和神经网络引导的重新参数化。我们将这些进步应用于新型中微子物理背景下的天体粒子物理实验结果，并根据传统的嵌套采样技术对其性能进行基准测试。与单独的嵌套采样相比，我们发现这些技术提高了嵌套采样和哈密顿蒙特卡罗的性能，分别通过 $\sim 100$ 和 $\sim 60$ 加速推理。由于嵌套采样还评估贝叶斯证据，因此可以利用这些进步来提高模型比较性能，同时保持与自然科学中广泛使用的现有实现的兼容性。]]></description>
      <guid>https://arxiv.org/abs/2405.14932</guid>
      <pubDate>Mon, 27 May 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>Mallows-DPO：通过偏好分散微调您的法学硕士</title>
      <link>https://arxiv.org/abs/2405.14953</link>
      <description><![CDATA[arXiv:2405.14953v1 公告类型：新
摘要：直接偏好优化 (DPO) 最近成为一种流行的改进强化学习和人类反馈 (RLHF) 的方法，从而带来了更好的微调大型语言模型 (LLM) 的技术。然而，DPO 的一个弱点在于它缺乏表征人类偏好多样性的能力。受 Mallows 偏好排序理论的启发，我们在本文中开发了一种新方法，即 Mallows-DPO。这种方法的一个显着特点是分散指数，它反映了人类对提示的偏好分散性。我们表明现有的 DPO 模型可以简化为这种分散指数的特殊情况，从而与 Mallows-DPO 统一。更重要的是，我们（通过经验）展示了如何使用这种分散指数来提高 DPO 在广泛基准任务中的表现，从合成强盗选择到可控生成和对话，同时保持强大的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2405.14953</guid>
      <pubDate>Mon, 27 May 2024 06:20:47 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯误差如何限制概率鲁棒精度</title>
      <link>https://arxiv.org/abs/2405.14923</link>
      <description><![CDATA[arXiv:2405.14923v1 公告类型：新 
摘要：对抗性示例对许多基于神经网络构建的关键系统构成安全威胁。鉴于确定性鲁棒性通常会显着降低准确性，因此概率鲁棒性（即邻近区域具有相同标签的概率为 $\ge 1-\kappa$）被认为是一种在保持准确性的同时实现鲁棒性的有前途的方法。然而，现有的概率鲁棒性训练方法仍然存在不小的准确性损失。目前还不清楚在针对概率鲁棒性进行优化时，准确性是否存在上限，以及 $\kappa$ 与该上限之间是否存在某种关系。这项工作从贝叶斯误差的角度研究这些问题。我们发现，虽然贝叶斯不确定性确实影响概率鲁棒性，但其影响小于确定性鲁棒性。贝叶斯不确定性的降低使得概率鲁棒精度的上限高于确定性鲁棒精度的上限。此外，我们证明，通过最佳概率鲁棒性，每个概率鲁棒输入在较小的附近也具有确定性鲁棒性。我们还表明，附近的投票总是会提高概率鲁棒精度，并且概率鲁棒精度的上限随着 $\kappa$ 的增长而单调增加。我们的实证研究结果也与我们的结果一致。]]></description>
      <guid>https://arxiv.org/abs/2405.14923</guid>
      <pubDate>Mon, 27 May 2024 06:20:46 GMT</pubDate>
    </item>
    <item>
      <title>SliM-LLM：大型语言模型的显着性驱动混合精度量化</title>
      <link>https://arxiv.org/abs/2405.14917</link>
      <description><![CDATA[arXiv:2405.14917v1 公告类型：新 
摘要：大型语言模型（LLM）在自然语言理解方面取得了显着的性能，但需要大量的计算和内存资源。训练后量化 (PTQ) 是一种强大的压缩技术，在法学硕士中得到了广泛研究。然而，现有的 PTQ 方法在精度和效率方面仍然不理想，特别是在 4 位宽度以下的情况下。使用分组量化的标准 PTQ 方法在将 LLM 精确量化到如此低位时遇到困难，但按元素保持高精度权重的先进方法很难实现其理论硬件效率。本文提出了一种 LLM 的显着性驱动混合精度量化方案，即 SliM-LLM。该方案利用权重的显着性分布来确定最佳位宽和量化器，以实现精确的 LLM 量化，同时将位宽分区与组对齐以实现紧凑的内存使用和快速整数推理。具体来说，所提出的SliM-LLM主要依赖于两种新颖的技术：（1）显着性确定位分配利用显着性分布的聚类特性来分配每个组的位宽，提高量化LLM的准确性并保持推理效率; (2)显着性加权量化器校准通过考虑组内元素显着性、平衡显着信息的维护和误差最小化来优化量化器的参数。综合实验表明，SliM-LLM在超低位下显着提高了LLM的精度，例如2位LLaMA-7B在NVIDIA A800 GPU上实现了比原始模型节省5.5倍的内存，并且困惑度降低了48%最先进的无梯度 PTQ 方法。此外，SliM-LLM+是SliM-LLM的扩展与基于梯度的量化器的集成，进一步降低了35.1%的困惑度。]]></description>
      <guid>https://arxiv.org/abs/2405.14917</guid>
      <pubDate>Mon, 27 May 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>AnalogCoder：通过免训练代码生成进行模拟电路设计</title>
      <link>https://arxiv.org/abs/2405.14918</link>
      <description><![CDATA[arXiv:2405.14918v1 公告类型：新 
摘要：模拟电路设计是现代芯片技术中的一项重要任务，重点是元件类型、连接性和参数的选择，以确保正确的电路功能。尽管大型语言模型 (LLM) 在数字电路设计方面取得了进步，但模拟电路中数据的复杂性和稀缺性带来了重大挑战。为了缓解这些问题，我们引入了 AnalogCoder，这是第一个无需培训的 LLM 代理，用于通过 Python 代码生成来设计模拟电路。首先，AnalogCoder 将反馈增强流程与定制的特定领域提示相结合，实现模拟电路的自动化和自我校正设计，并具有很高的成功率。其次，它提出了一个电路工具库，将成功的设计归档为可重复使用的模块化子电路，从而简化了复合电路的创建。第三，针对旨在涵盖广泛模拟电路任务的基准进行的大量实验表明，AnalogCoder 优于其他基于 LLM 的方法。已成功设计20个电路，比标准GPT-4o多5个。我们相信AnalogCoder可以显着改善劳动密集型芯片设计流程，使非专家也能高效地设计模拟电路。 https://github.com/anonyanalog/AnalogCoder 提供了代码和基准测试。]]></description>
      <guid>https://arxiv.org/abs/2405.14918</guid>
      <pubDate>Mon, 27 May 2024 06:20:45 GMT</pubDate>
    </item>
    <item>
      <title>YUI：使用不变性简化供需曲线进行日前电价预测</title>
      <link>https://arxiv.org/abs/2405.14893</link>
      <description><![CDATA[arXiv:2405.14893v1 公告类型：新 
摘要：在日前电力市场中，所有市场参与者在决策过程中获得可靠且准确的价格预测至关重要。目前工业应用中使用的预测方法往往忽视价格形成的底层机制，而从供需角度进行的经济研究对数据采集要求严格，难以应用于实际市场。观察日前电力市场的特点，我们引入两个不变性假设来简化供需曲线的建模。结合时间不变性假设，我们可以利用近期多个时段的市场均衡点来预测供给曲线。通过引入价格不敏感假设，我们可以使用直线来近似需求曲线。这两条曲线的交点为我们提供了预测价格。所提出的模型，预测由不变性简化的供应曲线和需求曲线，称为 YUI，比最先进的方法更有效。我们在山西日前电力市场的实验结果表明，与现有方法相比，YUI在MAE中可以减少13.8％的预测误差，在sMAPE中可以减少28.7％的预测误差。代码可在 https://github.com/wangln19/YUI 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2405.14893</guid>
      <pubDate>Mon, 27 May 2024 06:20:44 GMT</pubDate>
    </item>
    <item>
      <title>数据混合效率提升：语言模型预训练的双变量缩放律</title>
      <link>https://arxiv.org/abs/2405.14908</link>
      <description><![CDATA[arXiv:2405.14908v1 公告类型：新 
摘要：大型语言模型表现出卓越的泛化能力，这主要归因于对不同来源数据的利用。然而，整合这些不同数据的传统实践严重依赖启发式方案，缺乏理论指导。本研究通过研究基于数据混合的低成本代理的策略来解决这些限制，旨在简化数据管理以提高培训效率。具体来说，我们提出了一种统一的缩放法则，称为 BiMix，它可以准确地模拟数据量和混合比例的二元缩放行为。我们进行系统实验，并为 BiMix 的预测能力和基本原理提供经验证据。值得注意的是，我们的研究结果表明，与资源密集型方法相比，熵驱动的免训练数据混合可以实现可比甚至更好的性能。我们希望我们的定量见解能够为具有成本效益的语言建模的进一步明智的研究和开发提供启示。]]></description>
      <guid>https://arxiv.org/abs/2405.14908</guid>
      <pubDate>Mon, 27 May 2024 06:20:44 GMT</pubDate>
    </item>
    <item>
      <title>基于广义同步的广义读出储层计算</title>
      <link>https://arxiv.org/abs/2405.14885</link>
      <description><![CDATA[arXiv:2405.14885v1 公告类型：新 
摘要：油藏计算是一种利用非线性动力学的机器学习框架，具有显着的计算能力。储层计算的定义特征之一是其低成本且简单的训练算法，即仅训练由储层变量的线性组合给出的读数。受最近基于动力系统理论的数学研究的启发，特别是广义同步，我们提出了一种具有广义读出的新型油藏计算框架，包括油藏变量的非线性组合。使用广义读数的第一个关键优势是其提高信息处理能力的数学基础。其次，它仍然是在线性学习框架内，保留了储层计算的原始优势。总之，广义读数自然地源自数学理论，并且允许在不牺牲简单性的情况下从储层动力学中提取有用的基函数。在数值研究中，我们发现引入广义读数可以显着提高洛伦兹混沌短期和长期预测的准确性和鲁棒性，特别关注如何利用低维储层动力学。简要讨论了一种利用广义读出物理实现储层计算的新方法及其优点。]]></description>
      <guid>https://arxiv.org/abs/2405.14885</guid>
      <pubDate>Mon, 27 May 2024 06:20:43 GMT</pubDate>
    </item>
    </channel>
</rss>