<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Fri, 15 Nov 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>一致性模型中的不一致性：更好的 ODE 求解并不意味着更好的样本</title>
      <link>https://arxiv.org/abs/2411.08954</link>
      <description><![CDATA[arXiv:2411.08954v1 公告类型：新
摘要：尽管扩散模型可以生成非常高质量的样本，但它们本质上受到其昂贵的迭代采样程序的瓶颈。一致性模型 (CM) 最近成为一种有前途的扩散模型蒸馏方法，通过仅几次迭代即可生成高保真样本来降低采样成本。一致性模型蒸馏旨在解决现有扩散模型定义的概率流常微分方程 (ODE)。CM 不是直接训练来最小化针对 ODE 求解器的误差，而是使用更易于计算的目标。为了研究 CM 如何有效地解决概率流 ODE，以及任何诱导误差对生成样本质量的影响，我们引入了直接 CM，\textit{直接} 最小化此误差。有趣的是，我们发现与 CM 相比，直接 CM 降低了 ODE 求解误差，但样本质量也明显下降，这让人不禁要问，为什么 CM 会如此有效。完整代码可从以下网址获取：https://github.com/layer6ai-labs/direct-cms。]]></description>
      <guid>https://arxiv.org/abs/2411.08954</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>稀疏升级：推理效率低下的微调</title>
      <link>https://arxiv.org/abs/2411.08968</link>
      <description><![CDATA[arXiv:2411.08968v1 公告类型：新
摘要：小型、训练有素、开源的大型语言模型因其推理效率而被广泛使用，但进一步提高其质量仍然是一个挑战。稀疏升级是一种很有前途的方法，它将预训练的密集模型转换为混合专家 (MoE) 架构，从而增加了模型的参数数量和质量。在这项工作中，我们比较了不同模型大小、计算预算和预训练持续时间下稀疏升级与持续预训练 (CPT) 的有效性。我们的实验表明，稀疏升级可以实现更好的质量，在某些情况下相对于 CPT 的改进超过 20%。然而，这带来了巨大的推理成本，导致大型模型在高需求推理设置中速度降低 40%。我们的研究结果强调了模型质量和推理效率之间的权衡，为寻求平衡模型质量和部署约束的从业者提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2411.08968</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Lynx：通过动态批量感知专家选择实现高效的 MoE 推理</title>
      <link>https://arxiv.org/abs/2411.08982</link>
      <description><![CDATA[arXiv:2411.08982v1 公告类型：新
摘要：混合专家 (MoE) 架构最近在实现大型语言模型的有效扩展方面越来越受欢迎。然而，我们发现了一个根本的矛盾：虽然 MoE 是为选择性专家激活而设计的，但生产服务需要请求批处理，这会强制激活所有专家并抵消 MoE 在解码阶段的效率优势。我们提出了 Lynx，这是一个通过动态、批处理感知专家选择实现高效 MoE 推理的系统。我们的主要见解是专家的重要性在标记和推理阶段存在很大差异，为运行时优化创造了机会。Lynx 通过一个轻量级框架利用这一见解，该框架动态减少活跃专家，同时保持模型准确性。我们的评估表明，Lynx 在复杂的代码生成和数学推理任务中将推理延迟减少了 1.55 倍，同时保持与基线模型相比可忽略不计的准确性损失。]]></description>
      <guid>https://arxiv.org/abs/2411.08982</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士 (LLM) 的拒绝是一个仿射函数</title>
      <link>https://arxiv.org/abs/2411.09003</link>
      <description><![CDATA[arXiv:2411.09003v1 公告类型：新
摘要：我们提出仿射概念编辑 (ACE) 作为一种通过直接干预激活来控制语言模型行为的方法。我们从模型激活向量的仿射分解开始，并表明用于控制模型行为的先前方法对应于此分解的子集。然后，我们提供 ACE 的推导并使用 Llama 3 8B 和 Hermes Eagle RWKV v5 对其进行拒绝测试。ACE 最终结合了仿射子空间投影和激活添加，以可靠地控制模型在提示类型中的拒绝响应。我们使用基于 LLM 的评分对一系列有害和无害的提示进行结果评估。我们的实验表明，ACE 始终能够更精确地控制模型行为，并推广到仅通过仿射子空间投影进行定向消融会产生不连贯输出的模型。重现我们结果的代码可以在https://github.com/EleutherAI/steering-llama3找到。]]></description>
      <guid>https://arxiv.org/abs/2411.09003</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>减少大词汇量语言模型中的损失</title>
      <link>https://arxiv.org/abs/2411.09009</link>
      <description><![CDATA[arXiv:2411.09009v1 公告类型：新
摘要：随着语言模型变得越来越大，其词汇量也越来越大。这已将 LLM 在训练期间的内存占用不成比例地转移到一个单层：损失计算中的交叉熵。交叉熵会建立一个包含每对输入标记和词汇项的条目的逻辑矩阵，对于小型模型，其消耗的内存比其余 LLM 的总和要多一个数量级。我们提出了 Cut Cross-Entropy (CCE)，这是一种计算交叉熵损失的方法，无需将所有标记的逻辑实现到全局内存中。相反，CCE 仅计算正确标记的逻辑并动态评估所有逻辑的对数和指数。我们实现了一个自定义内核，它在闪存中对词汇表执行矩阵乘法和对数和指数缩减，使交叉熵计算的全局内存消耗可以忽略不计。这产生了显著的效果。以 Gemma 2 (2B) 模型为例，CCE 将损失计算的内存占用从 24 GB 减少到 1 MB，并将分类器头的总训练时间内存消耗从 28 GB 减少到 1 GB。为了提高 CCE 的吞吐量，我们利用 softmax 固有的稀疏性，并建议跳过对梯度贡献可忽略不计（即低于数值精度）的梯度计算元素。实验表明，在不牺牲训练速度或收敛性的情况下，内存消耗大幅减少。]]></description>
      <guid>https://arxiv.org/abs/2411.09009</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformer 的时间序列生物标志物发现，用于 COPD 诊断</title>
      <link>https://arxiv.org/abs/2411.09027</link>
      <description><![CDATA[arXiv:2411.09027v1 公告类型：新
摘要：慢性阻塞性肺病 (COPD) 是一种不可逆的进行性疾病，具有高度遗传性。临床上，COPD 是使用肺量计测试得出的总结指标来定义的，但这些指标并不总是足够的。在这里，我们表明，与仅使用总结指标相比，使用高维原始肺量图可以提供更丰富的信号。我们设计了一种基于 Transformer 的深度学习技术来处理原始肺量图值以及人口统计信息，并预测与 COPD 相关的临床相关终点。我们的方法能够比以前的工作表现更好，同时计算效率更高。使用模型学习到的权重，我们通过识别对模型预测很重要的肺量图部分，使框架更具可解释性。与委员会认证的肺病专家合作，我们还提供了对肺量图不同方面的临床见解，并表明从模型中获得的解释与基础医学知识相一致。]]></description>
      <guid>https://arxiv.org/abs/2411.09027</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>大规模云系统中的异常检测：行业案例和数据集</title>
      <link>https://arxiv.org/abs/2411.09047</link>
      <description><![CDATA[arXiv:2411.09047v1 公告类型：新
摘要：随着大规模云系统 (LCS) 变得越来越复杂，有效的异常检测对于确保系统可靠性和性能至关重要。然而，用于对异常检测方法进行基准测试的大规模真实世界数据集却很少。
为了解决这一差距，我们引入了来自 IBM Cloud 的新高维数据集，该数据集是从 IBM Cloud Console 收集的，历时 4.5 个月。该数据集包含 39,365 行和 117,448 列遥测数据。此外，我们展示了机器学习模型在异常检测中的应用，并讨论了在此过程中面临的主要挑战。
本研究及其随附的数据集为云系统监控的研究人员和从业者提供了资源。它有助于更​​有效地在真实世界数据中测试异常检测方法，有助于推动开发强大的解决方案以维护大规模云基础设施的健康和性能。]]></description>
      <guid>https://arxiv.org/abs/2411.09047</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SAFELOC：克服室内定位异构联合机器学习中的数据中毒攻击</title>
      <link>https://arxiv.org/abs/2411.09055</link>
      <description><![CDATA[arXiv:2411.09055v1 公告类型：新
摘要：基于机器学习 (ML) 的室内定位解决方案对于许多新兴应用至关重要，但它们的有效性通常会受到移动设备之间硬件/软件变化（即设备异质性）和 ML 数据中毒攻击威胁的影响。旨在应对这些挑战的传统方法对这些现象造成的不确定性表现出有限的弹性。作为回应，在本文中，我们介绍了 SAFELOC，这是一个新颖的框架，它不仅可以在这些具有挑战性的条件下最大限度地减少定位误差，而且还确保模型紧凑性以实现高效的移动设备部署。我们的框架针对分布式和协作式学习环境，该环境使用联邦学习 (FL) 来保护用户数据隐私，并假设用户携带异构移动设备（就像在大多数现实世界场景中一样）。在这种异构 FL 环境中，SAFELOC 引入了一种新颖的融合神经网络架构，可以执行数据中毒检测和定位，并且模型占用空间小。此外，还设计了一种基于动态显着图的聚合策略，可根据检测到的数据中毒场景的严重程度进行调整。实验评估表明，与最先进的室内定位框架相比，SAFELOC 在各种建筑平面图、移动设备和 ML 数据中毒攻击场景中，平均定位误差最高可降低 5.9 倍，最坏情况定位误差最高可降低 7.8 倍，模型推理延迟最高可降低 2.1 倍。]]></description>
      <guid>https://arxiv.org/abs/2411.09055</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>确保机器学习公平性的优化策略：有无人口统计数据</title>
      <link>https://arxiv.org/abs/2411.09056</link>
      <description><![CDATA[arXiv:2411.09056v1 公告类型：新
摘要：确保公平性已成为人工智能及其相关算法的主要关注点之一。随着时间的推移，机器学习公平性领域已经发展到解决这些问题的程度。本文对该领域进行了广泛的概述，并介绍了两个正式框架来解决机器学习公平性中的未决问题。
在一个框架中，采用运算符值优化和最小-最大目标来解决时间序列问题中的不公平性。这种方法在臭名昭著的 COMPAS 基准数据集上展示了最先进的性能，证明了其在现实场景中的有效性。
在第二个框架中，解决了常用数据集中缺乏敏感属性（例如性别和种族）的挑战。这个问题尤其紧迫，因为该领域的现有算法主要依赖此类属性的可用性或估计来评估和减轻不公平性。这里介绍了一个组盲偏差修复框架，旨在在不依赖敏感属性的情况下减轻偏差。通过对成人人口普查收入数据集进行的分析，展示了该方法的有效性。
此外，还提供了两个框架的详细算法分析，并提供了收敛保证，确保了所提出方法的稳健性和可靠性。]]></description>
      <guid>https://arxiv.org/abs/2411.09056</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用高效自适应知识图谱学习进行基于 GNN 的连续边缘异常检测</title>
      <link>https://arxiv.org/abs/2411.09072</link>
      <description><![CDATA[arXiv:2411.09072v1 公告类型：新
摘要：各行各业对强大安全解决方案的需求日益增长，使得视频异常检测 (VAD) 成为智能监控、证据调查和暴力检测等应用中的关键任务。传统的 VAD 方法通常依赖于对大型预训练模型进行微调，这在计算上可能很昂贵，并且对于实时或资源受限的环境来说不切实际。为了解决这个问题，MissionGNN 引入了一种更有效的方法，即使用从 GPT-4 等大型语言模型 (LLM) 派生的固定知识图 (KG) 来训练图神经网络 (GNN)。虽然这种方法在计算能力和内存方面表现出了显著的效率，但它在动态环境中面临限制，因为行为趋势的演变和数据模式的转变需要频繁更新 KG。这些更新通常需要基于云的计算，这对边缘计算应用提出了挑战。在本文中，我们提出了一个新颖的框架，可直接在边缘设备上促进 KG 的持续自适应，从而克服对云依赖的限制。我们的方法通过三阶段过程动态修改知识图谱：修剪、交替和创建节点，从而能够实时适应不断变化的数据趋势。这种持续学习方法增强了异常检测模型的稳健性，使其更适合在动态和资源受限的环境中部署。]]></description>
      <guid>https://arxiv.org/abs/2411.09072</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>降低推理成本——基于稀疏注意力机制的思路链优化之路</title>
      <link>https://arxiv.org/abs/2411.09111</link>
      <description><![CDATA[arXiv:2411.09111v1 Announce Type: new 
摘要：为了解决大型语言模型推理成本激增带来的思路链问题，本研究提出使用一种仅关注少数相关token的稀疏注意力机制。研究者构建了一种新的注意力机制，并使用自定义GPT训练的GiantRabbit作为实验工具。实验测试并比较了该模型与o1 Preview在解决MIT OpenCourseWare的线性代数测试题中的推理时间、正确性得分和思路链长度。结果表明，GiantRabbit的推理时间和思路链长度均明显低于o1 Preview，证实了稀疏注意力机制在减少思路链推理方面的可行性。详细的架构细节和实验过程已经上传至Github，链接为：https://github.com/brucewang123456789/GeniusTrail.git。]]></description>
      <guid>https://arxiv.org/abs/2411.09111</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过基于数据的初始化高效学习和采样多峰分布</title>
      <link>https://arxiv.org/abs/2411.09117</link>
      <description><![CDATA[arXiv:2411.09117v1 公告类型：新
摘要：我们考虑使用马尔可夫链对多峰分布进行采样的问题，给定少量来自平稳测量的样本。虽然混合可能非常慢，但我们表明，如果马尔可夫链具有 $k$ 阶谱间隙，则从来自平稳分布的一组 $\tilde O(k/\varepsilon^2)$ 样本进行初始化将以高概率有效地生成一个样本，其条件定律在 TV 距离上与平稳测量接近 $\varepsilon$。特别是，这适用于满足庞加莱不等式的 $k$ 分布混合，当它们满足对数索伯列夫不等式时，收敛速度更快。我们的边界对于马尔可夫链的扰动是稳定的，特别是对于具有分数估计误差的 $\mathbb R^d$ 上的朗之万扩散，以及与伪似然估计的近似误差相结合的 Glauber 动力学。这证明了基于数据的初始化对于分数匹配方法的成功，尽管数据分布的混合速度很慢，并且改进和推广了 Koehler 和 Vuong (2023) 的结果，使其具有对 $k$ 的线性依赖性，而不是指数依赖性，并适用于任意半群。作为我们结果的结果，我们首次表明可以从样本中有效地学习一类自然的低复杂度 Ising 测度。]]></description>
      <guid>https://arxiv.org/abs/2411.09117</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>复杂系统的神经图模拟器</title>
      <link>https://arxiv.org/abs/2411.09120</link>
      <description><![CDATA[arXiv:2411.09120v1 公告类型：新
摘要：数值模拟是研究复杂系统动力学的主要工具，但由于计算限制，大规模模拟通常难以实现。在这里，我们介绍了用于在图上模拟时不变自治系统的神经图模拟器 (NGS)。利用图神经网络，NGS 通过其非均匀时间步长和自回归方法提供了一个统一的框架，可以模拟具有不同拓扑和大小的各种动态系统，而不受评估时间的限制。NGS 不需要事先了解控制方程，并且可以通过强大的训练方案有效处理嘈杂或缺失的数据，因此与数值求解器相比具有显着优势。它比传统方法具有更高的计算效率，在刚性问题中将性能提高了 10^5 倍以上。此外，它还应用于实际交通数据，以最先进的精度预测交通流量。 NGS 的多功能性超出了所介绍的案例，提供了许多潜在的增强途径。]]></description>
      <guid>https://arxiv.org/abs/2411.09120</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>深度神经网络的复杂性感知训练以实现最佳结构发现</title>
      <link>https://arxiv.org/abs/2411.09127</link>
      <description><![CDATA[arXiv:2411.09127v1 公告类型：新
摘要：我们提出了一种用于深度神经网络的组合单元/过滤器和层修剪的新算法，该算法在训练期间起作用，并且不需要预先训练的网络即可应用。我们的算法仅使用三个用户定义的参数，在平衡层与单元/过滤器修剪以及计算与参数复杂性的同时，最佳地权衡了学习准确性和修剪水平，这些参数易于解释和调整。最佳网络结构是针对网络权重和 0/1 随机变量变分伯努利分布的参数的随机优化问题的解，这些参数缩放网络的单元和层。当变分参数收敛到 0 时，会发生修剪，导致相应的结构永久不活动，从而节省训练和预测期间的计算。我们方法的一个关键贡献是定义一个成本函数，该函数以计算/参数复杂性感知的方式结合预测准确性和网络修剪的目标以及许多正则化参数的自动选择。我们表明，算法收敛到的优化问题的解是确定性网络。我们分析了随机优化算法所依赖的 ODE 系统，并为网络参数的动态建立了零附近的吸引域。这些结果为在训练期间安全地修剪单元/过滤器和/或层提供了理论支持，并导致了实际的修剪条件。我们使用 ResNet 架构在 CIFAR-10/100 和 ImageNet 数据集上评估了我们的方法，并证明我们的方法改进了仅层或仅单元的修剪，并且在修剪率和测试准确度方面与需要预训练网络的组合单元/过滤器和层修剪算法相媲美。]]></description>
      <guid>https://arxiv.org/abs/2411.09127</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>差分隐私的拉普拉斯变换解释</title>
      <link>https://arxiv.org/abs/2411.09142</link>
      <description><![CDATA[arXiv:2411.09142v1 公告类型：新
摘要：我们根据隐私损失分布的拉普拉斯变换引入了一组有用的差分隐私 (DP) 概念表达。其裸形式表达式出现在分析 DP 的几项相关工作中，要么是积分，要么是期望。我们表明，将表达式识别为拉普拉斯变换可以通过利用时间和频域之间的对偶性来开启一种推理 DP 属性的新方法。利用我们的解释，我们将 $(q, \rho(q))$-R\&#39;enyi DP 曲线和 $(\epsilon, \delta(\epsilon))$-DP 曲线连接起来，作为彼此的拉普拉斯和逆拉普拉斯变换。这种联系表明，对于复数阶 $q = \gamma + i \omega$，R\&#39;enyi 散度是明确定义的。使用基于拉普拉斯变换的分析，我们还证明了 $(\epsilon, \delta)$-DP 的自适应组合定理，该定理保证了对于 $\epsilon$ 的所有值都是完全紧密的（即，即使在常数中也匹配）。此外，我们解决了有关 $f$-DP 在子采样上的对称性问题，该问题阻碍了所有函数 DP 概念之间的等价性。]]></description>
      <guid>https://arxiv.org/abs/2411.09142</guid>
      <pubDate>Fri, 15 Nov 2024 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>