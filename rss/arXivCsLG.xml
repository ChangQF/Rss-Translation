<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.LG 在 arXiv.org 上更新</title>
    <link>http://rss.arxiv.org/rss/cs.LG</link>
    <description>cs.LG 更新 arXiv.org 电子印刷档案。</description>
    <lastBuildDate>Wed, 16 Oct 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Duo-LLM：研究大型语言模型中的自适应计算的框架</title>
      <link>https://arxiv.org/abs/2410.10846</link>
      <description><![CDATA[arXiv:2410.10846v1 公告类型：新
摘要：大型语言模型 (LLM) 通常使用固定的计算预算逐个生成输出，导致资源利用率低下。为了解决这一缺点，混合专家 (MoE) 模型、推测解码和早期退出策略方面的最新进展利用了计算需求可能根据输入的复杂性和性质而有很大差异的洞察力。然而，确定动态执行的最佳路由模式仍然是一个悬而未决的挑战，限制了这些自适应方法的全部潜力。为了满足这一需求，我们更系统地研究了 LLM 中的自适应计算。我们提出了一个新颖的框架，该框架将较小的辅助模块集成到 LLM 的每个前馈网络层中。这种设计可以根据任务复杂性动态路由令牌：令牌可以由每层的小模块或大模块处理，甚至可以完全绕过某些层。这使我们能够引入一个令牌难度的新概念，该概念由其从额外计算资源中受益的潜力来定义。重要的是，通过使用 oracle 来识别自适应计算的最佳模式，我们可以深入了解 LLM 的内部工作原理以及简化的异构 MoE 设置中的路由过程。我们表明，经过训练的路由器的运行方式与 oracle 不同，并且通常会产生次优解决方案。值得注意的是，仅在一层中激活大型模块的性能优于在所有层中使用大型模块的模型，这突显了 MoE 模型中路由的实际实现与自适应计算的理论最优值之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2410.10846</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>连续近似法用于改进 LLM 的量化感知训练</title>
      <link>https://arxiv.org/abs/2410.10849</link>
      <description><![CDATA[arXiv:2410.10849v1 公告类型：新
摘要：模型压缩方法用于减少大型语言模型 (LLM) 的计算和能量需求。量化感知训练 (QAT) 是一种有效的模型压缩方法，旨在减少量化后的性能下降。为了进一步减少这种性能下降，我们在舍入函数（传统上由直通估计器 (STE) 近似）和夹紧函数上对 QAT 过程引入了两个连续近似。通过应用这两种方法，量化模型在 WikiText-v2 数据集上的困惑度 (PPL) 达到 9.0815，优于基线的 9.9621。此外，我们在 BoolQ 上实现了 2.76% 的改进，在 MMLU 上实现了 5.47% 的改进，证明了我们的方法可以更准确地学习步长和权重。我们的方法在相同的精度、模型大小和训练设置下实现了更好的性能，有助于开发符合全球可持续发展目标的更节能的 LLM 技术。]]></description>
      <guid>https://arxiv.org/abs/2410.10849</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLaCA：多模式大型语言持续助手</title>
      <link>https://arxiv.org/abs/2410.10868</link>
      <description><![CDATA[arXiv:2410.10868v1 公告类型：新
摘要：指令调整通过设计文本指令来指导多模态大型语言模型 (MLLM) 对齐不同的模态，这似乎是增强基础模型能力和可控性的重要技术。在此框架中，采用多模态持续指令调整 (MCIT) 来持续指示 MLLM 在连续数据集中遵循人类意图。我们观察到现有的梯度更新会严重破坏先前数据集的调整性能和持续指令调整期间的零样本能力。指数移动平均 (EMA) 更新策略具有跟踪先前参数的能力，这有助于减少遗忘。然而，其稳定的平衡权重无法处理不断变化的数据集，导致 MLLM 的可塑性和稳定性失衡。在本文中，我们提出了一种称为多模态大型语言持续助手 (LLaCA) 的方法来应对这一挑战。从权衡前提和EMA更新出发，提出了可塑性和稳定性理想条件。基于损失函数的泰勒展开，我们发现最佳平衡权重基本根据梯度信息和先前参数确定。我们自动确定平衡权重并显著提高性能。通过在持续视觉问答基准上对LLaVA-1.5进行全面实验，与基线相比，我们的方法不仅大大提高了抗遗忘能力（将遗忘从22.67降低到2.68），而且显著提升了持续调优性能（平均准确率从41.31提高到61.89）。我们的代码即将发布。]]></description>
      <guid>https://arxiv.org/abs/2410.10868</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于词频的图像文本对剪枝增强视觉语言模型预训练</title>
      <link>https://arxiv.org/abs/2410.10879</link>
      <description><![CDATA[arXiv:2410.10879v1 公告类型：新
摘要：我们提出了基于词频的图像文本对修剪 (WFPP)，这是一种新颖的数据修剪方法，可提高 VLM 的效率。与 MetaCLIP 不同，我们的方法不需要元数据进行修剪，而是根据文本的内容选择要修剪的文本图像对。具体来说，WFPP 会在整个训练数据集中修剪包含高频词的文本图像对。WFPP 的作用是降低频繁词的主导性。结果是数据集中的词频分布更加平衡，这可以改善词嵌入模型的训练。在对修剪后的子集进行预训练后，我们在整个数据集上对模型进行了一次额外的微调，以获得更好的性能。我们的实验表明，在训练 CLIP 模型时应用 WFPP 可以提高各种下游任务的性能。 WFPP 还具有使用更少样本来加快预训练速度的优势。此外，我们分析了修剪前后的训练数据，以直观地了解 WFPP 如何改变词频的平衡。我们希望我们的工作能够鼓励研究人员在预训练 VLM 时考虑训练数据中的单词分布，而不仅限于 CLIP。]]></description>
      <guid>https://arxiv.org/abs/2410.10879</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>ActNAS：使用激活 NAS 生成高效的 YOLO 模型</title>
      <link>https://arxiv.org/abs/2410.10887</link>
      <description><![CDATA[arXiv:2410.10887v1 公告类型：新
摘要：激活函数将非线性引入神经网络，使它们能够学习复杂的模式。不同的激活函数在速度和准确性上有所不同，从更快但准确性较低的选项（如 ReLU）到更慢但更准确的函数（如 SiLU 或 SELU）。通常，整个模型架构都使用相同的激活函数。在本文中，我们对在基于 YOLO 的模型中使用混合激活函数的效果进行了全面研究，评估了它们对 CPU、NPU 和 GPU 边缘设备的延迟、内存使用情况和准确性的影响。我们还提出了一种利用神经架构搜索 (NAS) 来设计具有优化混合激活函数的 YOLO 模型的新方法。通过此方法生成的最佳模型与基线模型 (SiLU) 相比，平均精度 (mAP) 略有提高，同时在参考 NPU 设备上速度提高了 22.28%，内存消耗减少了 64.15%。]]></description>
      <guid>https://arxiv.org/abs/2410.10887</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AT-MoE：通过 LoRA 方法实现专家的自适应任务规划混合</title>
      <link>https://arxiv.org/abs/2410.10896</link>
      <description><![CDATA[arXiv:2410.10896v1 公告类型：新 
摘要：大型语言模型（LLM）的出现开启了人工智能的新时代，它有可能通过自动化和深刻的分析改变各个领域。混合专家（MoE）架构已被提出作为提高复杂任务中模型性能的解决方案。然而，现有的 MoE 模型在特定任务的学习和可解释性方面存在困难，特别是在医学等精度至关重要的领域。本文介绍了自适应任务规划混合专家（AT-MoE），这是一种旨在解决这些限制的创新架构。我们首先通过 LoRA 方法训练特定任务的专家，以增强专业领域的问题解决能力和可解释性。随后，我们引入了一个分层自适应分组路由模块，该模块根据复杂任务指令优化模块融合，确保最佳任务解析。分组路由模块首先从专家组的维度进行整体权重分配，然后在组内进行局部权重规范化调整。这种设计保持了多维平衡、可控性和可解释性，同时促进了响应复杂指令的特定任务融合。]]></description>
      <guid>https://arxiv.org/abs/2410.10896</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>3DS：分解难度数据选择在 LLM 医学领域适应性方面的案例研究</title>
      <link>https://arxiv.org/abs/2410.10901</link>
      <description><![CDATA[arXiv:2410.10901v1 公告类型：新 
摘要：大型语言模型 (LLM) 在一般任务中表现出色，但由于领域特定知识有限，在医疗保健等专业领域表现不佳。用于领域适应的监督微调 (SFT) 数据构建通常依赖于启发式方法，例如 GPT-4 注释或手动数据选择，以数据为中心，重点关注假定的多样化、高质量数据集。然而，这些方法忽视了模型固有的知识分布，引入了噪音、冗余和不相关的数据，导致所选数据与模型的学习任务不匹配，从而导致性能不佳。为了解决这个问题，我们提出了一个两阶段以模型为中心的数据选择框架，即分解难度数据选择 (3DS)，它将数据与模型的知识分布对齐，以实现优化的适应性。在第 1 阶段，我们通过显式对齐应用提示驱动的数据选择，其中模型根据其内部知识过滤不相关或冗余数据。在第 2 阶段，我们执行分解难度数据选择，其中数据选择由我们定义的难度分解指导，使用三个指标：指令理解、响应置信度和响应正确性。此外，基于注意力的重要性加权机制捕获标记重要性，以实现更准确的难度校准。这种两阶段方法确保所选数据不仅与模型的知识和偏好一致，而且对模型学习具有适当的挑战性，从而实现更有效和有针对性的领域适应。在医学领域的案例研究中，我们对现实世界医疗保健数据集的大量实验表明，3DS 在准确率上优于现有方法，超过 5.29%。我们的数据集和代码将在 https://anonymous.4open.science/r/3DS-E67F 上开源。]]></description>
      <guid>https://arxiv.org/abs/2410.10901</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过简单的架构变化和规模提高 ProcGen 基准的泛化能力</title>
      <link>https://arxiv.org/abs/2410.10905</link>
      <description><![CDATA[arXiv:2410.10905v1 公告类型：新
摘要：我们证明强化学习 (RL) 的最新进展与简单的架构变化相结合，可显着提高 ProcGen 基准的泛化能力。这些变化包括帧堆叠、用 3D 卷积层替换 2D 卷积层以及增加每层的卷积核数量。在所有环境中使用一组超参数的实验结果显示，与基线相比，最优性差距减少了 37.9%（从 0.58 到 0.36）。这种性能达到或超过了当前最先进的方法。所提出的更改在很大程度上是正交的，因此与现有的用于改进 RL 泛化的方法相辅相成，我们的结果表明，在这个方向上的进一步探索可以大大改善深度强化学习中的泛化挑战。]]></description>
      <guid>https://arxiv.org/abs/2410.10905</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一种预测分化型甲状腺癌复发的可解释 AI 模型</title>
      <link>https://arxiv.org/abs/2410.10907</link>
      <description><![CDATA[arXiv:2410.10907v1 公告类型：新
摘要：甲状腺癌是一种重要但通常可控的癌症，其病例数有所增加，这主要是由于诊断方法的进步。分化型甲状腺癌 (DTC) 包括乳头状和滤泡状两种类型，在学术界通常与阳性预后有关。尽管如此，仍有一些人可能会复发。本研究采用机器学习，特别是深度学习模型来预测 DTC 的复发，目的是通过个性化治疗方法改善患者护理。通过分析包含患者临床病理特征的数据集，该模型在训练期间实现了 98% 的显着准确率，在测试期间实现了 96% 的显着准确率。为了提高模型的可解释性，我们使用了 LIME 和 Morris 敏感性分析等技术。这些方法为我们提供了有关模型如何做出决策的宝贵见解。结果表明，将深度学习模型与可解释性技术相结合对于快速识别患者甲状腺癌的复发非常有用。这有助于做出明智的治疗选择并针对个别患者定制治疗方法。]]></description>
      <guid>https://arxiv.org/abs/2410.10907</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Julia 在科学机器学习中的应用现状</title>
      <link>https://arxiv.org/abs/2410.10908</link>
      <description><![CDATA[arXiv:2410.10908v1 公告类型：新
摘要：Julia 被誉为科学机器学习和数值计算领域 Python 的潜在继任者，具有人体工程学和性能方面的改进。自 2012 年 Julia 成立并于 2017 年宣布语言目标以来，其生态系统和语言级功能得到了极大的发展。在本文中，我们以现代的眼光看待 Julia 的功能和生态系统，评估该语言的现状，并讨论其作为事实上的科学机器学习语言替代 Python 的可行性和缺陷。我们呼吁社区解决阻碍 Julia 进一步采用的语言级问题。]]></description>
      <guid>https://arxiv.org/abs/2410.10908</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AlphaPruning：利用重尾自正则化理论改进大型语言模型的分层剪枝</title>
      <link>https://arxiv.org/abs/2410.10912</link>
      <description><![CDATA[arXiv:2410.10912v1 公告类型：新
摘要：最近对大型语言模型 (LLM) 进行修剪的研究表明，人们可以在不影响性能的情况下消除大量参数，这使得修剪成为减少 LLM 模型大小的一种有前途的策略。现有的 LLM 修剪策略通常在各层之间分配统一的修剪率，从而限制了整体修剪能力；而最近对 LLM 分层修剪的研究通常基于启发式方法，这很容易导致性能不佳。在本文中，我们利用重尾自正则化 (HT-SR) 理论，特别是权重矩阵的经验谱密度 (ESD) 的形状，为 LLM 设计改进的分层修剪率。我们的分析揭示了 LLM 不同层的训练程度以及可修剪程度存在很大差异。基于此，我们提出了 AlphaPruning，它使用形状度量以更符合理论原则的方式分配逐层稀疏度比率。AlphaPruning 可以与多种现有的 LLM 修剪方法结合使用。我们的实证结果表明，AlphaPruning 将 LLaMA-7B 的稀疏度修剪至 80%，同时保持合理的困惑度，这在 LLM 文献中尚属首次。我们已在 https://github.com/haiquanlu/AlphaPruning 上开源了我们的代码。]]></description>
      <guid>https://arxiv.org/abs/2410.10912</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过通道样本排列实现更好的多头注意力</title>
      <link>https://arxiv.org/abs/2410.10914</link>
      <description><![CDATA[arXiv:2410.10914v1 公告类型：新 
摘要：Transformer 在许多基础深度学习模型中扮演着核心角色，例如计算机视觉中的 ViT 以及自然语言处理中的 BERT 和 GPT，其有效性主要归功于其多头注意力 (MHA) 机制。在本研究中，我们提出了一种简单而新颖的通道样本置换 (CSP) 算子，实现了具有更少参数和更低复杂度的新型结构化 MHA。给定一个输入矩阵，CSP 以不同的步长循环移动不同通道的样本，然后对每个通道的分组样本进行排序。该算子相当于将跨通道注意力图隐式实现为置换矩阵，从而实现了线性复杂度并抑制了表示数据时秩崩溃的风险。我们用 CSP 替换了一些代表性模型的 MHA，并在几个判别任务中测试了基于 CSP 的模型，包括图像分类和长序列分析。实验表明，基于 CSP 的模型以更少的参数和更低的计算成本实现了与经典 Transformer 及其最新变体相当或更好的性能。代码可在 https://github.com/DaShenZi721/CSP 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.10914</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于时空图学习的图掩蔽自动编码器</title>
      <link>https://arxiv.org/abs/2410.10915</link>
      <description><![CDATA[arXiv:2410.10915v1 公告类型：新 
摘要：有效的时空预测框架在城市传感应用中发挥着至关重要的作用，包括交通分析、人类流动行为建模和全市犯罪预测。然而，时空数据中数据噪声和标签稀疏性的存在对现有神经网络模型学习有效和鲁棒的区域表示提出了重大挑战。为了应对这些挑战，我们提出了一种新颖的时空图掩蔽自动编码器范式，探索生成自监督学习以实现有效的时空数据增强。我们提出的框架引入了一种时空异构图神经编码器，它可以从异构数据源捕获区域依赖关系，从而实现对各种空间依赖关系的建模。在我们的时空自监督学习范式中，我们在节点表示和结构上加入了一种掩蔽自动编码机制。该机制可自动提取随时间推移跨区域的异构时空依赖关系，从而增强动态区域空间相关性的学习过程。为了验证 STGMAE 框架的有效性，我们对各种时空挖掘任务进行了广泛的实验。我们将我们的方法与最先进的基线进行了比较。这些评估的结果证明了我们提出的框架在性能方面的优越性，以及它能够解决实际城市传感场景中空间和时间数据噪声和稀疏性的挑战。]]></description>
      <guid>https://arxiv.org/abs/2410.10915</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>剖析嵌入方法：从数据中学习高阶结构</title>
      <link>https://arxiv.org/abs/2410.10917</link>
      <description><![CDATA[arXiv:2410.10917v1 公告类型：新
摘要：人工智能的活跃研究领域是流形学习理论和寻找低维流形表示，以便从数据中学习几何图形，从而提供更高质量的精选数据集。然而，这些方法存在各种问题，这些问题与寻找数据的低维表示有关，即所谓的维数灾难。用于数据学习的几何深度学习方法通​​常包括对特征空间几何形状的一组假设。其中一些假设包括特征空间上的预选指标、底层图形结构的使用，该结构对数据点的接近度进行编码。然而，后面使用图形作为底层离散结构的假设只对数据点之间的二元成对关系进行编码，限制了我们捕捉更复杂的高阶关系，而这些关系通常存在于各种系统中。这些假设加上数据的离散性和有限性，可能会导致一些概括，这可能会导致对数据和模型输出的错误解释。因此，总的来说，这可能会导致嵌入模型本身的错误输出，而这些模型是相当稳定的，并且是在大量数据上训练的，例如 BERT、Yi 和其他类似模型。我们研究的目标有两个方面，首先，开发替代框架来表征嵌入方法，使用对嵌入数据进行编码的高阶结构的组合方法来剖析它们可能存在的不一致性。第二个目标是探索嵌入的底层结构是图的假设，用超图代替它，并使用超图理论来分析这种结构。我们还在 arXiv 数据的用例上展示了嵌入表征。]]></description>
      <guid>https://arxiv.org/abs/2410.10917</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>垂直联邦学习中的少量标签反学习</title>
      <link>https://arxiv.org/abs/2410.10922</link>
      <description><![CDATA[arXiv:2410.10922v1 公告类型：新
摘要：本文解决了垂直联邦学习 (VFL) 中取消学习的关键挑战，与水平联邦学习相比，该领域受到的关注有限。我们介绍了第一种专门设计用于解决 VFL 中标签取消学习的方法，重点关注主动方旨在减轻标签泄漏风险的场景。我们的方法利用有限数量的标记数据，利用流形混合来增强不足数据的前向嵌入，然后在增强的嵌入上进行梯度上升以从模型中删除标签信息。这种增强和梯度上升的组合可以在保持效率的同时实现较高的取消学习效果，在几秒钟内完成取消学习过程。在包括 MNIST、CIFAR10、CIFAR100 和 ModelNet 在内的各种数据集上进行的大量实验验证了我们方法的有效性和可扩展性。这项工作代表了联邦学习的重大进步，解决了 VFL 中取消学习的独特挑战，同时保护了隐私和计算效率。]]></description>
      <guid>https://arxiv.org/abs/2410.10922</guid>
      <pubDate>Wed, 16 Oct 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>