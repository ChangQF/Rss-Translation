<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sun, 02 Jun 2024 18:18:53 GMT</lastBuildDate>
    <item>
      <title>确定生存曲线显示问题的原因</title>
      <link>https://stats.stackexchange.com/questions/648490/identifying-the-cause-of-a-survival-curve-display-issue</link>
      <description><![CDATA[我正在 R 中进行生存分析（使用 Cox 比例风险回归）。我的总体样本死亡率约为 10％，但从我的分析得出的 Kaplan-Meier 生存曲线似乎显示生存率下降了约 40-50％，并且在 4 年左右急剧下降。这并不能反映实际数据集中死亡时间的模式，但我不确定是什么导致了这种差异……有人知道吗？

# 定义绘图的层
strata &lt;- expand.grid(
pression_level = levels(complete_cases_all$depression_level),
sex = levels(complete_cases_all$sex)
)
rownames(strata) &lt;- letters[1:nrow(strata)]

# 定义平均参与者
average_participant &lt;- expand.grid(
pression_level = levels(complete_cases_all$depression_level),
sex = levels(complete_cases_all$sex),
calendar_year = median(complete_cases_all$calendar_year),
age = median(complete_cases_all$age),
education_level = median(complete_cases_all$education_level),
employment = &quot;有竞争力的就业&quot;,
marital_status = &quot;单身&quot;,
rehab_payor_primary_type = &quot;非医疗补助&quot;,
func_score = median(complete_cases_all$func_score),
mental_health_tx_hx = &quot;否认有任何心理健康治疗史&quot;,
psych_hosp_hx = &quot;否认有任何精神病住院史&quot;,
problematic_substance_use = &quot;否&quot;,
suicide_attempt_hx = &quot;否认有任何自杀企图史&quot;
)
rownames(average_participant) &lt;- letters[1:nrow(average_participant)]

# 拟合生存模型
cxsf &lt;- survfit(model_5, newdata = average_participant, conf.type = &quot;none&quot;)
surv_cxsf &lt;- surv_summary(cxsf, data = complete_cases_all) |&gt; # 总结生存模型
tibble()
m_newdat &lt;- average_participant[as.character(surv_cxsf$strata), ] # 将新数据与生存总结层匹配

gg.surv.model5 &lt;- surv_model_5 |&gt;
ggsurvplot_df(
surv.geom = geom_line,
color = &quot;depression_level&quot;,
xlab = &quot;X 之后的时间（年）&quot;,
ylab = &quot;生存概率&quot;,
legend = c(0.175, 0.175),
conf.int = FALSE,
censor = FALSE,
surv.scale = &quot;percent&quot;,
break.time.by = 1,
xlim = c(0, 5), # 将 x 轴限制为 5 年
ylim = c(crop, 1),
palette = c(&quot;#90cbf9&quot;, &quot;#2196f3&quot;, &quot;#114b7a&quot;),
ggtheme = theme_classic(),
) +
labs(linetype = &quot;Sex&quot;, color = &quot;抑郁水平&quot;)

数据基于纵向数据集，并在初始研究登记日期后约 1 年和 4 年进行后续访谈。审查日期由每个审查参与者的最后一次后续访谈日期确定，死亡日期由每个死者官方死亡证明中列出的日期确定。时间 0 对应于每个参与者的研究登记日期，并在随后的 5 年内进行跟踪。
我检查了数据集，发现 4 年后死亡的死者数量异常，但在此期间似乎有合理的死亡频率（总死亡人数的约 23% 发生在 4 至 5 年之间）。我确保我的死亡时间基于日期，而不是（整数）跟踪间隔。最后，我确保受审查的参与者也在 4-5 年的时间范围内贡献数据。]]></description>
      <guid>https://stats.stackexchange.com/questions/648490/identifying-the-cause-of-a-survival-curve-display-issue</guid>
      <pubDate>Sun, 02 Jun 2024 18:03:43 GMT</pubDate>
    </item>
    <item>
      <title>R 中线性混合模型中随机效应结构的澄清</title>
      <link>https://stats.stackexchange.com/questions/648486/clarification-on-random-effects-structure-in-linear-mixed-models-in-r</link>
      <description><![CDATA[我正在使用线性混合模型来分析具有层次结构的数据集，其中随时间变化的测量值（级别 1）聚集在个体内（级别 2），而个体聚集在国家内（级别 3）。我最初在 R 中使用 lme4 包中的以下语法来建模此结构：
model &lt;- lmer(basdai ~ 1 + time + (1|country) + (1|id), data = data, REML = FALSE) 

但是，一位审阅者指出，我使用的语法并不代表 3 级模型，而是 2 级模型。他们建议改用以下语法：
model &lt;- lmer(basdai ~ 1 + time + (1|country) + (1|country:id), data = data, REML = FALSE)

据我了解，术语 (1|country) 表示每个国家（级别 3）的随机截距，而 (1|id) 表示每个个体（级别 2）的随机截距。我认为包括这两个术语将解释聚类。
如果您能提供关于这两个模型规范之间的差异的任何见解或解释，以及我的初始语法是否真正代表了 2 级结构而不是预期的 3 级层次结构，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/648486/clarification-on-random-effects-structure-in-linear-mixed-models-in-r</guid>
      <pubDate>Sun, 02 Jun 2024 17:13:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么如果 $T>>N$ 则 $O_p(N^{-1/2}) + O_p(T^{-1/2}) = O_p(N^{-1/2})$ 的数学推导</title>
      <link>https://stats.stackexchange.com/questions/648485/mathematical-derivation-of-why-o-pn-1-2-o-pt-1-2-o-pn-1-2-i</link>
      <description><![CDATA[我会尽量简单。假设我们有一系列随机变量，$X_{NT}$，其边界表达式为：$X_{NT} = O_p(N^{-1/2}) + O_p(T^{-1/2})$。现在，这意味着如果 $T&gt;&gt;N$，则 $X_{NT} = O_p(N^{-1/2})$，因为在这种情况下，$N^{-1/2}$ 占主导地位。使用$O_p(\cdot)$的定义，我们可以看到，对于每个$\varepsilon&gt;0$，存在一个常数$M$和$N_0$，使得
$$ P\left( \left|\frac{X_{NT}}{N^{-1/2} + T^{-1/2}} \right| &gt; M \right) &lt; \varepsilon $$
对于 $NT\geq N_0.$ 如果我犯了错误，请纠正我。
一般来说，这意味着 $O_p(N^{-1/2}) + O_p(T^{-1/2}) = O_p(C^{-1}_{NT})$，其中 $C_{NT} = min(\sqrt{T},\sqrt{N}).$
你能更严格地向我展示这一点吗？如果你能从概念上解释一下，我将不胜感激。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/648485/mathematical-derivation-of-why-o-pn-1-2-o-pt-1-2-o-pn-1-2-i</guid>
      <pubDate>Sun, 02 Jun 2024 16:54:29 GMT</pubDate>
    </item>
    <item>
      <title>领域知识在聚类中是否需要外部验证？</title>
      <link>https://stats.stackexchange.com/questions/648483/is-domain-knowledge-external-validation-in-clustering</link>
      <description><![CDATA[我有聚类结果，其 Silhuette Width 等值良好。聚类大小为：4998、1、1，这并不好，因为我知道我的客户没有该特定分区（它更平衡）。因此，我忽略了这些聚类结果并得出结论，它很糟糕。这是否意味着我使用了外部验证，因为我得出的聚类不好的结论是基于我的领域知识？]]></description>
      <guid>https://stats.stackexchange.com/questions/648483/is-domain-knowledge-external-validation-in-clustering</guid>
      <pubDate>Sun, 02 Jun 2024 16:08:19 GMT</pubDate>
    </item>
    <item>
      <title>从一副 40 张牌中选出 4 张总点数为 5 的牌的概率</title>
      <link>https://stats.stackexchange.com/questions/648482/probability-of-selecting-4-cards-that-add-up-to-5-from-a-deck-of-40-cards</link>
      <description><![CDATA[假设我们有一副牌，不包括人头牌，也就是从 A 到 10 的牌。
以下哪种方法可以正确计算 4 张随机选择的牌之和等于 5 的概率？
方法 1：
$P(X=5)=\frac{8}{40}\cdot\frac{7}{39}\cdot\frac{6}{38}\cdot\frac{4}{37}$
我的逻辑是，对于第一张牌，有 8 种可能性。4 张 A 和 4 张 2。对于第二张牌，如果第一张牌是一张 A，则有 7 种可能性。3 张 A 和 4 张 2。如果第二张牌是一张 A，则可以从 2 张 A 和 4 张 2 中选择第三张牌。最后，如果第三张牌是 A，那么第四张牌就必须是 2，因此它的样本空间只有 4 张 2。
方法 2：
$P(X=5) = \frac{8}{40}\cdot\frac{4}{39}\cdot\frac{3}{38}\cdot\frac{2}{37}$
与方法 1 一样，第一张牌可以从 4 张 A 和 4 张 2 中选择。不过这次，我们假设第一张牌是 2。根据定义，所有剩余的牌都必须是 A。
方法 3：
$P(X=5)=\frac{4}{40}\cdot\frac{4}{39}\cdot\frac{3}{38}\cdot\frac{2}{37}$
我们知道，为了得到总和 5，我们特别需要 1 张 2 牌和 3 张 A。第一个分数表示选择 2 的概率，其余分数与 A 有关。
我认为方法 3 是所有方法中最合理的，但我仍然觉得它们都是错的，我应该使用超几何公式来找出概率。]]></description>
      <guid>https://stats.stackexchange.com/questions/648482/probability-of-selecting-4-cards-that-add-up-to-5-from-a-deck-of-40-cards</guid>
      <pubDate>Sun, 02 Jun 2024 15:58:08 GMT</pubDate>
    </item>
    <item>
      <title>关于分解图（季节性）的问题</title>
      <link>https://stats.stackexchange.com/questions/648481/question-on-decomposition-plot-seasonality</link>
      <description><![CDATA[通过查看分解图，我无法真正理解季节性的存在。因此，我附上了两个分解图。该系列中存在季节性吗？
第一个是CPI系列（2003年=100），月度和季节性未调整系列

第二个是通货膨胀。
]]></description>
      <guid>https://stats.stackexchange.com/questions/648481/question-on-decomposition-plot-seasonality</guid>
      <pubDate>Sun, 02 Jun 2024 15:52:38 GMT</pubDate>
    </item>
    <item>
      <title>理解广义线性模型中高度相关特征的系数</title>
      <link>https://stats.stackexchange.com/questions/648479/understanding-the-coefficients-of-highly-correlated-features-in-generalized-line</link>
      <description><![CDATA[我正在尝试拟合广义线性模型，为简单起见，假设它是一个线性回归。我有一堆特征，我为它拟合了一个线性模型，特征 x9 并不重要（例如，t 统计量很小）。
 OLS 回归结果 
===========================================================================================
依赖变量：y R 平方：0.518
模型：OLS Adj。 R 平方：0.507
方法：最小二乘 F 统计量：46.27
日期：2017 年 3 月 8 日，星期三 概率（F 统计量）：3.83e-62
时间：10:08:24 对数似然：-2386.0
观测值数量：442 AIC：4794。
Df 残差：431 BIC：4839。
Df 模型：10
协方差类型：非稳健
============================================================================================
coef std err t P&gt;|t| [0.025 0.975]
-----------------------------------------------------------------------------
const 152.1335 2.576 59.061 0.000 147.071 157.196
x1 -10.0122 59.749 -0.168 0.867 -127.448 107.424
x2 -239.8191 61.222 -3.917 0.000 -360.151 -119.488
x3 519.8398 66.534 7.813 0.000 389.069 650.610
x4 324.3904 65.422 4.958 0.000 195.805 452.976
x5 -792.1842 416.684 -1.901 0.058 -1611.169 26.801
x6 476.7458 339.035 1.406 0.160 -189.621 1143.113
x7 101.0446 212.533 0.475 0.635 -316.685 518.774
x8 177.0642 161.476 1.097 0.273 -140.313 494.442
x9 1.2793 1.902 0.070 0.906 0.409 2.150
==========================================================================================

但是当我向其中添加另一个特征 x10 时，突然 x9 和 x10 都变得显著，两者都具有很大的系数但符号相反，并且 R^2 变得更高，我该如何解释这一点？
 OLS 回归结果
=====================================================================================
依赖变量：y R 平方：0.520
模型：OLS Adj。 R 平方：0.509
方法：最小二乘 F 统计量：46.27
日期：2017 年 3 月 8 日，星期三 概率（F 统计量）：3.81e-62
时间：10:08:24 对数似然：-2380.0
观测值数量：442 AIC：4781。
Df 残差：431 BIC：4831。
Df 模型：10 
协方差类型：非稳健 
================================================================================================
coef std err t P&gt;|t| [0.025 0.975]
-----------------------------------------------------------------------------
const 152.1335 2.576 59.061 0.000 147.071 157.196
x1 -10.0122 59.749 -0.168 0.867 -127.448 107.424
x2 -239.8191 61.222 -3.917 0.000 -360.151 -119.488
x3 519.8398 66.534 7.813 0.000 389.069 650.610
x4 324.3904 65.422 4.958 0.000 195.805 452.976
x5 -792.1842 416.684 -1.901 0.058 -1611.169 26.801
x6 476.7458 339.035 1.406 0.160 -189.621 1143.113
x7 101.0446 212.533 0.475 0.635 -316.685 518.774
x8 177.0642 161.476 1.097 0.273 -140.313 494.442
x9 5751.2793 5171.902 4.370 0.950 5413.409 5189.150
x10 -7867.6254 -7765.984 1.025 0.906 -9862.065 -6197.316
===================================================================================
]]></description>
      <guid>https://stats.stackexchange.com/questions/648479/understanding-the-coefficients-of-highly-correlated-features-in-generalized-line</guid>
      <pubDate>Sun, 02 Jun 2024 15:14:48 GMT</pubDate>
    </item>
    <item>
      <title>我被分配使用 Metropolis 算法来数值求解积分。我做错了什么？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/648478/i-have-been-assigned-to-use-the-metropolis-algorithm-in-order-to-solve-numerical</link>
      <description><![CDATA[几天来，我一直在尝试使用 metropolis 算法解决以下问题：
$A = \frac{\int_0^1...\int_0^1\prod_{l=1}^N d^3r_l\frac{1}{|\vec{r_1}-\vec{r_2}|_\alpha}*e^{\eta u(\vec{r_1},...,\vec{r_N})}}{\int_0^1...\int_0^1\prod_{l=1}^N d^3r_le^{\eta u(\vec{r_1},...,\vec{r_N})}}$
其中 $u(.)=\frac{1}{N}\sum_{1\le l &lt; j\le N}\frac{1}{|\vec{r_l}-\vec{r_j}|_\alpha}$。
对于$|\vec{r_l}-\vec{r_j}|\ge \alpha$，则$\frac{1}{|\vec{r_l}-\vec{r_j}|_\alpha}=\frac{1}{|\vec{r_l}-\vec{r_j}|}$，而对于$|\vec{r_l}-\vec{r_j}|\le \alpha$，则$\frac{1}{|\vec{r_l}-\vec{r_j}|_\alpha}=-\frac{1}{\alpha}$.
这取自：https://arxiv.org/pdf/astro-ph/0505561.
我基本上用大都市算法来近似 A，用一个小的增量因子取代之前的坐标。以下总和（m 是蒙特卡洛的迭代次数）是我在代码中使用的：
$&lt;A&gt; = \frac{1}{m}\sum_1^m \frac{1}{|\vec{r_1}-\vec{r_2}|_\alpha}$
（还有一个常数 $\frac{N(N-1)}{2}$，但我暂时忽略它，因为我将 A 代入另一个方程中，它们会相互抵消）
我的接受标准是，如果 $\Delta E \le 0$，则移动被接受，如果它大于 0，我会使用从 0 到 1 的随机数来执行典型的接受标准。
我是否正确使用了 Metropolis 算法，还是我做错了什么？
任何帮助都将不胜感激，因为我已经为此困扰了好几天 :)]]></description>
      <guid>https://stats.stackexchange.com/questions/648478/i-have-been-assigned-to-use-the-metropolis-algorithm-in-order-to-solve-numerical</guid>
      <pubDate>Sun, 02 Jun 2024 14:38:00 GMT</pubDate>
    </item>
    <item>
      <title>对于左截断计数数据，哪个对数似然要最大化？</title>
      <link>https://stats.stackexchange.com/questions/648477/which-log-likelihood-is-to-be-maximized-for-left-truncated-count-data</link>
      <description><![CDATA[如果计数数据缺少零点上的计数（即左截断数据），该怎么办？假​​设有人想估计泊松回归，目标是推导出最大化的对数似然。
我的方法：
$$
f(y_i \mid y_i &gt; 0, x_i) = \frac{f(y_i \mid x_i)}{1 - f(y_i = 0 \mid x_i)}
$$
$$
f(y_i \mid y_i &gt; 0, x_i) = \frac{e^{-\lambda} \lambda^{x_i}}{x_i! \left(1 - e^{-\lambda}\right)}
$$
要最大化的似然函数是：
$$
L(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{x_i}}{x_i! \left(1 - e^{-\lambda}\right)}
$$
对数似然函数为：
$$
\log L(\lambda) = \sum_{i=1}^{n} \left(-\lambda + x_i \log(\lambda) - \log(x_i!) - \log\left(1 - e^{-\lambda}\right)\right)
$$]]></description>
      <guid>https://stats.stackexchange.com/questions/648477/which-log-likelihood-is-to-be-maximized-for-left-truncated-count-data</guid>
      <pubDate>Sun, 02 Jun 2024 14:23:04 GMT</pubDate>
    </item>
    <item>
      <title>拒绝怀疑不属于目标人群的样本[关闭]</title>
      <link>https://stats.stackexchange.com/questions/648475/rejection-of-samples-suspected-of-not-coming-of-the-target-population</link>
      <description><![CDATA[假设有一个平稳过程，它应该类似于具有“已知”均值和方差的高斯分布。从中取出三次（或 n 次）Iid 样本。众所周知，一些样本可能会受到污染，即包括来自其他分布（具有不同的形状、均值和/或方差）的观测值。
假设这种污染可以通过产生具有不可思议的高范围的样本来表现出来。血压测量可以作为一个例子。在这种情况下，偶尔会出现不代表被测变量的“奇怪”测量值，虽然它们的起源尚有争议，但它们很容易是测量过程中血压本身偶尔出现的时间波动以及仪器和方法的怪癖的结果。这被认为是污染，因为此类异常值是由测量技术机械引入的，并且与临床无关。
因此，评估每个 n 重样本的范围，并将其与基于假定的“干净”高斯母分布选择的截止值进行比较。如果超出该值，则拒绝该样本，并且不将其纳入分析。
我的问题是（以三次重复为例，假设该程序是可接受的）：以下哪种方法更合适？

如果前两个单独的观察值超出允许的最大范围，则拒绝它们并从下一个观察值开始下一个三次重复样本。

无论如何都要取第三个样本以完成三次采样程序，然后拒绝它。


第一个选项保证（给定上述程序）无论尚未进行的第三次观察如何，样本都将被拒绝，但拒绝标准是基于三次重复的范围，而不是两个相邻测量之间的差异。]]></description>
      <guid>https://stats.stackexchange.com/questions/648475/rejection-of-samples-suspected-of-not-coming-of-the-target-population</guid>
      <pubDate>Sun, 02 Jun 2024 13:41:51 GMT</pubDate>
    </item>
    <item>
      <title>对顺序数据的依赖性是什么？</title>
      <link>https://stats.stackexchange.com/questions/648464/what-is-dependency-on-sequential-data</link>
      <description><![CDATA[我们知道，只要数据集中的点依赖于数据集中的其他点，数据就被称为顺序数据。
一个常见的例子是时间序列，例如天气数据。

我的问题是“依赖”一词是什么意思？在上面的例子中，数据点如何相互依赖？]]></description>
      <guid>https://stats.stackexchange.com/questions/648464/what-is-dependency-on-sequential-data</guid>
      <pubDate>Sun, 02 Jun 2024 05:33:05 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 CDF 而不是 PDF 来表达无意识统计学家的定律吗？[重复]</title>
      <link>https://stats.stackexchange.com/questions/648452/can-we-express-the-law-of-the-unconscious-statistician-using-the-cdf-instead-of</link>
      <description><![CDATA[我只见过LOTUS以密度形式给出
$$\mathbb{E}[g(X)] = \int g(x) f(x) dx$$
或以勒贝格-斯蒂尔杰斯积分形式给出
$$\mathbb{E}[g(X)] = \int g(x) dF_X(x).$$
我还见过这篇文章，其中展示了如何使用 CDF 而不是PDF：

与计算这些矩类似，有没有办法使用 CDF 而不是 PDF 来编写 LOTUS？]]></description>
      <guid>https://stats.stackexchange.com/questions/648452/can-we-express-the-law-of-the-unconscious-statistician-using-the-cdf-instead-of</guid>
      <pubDate>Sat, 01 Jun 2024 22:37:36 GMT</pubDate>
    </item>
    <item>
      <title>什么是协同过滤？</title>
      <link>https://stats.stackexchange.com/questions/648424/what-defines-collaborative-filtering</link>
      <description><![CDATA[协同过滤的定义特征是什么？
如果您采用两个嵌入向量（一个用于用户，一个用于项目），您可以进行点积并将结果传递给 S 型函数来预测 CTR。这是协同过滤吗？
或者，您可以在每个嵌入上放置多个 FC 层，然后进行点积。这是协同过滤吗？
您还可以将多个离散和连续特征连接到每个嵌入，然后将它们传递给 FC 层并进行 S 型函数。这是协同过滤吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648424/what-defines-collaborative-filtering</guid>
      <pubDate>Sat, 01 Jun 2024 09:23:06 GMT</pubDate>
    </item>
    <item>
      <title>从统计角度来看，R 中的 transform="" 在绘制平滑缩放的 Schoenfeld 残差图时起什么作用？</title>
      <link>https://stats.stackexchange.com/questions/648370/what-does-transform-in-r-statistically-wise-do-when-plotting-the-smoothed-s</link>
      <description><![CDATA[正如标题所示，我旨在解决 R 中的参数 transform = &#39;&#39;，特别是关于用于评估比例风险 (PH) 假设的平滑缩放 Schoenfeld 残差检验/图。
我知道默认值是 transform = &#39;km&#39;，但也有其他可用选项：
transform=&#39;log&#39;
transform=&#39;rank&#39;
transform=&#39;identity&#39;

我相信我已经掌握了每个选项的含义。如果我的理解不正确，请纠正我：
log 用于强调早期事件，因为它为它们分配了更大的权重。
rank 用于突出显示晚期事件，因为它为它们分配了更大的权重。
identity 表示未经任何修改的原始时间数据。
但是，我不确定 km 到底是做什么的。虽然它似乎可以节省时间，但我不确定它的含义。
我的问题：

从统计的角度来看，有人可以详细说明每个转换背后的细节吗？

从统计的角度来看，在哪种情况下我应该使用每个转换来评估 R 中的 cox.zph() 测试/绘图？

每个转换都会为 PH 假设产生不同的 p 值。根据所选的转换，我应该信任哪个 p 值？


编辑：我的问题已关闭，因为它似乎不属于这里，因为它与统计无关。我将尝试澄清，我想从统计的角度而不是编码的角度来理解问题。提前谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/648370/what-does-transform-in-r-statistically-wise-do-when-plotting-the-smoothed-s</guid>
      <pubDate>Fri, 31 May 2024 12:11:26 GMT</pubDate>
    </item>
    <item>
      <title>调整后的 R^2 或失拟值</title>
      <link>https://stats.stackexchange.com/questions/648362/adjusted-r2-or-lack-of-fit</link>
      <description><![CDATA[这可能是一个基本问题，但我还是会问，因为我没有从论坛中找到任何合适的结论。
我正在使用响应面方法拟合我的数据。因此，理想情况下，响应和独立变量之间的关系应描述如下：

当我包含所有系数时，R^2 调整很好，但缺乏拟合度是显著的。在缩小模型/添加参数/转换数据后，我仍然无法通过良好的 R^2 调整实现拟合度的缺失。
由于 R^2 调整很大，我是否仍可以使用方程来描述变量对响应的影响？因此，我不会将其用于预测目的，而只是用于描述目的。]]></description>
      <guid>https://stats.stackexchange.com/questions/648362/adjusted-r2-or-lack-of-fit</guid>
      <pubDate>Fri, 31 May 2024 10:26:42 GMT</pubDate>
    </item>
    </channel>
</rss>