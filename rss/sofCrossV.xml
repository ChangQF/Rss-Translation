<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 19 Oct 2024 18:21:09 GMT</lastBuildDate>
    <item>
      <title>理解条件独立性的因式分解，摘自 Kevin Murphy 第 2 册</title>
      <link>https://stats.stackexchange.com/questions/656014/understanding-the-factorization-of-conditional-independence-from-kevin-murphy-bo</link>
      <description><![CDATA[我正在自学 Kevin Murphy 的第二本书，我想澄清一个简单的推导。在第 156-157 页，对概率图模型进行了分解。 pgm 显示如下

在下面显示的概率模型中：
$$
p(\theta, D) = p(\theta_x)p(\theta_y) \prod_{n=1}^{N} p(y_n \mid \theta_y)p(x_n \mid y_n, \theta_x) \tag{4.38}
$$
由此可知：
$$
p(\theta, D) = \left[ p(\theta_y) \prod_{n=1}^{N} p(y_n \mid \theta_y) \right] \left[ p(\theta_x) \prod_{n=1}^{N} p(x_n \mid y_n, \theta_x) \right] \tag{4.39}
$$
下一步写为：
$$
p(\theta, D) = \left[ p(\theta_y) p(D_y \mid \theta_y) \right] \left[ p(\theta_x) p(D_x \mid \theta_x) \right] \tag{4.40}
$$
其中：
$$
D_y = \{y_n\}_{n=1}^{N} \quad \text{and} \quad D_x = \{x_n, y_n\}_{n=1}^{N}。
$$
我的问题是：等式 $(4.40)$ 能否从 $(4.39)$ 得出？具体来说，方程$(4.40)$表明联合概率项$p(x_n, y_n \mid \theta_x)$，而不是$(4.39)$中的条件项$p(x_n \mid y_n, \theta_x)$。
我理解以下说法是正确的：
$$
p(x_n \mid y_n, \theta_x) = \frac{p(x_n, y_n \mid \theta_x)}{p(y_n \mid \theta_x)}。
$$
但是，除非我们假设 $y_n$ 和 $\theta_x$ 是独立的，否则方程 $(4.40)$ 怎么能成立呢？观察 $x_n$ 是否使 $y_n$ 和 $\theta_x$ 独立？您能解释从 $(4.39)$ 到 $(4.40)$ 的转变的有效性吗？
PS：我已经在 math.stackoverflow 上问过这个问题，但没有得到任何答案。]]></description>
      <guid>https://stats.stackexchange.com/questions/656014/understanding-the-factorization-of-conditional-independence-from-kevin-murphy-bo</guid>
      <pubDate>Sat, 19 Oct 2024 17:56:27 GMT</pubDate>
    </item>
    <item>
      <title>高斯信息瓶颈与中心极限定理</title>
      <link>https://stats.stackexchange.com/questions/656012/gaussian-information-bottleneck-and-central-limit-theorem</link>
      <description><![CDATA[我有一个问题，可能很愚蠢，但我找不到答案。
我们知道高斯信息瓶颈解决方案以封闭形式存在，并且它对数据的任何线性和可逆变换都是不变的。让我们考虑这样的情况：我们有$(X,Y)$，使得$Y=AX+\epsilon$和$X$是非高斯分布的，但其数据分布验证了中心极限定理的所有假设。因此，$\tilde{X}=\boldsymbol{B}X$ 应该近似服从高斯分布，而$\tilde{Y}=A\tilde{X}+\tilde{\epsilon}$ 也服从高斯分布（因为它是高斯变量的线性变换），高斯信息瓶颈应该是最优的，但 GIB 的解在线性变换下应该保持不变。那么，如果解对线性变换不变，为什么 GIB 解在一种情况下是最优的，而在另一种情况下却不是最优的呢？]]></description>
      <guid>https://stats.stackexchange.com/questions/656012/gaussian-information-bottleneck-and-central-limit-theorem</guid>
      <pubDate>Sat, 19 Oct 2024 17:50:06 GMT</pubDate>
    </item>
    <item>
      <title>使用截断正态分布规律的简化公式P(t)是否正确？</title>
      <link>https://stats.stackexchange.com/questions/656011/is-it-correct-to-use-the-simplified-formula-pt-for-the-truncated-normal-distri</link>
      <description><![CDATA[对于我们的研究，我们有一本指南，其中包含截断正态分布的公式：$$
P(t) = \frac{0.5 - \Phi \left( \frac{t - m}{\sigma} \right)}{0.5 + \Phi \left( \frac{m}{\sigma} \right)}
$$我将其用于具有以下条件的任务：
汽车发电机的运行时间遵循截断正态分布，参数为 m = 8000 小时，σ = 1000 小时。需要找到 10,000 小时无故障运行的概率、6,000 小时的故障率、10,000 小时的故障强度以及首次故障的平均时间。我得到的值为 -0.318，这对于概率来说不正确。之后，我使用了截断正态分布的标准公式：
$$P(t) = \frac{\Phi \left( \frac{t - m}{\sigma} \right) - \Phi \left( \frac{0-m}{\sigma} \right)}{1 - \Phi \left( \frac{0-m}{\sigma} \right)}
$$ 我得到了 0.9972 的结果，然后我就可以继续解决问题了。我是不是漏掉了什么？或者，由于 𝑧 的值很大，简化公式在这种情况下根本不正确？接下来，我使用以下公式解决了另一个关于正态分布的问题：$$P(t) = 0.5 - \Phi \left( \frac{t - m}{\sigma} \right)
$$ 我得到了正确的值，但它们与标准正态分布公式的结果有很大不同。使用简化公式是否正确？即使我们将它们用于学习，我也看不出与使用标准公式有什么区别，因为它们总是有效的。我遗漏了什么吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/656011/is-it-correct-to-use-the-simplified-formula-pt-for-the-truncated-normal-distri</guid>
      <pubDate>Sat, 19 Oct 2024 17:47:11 GMT</pubDate>
    </item>
    <item>
      <title>MNAR（非随机缺失）不是一个无法证明的假设吗？</title>
      <link>https://stats.stackexchange.com/questions/656010/isnt-mnarmissing-not-at-random-just-an-unprovable-hypothesis</link>
      <description><![CDATA[如果参与者在干预或安慰剂分配后在研究中退出，并且我们根据这些退出结果分析数据，我们是否只能识别关联而不是因果关系，因为我们使用的是病例对照设计，其中病例是退出者，非病例是非退出者？
此外，除了随机分配的治疗（如药物或安慰剂）之外，难道不可能找到 MNAR 对任何变量的决定因素吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/656010/isnt-mnarmissing-not-at-random-just-an-unprovable-hypothesis</guid>
      <pubDate>Sat, 19 Oct 2024 17:40:43 GMT</pubDate>
    </item>
    <item>
      <title>OLS 线性回归为 +/-1 标​​准差，而不是平均值</title>
      <link>https://stats.stackexchange.com/questions/656008/ols-linear-regression-of-1-standard-deviation-and-not-the-mean</link>
      <description><![CDATA[我确信我忽略了一些细微差别...我知道 OLS 线性回归可用于确定观测数据 $y$ 的平均值如何依赖于独立数据 $x$。我还知道，分位数回归最小化绝对偏差，与 OLS 的平方偏差相反，可用于确定 $y$ 的中位数如何依赖于 $x$。如果我进一步以不同的方式衡量正偏差和负偏差，分位数回归可以确定 $y$ 的特定分位数/百分位数如何依赖于 $x$。在我的例子中，我使用分位数回归估计第 5 个百分位数截止值，以定义给定 $x$ 的 $y$ 观测值的正常/异常边界。
是否存在等效权重，可以确定 $y$ 的 $1.64\sigma$ 如何依赖于 $x$？]]></description>
      <guid>https://stats.stackexchange.com/questions/656008/ols-linear-regression-of-1-standard-deviation-and-not-the-mean</guid>
      <pubDate>Sat, 19 Oct 2024 16:42:26 GMT</pubDate>
    </item>
    <item>
      <title>什么时候 bagging 实际上会导致更高的方差？</title>
      <link>https://stats.stackexchange.com/questions/656007/when-can-bagging-actually-lead-to-higher-variance</link>
      <description><![CDATA[根据线性回归的高斯-马尔可夫假设，普通最小二乘估计 (OLS) 在所有无偏线性估计中具有最小方差。
在这种情况下，“Bagging”也是线性和无偏的，因此其方差必须严格更差。基于零独立变量（即只有一个偏差项）的情况，我的直觉是，当随机采样数据的模型数量趋于无穷大时，方差接近 OLS。
在决策树的设置中，与使用单个决策树相比，从 bagging（随机森林）获得的方差更低，这并不让我感到惊讶。但考虑到线性回归的情况，我看到的对此的解释似乎证明太多了。应满足哪些条件才能确保 bagging 不会使方差恶化？]]></description>
      <guid>https://stats.stackexchange.com/questions/656007/when-can-bagging-actually-lead-to-higher-variance</guid>
      <pubDate>Sat, 19 Oct 2024 15:28:15 GMT</pubDate>
    </item>
    <item>
      <title>稳健性度量应用于实际问题[关闭]</title>
      <link>https://stats.stackexchange.com/questions/656005/robustness-measure-applied-to-real-world-problem</link>
      <description><![CDATA[我必须为一个大学项目解决以下任务：
“建议一个稳健性度量以获得最佳定价，并计算手头数据集的此类参数。在这里，您可以测量对消费者数据可能发生变化的稳健性。”
我很乐意听到有关如何解决此问题的建议。请考虑到我的统计知识有限。以下是该项目的描述。
我正在开展的项目主要围绕通过最佳定价策略和税收政策来提高全球南方咖啡农的收入。它涉及三个群体：消费者、咖啡公司和政府，每个群体都关注不同的目标，例如最大化效用、利润和社会福利。这些团队正在分析优质和非优质咖啡的消费者行为和定价决策，并在 13 个时间段内跟踪了 6 个家庭。
关键假设包括：

家庭可以购买优质或非优质咖啡，但不能同时购买两者。
咖啡需求在价格限制内缺乏弹性，家庭行为理性。
公司对风险持中性态度，咖啡的边际效用会随着时间的推移而减少。

最佳价格点和税率是通过平衡消费者的支付意愿和公司的收入门槛来确定的。优质价格 45 和非优质价格 35.5 产生了最高的总收入 546.01，公司的最低门槛为 461.01。政府的税率定为 16%，计算方法为最高收入与公司最低门槛之间的差额除以最高收入。
我还可以提供更详细的文档和/或包含数据和所有计算的 Excel 文件。遗憾的是，我无法在此处附加文档。]]></description>
      <guid>https://stats.stackexchange.com/questions/656005/robustness-measure-applied-to-real-world-problem</guid>
      <pubDate>Sat, 19 Oct 2024 14:57:44 GMT</pubDate>
    </item>
    <item>
      <title>SEM 中的二阶潜变量</title>
      <link>https://stats.stackexchange.com/questions/656004/second-order-latent-variable-in-sem</link>
      <description><![CDATA[我计划使用 SEM 研究教学质量对幸福感的影响。幸福感被概念化为由五个维度组成，也可以计算一般因素。我的假设既适用于特定维度，也适用于一般因素。
验证性因子分析 (CFA) 表明，五因子模型和具有正交一阶因子的二阶因子模型都很好地拟合了数据。进行两次单独的 SEM 分析是否合适：一次以五因子模型为结果，另一次以二阶模型为结果？这种方法将使我能够测试与特定因素和一般因素相关的假设。
由于我对 SEM 还比较陌生，因此非常感谢您对我的研究设计的反馈。]]></description>
      <guid>https://stats.stackexchange.com/questions/656004/second-order-latent-variable-in-sem</guid>
      <pubDate>Sat, 19 Oct 2024 14:32:52 GMT</pubDate>
    </item>
    <item>
      <title>神经网络中 95% 的能量减少如何影响准确性和效率？</title>
      <link>https://stats.stackexchange.com/questions/656003/how-does-a-95-energy-reduction-in-neural-networks-affect-accuracy-and-efficienc</link>
      <description><![CDATA[本月初，arXiv 上发表了一篇引人入胜的论文：
“加法是节能语言模型所需的全部”
作者提出了一种算法，$\mathcal{L}\text{-Mul}$，该算法声称可将元素浮点 (FP) 张量乘法的能耗降低高达 95%，点积的能耗降低 80%。关键创新是使用整数加法近似 FP 乘法，有可能保持几乎相同的模型精度，同时大幅提高效率
从表面上看，这似乎是一个突破性的发现。
传统上，FP 算法涉及将两个浮点数的尾数相乘，其复杂度为二次（$O(m^2)$，其中 $m$ 是尾数中的位数）。这通常是一个计算瓶颈。 $\mathcal{L}\text{-Mul}$ 使用加法的线性组合来近似这个尾数乘法，将复杂度降低到 $O(m)$，从而优化了时间效率和功耗。
阅读本文时，我发现了一些问题：

精度比较仅针对 fp8 格式，但节能效果与 fp16 和 fp32 等较大格式进行了比较，这似乎不一致。需要澄清的是，fp8（8 位浮点）、fp16（16 位浮点）和 fp32（32 位浮点）代表浮点格式中不同级别的数值精度，其中 fp8 提供的精度最低，而 fp32 提供的精度最高。位大小越低（例如 fp8），用于存储数字的位就越少，这可以减少内存和计算需求，但代价是精度。将 $\mathcal{L}\text{-Mul}$ 的节能效果与 fp16 和 fp32（本质上使用更多位和功率）进行比较，可能会对能效产生不公平的乐观看法，因为该算法以较低的精度运行。理想情况下，应在一致的水平上比较精度和能量，以避免得出误导性结论。

该论文缺乏大规模测试或实际部署，因此大多数声明都只是理论上的。

有许多拼写和语法错误，有损专业语气，给人一种仓促发布的印象。我知道作者可能不是英语母语人士，但即使粗略校对也能发现大部分错误。

这篇论文包含相当多的拼写和语法错误，这降低了其可读性和专业性。它给人一种仓促完成的印象，这对于旨在引入重大突破的技术预印本来说是不寻常的。仔细校对会加强这篇论文。

实验似乎缺乏大规模测试或实际部署结果。鉴于降低能耗的雄心勃勃的主张，大规模的实际评估将为该方法提供可信度。目前的评估仅限于基准测试，而不是功耗更为关键的大型部署设置。

该方法在硬件效率方面仍处于理论阶段。虽然它在软件模拟中显示出前景，但需要硬件级实现来确认节能效果。如果没有这一点，节能声明可能无法完全转化为实践。


我还认为两位作者都在一家公司工作，而不是在大学工作，这很有趣：BitEnergy AI：https://bitenergy.ai/
好的，最后：
神经网络中 95% 的能量减少如何影响准确性和效率？]]></description>
      <guid>https://stats.stackexchange.com/questions/656003/how-does-a-95-energy-reduction-in-neural-networks-affect-accuracy-and-efficienc</guid>
      <pubDate>Sat, 19 Oct 2024 13:48:05 GMT</pubDate>
    </item>
    <item>
      <title>针对元分析，应选择哪个估计量^REML 还是使用 Wild Bootstrap 的 CR2？</title>
      <link>https://stats.stackexchange.com/questions/656000/which-estimator-to-choose-for-meta-analysis-reml-or-cr2-with-wild-bootstrap</link>
      <description><![CDATA[我正在关注以下书籍：https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/multilevel-ma.html
我无法选择要使用哪个估计器：REML 还是带有 Wild Bootstrap 的 CR2。
或者也许同时运行它们并呈现为敏感性分析，尽管我看不到任何其他元分析这样做，所以也许这会进一步使文章的读者感到困惑。
一些细节：
我的样本量很小（两者都可以帮助）
和依赖效应大小（CR2 最适合它）
我的结果是一个连续变量（REML 似乎更适合）
如果您有类似的问题，您能否建议您做了什么？谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/656000/which-estimator-to-choose-for-meta-analysis-reml-or-cr2-with-wild-bootstrap</guid>
      <pubDate>Sat, 19 Oct 2024 12:01:22 GMT</pubDate>
    </item>
    <item>
      <title>是否可以将其作为时间序列数据分析？我应该使用什么模型来处理这些数据？</title>
      <link>https://stats.stackexchange.com/questions/655999/is-it-possible-to-analyse-this-as-time-series-data-what-model-should-i-use-for</link>
      <description><![CDATA[我有一个从 1 到 10 的变量，其中 1 表示“永远无法证明”，而 10 表示“始终可以证明”。答案在极端值和中间值（1、10、5、6）处密度较高。此项目还有一个“不想回答”选项。这个问题是在 5 轮研究中从不同人（但在同一个国家）收集的。
我想使用其他 5 个分类和数值变量来预测此变量的响应。问题是，我不知道是否可以使用线性回归、逻辑回归，或者我是否需要切换到机器学习模型。无论如何，如果我可以将它们作为时间数据进行陈述，那将是理想的，但我认为 5 个数据集合少于任何模型所需的数量。
所以我的问题是：

是否可以仅使用 5 个数据集合来对此数据进行陈述？
如果我的 10 分项目作为因变量，我应该使用什么模型？
]]></description>
      <guid>https://stats.stackexchange.com/questions/655999/is-it-possible-to-analyse-this-as-time-series-data-what-model-should-i-use-for</guid>
      <pubDate>Sat, 19 Oct 2024 11:36:18 GMT</pubDate>
    </item>
    <item>
      <title>后验预测 p 值和模型复杂性[关闭]</title>
      <link>https://stats.stackexchange.com/questions/655998/posterior-predtive-p-values-and-model-complexity</link>
      <description><![CDATA[我正在执行贝叶斯后验预测检验，我发现更复杂模型（所有随机效应）的后验预测 p 值比更简单模型（所有固定效应或混合效应）的后验预测 p 值更远离（且低于）0.5。这可能吗？显然，一切都是正确的。]]></description>
      <guid>https://stats.stackexchange.com/questions/655998/posterior-predtive-p-values-and-model-complexity</guid>
      <pubDate>Sat, 19 Oct 2024 08:53:54 GMT</pubDate>
    </item>
    <item>
      <title>使用列范数限制精度矩阵的谱范数</title>
      <link>https://stats.stackexchange.com/questions/655993/bound-the-spectral-norm-of-the-precision-matrix-using-column-wise-norm</link>
      <description><![CDATA[在本文中，作者获得了关于谱范数的精度矩阵的收敛速度，（定理 1，第 7 页）
$$
\|\hat{\Omega}-\Omega\|_2 \le CM_p s_p\sqrt{\frac{\log p}{n}}
$$
其中$\|\Omega\|_{L1}=\underset{1\le j\le p}{\max}\sum_{i=1}^{p}|\omega_{ij}|\le M_p$ 和 $\underset{1\le j\le p}{\max}\sum_{i=1}^{p}1\left(\omega_{ij}\neq 0\right)\le s_p$.
他们通过控制精度矩阵的列来证明这个定理（第 37 页，不等式 (9)）：
$$
|\hat{\beta}_{S_i}-\omega_{S_i}|\le C\sqrt{\frac{\log p}{n}}
$$
其中 $\hat{\beta}_i$ 是 $\hat{\Omega}$ 的第 i 列。
我们如何使用逐列边界来控制收敛速度？非常感谢！

本文中的定理2给出了矩阵无穷范数的收敛速度，$|A|_{\infty}=\underset{1\le i\le p,1\le j\le p}{\sum}|a_{ij}|$，他们说
$$
|\hat{\Omega}-\Omega|_{\infty}\le CM_p\sqrt{\frac{\log p}{n}}.
$$
我们可以使用以下事实得到定理1中指定的结果：$\|A\|_2\le\|A\|_{L_1}\le s_p|A|_{\infty}$。也就是说，我们得到了结果
$$
\|\hat{\Omega}-\Omega\|_2\le CM_p s_p\sqrt{\frac{\log p}{n}}.
$$
但是，我仍然不知道如何按照本文所述方法得到谱范数的收敛速度。这可能是笔误。]]></description>
      <guid>https://stats.stackexchange.com/questions/655993/bound-the-spectral-norm-of-the-precision-matrix-using-column-wise-norm</guid>
      <pubDate>Sat, 19 Oct 2024 02:44:14 GMT</pubDate>
    </item>
    <item>
      <title>条件不平衡独立变量在回归中是一个问题吗？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/655709/are-conditional-unbalanced-independent-variables-an-issue-in-regression</link>
      <description><![CDATA[我不确定这种现象是否有名称 - 因此我将对其进行描述：
描述
我有一个具有以下特征的数据集：

Y：平衡的二元因变量（例如 50% 就业/50% 失业）
X1：平衡的二元自变量（例如 50% 治疗/50% 控制）
X2：分类自变量（例如国家）

X1 是感兴趣的变量，目标是估计 X1 对 Y 的因果影响，而不是预测。
可以观察到 X2 和 X1 以及 X2 和 Y 之间的相关性（或关系，因为变量不是度量）。
问题
但是，如果我将 X1 分组为 X2 （例如按国家划分治疗组和未治疗组），我可以观察到子组不平衡。在一些国家，接近 100% 的数据来自治疗组，在其他国家，接近 100% 的数据来自未治疗组，而在一些国家，数据是 50%/50% 平衡的。
问题：

条件子组不平衡会成为 x1 系数/估计量及其重要性的问题吗？如果一个子组由 100% 控制或治疗单位组成，会发生什么？Peter Floms 的回答暗示这可能是一个分离问题。然而，从我的角度来看，分离是一种被定义为预测因子完美地分裂/预测独立变量的现象。但在这种情况下，条件子组中没有方差，但结果中有方差。

PS：我发现了很多关于不平衡的独立变量或因变量或分离的资料 - 但我在寻找关于“条件独立变量”的资料时遇到了麻烦]]></description>
      <guid>https://stats.stackexchange.com/questions/655709/are-conditional-unbalanced-independent-variables-an-issue-in-regression</guid>
      <pubDate>Sun, 13 Oct 2024 00:15:01 GMT</pubDate>
    </item>
    <item>
      <title>余弦相似度对连续变量的有用性</title>
      <link>https://stats.stackexchange.com/questions/655510/cosine-similarity-usefulness-for-continuous-variables</link>
      <description><![CDATA[有人要求我识别与产品 A 相似的“同类”产品，有人建议我识别一组相关特征并计算每个可能相似的产品与产品 A 之间的余弦相似度。我将使用的特征主要是数字，尽管我也会包含一些离散变量。
这引出了一个关于余弦相似度何时有用的一般问题。例如，如果我有两个向量：
p1 = [10, 20, 40]

p2 = [1000, 2000, 4000]

在这种情况下，cosine_sim(p1, p2) = 1。
但是，如果虽然向量彼此成比例，但每个向量值的幅度差异是有意义的，该怎么办？例如，如果我考虑的特征之一是产品 A 和产品 B 的平均产品增长加速度，那么 10% 和 1000% 的加速度是有显著差异的。对于离散值，我更了解为什么这种度量有意义，但很难理解它对连续变量的用处。
我应该使用不同的相似性度量吗？我的想法正确吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/655510/cosine-similarity-usefulness-for-continuous-variables</guid>
      <pubDate>Wed, 09 Oct 2024 03:36:30 GMT</pubDate>
    </item>
    </channel>
</rss>