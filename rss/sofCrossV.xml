<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Thu, 15 Feb 2024 03:16:14 GMT</lastBuildDate>
    <item>
      <title>为随机森林模型选择合适的样本量</title>
      <link>https://stats.stackexchange.com/questions/639311/choosing-a-suitable-sample-size-for-a-random-forest-model</link>
      <description><![CDATA[我知道这并不总是一个需要直接回答的问题，但我正在研究一个全省范围的湿地分类模型，该模型有 7 个类别和 32 个左右的解释变量。在我的模型中，我的 OA 为 92%，PA 和 UA 介于 90 - 98 之间。在检查模型并每天使用它进行研究后，我发现它相当准确。
我有 2000 个训练/验证点。 20% 是验证。
一旦开始得到我想要的结果，我就停止训练模型并构建我​​的训练和验证集。你觉得我的样本量够大吗，站得住脚吗？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/639311/choosing-a-suitable-sample-size-for-a-random-forest-model</guid>
      <pubDate>Thu, 15 Feb 2024 02:55:15 GMT</pubDate>
    </item>
    <item>
      <title>自回归遇到多元回归？ - 帮助解决措辞和方法</title>
      <link>https://stats.stackexchange.com/questions/639310/autoregession-meets-multiple-regression-help-with-verbiage-and-approach</link>
      <description><![CDATA[需要一些关于我如何接近这个模型的措辞和意见方面的帮助。
我统计了过去 24 个月的人数。

&lt;标题&gt;

月
计数


&lt;正文&gt;

1
100


2
105


...
...


24
200



首先，我将月份反转为：

&lt;标题&gt;

月
计数


&lt;正文&gt;

24
100


23
105


...
...


1
200



我从 24 个月开始创建多个自定义期间...

&lt;标题&gt;

期间
月


&lt;正文&gt;

3
1


3
2


3
3


6
1


...
...


6
6


9
1


9
...


9
9


...
...


24
24



对于每个时期，我计算线性回归并预测下一个值

&lt;标题&gt;

期间
lr_坡度
lr_拦截
lr_r2
预测下一个
期间_权重*
贡献


&lt;正文&gt;

3
4.543
903
.4499
900
1/3*
300


6
44.67
309
.9944
903
1/6*
150.5


9
990.33
33.990
.9494
910
1/9*
101.11


...
...
...
...
980

...


24
776.677
77.09
.0009
990
1/24*
41.5



对于此示例，假设 SUM(period_weight) = 1

然后，我将上述所有贡献相加，得到最终的预测值。

这是一个有效的回归模型吗？它模仿当前模型吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/639310/autoregession-meets-multiple-regression-help-with-verbiage-and-approach</guid>
      <pubDate>Thu, 15 Feb 2024 01:20:40 GMT</pubDate>
    </item>
    <item>
      <title>估计最大效应大小和 Skillings-Mack 检验</title>
      <link>https://stats.stackexchange.com/questions/639309/estimating-maximum-effect-size-and-skillings-mack-test</link>
      <description><![CDATA[我正在处理符合 Skillings-Mack 测试的数据：不平衡不完整的块设计。或者类似弗里德曼测试的缺失数据。
如果测试结果为阴性，我想估计可以“隐藏”测试结果的最大效应大小。数据中。
通过弗里德曼检验，我可以相当容易地估计效果的大小，方法是选取任意两组并根据成对差异（无论是均值、中位数还是截尾均值）引导 95% 置信区间。
但是，当样本中的组大小遵循二项式分布时，即使是最高计数组的大小也非常小，因此自举置信区间非常大，即使 Skillings-Mack 检验非常显着（p 值 &lt; 1e -9)。
具体示例
我想要测量的是其中一个变量的汉明权重（RSA）对测量变量（使用该素数进行 RSA 签名的时间）有影响。问题在于变量（素数）是均匀随机的并且具有特定的位长度（如 1024）。这意味着预期的汉明权重将遵循 p=0.5 且 N=1024 的二项式分布。
在缺乏独立性产生影响之前，我可以执行大约 10 到 30 次测量，因此我的 Skillings-Mack 块不大于 30 次测量。块中的具体权重是独立且随机的。这意味着对于特定的汉明权重（例如 511 和 513），很少有块实际上会同时包含这两个权重，在我看来，引导单个对严重低估了样本中的效应大小（Skillings-Mack 测试可以报告1e-18 的 p 值和成对差异 95% 置信区间仍包含零，符号检验和 Wilcoxon 符号秩检验均为负，p 值 &gt; 0.01）。
问题
如何估计此类数据的效应大小？
据我了解，排名上的重复测量方差分析也可以用于测试此类数据，但如果组的大小不同（我的组非常多），我也看到很少有反对它的建议。那是对的吗？如果没有，有没有办法用 rANOVA 来估计排名的效应大小？
我知道，如果测试结果为阴性，我可以修改特定组中的测量值，然后一次又一次地运行 Skillings-Mack 测试，看看什么时候了解它可以检测到的数据效果是什么变得很重要手，但这对于我期望需要收集的样本量（数亿个测量值）来说是相当昂贵的，更不用说严格指定应该修改哪个组是相当困难的。
背景
由于我正在测量计算机上操作的计算时间，因此我需要能够区分大约 1 毫秒的操作的亚 1 纳秒效应，但我还需要知道样本何时足够大且足够干净检测特定尺寸的效果。]]></description>
      <guid>https://stats.stackexchange.com/questions/639309/estimating-maximum-effect-size-and-skillings-mack-test</guid>
      <pubDate>Thu, 15 Feb 2024 00:48:43 GMT</pubDate>
    </item>
    <item>
      <title>格兰杰因果关系和 FEVD 的输出彼此相反</title>
      <link>https://stats.stackexchange.com/questions/639307/outputs-of-granger-causality-and-fevd-are-opposite-of-each-other</link>
      <description><![CDATA[这是关于双变量系统的协整或 VAR 分析。格兰杰因果关系要么表示工具 1（例如基金）和工具 2（例如指数）不是彼此的格兰杰原因，要​​么是工具 2 是工具 1 的格兰杰原因。
但是，当我看到FEVD输出时，指数90%以上的FEV都归因于基金。无论两种工具之间是否存在协整，这种情况都会发生。换句话说，set1 是协整的，并且估计 ECM，而 set2 不是协整的，并且估计差异中的 VAR。
这意味着 FEVD 和格兰杰因果关系输出显示相反的结果。我该如何解决这个问题？
这些输出已通过 E-Views 和 R 检查。因此，这可能不是由于我在 R 中编写的一些错误代码所致。我正在使用“使用 R 进行集成和协整时间序列分析”作者：Bernhard Pfaff 的 R 代码。


R 代码如下：
comboFEVD1&lt;-tsDyn::fevd(var_model_diff1,n.ahead = 10)
情节（组合FEVD1）

## R 的输出
# 截图已上传

tsVAR &lt;- vars::VAR(ret_df1, p = 2)
vars::causality(tsVAR, Cause = &quot;dindex&quot;)$Granger

## R 的输出
## 格兰杰因果关系 H​​0：dindex 不格兰杰因果 dfund
##
## 数据：VAR 对象 tsVAR
## F 检验 = 7.6993，df1 = 2，df2 = 2980，p 值 = 0.0004622

vars::causality(tsVAR, Cause = &quot;dfund&quot;)$Granger

## R 的输出
## 格兰杰因果关系 H​​0：dfund 不格兰杰因果 dindex
##
## 数据：VAR 对象 tsVAR
## F 检验 = 4.6794，df1 = 2，df2 = 2980，p 值 = 0.009353
]]></description>
      <guid>https://stats.stackexchange.com/questions/639307/outputs-of-granger-causality-and-fevd-are-opposite-of-each-other</guid>
      <pubDate>Thu, 15 Feb 2024 00:38:21 GMT</pubDate>
    </item>
    <item>
      <title>运行回归并且不清楚如何思考问题</title>
      <link>https://stats.stackexchange.com/questions/639303/running-a-regression-and-unclear-how-to-think-about-the-problem</link>
      <description><![CDATA[我想运行线性回归，以更好地理解力如何与分层/扭曲面部框架的位置相关。这是针对与机械工程相关的制造产品。因此，我在 8 个不同位置对 20 个独立单元进行了力测量。因此，我使用单元位置编号作为 x 值（自变量），并将测量的力作为 y 值（因变量）。
我不确定如何在 Excel 中设置回归，因为面部框架上的每个位置都有 20 个单独的条目。但我希望能够查看数据，或许还可以根据 8 个位置中的 6 个位置来预测力的大小（在我的测试中显示出更高的力值）。我该如何设置呢？我将附上几张图片来提供一些背景。
在第一幅图像中，从左到右的测量值分别是最小值、平均值、最大值、标准偏差和范围（后面是 20 帧中每一帧的单独测量值）。第二张图显示了一个范围图，其中我根据位置（x 轴）确定了力测量的范围。

]]></description>
      <guid>https://stats.stackexchange.com/questions/639303/running-a-regression-and-unclear-how-to-think-about-the-problem</guid>
      <pubDate>Wed, 14 Feb 2024 21:57:45 GMT</pubDate>
    </item>
    <item>
      <title>计数数据和比例协变量：最佳实践</title>
      <link>https://stats.stackexchange.com/questions/639301/count-data-and-proportion-covariates-best-practices</link>
      <description><![CDATA[我正在处理空间数据，并且我有以下用于计数数据的对数线性模型。让 $y \sim Poisson(\lambda_{i})$ 使得
$$
\log \lambda_{i} = \text{x}_i^\top\beta_{} + \epsilon_{i}
$$
这样 $\epsilon_{ij}$ 就是站点 $i$ 处的空间效果。
我有每个站点的计数，我想将其作为响应变量进行分析，以及文献中所说的相关的其他辅助计数，并且可能会导致计数上升或下降。我的问题是将数据计数为协变量的最佳实践是什么，我应该使用

原始计数；
比例（我知道每个地点的总人口）；
$z$-分数。
]]></description>
      <guid>https://stats.stackexchange.com/questions/639301/count-data-and-proportion-covariates-best-practices</guid>
      <pubDate>Wed, 14 Feb 2024 21:44:05 GMT</pubDate>
    </item>
    <item>
      <title>如何使用具有成分（加起来为 1 的比例）预测变量的广义线性混合模型？</title>
      <link>https://stats.stackexchange.com/questions/639299/how-to-use-generalised-linear-mixed-model-with-compositional-proportions-that-a</link>
      <description><![CDATA[该研究分析了用户的视线，看看视线是否可以用来预测场景的记忆力。我的目标是观察时间和视觉目标（屏幕、机器人等）之间的交互，并查看用户在早期阶段（具体来说，120 秒中的前 25 秒）的注视分布是否更能体现记忆性比后期用户的注视分布。
以下变量与本研究相关：

已记忆：二进制响应变量，0 表示未记忆，1 表示已记忆
级别：它是一个从 0 -&gt; 0 的分类变量。 23. 这基本上是指时间间隔。 0 表示前 5 秒，23 表示第 23 个 5 秒间隔（即 115 秒-120 秒间隔）
机器人：这是用户看机器人的时间比例
屏幕：这是用户观看屏幕的时间比例
其他：这是用户注视非机器人或屏幕的目标的时间比例。
视觉：这是另一个分类变量，用于标识用户正在查看的场景。由于使用了具有多个参与者的单一场景，因此我将其用作随机效应。

请注意，机器人 + 屏幕 + 其他总和为 1。每个数据点都是 5 秒间隔（用 0 到 23 之间的数字表示），并具有相关的注视分布和可记忆性（响应）。
我有以下广义线性混合模型（广义是因为响应变量是二进制的）：
已记住 ~ 1 + 关卡 + 机器人 + 屏幕 + 关卡：屏幕 + 关卡：机器人 + (1 | 视觉)

我省略其中一个目标（其他）的原因是因为它们依赖于机器人和屏幕。
我的问题是，我可以使用比例作为预测变量吗？如果您对上下文有任何疑问，请随时提问。
我的另一个问题是，如果我有 24 个时间步长（5 秒的时间间隔），那么最好的建模方法是什么来检查初始时间步长是否比后面的时间步长在指示记忆性方面具有更好的效果？我不确定当前的建模是否可用于获取该信息。]]></description>
      <guid>https://stats.stackexchange.com/questions/639299/how-to-use-generalised-linear-mixed-model-with-compositional-proportions-that-a</guid>
      <pubDate>Wed, 14 Feb 2024 21:26:10 GMT</pubDate>
    </item>
    <item>
      <title>每个参与者具有随机条件子集的多级模型</title>
      <link>https://stats.stackexchange.com/questions/639297/multilevel-model-with-random-subset-of-conditions-per-participant</link>
      <description><![CDATA[我对 5 个因素进行了重复测量实验，每个因素有 3 个不同的水平。我正在尝试找到一种方法来减少对参与者的需求，让他们不必经历每一种情况。
我一直在研究部分因子设计，但我突然想到，我可以运行一个为每个参与者部分完成的多级模型。
所以，我的问题是：是否可以向每个参与者展示条件的随机子集（例如 50%），收集足够的参与者，以便每个单独的条件都有良好的代表性，然后使用多级线性模型来建立各因素的影响？我怀疑可能是这样，但我不确定是否有什么我忽略的事情。]]></description>
      <guid>https://stats.stackexchange.com/questions/639297/multilevel-model-with-random-subset-of-conditions-per-participant</guid>
      <pubDate>Wed, 14 Feb 2024 21:02:54 GMT</pubDate>
    </item>
    <item>
      <title>$P(\text{观察到的极端数据} | \text{替代假设})$</title>
      <link>https://stats.stackexchange.com/questions/639287/p-textdata-as-extreme-as-observed-textalternative-hypothesis</link>
      <description><![CDATA[在频率假设检验中，计算值 $P(\text{与观察到的极端数据} | H_0)$，如果它小于特定阈值，$H_0$ 被拒绝。我理解“极端”的概念是“极端”。需要定义，这取决于$H_1$。它可以使用似然比明确定义，但据我在文献（在我的例子中是生物学）中看到的，对 $H_1$ 的依赖通常是隐含的。这就提出了一个问题：人们使用的“正确”是什么？极端的定义？
现在，这听起来可能像是我很迂腐。但我问这个问题的原因是，在我正在分析的数据集上，我定义了一个统计量和极端的直观定义，并使用采样计算了 pvalue $P(\text {观察到的极端数据} | H_0)$。看到这个值很小，我心里就高兴了。由于我的替代假设 $H_1$ 也得到了很好的定义，我可以计算 $P(\text{与观察到的极端数据} | H_1)$ 也是如此，发现这个值也很小。这让我想知道：

我该如何解释这一点？直观上，这让我认为 $H_0$ 和 $H_1$ 都不是好的假设，并且“ “现实” （当然，定义很宽松），是另一回事。我能想到的另一种解释是，我的数据（或至少是我的汇总统计数据）并不能提供有关这些假设的信息。另一种看待它的方式是，我拒绝了复合假设 $H_C:H_0 \lor H_1$（如果这些概率的总和也很小）。
如果我不报告 $P(\text{观察到的极端数据} | H_1)$，而只报告 pvalue，这实际上可能令人信服至少有一些我能够拒绝的人 $H_0$，因为直观地导致测试的替代方案是 $ H_1$，对于那些人来说，这似乎是 $H_1$ 的证据。这告诉我，至少可能有一些论文存在类似的问题。有人研究过这个吗？我的担心有道理吗？如果这确实是一个值得关注的问题，那么补救措施是什么（除了完全放弃 pvalue 的概念并进行贝叶斯测试之外）？是否应该要求人们始终计算和报告 $P(\text{观察到的极端数据} | H_1)$ 以及 pvalue？这样就能解决问题吗？
]]></description>
      <guid>https://stats.stackexchange.com/questions/639287/p-textdata-as-extreme-as-observed-textalternative-hypothesis</guid>
      <pubDate>Wed, 14 Feb 2024 18:15:45 GMT</pubDate>
    </item>
    <item>
      <title>适用于具有重复的非正态、离散数据的假设检验</title>
      <link>https://stats.stackexchange.com/questions/639275/suitable-hypothesis-test-for-non-normal-discrete-data-with-repetitions</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/639275/suitable-hypothesis-test-for-non-normal-discrete-data-with-repetitions</guid>
      <pubDate>Wed, 14 Feb 2024 15:04:52 GMT</pubDate>
    </item>
    <item>
      <title>四个不同城市土壤有毒元素与表观遗传老化的关系</title>
      <link>https://stats.stackexchange.com/questions/639266/association-of-soil-toxic-element-with-epigenetic-aging-in-four-different-cities</link>
      <description><![CDATA[我想做土壤有毒元素和表观遗传年龄加速的线性回归分析；我们使用 ICP-MS 测量了四个不同城市的土壤元素（毫克/千克）。
您能否建议以下方法是否正确，或者我们是否应该仅使用 log2 变换土壤元素；
要将原始元素值划分为分位数以进行回归分析，您可以使用 R 中的 cut 函数。以下示例说明了如何修改脚本以包含原始元素值的分位数：
# 读取数据
df &lt;- read.table(&quot;clean.file2.txt&quot;, header = TRUE, sep = &quot;,&quot;)
 
# 检查列名
列名(df)
 
# 为原始元素值创建分位数
quantile_cols &lt;- c(“Zn”、“Cu”、“Hg”、“Pb”)
分位数_列

             锌 铜 汞 铅
          4.55000 0.73000 2.07500 192.00000
          5.33333 0.23667 1.90667 115.33333
 
for (分位数列中的列) {
  df[[paste0(col, &quot;_quantile&quot;)]] &lt;- cut(df[[col]],
               中断 = 分位数(df[[col]], 概率 = 0:4/4),
               标签=假）
}
 
# 拟合回归模型
model1 &lt;- lm(epigenic_age_acceleration ~ 0 + 城市 + 城市：
  (Zn_分位数 + Cu_分位数 + Hg_分位数 + Mo_分位数 +
  Pb_quantile + Smoking_Status + 性别 + 年龄), 数据 = df)
# 总结模型
摘要_模型1 &lt;- 摘要(模型1)

此脚本向数据框“(Zn_quantile, Cu_quantile, Hg_quantile, Mo_quantile, Pb_quantile)”添加新列，表示每个原始元素值的分位数。然后将这些新列用于回归模型。根据您的具体需求和数据结构调整代码。
]]></description>
      <guid>https://stats.stackexchange.com/questions/639266/association-of-soil-toxic-element-with-epigenetic-aging-in-four-different-cities</guid>
      <pubDate>Wed, 14 Feb 2024 14:14:30 GMT</pubDate>
    </item>
    <item>
      <title>特定对数密度的数值矩</title>
      <link>https://stats.stackexchange.com/questions/639258/numerical-moments-of-a-certain-log-density</link>
      <description><![CDATA[我有以下形式的对数密度：
$$P(\mathbf{x}) \propto \exp\left( - \mathbf{b}^{\top} e^{ \mathbf{x} } - \frac {1}{2}\mathbf{x}^{\top}A\mathbf{x} \right)$$
其中 $A$ 是对称正定矩阵。这里 $\mathbf x, \mathbf b \in\mathbb{R}^N$ 是 $N$&lt; 中的向量/span&gt; 维度，$A$ 为 $N\times N$。 $\mathbf b$ 的组成部分都是非负的。请注意，这里的 $e^\mathbf x$ 表示按分量取幂。
在我想到的特定应用程序中，$N=3$。此外， $\mathbf b$ 的组件可能很大。对于更多上下文， $P(\mathbf x)$ 出现为观察到的泊松计数下对数正态分布率的后验（即多元泊松对数正态分布） ）。
我需要数值计算（我认为没有解析表达式）$\mathbf x$&lt;的第一和第二时刻/span&gt;，即
$$
\langle \mathbf x\rangle, \quad
\langle \mathbf x\mathbf x^\top\rangle, \quad
\operatorname{cov}(\mathbf x), \quad
$$
全部位于 $P(\mathbf x)$ 下。
我尝试过采样（MALA、HMC...），但很难让它们收敛。我也在研究数值积分方法。
被积函数的一些很好的属性是 $\log P(\mathbf x)$ 是 $\mathbf x$，因此有一个最大值，很容易找到（例如使用牛顿优化）。
有什么建议吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/639258/numerical-moments-of-a-certain-log-density</guid>
      <pubDate>Wed, 14 Feb 2024 11:15:54 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用 AR、MA、ARMA 哪种建模方法？</title>
      <link>https://stats.stackexchange.com/questions/639222/what-modeling-approach-should-i-use-ar-ma-arma</link>
      <description><![CDATA[这些是我的时间序列经过两次微分后的 ACF 和 PACF 图。我看了几个教程，但我不知道应该使用什么方法。另外，对情节的解释会很好。
使用 diff() 是唯一不适合我的方法。我还使用了 decompose() 和 tslm()。我并不是想获得最好的模型，而是尝试许多模型并将它们相互比较，同时学习一些东西。


我正在使用的数据位于此处
https://fred.stlouisfed.org/series/CP0960PLM086NEST
我的训练模型（图中是从 2010 年到 2020 年）
感谢您的帮助！]]></description>
      <guid>https://stats.stackexchange.com/questions/639222/what-modeling-approach-should-i-use-ar-ma-arma</guid>
      <pubDate>Tue, 13 Feb 2024 21:37:30 GMT</pubDate>
    </item>
    <item>
      <title>对于高度相关的变量，特征重要性为何以及如何降低？</title>
      <link>https://stats.stackexchange.com/questions/639206/why-and-how-does-feature-importance-decrease-for-highly-correlated-variables</link>
      <description><![CDATA[我正在使用 sklearn 的 DecisionTreeClassifier 算法研究特征之间的相关性及其对其特征重要性的影响之间的关系。
我使用以下函数操纵变量的相关性。这会从数据集中获取现有特征（特征 A），并创建一个相关特征（特征 B），其中除了一些噪声之外，还包含与特征 A 相似的信息：
def create_highly_corlated_column(existing_column,correlation_coefficient,seed=0):
    “”“创建与基数相等的现有特征高度相关的特征。”“”“
    
    np.随机.种子（种子）
    嘈杂的列=现有的列.copy()
    噪声值=噪声列.值计数(归一化=真).索引
    分布=noisy_column.value_counts(normalize=True).tolist()
    嘈杂的列=嘈杂的列.应用（lambda x：np.random.choice（噪声值，p=分布）
                                      if np.random.rand() &gt;= correlation_coefficient else x)

    返回noise_column

因此，在相关性 = 0 时，特征 B 本质上是纯噪声，而在相关性 = 1 时，特征 B 是特征 A 的精确副本。以下是上述函数与两个特征之间的皮尔逊相关系数的相似之处：

然后，我绘制了函数相关系数与特征 A 和特征 B 在多个 RNG 种子上平均的特征重要性的图表。下图展示了特征A（橙色）和特征B（蓝色）的重要性：

随着特征 B 变得高度相关 (~ &gt;0.8)，特征 A 的特征重要性开始大幅下降。我认为原因是因为一些原本由特征A决定的分割现在由特征B决定，所以重要性在某种程度上开始从特征A转移到特征B（当特征多重共线时变得均匀分割）。&lt; /p&gt;
但是，当特征 A 更改时，出现在 0.8 左右的截止范围会有所不同。例如，选择将我的函数应用于不同的特征 A 会生成一个图表，其中仅当相关特征的相关值为 0.99 时才分割特征重要性。
不同的特征在特征重要性开始下降之前是否承认不同程度的相关性？哪些因素决定了这一点？
注意：由于我的绘图结果是对多个种子进行平均，因此特征 B 和特征 A 的特征重要性几乎相同，相关系数接近 1。]]></description>
      <guid>https://stats.stackexchange.com/questions/639206/why-and-how-does-feature-importance-decrease-for-highly-correlated-variables</guid>
      <pubDate>Tue, 13 Feb 2024 17:07:31 GMT</pubDate>
    </item>
    <item>
      <title>InstructGPT RL 阶段优化的损失是多少？</title>
      <link>https://stats.stackexchange.com/questions/638792/whats-the-loss-that-is-optimized-in-instructgpt-rl-stage</link>
      <description><![CDATA[在InstructGPT论文中，他们将强化学习阶段的目标定义为：

他们尝试使用 PPO 来最大化这一目标。
不过，我很难理解他们如何将此目标纳入 PPO。他们是否只是在 PPO 优势计算中替换每个 $r_t$ $\hat{A}_t$通过这个目标的价值？
（这也意味着他们使用 $L^\text{CLIP}$ 而不是 $L^\ text{KLPEN}$ 在最终的损失中，否则 KL 项会有点重复，不是吗？）]]></description>
      <guid>https://stats.stackexchange.com/questions/638792/whats-the-loss-that-is-optimized-in-instructgpt-rl-stage</guid>
      <pubDate>Wed, 07 Feb 2024 20:04:25 GMT</pubDate>
    </item>
    </channel>
</rss>