<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Fri, 24 May 2024 12:26:26 GMT</lastBuildDate>
    <item>
      <title>卡尔曼滤波器“信息滤波器”形成的直观解释</title>
      <link>https://stats.stackexchange.com/questions/647890/intuitive-explanation-of-information-filter-formation-of-kalman-filter</link>
      <description><![CDATA[有人可以直观地解释一下这个“信息过滤器”吗？形成引用自维基百科？特别是我很难理解为什么 $\mathbf{I}_k = \mathbf{H}_k^\textsf{T} \mathbf{R}_k^{-1} \mathbf {H}_k$ 和 $\mathbf{i}_k = \mathbf{H}_k^\textsf{T} \mathbf{R}_k^{-1 \mathbf{z}_k$ 成为我们可以直接添加为“信息”的东西到 $Y$ 矩阵。
&lt;块引用&gt;
如果观测向量 &#39;&#39;&#39;y&#39;&#39;&#39; 的维度为
大于状态空间向量 &#39;&#39;&#39;x&#39;&#39;&#39; 的维度，
信息过滤器可以避免较大矩阵的求逆
卡尔曼增益计算的代价是反转较小的矩阵
预测步骤，从而节省计算时间。在信息中
滤波器，或逆协方差滤波器，估计的协方差和
估计状态被信息矩阵和信息代替
分别向量。它们定义为： :\begin{align}
         \mathbf{Y}_{k \mid k} &amp;= \mathbf{P}_{k \mid k}^{-1} \\ \hat{\mathbf{y}}_{k \mid k} &amp;= \mathbf{P}_{k \mid
 k}^{-1}\hat{\mathbf{x}}_{k \mid k} \end{align}
类似地，预测的协方差和状态具有等效的
信息表单，定义为：\begin{align}
         \mathbf{Y}_{k \mid k-1} &amp;= \mathbf{P}_{k \mid k-1}^{-1} \\ \hat{\mathbf{y}}_{k \mid k-1} &amp;= \mathbf{P}_{k \mid
 k-1}^{-1}\hat{\mathbf{x}}_{k \mid k-1} \end{align}
以及测量协方差和测量向量，它们是
定义为： \begin{align} \mathbf{I}_k &amp;=
 \mathbf{H}_k^\textsf{T} \mathbf{R}_k^{-1} \mathbf{H}_k \\  
 \mathbf{i}_k &amp;= \mathbf{H}_k^\textsf{T} \mathbf{R}_k^{-1} \mathbf{z}_k
 \end{对齐}
信息更新现在变得微不足道\begin{align}
         \mathbf{Y}_{k \mid k} &amp;= \mathbf{Y}_{k \mid k-1} + \mathbf{I}_k \\ \hat{\mathbf{y}}_{k \mid k} &amp;= \hat{\mathbf{y}}_{k
 \mid k-1} + \mathbf{i}_k \end{align}
]]></description>
      <guid>https://stats.stackexchange.com/questions/647890/intuitive-explanation-of-information-filter-formation-of-kalman-filter</guid>
      <pubDate>Fri, 24 May 2024 12:05:56 GMT</pubDate>
    </item>
    <item>
      <title>聚类分析中的变量重要性</title>
      <link>https://stats.stackexchange.com/questions/647887/variable-importance-in-cluster-analysis</link>
      <description><![CDATA[我是聚类分析的新手，阅读了很多东西，但我无法理解如何将变量排序到聚类中。我的意思是，我发现我的数据聚集成 3 个不同的簇，但是我如何理解哪些变量与评估此结果最相关？我使用R并且有很多包来执行聚类分析，但是在函数手册中我没有找到任何与变量重要性相关的内容。有人可以尝试向我解释一下吗？提前致谢]]></description>
      <guid>https://stats.stackexchange.com/questions/647887/variable-importance-in-cluster-analysis</guid>
      <pubDate>Fri, 24 May 2024 11:35:54 GMT</pubDate>
    </item>
    <item>
      <title>他们如何称呼这种每次带有峰值条带的图表？</title>
      <link>https://stats.stackexchange.com/questions/647886/what-do-they-call-such-a-chart-with-a-strip-of-peak-values-per-time</link>
      <description><![CDATA[他们如何称呼下面的图表，该图表的 Y 轴为天数，X 轴为一天中的时间，而颜色代表某些值的级别（例如加载、使用计数等）？
图表如下所示：

我的意思是，如果我想在论坛上询问如何使用特定工具实现一个工具，那么在图像和描述之外添加它的最佳方式是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/647886/what-do-they-call-such-a-chart-with-a-strip-of-peak-values-per-time</guid>
      <pubDate>Fri, 24 May 2024 11:29:36 GMT</pubDate>
    </item>
    <item>
      <title>多个随机变量和似然函数之间的混淆</title>
      <link>https://stats.stackexchange.com/questions/647885/confused-between-multiple-random-variables-and-likelihood-function</link>
      <description><![CDATA[我从根本上对两者感到困惑。问题如下：
我进行观察$x$并创建直方图$\mathbf{n} = (n_1,\ldots ,n_N)$ 与 $N$ 箱一起取出。 bin $i$ 中的元素数量可以建模为泊松随机变量，即 $n_i \sim Pois(\lambda_i) $ 带有参数 $\lambda_i = \mathbb{E}[n_i] = \mu s_i +b_i$ 其中 $s_i$ 和 $b_i$ 是来自分布 $f_s(x;\ mathbf{\theta}_s)$ 和 $f_b(x;\mathbf{\theta}_b)$ 分别。
现在，当我们将此直方图的似然函数 $L(\mu,\theta_s,\theta_b|n_1,\ldots,n_N)$ 写为：
$$
\开始{对齐*}
  L(\mu,\theta_s,\theta_b;\vec{n}) &amp;= f(n_1|\lambda_1) \cdots f(n_N|\lambda_N) \\
&amp;= \prod_{i=1}^{N} f(n_i|\lambda_i)\\
&amp;= \prod_{i=1}^{N} \frac{\lambda_i^{n_i}}{n_i!}e^{-\lambda_i} \\
&amp;= \prod_{i=1}^{N} \frac{(\mu s_i + b_i)^{n_i}}{n_i!}e^{-(\mu s_i + b_i)}
\结束{对齐*}
$$
这是否意味着我的数据是 $N$ 维的（很可能不是，但我仍然想问）还是我的情况是计算 $N$ i.i.d 样本的可能性？]]></description>
      <guid>https://stats.stackexchange.com/questions/647885/confused-between-multiple-random-variables-and-likelihood-function</guid>
      <pubDate>Fri, 24 May 2024 11:06:29 GMT</pubDate>
    </item>
    <item>
      <title>学生化残差的计算[关闭]</title>
      <link>https://stats.stackexchange.com/questions/647883/calculation-of-studentised-residuals</link>
      <description><![CDATA[我正在尝试用 Python 编写一个函数，以在 R 中重新创建 rstudent 函数并计算 OLS 模型的（外部）学生化残差。
我尝试从此维基百科页面复制计算。
这是我的功能：
def Student_resid_ols(模型:RegressionResults) -&gt; np.ndarray：
    resid_squared: np.ndarray = np.power(model.resid, 2)
    var_robust: np.ndarray = (resid_squared.sum() - resid_squared) / model.df_resid
    sigma_robust: np.ndarray = np.sqrt(var_robust)
    h: np.ndarray = lever_ols(模型=模型)

    分母：np.ndarray = sigma_robust * np.sqrt(1 - h)
    Studentized_residuals: np.ndarray = np.divide(model.resid, 分母)
    返回studentized_residuals

不幸的是，我的函数与 R 中的函数不匹配。
我已经确定差异在于我对 var_robust 的计算。
据我所知，我的实现与维基百科上的相同。有人有想法吗？谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/647883/calculation-of-studentised-residuals</guid>
      <pubDate>Fri, 24 May 2024 10:51:56 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 Conv2D 对时空数据（非图像）进行预测？</title>
      <link>https://stats.stackexchange.com/questions/647882/how-to-use-conv2d-for-make-predictions-on-spatio-temporal-data-non-image</link>
      <description><![CDATA[我有多元时间序列数据，由 4 个自变量、1 个因变量（目标变量）和空间数据（纬度和经度）组成。数据取自 5 个不同的城市，因此有 5 对纬度和经度（每对代表每个城市）。我读过一些关于 Conv2D 可以应用于空间情况的参考文献。因此，我想使用Conv2D进行空间分析，但我的数据不是图像（我读过Conv2D常用图像数据作为输入）。然后，使用 LSTM 执行时间分析（我想使用这两种方法来执行空间（使用 Conv2D）和时间（LSTM）分析）。
我的问题是：

我可以使用 Conv 2D 对我的数据执行空间分析吗？ （数据存在空间异质性，即每个城市与其他城市有不同的特征）
如果我可以使用 Conv2D，合适的输入形状是什么？ （我当前的数据形状是（样本、时间步长、特征、位置），即 (500, 24, 4, 5)）

我使用的：

Python
Keras 与张量流

在这方面的任何帮助都将受到高度赞赏。谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/647882/how-to-use-conv2d-for-make-predictions-on-spatio-temporal-data-non-image</guid>
      <pubDate>Fri, 24 May 2024 10:45:45 GMT</pubDate>
    </item>
    <item>
      <title>在文档检索的背景下，什么是基本事实？</title>
      <link>https://stats.stackexchange.com/questions/647878/in-the-context-of-document-retrieval-what-serves-as-the-ground-truth</link>
      <description><![CDATA[在我用于文档检索的基准数据集的上下文中，样本通常由查询及其相应的正面和负面段落组成。正向段落的标签为 1，负向段落的标签为 0。
我当前的设置遵循以下过程：

创建所有段落的 FAISS 索引（正面 + 负面）。
循环遍历每个查询并从上述语料库中检索 top-$k$ 文档。
获取每个检索到的文档的标签（0 或 1）。
计算 nDCG。

我感到困惑的是基本事实应该是什么。上面检索到的二进制标签数组将是预测。
我正在使用 scikit-learn 的 nDCG 评分函数，但我不知道 y_true 的输入应该是什么。]]></description>
      <guid>https://stats.stackexchange.com/questions/647878/in-the-context-of-document-retrieval-what-serves-as-the-ground-truth</guid>
      <pubDate>Fri, 24 May 2024 09:51:02 GMT</pubDate>
    </item>
    <item>
      <title>当应用实体和时间固定效应时，数据会发生什么？</title>
      <link>https://stats.stackexchange.com/questions/647876/what-happens-to-the-data-when-entity-and-time-fixed-effects-are-applied</link>
      <description><![CDATA[假设真实的数据生成过程如下：
Y_it = beta_0 + beta_1 X_it + u_it

当我同时应用实体固定效果和时间固定效果时会发生什么？
(Y_it - Y_i - Y_t) = beta_0 + beta_1 (X_it - X_i - X_t) + (u_it - u_i - u_t)

其中 Y_i = 实体 i 的所有 Y 值的平均值，Y_t = 时间 t 内实体的所有 Y 值的平均值。
这真的是数据发生的情况吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/647876/what-happens-to-the-data-when-entity-and-time-fixed-effects-are-applied</guid>
      <pubDate>Fri, 24 May 2024 09:34:37 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归中的假设</title>
      <link>https://stats.stackexchange.com/questions/647875/assumption-in-multiple-linear-regression</link>
      <description><![CDATA[多元线性回归的原理已被广泛描述，但仍有一些方面我并不真正理解其中的原因。具体来说，我不明白为什么异方差性阻碍了运行线性回归模型的可能性？
为什么不均匀的分散会阻碍估计的可能性？]]></description>
      <guid>https://stats.stackexchange.com/questions/647875/assumption-in-multiple-linear-regression</guid>
      <pubDate>Fri, 24 May 2024 09:24:30 GMT</pubDate>
    </item>
    <item>
      <title>REML 解决方案与 BLUP 解决方案</title>
      <link>https://stats.stackexchange.com/questions/647874/reml-solutions-vs-blup-solutions</link>
      <description><![CDATA[当对随机效应的解决方案感兴趣时，常见的过程是使用 REML 来估计方差分量，然后使用 BLUP 来预测 RE。然而，REML 是一种迭代方法（例如期望最大化算法），这意味着随机 RE 的解是作为副产品获得的，它们也称为 BLUP。那么，为什么不使用 REML 最后一次迭代的解决方案呢？如果输入（方差）来自 REML，为什么它们会与从 BLUP 程序获得的解不同？这是用于反转 MME 左侧的不同算法的问题吗？或者这只是一个实际方便的问题，意味着 REML 需要很长时间才能收敛，然后可以重复使用估计的方差来进行相对快速的 BLUP 评估？]]></description>
      <guid>https://stats.stackexchange.com/questions/647874/reml-solutions-vs-blup-solutions</guid>
      <pubDate>Fri, 24 May 2024 09:12:18 GMT</pubDate>
    </item>
    <item>
      <title>固定效应模型“内部”“个人”添加时间虚拟人</title>
      <link>https://stats.stackexchange.com/questions/647873/fixed-effects-model-within-individual-adding-time-dummies</link>
      <description><![CDATA[目前我正在撰写有关自然资源依赖对儿童健康影响的论文。
我有一个包含 2002 年至 2020 年 117 个国家/地区的不平衡面板数据集。我使用所提供问题下的代码。
当我进行双向固定效应模型时，控制时间和国家/地区固定效应，我的 R2 变得非常低 (0.115)。然而，在这个例子中，我感兴趣的自变量 (NRR3) 处于预测方向并且显着。然而，我的一些控制变量并未达到预期的方向。在这种情况下，我的稳健性检查之一（对于回答我的研究问题很重要）是完美的。
当我进行个人固定效应和国家固定效应时，每个变量都处于预期方向，但我的自变量不再显着。我的 R2 是 0.550。
我预计这些年来将会发生重大变化（因为例如可持续发展目标的首要任务是降低死亡率）。我在我的国家固定效应模型中包含了时间虚拟变量，我的 r2 现在为 0.8，但它给出了与双向模型完全相同的系数和标准误差。我试图理解这一点，并试图找到实证文献来证实这种思维方式，但不幸的是我暂时找不到。
与双向固定效应模型相比，在国家固定效应模型中包含时间虚拟变量有何作用？这是可以通过实证文献证实的吗？
还有一个小问题：在进行固定效应模型后，我发现我的数据中仍然存在异方差和自相关，我现在正在做 HAC 标准错误，这是一件好事吗？我还需要对标准错误进行聚类吗？下面提供了代码。
包括两个滞后
###固定效应模型：主要分析
FEMA2 &lt;- plm(公式 = MU52 ~ NRR3_lag2 + MVAA3 + REER2 + CEPI3 + GE2 + COC2 + EYS + PM2.52 + TE2 + CHE2, 数据 = MD2, 索引 = c(“州”, “年份”) ，模型=“内部”，效果=“个体”）
观星者（FEMA2，类型=“文本”）
#HAC 标准错误 (FE)
vcov_hac_FEMA2 &lt;- vcovSCC(FEMA2, type = &quot;HC1&quot;, maxlag =7)
stargazer(FEMA2, type = “text”, se = list(sqrt(diag(vcov_hac_FEMA2))), title = “带有 HAC 标准误差的主要分析 (FE)（2 滞后）”,align = TRUE)]]></description>
      <guid>https://stats.stackexchange.com/questions/647873/fixed-effects-model-within-individual-adding-time-dummies</guid>
      <pubDate>Fri, 24 May 2024 07:53:43 GMT</pubDate>
    </item>
    <item>
      <title>线性判别分析降维中类内协方差的含义</title>
      <link>https://stats.stackexchange.com/questions/647871/meaning-of-within-class-covariance-in-linear-discriminant-analysis-dimensionalit</link>
      <description><![CDATA[在 Hastie、Tibshirani 和 Friedman 的统计学习的要素的第 4.3.3 节中，作者列出了减少输入矩阵维度的过程  $\mathbf{X}$，首先使用线性判别分析，然后使用主成分分析：

计算类质心的 $K\times p$ 矩阵 $\mathbf{M}$和公共协方差矩阵 $\mathbf{W}$ （类内协方差）；
使用谱分解计算 $\mathbf{M}^* = \mathbf{MW}^{-\frac{1}{2}}$ $\mathbf{W}$;
计算 $\mathbf{B}^*$，即 $\mathbf{M}^* 的协方差矩阵$ 及其谱分解 $\mathbf{B}^* = \mathbf{V}^*\mathbf{D}_B{\mathbf{V}^*} ^T$。

这给出了第 l 个判别变量 $Z_l = v_l^TX$ 以及相应的判别方向 $v_l=\mathbf{W}^{-\frac{1}{2}}v_l^*$。
我对该过程的理解是 $\mathbf{X}$ 首先投影到质心创建的子空间上（这不会影响 LDA 结果），对子空间进行主成分分析，进一步缩小其维度。我对我的理解如何与上面的矩阵运算保持一致感到困惑，特别是 $\mathbf{W}$ 的效果。]]></description>
      <guid>https://stats.stackexchange.com/questions/647871/meaning-of-within-class-covariance-in-linear-discriminant-analysis-dimensionalit</guid>
      <pubDate>Fri, 24 May 2024 06:58:28 GMT</pubDate>
    </item>
    <item>
      <title>不同版本的加法切尔诺夫</title>
      <link>https://stats.stackexchange.com/questions/647860/different-versions-of-additive-chernoff</link>
      <description><![CDATA[加法切尔诺夫界限表示 $X_i \in \{0,1\}$ 满足 $\mathbb {E}[X_i] = p,$
$$
\mathbb P\left(\sum\limits_{i}^nX_i \geq np+n\epsilon \right) \leq \exp\left(-\frac{(n\epsilon)^2}{2(np+\frac) {n\epsilon}{3})}\right) .$$
这个不平等是我的老师给出的。
我不明白切尔诺夫是什么样的添加剂？我在维基百科上没有看到任何地方。
我只知道这个，
附加版本说
$$
\mathbb P\left(\sum\limits_{i}^nX_i \geq np+n\epsilon \right) \leq e^{-2n\epsilon^2} 。
$$
任何人都可以帮助我如何获得我的顾问切尔诺夫版本。]]></description>
      <guid>https://stats.stackexchange.com/questions/647860/different-versions-of-additive-chernoff</guid>
      <pubDate>Fri, 24 May 2024 02:43:58 GMT</pubDate>
    </item>
    <item>
      <title>用于对两个同心圆对应的数据点进行分类的非线性内核[关闭]</title>
      <link>https://stats.stackexchange.com/questions/647830/non-linear-kernel-for-classifying-data-points-corresponding-to-two-concentric-ci</link>
      <description><![CDATA[在自学时看过关于非线性可分离问题的文章，这里&lt; /a&gt;.给出的图像位于此处和此处。 sstatic.net/f5tu2tO6.jpg&quot; rel=&quot;nofollow noreferrer&quot;&gt;此处。
它解决了一个常见的教科书问题，其中数据点位于两个不相交的同心圆组中。
给出的方法是将核函数设置为：$φ(x) = φ((x_1, x_2)) = (x_1^2, √2 x_1x_2, x_2^ 2).$
但是，如果举个例子，内圆的 radius 直径为 $2$ 单位，而外圆的直径为  radius $4$ 单位的直径；然后让我们在内圆上有两个点： $(2,2), (2,4)$;分别用 $x_{i1}$ 和 $x_{i2}$ 表示。
同样，让外圆上有两个数据点： $x_{o1}= (2,1),$ 和 $x_{o2}=(2,5).$
核函数的应用，似乎并没有用线性SVC将两个同心圆上的点分开，如新的坐标所示，以及Norms，如下所示：

$φ(x_{o1})=(2,1) = (4, 2\sqrt{2}, 1),$ 范数 $||φ(x_{o1})||=16+1+8 = 25,$
$φ(x_{i1})=(2,2) = (4, 4\sqrt{2}, 4),$ 范数 $||φ(x_{i1})||=64,$
$φ(x_{i2})=(2,4) = (4, 8\sqrt{2}, 16),$ 范数 $||φ(x_{i2})||=16+256+128 = 400,$
$φ(x_{o2})=(2,5) = (4, 10\sqrt{2}, 25),$ 范数 $||φ(x_{o2})||=481.$

目前尚不清楚给定的核函数如何将这些点转换/映射为两个线性可分离的类。]]></description>
      <guid>https://stats.stackexchange.com/questions/647830/non-linear-kernel-for-classifying-data-points-corresponding-to-two-concentric-ci</guid>
      <pubDate>Thu, 23 May 2024 14:50:12 GMT</pubDate>
    </item>
    <item>
      <title>statsmodels：吸收 3 个以上固定效应时更新 OLS 的自由度</title>
      <link>https://stats.stackexchange.com/questions/647824/statsmodels-update-ols-degrees-of-freedom-when-absorbing-3-fixed-effects</link>
      <description><![CDATA[我想运行具有 3 个以上固定效应的 OLS 回归。
（这是否是一个好主意超出了本问题的范围）。
我可以使用 Stata 来完成：
ssc 安装 reghdfe
网络使用nlswork
reghdfe ln_wage 工时 任期，吸收（年龄 工会 种族） vce（集群 ind_code）

现在，我想在 Python 中复制它。我已经在本地下载了示例数据集，并且可以简单地实现它通过为每个固定效应创建虚拟对象，结果与 Stata 类似：
将 pandas 导入为 pd
将 statsmodels.formula.api 导入为 sm
将 numpy 导入为 np

df = pd.read_stata(“路径/到/nlswork.dta”)
数据集 = df.dropna(
    子集=[
        “工资”，
        “小时”，
        “任期”，
        “年龄”，
        “联盟”，
        “种族”，
        “ind_code”，
    ],
    如何=“任何”，
）

模型 = sm.ols(
    公式=“ln_wage ~ 工时 + 任期 + C(年龄) + C(工会) + C(种族)”,
    数据=数据集，
）
结果=模型.fit(
    cov_type=“簇”，
    cov_kwds={“组”: np.array(dataset[“ind_code”])},
    use_t=真，
）

打印（结果.summary（））

但是，这对于高维固定效应是不可行的（例如，尝试用 idcode 替换 race，模型需要一分钟多的时间才能适应）。
我知道我可以预先吸收固定效应，并对残差运行 OLS。我知道我需要调整模型的自由度以考虑吸收的自由度。
将 pandas 导入为 pd
将 statsmodels.formula.api 导入为 sm
将 numpy 导入为 np
导入pyhdfe

df = pd.read_stata(“路径/到/nlswork.dta”)
数据集 = df.dropna(
    子集=[
        “工资”，
        “小时”，
        “任期”，
        “年龄”，
        “联盟”，
        “ID代码”，
        “ind_code”，
    ],
    如何=“任何”，
）

# 编码种族
pd.options.mode.copy_on_write = True
数据集[“race_id”] = 数据集[“race”].astype(“category”).cat.codes

columns_of_interest = [“ln_wage”、“小时”、“tenure”、“race_id”、“ind_code”]

算法= pyhdfe.create(
    数据集[[“年龄”,“联盟”,“race_id”]],
    drop_singletons=真，
）

demeaned_dataset = pd.DataFrame(
    data=algorithm.residualize(数据集[感兴趣的列]),
    列=感兴趣的列
）
模型 = sm.ols(
    公式=“ln_工资~工时+任期”，
    数据=demeaned_dataset，
）

# 调整自由度
model.df_model += 算法.度
model.df_resid -= 算法.度

结果=模型.fit(
    cov_type=“簇”，
    cov_kwds={“组”: np.array(demeaned_dataset[“ind_code”])},
    use_t=真，
）

打印（结果.summary（））

虽然系数是正确的，但 t 统计数据是关闭的。我错过了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/647824/statsmodels-update-ols-degrees-of-freedom-when-absorbing-3-fixed-effects</guid>
      <pubDate>Thu, 23 May 2024 13:16:17 GMT</pubDate>
    </item>
    </channel>
</rss>