<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Wed, 08 Jan 2025 18:23:25 GMT</lastBuildDate>
    <item>
      <title>FE 解释中的相互作用项</title>
      <link>https://stats.stackexchange.com/questions/659728/interaction-term-in-fe-interpretation</link>
      <description><![CDATA[我正在估算固定效应回归，如下所示：
unemployment_ict ~ b1 wage_ict + b2 heat_ict + b3 wage * heat_ict + FEc + error_ict
其中 i 为个人，c 为国家，t 为时间。
假设您得到以下效应：b1=0.7，b2=1.2，b3=-0.4
现在您想要解释交互效应。一种思考方式是边际效应：
这里，您已估计出给定工资和热量的失业预期值
E[y|a,b]=0.7a+1.2b-0.4ab
如果对上述表达式求 a 的导数：
∂E[y|a,b]∂a=0.7-0.4b
此表达式在 b（热量）中递减。然后，您可以将 b 设置为“有趣”的值以进行解释。通常，这些有趣的值是 b 中样本的平均值。
我想知道这在具有固定效应的应用中是否有意义。国家平均值已从固定效应中移除。在热量水平上评估 b 是否有意义。或者应该以 b 的平均增长率或标准差来评估，而不是以水平来评估？
如果有人能对此发表评论，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/659728/interaction-term-in-fe-interpretation</guid>
      <pubDate>Wed, 08 Jan 2025 17:35:13 GMT</pubDate>
    </item>
    <item>
      <title>右删失计数数据的等价性检验</title>
      <link>https://stats.stackexchange.com/questions/659725/equivalence-test-for-right-censored-count-data</link>
      <description><![CDATA[如何对右删失计数数据进行等效性检验？感兴趣的结果是某个时间段内的总癫痫发作次数。但是，用于记录癫痫发作次数的设备在 40 次时停止计数。这是一个硬性限制。因此，需要进行审查。审查的是计数，而不是记录时间——为了清楚起见，范围是 0 到 40+。该设备设置为一次记录几天。每日计数不可用。更复杂的是，有一个“故障”，因此总记录时间可能有所不同。对于某些受试者，记录时间为 168 小时。对于其他受试者，记录时间为 175 小时。我会在更普通的建模中使用这些时间作为偏移量。
因此，我有带偏移量的右删失计数数据。我想进行等效性检验。我应该从哪里开始呢？
这不是我的设计，我也没有记录数据或处理设备。]]></description>
      <guid>https://stats.stackexchange.com/questions/659725/equivalence-test-for-right-censored-count-data</guid>
      <pubDate>Wed, 08 Jan 2025 16:42:35 GMT</pubDate>
    </item>
    <item>
      <title>如何解释两个时间协变量中的时间自相关性？</title>
      <link>https://stats.stackexchange.com/questions/659724/how-to-account-for-temporal-auto-correlation-in-2-time-covariates</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/659724/how-to-account-for-temporal-auto-correlation-in-2-time-covariates</guid>
      <pubDate>Wed, 08 Jan 2025 16:41:35 GMT</pubDate>
    </item>
    <item>
      <title>当得到自由度的十进制值时该怎么办？</title>
      <link>https://stats.stackexchange.com/questions/659723/what-to-do-when-one-gets-a-decimal-value-as-degrees-of-freedom</link>
      <description><![CDATA[我正在尝试找到 2 个随机变量的样本均值差异的置信区间的 t 值。这里的方差是未知且不相等的，因此必须使用的 v 公式是：

完成此计算后，我确信它是正确的，我得到的是一个十进制值。在 t 分布表中，我只能看到 v 的整数值，所以我的问题是我应该向上舍入还是向下舍入。此外，这有什么逻辑吗，还是向上舍入或向下舍入完全是随机的？]]></description>
      <guid>https://stats.stackexchange.com/questions/659723/what-to-do-when-one-gets-a-decimal-value-as-degrees-of-freedom</guid>
      <pubDate>Wed, 08 Jan 2025 15:58:56 GMT</pubDate>
    </item>
    <item>
      <title>从其他测量中更新不可观测概率密度函数估计的技术</title>
      <link>https://stats.stackexchange.com/questions/659721/techniques-to-update-unobservable-probability-density-function-estimate-from-oth</link>
      <description><![CDATA[假设我有一个由许多模型组成的系统，这些模型最初被估计为 PDF。举一个简单的例子，假设我有 A、B、C 和 D 的模型。
这些模型相互关联，使得系统 S = Sum(A, B, C, D)。
如果我能够直接测量 S，并直接测量底层模型的 0-N，那么可以使用哪些技术来生成每个底层模型的后验估计？]]></description>
      <guid>https://stats.stackexchange.com/questions/659721/techniques-to-update-unobservable-probability-density-function-estimate-from-oth</guid>
      <pubDate>Wed, 08 Jan 2025 15:25:24 GMT</pubDate>
    </item>
    <item>
      <title>预测性维护的时间序列分类与回归</title>
      <link>https://stats.stackexchange.com/questions/659720/time-series-classification-vs-regression-for-predictive-maintenance</link>
      <description><![CDATA[假设我们有一个由传感器数据组成的时间序列数据集，每个时间戳都有一个目标列，指示特定组件是否会在一定天数内发生故障；具体来说，如果我们知道故障将发生在时间戳 X，那么对于 X 之前 Y 天内的每个时间戳，目标列标记为 1，否则标记为 0。这通常用于预测性维护论文中，以预测组件的健康状况。
基本上，我们要做的是一个二元分类。现在，我认为我遗漏了有关时间序列分类与时间序列预测的一些内容。我经常看到这样的代码来生成序列以训练模型：
def create_sequences(features, target, seq_length):
sequences = []
labels = []

for i in range(len(features) - seq_length):
sequences.append(features[i:i + seq_length])
labels.append(targets[i + seq_length])

return np.array(sequences), np.array(labels)

因此，我们获取一系列过去的数据，并尝试预测该序列的一个未来值（最终可能是未来目标的一个窗口）：但是，这是为了时间序列预测而做的，不是吗？
如果我们要将代码用于时间序列分类，则必须采用标签
targets[i:i + seq_length]

因此，与特征的方法相同，以便将每个序列与其自己的标签关联并执行分类。
所以我的问题是：

时间序列分类和预测之间的序列创建方式是否不同？
如果第一个问题的答案是肯定的，那么我们将使用targets[i:i + seq_length]，我们究竟如何预测某个组件将来会出现故障？为了更好地阐述第二个问题，考虑到我们在训练期间没有考虑未来的值，我们究竟如何才能提前检测到失败？

此外，如果我尝试在大型数据集（50 多个特征，经过预处理和规范化）上训练 DL 模型（例如 CNN），并将目标移位为给定序列的下一个值，我无法让模型表现得更好，而如果我不移动目标值，我会获得相当不错的性能，所以我不明白这背后的原因。]]></description>
      <guid>https://stats.stackexchange.com/questions/659720/time-series-classification-vs-regression-for-predictive-maintenance</guid>
      <pubDate>Wed, 08 Jan 2025 15:11:13 GMT</pubDate>
    </item>
    <item>
      <title>从文档中提取文本的最佳途径[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659718/best-route-for-text-extraction-from-documents</link>
      <description><![CDATA[在决定采取哪种方式完成这项任务之前，我试图收集尽可能多的信息。
我需要从发票文件中提取一些标准信息，例如发票号、产品名称和说明、发票日期等。我有大约 20,000 张发票，它们来自多个供应商（大约 200 个供应商），因此结构不同。当然，这意味着我所追求的数据位于这些发票的不同位置。
什么是提取高精度数据的最有效方法？：
a. 使用 LayoutLMv3 模型在我的自定义数据上进行训练。这首先涉及以适当的格式创建自定义数据，即使用 Tesseract 或类似库对发票图像进行 OCR，并使用生成的数据集作为 LayoutLMv3 的训练数据。问题是，即使我拥有的发票质量相当好且可读，Tesseract 仍然很难识别某些文本/符号/标点符号/空格。这进一步促使我深入研究对 Tesseract 模型本身进行微调，方法是从原始发票上创建带有文本的小块图像，并使用写在这些小图像上的实际文本（基本事实）手动注释它们。这是一项繁重的工作，需要大量的手动工作。对于 200 个供应商，这可能意味着我需要注释数千张小图像。
b.直接训练图像到文本模型，如 pic2struct，希望它自己就能产生足够准确的结果。我不确定哪种 ML 算法最适合在这里使用，CNN 吗？它会和上面的方法一样准确吗？
或者也许有更好的方法来完成这项任务？]]></description>
      <guid>https://stats.stackexchange.com/questions/659718/best-route-for-text-extraction-from-documents</guid>
      <pubDate>Wed, 08 Jan 2025 13:30:35 GMT</pubDate>
    </item>
    <item>
      <title>训练启发式函数对堆栈进行排序[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659717/train-an-heuristic-function-to-sort-stacks</link>
      <description><![CDATA[我目前正在研究一种 IDA* 算法，用于对带有临时第二个堆栈的堆栈进行排序，该算法正在寻找对第一个堆栈进行排序所需的最少指令数，这些指令包括交换、推送、旋转和反向旋转，我的问题是启发式函数，由于指数复杂度，该函数使该算法可用于堆栈中超过 7 或 8 个数字。我需要找到一种方法来了解实际堆栈状态和排序状态之间的“距离”。我尝试分析数据以找到逻辑，我也尝试训练 RNN，但没有成功，我使用的数据是使用简单搜索算法生成的解决方案，但我无法为超过 9 个数字的堆栈生成解决方案
因此，如果有人有此问题的解决方案或有关如何解决此问题的想法，欢迎提供任何帮助]]></description>
      <guid>https://stats.stackexchange.com/questions/659717/train-an-heuristic-function-to-sort-stacks</guid>
      <pubDate>Wed, 08 Jan 2025 13:10:19 GMT</pubDate>
    </item>
    <item>
      <title>将部分 eta 平方转换为 Cohen's d 并计算方差</title>
      <link>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</link>
      <description><![CDATA[我正在 R 中进行元分析。对于大多数研究，我使用组的均值和 SD 计算 Hedges 的 g。但是，对于某些研究，我只有部分 $\eta ^2$ 和 95% 置信区间。
例如，
部分 $\eta ^2 = 0.007$
95% CI= 0.08-0

我知道如何将部分 $\eta ^2$ 转换为 Cohen 的 d，然后将其转换为 Hedges 的 g。但是，我该如何计算 Hedges g 的方差呢？

对于某些研究，我只有部分 $\eta ^ 2$，没有置信区间。有没有办法根据这些信息计算出 Hedges g 的方差呢？

]]></description>
      <guid>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</guid>
      <pubDate>Wed, 08 Jan 2025 11:19:07 GMT</pubDate>
    </item>
    <item>
      <title>基线后混杂因素与并发事件之间的差异（来自估计框架）</title>
      <link>https://stats.stackexchange.com/questions/659711/difference-between-post-baseline-confounders-and-intercurrent-events-from-estim</link>
      <description><![CDATA[与估计框架相比，基线后混杂因素和并发事件有何不同？具体来说，以急救药物为例，它如何既被归类为基线后混杂因素，又被归类为并发事件，两者之间又有何区别？]]></description>
      <guid>https://stats.stackexchange.com/questions/659711/difference-between-post-baseline-confounders-and-intercurrent-events-from-estim</guid>
      <pubDate>Wed, 08 Jan 2025 11:03:54 GMT</pubDate>
    </item>
    <item>
      <title>使用非独立观测进行零假设显著性检验</title>
      <link>https://stats.stackexchange.com/questions/659709/null-hypothesis-significance-testing-with-non-independent-observations</link>
      <description><![CDATA[我正在处理一个数据集，其中的测量值被我们划分为不同的类别。主要目标是确定每个类别中的测量值是否与背景（定义为其他类别中所有数据点的集合）有显著差异。
但是，测量值违反了独立性假设。具体来说，测量值可能来自同一物体的不同区域（在本例中，是同一蛋白质内的不同位点）。虽然单个蛋白质之间存在跨越几个数量级的相当大的变异性，但很明显，如果测量值完全独立，分布并不是我们所期望的。虽然我相信我们使用常规统计测试获得的至少一些结果具有生物学相关性，但我还预计获得的 p 值会被夸大。
因此，问题是：什么才是解释数据中这种结构的合适方法？一种粗略而简单的方法可能是在蛋白质水平上“聚合”，并选择测量值在前 X% 和后 X% 范围内的蛋白质进行进一步分析。然而，由于上述单个蛋白质内部的巨大差异，这两组表现出相当大的重叠，使解释变得复杂。我也知道 bootstrap 方法可以解决涉及非 iid 数据的情况，但我不确定这种方法是否适合这种特定情况。]]></description>
      <guid>https://stats.stackexchange.com/questions/659709/null-hypothesis-significance-testing-with-non-independent-observations</guid>
      <pubDate>Wed, 08 Jan 2025 10:31:15 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验与置信区间有何关系？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for-in-relation-to-confidence-intervals</link>
      <description><![CDATA[我知道，当方差未知时，学生 t 分布用于查找样本均值的置信区间，但卡方检验何时使用？据我记得，当方差未知时也使用它，并且必须将其近似为 s 而不是 σ，因此，何时必须使用卡方分布，何时必须使用学生 t 分布。使用它的必要条件是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for-in-relation-to-confidence-intervals</guid>
      <pubDate>Wed, 08 Jan 2025 09:56:10 GMT</pubDate>
    </item>
    <item>
      <title>分子转化的逻辑回归建模</title>
      <link>https://stats.stackexchange.com/questions/659688/logistic-regression-modeling-of-molecule-conversion</link>
      <description><![CDATA[一个最近的问题涉及一种回归分析，其中一种化学物质用具有特定浓度的混合物处理，以查看转化为新物质的化学物质的比例。
回归分析使用浓度作为唯一特征，比例作为结果，而 OP 选择将比例强行塞入逻辑回归的结果变量中。
但是，假设我们知道分子的起始数量。然后，通过了解比例，我们就知道转化的分子数量。然后，我们可以使用更自然的逻辑回归，其中每个转化/非转化都是一个单独的结果，对应于浓度的特征值，例如$x = (0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6)$，$y = (0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1)$。
但是，每个浓度水平不会有四个分子。会有数不清的分子。从计算的角度来看，这让我很担心，但从统计上讲，这也感觉像是作弊。我们一开始有三个双变量观测值：$x = (0.2, 0.4, 0.6)$，$y = (0.25, 0.5, 0.75)$。我们最终得到了无数个观测值。
但是，如果我们在三个浓度水平上有不同数量的分子，我可以看到这会给反应更多的反应混合物更高的权重，例如如果我们对一升原始分子进行一次反应，对一毫升进行一次反应。我认为在高浓度物质有恶臭，并且其使用量有限，无法使用一升的大反应混合物的情况下，这是一种合理的实验做法。
如果这样做会增加样本量并降低标准误差，那么这样做是否是作弊？那么，每个浓度水平的分子数量可能不同吗？
（从化学物理学的角度来看，我担心这些转化不太可能是独立事件。答案可以但不必解决这个问题。）]]></description>
      <guid>https://stats.stackexchange.com/questions/659688/logistic-regression-modeling-of-molecule-conversion</guid>
      <pubDate>Tue, 07 Jan 2025 21:37:38 GMT</pubDate>
    </item>
    <item>
      <title>线性回归 - 响应变量为百分比改善或 m/s？</title>
      <link>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</link>
      <description><![CDATA[我正在尝试对包含 8 种不同跑步距离的数据集进行统计，这些距离在遵循训练方案之前和之后都有时间，并且基于距离对改进进行线性回归（所有完成时间都有所下降）。我不确定是否要转换为百分比改进或使用 m/s 之类的变量，然后减去跑步时间 1 和跑步时间 2，以便能够比较不同的距离组。显然，绝对时间差异并不大，因为更长的距离自然会有更大的改进。但我读到过，不建议将百分比改进转换为线性回归中的响应变量。我该怎么做？]]></description>
      <guid>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</guid>
      <pubDate>Tue, 07 Jan 2025 21:03:50 GMT</pubDate>
    </item>
    <item>
      <title>在 statsmodels 中 GLM 上使用哪些参数</title>
      <link>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</link>
      <description><![CDATA[我正在模拟试剂转化率对试剂 L 和 M 之间比率的依赖性。比率越高，试剂转化率越高，在 1.5 左右有一个明显的拐点。下面是我拥有的完整数据集

df = pd.DataFrame({&#39;L_M&#39;:[4.75, 3.8, 3.32, 2.85, 2.37, 1.9, 1.42, 0.95, 0.71, 0.47, 0.24, 0.09],
&#39;Conversion&#39;:[0.992, 0.987, 0.993, 1, 0.9, 0.7, 0.31, 0.1, 0.07, 0.06, 0.07, 0.065]})

为此，我使用二项式家族的 GLM 模型，以 Logit 作为链接函数（又称 Logistic 回归）。这似乎很合适，因为我正在对二项式过程中的成功率进行建模（试剂要么发生反应，要么不发生反应）。我尝试使用 statmodels.GLM 失败了，下面的代码产生了非常差的拟合效果：
# 定义因变量和自变量 
Xtrain = df[&#39;L_M&#39;] 
ytrain = df[&#39;Conversion&#39;]

# 构建模型并拟合数据 
log_reg = sm.GLM(ytrain, Xtrain, family=sm.families.Binomial()).fit()

# 推理 
df[&#39;Predicted&#39;] = log_reg.predict(Xtrain)


然而，我使用了一种 hack 的方法，为每个变量创建了 100 个原始数据实验点，每个原始 y 变量为 0 或 1，概率等于此实验点的转换，然后使用 sklearn 将 logreg 拟合到这个（膨胀很多的）数据集，效果很好，产生更好的拟合效果：
from sklearn.linear_model import LogisticRegression

# 实例化模型（使用默认参数）
logreg = LogisticRegression(random_state=16)

# 用数据拟合模型
logreg.fit(X_train, y_train)
new_X = pd.DataFrame(df[&#39;L_M&#39;])
new_X.columns = [&#39;ratio&#39;]
df[&#39;Predicted&#39;]= logreg.predict_proba(new_X)[::,1]



我在 statsmodels.GLM 中遗漏了哪些参数？
更新：为了解释我使用伯努利分布的理由，我可以说，试验总数确实是已知的，因为我准备了解决方案，并且知道试剂的浓度。每个实验点的试验次数也大致相同，而且数量巨大，约为 $10^{15}$。这就是为什么我认为将比率转换为 100 个二元结果集的方法很好地代表了现实。这个特定的实验可以被认为是有一个烧杯，并让分子一个接一个地进入烧杯。如果它发生反应 - 那就是成功，否则就是失败。这项试验在 12 个烧杯中重复进行，这些烧杯在某些特征“L-M”上有所不同。在每个烧杯中，试验重复 $10^{15}$ 至 $10^{16}$ 次，并记录发生反应的分子比例。我可以将此数据呈现为 $12\times10^{15}$ 个二元结果记录，感觉就像拥有 $12\times10^{2}$ 个记录传达了相同的信息。希望这可以解释为什么我相信伯努利模型是适用的。]]></description>
      <guid>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</guid>
      <pubDate>Tue, 07 Jan 2025 20:27:59 GMT</pubDate>
    </item>
    </channel>
</rss>