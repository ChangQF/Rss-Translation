<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sun, 15 Sep 2024 18:21:08 GMT</lastBuildDate>
    <item>
      <title>广义 Jensen-Shannon 散度 - 什么是小 JSD？</title>
      <link>https://stats.stackexchange.com/questions/654403/generalised-jensen-shannon-divergence-what-is-a-small-jsd</link>
      <description><![CDATA[我正在根据不同机器学习模型的输出比较多个分布之间的相似性。我正在应用广义的 JS 散度（wiki）：
$$
JSD_{\pi_1,...,\pi_n}(p_1,...p_n) = \sum_i \pi_i D_{KL}(p_i||M)
$$
其中 $M = \sum_i \pi_ip_i$。我使用了对数底数$n$，因此 JSD 被限制在 0 和 1 之间。但是，我希望有一个更易于解释的相似性度量，因为我想对机器学习模型的选择提出建议。例如，我想设置一个阈值 JSD，如果某些模型的 JSD 低于阈值，那么我们就可以选择该模型。
我应该如何设置阈值？或者 JSD 需要多小才能让我将其解释为“小”JSD，即分布“足够相似”？
非常感谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/654403/generalised-jensen-shannon-divergence-what-is-a-small-jsd</guid>
      <pubDate>Sun, 15 Sep 2024 17:46:29 GMT</pubDate>
    </item>
    <item>
      <title>确定适度回归的对比</title>
      <link>https://stats.stackexchange.com/questions/654401/determining-contrasts-for-a-moderated-regression</link>
      <description><![CDATA[我有一个研究设计，包含 4 种治疗条件（A、B、C 和对照组）和一个连续调节器 (Mod)。我正在研究治疗的效果以及它与 Mod 对因变量 (Y) 的相互作用。
我的假设是：
H1：与对照条件相比，所有治疗条件（A、B 和 C）中的 Y 值都会更高。
H2：与 B 和 C 相比，条件 A 中的 Y 值会更高，尤其是当 Mod 值更高时。
我计划使用正交对比来编码治疗变量。我提出的对比如下：
C1：A = -1，B = -1，C = -1，对照 = 3（测试处理组和对照组之间的差异）
C2：A = 2，B = -1，C = -1，对照 = 0（测试 A 与 B 和 C 的组合）
C3：A = 0，B = -1，C = 1，对照 = 0（测试 B 与 C）
我打算运行包含这些对比及其与 Mod 的相互作用的多元回归。我预计 C1 和 C2*Mod 将显著支持我的两个假设。
这种方法正确吗？
此外，我正在考虑另一种假设：
H1：Y 在条件 A 中最高，其次是 B 和 C（预计相等），在对照条件下最低。
H2：当 Mod 较高时，这种模式尤其正确。
在这种情况下，我不知道如何对对比进行编码以反映预期的模式。有人对如何为这种情况设置对比编码/分析有什么建议吗？
谢谢您的帮助！]]></description>
      <guid>https://stats.stackexchange.com/questions/654401/determining-contrasts-for-a-moderated-regression</guid>
      <pubDate>Sun, 15 Sep 2024 15:38:21 GMT</pubDate>
    </item>
    <item>
      <title>为什么随机效应和固定效应会产生相同的结果？</title>
      <link>https://stats.stackexchange.com/questions/654399/why-random-effects-and-fixed-effects-produce-the-same-results</link>
      <description><![CDATA[我进行了一项实验，有 40 名参与者和 16 种治疗组合（平衡数据，640 个数据点）。16 种治疗组合基于 5 个二元变量（我研究的主要影响）。
由于我对同一个人进行了重复测量，因此我选择了固定效应回归模型。
由于我还想在分析中包含参与者特定的控制变量（例如工作经验、年龄），因此我特意选择了“随机效应模型”。豪斯曼检验的结果：p 值 &lt; 0.01。
问题 1：
随机效应和固定效应模型对斜率和 t 统计量产生完全相同的结果。只有截距、个体效应和 $R^2$ 略有不同。随机效应和固定效应只改变个别效应，对斜率没有影响，这正常吗？
代码：
固定效应（Python：来自 linearmodels.panel 的 PanelOLS）：
mod = PanelOLS(y, X, entity_effects=True, time_effects=False)
results = mod.fit(cov_type=&#39;clustered&#39;, cluster_entity=True)
随机效应（Python：来自 linearmodels.panel 的 RandomEffects）：
mod = RandomEffects(y, X)
results = mod.fit(cov_type=&#39;clustered&#39;, cluster_entity=True)
问题2：
当我现在将控制变量（6 个参与者特定变量）添加到随机效应模型时，它再次对主效应的斜率没有影响。只有截距和个体效应不同。同样的问题：有和没有控制变量的随机效应产生相同的主效应斜率是正常的吗？
非常感谢您对这个主题的支持！]]></description>
      <guid>https://stats.stackexchange.com/questions/654399/why-random-effects-and-fixed-effects-produce-the-same-results</guid>
      <pubDate>Sun, 15 Sep 2024 15:09:08 GMT</pubDate>
    </item>
    <item>
      <title>在多元线性回归与简单线性回归中沿一个变量移动 1 个 SD</title>
      <link>https://stats.stackexchange.com/questions/654396/moving-1-sd-along-one-variable-in-multiple-linear-regression-vs-simple-linear-re</link>
      <description><![CDATA[在通过通常的最小二乘法进行的简单线性回归中，可以很容易地证明估计的线通过均值点$(\bar{x},\bar{y})$，并且沿$x$移动一个样本标准差（SD），$s_x$，会导致$r s_y$在$y$方向上发生变化，其中$r$是$x$和$y$之间的相关性，而$s_y$是$x$的样本 SD。 class=&quot;math-container&quot;&gt;$y$ 输入数据集的值。
通过最小二乘法进行的多元线性回归是否也得到类似的结果？也就是说，估计的平面是否通过 $(\bar{x}_1,\ldots,\bar{x}_p,\bar{y})$ 并且保持所有其他解释变量不变，如果我们沿 $x_1$ 移动一个 SD，$s_{x_1}$，y 方向上的 $r_{x_1y} s_y$ 会发生变化？]]></description>
      <guid>https://stats.stackexchange.com/questions/654396/moving-1-sd-along-one-variable-in-multiple-linear-regression-vs-simple-linear-re</guid>
      <pubDate>Sun, 15 Sep 2024 11:54:24 GMT</pubDate>
    </item>
    <item>
      <title>不保留部分顺序的收缩</title>
      <link>https://stats.stackexchange.com/questions/654394/shrinkage-that-does-not-preserve-partial-order</link>
      <description><![CDATA[一般来说，假设在估计多元均值时，所有收缩方法是否都保留部分顺序？我的意思是，MLE 估计的均值的顺序在收缩后会改变吗？
显然，形式为 $\hat{m}&#39; = (1-\lambda)\hat{m}$ 的收缩是保序的（确保 $\lambda$ 介于 0 和 1 之间）。
是否有任何不保留部分顺序的收缩方法（基本上在收缩后使它们相等不算在内，例如硬阈值）。
显然，人们可以将任意先验放在某个地方来实现这一点。但我更希望的是经验贝叶斯设置。]]></description>
      <guid>https://stats.stackexchange.com/questions/654394/shrinkage-that-does-not-preserve-partial-order</guid>
      <pubDate>Sun, 15 Sep 2024 10:37:41 GMT</pubDate>
    </item>
    <item>
      <title>双变量数据生成</title>
      <link>https://stats.stackexchange.com/questions/654392/bivariate-data-generation</link>
      <description><![CDATA[考虑具有双变量累积分布函数的分布$$F(t_1,t_2)=t_1^{1+\theta\log(t_2)}t_2,0&lt;t_1,t_2&lt;1; \theta\leq 0$$。
我想从此分布生成数据（使用 R 编程）。我知道如何在单变量情况下生成数据，但我不知道如何在双变量情况下生成数据。]]></description>
      <guid>https://stats.stackexchange.com/questions/654392/bivariate-data-generation</guid>
      <pubDate>Sun, 15 Sep 2024 09:02:57 GMT</pubDate>
    </item>
    <item>
      <title>“手动”加权回归</title>
      <link>https://stats.stackexchange.com/questions/654391/manually-weighted-regression</link>
      <description><![CDATA[我有一个包含加权观测值的数据集。我正在尝试找到一种无需指定权重即可执行加权回归的方法。也就是说，我想要找到变量 $X$ 和 $Y$ 的变换（表示为 $\tilde{X}$ 和 $\ddot{Y}$），这样当对 $\tilde{X}$ 执行 $\ddot{Y}$ 的 OLS 估计时，我实际上使用权重 $W$ 对 $X$ 执行 $Y$ 的加权回归。
如果 $W$ 是对角线矩阵，则 OLS 系数可按如下方式获得：
$$
\hat{\beta}_{wols} = (X&#39;WX)^{-1} X&#39;WY。
$$
这意味着我可以用 $W$ 的平方根来变换 $X$ 和 $Y$：
$$
\ddot{Y} = \sqrt{W}Y\\
\tilde{X} = \sqrt{W}X。
$$
请注意，这是一个无截距的回归，其中 $\ddot{Y}$ 是平方根加权变量 $X$，以及这些平方根权重的向量。
我的问题是模型统计数据（特别是 R 平方）与常规加权回归不同。
有没有人知道另一种变换，当我在变换后的变量上计算未加权 OLS 估计量 $\hat{\beta}_{ols}$ 时，我会得到 $\hat{\beta}_{wols}$ 和相应的模型统计数据？
我的用例是我想“破解”不支持权重来执行加权回归的估算程序。]]></description>
      <guid>https://stats.stackexchange.com/questions/654391/manually-weighted-regression</guid>
      <pubDate>Sun, 15 Sep 2024 07:58:18 GMT</pubDate>
    </item>
    <item>
      <title>如果误差是同质的但非正态的，那么线性估计量可以是蓝色吗？</title>
      <link>https://stats.stackexchange.com/questions/654388/if-the-errors-are-homogenous-but-non-normal-can-the-linear-estimator-be-blue</link>
      <description><![CDATA[在高斯-马尔可夫假设下，OLS 为 BLUE 的要求是：

线性：估计量必须是数据的线性函数
无偏性：估计量的预期值应等于总体参数
同方差性：误差方差为常数
无完美多重共线性：解释变量不应完全共线。
外生性：误差应为零均值，且与解释变量不相关。

这是否意味着只要满足这些假设，无论误差分布如何，OLS 估计量都是 BLUE？
我有时读到误差必须呈正态分布才能为 BLUE。我有时读到如果不是，也没关系。]]></description>
      <guid>https://stats.stackexchange.com/questions/654388/if-the-errors-are-homogenous-but-non-normal-can-the-linear-estimator-be-blue</guid>
      <pubDate>Sun, 15 Sep 2024 06:12:25 GMT</pubDate>
    </item>
    <item>
      <title>理解简单线性回归误差方差与 Y 的比较符号</title>
      <link>https://stats.stackexchange.com/questions/654387/understanding-notation-of-simple-linear-regression-error-variance-compared-with</link>
      <description><![CDATA[在简单线性回归中，不假设条件形式，我们说$Var(\epsilon) = \sigma^2$，因为误差方差是同质的。
但我不明白为什么会这样。
$Var(\epsilon) = E[(Y - \beta x)^2] = E[(Y - \hat{Y})^2]$
但这似乎不等于 Y 的方差。
这只是符号上的差异吗？误差方差是 Y 中无法解释的方差。Y 的方差是无法解释和解释的方差的总和。
那么我们是不是只想说误差项与 $Y$ 中的方差成比例，因为这个方差是导致误差围绕 $\beta x$ 的因素？]]></description>
      <guid>https://stats.stackexchange.com/questions/654387/understanding-notation-of-simple-linear-regression-error-variance-compared-with</guid>
      <pubDate>Sun, 15 Sep 2024 04:32:29 GMT</pubDate>
    </item>
    <item>
      <title>什么是移动平均模型（真的！）</title>
      <link>https://stats.stackexchange.com/questions/654385/what-is-a-moving-average-model-really</link>
      <description><![CDATA[一段时间以来，我一直在尝试理解时间序列环境中的移动平均模型是什么。
我知道它本质上是基于过去预测误差的回归，形式如下：https://otexts.com/fpp3/MA.html（请参阅本文第 3 行）。
我很难理解的是 - 这些过去的预测误差到底是什么？或者更具体地说，这些过去预测误差中的预测元素的性质是什么？例如，它只是目标时间序列到给定点的移动平均值吗？还是别的什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/654385/what-is-a-moving-average-model-really</guid>
      <pubDate>Sun, 15 Sep 2024 02:51:16 GMT</pubDate>
    </item>
    <item>
      <title>当似然函数本身是一个猜测时[关闭]</title>
      <link>https://stats.stackexchange.com/questions/654383/when-the-likeihood-function-is-itself-a-guess</link>
      <description><![CDATA[当存在观测值 x 和相关未知数 $T$，但没有明显的 $f(x \mid T)$ 模型时，贝叶斯会做什么？我举的例子是在商业领域，专家报告了新商业理念未来现金流的当前估值。在硬科学中是否有类似的例子，历史数据允许我们估计不同 $x$ 和 $T$ 的经验似然分布 $f(x \mid T)$？]]></description>
      <guid>https://stats.stackexchange.com/questions/654383/when-the-likeihood-function-is-itself-a-guess</guid>
      <pubDate>Sat, 14 Sep 2024 23:35:47 GMT</pubDate>
    </item>
    <item>
      <title>使用经典拟合优度检验检查逻辑回归假设</title>
      <link>https://stats.stackexchange.com/questions/654373/checking-the-logistic-regression-assumptions-with-classic-goodness-of-fit-tests</link>
      <description><![CDATA[在寻找在估计系数后检查模型充分性的方法时，我发现我需要 i) 检查逻辑回归假设和 ii) 检查拟合优度。
当我说拟合优度时，我并不是指测试预测能力（我用 McFadden Pseudo R^2 来测试）。我想检查估计值如何很好地表示观察到的数据。
由此得出我的疑问：当 gof 测试告诉你是否可以通过使模型更复杂来做得更好时，实际上是在测试是否存在你遗漏的非线性或相互作用，这是否与测试所有假设相同？简而言之：是否有必要通过使用例如来单独测试假设？低平滑（平滑 y）还是 HL 检验的替代方法测试相同？
因此：对于我的模型，它由一个连续解释变量（和一个二元结果）组成，样本量为 10，哪种检验最合适？由于我的样本量小，标准化 Pearson 不合适。另一个衡量标准可能是 Stukel 检验。很想听听你对此的看法！]]></description>
      <guid>https://stats.stackexchange.com/questions/654373/checking-the-logistic-regression-assumptions-with-classic-goodness-of-fit-tests</guid>
      <pubDate>Sat, 14 Sep 2024 16:35:11 GMT</pubDate>
    </item>
    <item>
      <title>如何计算给定函数 f(x) 的置信区间？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/654370/how-to-calculate-the-confidence-interval-for-a-given-function-fx</link>
      <description><![CDATA[我试图理解如何计算给定函数 $f(x)$ 的置信区间，该函数有 $n$ 个来自该分布的样本点。我知道我需要计算样本平均值和标准差，但我不确定接下来的步骤。有人可以解释一下计算非正态分布置信区间的一般过程或指南吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/654370/how-to-calculate-the-confidence-interval-for-a-given-function-fx</guid>
      <pubDate>Sat, 14 Sep 2024 14:45:03 GMT</pubDate>
    </item>
    <item>
      <title>多重比较校正，在对具有相关因变量的 4 个模型进行成对比较后，何时进行？</title>
      <link>https://stats.stackexchange.com/questions/654335/correction-for-multiple-comparison-after-pairwise-comparison-on-4-models-with-r</link>
      <description><![CDATA[我正在使用线性混合效应模型对我的实验进行统计评估。实验由在受试者内以随机顺序进行的几种不同的干预方案组成。每次干预期间都有 3 个不同的时间段。我希望评估每个时间段内干预方案的效果以及每个干预方案内时间段之间的差异。
具体来说，我的模型如下：
fit_simple &lt;- lmer(log(target_variable) ~ Protocol * period + sex + (1 | contestant), data = input_data, REML = TRUE)

数据是对数转换的，因此它符合假设，而且数据分布的性质在文献中几乎总是对数转换的。我的最终模型中的随机结构有点复杂，但这与此无关。
这就是棘手的地方。我对略有不同的数据（不同的 HRV 参数，均根据 ECG 数据计算得出）运行了此模型 4 次，数据高度相关。
对于 4 个模型中的每一个，我希望评估时间段内干预方案之间的差异以及干预方案内时间段之间的差异。我采用成对比较，并使用估计边际均值（R 中的包 emmeans）。默认情况下，emmeans 包已经包含了针对每个成对比较的多重比较校正（默认情况下为 Tukey HMD），但可以关闭此功能。
emm &lt;- emmeans(fit_simple, ~ Protocol * period)

# 每个 Protocol 内各个周期级别之间的成对比较

pairwise_comparisons_Protocol &lt;-contrast(emm, method = &quot;pairwise&quot;, by = &quot;Protocol&quot;)

# 每个周期内各个 Protocol 级别之间的成对比较

pairwise_comparisons_period &lt;-contrast(emm, method = &quot;pairwise&quot;, by = &quot;period&quot;)

#Summpary

summary_Protocol &lt;-summary(pairwise_comparisons_Protocol)

summary_period &lt;- summary(pairwise_comparisons_period)

因此，对于每个模型，我都运行两次成对比较。成对比较可以通过我能想到的三种方式进行：

每个成对比较都可以通过多重比较进行校正（默认情况下是这样）。
可以对每个模型的两次成对比较的所有 p 值同时进行多重比较校正。
可以一次对所有模型的成对比较的所有 p 值进行多重比较校正。

最后一种方法是一次对所有 4 个模型的所有 p 值进行校正，这似乎是正确的做法，但我不确定。测试多重比较的正确方法是什么？
另一个问题是，我应该使用哪种多重比较校正方法？如果可能的话，我希望避免使用 Bonferroni。
感谢您的帮助，非常感谢！
编辑：
数据经过对数转换，以实现残差的正态分布。没有对数转换就没有机会，所以要么对因变量进行对数转换，要么走广义线性混合效应建模的路线。由于在类似的研究中，作者往往对值进行对数转换，所以我也做了同样的事情。
所有 4 个模型的因变量来自相同的心跳间隔。从逐搏数据中计算出 4 个不同的 HRV 参数（SNDD、RMSSD、HF、LF）。
SDNN - 所有逐搏间隔（连续心跳之间的时间）的标准偏差。
RMSSD - 连续逐搏间隔之间的均方差的平方根。
HF 和 LF - FFT 频谱图上两个频率之间的曲线面积。
4 个模型中的每个模型中的因变量都是 4 个 HRV 参数之一。]]></description>
      <guid>https://stats.stackexchange.com/questions/654335/correction-for-multiple-comparison-after-pairwise-comparison-on-4-models-with-r</guid>
      <pubDate>Fri, 13 Sep 2024 17:15:54 GMT</pubDate>
    </item>
    <item>
      <title>OLS 回归变换，其中独立变量的总和是因变量的一部分</title>
      <link>https://stats.stackexchange.com/questions/654130/ols-regression-transformation-where-the-sum-of-the-independent-variables-is-part</link>
      <description><![CDATA[我最近看到了以下形式的回归（不是在书中，而是在业余环境中），其 ID 为 $i$，区域为 $j$：
$$\ln\left(\dfrac{1+y_i}{1+\sum_{j\in\{1,2,3\}}x_{ij}}\right) = \ln(1+x_{i1})\beta_1+\ln(1+x_{i2})\beta_2+\ln(1+x_{i3})\beta_3+\epsilon_i\\
=\boldsymbol{X}_i\boldsymbol \beta+\epsilon_i$$
现在，比率 $y_i/\sum_{j\in\{1,2,3\}} x_{ij}$ 具有一定的解释，我们称之为 $\textit{multiplier}$。
我们想要预测给定 $x_1,x_2,x_3$ 的 $\textit{multiplier}$、$\widehat{\textit{multiplier}}$。其实现方式如下：
$$\widehat{\textit{multiplier}}_i=\exp\left(\boldsymbol{X}_i\hat{\boldsymbol \beta}\right)\times\dfrac{ \left(1+\sum_{j\in\{1,2,3\}}x_{ij}\right)}{\sum_{j\in\{1,2,3\}}x_{ij}}-1.$$
我大概明白这种回归的原理。但是，我有以下几点批评意见：

我们已经知道 $x_{ij}$，因此我认为将 $x_{ij}$ 作为因变量是没有意义的。有人能证实这没有意义吗？
$\textbf{Edit}$：在我看来，我们引入了一个额外的变异源。我们不能只重写方程式，将所有 $x$ 都放在右边吗？


在预测中，上述回归的左侧预测如下：$$\hat{Y}_i=\ln\left(\dfrac{1+\hat{y}_i}{1+\sum_{j\in\{1,2,3\}}\hat{x}_{ij}}\right),$$当然，我们不会预测单个 $\hat{y}_i$ 和 $\sum_{j\in\{1,2,3\}} \hat{x}_{ij}$，我们预测这些的比率作为变换。将预测的 $\exp\left(\boldsymbol{X}_i\hat{\boldsymbol \beta}\right)=\dfrac{1+\hat{y}_i}{1+\sum_{j\in\{1,2,3\}}\hat{x}_{ij}}$ 的分母与观察到的 $\left(1+\sum_{j\in\{1,2,3\}}x_{ij}\right)$ 相消是否有意义？

有人能帮我吗？
$\textbf{编辑 2:}$
我认为，当您运行以下形式的回归时，问题似乎相当类似：
$$\dfrac{y_i}{x_i} = x_i\beta+\epsilon_i$$除以 $i$。
我认为，这样做
$x_i\hat{\beta}\times x_i=\hat{y}_i$是不对的。
此外，您始终可以将上述回归重写为 $y_i=x_i^2
\beta+\epsilon_i^*$，对吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/654130/ols-regression-transformation-where-the-sum-of-the-independent-variables-is-part</guid>
      <pubDate>Mon, 09 Sep 2024 21:30:38 GMT</pubDate>
    </item>
    </channel>
</rss>