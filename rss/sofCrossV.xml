<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Tue, 05 Mar 2024 18:16:30 GMT</lastBuildDate>
    <item>
      <title>重叠类的支持向量分类器</title>
      <link>https://stats.stackexchange.com/questions/641912/support-vector-classifiers-for-overlapping-classes</link>
      <description><![CDATA[我目前正在研究支持向量分类器（SVC），更具体地说，是在《统计学习的要素》一书的帮助下研究拉格朗日（沃尔夫）对偶函数的解决方案。作者：Trevor Hastie、Robert Tibshirani 和 Jerome Friedman。据说，从 Karush-Kuhn-Tucker (KKT) 条件，我们可以推断，对于位于边缘边缘的支持向量 $x_i$，相应的拉格朗日乘子 $\alpha_i$ 位于 $0 $0 $\alpha_i$ \alpha_i &lt; C$，其中 $C$ 是拉格朗日原始函数中的成本参数。
考虑到 KKT 条件


我怎样才能看到$\alpha_i \neq C$？
提前谢谢您。]]></description>
      <guid>https://stats.stackexchange.com/questions/641912/support-vector-classifiers-for-overlapping-classes</guid>
      <pubDate>Tue, 05 Mar 2024 18:11:22 GMT</pubDate>
    </item>
    <item>
      <title>A/A 测试中的高功率，解释？</title>
      <link>https://stats.stackexchange.com/questions/641911/high-power-in-a-a-test-interpretation</link>
      <description><![CDATA[随机化已经执行，并且已经分配了两组，其中治疗观察和对照观察分别由 df1、df2 捕获。
注意：尚未进行治疗暴露。
尽管存在重尾分布，但随机化似乎做得很好，但由于样本量大和中心极限定理，这也许没问题？
治疗：mu=420，std=178.77，n=2001
对照：mu=418，std=169.07，n=1999

根据我的研究，似乎在 t 测试中上下文，我需要按合并标准差缩放的均值差。但因为我还没有进行治疗，所以这是一个 A/A 测试！
from statsmodels.stats.power import tt_solve_power
def 功率（df1、df2、公制）：
    arr1 = df1[指标].to_list()
    arr2 = df2[metric].to_list()
    
    n1 = 长度（arr1）
    n2 = len(arr2)
    n = n1+n2
    
    mu1 = np.mean(arr1)
    mu2 = np.mean(arr2)
    
    s1 = np.std(arr1)
    s2 = np.std(arr2)
    
    pooled_sd = (s1**2/n1 + s2**2/n2)**(0.5)
    差异 = (mu1-mu2)/pooled_sd
    
    功率 = tt_solve_power(
        效果大小=差异，
        诺布=n，
        阿尔法=0.95，
        功率=无，
        替代=&#39;双面&#39;
    ）
    返回电源

当我运行此分析时，我发现 diff 等于 0.45，并且我的功效为 100%。这告诉我（诚然是一个新手），就科恩距离而言，我的 A/A 效应大小与 0 相差 0.45 个标准差……不太好！而我的力量如此之高，这似乎是对随机化过程的控诉。
我在这里做错了什么吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/641911/high-power-in-a-a-test-interpretation</guid>
      <pubDate>Tue, 05 Mar 2024 17:56:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 Jeffreys 先验的后验分布</title>
      <link>https://stats.stackexchange.com/questions/641910/posterior-distribution-using-jeffreys-prior</link>
      <description><![CDATA[我试图证明如果 $X_1, \cdots, X_n \stackrel{iid}{\sim} N(\mu, \sigma^2)$ 与未知的 $\mu$、$\sigma$ 和之前的 $\pi(\mu, \sigma^2) \propto 1/\sigma^2$ 则 $\mu$ 的后验分布span&gt; 由 $$\frac{\sqrt{n} (\mu - \bar{x})}{s_{n-1}} \sim t_{n- 给出1} $$
其中 $\bar{x} = \frac{1}{n} \sum x_i$ 是样本平均值，$ s_{n-1} = \frac{1}{n-1} \sum (x_i - \bar{x})^2$ 是样本方差。我知道 $\sum (x_i - \bar{x})^2 \sim \chi^2_{n-1}$ 通过标准结果。因此，要表明 $$\frac{\sqrt{n} (\mu - \bar{x})}{s_{n-1}} \sim t_{n-1 } ,$$
我只需要分子为 $N(0, 1)$ 但我不太确定如何。我注意到 $$\pi(\mu, \sigma^2|\mathbf{x}) \propto (\sigma^2)^{-(1 + n/2) } \text{exp} \left( \frac{-1}{2\sigma^2} \sum (x_i - \mu)^2 \right) $$
但我被困在这里了。有人可以从这里给我建议吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/641910/posterior-distribution-using-jeffreys-prior</guid>
      <pubDate>Tue, 05 Mar 2024 17:51:15 GMT</pubDate>
    </item>
    <item>
      <title>功效分析：不同的均值和方差[关闭]</title>
      <link>https://stats.stackexchange.com/questions/641908/power-analysis-different-means-and-variances</link>
      <description><![CDATA[我已经进行了随机化，现在已经得出 mu1、mu2（平均值）、s1、s2（标准差）和 n1、n2（每组样本数）。
鉴于上述情况，如何计算给定标准 80% 功率的最小可检测效应？同样，我也想扭转这个问题——给定一定的效应大小，我有什么权力？
是否有Python和/或R库/方法可以接受上述六个参数并返回所需的答案？
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/641908/power-analysis-different-means-and-variances</guid>
      <pubDate>Tue, 05 Mar 2024 16:33:53 GMT</pubDate>
    </item>
    <item>
      <title>分布模型和回归模型之间的差异</title>
      <link>https://stats.stackexchange.com/questions/641905/difference-between-a-distribution-model-and-a-regression-model</link>
      <description><![CDATA[请帮助我了解与随机性相关的分布模型和回归模型之间的区别]]></description>
      <guid>https://stats.stackexchange.com/questions/641905/difference-between-a-distribution-model-and-a-regression-model</guid>
      <pubDate>Tue, 05 Mar 2024 16:18:38 GMT</pubDate>
    </item>
    <item>
      <title>对同一时间序列数据使用双样本K-S检验</title>
      <link>https://stats.stackexchange.com/questions/641903/using-two-sample-k-s-test-for-the-same-time-series-data</link>
      <description><![CDATA[我有时间序列数据，它记录了神经元在某些刺激之前和之后的神经活动（我知道引入刺激的具体时间）。我正在考虑对刺激前后的神经信号强度分布使用双样本 K-S 检验来检查它们之间是否存在统计上的显着差异，这意味着该神经元对刺激做出了反应。
我是统计学新手，我不确定这些数据是否会违反 K-S 检验的一些假设。我读过 K-S 检验假设被比较的两个分布是独立的，这是否意味着比较来自同一神经元的信号分布会打破这个假设，因为刺激后的活动可能依赖于刺激前的活动？该数据是否可能违反 K-S 检验的任何其他假设？如果是这样，我可以对数据进行任何预处理吗？或者是否有其他更适合的统计测试？]]></description>
      <guid>https://stats.stackexchange.com/questions/641903/using-two-sample-k-s-test-for-the-same-time-series-data</guid>
      <pubDate>Tue, 05 Mar 2024 15:31:07 GMT</pubDate>
    </item>
    <item>
      <title>比较效应大小的排名？</title>
      <link>https://stats.stackexchange.com/questions/641902/comparing-the-ranking-of-effect-sizes</link>
      <description><![CDATA[我有很多效应大小，是根据相同的线性模型估计的，但测试的解释变量不同。这些模型在两个不同的组中运行，但它们是相同的模型，并且解释变量 ($x_1$) 是相同的。
A组：
$$ y_A = \alpha_A + \beta_{1A}x_{1A} + \beta_{2A}x_{2A} + ... + \epsilon_A $$
B组：
$$ y_B = \alpha_B + \beta_{1B}x_{1B} + \beta_{2B}x_{2B} + ... + \epsilon_B $$&lt;​​/span&gt;&lt; /p&gt;
所以 $x_1$ 是测试的参数，$y, x_1, x_2$ 是相同的变量，但当然它们在不同的组中有不同的值。
我想比较 $\beta_1$ 值的分布，看看两组之间是否不同。如果我想比较平均值，我假设我可以进行 t 检验（在检查正态性后）。但是，如果我想看看配对效应大小是否为 $\beta_{1A}$ 和 $\beta_{1B} $ 在两个组中始终具有不同的值？
我可以做一个测试来测试配对效应大小的排名是否不同吗？例如配对 Wilcoxon 符号秩检验？
这个想法是为了深入了解配对变量$\beta_{1A}$和$\beta_{1B}$ 两组之间差异具有统计学意义。这有道理吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/641902/comparing-the-ranking-of-effect-sizes</guid>
      <pubDate>Tue, 05 Mar 2024 15:24:15 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 emmeans 从估计值中添加 z 分数及其置信区间？</title>
      <link>https://stats.stackexchange.com/questions/641899/how-can-i-add-the-z-score-and-its-confidence-interval-from-an-estimate-using-emm</link>
      <description><![CDATA[在使用 emmeans 包后，我尝试将 z 分数值及其置信区间添加到以下模型中。
型号：
m &lt;- lmer(mode_usual_dic ~ 干预 * 时间 + fas + distance_walking +
         性别 + (1|中心/ID), 数据 = df)

功能：
emmeans2 &lt;- 函数（型号、规格、依据）{
  em1 &lt;- emmeans（对象 = 模型，规格 = 规格，by = by）
  em2 &lt;- emmeans(对象 = 模型，规格 = 规格，by = by) |&gt;限制（）
  
  em2$contrasts$p.value &lt;- as.data.frame(em1$contrasts)$p 。价值
  
  em2$contrasts &lt;- em2$对比 |&gt;
    mutate(p.value = ifelse(p.value &lt; 0.01, &#39;&lt; 0.01&#39;, round(p.value, 3)))
  
  回报（em2）
}

交互：
em1 &lt;- emmeans2(m, 规格 =pairwise~intervention, by = &#39;时间&#39;)
em2 &lt;- emmeans2(m, 规格 = 成对~时间, by = &#39;干预&#39;)

合同：
em1$对比 |&gt;
  as.data.frame() |&gt;
  选择（-SE，-df）|&gt;
  flextable(cwidth = 0.5) |&gt;;
  add_header_lines(&#39;组间对比&#39;) |&gt;;
  自动调整（）

因此，问题是如何将 z 分数及其置信区间添加到最终输出中。谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/641899/how-can-i-add-the-z-score-and-its-confidence-interval-from-an-estimate-using-emm</guid>
      <pubDate>Tue, 05 Mar 2024 15:15:13 GMT</pubDate>
    </item>
    <item>
      <title>确定变量选择过程的最佳模型</title>
      <link>https://stats.stackexchange.com/questions/641898/determine-best-model-for-variable-selection-process</link>
      <description><![CDATA[当我们进行变量选择时，理论上我们会为每个选定的变量子集拟合一个模型。然后我们可以比较所有模型并选择最好的。
有各种统计数据可以帮助选择最佳模型，例如贝叶斯信息准则、调整后的 R2。
我的问题是，为什么我们不能只使用 RSE 或 R2 作为模型准确性的统计数据？]]></description>
      <guid>https://stats.stackexchange.com/questions/641898/determine-best-model-for-variable-selection-process</guid>
      <pubDate>Tue, 05 Mar 2024 15:14:44 GMT</pubDate>
    </item>
    <item>
      <title>成对的概率</title>
      <link>https://stats.stackexchange.com/questions/641893/probability-of-pairs</link>
      <description><![CDATA[假设$N$个人每人生成一个随机数$[1,\cdots,N]$。平均而言，会有多少对，因为一对意味着 $i$ 生成了 $j$ 和人 $j$ 生成 $i$？]]></description>
      <guid>https://stats.stackexchange.com/questions/641893/probability-of-pairs</guid>
      <pubDate>Tue, 05 Mar 2024 14:11:25 GMT</pubDate>
    </item>
    <item>
      <title>对具有强季节性的时间序列进行建模</title>
      <link>https://stats.stackexchange.com/questions/641889/modeling-timeseries-with-strong-seasonality</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/641889/modeling-timeseries-with-strong-seasonality</guid>
      <pubDate>Tue, 05 Mar 2024 13:18:21 GMT</pubDate>
    </item>
    <item>
      <title>自举 t 统计量的近似误差阶</title>
      <link>https://stats.stackexchange.com/questions/641741/bootstrapping-t-statistics-order-of-approximation-error</link>
      <description><![CDATA[考虑引导 $t$-统计：$T_n = T(X_1,...,X_n) = \sqrt{n-1}\frac{\bar{X} - \mu}{\hat{\sigma}}$ 用于 iid 观测 $(X_1,. ..X_n)$，其中 $\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$&lt; /span&gt; 和 $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n} (X_i - \bar{X })^2$..
现在，我从中心极限定理知道 $P(T_n \leq x) \xrightarrow[n \rightarrow \infty]{} \Phi(x)$，对于 $\Phi(x)$ 标准正态 cdf。
对于每个引导程序重新采样，假设不是使 $n$ iid 从经验分布中进行替换，而是只进行 $m = n/2$ iid 从经验分布中进行替换。
我想知道引导近似误差的顺序是 $n$ （例如 $O( n^{-2})$) 用于以下内容：$| P(T_n \leq x \mid F_0) - P(T_n^* \leq x \mid F_1)|$。
也许我的符号不清楚，但我表示 $T_n$ 的引导版本，由 $T_n^{* }$，由 $T_n^{*} = T(X_1^*,...,X_m^*) = \sqrt{m-1 给出}\frac{\bar{X}^* - \bar{X}}{\hat{\sigma}^*}$，其中 $(X_1^* ,...,X_m^*)$ 是 $m = n/2$ iid 从经验分布中得出，$F_1$，它将概率 $\frac{1}{n}$ 置于每个观测值 $X_1,...X_n$。设 $\bar{X}^* = \frac{1}{m}\sum_{i=1}^{m}X_i^*$，另外让 $\hat{\sigma}^{*2} = \frac{1}{m}\sum_{i=1}^{m}(X_i^* - \bar {X}^*)^2$。
有人可以帮助我理解如何计算出上述近似误差的顺序吗？ $| P(T_n \leq x \mid F_0) - P(T_n^* \leq x \mid F_1)|$。同样，这与 $| 相比如何？ P(T_n \leq x \mid F_0) - \Phi(x)|$？我的想法是埃奇沃斯展开在这里会有所帮助。我的想法是收敛速度不会改变，误差也不会改变。]]></description>
      <guid>https://stats.stackexchange.com/questions/641741/bootstrapping-t-statistics-order-of-approximation-error</guid>
      <pubDate>Sun, 03 Mar 2024 20:01:49 GMT</pubDate>
    </item>
    <item>
      <title>导出线性回归中的残差分布</title>
      <link>https://stats.stackexchange.com/questions/641739/deriving-the-distributions-of-residuals-in-linear-regression</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/641739/deriving-the-distributions-of-residuals-in-linear-regression</guid>
      <pubDate>Sun, 03 Mar 2024 18:51:34 GMT</pubDate>
    </item>
    <item>
      <title>R pwr 包对于极小样本量 (<10) 的有效性</title>
      <link>https://stats.stackexchange.com/questions/641707/validity-of-r-pwr-package-for-extremely-small-sample-sizes-10</link>
      <description><![CDATA[编辑：这个问题不是：

关于编程
关于事后功效分析（至少不是为了在事后分析中使用样本统计数据 - 我有很好的人口估计值，我想向人们展示如何使用这些数据）

这个问题是关于：

正确使用科学界广泛使用的常用工具。
在此（或任何其他）工具中检查假设和近似值，包括默认的假设和近似值。

因此，我相信结束这个问题（甚至没有提供任何提示
在哪里以及如何得到答案）实际上伤害了这个社区，
通过传达这样的信息：这些检查是多余的，而它们
确实应该放在优先事项列表的首位。
我希望这个请求能够得到重新审理。
———————————-
pwr 包是否使用 Cohen 1988 中描述的小样本近似（例如 1 样本 t 检验中的自由度）来实现？我似乎在文档中找不到这个。
如果是这样，对于非常小的样本量功效计算是否有更好的选择？
感谢您的回复。
————————
可选阅读：我的问题的动机：
在工作（制造操作）中，我们发现很难重现测试结果。对我来说，很明显这是因为我们进行的研究动力严重不足。我的老板同意我应该解决一些问题。
我想计算我们使用的一些典型实验“设计”的功效。
R中的pwr包看起来非常方便。该小插图引用了 Cohen 的书：行为科学的统计功效分析 (1988)。不同的应用领域应该有相同的数学，但我还是决定看看这本书。
我注意到书中的一些近似值，当“小样本量”意味着 &gt; 时，这些近似值是有效的。 30. 对我来说，“非常大的样本量”有时意味着 10。我现在注意到 1 样本 t 检验和配对 t 检验的近似值：书中提到它没有具有正确自由度的表格。这导致 n &gt; 的差异非常小。 30，但我怀疑该近似值对于 n &lt; 30 可能无效。 10. 我想检查一下。这就是我的问题。
————————-
编辑：其中一条评论要求引用，这是合理的。
我引用 Cohen, J. (1988) 的话。行为科学的统计功效分析（第二版），如 pwr 包中引用。
这里有两个例子，科恩提到了近似值的使用：
第 42 页脚注 1 中关于样本量不等的 t 检验的一个不太严重的例子：“这是因为该表将 n 的 t 检验视为基于 df = 2n&#39; - 2，而实际上有 df=nA +nB - 2，较大的值。”
第 46 页，关于单样本 t 检验有一个更严重的例子：
“功效表的计算基础是 n 是两个样本中每个样本的大小，因此 t 检验将基于 2(n-1) 个自由度。在单样本情况下，t 必然仅基于 n - 1 个自由度。”并且：“除非样本量很小（比如小于 25 或 30），否则低估自由度的影响可以忽略不计。”
请注意，我同意这种分析和近似。然而，在我的情况下，我可能会误入歧途，因为我违反了其中一个条件。将错误的东西改成同样错误的东西并不是一种改进，所以我想避免这种情况。
对于现代计算机，不再需要这些近似值 - 但我无法在文档中找到 R 包是否是用近似值编写的。
我知道我们应该做什么，但是获得大于 20 的样本量对我们来说极其罕见。话又说回来，我们可以修改我们的实验以获得比科恩认为的“大”更大的效应量。我只需要向人们解释/激励我们应该这样做。要达到这一点需要付出努力。]]></description>
      <guid>https://stats.stackexchange.com/questions/641707/validity-of-r-pwr-package-for-extremely-small-sample-sizes-10</guid>
      <pubDate>Sun, 03 Mar 2024 10:35:10 GMT</pubDate>
    </item>
    <item>
      <title>机器学习每个类别的最佳观察计数是多少？</title>
      <link>https://stats.stackexchange.com/questions/641697/whats-the-optimal-observation-count-per-category-for-machine-learning</link>
      <description><![CDATA[我正在寻求一些有关分类变量中每个类别所需的最佳观察数量的建议或参考，特别是对于机器学习项目。举个例子，考虑标记为“颜色”的列，其包含三个类别：红色、绿色和蓝色。我遇到过一条一般规则，建议每个类别至少有 20 个观察值，这意味着我需要至少 20 个绿色、红色或蓝色的观察值（行）。然而，我有兴趣更深入地探讨这个话题，并且很想听听社区对此事的意见。是否有任何研究、文献或研究结果可以提供更全面的指导或验证这一经验法则？关于数据集中的红色、绿色和蓝色等类别理想需要多少条目或行的任何见解或建议阅读都将受到高度重视。预先感谢您的帮助！]]></description>
      <guid>https://stats.stackexchange.com/questions/641697/whats-the-optimal-observation-count-per-category-for-machine-learning</guid>
      <pubDate>Sun, 03 Mar 2024 01:33:53 GMT</pubDate>
    </item>
    </channel>
</rss>