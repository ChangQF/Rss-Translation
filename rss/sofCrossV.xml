<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Mon, 23 Sep 2024 18:22:23 GMT</lastBuildDate>
    <item>
      <title>从第一个时期开始初始验证损失就很低？</title>
      <link>https://stats.stackexchange.com/questions/654794/low-initial-validation-loss-from-the-first-epoch</link>
      <description><![CDATA[从第一个 epoch 开始，初始验证损失就很低，然后略有下降。这实际上意味着什么？这是否表明模型可以有效快速地识别此任务的模式？
我可以看到该模型在实践中有效，但结果（一些图像恢复）还不理想，所以我想进一步提高其性能。

鉴于第一个 epoch 的损失很低，我应该专注于使用更多数据进行训练还是调整架构和层以使其更加复杂等？

鉴于第一个 epoch 和最后一个 epoch 之间的差异很小，模型在这些 epoch 中几乎无法提高性能的可能性更大，还是损失的差异仍然有意义？


数据集数量为 10,000 张图像 - 0.9 用于训练，0.1 用于验证。
第一个 epoch 损失：
Epoch [1/50]，训练损失： 0.026428，验证损失：0.023727
最后一个时期和稳定期：
时期 [34/50]，训练损失：0.020682，验证损失：0.020651]]></description>
      <guid>https://stats.stackexchange.com/questions/654794/low-initial-validation-loss-from-the-first-epoch</guid>
      <pubDate>Mon, 23 Sep 2024 16:53:47 GMT</pubDate>
    </item>
    <item>
      <title>我是否只需要对一个主题进行重复测量的随机截距/随机斜率？</title>
      <link>https://stats.stackexchange.com/questions/654793/do-i-need-random-intercept-random-slope-for-repeated-measures-for-one-subject-on</link>
      <description><![CDATA[我仅对一个受试者进行了 7 次重复测量，测量了 1000 个不同的变量。时间点相隔两个月，并且不是在一天中的同一时间收集的（时间保持连续）。
如果只有一个受试者，我是否需要包含随机截距等？如果需要，我将如何在适合每个单独因变量的 lme 模型中指定这一点？
任何指针或示例方向都将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/654793/do-i-need-random-intercept-random-slope-for-repeated-measures-for-one-subject-on</guid>
      <pubDate>Mon, 23 Sep 2024 16:51:45 GMT</pubDate>
    </item>
    <item>
      <title>如何为支出不受控制但投资回报率可衡量的竞价策略设置 A/B 测试？</title>
      <link>https://stats.stackexchange.com/questions/654792/how-to-set-up-an-a-b-test-for-bidding-strategies-with-uncontrolled-spend-but-mea</link>
      <description><![CDATA[我正在对两种在线拍卖竞价策略进行 A/B 测试。一种策略（固定策略）只是在每次拍卖中（平均）出价固定金额。另一种策略根据过去几次拍卖的表现调整出价金额。
挑战在于我无法直接控制每种策略花费的金额。每次激活的目标费用 (CPA) 决定了总支出，这反过来又会影响转化次数，并最终影响投资回报率 (ROI)。我唯一能控制的方面是实验的持续时间。
我的目标是确定自适应策略在投资回报率方面是否能比固定策略高出至少 10%。
以下是我有几个关键问题：

在这种情况下，合适的随机化单位是什么？我们无法确定实验变体之间的美元金额或转化次数是否完全可比。

鉴于随机化单位的不确定性，简单的总利润（收入 - 支出）是否是确定实验结果的合适指标？在这种情况下，统计推断的合适方法是什么（我已阅读此帖子作为入门）。

样本大小和持续时间：如何确定适当的样本大小或实验持续时间以检测具有统计意义的 ROI 10% 提升？将随机化单位视为天数是否正确？

方差估计：鉴于支出不受目标每次转化费用直接控制，但受其影响，我应该使用哪些指标或技术来准确估计两组 ROI 的方差？

偏差缓解：如何解释可能造成偏差的支出或转化模式差异，尤其是当一种策略比另一种策略适应得更快时？

停止规则：由于我只能控制实验的运行时间，因此在实验期间和实验后应用哪些适当的停止标准或统计测试以确保结果可靠？


如能提供关于设置此类测试的最佳实践的任何指导，包括潜在的陷阱或确保稳健结果的方法，我将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/654792/how-to-set-up-an-a-b-test-for-bidding-strategies-with-uncontrolled-spend-but-mea</guid>
      <pubDate>Mon, 23 Sep 2024 16:46:42 GMT</pubDate>
    </item>
    <item>
      <title>如何在考虑不同回报模式的同时比较私募股权基金不同时期的 IRR？</title>
      <link>https://stats.stackexchange.com/questions/654788/how-to-compare-private-equity-fund-irrs-across-time-while-accounting-for-varying</link>
      <description><![CDATA[我有多个私募股权 (PE) 内部收益率 (IRR) 的时间序列，其中每个基金在开始时通常表现出较高的回报变化性，大部分回报在投资期结束时实现。这些系列也遵循典型的“J 曲线”模式，其中回报在最初几年为负或较低，而随着投资成熟和退出，正回报往往会在以后累积：

我的目标是比较 PE 基金投资组合在一段时间内的季度表现，类似于我们分析传统金融资产回报的方式，并最终构建一个包含其他变量的面板。然而，PE 回报的独特结构带来了一些挑战：

我尝试计算一阶差分来评估周期回报，但得到的序列不是方差平稳的，因为回报的波动性在基金生命周期的早期往往比后期高得多。
我还考虑过将确定性趋势拟合到时间序列中。在这里，我不确定：
(a) 我应该为每个基金的 IRR 系列拟合一个单独的趋势，还是
(b) 为所有 IRR 系列拟合一个单一的趋势，以解释 PE 回报的一般行为。

拟合单独趋势与拟合所有系列的统一趋势的优缺点是什么？在比较 PE 基金回报的长期表现时，是否有其他方法可以更好地解释其“J 曲线”性质？我希望转换后的时间序列是平稳的。
任何有关如何处理此类时间序列的指导，包括潜在的建模技术，都将不胜感激。
% 绘图数据 (MATLAB)

IRR =[-17.5000
-0.9000
-7.7000
-33.8000
-35.0115
-35.3700
-33.8000
-30.6000
-30.4000
-24.2000
-20.9000
-15.8500
-15.4000
-10.6500
-8.7500
-4.8000
-5.2000
-3.4000
-3.0500
-2.2000
-0.8500
    2.8500 3.9000 4.2500 5.4000 7.8800 6.9500 6.9500 5.4000 2.7500 2.6000 2.6300 3.3000 3.4100 3.7200 3.8000 3.7100 3.9250 4.4 000 4.4100 4.3600 4.3050 4.2300 4.4300 4.5000 4.2550 4.2500 4.2150 4.2200 4.7250 4.8100
    4.6150 4.6000 4.6500 4.7000 4.7000 4.8500 4.8000 5.0000 4.8000 4.9500 4.8000 4.9500 4.8000 4.9500 4.8000 4.9500 4.8000 4.9 500 4.8000 4.4800 4.4800 4.4800 4.4800 4.4800 4.7900 4.7900 4.6400 4.4800 4.8000 4.7900
4.7900
4.7900
4.7900
5.1000]
]]></description>
      <guid>https://stats.stackexchange.com/questions/654788/how-to-compare-private-equity-fund-irrs-across-time-while-accounting-for-varying</guid>
      <pubDate>Mon, 23 Sep 2024 15:48:33 GMT</pubDate>
    </item>
    <item>
      <title>使用 Lavaan 对二进制变量进行引导</title>
      <link>https://stats.stackexchange.com/questions/654787/bootstrapping-of-binary-variables-using-lavaan</link>
      <description><![CDATA[我是 R 的新用户，在使用我的模型进行引导时遇到了问题。
所有外生、内生和中介变量都是二元结果。我尝试按照解决方案添加 se = &quot;bootstrap&quot;, estimator = &quot;DWLS&quot;, verbose = TRUE，但仍然不起作用。
请就此提出您的建议和意见。感谢您的时间。
这是我的代码
model19 &lt;-&#39;ls ~ a1*participation1 + health_status
health_status ~ a2*participation1
hsu~ d1*ls + d2*health_status +c1*participation1 +e1*rural + e2*female +
e3*middle+e4*oldest+e5*primary+e6*secondary+e7*higher+e8*married+
e9*all+e10*employ+e11*support+e12*pension
indirect1:=a1*d1
indirect2:=a2*d2
Overallindirect:= indirect1+ indirect2
Total:=overallindirect +c1+e1+e2+e3+e4+e5+e6+e7+e8+e9+e10+e11+e12&#39;
fit19 &lt;- sem(model19, data = old_main, ordered = c(&quot;hsu&quot;, &quot;ls&quot;, &quot;health_status&quot;),
se = &quot;bootstrap&quot;, estimator = &quot;DWLS&quot;, verbose = TRUE)
summary(fit19, unified=TRUE, ci=TRUE, fit.measures=TRUE)
]]></description>
      <guid>https://stats.stackexchange.com/questions/654787/bootstrapping-of-binary-variables-using-lavaan</guid>
      <pubDate>Mon, 23 Sep 2024 15:16:57 GMT</pubDate>
    </item>
    <item>
      <title>加权后进行 g 计算时如何决定使用哪个样条曲线？</title>
      <link>https://stats.stackexchange.com/questions/654784/how-to-decide-which-spline-to-use-when-conducting-g-computation-after-weighting</link>
      <description><![CDATA[我已经进行了统计加权（R 包 WeightIt），现在我正在使用 g 计算（R 包边际效应）来估计树木覆盖率对作物产量的影响（遵循 https://ngreifer.github.io/WeightIt/articles/estimating-effects.html）。我不希望这种关系是线性的，所以我想使用自然样条曲线，但我不确定要指定多少 df，例如，df=4 和 df=2 会导致非常不同的结果和解释。进行模型选择（例如使用 AIC）感觉不太合适，因为它是一种预测技术，不适合因果推理。那么我该如何为我的样条曲线选择正确的 df 数量？]]></description>
      <guid>https://stats.stackexchange.com/questions/654784/how-to-decide-which-spline-to-use-when-conducting-g-computation-after-weighting</guid>
      <pubDate>Mon, 23 Sep 2024 14:43:42 GMT</pubDate>
    </item>
    <item>
      <title>多元线性回归——证明 B1=B2=B3=B4</title>
      <link>https://stats.stackexchange.com/questions/654782/multiple-linear-regression-proving-b1-b2-b3-b4</link>
      <description><![CDATA[给定 y = Bo +B1X1 +B2X2 +B3X3+ B4X4 + e
问题问：使用一般线性假设来展示如何测试：
a) Ho：B1=B2=B3=B4
b) Ho：B1=B2，B3=B4
给定照片中的解决方案，我正在寻找对上述问题部分 a) 和 b) 的解决方案的解释。
具体来说，在解决方案 a 和 b 中，此解决方案中的矩阵布局的原因是什么？
]]></description>
      <guid>https://stats.stackexchange.com/questions/654782/multiple-linear-regression-proving-b1-b2-b3-b4</guid>
      <pubDate>Mon, 23 Sep 2024 14:18:02 GMT</pubDate>
    </item>
    <item>
      <title>Box-Cox 变换和 t 检验 [重复]</title>
      <link>https://stats.stackexchange.com/questions/654779/box-cox-transformation-and-t-test</link>
      <description><![CDATA[我有两个数据集，希望测试它们的均值差异（T 检验）。但是，根据 R 中的 Shapiro 检验，它们不服从正态分布。
我已经看到 Box-Cox 变换是可行的方法，但我是否使用相同的 $\lambda$ 来转换数据集，还是单独转换它们？
即：
$$y_{i,\,transformed} = \begin{cases} 
\frac{y_i^\lambda-1}{\lambda} &amp; \lambda\neq 0 \\
\ln(y_i) &amp; \lambda = 0 
\end{cases}$$
对数据集 A 和数据集 B 使用一个 $\lambda$，还是使用两个 $\lambda$（$\lambda_A$ 和 $\lambda_B$）？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/654779/box-cox-transformation-and-t-test</guid>
      <pubDate>Mon, 23 Sep 2024 14:00:16 GMT</pubDate>
    </item>
    <item>
      <title>与其他架构相比，lstms 为 Apple 的语言识别提供了哪些优势？</title>
      <link>https://stats.stackexchange.com/questions/654778/what-advantage-do-lstms-provide-for-apples-language-identification-over-other-a</link>
      <description><![CDATA[既然 LSTM 的强大功能来自其长程依赖性记忆，那么为什么我们要使用 LSTM 而不是其他架构来从短文本字符串中进行基于字符的语言识别 (LID)？
例如，Apple 发布了一篇行业博客文章，指出他们使用 biLSTM 进行语言识别：https://machinelearning.apple.com/research/language-identification-from-very-short-strings
然后这篇论文试图复制它：https://aclanthology.org/2021.eacl-srw.6/
我在阅读 Karpathy 关于 RNN 的著名文章时，尝试训练一个小型语言识别模型进行练习。我首先尝试了一种简单、直观（对我来说）的方法：使用 tf-idf，使用在训练数据中的双或三元计数上训练的朴素贝叶斯分类器。我的数据集包含不同语系的 13 种语言。虽然我的简单分类器确实表现良好，但在查看类似语言时会出错。例如，西班牙语通常被归类为葡萄牙语。
我研究了神经网络架构，发现 LSTM 经常用于语言识别任务。在阅读了有关 RNN 和 LSTM 的内容后，我无法完全理解为什么 LSTM 更适合用于 LID，尤其是短文本字符串。这不是违反直觉的吗，因为 LSTM 擅长记住长距离依赖关系，而 RNN 则不然？对于短文本字符串，我建议使用 vanilla RNN....
Apple 博客确实说过：

在本文中，我们探讨了如何通过将其视为字符级别的序列标记问题，并使用在短字符序列上训练的双向长短期记忆 (bi-LSTM) 神经网络来提高 LID 准确性。

我觉得我没有理解这里的一些基本知识。
那么，他们的 LSTM 的学习目标是否是正确分类给定的字符 n-gram？这就是他们所说的“序列标记”问题吗？序列标记任务的根本难道不就是分类任务吗（“用 N 个预定义标签中的 1 个标记来自测试集的给定输入”）？
当您使用已知可以处理长序列的架构时，在短字符序列上训练 LSTM 有什么意义？]]></description>
      <guid>https://stats.stackexchange.com/questions/654778/what-advantage-do-lstms-provide-for-apples-language-identification-over-other-a</guid>
      <pubDate>Mon, 23 Sep 2024 13:52:29 GMT</pubDate>
    </item>
    <item>
      <title>矩阵的期望值=期望值矩阵？</title>
      <link>https://stats.stackexchange.com/questions/654777/expected-value-of-a-matrix-matrix-of-expected-value</link>
      <description><![CDATA[我对以下陈述感到疑惑：矩阵的期望等于期望矩阵。
例如，假设 A 是一个由 4 个随机变量组成的矩阵，W、X、Y、Z
那么，我们通常写为 E(A) = E(R.V.) 矩阵
这总是正确的吗？
R.V. 中的一些/全部是相关的吗？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/654777/expected-value-of-a-matrix-matrix-of-expected-value</guid>
      <pubDate>Mon, 23 Sep 2024 13:26:45 GMT</pubDate>
    </item>
    <item>
      <title>两个比率指标之间的变异系数</title>
      <link>https://stats.stackexchange.com/questions/654776/coefficient-of-variation-between-two-ratio-metrics</link>
      <description><![CDATA[我想比较哪个指标更稳定（每次展示费用与每次视频观看费用）。
我使用了 CV（变异系数），并寻找了在使用不同货币的多个广告系列中，对于同一广告系列，哪个指标 CV 较低。这种方法合适吗？数据不呈正态分布有关系吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/654776/coefficient-of-variation-between-two-ratio-metrics</guid>
      <pubDate>Mon, 23 Sep 2024 13:24:12 GMT</pubDate>
    </item>
    <item>
      <title>如何控制模型中的观测次数？</title>
      <link>https://stats.stackexchange.com/questions/654771/how-control-for-the-number-of-observations-in-the-models</link>
      <description><![CDATA[假设我们有两个时间序列，即 $Y^{(1)}$ 和 $Y^{(2)}$，分别具有 $n^{(1)}&gt;&gt;n^{(2)}$ 个观测值（换句话说，$Y^{(1)}$ 比 $Y^{(2)}$ 具有更多的观测值）。对于每个时间序列，我们估计一个 AR(1) 模型，然后我们的目标是比较系数的大小：
$$Y^{(1)}_{t}=\alpha^{(1)}+\beta^{(1)}Y^{(1)}_{t-1}+\epsilon^{(1)}_{t},$$
$$Y^{(2)}_{t}=\alpha^{(2)}+\beta^{(2)}Y^{(2)}_{t-1}+\epsilon^{(2)}_{t},$$
我们想要确定 $sgn(\hat{\beta}^{(1)}-\hat{\beta}^{(2)})$。
这个简单问题的复杂性在于，观测值的数量差异很大，并且它产生了 AR(1) 系数估计值之间的一致差异。我估计了几千对时间序列的 AR(1) 模型，并通过统计证实，在其他条件相同的情况下，时间序列长度较短的时间序列的系数小于时间序列长度较长的时间序列（该结果是通过回归和随机森林实现的）。对于上面的这一对，我们有 $|\hat{\beta}^{(1)}&gt;\hat{\beta}^{(2)}|$。因此，我想在进行任何比较之前以某种方式调整 AR(1) 系数。
我想到的一个方法是根据标准误差调整相应的系数，因为后者考虑了观测次数。换句话说，该方法是计算 $\hat{\beta}^{(1)}/SE^{(1)}$ 和 $\hat{\beta}^{(2)}/SE^{(2)}$，然后确定 $sgn(\hat{\beta}^{(1)}/SE^{(1)} - \hat{\beta}^{(2)}/SE^{(2)})$。但是，这种方法并不有效，因为标准误差（可以预料）对于较长的时间序列来说较小，并且小于 1。因此，相对于 $\hat{\beta}^{(1)}$，$\hat{\beta}^{(1)}/SE^{(1)}$ 的增加幅度远远大于 $\hat{\beta}^{(2)}/SE^{(2)}$ 与 $\hat{\beta}^{(2)}$ 相比，这意味着我们进一步增加了系数之间的差距，但 $sgn(\cdot)$ 函数仍然产生相同的结果结果。
问题：

您将使用什么方法根据观察次数调整 AR(1) 系数？

您将如何改进/修改我在此处描述的方法，以使其可行且有效？

]]></description>
      <guid>https://stats.stackexchange.com/questions/654771/how-control-for-the-number-of-observations-in-the-models</guid>
      <pubDate>Mon, 23 Sep 2024 11:42:34 GMT</pubDate>
    </item>
    <item>
      <title>Tobit 模型正态性假设</title>
      <link>https://stats.stackexchange.com/questions/654764/tobit-model-normality-assumption</link>
      <description><![CDATA[对于 tobit 模型，我对正态性假设有点困惑。是不是我的因变量必须服从正态分布？还是我的模型的潜在残差必须服从正态分布？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/654764/tobit-model-normality-assumption</guid>
      <pubDate>Mon, 23 Sep 2024 09:07:10 GMT</pubDate>
    </item>
    <item>
      <title>均值较小的逆高斯分布具有不可靠的样本均值</title>
      <link>https://stats.stackexchange.com/questions/654737/inverse-gaussian-with-small-mean-has-unreliable-sample-mean</link>
      <description><![CDATA[考虑一个平均值为$\mu$（如$0.001$）的逆高斯分布，我们将其方差固定为一个较大的值$\sigma^2$，比如说$0.5$。然后，如果我从该分布中抽取 $N$ 次样本，我预计样本平均值约为 $\mu$，但除非 $N$ 的值非常大，否则这种情况不会发生。
import scipy as sp

N = 1_000 # 我们至少需要使用 10_000_000，否则它会非常小，比如 1e-6 
mean = 0.001
var = 0.5
lmbda = mean**3 / var

print(f&quot;样本平均值：{sp.stats.invgauss.rvs(mu=(mean/lmbda), loc=0, scale=lmbda, size=N).mean()}&quot;)

如果您运行此脚本N 等于 1000、10_000、1_000_000，您将得到大约 1e-6，其数量级与均值的平方相同。

为什么会出现这种情况，我该如何避免？即，我可以从均值较小、方差相对较大的逆高斯中采样，而不会产生数值问题吗？
]]></description>
      <guid>https://stats.stackexchange.com/questions/654737/inverse-gaussian-with-small-mean-has-unreliable-sample-mean</guid>
      <pubDate>Sun, 22 Sep 2024 19:23:53 GMT</pubDate>
    </item>
    <item>
      <title>如何比较嵌套预测模型的泛化误差？</title>
      <link>https://stats.stackexchange.com/questions/654647/how-to-compare-generalization-errors-of-nested-predictive-models</link>
      <description><![CDATA[假设我想比较两个嵌套预测模型，例如具有和不具有特定预测因子 Z 的基于 Lasso 回归的预测算法。在第一个预测算法中，Z 在变量选择中不可用，而在第二个算法中，它将作为“新变量”可用，并在有用时包含在模型中。
为了评估将 Z 添加到预测因子集合的有用性，我报告了两种算法的泛化（又称样本外）预测误差。由于第二种算法（Z 可用）只是第一种算法（Z 不可用）的扩展，即第一种算法“嵌套”在第二种算法中，因此第二种算法的泛化误差必然小于或等于第一种算法。为了估计泛化误差，我使用嵌套交叉验证 (CV)或乐观调整引导法。
我的问题是：
我应该如何确定第二个广义预测误差是否实际上更小，
从而表明新变量 Z 的预测潜力？特别是：

我可以根据引导样本（遗漏的数据）或 CV 中的保留测试集来计算泛化误差估计的标准误差吗？在我看来，为每个 bootstrap 遗漏样本（或 CV 中的每个测试折叠）计算 2 个算法之间的性能指标差异（例如 RMSE），然后报告这些差异的平均值及其标准误差是有意义的。

报告标准误差会让读者相信观察到的差异不是由于 bootstrap 样本太少或交叉验证重复导致的随机波动。但是，我认为对差异进行正式测试是错误的，因为通过增加 bootstrap 样本（或 CV 重复）的数量，我总是可以任意地使标准误差变小。但我仍然可以争辩说，例如，2 个算法之间的 RMSE 差异为 0.01，置信区间为 95%（0.007, 0.015）。然后读者会相信我有足够的重复次数，然后取决于应用程序，0.01 的差异在实践中有多大意义。您认为这有道理吗？


我认为这是比较嵌套预测模型时的一个重要问题，但似乎经常被忽视。通常，报告和比较泛化误差时不会考虑估计中的不确定性，我发现这通常不够充分。我认为假设已经使用了足够数量的重复，但人们仍然希望看到对泛化误差不确定性的评估。]]></description>
      <guid>https://stats.stackexchange.com/questions/654647/how-to-compare-generalization-errors-of-nested-predictive-models</guid>
      <pubDate>Fri, 20 Sep 2024 10:53:52 GMT</pubDate>
    </item>
    </channel>
</rss>