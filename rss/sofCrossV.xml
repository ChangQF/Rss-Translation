<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 03 Dec 2024 21:16:53 GMT</lastBuildDate>
    <item>
      <title>探索逻辑回归中的所有选项</title>
      <link>https://stats.stackexchange.com/questions/658233/exploring-all-options-in-a-logistic-regression</link>
      <description><![CDATA[这组代码相当简单，使用了在线教程中的一些示例
# 导入并重命名数据集
library(kmed)
dat &lt;- heart
library(dplyr)

# 重命名变量
dat &lt;- dat |&gt;
重命名（
胸痛 = cp，
最大心率 = thalach，
心脏病 = 类
）

# 重新编码性别
dat$sex &lt;- factor(dat$sex，
levels = c(FALSE, TRUE)，
labels = c(&quot;female&quot;, &quot;male&quot;)
)

# 重新编码胸痛
dat$chest_pain &lt;- factor(dat$chest_pain，
levels = 1:4，
labels = c(&quot;典型心绞痛&quot;, &quot;非典型心绞痛&quot;, &quot;非心绞痛&quot;, &quot;无症状&quot;)
)

# 将心脏病重新编码为 2 个类别
dat$heart_disease &lt;- ifelse(dat$heart_disease == 0,
0,
1
)

m3 &lt;- glm(heart_disease ~ .,
data = dat,
family = &quot;binomial&quot;
)

# 打印结果
summary(m3)

但是，如果我想自动运行 dat 中所有列的预测变量，或者自动寻找最高的 AIC 模型，我应该用什么呢？]]></description>
      <guid>https://stats.stackexchange.com/questions/658233/exploring-all-options-in-a-logistic-regression</guid>
      <pubDate>Tue, 03 Dec 2024 21:08:08 GMT</pubDate>
    </item>
    <item>
      <title>哪个伪 R 平方值最适合 GLS 模型？</title>
      <link>https://stats.stackexchange.com/questions/658232/which-pseudo-r-squared-value-is-best-for-gls-models</link>
      <description><![CDATA[我需要帮助来理解一些统计数据。我是初学者。
我应用了 GLS 模型（具有空间相关结构，例如 CorExp）。对于我的 GLS 模型，我使用 R 中的 nagelkerke 函数计算了伪 R 平方值。此函数生成三种类型的伪 R 平方值：1. McFadden，2. Cox 和 Snell (ML)，3. Nagelkerke (Cragg 和 Uhler)。
这些值中的每一个在我的结果中都有很大的不同。我不知道哪一个最适合在 GLS 模型的特定环境中使用（以及我应该在结果中报告哪一个）。
感谢您的阅读，
L。]]></description>
      <guid>https://stats.stackexchange.com/questions/658232/which-pseudo-r-squared-value-is-best-for-gls-models</guid>
      <pubDate>Tue, 03 Dec 2024 20:28:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 cox 模型计算风险</title>
      <link>https://stats.stackexchange.com/questions/658231/calculating-hazard-using-cox-model</link>
      <description><![CDATA[我想计算 5 名男性和 3 名女性的数据中男性吸烟者的风险：3 名男性吸烟，2 名不吸烟。假设 z1=0（女性）且 z1=1（男性）。z2=0（吸烟者）且 z2=1（非吸烟者），B1j 为 0.35，B2j 为 0.33。我应该怎么做？]]></description>
      <guid>https://stats.stackexchange.com/questions/658231/calculating-hazard-using-cox-model</guid>
      <pubDate>Tue, 03 Dec 2024 20:19:33 GMT</pubDate>
    </item>
    <item>
      <title>进行多项方差分析时，我需要调整 p 值吗？</title>
      <link>https://stats.stackexchange.com/questions/658228/when-doing-multiple-anovas-do-i-need-to-adjust-the-p-values</link>
      <description><![CDATA[我测量了 80 个人的 12 个血液参数，并且我知道性别（F/M）、季节（开始/结束）和地区（A/B）。所以我有 12 个连续因变量和 3 个自变量，每个变量都有 2 个水平。如果我对每个因变量进行三向方差分析（-&gt; 12 个方差分析），我是否需要调整 p 值？如果需要，最好的方法是什么？我可以想到 Bonferroni 校正，但这意味着 0.05/12 ~ 0.004，这真的很低。
首先我做了 MANOVA（并且得到了季节和地区显著性），但后来我意识到我想知道哪些因素影响每个单独的参数，所以我假设我必须进行单独的方差分析。
我还得出结论，事后检验是没有意义的，因为所有因素只有 2 个水平，但我现在很困惑，所以我会问一下以确保完全确定。谢谢:)]]></description>
      <guid>https://stats.stackexchange.com/questions/658228/when-doing-multiple-anovas-do-i-need-to-adjust-the-p-values</guid>
      <pubDate>Tue, 03 Dec 2024 19:56:13 GMT</pubDate>
    </item>
    <item>
      <title>减少线性指数衰减混合函数中的极端行为</title>
      <link>https://stats.stackexchange.com/questions/658227/reducing-extreme-behavior-in-linear-exponential-decay-hybrid-function</link>
      <description><![CDATA[我正在用 Python 编写用于残余气体分析质谱的数据缩减软件。该软件的主要目标之一是将测量的气体强度$y$与气体平衡时间$t=0$进行拟合，以确定理论平衡强度$y_0$。
我目前正在努力拟合一个指数线性方程来描述气体消耗：
$$y = a \exp(-pt) + bt + c$$
其中：

$y$是因变量，强度在 A 中
$t$是自变量，时间在$t=0$
$a$、$b$、$c$ 和 $p$ 是拟合参数

以下是一些“良好”的例子消耗曲线拟合：

其中数据为：
质量 2 amu y：[1.03151253e-10 1.02764656e-10 1.00429739e-10 9.97269907e-11
9.89678330e-11 9.94845128e-11 9.73211157e-11 9.67399363e-11
9.69838881e-11 9.57109351e-11 9.54086215e-11 9.57198459e-11
9.54274476e-11 9.59283426e-11 9.53714218e-11 9.29636333e-11
9.44152076e-11 9.35152455e-11 9.32533980e-11 9.30672260e-11]
质量 2 amu t：[7.028 14.238 21.45 28.667 35.874 43.077 50.384 57.389 64.593
71.901 79.01 86.215 93.419 100.627 107.831 115.035 122.243 129.354
136.657 143.768]

质量 3 amu y：[3.29778253e-10 3.26265256e-10 3.20505639e-10 3.16877521e-10
3.16086483e-10 3.13754743e-10 3.11454306e-10 3.09359856e-10
3.08538478e-10 3.06927955e-10 3.05987101e-10 3.03379296e-10
3.00944658e-10 3.02971263e-10 3.01609542e-10 2.99715203e-10
2.99172588e-10 2.97572855e-10 2.97728968e-10 2.97632036e-10]
质量 3 amu t：[ 8.116 15.326 22.538 29.755 36.962 44.165 51.472 58.477 65.681
72.989 80.098 87.303 94.507 101.715 108.919 116.123 123.331 130.442
137.745 144.856]

以及一些非物理或不切实际的“过度”示例拟合：

Mass 2 amu y：[8.92991171e-11 8.76522093e-11 8.61052948e-11 8.70397469e-11
8.71706708e-11 8.68983223e-11 8.70813401e-11 8.68469981e-11
8.60065722e-11 8.68007260e-11 8.69227029e-11 8.73408691e-11
8.64896431e-11 8.70329228e-11 8.60186671e-11 8.62989490e-11
8.67176164e-11 8.59189454e-11 8.64824077e-11 8.64498292e-11]
质量 2 amu t：[3.529 10.636 17.841 25.047 32.259 39.469 46.676 53.882 60.986
68.191 75.399 82.611 89.816 97.023 104.226 111.335 118.54 125.745
132.95 140.157]

或者更糟，像这样：

Mass 2 amu y：[9.37982279e-11 9.12279184e-11 9.11408812e-11 9.16890908e-11
 9.11226495e-11 9.11011735e-11 9.09514699e-11 9.17458313e-11
 9.07343618e-11 9.07010068e-11 8.98410055e-11 9.09835375e-11
 9.19705903e-11 9.08708285e-11 9.18843137e-11 9.03661067e-11
 8.99796615e-11 9.12228596e-11 9.02898722e-11 9.06567840e-11]
质量 2 amu t：[5.326 12.433 19.639 26.849 34.058 41.267 48.475 55.577 62.782
69.987 77.294 84.496 91.7 98.909 106.022 113.129 120.335 127.54
134.861 141.971]

我正在寻找一种可以缓解这种极端指数行为的技术，这种技术并非：
a. 任意。也就是说，我可以对 $a$ 强制实施 看起来合理的边界，限制极端行为。但是，除非该边界是物理上已知的（这是不可能的），否则这是“绘制”而不是数学；如果我可以在合理范围内的任何地方“绘制”截距，这不是找到截距的真正解决方案。
b. 迭代。我知道重采样/引导等技术可以减轻单个异常值对拟合的影响，但是，每个序列由数十到数百个分析组成，每个分析都有四条需要拟合的强度曲线。对每个分析的每次拟合进行迭代以收敛可能需要太长时间。
是否有一种技术可以解决极端拟合的问题，而不会牺牲统计严谨性，但每次拟合也不需要数万次迭代？如果没有，我最好选择相对快速的迭代解决方案是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/658227/reducing-extreme-behavior-in-linear-exponential-decay-hybrid-function</guid>
      <pubDate>Tue, 03 Dec 2024 19:48:05 GMT</pubDate>
    </item>
    <item>
      <title>ROC 曲线阈值与预测变量的平均值一致</title>
      <link>https://stats.stackexchange.com/questions/658226/roc-curve-threshold-coincides-with-mean-of-the-predicted-variable</link>
      <description><![CDATA[我正在将一堆逻辑回归模型拟合到某个数据集，其中要预测的变量都是二进制的。拟合模型后，我运行了一些简单代码，使用 ROC 曲线为每个模型找到最佳阈值，我注意到我获得的所有阈值都与预测变量的平均值相吻合（即对于变量 $y_i$，如果其中 $16\%$ 为 1，其余为 0，则 ROC 曲线方法建议的阈值为 $0.16$，其他的也是如此）。
我还拟合了其他模型，您可以通过设置每个类的权重来克服数据不平衡，例如随机森林或 XGB；以及 ROC 曲线中针对多样性较高的情况所建议的阈值。所以我的问题是：在 Logit 模型中，ROC 曲线的最佳阈值是否始终与预测变量的平均值相同？如果是这样，为什么会发生这种情况？（可能只是一些关于它为什么有效的直觉）。最后，还有其他模型会发生这种情况吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658226/roc-curve-threshold-coincides-with-mean-of-the-predicted-variable</guid>
      <pubDate>Tue, 03 Dec 2024 19:43:36 GMT</pubDate>
    </item>
    <item>
      <title>面板数据集是否可以由随机时间点抽样的单位组成？</title>
      <link>https://stats.stackexchange.com/questions/658225/can-a-panel-dataset-consist-of-units-sampled-at-random-points-in-time</link>
      <description><![CDATA[我有一个数据框，其中包含变量 $Judge\ ID$（唯一标识法官）、$Case\ ID$（唯一标识法院案件）、$Decision$（记录案件结果）和 $Comp\ Date$（指定案件完成时间的日期时间变量）。数据集中的每个观察值都对应一个唯一的案件。有些法官比其他法官监督更多的案件，而且一般来说，案件不会在同一时间和日期完成。这是一个不平衡面板数据集吗？我知道不平衡面板数据的定义是，至少有一个面板单位（例如法官）不是每个时期都观察到的。但是，在这个数据集中，由于$Comp_ Date$记录的是日期时间，而日期时间是一个连续变量，所以周期数是不确定的。如果这个数据框不是不平衡面板数据，那么它是什么类型的统计数据？]]></description>
      <guid>https://stats.stackexchange.com/questions/658225/can-a-panel-dataset-consist-of-units-sampled-at-random-points-in-time</guid>
      <pubDate>Tue, 03 Dec 2024 19:33:42 GMT</pubDate>
    </item>
    <item>
      <title>mgcv 平滑的点约束</title>
      <link>https://stats.stackexchange.com/questions/658224/point-constraints-for-mgcv-smooths</link>
      <description><![CDATA[这个问题已经在以下之前的帖子中提出，但我怀疑那里给出的答案是否令人满意：Constaint on te() tensor product gam mgcv
对于 mgcv 用户来说，问题是：当使用 d 关键字修改了 mgcv 包中的平滑边界条件时，如何正确实现这些边界条件？
这里有一些例子来详细说明这个问题，并展示我到目前为止所尝试的
示例 1：
model &lt;- mgcv::bam(Y ~ s(x, y, k = 500, pc = c(0, 0)) +
s(z, k = 5, pc = 0) +
ti(x, y, z, d = c(2,1), k = c(20, 5), pc = c(0, 0, 0)),
data = data,
family = tw(link=log),
method = &quot;fREML&quot;)

示例 2：
model &lt;- mgcv::bam(Y ~ s(x, y, k = 500, pc = c(0, 0)) +
s(z, k = 5, pc = 0) +
ti(x, y, z, d = c(2,1), k = c(20, 5), pc = list(c(0, 0), c(0)),
data = data,
family = tw(link=log),
method = &quot;fREML&quot;)

示例3：
模型 &lt;- mgcv::bam(Y ~ s(x, y, k = 500, pc = c(0, 0)) +
s(z, k = 5, pc = 0) +
ti(x, y, z, d = c(2,1), k = c(20, 5), pc = list(list(0, 0), list(0)),
data = data,
family = tw(link=log),
method = &quot;fREML&quot;)

示例 4：
模型 &lt;- mgcv::bam(Y ~ s(x, y, k = 500, pc = c(0, 0)) +
s(z, k = 5, pc = 0) +
ti(x, y, z, d = c(2,1), k = c(20, 5), pc = list(0,0,0),
data = data,
family = tw(link=log),
method = &quot;fREML&quot;)

.. 但它们都会导致错误
if (length(pc) &lt; d) stop(&quot;supply a value for each variable for a point constrain&quot;) 中的错误：条件的长度 &gt; 1

仔细检查表明，它与 d 的指定有关，即 d=c(2,1)。有一种解决方案是切换到 d = 3。事实上，这就是上一篇文章中所建议的。然而，这并不令人满意，因为我想利用前两个坐标中的各向同性。]]></description>
      <guid>https://stats.stackexchange.com/questions/658224/point-constraints-for-mgcv-smooths</guid>
      <pubDate>Tue, 03 Dec 2024 19:16:16 GMT</pubDate>
    </item>
    <item>
      <title>标题：处理高维对数似然的倾斜重要性抽样权重</title>
      <link>https://stats.stackexchange.com/questions/658223/title-handling-skewed-importance-sampling-weights-for-high-dimensional-log-like</link>
      <description><![CDATA[问题：
我正在使用以下设置对贝叶斯推理问题执行重要性抽样 (IS)：
1. 数据和模型

我的数据有 (D = 1300) 个维度。
对数似然 $ \log p(x \mid \theta) $ 是根据多元正态分布计算得出的，结果为非常负的值（例如 (-3000) 或更低）。

2. 先验

先验 $ \log p(\theta)$ 是 (P = 31) 个参数的均匀分布。

3.后验

我有一个后验提议分布，$\pi(\theta)$，用于生成 ( N = 85,000 ) 个样本。

4.重要性抽样

对于后验提议中的每个样本 $ \theta_i $：

计算对数权重：
$$
\log w_i = \log p(x \mid \theta_i) + \log p(\theta_i) - \log \pi(\theta_i)
$$
使用对数和指数技巧对权重进行归一化：
$$
\hat{w}_i = \frac{\exp(\log w_i)}{\sum_{j=1}^{N} \exp(\log w_j)}
$$
评估有效样本量 (ESS):
$$
\text{ESS} = \frac{(\sum_{j=1}^{N} \hat{w}_j)^{2}}{\sum_{j=1}^{N} \hat{w}_j^2}
$$



5.问题

权重高度倾斜，导致 ESS 接近 1。即使使用具有更平坦可能性的参数，后验峰值也非常窄。
IS 后验预测与后验提议分布重叠（例如，在 $ 1\sigma $ 内），表明提议没有严重不匹配。
我需要使用重要性抽样来修剪后验，因为我的后验估计平滑了几个有间隙的退化区域，导致后验预测非常宽。

6.失败的尝试

我尝试转移权重，使用：
$$
\log w_i^\text{shifted} = \log w_i - \max(\log w)
$$
但这并没有减少方差。只会使处理负值变得更容易。
削减权重似乎不科学，并会引入偏差。


问题：
如何在不引入偏差的情况下降低重要性抽样权重的偏斜度并提高有效样本量 (ESS)？
具体来说：

是否有系统的技术可以解决这个问题？
如何确保 IS 后验正确地代表真实的后验，而不会过度缩小后验？

我的目标是使用重要性抽样来改进我的后验估计，同时保持科学严谨的方法。]]></description>
      <guid>https://stats.stackexchange.com/questions/658223/title-handling-skewed-importance-sampling-weights-for-high-dimensional-log-like</guid>
      <pubDate>Tue, 03 Dec 2024 19:15:13 GMT</pubDate>
    </item>
    <item>
      <title>过去考试的问题</title>
      <link>https://stats.stackexchange.com/questions/658221/question-from-past-exam</link>
      <description><![CDATA[本周五我有一场统计学考试，我遇到了这道题：
David 怀疑几年前卡梅尔山的大火导致鹿的出生率下降。他从卡梅尔山采集了 10 只鹿的样本，平均出生率为 10。他知道鹿的出生率呈正态分布，均值为 12，标准差为 2。他还阅读了有关环境急剧变化后鹿的出生率的文献。他发现这群鹿的均值为 10.5，标准差为 2，仍然呈正态分布。如果 David 假设他的样本中的鹿可能
与环境急剧变化后的鹿群相似，并且
他决定采用 α=0.05，那么他的检验功效是多少？

功效 = 0.2327
功效 = 0.8133
功效 = 0.1867
功效 = 0.7673

我不明白如何解决这个问题 - 这里的零假设和备择假设是什么？
谢谢大家！]]></description>
      <guid>https://stats.stackexchange.com/questions/658221/question-from-past-exam</guid>
      <pubDate>Tue, 03 Dec 2024 18:50:12 GMT</pubDate>
    </item>
    <item>
      <title>给定许多图游走的总成本，如何估算每条边的成本？</title>
      <link>https://stats.stackexchange.com/questions/658217/given-the-total-cost-of-many-graph-walks-how-to-estimate-the-cost-of-each-edge</link>
      <description><![CDATA[我有一个实际问题，其中我有一个节点及其边的集合。这个集合由数百个节点和数千个连接组成。然后我有大约 10 K 个数据点，每个数据点代表此图中采用的一条路径和每条路径的总成本（每条路径中经过的边的成本总和）。我正在寻找一个函数，给定路径，估算每条边的成本。
现实世界的情况有点复杂，因为模型需要考虑其他特征，例如一天中的时间，以及每个节点的输入/输出，所以我猜它需要一些强化学习管道，但任何时候的任务主要取决于我上面定义的算法，而且它看起来相当简单。
这个问题容易解决吗？文献中是否有任何算法可以解决它？因为我在整整一天的研究中都没有找到一个。]]></description>
      <guid>https://stats.stackexchange.com/questions/658217/given-the-total-cost-of-many-graph-walks-how-to-estimate-the-cost-of-each-edge</guid>
      <pubDate>Tue, 03 Dec 2024 15:58:08 GMT</pubDate>
    </item>
    <item>
      <title>在多级 LCA 中提取个体级后验类别成员概率</title>
      <link>https://stats.stackexchange.com/questions/658216/extracting-individual-level-posterior-class-memebership-probabilities-in-multile</link>
      <description><![CDATA[我正在使用 R 包 multilevLCA 进行多级潜在类别分析。
我已经使用多个步骤（即确定最佳类别数和聚类数）拟合了模型。现在我想使用 mPMsumX 命令提取在边缘化高级类别（比例分配）之后的低级单元的后验低级类别分配。我可以将其提取为数据框，但似乎无法为其添加唯一标识符，因此之后无法将其合并到我的数据集中。
我觉得自己缺少了一些非常基本的东西，但我已经尝试了我所知道的一切，但到目前为止没有结果。提取的数据框中的行与我用于派生模型的原始数据集的顺序不同（因此简单地添加行号作为 ID 是行不通的）。
有人能指出我在这里遗漏了什么吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658216/extracting-individual-level-posterior-class-memebership-probabilities-in-multile</guid>
      <pubDate>Tue, 03 Dec 2024 15:45:05 GMT</pubDate>
    </item>
    <item>
      <title>如何手动计算自相关</title>
      <link>https://stats.stackexchange.com/questions/658213/how-to-calculate-autocorrelation-manually</link>
      <description><![CDATA[我学过滞后 $k$ 的时间序列中的自相关是所有以此滞后为间隔的值对之间的相关性。
假设我想试一试，并手动计算滞后 1。
模拟一些白噪声
set.seed(123)
TS &lt;- ts(rnorm(1e3))

然后手动计算自相关并使用内置函数。
calc_autocorrelation_manually &lt;- function(x) { cor(x[-1], x[-length(x)]) }

&gt; calc_autocorrelation_manually(TS)
[1] -0.02741628
&gt; 
&gt; acf(TS, lag = 1, plot = F)

序列‘TS’的自相关，按滞后

0 1 
1.000 -0.027

两者给出相同或几乎相同的结果（我知道 acf() 中的计算并不完全相同，因为它不使用偏差调整。不过，我认为这不会产生实质性差异）。
但是，如果我对模拟 AR(1) 过程执行此操作，结果就不一样了！例如，使用没有噪音的“理想”AR(1)：
&gt; TS &lt;- ts(99999) # 初始值
&gt; for (i in 1:350) { TS &lt;- c(TS, TS[i] * 0.3) }
&gt; 
&gt; calc_autocorrelation_manually(TS)
[1] 1
&gt; 
&gt; acf(TS, lag = 1, plot = F)

序列‘TS’的自相关，按滞后

0 1 
1.0 0.3 

我认为手动计算自相关是不正确的，但正确的方法是什么？为什么在第一个例子中它仍然运行良好？]]></description>
      <guid>https://stats.stackexchange.com/questions/658213/how-to-calculate-autocorrelation-manually</guid>
      <pubDate>Tue, 03 Dec 2024 15:05:35 GMT</pubDate>
    </item>
    <item>
      <title>修正二项式 GLMM 以反映物种出现的时间趋势</title>
      <link>https://stats.stackexchange.com/questions/658211/correct-binomial-glmm-for-temporal-trends-in-species-occurrences</link>
      <description><![CDATA[我有一个包含 80 个物种的数据集，这些物种是在两个时间段（历史/最近）从大约 120 个水体中采样的。仅考虑每个水体中物种的存在/不存在。长格式的数据如下所示：
water_no_name 物种 成功时间 
水体 1 物种 A 0 t1 
水体 2 物种 A 0 t1 
水体 3 物种 A 1 t1 
水体 4 物种 A 0 t1 
水体 5 物种 A 0 t1 
水体 1 物种 A 1 t2 
水体 2 物种 A 0 t2 
水体 3 物种 A 0 t2 
水体 4 物种 A 0 t2 
水体 5 物种 A 0 t2 
水体 1 物种 B 1 t1 
水体 2 物种 B 0 t1 
水体 3 物种 B 0 t1 
水体 4 物种 B 1 t1 
水体 5 物种 B 1 t1 
水体 1 物种 B 1 t2 
水体 2 物种 B 0 t2 
水体 3 物种 B 1 t2 
水体 4 物种 B 1 t2 
水体 5 物种 B 0 t2 

我想检查哪些物种的频率随时间显著增加或减少。
因此，我决定使用水体（= 地点）作为随机因素来计算二项式 GLMM，以解释重复测量设计。
我不确定这两种方法中的哪一种更合适：

为每个物种计算单独的 GLMM：
mod &lt;- glmer(success ~ time + (1 | water_no_name), family = binomial)
使用这种方法，我将对物种数量进行 p 值校正 (FDR)。

使用时间和物种之间的相互作用计算所有物种的组合 GLMM：
mod &lt;- glmer(success ~ time * species + (1 | water_no_name), family = binomial)
为了分析相互作用，我将使用这种方法进行事后分析，以查看对比 t1/t2 对哪些物种有意义：
emmeans(mod, pairwise ~ time |物种)


在计算这两种模型时，结果大致相似，但计数较低的物种会有一些差异。
我理解，一个主要的区别是，复杂模型 (2) 对所有物种使用相同的随机位点效应，而使用单独的模型 (1) 我会为每个物种获得不同的随机位点效应（参见 几个单物种 GLMM -&gt; 一个多物种 GLMM（随机效应语法？））
我仍然不确定哪种方法更适合我的情况。我发现更简单的方法 (1) 更直观地解释，也足以回答我的问题。我也不确定是否应该让所有物种都具有随机站点效应。
有人能给我建议我应该选择哪种方法吗？
或者你推荐另一种方法？]]></description>
      <guid>https://stats.stackexchange.com/questions/658211/correct-binomial-glmm-for-temporal-trends-in-species-occurrences</guid>
      <pubDate>Tue, 03 Dec 2024 14:44:29 GMT</pubDate>
    </item>
    <item>
      <title>解释 A/B 测试结果中的 p 值</title>
      <link>https://stats.stackexchange.com/questions/658204/interpret-p-value-in-a-b-test-results</link>
      <description><![CDATA[我需要解释 A/B 测试结果。因此，我有一个名为 Baseline 的控制队列，还有另外两个名为 NewFTUE 和 AppReopenFS 的队列。
此 AB 测试数据的最大问题是 NewFTUE 的 p 值几乎等于 1，而 AppReopenFS 的 P 值要低得多。
我无法理解两件事：

据我所知，如果平均差异较大且
标准差较小，则 p 值应该变得更小，即零假设应该更容易被拒绝。但我们可以
在这里看到，对于较大的标准差和较小的均值差，p 值要小得多（0.187 vs 0.991）
第二件我无法理解的事情是 95% CI（正如仪表板上所解释的那样，这是“95% 可能包含均值真实差异的值范围”。）。因此，正如您所看到的，对于 NewFTUE，均值差异的置信区间完全位于 0 的左侧。即我们有 95% 的信心，无论我们采用什么样本（对于相同的样本量），与 Baseline 相比，我们都会得到更差的结果。

那么为什么 NewFTUE 的 p 值为 0.991 并且大于 AppReopenFS 的 0.187 值？

编辑：添加以下 p 值的描述：和 这里是文档页面。
]]></description>
      <guid>https://stats.stackexchange.com/questions/658204/interpret-p-value-in-a-b-test-results</guid>
      <pubDate>Tue, 03 Dec 2024 13:44:27 GMT</pubDate>
    </item>
    </channel>
</rss>