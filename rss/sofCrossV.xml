<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 01 Feb 2025 06:21:49 GMT</lastBuildDate>
    <item>
      <title>当我没有真实标签时，如何评估模型（预先训练的 BERT 和 VADER 进行情绪分析）？</title>
      <link>https://stats.stackexchange.com/questions/660839/how-can-i-evaluate-model-pre-trained-bert-and-vader-for-sentiment-analysis-whe</link>
      <description><![CDATA[我有一个包含文本的数据集，我使用预先训练的 BERT 和 VADER 模型来获取情感标签和情感分数。如果没有真实标签，我不知道如何评估我的模型。我可以使用哪些统计测试？有什么方法可以比较哪个模型更好？目前，我使用 Cohen&#39;s Kappa 来查看分类中的一致性。
此外，我计划对预先训练的 BERT 模型进行超调（仍然没有真实标签），在这种情况下，我如何才能更好地评估我的模型？]]></description>
      <guid>https://stats.stackexchange.com/questions/660839/how-can-i-evaluate-model-pre-trained-bert-and-vader-for-sentiment-analysis-whe</guid>
      <pubDate>Sat, 01 Feb 2025 05:14:14 GMT</pubDate>
    </item>
    <item>
      <title>堆叠泛化算法的特征选择</title>
      <link>https://stats.stackexchange.com/questions/660838/feature-selection-for-stacked-generalization-algorithm</link>
      <description><![CDATA[我对机器学习还比较陌生，我偶然发现了一篇我所在领域的论文，其中提出了一种新的集成模型。然而，他们的三个特征输入来自同一个基础模型，包括其预测值、FDR 校正分数以及 p 值校正分数，(score * (1 - x))，如果分数 &gt; 0.5 且 x &gt;= 0.5，其中 x 是 FDR 或 p 值。
我尝试四处寻找，但没有遇到任何类似的例子。这有道理吗？这似乎过分强调单个基础模型，还是“期望”集成模型能够在适当的训练过程中将其过滤掉？
任何链接或资源都非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/660838/feature-selection-for-stacked-generalization-algorithm</guid>
      <pubDate>Sat, 01 Feb 2025 03:46:46 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法的度量选择</title>
      <link>https://stats.stackexchange.com/questions/660835/metric-choice-for-machine-learning-algorithm</link>
      <description><![CDATA[我目前正在为二元分类问题构建一个 ML 模型。
我目前正在使用研究论文中提供的精选数据集，该数据集已经完全平衡。但是，众所周知，现实生活中的情况并非如此（负数的数量远高于正数的数量）。
我正在尝试选择一个合适的指标。我在网上读到，平衡的数据集应该选择准确度，而不平衡的数据集将是 F1 或 MCC。但是，考虑到现实世界数据不平衡的事实，我应该选择 MCC 或 F1 而不是准确度吗？或者我的模型应该根据数据集构建并使用准确度？
任何链接或资源都非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/660835/metric-choice-for-machine-learning-algorithm</guid>
      <pubDate>Sat, 01 Feb 2025 02:18:18 GMT</pubDate>
    </item>
    <item>
      <title>线性回归中 Y 的正态分布和多元正态分布之间的区别？</title>
      <link>https://stats.stackexchange.com/questions/660833/difference-between-normal-and-multivariate-normal-distribution-for-y-in-linear-r</link>
      <description><![CDATA[在线性回归中，我假设当 $y$ 属于正态分布时，是因为只有一个变量，就像一个简单的线性回归，例如 $y = \beta_{0} + \beta_1 x_1 + \epsilon$。当有更多变量时，$y$ 属于多元正态分布，我们讨论的是多元线性回归。
有人可以证实这一点吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660833/difference-between-normal-and-multivariate-normal-distribution-for-y-in-linear-r</guid>
      <pubDate>Sat, 01 Feb 2025 00:07:14 GMT</pubDate>
    </item>
    <item>
      <title>当所选模型未赢得所有外部折叠时解释嵌套 CV 结果</title>
      <link>https://stats.stackexchange.com/questions/660831/interpreting-nested-cv-results-when-selected-model-didnt-win-all-outer-folds</link>
      <description><![CDATA[在嵌套交叉验证中，我看到了一个有趣的场景，我希望更好地理解它：
使用 4 倍外部 CV，我的模型选择过程总体上选择了模型 A（它在内部 CV 循环中平均表现最佳）。但是，查看单个外折：

模型 A 只赢得了 4 个外折中的 2 个
其他模型赢得了剩余的 2 个折

在报告模型 A 在新数据上的预期性能时，我们使用所有外折结果的平均值 - 包括其他模型表现更好的 2 个折。
（值得一提的是，当我在完整数据集上重新执行模型选择过程时，模型 A 总体上获胜，但这些指标将是有偏估计量，因此无法报告）
问题：

为什么在估计模型 A 的性能时包含未选择模型 A 的外折结果是有效的？
如何说服那些可能对“混合其他模型的指标”感到不安的利益相关者
是否有一些我们可以使用数学基础来证明这一点？

我正在考虑类似这样的问题：$$\mathbb{E}[\text{Loss}(\text{Model A}(X), Y)| \text{Model A 由选择程序选择}] $$ 其中 $X$ 是训练数据。我很想深入了解数学细节，以及如何使用它来证明模型选择程序和嵌套 cv 本身的合理性。我理解为什么它会给出无偏估计，但很难说服不熟悉这个概念的技术受众。
这个问题这里解释了是什么，但我正在寻找更正式的理由来解释为什么，因为有必要向技术受众证明这种方法的有效性，而且直观地看，他们不相信对所有外循环的结果取平均是可以的，即使它与您选择的模型不同。]]></description>
      <guid>https://stats.stackexchange.com/questions/660831/interpreting-nested-cv-results-when-selected-model-didnt-win-all-outer-folds</guid>
      <pubDate>Fri, 31 Jan 2025 23:09:25 GMT</pubDate>
    </item>
    <item>
      <title>使用交叉验证来确定因子分析中的因子数量：为什么不是因子越多，可能性就越大？</title>
      <link>https://stats.stackexchange.com/questions/660830/use-cross-validation-to-determine-number-of-factors-in-factor-analysis-why-the</link>
      <description><![CDATA[考虑因素分析模型
\begin{方程*}
\开始{数组}{cccccccccc}
X &amp;=&amp; \mu&amp;+&amp; L&amp;\cdot&amp; f&amp; + &amp;u \\
p\乘以 1 &amp; &amp; p\times 1 &amp;&amp;p\times k&amp; &amp; k\times 1 ＆amp; &amp; p\times 1
\end{array} 
\end{equation*
其中 $\mu$ 为平均值，$L$ 是因子载荷矩阵，$f\sim N(0, I_k)$ 是因子，$u\sim N(0,\Psi)$ 是误差，其中 $\Psi$ 是对角矩阵。
在一些论文中，作者使用交叉验证来确定因子的数量$k$。
我读了代码，发现代码做了如下操作：
(1) 固定一个整数$k$。
&lt; p&gt;(2) 将数据集 $S$ 拆分为 $10$ 折叠，$S_1,\cdots, S_{10}$.
(3) 对于每个 $S_i$，使用$S\backslash S_i$来训练因子分析模型$\mu_i, L_i, \Psi_i$ .
(4) 计算对数似然
\begin{equation*}
f(i,k)= \sum\nolimits_{x\in S_i}\log p(x|\mu_i, L_iL_i^T+\Psi_i)
\end{equation *
其中 $p(x|\mu,\Sigma)$ 是多元正态分布的概率密度函数，其均值为 $\mu$ 和协方差矩阵 $\Sigma$。
在 $f$ 的参数中，$i$ 是测试集的索引，$k$ 是因子的数量。
(5) 计算对数似然的平均值 $\bar f(k)= \frac{1}{10}\sum_{i=1}^{10} f(i,k)$.
乍一看，我怀疑代码可以工作，因为我的直觉告诉我，肯定更大的$k$ 有更大的$\bar f(k)$。
并且，如果 $L$ 是一个 $p\times k$ 矩阵，并且该矩阵与模型拟合得很好，那么$l&gt;k$ 因子也很好地拟合了模型，因为我们可以构造 $p\times l$ 因子载荷矩阵为 $(L&#39;,\mathbf 0)&#39;$。
但当我运行代码时，我发现可能会发生 $\bar f(k)$ 没有增加。
我想知道为什么$\bar f(k)$没有像我直觉所想的那样增加。
举一个数值例子，我将该代码用于一个数据集，其中包含 $156$ 次试验和每次试验 $21$ 个特征。
当 $k=2,3,4,5,6$，对应的$\bar f(k)$为分别
\begin{equation*}
1000\times ( -2.7518, -2.7509, -2.7485, -2.7506, -2.7502 )。
\end{equation*
$k=4$ 具有最大值 $\bar f(k)$ 。
更新：由于我有 21 个特征，如果 $k\le 14$，因子分析才有意义。
有一个图表因子和平均交叉验证对数似然。
（不是我给出的数值示例。我忘了我用哪个数据集给出数值示例。）
许多数据集都遵循这种模式。当因子数接近 $14$，
平均对数似然甚至下降并变低。
]]></description>
      <guid>https://stats.stackexchange.com/questions/660830/use-cross-validation-to-determine-number-of-factors-in-factor-analysis-why-the</guid>
      <pubDate>Fri, 31 Jan 2025 22:47:06 GMT</pubDate>
    </item>
    <item>
      <title>glm 和（偏差）残差与拟合值的图[关闭]</title>
      <link>https://stats.stackexchange.com/questions/660829/glm-and-plot-of-deviance-residuals-vs-fitted-value</link>
      <description><![CDATA[您能验证这些是否正确吗？

使用包括 Gamma 的 GLM（您期望异方差）或正态分布（您期望同方差），您不应该能够看到偏差残差和目标方差随均值变化之间的任何直接联系。

即使在具有 Gamma 分布的 GLM 中发现异方差，您仍然应该看到偏差残差的均匀分布。您无法从偏差残差与拟合值的图中看出异方差性。

使用具有正态分布的 GLM，您必须看到同方差性，这样才不会违反正态假设（即目标的方差不应随均值而变化），但同样，您无法在偏差残差与拟合值的图中看到同方差性。

使用 MLR，您必须看到同方差性，这样才不会违反正态假设（即目标的方差不应随均值而变化），您可以在残差与拟合值的图中看到同方差性。

均匀分布意味着偏差残差（GLM）和残差（MLR）的变异性应在拟合值范围内保持一致

我认为残差的分布和假设都会发生变化需要恒定方差（例如在具有正态分布的 MLR 和 GLM 中）。否则，模型可能需要调整（例如对数变换、添加预测因子，甚至切换到另一个分布）。但是，在 GLM（非正态分布）中，预计会改变传播（Gamma），因此它本身并不表示存在问题。

]]></description>
      <guid>https://stats.stackexchange.com/questions/660829/glm-and-plot-of-deviance-residuals-vs-fitted-value</guid>
      <pubDate>Fri, 31 Jan 2025 22:34:43 GMT</pubDate>
    </item>
    <item>
      <title>样本与总体平均治疗效果抽样变异性</title>
      <link>https://stats.stackexchange.com/questions/660828/sample-vs-population-average-treatment-effect-sampling-variability</link>
      <description><![CDATA[我读到Gerber 和 Green 2012 年的田野实验时，了解到了随机化推断的概念，即在进行随机化实验时，可以有两个推断目标。其样本平均治疗效果与研究参与者的有限群体有关。以及与研究参与者所取样的人群相关的人口平均治疗效果。
我的问题是关于如何估计这两个量的抽样变异性。
将平均治疗效果估计量定义为以下形式，其中 $Y_i(t=0)$ 和 $Y_i(t= 1)$ 分别是如果给予治疗 $t = 0$、治疗 $t=1$，则观察 $i$ 的潜在结果：
$$
ATE = \mathbb{E} [Y_i(1)] - \mathbb{E} [Y_i(0)]
$$
该数量是通过假设稳定单位值假设 (SUTVA) 的样本平均值估算的。其中 $t_i$ 是观察到的处理分配，处理时为 1，未处理时为 0。 $N$ 是观测总数，$m$ 是处理过的观测数。
$$
\widehat{ATE} = \frac{1}{m}\sum_{i=1}^m{Y_i(1)|}t_i=1 + \frac{1}{N-m}\sum_{i=m+1}^N{Y_i(0)|t_i = 0}
$$
$\widehat{ATE}$ 的方差如下：
$$
\text{Var}[\widehat{ATE}] = \frac{1}{N-1}\left [\frac{(N-m) \text{Var}[Y_i(1)]}{m} + \frac{m \text{Var}[Y_i(0)]}{N-m} + 2\text{Cov}(Y_i(0), Y_i(1))\right] 
$$
我从现场实验 Gerber and Green 2012 eq 3.4 中得到了方差公式
在教科书中，这种变异性的特点是基于随机化推理。变异性来自随机分配，导致潜在结果的样本平均值不同。
我们没有观察到$\text{Cov}(Y_i(0), Y_i(1))$，因此我们可以估计方差如下：
$$
\widehat{\text{Var}}[\widehat{ATE}] = \frac{\widehat{\text{Var}}[Y_i(1)]}{m} + \frac{\widehat{\text{Var}}[Y_i(0)]}{N-m}
$$
在教科书中，这种变异性是真实抽样方差的保守估计。教科书提到，在以下情况下，该变异性估计量是无偏的：

治疗效果对每个人都是恒定的，或者，
实验中的样本是较大总体的随机样本，推论以此为基础。

总体平均治疗效果的方差为 Gerber and Green 2012 实地实验 等式 11.1：
$$
\text{Var}[\widehat{PATE}] = \frac{\text{Var}[Y_i(1)]}{m} + \frac{\text{Var}[Y_i(0)]}{N-m}
$$
我的问题是，如果治疗效果对每个人都是恒定的，那么为什么$\widehat{\text{Var}}[\widehat{ATE}] = \text{Var}[\widehat{PATE}]$]]></description>
      <guid>https://stats.stackexchange.com/questions/660828/sample-vs-population-average-treatment-effect-sampling-variability</guid>
      <pubDate>Fri, 31 Jan 2025 21:39:56 GMT</pubDate>
    </item>
    <item>
      <title>关于通过初始值（高值）和最终值（低值）进行归一化后查找方差的问题</title>
      <link>https://stats.stackexchange.com/questions/660817/question-about-finding-variance-after-normalizing-by-the-initial-high-and-fina</link>
      <description><![CDATA[我有来自许多“个人”的数据；从 0 到 1 的范围内，每个人最初的得分都很高，假设平均得分为 0.8，然后经过多次试验，他们在最后一次试验中的“最终”得分很低，假设平均得分为 0.2。休息一段时间后，他们的分数会有所恢复，假设平均得分为 0.6。
有了这三个平均值，我想计算恢复百分比（即，恢复得分恢复了初始得分和最终得分之间的差异的百分比）。使用这个标准化方程很容易计算：（平均恢复得分 - 平均最终得分/平均初始得分 - 平均最终得分）* 100。结果是 0.6-0.2/0.8-0.2 * 100 = 67%。
太好了。但是，我想用这个来计算统计数据，以比较不同组的数据，为此我需要这个百分比恢复值的方差（在本例中为 67%）。我不确定该怎么做。初始值、最终值和恢复值存在差异，所以我要使用误差传播吗？还是别的什么？如果能澄清这一点，我将不胜感激，我觉得这不是人们所做的非常罕见的规范化，但我还没有在任何地方找到这个特定问题的解决方案 :(]]></description>
      <guid>https://stats.stackexchange.com/questions/660817/question-about-finding-variance-after-normalizing-by-the-initial-high-and-fina</guid>
      <pubDate>Fri, 31 Jan 2025 17:14:41 GMT</pubDate>
    </item>
    <item>
      <title>与子集相比，glmer（）模型中固定效应对较大数据集的放大效果</title>
      <link>https://stats.stackexchange.com/questions/660823/amplified-effect-of-fixed-effects-in-glmer-model-for-larger-dataset-compared-t</link>
      <description><![CDATA[我正在用相同的变量将一系列模型拟合到不同形式的数据中。我选择了 GLMM，因为我的数据中有一些结构层次，这对于理解变化可能很重要。当我将模型拟合到较小的数据子集时，我遇到的唯一问题是一些收敛/优化问题，因为响应变量非常小——我进行了平方根变换以避免这个问题，因为我没有遇到任何问题。关系不显著（这是我们预期的）。然而，我们决定在“合并”版本的数据上拟合模型（即一个数据框中的所有子集）。当我这样做时，我必须对数据进行平方根变换以避免收敛问题，就像以前一样。但现在最佳拟合模型显示响应和预测变量之间存在强烈的负相关关系。这对我来说毫无意义，因为与子集数据的所有关系都是完全平坦且不显著的。为什么 GLMM 会预测与更多数据点之间存在强烈的负相关关系？这和转换有关吗？这是不好的做法吗？
以下是我拟合的模型类型的一些示例：
合并数据（无子集）——最佳模型
m1 &lt;- glmer(transformedY ~ scale(X) + (1 | Z) + (1 | W),
family = Gammma(link = &quot;log&quot;), data = d)
#scale(X) 估计 = -1.1341, pval ~0
#随机效应：Z 方差 = 1.76056，Z Std. dev = 1.3269；
# W 方差 = 0.09249，W Std. dev = 0.3041 

数据子集 -- 最佳模型
m2 &lt;- glmer(transformedY ~ scale(X) + (1 | Z) + (1 | Q),
family = Gamma(link = &quot;log&quot;), 
data = ds)
#scale(X) 估计 = -0.02805, p val = 1, 
#随机效应：Z 方差 = 0.061329，Z Std. dev = 0.2476；
# Q 方差 = 0.020675，Q Std. dv = 0.1438 

注意：Q 和 W 都以略有不同的单位来测量时间。
我假设它与池中的随机效应有关，这些随机效应吸收了太多的变化并以某种方式放大了固定效应，即使这种关系不是那么强？但这些随机效应是有意义的并且包含结构，所以我不想忽视这一点。有什么建议或资源可以配合使用吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660823/amplified-effect-of-fixed-effects-in-glmer-model-for-larger-dataset-compared-t</guid>
      <pubDate>Fri, 31 Jan 2025 17:08:18 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用 Beta 回归还是 Box Cox 变换线性回归？</title>
      <link>https://stats.stackexchange.com/questions/660815/should-i-use-beta-regression-or-box-cox-transformed-linear-regression</link>
      <description><![CDATA[我的响应变量是态度分数，它是比例分数（总分/最高分），因此是连续的，范围从 0-1。最初，我认为 Beta 回归会是一个不错的选择，所以我对分数进行了调整，以便没有真正的 0 或 1。
attitude_self$prop_score &lt;- pmin(pmax(attitude_self$prop_score, 0.001), 0.999)
我的模型如下，AIC 值为 -607：
beta_model &lt;- betareg(prop_score ~ age + group + gender + PerceivedKnowledge, 
data =itude_self, link = &quot;logit&quot;)

然而，经过进一步检查，似乎 prop_score 并不遵循真正的 beta 分布。我检查了分布/密度、偏度和峰度，并进行了 fitdist() 和 Kolmogorov-Smirnov (KS) 检验以测试拟合度。

偏度(prop_score)
[1] -2.792165
峰度(prop_score)
[1] 15.89607

fitdist(prop_score, &quot;beta&quot;)
通过最大似然法拟合分布“beta”
参数：
估计标准差。错误
shape1 2.6511869 0.17474578
shape2 0.7264323 0.03847695

&gt; ks.test(prop_score, &quot;pbeta&quot;, shape1 = 2.65, shape2 = 0.73)

渐近单样本 Kolmogorov-Smirnov 检验

数据：prop_score
D = 0.19366，p 值 &lt; 2.2e-16
备选假设：双侧

然后我使用 Box-Cox 变换对响应变量进行变换，并使用线性回归。
得到的 AIC 较低，为 -900，残差如下：

我转向变换后的线性回归而不是 beta 回归是否正确？]]></description>
      <guid>https://stats.stackexchange.com/questions/660815/should-i-use-beta-regression-or-box-cox-transformed-linear-regression</guid>
      <pubDate>Fri, 31 Jan 2025 16:53:16 GMT</pubDate>
    </item>
    <item>
      <title>McNemar 检验是否适合前后比较？</title>
      <link>https://stats.stackexchange.com/questions/660806/is-mcnemars-test-suitable-for-before-after-comparisons</link>
      <description><![CDATA[如果我理解正确的话，McNemar 检验通常用于测试两个二元结果之间的依赖关系。Agresti 在《分类数据分析》（我相信是 2002 年版）中给出了两个与环境相关的问题的示例，受试者同时回答这些问题。我提供了我自己的示例，其中的数字被夸大了，以便进行演示：
 Q2
Q1 同意 不同意
同意 945 5
不同意 10 40

在这里，我可以看到 McNemar 的检验是有道理的：大多数人同意这两个陈述，一些人同意其中一个而不同意另一个，还有少数人不同意两者。这听起来很合理。 p 值为 0.2，即两个问题的同意率没有显著差异。
但是，McNemar 检验也用于比较前后结果，如 Agresti 书中早期版本 (1990) 中的示例。因此，让我们使用上述数字，但将它们视为受试者对一个问题的回答，但在两个不同的时间点，$T_1$ 和 $T_2$。在这两个时间点，他们可能“同意”或“不同意”：
 T2
T1 同意 不同意
同意 945 5
不同意 10 40

由于计数的巨大不平衡，我发现 McNemar 检验中暗示的情景难以置信。我们想看看受试者是否会随着时间的推移而改变主意，结果发现一组（$T_1$ 的“同意者”）几乎没有变化（~0.5%），而“不同意者”则发生了很大变化：在 $T_1$ 不同意的受试者中，约有 20% 在 $T_2$ 同意。我认为这是一个“显著”的差异，但由于表中的数字与第一个示例中的数字相同，因此 McNemar 的检验仍然不显著。
在 $H_0$ 下，非对角线单元格中的绝对数字需要相似，而不是百分比。为了得到相似的数字，“T1 同意者”需要以与“不同意者”不同的概率改变主意，但这两个概率必须相关。换句话说，$H_0$ 意味着两组之间存在某种信息交换。或者，换句话说，“非对角线概率之间没有差异”假设可能意味着行概率之间存在相当大且非常精确的差异。这对我来说似乎非常不随机 - 与人们对零假设的期望完全相反。
或者我遗漏了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/660806/is-mcnemars-test-suitable-for-before-after-comparisons</guid>
      <pubDate>Fri, 31 Jan 2025 14:13:43 GMT</pubDate>
    </item>
    <item>
      <title>如果这些变量也包含在随机效应结构中，如何检验固定效应的显著性？</title>
      <link>https://stats.stackexchange.com/questions/660766/how-to-test-for-significance-of-fixed-effects-if-those-variables-are-also-includ</link>
      <description><![CDATA[我对混合效应模型有一个更一般/更实际的问题。我绝对不是这方面的专家，只是注意到一些似乎不对劲的地方。我想知道当固定效应也包含在模型的随机效应结构中时，是否可以测试该效应的显著性？
我将使用 R 中的 glmmTMB() 举例说明我的问题。假设您有一个模型，其中 Y 是二项分布（0 或 1）响应，X1 和 X2 是固定效应，g 是随机阻断变量。探索了最佳随机效应结构（使用 AIC 和 LRT），发现对于 g 的每个级别，Y~X1 的随机截距和斜率都很重要。最终模型如下所示：final_model&lt;-glmmTMB(Y ~ X1 + X2 + (1+X1|g), family=binomial(link=&quot;logit&quot;), data=dat)
下一步是获取固定效应的检验统计量和 p 值（换句话说，得出结论，它们是否显著影响响应 Y）。我知道有多种方法可以做到这一点，从不好的（例如，Wald 检验）到更好的（例如，似然比检验），再到最好的（例如，模拟？）。假设您正在使用似然比检验（例如，使用 drop1() 命令）。由于这种方法涉及拟合简化（或嵌套）模型进行比较，这是否会产生一个问题，即其中一个简化模型不包括 X1，但 X1 仍然是随机效应结构的一部分？换句话说，这是否会导致某些简化模型被错误指定？我尝试在 R 中运行此类测试并得到了结果，但我不确定是否要相信它。如果简化模型确实指定错误，那么您如何测试固定效应的显著性？由于它是随机效应结构的一部分，因此它永远无法从模型中移除。
***更新：为了回应一些评论，以下是我尝试过的一些测试。
在这里，我尝试移除 g 的随机斜率。final_model 的 AIC 明显较低，LRT 的结果表明模型非常不同（p&lt;0.001），final_model 的对数似然最接近于零。总体而言，这表明随机斜率可以解释变化，应将其纳入模型中。
no_slope_model&lt;-glmmTMB(Y ~ X1 + X2 + (1|g), family=binomial(link=&quot;logit&quot;), data=dat)
anova(final_model, no_slope_model, test=&quot;LRT&quot;)
AICtab(final_model, no_slope_model) 

在这里，我尝试使用 drop1() 命令，该命令从我的最终模型中顺序删除固定效应，以获得测试统计数据和 p 值。此测试运行时没有错误或警告，并提供了我需要的统计数据（例如：对于 X1 chisq=42.6，p&lt;0.01）。
drop1(final_model, test=&quot;Chisq&quot;)

但是，如果我手动重新创建 drop1() 所做的事情，那么问题就会变得更加明显。见下文，简化模型不会将 X1 作为固定效应，但 Y~X1 的随机斜率适合 g 的每个级别。这种方法产生与上述完全相同的 Chisq 和 p 值。
no_X1_model&lt;-glmmTMB(Y ~ X2 + (1+X1|g), family=binomial(link=&quot;logit&quot;), data=dat)
anova(final_model, no_X1_model, test=&quot;LRT&quot;)
]]></description>
      <guid>https://stats.stackexchange.com/questions/660766/how-to-test-for-significance-of-fixed-effects-if-those-variables-are-also-includ</guid>
      <pubDate>Thu, 30 Jan 2025 16:20:30 GMT</pubDate>
    </item>
    <item>
      <title>比较偏好与 3 个价值观（包括中性）的差异</title>
      <link>https://stats.stackexchange.com/questions/660691/comparing-differences-in-preference-with-3-values-including-neutral</link>
      <description><![CDATA[比较包括中性在内的 3 个值的偏好差异
场景：分析具有 3 个值的偏好数据（例如：您更喜欢哪个：足球、棒球还是没有偏好（即中性）？）
主要研究问题：

是否大于中性？
足球比棒球更受欢迎还是反之亦然？

奇怪的是，与连续数据不同，我没有看到很多关于这种情况的强烈建议。
我的问题是：
哪种统计数据最适合分析具有 3 个值（例如足球、中性、棒球）的偏好数据？
如果答案依赖于“弥补”预期值（例如，将响应分为 3 个值（33%，33%，33%），那么您建议使用什么值（或它们的计算）？（注意：我不喜欢 33%，33%，33%，因为不喜欢任何一个与喜欢其中一个的结果不同（参见上述主要研究问题）。
其他注意事项：
只是指出它不是因子设计（就像我们比较 2 个成功率，例如球队 1 的胜/负与球队 2 的胜/负），所以我们不能通过“平均”足球和棒球的成功来计算预期值。
我的数据集中的响应数量可能非常低。在一个例子中，当足球 = 12 和棒球 = 13 时，卡方显着。
McNemar 检验：Sauro/MeasuringU 推荐此检验。虽然它适用于名义变量，它采用 2x2 形式，并带有成对样本（重复测量）。因此，他的建议似乎适用于其他场景。
我考虑过的选项：
选项 A1 - 中性预期 = 观察值
首先，目测（或置信区间）中性和足球/棒球之间的差异。
其次，将中性预期值设置为等于观察值。将剩余的预期值分为足球和棒球（50/50 分割）以“删除”中性，但保持样本量。 （例如，见图片）



偏好
观察到的
预期的




足球
36
(58/2) =29


中立
42
42


棒球
22
(58/2) =29



一个问题似乎是统计数据本身，因为尝试解释它确实很棘手。就像，“在消除中性反应的影响后，参与者对足球和棒球的偏好不同（或没有不同）。”
选项 A2。中性与其他以及中性预期 = 观察到
除了上面的第一步，要么 (A2a) 取足球和棒球中较大的一个，(A2b) 将足球和棒球加在一起，看看它们加起来是否不同于中性，或者 (A2c) 取足球和棒球的平均值，看看该平均值是否不同于中性。一个问题是 A2a、A2b 和 A2c 的可解释性是……它们很难解释和/或需要大量语言来解释。然后使用上面的第二步。因此，可解释性问题与 A1 相同。
选项 B1 - 置信区间与预期值的重叠[不完整解决方案]
计算置信区间并与预期值进行比较。与上述问题相同：如何计算对 3 个值有意义的预期值（我认为 33,33,33 不是）。那么预期值是什么？
选项 B2 - 置信区间与 3 个观察值的重叠
类似于使用置信区间来目测连续数据之间的差异
选项 C。您的建议！
想法、意见、建议？谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/660691/comparing-differences-in-preference-with-3-values-including-neutral</guid>
      <pubDate>Wed, 29 Jan 2025 02:28:17 GMT</pubDate>
    </item>
    <item>
      <title>分层抽样和可变大小聚类的置信区间</title>
      <link>https://stats.stackexchange.com/questions/659897/confidence-intervals-for-stratified-sampling-and-variable-sized-clusters</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/659897/confidence-intervals-for-stratified-sampling-and-variable-sized-clusters</guid>
      <pubDate>Sun, 12 Jan 2025 00:25:49 GMT</pubDate>
    </item>
    </channel>
</rss>