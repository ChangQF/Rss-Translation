<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Wed, 31 Jan 2024 06:17:08 GMT</lastBuildDate>
    <item>
      <title>为什么 SSL 不能处理表格数据？</title>
      <link>https://stats.stackexchange.com/questions/638166/why-cant-ssl-handle-tabular-data</link>
      <description><![CDATA[
标准遗传编程自我监督策略的性能分析

&lt;块引用&gt;
1 简介
自监督学习（SSL）方法已被广泛用于训练计算机视觉和自然语言处理领域的深度学习模型。这种训练范例的工作原理是在特定领域的借口任务上利用大量未标记的数据，从而允许学习数据中的底层关系和模式。这些模型可以针对标记数据量减少的下游任务进行微调，与专门针对减少的标记数据进行训练相比，可以提高性能。这些策略是计算机视觉 [3, 7] 和自然语言处理 [11] 中的常见做法，借口任务通常涉及输入数据的更改。一些常见的借口任务是旋转[6]、上下文预测[5]和排列预测[8]。
虽然 SSL 在这些域中无缝工作，但对于表格数据却不能如此。表格数据本质上是异构的并且不具有全局结构，这使得借口任务的开发并非微不足道。最近，已经提出了为表格数据定义特定领域借口任务的框架，例如损坏的数据重建和对比 Mixup，它产生的模型可以产生强大的特征转换 [4, 14]。
大多数标准机器学习算法不符合经过大量处理的数据，甚至要求数据以特定方式分布。与更传统的机器学习模型相比，遗传编程（GP）[10]通过执行自己的特征构建和选择，其基本假设要少得多。这一特性引发了以下研究问题：标准 GP 是否能够利用 SSL 方法处理的数据来提高其性能？
据我们所知，之前尚未对 GP 的任何类型的 SSL 进行过研究。在这项工作中，我们的目标是通过分析标准 GP 算法在使用 SSL 模型进行数据转换时的性能来填补这一空白。

我不明白强调的文字。
为什么 SSL 处理表格数据更困难？
这是什么意思 - 表格数据本质上是异构的并且不具有全局结构？]]></description>
      <guid>https://stats.stackexchange.com/questions/638166/why-cant-ssl-handle-tabular-data</guid>
      <pubDate>Wed, 31 Jan 2024 05:23:22 GMT</pubDate>
    </item>
    <item>
      <title>Skellam 分布的分位数函数是什么？</title>
      <link>https://stats.stackexchange.com/questions/638165/what-is-the-quantile-function-of-a-skellam-distribution</link>
      <description><![CDATA[阅读 是否有 Skellam 分布或两个泊松 r.v. 差异的 95% 置信区间的公式？，我意识到 我宁愿使用分位数函数。不幸的是 Wiki 表明第一类修正贝塞尔函数位于概率质量函数中，这意味着我不知道如何直接求解 $k$。
能够计算 Skellam 分布式回归模型的预测区间在商业、医疗保健和物理（以及我确信的其他领域）中可能会很方便。]]></description>
      <guid>https://stats.stackexchange.com/questions/638165/what-is-the-quantile-function-of-a-skellam-distribution</guid>
      <pubDate>Wed, 31 Jan 2024 05:11:35 GMT</pubDate>
    </item>
    <item>
      <title>与常规 Copula 相比，使用 Vine Copula 有何优势？</title>
      <link>https://stats.stackexchange.com/questions/638164/advantages-of-using-vine-copulas-over-regular-copulas</link>
      <description><![CDATA[我是 Copula 的新手，我试图从概念上理解两种主要类型的 Copula 之间的差异：常规 Copula 和 Vine Copula。两者都用于模拟相关多元概率分布的数据（这是一个难题） - 特别是在边际分布不是同一类型的情况下（例如正态分布和指数分布的联合分布）。
据我所知，Vine Copula 似乎比常规 Copula 更复杂。这让我想知道：与常规 Copulas 相比，使用 Vine Copulas 可以获得哪些优势？
这是我自己尝试回答这个问题：
定义： Copula 是将边际分布与其多元分布联系起来的函数。如果我们有两个具有累积分布函数的随机变量 $X$ 和 $Y$ $F_X(x)$ 和 $F_Y(y)$ ，以及一个 copula 函数 $C(u, v)$，则联合 CDF 为：
$$
F(x, y) = C(F_X(x), F_Y(y))
$$
为了说明使用 Vine Copula 相对于常规 Copula 的额外优势，我尝试创建以下示例：
示例：考虑以下 3 维概率分布：
$$f(x, y, z)$$
使用链式法则https://en.wikipedia.org/wiki/Chain_rule_(概率） - 我们可以将其分为两部分：
$$
f(x, y, z) = f_X(x) f_{Y|X}(y|x) f_{Z|XY}(z|x, y)
$$
现在，比较使用 Vine Copula 与常规 Copula 的优势：

如果我们使用常规 Copula，我们可以使用单个 Copula 函数来表示此概率分布（即有 1 个 copula 函数是各个分布的函数）：

$$
F(x, y, z) = C[F_X(x) , F_{Y|X}(y|x) , F_{Z|XY}(z|x, y)]
$$

使用 Vine Copula，我认为我们可以使用多个 Copula 函数来表示此概率分布（即，我们将 3 个不同的 Copula 彼此相乘）：

$$
F(x, y, z) = C_{12}[F_X(x), F_{Y|X}(y|x)] \cdot C_{13}[(F_X(x), F_{Z|XY} (z|x, y)] \cdot C_{23|1}[F_{Y|X}(y|x), F_{Z|XY}(z|x, y)|F_X(x)]
$$
因此，与常规 Copula 相比，使用 Vine Copula 的优势在于您可以更灵活地决定要在各个分布集之间拟合哪些 Copula 函数。
结论：因此，这是 Vine Copulas 相对于常规 Copulas 的主要优势吗？我们有能力将不同的 Copula 函数（例如高斯函数、阿基米德函数）组合在一起，使我们能够捕获数据中更复杂的关系？
我的理解正确吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/638164/advantages-of-using-vine-copulas-over-regular-copulas</guid>
      <pubDate>Wed, 31 Jan 2024 05:07:45 GMT</pubDate>
    </item>
    <item>
      <title>是否有 Skellam 分布或两个 Poisson r.v. 差异的 95% 置信区间的公式？</title>
      <link>https://stats.stackexchange.com/questions/638163/is-there-a-formula-for-the-95-confidence-interval-for-the-skellam-distribution</link>
      <description><![CDATA[我有两个独立的泊松随机变量，想要评估 X - Y 的置信界限。我知道泊松置信界限有一个封闭形式的解决方案，但需要差异。我可以针对给定的泊松故障率进行数值计算吗？例如，假设 X ~ Poisson(20) 和 Y ~ Poisson(18)。我可以从 X 和 Y 生成样本，减去并找到 0.025 和 0.975 处的分位数吗？或者类似地，从 Skillam(20,18) 生成样本并找到 0.025 和 0.975 分位数？]]></description>
      <guid>https://stats.stackexchange.com/questions/638163/is-there-a-formula-for-the-95-confidence-interval-for-the-skellam-distribution</guid>
      <pubDate>Wed, 31 Jan 2024 04:57:49 GMT</pubDate>
    </item>
    <item>
      <title>关于二元随机变量的问题[关闭]</title>
      <link>https://stats.stackexchange.com/questions/638162/a-problem-on-bivariate-random-variables</link>
      <description><![CDATA[假设我们有绝对连续的随机向量 $X=(X_1,X_2)$ 和 $Y=(Y_1, Y_2)$。我们有 $Y_i=a_iX_i+b_i$ 和 $a_i&gt;0, b_i\geq 0$ $i=1,2$ 。令 ${F}$ 为分布函数，使得
${F}_X(x_1,x_2)=P(X_1\leq x_1,X_2\leq x_2).$
我们知道$\frac{\partial^2}{\partial x_1\partial x_2}F_X(x_1,x_2)=f_X(x_1,x_2).$&lt; /p&gt;
现在，\begin{align}
F_Y(y_1,y_2)&amp;=P(Y_1\leq y_1,Y_2\leq y_2)\\
&amp;=P(a_1X_1+b_1\leq y_1,a_2X_2+b_2\leq y_2)\\
&amp;=P(X_1\leq(y_1-b_1)/a_1,X_2\leq(y_2-b_2)/a_2)\\
&amp;=F_X((y_1-b_1)/a_1,(y_2-b_2)/a_2)\\
&amp;=F_X(x_1,x_2)
\end{对齐}
这里的形式我们可以说 $f(y_1,y_2)=f(x_1,x_2)$ 吗？
预先感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/638162/a-problem-on-bivariate-random-variables</guid>
      <pubDate>Wed, 31 Jan 2024 04:45:38 GMT</pubDate>
    </item>
    <item>
      <title>使用 IPTW 创建加权数据集后，我们可以在生存分析的回归过程中再次使用相同的协变量吗？</title>
      <link>https://stats.stackexchange.com/questions/638160/after-creating-a-weighted-dataset-using-iptw-can-we-use-the-same-covariates-aga</link>
      <description><![CDATA[我正在两个非随机治疗组之间进行一项观察性研究。我计划使用 IPTW（治疗加权的逆概率）来平衡两者。在加权之前，我将首先使用一些协变量（年龄、种族、癌症分期、农村地区等）来创建倾向得分。我的问题是，我是否必须在最终的回归模型中再次包含这些协变量才能评估两组之间的总体或癌症特异性生存率？ [我们不是已经将协变量纳入权重了吗？
我计划使用 R 进行总体生存分析（Kaplan-Meier 和多元 Cox 回归）以及癌症特异性生存分析（累积发生率 Fine 和 Gray，以及特定原因比例风险），分别用于预后和病因学问题。我将对未加权和加权数据集运行分析。
PS：这里绝对是新手:)！
TIA
~困惑的医生]]></description>
      <guid>https://stats.stackexchange.com/questions/638160/after-creating-a-weighted-dataset-using-iptw-can-we-use-the-same-covariates-aga</guid>
      <pubDate>Wed, 31 Jan 2024 04:08:53 GMT</pubDate>
    </item>
    <item>
      <title>纵向数据预测任务的上采样</title>
      <link>https://stats.stackexchange.com/questions/638159/upsampling-for-longitudinal-data-prediction-task</link>
      <description><![CDATA[我有一个纵向数据集，其中包含大约 14000 ID 和 110,000 观测值，每个 ID 不一定具有相同长度的时间窗口。
我正在执行一项预测任务，但是，我感兴趣的结果类别高度不平衡。鉴于我感兴趣的结果在给定 ID 的特定时间发生，并且仅发生一次（该 ID 的最后一个时间段，很像事件时间数据集）。
我的问题是：

如何在考虑结果变量的性质的同时对数据进行上采样？

是否有任何来源提供类似任务的实施细节？


我没有尝试对负类进行下采样，因为这会给我留下一个非常小的数据集。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/638159/upsampling-for-longitudinal-data-prediction-task</guid>
      <pubDate>Wed, 31 Jan 2024 03:36:02 GMT</pubDate>
    </item>
    <item>
      <title>Purging 和 Embargo 比 TimeSeriesSplit 更好吗？</title>
      <link>https://stats.stackexchange.com/questions/638157/are-purging-and-embargo-better-than-timeseriessplit</link>
      <description><![CDATA[众所周知，经典的 k 重 CV 在处理时间序列数据时效果不佳。我最近发现了两种方法，称为 Purging 和 Embargo，其目的是修改 k 倍 CV，从而不存在数据泄漏。我的问题是：

如果您可以简单地使用 TimeSeriesSplit，那么为什么要费力这样做呢？TimeSeriesSplit 更易于理解和实现？
是否有任何证据表明使用 Purging 和/或 Embargo 会产生更好的结果？

我特别有兴趣在金融时间序列的背景下回答这些问题。非常感谢任何对论文和/或直观论点的引用]]></description>
      <guid>https://stats.stackexchange.com/questions/638157/are-purging-and-embargo-better-than-timeseriessplit</guid>
      <pubDate>Wed, 31 Jan 2024 02:09:06 GMT</pubDate>
    </item>
    <item>
      <title>知识前测-后测/对照组和治疗组的 Cronbach 阿尔法</title>
      <link>https://stats.stackexchange.com/questions/638153/cronbachs-alpha-for-knowledge-pretest-posttest-control-and-treatment-groups</link>
      <description><![CDATA[我无法找到有关计算克朗巴赫阿尔法的这些具体问题的明确答案。为了我的论文研究，我进行了教学干预。我进行了前测和后测，以调查干预后知识的增长。我开发了测试项目，并且还开发了并行项目以尽量减少测试效果（因此参与者在测试前和测试后没有回答完全相同的问题）。我还有一个对照组，他们没有经历干预。
我使用 alpha 来衡量测试中六个分量表的内部一致性以及整个测试的内部一致性。每个子量表由 8 个多项选择项目组成，这些项目应该衡量我在治疗中教授的一个子主题。这些都是一个更大主题的一部分，这就是为什么我也想查看整个测试的 alpha 版本。
这是我的问题：

我是一起、单独还是仅计算和报告治疗组和对照组的 alpha？看起来预测时将它们结合起来就可以了，但是干预后，既然一组人经历了干预，那么他们不是不同的人群吗？我预计治疗组会有所改善，而对照组则保持不变。 （这就是实际发生的事情。）

我是否在预测和后测时单独、组合、仅后测、仅预测计算和报告 alpha？如果参与者在干预前不知道材料，那么预测试似乎会包含随机猜测。因此，我不确定测试该数据的内部一致性是否有意义。


测试题为知识选择题（不是李克特题）。我已将响应编码为正确或错误。我已经对所有这些不同的可能性进行了阿尔法分析，但我不知道我应该报告什么。我的顾问也不知道。
如果有人有信誉良好的引用，那将是理想的，但我也非常感谢了解您自己的做法和原因。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/638153/cronbachs-alpha-for-knowledge-pretest-posttest-control-and-treatment-groups</guid>
      <pubDate>Wed, 31 Jan 2024 00:38:50 GMT</pubDate>
    </item>
    <item>
      <title>PyTorch 中的手动梯度计算和权重更新</title>
      <link>https://stats.stackexchange.com/questions/638152/manual-gradient-computation-and-weight-update-in-pytorch</link>
      <description><![CDATA[我不想使用torch默认的loss.backward函数进行梯度计算。相反，我根据损失函数手动计算梯度（通过 torch.autograd.grad）。但几步之后我的梯度就变为零了。如果我使用 loss.backward 函数，相同的代码可以工作。 torch 是否对引擎盖下的渐变应用了任何隐藏的转换？ （例如裁剪[-1,1]范围内的渐变、分离渐变等）
我采用这样的渐变：
first_gradient=torch.autograd.grad(HSNR,NN.parameters(),retain_graph=True)
以下是我更新权重的方法：
 与 torch.no_grad()：
        对于参数，zip 中的 newgrad(NN.parameters(),final_gradient)：
            param.grad = 新的grad
    
    优化器.step() ```

]]></description>
      <guid>https://stats.stackexchange.com/questions/638152/manual-gradient-computation-and-weight-update-in-pytorch</guid>
      <pubDate>Wed, 31 Jan 2024 00:07:09 GMT</pubDate>
    </item>
    <item>
      <title>想知道对我的地理空间数据运行什么统计测试？ （初学者）[关闭]</title>
      <link>https://stats.stackexchange.com/questions/638151/wondering-what-statistical-tests-to-run-on-my-geospatial-data-beginner</link>
      <description><![CDATA[我正在为学校进行一个编码项目，您可以在其中选择主题和数据等。我选择的数据似乎很简单，但现在我意识到我实际上不知道要运行哪些分析.
我想看看臭氧水平与我所在城市家庭的种族群体之间是否存在相关性。基本上，我得到的人口普查数据告诉我给定人口普查区域中不同种族群体的家庭数量，因此种族群体是我的列，人口普查区域是我的行。
然后我还找到了每个人口普查区每月平均地面臭氧浓度的数据（因此在这种情况下，每个人口普查区只有一个数字）。
现在我被困住了。我知道如何编码和制作地图和图表，但我从未上过概率或统计课程。只计算到 calc 3。我的老师也是一名编码员，而不是统计学家，所以他也不确定要运行什么测试 - 他建议我谷歌，但谷歌没有帮助；这门课主要是学习绘制地理空间数据，然后制作图表，统计测试部分基本上是我完成之前需要的最后一件事。 Google 没有提供任何帮助，而且我现在已经进入该项目几个月了，无法选择新数据。
如何找到两个因素之间的相关性，同时考虑到不同组的总体数量不同？然后位置方面让我感到困惑？因为我的数据中有大约一千个人口普查区。我想这将是一个针对每个种族群体进行的测试？我真的不知道。或者某种测试或概率测试？
有什么想法吗？
我没有尝试太多，因为我是统计/数据分析/概率方面的初学者。我唯一的想法是我需要找到“相对曝光度”对于每个组，但我一直很困惑这到底意味着什么？考虑将其发布到数学或统计堆栈交换中。我首先在堆栈溢出中发布了这个问题，有人提到了“回归？”他们还建议在这里询问。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/638151/wondering-what-statistical-tests-to-run-on-my-geospatial-data-beginner</guid>
      <pubDate>Wed, 31 Jan 2024 00:01:38 GMT</pubDate>
    </item>
    <item>
      <title>为什么 OpenAI 的缩放定律论文低估了数据在模型缩放中的重要性？</title>
      <link>https://stats.stackexchange.com/questions/638150/why-did-the-openais-scaling-law-paper-underestimate-the-importance-of-data-in-m</link>
      <description><![CDATA[Chinchilla 论文著名地发现，在缩放模型时，您应该大致同等地增加参数数量和数据量，而不是早期的 OpenAI 缩放定律论文，该论文说您应该增加参数数量，其数量应远多于数据量.
在第 3 页，Chinchilla 论文对 OpenAI 论文为何犯此错误给出了以下解释：
&lt;块引用&gt;
首先，作者对所有模型使用固定数量的训练标记和学习率计划；这
阻止他们对这些超参数对损失的影响进行建模。相比之下，我们发现
将学习率计划设置为大致匹配训练令牌结果的数量
无论模型大小如何，都能获得最佳的最终损失——见图 A1。对于固定学习率余弦时间表
对于 130B 代币，中间损失估计（对于 𝐷&#39; &lt;&lt; 130B）因此高估了
丢失使用与 𝐷&#39; 匹配的时间表长度进行训练的模型。使用这些中间损失会导致
低估了在少于 130B 代币的数据上训练模型的有效性，最终
得出这样的结论：随着计算的进行，模型大小应该比训练数据大小增加得更快
预算增加。

但是为什么会这样呢？如果您的学习率计划导致高估了少量训练数据的损失，那么这是否会导致您高估数据对损失的影响，从而建议比参数计数更快地增加数据大小，而不是相反？]]></description>
      <guid>https://stats.stackexchange.com/questions/638150/why-did-the-openais-scaling-law-paper-underestimate-the-importance-of-data-in-m</guid>
      <pubDate>Tue, 30 Jan 2024 23:19:09 GMT</pubDate>
    </item>
    <item>
      <title>将协方差矩阵分解为不相关和相关部分</title>
      <link>https://stats.stackexchange.com/questions/638149/decompose-covariance-matrix-into-uncorrelated-and-correlated-part</link>
      <description><![CDATA[我有一个几乎对角的协方差矩阵，我想将其分解为不相关和相关的部分：
$$
\Sigma = \Sigma_U + \Sigma_C
$$
其中上面的所有矩阵都是协方差矩阵，$\Sigma_U$ 是对角矩阵。
我想这个解决方案不是唯一的，但我希望 $\Sigma_U$ 是“大”，例如具有大的行列式或迹。
这个问题过去被研究过吗？有封闭的解决方案吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/638149/decompose-covariance-matrix-into-uncorrelated-and-correlated-part</guid>
      <pubDate>Tue, 30 Jan 2024 23:13:34 GMT</pubDate>
    </item>
    <item>
      <title>消除弱外生性[关闭]</title>
      <link>https://stats.stackexchange.com/questions/638148/clearing-up-weak-exogeneity</link>
      <description><![CDATA[用语言来说，E[u_i|x_i] 到底是什么？
我被告知（根据假设），这意味着任何 x 值的误差项的条件均值都等于 0。但在这种情况下，为什么要包含“x_i”符号呢？这是否不会将 x_i 的值限制为仅与 y_i 的 1 个相应观察“i”相关的特定值？在这种情况下，您如何看待这种期望？]]></description>
      <guid>https://stats.stackexchange.com/questions/638148/clearing-up-weak-exogeneity</guid>
      <pubDate>Tue, 30 Jan 2024 22:55:18 GMT</pubDate>
    </item>
    <item>
      <title>来自同一类别的多个观察值的判别分析的后验概率</title>
      <link>https://stats.stackexchange.com/questions/638147/posterior-probabilities-from-a-discriminant-analysis-with-multiple-observations</link>
      <description><![CDATA[假设进行判别分析的目的是找到可能由三个物种之一产生的未来样本的后验概率。但现在假设我有两个未来的观察结果，并且我能够（不仅仅是愿意）假设这两个观察结果属于同一物种。
要使用两个未来观察结果（以及其他方法）获得 3 个物种的一组 3 个后验概率（当然，总和为 1）：

找到第一个未来的第一组后验概率
观察，然后使用这些概率作为先验
第二次未来观察以获得最终的后验集
概率。

求 2 个未来各自的后验概率
观察，然后使用相应的产品
后验概率除以这些乘积的总和。


我的问题是“是否有一种指定的分析技术/模型/软件可以使用指定的模型假设执行此类计算，从而可以解释判别函数构造中的不确定性？”
（在此网站上进行搜索确实发现了 2 个模糊相似的问题，但其中一个没有答案，另一个没有足够详细的答案。）]]></description>
      <guid>https://stats.stackexchange.com/questions/638147/posterior-probabilities-from-a-discriminant-analysis-with-multiple-observations</guid>
      <pubDate>Tue, 30 Jan 2024 22:42:33 GMT</pubDate>
    </item>
    </channel>
</rss>