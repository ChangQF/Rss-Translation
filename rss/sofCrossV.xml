<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 19 Nov 2024 03:29:06 GMT</lastBuildDate>
    <item>
      <title>当寻找空间自相关时，如何解释同一站点的重复？</title>
      <link>https://stats.stackexchange.com/questions/657479/how-do-i-account-for-replicates-at-the-same-sites-when-looking-for-spatial-autoc</link>
      <description><![CDATA[我试图检查空间模式/自相关对景观中分布不均匀的多个站点的响应变量的影响（可能使用 Moran&#39;s I），但我不确定如何解释这些站点中的每一个都有多个数据收集重复的事实。
我认为每个站点中的额外样本不能合理地被视为与其他年份同一站点的变量在统计上独立，并且数据集中最长时间序列也可能太短，无法从同步性而不是空间变化的角度分析数据。此外，一些站点不会出现在每年的数据收集中，“平均”值可能无法很好地描述这些站点。我也不认为我可以假设站点之间的关系每年都相同。
简而言之，我认为我可以对每个年份的站点之间的距离和相似性关系进行有效的统计测试，但我想不出一种方法将这些年份组合成一个更有意义的值。有人对我在这里可以做什么有什么建议吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657479/how-do-i-account-for-replicates-at-the-same-sites-when-looking-for-spatial-autoc</guid>
      <pubDate>Tue, 19 Nov 2024 02:46:55 GMT</pubDate>
    </item>
    <item>
      <title>如何处理对象检测数据集中的大量未标记数据</title>
      <link>https://stats.stackexchange.com/questions/657478/how-to-deal-with-many-unlabeled-data-in-an-object-detection-dataset</link>
      <description><![CDATA[我有一个大型的多类物体检测图像数据集。目标是使用 Yolo(v11) 模型在上述数据集上进行训练，以解决物体检测任务。
我的直觉告诉我，未标记的类实例的存在会降低我的 Yolo 模型的模型性能，因为这些未标记的模式可以理解为“训练误报”，因为如果在训练阶段发现未标记的物体，我的模型可能会受到惩罚。
在我的上下文中，我已经训练了一个视觉模型，它可以高精度地检测我想要检测的物体，但第二个模型只检测目标物体，除了它们的标签。
以我的问题为例，我的目标是在有猫和狗（没有其他动物）的图像上检测属于$\{Cat, Dog\}$类的物体，而我有一个非常好的模型，能够检测动物。
问题：

我想知道是否可以将数据集中的对象标记为通用类别，以便在 Yolo 训练阶段将其忽略？

很明显，我可以用模糊或裁剪来掩盖通用标记的对象，但我不确定此过程是否合适，或者在处理此类问题时是否有规范的过程选择。
提前致谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/657478/how-to-deal-with-many-unlabeled-data-in-an-object-detection-dataset</guid>
      <pubDate>Tue, 19 Nov 2024 02:00:47 GMT</pubDate>
    </item>
    <item>
      <title>学习进行参数引导</title>
      <link>https://stats.stackexchange.com/questions/657477/learning-to-do-the-parametric-bootstrap</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/657477/learning-to-do-the-parametric-bootstrap</guid>
      <pubDate>Tue, 19 Nov 2024 01:11:49 GMT</pubDate>
    </item>
    <item>
      <title>(mu,sigma) 正态分布的置信区域</title>
      <link>https://stats.stackexchange.com/questions/657475/confidence-regions-for-mu-sigma-normal-distribution</link>
      <description><![CDATA[在正态分布中，我想找到二维参数$(\mu,\sigma)$的置信区域。有没有比计算由 Bonferroni 校正的两个独立置信区间的笛卡尔积更好的方法？]]></description>
      <guid>https://stats.stackexchange.com/questions/657475/confidence-regions-for-mu-sigma-normal-distribution</guid>
      <pubDate>Mon, 18 Nov 2024 23:04:39 GMT</pubDate>
    </item>
    <item>
      <title>具有高 auc 值的不同模型</title>
      <link>https://stats.stackexchange.com/questions/657474/different-models-with-high-auc-values</link>
      <description><![CDATA[我正在尝试评估两个不同的逻辑回归模型：一个是通过前向选择获得的，另一个是通过套索正则化获得的。我使用嵌套交叉验证进行训练和测试，第一个模型的准确率为 0.9578，第二个模型的准确率为 0.9737。我认为这些值有点高，所以我对两个模型进行了 auc 评分（使用嵌套 cv），得到了 0.9979 和 0.9965，这些值也非常高。我还使用了 brier 评分，得到了 0.021 和 0.424，这对我来说似乎很奇怪，因为两个模型的表现基本相同。我最终绘制了模型的校准图，试图理解一些东西，但我得到的模型的平均绝对误差为 0.001 和 0.005。我做错了什么，或者我在这里遗漏了什么？
以下是代码：
第一个模型（前向选择）：
# 外循环分区
outer_cv &lt;- createFolds(y, k = outer_folds, list = TRUE)

# 保存结果
results &lt;- c()

# 外循环
for (i in 1:outer_folds) {
# 外循环训练和测试
test_idx &lt;- outer_cv[[i]]
train_idx &lt;- setdiff(1:nrow(data), test_idx)

train_data &lt;- data[train_idx, ]
test_data &lt;- data[test_idx, ]

# 内循环：前向选择
model_null &lt;- glm(diagnosis ~ 1, data = train_data, family = binomial())
model_full &lt;- glm(diagnosis ~ ., data = train_data, family = binomial())

# 变量选择
model_forward &lt;- stepAIC(model_null, 
scope = list(lower = model_null, upper = model_full), 
direction = &quot;forward&quot;, 
trace = FALSE)

# 测试集上的预测
predictions &lt;- predict(model_forward, newdata = test_data, type = &quot;response&quot;)
predict_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)

# 评估准确率
acc &lt;- mean(predicted_classes == test_data$diagnosis)
results &lt;- c(results, acc)
}

# 准确率 平均值
mean_accuracy &lt;- mean(results)
print(paste(&quot;Nested CV Accuracy with Forward Selection:&quot;, mean_accuracy))

第二个模型 (LASSO):
# 外循环分区
outer_cv &lt;- createFolds(y, k = outer_folds, list = TRUE)

# 保存结果
results &lt;- c()

# 外循环
for (i in 1:outer_folds) {
# 训练和测试
test_idx &lt;- outer_cv[[i]]
train_idx &lt;- setdiff(1:length(y), test_idx)

x_train &lt;- X[train_idx, ]
y_train &lt;- y[train_idx]
x_test &lt;- X[test_idx, ]
y_test &lt;- y[test_idx]

# 内部循环：选择最佳 lambda
inner_cv &lt;- cv.glmnet(x_train, y_train, alpha = 1, family = &quot;binomial&quot;, nfolds = inner_folds)
best_lambda &lt;- inner_cv$lambda.min

# 使用最佳 lambda 进行训练
lasso_model2 &lt;- glmnet(x_train, y_train, alpha = 1, family = &quot;binomial&quot;, lambda = best_lambda)

# 测试集上的预测
predictions &lt;-预测（lasso_model2，x_test，类型 = “响应”）
预测类 &lt;- ifelse（预测 &gt; 0.5, 1, 0)

# 评估准确率
acc &lt;- mean(predicted_classes == y_test)
results &lt;- c(results, acc)
}

# 准确率平均值
mean_accuracy &lt;- mean(results)
print(paste(&quot;Nested CV Accuracy:&quot;, mean_accuracy))

AUC 和 Brier:
diagnosis_numeric &lt;- as.numeric(data$diagnosis) - 1 # 从因子转换为数字
# 外循环分区
outer_cv &lt;- createFolds(y, k = outer_folds, list = TRUE)

# 保存结果
results &lt;- c()
results2 &lt;- c()

# 外循环
for (i in 1:outer_folds) {
# 训练和测试
test_idx &lt;- outer_cv[[i]]
train_idx &lt;- setdiff(1:nrow(data), test_idx)

train_data &lt;- data[train_idx, ]
test_data &lt;- data[test_idx, ]

# 内部循环：评估两个模型的 auc 分数
roc1 &lt;- roc(test_data$diagnosis, predict(model_forward, test_data, type = &quot;response&quot;))
roc2 &lt;- roc(test_data$diagnosis, predict(final_model, test_data, type = &quot;response&quot;))

results &lt;- c(results, auc(roc1))
results2 &lt;- c(results2, auc(roc2))
}

# auc 的平均值分数
mean_roc1 &lt;- 平均值（结果）
print(paste(&quot;嵌套 CV ROC 1:&quot;, mean_roc1))

mean_roc2 &lt;- 平均值（结果2）
print(paste(&quot;嵌套 CV ROC 2:&quot;, mean_roc2))

# 评估 Brier 分数
pred.prob &lt;- 预测（final_model,type=&#39;response&#39;）
brierScore &lt;- 平均值（（pred.prob-diagnosis_numeric）^2）
brierScore

pred.prob2 &lt;- 预测（model_forward,type=&#39;response&#39;）
brierScore2 &lt;- 平均值（（pred.prob2-diagnosis_numeric）^2）
brierScore2

校准图：
plot(rms::calibrate(final_model, B = 400))
plot(rms::calibrate(model_forward, B = 400))
]]></description>
      <guid>https://stats.stackexchange.com/questions/657474/different-models-with-high-auc-values</guid>
      <pubDate>Mon, 18 Nov 2024 22:40:22 GMT</pubDate>
    </item>
    <item>
      <title>固定效应水平内的观测值具有相同的随机效应水平</title>
      <link>https://stats.stackexchange.com/questions/657473/observations-within-a-level-of-fixed-effect-have-the-same-level-of-random-effect</link>
      <description><![CDATA[我的研究旨在比较人类与模型在数学任务上的表现。
设置如下：
共有 700 项独特的作业。每项作业招募 6 名随机人类参与者。总共约有 400 人参与。每位参与者完成大约 10 项作业。
对于每项作业，我还从 2 个模型中获得了绩效结果。
因此，对于每项作业，我有 6 名人类参与者的绩效分数和 2 个模型的绩效分数。
我使用混合线性模型来比较人类和模型的表现。这是 R 中的公式：
data.model &lt;- lmer(score ~ type + (1 | contestant_id) + (1 | assignment_id), data = data_1)
type：表示观察结果来自人类（human）还是模型（model）。
participant_id：每个人类参与者的唯一标识。对于模型，我将 contestant_id 设置为“model”。
assignment_id：标识任务。
我的问题是，模型的随机效应 contestant_id 只有一个级别，这有问题吗？如果是这样，我该如何调整模型以适当比较人类和模型的表现？]]></description>
      <guid>https://stats.stackexchange.com/questions/657473/observations-within-a-level-of-fixed-effect-have-the-same-level-of-random-effect</guid>
      <pubDate>Mon, 18 Nov 2024 22:33:15 GMT</pubDate>
    </item>
    <item>
      <title>如何解释 R 中 logit 模型 stargazer 中的系数方向？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/657472/how-to-interpret-coefficient-direction-in-logit-models-stargazer-in-r</link>
      <description><![CDATA[解释民主党和共和党的系数。请记住，与线性回归不同，我们只能从系数中知道关系的方向，而不是幅度。因此，解释这两个变量与因变量之间的关系的方向，同时记住排除的类别是独立的]]></description>
      <guid>https://stats.stackexchange.com/questions/657472/how-to-interpret-coefficient-direction-in-logit-models-stargazer-in-r</guid>
      <pubDate>Mon, 18 Nov 2024 22:25:25 GMT</pubDate>
    </item>
    <item>
      <title>多变量（双向/双因素）方差分析事后检验术语区分和显著性计算方法</title>
      <link>https://stats.stackexchange.com/questions/657470/multivariate-two-way-two-factor-anova-post-hoc-tests-nomenclature-distinction</link>
      <description><![CDATA[我希望对用于确定多元方差分析事后分析过程中相互作用原因的各种方法的命名法（名称）进行区分，并得到一些紧迫问题的答案。我选择了 R 和 SPSS 作为示例，但在我看来，这无关紧要。
考虑一个简单的 3x2 方差分析示例，三种药物（药物 A、药物 B 和安慰剂）和两个性别组（M/F）。我们获得了显著的相互作用效应，我们可以继续进行事后分析。
1)
首先，我们可以考虑将“一切与一切”进行比较- 我们有（某种程度上）6个子组，我们只是将它们相互比较，创建一个类似这样的矩阵（值是样本 p 值）：
 药物 A M 药物 B M 安慰剂 M 药物 A F 药物 B F 安慰剂 F
药物 A M 
药物 B M 0,321 
安慰剂 M 0,251 0,251 
药物 A F 0,181 0,181 0,181 
药物 B F 0,111 0,111 0,111 0,111 
安慰剂 F 0,041 0,041 0,041 0,041 0,041 

我不知道这种方法叫什么，但可以使用 Statistica 软件来完成，或者使用一些分析R.
2)
我们可以确定一个分组变量与另一个变量组之间的差异：
药物 A M vs 药物 A F 0,321
药物 B M vs 药物 B F 0,251
安慰剂 M vs 安慰剂 F 0,181

和
药物 A F vs 药物 B F 0,321
药物 B F vs 安慰剂 F 0,251
安慰剂 F vs 药物 A F 0,181
药物 A M vs 药物 B M 0,111
药物 B M vs 安慰剂 M 0,041
安慰剂 M vs 药物 A M 0,251




我们可以简单地接受与我们的理论一致的单一比较，合并组，等等。
药物 A M + 安慰剂 F vs 药物 A F 0,321
药物 B M vs 药物 B F + 安慰剂 M 0,251

在我看来，这三种方法给出了不同的结果，更确切地说，使用 Bonferroni 方法给出了不同的统计显著性水平。这并不奇怪，如果第一种方法我考虑了 15 个比较子组，第二种方法考虑了 3 或 6 个，第三种方法考虑了 2 个。根据 Bonferroni 校正规则（将 p 结果乘以比较次数），这给出了完全不同的显著性水平，第一种方法是最保守的。我在 R、SPSS 和 Statistica 中对其进行了测试，并手动计算了我能计算的结果。
因此，我有一系列问题，主要是关于这些技术的名称。

这三组技术的正确名称是什么？是否有任何科学来源可以引用这种命名？我记得在统计学课程中，第一种方法没有名字，第二种方法被称为“简单效应”，第三种方法的不同变体被称为“对比”——但我可能错了。
这些方法中的任何一种都可以被认为是普遍正确或不正确的吗？如果它取决于数据的类型和解释，那么是哪种？如果这取决于研究人员的理论，那么如何证明它大致正确？
在什么条件下，第一种方法会比第二种方法更清楚地展示交互作用？
由于这三种方法产生不同的重要性水平，如何证明选择其中一种而不是另一种？许多研究报告并解释了示例 2 中的比较，因为它们似乎回答了有关交互作用性质的问题，但这是正确的方法吗？
SPSS、Jamovi 和许多 R 软件包默认使用示例 2 中的这些比较，Statistica 默认使用示例 1。这些程序的理念有什么不同？

我不期望所有答案，任何建议对我来说都是有价值的。]]></description>
      <guid>https://stats.stackexchange.com/questions/657470/multivariate-two-way-two-factor-anova-post-hoc-tests-nomenclature-distinction</guid>
      <pubDate>Mon, 18 Nov 2024 21:32:09 GMT</pubDate>
    </item>
    <item>
      <title>功效分析能否计算 n 来检测总体平均值随时间的变化？</title>
      <link>https://stats.stackexchange.com/questions/657469/can-a-power-analysis-calculate-n-to-detect-changes-in-a-population-mean-over-tim</link>
      <description><![CDATA[将功效分析解释为：计算检测总体平均值从一个时间点到另一个时间点的变化（%）所需的最小样本量的一种可能方法（？）是否不正确。例如，在未来的调查中，我需要多大的样本量才能检测到总体平均值的 30% 变化？
以 Cohen 的 D 效应量公式作为功效分析的示例；如果“M”是样本平均值，“σ”是样本标准差（共同估计当前总体平均值），“µ”可以被视为未来总体平均值（当前平均值的 30% 变化，没有样本误差）吗？这是针对“前后”的重复测量设计。
那么得出这样的结论是错误的吗：
# 效应大小，Cohen&#39;s D：d=(M-μ)/σ
# M=1，σ=0.5，µ=0.7
# 0.6 = (1-0.7)/0.5

&gt; pwr.t.test(d = 0.6, 
+ power = 0.8000, 
+ n = NULL, 
+ type = &quot;paired&quot;,
+ alternative = &quot;two.sided&quot;,
+ sig.level = 0.05)

配对 t 检验功效计算 

n = 23.79452
d = 0.6
sig.level = 0.05
power = 0.8
alternative = two.sided

注意：n 是 *对* 的数量

结论：样本量为 24 足以在未来调查中检测到当前人口平均值的 +/-30% 变化（双尾检验），概率为 80%，alpha=0.05。
现在对配对数据定义功效分析是否不正确方式？
我猜这是不正确的措辞。功效分析不是为了检测从一个时间点到另一个时间点的变化，而是为了通过抽样来测试估计的总体参数的准确性，不是吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657469/can-a-power-analysis-calculate-n-to-detect-changes-in-a-population-mean-over-tim</guid>
      <pubDate>Mon, 18 Nov 2024 21:16:56 GMT</pubDate>
    </item>
    <item>
      <title>等到学生回答完调查问卷后才给予评分是否会对答案产生影响？</title>
      <link>https://stats.stackexchange.com/questions/657467/would-withholding-marks-until-students-respond-to-a-survey-bias-the-responses</link>
      <description><![CDATA[我的大学正在进行一项调查，主要是为了检查我们是否了解评估方式、是否对材料感到满意以及材料是否组织良好。参与人数很少，因此，一位讲师表示，他们将保留我们的评估成绩，直到参与人数达到至少 2/3。这让我想到，这是否会使答案产生偏差，因为动机是获得分数，而不是真正参与提供反馈？]]></description>
      <guid>https://stats.stackexchange.com/questions/657467/would-withholding-marks-until-students-respond-to-a-survey-bias-the-responses</guid>
      <pubDate>Mon, 18 Nov 2024 20:22:12 GMT</pubDate>
    </item>
    <item>
      <title>用于引导低方差数据的数据增强</title>
      <link>https://stats.stackexchange.com/questions/657466/data-augmentation-for-bootstrapping-low-variance-data</link>
      <description><![CDATA[我正在研究在线学习工具中学生正确/错误 (0,1) 数据的局部块引导。注意。（局部块引导会创建移动的响应块，但会设置任何替换可以与原始块相距多远的护栏。这是一种趋势时间序列的引导技术）学生在课程结束时往往会变得非常熟练（正如人们所希望的那样），因此引导往往会高估平均熟练程度，据我从对相同数据进行的参数引导中得知。其他人是如何解决这个问题的？如果您尝试过随机响应翻转（将 1 的随机比例更改为 0），您是如何设置执行此操作的参数的？非常感谢]]></description>
      <guid>https://stats.stackexchange.com/questions/657466/data-augmentation-for-bootstrapping-low-variance-data</guid>
      <pubDate>Mon, 18 Nov 2024 19:04:16 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯模型平均的误解</title>
      <link>https://stats.stackexchange.com/questions/657460/misunderstanding-in-bayesian-model-averaging</link>
      <description><![CDATA[我第一次尝试理解和实施贝叶斯模型平均法，以便对几个竞争模型进行加权混合。我读到的许多资料都提出了类似的我不理解的说法。我希望这里有人能解答我的困惑。以下是我对基本思想的理解：

给定竞争模型 $M_1,\ldots,M_K$ 和一些训练数据 $y_{\text{train}}$，加权模型平均值为
$$p(y)=\sum_{k=1}^Kp(y|M_k)p(M_k|y_{\text{train}}),\tag{1}$$
其中 $p(M_k|y_{\text{train}})$ 是给定训练数据后验概率 $M_k$ 模型正确。后验概率加起来为 1，因此可以将其视为权重。 模型 $M_k$ 的后验概率由以下公式给出
$$p(M_k|y_{\text{train}})=\frac{p(y_{\text{train}}|M_k)p(M_k)}{\sum_{l=1}^Kp(y_{\text{train}}|M_l)p(M_l)}.\tag{2}$$

我不清楚为什么后验概率加起来应该等于 $1$。很明显，$(2)$ 右边的表达式在对 $k=1,\ldots,K$ 求和时等于 $1$，但我不清楚为什么这应该是后验概率。它看起来很像贝叶斯规则的应用：
$$p(M_k|y_{\text{train}})=\frac{p(y_{\text{train}}|M_k)p(M_k)}{p(y_{\text{train}})},\tag{3}$$
如果我们有
$$p(y_{\text{train}})=\sum_{l=1}^Kp(y_{\text{train}}|M_l)p(M_l).$$，这将得到$(2)$

这看起来很像全概率定律的应用，只是$M_k$不必相互排斥。所以我并不确信 $(3)$ 成立。
作为一个极端的例子，我们可以通过设置 $M_{K+1}=M_1$ 来将另一个模型添加到组合中。现在，如果 $p(M_1|y_{\text{train}})&gt;0$，我们就不能同时得到 $\sum_{k=1}^K$ 和 $\sum_{k=1}^{K+1}$ 的总和，加起来等于 $1$。
我误解了什么？对于 $(2)$ 有不同的论据吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657460/misunderstanding-in-bayesian-model-averaging</guid>
      <pubDate>Mon, 18 Nov 2024 17:18:44 GMT</pubDate>
    </item>
    <item>
      <title>Logitstic 回归在 statsmodels GLM、statsmodels Logit 和 skylearn LogisticRegression 函数中有所不同 [重复]</title>
      <link>https://stats.stackexchange.com/questions/657458/logitstic-regression-varies-among-statsmodels-glm-statsmodels-logit-and-skylea</link>
      <description><![CDATA[大家好，
我发现，简单二元数据的逻辑回归在以下函数中差异很大。

statsmodels.api.GLM
statsmodels.api.Logit
sklearn.linear_model.LogisticRegression


statsmodels.api.GLM 完全符合数据，在我看来，sklearn.linear_model.LogisticRegression 比statsmodels.api.Logit。
上图表明，不同函数的解不同，但对数奇数（odd）的指数可能并非如此，如下所示：

statsmodels.api.GLM 1.2
sklearn.linear_model.LogisticRegression 1.1
statsmodels.api.Logit 1.03

知道这些函数为什么会有所不同吗？
谢谢
这是代码

从 scipy.special 导入 expit
导入 matplotlib.pyplot 作为 plt
导入 numpy 作为 np
从 sklearn.linear_model 导入 LogisticRegression
从 sklearn.metrics 导入 classes_report, chaos_matrix
导入 statsmodels.api 作为 sm

x_glm=np.arange(50)
x=x_glm.reshape(-1,1)

x_with_ones=np.ones((len(x),2))
x_with_ones[:,1]=x_glm

y=np.ones(50)
y[:25]=0
y[20:24]=1
y[30:34]=0

############Skylearn Logistic 回归 ######################
############Skylearn逻辑回归 ######################
model_logistic = LogisticRegression(solver=&#39;liblinear&#39;, random_state=0).fit(x, y)
sigmoid_auto=model_logistic.predict_proba(x)[:,1]
# sigmoid1=model_logistic.predict() # 应用截止值 0.5 后
print(model_logistic.intercept_)
print(model_logistic.coef_)

######### GLM 逻辑回归 ###############
######### GLM 逻辑回归 ###############
modelC =sm.GLM(y,x_with_ones,family=sm.families.Binomial(link=sm.families.links.Logit())).fit()
sigmoid_glm=modelC.predict(x_with_ones)
# 获取可用链接列表与特定家族相关，打印
打印（sm.families.family.Binomial.links）
打印（modelC.params）
######### sm.Logit 回归 ###############
######### sm.Logit 回归 ###############
model_logit = sm.Logit(y,x).fit()
sigmoid_logit=model_logit.predict(x)
打印（model_logit.params）
打印（model_logit.summary）

################ 图 #######################
################ 图 #######################
plt.figure()
plt.scatter(x,y,facecolors=&#39;none&#39;, edgecolors=&#39;b&#39;,label=&#39;observations&#39;)
plt.scatter(x,sigmoid_auto,facecolors=&#39;none&#39;, edgecolors=&#39;g&#39;,label=&#39;sklearn LogisticRegression&#39;)
plt.scatter(x,sigmoid_glm,c=&#39;k&#39;,marker=&#39;+&#39;,
label=&#39;sm.GLM&#39;)
plt.scatter(x,sigmoid_logit,marker=&#39;^&#39;,label=&#39;sm.Logit&#39;)

plt.legend( fontsize=&quot;20&quot;)

#%% 解释 
# coef
# sm.GLM
print(np.exp(modelC.params[3]))
# Skylearn LogisticRegression
# # x 增加一个单位，(y) 的奇数就会增加 .... 
print(np.exp(model_logistic.coef_))
#sm.Logit
print(np.exp(model_logit.params))```

]]></description>
      <guid>https://stats.stackexchange.com/questions/657458/logitstic-regression-varies-among-statsmodels-glm-statsmodels-logit-and-skylea</guid>
      <pubDate>Mon, 18 Nov 2024 16:54:09 GMT</pubDate>
    </item>
    <item>
      <title>治疗变量可以调节中介变量和结果变量之间的关系吗？</title>
      <link>https://stats.stackexchange.com/questions/657452/can-a-treatment-variable-moderate-the-relationship-between-the-mediator-and-the</link>
      <description><![CDATA[基本上，我的问题已经发布在这里：有调节的中介
由于没有人回答，我再试一次。

从经验上讲，治疗变量是否可能调节中介变量和结果变量之间的关系？
如果是这样，我该如何设置分析？

以下是我想要测试的关系的示例图：

关于第一个问题，我不确定Preacher et al. (2007)第 194 页图 2A 中的模型 1 是否描述了我的想法。
感谢您的任何建议！]]></description>
      <guid>https://stats.stackexchange.com/questions/657452/can-a-treatment-variable-moderate-the-relationship-between-the-mediator-and-the</guid>
      <pubDate>Mon, 18 Nov 2024 16:12:19 GMT</pubDate>
    </item>
    <item>
      <title>随机增加参数族中的 MLE</title>
      <link>https://stats.stackexchange.com/questions/657427/mle-in-stochastically-increasing-parametric-family</link>
      <description><![CDATA[设 $X$ 具有累积密度函数 $F_{\theta}$，假设该族在 $\theta$ 上随机递增，即对于 $\theta_1&lt;\theta_2$，$F(x;\theta_2) \le F(x;\theta_1)$。
我们有一个数据点，$X$。我相信 $\theta$ 的 MLE 必须是 $X$ 的非递减函数。直观地看，这应该是正确的，因为对于较大的 $\theta$，$X$ 会趋于更大，因此较大的 $X$ 值也可能来自较大的 $\theta$ 值。但是，我无法证明这一点。我发现将随机增加的定义转化为关于可能性的陈述非常困难。]]></description>
      <guid>https://stats.stackexchange.com/questions/657427/mle-in-stochastically-increasing-parametric-family</guid>
      <pubDate>Mon, 18 Nov 2024 07:42:21 GMT</pubDate>
    </item>
    </channel>
</rss>