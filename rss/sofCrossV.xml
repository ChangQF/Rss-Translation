<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Sat, 13 Apr 2024 03:12:17 GMT</lastBuildDate>
    <item>
      <title>称重数据问题</title>
      <link>https://stats.stackexchange.com/questions/644914/weighing-data-issue</link>
      <description><![CDATA[我正在研究一个城市内电子烟的流行程度。我通过调查向居民收集了数据，我对称重数据有疑问。
由于调查结果中特定群体的代表性过高或过低，我做出了这样的假设：应根据城市人口中的年龄分布来权衡数据，并据此权衡我的数据。例如。我从人口普查中收集了每个年龄段（例如 18-24 岁、25-34 岁等）内的人口数量数据，然后将城市中每个年龄段的人口分布与我的数据进行比较，以获得所需的响应和根据权重因素。然后我用这些来获得加权数据，这可以更好地代表城市的人口统计数据。
我的问题是，如果我错了，请纠正我，但我应该只在跨代年龄类别或整个人口进行比较时使用这些加权数据。在查看一代人本身的具体反应时，请使用未加权的数据。
例如如果我试图了解当前用户、以前的用户、从未使用过的 18-24 岁人群对电子烟危害的看法 - 我应该只查看未加权的数据来进行这些比较？但如果我要了解当前电子烟用户对每个年龄段之间的危害的看法，我会使用加权数据吗？
我只是想在对不同因素进行数百次统计测试之前确认这一点。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/644914/weighing-data-issue</guid>
      <pubDate>Sat, 13 Apr 2024 02:27:54 GMT</pubDate>
    </item>
    <item>
      <title>复杂随机变量的最小二乘回归</title>
      <link>https://stats.stackexchange.com/questions/644912/least-square-regression-of-complex-random-variable</link>
      <description><![CDATA[我查看了一些有关如何使用复杂随机变量计算最小二乘回归的答案。但我想知道以下内容：

作为多元最小二乘
回归，我们是否必须考虑实部和虚部之间的多重共线性？如果是这样，我们如何解决这个问题？

作为多元最小二乘回归，具有复数随机变量的最小二乘总是比单个实数变量更好，因为复数具有实部和虚部（即，像包含两个变量）？

]]></description>
      <guid>https://stats.stackexchange.com/questions/644912/least-square-regression-of-complex-random-variable</guid>
      <pubDate>Sat, 13 Apr 2024 00:56:28 GMT</pubDate>
    </item>
    <item>
      <title>风险价值/尾部风险价值</title>
      <link>https://stats.stackexchange.com/questions/644911/value-at-risk-tail-value-at-risk</link>
      <description><![CDATA[问题：设 X 为具有以下损失分布的随机变量
峰峰值
0 0.50
1000 0.30
2000年0.10
5000 0.06
10000 0.04
– 计算 95% 的风险价值。
– 计算 95% 尾部风险价值。
大家好，以上是我需要帮助的问题。我尝试了公式
风险值 (%) = [预期回报 – (投资组合的标准偏差 x 置信区间的 Z 分数)]。然而，答案是否定的，这看起来并不正确。任何帮助，将不胜感激。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/644911/value-at-risk-tail-value-at-risk</guid>
      <pubDate>Sat, 13 Apr 2024 00:01:35 GMT</pubDate>
    </item>
    <item>
      <title>两个原子光谱的兼容性</title>
      <link>https://stats.stackexchange.com/questions/644910/compatibility-of-two-atom-spectra</link>
      <description><![CDATA[我在实验室测量了未知元素发射线的波长。
我现在将这些值及其相对误差放在一个集合中（称为 x）。
现在我需要将我的测量结果与来自完全准确的源（存储在 xtrue 中）的已知值进行比较，也就是说，我必须检查两个光谱是否相同。
我应该如何计算这种兼容性？
我熟悉检验假设的概念，所以我的第一次尝试是定义类似于 chi2 的东西，所以在 python 中：
def chi2(x, xtrue, errs):
    如果 len(x) != len(xtrue) 或 len(xtrue) != len(errs): 返回 -1
    总和 = 0
    对于范围内的 i(len(x))：
        sum += abs(x[i] - xtrue[i]) / errs[i]

返回总和

我认为，在某种程度上，这是衡量我的测量质量的一个很好的指标，但我认为我无法以任何有意义的方式将其标准化，更不用说通过其分布找到 p 值了。&lt; /p&gt;
我错了吗？
处理此类问题有什么通用的方法吗？
我应该如何继续给出可靠的统计理由来解释为什么两个光谱相同？]]></description>
      <guid>https://stats.stackexchange.com/questions/644910/compatibility-of-two-atom-spectra</guid>
      <pubDate>Sat, 13 Apr 2024 00:00:23 GMT</pubDate>
    </item>
    <item>
      <title>联结函数揭示不明显的统计依赖性（或缺乏统计依赖性）的令人信服的例子有哪些？</title>
      <link>https://stats.stackexchange.com/questions/644909/what-are-the-convincing-examples-of-copulas-uncovering-not-obvious-statistical-d</link>
      <description><![CDATA[什么是一个好的、有力的、令人信服的例子，通过揭示一些不明显的统计依赖性来展示联结函数的力量？
我对对比联结函数与原始分布相关系数的简单计算的示例特别感兴趣。
类似这样的事情 - 双变量分布分量的（适当归一化的）相关系数并不表明它们之间存在很强的统计依赖性，但是这两个分量的 copula 分布显示出它们之间存在明显的依赖性（可能表现在值中）为 copula 分布计算的相关系数？）。或者相反-原始二元分布的相关系数表明有很强的依赖性，但其连接函数表明统计依赖性是“弱”的，或者只是不存在。
最感兴趣的是用公式描述的示例（以便可以生成样本，例如在 MATLAB 中），但如果有人可以指出特定的预先生成的二元分布数据集（或其图），那也可以。&lt; /p&gt;
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/644909/what-are-the-convincing-examples-of-copulas-uncovering-not-obvious-statistical-d</guid>
      <pubDate>Fri, 12 Apr 2024 23:38:40 GMT</pubDate>
    </item>
    <item>
      <title>Lipschitz 函数类的伪维数是多少？</title>
      <link>https://stats.stackexchange.com/questions/644907/what-is-the-pseudo-dimension-for-lipschitz-function-class</link>
      <description><![CDATA[我正在尝试使用 Lipschitz 类的伪维度或令人震惊的数字，以 $L_2(P_n)$ 范数覆盖数字。]]></description>
      <guid>https://stats.stackexchange.com/questions/644907/what-is-the-pseudo-dimension-for-lipschitz-function-class</guid>
      <pubDate>Fri, 12 Apr 2024 23:17:11 GMT</pubDate>
    </item>
    <item>
      <title>如何建立轮廓似然模型进行估计？</title>
      <link>https://stats.stackexchange.com/questions/644906/how-to-make-the-profile-likelihood-model-for-estimation</link>
      <description><![CDATA[我尝试使用土壤中的化合物结果来制作年龄估计模型。最初，我使用多变量回归模型。然而，由于过度拟合问题，审稿人强烈建议使用轮廓似然而不是回归模型。我学习了轮廓似然法。我想仅使用轮廓可能性来制作估计模型。然而，我发现仅仅使用轮廓似然来建立估计模型是不可能的。它需要创建基本函数（如线性模型），然后应用似然度。
但我也听说没有人将似然度应用到线性回归模型中，因为结果是相同的，而且更复杂。我确信我的方式是错误的。但我不明白如何使用轮廓似然方法来开发估计模型。
你能帮我借一下你的大脑吗？如果您有任何建议，请告诉我。
这是我用来制作估计模型的原始代码。
&lt;预&gt;&lt;代码&gt;Com1&lt;-c(-0.91475561、-1.39706018、-3.88102766、-2.15068888、3.36007488、8.09092947、5.10511848、4.98272449)
Com2＜-c(-0.642793922、-0.481519952、0.545645985、-0.151149253、-0.812447751、-0.363022089、4.288690815、0.523588945)
年龄 &lt;-c(20, 21, 22, 23, 93, 94, 95, 96)
ChemicalResult&lt;-data.frame(年龄、Com1、Com2)
RegModel &lt;-lm(Age~., data = ChemicalResult)
ExC1＜-c(0.00169)
ExC2＜-c(0.94)
Chem&lt;-data.frame（ExC1、ExC2）
名称(Chem)&lt;-c(“Com1”，“Com2”)
Expred&lt;-预测（PCR、化学）

感谢您的阅读]]></description>
      <guid>https://stats.stackexchange.com/questions/644906/how-to-make-the-profile-likelihood-model-for-estimation</guid>
      <pubDate>Fri, 12 Apr 2024 23:05:15 GMT</pubDate>
    </item>
    <item>
      <title>回归悖论</title>
      <link>https://stats.stackexchange.com/questions/644915/regression-paradox</link>
      <description><![CDATA[我必须使用两个二分自变量进行二元逻辑回归。
我发现自己面临着一个悖论，我不知道如何处理。
在完整的数据库中，我有 377 名 volo_1=1 患者的 21 名 (5.6%) 死亡患者，以及 2766 名 volo_1=0 患者的 86 (3.1%) 名死亡患者，volo_1 1vs 0 的 OR 1.84 预测死亡 1 vs 0 。
因此，volo_1 1 vs 0 预测死亡 1 vs 0 的 OR &gt;0，并且 volo_1=0 的死亡患者少于 volo_1=1 的死亡患者。
如果我将数据库替换为二分变量（RTS_cat2），我有两个新数据库，其中 volo_1 1 vs 0 预测死亡 1 vs 0 的 OR &lt;0，并且 volo_1=0 的死亡患者比死亡人数更多volo_1=1 的患者。
怎么可能呢？
我该如何处理这个问题？
这是我有问题的数据：
&lt;前&gt;&lt;代码&gt;&gt; x &lt;- xtabs(~dead + 交互(volo_1, RTS_cat2), data = db)
&gt; X
     交互（volo_1，RTS_cat2）
死亡 0.0 1.0 0.1 1.1
    0 2485 283 195 73
    1 12 1 74 20

&gt;表（db$dead，db$volo_1）
   
       0 1
  0 2680 356
  1 86 21

&gt; full.model &lt;- glm(dead ~ volo_1 , data = db,family=binomial())
&gt;物流.显示(完整.模型)

逻辑回归预测死亡 1 vs 0
 
               OR(95%CI) P(Wald 检验) P(LR 检验)
volo_1: 1 vs 0 1.84 (1.13,3) 0.015 0.021
                                                       
对数似然 = -464.1848
观察次数 = 3143
AIC 值 = 932.3696

&gt; full.model &lt;- glm(morto ~ volo_1 + RTS_cat2 , data = db,family=binomial())
&gt;物流.显示(完整.模型)

逻辑回归预测死亡率
 
                 粗 OR(95% CI) 调整值OR(95%CI) P(Wald 检验) P(LR 检验)
volo_1：1 vs 0 1.84 (1.13,3) 0.72 (0.42,1.24) 0.24 0.232
                                                                                     
RTS_cat2：1 vs 0 74.68 (41.26,135.18) 78.49 (43.12,142.9) &lt; 0.001＜ 0.001
                                                                                     
对数似然 = -289.3289
观察次数 = 3143
AIC 值 = 584.6577


db_rts &lt;- db[ 其中(db$RTS_cat2==1), ]

&gt;表（db_rts$morto，db_rts$volo_1）
   
      0 1
  0 195 73
  1 74 20
&gt;
&gt; full.model &lt;- glm(morto ~ volo_1 , data = db_rts,family=binomial())
&gt;物流.显示(完整.模型)

逻辑回归预测 morto ：1 vs 0
 
               OR(95%CI) P(Wald 检验) P(LR 检验)
volo_1: 1 vs 0 0.72 (0.41,1.27) 0.256 0.249
                                                          
对数似然 = -206.6552
观察次数 = 362
AIC 值 = 417.3104

db_rts1 &lt;- db[ 其中(db$RTS_cat2==0), ]

&gt;表（db_rts1$morto，db_rts1$volo_1）
   
       0 1
  0 2485 283
  1 12 1
&gt;
&gt; full.model &lt;- glm(morto ~ volo_1 , data = db_rts1,family=binomial())
&gt;物流.显示(完整.模型)

逻辑回归预测死亡率
 
               OR(95%CI) P(Wald 检验) P(LR 检验)
volo_1: 1 vs 0 0.73 (0.09,5.65) 0.765 0.754
                                                          
对数似然 = -82.6736
观察次数 = 2781
AIC 值 = 169.3472

]]></description>
      <guid>https://stats.stackexchange.com/questions/644915/regression-paradox</guid>
      <pubDate>Fri, 12 Apr 2024 22:47:07 GMT</pubDate>
    </item>
    <item>
      <title>$\mathbb{E}[X^2]\leq k \mathbb{E}[X]^2$，第一时刻的上界第二时刻</title>
      <link>https://stats.stackexchange.com/questions/644905/mathbbex2-leq-k-mathbbex2-upper-bound-second-moment-from-first-mo</link>
      <description><![CDATA[设 $X$ 为以 $[0,1]$。 $\mathbb{E}[X^2]\leq k \mathbb{E}[X]^2$ 对于某些常量 $k$？如果不是，是否有任何关于 $X$ 的最小假设？
很容易证明这适用于伯努利随机变量，但我不确定它们是最糟糕的例子。
相关问题是对称 r.v. 的第一时刻的期望就方差而言，这表明该陈述对于一般 $X$（非非负）来说是错误的。]]></description>
      <guid>https://stats.stackexchange.com/questions/644905/mathbbex2-leq-k-mathbbex2-upper-bound-second-moment-from-first-mo</guid>
      <pubDate>Fri, 12 Apr 2024 22:45:40 GMT</pubDate>
    </item>
    <item>
      <title>具有二元回归器的 ML Logit 模型的封闭式解决方案[重复]</title>
      <link>https://stats.stackexchange.com/questions/644904/a-closed-form-solution-for-ml-logit-model-with-a-binary-regressor</link>
      <description><![CDATA[考虑一个 logit 模型，其中
$$
 P(Y = 1 \mid X) = \frac{\exp(\alpha + X\beta)}{1 + \exp(\alpha + X\beta)}
$$
一般来说，我们知道 $\alpha$ 和 $\beta$&lt; 的 MLE 没有闭合形式表达式/span&gt;.
但是，假设 $X = \{0, 1\}$ 是二进制的，是否可以导出 $\hat{\alpha}_{MLE}$ 和 $\hat{\beta}_{MLE}$？
我问这个问题的原因是因为在另一篇文章中，其中一个答案声称
$$
\hat{\alpha}_{MLE} = \ln \frac{ \frac{1}{n_0} \sum_{X = 0} Y_i }{ 1 - \frac{1}{n_0} \sum_{X = 0 } 义}
$$
其中 $n_0$ 是 $X = 0$ 的观测值数量。 $\hat{\beta}_{MLE}$ 也有类似的表达式。然而，我自己无法推导出这个。
这是原始帖子的链接。
逻辑回归的最大似然估计量的偏差]]></description>
      <guid>https://stats.stackexchange.com/questions/644904/a-closed-form-solution-for-ml-logit-model-with-a-binary-regressor</guid>
      <pubDate>Fri, 12 Apr 2024 21:29:10 GMT</pubDate>
    </item>
    <item>
      <title>在应用回归模型后评估识别变量（例如种族）的显着性时，使用 Bonferroni 调整是否正确？</title>
      <link>https://stats.stackexchange.com/questions/644900/when-assessing-significance-of-identification-variables-e-g-race-after-apply</link>
      <description><![CDATA[我正在构建一个回归模型，以评估在控制信贷因素后，任何特定种族的成员（与对照组（白人）相比）是否与银行的统计显着贷款申请结果相关。
例如，我构建了一个逻辑回归模型，响应变量是贷款是否获得批准（0/1 指标），预测变量是信用评分、贷款价值比等。想法是首先构建一个很好的模型来预测批准/拒绝，然后将竞争变量添加到模型中以查看是否有任何类别很重要（这将需要进一步检查，例如文件审查）。
假设种族变量总共有六个类别，其 p 值如下。

白色：参考水平
非裔美国人：0.043
美洲印第安人：0.031
亚洲人：0.293
西班牙裔：0.019
未知：0.762

如果我只使用标准 0.05 显着性水平，那么我会得到三个潜在有问题的类别：非裔美国人、美洲印第安人和西班牙裔。
但是，我对显着性水平应用了 Bonferroni 调整；由于与参考水平有五次比较，因此 Bonferroni 调整的显着性水平为 0.01。有了这个新的显着性级别，不再有任何显着类别。
我的问题是这是否是多重比较调整的适当使用。我熟悉它的一般用法，但还没有看到它以这种方式应用于回归，特别是在我的领域，人们通常只采用基本的 0.05 显着性水平，无需调整。]]></description>
      <guid>https://stats.stackexchange.com/questions/644900/when-assessing-significance-of-identification-variables-e-g-race-after-apply</guid>
      <pubDate>Fri, 12 Apr 2024 20:14:58 GMT</pubDate>
    </item>
    <item>
      <title>使用 R 确定两种算法之间灵敏度的 p 值</title>
      <link>https://stats.stackexchange.com/questions/644899/determine-p-value-for-sensitivity-between-two-algorithms-using-r</link>
      <description><![CDATA[在 R 中，我有一个如下所示的数据框（2000 多行）：
示例 AlgoA AlgoB
FP TP
TN TN
TP TP
DFNTP
电子浮点法
FT TN TN
TP TP

TN/TP 等是通过将算法与真值集进行比较而得出的。
我通过执行常规计算生成了规格/感知值，并且还生成了 95% CI。
我现在只需要看看两组之间是否存在统计差异。这种情况我该怎么办？ AlgoA 和 AlgoB 完全无关，只是两种不同的实验室方法。
通过灵敏度计算 (TP / (TP+FN))，我知道 AlgoB 具有更好的灵敏度。我想知道这是否“显着”提高了灵敏度。]]></description>
      <guid>https://stats.stackexchange.com/questions/644899/determine-p-value-for-sensitivity-between-two-algorithms-using-r</guid>
      <pubDate>Fri, 12 Apr 2024 20:14:18 GMT</pubDate>
    </item>
    <item>
      <title>气候数据极值分析+解释</title>
      <link>https://stats.stackexchange.com/questions/644902/extreme-value-analysis-on-climate-data-interpretation</link>
      <description><![CDATA[我对极值分析的了解有限。
我想了解这两个函数之间的主要区别以及它们到底在做什么。
我的第一个猜测只是聚合差异。
第一个函数计算预定义 Return_Period 值的回报水平。
第二个函数确定每年的最大赤字并计算相应的 Return_Period。
这两个函数的结果可以相关吗？如果是这样，怎么办？
如果有人能帮助我理解。
函数1：
vals &lt;- 数据 %&gt;%
    group_by(年份(日期)) %&gt;%
    总结(赤字 = max_deficit) %&gt;%
    拉动（赤字）
  
  evd_gev &lt;- extRemes::fevd(x = vals, type = “Gumbel”, method = “MLE”)
  
  # 获取给定返回周期的 Cmagnitudes
  return_period &lt;- c(2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)
  return_level &lt;- extRemes::return.level(evd_gev, return.period = return_period)
  
  return_periods &lt;- 名称(return_level)
  return_values &lt;- as.numeric(return_level)
  
  # 将结果存储在数据框中
  return_level_df &lt;- data.frame(
    Return_Period = return_periods,
    返回级别 = 返回值，
    纬度 = unique(data$lat)[1],
    经度 = unique(data$lon)[1]
  ）
  
  返回（列表（return_level_df = return_level_df））
}

结果示例：
&lt;前&gt;&lt;代码&gt;&gt;头（返回级别_df）
  Return_Period Return_Level 纬度 经度
1 2 187.2124 35.125 -6.125
2 5 213.7497 35.125 -6.125
3 10 227.1241 35.125 -6.125
4 20 237.5110 35.125 -6.125
5 30 242.5829 35.125 -6.125
6 40 245.8131 35.125 -6.125

函数2
process_grid_cell_cwd &lt;- 函数（数据）{
  # 按年份分组并总结最大赤字
  vals &lt;- 数据 %&gt;%
    group_by(年份) %&gt;%
    总结(赤字 = max(max_deficit), .groups = &#39;drop&#39;) %&gt;%
    拉动（赤字）
  
  # 拟合 EVD Gumbel 模型
  cwd_gumbi &lt;- fevd(vals, type = “Gumbel”, method = “MLE”)
  
  # 计算给定 CWD 极值的回报期的函数
  calc_return_period &lt;- 函数（x，loc，scale）{
    1 / (1 - exp(-exp(-(x - loc) / 比例)))
  }
  
  extract_loc &lt;- 函数（mod）{
    loc &lt;- mod$results$par[ “位置”; ]
    if (!is.null(loc)){
      返回（loc）
    } 别的 {
      回报（不适用）
    }
  }
  
  extract_scale &lt;- 函数（mod）{
    比例 &lt;- mod$results$par[ “比例”; ]
    if (!is.null(scale)){
      返回（比例）
    } 别的 {
      回报（不适用）
    }
  }
  
  # 提取位置和比例
  loc &lt;- extract_loc(cwd_gumbi)
  比例 &lt;- extract_scale(cwd_gumbi)
  
  数据$Return_Period &lt;- sapply（数据$max_deficit，函数（x）calc_return_period（x，loc，scale））
  
  # 另外，保留每组第一次出现的纬度和经度
  数据 &lt;- 数据 %&gt;%
    group_by(年份) %&gt;%
    总结（Max_Deficit = max（max_deficit），
              返回周期 = max(返回周期),
              纬度 = 第一（纬度），
              经度=第一（经度），
              .groups = &#39;丢弃&#39;)
  
  返回（数据）
}

结果示例：
&lt;前&gt;&lt;代码&gt;&gt;打印（结果_df_cwd）
# 小标题：26,088 × 5
    年经纬度 max_deficit Return_Period
       
 1 2000 35.1 -6.12 192. 1.93
 2 2001 35.1 -6.12 200. 2.48
 3 2002年 35.1 -6.12 167. 1.13
 4 2003年 35.1 -6.12 232. 8.14
 5 2004年 35.1 -6.12 207. 3.10
 6 2005 35.1 -6.12 236. 9.49
 7 2006年 35.1 -6.12 152. 1.02
 8 2007年 35.1 -6.12 170. 1.16
 9 2008 35.1 -6.12 167. 1.12
10 2009 35.1 -6.12 208. 3.24
]]></description>
      <guid>https://stats.stackexchange.com/questions/644902/extreme-value-analysis-on-climate-data-interpretation</guid>
      <pubDate>Fri, 12 Apr 2024 19:30:00 GMT</pubDate>
    </item>
    <item>
      <title>如何在每周日记研究中比较压力事件类型的频率与随时间的重复测量？</title>
      <link>https://stats.stackexchange.com/questions/644898/how-to-compare-frequencies-of-type-of-stress-events-with-repeated-measurements-o</link>
      <description><![CDATA[我们希望您能帮助澄清等频卡方检验（在 SPSS 中）是否是正确的选择。
研究参与者在每周日记中报告了经历过的压力事件（文本变量），我们随后将压力事件分为不同类型的压力（0=不存在，1=存在）。
我们创建了至少 5 类（或更多）压力事件。我们想知道在 700 名儿童的样本中，这些类别的频率（比例）是否彼此存在显着差异，从而了解哪些类型的压力事件最常发生。
临床医生在就诊期间每 8 周审查一次每周日记。因此，每次访问可能包含过去 8 周内的多个压力事件（主要是 1-4 个事件）。研究参与者被跟踪了大约一年，并进行了多次研究访问。
我们想知道卡方检验是否正确，因为随着时间的推移，某些压力事件的重复报告可能并不相互独立？
仅总结研究期间的所有事件（按类型）是否正确？]]></description>
      <guid>https://stats.stackexchange.com/questions/644898/how-to-compare-frequencies-of-type-of-stress-events-with-repeated-measurements-o</guid>
      <pubDate>Fri, 12 Apr 2024 19:19:39 GMT</pubDate>
    </item>
    <item>
      <title>您应该计算重建图像的 FID 还是从文本获得的图像的 FID？</title>
      <link>https://stats.stackexchange.com/questions/644897/should-you-compute-the-fid-for-the-reconstructed-images-or-for-the-image-obtaine</link>
      <description><![CDATA[我正在尝试基于 Muse 训练文本到图像模型。我正在计划如何评估结果，但我意识到 Frechet 起始距离有一些我不明白的地方。
Muse（以及我的版本）大致是这样工作的：

有一个编码器使用多个标记（假设为 64 个）来表示图像，其中每个标记可以采用多个可能值之一（假设为 8192）
有一个解码器可以获取 64 个标记并根据它们重建图像
最后一个组件是一个转换器，它接收文本标记，并生成希望与给定文本匹配的图像标记

训练后的实际图像生成如下：

您将一段文本传递给转换器
转换器生成 64 个图像标记
令牌被传递给解码器，解码器将它们转换为实际图像

我的问题是，如何计算此类模型的 FID？我看到 $2$ 的方法：

获取原始图像，将它们传递给编码器，然后重建它们。计算原始图像和重建图像之间的 FID。这只会使用编码器和解码器。
获取图像说明，使用整个模型根据这些说明生成图像，然后计算生成的图像和原始图像之间的 FID。

第二个选项对我来说似乎更合理，因为它测试了整个模型。我无法在网上找到任何有关此问题的信息，虽然在写完这个问题后我似乎不再怀疑自己的直觉，但如果有人可以确认/证实它，那将会很有帮助。谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/644897/should-you-compute-the-fid-for-the-reconstructed-images-or-for-the-image-obtaine</guid>
      <pubDate>Fri, 12 Apr 2024 19:19:03 GMT</pubDate>
    </item>
    </channel>
</rss>