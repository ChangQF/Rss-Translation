<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 10 Aug 2024 15:16:26 GMT</lastBuildDate>
    <item>
      <title>xgboost 模型的超参数优化</title>
      <link>https://stats.stackexchange.com/questions/652593/hyperparameter-optimization-for-xgboost-model</link>
      <description><![CDATA[我正在尝试训练 xgboost 模型。准备好数据集后，我进行了训练/测试分割。为了加快训练过程，我想先使用网格搜索确定超参数的最佳范围，然后使用随机搜索训练模型。但我对得到的图表感到怀疑。例如，我获得了 max_depth 参数的验证图；
param_grid = {
&#39;max_depth&#39;: [1, 3, 5, 7, 10, 50, 100]
}

#model
xgb_model2 = xgb.XGBClassifier()

#grid_search
grid_search1 = GridSearchCV(xgb_model2, param_grid, cv=5,scoring = &quot;precision&quot;, error_score = &quot;raise&quot;, return_train_score=True)
grid_search1.fit(X_train, y_train)
results = pd.DataFrame(grid_search1.cv_results_)

max_depths = results[&#39;param_max_depth&#39;]
mean_train_scores = results[&#39;mean_train_score&#39;]
mean_test_scores = results[&#39;mean_test_score&#39;]

plt.figure(figsize=(10, 6))
plt.plot(max_depths, mean_train_scores, label=&#39;训练精度&#39;, marker=&#39;o&#39;)
plt.plot(max_depths, mean_test_scores, label=&#39;验证精度&#39;, marker=&#39;o&#39;)
plt.xlabel(&#39;最大深度&#39;)
plt.ylabel(&#39;精度&#39;)
plt.title(&#39;max_depth 对模型性能的影响&#39;)
plt.legend()
plt.grid(True)
plt.show()


我的导师说这个图可能有错，但我认为没有错。在我看来，根据这个图，max_depth 参数本身无法解决过拟合问题，但我可以解释为在进行随机搜索时尝试 max_depth 参数的 1-10 之间的值是合适的。这个图或我的解释有误吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652593/hyperparameter-optimization-for-xgboost-model</guid>
      <pubDate>Sat, 10 Aug 2024 15:13:03 GMT</pubDate>
    </item>
    <item>
      <title>用于 SDE 参数估计的 MLE</title>
      <link>https://stats.stackexchange.com/questions/652589/mle-for-sde-parameter-estimation</link>
      <description><![CDATA[我目前正在阅读 Särkkä 和 Solin 合著的《应用随机微分方程》（链接 - 第 234 页）。书中提到，如果已知转移密度，则可以使用最大似然估计 (MLE) 来估计随机微分方程 (SDE) 的参数。
但是，书中并未讨论 MLE 估计量是否一致且是否会收敛。我的具体问题是：

遍历过程的 MLE 一致性：如果底层随机过程是遍历的，我理解人们可能仍会期望 MLE 具有一致性。这种理解正确吗？

非平稳过程的一致性：当过程非平稳时会怎样？我们还能期望 MLE 是一致的并且会收敛吗？如果是，在什么条件下？如果不是，对于非平稳过程，哪些替代估计方法可能更合适？


任何见解或相关文献的引用都将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/652589/mle-for-sde-parameter-estimation</guid>
      <pubDate>Sat, 10 Aug 2024 11:17:52 GMT</pubDate>
    </item>
    <item>
      <title>指定 TWFE 模型（具有和不具有动态效应）的正确方法是什么？</title>
      <link>https://stats.stackexchange.com/questions/652588/what-is-the-correct-way-to-specify-model-a-twfe-with-and-without-dynamic-effect</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/652588/what-is-the-correct-way-to-specify-model-a-twfe-with-and-without-dynamic-effect</guid>
      <pubDate>Sat, 10 Aug 2024 10:57:00 GMT</pubDate>
    </item>
    <item>
      <title>处理大量非随机缺失值的最合适填补方法</title>
      <link>https://stats.stackexchange.com/questions/652587/most-appropriate-imputation-method-for-dealing-with-large-amount-of-non-random-m</link>
      <description><![CDATA[我正在尝试根据几个指标的值创建一个综合指数。这些指标为各国在其衡量的某个维度上每年提供分数。这个想法是使用 PCA 确定分配给这些分数的相对权重。
但是，我的数据集包含许多缺失值。这是因为并非所有国家/地区每年在每个维度上都获得分数。

要运行 PCA，我需要某种方法来处理这些缺失值。事实证明这非常困难，可能很大程度上是由于我对统计学的了解非常基础。
理想情况下，我想以某种方式估算数据。但是，我很难确定最好的方法是什么。首先，这是因为缺失的数据不是随机的。如果一个国家在某一年没有在某个指标上得分，那么它在其他年份也不太可能再次得分。反之亦然：获得分数的国家很可能在下一年再次获得分数。其次，由于我的数据涉及彼此独立的国家，因此我无法将一个国家的数据归因于另一个国家。如果尼日利亚获得分数但挪威没有，那么以某种方式将数据归因于挪威是没有意义的。
因此我面临一个问题。您对如何最好地处理这种情况有什么建议吗？或者最好根本不归因，而选择像 NIPALS 这样的方法？]]></description>
      <guid>https://stats.stackexchange.com/questions/652587/most-appropriate-imputation-method-for-dealing-with-large-amount-of-non-random-m</guid>
      <pubDate>Sat, 10 Aug 2024 10:40:00 GMT</pubDate>
    </item>
    <item>
      <title>聚类和样本加权（使用 GEE）</title>
      <link>https://stats.stackexchange.com/questions/652584/clustering-and-samples-weighting-using-gee</link>
      <description><![CDATA[我目前正在使用 GEE 模型分析与欧洲国家传染病流行相关的社会因素。我的数据基于特定健康保险覆盖的患者，这引发了对外部有效性的担忧。具体而言，由于不同族群在受保人群中所占比例过高，患病率可能会出现偏差。
我是否应该调整模型中每个族群（来自整个国家人口）的比例，使用这些比例作为加权因子，或考虑按族群进行聚类以解决潜在的偏差？确保我的结果更适用于总体人群的最佳方法是什么？
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/652584/clustering-and-samples-weighting-using-gee</guid>
      <pubDate>Sat, 10 Aug 2024 08:50:58 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用哪种回归来估计协方差？</title>
      <link>https://stats.stackexchange.com/questions/652583/which-regression-should-i-use-to-estimate-the-covariance</link>
      <description><![CDATA[我使用以下代码生成了一些合成数据。我想仅使用 (X, Z, Y) 中的信息推断 U。例如，泊松回归估计 U 的平均值，但不提供有关其协方差的信息。哪种回归模型合适？
# 加载必要的包
library(lme4)
library(MASS)

# 设置种子
set.seed(123)

# 生成 X
X &lt;- 1 + rnorm(10000, 0, 1)
beta &lt;- 0.5

# 生成 Z
Z &lt;- 0.2 * matrix(rnorm(20000), ncol = 2)

# 均值向量和协方差矩阵
mean &lt;- c(-0.5, 1)
covariance_matrix &lt;- matrix(c(1.0, 0.5, 0.5, 1.5), nrow = 2)

# 从多元正态分布中抽样
U &lt;- mvrnorm(10000, mu = mean, Sigma = covariance_matrix)

#计算线性组合
linear &lt;- X * beta + rowSums(Z * U)

# 从泊松分布中抽样
rate &lt;- exp(linear)
Y &lt;- rpois(10000, lambda = rate)

# 组织成数据框
data &lt;- data.frame(X = X, Z1 = Z[,1], Z2 = Z[,2], Y = Y)

example_model = glm(Y ~ 0 + X + Z1 + Z2, data = data, family =toxic)

我的尝试：
data$id = 1:nrow(data)
model = glmer(Y ~ 0 + X + Z1 + Z2 + (0 + Z1 + Z2 | id), data = data, family = poisson,
control=glmerControl(check.nobs.vs.nlev = &quot;ignore&quot;,
check.nobs.vs.rankZ = &quot;ignore&quot;,
check.nobs.vs.nRE=&quot;ignore&quot;))

虽然这个方法有效，但整体质量并不好。有什么想法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652583/which-regression-should-i-use-to-estimate-the-covariance</guid>
      <pubDate>Sat, 10 Aug 2024 07:52:35 GMT</pubDate>
    </item>
    <item>
      <title>表明“二分”连续结果变量会降低标准化效应大小</title>
      <link>https://stats.stackexchange.com/questions/652582/show-that-dichotomizing-a-continuus-outcome-variable-reduces-the-standardized</link>
      <description><![CDATA[我们一直在教授两组 A 和 B 的比较的样本量计算。我被要求提供一个数学解释，说明为什么通过选择截止值 $C$ 并将比例与 $Y &gt; C$ 进行比较，将连续结果变量 $Y$ “二分化”会降低功效。这足以表明，平均值的 z 检验比较的标准化效应大小将始终大于相应比例的 z 检验比较的标准化效应大小。我可以假设 Y 服从正态分布，方差为 $\sigma^2 = 1$。
假设：
$$\text{A 组均值} = 0$$
$$\text{B 组均值} = E$$
均值的 z 检验比较的标准化效应大小为：
$$
\text{效应大小} = \frac{\text{B 组均值} - \text{A 组均值}}{\sqrt{\sigma^2}} = \frac{E - 0}{1} = E
$$
现在，假设我们将连续变量 $Y$ 二分为阈值$C$。A 组中$Y &gt; C$的个体比例由以下公式给出：
$$
p_A = 1 - \Phi(C)
$$
类似地，B 组中$Y &gt; C$的个体比例由以下公式给出：
$$
p_A = 1 - \Phi(C)
$$ C$ 是：
$$
p_B = 1 - \Phi(C - E)
$$
$\Phi(\cdot)$ 表示标准正态分布的 CDF。

两组之间的比例差异是：
$$
D = p_B - p_A = \left[1 - \Phi(C - E)\right] - \left[1 - \Phi(C)\right] = \Phi(C) - \Phi(C - E)
$$
这个差异的方差是：
$$
V = p_A(1 - p_A) + p_B(1 - p_B)
$$
因此，差异的标准差是：
$$
S = \sqrt{V}
$$
最后，二分法后的标准化差异（效应大小）是：
$$
\text{标准化差异} = \frac{D}{S}
$$
如果我到目前为止的推理是正确的，我只需要证明，对于任意的 C 和 E，两者 &gt; 0，
\begin{align*}
E &amp;&gt; \frac{D}{S} \\
&amp;&gt; \frac{\Phi(C) - \Phi(C - E)}{\sqrt{\Phi(C)( 1 - \Phi(C)) + \Phi(C - E)(1 - \Phi(C - E))}}
\end{align*&gt;
根据实验，我认为不等式右边的表达式在 C = 0 时最大，并且始终小于 E。我无法证明表达式在 C = 0 时最大，也无法证明它始终 &lt; E。有什么想法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652582/show-that-dichotomizing-a-continuus-outcome-variable-reduces-the-standardized</guid>
      <pubDate>Sat, 10 Aug 2024 06:32:09 GMT</pubDate>
    </item>
    <item>
      <title>时变马尔可夫切换 AR 模型的自协方差和谱密度 [关闭]</title>
      <link>https://stats.stackexchange.com/questions/652580/autocovariance-and-spectral-density-of-time-varying-markov-switching-ar-model</link>
      <description><![CDATA[我想知道如何计算时变马尔可夫切换 AR 模型在频率 = 0 处的自协方差和谱密度，该模型如下所述

祝好，
温迪]]></description>
      <guid>https://stats.stackexchange.com/questions/652580/autocovariance-and-spectral-density-of-time-varying-markov-switching-ar-model</guid>
      <pubDate>Sat, 10 Aug 2024 05:06:41 GMT</pubDate>
    </item>
    <item>
      <title>ELBO 和“向后” KL 散度参数顺序</title>
      <link>https://stats.stackexchange.com/questions/652577/elbo-backwards-kl-divergence-argument-order</link>
      <description><![CDATA[在维基百科上，它说：“对 P 与 Q 的 KL 散度的简单解释 [即 D_KL(P||Q)] 是当实际分布为 P 时，使用 Q 代替 P 作为模型所产生的预期超额惊喜&amp;quot;
然而在维基百科 ELBO 页面上 &amp;在Bayes-by-backprop 的高引用论文中，它展示了使用 KL“反向”的 ELBO （即真实分布是第二个参数）：

更让我困扰的是，即使您没有对前一项使用反向 KL，您实际优化的成本函数（如下）似乎也会起作用（更好？）...并且定期使用 KL 对于该项在计算上同样可行，尽管我不知道 ELBO 推导的理论含义...

变分推理为什么以这种方式工作？使用 ELBO=KL[P(w)||q(w|θ)]+NLL 的理论含义是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/652577/elbo-backwards-kl-divergence-argument-order</guid>
      <pubDate>Sat, 10 Aug 2024 02:03:37 GMT</pubDate>
    </item>
    <item>
      <title>线性回归中的残差指数/偏协方差解释</title>
      <link>https://stats.stackexchange.com/questions/652566/residual-index-partial-covariance-interpretation-in-linear-regression</link>
      <description><![CDATA[我根据以下模型模拟数据

其中我的最终结果变量是鸟类物种的平均脑质量。
我需要帮助解释从以下 OLS 回归拟合的 beta 系数
$$
(residuals~of~G \thicksim B) \thicksim D
$$
它给出了最佳拟合线
$$
(residuals~of~G \thicksim B) = \alpha + \beta D
$$
我有 beta 的闭式解，包括偏协方差、偏相关和相关性，我认为这使其值特别清晰。尽管如此，我想将 $\beta$ 解释为告诉我 G 本身在某些条件下如何变化，而不是 G 对 B 的残差如何变化。这可能吗？如果它能给出更好的解释，我很乐意重新想象一下这个系统。例如，我可以翻转 B2 或 B3 箭头，这样 B/D 中的一个就直接导致另一个。任何关于 $\beta$ 到底告诉我什么的进一步见解都将不胜感激。以下是我对 $\beta$ 值的三个闭式表达式，它们仍然让我感到困惑。
$$
\beta = \frac{partial~covariance~of(G,D |B)}{var(D)}
$$
其中 (G,D |B) 的偏协方差为
$$
partial~covariance~of(G,D |B) = cov(G,D) - \frac{cov(G,B)cov(B,D)}{var(B)}
$$
此外，$\beta$ 可以用偏相关来表示
$$
\beta = \rho_{GD\cdot B}\frac{sd(residuals~of~G \thicksim B )sd(residuals~of~D \thicksim B)}{var(D)}
$$
最后，我最喜欢的表达式是下一个，因为它显示$\beta$是G和D之间的相关性，无法通过B标准化到回归的正确单位来追踪
$$
\beta = \big(cor(G,D) - cor(G,B)cor(B,D)\big)\frac{sd(G)}{sd(D)}
$$
但是我仍然很困惑。如何在我的模拟数据集中将D改变一个单位并控制改变B的影响，以便我的G实际上增加了$\beta$。如果可能的话，我想根据原始模型而不是残差来解释$\beta$，因为至少对我来说，残差实际上没有任何意义，据我所知，G ~ B 的残差是 E-A 的某种估计值，但这很令人困惑。
欢迎任何观点/意见。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/652566/residual-index-partial-covariance-interpretation-in-linear-regression</guid>
      <pubDate>Fri, 09 Aug 2024 20:31:47 GMT</pubDate>
    </item>
    <item>
      <title>在模型选择中执行内部 CV 的“元”学习器的性能如何？</title>
      <link>https://stats.stackexchange.com/questions/652525/what-is-the-performance-of-a-meta-learner-that-performs-internally-cv-for-mode</link>
      <description><![CDATA[我试图理解证明，即在模型选择期间将 CV 性能报告为性能估计存在乐观偏差。证明步骤如下：

设 $p_i, \pi_i$ 为样本真实表现 $\forall i$（每个 $i$ 对应一个配置，又称学习器，例如 $C=1$ 的 SVM、$C=10$ 的 SVM 等）
$\mathbb{E}[p_i] = \pi_i, \forall i$（无偏估计）
我们返回估计值 $\max(p_1, \ldots, p_n)$
我们平均回报$\mathbb{E}[\max(p_1, \ldots, p_n)]$
真正的最佳表现$\max(\pi_1, \ldots, \pi_n) =
\max(\mathbb{E}[p_1], \ldots, \mathbb{E}[p_n])$ 为什么？

\begin{align*}
&amp;\mathbb{E}[\max(p_1, \ldots, p_n)]
\geq
\max(\mathbb{E}[p_1], \ldots, \mathbb{E}[p_n])
\\
&amp;\therefore \text{Overestimation}
\end{align*&gt;
我不明白为什么内部执行 CV 的学习者的真实表现必须是 $\max(\pi_1, \ldots, \pi_n)$。
为了获得这个“元”的表现学习器，我们需要以下内容：

将数据集拆分为训练和测试
在训练中通过 CV 调整超参数
估计测试集中最佳选定模型的性能
对不同的数据集重复 1-3 的所有步骤，并平均步骤 3 的性能（这应该是期望等于“元”学习器的真实性能）

有人能证明为什么这个性能必须等于 $\max(\pi_1, \ldots, \pi_n)$ 吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652525/what-is-the-performance-of-a-meta-learner-that-performs-internally-cv-for-mode</guid>
      <pubDate>Fri, 09 Aug 2024 09:20:53 GMT</pubDate>
    </item>
    <item>
      <title>有哪些情况我们需要避免使用 IQR？</title>
      <link>https://stats.stackexchange.com/questions/633392/are-there-cases-where-we-need-to-avoid-the-usage-of-iqr</link>
      <description><![CDATA[让我们考虑一下四分位距 (IQR)、标准差 (SD) 和平均绝对差 (MAD)。我们知道&quot;最常见的稳健尺度测量之一是四分位距 (IQR)&quot;，而&quot;标准差受异常值的影响很大&quot;，其&quot;崩溃点为 0&quot;。此外，“平均绝对偏差是比标准偏差更稳健的离散度度量”。
当我在近似对称或中度偏斜的分布（没有异常值）中计算 IQR、SD 和 MAD 时，SD 和 MAD 给出的值相似（MAD 返回的值比 SD 略低），并且它们始终低于 IQR 值，并且与 IQR 值相对较远。
可能保留 IQR、SD 和 MAD 并没有什么错，因为它们只是“离散度”一词的不同定义，但如果一个人需要依赖一个数字来表示数据的离散度并问，“该分布的离散度是多少？”，我们应该说出 IQR、SD 和 MAD 的所有值吗？
或者我们应该 - 例如 - 丢弃 IQR，因为它离 SD 和 MAD 相当远，而只传达 SD 或 MAD，因为它们彼此相当接近？
从这个案例中，我会概括并问：是否存在我们需要避免使用 IQR 的情况？]]></description>
      <guid>https://stats.stackexchange.com/questions/633392/are-there-cases-where-we-need-to-avoid-the-usage-of-iqr</guid>
      <pubDate>Fri, 08 Dec 2023 11:24:41 GMT</pubDate>
    </item>
    <item>
      <title>Hessian 的一些特征值为正是什么意思？如何解决？</title>
      <link>https://stats.stackexchange.com/questions/597563/what-does-mean-some-eigenvalues-of-the-hessian-are-positive-how-to-fix-it</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/597563/what-does-mean-some-eigenvalues-of-the-hessian-are-positive-how-to-fix-it</guid>
      <pubDate>Thu, 01 Dec 2022 06:55:44 GMT</pubDate>
    </item>
    <item>
      <title>如何在有序 logit 和有序 probit 回归之间进行选择？</title>
      <link>https://stats.stackexchange.com/questions/594183/how-to-choose-between-ordered-logit-and-ordered-probit-regression</link>
      <description><![CDATA[如果因变量是离散序数，如 0-10，则适合使用有序对数或有序概率。它们相似，但解释不同，并且误差分布不同。有序对数服从逻辑分布，有序概率服从正态分布。有序对数有比值比，而有序概率没有。使用哪种方法没有显著差异。
以上是我从不同来源阅读和理解的内容。尽管我想绘制数据的分布图，看看它是服从逻辑分布还是正态分布。
为此，我发现可以绘制下面的图。那么我的问题是：这是在有序 logit 或有序 probit 之间进行选择的好方法吗？
library(effects)
library(fitdistrplus)

wvs.data = head(WVS) # 来自 effects 包的数据
wvs.data
贫困 宗教 学位 国家 年龄 性别
1 1 是 否 美国 44 男性
2 2 是 否 美国 40 女性
3 1 是 否 美国 36 女性
4 3 是 是 美国 25 女性
5 1 是 是 美国 39 男性
6 2 是 否 美国 80 女性

wvs.data$poverty = as.numeric(wvs.data$poverty)

fit1 &lt;- fitdist(wvs.data$poverty, &quot;norm&quot;)
fit2 &lt;- fitdist(wvs.data$poverty, &quot;logis&quot;)

par(mfrow=c(2,2))
denscomp(list(fit1, fit2),legendtext = c(&quot;Normal&quot;, &quot;Logistic&quot;), 
fitcol = c(&quot;green&quot;, &quot;blue&quot;)) 
qqcomp(list(fit1, fit2),legendtext = c(&quot;Normal&quot;, &quot;Logistic&quot;), 
fitcol = c(&quot;green&quot;, &quot;blue&quot;)) 
cdfcomp(list(fit1, fit2),legendtext = c(&quot;Normal&quot;, &quot;Logistic&quot;), 
fitcol = c(&quot;green&quot;, &quot;blue&quot;)) 
ppcomp(list(fit1, fit2),legendtext = c(&quot;Normal&quot;, &quot;Logistic&quot;), 
fitcol = c(&quot;green&quot;, &quot;blue&quot;))


par(mfrow=c(1, 1))
descdist(wvs.data$poverty, discrete = FALSE)
汇总统计
------
最小值：1 最大值：3 
中位数：1.5 
平均值：1.666667 
估计标准差：0.8164966
估计偏度：0.8573214 
估计峰度：2.7 

]]></description>
      <guid>https://stats.stackexchange.com/questions/594183/how-to-choose-between-ordered-logit-and-ordered-probit-regression</guid>
      <pubDate>Mon, 31 Oct 2022 18:44:00 GMT</pubDate>
    </item>
    <item>
      <title>按时间和约 1000 万条记录进行聚类数据</title>
      <link>https://stats.stackexchange.com/questions/579124/clustering-data-with-time-and-10-million-records</link>
      <description><![CDATA[我有一个数据集，其中包含产品类别、尺寸、价格、特定日期的销售量等特征。
我想从这个数据集（约 1200-1500 万条记录）中创建集群，我使用的是 1 年的数据。但是，有些产品可能是季节性的，年底的销量可能比年初的销量更高。最终目标是将单个产品分类为高销量或低销量。
我有以下问题 -

由于数据集很大，我仍然在相当大的集群上使用 K-Means++，并且运行 K-Means 很好。但是，对于如此大的数据集，K-Means 的性能会降低吗？如果是，这种情况的转折点应该是什么？我尝试了 BIRCH，但结果相似。

产品类别有大约 100 个不同的类别（超过 800-900 万个不同的单个产品）。进行独热编码是正确的方法吗？还是还有其他方法？根据我的理解，独热编码可能会导致稀疏数据集，并且可能不会给模型增加太多价值。我尝试使用 PCA 首先创建 2 个组件，然后进行聚类，但是，PCA1 本身解释了数据中 99% 的方差，其中“销售单位”的特征重要性为 99%，所以我认为我在这里遗漏了一些东西。我还使用了 robustscalar 在执行任何操作之前缩放数据。

由于我有全年每日级别的产品数据，如何将时间序列组件考虑在聚类中？面板数据聚类是否可行？如何进行？


现在，我进行的聚类练习是在按季度汇总产品销售额之后进行的。但是，我需要考虑季节性部分。如果有人可以提供详细的答复并让我知道是否需要更多信息，那就太好了。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/579124/clustering-data-with-time-and-10-million-records</guid>
      <pubDate>Fri, 17 Jun 2022 14:57:46 GMT</pubDate>
    </item>
    </channel>
</rss>