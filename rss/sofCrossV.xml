<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Wed, 14 Aug 2024 18:20:05 GMT</lastBuildDate>
    <item>
      <title>加权平均值（平均客户与产品互动）</title>
      <link>https://stats.stackexchange.com/questions/652806/weighted-averages-average-customer-product-interactions</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/652806/weighted-averages-average-customer-product-interactions</guid>
      <pubDate>Wed, 14 Aug 2024 18:10:16 GMT</pubDate>
    </item>
    <item>
      <title>两个独立的受试者内模型中回归系数之间的 G*Power 样本量计算</title>
      <link>https://stats.stackexchange.com/questions/652803/gpower-sample-size-calculation-between-regression-coefficients-in-two-separate</link>
      <description><![CDATA[我有两个相同的回归模型在两个不同的子样本上运行。在 R 中，我的模型如下：
library(fixest)
library(tidyverse)

mod1 &lt;- feols(outcome ~ treatment | id, data=df %&gt;% filter(group == 1)) 
mod2 &lt;- feols(outcome ~ treatment | id, data=df) %&gt;% filter(group == 2)) 

其中 treatment 是一个因子变量，其级别分别为控制、1 和 2（控制是参考，已被删除）。 id 是受访者 ID，因此上述内容代表了受访者固定效应，从而支持受试者内设计。
对于我的分析，我想比较 mod1 中的 treatment1 与 mod2 中的 treatment1 的效应显著性之间的差异。
我需要对需要以 0.8 功率和 95% 置信度进行测试的受访者数量进行先验功率计算。我正在尝试使用 G*Power 来计算，但我不确定要使用哪种设置。我的直觉是，鉴于我正在比较回归系数，t 检验适合此设置，但当它们来自两个不同的模型时，哪种设置合适？如果它要求标准差，我会使用特定于特定子集的标准差吗？我的做法是否错误，是否可以使用其他测试？如果是，您能否提供一个适当的功效计算示例，该示例基于感兴趣的估计值作为两个受访者模型中两个回归系数之间的差异。]]></description>
      <guid>https://stats.stackexchange.com/questions/652803/gpower-sample-size-calculation-between-regression-coefficients-in-two-separate</guid>
      <pubDate>Wed, 14 Aug 2024 16:56:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用自然三次样条解释逻辑回归的系数？</title>
      <link>https://stats.stackexchange.com/questions/652801/how-to-interpret-coefficients-of-logistic-regression-using-natural-cubic-splines</link>
      <description><![CDATA[我正在尝试使用自然三次样条函数在 R 中进行逻辑回归，但在解释和使用结果时遇到了麻烦。
出于各种原因，我需要能够在 Python 中绘制（并可能进一步分析）结果。问题是，虽然我可以使用标准逻辑回归来做到这一点，但我不知道如何使用（自然三次）样条函数进行逻辑回归。
示例：对螺旋星系中恒星棒的频率进行逻辑回归，作为单个星系的总恒星质量的函数。 （如果您想自己尝试拟合，这里有一个指向数据文件的链接。）
如果我使用二次逻辑函数进行标准逻辑回归——就像我对Erwin 2018中数据集的早期版本所做的那样[Github 链接到 Python 和 R 笔记本：https://github.com/perwin/s4g_barfractions]，我会在 R 中得到类似这样的结果（删除一些输出）：
ff &lt;- &quot;barpresence_vs_logmstar_for_R_sp_w30_updateddist.txt&quot;
logmstarBarSpTable &lt;- read.table(ff, header=TRUE)

logMstarFitSp_quad &lt;- glm(bar ~ logmstar + I(logmstar^2), 
family = binomial, logmstarBarSpTable)
summary(logMstarFitSp_quad)

系数：
估计标准差。误差 z 值 Pr(&gt;|z|)
(截距) -19.2096 26.0781 -0.737 0.461
logmstar 4.2945 5.3723 0.799 0.424
I(logmstar^2) -0.2332 0.2760 -0.845 0.398
AIC：828.4

我可以轻松地在 Python 中绘制结果，因为我知道“系数”值对应于二次方程中的系数：
logit = beta_0 + beta_1*x + beta_2*x^2
其中 beta_0 = &quot;(Intercept)&quot;，beta_1 = &quot;logmstar&quot;，beta_2 = &quot;I(logmstar^2)&quot;，因此实际表达式变为
logit = -19.2096 + 4.2945*x - 0.2332*(x**2)
然后我可以使用 Python 代码生成概率值，如下所示：
probability = 1.0 / (1.0 + np.exp(-logit))
这是通过以下方式要求 R 绘制拟合结果：
plot_model(logMstarFitSp_quad, type = &quot;pred&quot;, term = c(&quot;logmstar&quot;))

这是在 Python 中执行相同操作的结果，结果非常吻合：
def logit_quad( x, params=[-19.2096, 4.2945, -0.2332] ):
return params[0] + params[1]*x + params[2]*(x**2)
xx = np.arange(8.5,11,0.01)
plt.plot(xx, 1.0 / (1.0 + np.exp(-logit_quad(xx)))


另一方面，如果我尝试在 R 中进行简单的自然三次样条逻辑回归（为简单起见，仅从一个内部结开始，因此 df=2），我会得到：
require(&quot;splines&quot;)
logMstarFitSp_spline2df &lt;- glm(bar ~ ns(logmstar, 2),
family = 二项式，logmstarBarSpTable)
summary(logMstarFitSp_spline2df)

系数：
估计标准误差 z 值 Pr(&gt;|z|) 
(截距) 0.5049 0.2285 2.210 0.0271 *
ns(logmstar, 2)1 -0.2487 0.5061 -0.492 0.6231 
ns(logmstar, 2)2 -0.7645 0.4276 -1.788 0.0738 .
AIC：828.16

不管怎样，我可以通过以下方式找出单个内部结点位于 logmstar = 9.5575 处
str(attributes(summ$terms)$predvars)
language list(bar, ns(logmstar, knots = c(`50%` = 9.5575), Boundary.knots = c(8.759, 11.024), intercept = FALSE))

... 我不知道如何解释这一点，也不知道 Python 中的 logit 表达式是什么样子。什么是“系数”在这个模型中？
原则上，我认为它们应该是与样条函数相乘的系数，因此
 logit = beta_0*h_1(x) + beta_1*h_2(x) + beta_1*h_3(x)

其中 h_1、h_2 和 h_3 是三个基函数（并且 beta_0 = &quot;(Intercept)&quot;，等等）。
但是基函数是什么？（我尝试阅读 R 函数 ns 的文档，结果让我感到困惑；而且我在这个网站上的各种样条函数请求问题中没有找到太多帮助。）]]></description>
      <guid>https://stats.stackexchange.com/questions/652801/how-to-interpret-coefficients-of-logistic-regression-using-natural-cubic-splines</guid>
      <pubDate>Wed, 14 Aug 2024 16:10:12 GMT</pubDate>
    </item>
    <item>
      <title>当涉及到潜在变量时，为什么我们在寻找 p(data) 时需要边缘化？（elbo 推导的一部分）</title>
      <link>https://stats.stackexchange.com/questions/652800/why-do-we-need-to-marginalize-when-finding-pdata-when-latent-variables-are-inv</link>
      <description><![CDATA[与 elbo 的推导非常混淆。在推导过程中，p(data) 是难以处理的，因为它涉及高维潜在变量的积分。我不明白为什么在计算 p(data) 时会涉及潜在变量。以图像为例，为什么 p(特定图像) 不是 = 1 / 数据集中的图像总数？我不确定为什么在计算 p(data) 时会涉及潜在变量，因为它不在等式的左侧，并且我们提供数据集，因此我们应该了解有关数据的所有信息。
任何帮助都非常感谢]]></description>
      <guid>https://stats.stackexchange.com/questions/652800/why-do-we-need-to-marginalize-when-finding-pdata-when-latent-variables-are-inv</guid>
      <pubDate>Wed, 14 Aug 2024 16:03:53 GMT</pubDate>
    </item>
    <item>
      <title>用于建模财务回报的 AR(1) 过程的时间缩放</title>
      <link>https://stats.stackexchange.com/questions/652799/time-scaling-of-ar1-process-for-modelling-financial-returns</link>
      <description><![CDATA[过程：
考虑一个均值为零的 AR(1) 过程，*
$\lambda_t = \kappa \cdot \lambda_{t-1} + \omega_t$，
其中 $\kappa = 0.9$，$\omega \sim N(0, \sigma_{\omega}^2)$，并且 $\sigma_{\omega}^2 = 0.00027$。 $\lambda_0$ 的初始值取自平稳分布 $N \left(0, \frac{\sigma_{\omega}^2}{(1-\kappa^2)} \right)$
我使用此过程生成长度为 $T=672$ 的样本。
* 我知道，对于股票收益建模而言，均值为零是不现实的，但我的问题并不取决于此选择。

上述时间序列应解释为每月收益（以百分点表示），即 672 个月的观测值。
问题：
什么是合适的参数值生成总共 14,112 个每日观测值（即$672 \times 21$，如果我们假设一个月内有 21 个交易日）以符合上述（每月）流程？月回报率是给定月份内所有 21 天回报率的累计乘积。
尝试（在 R 中）：
DAYS &lt;- 21
T &lt;- 672
kappa &lt;- 0.9
variance_omega &lt;- 0.00027

get_init_lambda &lt;- function(variance) return(rnorm(n = 1, mean = 0, sd = sqrt(variance / (1-(kappa)^2))))

#### 月度分析

set.seed(1234)

lambda_T &lt;- vector(mode = &quot;numeric&quot;, length = T)

lambda_shock &lt;- rnorm(T, mean = 0, sd = sqrt(variance_omega))

lambda_T[1] &lt;- kappa * get_init_lambda(variance_omega) + lambda_shock[1]
for(i in 2:T) lambda_T[i] &lt;- kappa * lambda_T[i-1] + lambda_shock[i]

acf(lambda_T)$acf[2]
# [1] 0.9064949 # 符合预期

var(lambda_T)
# [1] 0.001547795

#### 每日分析

set.seed(1234)

kappa_daily &lt;- 0.90 # ??? 如何设置 kappa_daily，使月收益 AC = 0.9？

lambda_T_daily &lt;- vector(mode = &quot;numeric&quot;, length = T * DAYS)

lambda_shock_daily &lt;- rnorm(T * DAYS, mean = 0, sd = sqrt(variance_omega / DAYS))

lambda_T_daily[1] &lt;- kappa_daily * get_init_lambda(variance_omega / DAYS) + lambda_T_daily[1]
for(i in 2:(T * DAYS)) lambda_T_daily[i] &lt;- kappa_daily * lambda_T_daily[i-1] + lambda_shock_daily[i]

# 检索月末指数；假设每个月有 21 个交易日
begin_month &lt;- seq(1, T * DAYS, 21)
end_month &lt;- c(tail(begin_month, -1) - 1, T * DAYS)

lambda_monthly_aggregate &lt;- vector(mode = &quot;numeric&quot;, length = T)

for(i in 1:T){
month_ind &lt;- (begin_month[i]):(end_month[i])

daily_ret_within_month &lt;- 0.01*lambda_T_daily[month_ind] # 收益以 % 表示
monthly_return &lt;- 100*(cumprod(1+daily_ret_within_month) - 1) # 累计每日收益

lambda_monthly_aggregate[i] &lt;- monthly_return[DAYS] # 检索累计。 21 天后返回
}

# 与上述值不相同：自相关性太低，mth 方差返回值太高！
&gt; acf(lambda_monthly_aggregate)$acf[2]
[1] 0.2988249

var(lambda_monthly_aggregate)
[1] 0.01494742

]]></description>
      <guid>https://stats.stackexchange.com/questions/652799/time-scaling-of-ar1-process-for-modelling-financial-returns</guid>
      <pubDate>Wed, 14 Aug 2024 16:01:50 GMT</pubDate>
    </item>
    <item>
      <title>戴明回归和数值配方</title>
      <link>https://stats.stackexchange.com/questions/652796/deming-regression-and-numerical-recipes</link>
      <description><![CDATA[我正在尝试对戴明回归进行分类，这意味着我想通过 x 和 y 坐标中都有“错误”的数据来拟合一个函数（一条直线）。我正在查看《数值方法》第 15.3 章，发现他们的方法的基础很神秘。
对我来说最令人费解的是他们选择最小化的优值函数：
\begin{equation}
\chi^{2} = \sum_{i=1}^{N} \frac{(y_{i} - a - b\cdot x_{i})^{2}}{\sigma_{yi}^{2} + b^{2}\cdot\sigma_{xi}^{2}}
\end{equation&gt;
这对我来说看起来不对。 $y_{i} - a - b\cdot x_{i}$ 是 y 中的残差。难道不应该有一个附加项，用于最小化线和 x 数据之间的平方距离吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652796/deming-regression-and-numerical-recipes</guid>
      <pubDate>Wed, 14 Aug 2024 15:31:13 GMT</pubDate>
    </item>
    <item>
      <title>R 中的 CausalImpact 包 - 单尾概率</title>
      <link>https://stats.stackexchange.com/questions/652794/causalimpact-package-in-r-one-tailed-probability</link>
      <description><![CDATA[这是我第一次使用贝叶斯统计和 R 中的 CausalImpact 包。我有点困惑这是使用单尾还是双尾概率测试。有人可以提供一些建议吗？摘要输出的详细选项表明它是单尾的。我试图测试干预后是否发生了变化（正面或负面），而不是考虑特定的方向。这仍然是一种合适的方法吗？
提前致谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/652794/causalimpact-package-in-r-one-tailed-probability</guid>
      <pubDate>Wed, 14 Aug 2024 15:23:38 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用配对 t 检验还是非配对 t 检验来比较相似主题的两组测量值吗？</title>
      <link>https://stats.stackexchange.com/questions/652791/should-i-use-the-paired-t-test-or-the-unpaired-t-test-for-comparing-two-sets-of</link>
      <description><![CDATA[我遇到了以下问题：
我记录了 30 次程序的执行时间，然后实施了优化并再次记录了 30 次执行时间（在同一台机器上）。我不确定是否应该使用非配对 t 检验或配对 t 检验来证明优化对执行时间产生了显著影响。
一方面，分布不是独立的，哪些“个体”配对并不重要；另一方面，将这些测量值配对对我来说毫无意义，因为没有有意义的“个体”可以配对。
最接近使用配对 t 检验的例子是配对的个体共享一个特征而其他一些条件发生变化的实验（例如两个具有相同智商的个体），但我不知道哪个特征可以证明将我的问题中的两个测量值配对在一起是合理的。
任何帮助都将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/652791/should-i-use-the-paired-t-test-or-the-unpaired-t-test-for-comparing-two-sets-of</guid>
      <pubDate>Wed, 14 Aug 2024 14:41:48 GMT</pubDate>
    </item>
    <item>
      <title>将回归中的 β 解释为单位变化</title>
      <link>https://stats.stackexchange.com/questions/652790/interpreting-%ce%b2-in-regression-as-unit-change</link>
      <description><![CDATA[在变量已标准化的线性回归中，X 中一个 SD 的变化与 Y 中 β*1 SD 的变化相关，我见过人们用单位来解释这一点，因为我们知道 SD 是什么（例如，我在一本统计学教科书中看到过这种情况：“标准化 β = 0.192 表示，在图像量表上评分高出一个标准差（1.40 个单位）的乐队可以预期专辑销量增加 0.192 个标准差单位。这是 15,490
张销量（0.192 × 80,699）的变化。图像评分比另一乐队高 1.40 的乐队可以预期专辑销量增加 15,490销售额。”）。但标准化后，不是所有的 SD 都等于 1 吗？我确信我遗漏了什么，因为这似乎不太合理……]]></description>
      <guid>https://stats.stackexchange.com/questions/652790/interpreting-%ce%b2-in-regression-as-unit-change</guid>
      <pubDate>Wed, 14 Aug 2024 14:35:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么 ARIMA 对相同数据进行预测时，会指出数据在一个预测持续时间内是白噪声，而没有指出它在另一个持续时间内不是白噪声</title>
      <link>https://stats.stackexchange.com/questions/652789/why-arima-on-same-data-says-data-is-white-noise-for-one-forecast-duration-while</link>
      <description><![CDATA[我正在对我的数据运行 ARIMA 模型。我有 2021 年 1 月的每周数据。当我运行 12 周预测时，ARIMA 给出最佳参数值 (0,0,0)，表明数据是白噪声。但是当我使用相同的数据并运行 8 周预测时，最佳参数值是 (4,0,4)，这意味着数据不是白噪声。
有人可以告诉我预测持续时间是如何使数据成为白噪声的吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652789/why-arima-on-same-data-says-data-is-white-noise-for-one-forecast-duration-while</guid>
      <pubDate>Wed, 14 Aug 2024 14:08:01 GMT</pubDate>
    </item>
    <item>
      <title>Aalen Johansen - 绝对风险和置信区间</title>
      <link>https://stats.stackexchange.com/questions/652776/aalen-johansen-absolute-risks-and-confidence-intervals</link>
      <description><![CDATA[如果存在竞争风险情况，可以使用 AJ 估计量来计算累积发生率函数 (CIF)。
在 R 中，使用 survfit{survival 和事件 0,1,2 来实现 AJ 非常简单。
在检查时间 20 的绝对风险时，R 还会计算置信区间 $lower 和 $upper，例如：



时间
cif
cif_lower
cif_upper




20
0.0175
0.0097
0.0314



因此，在时间 20 时，1.75% 的人遭遇了该事件 (CI 0.97% - 3.14%)
问题：

那是什么样的置信区间？
AJ 估计量是否需要特定的置信区间？
这个特定的置信区间是否需要使用匹配/加权后需要进一步调整吗？
]]></description>
      <guid>https://stats.stackexchange.com/questions/652776/aalen-johansen-absolute-risks-and-confidence-intervals</guid>
      <pubDate>Wed, 14 Aug 2024 10:55:55 GMT</pubDate>
    </item>
    <item>
      <title>简单的 OLS 来测量相关性</title>
      <link>https://stats.stackexchange.com/questions/652774/simple-ols-to-measure-correlation</link>
      <description><![CDATA[我有两个变量，X 和 Y，我有充分的理由相信它们是同时确定的。
$$Y_{i} = a_{1i} + b_{1i}X_{i} + u_{1i}\tag{1}$$
$$X_{i} = a_{2i} + b_{2i}Y_{i} + u_{2i}\tag{2}$$
我的问题是：只要我远离任何因果关系，我是否仍可以使用简单的 OLS 研究变量之间的关系？
我有兴趣做出这种类型的陈述：“X 的单位增加与 Y 的预测值 b1 的变化相关”。我不想暗示任何有关因果关系方向的内容，我只想描述变量如何共同变动。在这种情况下，我承认明显的内生性，报告系数是否仍然具有误导性？
更新：数据纯粹是横截面数据]]></description>
      <guid>https://stats.stackexchange.com/questions/652774/simple-ols-to-measure-correlation</guid>
      <pubDate>Wed, 14 Aug 2024 10:25:27 GMT</pubDate>
    </item>
    <item>
      <title>Delta 方法和方差收敛</title>
      <link>https://stats.stackexchange.com/questions/652753/delta-method-and-the-variance-convergence</link>
      <description><![CDATA[在我看过的参考文献中，$\Delta$ 方法通常以分布收敛的形式来表述：对于 $X_i$ i.i.d.，$\mathbb{E}[X_i]=\mu$ 和 $\mathrm{Var}_\mu(X_i)=\sigma^2&lt;\infty$（因此 CLT 对样本均值成立，$\bar{X}_n$），并且 $g$ 是一个足够平滑的函数，
$$
\sqrt{n}g(\bar{X}_n) -g(\mu))\Rightarrow N(0,g&#39;(\mu)^2\sigma^2)
$$
我正在看一篇论文（参见 https://arxiv.org/abs/1910.06222 中的 (11)），其中有以下估计：
$$
\lim_{n\to \infty} n \mathbb{E}[(\bar{X}_n) -g(\mu))^2] = g&#39;(\mu)^2\sigma^2
$$
我意识到，我不确定如何将 CLT 型弱收敛映射到二阶中心矩的这种收敛。我将如何论证这一点？
编辑：不出所料，我发现如果我们对导数有先验界限，那么结果就会成立（参见 Bickel 和 Doksum 的教科书）。但这一假设似乎并不存在于 Song &amp; Ermon 的论文中。]]></description>
      <guid>https://stats.stackexchange.com/questions/652753/delta-method-and-the-variance-convergence</guid>
      <pubDate>Wed, 14 Aug 2024 00:00:08 GMT</pubDate>
    </item>
    <item>
      <title>MLE 和非闭形式的解 - 如何查看或证明它？</title>
      <link>https://stats.stackexchange.com/questions/652748/mle-and-non-closed-form-solutions-how-to-see-or-proof-it</link>
      <description><![CDATA[我真的无法理解 MLE 的非闭式解。在讲座中，我们没有深入探讨它，只是假设某些对数似然函数（或者说分数函数）无法针对其各自的参数进行求解。
例如，如果我们使用逻辑回归对二元响应进行建模：

$y_i \sim Ber(\pi_i)$
$\pi_i = P(y_i=1) = h(\eta_i) = E(y_i) = \frac{exp(\eta_i)}{1+exp(\eta_i)}$
$\eta_i = g(\pi_i)= log(\frac{\pi_i}{1-\pi_i}) = \beta_0 + \beta_1x_i$

由此我们可以得到：

$ \ell(\beta) = \sum\limits_{i=1}^{n}[y_ilog(\pi_i) + (1-y_i)log(1-\pi_i)]$
$s(\beta) = \sum\limits_{i=1}^{n} \frac{\partial\eta_i}{\partial\beta} \frac{\partial\pi_i}{\partial\eta_i} \frac{\partial}{\partial\pi_i} \ell_i(\beta)$ $\leftarrow$（如果我没记错的话）

$s(\beta)$ 的哪一部分导数是不可计算的，我如何才能看到甚至证明它？]]></description>
      <guid>https://stats.stackexchange.com/questions/652748/mle-and-non-closed-form-solutions-how-to-see-or-proof-it</guid>
      <pubDate>Tue, 13 Aug 2024 21:38:32 GMT</pubDate>
    </item>
    <item>
      <title>当需要使用 Johnson 变换对数据进行变换时，如何评估公差区间研究</title>
      <link>https://stats.stackexchange.com/questions/652736/how-to-assess-a-tolerance-interval-study-when-data-needs-to-be-transformed-using</link>
      <description><![CDATA[我正在做一个变量数据分析，其中我需要进行公差区间评估以确保我的总体符合 6 lbf 的下限规格限 (LSL)。为了能够进行公差区间分析，我需要证明总体符合正态分布。鉴于我的数据分析显示 p 值小于 0.05，表明数据不服从正态分布，我进行了分布识别分析并确定 Johnson 变换是唯一合适的拟合。但是，在计算公差区间时，结果为 -2.597。我如何评估我的分析是否已成功完成，以及尽管出现这种结果，结果是否有效？提醒一下，结果是下限公差区间 (LTL) = -2.597 和我的下限规格限 (LSL) = 6，这是否意味着我的研究不符合规格？我读到过我需要使用约翰逊变换来转换 6 lbf 的规格，但我不知道该怎么做。
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/652736/how-to-assess-a-tolerance-interval-study-when-data-needs-to-be-transformed-using</guid>
      <pubDate>Tue, 13 Aug 2024 18:34:38 GMT</pubDate>
    </item>
    </channel>
</rss>