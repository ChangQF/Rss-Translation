<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 03 Sep 2024 18:20:55 GMT</lastBuildDate>
    <item>
      <title>在使用 k 倍交叉验证训练随机森林分类模型后，我应该在训练集中使用 ROC 曲线吗？</title>
      <link>https://stats.stackexchange.com/questions/653801/should-i-use-roc-curve-in-my-training-set-after-training-a-random-forest-classif</link>
      <description><![CDATA[我有一个概念性问题：将数据集分为训练集和测试集（70:30），两者平衡且经过打乱，我是否应该使用由训练集的 k 倍交叉验证生成的模型的混淆矩阵和 ROC 曲线？
我之所以问这个问题，是因为每次我从网格搜索中获得的准确率总是接近 61%。当我用训练集制作混淆矩阵时，准确率是 57%。但是当我用训练集制作 ROC 曲线和混淆矩阵时，准确率总是接近 100%。这是过度拟合的迹象吗？还是我应该只考虑交叉验证和训练集的准确度值（61% 和 57%）？
这是我的代码：
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify=y, shuffle=True)

param_grid = {
&#39;n_estimators&#39;: [50, 100, 200, 250], # 植物的树状图数量
&#39;max_depth&#39;: [3, 5, 8, 10], # 植物的最大深度
&#39;min_samples_split&#39;: [2, 5, 10], # 最小样本数&#39;min_samples_leaf&#39;: [1, 2, 4], # &#39;max_features&#39;: [&#39;sqrt&#39;], # &#39;oob_score&#39; 所需的最少数量: [True] } rf = RandomForestClassifier( random_state=0) grid_search = GridSearchCV( estimator=rf, param_grid=param_grid, cv=10, # 10 倍交叉验证 n_jobs=-1, # CPU 调度核心的待办事项 verbose=2, # 总线处理的优化
评分=&#39;accuracy&#39;，# 评估指标
return_train_score=True
)

grid_search.fit(x_train, y_train)

print(&quot;最佳超参数：&quot;)
print(grid_search.best_params_)
#最佳超参数：
#{&#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 2, &#39;min_samples_split&#39;: 5, #&#39;n_estimators&#39;: 100, &#39;oob_score&#39;: True}

print(&quot;最佳交叉验证准确率：&quot;)
print(grid_search.best_score_)
#最佳交叉验证准确率：0.6137931034482758

best_model = grid_search.best_estimator_
test_accuracy = best_model.score(x_test, y_test)
test_accuracy
#0.5684931506849316

train_accuracy = best_model.score(x_train, y_train)
train_accuracy
#0.9844827586206897

PS：我在 scikt learn 文档这里中看到，它在 k 倍之后在测试集中的验证停止，但我没有发现任何提到使用训练数据集再次验证的内容。]]></description>
      <guid>https://stats.stackexchange.com/questions/653801/should-i-use-roc-curve-in-my-training-set-after-training-a-random-forest-classif</guid>
      <pubDate>Tue, 03 Sep 2024 17:40:52 GMT</pubDate>
    </item>
    <item>
      <title>带下标的期望的推导</title>
      <link>https://stats.stackexchange.com/questions/653800/derivation-of-expectation-with-subscript</link>
      <description><![CDATA[我正在根据Calvin Luo 的扩散教程推导去噪扩散概率模型 (DDPM)，他在教程中最终开发了重建项、先验匹配项和去噪匹配项。
至于去噪匹配项，我们有：
$$\mathbb{E}_{q(\mathbf{x}_{t}, \mathbf{x}_{t-1} \mid \mathbf{x}_0)} \left[ \log \frac{p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0)} \right]$$
不知何故，这变成了以下 KL 散度：
$$- \mathbb{E}_{q(\mathbf{x}_t \mid \mathbf{x}_0)} \left[ \mathcal{D}_{\text{KL}} (q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \mid\mid p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t)) \right]$$
我不明白前者如何变成后者。我不明白对数中翻转分子和分母如何改变符号，因此它看起来非常类似于 KL 散度。
但是：期望的下标如何变化？我猜它与概率的链式法则有关，但我在网上找到的有关带下标的期望的属性的信息很少。]]></description>
      <guid>https://stats.stackexchange.com/questions/653800/derivation-of-expectation-with-subscript</guid>
      <pubDate>Tue, 03 Sep 2024 17:14:16 GMT</pubDate>
    </item>
    <item>
      <title>季节性差分后 ADF/KPSS 检验的适用性</title>
      <link>https://stats.stackexchange.com/questions/653797/appropriateness-of-adf-kpss-tests-after-seasonal-differencing</link>
      <description><![CDATA[给定一个具有已知季节性的序列，我尝试在执行季节性差分后使用 ADF 和 KPSS 测试来验证平稳性。我正在使用这些测试的 statsmodels 实现。
import pandas as pd
from statsmodels.tsa.stattools import adfuller, kpss

def adf_test(timeseries, return=&#39;c&#39;):
test = adfuller(timeseries, return=regression, autolag=&#39;AIC&#39;)
output = pd.Series(
test[0:4],
index=[
&#39;测试统计量&#39;,
&#39;p 值&#39;,
&#39;使用的滞后数&#39;,
&#39;使用的观测值数量&#39;
]
)
for key, value in test[4].items():
output[f&#39;临界值 ({key})&#39;] = value
return output

def kpss_test(timeseries):
test = kpss(timeseries, return=&#39;c&#39;, nlags=&#39;auto&#39;)
output = pd.Series(
test[0:3], index=[&#39;测试统计量&#39;, &#39;p-value&#39;, &#39;# Lags Used&#39;]
)
for key, value in test[3].items():
output[f&#39;临界值 ({key})&#39;] = value
return output

可以使用以下方法重现原始数据：
data = {&#39;Value&#39;: {
&#39;2018-01-31&#39;: 104242, &#39;2018-02-28&#39;: 98662, &#39;2018-03-31&#39;: 105616, 
&#39;2018-04-30&#39;: 101179, &#39;2018-05-31&#39;: 98739, &#39;2018-06-30&#39;: 100133, &#39;2018-07-31&#39;: 107332, &#39;2018-08-31&#39;: 103455, &#39;2018-09-30&#39;: 81907, &#39;2018-10-31&#39;: 92845, &#39;2018-11-30&#39;: 83670, &#39;2018-1 2-31&#39;：84512，&#39;2019-01-31&#39;：108605，&#39;2019-02-28&#39;：97747，&#39;2019-03-31&#39;：101699，&#39;2019-04-30&#39;：103547，&#39;2019-05-31&#39;： 100278, &#39;2019-06-30&#39;: 98013, &#39;2019-07-31&#39;: 115181, &#39;2019-08-31&#39;: 110444, &#39;2019-09-30&#39;: 99455, &#39;2019-10-31&#39;: 100262, &#39;2019- 11-30&#39;: 88399, &#39;2019-12-31&#39;: 94493, &#39;2020-01-31&#39;: 109185, &#39;2020-02-29&#39;: 99482, &#39;2020-03-31&#39;: 108055, &#39;2020-04-30&#39;: 102379, &#39;2020-05-31&#39;: 115724, &#39;2020-06-30&#39;: 137875, &#39;2020-07-31&#39;: 136895, &#39;2020-08-31&#39;: 134005, &#39;2020-09-30&#39;: 123025, &#39;202 0-10-31&#39;：155176，&#39;2020-11-30&#39;：116240，&#39;2020-12-31&#39;：117168，&#39;2021-01-31&#39;：136058，&#39;2021-02-28&#39;：122891，&#39;2021-03-31&#39;： 166424, &#39;2021-04-30&#39;: 152417, &#39;2021-05-31&#39;: 126856, &#39;2021-06-30&#39;: 173682, &#39;2021-07-31&#39;: 166529, &#39;2021-08-31&#39;: 147111, &#39;202 1-09-30&#39;：137624，&#39;2021-10-31&#39;：133565，&#39;2021-11-30&#39;：132117，&#39;2021-12-31&#39;：124160，&#39;2022-01-31&#39;：135815，&#39;2022-02-28&#39;： 127005, &#39;2022-03-31&#39;: 145378,
&#39;2022-04-30&#39;: 138972, &#39;2022-05-31&#39;: 140055, &#39;2022-06-30&#39;: 164881,
&#39;2022-07-31&#39;: 164568, &#39;2022-08-31&#39;: 178675, &#39;2022-09-30&#39;: 149753,
&#39;2022-10-31&#39;: 127887, &#39;2022-11-30&#39;: 114095, &#39;2022-12-31&#39;: 115408}}
df = pd.DataFrame(data)

其图表为：

我的测试为：
adf_test(df.diff(12).dropna())
kpss_test(df.diff(12).dropna())

ADF 测试返回临界值 -3.61 和 p 值 0.005。
KPSS 测试返回临界值 0.25 和 p 值 0.1，插值警告实际 p 值大于返回的 p 值。
这些表明季节性差分后不存在单位根和（趋势）平稳性。但是，我收到反馈：

ADF 测试的 regression 参数应为 &#39;n&#39;。
KPSS 测试不合适，因为其回归参数只能是 &#39;c&#39; 或 &#39;ct&#39;，而不能是 &#39;n&#39;。
可视化的 12 个月差分序列似乎不是平稳的。

这些说法合适吗？尤其是，我从来没有听说过 KPSS 测试以这种方式不合适。]]></description>
      <guid>https://stats.stackexchange.com/questions/653797/appropriateness-of-adf-kpss-tests-after-seasonal-differencing</guid>
      <pubDate>Tue, 03 Sep 2024 16:01:06 GMT</pubDate>
    </item>
    <item>
      <title>功能错误指定的方法</title>
      <link>https://stats.stackexchange.com/questions/653796/a-method-for-functional-misspecification</link>
      <description><![CDATA[我最近一直在思考无偏性。一个想法浮现在我的脑海。想象一下，数据集被分成三个不同的部分，即训练、将添加到训练集和测试集的数据。我们首先修改和构建线性模型。
当我们将额外的观察结果纳入训练集时，估计值会发生变化。如果在测试集估计方面，从包含额外观察结果后获得的数据集中获得的估计值优于受限数据集的估计值（比如说更低的 SSR），那么我们可以假设最初指定的模型是无偏的，没有功能错误指定（考虑到它满足无偏估计量的假设）吗？它是解决功能错误指定问题的有用解决方案吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/653796/a-method-for-functional-misspecification</guid>
      <pubDate>Tue, 03 Sep 2024 15:52:22 GMT</pubDate>
    </item>
    <item>
      <title>lme4 包中 BLUPs 的适用功能</title>
      <link>https://stats.stackexchange.com/questions/653795/suitable-function-of-blups-in-lme4-package</link>
      <description><![CDATA[我想分析一个包含 4 年内 2 种条件（干旱胁迫和正常）下 12 种基因型的数据集。
我使用了一篇使用 BLUPs 方法的文章的分析作为我的分析模型，该模型使用 BLUPs 方法评估多年来单个基因型在每种处理下的特征，使用 R 中的“lme4”包，但我不知道该使用哪个函数。]]></description>
      <guid>https://stats.stackexchange.com/questions/653795/suitable-function-of-blups-in-lme4-package</guid>
      <pubDate>Tue, 03 Sep 2024 15:11:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vgg19 进行图像重建的训练收敛停滞</title>
      <link>https://stats.stackexchange.com/questions/653794/training-convergence-stagnation-in-image-reconstruction-using-vgg19</link>
      <description><![CDATA[-----------------------------------------------------上下文---------------------------
我正在尝试实现这篇论文通过特征变换实现通用风格迁移。在论文中，他们针对预训练的 vgg19 编码器编码的 relu1_1、relu2_1、relu3_1、relu4_1、relu5_1 特征输出训练了 5 个解码器。为了实现这一点，他们使用了每像素损失（生成的图像和训练图像之间的均方误差）和特征损失（训练特征输出和生成的图像特征输出之间的均方误差）。他们使用的这两个损失之间的权重是 1.0。在训练期间，预训练的 vgg 模型保持不变
-----------------------实现差异 --------------------------
在论文中，他们使用了来自 vgg 论文的预训练 vgg19。我使用了来自 pytorch 的预训练模型。因此预处理不同。在 vgg 论文中，仅从 0 到 255 的像素值中减去平均值。在 pytorch 中，平均值 = [0.485, 0.456, 0.406] 和 std = [0.229, 0.224, 0.225] 这些值用于规范化。
第二个区别是，我在解码器输出后使用了一个额外的 tanh 层来获取从 -1 到 1 的像素值，我在训练期间将其重新缩放到 0 和 1 之间，这后来导致了问题，因为与每个像素的特征损失相比，损失变得太低。他们在实现中没有使用 tanh 或任何边界函数。他们的官方推理代码可以在此处找到
--------------------------问题---------------------------
无论每像素损失和内容损失之间的权重如何。我能够为 relu1_1、relu2_1、relu3_1（此处使用权重 1）生成相当不错的图像重建。问题始于从 relu4_1（生成的图像中有一些伪影但仍然可以）和主要 relu5_1 进行图像重建，因为更多像素值和精细细节丢失了。在 relu5_1 中，如果我使用权重值 1。两个训练有效图像的生成图像如下所示-

因此，我将每像素损失增加到 200，并尝试在单个批次上进行过度拟合，经过 1000 次迭代后，我能够在大小为 16 的批次上进行过度拟合，损失小于 0.2。图像看起来也相当不错。但是当使用整个数据集进行训练时，训练变得非常不稳定。损失从 13 开始下降到 6。但此后，训练损失下降得非常非常慢。在整个训练过程中，训练损失也非常不稳定。输出如下所示

在图像中我们可以看到内容缺失。还产生了一些伪影。我检查了单个像素和特征损失。我发现特征损失下降非常缓慢，并且每个像素的损失更新不稳定。我使用了批处理大小为 16、lr=1e-4 和梯度裁剪和 adam 优化器。在 coco 2014 数据集上进行了 15K 次迭代。我的问题是

我现在应该做什么？
我是否应该花更多时间找到两种损失之间的完美平衡。但这似乎非常耗时，因为在小批量上，模型总是过度拟合各种权重值。但在全批次上训练时，训练损失仍然很高，并且在 3 个 epoch 后更新非常缓慢
我是否应该使用更高的批次大小和学习率（高学习率会导致更不稳定的训练）
我是否应该使用学习率调度程序
这种情况收敛问题是否常见，其中训练损失确实得到了理想的损失
]]></description>
      <guid>https://stats.stackexchange.com/questions/653794/training-convergence-stagnation-in-image-reconstruction-using-vgg19</guid>
      <pubDate>Tue, 03 Sep 2024 15:00:46 GMT</pubDate>
    </item>
    <item>
      <title>为什么 R 中的并行引导比顺序引导（使用 boot 和 future 包）慢？[迁移]</title>
      <link>https://stats.stackexchange.com/questions/653792/why-is-parallel-bootstrapping-in-r-slower-than-sequential-bootstrapping-using-b</link>
      <description><![CDATA[我正在使用 R 中的 boot 包进行引导分析，选项为 parallel = &#39;snow&#39;（适用于 Windows 计算机）。但是，并行计算比顺序计算更慢或相同。
我尝试更改设置以增加核心数量（ncpus = 2 和 ncpus = 4）并手动设置集群，然后使用 cl = mycluster 选项。增加核心数量和设置集群使时间进一步变慢。我找不到有关 boot 包中 boot 函数使用的默认核心数的文档。
我还尝试使用相同的 bootstrap 函数来使用 future 包，这再次比顺序计算慢，而且当我将核心数从 2 增加到 4 时，速度会更慢。我尝试了 15、30、500 和 1000 次迭代，发现了类似的结果（使用 2 个工作器的 1000 次迭代是最慢的）。
本质上，在我的 bootstrap 中，我正在计算权重并计算非常大样本的加权率（一次迭代需要 ~2 分 30 秒）。我的 bootstrap 函数很长，所以我在下面放了一个简化版本。在简化版本中，并行计算更快，但在我的实际代码中并非如此。
我是并行计算的新手，所以不确定为什么它比顺序计算慢。我是不是漏掉了什么？
set.seed(1)
R = 1000

library(boot)
library(survival)
library(dplyr)

my_bootstrap_fx &lt;- function(data, indices) {

d &lt;- data[indices,]

# 计算 IPTW 权重
denom_fit &lt;- glm(trt ~ age + previous + karno, data = d)
numer_fit &lt;- glm(trt ~ 1, data = d)
pn_trt &lt;- predict(numer_fit, type = &#39;response&#39;)
pd_trt &lt;- predict(denom_fit, type = &#39;response&#39;)
d$iptw &lt;- if_else(d$trt == 1, ((1-pn_trt) / (1-pd_trt)), pn_trt/pd_trt)

# 计算风险比
dtime &lt;- unique(veteran$time[veteran$status==1]) 
newdata &lt;- survSplit(Surv(time, status) ~ trt + celltype + karno,
data=veteran, cut=dtime)
fit &lt;- coxph(Surv(tstart, time, status) ~ trt + celltype + karno, weights = newdata$iptw, newdata)
result &lt;- c(fit$coef[1], fit$coef[2])

return(result)

}

# 顺序

system.time(
my_boostrap_results &lt;- boot(data = veteran, statistic = my_bootstrap_fx, R = R)
)

# 使用 boot 并行

system.time(
my_bootstrap_results &lt;- boot(data = veteran, statistic = my_bootstrap_fx, R = R, parallel = &#39;snow&#39;)
)

# 使用 boot 并行并手动设置集群
library(parallel)

mycluster &lt;- makeCluster(4)
clusterExport(mycluster, list(&#39;my_bootstrap_fx&#39;, &#39;veteran&#39;))
clusterEvalQ(mycluster, {
library(survival)
library(dplyr)
})

system.time(
my_bootstrap_results &lt;- boot(data = veteran, statistic = my_bootstrap_fx, R = R, parallel = &#39;snow&#39;, ncpus = 2, cl = mycluster)
)

stopCluster(mycluster)

# 使用 Future 进行并行
library(doFuture)
library(foreach)
doFuture::registerDoFuture()
future::plan(multisession, worker = 4)

system.time(
my_bootstrap_results &lt;- foreach(i = 1:R, .combine = rbind) %dofuture% {my_bootstrap_fx(veteran)}
)
```
]]></description>
      <guid>https://stats.stackexchange.com/questions/653792/why-is-parallel-bootstrapping-in-r-slower-than-sequential-bootstrapping-using-b</guid>
      <pubDate>Tue, 03 Sep 2024 13:33:24 GMT</pubDate>
    </item>
    <item>
      <title>完成充分统计量练习问题背后的动机</title>
      <link>https://stats.stackexchange.com/questions/653790/motivation-behind-this-exercise-problem-on-complete-sufficient-statistic</link>
      <description><![CDATA[这来自 Hogg 和 McKean 的《数理统计学导论》第 7 章（充分性），第 7.4 节（完整性和唯一性）。
练习 7.4.10。
设 $Y_1 &lt; Y_2 &lt; \cdots &lt; Y_n$ 为大小为 $n$ 的随机样本的顺序统计量，样本来自具有 pdf $f (x; \theta) = 1/\theta,$ $0 &lt; x &lt; \theta$ 的分布，其他地方为零。通过示例 $7.4.2$，统计量 $Y_n$ 是 $\theta$ 的完全充分统计量，并且它的 pdf
$$g(y_n;\theta)= \frac{ny_n^{n-1}}{\theta^n} , 0&lt;y_n &lt;\theta,$$
其他地方为零。
(a) 找到 $Z = n(\theta − Y_n)$ 的分布函数 $H_n(z; \theta)$。
(b) 找到 $\lim_{n \to \infty} H_n(z; \theta)$，因此是 Z 的极限分布。
我的问题：
这个问题的解决方法非常简单，我并没有问如何解决这个问题。事实上，他在上一章的练习中已经给出过这个问题，而我当时已经解决了。我的问题是“这与完整性和唯一性有什么关系？”
有关我的问题的更多详细信息：
在本文的这一点上，均匀分布$U(0,\theta)$中随机样本的$n^{th}$阶统计量可用作$\theta$的估计量，这一事实已被彻底驳倒。他还提到，已经确定（在示例 $7.4.2$ 中）它是一个完全充分统计量，并且 $n/(n+1)Y_n$ 是 $\theta$ 的所谓 MVUE。他提出的问题（其部分 (a) 和 (b) 之前已经问过）似乎与完整性无关，这感觉很奇怪。
如果您能看到此练习的解决方案与完整性之间的任何联系，即如果您能看到在完整性部分提出此问题的动机，请向我说明。]]></description>
      <guid>https://stats.stackexchange.com/questions/653790/motivation-behind-this-exercise-problem-on-complete-sufficient-statistic</guid>
      <pubDate>Tue, 03 Sep 2024 11:58:23 GMT</pubDate>
    </item>
    <item>
      <title>如何评估忽略重复测量的错误线性模型下的治疗效果估计？</title>
      <link>https://stats.stackexchange.com/questions/653786/how-to-evaluate-estimation-of-treatment-effect-under-wrong-linear-model-ignoring</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/653786/how-to-evaluate-estimation-of-treatment-effect-under-wrong-linear-model-ignoring</guid>
      <pubDate>Tue, 03 Sep 2024 08:11:52 GMT</pubDate>
    </item>
    <item>
      <title>经过超参数调整、数据集平衡和卷积分层后，模型仍然过拟合</title>
      <link>https://stats.stackexchange.com/questions/653776/model-still-overfits-after-hyperparameter-tuning-dataset-balancing-and-convolut</link>
      <description><![CDATA[我尝试将堆叠在一起的 25x25 像素图像分类为 50x25 像素图像是相同 (1) 还是不同 (0)。我使用 keras 创建 NN 层。训练集中有 1 和 0 的实例 10,000 个，而验证集中有 2000 张 1 的图像和 500 张 0 的图像。
相同 (1) 的示例数据集：

不同 (0) 的示例数据集：

Keras 顺序层如下所示：
layers.Input((2*imsize,imsize,3)), 
layers.Reshape((2,imsize,imsize,3)), 
layers.LayerNormalization(axis=[-1,-2,-3]), 
layers.Flatten(), 
layers.Dense(16,activation=&#39;relu&#39;), 
layers.Dense(2,activation=&#39;softmax&#39;) 

然后我使用 adam 优化器编译了这些层，损失和准确率如下：
ml.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), 
metrics=[&#39;accuracy&#39;])
ml_hist = ml.fit(x=training_dataset, epochs=20, validation_data = validation_dataset, shuffle=True)

之后，我使用 epoch=20 和 batch_size=100 训练模型。我根据 epoch 绘制了这些结果。
结果

尝试将验证集平衡为 500 个 1 和 500 个 0 后，结果仍然过度拟合

我甚至删除了正则化并添加了 2 个卷积和池化层有用。它只会导致较低的准确率
layers.Input((2*imsize,imsize,3)), 
layers.Reshape((2*imsize,imsize,3)), 
layers.LayerNormalization(axis=[-1,-2,-3]), 

# 新层 
layers.Conv2D(32, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;),
layers.MaxPooling2D(pool_size=(2, 2)),
layers.Conv2D(64, (3, 3),activation=&#39;relu&#39;,padding=&#39;same&#39;),
layers.MaxPooling2D(pool_size=(2, 2)),

layers.Flatten(), 
layers.Dropout(rate=0.7), 
layers.Dense(16,activation=&#39;relu&#39;),
layers.Dense(2,activation=&#39;softmax&#39;) 


我的问题是：如何解决这个过度拟合问题？我不知道我做错了什么，我尝试将学习率更改为 0.1、0.01 和 0.001。我还尝试在 Flatten() 之后、ReLu 之前添加 0.2、0.5 和 0.7 的 Dropout 层。]]></description>
      <guid>https://stats.stackexchange.com/questions/653776/model-still-overfits-after-hyperparameter-tuning-dataset-balancing-and-convolut</guid>
      <pubDate>Tue, 03 Sep 2024 00:30:42 GMT</pubDate>
    </item>
    <item>
      <title>3-D 图表适合于什么情况？</title>
      <link>https://stats.stackexchange.com/questions/653752/in-what-instances-are-3-d-charts-appropriate</link>
      <description><![CDATA[多年来，我读过的所有与数据分析相关的文章都建议在几乎所有情况下都不要使用三维图表。引用其中一句话，“当二维图表就足够时，永远不要使用三维图表。”
这引出了一个问题：在什么情况下三维图表是合适的？]]></description>
      <guid>https://stats.stackexchange.com/questions/653752/in-what-instances-are-3-d-charts-appropriate</guid>
      <pubDate>Mon, 02 Sep 2024 15:41:10 GMT</pubDate>
    </item>
    <item>
      <title>省略随机斜率时，如何使用 lmer()、anova() 和 bootstrap 评估混合模型？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/653749/how-to-evaluate-mixed-model-using-lmer-anova-and-bootstrap-when-omitting-ra</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/653749/how-to-evaluate-mixed-model-using-lmer-anova-and-bootstrap-when-omitting-ra</guid>
      <pubDate>Mon, 02 Sep 2024 15:12:12 GMT</pubDate>
    </item>
    <item>
      <title>逻辑回归给出的 AUC 比朴素贝叶斯高得多的场景</title>
      <link>https://stats.stackexchange.com/questions/653685/scenarios-where-logistic-regression-gives-much-higher-auc-than-naive-bayes</link>
      <description><![CDATA[据我所知，在预测变量之间不存在协变的情况下，逻辑回归和朴素贝叶斯在预测准确性方面应该给出几乎相同的结果（以 ROC/AUC 等为衡量标准）。
我试图将此与使用逻辑回归比朴素贝叶斯对一组模拟数据的预测准确性高得多的观察结果相协调。
简而言之，我正在模拟一个非常简单的基因型-表型图模型，其中有 n 个位点（基因座）i=1...n，每个位点都有 0 或 1 个等位基因，概率为 p_i。在每个基因座上，0 等位基因对应于“野生型”，1 对应于“风险”，即第 i 个位点的 1 等位基因与比值比 OR_i &gt; 相关1. 每种基因型的疾病（或某种二元表型）风险是通过计算所有位点的 OR_i 乘积来计算的，并且对照/疾病表型的概率与某个基线值的 OR 成比例。请注意，在这个最简单的情况下，每个位点的影响在统计上是独立的，即 OR 在各个位点之间相乘。
尽管如此，对于这个基因型-表型图的简单乘法模型，我始终发现逻辑回归分类器的 AUC 比使用朴素贝叶斯的分类器高得多（我在 R 中使用 glm family=binomial 执行逻辑回归，并在 e1071 R 库中使用 naiveBayes 函数，两者均采用默认设置）。
我试图找出逻辑回归性能更好的合理解释。]]></description>
      <guid>https://stats.stackexchange.com/questions/653685/scenarios-where-logistic-regression-gives-much-higher-auc-than-naive-bayes</guid>
      <pubDate>Sat, 31 Aug 2024 22:33:31 GMT</pubDate>
    </item>
    <item>
      <title>在线性回归下推导 MSE($\hat{\beta}$)</title>
      <link>https://stats.stackexchange.com/questions/653651/deriving-mse-hat-beta-under-linear-regression</link>
      <description><![CDATA[我能够推导出 MSE，但是推导过程中有一部分我不太明白。这是我得到的结果：
事实：

$\mathbb{E}(​​\hat{\beta})=\hat{\beta}\space$（无偏估计量）
$\text{Cov}(\hat{\beta})= \sigma^2[(X^TX)^{-1}] $

根据定义，
$$MSE = \mathbb{E}[||\hat{\beta}-\beta||^2] $$
$$= \space \mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]$$
由于 $\hat{\beta}$ 是无偏的，
$$ \boldsymbol{= \text{tr}[\text{Cov}(\hat{\beta})]} *$$
$$= \text{tr}[\sigma^2(X^T X)^{-1}]$$
$\sigma^2$ 是标量，因此可以分解出来，
$$= \sigma^2 tr[(X^T X)^{-1}] $$
我感到困惑的是行 $*$，我不确定我们是如何得到方程式 $ \text{tr}[\text{Cov}(\hat{\beta})] $。以下是我目前所理解的：
通过偏差-方差分解，
$$ \space \mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]=\mathbb{V}(\hat{\beta})+[\mathbb{E}(​​\hat{\beta})-\beta]^T[\mathbb{E}(​​\hat{\beta})-\beta]$$
我们的估计量是无偏的，因此 $\mathbb{E}(​​\hat{\beta})-\beta= 0$。因此，
$$\mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]=\mathbb{V}(\hat{\beta})$$

首先，$\mathbb{V}(\hat{\beta})$ 应该是一个标量，但对我来说这真的没什么意义，这让我想到了下一个问题...
我假设 $V(\hat{\beta}) = \text{tr}[\text{Cov}(\hat{\beta})]$。这是为什么呢？
]]></description>
      <guid>https://stats.stackexchange.com/questions/653651/deriving-mse-hat-beta-under-linear-regression</guid>
      <pubDate>Sat, 31 Aug 2024 00:13:56 GMT</pubDate>
    </item>
    <item>
      <title>纵向数据的粗化精确匹配 (CEM)</title>
      <link>https://stats.stackexchange.com/questions/653600/coarsened-exact-matching-cem-on-longitudinal-data</link>
      <description><![CDATA[我想比较两个教育项目在不同时间点的效果。换句话说，我在 3 个不同的时间点（T1、T2 和 T3）测试了 2 组学生。我对 T1 的数据进行了 CEM，以便 2 个组在项目开始时在 3 个协变量上尽可能接近匹配。这样，我就可以清楚地评估项目的影响，而不会受到这些协变量的混淆影响。现在，我想评估这些匹配组在 T2 和 T3 的表现差异。这是我遇到一些问题的地方。

这里的估计量是否与 ATT 和条件效应有关（因为匹配是分层进行的）？

如果我想计算效应大小来比较组表现的差异（例如，cohen 的 d），是否应该对其进行加权？因此，例如，在比较 T1、T2 或 T3 的组表现时，我应该使用 T1 匹配的权重吗？

我想在 T3 执行回归分析，包括匹配过程中未包括的一些其他混杂协变量（以便匹配后样本不会变得太小）。因为这是一个分层的纵向数据集，所以我通常执行线性混合建模 (LMM)，对科目和学校进行随机截距。这样可以吗？T1 的权重是否也需要包含在 LMM 中？

]]></description>
      <guid>https://stats.stackexchange.com/questions/653600/coarsened-exact-matching-cem-on-longitudinal-data</guid>
      <pubDate>Fri, 30 Aug 2024 07:05:05 GMT</pubDate>
    </item>
    </channel>
</rss>