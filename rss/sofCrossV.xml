<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Fri, 19 Apr 2024 15:14:19 GMT</lastBuildDate>
    <item>
      <title>是否存在正交性不是最佳的情况？</title>
      <link>https://stats.stackexchange.com/questions/645396/are-there-any-situations-where-orthogonality-is-not-optimal</link>
      <description><![CDATA[数据缩减通常用于避免过度拟合并增强可解释性。流行的数据缩减技术（例如 SVD 或 PCA）将高维数据映射/投影到低维表示空间，其中维度相互正交。自动编码器（通常）还构造输入的低维表示，但不能保证正交性。是否存在应用 SVD 或 PCA 来保证正交性不只是最好的做法的情况？]]></description>
      <guid>https://stats.stackexchange.com/questions/645396/are-there-any-situations-where-orthogonality-is-not-optimal</guid>
      <pubDate>Fri, 19 Apr 2024 15:00:36 GMT</pubDate>
    </item>
    <item>
      <title>随机森林模型运行时间较长[关闭]</title>
      <link>https://stats.stackexchange.com/questions/645394/random-forest-model-running-for-longer-time</link>
      <description><![CDATA[我在包含 80 个数据类的数据集上实现了随机森林分类模型。我的数据集由 3200 x 196608 维度的 X_train 和 3200 x 80 维度的 y_train 组成。我的 model.fit() 已经运行了近 30 分钟，仍然没有结果。我应该怎样做才能最大限度地减少编译/执行时间？]]></description>
      <guid>https://stats.stackexchange.com/questions/645394/random-forest-model-running-for-longer-time</guid>
      <pubDate>Fri, 19 Apr 2024 14:40:42 GMT</pubDate>
    </item>
    <item>
      <title>藤系词的回归期</title>
      <link>https://stats.stackexchange.com/questions/645393/return-period-of-vine-copula</link>
      <description><![CDATA[在“OR”的公式中， 3维Copula建模的联合回归周期，我们需要找到C(u,v,w)，但是当我们使用vine建模时，我们只能得到Copula C(uv|w)。那么，当我们使用 vine copula 对数据进行建模时，如何找到 C(u,v,w)，它仅提供第二棵树上的 C(u,v|w)。以及它如何应用于 4 维的 vine copula。有没有R包可以用来计算3维和4维vine copula的回报期？我想知道您是否可以就此事提供一些见解或指导。]]></description>
      <guid>https://stats.stackexchange.com/questions/645393/return-period-of-vine-copula</guid>
      <pubDate>Fri, 19 Apr 2024 14:40:20 GMT</pubDate>
    </item>
    <item>
      <title>错误传播：如何对 2D 网格上的错误求和？</title>
      <link>https://stats.stackexchange.com/questions/645392/error-propagation-how-to-sum-errors-over-2d-grid</link>
      <description><![CDATA[我有一个数据集，其中包含全球质量变化数据及其来自冰川的不确定性。两者的维度均为 720,360,45，其中前两个维度“i,j”（纬度、经度）坐标和第三个维度“k”为时间（即 45 年）。每个像素都有一个年度质量变化值以及该年度质量变化的相关不确定性（单位：千兆吨）。
我对每个像素应用了时间维度 k 的误差传播公式：
total_err_pixel(i,j) = sqrt((err_year(i,j,k))^2 + (err_year2(i,j,k+1))^2 + ... + (err_year(我,j,k+45))^2)
Total_err_pixel(i+1,j) = sqrt((err_year(i+1,j,k))^2 + (err_year2(i+1,j,k+1))^2 + ... + (err_year( i+1,j,k+45))^2)
等等...

这会产生一个 2D 网格 (720x360)，其总误差值在这 45 年中传播。现在我的问题是：要获得 45 年来全球冰川质量变化的总不确定性值，我可以对整个网格的误差求和，还是需要再次应用误差传播公式，但现在是在空间中（对于每个像素我，j）？
total_error = sqrt((total_err_pixel(i,j))^2 + (total_err_pixel(i+1,j))^2 + ... + (total_err_pixel(i+720,j+360)^ 2）

谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/645392/error-propagation-how-to-sum-errors-over-2d-grid</guid>
      <pubDate>Fri, 19 Apr 2024 14:30:12 GMT</pubDate>
    </item>
    <item>
      <title>估计非常高维度的分数匹配中分数的散度</title>
      <link>https://stats.stackexchange.com/questions/645390/estimating-the-divergence-of-the-score-in-score-matching-in-very-high-dimensions</link>
      <description><![CDATA[我正在训练一个网络以获得概率密度 $s:=\nabla\ln p$ &gt;$p$。假设 $\tilde s$ 是网络预测的分数。我的损失函数包含批量评估的散度 $\nabla\cdot\tilde s$ $x$来自 $p$ 的样本。
现在，痛苦来了：$p$实际上是像MNIST。如果您不知道此设置，也没关系。关键是，$p$定义的空间维度非常大（即使是超低分辨率的MNIST，也是$28\cdot28=784$)。
出于测试目的，我尝试直接计算差异（使用 JAX&lt; /a&gt;，这需要我计算整个雅可比行列式并随后进行跟踪）。但即使对于相对“低”的人来说，维度为 784 我遇到内存不足的异常。
所以，我做了一些研究，并偶然发现了Hutchinson 的技巧。它为所需的散度构建了一个估计器。这个用于分数匹配相关论文Sliced Score Matching: A Scalable Approach to Density and Score Estimation 。然而，事实证明，这个估计器需要大量的样本才能获得合理的估计精度...我再次面临内存不足的异常。 （旁注：我已经使用高斯和拉德马赫投影向量进行了测试；后者效果稍好一些。）
那么，有什么我可以做的吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/645390/estimating-the-divergence-of-the-score-in-score-matching-in-very-high-dimensions</guid>
      <pubDate>Fri, 19 Apr 2024 13:46:12 GMT</pubDate>
    </item>
    <item>
      <title>平稳性对于 ARIMA/ARIMAX/SARIMAX 对于预测目的的重要性</title>
      <link>https://stats.stackexchange.com/questions/645387/importance-of-stationarity-for-arima-arimax-sarimax-for-predictive-purposes</link>
      <description><![CDATA[我现在正在做一个预测项目，我可以利用一些理解来了解为什么平稳性在一般预测时很重要。特别是对于 SARIMAX 模型。
我知道虚假回归的问题。例如，如果 $y$ 和 $x$ 都是两个独立的随机游走，我们进行回归$y$ 在 $x$ 上，即我们适合  $y = ax+b$，那么我们可能会得到一个非常低的 p 值，表明我们非常确定 $a \neq 0$ 的值，即使真实值为 $0$。
但是，如果我对任何统计推断不感兴趣，只对预测能力感兴趣，为什么平稳性很重要？我想如果我们将数据分为训练数据和测试数据，其中训练数据与测试数据有很大不同，那么这就是一个问题，因为模型很难泛化。但如果情况并非如此，我们为什么要关心平稳性？
如果有人能给我一些见解，我将不胜感激。此外，如果有关于该主题的任何文献，那么参考文献会很好。
谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/645387/importance-of-stationarity-for-arima-arimax-sarimax-for-predictive-purposes</guid>
      <pubDate>Fri, 19 Apr 2024 13:12:21 GMT</pubDate>
    </item>
    <item>
      <title>应用总方差定律</title>
      <link>https://stats.stackexchange.com/questions/645386/applying-the-law-of-total-variance</link>
      <description><![CDATA[假设我们有 100 笔正态分布付款的样本，平均值 = 1000 美元，标准差 = 100 美元。其中 10% 的付款有误，应全额退还。另外90%的退款金额为0美元。退款金额有何差异？
我正在尝试使用总方差定律来解决这个问题。 X 是二元结果，支付错误概率=10%。
总方差定律指出：
Var(Y)= E[Var(Y|X)] + Var(E[Y|X])
我想我明白第一项只是 Y 在 X 的每个条件下方差的平均值。因此，当 X=付款错误时，方差为 100^2，即 10,000。当x=无支付错误时，退款金额始终为0，因此方差为0。加权平均值为(10,000 x 0.1) + (0 x .9)或1,000。
所以 E[Var(Y|X)]= 1000。
但是我对第二个术语的含义感到非常困惑。我相信给定 X 时 Y 的预期值为 0.1X。但我不明白如何获得这个术语的方差或者它真正代表什么。如果有人能指出我正确的方向，我将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/645386/applying-the-law-of-total-variance</guid>
      <pubDate>Fri, 19 Apr 2024 13:09:35 GMT</pubDate>
    </item>
    <item>
      <title>关于真实得分方差估计的（低）效率，我们了解多少？</title>
      <link>https://stats.stackexchange.com/questions/645385/what-is-known-about-the-inefficiency-of-true-score-variance-estimation</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/645385/what-is-known-about-the-inefficiency-of-true-score-variance-estimation</guid>
      <pubDate>Fri, 19 Apr 2024 13:09:09 GMT</pubDate>
    </item>
    <item>
      <title>在不平衡、重复测量、配对观察设置中最合适的统计模型</title>
      <link>https://stats.stackexchange.com/questions/645384/most-suitable-statistical-model-in-an-unbalanced-repeated-measures-paired-obse</link>
      <description><![CDATA[我有以下数据集。对于每个受试者，记录了不同日期以及每天不同状态（睡眠/清醒）的大脑数据。数据同时记录在多个通道中，这些通道跨越不同的大脑区域。经络属于不同的区域，可以根据是否为病理经络来区分。然后我计算了不同频段的特征(y)。
这些是我的专栏：
患者 ID |日 |状态 |陈 |乐队 |地区 |是生理性的| y
我想了解是否有某个大脑区域（比如额叶）出现了生理和病理的差异；而在另一个大脑区域（例如中央区域）则不然。
样本是相关的（同一受试者在生理学和病理学类别中有观察结果），因此方差分析不适用。此外，数据集不平衡，重复测量方差分析不适用。
我可能会使用线性混合模型，使用区域和 phys/path 之间的相互作用作为我的主要效果；和（至少）主题作为我的随机效应（因为我预计 sbj 之间存在固有的变异性）。
就像是
$$
y \sim Region*isPhysiological + (1|PatientID)
$$
我不关心区域内的差异（即，额叶当然与中央不同）。我关心的是同一区域内的生理/病理差异。
问题是，我有太多协变量，由于样本大小限制，我无法将所有内容集中到同一个模型中。所以：

我应该如何处理所有潜在的其他随机效应？州、日期、频道、地区......因为这些也对先前的变化负责。
我不确定如何处理频段：我应该为每个频段创建不同的模型吗？或者我应该将频带视为交互项中固定效应的一部分？明确导致某一区域的生理/病理差异，可能只出现在一个波段。
]]></description>
      <guid>https://stats.stackexchange.com/questions/645384/most-suitable-statistical-model-in-an-unbalanced-repeated-measures-paired-obse</guid>
      <pubDate>Fri, 19 Apr 2024 13:08:52 GMT</pubDate>
    </item>
    <item>
      <title>在PCA中，某些变量可以加权吗？</title>
      <link>https://stats.stackexchange.com/questions/645380/in-pca-can-certain-variables-be-weighted</link>
      <description><![CDATA[我正在处理大量疾病症状数据集，这些数据集均按数字等级进行评分（每个症状 0-4 分）。有些症状涉及左半身，有些涉及右半身，有些是轴向的（涉及身体的中心或轴线）。每行表示一个研究参与者。这是在 R 中创建的示例，但实际上还有更多症状：
&lt;前&gt;&lt;代码&gt;&gt;样本数据
 言语刚度颈部刚度ArmL 刚度ArmR SpeedArmL SpeedArmR
 1 0 0 1 2 0
 0 0 0 0 0 0
 2 1 2 1 1 2
 1 1 1 1 2 0
 4 1 1 0 1 0

现在，我想执行 PCA 和聚类分析（我对这两种方法都很陌生），目的是确定患者亚组。在感兴趣的疾病中，有些人会在右侧出现更多症状（例如，RigidityArmR 中出现更多点），而另一些人则会在左侧出现更多症状（RigidityArmL）。 RigidityNeck 是轴向症状的一个示例。这些数字可以相加得到总分（即所有列的总和），用于对疾病的严重程度进行分级。然而，我不希望左与右成为个人电脑的一个特征，并且我不想识别“右为主”的子组。和“左翼主导”。相反，我希望识别诸如“刚性主导”之类的集群。
我的问题：有没有办法消除此示例中左右之间的差异？我认为将变量添加到一个变量中是没有意义的（例如 RigidityArmR + RigidityArmL = RigidityArmTotal），因为这会相对于中轴症状，减少侧部特异性症状的严重程度；换句话说，在这种情况下，RigidityArmTotal 对分析的影响与 RigidityNeck 相同，尽管参数 RigidityArm 的影响应是其两倍（考虑到它是针对左侧和右侧进行评分的）。或者我可以以某种方式分配两倍的“权重”吗？到RigidityArmTotal？
如果您需要进一步说明，请告诉我！]]></description>
      <guid>https://stats.stackexchange.com/questions/645380/in-pca-can-certain-variables-be-weighted</guid>
      <pubDate>Fri, 19 Apr 2024 12:30:14 GMT</pubDate>
    </item>
    <item>
      <title>破解 R 包。是否可以估计过度训练的程度？</title>
      <link>https://stats.stackexchange.com/questions/645379/phacking-r-package-is-it-possible-to-estimate-the-degree-of-overtraining</link>
      <description><![CDATA[有一个名为 phacking 的软件包，它提供了演示 p-hacking 的工具，研究人员滥用数据分析来产生有意义的 p 值的做法。它包含执行 p-hacking 并演示其效果的功能，使其成为有用的学习工具。
这是一些初始使用示例
库（破解）
?money_priming_meta
？ phacking_meta
df &lt;- Money_priming_meta
ph &lt;- phacking_meta(yi = df$yi, vi = df$vi,parallelize = F)

我离这个话题很远，所以我的问题可能很愚蠢，我提前道歉。
我使用优化算法来运行许多模型并尝试选择最好的模型。也就是说，本质上，我面临着多重测试或 p-hacking 的问题。
我的问题是：我可以使用这个 phacking_meta 算法作为选择模型的标准来避免 p-hacking。如果可以的话，那么我应该向 phacking_meta 输入提交什么内容，以了解我的模型是经过重新训练的还是 p-hacking 的结果。
例如
X &lt;- iris[样本(150),-5]
时间 &lt;- 1:100
时间 &lt;- 101:150
mod &lt;- lm(Sepal.Length~., X[tr,])

ph &lt;- phacking_meta(yi = ???, vi = ???, 并行化 = F)
]]></description>
      <guid>https://stats.stackexchange.com/questions/645379/phacking-r-package-is-it-possible-to-estimate-the-degree-of-overtraining</guid>
      <pubDate>Fri, 19 Apr 2024 12:11:42 GMT</pubDate>
    </item>
    <item>
      <title>具有不同因变量的多个回归</title>
      <link>https://stats.stackexchange.com/questions/645369/several-regressions-with-different-dependent-variables</link>
      <description><![CDATA[我正在开发一个项目，我想根据几个不同的指标来比较两组参与者。现在，我正在估计具有不同因变量（即我考虑的指标）和类似解释变量的单独回归，以了解它们有何不同（例如，第 1 组的虚拟变量和一些控制变量）。这种方法可以吗，还是我应该考虑其他方法？在所有回归中，我都使用固定效应和聚类标准误差。所有回归的样本大小并不相同，因为对于特定事件需要实现的某些指标。
我正在寻求有关此方法是否合适或者是否有我应该考虑的更合适方法的指导。具体来说，我很好奇如何解决两组因变量之间的潜在相关性。任何有关处理此问题的见解或建议将不胜感激。此外，研究人员根据不同指标（因变量）对群体之间的差异进行建模的任何论文都将受到高度赞赏！]]></description>
      <guid>https://stats.stackexchange.com/questions/645369/several-regressions-with-different-dependent-variables</guid>
      <pubDate>Fri, 19 Apr 2024 09:51:55 GMT</pubDate>
    </item>
    <item>
      <title>确定嘈杂的医疗保险索赔数据中的政策变化</title>
      <link>https://stats.stackexchange.com/questions/645345/determine-policy-changes-in-noisy-medicare-insurance-claim-data</link>
      <description><![CDATA[我希望这是问这个问题的正确地方，我不会因为一个糟糕的问题而受到批评，但这里是。
背景：
我有一个包含五年内提交的所有医疗保险索赔（非个性化）的数据库。
该数据库由 8.68B 条记录组成。每条记录都包括提交索赔的日期、寻求的金额、索赔是否获得批准、金额以及时间。每条记录还通过适用于程序/设备的约 22,000 个代码（CPT 代码）之一对索赔进行分类。数据按这些 CPT 代码排序，并在 CPT 代码内按日期排序。
是否涵盖索赔取决于影响我无权访问的相关 CPT 代码的各种政策。此外，对于我的问题来说，重要的是，适用于 CPT 代码的政策可能会随着时间的推移而发生变化。一些没有被覆盖的东西可能会变得如此。曾经被覆盖的东西可能不再被覆盖。甚至可能会来来回回，例如，覆盖-不覆盖-覆盖。或者，该策略在数据库期间可能永远不会改变。
最后，数据是有噪音的。虽然某些内容通常会被承保，但索赔仍可能因各种原因被拒绝，例如，提交晚、未提供所需文件等。同样，某些内容通常会被拒绝，但它仍然可能被承保，例如，同情心护理。&lt; /p&gt;
问题：
有没有一种方法可以从数据本身检测这些保单变化，这样我就可以说，对于给定的 CPT 代码，一个时期内批准的索赔百分比是 X，而在不同时期是 Y等等？
另一种说法可能是：面对这些嘈杂的数据，我如何找出适当的窗口和截止百分比来计算平均批准率，以便我可以比较两个时间序列？ 
需要明确的是，我有软件背景，并准备好进行暴力破解，即，如有必要，尝试所有可能的窗口。
感谢您的帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/645345/determine-policy-changes-in-noisy-medicare-insurance-claim-data</guid>
      <pubDate>Thu, 18 Apr 2024 22:19:23 GMT</pubDate>
    </item>
    <item>
      <title>如何解决残差与预测分位数偏差（Dharma 图）？</title>
      <link>https://stats.stackexchange.com/questions/645320/how-to-resolve-the-residual-versus-predicted-quantile-devation-dharma-plot</link>
      <description><![CDATA[我一直在尝试执行具有随机效应的 beta 回归模型。我将性别比（0.561、0.765 等）作为
响应变量和气候变量+年份（1970-2021）作为预测变量。不同的农场被用作随机效应。我使用glmmTMB + beta family + logit进行建模。所有解释变量均已标准化。
但残差图显示显着分位数偏差。我可以知道为什么会发生这种情况吗？

预测变量已标准化
不转换响应变量
]]></description>
      <guid>https://stats.stackexchange.com/questions/645320/how-to-resolve-the-residual-versus-predicted-quantile-devation-dharma-plot</guid>
      <pubDate>Thu, 18 Apr 2024 15:40:12 GMT</pubDate>
    </item>
    <item>
      <title>比值比悖论？合并 OR 与子组 OR 不一致</title>
      <link>https://stats.stackexchange.com/questions/645304/odds-ratios-paradox-pooled-or-inconsistent-with-subgroup-ors</link>
      <description><![CDATA[我有两个组（A 和 B），每个组的 OR 分别为 1.44 和 1.50。但是，如果我将两个组的频率组合起来创建一个合并数据集，则 OR 值为 1.40。
我知道这不会是一个很好的简单加权平均值或任何东西，但我预计合并的 OR 至少会落在两个 OR 的范围内。经过几天的苦苦思索试图找出原因后，我放弃了，转而求助于这里的集体智慧。
这是我的数据：
A 组频率

&lt;标题&gt;


控制
测试


&lt;正文&gt;

低
1374
1422


高
4759
7062



产生 1.44 的 OR 
B 组频率

&lt;标题&gt;


控制
测试


&lt;正文&gt;

低
825
534


高
4033
3914



产生 1.50 的 OR
合并频率

&lt;标题&gt;


控制
测试


&lt;正文&gt;

低
2199
1956


高
8792
10976



产生 1.40 的 OR
对这里发生的事情有什么想法吗？我使用这两个组来显示模式，但我的数据实际上分为更多组。
我将其称为悖论，因为如果我包括所有组，尽管事实上几乎所有组都产生 1.2-1.8 范围内的 OR（有几个组产生 0.95-1.0 范围内的 OR） ），池化版本基本上变为空（OR = 1.00）。感觉就像存在某种辛普森悖论。
编辑：根据下面的@COOLSerdash，本文 解释了“不可折叠性” OR 的数量。经过一番尝试后，我们发现，如果较大的组的 RR 与其他组的 RR 非常不同，那么它就会产生强烈的“扭曲”效果。效果。]]></description>
      <guid>https://stats.stackexchange.com/questions/645304/odds-ratios-paradox-pooled-or-inconsistent-with-subgroup-ors</guid>
      <pubDate>Thu, 18 Apr 2024 13:07:54 GMT</pubDate>
    </item>
    </channel>
</rss>