<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sun, 29 Dec 2024 06:22:15 GMT</lastBuildDate>
    <item>
      <title>平方和与叉积 (SSCP) 矩阵的逆</title>
      <link>https://stats.stackexchange.com/questions/659323/inverse-of-sums-of-squares-and-cross-products-sscp-matrix</link>
      <description><![CDATA[在《应用线性回归》一书的第 3 章中，Weisberg 提出，已校正 SSCP 矩阵的逆$(\chi^T\chi)^{-1}$是未校正 SSCP 矩阵的逆$(X^TX)^{-1}$的第一行和第一列以外的所有元素。这是为什么呢？
$\chi$ 就是 $X$，但没有截距，并且从每列中减去了相应的平均值，仅供参考。]]></description>
      <guid>https://stats.stackexchange.com/questions/659323/inverse-of-sums-of-squares-and-cross-products-sscp-matrix</guid>
      <pubDate>Sun, 29 Dec 2024 02:08:49 GMT</pubDate>
    </item>
    <item>
      <title>关于如何从直方图创建 PDF/正态分布的问题</title>
      <link>https://stats.stackexchange.com/questions/659321/question-regarding-how-to-create-a-pdf-normal-distribution-from-histogram</link>
      <description><![CDATA[我有一个关于概率密度函数、直方图和正态分布的理论问题，我只需要有人确认我的描述（尽管很基础）是否正确。
如果我的书面版本看起来令人困惑，我已在此解释下附上我的问题图片。

据我所知，如果我有一个直方图，其中 y 轴上有频率密度，那么为了将这些 y 值转换为概率密度，

我会做频率密度/N（这与写频率/N*类宽度相同），因此，当我们计算条形的面积时，我们将得到一个概率值，因为它将是（频率/N*类宽度）*类宽度，并且类宽度取消以留下频率/N，即概率。
我们对每个条形都这样做，然后如果我们将这些面积相加，我们会得到 1，正态分布本质上是相同的想法，但类宽度接近于 0。显然我们不会总是得到一个正态分布，但我这样说只是为了强调这本质上就是我们得到平滑曲线的方式。

但是当我必须考虑比例常数 k 时，就会产生混淆
因为直方图通常采用条形面积 = K * 频率的形式，顺便说一句，当我在这里提到条形面积时，我指的是 Y 轴显示频率密度而不是 pdf 的情况。
那么我描述的整个过程将如何工作，我们只是在这个过程中取消 K 吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/659321/question-regarding-how-to-create-a-pdf-normal-distribution-from-histogram</guid>
      <pubDate>Sun, 29 Dec 2024 01:14:31 GMT</pubDate>
    </item>
    <item>
      <title>OLS 怎么会有遗漏变量偏差？OLS 不是应该总是消除内生性吗？</title>
      <link>https://stats.stackexchange.com/questions/659320/how-can-there-be-omitted-variable-bias-in-ols-shouldnt-ols-always-eliminate-en</link>
      <description><![CDATA[OLS 中怎么会存在遗漏变量偏差（OMS）？难道所有回归都不能在没有内生性的情况下计算吗？
作为介绍，这是 Greene（计量经济学分析，第 8 版）关于 OMB 的条目：

重新表述问题：为什么 $b$ 不捕获调用 $X$ 对 $y$ 的影响并使误差项不相关？
例如 - 假设正确指定的模型 ($R^2 = 1$) 和 $x_1$:
$ y = \beta.x_1 + \gamma.x_2$
$x_2= \delta.x_1 + w$
因此，我们看到 $x_1$ 和 $x_2$ 是相关的。
现在让我们使用 OLS 对 $y$ 进行回归：
$E[y\,|x_1] = a + b.x_1+ \epsilon$
由于 $x_2$ 应该位于 $\epsilon$ 上，$b$ 应该具有 OMB，对吗？这里有一个反对意见。
让我们重新表述 $y$ 的方程式：
$y = (\beta +\gamma.\delta).x_1 + \delta.w$
因此，回归量可以重新表述为 $x_1$ 和不相关的 $w$。如果是这样，为什么回归中会有 OMB？]]></description>
      <guid>https://stats.stackexchange.com/questions/659320/how-can-there-be-omitted-variable-bias-in-ols-shouldnt-ols-always-eliminate-en</guid>
      <pubDate>Sun, 29 Dec 2024 00:55:52 GMT</pubDate>
    </item>
    <item>
      <title>解决数学问题的一般方法[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659316/general-approach-to-solving-math-problems</link>
      <description><![CDATA[我有一个数学函数 f(x,y) -&gt; z。我想要一个 ml 模型来学习近似任何 x,y 的 f。我尝试了一种转换器方法，其中输入文件只是一堆方程式，但它似乎不太有效/通用。是否有任何适合解决复杂数学方程式的好方法或模型？]]></description>
      <guid>https://stats.stackexchange.com/questions/659316/general-approach-to-solving-math-problems</guid>
      <pubDate>Sun, 29 Dec 2024 00:25:15 GMT</pubDate>
    </item>
    <item>
      <title>SPSS 和 R 中 95% 置信区间的差异</title>
      <link>https://stats.stackexchange.com/questions/659314/difference-between-95-confidence-intervals-in-spss-and-r</link>
      <description><![CDATA[当我在 SPSS 中计算数据集的 95% CI（Kaplan Meier 曲线）时，它会给出一组值，而在 R 中，它会给出另一组值。
我尝试通过添加 conf.type = &quot;log&quot; 或 conf.type = &quot;plain&quot; 等进行补偿...
好吧，我一个接一个地尝试了每一个，但它仍然与 SPSS 中的不匹配。
有人可以指导我吗？我做错了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659314/difference-between-95-confidence-intervals-in-spss-and-r</guid>
      <pubDate>Sat, 28 Dec 2024 23:47:21 GMT</pubDate>
    </item>
    <item>
      <title>是否可以通过观察损失函数的变化来建立特征的正交性？</title>
      <link>https://stats.stackexchange.com/questions/659313/can-orthogonality-of-features-be-established-by-observing-changes-in-the-loss-fu</link>
      <description><![CDATA[假设我有一个回归或分类问题，并收集了大量数据。我的假设是，如果我选择 $2$ 个正交（或者可能是正交标准？）特征，那么每个特征都应将损失减少一定数值，而与引入它们的顺序无关。例如，如果我从空特征集开始，并合并特征 A 可将损失减少 $10$，那么如果我从特征 B 开始合并特征 A 也应将损失减少 $10$，当且仅当特征 A 和特征 B 是正交的。我可能从这样的事实中得到了这种直觉：概率独立性意味着$\Pr(A)\Pr(B) = \Pr(A\ |\ B)\Pr(B)$，而损失函数似乎或多或少是概率的概念概括（如果除以最大可能损失，$0-1$损失就是一个概率）。
或者，我假设我可能需要将均匀随机数据提供给特征，作为其正交性测试的一部分。
至少在某些条件下，这个范围内的任何情况都是正确的吗？或者您是否总是必须使用相关矩阵或类似工具来确定正交性？]]></description>
      <guid>https://stats.stackexchange.com/questions/659313/can-orthogonality-of-features-be-established-by-observing-changes-in-the-loss-fu</guid>
      <pubDate>Sat, 28 Dec 2024 23:45:25 GMT</pubDate>
    </item>
    <item>
      <title>使用高负 NLL 损失来规范流量</title>
      <link>https://stats.stackexchange.com/questions/659312/normalizing-flow-with-highly-negative-nll-loss</link>
      <description><![CDATA[我正在按照 Zuko“从数据训练”教程来训练神经样条流。我的目标是近似函数分布。
因此，我的每个函数样本实际上都是 20 个样条系数。如果我可以学习这些系数的分布，那么我就可以近似函数分布。总的来说，我使用 100K 个样本来拟合流。每个样本有 20 个维度。
它目前不起作用，流样本看起来不像我数据中的函数。此外，我的 NLL 损失为负，通常在 -30 左右。这意味着平均而言，我的样本密度在 exp(30) 的数量级上？！
这似乎对我的数据过度拟合，但我的训练/测试损失几乎相等。但我的采样函数仍然是垃圾。
这是我的训练循环的代码：
def train(samples):

train_ratio = 25.0/26.0
total_samples = samples.size(0)
indices = torch.randperm(total_samples)

train_size = int(train_ratio * total_samples)
train_indices = indices[:train_size]
test_indices = indices[train_size:]

train_samples, test_samples = samples[train_indices], samples[test_indices]

num_features = samples.shape[1]
flow = zuko.flows.NSF(features=num_features, transforms=5, hidden_​​features=(64, 128, 256))
# flow = zuko.flows.MAF(features=num_features, transforms=32)

num_epochs = 10000
optimizer = torch.optim.Adam(flow.parameters(), lr=1e-3)

ms = [int(0.2*num_epochs), int(0.7*num_epochs), int(0.9*num_epochs)]
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, miles=ms, gamma=0.1)

for epoch in range(num_epochs):
train_loss = -flow().log_prob(train_samples).mean()
train_loss.backward()

optimizer.step()
optimizer.zero_grad()
scheduler.step()

with torch.no_grad():
test_loss = -flow().log_prob(test_samples).mean()

lr = optimizer.param_groups[-1][&#39;lr&#39;]
print(f&#39;Epoch [{epoch + 1}/{num_epochs}], LR: {lr:.6f}, Train: {train_loss.item():.8f}, Test: {test_loss.item():.8f}&#39;)

return flow

这导致 400 个 epoch 之后训练和测试损失分别约为 -45 和 -44，这对我来说似乎很荒谬。]]></description>
      <guid>https://stats.stackexchange.com/questions/659312/normalizing-flow-with-highly-negative-nll-loss</guid>
      <pubDate>Sat, 28 Dec 2024 22:42:29 GMT</pubDate>
    </item>
    <item>
      <title>区间变量与名义变量之间的相关性</title>
      <link>https://stats.stackexchange.com/questions/659311/correlation-between-interval-and-nominal-variables</link>
      <description><![CDATA[我确实需要一些帮助来分析我的数据。我有一个带有村庄大小（以公顷为单位）的数字变量和一个带有四种土壤类型的分类变量。我想使用 R 研究土壤类型是否与村庄大小有关。我已经绘制了箱线图，但现在我想得到一些关联度量。我读过这里的一些答案，建议使用 ANOVA 的 Eta（相关比）。我的尺寸数据不是正态分布的，所以我怀疑我是否可以使用它。在包 rstatix 中，函数 kruskal_effsize 也给出了 eta-squared 的值，但基于 H。根据 rstatix 文档“...eta-squared...表示独立变量解释的因变量方差百分比。”使用这个度量来得出关于两个变量之间关系的一些结论是否正确？
如果有人可以对此进行解释，我将不胜感激。谢谢 Kelly Lee
我已获得所有 Eta 值，但我需要知道是否可以使用它们。]]></description>
      <guid>https://stats.stackexchange.com/questions/659311/correlation-between-interval-and-nominal-variables</guid>
      <pubDate>Sat, 28 Dec 2024 21:32:21 GMT</pubDate>
    </item>
    <item>
      <title>ARIMA(12,0,0) - 缺少第 14 个参数？拟合值 <> 已计算</title>
      <link>https://stats.stackexchange.com/questions/659310/arima12-0-0-missing-14th-param-fitted-values-calculated</link>
      <description><![CDATA[我在 R 中创建了一个 ARIMA(12, 0, 0) 模型。我计算出的拟合值为
$$\hat{y}(t) = \phi_1 y(t-1) + \phi_2 y(t-2) + \ldots + \phi_{12} y(t-12) + \text{intercept}$$
将我的计算结果与模型的最后 10 个拟合值进行比较，结果存在恒定的差异（这 10 个差异是相同的）。为什么会有差异？为了弄清楚这一点，我遇到了这个等式
$$ \text{AIC} = 2k - 2\ln(L) $$
其中 k 是参数的数量，L 是最大似然。当我求解 k 时，我得到的结果是 14，而预期结果是 13（12 个滞后和一个截距）。第 14 个参数是什么？我在 R 中哪里可以找到它？它能解释错误吗？重现该问题的代码如下：
library(readxl)
library(curl)
library(dplyr)
url &lt;- &quot;https://img1.wsimg.com/blobby/go/e5e77e0b-59d1-44d9-ab25-4763ac982e53/downloads/e1fcc664-eaa1-48ed-b682-88e0c33db496/ie_data.xls?ver=1733242673788&quot;
temp_file &lt;- tempfile(fileext = &quot;.zip&quot;)
curl_download(url, temp_file)
shiller_data &lt;- suppressWarnings(suppressMessages({
read_excel(
temp_file,
sheet = &quot;Data&quot;,
range = &quot;A8:V1856&quot;,
col_types = c(
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;skip&quot;,
&quot;numeric&quot;,
&quot;skip&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;,
&quot;numeric&quot;
)
)
}))
colnames(shiller_data) &lt;- c(
&quot;Date&quot;,
&quot;SPCompP&quot;,
&quot;D&quot;,
&quot;E&quot;,
&quot;CPI&quot;,
&quot;DateFraction&quot;,
&quot;LongRateGS10&quot;,
&quot;RealP&quot;,
&quot;RealD&quot;,
&quot;RealTRP&quot;,
&quot;RealE&quot;,
&quot;RealTRScaledE&quot;,
&quot;CAPE&quot;,
&quot;TRCAPE&quot;,
&quot;ExcessCAPEYld&quot;,
&quot;BondReturns&quot;,
&quot;RealBondReturns&quot;,
&quot;StockRealRet10Y&quot;,
&quot;BondRealRet10Y&quot;,
&quot;ExcessRealRet10Y&quot;
)

DateFraction2Date &lt;- function(dateFraction) {
y = floor(dateFraction)
m = ceiling((dateFraction - y)*12) + 1
idx &lt;- m &gt; 12
y[idx] &lt;- y[idx] + 1
m[idx] &lt;- 1
out &lt;- as.Date(paste0(y, &quot;-&quot;,m,&quot;-1&quot;), format = &quot;%Y-%m-%d&quot;) - 1
return(out) 
}

shiller_data &lt;- shiller_data %&gt;% mutate(Date1 = DateFraction2Date(DateFraction))
通货膨胀 &lt;- shiller_data$CPI[2:nrow(shiller_data)]/shiller_data$CPI[1:(nrow(shiller_data) -1)] - 1 
inflation_ts &lt;- ts(inflation, start = c(lubridate::year(shiller_data$Date1[2]), lubridate::month(shiller_data$Date1[2])), 频率 = 12)

window_data &lt;- indication_ts[1:360]
ar_model &lt;- Arima(window_data, order = c(12, 0, 0))
fitted10 &lt;- ar_model$fitted[351:360]
calcFitted10 &lt;- numeric(10)
for (i in 351:360) {
calcFitted10[i-350] &lt;- sum(window_data[(i-12):(i-1)] * ar_model$coef[12:1]) + ar_model$coef[13]
}
fitted10 - calcFitted10
implied_pa​​rams &lt;- (ar_model$aic - ar_model$loglik*-2)/2

本例中的数据来自 Shiller 的 CAPE 数据。
摘要：为什么不是

fitted10 - calcFitted10

全为零？是因为

implied_pa​​rams

等于 14，而我只使用了 13 个系数？
我读过类似问题和这个问题，但还没搞清楚。]]></description>
      <guid>https://stats.stackexchange.com/questions/659310/arima12-0-0-missing-14th-param-fitted-values-calculated</guid>
      <pubDate>Sat, 28 Dec 2024 20:40:40 GMT</pubDate>
    </item>
    <item>
      <title>如果我们知道总体分布不正常，那么创建参考 Z 分布的置信区间是否有意义？</title>
      <link>https://stats.stackexchange.com/questions/659299/does-it-make-sense-to-create-a-confidence-interval-referencing-the-z-distributio</link>
      <description><![CDATA[我正在学习生物学学位的入门统计学课程，在学习如何生成总体均值的置信区间时，我开始怀疑，如果我们知道原始总体不服从正态分布，这样做是否有意义。据我所知，CLT 在这种情况下没有任何用处，因为我们关心的是创建一个置信区间，其假定机会与原始概率分布（即总体的概率分布）一致。
我遗漏了什么吗？如果没有，评估引用此类理论分布来构建置信区间的正确性的程序是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659299/does-it-make-sense-to-create-a-confidence-interval-referencing-the-z-distributio</guid>
      <pubDate>Sat, 28 Dec 2024 15:19:07 GMT</pubDate>
    </item>
    <item>
      <title>$\text{U}(0, \theta)$ 中 ${θ}/{2}$ 的置信区间</title>
      <link>https://stats.stackexchange.com/questions/659276/confidence-interval-for-%ce%b8-2-in-textu0-theta</link>
      <description><![CDATA[我试图找到 ${\theta}/{ 2}$ 的置信区间，置信度为 $1-\alpha$（使用 $p_1 = \alpha$ 和 $p_2 = 1$ 处的分位数）。因此，我采用 ${\hat{\theta}}/{2} = {X_{(n)}}/{2}$ 的无偏估计量。然后从我们在课堂上学到的知识：
$$\mathbb{P} \bigg( \dfrac{cX_{(n)}}{2\theta} \leqslant q_{p_1} \bigg)
= \mathbb{P} \bigg( \dfrac{X_{(n)}}{\theta} \leqslant \dfrac{2q_{p_1}}{c} \bigg)
= \alpha
= \bigg( \dfrac{2q_{p_1}}{c} \bigg)^n,$$
这意味着 $q_{p_1}=\tfrac{c}{2}\alpha^{{1}/{n}}$ 并且对于 $q_{p_2}$ 将给我们 $q_{p_2} = {c}/{2}$。由此得出：
$$\mathbb{P} \bigg( \dfrac{c}{2}\alpha^{{1}/{n}} \leqslant \dfrac{cX_{(n)}}{\theta} \leqslant \dfrac{c}{2} \bigg)
= \mathbb{P} \bigg( X_{(n)} \leqslant \dfrac{\theta}{2} \leqslant \dfrac{X_{(n)}}{\alpha^{{1}/{n}}} \bigg)
= 1-\alpha,$$
因此得出的 CI 为：
$$\dfrac{\theta}{2} \in \bigg[ X_{(n)}, \dfrac{X_{(n)}}{\alpha^{{1}/{n}}} \bigg].$$
这看起来正确吗？还是我遗漏了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659276/confidence-interval-for-%ce%b8-2-in-textu0-theta</guid>
      <pubDate>Fri, 27 Dec 2024 21:45:01 GMT</pubDate>
    </item>
    <item>
      <title>多元方差分析，其中一些因变量是序数，一些是区间数</title>
      <link>https://stats.stackexchange.com/questions/659269/manova-with-some-dependent-variables-being-ordinal-and-some-being-interval</link>
      <description><![CDATA[MANOVA 测试似乎非常适合以下研究，只是因变量不是所需的间隔，我想研究传统和非传统学习者在两种写作类型上的差异，反映在 1) 总体印象、2) 内容、3) 组织、4) 词汇准确性和 5) 语法准确性上。学习者（传统和非传统）和类型（叙述和论证）是两个独立变量。总体而言，内容和组织按 1 到 5 的等级进行评分，因此这三个因变量是序数，而词汇和语法按百分比计算，因此这两个因变量是间隔。
我的问题是 1) 我还可以使用 MANOVA 吗？2) 如果不能，我可以使用什么其他测试？3) 如果可以，我可以使用哪些修改来使 MANOVA 发挥作用？
我对统计学的了解有限，大多数时候使用 SPSS，但如果其他软件可以更有效地处理这个问题，我愿意尝试它们。非常感谢！！

抱歉造成混淆。我刚刚修改了标题。其实我是想问，如果一些因变量是序数的，而一些是区间的，我是否可以使用 MANOVA。研究设计的细节已经解释清楚了。
谢谢你的回复。希望你的笔记本电脑能尽快恢复正常！]]></description>
      <guid>https://stats.stackexchange.com/questions/659269/manova-with-some-dependent-variables-being-ordinal-and-some-being-interval</guid>
      <pubDate>Fri, 27 Dec 2024 13:37:14 GMT</pubDate>
    </item>
    <item>
      <title>韦斯伯格的平方和性质</title>
      <link>https://stats.stackexchange.com/questions/659255/property-of-sums-of-squares-in-weisberg</link>
      <description><![CDATA[在 Weisberg 所著的《应用线性回归》第 23 页中，我看到了 $S_{xx}$ 的这种关系：
$$
\sum (x_i - \bar{x})^2 = \sum(x_i - \bar{x})x_i
$$
有人能解释一下为什么这个等式成立吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/659255/property-of-sums-of-squares-in-weisberg</guid>
      <pubDate>Fri, 27 Dec 2024 05:08:30 GMT</pubDate>
    </item>
    <item>
      <title>从指数函数分布获得幂律的必要充分条件？</title>
      <link>https://stats.stackexchange.com/questions/659182/necessary-and-sufficient-conditions-to-obtain-power-law-from-distribution-over-e</link>
      <description><![CDATA[分布 $p(x)$ 的哪些属性 (1) 是充分的且 (2) 是必要的，以使
$$-\log \Bigg(1 - \int_{x=0}^{x=1} p(x) \, (1-x)^k \, dx \Bigg)$$
导致幂律
$$\propto k^{-b}$$
对于某个常数 $b &gt; 0$？
我有一个隐含的假设，即分布 $p(x)$ &quot; n&quot;在某种意义上，但我不确定这个假设到底是什么。也许是平滑的、连续的，还是类似的东西？
我已经得出 Beta 分布和 Kumaraswamy 分布就足够了。从数值上讲，连续伯努利分布也同样有效。我正在寻找这些分布背后的一般“结构”。]]></description>
      <guid>https://stats.stackexchange.com/questions/659182/necessary-and-sufficient-conditions-to-obtain-power-law-from-distribution-over-e</guid>
      <pubDate>Tue, 24 Dec 2024 23:53:30 GMT</pubDate>
    </item>
    <item>
      <title>VAE 高斯分布的加权融合</title>
      <link>https://stats.stackexchange.com/questions/659124/weighted-fusion-of-vae-gaussian-distributions</link>
      <description><![CDATA[假设您有 4 个不同 VAE 的 4 个堆叠输出向量：$B \times 512 \times 4$
这些 $512$ 个元素对应于四个相互依赖的多元正态分布的 $256 \ \mu$ &amp; $256 \ \log(\sigma^2)$（对数方差）。
我的目标是将这四个组合成一个多元正态分布，定义为 $\mu^*$ &amp; $\log\sigma^{2*}$ 大小均为 $B \times 512$，采用加权融合方法。我正在考虑两个想法...

1.简单求和高斯函数
均值总和：$\mu^*=\sum w_i * \mu_i$
对数变量的对数和指数技巧：$\ln\sigma^{2*}=a + \ln ( \sum w_i * e^{\text{logvar}_i - a})$
其中

$w_i$ 是基于 VAE 输出的学习到的注意力权重，其中 $\sum w_i = 1$
$a = \max (\text{logvars})$

在 PyTroch 中实现：
# 加权均值融合
fused_mean = torch.sum(weights * means, dim=-1)

# 对 logvars 进行加权对数和指数融合 (https://raw.org/math/the-log-sum-exp-trick-in-machine-learning/)
max_logvar = torch.max(logvars, dim=-1, keepdim=True)[0] # 元组输出（丢弃索引）
weighted_var = weights * torch.exp(logvars - max_logvar)
sum_weighted_var = torch.sum(weighted_var, dim=-1，keepdim=True)
fused_logvar = max_logvar + torch.log(sum_weighted_var + eps)

返回 fused_mean，fused_logvar.squeeze(dim=-1)

2.混合模型方法
这个有点类似，但更复杂，我收集了它依赖关系 - 但我不是统计学家！
均值总和（与以前的方法相同）：$\mu^*=\sum w_i * \mu_i$
对数变量：$\ln\sigma^{2*}=a + \ln ([\sum w_i * (e^{\text{logvar}_i - a} + \mu_i^2)] - \mu^{*^{2}})$
在 PyTroch 中实现：
# 加权均值融合
fused_mean = torch.sum(weights * means, dim=-1, keepdim=True) # (batch_size, latent_dim, 1)

# 带交叉项的加权混合方差
max_logvar = torch.max(logvars, dim=-1, keepdim=True)[0] # (batch_size, latent_dim, 1)
_vars = torch.exp(logvars - max_logvar) # (batch_size, latent_dim, n_views)
fused_var = torch.sum(weights * (_vars + means**2), dim=-1, keepdim=True) - fused_mean**2
fused_logvar = max_logvar + torch.log(fused_var + eps)

return fused_mean.squeeze(dim=-1), fused_logvar.squeeze(dim=-1)


我的问题是：

这两种方法的统计意义是什么，什么时候应该优先选择一种方法？
我的公式/操作是否正确实现？

有关更多上下文，您可以访问这个 repo，我将在这里整理这些内容。简而言之，$x$ 是一个分子，它表示为字符串、图像、图形和指纹，每个都使用单独的 VAE 进行编码，并组合成用于采样和计算 KL 散度的单个多变量正态分布。最终目标实际上是通过减少 (KL) 损失项的数量来简化和稳定训练。]]></description>
      <guid>https://stats.stackexchange.com/questions/659124/weighted-fusion-of-vae-gaussian-distributions</guid>
      <pubDate>Mon, 23 Dec 2024 13:14:30 GMT</pubDate>
    </item>
    </channel>
</rss>