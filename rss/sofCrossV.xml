<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 08 Jun 2024 18:18:27 GMT</lastBuildDate>
    <item>
      <title>我的新个体内变异统计数据有意义吗？</title>
      <link>https://stats.stackexchange.com/questions/648877/are-my-new-intraindividual-variability-statistics-sensical</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/648877/are-my-new-intraindividual-variability-statistics-sensical</guid>
      <pubDate>Sat, 08 Jun 2024 18:05:17 GMT</pubDate>
    </item>
    <item>
      <title>两个国家死亡率的最佳测试</title>
      <link>https://stats.stackexchange.com/questions/648876/best-test-for-mortality-rate-in-two-countries</link>
      <description><![CDATA[我们试图比较两个不同国家 20 年内的死亡率。Mann Whitney u 检验法是否最适合使用？我们不清楚数据是否被假定为正态分布，因为这些数字只是每年的死亡人数。]]></description>
      <guid>https://stats.stackexchange.com/questions/648876/best-test-for-mortality-rate-in-two-countries</guid>
      <pubDate>Sat, 08 Jun 2024 17:21:58 GMT</pubDate>
    </item>
    <item>
      <title>构建变量选择的子组比较</title>
      <link>https://stats.stackexchange.com/questions/648874/constructing-subgroup-comparisons-for-variable-selection</link>
      <description><![CDATA[我正尝试根据以下描述构建一个协变量：

在构建协变量以测量子组效应时，我们生成协变量来捕获给定子组中给定变量的因果效应。协变量的构建方式是，在每个子组中，治疗组和对照组形成对比；在子组之外，协变量设置为零。
此协变量的构建方式是，回归此协变量的结果的系数给出该子组中均值的差异。它通过将子组的观察值归零来实现这一点，但在该子组中创建治疗组和对照组之间的对比。请注意，可以构建此协变量来将治疗条件 k 中的那些与其他条件进行对比。该协变量将除 k 之外的所有治疗水平视为

基线，因此该协变量的系数应解释为

给定治疗与该变量的所有其他治疗水平的平均值之间的平均差异。

Ratkovic, M. (2021)。子组分析：陷阱、承诺和诚实。在 J. N. Druckman &amp; D. P. Green (Eds.) 中，实验政治科学进展 (第 271-288 页)。剑桥大学出版社。
我不确定如何构建这些子组协变量。这些协变量的目的是将它们中的许多变量包含在套索模型中以选择最重要的变量。由于我们有兴趣发现治疗效果的异质性，因此协变量应该代表子组内治疗和对照之间的均值差异（或如最后一句所述，总和对比差异）。如果协变量或相互作用意味着其他东西，那么您将调整错误的效应。
这些协变量应该如何编码？我在想也许可以将子组内控制编码为 -0.5，将子组内治疗编码为 0.5，将子组外的所有其他内容编码为 0。但您仍然必须在套索模型中排除一个类别，对吗？
我说得对，简单的治疗 x 子组方法不行吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648874/constructing-subgroup-comparisons-for-variable-selection</guid>
      <pubDate>Sat, 08 Jun 2024 16:43:55 GMT</pubDate>
    </item>
    <item>
      <title>假设残差在模型构建时与预测变量不相关，何时违反 E[Xi*ui]=0？</title>
      <link>https://stats.stackexchange.com/questions/648873/when-is-exiui-0-violated-given-that-the-residuals-are-by-construction-of-the</link>
      <description><![CDATA[首先：我知道“误差项”和“残差”是两个不同的概念。但是，我很难理解它们对（多元）线性回归和线性投影模型的影响。
在我的课堂上，我们为任何 PRF（不一定是线性）引入了一个线性投影模型，其中一个关键概念是正交性：
E[Xi*ui)=0（其中 0 是 (k+1)*1 向量）。
如果模型中有一个常数，则 Xi 和 ui 不相关。假设这里的 ui 是误差项，我不明白如果误差项和预测因子有任何关联，那么该属性（无关联）如何成立。

我不明白如何识别 E[Xi * ui]。如果 Xi 和 ui 相关，那么 Xi 和残差在构造上仍然不相关，我假设我们将使用残差来识别 E[Xi * ui]，因为我们无法观察到 ui。

如果 Xi 和 ui 相关，这是否意味着 E[Xi *ui) 不等于零？

何时违反 E[Xi * ui]= 0？我们如何识别任何违规行为？


我希望我的问题有意义，提前谢谢您 :)]]></description>
      <guid>https://stats.stackexchange.com/questions/648873/when-is-exiui-0-violated-given-that-the-residuals-are-by-construction-of-the</guid>
      <pubDate>Sat, 08 Jun 2024 16:34:09 GMT</pubDate>
    </item>
    <item>
      <title>Cholesky 分解和 OLS 比较方法</title>
      <link>https://stats.stackexchange.com/questions/648872/cholesky-decomposition-and-comparing-methods-for-ols</link>
      <description><![CDATA[我一直在比较各种方法的速度和准确性，以找到最小二乘解。

$\beta = (X^TX)^{-1}X^Ty$
$R\beta = Q^Ty$ 和使用正向/反向替换进行 QR 因式分解求解
$\beta = V\Sigma^{-1}U^Ty$ 使用 SVD
$\beta = R^{-1}Q^Ty$
$L = Chol(X^TX)$，正向求解 $Z$：$LZ = X^Ty$，反向求解 $\beta$：$L^T\beta = Z$
$LU = X^TX$，正向求解 $Z$：$LZ = X^Ty$，反向求解 $\beta$：$U\beta = Z$

但是，$LU$ 和 Cholesky 得出的 MSE 比其他方法高得多。我的数据的条件数并不高，为 1.14，并且分解没有明显的精度损失。
L = np.linalg.cholesky(df.T.dot(df))
np.power(df.T.dot(df) - pd.DataFrame(L@L.T), 2).sum().sum() 
7.722374477305186e-22

结果：
QR w Triang Solve；平均时间=0.505；标准时间=0.307；MSE = 25.075
Numpy Inverse；平均时间=0.108；标准时间=0.035；MSE = 25.075
SVD Inverse；平均时间=0.353；标准时间=0.128；MSE = 25.075
R 的 QR 逆变换；平均时间=0.32；标准时间=0.115；MSE = 25.075
Cholesky；平均时间=0.064；标准时间=0.007；MSE = 46.143
LU；平均时间=0.109；标准时间=0.018； MSE = 46.143
问题：

我是否正确地执行了 Cholesky 和 ​​LU？
如果我做得正确，为什么 MSE 相对于其他方法如此之高？
为什么 numpy inverse 比其他方法（如 QR）快得多？我认为逆运算应该是昂贵的。

import time
import scipy
import numpy as np
import pandas as pd
from numba import jit

def print_info(method, times, mse):
print(f&quot;{method}; mean time={round(np.mean(times), 3)}; std time = {round(np.std(times), 3)}; MSE = {round(mse, 3)}&quot;)

np.random.seed(17)
ITERS = 10
SAMPLES = 20000
FEATURES = 100
df = pd.DataFrame(np.random.normal(size=(SAMPLES, FEATURES)))
df[&quot;intercept&quot;] = 1
BETA = np.random.normal(3, 10, size=FEATURES+1)
y = df.dot(BETA) + np.random.normal(0, 5, size=SAMPLES)

times = []
for i in range(ITERS):
start = time.time()
Q, R = np.linalg.qr(df)
beta = scipy.linalg.solve_triangular(R, Q.T.dot(y), check_finite=False)
times.append(time.time()-start)
print_info(&quot;QR w Triang Solve&quot;, times, np.mean((y-df.dot(beta))**2))

times = []
for i in range(ITERS):
start = time.time()
beta = np.linalg.inv(df.T.dot(df)).dot(df.T).dot(y)
times.append(time.time()-start)
print_info(&quot;Numpy Inverse&quot;, times, np.mean((y-df.dot(beta))**2))

times = []
for i in range(ITERS):
start = time.time()
U, s, Vh = np.linalg.svd(df, full_matrices=False)
beta = ((y.T@U)@np.diag(1/s))@Vh
times.append(time.time()-start)
print_info(&quot;SVD Inverse&quot;, times, np.mean((y-df.dot(beta))**2))

times = []
for i in range(ITERS):
start = time.time()
Q, R = np.linalg.qr(df)
beta = np.linalg.inv(R).dot(Q.T).dot(y)
times.append(time.time()-start)
print_info(&quot;QR Inv of R&quot;, times, np.mean((y-df.dot(beta))**2))

times = []
for i in range(ITERS):
start = time.time()
L = np.linalg.cholesky(df.T.dot(df))
Z = scipy.linalg.solve_triangular(L, df.T.dot(y), check_finite=False)
beta = scipy.linalg.solve_triangular(L.T, Z, check_finite=False)
times.append(time.time()-start)
print_info(&quot;Cholesky&quot;, times, np.mean((y-df.dot(beta))**2))

times = []
for i in range(ITERS):
start = time.time()
L, P, U = scipy.linalg.lu(df.T.dot(df), permute_l=False)
Z = scipy.linalg.solve_triangular(L, df.T.dot(y), check_finite=False)
beta = scipy.linalg.solve_triangular(U, Z, check_finite=False)
times.append(time.time()-start)
print_info(&quot;LU&quot;, times, np.mean((y-df.dot(beta))**2))

]]></description>
      <guid>https://stats.stackexchange.com/questions/648872/cholesky-decomposition-and-comparing-methods-for-ols</guid>
      <pubDate>Sat, 08 Jun 2024 16:19:45 GMT</pubDate>
    </item>
    <item>
      <title>在什么条件下，省略变量的回归的其余系数会相对于完整回归中的系数进行“缩放”？</title>
      <link>https://stats.stackexchange.com/questions/648871/under-what-conditions-are-the-remaining-coefficients-of-a-regression-with-a-vari</link>
      <description><![CDATA[我正在阅读这篇文章讨论了从回归方程中省略变量的影响，并分析了它将产生的影响。
这是他们使用的一些符号：给定一个数据矩阵$X$，响应变量$y$，我们有回归系数的正态方程：
$$\widehat{\beta} = (X&#39;X)^{-1}X&#39;y$$
现在假设我们省略了一些变量 z。如果我们重新运行包含该变量的回归，我们将得到新的回归系数$\beta, \gamma$，这样
$$y \sim X\beta + z\gamma$$
然后，他们给出了带有省略变量的回归系数的方程，该方程是完整回归系数的方程：
$$\widehat{\beta} = (X&#39;X)^{-1}X&#39;y=(X&#39;X)^{-1}X&#39;(X\beta + z\gamma + \epsilon)$$
$$=(X&#39;X)^{-1}X&#39;X\beta +(X&#39;X)^{-1}X&#39;z\gamma + (X&#39;X)^{-1}X&#39;\epsilon $$
在我的特定情况下，X 中的每个回归变量都是随机（不相关）伯努利变量，取值范围为 $\{0,1\}$，因此它们都具有相同的均值和方差，没有协方差。
我试图使用上述分析来理解以下条件成立的条件：当我们省略一个变量时，其余系数都会按比例缩放，因此当所有系数都设置为 1 时，预测值与之前保持不变。
换句话说，假设当完整回归中的所有回归变量都设置为 1（即所有回归变量的总和）：
$$C=\widehat{\beta}\mathbf{1}+\gamma$$
在什么条件下$$\widehat{\beta} = \frac{\beta}{C-\gamma}成立？$$
我的直觉是，在我的特殊情况下应该是这种情况，我正在尝试证明它是否正确。我认为它应该是真的的原因是，当你省略变量时，所有回归变量设置为 1 时的预测会太低，需要扩大。由于所有变量都具有相同的方差且没有相关性，我认为它们都应该按相同的比例增加。
有人能帮我证明这个说法吗？或者根据我的特殊情况的限制，证明另一个同样简单但实际上正确的说法？
不幸的是，从上面的数学运算来看，由于$X$和$z$中的每个回归量都是不相关的，$X&#39;z=0$，因此不完全回归中的回归系数不应出现预期的“膨胀”。但这似乎不正确……
谢谢，
保罗]]></description>
      <guid>https://stats.stackexchange.com/questions/648871/under-what-conditions-are-the-remaining-coefficients-of-a-regression-with-a-vari</guid>
      <pubDate>Sat, 08 Jun 2024 16:05:55 GMT</pubDate>
    </item>
    <item>
      <title>Kendall 的 tau 和 Spearman 函数用于混合 copula</title>
      <link>https://stats.stackexchange.com/questions/648870/kendalls-tau-and-spearman-for-mixture-copulas</link>
      <description><![CDATA[我对混合 copula 模型很感兴趣。我读过（本文第 1694 页）
&quot;Spearman 凸组合 copula 的 $\rho$ 等于单个 Spearman 凸组合的 $\rho$。&quot; 然而，Kendall 的 tau 并非如此。
我的问题是：
假设我对计算混合 copula 凸组合的相关性感兴趣。根据上述论文的研究结果，使用 Spearman 的 rho 比使用 Kendall 的 tau 更好吗？还是我需要在这里理解一些东西？]]></description>
      <guid>https://stats.stackexchange.com/questions/648870/kendalls-tau-and-spearman-for-mixture-copulas</guid>
      <pubDate>Sat, 08 Jun 2024 14:39:31 GMT</pubDate>
    </item>
    <item>
      <title>使用机器学习必须解决哪些方面的问题？</title>
      <link>https://stats.stackexchange.com/questions/648867/which-aspects-of-problem-are-must-for-using-machine-learning</link>
      <description><![CDATA[除了“数据”的存在，对于任何优化问题，还有哪些方面是必须使用机器学习方法的？
有许多优化问题解决技术，但并非都基于机器学习。
TSP 是一个 NP 难题，可以通过进化算法以更好的方式（即不需要指数级的步骤）解决，如此处所示，使用距离信息。
但是，还有其他优化问题不需要使用机器学习方法。
问题结构有什么不同，只是问题（如 TSP）是 NP-Hard 吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648867/which-aspects-of-problem-are-must-for-using-machine-learning</guid>
      <pubDate>Sat, 08 Jun 2024 11:55:50 GMT</pubDate>
    </item>
    <item>
      <title>Mplus 中的两级多组模型</title>
      <link>https://stats.stackexchange.com/questions/648864/twolevel-multgroup-model-in-mplus</link>
      <description><![CDATA[我想使用 Mplus 运行具有两个组的两级多组模型（例如用户指南中的示例 9.11）。
我的问题：我有一个独立变量 X，该变量在 A 组中仅在 2 级单位之间变化，即 X 是 A 组中的仅间变量。
因此，我将 X 指定为“between=”变量。然而，在 B 组中，相同的独立变量 X 在组间和组内都变化。是否有可能估计这样的模型 - 如果可以，如何估计？]]></description>
      <guid>https://stats.stackexchange.com/questions/648864/twolevel-multgroup-model-in-mplus</guid>
      <pubDate>Sat, 08 Jun 2024 10:22:41 GMT</pubDate>
    </item>
    <item>
      <title>线性回归：当结果变量是计数数字时，用百分比变化来解释系数</title>
      <link>https://stats.stackexchange.com/questions/648857/linear-regression-interpret-coefficient-in-terms-of-percentage-change-when-the</link>
      <description><![CDATA[我有一个线性回归，其中 Y 是一个计数数字。
Y = a + b1X1 + b2X2

我知道我们可以将 b1 解释为：X1 增加一个单位将使 Y 改变 b1。但我如何以百分比的形式解释它？我知道当 Y 是计数数字时我可以使用泊松。但仅给出线性回归，有没有办法将 b1 解释为百分比值？]]></description>
      <guid>https://stats.stackexchange.com/questions/648857/linear-regression-interpret-coefficient-in-terms-of-percentage-change-when-the</guid>
      <pubDate>Sat, 08 Jun 2024 04:31:37 GMT</pubDate>
    </item>
    <item>
      <title>回归方法之间的差异</title>
      <link>https://stats.stackexchange.com/questions/648856/difference-between-regression-methods</link>
      <description><![CDATA[在给定数据的统计建模中，何时使用逻辑回归，何时使用贝塔回归？如何知道它们之间的区别？什么时候我可以只拟合线性回归，而不用担心数据？]]></description>
      <guid>https://stats.stackexchange.com/questions/648856/difference-between-regression-methods</guid>
      <pubDate>Sat, 08 Jun 2024 03:50:36 GMT</pubDate>
    </item>
    <item>
      <title>使用二元结果的功率计算（使用 R 包和手动模拟）</title>
      <link>https://stats.stackexchange.com/questions/648852/power-calculation-with-a-binary-outcome-using-a-r-package-manual-simulation</link>
      <description><![CDATA[我试图在相同的设置下比较三种不同的功效计算方法。虽然我理解由于随机性，每种方法的功效估计不可能完全相同，但我预计它们会非常相似。然而，我观察到功效估计略有不同。
基本设置如下：

结果：二进制（成功 1；失败 0）
要比较的两组：G1 vs G2
G1：样本数 - 150；成功率为 0.2
G2：样本数 - 30；成功率为 0.4
显著性水平为 0.2（不是通常的 0.05）

也就是说，
p1 &lt;- 0.2
p2 &lt;- 0.4

n1 &lt;- 150
n2 &lt;- 30

我使用的三种方法是：

使用 pwr 包中的 pwr.2p2n.test 函数。
使用 prop.test 函数进行模拟。
使用 fisher.test 函数进行模拟

### -------------------------------- ###
### --- 版本 1：pwr.2p2n.test --- ###
### -------------------------------- ###
library(pwr)
pwr.2p2n.test(h = ES.h(p1 = p1, p2 = p2), 
n1 = n1, n2 = n2,
sig.level = 0.20,
alternative = &quot;less&quot;)

# power = 0.9145152

### ---------------------------- ###
### --- 版本 2：Prop 测试 --- ###
### ---------------------------- ###
nreps &lt;- 10000
y1 &lt;- rbinom(n = nreps, size = n1, p = p1)
y2 &lt;- rbinom(n = nreps, size = n2, p = p2)

pval &lt;- rep(NA, nreps)
for(i in 1:nreps) {
pval[i] &lt;- prop.test(c(y1[i], y2[i]), 
n= c(n1, n2), 
alternative = &quot;less&quot;,
p = NULL, correct = TRUE)$p.value
}

power &lt;- sum(pval &lt; 0.20) / nreps 
power # [1] 0.8756

### -------------------------------------- ###
### --- 版本 3：Fisher 精确检验 --- ###
### -------------------------------------- ###
pval.fin &lt;- c()
for(i in 1:nreps){
y1 &lt;- rbinom(n1, size = 1, p = p1)
y2 &lt;- rbinom(n2, size = 1, p = p2)
dat.comb &lt;- rbind(data.frame(res = y1, group = &quot;G1&quot;),
data.frame(res = y2, group = &quot;G2&quot;))
tab &lt;- table(dat.comb$group, dat.comb$res)
test.res &lt;- fisher.test(tab, alternative = &#39;less&#39;)
pval.fin[i] &lt;- test.res$p.value
}
mean(pval.fin&lt;0.2) # [1] 8e-04

我注意到了以下两点：

第三种方法使用 Fisher 精确检验，其幂为 8e-0.4。但是，如果我将“替代”选项从“less”更改为“greater”，幂将变为 0.8811，这与我们从第一种方法和第二种方法中获得的结果相似。但是第一种方法和第二种方法使用了“less”的“替代”选项……我不明白是什么导致了如此巨大的差异。

版本 1 和版本 2 的幂估计也有点偏差。我不太清楚是什么导致了这种差异。


有人能帮我理解为什么即使设置相同，我也没有获得类似的功率吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648852/power-calculation-with-a-binary-outcome-using-a-r-package-manual-simulation</guid>
      <pubDate>Fri, 07 Jun 2024 22:15:04 GMT</pubDate>
    </item>
    <item>
      <title>在 Wedderburn 1974 中的矩量法中使用 $n-m$ 的动机是什么？</title>
      <link>https://stats.stackexchange.com/questions/648811/what-is-the-motivation-for-the-use-of-n-m-in-the-method-of-moments-in-wedderbu</link>
      <description><![CDATA[在回答这个问题时方差和卡方之间有关系吗？
我写道

因此，要找到分散参数，必须使用不同的技巧。在 Wedderburn 1974 中，这是通过使用 矩法 来实现的。
方差的期望大约为：
$$E\left[\sum_{i=1}^n \frac{(x_i-\mu_i)^2}{V(\mu_i)} \right] \approx \phi({n-m})$$
（该近似值基于假设 $V(\mu_i)$ 近似为线性，且分布为 $\chi^2$ 分布）

在上面的引文中，表达式计算残差平方和 $(x_i-\mu_i)^2$，并由估计方差加权（对于 GLM，可以用均值函数表示），$V(\mu_i)$。 $\phi$ 是离散系数，$n$ 是观测值的数量，$m$ 是估计参数的数量。
括号内的部分不正确（我将其删除）。分布不是 $\chi^2$ 分布。这是针对正态分布族而言的，但一般情况下并非如此。
但它也不需要如此。对于矩量法来说，重要的是分布具有期望值。
然而，我想知道的是：
缩放残差平方和的期望值是否等于样本大小$n$减去参数$m$？
我们如何证明这一点？如果它只是近似正确，我们如何证明近似是合理的？
我们确实知道残差位于较小的子空间中（如此处所述：为什么残差在$\mathbb{R}^{n-p}$中？），但误差的分布不是球对称的，方差可能不需要均等分割。]]></description>
      <guid>https://stats.stackexchange.com/questions/648811/what-is-the-motivation-for-the-use-of-n-m-in-the-method-of-moments-in-wedderbu</guid>
      <pubDate>Fri, 07 Jun 2024 11:14:28 GMT</pubDate>
    </item>
    <item>
      <title>负二项模型的离散度</title>
      <link>https://stats.stackexchange.com/questions/648654/dispersion-of-a-negative-binomial-model</link>
      <description><![CDATA[在 R 的 glm.nb 摘要中，它表示色散参数 $\phi$ 设置为 1。当模型为
$Y \sim \text{Negbin}(\mu,\theta)$
其中 $E(Y)=\mu$ 和 $V(Y)=\mu+\mu^2/\theta$。这里的色散 $\phi$ 是否意味着指数族公式中的 $V(Y)=\phi(\mu+\mu^2/\theta)$？鉴于$\theta$可以自由调整，测试$H_0: \phi=1$是否有意义？我知道负二项式模型可能由于各种原因（例如，许多零和其他值之间的关系）不能很好地拟合数据，但如果我们说它是过度分散，我们还需要考虑正态分布的过度分散，但人们经常说高斯模型中没有过度分散。
set.seed(1)
x &lt;- round(rep(seq(3,30,by=3),each=10))
y &lt;- rnbinom(length(x),mu=exp(1+0.1*x),size=5)

plot(x,y)
library(MASS)
model &lt;- glm.nb(y~x)
r &lt;- resid(model,type=&quot;pearson&quot;)
(phi &lt;- sum(r^2)/(length(x)-3)) #分散

类似帖子：链接]]></description>
      <guid>https://stats.stackexchange.com/questions/648654/dispersion-of-a-negative-binomial-model</guid>
      <pubDate>Wed, 05 Jun 2024 02:46:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么聚合可以减少噪音</title>
      <link>https://stats.stackexchange.com/questions/648622/why-aggregation-reduces-the-noise</link>
      <description><![CDATA[我正在寻找一个证据来证明为什么将数据聚合（汇总）到更高级别（例如，将变量的每日值聚合到每周或每月）可以减少噪音。有人知道如何证明这一点吗？基本上，为什么更细粒度的数据比聚合的、更粗的数据更不稳定和不稳定？]]></description>
      <guid>https://stats.stackexchange.com/questions/648622/why-aggregation-reduces-the-noise</guid>
      <pubDate>Tue, 04 Jun 2024 16:10:47 GMT</pubDate>
    </item>
    </channel>
</rss>