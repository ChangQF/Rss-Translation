<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 13 Jul 2024 01:06:37 GMT</lastBuildDate>
    <item>
      <title>比较正的、略微非正态数据的高斯 GLMM 模型：解释相互冲突的模型选择标准</title>
      <link>https://stats.stackexchange.com/questions/650956/comparing-gaussian-glmm-models-for-positive-slightly-non-normal-data-interpret</link>
      <description><![CDATA[我正在使用 R 中的 glmmTMB 分析数据，其模型结构如下：
model &lt;- glmmTMB(Mean_Intensity ~ group + (1|Intensity_Filename), family=..., data=data, REML=T)。我的因变量为正，略微偏离正态性。
我已拟合三个模型：family=gaussian、family=gaussian(link=&quot;log&quot;)、family=lognormal(link=&quot;log&quot;)。
我已使用以下方法比较了这些模型：AIC、R 平方（通过 performance::compare_performance）和残差图（通过 DHARMa）
结果：
对数正态模型的 AIC 最低，残差最好看。
高斯模型的 R 平方最高。
问题：

我应该如何解释这些相互矛盾的结果？具有最佳 AIC 和残差的模型具有最差的 R 平方，这是否不常见？
在事后分析中选择这些模型时，主要考虑因素是什么？
我是否应该考虑其他诊断或测试来选择最合适的模型？

任何见解或建议都将不胜感激。
]]></description>
      <guid>https://stats.stackexchange.com/questions/650956/comparing-gaussian-glmm-models-for-positive-slightly-non-normal-data-interpret</guid>
      <pubDate>Fri, 12 Jul 2024 23:43:11 GMT</pubDate>
    </item>
    <item>
      <title>物种分布建模中的环境过滤与空间重采样</title>
      <link>https://stats.stackexchange.com/questions/650955/environmental-filtering-versus-spatial-resampling-in-species-distribution-modeli</link>
      <description><![CDATA[我正在使用基于 GBIF 数据（仅存在数据）的机器学习模型构建物种分布模型，并在非常大的空间尺度上工作，涵盖整个北美。在构建 SDM 之前，我应用了环境过滤，该过滤基于通过拆分数据集中的连续环境变量创建的箱。过滤过程旨在减少空间采样偏差的潜在影响并降低数据中的空间自相关性。
在构建 SDM 时，建议使用几种空间重采样方法来划分数据并适应自相关性。鉴于环境过滤已经减轻了自相关性，我想知道使用空间重采样是否仍然相关。因此，我想到几种情况，但我不确定哪种最好：

应用环境过滤并使用简单的随机重采样方法。
跳过环境过滤并使用空间重采样方法。例如，考虑空间阻塞（例如矩形、空间多边形和缓冲区），这可能比环境阻塞更能解释空间自相关（如果我没记错的话）。
应用环境过滤并使用空间重采样方法（可能是环境阻塞）。

我倾向于选择 3，但我非常感谢您的建议。我提出这个问题是因为环境过滤会减小数据集的大小，这可能会给交叉验证带来问题（对于某些模型，我收到有关观察不足的警告）。]]></description>
      <guid>https://stats.stackexchange.com/questions/650955/environmental-filtering-versus-spatial-resampling-in-species-distribution-modeli</guid>
      <pubDate>Fri, 12 Jul 2024 23:41:54 GMT</pubDate>
    </item>
    <item>
      <title>多级模型残差散点图假设</title>
      <link>https://stats.stackexchange.com/questions/650954/multilevel-model-residuals-scatterplot-assumptions</link>
      <description><![CDATA[我正在 SPSS（混合建模）中进行多层次建模 (MLM)，以分析横断面重复测量数据。我的一个因变量是一个按 1 到 10 缩放的调查问题，它产生了附加的残差散点图，该散点图向下倾斜但仍在零线附近保持平衡。这种“倾斜”是否会引起对 MLM 假设得到满足的解释的担忧？
]]></description>
      <guid>https://stats.stackexchange.com/questions/650954/multilevel-model-residuals-scatterplot-assumptions</guid>
      <pubDate>Fri, 12 Jul 2024 23:02:43 GMT</pubDate>
    </item>
    <item>
      <title>变量分解对其他系数的影响</title>
      <link>https://stats.stackexchange.com/questions/650953/effect-of-variable-disaggregation-on-other-coefficients</link>
      <description><![CDATA[假设我对变量$Y$和变量$X_1, X_2, \ldots X_k$进行多元回归，并找到系数$\beta_1, \beta_2, \ldots \beta_k$。
考虑变量$X_n$可以分解为$X_{n, 1}, X_{n, 2} \ldots X_{n, l}$，这样虽然$X_n$和$X_m$相关性较低，$X_{j, n}$ 和 $X_m$ 对于 $j$ 的某些值具有较高的相关性。
那么，如果我对相同变量进行变量 $Y$ 的多元回归，但 $X_n$ 完全分解，这将如何影响 $X_m$ 的系数 $(\beta_m)$？我特别好奇它是否会显示出更高的效果。如果会，那么 (a) 数学方法是什么，(b) 直觉是什么。]]></description>
      <guid>https://stats.stackexchange.com/questions/650953/effect-of-variable-disaggregation-on-other-coefficients</guid>
      <pubDate>Fri, 12 Jul 2024 22:28:47 GMT</pubDate>
    </item>
    <item>
      <title>变量的联合分布</title>
      <link>https://stats.stackexchange.com/questions/650952/joint-distribution-of-the-variables</link>
      <description><![CDATA[我想使用 copula 为多变量干旱分析建立降水和流量数据的联合分布模型。我已经确定了变量的边际分布。

如何使用 R 代码计算变量的联合分布？
]]></description>
      <guid>https://stats.stackexchange.com/questions/650952/joint-distribution-of-the-variables</guid>
      <pubDate>Fri, 12 Jul 2024 20:24:57 GMT</pubDate>
    </item>
    <item>
      <title>计算长期人口增长的死亡率</title>
      <link>https://stats.stackexchange.com/questions/650951/calculating-death-rates-for-an-increasing-population-over-the-long-term</link>
      <description><![CDATA[我参与了一项针对特定 MH 干预的小型长期结果研究。我们对似乎在 20 多岁时就去世的受试者数量感到有些担忧，我们想确保死亡率确实异常高，并开始对潜在问题发出一些声音。

简而言之，我们有大约 600 名受试者，他们在 25 年的时间里被送入 MH 设施。多年来，他们都是十几岁时被送进这个机构的，现在的年龄在 20-50 岁之间。
我们想做的是计算每 5 年（15-19 岁、20-24 岁等）的死亡率，以便将其与一般人口和更具体/相关的人群进行比较。

从表面上看，这似乎是一个基本问题，但我根本不知道如何面对这样一个事实：我们必须从 25 年的角度来看待这一事实，即人口在这段时间内不断增长（现在稳定下来）。如果有人能牵着我的手，给我指明正确的概念方向，我将不胜感激！谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/650951/calculating-death-rates-for-an-increasing-population-over-the-long-term</guid>
      <pubDate>Fri, 12 Jul 2024 20:20:02 GMT</pubDate>
    </item>
    <item>
      <title>随机变量乘积的分布</title>
      <link>https://stats.stackexchange.com/questions/650950/distribution-of-a-product-of-random-variables</link>
      <description><![CDATA[我有两个独立分布 $X$ 和 $Y$。$X$ 由分段 CDF 定义
$$F_X(x) = \begin{cases}
F_X^1(x) &amp; x \in (-\infty, a_1)\\
F_X^2(x) &amp; x \in [a_1, a_2)\\
F_X^3(x) &amp; x \in [a_2, \infty)
\end{cases}$$
和 PDF
$$f_X(x) = \begin{cases}
f_X^1(x) &amp; x \in (-\infty, a_1)\\
f_X^2(x) &amp; x \in [a_1, a_2)\\
f_X^3(x) &amp; x \in [a_2, \infty)
\end{cases}$$
并且 Y 由分段 CDF 定义
$$F_{Y}(y) = \begin{cases}
F_Y^1(y) &amp; y&lt;0\\
F_Y^2(y) &amp; y\geq0
\end{cases}$$
和 PDF
$$f_{Y}(y) = \begin{cases}
f_Y^1(y) &amp; y&lt;0\\
f_Y^2(y) &amp; y\geq0
\end{cases}$$
$Z = Xe^Y$ 的 CDF 和 PDF 是多少？

我尝试使用全概率定律来分解这个问题（我省略了下面的一些术语），但我在最后一步卡住了。我不知道如何将其转换为具有正确界限的积分。
\begin{align*}\mathbb{P}(Z&lt;z) &amp;=\mathbb{P}(Xe^{Y}&lt;z) \\
&amp;=\mathbb{P}(X&lt;ze^{-Y}) \\
&amp;=\mathbb{P}(X&lt;ze^{-Y},ze^{-Y} &lt; a_{1})+\mathbb{P}(X&lt;ze^{-Y},a_{1} \leq ze^{-Y} &lt; a_{2})+\dots\\
&amp;=\mathbb{P}(X&lt;ze^{-Y},\ln(z)-\ln\left(a_{1}\right) \leq Y) \\
&amp;\qquad+\mathbb{P}(X&lt;ze^{-Y},\ln(z)-\ln\left(a_{2}\right) &lt; Y \leq \ln(z)-\ln\left(a_{1}\right))+\dots \\
&amp;=\mathbb{P}(X&lt;ze^{-Y},\ln(z)-\ln\left(a_{1}\right) \leq Y,Y&lt;0) \\
&amp;\qquad+\mathbb{P}(X&lt;ze^{-Y},\ln(z)-\ln\left(a_{1}\right) \leq Y,Y\geq0)+\dots \end{align*&gt;]]></description>
      <guid>https://stats.stackexchange.com/questions/650950/distribution-of-a-product-of-random-variables</guid>
      <pubDate>Fri, 12 Jul 2024 19:44:48 GMT</pubDate>
    </item>
    <item>
      <title>选择正确数量的特征和正确参数的更快方法</title>
      <link>https://stats.stackexchange.com/questions/650949/a-faster-way-to-choose-the-right-number-of-features-and-the-right-parameters</link>
      <description><![CDATA[对于一个非常小的数据集，样本少于一百个，网格搜索无法给出我们想要的结果，并且存在过度拟合。但是当我手动对模型进行超参数调整并随机选择不同的数字时，会获得想要的结果，但这项工作需要花费大量时间。您有解决方案来加快这项工作吗？或者有比手动方法更好的替代方法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/650949/a-faster-way-to-choose-the-right-number-of-features-and-the-right-parameters</guid>
      <pubDate>Fri, 12 Jul 2024 18:32:34 GMT</pubDate>
    </item>
    <item>
      <title>计算逆协方差矩阵的偏差</title>
      <link>https://stats.stackexchange.com/questions/650947/computing-the-bias-of-the-inverse-covariance-matrix</link>
      <description><![CDATA[交叉发布到 CV 以防万一！
在统计类中，计算样本协方差矩阵的偏差（或缺乏偏差）是标准做法，但我无法找到任何关于样本协方差矩阵的逆如何估计精度矩阵（即逆协方差矩阵）的确切结果。即使只是取期望值，我也很难想象。取一个 IID 样本 $X_1, \cdots, X_n$，每个样本都在 $\mathbb{R}^d$ 中。如果 $C$ 是协方差矩阵，那么查看逆矩阵的单个分量
$$ 
E[C^{-1}(x,X_i)],
$$
似乎不适用于标准方法，因为逆矩阵隐式地依赖于数据集的所有其他值。所以它实际上看起来像
$$
E[C^{-1}(x,X_i)(X_1,\cdots, X_n)].
$$
我们可以使用类似塔属性的东西来获得
$$
E[C^{-1}(x,X_i)(X_1,\cdots, X_n)]= E_{X_i^c}\big [ E_{X_i}[C^{-1}(x,X_i)(X_1,\cdots, X_n)|X_i^c] \big ]
$$
其中 $X_i^c := \{ X_1, \cdots X_{i-1}, X_{i+1} \cdots X_n \}$。那么我可以扩展到积分定义吗？
我的困惑来自于在进行核密度估计器的计算时，我们得到的是
$$
\begin{align}
E[\frac{1}{hn}\sum_i K_h(x,x_i)] &amp;= E[\frac{1}{h}\sum_i K_h(x,x_i)]\\
&amp;= \frac{1}{h}\int_X K_h(x,y)p(y)d\mu(y)
\end{align}
$$
其中第一个等式来自 IID。我的想法是，理解矩阵求逆的期望意味着什么，将使其余的计算变得清晰起来。
我看过的相关作品：问题 1、问题 2 和 问题 3。]]></description>
      <guid>https://stats.stackexchange.com/questions/650947/computing-the-bias-of-the-inverse-covariance-matrix</guid>
      <pubDate>Fri, 12 Jul 2024 17:35:03 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何分析电视剧集的受欢迎程度并兼顾时间因素？</title>
      <link>https://stats.stackexchange.com/questions/650909/how-should-i-analyse-tv-episode-popularity-while-accounting-for-time</link>
      <description><![CDATA[我有一个玩具数据集，其中包含电视剧集、发布日期以及收到的流媒体数量：



剧集
日期
流媒体




暗影领域
2024-03-08
5987


禁书
2024-03-01
6315


魔鬼的茶
2024-02-23
9584


魔法师的鞋子
2024-02-16
8996


魔法森林​​
2024-02-09
7564


回响深渊
2024-02-02
7982


失落文明
2024-01-26
8456


次元裂隙
2024-01-19
8834


水晶迷宫
2024-01-12
9215


扎尔萨的权杖
2024-01-05
9763



我有兴趣分析每集的流媒体数量接收。
具体来说，从数据中我们可以看到，一集播出的时间越长，它收到的流媒体就越多（一般来说）。同时，一些较新的剧集的流媒体播放量高于“预期”流媒体数量与发布日期的关系（此处的剧集是《魔鬼的茶》和《魔法师的鞋子》）。
我应该使用什么分析方法来确定每集的受欢迎程度（更高的流媒体 = 更受欢迎），同时考虑/控制时间的影响（较早的发布日期 = 更高的流媒体）？
如果有帮助，数据也以 R dput 格式提供。
structure(list(episode = c(&quot;The Shadow Realm&quot;, &quot;The Forbidden Tome&quot;, 
&quot;The Devil&#39;s Tea&quot;, &quot;The Sorcerer&#39;s Shoes&quot;, &quot;The Enchanted Forest&quot;, 
&quot;The Echoing Abyss&quot;, &quot;The Lost Civilization&quot;, &quot;The Dimensional Rift&quot;, 
&quot;The Crystal迷宫”、“扎尔萨的权杖”），date = structure(c(19790, 
19783, 19776, 19769, 19762, 19755, 19748, 19741, 19734, 19727
), class = “日期”），streams = c(5987, 6315, 9584, 8996, 7564, 
7982, 8456, 8834, 9215, 9763)), class = “data.frame”，row.names = c(NA, 
-10L))
]]></description>
      <guid>https://stats.stackexchange.com/questions/650909/how-should-i-analyse-tv-episode-popularity-while-accounting-for-time</guid>
      <pubDate>Fri, 12 Jul 2024 00:13:49 GMT</pubDate>
    </item>
    <item>
      <title>如何对密度数据进行功效分析？</title>
      <link>https://stats.stackexchange.com/questions/650687/how-to-do-a-power-analysis-on-density-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/650687/how-to-do-a-power-analysis-on-density-data</guid>
      <pubDate>Mon, 08 Jul 2024 18:27:16 GMT</pubDate>
    </item>
    <item>
      <title>为什么考虑自相关残差几乎无助于分布滞后模型中的参数估计</title>
      <link>https://stats.stackexchange.com/questions/650626/why-does-accounting-for-autocorrelated-residuals-barely-help-parameter-estimatio</link>
      <description><![CDATA[这个问题困扰了我很长时间。基本上，我有一个分布式滞后模型$$y_t=\sum_{i=0}^{p} \beta_i x_{t-i} + u_t.$$
回归问题有点错误指定，所以我最终得到自相关错误$$u_t=\alpha u_{t-1}+\epsilon_t.$$
由于我的模型中自相关程度很高，$\beta$的估计应该非常低效，但是当我使用 GLS 校正自相关时，我的$\beta$估计没有任何改善。当我的问题略有不同，并且 $$y_t= \sum_{i=1}^p \beta_i x_t^i + u_t,$$ 时，$y$ 只是同时存在的不同 $x$ 的函数，GLS 表现惊人，而最小二乘则表现不佳。
为什么 GLS 在第一个例子中表现如此糟糕，我如何让它表现更好（我只关心参数估计）？
我在 R 中编写了两个例子：
n_sim &lt;- 15
LS_R2 &lt;- rep(0, n_sim)
GLS_R2 &lt;- rep(0, n_sim)
ar_noise &lt;- 0.7
for(sim in 1:n_sim){
n=5000
set.seed(sim)
x_vec &lt;- arima.sim(list(order=c(1,0,0), ar=c(0.5)), n=n, sd=1)
noise &lt;- arima.sim(list(order=c(1,0,0), ar=c(ar_noise)), n=n, sd=1)
y &lt;- rep(0,n)
p &lt;- 150
true_beta &lt;- seq(from=5, to=0, length.out=p) + rnorm(p,sd=0.1)
for(i in p:length(y)){
y[i] &lt;- y[i] + sum(x_vec[i:(i-p+1)]*true_beta)
}
y &lt;- y + noise*var(y)/var(noise)/50

# 尝试简单最小二乘法（应该非常低效）
Xmat &lt;- matrix(0, ncol = p, nrow = n)
for(i in 1:ncol(Xmat)){
Xmat[,i] &lt;- c(rep(0,i-1), x_vec[1:(length(x_vec)-i+1)])
}

XXProd &lt;- crossprod(Xmat)
XyProd &lt;- crossprod(Xmat, matrix(y, ncol = 1))

est_beta &lt;- as.numeric(solve(XXProd, XyProd, tol=0))
LS_R2[sim] &lt;- 1- sum((true_beta-est_beta)^2)/sum((true_beta - 
mean(true_beta))^2)

# 尝试广义最小二乘法（应该是最优且有效的）
get_T &lt;- function(ar, N){
i_vec &lt;- c(1:N,2:N)
j_vec &lt;- c(1:N,1:(N-1))
x_vec &lt;- c(sqrt(1-ar^2), rep(1,N-1), rep(-ar,N-1))
sparseMatrix(i = i_vec, j = j_vec, x = x_vec, dims = c(N,N))
}

T1 &lt;- get_T(ar_noise, length(y))

Xmat &lt;- T1%*% Xmat
y &lt;- as.numeric(T1%*% matrix(y, ncol=1))

XXProd &lt;- crossprod(Xmat)
XyProd &lt;- crossprod(Xmat, matrix(y, ncol = 1))

est_beta &lt;- as.numeric(solve(XXProd, XyProd, tol=0))
GLS_R2[sim] &lt;- 1- sum((true_beta-est_beta)^2)/sum((true_beta - 
mean(true_beta))^2)
}
LS_R2
GLS_R2
mean(LS_R2)
mean(GLS_R2)

和
n_sim &lt;- 15
LS_R2 &lt;- rep(0,n_sim)
GLS_R2 &lt;- rep(0,n_sim)
for(sim in 1:n_sim){
n=5000
set.seed(sim)
noise &lt;- arima.sim(list(order=c(1,0,0), ar=c(ar_noise)), n=n, sd=1)
y &lt;- rep(0,n)
p &lt;- 150
Xmat &lt;- matrix(0, ncol = p, nrow = n)
for(i in 1:ncol(Xmat)){
Xmat[,i] &lt;- arima.sim(list(order=c(1,0,0), ar=c(0.5)), n=n, sd=1)
}
true_beta &lt;- seq(from=5, to=0, length.out=p) + rnorm(p,sd=0.1)
for(i in p:length(y)){
y[i] &lt;- y[i] + sum(Xmat[i,]*true_beta)
}
y &lt;- y + noise*sd(y)/sd(noise)

# 尝试简单最小二乘法（应该非常低效）
XXProd &lt;- crossprod(Xmat)
XyProd &lt;- crossprod(Xmat, matrix(y, ncol = 1))

est_beta &lt;- as.numeric(solve(XXProd, XyProd, tol=0))
LS_R2[sim] &lt;- 1- sum((true_beta-est_beta)^2)/sum((true_beta - 
mean(true_beta))^2)

# 尝试广义最小二乘法（应该是最优和高效的)
get_T &lt;- function(ar,N){
i_vec &lt;- c(1:N,2:N)
j_vec &lt;- c(1:N,1:(N-1))
x_vec &lt;- c(sqrt(1-ar^2), rep(1,N-1), rep(-ar,N-1))
sparseMatrix(i = i_vec, j = j_vec, x = x_vec, dims = c(N,N))
}

T1 &lt;- get_T(ar_noise, length(y))

Xmat &lt;- T1 %*% Xmat
y &lt;- as.numeric(T1%*% matrix(y,ncol=1))

XXProd &lt;- crossprod(Xmat)
XyProd &lt;- crossprod(Xmat, matrix(y,ncol = 1))

est_beta &lt;- as.numeric(solve(XXProd, XyProd,tol=0))
GLS_R2[sim] &lt;- 1- sum((true_beta-est_beta)^2)/sum((true_beta - 
mean(true_beta))^2)

}
LS_R2
GLS_R2
mean(LS_R2)
mean(GLS_R2)
]]></description>
      <guid>https://stats.stackexchange.com/questions/650626/why-does-accounting-for-autocorrelated-residuals-barely-help-parameter-estimatio</guid>
      <pubDate>Sun, 07 Jul 2024 07:15:49 GMT</pubDate>
    </item>
    <item>
      <title>为什么逆倾向得分加权有效？</title>
      <link>https://stats.stackexchange.com/questions/650624/why-does-inverse-propensity-score-weighting-work</link>
      <description><![CDATA[假设某些治疗 $D = 0, 1$ 对结果 $Y = 0,1$ 的影响受到性别 $S = 0,1$ 的混淆。对 $D$ 对 $Y$ 的因果影响的无混淆估计将使我们估计层内风险，然后在计算其差异之前根据层的流行程度对这些风险进行加权。从数学上讲，我们会计算
$$ E[Y(D=d)] = \sum_s E[Y \mid D=d, S=s] P(S=s) $$
对每个 $d$ 计算差值。如果简单地写出均值估计量的简单差异，就会发现权重不正确，这是造成混杂的原因
$$ E[Y\mid D=d] = \sum_s E[Y \mid D=d, S=s] P(S=s \mid D=D) $$
请注意，通过贝叶斯规则
$$ P(S=s \mid D=D) = \dfrac{P(D=D \mid S=S) P(S=s)}{P(D=d)} $$
它是倾向得分和正确权重的函数$P(S=s)$。但是，简单地用倾向得分的倒数对 $E[Y \mid D=d]$ 的估计值进行加权，就会在 $E[Y \mid D=d]$ 的表达式中留下一个 $1/P(D=d)$ 因子。
那么，为什么 IPTW 会得出正确的因果对比估计值呢？我希望得到一个符合我在此处所写的期望加权和的答案。具体来说，我希望证明 IPTW 可以得出与我提出的第一个方程类似的表达式。

编辑：
这是我自己的尝试
让每个和的权重为
$$ w(s) = \dfrac{1}{\Pr(D=d \mid S=s)}$$
这些不能保证总和为 1，所以让我们计算它们的总和并创建新的、标准化的权重。
$$ \sum_s w(s, d) = \dfrac{1}{\Pr(D=d \mid S=0)} + \dfrac{1}{\Pr(D=d \mid S=1)}$$
寻找共同点...
$$ \sum_s w(s, d) = \dfrac{\Pr(D=d \mid S=1) + \Pr(D=d \mid S=0)}{\Pr(D=d \mid S=1) \times \Pr(D=d \mid S=0)}$$
现在，事情变得糟糕起来。首先我们来承认
$$ \Pr(D=d \mid S=s) = \dfrac{\Pr(S=s \mid D=d) \Pr(D=d)}{\Pr(S=s)}$$
如果我们用分子和分母替换$D$的条件概率，那么$\Pr(D=d)$的因子就会出现。我们在分子中得到一个因子，在分母中得到 2，因此
$$ \sum_s w(s, d) = \dfrac{f(s, d)}{\Pr(D=d)} $$
其中 $f$ 是使用贝叶斯规则重写的表达式。重要的是，我们得到了 $\Pr(D=d)$ 的因子。
现在，定义 $\Omega(s, d) = \dfrac{w(s, d)}{\sum_s w(s, d)} = \dfrac{w(s) \Pr(D=d)}{f(s, d)} = \dfrac{\Pr(D=d)}{\Pr(D=d \mid S=s) f(s, d)} $
因此，使用 $\Omega(s, d)$ 作为权重的加权和具有我们需要抵消“错误”的因子均值差异中的权重。
我的直觉告诉我 $f(s, d) = 1$，但我不确定，而且尚未证明这一点。]]></description>
      <guid>https://stats.stackexchange.com/questions/650624/why-does-inverse-propensity-score-weighting-work</guid>
      <pubDate>Sun, 07 Jul 2024 05:59:37 GMT</pubDate>
    </item>
    <item>
      <title>关于测试可靠性对测试组合权重的影响的问题</title>
      <link>https://stats.stackexchange.com/questions/650558/question-on-the-effect-of-test-reliability-on-weighting-of-a-test-battery</link>
      <description><![CDATA[最初，测试组有 4 个部分：两个 100 项多项选择题测试、一个口语测试和一个论文测试。每个部分测量不同的主题。4 个部分中的每一个权重为 25%。
现在，每个多项选择题测试的测试长度已减少到 60 项（出于实际原因）。每个部分仍占 25%。
如果两个 MC 测试的可靠性降低到零，则两个 MC 测试测量的两个主题的权重将为零。（两个 MC 测试只会产生误差。）实际上，可靠性降低了，但并没有降低到零。
我的问题是，可靠性的变化对两个 MC 测试测量的两个主题的权重有何影响。我如何估计由于变化而导致的权重差异？
澄清：
原始权重是根据相关方的共识选择的。因此，我首先假设原始测试组的权重是理想的（只要各方同意这些权重）。项目数量的减少是出于权宜之计。项目越少，测试可靠性就越低。我试图确定新的测试组与原始测试组相比，在对测试组中每个测试所测量的主题赋予的权重方面，MC 测试时间越短。（直观上看，可靠性越低的测试对测试所测量的主题赋予的有效权重就越低。在极端情况下，如果测试的可靠性为零，它们只会给测试组的总分增加随机噪音，尽管名义权重为 25%，但 MC 测试所测试的主题根本没有任何权重。）
更新以解决评论：
似乎我的问题包含一些不成熟或非标准的词汇。因此，这里是使用其他词语来陈述我的问题的另一种尝试。
25% 的原始权重假设四个测试中的每一个都具有现有的有效性水平，并且一​​致认为这些测试理想情况下应该具有相等的权重。我认为这意味着每个测试最初对总分贡献了 25% 的非随机有效方差。
我推测，切换到较短的 MC 测试已经改变了两个 MC 测试的可靠性，因此也改变了它们的有效性。因此，较短的 MC 测试对总分的有效方差贡献较少。从这个意义上说，它们不再是 25% 的权重。我的问题是，它们的新权重是多少。
两种可能的方法：
这是我对如何处理我提出的问题的一种想法。有效性受可靠性平方根的限制。因此，如果可靠性从 0.8 降低到 0.7，则有效性上限从 0.89 降至 0.84。这似乎意味着新的、更短的 MC 测试的有效贡献比原始测试少 5%。就比例而言，即少 0.05/0.89，或新的较低权重比原始权重少约 6%。也就是说，更短的 MC 测试对总分的有效信息贡献减少了 6%。也许可以通过调整 25% 的权重来弥补这一点。对这种方法有什么看法？
或者，由于可靠性与效用呈线性关系，因此可靠性比率可能是 MC 测试影响减少的更好指标。这可能是 0.7/0.8=0.875，或权重减少 12.5%。对这种方法有什么看法？]]></description>
      <guid>https://stats.stackexchange.com/questions/650558/question-on-the-effect-of-test-reliability-on-weighting-of-a-test-battery</guid>
      <pubDate>Fri, 05 Jul 2024 22:15:38 GMT</pubDate>
    </item>
    <item>
      <title>回归模型中的混杂变量：辛普森悖论</title>
      <link>https://stats.stackexchange.com/questions/650388/confounding-variable-in-regression-model-simpsons-paradox</link>
      <description><![CDATA[我正在研究一个混合效应回归模型，其中 Yi = 学生 i 的考试成绩。
解释变量如下：

级别 3：学校类型（公立与私立）和学校的社会经济水平（数字变量）

级别 2：每个班级的教育模式（主要路径、特殊教育）

级别 1：移民背景（学生是否是移民）、学生的社会经济水平（数字变量）、在家使用的语言（英语或其他语言）以及学生的身份（学生是否必须留级一年）
我使用两个模型来解释 Y。第一个模型不包括变量 student_socioeconomic 和 student_idoneity。
model1 = lmer(data = scores, English_score ~ (1| school_id/group_id) +
school_type + school_socioeconomic + group_educational_model +
student_inmigrant + student_language)
model2 = lmer(data = scores, English_score ~ (1| school_id/group_id) +
school_type + school_socioeconomic + group_educational_model +
student_socioeconomic + student_inmigrant + student_language +
student_idoneity)


在第一个模型中，变量“student_inmigrant”的估计系数为正，并且在 1% alpha 水平上显著。然而，当我添加变量“student_socioeconomic”时和“student_idoneity”，变量“student_inmigrant”的估计系数在 1% alpha 水平上变为负且显著。
我认为这里存在混杂变量的问题，但我不知道如何解决。您能给我一些关于如何处理这个问题的建议吗？
我检查了存在多重共线性时的 VIF 值，但 student_inmigrant、student_idoneity 和 student_socioeconomic 的调整后的 GVIF 都低于 2。]]></description>
      <guid>https://stats.stackexchange.com/questions/650388/confounding-variable-in-regression-model-simpsons-paradox</guid>
      <pubDate>Wed, 03 Jul 2024 12:15:21 GMT</pubDate>
    </item>
    </channel>
</rss>