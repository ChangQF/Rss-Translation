<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 10 Dec 2024 15:19:36 GMT</lastBuildDate>
    <item>
      <title>帮助解释回归假设图[重复]</title>
      <link>https://stats.stackexchange.com/questions/658525/help-with-interpreting-regression-assumption-plots</link>
      <description><![CDATA[寻求一些帮助来解释这些线性回归图。我认为它们表明存在异质性，并且 80 是一个潜在的高度影响点。但我不知道这个模型在多大程度上可以接受并可靠地解释？
我认为根据我见过的其他图，异质性还不错？
谢谢
]]></description>
      <guid>https://stats.stackexchange.com/questions/658525/help-with-interpreting-regression-assumption-plots</guid>
      <pubDate>Tue, 10 Dec 2024 15:12:29 GMT</pubDate>
    </item>
    <item>
      <title>p值如何排列</title>
      <link>https://stats.stackexchange.com/questions/658523/how-to-permutation-of-pvalue</link>
      <description><![CDATA[最近，我的同事在处理一个数据集 df_a 时遇到了一个问题，该数据集包含基因数据。每一行代表一个 基因（通常有数千个，但为了简单起见，我们假设有 1,000 个），列代表不同的样本，这些样本可以分为两组，A 和 B（每组 50 个样本）。
我们使用 t.test 计算每行的 p 值并进行校正。假设我们使用 p.adjust &lt; 0.05 并且得到 30 个阳性结果。
这带来了一个问题：我们如何确保这 30 个结果不是由随机事件产生的？（也许这个问题是一个问题？）
我们设计了一个使用置换检验的流程来解决这个问题。步骤如下：
(1) 对于 df_a，我们随机打乱其行标签，重新计算每行的 p 值和 p.adjust，并计算 p.adjust &lt; 0.05 的行数，记为 Ki。
(2) 重复步骤 1 1,000 次，得到 K1,K2,K3,...,K1000。
(3) 计算大于 30 的 Ki 的数量，记为 n。计算 n/1000。如果该值小于0.05，我们认为这30个结果不是由随机事件产生的。
作为一名程序员，我意识到数学上似乎存在问题，但我无法向同事提供严格的证明来纠正它。希望得到您的帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/658523/how-to-permutation-of-pvalue</guid>
      <pubDate>Tue, 10 Dec 2024 14:31:46 GMT</pubDate>
    </item>
    <item>
      <title>如何将数千种产品的每日销售数据汇总为每周销售数据？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/658522/how-to-aggregate-daily-sales-data-to-weekly-for-thousands-of-products</link>
      <description><![CDATA[我有一个数据集，其中包含 3 家商店 5 年内 3000 种产品的每日销售量和价格。我想可视化一年中各周的价格和销售趋势。我原本想创建一个热图，x 轴为周数，y 轴为产品数。但似乎无法包含所有产品。
这是正确的方法吗？
tx_agg &lt;- texas_r |&gt; mutate(week_start = floor_date(date, &quot;week&quot;, week_start = 1)) |&gt; group_by(item_id, cat_id, store_id, week_start, event_name_1) |&gt; summarise( weekly_price = mean(sell_price, na.rm = TRUE), weekly_sales = sum(daily_sales, na.rm = TRUE) )
此外，选择产品的最佳方法是什么？产品有三类 - 家居、爱好、食品。我不确定如何从每个类别中选择哪些产品并创建热图]]></description>
      <guid>https://stats.stackexchange.com/questions/658522/how-to-aggregate-daily-sales-data-to-weekly-for-thousands-of-products</guid>
      <pubDate>Tue, 10 Dec 2024 13:40:37 GMT</pubDate>
    </item>
    <item>
      <title>核密度估计的$L_2$范数一致性</title>
      <link>https://stats.stackexchange.com/questions/658521/l-2-norm-consistency-of-kernel-density-estimation</link>
      <description><![CDATA[我能找到的核密度估计的一致性结果几乎都针对 $L_1$-norm 或 $L_\infty$-norm，例如在这篇论文或这篇论文中。我无法简单地将它们推广到 $L_2$-norm，因为在 $\mathbb{R}$ 这样的无限测度场上，$L_2$ 和 $L_1$ 范数之间的关系尚不清楚。所以有人能告诉我 $L_2$-norm 的一致性结果吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658521/l-2-norm-consistency-of-kernel-density-estimation</guid>
      <pubDate>Tue, 10 Dec 2024 12:29:12 GMT</pubDate>
    </item>
    <item>
      <title>Beta 分布和矩问题（需要引用）</title>
      <link>https://stats.stackexchange.com/questions/658519/beta-distribution-and-the-moment-problem-citation-needed</link>
      <description><![CDATA[据我所知，Beta 分布根据其矩具有唯一定义（即矩问题在其矩值上具有唯一解）。维基百科关于 Beta 分布的文章甚至对此进行了阐述，但维基百科上的参考资料（Billingsley，“概率与测量”）仅提供了矩生成函数收敛半径为正的任意分布的唯一性 - 我无法在文献中找到有关 Beta 分布的结果。
如果我自己要推导收敛半径，我会用 (1/k!) 来限制矩生成函数的系数，其 k 次根收敛到 0，并使用柯西-哈达玛定理来获得无限的收敛半径。但我对收敛半径的理解相当肤浅，我宁愿不在我的文章中包含推导，否则这与此完全无关。由于结果看起来相当直接，我很难相信没有已发表的证明，尽管我找不到。这里有人知道我可以引用的参考资料吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658519/beta-distribution-and-the-moment-problem-citation-needed</guid>
      <pubDate>Tue, 10 Dec 2024 12:02:45 GMT</pubDate>
    </item>
    <item>
      <title>如何调整超参数以在小数据集下降低校准误差</title>
      <link>https://stats.stackexchange.com/questions/658518/how-to-tune-hyperparameters-for-low-calibration-error-under-small-dataset</link>
      <description><![CDATA[我正在研究变分自动编码器 (VAE) 的哪种变体在小数据集下可以提供更好的预期校准误差 (ECE)（另请参阅此文档）。根据 google 的调整手册，要比较科学超参数，我们需要“优化”干扰超参数。当应用于此处时，它意味着要比较 VAE 变体在 ECE 方面的性能，我们需要找到每个 VAE 变体在 ECE 方面的最佳其他超参数集（例如学习率），以便 VAE 变体之间的比较足够公平。显然，另一个经验法则是，我们应该在验证集而不是测试集上调整超参数。
问题是，为了可靠地计算 ECE，我们需要一个相对较大的数据集，这意味着验证集必须足够大，但在我的例子中，验证集不会足够大，因为它是一个小型训练集的一部分。我使用的是合成数据，因此我可以随时扩大验证集。所以我的问题是，我应该在优化干扰超参数（如学习率）的同时扩大验证集吗？
非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/658518/how-to-tune-hyperparameters-for-low-calibration-error-under-small-dataset</guid>
      <pubDate>Tue, 10 Dec 2024 10:36:20 GMT</pubDate>
    </item>
    <item>
      <title>联合样本的边际经验分布</title>
      <link>https://stats.stackexchange.com/questions/658516/marginal-empirical-distribution-from-joint-sample</link>
      <description><![CDATA[我有一个相当“简单”的疑问，想澄清一下。
假设我有一个分层模型，其中数据以以下方式采样：

从 $P_U$ 中采样 $U_i$
从 $P_{X|U_i}$ 中采样 $X_i$

换句话说，根据 $U_i$ 的值，从不同的分布中采样 $X_i$。$U$ 是 i.i.d。但 $X$ 仅在给定每个 $U_i$ 的情况下才是独立同分布的。
让 $\mathcal{U},\mathcal{X}$ 分别成为 U 和 X 的支持。我现在感兴趣的是估计数据的经验分布。如果我观察$(U_i,X_i)$的样本，那么\underline{joint}经验分布的估计量就是经验分布：
$$F_{U,X}(A)\approx\frac{1}{n}\sum_{i=1}^n1_{(U_i,X_i)\in A}\quad\quad A\in \mathcal{U}\times\mathcal{X}$$
假设我现在不观察潜在变量 U。我如何估计 X 的边际分布？对 U 的依赖是否会引起问题？我知道 $F_X(X) = \int F_U(X,U)dU$ 但这在实践中如何转化？X 的边际分布的估计量是否简单：
$$F_X(B)\approx\frac{1}{n}\sum_{i=1}^n1_{(X_i)\in B}\quad\quad B\in \mathcal{X}$$
我是否把一个简单的问题复杂化了？任何帮助都将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/658516/marginal-empirical-distribution-from-joint-sample</guid>
      <pubDate>Tue, 10 Dec 2024 10:07:51 GMT</pubDate>
    </item>
    <item>
      <title>斯托弗 (Stouffer) 方法：对产生 p 值的潜在假设检验进行限制？</title>
      <link>https://stats.stackexchange.com/questions/658515/stouffers-method-restriction-on-underlying-hypothesis-tests-producing-p-values</link>
      <description><![CDATA[当使用 Stouffer 方法来组合 p 值时，必须先将它们转换为 z 分数。这是否意味着，例如从置换测试中得出的 p 值不是有效输入，因为它们不是基于 z 分数？]]></description>
      <guid>https://stats.stackexchange.com/questions/658515/stouffers-method-restriction-on-underlying-hypothesis-tests-producing-p-values</guid>
      <pubDate>Tue, 10 Dec 2024 09:43:30 GMT</pubDate>
    </item>
    <item>
      <title>在因变量的原始分数中预先存在截止点的分位数回归</title>
      <link>https://stats.stackexchange.com/questions/658514/quantile-regression-with-a-pre-existing-cutoff-point-in-the-raw-score-of-depende</link>
      <description><![CDATA[这可能是一个愚蠢的问题，但我从未使用过分位数回归，所以我不确定。
我的情况如下：有一个连续的、数值的自我报告测量，其特定的、预先设定的截止点为 100 分。根据之前的研究，得分低于 100 分的人被认为没有某种“疾病”，而得分为 100 分或以上的人则有这种疾病。我的同事想运行一个模型，根据某些独立变量预测这个分数，但他们想看看那些得分低于 100 分和高于 100 分的人的预测因子与分数的关系是否不同。
我之前在这里得到过建议，建议分位数回归会起作用，我同意。但我不确定如何指定分位数。显然，我需要指定（至少）2 个分位数，以获得有条件的人和没有条件的人的预测，但我该如何选择它们呢？我是否应该检查样本中原始分数 100 对应的分位数，然后...以某种方式...选择一个略低于“100 分位数”的分位数和一个略高于“100 分位数”的分位数？但是低于和高于多少呢？我应该如何选择准确的分位数？
（进行这项研究的人更喜欢只有两个预测水平（两个分位数），因为以前的研究表明 100 是一个重要的截止值，也是为了保持简单。）]]></description>
      <guid>https://stats.stackexchange.com/questions/658514/quantile-regression-with-a-pre-existing-cutoff-point-in-the-raw-score-of-depende</guid>
      <pubDate>Tue, 10 Dec 2024 08:40:44 GMT</pubDate>
    </item>
    <item>
      <title>关于赫斯特指数定义和持久性属性的澄清</title>
      <link>https://stats.stackexchange.com/questions/658520/clarifications-on-hurst-exponent-definitions-and-persistence-properties</link>
      <description><![CDATA[我对赫斯特指数有疑问，希望有人能帮我澄清一下。
众所周知，赫斯特指数有不同的定义，但找到这些定义之间的明确联系或关系似乎出奇地具有挑战性。我遇到过各种各样的解释，但我还没有找到一个全面的解释来说明这些不同的定义在实际应用中是如何一致或不同的。
此外，人们普遍认为：

对于$H&lt;0.5$，该过程表现出反持久性。
对于$H&gt;0.5$，该过程表现出持久性。

然而，虽然这种分类经常出现，但很少提供其背后的原因或更深层次的解释。
有人可以提供一些澄清，或者给我指出可靠的参考书目来解决：

赫斯特指数的各种定义之间的关系。
清楚地解释为什么$H&lt;0.5$ 对应于反持久性，而 $H&gt;0.5$ 对应于持久性。

提前感谢您的见解！]]></description>
      <guid>https://stats.stackexchange.com/questions/658520/clarifications-on-hurst-exponent-definitions-and-persistence-properties</guid>
      <pubDate>Mon, 09 Dec 2024 09:22:40 GMT</pubDate>
    </item>
    <item>
      <title>IV 等级/相关性条件线性代数直觉</title>
      <link>https://stats.stackexchange.com/questions/658451/iv-rank-relevance-condition-linear-algebra-intuition</link>
      <description><![CDATA[考虑以下计量经济模型（IV）：$Y_1 = X&#39;\beta + e$，其中$Y_1 \in \mathbb{R}$是一些感兴趣的结果变量，我们有一组回归量$X = \begin{bmatrix} Z_1 \\ Y_2 \end{bmatrix} \in \mathbb{R}^k$。 ($Z_1 \in \mathbb{R}^{k_1}, Y_2 \in \mathbb{R}^{k_2}, k_1 + k_2 = k \:$) 假设我们可能有一些混杂因素，因此$\mathbb{E}[Xe] \neq 0$。但是假设我们也有一些工具变量$Z=\begin{bmatrix}Z_1\\Z_2 \end{bmatrix} \in \mathbb{R}^{\mathcal{l}}$，使得$\mathbb{E}[Ze] =0, \mathbb{E}[ZZ&#39;]$为psd，并且$\mathbb{E}[ZX&#39;]$的秩为$k$（相关性）。
我对相关性条件很好奇：$\mathbb{E}[ZX&#39;]$的秩必须为$k$。我可以理解这里与 $Cov(X, Z)$ 的联系，但我只是不太清楚为什么秩条件意味着这一点（与 $Z$ 变换对 $X$ 列的作用有关），以及这在某种意义上必须“保留”至少 $X$ 维度才能相关。
此外，在我看来，这似乎与原始 OLS 案例中的秩条件有些相关：$\hat{\beta} = (X&#39;X)^{-1}X&#39;Y_1$，其中秩也必须是 $k$（这次我们希望我们的回归量是线性独立，每个“提供新的东西”）。这再次将 $\beta$ 的最简单形式概括为 $\frac{Cov}{Var}$。
总结一下，我缺少一些几何直觉，无法理解这些变换如何告诉我们空间中不同变量的方差/协方差。即，形式 $ZX&#39;$ 与解释这些变量之间的协变有什么直观的几何联系？以 $X&#39;X$ 为例，我可以看到当 $X$ 被贬低（或包含常数）时，$Var(X) = Cov(X, X) = \mathbb{E}[XX&#39;]$ 如何，因此变为 $\frac{1}{n} \mathbf{X&#39;X}$ 并带有样本。对于上面的 $\mathbb{E}[ZX&#39;]$，为什么我们需要 rank = $k$ 才能使 $Cov$ &#39;不为 0&#39;？]]></description>
      <guid>https://stats.stackexchange.com/questions/658451/iv-rank-relevance-condition-linear-algebra-intuition</guid>
      <pubDate>Sun, 08 Dec 2024 11:34:47 GMT</pubDate>
    </item>
    <item>
      <title>标准化回归模型的变量与回归模型中的权重？</title>
      <link>https://stats.stackexchange.com/questions/658437/standardizing-variables-for-a-regression-model-vs-weights-in-a-regression-model</link>
      <description><![CDATA[我在 R 中有一个纵向 GAM（一般加性模型）回归。
这是模型和数据的一般形式（响应介于 0 和 1 之间）。所有变量均在州一级计算（例如州 GDP、州内疾病率）：
gam_model &lt;- gam(
response ~ 
te(time_var, var1) +
te(time_var, var2) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

state time_varpopulation var1 var2 response
state_1 2005-01-01 1000000 500000 10000 0.45
state_1 2005-02-01 1001000 520000 12000 0.47
state_1 2005-03-01 1002001 540000 11000 0.46
state_1 2005-04-01 1003002 560000 13000 0.48
state_2 2005-01-01 200000 100000 2000 0.42
state_2 2005-02-01 200200 105000 2400 0.44
state_2 2005-03-01 200400 110000 2200 0.43
state_2 2005-04-01 200600 115000 2600 0.45

这里是我遇到的问题：

数据按州提供（多个州，1 个国家），但每个州的人口不同
这让我认为需要对模型进行一些处理，以防止人口较多的州对响应产生比人口较少的州更大的影响
我有每个州的人口

我正在考虑使用以下权重公式（我从这里得到这个想法https://www.nature.com/articles/s41598-024-54441-x）：
$$avg\_weight_s = \frac{1}{T}\sum_{t=1}^{T} \frac{\ln(population_{s,t})}{\frac{1}{N}\sum_{i=1}^{N} \ln(population_{i,t})}$$
其中：

$T$ 是时间段的总数
$N$ 是州的总数
$population_{s,t}$ 是 $t$ 时刻 $s$ 州的人口数
$population_{i,t}$ 表示每个州 $i$ 在时间 $t$ 的人口
请注意，尽管我为每个州设置了多个时间点...但每个州都只有一个权重（将始终使用）。我只是想重申这一点。

这让我考虑不同的模型选项：
# 选项 1：非标准化，无权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1) +
te(time_var, var2) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

# 选项 2：标准化，无权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1/population) +
te(time_var, var2/population) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

# 选项 3：非标准化，权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1) +
te(time_var, var2) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
weights = avg_weight,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

# 选项 4：标准化，权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1/population) +
te(time_var, var2/population) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
weights = avg_weight,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

我有点困惑，不知道这些选项中哪一个在逻辑上是正确的。我认为其中一些可能有点过度，而另一些则完全不正确。有办法解决这个问题吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658437/standardizing-variables-for-a-regression-model-vs-weights-in-a-regression-model</guid>
      <pubDate>Sun, 08 Dec 2024 04:38:44 GMT</pubDate>
    </item>
    <item>
      <title>平稳性条件 VECM</title>
      <link>https://stats.stackexchange.com/questions/657939/stationarity-conditions-vecm</link>
      <description><![CDATA[假设我们有一个向量误差修正模型 (VECM)
$$
\Delta y_{t}=\Pi y_{t-1}+\Gamma_{1}\Delta y_{t-1}+\cdot\cdot\cdot+\Gamma_{p-1}\Delta y_{t-p+1}+u_{t}
$$
确认它是一个平稳过程的一个简单方法是将其转换为 VAR 模型
$$
y_{t} = \left(I+\Pi+\Gamma_{1}\right)y_{t-1}+\cdot\cdot\cdot+\left(\Gamma_{p-1}-\Gamma_{p-2}\right)y_{t-p+1}-\Gamma_{p-1}y_{t-p}+u_{t}
$$
找到伴随矩阵的特征值
$$
\left[\begin{array}{ccccc}
I+\Pi+\Gamma_{1} &amp; \Gamma_{2}-\Gamma_{1} &amp; \cdot\cdot\cdot &amp; \Gamma_{p-1}-\Gamma_{p-2} &amp; -\Gamma_{p-1}\\
I &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; I &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; I &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; I &amp; 0
\end{array}\right]
$$
并检查是否有任何模数大于 1。
我好奇的是，假设我们确切知道矩阵
$$
\left[\begin{array}{ccc}
\Gamma_{1} &amp; \cdot\cdot\cdot &amp; \Gamma_{p-1}\\
I &amp; 0 &amp; 0\\
0 &amp; I &amp; 0
\end{array}\right]
$$
没有任何模数大于或等于 1 的特征值。
基于该假设，我们必须对 $\Pi$ 做出哪些假设，以确保原始伴随矩阵的特征值不大于一？
换句话说，我们知道所有差异分量加在一起都是静止的。我们必须对 $\Pi$ 做出哪些假设，以确保整个过程不会爆炸？
我希望 $\Pi$ 所需的任何条件都与 $\Gamma_{i}$ 无关。]]></description>
      <guid>https://stats.stackexchange.com/questions/657939/stationarity-conditions-vecm</guid>
      <pubDate>Wed, 27 Nov 2024 15:56:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们通常不担心具有固定效应的面板数据模型中的平稳性？</title>
      <link>https://stats.stackexchange.com/questions/657907/why-dont-we-typically-worry-about-stationarity-in-panel-data-models-with-fixed</link>
      <description><![CDATA[为什么我们通常不担心具有固定效应的面板数据模型中的平稳性？
在时间序列分析中，平稳性通常是一个关键假设。然而，我注意到，在面板数据的应用微观计量经济学中，研究人员似乎不太关心变量$y_{it}$和$x_{it}$是否是平稳的。
考虑一个典型的固定效应模型：
$$y_{it} = \alpha_i + \alpha_t + \sum_{l=1}^L \beta_l x_{i,t-l} + \epsilon_{it}$$
在这种情况下，我们似乎常常不关心：

$y_{it}$是否满足传统的平稳性假设

协方差结构$Cov(y_{it}, y_{i,t+k})$是时不变的

条件期望$E[y_{it}|\alpha_i,\alpha_t]$
与 $t$ 无关


这与 VAR 文献形成鲜明对比，在 VAR 文献中平稳性起着核心作用。
为什么我们经常可以在面板数据环境中忽略平稳性？
拥有较大的 $N$（横截面单位）是否会使平稳性变得不那么重要？
有人可以提供直观的解释，如果可能的话，提供一些数学推理吗？我熟悉基本的面板数据方法和时间序列概念，但我试图理解这两个领域对平稳性的处理之间的具体区别。]]></description>
      <guid>https://stats.stackexchange.com/questions/657907/why-dont-we-typically-worry-about-stationarity-in-panel-data-models-with-fixed</guid>
      <pubDate>Wed, 27 Nov 2024 00:13:54 GMT</pubDate>
    </item>
    <item>
      <title>如何使用卡方检验来获取相对频率值？</title>
      <link>https://stats.stackexchange.com/questions/591697/how-do-i-use-the-chi-squared-test-for-relative-frequency-values</link>
      <description><![CDATA[我目前正在研究泊松分布，并尝试确定不同 $\lambda$ 值的泊松分布拟合优度。
本质上，我有一些系列，它们表示在某个间隔内观察到的某些现象的相对频率。$\text{my series[1]} \to \text{ 在间隔内观察到该现象 1 次的相对频率}$
将观察到的数据与泊松分布进行绘图，可以观察到相似性，但与预期值存在偏差。根据我所学，我的理解是卡方检验有助于我们反驳这种偏差是随机产生的噪声。它表明，如果 $\chi^2$ 大于我们的显着性水平 ($\alpha$)，则样本偏差足够大，表明所讨论的 pmf 不能模拟我们数据中每个 $k$ 的真实平均值；除了随机性之外，还有更多因素导致数据出现偏差。
我找到的 $\chi^2$ 公式如下。
$$ \chi^2 = \sum_{k \in K} \frac{(\text{observed}_k - \text{expected}_k)^2}{\text{expected}_k} $$
此外，我发现 $\alpha = 5$ 是相当正常的经验法则。
因此，我计算了不同 $\lambda$ 值与它们各自的 $k$ 值的 Pmf 值，绘制了观察到的相对概率，并且这就是我所看到的。

我观察到的系列没有 $k \in(13 - 20)$ 的值。因此，将我的系列缩减为 $[0 - 12]$。此外，为了防止分母中非常小的期望值造成失真，我将其四舍五入为 5 个有效数字，并跳过了 $\text{expected}[k] \to 0$ 的计算。以下是相应的 $\chi^2$ 值：
$$
\chi^2(\text{Poisson}(\lambda = 1, k), \text{observed} \; | \; k \in [0 -11]) = 5.947219461773308 \\
\chi^2(\text{Poisson}(\lambda = 2, k), \text{observed} \; | \; k \in [0 -11]) = 0.5089040900553339 \\
\chi^2(\text{Poisson}(\lambda = 3, k), \text{observed} \; | \; k \in [0 -11]) = 1.8428302521807396
$$
如果我的解释正确，$\lambda = 1$ 的泊松分布不能很好地模拟分布，但 $\lambda = 2$ 或 $\lambda = 3$ 的泊松分布可以。从分布来看，这对我来说似乎不正确。
我计算错了吗？在查看概率值与实际人口数量时，此公式不适用吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/591697/how-do-i-use-the-chi-squared-test-for-relative-frequency-values</guid>
      <pubDate>Sun, 09 Oct 2022 17:59:57 GMT</pubDate>
    </item>
    </channel>
</rss>