<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Sun, 12 May 2024 01:05:37 GMT</lastBuildDate>
    <item>
      <title>用两个变量相乘来优化目标？</title>
      <link>https://stats.stackexchange.com/questions/647060/optimizing-objective-with-two-variables-multiplied-with-each-other</link>
      <description><![CDATA[假设您想要优化以下目标函数：
$$\min_{a,b} \Vert ab^T - W \Vert_2^2$$
其中 $a \in \mathbb{R}^{m,n}$ 和 $b \in \ mathbb{R}^{n,p}$ 是可学习矩阵，$w \in \mathbb{R}^{m,p}$ 是给你的一些常数矩阵。
由于 $a$ 和 $b$ 通过乘法错综复杂地联系在一起，有哪些方法可以优化这些类型的多变量目标函数？
经过初步研究，我们似乎可以进行贪婪搜索：在每次迭代中，按住 $a$ （或 $b$) 常量，然后仅针对单个变量进行优化 $b$ （或 $a$ ）通过梯度下降。每次迭代后，交替变量。
贪婪搜索听起来与坐标下降类似，我想知道是否有其他方法在整体上更加稳健并且不易出现局部极小值。
似乎有另一种方法称为 EM（期望最大化算法），它假设 $a$ 或 $b $ 是一个潜在变量，但除此之外，感觉它在做与贪婪搜索相同的事情，只是在随机设置中。
除了贪婪搜索之外，还有哪些其他优化算法可以处理复杂链接的多个变量（通过乘法、除法等）？]]></description>
      <guid>https://stats.stackexchange.com/questions/647060/optimizing-objective-with-two-variables-multiplied-with-each-other</guid>
      <pubDate>Sun, 12 May 2024 00:51:01 GMT</pubDate>
    </item>
    <item>
      <title>如何提高LSTM模型的预测质量</title>
      <link>https://stats.stackexchange.com/questions/647059/how-to-improve-prediction-quality-of-lstm-model</link>
      <description><![CDATA[我正在尝试在 MATLAB 中训练基于 LSTM 的模型，以在给定时间序列的 365 个先前值的情况下预测 365 个下一个值。输入形状为 (1000, 365)，输出形状为 (1000, 365)，即有 1000 行，其中每个输入行是 1 年数据的时间序列，每个对应的输出行是对下一年的预测。这些值本身是浮点数。在输入模型之前，我使用以下公式对值进行标准化：
train_stdized = (train_array - mu_train) / sig_train;

哪里
mu_train = 训练数据的平均值
sig_train = 训练数据的标准差

我的模型架构和训练选项如下：
层 = [sequenceInputLayer(lag)
lstmL层(512)
全连接层(150)
完全连接层（未来）]；

Training_options = TrainingOptions(“adam”, “指标”,[“rmse”], ...
        初始学习率=0.01，详细=真，详细频率=10，...
        MaxEpochs=50,MiniBatchSize=1, ...
        ObjectiveMetricName=“rmse”, ...
        ValidationData={x_val, y_val}, ValidationFrequency=10, ...
        LearnRateSchedule=“分段”，LearnRateDropPeriod=100，LearnRateDropFactor=0.1，...
        OutputNetwork=&quot;最佳验证&quot;);


问题是，预测趋势与输入趋势不匹配。相反，情况恰恰相反——预测范围要么远高于输入数据，要么远低于输入数据。也许下面的图表会让你更清楚：


此处。

但是，这并没有太大区别。

我想知道是否可以尝试其他方法来改进预测，以便它们的范围反映输入的范围。我在这方面不是很有经验，所以如果这听起来像是一个基本问题，我深表歉意。感谢你的帮助。

附：我试图包含所有我认为相关的内容。但是，如果我错误地遗漏了任何信息，请告诉我，我会添加它。]]></description>
      <guid>https://stats.stackexchange.com/questions/647059/how-to-improve-prediction-quality-of-lstm-model</guid>
      <pubDate>Sun, 12 May 2024 00:18:38 GMT</pubDate>
    </item>
    <item>
      <title>样本量和变异系数</title>
      <link>https://stats.stackexchange.com/questions/647058/sample-size-and-coefficient-of-variation</link>
      <description><![CDATA[我想知道样本量和变异系数 (CV) 之间的关系。 CV 定义为标准差与平均值的比率。我们是否可以说“样本量越小，CV 越大，因此关系呈负相关”？]]></description>
      <guid>https://stats.stackexchange.com/questions/647058/sample-size-and-coefficient-of-variation</guid>
      <pubDate>Sun, 12 May 2024 00:00:18 GMT</pubDate>
    </item>
    <item>
      <title>通过逻辑回归绘制交互作用</title>
      <link>https://stats.stackexchange.com/questions/647057/plotting-interaction-by-logistic-regression</link>
      <description><![CDATA[我使用 R 绘制了我计算的 logit 函数中的交互项。
我使用标准 cat_plot() 来绘制交互作用。我清楚地看到两条线不平行，这是非常明显的（但它们也没有交叉）。
但交互作用项的 P 值为 0.079。
我该如何解释这个？我仍然可以假设存在交互作用，但交互作用并不显着吗？
我可能认为它不重要的原因是我的样本中主持人的稀有性。难道是这个原因吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/647057/plotting-interaction-by-logistic-regression</guid>
      <pubDate>Sat, 11 May 2024 23:51:25 GMT</pubDate>
    </item>
    <item>
      <title>选择自相关事件</title>
      <link>https://stats.stackexchange.com/questions/647056/select-autocorrelated-events</link>
      <description><![CDATA[我有 $N$ 个时间序列，每个时间序列的长度为 $L$ 和 $k$ 特征描述具有边 $ s \in \{-1, 1\}$ 的事件。我想构建一个模型，根据事件的特征来选择事件，即 $f : \mathbb{R}^k \rightarrow \{0, 1\}$。我希望选定的事件具有较高的自相关性，即每个时间序列中它们的平均值应接近 1 或 -1。我对预测侧面本身的模型不感兴趣，而是寻找自相关事件之间的内在相似性（就我拥有的特征而言）。我认为一个好的损失函数是 $-\text{std}(\underset{\text{of timeseries}}{\text{mean}}(side))$ 计算模型将选择的事件。如果我们使模型连续，我们可以将其给出的分数作为事件的权重，并获得可微的损失函数。这就是我所做的，但有一些问题。该模型（一开始我只采用了一层 MLP，即线性回归）很快收敛到选取所有事件。我必须通过正则化来惩罚它，这只会增加平均输出分数。通过正确调整正则化系数，可以使模型实际上选择自相关性高于整个群体的一部分事件。但模型在训练过程中对该参数非常敏感，我担心如果训练数据发生变化，模型可能会停止工作，并且必须再次调整参数。如果模型发生变化，它也必须进行调整，例如添加更多层。
您以前是否遇到过类似的问题，是否有更好的方法？如何使模型更加稳健，以便具有次优正则化系数会稍微降低质量但不会破坏一切？]]></description>
      <guid>https://stats.stackexchange.com/questions/647056/select-autocorrelated-events</guid>
      <pubDate>Sat, 11 May 2024 22:35:15 GMT</pubDate>
    </item>
    <item>
      <title>使用深度集成量化预测不确定性：如何组合拉普拉斯分布？</title>
      <link>https://stats.stackexchange.com/questions/647054/quantifying-prediction-uncertainty-using-deep-ensembles-how-to-combine-laplace</link>
      <description><![CDATA[对于回归问题，我想训练深度神经网络的集合来预测标记的输出以及不确定性，类似于论文中提出的方法使用深度集成进行简单且可扩展的预测不确定性估计。作者使用高斯分布的负对数似然（NLL）作为损失函数，使模型隐式学习方差
$$
\ell_{\text{NLL}_G}(x, y, θ) =
-\log p_G\left(y \mid \mu_\theta, \sigma_\theta^2\right)=
\frac{\log \sigma_\theta^2}{2}+
\frac{\left(y-\mu_\theta\right)^2}
{2 \sigma_\theta^2}+
\text{常量}
$$
其中 $y$ 是目标，$\mu_\theta$ 是预测平均值， $\sigma^2_\theta$ 是预测方差。
但是，我注意到当使用平均误差而不是均方误差作为损失函数时，我的模型收敛得更好。因此我想利用拉普拉斯 NLL，它对异常值应该更加鲁棒：
$$
\ell_{\text{NLL}_L}(x, y, θ) =
-\log p_L\left(y \mid \mu_\theta, b_\th​​eta\right)=
\log b_\th​​eta +
\frac{\left|y-\mu_\theta\right|}{b_\th​​eta} +
\text{常量}
$$
在高斯情况下，作者将各个模型的预测均值和方差结合起来$m$：
$$
\mu_*=M^{-1} \sum_m \mu_{\theta_m} \\
\sigma_*^2=M^{-1} \sum_m\left(\sigma_{\theta_m}^2+\mu_{\theta_m}^2\right)-\mu_*^2
$$
计算混合拉普拉斯分布的均值和方差的正确方法是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/647054/quantifying-prediction-uncertainty-using-deep-ensembles-how-to-combine-laplace</guid>
      <pubDate>Sat, 11 May 2024 21:35:48 GMT</pubDate>
    </item>
    <item>
      <title>变压器中的编码器屏蔽</title>
      <link>https://stats.stackexchange.com/questions/647053/encoder-masking-in-transformers</link>
      <description><![CDATA[我正在尝试了解机器翻译中使用的标准变压器架构。在原来的“注意力就是你所需要的”中他们在论文中表示，屏蔽注意力仅用于 Transformer 的解码器部分，而不是编码器。然而，在互联网上，我看到很多在编码器和解码器中使用屏蔽的实现（例如 此处和此处） 。如果我理解正确， PyTorch 中的转换器模块正在启用屏蔽对于编码器、解码器甚至存储器。这是为什么？这是原论文发布后引入的东西吗？使用它有什么好处吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/647053/encoder-masking-in-transformers</guid>
      <pubDate>Sat, 11 May 2024 21:19:53 GMT</pubDate>
    </item>
    <item>
      <title>标量乘法与概率密度函数[关闭]</title>
      <link>https://stats.stackexchange.com/questions/647051/scalar-multiplication-with-probability-density-function</link>
      <description><![CDATA[我有一个问题，我需要一种通用方法来将标量与概率密度/质量函数相乘？ （我对连续案例更感兴趣 - 所以 pdf，但适用于 PMF 的解决方案也很好。）
概率分布函数之和相当于概率分布函数的卷积。同样，有没有办法将概率分布与标量/常数相乘？
pdf1 + pdf2 == 卷积(pdf1, pdf2)

所以，
pdf1 + pdf1 == 卷积(pdf1, pdf1)
对于整数值标量，一个简单的解决方案是进行重复加法。但有没有更快的方法呢？但更重要的是，是否有适用于浮点标量乘法的解决方案？
也就是说，我想做0.5 * pdf1等
&lt;小时/&gt;
就上下文而言，主要用例是成本估算。假设一个请求由两个子请求处理，每个子请求都有一个延迟分布 pdf1 和 pdf2。如果这两者连续发生，那么总请求将遵循 pdf1 + pdf2 == convolve(pdf1, pdf2)。
但是，如果 70% 的请求由 pdf1 处理，30% 的请求使用 pdf2，则总预期延迟将为 0.7*pdf1 + 0.3*pdf2。
任何有关如何执行此操作的指导都会有所帮助。
（与 numpy/scipy 一起使用的解决方案更好）]]></description>
      <guid>https://stats.stackexchange.com/questions/647051/scalar-multiplication-with-probability-density-function</guid>
      <pubDate>Sat, 11 May 2024 20:36:31 GMT</pubDate>
    </item>
    <item>
      <title>小数据集的增强[重复]</title>
      <link>https://stats.stackexchange.com/questions/647046/augmentation-for-small-dataset</link>
      <description><![CDATA[0
我有一个非常小的数据集，因此它的性能非常差（65,20）。如何扩大数据集？我将数据集乘以一系列数字并将其输入到主数据集中（大约7倍），模型的性能得到了提高，但无论我在缩放器中放入多少，数据集都不正常。有谁知道该怎么办吗？
我想要的是能够以我拥有最低轨道的方式增加数据，有人有想法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/647046/augmentation-for-small-dataset</guid>
      <pubDate>Sat, 11 May 2024 18:17:13 GMT</pubDate>
    </item>
    <item>
      <title>公寓价格数据集：为什么系数符号不同，但以其他值为条件时却不同？</title>
      <link>https://stats.stackexchange.com/questions/647044/apartment-price-dataset-why-are-the-coefficient-signs-different-but-not-when-co</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/647044/apartment-price-dataset-why-are-the-coefficient-signs-different-but-not-when-co</guid>
      <pubDate>Sat, 11 May 2024 17:48:03 GMT</pubDate>
    </item>
    <item>
      <title>我计算格兰杰因果关系的方法有效吗？</title>
      <link>https://stats.stackexchange.com/questions/647031/is-my-approach-to-compute-granger-causality-valid</link>
      <description><![CDATA[我有两个时间序列，我们称它们为 A（红色）和 B（蓝色）。每个时间序列大约有 770 个数据点。
注意：这两个时间序列实际上都不是记录的原始信号，而是滑动窗口方法的结果。我对两个原始的原始时间序列应用了完全相同的滑动窗口方法，然后计算每个窗口的测量值。这使我能够比较两个信号的测量的时间演变或结构。
问题：虽然两个原始时间序列是平稳的，但滑动窗口方法的结果却不是。从图中可以很容易地看出，滑动窗口时间序列是非平稳的（它们当然无法通过 ADF 和 KPSS 测试）。
目标：我想计算 B（蓝色）格兰杰是否导致 A（红色），因此应用格兰杰因果检验。现在，如果在两个基于滑动窗口的时间序列之间应用格兰杰因果关系在概念上有意义的话，让我们转移这个问题。让我们假设这两个时间序列都源于非平稳原始信号。我遵循 Toda 和 Yamamoto (1995) 的方法来计算 Granger 因果关系，如 Python 链接中所述：
https://rishi-a.github.io/2020/ 05/25/granger-causality.html
这是一个我想测试格兰杰因果关系的图。显示的 p 值 p &lt; 0.001 是在获取每个时间序列的第一个差值后计算的，使它们静止。差分后的 p 值源自 ssr_chi2test 检验，最佳滞后基于 BIC 标准。 （事实上​​，即使没有差异，p 值仍然是 p &lt; 0.001。）差异时间序列通过了 ADF 和 KPSS 测试，而且看起来也完全平稳。

这是对两个时间序列进行差分后的相同图（一阶差分）。正是在这些时间序列上，按照上面链接的所有步骤执行格兰杰因果关系检验。

问题：我有几个问题。

鉴于时间序列基于滑动窗口，手动选择滞后 1 是否有效（同时假设 B 对 A 的影响几乎没有时间滞后）？&lt; /p&gt;

在测试格兰杰因果关系之前，由于其极端不稳定的时间结构，对两个时间序列进行差分以使其平稳是绝对必要的，或者由于非平稳时间序列，在不进行差分的情况下计算格兰杰因果关系是否有效反映测量的真实时间演变？

基于对原始（无差异）时间序列的目视检查，您是否愿意推荐一种与我所关注的链接中描述的方法完全不同的方法来计算格兰杰因果关系？


作为格兰杰因果关系和 VAR 模型的初学者，我通常想知道我引用的链接中的方法和代码（https://rishi-a.github.io/2020/05/25/granger-causality.html）通常适合我的具体数据，或者如果我正在做一些确实有缺陷的事情。]]></description>
      <guid>https://stats.stackexchange.com/questions/647031/is-my-approach-to-compute-granger-causality-valid</guid>
      <pubDate>Sat, 11 May 2024 14:00:36 GMT</pubDate>
    </item>
    <item>
      <title>生物神经网络信息传递中的傅里叶变换</title>
      <link>https://stats.stackexchange.com/questions/647024/fourier-transform-in-information-transfer-in-biological-neural-network</link>
      <description><![CDATA[神经设计原理作者：Peter Sterling 和 Simon Laughlin描述了信息论在计算大脑中信息传输速率时的用法。
&lt;块引用&gt;
...当连续信号状态不相关时...信息速率为 I = R \log2 (1 + S/N) 位每秒。该方程假设冗余为零，即信号状态之间不存在相关性。为了实现这一点，信号状态必须随机改变。为了真正随机，信号必须能够从任何状态跳到任何其他状态。但这种能力受到跳跃所需时间的限制...香农通过使用傅里叶变换将连续的模拟信号和噪声转换为其频率分量来解决这个问题...由此可见信号所携带的总信息是其每个分量频率所携带的信息的总和。 I = int(0-co)log2[1 + S(f) / N(f)] * df

所有状态都可以在一定时间内到达。为什么应用傅立叶变换可以解决这个问题？]]></description>
      <guid>https://stats.stackexchange.com/questions/647024/fourier-transform-in-information-transfer-in-biological-neural-network</guid>
      <pubDate>Sat, 11 May 2024 11:02:37 GMT</pubDate>
    </item>
    <item>
      <title>“随机变量的置信区间”是不正确的术语吗？</title>
      <link>https://stats.stackexchange.com/questions/647022/is-it-incorrect-terminology-to-say-confidence-interval-of-a-random-variable</link>
      <description><![CDATA[我见过“总体参数不是随机变量”的说法。在讨论置信区间时。
例如此处
&lt;块引用&gt;
请务必注意，总体参数不是随机变量。

在频率论解释的背景下，我毫不犹豫地接受这一说法。根据这种解释，总体参数是固定的，但未知。它们不是随机变量。
但是术语置信区间是否也带有特定的解释？或者它只是数学函数（如 wiki 所示）：
&lt;块引用&gt;


例如，假设我认为 𝜃（总体参数）是随机变量，X 是随机样本。考虑到它们的联合分布，我计算函数：
P( u(x) &lt; 𝜃 &lt; v(x)) = c ∀ 𝜃
看起来像下图中的红线：

（图借自此处）
现在，我将红线称为“随机变量的置信区间”是错误的吗？ 𝜃？]]></description>
      <guid>https://stats.stackexchange.com/questions/647022/is-it-incorrect-terminology-to-say-confidence-interval-of-a-random-variable</guid>
      <pubDate>Sat, 11 May 2024 10:12:44 GMT</pubDate>
    </item>
    <item>
      <title>任何点过程都可以细化为泊松点过程吗？</title>
      <link>https://stats.stackexchange.com/questions/646958/can-any-point-process-be-thinned-into-a-poisson-point-process</link>
      <description><![CDATA[以下情况是否正确？对于任何点过程 $X$ 都存在一个过程  $\mathcal{P}$ 和强度 $\lambda &gt; 的泊松点过程 $Y$ 0$ 这样从 $X$ 中提取实现并应用 $\mathcal{P} 的过程$ 与从 $Y$ 获取实现的过程没有区别。
请注意，我只是询问 $\mathcal{P}$ 和 $Y$ 是否存在span&gt;，而不是 $\mathcal{P}$ 是否实用。
我看过 Møller &amp; Schoenberg (2010)，特别是 Schoenberg (2009)，试图看看是否以及如何从 $\mathcal{P}$ 的存在。 math-container&quot;&gt;$\mathcal{P}$ 实用。我还回顾了有关点过程独立细化的文献。
参考文献
Møller, J., &amp;勋伯格，F.P. (2010)。 将空间点过程细化为泊松过程。 应用概率的进展，42(2), 347–358。
勋伯格，F.P. (2009)。 使用 Papangelou 强度将空间点过程细化为泊松过程.]]></description>
      <guid>https://stats.stackexchange.com/questions/646958/can-any-point-process-be-thinned-into-a-poisson-point-process</guid>
      <pubDate>Fri, 10 May 2024 04:52:12 GMT</pubDate>
    </item>
    <item>
      <title>最大似然法是否必须应用于简单随机样本或实现？</title>
      <link>https://stats.stackexchange.com/questions/646815/must-maximum-likelihood-method-be-applied-on-a-simple-random-sample-or-on-a-real</link>
      <description><![CDATA[我想我的麻烦不是一个大问题，但问题是：当一个人应用最大似然时，他考虑实现$(x_1, \dots, x_n)$ 简单随机样本 (SRS)，从而得出 ML估计。但如果有人想谈论偏差、一致性等问题，就必须提到估计器，对吗？
例如，在这个维基百科示例中，考虑正常示例，他们最终得到 $\hat{\mu}=\bar{x}$ ，然后写入 $\mathbb{E }(\hat{\mu})=\mu$ 这可以说有点草率，因为获取确定性数量的期望值并没有多大意义。
因此，我的问题是为什么通常（我的意思是，大多数来源：维基百科、教科书等）MLE 是基于实际数据（即基于 SRS 的实现）而不是基于 SRS 构建的（也就是说，随机变量的集合 $(X_1, \dots, X_n)$)，提供了计算期望值等有意义的估计器？我想这是一种虚假的微妙之处，没有什么实际意义，因为它足以“大写字母化”。估计得到相应的估计器，但我还是想问。]]></description>
      <guid>https://stats.stackexchange.com/questions/646815/must-maximum-likelihood-method-be-applied-on-a-simple-random-sample-or-on-a-real</guid>
      <pubDate>Wed, 08 May 2024 12:55:10 GMT</pubDate>
    </item>
    </channel>
</rss>