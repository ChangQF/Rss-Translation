<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sun, 02 Jun 2024 12:24:45 GMT</lastBuildDate>
    <item>
      <title>布伦特原油价格季节性调整</title>
      <link>https://stats.stackexchange.com/questions/648472/seasonal-adjustment-of-brent-crude-oil-price</link>
      <description><![CDATA[我需要使用布伦特原油的全球价格（月度数据；1990 年 1 月，2024 年 4 月）
数据集未经季节性调整。我分解了趋势和季节性，如下图所示。
我需要做季节性调整吗？如果是，我该如何在 R 中处理这种季节性调整？
谢谢。
]]></description>
      <guid>https://stats.stackexchange.com/questions/648472/seasonal-adjustment-of-brent-crude-oil-price</guid>
      <pubDate>Sun, 02 Jun 2024 12:06:40 GMT</pubDate>
    </item>
    <item>
      <title>将离散生存时间转换为连续生存时间</title>
      <link>https://stats.stackexchange.com/questions/648471/converting-discreate-survival-time-to-continuous-one</link>
      <description><![CDATA[我记得几年前有位科学家告诉我，我们可以根据以下示例方法将流行病学中的离散生存时间转换为连续生存时间。如果您能告诉我我们是否可以确认此方法（如果可以，请告诉我该技术的名称），我将不胜感激。
假设，我有一项研究，患者每三个月来医院检查一次。T0（手术前一周）、T1（手术后三个月）、T2（手术后 6 个月）直至 T8（手术后 21 个月）。如果患者 1 在 3 到 6 个月（T2）的间隔内发生事件，那么我们可以将生存时间转换为 3+1.5=4.5。如果患者 2 在 6 到 9 个月（T2）的间隔内发生事件，那么我们可以将生存时间转换为 6+1.5=7.5]]></description>
      <guid>https://stats.stackexchange.com/questions/648471/converting-discreate-survival-time-to-continuous-one</guid>
      <pubDate>Sun, 02 Jun 2024 11:42:26 GMT</pubDate>
    </item>
    <item>
      <title>在没有外生价格变化的情况下估计需求函数</title>
      <link>https://stats.stackexchange.com/questions/648470/estimating-demand-function-without-exogenous-variation-in-prices</link>
      <description><![CDATA[假设我拥有关于保险产品需求和价格的个人层面信息。如果消费者 i 购买了保险产品，则需求 D_i 为虚拟变量，等于 1，否则为 0。这是一次性决定，因此我没有面板数据（不过，我观察了不同的群体）。现在假设 i 的定价规则如下：
p_i=b+f(risk_i)，
其中 b 是某个在个体之间相等的基准价格，而 f(risk_i) 是影响风险的可观察个体特征的函数，并且假定计量经济学家知道该函数。
目标是估算标准需求曲线：
D_i=a+β*p_i+ε_i&lt;​​/code&gt;。
我面临的问题是 p_i 的变化不是外生于个体需求特征的（即，可观察风险特征中的支付意愿可能会增加），但如果我估算 D_i=a+β_1*b+β_2*f(risk_i)+ε_i&lt;​​/code&gt;，我缺乏价格。
不幸的是，我无法想出解决这个问题的方法。IV 方法是不可行的，因为定价没有随机成分（例如，取决于保险员工等）。我想，即使假设 f() 是非线性的，估计 D_i=a+β 1*p_i+β_2*risk+ε_i&lt;​​/code&gt; 也会不一致。
另一方面，定价规则是在 t 年引入的，没有可靠的个人预期。在此之前，每个 i 都面临着一个对所有个人都相同的通用时不变价格。但是，鉴于购买保险产品是一次性决定，我每年都会观察不同的人。
很高兴收到有关如何在这种情况下估计需求曲线的任何建议！]]></description>
      <guid>https://stats.stackexchange.com/questions/648470/estimating-demand-function-without-exogenous-variation-in-prices</guid>
      <pubDate>Sun, 02 Jun 2024 11:00:21 GMT</pubDate>
    </item>
    <item>
      <title>如何将“最大值”作为未知值来规范化自定义范围？</title>
      <link>https://stats.stackexchange.com/questions/648467/how-to-normalize-a-custom-range-with-max-being-the-unknown</link>
      <description><![CDATA[要对范围进行归一化，我们应用以下变换：
$$
\frac{x_i - \min(x)}{\max(x) - \min(x)}
$$
要将归一化调整为首选范围，其中 b=max_target 和 a=min_target：
$$
\text{norm}_i \cdot (b - a) + a
$$
如果我们希望调整后的范围之和等于 1，b 将是一个未知数，求解如下：
$$
\left( \frac{1 - n \cdot a}{\sum \text{norm}_i} \right) + a
$$
但是，当 n &gt; 9 时，查找 b 会失败。例如，假设我想规范化一个分布，其中 b=x 和 a=0.1：



标签
频率
adj_norm




1
7
0.35


2
10
0.55


3
3
0.10






标签
频率
adj_norm




1
7
0.09...


2
&lt; td&gt;10
0.08...


3
3
0.09...


4
5
0.09...


5
9
0.08... 


6
4
0.09...


7
6
0.09...


8
8
0.08...


9 
1
0.10


10
2
0.09...


11
20
0.07...



请注意，在第一个表中，频率最低的标签被正确识别为最小值，而频率最高的标签具有正确的最大值，使得数组等于 1。但是，在第二个表中，最大值被错误分类...
关于如何更正 n &gt; 9 的公式，您有什么想法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648467/how-to-normalize-a-custom-range-with-max-being-the-unknown</guid>
      <pubDate>Sun, 02 Jun 2024 06:14:01 GMT</pubDate>
    </item>
    <item>
      <title>对顺序数据的依赖性是什么？</title>
      <link>https://stats.stackexchange.com/questions/648464/what-is-dependency-on-sequential-data</link>
      <description><![CDATA[我们知道，只要数据集中的点依赖于数据集中的其他点，数据就被称为顺序数据。
一个常见的例子是时间序列，例如天气数据。

我的问题是“依赖”一词是什么意思？在上面的例子中，数据点如何相互依赖？]]></description>
      <guid>https://stats.stackexchange.com/questions/648464/what-is-dependency-on-sequential-data</guid>
      <pubDate>Sun, 02 Jun 2024 05:33:05 GMT</pubDate>
    </item>
    <item>
      <title>使用预测值置信区间来“预测”结果</title>
      <link>https://stats.stackexchange.com/questions/648463/using-predictive-value-confidence-intervals-to-predict-outcomes</link>
      <description><![CDATA[以下是简要版本：
假设我有一个混淆矩阵，其中包含以下数据，这些数据基于预测试的熟练程度分数线和课程结果（通过/不通过）。
TP：550 FP：200
FN：280 TN：825
以下是一些统计数据：
敏感度：66.27%
1 - 特异性：19.51%
准确度：74.12%
阳性预测值 (PPV；贝叶斯理论)：73.33% 95% 可信区间 (70.64%，75.86%)
1 - 阴性预测值 (1 - NPV)：25.34% 95% 可信区间 (23.49%， 27.28%)
是否可以将 PPV 置信区间中的最小值和最大值与 1 - NPV 置信区间中的最小值和最大值结合使用，根据预测试的结果“预测”某个班级中一组学生通过考试的概率范围？目标是了解预测试结果对学生在课堂上的成功有多“预测”。
例如，根据我们的熟练程度分数线，开学第一天 75% 的学生在预测试中获得了“熟练”的成绩。如果过去的数据（如混淆矩阵所示）表明，如果学生在预测试中表现良好，则通过该课程的概率 (PPV) 为 73.33%，95% CI（70.64%，75.86%），如果学生在预测试中表现不佳，则通过该课程的概率 (1 - NPV) 为 25.34%，95% CI（23.49%，27.28%），我们能否使用两个置信区间的最小值和最大值来表示：
0.7064 * 75 + 0.2349 * 25 [两个置信区间的最小值] = 58.9%
0.7586 * 75 + 0.2728 * 25 [两个置信区间的最大值] = 63.7%
我们预计当前学生群体中会有 58.9% 到 63.7%通过这门课程？
这实际上是使用贝叶斯定理/条件概率，使用过去收集的预测试和课程通过率数据来回答我们的问题。使用置信区间而不是实际条件概率 73.33% 和 25.34% 来做到这一点有问题吗？当然，这里的“预测”是笼统的。我们知道现实世界总是有一些有趣的事情在等着我们。 （旁注：我们将来会添加一门必修课程，希望能随着时间的推移提高通过率。）一旦有数据可供在学习本课程之前修读必修课程的学生使用，比较必修课程与无必修课程组的数据、两组在预考中的分数以及课程的通过/不通过率等，看看课程结果实际上如何随时间变化将会很有趣，但就目前而言……
根据过去的数据，根据该课程在预考中的表现，我们是否可以进行上述计算并说，根据该课程在预考中的表现，我们预计当前这组学生中 58.9% 至 63.7% 的学生会通过该课程？]]></description>
      <guid>https://stats.stackexchange.com/questions/648463/using-predictive-value-confidence-intervals-to-predict-outcomes</guid>
      <pubDate>Sun, 02 Jun 2024 04:55:58 GMT</pubDate>
    </item>
    <item>
      <title>按离散的、不相关的属性进行聚类？</title>
      <link>https://stats.stackexchange.com/questions/648455/clustering-by-discrete-unrelated-properties</link>
      <description><![CDATA[我有大量具有不相关属性的对象，例如

color=yellow
material=stone
is_important=true
等等。

这些属性基本上是随机的，只要用户认为有就好。没有实际的方法可以将这些属性强制转换为具有欧几里得距离的类似物。
我认为，如果不存在某种距离度量，那么从概念上讲，考虑通过这些属性对对象进行聚类是没有意义的（这是正确的吗？），但我希望存在可以产生类似输出的东西
&quot;90% 的 color=yellow 对象也有 material=plastic 和 is_important=false&quot;
除了强制通过数据集之外，还有其他东西可以进行此类分析吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648455/clustering-by-discrete-unrelated-properties</guid>
      <pubDate>Sun, 02 Jun 2024 00:19:01 GMT</pubDate>
    </item>
    <item>
      <title>量化（观察到的）空间聚类？</title>
      <link>https://stats.stackexchange.com/questions/648453/quantifying-observed-spatial-clustering</link>
      <description><![CDATA[我正在寻找一些有关空间统计的建议。
我有一个大型数据集，其中包含 5 种不同条件下的多个样本。每个样本由 2D 空间中的不同点类型组成。例如：
样本 1，条件 1：
X，Y，类型
1，1，A
5，3，A
11，2，B
6，22，L
...

我们注意到，对于某些条件，某些点类型（我们分别称它们为 A 和 B，红色和蓝色）显示不同的分布。在大多数情况下（“通常模式”），A 类与其他 A 类点聚类，B 类与其他 B 类点聚类；每种类型的簇在视觉上彼此不同（参见图像，右侧子图，“通常模式”）。但是，在某些条件下，我们注意到 A 和 B 的簇彼此混合更多； As 可能仍然与 As 聚类（多于 Bs），但 As 和 Bs 的聚类/合并在一起（见图像，左侧子图，“观察到的模式”）。
捕捉和量化这种观察到的现象的最佳方法是什么？（完整披露：我是空间统计的新手）。

到目前为止，我们已经研究了不同条件之间的香农熵（跨所有点类型，例如 A-M），以检查在感兴趣的条件下点类型分布是否存在更多异质性（但似乎没有显着差异）。我们还研究了中心性度量以及邻域丰富度（我们对所有点类型，即 A-M 进行了这些研究）。最近，我们尝试了 DBSCAN（仅关注点类型 A 和 B），希望能够证明结果簇大小存在差异（尽管我个人不确定这是否是解决问题的正确方法）。我们可能尚未令人满意地调整模型参数，但到目前为止，它似乎还没有解决问题。我还担心，虽然它可能有助于探索性数据分析，但我们不太可能使用 DBSCAN 输出来有意义地比较不同条件下的结果。
直观地讲，我认为答案可能在于网络分析和研究几个节点之外的聚类行为（而不是仅仅研究直接邻居，这可能是邻域富集分析对我们不起作用的原因）。
展示和量化观察到的聚类现象的最佳方法是什么？理想情况下，我们需要一种允许比较不同条件的方法。]]></description>
      <guid>https://stats.stackexchange.com/questions/648453/quantifying-observed-spatial-clustering</guid>
      <pubDate>Sat, 01 Jun 2024 23:50:11 GMT</pubDate>
    </item>
    <item>
      <title>我们可以使用 CDF 而不是 PDF 来表达无意识统计学家的定律吗？</title>
      <link>https://stats.stackexchange.com/questions/648452/can-we-express-the-law-of-the-unconscious-statistician-using-the-cdf-instead-of</link>
      <description><![CDATA[我只见过LOTUS以密度形式给出
$$\mathbb{E}[g(X)] = \int g(x) f(x) dx$$
或以勒贝格-斯蒂尔杰斯积分形式给出
$$\mathbb{E}[g(X)] = \int g(x) dF_X(x).$$
我还见过这篇文章，其中展示了如何使用 CDF 而不是PDF：

与计算这些矩类似，有没有办法使用 CDF 而不是 PDF 来编写 LOTUS？]]></description>
      <guid>https://stats.stackexchange.com/questions/648452/can-we-express-the-law-of-the-unconscious-statistician-using-the-cdf-instead-of</guid>
      <pubDate>Sat, 01 Jun 2024 22:37:36 GMT</pubDate>
    </item>
    <item>
      <title>两组时间序列之间的假设检验</title>
      <link>https://stats.stackexchange.com/questions/648405/hypothesis-testing-between-two-sets-of-time-series</link>
      <description><![CDATA[我想通过比较在不同时间捕获的两组时间序列来验证捕获方法的可重复性。
让我们考虑两组时间序列$A=(tsa_1,\dots,tsa_n)$和$B=(tsb_1,\dots,tsb_m)$，其中$n,m &gt;&gt; 100$。
我们还定义$H_0$=两个数据集彼此一致，并且$H_a$=两个数据集显示出显着差异。
测试这些假设的经典方法是什么？
我想出了基于距离的方法，但我想探索统计方法来进行比较。我读过关于统计测试的文章，但找不到太多关于处理时间序列的信息。任何帮助或资源指导都值得赞赏。
为了论证的目的，我们可以考虑类似于下图的数据。这些是围绕预定义模式用正常噪声生成的合成数据，但实际捕获的数据预计具有相同的属性。
]]></description>
      <guid>https://stats.stackexchange.com/questions/648405/hypothesis-testing-between-two-sets-of-time-series</guid>
      <pubDate>Fri, 31 May 2024 18:00:19 GMT</pubDate>
    </item>
    <item>
      <title>条件 Logit 模型 - 效用结构估计 - 元分析</title>
      <link>https://stats.stackexchange.com/questions/648401/conditional-logit-model-utility-structural-estimation-meta-analysis</link>
      <description><![CDATA[我正在使用 McFadden (2001) 框架（参见参考资料）对多个数据库（来自不同的文章）中的效用函数进行结构估计。
每篇文章的数据库包含 N 个主题、J 个选项，从而给出 J x N 行。每个主题都选择了 J 个选项中的一个。数据还包括一些特定于选择的特征。
使用此数据结构，我通过条件逻辑模型 (CLM) 以两种方式估计效用参数：
一般估计：我对整个数据集运行了一个唯一的 CLM。请注意，J 可以在不同文章中变化
逐篇文章估计：我对每篇文章运行一个 CLM，并对结果估计进行平均
但是，这两种方法给出的结果截然不同。
有谁知道哪种程序（如果有的话）最适合提供这些参数的元估计？
谢谢！
参考文献：McFadden，D.（2001）。经济选择。美国经济评论，91(3)，351-378。]]></description>
      <guid>https://stats.stackexchange.com/questions/648401/conditional-logit-model-utility-structural-estimation-meta-analysis</guid>
      <pubDate>Fri, 31 May 2024 16:58:04 GMT</pubDate>
    </item>
    <item>
      <title>随机效应模型：理解与、之间和上下文效应</title>
      <link>https://stats.stackexchange.com/questions/648373/random-effect-models-understanding-with-between-and-contextual-effect</link>
      <description><![CDATA[为了更好地理解和更有效地应用随机/固定效应和聚类标准误差，我一直在努力理解“内部”、“之间”和“上下文”效应这些术语。

据我所知，我们只能在没有上下文效应的情况下应用随机效应模型 - 否则随机效应假设不成立。对吗？这是否也意味着在这种情况下，组内和组间效应是相同的？

请看以下示例：



数据：汇总的横断面调查数据，其中各个受访者嵌套在各个国家/地区，并且调查多年来在每个国家/地区进行多次
研究问题旨在找出国家级变量（例如 GDP）是否影响受访者回答有关生活满意度的问题的方式
感兴趣的影响：GDP 对生活满意度的总体影响

我感兴趣的是哪种影响：组内、组间还是上下文？这样的研究是否需要随机效应模型？或者聚类标准误差就足够了？]]></description>
      <guid>https://stats.stackexchange.com/questions/648373/random-effect-models-understanding-with-between-and-contextual-effect</guid>
      <pubDate>Fri, 31 May 2024 12:58:22 GMT</pubDate>
    </item>
    <item>
      <title>在绘制平滑缩放的 Schoenfeld 残差图时，R 中的 transform="" 到底起什么作用？</title>
      <link>https://stats.stackexchange.com/questions/648370/what-does-transform-in-r-exactly-do-when-plotting-the-smoothed-scaled-schoenf</link>
      <description><![CDATA[编辑：“我的问题已被关闭，因为它似乎不属于这里，因为它与统计无关。我不同意这个决定，感谢@J-J-J 在这方面对我的支持。我会尝试澄清，我想从统计的角度而不是编码的角度来理解以下问题。我有 4 个问题。我删除了第四个，因为那是唯一一个与编码有关的问题。所以这是我的责任，但我希望有人能帮助我理解其他三个问题。提前谢谢您！”
正如标题所示，我旨在解决 R 中的参数 transform = &#39;&#39;，特别是关于用于评估比例风险 (PH) 假设的平滑缩放 Schoenfeld 残差检验/图。
我知道默认是 transform = &#39;km&#39;，但还有其他可用选项：
transform=&#39;log&#39;
transform=&#39;rank&#39;
transform=&#39;identity&#39;

我相信我了解每个选项的含义。如果我的理解不正确，请纠正我：
log 用于强调早期事件，因为它为它们分配了更大的权重。
rank 用于突出显示晚期事件，因为它为它们分配了更大的权重。
identity 表示未经任何修改的原始时间数据。
但是，我不确定 km 到底是做什么的。虽然它似乎可以缩短时间，但我不确定它的含义。
我的问题：

我应该何时使用每个转换来评估 R 中的 cox.zph() 测试/绘图？（编辑：从统计角度来看，我应该在什么情况下使用它？）

它们为 PH 假设产生不同的 p 值。根据所选的转换，我应该信任哪个 p 值？（编辑：再次从统计角度）

虽然我理解 transform = &#39;&#39; 参数的总体目的，但我仍然不确定在哪些情况下应该选择每个参数。（编辑：从统计角度来看，有人可以澄清何时使用每个转换吗？）

]]></description>
      <guid>https://stats.stackexchange.com/questions/648370/what-does-transform-in-r-exactly-do-when-plotting-the-smoothed-scaled-schoenf</guid>
      <pubDate>Fri, 31 May 2024 12:11:26 GMT</pubDate>
    </item>
    <item>
      <title>小样本与大样本的因果效应和外生性</title>
      <link>https://stats.stackexchange.com/questions/648335/causal-effect-and-exogeneity-in-small-samples-vs-in-large-samples</link>
      <description><![CDATA[这个标题下有两个问题。
在 Y 对 X 和控制 Z 的回归中，随机误差为 e，

在小样本中，如果满足严格外生性，即 E(e|X,Z)=0，是否意味着 X 的系数具有因果关系的意义？
在大样本中，严格外生性的条件是否可以放宽为不相关，即 Cov(e,X)=0 和 Cov(e,Z)=0，以描述因果关系？如果答案是肯定的，请用公式推导原因。

我发现很多书和论文（例如关于 IV）讨论的是相关性而不是外生性。我查过一些教科书，它们只解释了从小样本的无偏变为大样本的一致性的性质，这与因果关系无关。]]></description>
      <guid>https://stats.stackexchange.com/questions/648335/causal-effect-and-exogeneity-in-small-samples-vs-in-large-samples</guid>
      <pubDate>Fri, 31 May 2024 03:26:25 GMT</pubDate>
    </item>
    <item>
      <title>添加混杂因素“因子”与减去层内平均值</title>
      <link>https://stats.stackexchange.com/questions/648262/adding-a-confounder-factor-vs-subtracting-within-level-mean</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/648262/adding-a-confounder-factor-vs-subtracting-within-level-mean</guid>
      <pubDate>Thu, 30 May 2024 07:17:02 GMT</pubDate>
    </item>
    </channel>
</rss>