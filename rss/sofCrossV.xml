<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 07 Sep 2024 18:19:37 GMT</lastBuildDate>
    <item>
      <title>提高 glmm 准确性 - 我能做什么？</title>
      <link>https://stats.stackexchange.com/questions/654010/improving-glmm-accuracy-what-can-i-do-here</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/654010/improving-glmm-accuracy-what-can-i-do-here</guid>
      <pubDate>Sat, 07 Sep 2024 18:16:33 GMT</pubDate>
    </item>
    <item>
      <title>使用蒙特卡罗实验验证 HAC SE 的一致性</title>
      <link>https://stats.stackexchange.com/questions/654007/verifying-consistency-of-hac-ses-with-a-monte-carlo-experiment</link>
      <description><![CDATA[我试图通过蒙特卡洛实验证明 R 的 sandwhich 包中给出的默认 HAC 标准误差的一致性。我使用的线性模型定义为 $Y = X + \varepsilon$，其中 $X_i \sim N(1,1)$，$X\perp\varepsilon$，以及 $\varepsilon \sim N(\mathbf 0, \Sigma)$，其中
$ \boldsymbol\Sigma_{ij} = \rho^{|i-j|}$，其中 $\rho \in (-1,1)$。如果 $\hat{\beta}$ 是 OLS 估计量，我们可以将其渐近方差简化如下：
$$ \text{Avar}(\hat {\beta}) = \text{E}[X&#39;X]^{-1}X&#39;\Sigma X(X&#39;X)^{-1} = \frac{1}{2n(1-\rho)} - \frac{\rho(1-\rho^n)}{2n^2(1-\rho)^2}.$$
我已经通过以下蒙特卡洛实验验证了这个数学公式：
library(mvnfast)
library(tidyverse)
library(sandwich)

set_Sigma_ij &lt;- function(i, j, rho){
rho^abs(i-j)
}

generate_Sigma &lt;- function(rho, n){
expand_grid(i = 1:n, j = 1:n) %$% 
map2_dbl(i, j, \(i,j) set_Sigma_ij(i,j, rho)) %&gt;% 
matrix(nrow = n) 
}

draw_ols_realization &lt;- function(n, rho){
eps &lt;- rmvn(
n = 1, 
mu = rep(0, n), 
sigma = generate_Sigma(rho = rho, n = n)
) %&gt;% 
as.numeric()
X &lt;- rnorm(n = n, mean = 1, sd = 1)
Y &lt;- X + eps
lm(Y ~ X - 1) %&gt;% 
coef()
}

true_avar_ols &lt;- function(n, rho){
(1/(2*n*(1-rho))) - (rho*(1-rho^n)) / (2*n^2 * (1-rho)^2)
}

numerical_avar_ols &lt;- function(N, n, rho){
1:N %&gt;% 
map_dbl(\(t) draw_ols_realization(n, rho)) %&gt;% 
var()
}

# 插入几个值以确认（对于足够大的 n）
true_avar_ols(n = 140, rho = 0.8)
numerical_avar_ols(N = 1e4, n = 140, rho = 0.8)

当我确认 HAC 标准误差（或者更确切地说是相应的协方差矩阵）始终估计 $\text{Avar}(\hat {\beta})$ 时，结果没有多大意义。这是我的代码：
estimate_avar_beta &lt;- function(n, rho, X, Y){
estimate_model &lt;- lm(Y[1:n] ~ X[1:n] - 1) 
tibble(
standard = as.numeric(vcov(estimated_model)),
HAC = as.numeric(vcovHAC(estimated_model)),
sample_size = n
)
}

# 估计固定 (Y, X) 的 Avar(OLS)，同时让 n 增长
estimate_avar_beta_over_n &lt;- function(n_vals, rho){
eps &lt;- rmvn(
n = 1, 
mu = rep(0, max(n_vals)), 
sigma = generate_Sigma(rho = rho, n = max(n_vals))
) %&gt;% 
as.numeric()
X &lt;- rnorm(n = max(n_vals), mean = 1, sd = 1)
Y &lt;- X + eps
n_vals %&gt;% 
map_df(\(n) HAC_SEs(n, rho, X, Y))
}

# 现在执行此操作 N 次 
draw_N_estimates_avar_beta_over_n &lt;- function(N, n_vals, rho){
1:N %&gt;% 
map_df(\(t) draw_HAC_SEs_over_n(n_vals, rho))
}

结果 &lt;- draw_N_estimates_avar_beta_over_n(
N = 1e2, 
n_vals = (1:10)*100, 
rho = 0.9
)

vcovHAC 给出的估计值似乎不一致。
我在这里做错了吗？如果是，是什么？ n = 1000 的样本量是否太小？我对 $\text{Avar}(\hat {\beta})$ 的表达是否不正确？我是不是犯了一个愚蠢的错误，盯着这个看了一个多小时后才发现？提前谢谢大家！]]></description>
      <guid>https://stats.stackexchange.com/questions/654007/verifying-consistency-of-hac-ses-with-a-monte-carlo-experiment</guid>
      <pubDate>Sat, 07 Sep 2024 16:51:28 GMT</pubDate>
    </item>
    <item>
      <title>PCA 有助于选择变量吗？</title>
      <link>https://stats.stackexchange.com/questions/654004/pca-to-help-select-variables</link>
      <description><![CDATA[我考虑在预处理步骤中运行 PCA，然后再运行另一个 PCA，该 PCA 实际上将用于分析子采样数据。我认为对原始变量集进行子采样可能很有用，方法是选择对预处理 PCA 的最佳成分贡献最大的变量，然后进行第二次 PCA 运行，这次分析（主要是可视化）贡献最大的变量（数量相当多）的数据子集。在我看来，对最佳 PC 贡献最大的变量，对高维数据向量的方差贡献最大。因此，它们能够从原始数据中捕获更多信息。因此它们更具信息量。
这有意义吗？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/654004/pca-to-help-select-variables</guid>
      <pubDate>Sat, 07 Sep 2024 15:26:48 GMT</pubDate>
    </item>
    <item>
      <title>分析类型</title>
      <link>https://stats.stackexchange.com/questions/654003/type-of-analysis</link>
      <description><![CDATA[我需要你的建议。我已经绞尽脑汁一个星期了，但尽管进行了详尽的互联网研究，我还是没有得出任何新的见解。
我所做的：我从下面列出的两个独立但在各方面都同质的研究（研究 1、研究 2）中提取了总结结果，然后使用 Welch ANOVA 比较了汇总数据。请注意，两项研究中的对照完全相同。
研究 1



独立变量
样本大小
平均值
标准差
正态性




对照
10
0.200
0.040
是


探测a
10
0.150
0.020
是


探测 b
10
0.170
0.050
是


探测 c
10
0.230
0.030
是



研究 2



独立变量
样本大小
平均值
标准差
正态性




控制
10
0.200
0.040
是


探测 d
10
0.180
0.060
是


探测e
10
0.240
0.050
是


探测 f
10
0.230
0.090
是



汇总数据（研究 1 + 研究 2）



独立变量
样本大小
平均值
标准偏差
正常




控制
10
0.200
0.040
是


探测 a
10
0.150
0.020
是


探测b
10
0.170
0.050
是


探测 d
10
0.180
0.060
是


探测 e
10
0.240
0.050
是



我会将这种方法归类为汇集数据分析和网络荟萃分析之间的某个类别。但是，我在互联网上找不到类似的方法，我需要证明我的方法。
我的想法：
网络荟萃分析：
优点：首先，同质研究设计和方法支持传递性假设，这有利于像网络荟萃分析一样直接比较探针。其次，加权平均值（尽管出于异方差的原因，因为样本量相等）是连续结果变量荟萃分析中的一种可比方法。
缺点：但是，我没有像荟萃分析那样比较效应大小。
汇总数据分析：
优点汇总数据在组级别合并。
缺点但不在个人级别汇总。合并不是为了提高效应大小的精度。
我的问题：
所以它基本上是对合并数据的网络分析？有没有针对该特定方法的可比文献？或者我应该接受它，因为基于同质研究方法的传递性已经证明了使用数据的方法就像是同一项研究一样是合理的。]]></description>
      <guid>https://stats.stackexchange.com/questions/654003/type-of-analysis</guid>
      <pubDate>Sat, 07 Sep 2024 15:05:04 GMT</pubDate>
    </item>
    <item>
      <title>关于随机森林回归中的 R 平方</title>
      <link>https://stats.stackexchange.com/questions/654002/about-r-squared-in-random-forest-regression</link>
      <description><![CDATA[
假设我有三个特征 [x1、x2、x3] 和一个目标变量 y1，用于随机森林回归模型。

我有两个数据集：df1，仅包含相关特征 [x1、x2、x3]；df2，除了 [x1、x2、x3] 外，还包括其他不相关的列（例如 x4、x5）。

如果我仅使用特征 [x1、x2、x3] 和目标 y1 对 df1 和 df2 运行随机森林回归，这两个数据集的 R² 值会相同吗？为什么会相同或不相同？

]]></description>
      <guid>https://stats.stackexchange.com/questions/654002/about-r-squared-in-random-forest-regression</guid>
      <pubDate>Sat, 07 Sep 2024 14:54:08 GMT</pubDate>
    </item>
    <item>
      <title>加权线性回归——当加权矩阵不是对角线时如何解释</title>
      <link>https://stats.stackexchange.com/questions/654000/weighted-linear-regression-how-to-interpret-when-weighting-matrix-is-not-diag</link>
      <description><![CDATA[加权最小二乘问题的经典案例$$(X^TWX)\hat{\beta} = X^T Wy$$ 正在解决最小化问题$${\operatorname{arg\ min}}\, \sum_{i=1}^{n} w_i \left|y_i - \sum_{j=1}^{m} X_{ij}\beta_j\right|^2 =
\underset{\boldsymbol\beta}{\operatorname{arg\ min}}\, \left\|W^\frac{1}{2}\left(\mathbf{y} - X\boldsymbol\beta\right)\right\|^2$$
其中 $w_i$ 是我们分配给每个观察值的权重。 $W = diag(w_i）$ 矩阵是此类权重的对角矩阵。解释很清楚。
我的问题是，如果 $W$ 不是对角矩阵，而是有 $W_{ij}$ 个条目。在这种情况下，我该如何解释我的回归？ 目标函数中的每个项不是一次观察的误差，而是有很多“交叉项”，如 $W_{ij} (y_i - \beta x_i) (y_j - \beta x_j)$（在单变量回归的情况下）。这些项是什么意思，与对角线情况相比，它将如何影响我们的回归系数，当 $W_{ij} (y_i - \beta x_i) (y_j - \beta x_j)$ 时。 class=&quot;math-container&quot;&gt;$W_{ij}$ 存在吗？$W_{ij}&gt;0$ 或 $W_{ij}&lt;0$ 时有什么区别吗？
我之所以问这个问题，是因为这里似乎有合法的用例，例如 卡尔曼滤波器。P 矩阵是回归的权重。但我不确定如何解释它。
]]></description>
      <guid>https://stats.stackexchange.com/questions/654000/weighted-linear-regression-how-to-interpret-when-weighting-matrix-is-not-diag</guid>
      <pubDate>Sat, 07 Sep 2024 13:50:48 GMT</pubDate>
    </item>
    <item>
      <title>ARDL 模型需要平稳性吗？</title>
      <link>https://stats.stackexchange.com/questions/653998/does-ardl-model-require-stationarity</link>
      <description><![CDATA[背景：我有一些变量在水平上是非平稳的，但在一阶差分时是平稳的（至少在滞后 0 和 1 时是平稳的，但超过这个点后就变为非平稳的）。我使用了 VAR 模型，因为所有变量都具有相同的积分阶数，但该模型存在许多问题（自相关、异方差、非正态残差）。
因此，我尝试使用非平稳数据（即尚未差分）的 ARDL 模型，并为不同的变量分配不同的滞后（R 中的 ARDL::auto_ardl 函数根据 AIC 标准为每个变量提供了最佳滞后），一切都很好（所有诊断都正常）。
我担心的是，除了 Shreshta &amp; Bhatta &quot;选择适当的时间序列数据分析方法框架&quot; (2018) 和我在这里看到的另一篇文章，我找不到允许我确认可以在非平稳数据上使用 ARDL 的参考文献。]]></description>
      <guid>https://stats.stackexchange.com/questions/653998/does-ardl-model-require-stationarity</guid>
      <pubDate>Sat, 07 Sep 2024 11:37:39 GMT</pubDate>
    </item>
    <item>
      <title>散点图的视觉评估可以接受吗？</title>
      <link>https://stats.stackexchange.com/questions/653986/visual-assessment-of-scatterplots-acceptable</link>
      <description><![CDATA[我有一个相当基本的问题，关于分析对一些鱼类进行测量的数据集，这是我作为学生项目的一部分进行的。因此，我对四种鱼类进行了测量，我知道这些鱼类的种类（基于 DNA 分析），每组大约有 50 只鱼。此外，我还有十几条未知物种的鱼。我想探索是否可以根据测量结果将未知个体归类为一个物种。事实上，之前的一项研究已经证实了这一点。然而，未知个体很可能属于多个物种。因此，将它们作为一个群体进行分析是没有意义的。因此，它们必须作为个体进行独立分析，而之前研究中进行的典型测试（学生 t 检验、方差分析）将很难应用。
到目前为止，我只在 xy 轴上绘制了测量值，以直观地评估标本与已知鱼类群体的关系。然而，我因没有使用任何适当的统计测试来做出区分而受到批评。所以我有两个问题：1）仅仅对数据进行视觉评估在多大程度上是“错误的”？我知道这是处理数据的一种简单方法，但这是否必然会使我的分析无效？特别是因为未知样本一次只能分析一个。这在多大程度上是可以接受的？2）还有哪些其他测试有用？]]></description>
      <guid>https://stats.stackexchange.com/questions/653986/visual-assessment-of-scatterplots-acceptable</guid>
      <pubDate>Fri, 06 Sep 2024 20:01:15 GMT</pubDate>
    </item>
    <item>
      <title>正确分析解释异常研究设计</title>
      <link>https://stats.stackexchange.com/questions/653985/correct-analysis-interpretation-of-unusual-study-design</link>
      <description><![CDATA[英国皇家麻醉师学院的国家审计项目是一系列研究麻醉罕见并发症的项目。例子包括严重的过敏反应和心脏骤停。（对于任何想阅读具体内容的人，我建议阅读最简单的 NAP3）
他们使用的方法我不熟悉，我正在努力寻找正确分析的良好参考。他们试图估计这些事件的频率，并提出风险因素。基本思想是：

测量分母

执行“活动调查” - 让医院在短时间内（通常是几天到几周左右）记录（匿名）他们处理的每个病例
将其乘以得到感兴趣时间段的分母


测量分子

要求医院报告感兴趣的并发症 - 这一阶段持续更长的时间（通常是几个月）



尝试应用我已经知道的知识，显然这是一项观察性研究；它不是横断面数据，也不是病例对照研究；它看起来有点类似于前瞻性队列研究 - 我们正在观察一段时间内确定的一组人并计算事件 - 但有一些不同：

没有一个明确定义的队列 - 我们不能谈论被招募到这项研究中的个人，直到他们发生事件
“伪队列”在短时间内进行测量，然后推断出如果我们在感兴趣的时间段内招募，它可能会是什么样子
只有经历过事件的患者的暴露数据才是众所周知的

这可能是我职业生涯中收集的关于这些事件的最佳数据，它在麻醉实践中被广泛使用，因此正确理解它非常重要。
我的问题是：

这种研究有一个众所周知的名字吗？ （即我可以搜索什么？）
是否有分析此类研究的标准方法或指南？
是否有很好的方法来解释分母数据的不确定性？
使用标准频率论技术分析此类数据的传统队列研究是否合理？（这很重要，因为这是他们所做的！）

编辑：
我特别担心的是：

最初的“活动调查”是横断面数据 - 由此得出的统计数据不仅具有集中趋势，而且具有置信区间
然后，我们将该横断面数据的统计数据相乘以考虑更长的时间段 - 假设我们对得出的数据的置信区间呈线性增加？ （并且只是作为比例因子的直接倍数？）
然后他们使用 (2) 产生的点估计作为分母：

我们是否应该合并 (2) 产生的置信区间并使用 95% CI 的上限和下限以及集中趋势计算几个结果？
使用标准技术（即使用 R 中的 RelRisk）为这个“二阶”统计数据创建 95% CI 是否有效？


]]></description>
      <guid>https://stats.stackexchange.com/questions/653985/correct-analysis-interpretation-of-unusual-study-design</guid>
      <pubDate>Fri, 06 Sep 2024 19:47:15 GMT</pubDate>
    </item>
    <item>
      <title>样本中未观察到任何事件时的患病率上限</title>
      <link>https://stats.stackexchange.com/questions/653965/prevalence-upper-bound-when-no-events-are-observed-in-sample</link>
      <description><![CDATA[在 2000 个观察样本中，未发现阳性病例，但我仍然希望能够提供患病率的上限。
人们似乎使用的一般规则是简单地取 3/n，但这与样本中发现 1 个病例的 95% 上限相同。
3/2000=0.15%，2000 年患病率为 1 例的 95% 上限为 0.15%。
如果观察到 0 个病例，上限肯定应该低于发现 1 个病例的情况？我可以使用其他方法来与其他频率结果在一定置信水平下的上限保持一致吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/653965/prevalence-upper-bound-when-no-events-are-observed-in-sample</guid>
      <pubDate>Fri, 06 Sep 2024 12:04:43 GMT</pubDate>
    </item>
    <item>
      <title>当仅观察到数据的最大值时，统计量完整且充分</title>
      <link>https://stats.stackexchange.com/questions/653960/complete-and-sufficient-statistic-when-only-the-maximum-of-the-data-is-observed</link>
      <description><![CDATA[
我找到了联合 PDF，但之后无法应用因式分解。
我该如何解决这个问题？
联合 PDF 由以下公式给出：
$
F(z, \Delta) = \left(f_X(z) F_Y(z)\right)^\Delta \left(f_Y(z) F_X(z)\right)^{1-\Delta} 
$
其中 $f_X(z)$ 和$f_Y(z)$ 分别是 $X$ 和 $Y$ 的 PDF，而 $F_X(z)$ 和 $F_Y(z)$ 分别是 $X$ 和 $Y$ 的 CDF。]]></description>
      <guid>https://stats.stackexchange.com/questions/653960/complete-and-sufficient-statistic-when-only-the-maximum-of-the-data-is-observed</guid>
      <pubDate>Fri, 06 Sep 2024 10:05:12 GMT</pubDate>
    </item>
    <item>
      <title>如何解释统计上不显著的估计并排除较大的影响？</title>
      <link>https://stats.stackexchange.com/questions/653890/how-to-interpret-statistically-non-significant-estimates-and-rule-out-large-effe</link>
      <description><![CDATA[我正在做回归分析，并获得了一个统计上不显著的点估计。从经济角度来看，不显著的结果在我的语境中是有意义的，但我想确保这一发现不是由于缺乏效力。相反，我想确认这种影响确实接近于零，而不仅仅是统计上不显著。
我的目标是做出这样的陈述：“...从置信区间来看，我们可以排除大于 1-1.4 个月的预期寿命增加的影响”，正如 Meghir、Palme 和 Simeonova (2018) 所做的那样，或者“这些影响通常可以限制在零附近的一个狭窄区间内”，正如 Cesarini 等人 (2016) 所提到的那样。
根据我的点估计及其置信区间，我如何自信地解释结果以做出类似的陈述？具体来说，我该如何量化估计的精度以排除较大的影响？]]></description>
      <guid>https://stats.stackexchange.com/questions/653890/how-to-interpret-statistically-non-significant-estimates-and-rule-out-large-effe</guid>
      <pubDate>Thu, 05 Sep 2024 07:29:33 GMT</pubDate>
    </item>
    <item>
      <title>在使用 k 倍交叉验证训练随机森林分类模型后，我应该在训练集中使用 ROC 曲线吗？</title>
      <link>https://stats.stackexchange.com/questions/653801/should-i-use-roc-curve-in-my-training-set-after-training-a-random-forest-classif</link>
      <description><![CDATA[我有一个概念性问题：将数据集分为训练集和测试集（70:30），两者平衡且经过打乱，我是否应该使用由训练集的 k 倍交叉验证生成的模型的混淆矩阵和 ROC 曲线？
我之所以问这个问题，是因为每次我从网格搜索中获得的准确率总是接近 61%。当我用训练集制作混淆矩阵时，准确率是 57%。但是当我用训练集制作 ROC 曲线和混淆矩阵时，准确率总是接近 100%。这是过度拟合的迹象吗？还是我应该只考虑交叉验证和训练集的准确度值（61% 和 57%）？
这是我的代码：
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0, stratify=y, shuffle=True)

param_grid = {
&#39;n_estimators&#39;: [50, 100, 200, 250], # 植物的树状图数量
&#39;max_depth&#39;: [3, 5, 8, 10], # 植物的最大深度
&#39;min_samples_split&#39;: [2, 5, 10], # 最小样本数&#39;min_samples_leaf&#39;: [1, 2, 4], # &#39;max_features&#39;: [&#39;sqrt&#39;], # &#39;oob_score&#39; 所需的最少数量: [True] } rf = RandomForestClassifier( random_state=0) grid_search = GridSearchCV( estimator=rf, param_grid=param_grid, cv=10, # 10 倍交叉验证 n_jobs=-1, # CPU 调度核心的待办事项 verbose=2, # 总线处理的优化
评分=&#39;accuracy&#39;，# 评估指标
return_train_score=True
)

grid_search.fit(x_train, y_train)

print(&quot;最佳超参数：&quot;)
print(grid_search.best_params_)
#最佳超参数：
#{&#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 2, &#39;min_samples_split&#39;: 5, #&#39;n_estimators&#39;: 100, &#39;oob_score&#39;: True}

print(&quot;最佳交叉验证准确率：&quot;)
print(grid_search.best_score_)
#最佳交叉验证准确率：0.6137931034482758

best_model = grid_search.best_estimator_
test_accuracy = best_model.score(x_test, y_test)
test_accuracy
#0.5684931506849316

train_accuracy = best_model.score(x_train, y_train)
train_accuracy
#0.9844827586206897

PS：我在 scikt learn 文档这里中看到，它在 k 倍之后在测试集中的验证停止，但我没有发现任何提到使用训练数据集再次验证的内容。]]></description>
      <guid>https://stats.stackexchange.com/questions/653801/should-i-use-roc-curve-in-my-training-set-after-training-a-random-forest-classif</guid>
      <pubDate>Tue, 03 Sep 2024 17:40:52 GMT</pubDate>
    </item>
    <item>
      <title>交叉重复测量设计的线性混合模型</title>
      <link>https://stats.stackexchange.com/questions/653482/linear-mixed-model-for-crossed-repeated-measures-desing</link>
      <description><![CDATA[我正在尝试分析一个纵向实验心理学数据集。每个参与者都完成了一项任务，收集了他们对两种不同刺激（这里我称之为低和高）的反应时间。该任务有多个区块，每个参与者在多个年龄段接受评估。区块嵌套在年龄内，年龄和参与者交叉，每个参与者在所有年龄段接受评估，所有年龄段包含所有参与者的数据。以下是数据的简化子集，以显示一般结构。实际上我有更多区块，实际数据集包含 97 名参与者、20 个区块和 4 个年龄段。区块将被建模为连续的，并估计线性斜率，而年龄将被视为分类的，一个具有 4 个级别的因子。
 主题 刺激 区块 年龄 RT
&lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 10010 高 1 1 1136
2 10010 高 2 1 1111 
3 10010 高 3 1 1220 
4 10010 低 1 1 1224
5 10010 低 2 1 862 
6 10010 低 3 1 893 
7 10010 高 1 2 849 
8 10010 高 2 2 904 
9 10010 高 3 2 878 
10 10010 低 1 2 893 
11 10010 低 2 2 863 
12 10010 低 3 2 851 
13 10010 高 1 3 863 
14 10010 高 2 3 635 
15 10010 高 3 3 603 
16 10010 低 1 3 928
17 10010 低 2 3 606 
18 10010 低 3 3 568 
19 10010 高 1 4 623 
20 10010 高 2 4 538 
21 10010 高 3 4 544 
22 10010 低 1 4 610 
23 10010 低 2 4 670
24 10010 低 3 4 558

我们的主要问题是年龄如何影响刺激类型之间的 RT 差异。但是，我们希望控制基线 RT 和 RT 改进随年龄而产生的巨大差异。随着年龄的增长，基线 RT 变得更小，斜率变平（改进空间更小）。见下图。这是有道理的，也正是我们所期望的。

所以我认为固定效应结构是明确的，我们想要刺激 * 阻断 * 年龄，但我不确定最合适的随机效应结构是什么。 此问题描述了类似的情况，即不同气候模型（年龄）在不同时间（区块）和不同初始条件（主题）下的情况。Ben Bolker 对该问题的回答让我认为，我们案例中合适的模型是：
RT ~ Stimulus*Block*Age + (Stimulus*Block | 主题：年龄)

因此，我们将捕捉每个主题和每个年龄的截距和斜率差异，而无需为年龄设置单独的随机效应项，因为这与其固定效应是多余的。我的这个推理正确吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/653482/linear-mixed-model-for-crossed-repeated-measures-desing</guid>
      <pubDate>Wed, 28 Aug 2024 12:24:15 GMT</pubDate>
    </item>
    <item>
      <title>随机森林：MSE 和 R2 较好，但交叉验证结果较差</title>
      <link>https://stats.stackexchange.com/questions/652148/random-forest-good-mse-and-r2-but-poor-cross-validation-results</link>
      <description><![CDATA[我使用标准缩放器构建、预处理和清理（na/0s、winsorise、log 等）一些特征并对其进行标准化。
如果我仅使用平均值计算基线 MSE，我会得到 8.22
简单线性回归的 MSE 为 7.15，R2 为 0.13
具有 20 个估计量的随机森林模型的 MSE 为 3.07，R2 为 0.62 - 这似乎很奇怪。
因此，我尝试使用相同的随机森林模型进行 k 倍交叉验证，并得到了一些非常糟糕的结果：
交叉验证分数：[0.02259823 -0.70599662 -0.12186488 -0.18245173 -0.33035053]
平均 CV 分数：-0.2636131047737679
CV 分数的标准差：0.24849696694857612

在此阶段，我应该关注什么来排除模型故障？不确定该走哪条路。]]></description>
      <guid>https://stats.stackexchange.com/questions/652148/random-forest-good-mse-and-r2-but-poor-cross-validation-results</guid>
      <pubDate>Thu, 01 Aug 2024 12:24:54 GMT</pubDate>
    </item>
    </channel>
</rss>