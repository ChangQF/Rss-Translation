<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 10 Dec 2024 12:36:35 GMT</lastBuildDate>
    <item>
      <title>核密度估计的$L_2$范数一致性</title>
      <link>https://stats.stackexchange.com/questions/658521/l-2-norm-consistency-of-kernel-density-estimation</link>
      <description><![CDATA[我能找到的核密度估计的一致性结果几乎都针对 $L_1$-norm 或 $L_\infty$-norm，例如在这篇论文或这篇论文中。我无法简单地将它们推广到 $L_2$-norm，因为在 $\mathbb{R}$ 这样的无限测度场上，$L_2$ 和 $L_1$ 范数之间的关系尚不清楚。所以有人能告诉我 $L_2$-norm 的一致性结果吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658521/l-2-norm-consistency-of-kernel-density-estimation</guid>
      <pubDate>Tue, 10 Dec 2024 12:29:12 GMT</pubDate>
    </item>
    <item>
      <title>Beta 分布和矩问题（需要引用）</title>
      <link>https://stats.stackexchange.com/questions/658519/beta-distribution-and-the-moment-problem-citation-needed</link>
      <description><![CDATA[据我所知，Beta 分布根据其矩具有唯一定义（即矩问题在其矩值上具有唯一解）。维基百科关于 Beta 分布的文章甚至对此进行了阐述，但维基百科上的参考资料（Billingsley，“概率与测量”）仅提供了矩生成函数收敛半径为正的任意分布的唯一性 - 我无法在文献中找到有关 Beta 分布的结果。
如果我自己要推导收敛半径，我会用 (1/k!) 来限制矩生成函数的系数，其 k 次根收敛到 0，并使用柯西-哈达玛定理来获得无限的收敛半径。但我对收敛半径的理解相当肤浅，我宁愿不在我的文章中包含推导，否则这与此完全无关。由于结果看起来相当直接，我很难相信没有已发表的证明，尽管我找不到。这里有人知道我可以引用的参考资料吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658519/beta-distribution-and-the-moment-problem-citation-needed</guid>
      <pubDate>Tue, 10 Dec 2024 12:02:45 GMT</pubDate>
    </item>
    <item>
      <title>如何调整超参数以在小数据集下降低校准误差</title>
      <link>https://stats.stackexchange.com/questions/658518/how-to-tune-hyperparameters-for-low-calibration-error-under-small-dataset</link>
      <description><![CDATA[我正在研究变分自动编码器 (VAE) 的哪种变体在小数据集下可以提供更好的预期校准误差 (ECE)（另请参阅此文档）。根据 google 的调整手册，要比较科学超参数，我们需要“优化”干扰超参数。当应用于此处时，它意味着要比较 VAE 变体在 ECE 方面的性能，我们需要找到每个 VAE 变体在 ECE 方面的最佳其他超参数集（例如学习率），以便 VAE 变体之间的比较足够公平。显然，另一个经验法则是，我们应该在验证集而不是测试集上调整超参数。
问题是，为了可靠地计算 ECE，我们需要一个相对较大的数据集，这意味着验证集必须足够大，但在我的例子中，验证集不会足够大，因为它是一个小型训练集的一部分。我使用的是合成数据，因此我可以随时扩大验证集。所以我的问题是，我应该在优化干扰超参数（如学习率）的同时扩大验证集吗？
非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/658518/how-to-tune-hyperparameters-for-low-calibration-error-under-small-dataset</guid>
      <pubDate>Tue, 10 Dec 2024 10:36:20 GMT</pubDate>
    </item>
    <item>
      <title>联合样本的边际经验分布</title>
      <link>https://stats.stackexchange.com/questions/658516/marginal-empirical-distribution-from-joint-sample</link>
      <description><![CDATA[我有一个相当“简单”的疑问，想澄清一下。
假设我有一个分层模型，其中数据以以下方式采样：

从 $P_U$ 中采样 $U_i$
从 $P_{X|U_i}$ 中采样 $X_i$

换句话说，根据 $U_i$ 的值，从不同的分布中采样 $X_i$。$U$ 是 i.i.d。但 $X$ 仅在给定每个 $U_i$ 的情况下才是独立同分布的。
让 $\mathcal{U},\mathcal{X}$ 分别成为 U 和 X 的支持。我现在感兴趣的是估计数据的经验分布。如果我观察$(U_i,X_i)$的样本，那么\underline{joint}经验分布的估计量就是经验分布：
$$F_{U,X}(A)\approx\frac{1}{n}\sum_{i=1}^n1_{(U_i,X_i)\in A}\quad\quad A\in \mathcal{U}\times\mathcal{X}$$
假设我现在不观察潜在变量 U。我如何估计 X 的边际分布？对 U 的依赖是否会引起问题？我知道 $F_X(X) = \int F_U(X,U)dU$ 但这在实践中如何转化？X 的边际分布的估计量是否简单：
$$F_X(B)\approx\frac{1}{n}\sum_{i=1}^n1_{(X_i)\in B}\quad\quad B\in \mathcal{X}$$
我是否把一个简单的问题复杂化了？任何帮助都将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/658516/marginal-empirical-distribution-from-joint-sample</guid>
      <pubDate>Tue, 10 Dec 2024 10:07:51 GMT</pubDate>
    </item>
    <item>
      <title>斯托弗 (Stouffer) 方法：对产生 p 值的潜在假设检验进行限制？</title>
      <link>https://stats.stackexchange.com/questions/658515/stouffers-method-restriction-on-underlying-hypothesis-tests-producing-p-values</link>
      <description><![CDATA[当使用 Stouffer 方法来组合 p 值时，必须先将它们转换为 z 分数。这是否意味着，例如从置换测试中得出的 p 值不是有效输入，因为它们不是基于 z 分数？]]></description>
      <guid>https://stats.stackexchange.com/questions/658515/stouffers-method-restriction-on-underlying-hypothesis-tests-producing-p-values</guid>
      <pubDate>Tue, 10 Dec 2024 09:43:30 GMT</pubDate>
    </item>
    <item>
      <title>在因变量的原始分数中预先存在截止点的分位数回归</title>
      <link>https://stats.stackexchange.com/questions/658514/quantile-regression-with-a-pre-existing-cutoff-point-in-the-raw-score-of-depende</link>
      <description><![CDATA[这可能是一个愚蠢的问题，但我从未使用过分位数回归，所以我不确定。
我的情况如下：有一个连续的、数值的自我报告测量，其特定的、预先设定的截止点为 100 分。根据之前的研究，得分低于 100 分的人被认为没有某种“疾病”，而得分为 100 分或以上的人则有这种疾病。我的同事想运行一个模型，根据某些独立变量预测这个分数，但他们想看看那些得分低于 100 分和高于 100 分的人的预测因子与分数的关系是否不同。
我之前在这里得到过建议，建议分位数回归会起作用，我同意。但我不确定如何指定分位数。显然，我需要指定（至少）2 个分位数，以获得有条件的人和没有条件的人的预测，但我该如何选择它们呢？我是否应该检查样本中原始分数 100 对应的分位数，然后...以某种方式...选择一个略低于“100 分位数”的分位数和一个略高于“100 分位数”的分位数？但是低于和高于多少呢？我应该如何选择准确的分位数？
（进行这项研究的人更喜欢只有两个预测水平（两个分位数），因为以前的研究表明 100 是一个重要的截止值，也是为了保持简单。）]]></description>
      <guid>https://stats.stackexchange.com/questions/658514/quantile-regression-with-a-pre-existing-cutoff-point-in-the-raw-score-of-depende</guid>
      <pubDate>Tue, 10 Dec 2024 08:40:44 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 tt() 项绘制 coxph 模型中的风险比</title>
      <link>https://stats.stackexchange.com/questions/658513/how-to-plot-hazard-ratio-from-coxph-model-with-tt-term</link>
      <description><![CDATA[这个问题与我在此处发布的另一个问题相关：
如何直观地评估 coxph 中的 tt() 适用性
如果我们有一个随时间变化的 HR，它来自时间相关的系数，因为我们已经使用 tt() 指定了时间变换，我们如何绘制它？
使用生存小插图中的示例，如果我们指定一个包含 karno 上的样条项的模型，因为存在非比例风险：
library(survival)
fit.tt3 &lt;- coxph(Surv(time, status) ~ trt + Prior + karno + tt(karno), data =资深人士，
tt = function(x, t, ...) x * nsk(t, knots = c(5, 20, 50, 150, 300),
Boundary.knots = FALSE))

有没有一种直接的方法（嗯，任何方法都可以）绘制 karno 的 HR 作为时间函数？如果能够通过向研究团队的其他成员展示一个简单的图形来告诉他们时间依赖性意味着什么，那就太好了？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/658513/how-to-plot-hazard-ratio-from-coxph-model-with-tt-term</guid>
      <pubDate>Tue, 10 Dec 2024 02:41:16 GMT</pubDate>
    </item>
    <item>
      <title>如何直观地评估 coxph 中的 tt() 适用性？</title>
      <link>https://stats.stackexchange.com/questions/658512/how-to-visually-assess-tt-suitability-in-coxph</link>
      <description><![CDATA[对于此示例，我从时间相关的生存小插图中获取线索。
我有兴趣了解如何使用 R 中的 coxph() 评估协变量-时间交互的适用性，因为据我了解 cox.zph() 不接受包含 tt() 项的模型。这个出色的答案表明，目视检查也是一种很好的方法：
时间相关效应：使用 coxph() 中的 nsk () 的 tt 函数
因此，作为诊断的一部分，假设我们绘制模型中的 Schoenfeld 残差：
library(survival)
# 拟合不依赖时间的 cox 模型
fit &lt;- coxph(Surv(time, status) ~ trt + Prior + karno, data = veteran)
# 检查 Schoenfeld 残差
(zph &lt;- cox.zph(fit))
# 绘制残差和平均 beta (log HR)
plot(zph[3])
abline(a = coef(fit)[3], b = 0, col = 2)


这让我们意识到我们在卡诺的非比例风险方面存在问题，因此作为第一步，我们指定最简单的时间变换 - 对数风险和卡诺之间的线性相互作用。 Schoenfeld 残差现在是（假设我正确地绘制了它们）：
# 拟合线性时间相互作用
fit.tt &lt;- coxph(Surv(time, status) ~ trt + Prior + karno + tt(karno), data = veteran,
tt = function(x, t, ...) x * t)
# 对残差应用线性时间变换
zph.tt &lt;- cox.zph(fit, transform = function(t) t)
# 使用时间变换重新绘制残差
plot(zph.tt[3])
abline(coef(fit.tt)[3:4], col = 2)


这看起来不太好，所以我们进行实验并最终得出了在小插图中指定的时间变换：
# 拟合小插图时间交互
fit.tt2 &lt;- coxph(Surv(time, status) ~ trt + Prior + karno + tt(karno), data = veteran,
tt = function(x, t, ...) x * log(t + 20))
# 将小插图时间变换应用于残差
zph.tt2 &lt;- cox.zph(fit, transform = function(t) log(t + 20))
# 使用时间变换重新绘制残差
plot(zph.tt2[3])
abline(coef(fit.tt2)[3:4], col = 2)


看起来好多了。但是，我们希望使其尽可能灵活，因此我们改为指定样条项（如插图所示） - 我将结点位置更改为我认为可以捕捉曲率最大变化的位置：
# 拟合样条时间相互作用
fit.tt3 &lt;- coxph(Surv(time, status) ~ trt + Prior + karno + tt(karno), data = veteran,
tt = function(x, t, ...) x * nsk(t, knots = c(5, 20, 50, 150, 300),
Boundary.knots = FALSE))
# 将样条时间变换应用于残差
zph.tt3 &lt;- cox.zph(fit, transform = function(t) nsk(t, knots = c(5, 20, 50, 150, 300),
Boundary.knots = FALSE))
# 使用时间变换重新绘制残差
plot(zph.tt3[3])


我被困在了这一点上。我该如何解释这个图，因为残差相当倾斜。那么，我该如何绘制实际的样条函数（显然 abline 不再适用）？
除非我误解了什么，否则在时间变换中指定样条函数似乎是提供最大可能随时间变化的协变量风险形状覆盖范围的最灵活的方法。]]></description>
      <guid>https://stats.stackexchange.com/questions/658512/how-to-visually-assess-tt-suitability-in-coxph</guid>
      <pubDate>Tue, 10 Dec 2024 02:32:58 GMT</pubDate>
    </item>
    <item>
      <title>矩阵 t 分布的矢量化是多元 t 分布吗？</title>
      <link>https://stats.stackexchange.com/questions/658511/is-the-vectorization-of-a-matrix-t-distribution-a-multivariate-t-distribution</link>
      <description><![CDATA[正如我在标题中提到的，我想知道矩阵变量 t 分布的矢量化版本是否是多变量 t 分布。如果是的话，我们如何用前一个分布的参数来表达后一个分布的参数？如能提供有关该主题的参考资料，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/658511/is-the-vectorization-of-a-matrix-t-distribution-a-multivariate-t-distribution</guid>
      <pubDate>Tue, 10 Dec 2024 02:14:21 GMT</pubDate>
    </item>
    <item>
      <title>设计矩阵的病态性是否会影响深度学习环境下 Hessian 的病态性？</title>
      <link>https://stats.stackexchange.com/questions/658508/does-the-ill-conditioning-of-the-design-matrix-affect-the-ill-conditioning-of-th</link>
      <description><![CDATA[我知道，当我们在 DL 中使用平方损失作为成本函数时，Hessian 的病态与设计矩阵的病态直接相关。这是否适用于其他成本函数？
如果是这样，这是否可以（即使是松散地）与为什么规范化在处理神经网络的病态优化景观方面有效相关？
如果可能的话，我正在寻找直观而严谨的答案，或者我可以阅读的参考资料。]]></description>
      <guid>https://stats.stackexchange.com/questions/658508/does-the-ill-conditioning-of-the-design-matrix-affect-the-ill-conditioning-of-th</guid>
      <pubDate>Mon, 09 Dec 2024 22:28:15 GMT</pubDate>
    </item>
    <item>
      <title>SLR 中的 H0：beta_1=0 是否假设结果呈正态性，但这可能根本不是事实？</title>
      <link>https://stats.stackexchange.com/questions/658499/does-h0beta-1-0-in-slr-assume-normality-of-outcome-which-might-not-be-true-at-a</link>
      <description><![CDATA[零假设下的理论简单线性回归模型
零假设下的理论简单线性回归模型（$\beta_1 = 0$）是：
$$
Y_i = \beta_0 + \epsilon
$$
其中$\epsilon \sim N(0, \sigma^2)$。
因此本质上：
$$
Y_i \sim N(\beta_0, \sigma^2)
$$
不过我想知道是否使用$y_i$的平均值和标准差比使用从原始样本估计的 $\beta_0$ 和 $\sigma$ 更合理。
这是否意味着 $Y_i$ 必须是正态分布的，即：

不包含在简单线性回归 (SLR) 假设中。
很可能已经被我们的样本数据 拒绝，只要原始 $y_i$ 不是“足够正常”。

如果 $H_0$ 确实是 $Y_i \sim N(\beta_0, \sigma^2)$，我们可以通过量化 $y_i$ 值的 KDE 与完美正态分布的偏差程度并为 该特定统计数据 提供 p 值来测试它。这里不需要涉及回归。
有人可能会认为，更好的零假设是预测变量的值对结果无关紧要，并通过对 $x_i$ 的值进行混洗来模拟零假设下的样本。但话又说回来，这将是一个 $H_0$，不仅适用于线性关联，也适用于任何可能的关联。
在这种情况下，如果我们将线性形式拟合到通过改组生成的每个模拟样本中，以创建斜率的零采样分布，并发现我们观察到的斜率的 p 值小于 0.05：

我们是否拒绝没有线性关联的假设（这是我们想要的）？
或者我们只能拒绝没有任何关联（这没什么帮助，因为我们还不知道它是否是线性的）？

也许 $H_0: Y_i \sim N(\beta_0, \sigma^2)$ 实际上意味着存在一些需要正态性的其他关联结果，只是它不是线性的，所以$\beta_1 = 0$？
这有点像是同时提出两个问题，所以如果有点令人困惑，请见谅。我还想澄清一下 H_0:beta_1=0 下的 beta_0 和 sigma 是什么，因为我们的教授在课堂上没有具体说明。]]></description>
      <guid>https://stats.stackexchange.com/questions/658499/does-h0beta-1-0-in-slr-assume-normality-of-outcome-which-might-not-be-true-at-a</guid>
      <pubDate>Mon, 09 Dec 2024 19:11:48 GMT</pubDate>
    </item>
    <item>
      <title>关于赫斯特指数定义和持久性属性的澄清</title>
      <link>https://stats.stackexchange.com/questions/658520/clarifications-on-hurst-exponent-definitions-and-persistence-properties</link>
      <description><![CDATA[我对赫斯特指数有疑问，希望有人能帮我澄清一下。
众所周知，赫斯特指数有不同的定义，但找到这些定义之间的明确联系或关系似乎出奇地具有挑战性。我遇到过各种各样的解释，但我还没有找到一个全面的解释来说明这些不同的定义在实际应用中是如何一致或不同的。
此外，人们普遍认为：

对于$H&lt;0.5$，该过程表现出反持久性。
对于$H&gt;0.5$，该过程表现出持久性。

然而，虽然这种分类经常出现，但很少提供其背后的原因或更深层次的解释。
有人可以提供一些澄清，或者给我指出可靠的参考书目来解决：

赫斯特指数的各种定义之间的关系。
清楚地解释为什么$H&lt;0.5$ 对应于反持久性，而 $H&gt;0.5$ 对应于持久性。

提前感谢您的见解！]]></description>
      <guid>https://stats.stackexchange.com/questions/658520/clarifications-on-hurst-exponent-definitions-and-persistence-properties</guid>
      <pubDate>Mon, 09 Dec 2024 09:22:40 GMT</pubDate>
    </item>
    <item>
      <title>IV 等级/相关性条件线性代数直觉</title>
      <link>https://stats.stackexchange.com/questions/658451/iv-rank-relevance-condition-linear-algebra-intuition</link>
      <description><![CDATA[考虑以下计量经济模型（IV）：$Y_1 = X&#39;\beta + e$，其中$Y_1 \in \mathbb{R}$是一些感兴趣的结果变量，我们有一组回归量$X = \begin{bmatrix} Z_1 \\ Y_2 \end{bmatrix} \in \mathbb{R}^k$。 ($Z_1 \in \mathbb{R}^{k_1}, Y_2 \in \mathbb{R}^{k_2}, k_1 + k_2 = k \:$) 假设我们可能有一些混杂因素，因此$\mathbb{E}[Xe] \neq 0$。但是假设我们也有一些工具变量$Z=\begin{bmatrix}Z_1\\Z_2 \end{bmatrix} \in \mathbb{R}^{\mathcal{l}}$，使得$\mathbb{E}[Ze] =0, \mathbb{E}[ZZ&#39;]$为psd，并且$\mathbb{E}[ZX&#39;]$的秩为$k$（相关性）。
我对相关性条件很好奇：$\mathbb{E}[ZX&#39;]$的秩必须为$k$。我可以理解这里与 $Cov(X, Z)$ 的联系，但我只是不太清楚为什么秩条件意味着这一点（与 $Z$ 变换对 $X$ 列的作用有关），以及这在某种意义上必须“保留”至少 $X$ 维度才能相关。
此外，在我看来，这似乎与原始 OLS 案例中的秩条件有些相关：$\hat{\beta} = (X&#39;X)^{-1}X&#39;Y_1$，其中秩也必须是 $k$（这次我们希望我们的回归量是线性独立，每个“提供新的东西”）。这再次将 $\beta$ 的最简单形式概括为 $\frac{Cov}{Var}$。
总结一下，我缺少一些几何直觉，无法理解这些变换如何告诉我们空间中不同变量的方差/协方差。即，形式 $ZX&#39;$ 与解释这些变量之间的协变有什么直观的几何联系？以 $X&#39;X$ 为例，我可以看到当 $X$ 被贬低（或包含常数）时，$Var(X) = Cov(X, X) = \mathbb{E}[XX&#39;]$ 如何，因此变为 $\frac{1}{n} \mathbf{X&#39;X}$ 并带有样本。对于上面的 $\mathbb{E}[ZX&#39;]$，为什么我们需要 rank = $k$ 才能使 $Cov$ &#39;不为 0&#39;？]]></description>
      <guid>https://stats.stackexchange.com/questions/658451/iv-rank-relevance-condition-linear-algebra-intuition</guid>
      <pubDate>Sun, 08 Dec 2024 11:34:47 GMT</pubDate>
    </item>
    <item>
      <title>使用调查平均值从单个预测中得出估计值的难度</title>
      <link>https://stats.stackexchange.com/questions/658440/difficulty-in-deriving-a-estimator-using-survey-means-from-individual-forecasts</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/658440/difficulty-in-deriving-a-estimator-using-survey-means-from-individual-forecasts</guid>
      <pubDate>Sun, 08 Dec 2024 07:58:33 GMT</pubDate>
    </item>
    <item>
      <title>标准化回归模型的变量与回归模型中的权重？</title>
      <link>https://stats.stackexchange.com/questions/658437/standardizing-variables-for-a-regression-model-vs-weights-in-a-regression-model</link>
      <description><![CDATA[我在 R 中有一个纵向 GAM（一般加性模型）回归。
这是模型和数据的一般形式（响应介于 0 和 1 之间）。所有变量均在州一级计算（例如州 GDP、州内疾病率）：
gam_model &lt;- gam(
response ~ 
te(time_var, var1) +
te(time_var, var2) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

state time_varpopulation var1 var2 response
state_1 2005-01-01 1000000 500000 10000 0.45
state_1 2005-02-01 1001000 520000 12000 0.47
state_1 2005-03-01 1002001 540000 11000 0.46
state_1 2005-04-01 1003002 560000 13000 0.48
state_2 2005-01-01 200000 100000 2000 0.42
state_2 2005-02-01 200200 105000 2400 0.44
state_2 2005-03-01 200400 110000 2200 0.43
state_2 2005-04-01 200600 115000 2600 0.45

这里是我遇到的问题：

数据按州提供（多个州，1 个国家），但每个州的人口不同
这让我认为需要对模型进行一些处理，以防止人口较多的州对响应产生比人口较少的州更大的影响
我有每个州的人口

我正在考虑使用以下权重公式（我从这里得到这个想法https://www.nature.com/articles/s41598-024-54441-x）：
$$avg\_weight_s = \frac{1}{T}\sum_{t=1}^{T} \frac{\ln(population_{s,t})}{\frac{1}{N}\sum_{i=1}^{N} \ln(population_{i,t})}$$
其中：

$T$ 是时间段的总数
$N$ 是州的总数
$population_{s,t}$ 是 $t$ 时刻 $s$ 州的人口数
$population_{i,t}$ 表示每个州 $i$ 在时间 $t$ 的人口
请注意，尽管我为每个州设置了多个时间点...但每个州都只有一个权重（将始终使用）。我只是想重申这一点。

这让我考虑不同的模型选项：
# 选项 1：非标准化，无权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1) +
te(time_var, var2) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

# 选项 2：标准化，无权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1/population) +
te(time_var, var2/population) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

# 选项 3：非标准化，权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1) +
te(time_var, var2) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
weights = avg_weight,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

# 选项 4：标准化，权重

gam_model &lt;- gam(
response ~ 
te(time_var, var1/population) +
te(time_var, var2/population) +
s(time_var, by = state) +
s(state, bs = &quot;re&quot;),
data = sample_data,
weights = avg_weight,
method = &quot;REML&quot;,
family = betar(link = &quot;logit&quot;)
)

我有点困惑，不知道这些选项中哪一个在逻辑上是正确的。我认为其中一些可能有点过度，而另一些则完全不正确。有办法解决这个问题吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658437/standardizing-variables-for-a-regression-model-vs-weights-in-a-regression-model</guid>
      <pubDate>Sun, 08 Dec 2024 04:38:44 GMT</pubDate>
    </item>
    </channel>
</rss>