<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 21 Jan 2025 01:13:57 GMT</lastBuildDate>
    <item>
      <title>使用半层次生态数据的 GAM 的正确公式化？</title>
      <link>https://stats.stackexchange.com/questions/660295/proper-formulation-of-gam-with-semi-hierarchical-ecological-data</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/660295/proper-formulation-of-gam-with-semi-hierarchical-ecological-data</guid>
      <pubDate>Tue, 21 Jan 2025 00:25:08 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 R 计算第三类修正贝塞尔函数 (关于其参数) 的导数？[重复]</title>
      <link>https://stats.stackexchange.com/questions/660292/how-do-we-calculate-the-derivative-of-the-modified-bessel-function-of-the-third</link>
      <description><![CDATA[正如标题所述，我想知道如何使用 R 计算第三类修正贝塞尔函数的导数（关于其参数）。据我所知，没有包可以做到这一点。有人能提供一些指导吗？任何帮助都值得感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/660292/how-do-we-calculate-the-derivative-of-the-modified-bessel-function-of-the-third</guid>
      <pubDate>Mon, 20 Jan 2025 23:07:58 GMT</pubDate>
    </item>
    <item>
      <title>使用具有时间依赖性系数的 Cox 模型预测风险</title>
      <link>https://stats.stackexchange.com/questions/660291/predicting-risk-from-a-cox-model-with-time-dependent-coefficients</link>
      <description><![CDATA[假设我们想使用年龄、性别和 Epicel 的存在来预测恶性黑色素瘤的死亡率，该模型中的年龄系数是时间的阶跃函数，在 1500 天处有一个中断（年龄具有时间依赖性系数）。
library(tidyverse)

library(riskRegression)

data &lt;- as_tibble(Melanoma) %&gt;% mutate(status=as.numeric(status==1))

data2 &lt;- survSplit(Surv(time,status)~age+sex+epicel,data=data,cut=1500,episode=&#39;tgroup&#39;,id=&#39;id&#39;)

fit &lt;- coxph(Surv(tstart,time,status)~age:strata(tgroup)+sex+epicel,data=data2)

现在，为了计算一名 20 岁且有 epicel 的女性在 2000 天内的绝对风险，我们可以估算她的相关累积风险 ($H(t)$)，然后 2000 天内的绝对风险估计值为 $1-exp(-H(2000))$。根据 https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf 第 4.1 部分，我们将得到：
nd &lt;- tibble(id=1,tgroup=1:2,tstart=c(0,1500),time=c(1500,5000),status=0,

age=20,sex=&#39;Female&#39;,epicel=&#39;present&#39;)

H &lt;- with(survfit(fit,newdata=nd,id=id),splinefun(time,cumhaz))

1 - exp(-H(2000))

问题：

当我们具有时间相关系数时，survfit 如何始终为我们的示例患者生成一组累积风险估计值？换句话说，不同时间间隔的不同年龄系数如何有助于创建一组随时间推移的累积风险估计值？
我们是否需要估计 newdata 中每个人的累积风险来估计他们的风险？因为显然 $H(t)=H_0(t)exp(LP)$（$LP$ 是线性预测因子）在这里不成立，因为 $LP$ 包括时间的阶跃函数，因此它会随时间生成风险的阶跃函数。
]]></description>
      <guid>https://stats.stackexchange.com/questions/660291/predicting-risk-from-a-cox-model-with-time-dependent-coefficients</guid>
      <pubDate>Mon, 20 Jan 2025 22:29:38 GMT</pubDate>
    </item>
    <item>
      <title>如何训练具有较小ECE（预期校准误差）的模型？</title>
      <link>https://stats.stackexchange.com/questions/660282/how-to-train-a-model-with-a-small-eceexpected-calibration-error</link>
      <description><![CDATA[如果我们训练一个具有交叉熵损失的深度学习模型，我们期望该模型具有较低的交叉熵损失。有没有办法训练模型，使模型获得较小的预期校准误差，同时保持负对数似然较小？或者有没有好的后处理方法来降低 ECE，同时保持交叉熵损失较小？]]></description>
      <guid>https://stats.stackexchange.com/questions/660282/how-to-train-a-model-with-a-small-eceexpected-calibration-error</guid>
      <pubDate>Mon, 20 Jan 2025 19:19:42 GMT</pubDate>
    </item>
    <item>
      <title>Fisher 判别分析 (FDA) 是否会使类内协方差变白？其背后的直觉是什么？</title>
      <link>https://stats.stackexchange.com/questions/660281/does-fishers-discriminant-analysis-fda-whiten-the-within-classes-covariance</link>
      <description><![CDATA[作为一种降维技术，Fisher 判别分析或线性判别分析的目标是找到一组特征$W$，以最大化类间方差与类内方差之间的比率。表示该比率的一种方式（称为 Fisher 标准）是
$$J(W) = \frac{\det (W^T S_B W)}{\det (W^TS_W W)}$$
其中 $S_B$ 是类别均值之间的散点，$S_W$ 是类别内的方差。
现在，我已经在不同的地方看到了（例如 这里）该问题的解由对 $(S_{B}, S_W)$ 的广义特征向量给出，或者等效地，由 $S_W^{-1}S_B$ 的特征向量给出。
此外，我已经看到（例如 此处）对 $(S_B, S_W)$ 的广义特征值问题可以看作以下优化问题：
$$\max_{W} \quad \mathrm{Tr}(W^T{S_B}W) \\ \mathrm{s.t.} \quad W^T S_W W = \mathbf{I}$$
这两点知识让我想到，投射到特征集 $W$ 上的数据的类内方差应该被白化（即单位矩阵协方差），或者至少是不相关的，这取决于 $W$ 列的比例。但是，我没有在任何地方看到明确提到这一点，我也不明白为什么最大化上述目标会导致这种结果。
因此，鉴于上述情况，FDA/LDA 特征是否会使类内方差变白？这背后有什么直觉吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660281/does-fishers-discriminant-analysis-fda-whiten-the-within-classes-covariance</guid>
      <pubDate>Mon, 20 Jan 2025 19:14:41 GMT</pubDate>
    </item>
    <item>
      <title>根据百分点差异调整年龄组回答之间的百分比</title>
      <link>https://stats.stackexchange.com/questions/660280/adjusting-percentages-between-age-group-responses-based-on-a-percentage-point-di</link>
      <description><![CDATA[我正在对另一个组织的报告中的社会调查数据进行分析。
数据仅以百分比形式报告，我无法访问基础数据。
报告中指出，40 岁以下和 40 岁以上的人群在宗教信仰方面的自我认同存在 19 个百分点的差异。我想根据百分点差异正确调整百分比。我的“直觉”反应是将百分点差异添加到较年轻的组（差异表示为正数）并从较年长的组中扣除。但这似乎不会产生准确的结果，因为实际差异应该介于两者之间。
有人会数学吗？有什么想法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660280/adjusting-percentages-between-age-group-responses-based-on-a-percentage-point-di</guid>
      <pubDate>Mon, 20 Jan 2025 18:52:22 GMT</pubDate>
    </item>
    <item>
      <title>当使用短于一年的区块长度时，如何从 GEV 获得 1/100 的回报期？</title>
      <link>https://stats.stackexchange.com/questions/660277/how-to-get-1-in-100-return-period-from-gev-when-using-a-block-length-shorter-tha</link>
      <description><![CDATA[我正在计算一个数据集的广义极值分布，该数据集包含每 5 秒采样一次的约 15 年数据。我想从 GEV 估算 1/50 或 1/100 年的回报水平。我最初使用一年的块长度来计算 GEV（例如，取每个 1 年间隔的最大值并将其用作 GEV 拟合的输入）。这使我能够轻松计算超出回报的概率：
$$P_{exceed} = 1-1/T_{return}$$
其中 $T_{return}$ 为 50（或 100）年。然后我可以找到具有超出概率的 GEV 水平，$P_{exceed}$。
但是，数据集在给定年份通常具有多个独立的&quot;极端值&quot;，并且在给定年份可能有多达 10 个，因此采用年度区块最大值将低估极端事件的数量和回报水平。
我如何修改它以获得 50 年内 1 次的回报水平，同时使用例如GEV 中的 10 天区块长度？
我认为我可以使用 10 天区块计算 GEV，然后将 $T_{return}$ 乘以 36.5 以获得年度 $P_{exceed}$，而不是 10 天 $P_{exceed}$。这似乎有效，但我不确定它是否具有统计有效性。
这里有几个额外的注意事项：
首先，我的数据有一些差距。如果我执行年度区块最大值，那么就没有问题，因为每个区块仍然有一些数据来计算最大值，但如果我执行 10 天区块长度，那么有些区块根本没有数据，因此区块最大值为 NaN。是不是最好假装不存在间隙，时间序列更短但连续？（这就是我目前正在做的。）或者最好假设间隙具有非常小的最大值，然后只给它们一些默认的小值？
其次，极端事件很少见（呃）。如果我做一个年度区块最大值，我基本上可以保证在每个区块中都会得到一个“极端”事件。但如果我做一个 10 天的区块长度，那么很多区块的最大值都非常小。GEV 可以很好地拟合那些最小的区块最大值，但最远的极端值拟合得最差（见下图）。
任何澄清都值得赞赏！
谢谢
示例 1：年度区块最大值。 CI 确实很大，但最佳拟合 GEV 至少能很好地通过极值

示例 #2：10 天区块最大值。CI 要小得多，但最佳拟合 GEV 在约 1 年的重现间隔后甚至无法通过极值。为什么拟合度这么差？
]]></description>
      <guid>https://stats.stackexchange.com/questions/660277/how-to-get-1-in-100-return-period-from-gev-when-using-a-block-length-shorter-tha</guid>
      <pubDate>Mon, 20 Jan 2025 17:50:29 GMT</pubDate>
    </item>
    <item>
      <title>在 lme 中，我们应该使用 ML 还是 REML 方法进行假设检验？</title>
      <link>https://stats.stackexchange.com/questions/660274/in-lme-should-we-use-ml-or-reml-method-for-hypothesis-tesing</link>
      <description><![CDATA[我们有以下模型：
m &lt;- lme(y ~ x + time + event + time:event, data = df, 
random = ~ time | user_id, 
correlation = corAR1(form = ~ 1 | user_id), 
weights = varExp(form = ~ time))

我们对系数 time:event 的重要性感兴趣。我们使用 clubSandwich 包的函数进行估计：
robust_m &lt;- coef_test(m, vcov = &quot;CR2&quot;, cluster = df$user_id)
robust_se_ &lt;- robust_m$SE
robust_p &lt;- robust_m$p_Satt # Satterthwaite 调整后的 p 值

当我们使用 ML 方法时，交互系数在 p &lt; .01 处显著，但使用 REML 时，它不显著。
请注意，我们有大约 490,000 个观察值嵌套在 900 个用户下。
不确定应该使用哪种方法进行此类假设检验。]]></description>
      <guid>https://stats.stackexchange.com/questions/660274/in-lme-should-we-use-ml-or-reml-method-for-hypothesis-tesing</guid>
      <pubDate>Mon, 20 Jan 2025 15:57:24 GMT</pubDate>
    </item>
    <item>
      <title>有没有办法将标准化逻辑回归系数转换为相关性？</title>
      <link>https://stats.stackexchange.com/questions/660270/is-there-a-way-to-convert-a-standardized-logistic-regression-coefficient-into-a</link>
      <description><![CDATA[我正在对皮尔逊相关性 (Rs) 进行荟萃分析。我正在将其他相关性度量转换为 Rs（可能在计算它们之后进行，以防两个度量都是分类的），并使用有关均值和标准差的信息来估计 Cohen&#39;s d，然后将其转换为 R。
然而，有两篇论文，其中一个度量是连续的，另一个是二元的，并且只有优势比及其置信区间可用。我可以计算标准化（对数）优势比，因为我有连续预测变量的标准差，但是，有没有办法将其转换为相关性？当预测变量也是二进制时，我发现了很多关于这种转换的资料，但没有关于预测变量是连续的情况的资料。
编辑
本文：pmc.ncbi.nlm.nih.gov/articles/PMC8096648 指出：“比值比主要取决于受影响个体在人群中的比例或患病率”，“虽然比值比在这种情况下具有很大的实用性，但它们在衡量变量关联或相关程度方面做得很差”，“由于缺乏关于基准率的信息，比值比可能代表变量之间不同程度的相关度”和“比值比可以代表广泛的点双列相关性，具体取决于序数特征的普遍性”。因此，任何公式都应考虑结果的频率。]]></description>
      <guid>https://stats.stackexchange.com/questions/660270/is-there-a-way-to-convert-a-standardized-logistic-regression-coefficient-into-a</guid>
      <pubDate>Mon, 20 Jan 2025 13:02:46 GMT</pubDate>
    </item>
    <item>
      <title>《危险边缘》玩家的贝叶斯分析</title>
      <link>https://stats.stackexchange.com/questions/660260/bayesian-analysis-of-jeopardy-players</link>
      <description><![CDATA[这是我的一个想法。
有一个著名的电视智力竞赛节目叫做Jeopardy。玩家回答琐事问题并相互竞争。这个游戏有一个速度元素——虽然多个玩家可能知道同一个问题的答案，但更快响铃的玩家将优先回答问题。
我很好奇，想估计一下节目中玩家的“真实知识”。我天真地以为玩家可以大致分为 4 类：

第 1 组：回答很多问题且经常正确的玩家
第 2 组：回答很多问题且经常错误的玩家
第 3 组：回答不多但经常正确的玩家
第 4 组：回答不多但经常错误的玩家

我在 R 中模拟了一些数据，以直观地展示其可能的样子：

当我们拥有关于玩家的更多数据（即第 1 组和第 2 组）时，应该有玩家知道多少的模糊性较小。但是，对于数据较少的玩家（即第 3 组和第 4 组），这些玩家可能实际上知道的比数据显示的要多（例如，如果他们参加有相同问题的书面测试，他们的表现会与他们在智力竞赛节目中的表现不同）。这似乎应该受到他们在智力竞赛节目中回答的问题数量的影响，这合乎逻辑。
贝叶斯方法（例如标准贝叶斯、经验贝叶斯）可用于尝试估计“修正”这些玩家的得分比例是多少？
这是我的想法。

标准贝叶斯：对于每个玩家$i$：


$\theta_i$是他们正确回答问题的真实概率
$n_i$是他们尝试的问题数量
$y_i$是正确答案的数量

每个玩家的似然函数遵循二项分布：
$$y_i|\theta_i \sim \text{Binomial}(n_i, \theta_i)$$
$$\theta_i \sim \text{Beta}(\alpha, \beta)$$
$$\theta_i|y_i \sim \text{Beta}(\alpha + y_i, \beta + n_i - y_i)$$
$$E[\theta_i|y_i] = \frac{\alpha + y_i}{\alpha + \beta + n_i}$$
我不确定经验贝叶斯在这里会如何使用。这些方法对这个问题有意义吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660260/bayesian-analysis-of-jeopardy-players</guid>
      <pubDate>Mon, 20 Jan 2025 04:23:31 GMT</pubDate>
    </item>
    <item>
      <title>用于样本量估计的 Bootstrap。序数尺度。分布远离正态</title>
      <link>https://stats.stackexchange.com/questions/660200/bootstrap-for-sample-size-estimation-ordinal-scale-distribution-far-from-norma</link>
      <description><![CDATA[我们经常在约 100 个句子的测试翻译上比较人工翻译或机器翻译系统（每次实验中为 2 到 4 个）。
每个句子的翻译都由一名称职的翻译人员进行评分，通常为 5 分制，有时为 10 分制（分数通常为整数，但可以包括中点，例如 3.5），费用昂贵。这是我们的黄金标准，尽管不同评分者之间的结果并非 100% 一致。这意味着我正在寻找经验法则，而不是最严格的方法。
量表由口头描述定义（“小幅编辑”、“严重改变含义”等）。因此它们本质上是有序的，通常是倾斜的，有时是双峰的。
如果有两种翻译，则使用 Wilcoxon 检验来评估结果，如果有两种以上的翻译，则使用 Friedman 检验来评估结果。
如果我们无法拒绝原假设，我们可能想看看更大的样本是否有帮助。下面是我想要使用的算法。

从原始样本中生成引导样本。使用 Wilcoxon/Friedman 检验评估每个样本，并计算我们可以拒绝原假设的样本比例。这是我们估计的功效。

通过从原始样本中重复抽样来生成更大的样本，但这次取的项目要多于原始样本量。按照与上述第 (1) 点相同的方式估计检验功效。

不断增加生成的样本量，直到达到所需功效或样本量变得过大。


从实际角度来看，这有意义吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660200/bootstrap-for-sample-size-estimation-ordinal-scale-distribution-far-from-norma</guid>
      <pubDate>Sat, 18 Jan 2025 12:37:22 GMT</pubDate>
    </item>
    <item>
      <title>如何根据诊断图改进线性模型</title>
      <link>https://stats.stackexchange.com/questions/660185/how-to-improve-linear-model-based-on-diagnostic-plots</link>
      <description><![CDATA[我一直在处理包含五个变量和一个响应的数据集，如下所示：
.CSV 文件
使用 Python，我从以下代码开始：
import statsmodels.api as sm
import numpy as np
X = subset[[&#39;X1&#39;, &#39;X2&#39;, &#39;X3&#39;, &#39;X4&#39;]]
y = subset[&#39;Y&#39;]
X = sm.add_constant(X)
model = sm.OLS(y, X)
result = model.fit()
plt.figure(figsize=(8, 6))
plt.scatter(fitted_values, residuals, color=&#39;blue&#39;, edgecolors=&#39;k&#39;, alpha=0.7)
plt.axhline(y=0, color=&#39;red&#39;, linestyle=&#39;--&#39;)
plt.title(&#39;残差 x 预测值&#39;)
plt.xlabel(&#39;预测值&#39;)
plt.ylabel(&#39;残差&#39;)
plt.show()

给出模型摘要：

我不知道如何处理“残差 x 预测图”的结果。在探索性分析过程中，我发现这些变量与 Y 有中等相关性，因此我选择它们来构建初始模型。从该图中，我们可以说方差不是恒定的吗？此外，从这种模式中，我们可以获得有关如何改进模型的任何见解吗？

我尝试对 X 变量应用对数或 box-cox 变换，但它并没有改变变量散点图的可视化。我考虑过应用加权最小二乘法，例如，对 Y 大于 100 的位置赋予较低的权重，但图并没有改善，模式几乎相同。或者如果它真的是一个非线性问题，我想如果一些机器学习方法在这里可以更好地发挥作用，但我首先想尝试基本方法。
如果有人能提供一些想法/代码来处理这种情况，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/660185/how-to-improve-linear-model-based-on-diagnostic-plots</guid>
      <pubDate>Fri, 17 Jan 2025 20:39:09 GMT</pubDate>
    </item>
    <item>
      <title>当其中一个分布比较简单时，使用蒙特卡洛估计 Kullback-Leibler 散度</title>
      <link>https://stats.stackexchange.com/questions/660116/estimate-kullback-leibler-divergence-with-monte-carlo-when-one-of-the-distributi</link>
      <description><![CDATA[我感兴趣的是估算$D_\mathrm{KL}(q \parallel p) = \int q(x) \log \frac{q(x)}{p(x)}\,\mathrm dx$，其中$p$是多元高斯分布，$q$是通过神经网络参数化的隐式分布，例如GAN中的生成器网络。背景：分布是隐式的，这意味着虽然我们可以从分布中抽样，但其密度是难以处理的。
一种直接的估计方法是拟合一个 Logistic 分类器 $f(x)$，将 $q$ 的样本与 $p$ 的样本区分开来，然后通过蒙特卡洛近似 KL 为：$\frac{1}{S}\sum_{j=1}^S \log\frac{\sigma(f(x_j))}{1-\sigma(f(x_j))}$，又名密度比技巧。我们用$\sigma(\cdot)$表示 S 型函数。
这种方法的一个明显缺点，也是我想问这个问题的原因，是我们没有利用$p(x)$的密度是已知且简单的事实，参见这篇文章。我们能做些什么来利用$p(x)$的高斯分布来改进估计吗？
非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/660116/estimate-kullback-leibler-divergence-with-monte-carlo-when-one-of-the-distributi</guid>
      <pubDate>Thu, 16 Jan 2025 16:12:32 GMT</pubDate>
    </item>
    <item>
      <title>Rao-Blackwell 定理中估计量和统计量的区别</title>
      <link>https://stats.stackexchange.com/questions/660129/difference-between-estimator-and-statistic-in-rao-blackwell-theorem</link>
      <description><![CDATA[问题：当“估计量”和“统计量”都用于查找参数$\theta$时，这两个词之间有区别吗？我的理解是，估计量“估计”一个参数，而统计量基本上做同样的事情，所以它们似乎是同一事物的名称。在这种情况下，下面的 Rao-Blackwell 定理基本上是在谈论两个估计量，但事实上不同的词语适用于 $T$ 和 $\tilde{\theta}$，这让我不确定……
我看到 Rao-Blackwell 定理说，如果我们有一个充分统计量 $T$ 用于 $\theta$，并且有一个估计量 $\tilde{\theta}$ 用于 $\theta$，使得 $\mathbb{E}(​​\tilde{\theta}^2) &lt; \infty$，我们可以构造 $\hat{\theta} = \mathbb{E}[\tilde{\theta}|T]$ 作为另一个无偏估计量或统计数据，并且 $\hat{\theta}$ 的 m.s.e. 低于 $\tilde{\theta}$。在这种情况下，$T$ 和 $\tilde{\theta}$ 都可以称为“统计数据”，或者都可以称为“估计量”，这样是否有效？]]></description>
      <guid>https://stats.stackexchange.com/questions/660129/difference-between-estimator-and-statistic-in-rao-blackwell-theorem</guid>
      <pubDate>Tue, 14 Jan 2025 21:18:54 GMT</pubDate>
    </item>
    <item>
      <title>连续随机变量的充分统计量的因式分解定理</title>
      <link>https://stats.stackexchange.com/questions/659990/factorization-theorem-for-sufficient-statistics-in-case-of-continuous-random-var</link>
      <description><![CDATA[根据因式分解定理 (Fisher-Neyman)，我们得到一个统计量 $ T(X) $ 充分当且仅当存在因式分解：$ f(x\mid \theta) = g(T(x)\mid \theta)h(x) $。符号遵循 Casella/Berger 第页。 276.
Casella/Berger 在离散情况下给出了证明，指出具体分解的形式如下：$$ P(X=x \mid {\theta}) = P(T(X) = T(x) \mid {\theta})P(X=x \mid T(X) = T(x)) $$
我的问题是：我们能否将这种解释应用于连续情况，并且它总是成立吗？因此：$ f(x\mid \theta) = f(T(x)\mid \theta)f(x\mid T(x)) $ 其中 $f$ 表示相应的 pdf？]]></description>
      <guid>https://stats.stackexchange.com/questions/659990/factorization-theorem-for-sufficient-statistics-in-case-of-continuous-random-var</guid>
      <pubDate>Mon, 13 Jan 2025 22:52:25 GMT</pubDate>
    </item>
    </channel>
</rss>