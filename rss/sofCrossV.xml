<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Mon, 09 Dec 2024 01:24:37 GMT</lastBuildDate>
    <item>
      <title>如何确定回归中逻辑链接函数的最佳用途？</title>
      <link>https://stats.stackexchange.com/questions/658467/how-to-identify-the-best-use-of-the-logit-link-function-in-regression</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/658467/how-to-identify-the-best-use-of-the-logit-link-function-in-regression</guid>
      <pubDate>Mon, 09 Dec 2024 01:13:18 GMT</pubDate>
    </item>
    <item>
      <title>ROUGE-L 分数示例和相应的 ROUGE-n，其中 n = 最长公共子串的长度</title>
      <link>https://stats.stackexchange.com/questions/658463/rouge-l-score-example-and-the-corresponding-rouge-n-where-n-length-of-longest</link>
      <description><![CDATA[我正在关注这个关于 ROUGE 乐谱的视频，他讲解了ROUGE-1、ROUGE-2 和 ROUGE-L。我可以计算前两个，并确认视频中的答案，但没有给出 ROUGE-L 的答案。



这是计算 ROUGE-L 的正确方法吗？

precision = 1/3
召回率 = 1/5
ROUGE-L (F1 分数) = (2 * 1/3 * 1/5) / (1/3 + 1/5) = 2/8 = 0.25

（假设上述 ROUGE-L 值正确）由于最长公共子串是二元组，ROUGE-L 与 ROUGE-2 = 0.33（视频中显示）相比如何？哪一个是更好的指标？
]]></description>
      <guid>https://stats.stackexchange.com/questions/658463/rouge-l-score-example-and-the-corresponding-rouge-n-where-n-length-of-longest</guid>
      <pubDate>Sun, 08 Dec 2024 19:53:03 GMT</pubDate>
    </item>
    <item>
      <title>如何在不最大化聚类质心之间距离的情况下执行 k 均值聚类？</title>
      <link>https://stats.stackexchange.com/questions/658461/how-to-perform-k-means-clustering-without-maximising-distance-between-cluster-ce</link>
      <description><![CDATA[K 均值聚类自然地最小化了到每个聚类质心的聚类内距离之和。用过于简单的术语来说，它实现了 $$min \sum_{k=0}^k\sum_{i=0}^n(s_{ik} ) $$，其中 $s_{ik}$ 是从质心 $k$ 到点 $i$ 的距离。

我想引入 $z_k$ 作为聚类质心与假设已知点 Q（标记为紫色）之间的距离，以及 $c$ 作为额外的常数“成本”对于这个距离。

因此，问题变成了 $$min \sum_{k=0}^k\sum_{i=0}^n(s_{ik} + cz_k) $$
我希望 $c$ 充当参数，以调节 k 均值聚类的“不完美”程度。参数越高，聚类质心与 Q 和彼此的距离越近，并且它们与属于这些聚类的点的距离越远。因此，对于非常大的 $c$，我希望质心接近于 Q，而对于非常小的 $c$，我希望质心处于其&quot;正常&quot;聚类平均位置。  如何实现一种通过考虑质心与 Q 之间（或它们之间）的距离来执行不完美聚类的算法？]]></description>
      <guid>https://stats.stackexchange.com/questions/658461/how-to-perform-k-means-clustering-without-maximising-distance-between-cluster-ce</guid>
      <pubDate>Sun, 08 Dec 2024 18:50:18 GMT</pubDate>
    </item>
    <item>
      <title>处理泊松分布中的负数据</title>
      <link>https://stats.stackexchange.com/questions/658460/dealing-with-negative-data-in-a-poisson-distribution</link>
      <description><![CDATA[我正在开发质谱数据缩减软件，在处理泊松分布中的负数据时遇到了问题。
我的主要目标是计算质谱以安培为单位报告的强度测量值$y$的误差。我假设这些误差遵循泊松分布，该分布基于停留时间 $t$ 内计数的电子数 $N$：
$$N = \frac{yt}{q}$$
其中 $q$ 是电子的电荷。
因此，强度 $\sigma_y$ 的误差为
$$\sigma_y = \sqrt{\frac{yq}{t}}$$
计算这些强度的误差应该可以提高曲线拟合的稳健性。
但是，有时 MS 会报告当信号太小而无法测量时，由于放大器偏移，强度会为 负。放大器偏移会从信号中减去，以考虑热噪声、电压漂移等。每个系统的情况都不同。当来自少数计数离子的电流小于放大器偏移时，数据可能会变成非常小的负数（例如 $-1e^{-15}$，其中低信号约为 $1e^{-15}$ 至 $1e^{-12}$，高信号上升至 $1e^{-8}$）。
负数显然与泊松分布不相容，因为您无法计算某事物的负数。虽然这里的理想解决方案显然是修复放大器偏移以不产生负值，但我的软件无论如何都会遇到负强度，因此必须有某种方式来分配它们错误。
纠正放大器偏移是显而易见的解决方案，但是，由于偏移在系统和序列之间有所不同，我不能依赖它。
我看到了一些前进的方法：

为负数据分配 100% 相对误差。感觉很老套。
找到序列中最负的强度，将其值添加到所有其他强度，并将该强度设置为等于机器 epsilon。我过去曾向拒绝这个想法的用户提出过类似的建议（我同意）。因此，我被限制将所有其他强度提高某个数字以强制它们为非负值。

有哪些不同的处理方法可能更强大或数学上更合理？]]></description>
      <guid>https://stats.stackexchange.com/questions/658460/dealing-with-negative-data-in-a-poisson-distribution</guid>
      <pubDate>Sun, 08 Dec 2024 18:43:36 GMT</pubDate>
    </item>
    <item>
      <title>IV 等级/相关性条件线性代数直觉</title>
      <link>https://stats.stackexchange.com/questions/658451/iv-rank-relevance-condition-linear-algebra-intuition</link>
      <description><![CDATA[考虑以下计量经济模型（IV）：$Y_1 = X&#39;\beta + e$，其中$Y_1 \in \mathbb{R}$是一些感兴趣的结果变量，我们有一组回归量$X = \begin{bmatrix} Z_1 \\ Y_2 \end{bmatrix} \in \mathbb{R}^k$。 ($Z_1 \in \mathbb{R}^{k_1}, Y_2 \in \mathbb{R}^{k_2}, k_1 + k_2 = k \:$) 假设我们可能有一些混杂因素，因此$\mathbb{E}[Xe] \neq 0$。但是假设我们也有一些工具变量$Z=\begin{bmatrix}Z_1\\Z_2 \end{bmatrix} \in \mathbb{R}^{\mathcal{l}}$，使得$\mathbb{E}[Ze] =0, \mathbb{E}[ZZ&#39;]$为psd，并且$\mathbb{E}[ZX&#39;]$的秩为$k$（相关性）。
我对相关性条件很好奇：$\mathbb{E}[ZX&#39;]$的秩必须为$k$。我可以理解这里与 $Cov(X, Z)$ 的联系，但我只是不太清楚为什么秩条件意味着这一点（与 $Z$ 变换对 $X$ 列的作用有关），以及这在某种意义上必须“保留”至少 $X$ 维度才能相关。
此外，在我看来，这似乎与原始 OLS 案例中的秩条件有些相关：$\hat{\beta} = (X&#39;X)^{-1}X&#39;Y_1$，其中秩也必须是 $k$（这次我们希望我们的回归量是线性独立，每个都“提供新的东西”）。这再次将$\beta$的最简单形式概括为$\frac{Cov}{Var}$。（这里的$Var(X)$和$(X&#39;X)$之间有什么联系？）。
总而言之，我缺少一些几何直觉，无法理解这些变换如何告诉我们空间中不同变量的方差/协方差。我一直在尝试复习一些线性代数知识，但还是无法理解。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/658451/iv-rank-relevance-condition-linear-algebra-intuition</guid>
      <pubDate>Sun, 08 Dec 2024 11:34:47 GMT</pubDate>
    </item>
    <item>
      <title>即使在标准化之后，K 均值聚类仍然过于强调单个变量：如何纠正？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/658448/k-means-clustering-is-giving-too-much-emphasis-on-a-single-variable-even-after-n</link>
      <description><![CDATA[即使在基于两个变量的 z 分数进行聚类的情况下，K 均值聚类的结果也主要基于单个变量。我真的不明白为什么下面散点图右下角的点不属于同一组。
问题是：我应该使用另一种聚类算法，还是应该调整 Kmeans（给我的数据添加一定的权重？怎么做？）？
谢谢，
编辑：聚类数任意高，以查看是否发生沿 zscore(C) 的分区。事实证明，Christian Hennig 是对的，在检查了代码后，问题得到了解决（原始数据仍然在标准化值旁边的向量中，我猜这搞乱了整个过程）。感谢您的回答。
]]></description>
      <guid>https://stats.stackexchange.com/questions/658448/k-means-clustering-is-giving-too-much-emphasis-on-a-single-variable-even-after-n</guid>
      <pubDate>Sun, 08 Dec 2024 11:15:20 GMT</pubDate>
    </item>
    <item>
      <title>如何根据连续性校正的卡方检验计算样本量？</title>
      <link>https://stats.stackexchange.com/questions/658456/how-to-calculate-sample-size-based-on-chi-square-test-with-continuity-correction</link>
      <description><![CDATA[
library(stats)
power.prop.test(p1 = .45, p2 = .25, power = .80, sig.level = 0.05, alternative = c(&quot;two.sided&quot;), strict = FALSE)

双样本比例功效计算比较 

n = 88.0928
p1 = 0.45
p2 = 0.25
sig.level = 0.05
power = 0.8
alternative = two.sided

为什么我的样本量计算与这篇 NEJM 发表的论文中的不同？
为什么 prop.power.test 函数计算功率时没有 Yates 连续性校正？但 prop.test 实现了此校正？

我可以使用这个在线计算器获得与本文相同的结果。可能是什么原因？我可以在 r 中获得这个结果吗？
非常感谢你的帮助！！！！
在线计算器：https://homepage.univie.ac.at/robin.ristl/samplesize.php?test=proptest]]></description>
      <guid>https://stats.stackexchange.com/questions/658456/how-to-calculate-sample-size-based-on-chi-square-test-with-continuity-correction</guid>
      <pubDate>Sun, 08 Dec 2024 09:39:29 GMT</pubDate>
    </item>
    <item>
      <title>我可以在 Cox PH 中使用年龄作为随时间变化的协变量吗？</title>
      <link>https://stats.stackexchange.com/questions/658439/can-i-use-age-as-a-time-varying-covariate-in-a-cox-ph</link>
      <description><![CDATA[我正在研究叛乱团体领导人在被迫下台之前会掌管叛乱团体多长时间。我有领导人年份数据，其中包含以下变量：

groupid = 每个团体的唯一 ID 号
leaderid = 每个领导人的唯一 ID 号
tenure = 领导人首次掌权以来的时间（从 0 开始计数）
forcedexit = 领导人被迫下台时取值为 1，其他年份取值为 0
groupage = 团体成立以来的时间（从 0 开始计数）

我的数据集中，叛乱团体活跃的每一年都有一行。数据通常如下所示：
 groupid leaderid forcedexit tenure groupage
&lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 411 1 0 0 0
2 411 1 0 1 1
3 411 1 0 2 2
4 411 2 1 0 3
5 411 2 0 1 4
6 411 2 0 2 5
7 411 2 0 3 6
8 411 3 0 0 7
9 411 3 0 1 8
10 411 3 0 2 9

请注意，新领导人可以通过强制退出以外的方式上台，因此即使 forcedexit = 0，领导人也可能离职（例如，参见上述数据中的 leaderid = 3）。
我想测试一下在较老的群体中，领导人是否更难被罢免。我的直觉是运行 Cox PH 模型（我使用的是 R）：
mod &lt;- coxph(Surv(tenure, forcedexit) ~ groupage, data = df)
如果我运行这个，我会得到 groupage 系数的负且显著的估计值，这与我的直觉一致。但是，出于两个原因，我对这个结果持谨慎态度。
首先，我可以在 Cox PH 中使用 groupage 作为随时间变化的变量吗？我读过的大多数使用 Cox PH 的医学研究都使用不会随时间变化的年龄变量（例如，试验开始时的年龄）。我的 groupage 变量确实会随时间而变化。
其次，我担心 groupage 和 tenure 变量之间的关系会出现一些问题。某些 tenure 值在 groupage 的某些值下不可能存在。例如，tenure = 5 和 groupage = 0 永远不可能存在，因为组必须存在，领导者才能负责该组。因此，较高的 groupage 值可能与较高的 tenure 值一起机械地出现。如果领导者拥有较大的 tenure，则 groupage 也必须较大。
对这两个问题的任何想法都将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/658439/can-i-use-age-as-a-time-varying-covariate-in-a-cox-ph</guid>
      <pubDate>Sun, 08 Dec 2024 05:02:32 GMT</pubDate>
    </item>
    <item>
      <title>plm 和时间效应</title>
      <link>https://stats.stackexchange.com/questions/658464/plm-and-time-effects</link>
      <description><![CDATA[要使用内部模型估计面板数据，我们使用 plm 包
例如，如果我有两个变量，y、x，我们有
m1&lt;- plm(y~ x, data = df, index = c(&quot;Country&quot;, &quot;year&quot;), model=&quot;within&quot;)

在此链接 https://cran.r-project.org/web/packages/plm/vignettes/A_plmPackage.html
它读取

plm 的默认行为是引入单独的效应。使用效果参数，还可以引入：
时间效应（effect = &quot;time&quot;），
个体和时间效应（effect = &quot;twoways&quot;）。

我的问题如下。如果
 m1&lt;- plm(y~ x, data = df, index = c(&quot;Country&quot;, &quot;year&quot;), effect = &quot;time&quot;, model=&quot;within&quot;)

这是否意味着上述回归仅包含时间固定效应而不包含国家固定效应？]]></description>
      <guid>https://stats.stackexchange.com/questions/658464/plm-and-time-effects</guid>
      <pubDate>Sat, 07 Dec 2024 23:18:54 GMT</pubDate>
    </item>
    <item>
      <title>计算具有两个交叉随机效应的随机效应模型的边际效应</title>
      <link>https://stats.stackexchange.com/questions/658421/calculate-marginal-effects-for-random-effects-model-with-two-crossed-random-effe</link>
      <description><![CDATA[我试图获取两个交叉随机效应的边际效应（使用 STAN 或 brms）。我了解如何按照 McElreath 的书和 Kurtz 的同一本书的 brms 版本对单个随机效应进行此操作。我不确定如何对多个随机效应执行此操作。例如，获取“向左拉”的概率Kurtz 翻译的《统计再思考》模型 12.5 中的区块或参与者的边际。对我来说，做类似下面的事情并将这些随机效应添加到固定效应中是有意义的。
random_effect = rnorm(n = 1000, mean = 0, sd = (post$sd_actor_Intercept + post$sd_block_Intercept)

这合理吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/658421/calculate-marginal-effects-for-random-effects-model-with-two-crossed-random-effe</guid>
      <pubDate>Sat, 07 Dec 2024 19:01:59 GMT</pubDate>
    </item>
    <item>
      <title>适合的统计检验</title>
      <link>https://stats.stackexchange.com/questions/658420/suitable-statistical-test</link>
      <description><![CDATA[我有 2 个组 - 曾经在我的网站上活跃的人和从未活跃的人。活动是基于某些操作的指标 - 两组人都访问了网站。
每个组在网站上的第一周使用了一组功能（它是一种平台，人们是用户），我想知道哪些功能与“变得活跃”相关。
例如：
100 个活跃的人中有 20 个在该组中使用了功能 A 100 次，而总共使用了 3000 次功能。
在 50 位从未活跃过的用户中，有 3 位使用了功能 A，总共使用了 1500 次。



曾活跃
功能
使用的帐户
帐户总数
使用的功能
功能总使用量




1
A
20
100
100
3000


0
A
3
50
20
1500



我还可以添加特征使用情况的方差和标准差。
我可以在这里做什么测试来表明使用特征 A 的人与活跃度的相关性明显更高？
此外，它对账户比例本身进行任何测试并使用该测试声称特征 A 与活跃度显著相关是否有意义？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/658420/suitable-statistical-test</guid>
      <pubDate>Sat, 07 Dec 2024 18:45:42 GMT</pubDate>
    </item>
    <item>
      <title>考虑 HGAM 中的非独立性和自相关性</title>
      <link>https://stats.stackexchange.com/questions/658383/accounting-for-non-independence-and-autocorrelation-in-hgam</link>
      <description><![CDATA[我目前正在尝试拟合 HGAM，以模拟两种处理中鱼类日常活动模式的差异。数据是通过高分辨率遥测技术收集的，目前我已估算出三个湖泊（湖泊 A-C）中 30 条鱼（每种处理约 15 条）在约 35 个实验日内每小时（1-24）游动的总距离（米）。这些数据具有清晰的层次结构，我正尝试用随机效应来解释它（如果我的语法有误，请告诉我，因为我习惯于在 brms /lme4 中建模）。我正在为每个湖泊创建一个单独的模型，以降低模型复杂性。模型结构如下：
mod &lt;- gam(as.numeric(total_distance) ~ 
Treatment + exp_day_z + weight_z +
s(hour, by = Treatment, k = 22, bs = &quot;cc&quot;) + # 这是我感兴趣的交互
s(individual_ID, bs = &#39;re&#39;) + # 个体级随机截距
s(individual_ID, exp_day_z, bs = &#39;re&#39;) + # 实验日的个体级随机斜率（居中）
s(individual_ID, hour, bs = &#39;re&#39;), # 一小时的个体级随机斜率
data = filter(perch, lake == &#39;A&#39;),
method = &quot;REML&quot;,
knots=list(hour=c(0.5, 24.5)),
family = Gamma(link = &#39;log&#39;))

然而，残差中似乎仍然存在相当大的自相关性。

在阅读了 Eric Pedersen 的 HGAM 论文、观看了他的和Gavin Simpson的精彩教程，并阅读了一些较早的帖子，我决定在模型中尝试一个自回归项来解释这一点，如下所示：
perch &lt;- perch %&gt;%
allocate(individual_ID, exp_day, hour) %&gt;%
mutate(start = if_else(is.na(lag(individual_ID)) | individual_ID != lag(individual_ID), TRUE, FALSE))

mod2 &lt;- bam(as.numeric(total_distance) ~ 
Treatment + exp_day_z + weight_z +
s(hour, by = Treatment, k = 22, bs = &quot;cc&quot;) + 
s(individual_ID, bs = &#39;re&#39;) +
s(individual_ID, exp_day_z, bs = &#39;re&#39;) +
s(individual_ID, hour, bs = &#39;re&#39;),
data = filter(perch, lake == &#39;A&#39;),
method = &quot;fREML&quot;,
rho = 0.6,
AR.start = start,
discrete = TRUE,
knots=list(hour=c(0.5, 24.5)),
family = Gamma(link = &#39;log&#39;))

这似乎改善了残差自相关（注意：此 acf 图是使用 check_resid(mod2) 生成的，其中灰色条表示标准残差，黑色条表示标准化 AR1 校正残差）。

与我之前的模型相比，该模型似乎也产生了合理的预测，其 CI 略宽（注意：对比度是使用 marginaleffects 包中的 comparisons 函数估计的）。

我仍然对一些事情有点不确定：

我是否应该在同一个模型中包含随机效应和自回归项？
我的随机效应结构格式是否正确，其中小时和实验日是个体级随机斜率？
使用 bam() 函数时选择特定 rho 值的理由是什么？
最后，有点不相关，有没有办法从一个整体模型中获得湖泊和处理特定的平滑度，而不是分别对三个湖泊进行建模（我最初尝试使用 by =互动（治疗，湖泊）但这需要很长时间才能运行，但也许这是唯一的方法）？

如能提供任何帮助，我们将不胜感激！
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/658383/accounting-for-non-independence-and-autocorrelation-in-hgam</guid>
      <pubDate>Fri, 06 Dec 2024 16:25:03 GMT</pubDate>
    </item>
    <item>
      <title>如何计算这个替代方案的密度？</title>
      <link>https://stats.stackexchange.com/questions/658289/how-to-compute-the-density-of-this-alternate-proposal</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/658289/how-to-compute-the-density-of-this-alternate-proposal</guid>
      <pubDate>Thu, 05 Dec 2024 00:15:38 GMT</pubDate>
    </item>
    <item>
      <title>具有局部二次估计的函数导数的交叉验证带宽</title>
      <link>https://stats.stackexchange.com/questions/658285/cross-validated-bandwidth-for-the-derivative-of-the-function-with-local-quadrati</link>
      <description><![CDATA[我正在尝试非参数化地估计函数 g(x) 的一阶导数。我正在使用局部多项式（二次）程序估计 $g(x)$。我知道如何计算 $g(x)$ 的留一交叉验证带宽，但是，我想要做的是直接估计 $g’(x)$ 的交叉验证带宽。我的猜测是，导数的最佳带宽不一定与 $g(x)$ 的最佳带宽相同，因此 $g(x)$ 的交叉验证实际上不是我应该做的。但是，我还没有找到任何关于如何对 $g’(x)$ 进行交叉验证带宽的资料，这要困难得多，因为我没有与 $Y_i$ 等效的导数。
有关于如何做到这一点的资源吗？任何 R、Python 或 Stata 中的实现也将不胜感激！
到目前为止，我正在实施 Fan 和 Gijbels (1995) RSC 标准，但感觉它不如适当的交叉验证那么充分（特别是因为从原始带宽到导数带宽的路径仅由预定的调整常数驱动）。也许我可以在交叉验证中使用相同的调整常数？但这感觉“太简单了”。
当我对函数（而不是导数）使用交叉验证或 RSC 或经验法则（来自 Fan 和 Gijbels，1996）时，我得到的带宽通常“太小”，因为导数太不稳定。我尝试了模拟数据，结果总是比建议的带宽大得多。但我想用“科学”标准来证明我的选择是正确的，而不是随意的...
PS：我也尝试了 R 中的 locfit 包，默认情况下它实现了自适应带宽（使用最近邻算法，我猜是取 70% 的最近邻居并据此确定自适应带宽，但我不太确定）。如果我也能找到“最佳”的最近邻居数量，我会没问题的。]]></description>
      <guid>https://stats.stackexchange.com/questions/658285/cross-validated-bandwidth-for-the-derivative-of-the-function-with-local-quadrati</guid>
      <pubDate>Wed, 04 Dec 2024 22:47:00 GMT</pubDate>
    </item>
    <item>
      <title>修正二项式 GLMM 以反映物种出现的时间趋势</title>
      <link>https://stats.stackexchange.com/questions/658211/correct-binomial-glmm-for-temporal-trends-in-species-occurrences</link>
      <description><![CDATA[我有一个包含 80 个物种的数据集，这些物种是在两个时间段（历史/最近）从大约 120 个水体中采样的。仅考虑每个水体中物种的存在/不存在。长格式的数据如下所示：
water_no_name 物种 成功时间 
水体 1 物种 A 0 t1 
水体 2 物种 A 0 t1 
水体 3 物种 A 1 t1 
水体 4 物种 A 0 t1 
水体 5 物种 A 0 t1 
水体 1 物种 A 1 t2 
水体 2 物种 A 0 t2 
水体 3 物种 A 0 t2 
水体 4 物种 A 0 t2 
水体 5 物种 A 0 t2 
水体 1 物种 B 1 t1 
水体 2 物种 B 0 t1 
水体 3 物种 B 0 t1 
水体 4 物种 B 1 t1 
水体 5 物种 B 1 t1 
水体 1 物种 B 1 t2 
水体 2 物种 B 0 t2 
水体 3 物种 B 1 t2 
水体 4 物种 B 1 t2 
水体 5 物种 B 0 t2 

我想检查哪些物种的频率随时间显著增加或减少。
因此，我决定使用水体（= 地点）作为随机因素来计算二项式 GLMM，以解释重复测量设计。
我不确定这两种方法中的哪一种更合适：
1) 为每个物种计算单独的 GLMM：
mod &lt;- glmer(success ~ time + (1 | water_no_name), family = binomial)
使用这种方法，我将对物种数量进行 p 值校正 (FDR)。
2) 使用时间和物种之间的相互作用计算所有物种的组合 GLMM：
mod &lt;- glmer(success ~ time * species + (1 | water_no_name), family = binomial)
为了分析相互作用，我将使用这种方法进行事后分析，以查看对比 t1/t2 对哪些物种有意义：
emmeans(mod, pairwise ~ time |物种)
在计算这两种模型时，结果大致相似，但计数较低的物种会有一些差异。
我理解，一个主要的区别是，复杂模型 (2) 对所有物种使用相同的随机位点效应，而使用单独的模型 (1) 我会为每个物种获得不同的随机位点效应（参见 几个单物种 GLMM -&gt; 一个多物种 GLMM（随机效应语法？））
我仍然不确定哪种方法更适合我的情况。我发现更简单的方法 (1) 更直观地解释，也足以回答我的问题。我也不确定是否应该让所有物种都具有随机站点效应。
有人能给我建议我应该选择哪种方法吗？
或者你推荐另一种方法？]]></description>
      <guid>https://stats.stackexchange.com/questions/658211/correct-binomial-glmm-for-temporal-trends-in-species-occurrences</guid>
      <pubDate>Tue, 03 Dec 2024 14:44:29 GMT</pubDate>
    </item>
    </channel>
</rss>