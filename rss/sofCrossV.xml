<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Fri, 16 Aug 2024 12:29:46 GMT</lastBuildDate>
    <item>
      <title>混合因子分析仪：似然函数的正确表达？</title>
      <link>https://stats.stackexchange.com/questions/652926/mixture-of-factor-analyzers-correct-expression-for-likelihood-function</link>
      <description><![CDATA[我正在阅读这些笔记关于混合因子分析器，其中描述了以下生成模型：

数据$x$的条件分布（在第 3 节，方程 9）表示为：
$$
P(x \mid z, \omega_j) = \mathcal{N}(\mu_j+\Lambda_j z, \Psi)
$$
其中 $\omega_j$ 表示 $j$ 个因子分析器（或者混合成分，如果我们可以这样称呼它）的索引，并且 $z \mid \omega_j \sim \mathcal{N}(0,I)$。
我的问题是：我说 $x \mid \omega_j \sim \mathcal{N}(\mu_j, \Lambda_j \Lambda_j^\top + \Psi)$ 是否正确？如果我想找到最有可能生成 $x$ 的混合成分，我会最大化以下函数：
$$
L(\omega_j) = P(x \mid \omega_j) = \mathcal{N}(x ; \mu_j, \Lambda_j \Lambda_j^\top+ \Psi) 
$$
对吗？
如果正确，为什么 $P(\omega_j)$ 在这个似然函数中不重要？
这个表达式看起来很像高斯混合模型中的后验推断，即找到每个聚类对给定数据的“责任”。当然，除了在这种情况下，协方差矩阵具有稀疏结构，并且我正在寻找最大似然估计量（我想？）。]]></description>
      <guid>https://stats.stackexchange.com/questions/652926/mixture-of-factor-analyzers-correct-expression-for-likelihood-function</guid>
      <pubDate>Fri, 16 Aug 2024 11:53:45 GMT</pubDate>
    </item>
    <item>
      <title>估计未知分布分位数的高置信上限</title>
      <link>https://stats.stackexchange.com/questions/652924/estimating-high-confidence-upper-bound-for-quantile-of-unknown-distribution</link>
      <description><![CDATA[我获得了一个分布未知的随机变量 $\mathbb R$ 上的 $X$。
我想确定获得 X 的 $1-\alpha$ 分位数的高置信上限 $\hat Q_{1-\alpha}$ 所需的最小样本量 $n$。
具体来说，我对高百分位数感兴趣，比如第 95 或第 99 个百分位数。
对于给定的置信水平 $1-\epsilon$，我想确保 $\Pr(\Pr(X&gt;\hat Q_{1-\alpha})&lt;\alpha) \geq 1-\epsilon$，而不管 $X$ 的分布如何。
虽然这个问题看起来很笼统，但我不确定如何使用经典统计方法来解决它。
直观地说，我会从我的样本中估计 $1-\alpha$ 分位数。
使用 bootstrapping 等方法，我也可以获得该参数的置信区间。
但是，我不确定如何根据基础样本的大小$n$准确量化这些方法的不确定性。
我的问题有两个方面。
首先，是否有统计工具可以回答这个问题并根据样本大小估计参数估计器的置信度？我似乎缺乏查找相关信息的统计背景。如果能提供来源或关键字来为我指明正确的方向，我将不胜感激。
其次，我知道这个问题可以用 PAC-Learning 的方法来解决。
但是，尽管这个问题相对普遍，但我也找不到使用这种界限的例子。
我怀疑有更好的工具来回答这个问题，或者这个问题一开始就不是很有趣，因为人们可以简单地假设某种分布，然后使用参数方法。
如果有人能告诉我这两个假设是否正确，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/652924/estimating-high-confidence-upper-bound-for-quantile-of-unknown-distribution</guid>
      <pubDate>Fri, 16 Aug 2024 11:09:53 GMT</pubDate>
    </item>
    <item>
      <title>使用缺失的消费者数据进行偏好映射</title>
      <link>https://stats.stackexchange.com/questions/652923/preference-mapping-using-missing-consumer-data</link>
      <description><![CDATA[我正在使用基于 R 中 SensoMineR 包中的 carto() 函数的感官和消费者喜好数据进行偏好映射分析。
结构

感官数据：每行代表一个产品，每列代表所有产品特定属性的给定分数，
消费者数据：每行代表一个产品，每列代表许多消费者的给定总体喜好分数。

但是由于设计原因，我的消费者数据缺少一些案例，因为每个消费者只对 3 种产品进行评分。在这种情况下，数据集中填充了许多 NA。并且 carto 函数会抛出错误
Error in `[&lt;-.data.frame`(`*tmp*`, missing, value = c(NA_real_, NA_real_, : 
新列会在现有列后留下空洞
此外：警告消息：
在 mean.default(MatH, na.rm = TRUE) 中：
参数不是数字或逻辑：返回 NA

我以为我可以使用这 3 个产品喜好分数的平均值来估算给定消费者的 NA。这是一种合法的方法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/652923/preference-mapping-using-missing-consumer-data</guid>
      <pubDate>Fri, 16 Aug 2024 10:59:39 GMT</pubDate>
    </item>
    <item>
      <title>如何按年龄、性别和身高调整数据</title>
      <link>https://stats.stackexchange.com/questions/652921/how-to-adjust-data-by-age-sex-and-height</link>
      <description><![CDATA[以下是一个完全虚构的例子（基于真实的例子），因此它没有意义。
我有一个包含 100 个受试者的数据库，其中有他们的年龄、性别、身高、公司职位（初级、中层管理和高层管理）和认知测试（结果范围为 0-20）。然而，我发现这三组在年龄、性别和身高方面存在统计上的显著差异，所以我想纠正他们的认知分数测试，根据这些差异进行调整。我该怎么做？
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/652921/how-to-adjust-data-by-age-sex-and-height</guid>
      <pubDate>Fri, 16 Aug 2024 10:14:10 GMT</pubDate>
    </item>
    <item>
      <title>非参数和半参数 cif</title>
      <link>https://stats.stackexchange.com/questions/652920/non-parametric-and-semi-parametric-cif</link>
      <description><![CDATA[我最近尝试使用 {survival 来深入了解竞争风险分析。
使用的数据来自 {survival。
library(survival)
library(ggsurvfit)

mgus2$etime &lt;- with(mgus2, ifelse(pstat==0, futime, ptime))
event &lt;- with(mgus2, ifelse(pstat==0, 2*death, 1))
mgus2$event &lt;- factor(event, 0:2, labels=c(&quot;censor&quot;, &quot;pcm&quot;, &quot;death&quot;))

我使用非参数方法计算了累积发生率函数，并得出了事件或死亡在时间的概率t。
可以通过 summary_mfit2$lower 和 summary_mfit2$upper 显示概率的置信区间。
此外，我绘制了累积发生率函数：
mfit2 &lt;- survfit(Surv(etime, event) ~ sex, data=mgus2)

summary_mfit2 &lt;- summary(mfit2, times=c(20))

&gt; summary_mfit2
调用：survfit(formula = Surv(etime, event) ~ sex, data = mgus2)

sex=F 
time n.risk n.event Pr((s0)) Pr(pcm) Pr(death)
20 540 93 0.852 0.0175 0.130

sex=M 
time n.risk n.event Pr((s0)) Pr(pcm) Pr(death)
20 621 137 0.818 0.0106 0.171

ggcuminc(mfit2, consequence = c(&quot;pcm&quot;, &quot;death&quot;)) +
add_confidence_interval()

接下来，我尝试了半参数方法，如下所示：
cfit1 &lt;- coxph(Surv(etime, event) ~ sex, mgus2, id=id)

dummy &lt;- expand.grid(sex = c(&quot;F&quot;, &quot;M&quot;))

csurv &lt;- survfit(cfit1, newdata = dummy)

temp &lt;- summary(csurv, times = c(20))

&gt; temp 
调用：survfit(formula = cfit1, newdata = dummy)

数据 1 
时间 n.风险 n.事件 Pr((s0)) Pr(pcm) Pr(死亡) 
20.0000 1161.0000 230.0000 0.8668 0.0111 0.1221 

数据 2 
时间 n.风险 n.事件 Pr((s0)) Pr(pcm) Pr(死亡) 
20.0000 1161.0000 230.0000 0.8388 0.0102 0.1510 

问题：

由于只使用了一个二元协变量（性别），使用半参数模型是否有意义方法还是应该使用非参数方法？
当有人描述使用特定原因危害时，我的理解是指使用半参数方法来模拟风险，还是我错了？
非参数方法为时间 t 的风险提供 CI。如何为半参数方法推导出这些 CI？
如果推导出 CI，如何绘制半参数对象 csurv 的置信区间？函数 ggcuminc() 不起作用，使用基本函数 plot()（确实有效）很麻烦。
]]></description>
      <guid>https://stats.stackexchange.com/questions/652920/non-parametric-and-semi-parametric-cif</guid>
      <pubDate>Fri, 16 Aug 2024 09:18:11 GMT</pubDate>
    </item>
    <item>
      <title>用类高斯因子近似高维积分的有效方法</title>
      <link>https://stats.stackexchange.com/questions/652919/efficient-methods-for-approximating-high-dimensional-integrals-with-gaussian-lik</link>
      <description><![CDATA[我正在寻找一种计算效率高的方法来近似地评估以下形式的高维积分：
$\int f(\textbf{x}) \prod_i g_i(x_i) d\textbf{x}$
其中 $f(\mathbf{x}) = (\mathbf{x}&#39;\mathbf{A}\mathbf{x})^{-k}$，其中 $k$ 是一个大整数，而 $\mathbf{x}$ 的维度通常为 $n &gt; 5,000$。函数$g_i(x_i)$是概率密度函数，可以用高斯分布$\mathcal{N}(x_i; \mu_i, \tau_i)$来近似，因为$g_i(x_i)$的拉普拉斯近似很容易推导。一般而言，$g_i(x_i)$ 可以假设为具有简单且易处理的单变量形式，并且易于从中采样。
到目前为止我尝试过的方法：

蒙特卡洛积分：虽然从 $g_i(x_i)$ 采样很简单，但收敛速度非常慢。一般而言，基于采样的方法可能不可行，因为计算成本高。

拉普拉斯近似：这种方法不切实际，因为$\mathbf{A}$的结构、$f(\mathbf{x})$的行为（当$\mathbf{x} = 0$时会飙升至无穷大）以及高维性使得以计算成本低廉的方式找到被积函数的模式非常具有挑战性。

关注$g_i(x_i)$：当所有$\tau_i$都很小时，拉普拉斯近似在近似被积函数模式$\mathbf{\mu}$下进行近似，甚至简单地使用$\int f(\mathbf{x}) \prod_i g_i(x_i) d\mathbf{x} \approx (\mathbf{\mu}&#39;\mathbf{A}\mathbf{\mu})^{-k}$ 都非常有效。但是，假设所有 $i$ 的 $\tau_i$ 都很小并不总是可行的。

直接近似：尝试近似随机变量 $g(\mathbf{x})$ 的高斯近似值 $\mathbf{x}&#39;\mathbf{A}\mathbf{x}$ 的密度，然后估计所得分布的第 $k$ 个负矩，这有点有效，但结果并不令人满意。


我很好奇，考虑到$g_i(x_i)$ 是（近似）高斯分布且独立的。考虑到此问题的结构，还有其他近似方法可能在计算上可行吗？如能提供任何替代方向的建议或提示，我们将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/652919/efficient-methods-for-approximating-high-dimensional-integrals-with-gaussian-lik</guid>
      <pubDate>Fri, 16 Aug 2024 09:16:27 GMT</pubDate>
    </item>
    <item>
      <title>比较不同调查方法生态调查数据在两个时间点的变化</title>
      <link>https://stats.stackexchange.com/questions/652918/comparing-changes-between-two-time-points-in-ecological-survey-data-with-differe</link>
      <description><![CDATA[我正在分析一些水下生态调查数据，希望能得到一些关于最佳统计方法的建议。调查是在 2009 年和 2024 年在同一地点进行的，所以我有两个时间点的数据。我的目标是了解底栖生物覆盖随时间的变化，以及深度、暴露和沉积等环境因素如何影响这些变化。
有一件重要的事情需要注意：2009 年的调查使用了线截距法，而 2024 年的调查使用了点截距法。我关心的是如何确保这些不同方法的数据具有可比性。
考虑到这些因素，考虑到环境变量是潜在的预测因素，您会推荐哪种统计测试来分析这些数据？任何关于处理调查方法差异的提示也将不胜感激！
非常感谢您的帮助！]]></description>
      <guid>https://stats.stackexchange.com/questions/652918/comparing-changes-between-two-time-points-in-ecological-survey-data-with-differe</guid>
      <pubDate>Fri, 16 Aug 2024 09:06:38 GMT</pubDate>
    </item>
    <item>
      <title>这个GARCH(1,1)模型方程正确吗？</title>
      <link>https://stats.stackexchange.com/questions/652915/is-this-equation-of-the-garch1-1-model-correct</link>
      <description><![CDATA[我是 GRACH 模型的新手，读过很多论文，但对下面提供的方程感到困惑。方程 (15) 正确吗？如果 $z_{t}$ 是误差，为什么作者将它们定义为残差，然后用它们来拟合 copula 模型？根据我的理解，$\alpha$ 系数必须乘以误差，而不是标准差。]]></description>
      <guid>https://stats.stackexchange.com/questions/652915/is-this-equation-of-the-garch1-1-model-correct</guid>
      <pubDate>Fri, 16 Aug 2024 05:40:29 GMT</pubDate>
    </item>
    <item>
      <title>我可以比较 4 点李克特量表和 5 点李克特量表的数据吗？</title>
      <link>https://stats.stackexchange.com/questions/652914/can-i-compare-data-from-a-4-point-likert-scale-and-a-5-point-likert-scale</link>
      <description><![CDATA[我正在计划撰写一份政策文件，比较一组国家/地区公众的政策偏好，并试图解释为什么某些国家/地区倾向于强烈支持冰淇淋，但不支持披萨，反之亦然。（这些是任意示例，实际类别与特定政策问题有关。）
为了说明我的论点，我想将每个国家/地区的数据散点图，以便冰淇淋认可的百分比在 x 轴上，披萨认可的百分比在 y 轴上。
我自己没有进行过任何调查（本文旨在探讨未来开展此类项目的可行性）。目前，我对每个问题都进行了单独的调查。两项调查都声称提供了相关国家人口的代表性样本，并且是在大致相同的时间进行的。
冰淇淋调查要求参与者在 4 点李克特量表上表示赞成或反对，没有中立选项；披萨调查要求参与者在 5 点李克特量表上表示赞成或反对，有中立选项。
我想通过将两个样本的“非常赞成”和“有点赞成”加在一起来绘制每个选项的净赞成率。当然，由于冰淇淋调查没有中立选项，它可能迫使一些人选择“有点赞成”或“有点反对”而不是中性选项。
是否有某种方法可以规范化数据，以使两个数据集具有可比性，或者我是否需要做更多研究来找到在李克特量表上具有相同点数的研究？]]></description>
      <guid>https://stats.stackexchange.com/questions/652914/can-i-compare-data-from-a-4-point-likert-scale-and-a-5-point-likert-scale</guid>
      <pubDate>Fri, 16 Aug 2024 04:49:23 GMT</pubDate>
    </item>
    <item>
      <title>瑞利或指数参数估计的预测区间</title>
      <link>https://stats.stackexchange.com/questions/652891/prediction-interval-for-rayleigh-or-exponential-parameter-estimate</link>
      <description><![CDATA[我正在研究瑞利和指数随机变量。它们具有方便的闭式置信区间，可用于参数估计。但现在我们感兴趣的是做出以下形式的陈述：“给定一些实验数据，我们可以对未来实验的参数估计值有什么期望？”我明白我要求的是 MLE 上的预测区间。
我最初认为这会是置信区间 (CI) 平方之类的东西。例如，“第一个实验在 $\hat{\sigma}$ = [1, 2] 上产生了 90% 的置信区间。因此，真正的 $\sigma$ 有 90% 的概率处于该范围内，这意味着 90% 的类似实验将产生包含区间 [1, 2] 部分内容的 90% 置信区间。”但有 10% 的概率，真正的 $\sigma$ 不在该区间，在这种情况下……嗯，我不知道。但考虑到指数分布的“良好”和正态相邻性，我怀疑预测区间存在一个闭式表达式。有人能建议一个吗？
（我可以使用蒙特卡罗技术验证公式，但推导可能更有帮助。我没有成功找到答案，因为预测区间在回归场景中被广泛用于获取未来值。）

示例问题陈述：

设 $X \sim \operatorname{Exponential}(\lambda)$，其中 $\lambda$ 未知。
估计 $\lambda$ 的实验抽取 $n$ 个 $X$ 样本，并估计 $\hat{\lambda} = \bar{x}$。
给定一个实验的结果 $\hat{\lambda_1}$，通过重复实验（使用相同的 $n$）找到值 $\hat{\lambda_2}$ 的 90% 预测区间。

为了通过模拟验证预测区间公式，我将：

设置任意 $\lambda^*$ 和整数 $n &gt; 2$。
进行第一个实验：从用 $\lambda^*$ 参数化的指数 RNG 中抽取 $n$ 个样本。计算 $\hat{\lambda_1} = \bar{x}$。
使用公式计算预测区间的上限，$\operatorname{PI}_{90\%}$。
模拟重复实验并计算 $\hat{\lambda_i} &gt; \operatorname{PI}_{90\%}$ 的次数。
如果该频率收敛至 5%，则公式得到验证。
]]></description>
      <guid>https://stats.stackexchange.com/questions/652891/prediction-interval-for-rayleigh-or-exponential-parameter-estimate</guid>
      <pubDate>Thu, 15 Aug 2024 18:11:04 GMT</pubDate>
    </item>
    <item>
      <title>R MSM 包：如何标记事件和流行疾病？</title>
      <link>https://stats.stackexchange.com/questions/652883/r-msm-package-how-to-flag-incident-and-prevalent-disease</link>
      <description><![CDATA[我正在尝试使用 R 中的 msm 包创建一个 2 状态马尔可夫模型（状态 1 到 2 和 2 到 1）。hpv_state 在感染时为 2，在未感染时为 1，在状态未知时为 999。我想创建一个变量来标记每次感染事件是普遍发生的（从跟踪开始连续为 2）还是偶发事件（在观察到 1 后为 2）。当我在 hpv_state=1 时将 infection_type 保留为缺失时，模型似乎会忽略这些条目。当 hpv_state=1 或 999 时，我该如何正确标记 infection_type？
infection_type 变量的用途仅用于 2 到 1 的转换：
hpv16_inft.msm &lt;- msm(hpv_state ~ years_followup, subject=patientid, data=df_hpv16, qmatrix=Q, censor=999, covariates = list(&quot;2-1&quot; = ~infection_type))

数据示例：
patientid hpv_state infection_type 
10 1-0002 2 流行 
11 1-0002 1 NA 
12 1-0002 2 发病率
13 1-0002 2 事件 
14 1-0002 999 NA 
15 1-0002 999 NA 
16 1-0002 1 NA 

我尝试为 NA（即“neg”）分配一个值，但从 infection_type=neg 到 hpv_state=1 的转变是不合逻辑的。]]></description>
      <guid>https://stats.stackexchange.com/questions/652883/r-msm-package-how-to-flag-incident-and-prevalent-disease</guid>
      <pubDate>Thu, 15 Aug 2024 16:50:20 GMT</pubDate>
    </item>
    <item>
      <title>R 中的二元预测分类，预测因子由多个值组成</title>
      <link>https://stats.stackexchange.com/questions/652858/binary-predictive-classification-in-r-with-predictors-consisting-of-multiple-va</link>
      <description><![CDATA[由于维度问题，我目前正在努力构建二元 glm 预测分类器。
我有一个包含 N 个样本的数据集，其中每个样本都有 M 个条目（基因）的值，并且是病例或对照。但是，每个基因由 1 ... k 个数据点组成，每个数据点都是一个数值，范围从 0 到 1，代表与基因重叠的 CpG 位点的甲基化。并且每个基因的数据点数量依次变化（因此意味着每个基因由不同大小的数据框表示）。分类器应该是二进制的，根据通过交叉验证选择的一组预测因子（基因）的值将样本分配为病例/对照。
我之前构建了二元预测分类器，其中每个样本的一个值代表一个基因（即来自 NxM 数据框），但我不知道如何处理当前情况，其中我的 NxM 数据框中的每个 1...M 列实际上可以说是 NxK 数据框而不是 Nx1 列。
我曾想过通过从 N 个样本中的每一个的 1...k 数据点中取出中值来将 NxK 数据框表示为 Nx1 列，或者通过选择 NxK 数据框中的一列来表示数据框（基于适合我的研究问题的选择标准），但理想情况下，我希望在构建分类器时指向代表基因的 NxK 数据框。
有人有这种类型的经验吗问题，如果可以，是否有可能从预测器的数据框构建一个预测分类器，其中预测器依次指向单独的数据框？
如果没有，我从每个样本的 1...k 个数据点中取出中值的方法是否有效？
我之前构建过分类器，其中每个基因都由一个值表示，但我没有处理此类问题的经验。]]></description>
      <guid>https://stats.stackexchange.com/questions/652858/binary-predictive-classification-in-r-with-predictors-consisting-of-multiple-va</guid>
      <pubDate>Thu, 15 Aug 2024 10:04:31 GMT</pubDate>
    </item>
    <item>
      <title>具有趋势和季节性的时间序列数据中的因果推断</title>
      <link>https://stats.stackexchange.com/questions/652855/causal-inference-in-time-series-data-with-trend-and-seasonality</link>
      <description><![CDATA[各位统计学家大家好，
我有一个关于时间序列数据中的因果推断和影响评估的问题，特别是在处理趋势、季节性和政策干预或结构性中断时。
在横截面数据中，我们通常应用 OLS 或非参数模型（如 KNN）来估计独立变量 𝑋 和 𝑍 对因变量 𝑌 的影响。然后，对于 OLS，我会检查假设；对于 KNN，我会检查 𝑌 与预测变量 𝑋 和 𝑍 之间的关系，而无需假设参数形式。
但是，我将处理时间序列数据，其中 𝑌、𝑋 和 𝑍 是同时显示趋势和季节性模式的月度序列。此外，在研究期间的中间可能存在已知的政策干预/趋势中断。
我的问题是：

哪些模型最适合估计时间序列数据中的因果关系，特别是在处理趋势、自相关和结构中断时？

我正在考虑对数据进行季节性调整，但我该如何处理趋势成分？由于趋势存在时均值不是恒定的，我应该采取什么方法来处理这个问题？

我如何将政策干预或趋势突破纳入我的因果分析？

这些时间序列模型做出了哪些假设，我如何在因果推断的背景下验证这些假设？


我不是一个铁杆统计学家，所以如果你能推荐任何相关的资料或教科书来解释这些概念——最好是用 R 中的实际例子/代码——那就太好了。任何指导或参考都将不胜感激。
提前感谢您的见解和帮助！]]></description>
      <guid>https://stats.stackexchange.com/questions/652855/causal-inference-in-time-series-data-with-trend-and-seasonality</guid>
      <pubDate>Thu, 15 Aug 2024 10:03:20 GMT</pubDate>
    </item>
    <item>
      <title>用于建模财务回报的 AR(1) 过程的时间缩放</title>
      <link>https://stats.stackexchange.com/questions/652799/time-scaling-of-ar1-process-for-modelling-financial-returns</link>
      <description><![CDATA[过程：
考虑一个均值为零的 AR(1) 过程，*
$\lambda_t = \kappa \cdot \lambda_{t-1} + \omega_t$，
其中 $\kappa = 0.9$，$\omega \sim N(0, \sigma_{\omega}^2)$，并且 $\sigma_{\omega}^2 = 0.00027$。 $\lambda_0$ 的初始值取自平稳分布 $N \left(0, \frac{\sigma_{\omega}^2}{(1-\kappa^2)} \right)$
我使用此过程生成长度为 $T=672$ 的样本。
* 我知道，对于股票收益建模而言，均值为零是不现实的，但我的问题并不取决于此选择。

上述时间序列应解释为每月收益（以百分点表示），即 672 个月的观测值。
问题：
什么是合适的参数值生成总共 14,112 个每日观测值（即$672 \times 21$，如果我们假设一个月内有 21 个交易日）以符合上述（每月）流程？月回报率是给定月份内所有 21 天回报率的累计乘积。
尝试（在 R 中）：
DAYS &lt;- 21
T &lt;- 672
kappa &lt;- 0.9
variance_omega &lt;- 0.00027

get_init_lambda &lt;- function(variance) return(rnorm(n = 1, mean = 0, sd = sqrt(variance / (1-(kappa)^2))))

#### 月度分析

set.seed(1234)

lambda_T &lt;- vector(mode = &quot;numeric&quot;, length = T)

lambda_shock &lt;- rnorm(T, mean = 0, sd = sqrt(variance_omega))

lambda_T[1] &lt;- kappa * get_init_lambda(variance_omega) + lambda_shock[1]
for(i in 2:T) lambda_T[i] &lt;- kappa * lambda_T[i-1] + lambda_shock[i]

acf(lambda_T)$acf[2]
# [1] 0.9064949 # 符合预期

var(lambda_T)
# [1] 0.001547795

#### 每日分析

set.seed(1234)

kappa_daily &lt;- 0.90 # ??? 如何设置 kappa_daily，使月收益 AC = 0.9？

lambda_T_daily &lt;- vector(mode = &quot;numeric&quot;, length = T * DAYS)

lambda_shock_daily &lt;- rnorm(T * DAYS, mean = 0, sd = sqrt(variance_omega / DAYS))

lambda_T_daily[1] &lt;- kappa_daily * get_init_lambda(variance_omega / DAYS) + lambda_T_daily[1]
for(i in 2:(T * DAYS)) lambda_T_daily[i] &lt;- kappa_daily * lambda_T_daily[i-1] + lambda_shock_daily[i]

# 检索月末指数；假设每个月有 21 个交易日
begin_month &lt;- seq(1, T * DAYS, 21)
end_month &lt;- c(tail(begin_month, -1) - 1, T * DAYS)

lambda_monthly_aggregate &lt;- vector(mode = &quot;numeric&quot;, length = T)

for(i in 1:T){
month_ind &lt;- (begin_month[i]):(end_month[i])

daily_ret_within_month &lt;- 0.01*lambda_T_daily[month_ind] # 收益以 % 表示
monthly_return &lt;- 100*(cumprod(1+daily_ret_within_month) - 1) # 累计每日收益

lambda_monthly_aggregate[i] &lt;- monthly_return[DAYS] # 检索累计。 21 天后返回
}

# 与上述值不相同：自相关性太低，mth 方差返回值太高！
&gt; acf(lambda_monthly_aggregate)$acf[2]
[1] 0.2988249

var(lambda_monthly_aggregate)
[1] 0.01494742

]]></description>
      <guid>https://stats.stackexchange.com/questions/652799/time-scaling-of-ar1-process-for-modelling-financial-returns</guid>
      <pubDate>Wed, 14 Aug 2024 16:01:50 GMT</pubDate>
    </item>
    <item>
      <title>简单的 OLS 来测量相关性</title>
      <link>https://stats.stackexchange.com/questions/652774/simple-ols-to-measure-correlation</link>
      <description><![CDATA[我有两个变量，X 和 Y，我有充分的理由相信它们是同时确定的。
$$Y = a_{1} + b_{1}X + u_{1}\tag{1}$$
$$X = a_{2} + b_{2}Y + u_{2}\tag{2}$$
我的问题是：只要我远离任何因果关系陈述，我是否仍可以使用简单的 OLS 研究变量之间的关系？
我感兴趣的是做出这种陈述：“X 的单位增加与 Y 的预测值 b1 的变化相关”。我不想暗示因果关系的方向，我只想描述变量如何共同移动。在这种情况下，我承认明显的内生性，但报告系数是否仍然具有误导性？
更新：数据纯粹是横截面的]]></description>
      <guid>https://stats.stackexchange.com/questions/652774/simple-ols-to-measure-correlation</guid>
      <pubDate>Wed, 14 Aug 2024 10:25:27 GMT</pubDate>
    </item>
    </channel>
</rss>