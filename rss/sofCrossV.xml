<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Jun 2024 01:03:22 GMT</lastBuildDate>
    <item>
      <title>辛普森悖论如何解释结果</title>
      <link>https://stats.stackexchange.com/questions/648717/simpsons-paradox-how-interpret-results</link>
      <description><![CDATA[我正在构建 GLM 来研究环境变量对青蛙占有率和丰度的影响（负二项式）。我遇到了估计值反转的问题，从我在网上找到的信息来看，这似乎是由辛普森悖论引起的。受其影响的两个变量是 bio6（最冷月份的平均最低温度）和年降雨量。Bio6 在单独建模时具有负估计值，但在与其他参数建模时具有正估计值。年降雨量有同样的现象，但方向相反（从正估计值变为负估计值）。
顶级模型有 bio6+rain+bio8+bio17（其中 bio8 是最潮湿季度的平均温度，即澳大利亚东部的夏季，bio17 是最干旱季度的降水量，即冬季）。我使用 VIF&lt;10 作为过滤器以避免多重共线性，但在这种情况下 VIF 甚至&lt;5（bio8 约为 4.4，其他所有都小于 3）。此外，我还尝试避免将 bio6 与 bio8 以及 bio17 与年降雨量相结合，以避免出现有关同一现象的信息，尽管它们之间的相关性并不高，但这种情况仍然会发生。例如，我做了一个模型 bio6+bio17，bio6 仍然从负值反转为正值（VIF 2.5）。同样，我做了一个模型 bio8+年降雨量，年降雨量仍然从正值反转为负值（VIF 1.38）。无论变量是否标准化，都会发生这种情况。现在我陷入了困境，因为我必须讨论这些变量对青蛙的影响，我不知道该如何解释这些相反的结果。我该如何处理这种情况？]]></description>
      <guid>https://stats.stackexchange.com/questions/648717/simpsons-paradox-how-interpret-results</guid>
      <pubDate>Wed, 05 Jun 2024 22:24:53 GMT</pubDate>
    </item>
    <item>
      <title>自回归交叉滞后模型</title>
      <link>https://stats.stackexchange.com/questions/648716/autoregressive-cross-lagged-models</link>
      <description><![CDATA[我正在研究一个自回归交叉滞后模型，该模型有两个测量值和三个时间点。从 $t_1$ 到 $t_2$ 的路径是显著的，但从 $t_2$ 到 $t_3$ 的路径都不显著。我对编码/模型/等非常不熟悉，我想知道最好的前进路径是什么？我读过关于限制某些变量的文章，但我真的不明白如何最好地描述/解释这些不显著的路径。我知道这可能会令人困惑，所以感谢您的耐心。]]></description>
      <guid>https://stats.stackexchange.com/questions/648716/autoregressive-cross-lagged-models</guid>
      <pubDate>Wed, 05 Jun 2024 21:24:33 GMT</pubDate>
    </item>
    <item>
      <title>似然比未呈具有正确自由度的卡方分布（威尔克斯定理）</title>
      <link>https://stats.stackexchange.com/questions/648715/likelihood-ratios-not-distributed-as-a-chi2-distribution-with-the-correct-dof-w</link>
      <description><![CDATA[我对混合模型执行贝叶斯推理，使得 μ 是混合中特征的混合权重
p(x|μ,theta) = μ p_feature(x|theta) + (1-μ) p_nofeature(x|theta)。
我计算 T(μ) =-2* ( log( p(x|μ=0, theta(μ=0)) - log p(x | μ*, theta*) )，其中 x 是固定的，其中
theta(μ=0) = \arg max_theta p(x | μ=0, theta)（零假设）
μ*, theta* = \arg max_{μ,theta) p(x | μ, theta)
我无法获得可能性，因此我通过最大化 p(μ, theta|x) / p(μ, theta) 找到上述两个解决方案，这与p(x | μ, theta) / p(x)。
我通过从 theta 的先验中抽样来计算几个 x 的 LLR 统计量，因为 x= f(μ, theta)。https://towardsdatascience.com/the-likelihood-ratio-test-463455b34de9 告诉我我应该获得一个卡方分布，其中 dof 是整个模型的模型参数之间的差异 - 零假设。但是，这是我从两个自由度为 6 的独立模型获得的结果。


我得到了第二个模型的自由度卡方 19，并且几乎是均匀分布。我期望自由度卡方 6。我遗漏了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/648715/likelihood-ratios-not-distributed-as-a-chi2-distribution-with-the-correct-dof-w</guid>
      <pubDate>Wed, 05 Jun 2024 21:21:27 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么技术可以帮助我实现所需的计算而无需读取全部 1000 万行？</title>
      <link>https://stats.stackexchange.com/questions/648714/is-there-any-technique-that-i-can-use-to-achieve-the-required-computation-withou</link>
      <description><![CDATA[我有一个来自蒙特卡罗模拟的 15 条蛋白质链的数据文件。该文件包含 1000 万个 r_end_to_end 3D 向量，每行 3 x 15 = 45 列。
我的任务是找到模拟稳定的确切行。换句话说，我需要找到自相关时间。换句话说，我需要找到 Tau0 时间。
现在，问题是数据文件很大。
因此，读取全部 1000 万行对于频繁测试、绘图生成、曲线拟合等来说并不实用。
是否有任何技术可以在不读取全部 1000 万行的情况下实现所需的计算？]]></description>
      <guid>https://stats.stackexchange.com/questions/648714/is-there-any-technique-that-i-can-use-to-achieve-the-required-computation-withou</guid>
      <pubDate>Wed, 05 Jun 2024 21:17:52 GMT</pubDate>
    </item>
    <item>
      <title>计算动态时间规整（DTW）之前对时间序列数据进行规范化</title>
      <link>https://stats.stackexchange.com/questions/648713/normalization-of-time-series-data-before-computing-dynamic-time-warping-dtw</link>
      <description><![CDATA[在计算动态时间扭曲 (DTW) 距离之前，我对时间序列数据的适当归一化方法感兴趣。
情况：
我有四个时间序列（用两种不同的绿色色调、紫色和红色着色），它们基于滑动窗口皮尔逊相关性。这意味着这些图的每个数据点都反映了皮尔逊相关系数。
此外，我有一个测量 x 的滑动窗口结果（蓝色时间序列）。仅出于绘图目的，我已将此时间序列归一化为 0 到 1 的区间。这个时间序列的实际值范围更像是 ~1.1 到 ~1.8。
可以看出，紫色和红色时间序列似乎与蓝色时间序列收敛。我的目的是（以定量的方式）显示紫色/红色时间序列与蓝色时间序列之间的距离小于（或接近）两个绿色时间序列与蓝色时间序列之间的距离。
我不想使用相关性或互相关性，因为这种关系是非线性的或随时间不稳定。因此，我考虑在这里应用 DTW。
Eamonn Keogh 指出，在应用 DTW 之前，z 分数标准化在约 99% 的情况下是合适的，甚至是必要的。 （例如，请参见此处：https://www.cs.unm.edu/~mueen/DTW.pdf）
问题：简单地将 z 分数标准化应用于每个时间序列，无论是在动态（滑动窗口）皮尔逊相关时间序列上，还是在测量 x（蓝色）时间序列上，都可以吗？
或者我应该只将蓝色时间序列标准化为 [-1, 1] 的区间，即皮尔逊相关的理论区间？我感到很困惑，不知道这里什么是合理的，而且考虑到蓝色时间序列是非平稳的。
]]></description>
      <guid>https://stats.stackexchange.com/questions/648713/normalization-of-time-series-data-before-computing-dynamic-time-warping-dtw</guid>
      <pubDate>Wed, 05 Jun 2024 21:12:48 GMT</pubDate>
    </item>
    <item>
      <title>如何根据测试数据的分布均匀程度来评估预测准确率</title>
      <link>https://stats.stackexchange.com/questions/648712/how-to-grade-the-prediction-accuracy-with-how-evenly-distributed-the-test-data-i</link>
      <description><![CDATA[我正在编写一个房价预测系统，试图预测房价是上涨还是下跌。每个地理区域都有一个平均房价变量。我为每个地理区域都设计了一个预测模型，即一个模型预测一个地理区域的一个价格。
为了评估每个模型，我会查看模型预测 20 个之前历史价格的正确程度（使用实际价格作为测试数据）。
但如果测试中的所有历史价格总是上涨或下跌，我就不会对模型抱有太大信心。
我想看看模型是否使用分布更均匀的历史价格进行测试，即完美的模型测试是 50% 的价格上涨，50% 的价格下跌，而完美的模型会预测 50% 的价格上涨和 50% 的价格下跌。
我如何评估每个模型的预测准确性，并包括测试数据的分布情况？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/648712/how-to-grade-the-prediction-accuracy-with-how-evenly-distributed-the-test-data-i</guid>
      <pubDate>Wed, 05 Jun 2024 21:10:17 GMT</pubDate>
    </item>
    <item>
      <title>验证二元预测模型</title>
      <link>https://stats.stackexchange.com/questions/648709/validating-binary-prediction-model</link>
      <description><![CDATA[假设我们有一个模型，可以预测二元事件 $e$ ($0$ 或 $1$)，且只有一个输出 $p$ (预期概率 $e$ 发生)。
如果我们能够将 $p$ 与 $e$ 的真实值 ($0$ 或 $1$) 进行比较，那么我们如何验证我们的模型有多好。我相信我们可以得出一个残差，预期 - 实际为 $p - e$。我们如何利用这个残差做一些有意义的事情，并得出类似于 $R^2$ 的东西，或者一些指标来告诉我们使用生成的 $p$ 值并将其与 $e$ 的真实值进行比较，我们的模型有多好？]]></description>
      <guid>https://stats.stackexchange.com/questions/648709/validating-binary-prediction-model</guid>
      <pubDate>Wed, 05 Jun 2024 19:58:57 GMT</pubDate>
    </item>
    <item>
      <title>证明二元分类多元回归的两种不同方法的等价性</title>
      <link>https://stats.stackexchange.com/questions/648706/proving-the-equivalence-of-two-distinct-approaches-to-multiple-regression-for-bi</link>
      <description><![CDATA[我被这个使用多元线性回归来解决二元分类问题的特殊问题所困扰（注意：它没有考虑逻辑版本或任何其他 GLM 方法）。
问题指出，在 $K = 2$ 个类的情况下，使用多元回归作为分类方法有两种可能的方式：

一种方法是使用一列指示变量并选择最接近插值值的类（即，我们正在进行两个线性回归，规则要求取与两个预测中的最大值相对应的类）；
在另一种情况下，规则是对两个类进行单个线性回归，如果预测为 $&gt; 0.5$，则预测 $1$，如果预测为 $&lt; 0.5$ 预测 $0$。

证明这两种方法是等效的。
我对这个问题的看法如下。
如果我们应用第 2 点中描述的单一线性回归，我们有
$$ \hat y = \begin{cases} 0 &amp; \mbox{if } \beta_0 + x^T \beta &lt; 0.5 \\ 1 &amp; \mbox{if } \beta_0 + x^T \beta \geq 0.5 \end{cases} $$
如果我们应用第 1 点中描述的方法，则让 $Z$ 为响应变量（即具有一列的矩阵）
$$ Z = XB + E$$
$B$ 是 $(p \times 1)$ 参数矩阵，并且 $E$ 是误差矩阵。假设一个通用点 $x$（与新观察相关）的“插值”为：
$$ \hat z = \hat B^T x, \quad \quad (x \in \mathbb{R}^K) $$
此时，$\hat z$ 中的 2 个倾向性度量（说概率是错误的）为 1。要将该观察分类为一个组，我将选择得分最高的那个。
最后，我没有看到代数方法来证明这两种方法是相同的。]]></description>
      <guid>https://stats.stackexchange.com/questions/648706/proving-the-equivalence-of-two-distinct-approaches-to-multiple-regression-for-bi</guid>
      <pubDate>Wed, 05 Jun 2024 19:42:56 GMT</pubDate>
    </item>
    <item>
      <title>混合效应随机森林（MERF）中的固定效应训练模型检查</title>
      <link>https://stats.stackexchange.com/questions/648705/fixed-effect-trained-model-inspection-in-mixed-effects-random-forest-merf</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/648705/fixed-effect-trained-model-inspection-in-mixed-effects-random-forest-merf</guid>
      <pubDate>Wed, 05 Jun 2024 19:30:56 GMT</pubDate>
    </item>
    <item>
      <title>我应该如何完全去相关数字信号？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/648703/how-should-i-go-about-completely-decorrelating-a-digital-signal</link>
      <description><![CDATA[因此，我正在研究实时信号压缩，我需要想出最佳卷积来最小化传入数据的熵（然后我将对其进行压缩），据我所知，这是通过完全去相关数据的卷积（最小化自相关）实现的。我还知道有些人称之为“信号白化”。
据我目前了解，这个问题的连续版本已经解决，其中涉及“标准化”功率谱密度，然后应用反向傅里叶变换。在离散情况下，我认为问题在于给定一组有限的数据点来近似该功率谱密度分布。您是否知道任何可以完全去相关（或非常接近）有限数据的好方法？我的假设是否正确，即在连续情况下这是一个已解决的问题，但在离散情况下存在计算问题？此外，是否可以公平地假设，预先计算卷积并将其用于来自同一来源的新数据也会导致良好的去相关？
到目前为止，我的许多假设可能都是错误的，对此我深表歉意。我对这个领域还很陌生。我非常感谢对我的知识的任何纠正。]]></description>
      <guid>https://stats.stackexchange.com/questions/648703/how-should-i-go-about-completely-decorrelating-a-digital-signal</guid>
      <pubDate>Wed, 05 Jun 2024 19:20:22 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中的 GAM 中整合站点级随机效应和二维坐标样条</title>
      <link>https://stats.stackexchange.com/questions/648695/incorporating-site-level-random-effects-and-2-dimensional-spline-of-coordinates</link>
      <description><![CDATA[使用 R 中 mgcv 包中的 gam() 构建广义加性模型时，样条项 s() 中的一个选项是 bs= 参数。选项的描述在此处给出，但简而言之，有许多平滑选项可用于生成基于地理坐标的样条。还有一个选项 bs = &quot;re&quot; 来合并随机效应。
我正在处理一种情况，我在不同位置的采样点重复进行现场访问。我的目标是既结合重复采样的随机效应，又结合每个站点的两个坐标值的样条函数。
模型调用如下所示：
gam(formula = DV ~ IV + s(lat,lon) + s(site,bs = &quot;re&quot;),
data = df)

以这种方式结合随机效应的方法有哪些，这里是否需要两个单独的术语？即，我可以用以下方式对其进行建模吗？
gam(formula = DV ~ IV + s(lat,lon,bs = &quot;re&quot;),
data = df)

或者类似的东西，假设纬度和经度对是唯一的并且与分类站点完全相等？]]></description>
      <guid>https://stats.stackexchange.com/questions/648695/incorporating-site-level-random-effects-and-2-dimensional-spline-of-coordinates</guid>
      <pubDate>Wed, 05 Jun 2024 15:47:10 GMT</pubDate>
    </item>
    <item>
      <title>如何检验 B 样条曲线中的变化点是否具有统计显著性？</title>
      <link>https://stats.stackexchange.com/questions/648686/how-to-test-if-change-point-in-b-spline-is-statistically-significant</link>
      <description><![CDATA[我想测试一下结点两侧斜率之间的差异是否具有统计显著性。忽略此示例中的模型假设违规，如何查看下图中 x 和 y 之间的关系/斜率是否发生了显著变化？我之前从未使用过带有 b-spline 命令的线性回归，p 值是否表示这两条线之间的差异？我使用的是数据“mtcars”作为我自己数据的替代品。
示例：

library(ggplot2)
library(splines)

data(mtcars)

wt=3 处是否有显著变化点？
ggplot(data = mtcars, aes(x = wt, y = mpg)) +
geom_point() + 
geom_vline(xintercept=3, linetype=&quot;dashed&quot;, color = &quot;red&quot;) + 
geom_smooth(method=&quot;lm&quot;,
formula= y ~ splines::bs(x, knots = c(3), degree = 1), se=F)


# 构建模型：
mod &lt;- lm(mpg ~ bs(wt, knots = c(3), degree = 1), data = mtcars)

summary(mod)

调用：
lm(formula = mpg ~ bs(wt, knots = c(3), degree = 1), data = mtcars)

残差：
最小值 1Q 中位数 3Q 最大值 
-3.2027 -1.9072 -0.7627 0.9611 6.1070 

系数：
估计标准差误差 t 值 Pr(&gt;|t|) 
(截距) 32.246 1.383 23.313 &lt; 2e-16 ***
bs(wt, knots = c(3), degree = 1)1 -12.884 1.764 -7.306 4.78e-08 ***
bs(wt, knots = c(3), degree = 1)2 -21.133 1.932 -10.937 8.32e-12 ***
---
显著性代码：0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

残差标准误差：29 个自由度上的 2.689
多重 R 平方：0.8137，调整后的 R 平方：0.8009
F 统计量：2 和 29 DF 上的 63.34，p 值：2.614e-11

可能的相关问题：
在结点之间分离 B 样条线并比较每个部分的拟合优度]]></description>
      <guid>https://stats.stackexchange.com/questions/648686/how-to-test-if-change-point-in-b-spline-is-statistically-significant</guid>
      <pubDate>Wed, 05 Jun 2024 02:18:10 GMT</pubDate>
    </item>
    <item>
      <title>比较匹配的前后李克特量表数据（n = 40）</title>
      <link>https://stats.stackexchange.com/questions/648644/comparing-matched-pre-post-likert-scale-data-n-40</link>
      <description><![CDATA[我有需要分析的前后匹配李克特量表数据（约 20 个问题）。1) 分析这些数据的最佳方法是什么？我最初认为使用均值并进行配对样本 t 检验是可以的，但这似乎不适用于李克特量表数据。配对样本 wilcoxin 合适吗？一些背景信息，这是评估数据，用于查看课程前后学生的知识和技能。]]></description>
      <guid>https://stats.stackexchange.com/questions/648644/comparing-matched-pre-post-likert-scale-data-n-40</guid>
      <pubDate>Tue, 04 Jun 2024 20:53:20 GMT</pubDate>
    </item>
    <item>
      <title>为什么聚合可以减少噪音</title>
      <link>https://stats.stackexchange.com/questions/648622/why-aggregation-reduces-the-noise</link>
      <description><![CDATA[我正在寻找一个证据来证明为什么将数据聚合（汇总）到更高级别（例如，将变量的每日值聚合到每周或每月）可以减少噪音。有人知道如何证明这一点吗？基本上，为什么更细粒度的数据比聚合的、更粗的数据更不稳定和不稳定？]]></description>
      <guid>https://stats.stackexchange.com/questions/648622/why-aggregation-reduces-the-noise</guid>
      <pubDate>Tue, 04 Jun 2024 16:10:47 GMT</pubDate>
    </item>
    <item>
      <title>如何根据一组百分位数重建正态分布？</title>
      <link>https://stats.stackexchange.com/questions/648589/how-can-i-reconstruct-a-normal-distribution-from-a-set-of-percentiles</link>
      <description><![CDATA[我有一个正态分布变量的第 3、10、50、90 和 97 个百分位数值，我希望生成一个数据集，使我能够查询其他百分位数值（例如，第 67 个百分位数值）。
我（天真地，我敢肯定）尝试了以下操作，但失败了：
underlying_data = np.random.normal(loc=0.0, scale=1.0, size=[1000])

percentiles = np.percentile(underlying_data, [3, 10, 50, 90, 97])

#

generated_data = []

for i in range(3):
generated_data.append(underlying_data[0])

for i in range(7):
generated_data.append(underlying_data[1])

for i in range(80):
generated_data.append(underlying_data[2])

for i in range(7):
generated_data.append(underlying_data[3])

for i in range(3):
generated_data.append(underlying_data[4])

print(&quot;from underground distribution: &quot;, np.percentile(np.array(underlying_data), [67]))
print(&quot;from generated distribution: &quot;, np.percentile(np.array(generated_data), [67]))

输出：
from underground distribution: [0.44470627]
from generated distribution: [-0.73888881]
]]></description>
      <guid>https://stats.stackexchange.com/questions/648589/how-can-i-reconstruct-a-normal-distribution-from-a-set-of-percentiles</guid>
      <pubDate>Tue, 04 Jun 2024 06:25:11 GMT</pubDate>
    </item>
    </channel>
</rss>