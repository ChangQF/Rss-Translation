<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sun, 12 Jan 2025 03:27:28 GMT</lastBuildDate>
    <item>
      <title>零协方差何时意味着独立？</title>
      <link>https://stats.stackexchange.com/questions/659900/when-does-null-covariance-imply-independence</link>
      <description><![CDATA[众所周知，如果 $X,Y$ 是独立随机变量，则 $\text{Cov}(X,Y) = 0$。认为这一事实的反面成立也是一种很常见的谬论。但是，我的问题是，反面成立的情况不是经常发生吗？
例如，如果随机向量 $(X,Y)$ 呈正态分布，是一个二维随机向量，则 $X,Y$ 独立就等同于它们具有零协方差。
继续下去，由于 CLT，假设 $(X,Y)$ 呈正态分布是很常见的（即使在大多数情况下不是这样）。因此，尽管零协方差在学术意义上并不意味着独立性，但从更实际的意义上来说却意味着独立性？]]></description>
      <guid>https://stats.stackexchange.com/questions/659900/when-does-null-covariance-imply-independence</guid>
      <pubDate>Sun, 12 Jan 2025 03:11:46 GMT</pubDate>
    </item>
    <item>
      <title>分层抽样和可变大小聚类的置信区间</title>
      <link>https://stats.stackexchange.com/questions/659897/confidence-intervals-for-stratified-sampling-and-variable-sized-clusters</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/659897/confidence-intervals-for-stratified-sampling-and-variable-sized-clusters</guid>
      <pubDate>Sun, 12 Jan 2025 00:25:49 GMT</pubDate>
    </item>
    <item>
      <title>对已进行 FDR 校正的数据批次进行 FDR 校正</title>
      <link>https://stats.stackexchange.com/questions/659893/fdr-correction-on-batches-of-already-fdr-corrected-data</link>
      <description><![CDATA[背景：
这个问题来自组学背景，其中大量数据有时需要将整个数据集分成几批，并用一些分析软件分别处理它们。
示例：
对于此示例，请考虑两个大小相等的测量数据批次。在每个批次中，可以用各种置信度分数识别 50000 个分子。在 1% 阈值下应用错误发现校正后，每个数据集中仍保留 1000 个具有显著 q 值的分子。任务是重新组合两个批次并获得一个仍具有 1% FDR 的数据集。
问题：
我的问题是，BH 校正（或任何其他类型的调整）是否可用于重新校正批次 q 值并允许组合批次而不会夸大错误发现？任何有关现有实现和/或统计背景的评论都将不胜感激。
示例方法：
我能想到一种方法，其中从每个批次中取出 50000 个 q 值并对整个 100000 个值集运行 BH 校正，同时取 q 值 &lt; 0.01。这基本上相当于解释批次 q 值，就像解释单个数据集中的原始 p 值一样。]]></description>
      <guid>https://stats.stackexchange.com/questions/659893/fdr-correction-on-batches-of-already-fdr-corrected-data</guid>
      <pubDate>Sat, 11 Jan 2025 20:52:03 GMT</pubDate>
    </item>
    <item>
      <title>Grover 算法是否比同时进行的贝叶斯更新更具优势？</title>
      <link>https://stats.stackexchange.com/questions/659892/does-grover-s-algorithm-offer-advantages-over-simultaneous-bayesian-updates</link>
      <description><![CDATA[最近我一直在学习 Grover 算法，该算法用于量子计算中的非结构化搜索。该算法的思路是这样的：您有一个大型未排序列表，并且想要找到某个东西（称为目标）的位置。您假设目标在列表中，但您不知道它在列表中的位置，并且想要找到这个位置。在开始搜索之前，您为列表中的每个项目分配相等的概率（从技术上讲，您为每个项目分配一个幅度，但平方幅度对应于概率）。例如，如果列表中有 100 个项目，则每个项目的（先验）概率为 1%。您可以通过使用这些概率初始化一个 100 个元素的向量来实现这一点。然后，您执行一系列计算来更新此概率向量，并多次执行这些计算（大约是列表大小的平方根，在本例中为 10，即 100 的平方根），直到（希望）表示目标的向量元素的概率很大（理想情况下为 1）。然后，您使用概率作为权重对向量索引进行采样（量子计算和物理学家称之为“测量”），希望您获得的采样向量索引对应于包含目标的向量元素。
Grover 算法（在某些条件下）比传统计算替代方案的速度快了二次。其中一个原因是 sqrt{100}=10 向量更新中的每一个实际上都会同时更新所有 100 个向量元素，这有时被称为量子并行性。
然而，我认为这种方法的精神与贝叶斯更新非常相似甚至相同。我到底错过了什么？例如，我曾经研究过一种贝叶斯聚类算法，我们逐一更新每个观测与其他 $N - 1$ 个观测聚类的概率，并将结果存储在一个向量中。显然，我应该考虑将该更新计算重新表述为单个操作，然后我可以同时更新所有概率。除了这种并行性（这似乎并不是 Grover 算法或量子计算所独有的）之外，Grover 算法是否还有一些额外的东西可以使其比某些可以同时更新所有向量元素的贝叶斯计算更具优势？
顺便说一句，这个论点可以扩展到任何阶的张量，而不仅仅是向量，尽管合理的更新计算可能很难得出。]]></description>
      <guid>https://stats.stackexchange.com/questions/659892/does-grover-s-algorithm-offer-advantages-over-simultaneous-bayesian-updates</guid>
      <pubDate>Sat, 11 Jan 2025 20:42:17 GMT</pubDate>
    </item>
    <item>
      <title>跨维度网格上的后验</title>
      <link>https://stats.stackexchange.com/questions/659890/posterior-on-a-grid-across-dimensions</link>
      <description><![CDATA[我已经注意到，虽然每维 100 个点可以很好地近似一维后验，但当你进入高维（3 维或 4 维）时，我通常必须增加每维的点数才能获得良好的近似值。这一定是由于“高维欧几里得空间的广阔性”，正如维基百科的距离函数部分所指出的那样。
有没有估计一下，每维应该使用多少个点才能正确覆盖 N 维的整个空间？]]></description>
      <guid>https://stats.stackexchange.com/questions/659890/posterior-on-a-grid-across-dimensions</guid>
      <pubDate>Sat, 11 Jan 2025 18:39:46 GMT</pubDate>
    </item>
    <item>
      <title>并行化蒙特卡罗模拟最简单的方法是什么？</title>
      <link>https://stats.stackexchange.com/questions/659889/what-is-the-easist-way-to-parallelize-a-monte-carlo-simulation</link>
      <description><![CDATA[据我所知，并行化蒙特卡罗模拟归结为将问题分解为子问题，并行运行它们以产生多个马尔可夫链，最后将马尔可夫链连接在一起。
现在的问题是，没有直接的方法来连接马尔可夫链。
并行化蒙特卡罗模拟最简单的方法是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659889/what-is-the-easist-way-to-parallelize-a-monte-carlo-simulation</guid>
      <pubDate>Sat, 11 Jan 2025 18:02:09 GMT</pubDate>
    </item>
    <item>
      <title>固定效应面板回归中的特征选择和异常值检测</title>
      <link>https://stats.stackexchange.com/questions/659888/feature-selection-and-outlier-detection-in-panel-regression-with-fixed-effects</link>
      <description><![CDATA[我试图拟合以下具有固定实体效应的面板回归
$$Y_{it} = \alpha_i + \sum_j \beta_jX^{(j)}_{it} + \epsilon_{it},$$
其中索引$j$标记了不同的特征。我知道其中一些特征对所有实体都很重要（领域知识），而有些可能只对某些实体很重要。此外，某些特征 $X^{(j)}_{it}$ 可能包含某些实体的错误数据（或极端异常值）。
我正在寻找此模型的诊断方法，以便能够检测：

某些特征仅与面板中的一小部分实体相关，并识别该子集；
某些特征包含异常值或高杠杆点，并识别它们。
]]></description>
      <guid>https://stats.stackexchange.com/questions/659888/feature-selection-and-outlier-detection-in-panel-regression-with-fixed-effects</guid>
      <pubDate>Sat, 11 Jan 2025 17:54:42 GMT</pubDate>
    </item>
    <item>
      <title>为什么时间序列的差分无助于在 SARIMA 模型中使其平稳？</title>
      <link>https://stats.stackexchange.com/questions/659898/why-does-differencing-of-time-series-not-help-to-make-it-stationary-in-the-sarim</link>
      <description><![CDATA[我在 SARIMA 模型中差分后无法获得平稳时间序列。
我是时间序列分析的初学者，但我读到时间序列 Y_t = \nabla^d (1-L^s)^D X_t 应该是平稳时间序列。然而，在用值 s=12 和不同的 d 和 D 值进行差分后，我的时间序列似乎不是平稳分析 ACF 函数。我也尝试过一开始 lambda=0 的 Box–Cox 变换，但同样没用。
这是我的原始数据
原始数据 (data_ts)
它是非平稳的。
经过 lambda = 0 的 Box-Cox 变换之后
data_bc &lt;- BoxCox(data_ts, lambda=0)

Box-Cox 变换之后
这是 ACF 函数：
ACF 函数
然后我尝试对我的时间序列进行差分，结果如下：
ts_plot(diff(data_bc))
Acf(diff(data_bc))

差分后的时间序列图。
我的时间的 ACF系列。
在我看来，它可以是静止的，但从 ACF 来看，它不是。可能是因为季节性？因此，我尝试对滞后 s=12 的时间序列进行差分，然后再次进行差分（s=12、d=1、D=1）。
ts_plot(diff(diff(data_bc, lag=12)))
Acf(diff(diff(data_bc, lag=12)))

新时间序列
ACF
再次分析 ACF 函数，它似乎不是平稳的。我错了吗？那我该怎么办？
Box-Cox 变换、差分、消除季节性。我希望获得平稳的时间序列。]]></description>
      <guid>https://stats.stackexchange.com/questions/659898/why-does-differencing-of-time-series-not-help-to-make-it-stationary-in-the-sarim</guid>
      <pubDate>Sat, 11 Jan 2025 14:36:19 GMT</pubDate>
    </item>
    <item>
      <title>联合高斯的充分条件</title>
      <link>https://stats.stackexchange.com/questions/659878/sufficient-condition-for-jointly-gaussian</link>
      <description><![CDATA[设 $Z = f(X) + V$，其中 $X$ 和 $V$ 是独立的高斯随机变量，并且 $f:\mathbb{R}\to \mathbb{R}$。
联合 $(Z,X)$ 为高斯的必要充分条件是什么？我怀疑 $f$ 应该是仿射线性的，比如 $ax + b$，但我该如何正式证明这一点？]]></description>
      <guid>https://stats.stackexchange.com/questions/659878/sufficient-condition-for-jointly-gaussian</guid>
      <pubDate>Sat, 11 Jan 2025 12:55:03 GMT</pubDate>
    </item>
    <item>
      <title>无法拟合高斯混合模型，估计错误参数</title>
      <link>https://stats.stackexchange.com/questions/659851/cant-fit-gaussian-mixture-model-estimates-wrong-parameters</link>
      <description><![CDATA[下面的测试从高斯混合模型生成样本，然后对其进行拟合。
拟合模型与原始模型完全不同。为什么？这怎么可能呢，结果不只是稍微不同——错误是巨大的。

原文：weights=[0.5, 0.5], means=[0, 0], sigmas=[1, 2]
拟合：weights=[0.525, 0.475], means=[-0.617, 0.716], sigmas=[1.434, 1.443]
代码（带图的完整代码)
import numpy as np
from sklearn.mixture import GaussianMixture

def fit_normal_mixture(*, n_components, values, random_state, n_init):
values = np.array(values).reshape(-1, 1) # 转换为 2D 数组
nmm = GaussianMixture(n_components, covariance_type=&#39;diag&#39;, random_state=random_state, n_init=n_init)
nmm.fit(values)
means = nmm.means_.flatten().tolist()
sigmas = np.sqrt(nmm.covariances_.flatten()).tolist()
weights = nmm.weights_.flatten().tolist()
return weights, means, sigmas

def sample_normal_mixture(*, weights, means, sigmas, n):
if not np.isclose(sum(weights), 1):
raise ValueError(&quot;Weights must sum to 1&quot;)
components = np.random.choice(len(weights), size=n, p=weights)
return np.random.normal(loc=np.array(means)[components], scale=np.array(sigmas)[components])

# 测试
if __name__ == &#39;__main__&#39;:
# 从正态混合模型生成样本
nmm_sample = sample_normal_mixture(weights=[0.5, 0.5], means=[0, 0], sigmas=[1, 2], n=10000)

# 将样本拟合到正态混合模型
print(fit_normal_mixture(n_components=2, values=nmm_sample, random_state=0, n_init = 10))

附言：是否可以告诉拟合算法使用 0 作为平均值？]]></description>
      <guid>https://stats.stackexchange.com/questions/659851/cant-fit-gaussian-mixture-model-estimates-wrong-parameters</guid>
      <pubDate>Sat, 11 Jan 2025 06:59:18 GMT</pubDate>
    </item>
    <item>
      <title>偏最小二乘回归 - 解释 X 的方差高于 100%</title>
      <link>https://stats.stackexchange.com/questions/659831/partial-least-squares-regression-explained-variance-for-x-above-100</link>
      <description><![CDATA[我正在使用 58 个参数（股票价格、历史回报、利率、国债收益率、商品……等）对标准普尔 500 指数的每日回报进行建模。
我使用偏最小二乘法创建了一个回归模型。
我的模型总共有 58 个组件，从 50 个组件开始，独立变量的解释方差超过 100%。
可能是什么原因造成的？
如果您需要更多信息，请告诉我。
编辑：
我使用 pls 包中的 plsr 函数实现了 PLSR：
dfAnalysisNumeric = read.csv(&quot;dfAnalysisNumeric.csv&quot;)[,-1]

plsrModel = plsr(indexReturn_t~.,data=dfAnalysisNumeric,
validation =&quot;CV&quot;,
scale=TRUE,
center=TRUE)
summary(plsrModel)

dfAnalysisNumeric 是我的数据集。我将其导出为 csv 文件。
这是链接：https://mega.nz/file/3ZsWDJpa#oueRGUqrVnyWqEIMdvU0mZYBjkAW57xklxkjbpTD_nQ
]]></description>
      <guid>https://stats.stackexchange.com/questions/659831/partial-least-squares-regression-explained-variance-for-x-above-100</guid>
      <pubDate>Fri, 10 Jan 2025 16:48:36 GMT</pubDate>
    </item>
    <item>
      <title>对于 OLS 假设，随机样本是否需要 IID</title>
      <link>https://stats.stackexchange.com/questions/659821/does-random-sample-need-to-be-iid-for-ols-assumption</link>
      <description><![CDATA[假设要为 OLS 回归 创建样本，我分 2 个阶段 在不同的总体中抽样数据。例如，在一个总体中，我有 5000 个数据点，我从该总体中选择了 1000 个数据。而在另一个总体中，有 3000 个数据点，我从该总体中选择了 500 个数据。
然后我 组合 2 个抽样数据集（因此，组合数据集有 1500 个数据点），并构建横截面 OLS 回归。
我的问题是，在这种情况下，随机样本的 OLS 假设 是否得到满足？对于随机样本，我们是否需要数据为 IID？
在另一个抽样选项中，我有相同的 2 个数据总体。但是对于 1 个样本，我进行了 分层抽样，而对于另一个样本，我进行了 简单随机抽样。然后 合并 2 个抽样数据集。
在进行 OLS 回归时，随机样本假设在这种抽样方法下是否成立？
随机样本假设取自 Wooldridge 的计量经济学入门。
]]></description>
      <guid>https://stats.stackexchange.com/questions/659821/does-random-sample-need-to-be-iid-for-ols-assumption</guid>
      <pubDate>Fri, 10 Jan 2025 11:50:05 GMT</pubDate>
    </item>
    <item>
      <title>R 中 Frank's Copula 负二项式模型中 Theta 参数发散问题</title>
      <link>https://stats.stackexchange.com/questions/659647/issue-with-diverging-theta-parameter-in-negative-binomial-models-with-franks-co</link>
      <description><![CDATA[我尝试使用负二项分布结合 Frank 的 copula 来对足球比赛结果进行建模，以建立依赖结构。每支球队都被分配了单独的进攻和防守参数，并且有主场和客场进球的离散参数，以及 Frank 的 copula 的 theta 参数。
我的方法受到 McHale、Ian 和 Phil Scarf 的论文“国际足球比赛中对方球队进球依赖关系建模”中描述的方法的启发（统计建模，11.3（2011）：219-236）。但是，我没有使用基于 FIFA 世界排名位置差异的参数来对进球依赖关系进行建模（如原始研究），而是对其进行了修改，使用单独的球队参数进行进攻和防守。
不幸的是，在优化过程中，theta 参数在几次迭代后开始发散，我不知道如何解决这个问题。我怀疑 Frank 的 copula 的 theta 参数可能需要限制在特定范围内以确保数值稳定性。但是，我使用的优化函数 (nlm) 不直接支持参数约束，这使得这很难实现。
我的方法：

示例数据：

set.seed(123)

data_model &lt;- data.frame(
Heim = sample(c(&quot;Team A&quot;, &quot;Team B&quot;, &quot;Team C&quot;, &quot;Team D&quot;), 100, 
replace = TRUE),
Gast = sample(c(&quot;Team A&quot;, &quot;Team B&quot;, &quot;Team C&quot;, &quot;Team D&quot;), 100, 
replace = TRUE),
ToreHeim = rpois(100, lambda = 1.5), # 主队进球
ToreGast = rpois(100, lambda = 1.2) # 客队进球
)

# 删除主队与客队平分的比赛
data_model &lt;- data_model[data_model$Heim != data_model$Gast, ]


模型函数：

negloglik_double_nb_copula &lt;- function(params, goals_home, 
goals_visitor, team_home, team_visitor, param_skeleton) {
plist &lt;- relist(params, param_skeleton)
plist$defense &lt;- c(sum(plist$defense)*-1, plist$defense)
names(plist$defense)[1] &lt;-名称（plist$attack[1]）

lambda_home &lt;- exp（plist$attack[team_home] + 
plist$defense[team_visitor] + plist$home）
lambda_visitor &lt;- exp（plist$attack[team_visitor] + 
plist$defense[team_home]）

m1 &lt;- exp（plist$m1）
m2 &lt;- exp（plist$m2）
theta &lt;- exp（plist$theta）

log_lik_home &lt;- dnbinom（goals_home，mu = lambda_home，
size = m1^(-1)，log = FALSE）
log_lik_visitor &lt;- dnbinom(goals_visitor, mu = lambda_visitor, 
size = m2^(-1), log = FALSE)
log_lik_copula &lt;- log(frank_copula_density(log_lik_home, 
log_lik_visitor, theta))

log_lik &lt;- sum(log_lik_copula)
return(log_lik * -1)
}

frank_copula_density &lt;- function(p_home, p_visitor, theta) {
num &lt;- theta * exp(-theta * (p_home + p_visitor)) * 
(1 - exp(-theta))
denom &lt;- ((1 - exp(-theta * p_home)) * 
(1 - exp(-theta * p_visitor)) + exp(-theta) - 1)^2
return(num / denom)
}



初始参数：

parameter_list_copula &lt;- list(
attack = rep(0.2, n_teams),
defense = rep(-0.01, n_teams-1),
home = 0.1,
m1 = 0.1,
m2 = 0.15,
theta = 0.5
)

names(parameter_list_copula$attack) &lt;- all_teams
names(parameter_list_copula$defense) &lt;- all_teams[-1]



优化：

nlm_nb_copula &lt;- nlm(negloglik_double_nb_copula, 
unlist(parameter_list_copula), 
goals_home = data_model$ToreHeim,
goals_visitor = data_model$ToreGast,
team_home = data_model$Heim, 
team_visitor = data_model$Gast,
param_skeleton = param_list_copula, 
print.level = 2, 
iterlim = 10000, hessian = FALSE)

尽管设置了合理的起始值，但经过几次迭代后，优化结果在 theta 参数上出现分歧。
切换到支持参数约束的优化方法（例如，使用带有框约束的 optim）是否有意义？或者有其他推荐的策略来解决这个问题？
任何稳定优化的建议或改进该模型的见解都将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/659647/issue-with-diverging-theta-parameter-in-negative-binomial-models-with-franks-co</guid>
      <pubDate>Sun, 05 Jan 2025 22:58:36 GMT</pubDate>
    </item>
    <item>
      <title>结合多个调查设计对象的年份，何时适当使用交互项</title>
      <link>https://stats.stackexchange.com/questions/659458/combining-years-for-multiple-survey-design-object-when-to-appropriately-use-inte</link>
      <description><![CDATA[我正在使用 KID（儿童住院数据库）并使用 R 中的调查包。我有两个问题

何时使用 Strata 和 PSU 的交互项：
KID 数据库包含每年重新采样的 PSU（医院），而 Strata 包括 10% 的健康新生儿出院和 80% 的其他出院。
我不确定何时在定义 svydesign() 的 id 和 strata 参数时使用交互项。

id 参数应该使用 ~ PSU 还是 ~ interaction(PSU, YEAR) 来解释医院的年度重新采样？
strata 参数应该使用 ~ strata 还是 ~ interaction(strata, YEAR)？
我想确保设计反映了调查结构。

子集化后是否可以合并 survey.design 对象？还是需要先合并原始数据框，创建一个调查设计对象，然后再子集化？

例如，如果我每年按人口统计组对 survey.design 对象进行子集化，是否可以合并生成的对象？]]></description>
      <guid>https://stats.stackexchange.com/questions/659458/combining-years-for-multiple-survey-design-object-when-to-appropriately-use-inte</guid>
      <pubDate>Thu, 02 Jan 2025 16:44:23 GMT</pubDate>
    </item>
    <item>
      <title>治疗方案的无偏估计与随时间变化的治疗和结果形成对比</title>
      <link>https://stats.stackexchange.com/questions/630973/unbiased-estimation-of-treatment-regime-contrast-with-time-varying-treatment-and</link>
      <description><![CDATA[我在寻找一种策略来从观察到的数据中识别我正在寻找的因果关系时遇到了一些麻烦。我假设以下 DAG：

其中 $Z$ 是抛硬币（随机化）的结果，$A$ 是感兴趣的曝光，$Y$ 是感兴趣的结果。我假设 $A$ 是二进制（0 或 1），$Y$ 是连续的。感兴趣的估计量为：
$$E\left(Y_6^{\bar{A} = \bar{1}} - Y_6^{\bar{A} = \bar{0}}\right)$$
其中 $Y_6^{\bar{A}=\bar{1}}$ 表示如果在每个考虑的时间点将 $A$ 设置为 $1$，则在 $t = 6$ 时观察到的 $Y$ 的值。 $Y_6^{\bar{A}=\bar{0}}$ 遵循相同的定义，但 $A$ 在所有时间点均设置为 $0$。所有变量均完全观察到，没有任何缺失值或测量误差。
我确实认为应该能够根据数据以无偏的方式估计这种影响，但我不确定如何做到这一点。我曾考虑过使用混合模型，但这并不能真正给我我感兴趣的边际估计。
结构边际模型是否有效？如果是，我该如何将其应用于这些数据？如果有人能指出正确的方向，那就太好了。]]></description>
      <guid>https://stats.stackexchange.com/questions/630973/unbiased-estimation-of-treatment-regime-contrast-with-time-varying-treatment-and</guid>
      <pubDate>Fri, 10 Nov 2023 13:53:05 GMT</pubDate>
    </item>
    </channel>
</rss>