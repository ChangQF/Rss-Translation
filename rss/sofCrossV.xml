<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 31 Aug 2024 15:16:05 GMT</lastBuildDate>
    <item>
      <title>使用交叉验证来评估特征选择的不同互信息阈值</title>
      <link>https://stats.stackexchange.com/questions/653673/using-crossvalidation-to-evaluate-different-mutual-information-threshholds-for-f</link>
      <description><![CDATA[我目前正在尝试确定分类任务中特征选择的互信息阈值。我的想法是根据位置参数（例如中位数、top75% 等）设置不同的阈值，然后通过 CV 对训练集进行评估。由于我计划部署几个不同的模型（朴素贝叶斯、CART、TAN），那么是否有必要单独比较每个模型的每个阈值的差异，还是只需比较阈值 1 的平均 acc 和阈值 2 的平均 acc 就足够了？
这通常是一种可接受的方法，还是您会推荐一种更科学的方法？
我也尝试了 boruta 方法，但该算法只排除了 29 个特征中的 1 个，尽管其中许多特征的 MI 分数很低（0,001）。与仅使用 top50%/75% 的模型相比，使用其余 28 个特征的模型在 CV 中的表现也更差。
总的来说，我试图避免特征选择过程被批评为不科学。然而，我读过的许多论文都指出，没有最好的方法来做到这一点。
你会推荐什么方法？从我的方法来看，应该改进什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/653673/using-crossvalidation-to-evaluate-different-mutual-information-threshholds-for-f</guid>
      <pubDate>Sat, 31 Aug 2024 14:13:00 GMT</pubDate>
    </item>
    <item>
      <title>正态分布是如何由现实世界数据构成的</title>
      <link>https://stats.stackexchange.com/questions/653672/how-is-the-normal-distribution-made-from-real-world-data</link>
      <description><![CDATA[我是一名中学生，我正在尝试理解人们在现实世界中如何创建和应用正态分布。
从我有限的知识来看，常识告诉我，他们收集原始数据，然后继续从中创建一个直方图，如果这个直方图足够接近完美的钟形曲线（我假设他们使用其他统计指标（如偏度等）来确定这一点），然后他们使用标准偏差和平均值并将其插入正态分布函数以获得实际的钟形曲线。
因为即使身高之类的东西是正态分布的，我实际上假设在现实世界中，当有人收集数据时，他们可能会发现身高可能不是正态分布的（例如，一个有很多孩子的城镇等）。所以在我的脑海里，我假设科学家首先制作一个直方图来检查它是否大致接近钟形曲线。]]></description>
      <guid>https://stats.stackexchange.com/questions/653672/how-is-the-normal-distribution-made-from-real-world-data</guid>
      <pubDate>Sat, 31 Aug 2024 12:24:07 GMT</pubDate>
    </item>
    <item>
      <title>模拟具有特定事件和多个变量的 HR 的生存数据集</title>
      <link>https://stats.stackexchange.com/questions/653670/simulate-a-survival-dataset-with-specific-events-and-hr-for-multiple-variables</link>
      <description><![CDATA[在我的教学课上，学生需要阅读并评论一篇科学文章。
下一课是编码课，我希望他们根据这篇文章重新创建一个情节，以便他们了解其中的利害关系。
这是源数据集，情节（带注释的森林图）无关紧要：
tibble::tibble(
category = rep(c(&quot;Marker 1&quot;, &quot;Marker 2&quot;), each = 2L),
level = rep(c(&quot;Yes&quot;, &quot;No&quot;), 2),
gpA_n_events = c(149, 60, 119, 92),
gpA_n_patients = c(233, 98, 192, 139),
gpB_n_events = c(74, 35, 66, 43),
gpB_n_patients = c(115, 47, 96, 67),
hr_pfs__estimate = c(0.55, 0.51, 0.48, 0.54),
hr_pfs__ci_inf = c(0.42, 0.33, 0.35, 0.37),
hr_pfs__ci_sup = c(0.73, 0.79, 0.65, 0.73),
gpA_pfs__median = c(10, 11.7, 10.3, 10.8),
gpA_pfs__ci_inf = c(8.3, 9.5, 8.6, 8.4),
gpA_pfs__ci_sup = c(11.4, 17.7, 12.3, 13.7),
gpB_pfs__median = c(5.4, 5.9, 5.3, 5.3),
gpB_pfs__ci_inf = c(4, 3.4, 4.1, 3.4),
gpB_pfs__ci_sup = c(7.8, 8.2, 7.8, 7.2),
)


这里，多个标记（我有 8 个类别）每个级别的事件数量不同，风险比也不同（计算得出来自 Cox 模型）。
我想生成一个数据集，该数据集将产生与表格相同的结果：A 组 331 名患者和 B 组 163 名患者，生存时间以获得指定的 HR（gpA vs gpB）。事件数和 CI 中位数不太重要（但如果简单的话会更好）。
我看到了关于该主题的几个很棒的答案（例如 此处、此处 和 此处），但目的不同（不是相同的 N，只有一个模型...）。
我想做的事情可行吗？如果可以，哪种方法合适？]]></description>
      <guid>https://stats.stackexchange.com/questions/653670/simulate-a-survival-dataset-with-specific-events-and-hr-for-multiple-variables</guid>
      <pubDate>Sat, 31 Aug 2024 10:41:06 GMT</pubDate>
    </item>
    <item>
      <title>我可以在 R 中使用哪些统计测试来确定使用 DIANA（DIvisive ANAlysis Clustering）获得的聚类的统计意义？</title>
      <link>https://stats.stackexchange.com/questions/653669/what-statistical-tests-can-i-use-in-r-to-ascertain-the-statistical-significance</link>
      <description><![CDATA[我有一组表皮碳氢化合物 (CHC) 数据，这些数据来自 60 个蚜虫样本，这些样本来自蚜虫活动季节的中期、早期和晚期，蚜虫以三种不同的植物为食。由于季节、蚜虫以哪种植物为食，或者由于许多不同的非生物因素，CHC 可能会有所不同。我想使用 DIANA 来可视化数据。我在网上找到了一个类似的分析，其中使用 R 完成了此分析：
chcmain &lt;- read.csv(&quot;chcsitobion.csv&quot;, header = TRUE)
row.names(chcmain) &lt;- chcmain$X
chcmain &lt;- chcmain[,2:18]

#对数变换数据
chc_matrixmain &lt;- log(data.matrix(chcmain))

#计算除法层次聚类
hc_d &lt;- diana(chc_matrix)

#除法系数；发现的聚类结构数量
hc_d$dc

#绘制树状图
pltree(hc_d, cex = 0.6, hang = -1, main = &quot;Dendrogram of diana&quot;)

#将 diana()tree 切成 4 组
clust &lt;- cutree(as.hclust(hc_d), k=4)

#在散点图中可视化结果
fviz_cluster(list(data = chc_matrix, cluster = clust))

#在树状图内可视化聚类
pltree(hc_d, hang=-1, cex = 0.6)
rect.hclust(hc_d, k =4, border = 2:10)

我真的不明白他们是如何得出 4 个聚类的，以及我是否可以进行任何统计以确保获得的聚类在统计上与每个聚类不同其他。
如果我使用 DIANA，您能否指导我如何确定簇的数量以及我可以做哪些统计来支持我的分析？
我不可能对预期的簇数或测试特定假设做出先验假设，因为我的数据可能受到多种因素的影响。
散点图显示了两个主成分，但我不明白这是使用前两个 PC 还是所有 PC 创建的。
上面的代码对我来说是有效的，但缺少关于他们如何确定 4 个簇以及应该进行哪些统计测试来支持结果的步骤，而且我不太清楚如何将此分析用于我自己的数据。]]></description>
      <guid>https://stats.stackexchange.com/questions/653669/what-statistical-tests-can-i-use-in-r-to-ascertain-the-statistical-significance</guid>
      <pubDate>Sat, 31 Aug 2024 10:22:17 GMT</pubDate>
    </item>
    <item>
      <title>与 GARCH-DCC 方法相比，使用多元滤波历史模拟和单变量 GARCH 模型的优缺点是什么？</title>
      <link>https://stats.stackexchange.com/questions/653668/what-are-the-pros-and-cons-of-using-multivariate-filtered-historical-simulation</link>
      <description><![CDATA[我正在评估股票投资组合的市场风险，并在 MATLAB 文档中看到了一个使用多元过滤历史模拟技术的示例：
https://it.mathworks.com/help/econ/using-bootstrapping-and-filtered-historical-simulation-to-evaluate-market-risk.html
这种方法将单变量 GARCH 模型与资产收益概率分布的非参数规范相结合。 FHS 允许通过引导标准化残差和模拟未来回报来生成预测。
我还知道 GARCH-DCC（动态条件相关性）方法，该方法对资产回报之间的时变相关性进行建模。我有兴趣了解使用 FHS 和 GARCH 模型与 GARCH-DCC 方法的优缺点。
每种方法的优点和局限性是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/653668/what-are-the-pros-and-cons-of-using-multivariate-filtered-historical-simulation</guid>
      <pubDate>Sat, 31 Aug 2024 09:51:03 GMT</pubDate>
    </item>
    <item>
      <title>MatchIt 边际效应</title>
      <link>https://stats.stackexchange.com/questions/653667/matchit-marginal-effects</link>
      <description><![CDATA[我正尝试使用 MatchIt 估计治疗的 ATT，但不确定它是边际效应还是条件效应。在估计结果的插图中，它说：

此外，对于连续结果，当结果模型中存在治疗-协变量相互作用时，条件效应可能会被错误地解释为边际效应估计。如果协变量不是以目标人群中的平均值为中心（例如，ATT 的治疗组、ATE 的完整样本或 ATM 的剩余匹配样本），治疗系数将不对应于目标人群中的边际效应；它将对应于协变量值等于零时的治疗效果，这可能没有意义或不合理。当结果模型中包含协变量时，G 计算始终是估计效应的最安全方法，尤其是在存在治疗-协变量相互作用的情况下。

在这里，结果是连续的，在结果模型中，我有一个二元治疗、匹配变量 X（全部连续）和一个相互作用。由于我正在使用 G-Computation（我认为），这是否估计了边际效应？
m &lt;- matchit(D ~ X,
data = data,
method = &quot;cem&quot;, 
estimand = &quot;ATT&quot;, 
k2k = FALSE)

matchdata &lt;- match.data(m)

fit &lt;- lm(Y ~ D + X + D * X,
data = matchdata, weights = weights)

avg_comparisons(fit, variable = &quot;D&quot;,
vcov = &quot;HC3&quot;, 
newdata = subset(matchdata, D == 1),
wts = &quot;weights&quot;) 
]]></description>
      <guid>https://stats.stackexchange.com/questions/653667/matchit-marginal-effects</guid>
      <pubDate>Sat, 31 Aug 2024 09:50:38 GMT</pubDate>
    </item>
    <item>
      <title>分类变量的效应大小[关闭]</title>
      <link>https://stats.stackexchange.com/questions/653666/effect-size-of-categorical-variables</link>
      <description><![CDATA[如果我对两个不同维度的类别进行生物测试，样本量较大，为 190 个，df=63。Cramer V 是否适合这种情况？考虑到效应大小，解释会如何？SPSS 计算出的 lambda、列联系数是否也有效？]]></description>
      <guid>https://stats.stackexchange.com/questions/653666/effect-size-of-categorical-variables</guid>
      <pubDate>Sat, 31 Aug 2024 09:30:33 GMT</pubDate>
    </item>
    <item>
      <title>为什么我们使用 KL 散度而不是交叉熵？[重复]</title>
      <link>https://stats.stackexchange.com/questions/653664/why-are-we-using-kl-divergence-over-cross-entropy</link>
      <description><![CDATA[我读过这个问题
为什么我们在 t-SNE 目标函数中使用 Kullback-Leibler 散度而不是交叉熵？
我无法完全理解答案。
如果我们使用 KL 散度作为损失，它不是与使用交叉熵具有相同的效果吗？当 KL 散度很大时，交叉熵也很大，反之亦然。两者的变化量也没有太大区别。此外，KL 散度还有更多项需要计算，即熵。
从这个角度来看，我找不到使用 KL 散度而不是交叉熵的理由。
有人能帮我吗？
（+ 如果 KL 散度真的比交叉熵更好（对于损失函数），我也很好奇为什么有些任务仍在使用交叉熵）]]></description>
      <guid>https://stats.stackexchange.com/questions/653664/why-are-we-using-kl-divergence-over-cross-entropy</guid>
      <pubDate>Sat, 31 Aug 2024 08:48:16 GMT</pubDate>
    </item>
    <item>
      <title>聚类混合数据类型：算法选择、距离测量和特征加权</title>
      <link>https://stats.stackexchange.com/questions/653662/clustering-mixed-data-types-algorithm-selection-distance-measurement-and-feat</link>
      <description><![CDATA[我有一个包含 74,000 条记录的数据库，其中包含 29 个特征。其中 14 个特征是分类特征，并且为 0 或 1，而其他 15 个特征是连续特征，并且已在 0 和 1 之间进行归一化和缩放。我还执行了一些数据清理，并删除了大约 0.4% 的数据以消除异常值。我想运行一个聚类模型，将数据划分为 3 或 4 个聚类。我有以下问题：

如果我有一个包含分类特征和连续特征的相当大的数据库，我应该使用什么算法？
当我同时具有分类特征和连续特征时，我应该如何测量特征之间的距离？
其中三个特征更重要，我想为它们分配更多权重。我应该使用多少权重？
]]></description>
      <guid>https://stats.stackexchange.com/questions/653662/clustering-mixed-data-types-algorithm-selection-distance-measurement-and-feat</guid>
      <pubDate>Sat, 31 Aug 2024 07:46:55 GMT</pubDate>
    </item>
    <item>
      <title>如何概括半参数模型中无偏估计量的非渐近 Cramer-Rao 下界？</title>
      <link>https://stats.stackexchange.com/questions/653661/how-to-generalise-non-asymptotic-cramer-rao-lower-bound-for-unbiased-estimators</link>
      <description><![CDATA[我们都知道经典的 Cramer-Rao 界限，它指定了参数模型中任何无偏估计量的方差的下限。请注意，这个界限是非渐近的，因为它对所有样本数 $n$ 都有效，并且不需要考虑渐近方差。
我对半参数模型进行了调查，但我发现的所有半参数文献都只考虑估计量的渐近方差下限。
因此，我想知道是否有可能在半参数模型中获得 Cramer-Rao 界限的对应项。这个方差下限只能对所有无偏估计量有效，但应该是非渐近的。
欢迎任何相关的想法和文献！]]></description>
      <guid>https://stats.stackexchange.com/questions/653661/how-to-generalise-non-asymptotic-cramer-rao-lower-bound-for-unbiased-estimators</guid>
      <pubDate>Sat, 31 Aug 2024 07:36:55 GMT</pubDate>
    </item>
    <item>
      <title>在线性回归下推导 MSE($\hat{\beta}$)</title>
      <link>https://stats.stackexchange.com/questions/653651/deriving-mse-hat-beta-under-linear-regression</link>
      <description><![CDATA[我能够推导出 MSE，但是推导过程中有一部分我不太明白。这是我得到的结果：
事实：

$\mathbb{E}(​​\hat{\beta})=\hat{\beta}\space$（无偏估计量）
$\text{Cov}(\hat{\beta})= \sigma^2[(X^TX)^{-1}] $

根据定义，
$$MSE = \mathbb{E}[||\hat{\beta}-\beta||^2] $$
$$= \space \mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]$$
由于 $\hat{\beta}$ 是无偏的，
$$ \boldsymbol{= \text{tr}[\text{Cov}(\hat{\beta})]} *$$
$$= \text{tr}[\sigma^2(X^T X)^{-1}]$$
$\sigma^2$ 是标量，因此可以分解出来，
$$= \sigma^2 tr[(X^T X)^{-1}] $$
我感到困惑的是行 $*$，我不确定我们是如何得到方程式 $ \text{tr}[\text{Cov}(\hat{\beta})] $。以下是我目前所理解的：
通过偏差-方差分解，
$$ \space \mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]=\mathbb{V}(\hat{\beta})+[\mathbb{E}(​​\hat{\beta})-\beta]^T[\mathbb{E}(​​\hat{\beta})-\beta]$$
我们的估计量是无偏的，因此 $\mathbb{E}(​​\hat{\beta})-\beta= 0$。因此，
$$\mathbb{E}[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)]=\mathbb{V}(\hat{\beta})$$

首先，$\mathbb{V}(\hat{\beta})$ 应该是一个标量，但对我来说这真的没什么意义，这让我想到了下一个问题...
我假设 $V(\hat{\beta}) = \text{tr}[\text{Cov}(\hat{\beta})]$。这是为什么呢？
]]></description>
      <guid>https://stats.stackexchange.com/questions/653651/deriving-mse-hat-beta-under-linear-regression</guid>
      <pubDate>Sat, 31 Aug 2024 00:13:56 GMT</pubDate>
    </item>
    <item>
      <title>简单线性回归中的统计误差</title>
      <link>https://stats.stackexchange.com/questions/653635/statistical-error-in-simple-linear-regression</link>
      <description><![CDATA[首先我想说的是，我正在寻找一个简单的回归模型，而不是一个数学模型，来对这个术语进行更多的概念性理解。
在计量经济学中，简单的线性回归有一个“误差项”，称为mu，它表示“影响y的除x之外的因素，或未观察到的数据”。在统计学课上，这个术语被称为“统计误差”或“噪声”，被视为epsilon，这又是现实世界中我们的模型无法捕捉到的随机噪声。
在《统计学习简介》中，这个术语被称为“不可约误差”，是一个表示我们尚未测量的未测量变量的数量，或者只是现实世界数据中无法测量的变化。作者将这个术语与可约误差区分开来。
这就是我感到困惑的地方：在一个理论上完全确定的世界中，我们拥有无限量的数据，这个“不可约误差”在理论上是可以约化的吗？也就是说，如果在这个理论世界中，我们的协变量，大学 GPA，可以映射到我们的响应，比如“薪水”，那么这个术语在技术上可以为零吗？
我理解，在我们生活的世界中，数据总是存在不可测量的变化，我们永远无法获得无限量的数据或事先知道每一个协变量，因此会出现这个错误。但是假设我们有 5 个数据点，这是我们的“理论总体”，这个“不可约误差”会是零吗？术语消失？
例如，在《统计学习简介》中，他们有这张图片：

我对这张图片的困惑源于这个数据是模拟的，所以我们实际上知道真正的底层函数 f。但如果是这样的话，为什么还有错误？如果我们知道 f，为什么我们不能完美地将这个函数与数据拟合？如果这个数据是基于 f 模拟的，为什么模拟不会在函数 f 上产生数据点？
我知道这似乎是一个愚蠢的问题，但我试图理解这里的细微差别。非常感谢您的帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/653635/statistical-error-in-simple-linear-regression</guid>
      <pubDate>Fri, 30 Aug 2024 17:14:15 GMT</pubDate>
    </item>
    <item>
      <title>基于模拟随机事件概率平均值的预期泊松二项分布</title>
      <link>https://stats.stackexchange.com/questions/653630/expected-poisson-binomial-distribution-based-on-average-of-simulated-random-even</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/653630/expected-poisson-binomial-distribution-based-on-average-of-simulated-random-even</guid>
      <pubDate>Fri, 30 Aug 2024 15:25:44 GMT</pubDate>
    </item>
    <item>
      <title>利用混合样本确定生物学重复</title>
      <link>https://stats.stackexchange.com/questions/653624/determining-biological-replicates-with-pooled-samples</link>
      <description><![CDATA[我们小组有一个长期存在的方案，其中汇集了多个组织纯化样本。这是因为每只动物的组织很少。然后对该池进行多次采样，并测量感兴趣的结果。每次测量都被视为推理分析的独立数据点。虽然这些单个样本可能是“重复”，但它们作为生物重复的状态充其量是高度可疑的。它们看起来可能是伪重复。
即，用两种药物治疗动物。提取的组织被合并到两个池中。每个池采样 x 次。然后通过药物治疗分析这些样品，每次治疗 n = x。
应该如何调整这种设计？目前没有办法只使用一只动物的组织进行测定。必须将组织组合起来。
如果我们使用更多动物并创建多个池，那么每个池可以采样几次，以解释因组合动物而引入的变化。这将使用混合级别模型进行建模。每个池是否都是一个单一的生物学重复，并且池中的每个样本都是随机效应内的聚类？
因此，两种治疗方法，每种治疗方法有 M 只动物。组织提取物组合成 P 个池，每个池采样 N 次。模型中每个治疗方法的 n = P，单个样本按 P 聚类。
还有其他方法可以做到这一点吗？
（这是我最近问过的一个问题的重述，但措辞不当且令人困惑。]]></description>
      <guid>https://stats.stackexchange.com/questions/653624/determining-biological-replicates-with-pooled-samples</guid>
      <pubDate>Fri, 30 Aug 2024 14:35:33 GMT</pubDate>
    </item>
    <item>
      <title>努力证明 Hessian 是简单线性回归最小二乘法的 PSD</title>
      <link>https://stats.stackexchange.com/questions/653335/struggling-to-prove-that-the-hessian-is-psd-for-simple-linear-regression-least-s</link>
      <description><![CDATA[我正在研究最小化以下函数在 $b_0$ 和 $b_1$ 上的最简单情况：
$$(y_1 - [b_0 + b_1 x_1])^2 + \ldots + (y_n - [b_0 + b_1 x_n])^2$$
为了确定此函数在 $b_0$ 和 $b_1$ 上是凸函数，我已获得如下 Hessian 矩阵：
$$\Big(\begin{matrix}
2n &amp; 2(x_1 + \ldots + x_n)\\
2(x_1 + \ldots + x_n) &amp; 2(x_1^2 + \ldots + x_n^2)
\end{matrix}\Big)$$
如果我能证明这个 $2\times 2$ 矩阵的行列式是非负的，那么我就完成了。但在这里，我必须证明以下不等式：
$$n(x_1^2 + \ldots + x_n^2) \geq (x_1 + \ldots + x_n)^2$$
有没有简单的方法可以证明这个不等式？
我看过几本关于回归的书，包括 Wackerley、Mendenhall 和 Scheaffer。虽然他们推导出一阶条件，但并未考虑/提供全局最小值的二阶必要条件。任何帮助都值得感激。

已编辑并添加。啊，找到了一种相当优雅的方法来进一步探究这个问题。
在尝试通过归纳法证明这一点时，我需要证明以下不等式：
$$x_1^2 + \ldots + x_n^2 + nx_{n+1}^2 \geq 2(x_1 + \ldots + x_n)x_{n+1}$$
但这很容易通过重写 LHS 来证明：
$$(x_1^2 + x_{n+1}^2)+\ldots + (x_n^2 + x_{n+1}^2) \geq 2(x_1 + \ldots + x_n)x_{n+1}$$
然后，通过从 RHS 到 LHS，我们有：
$$(x_1^2 + x_{n+1}^2 - 2x_1 x_{n+1})+\ldots + (x_n^2 + x_{n+1}^2 - 2x_n x_{n+1}) \geq 0$$
这显然是正确的。]]></description>
      <guid>https://stats.stackexchange.com/questions/653335/struggling-to-prove-that-the-hessian-is-psd-for-simple-linear-regression-least-s</guid>
      <pubDate>Mon, 26 Aug 2024 14:05:06 GMT</pubDate>
    </item>
    </channel>
</rss>