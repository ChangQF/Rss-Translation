<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Fri, 12 Apr 2024 00:59:15 GMT</lastBuildDate>
    <item>
      <title>扩展隐马尔可夫模型 (HMM) 参数估计</title>
      <link>https://stats.stackexchange.com/questions/644843/extended-hidden-markov-models-hmm-parameter-estimation</link>
      <description><![CDATA[对于更简单的 HMM，我们可以使用 Viterbi 训练（而不是解码）或 Baum Welch 等算法来估计最能描述观察到的数据的参数。
当使用更复杂的多元 HMM 或二阶 HMM 时，我们如何做同样的事情？是否有资源可以用来查看在这些情况下如何调整算法和设置？如果有它们的伪代码而不是描述，那就更好了，尽管我知道早期的算法（例如向前和向后）也会改变。]]></description>
      <guid>https://stats.stackexchange.com/questions/644843/extended-hidden-markov-models-hmm-parameter-estimation</guid>
      <pubDate>Fri, 12 Apr 2024 00:37:56 GMT</pubDate>
    </item>
    <item>
      <title>组内系统回顾和荟萃</title>
      <link>https://stats.stackexchange.com/questions/644842/within-group-systematic-review-and-metalysis</link>
      <description><![CDATA[我正在研究 SRMA，以评估干预前后研究的效果。效应大小是速率比。怎么做？对于使用没有真实对照组的前后设计给出的结果，我需要进行什么修正？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/644842/within-group-systematic-review-and-metalysis</guid>
      <pubDate>Thu, 11 Apr 2024 23:43:21 GMT</pubDate>
    </item>
    <item>
      <title>我们如何摆脱 MLE 中的 $p(x|\theta)$</title>
      <link>https://stats.stackexchange.com/questions/644841/how-do-we-get-rid-of-px-theta-in-mle</link>
      <description><![CDATA[简单的问题...通常机器学习的介绍会告诉您以下内容：

您想要最大化$p(\theta|D)$
应用贝叶斯定理$p(\theta|D) \propto p(D|\theta)p(\theta)$
如果你考虑 $p(\theta)$ 常数，你会得到 MLE，如果你考虑它高斯，你会得到 L2 正则化等等

但是，我想更详细一点，这是我的问题：
$$
p(\theta|D) \propto p(D|\theta)p(\theta) = p(x,y|\theta)p(\theta) = p(y|x, \theta)p(x| θ)p(θ)
$$
现在，通常我们优化的实际上是 $p(y|x,\theta)p(\theta)$，所以我的问题是...下我们要删除哪个假设 $p(x|\theta)$？
我们是否认为 $x$ 独立于 $\theta$ 并且保持不变？.. .]]></description>
      <guid>https://stats.stackexchange.com/questions/644841/how-do-we-get-rid-of-px-theta-in-mle</guid>
      <pubDate>Thu, 11 Apr 2024 22:11:00 GMT</pubDate>
    </item>
    <item>
      <title>估计梯度提升树中的最佳树数</title>
      <link>https://stats.stackexchange.com/questions/644840/estimating-optimal-number-of-trees-in-gradient-boosted-trees</link>
      <description><![CDATA[我试图提出一个公式来估计梯度提升树中树的最佳数量（我知道在实践中使用早期停止或交叉验证是更好的方法，但我只想得到一个近似值）。 
我的直觉如下：设 $y = \mu + \epsilon$ 其中 $y$ 和 $\epsilon$ 是向量。令学习率为 $l$。然后，根据我对 gbdt 在每一步的理解，我们将预测大约移动为 $l \times \epsilon_t$，其中 $\epsilon_t$ 在 $t$ 时出现无法解释的错误（可能会更糟，因为我们无法适应  $\epsilon_t$ 与树完全一样）
因此，在 $t$ 树之后，剩余误差应约为 $(1-l)^t \epsilon$&lt; /span&gt;，这意味着我们正在解释 $(1-(1-l)^t)^2$ 方差百分比
因此，如果理想模型可以解释 $r$ 的变化百分比，那么可以从  中找到最佳树数$(1-(1-l)^t)^2 = r$ 这意味着 $t=\frac{\log(1-\sqrt{r} )}{\log(1-l)}$。我知道这只是一个近似值，实际上它会受到树深度等的影响。
我尝试在合成数据上进行测试，结果非常不同。例如，在具有 10 个特征的模型中，根据此公式， $l=0.01$ 和理想的 r2 为 10% 应该有大约 40 棵树，但实际上我发现基于早期停止，近 1000 棵树是最佳的。我还发现，即使数据生成过程没有改变，最佳树数也会随着数据大小的增加而增加。我使用 lightgbm 进行实验。
您能否指出我的推理中的错误，并解释为什么最佳树数随着数据大小的增加而增长？]]></description>
      <guid>https://stats.stackexchange.com/questions/644840/estimating-optimal-number-of-trees-in-gradient-boosted-trees</guid>
      <pubDate>Thu, 11 Apr 2024 22:04:49 GMT</pubDate>
    </item>
    <item>
      <title>如何证明K-Means将所有空间分割成凸多边形？</title>
      <link>https://stats.stackexchange.com/questions/644838/how-to-prove-that-k-means-splits-all-space-into-convex-polygons</link>
      <description><![CDATA[我想证明 K-Means 算法将整个对象空间分割成凸（可能没有边界）多边形。我试图利用K-Means算法收敛的事实并与之矛盾，但我没有成功。
任何帮助将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/644838/how-to-prove-that-k-means-splits-all-space-into-convex-polygons</guid>
      <pubDate>Thu, 11 Apr 2024 21:41:29 GMT</pubDate>
    </item>
    <item>
      <title>马尔可夫决策过程和消息传递</title>
      <link>https://stats.stackexchange.com/questions/644835/markov-decision-process-and-message-passing</link>
      <description><![CDATA[我正在读《贝叶斯推理与机器学习》这本书。我正在阅读第 7.5 章有关马尔可夫决策过程的内容。我来自最佳停止/最佳控制背景，熟悉贝尔曼原理等。本质上，这个想法是从最后开始并向后工作。
令我困惑的是，Bellamn 原理（向后步进）的应用是用消息传递来解释的，其中消息是状态的价值函数。
这样表达有什么特殊原因吗？我觉得我一定没有抓住要点。]]></description>
      <guid>https://stats.stackexchange.com/questions/644835/markov-decision-process-and-message-passing</guid>
      <pubDate>Thu, 11 Apr 2024 20:58:20 GMT</pubDate>
    </item>
    <item>
      <title>使用变量公式的变化计算后验</title>
      <link>https://stats.stackexchange.com/questions/644834/compute-posterior-using-change-of-variable-formula</link>
      <description><![CDATA[给定一个从单位圆盘到 $\mathbb R^2$ 的双射 f，$w \sim N( 0、\gamma^2 I)$ 和 $y = f^{-1}(f(x) + w)$，如何显示那
$$ \pi(y | x) = |\det Df(y)| \pi_w(f(x) - f(y))。 $$
我知道 $y = f(x)$ 的变量变化是 $\pi(y) = \pi( f^{-1}(y)) |\det Df^{-1}(y)|$，但这仅适用于 $\pi(y) $ 而不是 $\pi(y|x)$，那么我该如何使用它呢？
编辑更多详细信息：$f(x) = \frac{x}{1- ||x||^2}$。给定单位圆盘中的$x$，计算$f(x)$并随机抽取$w$ 来自正态分布，则 $y = f^{-1}(f(x) + w)$。我需要证明后验分布 $\pi(y | x)$ 如上]]></description>
      <guid>https://stats.stackexchange.com/questions/644834/compute-posterior-using-change-of-variable-formula</guid>
      <pubDate>Thu, 11 Apr 2024 20:23:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 PROC PHREG 的边际结构模型</title>
      <link>https://stats.stackexchange.com/questions/644833/marginal-structural-model-using-proc-phreg</link>
      <description><![CDATA[我想拟合一个边际结构模型来解释随访期间的治疗转换。
我发现了一篇关于如何在 R 中执行此操作的精彩论文：https: //www.sciencedirect.com/science/article/abs/pii/S0010482519302082
但是，当然，我正在从事临床试验并使用 SAS。
2000 年，Hernan、Brumback 和 Robins 提供了在 SAS 中构建 MSM 的代码：
https://journals.lww.com/epidem/fulltext/ 2000/09000/marginal_structural_models_to_estimate_the_causal.12.aspx
由于PROC PHREG不支持时间更新权重，因此使用合并逻辑回归来获得边际风险比。
现在是 2024 年，而不是 2000 年，我想知道时间更新的权重现在是否可以合并到 PROC PHREG 中，或者合并逻辑回归仍然是事实上的 MSM 拟合方法？或者，是否有众所周知的宏可以用来适应这个模型？]]></description>
      <guid>https://stats.stackexchange.com/questions/644833/marginal-structural-model-using-proc-phreg</guid>
      <pubDate>Thu, 11 Apr 2024 20:00:26 GMT</pubDate>
    </item>
    <item>
      <title>通过具有时空相关性的二元测量的重复横截面调查进行区域层面的预测</title>
      <link>https://stats.stackexchange.com/questions/644832/forecasting-at-areal-level-from-repeated-cross-sectional-survey-with-binary-meas</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/644832/forecasting-at-areal-level-from-repeated-cross-sectional-survey-with-binary-meas</guid>
      <pubDate>Thu, 11 Apr 2024 19:28:44 GMT</pubDate>
    </item>
    <item>
      <title>为什么 Hotellings T 方统计量按 n 倍缩放？</title>
      <link>https://stats.stackexchange.com/questions/644831/why-does-the-hotellings-t-squared-statistic-scale-by-a-factor-of-n</link>
      <description><![CDATA[根据 NIST：https://www. itl.nist.gov/div898/handbook/pmc/section5/pmc543.htm T 方统计量的方程的比例因子为 n（有趣的是，维基百科页面引用了此页面，但省略了n 比例因子）。
我想知道为什么要包含这个比例因子？这背后有什么直觉吗？
这是否意味着随着样本数量的增加，正在测试的相同样本的可能性会导致我们拒绝零假设的可能性增加？
我想象 2-D 多元正态数据作为样本，可能 n=100 和 n=1000（从相同的真实分布中采样）。这些将具有（接近）相同的样本均值和方差，但是由于这个 n 因素，来自该分布的新样本不符合零假设的可能性随着 n 的增加而增加？
当然，随着 n 的增加，与 F 分布（我们用来检查的）的关系会发生变化，但变化幅度不大。对于 p=2，T^2 (n-p)/(pn-p) 的缩放比例为 98/198 ~ 1/2（n=100）和 998/1998 ~ 1/2（n=1000），并且 F 临界值对于 alpha=0.05 来说，两者都约为 3，因此它们的影响很小。]]></description>
      <guid>https://stats.stackexchange.com/questions/644831/why-does-the-hotellings-t-squared-statistic-scale-by-a-factor-of-n</guid>
      <pubDate>Thu, 11 Apr 2024 19:13:50 GMT</pubDate>
    </item>
    <item>
      <title>具有大量预测变量的逻辑回归/一般理解</title>
      <link>https://stats.stackexchange.com/questions/644828/logistic-regression-with-a-lot-of-predictors-general-understanding</link>
      <description><![CDATA[我目前正计划写我的学士论文，但距离上一次统计研讨会已经有一段时间了，我已经非常生疏了，所以这里的任何指导将不胜感激。
我的样本规模相对较小，n=40，其中 20 名参与者均患有学习障碍，20 名参与者没有作为对照。每个参与者完成三项不同的任务，从中生成大量数据点。
我的数据的简单示例：

&lt;标题&gt;

参与者
疾病
任务
Var1
Var2
Var3
...
Var38


&lt;正文&gt;

1
0
1
.67555
.5353
.4456
.2345
.3812


1
0
2
.5514
.7753
.2239
.3567
.2234


1
0
3
.4563
.6675
.3345
.5681
.5234


...









40
1
2
.8898
.7798
.7887
.9989
.5662


40
1
3
.7854
.8334
.7687
.7878
.6534



我的计划是为每项任务建立一个具有二分 DV（疾病/非疾病）的逻辑回归模型，问题是其中一个模型是否可以比另一个更好地分离 DV，以及生成的数据点是哪个是这项任务中最好的预测因子。经过简短的研究后，人们应该更好地预测 DV，因为它是测量 DV 的现场标准，但我“在方法上不安全”。该怎么做。
我的第一个问题是：

这种方法是否可行，是否可以直接比较任务（例如 AUC、RMSE 和 r2 等拟合指数）？

其次，由于我使用的分析技术会产生许多可能的预测变量 (38)，因此我试图减少这些预测变量以避免过度拟合。我听说过有关 LASSO 的好消息，但由于许可原因我一直使用 SPSS。由于单变量和逐步技术似乎不建议使用，这将是一种将它们降低到更易于管理的水平的明智方法。
对于宽泛的问题和稀疏的理解，我深表歉意，很长一段时间以来，我一直在努力忘记我学到的一切。]]></description>
      <guid>https://stats.stackexchange.com/questions/644828/logistic-regression-with-a-lot-of-predictors-general-understanding</guid>
      <pubDate>Thu, 11 Apr 2024 18:41:02 GMT</pubDate>
    </item>
    <item>
      <title>对于模型调整参数不稳定我该怎么办？</title>
      <link>https://stats.stackexchange.com/questions/644826/what-can-i-do-about-model-tuning-parameter-instability</link>
      <description><![CDATA[我正在尝试确定流域特征对几条河流浓度-流量关系斜率的重要性。由于分水岭特征之间存在多重共线性，我使用偏最小二乘回归 (plsr)。拟合 plsr 模型后，我将使用投影上的变量重要性对分水岭特征进行排序，以解释响应变量。
我的第一步是使用 caret::train 确定 plsr 的理想组件数量。然而，组件的数量根据种子和交叉验证方法的不同而有很大差异。 此处是我的数据的链接，我的代码如下。请注意，第 1 列 term 是响应变量，其余列是预测变量。
库（插入符号）
图书馆（tidyverse）

# 加载数据

加载（&#39;Processed_Data/df1.Rdata&#39;）

# 设置不同的 CV 方法：

trC.cv &lt;- trainControl(method = &quot;cv&quot;, number = 10)
trC.repeatedcv &lt;- trainControl(方法 = &quot;repeatedcv&quot;, 数量 = 10, 重复 = 10)
trC.LGOCV &lt;- trainControl(方法 = “LGOCV”, p = 0.8)
trC.boot &lt;- trainControl(method = &quot;boot&quot;)
trC.LOOCV &lt;- trainControl(method = &quot;LOOCV&quot;)

# 将 CV 方法合并到列表中：

l.trCon &lt;- 列表(trC.boot, trC.cv, trC.repeatedcv, trC.LGOCV, trC.LOOCV)
名称(l.trCon) &lt;- c(“trC.boot”、“trC.cv”、“trC.repeatedcv”、“trC.LGOCV”、“trC.LOOCV”)

# 比较不同种子/CV方法中的ncomp的函数：

fun.compare.plsr.models &lt;- 函数（df，种子）{
  df.i &lt;- data.frame(seed = NA, CV.method = NA, ncomp = NA) # 对于每个 i 循环，将 df 初始化为 cbind：
  for (i in seq_along(seeds)){ # 循环遍历种子：
    v.CV.method &lt;- NA # 为每个 j 循环初始化 Cv 方法和 ncomp 的向量：
    v.ncomp &lt;- NA
    for(j in seq_along(l.trCon)){ # 循环 CV 方法：
      set.seed(i) # 设置种子：
      model.j &lt;- train(term ~., data = df, # 使用 j 循环对应的 CV 方法构建模型：
                       方法=&#39;请&#39;，
                       规模=真，
                       trControl = l.trCon[[j]],
                       tuneGrid = data.frame(ncomp = c(1:30)))
      v.CV.method[j] &lt;- names(l.trCon)[j] # 设置 j 循环的向量元素：
      v.ncomp[j] &lt;- model.j$bestTune[1,1]
    }
    df.j &lt;- data.frame(seed = i, CV.method = v.CV.method, ncomp = v.ncomp) # 使用 j 循环的结果向量创建 df：
    df.i &lt;- rbind(df.i, df.j) # cbind j 循环 df 到整体 df:
  }
  df.i &lt;- df.i[-1,] # 删除第一行，因为它不适用：
  返回（df.i）
}

# run 函数（运行需要一分钟）：

system.time(df.compare10 &lt;- fun.compare.plsr.models(df = df1, seeds = 1:10)) # 95 秒

# 看结果：

ggplot(df.compare10, aes(x = 种子, y = ncomp, 颜色 = CV.method)) +
  几何线（）

除了这篇文章，我无法找到与“模型调整参数不稳定”相关的在线资源（同样在那篇文章中，他们没有提供不稳定的示例，我相信我在这里看到了）。
我看到参数不稳定是什么意思？我能做些什么来解决这个问题吗？
我已经研究过 PCA 来识别异常值观察结果，但我对于是否丢弃数据犹豫不决。
我唯一的另一个想法是响应变量和预测变量之间的关系并不存在于 plsr 模型的框架中。也许我应该尝试另一个模型？我是 caret 和交叉验证的新手，所以请原谅这篇文章的多个问题/含糊之处。我确实尝试在网络上搜索这方面的帮助，但我没有找到任何与我的类似的示例。
非常感谢任何帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/644826/what-can-i-do-about-model-tuning-parameter-instability</guid>
      <pubDate>Thu, 11 Apr 2024 18:29:24 GMT</pubDate>
    </item>
    <item>
      <title>导出新实验的 F 统计量</title>
      <link>https://stats.stackexchange.com/questions/644825/deriving-the-f-statistic-for-a-new-experiment</link>
      <description><![CDATA[我正在解决一个家庭作业问题，我得到了以下信息：

使用此信息，我能够复制 ANOVA 表：
#从 EXAM II 创建数据帧
分数 &lt;- c(7,8,6,7,8,9,4,5,8,8,7,7,5,7,2,4,5,6,3,4,9,10, 7,9,5,7,1,4,4,6,5,5)

plant.type&lt;-c(“WT”、“WT”、“MU”、“MU”、“WT”、“WT”、“MU”、“MU”、” WT”、“WT”、“MU”、“MU”、“WT”、“WT”、“MU”、“MU”、“WT”、“WT”、“ MU”、“MU”、“WT”、“WT”、“MU”、“MU”、“WT”、“WT”、“MU”、“MU”、“ WT”、“WT”、“MU”、“MU”)

锅 &lt;-c(1,1,2,2,1,1,2,2,1,1,2,2,1,1,2,2,1,1,2,2,1,1, 2,2,1,1,2,2,1,1,2,2)

托盘 &lt;- c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5,6,6, 6,6,7,7,7,7,8,8,8,8)

农药&lt;-c(“H”、“H”、“H”、“H”、“L”、“L”、“L”、“L”、“H”； 、“H”、“H”、“H”、“L”、“L”、“L”、“L”、“L”、“L”、“L”等。 、“L”、“H”、“H”、“H”、“H”、“L”、“L”、“L”、“L”、“H” 、“H”、“H”、“H”）

full.df &lt;- data.frame(plant.type = as.factor(plant.type),
                      分数=分数,
                      锅=as.因子(锅),
                      托盘=as.factor(托盘),
                      农药=as.factor(农药))

anova(model1 &lt;- lm(score~农药+托盘+植物.类型+农药:植物.类型+托盘:植物.类型, data=full.df)

对于一个问题，我们需要计算 ${\overline{\mu_{l}}= \frac{(\mu_{WTl} + \mu_ {MUl})}{2}}$ 其中 $l = L, H$。我们测试 $H_0: \overline{\mu_{.l}} = \overline{\mu_{.H}}$。
我对如何计算 F-stat 有点困惑，因为之前我们的响应只是作为预测变量函数的平均分数，但现在它是“l”以下的平均分数。
我尝试通过使用线性模型找到作为植物类型和农药函数的平均得分来获得 F 检验统计量来解决该问题。因为我们本质上是比较不同两种不同农药水平下植物类型之间的变异性。：
grp.df &lt;- full.df %&gt;% select(植物类型, 农药, 分数)

model2 &lt;- lm(分数 ~ plant.type + 农药,data=grp.df)

摘要（模型2）

但是，我不确定我的做法是否正确。]]></description>
      <guid>https://stats.stackexchange.com/questions/644825/deriving-the-f-statistic-for-a-new-experiment</guid>
      <pubDate>Thu, 11 Apr 2024 18:15:54 GMT</pubDate>
    </item>
    <item>
      <title>为什么选择术语“显着性”($a$) 来表示 I 类错误的概率？</title>
      <link>https://stats.stackexchange.com/questions/644808/why-was-the-term-signifiance-a-chosen-for-the-probability-of-type-i-error</link>
      <description><![CDATA[我目前正在学习“统计 1”作为我计算机科学学位的一部分，我很难理解“重要性”的概念。
我们获得了以下定义：
$H_0$ - 零假设
$H_1$ - 替代假设。
$R$ - $H_0$ 拒绝区
$\bar{R}$ - $H_0$ 无拒绝区
$\begin{aligned} \alpha &amp;= P(\text{I 类错误})\\&amp;=P_{H_0}(\text{拒绝 } H_0 ) \\&amp;= P_{H_0}(X\in R) \\&amp;; \end{对齐} $
虽然我相信我已经很好地掌握了这些定义以及它们与 p 值的关系，但术语“显着性”并不适用。我仍然感到困惑。
我理解某事物“具有统计显着性”的概念，但显着性水平越高，出错的风险就越大，这似乎违反直觉。直觉上，当某件事非常重要时，我预计风险会较低。
有人可以解释一下为什么“重要性”一词如此重要吗？被选中了？
对于那些正在寻找的人来说，这些是关于这些术语的统计含义的一些非常好的讨论。我正在寻找更直观的解释来解释为什么选择这个术语。
比较和对比，p -值、显着性水平和 I 类错误
显着性水平 alpha 与 1 类误差 alpha 之间的关系是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/644808/why-was-the-term-signifiance-a-chosen-for-the-probability-of-type-i-error</guid>
      <pubDate>Thu, 11 Apr 2024 15:02:37 GMT</pubDate>
    </item>
    <item>
      <title>RCT 中的组内差异与组间差异：如何处理？</title>
      <link>https://stats.stackexchange.com/questions/644806/within-group-vs-between-group-variance-in-an-rct-how-to-handle</link>
      <description><![CDATA[我有一个问题，当随机对照试验中组内方差与组间方差存在明显差异时，朴素治疗效果估计器可能会出错多远，以及我们实际上可以采取什么措施。
我正在进行一项实验，组内方差明显大于组间方差。为了对此进行一些统计，我们尝试查看组之间是否有 ~\$2 ，但组内方差为 ~\$20。由于一系列操作原因，我无能为力来改变这一点。
问题在于，各组中的实验前差异可能已经达到 ~$1 或 ~$2。在这种情况下，我该如何估计治疗效果（组间差异）？
进行 RCT 时，治疗效果始终是治疗组和对照组之间的简单差异：
$$ \Delta_{ATE} = \frac{1}{n_T}\sum_{i\in n_T}Y_{i} - \frac{1}{n_C} \sum_{i\in n_C}Y_{i}$$
我们可以使用实验前的差异来提高此估计的精度（降低标准误差），但从根本上来说我们无法改变估计。
为了验证这一点，我模拟了简单差异、CUPED、差异中的差异、协方差的潜在分析和变化分析估计器。我发现，在预期中，估计量都具有相同的值（简单差异），但当基本上不存在组内方差时，它们的标准误差有所不同。当组间存在一些方差时，所有这些估计量都具有相同的标准误差。
这是正确的吗？在这种情况下是否有首选估计器，或者我无法做比简单差异更好的事情？会例如面板数据模型帮助（我对这些不太了解）？另外，未来这个案例的样本量指南是多少，我们目前一直在使用Lehr规则：
$$ N = 16\frac{\sigma^2}{\Delta^2}$$
估计样本大小。


我使用以下函数生成了数据：
导入 pandas 作为 pd
将 numpy 导入为 np
将seaborn导入为sns

defgenerate_data_v2(alpha, beta, gamma, delta, rng, Between_variation, N, seeds=42):
# 个人
我 = 范围(1, N+1)

# 治疗状态
d = rng.二项式(1, 0.5, N)

# 个体结果预处理
# 治疗前结果的产生具有一定的基本变化水平
y0 = alpha + beta*d + rng.normal(0, 1, N)

# 修改治疗后结果以纳入 Between_variation 控制
# 这将确保跨单元的处理后变化由 Between_variation 参数驱动
y1 = y0 + gamma + delta*d + rng.normal(0, Between_variation, N) * d # 放大处理单位的变异

# 生成数据帧
df = pd.DataFrame({&#39;i&#39;: i, &#39;ad_campaign&#39;: d, &#39;revenue0&#39;: y0, &#39;revenue1&#39;: y1})

返回df

df =generate_data_v2(alpha, beta, gamma, delta, rng, Between_variation=5, N=100)

results_no_group_var = 模拟（
    func=generate_data_v2, # 您的数据生成函数
    func_args={
        “阿尔法”：阿尔法，
        “测试版”：测试版，
        ‘伽玛’：伽玛，
        “增量”：0.0，
        &#39;rng&#39;：rng，
        “N”：N，
    &#39;变化之间&#39;：1，
    },
    K=K,
    x=&#39;收入0&#39;
）

results_group_var = 模拟（
    func=generate_data_v2, # 您的数据生成函数
    func_args={
        “阿尔法”：阿尔法，
        “测试版”：测试版，
        ‘伽玛’：伽玛，
        “增量”：0.0，
        &#39;rng&#39;：rng，
        “N”：N，
    &#39;变化之间&#39;：5，
    },
    K=K,
    x=&#39;收入0&#39;
）

sns.kdeplot(data=结果，x=“估计”，hue=“估计器”);
plt.title(&#39;模拟分布&#39;);
]]></description>
      <guid>https://stats.stackexchange.com/questions/644806/within-group-vs-between-group-variance-in-an-rct-how-to-handle</guid>
      <pubDate>Thu, 11 Apr 2024 14:38:13 GMT</pubDate>
    </item>
    </channel>
</rss>