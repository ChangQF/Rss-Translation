<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Thu, 21 Dec 2023 00:59:13 GMT</lastBuildDate>
    <item>
      <title>我们可以在基于速率的指标（例如转化率）上使用 Google 的 causalImpact 方法吗？</title>
      <link>https://stats.stackexchange.com/questions/635385/can-we-use-causalimpact-method-by-google-on-a-rate-based-metric-like-conversion</link>
      <description><![CDATA[我有一个影响测量问题，其中主要指标是转化率，并且不可能运行 A/B 测试，所以我正在考虑 causalImpact 包。有人在基于速率的指标上使用过 causalImpact 包吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/635385/can-we-use-causalimpact-method-by-google-on-a-rate-based-metric-like-conversion</guid>
      <pubDate>Wed, 20 Dec 2023 23:49:19 GMT</pubDate>
    </item>
    <item>
      <title>我们可以解释趋势季节性分解中的残差吗？</title>
      <link>https://stats.stackexchange.com/questions/635383/can-we-interpret-residuals-in-trend-seasonality-decomposition</link>
      <description><![CDATA[一般情况：建立市场模型以进一步评估我们的算法的质量。 （预测最优价格、需求预测等）
目前的方法：采用产品的两个特征——价格和需求，将其表示为时间序列。
下一步我试图“摆脱”该系列的时间依赖性——通过“减法”或“划分”趋势和季节性成分。
分解模型有两个主要假设：线性和乘法。我们假设我们的平均需求曲线类似于某些 $\exp(x)$ 函数，因此我们必须使用乘法方法。
我尝试过使用 statsmodels.tsa.seasonal 的一些经典方法：  seasonal_decompose 和 STL。使用经典方法，我只能分解一个特征（需求或价格）。
结果是这样的（对于需求系列，某些类别的平均值，一年的数据）：

还使用了 prophet，但这次有两个特征（这次假设乘法模型，但在 Prophet 乘法中）模型是
$$y(t) = \text{趋势}(t)*(1 + \text{季节}(t) + \text{beta} * \text{残差}( t))$$，与经典不同
$$y(t) = \text{趋势}(t)*\text{季节}(t)*\text{残差}(t)$$
我已经拟合了该系列，然后根据我之前拟合的整个数据进行预测，获得趋势和季节性成分，得到 $\text{residual}(t)$ :

那么，有两个问题：

这是消除趋势和季节性成分的有效方法吗？尤其是在数据量较少的情况下。
我们可以将残留成分解释为“干净”吗？要求？或者更高的噪声值不允许我们解释这个组件？
]]></description>
      <guid>https://stats.stackexchange.com/questions/635383/can-we-interpret-residuals-in-trend-seasonality-decomposition</guid>
      <pubDate>Wed, 20 Dec 2023 23:03:07 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用 Fisher 精确检验或单独 t 检验来分析这些数据吗？</title>
      <link>https://stats.stackexchange.com/questions/635381/should-i-use-fishers-exact-test-or-individual-t-tests-to-analyze-this-data</link>
      <description><![CDATA[突变体OH1-Y151F和OH1-Y152F对STAT1转录因子转位至细胞核的影响。
进行了两个独立的实验来评估 STAT1 在转染以下质粒的 HeLa 细胞中的定位：pHSV-IRES-EGFP、pHSV-OH1WT-IRES-EGFP、pHSV-OH1C112S-IRES-EGFP、pHSV-OH1Y151F-IRES -EGFP 和 pHSV-OH1Y152F-IRES-EGFP。由于这些质粒载体编码 EGFP 与感兴趣的蛋白质串联的表达，因此使用绿色荧光发射来选择用于分析的细胞。通过这种方式，在表达 OH1 变体或不表达 OH1 变体的细胞（仅表达 EGFP 的对照）中评估不同的条件。在这些测定中，总共对 214 个指示 GFP 表达以及 OH1 磷酸酶表达的荧光细胞进行了计数。
下表显示荧光细胞数换算为%，每行相加为100%。我应该使用百分比来进行统计测试还是应该仅使用细胞计数来进行？我是否应该对两个实验取平均值？如果一个实验显示的数据与第二个实验相矛盾怎么办？
如有任何澄清，我们将不胜感激。谢谢。
]]></description>
      <guid>https://stats.stackexchange.com/questions/635381/should-i-use-fishers-exact-test-or-individual-t-tests-to-analyze-this-data</guid>
      <pubDate>Wed, 20 Dec 2023 22:46:17 GMT</pubDate>
    </item>
    <item>
      <title>GEE 参数的渐近正态性</title>
      <link>https://stats.stackexchange.com/questions/635379/asymptotic-normality-for-gee-parameters</link>
      <description><![CDATA[在著名的Liang和Zeger 1986年关于GEE的论文中https://www.jstor .org/stable/2336267?seq=9，他们使用标准 m 估计参数绘制了一个证明：（未说明的）正则条件 + 一阶泰勒展开 + 概率中的余数消失（由正则条件覆盖） ) + 用于证明速率 $\sqrt{n_c}$ 的渐近正态性的常规收敛参数，其中 $n_c$ &gt; 是簇的数量。通常引用独立数据的中心极限定理 (CLT) 的哪个版本来实现渐近正态性？我觉得我从来没有在任何地方看到过特别指出这一点。
为了进一步详细说明，让 $\varphi_{n_i}(Z_i;\theta)$ 其中 $ Z_1,...,Z_{n_c}$ 是相互独立的组级数据，使得 $E(\varphi_{n_i}(Z_i;\theta))=0 $ 对于所有 $i=1,...n_c$ 和 $n_i$ 是簇的大小。假设求解无偏估计方程
$$\sum_{i=1}^{n_c} \varphi_{n_i}(Z_i;\theta) = 0$$
为您提供 GEE 估算器。为简单起见，假设随机效应的方差已知。 $\varphi_{n_i}(Z_i;\theta)$ 是独立的并且具有相同的维度，但它们只是相互独立的；事实上，鉴于 $n_i$ 不是常数，它们通常不会相同分布。系数渐近正态性的证明要求（除其他外）$\sqrt{n_c}\left (\frac{1}{n_c} \sum_{i=1}^ {n_c} \varphi_{n_i}(Z_i;\theta)\right )$ 分布收敛于均值零多元正态分布。通常需要哪个版本的 CLT 来发表此声明？]]></description>
      <guid>https://stats.stackexchange.com/questions/635379/asymptotic-normality-for-gee-parameters</guid>
      <pubDate>Wed, 20 Dec 2023 21:40:57 GMT</pubDate>
    </item>
    <item>
      <title>灾难暴露作为生存分析中的治疗 - 多重“治疗”/“治疗”前趋势（R 中）</title>
      <link>https://stats.stackexchange.com/questions/635378/disaster-exposure-as-treatment-in-survival-analysis-multiple-treatments-pr</link>
      <description><![CDATA[我正在考虑使用考克斯比例风险模型来研究灾难后的迁移。我在 T1 和 T2 发生了两次灾难（相隔两年）。我创建了两个模型来研究灾难发生后 18 个月内的暴露风险，以了解灾难暴露是否与迁移风险的变化相关。遭受灾难的个人更有可能搬家（模型有点复杂，但我认为这是问题的重要部分）。
灾后
coxph(Surv(时间1，状态1) ~ 灾难1 + 控制，数据 = T1人口)
coxph(Surv(时间2, 状态2) ~ 灾难2 + 控制, 数据 = T2Population)
其中T1Population是T1区域的人口； time1 是 T1 后 18 个月内迁移的时间，status1 是个体是否在 time1 内迁移。 disaster1 是针对遭受灾难的治疗。
对于灾难 2，T2 后 18 个月以及 T2 区域的人口（由于临时迁移，与 T1 人口相似但不完全相同），重复此操作。 disaster2 是针对遭受灾难的治疗。
我的顾问希望我看看是否存在内生性问题，即可能发生灾害的地区是否存在与有或没有灾害暴露情况下更大的流动性相关的因素。因此，我使用 T0（T1 前两年）重新创建了模型，以观察受灾地区的个人是否比对照组更有可能移动。事实确实如此，但效果没有那么强烈。
灾前稳健性检查
coxph(Surv(time0, status0) ~ 灾难1 + 控制，数据 = T0Population)
coxph(Surv(time0, status0) ~ 灾难2 + 控制, 数据 = T0Population)
其中T0Population是T0时该地区的人口数量； time0是T0后18个月内迁移的时间，status0是个体是否在time0内迁移。这些模型中的disaster1和disaster2指的是个体是否处于将受到这些灾害袭击的地区。
现在我的顾问希望我将这些组合成一个模型。基本上，从 T0 到 T2+18 个月有 5.5 年的时间，人口将是 T0 时居住在那里或 T2 时搬入该地区的所有人。将有一个治疗前阶段和两个治疗后阶段。我不知道这是否有效，或者如何对其进行建模。有谁知道这是否有意义，以及我如何在 R 中对其进行编码。]]></description>
      <guid>https://stats.stackexchange.com/questions/635378/disaster-exposure-as-treatment-in-survival-analysis-multiple-treatments-pr</guid>
      <pubDate>Wed, 20 Dec 2023 21:36:30 GMT</pubDate>
    </item>
    <item>
      <title>如何建立具有多个协变量和交互作用的线性混合效应模型</title>
      <link>https://stats.stackexchange.com/questions/635377/how-to-set-up-a-linear-mixed-effects-model-with-multiple-covariates-and-interact</link>
      <description><![CDATA[我对当前关注的项目有点不确定。
我有一个感兴趣的结果，例如 $y$，还有一个主要预测变量，例如 $x$，两者都是连续的。我在不同时间点对 $x$ 和 $y$ 的每个主题进行了多次测量。除此之外，我还有一些其他协变量，应该包含在模型中。
我现在的印象是，当我只指定截距+斜率（主要预测变量）的随机效应时，我忽略了时间信息。但是，如果我指定随机截距 + 斜率（时间），我觉得我没有解决研究问题，即主要预测变量如何影响结果变量。
基本上有两个问题：

主要预测因素如何影响结果？
时间如何影响结果？

我不知道如何处理这个/设置随机效果。我从来没有遇到过包含两个随机效应的情况，我认为这不是正确的方法。我可能会将所有协变量作为固定效应，加上交互项 Time * Main Predictor，但是随机效应又如何呢？]]></description>
      <guid>https://stats.stackexchange.com/questions/635377/how-to-set-up-a-linear-mixed-effects-model-with-multiple-covariates-and-interact</guid>
      <pubDate>Wed, 20 Dec 2023 21:24:05 GMT</pubDate>
    </item>
    <item>
      <title>如何在缺乏基本事实的情况下验证无监督异常检测？</title>
      <link>https://stats.stackexchange.com/questions/635373/how-to-validate-unsupervised-anomaly-detection-in-absence-of-ground-truth</link>
      <description><![CDATA[我目前正在从事一个无监督异常检测项目，由于缺乏真实标签，我面临着模型性能验证的挑战。
我正在使用隔离森林、LOF 和自动编码器进行异常检测。
该数据集包含与 Splunk 事件相关的非常稀疏的高维表格数据。此数据集中没有地面实况标签（即已知异常）。
在缺乏基本事实的情况下，有哪些有效的策略或方法可以验证无监督异常检测模型的性能？是否有任何最佳实践或普遍接受的指标可用于衡量检测到的异常的可靠性？
我尝试过 xAI（主要是 SHAP）技术来深入了解异常产生的原因以及哪些特征会导致异常，并执行统计测试以将这些模型输出的异常与所谓的正常数据进行比较。
我的主要目标是建立一种可靠的方法来评估异常检测模型的性能，以确保它有效地对与正常活动相关的数据进行建模，识别真正的异常，而不仅仅是异常值或噪音。
感谢您分享的任何可能有助于应对这一挑战的见解、参考资料或经验。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/635373/how-to-validate-unsupervised-anomaly-detection-in-absence-of-ground-truth</guid>
      <pubDate>Wed, 20 Dec 2023 20:41:49 GMT</pubDate>
    </item>
    <item>
      <title>二元向量中的相似性和普遍性</title>
      <link>https://stats.stackexchange.com/questions/635372/similarity-and-prevalence-in-binary-vectors</link>
      <description><![CDATA[假设我有 N 个向量，长度均为 L。每个向量都是二进制的，由 0 和 1 组成，其中 0 表示“不存在”，1 表示“存在”由其列表示的元素.
例如，考虑代表两个购物篮的两个向量。各有哪些杂货？假设我们想要捕获五种产品（即 L = 5）：牛奶、鸡蛋、奶酪、面包、苹果。这些是按固定顺序排列的“列”。
爱丽丝买了鸡蛋和面包。鲍勃买了牛奶、鸡蛋、奶酪和苹果。
爱丽丝的向量 &lt;- [0, 1, 0, 1, 0]
Bob 的向量 &lt;- [1, 1, 1, 0, 1]
我想要一个能够捕获所有 N 个向量之间相似性的度量。我发现计算这个的方法是首先计算两个向量的每个组合之间的成对距离，生成一个 N × N 矩阵，其中 N(x,y) 表示向量 x 和 y 之间的距离/相异度。最初，我使用的距离度量是欧几里德距离（在 R 中：stats::dist(method=&quot;euclidean&quot;)）。然而，考虑到我使用的是 0 和 1 的二进制向量，似乎使用 Jaccard 距离更合适（在 R: stats::dist(method=&quot;binary&quot;) 中）。有了这个距离矩阵，我就可以用平均距离来衡量向量之间的整体相似程度。
这提出了一个问题：相似性是否一定与流行程度混为一谈？在这里，我将流行度定义为 N 个向量中 1 的比例。
如果我们有长度为 30 的向量，并且这样的向量有 29 个 1，那么将有 30 种可能的向量组合，其中每个可能的位置都有一个 0，其余的都是 1。但是，如果 0 和 1 的数量相等，则有 30 选择 15 向量组合。因此，当患病率高或低时，载体更有可能相似，对吗？
当我使用杰卡德距离时，我观察到相似性和流行率之间呈负相关。但这不应该更像是U型关系吗？高患病率和低患病率应同等对待。上述 29 个 1 的情况应该与 29 个 0 的情况相同。那么为什么这种关系是线性的呢？我尝试使用其他距离度量，例如 Jaccard 指数和 Dice 系数，但在所有情况下，我发现相似性和流行度以线性方式密切相关。
我一直在试图弄清楚是否可以区分相似性和普遍性，如果不能，两者之间的关系应该是什么样的。我对 U 形关系的直觉在流行度和相似度之间是否有意义？我可能不应该对线性关系感到惊讶，或者我使用了错误的距离/相似性度量，所以我将不胜感激您可能提供的任何提示。谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/635372/similarity-and-prevalence-in-binary-vectors</guid>
      <pubDate>Wed, 20 Dec 2023 20:38:46 GMT</pubDate>
    </item>
    <item>
      <title>Pearson 相关 p 值图哪种方法可以更好地调整我的 p 值 BH 或 BY？帮助我理解</title>
      <link>https://stats.stackexchange.com/questions/635371/pearson-correlation-p-values-plot-which-method-better-adjusted-my-p-values-bh-or</link>
      <description><![CDATA[我正在研究 DEG 数据集，并且已经完成了 Pearson 相关性来为下游分析（共表达网络）准备数据集，因此这不是探索阶段。当我使用 Hmisc 对 R 进行 pearson 处理时，我得到 P 和 R 值。我试图使用这些 p 值来确保相关值 r 显着，但由于我不知道如何纠正皮尔逊偏差，所以我选择了 p.adjust。
我将附上 3 个图像 1，分别表示原始 p 值、BY 调整后的 p 值和 BH 调整后的 p 值。看看BH和native，我认为BH没有调整任何东西，曲线仍然显示出从1到0的完美逐渐下降，没有调整p.values。而使用 BY 时，1 处有一条明显的新曲线，显示了之前可能被认为很重要的调整值。
我最初想选择BY的原因是因为它假设两个相关变量相互调节或依赖，这符合这个生物学概念。
我的推理正确吗？
我尝试阅读有关 p 值的预期图，以及有关此问题的其他 stackoverflow 先前问题，不幸的是没有完全解决我的问题。我希望在这个问题中得到针对我的案例的更有针对性的答复。
谢谢。


]]></description>
      <guid>https://stats.stackexchange.com/questions/635371/pearson-correlation-p-values-plot-which-method-better-adjusted-my-p-values-bh-or</guid>
      <pubDate>Wed, 20 Dec 2023 20:34:48 GMT</pubDate>
    </item>
    <item>
      <title>探索性分析以找出低分者的特征</title>
      <link>https://stats.stackexchange.com/questions/635362/exploratory-analysis-to-find-out-characteristics-of-low-scorers</link>
      <description><![CDATA[我目前正在研究反馈调查的三个具体问题，并负责找出得分最低者的特征，看看在这些项目上得分较低的人是否有任何模式或共同特征。这些特征是人口统计变量（年龄、性别、教育程度等）以及调查特有的其他变量。
调查的设置方式是，我们使用顶框评分。顶盒评分的工作原理是，每个项目都有一个指定的最佳答案供参与者选择（如果回答按比例进行，有时会有多个最佳答案）。一个例子是“您觉得您在整个过程中受到尊重吗？”的项目。顶框对此的响应将是“是”。因为我们希望参与者感到受到尊重。因此，当我们谈论低分者时，我们谈论的是那些没有对该特定项目的顶框回复做出回应的人。
我同时使用 SAS 和 R，并且正在考虑如何探索这一点，目前在 SAS 中我正在使用 proc freq  查看这些特定项目的分数并单独查看每个变量，但我想知道是否有人对查看分组变量有任何想法，或者有任何一般想法。
谢谢！！]]></description>
      <guid>https://stats.stackexchange.com/questions/635362/exploratory-analysis-to-find-out-characteristics-of-low-scorers</guid>
      <pubDate>Wed, 20 Dec 2023 17:55:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将我之前的信念与贝塔分布相匹配？</title>
      <link>https://stats.stackexchange.com/questions/635325/how-to-match-my-prior-beliefs-to-beta-distribution</link>
      <description><![CDATA[我有一些数据，我认为这些数据来自二项式分布。我还有一些来自过去实验的旧数据，我想将其作为我之前信念的基础。旧的数据观测值是： $$6, 5, 4, 2, 2, 1, 5, 4, 5, 1 $$
我想先根据这些观察结果确定 beta 的均值和方差。现在，我可以使用该数据的 MLE 作为平均值，但是我可以使用什么作为方差呢？如果我要获取此数据的方差并将其设置为我之前的方差，则这将不起作用，因为我们将得到负值 $\alpha$或 $\beta$。那么，根据先验数据，确定先验 beta 方差的正确方法是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/635325/how-to-match-my-prior-beliefs-to-beta-distribution</guid>
      <pubDate>Wed, 20 Dec 2023 09:25:52 GMT</pubDate>
    </item>
    <item>
      <title>如何从变分自动编码器进行降维</title>
      <link>https://stats.stackexchange.com/questions/635292/how-to-do-dimension-reduction-from-a-variational-autoencoder</link>
      <description><![CDATA[我正在考虑变分自动编码器。据我了解，在编码部分中，您压缩为 px1 张量，然后创建一个 $\mu$ 和 $\sigma$ 我选择的维度（尽管小于 $p$）。解码层则相反。
但是如果我想使用 VAE 进行降维怎么办？显然，我不会从 $\mu$ 和 $\sigma$ 中随机采样。我是否只转到 px1 张量？
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/635292/how-to-do-dimension-reduction-from-a-variational-autoencoder</guid>
      <pubDate>Tue, 19 Dec 2023 19:51:26 GMT</pubDate>
    </item>
    <item>
      <title>混合效应模型中 p 值计算的当前观点</title>
      <link>https://stats.stackexchange.com/questions/635275/current-perspectives-on-p-value-computation-in-mixed-effects-models</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/635275/current-perspectives-on-p-value-computation-in-mixed-effects-models</guid>
      <pubDate>Tue, 19 Dec 2023 17:06:52 GMT</pubDate>
    </item>
    <item>
      <title>功能数据分析 - 每条曲线的最少点数？</title>
      <link>https://stats.stackexchange.com/questions/635135/functional-data-analysis-minimum-points-per-curve</link>
      <description><![CDATA[我正在着手一个项目，我想在其中使用功能数据分析 (FDA)。我有几千个离散曲线对象，我想在它们上拟合连续时间曲线。这些离散曲线对象各自包含大约 6 个点，用于拟合每条连续时间曲线。
我担心 6 个点太少，无法可靠地拟合曲线。根据经验，申请 FDA 的最低分数是多少？我看到的例子对数据进行拟合曲线，每条离散曲线多了 2 个数量级的点，我们在其上拟合连续曲线。]]></description>
      <guid>https://stats.stackexchange.com/questions/635135/functional-data-analysis-minimum-points-per-curve</guid>
      <pubDate>Sun, 17 Dec 2023 19:51:14 GMT</pubDate>
    </item>
    <item>
      <title>对异方差和自相关 VIF 公式具有鲁棒性</title>
      <link>https://stats.stackexchange.com/questions/635123/robust-to-heteroskedasticity-and-autocorrelation-vif-formula</link>
      <description><![CDATA[使用辅助回归计算的经典 VIF 系数为 $VIF_i = \frac{1}{1 - R^2_i}$。但这个公式源自经典线性模型假设（误差球面矩阵），其中 $Var(\hat{\beta}_{OLS}) = \sigma^2 (X&#39;X )^{-1}$。我的问题是基于系数方差的稳健表示的 VIF 公式的任何推广 $Var(\hat{\beta}_{OLS}) = (X&#39;X)^{- 1} X&#39; \Sigma X (X&#39;X)^{-1}$ 可以同时容纳 White 和 Newey-West 估计器吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/635123/robust-to-heteroskedasticity-and-autocorrelation-vif-formula</guid>
      <pubDate>Sun, 17 Dec 2023 16:11:05 GMT</pubDate>
    </item>
    </channel>
</rss>