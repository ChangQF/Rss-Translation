<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Jun 2024 03:18:11 GMT</lastBuildDate>
    <item>
      <title>逻辑回归</title>
      <link>https://stats.stackexchange.com/questions/648730/logistic-regresion</link>
      <description><![CDATA[
&gt; Reglog_JKK &lt;- glm(JKK ~ X1+X2+X3+X4+X5+X6+X7+X8+X9, 
+ family = binomial)
&gt; vif(Reglog_JKK)
GVIF Df GVIF^(1/(2*Df))
X1 3.411006 1 1.846891
X2 4.899361 1 2.213450
X3 31.723130 5 1.412985
X4 19.833157 3 1.645250
X5 4.305749 1 2.075030
X6 12.188838 3 1.517028
X7 5.436424 2 1.526962
X8 2.525105 1 1.589058
X9 3.045529 1 1.745144
```
]]></description>
      <guid>https://stats.stackexchange.com/questions/648730/logistic-regresion</guid>
      <pubDate>Thu, 06 Jun 2024 02:55:23 GMT</pubDate>
    </item>
    <item>
      <title>KNNImputer 如何存储训练集的拟合值？</title>
      <link>https://stats.stackexchange.com/questions/648728/how-does-knnimputer-stores-fitted-values-of-the-train-set</link>
      <description><![CDATA[如果这里有人熟悉 Scikit-learn 的 KNNImputer 实现，我会很想向他学习。
当你在训练数据上安装 Imputer 变换器时，它会保留训练集的参数以便相应地转换测试集（例如 SimpleImputer(method=&#39;mean&#39;) 将存储训练集所有特征向量的平均值并用它们来插补测试集）。
但我无法理解 KNNImputer 除了存储所有训练样本外如何做到这一点。
谢谢你的回答。]]></description>
      <guid>https://stats.stackexchange.com/questions/648728/how-does-knnimputer-stores-fitted-values-of-the-train-set</guid>
      <pubDate>Thu, 06 Jun 2024 01:47:56 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯分类器的收敛</title>
      <link>https://stats.stackexchange.com/questions/648727/convergence-of-a-bayesian-classifier</link>
      <description><![CDATA[背景
令 $y_k$ 为时间 $k$ 的噪声测量值，令 $\{p_{k-1}(i)\}_{i=1}^n$ 为（离散的）先验概率分布。使用贝叶斯规则，可以根据以下公式更新 $y_k$ 函数中的先验
\begin{equation}
p_k(i)\triangleq \frac{\mathcal{L}(y_k|i)\,p_{k-1}(i)}{\sum_{j=1}^n \mathcal{L}(y_k|j)\,p_{k-1}(j)} \qquad i=1,\dots,n
\end{equation&gt;
其中 $\mathcal{L}(y_k|i)$ 是某个似然函数。然后，我们可以在时间 $k+1$ 重复该过程，在 $y_{k+1}$ 函数中获得 $\{p_{k+1}(i)\}_{i=1}^n$，对于 $k+2,k+3,\dots$ 也是如此
我想了解当 $k\to\infty$ 时会发生什么。我的直觉是，如果可能性对于每个 $k$ 都是恒定的，即
\begin{equation*}
\tag{1}
\mathcal{L}(y_k|i)=C_k \qquad \forall i=1,\dots,n
\end{equation*&gt;
那么根本不会执行任何更新，也就是说，我们每次都会保留初始分布。更准确地说，对于每个 $j\in\mathbb{N}$
\begin{equation*}
p_{k+j}(i) =p_k(i) \qquad \forall i=1,\dots,n
\end{equation*&gt;
另一方面，我从数值实验中看到，概率分布趋向于收敛到唯一集中在单个类 $i^*$ 中的狄拉克分布，即
\begin{equation*}
\lim_{k\to\infty} p_k(i)= \delta_{i^*}(i)
\end{equation*&gt;
就我所见，这种收敛始终是正确的。这可能是真的，因为由于噪音，$(1)$永远不会发生。
问题

排除奇异情况$(1)$，后验总是收敛到狄拉克分布，这是真的吗？

排除奇异情况$(1)$，后验可以收敛到不同于起始分布或狄拉克分布的某个概率分布吗？

]]></description>
      <guid>https://stats.stackexchange.com/questions/648727/convergence-of-a-bayesian-classifier</guid>
      <pubDate>Thu, 06 Jun 2024 01:38:16 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中计算 CECDF（互补经验累积分布函数）</title>
      <link>https://stats.stackexchange.com/questions/648726/computing-the-cecdf-complementary-empirical-cumulative-distribution-function-i</link>
      <description><![CDATA[可以使用 R 中的函数 ecdf() 计算 ECDF。
我有以下表达式，我想根据 CECDF 来定义，但由于某些奇怪的原因，事情并不匹配。
表达式为
\begin{align}
p &amp;= \frac{\#\{d \geq a\}}{\#\{d\}} \\[1mm]
\end{align&gt;
应该等于 $1 - F_d(a)$，其中 $F$ 表示 CDF。
在 R 中，$p$ 只是
p &lt;- mean(d &gt;= a)。

使用 ecdf()，这是
ecdf_d &lt;- ecdf(a)
p_cecdf &lt;- 1 - ecdf_d

这里有一个完整的 MRE 来说明这个问题：
set.seed(123)

d &lt;- rnorm(100)
f &lt;- rnorm(50)
a &lt;- min(f)
b &lt;- min(a)
d_ecdf &lt;- ecdf(d)
f_ecdf &lt;- ecdf(f)
(p_cecdf &lt;- 1 - d_ecdf(b))
# [1] 0.99
(q_cecdf &lt;- 1 - f_ecdf(a))
# [1] 0.98
res_d &lt;- mean(d &gt;= b)
# [1] 0.99
res_f &lt;- mean(f &gt;= a)
# [1] 1

除非我忘记了什么，否则不应该是 $p_\mathrm{cecdf} = res\_d$ 和 $q_\mathrm{cecdf} = res\_f$]]></description>
      <guid>https://stats.stackexchange.com/questions/648726/computing-the-cecdf-complementary-empirical-cumulative-distribution-function-i</guid>
      <pubDate>Thu, 06 Jun 2024 01:33:54 GMT</pubDate>
    </item>
    <item>
      <title>辛普森悖论如何解释结果</title>
      <link>https://stats.stackexchange.com/questions/648717/simpsons-paradox-how-interpret-results</link>
      <description><![CDATA[我正在构建 GLM 来研究环境变量对青蛙占有率和丰度的影响（负二项式）。我遇到了估计值反转的问题，从我在网上找到的信息来看，这似乎是由辛普森悖论引起的。受其影响的两个变量是 bio6（最冷月份的平均最低温度）和年降雨量。Bio6 在单独建模时具有负估计值，但在与其他参数建模时具有正估计值。年降雨量有同样的现象，但方向相反（从正估计值变为负估计值）。
顶级模型有 bio6+rain+bio8+bio17（其中 bio8 是最潮湿季度的平均温度，即澳大利亚东部的夏季，bio17 是最干旱季度的降水量，即冬季）。我使用 VIF&lt;10 作为过滤器以避免多重共线性，但在这种情况下 VIF 甚至&lt;5（bio8 约为 4.4，其他所有都小于 3）。此外，我还尝试避免将 bio6 与 bio8 以及 bio17 与年降雨量相结合，以避免出现有关同一现象的信息，尽管它们之间的相关性并不高，但这种情况仍然会发生。例如，我做了一个模型 bio6+bio17，bio6 仍然从负值反转为正值（VIF 2.5）。同样，我做了一个模型 bio8+年降雨量，年降雨量仍然从正值反转为负值（VIF 1.38）。无论变量是否标准化，都会发生这种情况。现在我陷入了困境，因为我必须讨论这些变量对青蛙的影响，我不知道该如何解释这些相反的结果。我该如何处理这种情况？]]></description>
      <guid>https://stats.stackexchange.com/questions/648717/simpsons-paradox-how-interpret-results</guid>
      <pubDate>Wed, 05 Jun 2024 22:24:53 GMT</pubDate>
    </item>
    <item>
      <title>自回归交叉滞后模型</title>
      <link>https://stats.stackexchange.com/questions/648716/autoregressive-cross-lagged-models</link>
      <description><![CDATA[我正在研究一个自回归交叉滞后模型，该模型有两个测量值和三个时间点。从 $t_1$ 到 $t_2$ 的路径是显著的，但从 $t_2$ 到 $t_3$ 的路径都不显著。我对编码/模型/等非常不熟悉，我想知道最好的前进路径是什么？我读过关于限制某些变量的文章，但我真的不明白如何最好地描述/解释这些不显著的路径。我知道这可能会令人困惑，所以感谢您的耐心。]]></description>
      <guid>https://stats.stackexchange.com/questions/648716/autoregressive-cross-lagged-models</guid>
      <pubDate>Wed, 05 Jun 2024 21:24:33 GMT</pubDate>
    </item>
    <item>
      <title>似然比未呈具有正确自由度的卡方分布（威尔克斯定理）</title>
      <link>https://stats.stackexchange.com/questions/648715/likelihood-ratios-not-distributed-as-a-chi2-distribution-with-the-correct-dof-w</link>
      <description><![CDATA[我对混合模型执行贝叶斯推理，使得 μ 是混合中特征的混合权重
p(x|μ,theta) = μ p_feature(x|theta) + (1-μ) p_nofeature(x|theta)。
我计算 T(μ) =-2* ( log( p(x|μ=0, theta(μ=0)) - log p(x | μ*, theta*) )，其中 x 是固定的，其中
theta(μ=0) = \arg max_theta p(x | μ=0, theta)（零假设）
μ*, theta* = \arg max_{μ,theta) p(x | μ, theta)
我无法获得可能性，因此我通过最大化 p(μ, theta|x) / p(μ, theta) 找到上述两个解决方案，这与p(x | μ, theta) / p(x)。
我通过从 theta 的先验中抽样来计算几个 x 的 LLR 统计量，因为 x= f(μ, theta)。https://towardsdatascience.com/the-likelihood-ratio-test-463455b34de9 告诉我我应该获得一个卡方分布，其中 dof 是整个模型的模型参数之间的差异 - 零假设。但是，这是我从两个自由度为 6 的独立模型获得的结果。


我得到了第二个模型的自由度卡方 19，并且几乎是均匀分布。我期望自由度卡方 6。我遗漏了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/648715/likelihood-ratios-not-distributed-as-a-chi2-distribution-with-the-correct-dof-w</guid>
      <pubDate>Wed, 05 Jun 2024 21:21:27 GMT</pubDate>
    </item>
    <item>
      <title>有没有什么技术可以帮助我实现所需的计算而无需读取全部 1000 万行？</title>
      <link>https://stats.stackexchange.com/questions/648714/is-there-any-technique-that-i-can-use-to-achieve-the-required-computation-withou</link>
      <description><![CDATA[我有一个来自蒙特卡罗模拟的 15 条蛋白质链的数据文件。该文件包含 1000 万个 r_end_to_end 3D 向量，每行 3 x 15 = 45 列。
我的任务是找到模拟稳定的确切行。换句话说，我需要找到自相关时间。换句话说，我需要找到 Tau0 时间。
现在，问题是数据文件很大。
因此，读取全部 1000 万行对于频繁测试、绘图生成、曲线拟合等来说并不实用。
是否有任何技术可以在不读取全部 1000 万行的情况下实现所需的计算？]]></description>
      <guid>https://stats.stackexchange.com/questions/648714/is-there-any-technique-that-i-can-use-to-achieve-the-required-computation-withou</guid>
      <pubDate>Wed, 05 Jun 2024 21:17:52 GMT</pubDate>
    </item>
    <item>
      <title>计算动态时间规整（DTW）之前对时间序列数据进行规范化</title>
      <link>https://stats.stackexchange.com/questions/648713/normalization-of-time-series-data-before-computing-dynamic-time-warping-dtw</link>
      <description><![CDATA[在计算动态时间扭曲 (DTW) 距离之前，我对时间序列数据的适当归一化方法感兴趣。
情况：
我有四个时间序列（用两种不同的绿色色调、紫色和红色着色），它们基于滑动窗口皮尔逊相关性。这意味着这些图的每个数据点都反映了皮尔逊相关系数。
此外，我有一个测量 x 的滑动窗口结果（蓝色时间序列）。仅出于绘图目的，我已将此时间序列归一化为 0 到 1 的区间。这个时间序列的实际值范围更像是 ~1.1 到 ~1.8。
可以看出，紫色和红色时间序列似乎与蓝色时间序列收敛。我的目的是（以定量的方式）显示紫色/红色时间序列与蓝色时间序列之间的距离小于（或接近）两个绿色时间序列与蓝色时间序列之间的距离。
我不想使用相关性或互相关性，因为这种关系是非线性的或随时间不稳定。因此，我考虑在这里应用 DTW。
Eamonn Keogh 指出，在应用 DTW 之前，z 分数标准化在约 99% 的情况下是合适的，甚至是必要的。 （例如，请参见此处：https://www.cs.unm.edu/~mueen/DTW.pdf）
问题：简单地将 z 分数标准化应用于每个时间序列，无论是在动态（滑动窗口）皮尔逊相关时间序列上，还是在测量 x（蓝色）时间序列上，都可以吗？
或者我应该只将蓝色时间序列标准化为 [-1, 1] 的区间，即皮尔逊相关的理论区间？我感到很困惑，不知道这里什么是合理的，而且考虑到蓝色时间序列是非平稳的。
]]></description>
      <guid>https://stats.stackexchange.com/questions/648713/normalization-of-time-series-data-before-computing-dynamic-time-warping-dtw</guid>
      <pubDate>Wed, 05 Jun 2024 21:12:48 GMT</pubDate>
    </item>
    <item>
      <title>离散时间生存中的审查类型</title>
      <link>https://stats.stackexchange.com/questions/648701/type-of-censoring-in-discrete-time-survival</link>
      <description><![CDATA[*我有一个前瞻性纵向研究。在这项研究中，患者每三个月来医院检查一次。T0（手术前一周）、T3（手术后三个月）、T6（手术后六个月）、T9、T12、、、、、、T24（手术后 24 个月）。所以有 9 个时间点。
我们对包含二元时间相关协变量和连续自变量的预测性 Cox 回归感兴趣。在事件发生前退出、完成无事件随访（T24）或在研究结束时显示特定水平的时间相关协变量的病例为审查员。
似乎在计数过程方法中，只有右审查员。而要使用离散时间生存，必须满足以下条件（如果我说得不对，请纠正我）

所有患者的时间间隔相同
时间点数量有限
区间审查

我不确定我的研究中的审查类型。如果研究中同时存在右审查和区间审查，我们可以使用离散时间生存方法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648701/type-of-censoring-in-discrete-time-survival</guid>
      <pubDate>Wed, 05 Jun 2024 19:03:51 GMT</pubDate>
    </item>
    <item>
      <title>如何通过统计方法检验特定地点的物种群落是否越来越像另一类型的地点</title>
      <link>https://stats.stackexchange.com/questions/648689/how-to-statistically-test-if-a-community-of-species-at-a-particular-site-is-beco</link>
      <description><![CDATA[我有以下数据框（我在这里使用 R）。
set.seed(2)
Site &lt;- rep(LETTERS[1:13], length.out = 39)
Year &lt;- rep(2019:2022, each = 13, length.out = 39)
Site_Category &lt;- rep(c(paste(&quot;Type&quot;, rep(1:4, each = 3), sep = &quot;_&quot;), 
&quot;Test_Site&quot;), length.out = 39)
Species_1 &lt;- sample(1:100, 39, replace = TRUE)
Species_2 &lt;- sample(1:25, 39, replace = TRUE)
Species_3 &lt;- sample(1:50, 39, replace = TRUE)
Species_4 &lt;- sample(1:500, 39, replace = TRUE)
Species_5 &lt;- sample(1:100, 39, replace = TRUE)
My_Data &lt;- data.frame(Site = Site, Year = Year, Site_Category = 
Site_Category, Species_1 = Species_1, Species_2 = Species_2, 
Species_3 = Species_3, Species_4 = Species_4, 
Species_5 = Species_5)

本质上，我有四个独立的社区数据矩阵（研究的每个年份一个）。您还会注意到，我有一个附加列 Site_Category，它将某些站点分组在一起，并且还指定其中一个站点是独一无二的，因为它是我们感兴趣的站点。
我们的研究问题是判断这个感兴趣的站点是否随着时间的推移变得更像其他站点类型之一。
我熟悉多元分析，例如 Mantel 检验、多响应置换程序和置换方差分析，但我不仅仅是想看看是否存在差异 - 我想结合某种趋势分析来判断随着时间的推移，是否有朝着其他站点类型之一的特定方向的变化。
我可以在这里使用某种统计分析吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648689/how-to-statistically-test-if-a-community-of-species-at-a-particular-site-is-beco</guid>
      <pubDate>Wed, 05 Jun 2024 14:28:26 GMT</pubDate>
    </item>
    <item>
      <title>简单线性回归的$t$值在线更新</title>
      <link>https://stats.stackexchange.com/questions/648669/online-updating-of-t-value-for-simple-linear-regression</link>
      <description><![CDATA[假设我使用一个简单的普通最小二乘回归模型$y = \beta_1 x + \beta_0$将一个因变量$y$回归到一个独立变量$x$。假设我从 $n$ 个数据点 $(x_1,y_1), \ldots (x_n, y_n)$ 开始，然后拟合回归以获得
$$
\hat{\beta_1} = \frac{s_{xy}}{s^2_x}
$$
and
$$
\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x}
$$
现在假设我有一系列新传入数据 $(x_{n+1},y_{n+1}),\ldots$。
我 知道如何计算新数据集的项$\bar{x}, \bar{y},s^2_x,s^2_y,s_{xy}$，并使用新到达的点进行扩充，从而实现高效计算 - 也就是说，我可以利用我从前$n$个数据点对它们的了解，在恒定的时间内更新所有这些数量。因此，我也知道如何有效地更新数量 $\hat{\beta_1},\hat{\beta_0},r^2$。
我想知道是否有任何巧妙的技巧可以让我有效地更新我的 $t$ 分数，即
$$
t = \frac{\hat{\beta_1}}{\frac{s}{\sqrt{\sum{(x_i-\bar{x})^2}}}}
$$
其中 $s$ 是我们在模型中假设的误差中底层常数标准差 $\sigma$ 的无偏估计量，由
$$
s = \frac{1}{n-2} \sum{(y_i - \hat{y_i})^2}
$$
在我看来，主要问题是，当我们添加新数据点时，所有 $\hat{y_i}$ 值都会发生变化，因此，在重新计算 $s$ 时，它们都是必需的。有没有办法解决这个问题？我可以在恒定时间内更新 $t$ 得分（或等效地 $s$）吗？
假设答案是否定的，我认为对 $t$ 得分可以改变多少进行一些控制会很有用。我可以推导出这一点，但我也希望得到任何文献指点。]]></description>
      <guid>https://stats.stackexchange.com/questions/648669/online-updating-of-t-value-for-simple-linear-regression</guid>
      <pubDate>Wed, 05 Jun 2024 10:13:51 GMT</pubDate>
    </item>
    <item>
      <title>如何检验 B 样条曲线中的变点是否具有统计显著性？</title>
      <link>https://stats.stackexchange.com/questions/648686/how-to-test-if-change-point-in-b-spline-is-statistically-significant</link>
      <description><![CDATA[我想测试一下结点两侧斜率之间的差异是否具有统计显著性。忽略此示例中的模型假设违规，如何查看下图中 x 和 y 之间的关系/斜率是否发生了显著变化？我之前从未使用过带有 b-spline 命令的线性回归，p 值是否表示这两条线之间的差异？我使用的是数据“mtcars”作为我自己数据的替代品。
示例：

library(ggplot2)
library(splines)

data(mtcars)

wt=3 处是否有显著变化点？
ggplot(data = mtcars, aes(x = wt, y = mpg)) +
geom_point() + 
geom_vline(xintercept=3, linetype=&quot;dashed&quot;, color = &quot;red&quot;) + 
geom_smooth(method=&quot;lm&quot;,
formula= y ~ splines::bs(x, knots = c(3), degree = 1), se=F)


# 构建模型：
mod &lt;- lm(mpg ~ bs(wt, knots = c(3), degree = 1), data = mtcars)

summary(mod)

调用：
lm(formula = mpg ~ bs(wt, knots = c(3), degree = 1), data = mtcars)

残差：
最小值 1Q 中位数 3Q 最大值 
-3.2027 -1.9072 -0.7627 0.9611 6.1070 

系数：
估计标准差误差 t 值 Pr(&gt;|t|) 
(截距) 32.246 1.383 23.313 &lt; 2e-16 ***
bs(wt, knots = c(3), degree = 1)1 -12.884 1.764 -7.306 4.78e-08 ***
bs(wt, knots = c(3), degree = 1)2 -21.133 1.932 -10.937 8.32e-12 ***
---
显著性代码：0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

残差标准误差：29 个自由度上的 2.689

多重 R 平方：0.8137，调整后的 R 平方：0.8009

F 统计量：2 和 29 DF 上的 63.34，p 值：2.614e-11

可能的相关问题：
在结点之间分离 B 样条线并比较每个部分的拟合优度]]></description>
      <guid>https://stats.stackexchange.com/questions/648686/how-to-test-if-change-point-in-b-spline-is-statistically-significant</guid>
      <pubDate>Wed, 05 Jun 2024 02:18:10 GMT</pubDate>
    </item>
    <item>
      <title>为什么聚合可以减少噪音</title>
      <link>https://stats.stackexchange.com/questions/648622/why-aggregation-reduces-the-noise</link>
      <description><![CDATA[我正在寻找一个证据来证明为什么将数据聚合（汇总）到更高级别（例如，将变量的每日值聚合到每周或每月）可以减少噪音。有人知道如何证明这一点吗？基本上，为什么更细粒度的数据比聚合的、更粗的数据更不稳定和不稳定？]]></description>
      <guid>https://stats.stackexchange.com/questions/648622/why-aggregation-reduces-the-noise</guid>
      <pubDate>Tue, 04 Jun 2024 16:10:47 GMT</pubDate>
    </item>
    <item>
      <title>如何根据一组百分位数重建正态分布？</title>
      <link>https://stats.stackexchange.com/questions/648589/how-can-i-reconstruct-a-normal-distribution-from-a-set-of-percentiles</link>
      <description><![CDATA[我有一个正态分布变量的第 3、10、50、90 和 97 个百分位数值，我希望生成一个数据集，使我能够查询其他百分位数值（例如，第 67 个百分位数值）。
我（天真地，我敢肯定）尝试了以下操作，但失败了：
underlying_data = np.random.normal(loc=0.0, scale=1.0, size=[1000])

percentiles = np.percentile(underlying_data, [3, 10, 50, 90, 97])

#

generated_data = []

for i in range(3):
generated_data.append(underlying_data[0])

for i in range(7):
generated_data.append(underlying_data[1])

for i in range(80):
generated_data.append(underlying_data[2])

for i in range(7):
generated_data.append(underlying_data[3])

for i in range(3):
generated_data.append(underlying_data[4])

print(&quot;from underground distribution: &quot;, np.percentile(np.array(underlying_data), [67]))
print(&quot;from generated distribution: &quot;, np.percentile(np.array(generated_data), [67]))

输出：
from underground distribution: [0.44470627]
from generated distribution: [-0.73888881]
]]></description>
      <guid>https://stats.stackexchange.com/questions/648589/how-can-i-reconstruct-a-normal-distribution-from-a-set-of-percentiles</guid>
      <pubDate>Tue, 04 Jun 2024 06:25:11 GMT</pubDate>
    </item>
    </channel>
</rss>