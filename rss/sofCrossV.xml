<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 09 Nov 2024 15:17:36 GMT</lastBuildDate>
    <item>
      <title>Kruskal-Wallis H 检验</title>
      <link>https://stats.stackexchange.com/questions/656990/kruskal-wallis-h-test</link>
      <description><![CDATA[在总共 4,200 名患者中，Q1-Q4 组基于连续变量 tyg 进行划分。在基线分析期间，总人口的年龄是一个偏斜的连续变量，中位数为 58.0（第 25 百分位数：52.0，第 75 百分位数：64.0）。每组的中位年龄（第 25 和第 75 百分位数）如下：
第 1 组：60.0（55.0，67.0）第 2 组：58.0（52.0，65.0）第 3 组：57.0（51.0，63.0）第 4 组：57.0（50.0，62.8）当执行 Kruskal-Wallis H 检验来比较各组时，获得的 p 值小于 0.001。但是审稿人质疑，为什么四组之间的年龄差异不大，但p值却小于0.001。
我重新检查了数据和统计方法（Kruskal-Wallis H检验），发现结果仍然是p &lt; 0.001。我应该如何解释审稿人的问题并做出回应？还是我的统计方法不正确？
在此处输入图片描述]]></description>
      <guid>https://stats.stackexchange.com/questions/656990/kruskal-wallis-h-test</guid>
      <pubDate>Sat, 09 Nov 2024 13:41:32 GMT</pubDate>
    </item>
    <item>
      <title>在逻辑回归中，什么使得曲线拟合得好</title>
      <link>https://stats.stackexchange.com/questions/656987/what-makes-a-curve-a-good-fit-in-the-context-of-logistic-regression</link>
      <description><![CDATA[因为我想更好地理解为什么在逻辑回归的背景下分离是一个问题，所以我在 R 中创建了两个模型，一个模型中 y 在 $x=5$ 处完全分离，另一个模型则不是。为了可重复性，这里是生成的数据集和用于生成具有一定范围的 beta（具有相同截距）的各种曲线估计的代码，并添加了 ML 估计器，并将所有内容与数据点一起绘制：
library(dplyr)
library(ggplot2)
library(boot)
library(tidyr)

df_sep &lt;- data.frame(x=seq(0,9.9,by=0.1),y=c(rep(0,50),rep(1,50)))
df_notsep &lt;- data.frame(x=seq(0,9.9,by=0.1),y=c(rbinom(50,1,0.2),rbinom(50,1,0.8)))

mod_sep &lt;- glm(y~x,df_sep,family=&quot;binomial&quot;)
# (截距) x 
# -1650.0708 333.3476 
df_sep &lt;- df_sep %&gt;% mutate(y_beta100= inv.logit(coef(mod_sep)[1]+100*x), 
y_beta200=inv.logit(coef(mod_sep)[1]+200*x), 
y_beta300=inv.logit(coef(mod_sep)[1]+300*x),
y_beta400=inv.logit(coef(mod_sep)[1]+400*x))
df_sep$y_pred &lt;-预测（mod_sep，type=&quot;response&quot;）

df_sep_long &lt;- df_sep %&gt;% pivot_longer（cols = starts_with(&quot;y&quot;))

mod_notsep &lt;- glm(y~x,df_notsep,family=&quot;binomial&quot;)
# (截距) x 
# -2.5422783 0.5515725 

df_notsep &lt;- df_notsep %&gt;% mutate(y_beta0_2=inv.logit(coef(mod_notsep)[1]+0.2*x), 
y_beta0_3=inv.logit(coef(mod_notsep)[1]+0.3*x),
y_beta0_5=inv.logit(coef(mod_notsep)[1]+0.5*x),
y_beta0_7=inv.logit(coef(mod_notsep)[1]+0.7*x))

df_notsep$y_pred &lt;- predict(mod_notsep,type=&quot;response&quot;)

df_notsep_long &lt;- df_notsep %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;))

ggplot(df_sep_long, aes(x = x, y = value, colour = name, shape=name, group = name)) + 
geom_point(data=df_sep_long %&gt;% filter(name==&quot;y&quot;),alpha = 0.5) + 
geom_path(data=df_sep_long %&gt;% filter(name!=&quot;y&quot;), aes(x = x, y = value, colour = name, group = name)) +
theme_bw()

ggplot(df_notsep_long, aes(x = x, y = value, colour = name, shape=name, group = name)) + 
geom_point(data=df_notsep_long %&gt;% filter(name==&quot;y&quot;),alpha = 0.5) + 
geom_path(data=df_notsep_long %&gt;% filter(name!=&quot;y&quot;), aes(x = x, y = value, colour = name, group = name)) +
theme_bw()

我没有更好地理解分离数据的收敛问题，但另一方面，能够想象一下为什么预测的 beta 在该上下文中提供最大似然，因为它几乎是一个分段函数，因此很明显它是允许 $\max{x}$ 的值与 $y=0$ 的值和 $\min{x}$ 的值与 $y=1$ 的值之间的最短线的值。
但是，对于非分离数据，我不清楚为什么 ML 是最佳拟合。如果我们有一个 OLS，我们会尝试最小化点和线之间的距离，这很容易理解，但是在估计最大化似然函数的 $\beta$ 时我们到底在做什么？

]]></description>
      <guid>https://stats.stackexchange.com/questions/656987/what-makes-a-curve-a-good-fit-in-the-context-of-logistic-regression</guid>
      <pubDate>Sat, 09 Nov 2024 10:22:18 GMT</pubDate>
    </item>
    <item>
      <title>在岭回归中修复 $\lambda$ 的动机</title>
      <link>https://stats.stackexchange.com/questions/656986/motivation-for-fixing-lambda-in-ridge-regression</link>
      <description><![CDATA[在岭回归中，您想要优化：
$$
f(\beta_0,\beta_1,\beta_n)=\sum_n(y_n-\beta_0+\beta_1x^1_n,+\beta_2x^2_n)^2
$$
其中
$x^i=(x_1^i,x_2^i,...,x_N^i)$
取决于
$||(\beta_0,\beta_1,\beta_2)||_2\leq c$。
我们可以定义$\mathcal{L}(\beta_0,\beta_1,\beta_2,\lambda)=f(\beta_0,\beta_1,\beta_2)-\lambda(c-||(\beta_0,\beta_1\beta_2)||_2)$。从此以后，我们可以寻找 $\mathcal{L}$ 的临界点。
但是，据我所知，不是对 $\lambda$ 进行优化，而是只对 $(\beta_0,\beta_1,\beta_2)$ 进行优化，这给出了岭回归的形式。我的问题是，为什么我们要固定$\lambda$，而不是试图找到$(\beta_0,\beta_1,\beta_2,\lambda)$，使得$\mathcal{L}$为零，因此$f$对于那些$\beta_0,\beta_1,\beta_2$是最优的？]]></description>
      <guid>https://stats.stackexchange.com/questions/656986/motivation-for-fixing-lambda-in-ridge-regression</guid>
      <pubDate>Sat, 09 Nov 2024 10:21:23 GMT</pubDate>
    </item>
    <item>
      <title>关于 Kruskal-Wallis H 检验结果解释的澄清</title>
      <link>https://stats.stackexchange.com/questions/656984/clarification-on-the-interpretation-of-kruskal-wallis-h-test-results</link>
      <description><![CDATA[在总共 4,200 名患者中，Q1-Q4 组根据连续变量 tyg 进行划分。在基线分析期间，总人口的年龄是一个偏态连续变量，中位数为 58.0（第 25 百分位数：52.0，第 75 百分位数：64.0）。每组的中位年龄（第 25 和第 75 百分位数）如下：
第 1 组：60.0（55.0，67.0）
第 2 组：58.0（52.0，65.0）
第 3 组：57.0（51.0，63.0）
第 4 组：57.0（50.0，62.8）
当执行 Kruskal-Wallis H 检验来比较各组时，获得的 p 值小于 0.001。但是审稿人质疑为什么四组之间的年龄差异不大，但p值却小于0.001。
我重新检查了数据和统计方法（Kruskal-Wallis H检验），发现结果仍然是p&lt;0.001。我应该如何解释审稿人的问题并做出回应？还是我的统计方法不正确？]]></description>
      <guid>https://stats.stackexchange.com/questions/656984/clarification-on-the-interpretation-of-kruskal-wallis-h-test-results</guid>
      <pubDate>Sat, 09 Nov 2024 08:50:12 GMT</pubDate>
    </item>
    <item>
      <title>当转换和聚类算法没有帮助时，如何对高度倾斜的 DALY 数据进行聚类？</title>
      <link>https://stats.stackexchange.com/questions/656983/how-to-cluster-highly-skewed-daly-data-when-transformations-and-clustering-algor</link>
      <description><![CDATA[我正在处理一个具有高度偏斜度的残疾调整生命年 (DALY) 值数据集，如下面的直方图所示，其中大多数值聚集在零附近，长尾向更高值延伸。我的目标是对这些数据进行聚类以获得见解，但尽管尝试了各种转换和聚类算法，结果仍然不令人满意。
我尝试了以下操作：
转换：对数、Box-Cox 和幂转换，但没有一个能有效地使分布标准化。
聚类算法：k-means、DBSCAN、HDBSCAN、K 原型和高斯混合模型 (GMM) 无法提供有意义的聚类。特别是 k-means 由于偏斜而苦苦挣扎，而 DBSCAN 经常将许多点标记为噪声。
鉴于这些挑战，是否有人对可以以允许有效聚类的方式处理这种偏斜数据的替代方法或方法提出建议？或者是否有特定的参数调整可以帮助这些方法更好地处理数据？
附加上下文：
数据集包含不同国家/地区的 DALY 值的高度可变性，即使在异常检测和过滤后也存在异常值。
我根据 DALY 和其他特征（6 个分类特征）进行聚类，例如 SDI（社会人口指数），这也显示出一定程度的偏差。
任何指导都将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/656983/how-to-cluster-highly-skewed-daly-data-when-transformations-and-clustering-algor</guid>
      <pubDate>Sat, 09 Nov 2024 06:14:59 GMT</pubDate>
    </item>
    <item>
      <title>关于 Clayton Copula 的问题</title>
      <link>https://stats.stackexchange.com/questions/656982/question-on-clayton-copula</link>
      <description><![CDATA[Clayton Copula 定义为 $u=F_X(x)$ 和 $v=F_Y(y)$ 分布函数，例如两个连续 rv，
$$C_{\rm Clayton} = \Big[\max\{u^{-\theta} + v^{-\theta}-1;\,0\}\Big]^{-1/\theta},\quad \theta \in (-1,\infty) \backslash \{0\}.$$
$\max$ 中的 $0$ 值只能在 $\theta &lt;0$。
假设情况如此。那么对于 $[0,1]^2$ 的子集，Copula 也将取零值。很好，但是 Copula 的参数（两个分布函数）确实会在此子集中单独取值。
这是否意味着只有当我们想要反映两个 rv 在联合支持中的此值子集中联合实现的概率为零时，才使用具有 $\theta &lt;0$ 的 Clayton Copula？]]></description>
      <guid>https://stats.stackexchange.com/questions/656982/question-on-clayton-copula</guid>
      <pubDate>Sat, 09 Nov 2024 03:14:04 GMT</pubDate>
    </item>
    <item>
      <title>如何确定固定数量的目标函数评估的最佳开发-探索权衡</title>
      <link>https://stats.stackexchange.com/questions/656939/how-to-determine-the-optimal-exploitation-exploration-trade-off-for-a-fixed-numb</link>
      <description><![CDATA[在贝叶斯优化中，我们通过查找$x = \textrm{argmax}_x \alpha(x)$来猜测下一个采样点，其中$\alpha(x)$是采集函数。为简单起见，我们考虑上限置信边界 (UCB) 采集函数：$\alpha(x)=\mu(x) + \sqrt\beta \sigma(x)$，其中$\mu(x)$是通常的平均值，$\sigma(x)$表示方差的通常平方根。这里，$\beta$ 当然是无量纲参数 ($\beta\geq0$)，用于调整探索 (高 $\beta$) 和利用 (低 $\beta$) 之间的权衡。
假设我有 $N$ 个目标函数评估的计算或实验预算。是否有将 $\beta$ 设置为 $N$ 函数的算法或启发式方法？]]></description>
      <guid>https://stats.stackexchange.com/questions/656939/how-to-determine-the-optimal-exploitation-exploration-trade-off-for-a-fixed-numb</guid>
      <pubDate>Fri, 08 Nov 2024 08:21:54 GMT</pubDate>
    </item>
    <item>
      <title>R 中的卡方检验困难</title>
      <link>https://stats.stackexchange.com/questions/656914/difficulties-with-chi-squared-test-in-r</link>
      <description><![CDATA[我尝试执行 3 个单独的卡方检验来确定 BMI 和 2 型糖尿病之间是否存在显著性。一个针对男性和女性，一个仅针对男性，一个仅针对女性。但是，当我尝试运行代码时，我得到了非常大的 X 平方值和 n/a 的 df 值。
EMLabData 表
代码：
combined &lt;- table(EMLabData$BMI, EMLabData$T2D)
men &lt;- table(EMLabData$BMI[EMLabData$Sex==&quot;Male&quot;], 
EMLabData$T2D[EMLabData$Sex==&quot;Male&quot;])
women &lt;- table(EMLabData$BMI[EMLabData$Sex==&quot;Female&quot;], 
EMLabData$T2D[EMLabData$Sex==&quot;Female&quot;])

chisq.test(combined, mock.p.value = TRUE)
chisq.test(men, mock.p.value = TRUE)
chisq.test(women, mock.p.value = TRUE)

输出：
数据：combined
X-squared = 1423.4, df = NA, p-value = 0.0004998

数据：men
X-squared = 727.94, df = NA, p-value = 0.04798

数据：女性
X 平方 = 1.2297，df = NA，p 值 = 1

我不确定出了什么问题以及如何解决。如能得到任何帮助我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/656914/difficulties-with-chi-squared-test-in-r</guid>
      <pubDate>Thu, 07 Nov 2024 16:09:45 GMT</pubDate>
    </item>
    <item>
      <title>样本量越大，错误拒绝原假设的风险是否会越大？</title>
      <link>https://stats.stackexchange.com/questions/656828/does-the-risk-of-incorrectly-rejecting-the-null-hypothesis-increase-with-a-large</link>
      <description><![CDATA[我经常看到类似这样的话：“如果样本量足够大，较小的效应量可以产生显著的结果”。我不太明白这一点。
对我来说，这听起来是这样的：增加样本量，你最终肯定会拒绝零假设（也就是说，如果你将样本量增加到一定大小，你 100% 肯定会拒绝 0 假设）。
重新阅读此主题。据我所知，鉴于我在统计学方面的经验，零假设被正确地拒绝了。
决定在 ttest_1samp 测试的代码中检查这一点，使 popmean=15.03 与样本 (mean=15) 的差异非常小。滑块选择 N - 样本，并实时重新绘制分布，垂直条是上限值（蓝色）和 p_value（橙色）。
更新 08.11.2024
由于数组的平均值与声明的平均值不同，为了准确起见，我通过向其添加一些值使值 mu_0 = mu_1 + value。mu_0 在图表上以绿色绘制，置信区间为洋红色。该图还显示 p 值本身、平均值和间隔（CI）。更改 N，图形将动态重新绘制。现在我还检查了均值差异很大，需要样本量来拒绝更小的样本。可以通过 value 设置差异。
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t, norm, ttest_1samp
from matplotlib.widgets import Slider

fig, ax = plt.subplots()
fig.subplots_adjust(bottom=0.25)

ax_n = fig.add_axes([0.25, 0.15, 0.65, 0.03])
s_n = Slider(ax_n, &quot;N&quot;, valmin=7, valmax=30000,valinit=21, valstep=1)

value = 0.03

def update(val):
N = s_n.val
rvs = np.sort(norm.rvs(loc=15,比例=1，大小=N，random_state=1））
    mu, std = np.mean(rvs), np.std(rvs)
    mu_0 = mu + 值
    z_values = (rvs - mu) / std
    df = N - 1
    pdf = t.pdf(rvs, loc=mu, 比例=std, df=df)
    tst = ttest_1samp(rvs, popmean=mu_0)
    l、r = t.ppf(1 - 0.975, df=df), t.ppf(0.975, df=df)
    ci = tst.confidence_interval(confidence_level=0.95)
    ci_0, ci_1 = np.round(ci, 4)
    ci = (ci - mu) / std
ax.clear()
ax.plot(z_values, pdf, lw=2, color=&quot;red&quot;)
ax.axvline(x=(mu_0 - mu) / std, color=&quot;green&quot;, label=&quot;mean 0&quot;)# 0 假设的平均值
ax.axvline(x=ci[0], linestyle=&quot;--&quot;, color=&quot;magenta&quot;, label=&quot;CI&quot;)
ax.axvline(x=ci[1], linestyle=&quot;--&quot;, color=&quot;magenta&quot;)
ax.axvline(x=l, linestyle=&quot;--&quot;, color=&quot;blue&quot;, label=&quot;alfa&quot;)# alfa
ax.axvline(x=r, linestyle=&quot;--&quot;, color=&quot;blue&quot;)# alfa
ax.axvline(x=t.ppf(tst[1]/2, df=df), linestyle=&quot;--&quot;, color=&quot;orange&quot;, label=&quot;p-value&quot;)#p-value
ax.axvline(x=-t.ppf(tst[1]/2, df=df), linestyle=&quot;--&quot;, color=&quot;orange&quot;)#p-value
ax.text(0, (np.max(pdf) + np.min(pdf))/2, &quot;p_value =&quot; + str(round(tst[1], 4))
+ &quot;\n&quot; + &quot;mu_0 =&quot; + str(round(mu_0, 5)) + &quot;\n&quot; + &quot;mu_1 =&quot; + str(round(mu, 5)) +
&quot;\n&quot; + &quot;ci_0 =&quot; + str(ci_0) + &quot;\n&quot; + &quot;ci_1 =&quot; + str(ci_1), fontsize=12)
ax.legend()

s_n.on_changed(update)
update(0)

plt.show()

]]></description>
      <guid>https://stats.stackexchange.com/questions/656828/does-the-risk-of-incorrectly-rejecting-the-null-hypothesis-increase-with-a-large</guid>
      <pubDate>Wed, 06 Nov 2024 11:43:12 GMT</pubDate>
    </item>
    <item>
      <title>因子分析，特征值与平方因子载荷与碎石图特征值 [重复]</title>
      <link>https://stats.stackexchange.com/questions/656682/factor-analysis-eigenvalues-vs-squared-factor-loadings-vs-scree-plot-eigenval</link>
      <description><![CDATA[我对一个包含 8 个变量的数据集进行了探索性因子分析。我还进行了 KMO 和 Bartlett 检验，以确保数据符合本次检查的条件。
我准备了碎石图，并对不同数量的因子（从 1 到 9）进行了因子分析，使用了 2 种不同的旋转：方差最大和斜率最小。到目前为止一切顺利。
肘部法建议使用 2 个因子，碎石图上的特征值仅对 1 个因子高于 1。从整个数据集捕获的因子的方差在 5 个因子的情况下达到最大值，并且卡方拟合优度检验在因子大于 3 时失去其显著性。
我复制了 fa() 调用的输出，其中包含 5 个因子和方差最大旋转（psych 包）：
使用 method = minres 的因子分析
调用：fa(r = df_sq1sq9, nfactors = 5, rotate = &quot;varimax&quot;)
基于相关矩阵的标准化载荷（模式矩阵）
MR1 MR3 MR2 MR5 MR4 h2 u2 com
SQ1 0.13 0.37 0.01 -0.01 0.42 0.33 0.665 2.2
SQ2 0.16 0.07 -0.06 0.09 0.66 0.48 0.517 1.2
SQ3 0.33 0.88 0.01 0.04 0.21 0.94 0.063 1.4
SQ4 0.43 0.10 0.16 0.82 0.11 0.89 0.106 1.7
SQ5 0.85 0.16 -0.06 0.17 0.28 0.86 0.141 1.4
SQ6 0.85 0.34 0.02 0.25 0.16 0.92 0.079 1.6
SQ7 0.34 0.53 0.25 0.30 0.04 0.55 0.450 2.9
SQ8 -0.04 0.08 0.98 0.12 -0.07 1.00 0.005 1.1
SQ9 0.75 0.30 0.00 0.21 0.13 0.70 0.296 1.5

MR1 MR3 MR2 MR5 MR4
SS 载荷 2.44 1.45 1.06 0.91 0.81
比例方差 0.27 0.16 0.12 0.10 0.09
累积方差 0.27 0.43 0.55 0.65 0.74
解释比例 0.37 0.22 0.16 0.14 0.12
累积比例 0.37 0.58 0.74 0.88 1.00

平均项目复杂度 = 1.7
检验假设 5 个因素就足够。

df 零模型 = 36，目标函数 = 4.83，卡方 = 290.78

模型的 df 为 1，目标函数为 0

残差的均方根 (RMSR) 为 0

df 校正的残差均方根为 0.01

谐波 n.obs 为 65，经验卡方为 0.02，概率 &lt; 0.89

总 n.obs 为 65，似然卡方 = 0.15，概率 &lt; 0.69 


fa() 调用的结果还包含一个列表 fa_results[[&quot;e.values&quot;]]
其值如下：
fa_results[[&quot;e.values&quot;]]
4.2142109 1.3128191 1.0559469 0.8085356 0.5378116 0.4403557 0.2829727 0.2444338 0.1029137

问题 1：为什么平方载荷之和与特征值不同？理解这一点很重要，因为如果我计算因子可以捕获的方差，那么在一种情况下，我会得到 74%（SS 载荷），而根据列表中的特征值，我会得到 88%。
答案： (1) PCA 将整个方差归因于已识别的主成分，而因子分析假设方差被划分为共享方差（由因子解释的方差）和唯一方差（不能由因子解释）；(2) 由于旋转，它们也不同。请参阅易于阅读的清晰解释：https://stats.oarc.ucla.edu/spss/seminars/efa-spss/ 或 ttnphns 发布的链接上的答案：因子分析中初始特征值和平方载荷和之间的关系是什么？
问题 2：碎石图（平行分析）上的特征值与fa_results[[&quot;e.values&quot;]]  并且它们也与 ss 因子载荷不同。为什么？发布的链接上没有解释这一点。
感谢大家的回复！]]></description>
      <guid>https://stats.stackexchange.com/questions/656682/factor-analysis-eigenvalues-vs-squared-factor-loadings-vs-scree-plot-eigenval</guid>
      <pubDate>Sun, 03 Nov 2024 19:10:19 GMT</pubDate>
    </item>
    <item>
      <title>方差-偏差权衡公式，用于 X 固定且 X 随机的简单线性回归</title>
      <link>https://stats.stackexchange.com/questions/656503/variance-bias-tradeoff-formula-for-simple-linear-regression-with-both-x-fixed-an</link>
      <description><![CDATA[我试图使用简单的线性回归来理解方差-偏差权衡公式。但有些公式我无法推导。我将通过首先对固定的$X$进行操作，然后对随机的$X$进行操作来解释我的意思。以下是偏差-方差权衡公式：

1.简单线性回归，固定$X$。
我假设模型为$Y=\beta_0+\beta_1 X+\epsilon$，其中$\epsilon$为$N(0,\sigma^2)$。我假设我有 $k$ 个 x 值 $x_1,x_2,\ldots,x_k$，它们是固定的，并且我观察到 $y_1,y_2,\ldots,y_k$。
定义 $S_{xy}=\sum_{j=1}^k (x_j-\overline{x})(y_j-\overline{y}), S_{xx}=\sum_{j=1}^k (x_j-\overline{x})^2$。
从初等统计学中，我们可以得出最小二乘估计量为：
$\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}, \hat{\beta_0}=\overline{y}-\hat{\beta_1}\overline{x}.$
假设您想在点 $x^*$ 处观察一个新的观测值。然后我们有
$\hat{f}(x^*)=\hat{\beta_0}+\hat{\beta_1}x^*$。在初等统计学教材中，有如下公式：$E[\hat{f}(x^*)]=E[\hat{\beta_0}+\hat{\beta_1}x^*]=\beta_0+\beta_1x^*=f(x^*)$。
还有如下公式：$V(\hat{f}(x^*))=\sigma^2\left(\frac{1}{n}+\frac{(\overline{x}-x^*)^2}{S_{xx}}\right)$。
问题：您是否同意偏差-方差权衡公式变为：
$E((Y^*-\hat{f}(x^*))^2)=\sigma^2\left(\frac{1}{n}+\frac{(\overline{x}-x^*)^2}{S_{xx}}\right)+0+\sigma^2$?
在这种情况下，我是否对线性回归中所做的工作做出了正确的假设？
2.简单线性回归，随机$X$。
仍然假设$Y=\beta_0+\beta_1 X+\epsilon$，还假设$\epsilon$和$X$是独立的。还假设$X\text{~} N(\mu_X,\sigma_X^2)$。现在假设您观察到$(x_1,y_1),\ldots,(x_k,y_k)$。然后您按上述方法创建 $\hat{\beta_0}$ 和 $\hat{\beta_1}$，从而创建 $\hat{f}$。假设您还固定了点 $x^*$。现在 $E((Y^*-\hat{f}(x^*))^2)$ 是多少？注意，它不可能是上面那样，因为点 $x_1,\ldots,x_k$ 不是固定的，但公式依赖于它们（例如 $S_{xx}$），但 $E((Y^*-\hat{f}(x^*))^2)$ 应该仍然存在？]]></description>
      <guid>https://stats.stackexchange.com/questions/656503/variance-bias-tradeoff-formula-for-simple-linear-regression-with-both-x-fixed-an</guid>
      <pubDate>Wed, 30 Oct 2024 11:26:46 GMT</pubDate>
    </item>
    <item>
      <title>SLOPE 与 Benjamini-Hochberg 程序之间的联系</title>
      <link>https://stats.stackexchange.com/questions/652379/the-link-between-slope-and-the-benjamini-hochberg-procedure</link>
      <description><![CDATA[假设我们正在执行 $m$ 测试，该测试生成 $m$ 个 p 值 $p_1 \leq \ldots \leq p_m$（按索引排序）。 BH 程序如下：

对于给定的 $\alpha$（所需的 FDR 水平），找到最大的 $k$，使得 $p_k \leq \frac{k}{m}\alpha$。
对所有 $i\leq k$，拒绝零假设 $H_i$。

现在，我已经阅读了一些关于基于此的 Sorted L-One Penalized Estimation (SLOPE) 模型，但我很难看出其中的确切联系。在 SLOPE 论文 中，BH 程序描述如下（并且该公式用于推导 SLOPE 惩罚序列）：
对于回归模型 $y = X\beta + z,$，其中 $z \sim N(0,\sigma^2)$ 和正交矩阵 $X^TX = I$，将 $\tilde{y} = X^Ty$ 的条目排序为 $|y|_{(1)} \geq \ldots \geq |y|_{(m)}$ 和拒绝所有假设 $H_i$ 其中 $i\leq k_\text{BH}$ 其中
$$k_\text{BH} = \max\left\{k:
\frac{|\tilde{y}|_{(k)}}{\sigma} \geq \Phi^{(-1)}\left(1-\frac{k\alpha}{2m}\right)\right\}.$$
现在，我想知道这两者之间有什么联系以及分位数是如何使用的。我猜这与我们在 SLOPE 中使用 $\tilde{y}$ 而不是 p 值有关，我们需要以某种方式将其转换为与 p 值比较相同的比例。]]></description>
      <guid>https://stats.stackexchange.com/questions/652379/the-link-between-slope-and-the-benjamini-hochberg-procedure</guid>
      <pubDate>Tue, 06 Aug 2024 12:44:45 GMT</pubDate>
    </item>
    <item>
      <title>如何正确解释卡方大于 2x2 的交叉表中的调整残差？</title>
      <link>https://stats.stackexchange.com/questions/585735/how-to-properly-interpret-adjusted-residuals-in-crosstabs-with-chi-squares-large</link>
      <description><![CDATA[如果能给我一个通俗易懂的答案，我将不胜感激。据我所知，如果调整后的残差为 +/- 1.96 或更大（如果为负则更小），那么一定有什么问题。调整后的残差到底告诉了我什么，如果可能的话，请举例说明。我正在使用 SPSS。
根据 SPSS 中的解释（来自 https://www.ibm.com/support/pages/interpreting-adjusted-residuals-crosstabs-cell-statistics），他们陈述如下：“正如 Agresti 所指出的，标准化残差（在 Agresti 中称为 Pearson 残差）也作为 Crosstabs 中的单元格统计数据提供，在独立性零假设下也呈渐近正态分布，均值为 0。”
但是，如果调整后的残差大于 1.96 或小于 -1.96，这究竟意味着什么呢？]]></description>
      <guid>https://stats.stackexchange.com/questions/585735/how-to-properly-interpret-adjusted-residuals-in-crosstabs-with-chi-squares-large</guid>
      <pubDate>Tue, 16 Aug 2022 20:31:15 GMT</pubDate>
    </item>
    <item>
      <title>回归中缺少一些控制变量的数据点 - 仍然可行吗？</title>
      <link>https://stats.stackexchange.com/questions/580913/data-points-for-some-control-variables-missing-in-regression-still-feasible</link>
      <description><![CDATA[我目前正在进行一项事件研究，以检查异常回报。
在第一步中，我计算了特定类型的公司事件的异常回报，该事件包括大约 13,000 个事件和 4,000 多家公司。
在第二步中，我打算使用多个（控制？）变量运行回归分析，以查看某些影响是否源于事件的某些方面。
到目前为止一切顺利，现在我遇到的问题是我想控制 5-6 个因素，例如市值和企业总价值。不幸的是，我没有 13,000 个事件中的每一个数据点。例如，对于事件 1，我缺少市值，对于事件 2，缺少企业总价值，对于事件 3，缺少 M/B 比率等等。
问题：即使我有大量 NA，我是否仍能运行有意义的回归分析，还是必须删除每个数据不完整的事件？鉴于某些变量的数据可用性较差（我通常仍希望包括这些变量），这将导致删除大量事件（&gt;7,000）。]]></description>
      <guid>https://stats.stackexchange.com/questions/580913/data-points-for-some-control-variables-missing-in-regression-still-feasible</guid>
      <pubDate>Mon, 04 Jul 2022 14:27:06 GMT</pubDate>
    </item>
    <item>
      <title>为什么与每个匹配协变量的单变量不平衡相比，粗化精确匹配 (CEM) 的多元 L1 距离很高？</title>
      <link>https://stats.stackexchange.com/questions/552793/why-multivariate-l1-distance-from-coarsened-exact-matching-cem-is-high-compare</link>
      <description><![CDATA[我正在看一篇 Blackwell 等人 (2010) 撰写的关于 Stata 中 CEM 的论文链接此处。
在一个使用示例数据集的示例中，作者使用匹配的协变量（例如年龄、教育、黑人、无学位和 re74）运行 CEM。
对于每个协变量的不平衡度量（单变量不平衡），所有度量都低于 0.1（其中 0 表示完全平衡，1 表示完全不平衡）。
但是，为什么一次考虑所有不平衡度量的多元 L1 距离相对较高（接近 0.51）？
我阅读了多元 L1 距离如何计算的定义。
上述原因是否是因为多元 L1 距离是根据治疗组和对照组之间所有匹配协变量的频率绝对差异计算得出的？
换句话说，多元 L1 距离可能很高，是因为每个匹配协变量的值范围可能彼此不同（例如，年龄范围为 17-55，而黑人和无学位具有二进制值），因此计算所有绝对差异自然会产生更高的多元 L1 距离？
我真的很想了解这一点，但我在互联网上找不到很好的解释。
如果我错了，我很高兴听到有人对此的评论。]]></description>
      <guid>https://stats.stackexchange.com/questions/552793/why-multivariate-l1-distance-from-coarsened-exact-matching-cem-is-high-compare</guid>
      <pubDate>Thu, 18 Nov 2021 20:37:25 GMT</pubDate>
    </item>
    </channel>
</rss>