<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 20 Aug 2024 15:16:39 GMT</lastBuildDate>
    <item>
      <title>预测评估中是否应将样本外预测值与实际值之间的相关性与 RMSE 一起纳入？</title>
      <link>https://stats.stackexchange.com/questions/653058/should-correlation-between-out-of-sample-forecasts-and-actual-values-be-included</link>
      <description><![CDATA[我目前正在使用均方根误差 (RMSE) 作为主要指标来评估预测模型的性能。我注意到一些文献还将样本外预测值与实际值之间的皮尔逊相关系数作为评估的一部分。
在我的预测评估中包含这种相关性是否有意义？它是否提供了 RMSE 未捕获的任何其他相关信息？我特别感兴趣的是相关性是否有助于理解预测值和实际值之间的关系，以及同时报告这两个指标是否是标准做法。
任何见解或最佳实践都将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/653058/should-correlation-between-out-of-sample-forecasts-and-actual-values-be-included</guid>
      <pubDate>Tue, 20 Aug 2024 15:14:52 GMT</pubDate>
    </item>
    <item>
      <title>执行 Dwass-Steel-Crichtlow-Fligner 事后检验后，我是否应该应用 FWER 校正？</title>
      <link>https://stats.stackexchange.com/questions/653056/should-i-apply-fwer-correction-after-performing-the-dwass-steel-crichtlow-fligne</link>
      <description><![CDATA[我有一个场景，需要比较多个非高斯分布的组（$N &gt; 2$）。在执行 Kruskal-Wallis 检验并找到显著的 p 值（$p &lt; \alpha$）后，我继续使用 Dwass-Steel-Crichtlow-Fligner (DSCF) 事后检验进行成对比较，以确定特定组之间的差异。
DSCF 检验为成对比较生成了多个 p 值。我的问题是，我是否需要对这些 p 值应用额外的家族错误率 (FWER) 校正，例如 Bonferroni、Holm 或 Hochberg 调整。
在问题事后成对分析中 Dwass-Steel-Critchlow-Fligner 与 Mann-Whitney U 检验之间的差异中提到了这一点，即“Dwass-Steel-Crichtlow-Fligner 检验也解决了 (1) 和 (2)，但内置了一种控制家族错误率 (FWER) 的特定方法。” Alexis 说过，但我没有足够的声誉来评论他的答案，所以发布了一个单独的问题。
有人可以澄清一下 DSCF 测试后是否需要额外的 FWER 校正，或者测试是否固有地控制了这些错误？
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/653056/should-i-apply-fwer-correction-after-performing-the-dwass-steel-crichtlow-fligne</guid>
      <pubDate>Tue, 20 Aug 2024 15:03:20 GMT</pubDate>
    </item>
    <item>
      <title>报告机器学习结果可重复性的环境详细信息</title>
      <link>https://stats.stackexchange.com/questions/653054/reporting-environment-details-for-reproducibility-of-results-in-machine-learning</link>
      <description><![CDATA[我是一名计算机科学硕士生，研究深度学习模型。
在我的大学，我们有一个带有 GPU 的计算机节点集群，用于训练机器学习模型。
我最近为我的论文开发了一个模型，我想保证它的开箱即用可重复性，而不仅仅是在我们的组织内。
我以编程方式报告每个作业的以下环境详细信息：

CPU 架构（例如 x86_64）
Linux 内核版本（例如 5.14.0-427.18.1.el9_4.x86_64）
glibc 版本（例如 2.34）
GPU 型号（例如 NVIDIA GeForce RTX 3090)
NVIDIA 驱动程序版本（例如 555.42.02）
CUDA 版本（例如 12.1）
cuDNN 版本（例如 8902）
Python 版本（例如 3.11.9）
pip 版本（例如 24.0）
NumPy 版本（例如 2.0.0）
PyTorch 版本（例如 2.3.1+cu121）
PyTorch Lightning 版本（例如2.3.0)
软件包版本（pip list --format=freeze 的输出）

但是，我们有两组节点，假设以上所有内容都相同（并且使用相同的数据、超参数、代码修订和随机种子，以及在训练器上设置了 deterministic=True），在从头开始进行全面训练时，在其中一个节点中可以重现我的结果，而在另一组节点中则不能。因此，似乎即使如此详细的报告也是不完整的。
我不知道为了保证可重复性，还有什么关于环境的重要报告。我不知道编译器设置有什么不同，也不确定其他哪些硬件差异可能会影响可重复性，但我希望我的报告尽可能具体，因为从我在硕士课程期间的经验来看，重现其他人的作品很困难，我不希望其他人在我的模型上遇到同样的事情。我只是不同意目前不可重复研究的状态，我选择改变这种状况，并通过尽我所能报告环境细节来帮助机器学习社区为可重复研究设定新的标准。
关于两组节点之间的差异，我想到了一些事情：

CPU 型号可能因节点而异。即使它们使用相同的架构，它对可重复性有任何影响吗？
GPU 型号可能采用不同的冷却和超频品牌包装。即使它们由相同的芯片组组成，它对可重复性有影响吗？
据我所知，两组节点在CPU 核心和 RAM的数量上有所不同。这些对可重复性有影响吗？

我不确定我是否可以获得集群中每个节点的硬件的完整描述，但如果可以的话，我应该报告哪些其他属性？
不幸的是，据我所知，目前关于机器学习可重复性的文献没有一份可能影响可重复性的所有环境细节的综合列表。一些有用的资源包括：

ML 可重复性工具和最佳实践
ML 可重复性挑战 ——&gt;资源
机器学习驱动研究中的可重复性
基于机器学习的研究中的可重复性：概述、障碍和驱动因素

&quot;一般来说，开箱即用的可重复性在多大程度上是值得怀疑的。&quot;


]]></description>
      <guid>https://stats.stackexchange.com/questions/653054/reporting-environment-details-for-reproducibility-of-results-in-machine-learning</guid>
      <pubDate>Tue, 20 Aug 2024 13:56:59 GMT</pubDate>
    </item>
    <item>
      <title>混合模型与问卷量表评分的归因模型？</title>
      <link>https://stats.stackexchange.com/questions/653052/mixed-model-vs-imputation-for-questionnaire-scale-score</link>
      <description><![CDATA[我想拟合一个统计模型，其中因变量（响应变量）是来自问卷的经过验证的量表分数。对于每个主题，这个因变量是根据一系列问题（项目）的值计算出来的。如果只有一个问题答案缺失，这意味着无法计算量表分数，因此目前被编码为缺失。
我想知道如何最好地处理这个问题。到目前为止，我想到了三个选择。第一个是对组成问题（而不是量表分数）使用多重插补，这样我们就可以为每个主题获得完整的记录，然后计算量表分数。第二个是插补量表分数本身。第三个是运行混合效应模型，而不必担心插补，因为众所周知，混合模型对缺失数据具有很强的鲁棒性。
我非常感谢您的指导！]]></description>
      <guid>https://stats.stackexchange.com/questions/653052/mixed-model-vs-imputation-for-questionnaire-scale-score</guid>
      <pubDate>Tue, 20 Aug 2024 13:47:12 GMT</pubDate>
    </item>
    <item>
      <title>“重复测量方差分析” vs “以受试者为随机效应的方差分析” vs “以受试者为随机效应的重复测量方差分析”</title>
      <link>https://stats.stackexchange.com/questions/653051/repeated-measures-anova-vs-anova-with-subject-as-random-effect-vs-repeated</link>
      <description><![CDATA[运行重复测量方差分析（对每个受试者随时间重复测量）与以受试者为随机效应的方差分析之间有什么区别？`
我也看到过提到“以受试者为随机效应的重复测量方差分析”。
对于上下文，我有 6 个组和每个组/受试者 3 个基线后时间点。每个受试者都是一个组的成员。我想比较同一时间点的两个不同组的平均值（例如，时间点 B 的组 1 与组 3）以及不同时间点的同一组的平均值（例如，时间点 B 的组 1 与时间点 C 的组 1）。
我将使用 ls-means 来比较均值，但我真的不明白上述三个模型类别之间的区别。如果有区别的话！
非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/653051/repeated-measures-anova-vs-anova-with-subject-as-random-effect-vs-repeated</guid>
      <pubDate>Tue, 20 Aug 2024 13:42:20 GMT</pubDate>
    </item>
    <item>
      <title>向 2x2 方差分析添加控制组</title>
      <link>https://stats.stackexchange.com/questions/653050/adding-a-control-group-to-a-2x2-anova</link>
      <description><![CDATA[我正在做一个有 2 个因素的实验，每个因素有两个水平，但我还想添加一个完全不会被操纵的控制组。
所以我很困惑 - 这个控制组应该被视为一个因素吗？如果是，它代表哪个因素？或者它会代表两个因素吗？那么它将如何作为 3x3 方差分析而不是 2x2 进行分析？]]></description>
      <guid>https://stats.stackexchange.com/questions/653050/adding-a-control-group-to-a-2x2-anova</guid>
      <pubDate>Tue, 20 Aug 2024 13:35:58 GMT</pubDate>
    </item>
    <item>
      <title>对多项混合效应模型的质疑</title>
      <link>https://stats.stackexchange.com/questions/653045/a-doubt-for-a-multinomial-mixed-effect-model</link>
      <description><![CDATA[假设我让 243 个人参加问卷调查。问卷包含 100 个涉及某些动词的问题，对于每个动词，个人必须猜出该动词的词根是名词还是形容词。然后我有一个数据集，其中包含响应变量 y，该变量与个人回答正确、回答错误或不愿意回答（因此是一个三级因素）、个人的唯一标识符 ID 和动词 ITEM 的标识符有关，以及许多其他解释变量 x1、x2、...，这些变量与动词的稀有性以及动词的词根是名词还是形容词有关。
我想我可以通过多项混合模型对响应变量 y 进行建模，其中 y 根据固定效应 x1、x2、... 而变化，其中随机效应由个人和动词给出。随机效应的正确规范是什么（例如 ~ (1|ID) + (1|ITEM) 或其他内容）？我应该使用哪个 R 包？]]></description>
      <guid>https://stats.stackexchange.com/questions/653045/a-doubt-for-a-multinomial-mixed-effect-model</guid>
      <pubDate>Tue, 20 Aug 2024 11:15:01 GMT</pubDate>
    </item>
    <item>
      <title>三个治疗组的差异</title>
      <link>https://stats.stackexchange.com/questions/653044/difference-in-differences-with-three-treated-groups</link>
      <description><![CDATA[我试图在 R 中进行差异分析，但不是传统意义上的差异分析。我的研究涉及三个同时接受治疗的组，没有真正的对照组。但是，我预计，与其他两个组（“出口导向型部门”和“进口导向型部门”）相比，治疗对一个组（“非贸易部门”）的影响会小得多，我预计其他两个组（“出口导向型部门”和“进口导向型部门”）的影响会更大。这就是为什么我想将“非贸易部门”作为对照组，将后两个作为治疗组。
我的主要目标是探索这些组之间的差异，特别是关注“治疗”组和我的“未治疗”组之间的比较。在我的设置下，我认为我只能探索组间平均治疗效果的差异，而不是整体 ATE，对吗？我正在研究的关系可以用以下模型描述（引用自这篇文章）
$$
y_{it} = \alpha + \beta_1 \text{Treat}_i^{\text{exp}} + \beta_2 \text{Treat}_i^{\text{imp}} + \beta \text{Post}_t + \delta_1 (\text{Treat}_i^{\text{exp}} \times \text{Post}_t) + \delta_2 (\text{Treat}_i^{\text{imp}} \times \text{Post}_t) + \gamma_t + \mu_i + \epsilon_{it}
$$
其中 $\text{Treat}_i^{\text{exp}}$ 是出口导向型组的指标， $\text{Treat}_i^{\text{imp}}
$￼是进口导向型组的指标。$\text{Post}_t$ 是一个虚拟变量，表示治疗后时期。$\gamma_t$ 是时间固定效应，$\mu_i$ 是组固定效应。
我正在测量对独立性的支持（一个虚拟变量）在治疗前后的变化情况。我预计，这种处理方式将使出口导向型群体更加反对独立，而进口导向型群体可能会变得更加支持。至于非贸易部门，我预计他们的反应将与出口导向型群体相似，尽管不那么明显。
由于我缺乏面板数据或纵向数据，只有来自不同时间段和个人的横截面数据，因此我根据特定的社会人口变量和个人工作的部门创建了一个伪面板。这意味着个人不能从一个群体转移到另一个群体。然而，鉴于所有群体都得到了处理，我担心潜在的溢出效应。
一个挑战是，虽然所有群体都同时得到了处理，但这种处理是通过媒体传播的。这意味着这种处理的覆盖范围可能取决于个人消费的媒体以及他们参与其中的强度，从而导致不同程度的接触。 （这可能意味着治疗效果并不均匀，尽管我不确定这种异质性是否更多地指的是群体对治疗的反应方式的差异。）为了解决这个问题，我计划另外根据媒体消费进行亚组分析。我想到对数据进行子集化（分别将我的两个治疗组之一排除在分析之外）并执行以下操作：
$$y_{it} = \alpha + \gamma_1 \text{Treat}_i^{\text{exp}} + \lambda \text{Post}_t + \delta_1 (\text{Treat}_i^{\text{exp}} \times \text{Post}_t) + \delta_2 (\text{Treat}_i^{\text{exp}} \times \text{Post}_t \times \text{Media}) + \gamma_t + \mu_i + \epsilon_{it}
$$
$$
y_{it} = \alpha + \gamma_1 \text{Treat}_i^{\text{imp}} + \lambda \text{Post}_t + \delta_1 (\text{Treat}_i^{\text{imp}} \times \text{Post}_t) + \delta_2 (\text{Treat}_i^{\text{imp}} \times \text{Post}_t \times \text{Media}) + \gamma_t + \mu_i + \epsilon_{it}
$$
最后考虑一下我的数据：我的组大小不均匀，“出口导向部门”有 76 人，“进口导向部门”有 110 人，“非贸易部门”有 558 人。我希望这不会给分析带来太多问题。

由于我对差异分析和复杂设置还不熟悉，因此我非常感谢任何指导或建议。]]></description>
      <guid>https://stats.stackexchange.com/questions/653044/difference-in-differences-with-three-treated-groups</guid>
      <pubDate>Tue, 20 Aug 2024 10:55:29 GMT</pubDate>
    </item>
    <item>
      <title>使用弹性网络进行特征选择</title>
      <link>https://stats.stackexchange.com/questions/653042/feature-selection-with-elastic-net</link>
      <description><![CDATA[我是机器学习中特征选择的新手。我读到弹性网络相对优于 Lasso 和 Ridge 回归，并且还考虑了多重共线性，所以我选择了这种方法进行特征选择。
我编写了一个代码，该代码将在嵌套交叉验证设置（5 个内折和 20 个外折）之后从最佳弹性网络模型（即具有 alpha 和 lambda 组合的弹性网络，可检索最佳 AUC）中选择最佳特征，并使用这些特征进行进一步的模型训练。
尽管如此，我不太确定我是否做对了，特别是在嵌套交叉验证方面。此外，当我尝试检索 alpha 和 lambda 以报告其可重复性时，我无法在结果中找到它。
以下是代码：
set.seed(1)
train_elastic_net &lt;- function(X_train, y_train, alphas, lambdas) {
best_auc &lt;- 0
best_model &lt;- NULL
best_features &lt;- NULL

for (alpha_value in alphas) {
# 设置交叉验证的训练控制
train_control &lt;- trainControl(method = &quot;cv&quot;, 
number = 5, 
classProbs = TRUE, 
summaryFunction = twoClassSummary)

# 定义调整网格
tune_grid &lt;- expand.grid(alpha = alpha_value, lambda = lambdas)

# 训练使用插入符号的模型
模型 &lt;- train(X_train, y_train,
method = &quot;glmnet&quot;,
trControl = train_control,
tuneGrid = tune_grid,
metric = &quot;ROC&quot;,
family = &quot;binomial&quot;)

# 获取最佳模型
best_lambda &lt;- model$bestTune$lambda
elastic_net_model &lt;- glmnet(as.matrix(X_train), y_train, alpha = alpha_value, lambda = best_lambda, family = &quot;binomial&quot;)

# 获取最佳模型的 AUC
auc &lt;- max(model$results$ROC)

# 如果当前模型更好，则更新最佳模型
if (auc &gt; best_auc) {
best_auc &lt;- auc
best_model &lt;- elastic_net_model
coef_elastic_net &lt;- coef(best_model, s = best_lambda)
selected_features &lt;- which(coef_elastic_net != 0) - 1 # 获取非零系数的索引
selected_features &lt;- selected_features[selected_features != 0] # 删除截距索引
best_features &lt;- selected_features
}
}

list(model = best_model, features = best_features)
}

y_train_factor &lt;- as.factor(y_train_total)
X_train_matrix &lt;- as.matrix(X_train_total)

# 定义要搜索的 alpha 和 lambda 值
alpha_values &lt;- seq(0.1, 1, by = 0.1)
lambda_values &lt;- 10^seq(-4, 1, length = 100)

# 为每个分割执行 20 次重复
n_repeats &lt;- 20
selected_features_list &lt;- list()

for (i in 1:n_repeats) {

# 使用弹性网络执行变量选择
result &lt;- train_elastic_net(X_train_matrix, y_train_factor, alpha_values, lambda_values)
best_model &lt;- result$model
best_features &lt;- result$features

# 存储此重复的选定特征
selected_features_list[[i]] &lt;- best_features
}
selected_features_list
# 聚合选定特征跨越所有重复
selected_features_final &lt;- Reduce(intersect, selected_features_list)
selected_features_final
# 使用最终选定的特征对训练数据进行子集化
X_train_total &lt;- X_train_total[, selected_features_final]
```
]]></description>
      <guid>https://stats.stackexchange.com/questions/653042/feature-selection-with-elastic-net</guid>
      <pubDate>Tue, 20 Aug 2024 09:37:53 GMT</pubDate>
    </item>
    <item>
      <title>寻找统计函数来表示数据</title>
      <link>https://stats.stackexchange.com/questions/653041/looking-for-statistical-function-to-represent-data</link>
      <description><![CDATA[假设我有一个包含 9 个原子的分子。对于每个原子，我使用某种方法计算了 prop_1、prop_2...prop_5 的属性。为了检查该方法是否稳定，我对每个属性进行了 10 次相同的计算。每次运行，我都有 MAE、maxAE 和 RMSE 值。例如，对于 prop_1，每个原子有 10 个 MAE、10 个 RMSE 和 10 个 maxAE 值。
我试图弄清楚两件事，

现在，假设对于 prop_1，我想报告 prop_1 上所有运行的数字（例如平均值 +- SD）。除了平均值+-SD之外，还有其他统计函数可以使用吗？

合并所有属性并检查方法的稳定性是否合理？


我是化学专业的，请原谅我的无知。]]></description>
      <guid>https://stats.stackexchange.com/questions/653041/looking-for-statistical-function-to-represent-data</guid>
      <pubDate>Tue, 20 Aug 2024 09:25:29 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 R 的元模型通过 tau2 报告疫苗有效性研究之间的差异？</title>
      <link>https://stats.stackexchange.com/questions/653040/how-to-report-the-between-study-variance-through-tau2-for-vaccine-effectiveness</link>
      <description><![CDATA[我和我的同事一直在使用 R 的 metafor 包和 rma.uni 函数进行元分析，以估计合并优势比。我们为 rma 提供 对数转换的 优势比和相应的方差。方法是 REML。
进行元分析后，我们将其反向转换为正态比 ($OR=10^{log(OR)}$)，然后使用 $ VE = (1-OR)*100$ 将其转换为以百分比给出的疫苗有效性 (VE) 值。
但是，当我们使用 $tau2 提取研究间标准方差 $τ^2$ 时，$τ$ 的单位基于将对数 OR 作为结果估计。
对于 $I^2$，这并不重要，因为它是以百分比显示的相对度量，但由于 $τ^2$ 是标准方差，它们只对估计的对数 OR 有意义，对吗？
我们如何转换获得的 $τ^2$ 值，以便将它们放在 VE 百分比旁边时有意义。我们是否只需将其转换为类似于将 OR 转换为 VE 的方式，例如：
$τ^2_{VE}=(1-τ^2_{OR})*100 $
或者更复杂？例如，我们是否还需要反向转换 $tau2？]]></description>
      <guid>https://stats.stackexchange.com/questions/653040/how-to-report-the-between-study-variance-through-tau2-for-vaccine-effectiveness</guid>
      <pubDate>Tue, 20 Aug 2024 09:21:56 GMT</pubDate>
    </item>
    <item>
      <title>远端结果的多级潜在类别分析中的 3 级聚类</title>
      <link>https://stats.stackexchange.com/questions/653039/3-level-clustering-in-multilevel-latent-class-analysis-of-distal-outcomes</link>
      <description><![CDATA[我正在根据 PISA 数据（横断面、连续的“远端结果”和类别的分类序数指标）研究学生经历的潜在类别（也汇总在学校层面）与学生成绩之间的关系。我打算使用多级潜在类别分析，这样我就可以在学生层面和学校层面对关系进行建模。但是，我很担心，因为学生数据不仅嵌套在学校中，还嵌套在国家/地区中（有 60 多个国家/地区）。由于我对国家/地区的影响不感兴趣，只想考虑国家/地区内的聚类，有没有办法进行这样的调整？我可以使用 Mplus 或 R。您有什么建议吗？或者您会推荐任何资源，其中有人使用这种 3 级聚类的 MLCA 来预测远端结果？]]></description>
      <guid>https://stats.stackexchange.com/questions/653039/3-level-clustering-in-multilevel-latent-class-analysis-of-distal-outcomes</guid>
      <pubDate>Tue, 20 Aug 2024 09:03:44 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯方法：模型比较</title>
      <link>https://stats.stackexchange.com/questions/653037/bayesian-approach-models-comparison</link>
      <description><![CDATA[我有两组配对样本，每组包含 3,000 个数据点。这些样本中的每个值代表特定模型的输出。

第一个模型作为验证参考，基于具有已知预测因子的现场测量，由于其理论假设，误差幅度为 10-20%。

第二个模型基于估计测量，误差幅度为 10%，但预测因子未知。


此外，两个样本都不是正态分布的。
我希望使用贝叶斯方法评估第二个模型的预测性能，而不考虑预测因子的贡献。
有人可以推荐使用贝叶斯方法解决这个问题的参考资料吗？R 中的示例将非常有帮助。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/653037/bayesian-approach-models-comparison</guid>
      <pubDate>Tue, 20 Aug 2024 07:25:47 GMT</pubDate>
    </item>
    <item>
      <title>在生存随机森林中，采样越大，误差越大</title>
      <link>https://stats.stackexchange.com/questions/653028/bigger-sampling-gives-bigger-errors-in-survival-random-forests</link>
      <description><![CDATA[我有一个包含 7 个分类变量的生存数据集。我使用随机生存森林（R 中的 randomForestSRC），并且尝试了不同的样本大小和树的数量。
在训练模型之前，我会随机抽取数据样本。总的来说，我有 180 万个数据点。我尝试了 1%、5%、10% 和 50% 的随机样本。由于相关的计算复杂性（每次训练都需要几个小时），我无法使用整个数据集。
我观察到，样本量越大，误差越大（以 1 减去 C 指数计算）。
以下是我的结果：

我的问题是为什么会发生这种情况。似乎，我用来训练随机森林模型的数据越多，误差就越大。
我的猜测是，我使用的数据越多，算法就越难找到解决方案，最终找到的答案就越不准确。]]></description>
      <guid>https://stats.stackexchange.com/questions/653028/bigger-sampling-gives-bigger-errors-in-survival-random-forests</guid>
      <pubDate>Mon, 19 Aug 2024 20:57:51 GMT</pubDate>
    </item>
    <item>
      <title>纪元 (epoch) 与数据复制相同吗？</title>
      <link>https://stats.stackexchange.com/questions/653012/are-epochs-the-same-as-data-duplication</link>
      <description><![CDATA[时期，即对原始数据重复训练的次数，对于神经网络来说是绝对必要的，因为神经网络的参数通常比原始实例多得多。
对于具有许多参数的其他 ML 方法，使用时期的方法论地位是什么？它们是否用于（我称之为）类技术，如随机森林？通过“方法论地位”，我隐藏了我的真实意图。也就是说，为什么不将时期用于普通的逻辑回归或决策树？为什么不将每个额外的时期视为数据增强，但使用相同的实例？如果建议重复数据以减少偏差，我觉得任何统计学家都会感到恶心。 ML 建模者会毫不犹豫地无限期地增加 epoch，直到训练/测试准确率开始变差（甚至可能到那时也不会变差！）。
那么

在传统预测模型中使用 epoch（基本重复实例）可以吗（只要考虑到过度拟合？
另一方面，一个 epoch 是否应该成为 NN 训练的标准，或者更确切地说，使用重复数据作为增强技术是否存在各种统计问题？
]]></description>
      <guid>https://stats.stackexchange.com/questions/653012/are-epochs-the-same-as-data-duplication</guid>
      <pubDate>Mon, 19 Aug 2024 14:12:29 GMT</pubDate>
    </item>
    </channel>
</rss>