<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Mon, 09 Sep 2024 03:19:27 GMT</lastBuildDate>
    <item>
      <title>如何限制浅层神经网络的泛化误差？</title>
      <link>https://stats.stackexchange.com/questions/654068/how-to-bound-generalization-error-of-a-shallow-nn</link>
      <description><![CDATA[我有一个使用 ReLU 训练的单隐藏层 NN。总共有 2N+d 个参数（N 是节点数，d 是维数）。训练此 NN 的样本大小正好是 N。
现在，在这种特殊情况下，我有一种方法可以确保优化误差正好为 0（即平方损失为 0）
由于参数数量和样本大小之间的特殊关系，我很难限制泛化误差。请给我一些提示。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/654068/how-to-bound-generalization-error-of-a-shallow-nn</guid>
      <pubDate>Mon, 09 Sep 2024 02:17:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么较小的权重有助于泛化？</title>
      <link>https://stats.stackexchange.com/questions/654067/why-does-having-a-smaller-set-of-weight-help-with-generalization</link>
      <description><![CDATA[当我第一次学习机器学习时，我了解到我们需要使用 l2 正则化来提高泛化能力。原因基于 Chris Bishop 教科书中的多项式回归实验，作者展示了过度拟合的多项式的权重具有较大的权重，这将对测试数据产生错误的预测（这里的论点是，较大的权重将与数据相乘得到一个较大的预测）。正则化将降低权重的大小。
但是，这种（较小的权重 = 更好的泛化）见解是否真的可以推广到多项式回归的范围之外？例如，具有较小权重的神经网络必须表现更好吗？即使在多项式回归的背景下，我也不太确定。例如，我想将百万富翁的年龄与他们的收入联系起来。我的权重必须很大。小权重在这里实际上没有意义。
同样的论点出现在关于双下降和隐式正则化的文献中。据说双下降法有效，因为 SGD 隐式地在过度参数化的状态下找到了最小范数权重。但问题又来了：为什么小的权重集可以转化为更好的泛化？
有人可以对此发表意见吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/654067/why-does-having-a-smaller-set-of-weight-help-with-generalization</guid>
      <pubDate>Mon, 09 Sep 2024 02:07:10 GMT</pubDate>
    </item>
    <item>
      <title>如何获得高斯和的 LRT 和 ROC（高斯项的数量具有泊松分布）？</title>
      <link>https://stats.stackexchange.com/questions/654066/how-to-obtain-lrt-and-roc-of-sum-of-gaussians-with-number-of-gaussian-terms-has</link>
      <description><![CDATA[若
$$
y = \sum_{i=0}^n x_i
$$
其中$x_i$为独立同分布随机变量，其密度为高斯分布$N(0, \sigma^2)$。总和中的变量数是具有泊松分布的随机变量：
$$
\Pr(n = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, \ldots
$$
我们想在两个假设之间做出决定：
$$
H_1 : n \leq 1 \quad \text{and} \quad H_0 : n &gt; 1
$$.
我想获得 LRT（并尽可能简化它）。然后，我想获得 ROC 曲线（检测概率与误报概率）？
首先，我们有：
（H_1）下的似然：
在（H_1）下，（n）可以是 0 或 1。因此，（H_1）下（y）的概率分布是两个高斯的混合：
$$
f_{Y|H_1}(y) = e^{-\lambda} \delta(y) + \frac{\lambda e^{-\lambda}}{1!} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{y^2}{2\sigma^2}}
$$
其中（ \delta(y) ) 是狄拉克 delta 函数，表示 ( n = 0 ) 时概率质量为零。
( H_0 ) 下的可能性：
( H_0 ) 下，( n &gt; 1 )。 ( H_0 ) 下的 ( y ) 概率分布是 $n \geq 2$ 的高斯加权和：
$$
f_{Y|H_0}(y) = \sum_{k=2}^{\infty} \frac{e^{-\lambda} \lambda^k}{k!} \frac{1}{\sqrt{2\pi k\sigma^2}} e^{-\frac{y^2}{2k\sigma^2}}
$$
似然比检验 (LRT)：
LRT 统计量定义为：
$$
\Lambda(y) = \frac{f_{Y|H_1}(y)}{f_{Y|H_0}(y)}
$$
然后，我卡在这里了；我怎样才能进一步简化它以获得 ROC？]]></description>
      <guid>https://stats.stackexchange.com/questions/654066/how-to-obtain-lrt-and-roc-of-sum-of-gaussians-with-number-of-gaussian-terms-has</guid>
      <pubDate>Mon, 09 Sep 2024 02:07:03 GMT</pubDate>
    </item>
    <item>
      <title>重复测量与独立方差分析用于比较针对参与者特定数据训练的机器学习模型</title>
      <link>https://stats.stackexchange.com/questions/654065/repeated-measures-vs-independent-anova-for-comparing-machine-learning-models-tr</link>
      <description><![CDATA[我有一个机器学习项目，其中我在包含来自多个参与者的数据的单个数据集上训练多个模型（例如，模型 A、模型 B、模型 C）。每个参与者的数据都用于训练所有三个模型，从而产生每个模型的特定于参与者的实例（例如，参与者 1 有模型 A1、B1、C1）。
我评估每个模型（A、B、C）在保留测试集上对每个参与者的准确性。
我的目标是比较这些模型（A、B、C）在所有参与者中的整体准确性。我目前对任何交互作用不感兴趣。
我应该使用重复测量方差分析还是独立方差分析进行此分析？
假设：
我们假设方差分析的必要假设（正态性、方差齐性）得到满足。如果不满足，我们准备使用非参数替代方案。]]></description>
      <guid>https://stats.stackexchange.com/questions/654065/repeated-measures-vs-independent-anova-for-comparing-machine-learning-models-tr</guid>
      <pubDate>Mon, 09 Sep 2024 01:42:56 GMT</pubDate>
    </item>
    <item>
      <title>Sklearn PLS 属性 X_scores、X_weights、X_Loadings 含义</title>
      <link>https://stats.stackexchange.com/questions/654062/sklearn-pls-attributes-x-scores-x-weights-x-loadings-meaning</link>
      <description><![CDATA[尝试阅读 sci-kit 文档，但文档很短，帮助不大。有人能用通俗易懂的术语来描述通过 sklearn 进行偏最小二乘回归 PLS 的 X 和 Y 分数、权重和载荷吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/654062/sklearn-pls-attributes-x-scores-x-weights-x-loadings-meaning</guid>
      <pubDate>Sun, 08 Sep 2024 22:19:14 GMT</pubDate>
    </item>
    <item>
      <title>主教高斯基</title>
      <link>https://stats.stackexchange.com/questions/654060/bishop-gaussian-basis</link>
      <description><![CDATA[在 Christopher Bishop 所著的模式识别与机器学习一书中，他在第 3.3.2 节“预测分布”中说道

如果我们使用局部基函数（如高斯函数），那么在远离基函数中心的区域中，预测方差（3.59）中第二项的贡献将变为零，只留下噪声贡献$\beta^{−1}$。因此，
当在基函数所占区域外进行外推时，模型对其预测非常有信心，这通常是不受欢迎的行为。

但就在本段之前，它说

请注意，预测不确定性取决于$x$，并且在数据点的邻域中最小。还要注意，随着观察到更多的数据点，不确定性水平会降低。

在图中，当远离数据点时，方差似乎很高。]]></description>
      <guid>https://stats.stackexchange.com/questions/654060/bishop-gaussian-basis</guid>
      <pubDate>Sun, 08 Sep 2024 21:42:32 GMT</pubDate>
    </item>
    <item>
      <title>使用分类数据和现有标签对数据点进行聚类的算法</title>
      <link>https://stats.stackexchange.com/questions/654059/algorithm-to-cluster-data-points-with-categorical-data-and-existing-labels</link>
      <description><![CDATA[假设我有一个电子商务产品浏览数据的数据框，其中每一行代表一个产品浏览量以及它是否导致转化（购买/未购买）
数据的示例列：currencyCode、country、pageType、isMobile、browser、hourOfDay、productPrice、converted。
我的目标是使用除价格之外的所有特征（所有/大多数都是分类的）合理地将“群组”分配给用户浏览量。同时考虑数据中的价格和标签（几乎像半监督）。这样，当出现新视图（没有价格和转化数据）时，我知道该视图属于哪个群集。
这些视图的聚类应该以某种方式考虑productPrice和converted的影响。想知道对这些数据点进行聚类的最佳方法是什么？
我知道存在处理分类数据的 kprototypes / kmodes，但还没有找到一种方法来考虑价格和转换的影响，而且它也没有考虑数据的标签。]]></description>
      <guid>https://stats.stackexchange.com/questions/654059/algorithm-to-cluster-data-points-with-categorical-data-and-existing-labels</guid>
      <pubDate>Sun, 08 Sep 2024 21:25:11 GMT</pubDate>
    </item>
    <item>
      <title>样本量计算的假设检验与置信区间</title>
      <link>https://stats.stackexchange.com/questions/654055/hypothesis-testing-vs-confidence-intervals-for-sample-size-calculation</link>
      <description><![CDATA[假设我们正在审计一个低风险集团，该集团的历史平均逃税额为 \$7,883，审计时的标准差为 \$27,274。我的目标是确定平均逃税额是否接近于零。使用 t 检验，假设平均逃税额为零（双侧，$\alpha = 0.05$，功效 = 0.8），双侧检验所需的样本量为 96，单侧检验所需的样本量为 76。我假设备择假设下的平均值和标准差就是历史数据所显示的。
但是，如果我使用置信区间 $\bar{x} \pm 1.96 * \frac{\sigma}{\sqrt{n}}$ 进行计算并将下限设置为接近零，则求解 $n$ 得到的样本量为 46。
使用样本量 46 来确定零是否在置信区间内有效，从而否定了假设检验所建议的更大样本量的需要吗？如果不是，为什么这种方法是错误的？]]></description>
      <guid>https://stats.stackexchange.com/questions/654055/hypothesis-testing-vs-confidence-intervals-for-sample-size-calculation</guid>
      <pubDate>Sun, 08 Sep 2024 20:43:37 GMT</pubDate>
    </item>
    <item>
      <title>使用诱导点进行精确的高斯过程推断</title>
      <link>https://stats.stackexchange.com/questions/654054/using-inducing-points-for-exact-gaussian-process-inference</link>
      <description><![CDATA[我对使用诱导点进行高斯过程推断有点困惑，特别是在应该是精确推断而不是近似的情况下。
对于高斯过程$f\sim GP(\textbf{0}, \kappa)$，其中
$ \kappa :X\times X \rightarrow \mathbb{R}$为正定核，训练数据为$f(x_1), \dots, f(x_n)$，如果想推断新点$x_*$的值，我们通常以$\mathcal{O}(n^3)$ 来反转矩阵 $\kappa(f(x_i), f(x_j))$，其中 $i,j=1,\dots n$，其中 $n$ 是提供正态分布 $p(f(x_*)|f(x_1), \dots, f(x_n))$ 的训练点数。
我的问题是关于使用诱导点来加速这种推理。具体来说，如果我们让
$y= f(x_*) + z$其中$z \sim N(0, \varepsilon)$，
则$p(y | f(x_*),f(x_1), \dots, f(x_n)) = p(y | f(x_*))$，
但同时$\{ f(x) | x \in X\} \cup \{y\}$ 是一个高斯过程，如果我们将 $\kappa$ 扩展到 $X \cup \{x_{**}\}$ 域并添加一个点，其中 $f(x_{**}) :=y$，以自然的方式（考虑到它提供了 GP 的协方差）：$\kappa&#39;(x_{**},x) := \kappa(x_*,x) \text{ for } x\neq x_{**} \text{ and } \kappa&#39;(x_{**},x_{**}) := \kappa(x_{*},x_{*}) + \varepsilon$
那么为什么我们不能使用诱导点方法，以诱导点为 $x_*$，推断 $\mathcal{O}(m^2n) = \mathcal{O}(n)$ 时间中 $f(x_{**})$ 的值，其中诱导点的数量为 $m=1$，如此处所用：https://andrewcharlesjones.github.io/journal/inducing-points.html，或在本文中：https://mlg.eng.cam.ac.uk/zoubin/papers/nips05spgp.pdf。
更详细地说，由于我们有 $p(y | f(x_*),f(x_1), \dots, f(x_n)) = p(y | f(x_*))$，我们知道 $p(y|f(x_1), \dots, f(x_n)) = \int p(y| f(x_*))\ p(f(x_*)|f(x_1), \dots, f(x_n))\ df(x_*)$，并且从右侧表达式中我们得到高斯形式，其参数我们可以在亚立方时间内推断出来，如上链接所示。
然后在这种情况下，如果 $\varepsilon$ 很小，我们可以计算后验的近似值 $p(y|f(x_1), \dots, f(x_n))$ 在线性时间内完成，这似乎是不可能的，因为这应该是 $\mathcal{O}(n^3)$。]]></description>
      <guid>https://stats.stackexchange.com/questions/654054/using-inducing-points-for-exact-gaussian-process-inference</guid>
      <pubDate>Sun, 08 Sep 2024 20:36:59 GMT</pubDate>
    </item>
    <item>
      <title>是否可以在循环图中定义有限路径？</title>
      <link>https://stats.stackexchange.com/questions/654051/it-is-possible-to-define-finite-path-in-looped-graphs</link>
      <description><![CDATA[我对图论不太感兴趣，所以这个问题可能毫无意义。从非指导的角度来看，有向图或无向图中的路径和循环概念是清晰的。但如果我们转向循环图（即节点可以有循环），仍然可以在节点之间定义一条有限路径，其中中间的某些顶点是循环的？有人能给我指出一些文献参考吗？
提前谢谢
Paolo]]></description>
      <guid>https://stats.stackexchange.com/questions/654051/it-is-possible-to-define-finite-path-in-looped-graphs</guid>
      <pubDate>Sun, 08 Sep 2024 19:28:11 GMT</pubDate>
    </item>
    <item>
      <title>注意力是一种 K 近邻回归器吗？</title>
      <link>https://stats.stackexchange.com/questions/654050/is-attention-a-kind-of-k-nearest-neighbors-regressor</link>
      <description><![CDATA[我想出了一个奇怪的 KNN 算法，我认为它相当于自注意力：

假设我们有一个典型的 (特征、标签) 样式数据集 $\mathscr D = \{(x_1, Vx_1), (x_2, Vx_2),\dots, (x_N, Vx_N)\}$，其中 $x_n$ 是向量，V 是某个矩阵。
原则上，我们可以将常规 KNN 应用于此集合，将每个向量 $x_n$ 视为查询向量，并让 $K=|\mathscr D \setminus (x_n, Vx_n)|=N-1$，因此所有都是邻居，没有前 K 个最近向量：

给定某个特征向量 $x_n$，计算其与所有其他向量的相似度：$S(x_n)=\left\{\frac{1}{\lVert x_n - x \rVert} : x \in \mathscr D \setminus x_n\right\}$。我猜这也需要规范化，这样 $\sum_i S_i(x_n)=1$。
输出是标签的加权和：$y_n^* = \sum_{i} S_i(x_n) Vx_i$。
对所有其他向量重复此操作，并获得一组对应于每个特征向量的预测标签：$\mathscr Y = \{y_1^*, y_2^*, \dots, y_N^*\}$。


现在像这样修改上面的 KNN：

使用点积作为相似度度量：$s_{Q,K}(x_n, x_m) = x_n&#39; Q&#39;K x_m$.
现在 $x_n$ 的权重为：$S(x_n)=\mathrm{softmax}\left(\left\{s_{Q,K}(x_n, x) : x \in \mathscr D\right\}\right)$.

我们迭代整个数据集，包括 $x_n$ 本身，因为这里不可能除以零。在我看来，我们也可以像上面一样使用 $\mathscr D \setminus x_n$。
我们使用 softmax，而不是通过 $\sum_{x \in \mathscr D \setminus x_n} \frac{1}{\lVert x_n - x \rVert}$ 进行归一化。


预测标签再次为 $y_n^* = \sum_{i=1}^N S_i(x_n) Vx_i$。


如果 $Vx_n$（因此 $y_n^*$) 具有相同的维度，我们可以堆叠这些算法，使用 $\{y_n^*\}$ 作为下一层的 $\{x_n\}$。

我认为我的算法 (2) 1) 与自注意力相同，2) 是一种 KNN，或“全最近邻”。因此，似乎可以将自注意力理解为一种 KNN。
这有意义吗？我可以这样思考注意力吗？

编辑：我发现了一些介绍某种“连续最近邻居”的论文：

“神经最近邻居网络”，2018 年（方程 10-11）：https://proceedings.neurips.cc/paper_files/paper/2018/file/f0e52b27a7a5d6a1a87373dffa53dbe5-Paper.pdf
“KVT： k-NN Attention for Boosting Vision Transformers”，2022 年（第 3.2 节似乎讨论了与上述算法 (2) 类似的内容）：https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136840281.pdf
]]></description>
      <guid>https://stats.stackexchange.com/questions/654050/is-attention-a-kind-of-k-nearest-neighbors-regressor</guid>
      <pubDate>Sun, 08 Sep 2024 19:26:57 GMT</pubDate>
    </item>
    <item>
      <title>是否可以对多项研究的总体方差取平均值以用于样本量计算？</title>
      <link>https://stats.stackexchange.com/questions/654048/can-population-variance-from-multiple-studies-be-averaged-to-use-for-a-sample-si</link>
      <description><![CDATA[想象一下，您正在计划一项临床试验，以评估一种新疗法对改善慢性中风患者 VO2peak 的有效性。根据 Jin 等人的初步研究，您使用 Jin 等人报告的 VO2peak 方差估计试验所需的样本量。
来自 Jin 等人的数据：

VO2peak 方差：2.5 ml/kg/min
估计样本量：每组 40 名参与者

但是，您后来发现，您的试点研究中的方差比 Jin 等人的估计值高得多。
来自其他研究的数据：

DaCun：方差 = 10 ml/kg/min
Mac：方差 = 15 ml/kg/min
Len：方差 = 20 ml/kg/min
Ive：方差 = 18 ml/kg/min
Glob：方差 = 25 ml/kg/min

修订方法：
现在，您不再仅仅依赖 Jin 等人的方差估计，而是包括来自这些其他研究的数据：

来自其他研究的平均方差：(10 + 15 + 20 + 18 + 25) / 5 = 17.6 ml/kg/min

修订样本量计算：使用这个更高的平均方差，您可以计算出一个新的样本量估计值。


结果：

新的样本量估计：每组 80 名参与者（基于更高的平均方差）

这是一种可接受的方法吗？计算样本量？]]></description>
      <guid>https://stats.stackexchange.com/questions/654048/can-population-variance-from-multiple-studies-be-averaged-to-use-for-a-sample-si</guid>
      <pubDate>Sun, 08 Sep 2024 17:52:41 GMT</pubDate>
    </item>
    <item>
      <title>协方差矩阵的浓度不等式</title>
      <link>https://stats.stackexchange.com/questions/654041/concentration-inequality-of-the-covariance-matrix</link>
      <description><![CDATA[在高维统计领域，我们经常利用集中不等式来获得高概率的收敛速度。当我阅读有关图形模型的讲义时，我发现$\|\Sigma-\hat{\Sigma}\|_{max}$的集中不等式经常出现在$\|A\|_{max}=\underset{i,j}{\max}|X_{ij}|$。
我的问题：

范数$\|\cdot\|_{\max}$的频繁出现意味着什么，背后隐藏着什么直觉？

我想知道协方差矩阵与其他范数的集中不等式，例如谱范数$\|\Sigma-\hat{\Sigma}\|_2$。据您所知，此浓度不等式的最佳结果是什么？

]]></description>
      <guid>https://stats.stackexchange.com/questions/654041/concentration-inequality-of-the-covariance-matrix</guid>
      <pubDate>Sun, 08 Sep 2024 15:37:24 GMT</pubDate>
    </item>
    <item>
      <title>传递核的分解是否定义明确？</title>
      <link>https://stats.stackexchange.com/questions/654039/is-the-decomposition-of-a-transitive-kernel-well-defined</link>
      <description><![CDATA[在下面的论文中，作者写道：
假设某个函数 (p(x, y)) 的转换核表示为
$$
P(x, d y)=p(x, y) d y+r(x) \delta_x(d y),
$$
其中 $p(x, x)=0, \delta_x(d y)=1$ 如果 $x \in d y$ 否则为 0，并且 $r(x)=1-\int_{\mathbb{R}^d} p(x, y) d y$ 是链保持在 $x$。从 $r(x) \neq 0$ 的可能性来看，应该清楚的是 $p(x, y)$ 对 $y$ 的积分不一定是 1。
我的问题：您如何验证此分解实际上等于过渡核？换句话说：我如何知道满足此等式的函数 $p(x,y)$ 和 $r(x)$ 确实存在？]]></description>
      <guid>https://stats.stackexchange.com/questions/654039/is-the-decomposition-of-a-transitive-kernel-well-defined</guid>
      <pubDate>Sun, 08 Sep 2024 13:30:32 GMT</pubDate>
    </item>
    <item>
      <title>矩阵范数的次梯度</title>
      <link>https://stats.stackexchange.com/questions/654025/subgradient-of-the-matrix-norm</link>
      <description><![CDATA[当我想要获得矩阵变量套索方法的统计特性时，例如，
$$
\hat{X}=\underset{X\in \mathbb{R}^{n\times n}}{\arg \min}\mathcal{L}\left(X\right)+\lambda_n \|X\|_1,
$$
其中 $\mathcal{L}$ 是损失函数，$\|X\|_1=\sum_{i,j}|X_{ij}|$，我总是关注其方向导数的 KKT 条件。因此，我有
$$
\mathcal{L}^{\prime}\left(\hat{X}\right)+\lambda_n Z,
$$
其中$Z\in \partial\|\hat{X}\|_1$且$\|Z\|_{\max}=\underset{i,j}{\max}|Z_{ij}|\le 1$。
我们能否限制$Z$的谱范数？我了解到，如果 $Z$ 是核范数的次梯度，则 $Z$ 的谱范数小于 1。逐元素 1 范数是否具有类似的性质？]]></description>
      <guid>https://stats.stackexchange.com/questions/654025/subgradient-of-the-matrix-norm</guid>
      <pubDate>Sun, 08 Sep 2024 04:56:35 GMT</pubDate>
    </item>
    </channel>
</rss>