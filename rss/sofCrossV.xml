<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 01 Feb 2025 12:28:48 GMT</lastBuildDate>
    <item>
      <title>样本均值与自举均值相同，但 T 检验拒绝原假设</title>
      <link>https://stats.stackexchange.com/questions/660844/sample-mean-is-same-as-bootstrapped-mean-but-t-test-rejects-null-hypothesis</link>
      <description><![CDATA[我有 2 个样本（每个样本有大约 8500 个数据点）。我进行了引导（有替换）并绘制了平均差异的分布：

然后，我计算了 2 个样本的实际平均差异，结果是-5.53
我的理解表明，这意味着我的样本之间没有显着差异。但是，当我进行 T 检验时，我得到p_value=0.048
有人可以解释这是怎么可能的吗？据我理解，这本质上就是 T 检验所做的工作 - 创建均值差异分布并检查样本之间的实际均值差异在该分布范围内的可能性。
每个样本的原始分布都非常右偏，但我认为 8500 的样本量可以弥补这一点。此外，我进行了 Levene 检验，两个样本之间的方差没有差异。]]></description>
      <guid>https://stats.stackexchange.com/questions/660844/sample-mean-is-same-as-bootstrapped-mean-but-t-test-rejects-null-hypothesis</guid>
      <pubDate>Sat, 01 Feb 2025 11:34:50 GMT</pubDate>
    </item>
    <item>
      <title>选择赔率是多少[关闭]</title>
      <link>https://stats.stackexchange.com/questions/660843/what-are-the-select-odds</link>
      <description><![CDATA[我参加了一场有 150 张彩票的抽奖，我买了 10 张，赢得了前 3 次抽奖，之后又赢得了总共 25 个奖品中的 6 个。几率是多少？]]></description>
      <guid>https://stats.stackexchange.com/questions/660843/what-are-the-select-odds</guid>
      <pubDate>Sat, 01 Feb 2025 10:56:13 GMT</pubDate>
    </item>
    <item>
      <title>哪种流行的参数分布（或分布混合）可以很好地模拟人类的死亡年龄？</title>
      <link>https://stats.stackexchange.com/questions/660842/what-popular-parametric-distribution-or-mixture-of-distributions-does-a-good-j</link>
      <description><![CDATA[对于那些不知道的人来说，某人死亡年龄的分布看起来是这样的：

显然，在接近 0 岁时有一个峰值，这可以解释婴儿死亡率。这可以通过以接近 0-1 岁为中心的狄拉克 Delta 或薄伽马分布轻松解释。但对于其余分布，是否有一个流行的参数分布可以很好地捕捉其形状？传统上，人们可能会使用正态分布，或者更好的是伽马分布。但这些都不能解释分布的“尾部”似乎偏左。如果我必须选择一个能够很好地拟合形状的分布，我可能会使用缩放/拉伸的 Beta 分布。但是，虽然它完全符合形状，但似乎没有任何强有力的理论理由来证明它是所使用的分布。是否有另一个参数分布（或分布混合）可以更好地拟合该数据并且具有相当强的理论理由来应用？]]></description>
      <guid>https://stats.stackexchange.com/questions/660842/what-popular-parametric-distribution-or-mixture-of-distributions-does-a-good-j</guid>
      <pubDate>Sat, 01 Feb 2025 10:34:04 GMT</pubDate>
    </item>
    <item>
      <title>当我没有真实标签时，如何评估模型（预先训练的 BERT 和 VADER 进行情绪分析）？</title>
      <link>https://stats.stackexchange.com/questions/660839/how-can-i-evaluate-model-pre-trained-bert-and-vader-for-sentiment-analysis-whe</link>
      <description><![CDATA[我有一个包含文本的数据集，我使用预先训练的 BERT 和 VADER 模型来获取情感标签和情感分数。如果没有真实标签，我不知道如何评估我的模型。我可以使用哪些统计测试？有什么方法可以比较哪个模型更好？目前，我使用 Cohen&#39;s Kappa 来查看分类中的一致性。
此外，我计划对预先训练的 BERT 模型进行超调（仍然没有真实标签），在这种情况下，我如何才能更好地评估我的模型？]]></description>
      <guid>https://stats.stackexchange.com/questions/660839/how-can-i-evaluate-model-pre-trained-bert-and-vader-for-sentiment-analysis-whe</guid>
      <pubDate>Sat, 01 Feb 2025 05:14:14 GMT</pubDate>
    </item>
    <item>
      <title>堆叠泛化算法的特征选择</title>
      <link>https://stats.stackexchange.com/questions/660838/feature-selection-for-stacked-generalization-algorithm</link>
      <description><![CDATA[我对机器学习还比较陌生，我偶然发现了一篇我所在领域的论文，其中提出了一种新的集成模型。然而，他们的三个特征输入来自同一个基础模型，包括其预测值、FDR 校正分数以及 p 值校正分数，(score * (1 - x))，如果分数 &gt; 0.5 且 x &gt;= 0.5，其中 x 是 FDR 或 p 值。
我尝试四处寻找，但没有遇到任何类似的例子。这有道理吗？这似乎过分强调单个基础模型，还是“期望”集成模型能够在适当的训练过程中将其过滤掉？
任何链接或资源都非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/660838/feature-selection-for-stacked-generalization-algorithm</guid>
      <pubDate>Sat, 01 Feb 2025 03:46:46 GMT</pubDate>
    </item>
    <item>
      <title>机器学习算法的度量选择</title>
      <link>https://stats.stackexchange.com/questions/660835/metric-choice-for-machine-learning-algorithm</link>
      <description><![CDATA[我目前正在为二元分类问题构建一个 ML 模型。
我目前正在使用研究论文中提供的精选数据集，该数据集已经完全平衡。但是，众所周知，现实生活中的情况并非如此（负数的数量远高于正数的数量）。
我正在尝试选择一个合适的指标。我在网上读到，平衡的数据集应该选择准确度，而不平衡的数据集将是 F1 或 MCC。但是，考虑到现实世界数据不平衡的事实，我应该选择 MCC 或 F1 而不是准确度吗？或者我的模型应该根据数据集构建并使用准确度？
任何链接或资源都非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/660835/metric-choice-for-machine-learning-algorithm</guid>
      <pubDate>Sat, 01 Feb 2025 02:18:18 GMT</pubDate>
    </item>
    <item>
      <title>线性回归中 Y 的正态分布和多元正态分布之间的区别？</title>
      <link>https://stats.stackexchange.com/questions/660833/difference-between-normal-and-multivariate-normal-distribution-for-y-in-linear-r</link>
      <description><![CDATA[在线性回归中，我假设当 $y$ 属于正态分布时，是因为只有一个变量，就像一个简单的线性回归，例如 $y = \beta_{0} + \beta_1 x_1 + \epsilon$。当有更多变量时，$y$ 属于多元正态分布，我们讨论的是多元线性回归。
有人可以证实这一点吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/660833/difference-between-normal-and-multivariate-normal-distribution-for-y-in-linear-r</guid>
      <pubDate>Sat, 01 Feb 2025 00:07:14 GMT</pubDate>
    </item>
    <item>
      <title>当所选模型未赢得所有外部折叠时解释嵌套 CV 结果</title>
      <link>https://stats.stackexchange.com/questions/660831/interpreting-nested-cv-results-when-selected-model-didnt-win-all-outer-folds</link>
      <description><![CDATA[在嵌套交叉验证中，我看到了一个有趣的场景，我希望更好地理解它：
使用 4 倍外部 CV，我的模型选择过程总体上选择了模型 A（它在内部 CV 循环中平均表现最佳）。但是，查看单个外折：

模型 A 只赢得了 4 个外折中的 2 个
其他模型赢得了剩余的 2 个折

在报告模型 A 在新数据上的预期性能时，我们使用所有外折结果的平均值 - 包括其他模型表现更好的 2 个折。
（值得一提的是，当我在完整数据集上重新执行模型选择过程时，模型 A 总体上获胜，但这些指标将是有偏估计量，因此无法报告）
问题：

为什么在估计模型 A 的性能时包含未选择模型 A 的外折结果是有效的？
如何说服那些可能对“混合其他模型的指标”感到不安的利益相关者
是否有一些我们可以使用数学基础来证明这一点？

我正在考虑类似这样的问题：$$\mathbb{E}[\text{Loss}(\text{Model A}(X), Y)| \text{Model A 由选择程序选择}] $$ 其中 $X$ 是训练数据。我很想深入了解数学细节，以及如何使用它来证明模型选择程序和嵌套 cv 本身的合理性。我理解为什么它会给出无偏估计，但很难说服不熟悉这个概念的技术受众。
这个问题这里解释了是什么，但我正在寻找更正式的理由来解释为什么，因为有必要向技术受众证明这种方法的有效性，而且直观地看，他们不相信对所有外循环的结果取平均是可以的，即使它与您选择的模型不同。]]></description>
      <guid>https://stats.stackexchange.com/questions/660831/interpreting-nested-cv-results-when-selected-model-didnt-win-all-outer-folds</guid>
      <pubDate>Fri, 31 Jan 2025 23:09:25 GMT</pubDate>
    </item>
    <item>
      <title>使用交叉验证来确定因子分析中的因子数量：为什么不是因子越多，可能性就越大？</title>
      <link>https://stats.stackexchange.com/questions/660830/use-cross-validation-to-determine-number-of-factors-in-factor-analysis-why-the</link>
      <description><![CDATA[考虑因素分析模型
\begin{方程*}
\开始{数组}{cccccccccc}
X &amp;=&amp; \mu&amp;+&amp; L&amp;\cdot&amp; f&amp; + &amp;u \\
p\乘以 1 &amp; &amp; p\times 1 &amp;&amp;p\times k&amp; &amp; k\times 1 ＆amp; &amp; p\times 1
\end{array} 
\end{equation*
其中 $\mu$ 为平均值，$L$ 是因子载荷矩阵，$f\sim N(0, I_k)$ 是因子，$u\sim N(0,\Psi)$ 是误差，其中 $\Psi$ 是对角矩阵。
在一些论文中，作者使用交叉验证来确定因子的数量$k$。
我读了代码，发现代码做了如下操作：
(1) 固定一个整数$k$。
&lt; p&gt;(2) 将数据集 $S$ 拆分为 $10$ 折叠，$S_1,\cdots, S_{10}$.
(3) 对于每个 $S_i$，使用$S\backslash S_i$来训练因子分析模型$\mu_i, L_i, \Psi_i$ .
(4) 计算对数似然
\begin{equation*}
f(i,k)= \sum\nolimits_{x\in S_i}\log p(x|\mu_i, L_iL_i^T+\Psi_i)
\end{equation *
其中 $p(x|\mu,\Sigma)$ 是多元正态分布的概率密度函数，其均值为 $\mu$ 和协方差矩阵 $\Sigma$。
在 $f$ 的参数中，$i$ 是测试集的索引，$k$ 是因子的数量。
(5) 计算对数似然的平均值 $\bar f(k)= \frac{1}{10}\sum_{i=1}^{10} f(i,k)$.
乍一看，我怀疑代码可以工作，因为我的直觉告诉我，肯定更大的$k$ 有更大的$\bar f(k)$。
并且，如果 $L$ 是一个 $p\times k$ 矩阵，并且该矩阵与模型拟合得很好，那么$l&gt;k$ 因子也很好地拟合了模型，因为我们可以构造 $p\times l$ 因子载荷矩阵为 $(L&#39;,\mathbf 0)&#39;$。
但当我运行代码时，我发现可能会发生 $\bar f(k)$ 没有增加。
我想知道为什么$\bar f(k)$没有像我直觉所想的那样增加。
举一个数值例子，我将该代码用于一个数据集，其中包含 $156$ 次试验和每次试验 $21$ 个特征。
当 $k=2,3,4,5,6$，对应的$\bar f(k)$为分别
\begin{equation*}
1000\times ( -2.7518, -2.7509, -2.7485, -2.7506, -2.7502 )。
\end{equation*
$k=4$ 具有最大值 $\bar f(k)$ 。
更新：由于我有 21 个特征，如果 $k\le 14$，因子分析才有意义。
有一个图表因子和平均交叉验证对数似然。
（不是我给出的数值示例。我忘了我用哪个数据集给出数值示例。）
许多数据集都遵循这种模式。当因子数接近 $14$，
平均对数似然甚至下降并变低。
]]></description>
      <guid>https://stats.stackexchange.com/questions/660830/use-cross-validation-to-determine-number-of-factors-in-factor-analysis-why-the</guid>
      <pubDate>Fri, 31 Jan 2025 22:47:06 GMT</pubDate>
    </item>
    <item>
      <title>关于通过初始值（高值）和最终值（低值）进行归一化后查找方差的问题</title>
      <link>https://stats.stackexchange.com/questions/660817/question-about-finding-variance-after-normalizing-by-the-initial-high-and-fina</link>
      <description><![CDATA[我有来自许多“个人”的数据；从 0 到 1 的范围内，每个人最初的得分都很高，假设平均得分为 0.8，然后经过多次试验，他们在最后一次试验中的“最终”得分很低，假设平均得分为 0.2。休息一段时间后，他们的分数会有所恢复，假设平均得分为 0.6。
有了这三个平均值，我想计算恢复百分比（即，恢复得分恢复了初始得分和最终得分之间的差异的百分比）。使用这个标准化方程很容易计算：（平均恢复得分 - 平均最终得分/平均初始得分 - 平均最终得分）* 100。结果是 0.6-0.2/0.8-0.2 * 100 = 67%。
太好了。但是，我想用这个来计算统计数据，以比较不同组的数据，为此我需要这个百分比恢复值的方差（在本例中为 67%）。我不确定该怎么做。初始值、最终值和恢复值存在差异，所以我要使用误差传播吗？还是别的什么？如果能澄清这一点，我将不胜感激，我觉得这不是人们所做的非常罕见的规范化，但我还没有在任何地方找到这个特定问题的解决方案 :(]]></description>
      <guid>https://stats.stackexchange.com/questions/660817/question-about-finding-variance-after-normalizing-by-the-initial-high-and-fina</guid>
      <pubDate>Fri, 31 Jan 2025 17:14:41 GMT</pubDate>
    </item>
    <item>
      <title>McNemar 检验是否适合前后比较？</title>
      <link>https://stats.stackexchange.com/questions/660806/is-mcnemars-test-suitable-for-before-after-comparisons</link>
      <description><![CDATA[如果我理解正确的话，McNemar 检验通常用于测试两个二元结果之间的依赖关系。Agresti 在《分类数据分析》（我相信是 2002 年版）中给出了两个与环境相关的问题的示例，受试者同时回答这些问题。我提供了我自己的示例，其中的数字被夸大了，以便进行演示：
 Q2
Q1 同意 不同意
同意 945 5
不同意 10 40

在这里，我可以看到 McNemar 的检验是有道理的：大多数人同意这两个陈述，一些人同意其中一个而不同意另一个，还有少数人不同意两者。这听起来很合理。 p 值为 0.2，即两个问题的同意率没有显著差异。
但是，McNemar 检验也用于比较前后结果，如 Agresti 书中早期版本 (1990) 中的示例。因此，让我们使用上述数字，但将它们视为受试者对一个问题的回答，但在两个不同的时间点，$T_1$ 和 $T_2$。在这两个时间点，他们可能“同意”或“不同意”：
 T2
T1 同意 不同意
同意 945 5
不同意 10 40

由于计数的巨大不平衡，我发现 McNemar 检验中暗示的情景难以置信。我们想看看受试者是否会随着时间的推移而改变主意，结果发现一组（$T_1$ 的“同意者”）几乎没有变化（~0.5%），而“不同意者”则发生了很大变化：在 $T_1$ 不同意的受试者中，约有 20% 在 $T_2$ 同意。我认为这是一个“显著”的差异，但由于表中的数字与第一个示例中的数字相同，因此 McNemar 的检验仍然不显著。
在 $H_0$ 下，非对角线单元格中的绝对数字需要相似，而不是百分比。为了得到相似的数字，“T1 同意者”需要以与“不同意者”不同的概率改变主意，但这两个概率必须相关。换句话说，$H_0$ 意味着两组之间存在某种信息交换。或者，换句话说，“非对角线概率之间没有差异”假设可能意味着行概率之间存在相当大且非常精确的差异。这对我来说似乎非常不随机 - 与人们对零假设的期望完全相反。
或者我遗漏了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/660806/is-mcnemars-test-suitable-for-before-after-comparisons</guid>
      <pubDate>Fri, 31 Jan 2025 14:13:43 GMT</pubDate>
    </item>
    <item>
      <title>如果这些变量也包含在随机效应结构中，如何检验固定效应的显著性？</title>
      <link>https://stats.stackexchange.com/questions/660766/how-to-test-for-significance-of-fixed-effects-if-those-variables-are-also-includ</link>
      <description><![CDATA[我对混合效应模型有一个更一般/更实际的问题。我绝对不是这方面的专家，只是注意到一些似乎不对劲的地方。我想知道当固定效应也包含在模型的随机效应结构中时，是否可以测试该效应的显著性？
我将使用 R 中的 glmmTMB() 举例说明我的问题。假设您有一个模型，其中 Y 是二项分布（0 或 1）响应，X1 和 X2 是固定效应，g 是随机阻塞变量。探索了最佳随机效应结构（使用 AIC 和 LRT），发现对于 g 的每个级别，Y~X1 的随机截距和斜率都很重要。最终模型如下所示：final_model&lt;-glmmTMB(Y ~ X1 + X2 + (1+X1|g), family=binomial(link=&quot;logit&quot;), data=dat)
下一步是获取固定效应的检验统计量和 p 值（换句话说，得出结论，它们是否显著影响响应 Y）。我知道有多种方法可以做到这一点，从不好的（例如，Wald 检验）到更好的（例如，似然比检验），再到最好的（例如，模拟？）。假设您正在使用似然比检验（例如，使用 drop1() 命令）。由于这种方法涉及拟合简化（或嵌套）模型进行比较，这是否会产生一个问题，即其中一个简化模型不包括 X1，但 X1 仍然是随机效应结构的一部分？换句话说，这是否会导致某些简化模型被错误指定？我尝试在 R 中运行此类测试并得到了结果，但我不确定是否要相信它。如果简化模型确实指定错误，那么您如何测试固定效应的显著性？由于它是随机效应结构的一部分，因此它永远无法从模型中移除。
***更新：为了回应一些评论，以下是我尝试过的一些测试。
在这里，我尝试移除 g 的随机斜率。final_model 的 AIC 明显较低，LRT 的结果表明模型非常不同（p&lt;0.001），final_model 的对数似然最接近于零。总体而言，这表明随机斜率可以解释变化，应将其纳入模型中。
no_slope_model&lt;-glmmTMB(Y ~ X1 + X2 + (1|g), family=binomial(link=&quot;logit&quot;), data=dat)
anova(final_model, no_slope_model, test=&quot;LRT&quot;)
AICtab(final_model, no_slope_model) 

在这里，我尝试使用 drop1() 命令，该命令从我的最终模型中顺序删除固定效应，以获得测试统计数据和 p 值。此测试运行时没有错误或警告，并提供了我需要的统计数据（例如：对于 X1 chisq=42.6，p&lt;0.01）。
drop1(final_model, test=&quot;Chisq&quot;)

但是，如果我手动重新创建 drop1() 所做的事情，那么问题就会变得更加明显。见下文，简化模型不会将 X1 作为固定效应，但 Y~X1 的随机斜率适合 g 的每个级别。这种方法产生与上述完全相同的 Chisq 和 p 值。
no_X1_model&lt;-glmmTMB(Y ~ X2 + (1+X1|g), family=binomial(link=&quot;logit&quot;), data=dat)
anova(final_model, no_X1_model, test=&quot;LRT&quot;)
]]></description>
      <guid>https://stats.stackexchange.com/questions/660766/how-to-test-for-significance-of-fixed-effects-if-those-variables-are-also-includ</guid>
      <pubDate>Thu, 30 Jan 2025 16:20:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么必须估计线性回归模型中的参数值？</title>
      <link>https://stats.stackexchange.com/questions/660750/why-must-the-values-of-the-parameters-in-a-linear-regression-model-be-estimated</link>
      <description><![CDATA[我不明白为什么在线性回归模型中无法找到参数 α 和 β 的真实值。为什么它们总是需要估计？
我读到它与包含其他变量的误差项有关（$x_2,x_3 ...$），但我并不完全理解。]]></description>
      <guid>https://stats.stackexchange.com/questions/660750/why-must-the-values-of-the-parameters-in-a-linear-regression-model-be-estimated</guid>
      <pubDate>Thu, 30 Jan 2025 04:46:11 GMT</pubDate>
    </item>
    <item>
      <title>确定嘈杂的医疗保险索赔数据中的政策变化</title>
      <link>https://stats.stackexchange.com/questions/645345/determine-policy-changes-in-noisy-medicare-insurance-claim-data</link>
      <description><![CDATA[我希望这是提出这个问题的正确地方，我不会因为一个糟糕的问题而被指责，但我还是要问。
背景：
我有一个数据库，其中包含五年内提交的所有医疗保险索赔（非个人化）。
该数据库包含 86.8 亿条记录。每条记录都包括提交索赔的日期、索赔金额、索赔是否被批准、金额多少以及何时批准。每条记录还根据适用于该程序/设备的约 22,000 个代码（CPT 代码）之一对索赔进行分类。数据按这些 CPT 代码排序，并在 CPT 代码内按日期排序。
索赔是否受制于影响相关 CPT 代码的各种政策，而我无法访问这些政策。此外，对于我的问题来说，重要的是，适用于 CPT 代码的政策可能会随着时间的推移而发生变化。未涵盖的内容可能会变成涵盖的内容。涵盖的内容可能不再涵盖。甚至可能会出现反复，例如覆盖-未覆盖-覆盖。或者，策略可能在数据库期间永远不会改变。
最后，数据很嘈杂。虽然有些事情通常可以得到保障，但索赔仍可能因各种原因被拒绝，例如提交时间过晚、文件不齐全等。同样，有些事情通常可以得到保障，但索赔仍可能得到保障，例如同情护理。
问题：
有没有一种方法可以从数据本身检测出这些政策变化，这样我就可以这么说，对于给定的 CPT 代码，在某个时间段内批准的索赔百分比是 X，而在另一时间段内批准的索赔百分比是 Y，等等？
另一种说法可能是：面对这些嘈杂的数据，我如何找出合适的窗口和截止百分比来计算平均批准率，以便我可以比较两个时间序列？
需要说明的是，我有软件背景，并准备强行解决问题，即，如果有必要，尝试每一个可能的窗口。]]></description>
      <guid>https://stats.stackexchange.com/questions/645345/determine-policy-changes-in-noisy-medicare-insurance-claim-data</guid>
      <pubDate>Thu, 18 Apr 2024 22:19:23 GMT</pubDate>
    </item>
    <item>
      <title>具有二进制数据的转换器的嵌入/标记器</title>
      <link>https://stats.stackexchange.com/questions/627714/embeddings-tokenizers-for-a-transformer-with-binary-valued-data</link>
      <description><![CDATA[我正在尝试训练一个编码器-解码器转换器模型，以完成二进制值数据。每个输入基本上是一个长度为 n 的位串 $x = (x_1, \dots, x_n) \in \{0,1\}^n$，根据某个概率分布生成，该概率分布在训练期间已被屏蔽以隐藏一个位子集。
我对这种输入/输出数据应使用哪种嵌入感到困惑。在翻译的情况下，人们会将输入文本标记为 $N$ 个标记，然后从 $\{1, \dots, N\} \rightarrow \mathbb{R}^d$ 预训练一个嵌入，当 $N \gg d$ 时，这似乎是合理的。但是对于二进制数据，使用单个位作为标记意味着$N=2$，因此不清楚将如此小的标记字母表嵌入到某个高维空间中有什么优势。
在变压器中使用二进制值数据时，有哪些合理的选择？]]></description>
      <guid>https://stats.stackexchange.com/questions/627714/embeddings-tokenizers-for-a-transformer-with-binary-valued-data</guid>
      <pubDate>Sun, 01 Oct 2023 20:46:30 GMT</pubDate>
    </item>
    </channel>
</rss>