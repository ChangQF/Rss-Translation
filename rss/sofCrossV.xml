<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 19 Nov 2024 06:25:02 GMT</lastBuildDate>
    <item>
      <title>高斯分布关于其均值和方差的导数</title>
      <link>https://stats.stackexchange.com/questions/657480/derivative-of-a-gaussian-r-v-w-r-t-its-mean-and-variance</link>
      <description><![CDATA[我有个问题。我有一个随机变量 $$y \sim \mathcal{N}(\mu,\sigma^2)$$ 我有个问题：
i. 如何求出 $y$ 相对于 $\mu$ 和 $\sigma$ 的导数，即 $\frac{dy}{d\mu}$ 和 $\frac{dy}{d\sigma}$？
提前感谢您的帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/657480/derivative-of-a-gaussian-r-v-w-r-t-its-mean-and-variance</guid>
      <pubDate>Tue, 19 Nov 2024 06:05:37 GMT</pubDate>
    </item>
    <item>
      <title>当寻找空间自相关时，如何解释同一站点的重复？</title>
      <link>https://stats.stackexchange.com/questions/657479/how-do-i-account-for-replicates-at-the-same-sites-when-looking-for-spatial-autoc</link>
      <description><![CDATA[我试图检查空间模式/自相关对景观中分布不均匀的多个站点的响应变量的影响（可能使用 Moran&#39;s I），但我不确定如何解释这些站点中的每一个都有多个数据收集重复的事实。
我认为每个站点中的额外样本不能合理地被视为与其他年份同一站点的变量在统计上独立，并且数据集中最长时间序列也可能太短，无法从同步性而不是空间变化的角度分析数据。此外，一些站点不会出现在每年的数据收集中，“平均”值可能无法很好地描述这些站点。我也不认为我可以假设站点之间的关系每年都相同。
简而言之，我认为我可以对每个年份的站点之间的距离和相似性关系进行有效的统计测试，但我想不出一种方法将这些年份组合成一个更有意义的值。有人对我在这里可以做什么有什么建议吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657479/how-do-i-account-for-replicates-at-the-same-sites-when-looking-for-spatial-autoc</guid>
      <pubDate>Tue, 19 Nov 2024 02:46:55 GMT</pubDate>
    </item>
    <item>
      <title>如何处理对象检测数据集中的大量未标记数据</title>
      <link>https://stats.stackexchange.com/questions/657478/how-to-deal-with-many-unlabeled-data-in-an-object-detection-dataset</link>
      <description><![CDATA[我有一个大型的多类物体检测图像数据集。目标是使用 Yolo(v11) 模型在上述数据集上进行训练，以解决物体检测任务。
我的直觉告诉我，未标记的类实例的存在会降低我的 Yolo 模型的模型性能，因为这些未标记的模式可以理解为“训练误报”，因为如果在训练阶段发现未标记的物体，我的模型可能会受到惩罚。
在我的上下文中，我已经训练了一个视觉模型，它可以高精度地检测我想要检测的物体，但第二个模型只检测目标物体，除了它们的标签。
以我的问题为例，我的目标是在有猫和狗（没有其他动物）的图像上检测属于$\{Cat, Dog\}$类的物体，而我有一个非常好的模型，能够检测动物。
问题：

我想知道是否可以将数据集中的对象标记为通用类别，以便在 Yolo 训练阶段将其忽略？

很明显，我可以用模糊或裁剪来掩盖通用标记的对象，但我不确定此过程是否合适，或者在处理此类问题时是否有规范的过程选择。
提前致谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/657478/how-to-deal-with-many-unlabeled-data-in-an-object-detection-dataset</guid>
      <pubDate>Tue, 19 Nov 2024 02:00:47 GMT</pubDate>
    </item>
    <item>
      <title>学习进行参数引导</title>
      <link>https://stats.stackexchange.com/questions/657477/learning-to-do-the-parametric-bootstrap</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/657477/learning-to-do-the-parametric-bootstrap</guid>
      <pubDate>Tue, 19 Nov 2024 01:11:49 GMT</pubDate>
    </item>
    <item>
      <title>(mu,sigma) 正态分布的置信区域</title>
      <link>https://stats.stackexchange.com/questions/657475/confidence-regions-for-mu-sigma-normal-distribution</link>
      <description><![CDATA[在正态分布中，我想找到二维参数$(\mu,\sigma)$的置信区域。有没有比计算由 Bonferroni 校正的两个独立置信区间的笛卡尔积更好的方法？]]></description>
      <guid>https://stats.stackexchange.com/questions/657475/confidence-regions-for-mu-sigma-normal-distribution</guid>
      <pubDate>Mon, 18 Nov 2024 23:04:39 GMT</pubDate>
    </item>
    <item>
      <title>具有高 auc 值的不同模型</title>
      <link>https://stats.stackexchange.com/questions/657474/different-models-with-high-auc-values</link>
      <description><![CDATA[我正在尝试评估两个不同的逻辑回归模型：一个是通过前向选择获得的，另一个是通过套索正则化获得的。我使用嵌套交叉验证进行训练和测试，第一个模型的准确率为 0.9578，第二个模型的准确率为 0.9737。我认为这些值有点高，所以我对两个模型进行了 auc 评分（使用嵌套 cv），得到了 0.9979 和 0.9965，这些值也非常高。我还使用了 brier 评分，得到了 0.021 和 0.424，这对我来说似乎很奇怪，因为两个模型的表现基本相同。我最终绘制了模型的校准图，试图理解一些东西，但我得到的模型的平均绝对误差为 0.001 和 0.005。我做错了什么，或者我在这里遗漏了什么？
以下是代码：
第一个模型（前向选择）：
# 外循环分区
outer_cv &lt;- createFolds(y, k = outer_folds, list = TRUE)

# 保存结果
results &lt;- c()

# 外循环
for (i in 1:outer_folds) {
# 外循环训练和测试
test_idx &lt;- outer_cv[[i]]
train_idx &lt;- setdiff(1:nrow(data), test_idx)

train_data &lt;- data[train_idx, ]
test_data &lt;- data[test_idx, ]

# 内循环：前向选择
model_null &lt;- glm(diagnosis ~ 1, data = train_data, family = binomial())
model_full &lt;- glm(diagnosis ~ ., data = train_data, family = binomial())

# 变量选择
model_forward &lt;- stepAIC(model_null, 
scope = list(lower = model_null, upper = model_full), 
direction = &quot;forward&quot;, 
trace = FALSE)

# 测试集上的预测
predictions &lt;- predict(model_forward, newdata = test_data, type = &quot;response&quot;)
predict_classes &lt;- ifelse(predictions &gt; 0.5, 1, 0)

# 评估准确率
acc &lt;- mean(predicted_classes == test_data$diagnosis)
results &lt;- c(results, acc)
}

# 准确率 平均值
mean_accuracy &lt;- mean(results)
print(paste(&quot;Nested CV Accuracy with Forward Selection:&quot;, mean_accuracy))

第二个模型 (LASSO):
# 外循环分区
outer_cv &lt;- createFolds(y, k = outer_folds, list = TRUE)

# 保存结果
results &lt;- c()

# 外循环
for (i in 1:outer_folds) {
# 训练和测试
test_idx &lt;- outer_cv[[i]]
train_idx &lt;- setdiff(1:length(y), test_idx)

x_train &lt;- X[train_idx, ]
y_train &lt;- y[train_idx]
x_test &lt;- X[test_idx, ]
y_test &lt;- y[test_idx]

# 内部循环：选择最佳 lambda
inner_cv &lt;- cv.glmnet(x_train, y_train, alpha = 1, family = &quot;binomial&quot;, nfolds = inner_folds)
best_lambda &lt;- inner_cv$lambda.min

# 使用最佳 lambda 进行训练
lasso_model2 &lt;- glmnet(x_train, y_train, alpha = 1, family = &quot;binomial&quot;, lambda = best_lambda)

# 测试集上的预测
predictions &lt;-预测（lasso_model2，x_test，类型 = “响应”）
预测类 &lt;- ifelse（预测 &gt; 0.5, 1, 0)

# 评估准确率
acc &lt;- mean(predicted_classes == y_test)
results &lt;- c(results, acc)
}

# 准确率平均值
mean_accuracy &lt;- mean(results)
print(paste(&quot;Nested CV Accuracy:&quot;, mean_accuracy))

AUC 和 Brier:
diagnosis_numeric &lt;- as.numeric(data$diagnosis) - 1 # 从因子转换为数字
# 外循环分区
outer_cv &lt;- createFolds(y, k = outer_folds, list = TRUE)

# 保存结果
results &lt;- c()
results2 &lt;- c()

# 外循环
for (i in 1:outer_folds) {
# 训练和测试
test_idx &lt;- outer_cv[[i]]
train_idx &lt;- setdiff(1:nrow(data), test_idx)

train_data &lt;- data[train_idx, ]
test_data &lt;- data[test_idx, ]

# 内部循环：评估两个模型的 auc 分数
roc1 &lt;- roc(test_data$diagnosis, predict(model_forward, test_data, type = &quot;response&quot;))
roc2 &lt;- roc(test_data$diagnosis, predict(final_model, test_data, type = &quot;response&quot;))

results &lt;- c(results, auc(roc1))
results2 &lt;- c(results2, auc(roc2))
}

# auc 的平均值分数
mean_roc1 &lt;- 平均值（结果）
print(paste(&quot;嵌套 CV ROC 1:&quot;, mean_roc1))

mean_roc2 &lt;- 平均值（结果2）
print(paste(&quot;嵌套 CV ROC 2:&quot;, mean_roc2))

# 评估 Brier 分数
pred.prob &lt;- 预测（final_model,type=&#39;response&#39;）
brierScore &lt;- 平均值（（pred.prob-diagnosis_numeric）^2）
brierScore

pred.prob2 &lt;- 预测（model_forward,type=&#39;response&#39;）
brierScore2 &lt;- 平均值（（pred.prob2-diagnosis_numeric）^2）
brierScore2

校准图：
plot(rms::calibrate(final_model, B = 400))
plot(rms::calibrate(model_forward, B = 400))
]]></description>
      <guid>https://stats.stackexchange.com/questions/657474/different-models-with-high-auc-values</guid>
      <pubDate>Mon, 18 Nov 2024 22:40:22 GMT</pubDate>
    </item>
    <item>
      <title>固定效应水平内的观测值具有相同的随机效应水平</title>
      <link>https://stats.stackexchange.com/questions/657473/observations-within-a-level-of-fixed-effect-have-the-same-level-of-random-effect</link>
      <description><![CDATA[我的研究旨在比较人类与模型在数学任务上的表现。
设置如下：
共有 700 项独特的作业。每项作业招募 6 名随机人类参与者。总共约有 400 人参与。每位参与者完成大约 10 项作业。
对于每项作业，我还从 2 个模型中获得了绩效结果。
因此，对于每项作业，我有 6 名人类参与者的绩效分数和 2 个模型的绩效分数。
我使用混合线性模型来比较人类和模型的表现。这是 R 中的公式：
data.model &lt;- lmer(score ~ type + (1 | contestant_id) + (1 | assignment_id), data = data_1)
type：表示观察结果来自人类（human）还是模型（model）。
participant_id：每个人类参与者的唯一标识。对于模型，我将 contestant_id 设置为“model”。
assignment_id：标识任务。
我的问题是，模型的随机效应 contestant_id 只有一个级别，这有问题吗？如果是这样，我该如何调整模型以适当比较人类和模型的表现？]]></description>
      <guid>https://stats.stackexchange.com/questions/657473/observations-within-a-level-of-fixed-effect-have-the-same-level-of-random-effect</guid>
      <pubDate>Mon, 18 Nov 2024 22:33:15 GMT</pubDate>
    </item>
    <item>
      <title>如何解释 R 中 logit 模型 stargazer 中的系数方向？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/657472/how-to-interpret-coefficient-direction-in-logit-models-stargazer-in-r</link>
      <description><![CDATA[解释民主党和共和党的系数。请记住，与线性回归不同，我们只能从系数中知道关系的方向，而不是幅度。因此，解释这两个变量与因变量之间的关系的方向，同时记住排除的类别是独立的]]></description>
      <guid>https://stats.stackexchange.com/questions/657472/how-to-interpret-coefficient-direction-in-logit-models-stargazer-in-r</guid>
      <pubDate>Mon, 18 Nov 2024 22:25:25 GMT</pubDate>
    </item>
    <item>
      <title>多变量（双向/双因素）方差分析事后检验术语区分和显著性计算方法</title>
      <link>https://stats.stackexchange.com/questions/657470/multivariate-two-way-two-factor-anova-post-hoc-tests-nomenclature-distinction</link>
      <description><![CDATA[我希望对用于确定多元方差分析事后分析过程中相互作用原因的各种方法的命名法（名称）进行区分，并得到一些紧迫问题的答案。我选择了 R 和 SPSS 作为示例，但在我看来，这无关紧要。
考虑一个简单的 3x2 方差分析示例，三种药物（药物 A、药物 B 和安慰剂）和两个性别组（M/F）。我们获得了显著的相互作用效应，我们可以继续进行事后分析。
1)
首先，我们可以考虑将“一切与一切”进行比较- 我们有（某种程度上）6个子组，我们只是将它们相互比较，创建一个类似这样的矩阵（值是样本 p 值）：
 药物 A M 药物 B M 安慰剂 M 药物 A F 药物 B F 安慰剂 F
药物 A M 
药物 B M 0,321 
安慰剂 M 0,251 0,251 
药物 A F 0,181 0,181 0,181 
药物 B F 0,111 0,111 0,111 0,111 
安慰剂 F 0,041 0,041 0,041 0,041 0,041 

我不知道这种方法叫什么，但可以使用 Statistica 软件来完成，或者使用一些分析R.
2)
我们可以确定一个分组变量与另一个变量组之间的差异：
药物 A M vs 药物 A F 0,321
药物 B M vs 药物 B F 0,251
安慰剂 M vs 安慰剂 F 0,181

和
药物 A F vs 药物 B F 0,321
药物 B F vs 安慰剂 F 0,251
安慰剂 F vs 药物 A F 0,181
药物 A M vs 药物 B M 0,111
药物 B M vs 安慰剂 M 0,041
安慰剂 M vs 药物 A M 0,251




我们可以简单地接受与我们的理论一致的单一比较，合并组，等等。
药物 A M + 安慰剂 F vs 药物 A F 0,321
药物 B M vs 药物 B F + 安慰剂 M 0,251

在我看来，这三种方法给出了不同的结果，更确切地说，使用 Bonferroni 方法给出了不同的统计显著性水平。这并不奇怪，如果第一种方法我考虑了 15 个比较子组，第二种方法考虑了 3 或 6 个，第三种方法考虑了 2 个。根据 Bonferroni 校正规则（将 p 结果乘以比较次数），这给出了完全不同的显著性水平，第一种方法是最保守的。我在 R、SPSS 和 Statistica 中对其进行了测试，并手动计算了我能计算的结果。
因此，我有一系列问题，主要是关于这些技术的名称。

这三组技术的正确名称是什么？是否有任何科学来源可以引用这种命名？我记得在统计学课程中，第一种方法没有名字，第二种方法被称为“简单效应”，第三种方法的不同变体被称为“对比”——但我可能错了。
这些方法中的任何一种都可以被认为是普遍正确或不正确的吗？如果它取决于数据的类型和解释，那么是哪种？如果这取决于研究人员的理论，那么如何证明它大致正确？
在什么条件下，第一种方法会比第二种方法更清楚地展示交互作用？
由于这三种方法产生不同的重要性水平，如何证明选择其中一种而不是另一种？许多研究报告并解释了示例 2 中的比较，因为它们似乎回答了有关交互作用性质的问题，但这是正确的方法吗？
SPSS、Jamovi 和许多 R 软件包默认使用示例 2 中的这些比较，Statistica 默认使用示例 1。这些程序的理念有什么不同？

我不期望所有答案，任何建议对我来说都是有价值的。]]></description>
      <guid>https://stats.stackexchange.com/questions/657470/multivariate-two-way-two-factor-anova-post-hoc-tests-nomenclature-distinction</guid>
      <pubDate>Mon, 18 Nov 2024 21:32:09 GMT</pubDate>
    </item>
    <item>
      <title>功效分析能否计算 n 来检测总体平均值随时间的变化？</title>
      <link>https://stats.stackexchange.com/questions/657469/can-a-power-analysis-calculate-n-to-detect-changes-in-a-population-mean-over-tim</link>
      <description><![CDATA[将功效分析解释为：计算检测总体平均值从一个时间点到另一个时间点的变化（%）所需的最小样本量的一种可能方法（？）是否不正确。例如，在未来的调查中，我需要多大的样本量才能检测到总体平均值的 30% 变化？
以 Cohen 的 D 效应量公式作为功效分析的示例；如果“M”是样本平均值，“σ”是样本标准差（共同估计当前总体平均值），“µ”可以被视为未来总体平均值（当前平均值的 30% 变化，没有样本误差）吗？这是针对“前后”的重复测量设计。
那么得出这样的结论是错误的吗：
# 效应大小，Cohen&#39;s D：d=(M-μ)/σ
# M=1，σ=0.5，µ=0.7
# 0.6 = (1-0.7)/0.5

&gt; pwr.t.test(d = 0.6, 
+ power = 0.8000, 
+ n = NULL, 
+ type = &quot;paired&quot;,
+ alternative = &quot;two.sided&quot;,
+ sig.level = 0.05)

配对 t 检验功效计算 

n = 23.79452
d = 0.6
sig.level = 0.05
power = 0.8
alternative = two.sided

注意：n 是 *对* 的数量

结论：样本量为 24 足以在未来调查中检测到当前人口平均值的 +/-30% 变化（双尾检验），概率为 80%，alpha=0.05。
现在对配对数据定义功效分析是否不正确方式？
我猜这是不正确的措辞。功效分析不是为了检测从一个时间点到另一个时间点的变化，而是为了通过抽样来测试估计的总体参数的准确性，不是吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657469/can-a-power-analysis-calculate-n-to-detect-changes-in-a-population-mean-over-tim</guid>
      <pubDate>Mon, 18 Nov 2024 21:16:56 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯模型平均的误解</title>
      <link>https://stats.stackexchange.com/questions/657460/misunderstanding-in-bayesian-model-averaging</link>
      <description><![CDATA[我第一次尝试理解和实施贝叶斯模型平均法，以便对几个竞争模型进行加权混合。我读到的许多资料都提出了类似的我不理解的说法。我希望这里有人能解答我的困惑。以下是我对基本思想的理解：

给定竞争模型 $M_1,\ldots,M_K$ 和一些训练数据 $y_{\text{train}}$，加权模型平均值为
$$p(y)=\sum_{k=1}^Kp(y|M_k)p(M_k|y_{\text{train}}),\tag{1}$$
其中 $p(M_k|y_{\text{train}})$ 是给定训练数据后验概率 $M_k$ 模型正确。后验概率加起来为 1，因此可以将其视为权重。 模型 $M_k$ 的后验概率由以下公式给出
$$p(M_k|y_{\text{train}})=\frac{p(y_{\text{train}}|M_k)p(M_k)}{\sum_{l=1}^Kp(y_{\text{train}}|M_l)p(M_l)}.\tag{2}$$

我不清楚为什么后验概率加起来应该等于 $1$。很明显，$(2)$ 右边的表达式在对 $k=1,\ldots,K$ 求和时等于 $1$，但我不清楚为什么这应该是后验概率。它看起来很像贝叶斯规则的应用：
$$p(M_k|y_{\text{train}})=\frac{p(y_{\text{train}}|M_k)p(M_k)}{p(y_{\text{train}})},\tag{3}$$
如果我们有
$$p(y_{\text{train}})=\sum_{l=1}^Kp(y_{\text{train}}|M_l)p(M_l).$$，这将得到$(2)$

这看起来很像全概率定律的应用，只是$M_k$不必相互排斥。所以我并不确信 $(3)$ 成立。
作为一个极端的例子，我们可以通过设置 $M_{K+1}=M_1$ 来将另一个模型添加到组合中。现在，如果 $p(M_1|y_{\text{train}})&gt;0$，我们就不能同时得到 $\sum_{k=1}^K$ 和 $\sum_{k=1}^{K+1}$ 的总和，加起来等于 $1$。
我误解了什么？对于 $(2)$ 有不同的论据吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657460/misunderstanding-in-bayesian-model-averaging</guid>
      <pubDate>Mon, 18 Nov 2024 17:18:44 GMT</pubDate>
    </item>
    <item>
      <title>广义线性模型真的需要假设误差的分布吗？[重复]</title>
      <link>https://stats.stackexchange.com/questions/657423/do-generalized-linear-models-really-need-to-assume-the-distribution-of-errors</link>
      <description><![CDATA[GLM 具有以下形式：
$$
g(\mathbb E[Y\mathop | X])=X^T\beta
$$
其中 $g$ 是链接函数。我们可以将矩条件写为
$$
Y=g^{-1}(X^T\beta)+\varepsilon,\\
\mathbb E[X^T\varepsilon]=0,
$$
其样本模拟为我们提供了 GMM 估计量：
$$
\frac 1n\sum_{i=1}^nX_i^T[Y_i-g^{-1}(X_i^T\hat\beta_{\text{GMM}})]=0。
$$
有一点我不太明白：很多在线资料都说 GLM 需要指定误差项的分布（例如高斯、泊松、二项式等），但这真的是 GLM 有效所需的假设吗？当指定函数 $g$ 时，我们只需要矩条件即可使 GMM 估计量有效（当模型正确指定时，即 $\mathbb E[\varepsilon \mathop |X]=0$，即使模型指定错误，我们仍然会得到一个近似于“真实”$\mathbb E[Y\mathop | X]$ 的“最佳参数预测器”）。那么为什么在 GLM 中经常假设误差分布呢？]]></description>
      <guid>https://stats.stackexchange.com/questions/657423/do-generalized-linear-models-really-need-to-assume-the-distribution-of-errors</guid>
      <pubDate>Mon, 18 Nov 2024 03:54:18 GMT</pubDate>
    </item>
    <item>
      <title>估计具有未知偏差 $p \in [0,1]$ 的硬币的偏差，近似正态分布</title>
      <link>https://stats.stackexchange.com/questions/657417/estimating-bias-of-a-coin-with-an-unknown-bias-p-in-0-1-approximately-norm</link>
      <description><![CDATA[我把这个发布在 MSE 上，但有人建议把它发布在这里。假设你得到一枚硬币，它正面朝上的概率为 $p$。你不知道 $p$，但你知道 $p$ 是从 $[0,1]$ 中的某个截断正态分布中得出的，中心为 $1/2$。给定 $N$ 次翻转，你能对 $p$ 做出什么判断，你有多大的信心？
由于我的问题很模糊，我不是在寻找一个具体的答案，而是在寻找一种解决问题的方法。我对$p$从$[0,1]$均匀抽取的情况比较熟悉，但我不确定如何将推理应用到这种情况。]]></description>
      <guid>https://stats.stackexchange.com/questions/657417/estimating-bias-of-a-coin-with-an-unknown-bias-p-in-0-1-approximately-norm</guid>
      <pubDate>Mon, 18 Nov 2024 00:10:22 GMT</pubDate>
    </item>
    <item>
      <title>预测模型和机器学习中的信噪比</title>
      <link>https://stats.stackexchange.com/questions/657395/signal-to-noise-ratio-in-predictive-modeling-and-machine-learning</link>
      <description><![CDATA[对此问题的有趣评论涉及信噪比如何影响预测能力。更明确地说，信噪比如何影响预测的准确性？例如，信噪比究竟是什么意思？它对预测建模意味着什么？
编辑
在看到对链接答案的进一步评论后，我想知道（人口级）相互信息是否是思考这个问题的正确方法。那是信号，结果的熵是噪音吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657395/signal-to-noise-ratio-in-predictive-modeling-and-machine-learning</guid>
      <pubDate>Sun, 17 Nov 2024 13:42:20 GMT</pubDate>
    </item>
    <item>
      <title>X-帕累托分布</title>
      <link>https://stats.stackexchange.com/questions/657358/x-pareto-distribution</link>
      <description><![CDATA[我有一个作业，其中我必须证明威布尔帕累托属于指数族，然后是项$a(x)$的均值和方差。
$g(x)=\frac{\beta c}{x}\{\beta \log (\frac{x}{\theta})\}^{\lambda-1}\exp \{-(\beta \log (\frac{x}{\theta})^\lambda\}$
为此，我使用了Dobson，因为$g(y;\tau)=\exp\{a(x)b(\tau)+c(\tau)d(x)\}$并得到$a(x)=\log(x)-\log (\lambda)$，$c(\tau) =-(\alpha \log (\frac{x}{\lambda}))$, $b(\tau) = (c-1)\log (\alpha)$ 和 $d(x)= -\log (x)$。请检查这是否正确。]]></description>
      <guid>https://stats.stackexchange.com/questions/657358/x-pareto-distribution</guid>
      <pubDate>Sat, 16 Nov 2024 13:19:44 GMT</pubDate>
    </item>
    </channel>
</rss>