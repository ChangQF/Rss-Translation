<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Thu, 30 May 2024 18:20:13 GMT</lastBuildDate>
    <item>
      <title>尽管类别不平衡，Logit 模型仍未预测任何 < 0 的值</title>
      <link>https://stats.stackexchange.com/questions/648302/logit-model-not-predicting-any-values-0-despite-class-imbalance</link>
      <description><![CDATA[我正在建立一个逻辑回归模型来识别潜在的渠道因素，这些因素可以预测患者是否会在疾病的特定阶段开始使用两种抗糖尿病药物中的一种。我的数据集非常不平衡，97.5% (n = 15292) 的患者开始属于 B 类，只有 2.5% (n = 394) 的患者开始属于 A 类。
logit_model &lt;- glm(class_A ~ centered_age + female + creat*days_to_first_fill + 
cause + centered_bmi + dgf + LiverDisease + 
fill_era, 
family = binomial(link = &quot;logit&quot;),
data = dt)

在拟合逻辑回归模型并使用 expit 函数将预测的对数几率转换为概率后，我发现所有预测概率均不低于 0.5（即所有对数几率均不小于 0）。预测概率的范围也很窄，绝大多数患者的概率约为 50%。
summary(dt$pred_prob)
最小值 第 1 组 中位数 平均值 第 3 组 最大值 
0.5002 0.5014 0.5029 0.5063 0.5054 0.6724

鉴于数据的不平衡性质，我预计预测概率将反映观察到的类别分布，胰岛素启动类别的大多数概率低于 0.5。然而，该模型似乎为大多数患者分配了 0.5 左右的概率，表明这两个类别之间的区分度较差。几乎感觉模型的拟合值是概率而不是对数几率。
summary(logit_model$fitted.values)
最小值 第 1 组中位数 平均值 第 3 区 最大值 
0.0007501 0.0057085 0.0115473 0.0251822 0.0214111 0.7189886 

作为参考，这是我的模型输出：
系数：
估计标准误差 z 值 Pr(&gt;|z|) 
(截距) -5.56850007 0.17583038 -31.670 &lt; 0.0000000000000002 ***
centered_age 0.00007974 0.00480505 0.017 0.986759 
female女性 0.04008379 0.11260580 0.356 0.721866 
creat -0.08360708 0.03568126 -2.343 0.019121 * 
days_to_first_fill 0.01944154 0.00120843 16.088 &lt; 0.0000000000000002 ***
原因肾小球肾炎 1.51663048 0.19201817 7.898 0.0000000000000283 ***
原因高血压性肾硬化 1.30805147 0.14719779 8.886 &lt; 0.000000000000002 ***
原因其他 1.08617629 0.14338594 7.575 0.00000000000003586 ***
centered_bmi -0.02047943 0.01071770 -1.911 0.056030 . 
dgf 延迟移植物功能 -0.54979993 0.14787923 -3.718 0.000201 ***
LiverDisease 中度至重度肝病 -0.43294637 0.24443026 -1.771 0.076520 . 
fill_era 首次填充 2013-2020 1.10826539 0.14895235 7.440 0.00000000000010038 ***
centered_mrcreat:days_to_first_fill 0.00101324 0.00036170 2.801 0.005089 ** 
---
Signif.代码：0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

（二项式系列的分散参数取为 1）

零偏差：15645 个自由度上的 3679.1

残差偏差：15633 个自由度上的 2975.4

AIC：3001.4

Fisher 评分迭代次数：7

有人遇到过类似的事情吗？我不确定如何解决这个问题。]]></description>
      <guid>https://stats.stackexchange.com/questions/648302/logit-model-not-predicting-any-values-0-despite-class-imbalance</guid>
      <pubDate>Thu, 30 May 2024 18:17:09 GMT</pubDate>
    </item>
    <item>
      <title>k 倍时间序列交叉验证的某些分割中高度不平衡的测试数据如何影响模型性能？</title>
      <link>https://stats.stackexchange.com/questions/648298/how-does-highly-imbalanced-test-data-in-certain-splits-of-k-fold-time-series-cro</link>
      <description><![CDATA[我正在研究一个时间序列分类 (TSC) 问题，使用 k 倍时间序列交叉验证 (TSCV) 来评估我的模型的性能。我每个分割的训练数据都相当平衡，类别分布范围从 40% 到 60%，少数情况为 30/70%。但是，我注意到某些分割中的测试数据高度不平衡。例如，在 6 倍 TSCV 设置中，分割 2、5 和 6 具有高度不平衡的测试集。我担心这种不平衡会如何影响我的模型评估。
以下是我的设置的简要概述：
训练和测试数据中的类别分布：
训练数据：平衡（40-60% 的类别分布）。测试数据：在某些分割中高度不平衡（例如，分割 2、5 和 6）。我使用标准指标来评估性能；准确度、精确度、召回率、F1 分数、AUC-ROC 分数。
我使用前向时间序列交叉验证方法来分割数据。我使用 Python 和 pandas 以及 scikit-learn 分析分割过程中的类分布和模型性能。我使用 LSTM-FCN、InceptionTime、ResNet、TCN 和 ROCKET 等模型进行 TSC 分析。在此示例中，我处理时间序列数据，其中每个时间步骤代表一个数据点。目标变量 Fault_Status 指示每个时间步骤中是否存在故障 (1) 或不存在故障 (0)。
下面是一个与我的问题相关的虚拟示例，它不是实际的数据集。它只是传达观点的一种手段。
具体问题：

对实验的影响：

某些分组（例如分组 2、5 和 6）中的不平衡测试数据会如何影响我的 TSC 模型的整体评估？

缓解策略：

我可以使用哪些策略来缓解这种不平衡的负面影响？
分层拆分是否适合 TSC，我该如何实现它？
任何见解或建议都将不胜感激！

import numpy as np
from sklearn.model_selection import TimeSeriesSplit

def walk_forward_tscv(X, y, n_splits=7):
tscv = TimeSeriesSplit(n_splits=n_splits)
splits = []

for i, (train_index, test_index) in enumerate(tscv.split(X)):
X_train, X_test = X[train_index], X[test_index]
y_train, y_test = y[train_index], y[test_index]

unique, counts_train = np.unique(y_train, return_counts=True)
train_distribution = dict(zip(unique, counts_train))

unique, counts_test = np.unique(y_test, return_counts=True)
test_distribution = dict(zip(unique, counts_test))

splits.append((train_index, test_index))

print(f&quot;拆分 {i+1}：&quot;)
print(f&quot;训练集类别分布：{train_distribution}&quot;)
print(f&quot; 测试集类别分布：{test_distribution}&quot;)
print(&quot;-&quot; * 50)

返回分割

]]></description>
      <guid>https://stats.stackexchange.com/questions/648298/how-does-highly-imbalanced-test-data-in-certain-splits-of-k-fold-time-series-cro</guid>
      <pubDate>Thu, 30 May 2024 16:52:39 GMT</pubDate>
    </item>
    <item>
      <title>“独立关联”和“独立预测因子”之间有什么区别？</title>
      <link>https://stats.stackexchange.com/questions/648296/is-there-any-difference-between-an-independent-association-and-to-be-independ</link>
      <description><![CDATA[我想知道作为变量或特定事件的独立预测因子与独立相关之间是否存在差异，即激素是总胆固醇水平的独立预测因子，还是激素与总胆固醇不独立相关。
因为在我看来，从语义角度（甚至临床角度）来看，这两者是相同的，或者至少是两种方式。但魔鬼在细节中，如果从统计学上讲不一样，那么从医学或流行病学上讲也不应该一样。
我已经浏览过线程]]></description>
      <guid>https://stats.stackexchange.com/questions/648296/is-there-any-difference-between-an-independent-association-and-to-be-independ</guid>
      <pubDate>Thu, 30 May 2024 16:06:35 GMT</pubDate>
    </item>
    <item>
      <title>交错式 diff-in-diff 与 堆叠式 diff-in-diff</title>
      <link>https://stats.stackexchange.com/questions/648291/staggered-diff-in-diff-vs-stacked-diff-in-diff</link>
      <description><![CDATA[只是想更好地理解交错式差分和堆​​叠式差分之间的技术差异到底是什么。我知道 TWFE 交错式差分有其自身的问题，而且几位作者已经想出了更准确的 ATT 估计方法。但我也知道有些作者更依赖堆叠式差分设计，我想更好地理解其中的区别。我的先验是堆叠式差分不允许将已处理过的单元用作对照，而交错式差分可以？但不确定。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/648291/staggered-diff-in-diff-vs-stacked-diff-in-diff</guid>
      <pubDate>Thu, 30 May 2024 14:16:06 GMT</pubDate>
    </item>
    <item>
      <title>运行逻辑回归（LMM）时，R 会抛出缺陷警告</title>
      <link>https://stats.stackexchange.com/questions/648293/r-throws-deficiency-warning-when-running-a-logistic-regression-lmm</link>
      <description><![CDATA[我从未在 R 中运行过逻辑回归分析（线性混合效应模型），但它似乎是一种合理的方法来回答与条件 NB 相比，条件 BB 在多大程度上影响元音 i 和 e 的辨别能力的问题。
因变量有两个级别（I 或 E），有六个预测变量。
运行逻辑回归时，R 会抛出警告消息。
对于第一个模型，我有：
glmer1 &lt;- glmer(response_converted ~ (1|subject_nr) + condition + 
continuum_step + block_order + 
vowel_carrier +stimulus, 
data = data, family = binomial)

我在 R 中收到警告：

固定效应模型矩阵是秩不足的，因此删除 3 列/系数

因此，我运行了
allFit(glmer1)

结果是两个优化器都失败了。因此，我逐个删除了固定效应（第一个“block_order”，第二个“vowel_carrier”，第三个“stimulus”），直到没有收到警告。最终模型具有以下变量：
glmer4 &lt;- glmer(response_converted ~ (1|subject_nr) + condition + continuum_step, data = data, family = binomial)

我添加了“stimulus”作为模型 5 中的随机效应：
glmer5 &lt;- glmer(response_converted ~ (1|subject_nr) + (1|stimulus) + condition + continuum_step, data = data, family = binomial)

我使用 anova() 函数比较了 glmer4 和 glmer5：
anova(glmer4, glmer5, test = &quot;Chisq&quot;)

glmer5 似乎更合适。
这个过程有意义吗？有什么建议吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648293/r-throws-deficiency-warning-when-running-a-logistic-regression-lmm</guid>
      <pubDate>Thu, 30 May 2024 14:13:02 GMT</pubDate>
    </item>
    <item>
      <title>我们有敏感度-特异性空间（ROC 曲线）和精确度-召回率空间（PR 曲线，$F_1$ 分数）。PPV-NPV 空间方面做了哪些工作？</title>
      <link>https://stats.stackexchange.com/questions/648290/we-have-sensitivity-specificity-space-roc-curves-and-precision-recall-space-p</link>
      <description><![CDATA[接收者操作特性 (ROC) 曲线显示了敏感度和特异性之间的平衡：您在检测类别 $1$（敏感度）方面有多好，同时不会将类别 $0$ 错误地识别为类别 $1$（特异性）。这样，我们可以相信预测的 $1$ 确实是 $1$。
精确召回率 (PR) 曲线可以说通过使用精确度直接衡量怀疑程度来改善这一点，精确度是特异性的函数（以及分类器性能的其他方面）。召回率/敏感度衡量的是在一个观测值为 $1$ 的情况下检测到 $1$ 的概率，而准确率则是阳性预测值 (PPV)：你所声称的 $1$ 确实属于 $1$ 类别的概率。此外，$F_1$ 和更一般的 $F_{\beta}$ 分数同时考虑了精确度和召回率。
负预测值 (NPV，不要与金融中的净现值混淆) 与敏感度/召回率的关系，就像正预测值与特异性的关系一样，因为 NPV 给出了您声称的 $0$ 确实属于 $0$ 类别的概率。与 PPV 一样，NPV 通过调节已知预测来估计未知概率，从而测量信息的正向流动。比较 PPV 与 NPV 似乎很有价值：对于 $0$ 类别声明的给定可信度水平，我们对 $1$ 类别的声明的可信度有多高。这可能并不总是我们想知道的，但肯定是可能的。
有哪些工作明确处理 PPV-NPV 空间，无论是 PPV-NPV 曲线还是结合 PPV 和 NPV 的分类器指标，类似于 $F_{\beta}$ 结合 PPV/精度和召回率/灵敏度的方式？当然，我们可以将 ROC 曲线与流行度一起转换为 PPV-NPV 曲线，但这并不明确使用 PPV 和 NPV。]]></description>
      <guid>https://stats.stackexchange.com/questions/648290/we-have-sensitivity-specificity-space-roc-curves-and-precision-recall-space-p</guid>
      <pubDate>Thu, 30 May 2024 14:06:04 GMT</pubDate>
    </item>
    <item>
      <title>如何确定 Gamma-Poisson 模型的 Gamma 分布的参数？</title>
      <link>https://stats.stackexchange.com/questions/648289/how-to-decide-the-parameters-of-a-gamma-distribution-for-a-gamma-poisson-model</link>
      <description><![CDATA[在贝叶斯推断中，Gamma-Poisson 模型通常使用泊松分布的 $\lambda$ 参数上的 Gamma($\alpha$,$\beta$) 先验。
是否有任何规则可以对 Gamma 先验的这些 $\alpha$ 和 $\beta$ 参数设置适当的值？
先验通常在看到数据之前设置；那么，我们需要什么信息才能知道应该给予 $\alpha$ 和 $\beta$ 的值？
我们是否需要至少大致了解我们计划的泊松实验中应该观察到的平均 $\lambda$ 值是多少？
或者我们是否需要至少大致了解我们计划的泊松实验中应该观察到的最大值是多少？
或者任何其他类型的信息，以免立即盲目地给出 $\alpha$ 和 $\beta$ 无意义的值？
任何回报都值得赞赏。]]></description>
      <guid>https://stats.stackexchange.com/questions/648289/how-to-decide-the-parameters-of-a-gamma-distribution-for-a-gamma-poisson-model</guid>
      <pubDate>Thu, 30 May 2024 14:01:29 GMT</pubDate>
    </item>
    <item>
      <title>训练集和验证集之间的性能显著下降</title>
      <link>https://stats.stackexchange.com/questions/648288/significant-performance-drop-between-train-and-validation-set</link>
      <description><![CDATA[我正在尝试使用 Lgbm 和 RandomForest 进行分类，并且我观察到了同样的问题。我使用各种元参数来防止过度拟合，例如 max_depth、num_trees（对于 Lgbm 保持较小）、reg_alpha 和 reg_lambda（对于 Lgbm）等。
在这两种情况下，AUC 从训练集上的 70% 左右下降到验证集上的 55%（对于最佳参数组合），测试集也是如此，约为 55%。
我在想：我能做些什么来将它们结合起来？提前停止是一种可能性吗？我还能尝试什么来避免在训练集上过度拟合？显然，学到的见解不会从样本中转化出来。
或者它可能是早期步骤中更根本的问题，例如特征工程等？]]></description>
      <guid>https://stats.stackexchange.com/questions/648288/significant-performance-drop-between-train-and-validation-set</guid>
      <pubDate>Thu, 30 May 2024 13:34:48 GMT</pubDate>
    </item>
    <item>
      <title>为什么 $S = X \cdot W + b$，$dS$ 的梯度等于 $\frac{\text{probs} - 1} {N}$，其中 $\text{probs}$ 是 $S$ 的 softmax 分类器？</title>
      <link>https://stats.stackexchange.com/questions/648285/why-is-the-gradient-of-s-x-cdot-w-b-ds-equal-to-frac-textprobs</link>
      <description><![CDATA[为什么$S = X \cdot W + b$，$dS$的梯度等于$\frac{\text{probs} - 1} {N}$？
这里，probs 是一个$1 \times K$数组，表示$X_i$属于类$k$的概率，其中$k = 1, 2, ..., K$，$N$是数据集的大小。
$X_i$ 具有 $J$ 个特征/列 ($X_{i1}$, $X_{i2}$, ... ,$X_{iJ}$)，并且我们在 $S$ 上使用 softmax 激活函数：
$$\text{probs}_k = \frac{\exp(S_K)}{\sum_{k = 1}^{K} \exp(S_k)}$$
以下是代码的具体部分，其中指出 $dS = \frac{\text{probs} - 1} {N}$: 
以下是 NN 的结构：]]></description>
      <guid>https://stats.stackexchange.com/questions/648285/why-is-the-gradient-of-s-x-cdot-w-b-ds-equal-to-frac-textprobs</guid>
      <pubDate>Thu, 30 May 2024 12:58:10 GMT</pubDate>
    </item>
    <item>
      <title>回归：仅具有正值的负截距[关闭]</title>
      <link>https://stats.stackexchange.com/questions/648284/regression-negative-intercept-with-only-positive-values</link>
      <description><![CDATA[我正在对一些标准差值进行多元线性回归，但许多预测因子的截距和系数为负。
关于我应该如何解释结果，您有什么想法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/648284/regression-negative-intercept-with-only-positive-values</guid>
      <pubDate>Thu, 30 May 2024 12:38:25 GMT</pubDate>
    </item>
    <item>
      <title>如何在 R 中执行（流行病学）等时替代分析？</title>
      <link>https://stats.stackexchange.com/questions/648264/how-to-perform-an-epidemiologic-isotemporal-substitution-analysis-in-r</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/648264/how-to-perform-an-epidemiologic-isotemporal-substitution-analysis-in-r</guid>
      <pubDate>Thu, 30 May 2024 07:26:18 GMT</pubDate>
    </item>
    <item>
      <title>结果变量可以在同一个模型中使用两次吗？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/648251/can-an-outcome-variable-be-used-twice-in-the-same-model</link>
      <description><![CDATA[在什么时候，在同一个模型框架中的两个似然中使用同一个结果变量是合适的？
下面是一个具体的例子：
model{
y ~ bernoulli( (1-psi) * mu);
y ~ bernoulli( (1-psi) * theta);

u ~ bernoulli( theta);
}


我能够使这个模型工作的唯一方法是在两个单独的似然中使用 y。该模型公式完美地恢复了模拟参数。
在联合似然（混合模型，Stan 中的 log_mix）中执行此操作会导致 mu 的估计有偏差。它仅在似然独立时才有效。
任何帮助和背景信息都将不胜感激。
谢谢，
Benny]]></description>
      <guid>https://stats.stackexchange.com/questions/648251/can-an-outcome-variable-be-used-twice-in-the-same-model</guid>
      <pubDate>Wed, 29 May 2024 22:38:31 GMT</pubDate>
    </item>
    <item>
      <title>理解元分析中的相关和分层效应模型</title>
      <link>https://stats.stackexchange.com/questions/648224/understanding-correlated-and-hierarchical-effects-models-in-meta-analysis</link>
      <description><![CDATA[我正在做一个三级荟萃分析，我想比较一个三级相关和分层效应模型（其中效应嵌套在研究之间）和一个两级模型（其中研究级别的方差被限制为 0）。
最初，我使用以下简单的三级随机效应模型：
model &lt;- rma.mv(yi = es, 
V = var, 
slab = study,
data =pression,
random = ~ 1 | study/es.id, 
test = &quot;t&quot;, 
method = &quot;REML&quot;)

并将其与这个混合效应模型进行比较。
model.reduced &lt;- rma.mv(yi = es, 
V = var, 
slab = study,
data =pression,
random = ~ 1 | study/es.id, 
test = &quot;t&quot;, 
method = &quot;REML&quot;,
sigma2 = (0,NA))

anova(model, model.reduced)

这使我能够理解模型适应度如何随着聚类效果的变化而变化。到目前为止很简单。
但是，从那时起，我将上述模型拟合为如下所示的相关分层模型（V.6），这将使我能够进一步使用稳健方差估计对其进行拟合，并避免模型错误指定。我通过将方差输入更改为使用 R 中的 ClubSandwich 包得出的 imputed_variance_matrix 来实现此目的。请参阅下面的示例：
V.6 &lt;- with(depression, 
impute_covariance_matrix(vi = var,
cluster = study,
r = rho))

我现在使用以下模型 [che.model.6]，但我无法将其与简化模型 [model.reduced] 进行比较，因为两个模型之间的协方差不相等。使用与上述相同的逻辑，将 che.model.6 与 che.model.60 进行比较，以比较在研究集群水平上将方差限制为 0 时异质性的差异，这是否有问题。
che.model.6 &lt;- rma.mv(yi =es,
V = V.6,
random = ~ 1 | study/es.id,
data =pression,
sparse = TRUE)

che.model.60 &lt;- rma.mv(yi =es,
V = V.6,
random = ~ 1 | study/es.id,
data =pression,
sparse = TRUE,
sigma2 = c(0, NA))

anova(che.model.6, che.model.60)

第二个方差分析是否与第一个方差分析在随机效应模型中发挥相同的作用？或者 RVE，例如：
coef_test(che.model.6, 
vcov = &quot;CR2&quot;)

对于我的 CHE 模型，其功能与 anova 对于我的随机效应三级模型的功能基本相同？
感谢您的帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/648224/understanding-correlated-and-hierarchical-effects-models-in-meta-analysis</guid>
      <pubDate>Wed, 29 May 2024 16:22:41 GMT</pubDate>
    </item>
    <item>
      <title>VAE：为什么我们需要编码器来生成图像？</title>
      <link>https://stats.stackexchange.com/questions/647848/vaes-why-do-we-need-the-encoder-for-image-generation</link>
      <description><![CDATA[我可能忽略了一些显而易见的东西，但如果我们只想生成图像，而对潜在空间不感兴趣，为什么我们甚至需要 VAE 中的编码器？
据我理解，VAE 损失的第二项主要确保编码器分布接近$N(0,1)$，这使得生成新输出变得更容易，因为我们只需要从标准正态分布中采样，而不必明确知道编码器分布。
为什么我们不能从正态分布中采样开始，并使用重构损失训练生成器，而无需编码器？
或者只是一个很好的副作用，我们也可以“免费”获得一个编码器，例如在 BiGAN 中？]]></description>
      <guid>https://stats.stackexchange.com/questions/647848/vaes-why-do-we-need-the-encoder-for-image-generation</guid>
      <pubDate>Thu, 23 May 2024 19:08:21 GMT</pubDate>
    </item>
    <item>
      <title>详尽测试所有可能的特征组合以找到最佳组合是否有效？</title>
      <link>https://stats.stackexchange.com/questions/647517/is-it-valid-to-exhaustively-test-all-possible-combinations-of-features-to-find-t</link>
      <description><![CDATA[我有大约 1000 个带标签的观察结果，这些观察结果来自大约 50 名受试者，他们在不同的情境下做出生理反应，我正在尝试对情境进行分类（通常分为频率大致相等的三个类别，但有时分为四个或五个类别）。对于每个观察结果，我生成大约 2000 个特征，需要选择最佳的特征子集进行分类。使用 mRMR（最小冗余最大相关性），我可以将特征减少到大约 25-100 个特征，然后 mRMR 得分会降得很低（取决于我包括哪些类别）。
我取其中的前 20 个（以保持数字的可行性），并使用留一交叉验证测试最多 8 个特征的所有可能组合，得到大约 264,000 个组合。然后，我选择准确率最高的组合。对于 LDA，这只需要我的笔记本电脑 6 个小时，但我也在测试需要更长时间的其他分类器。
我的问题是，这是否是一种有效的方法，或者我是否引入了偏差、过度拟合等，如果是，如何弥补。我读到最好在每个折叠内进行特征选择，但我甚至不确定如何做到这一点，尤其是在遗漏一个主题的情况下。
我发现单独使用 mRMR 并不合适：如果我只使用 mRMR 的前 8 个特征，我得到的准确度会比找到最佳组合要低得多。在一个极端情况下，使用高斯 SVM 仅比较两个类，差异是 99% 的准确率和 50% 的准确率，但通常准确率差异约为 10-20%。）
我有两个目标：1）选择最佳特征来在所有数据上训练模型，并用它来对新观察进行分类，2）估计此类模型对未来观察的准确率。
有没有办法可以考虑对准确率的影响，如果我愿意分配这么多的计算量，有没有更好的方法来选择特征？
当我问的时候，使用 PCA 创建 5-10 个组件而不是直接使用特征是否更有意义？
任何建议都值得赞赏！非常感谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/647517/is-it-valid-to-exhaustively-test-all-possible-combinations-of-features-to-find-t</guid>
      <pubDate>Sat, 18 May 2024 18:21:32 GMT</pubDate>
    </item>
    </channel>
</rss>