<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Sat, 25 May 2024 12:25:28 GMT</lastBuildDate>
    <item>
      <title>R中的stR包中的严格周期性季节性</title>
      <link>https://stats.stackexchange.com/questions/647951/strictly-periodic-seasonality-in-str-package-in-r</link>
      <description><![CDATA[我正在进行分解练习，将每日用电量分解为每周、每年的季节性波动以及天气温度与正常温度的偏差。为此，我使用了 Documentov, Hyndman (2015) 论文中最近开发的一个很好的“stR”包。问题是我需要设置严格的季节性波动周期而不是灵活的周期：我的时间序列涵盖5年，不太可能观察到季节性波动的变化。因此，我受到外生变量（温度波动）的偏差影响，因为季节项的变化与它们相关。我怎样才能解决这个问题，并将季节性的严格周期性设置为来自寓言/盛宴包的STL中的“周期性”选项？
不幸的是，我无法在这里发布代码和数据，因为它处于 NDA 之下。]]></description>
      <guid>https://stats.stackexchange.com/questions/647951/strictly-periodic-seasonality-in-str-package-in-r</guid>
      <pubDate>Sat, 25 May 2024 10:29:12 GMT</pubDate>
    </item>
    <item>
      <title>如何使用 convLSTM2D 对可变输入形状进行训练？</title>
      <link>https://stats.stackexchange.com/questions/647949/how-to-train-with-convlstm2d-on-variable-input-shape</link>
      <description><![CDATA[我正在使用 4 过滤器对 72x72 图像的时间序列进行分类（就像 RGB）。如果我的所有样本都具有相同的时间步长（或纪元数），那么事情就会很好地进行。然而，实际上我每个样本都有不同数量的时间步长。 （这是天文学中的，我无法随意获取数据。）但是当我尝试包含不同的时间步长时，我收到错误。请查看 MWC。
&lt;前&gt;&lt;代码&gt;N=2000
train_data_arr=np.random.rand(N, 11, 72, 72, 4)
train_label=np.repeat(np.random.randint(2,size=N)[:, np.newaxis], 11,axis=1)
Ntot,dum=train_label.shape;打印(Ntot,dum)

#===可变数据形状====
def select_random_time_epochs(数据、标签、max_time_steps=11):
    变量数据 = []
    变量标签 = []
    对于 zip（数据，标签）中的 d、l：
        #时间步数 = 11
        time_steps = np.random.randint(1, max_time_steps + 1)
        variable_data.append(d[:time_steps])
        variable_labels.append(l[:time_steps])
    返回变量数据、变量标签
train_data_var, train_label_var = select_random_time_epochs(train_data_arr, train_label)

#====数据生成器======
def make_generator（数据，标签）：
    def 生成器():
        对于 zip（数据，标签）中的 d、l：
            产量 d, l
    返回发电机

train_ds = tf.data.Dataset.from_generator(
    生成器=make_generator(train_data_var, train_label_var),
    输出类型=（tf.float32，tf.int32），
    output_shapes=(tf.TensorShape([无, 72, 72, 4]), tf.TensorShape([无]))
）
批量大小 = 32
train_ds = train_ds.batch(batch_size)

#===型号===
输入形状 = (无, 72, 72, 4)  
模型 = tf.keras.Sequential()
model.add(输入(形状=input_shape))
model.add(ConvLSTM2D(32, (9, 9), 激活=&#39;relu&#39;, padding=&#39;valid&#39;, return_sequences=True, data_format=&#39;channels_last&#39;))
model.add(BatchNormalization())
model.add(TimeDistributed(MaxPooling2D((2, 2), data_format=&#39;channels_last&#39;)))
model.add(TimeDistributed(Flatten()))
model.add(TimeDistributed(Dense(64,activation=&#39;relu&#39;)))
model.add(TimeDistributed(Dense(1,activation=&#39;sigmoid&#39;)))
model.compile(optimizer=&#39;adam&#39;,loss=&#39;binary_crossentropy&#39;,metrics=[&#39;accuracy&#39;])
模型.summary()

model.fit(train_ds, epochs=20)

如果我将时间步长固定为 11（在函数 select_random_time_epochs() 中），则一切正常。但是当我使用可变数量的时间步长时，我收到错误：
无法对组件 0 中具有不同形状的张量进行批处理。第一个元素的形状为 [3,72,72,4]，元素 3 的形状为 [6,72,72,4]。

我知道它无法处理 32 个批次内的可变时间步长。实际上，当我设置 batch_size = 1 时，上面的代码可以工作，但这需要太多时间，而且很可能永远不会融合到真实的用例场景中。
所以我的问题如下。

假设我完全不需要任何填充。当批次内的样本具有不同形状时，是否有更快的方法来实现 model.fit() ？否则，我可以动态批处理具有相同时间步长的样本吗？这是唯一的方法吗？评估可能不遵循同一批次分布的测试数据时会出现问题吗？

现在来到 padding 选项：tensorflow.keras.layers.Masking 是否真的能够处理我在缺失时期放置的“坏图像” ？换句话说，Masking 或其他东西可以完全使填充值变得无关吗？


重要的一点：我必须使用生成器来避免立即在 GPU 上加载数据，因为我的实际数据很大（字面意思是天文数字）。
另外，以下线程讨论了 LSTM 的变量输入，但我的用例稍微复杂一些。
https://stackoverflow.com/questions/63663399/how -处理lstm的可变长度数据
https://stackoverflow.com /questions/38189070/how-do-i-create-a-variable-length-input-lstm-in-keras]]></description>
      <guid>https://stats.stackexchange.com/questions/647949/how-to-train-with-convlstm2d-on-variable-input-shape</guid>
      <pubDate>Sat, 25 May 2024 08:54:08 GMT</pubDate>
    </item>
    <item>
      <title>书籍：Rice 还是 Casella、Berger？</title>
      <link>https://stats.stackexchange.com/questions/647948/books-rice-or-casella-berger</link>
      <description><![CDATA[我对您更喜欢哪个以及出于什么（主观和客观）原因感兴趣：
Rice（数学、统计和数据分析）或 Casella、Berger（统计推断）？
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/647948/books-rice-or-casella-berger</guid>
      <pubDate>Sat, 25 May 2024 08:53:30 GMT</pubDate>
    </item>
    <item>
      <title>如何估计很小比例的置信区间？</title>
      <link>https://stats.stackexchange.com/questions/647947/how-to-estimate-the-confidence-interval-of-a-very-small-proportion</link>
      <description><![CDATA[我想估计很小比例的置信区间。假设我有 1000 人的简单随机样本，有 4 个人回答了“是”这个问题。人们可能想利用正态性假设并使用基本公式计算置信区间
CI ± z * sqrt(p^ * (1-p^)/n))
然而，要使这一点成立，通常会施加 n * p ≥ 5 且 n * (1-p) ≥ 5 的限制，在本例中违反了这一限制。
因此，我的问题是：如果我想说：“在 95% 的置信度下，回答“是”的人数比例小于 x%”。我如何找到x？]]></description>
      <guid>https://stats.stackexchange.com/questions/647947/how-to-estimate-the-confidence-interval-of-a-very-small-proportion</guid>
      <pubDate>Sat, 25 May 2024 08:51:01 GMT</pubDate>
    </item>
    <item>
      <title>改进股票交易的 LSTM 模型并加快代码执行速度 [关闭]</title>
      <link>https://stats.stackexchange.com/questions/647945/improving-lstm-model-for-stock-trading-and-speeding-up-code-execution</link>
      <description><![CDATA[我一直在研究使用 LSTM 模型的股票交易算法。该算法获取实时数据，进行预测，并根据预测价格决定是否买入或卖出股票。这是代码：
将 numpy 导入为 np
将 pandas 导入为 pd
将 matplotlib.pyplot 导入为 plt
从 sklearn.preprocessing 导入 MinMaxScaler
从tensorflow.keras.models导入顺序
从 keras.layers 导入 LSTM，密集
导入时间
导入请求

# 定义股票代码和您的 Alpha Vantage API 密钥
符号 = &#39;AAPL&#39;
api_key = &#39;MY_API&#39;

# 定义初始资本
初始资本 = 10000
资本=初始资本

# 每只股票的经纪费
每只股票经纪费 = 0.02

# 最低经纪费
最低经纪费 = 18

# 美国证券交易委员会税率
秒税率 = 0.0000278

# 创建一个数据框来存储一段时间内的资本和行动
Capital_df = pd.DataFrame(columns=[&#39;时间&#39;, &#39;资本&#39;, &#39;行动&#39;, &#39;价格&#39;])
Capital_df.loc[0] = {&#39;time&#39;: pd.Timestamp.now(), &#39;capital&#39;: Capital, &#39;action&#39;: &#39;Initial&#39;, &#39;price&#39;: 0} # 使用初始资本进行初始化

# 定义回顾期
回顾=60

# 定义LSTM模型
模型=顺序（）
model.add(LSTM(单位=50, return_sequences=True, input_shape=(lookback, 1)))
model.add(LSTM(单位=50))
model.add(密集(1))

# 编译LSTM模型
model.compile(loss=&#39;mean_squared_error&#39;, 优化器=&#39;adam&#39;)

而真实：
    # 使用 Alpha Vantage API 获取实时数据
    url = f&#39;https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&amp;symbol={symbol}&amp;interval=1min&amp;apikey={api_key}&#39;
    响应 = requests.get(url)
    数据 = 响应.json()
    current_price = float(data[&#39;时间序列 (1min)&#39;][list(data[&#39;时间序列 (1min)&#39;].keys())[0]][&#39;4.收盘&#39;])

    # 预处理数据
    缩放器 = MinMaxScaler(feature_range=(0, 1))
    Price_data = np.array([范围内 _ 的当前价格(回溯)]).reshape(-1, 1)
    缩放数据 = 缩放器.fit_transform(price_data)

    # 为 LSTM 模型准备输入
    输入=scaled_data.reshape（1，回顾，1）

    # 拟合 LSTM 模型
    model.fit(输入，np.array([current_price])，epochs=1，batch_size=1，verbose=2)

    # 使用经过训练的 LSTM 模型进行预测
    预测价格 = model.predict(输入)
    预测价格 = scaler.inverse_transform(预测价格)[0][0]

    # 计算买入或卖出的股票数量
    num_stocks = 资本 // 当前价格

    # 计算买入和卖出的经纪费用总额
    总经纪费 = 最大（每只股票经纪费 * 股票数量，最低经纪费） * 2

    # 计算 SEC 税
    sec_tax = sec_tax_rate * 预测价格 * 股票数量

    # 根据预测价格更新资本并记录操作
    如果预测价格&gt;时价：
        资本 += 股票数量 * 当前价格 - 总经纪费用 - 秒税
        行动=“购买”
    别的：
        资本 -= 股票数量 * 当前价格 + 经纪费用总额 + 秒税
        行动=&#39;卖出&#39;

    # 将当前资本和操作附加到数据框
    new_row = {&#39;time&#39;: pd.Timestamp.now(), &#39;capital&#39;: 资本, &#39;action&#39;: 操作, &#39;price&#39;: current_price}
    Capital_df = pd.concat([capital_df, pd.DataFrame([new_row])],ignore_index=True)

    # 绘制资本随时间变化的图
    plt.figure(figsize=(10, 5))
    plt.plot(capital_df[&#39;时间&#39;], Capital_df[&#39;资本&#39;])
    plt.title(&#39;资本随着时间的推移&#39;)
    plt.xlabel(&#39;时间&#39;)
    plt.ylabel(&#39;大写&#39;)
    plt.xticks（旋转=45）
    plt.gca().xaxis.set_major_locator(plt.MaxNLocator(10)) # 显示 10 个刻度
    plt.show()

    print(f&#39;当前收益：{capital - initial_capital}，操作：{action}，价格：{current_price}&#39;)

    # 等待1分钟
    时间.睡眠(60)

我对此有几个问题：

奇怪的情节：随着时间的推移，资本的情节看起来很奇怪。它实际上只是一条直线。我不确定为什么会发生这种情况，但我相信这可能会带来一些统计问题。难道是由于我更新和绘制首都的方式所致？

确切时间和操作：我希望我的代码指定 LSTM 模型计算的确切时间和确切操作（买入/卖出）。我怎样才能实现这个目标？

加快代码执行速度：代码需要在一分钟内运行才能按分钟更新。然而，目前需要一分多钟的时间。有没有办法优化代码，使其运行得更快？


任何帮助将不胜感激。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/647945/improving-lstm-model-for-stock-trading-and-speeding-up-code-execution</guid>
      <pubDate>Sat, 25 May 2024 07:27:56 GMT</pubDate>
    </item>
    <item>
      <title>XGBoost：操纵样本是否会使其“推断”？</title>
      <link>https://stats.stackexchange.com/questions/647942/xgboost-does-manipulating-the-sample-make-it-extrapolate</link>
      <description><![CDATA[假设我想使用 XGBoost 进行时间序列预测。我知道基于树的模型无法推断。然而，我正在使用的时间序列是固定的（没有趋势或明显的季节性+ ADF 测试 在训练样本上给出的 p 值基本上为零）。我的问题是，由于我将数据分为训练和测试子样本，因此模型无法预测测试集中的一些观察结果（异常值），因为它在训练集中没有看到如此低/高的值。该模型在边界附近给出了平坦的线。

我知道堆叠/混合模型（例如 XGBoost + LinReg）以允许外推。然而，据我所知，这只能解决推断趋势或季节性模式的问题，而我关心的是数据范围或异常值。如果我要拟合 XGBoost 模型，请获取预测 $\hat{y}_t$，然后对残差进行建模 $y_t -\hat{y}_t$ 与其他一些堆叠模型，那么我不知道有任何模型能够很好地预测一些突然的峰值，而所有其他值都接近于零（所以，基本上，&lt; a href=&quot;https://en.wikipedia.org/wiki/White_noise&quot; rel=&quot;nofollow noreferrer&quot;&gt;白噪声，突然出现峰值）
我还知道 XGBoost 中有一个选项可以选择 gblinear 作为助推器。然而，在我的特殊情况下，我的数据范围是在 $(0,+\infty)$ 中定义的（我预测波动性），所以这将允许模型得到负值。另外，我已经尝试拟合这个模型，但拟合效果很糟糕，比默认的 gbtree 差得多
我唯一想到的就是将训练集中的前两个观察值更改为一些虚构的异常值（例如，我设置 $y_1=0$  和 $y_2=100$），模型肯定不会再看到它来尝试强制这个范围，所以它至少可以考虑值接近这些。尽管在视觉上我没有看到差异，但我使用的所有指标（如 RMSE 和 MAE）仅通过这个简单的修复就得到了很大的改善。然而，该模型仍然在与以前相同的位置上保持平坦

我的问题是：还有其他技术可以尝试解决此问题吗？我的解决方案是否合法？是否允许“外推”？ 
如有任何建议，我们将不胜感激]]></description>
      <guid>https://stats.stackexchange.com/questions/647942/xgboost-does-manipulating-the-sample-make-it-extrapolate</guid>
      <pubDate>Sat, 25 May 2024 04:29:51 GMT</pubDate>
    </item>
    <item>
      <title>Vision Transformer 模型训练和验证准确度停留在 50 [关闭]</title>
      <link>https://stats.stackexchange.com/questions/647941/vision-transformer-model-training-and-validation-accuracy-stuck-at-50</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/647941/vision-transformer-model-training-and-validation-accuracy-stuck-at-50</guid>
      <pubDate>Sat, 25 May 2024 04:23:01 GMT</pubDate>
    </item>
    <item>
      <title>Wishart 密度的积分界限</title>
      <link>https://stats.stackexchange.com/questions/647928/bounds-of-integration-for-the-wishart-density</link>
      <description><![CDATA[我曾经上过一门课程，其中包含无数有关威沙特分布的练习，但据我记得，从未提及威沙特密度。我在这个问题中问了一些关于这个问题的问题，其中我提到了集成可能会很混乱。
回复表示了有关的措施一个积分（在非奇异情况下）是
$$
\prod_{i,j\,:\,1\,\le\,i\,\le\,j\,\le\,p} dx_{i,j}。
$$
我的问题是关于那些“混乱”的问题。整合的界限。
我将进行猜测，看看知道某事的人是否可以证实或否认它，或者（也许是最好的选择）对其进行改进。
我们正在矩阵空间上进行积分$(x_{i,j})_{i,j \, \in\, \{1,\,\ldots, \,p\}}$ 是对称且正定的。
假设这样一个矩阵是
$$
\left[ \begin{array}{cc} \underset{p_1\times p_1} A &amp; \underset{p_1\times p_2} B \\[8pt] \underset{p_2\times p_1}{B^T} &amp; \underset{p_2\times p_2}C \end{array} \right]。
$$
那么 $A$ 和 $C$ 是对称且正定的，且 $B$ 必须使得整个矩阵为正定。
存在这样的一一对应关系：
$$
\left[ \begin{array}{cc} A &amp; B \\[8pt] B^T &amp; C \end{array} \right] \longleftrightarrow \left( \underset{p_1\times p_1} A,\quad \underset{p_2\times p_1} {B^T A^{-1}}, \quad \underset{ p_2\times p_2} {C- B^T A^{-1} B} \right) = (J,K,L)。
$$
因此 $J$ 和 $L$ 是正定的。
这个三元组的第二个组成部分出现在条件期望值的表达式中，第三个组成部分是相应的条件方差。
由此可见
\begin{align}
A&amp; = J，\\
住宿加早餐旅馆= JK^T, \\
C&amp; = L + KJK^T。
\end{对齐}
$(J,K,L)\mapsto(A,B,C)$ 的域是笛卡尔积，其第一和第三因子是所有正数的集合- 适当大小的定对称实矩阵，其第二个因子是（这是有趣的部分）全部 $p_2\times p_1$ 实数矩阵 $K.$ （简单练习：证明这一点。）
因此，我们将积分界限问题简化为较小正定对称实矩阵的积分界限问题，并在矩阵空间上进行积分，其中每个条目的界限为 $-\infty$ 和 $+\infty.$
可以迭代这个过程，直到我们得到 $p=p_1+p_2$ 因子的笛卡尔积，其中每个因子都是  $(0,+\infty),$ 和 $\binom p2$ 因子，每个因子都是 $ (-\infty,+\infty).$
我的问题是：这有用吗？这是标准技术吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/647928/bounds-of-integration-for-the-wishart-density</guid>
      <pubDate>Fri, 24 May 2024 23:01:09 GMT</pubDate>
    </item>
    <item>
      <title>在建模逆关系（曝光〜结果）时，如何在 dagitty/ggdag 中正确指定“曝光”和“结果”？</title>
      <link>https://stats.stackexchange.com/questions/647952/how-to-correctly-specify-exposure-and-outcome-in-dagitty-ggdag-when-modelli</link>
      <description><![CDATA[我正在尝试使用 R 中的包 ggdag 来更好地理解我的建模结果。
如果我希望对响应 ~ 事件关系进行建模，我可以轻松找到要控制的变量：
库(ggdag)

达格化（
  响应〜事件+混杂因素，
  事件〜混淆者，
  曝光=c(“事件”)，
  结果 = c(“响应”)
) |&gt; ggdag_调整_设置()


但是，如果我现在有兴趣建模相反的关系，即事件〜响应（例如，预测给定响应的事件的概率），我收到一条警告，指出无法关闭后门路径：
dagify(
  响应〜事件+混杂因素，
  事件〜混淆者，
  暴露=c(“响应”)，
  结果 = c(“事件”)
) |&gt; ggdag_调整_设置()
#&gt; dag_adjustment_sets(.,exposure=exposure,outcome=outcome,：关闭后门路径失败。常见原因包括：
#&gt;             * 图不是无环的
#&gt;             * 后门路径无法使用给定的变量集关闭
#&gt;             * 必要的变量是未测量的（潜在的）


同样的情况也发生在更简单的 DAG 中：
dagify(
  响应〜事件，
  暴露=c(“响应”)，
  结果 = c(“事件”)
) |&gt; ggdag_调整_设置()

在我看来，由于因果关系的方向是事件 -&gt;响应 我要求的路径（响应 -&gt; 事件）被 ggdag 解释为后门路径，因此出现警告。
由于我的实际模型比这些简单的示例复杂得多，我仍然想从 ggdag 获取调整集。
DAG 的正确规范是什么？如果人们只是离开“曝光”？和“结果”和第一种情况一样，即使是反向建模？]]></description>
      <guid>https://stats.stackexchange.com/questions/647952/how-to-correctly-specify-exposure-and-outcome-in-dagitty-ggdag-when-modelli</guid>
      <pubDate>Fri, 24 May 2024 16:04:00 GMT</pubDate>
    </item>
    <item>
      <title>期望的建议：通过多级模型预测美国龙卷风数量</title>
      <link>https://stats.stackexchange.com/questions/647861/advice-desired-predicting-us-tornado-counts-via-multi-level-model</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/647861/advice-desired-predicting-us-tornado-counts-via-multi-level-model</guid>
      <pubDate>Fri, 24 May 2024 03:07:14 GMT</pubDate>
    </item>
    <item>
      <title>为什么在此随机效应方差分析中估计总体方差与估计 $\sigma^2 + \tau^2$ 不同？</title>
      <link>https://stats.stackexchange.com/questions/647826/why-estimated-population-variance-differs-from-estimated-sigma2-tau2-in</link>
      <description><![CDATA[随机效应方差分析模型通常写为
$Y_{ij} = \gamma_{00} + u_{0j} + \epsilon_{ij}$
。结果变量的总方差分解为
$var(Y_{ij}) = \tau^2 + \sigma^2$
其中$\tau^2$表示$u_{0j}$和&lt;的方差span class=&quot;math-container&quot;&gt;$\sigma^2$ $\epsilon_{ij}$ 的方差。
我有一些代码可以创建 $\sigma^2$ 和 $\tau^2$ 的估计值span&gt; 来自人工数据。
然后我运行一个空的多级模型，可以看到 $\sigma^2$ 和 $\ 的估计值tau^2$ 与 lme4 给我的结果完全匹配。
但是，当我将估计总体方差计算为
estimated_population_variance &lt;- var(my_data$y) * (N-1)/N

它不等于$\sigma^2 + \tau^2$。为什么不？ $\sigma^2 + \tau^2$ 不就是总方差的分解吗？
我认为这一定与我估计总体方差的方式有关，但我不确定我做错了什么。
代码：
库(lme4)

设置.种子(123) 
组 &lt;- 代表（1:10，每组 = 10）
random_numbers &lt;- 样本（1:99，长度（组），替换 = TRUE）
my_list &lt;- 列表(group = groups, y = random_numbers)
my_data &lt;- as.data.frame(my_list)

N &lt;- 100
组数 &lt;- 10
平均组大小 &lt;- 10

table_of_groups &lt;- my_data %&gt;%
  group_by(组) %&gt;%
  总结（均值 = 均值（y），方差 = var（y），n = n（））

s2within &lt;- 均值(table_of_groups$variance)
估计_sigma2 &lt;- s2within

s2之间 &lt;- sum((table_of_groups$mean-mean(my_data$y))^2) * (1/(num_groups-1))
estimated_tau2 &lt;- s2 Between - (s2within/average_group_size)

empty_model &lt;- lmer(“y ~ 1 + (1 | group)”, data = my_data) 
摘要（空模型）

估计总体方差 &lt;- var(my_data$y) * (N-1)/N

估计_tau2 + 估计_sigma2
]]></description>
      <guid>https://stats.stackexchange.com/questions/647826/why-estimated-population-variance-differs-from-estimated-sigma2-tau2-in</guid>
      <pubDate>Thu, 23 May 2024 13:55:02 GMT</pubDate>
    </item>
    <item>
      <title>Cox 模型 - 不确定分析的时间单位</title>
      <link>https://stats.stackexchange.com/questions/647753/cox-model-unsure-of-time-unit-of-analysis</link>
      <description><![CDATA[我正在对癌症患者进行按时间到事件的生存分析（Cox 模型）。随访的开始是治疗的结束。自癌症诊断后每 6 个月进行一次事件（复发）测试，并且每个患者的这些测试的确切日期（日、月、年）都是已知的。我不确定 Cox 模型所需的分析单位和计算事件变量的时间。因为我们知道随访的开始日期和测试日期，所以我们可以以天为单位计算事件发生时间变量。然而，我认为这不是正确的方法，因为这肯定意味着在随访期间每天进行一次测试，而不是像我们那样每 6 个月进行一次。如果在测试日期检测到事件，我们所知道的是该事件发生在先前（无事件）和当前（检测到事件）测试日期之间的某个时间。考虑到这一点，在这种情况下，首先以天为单位计算事件变量的时间，然后将其分成间隔，每个间隔长度 6 个月，是否有意义？分析单位将是半年，因此危险的解释将是例如每半年每人有 X 次事件？
评论后更新
我正在考虑按时做一个普通的考克斯模型来检测并认识到这是检测而不是复发本身。 （参见@EdM 的评论）。
我们有死亡日期和死因。我不确定如何合并这些信息。例如，对于没有发生事件（复发）也没有死亡的患者，随访结束是他们的最后一次测试日期。对于在最后一次测试日期之前没有发生事件但随后死亡的患者，随访结束日期应该是死亡日期吗？如果死亡原因是癌症，我认为不会，因为根据定义，复发必须在死亡之前发生。但对于非癌症相关的死亡，随访结束应该是死亡日期还是最后一次测试日期？我们不确定上次测试日期（事件呈阴性）和死亡日期之间是否可能发生复发。]]></description>
      <guid>https://stats.stackexchange.com/questions/647753/cox-model-unsure-of-time-unit-of-analysis</guid>
      <pubDate>Wed, 22 May 2024 12:28:53 GMT</pubDate>
    </item>
    <item>
      <title>使用费舍尔推理可以帮助我对动力不足的结果更有信心吗？</title>
      <link>https://stats.stackexchange.com/questions/647457/can-using-fisherian-inference-help-me-to-be-more-confident-of-an-underpowered-re</link>
      <description><![CDATA[我正在使用贫困指数作为运行变量进行不连续回归，以了解现金转移对结果的影响。问题在于，该评分并不能很好地预测治疗效果，依从性约为 40%。此外，文献建议我应该寻找大约 0.1 个标准差的最小可检测效应大小。因此，在进行功率计算时，我发现虽然我的样本不小（~16 000 obs.），但功率仍然不足。
阅读 Cattaneo 等人的论文。 （2024；回归不连续性设计的实用介绍：扩展），我注意到，在存在以下情况的情况下观察值并不多，可以假设在截止值的上方和下方进行随机分配，并使用费舍尔推理（假设非随机的潜在结果并使用尖锐的零假设）仍然可以获得稳健的结果（尽管您必须放弃点估计）。
当您的结果不是因为样本量小而是因为合规性不完善而导致结果不足时，是否可以应用相同的逻辑？
也就是说，如果我使用常见的回归不连续性方法得到一个不显着的结果，然后在运行相同的回归但使用费舍尔推理时也得到一个不显着的结果，那么最后的结果是否令人放心？我是否可以相当有信心地认为，这种不显着性并非偶然，而是由于缺乏影响？]]></description>
      <guid>https://stats.stackexchange.com/questions/647457/can-using-fisherian-inference-help-me-to-be-more-confident-of-an-underpowered-re</guid>
      <pubDate>Fri, 17 May 2024 22:18:08 GMT</pubDate>
    </item>
    <item>
      <title>理解分数实验中的“乘法/群运算”</title>
      <link>https://stats.stackexchange.com/questions/643907/understanding-multiplication-group-operation-in-fractional-experiments</link>
      <description><![CDATA[在我试图理解的分数阶乘设计中，至少有一种群运算。为了确定性，假设我们有 3 个因素； A、B、C，各两个级别。请批评我的理解。我们假设效应稀疏，因此复合 ABC 是微不足道的。然后我们以某种方式设置 ABC=1。这1到底是什么？我们是否以某种方式“乘以”？或者将 A 与 B、C 组合以获得这个 1？我们可以单独围绕这个关系和别名定义一个乘法，其中等号的列是等效的吗？
编辑：我假设“1”代表一个无关紧要的结果，其他关系是通过别名给出的，即组合对（抱歉，我在这里在精确的术语上画了一个空白）如果它们的列被认为是相等的对于每个列条目来说都是相等的，例如 AB=C。就是这样，还是还有更多，比如正式的数学描述？]]></description>
      <guid>https://stats.stackexchange.com/questions/643907/understanding-multiplication-group-operation-in-fractional-experiments</guid>
      <pubDate>Sat, 30 Mar 2024 14:41:31 GMT</pubDate>
    </item>
    <item>
      <title>用于比较学生考试成绩的适当统计测试</title>
      <link>https://stats.stackexchange.com/questions/613204/appropriate-statistical-test-for-comparing-student-test-scores</link>
      <description><![CDATA[一个班级的学生参加了一个关于某个主题的课程。该科目分为示例 A（使用主动方法教授）和示例 B（使用被动方法教授）。学生在课程开始前、课程结束后以及一周后接受了有关该主题的检查。每次测试都是相同的。该测试是匿名的，因此我无法评估个人的测试分数。
我正在尝试找到适当的测试来评估以下假设：

示例 A 的学生分数将高于示例 B（考虑到先验知识）。

示例 A 的长期保留（一周后）会比示例 B 更好。


我有点困惑哪些统计测试是合适的，因为我没有配对值，所以我只是总体评估班级。
有什么想法吗？ TIA]]></description>
      <guid>https://stats.stackexchange.com/questions/613204/appropriate-statistical-test-for-comparing-student-test-scores</guid>
      <pubDate>Mon, 17 Apr 2023 11:45:13 GMT</pubDate>
    </item>
    </channel>
</rss>