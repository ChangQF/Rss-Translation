<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Thu, 06 Feb 2025 06:24:53 GMT</lastBuildDate>
    <item>
      <title>哪一个是正确的 SE，为什么？</title>
      <link>https://stats.stackexchange.com/questions/661030/which-one-is-the-right-se-and-why</link>
      <description><![CDATA[我不确定这是否更像是一个 stackoverflow 问题，但我认为这里可能更好，因为我正在尝试了解如何确定正确的答案。
我创建了一个可重现的小示例：
library(survey)
library(broom)
library(broom.helpers)

mtcars$weights &lt;- rnorm(dim(mtcars)[1],1,0.1)

model &lt;- svyglm(as.formula(&quot;vs ~ wt&quot;),family=quasibinomial(link=&quot;logit&quot;),design=svydesign(ids=~1,weights=~weights,data=mtcars))

假设我想在指数化系数后提取输出表，使用来自 broom 的 tidy 的输出和broom.helpers 中的 tidy_parameters 几乎完全相同，除了 SE（差异很大）之外
tidy(model,exp=TRUE,conf.int=TRUE)

给出此输出

但是，
tidy_parameters(model,exp=TRUE,conf.int=TRUE) 

给出此输出

我如何确定哪些是正确的 SE？什么可以解释两个输出之间的 SE 差异（但不能解释任何其他参数）？]]></description>
      <guid>https://stats.stackexchange.com/questions/661030/which-one-is-the-right-se-and-why</guid>
      <pubDate>Thu, 06 Feb 2025 05:33:28 GMT</pubDate>
    </item>
    <item>
      <title>使用距离和方差的遗传算法多目标聚类</title>
      <link>https://stats.stackexchange.com/questions/661029/genetic-algorithm-multi-objective-clustering-using-distance-and-variance</link>
      <description><![CDATA[我尝试使用 PyGAD 进行聚类，通过最小化 2d 点的欧几里德距离（vanilla kmeans），同时还希望最小化第三个特征（权重）的聚类间方差。我使用以下代码获得了合理的聚类，但我想知道这是否有意义。欢迎提供任何反馈。我将在下面包含聚类函数。
def cluster_data(solution, solution_idx):
global num_cluster, data
feature_vector_length = data.shape[1] - 1
cluster_centers = []
all_clusters_dists = []
clusters = []
clusters_sum_dist = []

clusters_sum_var = []

for clust_idx in range(num_clusters):
cluster_centers.append(solution[feature_vector_length*clust_idx:feature_vector_length*(clust_idx+1)])
cluster_center_dists = euclidean_distance(data[:,:-1], cluster_centers[clust_idx])
all_clusters_dists.append(numpy.array(cluster_center_dists))

cluster_centers = numpy.array(cluster_centers)
all_clusters_dists = numpy.array(all_clusters_dists) 
cluster_indices = numpy.argmin(all_clusters_dists, axis=0)
对于 clust_idx 在 range(num_clusters):
clusters.append(numpy.where(cluster_indices == clust_idx)[0])
如果 len(clu​​sters[clust_idx]) == 0:
clusters_sum_dist.append(0)
否则:
clusters_sum_dist.append(numpy.sum(all_clusters_dists[clust_idx, clusters[clust_idx]]))

clusters_sum_dist = numpy.array(clusters_sum_dist)

对于 clust_idx 在range(num_clusters):
if len(clu​​sters[clust_idx]) == 0:
clusters_sum_var.append(0)
else:
cluster_current_data = data[np.where(cluster_indices == clust_idx)[0],-1]
clusters_sum_var.append(np.std(cluster_current_data))
clusters_sum_var = np.array(clusters_sum_var)

return clusters_sum_var, cluster_centers, all_clusters_dists, cluster_indices, clusters, clusters_sum_dist

这是适应度函数
def fitness_func(ga_instance,solution, solution_idx):
clusters_sum_var,_, _, _, _, clusters_sum_dist = cluster_data(solution, solution_idx)

clusters_sum_var = rescale_linear(clusters_sum_var,clusters_sum_dist.min(),clusters_sum_dist.max())

fitness_1 =1.0 / (np.sum(clusters_sum_var) + 0.00000001)

fitness = 1.0 / (numpy.sum(clusters_sum_dist) + 0.00000001)

返回 fitness,fitness_1
]]></description>
      <guid>https://stats.stackexchange.com/questions/661029/genetic-algorithm-multi-objective-clustering-using-distance-and-variance</guid>
      <pubDate>Thu, 06 Feb 2025 04:16:40 GMT</pubDate>
    </item>
    <item>
      <title>使用引导程序生成预测区间</title>
      <link>https://stats.stackexchange.com/questions/661026/generating-prediction-intervals-using-bootstrap</link>
      <description><![CDATA[我有一些数据并拟合线性回归：
$$ y_i = \beta_0 + \beta_1x_i + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2) $$
我想对新点 $x_{\text{new}}$ 进行预测并记录预测区间。我对以下内容有些困惑。我能想到 3 种不同的方法来做到这一点：
我已将它们概述如下：

方法 1：没有单独预测区间的引导样本
使用引导抽样。对于一个新点$x_{\text{new}}$，生成引导样本。对于每个引导样本$b = 1, \ldots, B$:

从我们的原始数据中进行替换抽样，以获得$i = 1, \ldots, n$的$(x_i^{(b)}, y_i^{(b)})$
拟合新的回归模型以获得估计值$\hat{\beta}_0^{(b)}$和$\hat{\beta}_1^{(b)}$

对于每个引导样本$b$，仅计算点预测：
$$ \hat{y}_{\text{new}}^{(b)} = \hat{\beta}_0^{(b)} + \hat{\beta}_1^{(b)}x_{\text{new}} $$
然后根据这些预测的经验分布形成预测区间：
$$ [\hat{y}_{\text{new},\text{lower}}, \hat{y}_{\text{new},\text{upper}}] = [\text{Percentile}_{2.5}(\{\hat{y}_{\text{new}}^{(b)}\}), \text{Percentile}_{97.5}(\{\hat{y}_{\text{new}}^{(b)}\})] $$

方法 2：具有单独预测区间的引导样本
对于每个引导样本 $b$，我们计算完整的预测区间：
$$ \hat{y}_{\text{new}}^{(b)} \pm t_{n-2,\alpha/2}\hat{\sigma}^{(b)}\sqrt{1 + \frac{1}{n} + \frac{(x_{\text{new}} - \bar{x}^{(b)})^2}{\sum(x_i^{(b)} - \bar{x}^{(b)})^2}} $$
其中：

$\hat{\sigma}^{(b)}$ 是自举样本的残差标准误差 $b$
$t_{n-2,\alpha/2}$ 是 t 分布的临界值
$\bar{x}^{(b)}$ 是自举样本中 x 值的平均值 $b$

这为我们提供了 $b = 1, \ldots, B$ 的区间集合 $[L^{(b)}, U^{(b)}]$。最终的预测区间是：
$$ [\text{百分位数}_{2.5}(\{L^{(b)}\}), \text{百分位数}_{97.5}(\{U^{(b)}\})] $$

方法 3：无引导，使用封闭形式预测区间
使用经典方法和封闭形式公式：
$$ \hat{y}_{\text{new}} \pm t_{n-2,\alpha/2}\hat{\sigma}\sqrt{1 + \frac{1}{n} + \frac{(x_{\text{new}} - \bar{x})^2}{\sum(x_i - \bar{x})^2}} $$
其中：

$\hat{\sigma}$ 是原始模型的残差标准误差
$\bar{x}$ 是原始 x 值的平均值


总结：

方法 1 仅捕获回归系数中的不确定性（$\beta_0$ 和 $\beta_1$）。这将是最小的。
方法 2 通过引导法考虑了系数不确定性和固有可变性（$\sigma^2$）。这将是最大的
方法 3 使用基于 t 分布假设的理论公式。这将是中间

在实践中，建议使用方法 2 来获得最现实的预测区间吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/661026/generating-prediction-intervals-using-bootstrap</guid>
      <pubDate>Thu, 06 Feb 2025 03:45:25 GMT</pubDate>
    </item>
    <item>
      <title>安德森吉尔斯模型是否严格模拟事件重复发生的风险或事件之间的时间？</title>
      <link>https://stats.stackexchange.com/questions/661025/does-the-andersen-gills-model-strictly-model-the-hazard-of-an-event-recurring-or</link>
      <description><![CDATA[如果我对协变量对复发事件间隔时间的影响感兴趣，也就是说，我想探索特定协变量是否会缩短或延长后续事件之间的间隔，那么复发事件的 Cox 比例风险模型（如 Andersen Gills 模型）是否可以做到这一点？或者，这些模型是否严格模拟事件复发的风险，并计算协变量如何增加或减少事件复发的风险？]]></description>
      <guid>https://stats.stackexchange.com/questions/661025/does-the-andersen-gills-model-strictly-model-the-hazard-of-an-event-recurring-or</guid>
      <pubDate>Thu, 06 Feb 2025 02:19:01 GMT</pubDate>
    </item>
    <item>
      <title>为什么 CL% 的 CI 必须正确捕获参数？（证明）[关闭]</title>
      <link>https://stats.stackexchange.com/questions/661020/why-must-a-cl-of-cis-correctly-capture-the-parameter-proof</link>
      <description><![CDATA[据我所知，如果我们取一个样本，我们可以从该样本构建一个置信区间。如果我们从不同的样本中获取许多这样的置信区间，则正确的百分比将等于置信水平。有什么证据表明置信区间公式保证这是正确的？需要说明的是，我主要讨论的是比例。
我使用的公式是：
$$CI = \hat{p} \pm z^\star\sqrt{\frac{(\hat{p})(1-\hat{p})}{n}}$$
其中 $\hat{p}$ = 样本比例
$n$ = 样本大小
$z^\star$ = 置信水平的临界值
$CI$ = 置信区间
我使用了一个简单的随机样本作为样本。]]></description>
      <guid>https://stats.stackexchange.com/questions/661020/why-must-a-cl-of-cis-correctly-capture-the-parameter-proof</guid>
      <pubDate>Thu, 06 Feb 2025 00:12:59 GMT</pubDate>
    </item>
    <item>
      <title>K 均值成本函数</title>
      <link>https://stats.stackexchange.com/questions/661018/k-means-cost-function</link>
      <description><![CDATA[在《统计学习要素 (ESL)》中，他们在方程 (14.31) 中指出，k-Means 目标函数为
$$W(C) = \sum_{k=1}^KN_k \sum_{C(i)=k} ||x_i - \bar{x}_k||^2$$
其中 $K$ 是聚类的数量，$N_k$ 是聚类 $k$ 中的点的数量，$C(i)$ 是聚类分配函数。但是，从其他来源了解到 k-means 后，我总是看到成本函数被理解为
$$L(C) = \sum_{k=1}^K \sum_{C(i)=k}||x_i - \bar{x}_k||^2 $$
即不按大小对每个簇进行加权。我只想说，人们可以根据任务选择不同的权重，但我很确定 ESL 中提出的算法是局部最小化 $L(C)$ 的标准算法，而不是 $W(C)$，即他们描述的迭代步骤没有考虑簇大小。我这里遗漏了什么，还是书中有错误？]]></description>
      <guid>https://stats.stackexchange.com/questions/661018/k-means-cost-function</guid>
      <pubDate>Wed, 05 Feb 2025 22:32:42 GMT</pubDate>
    </item>
    <item>
      <title>使用 ggcorrplot 过滤具有特定敏感性的相关矩阵数据[关闭]</title>
      <link>https://stats.stackexchange.com/questions/661017/filtering-correlation-matrix-data-with-particular-sensitivity-with-ggcorrplot</link>
      <description><![CDATA[我有一个包含大量数据集（&gt;10 列）的相关矩阵，显然它既不清晰也无意义。我想通过过滤超出特定敏感度水平的相关矩阵来从数据中得出一些意义。该函数应具有参数 M、s 和 dir，并带有以下选项：
filter(corrM, s, &#39;ge&#39;)
filter(corrM, s, &#39;gt&#39;)
filter(corrM, s, &#39;eq&#39;)
filter(corrM, s, &#39;le&#39;)
filter(corrM, s, &#39;lt&#39;)
它应仅返回 corrM 中分别等于或大于、严格大于、等于、严格小于或小于或等于 S 值的条目。例如，如果 s=0.6 和 dir=&#39;ge&#39;，则所有小于 0.6 的值都将被忽略并设置为 NULL，以便可以从矩阵图中删除它们。
所讨论的矩阵图是
model.matrix(~0+., data=M) %&gt;%
cor(use=&quot;pairwise.complete.obs&quot;) %&gt;%
ggcorrplot(show.diag=FALSE, type=&quot;lower&quot;, lab=TRUE, lab_size=2)

但我希望将 corrM 导出为 tibble，而不是 ggplot。我不能简单地使用 cor(M)，因为我有数字和非数字数据的混合，所以我使用了这里描述的技术：https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi。]]></description>
      <guid>https://stats.stackexchange.com/questions/661017/filtering-correlation-matrix-data-with-particular-sensitivity-with-ggcorrplot</guid>
      <pubDate>Wed, 05 Feb 2025 22:11:22 GMT</pubDate>
    </item>
    <item>
      <title>计算认知模型对实验心理学是否必要？DDM/LBA 的困难</title>
      <link>https://stats.stackexchange.com/questions/661015/are-computational-cognitive-models-necessary-for-experimental-psychology-diffic</link>
      <description><![CDATA[一段时间以来，我一直在做一个监督研究项目，不幸的是，由于不可预见的情况，我不得不在夏天毕业之前全力以赴地完成这个项目。
我最初计划使用 GDDM 研究主观奖励推理对 MDD、ADHD 和 G.A.D 患者在不确定情况下的速度/准确度权衡的影响。
当我尝试将 DDM 或 LBA 模型与我的 RT 和来自包含西蒙效应的视觉异常任务的误差数据相匹配时，我遇到了技术问题。与我的 python/R 环境的持续依赖冲突、参数估计和时间都是一个问题。实现起始偏差的奖励函数或作为作用于漂移率的函数也具有挑战性，因此实际上，我认为在未来 5-7 个月内，没有可行的方法可以实现 20-40 人的队列。
我可以使用常规统计方法或经典过程模型，还是混合线性模型来实现这一点？双向方差分析和更通用（且不太复杂）的统计方法是否有效？
如果我的假设成立，我想与我的主管一起发表论文，但我不确定如果没有 DDM 或其他证据积累/非顺序抽样认知模型等模型，是否有可能这样做。
我怀疑能否在此时间范围内使用可与我的数据和其他研究的类似数据很好地匹配的通用模型完成这个项目，但建模是否有必要获得有用的结果和见解？
提前致谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/661015/are-computational-cognitive-models-necessary-for-experimental-psychology-diffic</guid>
      <pubDate>Wed, 05 Feb 2025 21:32:57 GMT</pubDate>
    </item>
    <item>
      <title>r 中随时间变化的暴露，但非随时间变化的协变量的生存分析</title>
      <link>https://stats.stackexchange.com/questions/661014/survival-analysis-with-time-varying-exposure-but-non-time-varying-covariates-in</link>
      <description><![CDATA[我想在 r 中估计一个 cox 比例风险模型。结果是死亡。暴露（或主要预测因素）是高血压的发展。我希望我的高血压变量随时间变化，但我不希望协变量随时间变化。我正在尝试弄清楚如何设置我的模型。
我查看了这些来源：
1)https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#Time-dependent_covariate_approach
2) https://stackoverflow.com/questions/63944551/two-different-results-from-coxph-in-r-using-same-stop-and-start-times-why
但是，我仍然不确定我应该采取什么方法。这是我的数据：
structure(list(MRN = c(1, 1, 2, 2, 3, 4, 5, 6, 7, 7), StopTime = c(4.65205479452055, 
4.65205479452055, 12.4767123287671, 12.4767123287671, 5.68493150684932, 
3.13698630136986, 0.797260273972603, 8.13972602739726, 22.0904109589041, 
22.0904109589041), death = c(0, 0, 0, 0, 0, 0, 0, 0, 0), tstart = c(0, 
0.643835616438356, 0, 2.92054794520548, 0, 0, 0, 0, 0, 14.2465753424658
), tstop = c(0.643835616438356, 4.65205479452055, 2.92054794520548, 
12.4767123287671, 5.68493150684932, 3.13698630136986, 0.797260273972603,
8.13972602739726, 14.2465753424658, 22.0904109589041), 高血压 = c(0L, 
1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L), 阶段 = 结构(c(1L, 
1L, 1L, 1L, 5L, 6L, 3L, 4L, 3L, 3L), 水平 = c(&quot;1&quot;, &quot;0&quot;, &quot;2&quot;, 
&quot;3&quot;, &quot;4&quot;, &quot;999&quot;), 类别 = &quot;因素&quot;)), row.names = c(NA, 10L), 类别 = &quot;data.frame&quot;)

我使用 tmerge 函数创建了这个长数据框。因此，如果人们患上了高血压，他们会得到 2 行：1 行高血压为 0，另一行高血压为 1。如果他们没有患上高血压，他们就有 1 行。停止时间因人而异，并且间隔不相等。一旦他们患上高血压，那就是他们被给予停止时间的时间点，然后创建一个新行并给出一个新的开始时间，该时间与上一行的停止时间相同。
例如取 id 1：
行 1：开始时间：0。停止时间：他们患上高血压的时间
行 2：开始时间：与上一行的停止时间相同。停止时间：死亡或被审查（死亡是主要结果）
根据我查阅的资料，我最初认为我需要以这种方式设置我的模型：
cox_full &lt;- 生存::coxph(Surv(time = (tstop - tstart), event = 死亡) ~ 高血压 + 阶段 +
cluster(MRN), 
data = td_dat)

您可以看到我从 tstop 中减去 tstart。但是，经过进一步审查，我不确定我是否做得正确。也许我需要做的是：time = tstart, time2 = tstop？]]></description>
      <guid>https://stats.stackexchange.com/questions/661014/survival-analysis-with-time-varying-exposure-but-non-time-varying-covariates-in</guid>
      <pubDate>Wed, 05 Feb 2025 21:19:09 GMT</pubDate>
    </item>
    <item>
      <title>拟合概率图斜率不是常数的三维逻辑回归</title>
      <link>https://stats.stackexchange.com/questions/661013/fitting-a-3d-logistic-regression-where-the-probability-graphs-slope-is-not-cons</link>
      <description><![CDATA[我尝试使用逻辑回归来预测给定事件在可能的 X 和 Y 输入范围内为 1 的概率 Z。问题是，当我在各个点保持 X 不变时，二维逻辑回归呈现出非常不同的形状，我不知道如何构建一个在这些形状之间正确流动的三维逻辑回归。
我整理了一个简单的演示和一些数据，以更好地说明我想要实现的目标。
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression

# 通用示例数据 -------------------------------------------------
# 暗示逻辑函数的点具有逐渐倾斜的斜率，集中在 y 范围的中心
x1 = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
y1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
z1 = [0, 0, 1, 0, 1, 0, 1, 0, 1, 1]

# 表示斜率较平缓的逻辑函数略偏向 y 范围较大一侧的点
x2 = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
y2 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
z2 = [0, 0, 0, 0, 1, 0, 1, 0, 1, 1]

# 表示斜率较陡的逻辑函数明显偏向 y 范围较大一侧的点
x3 = [10, 10, 10, 10, 10, 10, 10, 10, 10]
y3 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
z3 = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1]

# 绘制 X 的每个值处的逻辑回归图，以显示该点的一般回归形状 ----------------------------
def fit_and_show_logistic_regression(y_data, z_data, title):
y = np.array(y_data).reshape((-1, 1))
z = np.array(z_data)

log_regression = LogisticRegression(penalty=None).fit(y, z)

y_steps = np.arange(0, 10, 0.1).reshape((-1, 1))
z_prediction = log_regression.predict_proba(y_steps)[:, 1]
# 给像我这样的人的提示▼
# [:, 1] 是必需的，因为 predict_proba 返回的列表列表类似于 [prob_of_being_zero, prob_of_being_one]
# [:, 1] 过滤掉 prob_of_being_zero 部分，因此您可以获得整齐的一对一输入和输出

#plt.title(title)
#plt.scatter(y, z)
#plt.plot(y_steps, z_prediction)
#plt.show()

return [y_steps, z_prediction]

x1_regression = [0 for i in range(100)]
y1_regression, z1_regression = fit_and_show_logistic_regression(y1, z1, &quot;显示 x = 0 时逻辑回归的样子&quot;)

x2_regression = [5 for i in range(100)]
y2_regression, z2_regression = fit_and_show_logistic_regression(y2, z2, &quot;显示 x = 5 时逻辑回归的样子&quot;)

x3_regression = [10 for i in range(100)]
y3_regression, z3_regression = fit_and_show_logistic_regression(y3, z3, &quot;显示 x = 10 时逻辑回归的样子&quot;)

# 以 3D 形式一次性显示所有内容 ------------------------------------------------------------------------
fig = plt.figure()
ax = fig.add_subplot(projection=&#39;3d&#39;)
ax.set_xlabel(&#39;X&#39;)
ax.set_ylabel(&#39;Y&#39;)
ax.set_zlabel(&#39;Z&#39;)

all_x = x1 + x2 + x3
all_y = y1 + y2 + y3
all_z = z1 + z2 + z3
ax.scatter(all_x, all_y, all_z)
ax.plot3D(x1_regression, y1_regression, z1_regression)
ax.plot3D(x2_regression, y2_regression, z2_regression)
ax.plot3D(x3_regression, y3_regression, z3_regression)

plt.show()

演示中的数据生成如下所示的 3D 图形：

问题：有没有办法在 3D 空间中执行逻辑回归是否能形成平滑流动的表面？]]></description>
      <guid>https://stats.stackexchange.com/questions/661013/fitting-a-3d-logistic-regression-where-the-probability-graphs-slope-is-not-cons</guid>
      <pubDate>Wed, 05 Feb 2025 20:04:13 GMT</pubDate>
    </item>
    <item>
      <title>无法重现 Garman & Klass 1980 论文中的模拟结果 [关闭]</title>
      <link>https://stats.stackexchange.com/questions/661012/unable-to-reproduce-simulation-results-from-garman-klass-1980-paper</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/661012/unable-to-reproduce-simulation-results-from-garman-klass-1980-paper</guid>
      <pubDate>Wed, 05 Feb 2025 19:43:02 GMT</pubDate>
    </item>
    <item>
      <title>处理高维数据集时如何处理特征选择？</title>
      <link>https://stats.stackexchange.com/questions/661011/how-do-you-deal-with-feature-selection-when-working-with-high-dimensional-datase</link>
      <description><![CDATA[我一直在研究一个包含数千个特征的分类问题，在特征选择方面遇到了困难。我发现这篇文章对不同的特征选择技术等进行了很好的细分。PCA 似乎有助于降低维数，但我担心会失去可解释性。递归特征消除 (RFE) 是一种选择，但计算成本很高。在高维数据集中，是否有任何最佳实践或被忽视的特征选择技术？]]></description>
      <guid>https://stats.stackexchange.com/questions/661011/how-do-you-deal-with-feature-selection-when-working-with-high-dimensional-datase</guid>
      <pubDate>Wed, 05 Feb 2025 19:28:46 GMT</pubDate>
    </item>
    <item>
      <title>平滑的 AIC 选择</title>
      <link>https://stats.stackexchange.com/questions/661003/smooth-aic-selection</link>
      <description><![CDATA[假设我有一组针对相同数据的 $N$ 个模型，索引为 $n\in\{1,\dots,N\}$。
假设模型 $n\in\{1,\dots,N\}$ 具有以下对数似然：
$$L(X_n \theta_n),$$
其中 $L:\mathbb{R}^J\rightarrow \mathbb{R}$ 是某个已知函数，$X_n\in \mathbb{R}^{J\times K_n}$ 是解释变量矩阵，并且 $\theta_n\in\mathbb{R}^{K_n}$ 是模型的参数向量。
最后假设我已经通过最大似然估计了所有 $N$ 个模型，给出了估计值 $\hat{\theta}_1,\dots,\hat{\theta}_N$。
我现在想要进行模型选择。标准 AIC 方法是选择最大化的方法：
$$L(X_n\hat{\theta}_n)-K_n.$$
但假设我想平滑地选择，保持数据的连续性。
一种可能性是选择 $w_1,\dots,w_n\ge 0$ 来最大化：
$$L(w_1 X_1\hat{\theta}_1+\dots+w_N X_N\hat{\theta}_N)-w_1 K_1 -\dots-w_N K_N$$
受 $w_1+\dots+w_N=1$ 约束。 （至关重要的是，我没有重新估计$\hat{\theta}_1,\dots,\hat{\theta}_N$，从而降低了过度拟合的可能性。）
此过程在先前的文献中是否已被研究过？它有名字吗？我估计$w_1,\dots,w_N$是否会导致过度拟合？或者它最终会像人们希望的那样偏向更简单的模型。

旁白（？）：在我的确切情况下，也存在矩阵$A_1,\dots, A_{N-1}$，使得对于所有$n\in\{2,\dots,N\}$，$X_{n-1}=X_n A_{n-1}$，这意味着较低指标模型的通用性不如较高指标模型。但我不认为这从根本上简化了问题。]]></description>
      <guid>https://stats.stackexchange.com/questions/661003/smooth-aic-selection</guid>
      <pubDate>Wed, 05 Feb 2025 16:57:04 GMT</pubDate>
    </item>
    <item>
      <title>对模拟数据重复 Anderson-Darling 检验（R） - 为什么 p 值不一致？</title>
      <link>https://stats.stackexchange.com/questions/661001/repeating-anderson-darling-test-on-simulated-data-r-why-are-p-values-not-uni</link>
      <description><![CDATA[我重复一个实验 k 次：从标准正态分布中生成 n 个值，然后对每个样本运行 AD 测试。然后，我绘制了所有 k 个实验的 p 值分布。令我惊讶的是，它并不均匀，而是在主体中出现了相当奇怪的尖峰。
set.seed(1)
mtx = matrix(rnorm(2e4 * 1e4), nrow=2e4) # n=2e4; k=1e4
ad_pval = apply(mtx, 2, \(x) nortest::ad.test(x)$p.value) # 需要几分钟！
hist(ad_pval, breaks = 50)


k 和 n 都很大。如果我使用不同的 k（例如 50k）和 n，则图像相同。
我期望 p 值具有均匀分布，因为这些是在正态性假设下生成的统计数据的某些理论 AD 分布的百分位数。
这似乎不是模拟错误。ad.test 函数不再需要其他参数。我遗漏了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/661001/repeating-anderson-darling-test-on-simulated-data-r-why-are-p-values-not-uni</guid>
      <pubDate>Wed, 05 Feb 2025 16:50:52 GMT</pubDate>
    </item>
    <item>
      <title>当 $y$ 已从 $x$ 计算出来时进行回归</title>
      <link>https://stats.stackexchange.com/questions/660990/regression-when-y-has-been-calculated-from-x</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/660990/regression-when-y-has-been-calculated-from-x</guid>
      <pubDate>Wed, 05 Feb 2025 12:43:17 GMT</pubDate>
    </item>
    </channel>
</rss>