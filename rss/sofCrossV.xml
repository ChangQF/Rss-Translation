<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Fri, 26 Apr 2024 03:15:52 GMT</lastBuildDate>
    <item>
      <title>关于比较不同性别犯罪群体的分析类型的建议</title>
      <link>https://stats.stackexchange.com/questions/645868/advice-on-the-type-of-analysis-for-comparing-offending-groups-that-are-distinct</link>
      <description><![CDATA[
非常感谢任何帮助！提前非常感谢您。]]></description>
      <guid>https://stats.stackexchange.com/questions/645868/advice-on-the-type-of-analysis-for-comparing-offending-groups-that-are-distinct</guid>
      <pubDate>Fri, 26 Apr 2024 02:56:59 GMT</pubDate>
    </item>
    <item>
      <title>“单样本测试”是什么意思？下面显然是 wilcox.test 的两样本测试</title>
      <link>https://stats.stackexchange.com/questions/645864/what-is-the-meaning-of-one-sample-test-the-following-is-clearly-a-two-sample</link>
      <description><![CDATA[激发这个问题的代码可以找到这里和下面：
需要（图形）
## 单样本测试。
## 霍兰德 &amp;沃尔夫（1973），29f。
## 9 名患有抑郁症的患者的汉密尔顿抑郁量表因子测量
## 混合焦虑和抑郁，在第一次 (x) 和第二次测量
## (y) 开始治疗后的访视（施用药物）
##镇静剂）。
x＜-c(1.83、0.50、1.62、2.48、1.68、1.88、1.55、3.06、1.30)
y &lt;- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)
wilcox.test(x, y, 配对 = TRUE, 替代 = “更大”)
wilcox.test(y - x, Alternative = &quot;less&quot;) # 相同。
wilcox.test（y - x，替代=“更少”，
            准确 = FALSE，正确 = FALSE) # H&amp;W 大样本
                                            # 近似值

“单样本测试”是什么意思？下面显然是两个样本的测试。]]></description>
      <guid>https://stats.stackexchange.com/questions/645864/what-is-the-meaning-of-one-sample-test-the-following-is-clearly-a-two-sample</guid>
      <pubDate>Fri, 26 Apr 2024 02:08:14 GMT</pubDate>
    </item>
    <item>
      <title>留一交叉验证</title>
      <link>https://stats.stackexchange.com/questions/645857/leave-one-in-cross-validation</link>
      <description><![CDATA[我试图证明我的模型非常稳健。
给出了 $N$-许多 x-y 对作为样本。
迭代所有样本，对于每个样本，仅使用该单个样本来训练模型。
然后使用该模型来预测所有其他样本。
对于每个预测，都会记录错误（生成 $N-1$ 错误值）。
总共，这会产生 $N \cdot (N-1)$ 错误。
然后，所有这些错误均通过 RMSE 进行聚合。
由于结果接近使用所有数据训练模型的 RMSE，因此我称该模型为稳健模型。
由于缺乏更好的词，我将其描述为留一交叉验证。
这是一个已知的想法吗？
&lt;小时/&gt;
我意识到这是一个不寻常的情况。
为了澄清起见，举一个例子。
假设有 5 个样本，编号为 1 到 5。
使用样本 1 训练模型。计算模型在样本 2 至 5 上的误差。
然后用样本 2 训练一个新模型，并计算其在样本 1、3、4 和 5 上的误差。
其他 3 个样本的程序相同，每个样本都有自己的模型。
然后，对所有误差进行平方、平均和开方，从而得到 RMSE。
此 RMSE 仅比 LOO CV RMSE 或完全训练的模型差一点（差别不大）。]]></description>
      <guid>https://stats.stackexchange.com/questions/645857/leave-one-in-cross-validation</guid>
      <pubDate>Thu, 25 Apr 2024 23:59:55 GMT</pubDate>
    </item>
    <item>
      <title>当 CFA 模型通过忽略一个因素而显着改善时，这意味着什么？</title>
      <link>https://stats.stackexchange.com/questions/645853/what-does-it-mean-when-a-cfa-model-improves-drastically-by-ommiting-a-factor</link>
      <description><![CDATA[我试图在 Amos 中拟合一个具有四个因素的 CFA 模型（例如，F1 有 4 个指标，F2 有 6 个指标，F3 有 3 个指标，F4 有 5 个指标）。每个指标都是通过对问卷中的一些李克特量表项目求和计算得出的。样本量为 $n = 451$，指标呈近似正态分布。未观察到共线性的迹象。删除4个不显著/低因子载荷指标后的拟合指数为：

CMIN/df = 7.824
CFI = .913
TLI = .889
NFI = .902
RMSEA = .123
SRMR = .060.

但是我发现删除F3后拟合指数变为：

CMIN/df = 4.054
CFI = .972
TLI = .962
NFI =.963
RMSEA =.082
SRMR =.033

所有 F3 指标的因子载荷值均大于 .7，且标准误差较小。这该如何解释？]]></description>
      <guid>https://stats.stackexchange.com/questions/645853/what-does-it-mean-when-a-cfa-model-improves-drastically-by-ommiting-a-factor</guid>
      <pubDate>Thu, 25 Apr 2024 21:23:21 GMT</pubDate>
    </item>
    <item>
      <title>中值回归</title>
      <link>https://stats.stackexchange.com/questions/645852/median-regression</link>
      <description><![CDATA[我试图理解为什么我使用 R median 和 rq (quantreg) 函数估计中值时得到不同的结果。 
这是一个让我困惑的例子：
PC &lt;- gl(3, 10, 标签 = 字母[1:3])

PU &lt;- c(rlnorm(10,meanlog = 8,sdlog = 1),
        rlnorm(10,均值对数 = 9,sdlog = 1),
        rlnorm(10, 平均对数 = 10, sdlog = 1))

df &lt;- data.frame(PU, PC)

tapply(df$PU, df$PC, 平均值)
tapply(df$PU, df$PC, 中位数)

LinFit &lt;- lm(PU~PC-1, 数据 = df)
摘要(LinFit) # Coefs 与获得的tapply 相同(df$PU, df$PC,mean)

rqFit &lt;- rq(PU~PC-1, tau = .5, 数据 = df)
摘要(rqFit) # 系数与tapply估计的中位数不同(df$PU, df$PC,median)
```
]]></description>
      <guid>https://stats.stackexchange.com/questions/645852/median-regression</guid>
      <pubDate>Thu, 25 Apr 2024 21:19:25 GMT</pubDate>
    </item>
    <item>
      <title>机器学习基准：MAE、RMSE 和 R 平方</title>
      <link>https://stats.stackexchange.com/questions/645848/machine-learning-benchmarks-mae-rmse-and-r-squared</link>
      <description><![CDATA[我正在研究机器学习问题，但在解释模型性能的不同衡量标准时遇到困难。我使用三种方法建模的数据集中有一个因变量（两种治疗之间的比例变化，范围从 -1 到 1，但大多接近 0）：1）XGboost，2）简单的神经网络，3 ) 一个空模型（用 mlr 的说法来说就是“无特征”）。为了比较模型的性能，我比较了数据集中重复随机折叠的样本外模型性能，并具有如下所示的平均绝对误差 (MAE)、均方根误差 (RMSE) 和 R 平方，全部用于数据集的独立测试折叠：

正如您所看到的，MAE 分数显示两个模型并不比空模型（无特征）更好，但 RMSE 和 R 平方显示相反的结果（XGboost 和 NNet 优于空模型）。我读过有关 RMSE 和 MAE 之间差异的其他文章 (此处，此处，以及此处），我很确定这与均值误差和中值误差之间的差异有关，其中误差是倾斜的或不对称的（即 Jensen 不等式）。
我应该如何正确解释这些基准测试的结果？ XGboost 和 NNet 模型“真正”有效吗？比空模型更好，还是我错过了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/645848/machine-learning-benchmarks-mae-rmse-and-r-squared</guid>
      <pubDate>Thu, 25 Apr 2024 20:07:20 GMT</pubDate>
    </item>
    <item>
      <title>方差分析与 Kruskal Wallis - 小样本量</title>
      <link>https://stats.stackexchange.com/questions/645845/anova-vs-kruskal-wallis-small-sample-size</link>
      <description><![CDATA[我有一项比较 4 个独立处理组的植物重量的实验数据。数据似乎呈正态分布（有人警告我使用统计检验来检验正态性）。但是，我的样本量非常小（总共 $n=16$；$n=4$每组）。 Kruskal-Wallis 检验对于小样本量是否更稳健，或者方差分析是否同样稳健？]]></description>
      <guid>https://stats.stackexchange.com/questions/645845/anova-vs-kruskal-wallis-small-sample-size</guid>
      <pubDate>Thu, 25 Apr 2024 19:39:47 GMT</pubDate>
    </item>
    <item>
      <title>了解 lme4 输出：意外的不同结果</title>
      <link>https://stats.stackexchange.com/questions/645838/understanding-lme4-output-unexpected-different-results</link>
      <description><![CDATA[我正在自学如何在 R 中做多层模型 (MLM)。我有两个模型，我认为这应该给我相同的信息（在 M2 中遗漏了一些），但它们并不完全相同。我希望你能帮忙解释一下差异从何而来。数据并不重要，所以这里是模型：
M1：
lmer(声音 ~ 1 + neg_ttbi*con_bin + (1 |pair_id) + (1 | id) + (1 | round), REML = FALSE,
                数据=df[其中(df$win == 1),])

M1固定效果：
修复效果：
                               估计标准。误差df t值Pr(&gt;|t|)
（截距）3.57319 0.39129 58.83567 9.132 7.07e-13 ***
neg_ttbi 2.38973 0.41013 1522.73067 5.827 6.89e-09 ***
con_binforced_break 0.04115 0.44161 57.23054 0.093 0.926077
neg_ttbi：con_binforced_break -1.97734 0.52579 1503.80584 -3.761 0.000176 ***

我对M1的解读：

neg_ttbi 与声音之间存在正相关性
forced_break 条件对
声音
neg_ttbi 和 forced_break 之间存在交互作用
影响声音

M2：
lmer(声音 ~ 1 + neg_ttbi:con_bin + (1 |pair_id) + (1 | id) + (1 | round), REML = FALSE,
                数据=df[其中(df$win == 1),])

M2固定效果：
修复效果：
                              估计标准。误差df t值Pr(&gt;|t|)
(截距) 3.6053 0.1865 58.1482 19.333 ＜ 2e-16 ***
neg_ttbi:con_bincontrol 2.3823 0.4025 1536.2505 5.920 3.97e-09 ***
neg_ttbi:con_binforced_break 0.4147 0.3286 1471.6785 1.262 0.207

我对M2的解读：

neg_ttbi 和 control 之间存在交互
影响声音
neg_ttbi 和 forced_break 之间不存在交互作用
影响声音

我的问题：

为什么 M2 为我提供了 con_bin 每个级别的数字，而 M1 却没有？
为什么 M1 和 M2 中的 neg_ttbi:con_binforced_break 不同？
M2 告诉了我哪些 M1 没有告诉我的信息？
如何制作一个模型来告诉我 neg_ttbi、con_bin 的两个级别的影响，以及 neg_ttbi 与两者之间的交互con_bin 的级别（截距 + 下面 5 行数据）？ （或者，为什么这样做没有意义？）

如果这些问题揭示了根本性的误解，我很高兴被推荐参考文章。我怀疑我在尝试使用固定的、独立的分组因素时做了一些不正确的事情。]]></description>
      <guid>https://stats.stackexchange.com/questions/645838/understanding-lme4-output-unexpected-different-results</guid>
      <pubDate>Thu, 25 Apr 2024 17:29:32 GMT</pubDate>
    </item>
    <item>
      <title>后续步骤 - 变量不能是固定的随机效应？</title>
      <link>https://stats.stackexchange.com/questions/645830/next-steps-variable-cant-be-a-fixed-and-random-effect</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/645830/next-steps-variable-cant-be-a-fixed-and-random-effect</guid>
      <pubDate>Thu, 25 Apr 2024 15:46:34 GMT</pubDate>
    </item>
    <item>
      <title>$[0,1]$ 上标准均匀分布与其他分布之间 1-Wasserstein 距离的上限</title>
      <link>https://stats.stackexchange.com/questions/645817/upper-bound-for-1-wasserstein-distance-between-standard-uniform-and-other-distri</link>
      <description><![CDATA[我想使用以下指标来测量 $[0,1]$ 上标准均匀分布与任何其他概率分布之间的距离。
$$\int_0^1 |F(x) - x| dx$$
$F(x)$ 是 $[0,1]$ 和 $x$ 是标准均匀分布的 cdf。这本质上是 1-Wasserstein 距离。我想要证明该距离的上限为 $1/2$ 或显示 $\int_0^1 |F( x) - x| dx \leq 1/2$，但我无法想出任何东西。有人知道如何证明这一点吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/645817/upper-bound-for-1-wasserstein-distance-between-standard-uniform-and-other-distri</guid>
      <pubDate>Thu, 25 Apr 2024 13:30:20 GMT</pubDate>
    </item>
    <item>
      <title>GLM 相对于转换模型的优势</title>
      <link>https://stats.stackexchange.com/questions/645797/advantage-of-glms-over-transformation-models</link>
      <description><![CDATA[根据我目前正在阅读的书，我们应该更喜欢 GLM 而不是简单的转换模型（例如 $\log(y) = x_i^T \beta + u_i$ ）该论证是由 Jensen 不等式导出的：
$$
E(\log(\text{y}) \mid x) \leq \log E(\text{y} \mid x)
$$
$$
\exp\{E(\log(\text{y}) \mid x)\} \leq E(\text{y} \mid x)
$$
我不确定我是否真正理解了不平等的要点。
到目前为止我的想法：
我们真正感兴趣的通常是 $E(y | x)$。这可能就是为什么我们采用 $\exp()$ 来得出第二个不等式。但左侧只有在应用 GLM 时才可行，而我在转换模型时遇到了麻烦。这可能是指众所周知的问题，当我用转换模型得出结果时，我不能简单地采用 $\exp()$ ，对吗？因此，如果我使用 GLM 进行回归，我可以确定我的结果等于或低于真实期望吗？
欢迎任何意见！]]></description>
      <guid>https://stats.stackexchange.com/questions/645797/advantage-of-glms-over-transformation-models</guid>
      <pubDate>Thu, 25 Apr 2024 10:46:09 GMT</pubDate>
    </item>
    <item>
      <title>这个统计假设有多容易理解？</title>
      <link>https://stats.stackexchange.com/questions/645744/how-understandable-is-this-statistical-hypothesis</link>
      <description><![CDATA[我正在尝试将一个统计假设翻译成英文，作为提交同行评审论文的一部分，但不确定我提出的翻译是否正确。
“假设 XYZ 维度内的变量测量值不存在统计显着性差异”这有道理吗？
我问过一些精通统计的人，但他们都不是母语人士。基本上，问题是我不能 100% 确定“变量的测量”是否正确。是一个有效的术语还是多余的等等。
为了向您提供更多见解，以下是有关如何进行此特定假设分析的翻译部分：
对于回答研究子问题的分析，通过评估受访者答案的数量和百分比分布来进行频率分析。然后通过 Shapiro-Wilk 正态性检验对数据分布的正态性进行详细分析，结果表明数据不符合高斯曲线正态分布。鉴于受访者数量相对较少且顺序响应范围广泛，这一结果在意料之中。]]></description>
      <guid>https://stats.stackexchange.com/questions/645744/how-understandable-is-this-statistical-hypothesis</guid>
      <pubDate>Wed, 24 Apr 2024 15:51:30 GMT</pubDate>
    </item>
    <item>
      <title>Wilcoxon 可以用于非正态分布的大样本吗？</title>
      <link>https://stats.stackexchange.com/questions/645703/can-wilcoxon-be-used-in-large-sample-with-non-normal-distribution</link>
      <description><![CDATA[我正在做本科研究，旨在了解干预前后的差异。
我们的样本量是 37，这已经算是一个大样本了，对吧？
然而，当我们测试数据的正态性时，它是不正常的，这违反了假设。
使用 Wilcoxon 签名检验或进行 t 检验等参数检验是否正确？]]></description>
      <guid>https://stats.stackexchange.com/questions/645703/can-wilcoxon-be-used-in-large-sample-with-non-normal-distribution</guid>
      <pubDate>Wed, 24 Apr 2024 03:45:18 GMT</pubDate>
    </item>
    <item>
      <title>为什么系统 GMM 由于我的设置中的计算单一系统而失败？</title>
      <link>https://stats.stackexchange.com/questions/645610/why-does-system-gmm-fail-due-to-computationally-singular-system-in-my-setup</link>
      <description><![CDATA[我正在用 R 中的 gmm::sysGmm 估计一个看似不相关的回归 (SUR) 系统。每个方程都有一个唯一的回归量和一个公共回归量。公共回归量是一个虚拟变量，指示最后的观察结果。我施加了一个限制，即公共回归量的系数在方程中相等。拟合例程会产生以下错误：
solve.default(w, t(hm)) 中出现错误：
  系统在计算上是奇异的：倒数条件数 = 3.35459e-19

这种情况仅发生在某些类型的协方差矩阵估计器上：MDS 和 HAC。 CondHom 不会发生这种情况。此外，我还可以通过 systemfit::systemfit 使用受限 OLS 估计来毫无问题地估计模型。我认为后一个估计器在代数上与 gmm::sysGmm 相同，具有单位权重矩阵和误差协方差矩阵的 CondHom 选项。这表明问题在于考虑误差异方差性时估计误差协方差矩阵。这个问题到底是什么？
我可以理解，如果我不对公共回归量施加系数相等的交叉方程限制，就会出现问题。然后，每个方程的最后一个误差将被估计为零，这将扰乱误差异方差下的估计例程。而且，如果在 GMM 的第一阶段使用这样的估计器，则会在那里造成麻烦。不过，对于GMM中的恒等权重矩阵，我不希望在第一阶段使用它。
问题：为什么系统 GMM 会因我的设置中的计算奇异系统错误而失败？我可以做什么来解决这个问题？
库(gmm)

# 生成并准备数据
n &lt;- 100 # 样本量
m＜-1#“第二部分”的长度。样本的
N &lt;- 3 # 方程数
设置.种子(321); x &lt;- 矩阵(rnorm(n*N),ncol=N); colnames(x) &lt;- Paste0(“x”,1:N) # 生成回归量
dummy &lt;- c(rep(0,n-m),rep(1,m))#生成一个公共回归器
x &lt;- cbind(x,dummy) # 包含公共回归器和其余回归器
设置.种子(123); y &lt;- 矩阵(rnorm(n*N),ncol=N); colnames(y) &lt;-paste0(“y”,1:N) # 因变量的占位符
for(i in 1:N){
 y[,i] &lt;- i + sqrt(i)*x[,i] - i*虚拟 + y[,i]*15*sqrt(i)
 # y[,i] 是 x[,i] 和虚拟变量的线性函数，
 # 加上一个带有方程特定方差的误差项
}
data1 &lt;- as.data.frame(cbind(y,x)) # 创建所有数据（y 和 x）的数据框

# 创建模型方程和力矩条件
ES_g = eqSystem_h &lt;- list()
for(i in 1:N){
 ES_g[[i]] &lt;- as.formula(assign(paste0(&quot;eq&quot;,i), value=paste0(&quot;y&quot;,i,&quot; ~ x&quot;,i,&quot; + dummy&quot;) )) # 定义 SUR 的线性方程
 ES_h[[i]] &lt;- as.formula(assign(paste0(&quot;eq&quot;,i), value=paste0( &quot;~ x&quot;,i,&quot; + dummy&quot;))) # 定义矩条件对于高斯模型
}

# 使用不同的权重矩阵“sysGmm”估计模型：identity、“optimal”并手动指定
m1 &lt;- sysGmm(g=ES_g, h=ES_h, wmatrix=“ident”, vcov=“MDS” , crossEquConst=3, data=data1) # 错误：系统在计算上是奇异的
m1 &lt;- sysGmm(g=ES_g, h=ES_h, wmatrix=“ident”, vcov=“HAC” , crossEquConst=3, data=data1) # 错误：系统在计算上是奇异的
m1 &lt;- sysGmm(g=ES_g, h=ES_h, wmatrix=“ident”, vcov=“CondHom”, crossEquConst=3, data=data1) # 就可以了
摘要(m1)

相关：
* 系统 GMM 对于任何加权矩阵都会产生相同的结果 
* 通过 systemfit 与 sysGmm 估计的 SUR：不同的标准误差 ]]></description>
      <guid>https://stats.stackexchange.com/questions/645610/why-does-system-gmm-fail-due-to-computationally-singular-system-in-my-setup</guid>
      <pubDate>Tue, 23 Apr 2024 08:17:25 GMT</pubDate>
    </item>
    <item>
      <title>交叉验证的贝叶斯论证</title>
      <link>https://stats.stackexchange.com/questions/645577/bayesian-justification-of-cross-validation</link>
      <description><![CDATA[如果我理解正确的话，K 折交叉验证应该近似预期的对数预测密度（ELPD），其定义为 $\mathop{\mathbb{E}} _{D_{new}\sim P(.|M_{true})}\log P(D_{new}|D_{observed},M)$，对于模型 $M$，并假设存在“true”数据生成过程$M_{true}$（我们不知道它是什么）。这用于衡量模型 $M$ 的好坏程度。对此，我有两个问题：

在这个期望中，为什么我们使用新数据的似然对数，即 $\log P(D_{new}|D_{observed}, M)$ 而不是新的后验，即 $\log P(M|D_{new},D_{observed})$。前者如何“更好”，无论这意味着什么？有没有使用过后者的来源？

更一般地说，ELPD 或其使用交叉验证的近似值是比使用我们所有数据计算的纯后验更好的模型选择度量，其数学依据是什么？贝叶斯后验唯一地源自一组一致性标准，任何其他度量都严格低于它（至少当我们只关心这些一致性标准时）。我认为当我们无法充分量化我们的先验知识时，ELPD 和交叉验证的有用性通常会体现出来。但我的这种理解纯粹是基于直觉，我什至不太相信它。对此有更严格的处理吗？例如，是否可以从数学上证明 ELPD 在某种程度上比使用不准确的先验计算出的后验结果更好？

]]></description>
      <guid>https://stats.stackexchange.com/questions/645577/bayesian-justification-of-cross-validation</guid>
      <pubDate>Mon, 22 Apr 2024 20:33:29 GMT</pubDate>
    </item>
    </channel>
</rss>