<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Fri, 22 Nov 2024 03:29:28 GMT</lastBuildDate>
    <item>
      <title>对偶自然对数模型中的过度拟合</title>
      <link>https://stats.stackexchange.com/questions/657652/overfitting-in-a-dual-natural-logarithm-model</link>
      <description><![CDATA[我有这个函数
$$y = a \ln(p(t + 32)) - b \ln(q(t+30))$$
其中：

$t,y$ 是测量值
$a, b, p$ 和 $q$ 是拟合值
$30$ 和 $32$ 是偏移项

我使用此函数拟合质谱数据，其中 $y$ 是强度，$t$ 是气体平衡后的时间。目标是尽可能准确地测量 $t=0$ 时的 $y$。
使用标准曲线拟合程序，特别是 scipy.optimize.curve_fit，在绝大多数情况下会产生理想的结果。示例：
函数：
 def natural_log_func(self, t, a, b, p, q):
return a * np.log(p*(t + self.pos_lneq_time)) - b * np.log(q*(t + self.neg_lneq_time))

def fit(self):
return curve_fit(self.natural_log_func, self.t, self.y, maxfev=100000)


然而，在少数情况下，我看到了过度拟合：

其中数据是：
7.083,3.63E-12
14.291,3.49E-12
21.494,3.48E-12
28.709,3.59E-12
35.915,3.79E-12
43.02,3.60E-12
50.226,3.80E-12
57.431,3.84E-12
64.639,3.76E-12
71.846,3.79E-12
79.05 6,3.84E-12
86.262,3.71E-12
93.467,4.05E-12
100.578,3.93E-12
107.783,4.12E-12
114.992,4.01E-12
122.2,4.12E-12
129.403,4.30E-12
136.606,4.12E-12
143.812,4.28E-12

另一个两个例子：

其中数据为：
$t$ = 7.208,14.411,21.624,28.833,36.038,43.142,50.347,57.553,64.759,71.964,79.173,86.378,93.583,100.785,107.892,115.095,122.303,129.51,136.711,143.92
4 amu = 5.48E-14,3.60E-14,2.75E-14,5.22E-14,5.87E-14,5.14E-14,5.70E-14,5.17E-14,5.41E-14,5.53E-14,4.58E-14,7.52E-14,4.85E-14,7.06E-14,9.48E-14,6.42E-14,8.56E-14,6.71E-14,8.48E-14,9.84E-14
40 amu = 3.66E-12,3.73E-12,3.94E-12,3.94E-12,4.02E-12,3.92E-12,3.90E-12,4.09E-12,4.18E-12,4.17E-12,4.24E-12,4.13E-12,4.35E-12,4.32E-12,4.51E-12,4.41E-12,4.74E-12,4.62E-12,4.68E-12,4.64E-12
我可以做些什么来防止这些“骤降”在低$t$值时形成？]]></description>
      <guid>https://stats.stackexchange.com/questions/657652/overfitting-in-a-dual-natural-logarithm-model</guid>
      <pubDate>Fri, 22 Nov 2024 00:53:34 GMT</pubDate>
    </item>
    <item>
      <title>rugarch 包中条件方差的初始值如何计算</title>
      <link>https://stats.stackexchange.com/questions/657651/how-are-initial-values-of-conditional-variance-calculated-in-rugarch-package</link>
      <description><![CDATA[我正在尝试使用 rugarch 库验证我的 0 均值 GARCH(1,1) 模型的计算，起初我认为条件方差的初始值与非条件方差（python 中的 arch 包使用）是相同的值，但当我重新检查条件方差的第一个初始值时，它与非条件方差不同。
我尝试使用计算器手动计算它，它给了我与 uncvariance(garch_fit) 相同的输出，这不是来自模型的条件方差的初始值。]]></description>
      <guid>https://stats.stackexchange.com/questions/657651/how-are-initial-values-of-conditional-variance-calculated-in-rugarch-package</guid>
      <pubDate>Fri, 22 Nov 2024 00:48:50 GMT</pubDate>
    </item>
    <item>
      <title>时间序列模型和平稳性</title>
      <link>https://stats.stackexchange.com/questions/657649/time-series-model-and-stationarity</link>
      <description><![CDATA[考虑一个非常简单的模型，该模型预测$y_t$作为$x_t$上的函数，使用真实数据构建，如下所示：
$y_t = a + b.x_t$
对于这样的系统，我们需要$y_t$和$x_t$都是I(0)，但这里我谈论的是具有真实噪声数据的实际应用。在我的例子中，使用ADF、PP和KPSS测试，$y_t$绝对是平稳的。问题是$x_t$“看起来”平稳且“直观地”是平稳的，但根据用于构建我的模型的时间范围，它在 3 次平稳性测试中没有通过 2 次。
如果我扩大范围来评估平稳性，那么 $x_t$ 会通过 2 或 3 次测试，这意味着从长期来看 $x_t$ 是平稳的。请注意，如果我将范围从 1980 年推至 2024 年，它将再次变为非平稳。
问题：

扩大范围以评估平稳性的论点是否有效，以支持使用 $x_t$？

有关于这个主题的文献吗？

如果可以在与用于构建有效模型的范围不同的范围内评估平稳性，那么我如何将其与回归系数不变的事实相协调，同时论证 $x_t$ 在较长时期内是平稳的，甚至 $x_t$ 是“直观地”平稳的。因此，如何通过论证模型在数学上合理？

]]></description>
      <guid>https://stats.stackexchange.com/questions/657649/time-series-model-and-stationarity</guid>
      <pubDate>Fri, 22 Nov 2024 00:14:27 GMT</pubDate>
    </item>
    <item>
      <title>我应该为梦幻英超联赛模型预测个人数据还是整体得分？</title>
      <link>https://stats.stackexchange.com/questions/657648/should-i-predict-individual-stats-or-points-as-a-whole-for-a-fantasy-premier-lea</link>
      <description><![CDATA[我正在做一项大学作业，目的是使用历史比赛数据预测梦幻英超联赛球员的得分。在 FPL 中，球员根据各种比赛统计数据（例如，上场时间、进球、助攻、零封）获得积分。得分公式很简单，但因位置而异（例如，后卫零封对手的得分高于中场球员）。
我的方法是使用 XGBoost 为各个统计数据（例如，助攻、扑救）训练单独的模型，然后结合这些预测，使用已知方程计算得分。例如：
预测预期助攻的特征：
player_id、gameweek、value、home_crowd_effect、opponent_defense_strength、own_attack_strength、rolling_xa_5、season、position
预测扑救的特征：
player_id、gameweek、value、home_crowd_effect、opponent_attack_strength、own_defense_strength、rolling_saves_5、season
这与我在网上看到的大多数例子不同，人们只训练一个模型来直接预测得分。例如，在这篇文章中，他们训练了一个积分模型，并得到了 ~0.74 的 R 平方。另一方面，我的方法似乎无法超过 ~0.33，这感觉差距很大。
现在我想知道：

使用多个模型来处理单个统计数据而不是使用单个模型来处理积分，是否有理论或实践优势？
是否有任何资源或研究论文详细讨论这种建模权衡？我还没有找到任何关于 Fantasy Premier League 的资源或研究论文，但也许同样的情况可能发生在不同的问题中。
]]></description>
      <guid>https://stats.stackexchange.com/questions/657648/should-i-predict-individual-stats-or-points-as-a-whole-for-a-fantasy-premier-lea</guid>
      <pubDate>Thu, 21 Nov 2024 23:49:32 GMT</pubDate>
    </item>
    <item>
      <title>对物理学家来说有效的逻辑分析？</title>
      <link>https://stats.stackexchange.com/questions/657647/valid-logistic-analysis-for-physicists</link>
      <description><![CDATA[我是一名物理学家，正在撰写一篇论文，论文内容涉及蒙特卡罗引擎计算辐射场中击中目标的辐射量的结果。有趣的是，虽然结果通常是准确的，但在某些情况下，结果会大相径庭。据我们所知，没有确切的参数集会导致错误，但有些参数集会使错误更有可能发生 - 例如，将计算网格大小设置为更粗会导致这些错误更有可能发生，但这种情况并不总是发生，即使计算网格大小很细，也可能发生。
我现在有大约 20,000 个来自各种计算的数据点。对于每次计算，我都记录了大约 15 个不同输入参数的值。我稍微知道哪些与输出变量相关，即此计算与已知对几何正确的基准情况之间的计算辐射剂量差异，但许多我不确定它们会如何影响输出变量。我知道我的许多变量是相关的。我敢打赌我的一些变量是多重共线性的。
该项目的最终目标是为在放射治疗领域使用这些计算提供粗略的经验法则 - 也就是说，让计划进行放射治疗的人可以遵循一些规则，例如“当目标这么大，或者离另一个东西这么近时，那么你必须像这样设置参数 Y 以降低计算错误的概率”。为了获得这些，我试图从输入参数创建一个逻辑回归模型，这样我就可以说“当使用参数集 Y、Z、W 时，计算错误的几率小于 X”。
我不知道最好的解决方法。目前，我正尝试通过迭代计算 VIF 来消除多重共线性，然后删除具有最高 VIF 的解释变量，重复此过程，然后，一旦所有变量的 VIF 都低于 10，就对所有剩余的变量组合运行 BIC 以找到最简约的集合，然后通过使用该集合创建逻辑模型（当然是缩放和居中数据）来创建我的经验法则，然后根据我的逻辑模型检查参数空间的哪些区域具有最低概率。
但是，这个想法只是我从各种 CrossValidated 帖子和阅读教科书的片段中拼凑起来的。真的，我不知道最好的方法是什么。
对于不太了解统计学的物理学家来说，什么方法最能防错？获得我想要的最终结果（经验法则）的正确方法是什么？
我确实读过此处的帖子，该帖子与此相关，但并未明确给出前进的方向。我知道“进行统计分析的最佳方法”这一想法将极具争议，但我主要在寻找对我当前方法的批评、对统计学以外专业的人提出的防错方法的建议以及进一步的阅读。
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/657647/valid-logistic-analysis-for-physicists</guid>
      <pubDate>Thu, 21 Nov 2024 23:35:22 GMT</pubDate>
    </item>
    <item>
      <title>作业数据挖掘/需要帮助[关闭]</title>
      <link>https://stats.stackexchange.com/questions/657646/assignment-data-mining-need-help</link>
      <description><![CDATA[希望你们都很好。
我迫切需要你们中一个对数据挖掘有所了解的人的帮助，足以帮助我回答作业上的问题……几天后我必须向我的数据挖掘老师提交一份作业，但我真的很难决定我做得对不对。我担心我的所有结果都是错的，我会得到一个很差的分数……
我附上了作业说明和包含完成作业所需的所有数据的文件。我无法在此处附上我的 Jupyter Notebook，因为格式不起作用，但如果您要求我这样做，我很乐意通过电子邮件或其他方式将其发送给您。
我真的需要帮助，我本学期的成功真的取决于我这份作业的分数。有人能帮我吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/657646/assignment-data-mining-need-help</guid>
      <pubDate>Thu, 21 Nov 2024 23:18:20 GMT</pubDate>
    </item>
    <item>
      <title>使用 Hausman-McFadden 检验结果解决不相关替代独立性 (IIA) 偏差的有效性</title>
      <link>https://stats.stackexchange.com/questions/657645/validity-of-using-hausman-mcfadden-test-results-for-addressing-departures-from-t</link>
      <description><![CDATA[上下文正在运行多项 Logit 模型，主要目的是衡量案例特定变量 A 对多项结果 B 的影响。运行 Hausman-McFadden 检验显示完整模型与 B 的替代方案减少（违反 IIA 的程度）的每个模型有何不同。理想情况下，系数不会改变。我的问题是，使用这些测试和比较来显示违反 IIA 的程度是否足够，同时论证在这些变化中持续存在的影响仍然有效？例如，假设在 5 个系数中，4 个没有变化或变化不大，但 1 个有变化，或者对于所有系数，方向在简化模型中没有变化。]]></description>
      <guid>https://stats.stackexchange.com/questions/657645/validity-of-using-hausman-mcfadden-test-results-for-addressing-departures-from-t</guid>
      <pubDate>Thu, 21 Nov 2024 23:01:57 GMT</pubDate>
    </item>
    <item>
      <title>Python 中的代码仅适用于较小的值[关闭]</title>
      <link>https://stats.stackexchange.com/questions/657643/code-in-python-works-only-with-small-values</link>
      <description><![CDATA[我有一个 python 代码，它对我的​​主变量（表示为 m）的小整数值很有效，但对于较大的整数值（m&gt;6），它从不给我结果，猜想问题出在库上，有什么建议吗？
我的代码：
...
import sympy as sp

# 使用 m 指定类别数
m = 6

# 使用 i 变量指定预期副本数的间隔 o) 最小值和最大值
i_min = 1
i_max = 10

# 计算预期的文章数量以获得每个类别的至少一个副本。
E_m = m * sum(1 / i for i in range(1, m + 1))

# 定义辅助变量 t 进行泰勒展开
t = sp.symbols(&#39;t&#39;)

# 生成公式的定义
def calculate_probability(m, i_min, i_max):
# 生成公式成分
numerator = sp.factorial(m)
denominator = 1
# 分母为 (j - t) 的乘积，j=2 到 m
for j in range(2, m + 1):
denominator *= (j - t)

# 生成公式的比率
generative = t - 1 + numerator / denominator

# 生成公式的泰勒展开为幂级数
expansion = sp.series(generative, t, 0, m) # 使用 t=0，展开为 t^m

# 显示 m 和区间的值，使用 i_min 到i_max
print(f&quot;对于具有 {m} 个类别和副本数 (i) 从 = {i_min} 到 {i_max} 的集合：&quot;)

# 显示扩展
#print(f&quot;生成系列的泰勒扩展：&quot;)
#print(expansion)

# 显示结果
print(f&quot;预期论文数量：{E_m:.6f}&quot;)

# 在 i 的间隔内进行迭代（从 i_min 到 i_max）
for i in range(i_min, i_max + 1):
# 获取 t^i 的系数
coef_ti = extension.coeff(t, i)

# 显示四舍五入到小数点后 6 位的预期数字
probability_num = coef_ti.evalf(6)

# 具有恰好 i 个副本的元素的预期数量。
print(f&quot;具有 {i} 个副本的元素的预期数量：{probability_num}&quot;)

# 调用函数计算参考间隔的概率
calculate_probability(m, i_min, i_max)

...]]></description>
      <guid>https://stats.stackexchange.com/questions/657643/code-in-python-works-only-with-small-values</guid>
      <pubDate>Thu, 21 Nov 2024 22:58:06 GMT</pubDate>
    </item>
    <item>
      <title>如何计算非正态分布的数据的条件功效？</title>
      <link>https://stats.stackexchange.com/questions/657641/how-to-calculate-conditional-power-for-data-that-is-not-normally-distributed</link>
      <description><![CDATA[研究信息：
两个平行组，活性组和安慰剂组。计划样本量为 168 = 84*2。由于我们的主要终点不呈正态分布（从其他先前的类似研究中得知），因此指定的主要分析是 Wilcoxon 秩和检验。计划进行中期分析 (IA)，在 ~50% 的受试者完成 DB 期间进行。计划计算条件功效。统计分析计划中指定的公式如下。
正常CDF [ Zn/sqrt(t(1-t)) - Zα/2/sqrt(1-t)]。
CDF = 累积分布函数。
Zn 是 Wilcoxon 秩和检验的 z 统计量，用于比较活性药物与安慰剂。
t 是信息因子，即中期分析时研究中的实际受试者数量除以研究中计划的受试者数量（168 = 84*2），即 n/168。
现在决定提前进行 IA。也就是说，总共会有 78 或 80 名受试者。每只手臂将是 39 或 40。
我已经在网上搜索、阅读论文并使用了 ChatGPT，但我仍然有问题想在这里寻求专家的帮助。
1. 如果按照计划将 Wilcoxon 秩和检验中的“z”作为 Zn 代入上述公式来计算条件功效，那么对于我们在 IA 的样本量，这是否是一种有效的方法？

这个公式是否适合我们的数据，尤其是在这样的样本量下？如果不适合，还有其他公式吗？

我目前的想法是，由于我的样本量不够大，我应该进行模拟，至少作为计划的“插入 z”方法的备份。看看会有多少差异。但我不知道该怎么做。有人能帮我吗？


提前谢谢大家。
珍妮特]]></description>
      <guid>https://stats.stackexchange.com/questions/657641/how-to-calculate-conditional-power-for-data-that-is-not-normally-distributed</guid>
      <pubDate>Thu, 21 Nov 2024 22:19:39 GMT</pubDate>
    </item>
    <item>
      <title>如何在没有历史比例数据的情况下预测子地点的销售额？</title>
      <link>https://stats.stackexchange.com/questions/657640/how-to-forecast-sales-for-sub-locations-without-historical-proportion-data</link>
      <description><![CDATA[我有一个时间序列数据集，记录了商店中某个产品在一段时间内的总销售额。该产品在商店内的两个不同位置有售：一个位于收银台附近，另一个位于商店中心。
目前，我只有整个商店的产品总销售数据。我需要分别预测每个摊位的销售额。这似乎是一个使用自上而下方法的分层预测问题，其中总销售额被分解到子位置。
但是，我没有每个摊位的历史数据，所以我不知道每个位置的销售额比例。
我可以使用哪些方法或途径来估计这些比例并为每个摊位做出准确的预测？]]></description>
      <guid>https://stats.stackexchange.com/questions/657640/how-to-forecast-sales-for-sub-locations-without-historical-proportion-data</guid>
      <pubDate>Thu, 21 Nov 2024 21:01:43 GMT</pubDate>
    </item>
    <item>
      <title>使用线性混合模型时，输出变量中缺失多少数据是可以接受的？</title>
      <link>https://stats.stackexchange.com/questions/657631/how-much-missing-data-in-the-output-variable-is-ok-when-using-linear-mixed-model</link>
      <description><![CDATA[我有重复测量数据，但由于缺失，我计划使用线性混合模型。我唯一的预测因子是时间，结果变量是定量测量的幸福感得分。目标是确定幸福感是否随着时间的推移而发生整体变化。
幸福感得分是通过四个时间点的季度调查收集的，参与人数如下：

时间 1：25 名参与者
时间 2：54 名参与者
时间 3：70 名参与者
时间 4：120 名参与者

由于大量缺失数据，近 80% 的答复不完整。许多参与者只参加了一次，特别是在时间 4，缺乏后续数据。为了解决这个问题，我过滤了数据集以仅包含至少参加过两个时间点的参与者。经过筛选后，参与人数如下：

时间 1：21 名参与者
时间 2：35 名参与者
时间 3：46 名参与者
时间 4：47 名参与者

此调整将缺失数据减少至约 36%。排除仅参加过一次的参与者是否合适？缺失数据多少才合适？
我非常感谢任何反馈或建议。]]></description>
      <guid>https://stats.stackexchange.com/questions/657631/how-much-missing-data-in-the-output-variable-is-ok-when-using-linear-mixed-model</guid>
      <pubDate>Thu, 21 Nov 2024 17:24:52 GMT</pubDate>
    </item>
    <item>
      <title>glmmTMB nbinom1 计算不正确？coef</title>
      <link>https://stats.stackexchange.com/questions/657620/glmmtmb-nbinom1-computing-incorrect-coef</link>
      <description><![CDATA[我有固定效应模型（无随机效应）葡萄糖 ~ 基因型，其中基因型是具有两个水平的因子。拟泊松和负二项式 GLM 应该估计相同的系数。但是，glmmTMB(nbinom1) 不会计算与 glm(quasipoisson)、glmTMBB(nbinom2) 或 glm.nb 匹配的模型系数。由于系数不同，估计的组边际均值也不同。这些在函数之间应该相同并且等同于样本均值，对吗？发生了什么？
library(data.table)
library(MASS)
library(glmmTMB)
fd &lt;- data.table(
genotype = rep(c(&quot;WT&quot;, &quot;KO&quot;), each = 8),
tumors = rnegbin(8*2, mu = rep(c(5, 10), each = 8), theta = 1)
)
qp1 &lt;- glm(tumors ~ genotype,
family = quasipoisson(link = &quot;log&quot;),
data = fd)
qp2 &lt;- glmmTMB(tumors ~ genotype,
family = nbinom1(link = &quot;log&quot;),
data = fd)
nb1 &lt;- glm.nb(tumors ~ genotype,
data = fd)
nb2 &lt;- glmmTMB(tumors ~ genotype,
family = nbinom2(link = &quot;log&quot;),
data = fd)
coef(summary(qp1))
coef(summary(qp2))$cond
coef(summary(nb1))
coef(summary(nb2))$cond

 估计标准误差 t 值 Pr(&gt;|t|)
(截距) 2.431418 0.2891563 8.408662 7.631869e-07
genotypeWT -1.215023 0.6044945 -2.009981 6.410986e-02
估计标准误差误差 z 值 Pr(&gt;|z|)
(截距) 2.3225197 0.3044603 7.628318 2.378361e-14
genotypeWT -0.8076885 0.4786590 -1.687399 9.152671e-02
估计标准误差 z 值 Pr(&gt;|z|)
(截距) 2.431418 0.3515361 6.916553 4.627644e-12
genotypeWT -1.215023 0.5226885 -2.324564 2.009530e-02
估计标准误差 z 值 Pr(&gt;|z|)
(截距) 2.431419 0.3515362 6.916554 4.627620e-12
genotypeWT -1.215022 0.5226886 -2.324563 2.009535e-02

更新
PBulls 链接到帖子，该帖子显示任何具有正交分类因子的 GLM 的系数都相等（如果包含截距，则加起来等于 log(mean)，如果排除截距，则等于 log(mean)）。那么为什么 glmmTMB(family = nbinom) 不是这种情况呢？
我修改了该链接中用户 cardinal 的回复中的代码，以包含两个准泊松拟合。这些是无截距模型中的系数（因此这些系数应等于样本均值）。
 negbin poisson gaussian invgauss gamma qpoisson qpoissson_tmb
XX1 4.234107 4.234107 4.234107 4.234107 4.234107 4.234107 4.288181
XX2 4.790820 4.790820 4.790820 4.790820 4.790820 4.789353
XX3 4.841033 4.841033 4.841033 4.841033 4.841033 4.841033 4.811718

以下是带有截距的模型的系数。这里，X2 和 X3 的系数（我们试图估计的影响）与其他 glms 相差约 10%。
 negbin poisson gaussian invgauss gamma qpoisson qpoissson_tmb
（截距）4.234107 4.234107 4.234107 4.234107 4.234107 4.234107 4.2881807
XX2 0.556713 0.556713 0.556713 0.556713 0.556713 0.5011718
XX3 0.606926 0.606926 0.606926 0.606926 0.606926 0.606926 0.5235374
]]></description>
      <guid>https://stats.stackexchange.com/questions/657620/glmmtmb-nbinom1-computing-incorrect-coef</guid>
      <pubDate>Thu, 21 Nov 2024 15:14:54 GMT</pubDate>
    </item>
    <item>
      <title>调查数据集处理</title>
      <link>https://stats.stackexchange.com/questions/657609/survey-data-set-manipulation</link>
      <description><![CDATA[我很抱歉这个问题太过简单，但我需要一些指导。我有两个数据集，第一个数据集是我认为的控制集，它是整个区域的 GPS 测量值。这些测量值的海拔就是问题所在。我还有另一个数据集，其海拔值与第一个数据集不同，是二阶获得的。我需要找到一个值来移动第二个数据集，以便它以 95% 的置信度匹配第一个数据值，也就是说，我想将第二个数据集移动一个我还没有找到的数字，以便移动后第二个数据集的新值都在控制 GPS 测量值的 5% 以内。找到移动第二个数据集的正确值的正确方法是什么，我如何验证移动的数据集是否在容差范围内？
非常感谢您的帮助]]></description>
      <guid>https://stats.stackexchange.com/questions/657609/survey-data-set-manipulation</guid>
      <pubDate>Thu, 21 Nov 2024 11:35:22 GMT</pubDate>
    </item>
    <item>
      <title>McNemar 检验为 z 检验或卡方检验</title>
      <link>https://stats.stackexchange.com/questions/657582/mcnemar-test-as-z-test-or-chi-squared-test</link>
      <description><![CDATA[我是统计学入门课的助教。我们正​​在向学生介绍 McNemar 检验，以比较相关样本上的两个比例。我注意到教科书中，我们将 McNemar 检验定义为遵循 z 分布的 z 统计量：
$$
z = \frac{b-c}{\sqrt{b+c}}
$$
而其他来源，例如维基百科，将检验统计量定义为遵循卡方分布
$$
\chi_1^2 = \frac{(b-c)^2}{b+c}
$$
我从数理统计中得知，z 统计量的平方服从自由度为 1 的卡方分布。
有人知道为什么人们可能更喜欢将一种框架作为 z 检验或将另一种框架作为 z 检验吗？我认为这两个统计数据会产生相同的推论。我怀疑如果学生决定自己阅读这个主题，他们可能会感到困惑。
将测试框架为 z 检验确实允许计算与卡方框架无关的标准误差。]]></description>
      <guid>https://stats.stackexchange.com/questions/657582/mcnemar-test-as-z-test-or-chi-squared-test</guid>
      <pubDate>Wed, 20 Nov 2024 21:28:46 GMT</pubDate>
    </item>
    <item>
      <title>ECM：将 I(0) 添加到长期关系中</title>
      <link>https://stats.stackexchange.com/questions/657538/ecm-adding-i0-to-long-term-relation</link>
      <description><![CDATA[我有一个 ECM 模型，其中长期关系是 $y$ 和 $x_1$ 之间的，它们都是 I(1) 且是协整的，即
$y_t = a + b.x_{1,t} + \epsilon_t$
对于这个长期关系，我需要添加 $x_2$，它是 I(0)，但对 $y$ 有很大影响。请注意 $x_2$ 影响 $y$，而不是 $\Delta y$，因此将 $x_2$ 添加到短期响应中是没有意义的。因此，关注长期方程，有：
$y_t = \bar{a} + \bar{b}.x_{1,t} + c.x_{2,t} + \mu_t$
听起来正确。 $y$ 和 $x_1$ 是协整的，因此添加 $x_2$ 应该不会对模型的稳健性产生影响。残差仍为 I(0)。我的问题是：

我遗漏了什么吗？添加 $x_2$ 的实际问题是什么？

您有关于该主题的文献吗？文献重点关注 I(1) 过程，但没有关于添加 I(0) 变量的参考。

或者，长期内如何做以下事情：


替代方法
$y_t = a + b.x_{1,t} + \epsilon_t$
对 $x_2$ 进行 $\epsilon_t$ 回归（本质上是进行 2 步回归）如下
$\epsilon_t = d + e.x_{2,t} + \nu_t$
得到
$y_t = a + b.x_{1,t} + d + e.x_{2,t} + \nu_t$
最后从 $\nu_t$ 构建短期响应：
$\Delta y_t = \gamma \Delta x_{1,t} + \rho.\nu_{t-1}$

继续讨论在 ECM 模型中添加 I(0) 变量的话题，我测试了两个选项。
选项 1：在长期均衡中引入 $x_2$
$y_t = a_0 + a_1.x_{1,t} + a_2.x_{2,t} + \epsilon_t$
$\Delta y_t = \gamma_1 \Delta x_{1,t} + \gamma_2.\epsilon_{t-1}$
选项 2：在短期响应中引入 $x_2$（保留 1 个月以与选项 1 保持一致）
$y_t = b_0 + b_1.x_{1,t} + \epsilon_t$
$\Delta y_t = \mu_1 \Delta x_{1,t} + \mu_2.x_{2,t-1} + \mu_3.\epsilon_{t-1}$
结果

选项 1 的表现符合预期：模型收敛至 LT 均衡关系。“问题”在于混合 I(1) 和 I(0) 可能会引发问题。

选项 2 紧跟选项 1。有趣的是，LT 方程不同，这意味着选项 2 不会恢复到其 LT 均衡。它恢复为 (LT+$\mu_2.x_{2,t-1}$)，这是有道理的。

我尝试按照建议使用单步方法估计模型，但这给我带来了几个问题，例如：(1) 不显著的系数，(2) 不同的预测，(3) 系数值不能与选项 1 或 2 相关。问题一定来自于变量不是完全独立的这一事实。

上面讨论的替代方法引入 $x_2$ 作为残差的回归量，应该会给出与选项 1 和 2 非常相似的结果。


我很乐意听取您的反馈]]></description>
      <guid>https://stats.stackexchange.com/questions/657538/ecm-adding-i0-to-long-term-relation</guid>
      <pubDate>Wed, 20 Nov 2024 01:41:51 GMT</pubDate>
    </item>
    </channel>
</rss>