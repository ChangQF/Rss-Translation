<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 07 Jan 2025 09:18:26 GMT</lastBuildDate>
    <item>
      <title>如何理解概率中的句子结构</title>
      <link>https://stats.stackexchange.com/questions/659651/how-to-understand-structure-of-sentences-in-probability</link>
      <description><![CDATA[在我的教科书中，它说
（I）随机选择的高中生吃早餐
（II）随机选择的青少年是吃早餐的高中生
（III）随机选择的吃早餐的青少年是高中生
我应该为上述概率选择正确的大小顺序。
书中说（I）的概率是$P(I)=P(早餐 | 高级)$
（II）是$P(II)=P(breakfast \cap 高级)$
（III）是$P(III)=P(高级 | 早餐)$
那么我的问题是，例如在（II）中，为什么不是$P(II)=P(早餐 |高级）$？
因为我觉得如果你把（III）写成条件概率，你也应该把（II）写成条件概率。
我这里漏掉了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659651/how-to-understand-structure-of-sentences-in-probability</guid>
      <pubDate>Tue, 07 Jan 2025 08:58:39 GMT</pubDate>
    </item>
    <item>
      <title>这里的 95% 置信区间是多少？我的答案正确吗？</title>
      <link>https://stats.stackexchange.com/questions/659650/what-is-the-95-confidence-interval-here-is-my-answer-correct</link>
      <description><![CDATA[我正在处理这个问题：

这个问题要求找出两个城市超速罚单金额差异的 95% 置信区间；换句话说，mu(Orange) - mu(DeLand)。我得到的区间在 31.2103907 和 39.5396093 之间。这是正确的吗？如果不是，正确答案是什么？
我使用了以下公式：
]]></description>
      <guid>https://stats.stackexchange.com/questions/659650/what-is-the-95-confidence-interval-here-is-my-answer-correct</guid>
      <pubDate>Tue, 07 Jan 2025 08:38:28 GMT</pubDate>
    </item>
    <item>
      <title>关于具有单位根的 AR(1) 过程的方差</title>
      <link>https://stats.stackexchange.com/questions/659648/about-the-variance-of-ar1-processes-with-unit-root</link>
      <description><![CDATA[我目前正在研究 ARIMA 过程，AR 过程在其中起着重要作用。让我们考虑以下形式的 AR(1) 过程：
$$X_t = X_{t-1} + Z_t$$
这也称为随机游走。这里 $t$ 的索引集是 $\mathbb{Z}$，而 $Z_t$ 是一个白噪声过程。
我想证明，这个过程的方差确实依赖于 $t$，因此它不是弱平稳的。
我发现了很多关于这个主题的问题，通常的“证明”是这样写的：
$$X_t = X_0 + \sum_{i=1}^t Z_i$$
现在论证：
$$Var[X_t] = Var[X_0] + \sum_{i=1}^t Var[Z_i]$$
显然取决于 $t$。
但是，我认为这个证明并不正确，或者至少总体上不正确，因为上面的陈述假设 $X_0$ 和总和不相关，这一点根本不明显。
有谁知道在不假设 $X_0$ 和 $\sum_{i=1}^t Z_i$ 不相关的情况下证明这个陈述的方法吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/659648/about-the-variance-of-ar1-processes-with-unit-root</guid>
      <pubDate>Tue, 07 Jan 2025 07:06:53 GMT</pubDate>
    </item>
    <item>
      <title>在 GLMM 中使用 k 均值空间聚类作为随机效应是否正确？</title>
      <link>https://stats.stackexchange.com/questions/659646/is-it-correct-to-use-spatial-clusters-by-k-means-as-random-effect-in-glmm</link>
      <description><![CDATA[我是统计学新手，我会尽力解释我的问题。
我正在攻读学位，研究大象袭击的驱动因素。对于响应变量，我有袭击的位置（坐标），并尝试使用一组独立变量进行预测。我使用 R 中“lme4”包中实现的逻辑回归模型，将系列设置为二项式。
我想解释任何方差以及与袭击地理位置相关的可能自相关。我正在考虑使用 k-最近邻生成坐标聚类，并将这些聚类作为随机效应拟合到我的模型中以解释这些。模型结果表明，这种方法确实解释了一些方差并增加了模型的 R2。
我没有找到使用相同方法的研究，我想知道这种方法在统计上是否正确。我只是担心随机变量的力量会被固定效应“窃取”。
根据我与教授的讨论，出于一个非常具体的原因，我不打算使用“glmmTMB”或“mgcv”等空间模型。
提前致谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/659646/is-it-correct-to-use-spatial-clusters-by-k-means-as-random-effect-in-glmm</guid>
      <pubDate>Tue, 07 Jan 2025 06:02:00 GMT</pubDate>
    </item>
    <item>
      <title>何时使用哪种样本方差公式？</title>
      <link>https://stats.stackexchange.com/questions/659644/when-to-use-which-formula-for-sample-variance</link>
      <description><![CDATA[我知道样本方差的公式为
$$s^2 = \frac{\sum (x_i - \bar{x})^2}{n - 1}$$
我还知道样本方差的公式为“均值平方减均值平方”。
在计算给定样本的样本方差时，我使用了这两个公式，但发现它们给出了两个不同的答案，因此我想问，什么时候该使用第一个公式，什么时候该使用第二个公式？]]></description>
      <guid>https://stats.stackexchange.com/questions/659644/when-to-use-which-formula-for-sample-variance</guid>
      <pubDate>Tue, 07 Jan 2025 05:32:19 GMT</pubDate>
    </item>
    <item>
      <title>为什么标准差优于平均值的绝对偏差？</title>
      <link>https://stats.stackexchange.com/questions/659643/why-is-standard-deviation-superior-to-absolute-deviations-from-mean</link>
      <description><![CDATA[在阅读了大量资料和论坛后，我明白了：

由于标准偏差的可加性，它更优越 - 同意

平均值是使平方偏差之和最小化的数字。同样，中位数是使绝对偏差之和最小化的（有时不是唯一的）数字 - 有点同意，但我不明白为什么最小结果是相关的

因为平方偏差在 x = 0 处可微，但绝对偏差不可微 - 我无法理解这一点。x^2 的曲线是从负无穷到正无穷的平滑 V 形曲线。另一方面，| x | 的曲线是从负无穷到正无穷的急剧 V 形曲线。那么，如果一个在 x = 0 处可微，为什么另一个不可微？

一个独立的想法/问题：方差是与平均值的平方偏差的平均值。标准差是方差的平方根。但是，如果我们只对平方偏差的总和求平方根，然后求平均值（即除以 n），不是更准确吗？

]]></description>
      <guid>https://stats.stackexchange.com/questions/659643/why-is-standard-deviation-superior-to-absolute-deviations-from-mean</guid>
      <pubDate>Tue, 07 Jan 2025 05:23:59 GMT</pubDate>
    </item>
    <item>
      <title>具有重复数据但权重不同的加权逻辑回归 - 正确的方法是什么？</title>
      <link>https://stats.stackexchange.com/questions/659641/weighted-logistic-regression-with-duplicated-data-but-different-weights-whats</link>
      <description><![CDATA[设置和传统方法
假设对于每个观察值 1 到 N，您有概率估计 $p_a=P(race=a)$, $p_b=P(race=b)$, $p_c=P(race=c)$，其中 $\sum_{r=\{a,b,c\}}p_r=1$ 和一些二元结果变量 $y$，并且我们希望估计逻辑回归 $\text{logit }y\sim \beta_0 + \beta_1 D^{r}+X\beta$，其中 $\sum_{r=\{a,b,c\}}p_r=1$ class=&quot;math-container&quot;&gt;当 P(race=r)&gt;.8 时，$D^r=1$，否则为 0。但是，由于我们希望明确地并且只比较 a 与 b，因此我们在 P(race=a)&gt;.8 或 P(race=b)&gt;.8 的数据子集上估计 $\text{logit }y\sim \beta_0 + \beta_1 D^{b}+X\beta$。这是传统方法，但它会丢弃所有 P(race=a)&lt;.8 AND P(race=b)&lt;.8 的数据。
为了直观的理解，想象一下比较黑人与白人、西班牙裔与白人，但不知道谁是白人、西班牙裔和黑人，你只能对每个类别有一个概率估计。
替代方案：重复数据集，但每个重复项的权重不同
为了不丢弃 P(race=a)&lt;.8 AND P(race=b)&lt;.8 为真的观测值所包含的信息，有人提出估计一个加权逻辑回归，其中
\begin{align}
\text{logit }\begin{pmatrix}
y \\
y \\
\end{pmatrix} &amp;= \beta_0+\beta_1\begin{pmatrix}
D^{b} \\
D^{b} \\
\end{pmatrix}
+ \begin{pmatrix} X \\ X\\ \end{pmatrix} \beta
\end{align&gt;
其中 $$\text{weights} =\begin{pmatrix} p_b \\ p_a \\ \end{pmatrix}$$ 且 $D^b_{1\dots N}$=1 且 $D^b_{(N+1)\dots 2N}=0$（基本上，第一个 1...N 堆栈具有虚拟 $D^b=1$ 而第二个 (N+1)...2N 有虚拟 $D^b=0$。
问题/担忧
直观地看，这似乎是错误的，因为即使有权重，它也会人为地缩小估计误差（我无法想象权重在这种情况下能解决问题）。有人可以确认吗，如果可能的话，提出一个更好的替代方案？
请原谅我草率的符号，希望它足以传达这一点。]]></description>
      <guid>https://stats.stackexchange.com/questions/659641/weighted-logistic-regression-with-duplicated-data-but-different-weights-whats</guid>
      <pubDate>Tue, 07 Jan 2025 04:28:49 GMT</pubDate>
    </item>
    <item>
      <title>不知不觉地出现在媒体背景中的可能性有多大？</title>
      <link>https://stats.stackexchange.com/questions/659636/chances-of-unknowingly-appearing-in-the-background-of-media</link>
      <description><![CDATA[这个问题太过简单，可能不值得问。如果是这样，请直接告诉我。
鉴于几十年来拍照和录像的人越来越多，你出现在网上的某些照片或视频中，或者出现在某人的个人相册中而你却浑然不知的几率似乎越来越高。事实上，在我看来，这是不可避免的。YouTube 上充斥着标题糟糕、观看次数少的现实世界视频。
有没有办法在不诉诸哲学论证的情况下对这种情况发生的可能性做出某种估计？我立即猜测，有太多变量在起作用，无法进行任何有意义的计算。更糟糕的是，要考虑哪些变量才能给出估计值似乎很难，而且你必须对位置、个人习惯等做出大量假设。
也许为了尝试让我的问题更深入一些，我可以补充一点：当统计学家面临上述问题时，这本质上是一个失败的原因吗？如何决定要考虑哪些变量以及要做出哪些假设，或者当问题太过宽泛而无法得出任何有意义的结论时？]]></description>
      <guid>https://stats.stackexchange.com/questions/659636/chances-of-unknowingly-appearing-in-the-background-of-media</guid>
      <pubDate>Tue, 07 Jan 2025 02:01:41 GMT</pubDate>
    </item>
    <item>
      <title>如何根据前几年的数据最好地预测非线性时间序列末尾的值</title>
      <link>https://stats.stackexchange.com/questions/659635/how-best-to-forecast-the-value-at-the-end-of-a-non-linear-time-series-based-on-p</link>
      <description><![CDATA[我有前几个财年的每周支出数据，我想用它（以及本财年迄今为止的支出数据）预测本财年末的总支出。我并不关心在最后一周之前预测每周支出，但它可能对质量保证有用。
每个周期的支出都遵循大致相似的模式，在周期开始时更多，在结束时更少（类似于 y = x^0.5），因此使用过去几年应该有助于了解本年度剩余时间的支出进展情况。此外，我想预测不同预算的支出以及总体支出。
我一直在做一些研究，这似乎表明非线性混合效应模型将是最好的方法，使用不同的财年作为重复测量。但是，我对此不是很熟悉，并且想在开始之前检查这是否是一种好的方法（特别是因为我想在拟合模型时优先考虑最终支出，但我不确定这是否会这样做） - 此外，如果有任何关于如何最好地进行的指南（特别是如何使用 R 拟合模型），它将非常有用。]]></description>
      <guid>https://stats.stackexchange.com/questions/659635/how-best-to-forecast-the-value-at-the-end-of-a-non-linear-time-series-based-on-p</guid>
      <pubDate>Tue, 07 Jan 2025 01:55:43 GMT</pubDate>
    </item>
    <item>
      <title>我如何改进季节性的实施，以便更好地为扩展和有效地自动化此方法做好准备？</title>
      <link>https://stats.stackexchange.com/questions/659634/how-can-i-refine-my-seasonality-implementation-to-better-prepare-for-scaling-and</link>
      <description><![CDATA[我不会深入探讨我的模型的细节，而是谈谈我希望解决的问题。
当前方法
我使用离散概率矩阵 (DPM) 来预测单个事件在每月期间发生的可能性。
DPM 按用户特征（例如年龄或性别）细分，每个单元格代表如果特征 Y 存在于 Z 月，则事件 X 发生的概率。
当前方法仅依赖于使用最新的实际值作为所有预测月份的基线。
此基线根据工作日进行了调整，以考虑月份长度的变化，从而有效地跨月份转移概率。尽管如此，概率仍然平均围绕实际值的最后一个点。
挑战
这种方法在某些 DPM 线中难以应对季节性。使用实际值的最后一点可能会严重高估或低估季节性活动。
此外，缺乏季节性成分意味着该模型无法捕捉一年中特定时间的重复模式。这很重要，因为该模型包含在 X 或 Y 月份等发生的几种经济条件。
引入的改进
为了解决季节性问题，我引入了经典加性季节性分解 (ASD) 来得出 DPM 中每个概率系列的季节性因素。
按照现有方法，我对基线应用了工作日调整，并在其上分层了季节性因素以保留季节性模式。
这种改进使模型能够同时考虑月份长度的变化和重复的季节性模式，从而使具有季节性活动的 DPM 线的预测更加准确。
方法的优势
ASD 假设有规律的季节性，这很符合我的模型的需求。
它有效地处理了 DPM 中的零/零概率，使其与我的数据兼容。
该方法很简单在 Excel 和 Python 等工具中实现。
季节性因素被标准化为零，确保它们只会改变值而不会改变年度概率总数。
方法保留了阶跃变化 - 这些变化可以在数据中自然发生。
此外，由于还有其他模型组件控制预测水平，因此这里需要一个相当平坦的基于证据的 DPM。
剩余挑战
ASD 假设所有数据点都是正确的，这突出了异常值检测的重要性。我已经测试了四分位距、Z 分数和修正 Z 分数，但我正在考虑是否存在更有效的方法来处理异常值。是否有测试可以区分阶跃变化和异常值。
使用实际值的最后一点作为基线有局限性。虽然平均值已经足够了，但我测试了更先进的技术，例如 Holt 和 Holt-Winters 指数平滑法，这些技术可能会提供更好的准确性。不过，我想知道是否有更好的方法，因为这些方法在 Python 中不是最容易实现的。
另一个问题是，我的方法将季节性应用于所有 DPM 线，无论它们是否显示季节性（是的，这是一个矛盾的评论）。我正在探索自动检测季节性的方法。我尝试使用自相关函数 (ACF) 来可视化我的时间序列数据中的季节性模式和依赖关系。如果有一个统计数据就太好了，可以利用它来决定是否应用季节性。
询问
我将不胜感激任何关于如何应对这些挑战的建议。我对时间序列分析和方法还很陌生。虽然我当前的方法是有效的，但它需要一定程度的人工干预，随着我扩大其应用范围，这种干预可能会变得难以为继。]]></description>
      <guid>https://stats.stackexchange.com/questions/659634/how-can-i-refine-my-seasonality-implementation-to-better-prepare-for-scaling-and</guid>
      <pubDate>Tue, 07 Jan 2025 01:48:00 GMT</pubDate>
    </item>
    <item>
      <title>以 Y 的指标为条件，对 X 对 Y 的影响进行条件化</title>
      <link>https://stats.stackexchange.com/questions/659633/condition-the-effect-of-x-on-y-on-the-indicator-of-y</link>
      <description><![CDATA[正如标题所示，我正在做一个研究项目，研究X（因变量）和Y（自变量）之间的关系。它们都是连续变量，Y在0附近对称分布。朴素的OLS回归模型是：
Y ~ X + e 

但我们有强有力的理论论据，即X对Y的影响应该取决于Y是正还是负：
Y ~ X + Z + X*Z + e
Z = I(Y&gt;0)

显然，Z是内生的。我有两个问题：
1，我可以使用交互模型吗？如果可以，需要什么条件？
2，还有其他方法可以实现我正在做的事情吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/659633/condition-the-effect-of-x-on-y-on-the-indicator-of-y</guid>
      <pubDate>Tue, 07 Jan 2025 01:07:53 GMT</pubDate>
    </item>
    <item>
      <title>条件变量的影响是否独立？</title>
      <link>https://stats.stackexchange.com/questions/659629/independence-of-conditioning-variables-effects</link>
      <description><![CDATA[通常，我们将独立性称为$f(A,B) = f(A)f(B)$，或将条件独立性称为$f(A,B|X) = f(A|X)f(B|X)$。
但是，我想知道您会如何称呼这样的事物：
$$
f(Y|A,B) = f(Y|A)f(Y|B)
$$
也就是说，给定 A 和 B，Y 发生的概率与您分别给定 A 和 B 计算 Y 发生的概率相同。换​​句话说，A 和 B 不是关于 Y 的联合信息，而是提供“唯一”的信息有关 Y 的信息。
例如，在预测的背景下，上述方程将表明 A 和 B 提供对 Y 的独立预测。[当然，如果它不成立，那么 $f(Y|A,B) \neq f(Y|A)f(Y|B)$。]
这个概念有通用术语吗？如果有，它在什么情况下使用/应用？]]></description>
      <guid>https://stats.stackexchange.com/questions/659629/independence-of-conditioning-variables-effects</guid>
      <pubDate>Mon, 06 Jan 2025 21:46:29 GMT</pubDate>
    </item>
    <item>
      <title>主成分的独立性</title>
      <link>https://stats.stackexchange.com/questions/659626/independence-of-principal-components</link>
      <description><![CDATA[好的，情况如下。我们有一堆数据，比如说 x1,...,x300，这些数据是我们为一堆样本 y1,...,y1000 收集的。数据 x1,...,x300 是依赖的（x1 的值与 x2-&gt;x300 的值相关，等等）。
作者 A（我试图优雅地证明他犯了一个错误）使用 PCA 来创建新变量。PC1,...,PC300 并指出这些主成分现在是独立的。因此，他们说，我们可以通过将各个主成分生成的概率相乘来计算联合概率（随机匹配概率）。
作者 A 的错误在于他们只表明变量 x1,...,x300 都是正态分布的，但没有表明 P(x1,...,x300) 的 PDF 是多元正态分布的。我的理解是，联合分布必须是多元正态分布，以确保主成分的独立性，而不是每个单独的变量都必须是正态分布（差别很大）。
所以这很好（我想，如果我错了请告诉我）……但我感到困惑的是……假设 P(x1,...,x300) 是多元正态分布。从高度依赖的数据中获取主成分并通过乘以（现在神奇地独立的）值来计算随机匹配概率的想法似乎是错误的……但我无法清楚地说明为什么会这样（或者也许我错了，那完全没问题）。
欢迎任何评论、指导或争论！
谢谢！！]]></description>
      <guid>https://stats.stackexchange.com/questions/659626/independence-of-principal-components</guid>
      <pubDate>Mon, 06 Jan 2025 20:53:28 GMT</pubDate>
    </item>
    <item>
      <title>探索性因子分析，非正态数据</title>
      <link>https://stats.stackexchange.com/questions/659613/exploratory-factor-analysis-non-normal-data</link>
      <description><![CDATA[我正在分析的数据对于几乎每个观察到的变量都是非正态的。我想在 R 中进行探索性因子分析 (EFA)。KMO 检验和 Bartlett 检验表明数据符合 EFA 条件。在 psych 包的文档中，我没有找到应如何处理非正态分布的数据 (https://personality-project.org/r/psych/HowTo/factor.pdf)。我的样本相对较小 (N=60)。
在进行验证性因子分析时，我可以使用 lavaan 包中的 Satorra-Bentler 校正来处理非正态数据，cfa() 函数，例如cfa(model = spec, data = df_data, std.lv=TRUE, estimator = &quot;MLM&quot;) ，这种使用稳健标准误差的校正是一种可接受的方法。 estimator=&quot;MLM&quot; 代表 Satorra-Bentler 校正。我可以用 psych 包 以某种方式做到这一点吗？
psych 包执行 EFA，fa() 函数，例如fa(data, nfactors = 4, rotate = &quot;oblimin&quot;) 并提供可行的因子，但如果我没有弄错的话，它被设计用于正态分布的数据。
如果我尝试使用 lavaan 包 中的 efa() 和 Satorra-Bentler 校正，它会发出 警告，表示协方差矩阵包含较小的负值，这可能表明模型未被识别：
efa(data = df_data, nfactors=4, rotation=&quot;oblimin&quot;, std.lv=TRUE, estimator = &quot;MLM&quot;)

警告消息：
1：lavaan-&gt;lav_model_vcov()： 
估计参数 (vcov) 的方差-协方差矩阵似乎不是
正定！最小特征值 (= -5.425180e-15) 小于零。这可能是模型无法识别的症状。
2：lavaan-&gt;lav_model_vcov()：
估计参数 (vcov) 的方差-协方差矩阵似乎不是正定的！最小特征值 (= -2.190854e-32) 小于零。这可能是模型无法识别的症状。


那么问题是：如何使用心理学包计算非正态数据的 EFA？
更新：
数据分布：我有 9 个变量，每个变量都有比例尺度，因此理论上它们可以从零到无穷大。在实践中，它们计算活动，因此它们包含每个研究对象的整数值。其中一些只有 2、3 个不同的值，这给出了与理论正态分布不同的分布。我添加了一个关于一个变量的 QQ 图，如下图所示。

我使用 psych 包进行了 EFA，以发现数据可能具有从 1 个因子到 9 个因子的内容，以便我可以看到每种情况的拟合优度指标。 这产生了具有 4 个潜在变量的最佳指标。这就是我从 4 开始的原因。尽管如此，我并没有更改默认的因子提取，也没有提供相关矩阵。
我如何为 fa() 函数提供 Spearman 相关矩阵？如果我提供这个矩阵，我是否应该从 minres 更改因子提取方法？哪种因子提取方法对非正态性不太敏感？非正态性对 TLI、CFI、RMSEA 和 Chi2 指标的计算没有影响吗？
我很感激每一位回复，谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/659613/exploratory-factor-analysis-non-normal-data</guid>
      <pubDate>Mon, 06 Jan 2025 12:41:19 GMT</pubDate>
    </item>
    <item>
      <title>R 中 Frank's Copula 负二项式模型中 Theta 参数发散问题</title>
      <link>https://stats.stackexchange.com/questions/659647/issue-with-diverging-theta-parameter-in-negative-binomial-models-with-franks-co</link>
      <description><![CDATA[我尝试使用负二项分布结合 Frank 的 copula 来对足球比赛结果进行建模，以建立依赖结构。每支球队都被分配了单独的进攻和防守参数，并且有主场和客场进球的离散参数，以及 Frank 的 copula 的 theta 参数。
我的方法受到 McHale、Ian 和 Phil Scarf 的论文“国际足球比赛中对方球队进球依赖关系建模”中描述的方法的启发（统计建模，11.3（2011）：219-236）。但是，我没有使用基于 FIFA 世界排名位置差异的参数来对进球依赖关系进行建模（如原始研究），而是对其进行了修改，使用单独的球队参数进行进攻和防守。
不幸的是，在优化过程中，theta 参数在几次迭代后开始发散，我不知道如何解决这个问题。我怀疑 Frank 的 copula 的 theta 参数可能需要限制在特定范围内以确保数值稳定性。但是，我使用的优化函数 (nlm) 不直接支持参数约束，这使得这很难实现。
我的方法：

示例数据：

 set.seed(123)

data_model &lt;- data.frame(
Heim = sample(c(&quot;Team A&quot;, &quot;Team B&quot;, &quot;Team C&quot;, &quot;Team D&quot;), 100, replace = TRUE),
Gast = sample(c(&quot;Team A&quot;, &quot;Team B&quot;, &quot;Team C&quot;, &quot;Team D&quot;), 100, replace = TRUE),
ToreHeim = rpois(100, lambda = 1.5), # 主队进球
ToreGast = rpois(100, lambda = 1.2) # 客队进球
)

# 删除主队等于客队的比赛
data_model &lt;- data_model[data_model$Heim != data_model$Gast, ]


模型函数：

negloglik_double_nb_copula &lt;- function(params, goals_home, goals_visitor,
team_home, team_visitor, param_skeleton) {
plist &lt;- relist(params, param_skeleton)
plist$defense &lt;- c(sum(plist$defense)*-1, plist$defense)
names(plist$defense)[1] &lt;- names(plist$attack[1])

lambda_home &lt;- exp(plist$attack[team_home] + plist$defense[team_visitor] + plist$home)
lambda_visitor &lt;- exp(plist$attack[team_visitor] + plist$defense[team_home])

m1 &lt;- exp(plist$m1)
m2 &lt;- exp(plist$m2)
theta &lt;- exp(plist$theta)

log_lik_home &lt;- dnbinom(goals_home, mu = lambda_home, size = m1^(-1), log = F)
log_lik_visitor &lt;- dnbinom(goals_visitor, mu = lambda_visitor, size = m2^(-1), log = F)
log_lik_copula &lt;- log(frank_copula_density(log_lik_home, log_lik_visitor, theta))

log_lik &lt;- sum(log_lik_copula)
return(log_lik * -1)
}

frank_copula_density &lt;- function(p_home, p_visitor, theta) {
num &lt;- theta * exp(-theta * (p_home + p_visitor)) * (1 - exp(-theta))
denom &lt;- ((1 - exp(-theta * p_home)) * (1 - exp(-theta * p_visitor)) + exp(-theta) - 1)^2
return(num / denom)
}



初始参数：

parameter_list_copula &lt;- list(
attack = rep(0.2, n_teams),
defense = rep(-0.01, n_teams-1),
home = 0.1,
m1 = 0.1,
m2 = 0.15,
theta = 0.5
)

names(parameter_list_copula$attack) &lt;- all_teams
names(parameter_list_copula$defense) &lt;- all_teams[-1]



优化：

nlm_nb_copula &lt;- nlm(negloglik_double_nb_copula, unlist(parameter_list_copula), 
goals_home = data_model$ToreHeim,
goals_visitor = data_model$ToreGast,
team_home = data_model$Heim, team_visitor = data_model$Gast,
param_skeleton = parameter_list_copula, print.level = 2, 
iterlim = 10000, hessian = F)

尽管设置了合理的起始值，但经过几次迭代后，优化结果在 theta 参数上出现了分歧。
切换到支持参数约束的优化方法是否有意义（例如，使用带框约束的优化）？或者是否有其他推荐的策略来解决这个问题？
任何稳定优化的建议或改进此模型的见解都将不胜感激。
提前谢谢您！]]></description>
      <guid>https://stats.stackexchange.com/questions/659647/issue-with-diverging-theta-parameter-in-negative-binomial-models-with-franks-co</guid>
      <pubDate>Sun, 05 Jan 2025 22:58:36 GMT</pubDate>
    </item>
    </channel>
</rss>