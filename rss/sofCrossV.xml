<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Wed, 08 Jan 2025 15:17:44 GMT</lastBuildDate>
    <item>
      <title>预测性维护的时间序列分类与回归</title>
      <link>https://stats.stackexchange.com/questions/659720/time-series-classification-vs-regression-for-predictive-maintenance</link>
      <description><![CDATA[假设我们有一个由传感器数据组成的时间序列数据集，每个时间戳都有一个目标列，指示特定组件是否会在一定天数内发生故障；具体来说，如果我们知道故障将发生在时间戳 X，那么对于 X 之前 Y 天内的每个时间戳，目标列标记为 1，否则标记为 0。这通常用于预测性维护论文中，以预测组件的健康状况。
基本上，我们要做的是一个二元分类。现在，我认为我遗漏了有关时间序列分类与时间序列预测的一些内容。我经常看到这样的代码来生成序列以训练模型：
def create_sequences(features, target, seq_length):
sequences = []
labels = []

for i in range(len(features) - seq_length):
sequences.append(features[i:i + seq_length])
labels.append(targets[i + seq_length])

return np.array(sequences), np.array(labels)

因此，我们获取一系列过去的数据，并尝试预测该序列的一个未来值（最终可能是未来目标的一个窗口）：但是，这是为了时间序列预测而做的，不是吗？
如果我们要将代码用于时间序列分类，则必须采用标签
targets[i:i + seq_length]

因此，与特征的方法相同，以便将每个序列与其自己的标签关联并执行分类。
所以我的问题是：

时间序列分类和预测之间的序列创建方式是否不同？
如果第一个问题的答案是肯定的，那么我们将使用targets[i:i + seq_length]，我们究竟如何预测某个组件将来会出现故障？为了更好地阐述第二个问题，考虑到我们在训练期间没有考虑未来的值，我们究竟如何才能提前检测到失败？

此外，如果我尝试在大型数据集（50 多个特征，经过预处理和规范化）上训练 DL 模型（例如 CNN），并将目标移位为给定序列的下一个值，我无法让模型表现得更好，而如果我不移动目标值，我会获得相当不错的性能，所以我不明白这背后的原因。]]></description>
      <guid>https://stats.stackexchange.com/questions/659720/time-series-classification-vs-regression-for-predictive-maintenance</guid>
      <pubDate>Wed, 08 Jan 2025 15:11:13 GMT</pubDate>
    </item>
    <item>
      <title>从文档中提取文本的最佳途径[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659718/best-route-for-text-extraction-from-documents</link>
      <description><![CDATA[在决定采取哪种方式完成这项任务之前，我试图收集尽可能多的信息。
我需要从发票文件中提取一些标准信息，例如发票号、产品名称和说明、发票日期等。我有大约 20,000 张发票，它们来自多个供应商（大约 200 个供应商），因此结构不同。当然，这意味着我所追求的数据位于这些发票的不同位置。
什么是提取高精度数据的最有效方法？：
a. 使用 LayoutLMv3 模型在我的自定义数据上进行训练。这首先涉及以适当的格式创建自定义数据，即使用 Tesseract 或类似库对发票图像进行 OCR，并使用生成的数据集作为 LayoutLMv3 的训练数据。问题是，即使我拥有的发票质量相当好且可读，Tesseract 仍然很难识别某些文本/符号/标点符号/空格。这进一步促使我深入研究对 Tesseract 模型本身进行微调，方法是从原始发票上创建带有文本的小块图像，并使用写在这些小图像上的实际文本（基本事实）手动注释它们。这是一项繁重的工作，需要大量的手动工作。对于 200 个供应商，这可能意味着我需要注释数千张小图像。
b.直接训练图像到文本模型，如 pic2struct，希望它自己就能产生足够准确的结果。我不确定哪种 ML 算法最适合在这里使用，CNN 吗？它会和上面的方法一样准确吗？
或者也许有更好的方法来完成这项任务？]]></description>
      <guid>https://stats.stackexchange.com/questions/659718/best-route-for-text-extraction-from-documents</guid>
      <pubDate>Wed, 08 Jan 2025 13:30:35 GMT</pubDate>
    </item>
    <item>
      <title>训练启发式函数对堆栈进行排序[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659717/train-an-heuristic-function-to-sort-stacks</link>
      <description><![CDATA[我目前正在研究一种 IDA* 算法，用于对带有临时第二个堆栈的堆栈进行排序，该算法正在寻找对第一个堆栈进行排序所需的最少指令数，这些指令包括交换、推送、旋转和反向旋转，我的问题是启发式函数，由于指数复杂度，该函数使该算法可用于堆栈中超过 7 或 8 个数字。我需要找到一种方法来了解实际堆栈状态和排序状态之间的“距离”。我尝试分析数据以找到逻辑，我也尝试训练 RNN，但没有成功，我使用的数据是使用简单搜索算法生成的解决方案，但我无法为超过 9 个数字的堆栈生成解决方案
因此，如果有人有此问题的解决方案或有关如何解决此问题的想法，欢迎提供任何帮助]]></description>
      <guid>https://stats.stackexchange.com/questions/659717/train-an-heuristic-function-to-sort-stacks</guid>
      <pubDate>Wed, 08 Jan 2025 13:10:19 GMT</pubDate>
    </item>
    <item>
      <title>将部分 eta 平方转换为 Cohen's d 并计算方差</title>
      <link>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</link>
      <description><![CDATA[我正在 R 中进行元分析。对于大多数研究，我使用组的均值和 SD 计算 Hedges 的 g。但是，对于某些研究，我只有部分 $\eta ^2$ 和 95% 置信区间。
例如，
部分 $\eta ^2 = 0.007$
95% CI= 0.08-0

我知道如何将部分 $\eta ^2$ 转换为 Cohen 的 d，然后将其转换为 Hedges 的 g。但是，我该如何计算 Hedges g 的方差呢？

对于某些研究，我只有部分 $\eta ^ 2$，没有置信区间。有没有办法根据这些信息计算出 Hedges g 的方差呢？

]]></description>
      <guid>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</guid>
      <pubDate>Wed, 08 Jan 2025 11:19:07 GMT</pubDate>
    </item>
    <item>
      <title>基线后混杂因素与并发事件之间的差异（来自估计框架）</title>
      <link>https://stats.stackexchange.com/questions/659711/difference-between-post-baseline-confounders-and-intercurrent-events-from-estim</link>
      <description><![CDATA[与估计框架相比，基线后混杂因素和并发事件有何不同？具体来说，以急救药物为例，它如何既被归类为基线后混杂因素，又被归类为并发事件，两者之间又有何区别？]]></description>
      <guid>https://stats.stackexchange.com/questions/659711/difference-between-post-baseline-confounders-and-intercurrent-events-from-estim</guid>
      <pubDate>Wed, 08 Jan 2025 11:03:54 GMT</pubDate>
    </item>
    <item>
      <title>将一个数据集的PCA负载应用于R中具有相同变量的新数据集[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659710/applying-loadings-of-pca-from-one-dataset-to-a-new-dataset-with-the-same-variabl</link>
      <description><![CDATA[我正在运行 PCA 来为数据集创建财富指数。我们有基线和终点数据，两者都有资产变量。我想先对基线数据运行 PCA，然后将负载应用于包含基线和终点线的整个数据集。
我已经编写了此代码，但想了解这是否是正确的方法。
# 加载必要的库
library(psych)
library(dplyr)

# 虚拟数据集

# 设置数据集的行数
n_rows &lt;- 2800

# 设置可重复性的种子
set.seed(123)

df &lt;- data.frame(
ha01 = sample(0:1, n_rows, replace = TRUE),
ha02 = sample(0:1, n_rows, replace = TRUE),
ha03 = sample(0:1, n_rows, replace = TRUE),
ha04 = sample(0:1, n_rows, replace = TRUE),
ha05 =样本（0：1，n_rows，替换 = TRUE），
ha06 = 样本（0：1，n_rows，替换 = TRUE），
ha07 = 样本（0：1，n_rows，替换 = TRUE），
ha08 = 样本（0：1，n_rows，替换 = TRUE），
ha09 = 样本（0：1，n_rows，替换 = TRUE），
ha10 = 样本（0：1，n_rows，替换 = TRUE），
ha11 = 样本（0：1，n_rows，替换 = TRUE），
ha12 = 样本（0：1，n_rows，替换 = TRUE），
ha13 = 样本（0：1，n_rows，替换 = TRUE），
ha14 = 样本（0：1，n_rows，替换 = TRUE），
ha15 = 样本（0：1，n_rows，替换 = TRUE），
ha16 =样本（0：1，n_rows，替换 = TRUE），
ha17 = 样本（0：1，n_rows，替换 = TRUE），
ha19 = 样本（0：1，n_rows，替换 = TRUE），
ha20 = 样本（0：1，n_rows，替换 = TRUE），
sleeproom1 = 样本（0：1，n_rows，替换 = TRUE），
牲畜_cow_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_oth_cattle_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_donkeys_mules_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_goats_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_sheep_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_pigs_bin = sample(0:1, n_rows, replace = TRUE),
liver_chicken_bin = sample(0:1, n_rows, replace = TRUE),
liver_oth_poultry_bin = sample(0:1, n_rows, replace = TRUE),
pca_floor1 = sample(0:1, n_rows, replace = TRUE),
pca_wall1 = sample(0:1, n_rows, replace = TRUE),
pca_roof1 = sample(0:1, n_rows, replace = TRUE),
elec_yes = sample(0:1, n_rows, replace = TRUE)
)

# 创建 baseline_endline 列，其中包含 1400 个 0 和 1400 个 1
df$baseline_endline &lt;- c(rep(0, 1400), rep(1, 1400))

# 仅过滤基线数据
df_pca_b &lt;- df %&gt;% 
filter(baseline_endline == 0)

#Assets 
assets2 &lt;- c(&quot;ha01&quot;, &quot;ha02&quot;, &quot;ha03&quot;, &quot;ha04&quot;, &quot;ha05&quot;, &quot;ha06&quot;, &quot;ha07&quot;, &quot;ha08&quot;, &quot;ha09&quot;, &quot;ha10&quot;, &quot;ha11&quot;, &quot;ha12&quot;, &quot;ha13&quot;, &quot;ha14&quot;, &quot;ha15&quot;, &quot;ha16&quot;, &quot;ha17&quot;, &quot;ha19&quot;, &quot;ha20&quot;,&quot;sleeproom1&quot;,&quot;livestock_cow_bin&quot;,&quot;livestock_oth_cattle_bin&quot;,&quot;livestock_donkeys_mules_bin&quot;,&quot;livestock_goats_bin&quot;,&quot;livestock_sheep_bin&quot;,&quot;livestock_pigs_bin&quot;, 
&quot;livestock_chicken_bin&quot;,&quot;livestock_oth_poultry_bin&quot;,&quot;pca_floor1&quot;,
&quot;pca_wall1&quot;,&quot;pca_roof1&quot;,&quot;elec_yes&quot;)

# 标准化数据（居中并缩放至单位方差）
df_pca_b_std &lt;- as.data.frame(scale(df_pca_b[, assets2]))

# 执行 PCA 并检查特征值
pca_b &lt;- principal(df_pca_b_std, nfactors = length(assets2), rotate = &quot;none&quot;)

# 确定要保留的因子数量（例如，基于特征值 &gt; 1)
n_factors_pca_b &lt;- sum(pca_b$values &gt; 1)

# 使用选定的因子数执行 PCA
pca_b_final &lt;- principal(df_pca_b[, assets2], nfactors = n_factors_pca_b, rotate = &quot;varimax&quot;)

###### 将 PCA 应用于整个数据集 ---- 

# 来自原始 pca 的加载
loadings &lt;- pca_b_final$loadings

# 仅使用 `assets` 中的列对完整数据集 `df` 进行标准化
df_standardized &lt;- scale(df[, assets2])

# 计算完整数据集的第一个组件得分
df$pca_index &lt;- as.matrix(df_standardized) %*% loadings[, 1]

# 创建 df 五分位数
df &lt;- df %&gt;%
mutate(
quintile = ntile(pca_index, 5))

期待您的意见/回复。非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/659710/applying-loadings-of-pca-from-one-dataset-to-a-new-dataset-with-the-same-variabl</guid>
      <pubDate>Wed, 08 Jan 2025 10:45:04 GMT</pubDate>
    </item>
    <item>
      <title>使用非独立观测进行零假设显著性检验</title>
      <link>https://stats.stackexchange.com/questions/659709/null-hypothesis-significance-testing-with-non-independent-observations</link>
      <description><![CDATA[我正在处理一个数据集，其中的测量值被我们划分为不同的类别。主要目标是确定每个类别中的测量值是否与背景（定义为其他类别中所有数据点的集合）有显著差异。
但是，测量值违反了独立性假设。具体来说，测量值可能来自同一物体的不同区域（在本例中，是同一蛋白质内的不同位点）。虽然单个蛋白质之间存在跨越几个数量级的相当大的变异性，但很明显，如果测量值完全独立，分布并不是我们所期望的。虽然我相信我们使用常规统计测试获得的至少一些结果具有生物学相关性，但我还预计获得的 p 值会被夸大。
因此，问题是：什么才是解释数据中这种结构的合适方法？一种粗略而简单的方法可能是在蛋白质水平上“聚合”，并选择测量值在前 X% 和后 X% 范围内的蛋白质进行进一步分析。然而，由于上述单个蛋白质内部的巨大差异，这两组表现出相当大的重叠，使解释变得复杂。我也知道 bootstrap 方法可以解决涉及非 iid 数据的情况，但我不确定这种方法是否适合这种特定情况。]]></description>
      <guid>https://stats.stackexchange.com/questions/659709/null-hypothesis-significance-testing-with-non-independent-observations</guid>
      <pubDate>Wed, 08 Jan 2025 10:31:15 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验有什么用处？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for</link>
      <description><![CDATA[我知道，当方差未知时，学生 t 分布用于查找样本均值的置信区间，但卡方检验何时使用？据我记得，当方差未知时也使用它，并且必须将其近似为 s 而不是 σ，因此，何时必须使用卡方分布，何时必须使用学生 t 分布。
此外，卡方分布通常有什么用处？]]></description>
      <guid>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for</guid>
      <pubDate>Wed, 08 Jan 2025 09:56:10 GMT</pubDate>
    </item>
    <item>
      <title>如何找到解释最高范围数据的线性回归？</title>
      <link>https://stats.stackexchange.com/questions/659705/how-to-find-a-linear-regression-that-explains-highest-range-of-data</link>
      <description><![CDATA[
颜色是虚构的。你不知道前方的颜色。我用颜色来表示它们是从不同的潜在空间中采样的。
LR 是一组线性回归，其中每个线性回归 i 都由 拟合
$X=1,2,...,i$
$Y=y_1,y_2,...,y_i$。
如何为集合 $LR$ 找到一个最佳线性回归，使其适合大多数蓝点，但最少适合橙色点。
例如，适合 x = 1,2, ...,11, y = y1, y2, ...,y11 的模型将满足我的需求。是否有任何指标表明该模型比其他模型更大？]]></description>
      <guid>https://stats.stackexchange.com/questions/659705/how-to-find-a-linear-regression-that-explains-highest-range-of-data</guid>
      <pubDate>Wed, 08 Jan 2025 07:06:00 GMT</pubDate>
    </item>
    <item>
      <title>线性回归 - 响应变量为百分比改善或 m/s？</title>
      <link>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</link>
      <description><![CDATA[我正在尝试对包含 8 种不同跑步距离的数据集进行统计，这些距离在遵循训练方案之前和之后都有时间，并且基于距离对改进进行线性回归（所有完成时间都有所下降）。我不确定是否要转换为百分比改进或使用 m/s 之类的变量，然后减去跑步时间 1 和跑步时间 2，以便能够比较不同的距离组。显然，绝对时间差异并不大，因为更长的距离自然会有更大的改进。但我读到过，不建议将百分比改进转换为线性回归中的响应变量。我该怎么做？]]></description>
      <guid>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</guid>
      <pubDate>Tue, 07 Jan 2025 21:03:50 GMT</pubDate>
    </item>
    <item>
      <title>在 statsmodels 中 GLM 上使用哪些参数</title>
      <link>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</link>
      <description><![CDATA[我正在模拟试剂转化率对试剂 L 和 M 之间比率的依赖性。比率越高，试剂转化率越高，在 1.5 左右有一个明显的拐点。下面是我拥有的完整数据集

df = pd.DataFrame({&#39;L_M&#39;:[4.75, 3.8, 3.32, 2.85, 2.37, 1.9, 1.42, 0.95, 0.71, 0.47, 0.24, 0.09],
&#39;Conversion&#39;:[0.992, 0.987, 0.993, 1, 0.9, 0.7, 0.31, 0.1, 0.07, 0.06, 0.07, 0.065]})

为此，我使用二项式家族的 GLM 模型，以 Logit 作为链接函数（又称 Logistic 回归）。这似乎很合适，因为我正在对二项式过程中的成功率进行建模（试剂要么发生反应，要么不发生反应）。我尝试使用 statmodels.GLM 失败了，下面的代码产生了非常差的拟合效果：
# 定义因变量和自变量 
Xtrain = df[&#39;L_M&#39;] 
ytrain = df[&#39;Conversion&#39;]

# 构建模型并拟合数据 
log_reg = sm.GLM(ytrain, Xtrain, family=sm.families.Binomial()).fit()

# 推理 
df[&#39;Predicted&#39;] = log_reg.predict(Xtrain)


然而，我使用了一种 hack 的方法，为每个变量创建了 100 个原始数据实验点，每个原始 y 变量为 0 或 1，概率等于此实验点的转换，然后使用 sklearn 将 logreg 拟合到这个（膨胀很多的）数据集，效果很好，产生更好的拟合效果：
from sklearn.linear_model import LogisticRegression

# 实例化模型（使用默认参数）
logreg = LogisticRegression(random_state=16)

# 用数据拟合模型
logreg.fit(X_train, y_train)
new_X = pd.DataFrame(df[&#39;L_M&#39;])
new_X.columns = [&#39;ratio&#39;]
df[&#39;Predicted&#39;]= logreg.predict_proba(new_X)[::,1]



我在 statsmodels.GLM 中遗漏了哪些参数？
更新：为了解释我使用伯努利分布的理由，我可以说，试验总数确实是已知的，因为我准备了解决方案，并且知道试剂的浓度。每个实验点的试验次数也大致相同，而且数量巨大，约为 $10^{15}$。这就是为什么我认为将比率转换为 100 个二元结果集的方法很好地代表了现实。这个特定的实验可以被认为是有一个烧杯，并让分子一个接一个地进入烧杯。如果它发生反应 - 那就是成功，否则就是失败。这项试验在 12 个烧杯中重复进行，这些烧杯在某些特征“L-M”上有所不同。在每个烧杯中，试验重复 $10^{15}$ 至 $10^{16}$ 次，并记录发生反应的分子比例。我可以将此数据呈现为 $12\times10^{15}$ 个二元结果记录，感觉就像拥有 $12\times10^{2}$ 个记录传达了相同的信息。希望这可以解释为什么我相信伯努利模型是适用的。]]></description>
      <guid>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</guid>
      <pubDate>Tue, 07 Jan 2025 20:27:59 GMT</pubDate>
    </item>
    <item>
      <title>我应该将点大小（面积）比例缩放为 1/标准误差还是 1/SE^2？</title>
      <link>https://stats.stackexchange.com/questions/659678/should-i-scale-point-size-area-proportion-to-1-standard-error-or-1-se2</link>
      <description><![CDATA[我有一堆估计值，我想将它们绘制在 y 轴上，而 x 轴上则绘制其他值。我想传达每个点的不确定性（x 轴值已知，y 轴值是估计值）。误差线会使图变得非常混乱，所以我想使用点的大小来传达我们对每个点的确定程度。据我所知，建议人们将面积（而不是直径）视为信息量（例如，参见本文或此链接 - 不确定这些是否是标准参考文献）。
但是，有没有研究信息度量应该是 1/SE（标准误差的倒数）还是 1/SE^2（抽样方差的倒数）。鉴于对于误差线，我们将使用 +-SE 或置信区间（95% 的置信区间大约为 +- 1.96*SE），我猜是 1/SE？不知何故，我找不到是否有人尝试过实证测试这是否有效（例如，当人们被问到问题时，他们会根据所选的可视化做出适当的回答，即当给出基于 1/SE 或 1/SE^2 或其他东西的点大小时，他们是否表现得更好）。
更新：一位同事指出，对于元回归，我们通常根据信息缩放点。信息（或研究权重）与 1/方差成正比，因此可以说按 1/方差缩放面积是有意义的。虽然，查看流行的 metafor R 包，似乎我们默认获得了按元回归权重（即 1/方差）缩放的半径（与直径成比例）（我还仔细检查了源代码，并支持按 1/方差和 1/se 缩放直径）。奇怪的是，按 1/方差缩放半径意味着按 1/方差缩放面积（与半径^2 成比例）。这似乎夸大了点所获得的权重。相比之下，为了根据信息缩放面积，将半径或直径缩放 1/SE 似乎更有意义（或者如果您想要与 SE 成比例的面积，则可以将半径缩放 $1/\sqrt(SE)$）。]]></description>
      <guid>https://stats.stackexchange.com/questions/659678/should-i-scale-point-size-area-proportion-to-1-standard-error-or-1-se2</guid>
      <pubDate>Tue, 07 Jan 2025 17:47:22 GMT</pubDate>
    </item>
    <item>
      <title>对 DiD 设计中均值中位数估计量的洞察</title>
      <link>https://stats.stackexchange.com/questions/659655/insights-into-median-of-means-estimators-in-did-designs</link>
      <description><![CDATA[我目前正在开展一个研究项目，重点研究双差分 (DiD) 设计中的稳健估计量，特别是在经典的两期两组设置中。我的主要兴趣是对 y 方向异常值的稳健性（我污染了误差项），并且我一直在探索均值中位数 (MoM) 估计量作为标准 DiD 治疗效果估计量（基于均值）的潜在替代方案。
但是，我遇到了几个问题：

缺乏文献：我找不到任何先前的研究或 DiD 环境中均值中位数估计量的应用。有人知道是否存在这样的研​​究或我可以在哪里找到吗？
意外结果：当我模拟和实施 MoM 估计量时，我得到的结果与标准 DiD 估计量相同 - 即使包含受污染的数据点也是如此。这让我开始质疑为什么 MoM 估计量在这种设置下不是更稳健。

有人能解释为什么中位数估计量在这种情况下可能无法按预期执行吗？或者 DiD 框架中是否存在一些基本问题，导致 MoM 方法不太合适？
如果您有任何想法、文献建议，甚至直观的解释，我将不胜感激！
在此先感谢您的帮助！

这是我的模拟设置：

数据生成：我模拟了 N=500 和 T=2 个时间段（治疗前和治疗后）的面板数据。一半的单位在第二期进行治疗。结果方程为 $Y = 0.1 + 0.2⋅time − 0.1⋅group + β_{true}⋅D + ϵ $。这里，$β_{true} = 0.4 $，ϵ 是带有污染的随机噪声。
污染：随机噪声 𝜖 以混合形式生成：
$ ϵ = \begin{cases} N(0, \sigma^2) &amp; \text{with probability } (1 - p), \\
U(-c, c) &amp; \text{with probability } p \end{cases}$ 
我尝试过参数 $\sigma$、$p$ 和 $c$（也尝试过非对称污染），但这似乎没有什么区别。在下图中，$\sigma = 0.1$，$p = 0.1$ 和 $c = 50$。
估计量构造如下：
$ \text{DiD 估计} =
\left(
\overline{Y}_{\text{Treated, Post}} - \overline{Y}_{\text{Treated, Pre}}
\right)
-
\left(
\overline{Y}_{\text{Control, Post}} - \overline{Y}_{\text{Control, Pre}}
\right)
$
$
\text{中位数（MoM）：}
$

将数据拆分为 $ K = \sqrt(N) $ 个块
计算特定于块的 DiD 估计值：$
\text{Block DiD Estimate} = \text{DiD(Block_i})$
最终 MoM 估计值是块估计值的中位数：$
\text{MoM Estimate} = \text{Median(Block DiD Estimates)}$




块在单元级别定义。因此，每个单元被随机分配到 K 个块中的一个：
block_assignment &lt;- sample(1:K, length(unique(Y_df$unit)), replace = TRUE)

同一单元（在两个时间段内）的所有观察结果都分配给同一个块：
block_map &lt;- data.frame(unit = unique_units, block = block_assignment)Y_df &lt;- Y_df %&gt;% left_join(block_map, by = &quot;unit&quot;)



时间 (T) 取值 {1,2} = {治疗前、治疗后

组 (P) 取值 {0,1} = {对照、治疗
 P &lt;- rbinom(N, 1, 0.5)


治疗虚拟变量 (D) 取值 {0,1}
 D[P == 1, 2] &lt;- 1 # 治疗开始于第 2 阶段，适用于接受治疗的单位





 ]]></description>
      <guid>https://stats.stackexchange.com/questions/659655/insights-into-median-of-means-estimators-in-did-designs</guid>
      <pubDate>Tue, 07 Jan 2025 10:05:08 GMT</pubDate>
    </item>
    <item>
      <title>如何理解概率中的句子结构</title>
      <link>https://stats.stackexchange.com/questions/659651/how-to-understand-structure-of-sentences-in-probability</link>
      <description><![CDATA[我的教科书上是这么写的

(I) 随机选择的一名高中生吃早餐
(II) 随机选择的一名青少年是吃早餐的高中生
(III) 随机选择的一名吃早餐的青少年是高中生

我应该为上述概率选择正确的大小顺序。
书上说概率是

(I): $P(\mathrm I)=\Pr(\text{Breakfast }\mid\text{ Senior})$
(II): $P(\mathrm{II})=\Pr(\text{Breakfast} \cap \text{Senior})$
(III): $P(\mathrm{III})=\Pr(\textrm{Senior} \mid \textrm{Breakfast})$

那么我的问题是，例如在 (II) 中，为什么不是 $P(\mathrm{II})=\Pr(\text{Breakfast} \mid\text{ Senior})$？
因为我觉得如果你把 (III) 写成条件概率，你也应该把 (II) 写成条件概率。
我这里漏掉了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659651/how-to-understand-structure-of-sentences-in-probability</guid>
      <pubDate>Tue, 07 Jan 2025 08:58:39 GMT</pubDate>
    </item>
    <item>
      <title>探索性因子分析，非正态数据[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659613/exploratory-factor-analysis-non-normal-data</link>
      <description><![CDATA[我正在分析的数据对于几乎每个观察到的变量都是非正态的。我想在 R 中进行探索性因子分析 (EFA)。KMO 检验和 Bartlett 检验表明数据符合 EFA 条件。在 psych 包的文档中，我没有找到应如何处理非正态分布的数据 (https://personality-project.org/r/psych/HowTo/factor.pdf)。我的样本相对较小 (N=60)。
在进行验证性因子分析时，我可以使用 lavaan 包中的 Satorra-Bentler 校正来处理非正态数据，cfa() 函数，例如cfa(model = spec, data = df_data, std.lv=TRUE, estimator = &quot;MLM&quot;) ，这种使用稳健标准误差的校正是一种可接受的方法。 estimator=&quot;MLM&quot; 代表 Satorra-Bentler 校正。我可以用 psych 包 以某种方式做到这一点吗？
psych 包执行 EFA，fa() 函数，例如fa(data, nfactors = 4, rotate = &quot;oblimin&quot;) 并提供可行的因子，但如果我没有弄错的话，它被设计用于正态分布的数据。
如果我尝试使用 lavaan 包 中的 efa() 和 Satorra-Bentler 校正，它会发出 警告，表示协方差矩阵包含较小的负值，这可能表明模型未被识别：
efa(data = df_data, nfactors=4, rotation=&quot;oblimin&quot;, std.lv=TRUE, estimator = &quot;MLM&quot;)

警告消息：
1：lavaan-&gt;lav_model_vcov()： 
估计参数 (vcov) 的方差-协方差矩阵似乎不是
正定！最小特征值 (= -5.425180e-15) 小于零。这可能是模型无法识别的症状。
2：lavaan-&gt;lav_model_vcov()：
估计参数 (vcov) 的方差-协方差矩阵似乎不是 
正定的！最小特征值 (= -2.190854e-32) 小于零。这可能是模型无法识别的症状。


所以问题是： 如何使用非正态数据计算 EFA，可能使用 psych 包？
从响应来看，我似乎可以为 fa() 函数提供一个相关矩阵，例如 Spearman 相关，并从最大似然 (ML) 中选择不同的因子提取方法。
关于响应的更新问题：
(1) 我如何为 fa() 函数提供 Spearman 相关矩阵？我会选择“wls”作为因子提取方法。在这种情况下可以接受吗？(2) Muthen 的方法是否在任何 R 包中可用？
更新，有关数据的响应的更多信息：
数据的分布：我有 9 个变量，每个变量都有比例尺度，因此理论上它们可以从零到无穷大。在实践中，它们计算活动，因此它们包含每个研究对象的整数值。其中一些只有 2、3 个不同的值，这给出了与理论正态分布不同的分布。我添加了一个关于一个变量的 QQ 图，如下图所示。

我使用心理学包进行了 EFA，以发现数据可能具有从 1 个因子到 9 个因子的内容，以便我可以看到每种情况的拟合优度指标。 这产生了具有 4 个潜在变量的最佳指标。这就是我从 4 开始的原因。尽管如此，我并没有更改默认的因子提取，也没有提供相关矩阵。
我很感激每一个回复，谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/659613/exploratory-factor-analysis-non-normal-data</guid>
      <pubDate>Mon, 06 Jan 2025 12:41:19 GMT</pubDate>
    </item>
    </channel>
</rss>