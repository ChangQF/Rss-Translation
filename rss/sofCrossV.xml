<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Sat, 09 Nov 2024 18:20:10 GMT</lastBuildDate>
    <item>
      <title>该图同时显示平均值、标准差和标准误差吗？</title>
      <link>https://stats.stackexchange.com/questions/656996/figure-that-shows-both-mean-standard-deviation-and-standard-error</link>
      <description><![CDATA[我是新手，正在处理一组行为数据（例如持续时间），由于存在大量变异，这些数据是不正常的。数据中还有很多零，这就是为什么我计划使用均值来呈现它，因为中位数为 0，但我不确定如何最好地呈现数据。
我的主管建议使用均值、SEM 和 SD 来制作箱线图。但对我来说，这真的感觉不对，因为箱线图应该显示一般样本分布的中位数、四分位数等。
我正在考虑使用带有均值和 SEM 的条形图，因为这通常是常用的。但是，这样做不会显示数据中的巨大变异性。因此，我想知道：

是否有任何类型的图表可以同时展示均值、SD 和 SEM？我不知道。

在图表中仅使用平均值和 SEM 来呈现数据是否可以接受？我很少（如果有的话）看到我所在领域的文章使用 SD。我确实打算讨论巨大的变异性，但我不确定在结果中这样做是否重要。


编辑。我正在处理与动物行为相关的数据，例如持续时间（即移动所花费的时间）或总距离。有三种治疗方法，每种治疗方法有 30 个个体。我正在使用 R。
我想要的是在结果部分的报告/文章中呈现数据。我已经分析了数据本身。]]></description>
      <guid>https://stats.stackexchange.com/questions/656996/figure-that-shows-both-mean-standard-deviation-and-standard-error</guid>
      <pubDate>Sat, 09 Nov 2024 16:30:18 GMT</pubDate>
    </item>
    <item>
      <title>贝塔回归中的置信区间</title>
      <link>https://stats.stackexchange.com/questions/656994/confidence-intervals-in-beta-regressions</link>
      <description><![CDATA[我在研究中使用混合效应 beta 回归模型，因为我的值介于 0 和 1 之间。当我使用线性混合效应模型运行相同的分析时，我得到了类似的结果（预测因子具有相同的方向并且很重要）。在线性模型中，置信区间保持在数据范围内（即没有负值或大于 1 的值），这使得它们易于解释。虽然效果很重要，但置信区间表明效果大小较小，我想报告这一点。
但是，从我读到的内容来看，beta 回归更适合我的数据。我可以报告 beta 回归模型的置信区间，但有没有办法转换它们以便于解释？此外，beta 回归中是否有类似于效果大小的指标？]]></description>
      <guid>https://stats.stackexchange.com/questions/656994/confidence-intervals-in-beta-regressions</guid>
      <pubDate>Sat, 09 Nov 2024 16:10:47 GMT</pubDate>
    </item>
    <item>
      <title>Kruskal-Wallis H 检验</title>
      <link>https://stats.stackexchange.com/questions/656990/kruskal-wallis-h-test</link>
      <description><![CDATA[在总共 4,200 名患者中，Q1-Q4 组基于连续变量 tyg 进行划分。在基线分析期间，总人口的年龄是一个偏态连续变量，中位数为 58.0（第 25 百分位数：52.0，第 75 百分位数：64.0）。每组的中位年龄（第 25 和第 75 百分位数）如下：
第 1 组：60.0（55.0，67.0）第 2 组：58.0（52.0，65.0）第 3 组：57.0（51.0，63.0）第 4 组：57.0（50.0，62.8）当执行 Kruskal-Wallis H 检验来比较各组时，获得的 p 值小于 0.001。但是审稿人质疑为什么四组之间的年龄差异不大，但p值却小于0.001。
我重新检查了数据和统计方法（Kruskal-Wallis H检验），发现结果仍然是p&lt;0.001。我应该如何解释审稿人的问题并做出回应？还是我的统计方法不正确？]]></description>
      <guid>https://stats.stackexchange.com/questions/656990/kruskal-wallis-h-test</guid>
      <pubDate>Sat, 09 Nov 2024 13:41:32 GMT</pubDate>
    </item>
    <item>
      <title>STATA - 滚动回归面板数据 - 平行趋势假设</title>
      <link>https://stats.stackexchange.com/questions/656993/stata-rolling-regression-panel-data-parallel-trend-assumption</link>
      <description><![CDATA[我是 STATA 的新手，正在尝试对面板数据进行滚动回归分析；
我有 54 家公司 10 年来的月度股票价格，并将它们转换为面板数据。前 10 家公司是治疗组，其余是对照组（差异差异模型）

生成面板回归分析：
encode code, gen(con_cod)
xtset con_cod time
xtreg ln_ri dummy, fe

生成滚动回归分析
asreg ln_ri dummy, wind(time 36)


第二部分的问题在于，它为每个公司在每个时间点提供了系数。但是，我需要将治疗组与对照组进行回归，并得到每个月的总体系数（平行趋势假设，图 3。y 轴：系数和 x 轴：时间）
任何帮助都将不胜感激。
面板数据
平行趋势]]></description>
      <guid>https://stats.stackexchange.com/questions/656993/stata-rolling-regression-panel-data-parallel-trend-assumption</guid>
      <pubDate>Sat, 09 Nov 2024 11:35:45 GMT</pubDate>
    </item>
    <item>
      <title>在逻辑回归中，什么使得曲线拟合得好</title>
      <link>https://stats.stackexchange.com/questions/656987/what-makes-a-curve-a-good-fit-in-the-context-of-logistic-regression</link>
      <description><![CDATA[因为我想更好地理解为什么在逻辑回归的背景下分离是一个问题，所以我在 R 中创建了两个模型，一个模型中 y 在 $x=5$ 处完全分离，另一个模型则不是。为了可重复性，这里是生成的数据集和用于生成具有一定范围的 beta（具有相同截距）的各种曲线估计的代码，并添加了 ML 估计器，并将所有内容与数据点一起绘制：
library(dplyr)
library(ggplot2)
library(boot)
library(tidyr)

df_sep &lt;- data.frame(x=seq(0,9.9,by=0.1),y=c(rep(0,50),rep(1,50)))
df_notsep &lt;- data.frame(x=seq(0,9.9,by=0.1),y=c(rbinom(50,1,0.2),rbinom(50,1,0.8)))

mod_sep &lt;- glm(y~x,df_sep,family=&quot;binomial&quot;)
# (截距) x 
# -1650.0708 333.3476 
df_sep &lt;- df_sep %&gt;% mutate(y_beta100= inv.logit(coef(mod_sep)[1]+100*x), 
y_beta200=inv.logit(coef(mod_sep)[1]+200*x), 
y_beta300=inv.logit(coef(mod_sep)[1]+300*x),
y_beta400=inv.logit(coef(mod_sep)[1]+400*x))
df_sep$y_pred &lt;-预测（mod_sep，type=&quot;response&quot;）

df_sep_long &lt;- df_sep %&gt;% pivot_longer（cols = starts_with(&quot;y&quot;))

mod_notsep &lt;- glm(y~x,df_notsep,family=&quot;binomial&quot;)
# (截距) x 
# -2.5422783 0.5515725 

df_notsep &lt;- df_notsep %&gt;% mutate(y_beta0_2=inv.logit(coef(mod_notsep)[1]+0.2*x), 
y_beta0_3=inv.logit(coef(mod_notsep)[1]+0.3*x),
y_beta0_5=inv.logit(coef(mod_notsep)[1]+0.5*x),
y_beta0_7=inv.logit(coef(mod_notsep)[1]+0.7*x))

df_notsep$y_pred &lt;- predict(mod_notsep,type=&quot;response&quot;)

df_notsep_long &lt;- df_notsep %&gt;% pivot_longer(cols = starts_with(&quot;y&quot;))

ggplot(df_sep_long, aes(x = x, y = value, colour = name, shape=name, group = name)) + 
geom_point(data=df_sep_long %&gt;% filter(name==&quot;y&quot;),alpha = 0.5) + 
geom_path(data=df_sep_long %&gt;% filter(name!=&quot;y&quot;), aes(x = x, y = value, colour = name, group = name)) +
theme_bw()

ggplot(df_notsep_long, aes(x = x, y = value, colour = name, shape=name, group = name)) + 
geom_point(data=df_notsep_long %&gt;% filter(name==&quot;y&quot;),alpha = 0.5) + 
geom_path(data=df_notsep_long %&gt;% filter(name!=&quot;y&quot;), aes(x = x, y = value, colour = name, group = name)) +
theme_bw()

我没有更好地理解分离数据的收敛问题，但另一方面，能够想象一下为什么预测的 beta 在该上下文中提供最大似然，因为它几乎是一个分段函数，因此很明显它是允许 $\max{x}$ 的值与 $y=0$ 的值和 $\min{x}$ 的值与 $y=1$ 的值之间的最短线的值。
但是，对于非分离数据，我不清楚为什么 ML 是最佳拟合。如果我们有一个 OLS，我们会尝试最小化点和线之间的距离，这很容易理解，但是在估计最大化似然函数的 $\beta$ 时我们到底在做什么？

]]></description>
      <guid>https://stats.stackexchange.com/questions/656987/what-makes-a-curve-a-good-fit-in-the-context-of-logistic-regression</guid>
      <pubDate>Sat, 09 Nov 2024 10:22:18 GMT</pubDate>
    </item>
    <item>
      <title>在岭回归中修复 $\lambda$ 的动机</title>
      <link>https://stats.stackexchange.com/questions/656986/motivation-for-fixing-lambda-in-ridge-regression</link>
      <description><![CDATA[在岭回归中，您想要优化：
$$
f(\beta_0,\beta_1,\beta_n)=\sum_n(y_n-\beta_0+\beta_1x^1_n,+\beta_2x^2_n)^2
$$
其中
$x^i=(x_1^i,x_2^i,...,x_N^i)$
取决于
$||(\beta_0,\beta_1,\beta_2)||_2\leq c$。
我们可以定义$\mathcal{L}(\beta_0,\beta_1,\beta_2,\lambda)=f(\beta_0,\beta_1,\beta_2)-\lambda(c-||(\beta_0,\beta_1\beta_2)||_2)$。从此以后，我们可以寻找 $\mathcal{L}$ 的临界点。
但是，据我所知，不是对 $\lambda$ 进行优化，而是只对 $(\beta_0,\beta_1,\beta_2)$ 进行优化，这给出了岭回归的形式。我的问题是，为什么我们要固定$\lambda$，而不是试图找到$(\beta_0,\beta_1,\beta_2,\lambda)$，使得$\mathcal{L}$为零，因此$f$对于那些$\beta_0,\beta_1,\beta_2$是最优的？]]></description>
      <guid>https://stats.stackexchange.com/questions/656986/motivation-for-fixing-lambda-in-ridge-regression</guid>
      <pubDate>Sat, 09 Nov 2024 10:21:23 GMT</pubDate>
    </item>
    <item>
      <title>关于 Kruskal-Wallis H 检验结果解释的澄清 [重复]</title>
      <link>https://stats.stackexchange.com/questions/656984/clarification-on-the-interpretation-of-kruskal-wallis-h-test-results</link>
      <description><![CDATA[在总共 4,200 名患者中，Q1-Q4 组根据连续变量 tyg 进行划分。在基线分析期间，总人口的年龄是一个偏态连续变量，中位数为 58.0（第 25 百分位数：52.0，第 75 百分位数：64.0）。每组的中位年龄（第 25 和第 75 百分位数）如下：
第 1 组：60.0（55.0，67.0）
第 2 组：58.0（52.0，65.0）
第 3 组：57.0（51.0，63.0）
第 4 组：57.0（50.0，62.8）
当执行 Kruskal-Wallis H 检验来比较各组时，获得的 p 值小于 0.001。但是审稿人质疑为什么四组之间的年龄差异不大，但p值却小于0.001。
我重新检查了数据和统计方法（Kruskal-Wallis H检验），发现结果仍然是p&lt;0.001。我应该如何解释审稿人的问题并做出回应？还是我的统计方法不正确？]]></description>
      <guid>https://stats.stackexchange.com/questions/656984/clarification-on-the-interpretation-of-kruskal-wallis-h-test-results</guid>
      <pubDate>Sat, 09 Nov 2024 08:50:12 GMT</pubDate>
    </item>
    <item>
      <title>为什么大型 GAMM 模型中的随机项会使曲线变得尖锐和弯曲？</title>
      <link>https://stats.stackexchange.com/questions/656978/why-does-a-random-term-in-a-large-gamm-model-make-the-curves-spiky-and-wiggly</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/656978/why-does-a-random-term-in-a-large-gamm-model-make-the-curves-spiky-and-wiggly</guid>
      <pubDate>Fri, 08 Nov 2024 23:00:16 GMT</pubDate>
    </item>
    <item>
      <title>MCMC 采样器给出明显不同的分布——哪个更好？</title>
      <link>https://stats.stackexchange.com/questions/656972/mcmc-samplers-give-noticeably-different-distributions-which-is-better</link>
      <description><![CDATA[背景
我们构建了两个 MCMC 采样器，用于对具有 4-20 多个参数的分层贝叶斯模型的后验进行采样。这两个采样器使用不同的方法。对于大多数数据集，它们给出的结果几乎相同（基于比较边际分布）。但是，我们遇到了一些数据集，结果明显不同——边际分布具有相同的基本形状，但根据采样器略有偏移。
问题
如何最好地展示哪个采样器生成的样本更接近后验？在任何给定点，比例后验似然都很容易计算。
一个想法
到目前为止，我的一个想法是使用核密度估计来计算两个样本相对于后验分布的重要性权重。权重的方差将表明每个 KDE 与后验的匹配程度（权重的低方差应该对应于更好的匹配）。但是，如果有更好的方法，我会很高兴。
更新以回应评论
关于先验和后验的性质：先验都是（适当的）伽马，后验也是适当的。第二个采样器的原因是因为第一个采样器的超参数具有高自相关性，因此需要大量样本。第二个采样器使用重新参数化来消除大部分自相关性。提供不太匹配样本的数据集也会导致其中一个超参数的众数为 0。我强烈怀疑这两件事是相关的。
关于错误的可能性：第一个采样器已经非常彻底地检查过错误，第二个采样器的错误检查稍微少一些。不过，我认为这并不能保证，这也是想要比较样本质量的原因之一——出现差异是因为第一个在一般情况下是一个较差的采样器，还是因为第二个有潜在的错误？]]></description>
      <guid>https://stats.stackexchange.com/questions/656972/mcmc-samplers-give-noticeably-different-distributions-which-is-better</guid>
      <pubDate>Fri, 08 Nov 2024 20:43:05 GMT</pubDate>
    </item>
    <item>
      <title>相关组：混合效应作为因子模型</title>
      <link>https://stats.stackexchange.com/questions/656851/correlated-groups-mixed-effects-as-a-factor-model</link>
      <description><![CDATA[这已发布在量化交易所中，但没有人感兴趣，所以我想我可以在这里重新发布它。
我正在尝试建立一个 Fama-French 风格的基本因子模型。我有 FF 因子以及我的资产所属的行业。对于不熟悉 FF 的人，您可以将其视为一些基本因素（与通货膨胀或 GDP 相同）
该模型指定为：
$$
\mathbf{Y} = \mathbf{X}\beta + \mathbf{Z}\gamma + \epsilon 
$$
其中，$\mathbf{Y}$ 是 $N$ 资产和 $T$ 每周回报的横截面时间序列矩阵； $\mathbf{X}$ 是 Fama-French 因子（HML、SMB、WML 和 Market-RF 投资组合的每周回报）；$\mathbf{Z}$ 是随机效应矩阵。我有一个简单的案例，其中只有随机截距而没有随机斜率，即$\mathbf{Z}$ 是一个由 1 和 0 组成的矩阵，表示资产所属的行业。 $\epsilon$ 是分布为 $N(0,1)$ 的残差，$\gamma$ 是随机截距 $N(0,\mathbf{G})$，而 $\beta$ 是本例中通常的固定斜率（和一个固定截距）。
我们可以使用 statsmodels&#39; 混合线性模型 来拟合该模型。
数据框如下所示：

我们只需通过以下方式拟合模型：
md = smf.mixedlm(&quot;y ~ MktRF + SMB + HML + WML&quot;, X, groups=X[&quot;sector&quot;])
res = md.fit()

摘要如下所示。然而，结果很糟糕。残差和拟合值之间几乎存在完美的线性关系。


我怀疑协方差矩阵$\mathbf{G}$出了问题，因为它接近奇异值。关于这个问题，我读到的常见方法之一是，随机效应可能过度参数化，并且可能存在多重共线性。因此，我计算了（出于必要，做了一些填充）行业之间的相关性 - 我们的随机截距。

嗯，是的，这些组是相互关联的（尤其是一些组）。
问题：

我们能做些什么？我可以重新分组一些相关的行业，但这似乎不是一个好主意。例如，“金融”和“工业”的相关性很高，但这些是完全不同的组，可能与其他组具有不同的相关性。

协方差真的是个问题吗？也可能是其他原因？我读过这篇帖子，其中讨论了零方差并不一定意味着模型有问题，即可能没有组变异，所有变异都被残差变异捕获 - 但这仍然不能解释数据和残差之间的线性关系。

]]></description>
      <guid>https://stats.stackexchange.com/questions/656851/correlated-groups-mixed-effects-as-a-factor-model</guid>
      <pubDate>Wed, 06 Nov 2024 18:28:13 GMT</pubDate>
    </item>
    <item>
      <title>样本量越大，错误拒绝原假设的风险是否会越大？</title>
      <link>https://stats.stackexchange.com/questions/656828/does-the-risk-of-incorrectly-rejecting-the-null-hypothesis-increase-with-a-large</link>
      <description><![CDATA[我经常看到类似这样的话：“如果样本量足够大，较小的效应量可以产生显著的结果”。我不太明白这一点。
对我来说，这听起来是这样的：增加样本量，你最终肯定会拒绝零假设（也就是说，如果你将样本量增加到一定大小，你 100% 肯定会拒绝 0 假设）。
重新阅读此主题。据我所知，鉴于我在统计学方面的经验，零假设被正确地拒绝了。
决定在 ttest_1samp 测试的代码中检查这一点，使 popmean=15.03 与样本 (mean=15) 的差异非常小。滑块选择 N - 样本，并实时重新绘制分布，垂直条是上限值（蓝色）和 p_value（橙色）。
更新 08.11.2024
由于数组的平均值与声明的平均值不同，为了准确起见，我通过向其添加一些值使值 mu_0 = mu_1 + value。mu_0 在图表上以绿色绘制，置信区间为洋红色。该图还显示 p 值本身、平均值和间隔（CI）。更改 N，图形将动态重新绘制。现在我还检查了均值差异很大，需要样本量来拒绝更小的样本。可以通过 value 设置差异。
import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import t, norm, ttest_1samp
from matplotlib.widgets import Slider

fig, ax = plt.subplots()
fig.subplots_adjust(bottom=0.25)

ax_n = fig.add_axes([0.25, 0.15, 0.65, 0.03])
s_n = Slider(ax_n, &quot;N&quot;, valmin=7, valmax=30000,valinit=21, valstep=1)

value = 0.03

def update(val):
N = s_n.val
rvs = np.sort(norm.rvs(loc=15,比例=1，大小=N，random_state=1））
    mu, std = np.mean(rvs), np.std(rvs)
    mu_0 = mu + 值
    z_values = (rvs - mu) / std
    df = N - 1
    pdf = t.pdf(rvs, loc=mu, 比例=std, df=df)
    tst = ttest_1samp(rvs, popmean=mu_0)
    l、r = t.ppf(1 - 0.975, df=df), t.ppf(0.975, df=df)
    ci = tst.confidence_interval(confidence_level=0.95)
    ci_0, ci_1 = np.round(ci, 4)
    ci = (ci - mu) / std
ax.clear()
ax.plot(z_values, pdf, lw=2, color=&quot;red&quot;)
ax.axvline(x=(mu_0 - mu) / std, color=&quot;green&quot;, label=&quot;mean 0&quot;)# 0 假设的平均值
ax.axvline(x=ci[0], linestyle=&quot;--&quot;, color=&quot;magenta&quot;, label=&quot;CI&quot;)
ax.axvline(x=ci[1], linestyle=&quot;--&quot;, color=&quot;magenta&quot;)
ax.axvline(x=l, linestyle=&quot;--&quot;, color=&quot;blue&quot;, label=&quot;alfa&quot;)# alfa
ax.axvline(x=r, linestyle=&quot;--&quot;, color=&quot;blue&quot;)# alfa
ax.axvline(x=t.ppf(tst[1]/2, df=df), linestyle=&quot;--&quot;, color=&quot;orange&quot;, label=&quot;p-value&quot;)#p-value
ax.axvline(x=-t.ppf(tst[1]/2, df=df), linestyle=&quot;--&quot;, color=&quot;orange&quot;)#p-value
ax.text(0, (np.max(pdf) + np.min(pdf))/2, &quot;p_value =&quot; + str(round(tst[1], 4))
+ &quot;\n&quot; + &quot;mu_0 =&quot; + str(round(mu_0, 5)) + &quot;\n&quot; + &quot;mu_1 =&quot; + str(round(mu, 5)) +
&quot;\n&quot; + &quot;ci_0 =&quot; + str(ci_0) + &quot;\n&quot; + &quot;ci_1 =&quot; + str(ci_1), fontsize=12)
ax.legend()

s_n.on_changed(update)
update(0)

plt.show()

]]></description>
      <guid>https://stats.stackexchange.com/questions/656828/does-the-risk-of-incorrectly-rejecting-the-null-hypothesis-increase-with-a-large</guid>
      <pubDate>Wed, 06 Nov 2024 11:43:12 GMT</pubDate>
    </item>
    <item>
      <title>因子分析，特征值与平方因子载荷与碎石图特征值 [重复]</title>
      <link>https://stats.stackexchange.com/questions/656682/factor-analysis-eigenvalues-vs-squared-factor-loadings-vs-scree-plot-eigenval</link>
      <description><![CDATA[我对一个包含 8 个变量的数据集进行了探索性因子分析。我还进行了 KMO 和 Bartlett 检验，以确保数据符合本次检查的条件。
我准备了碎石图，并对不同数量的因子（从 1 到 9）进行了因子分析，使用了 2 种不同的旋转：方差最大和斜率最小。到目前为止一切顺利。
肘部法建议使用 2 个因子，碎石图上的特征值仅对 1 个因子高于 1。从整个数据集捕获的因子的方差在 5 个因子的情况下达到最大值，并且卡方拟合优度检验在因子大于 3 时失去其显著性。
我复制了 fa() 调用的输出，其中包含 5 个因子和方差最大旋转（psych 包）：
使用 method = minres 的因子分析
调用：fa(r = df_sq1sq9, nfactors = 5, rotate = &quot;varimax&quot;)
基于相关矩阵的标准化载荷（模式矩阵）
MR1 MR3 MR2 MR5 MR4 h2 u2 com
SQ1 0.13 0.37 0.01 -0.01 0.42 0.33 0.665 2.2
SQ2 0.16 0.07 -0.06 0.09 0.66 0.48 0.517 1.2
SQ3 0.33 0.88 0.01 0.04 0.21 0.94 0.063 1.4
SQ4 0.43 0.10 0.16 0.82 0.11 0.89 0.106 1.7
SQ5 0.85 0.16 -0.06 0.17 0.28 0.86 0.141 1.4
SQ6 0.85 0.34 0.02 0.25 0.16 0.92 0.079 1.6
SQ7 0.34 0.53 0.25 0.30 0.04 0.55 0.450 2.9
SQ8 -0.04 0.08 0.98 0.12 -0.07 1.00 0.005 1.1
SQ9 0.75 0.30 0.00 0.21 0.13 0.70 0.296 1.5

MR1 MR3 MR2 MR5 MR4
SS 载荷 2.44 1.45 1.06 0.91 0.81
比例方差 0.27 0.16 0.12 0.10 0.09
累积方差 0.27 0.43 0.55 0.65 0.74
解释比例 0.37 0.22 0.16 0.14 0.12
累积比例 0.37 0.58 0.74 0.88 1.00

平均项目复杂度 = 1.7
检验假设 5 个因素就足够。

df 零模型 = 36，目标函数 = 4.83，卡方 = 290.78

模型的 df 为 1，目标函数为 0

残差的均方根 (RMSR) 为 0

df 校正的残差均方根为 0.01

谐波 n.obs 为 65，经验卡方为 0.02，概率 &lt; 0.89

总 n.obs 为 65，似然卡方 = 0.15，概率 &lt; 0.69 


fa() 调用的结果还包含一个列表 fa_results[[&quot;e.values&quot;]]
其值如下：
fa_results[[&quot;e.values&quot;]]
4.2142109 1.3128191 1.0559469 0.8085356 0.5378116 0.4403557 0.2829727 0.2444338 0.1029137

问题 1：为什么平方载荷之和与特征值不同？理解这一点很重要，因为如果我计算因子可以捕获的方差，那么在一种情况下，我会得到 74%（SS 载荷），而根据列表中的特征值，我会得到 88%。
答案： (1) PCA 将整个方差归因于已识别的主成分，而因子分析假设方差被划分为共享方差（由因子解释的方差）和唯一方差（不能由因子解释）；(2) 由于旋转，它们也不同。请参阅易于阅读的清晰解释：https://stats.oarc.ucla.edu/spss/seminars/efa-spss/ 或 ttnphns 发布的链接上的答案：因子分析中初始特征值和平方载荷和之间的关系是什么？
问题 2：碎石图（平行分析）上的特征值与fa_results[[&quot;e.values&quot;]]  并且它们也与 ss 因子载荷不同。为什么？发布的链接上没有解释这一点。
感谢大家的回复！]]></description>
      <guid>https://stats.stackexchange.com/questions/656682/factor-analysis-eigenvalues-vs-squared-factor-loadings-vs-scree-plot-eigenval</guid>
      <pubDate>Sun, 03 Nov 2024 19:10:19 GMT</pubDate>
    </item>
    <item>
      <title>方差-偏差权衡公式，用于 X 固定且 X 随机的简单线性回归</title>
      <link>https://stats.stackexchange.com/questions/656503/variance-bias-tradeoff-formula-for-simple-linear-regression-with-both-x-fixed-an</link>
      <description><![CDATA[我试图使用简单的线性回归来理解方差-偏差权衡公式。但有些公式我无法推导。我将通过首先对固定的$X$进行操作，然后对随机的$X$进行操作来解释我的意思。以下是偏差-方差权衡公式：

1.简单线性回归，固定$X$。
我假设模型为$Y=\beta_0+\beta_1 X+\epsilon$，其中$\epsilon$为$N(0,\sigma^2)$。我假设我有 $k$ 个 x 值 $x_1,x_2,\ldots,x_k$，它们是固定的，并且我观察到 $y_1,y_2,\ldots,y_k$。
定义 $S_{xy}=\sum_{j=1}^k (x_j-\overline{x})(y_j-\overline{y}), S_{xx}=\sum_{j=1}^k (x_j-\overline{x})^2$。
从初等统计学中，我们可以得出最小二乘估计量为：
$\hat{\beta_1}=\frac{S_{xy}}{S_{xx}}, \hat{\beta_0}=\overline{y}-\hat{\beta_1}\overline{x}.$
假设您想在点 $x^*$ 处观察一个新的观测值。然后我们有
$\hat{f}(x^*)=\hat{\beta_0}+\hat{\beta_1}x^*$。在初等统计学教材中，有如下公式：$E[\hat{f}(x^*)]=E[\hat{\beta_0}+\hat{\beta_1}x^*]=\beta_0+\beta_1x^*=f(x^*)$。
还有如下公式：$V(\hat{f}(x^*))=\sigma^2\left(\frac{1}{n}+\frac{(\overline{x}-x^*)^2}{S_{xx}}\right)$。
问题：您是否同意偏差-方差权衡公式变为：
$E((Y^*-\hat{f}(x^*))^2)=\sigma^2\left(\frac{1}{n}+\frac{(\overline{x}-x^*)^2}{S_{xx}}\right)+0+\sigma^2$?
在这种情况下，我是否对线性回归中所做的工作做出了正确的假设？
2.简单线性回归，随机$X$。
仍然假设$Y=\beta_0+\beta_1 X+\epsilon$，还假设$\epsilon$和$X$是独立的。还假设$X\text{~} N(\mu_X,\sigma_X^2)$。现在假设您观察到$(x_1,y_1),\ldots,(x_k,y_k)$。然后您按上述方法创建 $\hat{\beta_0}$ 和 $\hat{\beta_1}$，从而创建 $\hat{f}$。假设您还固定了点 $x^*$。现在 $E((Y^*-\hat{f}(x^*))^2)$ 是多少？注意，它不可能是上面那样，因为点 $x_1,\ldots,x_k$ 不是固定的，但公式依赖于它们（例如 $S_{xx}$），但 $E((Y^*-\hat{f}(x^*))^2)$ 应该仍然存在？]]></description>
      <guid>https://stats.stackexchange.com/questions/656503/variance-bias-tradeoff-formula-for-simple-linear-regression-with-both-x-fixed-an</guid>
      <pubDate>Wed, 30 Oct 2024 11:26:46 GMT</pubDate>
    </item>
    <item>
      <title>多项式回归中数据中心化和标准化的正确方法</title>
      <link>https://stats.stackexchange.com/questions/656248/correct-way-of-centering-and-standardizing-data-in-polynomial-regression</link>
      <description><![CDATA[我想使用套索回归建立一个回归模型。我的理解是，我应该首先缩放和集中我的数据（例如，如此处所问：回归中需要集中和标准化数据）。
但是，我想在模型中包含测量数据的交互和二次项（因此，如果我的数据包含 A 和 B，我还想在我的模型中包含 A^2、A*B 和 B^2 作为预测因子。
我的问题是，是否先计算交互和二次项（因此先构建 X），然后独立标准化每一列，还是先标准化 A 和 B，然后从这些标准化值构建 X。
对于正态最小二乘回归，这没有什么区别，因为多项式回归模型在线性变换下不变，当且仅当多项式是“分层良好公式化”的。因此，使用任一方法的模型所做的预测都是相同的（就像它们对非标准化数据所做的预测一样）。但每个系数的值和符号可能不同（事实上，该值几乎肯定会不同），并且由于 Lasso 会规范系数的大小，因此它应该得出不同的结论。
我找不到关于哪种操作顺序更好的确切答案。一方面，在构建 X 之后进行第二次标准化将确保所有预测因子具有相同的比例并正确居中，这在直觉上是合理的。但另一方面，您会失去这些术语之间的数学联系，其中 (A^2)_scaled 不会是 A_scaled 的平方。首先缩放也是我在一些统计软件中看到的方法（尽管对于普通最小二乘回归来说，这并不重要），并且由于套索回归不一定是“层次良好地制定的”，所以我不确定它是否有效。
我很感激任何关于为什么这种方式更好/正确的建议或解释。最好有一些参考资料。]]></description>
      <guid>https://stats.stackexchange.com/questions/656248/correct-way-of-centering-and-standardizing-data-in-polynomial-regression</guid>
      <pubDate>Thu, 24 Oct 2024 11:52:06 GMT</pubDate>
    </item>
    <item>
      <title>SLOPE 与 Benjamini-Hochberg 程序之间的联系</title>
      <link>https://stats.stackexchange.com/questions/652379/the-link-between-slope-and-the-benjamini-hochberg-procedure</link>
      <description><![CDATA[假设我们正在执行 $m$ 测试，该测试生成 $m$ 个 p 值 $p_1 \leq \ldots \leq p_m$（按索引排序）。 BH 程序如下：

对于给定的 $\alpha$（所需的 FDR 水平），找到最大的 $k$，使得 $p_k \leq \frac{k}{m}\alpha$。
对所有 $i\leq k$，拒绝零假设 $H_i$。

现在，我已经阅读了一些关于基于此的 Sorted L-One Penalized Estimation (SLOPE) 模型，但我很难看出其中的确切联系。在 SLOPE 论文 中，BH 程序描述如下（并且该公式用于推导 SLOPE 惩罚序列）：
对于回归模型 $y = X\beta + z,$，其中 $z \sim N(0,\sigma^2)$ 和正交矩阵 $X^TX = I$，将 $\tilde{y} = X^Ty$ 的条目排序为 $|y|_{(1)} \geq \ldots \geq |y|_{(m)}$ 和拒绝所有假设 $H_i$ 其中 $i\leq k_\text{BH}$ 其中
$$k_\text{BH} = \max\left\{k:
\frac{|\tilde{y}|_{(k)}}{\sigma} \geq \Phi^{(-1)}\left(1-\frac{k\alpha}{2m}\right)\right\}.$$
现在，我想知道这两者之间有什么联系以及分位数是如何使用的。我猜这与我们在 SLOPE 中使用 $\tilde{y}$ 而不是 p 值有关，我们需要以某种方式将其转换为与 p 值比较相同的比例。]]></description>
      <guid>https://stats.stackexchange.com/questions/652379/the-link-between-slope-and-the-benjamini-hochberg-procedure</guid>
      <pubDate>Tue, 06 Aug 2024 12:44:45 GMT</pubDate>
    </item>
    </channel>
</rss>