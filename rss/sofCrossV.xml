<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Sun, 12 May 2024 06:18:45 GMT</lastBuildDate>
    <item>
      <title>如何将多元 DKW 不等式排列到 n 的上限？</title>
      <link>https://stats.stackexchange.com/questions/647066/how-to-arrange-multivariate-dkw-inequality-to-a-upper-bound-on-n</link>
      <description><![CDATA[多元 DKW 不等式 给出
$$\Pr \left[ \sup_{t \in \mathbb{R}^k} \left| F_n(t) - F(t) \right| &gt; \epsilon \right] \leq (n+1)k e^{-2n\epsilon^2},$$
其中 $F_n(t)$ 是 eCDF，$F(t)$ 是总体 CDF、$\epsilon \in \mathbb{R}_{&gt;0}$ 和 $n,k \in \mathbb{N}$。
我想要找到的是给定其他输入的 $n$ 的上限。我尝试使用 Lambert $W$ 函数&lt; /a&gt; 但我未能成功地将事物安排成正确的形式来应用它。]]></description>
      <guid>https://stats.stackexchange.com/questions/647066/how-to-arrange-multivariate-dkw-inequality-to-a-upper-bound-on-n</guid>
      <pubDate>Sun, 12 May 2024 05:30:17 GMT</pubDate>
    </item>
    <item>
      <title>带约束和加权最小二乘的矩阵分解</title>
      <link>https://stats.stackexchange.com/questions/647065/matrix-decomposition-with-constraints-and-weighted-least-squares</link>
      <description><![CDATA[我们有一个矩阵，$\mathbf{X}$，包含 6 个不同结果之间的概率分布，因此每行 $\mathbf{x}_i$ 总和为 1。我们想要执行降维，以便每一行都是两个分量的线性插值，$\mathbf{f }$ 和 $\mathbf{g}$，也是概率分布。换句话说，每一行 $\mathbf{x}_i$ 都有一个潜在变量 $c_i$ 所以$\mathbf{x}_i \sim c_i \mathbf{f} + (1 - c_i) \mathbf{g}$。此外，我们还有以下限制：

组件中的某些概率为 0：

$\mathbf{f}$ 的形式为 $(f_0, 0, f_2, f_3, f_4, f_5)$。
$\mathbf{g}$ 的形式为 $(0, g_1, g_2, g_3, g_4, 0)$.


作为概率分布，$\mathbf{f}$ 和 $\mathbf{g}$总和也为 1。

此外，对于准确估计，某些结果比其他结果更重要，因此我们希望对最小二乘误差进行加权：让行的估计 $x_i$为 $\hat{x}_i = c_i \mathbf{f} + (1 - c_i) \mathbf{g}$。我们希望最小化残差平方的加权和 $$\sum_{j=0}^{5} w_j (x_j - \hat{x}_j)^2.$ $
我目前正在使用 Python，但愿意使用其他工具来解决这个问题。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/647065/matrix-decomposition-with-constraints-and-weighted-least-squares</guid>
      <pubDate>Sun, 12 May 2024 03:45:08 GMT</pubDate>
    </item>
    <item>
      <title>条件模型中的 VIF 较低，但零膨胀模型中的 VIF 较高</title>
      <link>https://stats.stackexchange.com/questions/647064/low-vif-in-conditional-model-but-high-in-zero-inflated-model</link>
      <description><![CDATA[我正在使用 glm ZINB 制作模型，根据 57 个地点的观察结果预测青蛙的丰度。我使用了性能包中的 check_model 函数，它显示最佳模型在条件模型中具有较低的 vif（vif&lt;5），但在零膨胀模型中具有较高的 vif（最多 50）（我在下面附上了图片） 。我是统计领域的新手，我想知道应该如何处理这些结果。我知道 vif&gt;5 的变量不应在 glms 中组合，但我不确定它如何适用于零膨胀 glms 中的两个模型。
]]></description>
      <guid>https://stats.stackexchange.com/questions/647064/low-vif-in-conditional-model-but-high-in-zero-inflated-model</guid>
      <pubDate>Sun, 12 May 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>使用 fpp3 的 R 中的 SEATS 和 X-11 分解模型结果不一致</title>
      <link>https://stats.stackexchange.com/questions/647062/inconsistent-results-with-seats-and-x-11-decomposition-models-in-r-using-fpp3</link>
      <description><![CDATA[在使用 fpp3 包在 R 中应用时间序列分解模型时，我遇到了不一致的行为，并且需要一些指导。我有来自巴西中央银行的 IPCA 数据时间序列，我已成功将其转换为 tsibble 对象。当我应用 X-11 分解模型时，它运行得很好，但我遇到了 SEATS 模型的问题。
&lt;前&gt;&lt;代码&gt;库(rbcb)
库（fpp3）

ipc_pre = get_series(433, start_date=&#39;2007-02-01&#39;, end_date=&#39;2017-01-01&#39;)
ipca &lt;- ts(ipc_pre$`433`, start=c(2007,02), freq=12)
# 将 ts 对象转换为 tsibble 以与 fpp3 一起使用
ipca_tsibble &lt;- as_tsibble(ipca)

# 应用 X-11 分解模型，没有任何问题：
ipca_x11_dcmp &lt;- ipca_tsibble |&gt;;
  模型（x11 = X_13ARIMA_SEATS（值〜x11（）））|&gt;
  成分（）

自动绘图（ipca_x11_dcmp）+
  labs(title = “使用 X-11 进行 IPCA 分解。”)

# 应用SEATS模型时遇到错误：
ipca_seats_dcmp &lt;- ipca_tsibble |&gt;;
  模型（座位= X_13ARIMA_SEATS（值〜座位（）））|&gt;
  成分（）

错误消息是：
SEATS 中使用的模型不同：(0 2 2)
`transmute()` 中的错误：
ℹ 在参数中：`cmp = map(.fit, elements)`。
由 `mutate()` 中的错误引起：
ℹ 在参数中：`seasonal = dcmp[, &quot;adjustfac&quot;]`。
由 `dcmp[, &quot;adjustfac&quot;]` 中的错误引起：
！限制论坛指数
运行 rlang::last_trace() 查看错误发生的位置。

在第二次尝试时，SEATS 模型在没有 Components() 函数的情况下执行，但我不确定为什么第一次尝试失败。
我很困惑为什么 X-11 模型可以无缝运行，而 SEATS 模型却不能。 Components() 函数是否存在问题，或者是否还有其他原因？对于这两个模型之间的不同结果的任何见解或解释，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/647062/inconsistent-results-with-seats-and-x-11-decomposition-models-in-r-using-fpp3</guid>
      <pubDate>Sun, 12 May 2024 02:01:59 GMT</pubDate>
    </item>
    <item>
      <title>如何处理极小的训练数据？</title>
      <link>https://stats.stackexchange.com/questions/647061/how-to-deal-with-extremely-small-training-data</link>
      <description><![CDATA[我有大约 100 行带有标签的数据
国家/地区 |类别 |标签
-----------+----------------+--------------------
[美国、英国、日本] |电子| 1
[美国] |体育 | 1
[台湾、英国] |杂货| 2
[日本] |预订 | 3

大约 900 行没有标签的数据
国家/地区 |类别
-----------+--------------
[美国、英国] |运动的
[台湾] |运动的
[美国] |杂货店
[日本] |电子的

使用这么小的数据集做分类模型可以吗？如果是这样，我该如何编码Country？或者是否有任何其他聚类/数学方法可以用来处理这种情况？]]></description>
      <guid>https://stats.stackexchange.com/questions/647061/how-to-deal-with-extremely-small-training-data</guid>
      <pubDate>Sun, 12 May 2024 01:44:33 GMT</pubDate>
    </item>
    <item>
      <title>用两个变量相乘来优化目标？</title>
      <link>https://stats.stackexchange.com/questions/647060/optimizing-objective-with-two-variables-multiplied-with-each-other</link>
      <description><![CDATA[假设您想要优化以下目标函数：
$$\min_{a,b} \Vert ab - W \Vert_2^2$$
$$\min_{a,b} \Vert (ab) \cdot W \Vert_2^2 \quad\quad\text{(阿达玛积)}$$
$$\min_{a,b} \Vert abW^T \Vert_2^2$$
$$\min_{a,b} \Vert ab + Wb^T \Vert_2^2$$
其中 $a \in \mathbb{R}^{m,n}$ 和 $b \in \ mathbb{R}^{n,p}$ 是可学习矩阵，$W \in \mathbb{R}^{m,p}$ 是给你的一些常数矩阵。
由于 $a$ 和 $b$ 通过某些操作错综复杂地链接在一起（可以是，例如，乘法、除法、哈达玛积等），有哪些方法可以优化这些类型的多元目标函数？
普通梯度下降有效吗？我认为我的理解还不足以肯定地说。
经过初步研究，我们似乎可以进行贪婪搜索：在每次迭代中，按住 $a$ （或 $b$) 常量，然后仅针对单个变量进行优化 $b$ （或 $a$ ）通过梯度下降。每次迭代后，交替变量。
贪婪搜索听起来与坐标下降类似，我想知道是否有其他方法在整体上更加稳健并且不易出现局部极小值。
似乎有另一种方法称为 EM（期望最大化算法），它假设 $a$ 或 $b $ 是一个潜在变量，但除此之外，感觉它在做与贪婪搜索相同的事情，只是在随机设置中。
除了贪婪搜索之外，还有哪些其他优化算法可以处理复杂链接的多个变量（通过乘法、除法等）？]]></description>
      <guid>https://stats.stackexchange.com/questions/647060/optimizing-objective-with-two-variables-multiplied-with-each-other</guid>
      <pubDate>Sun, 12 May 2024 00:51:01 GMT</pubDate>
    </item>
    <item>
      <title>如何提高LSTM模型的预测质量</title>
      <link>https://stats.stackexchange.com/questions/647059/how-to-improve-prediction-quality-of-lstm-model</link>
      <description><![CDATA[我正在尝试在 MATLAB 中训练基于 LSTM 的模型，以在给定时间序列的 365 个先前值的情况下预测 365 个下一个值。输入形状为 (1000, 365)，输出形状为 (1000, 365)，即有 1000 行，其中每个输入行是 1 年数据的时间序列，每个对应的输出行是对下一年的预测。这些值本身是浮点数。在输入模型之前，我使用以下公式对值进行标准化：
train_stdized = (train_array - mu_train) / sig_train;

哪里
mu_train = 训练数据的平均值
sig_train = 训练数据的标准差

我的模型架构和训练选项如下：
层 = [sequenceInputLayer(lag)
lstmL层(512)
全连接层(150)
完全连接层（未来）]；

Training_options = TrainingOptions(“adam”, “指标”,[“rmse”], ...
        初始学习率=0.01，详细=真，详细频率=10，...
        MaxEpochs=50,MiniBatchSize=1, ...
        ObjectiveMetricName=“rmse”, ...
        ValidationData={x_val, y_val}, ValidationFrequency=10, ...
        LearnRateSchedule=“分段”，LearnRateDropPeriod=100，LearnRateDropFactor=0.1，...
        OutputNetwork=&quot;最佳验证&quot;);


问题是，预测趋势与输入趋势不匹配。相反，情况恰恰相反——预测范围要么远高于输入数据，要么远低于输入数据。也许下面的图表会让你更清楚：


此处。

但是，这并没有太大区别。

我想知道是否可以尝试其他方法来改进预测，以便它们的范围反映输入的范围。我在这方面不是很有经验，所以如果这听起来像是一个基本问题，我深表歉意。感谢你的帮助。

附：我试图包含所有我认为相关的内容。但是，如果我错误地遗漏了任何信息，请告诉我，我会添加它。]]></description>
      <guid>https://stats.stackexchange.com/questions/647059/how-to-improve-prediction-quality-of-lstm-model</guid>
      <pubDate>Sun, 12 May 2024 00:18:38 GMT</pubDate>
    </item>
    <item>
      <title>样本量和变异系数</title>
      <link>https://stats.stackexchange.com/questions/647058/sample-size-and-coefficient-of-variation</link>
      <description><![CDATA[我想知道样本量和变异系数 (CV) 之间的关系。 CV 定义为标准差与平均值的比率。我们是否可以说“样本量越小，CV 越大，因此关系呈负相关”？
库(dplyr)
库（r样本）

cv &lt;- 函数(x){返回(sd(x)/mean(x))}
sim &lt;- data.frame(id=1:100000,
                变量=rpois(100000, 5),
                group10=样本(1:10,100000,替换=TRUE),
                group100=样本(1:100,100000,替换=TRUE),
                group1000=样本(1:1000,100000,替换=TRUE),
                group10000=样本(1:10000,100000,替换=TRUE)
                ）
AA &lt;- NULL
G &lt;- c(10,100,1000,10000)
for(k in 1:4){
  for(j in 1:G[k]){

  如果（k==1）{
    A &lt;- dplyr::filter(sim, group10==j)
  }否则如果(k==2){
    A &lt;- dplyr::filter(sim, group100==j)
  }否则如果(k==3){
    A &lt;- dplyr::filter(sim, group1000==j)
  }否则如果(k==4){
    A &lt;- dplyr::filter(sim, group10000==j)
  }
  
  AA &lt;- rbind(AA, c(G[k],
                    j,
                    n行(A),
                    平均值（A  $变量），
                var(A$变量),
                    sd(A$变量),
                简历（A$变量）
                    ）
              ）
  }
}
AA &lt;- as.data.frame(AA)
colnames(AA)＜-c(“组”、“组N”、“N”、“平均值”、“Var”、“SD”、“CV”)
头(AA)
尾部(AA)

AA10 &lt;- AA %&gt;% 过滤器(组==10)
AA100 &lt;- AA %&gt;% 过滤器(组==100)
AA1000 &lt;- AA %&gt;% 过滤器(组==1000)
AA10000 &lt;- AA %&gt;% 过滤器(组==10000)

(简历(sim$变量))

par(mfrow = c(1, 4))
图(AA10$N,AA10$CV, ylim=c(0,1.2), ylab=“CV”,xlab=“样本大小”,
     主=“10组”）
abline(h=cv(sim$variable),col=&quot;red&quot;)
绘图(AA100$N,AA100$CV, ylim=c(0,1.2), ylab=&quot;CV&quot;,xlab=&quot;样本大小&quot;,
 主要=“100组”）
abline(h=cv(sim$变量),col=“红色”)
绘图(AA1000$N,AA1000$CV, ylim=c(0,1.2), ylab=“CV”,xlab=“样本大小”,
     main=“1,000 组”）
abline(h=cv(sim$variable),col=&quot;red&quot;)
绘图(AA10000$N,AA10000$CV, ylim=c(0,1.2), ylab=&quot;CV&quot;,xlab=&quot;样本大小&quot;,
 主要=“10,000组”）
abline(h=cv(sim$变量),col=“红色”)
dev.off()

]]></description>
      <guid>https://stats.stackexchange.com/questions/647058/sample-size-and-coefficient-of-variation</guid>
      <pubDate>Sun, 12 May 2024 00:00:18 GMT</pubDate>
    </item>
    <item>
      <title>通过逻辑回归绘制交互作用</title>
      <link>https://stats.stackexchange.com/questions/647057/plotting-interaction-by-logistic-regression</link>
      <description><![CDATA[我使用 R 绘制了我计算的 logit 函数中的交互项。
我使用标准 cat_plot() 来绘制交互作用。我清楚地看到两条线不平行，这是非常明显的（但它们也没有交叉）。
但交互作用项的 P 值为 0.079。
我该如何解释这个？我仍然可以假设存在交互作用，但交互作用并不显着吗？
我可能认为它不重要的原因是我的样本中主持人的稀有性。难道是这个原因吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/647057/plotting-interaction-by-logistic-regression</guid>
      <pubDate>Sat, 11 May 2024 23:51:25 GMT</pubDate>
    </item>
    <item>
      <title>选择自相关事件</title>
      <link>https://stats.stackexchange.com/questions/647056/select-autocorrelated-events</link>
      <description><![CDATA[我有 $N$ 个时间序列，每个时间序列的长度为 $L$ 和 $k$ 特征描述具有边 $ s \in \{-1, 1\}$ 的事件。我想构建一个模型，根据事件的特征来选择事件，即 $f : \mathbb{R}^k \rightarrow \{0, 1\}$。我希望选定的事件具有较高的自相关性，即每个时间序列中它们的平均值应接近 1 或 -1。我对预测侧面本身的模型不感兴趣，而是寻找自相关事件之间的内在相似性（就我拥有的特征而言）。我认为一个好的损失函数是 $-\text{std}(\underset{\text{of timeseries}}{\text{mean}}(side))$ 计算模型将选择的事件。如果我们使模型连续，我们可以将其给出的分数作为事件的权重，并获得可微的损失函数。这就是我所做的，但有一些问题。该模型（一开始我只采用了一层 MLP，即线性回归）很快收敛到选取所有事件。我必须通过正则化来惩罚它，这只会增加平均输出分数。通过正确调整正则化系数，可以使模型实际上选择自相关性高于整个群体的一部分事件。但模型在训练过程中对该参数非常敏感，我担心如果训练数据发生变化，模型可能会停止工作，并且必须再次调整参数。如果模型发生变化，它也必须进行调整，例如添加更多层。
您以前是否遇到过类似的问题，是否有更好的方法？如何使模型更加稳健，以便具有次优正则化系数会稍微降低质量但不会破坏一切？]]></description>
      <guid>https://stats.stackexchange.com/questions/647056/select-autocorrelated-events</guid>
      <pubDate>Sat, 11 May 2024 22:35:15 GMT</pubDate>
    </item>
    <item>
      <title>使用深度集成量化预测不确定性：如何组合拉普拉斯分布？</title>
      <link>https://stats.stackexchange.com/questions/647054/quantifying-prediction-uncertainty-using-deep-ensembles-how-to-combine-laplace</link>
      <description><![CDATA[对于回归问题，我想训练深度神经网络的集合来预测标记的输出以及不确定性，类似于论文中提出的方法使用深度集成进行简单且可扩展的预测不确定性估计。作者使用高斯分布的负对数似然（NLL）作为损失函数，使模型隐式学习方差
$$
\ell_{\text{NLL}_G}(x, y, θ) =
-\log p_G\left(y \mid \mu_\theta, \sigma_\theta^2\right)=
\frac{\log \sigma_\theta^2}{2}+
\frac{\left(y-\mu_\theta\right)^2}
{2 \sigma_\theta^2}+
\text{常量}
$$
其中 $y$ 是目标，$\mu_\theta$ 是预测平均值， $\sigma^2_\theta$ 是预测方差。
但是，我注意到当使用平均误差而不是均方误差作为损失函数时，我的模型收敛得更好。因此我想利用拉普拉斯 NLL，它对异常值应该更加鲁棒：
$$
\ell_{\text{NLL}_L}(x, y, θ) =
-\log p_L\left(y \mid \mu_\theta, b_\th​​eta\right)=
\log b_\th​​eta +
\frac{\left|y-\mu_\theta\right|}{b_\th​​eta} +
\text{常量}
$$
在高斯情况下，作者将各个模型的预测均值和方差结合起来$m$：
$$
\mu_*=M^{-1} \sum_m \mu_{\theta_m} \\
\sigma_*^2=M^{-1} \sum_m\left(\sigma_{\theta_m}^2+\mu_{\theta_m}^2\right)-\mu_*^2
$$
计算混合拉普拉斯分布的均值和方差的正确方法是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/647054/quantifying-prediction-uncertainty-using-deep-ensembles-how-to-combine-laplace</guid>
      <pubDate>Sat, 11 May 2024 21:35:48 GMT</pubDate>
    </item>
    <item>
      <title>“随机变量的置信区间”是不正确的术语吗？</title>
      <link>https://stats.stackexchange.com/questions/647022/is-it-incorrect-terminology-to-say-confidence-interval-of-a-random-variable</link>
      <description><![CDATA[我见过“总体参数不是随机变量”的说法。在讨论置信区间时。
例如此处
&lt;块引用&gt;
请务必注意，总体参数不是随机变量。

在频率论解释的背景下，我毫不犹豫地接受这一说法。根据这种解释，总体参数是固定的，但未知。它们不是随机变量。
但是术语置信区间是否也带有特定的解释？或者它只是数学函数（如 wiki 所示）：
&lt;块引用&gt;


例如，假设我认为 𝜃（总体参数）是随机变量，X 是随机样本。考虑到它们的联合分布，我计算函数：
P( u(x) &lt; 𝜃 &lt; v(x)) = c ∀ 𝜃
看起来像下图中的红线：

（图借自此处）
现在，我将红线称为“随机变量的置信区间”是错误的吗？ 𝜃？]]></description>
      <guid>https://stats.stackexchange.com/questions/647022/is-it-incorrect-terminology-to-say-confidence-interval-of-a-random-variable</guid>
      <pubDate>Sat, 11 May 2024 10:12:44 GMT</pubDate>
    </item>
    <item>
      <title>生物神经元的多向传播 - 我们可以/应该重新创建它，例如与关节分布神经元？</title>
      <link>https://stats.stackexchange.com/questions/646959/multidirectional-propagation-of-biological-neurons-could-should-we-recreate-it</link>
      <description><![CDATA[虽然人工神经网络经过相当多的单向传播训练，但生物神经元中的动作电位传播是对称的，例如“动作电位的轴突传播在两个方向上发生的情况并不少见”（摘自“轴突中信号传播和碰撞的动力学&quot; PRE)。由于可能，它们应该针对这种多向传播进行进化优化，这可能是至关重要的，例如对于学习（目前还不太理解），意识（？）
是否考虑过以多向方式运行的人工神经元？
一种方法是以某种方式包含联合分布模型的表示，例如$\rho(x,y,z)$，它允许通过替换一些变量并标准化来找到任何方向的条件分布 - 下面是来自 &lt; 的廉价实际实现a href=&quot;https://arxiv.org/pdf/2405.05097&quot; rel=&quot;nofollow noreferrer&quot;&gt;https://arxiv.org/pdf/2405.05097 ，允许许多额外的训练方法 - 生物学可以使用一些其中？
有不同的方法吗？研究这个方向？
多向传播对于（例如学习）生物神经网络重要/关键吗？
]]></description>
      <guid>https://stats.stackexchange.com/questions/646959/multidirectional-propagation-of-biological-neurons-could-should-we-recreate-it</guid>
      <pubDate>Fri, 10 May 2024 04:53:45 GMT</pubDate>
    </item>
    <item>
      <title>任何点过程都可以细化为泊松点过程吗？</title>
      <link>https://stats.stackexchange.com/questions/646958/can-any-point-process-be-thinned-into-a-poisson-point-process</link>
      <description><![CDATA[以下情况是否正确？对于任何点过程 $X$ 都存在一个过程  $\mathcal{P}$ 和强度 $\lambda &gt; 的泊松点过程 $Y$ 0$ 这样从 $X$ 中提取实现并应用 $\mathcal{P} 的过程$ 与从 $Y$ 获取实现的过程没有区别。
请注意，我只是询问 $\mathcal{P}$ 和 $Y$ 是否存在span&gt;，而不是 $\mathcal{P}$ 是否实用。
我看过 Møller &amp; Schoenberg (2010)，特别是 Schoenberg (2009)，试图看看是否以及如何从 $\mathcal{P}$ 的存在。 math-container&quot;&gt;$\mathcal{P}$ 实用。我还回顾了有关点过程独立细化的文献。
参考文献
Møller, J., &amp;勋伯格，F.P. (2010)。 将空间点过程细化为泊松过程。 应用概率的进展，42(2), 347–358。
勋伯格，F.P. (2009)。 使用 Papangelou 强度将空间点过程细化为泊松过程.]]></description>
      <guid>https://stats.stackexchange.com/questions/646958/can-any-point-process-be-thinned-into-a-poisson-point-process</guid>
      <pubDate>Fri, 10 May 2024 04:52:12 GMT</pubDate>
    </item>
    <item>
      <title>最大似然法是否必须应用于简单随机样本或实现？</title>
      <link>https://stats.stackexchange.com/questions/646815/must-maximum-likelihood-method-be-applied-on-a-simple-random-sample-or-on-a-real</link>
      <description><![CDATA[我想我的麻烦不是一个大问题，但问题是：当一个人应用最大似然时，他考虑实现$(x_1, \dots, x_n)$ 简单随机样本 (SRS)，从而得出 ML估计。但如果有人想谈论偏差、一致性等问题，就必须提到估计器，对吗？
例如，在这个维基百科示例中，考虑正常示例，他们最终得到 $\hat{\mu}=\bar{x}$ ，然后写入 $\mathbb{E }(\hat{\mu})=\mu$ 这可以说有点草率，因为获取确定性数量的期望值并没有多大意义。
因此，我的问题是为什么通常（我的意思是，大多数来源：维基百科、教科书等）MLE 是基于实际数据（即基于 SRS 的实现）而不是基于 SRS 构建的（也就是说，随机变量的集合 $(X_1, \dots, X_n)$)，提供了计算期望值等有意义的估计器？我想这是一种虚假的微妙之处，没有什么实际意义，因为它足以“大写字母化”。估计得到相应的估计器，但我还是想问。]]></description>
      <guid>https://stats.stackexchange.com/questions/646815/must-maximum-likelihood-method-be-applied-on-a-simple-random-sample-or-on-a-real</guid>
      <pubDate>Wed, 08 May 2024 12:55:10 GMT</pubDate>
    </item>
    </channel>
</rss>