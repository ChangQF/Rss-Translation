<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Fri, 12 Apr 2024 06:18:51 GMT</lastBuildDate>
    <item>
      <title>我对字典学习的理解或者代码是错误的吗？</title>
      <link>https://stats.stackexchange.com/questions/644852/is-my-understanding-or-my-code-for-dictionary-learning-wrong</link>
      <description><![CDATA[所以我正在做一些休闲字典学习，我不确定我是否有正确的理解或者我的代码是否是错误的。目前，我采用 MNIST 数据集 $Y$，图像大小为 $28\times 28$ ，并尝试查找字典 $D$ 和稀疏代码 $X$，s.t. $Y \大约 DX$。在我的实验中，我采用 $Y\in \mathbb{R}^{28^2 \times 3}$，其中每一列都是手写的样本数字“5” （所以我不会将图像转换成补丁）。我初始化一个随机字典 $D\in\mathbb{R}^{28^2 \times 3}$，也就是说我总共有三个原子，即每个样品一个。对于我的稀疏代码 $X\in\mathbb{R}^{3\times 3}$，我将稀疏级别设置为 1，s.t。我在每一列中仅获得一个非零值。
现在，我希望字典算法能够“学习”只需将 $Y$ 的列放入 $D$ 中，并使稀疏代码接近到看起来像的东西
\begin{align}
X \approx \begin{pmatrix}
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{pmatrix},
\end{对齐}
或行的任何其他排列。这样， $D$ 中的一个原子只需乘以身份，我就可以恢复我的 $Y$&lt; 列/span&gt; 完全正确。但是，我获得的代码的输出
\begin{align}
X = \begin{pmatrix}
0 &amp; 5.58615414&amp; 6.33388578\\
0 &amp; 0 &amp; 0 \\
9.55769633&amp; 0 &amp; 1
\end{pmatrix}。
\end{对齐}
现在，由于第一行中有两个非零值，这意味着我可以几乎完全恢复 $y_1$ ，但不能恢复 $y_2, y_3$ 因为它们都混合了。
我的直觉错了吗？当我只取两个样本时，它按预期工作，即 $Y\in\mathbb{R}^{28^2 \times 2}$。
提前致谢！
我的代码：
导入 mnist
将 numpy 导入为 np
将 matplotlib.pyplot 导入为 plt
从 sklearn. Linear_model 导入 o​​rthogonal_mp

def approx_ksvd_dictionary_update(D: np.ndarray, X: np.ndarray, Y: np.ndarray) -&gt; np.ndarray：
    # 取自 https://github.com/fubel/sparselandtools/tree/master/sparselandtools
    n, K = D.形状
    对于范围 (K) 内的 k：
        # print(f&quot;{k} / {K}&quot;)
        wk = np.nonzero(X[k, :])[0]
        如果 len(wk) == 0:
            继续
        D[:, k] = 0
        g = np.transpose(X)[wk, k]
        d = np.matmul(Y[:, wk], g) - np.matmul(D, X[:, wk]).dot(g)
        d = d / np.linalg.norm(d)
        g = np.matmul(Y[:, wk].T, d) - np.transpose(np.matmul(D, X[:, wk])).dot(d)
        D[:, k] = d
        X[k, wk] = g.T
    返回D

# 选择仅代表数字5的MNIST数据
数据 = mnist.mnist()
图像 = 数据[0][数据[1] == 5]

np.随机.种子(1)

# 选择三张样本图片，放入Y的列中
N = 3
图片 = []
对于范围 (N) 内的 j：
    imgs.append(images[j].reshape(28*28, 1))
Y = np.concatenate(imgs, 轴=1)
Y /= np.max(Y)

# 用 K=N 个原子初始化随机字典
K=N
n = Y.形状[0]
D = np.zeros([n, K])
对于范围 (K) 内的 j：
    D[:, j] = np.random.randn(n)
    D[:, j] *= 1 / np.linalg.norm(D[:, j])

# 进行字典学习
n_nonzeros_coefs=1
对于范围（10）内的 _：
    X = orthogonal_mp(D, Y, n_nonzero_coefs=n_nonzeros_coefs)
    D = approx_ksvd_dictionary_update(D, X, Y)

打印（X）
]]></description>
      <guid>https://stats.stackexchange.com/questions/644852/is-my-understanding-or-my-code-for-dictionary-learning-wrong</guid>
      <pubDate>Fri, 12 Apr 2024 05:26:21 GMT</pubDate>
    </item>
    <item>
      <title>CNN（一维和二维）必须采用相同大小的输入吗？</title>
      <link>https://stats.stackexchange.com/questions/644846/must-a-cnn-both-1d-and-2d-take-input-of-the-same-size</link>
      <description><![CDATA[我认为 CNN 输入数据必须始终具有相同的维度。如果我们输入一维表格数据，列的编号必须相同；如果我们输入 2D 图像数据，所有图像必须具有相同的尺寸。这种大小调整可能发生在 CNN 的输入层中，也可能发生在 CNN 外部 - 在数据处理阶段。
因此，我给我的主管写了以下电子邮件：
&lt;块引用&gt;
你好。
CNN 可以处理一维表格数据和二维数据。但是，它无法处理具有不同列长度的一维表格数据。
我们不能使用以下文件：
ABC.dat
 ------------------数据-------------------- --matrix- -
  6 赖氨酸 C 4.768 7.342 10.221 1 0 1 0 1 0 1 2 3
  7 酪氨酸C 8.992 6.431 4.110 0 1 0 1 0 0 1 2 3
  8 SER C 2.345 8.901 12.345 0 0 1 0 1 1 1 2 3

PQR.dat
--------------------数据-------------------- ---矩阵--
  0 丙氨酸 C 0.000 0.000 0.000 1 1 1 1 1 0 1 2 3 4 5 6
  1 GLU C 6.691 9.772 0.000 1 1 1 1 1 0 1 2 3 4 5 6
  2 PHE C 6.601 8.709 12.389 0 0 0 0 1 0 1 2 3 4 5 6
  3 参数 C 6.489 9.682 11.525 0 0 0 0 0 0 1 2 3 4 5 6
  4 HIS C 6.249 0.000 0.000 0 0 0 0 0 0 1 2 3 4 5 6
  5 ASP C 0.000 0.000 0.000 0 0 0 0 0 0 1 2 3 4 5 6

XYZ.dat
&lt;前&gt;&lt;代码&gt;--------------------数据-------------------- ---- - -矩阵 -  -  - -
  0 GLU C 0.773 6.403 9.702 1 0 1 0 1 0 0 1 2 3 4 5 6 7 8 9 10
  1 酪氨酸C 1.710 3.978 3.997 1 0 1 1 1 0 1 1 2 3 4 5 6 7 8 9 10
  2 ASP C 2.564 9.689 4.051 1 0 1 0 0 0 1 1 2 3 4 5 6 7 8 9 10
  3 酪氨酸C 4.485 4.886 8.724 1 0 0 1 1 1 0 1 2 3 4 5 6 7 8 9 10
  4 HIS C 6.145 7.992 9.437 1 0 0 1 0 0 1 1 2 3 4 5 6 7 8 9 10
  5 丙氨酸 C 5.373 3.810 6.506 1 0 0 0 0 0 1 1 2 3 4 5 6 7 8 9 10
  6 HIS C 6.314 3.519 1.994 0 1 0 1 1 0 0 1 2 3 4 5 6 7 8 9 10
  7 ASP C 8.348 3.026 9.201 0 1 1 0 1 0 1 1 2 3 4 5 6 7 8 9 10
  8 酪氨酸 5.810 2.019 9.094 0 1 0 1 1 1 0 1 2 3 4 5 6 7 8 9 10
  9 赖氨酸 C 7.361 1.221 2.055 0 0 0 1 0 0 0 1 2 3 4 5 6 7 8 9 10

我们可以将这三个文件输入 CNN 吗？当然，我们可以。然而，问题将是标签。在我们的数据中，每一行的第三列都有一个标签。如果我们将这些表作为 2D 数据输入 CNN，我们就会丢失这些标签。因为 CNN 需要为每条 2D 数据分配一个标签，在我们的例子中就是蛋白质名称。
另一个问题是 CNN 必须（并且必须）采用相同维度的 2D 输入。在我们的例子中，每个文件都有不同的尺寸。因此，我们必须将文件大小调整为通用尺寸（平均值、中位数或众数），或者需要将较小的图像用零填充到文件中找到的最大尺寸。在我们的例子中，这将一事无成。因为由于上一段中描述的标签问题，我们的 2D 数据首先将毫无用处。
亲切的问候。
学生

我的教授不同意这一点。
他写道：
&lt;块引用&gt;
&lt;块引用&gt;
另一个问题是 CNN 必须（并且必须）采用相同维度的 2D 输入。

不，没必要。以我们之前的论文为例：对于 N 个残基的蛋白质，输入的大小为 NxK，每个残基有 K 个特征。输出层为Nx2。对于每种蛋白质，输入的大小不同，我们使用相同的网络。它使用一维卷积。
&lt;块引用&gt;
在我们的例子中，每个文件都有不同的尺寸。因此，我们必须将文件大小调整为通用尺寸（平均值、中位数或众数），或者需要将较小的图像用零填充到文件中找到的最大尺寸。在我们的例子中，这将一事无成。因为由于上一段中描述的标签问题，我们的 2D 数据首先将毫无用处。

不，这不是必需的。
但是，您可以使用填充，否则您的图层将会收缩。更具体地说，当您的卷积窗口为 WxW 时，大小为 NxN 的输入将产生 (N-W) x (N-W) 的输出。
亲切的问候
主管

你能解决我的困惑吗？
CNN（一维和二维）是否必须接受相同大小的输入？]]></description>
      <guid>https://stats.stackexchange.com/questions/644846/must-a-cnn-both-1d-and-2d-take-input-of-the-same-size</guid>
      <pubDate>Fri, 12 Apr 2024 02:50:18 GMT</pubDate>
    </item>
    <item>
      <title>FDR 置信区间调整</title>
      <link>https://stats.stackexchange.com/questions/644844/fdr-adjustment-for-confidence-intervals</link>
      <description><![CDATA[我在 R 中有一个多元回归模型，我应该报告 beta 估计值、95 CI 和 p 值。模型中有很多变量，因此我使用 Benjamini-Hochberg 方法来调整 p 值。然而，现在 CI 和调整后的 p 值不匹配；有些变量的 95% CI 不包含零，但调整后的 p 值不显着。
我想知道如何调整 FDR 的 CI。我可以使用调整后的 p 值通过下面本文中描述的过程对 CI 进行逆向工程吗？
https://www.bmj.com/content/343/bmj.d2090 
谢谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/644844/fdr-adjustment-for-confidence-intervals</guid>
      <pubDate>Fri, 12 Apr 2024 01:07:52 GMT</pubDate>
    </item>
    <item>
      <title>扩展隐马尔可夫模型 (HMM) 参数估计</title>
      <link>https://stats.stackexchange.com/questions/644843/extended-hidden-markov-models-hmm-parameter-estimation</link>
      <description><![CDATA[对于更简单的 HMM，我们可以使用 Viterbi 训练（而不是解码）或 Baum Welch 等算法来估计最能描述观察到的数据的参数。
当使用更复杂的多元 HMM 或二阶 HMM 时，我们如何做同样的事情？是否有资源可以用来查看在这些情况下如何调整算法和设置？如果有它们的伪代码而不是描述，那就更好了，尽管我知道早期的算法（如向前和向后）也会改变。]]></description>
      <guid>https://stats.stackexchange.com/questions/644843/extended-hidden-markov-models-hmm-parameter-estimation</guid>
      <pubDate>Fri, 12 Apr 2024 00:37:56 GMT</pubDate>
    </item>
    <item>
      <title>如何在 R 中通过复制运行回归</title>
      <link>https://stats.stackexchange.com/questions/644854/how-to-run-regression-with-replication-in-r</link>
      <description><![CDATA[看看是否有人知道如何在 R 中运行带有复制的回归分析。我有一个数据集，每个 X 值都有多个 Y 值。我可以取 Y 值的平均值，但这会丢弃信息和统计分析将失去力量。我有一段参考文本描述了数学，当然我可以尝试手工完成，但是其中的乐趣在哪里？
我尝试寻找将这一点考虑在内的代码示例，但尚未提出任何建议。我想象它就在那里，但出于某种原因我找不到它。我确实知道我不是在寻找多元回归或具有多个回归量的回归。
示例数据：

&lt;标题&gt;

x
y


&lt;正文&gt;

15
5,2,5,6,1,3


30
2,4,7,1,4


40
1,6,8


45
5、4、9、10、2、9



对于我的应用程序，我有 45 个不同的 x 变量和 3 个关联的 y 数据点。我相信带有复制的回归应该是可能的，如果需要手动计算而不是在 R 中计算，那么这只是一个沉重的负担。
感谢您的宝贵时间。]]></description>
      <guid>https://stats.stackexchange.com/questions/644854/how-to-run-regression-with-replication-in-r</guid>
      <pubDate>Fri, 12 Apr 2024 00:01:55 GMT</pubDate>
    </item>
    <item>
      <title>组内系统回顾和荟萃</title>
      <link>https://stats.stackexchange.com/questions/644842/within-group-systematic-review-and-metalysis</link>
      <description><![CDATA[我正在研究 SRMA，以评估干预前后研究的效果。效应大小是速率比。怎么做？对于使用没有真实对照组的前后设计给出的结果，我需要进行什么修正？
谢谢]]></description>
      <guid>https://stats.stackexchange.com/questions/644842/within-group-systematic-review-and-metalysis</guid>
      <pubDate>Thu, 11 Apr 2024 23:43:21 GMT</pubDate>
    </item>
    <item>
      <title>我们如何摆脱 MLE 中的 $p(x|\theta)$</title>
      <link>https://stats.stackexchange.com/questions/644841/how-do-we-get-rid-of-px-theta-in-mle</link>
      <description><![CDATA[简单的问题...通常机器学习的介绍会告诉您以下内容：

您想要最大化$p(\theta|D)$
应用贝叶斯定理$p(\theta|D) \propto p(D|\theta)p(\theta)$
如果你考虑 $p(\theta)$ 常数，你会得到 MLE，如果你考虑它高斯，你会得到 L2 正则化等等

但是，我想更详细一点，这是我的问题：
$$
p(\theta|D) \propto p(D|\theta)p(\theta) = p(x,y|\theta)p(\theta) = p(y|x, \theta)p(x| θ)p(θ)
$$
现在，通常我们优化的实际上是 $p(y|x,\theta)p(\theta)$，所以我的问题是...下我们要删除哪个假设 $p(x|\theta)$？
我们是否认为 $x$ 独立于 $\theta$ 并且保持不变？.. .]]></description>
      <guid>https://stats.stackexchange.com/questions/644841/how-do-we-get-rid-of-px-theta-in-mle</guid>
      <pubDate>Thu, 11 Apr 2024 22:11:00 GMT</pubDate>
    </item>
    <item>
      <title>估计梯度提升树中的最佳树数</title>
      <link>https://stats.stackexchange.com/questions/644840/estimating-optimal-number-of-trees-in-gradient-boosted-trees</link>
      <description><![CDATA[我试图提出一个公式来估计梯度提升树中树的最佳数量（我知道在实践中使用提前停止或交叉验证是更好的方法，但我只想得到一个近似值）。 
我的直觉如下：设 $y = \mu + \epsilon$ 其中 $y$ 和 $\epsilon$ 是向量。令学习率为 $l$。然后，根据我对 gbdt 在每一步的理解，我们将预测大约移动为 $l \times \epsilon_t$，其中 $\epsilon_t$ 在 $t$ 时是无法解释的错误（可能更糟，因为我们无法适应  $\epsilon_t$ 与树完全一样）
因此，在 $t$ 树之后，剩余误差应约为 $(1-l)^t \epsilon$&lt; /span&gt;，这意味着我们正在解释 $(1-(1-l)^t)^2$ 方差百分比
因此，如果理想模型可以解释 $r$ 的变化百分比，那么可以从  中找到最佳树数$(1-(1-l)^t)^2 = r$ 这意味着 $t=\frac{\log(1-\sqrt{r} )}{\log(1-l)}$。我知道这只是一个近似值，实际上它会受到树深度等的影响。
我尝试在合成数据上对此进行测试，结果非常不同。例如，在具有 10 个特征的模型中，根据此公式， $l=0.01$ 和理想的 r2 为 10% 应该有大约 40 棵树，但实际上我发现基于早期停止，近 1000 棵树是最佳的。我还发现，即使数据生成过程没有改变，最佳树数也会随着数据大小的增加而增加。我使用 lightgbm 进行实验。
您能否指出我的推理中的错误，并解释为什么最佳树数随着数据大小的增加而增长？]]></description>
      <guid>https://stats.stackexchange.com/questions/644840/estimating-optimal-number-of-trees-in-gradient-boosted-trees</guid>
      <pubDate>Thu, 11 Apr 2024 22:04:49 GMT</pubDate>
    </item>
    <item>
      <title>如何证明K-Means将所有空间分割成凸多边形？</title>
      <link>https://stats.stackexchange.com/questions/644838/how-to-prove-that-k-means-splits-all-space-into-convex-polygons</link>
      <description><![CDATA[我想证明 K-Means 算法将整个对象空间分割成凸（可能没有边界）多边形。我试图利用K-Means算法收敛的事实并与之矛盾，但我没有成功。
任何帮助将不胜感激！]]></description>
      <guid>https://stats.stackexchange.com/questions/644838/how-to-prove-that-k-means-splits-all-space-into-convex-polygons</guid>
      <pubDate>Thu, 11 Apr 2024 21:41:29 GMT</pubDate>
    </item>
    <item>
      <title>如何在 R 中模拟具有指定最小值和最大值以及 cov 和平均值的相关数据</title>
      <link>https://stats.stackexchange.com/questions/644848/how-to-simulate-correlated-data-with-specified-min-and-max-and-cov-and-mean-in-r</link>
      <description><![CDATA[我想将具有固定均值和协方差矩阵的数据模拟为多变量分布，其中生成的数据必须在范围 [floor ,上限] 内。
使用 MASS::mvrnorm 我无法指定范围（最大值和最小值）。这个问题有什么解决办法吗？
我阅读了所有相关主题和文档，但找不到可以设置生成数据的最大值和最小值的解决方案
xy &lt;- MASS::mvrnorm(n = 500, mu = c(mean_a,mean_b), Sigma = Sigma)
colnames(xy) &lt;- c(“a”, “b”)
xydf &lt;- data.frame(xy)
头(xydf)
]]></description>
      <guid>https://stats.stackexchange.com/questions/644848/how-to-simulate-correlated-data-with-specified-min-and-max-and-cov-and-mean-in-r</guid>
      <pubDate>Thu, 11 Apr 2024 19:55:26 GMT</pubDate>
    </item>
    <item>
      <title>为什么选择术语“显着性”($\alpha$) 来表示 I 类错误的概率？</title>
      <link>https://stats.stackexchange.com/questions/644808/why-was-the-term-signifiance-alpha-chosen-for-the-probability-of-type-i-e</link>
      <description><![CDATA[我目前正在学习“统计 1”作为我计算机科学学位的一部分，我很难理解“重要性”的概念。
我们获得了以下定义：
$H_0$ - 零假设
$H_1$ - 替代假设。
$R$ - $H_0$ 拒绝区
$\bar{R}$ - $H_0$ 无拒绝区
$\begin{对齐} \alpha &amp;= P(\text{I 型错误})\\&amp;=P_{H_0}(\text{拒绝 } H_0 ) \\&amp;= P_{H_0}(X\in R) \\&amp;; \end{对齐} $
虽然我相信我已经很好地掌握了这些定义以及它们与 p 值的关系，但术语“显着性”并不适用。我仍然感到困惑。
我理解某事物“具有统计显着性”的概念，但显着性水平越高，出错的风险就越大，这似乎违反直觉。直觉上，当某件事非常重要时，我预计风险会较低。
有人可以解释一下为什么“重要性”一词如此重要吗？被选中了？
对于那些正在寻找的人来说，这些是关于这些术语的统计含义的一些非常好的讨论。我正在寻找更直观的解释来解释为什么选择这个术语。
比较和对比，p -值、显着性水平和 I 类错误
显着性水平 alpha 与 1 类误差 alpha 之间的关系是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/644808/why-was-the-term-signifiance-alpha-chosen-for-the-probability-of-type-i-e</guid>
      <pubDate>Thu, 11 Apr 2024 15:02:37 GMT</pubDate>
    </item>
    <item>
      <title>如何计算下面的Dirichlet分布和Beta分布的期望？ [关闭]</title>
      <link>https://stats.stackexchange.com/questions/644769/how-to-calculate-the-expectation-of-the-following-dirichlet-distribution-and-bet</link>
      <description><![CDATA[这是我的研究中的一个问题，涉及基于LDA的模型的平均场假设的变分EM算法的推导。
我们都知道，鉴于 $\boldsymbol{\theta} \sim \mathrm{Dir}(\boldsymbol{\alpha})$，则 $E_{p(\boldsymbol{\theta} \mid \boldsymbol{\alpha})}[\log{\theta_k}] = \Psi(\alpha_k) - \Psi(\sum_ {k&#39;=1}^K \alpha_{k&#39;})$，其中 $\Psi$ 是 Digamma 函数。
然后，如何计算以下期望： $$E_{q(\psi, \boldsymbol{\varphi} \mid \boldsymbol{\lambda}, \boldsymbol{\ mu})}[\log{((1-\psi)\cdot \varphi_{v} +\psi)}]$$
其中 $\psi$ 和 $\boldsymbol{\varphi}$ 是独立的， $\psi \sim \mathrm{Beta}(\lambda_1, \lambda_2), \boldsymbol{\varphi} = (\varphi_1, \varphi_2,\cdots, \varphi_V) \sim \mathrm{Dir} (\boldsymbol{\mu})$]]></description>
      <guid>https://stats.stackexchange.com/questions/644769/how-to-calculate-the-expectation-of-the-following-dirichlet-distribution-and-bet</guid>
      <pubDate>Thu, 11 Apr 2024 02:03:03 GMT</pubDate>
    </item>
    <item>
      <title>预测具有不确定性的序列</title>
      <link>https://stats.stackexchange.com/questions/644741/forecasting-a-series-that-comes-with-uncertainty</link>
      <description><![CDATA[我有一个由空间域上的时空聚合产生的时间序列。
因此，我有一个中心测量值（假设为平均数）和一个离散度（假设为标准差）。
如果我想做传统的时间序列预测，例如 ARIMA，我应该如何对中心测量和离散度进行编码，以便最终模型包含此类不确定性？
我是否应该将中心测量值视为主序列，将离散度视为外生协变量？我应该创建一系列下限（假设平均值 - 2 个标准差）吗？然后将其预测为下限预测？
我想了解更多有关选项的信息。]]></description>
      <guid>https://stats.stackexchange.com/questions/644741/forecasting-a-series-that-comes-with-uncertainty</guid>
      <pubDate>Wed, 10 Apr 2024 17:45:53 GMT</pubDate>
    </item>
    <item>
      <title>对连续解释变量每单位变化结果的纵向变化进行建模</title>
      <link>https://stats.stackexchange.com/questions/644717/modeling-longitudinal-change-in-outcome-per-unit-of-change-in-a-continuous-expla</link>
      <description><![CDATA[我的问题与相关您如何处理“嵌套”问题？回归模型中的变量？但是，我的问题略有不同。
我正在分析来自减肥干预的 RNA-seq 基因表达数据（&gt; 10 000 个基因）。对参与者进行测量，并根据干预前和两个随访点（可变访视）采集的样本进行 RNA 测序。从随访 1 到随访 2，参与者的体重减轻了不同程度，有些甚至反弹了。我们还有来自其他队列的数据，其中减肥干预和减肥结果不同。这些队列不能直接比较，因为 RNA-seq 实验是分开的。然而，为了产生稍微具有可比性的结果，我想对基因表达的干预效果进行建模，以便结果能够显示从基线到每次随访的体重减轻百分比的变化 (wlp) .
在本例中，visit 是解释变量，wlp 嵌套在其中。如果我只有两个时间点，我将仅使用 wlp 作为解释变量来获得我想要的结果（给出与 visit:wlp 交互相同的结果）。 
我的问题是：在这种情况下，当有三个时间点时，没有主效应的 visit:wlp 交互是否会给我每个 wlp 结果（基因表达）的变化code&gt; 处两次不同的访问？如果我分别分析两个后续点，估计值非常相似；然而，p 值较小。]]></description>
      <guid>https://stats.stackexchange.com/questions/644717/modeling-longitudinal-change-in-outcome-per-unit-of-change-in-a-continuous-expla</guid>
      <pubDate>Wed, 10 Apr 2024 11:01:24 GMT</pubDate>
    </item>
    <item>
      <title>分割稀疏数据集时生成没有数据的集合的概率</title>
      <link>https://stats.stackexchange.com/questions/644617/probability-of-making-a-set-with-no-data-when-splitting-a-sparse-dataset</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/644617/probability-of-making-a-set-with-no-data-when-splitting-a-sparse-dataset</guid>
      <pubDate>Tue, 09 Apr 2024 07:57:09 GMT</pubDate>
    </item>
    </channel>
</rss>