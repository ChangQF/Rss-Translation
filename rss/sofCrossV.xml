<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>最近 30 个来自 stats.stackexchange.com</description>
    <lastBuildDate>Fri, 19 Apr 2024 06:18:39 GMT</lastBuildDate>
    <item>
      <title>我们如何应用敏感性分析来比较两个衡量干预效果的 ARIMA 模型？</title>
      <link>https://stats.stackexchange.com/questions/645360/how-can-we-apply-sensitivity-analysis-in-comparing-two-arima-models-both-measur</link>
      <description><![CDATA[我有一个从基线（4 个月）和干预期间（4 个月）收集的 8 个月期间报告率数据集。目标是确定干预措施是否具有显着效果。我使用 ARIMA 模型来比较观测值和预测值。所有差异均不显着。然而，干预期间第三个月的比率似乎是一个严重的异常值，因此需要使用 SPSS ARIMA 建模中的敏感性分析来比较包含该比率的模型和排除该比率的另一个模型。我以前从未经历过这种情况，需要您的帮助。谢谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/645360/how-can-we-apply-sensitivity-analysis-in-comparing-two-arima-models-both-measur</guid>
      <pubDate>Fri, 19 Apr 2024 06:13:05 GMT</pubDate>
    </item>
    <item>
      <title>在 Heckman 2 步模型中指定回归量的条件是什么</title>
      <link>https://stats.stackexchange.com/questions/645359/what-are-the-conditions-to-specify-the-regressors-in-heckman-2-step-model</link>
      <description><![CDATA[我在解释 STATA 命令 Twostep Heckman 模型以及向模型添加固定效应时遇到问题。
我的分析基于面板数据集，我想解决在上学之前死亡的个体的选择偏差，并且需要理解以下结果的解释，我的因变量是教育（连续变量），自变量是家庭规模。我在这个例子中只包含了 2 个变量。我想添加固定效果并使用 R 复制相同的效果。
我想知道如何在 R studio 概率回归中运行它，然后提取预测值，计算米尔斯比率并使用固定效应（集群和月份固定效应）运行模型。
希望图片清晰
谢谢
]]></description>
      <guid>https://stats.stackexchange.com/questions/645359/what-are-the-conditions-to-specify-the-regressors-in-heckman-2-step-model</guid>
      <pubDate>Fri, 19 Apr 2024 05:41:27 GMT</pubDate>
    </item>
    <item>
      <title>我们能否通过贝叶斯推理的适当评分规则获得可评估的概率预测，而不评估边际可能性？</title>
      <link>https://stats.stackexchange.com/questions/645358/can-we-get-probabilistic-predictions-evaluable-by-proper-scoring-rules-from-baye</link>
      <description><![CDATA[假设我们有一个输入向量 $X=[x_0,\dots, x_{n-1}]$ 和一个输出向量， $Y=[y_0, \dots, y_{n-1}]$。
我们希望在给定新输入 $\hat{y}$ 的情况下预测新输出的分布  &quot;&gt;$\帽子{x}$。
贝叶斯推理为我们提供了一种通过后验预测分布来做到这一点的方法，在本例中为
$$p(\hat{y}|\hat{x}, X, Y)=\int p(\hat{y}|\hat{x}, \theta)p(\theta|X,Y)\text{d}\theta$$
后验进一步定义为
$$p(\theta|X,Y)=\frac{p(Y|X,\theta)p(\theta)}{\int p(Y|X,\ θ)p(\θ)\text{d}\theta}.$$
当没有封闭形式时，$p(\theta|X,Y)$ 的分母有时很难或耗时地进行数值计算或共轭先验，有没有一种方法能够在不评估分母的情况下获得某种密度预测，然后可以通过适当的评分规则（例如负向对数分数）进行评估？
对于密度预测 $f_{\hat{y}}(y)$ 和观测值 $y$：
$$L(y, f_{\hat{y}}) = -\text{log}(f_{\hat{y}}(y)).$ $
我觉得我们会遇到密度预测不是真实密度的问题（即不积分到 1），这会给我们在使用它评估评分规则时提供不正确的评估，即使我们仍然可以在技术上计算它.]]></description>
      <guid>https://stats.stackexchange.com/questions/645358/can-we-get-probabilistic-predictions-evaluable-by-proper-scoring-rules-from-baye</guid>
      <pubDate>Fri, 19 Apr 2024 03:59:34 GMT</pubDate>
    </item>
    <item>
      <title>关于样本矩的术语澄清</title>
      <link>https://stats.stackexchange.com/questions/645352/terminology-clarification-about-sample-moments</link>
      <description><![CDATA[根据 MathWorld (链接)：“样本原始矩是无偏估计量人口原始时刻”。
维基百科（链接）说：
...可以使用 $k$&lt; 来估计总体的第 $k$ 个原始矩/span&gt;-第一个原始样本时刻
$$\frac{1}{n}\sum_{i = 1}^{n} X^k_i$$
应用于从总体中抽取的$X_1\dots X_n$样本。
这让我感到困惑，因为据我了解，估计量是统计数据（link）用于估计一些参数。因此，它们是随机变量的函数。但文章中发现的表达式似乎是随机变量结果的函数，而不是变量本身的函数。
我倾向于认为“样本时刻”是指一个“样本时刻”。是可以直接从样本计算的“东西”（或者据我从 这个答案）。这意味着，从特定的值来看。人们可以定义一个统计量，然后通过用特定结果替换随机变量来修改它，并执行计算。但我感觉这和原来的物体不一样。
任何人都可以向我澄清这些细微的差异，以了解什么是“样本时刻”吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/645352/terminology-clarification-about-sample-moments</guid>
      <pubDate>Fri, 19 Apr 2024 01:22:58 GMT</pubDate>
    </item>
    <item>
      <title>似然比检验的拒绝区域（均匀分布）</title>
      <link>https://stats.stackexchange.com/questions/645350/rejection-region-for-the-likelihood-ratio-test-uniform-distribution</link>
      <description><![CDATA[让 $x_1 ... x_n$ 从均匀分布中采样 $f(x;\theta) = (1/\theta), \theta; &gt;0, x \in [0,\theta].$
找到假设的似然函数后：
$H_0 : \theta = \theta_0 ~\textrm{vs} ~H_A : \theta \neq \theta_0$
到目前为止我发现比率测试是
$\Lambda(\theta)= (\bar{X}/\theta_0)^n$ 其中 $\bar{ X}$是x采样的最大值。
如果我不尝试定义拒绝区域，我会稍微迷失，我是否定义与此比率测试相关的 c 或将其定义为 X 估计值的函数？我仍然对统计数据有疑问，在此先感谢。]]></description>
      <guid>https://stats.stackexchange.com/questions/645350/rejection-region-for-the-likelihood-ratio-test-uniform-distribution</guid>
      <pubDate>Fri, 19 Apr 2024 00:12:00 GMT</pubDate>
    </item>
    <item>
      <title>如何解释特征样本和基于先验分布的技术来估计后验似然？</title>
      <link>https://stats.stackexchange.com/questions/645348/how-to-interpret-a-feature-sample-and-prior-distribution-based-technique-to-esti</link>
      <description><![CDATA[我试图理解实验方法背后的数学原理，该方法基于使用先验分布权衡实时观察。例如，考虑包含某个关键字（例如 Alexa）的音频波形数据集，目标是在知道关键字结束位置的情况下找到关键字的起始位置。
如果关键字结束帧是 $n$ 则起始帧 $s \in [n-k-w, n-k]$ 。利用这些先验知识，我使用误差平方和方法在直方图样本之上拟合多个连续分布 $f_S(s)$ 来找到最佳拟合。
现在，对于具有关键字和结束帧的给定新波形，我计算 $[n-k-w, n-k]$ 范围内的帧的梅尔能量。然后，我使用 $f_S(s)$ 权衡此能量，并选择得分最高的帧作为 $s$。这种方法给出了良好的结果，但我很难将其形式化。
该方法似乎与 MAP 类似，但我没有在以下等式中明确计算 $P(O|S=s)$：
$P(S|O) \propto P(O|S) \times P(S)$
其中 $(O=o)$ 是观测到的倒谱系数能量。
这种方法有什么理论依据吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/645348/how-to-interpret-a-feature-sample-and-prior-distribution-based-technique-to-esti</guid>
      <pubDate>Thu, 18 Apr 2024 23:38:47 GMT</pubDate>
    </item>
    <item>
      <title>如何检查匹配病例对照研究中的组间相似性？</title>
      <link>https://stats.stackexchange.com/questions/645347/how-to-check-for-group-similarity-in-matched-case-control-studies</link>
      <description><![CDATA[我正在开展一项病例对照 (1:2) 临床研究，其中根据大量变量（性别、年龄、一些考虑风险、活动能力、认知能力的数字变量......）对患者进行匹配。 ）。我想评估病例和对照之间的相似性。
尽管我发现文献和讨论中缺乏一些一致性，但我认为样本是独立的，因为它们没有自然配对；我认为匹配过程是为了保证组之间的相似性。因此，我对独立样本进行统计检验。
为了评估两组是否相似，我独立测试所有上述变量（例如性别的卡方、年龄的曼-惠特尼 U 等）。
这种方法正确吗？我可以从不同的、也许更正式的方法中受益吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/645347/how-to-check-for-group-similarity-in-matched-case-control-studies</guid>
      <pubDate>Thu, 18 Apr 2024 22:40:02 GMT</pubDate>
    </item>
    <item>
      <title>识别分层簇的最佳方法</title>
      <link>https://stats.stackexchange.com/questions/645346/best-method-to-identify-layered-clusters</link>
      <description><![CDATA[问题
大家好。我正在使用一个数据集，该数据集包含 15300 个样本，每个样本有 49 个特征，均匀分布在三个类别中。我使用 TSNE 将特征空间的维度减少到二维，以更好地可视化数据的分布，并发现一些有点奇怪的东西（至少对我来说）。这些类似乎是相互叠加的。
嵌入数据分布

欧几里德距离

余弦相似度

我知道这些图表很恶心，对此感到抱歉。无论如何，是否有一种聚类算法可以正确识别这些类别？我听说谱聚类方法非常擅长识别非凸簇，但它们的空间和计算复杂度非常高，这不太好，因为我仅限于 16GB RAM。是否有一种迭代方法可以逼近传统谱聚类方法（图拉普拉斯特征分解）提供的解？]]></description>
      <guid>https://stats.stackexchange.com/questions/645346/best-method-to-identify-layered-clusters</guid>
      <pubDate>Thu, 18 Apr 2024 22:23:44 GMT</pubDate>
    </item>
    <item>
      <title>确定嘈杂的医疗保险索赔数据中的政策变化</title>
      <link>https://stats.stackexchange.com/questions/645345/determine-policy-changes-in-noisy-medicare-insurance-claim-data</link>
      <description><![CDATA[我希望这是问这个问题的正确地方，我不会因为一个糟糕的问题而受到批评，但这里是。
背景：
我有一个包含五年内提交的所有医疗保险索赔（非个性化）的数据库。
该数据库由 8.68B 条记录组成。每条记录都包括提交索赔的日期、寻求的金额、索赔是否获得批准、金额以及时间。每条记录还通过适用于程序/设备的约 22,000 个代码（CPT 代码）之一对索赔进行分类。数据按这些 CPT 代码排序，并在 CPT 代码内按日期排序。
是否涵盖索赔取决于影响我无权访问的相关 CPT 代码的各种政策。此外，对于我的问题来说，重要的是，适用于 CPT 代码的政策可能会随着时间的推移而发生变化。一些没有被覆盖的东西可能会变得如此。曾经被覆盖的东西可能不再被覆盖。甚至可能会来来回回，例如，覆盖-不覆盖-覆盖。或者，该策略在数据库期间可能永远不会改变。
最后，数据是有噪音的。虽然某些内容通常会被承保，但索赔仍可能因各种原因被拒绝，例如，提交晚、未提供所需文件等。同样，某些内容通常会被拒绝，但它仍然可能被承保，例如，同情心护理。&lt; /p&gt;
问题：
有没有一种方法可以从数据本身检测这些保单变化，这样我就可以说，对于给定的 CPT 代码，一个时期内批准的索赔百分比是 X，而在不同时期是 Y等等？
另一种说法可能是：面对这些嘈杂的数据，我如何找出适当的窗口和截止百分比来计算平均批准率，以便我可以比较两个时间序列？ 
需要明确的是，我有软件背景，并准备好进行暴力破解，即，如有必要，尝试所有可能的窗口。
感谢您的帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/645345/determine-policy-changes-in-noisy-medicare-insurance-claim-data</guid>
      <pubDate>Thu, 18 Apr 2024 22:19:23 GMT</pubDate>
    </item>
    <item>
      <title>余弦相似度的缩放</title>
      <link>https://stats.stackexchange.com/questions/645343/scaling-of-cosine-similarty</link>
      <description><![CDATA[假设我有一个特定的度量存储在两个向量a和b中，我使用余弦相似度来度量两个向量之间的相似度。
形式上，余弦相似度表示为：
$$ \cos\theta =\frac{a\cdot b}{\|a\|\|b\|}= \frac{\sum_{i=1}^{ n}{a_i b_i}}{\sqrt{\sum_{i=1}^{n}a_i^2} \cdot\sqrt{\sum_{i=1}^{n}b_i^2}}$$&lt; /跨度&gt;
在阅读时，我发现了一篇帖子 关于两个向量之间的相似性。一个答案如下：
&lt;块引用&gt;
我在比较 2 个向量（速度）时遇到了类似的问题。我目前使用两个向量的点积除以最大向量长度的平方。这样做的优点是，在测量“相似性”时还考虑了方向和大小。该公式给出 -1 到 +1 之间的数字。如果向量相同，您将获得 +1。如果方向相差超过 180 度，结果为负。

指定如下：相似度 = dotproduct(a, b) / max(norm(a),norm(b))^2
因此，在上面的符号中，可以表示为：
$$ = \frac{\sum_{i=1}^{n}{a_i b_i}}{\max\left(\sqrt{\sum_{i=1) }^{n}a_i^2},\sqrt{\sum_{i=1}^{n}b_i^2}\right)^2}$$
考虑到正则余弦相似度也在区间内 $[-1,1]$？这是余弦相似度的标准缩放吗？如果是，这种方法通常被称为什么？最后，使用一些示例数据，似乎替代缩放会导致相似度稍低（请参阅下面的代码示例），这告诉我什么？
&lt;小时/&gt;
&lt;前&gt;&lt;代码&gt;n &lt;- 10

设置.种子(123)
a &lt;- rnorm(n)
b &lt;- rnorm(n)

余弦 &lt;- 函数(a, b) {
  总和（a * b）/（sqrt（总和（a * a））* sqrt（总和（b * b）））
}

cosine_norm &lt;- 函数(a, b) {
  sum(a*b) / max(sqrt(sum(a*a)), sqrt(sum(b*b)))^2
}

余弦（a，b）
[1]0.5801971

余弦范数（a，b）
[1] 0.5232835
```
]]></description>
      <guid>https://stats.stackexchange.com/questions/645343/scaling-of-cosine-similarty</guid>
      <pubDate>Thu, 18 Apr 2024 22:05:19 GMT</pubDate>
    </item>
    <item>
      <title>变换后的正态分布[关闭]</title>
      <link>https://stats.stackexchange.com/questions/645325/distribution-of-transformed-normal</link>
      <description><![CDATA[假设 $X$ 是正态分布，均值 $\mu$ 和标准差 $\sigma$ 和 $\Phi$ 是标准正态分布函数。 $\Phi(X)$ 的分布是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/645325/distribution-of-transformed-normal</guid>
      <pubDate>Thu, 18 Apr 2024 16:26:58 GMT</pubDate>
    </item>
    <item>
      <title>基于概率分布而非数据集的 k 均值聚类</title>
      <link>https://stats.stackexchange.com/questions/645286/k-means-clustering-on-a-probability-distribution-instead-of-a-dataset</link>
      <description><![CDATA[通常，聚类算法（例如 $k$-means）在数据集上定义如下：if $ D$ 是一个数据集，将 $D$ 划分为集合 $\{S_1, \dots , S_n\}$ 最小化 $$\sum_{i=1}^n |S_i| 给出的簇内平方和\text{Var} (S_i) = \sum_{i=1}^n \frac{1}{|S_i|} \sum_{x, y\in S_i}\lVert x-y\rVert.$$我想知道是否可以参考概率分布定义 $k$-means 算法 $\mu$  而不是数据集 $D$。例如，假设我们的数据集仅由根据以下概率分布分布的 2d 点的集合组成（其中深色表示低概率）：

然后，如果我们使用 $k$ - 表示有 2 个簇的聚类，我们（很可能）会得到一个以 -1 为中心的簇，另一个以 1 为中心的簇。我是想知道我们是否可以在分布本身上定义聚类算法，并将输入空间分割成 Voronoi cells 不参考数据集。换句话说，我想找到两件事： 1. 一个目标函数 $f$ 评估输入空间相对于给定分布的给定 Voronoi 分区2. 针对给定分布最小化目标函数的方法。
我希望它与 $k$ 保持一致 - 在以下意义上意味着目标函数。假设我们有 IID 随机变量 $Z_1, \dots, Z_m$ 根据 $\mu$ 分布，这样$\{Z_1(\omega), \dots, Z_m(\omega)\}$ 是根据 $\mu$ 对于任何随机结果$\omega$。然后令 $\mathcal{V}(\omega)$ 为由 $k$-表示此数据集上的算法，并让 $SS(\omega)$ 成为此数据集上此分区的簇内平方和。然后我想要属性 $$\mathbb{E}[f(\mathcal{V})] = \mathbb{E}[SS].$$
这样做的目的是提供一种方法来量化给定理论输入分布的$k$均值的适用性。例如，如果将聚类作为分类任务，则可以计算出簇内距离的预期总和以及预期错误率等数量。]]></description>
      <guid>https://stats.stackexchange.com/questions/645286/k-means-clustering-on-a-probability-distribution-instead-of-a-dataset</guid>
      <pubDate>Thu, 18 Apr 2024 08:06:04 GMT</pubDate>
    </item>
    <item>
      <title>为什么即使对于 IID 随机变量我们也能得到更好的渐近全局估计量？</title>
      <link>https://stats.stackexchange.com/questions/645203/why-can-we-get-better-asymptotic-global-estimators-even-for-iid-random-variables</link>
      <description><![CDATA[设 $X_1,...,X_N$ 为从参数化分布中采样的 IID 随机变量 $p_\theta $，并假设我的目标是从这些样本中检索 $\theta$。
我们知道 MLE 在渐近状态下提供了一种有效的无偏估计，这意味着它将是渐近无偏的，并且方差（渐近）饱和 Cramer-Rao 界。
另一方面，我们也知道，即使在单次状态下，即在一次观察之后，我们也可以找到一个有效的局部无偏估计量，该估计量的主要警告通常只是local（这意味着它取决于有关 $\theta$ 的一些先验知识）。
相比之下，MLE 是无偏且有效的（尽管只是渐近），不需要参数的先验知识来进行估计。
为了用一个明确的玩具示例来具体化讨论，假设我们要从伯努利过程中估计 $p^2$，$X_i\sim\operatorname{伯尔尼}(p)$。
标准计算将显示此问题的 MLE 为
$$\hat p_N^2 = \left(\frac{\sum_{i=1}^N X_i}{N}\right)^2,
\\ \mathbb{E}[\hat p_N] = p^2 + \frac{p(1-p)}{N},
\\ \operatorname{Var}[\hat p_N] = \frac{4p^3(1-p)}{N} + O(1/N^2).$$
另一方面，局部围绕参数值 $p^2$ 饱和 Cramer-Rao 边界的单次估计量为
$$\hat f_{p^2}(X) = -p^2 + 2p X.$$
此处显示了 MLE 的计算例如，而局部有效估计器的计算此处。
因此，这两个估计量具有渐近相同的方差（取较大 $N$ 的局部估计量的平均值），但 MLE 的一大优势是非局部，即不是要估计的参数的函数。
我的问题是：是否有任何直觉可以理解为什么会发生这种情况？由于我们正在讨论 IID 变量，因此一系列观察中的信息不应比每个单独观察中的信息多。如果 $X_i,X_j$ 相关，我会理解需要使用同时对完整统计数据进行操作的估计器，但为什么会出现这样的情况当单个样本是 IID 时有用吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/645203/why-can-we-get-better-asymptotic-global-estimators-even-for-iid-random-variables</guid>
      <pubDate>Wed, 17 Apr 2024 08:42:08 GMT</pubDate>
    </item>
    <item>
      <title>我应该什么时候进行标准化，在回归之前还是之后？</title>
      <link>https://stats.stackexchange.com/questions/645188/when-should-i-standardize-before-or-after-a-regression</link>
      <description><![CDATA[我有一个面板数据集，我的因变量是长期合同农场工人的 Logit 转换比例。我对两个变量的影响特别感兴趣，即农业中的田园重点（以变量“田园”为代表）和马的存在（以“马”为代表）。这些变量的影响随着时间的推移而变化，我运行一个带有交互项的模型：
模型 &lt;- lm(logitshare ~ 田园 + 田园:因子(年份) + 马 + 马:因子(年份) +
            其他变量，数据= mydata）

我需要比较变量的相对重要性并使用两种方法，在运行回归之前或之后进行标准化。结果明显不同。
。
“模型”列显示了对非标准化系数运行回归并使用 lm.beta 进行事后标准化的情况。 ModelST 在使用 R 的 scale 函数转换的系数上运行。哪种方法更可靠？]]></description>
      <guid>https://stats.stackexchange.com/questions/645188/when-should-i-standardize-before-or-after-a-regression</guid>
      <pubDate>Tue, 16 Apr 2024 23:50:11 GMT</pubDate>
    </item>
    <item>
      <title>双面 t 检验：如果您的估计已经告诉您要查看哪条尾部，为什么我们需要进行双面检验？</title>
      <link>https://stats.stackexchange.com/questions/645128/two-sided-t-tests-why-do-we-need-to-test-two-sided-if-your-estimate-is-telling</link>
      <description><![CDATA[想象一下以下场景：
H0：镇上的孩子平均睡眠 7 小时 (mu= 7)
HA：城镇里的孩子平均睡眠时间不足 7 小时 (mu=!7)
然后你收集城镇孩子的样本，得到平均值的估计值：7.8 小时，标准差为 1.0 小时。
现在，我真的不明白为什么我们要执行双边 t 检验。
双边检验似乎给出了以下问题的答案：“获得 x 距离且远离假定 (H0) 均值的结果的机会有多大？”。因此，在我们的例子中：7.8 小时及以上的机会 + 6.2 小时及以下的机会。
我不明白为什么我们对“6.2及以下”感兴趣部分。假设 H0 为真，为什么我们不想知道我们得到的结果以及更极端的结果的机会有多大？
在互联网上搜索答案后，我读到“你不能从双边 t 检验切换到单边 t 检验，因为那是作弊”。我还是不太明白为什么。到目前为止我所理解的是“如果你根据数据选择测试，你可能最终会“过度拟合”对您的数据进行测试”。但我不确定这种与统计模型的类比是否正确。
非常感谢您的阅读！]]></description>
      <guid>https://stats.stackexchange.com/questions/645128/two-sided-t-tests-why-do-we-need-to-test-two-sided-if-your-estimate-is-telling</guid>
      <pubDate>Tue, 16 Apr 2024 11:39:49 GMT</pubDate>
    </item>
    </channel>
</rss>