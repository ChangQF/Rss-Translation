<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Mon, 10 Jun 2024 18:19:36 GMT</lastBuildDate>
    <item>
      <title>非中心 t 分布的非中心参数</title>
      <link>https://stats.stackexchange.com/questions/648983/noncentrality-parameter-of-noncentral-t-distribution</link>
      <description><![CDATA[设 $X_{1}, \dots, X_{n}$ 为 iid $\mathbb{N}(\mu, \sigma^{2})$。接下来，设
$$
T_{n} = \frac{\bar{X}}{s/\sqrt{n}},
$$
其中 $\bar{X}$ 和 $s$ 分别为样本均值和样本方差。据我所知，$T_{n}$ 具有非中心 t 分布，自由度为 $n-1$。我很困惑，非中心参数是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/648983/noncentrality-parameter-of-noncentral-t-distribution</guid>
      <pubDate>Mon, 10 Jun 2024 18:08:12 GMT</pubDate>
    </item>
    <item>
      <title>在评估性能之前使用异常检测器标记数据</title>
      <link>https://stats.stackexchange.com/questions/648982/labeling-data-with-an-anomaly-detector-prior-to-evaluating-performance</link>
      <description><![CDATA[利用模型标记数据，然后执行训练/测试拆分以评估该模型的性能，这样做是否错误？
假设我有一个未标记的数据集，其中缺失的标签是一个表示类别的二进制变量，并且类别非常不平衡（异常检测场景的典型情况）。
我一直在考虑使用异常检测器来帮助标记过程。该过程看起来像这样。

假设数据仅包含一个类
在整个数据集上训练模型
仔细查看检测器的输出并确定排名更独特/孤立的数据是否实际上包含稀有类。
在训练模型之前，可能会重复此过程多次，并删除新发现的稀有类实例。

这对于标记来说很有意义，但在评估性能时，这似乎是错误的。如果我使用检测器来帮助我标记数据，那么我会因此而预期性能估计会过于乐观。当然，我（人类）实际上会审查和添加稀有类的标签。
一旦数据有了标签，我就会将数据分成训练集和测试集，其中训练集没有稀有类的实例，而测试集包含稀有类的所有实例以及常见类的许多实例。在这样的流程中，在测试集中找到“真阳性”似乎是一个自我实现的预言。但是，这种训练/测试分割的模型将不同于用于帮助标记的模型，因为模型参数将用较少的数据来估计。
在本次对话的背景下，异常检测器是一个通用术语，可能是许多不同算法中的一种。有关示例，请参阅https://builtin.com/machine-learning/anomaly-detection-algorithms。]]></description>
      <guid>https://stats.stackexchange.com/questions/648982/labeling-data-with-an-anomaly-detector-prior-to-evaluating-performance</guid>
      <pubDate>Mon, 10 Jun 2024 17:58:49 GMT</pubDate>
    </item>
    <item>
      <title>平行于 x 轴的直线的自相关曲线</title>
      <link>https://stats.stackexchange.com/questions/648980/autocorrelation-curve-of-a-line-parallel-to-x-axis</link>
      <description><![CDATA[以下是两个图：

直线，与 x 轴平行
滞后 = 0 至 100 的直线自相关

现在，我的问题是：为什么自相关曲线逐渐减小并接近于零？这种减少代表什么？

]]></description>
      <guid>https://stats.stackexchange.com/questions/648980/autocorrelation-curve-of-a-line-parallel-to-x-axis</guid>
      <pubDate>Mon, 10 Jun 2024 16:59:54 GMT</pubDate>
    </item>
    <item>
      <title>我应该使用什么假设检验方法来比较分级延迟计数</title>
      <link>https://stats.stackexchange.com/questions/648979/what-hypothesis-testing-method-should-i-use-for-comparing-binned-latency-counts</link>
      <description><![CDATA[我正在尝试选择一种适当的假设检验方法来测试一组服务器的延迟是否高于另一组。但是，我所拥有的只是对延迟箱数量的计数，而我无法访问原始延迟值。数据如下所示




[0, 0.01)
[0.01, 0.05)
[0.05, 1)
...




第 1 组
$n_{11}$
$n_{12}$
$n_{13}$
...


组2
$n_{21}$
$n_{22}$
$n_{23}$
...



其中列是延迟箱，$n_{ij}$ 显示 (组，延迟箱) 对记录的请求数。
最初，我考虑使用 Pearson 的 $\chi^2$ 检验，但这没有考虑到箱的排序。另外，不清楚如何使用 $\chi^2$ 测试进行定向测试。有没有更好的推荐？]]></description>
      <guid>https://stats.stackexchange.com/questions/648979/what-hypothesis-testing-method-should-i-use-for-comparing-binned-latency-counts</guid>
      <pubDate>Mon, 10 Jun 2024 16:57:46 GMT</pubDate>
    </item>
    <item>
      <title>从 MFVI 获取准确的不确定性？</title>
      <link>https://stats.stackexchange.com/questions/648978/getting-accurate-uncertainty-from-mfvi</link>
      <description><![CDATA[我想知道是否有任何研究方法可以提高均值场变分推理（不丢弃均值场近似）的准确性。显然，它低估了不确定性，我理解这是因为它是贝叶斯推理的单峰近似。
但我特别想知道，在贝叶斯深度学习的背景下，是否有后处理校准方法（例如，拟合线性模型来转换预测）可以使贝叶斯高密度区间（或贝叶斯可信区间）合理准确？]]></description>
      <guid>https://stats.stackexchange.com/questions/648978/getting-accurate-uncertainty-from-mfvi</guid>
      <pubDate>Mon, 10 Jun 2024 16:26:30 GMT</pubDate>
    </item>
    <item>
      <title>为什么增加模型复杂性会减少整个数据分布的偏差？</title>
      <link>https://stats.stackexchange.com/questions/648977/why-does-increasing-model-complexity-reduce-bias-over-the-entire-data-distributi</link>
      <description><![CDATA[在机器学习中，我们经常谈论偏差-方差权衡，以及如何增加模型复杂度既可以减少偏差又可以增加方差。我理解为什么增加模型复杂度一开始会减少偏差，但一旦进入过度拟合领域，我就不太清楚了。
以下是偏差的公式：
$\operatorname{Bias}_D\big[\hat{f}(x;D)\big] = \operatorname{E}_D\big[\hat{f}(x;D)- f(x)\big]$。随着模型复杂度超过某个点（忽略双重下降），模型开始严重过度拟合其训练数据，并开始对其余大部分数据分布做出疯狂且越来越错误的预测。为什么这会减少整个数据分布的偏差？
在这篇文章中：偏差最终会随着模型复杂度的增加而增加吗？，公认的答案声称，从数据分布中抽样的训练集的模型预测平均值将接近真实值。但我不清楚为什么这是真的，以及为什么它不能平均为其他值。]]></description>
      <guid>https://stats.stackexchange.com/questions/648977/why-does-increasing-model-complexity-reduce-bias-over-the-entire-data-distributi</guid>
      <pubDate>Mon, 10 Jun 2024 16:01:10 GMT</pubDate>
    </item>
    <item>
      <title>具有比观测值更多的预测变量（p > n）和相同相关性的套索估计量的行为？</title>
      <link>https://stats.stackexchange.com/questions/648976/behavior-of-lasso-estimator-with-more-predictors-than-observations-p-n-and-i</link>
      <description><![CDATA[如果将 Lasso 估计器用于预测因子 (p) 多于观测值 (n) 的数据集，其中所有预测因子均不相关但与
𝑦
y 高度相关，并且与
𝑦
y 具有完全相同的相关性，那么 Lasso 估计器的行为会如何？Lasso 估计器会将哪些预测因子缩小为零，哪些预测因子会保留？
一致的估计器不会将任何
𝑝
p 变量减少为零。但是，据我了解，Lasso 估计器最多会选择
𝑛
n 个预测因子。我的问题是：在这些条件下，Lasso 会选择哪些预测因子以及为什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/648976/behavior-of-lasso-estimator-with-more-predictors-than-observations-p-n-and-i</guid>
      <pubDate>Mon, 10 Jun 2024 15:04:58 GMT</pubDate>
    </item>
    <item>
      <title>如何在两个不同的 y 轴上显示箱线图？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/648975/how-to-show-boxplots-on-two-different-y-axes</link>
      <description><![CDATA[我有一个带有几个箱线图的 ggplot 图，我希望每组中的最后一个箱线图（在我的示例中是所有箱线图 c）显示在右侧的不同 y 轴上（最好用箱线图的颜色标记）。箱线图 c 显示不同的测量值，其数量级可能存在巨大差异，并且以不同的单位进行测量，因此让它们与 a 和 b 共享轴是没有意义的。 （在我的示例中，我使用函数 o 和 f 更改了箱线图的定义。）
我的工作示例：
library(ggplot2)

a1 &lt;- rnorm(n=100, mean=5, sd=20)
a2 &lt;- rnorm(n=100, mean=6, sd=20)
a3 &lt;- rnorm(n=100, mean=8, sd=20)

b &lt;- rnorm(n=100, mean=11, sd=10)

c1 &lt;- rnorm(n=100, mean=500, sd=80)
c2 &lt;- rnorm(n=100, mean=600, sd=80)
c3 &lt;- rnorm(n=100,平均值=800，标准差=80)

字母 &lt;- c(rep(&quot;a&quot;, 300), rep(&quot;b&quot;, 300), rep(&quot;c&quot;, 300))
类型 &lt;- rep(c(rep(1,100), rep(2,100), rep(3,100)),3)

数据 &lt;- data.frame(y=c(a1,a2,a3,b,b,b,c1,c2,c3), 字母, 类型)

f &lt;- 函数(x) {
r &lt;- 分位数(x, probs = c(0.05, 0.25, 0.5, 0.75, 0.95))
名称(r) &lt;- c(&quot;ymin&quot;, &quot;lower&quot;, &quot;middle&quot;, &quot;upper&quot;, &quot;ymax&quot;)
r
}
o &lt;- function(x) {
subset(x, x &lt; quantile(x, 0.05) | quantile(x, 0.95) &lt; x)
}
ggplot(dat, aes(x=letters, y=y, fill=type, group=letters)) + 
stat_summary(fun.data = f, geom=&quot;boxplot&quot;, fill=&quot;white&quot;) +
stat_summary(fun = o, geom=&quot;point&quot;) + facet_wrap(~type, nrow=1) + 
guides(fill=&quot;none&quot;)


我想要的：

当然，一旦轴不再共享，轴范围就会改变，我想要这样，但这太难画了。]]></description>
      <guid>https://stats.stackexchange.com/questions/648975/how-to-show-boxplots-on-two-different-y-axes</guid>
      <pubDate>Mon, 10 Jun 2024 14:35:37 GMT</pubDate>
    </item>
    <item>
      <title>如何处理缺失值？（面板数据）</title>
      <link>https://stats.stackexchange.com/questions/648974/how-to-deal-with-missing-values-panel-data</link>
      <description><![CDATA[我对面板数据有疑问。我正在处理一个面板数据集，该数据集将公司代码描述为 ID 变量，将财政年度描述为时间变量（2013 年至 2022 年的 1500 家公司）。我有几个独立变量和控制变量。但是，在上述时间窗口内，我的变量中有几个缺失值。我还需要滞后独立变量和控制变量。我不确定如何处理缺失值以及在运行描述性分析和回归之前该做什么。我以前从未进行过实证研究，所以我真的很感谢你的帮助。（我使用 Stata）
提前非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/648974/how-to-deal-with-missing-values-panel-data</guid>
      <pubDate>Mon, 10 Jun 2024 14:24:28 GMT</pubDate>
    </item>
    <item>
      <title>如何处理重复和 z 分数？</title>
      <link>https://stats.stackexchange.com/questions/648973/how-to-deal-with-replicates-and-z-scores</link>
      <description><![CDATA[如果您想计算 z 分数，您将如何处理重复？
我的最终目标是制作基因表达热图。为此，我必须计算 z 分数。
分析包括 6 种条件下 6 个基因的一些基因表达数据。
但是，我们用于分析的每个基因都分析了 5 次（因此每个基因有 5 个重复）。
此外，我在基因 5 次重复 3 中有 2 个 NA 值，并且重复 5 在每种情况下都是异常值。
您将如何处理这个问题？作为一名无知的生物学家，我的第一个问题是：我应该对我的数据取平均值（或由于异常值而取中位数），然后计算 z 分数吗？但如果是这样，在公式 (x-mean)/sd 中，x 应该是多少？
我也尝试计算每个基因的每个重复的 z 分数，然后取平均值，但结果看起来很奇怪。]]></description>
      <guid>https://stats.stackexchange.com/questions/648973/how-to-deal-with-replicates-and-z-scores</guid>
      <pubDate>Mon, 10 Jun 2024 14:22:37 GMT</pubDate>
    </item>
    <item>
      <title>Salesforce Marketing Cloud 群发电子邮件打开率问题 [关闭]</title>
      <link>https://stats.stackexchange.com/questions/648970/salesforce-marketing-cloud-mass-email-open-rate-issue</link>
      <description><![CDATA[我正在使用 Salesforce Marketing Cloud 向订阅者发送群发邮件，发现我在 2023 年发送的群发邮件的打开率比 2024 年发送的群发邮件更高，这意味着 2024 年群发邮件的打开率大幅下降
两年发送的目标受众相同
有人能帮我找出这个问题的根本原因吗？我们应该考虑什么来解决这个问题？]]></description>
      <guid>https://stats.stackexchange.com/questions/648970/salesforce-marketing-cloud-mass-email-open-rate-issue</guid>
      <pubDate>Mon, 10 Jun 2024 13:57:36 GMT</pubDate>
    </item>
    <item>
      <title>样本量中固定 alpha 和 beta 的平均差计算的 z 分数总和</title>
      <link>https://stats.stackexchange.com/questions/648954/z-score-total-for-fixed-alpha-and-beta-in-sample-size-calculation-of-mean-differ</link>
      <description><![CDATA[有人能详细解释一下在均值差$\mu_1 - \mu_2$的单尾假设检验中，$z_\text{total}=z_{\alpha} + z_{1-\beta}$如何用共同方差来确定样本量吗？
我找到的所有参考资料都得出了这个公式，但没有分解得出该公式所涉及的步骤。]]></description>
      <guid>https://stats.stackexchange.com/questions/648954/z-score-total-for-fixed-alpha-and-beta-in-sample-size-calculation-of-mean-differ</guid>
      <pubDate>Mon, 10 Jun 2024 10:02:27 GMT</pubDate>
    </item>
    <item>
      <title>标准差的最大似然法</title>
      <link>https://stats.stackexchange.com/questions/648950/maximum-likelihood-of-standard-deviation</link>
      <description><![CDATA[我试图更好地理解样本标准差的分布和不确定性。由于我不是数学家，我尝试将数学文献与一些模拟结果进行比较。
我做了一个相当简单的模拟研究，首先假设 X∼N(μ,σ²)。我从这个分布中抽样三次并估计样本方差。
我使用估计的样本方差并模拟卡方分布以获得方差的上限和下限 95% CI。（我知道，我可以通过取分布的分位数而不模拟结果来做到这一点，但出于比较的原因，我希望得到分布）。我取这些结果的平方根并将它们与总体标准差进行比较。通过重复这种方法 2,500 次，我可以显示样本标准差的 95% 置信区间的覆盖率约为 95%。这就是我所期望的。
不过，我还对 2,500 个卡方分布的众数取了平均值，取了平方根，并将该值与平均样本方差的平方根进行了比较。
众所周知，方差是真实总体方差的无偏估计量，我得到了 1 作为结果。但是，平均模式的平方根导致结果约为 0.77。
为了更好地理解这一点，我绘制了模拟 2500 的卡方分布，并将该分布的模式与样本方差（红线）进行了比较。

模式和估计的样本方差之间存在明显偏差。我的下一个想法是，这可能与最大似然 (ML) 与受限最大似然估计 (REML) 有关。所以，我做了一个小改动。我没有将样本方差乘以 n-1，而是将其乘以 n，然后重复该过程。
var_sim &lt;- (n-1)*sample_var[i]/rchisq(n = n_sim, df = n-1)
对比
var_sim &lt;- (n)*sample_var[i]/rchisq(n = n_sim, df = n-1)
样本方差的预期值再次为 1，平均模式的平方根约为 0.95，现在更接近预期值 1。然而，这带来了覆盖率损失（89%）的代价。

我预计卡方分布的众数/最大似然估计量应该与样本方差一起下降。然而，这两种方法都不是这种情况。我想，我对 ML 估计量和卡方样本分布的模式有一个根本性的误解，我希望有人能解释一下我的问题。
这里是重现我的结果的代码：
set.seed(09062024)

result_var &lt;- NULL
sd_qt_upper &lt;- NULL
sd_qt_lower &lt;- NULL
sd_mean &lt;- NULL
sd_mode &lt;- NULL
sample_var &lt;- NULL

n_sim &lt;- 2500
n &lt;- 3

for(i in 1:n_sim){
x &lt;- rnorm(n = n, 0,1)
sample_var[i] &lt;- var(x)

var_sim &lt;- (n-1)*sample_var[i]/rchisq(n = n_sim, df = n-1)

sd_qt_lower[i] &lt;- sqrt(quantile(var_sim, probs = 0.025)) 
sd_qt_upper[i] &lt;- sqrt(quantile(var_sim, probs = 0.975))
sd_mean[i] &lt;- sqrt(mean(var_sim))
var_density_temp &lt;- density(var_sim, n = n_sim, from = 0, to = 10 )
sd_mode[i] &lt;- sqrt(var_density_temp[[&quot;x&quot;]][which(var_density_temp[[&quot;y&quot;]]==max(var_density_temp[[&quot;y&quot;]]))])
}

coverage_sd &lt;- sd_qt_upper&gt;1&amp;sd_qt_lower&lt;1
mean(coverage_sd) #覆盖率
mean(sd_mean) #基于每个卡方分布和模拟轮次的预期值的预期值
sqrt(mean(sample_var)) #基于初始 var 计算的预期值
mean(sd_mode) #基于模拟模式的预期值

#plot
plot(var_density_temp, main = paste(&quot;chi square distribution simulation:&quot;, i, &quot;\n Mode = &quot;, 
round(var_density_temp[[&quot;x&quot;]][which(var_density_temp[[&quot;y&quot;]]==max(var_density_temp[[&quot;y&quot;]]))],2),
&quot;\n 样本方差 = &quot;, round(sample_var[n_sim], 2)))
abline(v = sample_var[n_sim], col = &quot;red&quot;, lwd = 2)
```
]]></description>
      <guid>https://stats.stackexchange.com/questions/648950/maximum-likelihood-of-standard-deviation</guid>
      <pubDate>Mon, 10 Jun 2024 08:07:12 GMT</pubDate>
    </item>
    <item>
      <title>计算结果的标准差</title>
      <link>https://stats.stackexchange.com/questions/648949/standard-deviation-on-result-of-calculation</link>
      <description><![CDATA[我正在根据实验数据计算一个值。计算结果为
$$ k = \frac{a - b}{c} \cdot \frac{1}{T}, $$
并且 $a$、$b$ 和 $c$ 都是通过多次实验测量的，样本之间会有一些差异，并且 $T$ 是一个常数。它们也是独立测量的，因此即使我碰巧有 $a$、$b$ 和 $c$ 各 $N$ 个值，我也不能说它们在三元组中彼此“属于”对方，或诸如此类。
计算结果 $k$ 中的标准差的正确方法是什么？我相信我不能完全不合理地假设 $a$、$b$ 和 $c$ 服从正态分布，尽管严格来说，这些数字（无论是在现实中还是在测量中）都被限制为正值，因此它们不是真正正态的。
（我尝试从 $a$、$b$ 和 $c$ 的不同值的所有排列中计算 $k$。假设 $N = 4$，那么我有 $4 \times 4我可以使用 \times 4 = 64$ 种不同的组合来计算 $k$，然后我可以取其标准差。不过，这有点像黑客攻击，所以一些理论见解会很好。）
为了提供一个更具体的例子，这里有一些数字（以 Python 代码的形式，为方便起见）。这些是通过测量获得的，其中 $a$ 的值是从四个不同的样本中测得的，$b$ 的值是从另外四个样本中测得的，而 $c$ 的值是从另外四个样本中测得的。
a = np.array([286641, 266093, 227900, 165559])
b = np.array([136748, 159846, 108337, 164340])
c = np.array([303791, 327579, 410016, 340820])
T = 5

因此问题就变成了：给出这些数据，并且假设 $k$ 是根据上面的公式计算出来的，如果我能说出的话，那么关于我计算出的 $k$ 值中的标准差（或其他表达不确定性的方式）是什么呢？]]></description>
      <guid>https://stats.stackexchange.com/questions/648949/standard-deviation-on-result-of-calculation</guid>
      <pubDate>Mon, 10 Jun 2024 07:59:51 GMT</pubDate>
    </item>
    <item>
      <title>广义加性模型中的估计</title>
      <link>https://stats.stackexchange.com/questions/648924/estimation-in-generalized-additive-models</link>
      <description><![CDATA[我目前正在尝试通过 Simon N. Wood 所著的《广义可加模型：R 语言简介》一书来了解广义可加模型 (GAM)。但是，我对以下部分有一些疑问。
第 6.1 章指出，GAM 成为过度参数化的 GLM，形式为
$$g(\mu_i) = X_i\beta, \quad y_i \sim EF(\mu_i, \phi),$$
其中 $EF$ 代表指数族，并且 $\beta$ 通过最大化来估计
$$l_p(\beta) = l(\beta) - \frac{1}{2\phi}\sum_j\lambda_j\beta^TS_j\beta.$$
我的问题是：

简单地从对数似然中减去惩罚项的总和，然后最大化相对于$\beta$的结果函数的理由是什么？
因子$\frac{1}{2\phi}$从何而来？

任何帮助都非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/648924/estimation-in-generalized-additive-models</guid>
      <pubDate>Sun, 09 Jun 2024 20:48:15 GMT</pubDate>
    </item>
    </channel>
</rss>