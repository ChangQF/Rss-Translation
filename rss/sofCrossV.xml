<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Wed, 08 Jan 2025 21:15:20 GMT</lastBuildDate>
    <item>
      <title>如何更好地测试数据是否接受多参数分布的简化？</title>
      <link>https://stats.stackexchange.com/questions/659732/how-can-i-better-test-whether-the-data-accepts-simplification-of-a-many-paramete</link>
      <description><![CDATA[我有一些数据，它们与特定的五参数分布非常吻合，该分布包括许多常见的 2 和 3 参数分布以及一些四参数分布，作为某些参数的特定值的特殊情况，通常为零或一，作为特殊情况。我一直在尝试看看我是否可以通过使用重采样来获得参数值的经验分布并从经验 CDF 创建置信区间来简化分布，但我怀疑有一些更简单的方法可以做到这一点。（我有很多数据 - 大约 60,000 个观测值）。我对五参数分布的估计方法的优点是易于理解，但计算量非常大。因此，重采样后的重新估计既慢又乏味。有没有一种常规方法来完成这项任务，即查看数据是否接受这些简化中的任何一种，如果可以，它是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659732/how-can-i-better-test-whether-the-data-accepts-simplification-of-a-many-paramete</guid>
      <pubDate>Wed, 08 Jan 2025 19:21:02 GMT</pubDate>
    </item>
    <item>
      <title>FE 解释中的相互作用项</title>
      <link>https://stats.stackexchange.com/questions/659728/interaction-term-in-fe-interpretation</link>
      <description><![CDATA[我正在估算固定效应回归，如下所示：
unemployment_ict ~ b1 wage_ict + b2 heat_ict + b3 wage * heat_ict + FEc + error_ict
其中 i 为个人，c 为国家，t 为时间。
假设您得到以下效应：b1=0.7，b2=1.2，b3=-0.4
现在您想要解释交互效应。一种思考方式是边际效应：
这里，您已估计出给定工资和热量的失业预期值
E[y|a,b]=0.7a+1.2b-0.4ab
如果对上述表达式求 a 的导数：
∂E[y|a,b]∂a=0.7-0.4b
此表达式在 b（热量）中递减。然后，您可以将 b 设置为“有趣”的值以进行解释。通常，这些有趣的值是 b 中样本的平均值。
我想知道这在具有固定效应的应用中是否有意义。国家平均值已从固定效应中移除。在热量水平上评估 b 是否有意义。或者应该以 b 的平均增长率或标准差来评估，而不是以水平来评估？
如果有人能对此发表评论，我将不胜感激。]]></description>
      <guid>https://stats.stackexchange.com/questions/659728/interaction-term-in-fe-interpretation</guid>
      <pubDate>Wed, 08 Jan 2025 17:35:13 GMT</pubDate>
    </item>
    <item>
      <title>右删失计数数据的等价性检验</title>
      <link>https://stats.stackexchange.com/questions/659725/equivalence-test-for-right-censored-count-data</link>
      <description><![CDATA[如何对右删失计数数据进行等效性检验？感兴趣的结果是某个时间段内的总癫痫发作次数。但是，用于记录癫痫发作次数的设备在 40 次时停止计数。这是一个硬性限制。因此，需要进行审查。审查的是计数，而不是记录时间——为了清楚起见，范围是 0 到 40+。该设备设置为一次记录几天。每日计数不可用。更复杂的是，有一个“故障”，因此总记录时间可能有所不同。对于某些受试者，记录时间为 168 小时。对于其他受试者，记录时间为 175 小时。我会在更普通的建模中使用这些时间作为偏移量。
因此，我有带偏移量的右删失计数数据。我想进行等效性检验。我应该从哪里开始呢？
这不是我的设计，我也没有记录数据或处理设备。]]></description>
      <guid>https://stats.stackexchange.com/questions/659725/equivalence-test-for-right-censored-count-data</guid>
      <pubDate>Wed, 08 Jan 2025 16:42:35 GMT</pubDate>
    </item>
    <item>
      <title>如何解释两个时间协变量中的时间自相关性？</title>
      <link>https://stats.stackexchange.com/questions/659724/how-to-account-for-temporal-auto-correlation-in-2-time-covariates</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/659724/how-to-account-for-temporal-auto-correlation-in-2-time-covariates</guid>
      <pubDate>Wed, 08 Jan 2025 16:41:35 GMT</pubDate>
    </item>
    <item>
      <title>当得到自由度的十进制值时该怎么办？</title>
      <link>https://stats.stackexchange.com/questions/659723/what-to-do-when-one-gets-a-decimal-value-as-degrees-of-freedom</link>
      <description><![CDATA[我正在尝试找到 2 个随机变量的样本均值差异的置信区间的 t 值。这里的方差是未知且不相等的，因此必须使用的 v 公式是：

完成此计算后，我确信它是正确的，我得到的是一个十进制值。在 t 分布表中，我只能看到 v 的整数值，所以我的问题是我应该向上舍入还是向下舍入。此外，这有什么逻辑吗，还是向上舍入或向下舍入完全是随机的？]]></description>
      <guid>https://stats.stackexchange.com/questions/659723/what-to-do-when-one-gets-a-decimal-value-as-degrees-of-freedom</guid>
      <pubDate>Wed, 08 Jan 2025 15:58:56 GMT</pubDate>
    </item>
    <item>
      <title>从其他测量中更新不可观测概率密度函数估计的技术</title>
      <link>https://stats.stackexchange.com/questions/659721/techniques-to-update-unobservable-probability-density-function-estimate-from-oth</link>
      <description><![CDATA[假设我有一个由许多模型组成的系统，这些模型最初被估计为 PDF。举一个简单的例子，假设我有 A、B、C 和 D 的模型。
这些模型相互关联，使得系统 S = Sum(A, B, C, D)。
如果我能够直接测量 S，并直接测量底层模型的 0-N，那么可以使用哪些技术来生成每个底层模型的后验估计？]]></description>
      <guid>https://stats.stackexchange.com/questions/659721/techniques-to-update-unobservable-probability-density-function-estimate-from-oth</guid>
      <pubDate>Wed, 08 Jan 2025 15:25:24 GMT</pubDate>
    </item>
    <item>
      <title>预测性维护的时间序列分类与回归</title>
      <link>https://stats.stackexchange.com/questions/659720/time-series-classification-vs-regression-for-predictive-maintenance</link>
      <description><![CDATA[假设我们有一个由传感器数据组成的时间序列数据集，每个时间戳都有一个目标列，指示特定组件是否会在一定天数内发生故障；具体来说，如果我们知道故障将发生在时间戳 X，那么对于 X 之前 Y 天内的每个时间戳，目标列标记为 1，否则标记为 0。这通常用于预测性维护论文中，以预测组件的健康状况。
基本上，我们要做的是一个二元分类。现在，我认为我遗漏了有关时间序列分类与时间序列预测的一些内容。我经常看到这样的代码来生成序列以训练模型：
def create_sequences(features, target, seq_length):
sequences = []
labels = []

for i in range(len(features) - seq_length):
sequences.append(features[i:i + seq_length])
labels.append(targets[i + seq_length])

return np.array(sequences), np.array(labels)

因此，我们获取一系列过去的数据，并尝试预测该序列的一个未来值（最终可能是未来目标的一个窗口）：但是，这是为了时间序列预测而做的，不是吗？
如果我们要将代码用于时间序列分类，则必须采用标签
targets[i:i + seq_length]

因此，与特征的方法相同，以便将每个序列与其自己的标签关联并执行分类。
所以我的问题是：

时间序列分类和预测之间的序列创建方式是否不同？
如果第一个问题的答案是肯定的，那么我们将使用targets[i:i + seq_length]，我们究竟如何预测某个组件将来会出现故障？为了更好地阐述第二个问题，考虑到我们在训练期间没有考虑未来的值，我们究竟如何才能提前检测到失败？

此外，如果我尝试在大型数据集（50 多个特征，经过预处理和规范化）上训练 DL 模型（例如 CNN），并将目标移位为给定序列的下一个值，我无法让模型表现得更好，而如果我不移动目标值，我会获得相当不错的性能，所以我不明白这背后的原因。]]></description>
      <guid>https://stats.stackexchange.com/questions/659720/time-series-classification-vs-regression-for-predictive-maintenance</guid>
      <pubDate>Wed, 08 Jan 2025 15:11:13 GMT</pubDate>
    </item>
    <item>
      <title>将部分 eta 平方转换为 Cohen's d 并计算方差</title>
      <link>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</link>
      <description><![CDATA[我正在 R 中进行元分析。对于大多数研究，我使用组的均值和 SD 计算 Hedges 的 g。但是，对于某些研究，我只有部分 $\eta ^2$ 和 95% 置信区间。
例如，
部分 $\eta ^2 = 0.007$
95% CI= 0.08-0

我知道如何将部分 $\eta ^2$ 转换为 Cohen 的 d，然后将其转换为 Hedges 的 g。但是，我该如何计算 Hedges g 的方差呢？

对于某些研究，我只有部分 $\eta ^ 2$，没有置信区间。有没有办法根据这些信息计算出 Hedges g 的方差呢？

]]></description>
      <guid>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</guid>
      <pubDate>Wed, 08 Jan 2025 11:19:07 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验与置信区间有何关系？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for-in-relation-to-confidence-intervals</link>
      <description><![CDATA[我知道，当方差未知时，学生 t 分布用于查找样本均值的置信区间，但卡方检验何时使用？据我记得，当方差未知时也使用它，并且必须将其近似为 s 而不是 σ，因此，何时必须使用卡方分布，何时必须使用学生 t 分布。使用它的必要条件是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for-in-relation-to-confidence-intervals</guid>
      <pubDate>Wed, 08 Jan 2025 09:56:10 GMT</pubDate>
    </item>
    <item>
      <title>分子转化的逻辑回归建模</title>
      <link>https://stats.stackexchange.com/questions/659688/logistic-regression-modeling-of-molecule-conversion</link>
      <description><![CDATA[一个最近的问题涉及一种回归分析，其中一种化学物质用具有特定浓度的混合物处理，以查看转化为新物质的化学物质的比例。
回归分析使用浓度作为唯一特征，比例作为结果，而 OP 选择将比例强行塞入逻辑回归的结果变量中。
但是，假设我们知道分子的起始数量。然后，通过了解比例，我们就知道转化的分子数量。然后，我们可以使用更自然的逻辑回归，其中每个转化/非转化都是一个单独的结果，对应于浓度的特征值，例如$x = (0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4, 0.4, 0.4, 0.6, 0.6, 0.6, 0.6)$，$y = (0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1)$。
但是，每个浓度水平不会有四个分子。会有数不清的分子。从计算的角度来看，这让我很担心，但从统计上讲，这也感觉像是作弊。我们一开始有三个双变量观测值：$x = (0.2, 0.4, 0.6)$，$y = (0.25, 0.5, 0.75)$。我们最终得到了无数个观测值。
但是，如果我们在三个浓度水平上有不同数量的分子，我可以看到这会给反应更多的反应混合物更高的权重，例如如果我们对一升原始分子进行一次反应，对一毫升进行一次反应。我认为在高浓度物质有恶臭，并且其使用量有限，无法使用一升的大反应混合物的情况下，这是一种合理的实验做法。
如果这样做会增加样本量并降低标准误差，那么这样做是否是作弊？那么，每个浓度水平的分子数量可能不同吗？
（从化学物理学的角度来看，我担心这些转化不太可能是独立事件。答案可以但不必解决这个问题。）]]></description>
      <guid>https://stats.stackexchange.com/questions/659688/logistic-regression-modeling-of-molecule-conversion</guid>
      <pubDate>Tue, 07 Jan 2025 21:37:38 GMT</pubDate>
    </item>
    <item>
      <title>线性回归 - 响应变量为百分比改善或 m/s？</title>
      <link>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</link>
      <description><![CDATA[我正在尝试对包含 8 种不同跑步距离的数据集进行统计，这些距离在遵循训练方案之前和之后都有时间，并且基于距离对改进进行线性回归（所有完成时间都有所下降）。我不确定是否要转换为百分比改进或使用 m/s 之类的变量，然后减去跑步时间 1 和跑步时间 2，以便能够比较不同的距离组。显然，绝对时间差异并不大，因为更长的距离自然会有更大的改进。但我读到过，不建议将百分比改进转换为线性回归中的响应变量。我该怎么做？]]></description>
      <guid>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</guid>
      <pubDate>Tue, 07 Jan 2025 21:03:50 GMT</pubDate>
    </item>
    <item>
      <title>在 statsmodels 中 GLM 上使用哪些参数</title>
      <link>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</link>
      <description><![CDATA[我正在模拟试剂转化率对试剂 L 和 M 之间比率的依赖性。比率越高，试剂转化率越高，在 1.5 左右有一个明显的拐点。下面是我拥有的完整数据集

df = pd.DataFrame({&#39;L_M&#39;:[4.75, 3.8, 3.32, 2.85, 2.37, 1.9, 1.42, 0.95, 0.71, 0.47, 0.24, 0.09],
&#39;Conversion&#39;:[0.992, 0.987, 0.993, 1, 0.9, 0.7, 0.31, 0.1, 0.07, 0.06, 0.07, 0.065]})

为此，我使用二项式家族的 GLM 模型，以 Logit 作为链接函数（又称 Logistic 回归）。这似乎很合适，因为我正在对二项式过程中的成功率进行建模（试剂要么发生反应，要么不发生反应）。我尝试使用 statmodels.GLM 失败了，下面的代码产生了非常差的拟合效果：
# 定义因变量和自变量 
Xtrain = df[&#39;L_M&#39;] 
ytrain = df[&#39;Conversion&#39;]

# 构建模型并拟合数据 
log_reg = sm.GLM(ytrain, Xtrain, family=sm.families.Binomial()).fit()

# 推理 
df[&#39;Predicted&#39;] = log_reg.predict(Xtrain)


然而，我使用了一种 hack 的方法，为每个变量创建了 100 个原始数据实验点，每个原始 y 变量为 0 或 1，概率等于此实验点的转换，然后使用 sklearn 将 logreg 拟合到这个（膨胀很多的）数据集，效果很好，产生更好的拟合效果：
from sklearn.linear_model import LogisticRegression

# 实例化模型（使用默认参数）
logreg = LogisticRegression(random_state=16)

# 用数据拟合模型
logreg.fit(X_train, y_train)
new_X = pd.DataFrame(df[&#39;L_M&#39;])
new_X.columns = [&#39;ratio&#39;]
df[&#39;Predicted&#39;]= logreg.predict_proba(new_X)[::,1]



我在 statsmodels.GLM 中遗漏了哪些参数？
更新：为了解释我使用伯努利分布的理由，我可以说，试验总数确实是已知的，因为我准备了解决方案，并且知道试剂的浓度。每个实验点的试验次数也大致相同，而且数量巨大，约为 $10^{15}$。这就是为什么我认为将比率转换为 100 个二元结果集的方法很好地代表了现实。这个特定的实验可以被认为是有一个烧杯，并让分子一个接一个地进入烧杯。如果它发生反应 - 那就是成功，否则就是失败。这项试验在 12 个烧杯中重复进行，这些烧杯在某些特征“L-M”上有所不同。在每个烧杯中，试验重复 $10^{15}$ 至 $10^{16}$ 次，并记录发生反应的分子比例。我可以将此数据呈现为 $12\times10^{15}$ 个二元结果记录，感觉就像拥有 $12\times10^{2}$ 个记录传达了相同的信息。希望这可以解释为什么我相信伯努利模型是适用的。]]></description>
      <guid>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</guid>
      <pubDate>Tue, 07 Jan 2025 20:27:59 GMT</pubDate>
    </item>
    <item>
      <title>mgcv::gam 不能正确地从平滑中分解线性分量</title>
      <link>https://stats.stackexchange.com/questions/659680/mgcvgam-does-not-correctly-decompose-the-linear-component-from-the-smooth</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/659680/mgcvgam-does-not-correctly-decompose-the-linear-component-from-the-smooth</guid>
      <pubDate>Tue, 07 Jan 2025 18:52:03 GMT</pubDate>
    </item>
    <item>
      <title>调整 Chernozhukov 等人的通用机器学习推理以获得事件发生时间结果</title>
      <link>https://stats.stackexchange.com/questions/659674/adapting-chernozhukov-et-al-s-generic-machine-learning-inference-for-time-to-ev</link>
      <description><![CDATA[这是一个非常具体的问题，但我们开始吧！
我正在努力将 Chernozhukov 等人的随机实验中对异质性治疗效果的通用机器学习推断中的方法应用于事件发生时间结果（例如生存分析）。本文通过将机器学习方法与最佳线性预测器 (BLP) 估计和有效推理技术相结合，提供了一个估计异质治疗效果 (HTE) 的框架。
虽然对连续或二元结果有一般的扩展，但将其应用于事件发生时间数据会因审查和生存特定指标而带来独特的挑战。
一般问题：
是否有人实施或改编了 Chernozhukov 等人的事件发生时间结果方法？如果是，需要进行哪些修改才能使该方法与生存分析兼容？
是否有任何通用工具、库或资源可用于解决此类问题，特别是用于调整生存数据的第 1 阶段（代理函数估计）和第 2 阶段（BLP）？

具体挑战：
第 1 阶段：代理函数估计（B(Z) 和 S(Z)）
第 1 阶段的损失函数针对 BLP 估计量身定制。如何调整它以考虑生存数据中的审查？
标准生存损失函数（例如 Cox 模型或 Brier 分数的负偏对数似然）是否足够？如果不够，建议使用哪些替代方案？

第 2 阶段：最佳线性预测器 (BLP) 估计
这是我的主要问题。如何对事件发生时间数据进行 BLP？我可以只实现 Cox 模型吗？这是否已经过研究，并且它是否改进了第 1 阶段的模型估计？

实施和工具
是否有针对事件发生时间数据量身定制的此方法的具体实现（在 R 或 Python 中）？

更广泛的背景：
目标是在存在审查的事件发生时间结果的情况下估计异质性治疗效果，同时通过重复数据分割和分位数聚合等技术保持有效推断。任何与将此方法用于生存分析相关的见解、参考或示例都将非常有帮助。]]></description>
      <guid>https://stats.stackexchange.com/questions/659674/adapting-chernozhukov-et-al-s-generic-machine-learning-inference-for-time-to-ev</guid>
      <pubDate>Tue, 07 Jan 2025 17:24:47 GMT</pubDate>
    </item>
    <item>
      <title>为什么标准差比平均值绝对偏差更受青睐？</title>
      <link>https://stats.stackexchange.com/questions/659643/why-is-standard-deviation-preferred-over-absolute-deviations-from-the-mean</link>
      <description><![CDATA[第一部分
在探索了各种资源和论坛之后，我了解到标准差是一种广泛使用的离散度测量方法，通常比绝对平均偏差更受欢迎（我个人认为后者更简单、更直观），原因如下：
1. 方差的加性：
同意。
2. 平均值最小化平方偏差之和，而中位数（有时不是唯一的）最小化绝对偏差之和：
有点同意，但我不完全理解为什么在这里实现最小值是相关的。有人能解释一下为什么这个属性在选择离散度测量方法时很重要吗？
3.平方偏差在 𝑥 = 0 时可微分，而绝对偏差则不可微分：
有点同意，但我还是不明白为什么可微分性如此重要。这个属性在哪些方面有实际用途？
第二部分
我的独立想法/问题：
方差定义为与平均值的平方偏差的平均值，标准偏差是其平方根（“实际 SD”）。但是，如果标准偏差是平方偏差的平方根的平均值，那不是更有意义吗？ （我提议的是标准差的新定义“建议的 SD”）
(1 / n) * √ Σ((xᵢ - μ)²) 而不是 √(Σ (xᵢ - μ)² / n)

我的理由如下：
平方偏差主要是为了确保：

所有值都是正数。
偏差越大，权重越大。
平均值是使平方偏差之和最小化的数字。

绝对平均偏差达到点 (1)，绝对中位数偏差达到点 (1) 和 (3)。在这些情况下，我们将偏差相加，然后除以 𝑛 得到平均值。
但是，在方差和标准差的情况下，类似的“平均值”就是方差本身。但方差作为一个数字并不能直观地传达分布的扩展。这就是为什么我们要取方差的平方根来得到标准差。
所以，我的问题是：
为什么不使用&quot;建议 SD&quot;作为分散度的度量？
&quot;建议 SD&quot;有什么缺陷？
&quot;实际 SD&quot;为什么比&quot;建议 SD&quot;更适合作为分散度的度量？]]></description>
      <guid>https://stats.stackexchange.com/questions/659643/why-is-standard-deviation-preferred-over-absolute-deviations-from-the-mean</guid>
      <pubDate>Tue, 07 Jan 2025 05:23:59 GMT</pubDate>
    </item>
    </channel>
</rss>