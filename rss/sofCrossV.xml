<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Tue, 16 Jul 2024 01:07:39 GMT</lastBuildDate>
    <item>
      <title>AUC 还可以，但对数损失很糟糕 [重复]</title>
      <link>https://stats.stackexchange.com/questions/651120/getting-ok-auc-but-awful-logloss</link>
      <description><![CDATA[我有一个二元分类问题，目前正尝试使用 xgboost 解决。这是一个处理时间序列的低信噪比情况。我的样本外 AUC 为 0.65，这还算可以（比随机猜测的结果好，后者会给出 0.5）。但是，我的对数损失非常高，大约为 0.68。看看这个答案二元分类器的“愚蠢”对数损失，考虑到我有 20% 的正样本，我预计对数损失会低于
0.2*log(0.2) + 0.8*np.log(0.8) = 0.5

但是，我的对数损失看起来相当糟糕，但同时，AUC 看起来还可以。 F1 也还好，大约为 0.35（以 20% 的概率猜测 1 的随机分类器将实现 F1=0.2）。
我主要困惑于我如何得到如此高的对数损失，是我做错了什么，还是我误解了什么？
注意：我上面使用和报告的 AUC 来自 sklearn.metrics.roc_auc_score，其他所有指标也一样。]]></description>
      <guid>https://stats.stackexchange.com/questions/651120/getting-ok-auc-but-awful-logloss</guid>
      <pubDate>Tue, 16 Jul 2024 00:54:34 GMT</pubDate>
    </item>
    <item>
      <title>如何将两个变量与滞后关联起来</title>
      <link>https://stats.stackexchange.com/questions/651118/how-to-correlate-two-variables-together-with-lag</link>
      <description><![CDATA[我很想知道事件频率与连续变量之间的相关性度量（我知道这个问题类似于这个）。
在这种情况下，我绘制了某一天的体重（以公斤为单位，实值）与吃某种食物的频率的关系图，并按日汇总。在这两种情况下，图中都使用了移动平均值，k=15（即 15 天窗口）并使用复制填充。复制 是指重复两端的边缘值来处理输入的边缘，而不是零填充或有效填充。右下角的图是各种食物类型的汇总。
尽管这种方法存在问题（复制填充的偏差、平滑将未来的数据泄漏到当前值中），但我很想知道如何将体重与不同的变量关联起来，看看哪些因素会产生更大的影响，哪些因素不会。我并不坚持这种方法，我只是认为它会很好地绘制出来。
我尝试使用 Pearson 相关系数来回答这个问题，但不明白为什么有些图的 p 值为 0。这对我来说似乎很奇怪。我也不知道如何解释滞后问题。
任何关于此的建议或资源/概念都非常感激 - 并且很乐意根据需要披露任何数据或代码！
]]></description>
      <guid>https://stats.stackexchange.com/questions/651118/how-to-correlate-two-variables-together-with-lag</guid>
      <pubDate>Mon, 15 Jul 2024 23:49:35 GMT</pubDate>
    </item>
    <item>
      <title>方差分析发现主效应可防止 I 类错误率膨胀</title>
      <link>https://stats.stackexchange.com/questions/651116/anova-finding-a-main-effect-prevents-inflation-of-type-i-error-rate</link>
      <description><![CDATA[在（免费）书籍理解统计学和实验设计第 6.3 节中，他们说

&#39;...ANOVA 提供了第二个技巧。如果我们拒绝了零假设，则适合将均值对与所谓的“事后检验”进行比较，这大致相当于计算成对比较。与第 1 章讨论的
多重测试情况相反。 5，这些多重比较不会夸大 I 型错误率，因为它们仅在方差分析发现主效应时进行。&#39;

为什么方差分析发现主效应可以防止 I 型错误率膨胀？
至少另一本书似乎暗示，在进行事后检验时，确实需要纠正 I 型错误率的膨胀。下面的 JASP 截图似乎也表明需要进行修正。
]]></description>
      <guid>https://stats.stackexchange.com/questions/651116/anova-finding-a-main-effect-prevents-inflation-of-type-i-error-rate</guid>
      <pubDate>Mon, 15 Jul 2024 22:50:53 GMT</pubDate>
    </item>
    <item>
      <title>生成高度非独立的随机样本</title>
      <link>https://stats.stackexchange.com/questions/651114/generating-highly-non-independent-random-samples</link>
      <description><![CDATA[我正在测试非独立数据下的统计测试性能，我想生成随机数据，其中我知道底层的统计分布。
最简单的方法是生成相关数据，例如像这样：
a &lt;- rnorm(10000)
b &lt;- -a + rnorm(10000, 0, 0.00001)

在此示例中，a 和 b 具有非常强的负相关性，但使用配对 t 检验进行测试时，它仍将显示零假设成立（执行多个测试时，p 值的分布是均匀的）。
问题是，当我使用常规 t 检验对独立样本测试此类相关数据时，在 alpha=0.05 时，我得到的假阳性率仅为 16.6%。
有没有办法生成更多非独立数据，使得配对 t 检验（或非参数检验，如符号检验或 Wilcoxon 符号秩检验）不会显示过多的假阳性率，但非配对 t 检验会显示假阳性率非常高？
理想情况下，我希望能够生成数据，对于几千个测量值的样本，非配对 t 检验或 Kolmogorov-Smirnov 检验通常会提供小于 1e-6 的 p 值，但使用成对检验进行测试时，p 值仍然分布均匀。]]></description>
      <guid>https://stats.stackexchange.com/questions/651114/generating-highly-non-independent-random-samples</guid>
      <pubDate>Mon, 15 Jul 2024 22:27:20 GMT</pubDate>
    </item>
    <item>
      <title>根据分类数据对编码器语言模型的嵌入空间进行归一化</title>
      <link>https://stats.stackexchange.com/questions/651113/normalizing-the-embedding-space-of-an-encoder-language-model-with-respect-to-cat</link>
      <description><![CDATA[假设我们有一个类别树/层次结构（例如电子商务网站中的产品类别），每个节点都被分配一个标题。假设每个节点的标题在语义上都是准确的，这意味着它与它所代表的类别（子节点的标题）一致。现在，采用编码器语言模型（如 BERT 或 Word2Vec）并为每个节点/类别生成嵌入。我的目标是确保这些嵌入代表分类层次结构。我想知道：

嵌入（大部分）是否应该与分类层次结构一致，即使分类层次结构非常小众和/或不均匀（意味着分类粒度不一定均匀分布，如果这有意义的话）？一致的意思是，一个父节点下的兄弟节点的嵌入应该比其他父节点的子节点更接近彼此，并且比其他父节点的嵌入更接近其父节点的嵌入。有没有一个很好的指标来衡量/验证这一点？
如果没有，有什么好的方法可以重新映射潜在空间，使其相对于分类层次结构“更漂亮”？我在想 1) 我们直接转换潜在空间或 2) 微调语言编码器模型（即 BERT）。例如：1) 假设类别/父类的子类别/子类形成的形状是一个细椭圆形，是否有可能并且有意义地将空间映射为使这个细椭圆形变成圆形？同样，对于 2)，在正确的父嵌入下对叶嵌入进行分类的任务中，对 BERT 进行微调会做同样的事情吗？

我认为这要求类别不要太小众，并且要与常规语言用法有一定的对应关系（在提供上下文之后甚至更多），这就是我的情况。
这只是我的直觉，所以请让我知道你的想法。我有兴趣探索从最简单的启发式方法到可能与这个问题相关的 SOTA NLP 技术的任何内容。]]></description>
      <guid>https://stats.stackexchange.com/questions/651113/normalizing-the-embedding-space-of-an-encoder-language-model-with-respect-to-cat</guid>
      <pubDate>Mon, 15 Jul 2024 22:21:09 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯估计量何时可以作为充分统计数据的函数注入？</title>
      <link>https://stats.stackexchange.com/questions/651112/when-are-bayes-estimators-injective-as-a-function-of-sufficient-statistics</link>
      <description><![CDATA[我知道贝叶斯估计量只能写成充分统计数据的函数。这些函数什么时候是可注入的？也就是说，什么时候我可以说，给定一个贝叶斯估计量$\delta (\cdot)$和两个充分统计数据$t \neq t&#39;$，我们将得到$\delta(t) \neq \delta(t&#39;)$？
使用二次损失（贝叶斯估计量只是后验分布的平均值）和共轭先验，我发现情况似乎确实如此。具体来说，贝叶斯估计量似乎始终是充分统计数据的仿射函数（请参阅此 Wikipedia 文章中的示例）。这是一个更通用的模式/定理吗？它成立的条件是什么？有没有不成立的反例？]]></description>
      <guid>https://stats.stackexchange.com/questions/651112/when-are-bayes-estimators-injective-as-a-function-of-sufficient-statistics</guid>
      <pubDate>Mon, 15 Jul 2024 22:11:48 GMT</pubDate>
    </item>
    <item>
      <title>残差平方和倒数的期望</title>
      <link>https://stats.stackexchange.com/questions/651110/expectation-of-reciprocal-residual-sum-of-squares</link>
      <description><![CDATA[考虑一个 IID 数据集 $X_1 , \cdots, X_n \in \mathbb{R}^d$，那么我们可以对残差倒数的期望说些什么呢？也就是说我们可以计算
$$
E \Big [ \frac{1}{||r_i||^2} \Big ] = E \Big [ \frac{1}{X_i^T (I - P_i^\perp) X_i} \Big ] .
$$
这是通过考虑精度矩阵的对角线分量得出的。我怀疑期望是无限的，但我无法证明这一点。我还假设 $X_i^TX_i = 1$ 以使问题更简单。
一个可能更简单的问题是考虑
$$
E \Big [ \sum_{j \not = i} \frac{r_i \cdot r_j}{||r_i||^2 ||r_j||^2} \Big ] = E \Big [\sum_{j \not = i} \frac{(X_i - P_iX_i)(X_j - P_jX_j)}{||X_i - P_iX_i||^2||X_j - P_jX_j||^2} \Big ] 
$$
因为我怀疑非对角线项要多得多表现更好，总体平均值为 0。
我尝试应用塔属性，因为 $P_i^\perp$ 取决于 $X_i^c:= \{ X_1, \cdots, X_n \} \backslash \{X_i\}$
$$E[ \frac{1}{1 - X_i^TP_iX_i}] = E[ E[ \frac{1}{1 - X_i^TP_iX_i}|X_i^c]] = E[ E[ \frac{1}{1 - tr (P_iX_iX_i^T)}|X_i^c]],
$$
但感兴趣的项在分母中继续阻碍任何计算期望的标准方法。我认为我可能把事情复杂化了，所以任何关于这个计算的想法都会非常感谢！！]]></description>
      <guid>https://stats.stackexchange.com/questions/651110/expectation-of-reciprocal-residual-sum-of-squares</guid>
      <pubDate>Mon, 15 Jul 2024 21:37:32 GMT</pubDate>
    </item>
    <item>
      <title>使用熵和卡方约束对数据进行去噪</title>
      <link>https://stats.stackexchange.com/questions/651108/denoising-data-with-entropy-and-chi-squared-constraint</link>
      <description><![CDATA[我试图重现 1992 年一篇题为“使用最大熵准则的近似最优平滑”的论文中的去噪方法链接。它需要付费才能观看，但是为了读者的兴趣，我重现了一些必要的文本。有人自己尝试过基于熵的去噪方法吗？除了简要提到他们在 MATLAB 中使用单纯形优化外，本文没有提及必要的实现细节。假设我们有一个带有高斯噪声的嘈杂一维数据。
(i) 由于信号将是均值中心噪声，它将具有负值和接近零的值。熵去噪方法如何在熵去噪中处理这个问题，因为对数函数将无法正常运行。应该用正常数抵消所有值吗？
(ii) 当我们实际上不知道“真实”数据是什么样子时，我们如何估计给定数据的卡方函数中的标准偏差噪声。 是否有任何好的技巧可以估计每个离散数据点的$\sigma_i$。
(iii) 如果噪声不是高斯分布的，除了卡方作为约束之外，还使用哪些其他度量？
来自上述参考资料的背景环境：

熵。 信息熵在离散形式中的数学定义如下：执行具有$n$个可能结果的随机实验。此实验进行了 $j$ 次，因此，假设独立性，则可能存在 $N=n^j$ 种结果。每个不同结果的频率 $f_i$ 为 $$ f_i=\frac{N_i}{N}
\quad \text { for } i=1-\mathrm{N} $$，其中 $i$ 是实验的一种可能结果，而 $N_i$ 是结果 $i$ 发生的次数。向量$\mathbf{F}=\left\{f_1, f_2, \ldots,
f_n\right\}$是一个离散概率分布函数。该分布的熵定义为 $$
H=-\sum_{i=1}^n f_i \log f_i $$ 通过将与数据向量 $\mathbf{X}$ 相关的信息熵
$H$ 定义为 $$ H=-\sum_{i=1}^n x_i
\log x_i $$ 其中 $x_i$ 是数据向量中第 $i$ 个点的值，按向量范数 $\|\mathbf{X}\|$ 缩放，可以计算出频谱的信息熵。当均值为零的高斯噪声随机分布在无噪声数据向量上时，与数据向量相关的不确定性会增加。它是参数空间。最大化 $\mathbf{X}$ 的平滑表示的信息熵（如公式 20 所示）可得出 $S$ 的最可能平滑版本。在最大化熵时，$\mathbf{X}$ 的平滑版本中包含的噪声量实际上在约束条件下最小化。该约束由未知的无噪声信号本身的性质所施加，根据奈奎斯特准则，该信号需要最少的频率数。约束可以用 $\chi^2$
统计量来指定，其中平滑信号 $\mathbf{S}$ 与噪声数据 $\mathbf{X}$ 的拟合通过 $$
\chi^2=\sum_{i=1}^n\left(x_i-\hat{s}_i\right)^2 / \sigma_i{ }^2 $$ 来评估
点 $i$ 处的估计噪声由 $\sigma_i{ }^2$ 给出。因此，$\chi^2$ 分布设定了可能解决方案的置信带的限制区域，其中寻求信息熵的最大值。因此，通过最大化此区域的信息熵，从许多可能的“真实”信号中选择与未知信号形状一致但噪声小于原始数据的最可能信号。根据熵集中定理，可以证明许多其他与数据一致的高度可能的解决方案非常接近具有最大熵的解决方案。
]]></description>
      <guid>https://stats.stackexchange.com/questions/651108/denoising-data-with-entropy-and-chi-squared-constraint</guid>
      <pubDate>Mon, 15 Jul 2024 21:30:37 GMT</pubDate>
    </item>
    <item>
      <title>lmer-如何报告结果和群体差异？</title>
      <link>https://stats.stackexchange.com/questions/651105/lmer-how-to-report-results-and-group-differences</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/651105/lmer-how-to-report-results-and-group-differences</guid>
      <pubDate>Mon, 15 Jul 2024 21:25:16 GMT</pubDate>
    </item>
    <item>
      <title>使用引导偏差校正和加速 (BCa) 置信区间计算 p 值 - R</title>
      <link>https://stats.stackexchange.com/questions/651101/computing-p-value-using-bootstrap-bias-corrected-and-accelerated-bca-confidenc</link>
      <description><![CDATA[在计算 BCa p 值时，boot.pval 包会抛出错误。建议的修复似乎有效，但我不确定这是否正确。该修复是在两年多前提出的，但尚未被接受。已经在 StackExchange 上检查了这个问题，但没有关于 BCa 的具体指导。在下面的代码中，我展示了问题以及使用函数 boot.pval2 的建议修复。
还有其他方法（包等）来计算 bootstrap BCa p 值吗？
library(boot.pval)
library(boot)
# BCA 不起作用

ratio &lt;- function(d, w) sum(d$x * w)/sum(d$u * w)

city.boot &lt;- boot(
city, 
ratio, 
R = 99, 
stype = &quot;w&quot;,
sim = &quot;ordinary&quot;)

boot.pval(city.boot, 
type = &#39;bca&#39;)

# 抛出错误：
# if (any(ints)) out[inds[ints]] 中的错误&lt;- tstar[k[inds[ints]]] :
# 需要 TRUE/FALSE 的缺失值

# 根据建议的修复进行了调整：
# alpha_seq &lt;- seq(pval_precision, 1-pval_precision, pval_precision)

# 调整后的函数：

boot.pval2 &lt;- function(boot_res,
type = &quot;perc&quot;,
theta_null = 0,
pval_precision = NULL,
...)
{
if(is.null(pval_precision)) { pval_precision = 1/boot_res$R }

# 创建一个 alpha 序列：
alpha_seq &lt;- seq(pval_precision, 1-pval_precision, pval_precision)

# 上面的内容从 alpha_seq &lt;- seq(1e-16, 1-1e-16, pval_precision)

# 计算 1-alpha 置信区间，并提取
# 它们的边界：
ci &lt;- suppressWarnings(boot::boot.ci(boot_res,
conf = 1- alpha_seq,
type = type,
...))

bounds &lt;- switch(type,
norm = ci$normal[,2:3],
basic = ci$basic[,4:5],
stud = ci$student[,4:5],
perc = ci$percent[,4:5],
bca = ci$bca[,4:5])

# 找到最小的 alpha，使得 theta_null 不包含在 1-alpha
# 置信区间中：
alpha &lt;- alpha_seq[which.min(theta_null &gt;= bounds[,1] &amp; theta_null &lt;= bounds[,2])]

# 返回 p 值：
return(alpha)
}

boot.pval2(city.boot, 
type = &#39;bca&#39;)

# 结果：
# 0.01010101

可能相关的一个问题是，BCa 引导间隔至少需要与观测次数一样多的迭代次数。请参阅此处和此处。]]></description>
      <guid>https://stats.stackexchange.com/questions/651101/computing-p-value-using-bootstrap-bias-corrected-and-accelerated-bca-confidenc</guid>
      <pubDate>Mon, 15 Jul 2024 20:01:23 GMT</pubDate>
    </item>
    <item>
      <title>风险函数怎么会是负数呢？</title>
      <link>https://stats.stackexchange.com/questions/651100/how-can-a-hazard-function-be-negative</link>
      <description><![CDATA[我一直在阅读R 中的生存分析，这是我为独立学习而找到的一本电子书，但遇到了一个困惑。他们对风险函数的定义如下：

其中 T 是事件时间或正确审查时间中先到的那个。分子是概率，因此是非负的，我假设我们应该将 δ 取为正，因为当 δ ≤ 0 时，P(t &lt; T &lt; t + δ) = 0。因此，风险函数似乎应该是非负的。然而，在第 3 章关于参数建模的文章中，他们展示了以下观察到的瞬时风险函数的示例：

这与风险函数定义如何一致？风险函数为负的解释是什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/651100/how-can-a-hazard-function-be-negative</guid>
      <pubDate>Mon, 15 Jul 2024 19:45:32 GMT</pubDate>
    </item>
    <item>
      <title>我可以使用哪种匹配，为什么我的倾向得分匹配不起作用？</title>
      <link>https://stats.stackexchange.com/questions/651099/which-kind-of-matching-can-i-use-and-why-doesnt-my-propensity-score-matching-wor</link>
      <description><![CDATA[我目前正在尝试找出某种疾病特征是否会影响长期生存。我有观察数据，我正在尝试匹配我的队列，因为两组之间存在巨大的基线差异。我尝试过倾向得分匹配，但结果很差，它稍微增加了平衡，但不是很多。我有大约 6 个变量想要匹配，它们与长期生存有关。我正在使用 R 上的 matchit() 包。这是我用于匹配的代码：
 m.data &lt;- matchit(mydata$cto ~ X, data = mydata, method = &quot;optimal&quot;, distance = &quot;glm&quot;, estimand = &quot;ATC&quot;) X 是我的协变量列表。这是产生最平衡数据集的参数排列。但仍然存在显着的基线差异，我想摆脱这些差异。您将如何实现更好的匹配。
PS：我知道倾向得分匹配的缺陷。我并不是想在这里证明因果关系，我只是希望我的数据能为我未来的工作指明方向。]]></description>
      <guid>https://stats.stackexchange.com/questions/651099/which-kind-of-matching-can-i-use-and-why-doesnt-my-propensity-score-matching-wor</guid>
      <pubDate>Mon, 15 Jul 2024 19:35:44 GMT</pubDate>
    </item>
    <item>
      <title>清理 Pandas 数据集时的奇怪行为（数据类型转换和缺失值）[关闭]</title>
      <link>https://stats.stackexchange.com/questions/651098/strange-behavior-when-cleaning-dataset-in-pandas-data-type-conversion-and-missi</link>
      <description><![CDATA[我当时处理的数据集的特征是未知的（编码名称），尽管所有特征都已包含数字，但数据类型是字符串或对象。此外，还指出缺失值被标记为“na”。其中一个挑战要求我确定某些列的平均值、中位数和标准差，忽略空值。
我所做的是使用带有参数 error=“coerce” 的 to_numeric() 函数，以便“na”值会自动转换为 NaN，我可以删除它们或使用自动忽略这些值的函数（如 describe()）。但是，我无法得出任何列出的选项答案。
在下面的例子中，我得出了 corr 的值为 0.195。
train_df[&#39;class&#39;] = train_df[&#39;class&#39;].map({&#39;neg&#39;: 0, &#39;pos&#39;: 1})

for column in train_df.columns:
train_df[column] = pd.to_numeric(train_df[column], errors=&#39;coerce&#39;)

train_df.dropna(inplace=True, subset=&#39;var1&#39;)
train_df.dropna(inplace=True, subset=&#39;var2&#39;)

corr, _ = spearmanr(train_df[&#39;var1&#39;], train_df[&#39;var2&#39;])
corr

所以，我决定采取不同的方法，看看我是否能找到正确的答案：我决定使用布尔掩码手动删除所有“na”值，然后使用 to_numeric() 转换整个数据集并计算统计数据。这种方法有效，我甚至使用了参数 error=“raise”，没有抛出任何错误。我终于得到了我想要的答案。
在下面的例子中，我得到了 corr 的值为 0.310。
mask = ~train_df.apply(lambda x: x.astype(str).str.contains(&#39;na&#39;)).any(axis=1)
df_cleaned = train_df[mask]

df_cleaned[&#39;var1&#39;] = pd.to_numeric(df_cleaned[&#39;var1&#39;])
df_cleaned[&#39;var2&#39;] = pd.to_numeric(df_cleaned[&#39;var2&#39;])

corr, _ = spearmanr(df_cleaned[&#39;var1&#39;], df_cleaned[&#39;var2&#39;])
corr

有人知道为什么会发生这种情况吗？为什么使用第一种方法得到的统计数据与使用第二种方法得到的统计数据不同？从理论上讲，这两种方法不应该得到相同的干净数据集吗？我的意思是，如果除了“na”之外还有其他空值在第一种方法中自动转换为 NaN，那么在使用 error=“raise” 时，这些值会在第二种方法中导致错误，因为我只从数据集中删除了“na”。有人知道为什么会发生这种情况吗？]]></description>
      <guid>https://stats.stackexchange.com/questions/651098/strange-behavior-when-cleaning-dataset-in-pandas-data-type-conversion-and-missi</guid>
      <pubDate>Mon, 15 Jul 2024 19:30:22 GMT</pubDate>
    </item>
    <item>
      <title>最佳序列分析和假设检验</title>
      <link>https://stats.stackexchange.com/questions/651093/optimal-sequence-analyses-and-hypothesis-testing</link>
      <description><![CDATA[我正在使用 TraMineR 包处理 R 中的数据。我对这种类型的分析还不太熟悉，所以请耐心等待。
我有一个随时间推移在二元组中发生的唯一事件列表（例如：A-F）。我使用 TraMineR 来识别二元组中的聚类模式。最终，我想知道这些聚类是否预测了二元组之间发生事件 Y 的频率（事件 Y 未包含在序列中）。我想我可以使用某种回归方法，但有人能确认这种方法是否合适或哪种回归类型最好吗？
我还在考虑使用生存分析来分析事件发生所需的时间（例如，从事件 A 到 B、从 B 到 C 的过渡...）以及序列成员如何影响该时间范围。同样，我想知道这是否是适合此数据的测试]]></description>
      <guid>https://stats.stackexchange.com/questions/651093/optimal-sequence-analyses-and-hypothesis-testing</guid>
      <pubDate>Mon, 15 Jul 2024 18:20:53 GMT</pubDate>
    </item>
    <item>
      <title>差异对数水平回归的弹性</title>
      <link>https://stats.stackexchange.com/questions/651090/elasticity-from-differenced-log-level-regression</link>
      <description><![CDATA[我有回归
$ \Delta \ln Y_i = \alpha + \beta \Delta X_i + \varepsilon_i $
其中 $\Delta \ln Y_i = \ln Y_{t,i} - \ln Y_{t-1,i}$ 和 $\Delta X_i = ((X_{t,i} - X_{t-1,i}) / T_{t-1,i} ) \cdot 100$。T 是总人口，X 是子集，i 是地区。
假设 $\beta$ 为 -0.073。我对半弹性的解释是，X 相对于基线 T 增加 1 个百分点会导致 Y 减少 7.3%。（这是正确的吗？）
问题：我如何才能消除弹性？也许可以将 $\beta$ 除以 $\Delta X_i$ 的平均值？]]></description>
      <guid>https://stats.stackexchange.com/questions/651090/elasticity-from-differenced-log-level-regression</guid>
      <pubDate>Mon, 15 Jul 2024 17:54:26 GMT</pubDate>
    </item>
    </channel>
</rss>