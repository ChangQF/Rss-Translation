<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>最近的问题 - 交叉验证</title>
    <link>https://stats.stackexchange.com/questions</link>
    <description>来自 stats.stackexchange.com 的最新 30 条</description>
    <lastBuildDate>Wed, 08 Jan 2025 12:33:38 GMT</lastBuildDate>
    <item>
      <title>将部分 eta 平方转换为 Cohen's d 并计算方差</title>
      <link>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</link>
      <description><![CDATA[我正在 R 中进行元分析。对于大多数研究，我使用组的均值和 SD 计算 Hedges 的 g。但是，对于某些研究，我只有部分 $\eta ^2$ 和 95% 置信区间。
例如，
部分 $\eta ^2 = 0.007$
95% CI= 0.08-0

我知道如何将部分 $\eta ^2$ 转换为 Cohen 的 d，然后将其转换为 Hedges 的 g。但是，我该如何计算 Hedges g 的方差呢？

对于某些研究，我只有部分 $\eta ^ 2$，没有置信区间。有没有办法根据这些信息计算出 Hedges g 的方差呢？

]]></description>
      <guid>https://stats.stackexchange.com/questions/659713/convert-partial-eta-squared-to-cohens-d-and-find-variance</guid>
      <pubDate>Wed, 08 Jan 2025 11:19:07 GMT</pubDate>
    </item>
    <item>
      <title>将赔率比转换为 Hedges' g 并计算方差</title>
      <link>https://stats.stackexchange.com/questions/659712/converting-odds-ratio-to-hedges-g-and-computing-variance</link>
      <description><![CDATA[我正在开展荟萃分析，并将所有效应大小转换为 Hedges&#39; g。

对于具有均值和标准差的论文，我使用 metafor 包中的 escalc() 函数，我相信该函数可以计算 Hedges&#39; g 和相应的方差。有人能确认输出确实是 Hedges&#39; g 吗？

但是，我有一些论文只报告了比值比 (OR) 或部分 eta 平方和 95% 置信区间 (CI)。
我知道如何从 OR 计算 Cohen&#39;s d 并将其转换为 Hedges&#39; g。我也知道如何计算 Cohen&#39;s d 的方差。 但是，我不确定如何将 Cohen&#39;s d 的方差转换为 Hedges&#39; g 的方差？
示例：
OR= 4
95% CI：11.7 - 1.4
#根据 OR 计算 Hedges&#39; g
d&lt;- cohens_d(or=OR)
g&lt;- hedges_g(d=d, totaln= 337)

SE_LogOR&lt;-(log(11.7)-log(1.4))/ (2*1.96) # 计算 log OR 的 SE
var_LogOR&lt;- SE_LogOR^2 # 计算 log OR 的方差

#计算 Cohen&#39;s d 的方差
var_d&lt;- var_LogOR * (3/(pi)^2)
]]></description>
      <guid>https://stats.stackexchange.com/questions/659712/converting-odds-ratio-to-hedges-g-and-computing-variance</guid>
      <pubDate>Wed, 08 Jan 2025 11:07:44 GMT</pubDate>
    </item>
    <item>
      <title>基线后混杂因素与并发事件之间的差异（来自估计框架）</title>
      <link>https://stats.stackexchange.com/questions/659711/difference-between-post-baseline-confounders-and-intercurrent-events-from-estim</link>
      <description><![CDATA[与估计框架相比，基线后混杂因素和并发事件有何不同？具体来说，以急救药物为例，它如何既被归类为基线后混杂因素，又被归类为并发事件，两者之间又有何区别？]]></description>
      <guid>https://stats.stackexchange.com/questions/659711/difference-between-post-baseline-confounders-and-intercurrent-events-from-estim</guid>
      <pubDate>Wed, 08 Jan 2025 11:03:54 GMT</pubDate>
    </item>
    <item>
      <title>在 R 中将一个数据集的 PCA 负载应用到具有相同变量的新数据集</title>
      <link>https://stats.stackexchange.com/questions/659710/applying-loadings-of-pca-from-one-dataset-to-a-new-dataset-with-the-same-variabl</link>
      <description><![CDATA[我正在运行 PCA 来为数据集创建财富指数。我们有基线和终点数据，两者都有资产变量。我想先对基线数据运行 PCA，然后将负载应用于包含基线和终点线的整个数据集。
我已经编写了此代码，但想了解这是否是正确的方法。
# 加载必要的库
library(psych)
library(dplyr)

# 虚拟数据集

# 设置数据集的行数
n_rows &lt;- 2800

# 设置可重复性的种子
set.seed(123)

df &lt;- data.frame(
ha01 = sample(0:1, n_rows, replace = TRUE),
ha02 = sample(0:1, n_rows, replace = TRUE),
ha03 = sample(0:1, n_rows, replace = TRUE),
ha04 = sample(0:1, n_rows, replace = TRUE),
ha05 =样本（0：1，n_rows，替换 = TRUE），
ha06 = 样本（0：1，n_rows，替换 = TRUE），
ha07 = 样本（0：1，n_rows，替换 = TRUE），
ha08 = 样本（0：1，n_rows，替换 = TRUE），
ha09 = 样本（0：1，n_rows，替换 = TRUE），
ha10 = 样本（0：1，n_rows，替换 = TRUE），
ha11 = 样本（0：1，n_rows，替换 = TRUE），
ha12 = 样本（0：1，n_rows，替换 = TRUE），
ha13 = 样本（0：1，n_rows，替换 = TRUE），
ha14 = 样本（0：1，n_rows，替换 = TRUE），
ha15 = 样本（0：1，n_rows，替换 = TRUE），
ha16 =样本（0：1，n_rows，替换 = TRUE），
ha17 = 样本（0：1，n_rows，替换 = TRUE），
ha19 = 样本（0：1，n_rows，替换 = TRUE），
ha20 = 样本（0：1，n_rows，替换 = TRUE），
sleeproom1 = 样本（0：1，n_rows，替换 = TRUE），
牲畜_cow_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_oth_cattle_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_donkeys_mules_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_goats_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_sheep_bin = 样本（0：1，n_rows，替换 = TRUE），
牲畜_pigs_bin = sample(0:1, n_rows, replace = TRUE),
liver_chicken_bin = sample(0:1, n_rows, replace = TRUE),
liver_oth_poultry_bin = sample(0:1, n_rows, replace = TRUE),
pca_floor1 = sample(0:1, n_rows, replace = TRUE),
pca_wall1 = sample(0:1, n_rows, replace = TRUE),
pca_roof1 = sample(0:1, n_rows, replace = TRUE),
elec_yes = sample(0:1, n_rows, replace = TRUE)
)

# 创建 baseline_endline 列，其中包含 1400 个 0 和 1400 个 1
df$baseline_endline &lt;- c(rep(0, 1400), rep(1, 1400))

# 仅过滤基线数据
df_pca_b &lt;- df %&gt;% 
filter(baseline_endline == 0)

#Assets 
assets2 &lt;- c(&quot;ha01&quot;, &quot;ha02&quot;, &quot;ha03&quot;, &quot;ha04&quot;, &quot;ha05&quot;, &quot;ha06&quot;, &quot;ha07&quot;, &quot;ha08&quot;, &quot;ha09&quot;, &quot;ha10&quot;, &quot;ha11&quot;, &quot;ha12&quot;, &quot;ha13&quot;, &quot;ha14&quot;, &quot;ha15&quot;, &quot;ha16&quot;, &quot;ha17&quot;, &quot;ha19&quot;, &quot;ha20&quot;,&quot;sleeproom1&quot;,&quot;livestock_cow_bin&quot;,&quot;livestock_oth_cattle_bin&quot;,&quot;livestock_donkeys_mules_bin&quot;,&quot;livestock_goats_bin&quot;,&quot;livestock_sheep_bin&quot;,&quot;livestock_pigs_bin&quot;, 
&quot;livestock_chicken_bin&quot;,&quot;livestock_oth_poultry_bin&quot;,&quot;pca_floor1&quot;,
&quot;pca_wall1&quot;,&quot;pca_roof1&quot;,&quot;elec_yes&quot;)

# 标准化数据（居中并缩放至单位方差）
df_pca_b_std &lt;- as.data.frame(scale(df_pca_b[, assets2]))

# 执行 PCA 并检查特征值
pca_b &lt;- principal(df_pca_b_std, nfactors = length(assets2), rotate = &quot;none&quot;)

# 确定要保留的因子数量（例如，基于特征值 &gt; 1)
n_factors_pca_b &lt;- sum(pca_b$values &gt; 1)

# 使用选定的因子数执行 PCA
pca_b_final &lt;- principal(df_pca_b[, assets2], nfactors = n_factors_pca_b, rotate = &quot;varimax&quot;)

###### 将 PCA 应用于整个数据集 ---- 

# 来自原始 pca 的加载
loadings &lt;- pca_b_final$loadings

# 仅使用 `assets` 中的列对完整数据集 `df` 进行标准化
df_standardized &lt;- scale(df[, assets2])

# 计算完整数据集的第一个组件得分
df$pca_index &lt;- as.matrix(df_standardized) %*% loadings[, 1]

# 创建 df 五分位数
df &lt;- df %&gt;%
mutate(
quintile = ntile(pca_index, 5))

期待您的意见/回复。非常感谢！]]></description>
      <guid>https://stats.stackexchange.com/questions/659710/applying-loadings-of-pca-from-one-dataset-to-a-new-dataset-with-the-same-variabl</guid>
      <pubDate>Wed, 08 Jan 2025 10:45:04 GMT</pubDate>
    </item>
    <item>
      <title>使用非独立观测进行零假设显著性检验</title>
      <link>https://stats.stackexchange.com/questions/659709/null-hypothesis-significance-testing-with-non-independent-observations</link>
      <description><![CDATA[我正在处理一个数据集，其中的测量值被我们划分为不同的类别。主要目标是确定每个类别中的测量值是否与背景（定义为其他类别中所有数据点的集合）有显著差异。
但是，测量值违反了独立性假设。具体来说，测量值可能来自同一物体的不同区域（在本例中，是同一蛋白质内的不同位点）。虽然单个蛋白质之间存在跨越几个数量级的相当大的变异性，但很明显，如果测量值完全独立，分布并不是我们所期望的。虽然我相信我们使用常规统计测试获得的至少一些结果具有生物学相关性，但我还预计获得的 p 值会被夸大。
因此，问题是：什么才是解释数据中这种结构的合适方法？一种粗略而简单的方法可能是在蛋白质水平上“聚合”，并选择测量值在前 X% 和后 X% 范围内的蛋白质进行进一步分析。然而，由于上述单个蛋白质内部的巨大差异，这两组表现出相当大的重叠，使解释变得复杂。我也知道 bootstrap 方法可以解决涉及非 iid 数据的情况，但我不确定这种方法是否适合这种特定情况。]]></description>
      <guid>https://stats.stackexchange.com/questions/659709/null-hypothesis-significance-testing-with-non-independent-observations</guid>
      <pubDate>Wed, 08 Jan 2025 10:31:15 GMT</pubDate>
    </item>
    <item>
      <title>卡方检验有什么用处？[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for</link>
      <description><![CDATA[我知道，当方差未知时，学生 t 分布用于查找样本均值的置信区间，但卡方检验何时使用？据我记得，当方差未知时也使用它，并且必须将其近似为 s 而不是 σ，因此何时必须使用卡方分布，何时必须使用学生 t 分布。
此外，卡方分布通常有什么用处？]]></description>
      <guid>https://stats.stackexchange.com/questions/659707/what-is-the-chi-squared-test-used-for</guid>
      <pubDate>Wed, 08 Jan 2025 09:56:10 GMT</pubDate>
    </item>
    <item>
      <title>如何找到解释最高范围数据的一个线性回归？</title>
      <link>https://stats.stackexchange.com/questions/659705/how-to-find-a-linear-regression-that-explains-highest-range-of-data</link>
      <description><![CDATA[
颜色是虚构的。你不知道前方的颜色。我用颜色来表示它们是从不同的潜在空间中采样的。
LR 是一组线性回归，其中每个线性回归 i 都由以下函数拟合：
$X=1,2,...,i$
$Y=y_1,y_2,...,y_i$。
如何为集合 $LR$ 找到一个最优线性回归，使其适合大多数蓝点，但适合最少的橙色点。
例如，适合 x = 1,2, ...,11, y = y1, y2, ...,y11 的模型将满足我的需求。是否有任何指标表明该模型比其他模型更大？]]></description>
      <guid>https://stats.stackexchange.com/questions/659705/how-to-find-a-linear-regression-that-explains-highest-range-of-data</guid>
      <pubDate>Wed, 08 Jan 2025 07:06:00 GMT</pubDate>
    </item>
    <item>
      <title>不带 MAR 假设计算临床试验的敏感性和特异性</title>
      <link>https://stats.stackexchange.com/questions/659704/computing-sensitivity-and-specificity-of-a-clinical-test-without-mar-assumption</link>
      <description><![CDATA[如周氏《诊断医学中的统计方法》第 337-338 页所述，假设 $D$ 是一个随机变量，如果受试者患有疾病，则假设其值为 $1$。假设 $T$ 是一个随机变量，如果疾病检测结果为阳性，则假设其值为 $1$。假设 $V$ 是一个随机变量，如果受试者已对该疾病进行了进一步验证，则假设其值为 $1$。给定以下参数的值$\lambda_{11} = P(V=1|T=1,D=1)$, $\lambda_{01} = P(V=1|T=1,D=0)$, $\lambda_{10} = P(V=1|T=0,D=1)$, $\lambda_{00} = P(V=1|T=0,D=0)$, $\phi_1 = P(T=1)$, $\phi_{20} = P(D=1|T=0)$, $\phi_{21} = P(D=1|T=1)$，根据这些参数计算敏感度和特异性的正确方法是什么？例如，我知道敏感度是 $P(T=1|D=1)$，不考虑 $V$，它应该计算为 $P(T=1|D=1)P(D=1)/P(T=1)$，但如果必须考虑随机变量 $V$，公式会如何变化？]]></description>
      <guid>https://stats.stackexchange.com/questions/659704/computing-sensitivity-and-specificity-of-a-clinical-test-without-mar-assumption</guid>
      <pubDate>Wed, 08 Jan 2025 06:29:17 GMT</pubDate>
    </item>
    <item>
      <title>如何在线性混合效应模型中使用重复测量数据描述 RCS？</title>
      <link>https://stats.stackexchange.com/questions/659700/how-to-depict-rcs-using-repeated-measured-data-in-linear-mixed-effect-model</link>
      <description><![CDATA[我正在使用“lmer”包来拟合线性混合效应模型，以访问 Y 和 X 的关联。我的数据的响应变量是连续且重复测量的，这需要随机效应和混合效应。但是“rms”包中的“ols”函数仅适用于拟合线性回归模型，无法捕捉时间的随机效应，从而导致模型拟合不准确。 R 代码如下：
fit3 &lt;- lmer(HDS_Z_score ~ VBGP +age + sex + basecog + diftime + (1 | ID), data = main)
fit_rcs &lt;- ols(HDS_Z_score ~ rcs(VB12,5)+ age + sex+ basecog + diftime, data=main) # knots set as 5
我对使用此代码有一些疑问：

“ols”函数是否适用于拟合重复测量数据？
还有什么可以描述线性混合效应模型中重复测量数据的 RCS，无论是 R 包还是 SAS 宏？
]]></description>
      <guid>https://stats.stackexchange.com/questions/659700/how-to-depict-rcs-using-repeated-measured-data-in-linear-mixed-effect-model</guid>
      <pubDate>Wed, 08 Jan 2025 02:19:43 GMT</pubDate>
    </item>
    <item>
      <title>线性回归 - 响应变量为百分比改善或 m/s？</title>
      <link>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</link>
      <description><![CDATA[我正在尝试对包含 8 种不同跑步距离的数据集进行统计，这些距离在遵循训练方案之前和之后都有时间，并且基于距离对改进进行线性回归（所有完成时间都有所下降）。我不确定是否要转换为百分比改进或使用 m/s 之类的变量，然后减去跑步时间 1 和跑步时间 2，以便能够比较不同的距离组。显然，绝对时间差异并不大，因为更长的距离自然会有更大的改进。但我读到过，不建议将百分比改进转换为线性回归中的响应变量。我该怎么做？]]></description>
      <guid>https://stats.stackexchange.com/questions/659686/linear-regression-response-variabel-as-percent-improvement-or-m-s</guid>
      <pubDate>Tue, 07 Jan 2025 21:03:50 GMT</pubDate>
    </item>
    <item>
      <title>在 statsmodels 中 GLM 上使用哪些参数</title>
      <link>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</link>
      <description><![CDATA[我正在模拟试剂转化率对试剂 L 和 M 之间比率的依赖性。比率越高，试剂转化率越高，在 1.5 左右有一个明显的拐点。下面是我拥有的完整数据集

df = pd.DataFrame({&#39;L_M&#39;:[4.75, 3.8, 3.32, 2.85, 2.37, 1.9, 1.42, 0.95, 0.71, 0.47, 0.24, 0.09],
&#39;Conversion&#39;:[0.992, 0.987, 0.993, 1, 0.9, 0.7, 0.31, 0.1, 0.07, 0.06, 0.07, 0.065]})

为此，我使用二项式家族的 GLM 模型，以 Logit 作为链接函数（又称 Logistic 回归）。这似乎很合适，因为我正在对二项式过程中的成功率进行建模（试剂要么发生反应，要么不发生反应）。我尝试使用 statmodels.GLM 失败了，下面的代码产生了非常差的拟合效果：
# 定义因变量和自变量 
Xtrain = df[&#39;L_M&#39;] 
ytrain = df[&#39;Conversion&#39;]

# 构建模型并拟合数据 
log_reg = sm.GLM(ytrain, Xtrain, family=sm.families.Binomial()).fit()

# 推理 
df[&#39;Predicted&#39;] = log_reg.predict(Xtrain)


然而，我使用了一种 hack 的方法，为每个变量创建了 100 个原始数据实验点，每个原始 y 变量为 0 或 1，概率等于此实验点的转换，然后使用 sklearn 将 logreg 拟合到这个（膨胀很多的）数据集，效果很好，产生更好的拟合效果：
from sklearn.linear_model import LogisticRegression

# 实例化模型（使用默认参数）
logreg = LogisticRegression(random_state=16)

# 用数据拟合模型
logreg.fit(X_train, y_train)
new_X = pd.DataFrame(df[&#39;L_M&#39;])
new_X.columns = [&#39;ratio&#39;]
df[&#39;Predicted&#39;]= logreg.predict_proba(new_X)[::,1]



我在 statsmodels.GLM 中遗漏了哪些参数？
更新：为了解释我使用伯努利分布的理由，我可以说，试验总数确实是已知的，因为我准备了解决方案，并且知道试剂的浓度。每个实验点的试验次数也大致相同，而且数量巨大，约为 $10^{15}$。这就是为什么我认为将比率转换为 100 个二元结果集的方法很好地代表了现实。这个特定的实验可以被认为是有一个烧杯，并让分子一个接一个地进入烧杯。如果它发生反应 - 那就是成功，否则就是失败。这项试验在 12 个烧杯中重复进行，这些烧杯在某些特征“L-M”上有所不同。在每个烧杯中，试验重复 $10^{15}$ 至 $10^{16}$ 次，并记录发生反应的分子比例。我可以将此数据呈现为 $12\times10^{15}$ 个二元结果记录，感觉就像拥有 $12\times10^{2}$ 个记录传达了相同的信息。希望这可以解释为什么我相信伯努利模型是适用的。]]></description>
      <guid>https://stats.stackexchange.com/questions/659683/what-params-to-use-on-glm-from-statsmodels</guid>
      <pubDate>Tue, 07 Jan 2025 20:27:59 GMT</pubDate>
    </item>
    <item>
      <title>mgcv::gam 不能正确地从平滑中分解线性分量[关闭]</title>
      <link>https://stats.stackexchange.com/questions/659680/mgcvgam-does-not-correctly-decompose-the-linear-component-from-the-smooth</link>
      <description><![CDATA[]]></description>
      <guid>https://stats.stackexchange.com/questions/659680/mgcvgam-does-not-correctly-decompose-the-linear-component-from-the-smooth</guid>
      <pubDate>Tue, 07 Jan 2025 18:52:03 GMT</pubDate>
    </item>
    <item>
      <title>我应该将点大小（面积）比例缩放为 1/标准误差还是 1/SE^2？</title>
      <link>https://stats.stackexchange.com/questions/659678/should-i-scale-point-size-area-proportion-to-1-standard-error-or-1-se2</link>
      <description><![CDATA[我有一堆估计值，我想将它们绘制在 y 轴上，而 x 轴上则绘制其他值。我想传达每个点的不确定性（x 轴值已知，y 轴值是估计值）。误差线会使图变得非常混乱，所以我想使用点的大小来传达我们对每个点的确定程度。据我所知，建议人们将面积（而不是直径）视为信息量（例如，参见本文或此链接 - 不确定这些是否是标准参考文献）。
但是，有没有研究信息度量应该是 1/SE（标准误差的倒数）还是 1/SE^2（抽样方差的倒数）。鉴于对于误差线，我们将使用 +-SE 或置信区间（95% 的置信区间大约为 +- 1.96*SE），我猜是 1/SE？不知何故，我找不到是否有人尝试过实证测试这是否有效（例如，当人们被问到问题时，他们会根据所选的可视化做出适当的回答，即当给出基于 1/SE 或 1/SE^2 或其他东西的点大小时，他们是否表现得更好）。
更新：一位同事指出，对于元回归，我们通常根据信息缩放点。信息（或研究权重）与 1/方差成正比，因此可以说按 1/方差缩放面积是有意义的。虽然，查看流行的 metafor R 包，似乎我们默认获得了按元回归权重（即 1/方差）缩放的半径（与直径成比例）（我还仔细检查了源代码，并支持按 1/方差和 1/se 缩放直径）。奇怪的是，按 1/方差缩放半径意味着按 1/方差缩放面积（与半径^2 成比例）。这似乎夸大了点所获得的权重。相比之下，为了根据信息缩放面积，将半径或直径缩放 1/SE 似乎更有意义（或者如果您想要与 SE 成比例的面积，则可以将半径缩放 $1/\sqrt(SE)$）。]]></description>
      <guid>https://stats.stackexchange.com/questions/659678/should-i-scale-point-size-area-proportion-to-1-standard-error-or-1-se2</guid>
      <pubDate>Tue, 07 Jan 2025 17:47:22 GMT</pubDate>
    </item>
    <item>
      <title>对 DiD 设计中均值中位数估计量的洞察</title>
      <link>https://stats.stackexchange.com/questions/659655/insights-into-median-of-means-estimators-in-did-designs</link>
      <description><![CDATA[我目前正在开展一个研究项目，重点研究双差分 (DiD) 设计中的稳健估计量，特别是在经典的两期两组设置中。我的主要兴趣是对 y 方向异常值的稳健性（我污染了误差项），并且我一直在探索均值中位数 (MoM) 估计量作为标准 DiD 治疗效果估计量（基于均值）的潜在替代方案。
但是，我遇到了几个问题：

缺乏文献：我找不到任何先前的研究或 DiD 环境中均值中位数估计量的应用。有人知道是否存在这样的研​​究或我可以在哪里找到吗？
意外结果：当我模拟和实施 MoM 估计量时，我得到的结果与标准 DiD 估计量相同 - 即使包含受污染的数据点也是如此。这让我开始质疑为什么 MoM 估计量在这种设置下不是更稳健。

有人能解释为什么中位数估计量在这种情况下可能无法按预期执行吗？或者 DiD 框架中是否存在一些基本问题，导致 MoM 方法不太合适？
如果您有任何想法、文献建议，甚至直观的解释，我将不胜感激！
在此先感谢您的帮助！

这是我的模拟设置：

数据生成：我模拟了 N=500 和 T=2 个时间段（治疗前和治疗后）的面板数据。一半的单位在第二期进行治疗。结果方程为 $Y = 0.1 + 0.2⋅time − 0.1⋅group + β_{true}⋅D + ϵ $。这里，$β_{true} = 0.4 $，ϵ 是带有污染的随机噪声。
污染：随机噪声 𝜖 以混合形式生成：
$ ϵ = \begin{cases} N(0, \sigma^2) &amp; \text{with probability } (1 - p), \\
U(-c, c) &amp; \text{with probability } p \end{cases}$ 
我尝试过参数 $\sigma$、$p$ 和 $c$（也尝试过非对称污染），但这似乎没有什么区别。在下图中，$\sigma = 0.1$，$p = 0.1$ 和 $c = 50$。
估计量构造如下：
$ \text{DiD 估计} =
\left(
\overline{Y}_{\text{Treated, Post}} - \overline{Y}_{\text{Treated, Pre}}
\right)
-
\left(
\overline{Y}_{\text{Control, Post}} - \overline{Y}_{\text{Control, Pre}}
\right)
$
$
\text{中位数（MoM）：}
$

将数据拆分为 $ K = \sqrt(N) $ 个块
计算特定于块的 DiD 估计值：$
\text{Block DiD Estimate} = \text{DiD(Block_i})$
最终 MoM 估计值是块估计值的中位数：$
\text{MoM Estimate} = \text{Median(Block DiD Estimates)}$




块在单元级别定义。因此，每个单元被随机分配到 K 个块中的一个：
block_assignment &lt;- sample(1:K, length(unique(Y_df$unit)), replace = TRUE)

同一单元（在两个时间段内）的所有观察结果都分配给同一个块：
block_map &lt;- data.frame(unit = unique_units, block = block_assignment)Y_df &lt;- Y_df %&gt;% left_join(block_map, by = &quot;unit&quot;)



时间 (T) 取值 {1,2} = {治疗前、治疗后

组 (P) 取值 {0,1} = {对照、治疗
 P &lt;- rbinom(N, 1, 0.5)


治疗虚拟变量 (D) 取值 {0,1}
 D[P == 1, 2] &lt;- 1 # 治疗开始于第 2 阶段，适用于接受治疗的单位





 ]]></description>
      <guid>https://stats.stackexchange.com/questions/659655/insights-into-median-of-means-estimators-in-did-designs</guid>
      <pubDate>Tue, 07 Jan 2025 10:05:08 GMT</pubDate>
    </item>
    <item>
      <title>如何理解概率中的句子结构</title>
      <link>https://stats.stackexchange.com/questions/659651/how-to-understand-structure-of-sentences-in-probability</link>
      <description><![CDATA[我的教科书上是这么写的

(I) 随机选择的一名高中生吃早餐
(II) 随机选择的一名青少年是吃早餐的高中生
(III) 随机选择的一名吃早餐的青少年是高中生

我应该为上述概率选择正确的大小顺序。
书上说概率是

(I): $P(\mathrm I)=\Pr(\text{Breakfast }\mid\text{ Senior})$
(II): $P(\mathrm{II})=\Pr(\text{Breakfast} \cap \text{Senior})$
(III): $P(\mathrm{III})=\Pr(\textrm{Senior} \mid \textrm{Breakfast})$

那么我的问题是，例如在 (II) 中，为什么不是 $P(\mathrm{II})=\Pr(\text{Breakfast} \mid\text{ Senior})$？
因为我觉得如果你把 (III) 写成条件概率，你也应该把 (II) 写成条件概率。
我这里漏掉了什么？]]></description>
      <guid>https://stats.stackexchange.com/questions/659651/how-to-understand-structure-of-sentences-in-probability</guid>
      <pubDate>Tue, 07 Jan 2025 08:58:39 GMT</pubDate>
    </item>
    </channel>
</rss>