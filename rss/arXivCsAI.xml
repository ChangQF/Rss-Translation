<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 12 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>2/3级自动驾驶技术对道路工作区安全的影响</title>
      <link>https://arxiv.org/abs/2503.07634</link>
      <description><![CDATA[ARXIV：2503.07634V1公告类型：新 
摘要：随着中国的道路网络进入维护时代，工作区将成为道路上的普遍景象。随着自动驾驶的发展，配备2/3级自动驾驶能力的车辆也将成为道路上的普遍存在。当这些车辆通过工作区时，自动驾驶可能会脱离接触，这可能会对交通安全产生复杂的影响。本文探讨了2/3级自动驾驶技术对高速公路工作区环境中道路安全的影响。通过微观交通仿真方法和使用全型交通冲突技术，研究因素，诸如市场渗透率（MPR），交通量水平，脱离接触阈值和驾驶员接管样式等因素，以了解其对工作区安全的影响。研究发现，自动驾驶技术对工作区安全的影响很复杂。自动化车辆在工作区域中的脱离接触会减少可以保持自动驾驶状态的车辆的比例。如果收购不是及时或足够的，它很容易导致新的交通冲突。不同的因素对工作区的安全有不同程度的影响。增加的MPR有助于减少单车冲突的发生，但也增加了多车冲突的可能性。因此，未来的研究和改进方向应着重于优化自动驾驶系统的脱离接触检测和接管机制。]]></description>
      <guid>https://arxiv.org/abs/2503.07634</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>对超级对准的研究现在应通过平行优化能力和合规性进行进步</title>
      <link>https://arxiv.org/abs/2503.07660</link>
      <description><![CDATA[ARXIV：2503.07660V1公告类型：新 
摘要：由大生成模型驱动的AI功能的最新飞跃引发了实现人工通用智能（AGI）的可能性，并进一步触发了有关人工超级智能（ASI）的讨论，该系统超过了所有域中所有人类的系统。这引起了一个关键的研究问题：如果我们意识到ASI，我们如何将其与人类价值观保持一致，确保其受益而不是损害人类社会，又称超级对象问题。尽管ASI被许多人视为一个假设的概念，但在本文中，我们认为，超级对象是可以实现的，并且应该通过同时且交替优化任务能力和价值合规的优化来实现它的研究。我们认为，超级对象不仅是ASI的保障，而且是实现其实现的必要条件。为了支持这一立场，我们首先提供了植根于能力和能力之间的差距并详细说明我们的论点之间的差距的正式定义。然后，我们回顾了现有的范例，探索它们的互连和局限性，并说明了以两个基本原则为中心的超级对准的潜在途径。我们希望这项工作能阐明一种实用方法来开发价值一致的下一代AI，从而获得更大的好处并减少对人类的潜在危害。]]></description>
      <guid>https://arxiv.org/abs/2503.07660</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型的迭代多代理调试完全自主编程</title>
      <link>https://arxiv.org/abs/2503.07693</link>
      <description><![CDATA[ARXIV：2503.07693V1公告类型：新 
摘要：与大语言模型（LLMS）的程序合成患有“近乎失误的综合症”：生成的代码与正确的解决方案非常相似，但由于较小的错误而导致单位测试失败。我们使用称为合成，执行，指令，调试和维修的多代理框架（SEIDR）来解决此问题。有效地将SEIDR应用于指令调整的LLM需要确定（a）最佳提示LLMS，（b）哪种排名算法在调试回合中选择最佳程序，以及（c）平衡不成功的程序与新的程序的维修。我们通过比较以替换为重点，以维修为重点的和混合调试策略来探索这些权衡。我们还评估了词汇酶和比赛的选择，以对每一代人的候选人进行排名。在程序合成基准2（PSB2）上，我们的框架在没有维修阶段和传统遗传编程方法的情况下，均优于常规使用OpenAI Codex。 SEIDR的表现要优于单独使用LLM，在PSB2上至少在PSB2上求解了18个问题。为了评估概括性，我们在PSB2和HumaneVal-X基准测试中采用GPT-3.5和Llama 3。尽管具有这些模型的SEIDR并未超过Python基准上的当前最新方法，但人类Val-C ++的结果是有希望的。 Seidr与Llama 3-8b的平均通过@100属于84.2％的100。在所有SEIDR运行中，在HumaneVal-C ++中，GPT-3.5至少解决了164个问题中的163个问题，而164个中的162个问题与较小的Llama 3-8B解决了。我们得出的结论是，SEIDR有效地克服了与LLM的程序合成中几乎缺乏的综合征。]]></description>
      <guid>https://arxiv.org/abs/2503.07693</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>新颖环境中的感官：人类认知如何告知人造药物</title>
      <link>https://arxiv.org/abs/2503.07783</link>
      <description><![CDATA[ARXIV：2503.07783V1公告类型：新 
摘要：拥有最重要的认知能力之一是能够理解世界上的物体，事件和情况。在当前的论文中，我们提供了一种在新环境中创建具有感官能力的人工智能代理的方法。目标：提出几个关键思想：（1）一种新颖的统一概念框架（包括存在嵌入内部和跨框架的标志关系的存在）； （2）通过共享属性（其净响应将代表合成的对象，事件或情况）在各种内容构造的，分布式知识结构之间的相互作用，这是在新型环境中进行感应的标志）。发现：我们建议，跨记忆的属性可以通过新颖的方式共享和重组以创建合成的迹象，这可以表示新型环境中的某些结果（即感官制作）。]]></description>
      <guid>https://arxiv.org/abs/2503.07783</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>有效的神经条款选择加固</title>
      <link>https://arxiv.org/abs/2503.07792</link>
      <description><![CDATA[ARXIV：2503.07792V1公告类型：新 
摘要：子句选择可以说是基于饱和定理证明的最重要的选择点。将其作为加强学习（RL）任务构建是一种挑战最先进的牺牲者启发式启发式方法的一种方式，而是自动发展（仅从权益体验）来自动发展 - 他们的潜在最佳替代者。在这项工作中，我们提出了一个神经网络体系结构，用于评分条款的选择，该条款是有效但有效评估的。遵循RL制定设计决策的原则，我们将网络集成到吸血鬼定理供体中，并通过成功的证明尝试训练它。关于不同TPTP基准测试的实验发现，在基线策略上，神经指导的供者最初从中学到了这一点，就其在实际相关的，短的CPU指令限制下解决的训练中的问题的数量而言，它是从中学到的。]]></description>
      <guid>https://arxiv.org/abs/2503.07792</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RefactorBench：通过代码评估语言代理中的状态推理</title>
      <link>https://arxiv.org/abs/2503.07832</link>
      <description><![CDATA[ARXIV：2503.07832V1公告类型：新 
摘要：语言模型（LM）代理和功能调用的最新进展已使自主，反馈驱动的系统可以解决各种数字域之间的问题。为了更好地理解LM代理的独特局限性，我们引入了Repactorbench，这是一个由100个大型手工制作的多文件重构任务组成的基准，该基准在流行的开源存储库中。在RefactorBench中解决任务需要彻底探索跨多个文件的依赖项，并强烈遵守相关指令。每个任务都由3种不同特异性的自然语言指令定义，并且是相互排斥的，从而可以在同一存储库上创建更长的组合任务。 RefactorBench上的基线表明，与简单的组成任务相比，目前的LM代理仅解决了22％的任务，与基本指令相比，与人类开发人员相反，该任务的时间很短。通过轨迹分析，我们确定了LM代理的各种独特故障模式，并进一步探索了跟踪过去动作的故障模式。通过调整基线代理以条件对国家表示，我们在解决重构培训任务方面取得了43.9％的提高。我们进一步扩展了我们的国家感知方法，以涵盖整个数字环境，并概述未来研究的潜在方向。 RefactorBench旨在通过在代码领域内提供一组现实世界中的多跳任务来支持LM代理的研究。]]></description>
      <guid>https://arxiv.org/abs/2503.07832</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>安全的可解释政策搜索</title>
      <link>https://arxiv.org/abs/2503.07848</link>
      <description><![CDATA[ARXIV：2503.07848V1公告类型：新 
摘要：当用户与AI代理合作时，他们会形成意识或潜意识的期望。满足用户的期望对于此类代理商进行成功的互动和团队至关重要。但是，用户可能会形成与代理商计划行为不同的代理商的期望。这些差异导致在计划过程中考虑了两个独立的决策模型，以产生可阐明的行为。但是，几乎没有做任何事情来纳入安全考虑因素，尤其是在学习环境中。我们提出了安全的可阐明政策搜索（SEP），该搜索旨在提供一种学习方法来产生可解释的行为，同时在学习期间和之后最小化安全风险。我们将SEPs作为一个受约束的优化问题提出，其中代理旨在最大程度地提高基于代理模型的安全性和次级次要标准的限制。创新地结合了约束策略优化和可解释的策略搜索的功能。我们在安全性环境中评估SEP，并通过实体机器人实验来表明它可以学习符合代理商安全要求并有效的可阐明行为。结果表明，SEP可以产生安全可解释的行为，同时确保所需的性能水平W.R.T.代理商的目标，并且在人类ai团队中具有现实世界中的意义。]]></description>
      <guid>https://arxiv.org/abs/2503.07848</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实际因果关系和非确定因果模型</title>
      <link>https://arxiv.org/abs/2503.07849</link>
      <description><![CDATA[ARXIV：2503.07849V1公告类型：新 
摘要：在（Beckers，2025）中，我引入了非确定性因果模型，作为Pearl标准确定性因果模型的概括。我在这里利用这些模型提供的表现力提高，以提供对实际因果关系的新定义（这也适用于确定性模型）。我没有通过（通常是主观的）直觉来激励示例的定义，而是完全基于它在沟通和学习因果模型时能够实现的独特功能而进行的。首先，我概括了反事实依赖性的更基本的概念，其次，我展示了该概念在因果发现逻辑中如何起着至关重要的作用，第三我介绍了因果模型的结构简化概念，最后我将两个概念融合在一起。尽管新颖，但最终的定义作出了与我以前的定义几乎相同的判决（Beckers，2021，2022）。]]></description>
      <guid>https://arxiv.org/abs/2503.07849</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>揭开准确性解除性权衡的神秘面纱：从评论中推断评级的案例研究</title>
      <link>https://arxiv.org/abs/2503.07914</link>
      <description><![CDATA[ARXIV：2503.07914V1公告类型：新 
摘要：可解释的机器学习模型在决策过程背后提供了可理解的推理，尽管它们可能并不总是与黑手盒同行的性能相匹配。可解释性和模型绩效之间的这种权衡引发了围绕AI部署的讨论，尤其是在了解决策的基本原理对信任和问责制至关重要的关键应用程序中。在这项研究中，我们对几种黑盒和可解释模型进行了比较分析，重点是受到有限注意的特定NLP用例：从评论中推断评级。通过此用例，我们探讨了不同模型的性能和可解释性之间的复杂关系。我们引入了一个称为复合解释性（CI）的定量分数，以帮助可视化解释性和性能之间的权衡，尤其是在复合模型的情况下。我们的结果表明，一般而言，随着解释性的降低，学习绩效会提高，但是这种关系并非严格单调，并且在某些情况下，可解释的模型更有利。]]></description>
      <guid>https://arxiv.org/abs/2503.07914</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Bearcubs：用于计算机使用网络代理的基准</title>
      <link>https://arxiv.org/abs/2503.07919</link>
      <description><![CDATA[ARXIV：2503.07919V1公告类型：新 
摘要：现代网络代理具有计算机使用能力，使他们可以通过将命令发送到虚拟键盘和鼠标来与网页进行交互。尽管此类代理具有很大的潜力来帮助人类用户完成复杂的任务，但评估其在现实世界中的能力却带来了重大挑战。为此，我们介绍了Bearcubs，这是111个寻求信息的问题的“小而强大”的基准，旨在评估Web代理的搜索，浏览和从Web中识别事实信息的能力。与以前的Web代理基准不同，求解Bearcub需要（1）访问实时Web内容，而不是综合或模拟页面，这可以捕获现实世界Web交互的不可预测性； （2）执行无法通过基于文本的解决方法绕过的多种模式相互作用（例如，视频理解，3D导航）。 Bearcubs中的每个问题都有相应的简短，明确的答案和人类验证的浏览轨迹，可以透明地评估代理性能和策略。一项人类研究证实，熊柜问题是可解决的，但非平凡（人类准确性为84.7％），揭示了搜索效率低下，并且域知识差距是常见的失败点。相比之下，最先进的计算机代理表现不佳，得分最高的系统（OpenAI的操作员）的精度仅为24.3％。这些结果突出了要改进的关键领域，包括可靠的源选择和更强大的多模式功能。为了促进未来的研究，将定期更新Bearcubs，以替代无效或污染的问题，以使基准测试新鲜，以备将来子孙后代的网络代理商。]]></description>
      <guid>https://arxiv.org/abs/2503.07919</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>研究CHAT数据集：人工智能课程中与Chatgpt的学生对话</title>
      <link>https://arxiv.org/abs/2503.07928</link>
      <description><![CDATA[ARXIV：2503.07928V1公告类型：新 
摘要：大语言模型（LLM）的广泛可用性，例如Chatgpt，对教育产生了重大影响，从而提高了机会和挑战。学生经常可以与LLM驱动的互动学习工具进行互动，但是需要分析其使用模式以确保这些工具的道德使用。为了更好地了解学生如何在学术环境中与LLM互动，我们介绍了\ textbf {studychat}，这是一个公开可用的数据集，该数据集捕获了与LLM驱动的辅导聊天机器人在一个学期的，大学级的，大学级的人工智能（AI）中的现实世界中的互动。我们部署了一个复制ChatGpt的核心功能的Web应用程序，并在进行编程分配时使用它来与LLM进行记录。我们收集了1,197次对话，我们使用对话行为标记架构的标签，以观察到的相互作用模式和先前的研究启发。此外，我们分析了这些相互作用，突出行为趋势，并分析特定用法模式与课程结果的关系。 \ textbf {studychat}为教育社区的学习科学和AI提供了丰富的资源，从而进一步研究了LLM在教育中的发展作用。]]></description>
      <guid>https://arxiv.org/abs/2503.07928</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于LLM的佐证和反驳证据检索科学主张验证</title>
      <link>https://arxiv.org/abs/2503.07937</link>
      <description><![CDATA[ARXIV：2503.07937V1公告类型：新 
摘要：在本文中，我们介绍了Ciber（基于证据检索的索赔调查），这是旨在识别佐证和反驳文档作为科学索赔验证的证据的检索生成（RAG）框架的扩展。 CIBER通过评估各种审讯探针的响应一致性来解决大语言模型（LLM）中固有的不确定性。通过关注LLM的行为分析而无需访问其内部信息，CIBER适用于白色框和Black-Box模型。此外，CIBER以无监督的方式运行，从而使各种科学领域的概括易于概括。使用具有不同语言水平的LLM进行的综合评估，与常规的破布方法相比，Ciber的表现出色。这些发现不仅强调了CIBER的有效性，而且还为基于LLM的科学主张验证的未来进步提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2503.07937</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>边界提示：通过基于图的空间令牌化弹性城市区域表示</title>
      <link>https://arxiv.org/abs/2503.07991</link>
      <description><![CDATA[ARXIV：2503.07991V1公告类型：新 
摘要：城市地区代表制对于各种应用程序，例如城市计划，资源分配和政策制定至关重要。传统方法取决于固定的预定义区域边界，这些边界无法捕获现实世界中城市地区的动态和复杂性。在本文中，我们提出了促使城市区域代表框架（BPURF）的边界，这是一种新型方法，允许弹性的城市地区定义。 BPURF comprises two key components: (1) A spatial token dictionary, where urban entities are treated as tokens and integrated into a unified token graph, and (2) a region token set representation model which utilize token aggregation and a multi-channel model to embed token sets corresponding to region boundaries.此外，我们提出了快速令牌设置的提取策略，以在培训和提示期间启用在线令牌设置提取。该框架通过边界提示来实现城市区域的定义，支持不同的区域边界并适应不同的任务。广泛的实验表明，BPURF在捕获城市地区的复杂特征方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.07991</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>企业智能和分析的LLM驱动知识图</title>
      <link>https://arxiv.org/abs/2503.07993</link>
      <description><![CDATA[ARXIV：2503.07993V1公告类型：新 
摘要：企业内部断开的数据孤岛阻碍了可行的见解的提取，降低了产品开发，客户参与，会议准备和分析驱动的决策等领域的效率。本文介绍了一个使用大型语言模型（LLM）将各种数据源统一为以活动为中心的知识图的框架。该框架可以自动化诸如实体提取，关系推理和语义丰富之类的任务，从而跨电子邮件，日历，聊天，文档和日志启用高级查询，推理和分析。它专为企业灵活性而设计，支持应用程序搜索，任务优先级，专业知识发现，个性化建议和高级分析，以识别趋势和可行的见解。实验结果证明了其在发现专业知识，任务管理和数据驱动决策方面的成功。通过将LLM与知识图集成在一起，该解决方案桥梁断开了连接的系统，并提供了智能分析驱动的企业工具。]]></description>
      <guid>https://arxiv.org/abs/2503.07993</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>SQLCritic：通过条款评论家纠正文本到SQL的生成</title>
      <link>https://arxiv.org/abs/2503.07996</link>
      <description><![CDATA[ARXIV：2503.07996V1公告类型：新 
摘要：文本到SQL系统的最新进步改善了自然语言查询为SQL的转化，但是在确保准确性和可靠性方面仍然存在挑战。尽管自我纠正技术会完善输出，但它们通常会引入新的错误。现有的方法集中在执行反馈上主要解决语法问题，留下语义错误 - 查询的逻辑无法与用户的意图保持一致 - 在很大程度上没有解决。
  我们提出了一种新颖的方法，将结构化的执行反馈与受过训练的评论家的代理商相结合，该反馈提供了详细的，可解释的批评。该方法有效地识别并纠正了句法和语义错误，从而提高了准确性和解释性。实验结果表明，蜘蛛和鸟类的两个主要文本至SQL基准有了显着改善，证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2503.07996</guid>
      <pubDate>Wed, 12 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>