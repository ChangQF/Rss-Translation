<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.AI 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.AI 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 09 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>交互式代理基础模型</title>
      <link>https://arxiv.org/abs/2402.05929</link>
      <description><![CDATA[人工智能系统的发展正在从创建静态的、特定于任务的模型转向动态的、基于代理的系统，能够在广泛的应用中表现良好。我们提出了一种交互式代理基础模型，该模型使用一种新颖的多任务代理训练范例来训练跨广泛领域、数据集和任务的人工智能代理。我们的训练范式统一了不同的预训练策略，包括视觉蒙版自动编码器、语言建模和下一步动作预测，从而实现了多功能且适应性强的人工智能框架。我们展示了我们的框架在三个不同领域的性能——机器人、游戏人工智能和医疗保健。我们的模型展示了其在每个领域生成有意义且与上下文相关的输出的能力。我们方法的优势在于其通用性，利用各种数据源（例如机器人序列、游戏数据、大规模视频数据集和文本信息）进行有效的多模式和多任务学习。我们的方法为开发通才、采取行动的多模式系统提供了一条有前途的途径。]]></description>
      <guid>https://arxiv.org/abs/2402.05929</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:25 GMT</pubDate>
    </item>
    <item>
      <title>LLM 的谈判能力如何？ NegotiationArena 平台和分析</title>
      <link>https://arxiv.org/abs/2402.05863</link>
      <description><![CDATA[谈判是社会交往的基础；人类谈判一切事务，从汽车价格到如何共享公共资源。随着人们对使用大型语言模型 (LLM) 代表人类用户充当代理的兴趣迅速增长，此类 LLM 代理也需要能够进行协商。在本文中，我们研究了法学硕士之间的谈判能力。我们开发了 NegotiationArena：一个用于评估和探讨法学硕士代理人谈判能力的灵活框架。我们在 NegotiationArena 中实现了三种场景来评估 LLM 在分配共享资源（最后通牒游戏）、聚合资源（交易游戏）和买卖商品（价格谈判）方面的行为。每个场景都允许 LLM 代理之间进行多轮灵活对话，以进行更复杂的谈判。有趣的是，法学硕士代理人可以通过采用某些行为策略来显着提高他们的谈判结果。例如，通过假装落寞和绝望，法学硕士在与标准 GPT-4 谈判时可以将他们的收益提高 20%。我们还量化了 LLM 代理人表现出的非理性谈判行为，其中许多也出现在人类身上。总之，\NegotiationArena 提供了一个研究法学硕士互动的新环境，使人们能够对法学硕士的心理理论、非理性和推理能力有新的见解。]]></description>
      <guid>https://arxiv.org/abs/2402.05863</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:24 GMT</pubDate>
    </item>
    <item>
      <title>大语言模型与图神经网络在知识蒸馏中的相遇</title>
      <link>https://arxiv.org/abs/2402.05894</link>
      <description><![CDATA[尽管最近社区披露了大型语言模型 (LLM) 在理解文本属性图 (TAG) 方面的进步和潜力，但 LLM 的高计算和存储要求以及推理过程中的长延迟阻碍了 LLM 的生产部署。同时，虽然传统的图神经网络（GNN）重量轻并且擅长学习图的结构特征，但它们掌握标签中复杂语义的能力在实际应用中受到一定限制。为了解决这些限制，我们专注于 TAG 中节点分类的下游任务，并提出了一种新颖的图知识蒸馏框架，称为语言图知识蒸馏（LinguGKD），使用 LLM 作为教师模型和 GNN 作为知识蒸馏的学生模型。它涉及在设计的节点分类提示上对 LLM 进行面向 TAG 的指令调整，然后采用层自适应对比学习策略，在潜在空间中对齐教师 LLM 和学生 GNN 的分层学习节点特征。通过对各种LLM和GNN模型以及多个基准数据集的广泛实验，所提出的LinguGKD显着提高了学生GNN的预测精度和收敛速度，而不需要额外的数据或模型参数。与教师 LLM 相比，蒸馏 GNN 在某些基准数据集上超越教师 LLM 的分类性能时，以更少的计算和存储需求实现了卓越的推理速度。]]></description>
      <guid>https://arxiv.org/abs/2402.05894</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:24 GMT</pubDate>
    </item>
    <item>
      <title>通过逆向课程强化学习训练大型语言模型进行推理</title>
      <link>https://arxiv.org/abs/2402.05808</link>
      <description><![CDATA[在本文中，我们提出了 R$^3$：通过逆向课程强化学习（RL）进行学习推理，这是一种仅采用结果监督来实现大型语言模型过程监督的好处的新方法。将强化学习应用于复杂推理的核心挑战是识别一系列能够产生积极奖励的动作序列，并为优化提供适当的监督。结果监督为最终结果提供稀疏奖励，而无需识别错误位置，而过程监督提供逐步奖励，但需要大量的手动注释。 R$^3$ 通过从正确的演示中学习来克服这些限制。具体来说，R$^3$ 逐步将推理的开始状态从演示结束滑动到开始，从而促进各个阶段更轻松的模型探索。因此，R$^3$ 建立了阶梯式课程，允许结果监督提供阶梯级信号并精确定位错误。使用 Llama2-7B，我们的方法在 8 个推理任务上平均超过 RL 基线 4.1 美元点。值得注意的是，在 GSM8K 上基于程序的推理中，它在三个骨干模型中超出了基线 4.2$ 点，并且在没有任何额外数据的情况下，Codellama-7B + R$^3$ 的性能与更大的模型或闭源模型相当。]]></description>
      <guid>https://arxiv.org/abs/2402.05808</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:23 GMT</pubDate>
    </item>
    <item>
      <title>预测模型模拟智能体的局限性</title>
      <link>https://arxiv.org/abs/2402.05829</link>
      <description><![CDATA[人们越来越关注将预测模型应用到类似代理的系统中，尤其是基于语言模型的人工智能助手。我们概述了这些模型在转变为代理时可能失败的两个结构性原因。首先，我们讨论自我暗示妄想。先前的工作从理论上表明，如果代理依赖于隐藏的观察，则模型无法模仿生成训练数据的代理：隐藏的观察充当混杂变量，并且模型将它们生成的动作视为不存在的观察的证据。其次，我们引入并正式研究一个相关的、新颖的局限性：预测策略不一致。当模型生成一系列操作时，模型对生成这些操作的策略的隐式预测可以充当混杂变量。结果是模型选择行动就好像它们预计未来的行动不是最优的一样，导致它们过于保守。我们表明，这两种失败都可以通过包含环境的反馈循环来解决，即根据模型自己的行为重新训练模型。我们使用决策转换器对这两种限制进行了简单的演示，并确认实证结果与我们的概念和形式分析一致。我们的治疗方法提供了对这些失败模式的统一看法，并提出了为什么通过在线学习微调离线学习策略会使它们更有效的问题。]]></description>
      <guid>https://arxiv.org/abs/2402.05829</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:23 GMT</pubDate>
    </item>
    <item>
      <title>通过潜意识利用和 Echopraxia 快速优化越狱法学硕士</title>
      <link>https://arxiv.org/abs/2402.05467</link>
      <description><![CDATA[大型语言模型 (LLM) 已在各个领域盛行，以其非凡的推理和理解能力改变了人类的生活。随着它们在敏感任务中的使用越来越多，安全问题受到了广泛关注。我们付出了广泛的努力，使法学硕士符合人类道德原则，以确保其安全部署。尽管有潜力，但最近的研究表明，一致的法学硕士很容易受到专门的越狱提示，绕过安全措施，引发暴力和有害内容。当代法学硕士固有的离散性和巨大的规模对自动生成多样化、高效和有效的越狱提示提出了重大挑战，构成了持续的障碍。在本文中，我们介绍了 RIPPLE（通过潜意识利用和回声行为进行快速优化），这是一种基于优化的新颖方法，其灵感来自两个心理学概念：潜意识和回声行为，它们描述了在无意识的情况下发生的思维过程以及不自觉的模仿。分别采取行动。对 6 个开源 LLM 和 4 个商业 LLM API 的评估显示，RIPPLE 的平均攻击成功率为 91.5%，比当前的五种方法高出 47.0%，开销减少了 8 倍。此外，它还表现出显着的可转移性和隐秘性，成功地逃避了既定的检测机制。我们的工作代码可以在 \url{https://github.com/SolidShen/RIPPLE_official/tree/official} 获取]]></description>
      <guid>https://arxiv.org/abs/2402.05467</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:22 GMT</pubDate>
    </item>
    <item>
      <title>优化人类-人工智能混合团队中的委派</title>
      <link>https://arxiv.org/abs/2402.05605</link>
      <description><![CDATA[当人类和自主系统作为我们所说的混合团队一起运作时，我们当然希望确保团队成功有效地运作。我们将团队成员称为代理。在我们提出的框架中，我们解决了混合团队的情况，在这种情况下，在任何时候，只有一名团队成员（控制代理）被授权充当团队的控制者。为了确定控制代理的最佳选择，我们建议添加一个人工智能管理器（通过强化学习），它作为团队的外部观察者进行学习。经理学习一个行为模型，该模型将代理绩效的观察与团队正在运行的环境/世界联系起来，并根据这些观察做出最理想的控制代理选择。我们通过引入一组约束来限制经理的任务。经理约束表明团队操作是可接受的，因此如果团队进入不可接受的情况并需要经理干预，就会发生违规。为了确保团队的复杂性增加或潜在的低效率最小化，经理应尝试尽量减少团队违反约束并需要后续经理干预的次数。因此，我们的经理正在优化授权代理的选择，以提高团队的整体绩效，同时最大限度地减少经理干预的频率。我们在模拟驾驶场景中展示了经理的表现，该场景代表了由人类驾驶员和自动驾驶系统组成的混合代理团队的情况。我们针对干扰车辆的驾驶场景进行了实验，表明需要避免碰撞和适当的速度控制。我们的结果表明我们的经理产生了积极的影响，在某些情况下，团队绩效提高了约 187%，达到最佳单独代理绩效的 187%。]]></description>
      <guid>https://arxiv.org/abs/2402.05605</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:22 GMT</pubDate>
    </item>
    <item>
      <title>促进公平：人工智能作为游戏玩家</title>
      <link>https://arxiv.org/abs/2402.05786</link>
      <description><![CDATA[功利主义游戏，例如衡量公平性的独裁者游戏，已经在社会科学中研究了几十年。这些游戏不仅让我们深入了解人类如何看待公平，还让我们了解公平、利他主义和贪婪的频率在什么条件下增加或减少。虽然这些游戏传统上主要针对人类，但人工智能的兴起使我们能够研究这些模型如何玩这些游戏。人工智能正在成为人类互动中的常态，研究这些模型如何在游戏中体现公平性可以让我们深入了解人工智能如何做出决策。经过 101 轮的独裁者游戏，我得出的结论是，人工智能具有强烈的公平感，这取决于它认为与之一起玩的人是值得信赖的，当被指定为受托人时，框架对人工智能给予接收者的金额有很大影响，并且可能有证据表明人工智能和人类一样经历不平等厌恶。]]></description>
      <guid>https://arxiv.org/abs/2402.05786</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:22 GMT</pubDate>
    </item>
    <item>
      <title>用分而治之的程序指导大型语言模型来解决问题</title>
      <link>https://arxiv.org/abs/2402.05359</link>
      <description><![CDATA[基础模型，例如大语言模型（LLM），由于其大量的应用而引起了人们的极大兴趣。现有的作品表明，适当的提示设计，例如思想链，可以释放LLM在不同领域的强大能力。然而，在处理涉及重复性子任务和/或欺骗性内容的任务时，例如算术计算和文章级假新闻检测，现有的提示策略要么表现力不足，要么因幻觉而引发中间错误。为了使LLM更能识别此类中间错误，我们建议用分而治之的程序来指导LLM，该程序同时确保卓越的表达能力并解开任务分解、子任务解析和解析组装过程。理论分析表明，我们的策略可以指导LLM扩展固定深度Transformer的表达能力。实验表明，在受中间错误和欺骗性内容困扰的任务中，例如大整数乘法、幻觉检测和错误信息检测，我们提出的方法可以比典型的提示策略取得更好的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.05359</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:21 GMT</pubDate>
    </item>
    <item>
      <title>知识图满足多模态学习：综合调查</title>
      <link>https://arxiv.org/abs/2402.05391</link>
      <description><![CDATA[知识图谱 (KG) 在推进各种人工智能应用方面发挥着关键作用，语义网络社区对多模态维度的探索开启了新的创新途径。在本次调查中，我们仔细回顾了 300 多篇文章，重点关注两个主要方面的知识图谱感知研究：知识图谱驱动的多模态（KG4MM）学习，其中知识图谱支持多模态任务，以及多模态知识图谱（MM4KG） ，它将 KG 研究扩展到 MMKG 领域。我们首先定义 KG 和 MMKG，然后探讨它们的构建进度。我们的回顾包括两个主要任务类别：KG 感知的多模态学习任务，例如图像分类和视觉问答，以及内在的 MMKG 任务，例如多模态知识图补全和实体对齐，突出了具体的研究轨迹。对于大多数此类任务，我们提供了定义、评估基准，并另外概述了进行相关研究的基本见解。最后，我们讨论当前的挑战并确定新兴趋势，例如大语言建模和多模式预训练策略的进展。本调查旨在为已经参与或考虑深入研究 KG 和多模态学习研究的研究人员提供全面的参考，为 MMKG 研究不断发展的前景提供见解并支持未来的工作。]]></description>
      <guid>https://arxiv.org/abs/2402.05391</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:21 GMT</pubDate>
    </item>
    <item>
      <title>通过可解释模型和策略网络进行神经符号强化学习的三种途径</title>
      <link>https://arxiv.org/abs/2402.05307</link>
      <description><![CDATA[神经符号人工智能将经典符号方法的可解释性、简约性和显式推理与数据驱动神经方法的统计学习相结合。同时可微分和可解释的模型和政策可能是这种婚姻的关键推动者。本文展示了在现实世界的强化学习环境中实施此类模型和策略的三种途径。具体来说，我们研究了一类广泛的神经网络，它们将可解释的语义直接构建到其架构中。我们揭示并强调了逻辑、模拟和学习相结合的潜力和本质困难。一个教训是，学习受益于连续性和可微性，但经典逻辑是离散且不可微的。对实值、可微表示的放松提出了一种权衡；可学习的东西越多，可解释的东西就越少。另一个教训是，在数值模拟的背景下使用逻辑涉及从原始（例如，实值时间序列）模拟数据到逻辑谓词的重要映射。本说明提出的一些开放性问题包括：基于规则的控制器的局限性是什么，以及它们的可学习性如何？这里讨论的可微分可解释方法是否可以扩展到大型、复杂、不确定的系统？我们真的能实现可解释性吗？我们通过三种方法强调这些和其他主题。]]></description>
      <guid>https://arxiv.org/abs/2402.05307</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:20 GMT</pubDate>
    </item>
    <item>
      <title>KIX：元认知泛化框架</title>
      <link>https://arxiv.org/abs/2402.05346</link>
      <description><![CDATA[人类和其他动物在解决各种任务时恰当地表现出一般智力行为，通过重用和应用随着时间的推移获得的高水平知识，具有灵活性和适应新情况的能力。但人工代理更像是专家，缺乏这种通才行为。人工代理将需要理解和利用关键的结构化知识表示。我们提出了一个元认知泛化框架——知识交互执行（KIX），并认为利用类型空间与对象交互有助于学习可迁移的交互概念和泛化。这是将知识融入强化学习的自然方式，并有望成为人工智能系统中自主和通才行为的推动者。]]></description>
      <guid>https://arxiv.org/abs/2402.05346</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:20 GMT</pubDate>
    </item>
    <item>
      <title>CADReN：用于可控交叉图节点重要性估计的上下文锚驱动关系网络</title>
      <link>https://arxiv.org/abs/2402.05135</link>
      <description><![CDATA[节点重要性估计 (NIE) 对于通过检索器增强生成将外部信息集成到大型语言模型中至关重要。传统方法侧重于静态、单图特征，缺乏对新图和用户特定需求的适应性。我们提出的方法 CADReN 通过引入上下文锚定 (CA) 机制来解决这些限制。这种方法使网络能够评估节点相对于 CA 的重要性，同时考虑知识图 (KG) 中的结构和语义特征。大量实验表明，CADReN在跨图NIE任务中取得了更好的性能，具有零样本预测能力。 CADReN 还被证明可以与之前模型在单图 NIE 任务上的性能相匹配。此外，我们还引入并开源了两个新数据集 RIC200 和 WK1K，专门为跨图 NIE 研究而设计，为该领域的未来发展提供了宝贵的资源。]]></description>
      <guid>https://arxiv.org/abs/2402.05135</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:19 GMT</pubDate>
    </item>
    <item>
      <title>SceMQA：科学大学入学级多模态问答基准</title>
      <link>https://arxiv.org/abs/2402.05138</link>
      <description><![CDATA[本文介绍了 SceMQA，这是一种针对高考水平的科学多模态问答的新颖基准。它解决了现有基准中经常被忽视的关键教育阶段，从高中到大学预科阶段。 SceMQA 专注于核心科学科目，包括数学、物理、化学和生物学。它采用多项选择和自由回答的形式，确保对人工智能模型能力的全面评估。此外，我们的基准测试还提供了每个问题的具体知识点以及每个答案的详细解释。 SceMQA 还独特地呈现具有相同上下文但不同问题的问题，以促进对推理能力进行更彻底和准确的评估。在实验中，我们在各种实验设置中评估了开源和闭源最先进的多模态大型语言模型（MLLM）。结果表明，需要进一步研究和开发来开发更强大的 MLLM，最强大的模型只能达到 50% 到 60% 的准确度。我们的基准测试和分析将在 https://scemqa.github.io/ 上提供]]></description>
      <guid>https://arxiv.org/abs/2402.05138</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:19 GMT</pubDate>
    </item>
    <item>
      <title>表处理的大型语言模型：调查</title>
      <link>https://arxiv.org/abs/2402.05121</link>
      <description><![CDATA[表通常是二维的，可存储大量数据，在数据库查询、电子表格计算和从 Web 表生成报告等日常活动中至关重要。使用大型语言模型 (LLM) 自动执行这些以表为中心的任务可以提供显着的公共利益，引起学术界和工业界的兴趣。这项调查对表格任务进行了广泛的概述，不仅涵盖表格问答（Table QA）和事实验证等传统领域，还包括表格操作和高级表格数据分析等新强调的方面。此外，它超越了预训练和微调小语言模型的早期策略，包括 LLM 使用中的最新范式。这里的重点特别是法学硕士领域内的指令调整、提示和基于代理的方法。最后，我们强调了几个挑战，从私有部署和高效推理到为表操作和高级数据分析开发广泛的基准。]]></description>
      <guid>https://arxiv.org/abs/2402.05121</guid>
      <pubDate>Fri, 09 Feb 2024 06:16:18 GMT</pubDate>
    </item>
    </channel>
</rss>