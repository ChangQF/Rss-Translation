<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Fri, 01 Nov 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>论证与机器学习</title>
      <link>https://arxiv.org/abs/2410.23724</link>
      <description><![CDATA[arXiv:2410.23724v1 公告类型：新
摘要：本章概述了研究工作，这些工作提出了计算论证和机器学习之间某种程度的相互影响的方法。我们对文献的回顾确定了两个广泛的主题，代表了这两个领域之间相互作用的目的：机器学习的论证和论证的机器学习。在这两个主题中，我们系统地评估了各个维度的作品范围，包括学习类型和使用的论证框架形式。此外，我们确定了这两个领域之间的三种相互作用类型：协同方法，其中论证和机器学习组件紧密集成；分段方法，其中两者交错，使得一个组件的输出是另一个组件的输入；近似方法，其中一个组件在选定的细节级别上遮蔽另一个组件。我们得出关于某些形式的论证是否适合支持某些类型的机器学习的结论，反之亦然，从审查中可以看出清晰的模式。虽然所评论的作品为成功结合这两个研究领域提供了启发，但我们也确定并讨论了需要解决的局限性和挑战，以确保它们在人工智能进步的过程中仍然是富有成效的组合。]]></description>
      <guid>https://arxiv.org/abs/2410.23724</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>实现可靠的对齐：不确定性感知的 RLHF</title>
      <link>https://arxiv.org/abs/2410.23726</link>
      <description><![CDATA[arXiv:2410.23726v1 公告类型：新
摘要：大型语言模型与人类偏好对齐的最新进展得益于更大的奖励模型和更好的偏好数据。然而，这些方法中的大多数都依赖于奖励模型的准确性。强化学习与人类反馈 (RLHF) 中使用的奖励模型通常使用随机优化算法从小数据集中学习，因此容易出现高度可变性。我们在众多开源数据集上通过经验说明了奖励模型之间的不一致性。
我们从理论上表明，奖励模型的波动可能对对齐问题有害，因为派生的策略更适合奖励模型，因此如果奖励模型本身不确定，则风险更大。我们使用测量集中来激励一种不确定性感知的保守策略优化算法。我们表明，这种政策更具风险规避性，因为它们对不确定的奖励更加谨慎。我们从理论上证明了我们提出的方法比原始方法风险更小。
我们通过基于设计一组奖励模型的实验证实了我们的理论结果。我们使用这组奖励模型来使用我们的方法对齐语言模型，并观察到我们的实证结果与我们的理论预测相符。]]></description>
      <guid>https://arxiv.org/abs/2410.23726</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>编辑后模型性能下降的原因及解决方法</title>
      <link>https://arxiv.org/abs/2410.23843</link>
      <description><![CDATA[arXiv:2410.23843v1 公告类型：new 
摘要：知识编辑技术因能够低成本更新大规模语言模型中不正确或过时的知识而受到广泛关注。然而近期研究发现，编辑后的模型往往会表现出不同程度的性能下降，这一现象背后的原因及潜在的解决方案尚未给出。为了探究编辑模型性能下降的原因并优化编辑方法，本文从数据和模型两个角度探究其背后的原因。具体而言，1）从数据角度，为明确数据对编辑模型性能的影响，本文首先构建了一个多问题数据集（MQD）来评估不同类型的编辑数据对模型性能的影响。通过实验确定，编辑模型的性能主要受编辑目标的多样性和序列长度的影响。2）从模型角度，本文探究影响编辑模型性能的因素。结果表明编辑模型层的 L1 范数与编辑准确率之间存在很强的相关性，阐明了这是导致编辑性能瓶颈的重要因素。最后，为了提高编辑模型的性能，本文进一步提出了一种 Dump for Sequence (D4S) 方法，通过降低编辑层的 L1 范数成功克服了之前的编辑瓶颈，允许用户进行多次有效编辑，并最大限度地减少了模型损坏。我们的代码可以在 https://github.com/nlpkeg/D4S 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.23843</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>图上规划：知识图谱上大型语言模型的自校正自适应规划</title>
      <link>https://arxiv.org/abs/2410.23875</link>
      <description><![CDATA[arXiv:2410.23875v1 Announce Type: new 
摘要：大型语言模型（LLM）在复杂任务上表现出卓越的推理能力，但仍然存在知识过时、幻觉和决策不透明等问题。相比之下，知识图谱（KG）可以为LLM提供明确且可编辑的知识，以缓解这些问题。现有的KG增强型LLM范式手动预定义了探索空间的广度，并要求在KG中进行无缺陷导航。然而，该范式无法基于问题语义自适应地探索KG中的推理路径并自我纠正错误的推理路径，导致效率和效果的瓶颈。为了解决这些限制，我们提出了一种新的自校正自适应规划范式，用于 KG 增强的 LLM，称为 Plan-on-Graph (PoG)，它首先将问题分解为几个子目标，然后重复自适应探索推理路径、更新记忆和反思自我纠正错误推理路径的需要的过程，直到得出答案。具体来说，设计了指导、记忆和反思三个重要机制来协同工作，以保证图推理的自校正规划的自适应广度。最后，在三个真实数据集上进行的大量实验证明了 PoG 的有效性和效率。]]></description>
      <guid>https://arxiv.org/abs/2410.23875</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用 PyRAT 进行神经网络验证</title>
      <link>https://arxiv.org/abs/2410.23903</link>
      <description><![CDATA[arXiv:2410.23903v1 公告类型：新
摘要：随着人工智能系统越来越受欢迎并应用于各种关键领域（健康、交通、能源等），对其安全性提供保证和信任的必要性是不可否认的。为此，我们提出了 PyRAT，这是一种基于抽象解释的工具，用于验证神经网络的安全性和鲁棒性。在本文中，我们描述了 PyRAT 用于从神经网络的输入开始查找神经网络可达状态的不同抽象，以及该工具的主要功能，以提供对神经网络的快速和准确分析。PyRAT 已经在多个合作中用于确保安全保障，其在 VNN-Comp 2024 上获得第二名，展示了其性能。]]></description>
      <guid>https://arxiv.org/abs/2410.23903</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RL-STaR：自学推理器的强化学习框架的理论分析</title>
      <link>https://arxiv.org/abs/2410.23912</link>
      <description><![CDATA[arXiv:2410.23912v1 公告类型：新 
摘要：大型语言模型 (LLM) 的推理能力随着思路链 (CoT) 的提示而得到改善，使模型能够逐步解决复杂任务。然而，训练 CoT 能力需要详细的推理数据，而这些数据往往很少。自学推理器 (STaR) 框架通过使用强化学习自动生成推理步骤来解决这一问题，减少对人工标记数据的依赖。尽管 STaR 及其变体已证明取得了实证成功，但缺乏解释这些改进的理论基础。这项工作为理解强化学习对 CoT 推理和 STaR 的有效性提供了一个理论框架。我们的贡献是：(1) 对策略改进的分析，展示了为什么 LLM 推理会随着 STaR 而迭代改进；(2) 收敛到最佳推理策略的条件； (3) 检查 STaR 的稳健性，解释它如何在偶尔出现错误步骤的情况下提高推理能力；(4) 启动有效推理改进所需的预训练模型质量标准。该框架旨在将实证研究结果与理论见解结合起来，推动 LLM 推理强化学习方法的发展。]]></description>
      <guid>https://arxiv.org/abs/2410.23912</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>总结因果图中的平均受控和平均自然微直接效应</title>
      <link>https://arxiv.org/abs/2410.23975</link>
      <description><![CDATA[arXiv:2410.23975v1 公告类型：新
摘要：在本文中，我们研究了因果系统中平均控制直接效应和平均自然直接效应的可识别性，这些因果系统由摘要因果图表示，摘要因果图是完整因果图的抽象，通常用于动态系统中，其中循环和省略的时间信息使因果推理变得复杂。与传统的线性设置不同，在传统的线性设置中，直接效应通常更容易识别和估计，而非参数直接效应对于处理现实世界的复杂性至关重要，特别是在流行病学背景下，变量之间的关系（例如遗传、环境和行为因素）通常是非线性的，更难定义和识别。特别是，我们给出了在存在隐藏混杂的情况下从摘要因果图中识别平均控制微直接效应和平均自然微直接效应的充分条件。此外，我们表明，在没有隐藏混淆且我们只对通过调整实现的可识别性感兴趣的环境中，给出的平均控制微直接效应的条件也是必要的。]]></description>
      <guid>https://arxiv.org/abs/2410.23975</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>AndroidLab：Android 自主代理的训练和系统基准测试</title>
      <link>https://arxiv.org/abs/2410.24024</link>
      <description><![CDATA[arXiv:2410.24024v1 公告类型：新
摘要：自主代理对于与现实世界的交互变得越来越重要。特别是 Android 代理，最近成为一种经常被提及的交互方法。然而，现有的用于训练和评估 Android 代理的研究缺乏对开源和闭源模型的系统研究。在这项工作中，我们提出了 AndroidLab 作为一个系统的 Android 代理框架。它包括一个具有不同模态的操作环境、动作空间和一个可重现的基准。它在同一动作空间中支持大型语言模型 (LLM) 和多模态模型 (LMM)。AndroidLab 基准包括预定义的 Android 虚拟设备和在这些设备上构建的 9 个应用程序中的 138 个任务。通过使用 AndroidLab 环境，我们开发了一个 Android 指令数据集并训练了六个开源 LLM 和 LMM，将 LLM 的平均成功率从 4.59\% 提高到 21.50\%，将 LMM 的平均成功率从 1.93\% 提高到 13.28\%。 AndroidLab 是开源的，可在 \url{https://github.com/THUDM/Android-Lab} 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2410.24024</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>数值规划的图学习</title>
      <link>https://arxiv.org/abs/2410.24080</link>
      <description><![CDATA[arXiv:2410.24080v1 公告类型：新
摘要：图形学习自然非常适合用于符号、以对象为中心的规划，因为它能够利用规划领域中展示的关系结构，并将具有任意数量对象的规划实例作为输入。数值规划是符号规划的扩展，其中状态现在也可以显示数值变量。在这项工作中，我们提出了数据高效且可解释的机器学习模型，用于学习解决数值规划任务。这涉及为具有连续和分类属性的图构建新的图形内核，以及用于学习启发式函数进行数值规划的新优化方法。实验表明，我们的图内核比用于数值规划的图神经网络效率更高，泛化性更好，并且与独立于领域的数值规划器相比，覆盖性能也更具竞争力。代码可在 https://github.com/DillonZChen/goose 获得]]></description>
      <guid>https://arxiv.org/abs/2410.24080</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Text2Motion：从自然语言指令到可行计划</title>
      <link>https://arxiv.org/abs/2303.12153</link>
      <description><![CDATA[arXiv:2303.12153v5 公告类型：交叉 
摘要：我们提出了 Text2Motion，这是一个基于语言的规划框架，使机器人能够解决需要长期推理的顺序操作任务。给定自然语言指令，我们的框架构建任务和运动级计划，并验证该计划是否达到推断的符号目标。Text2Motion 使用编码在技能库的 Q 函数中的可行性启发式方法来指导使用大型语言模型进行任务规划。以前的基于语言的规划器只考虑单个技能的可行性，而 Text2Motion 通过在搜索过程中执行几何可行性规划来主动解决跨越技能序列的几何依赖关系。我们在一系列需要长期推理、抽象目标解释和部分可供性感知处理的问题上评估了我们的方法。我们的实验表明，Text2Motion 可以以 82% 的成功率解决这些具有挑战性的问题，而之前最先进的基于语言的规划方法只能达到 13%。因此，Text2Motion 为具有技能间几何依赖关系的语义多样化顺序操作任务提供了有希望的泛化特性。]]></description>
      <guid>https://arxiv.org/abs/2303.12153</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>解析生成策略的失败模式：运行时监控一致性和进度</title>
      <link>https://arxiv.org/abs/2410.04640</link>
      <description><![CDATA[arXiv:2410.04640v2 公告类型：交叉 
摘要：通过模仿学习训练的机器人行为策略在偏离训练数据的条件下容易失败。因此，在测试时监控学习到的策略并提供故障早期预警的算法对于促进可扩展部署是必不可少的。我们提出了 Sentinel，这是一个运行时监控框架，将故障检测分为两个互补的类别：1) 不稳定故障，我们使用时间动作一致性的统计度量来检测；2) 任务进展故障，我们使用视觉语言模型 (VLM) 来检测策略何时自信且一致地采取无法解决任务的行动。我们的方法有两个主要优势。首先，由于学习到的策略表现出不同的故障模式，因此结合互补的检测器可以显着提高故障检测的准确性。其次，使用统计时间动作一致性度量可确保我们以可忽略不计的计算成本快速检测到多模态生成策略何时表现出不稳定的行为。相反，我们只使用 VLM 来检测对时间不太敏感的故障模式。我们在模拟和现实世界中针对机器人移动操作领域训练的扩散策略背景下展示了我们的方法。通过统一时间一致性检测和 VLM 运行时监控，Sentinel 检测到的故障比单独使用两种检测器多 18%，并且性能明显优于基线，从而凸显了将专用检测器分配给互补故障类别的重要性。定性结果可在 https://sites.google.com/stanford.edu/sentinel 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.04640</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>FVEval：了解数字硬件形式验证中的语言模型功能</title>
      <link>https://arxiv.org/abs/2410.23299</link>
      <description><![CDATA[arXiv:2410.23299v1 公告类型：交叉 
摘要：大型语言模型 (LLM) 卓越的推理和代码生成能力激发了人们对应用 LLM 实现数字芯片设计任务自动化的极大兴趣。特别是，最近的工作研究了将这些模型应用于形式验证 (FV) 的早期想法，这是一种验证硬件实现的方法，可以提供强有力的信心保证，但需要大量的人力。虽然 LLM 驱动的自动化的价值显而易见，但由于缺乏整体评估，我们对模型性能的理解受到了阻碍。作为回应，我们提出了 FVEval，这是第一个全面的基准和评估框架，用于表征与 FV 相关的任务中的 LLM 性能。基准测试由三个子任务组成，它们在不同级别上衡量 LLM 的能力：从根据自然语言描述生成 SystemVerilog 断言 (SVA) 到推理设计 RTL 并直接提出断言而无需额外的人工输入。作为测试实例，我们提供了专家编写的验证材料集合和方法，以可扩展地生成与工业 FV 工作流程相一致的合成示例。我们根据 FVEval 对各种现有的 LLM（包括专有和开源）进行了评估，在此基础上，我们研究了当今 LLM 的现状以及如何进一步使它们的应用能够提高数字 FV 的生产力。我们的基准和评估代码可在 \url{https://github.com/NVlabs/FVEval} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2410.23299</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络在医疗物联网 (IoMT) 中实现高级网络攻击检测</title>
      <link>https://arxiv.org/abs/2410.23306</link>
      <description><![CDATA[arXiv:2410.23306v1 公告类型：交叉 
摘要：医疗物联网 (IoMT) 与医疗保健系统的日益融合显著增强了患者护理，但也带来了严重的网络安全挑战。本文提出了一种基于卷积神经网络 (CNN) 的新方法，用于检测 IoMT 环境中的网络攻击。与以前主要使用传统机器学习 (ML) 模型或更简单的深度神经网络 (DNN) 的研究不同，所提出的模型利用 CNN 的功能有效地分析网络流量数据的时间特征。在 CICIoMT2024 数据集上进行训练和评估，该数据集包含一系列 IoMT 设备中的 18 种不同类型的网络攻击，所提出的 CNN 模型与以前最先进的方法相比表现出卓越的性能，在二元、分类和多类分类任务中实现了 99% 的完美准确率。这一性能超越了传统的 ML 模型，例如 Logistic 回归、AdaBoost、DNN 和随机森林。这些发现凸显了 CNN 大幅改善 IoMT 网络安全的潜力，从而确保联网医疗保健系统的保护和完整性。]]></description>
      <guid>https://arxiv.org/abs/2410.23306</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>系统地分析不同 LLM 架构中的即时注入漏洞</title>
      <link>https://arxiv.org/abs/2410.23308</link>
      <description><![CDATA[arXiv:2410.23308v1 公告类型：交叉 
摘要：本研究系统地分析了 36 个大型语言模型 (LLM) 对各种提示注入攻击的脆弱性，这是一种利用精心设计的提示来引发恶意 LLM 行为的技术。在 144 次提示注入测试中，我们观察到模型参数与漏洞之间存在很强的相关性，通过逻辑回归和随机森林特征分析等统计分析表明参数大小和架构显着影响敏感性。结果显示，56% 的测试导致成功的提示注入，强调了各种参数大小的广泛漏洞，聚类分析识别了与特定模型配置相关的不同漏洞配置文件。此外，我们的分析发现了某些提示注入技术之间的相关性，表明漏洞可能存在重叠。这些发现强调了在关键基础设施和敏感行业部署的 LLM 中迫切需要强大的多层防御。成功的提示注入攻击可能导致严重后果，包括数据泄露、未经授权的访问或错误信息。未来的研究应该探索多语言和多步骤防御以及自适应缓解策略，以加强多样化现实环境中的 LLM 安全性。]]></description>
      <guid>https://arxiv.org/abs/2410.23308</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>计算机中的道德主体：在大型语言模型中探索自由意志</title>
      <link>https://arxiv.org/abs/2410.23310</link>
      <description><![CDATA[arXiv:2410.23310v1 公告类型：交叉 
摘要：本研究探讨了确定性系统（特别是大型语言模型 (LLM)）展示道德主体和兼容自由意志的功能能力的潜力。我们以丹尼特的兼容框架为基础，建立了自由意志的功能定义，该定义建立在跨学科理论基础之上，该理论基础融合了香农的信息论、丹尼特的兼容论和弗洛里迪的信息哲学。该框架强调了理性响应和价值观一致在确定道德责任方面的重要性，而不是要求形而上学的自由意志。香农的理论强调了处理复杂信息在实现自适应决策方面的作用，而弗洛里迪的哲学通过将主体概念化为一个频谱来调和这些观点，从而允许基于系统的复杂性和响应性对道德地位进行分级观察。我们对法学硕士在道德困境中的决策的分析表明，他们具有理性思考的能力，能够根据新信息和发现的不一致之处调整选择。因此，他们表现出与我们对自由意志的功能定义相一致的道德主体特征。这些结果挑战了关于意识对道德责任必要性的传统观点，表明具有自指推理能力的系统可以在人工和生物环境中体现一定程度的自由意志和道德推理。本研究提出了一个简洁的框架，用于将自由意志理解为一个涵盖人工和生物系统的范围，为人工智能时代的主体和伦理的进一步跨学科研究奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2410.23310</guid>
      <pubDate>Fri, 01 Nov 2024 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>