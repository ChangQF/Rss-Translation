<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.AI 更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 05 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>动物友好型人工智能案例</title>
      <link>https://arxiv.org/abs/2403.01199</link>
      <description><![CDATA[arXiv:2403.01199v1 公告类型：新
摘要：人工智能被认为越来越重要，并且可能会产生深远的影响，但人工智能伦理和人工智能工程领域尚未完全认识到这些技术，包括大语言模型（LLM）将对动物产生巨大影响。我们认为这种影响很重要，因为动物在道德上很重要。
  作为评估法学硕士中动物考虑的第一个实验，我们构建了一个概念验证评估系统，该系统从多个角度评估法学硕士的反应和偏见。该系统通过两个标准评估法学硕士的输出：其真实性以及对动物利益的考虑程度。我们使用一组结构化查询和预定义的规范视角测试了 OpenAI ChatGPT 4 和 Anthropic Claude 2.1。初步结果表明，测试模型的结果可以根据它们对动物的考虑进行基准测试，并且可以通过更发达和经过验证的系统来解决和减轻所产生的立场和偏见。
  我们的研究为将动物伦理融入人工智能提供了一种可能的方法，为涉及动物和社会或与动物和社会相关的各个领域的未来研究和实际应用开辟了道路，包括教育、公共政策和监管。总的来说，这项研究是迈向更有用、更负责任的人工智能系统的一步，可以更好地认识和尊重所有众生的切身利益和观点。]]></description>
      <guid>https://arxiv.org/abs/2403.01199</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:07 GMT</pubDate>
    </item>
    <item>
      <title>不确定知识图上的软推理</title>
      <link>https://arxiv.org/abs/2403.01508</link>
      <description><![CDATA[arXiv:2403.01508v1 公告类型：新
摘要：基于机器学习的逻辑查询回答研究使得能够利用大规模且不完整的知识图谱进行推理。本文通过考虑知识的不确定性进一步推进了这一研究方向。知识的不确定性在现实世界中被广泛观察到，但 \textit{并不}与支撑现有研究的一阶逻辑无缝地一致。为了弥补这一差距，我们研究了对不确定知识的软查询的设置，这是由软约束规划的建立所激发的。我们进一步提出了一种基于机器学习的方法，具有前向推理和后向校准功能，可以回答大规模、不完整和不确定的知识图谱上的软查询。理论讨论表明，我们的方法与一阶查询的最先进的推理算法具有相同的复杂性。实证结果证明我们的方法相对于之前基于数字嵌入扩展的基于机器学习的方法具有优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.01508</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:07 GMT</pubDate>
    </item>
    <item>
      <title>算法配置问题</title>
      <link>https://arxiv.org/abs/2403.00898</link>
      <description><![CDATA[arXiv:2403.00898v1 公告类型：新
摘要：随着算法参数自动配置方法的发展，算法优化领域取得了显着的进步。本文深入研究算法配置问题，重点关注优化参数化算法以解决决策/优化问题的特定实例。我们提出了一个全面的框架，不仅形式化了算法配置问题，而且还概述了利用机器学习模型和启发式策略解决其问题的不同方法。本文将现有方法分为按实例方法和按问题方法，区分模型构建和部署的离线和在线策略。通过综合这些方法，我们的目标是为理解和解决算法配置固有的复杂性提供一条清晰的途径。]]></description>
      <guid>https://arxiv.org/abs/2403.00898</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:06 GMT</pubDate>
    </item>
    <item>
      <title>假设中的假设：最好的半事实解释是使用反事实作为指导找到的吗？</title>
      <link>https://arxiv.org/abs/2403.00980</link>
      <description><![CDATA[arXiv:2403.00980v1 公告类型：新
摘要：最近，使用“仅当”解释的反事实在可解释人工智能（XAI）中变得非常流行，因为它们描述了黑盒人工智能系统的特征输入的哪些变化会导致（通常是负面的）决策的变化——结果。甚至最近，使用“即使”解释的半事实也获得了更多关注。他们阐明了特征输入的变化，但这些变化并没有改变人工智能系统的决策结果，并有可能提出更有益的资源。一些半事实方法使用查询实例的反事实来指导半事实产生（所谓的反事实引导方法），而其他方法则不使用（所谓的无反事实方法）。在这项工作中，我们使用 5 个关键指标对 7 个数据集上的 8 种半事实方法进行了全面测试，以确定是否需要反事实指导来找到最佳的半事实。这些测试的结果表明并非如此，而是计算决策空间的其他方面会带来更好的半事实 XAI。]]></description>
      <guid>https://arxiv.org/abs/2403.00980</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:06 GMT</pubDate>
    </item>
    <item>
      <title>ToolNet：通过工具图连接大型语言模型和海量工具</title>
      <link>https://arxiv.org/abs/2403.00839</link>
      <description><![CDATA[arXiv:2403.00839v1 公告类型：新
摘要：虽然在广泛的任务中取得了显着的进展，但大型语言模型（LLM）在正确使用大量外部工具方面仍然受到严重限制。现有的情境学习方法只是将工具格式化为纯文本描述列表，并将其输入到法学硕士，法学硕士从中生成一系列工具调用来逐步解决问题。这种范式忽略了工具之间的内在依赖性，并将所有推理负载卸载给法学硕士，使它们仅限于有限数量的专门设计的工具。因此，对于法学硕士来说，使用大量工具库仍然具有挑战性，在面对现实场景时会产生很大的限制。本文提出了 ToolNet，这是一个即插即用的框架，可以将工具的数量扩展到数千个，同时适度增加代币消耗。 ToolNet 将工具组织成有向图。每个节点代表一个工具，加权边表示工具转换。从初始工具节点开始，LLM 通过从其后继节点中迭代选择下一个工具节点在图中进行导航，直到任务得到解决。大量实验表明，ToolNet 在具有挑战性的多跳工具学习数据集上可以取得令人印象深刻的结果，并且对工具故障具有弹性。]]></description>
      <guid>https://arxiv.org/abs/2403.00839</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:05 GMT</pubDate>
    </item>
    <item>
      <title>冲突中的团队组建</title>
      <link>https://arxiv.org/abs/2403.00859</link>
      <description><![CDATA[arXiv:2403.00859v1 公告类型：新
摘要：在这项工作中，我们提出了冲突中的团队组建问题。目标是将个人分配给具有给定能力的任务，同时考虑到个人的任务偏好以及它们之间的冲突。使用相关舍入方案作为我们的主要工具箱，我们提供有效的近似算法。我们的框架非常通用，可以模拟教育环境和人力资源管理中出现的许多不同的现实场景。我们在现实世界的数据集上测试和部署我们的算法，并表明我们的算法找到的分配比自然基线找到的分配更好。在教育环境中，我们还展示了我们的作业如何远远优于人类专家手动完成的作业。在人力资源管理应用程序中，我们展示了我们的任务如何增加团队的多样性。最后，使用合成数据集，我们证明我们的算法在实践中可以很好地扩展。]]></description>
      <guid>https://arxiv.org/abs/2403.00859</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:05 GMT</pubDate>
    </item>
    <item>
      <title>利用深层生成技术推动零售供应链：分类、调查和见解</title>
      <link>https://arxiv.org/abs/2403.00861</link>
      <description><![CDATA[arXiv:2403.00861v1 公告类型：新
摘要：生成式人工智能应用程序，例如 ChatGPT 或 DALL-E，已经向世界展示了它们在生成类人文本或图像方面的令人印象深刻的能力。更深入地说，这些人工智能应用程序的科学利益相关者是深度生成模型，又名 DGM，其旨在学习数据的底层分布并生成在统计上与原始数据集相似的新数据点。提出了一个关键问题：我们如何将 DGM 应用于现代零售供应链领域？为了解决这个问题，本文希望对 DGM 进行全面回顾，并讨论其在零售供应链中现有和潜在的用例，方法是 (1) 提供最先进的 DGM 及其变体的分类和概述，( 2）从端到端的角度回顾零售供应链中现有的 DGM 应用程序，以及（3）讨论如何进一步利用 DGM 来解决零售供应链问题的见解和潜在方向。]]></description>
      <guid>https://arxiv.org/abs/2403.00861</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:05 GMT</pubDate>
    </item>
    <item>
      <title>MedAide：利用大型语言模型在边缘设备上提供本地医疗援助</title>
      <link>https://arxiv.org/abs/2403.00830</link>
      <description><![CDATA[arXiv:2403.00830v1 公告类型：新
摘要：大型语言模型（LLM）以其卓越的自然语言处理（NLP）能力正在给各个领域带来革命性的变化。然而，在资源有限的边缘计算和嵌入式系统中部署法学硕士面临着巨大的挑战。另一个挑战在于在医疗设施和基础设施有限的偏远地区提供医疗援助。为了解决这个问题，我们推出了 MedAide，一个本地医疗保健聊天机器人。它利用与LangChain集成的微型法学硕士，提供高效的基于边缘的初步医疗诊断和支持。 MedAide 采用模型优化，在没有服务器基础设施的嵌入式边缘设备上实现最小的内存占用和延迟。使用低秩适应（LoRA）优化训练过程。此外，该模型还根据不同的医疗数据集进行训练，利用人类反馈的强化学习 (RLHF) 来增强其特定领域的能力。该系统在各种消费级 GPU 和 Nvidia Jetson 开发板上实现。 MedAide 在医疗咨询方面实现了 77% 的准确率，在 USMLE 基准中得分为 56，打造了一个节能的医疗保健援助平台，减轻了基于边缘的部署带来的隐私问题，从而为社区赋能。]]></description>
      <guid>https://arxiv.org/abs/2403.00830</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:04 GMT</pubDate>
    </item>
    <item>
      <title>立场文件：人工智能代理迈向整体智能</title>
      <link>https://arxiv.org/abs/2403.00833</link>
      <description><![CDATA[arXiv:2403.00833v1 公告类型：新
摘要：大型基础模型的最新进展显着增强了我们对开放世界环境中感官信息的理解。在利用基础模型的力量时，人工智能研究必须摆脱过度还原论，转而强调作为有凝聚力的整体的系统。具体来说，我们强调开发代理人工智能——一种将大型基础模型集成到代理操作中的具体系统。 Agent AI 的新兴领域涵盖了广泛的现有体现和基于代理的多模式交互，包括机器人、游戏和医疗保健系统等。在本文中，我们提出了一种新颖的大型动作模型来实现体现的智能行为，Agent基础模型。除此之外，我们还讨论了智能体如何在各种领域和任务中展现出卓越的能力，挑战我们对学习和认知的理解。此外，我们从跨学科的角度讨论智能体的潜力，强调科学话语中的人工智能认知和意识。我们相信这些讨论可以作为未来研究方向的基础，并鼓励更广泛的社会参与。]]></description>
      <guid>https://arxiv.org/abs/2403.00833</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:04 GMT</pubDate>
    </item>
    <item>
      <title>在合作语言游戏中适应队友</title>
      <link>https://arxiv.org/abs/2403.00823</link>
      <description><![CDATA[arXiv:2403.00823v1 公告类型：新
摘要：Codenames 游戏最近成为智能代理设计的一个感兴趣的领域。该游戏的独特之处在于队友之间的语言和协调发挥着重要作用。以前为该游戏设计代理的方法使用单一内部语言模型来确定动作选择。这通常会导致某些队友的表现良好，而其他队友的表现较差，因为代理无法适应任何特定的队友。在本文中，我们提出了第一个用于播放 Codenames 的自适应代理。我们采用集成方法，其目标是在与特定队友交互的过程中确定我们的内部专家代理（每个代理可能都有自己的语言模型）是最佳匹配。这种方法面临的一个困难是缺乏能够准确捕捉 Codenames 团队绩效的单一数字指标。之前的 Codenames 研究利用了一些不同的指标来评估代理团队。我们提出了一种新颖的单一指标来评估 Codenames 团队的表现，无论是玩单团队（纸牌）游戏，还是与另一个团队进行竞争性游戏。然后，我们提出并分析一个集成代理，该代理在每轮选择一名内部专家，以最大化该建议指标。实验分析表明，这种集成方法适用于单个队友，并且通常与最好的内部专家与队友的表现几乎一样好。至关重要的是，这种成功并不依赖于之前对队友、整体特工或他们的兼容性的了解。这项研究代表了使基于语言的代理（例如代号）更适合单个队友的合作语言设置的重要一步。]]></description>
      <guid>https://arxiv.org/abs/2403.00823</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:03 GMT</pubDate>
    </item>
    <item>
      <title>TroubleLLM：与红队专家结盟</title>
      <link>https://arxiv.org/abs/2403.00829</link>
      <description><![CDATA[arXiv:2403.00829v1 公告类型：新
摘要：大型语言模型（LLM）成为各种自然语言任务的最先进的解决方案，并集成到现实世界的应用程序中。然而，法学硕士在表现出不良的安全问题（例如社会偏见和有毒内容）方面可能具有潜在的危害。在部署之前必须评估其安全问题。然而，现有方法生成的测试提示的质量和多样性仍远不能令人满意。这些方法不仅劳动密集型、需要大量预算成本，而且对于LLM申请的特定测试领域缺乏测试提示生成的可控性。借助LLM进行LLM测试的想法，我们提出了第一个LLM，称为TroubleLLM，用于针对LLM安全问题生成可控的测试提示。大量的实验和人工评估证明了TroubleLLM在发电质量和发电可控性方面的优越性。]]></description>
      <guid>https://arxiv.org/abs/2403.00829</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:03 GMT</pubDate>
    </item>
    <item>
      <title>使用大型语言模型引导认知代理</title>
      <link>https://arxiv.org/abs/2403.00810</link>
      <description><![CDATA[arXiv:2403.00810v1 公告类型：新
摘要：大型语言模型包含关于世界的嘈杂的常识，但很难训练或微调。另一方面，认知架构具有出色的可解释性，并且可以灵活更新，但需要大量的手动工作来实例化。在这项工作中，我们结合了两方面的优点：利用大型语言模型中编码的噪声知识引导基于认知的模型。通过执行厨房任务的具体代理，我们表明，与完全基于大型语言模型的代理相比，我们提出的框架具有更高的效率。我们的实验表明，大型语言模型是认知架构的良好信息来源，而认知架构反过来可以验证大型语言模型的知识并将其更新到特定领域。]]></description>
      <guid>https://arxiv.org/abs/2403.00810</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:02 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士高风险决策中的认知偏差</title>
      <link>https://arxiv.org/abs/2403.00811</link>
      <description><![CDATA[arXiv:2403.00811v1 公告类型：新
摘要：大型语言模型（LLM）作为支持不断扩大的决策任务范围的工具具有巨大的潜力。然而，鉴于法学硕士对人类（创建的）数据进行的培训，法学硕士可能会继承针对受保护群体的社会偏见，并受到认知偏见的影响。这种类似人类的偏见可能会妨碍在法学硕士的协助下做出公平且可解释的决策。我们的工作引入了 BiasBuster，这是一个旨在发现、评估和减轻法学硕士认知偏差的框架，特别是在高风险决策任务中。受先前心理学和认知科学研究的启发，我们开发了一个包含 16,800 个提示的数据集，用于评估不同的认知偏差（例如，提示引起的、顺序的、固有的）。我们测试了各种偏见缓解策略，同时提出了一种使用法学硕士来消除他们自己的提示偏见的新方法。我们的分析提供了关于不同商业和开源模型中认知偏差的存在及其影响的全面图景。我们证明，我们的自助消除偏见可以有效地减轻认知偏见，而无需为每种偏见类型手动制作示例。]]></description>
      <guid>https://arxiv.org/abs/2403.00811</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:02 GMT</pubDate>
    </item>
    <item>
      <title>关于法学硕士在规划中的作用：将法学硕士嵌入规划图中</title>
      <link>https://arxiv.org/abs/2403.00783</link>
      <description><![CDATA[arXiv:2403.00783v1 公告类型：新
摘要：计划综合旨在生成一系列行动或政策，将给定的初始状态转变为目标状态，提供可由专家设计或从训练数据或与世界交互中学习的领域模型。出于对大型语言模型（LLM）中的紧急规划能力的兴趣，人们提出了研究 LLM 规划有效性的工作，而不考虑在 LLM 中使用任何现成的规划技术。在本文中，我们旨在通过调查法学硕士在现成规划框架中的作用，进一步研究法学硕士规划能力的洞察力。为此，我们研究了将法学硕士嵌入到著名的规划框架之一（基于图的规划）中的有效性，提出了一种新颖的基于法学硕士的规划框架，其中法学硕士嵌入了两个级别的规划图，即相互约束生成级别和约束解决水平。我们凭经验展示了我们提出的框架在各个规划领域的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.00783</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:01 GMT</pubDate>
    </item>
    <item>
      <title>一种新的动态分布式规划方法：在 DPDP 问题中的应用</title>
      <link>https://arxiv.org/abs/2403.00805</link>
      <description><![CDATA[arXiv:2403.00805v1 公告类型：新
摘要：在这项工作中，我们提出了一种新的动态分布式规划方法，该方法能够考虑代理对其要计划的操作集引入的变化，以便考虑其环境中发生的变化。我们的方法适合分布式计划的分布式规划环境，其中每个代理都可以生成自己的计划。根据我们的方法，计划的生成基于使用遗传算法对约束的满足。我们的方法是，每当要计划的行动集发生变化时，每个代理都会生成一个新计划。这是为了考虑到新计划中引入的新行动。在这个新计划中，智能体每次都将旧计划中所有未执行的旧动作以及变化产生的新动作作为一个新的动作集，并作为新的初始状态；主体的动作集发生变化的状态。在我们的工作中，我们使用了一个具体案例来说明和展示我们方法的实用性。]]></description>
      <guid>https://arxiv.org/abs/2403.00805</guid>
      <pubDate>Tue, 05 Mar 2024 21:12:01 GMT</pubDate>
    </item>
    </channel>
</rss>