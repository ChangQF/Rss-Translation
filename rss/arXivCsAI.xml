<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.AI 更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 02 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>消失偏差启发式强化学习算法</title>
      <link>https://arxiv.org/abs/2306.10216</link>
      <description><![CDATA[arXiv:2306.10216v1 公告类型：交叉
摘要：强化学习在许多 Atari 游戏中取得了巨大的成功。在本文中，我们对月球着陆器环境进行了探索，并实现了包括 Q-Learning、SARSA、MC 以及平铺编码在内的经典方法。我们还实现了基于神经网络的方法，包括 DQN、Double DQN、Clipped DQN。除此之外，我们提出了一种称为启发式强化学习的新算法，它利用启发式方法指导早期训练，同时减轻引入的人为偏差。我们的实验表明我们提出的方法在月球着陆器环境中取得了有希望的结果。]]></description>
      <guid>https://arxiv.org/abs/2306.10216</guid>
      <pubDate>Thu, 02 May 2024 06:18:02 GMT</pubDate>
    </item>
    <item>
      <title>ULLER：学习和推理的统一语言</title>
      <link>https://arxiv.org/abs/2405.00532</link>
      <description><![CDATA[arXiv:2405.00532v1 公告类型：新
摘要：结合学习和推理的神经符号人工智能（NeSy）领域最近经历了显着的增长。现在有各种各样的 NeSy 框架，每个框架都有自己特定的语言来表达背景知识以及如何将其与神经网络联系起来。这种异构性阻碍了新手的可访问性，并使比较不同的 NeSy 框架变得具有挑战性。我们为 NeSy 提出了一种统一的语言，我们称之为 ULLER，一种学习和推理的统一语言。 ULLER 涵盖了各种各样的设置，同时确保其中描述的知识可以在现有的 NeSy 系统中使用。 ULLER 具有神经符号一阶语法，我们为其提供示例语义，包括经典逻辑、模糊逻辑和概率逻辑。我们相信 ULLER 是使 NeSy 研究更易于访问和比较的第一步，为图书馆简化跨多种语义、知识库和 NeSy 系统的培训和评估铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2405.00532</guid>
      <pubDate>Thu, 02 May 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>ConstrainedZero：使用学习概率故障代理和自适应安全约束进行机会约束 POMDP 规划</title>
      <link>https://arxiv.org/abs/2405.00644</link>
      <description><![CDATA[arXiv:2405.00644v1 公告类型：新
摘要：为了在不确定的环境中安全地进行规划，智能体必须平衡效用与安全约束。安全规划问题可以建模为机会受限的部分可观察马尔可夫决策过程 (CC-POMDP)，解决方案通常使用昂贵的推出或启发式方法来估计最优值和行动选择策略。这项工作引入了 ConstrainedZero 策略迭代算法，该算法通过学习最优值和策略的神经网络近似值来解决置信空间中的 CC-POMDP，并使用额外的网络头来估计给定置信的故障概率。此故障概率指导在线蒙特卡罗树搜索 (MCTS) 期间的安全操作选择。为了避免过分强调基于故障估计的搜索，我们引入了 $\Delta$-MCTS，它使用自适应共形推理来在规划期间更新故障阈值。该方法在安全关键的 POMDP 基准、飞机防撞系统以及安全 CO$_2$ 存储的可持续性问题上进行了测试。结果表明，通过将安全约束与目标分离，我们可以实现目标安全水平，而无需优化奖励和成本之间的平衡。]]></description>
      <guid>https://arxiv.org/abs/2405.00644</guid>
      <pubDate>Thu, 02 May 2024 06:18:01 GMT</pubDate>
    </item>
    <item>
      <title>蒙特卡洛树搜索通过迭代偏好学习促进推理</title>
      <link>https://arxiv.org/abs/2405.00451</link>
      <description><![CDATA[arXiv:2405.00451v1 公告类型：新
摘要：我们引入了一种旨在通过迭代偏好学习过程来增强大型语言模型 (LLM) 推理能力的方法，该方法的灵感来自 AlphaZero 采用的成功策略。我们的工作利用蒙特卡罗树搜索（MCTS）迭代收集偏好数据，利用其前瞻能力将实例级奖励分解为更细粒度的步骤级信号。为了增强中间步骤的一致性，我们将结果验证和逐步自我评估相结合，不断更新新生成数据的质量评估。所提出的算法采用直接偏好优化（DPO）来使用新生成的步骤级偏好数据来更新 LLM 策略。理论分析揭示了使用政策采样数据对于成功自我改进的至关重要性。对各种算术和常识推理任务的广泛评估表明，与现有模型相比，性能有了显着的提高。例如，我们的方法在 GSM8K、MATH 和 SciQ 上优于 Mistral-7B 监督微调 (SFT) 基线，准确率大幅提高至 $80.7\%$ (+$4.8\%$)、$32.2\%$ (分别为+$3.3\%$) 和$88.5\%$ (+$7.7\%$)。此外，我们的研究深入研究了训练和推理计算的权衡，为我们的方法如何有效地最大化性能提升提供了见解。]]></description>
      <guid>https://arxiv.org/abs/2405.00451</guid>
      <pubDate>Thu, 02 May 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>金奖：具有自然语言描述的几何问题求解器</title>
      <link>https://arxiv.org/abs/2405.00494</link>
      <description><![CDATA[arXiv:2405.00494v1 公告类型：新
摘要：解决人工智能（AI）中自动几何数学问题解决的挑战涉及理解多模态信息和数学。当前的方法难以准确解释几何图，这阻碍了有效解决问题。为了解决这个问题，我们提出了带有自然语言描述（GOLD）模型的几何问题求解器。 GOLD 通过单独处理图中的符号和几何图元来增强几何关系的提取。随后，它将提取的关系转换为自然语言描述，有效地利用大型语言模型来解决几何数学问题。实验表明，GOLD 模型在计算和证明子集方面均优于之前在 UniGeo 数据集上最好的方法 Geoformer 模型，分别实现了 12.7% 和 42.1% 的精度提升。此外，它还超越了之前在 PGPS9K 和 Geometry3K 数据集上的最佳模型 PGPSNet，准确率分别提高了 1.8% 和 3.2%。]]></description>
      <guid>https://arxiv.org/abs/2405.00494</guid>
      <pubDate>Thu, 02 May 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>CookingSense：具有多学科主张的烹饪知识库</title>
      <link>https://arxiv.org/abs/2405.00523</link>
      <description><![CDATA[arXiv:2405.00523v1 公告类型：新
摘要：本文介绍了 CookingSense，它是从各种来源（包括网络数据、科学论文和食谱）提取的烹饪领域知识断言的描述性集合，从中获取涵盖广泛方面的知识。 CookingSense是通过一系列基于字典的过滤和基于语言模型的语义过滤技术构建的，从而形成了丰富的多学科食品相关断言知识库。此外，我们还推出了 FoodBench，这是一种评估烹饪决策支持系统的新颖基准。通过 FoodBench 的评估，我们凭经验证明 CookingSense 提高了检索增强语言模型的性能。我们还通过定性分析验证 CookingSense 中断言的质量和多样性。]]></description>
      <guid>https://arxiv.org/abs/2405.00523</guid>
      <pubDate>Thu, 02 May 2024 06:18:00 GMT</pubDate>
    </item>
    <item>
      <title>基于 Transformer 的推理，用于学习时态知识图上的事件进化链</title>
      <link>https://arxiv.org/abs/2405.00352</link>
      <description><![CDATA[arXiv:2405.00352v1 公告类型：新
摘要：时态知识图（TKG）推理通常涉及沿着时间线完成缺失的事实元素。尽管现有方法可以通过整合时间信息来学习四元组中每个事实元素的良好嵌入，但它们通常无法推断时间事实的演变。这主要是因为（1）没有充分探索各个四元组内的内部结构和语义关系；（2）没有充分学习不同四元组之间上下文和时间相关性的统一表示。为了克服这些限制，我们提出了一种新颖的基于 Transformer 的推理模型（称为 ECEformer），供 TKG 学习事件进化链（ECE）。具体来说，我们按时间顺序展开实体节点的邻域子图，形成事件的进化链作为模型的输入。随后，我们利用 Transformer 编码器来学习 ECE 的四元组内嵌入。然后，我们构建了一个基于多层感知器（MLP）的混合上下文推理模块，以学习 ECE 的四元组间的统一表示，同时完成时间知识推理。此外，为了增强事件的及时性，我们设计了一个额外的时间预测任务，以在学习的统一表示内完成有效的时间信息。对六个基准数据集的大量实验验证了我们方法的最先进性能和有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.00352</guid>
      <pubDate>Thu, 02 May 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>用于时态知识图嵌入的多项式逼近的任意时间信息建模</title>
      <link>https://arxiv.org/abs/2405.00358</link>
      <description><![CDATA[arXiv:2405.00358v1 公告类型：新
摘要：与传统知识图谱（KG）不同，时序知识图谱（TKG）必须充分探索和推理随时间演变的事实。然而，现有的TKG方法仍然面临两个主要挑战，即连续建模任意时间戳的能力有限以及在时间约束下缺乏丰富的推理模式。在本文中，我们提出了一种创新的TKGE方法（PTBox），通过基于多项式分解的时间表示和基于框嵌入的实体表示来解决上述问题。具体来说，我们通过多项式分解时间信息，然后通过结合可学习的时间基础张量来增强模型灵活表示任意时间戳的能力。此外，我们将每个实体建模为超矩形框，并将每个关系定义为头尾实体框上的变换。实体框可以捕获复杂的几何结构并学习鲁棒表示，从而提高模型对丰富推理模式的归纳能力。理论上，我们的 PTBox 可以编码任意时间信息甚至看不见的时间戳，同时捕获知识库的丰富推理模式和高元数关系。在真实数据集上进行的大量实验证明了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.00358</guid>
      <pubDate>Thu, 02 May 2024 06:17:59 GMT</pubDate>
    </item>
    <item>
      <title>通过个性化和偏好聚合从异质反馈中获得有原则的 RLHF</title>
      <link>https://arxiv.org/abs/2405.00254</link>
      <description><![CDATA[arXiv:2405.00254v1 公告类型：新
摘要：人类反馈强化学习（RLHF）是一种使人工智能系统与人类价值观保持一致的有效技术，最近在微调大语言模型方面取得了显着的成功。大多数现有的 RLHF 范式都做出了基本假设，即人类偏好相对同质，并且可以通过单一奖励模型进行编码。在本文中，我们重点解决由于人类偏好固有的异质性以及他们在提供反馈方面的潜在策略行为而导致的问题。具体来说，我们提出了两种框架来以原则性的方式解决异构的人类反馈：基于个性化的框架和基于聚合的框架。对于前者，我们提出了两种分别基于表示学习和聚类的方法，用于学习多个奖励模型，权衡偏差（由于偏好异质性）和方差（由于通过个性化学习每个模型时使用的数据较少） 。然后，我们为这两种方法建立样本复杂性保证。对于后者，我们的目标是通过仔细聚合人类多样化和真实的偏好，坚持单一模型框架，正如当前 RLHF 范式中已经部署的那样。我们分别提出了两种基于奖励和偏好聚合的方法：前者利用功利主义和Leximin方法来聚合个体奖励模型，并保证样本复杂性；后者利用功利主义和Leximin方法来聚合个体奖励模型，并保证样本复杂性。后者以概率意见的形式直接聚合人类反馈。在概率意见反馈模型下，我们还开发了一种方法来处理战略人类贴标签者，他们可能会通过不真实的反馈来偏见和操纵汇总偏好。基于机制设计的思想，我们的方法确保真实的偏好报告，并通过诱导聚合规则最大化社会福利函数。]]></description>
      <guid>https://arxiv.org/abs/2405.00254</guid>
      <pubDate>Thu, 02 May 2024 06:17:58 GMT</pubDate>
    </item>
    <item>
      <title>iMTSP：用命令式学习解决最小-最大多重旅行商问题</title>
      <link>https://arxiv.org/abs/2405.00285</link>
      <description><![CDATA[arXiv:2405.00285v1 公告类型：新
摘要：本文考虑了最小-最大多重旅行推销员问题（MTSP），其目标是找到一组旅行，每个代理人一个，共同访问所有城市，同时最小化最长旅行的长度。尽管 MTSP 已被广泛研究，但由于其 NP 难度，获得大规模问题的近乎最优解决方案仍然具有挑战性。最近在数据驱动方法方面的努力面临着难以获得监督的挑战以及梯度估计的高方差问题，导致收敛缓慢和高度次优的解决方案。我们使用命令式学习 (IL) 的概念，通过将 MTSP 重新表述为双层优化问题来解决这些问题。这涉及引入一个分配网络，将 MTSP 分解为多个单代理旅行商问题 (TSP)。然后，使用这些 TSP 解决方案中的最长路径来自我监督分配网络，从而产生新的自我监督、双层、端到端学习框架，我们将其称为命令式 MTSP (iMTSP)。此外，为了解决优化过程中的高方差梯度问题，我们引入了一种基于控制变量的梯度估计算法。我们的实验表明，这些创新设计使我们的梯度估计器的收敛速度比高级强化学习基线快 20%，并且与 Google OR-Tools MTSP 求解器相比，旅行长度缩短了 80%，特别是在大规模问题（例如 1000 个城市）中和 15 名代理人）。]]></description>
      <guid>https://arxiv.org/abs/2405.00285</guid>
      <pubDate>Thu, 02 May 2024 06:17:58 GMT</pubDate>
    </item>
    <item>
      <title>接地可实现的实体</title>
      <link>https://arxiv.org/abs/2405.00197</link>
      <description><![CDATA[arXiv:2405.00197v1 公告类型：新
摘要：在过去的十年里，品质、性格和角色的本体论表征得到了完善，澄清了生命科学研究中的微妙区别。在基本形式本体论 (BFO) 的背景下阐明这些实体的广泛使用的特征之后，我们确定了这种处理中的差距，并激发了补充 BFO 特征的需要。作为补充，我们提出了品质和性格、性格和角色之间的基础关系的定义，通过代表宿主-病原体相互作用的微妙方面来说明我们的建议。]]></description>
      <guid>https://arxiv.org/abs/2405.00197</guid>
      <pubDate>Thu, 02 May 2024 06:17:57 GMT</pubDate>
    </item>
    <item>
      <title>聚合组合图神经网络的推理逻辑</title>
      <link>https://arxiv.org/abs/2405.00205</link>
      <description><![CDATA[arXiv:2405.00205v1 公告类型：新
摘要：我们提出了一种模态逻辑，其中计数模态出现在线性不等式中。我们证明每个公式都可以转化为等效的图神经网络（GNN）。我们还表明，一大类 GNN 可以有效地转化为公式，从而显着改进有关 GNN 逻辑表达能力的文献。我们还表明可满足性问题是 PSPACE 完全的。这些结果带来了使用标准逻辑方法来推理 GNN 及其属性的希望，特别是在 GNN 查询、等价性检查等应用中。我们证明这些自然问题可以在多项式空间中解决。]]></description>
      <guid>https://arxiv.org/abs/2405.00205</guid>
      <pubDate>Thu, 02 May 2024 06:17:57 GMT</pubDate>
    </item>
    <item>
      <title>能力</title>
      <link>https://arxiv.org/abs/2405.00183</link>
      <description><![CDATA[arXiv:2405.00183v1 公告类型：新
摘要：在我们的日常生活中，就像在科学和所有其他领域一样，我们会遇到大量的性格（倾向、潜力、力量），这些性格是在打喷嚏、出汗、脱落头皮屑等过程中实现的。在这些我们可以认为仅仅是倾向的众多倾向中，有一个倾向的子集，我们对这些倾向的实现感兴趣，比如汽车在冰上行驶时的良好反应，兔子被狼追赶时的肺部反应良好，等等。我们将后者称为能力，并且我们尝试提供一个强大的本体论说明，说明什么是能力，该能力具有足够的通用性以服务于各种目的，例如，通过在能力数据当前正在使用的领域中为基于本体的研究提供有用的扩展。以孤立的方式收集。]]></description>
      <guid>https://arxiv.org/abs/2405.00183</guid>
      <pubDate>Thu, 02 May 2024 06:17:56 GMT</pubDate>
    </item>
    <item>
      <title>职业本体论中的证书</title>
      <link>https://arxiv.org/abs/2405.00186</link>
      <description><![CDATA[arXiv:2405.00186v1 公告类型：新
摘要：“证书”一词包括教育证书、学位、证书和政府颁发的许可证。职业资格证书是具有相关权威的第三方颁发的对个人资格或能力的证明。求职者经常利用此类证书作为其持有者满足所需资格的证据。许多美国教育和劳动力发展组织已经认识到证书对于就业的重要性以及理解证书价值的挑战。在本研究中，我们基于职业本体（OccO）（一种基于 BFO 的本体），在文本和语义层面上识别并本体定义了凭证和凭证相关术语。对不同的凭证类型及其授权逻辑进行建模。我们还定义了与证书相关的术语以及许多术语之间的关系的高级层次结构，这些术语是与阿拉巴马州人才三合一 (ATT) 计划一起启动的，该计划旨在通过证书和证书将学习者、收入者、雇主和教育/培训提供者联系起来。技能。据我们所知，我们的研究首次提供了凭证重要领域及相关内容的系统本体论建模，支持未来增强凭证数据和知识集成。]]></description>
      <guid>https://arxiv.org/abs/2405.00186</guid>
      <pubDate>Thu, 02 May 2024 06:17:56 GMT</pubDate>
    </item>
    <item>
      <title>创意光束搜索</title>
      <link>https://arxiv.org/abs/2405.00099</link>
      <description><![CDATA[arXiv:2405.00099v1 公告类型：新
摘要：大型语言模型正在彻底改变多个领域，包括人工创造力。然而，机器的生成过程与人类观察到的生成过程截然不同。特别是，机器生成的特点是缺乏意向性和潜在的创造性过程。我们提出了一种称为 Creative Beam Search 的方法，该方法使用 Diverse Beam Search 和 LLM-as-a-Judge 来执行响应生成和响应验证。定性实验的结果表明，我们的方法如何提供比标准采样技术更好的输出。我们还表明，响应验证步骤是响应生成步骤的必要补充。]]></description>
      <guid>https://arxiv.org/abs/2405.00099</guid>
      <pubDate>Thu, 02 May 2024 06:17:55 GMT</pubDate>
    </item>
    </channel>
</rss>