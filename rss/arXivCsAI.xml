<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.AI 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.AI 对 arXiv.org 电子印刷档案进行更新。</description>
    <lastBuildDate>Wed, 12 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>形式化验证的近似策略迭代</title>
      <link>https://arxiv.org/abs/2406.07340</link>
      <description><![CDATA[arXiv:2406.07340v1 公告类型：新
摘要：我们使用交互式定理证明器 Isabelle/HOL 正式验证了分解马尔可夫决策过程的近似策略迭代算法。接下来，我们展示如何将形式化算法细化为可执行的、经过验证的实现。在基准问题上评估实现以显示其实用性。作为细化的一部分，我们开发了经过验证的软件来认证线性规划解决方案。该算法建立在多样化的形式化数学库之上，并将现有的交互式定理证明器方法推向极限。我们讨论了验证项目的过程以及形式验证所需的算法修改。]]></description>
      <guid>https://arxiv.org/abs/2406.07340</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:17 GMT</pubDate>
    </item>
    <item>
      <title>人工智能沙袋：语言模型在评估中可能战略性地表现不佳</title>
      <link>https://arxiv.org/abs/2406.07358</link>
      <description><![CDATA[arXiv:2406.07358v2 公告类型：新
摘要：值得信赖的能力评估对于确保人工智能系统的安全至关重要，并且正在成为人工智能监管的关键组成部分。然而，人工智能系统的开发者或人工智能系统本身可能有动机在评估中低估人工智能的实际能力。这些利益冲突导致了沙袋问题$\unicode{x2013}$，我们将其定义为“评估中的战略表现不佳”。在本文中，我们评估了当代语言模型 (LM) 中的沙袋能力。我们提示前沿 LM，如 GPT-4 和 Claude 3 Opus，在危险能力评估中选择性表现不佳，同时在一般（无害）能力评估中保持性能。此外，我们发现可以在合成数据集上对模型进行微调，以隐藏特定功能，除非给出密码。这种行为推广到高质量、坚持的基准，例如 WMDP。此外，我们表明，无论是前沿模型还是较小的模型，都可以被提示或密码锁定，以针对能力评估的特定分数。此外，我们发现一个功能强大的密码锁定模型（Llama 3 70b）能够合理地模拟一个功能较弱的模型（Llama 2 7b）。总体而言，我们的结果表明，能力评估容易受到沙袋攻击。这种弱点降低了评估的可信度，从而破坏了有关开发和部署高级人工智能系统的重要安全决策。]]></description>
      <guid>https://arxiv.org/abs/2406.07358</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:17 GMT</pubDate>
    </item>
    <item>
      <title>超越训练：通过自适应动作抽样优化基于强化学习的车间调度</title>
      <link>https://arxiv.org/abs/2406.07325</link>
      <description><![CDATA[arXiv:2406.07325v1 公告类型：新
摘要：近年来，用于调度问题的学习构造启发式方法与成熟的求解器和启发式方法的竞争力越来越强。特别是，使用深度强化学习 (DRL) 的解决方法取得了显着的改进。虽然人们非常关注网络架构和训练算法的设计以获得最先进的结果，但很少有研究调查在推理过程中训练有素的 DRL 代理的最佳使用。我们的工作基于这样的假设：与搜索算法类似，训练有素的 DRL 代理的利用应该取决于可接受的计算预算。我们提出了一种简单而有效的参数化，称为 $\delta$-sampling，它操纵训练有素的动作向量，使代理行为在解决方案构建期间偏向探索或利用。通过遵循这种方法，我们可以实现对搜索空间的更全面覆盖，同时仍然生成可接受数量的解决方案。此外，我们提出了一种算法，用于获得给定数量的解决方案和任何给定的训练有素的代理的最佳参数化。使用我们的推理方法扩展现有的车间调度问题训练协议的实验验证了我们的假设，并导致了生成的解决方案的预期改进。]]></description>
      <guid>https://arxiv.org/abs/2406.07325</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:16 GMT</pubDate>
    </item>
    <item>
      <title>3D-Properties：识别 DPO 中的挑战并规划前进的道路</title>
      <link>https://arxiv.org/abs/2406.07327</link>
      <description><![CDATA[arXiv:2406.07327v1 公告类型：新
摘要：将大型语言模型 (LLM) 与人类偏好对齐最近引起了极大的关注，其中典型但成本高昂的 RLHF-PPO 和简单直接的直接偏好优化 (DPO) 就是两个例子。尽管效率很高，但 DPO 很少用于最先进的生产级 LLM，这意味着它存在潜在的病态。在这项工作中，我们重新审视 DPO，全面检查其经验有效性并与 RLHF-PPO 进行系统比较。我们通过对精心设计的玩具模型和实际 LLM 进行实验，确定了 DPO 学习成果的 \textbf{3D} 属性：拒绝响应的可能性的 \textbf{D} 急剧下降、\textbf{D} 退化为 LLM 反学习，以及对未见响应的 \textbf{D} 分散效应，这些实验包括数学问题解决和指令遵循等任务。这些发现本质上与相关工作的一些观察结果相关，我们还为它们提供了一个合理的理论解释。因此，我们提出了简单的正则化方法来缓解 \textbf{3D} 属性引起的问题，从而提高 DPO 的训练稳定性和最终性能。我们的贡献还包括调查成对偏好数据的分布如何影响 DPO 的有效性。我们希望这项工作可以提供研究方向，以缩小无奖励偏好学习方法和基于奖励的偏好学习方法之间的差距。]]></description>
      <guid>https://arxiv.org/abs/2406.07327</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:16 GMT</pubDate>
    </item>
    <item>
      <title>扩展基于大型语言模型的多智能体协作</title>
      <link>https://arxiv.org/abs/2406.07155</link>
      <description><![CDATA[arXiv:2406.07155v1 公告类型：新
摘要：大型语言模型驱动的代理的开创性进步强调了多代理协作的设计模式，表明集体智慧可以超越每个个体的能力。受神经缩放定律的启发，该定律认为增加神经元会导致出现能力，本研究调查了类似的原则是否适用于多代理协作中代理的增加。从技术上讲，我们提出了多代理协作网络 (MacNet)，它利用有向无环图来组织代理并通过拓扑排序简化其交互推理，解决方案来自他们的对话。大量实验表明，MacNet 始终优于基线模型，能够在各种网络拓扑中实现有效的代理协作，并支持一千多个代理之间的合作。值得注意的是，我们观察到了一种小世界协作现象，其中类似于小世界属性的拓扑实现了卓越的性能。此外，我们确定了协作扩展定律，表明标准化解决方案质量遵循逻辑增长模式作为扩展代理，协作出现的时间比以前观察到的神经出现时间要早得多。代码和数据将在 https://github.com/OpenBMB/ChatDev 上提供。]]></description>
      <guid>https://arxiv.org/abs/2406.07155</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>DCA-Bench：数据集管理代理的基准</title>
      <link>https://arxiv.org/abs/2406.07275</link>
      <description><![CDATA[arXiv:2406.07275v1 公告类型：新
摘要：数据集的质量在现代人工智能（AI）的研究和开发中起着越来越重要的作用。尽管如今开放数据集平台激增，但数据质量问题（例如文档不足、注释不准确和道德问题）在人工智能中广泛使用的数据集中仍然很常见。此外，这些问题通常很微妙，难以通过基于规则的脚本检测到，需要数据集用户或维护者进行昂贵的手动识别和验证。随着大型语言模型（LLM）功能的不断增强，使用 LLM 代理简化数据集的管理有望成为可能。在这项工作中，作为实现这一目标的第一步，我们提出了一个数据集管理代理基准 DCA-Bench，以衡量 LLM 代理检测隐藏数据集质量问题的能力。具体来说，我们从八个开放数据集平台收集了各种现实世界的数据集质量问题作为测试平台。此外，为了建立评估 LLM 代理成功与否的自动管道（这需要对代理输出有细致的理解），我们使用另一个 LLM 代理实现了专用评估器。我们证明基于 LLM 的评估器在经验上与人工评估非常吻合，可以在所提出的基准上进行可靠的自动评估。我们进一步在所提出的基准上对几个基线 LLM 代理进行了实验，并展示了任务的复杂性，这表明将 LLM 应用于现实世界的数据集管理仍然需要进一步深入探索和创新。最后，所提出的基准还可以作为衡量 LLM 在问题发现而不仅仅是解决问题方面的能力的试验台。基准套件可在 \url{https://github.com/TRAIS-Lab/dca-bench} 获得。]]></description>
      <guid>https://arxiv.org/abs/2406.07275</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>挖掘概念模型中的频繁结构</title>
      <link>https://arxiv.org/abs/2406.07129</link>
      <description><![CDATA[arXiv:2406.07129v1 公告类型：新
摘要：使用结构化方法表示知识的问题在概念建模中是众所周知的，并且已经研究了很多年。事实证明，采用建模模式是一种有效的结构方法。模式确实是可推广的递归结构，可以用作设计问题的解决方案。它们有助于理解和改进创建模型的过程。在概念建模中使用模式的不可否认的价值已在几项实验研究中得到证明。然而，在概念模型中发现模式被广泛认为是一项高度复杂的任务，目前缺乏对模式识别的系统解决方案。在本文中，我们提出了一种通用方法来发现概念建模语言中出现的频繁结构。作为我们科学贡献的概念证明，我们通过关注 UML 类图，特别是 OntoUML 模型，提供了该方法的实现。该实现包括一个探索性工具，通过结合频繁子图挖掘算法和图形操作技术，可以处理多个概念模型并根据多个标准发现递归结构。主要目标是为语言工程师提供支持设施。这可以用于利用好的和坏的建模实践，发展和维护概念建模语言，并促进在使用给定语言设计更好的模型时重复使用编码经验。]]></description>
      <guid>https://arxiv.org/abs/2406.07129</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>使用未标记数据增强离线强化学习</title>
      <link>https://arxiv.org/abs/2406.07117</link>
      <description><![CDATA[arXiv:2406.07117v1 公告类型：新
摘要：离线强化学习 (Offline RL) 的最新进展导致人们更加关注基于保守策略更新的方法来解决分布外 (OOD) 问题。这些方法通常涉及添加行为正则化或修改批评学习目标，主要关注具有大量数据集支持的状态或动作。然而，我们通过断言数据集中缺少动作或状态并不一定意味着其次优性来挑战这一流行观念。在本文中，我们提出了一种解决 OOD 问题的新方法。我们引入了一个离线 RL 师生框架，并辅以策略相似性度量。该框架使学生政策不仅可以从离线 RL 数据集中获得见解，还可以从教师政策传递的知识中获得见解。教师政策使用另一个由状态-动作对组成的数据集进行训练，这些数据集可以被视为无需与环境直接交互即可获得的实际领域知识。我们相信这些额外的知识是有效解决 OOD 问题的关键。这项研究代表了将师生网络融入演员-评论家框架的重大进步，为离线 RL 中的知识转移研究开辟了新途径，并有效地解决了 OOD 挑战。]]></description>
      <guid>https://arxiv.org/abs/2406.07117</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>CHARME：一种基于链的强化学习方法，用于解决小嵌入问题</title>
      <link>https://arxiv.org/abs/2406.07124</link>
      <description><![CDATA[arXiv:2406.07124v1 公告类型：新
摘要：量子退火 (QA) 在有效解决组合优化问题方面具有巨大潜力。然而，QA 算法的有效性在很大程度上依赖于将问题实例（表示为逻辑图）嵌入到量子单元处理 (QPU) 中，其拓扑形式为有限连通图，称为次要嵌入问题。现有的次要嵌入问题方法在面对更大的问题规模时会遇到可扩展性问题。在本文中，我们提出了一种利用强化学习 (RL) 技术解决次要嵌入问题的新方法，称为 CHARME。CHARME 包括三个关键组件：用于策略建模的图神经网络 (GNN) 架构、确保解决方案有效性的状态转换算法和用于有效训练的顺序探索策略。通过对合成和真实世界实例的全面实验，我们证明了我们提出的顺序探索策略以及我们提出的 RL 框架 CHARME 的效率。具体来说，与 Minorminer 和 ATOM 等快速嵌入方法相比，CHARME 可产生更优的解决方案。此外，我们的方法在某些情况下优于基于 OCT 的方法，后者以运行时间较慢但解决方案质量较高而闻名。此外，我们提出的探索通过提供比贪婪策略更好的解决方案来提高 CHARME 框架的训练效率。]]></description>
      <guid>https://arxiv.org/abs/2406.07124</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>CAAP：情境感知行动规划，仅使用前端 UI 即可解决计算机任务</title>
      <link>https://arxiv.org/abs/2406.06947</link>
      <description><![CDATA[arXiv:2406.06947v1 公告类型：新
摘要：软件机器人早已被部署在机器人过程自动化 (RPA) 中，以自动执行平凡而重复的计算机任务。具有高级推理能力的大型语言模型 (LLM) 的出现为这些代理现在可以执行更复杂甚至以前从未见过的任务奠定了基础。然而，最近文献中基于 LLM 的自动化技术经常依赖 HTML 源代码进行输入，将其应用限制在 Web 环境中。此外，HTML 代码中包含的信息通常不准确或不完整，使得代理在实际应用中不太可靠。我们提出了一种基于 LLM 的代理，它仅基于屏幕截图来识别环境，同时利用上下文学习来消除收集大量人类演示数据集的需要。我们的策略称为上下文感知行动计划 (CAAP) 提示，鼓励代理从各个角度仔细审查上下文。通过我们提出的方法，我们在 67 种 MiniWoB++ 问题上实现了 94.4% 的成功率，每种问题类型仅使用 1.48 次演示。我们的方法为更广泛的应用提供了潜力，尤其是对于需要在计算机或智能手机上进行应用程序间协调的任务，展示了自动化代理领域的重大进步。代码和模型可在 https://github.com/caap-agent/caap-agent 上访问。]]></description>
      <guid>https://arxiv.org/abs/2406.06947</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:12 GMT</pubDate>
    </item>
    <item>
      <title>利用上下文感知查询表示学习改进知识图谱中的多跳逻辑推理</title>
      <link>https://arxiv.org/abs/2406.07034</link>
      <description><![CDATA[arXiv:2406.07034v1 公告类型：新
摘要：知识图谱上的多跳逻辑推理是自然语言处理中的一项关键任务，有许多方法旨在回答一阶逻辑 (FOL) 查询。最近的基于几何（例如，盒子、锥体）和概率（例如，beta 分布）的方法已经有效地解决了复杂的 FOL 查询。然而，这些方法面临的一个共同挑战在于确定这些查询的准确几何边界或概率参数。挑战的出现是因为现有方法依赖于其计算图中的线性顺序操作，忽略了查询的逻辑结构和可以从查询关系中收集的关系诱导信息，我们称之为查询的上下文。为了解决这个问题，我们提出了一种与模型无关的方法，通过完全集成 FOL 查询图的上下文来提高现有多跳逻辑推理方法的有效性。我们的方法可以清晰地辨别 (1) 查询结构固有的结构上下文和 (2) 查询图中每个节点独有的关系诱导上下文，如相应的知识图谱所示。这种双重上下文范式可帮助查询图中的节点在整个多跳推理步骤中获得精细的内部表示。通过对两个数据集的实验，我们的方法持续增强了三个多跳推理基础模型，实现了高达 19.5% 的性能提升。我们的代码可在 https://github.com/kjh9503/caqr 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.07034</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:12 GMT</pubDate>
    </item>
    <item>
      <title>嵌入物中有什么？嵌入任何植物的玫瑰都会散发出同样芬芳的香气吗？</title>
      <link>https://arxiv.org/abs/2406.06870</link>
      <description><![CDATA[arXiv:2406.06870v1 公告类型：新
摘要：大型语言模型 (LLM) 经常被批评为缺乏真正的“理解”和用知识“推理”的能力，仅仅被视为高级自动完成系统。我们认为这种观点可能缺少一个重要的见解。我们认为 LLM 确实发展了一种类似于“几何”的经验“理解”，这似乎足以满足 NLP、计算机视觉、编码辅助等一系列应用的需求。然而，这种基于不完整和嘈杂数据的“几何”理解使它们不可靠、难以概括，并且缺乏推理能力和解释，类似于几十年前基于启发式的专家系统所面临的挑战。
为了克服这些限制，我们建议 LLM 应该与知识的“代数”表示相结合，其中包括专家系统中使用的符号 AI 元素。这种整合旨在创建大型知识模型 (LKM)，这些模型不仅拥有基于第一原理的“深度”知识，而且还具有推理和解释能力，模仿人类专家的能力。为了安全有效地利用生成式人工智能的全部潜力，需要从 LLM 转向更全面的 LKM。]]></description>
      <guid>https://arxiv.org/abs/2406.06870</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:11 GMT</pubDate>
    </item>
    <item>
      <title>联合演示和偏好学习提高了策略与人类反馈的一致性</title>
      <link>https://arxiv.org/abs/2406.06874</link>
      <description><![CDATA[arXiv:2406.06874v1 公告类型：新
摘要：协调人类偏好和价值观是构建当代基础模型和具身人工智能的重要要求。然而，诸如带人类反馈的强化学习 (RLHF) 等流行方法将任务分解为连续阶段，例如监督微调 (SFT)、奖励建模 (RM) 和强化学习 (RL)，每个阶段执行一项特定的学习任务。这种顺序方法会导致严重问题，例如数据利用率严重不足以及学习到的奖励模型与生成的策略之间的分布不匹配，最终导致对齐性能不佳。我们开发了一种名为集成人类反馈对齐 (AIHF) 的单阶段方法，能够整合人类偏好和演示来训练奖励模型和策略。所提出的方法采用了一套高效的算法，可以轻松简化并利用流行的对齐算法，例如 RLHF 和直接策略优化 (DPO)，并且只需要对现有的对齐管道进行微小的更改。我们通过大量实验证明了所提解决方案的有效性，这些实验涉及 LLM 中的对齐问题和 MuJoCo 中的机器人控制问题。我们观察到，所提解决方案的性能远胜于现有的对齐算法（如 RLHF 和 DPO），尤其是在高质量偏好数据量相对有限的情况下。]]></description>
      <guid>https://arxiv.org/abs/2406.06874</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:11 GMT</pubDate>
    </item>
    <item>
      <title>DISCOVERYWORLD：开发和评估自动化科学发现代理的虚拟环境</title>
      <link>https://arxiv.org/abs/2406.06769</link>
      <description><![CDATA[arXiv:2406.06769v1 公告类型：新
摘要：自动化科学发现有望加速各个科学领域的进步。然而，开发和评估人工智能代理的端到端科学推理能力具有挑战性，因为运行现实世界的实验通常成本过高或不可行。在这项工作中，我们引入了 DISCOVERYWORLD，这是第一个用于开发和基准测试代理执行完整新颖科学发现周期的能力的虚拟环境。DISCOVERYWORLD 包含各种不同的挑战，涵盖放射性同位素测年、火箭科学和蛋白质组学等多种主题，以鼓励开发一般的发现技能而不是特定于任务的解决方案。DISCOVERYWORLD 本身是一个廉价的、模拟的、基于文本的环境（带有可选的 2D 视觉叠加）。它包括 120 个不同的挑战任务，涵盖八个主题，每个主题有三个难度级别和几个参数变化。每个任务都需要一个代理形成假设、设计和运行实验、分析结果并根据结论采取行动。 DISCOVERYWORLD 还提供了三个自动指标来评估绩效，基于 (a) 任务完成情况、(b) 采取的与任务相关的行动以及 (c) 发现的解释性知识。我们发现，在之前发布的环境中表现良好的强大基线代理在大多数 DISCOVERYWORLD 任务中都表现不佳，这表明 DISCOVERYWORLD 抓住了一些发现的新挑战，因此 DISCOVERYWORLD 可能有助于加速代理的近期发展和科学发现能力评估。代码可在以下网址获取：www.github.com/allenai/discoveryworld]]></description>
      <guid>https://arxiv.org/abs/2406.06769</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:10 GMT</pubDate>
    </item>
    <item>
      <title>目测组合问题：使用多模态大型语言模型解决旅行商问题的案例研究</title>
      <link>https://arxiv.org/abs/2406.06865</link>
      <description><![CDATA[arXiv:2406.06865v1 公告类型：新
摘要：多模态大型语言模型 (MLLM) 已证明能够熟练处理多种模态，包括文本、图像和音频。这些模型利用大量预先存在的知识，使它们能够用最少甚至没有特定的训练示例来解决复杂问题，这一点在少数样本和零样本上下文学习场景中得到了证明。本文通过分析二维平面上的点分布图像，研究了如何使用 MLLM 的视觉能力来“目测”旅行商问题 (TSP) 的解决方案。我们的实验旨在验证 MLLM 可以有效“目测”可行的 TSP 路线的假设。零样本、少数样本、自集成和自优化零样本评估的结果显示出令人鼓舞的结果。我们预计这些发现将激发进一步探索 MLLM 的视觉推理能力，以解决其他组合问题。]]></description>
      <guid>https://arxiv.org/abs/2406.06865</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:10 GMT</pubDate>
    </item>
    </channel>
</rss>