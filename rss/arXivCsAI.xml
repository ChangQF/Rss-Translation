<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.AI 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 人工智能 (cs.AI) 更新 arXiv.org 电子打印档案</description>
    <lastBuildDate>Tue, 02 Jan 2024 15:14:12 GMT</lastBuildDate>
    <item>
      <title>用于深度强化学习的基于对比学习的代理建模。 （arXiv：2401.00132v1 [cs.MA]）</title>
      <link>http://arxiv.org/abs/2401.00132</link>
      <description><![CDATA[多代理系统通常需要代理进行协作或竞争
与具有不同目标、行为或策略的其他代理进行对抗。代理人
为智能机器设计自适应策略时，建模至关重要
多代理系统中的代理，因为这是自我代理的手段
了解其他智能体的行为并提取他们有意义的策略
交涉。这些表征可以用来增强自我代理的
通过强化学习训练的自适应策略。然而，现有的
代理建模方法通常假设本地的可用性
在训练期间或长时间观察其他智能体（建模智能体）的观察结果
政策适应的观察轨迹。为了消除这些限制
假设并提高代理建模性能，我们设计了一个对比
仅依赖于局部的基于学习的代理建模（CLAM）方法
自我代理在训练和执行期间的观察。用这些
根据观察，CLAM 能够制定一致的高质量政策
从每集一开始就实时呈现。我们
评估了我们的方法在合作和竞争方面的有效性
多代理环境。我们的实验表明我们的方法
在合作和竞争任务上均达到最先进的水平，
强调基于对比学习的代理建模的潜力
加强强化学习。
]]></description>
      <guid>http://arxiv.org/abs/2401.00132</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:12 GMT</pubDate>
    </item>
    <item>
      <title>具有感知损失的扩散模型。 （arXiv：2401.00110v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.00110</link>
      <description><![CDATA[用均方误差损失训练的扩散模型往往会生成
不切实际的样本。当前最先进的模型依赖于无分类器
提高样本质量的指导，但其令人惊讶的有效性却并非如此
完全明白了。在本文中，我们证明了
无分类器指导部分源于它是一种隐式形式
感性引导。因此，我们可以直接将感知损失纳入
进行扩散训练以提高样本质量。由于比分匹配
扩散训练中使用的目标与去噪非常相似
用于感知网络无监督训练的自动编码器目标，
扩散模型本身是一个感知网络，可以用来生成
有意义的知觉损失。我们提出了一个新颖的自我感知目标
结果是扩散模型能够生成更真实的样本。为了
条件生成，我们的方法只提高了样本质量，而没有
与条件输入纠缠，因此不会牺牲样本
多样性。我们的方法还可以提高无条件样本的质量
生成，这在以前的无分类器指导下是不可能的。
]]></description>
      <guid>http://arxiv.org/abs/2401.00110</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:11 GMT</pubDate>
    </item>
    <item>
      <title>LLM-Assist：通过基于语言的推理增强闭环规划。 （arXiv：2401.00125v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00125</link>
      <description><![CDATA[虽然规划是自动驾驶堆栈的重要组成部分，
研究人员尚未开发出能够
安全地处理各种可能的驾驶场景。学习为本
规划者遭受过度拟合和长尾性能不佳的困扰。在另一
另一方面，基于规则的规划器概括得很好，但可能无法处理场景
需要复杂的驾驶操作。为了解决这些限制，我们
研究利用常识推理的可能性
GPT4 和 Llama2 等大型语言模型 (LLM) 的功能
制定自动驾驶车辆计划。我们特别开发了一部小说
结合利用传统的基于规则的规划器的混合规划器
与基于法学硕士的规划师。以法学硕士常识推理能力为指导，
我们的方法可以应对现有规划者难以应对的复杂场景，
产生合理的输出，同时通过工作保持脚踏实地
与基于规则的方法一起。通过对 nuPlan 的广泛评估
基准，我们实现了最先进的性能，超越了所有现有的
涵盖大多数指标的纯粹基于学习和规则的方法。我们的代码将是
可以在 https://llmassist.github.io 上找到。
]]></description>
      <guid>http://arxiv.org/abs/2401.00125</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:11 GMT</pubDate>
    </item>
    <item>
      <title>组织有效性的语义计算：通过基于语义的建模从组织理论到实践。 （arXiv：2401.00062v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00062</link>
      <description><![CDATA[组织的一个关键职能是促进整合水平
（协调与合作）为实现其目标所必需的。需要
无数的依赖产生了协调和合作的动力
组织成员及其工作之间的关系。因此，要推理
协调与合作问题的解决需要强有力的
包括底层依赖关系的表示。我们发现这样一个
正式的组织模式中仍然缺少代表性，我们
利用语义来弥补这一差距。借鉴成熟的
组织研究以及我们与北美之一的广泛实地考察
最大的城市，（1）我们引入一个本体，以一阶形式化
逻辑，可操作结果、奖励和认知等概念
依赖性及其与潜在整合风险的联系； (2) 出席
该本体论的实际应用来分析和支持集成
复杂的政府基础设施项目。我们的本体已经实现并且
在 Z3 和 OWL 中均得到验证。我们模型的主要特征包括可推断的
依赖性、可解释的协调和合作风险以及可操作的
关于如何改变组织内的依赖性结构的见解
减轻风险。概念化现实世界的挑战，例如激励
依赖方面的错位、搭便车和子目标优化
结构，我们基于语义的方法代表了一种新颖的方法
示范并加强协调与合作。集成在一个
决策支持系统，我们的模型可以作为有影响力的帮助
组织设计和有效性。更广泛地说，我们的方法强调
语义在推导有形的现实世界方面的变革潜力
现有组织理论的价值。
]]></description>
      <guid>http://arxiv.org/abs/2401.00062</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:10 GMT</pubDate>
    </item>
    <item>
      <title>用于可解释强化学习的因果状态蒸馏。 （arXiv：2401.00104v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.00104</link>
      <description><![CDATA[强化学习 (RL) 是训练智能的强大技术
代理，但了解这些代理为何做出特定决定可能会非常困难
具有挑战性的。强化学习模型缺乏透明度是一个长期存在的问题
问题，导致用户很难掌握代理商的原因
行为。人们已经探索了各种方法来解决这个问题，其中
一种有希望的途径是奖励分解（RD）。 RD 很有吸引力，因为它
回避了与其他方法相关的一些问题，这些方法试图
以事后方式合理化代理人的行为。 RD 通过暴露来发挥作用
奖励的各个方面有助于代理的目标
训练。然而，单独的 RD 也有其局限性，因为它主要提供见解
基于子奖励，不深究错综复杂的因果关系
RL 智能体的神经模型中发生的关系。在本文中，我们
提出 RD 的扩展，超越子奖励，提供更多
信息丰富的解释。我们的方法以因果学习为中心
利用信息论措施进行解释的框架
鼓励因果因素的三个关键属性的目标：
\emph{因果充分性}、\emph{稀疏性}和\emph{正交性}。这些
属性帮助我们提炼出事物之间的因果关系
代理的状态和行为或奖励，可以更深入地了解
其决策过程。我们的框架旨在生成本地
解释并可应用于广泛的 RL 任务
奖励渠道。通过一系列的实验，我们证明了我们的
方法为代理人的行为提供了更有意义和更有洞察力的解释
动作选择。
]]></description>
      <guid>http://arxiv.org/abs/2401.00104</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:10 GMT</pubDate>
    </item>
    <item>
      <title>混合建模设计模式。 （arXiv：2401.00033v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00033</link>
      <description><![CDATA[设计模式提供了一种系统的方法来向重复出现的问题传达解决方案
建模挑战。本文介绍了混合建模的设计模式，
一种将基于第一原理的建模与数据驱动相结合的方法
建模技术。虽然两种方法都有互补的优势
通常有多种方法将它们组合成混合模型，并且
适当的解决方案将取决于当前的问题。在本文中，我们
提供四种基本模式，可以作为组合的蓝图
将具有领域知识的数据驱动组件融入混合方法中。在
此外，我们还提出了两种控制组合的组合模式
将基本模式转化为更复杂的混合模型。每个设计模式都是
通过气候等应用领域的典型用例进行说明
建模、工程和物理。
]]></description>
      <guid>http://arxiv.org/abs/2401.00033</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:09 GMT</pubDate>
    </item>
    <item>
      <title>通过预期分区函数和持续优化进行信使和非编码 RNA 设计。 (arXiv:2401.00037v1 [q-bio.BM])</title>
      <link>http://arxiv.org/abs/2401.00037</link>
      <description><![CDATA[设计信使 RNA 和非编码 RNA 的任务是不同的
优化问题，并且这些问题的几个版本都是 NP 困难的。作为
作为常用本地搜索方法的替代方法，我们制定了这些
持续优化的问题并为此制定通用框架
基于“预期配分函数”新概念的优化。基础的
想法是从所有可能的候选序列的分布开始，并且
将目标函数从序列扩展到分布。然后我们使用
基于梯度下降的优化方法来改进扩展目标
函数，并且分布将逐渐收缩为one-hot序列
（即单个序列）。我们在此考虑两个重要的案例研究
框架，优化配分函数的 mRNA 设计问题（即
集合自由能）和非编码RNA设计问题优化
条件（即玻尔兹曼）概率。在这两种情况下，我们的方法
展示有希望的初步结果。我们的代码位于
https://github.com/KuNyaa/RNA_Design_codebase。
]]></description>
      <guid>http://arxiv.org/abs/2401.00037</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:09 GMT</pubDate>
    </item>
    <item>
      <title>决策基础模型的自监督预训练：公式、流程和挑战。 （arXiv：2401.00031v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.00031</link>
      <description><![CDATA[决策是一个动态过程，需要感知、记忆和
做出选择并找到最佳政策的推理。传统方法
决策受到样本效率和泛化的影响，而
大规模的自监督预训练实现了快速适应
语言和视觉方面的微调或少量学习。因此我们主张
整合从通用大规模自我监督中获得的知识
预训练下游决策问题。我们建议
Pretrain-Then-Adapt 管道并调查最近的数据收集工作，
预训练目标和决策适应策略
预训练和下游推理。最后，我们确定了关键挑战
以及借助以下方法开发决策基础模型的未来方向
通用且灵活的自我监督预训练。
]]></description>
      <guid>http://arxiv.org/abs/2401.00031</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:08 GMT</pubDate>
    </item>
    <item>
      <title>不平衡解决机制中基于分布式强化学习的能源套利策略。 （arXiv：2401.00015v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.00015</link>
      <description><![CDATA[可再生能源渗透率的增长使得供应量增加
不确定性，导致系统失衡加剧。这种趋势，
与单一不平衡定价一起，打开了平衡的机会
责任方（BRP）在失衡中进行能源套利
结算机制。为此，我们提出了一种基于电池控制框架
关于分布式强化学习（DRL）。我们提出的控制框架
采取风险敏感的视角，允许 BRP 调整风险
偏好：我们的目标是优化套利利润和
风险措施，同时限制电池的每日循环次数。我们
使用比利时评估我们提出的控制框架的性能
2022 年的不平衡价格并比较两种最先进的 RL 方法，深度 Q
学习和软演员评论家。结果表明，分布软
演员批评家方法可以胜过其他方法。此外，我们注意到我们的
完全规避风险的代理人适当地学习对冲相关风险
仅当代理时才通过对电池充电（放电）来达到未知的不平衡价格
对价格更加确定。
]]></description>
      <guid>http://arxiv.org/abs/2401.00015</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:07 GMT</pubDate>
    </item>
    <item>
      <title>人工智能驱动的天然药材系统命名和智能知识获取平台。 （arXiv：2401.00020v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00020</link>
      <description><![CDATA[天然药材（NMM）在全球临床上有着悠久的历史
应用程序，并附有大量信息记录。尽管他们的
对医疗保健产生重大影响，该领域面临重大挑战：
由于历史的复杂性和 NMM 知识的非标准化
导致更广泛的应用受到限制。为了解决这个问题，我们引入了一个
以 AI 驱动的 ShennongAlpha 为基础的 NMM 系统命名法
专为智能知识获取而设计的平台。这个命名法
系统能够精确识别和区分 NMM。
神农阿尔法，标准化双语编目万余个NMM
信息，增强知识管理和应用能力，
从而克服传统障碍。此外，它还开创了人工智能赋能的先河
会话知识获取和标准化机器翻译。
这些协同创新标志着集成领域的第一个重大进展
特定领域的 NMM 知识与 AI 相结合，推动研究和应用
跨越 NMM 和 AI 领域，同时开创了
这个关键领域。
]]></description>
      <guid>http://arxiv.org/abs/2401.00020</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:07 GMT</pubDate>
    </item>
    <item>
      <title>使用贝叶斯信息增益对认知情绪的唤醒潜力进行建模：自由能波动驱动的询问周期。 （arXiv：2401.00007v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00007</link>
      <description><![CDATA[认知情绪，例如好奇心和兴趣，推动探究
过程。这项研究提出了一种认知情感的新颖表述，例如
好奇心和兴趣使用两种类型的信息增益产生
自由能最小化原理：Kullback-Leibler divergence(KLD) from
贝叶斯后验先验，表示自由能减少
识别和贝叶斯惊喜 (BS)，代表预期
通过贝叶斯先验更新获得信息。通过应用高斯生成式
具有附加均匀似然的模型，我们发现 KLD 和 BS 形成
意外的上凸函数（最小化自由能和预测
误差），类似于 Berlyne 的唤醒电位函数或 Wundt 曲线。
我们认为 BS 和 KLD 的交替最大化会产生理想的结果
询问周期以接近最佳唤醒水平，并且波动
惊喜，以及促进循环的好奇心和兴趣驱动力
过程。我们详尽地分析了预测不确定性的影响（之前
方差）和峰值上的观测不确定性（似然方差）
信息增益函数作为最佳惊喜。结果表明，更大
预测的不确定性，意味着开放的态度，更少的观察
不确定性，意味着注意精确观察，预计
通过更大范围的探索提供更多的信息增益。这
提出的数学框架统一了大脑的自由能原理
以及唤醒电位理论来解释冯特曲线作为信息
增益函数并提出了由认知驱动的理想探究过程
情绪。
]]></description>
      <guid>http://arxiv.org/abs/2401.00007</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:06 GMT</pubDate>
    </item>
    <item>
      <title>图灵测试，一个美丽的思想实验。 （arXiv：2401.00009v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00009</link>
      <description><![CDATA[随着大型语言模型的出现，相关主张再度兴起
以及关于图灵测试及其对人工智能的价值的问题，这些都让人想起
数十年的实际“图灵”测试。如果人工智能是量子物理学，那么现在
几只“薛定格”的猫可能会被杀死。迟到总比不做好，
是时候对图灵的美好思想进行历史性的重构了
实验。在本文中，我提供了大量证据，包括新的证据
档案来源，对几个悬而未决的问题给出原始答案
图灵 1950 年的论文，并解决了图灵的价值的核心问题
测试。
]]></description>
      <guid>http://arxiv.org/abs/2401.00009</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:06 GMT</pubDate>
    </item>
    <item>
      <title>意识作为现实的逻辑一致和预测模型。 （arXiv：2401.00005v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00005</link>
      <description><![CDATA[这项工作表明大脑可能反映外部世界的因果关系
逻辑上一致的预测模型形式的关系
现实，表现为意识。本文分析并解决了
统计模糊性问题并提供因果关系的正式模型
关系作为概率最大特定规则。我们假设大脑
从因果关系中做出所有可能的推论。我们证明
建议的形式模型具有明确推论的属性：来自
一致的前提我们推断出一致的结论。它使一组所有
推断以形成感知世界的一致模型。因果关系
关系可以创建循环互预测属性的固定点。
我们考虑约翰·圣米尔 (John St. Mill) 提出的“自然”分类，
证明物体属性的各种固定点形成一个
外部世界的“自然”分类。然后我们考虑以下概念
埃莉诺提出的“自然”类别和类别的因果模型
Rosch 和 Bob Rehder 证明了因果关系的固定点
我们所感知的对象属性之间的关系将这些概念形式化。如果
“自然”分类描述了外部世界的对象，并且
“自然”概念是对这些物体的感知，然后是理论
综合信息，由 G. Tononi 提出，描述了信息
大脑形成“自然”概念的过程反映了
“自然”分类。我们认为综合信息提供了高
物体识别的准确性。提供基于计算机的实验
该图说明了编码数字的定点形成。
]]></description>
      <guid>http://arxiv.org/abs/2401.00005</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:05 GMT</pubDate>
    </item>
    <item>
      <title>通过语言策略双向适应构建开放式具体代理。 （arXiv：2401.00006v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00006</link>
      <description><![CDATA[构建开放式学习代理涉及预训练方面的挑战
语言模型（LLM）和强化学习（RL）方法。 LLM 的挣扎
具有特定于上下文的实时交互，而 RL 方法面临效率
有待探索的问题。为此，我们提出了 OpenContra，一个联合训练
LLM 和 GRL 合作构建开放式代理的框架
理解任意的人类指令。该实现包括两个
阶段：(1) 微调法学硕士，将人类指令转化为结构化指令
目标和课程培训以目标为条件的 RL 政策来执行
任意目标； (2) 协同训练使LLM和RL政策学习
适应每一个，实现教学空间的开放性。我们进行
Contra 的实验，一款有着复杂而宏大目标的大逃杀 FPS 游戏
空间。结果表明，使用 OpenContra 训练的智能体能够理解
任意的人类指令并以高完成率完成目标，
这证明 OpenContra 可能是第一个实用的解决方案
构建开放式的具体代理。
]]></description>
      <guid>http://arxiv.org/abs/2401.00006</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:05 GMT</pubDate>
    </item>
    <item>
      <title>信息非还原论意识理论，提供现实预测的最大准确性。 （arXiv：2401.00004v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2401.00004</link>
      <description><![CDATA[本文考虑了一种非还原论的意识理论，它不是
可还原为现实理论以及生理或心理理论
理论。遵循 D.I.Dubrovsky 的“信息方法”到“心脑”
问题”，我们通过信息的棱镜来考虑现实
观察到的现象，反过来又被主观现实所感知
感觉、知觉、感受等，而这些又是关于
相应的大脑过程。在此框架内，以下
意识信息论（ITS）发展的原则被提出
前进：大脑发现外部所有可能的因果关系
世界并通过它们做出所有可能的推论。论文显示，ITS 构建了
在此原则上：（1）同样基于结构的信息规律
外部世界； (2) 解释大脑的结构和功能
功能系统和细胞群； (3) 确保最大精度
预测和对现实的预期； （四）解决新兴问题
矛盾，（5）是大脑反映的信息论
现实。
]]></description>
      <guid>http://arxiv.org/abs/2401.00004</guid>
      <pubDate>Tue, 02 Jan 2024 15:14:04 GMT</pubDate>
    </item>
    </channel>
</rss>