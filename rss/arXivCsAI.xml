<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Mon, 17 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>启用区块链的移动体现的AI网络的高效且值得信赖的区块传播：一种图形再融合方法</title>
      <link>https://arxiv.org/abs/2502.09624</link>
      <description><![CDATA[ARXIV：2502.09624V1公告类型：新 
摘要：通过协同整合移动网络和体现的人工智能（AI），移动体现的AI网络（平均值）代表了一个高级范式，可在动态环境中促进自主，上下文感知和互动行为。然而，平均值的快速发展伴随着可信赖和运营效率方面的挑战。幸运的是，区块链技术具有分散且不变的特征，为平均值提供了有希望的解决方案。但是，现有的块传播机制遇到了诸如低传播效率和阻止传播的安全性弱的挑战，这导致车辆信息的传播延迟或易受恶意篡改的脆弱性，并可能导致具有区块链的平均值的严重交通事故。此外，当前的块传播策略无法有效地适应平均动态拓扑的实时变化。因此，在本文中，我们提出了一个基于图形再输注模型的基于启用区块链的平均值的基于图形的可信赖块传播优化框架。具体而言，我们根据信任云模型提出了一种创新的信任计算机制，该机制在矿工信任评估中全面说明了随机性和模糊性。此外，通过利用图形神经网络和扩散模型的优势，我们开发了一个图形再灌注模型，以有效和适应地生成最佳的块传播轨迹。仿真结果表明，所提出的模型在块传播效率和可信度方面优于其他路由机制。此外，结果突出了其对动态环境的强大适应性，使其特别适合快速变化的平均值。]]></description>
      <guid>https://arxiv.org/abs/2502.09624</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>IMIT差异：具有双分辨率融合的语义引导扩散变压器用于模仿学习</title>
      <link>https://arxiv.org/abs/2502.09649</link>
      <description><![CDATA[ARXIV：2502.09649V1公告类型：新 
摘要：Visuomotor模仿学习使体现的代理能够从视频演示和机器人本体感受中获得有效的操纵技巧。但是，随着场景的复杂性和视觉干扰的增加，现有在简单场景中表现良好的方法往往会降低性能。为了应对这一挑战，我们介绍了IMIT DIFF，这是一种具有双分辨率融合的半横向引导扩散变压器，用于模仿学习。我们的方法利用视觉语言基础模型的先验知识将高级语义教学转化为像素级的视觉定位。该信息明确集成到由双分辨率编码器构建的多尺度视觉增强框架中。此外，我们在扩散变压器体系结构中介绍了一致性策略的实现，以提高体现代理控制中的实时性能和运动平滑度。我们评估了几个具有挑战性的现实世界任务的IMIT差异。由于其面向任务的视觉定位和细粒度的感知，它极大地胜过最先进的方法，尤其是在具有视觉分散注意力的复杂场景中，包括零拍实验的重点是视觉分散分散注意力和类别概括。该代码将公开可用。]]></description>
      <guid>https://arxiv.org/abs/2502.09649</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>MUDOC：互动的多模式的互动文档接地AI系统</title>
      <link>https://arxiv.org/abs/2502.09843</link>
      <description><![CDATA[ARXIV：2502.09843V1公告类型：新 
摘要：多模式AI是建立有效工具以利用人类交流中多种方式的重要一步。构建一个多模式文档的AI系统与长文档进行交互是一个挑战。我们的工作旨在填补直接利用文档中的接地视觉效果以及文档中的文本内容进行响应生成的研究空白。我们提出了基于GPT-4O的交互式对话AI代理“ mudoc”，以通过交织的文本和数字生成文档接地的响应。 Mudoc的智能教科书接口促进了信任度，并通过允许即时导航来源文本和文档中的数字来验证系统响应。我们还基于MUDOC响应强调其优势和局限性讨论定性观察。]]></description>
      <guid>https://arxiv.org/abs/2502.09843</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>可解释AI的得分表</title>
      <link>https://arxiv.org/abs/2502.09861</link>
      <description><![CDATA[ARXIV：2502.09861V1公告类型：新 
摘要：解释性对于自主和智能系统的透明度很重要，并有助于支持适当的信任水平的发展。开发用于解释系统的方法已有大量工作，并且有一些标准可以指定透明度的要求。但是，存在一个差距：标准太高了，不能充分指定解释性的要求。本文开发了一个得分表，可用于指定解释性要求或评估针对特定应用程序提供的解释性方面。得分表是通过考虑一系列利益相关者的要求而开发的，并且适用于多种系统以及其他AI技术。我们还为如何使用得分表提供指导，并通过将其应用于一系列应用程序来说明其一般性和实用性。]]></description>
      <guid>https://arxiv.org/abs/2502.09861</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>光谱法中的人工智能：从预测到世代及以后的化学反应前进</title>
      <link>https://arxiv.org/abs/2502.09897</link>
      <description><![CDATA[ARXIV：2502.09897V1公告类型：新 
摘要：机器学习（ML）和人工智能（AI）的快速出现已经催化了化学的重大转化，但是这些方法将这些方法应用于光谱和光谱数据，称为光谱机器学习（SpectRAML），仍然相对疏忽了。现代光谱技术（MS，NMR，IR，Raman，UV-VIS）产生了不断增长的高维数据，超出了传统的基于专家的工作流程，对自动化和智能分析产生了迫切需求。在这项调查中，我们提供了统一的综述，对远期任务（分子到光谱预测）和倒数任务（频谱到 - 分子推断）的谱系进行了系统检查的最新方法。我们追踪了光谱中ML的历史演变，从早期模式识别到能够进行高级推理的最新基础模型，并提供了代表性神经体系结构的分类学，包括基于图基和基于变压器的方法。在应对数据质量，多模式集成和计算可扩展性等关键挑战时，我们突出了新兴方向，例如合成数据生成，大规模预处理和少数或零摄像的学习。为了培养可重复的研究，我们还发布了一个开源存储库，其中包含最近的论文及其相应的策划数据集（https://github.com/mine-lab-nd/spectrumml_survey_papers）。我们的调查是研究人员的路线图，并指导光谱和AI交集的进步。]]></description>
      <guid>https://arxiv.org/abs/2502.09897</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>针对代理编程的Ann Arbor架构</title>
      <link>https://arxiv.org/abs/2502.09903</link>
      <description><![CDATA[ARXIV：2502.09903V1公告类型：新 
摘要：在本文中，我们通过自动机理论的镜头重新检查了大型语言模型的工程。我们认为，语言模型的功能是自动机，并且像所有自动机一样，应以他们接受的语言（统一的所有自然和正式语言集合）进行编程。因此，传统的软件工程实践 - 根据编程语言和自然语言的明确分离，必须重新考虑。我们介绍了Ann Arbor Architecture，这是一种面向代理的语言模型编程的概念框架，作为与原始令牌生成相比的高级抽象，并提供了有关秘密学习的新观点。基于此框架，我们介绍了我们的代理平台的设计，并报告了我们在代理培训中的初步实验。]]></description>
      <guid>https://arxiv.org/abs/2502.09903</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Autos $^2 $ earch：解锁大型模型的推理潜力用于基于Web的源搜索</title>
      <link>https://arxiv.org/abs/2502.09913</link>
      <description><![CDATA[ARXIV：2502.09913V1公告类型：新 
摘要：基于Web的管理系统已被广泛用于风险控​​制和工业安全。但是，有效地将源搜索功能集成到这些系统中，以使决策者能够找到和解决危害（例如，气体泄漏检测）仍然是一个挑战。尽管先前的努力使用网络众包和AI算法来探索来源搜索决策支持，但这些方法在招募人类参与者和时间敏感的情况下的响应时间缓慢而遭受了间接费用。为了解决这个问题，我们介绍了Autos $^2 $ earch，这是一个新颖的框架，利用大型型号在Web应用程序中进行零拍源搜索。 Autos $^2 $ earch在通过基于Web的显示器投影的简化视觉环境中运行，利用旨在模拟人类推理的经过经过经过经过经过经过经验的促进及。多模式大型语言模型（MLLM）将视觉观测转换为语言描述，使LLM能够在四个方向选择上执行语言推理。广泛的实验表明，汽车$^2 $ earch的性能几乎等同于人类协作源搜索，同时消除了对众包劳动的依赖。我们的工作为使用Web工程设计在其他工业应用中设计此类自主系统提供了宝贵的见解。]]></description>
      <guid>https://arxiv.org/abs/2502.09913</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>mir台：通过许多镜头中的归纳推理，基准LLM的长篇文化智能</title>
      <link>https://arxiv.org/abs/2502.09933</link>
      <description><![CDATA[ARXIV：2502.09933V1公告类型：新 
摘要：归纳推理（IR）是总结示例规则并应用新规则的能力，长期以来一直被视为通用智力的原始能力，并由认知科学和AI研究人员广泛研究。已经提出了许多基准测量大语模型（LLMS）的能力。但是，他们专注于少数拍摄（通常$ &lt;$ 10）的设置，并且缺乏评估，以汇总长篇小说中的许多信息。另一方面，LLM的不断增长的上下文长度提出了许多镜头中文化学习（ICL）的新颖范式，该范式以数百到数千至数千个示例而没有昂贵且效率低下的微调来解决新任务。但是，许多射击评估主要集中在分类上（IR的一个非常有限的方面），并且流行的长篇小说LLM任务（例如针中的纳巴克（NIAH））很少需要复杂的智能来整合许多信息。为了解决两个世界的问题，我们提出了Mir Bench，这是第一个弹出式诱导推理基准测试基准，该基准要求LLM通过具有不同数据格式的基本函数通过输入输出示例诱导输出。基于mir板，我们研究了许多用于归纳推理和许多射击ICL的新问题，包括针对错误射击的稳健性以及对链链（COT）（COT）的影响，以及获得的有见地的发现。]]></description>
      <guid>https://arxiv.org/abs/2502.09933</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>使用两阶段编码模型分析患者日常运动行为动态</title>
      <link>https://arxiv.org/abs/2502.09947</link>
      <description><![CDATA[ARXIV：2502.09947V1公告类型：新 
摘要：在远程医疗保健监视数据的分析中，时间序列表示学习在发现更深层次的患者行为模式方面具有实质性价值，尤其是考虑到数据的良好时间粒度。在这项研究中，我们专注于痴呆症患者的家庭活动记录数据集。我们提出了一种两阶段的自我监督学习方法。第一阶段涉及将时间序列活动转换为文本字符串，然后通过微调语言模型对其进行编码。在第二阶段，这些时间序列向量被双二维用于应用Pagerank方法，以分析潜在状态过渡以定量评估参与者的行为模式并确定活动偏见。这些见解与诊断数据相结合，旨在支持个性化的护理干预措施。]]></description>
      <guid>https://arxiv.org/abs/2502.09947</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高级推理的多种推论和验证</title>
      <link>https://arxiv.org/abs/2502.09955</link>
      <description><![CDATA[ARXIV：2502.09955V1公告类型：新 
摘要：诸如OpenAI O1，O3和DeepSeek R1之类的推理LLM在数学和编码方面取得了重大进展，但找到了诸如国际数学奥林匹克（IMO）组合问题，抽象和推理语料库（ARC）以及人类的最后一部分，以及人类的最后一部分，诸如国际数学奥林匹克（IMO）组合问题等挑战性的高级任务取得了重大进展。考试（HLE）问题。我们使用一种多种推理方法，该方法在测试时结合了多种模型和方法。我们发现验证数学和代码问题，以及对其他问题的拒绝抽样是简单有效的。我们通过精益来自动验证IMO问题解决方案的正确性，并通过代码弧形难题，并发现最佳n有效地回答了HLE问题。我们的方法将IMO组合问题问题的答案的准确性从33.3％提高到77.8％，HLE问题的准确性从8％到37％，解决了948人无法使用的80％的弧形难题，而O3高度计算的弧形难题中有26.5％不是。通过适应代理图表和不同提示，代码和数据集改用推理反馈的测试时间仿真，增强学习和元学习的元学习。我们的方法是可靠，健壮和可扩展的，本着可重复的研究精神，我们将在出版时公开使用。]]></description>
      <guid>https://arxiv.org/abs/2502.09955</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>我的系统提示是否使用过？大型语言模型促使会员推理</title>
      <link>https://arxiv.org/abs/2502.09974</link>
      <description><![CDATA[ARXIV：2502.09974V1公告类型：新 
摘要：及时工程已成为一种强大的技术，用于优化针对特定应用的大型语言模型（LLM），实现更快的原型和改进的性能，并引起社区对保护专有系统提示的兴趣。在这项工作中，我们通过会员推理的角度探讨了有关迅速隐私的新颖观点。我们开发了及时的侦探，这是一种统计方法，可靠地确定第三方语言模型是否使用了给定的系统提示。我们的方法依赖于统计测试，比较了与不同系统提示相对应的两组模型输出的分布。通过对各种语言模型进行的广泛实验，我们证明了迅速侦探迅速成员推理的有效性。我们的工作表明，即使是系统提示的微小变化也会在不同的响应分布中表现出来，从而使我们能够验证具有统计意义的及时使用情况。]]></description>
      <guid>https://arxiv.org/abs/2502.09974</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>决策信息符合大型语言模型：可解释的操作研究的未来</title>
      <link>https://arxiv.org/abs/2502.09994</link>
      <description><![CDATA[ARXIV：2502.09994V1公告类型：新 
摘要：运营研究（OR）对于许多行业的决策至关重要。尽管最近或方法通过整合大型语言模型（LLM）的自动化和效率取得了重大改善，但它们仍然很难产生有意义的解释。缺乏清晰度引起了人们对或应用中透明度和可信度的担忧。为了应对这些挑战，我们提出了一个全面的框架，可解释的操作研究（EOR），强调伴随优化的可行和可理解的解释。 EOR的核心是决策信息的概念，该概念是从何种分析中出现的，并着重于评估复杂约束（或参数）变化对决策的影响。具体而言，我们利用二分图来量化或模型中的更改并采用LLMS来提高解释功能。此外，我们介绍了第一个工业基准，以严格评估OR在OR中的解释和分析的有效性，以建立该领域的透明度和清晰度的新标准。]]></description>
      <guid>https://arxiv.org/abs/2502.09994</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>梦想驾驶：使用分析世界模型的基于模型的车辆控制</title>
      <link>https://arxiv.org/abs/2502.10012</link>
      <description><![CDATA[ARXIV：2502.10012V1公告类型：新 
摘要：可区分的模拟器最近显示了训练自动驾驶汽车控制器的巨大希望。能够通过它们倒退，可以将它们放入端到端的训练循环中，其中他们已知的动态变成了学习的有用的先验，以学习学习，从而消除了环境的典型黑匣子假设。到目前为止，这些系统仅用于培训政策。但是，就他们所能提供的内容而言，这并不是故事的终结。在这里，我们第一次使用它们来训练世界模型。具体来说，我们提出了三个新的任务设置，使我们能够学习下一个状态预测指标，最佳计划者和最佳逆向状态。与分析策略梯度（APG）不同，该梯度需要下一个模拟器状态的梯度相对于当前的行为，我们的拟议设置依赖于下一个状态的梯度相对于当前状态。我们称这种方法分析世界模型（AWMS）并展示其应用程序，包括如何在Waymax模拟器中使用它来计划。除了推动此类模拟器的可能性限制外，我们还提供了改进的培训配方，该配方将大规模Waymo Open Motion Dataset上的性能提高了高达12％，而基本上没有额外的费用。]]></description>
      <guid>https://arxiv.org/abs/2502.10012</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Poi-Enhancer：POI表示学习的基于LLM的语义增强框架</title>
      <link>https://arxiv.org/abs/2502.10038</link>
      <description><![CDATA[ARXIV：2502.10038V1公告类型：新 
摘要：POI表示学习在处理与用户移动性数据有关的任务中起着至关重要的作用。最近的研究表明，通过多模式信息丰富POI表示可以显着提高其任务绩效。以前，包含在POI表示形式中的文本信息通常仅涉及POI类别或登机内容，从而导致现有方法中的文本特征相对较弱。相比之下，已经发现接受广泛文本数据培训的大型语言模型（LLM）具有丰富的文本知识。但是，利用这种知识来增强POI表示学习提出了两个关键挑战：首先，如何有效地从LLM中提取与POI相关的知识，其次，如何整合提取的信息以增强POI表示。为了应对这些挑战，我们提出了Poi-Enhancer，这是一个便携式框架，利用LLMS改善经典POI学习模型产生的POI表示。我们首先设计了三个专门提示，以有效地从LLM中提取语义信息。然后，双功能对齐模块增强了提取信息的质量，而语义特征融合模块则保持其完整性。然后，交叉注意的融合模块将这种高质量信息完全自适应地整合到POI表示中，多视图对比度学习进一步将人类理解的语义信息注入了这些表示形式。在三个现实世界数据集上进行的广泛实验证明了我们框架的有效性，显示了所有基线表示的显着改善。]]></description>
      <guid>https://arxiv.org/abs/2502.10038</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>基于个性化歧视性根树的无监督实体对齐</title>
      <link>https://arxiv.org/abs/2502.10044</link>
      <description><![CDATA[ARXIV：2502.10044V1公告类型：新 
摘要：实体对齐（EA）是将不同知识图（kgs）的潜在等效实体链接。大多数现有的EA方法受到监督，因为它们需要监督种子对齐的监督，即手动指定的对齐实体对。最近，几项EA研究已尝试摆脱种子对齐。尽管取得了初步的进展，但他们仍然遭受两个局限性：（1）由于其GNN式编码器所产生的实体嵌入缺乏个性化，因为某些聚合子路口在不同的实体之间共享。 （2）由于缺乏监督信号，他们无法完全减轻候选KG之间的分布失真问题。在这项工作中，我们提出了一种名为UNEA的新颖的无监督实体一致性方法，以解决上述两个问题。首先，我们参数采样了一个植根于每个实体的树邻域，并因此开发了树木注意聚集机制，以提取每个实体的个性化嵌入。其次，我们引入了一项辅助任务，即在kg编码器的输入和输出之间最大化相互信息，以正规化模型并防止分布失真。广泛的实验表明，我们的联合国AEA为无监督的EA任务实现了新的最先进，甚至可以超越许多现有的监督EA基线。]]></description>
      <guid>https://arxiv.org/abs/2502.10044</guid>
      <pubDate>Mon, 17 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>