<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Wed, 19 Feb 2025 05:00:00 GMT</lastBuildDate>
    <item>
      <title>评估回形针最大化器：基于RL的语言模型是否更有可能追求工具目标？</title>
      <link>https://arxiv.org/abs/2502.12206</link>
      <description><![CDATA[ARXIV：2502.12206V1公告类型：新 
摘要：随着大型语言模型（LLM）的不断发展，确保他们与人类目标和价值观保持一致仍然是一个紧迫的挑战。一个关键问题是\ textIt {工具融合}，其中AI系统在优化给定的目标时，开发出意想不到的中间目标，覆盖了最终目标并偏离人类意义的目标。这个问题在强化学习（RL）培训的模型中尤其重要，该模型可以产生创造性但意外的策略以最大程度地提高奖励。在本文中，我们通过将用直接RL优化（例如O1模型）训练的模型与从人类反馈（RLHF）进行强化学习的模型进行比较，探讨了LLMS中的仪器融合。我们假设RL驱动的模型由于对目标指导行为的优化方式表现出更强的工具收敛趋势，其方式可能与人类意图失调。为了评估这一点，我们引入了仪器雷瓦尔，这是评估RL训练LLM中仪器收敛的基准。最初的实验揭示了一个案例，即要赚钱的模型出乎意料地追求工具目标，例如自我复制，暗示了乐器融合的迹象。我们的发现有助于更深入地了解AI系统中的一致性挑战以及意外模型行为带来的风险。]]></description>
      <guid>https://arxiv.org/abs/2502.12206</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过跨层门中的MOE推断的精确专家预测</title>
      <link>https://arxiv.org/abs/2502.12224</link>
      <description><![CDATA[ARXIV：2502.12224V1公告类型：新 
摘要：大型语言模型（LLM）在各种任务中都表现出了令人印象深刻的表现，并且它们在边缘方案中的应用引起了极大的关注。然而，非常适合边缘方案的稀疏激活混合物（MOE）模型由于其高内存需求而受到相对较少的关注。已经提出了基于卸载的方法来应对这一挑战，但他们面临着专家预测的困难。不准确的专家预测可能会导致推理延迟。为了促进MOE模型在边缘方案中的应用，我们提出了命运，这是一种旨在MOE模型的卸载系统，以便在资源约束环境中有效推断。命运背后的关键见解是，来自相邻层的门输入可以有效地用于专家预取，可实现高预测准确性，而无需其他GPU开销。此外，命运采用了一种肤色浅的专家缓存策略，将专家的命中率提高到99％。此外，命运还集成了量身定量的量化策略，以进行缓存优化和IO效率。实验结果表明，与按需负载和基于专家激活路径的方法相比，命运在预填充速度中最多可达到4.5倍和1.9倍的速度，并分别以解码速度达到4.1倍和2.2倍加速，同时保持推理质量。此外，在不同的内存预算中，命运的绩效改进是可扩展的。]]></description>
      <guid>https://arxiv.org/abs/2502.12224</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过LLMS将专家知识纳入逻辑计划</title>
      <link>https://arxiv.org/abs/2502.12275</link>
      <description><![CDATA[ARXIV：2502.12275V1公告类型：新 
摘要：本文介绍了Exklop，这是一个新颖的框架，旨在评估大型语言模型（LLMS）如何将专家知识整合到逻辑推理系统中。这种能力在工程中尤其有价值，在工程学中，专家知识（例如制造商推荐的操作范围）可以直接嵌入自动监测系统中。通过镜像专家验证步骤，范围检查和约束验证等任务有助于确保系统的安全性和可靠性。我们的方法系统地评估了LLM生成的逻辑规则，评估了这些关键验证任务中的句法流利性和逻辑正确性。我们还根据代码执行成果通过迭代反馈循环探索自我纠正的模型能力。 Exklop介绍了一个可扩展的数据集，其中包括130个工程场所，950个提示和相应的验证点。它可以实现全面的基准测试，同时可以控制实验的任务复杂性和可扩展性。我们利用合成数据创建方法来对包括Llama3，Gemma，Mixtral，Mistral和Qwen在内的各种LLM进行广泛的经验评估进行广泛的经验评估。结果表明，尽管模型产生了几乎完美的句法正确的代码，但它们经常在翻译专家知识时表现出逻辑错误。此外，迭代自我纠正仅产生边际改善（最高3％）。总体而言，Exklop是一个强大的评估平台，可以简化自我校正系统的有效模型的选择，同时清楚地描述遇到的错误类型。完整的实现以及所有相关数据可在GitHub上获得。]]></description>
      <guid>https://arxiv.org/abs/2502.12275</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>一项关于自动计划大型语言模型的调查</title>
      <link>https://arxiv.org/abs/2502.12435</link>
      <description><![CDATA[ARXIV：2502.12435V1公告类型：新 
摘要：大型语言模型（LLMS）的计划能力近年来引起了人们的关注，因为它们具有出色的多步推理能力及其在广泛领域中概括的能力。尽管一些研究人员强调了LLMS执行复杂的计划任务的潜力，但其他研究人员则强调了其性能的重大限制，尤其是当这些模型的任务是处理长途推理的复杂性时。在这项调查中，我们对使用LLM在自动化计划中使用的现有研究进行了严格的研究，并详细研究了其成功和缺点。我们说明，尽管LLM并不适合作为独立计划者，但由于这些局限性，但它们还是有很大的机会，可以与其他方法结合使用。因此，我们提倡一种平衡的方法，该方法利用了LLM的固有灵活性和广义知识，以及传统计划方法的严格和成本效益。]]></description>
      <guid>https://arxiv.org/abs/2502.12435</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI的计算安全：A信号处理视角</title>
      <link>https://arxiv.org/abs/2502.12445</link>
      <description><![CDATA[Arxiv：2502.12445V1公告类型：新 
摘要：AI安全是一个快速增长的研究领域，旨在防止Frontier AI技术的伤害和滥用，尤其是在生成AI（Genai）工具方面，能够通过文本提示来创建现实和高质量的内容。此类工具的示例包括大型语言模型（LLM）和文本对图像（T2I）扩散模型。由于各种领先的Genai模型的性能由于类似的培训数据源和神经网络架构设计而接近饱和，可靠的安全护栏的开发已成为责任和可持续性的关键区别。本文介绍了计算安全概念的形式化，该概念是一个数学框架，可以通过信号处理理论和方法的镜头对Genai的安全挑战进行定量评估，制定和研究。特别是，我们探讨了Genai中计算安全挑战的两个示例类别，这些类别可以作为假设测试问题提出。为了确保模型输入的安全性，我们显示了如何使用敏感性分析和损失景观分析来检测恶意提示通过越狱尝试。为了确保模型输出的安全性，我们阐明了如何使用统计信号处理和对抗性学习来检测AI生成的内容。最后，我们讨论了关键的开放研究挑战，机会以及信号处理在计算AI安全中的基本作用。]]></description>
      <guid>https://arxiv.org/abs/2502.12445</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过大型语言模型的代理商调查和扩展霍曼斯的社会交流理论</title>
      <link>https://arxiv.org/abs/2502.12450</link>
      <description><![CDATA[ARXIV：2502.12450V1公告类型：新 
摘要：霍曼斯的社会交流理论（集合）被广泛认为是理解人类文明和社会结构的形成和出现的基本框架。在社会科学中，通常根据简单的仿真实验或现实世界的人类研究对该理论进行研究，这些实验都缺乏现实主义或太昂贵而无法控制。在人工智能中，大语言模型（LLM）的最新进展表现出有希望的模拟人类行为的能力。受这些见解的启发，我们采用了跨学科研究的观点，并建议使用基于LLM的代理来研究Homans的集合。具体来说，我们构建了一个由三个LLM代理组成的虚拟社会，并让他们从事社会交流游戏以观察他们的行为。通过广泛的实验，我们发现霍曼斯的集合在我们的代理社会中得到了很好的验证，证明了代理人和人类行为之间的一致性。在这个基础的基础上，我们故意改变了代理商协会的设置，以扩展传统的霍曼斯的场景，从而更加全面和详细。据我们所知，本文标志着研究Homans与基于LLM的代理商进行的第一步。更重要的是，它引入了一种新颖且可行的研究范式，该范式通过基于LLM的代理来桥接社会科学和计算机科学领域。代码可在https://github.com/paitesanshi/set上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12450</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>提升，删除和自定义：强大的System2-system1代码生成管道</title>
      <link>https://arxiv.org/abs/2502.12492</link>
      <description><![CDATA[ARXIV：2502.12492V1公告类型：新 
摘要：大型语言模型（LLMS）在各个领域，尤其是在系统1任务中表现出了显着的功能，但是在系统2任务中解决问题的机制的复杂性尚未得到充分探索。对System2到System1方法方法的最新研究激增，通过推理时间计算探索系统2推理知识，并将探索知识压缩到System 1过程中。在本文中，我们专注于代码生成，这是代表性系统2任务，并确定了两个主要挑战：（1）复杂的隐藏推理过程和（2）异质数据分布，使强大的LLM Solvers探索和培训变得复杂。为了解决这些问题，我们提出了一个新颖的BDC框架，该框架探讨了有见地的系统2使用MC-Tree Agent-agent算法，具有相互的\ textbf {b} osting，\ textbf {d} isentangles iSentAngle lora-experts，并为每个数据实例获得\ textbf {c} ustomized问题求解器具有输入感知的超级net工作，可以在Lora-Experts上进行重量，从而提供有效性，灵活性和鲁棒性。该框架通过相互验证和增强来利用多个LLM，并集成到蒙特卡洛树搜索过程中，通过基于反射的修剪和改进来增强。此外，我们介绍了disenlora算法，该算法将异质数据簇定为可综合的LORA专家，从而通过输入感知的Hypernetwork实现了适应性的定制问题解决者。这项工作为在复杂的推理任务中推进LLM功能奠定了基础，并提供了一种新颖的System2-System1解决方案。]]></description>
      <guid>https://arxiv.org/abs/2502.12492</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM推理和计划的推理时间计算：基准和见解</title>
      <link>https://arxiv.org/abs/2502.12521</link>
      <description><![CDATA[ARXIV：2502.12521V1公告类型：新 
摘要：我们研究了大语言模型（LLM）在解决复杂任务中的推理和计划功能。推理时间技术的最新进展表明，通过探索推理过程中的中间步骤，在没有额外培训的情况下增强LLM推理的潜力。值得注意的是，OpenAI的O1模型通过新颖的多步推理和验证来显示出令人鼓舞的性能。在这里，我们探讨了扩展推理时间技术如何改善推理和计划，重点是理解计算成本和性能之间的权衡。为此，我们构建了一个全面的基准，称为SYS2Bench，并进行了广泛的实验，对跨五个类别的11个不同任务进行评估，包括算术推理，逻辑推理，常识推理，算法推理，算法推理和计划。我们的发现表明，简单地扩展推理时间计算具有局限性，因为在所有推理和计划任务中，没有一个推理时间技术始终如一地表现良好。]]></description>
      <guid>https://arxiv.org/abs/2502.12521</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Cityeqa：一个层次的LLM代理商在体现的问题上回答城市空间中的基准测试</title>
      <link>https://arxiv.org/abs/2502.12532</link>
      <description><![CDATA[Arxiv：2502.12532V1公告类型：新 
摘要：体现的问答（EQA）主要集中在室内环境上，留下了城市环境的复杂性 - 跨越环境，行动和感知 - 在很大程度上没有探索。为了弥合这一差距，我们介绍了Cityeqa，这是一项新任务，具体的代理商通过在动态城市空间中的积极探索来回答开放式摄影问题。为了支持这项任务，我们介绍了Cityeqa-EC，这是第一个基准的基准数据集，该数据集跨越了六个类别的1,412个人类注销的任务，这是基于现实的3D Urban Simulator。此外，我们提出了为Cityeqa量身定制的新型特工计划者 - 经理演员（PMA）。 PMA启用了长马计划和层次任务执行：计划者将回答的问题分解为子任务，经理在过程控制过程中维护以对象为中心的认知映射，用于空间推理，专业演员处理导航，勘探和探索，探索和集合子任务。实验表明，PMA达到了60.7％的人级答案准确性，明显优于基于边界的基层。在有前途的同时，与人类相比，性能差距强调了Cityeqa的视觉推理的需求。这项工作为城市空间情报的未来进步铺平了道路。数据集和代码可在https://github.com/biluyong/cityeqa.git上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12532</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>探索人格特质对LLM偏见和毒性的影响</title>
      <link>https://arxiv.org/abs/2502.12566</link>
      <description><![CDATA[ARXIV：2502.12566V1公告类型：新 
摘要：随着AI在人类生活中的不同角色，将大型语言模型（LLMS）具有不同的个性吸引了研究兴趣。尽管“拟人化”增强了人类的交互性和LLM的适应性经验，但它引起了人们对内容安全的关键关注，尤其是在LLM生成的偏见，情感和毒性方面。这项研究探讨了将不同的人格特征分配给LLM的如何影响其产出的毒性和偏见。利用在社会心理学中开发的广泛接受的己科人格框架时，我们设计了实验性的声音促使在三个有毒和偏见基准上测试三个LLM的表现。这些发现证明了所有三种模型对己科人格特征的敏感性，更重要的是，其产出的偏见，负面情绪和毒性的一致变化。特别是，调整几种人格特征的水平可以有效地降低模型表现的偏见和毒性，类似于人格特征与有毒行为之间的人类相关性。这些发现突出了除了培训或通过LLM拟人化的培训方法或微调方法外，还需要检查内容安全性。他们还建议将个性调整为一种简单且低成本的方法来进行受控文本生成。]]></description>
      <guid>https://arxiv.org/abs/2502.12566</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>RM-POT：重新解决数学问题并通过思想计划解决</title>
      <link>https://arxiv.org/abs/2502.12589</link>
      <description><![CDATA[ARXIV：2502.12589V1公告类型：新 
摘要：最近，在培训语言模型中取得了重大进步，以进行分步推理，以解决复杂的数值推理任务。除了解决这些问题的方法之外，问题本身的结构和表述在确定大语言模型的性能中也起着至关重要的作用。我们观察到，即使是数学问题的表面形式的微小变化也会对答案分布和解决率产生深远的影响。这突出了LLMS对表面级别变化的脆弱性，在通过复杂问题推理时揭示了其有限的鲁棒性。在本文中，我们提出了RM-POT，RM-POT是一个整合问题重新重新制定（RM），代码辅助推理（POT）和域名的三阶段框架，几乎没有学会来解决这些局限性。我们的方法首先将输入问题重新制定为各种表面形式，以减少结构性偏见，然后从特定于特定于域的特定领域问题库中检索五个语义上的示例，以提供上下文指导，并最终生成可执行的Python代码以进行精确计算。]]></description>
      <guid>https://arxiv.org/abs/2502.12589</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Perovskite-llm：知识增强的钙钛矿太阳能细胞研究的大型语言模型</title>
      <link>https://arxiv.org/abs/2502.12669</link>
      <description><![CDATA[Arxiv：2502.12669V1公告类型：新 
摘要：钙钛矿太阳能电池的快速发展（PSC）导致了研究出版物的指数增长，从而迫切需要该领域中有效的知识管理和推理系统。我们为PSC提供了一个全面的知识增强系统，该系统集成了三个关键组成部分。首先，我们开发了Perovskite-KG，这是一个特定于领域的知识图，该图表由1,517篇研究论文构建，其中包含23,789个实体和22,272个关系。其次，我们创建了两个互补数据集：Perovskite-Chat，包括55,101个高质量的问题 - 答案对，通过新型的多代理框架生成，以及钙钛矿 - 季节性，包含2,217个精心策划的材料科学问题。第三，我们介绍了两种专业的大型语言模型：用于领域特定知识援助的Perovskite-Chat-llm和用于科学推理任务的Perovskite-Rounowing-llm。实验结果表明，我们的系统在特定于领域的知识检索和科学推理任务中大大优于现有模型，从而为研究人员提供了有效的文献综述工具，实验设计和PSC研究中的复杂问题解决。]]></description>
      <guid>https://arxiv.org/abs/2502.12669</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>vidcapbench：可控文本的视频字幕的综合基准标题</title>
      <link>https://arxiv.org/abs/2502.12782</link>
      <description><![CDATA[ARXIV：2502.12782V1公告类型：新 
摘要：可控文本对视频（T2V）模型的培训在很大程度上取决于视频和字幕之间的一致性，但是现有的研究很少将视频字幕评估与T2V生成评估联系起来。本文介绍了Vidcapbench，这是一种视频标题评估方案，专为T2V生成而设计，不可知论为任何特定的字幕格式。 Vidcapbench采用数据注释管道，结合了专家模型标签和人类精致，将每个收集的视频与跨越视频美学，内容，运动和物理定律的关键信息相关联。然后，vidcapbench将这些关键信息属性划分为自动评估和手动评估子集，以满足敏捷开发的快速评估需求以及彻底验证的准确性要求。通过评估众多最先进的字幕模型，我们证明了与现有的视频字幕评估方法相比，Vidcapbench的稳定性和全面性。使用现成的T2V模型进行验证表明，Vidcapbench上的分数与T2V质量评估指标之间存在显着的正相关，这表明VidCapbench可以为培训T2V模型提供宝贵的指导。该项目可在https://github.com/vidcapbench/vidcapbench上找到。]]></description>
      <guid>https://arxiv.org/abs/2502.12782</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>通过AI进行自适应反馈：比较LLM和教师在实验方案中的反馈质量</title>
      <link>https://arxiv.org/abs/2502.12842</link>
      <description><![CDATA[arxiv：2502.12842v1公告类型：新 
摘要：有效的反馈对于促进学生在科学探究方面的成功至关重要。随着人工智能的进步，大语言模型（LLMS）为提供即时和自适应反馈提供了新的可能性。但是，这种反馈通常缺乏现实世界实践者提供的教学验证。为了解决这一局限性，我们的研究评估并比较了LLM代理的反馈质量与学生写的实验协议方面的人类教师和科学教育专家的反馈质量。四名盲人评估者，所有科学探究和科学教育专业人士，评估了1）LLM代理人产生的反馈文本，2）教师和3）科学教育专家使用五点李克特量表，基于有效反馈的六个标准：馈送，反馈，向前喂养，建设性语调，语言清晰度和技术术语。我们的结果表明，LLM生成的反馈与教师和专家在整体质量方面没有显着差异。但是，LLM代理商的性能滞后在馈后维度，其中涉及在学生的工作环境中识别和解释错误。定性分析强调了LLM代理在上下文理解和明确交流特定错误中的局限性。我们的发现表明，将LLM生成的反馈与人类专业知识相结合可以通过利用LLM的效率和对教育工作者的细微理解来增强教育实践。]]></description>
      <guid>https://arxiv.org/abs/2502.12842</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    <item>
      <title>持续学习对话AI：通过A2C增强学习的个性化代理框架</title>
      <link>https://arxiv.org/abs/2502.12876</link>
      <description><![CDATA[ARXIV：2502.12876V1公告类型：新 
摘要：创建个性化和适应性的对话AI仍然是一个关键挑战。本文介绍了使用A2C增强学习实施的连续学习对话AI（CLCA）方法，以超越静态大语言模型（LLMS）。我们使用LLMS生成的模拟销售对话来培训A2C代理。该代理商学会了优化个性化对话策略，专注于参与和交付价值。我们的系统体系结构将增强学习与LLMS集成，以进行数据创建和响应选择。这种方法提供了一种实用的方式来建立个性化的AI同伴，通过不断学习，超越传统的静态LLM技术来发展。]]></description>
      <guid>https://arxiv.org/abs/2502.12876</guid>
      <pubDate>Wed, 19 Feb 2025 05:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>