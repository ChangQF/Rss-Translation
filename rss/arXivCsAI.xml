<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.ai更新在arxiv.org上</title>
    <link>http://rss.arxiv.org/rss/cs.AI</link>
    <description>cs.ai在arxiv.org e-print档案中更新。</description>
    <lastBuildDate>Fri, 14 Mar 2025 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过验证器在循环中进行自动定理的验证指导</title>
      <link>https://arxiv.org/abs/2503.09730</link>
      <description><![CDATA[ARXIV：2503.09730V1公告类型：新 
摘要：AI推理的最新方法最有前途的方法需要在模型中推出轨迹上应用强化学习的变体（RL），即使是逐步奖励或大量人类注释的轨迹数据。对推出轨迹的依赖使计算成本和时间过高。特别是，推理轨迹的正确性通常只能在完成时进行判断，从而导致RL的稀疏奖励或需要在类似专家迭代的方法中生成昂贵的合成数据。在这项工作中，我们专注于自动定理证明（ATP）任务，并提出了一种新颖的验证器在循环设计中，该设计与现有方法不同，该方法利用了对整个推理轨迹的反馈，它采用自动化验证程序来在推理过程的每个步骤中提供中等反馈。使用精益作为验证者，我们从经验上表明，逐步的本地验证可在模型的推理准确性和效率方面产生全球提高。]]></description>
      <guid>https://arxiv.org/abs/2503.09730</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Agentdam：自主网络代理的隐私泄漏评估</title>
      <link>https://arxiv.org/abs/2503.09780</link>
      <description><![CDATA[ARXIV：2503.09780V1公告类型：新 
摘要：LLM驱动的AI代理是一种新兴的边界，具有提高人类生产力的巨大潜力。但是，授权AI代理在日常任务中代表用户采取行动涉及使他们访问潜在敏感和私人信息，从而导致当代理发生故障时，可能会造成无意间隐私泄漏的风险。在这项工作中，我们提出了一种解决潜在风险的方法，通过训练AI代理更好地满足数据最小化的隐私原则。出于此基准测试的目的，通过“数据最小化”，我们的意思是，仅在必要时才能实现特定任务相关的目的时共享私人信息。我们开发了一个名为Agentdam的基准测试，以评估现有和未来的AI代理可以限制我们指定“必要”来完成任务的潜在私人信息的处理。我们的基准测试了现实的Web交互情况，并适用于所有现有的Web导航代理。我们使用AgentDam来评估AI代理在GPT-4，Llama-3和Claude顶部的构建状况如何，在不必要的情况下可以限制潜在的私人信息的处理，并表明这些代理通常容易无意地使用不必要的敏感信息。我们最终提出了一种基于促进的方法，以减少这种方法。]]></description>
      <guid>https://arxiv.org/abs/2503.09780</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>媒体和负责AI治理：游戏理论和LLM分析</title>
      <link>https://arxiv.org/abs/2503.09858</link>
      <description><![CDATA[ARXIV：2503.09858V1公告类型：新 
摘要：本文研究了AI开发人员，监管机构，用户和媒体之间的复杂相互作用，以促进值得信赖的AI系统。使用进化游戏理论和大型语言模型（LLM），我们在不同的监管制度下对这些参与者之间的战略互动进行了建模。该研究探讨了实现负责任治理的两个关键机制，安全的AI开发和安全AI的采用：通过媒体报告激励有效的法规，并根据评论的建议来调节用户信任。这些发现突出了媒体在向用户提供信息中的关键作用，这可能是通过调查开发人员或监管机构的一种“软”调节形式，作为替代机构AI法规的替代（在许多地区仍然不存在）。游戏理论分析和基于LLM的模拟都揭示了有效的监管和值得信赖的AI开发的条件，从而强调了从进化游戏理论的角度考虑不同监管制度的影响的重要性。该研究得出的结论是，有效的治理需要管理高质量评论的激励措施和成本。]]></description>
      <guid>https://arxiv.org/abs/2503.09858</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>用于几次班级学习的新基准：重新定义上限</title>
      <link>https://arxiv.org/abs/2503.10003</link>
      <description><![CDATA[ARXIV：2503.10003V1公告类型：新 
摘要：班级学习（CIL）的目的是不断适应新兴课程，同时保留对先前学到的课程的知识。几乎没有射门的课堂学习（FSCIL）提出了更大的挑战，这需要模型仅使用有限数量的样本学习增量类。在常规的CIL中，联合训练被广泛认为是上限，既是基准又是方法论指南。但是，我们发现，由于严重的阶级失衡引起的任务间分离（ICS）的固有难度，联合训练在FSCIL中没有有意义的上限。在这项工作中，我们通过整合不平衡感知的技术，有效地弥合了基础和增量类之间的性能差距，从而引入了针对FSCIL量身定制的新的联合培训基准。此外，我们指出了现有FSCIL方法的实验设置和评估中的不一致。为了确保不同的FSCIL方法与联合培训之间的公平比较，我们将培训条件标准化，并提出了同时考虑验证集和计算复杂性的统一评估方案。通过为FSCIL建立可靠的上限和标准化的评估框架，我们的工作为未来的研究提供了明确的基准和实用的基础。]]></description>
      <guid>https://arxiv.org/abs/2503.10003</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>OR-LLM-ENSENT：通过推理大语言模型的操作研究优化问题的自动建模和解决</title>
      <link>https://arxiv.org/abs/2503.10009</link>
      <description><![CDATA[ARXIV：2503.10009V1公告类型：新 
摘要：运营研究（OR）已广泛应用于各个领域，例如资源分配，生产计划和供应链管理。但是，解决现实世界或问题需要或专家执行数学建模和程序员来开发解决方案算法。这种传统的方法在很大程度上依赖专家，是昂贵的，并且具有长期的开发周期，严重限制了广泛采用或技术的采用。很少有人考虑使用人工智能（AI）代替专业人士来实现或问题的完全自动化解决方案。我们提出了OR-LLM-AGENT，这是第一个实现端到端自动化解决现实世界或问题的AI代理。 OR-LLM代理利用大语言模型（LLMS）的思维链（COT）推理能力将自然语言问题描述转化为正式的数学模型，并自动生成Gurobi求解器代码。在OR-LLM-AGENT中，或编码旨在在沙盒环境中自动执行和维修，从而促进了最终解决方案的推导。由于缺乏用于评估自动解决或问题的专用基准数据集，因此我们构建了一个包括83个现实世界或自然语言所描述的问题的基准数据集。我们使用最新的（SOTA）推理LLM进行比较实验，包括GPT-O3-Mini，DeepSeek-R1和Gemini 2.0 Flash Thinky。 OR-LLM代理达到了100％的最高通行率和85％的最高解决方案精度，这表明自动化或解决问题的可行性。数据和代码已在https://github.com/bwz96sco/or_llm_agent上公开获得。]]></description>
      <guid>https://arxiv.org/abs/2503.10009</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>高级工具学习和选择系统（Atlass）：使用LLM的闭环框架</title>
      <link>https://arxiv.org/abs/2503.10071</link>
      <description><![CDATA[ARXIV：2503.10071V1公告类型：新 
摘要：LLM代理与外部工具的结合使模型可以解决其知识库之外的复杂任务。人工设计的工具不灵活，仅限于专家创建的现有工具范围内的解决方案。为了解决这个问题，我们提出了Atlass，这是一种设计为闭环框架的高级工具学习和选择系统。它使LLM能够通过需求动态生成外部工具来解决问题。在此框架中，代理商在编排工具选择，执行和改进中发挥着至关重要的作用，从而确保了自适应解决问题的能力。 Atlass的操作遵循三个阶段：第一阶段，了解工具要求，涉及确定是否需要工具并指定其功能的代理；第二阶段是工具检索/生成，涉及代理根据其可用性检索或生成工具的代理；第三阶段（任务解决）涉及组合完成初始任务所需的所有组件工具。该工具数据集存储生成的工具，确保可重复使用和最大程度地降低推理成本。当前基于LLM的工具生成系统难以创建需要API或外部软件包的复杂工具。在Atlass中，我们通过自动设置环境，在线获取相关的API文档，并使用Python解释器来创建一个可靠的多功能工具，以在更广泛的情况下起作用，从而解决了问题。 OpenAI GPT-4.0被用作LLM代理，在执行生成的代码之前，通过人为反馈来处理安全和道德问题。通过解决预定义工具集的局限性并增强适应性，Atlass可以用作现实世界的解决方案，该解决方案使用户具有动态生成的工具以进行复杂的问题解决。]]></description>
      <guid>https://arxiv.org/abs/2503.10071</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>并行化多目标A*搜索</title>
      <link>https://arxiv.org/abs/2503.10075</link>
      <description><![CDATA[ARXIV：2503.10075V1公告类型：新 
摘要：多目标最短路径（MOSP）问题是一个经典的网络优化问题，旨在在图表中有多个边缘成本的两个点之间找到所有帕累托最佳路径。关于使用A*（MOA*）的多目标搜索的最新研究表明，在解决困难的MOSP实例方面表现出色。本文提出了一个新颖的搜索框架，该框架允许MOA*具有不同的客观顺序的有效并行化。该框架结合了独特的上边界策略，可帮助搜索在某些情况下将问题的维度降低到一个。实验结果表明，所提出的框架可以增强基于A*的解决方案的性能，并且与问题维度成正比。]]></description>
      <guid>https://arxiv.org/abs/2503.10075</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>语义协同作用：通过高级技能映射解锁政策见解和学习途径</title>
      <link>https://arxiv.org/abs/2503.10094</link>
      <description><![CDATA[ARXIV：2503.10094V1公告类型：新 
摘要：这项研究介绍了一个基于最先进的自然语言处理，语义嵌入和有效搜索技术的综合系统，以检索相似之处，从而从原始文本信息中产生可行的见解。该系统自动从多个文档（例如策略文件和课程Vitae）中提取并汇总了标准能力，并在公认的能力，职业概况和相关学习课程之间建立牢固的关系。为了验证其性能，我们进行了多层评估，其中包括合成和现实世界中文档中的明确和隐性技能参考。结果显示出近乎人类的准确性，显式技能检测的F1得分超过0.95，而隐式提及则高于0.93。因此，该系统为支持整个AE4RIA网络的深入协作建立了合理的基础。该方法涉及一条基于广泛的预处理和数据清洁，语义嵌入和分割的多阶段管道，并使用基于FAISS的搜索方法进行技能提取。提取的技能与职业框架（在ESCO本体论中提出）以及通过可持续发展目标学院提供的学习途径有关。此外，通过破折号和情节实施的交互式可视化软件，呈现图形和表格，用于实时探索，并由参与决策，培训和学习供应，职业过渡和招聘的人明智的决策。总体而言，通过严格验证的支持，该系统通过从原始，复杂的文本信息中提供结构化和可行的见解，为改善决策，人力资源发展和终身学习提供了有希望的前景。]]></description>
      <guid>https://arxiv.org/abs/2503.10094</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Stepmathagent：通过误差树评估数学过程的逐步代理</title>
      <link>https://arxiv.org/abs/2503.10105</link>
      <description><![CDATA[ARXIV：2503.10105V1公告类型：新 
摘要：评估数学能力对于评估大语言模型（LLMS）的整体表现至关重要。但是，现有的评估方法通常只关注最终答案，从而导致高度不准确和无法解释的评估结果，以及他们未能评估证据或开放式问题。为了解决这些问题，我们提出了一种基于Error树的新型数学过程评估代理，称为Stepmathagent。该代理结合了四个内部核心操作：逻辑步骤细分，步骤评分，得分聚集和错误树的产生，以及四个外部扩展模块：难度校准，简单性评估，完整性验证和格式评估。此外，我们引入了Stepmathbench，这是一个包括1,000个踩踏过程评估实例的基准测试，该基准源自200个由问题类型，主题类别和难度级别分组的高质量数学问题。在Stepmathbench上进行的实验表明，我们提出的Stepmathagent胜过所有最新方法，证明了人类一致的评估偏好以及对各种情况的广泛适用性。我们的数据和代码可在https://github.com/shu-xun/stepmathagent上找到。]]></description>
      <guid>https://arxiv.org/abs/2503.10105</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>自适应偏好聚集</title>
      <link>https://arxiv.org/abs/2503.10215</link>
      <description><![CDATA[ARXIV：2503.10215V1公告类型：新 
摘要：AI对齐是确保AI系统按照人类价值行为的挑战，它已成为基础模型和推荐系统等系统开发的关键问题。尽管如此，目前的主要方法，人类反馈（RLHF）的强化学习还是在汇总了多样化人类偏好的理论局限性。社会选择理论提供了一个框架来汇总偏好，但不是针对AI典型的多维应用程序开发的。利用最近发布的URN过程中的见解，这项工作引入了一种偏好聚合策略，该策略适应用户的上下文，并继承了最大彩票的良好属性，这是一种condorcet concotencet consensiscetent的解决方案概念。]]></description>
      <guid>https://arxiv.org/abs/2503.10215</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM代理显示人类偏见，但表现出独特的学习模式</title>
      <link>https://arxiv.org/abs/2503.10248</link>
      <description><![CDATA[ARXIV：2503.10248V1公告类型：新 
摘要：我们从经验任务的决策中调查了大语言模型（LLM）的选择模式，涉及重复选择和从反馈中学习，并将其行为与人类参与者进行比较。我们发现，在总体上，LLM似乎显示出类似于人类的行为偏见：两者都表现出不足的罕见事件和相关效应。但是，对选择模式的更细微的分析表明，这是出于不同的原因而发生的。与人类不同，LLM表现出强烈的新近度偏见，他们似乎以更复杂的方式做出反应。尽管这些不同的过程平均可能导致相似的行为，但两组之间的选择模式各有近期事件的差异很大。具体而言，诸如``惊喜触发器的变化&#39;&#39;之类的现象和``罕见事件的波浪重新效果&#39;&#39;&#39;在人类中是有力的，但在LLM中完全不存在。我们的发现提供了对使用LLM在学习环境中模拟和预测人类的局限性的见解，并在研究是否复制人类决策趋势时强调了对其行为进行精致分析的必要性。]]></description>
      <guid>https://arxiv.org/abs/2503.10248</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>Surgraw：具有手术智能的经过思考推理的多代理工作流程</title>
      <link>https://arxiv.org/abs/2503.10265</link>
      <description><![CDATA[ARXIV：2503.10265V1公告类型：新 
摘要：幻觉，领域知识差距以及对手术场景中任务相互依存的有限理解的阻碍，妨碍了手术智能中视觉模型（VLM）的整合，从而破坏了临床可靠性。尽管最近的VLMS表现出强大的一般推理和思维能力，但它们仍然缺乏精确的手术场景解释所需的领域专业知识和任务意识。尽管经过思考链（COT）可以更有效地构建推理，但当前的方法依赖于自我生成的COT步骤，这些步骤通常会加剧固有的域间隙和幻觉。为了克服这一点，我们提出了Surgraw，这是一个由COT驱动的多代理框架，可为机器人辅助手术中的大多数任务提供透明，可解释的见解。通过在五个任务中采用专门的COT提示：仪器识别，行动识别，行动预测，患者数据提取和结果评估，Surgraw通过结构化的，域内感知的推理来减轻幻觉。检索增强的生成（RAG）也已集成到外部医学知识中，以弥合域间隙并提高响应可靠性。最重要的是，分层的代理系统可确保COT所包裹的VLM代理有效地协作，同时了解任务相互依存关系，并通过小组讨论机制促进了逻辑一致性。为了评估我们的方法，我们介绍了Surgcotbench，这是第一个带有结构化框架级注释的基于推理的数据集。通过全面的实验，我们证明了拟议的Surgraw的有效性，对12个机器人程序的基线VLM的准确性提高了29.32％，实现了最先进的性能，并提高了可解释，可信赖的，可信赖的和自主的手术援助。]]></description>
      <guid>https://arxiv.org/abs/2503.10265</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>声明签名人：朝着声明过程模型符合检查的有效最佳一致性朝着迈进</title>
      <link>https://arxiv.org/abs/2503.10479</link>
      <description><![CDATA[ARXIV：2503.10479V1公告类型：新 
摘要：在许多工程应用中，必须精确遵循过程，从而使事件日志和声明过程模型之间的一致性检查对于确保遵守所需行为至关重要。这是一个关键领域，人工智能（AI）在推动有效过程改进方面起着关键作用。但是，由于这些模型固有的巨大搜索空间，计算最佳对齐构成了重大的计算挑战。因此，现有的方法通常会在可扩展性和效率上挣扎，从而限制了它们在现实世界中的适用性。本文介绍了一种新型算法，该算法使用A*搜索算法（已建立的AI探路技术）从新的角度利用声明模型的灵活性来解决问题。声明签名人的关键特征包括仅执行积极促进限制违规行为的动作，利用量身定制的启发式措施来导航到最佳解决方案，并采用早期修剪来消除非生产力的分支，同时还通过预处理和整合多个固定的动作来简化该过程。使用8,054个合成和现实生活对准问题评估了所提出的方法，这表明了其通过显着优于当前技术状态来有效计算最佳比对的能力。通过使过程分析师更有效地识别和理解一致性问题，Declarealigner有可能推动有意义的过程改进和管理。]]></description>
      <guid>https://arxiv.org/abs/2503.10479</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>攻城：通过树搜索自主多扭转大型语言模型的越狱</title>
      <link>https://arxiv.org/abs/2503.10619</link>
      <description><![CDATA[ARXIV：2503.10619V1公告类型：新 
摘要：我们介绍了Siege，这是一个多转化的对抗框架，该框架通过树搜索的角度对大语言模型（LLM）安全的逐渐侵蚀进行建模。与依靠一个精心设计的提示的单转弯越狱不同，攻城以广度优先的方式扩展了对话，分支了多个对抗性提示，从而利用部分依从性从先前的回应中剥夺。通过跟踪这些增量策略泄漏并将其重新注入随后的查询，围攻揭示了如何将小特许权积累到完全不允许的输出中。对越狱板数据集的评估表明，攻城在单个多转弯运行中的GPT-3.5涡轮增压率为100％，而GPT-4的成功率比Crescendo或goat等基线的较少。该树搜索方法论提供了一个深入的视图，即模型保护如何在连续的对话转弯中降低了降级，从而强调了语言模型的强大多转弯测试程序的紧迫性。]]></description>
      <guid>https://arxiv.org/abs/2503.10619</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    <item>
      <title>动作不确定性：体现药物的信心启发</title>
      <link>https://arxiv.org/abs/2503.10628</link>
      <description><![CDATA[ARXIV：2503.10628V1公告类型：新 
摘要：表达置信度对于导航动态多模式环境的具体体现的代理人来说是一项挑战，在这种环境中，不确定性均来自感知和决策过程。我们介绍了在开放式多模式环境中调查体现置信度启发的第一批工作。我们介绍了启发策略，该政策构建了跨感应，演绎和绑架推理的置信度评估以及执行策略，从而通过场景重新解释，动作抽样和假设推理来增强信心校准。评估Minecraft环境中校准和故障预测任务的代理，我们表明结构化推理方法（例如经过思考链）可以改善置信度校准。但是，我们的发现还揭示了区分不确定性的持续挑战，尤其是在绑架环境下，强调了对更复杂的体现置信度启发方法的需求。]]></description>
      <guid>https://arxiv.org/abs/2503.10628</guid>
      <pubDate>Fri, 14 Mar 2025 04:00:00 GMT</pubDate>
    </item>
    </channel>
</rss>