<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 12 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>适配器反击</title>
      <link>https://arxiv.org/abs/2406.06820</link>
      <description><![CDATA[arXiv:2406.06820v1 公告类型：新
摘要：适配器提供了一种高效且轻量级的机制，用于将经过训练的 Transformer 模型适应各种不同的任务。然而，人们经常发现它们的表现不如其他适应机制，包括低秩适应。在本文中，我们对适配器、其内部结构以及各种实现选择进行了深入研究。我们发现了使用适配器的缺陷，并提出了一种具体的、改进的适配器架构，称为 Adapter+，它不仅优于以前的适配器实现，而且在几个具有挑战性的设置中超越了许多其他更复杂的适应机制。尽管如此，我们建议的适配器非常稳健，而且与以前的工作不同，在解决新场景时几乎不需要人工干预。即使没有每个任务的超参数优化，Adapter+ 在 VTAB 基准上也达到了最先进的平均准确率。]]></description>
      <guid>https://arxiv.org/abs/2406.06820</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>HO-Cap：用于手与物体交互的 3D 重建和姿势跟踪的捕获系统和数据集</title>
      <link>https://arxiv.org/abs/2406.06843</link>
      <description><![CDATA[arXiv:2406.06843v1 公告类型：新
摘要：我们介绍了一个数据捕获系统和一个名为 HO-Cap 的新数据集，可用于研究视频中手和物体的 3D 重建和姿势跟踪。捕获系统使用多个 RGB-D 摄像头和一个 HoloLens 耳机进行数据收集，避免使用昂贵的 3D 扫描仪或动作捕捉系统。我们提出了一种半自动方法来获取收集的视频中手和物体的形状和姿势的注释，与手动标记相比，这大大减少了所需的注释时间。通过这个系统，我们捕获了一个视频数据集，其中包含人类使用物体执行不同的任务，以及从一只手到另一只手的简单拾取和放置和交接物体，这些可以用作具身人工智能和机器人操作研究的人类演示。我们的数据捕获设置和注释框架可供社区用于重建物体和人手的 3D 形状并跟踪它们在视频中的姿势。]]></description>
      <guid>https://arxiv.org/abs/2406.06843</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>FlexLoc：用于分布式多模态传感器物体定位的零样本传感器透视不变性的条件神经网络</title>
      <link>https://arxiv.org/abs/2406.06796</link>
      <description><![CDATA[arXiv:2406.06796v1 公告类型：新
摘要：定位是从导航和监视到辅助生活等各种应用的关键技术。定位系统通常融合来自从不同角度观察场景的传感器的信息来估计目标位置，同时还采用多种模式来增强稳健性和准确性。最近，此类系统采用了在大型数据集上训练的端到端深度神经模型，因为它们具有出色的性能和处理来自不同传感器模式的数据的能力。然而，这种神经模型通常是在从一组特定的传感器姿势（即位置和方向）收集的数据上进行训练的。在实际部署过程中，这些传感器姿势的轻微偏差都可能导致极端不准确。为了应对这一挑战，我们引入了 FlexLoc，它采用条件神经网络注入节点视角信息来调整定位管道。具体而言，一小部分模型权重是在运行时从节点姿势得出的，从而能够以最小的额外开销准确地推广到看不见的视角。我们对多模态、多视角室内跟踪数据集的评估表明，与基线相比，FlexLoc 在零样本情况下（没有可用的校准数据）将定位精度提高了近 50%。FlexLoc 的源代码可在 https://github.com/nesl/FlexLoc 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.06796</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>用于无源域自适应分割的稳定邻域去噪</title>
      <link>https://arxiv.org/abs/2406.06813</link>
      <description><![CDATA[arXiv:2406.06813v1 公告类型：新
摘要：我们研究了用于语义分割的无源无监督域自适应（SFUDA），旨在使源训练模型适应目标域而无需访问源数据。已经提出了许多工作来解决这个具有挑战性的问题，其中基于不确定性的自训练是一种主要方法。然而，如果没有全面的去噪机制，它们在处理不同领域和确认偏差时仍然会在很大程度上陷入有偏差的估计。在本文中，我们观察到伪标签噪声主要包含在不稳定样本中，其中大多数像素的预测在自训练过程中会发生显着变化。受此启发，我们提出了一种用稳定样本去噪不稳定样本的新机制。具体而言，我们引入了稳定邻居去噪（SND）方法，该方法通过最近邻检索有效地发现高度相关的稳定和不稳定样本，并通过双层学习指导不稳定样本的可靠优化。此外，我们通过对象级对象粘贴来补偿稳定集，这可以进一步消除较少学习的类别造成的偏差。我们的 SND 具有两个优点。首先，SND 不需要特定的分割器结构，从而赋予其通用性。其次，SND 在适应过程中同时解决了类别、领域和确认偏差的问题，确保了其有效性。大量实验表明，SND 在各种 SFUDA 语义分割设置中始终优于最先进的方法。此外，SND 可以轻松地与其他方法集成，从而获得进一步的改进。]]></description>
      <guid>https://arxiv.org/abs/2406.06813</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>SeeFar：适用于地理空间基础模型的卫星无关多分辨率数据集</title>
      <link>https://arxiv.org/abs/2406.06776</link>
      <description><![CDATA[arXiv:2406.06776v1 公告类型：新
摘要：SeeFar 是一个不断发展的多分辨率卫星图像集合，来自公共和商业卫星。我们专门策划了这个数据集，用于训练地理空间基础模型，不受卫星类型的限制。近年来，技术的进步使卫星图像比以往任何时候都更容易获得。过去五年发射的地球观测卫星比前五十年发射的还要多。现代商业卫星现在提供的空间分辨率是公共访问卫星的 100 倍。然而，商业卫星图像的高成本和有限的历史可用性阻碍了基础模型的训练，影响了推理过程中可以使用哪些图像。SeeFar 数据集代表了通过结合多分辨率商业和公共访问预处理图像，朝着训练与卫星无关的模型迈出了一步。这将使用户能够利用历史数据以及更高分辨率、更昂贵的卫星图像，从而在推理过程中提供更大的灵活性。为了实现这一目标，我们描述了一种标准化来自不同卫星源的数据、规范不同数据格式以及对齐光谱带以增强互操作性的过程。SeeFar 数据集包括分辨率为 384x384 像素的图像，涵盖四个光谱带（蓝、绿、红和近红外）和扩展的空间分辨率（从 30、10、1.5 和 1.0 米开始），全部采用云优化的 GeoTIFF 格式。它还提供一致且全面的元数据，以提高数据透明度和可靠性。通过汇总来自多个来源的数据，SeeFar 使更广泛的用户（从研究人员到决策者）可以访问经过处理且一致的卫星数据，从而促进卫星图像分析的竞争和创新。数据集可在 \url{coastalcarbon.ai/seefar} 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.06776</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>MolX：通过多模态扩展增强分子学习的大型语言模型</title>
      <link>https://arxiv.org/abs/2406.06777</link>
      <description><![CDATA[arXiv:2406.06777v1 公告类型：新
摘要：最近，大型语言模型 (LLM) 凭借其强大的任务处理能力，在各个领域取得了显著的进步，超越了自然语言理解。然而，它们在化学领域的熟练程度仍然有限​​，特别是在解决专业的分子相关任务方面。这一挑战归因于它们在理解分子时仅使用常见文本表示（即 SMILES 字符串）的固有局限性。在本研究中，我们试图通过设计和为 LLM 配备多模态外部模块（即 MolX）来增强其理解分子的能力。具体来说，我们不是直接使用 SMILES 字符串来表示分子，而是利用特定的编码器从 SMILES 字符串和 2D 分子图表示中提取细粒度特征以输入到 LLM 中。此外，还结合了人类定义的分子指纹以利用其嵌入的领域知识。然后，为了在 MolX 和 LLM 的文本输入空间之间建立对齐，使用包括多种任务的多功能策略对 LLM 冻结的整个模型进行预训练。大量实验评估表明，我们提出的方法仅引入了少量可训练参数，而在对 LLM 进行微调和不微调的情况下，在从分子到文本的翻译到逆合成等各种下游分子相关任务上都优于基线。]]></description>
      <guid>https://arxiv.org/abs/2406.06777</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>TRINS：迈向可阅读的多模态语言模型</title>
      <link>https://arxiv.org/abs/2406.06730</link>
      <description><![CDATA[arXiv:2406.06730v1 公告类型：新
摘要：大型多模态语言模型在理解和编辑图像方面表现出非凡的能力。然而，这些视觉调整模型中的大多数都难以理解图像中嵌入的文本内容，这主要是由于训练数据的限制。在这项工作中，我们引入了 TRINS：一个文本丰富的图像指令数据集，目的是增强多模态大型语言模型的阅读能力。TRINS 建立在 LAION 之上，使用混合数据注释策略，包括机器辅助和人工辅助注释过程。它包含 39,153 张文本丰富的图像、标题和 102,437 个问题。具体来说，我们表明 TRINS 中每个注释的单词数明显长于相关数据集，这带来了新的挑战。此外，我们引入了一个简单有效的架构，称为语言视觉阅读助手 (LaRA)，它擅长理解图像中的文本内容。 LaRA 在 TRINS 数据集以及其他经典基准上的表现优于现有的最先进的多模态大型语言模型。最后，我们使用 TRINS 对各种文本丰富的图像理解和生成任务进行了全面的评估，证明了其有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.06730</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>用于高光谱分离的椭圆核无监督自编码器-图卷积网络集成模型</title>
      <link>https://arxiv.org/abs/2406.06742</link>
      <description><![CDATA[arXiv:2406.06742v1 公告类型：新
摘要：光谱解混是遥感中一种重要的技术，用于分析高光谱图像以识别端元并估计丰度图。在过去的几十年里，端元提取和分数丰度图估计技术的性能得到了显著提高。本文介绍了一种集成模型工作流程，称为自动编码器图集成模型 (AEGEM)，旨在提取端元和分数丰度图。应用椭圆核来测量光谱距离，生成椭圆邻域内的邻接矩阵。此信息用于构建椭圆图，其中质心作为发送方，几何内的剩余像素作为接收方。下一步是将丰度图、发送方和接收方作为输入堆叠到图卷积网络，该网络处理此输入以细化丰度图。最后，集成决策过程根据均方根误差度量确定最佳丰度图。使用 Samson、Jasper 和 Urban 等基准数据集对所提出的 AEGEM 进行了评估，其结果优于基线算法获得的结果。对于 Samson 数据集，AEGEM 在三种丰度图方面表现出色：水、树和土壤，分别产生 0.081、0.158 和 0.182 的值。对于 Jasper 数据集，树和水端元的结果有所改善，值依次为 0.035 和 0.060，光谱角距离度量的平均值也为 0.109。对于 Urban 数据集，AEGEM 在屋顶和沥青丰度图方面的表现优于之前的结果，分别达到 0.135 和 0.240 的值。此外，对于草地和屋顶的端元，AEGEM 达到 0.063 和 0.094 的值。]]></description>
      <guid>https://arxiv.org/abs/2406.06742</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    <item>
      <title>PatchRefiner：利用合成数据进行实域高分辨率单目度量深度估计</title>
      <link>https://arxiv.org/abs/2406.06679</link>
      <description><![CDATA[arXiv:2406.06679v1 公告类型：新
摘要：本文介绍了 PatchRefiner，这是一种用于度量单图像深度估计的高级框架，旨在实现高分辨率实域输入。虽然深度估计对于自动驾驶、3D 生成建模和 3D 重建等应用至关重要，但由于现有架构的限制以及详细的现实世界深度数据的稀缺，在现实世界场景中实现准确的高分辨率深度具有挑战性。PatchRefiner 采用基于图块的方法，将高分辨率深度估计重新概念化为细化过程，从而显着提高性能。利用利用合成数据的伪标记策略，PatchRefiner 结合了细节和尺度解缠 (DSD) 损失来增强细节捕捉，同时保持尺度准确性，从而促进知识从合成数据到现实世界数据的有效转移。我们广泛的评估证明了 PatchRefiner 的卓越性能，在均方根误差 (RMSE) 方面显著超越 Unreal4KStereo 数据集上的现有基准 18.1%，并在 CityScape、ScanNet++ 和 ETH3D 等各种真实世界数据集上的细节准确性和一致尺度估计方面显示出显着的改善。]]></description>
      <guid>https://arxiv.org/abs/2406.06679</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>使用混合 X3D-SlowFast 网络进行基于视频的运动分类和激活肌肉群预测</title>
      <link>https://arxiv.org/abs/2406.06703</link>
      <description><![CDATA[arXiv:2406.06703v1 公告类型：新
摘要：本文介绍了一种简单而有效的运动分类和肌肉群激活预测 (MGAP) 策略。这些任务对个人健身具有重要意义，有助于实现更实惠、更方便、更安全、更简单的锻炼程序。这对新手和残疾人士尤其重要。该领域的先前研究主要依赖安装的传感器和有限的锻炼范围，降低了日常使用的实用性。此外，现有的 MGAP 方法也存在对传感器的类似依赖和肌肉群范围受限的问题，通常不包括力量训练练习，而力量训练练习对于全面的健身方案至关重要。为了解决这些限制，我们的研究采用了基于视频的深度学习框架，该框架涵盖了广泛的锻炼和肌肉群，包括对力量训练至关重要的锻炼和肌肉群。利用“锻炼/锻炼视频”数据集，我们的方法以有效的方式集成了 X3D 和 SlowFast 视频活动识别模型，以增强锻炼分类和 MGAP 性能。我们的研究结果表明，通过加权集成获得的这种混合方法在准确度方面优于现有的基线模型。预训练模型在提高整体性能方面起着至关重要的作用，SlowFast 模型的最佳通道减少值确定在 10 附近。通过探索微调的消融研究，我们进一步阐明了这两项任务之间的相互关系。我们的复合模型是 X3D 和 SlowFast 的加权平均集成，在所有评估类别中为运动分类和 MGAP 树立了新的标杆，为以前方法的局限性提供了强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2406.06703</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:37 GMT</pubDate>
    </item>
    <item>
      <title>MatFusion：用于 SVBRDF 捕获的生成扩散模型</title>
      <link>https://arxiv.org/abs/2406.06539</link>
      <description><![CDATA[arXiv:2406.06539v1 公告类型：新
摘要：我们将照片中的 SVBRDF 估计公式化为扩散任务。为了对空间变化材料的分布进行建模，我们首先在 312,165 个合成空间变化材料样本的大型数据集上训练一种新颖的无条件 SVBRDF 扩散主干模型。然后，这个名为 MatFusion 的 SVBRDF 扩散主干模型可以作为改进条件扩散模型的基础，以在受控或不受控的照明下从照片中估计材料特性。我们的主干 MatFusion 模型仅使用反射特性的损失进行训练，因此改进可以与更昂贵的渲染方法配对，而无需在训练期间进行反向传播。由于条件 SVBRDF 扩散模型是生成性的，我们可以从同一张输入照片中合成多个 SVBRDF 估计值，用户可以从中选择最符合用户期望的估计值。我们通过改进基于不同类型的入射光的不同 SVBRDF 扩散模型来证明我们方法的灵活性，并表明对于在同一位置的闪光灯下拍摄的单张照片，我们的方法实现了与现有 SVBRDF 估计方法相同或更好的精度。]]></description>
      <guid>https://arxiv.org/abs/2406.06539</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>SEE-2-SOUND：零样本空间环境到空间声音</title>
      <link>https://arxiv.org/abs/2406.06612</link>
      <description><![CDATA[arXiv:2406.06612v1 公告类型：新
摘要：生成视觉和听觉相结合的感官体验对于沉浸式内容的消费至关重要。神经生成模型的最新进展使得能够创建跨多种模式（例如图像、文本、语音和视频）的高分辨率内容。尽管取得了这些成功，但在生成补充生成的视觉内容的高质量空间音频方面仍然存在很大差距。此外，当前的音频生成模型在生成自然音频、语音或音乐方面表现出色，但在整合沉浸式体验所需的空间音频提示方面却存在不足。在这项工作中，我们引入了 SEE-2-SOUND，这是一种零样本方法，将任务分解为 (1) 识别感兴趣的视觉区域；(2) 在 3D 空间中定位这些元素；(3) 为每个元素生成单声道音频；(4) 将它们集成到空间音频中。使用我们的框架，我们展示了为来自互联网的高质量视频、图像和动态图像以及通过学习方法生成的媒体生成空间音频的令人信服的结果。]]></description>
      <guid>https://arxiv.org/abs/2406.06612</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:36 GMT</pubDate>
    </item>
    <item>
      <title>利用图形生成实现增强域自适应对象检测</title>
      <link>https://arxiv.org/abs/2406.06535</link>
      <description><![CDATA[arXiv:2406.06535v1 公告类型：新
摘要：对象检测领域的领域自适应问题涉及将对象检测模型从标记的源域转移到未注释的目标域。该领域的最新进展旨在通过在非欧几里得图形空间内跨域对齐像素对来解决域差异，从而最小化语义分布方差。尽管取得了显著的成就，但这些方法通常使用粗略的语义表示来建模图形，主要是因为忽略了非信息元素并且未能专注于精确的语义对齐。此外，粗图的生成固有地引入了异常节点，带来了挑战并可能偏向域适应结果。因此，我们提出了一个框架，该框架利用图形生成来提高 DAOD（\method{}）的质量。具体来说，我们引入了一个节点细化模块，该模块利用存储库重建嘈杂的采样节点，同时对嘈杂的特征应用对比正则化。为了增强语义对齐，我们建议将领域特定样式与图协方差中编码的类别不变性分开，这使我们能够有选择地删除领域特定样式，同时保留类别不变信息，从而促进不同领域之间更准确的语义对齐。此外，我们提出了一个图优化适配器，利用变分推理来减轻异常节点的影响。在三个适应基准上进行的大量实验验证了 \method{} 在无监督领域适应任务中实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2406.06535</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>理解基于注意力机制的编码器-解码器网络：以国际象棋记分表识别为例</title>
      <link>https://arxiv.org/abs/2406.06538</link>
      <description><![CDATA[arXiv:2406.06538v1 公告类型：新
摘要：深度神经网络主要用于复杂的预测任务。有大量经验证据表明，它们可以成功地完成各种任务的端到端训练。成功通常仅基于训练后网络的最终性能来衡量，而对它们何时、为何以及如何工作的解释则较少被强调。在本文中，我们研究了具有注意机制的编码器-解码器循环神经网络，用于阅读手写国际象棋记分表的任务。与预测性能相比，我们关注的是更好地了解此类网络中的学习方式。我们将任务描述为三个子任务，即输入输出对齐、顺序模式识别和手写识别，并通过实验研究哪些因素会影响它们的学习。我们确定了子任务之间的竞争、协作和依赖关系，并认为这些知识可能有助于人们更好地平衡因素以正确训练网络。]]></description>
      <guid>https://arxiv.org/abs/2406.06538</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:35 GMT</pubDate>
    </item>
    <item>
      <title>用于图像分类的压缩元光学编码器</title>
      <link>https://arxiv.org/abs/2406.06534</link>
      <description><![CDATA[arXiv:2406.06534v1 公告类型：新
摘要：光学和混合卷积神经网络 (CNN) 最近越来越受到关注，以实现低延迟、低功耗图像分类和计算机视觉任务。然而，实现光学非线性具有挑战性，并且在标准 CNN 中省略非线性层会显著降低准确性。在这项工作中，我们使用知识蒸馏将修改后的 AlexNet 压缩为单个线性卷积层和电子后端（两个完全连接的层）。我们获得的性能与具有五个卷积层和三个完全连接层的纯电子 CNN 相当。我们通过设计逆设计的元光学的点扩展函数来光学实现卷积。使用这种混合方法，我们估计乘法累加运算从传统电子修改的 AlexNet 中的 688M 减少到光学前端启用的混合压缩网络中的仅 86K。这构成了延迟和功耗的四个数量级的降低。此外，我们通过实验证明该系统在 MNIST 数据集上的分类准确率超过 93％。]]></description>
      <guid>https://arxiv.org/abs/2406.06534</guid>
      <pubDate>Thu, 13 Jun 2024 03:16:34 GMT</pubDate>
    </item>
    </channel>
</rss>