<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Wed, 17 Jan 2024 06:18:40 GMT</lastBuildDate>
    <item>
      <title>RALAC：使用交互编码和光流的自动驾驶车辆动作识别。 （arXiv：2209.14408v3 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2209.14408</link>
      <description><![CDATA[当应用于自动驾驶汽车 (AV) 设置时，动作识别可以
增强环境模型的态势感知。这尤其是
普遍存在于传统几何描述和启发式的场景中
AV方面的不足。然而，动作识别传统上是
针对人类进行的研究，其对嘈杂、未剪辑、
未经处理的原始 RGB 数据限制了其在其他领域的应用。推
为了在自动驾驶汽车中推进和采用动作识别，这项工作
提出了一种新颖的两阶段动作识别系统，称为 RALAC。拉拉克
提出了道路场景的动作识别问题，并弥合了
它与人类行为识别的既定领域之间存在差距。这部作品
展示了注意力层如何有助于编码跨域之间的关系
代理，并强调这样的方案如何与类别无关。此外，为了
为了解决道路上智能体的动态特性，RALAC 构建了一种新颖的
调整感兴趣区域 (ROI) 与代理轨迹对齐的方法
下游动作分类。最后，我们的方案还考虑了
活性剂检测问题，并利用融合的新颖应用
光流图可识别道路场景中的相关代理。我们表明我们的
所提出的方案可以超越 ICCV2021 道路挑战赛的基线
数据集并通过将其部署在实车平台上，我们提供了初步的
洞察行动识别在决策中的有用性。
]]></description>
      <guid>http://arxiv.org/abs/2209.14408</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:40 GMT</pubDate>
    </item>
    <item>
      <title>从激励中理解 CNN。 （arXiv：2205.00932v3 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2205.00932</link>
      <description><![CDATA[显着图已被证明是一种非常有效的解释方法
卷积神经网络的决策。然而，现有的方法论
主要依赖梯度，这限制了他们解释的能力
复杂的模型。此外，此类方法并不完全擅长利用
负梯度信息以提高解释的准确性。在这项研究中，
我们提出了一个新的概念，称为正激励和负激励，
能够直接提取每个的正和负激励
层，从而实现完整的逐层信息利用
梯度。为了将这些激励组织成最终的显着性图，我们引入
双链反向传播过程。综合实验
评估，包括二分类和多分类
任务，旨在衡量我们提出的方法的有效性。
令人鼓舞的是，结果表明我们的方法提供了显着的效果
在显着像素方面对最先进方法的改进
去除、小像素去除和不明显的对抗性扰动
一代指导。此外，我们验证了正向之间的相关性
和负激励。
]]></description>
      <guid>http://arxiv.org/abs/2205.00932</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:39 GMT</pubDate>
    </item>
    <item>
      <title>你实际上看了两次（YALTAi）：在 Kraken 引擎中使用对象检测方法而不是区域分割。 （arXiv：2207.11230v2 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2207.11230</link>
      <description><![CDATA[布局分析（区域的识别及其分类）是
光学字符识别和类似中的线分割的第一步
任务。从边缘文本或文本中识别文本主体的能力
运行标题使得提取作品全文有所不同
数字化书籍和嘈杂的输出。我们表明大多数分割器都关注像素
分类并且该输出的多边形化尚未用作
历史文献最新竞赛的目标（ICDAR 2017 和
尽管是 2010 年代初的焦点。我们建议进行转变，因为
效率，任务从基于像素分类的多边形化到
使用等角矩形进行物体检测。我们比较 Kraken 的输出
和 YOLOv5 在分割方面并表明后者严重
在小数据集（1110 个样本及以下）上优于第一个。我们发布
两个用于历史文档训练和评估的数据集以及
新包 YALTAi，它将 YOLOv5 注入到
海妖 4.1。
]]></description>
      <guid>http://arxiv.org/abs/2207.11230</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:39 GMT</pubDate>
    </item>
    <item>
      <title>Lirot.ai：一个用于众包视网膜图像分割的新颖平台。 （arXiv：2208.10100v2 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2208.10100</link>
      <description><![CDATA[简介：对于有监督的深度学习 (DL) 任务，研究人员需要
大型注释数据集。在医学数据科学中，主要限制之一
开发DL模型缺乏大量带注释的例子。这
最常见的是由于注释所需的时间和专业知识。我们介绍
利罗特。 ai，一个促进和众包图像的新颖平台
细分。方法：利罗特。 ai由三个部分组成； iPadOS
名为 Lirot 的客户端应用程序。 ai-app，一个名为 Lirot 的后端服务器。人工智能服务器
和一个 Python API 名称 Lirot。 ai-API。利罗特。 ai-app 是在 Swift 5.6 中开发的
和利罗特。 ai-server 是一个 firebase 后端。利罗特。 ai-API允许管理
数据库的。利罗特。 ai-app 可以安装在尽可能多的 iPadOS 设备上
需要注释者能够执行他们的分割
同时和远程。我们融入了 Apple Pencil 兼容性，使得
对于专家来说，分割更快、更准确、更直观
任何其他基于计算机的替代方案。结果：我们演示了
利罗特。 ai用于创建视网膜眼底数据集并提供参考
脉管系统分割。讨论和未来的工作：我们将使用主动
继续扩大我们的视网膜眼底数据集的学习策略
包括更有效的过程来选择要注释的图像和
将它们分发给注释者。
]]></description>
      <guid>http://arxiv.org/abs/2208.10100</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:39 GMT</pubDate>
    </item>
    <item>
      <title>用于无监督视频对象分割的隐式运动补偿网络。 （arXiv：2204.02791v2 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2204.02791</link>
      <description><![CDATA[无监督视频对象分割（UVOS）旨在自动
将视频中的主要前景对象与背景分开
顺序。现有的 UVOS 方法要么在视觉上缺乏鲁棒性，要么缺乏鲁棒性。
类似的环境（基于外观）或遭受恶化
由于动态背景和不准确的流程，他们的预测质量受到影响
（基于流量）。为了克服这些限制，我们提出了一个隐式的
结合互补线索的运动补偿网络 (IMCNet)
（$\textit{i.e.}$，外观和运动）与来自对齐的运动信息
在特征级别上与当前帧相邻的帧没有
估计光流。所提出的 IMCNet 由亲和力计算组成
模块（ACM）、注意力传播模块（APM）和运动补偿
模块（MCM）。轻量级 ACM 提取相邻节点之间的共性
基于外观特征的输入帧。然后APM发送全局
以自上而下的方式进行关联。通过由粗到细的迭代启发，
APM 将从多个分辨率中细化对象区域，以便
有效避免丢失细节。最后，MCM对齐运动信息
从时间上相邻的帧到当前帧，这实现了隐式
特征级别的运动补偿。我们进行了广泛的实验
$\textit{DAVIS}_{\textit{16}}$ 和 $\textit{YouTube-Objects}$。我们的网络
与相比，以更快的速度运行时实现了良好的性能
最先进的方法。
]]></description>
      <guid>http://arxiv.org/abs/2204.02791</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:38 GMT</pubDate>
    </item>
    <item>
      <title>具有双稀疏光流分解的拉格朗日运动放大。 （arXiv：2204.07636v2 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2204.07636</link>
      <description><![CDATA[微表情是快速且空间较小的面部表情，
很难检测到。因此运动放大技术的目的是
放大并从而揭示视频中的微妙运动，对于以下方面似乎很有用
处理此类表达式。基本上有两种主要方法，即通过
欧拉或拉格朗日技术。虽然第一个放大了运动
通过直接对图像像素进行隐式操作，拉格朗日方法使用
光流（OF）技术提取和放大像素轨迹。在这个
论文中，我们提出了一种局部拉格朗日运动放大的新方法
面部微动作。我们的贡献有三方面：首先，我们微调
OFs 的循环全对场变换 (RAFT) 深度学习方法
通过添加从变分密集逆搜索获得的地面实况来面孔
(DIS)用于OF算法应用于面部微距CASME II视频集
表达式。这使我们能够高效地制作面部视频的 OF
和足够准确的方式。其次，由于面部微动作既
在空间和时间上都是局部的，我们建议通过稀疏来近似 OF 场
空间和时间上的分量导致双重稀疏分解。
第三，我们使用这种分解来放大特定区域的微运动
面，我们使用三角形引入了一种新的前向扭曲策略
图像网格的分割和 RGB 向量的重心插值
变换后的三角形的角。我们论证了我们的可行性
通过各种例子的方法。
]]></description>
      <guid>http://arxiv.org/abs/2204.07636</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:38 GMT</pubDate>
    </item>
    <item>
      <title>用于多标签图像分类的图注意力变换器网络。 （arXiv：2203.04049v2 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2203.04049</link>
      <description><![CDATA[多标签分类旨在识别多个对象或属性
从图像。然而，从正确的标签图中学习来学习是具有挑战性的
有效地表征这种标签间的相关性或依赖性。当前的
方法通常使用基于训练的标签共现概率
设置为邻接矩阵来建模这种相关性，这有很大的限制
受数据集影响并影响模型的泛化能力。在本文中，
我们提出了一个图注意力变换网络（GATN），一个通用框架
用于多标签图像分类，可以有效挖掘复杂的
标签间的关系。首先，我们使用基于余弦相似度
将标签词嵌入作为初始相关矩阵，可以表示
丰富的语义信息。随后，我们设计了图注意力
Transformer层来传输这个邻接矩阵以适应当前
领域。我们广泛的实验证明我们提出的方法
可以在三个数据集上实现最先进的性能。
]]></description>
      <guid>http://arxiv.org/abs/2203.04049</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:37 GMT</pubDate>
    </item>
    <item>
      <title>学习盲图像超分辨率的退化分布。 （arXiv：2203.04962v2 [eess.IV] 已更新）</title>
      <link>http://arxiv.org/abs/2203.04962</link>
      <description><![CDATA[合成高分辨率 (HR) \&amp;低分辨率（LR）对被广泛使用
在现有的超分辨率（SR）方法中。为了避免域之间的差距
合成图像和测试图像，以前的大多数方法都尝试自适应地学习
通过确定性模型合成（降级）过程。然而，一些
实际场景中的退化是随机的，不能由
图像的内容。这些确定性模型可能无法对随机模型进行建模
因素和与内容无关的降解部分，这将限制
以下 SR 型号的性能。在本文中，我们提出了一个
概率退化模型（PDM），研究退化
$\mathbf{D}$ 作为随机变量，并通过建模来学习其分布
从先验随机变量$\mathbf{z}$到$\mathbf{D}$的映射。比较的
与之前的确定性退化模型相比，PDM 可以建模更加多样化
退化并生成 HR-LR 对，可以更好地覆盖各种
测试图像的退化，从而防止SR模型过度拟合
具体的。大量的实验表明我们的降解
模型可以帮助SR模型在不同数据集上取得更好的性能。
源代码发布于 \url{git@github.com:greatlog/UnpairedSR.git}。
]]></description>
      <guid>http://arxiv.org/abs/2203.04962</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:37 GMT</pubDate>
    </item>
    <item>
      <title>谷歌搜索成功就完成了一半：利用基于图像的谷歌趋势对新时尚产品销售进行多模式预测。 （arXiv：2109.09824v6 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2109.09824</link>
      <description><![CDATA[新时尚产品销售预测是一个具有挑战性的问题，涉及
许多业务动态无法通过经典预测来解决
接近。在本文中，我们系统地研究了
以 Google Trends 时间序列的形式探索外生知识
将其与与全新时尚单品相关的多模式信息相结合，
尽管缺乏过去的数据，但仍可以有效地预测其销售额。在
特别是，我们提出了一种基于神经网络的方法，其中编码器学习
外生时间序列的表示，而解码器则预测
基于 Google 趋势编码以及可用视觉和元数据的销售
信息。我们的模型以非自回归方式工作，避免了
大的第一步误差的复合效应。作为第二个贡献，我们
展示 VISUELLE，一个用于新时尚任务的公开数据集
产品销售预测，包含 5577 个真实的多式联运信息，新
意大利快时尚公司 Nunalie 2016-2019 年间销售的产品。
该数据集配备了产品图像、元数据、相关销售和
相关的谷歌趋势。我们使用 VISUELLE 来比较我们的方法
最先进的替代方案和几个基线，表明我们的神经网络
基于网络的方法无论是在百分比还是在百分比方面都是最准确的
绝对错误。值得注意的是，外生知识的加入
按加权绝对值计算，预测准确度提高 1.5%
百分比误差 (WAPE)，揭示了利用信息的重要性
外部信息。代码和数据集均可在以下位置获取
https://github.com/HumaticsLAB/GTM-Transformer。
]]></description>
      <guid>http://arxiv.org/abs/2109.09824</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:36 GMT</pubDate>
    </item>
    <item>
      <title>优化可解释性：卷积动态对齐网络。 （arXiv：2109.13004v2 [stat.ML] 已更新）</title>
      <link>http://arxiv.org/abs/2109.13004</link>
      <description><![CDATA[我们引入了一个新的神经网络模型系列，称为卷积
动态对齐网络（CoDA Nets），是具有
高度固有的可解释性。他们的核心构建模块是
动态对齐单元 (DAU)，经过优化以转换其输入
具有与任务相关的动态计算的权重向量
模式。因此，CoDA Nets 通过以下方式对分类预测进行建模：
一系列依赖于输入的线性变换，允许线性
将输出分解为单独的输入贡献。鉴于
DAU 的对齐，所得的贡献图与
歧视性输入模式。这些模型固有的分解具有很高的
视觉质量并优于定量下的现有归因方法
指标。此外，CoDA Nets 构成了高性能分类器，达到了同等水平
ResNet 和 VGG 模型的结果CIFAR-10 和 TinyImagenet。最后，
CoDA Nets 可以与传统的神经网络模型相结合，以产生
强大的分类器，可以更轻松地扩展到复杂的数据集，例如
Imagenet 同时表现出增加的可解释深度，即输出
可以用内部中间层的贡献来很好地解释
网络。
]]></description>
      <guid>http://arxiv.org/abs/2109.13004</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:36 GMT</pubDate>
    </item>
    <item>
      <title>将自注意力修剪成单路径的卷积层。 （arXiv：2111.11802v4 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2111.11802</link>
      <description><![CDATA[Vision Transformers (ViTs) 在各种方面取得了令人印象深刻的性能
计算机视觉任务。然而，用多头建模全局相关性
自注意力（MSA）层导致了两个广泛认可的问题：
计算资源消耗和缺乏内在的归纳偏差
建模局部视觉模式。为了解决这两个问题，我们设计了一个简单但
称为单路径视觉变压器剪枝（SPViT）的有效方法，
高效、自动地将预训练的 ViT 压缩为紧凑模型
添加了适当的位置。具体来说，我们首先提出一部小说
MSA 和卷积运算之间的权重共享方案，提供
用于编码所有候选操作的单路径空间。通过这种方式，我们投出了
操作搜索问题是查找每个参数中使用哪个参数子集
MSA层，显着降低计算成本和优化
困难，并且可以使用以下方法很好地初始化卷积核
预训练的 MSA 参数。依靠单路径空间，我们引入
可学习的二元门对 MSA 层中的操作选择进行编码。
类似地，我们进一步采用可学习门来编码细粒度 MLP
FFN层的扩展率。通过这种方式，我们的 SPViT 优化了可学习的
从广阔且统一的搜索空间中自动探索的大门，以及
灵活调整每个个体密集的MSA-FFN剪枝比例
模型。我们对两个有代表性的 ViT 进行了广泛的实验，结果表明
我们的 SPViT 在 ImageNet-1k 上实现了新的剪枝 SOTA。例如，我们的
SPViT 可以将 DeiT-B 的 FLOP 减少 52.0%，并获得令人印象深刻的 0.6% top-1 准确率
同时增益。源代码位于
https://github.com/ziplab/SPViT。
]]></description>
      <guid>http://arxiv.org/abs/2111.11802</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:36 GMT</pubDate>
    </item>
    <item>
      <title>海洋除雪基准数据集。 （arXiv：2103.14249v3 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2103.14249</link>
      <description><![CDATA[本文介绍了一个新的海洋除雪基准数据集
水下图像。海雪是海洋生物退化的主要来源之一
由小颗粒（例如有机物和
沙子，位于水下场景和光电传感器之间。我们用数学模型
真实水下观测中的两种典型海洋雪类型
图片。将建模的伪影与水下图像合成
构建大规模的地面真实图像和退化图像对来计算
海洋除雪的客观质量和训练深度神经网络。
我们使用数据集提出了两个海洋除雪任务，并显示了第一个
海洋除雪的基准测试结果。海洋除雪
基准数据集可在线公开获取。
]]></description>
      <guid>http://arxiv.org/abs/2103.14249</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:35 GMT</pubDate>
    </item>
    <item>
      <title>在稀疏突触爆发的域转移下持续学习。 （arXiv：2108.12056v9 [cs.LG] 已更新）</title>
      <link>http://arxiv.org/abs/2108.12056</link>
      <description><![CDATA[现有机器是功能特定的工具，旨在轻松实现
预测和控制。明天的机器可能更接近生物系统
在于他们的可变性、弹性和自主性。但首先他们必须有能力
在不接触新信息的情况下学习和保留新信息
经常随意。过去设计此类系统的努力一直试图建立
或使用不相交的权重集来调节人工神经网络
对特定任务或输入特别敏感。该功能尚未启用
对以前未见过的数据的长序列进行持续学习，而无需
破坏现有知识：一个被称为灾难性遗忘的问题。在
在本文中，我们介绍了一个可以比以前顺序学习的系统
未见过的数据集（ImageNet、CIFAR-100）随着时间的推移很少被遗忘。这是
通过控制卷积神经网络中权重的活动来完成
基于使用第二个产生的自上而下调节的输入
前馈神经网络。我们发现我们的方法在
具有可回收权重的稀疏突发活动的域转移
跨任务，而不是通过维护特定于任务的模块。稀疏突触
发现爆发可以平衡活性和抑制，从而产生新功能
可以在不破坏现有知识的情况下学习，从而反映平衡
处于混乱边缘的系统的有序和无序。这种行为的出现
在之前的预训练（或“元学习”）阶段，其中受监管
突触从初始状态选择性地去抑制或生长
通过预测误差最小化进行均匀抑制。
]]></description>
      <guid>http://arxiv.org/abs/2108.12056</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:35 GMT</pubDate>
    </item>
    <item>
      <title>通过正交分集实现稳健的神经网络。 （arXiv：2010.12190v5 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2010.12190</link>
      <description><![CDATA[深度神经网络 (DNN) 很容易受到看不见的扰动的影响
由对抗性攻击生成的图像，这引发了对
DNN 的对抗鲁棒性。以一系列方法为代表
对抗性训练及其变体已被证明是最有效的方法之一
增强 DNN 鲁棒性的技术。一般来说，对抗性训练
专注于通过涉及扰动数据来丰富训练数据。这样的数据
对抗性训练中涉及的扰动数据的增强效应确实
对 DNN 本身的鲁棒性没有贡献，并且通常会受到干净的影响
精度下降。为了 DNN 本身的鲁棒性，我们在本文中提出了
旨在增强模型以学习以下特征的新颖防御
适应不同的输入，包括对抗性例子。更多的
具体来说，为了增强模型，将多个路径嵌入到
网络，并对这些路径施加正交性约束以保证
他们之间的多样性。然后设计保证金最大化损失
通过正交性（DIO）进一步增强这种多样性。这样，提出的
DIO 增强了模型并增强了 DNN 本身的鲁棒性
特征可以通过这些相互正交的路径来校正。广泛的
各种数据集、结构和攻击的实证结果验证了
所提出的 DIO 利用模型具有更强的对抗鲁棒性
增强。此外，DIO还可以与不同的数据灵活组合
增强技术（例如 TRADES 和 DDPM），进一步提高稳健性
收益。
]]></description>
      <guid>http://arxiv.org/abs/2010.12190</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:34 GMT</pubDate>
    </item>
    <item>
      <title>基于分解、压缩和合成 (DCS) 的视频编码：通过分辨率自适应学习进行神经探索。 （arXiv：2012.00650v5 [cs.CV] 已更新）</title>
      <link>http://arxiv.org/abs/2012.00650</link>
      <description><![CDATA[受到视网膜细胞实际上隔离视觉场景这一事实的启发
分为不同的属性（例如，空间细节、时间运动）
各自的神经元处理，我们建议首先分解输入视频
以其原始空间分辨率进入各自的空间纹理帧（STF）
保留丰富的空间细节和其他时间运动帧
（TMF）在较低的空间分辨率下保持运动平滑度；然后
使用任何流行的视频编码器将它们压缩在一起；最后合成
解码的 STF 和 TMF 同时用于高保真视频重建
分辨率作为其本机输入。这项工作只是应用了双三次重采样
分解和压缩中符合 HEVC 的编解码器，并将重点放在
合成部分。对于分辨率自适应合成，运动补偿
网络（MCN）是在TMF上设计的，以有效地对齐和聚合时间
将使用相应的 STF 联合处理运动特征
非局部纹理传输网络（NL-TTN）可以更好地增强空间细节，
从而可以有效地抑制压缩和分辨率重采样噪声
通过更好的率失真效率来缓解。这样的“分解，
基于压缩、合成（DCS）”的方案与编解码器无关，目前
举例说明平均 $\approx$1 dB PSNR 增益或 $\approx$25% BD 速率节省，
使用参考软件针对 HEVC 锚点。此外，实验
与最先进的方法和消融研究进行比较
进一步报告DCS算法的效率和泛化性，有希望
未来视频编码的一个令人鼓舞的方向。
]]></description>
      <guid>http://arxiv.org/abs/2012.00650</guid>
      <pubDate>Wed, 17 Jan 2024 06:18:34 GMT</pubDate>
    </item>
    </channel>
</rss>