<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 28 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>CharNeRF：从概念艺术生成 3D 角色</title>
      <link>https://arxiv.org/abs/2402.17115</link>
      <description><![CDATA[arXiv:2402.17115v1 公告类型：新
摘要：3D 建模在 AR/VR 和游戏领域具有重要意义，可以实现艺术创造力和实际应用。然而，这个过程通常非常耗时并且需要高水平的技能。在本文中，我们提出了一种从一致的周转概念艺术创建 3D 角色体积表示的新颖方法，该概念艺术作为 3D 建模行业的标准输入。虽然神经辐射场 (NeRF) 已经成为基于图像的 3D 重建领域的游戏规则改变者，但据我们所知，尚无已知的研究可以优化概念艺术的流程。为了利用概念艺术的潜力，通过其定义的身体姿势和特定的视角，我们建议将其编码为我们模型的先验。我们训练网络通过可学习的视图方向参与的多头自注意力层将这些先验用于各种 3D 点。此外，我们还证明了射线采样和表面采样的结合可以增强我们网络的推理能力。我们的模型能够生成高质量的 360 度角色视图。随后，我们提供了一个简单的指南，以更好地利用我们的模型来提取 3D 网格。值得注意的是，我们模型的推理能力受到训练数据特征的影响，主要关注单头、两条手臂和两条腿的角色。尽管如此，我们的方法仍然是通用的，并且适用于不同主题的概念艺术，而不对数据强加任何特定的假设。]]></description>
      <guid>https://arxiv.org/abs/2402.17115</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>OSCaR：对象状态描述和状态变化表示</title>
      <link>https://arxiv.org/abs/2402.17128</link>
      <description><![CDATA[arXiv:2402.17128v1 公告类型：新
摘要：智能模型推断和理解物体状态变化的能力是人工智能研究的一个至关重要但要求很高的方面，特别是通过现实世界环境中人类交互的视角。这项任务涉及描述复杂的视觉环境，识别活动对象，并解释通过语言传达的它们的变化。传统方法将对象描述和状态变化检测隔离开来，只能提供有限的动态环境视图。而且，依靠一小组符号词来表示变化，限制了语言的表达能力。为了应对这些挑战，在本文中，我们介绍了对象状态描述和状态变化表示（OSCaR）数据集和基准。 OSCaR 由 14,084 个带注释的视频片段组成，其中包含来自各种以自我为中心的视频集合中的近 1,000 个独特对象。它为评估多模式大语言模型（MLLM）设置了一个新的测试平台。我们的实验表明，虽然 MLLM 表现出一定的技能，但它们缺乏对对象状态变化的充分理解。该基准包括一个微调模型，尽管具有初始功能，但仍需要显着提高准确性和泛化能力，以便有效理解这些变化。我们的代码和数据集可在 https://github.com/nguyennm1024/OSCaR 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.17128</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:18 GMT</pubDate>
    </item>
    <item>
      <title>T-HITL 有效解决图像生成中的问题关联并保持整体视觉质量</title>
      <link>https://arxiv.org/abs/2402.17101</link>
      <description><![CDATA[arXiv:2402.17101v1 公告类型：新
摘要：生成式人工智能图像模型可能会无意中生成有问题的人物表示。过去的研究指出，世界各地有数以百万计的用户每天都在使用这些模型，而这些模型，包括通过对人进行有问题的表述，有可能加剧和加速现实世界的歧视和其他危害（Bianchi 等人，2023）。在本文中，我们重点解决人口群体和语义概念之间产生的有问题的关联，这些关联可能反映和强化社会数据中嵌入的负面叙述。基于社会学文献（Blumer，1958）并将表征映射到模型行为，我们开发了一种分类法来研究图像生成模型中的问题关联。我们探索了模型级别微调作为解决这些关联的方法的有效性，并将视觉质量的潜在降低确定为传统微调的限制。我们还提出了一种采用两次人机循环 (T-HITL) 的新方法，该方法有望在减少有问题的关联和保持视觉质量方面有所改进。我们通过提供 T-HITL 在模型级别解决的三个有问题的关联的证据来证明 T-HITL 的有效性。我们对学术的贡献有两个方面。通过在机器学习模型和生成人工智能的背景下定义有问题的关联，我们引入了概念和技术分类法来解决其中一些关联。最后，我们提供了一种方法 T-HITL，它可以解决这些关联并同时保持图像模型生成的视觉质量。这种缓解不一定是一种权衡，而是一种增强。]]></description>
      <guid>https://arxiv.org/abs/2402.17101</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>使用潜在透明度的透明图像层扩散</title>
      <link>https://arxiv.org/abs/2402.17113</link>
      <description><![CDATA[arXiv:2402.17113v1 公告类型：新
摘要：我们提出了 LayerDiffusion，一种使大规模预训练潜在扩散模型能够生成透明图像的方法。该方法允许生成单个透明图像或多个透明层。该方法学习“潜在透明度”，将 alpha 通道透明度编码到预训练潜在扩散模型的潜在流形中。它通过将增加的透明度调节为潜在偏移，并对预训练模型的原始潜在分布进行最小程度的更改，从而保留了大型扩散模型的生产就绪质量。这样，任何潜在扩散模型都可以通过使用调整后的潜在空间进行微调来转换为透明图像生成器。我们使用人机循环收集方案收集的 1M 透明图像层对来训练模型。我们表明，潜在透明度可以应用于不同的开源图像生成器，或者适应各种条件控制系统，以实现前景/背景条件层生成、联合层生成、层内容的结构控制等应用。 用户研究研究发现，在大多数情况下 (97%)，用户更喜欢我们原生生成的透明内容，而不是以前的临时解决方案（例如生成然后抠图）。用户还报告说，我们生成的透明图像的质量可与 Adob​​e Stock 等真正的商业透明资源相媲美。]]></description>
      <guid>https://arxiv.org/abs/2402.17113</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:17 GMT</pubDate>
    </item>
    <item>
      <title>使用数字图像相关性表征沥青混凝土：最佳实践、应用和未来愿景的系统回顾</title>
      <link>https://arxiv.org/abs/2402.17074</link>
      <description><![CDATA[arXiv:2402.17074v1 公告类型：新
摘要：数字图像相关（DIC）是一种光学技术，通过跟踪测试过程中捕获的图像序列中的图案运动来测量位移和应变。自 2000 年代初以来，DIC 在沥青路面工程领域获得了认可。然而，用户常常将 DIC 技术视为一种开箱即用的工具，并且对其操作和测量原理缺乏透彻的了解。本文对 DIC 作为沥青混凝土 (AC) 实验室测试的重要工具进行了最新综述，主要关注广泛使用的 2D-DIC 和 3D-DIC 技术。为了解决用户常见的问题，该评论彻底检查了准备散斑图案、配置单摄像头或双摄像头成像系统、进行 DIC 分析以及探索各种应用的最佳方法。此外，还介绍了数字体积相关和基于深度学习的 DIC 等新兴 DIC 方法，突出了它们在路面工程中未来应用的潜力。本文还提供了在 AC 表征中实现 DIC 的全面且可靠的流程图。最后，提出了未来研究的关键方向。]]></description>
      <guid>https://arxiv.org/abs/2402.17074</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>用于多类异常检测和定位的结构性师生正态学习</title>
      <link>https://arxiv.org/abs/2402.17091</link>
      <description><![CDATA[arXiv:2402.17091v1 公告类型：新
摘要：视觉异常检测是一项具有挑战性的开放集任务，旨在在对正常数据进行建模的同时识别未知的异常模式。通过利用师生网络特征比较，知识蒸馏范式在一类异常检测中表现出了卓越的性能。然而，将此范例扩展到多类异常检测会带来新的可扩展性挑战。在这项研究中，我们解决了以前的师生模型在应用于多类异常检测时观察到的显着性能下降问题，我们将其确定为跨类干扰造成的。为了解决这个问题，我们引入了一种称为结构性师生常态学习（SNL）的新方法：（1）我们提出了空间通道蒸馏和内部和内部亲和力蒸馏技术来测量教师和学生之间的结构距离网络。 （2）我们引入了一个中央残差聚合模块（CRAM）来封装学生网络的正常表示空间。我们在两个异常检测数据集 MVTecAD 和 VisA 上评估了我们提出的方法。在多类异常检测和定位任务中，我们的方法在 MVTecAD 上超过了最先进的基于蒸馏的算法，分别超过了 3.9% 和 1.5%，在 VisA 上超过了 1.2% 和 2.5%。此外，我们的算法在 MVTecAD 和 VisA 上均优于当前最先进的统一模型。]]></description>
      <guid>https://arxiv.org/abs/2402.17091</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>热红外物体跟踪贝叶斯过滤的防御和复兴</title>
      <link>https://arxiv.org/abs/2402.17098</link>
      <description><![CDATA[arXiv:2402.17098v1 公告类型：新
摘要：基于深度学习的方法垄断了热红外（TIR）目标跟踪领域的最新研究。然而，单纯依靠深度学习模型来获得更好的跟踪结果需要仔细选择有利于表示目标物体的特征信息并设计合理的模板更新策略，这无疑增加了模型设计的难度。因此，最近的 TIR 跟踪方法在复杂场景中面临着许多挑战。本文介绍了一种新颖的深度贝叶斯过滤 (DBF) 方法，可在这些具有挑战性的情况下增强 TIR 跟踪。 DBF 的独特之处在于其双模型结构：系统模型和观测模型。系统模型利用运动数据基于二维布朗运动来估计目标物体的潜在位置，从而生成先验概率。此后，观察模型在捕获 TIR 图像时发挥作用。它充当分类器并利用红外信息来确定这些估计位置的可能性，从而创建似然概率。根据两个模型的引导，可以确定目标物体的位置，并且可以动态更新模板。对多个基准数据集的实验分析表明，DBF 实现了具有竞争力的性能，在复杂场景中超越了大多数现有的 TIR 跟踪方法。]]></description>
      <guid>https://arxiv.org/abs/2402.17098</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:16 GMT</pubDate>
    </item>
    <item>
      <title>使用卷积神经网络激活功能进行离线作家识别</title>
      <link>https://arxiv.org/abs/2402.17029</link>
      <description><![CDATA[arXiv:2402.17029v1 公告类型：新
摘要：卷积神经网络（CNN）最近已成为大规模图像分类的最先进工具。在这项工作中，我们建议使用 CNN 的激活特征作为作者识别的局部描述符。然后通过 GMM 超向量编码形成全局描述符，并通过 KL-Kernel 归一化进一步改进。我们在两个公开可用的数据集上评估我们的方法：ICDAR 2013 基准数据库和 CVL 数据集。虽然我们在 CVL 上的表现与最先进的技术相当，但我们提出的方法在具有挑战性的双语 ICDAR 数据集上的 mAP 方面产生了约 0.21 的绝对改进。]]></description>
      <guid>https://arxiv.org/abs/2402.17029</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>HOISDF：使用全局有符号距离场约束 3D 手部物体姿势估计</title>
      <link>https://arxiv.org/abs/2402.17062</link>
      <description><![CDATA[arXiv:2402.17062v1 公告类型：新
摘要：人类的双手在处理物体方面具有高度的灵活性和多功能性。由于频繁的遮挡，通过单目相机联合估计手部及其操作的物体的 3D 姿势非常具有挑战性。因此，现有方法通常依赖于中间 3D 形状表示来提高性能。这些表示通常是明确的，例如 3D 点云或网格，因此提供中间手部姿势估计的直接环境中的信息。为了解决这个问题，我们引入了 HOISDF，一种有符号距离场 (SDF) 引导的手部物体姿态估计网络，它联合利用手部和物体 SDF 在整个重建体积上提供全局、隐式表示。具体来说，SDF 的作用有三重：为视觉编码器配备隐式形状信息，帮助编码手部与物体的交互，并通过基于 SDF 的采样和增强特征表示来指导手部和物体姿势回归。我们证明 HOISDF 在手部物体姿势估计基准（DexYCB 和 HO3Dv2）上取得了最先进的结果。代码可在 https://github.com/amathislab/HOISDF 获取]]></description>
      <guid>https://arxiv.org/abs/2402.17062</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>驯服类条件 GAN 中的尾部：通过较低分辨率的无条件训练实现知识共享</title>
      <link>https://arxiv.org/abs/2402.17065</link>
      <description><![CDATA[arXiv:2402.17065v1 公告类型：新
摘要：尽管对使用有限的训练数据训练生成对抗网络（GAN）进行了广泛的研究，但学习从长尾训练分布生成图像仍然尚未得到探索。在存在不平衡的多类训练数据的情况下，GAN 倾向于偏爱样本较多的类，从而导致尾部类中生成低质量且多样性较低的样本。在这项研究中，我们的目标是利用长尾数据改进类条件 GAN 的训练。我们提出了一种简单而有效的知识共享方法，允许尾类从具有更丰富训练数据的类中借用丰富的信息。更具体地说，我们建议对现有的类条件 GAN 架构进行修改，以确保生成器的较低分辨率层完全无条件地进行训练，同时为较高分辨率层保留类条件生成。对多个长尾基准和 GAN 架构的实验表明，在生成图像的多样性和保真度方面，现有方法都有显着改进。该代码可在 https://github.com/khorrams/utlo 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.17065</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:15 GMT</pubDate>
    </item>
    <item>
      <title>显着性自动佛像识别</title>
      <link>https://arxiv.org/abs/2402.16980</link>
      <description><![CDATA[arXiv:2402.16980v1 公告类型：新
摘要： 佛像作为多种宗教的象征，具有重要的文化内涵，对于了解不同地区的文化和历史至关重要，因此对佛像的认识是佛学研究领域的关键环节。然而，佛像识别需要知识渊博的专业人士花费大量的时间和精力，这使得其成本高昂。卷积神经网络 (CNN) 在处理视觉信息方面本质上是高效的，但当遇到类别不平衡问题时，单独使用 CNN 可能会做出不准确的分类决策。因此，本文提出一种基于显着图采样的端到端自动佛像识别模型。所提出的网格局部自注意力模块（GLSA）提供了额外的显着特征，可以丰富数据集并允许 CNN 以更全面的方式进行观察。最终，我们的模型在佛陀专家帮助下收集的佛陀数据集上进行了评估，在 Top-1 准确率方面优于最先进的网络，平均提高了 4.63%，同时仅略微增加了 MUL-ADD。]]></description>
      <guid>https://arxiv.org/abs/2402.16980</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>GEM3D：3D 形状合成的生成媒体抽象</title>
      <link>https://arxiv.org/abs/2402.16994</link>
      <description><![CDATA[arXiv:2402.16994v1 公告类型：新
摘要：我们介绍 GEM3D——一种新的深度、拓扑感知的 3D 形状生成模型。我们方法的关键要素是基于神经骨架的表示编码形状拓扑和几何信息。通过去噪扩散概率模型，我们的方法首先按照中轴变换（MAT）生成基于骨架的表示，然后通过骨架驱动的神经隐式公式生成表面。神经隐式考虑了存储在生成的骨架表示中的拓扑和几何信息，以产生与之前的神经场公式相比在拓扑和几何上更准确的表面。我们讨论了我们的方法在形状合成和点云重建任务中的应用，并定性和定量地评估了我们的方法。与最先进的技术相比，我们展示了更加忠实的表面重建和多样化的形状生成结果，还涉及从 Thingi10K 和 ShapeNet 重建和合成结构复杂、高属形状表面的挑战性场景。]]></description>
      <guid>https://arxiv.org/abs/2402.16994</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:14 GMT</pubDate>
    </item>
    <item>
      <title>MB-RACS：基于测量范围的速率自适应图像压缩感知网络</title>
      <link>https://arxiv.org/abs/2402.16855</link>
      <description><![CDATA[arXiv:2402.16855v1 公告类型：新
摘要：传统的压缩感知（CS）算法通常对不同的图像块采用统一的采样率。更具策略性的方法可能是根据每个图像块的复杂性自适应地分配测量数量。在本文中，我们提出了一种基于测量范围的速率自适应图像压缩感知网络（MB-RACS）框架，其目的是根据传统的测量范围理论自适应地确定每个图像块的采样率。此外，由于在现实场景中无法直接获得原始图像的统计信息，因此我们建议采用多阶段速率自适应采样策略。该策略根据先前采样收集的信息顺序调整采样率分配。我们将多级速率自适应采样表述为凸优化问题，并结合牛顿法和二分搜索技术来解决它。此外，我们通过在连续迭代之间合并跳跃连接来增强解码过程，以促进跨迭代更丰富的特征信息传输。我们的实验表明，所提出的 MB-RACS 方法超越了当前的领先方法，实验证据也强调了我们提出的框架内每个模块的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.16855</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>具有可解释机器学习功能的基于内容的交互式火星图像搜索</title>
      <link>https://arxiv.org/abs/2402.16860</link>
      <description><![CDATA[arXiv:2402.16860v1 公告类型：新
摘要：NASA 行星数据系统 (PDS) 托管着在许多任务中收集的数百万张行星、卫星和其他天体的图像。数据和用户参与不断扩展的性质需要可解释的内容分类系统来支持科学发现和个人好奇心。在本文中，我们利用基于原型的架构，使用户能够理解和验证根据火星科学实验室 (MSL) 好奇号漫游车任务的图像训练的分类器所使用的证据。除了提供解释之外，我们还调查基于内容的分类器使用的证据的多样性和正确性。本文介绍的工作将部署在 PDS 图像图集上，取代其不可解释的对应部分。]]></description>
      <guid>https://arxiv.org/abs/2402.16860</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:13 GMT</pubDate>
    </item>
    <item>
      <title>通过布局学习解开 3D 场景生成</title>
      <link>https://arxiv.org/abs/2402.16936</link>
      <description><![CDATA[arXiv:2402.16936v1 公告类型：新
摘要：我们介绍了一种生成 3D 场景的方法，这些场景被分解为其组件对象。这种解开是无监督的，仅依赖于大型预训练文本到图像模型的知识。我们的主要见解是，可以通过查找 3D 场景的部分来发现对象，这些部分在空间上重新排列时，仍然会产生同一场景的有效配置。具体来说，我们的方法从头开始联合优化多个 NeRF（每个 NeRF 代表自己的对象）以及一组将这些对象组合到场景中的布局。然后，我们鼓励这些合成场景根据图像生成器进行分布。我们表明，尽管我们的方法很简单，但它成功地生成了分解为单个对象的 3D 场景，从而实现了文本到 3D 内容创建的新功能。有关结果和交互式演示，请参阅我们的项目页面：https://dave.ml/layoutlearning/]]></description>
      <guid>https://arxiv.org/abs/2402.16936</guid>
      <pubDate>Wed, 28 Feb 2024 06:17:13 GMT</pubDate>
    </item>
    </channel>
</rss>