<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 04 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于无监督人员重新识别的空间级联聚类和加权记忆</title>
      <link>https://arxiv.org/abs/2403.00261</link>
      <description><![CDATA[arXiv:2403.00261v1 公告类型：新
摘要：最近的无监督行人重新识别（re-ID）方法通过利用细粒度的本地上下文实现了高性能。这些方法称为基于部分的方法。然而，大多数基于部位的方法通过水平划分来获取局部上下文，这会由于各种人体姿势而出现错位。此外，零件特征中语义信息的错位限制了度量学习的使用，从而影响了基于零件的方法的有效性。上述两个问题导致基于零件的方法中零件特征的利用不足。我们引入空间级联聚类和加权内存（SCWM）方法来应对这些挑战。 SCWM 旨在解析和对齐不同人体部位的更准确的局部上下文，同时允许内存模块平衡困难示例挖掘和噪声抑制。具体来说，我们首先分析之前方法中的前景遗漏和空间混乱问题。然后，我们提出前景和空间校正，以增强人体解析结果的完整性和合理性。接下来，我们引入加权内存并利用两种加权策略。这些策略解决了全局特征的硬样本挖掘问题，并增强了部分特征的抗噪性，从而可以更好地利用全局特征和部分特征。 Market-1501 和 MSMT17 上的大量实验验证了所提出的方法相对于许多最先进方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.00261</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:27 GMT</pubDate>
    </item>
    <item>
      <title>重新思考长尾识别中的分类器重新训练：一种简单的 Logits 重定向方法</title>
      <link>https://arxiv.org/abs/2403.00250</link>
      <description><![CDATA[arXiv:2403.00250v1 公告类型：新
摘要：在长尾识别领域，解耦训练范式在各种方法中表现出了卓越的能力。这种范例将训练过程解耦为单独的表示学习和分类器重新训练。以前的工作试图同时改进这两个阶段，这使得很难隔离分类器重新训练的效果。此外，最近的实证研究表明，简单的正则化可以产生强大的特征表示，强调需要重新评估现有的分类器重新训练方法。在本研究中，我们重新审视基于统一特征表示的分类器重新训练方法，并重新评估其性能。我们提出了一种称为 Logits Magnitude 的新指标，作为模型性能的高级衡量指标，取代常用的权重范数。然而，由于在训练过程中很难直接优化新指标，因此我们引入了一种合适的近似不变量，称为正则化标准差。基于这两个新提出的指标，我们证明在接近平衡时降低 Logits Magnitude 的绝对值可以有效减少训练过程中的错误和干扰，从而获得更好的模型性能。受这些发现的启发，我们开发了一种简单的逻辑重定向方法（LORT），无需事先了解每类样本的数量。 LORT 将原始单热标签分为分布在每个类别中的较小的真实标签概率和较大的负标签概率。我们的方法在各种不平衡数据集上实现了最先进的性能，包括 CIFAR100-LT、ImageNet-LT 和 iNaturalist2018。]]></description>
      <guid>https://arxiv.org/abs/2403.00250</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:26 GMT</pubDate>
    </item>
    <item>
      <title>使用挤压和激励卷积神经网络对放射性肺气肿亚型进行稳健深度标记：MESA Lung 和 SPIROMICS 研究</title>
      <link>https://arxiv.org/abs/2403.00257</link>
      <description><![CDATA[arXiv:2403.00257v1 公告类型：新
摘要：肺气肿是一种进行性、不可逆的肺组织损失，通常根据病理学和肺部计算机断层扫描（CT）图像将其分为三种亚型。最近的工作导致了对肺部 CT 上 10 种空间信息肺纹理模式 (sLTP) 的无监督学习，这些模式代表基于肺内纹理外观和空间位置的肺气肿肺实质的不同模式，并聚合成 6 种稳健且可重复的模式CT 肺气肿亚型 (CTES)。然而，现有的 sLTP 分割方法速度缓慢，并且对 CT 采集协议的变化高度敏感。在这项工作中，我们提出了一种强大的 3D 挤压和激励 CNN，用于肺部 CT 上 sLTP 和 CTES 的监督分类。我们的结果表明，该模型在肺部 CT 扫描上实现了准确且可重复的 sLTP 分割，跨越两个独立队列，并且独立于扫描仪制造商和型号。]]></description>
      <guid>https://arxiv.org/abs/2403.00257</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:26 GMT</pubDate>
    </item>
    <item>
      <title>多模态 ArXiv：用于提高大型视觉语言模型科学理解的数据集</title>
      <link>https://arxiv.org/abs/2403.00231</link>
      <description><![CDATA[arXiv:2403.00231v1 公告类型：新
摘要：以 GPT-4V 为代表的大型视觉语言模型（LVLM）在涉及自然场景中的具体图像的各种任务中表现出色。然而，由于科学领域训练数据集的缺乏，他们解释抽象图形（例如几何形状和科学绘图）的能力仍然有限。为了填补这一空白，我们引入了由 ArXivCap 和 ArXivQA 组成的 Multimodal ArXiv，用于增强 LVLM 的科学理解。 ArXivCap 是一个图形标题数据集，包含 640 万张图像和 390 万张标题，这些图像和标题源自跨越各个科学领域的 572K ArXiv 论文。借鉴ArXivCap，我们引入了ArXivQA，这是一个基于科学数据提示GPT-4V生成的问答数据集。 ArXivQA 极大地增强了 LVLM 的数学推理能力，在多模态数学推理基准上实现了 10.4% 的绝对精度增益。此外，我们利用 ArXivCap 设计了四个视觉到文本任务来对 LVLM 进行基准测试。最先进的 LVLM 的评估结果突显了他们与学术人物微妙语义的斗争，而特定领域的培训带来了显着的性能提升。我们的错误分析揭示了当前 LVLM 对视觉上下文的误解、识别错误以及过度简化的字幕生成，为未来的改进提供了线索。]]></description>
      <guid>https://arxiv.org/abs/2403.00231</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:25 GMT</pubDate>
    </item>
    <item>
      <title>YOLO-MED：生物医学图像的多任务交互网络</title>
      <link>https://arxiv.org/abs/2403.00245</link>
      <description><![CDATA[arXiv:2403.00245v1 公告类型：新
摘要：目标检测和语义分割是生物医学图像分析的关键组成部分。当前的单任务网络在检测和分割任务中都表现出了有希望的结果。多任务网络因其同时处理分割和检测任务的能力而受到关注，同时还加速了分割推理。然而，最近的多任务网络面临着明显的局限性，例如难以在准确性和推理速度之间取得平衡。此外，他们经常忽视跨尺度特征的整合，这对于生物医学图像分析尤其重要。在本研究中，我们提出了一种高效的端到端多任务网络，能够同时执行对象检测和语义分割，称为 YOLO-Med。我们的模型采用主干和颈部进行多尺度特征提取，并辅以两个特定于任务的解码器。采用跨尺度任务交互模块以促进各种任务之间的信息融合。在 Kvasir-seg 数据集和私人生物医学图像数据集上进行评估时，我们的模型在平衡准确性和速度方面表现出了有希望的结果。]]></description>
      <guid>https://arxiv.org/abs/2403.00245</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:25 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉语言预训练的语义增强跨模态掩模图像建模</title>
      <link>https://arxiv.org/abs/2403.00249</link>
      <description><![CDATA[arXiv:2403.00249v1 公告类型：新
摘要：在视觉语言预训练（VLP）中，最近引入了掩模图像建模（MIM）来进行细粒度的跨模态对齐。然而，在大多数现有方法中，MIM 的重建目标缺乏高级语义，并且文本没有充分参与掩模建模。这两个缺点限制了 MIM 在促进跨模态语义对齐方面的效果。在这项工作中，我们提出了一种用于视觉语言表示学习的语义增强的跨模态 MIM 框架（SemMIM）。具体来说，为了为 MIM 提供更多语义上有意义的监督，我们提出了一种局部语义增强方法，该方法通过自监督协议学习从全局图像特征中获取高级语义，并通过共享编码空间将它们转移到局部补丁编码。此外，为了在整个 MIM 过程中实现文本的深度参与，我们提出了一种文本引导的掩蔽策略，并设计了一种在掩蔽建模和重建目标获取中注入文本信息的有效方法。实验结果验证了我们的方法提高了 MIM 任务在促进跨模态语义对齐方面的有效性。与之前具有相似模型大小和数据规模的 VLP 模型相比，我们的 SemMIM 模型在多个下游视觉语言任务上实现了最先进的或有竞争力的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.00249</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:25 GMT</pubDate>
    </item>
    <item>
      <title>值得信赖的自我关注：使网络仅关注最相关的参考文献</title>
      <link>https://arxiv.org/abs/2403.00211</link>
      <description><![CDATA[arXiv:2403.00211v1 公告类型：新
摘要： 遮挡点的光流预测仍然是一个尚未解决的难题。最近的方法使用自注意力来寻找相关的非遮挡点，作为基于自相似性假设估计遮挡点的光流的参考。然而，它们依赖于单个图像的视觉特征和弱约束，这不足以约束训练后的网络关注错误和弱相关的参考点。我们充分利用在线遮挡识别信息来构造遮挡扩展视觉特征和两个强约束，使网络能够学习仅关注最相关的参考，而不需要遮挡地面实况参与网络的训练。我们的方法在原始框架上添加了很少的网络参数，使其非常轻量级。大量的实验表明，我们的模型具有最大的跨数据集泛化能力。与最先进的基于 GMA 的方法 MATCHFlow(GMA) 相比，我们的方法在所有点、非遮挡点和遮挡点上实现了更大的误差减少，分别为 18.6%、16.2% 和 20.1%。辛特尔反照率山口。此外，我们的模型在 Sintel 基准上实现了最先进的性能，在 Sintel clean pass 上所有已发布的方法中排名第一。该代码将是开源的。]]></description>
      <guid>https://arxiv.org/abs/2403.00211</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:24 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言模型的多模态属性提示</title>
      <link>https://arxiv.org/abs/2403.00219</link>
      <description><![CDATA[arXiv:2403.00219v1 公告类型：新
摘要：大型预训练视觉语言模型（VLM），如 CLIP，对下游任务表现出强大的泛化能力，但在少量场景中表现不佳。现有的提示技术主要关注全局文本和图像表示，而忽视了多模态属性特征。这种限制阻碍了模型感知细粒度视觉细节的能力，并限制了其对更广泛的看不见的类别的泛化能力。为了解决这个问题，我们通过共同探索文本属性提示、视觉属性提示和属性级别对齐，提出了一种多模态属性提示方法（MAP）。拟议的 MAP 有几个优点。首先，我们引入了通过文本属性语义增强的可学习视觉属性提示，以自适应地捕获来自未知类别的图像的视觉属性，从而提高 CLIP 的细粒度视觉感知能力。其次，所提出的属性级对齐补充了全局对齐，以增强开放词汇对象的跨模式对齐的鲁棒性。据我们所知，这是第一个为基于 CLIP 的小样本适应建立跨模式属性级对齐的工作。对 11 个数据集的广泛实验结果表明，我们的方法优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2403.00219</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:24 GMT</pubDate>
    </item>
    <item>
      <title>MaskLRF：通过局部参考系的屏蔽自动编码进行自监督预训练，用于旋转不变的 3D 点集分析</title>
      <link>https://arxiv.org/abs/2403.00206</link>
      <description><![CDATA[arXiv:2403.00206v1 公告类型：新
摘要：继视觉和语言领域的成功之后，通过 3D 点集数据的屏蔽自动编码或屏蔽点建模 (MPM) 进行的自监督预训练在各种下游任务中实现了最先进的准确性。然而，当前的 MPM 方法缺乏 3D 点集分析所必需的属性，即 3D 对象/场景旋转的不变性。因此，现有的 MPM 方法不一定适合 3D 点集可能具有不一致方向的现实应用。本文首次开发了一种用于实际 3D 点集分析的旋转不变自监督预训练框架。所提出的算法称为 MaskLRF，通过局部参考框架 (LRF) 内的 3D 点的掩码自动编码来学习旋转不变和高度通用的潜在特征，这些特征不受 3D 点集旋转的影响。 MaskLRF 通过集成使用相对姿态编码的特征细化和使用低级但丰富的 3D 几何的特征重建来增强潜在特征的质量。 MaskLRF 的功效通过对各种下游任务（包括分类、分割、注册和域适应）的广泛实验得到验证。我确认 MaskLRF 在分析方向不一致的 3D 点集方面达到了最先进的新精度。代码可在以下位置获取：https://github.com/takahikof/MaskLRF]]></description>
      <guid>https://arxiv.org/abs/2403.00206</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:23 GMT</pubDate>
    </item>
    <item>
      <title>ChartReformer：自然语言驱动的图表图像编辑</title>
      <link>https://arxiv.org/abs/2403.00209</link>
      <description><![CDATA[arXiv:2403.00209v1 公告类型：新
摘要：图表可视化对于数据解释和交流至关重要；然而，大多数图表只能以图像格式访问，缺乏相应的数据表和补充信息，因此很难根据不同的应用场景改变其外观。为了消除对原始基础数据和信息进行图表编辑的需要，我们提出了 ChartReformer，这是一种自然语言驱动的图表图像编辑解决方案，可以根据给定的指令提示直接从输入图像编辑图表。该方法的关键是让模型理解图表并根据提示进行推理，为新图表生成相应的底层数据表和视觉属性，从而实现精确编辑。此外，为了概括 ChartReformer，我们定义并标准化了各种类型的图表编辑，包括样式、布局、格式和以数据为中心的编辑。这些实验显示了自然语言驱动的图表图像编辑的有希望的结果。]]></description>
      <guid>https://arxiv.org/abs/2403.00209</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:23 GMT</pubDate>
    </item>
    <item>
      <title>FusionVision：使用 YOLO 和快速分割任何内容从 RGB-D 相机进行 3D 对象重建和分割的综合方法</title>
      <link>https://arxiv.org/abs/2403.00175</link>
      <description><![CDATA[arXiv:2403.00175v1 公告类型：新
摘要：在计算机视觉领域，鉴于不同的环境条件和不同的物体外观所产生的固有复杂性，将先进技术集成到 RGB-D 相机输入的处理中提出了重大挑战。因此，本文介绍了 FusionVision，这是一种适用于 RGB-D 图像中对象的鲁棒 3D 分割的详尽管道。传统的计算机视觉系统主要针对 RGB 相机，因此在同时捕获精确的物体边界和在深度图上实现高精度物体检测方面面临着局限性。为了应对这一挑战，FusionVision 采用了一种集成方法，将最先进的对象检测技术与先进的实例分割方法相结合。这些组件的集成可以对 RGB-D 数据进行整体（对从颜色 \textit{RGB} 和深度 \textit{D} 通道获得的信息进行统一分析）解释，从而有助于提取全面且准确的对象信息。拟议的 FusionVision 管道采用 YOLO 来识别 RGB 图像域内的对象。随后，应用创新的语义分割模型 FastSAM 来描绘对象边界，从而产生精细的分割掩模。这些组件之间的协同作用及其与 3D 场景理解的集成确保了对象检测和分割的紧密融合，从而提高了 3D 对象分割的整体精度。代码和预训练模型可在 https://github.com/safouaneelg/FusionVision/ 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2403.00175</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:22 GMT</pubDate>
    </item>
    <item>
      <title>学习通过合成数据增强查找丢失的视频帧：使用 RGB 相机生成热图像的通用框架和应用</title>
      <link>https://arxiv.org/abs/2403.00196</link>
      <description><![CDATA[arXiv:2403.00196v1 公告类型：新
摘要：智能车辆中的高级驾驶员辅助系统（ADAS）依赖于驾驶室内准确的驾驶员感知，通常利用传感模式的组合。然而，这些模式的运行速度各不相同，给实时、全面的驾驶员状态监控带来了挑战。本文解决了由于传感器帧速率不匹配而导致的数据丢失问题，引入了一种生成模型方法来创建合成且真实的热图像。我们建议使用条件生成对抗网络（cGAN），特别是比较 pix2pix 和 CycleGAN 架构。实验结果表明，pix2pix 的性能优于 CycleGAN，并且利用多视图输入样式，尤其是堆叠视图，提高了热图像生成的准确性。此外，该研究评估了该模型在不同科目中的普遍性，揭示了个性化训练对于最佳表现的重要性。研究结果表明，生成模型在解决丢失帧问题、推进智能车辆驾驶员状态监控方面具有潜力，并强调需要继续研究模型泛化和定制。]]></description>
      <guid>https://arxiv.org/abs/2403.00196</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:22 GMT</pubDate>
    </item>
    <item>
      <title>大规模视觉语言模型中的艺术品解释</title>
      <link>https://arxiv.org/abs/2403.00068</link>
      <description><![CDATA[arXiv:2403.00068v1 公告类型：新
摘要：大规模视觉语言模型（LVLM）从图像和指令输出文本，展示了文本生成和理解的先进能力。然而，目前尚不清楚LVLM在多大程度上理解解释图像所需的知识、各种知识之间的复杂关系以及它们如何将这些理解整合到他们的解释中。为了解决这个问题，我们提出了一个新任务：艺术品解释生成任务，及其评估数据集和用于定量评估对艺术品知识的理解和利用的指标。该任务适用于图像描述，前提是 LVLM 需要预先存在艺术品知识，而艺术品通常是广泛认可和记录信息的主题。它由两部分组成：从图像和艺术品标题生成解释，以及仅使用图像生成解释，从而评估 LVLM 基于语言和基于视觉的知识。此外，我们还发布了 LVLM 的训练数据集，用于学习包含艺术品知识的解释。我们的研究结果表明，LVLM 不仅难以整合语言和视觉信息，而且在仅从图像获取知识方面也表现出更明显的局限性。数据集 (ExpArt=Explain Artworks) 可在 https://huggingface.co/datasets/naist-nlp/ExpArt 上获取。]]></description>
      <guid>https://arxiv.org/abs/2403.00068</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:21 GMT</pubDate>
    </item>
    <item>
      <title>政治学法学硕士：预示着视觉分析的新时代</title>
      <link>https://arxiv.org/abs/2403.00154</link>
      <description><![CDATA[arXiv:2403.00154v1 公告类型：新
摘要：政治科学家对利用图像中的广泛信息越来越感兴趣。然而，解释这些图像的挑战在于需要计算机视觉方面的专业知识和使用专用硬件。因此，图像分析仅限于政治科学界内相对较小的群体。由于大型语言模型（LLM）的兴起，这种情况可能会发生改变。本文旨在提高人们对使用 Gemini 进行图像内容分析的可行性的认识。对 688 张图像的语料库进行了回顾性分析。每张图像的内容报告均由 Gemini 引出，然后由作者手动评估。我们发现 Gemini 在执行对象检测方面非常准确，这可以说是政治科学家图像分析中最常见和基本的任务。同样重要的是，我们表明它很容易实现，因为整个命令由自然语言的单个提示组成；它运行速度快，应该可以满足大多数研究人员的时间预算；而且它是免费使用的，不需要任何专门的硬件。此外，我们还说明了政治科学家如何利用 Gemini 进行其他图像理解任务，包括面部识别、情绪分析和标题生成。我们的研究结果表明，双子座和其他类似的法学硕士有潜力极大地刺激和加速更广泛的政治学和社会科学领域的图像研究。]]></description>
      <guid>https://arxiv.org/abs/2403.00154</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:21 GMT</pubDate>
    </item>
    <item>
      <title>公民科学工具包，使用开放街景图像收集人类对城市环境的感知</title>
      <link>https://arxiv.org/abs/2403.00174</link>
      <description><![CDATA[arXiv:2403.00174v1 公告类型：新
摘要：街景级图像（SVI）是研究（例如环境评估、绿地识别或土地覆盖分类）的宝贵数据源。虽然商业 SVI 可用，但此类提供商通常会限制研究所需的复制或重复使用。开放 SVI 数据集很容易从限制较少的来源获得，例如 Mapillary，但由于图像的异质性，这些数据需要大量的预处理、过滤和仔细的质量检查。我们提出了一种自动下载、处理、裁剪和过滤开放 SVI 的有效方法，用于调查人类对这些图像中描绘的街道的看法。我们以阿姆斯特丹（荷兰）为案例研究，展示了我们的开源可重复使用的 SVI 准备和智能手机友好的感知调查软件。我们采用公民科学方法，从 331 人中收集了 22,637 个关于他们对各种标准的看法的评分。我们已在公共存储库中发布了我们的软件，以供将来重用和再现。]]></description>
      <guid>https://arxiv.org/abs/2403.00174</guid>
      <pubDate>Mon, 04 Mar 2024 07:03:21 GMT</pubDate>
    </item>
    </channel>
</rss>