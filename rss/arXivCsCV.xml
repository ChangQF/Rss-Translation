<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 12 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>改善牙科诊断：利用空间注意机制增强卷积</title>
      <link>https://arxiv.org/abs/2407.08114</link>
      <description><![CDATA[arXiv:2407.08114v1 公告类型：新
摘要：深度学习已成为医疗保健领域的一种变革性工具，通过分析复杂的成像数据，为牙科诊断提供了重大进步。本文介绍了一种增强型 ResNet50 架构，集成了 SimAM 注意力模块，以解决牙科图像对比度有限的挑战，并在降低计算需求的同时优化深度学习性能。在第二个 ResNet 块之后合并的 SimAM 模块通过捕获空间依赖性和增强重要特征来改进特征提取。我们的模型在各种特征提取技术中都表现出色，F1 得分达到 0.676，优于 VGG、EfficientNet、DenseNet 和 AlexNet 等传统架构。这项研究强调了我们的方法在提高牙科图像分析的分类准确性和稳健性方面的有效性，强调了深度学习在提高牙科诊断准确性和效率方面的潜力。像我们这样的先进人工智能模型的整合将彻底改变牙科诊断，有助于改善患者的治疗效果并促进人工智能在牙科领域的更广泛应用。]]></description>
      <guid>https://arxiv.org/abs/2407.08114</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:39 GMT</pubDate>
    </item>
    <item>
      <title>城市涝渍检测：具有挑战性的基准和大小模型协同适配器</title>
      <link>https://arxiv.org/abs/2407.08109</link>
      <description><![CDATA[arXiv:2407.08109v1 公告类型：新
摘要：城市内涝对公共安全和基础设施构成重大风险。使用水位传感器的传统方法需要高维护才能难以实现全面覆盖。最近的进展是利用监控摄像机图像和深度学习进行检测，但这些在数据稀缺和不利的环境条件下举步维艰。在本文中，我们在各种不利条件下建立了一个具有挑战性的城市内涝基准 (UW-Bench)，以推进现实世界的应用。我们提出了一个大型-小型模型协同适配器范式 (LSM-adapter)，它利用大型模型的巨大通用分割潜力和小型模型的特定任务导向指导。具体而言，提出了一个 Triple-S Prompt Adapter 模块和一个动态提示组合器，以生成然后合并多个提示以进行掩码解码器自适应。同时，设计了一个直方图均衡适配器模块来注入图像特定信息以进行图像编码器自适应。结果和分析表明了我们开发的基准和算法的挑战性和优越性。项目页面：\url{https://github.com/zhang-chenxu/LSM-Adapter}]]></description>
      <guid>https://arxiv.org/abs/2407.08109</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>仅供参考：翻转图像以进行数据集提炼</title>
      <link>https://arxiv.org/abs/2407.08113</link>
      <description><![CDATA[arXiv:2407.08113v1 公告类型：新
摘要：数据集蒸馏从大规模真实数据集中合成一小组图像，使得合成图像和真实图像在训练过程中具有相似的行为属性（例如，梯度或特征的分布）。通过对当前方法和真实数据集的广泛分析以及经验观察，我们在本文中提供了两个重要的数据集蒸馏要点。首先，出现在真实图像一侧的物体部分很可能出现在数据集中另一幅图像的另一侧，我们称之为双边等价。其次，双边等价迫使合成图像在图像的左侧和右侧复制物体的判别部分，从而限制了对物体之间细微差别的识别。为了解决这个问题，我们引入了一种非常简单但有效的数据集蒸馏技术，称为 FYI，它可以将真实图像的丰富语义蒸馏成合成图像。为此，FYI 在蒸馏过程中嵌入了水平翻转技术，减轻了双边等价性的影响，同时捕获了更多物体的细节。在 CIFAR-10/100、Tiny-ImageNet 和 ImageNet 上的实验表明，FYI 可以无缝集成到几种最先进的方法中，而无需修改训练目标和网络架构，并且它显著提高了性能。]]></description>
      <guid>https://arxiv.org/abs/2407.08113</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>地理特定视图生成——从卫星视图推断几何上下文感知高分辨率地面视图</title>
      <link>https://arxiv.org/abs/2407.08061</link>
      <description><![CDATA[arXiv:2407.08061v1 公告类型：新
摘要：由于卫星图像和地面图像之间存在显著的视图差距，因此从城市场景中的卫星图像预测真实的地面视图是一项具有挑战性的任务。我们提出了一种新颖的流程来应对这一挑战，通过生成最大限度地尊重多视图卫星图像中弱几何和纹理的地理特定视图。与现有的从部分语义或高架卫星图像的几何形状等线索中幻化图像的方法不同，我们的方法使用来自卫星图像的一组综合信息直接预测地理位置的地面视图图像，从而产生分辨率提高十倍或更多的地面图像。我们利用一种新颖的建筑细化方法来减少地面卫星数据的几何失真，从而确保为使用扩散网络进行视图合成创造准确的条件。此外，我们提出了一种新颖的地理特定先验，它促使扩散模型的分布学习尊重更接近预测图像地理位置的图像样本。我们证明我们的流程是第一个仅基于卫星图像就能生成接近真实和地理特定的地面视图的流程。]]></description>
      <guid>https://arxiv.org/abs/2407.08061</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>MambaVision：混合 Mamba-Transformer 视觉主干</title>
      <link>https://arxiv.org/abs/2407.08083</link>
      <description><![CDATA[arXiv:2407.08083v1 公告类型：新
摘要：我们提出了一种新型混合 Mamba-Transformer 主干，称为 MambaVision，专门针对视觉应用而量身定制。我们的核心贡献包括重新设计 Mamba 公式以增强其高效建模视觉特征的能力。此外，我们对将 Vision Transformers (ViT) 与 Mamba 集成的可行性进行了全面的消融研究。我们的结果表明，在 Mamba 架构的最后几层配备多个自注意力块可大大提高捕获远程空间依赖关系的建模能力。根据我们的研究结果，我们引入了一系列具有分层架构的 MambaVision 模型来满足各种设计标准。对于 ImageNet-1K 数据集上的图像分类，MambaVision 模型变体在 Top-1 准确率和图像吞吐量方面实现了新的最先进 (SOTA) 性能。在 MS COCO 和 ADE20K 数据集上的对象检测、实例分割和语义分割等下游任务中，MambaVision 的表现优于同等规模的主干，并表现出更佳的性能。代码：https://github.com/NVlabs/MambaVision。]]></description>
      <guid>https://arxiv.org/abs/2407.08083</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>现场健身指导作为情境互动的试验台</title>
      <link>https://arxiv.org/abs/2407.08101</link>
      <description><![CDATA[arXiv:2407.08101v1 公告类型：新
摘要：视觉和语言交叉的任务对提升基于对话的助手等视觉语言模型的能力产生了深远的影响。然而，在现有任务上训练的模型主要局限于回合制交互，其中每个回合都必须由用户逐步完成（即提示）。开放式异步交互是一个开放的挑战，其中人工智能模型可以根据实时展开的情况主动提供及时的响应或反馈。在这项工作中，我们提出了 QEVD 基准和数据集，它探索了具有挑战性但受控的现实世界健身教练领域的人机交互——这项任务本质上需要监控实时用户活动并提供及时反馈。这是第一个需要辅助视觉语言模型识别复杂人类行为、识别基于这些行为的错误并提供适当反馈的基准。我们的实验揭示了现有最先进的视觉语言模型在这种异步情境交互中的局限性。受此启发，我们提出了一个简单的端到端流式传输基线，可以在适当的时间以适当的反馈异步响应人类动作。]]></description>
      <guid>https://arxiv.org/abs/2407.08101</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>TACLE：任务和类别感知的无样本半监督类别增量学习</title>
      <link>https://arxiv.org/abs/2407.08041</link>
      <description><![CDATA[arXiv:2407.08041v1 公告类型：新
摘要：我们提出了一种新颖的 TACLE（TAsk and CLass-awarE）框架来解决相对未开发且具有挑战性的无样本半监督类增量学习问题。在这种情况下，在每个新任务中，模型都必须从（少量）标记和未标记数据中学习新类，而无需访问以前类的样本。除了利用预训练模型的功能外，TACLE 还提出了一种新颖的任务自适应阈值，从而在增量学习过程中最大限度地利用可用的未标记数据。此外，为了提高每个任务中代表性不足的类别的性能，我们提出了一种类感知加权交叉熵损失。我们还利用未标记数据进行分类器对齐，从而进一步提高模型性能。在基准数据集（即 CIFAR10、CIFAR100 和 ImageNet-Subset100）上进行的大量实验证明了所提出的 TACLE 框架的有效性。我们进一步展示了当未标记数据不平衡时以及在每个类中只有一个标记示例的极端情况下的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.08041</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>通过融合毫米波雷达和摄像头传感器实现基于深度学习的稳健多目标跟踪</title>
      <link>https://arxiv.org/abs/2407.08049</link>
      <description><![CDATA[arXiv:2407.08049v1 公告类型：新
摘要：自动驾驶利用人工智能和传感器技术，在解决交通安全问题方面大有可为。多目标跟踪在确保在复杂交通场景中更安全、更高效地导航方面发挥着关键作用。本文提出了一种基于深度学习的新型方法，该方法集成了雷达和摄像头数据，以提高自动驾驶系统中多目标跟踪的准确性和稳健性。所提出的方法利用双向长短期记忆网络来整合长期时间信息并改进运动预测。受 FaceNet 启发的外观特征模型用于在不同帧之间建立对象之间的关联，从而确保一致的跟踪。采用三输出机制，包括雷达和摄像头传感器的单独输出以及融合输出，以提供对传感器故障的稳健性并产生准确的跟踪结果。通过对现实世界数据集的广泛评估，我们的方法在跟踪精度方面表现出显着的提高，即使在低能见度场景下也能确保可靠的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.08049</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>混合运动结构和相机重定位，增强自我中心定位</title>
      <link>https://arxiv.org/abs/2407.08023</link>
      <description><![CDATA[arXiv:2407.08023v1 公告类型：新
摘要：我们构建了我们的管道 EgoLoc-v1，主要受到 EgoLoc 的启发。我们提出了一种模型集成策略来改进 VQ3D 任务的相机姿势估计部分，这在以前的工作中已被证明是必不可少的。核心思想不仅对以自我为中心的视频进行 SfM，而且还在现有的 3D 扫描和 2D 视频帧之间进行 2D-3D 匹配。通过这种方式，我们有一个混合的 SfM 和相机重新定位管道，它可以为我们提供更多的相机姿势，从而提高 QwP 和总体成功率。我们的方法在最重要的指标——总体成功率方面取得了最佳表现。我们超越了之前最先进的竞争对手 EgoLoc，高出 $1.5\%$。代码可在 \url{https://github.com/Wayne-Mai/egoloc_v1} 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.08023</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>Fish-Vista：用于理解和识别图像特征的多用途数据集</title>
      <link>https://arxiv.org/abs/2407.08027</link>
      <description><![CDATA[arXiv:2407.08027v1 公告类型：新
摘要：鱼类是生态系统和经济部门不可或缺的一部分，研究鱼类特征对于了解生物多样性模式和宏观进化趋势至关重要。为了能够分析鱼类图像的视觉特征，我们引入了鱼类视觉特征分析 (Fish-Vista) 数据集 - 一个大型的带注释的集合，包含约 60K 张鱼类图像，涵盖 1900 个不同的物种，支持多项具有挑战性且与生物学相关的任务，包括物种分类、特征识别和特征分割。这些图像是通过复杂的数据处理流程整理的，该流程应用于从各个博物馆收藏中获得的一组累积图像。Fish-Vista 为每张图像中存在的各种视觉特征提供了细​​粒度的标签。它还为 2427 张鱼类图像提供了 9 种不同特征的像素级注释，从而促进了额外的特征分割和定位任务。 Fish-Vista 的最终目标是提供干净、精心策划的高分辨率数据集，作为利用人工智能的进步加速生物发现的基础。最后，我们对 Fish-Vista 上最先进的深度学习技术进行了全面分析。]]></description>
      <guid>https://arxiv.org/abs/2407.08027</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>通过潜在空间优化实现连贯和多模态图像修复</title>
      <link>https://arxiv.org/abs/2407.08019</link>
      <description><![CDATA[arXiv:2407.08019v1 公告类型：新
摘要：随着去噪扩散概率模型 (DDPM) 的进步，图像修复已从仅仅基于附近区域填充信息发展到根据各种提示（例如文本、示例图像和草图）生成内容。然而，现有的方法，例如模型微调和潜在向量的简单连接，经常由于过度拟合和修复区域与背景不一致而导致生成失败。在本文中，我们认为当前的大型扩散模型足够强大，无需进一步调整即可生成逼真的图像。因此，我们引入了 PILOT（in\textbf{P}ainting v\textbf{I}a \textbf{L}atent \textbf{O} p\textbf{T}imization），一种基于新颖的 \textit{语义中心化} 和 \textit{背景保存损失} 的优化方法。我们的方法搜索能够生成修复区域的潜在空间，这些区域对用户提供的提示表现出高保真度，同时与背景保持一致。此外，我们提出了一种平衡优化成本和图像质量的策略，显著提高了生成效率。我们的方法可以与任何预训练模型无缝集成，包括 ControlNet 和 DreamBooth，使其适合部署在多模式编辑工具中。我们的定性和定量评估表明，PILOT 优于现有方法，因为它可以根据提供的提示生成更连贯、多样和忠实的修复区域。]]></description>
      <guid>https://arxiv.org/abs/2407.08019</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>从 3D 超声图像中分割胎盘的交互式分割模型</title>
      <link>https://arxiv.org/abs/2407.08020</link>
      <description><![CDATA[arXiv:2407.08020v1 公告类型：新
摘要：从 3D 超声图像测量胎盘体积对于预测妊娠结果至关重要，而手动注释是黄金标准。然而，这种手动注释既昂贵又耗时。自动分割算法通常可以成功分割胎盘，但这些方法可能无法始终如一地产生适合实际使用的稳健分割。最近，受 Segment Anything Model (SAM) 的启发，基于深度学习的交互式分割模型已广泛应用于医学成像领域。这些模型根据提供的视觉提示生成分割以指示目标区域，这可能为实际使用提供可行的解决方案。然而，这些模型都不是专门为交互式分割 3D 超声图像而设计的，由于这种方式固有的噪声，这仍然具有挑战性。在本文中，我们评估了公开可用的最先进的 3D 交互式分割模型，并与胎盘分割任务的人机交互方法进行了对比。 Dice 分数、归一化表面 Dice、平均对称表面距离和 95% Hausdorff 距离被用作评估指标。我们认为 Dice 分数为 0.95 表示分割成功。我们的结果表明，人机环路分割模型达到了这一标准。此外，我们根据提示量来评估人机环路模型的效率。我们的结果表明，人机环路模型对于交互式胎盘分割既有效又高效。代码可在 \url{https://github.com/MedICL-VU/PRISM-placenta} 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.08020</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>Flow4D：利用 4D 体素网络进行 LiDAR 场景流量估计</title>
      <link>https://arxiv.org/abs/2407.07995</link>
      <description><![CDATA[arXiv:2407.07995v1 公告类型：新
摘要：了解周围环境的运动状态对于安全的自动驾驶至关重要。这些运动状态可以从场景流中准确得出，场景流可以捕捉点的三维运动场。现有的 LiDAR 场景流方法从每个点云中提取空间特征，然后逐通道融合它们，从而隐式提取时空特征。此外，它们利用 2D 鸟瞰图并仅处理两帧，缺少沿 Z 轴的关键空间信息和更广泛的时间背景，导致性能不佳。为了解决这些限制，我们提出了 Flow4D，它在 3D 体素内特征编码器之后在时间上融合多个点云，从而能够通过 4D 体素网络更明确地提取时空特征。然而，虽然使用 4D 卷积可以提高性能，但它显著增加了计算负荷。为了进一步提高效率，我们引入了时空分解块 (STDB)，它结合了 3D 和 1D 卷积，而不是使用繁重的 4D 卷积。此外，Flow4D 通过使用五帧来利用更丰富的时间信息，进一步提高了性能。因此，与最先进的方法相比，所提出的方法在实时运行时实现了 45.9% 的性能提升，并在 2024 年 Argoverse 2 Scene Flow Challenge 中获得第一名。代码可在 https://github.com/dgist-cvlab/Flow4D 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.07995</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:33 GMT</pubDate>
    </item>
    <item>
      <title>融合短期和长期注意力进行视频镜像检测</title>
      <link>https://arxiv.org/abs/2407.07999</link>
      <description><![CDATA[arXiv:2407.07999v1 公告类型：新
摘要：近年来，从静态图像中检测镜子的技术发展迅速。然而，这些方法只能从单个输入图像中检测镜子。从视频中检测镜子需要进一步考虑帧之间的时间一致性。我们观察到，人类可以根据外观（例如形状、颜色）从一两帧中识别出镜子候选对象。然而，为了确保候选对象确实是镜子（而不是图片或窗户），我们通常需要观察更多帧以获得全局视图。这一观察促使我们通过融合从短期注意模块中提取的外观特征和从长期注意模块中提取的上下文信息来检测镜子。为了评估性能，我们从 281 个视频中构建了一个包含 19,255 帧的具有挑战性的基准数据集。实验结果表明，我们的方法在基准数据集上实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2407.07999</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:33 GMT</pubDate>
    </item>
    <item>
      <title>贝叶斯检测器组合用于众包注释对象检测</title>
      <link>https://arxiv.org/abs/2407.07958</link>
      <description><![CDATA[arXiv:2407.07958v1 公告类型：新
摘要：在不受约束的图像中获取细粒度的对象检测注释非常耗时、昂贵且容易产生噪音，尤其是在众包场景中。大多数先前的对象检测方法都假设注释准确；最近的一些研究研究了使用嘈杂的众包注释进行对象检测，并在人为假设下对不同设置的不同合成众包数据集进行了评估。为了解决这些算法限制和评估不一致的问题，我们首先提出了一种新颖的贝叶斯检测器组合 (BDC) 框架，以更有效地训练具有嘈杂众包注释的对象检测器，并具有自动推断注释者标签质量的独特能力。与以前的方法不同，BDC 与模型无关，不需要事先了解注释者的技能水平，并且可以与现有的对象检测模型无缝集成。由于现实世界众包数据集的稀缺，我们通过模拟不同的众包场景来引入大型合成数据集。这样就可以对不同模型进行大规模的一致评估。在真实和合成众包数据集上进行的大量实验表明，BDC 的表现优于现有的最先进方法，证明了其在利用众包数据进行对象检测方面的优势。我们的代码和数据可在 https://github.com/zhiqin1998/bdc 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.07958</guid>
      <pubDate>Fri, 12 Jul 2024 06:20:32 GMT</pubDate>
    </item>
    </channel>
</rss>