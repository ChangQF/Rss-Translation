<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 26 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>用于学习图像表示的注意力引导掩模自动编码器</title>
      <link>https://arxiv.org/abs/2402.15172</link>
      <description><![CDATA[arXiv:2402.15172v1 公告类型：新
摘要：掩模自动编码器（MAE）已成为计算机视觉任务无监督预训练的强大方法。虽然普通 MAE 同样重视重建图像的各个部分，但我们建议通过注意力引导损失函数来告知重建过程。通过利用无监督对象发现方面的进步，我们获得了场景的注意力图，我们在损失函数中使用该注意力图来更加重视重建相关对象，从而有效地激励模型学习更多以对象为中心的表示，而不会影响已建立的掩蔽策略。我们的评估表明，我们的预训练模型比普通 MAE 能够学习更好的潜在表示，这通过多个基准上改进的线性探测和 k-NN 分类结果得到证明，同时使 ViT 在不同背景下更加稳健。]]></description>
      <guid>https://arxiv.org/abs/2402.15172</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>用于在线人员重新识别的源引导相似性保留</title>
      <link>https://arxiv.org/abs/2402.15206</link>
      <description><![CDATA[arXiv:2402.15206v1 公告类型：新
摘要：用于人员重新识别（Re-ID）的在线无监督域适应（OUDA）是不断地将在注释良好的源域数据集上训练的模型适应作为数据流观察的目标域的任务。在 OUDA 中，行人重新识别模型面临两个主要挑战：灾难性遗忘和领域转移。在这项工作中，我们提出了一种新的源引导相似性保留（S2P）框架来缓解这两个问题。我们的框架基于提取由源图像组成的支持集，最大限度地提高与目标数据的相似度。该支持集用于识别在学习过程中必须保留的特征相似性。 S2P 可以整合多种现有的 UDA 方法来减轻灾难性遗忘。我们的实验表明，S2P 在多个真实到真实和合成到真实的挑战性 OUDA 基准测试中优于以前最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2402.15206</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>视觉语音与语言的结合：用于高效且上下文感知的视觉语音处理的 VSP-LLM 框架</title>
      <link>https://arxiv.org/abs/2402.15151</link>
      <description><![CDATA[arXiv:2402.15151v1 公告类型：新
摘要：在视觉语音处理中，由于嘴唇运动的模糊性，上下文建模能力是最重要的要求之一。例如，同音词，即具有相同嘴唇动作但发出不同声音的单词，可以通过考虑上下文来区分。在本文中，我们提出了一种新颖的框架，即与法学硕士相结合的视觉语音处理（VSP-LLM），通过发挥法学硕士的压倒性力量来最大化上下文建模能力。具体来说，VSP-LLM 旨在执行视觉语音识别和翻译的多任务，其中给定的指令控制任务的类型。通过采用自监督视觉语音模型将输入视频映射到 LLM 的输入潜在空间。针对输入帧中存在冗余信息的事实，我们提出了一种新颖的重复数据删除方法，通过使用视觉语音单元来减少嵌入的视觉特征。通过所提出的重复数据删除和低秩适配器（LoRA），可以以计算高效的方式训练 VSP-LLM。在翻译数据集（MuAViC 基准）中，我们证明，与最近使用 433 小时的标记数据训练的翻译模型相比，VSP-LLM 只需 15 小时的标记数据即可更有效地识别和翻译嘴唇运动。]]></description>
      <guid>https://arxiv.org/abs/2402.15151</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:17 GMT</pubDate>
    </item>
    <item>
      <title>修改后的 CycleGAN 用于合成小麦头部分割的样本</title>
      <link>https://arxiv.org/abs/2402.15135</link>
      <description><![CDATA[arXiv:2402.15135v1 公告类型：新
摘要：深度学习模型已用于各种图像处理任务。然而，这些模型大多数是通过监督学习方法开发的，这在很大程度上依赖于大规模注释数据集的可用性。开发此类数据集既乏味又昂贵。在没有带注释的数据集的情况下，可以使用合成数据进行模型开发；然而，由于模拟数据和真实数据之间存在巨大差异（一种称为域差距的现象），所得模型在应用于真实数据时通常表现不佳。在这项研究中，我们的目标是通过首先计算模拟大规模带注释的数据集，然后使用生成对抗网络（GAN）来填补模拟图像和真实图像之间的差距来应对这一挑战。这种方法产生了一个合成数据集，可以有效地用于训练深度学习模型。使用这种方法，我们开发了一个用于小麦头部分割的真实注释合成数据集。然后使用该数据集开发用于语义分割的深度学习模型。生成的模型在内部数据集上获得了 83.4% 的 Dice 分数，在两个外部全球小麦头检测数据集上获得了 79.6% 和 83.6% 的 Dice 分数。虽然我们在小麦头部分割的背景下提出了这种方法，但它可以推广到其他作物类型，或更广泛地推广到具有密集、重复模式的图像，例如细胞图像中发现的图像。]]></description>
      <guid>https://arxiv.org/abs/2402.15135</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:16 GMT</pubDate>
    </item>
    <item>
      <title>PUAD：用于稳健异常检测的极其简单的方法</title>
      <link>https://arxiv.org/abs/2402.15143</link>
      <description><![CDATA[arXiv:2402.15143v1 公告类型：新
摘要：开发准确、快速的异常检测模型是实时计算机视觉应用中的一项重要任务。人们已经进行了大量的研究来开发一个单一的模型来检测结构或逻辑异常，这些异常本质上是不同的。大多数现有方法隐含地假设可以通过识别异常位置来表示异常。然而，我们认为逻辑异常（例如对象数量错误）无法通过空间特征图很好地表示，需要采用替代方法。此外，我们还重点研究了通过在特征空间上使用分布外检测方法来检测逻辑异常的可能性，该方法聚合了特征图的空间信息。作为演示，我们提出了一种方法，该方法将特征空间上的简单分布外检测方法与最先进的基于重建的方法相结合。尽管我们的提案很简单，但我们的方法 PUAD（可描绘和不可描绘异常检测）在 MVTec LOCO AD 数据集上实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.15143</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:16 GMT</pubDate>
    </item>
    <item>
      <title>用于面部标志检测的基准焦点增强</title>
      <link>https://arxiv.org/abs/2402.15044</link>
      <description><![CDATA[arXiv:2402.15044v1 公告类型：新
摘要：深度学习方法显着提高了面部标志检测（FLD）任务的性能。然而，由于高变异性和样本不足，在具有挑战性的环境中检测地标仍然是一个挑战，例如头部姿势变化、夸张的表情或不均匀的照明。这种不足可归因于模型无法有效地从输入图像中获取适当的面部结构信息。为了解决这个问题，我们提出了一种专门为 FLD 任务设计的新型图像增强技术，以增强模型对面部结构的理解。为了有效利用新提出的增强技术，我们采用基于暹罗架构的训练机制和基于深度典型相关分析（DCCA）的损失，以实现从输入图像的两个不同视图进行高级特征表示的集体学习。此外，我们采用基于 Transformer + CNN 的网络和自定义沙漏模块作为 Siamese 框架的强大骨干。大量的实验表明，我们的方法在各种基准数据集上优于多种最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2402.15044</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:15 GMT</pubDate>
    </item>
    <item>
      <title>大型多式联运代理：调查</title>
      <link>https://arxiv.org/abs/2402.15116</link>
      <description><![CDATA[arXiv:2402.15116v1 公告类型：新
摘要：大语言模型（LLM）在支持基于文本的人工智能代理方面取得了卓越的性能，赋予它们类似于人类的决策和推理能力。与此同时，出现了一种新兴的研究趋势，重点是将这些由法学硕士支持的人工智能代理扩展到多模式领域。此扩展使人工智能代理能够解释和响应不同的多模式用户查询，从而处理更复杂和细致的任务。在本文中，我们对 LLM 驱动的多模式代理进行了系统回顾，我们将其称为大型多模式代理（简称 LMA）。首先，我们介绍了开发 LMA 所涉及的基本组成部分，并将当前的研究主体分为四种不同的类型。随后，我们回顾了整合多个 LMA 的协作框架，以提高集体效率。该领域的关键挑战之一是现有研究中使用的评估方法多种多样，阻碍了不同 LMA 之间的有效比较。因此，我们编制了这些评估方法并建立了一个全面的框架来弥补差距。该框架旨在标准化评估，促进更有意义的比较。在总结我们的审查时，我们强调了 LMA 的广泛应用，并提出了未来可能的研究方向。我们的讨论旨在为这个快速发展的领域的未来研究提供有价值的见解和指南。最新资源列表位于 https://github.com/jun0wanan/awesome-large-multimodal-agents。]]></description>
      <guid>https://arxiv.org/abs/2402.15116</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:15 GMT</pubDate>
    </item>
    <item>
      <title>通过两步释义微调 CLIP 文本编码器</title>
      <link>https://arxiv.org/abs/2402.15120</link>
      <description><![CDATA[arXiv:2402.15120v1 公告类型：新
摘要：对比语言图像预训练（CLIP）模型在各种视觉语言任务中取得了相当大的成功，例如文本到图像检索，其中模型需要有效处理自然语言输入以产生准确的视觉输出。然而，当前模型在处理输入查询中的语言变化（例如释义）方面仍然面临限制，这使得在现实应用程序中处理广泛的用户查询具有挑战性。在本研究中，我们引入了一种简单的微调方法来增强 CLIP 模型的释义表示。我们的方法涉及两步释义生成过程，其中我们利用大型语言模型从网络规模的图像标题自动创建两类释义。随后，我们使用这些生成的释义微调 CLIP 文本编码器，同时冻结图像编码器。我们生成的模型（我们称之为 ParaCLIP）在各种任务上都比基线 CLIP 模型有了显着改进，包括释义检索（排名相似度得分提高了 2.0% 和 5.6%）、视觉基因组关系和归因，以及七种语义文本相似度任务。]]></description>
      <guid>https://arxiv.org/abs/2402.15120</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:15 GMT</pubDate>
    </item>
    <item>
      <title>薄岩部分的自动描述：Web 应用程序</title>
      <link>https://arxiv.org/abs/2402.15039</link>
      <description><![CDATA[arXiv:2402.15039v1 公告类型：新
摘要： 各种岩石类型的识别和表征是地质学及采矿、石油、环境、工业和建筑等相关领域的基本活动之一。传统上，人类专家负责使用现场收集或实验室制备的岩石样本来分析和解释有关类型、成分、纹理、形状和其他属性的详细信息。除了耗费大量时间和精力之外，结果还变得基于经验而变得主观。本提案使用结合计算机视觉和自然语言处理的人工智能技术，从岩石的薄片图像生成文本和口头描述。我们构建了图像及其各自文本描述的数据集，用于训练模型，将 EfficientNetB7 提取的图像的相关特征与 Transformer 网络生成的文本描述相关联，达到 0.892 的准确度值和 0.71 的 BLEU 值。该模型可以成为研究、专业和学术工作的有用资源，因此已通过 Web 应用程序部署供公众使用。]]></description>
      <guid>https://arxiv.org/abs/2402.15039</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:14 GMT</pubDate>
    </item>
    <item>
      <title>深层基础潜在空间内的无监督域适应</title>
      <link>https://arxiv.org/abs/2402.14976</link>
      <description><![CDATA[arXiv:2402.14976v1 公告类型：新
摘要：基于视觉变换器的基础模型，例如 ViT 或 Dino-V2，旨在解决很少或没有特征微调的问题。使用原型网络设置，我们分析了此类基础模型可以在多大程度上解决无监督域适应问题，而无需对源域或目标域进行微调。通过定量分析以及决策的定性解释，我们证明所建议的方法可以改进现有的基线，并展示这种方法尚未解决的局限性。]]></description>
      <guid>https://arxiv.org/abs/2402.14976</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>CLoVe：在对比视觉语言模型中编码组合语言</title>
      <link>https://arxiv.org/abs/2402.15021</link>
      <description><![CDATA[arXiv:2402.15021v1 公告类型：新
摘要：近年来，视觉和语言任务的表现显着提高。基础视觉语言模型 (VLM)，例如 CLIP，已在多种环境中得到利用，并在多项任务中表现出卓越的性能。此类模型擅长以对象为中心的识别，但学习的文本表示似乎与词序无关，无法以新颖的方式组成已知概念。然而，没有证据表明任何 VLM（包括 GPT-4V 等大型单流模型）能够成功识别成分。在本文中，我们引入了一个框架，可以显着提高现有模型编码组合语言的能力，组合性基准的绝对改进超过 10%，同时保持或提高标准对象识别和检索基准的性能。我们的代码和预训练模型可在 https://github.com/netflix/clove 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2402.15021</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:13 GMT</pubDate>
    </item>
    <item>
      <title>大多数自监督学习方法背后的共同稳定性机制</title>
      <link>https://arxiv.org/abs/2402.14957</link>
      <description><![CDATA[arXiv:2402.14957v1 公告类型：新
摘要：过去几年见证了自我监督学习（SSL）的巨大进步，其成功可归因于在学习过程中引入有用的归纳偏差，以学习有意义的视觉表示，同时避免崩溃。这些归纳偏差和约束以 SSL 技术中不同优化公式的形式表现出来，例如通过在对比公式中利用反例，或在 BYOL 和 SimSiam 中利用指数移动平均值和预测器。在本文中，我们提供了一个框架来解释这些不同 SSL 技术的稳定性机制：i）我们讨论了对比技术（如 SimCLR）和非对比技术（如 BYOL、SWAV、SimSiam、Barlow Twins 和 DINO）的工作机制； ii）我们提供了一个论点，尽管公式不同，但这些方法隐式地优化了相似的目标函数，即最小化所有数据样本的预期表示的大小或数据分布的平均值，同时最大化个体的预期表示的大小不同数据增强的样本； iii）我们提供数学和经验证据来支持我们的框架。我们提出不同的假设并使用 Imagenet100 数据集对其进行测试。]]></description>
      <guid>https://arxiv.org/abs/2402.14957</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>EE3P：基于事件的周期性现象属性估计</title>
      <link>https://arxiv.org/abs/2402.14958</link>
      <description><![CDATA[arXiv:2402.14958v1 公告类型：新
摘要：我们介绍了一种使用事件相机测量周期性现象特性的新方法，事件相机是一种异步报告独立操作像素的亮度变化的设备。该方法假设对于快速周期性现象，在其发生的任何空间窗口中，在与运动频率相对应的时间差处生成一组非常相似的事件。为了估计频率，我们计算事件空间中时空窗口的相关性。该周期是根据相关响应峰值之间的时间差计算的。该方法是非接触式的，无需标记，也不需要可区分的地标。我们在周期性现象的三个实例上评估所提出的方法：（i）闪光，（ii）振动和（iii）旋转速度。在所有实验中，我们的方法实现了低于 0.04% 的相对误差，在地面实况测量的误差范围内。]]></description>
      <guid>https://arxiv.org/abs/2402.14958</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:12 GMT</pubDate>
    </item>
    <item>
      <title>Deepfake 检测和有限计算能力的影响</title>
      <link>https://arxiv.org/abs/2402.14825</link>
      <description><![CDATA[arXiv:2402.14825v1 公告类型：新
摘要：技术和人工智能的快速发展使得深度伪造成为一种日益复杂且难以识别的技术。为了确保信息的准确性并控制错误信息和大规模操纵，发现和开发能够对伪造视频进行通用检测的人工智能模型至关重要。这项工作旨在解决在计算资源有限的情况下跨各种现有数据集检测深度伪造的问题。目标是分析不同深度学习技术在这些限制下的适用性，并探索提高其效率的可能方法。]]></description>
      <guid>https://arxiv.org/abs/2402.14825</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>别再推理了！当具有思想链推理的多模式法学硕士遇到对抗性图像时</title>
      <link>https://arxiv.org/abs/2402.14899</link>
      <description><![CDATA[arXiv:2402.14899v1 公告类型：新
摘要：最近，多模态法学硕士（MLLM）表现出了强大的图像理解能力。然而，与传统视觉模型一样，它们仍然容易受到对抗性图像的影响。同时，思想链（CoT）推理在MLLM上得到了广泛的探索，它不仅提高了模型的性能，而且通过给出中间推理步骤增强了模型的可解释性。尽管如此，仍然缺乏关于 MLLM 与 CoT 的对抗鲁棒性的研究，也缺乏对 MLLM 用对抗性图像推断错误答案时的基本原理的理解。我们的研究评估了 MLLM 在采用 CoT 推理时的对抗鲁棒性，发现 CoT 略微提高了针对现有攻击方法的对抗鲁棒性。此外，我们引入了一种新颖的停止推理攻击技术，可以有效绕过 CoT 引起的鲁棒性增强。最后，我们展示了当 MLLM 面对对抗性图像时 CoT 推理的变化，揭示了它们在对抗性攻击下的推理过程。]]></description>
      <guid>https://arxiv.org/abs/2402.14899</guid>
      <pubDate>Mon, 26 Feb 2024 06:18:11 GMT</pubDate>
    </item>
    </channel>
</rss>