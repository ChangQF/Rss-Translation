<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 26 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过内在和外在注意力分散实现面部身份匿名化</title>
      <link>https://arxiv.org/abs/2406.17219</link>
      <description><![CDATA[arXiv:2406.17219v1 公告类型：新
摘要：前所未有的人脸图像捕获和应用引发了人们对匿名化以防止隐私泄露的关注。大多数现有方法可能存在身份独立信息更改过多或身份保护不足的问题。在本文中，我们通过分散内在和外在身份注意力提出了一种新的人脸匿名化方法。一方面，我们通过分散内在身份注意力来匿名化特征空间中的身份信息。另一方面，我们通过分散外在身份注意力来匿名化视觉线索（即外观和几何结构）。我们的方法允许灵活直观地操纵面部外观和几何结构以产生不同的结果，并且它还可用于指导用户执行个性化匿名化。我们对多个数据集进行了广泛的实验，并证明我们的方法优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.17219</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:21 GMT</pubDate>
    </item>
    <item>
      <title>虚拟矿山——利用深度学习实现印刷电路板的组件级回收</title>
      <link>https://arxiv.org/abs/2406.17162</link>
      <description><![CDATA[arXiv:2406.17162v1 公告类型：新
摘要：本文概述了一个正在进行的项目，该项目使用机器学习和计算机视觉组件来改进电子垃圾回收过程。在循环经济中，“虚拟矿山”概念是指以高效且经济的方式从报废物品中回收有趣原材料的生产周期。特别是，由于高科技产品的生命周期越来越短，电子垃圾的增长是一个全球性问题。在本文中，我们描述了一种基于深度学习模型的流程，用于在组件级别回收印刷电路板。使用预先训练的 YOLOv5 模型来分析本地开发的数据集的结果。通过不同的类实例分布，YOLOv5 成功实现了令人满意的精度和召回率，并能够使用大型组件实例进行优化。]]></description>
      <guid>https://arxiv.org/abs/2406.17162</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:20 GMT</pubDate>
    </item>
    <item>
      <title>POPCat：用于复杂注释任务的粒子传播</title>
      <link>https://arxiv.org/abs/2406.17183</link>
      <description><![CDATA[arXiv:2406.17183v1 公告类型：新
摘要：当面对密集填充视频序列的独特类别时，为所有多目标跟踪、人群计数和基于工业的视频创建新的数据集是艰巨且耗时的。我们提出了一种称为 POPCat 的时间高效方法，该方法利用视频数据的多目标和时间特征来生成用于分割或基于框的视频注释的半监督管道。该方法保留了与人类级别注释相关的准确度水平，同时生成大量半监督注释以实现更大的泛化。该方法通过使用粒子跟踪器来扩展人类提供的目标点的范围，从而利用时间特征。这是通过使用粒子跟踪器将初始点重新关联到跟随标记帧的一组图像来实现的。然后使用该生成的数据训练 YOLO 模型，然后快速推断目标视频。评估是在 GMOT-40、AnimalTrack 和 Visdrone-2019 基准上进行的。这些多目标视频跟踪/检测集包含多个相似的目标、摄像机运动和其他在“野生”情况下常见的特征。我们特意选择了这些困难的数据集来展示管道的有效性并进行比较。在 GMOT-40、AnimalTrack 和 Visdrone 上应用的方法在召回率/mAP50/mAP 方面比最佳结果有改善，其中收集指标的值分别为 24.5%/9.6%/4.8%、-/43.1%/27.8% 和 7.5%/9.4%/7.5%。]]></description>
      <guid>https://arxiv.org/abs/2406.17183</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:20 GMT</pubDate>
    </item>
    <item>
      <title>MM-SpuBench：更好地理解多模态法学硕士 (LLM) 中的虚假偏差</title>
      <link>https://arxiv.org/abs/2406.17126</link>
      <description><![CDATA[arXiv:2406.17126v1 公告类型：新
摘要：虚假偏差是一种使用非必要输入属性和目标变量之间的虚假相关性进行预测的倾向，它揭示了在单模态数据上训练的深度学习模型中存在严重的鲁棒性缺陷。集成视觉和语言模型的多模态大型语言模型 (MLLM) 已展示出强大的视觉-语言联合理解能力。然而，虚假偏差是否在 MLLM 中普遍存在仍未得到充分探索。我们通过分析多模态环境中的虚假偏差来弥补这一差距，揭示了当视觉模型中的偏差级联到 MLLM 中视觉和文本标记之间的对齐时可能体现此问题的特定测试数据模式。为了更好地理解这个问题，我们引入了 MM-SpuBench，这是一个全面的视觉问答 (VQA) 基准，旨在评估 MLLM 对来自五个开源图像数据集的九种不同类别的虚假相关性的依赖程度。VQA 数据集是根据人类可理解的概念信息（属性）构建的。利用这个基准，我们对当前最先进的 MLLM 进行了全面评估。我们的研究结果阐明了这些模型对虚假相关性的依赖仍然存在，并强调了对减轻虚假偏差的新方法的迫切需求。为了支持 MLLM 稳健性研究，我们在 https://huggingface.co/datasets/mmbench/MM-SpuBench 发布了我们的 VQA 基准。]]></description>
      <guid>https://arxiv.org/abs/2406.17126</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:19 GMT</pubDate>
    </item>
    <item>
      <title>Vastextures：使用无监督方法从真实世界图像中提取的大量纹理和 PBR 材料库</title>
      <link>https://arxiv.org/abs/2406.17146</link>
      <description><![CDATA[arXiv:2406.17146v1 公告类型：新
摘要：Vastextures 是一个庞大的存储库，其中包含使用无监督过程从真实世界图像中提取的 500,000 个纹理和 PBR 材料。提取的材料和纹理极其多样化，涵盖了广泛的真实世界模式，但与现有存储库相比，其精细度较低。该存储库由从自然图像中裁剪的 2D 纹理和从这些纹理生成的 SVBRDF/PBR 材料组成。纹理和 PBR 材料对于 CGI 至关重要。现有的材料库专注于游戏、动画和艺术，这些都需要有限数量的高质量资产。然而，虚拟世界和合成数据对于训练用于计算机视觉的人工智能系统变得越来越重要。此应用程序需要大量多样化的资产，但同时受噪声和未精炼资产的影响较小。 Vastexture 旨在通过创建一个免费、庞大且多样化的资产存储库来满足这一需求，该存储库涵盖尽可能多的现实世界材料。这些材料通过两个步骤从自然图像中自动提取：1）自动扫描大量图像以识别和裁剪具有均匀纹理的区域。这是通过将图像分割成一个单元格网格并识别所有单元格共享相似统计分布的区域来完成的。2）从裁剪的纹理中提取 PBR 材料的属性。这是通过随机猜测纹理图像的属性和 PBR 材料的属性之间的每个相关性来完成的。生成的 PBR 材料表现出大量现实世界模式以及意想不到的突发属性。在这个存储库上训练的中性网络优于使用手工制作的资产训练的网络。]]></description>
      <guid>https://arxiv.org/abs/2406.17146</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:19 GMT</pubDate>
    </item>
    <item>
      <title>明确识别不应仅仅依赖自然语言训练</title>
      <link>https://arxiv.org/abs/2406.17148</link>
      <description><![CDATA[arXiv:2406.17148v1 公告类型：新
摘要：在使用基于 Transformer 的架构进行 LaTeX 文本识别时，本文确定了某些“偏差”问题。例如，$e-t$ 经常被误识别为 $e^{-t}$。这种偏差源于数据集的固有特性。为了减轻这种偏差，我们提出了一种在伪公式和伪文本混合数据集上训练的 LaTeX 印刷文本识别模型。该模型采用 Swin Transformer 作为编码器，采用 RoBERTa 模型作为解码器。实验结果表明，这种方法减少了“偏差”，提高了文本识别的准确性和鲁棒性。对于清晰的图像，该模型严格遵循图像内容；对于模糊的图像，它整合了图像和上下文信息以产生合理的识别结果。]]></description>
      <guid>https://arxiv.org/abs/2406.17148</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:19 GMT</pubDate>
    </item>
    <item>
      <title>评估大型视觉语言模型的幻觉基准的质量</title>
      <link>https://arxiv.org/abs/2406.17115</link>
      <description><![CDATA[arXiv:2406.17115v1 公告类型：新
摘要：尽管近年来大型视觉语言模型（LVLM）取得了快速发展并表现出色，但 LVLM 一直受到幻觉问题的困扰，即 LVLM 往往会产生与相应视觉输入不一致的反应。为了评估 LVLM 中的幻觉程度，先前的研究提出了一系列具有不同类型任务和评估指标的基准。然而，我们发现现有的幻觉基准质量参差不齐，有些存在问题，例如重复测试下的评估结果不一致，以及与人类评估不一致。为此，我们提出了一个幻觉基准质量测量框架（HQM），该框架利用各种指标分别评估现有幻觉基准的可靠性和有效性。具体来说，对于信度，我们探索重测信度和平行形式信度，而对于效度，我们检查幻觉类型的标准效度和覆盖范围。此外，基于质量测量的结果，我们为 LVLM 构建了高质量幻觉基准 (HQH)。我们对 10 多个代表性 LVLM（包括 GPT-4o 和 Gemini-Vision-Pro）进行了广泛的评估，以对现有模型中的幻觉问题进行深入分析。我们的基准在 https://github.com/HQHBench/HQHBench 上公开提供。]]></description>
      <guid>https://arxiv.org/abs/2406.17115</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:18 GMT</pubDate>
    </item>
    <item>
      <title>使用 Little Companions 加速图像分类器</title>
      <link>https://arxiv.org/abs/2406.17117</link>
      <description><![CDATA[arXiv:2406.17117v1 公告类型：新
摘要：扩大神经网络一直是大型语言和视觉模型成功的关键秘诀。然而，在实践中，扩大规模的模型在计算方面的成本可能不成比例，性能的改善却微不足道；例如，EfficientViT-L3-384 在 ImageNet-1K 准确率上比基础 L1-224 模型提高了不到 2%，同时需要多 $14\times$ 次乘法累加运算 (MAC)。在本文中，我们研究了用于图像分类的流行神经网络系列的扩展特性，并发现扩大规模的模型主要有助于解决“困难”样本。通过按难度分解样本，我们开发了一个简单的与模型无关的两遍 Little-Big 算法，该算法首先使用轻量级的“小”模型对所有样本进行预测，然后只传递困难的样本以供“大”模型解决。好伙伴为各种模型系列和规模实现了 MAC 的大幅减少。在不损失准确度或修改现有模型的情况下，我们的 Little-Big 模型在 ImageNet-1K 上实现了 EfficientViT-L3-384 76%、EfficientNet-B7-600 81% 和 DeiT3-L-384 71% 的 MAC 减少。Little-Big 还将 InternImage-G-512 模型的速度提高了 62%，同时实现了 90% 的 ImageNet-1K top-1 准确度，既是强大的基准，也是大型模型压缩的简单实用方法。]]></description>
      <guid>https://arxiv.org/abs/2406.17117</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:18 GMT</pubDate>
    </item>
    <item>
      <title>减少 3D 高斯溅射的内存占用</title>
      <link>https://arxiv.org/abs/2406.17074</link>
      <description><![CDATA[arXiv:2406.17074v1 公告类型：新
摘要：3D 高斯分层为新颖的视图合成提供了出色的视觉质量，具有快速训练和实时渲染的功能；不幸的是，这种方法的存储和传输内存要求过高。我们首先分析了造成这种情况的原因，确定了可以减少存储的三个主要领域：用于表示场景的 3D 高斯基元的数量、用于表示方向辐射的球面谐波的系数数量以及存储高斯基元属性所需的精度。我们针对每个问题提出了解决方案。首先，我们提出了一种有效的、分辨率感知的基元修剪方法，将基元数量减少了一半。其次，我们引入了一种自适应调整方法来选择用于表示每个高斯基元的方向辐射的系数数量，最后引入了一种基于码本的量化方法，以及半浮点表示以进一步减少内存。综合起来，在我们测试的标准数据集上，这三个组件使磁盘占用总体大小减少了 27%，同时渲染速度提高了 1.7%。我们在标准数据集上演示了我们的方法，并展示了我们的解决方案如何在移动设备上使用该方法时显著缩短下载时间。]]></description>
      <guid>https://arxiv.org/abs/2406.17074</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:17 GMT</pubDate>
    </item>
    <item>
      <title>微调扩散模型以增强文本到图像生成中的面部质量</title>
      <link>https://arxiv.org/abs/2406.17100</link>
      <description><![CDATA[arXiv:2406.17100v1 公告类型：新
摘要：扩散模型 (DM) 在根据文本描述生成富有想象力的图像方面取得了显著成功。然而，当涉及到具有复杂细节的现实场景时，它们可能会失败。文本到图像生成中低质量、不真实的人脸是最突出的问题之一，阻碍了 DM 在实践中的广泛应用。针对这一问题，我们首先在人工注释者的帮助下评估流行的预训练 DM 生成的脸部质量，然后评估现有指标（如 ImageReward、人类偏好分数、美学分数预测器和脸部质量评估）与人类判断之间的一致性。观察到现有指标无法令人满意地量化人脸质量，我们通过在由 DM 修复流程廉价制作的（好、坏）人脸对数据集上微调 ImageReward，开发了一种名为人脸分数 (FS) 的新指标。大量研究表明，FS 与人类具有更好的一致性。另一方面，FS 为改进 DM 以生成更好的人脸打开了大门。为了实现这一点，我们在上述人脸对的去噪轨迹上加入了指导损失，以微调预先训练的 DM，例如 Stable Diffusion V1.5 和 Realistic Vision V5.1。直观地说，这种损失将坏人脸的轨迹推向好人脸的轨迹。全面的实验验证了我们的方法在保持一般能力的同时提高人脸质量的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.17100</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:17 GMT</pubDate>
    </item>
    <item>
      <title>GMT：用于叶子实例分割的引导式掩模变换器</title>
      <link>https://arxiv.org/abs/2406.17109</link>
      <description><![CDATA[arXiv:2406.17109v1 公告类型：新
摘要：叶子实例分割是一项具有挑战性的多实例分割任务，旨在分离和描绘植物图像中的每片叶子。描绘每片叶子是几个与生物学相关的应用的必要先决任务，例如植物生长的细粒度监测和作物产量估计。这项任务具有挑战性，因为实例的自相似性很高（形状和颜色相似），并且在重度遮挡下实例的大小差异很大。
我们认为克服上述挑战的关键在于叶子分布的特定空间模式。例如，叶子通常围绕植物的中心生长，较小的叶子聚集并重叠在这个中心点附近。在本文中，我们提出了一种名为引导式掩模变换器 (GMT) 的新方法，该方法包含三个关键组件，即引导式位置编码 (GPE)、引导式嵌入融合模块 (GEFM) 和引导式动态位置查询 (GDPQ)，以扩展 Mask2Former 的元架构并结合一组谐波引导函数。这些引导函数根据实例的像素位置进行定制，并经过训练以在嵌入空间中分离不同的实例。所提出的 GMT 在三个公共植物数据集上的表现始终优于最先进的模型。]]></description>
      <guid>https://arxiv.org/abs/2406.17109</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:17 GMT</pubDate>
    </item>
    <item>
      <title>Dwarf：用于改善注意力图谱的疾病加权网络</title>
      <link>https://arxiv.org/abs/2406.17032</link>
      <description><![CDATA[arXiv:2406.17032v1 公告类型：新
摘要：深度学习的可解释性对于评估医学成像模型的可靠性和降低患者推荐不准确的风险至关重要。本研究通过将医疗专业人员纳入可解释性过程来解决医学图像分析中的“人为脱节”和“可信度”问题。我们提出了一种疾病加权注意力图细化网络 (Dwarf)，利用专家反馈来增强模型的相关性和准确性。我们的方法采用循环训练来迭代提高诊断性能，生成精确且可解释的特征图。实验结果表明，跨多个医学成像数据集的可解释性和诊断准确性有显著提高。这种方法促进了人工智能系统和医疗专业人员之间的有效合作，最终旨在改善患者的治疗效果]]></description>
      <guid>https://arxiv.org/abs/2406.17032</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:16 GMT</pubDate>
    </item>
    <item>
      <title>通过跨模态学习增强科学图形字幕</title>
      <link>https://arxiv.org/abs/2406.17047</link>
      <description><![CDATA[arXiv:2406.17047v1 公告类型：新 
摘要：科学图表是有效传达研究成果的重要工具，是传达信息和揭示数据模式的重要媒介。随着科技的飞速进步，再加上大数据时代的到来，科研数据的数量和多样性激增，导致图表的数量和种类也随之增加。这一趋势给研究人员带来了新的挑战，特别是如何高效、准确地为这些图表生成合适的标题，以更好地传达他们的信息和结果。自动生成的图表标题可以为详细的图表分类提供精确的数据，从而增强信息检索系统。随着图像字幕和文本摘要研究的日趋成熟，科学图表标题的自动生成受到了广泛关注。通过利用自然语言处理、机器学习和多模态技术，可以自动从图表中提取关键信息，并生成准确、简洁的标题，以更好地满足研究人员的需求。本文提出了一种科学图表标题生成的新方法，证明了其在提高研究数据清晰度和可访问性方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2406.17047</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:16 GMT</pubDate>
    </item>
    <item>
      <title>使用带有软标签的合成样本减轻噪声监督</title>
      <link>https://arxiv.org/abs/2406.16966</link>
      <description><![CDATA[arXiv:2406.16966v1 公告类型：新
摘要：嘈杂标签在现实世界的数据集中无处不在，尤其是在来自众包和网络搜索的大规模数据中。使用嘈杂数据集训练深度神经网络是一项挑战，因为网络在训练过程中容易过度拟合嘈杂的标签，导致泛化性能不佳。在早期学习阶段，已经观察到深度神经网络在记忆错误标记的样本之前先拟合干净的样本。在本文中，我们深入研究了早期学习阶段的表示分布，发现无论它们的噪声标签如何，来自同一类别的图像的学习到的表示仍然聚集在一起。受此启发，我们提出了一个框架，使用新的合成样本训练模型以减轻噪声标签的影响。具体而言，我们提出了一种混合策略，通过将原始样本与其前 K 个最近邻居聚合来创建合成样本，其中使用从每个样本损失分布中学习的混合模型计算权重。为了在标签噪声极大的情况下提高性能，我们通过逐步校正噪声标签来估计软目标。此外，我们证明估计的软目标可以更准确地近似真实标签，并且所提出的方法可以产生更优质的学习表示，具有更分离和更清晰的边界簇。在两个基准（CIFAR-10 和 CIFAR-100）和两个大规模真实世界数据集（Clothing1M 和 Webvision）中进行的大量实验表明，我们的方法优于最先进的方法和学习表示的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2406.16966</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:15 GMT</pubDate>
    </item>
    <item>
      <title>PVUW 2024 复杂视频理解挑战赛：方法与结果</title>
      <link>https://arxiv.org/abs/2406.17005</link>
      <description><![CDATA[arXiv:2406.17005v1 公告类型：新
摘要：野外像素级视频理解挑战赛 (PVUW) 专注于复杂视频理解。在本次 CVPR 2024 研讨会上，我们添加了两个新赛道，基于 MOSE 数据集的复杂视频对象分割赛道和基于 MeViS 数据集的运动表达引导视频分割赛道。在这两个新赛道中，我们提供了额外的视频和注释，这些视频和注释具有具有挑战性的元素，例如 MOSE 中物体的消失和重新出现、不显眼的小物体、严重遮挡和拥挤的环境。此外，我们提供了一个新的运动表达引导视频分割数据集 MeViS，以研究复杂环境中的自然语言引导视频理解。这些新的视频、句子和注释使我们能够促进对复杂环境和现实场景中视频场景的更全面、更强大的像素级理解的发展。 MOSE 挑战赛共有 140 支队伍报名，其中 65 支队伍参与验证阶段，12 支队伍在最终挑战阶段提交了有效作品；MeViS 挑战赛共有 225 支队伍报名，其中 50 支队伍参与验证阶段，5 支队伍在最终挑战阶段提交了有效作品。]]></description>
      <guid>https://arxiv.org/abs/2406.17005</guid>
      <pubDate>Wed, 26 Jun 2024 06:21:15 GMT</pubDate>
    </item>
    </channel>
</rss>