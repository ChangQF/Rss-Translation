<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 19 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>TutteNet：通过组合二维网格变形实现注入式三维变形</title>
      <link>https://arxiv.org/abs/2406.12121</link>
      <description><![CDATA[arXiv:2406.12121v1 公告类型：新
摘要：这项工作提出了一种 3D 空间注入变形的新表示，它克服了注入方法现有的局限性：不准确、缺乏鲁棒性以及与一般学习和优化框架不兼容。核心思想是将问题简化为多个基于 2D 网格的分段线性映射的深度组合。即，我们构建可微分层，通过 Tutte 嵌入产生网格变形（保证在 2D 中是注入的），并在不同的平面上组合这些层以创建 3D 体积的复杂 3D 注入变形。我们展示了我们的方法能够有效、准确地优化和学习复杂变形，优于其他注入方法。作为主要应用，我们生成复杂且无伪影的 NeRF 和 SDF 变形。]]></description>
      <guid>https://arxiv.org/abs/2406.12121</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>COT Flow：通过对比对学习最佳传输图像采样和编辑</title>
      <link>https://arxiv.org/abs/2406.12140</link>
      <description><![CDATA[arXiv:2406.12140v1 公告类型：新
摘要：扩散模型在采样和编辑多模态数据方面表现出色，生成质量高，但它们受到计算成本高且速度慢的迭代生成过程的影响。此外，大多数方法都局限于从高斯噪声中生成数据，这限制了它们的采样和编辑灵活性。为了克服这两个缺点，我们提出了对比最优传输流 (COT Flow)，这是一种新方法，与之前的扩散模型相比，它可以实现快速、高质量的生成，并提高零样本编辑灵活性。得益于最优传输 (OT)，我们的方法对先验分布没有限制，与其他零样本编辑方法相比，可以实现不成对的图像到图像 (I2I) 转换，并将可编辑空间加倍（在轨迹的开始和结束处）。在质量方面，与之前最先进的不成对的图像到图像 (I2I) 转换方法相比，COT Flow 仅需一步即可生成具有竞争力的结果。为了通过引入 OT 来凸显 COT Flow 的优势，我们引入了 COT Editor，以极佳的灵活性和质量进行用户引导式编辑。代码将发布在 https://github.com/zuxinrui/cot_flow。]]></description>
      <guid>https://arxiv.org/abs/2406.12140</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>MiSuRe 可以解释你的图像分割</title>
      <link>https://arxiv.org/abs/2406.12173</link>
      <description><![CDATA[arXiv:2406.12173v1 公告类型：新
摘要：由于深度学习架构取得了无与伦比的成功，在过去十年的计算机视觉领域中，深度学习架构一直占据主导地位。然而，由于其高度非线性的性质，它们的性能往往以牺牲可解释性为代价。因此，可解释人工智能 (XAI) 的平行领域已经发展起来，旨在深入了解深度学习模型的决策过程。XAI 中的一个重要问题是生成显着图。这些是输入图像中对模型的最终决策贡献最大的区域。然而，这方面的大多数工作都集中在图像分类上，而图像分割——尽管是一项普遍的任务——并没有受到同样的关注。在目前的研究中，我们提出了 MiSuRe（最小充分区域）作为一种生成图像分割显着图的算法。 MiSuRe 生成的显著性图的目的是去除不相关的区域，只突出显示输入图像中对图像分割决策至关重要的区域。我们对 3 个数据集进行了分析：Triangle（人工构建）、COCO-2017（自然图像）和 Synapse 多器官（医学图像）。此外，我们确定了这些事后显著性图的潜在用例，以便对分割模型进行事后可靠性分析。]]></description>
      <guid>https://arxiv.org/abs/2406.12173</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:40 GMT</pubDate>
    </item>
    <item>
      <title>DistillNeRF：通过提炼神经场和基础模型特征从单眼图像中感知 3D 场景</title>
      <link>https://arxiv.org/abs/2406.12095</link>
      <description><![CDATA[arXiv:2406.12095v1 公告类型：新
摘要：我们提出了 DistillNeRF，这是一个自监督学习框架，旨在解决自动驾驶中从有限的 2D 观察中理解 3D 环境的挑战。我们的方法是一个可泛化的前馈模型，可以从稀疏的单帧多视角摄像机输入中预测丰富的神经场景表示，并通过可微分渲染进行自监督训练以重建 RGB、深度或特征图像。我们的第一个见解是通过生成密集的深度和虚拟摄像机目标进行训练来利用针对每个场景优化的神经辐射场 (NeRF)，从而帮助我们的模型从稀疏的非重叠图像输入中学习 3D 几何。其次，为了学习语义丰富的 3D 表示，我们建议从预先训练的 2D 基础模型（例如 CLIP 或 DINOv2）中提取特征，从而无需昂贵的 3D 人工注释即可实现各种下游任务。为了利用这两个见解，我们引入了一种新颖的模型架构，该架构具有两阶段升压-溅射-射击编码器和参数化的稀疏分层体素表示。 NuScenes 数据集上的实验结果表明，DistillNeRF 在场景重建、新颖视图合成和深度估计方面明显优于现有的同类自监督方法；并且它允许进行具有竞争力的零样本 3D 语义占用预测，以及通过提炼的基础模型特征进行开放世界场景理解。演示和代码将在 https://distillnerf.github.io/ 上提供。]]></description>
      <guid>https://arxiv.org/abs/2406.12095</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:39 GMT</pubDate>
    </item>
    <item>
      <title>Deep HM-SORT：利用深度特征、谐波均值和扩展 IOU 增强体育运动中的多目标跟踪</title>
      <link>https://arxiv.org/abs/2406.12081</link>
      <description><![CDATA[arXiv:2406.12081v1 公告类型：新
摘要：本文介绍了 Deep HM-SORT，这是一种新颖的在线多目标跟踪算法，专门用于增强运动场景中运动员的跟踪。传统的多目标跟踪方法通常会因运动员的相似外观、不规则和不可预测的动作以及明显的相机运动而难以适应运动环境。Deep HM-SORT 通过集成深度特征、谐波平均值和扩展 IOU 解决了这些挑战。通过利用谐波平均值，我们的方法有效地平衡了外观和运动线索，显著减少了 ID 交换。此外，我们的方法无限期地保留所有轨迹，从而提高了离开并重新进入画面的玩家的重新识别能力。实验结果表明，Deep HM-SORT 在两个大型公共基准 SportsMOT 和 SoccerNet Tracking Challenge 2023 上取得了最佳表现。具体来说，我们的方法在 SportsMOT 数据集上取得了 80.1 HOTA，在 SoccerNet-Tracking 数据集上取得了 85.4 HOTA，在 HOTA、IDF1、AssA 和 MOTA 等关键指标上的表现优于现有追踪器。这种强大的解决方案为自动体育分析提供了更高的准确性和可靠性，与以前的方法相比有显著的改进，而不会增加额外的计算成本。]]></description>
      <guid>https://arxiv.org/abs/2406.12081</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>微调隐函数的不确定性建模</title>
      <link>https://arxiv.org/abs/2406.12082</link>
      <description><![CDATA[arXiv:2406.12082v1 公告类型：新
摘要：隐式函数（例如神经辐射场 (NeRF)、占用网络和有符号距离函数 (SDF)）已成为计算机视觉中从稀疏视图重建详细物体形状的关键。由于输入的极端稀疏性和数据损坏引起的分布变化，使用这些模型实现最佳性能可能具有挑战性。为此，大型无噪声合成数据集可以作为形状先验来帮助模型填补空白，但必须谨慎处理由此产生的重建。不确定性估计对于评估这些重建的质量至关重要，特别是在识别模型对其从先验推断出的部分不确定的区域时。在本文中，我们介绍了 Dropsembles，这是一种用于调整隐式函数中的不确定性估计的新方法。我们通过一系列实验证明了我们方法的有效性，从玩具示例开始，然后发展到真实场景。具体来说，我们在合成解剖数据上训练卷积占用网络，并在腰椎的低分辨率 MRI 分割上对其进行测试。我们的结果表明，Dropsembles 达到了深度集成的准确度和校准水平，但计算成本却大大降低。]]></description>
      <guid>https://arxiv.org/abs/2406.12082</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:38 GMT</pubDate>
    </item>
    <item>
      <title>多维修剪：具有延迟约束的联合通道、层和块修剪</title>
      <link>https://arxiv.org/abs/2406.12079</link>
      <description><![CDATA[arXiv:2406.12079v1 公告类型：新
摘要：随着我们在各种视觉任务中突破性能界限，模型的大小也相应增大。为了跟上这种增长，我们需要非常积极的修剪技术，以便在边缘设备上进行有效的推理和部署。现有的修剪方法仅限于通道修剪，并且难以进行积极的参数减少。在本文中，我们提出了一种新颖的多维修剪框架，该框架在遵守延迟约束的同时，联合优化了跨通道、层和块的修剪。我们开发了一种延迟建模技术，可以准确捕获修剪过程中模型范围内的延迟变化，这对于在高修剪率下实现最佳延迟-准确性权衡至关重要。我们将修剪重新表述为混合整数非线性程序 (MINLP)，以便仅通过一次传递即可有效地确定最佳修剪结构。我们的大量结果表明，与以前的方法相比，这种方法有显著的改进，尤其是在较大的修剪率下。在分类方面，我们的方法明显优于现有技术的 HALP，Top-1 准确率为 70.0（v.s. 68.6），FPS 为 5262 英米/秒（v.s. 4101 英米/秒）。在 3D 物体检测方面，我们通过以 45% 的修剪率修剪 StreamPETR 建立了新的最先进技术，实现了比密集基线更高的 FPS（37.3 vs. 31.7）和 mAP（0.451 vs. 0.449）。]]></description>
      <guid>https://arxiv.org/abs/2406.12079</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>用于实时渲染超大数据集的分层三维高斯表示</title>
      <link>https://arxiv.org/abs/2406.12080</link>
      <description><![CDATA[arXiv:2406.12080v1 公告类型：新
摘要：近年来，新颖的视图合成取得了重大进展，3D 高斯拼贴提供了出色的视觉质量、快速训练和实时渲染。然而，训练和渲染所需的资源不可避免地限制了可以以良好的视觉质量表示的捕获场景的大小。我们引入了一个 3D 高斯层次结构，它可以为非常大的场景保留视觉质量，同时提供有效的细节层次 (LOD) 解决方案，以有效选择级别和级别之间的平滑过渡来高效渲染远处的内容。我们引入了一种分而治之的方法，使我们能够以独立的块来训练非常大的场景。我们将这些块合并到一个层次结构中，可以对其进行优化，以进一步提高合并到中间节点的高斯的视觉质量。非常大的捕获通常对场景的覆盖很稀疏，这对原始的 3D 高斯拼贴训练方法提出了许多挑战；我们调整并规范了训练以解决这些问题。我们提出了一个完整的解决方案，该解决方案能够实时渲染非常大的场景，并且由于我们的 LOD 方法，可以适应可用的资源。我们展示了使用简单且经济实惠的设备捕获的场景的结果，其中包含多达数万张图像，覆盖的轨迹长达数公里，持续时间长达一小时。项目页面：https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/]]></description>
      <guid>https://arxiv.org/abs/2406.12080</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:37 GMT</pubDate>
    </item>
    <item>
      <title>MEDeA：多视角高效深度调整</title>
      <link>https://arxiv.org/abs/2406.12048</link>
      <description><![CDATA[arXiv:2406.12048v1 公告类型：新
摘要：大多数现代单视图深度估计方法预测相对深度，因此尽管在基准测试中表现出色，但无法直接应用于许多实际场景。此外，单视图方法无法保证帧序列之间的一致性。一致性通常通过测试时间优化视图之间的差异来解决；然而，处理一个场景需要几个小时。在本文中，我们提出了一种高效的多视图测试时间深度调整方法 MEDeA，它比现有的测试时间方法快一个数量级。给定具有相机参数的 RGB 帧，MEDeA 预测初始深度图，通过优化局部缩放系数对其进行调整，并输出时间一致的深度图。与需要法线、光流或语义估计的测试时间方法相反，MEDeA 仅使用深度估计网络即可产生高质量的预测。我们的方法在 TUM RGB-D、7Scenes 和 ScanNet 基准上树立了新的领先地位，并成功处理了来自 ARKitScenes 数据集的智能手机捕获的数据。]]></description>
      <guid>https://arxiv.org/abs/2406.12048</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>FAWN：用于直接神经 TSDF 重建的 Floor-And-Walls 正则化</title>
      <link>https://arxiv.org/abs/2406.12054</link>
      <description><![CDATA[arXiv:2406.12054v1 公告类型：新
摘要：利用 3D 语义进行直接 3D 重建具有巨大的潜力。例如，通过假设墙壁是垂直的，地板是平面和水平的，我们可以校正扭曲的房间形状并消除局部伪影，例如洞、坑和山丘。在本文中，我们提出了 FAWN，这是截断有符号距离函数 (TSDF) 重建方法的修改，它通过检测场景中的墙壁和地板来考虑场景结构，并对偏离水平和垂直方向的相应表面法线进行惩罚。FAWN 作为 3D 稀疏卷积模块实现，可以合并到任何可预测 TSDF 的可训练管道中。由于 FAWN 只需要 3D 语义进行训练，因此对进一步使用没有额外的限制。我们证明，与现有的基于语义的方法相比，FAWN 修改的方法更有效地使用语义。此外，我们将修改应用于最先进的 TSDF 重建方法，并在 SCANNET、ICL-NUIM、TUM RGB-D 和 7SCENES 基准中展示了质量的提升。]]></description>
      <guid>https://arxiv.org/abs/2406.12054</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:36 GMT</pubDate>
    </item>
    <item>
      <title>并非所有提示都是平等的：基于提示的文本到图像扩散模型的修剪</title>
      <link>https://arxiv.org/abs/2406.12042</link>
      <description><![CDATA[arXiv:2406.12042v1 公告类型：新
摘要：文本到图像 (T2I) 传播模型已经展示了令人印象深刻的图像生成能力。尽管如此，它们的计算强度阻止资源受限的组织在其内部目标数据上对 T2I 模型进行微调后部署它们。虽然修剪技术提供了一种潜在的解决方案来减少 T2I 模型的计算负担，但静态修剪方法对所有输入提示使用相同的修剪模型，忽略了不同提示的不同容量要求。动态修剪通过为每个提示使用单独的子网络来解决此问题，但它阻止了 GPU 上的批量并行性。为了克服这些限制，我们引入了自适应提示定制修剪 (APTP)，这是一种专为 T2I 传播模型设计的新型基于提示的修剪方法。我们方法的核心是提示路由器模型，它学习确定输入文本提示所需的容量并将其路由到架构代码，给定提示所需的总计算预算。每个架构代码都代表一个专门针对分配给它的提示而定制的专门模型，代码数量是一个超参数。我们使用对比学习来训练提示路由器和架构代码，确保将类似的提示映射到附近的代码。此外，我们采用最佳传输来防止代码折叠成单个代码。我们通过使用 CC3M 和 COCO 作为目标数据集修剪稳定扩散 (SD) V2.1 来证明 APTP 的有效性。在 FID、CLIP 和 CMMD 分数方面，APTP 的表现优于单模型修剪基线。我们对 APTP 学习到的集群的分析表明它们具有语义意义。我们还表明，APTP 可以自动发现以前经验性发现的具有挑战性的 SD 提示，例如，生成文本图像的提示，并将它们分配给更高容量的代码。]]></description>
      <guid>https://arxiv.org/abs/2406.12042</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>艺术家：通过解缠改进富文本图像的生成</title>
      <link>https://arxiv.org/abs/2406.12044</link>
      <description><![CDATA[arXiv:2406.12044v1 公告类型：新
摘要：扩散模型在生成广泛的视觉内容方面表现出卓越的能力，但它们在渲染文本方面的能力仍然有限：它们经常生成不准确的字符或单词，无法与底层图像很好地融合。为了解决这些缺点，我们引入了一个名为 ARTIST 的新框架。该框架包含一个专用的文本扩散模型，专门用于学习文本结构。最初，我们对这个文本模型进行预训练，以捕捉文本表示的复杂性。随后，我们对视觉扩散模型进行微调，使其能够从预训练的文本模型中吸收文本结构信息。这种解开的架构设计和训练策略显着增强了扩散模型在生成富文本图像时的文本渲染能力。此外，我们利用预训练的大型语言模型的功能来更好地解释用户意图，从而有助于提高生成质量。 MARIO-Eval 基准上的经验结果强调了所提出方法的有效性，显示出各项指标均提高了 15%。]]></description>
      <guid>https://arxiv.org/abs/2406.12044</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:35 GMT</pubDate>
    </item>
    <item>
      <title>文本到图像模型中地理差异的分解评估</title>
      <link>https://arxiv.org/abs/2406.11988</link>
      <description><![CDATA[arXiv:2406.11988v1 公告类型：新
摘要：最近的研究发现，不同地理区域的生成图像存在很大差异，包括对房屋和汽车等日常物品的刻板描述。然而，现有的衡量这些差异的指标要么是耗时且昂贵的人工评估，要么是评估完整图像的自动指标，无法将这些差异归因于生成图像的特定部分。在这项工作中，我们引入了一组新的指标，即图像生成差异的分解指标 (Decomposed-DIG)，它使我们能够分别测量生成图像中对象和背景描述的地理差异。使用 Decomposed-DIG，我们审核了一个广泛使用的潜在扩散模型，发现生成的图像对对象的描绘比背景更真实，并且生成的图像中的背景往往比对象包含更大的区域差异。我们使用 Decomposed-DIG 来精确定位差异的具体示例，例如非洲刻板的背景生成、难以在非洲生成现代车辆以及不切实际地将某些物体放置在户外环境中。根据我们的指标，我们使用了一种新的提示结构，该结构可使最差区域改进 52%，生成的背景多样性平均改进 20%。]]></description>
      <guid>https://arxiv.org/abs/2406.11988</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>SPA-VL：视觉语言模型的综合安全偏好对齐数据集</title>
      <link>https://arxiv.org/abs/2406.12030</link>
      <description><![CDATA[arXiv:2406.12030v1 公告类型：新
摘要：视觉语言模型（VLM）的出现为理解多模态信息带来了前所未有的进步。VLM 中文本和视觉语义的组合高度复杂且多样，使得这些模型的安全对齐具有挑战性。此外，由于对 VLM 安全对齐的研究有限，缺乏大规模、高质量的数据集。为了解决这些限制，我们提出了一个名为 SPA-VL 的视觉语言模型安全偏好对齐数据集。在广度方面，SPA-VL 涵盖 6 个有害域、13 个类别和 53 个子类别，并包含四元组（问题、图像、选择的响应、拒绝的响应）的 100,788 个样本。在深度方面，响应是从 12 个开源（例如 QwenVL）和闭源（例如 Gemini）VLM 收集的，以确保多样性。实验结果表明，在 SPA-VL 数据集上使用对齐技术训练的模型在保持核心能力的同时，在无害性和有用性方面表现出了显著的提升。SPA-VL 作为一个大规模、高质量和多样化的数据集，代表了确保 VLM 同时实现无害性和有用性的重要里程碑。我们已将代码 https://github.com/EchoseChen/SPA-VL-RLHF 和 SPA-VL 数据集网址 https://huggingface.co/datasets/sqrti/SPA-VL 公开。]]></description>
      <guid>https://arxiv.org/abs/2406.12030</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:34 GMT</pubDate>
    </item>
    <item>
      <title>在大型遥感数据集上扩展高效的掩蔽自动编码器学习</title>
      <link>https://arxiv.org/abs/2406.11933</link>
      <description><![CDATA[arXiv:2406.11933v1 公告类型：新
摘要：蒙版图像建模 (MIM) 已成为开发遥感 (RS) 领域基础视觉模型的关键方法。然而，当前的 RS 数据集在数量和多样性上有限，这严重限制了 MIM 方法学习可泛化表示的能力。在本研究中，我们引入了 \textbf{RS-4M}，这是一个大规模数据集，旨在实现对 RS 图像的高效 MIM 训练。RS-4M 包含 400 万张光学图像，涵盖丰富且细粒度的 RS 视觉任务，包括对象级检测和像素级分割。与自然图像相比，RS 图像通常包含大量冗余背景像素，这限制了传统 MIM 模型的训练效率。为了解决这个问题，我们提出了一种有效的 MIM 方法，称为 \textbf{SelectiveMAE}，它根据语义丰富度动态编码和重建选择的补丁标记子集。 SelectiveMAE 根植于渐进式语义标记选择模块，该模块从重建语义类比标记演变为编码互补语义依赖关系。这种方法将传统的 MIM 训练转变为渐进式特征学习过程，使 SelectiveMAE 能够有效地学习 RS 图像的鲁棒表示。大量实验表明，SelectiveMAE 显著提高了训练效率 2.2-2.7 倍，并增强了基线 MIM 模型的分类、检测和分割性能。数据集、源代码和训练好的模型即将发布。]]></description>
      <guid>https://arxiv.org/abs/2406.11933</guid>
      <pubDate>Wed, 19 Jun 2024 06:20:33 GMT</pubDate>
    </item>
    </channel>
</rss>