<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 27 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>AdjointDEIS：扩散模型的有效梯度</title>
      <link>https://arxiv.org/abs/2405.15020</link>
      <description><![CDATA[arXiv:2405.15020v1 公告类型：新 
摘要：根据模型输出定义的一些可微度量来优化扩散模型的潜伏和参数是一个具有挑战性且复杂的问题。扩散模型的采样是通过求解概率流 ODE 或扩散 SDE 来完成的，其中神经网络近似得分函数或相关量，从而允许使用数值 ODE/SDE 求解器。然而，朴素的反向传播技术是内存密集型的，需要存储所有中间状态，并且在处理来自扩散 SDE 的扩散项的注入噪声时面临额外的复杂性。我们提出了一种基于随机伴随灵敏度方法的新方法通过求解附加 SDE 来计算相对于初始噪声、条件信息和模型参数的梯度，该 SDE 的解是扩散 SDE 的梯度。我们利用扩散 SDE 的独特结构来进一步简化伴随扩散 SDE 的公式，并且使用变量变化将解简化为指数加权积分。使用此公式，我们可以导出伴随 SDE 的自定义求解器以及更简单的伴随 ODE。所提出的伴随扩散求解器可以有效地计算两者的梯度。模型潜伏和参数的概率流 ODE 和扩散 SDE 最后，我们证明了伴随扩散求解器在面部变形问题上的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.15020</guid>
      <pubDate>Mon, 27 May 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>生成相机故障作为一类基于物理的对抗示例</title>
      <link>https://arxiv.org/abs/2405.15033</link>
      <description><![CDATA[arXiv:2405.15033v1 公告类型：新 
摘要：虽然最近在生成基于物理的对抗样本方面进行了大量工作，但此类样本中有一类被忽视的样本来自相机的物理故障。相机故障可能是由于外部物理过程造成的，即由于压力导致的组件故障或内部组件故障。在这项工作中，我们开发了一种模拟物理过程，用于生成破碎的镜片作为一类基于物理的对抗样本。我们通过生成约束在网格中的粒子并在随机点和随机角度施加应力来创建基于应力的物理模拟。我们通过网格进行应力传播，网格的最终结果是模拟破碎透镜图案的相应图像。我们还开发了一个神经模拟器，它可以学习作为图形的网格与使用约束传播设置的应力传播之间的非线性映射。然后，我们可以使用不同类别的检测失败率，以及使用 Frechet Inception 距离，统计比较生成的对抗样本与真实、模拟和仿真对抗样本之间的差异。我们通过这项工作的目标是提供一个强大的基于物理的过程来生成对抗性样本。]]></description>
      <guid>https://arxiv.org/abs/2405.15033</guid>
      <pubDate>Mon, 27 May 2024 06:20:59 GMT</pubDate>
    </item>
    <item>
      <title>LOVA3：学习视觉问答、提问和评估</title>
      <link>https://arxiv.org/abs/2405.14974</link>
      <description><![CDATA[arXiv:2405.14974v1 公告类型：新 
摘要：回答、提问和评估是人类与生俱来的三个特征，对于理解世界和获取知识至关重要。通过增强这些能力，人类可以更有效地利用数据，从而获得更好的理解和学习成果。然而，当前的多模态大语言模型（MLLM）主要关注问题回答，往往忽视提问和评估技能的全部潜力。在这项研究中，我们介绍了 LOVA3，这是一个名为“学习视觉问答、提问和评估”的创新框架，旨在为 MLLM 配备这些附加功能。我们的方法涉及创建两个补充训练任务 GenQA 和 EvalQA，旨在培养在图像背景下提出和评估问题的技能。为了培养提问能力，我们编制了一套全面的多模式基础任务。为了进行评估，我们引入了一个名为 EvalQABench 的新基准，其中包含 64,000 个训练样本（正负样本均等）和 5,000 个测试样本。我们认为，增强 MLLM 回答、提问和评估问题的能力将提高他们的多模式理解能力并带来更好的表现。我们通过使用 LOVA3 框架训练 MLLM 并在 10 个多模态基准上进行测试来验证我们的假设。结果证明了性能的持续改进，从而证实了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.14974</guid>
      <pubDate>Mon, 27 May 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>视觉语言模型失去的机会：视觉语言模型在线测试时间适应的比较研究</title>
      <link>https://arxiv.org/abs/2405.14977</link>
      <description><![CDATA[arXiv:2405.14977v1 公告类型：新 
摘要：在深度学习领域，保持模型对分布变化的鲁棒性至关重要。本文研究了视觉语言模型的测试时适应策略，特别关注 CLIP 及其变体。通过对基于提示的技术和现有测试时间适应方法的系统探索，该研究旨在增强视觉语言模型在不同现实场景中的适应性和鲁棒性。该调查包括对提示工程策略的分析，例如手工制作的提示、提示集成和提示学习技术。我们引入了一种视觉-文本-空间集成，与仅文本空间集成相比，它显着提高了平均性能。此外，我们的比较研究深入探讨了利用最初为图像分类任务设计的现有测试时间适应方法。跨各种数据集和模型架构进行的实验评估证明了不同适应策略的有效性。我们进一步深入了解更新视觉编码器的重要性以及更新文本编码器是否有益。代码可在 https://github.com/mariodoebler/test-time-adaptation 获取]]></description>
      <guid>https://arxiv.org/abs/2405.14977</guid>
      <pubDate>Mon, 27 May 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>使用分而治之策略和轻量级卷积神经网络估计手骨年龄</title>
      <link>https://arxiv.org/abs/2405.14986</link>
      <description><![CDATA[arXiv:2405.14986v1 公告类型：新 
摘要： 估算儿童骨龄对于诊断生长缺陷和相关疾病以及估算儿童成熟后的最终身高非常重要。因此，它在不同国家被广泛使用。传统的骨龄估算方法是通过比较左手的寰椎图像和放射线图像来进行，这种方法既耗时又容易出错。为了使用深度神经网络模型估计骨龄，已经进行了大量研究，我们的努力是通过使用所介绍的方法来提高该过程的准确性和速度。创建和分析我们的初始模型后，我们专注于预处理并缩小输入，并提高其质量。我们选择了手部X光片的小区域，并仅根据这些区域来估计骨骼的年龄。通过这样做，我们在不增加所需计算资源的情况下，比相关工作中所实现的更进一步提高了骨龄估计的准确性。在 RSNA 测试集上，我们在 0-20 年范围内达到了 3.90 个月的平均绝对误差 (MAE)，在 1-18 年范围内达到了 3.84 个月的 MAE。]]></description>
      <guid>https://arxiv.org/abs/2405.14986</guid>
      <pubDate>Mon, 27 May 2024 06:20:58 GMT</pubDate>
    </item>
    <item>
      <title>EvGGS：基于事件的可推广高斯泼溅的协作学习框架</title>
      <link>https://arxiv.org/abs/2405.14959</link>
      <description><![CDATA[arXiv:2405.14959v1 公告类型：新 
摘要：事件摄像机具有高动态范围和低延迟等有前景的优势，使其非常适合具有挑战性的照明条件和快速移动的场景。然而，从原始事件流重建 3D 场景很困难，因为事件数据稀疏且不携带绝对颜色信息。为了释放其在 3D 重建中的潜力，我们提出了第一个基于事件的可泛化 3D 重建框架，称为 EvGGS，它以前馈方式仅根据事件输入将场景重建为 3D 高斯，并且可以泛化到未见过的情况而无需任何重新训练。该框架包括深度估计模块、强度重建模块和高斯回归模块。这些子模块以级联的方式连接，我们通过设计的联合损失来协作训练它们，使它们相互促进。为了促进相关研究，我们构建了一个新颖的基于事件的 3D 数据集，其中包含各种材质对象以及灰度图像、深度图、相机姿势和轮廓的校准标签。实验表明，联合训练的模型明显优于单独训练的模型。我们的方法在重建质量和深度/强度预测方面比所有基线表现更好，并且具有令人满意的渲染速度。]]></description>
      <guid>https://arxiv.org/abs/2405.14959</guid>
      <pubDate>Mon, 27 May 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>SFDDM：扩散模型的单折蒸馏</title>
      <link>https://arxiv.org/abs/2405.14961</link>
      <description><![CDATA[arXiv:2405.14961v1 公告类型：新 
摘要：虽然扩散模型有效地生成了出色的合成图像，但一个关键的限制是推理效率低下，需要大量的采样步骤。为了加速推理并保持高质量的合成，师生蒸馏通过再训练以渐进和二元的方式压缩扩散模型，例如将1024步模型减少3倍至128步模型。在本文中，我们提出了一种单重蒸馏算法SFDDM，该算法可以基于教师模型中间输入的重新参数化，灵活地将教师扩散模型压缩为任何所需步骤的学生模型。为了训练学生扩散，我们不仅最小化输出距离，还最小化教师和学生模型之间隐藏变量的分布。对四个数据集的广泛实验表明，我们的学生模型由所提出的 SFDDM 训练而成，能够采样高质量的数据，步长减少到大约 1%，从而权衡推理时间。我们卓越的表现凸显了 SFDDM 在单重蒸馏中有效地传递知识，实现语义一致性和有意义的图像插值。]]></description>
      <guid>https://arxiv.org/abs/2405.14961</guid>
      <pubDate>Mon, 27 May 2024 06:20:57 GMT</pubDate>
    </item>
    <item>
      <title>用于多源数据增强的光谱图像数据融合</title>
      <link>https://arxiv.org/abs/2405.14883</link>
      <description><![CDATA[arXiv:2405.14883v1 公告类型：新 
摘要：多光谱和高光谱图像在遥感、天文成像或精准农业等不同研究领域越来越受欢迎。然而，可用于执行机器学习任务的免费数据量相对较少。此外，在光谱成像领域开发的人工智能模型需要具有固定光谱特征的输入图像，期望数据具有相同数量的光谱带或相同的光谱分辨率。此要求显着减少了可用于给定模型的可用源的数量。本研究的范围是引入一种光谱图像数据融合的方法，以便允许机器学习模型在来自大量来源的数据上进行训练和/或使用，从而提供更好的泛化能力。为此，我们提出了不同的插值技术，以使多源光谱数据相互兼容。插值结果通过各种方法进行评估。这包括使用曲面图和自定义均方误差 (CMSE) 和归一化植被指数 (NDVI) 等指标进行直接评估。此外，间接评估是通过估计它们对机器学习模型训练的影响来完成的，特别是对于语义分割。]]></description>
      <guid>https://arxiv.org/abs/2405.14883</guid>
      <pubDate>Mon, 27 May 2024 06:20:56 GMT</pubDate>
    </item>
    <item>
      <title>DiffuseMix：使用扩散模型进行标签保留数据增强</title>
      <link>https://arxiv.org/abs/2405.14881</link>
      <description><![CDATA[arXiv:2405.14881v1 公告类型：新
摘要：最近，引入了许多基于图像混合的增强技术来提高深度神经网络的泛化能力。在这些技术中，将两个或多个随机选择的自然图像混合在一起以生成增强图像。这些方法不仅可能忽略输入图像的重要部分，而且还可能通过跨标签混合图像而引入标签歧义，从而产生误导性的监督信号。为了解决这些限制，我们提出了 DiffuseMix，这是一种新颖的数据增强技术，它利用扩散模型重塑训练图像，并由我们定制的条件提示进行监督。首先，获得部分自然图像及其生成的对应图像的连接，这有助于避免生成不切实际的图像或标签歧义。然后，为了增强对对抗性攻击的抵御能力并提高安全措施，从一组分形图像中随机选择的结构模式被混合到连接的图像中以形成最终的增强图像以供训练。我们在七个不同数据集上的经验结果表明，DiffuseMix 在一般分类、细粒度分类、微调、数据稀缺性和对抗鲁棒性等任务上的表现优于现有的最先进方法。增强数据集和代码可在此处获取：https://diffusemix.github.io/]]></description>
      <guid>https://arxiv.org/abs/2405.14881</guid>
      <pubDate>Mon, 27 May 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>LookUp3D：数据驱动的 3D 扫描</title>
      <link>https://arxiv.org/abs/2405.14882</link>
      <description><![CDATA[arXiv:2405.14882v1 公告类型：新 
摘要：我们引入了一种新颖的结构光扫描校准和重建程序，放弃了显式点三角测量，转而采用数据驱动的查找程序。关键思想是使用线性平台在整个扫描体积上扫描校准棋盘，并获取密集的图像堆栈，以构建从颜色到深度的每像素查找表。设置缺陷、镜头畸变和传感器缺陷都被纳入校准数据中，从而实现更可靠、更准确的重建。现有的结构光扫描仪无需修改即可重复使用，同时享受我们的校准和重建算法提供的卓越精度和弹性。当与定制设计的模拟投影仪配合使用时，我们的算法表现出色，可实现高达 500 fps 的 1 兆像素高速 3D 扫描。我们描述了我们的高速 3D 扫描算法和硬件原型，并将它们与商业和开源结构光扫描方法进行比较。]]></description>
      <guid>https://arxiv.org/abs/2405.14882</guid>
      <pubDate>Mon, 27 May 2024 06:20:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 YOLO 进行自动珊瑚检测：一种用于高效、准确监测珊瑚礁的深度学习方法</title>
      <link>https://arxiv.org/abs/2405.14879</link>
      <description><![CDATA[arXiv:2405.14879v1 公告类型：新 
摘要：珊瑚礁是重要的生态系统，由于当地人类影响和气候变化而受到日益严重的威胁。有效、准确的珊瑚礁监测对于珊瑚礁的保护和管理至关重要。在本文中，我们提出了一种利用 You Only Look Once (YOLO) 深度学习模型的自动珊瑚检测系统，该系统专为水下图像分析而定制。为了训练和评估我们的系统，我们使用了由 400 张原始水下图像组成的数据集。我们通过使用数据增强技术的图像处理将带注释的图像数量增加到 580 个，这可以通过提供更多样化的训练示例来提高模型的性能。该数据集是从水下视频中精心收集的，捕捉了各种珊瑚礁环境、物种和照明条件。我们的系统利用YOLOv5算法的实时目标检测能力，实现高效、准确的珊瑚检测。我们使用 YOLOv5 从带注释的数据集中提取区分特征，使系统能够泛化，包括以前未见过的水下图像。在我们的原始图像数据集上成功实施使用 YOLOv5 的自动珊瑚检测系统，凸显了先进计算机视觉技术在珊瑚礁研究和保护方面的潜力。进一步的研究将集中于完善算法以处理具有挑战性的水下图像条件，并扩展数据集以纳入更广泛的珊瑚物种和时空变化。]]></description>
      <guid>https://arxiv.org/abs/2405.14879</guid>
      <pubDate>Mon, 27 May 2024 06:20:54 GMT</pubDate>
    </item>
    <item>
      <title>剖析 Vision Transformer 中的查询键交互</title>
      <link>https://arxiv.org/abs/2405.14880</link>
      <description><![CDATA[arXiv:2405.14880v1 公告类型：新 
摘要：视觉变换器中的自注意力被认为可以执行感知分组，其中标记关注具有相似嵌入的其他标记，这些标记可以对应于图像中语义上相似的特征。然而，上下文化也是处理信号的重要且必要的计算。语境化可能需要标记关注不同的标记，例如对应于背景或不同对象的标记，但这种效果在之前的研究中尚未报道。在这项研究中，我们研究了视觉变换器中的自我注意是否表现出对相似标记或不同标记的偏好，分别提供感知分组和情境化的证据。为了研究这个问题，我们建议对查询键矩阵 ${\textbf{W}_q}^T\textbf{W}_k$ 使用奇异值分解。当然，左右奇异向量是自注意力层的特征方向，可以成对分析以解释令牌之间的交互。我们发现早期层更多地关注相似的标记，而后期层则更多地关注不同的标记。此外，由奇异向量表示的特征之间的许多相互作用是可以解释的。我们提出了解释注意力机制的新颖视角，这可能有助于理解 Transformer 模型在处理图像时如何利用上下文和显着特征。]]></description>
      <guid>https://arxiv.org/abs/2405.14880</guid>
      <pubDate>Mon, 27 May 2024 06:20:54 GMT</pubDate>
    </item>
    <item>
      <title>精确而稳健的人行道检测：利用集成学习超越城市环境中法学硕士的限制</title>
      <link>https://arxiv.org/abs/2405.14876</link>
      <description><![CDATA[arXiv:2405.14876v1 公告类型：新
摘要：本研究旨在比较稳健的集成模型与最先进的 ONE-PEACE 大型语言模型 (LLM) 在准确检测人行道方面的有效性。准确的人行道检测对于改善道路安全和城市规划至关重要。该研究评估了该模型在 Cityscapes、Ade20k 和波士顿数据集上的性能。结果表明，集成模型的表现优于单个模型，在理想条件下在这些数据集上的平均交并比 (mIOU) 得分分别为 93.1%、90.3% 和 90.6%。此外，即使在椒盐噪声和斑点噪声等具有挑战性的条件下，集成模型也能保持一致的性能水平，效率仅逐渐下降。另一方面，ONE-PEACE LLM 在理想情况下的表现略好于集成模型，但在嘈杂条件下性能显着下降。这些发现证明了集成模型的稳健性和可靠性，使其成为改善与道路安全和路边空间管理相关的城市基础设施的宝贵资产。这项研究对更广泛的城市健康和出行环境做出了积极贡献。]]></description>
      <guid>https://arxiv.org/abs/2405.14876</guid>
      <pubDate>Mon, 27 May 2024 06:20:53 GMT</pubDate>
    </item>
    <item>
      <title>使用软材料模拟进行视觉变形检测以预训练状态评估模型</title>
      <link>https://arxiv.org/abs/2405.14877</link>
      <description><![CDATA[arXiv:2405.14877v1 公告类型：新 
摘要：本文解决了制造中几何质量保证的挑战，特别是在需要人工评估时。它建议使用开源模拟工具 Blender 为机器学习 (ML) 模型创建合成数据集。该过程涉及将专家信息转换为形状关键参数以模拟变形，生成变形和非变形物体的图像。该研究探讨了真实环境和模拟环境之间的差异对机器学习模型性能的影响，并研究了不同模拟背景对模型灵敏度的影响。此外，该研究旨在通过生成具有各种随机视点的数据集来增强模型对相机定位的鲁棒性。从数据合成到模型训练和测试的整个过程是使用与 Blender 接口的 Python API 实现的。用汽水罐物体进行的实验验证了所提出的管道的准确性。]]></description>
      <guid>https://arxiv.org/abs/2405.14877</guid>
      <pubDate>Mon, 27 May 2024 06:20:53 GMT</pubDate>
    </item>
    <item>
      <title>研究分布变化下开放词汇基础对象检测器的鲁棒性</title>
      <link>https://arxiv.org/abs/2405.14874</link>
      <description><![CDATA[arXiv:2405.14874v1 公告类型：新 
摘要：分布外（OOD）鲁棒性的挑战仍然是部署深度视觉模型的关键障碍。开放词汇对象检测扩展了传统对象检测框架的功能，可以识别和分类预定义类别之外的对象。研究开放词汇对象检测中的 OOD 鲁棒性对于提高这些模型的可信度至关重要。本研究对最近三种开放词汇基础对象检测模型（即 OWL-ViT、YOLO World 和 Grounding DINO）的零样本能力进行了全面的鲁棒性比较。在包含分布变化的 COCO-O 和 COCO-C 基准上进行的实验凸显了模型稳健性的挑战。源代码应在 GitHub 上向研究社区提供。]]></description>
      <guid>https://arxiv.org/abs/2405.14874</guid>
      <pubDate>Mon, 27 May 2024 06:20:52 GMT</pubDate>
    </item>
    </channel>
</rss>