<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 18 Jun 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>超越原始视频：使用大型多模态模型理解编辑后的视频</title>
      <link>https://arxiv.org/abs/2406.10484</link>
      <description><![CDATA[arXiv:2406.10484v1 公告类型：新 
摘要：新兴的视频 LMM（大型多模态模型）以 VQA（视觉问答）的形式在通用视频理解方面取得了显着的进步，其中原始视频由摄像机捕获。然而，现实世界应用中的很大一部分视频都是经过编辑的视频，\textit{例如}，用户通常会在原始视频发布到社交媒体平台上之前对其进行剪切和添加效果/修改。编辑后的视频通常具有很高的观看次数，但现有的视频 LMM、\textit{i.e.}、ActivityNet-QA 或 VideoChatGPT 基准测试并未涵盖它们。在本文中，我们利用流行的短视频平台 \textit{i.e.} TikTok 上的编辑视频，并构建了一个视频 VQA 基准测试（名为 EditVid-QA），涵盖了四个典型的编辑类别，即效果、搞笑、模因和游戏。搞笑视频和 meme 视频是对细微理解和高级推理的基准测试，而效果和游戏则评估人工智能设计的理解能力。大多数开源视频 LMM 在 EditVid-QA 基准测试中表现不佳，这表明社交媒体上编辑过的短视频和普通原始视频之间存在巨大的领域差距。为了提高 LMM 的泛化能力，我们基于 Panda-70M/WebVid 原始视频和小规模 TikTok/CapCut 编辑视频为所提出的基准测试收集了一个训练集，这提高了所提出的 EditVid-QA 基准测试的性能，表明高质量训练数据的有效性。我们还发现了使用 GPT-3.5 Judge 的现有评估协议中的一个严重问题，即“抱歉”攻击，其中抱歉风格的幼稚答案可以从 GPT Judge 那里获得极高的评分，例如，在 VideoChatGPT 评估协议上的正确性得分超过 4.3。为了避免“抱歉”攻击，我们使用 GPT-4 Judge 和关键字过滤来评估结果。该数据集仅供学术目的发布。]]></description>
      <guid>https://arxiv.org/abs/2406.10484</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:46 GMT</pubDate>
    </item>
    <item>
      <title>CoMM：用于多模态理解和生成的连贯交错图像文本数据集</title>
      <link>https://arxiv.org/abs/2406.10462</link>
      <description><![CDATA[arXiv:2406.10462v1 公告类型：新
摘要：交错图像文本生成已成为一项重要的多模态任务，旨在根据查询创建交错的视觉和文本内容序列。尽管最近的多模态大型语言模型 (MLLM) 取得了显着进步，但由于训练数据质量差，生成表现出叙述连贯性和实体和风格一致性的集成图像文本序列仍然具有挑战性。为了解决这一差距，我们推出了 CoMM，这是一个高质量的连贯交错图像文本多模态数据集，旨在增强生成的多模态内容的连贯性、一致性和一致性。最初，CoMM 利用来自不同来源的原始数据，重点关注教学内容和视觉叙事，为连贯一致的内容奠定基础。为了进一步提高数据质量，我们设计了一种多视角过滤策略，利用先进的预训练模型来确保句子的发展、插入图像的一致性以及它们之间的语义对齐。各种质量评估指标旨在证明过滤数据集的高质量。同时，在各种下游任务上进行的大量小样本实验证明了 CoMM 在显著增强 MLLM 的上下文学习能力方面的有效性。此外，我们提出了四项新任务来评估 MLLM 的交错生成能力，并由一个全面的评估框架提供支持。我们相信 CoMM 为具有卓越多模态上下文学习和理解能力的高级 MLLM 开辟了一条新途径。]]></description>
      <guid>https://arxiv.org/abs/2406.10462</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>用于分割和检测的离散潜在视角学习</title>
      <link>https://arxiv.org/abs/2406.10475</link>
      <description><![CDATA[arXiv:2406.10475v1 公告类型：新
摘要：在本文中，我们解决了机器学习和计算机视觉中的视角不变学习挑战，这涉及使网络能够从不同角度理解图像以实现一致的语义解释。虽然标准方法依赖于劳动密集型的多视图图像收集或有限的数据增强技术，但我们提出了一种新颖的框架，即离散潜在视角学习 (DLPL)，用于使用传统单视图图像进行潜在多视角融合学习。DLPL 包括三个主要模块：视角离散分解 (PDD)、视角单应性变换 (PHT) 和视角不变注意 (PIA)，它们共同作用以离散化视觉特征、转换视角和融合多视角语义信息。DLPL 是一个通用的视角学习框架，适用于各种场景和视觉任务。大量实验表明，DLPL 显著增强了网络在不同场景（日常照片、无人机、自动驾驶）和任务（检测、分割）中描绘图像的能力。]]></description>
      <guid>https://arxiv.org/abs/2406.10475</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:45 GMT</pubDate>
    </item>
    <item>
      <title>BabyView 数据集：婴儿和幼儿日常经历的高分辨率自我中心视频</title>
      <link>https://arxiv.org/abs/2406.10447</link>
      <description><![CDATA[arXiv:2406.10447v1 公告类型：新
摘要：人类儿童在样本效率方面远远超过现代机器学习算法，在关键领域以比当前模型少得多的数据实现高性能。这种“数据差距”对于构建智能人工智能系统和理解人类发展都是一个关键挑战。以自我为中心的视频捕捉儿童的体验——他们的“训练数据”——是比较人类和模型以及开发算法创新以弥合这一差距的关键因素。然而，这样的数据集很少，现存数据分辨率低，元数据有限，而且重要的是，只代表了一小部分儿童的经历。在这里，我们提供了迄今为止最大的发展性以自我为中心的视频数据集的首次发布——BabyView 数据集——使用具有大垂直视场和陀螺仪/加速度计数据的高分辨率相机记录。这个长达 493 小时的数据集包括以自我为中心的视频，这些视频来自 6 个月至 5 岁的儿童，这些视频既包括纵向视频，也包括家庭环境和学龄前环境。我们为语音转录、说话人分类和人体姿势估计的评估提供了黄金标准注释，并评估了每个领域的模型。我们训练自监督语言和视觉模型，并评估它们迁移到分布外任务的能力，包括句法结构学习、对象识别、深度估计和图像分割。虽然每个尺度上的性能都与数据集大小有关，但总体性能相对低于在精选数据集上训练模型时的性能，尤其是在视觉领域。我们的数据集对强大的类人 AI 系统提出了一个开放的挑战：这样的系统如何在与人类相同的训练数据规模和分布上取得人类水平的成功？]]></description>
      <guid>https://arxiv.org/abs/2406.10447</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>使用半摊销姿势推断改进从头算低温电子显微镜重建</title>
      <link>https://arxiv.org/abs/2406.10455</link>
      <description><![CDATA[arXiv:2406.10455v1 公告类型：新
摘要：低温电子显微镜 (cryo-EM) 是一种越来越流行的实验技术，用于基于 2D 图像估计大分子复合物（例如蛋白质）的 3D 结构。众所周知，这些图像噪声很大，并且每个图像中结构的姿势都是未知的 \textit{a Priori}。从 2D 图像进行从头算 3D 重建需要估计姿势和结构。在这项工作中，我们提出了一种解决此问题的新方法。我们首先采用多头架构作为姿势编码器，以摊销方式推断每个图像的多个合理姿势。这种方法通过鼓励在重建早期探索姿势空间来减轻姿势估计中的高度不确定性。一旦不确定性降低，我们就会以自动解码的方式改进姿势。特别是，我们使用最可能的姿势进行初始化，并使用随机梯度下降 (SGD) 针对单个图像迭代更新它。通过对合成数据集的评估，我们证明了我们的方法能够在摊销推理阶段处理多模态姿势分布，而后期更灵活的直接姿势优化阶段与基线相比可以更快、更准确地收敛姿势。最后，在实验数据上，我们表明我们的方法比最先进的 cryoAI 更快，并实现了更高分辨率的重建。]]></description>
      <guid>https://arxiv.org/abs/2406.10455</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:44 GMT</pubDate>
    </item>
    <item>
      <title>人类与多模态法学硕士之间的视觉认知差距是什么？</title>
      <link>https://arxiv.org/abs/2406.10424</link>
      <description><![CDATA[arXiv:2406.10424v1 公告类型：新 
摘要：最近，多模态大型语言模型 (MLLM) 在语言引导的感知任务（如识别、分割和物体检测）中显示出巨大的前景。然而，它们在解决需要高级推理的视觉认知问题方面的有效性尚未得到充分证实。其中一个挑战是抽象视觉推理 (AVR)——辨别一组图像中模式之间关系并推断以预测后续模式的认知能力。这种技能在儿童早期神经发育阶段至关重要。受瑞文渐进矩阵 (RPM) 和韦氏儿童智力量表 (WISC) 中的 AVR 任务的启发，我们提出了一个新的数据集 MaRs-VQA 和一个包含三个数据集的新基准 VCog-Bench，以评估 MLLM 的零样本 AVR 能力并将其性能与现有的人类智能调查进行比较。我们在 VCog-Bench 上对不同的开源和闭源 MLLM 进行了比较实验，结果揭示了 MLLM 与人类智能之间的差距，凸显了当前 MLLM 的视觉认知局限性。我们相信，由 MaRs-VQA 和推理管道组成的 VCog-Bench 的公开发布将推动具有类似人类视觉认知能力的下一代 MLLM 的发展。]]></description>
      <guid>https://arxiv.org/abs/2406.10424</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>一致性-多样性-现实主义条件图像生成模型的帕累托前沿</title>
      <link>https://arxiv.org/abs/2406.10429</link>
      <description><![CDATA[arXiv:2406.10429v1 公告类型：新 
摘要：构建准确、全面地表示现实世界的世界模型是条件图像生成模型的最大愿望，因为它将使它们能够用作世界模拟器。为了使这些模型成为成功的世界模型，它们不仅应该在图像质量和即时图像一致性方面表现出色，还应该确保高度的表示多样性。然而，目前对生成模型的研究主要集中在创造性应用上，这些应用主要关注人类对图像质量和美学的偏好。我们注意到生成模型具有推理时间机制 - 或旋钮 - 允许控制生成的一致性、质量和多样性。在本文中，我们使用最先进的文本到图像和图像和文本到图像模型及其旋钮来绘制一致性-多样性-现实主义帕累托前沿，从而提供一致性-多样性-现实主义多目标的整体视图。我们的实验表明，现实主义和一致性都可以同时得到改善；然而，真实性/一致性和多样性之间存在着明显的权衡。通过查看帕累托最优点，我们注意到早期模型在表示多样性方面较好而在一致性/真实性方面较差，而较新的模型在一致性/真实性方面表现出色，同时显著降低了表示多样性。通过计算地理多样性数据集上的帕累托前沿，我们发现潜在扩散模型的第一个版本在所有评估轴上的表现往往都优于较新的模型，并且地理区域之间存在明显的一致性-多样性-真实性差异。总体而言，我们的分析清楚地表明，没有最佳模型，模型的选择应由下游应用决定。通过此分析，我们邀请研究界将帕累托前沿视为衡量世界模型进展的分析工具。]]></description>
      <guid>https://arxiv.org/abs/2406.10429</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:43 GMT</pubDate>
    </item>
    <item>
      <title>从像素到散文：密集图像标题的大型数据集</title>
      <link>https://arxiv.org/abs/2406.10328</link>
      <description><![CDATA[arXiv:2406.10328v1 公告类型：新
摘要：训练大型视觉语言模型需要大量高质量的图像文本对。然而，现有的网络爬取数据集很嘈杂，缺乏详细的图像描述。为了弥补这一差距，我们引入了 PixelProse，这是一个包含超过 16M（百万）合成字幕的综合数据集，利用尖端的视觉语言模型进行详细而准确的描述。为了确保数据完整性，我们严格分析数据集中的问题内容，​​包括儿童性虐待材料 (CSAM)、个人身份信息 (PII) 和毒性。我们还提供有价值的元数据，例如水印存在和美学分数，以帮助进一步过滤数据集。我们希望 PixelProse 将成为未来视觉语言研究的宝贵资源。PixelProse 可在 https://huggingface.co/datasets/tomg-group-umd/pixelprose 上找到]]></description>
      <guid>https://arxiv.org/abs/2406.10328</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>Wild-GS：从不受约束的照片集合中进行实时新颖视图合成</title>
      <link>https://arxiv.org/abs/2406.10373</link>
      <description><![CDATA[arXiv:2406.10373v1 公告类型：新
摘要：在非结构化旅游环境中拍摄的照片经常表现出多变的外观和瞬态遮挡，这对准确的场景重建提出了挑战，并在新颖的视图合成中引入了伪影。尽管之前的方法已经将神经辐射场 (NeRF) 与额外的可学习模块相结合，以处理动态外观并消除瞬态物体，但它们广泛的训练需求和缓慢的渲染速度限制了实际部署。最近，3D 高斯溅射 (3DGS) 已成为 NeRF 的有前途的替代方案，它提供了卓越的训练和推理效率以及更好的渲染质量。本文介绍了 Wild-GS，这是 3DGS 的创新改编，针对不受约束的照片集进行了优化，同时保留了其效率优势。Wild-GS 通过其固有的材料属性、每幅图像的全局照明和相机属性以及点级局部反射率方差来确定每个 3D 高斯的外观。与之前在图像空间中建模参考特征的方法不同，Wild-GS 通过对从参考图像中提取的三平面进行采样，明确地将像素外观特征与相应的局部高斯对齐。这种新颖的设计有效地将参考视图的高频细节外观转移到 3D 空间，并显著加快了训练过程。此外，还利用 2D 可见性图和深度正则化分别减轻瞬态效应和约束几何形状。大量实验表明，Wild-GS 在所有现有技术中实现了最先进的渲染性能和最高的训练和推理效率。]]></description>
      <guid>https://arxiv.org/abs/2406.10373</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:42 GMT</pubDate>
    </item>
    <item>
      <title>L4GM：大型 4D 高斯重建模型</title>
      <link>https://arxiv.org/abs/2406.10324</link>
      <description><![CDATA[arXiv:2406.10324v1 公告类型：新
摘要：我们介绍了 L4GM，这是第一个 4D 大型重建模型，它从单视图视频输入生成动画对象 - 只需一秒钟即可完成一次前馈传递。我们成功的关键是一个新颖的多视图视频数据集，其中包含来自 Objaverse 的精选渲染动画对象。该数据集描绘了 44K 个不同的对象，并在 48 个视点中渲染了 110K 个动画，从而产生了 12M 个视频，总共 3 亿帧。我们保持 L4GM 简单以实现可扩展性，并直接在 LGM 之上构建，LGM 是一个预训练的 3D 大型重建模型，可从多视图图像输入输出 3D 高斯椭球。L4GM 从以低 fps 采样的视频帧输出每帧 3D 高斯溅射表示，然后将表示上采样到更高的 fps 以实现时间平滑度。我们在基础 LGM 中添加了时间自注意力层，以帮助它学习跨时间的一致性，并利用每个时间步的多视图渲染损失来训练模型。通过训练产生中间 3D 高斯表示的插值模型，将表示上采样到更高的帧速率。我们展示了仅在合成数据上训练的 L4GM 在野外视频上具有极好的泛化能力，可生成高质量的动画 3D 资产。]]></description>
      <guid>https://arxiv.org/abs/2406.10324</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>VANE-Bench：对话式 LMM 的视频异常评估基准</title>
      <link>https://arxiv.org/abs/2406.10326</link>
      <description><![CDATA[arXiv:2406.10326v1 公告类型：新
摘要：大型多模态视频模型 (Video-LMM) 的最新发展显著增强了我们解释和分析视频数据的能力。尽管 Video-LMM 具有令人印象深刻的功能，但目前的 Video-LMM 尚未针对异常检测任务进行评估，这对于它们在实际场景中的部署至关重要，例如识别深度伪造、操纵的视频内容、交通事故和犯罪。在本文中，我们介绍了 VANE-Bench，这是一个旨在评估 Video-LMM 在检测和定位视频中的异常和不一致方面的能力的基准。我们的数据集包括使用现有的最先进的文本到视频生成模型合成生成的一系列视频，涵盖了各种细微的异常和不一致，分为五类：不自然的变换、不自然的外观、穿透、消失和突然出现。此外，我们的基准测试采用来自现有异常检测数据集的真实样本，重点关注与犯罪相关的违规行为、非典型行人行为和异常事件。该任务被构建为视觉问答挑战，以衡量模型准确检测和定位视频中异常的能力。我们在这个基准测试任务上评估了九个现有的 Video-LMM（包括开放源和封闭源），发现大多数模型在有效识别细微异常方面遇到困难。总之，我们的研究为 Video-LMM 在异常检测领域的当前能力提供了重要的见解，突出了我们的工作在评估和改进这些模型以用于实际应用方面的重要性。我们的代码和数据可在 https://hananshafi.github.io/vane-benchmark/ 获得]]></description>
      <guid>https://arxiv.org/abs/2406.10326</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:41 GMT</pubDate>
    </item>
    <item>
      <title>创造中国文化的视角：用于理解中国双关语字谜艺术的多模态数据集</title>
      <link>https://arxiv.org/abs/2406.10318</link>
      <description><![CDATA[arXiv:2406.10318v1 公告类型：新
摘要：大型视觉语言模型 (VLM) 在理解日常内容方面表现出了非凡的能力。然而，它们在艺术领域的表现，尤其是文化丰富的艺术形式，仍然没有得到充分探索。作为人类智慧和创造力的瑰宝，艺术蕴含着复杂的文化叙事和象征意义。在本文中，我们提供了双关语艺术数据集，这是一个根植于中国传统文化的多模态艺术理解数据集。我们专注于三个主要任务：识别显着的视觉元素、将元素与其象征意义相匹配以及对所传达信息的解释。我们的评估表明，最先进的 VLM 难以完成这些任务，通常提供有偏见和幻觉的解释，并且通过情境学习显示出有限的改进。通过发布双关语艺术数据集，我们旨在促进能够更好地理解和解释特定文化内容的 VLM 的开发，促进超越英语语料库的更大包容性。]]></description>
      <guid>https://arxiv.org/abs/2406.10318</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>LieRE：推广旋转位置编码</title>
      <link>https://arxiv.org/abs/2406.10322</link>
      <description><![CDATA[arXiv:2406.10322v1 公告类型：新
摘要：虽然自然语言的旋转位置嵌入 (RoPE) 表现良好并且已被广泛采用，但它在其他模态中的采用速度较慢。在这里，我们引入了李群相对位置编码 (LieRE)，它在支持更高维输入方面超越了 RoPE。我们评估了 LieRE 在 2D 和 3D 图像分类任务上的表现，并观察到与 RoFormer、DeiT III、RoPE-Mixed 和 Vision-Llama 的基线相比，LieRE 在性能（高达 6%）、训练效率（降低 3.5 倍）、数据效率（30%）方面有显著的提高]]></description>
      <guid>https://arxiv.org/abs/2406.10322</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:40 GMT</pubDate>
    </item>
    <item>
      <title>物体关键性可提高导航安全性</title>
      <link>https://arxiv.org/abs/2406.10232</link>
      <description><![CDATA[arXiv:2406.10232v1 公告类型：新
摘要：自动驾驶中的物体检测包括在多维数据（例如图像或激光雷达扫描）中感知和定位物体实例。最近，多项研究提出通过测量物体检测器检测最有可能干扰驾驶任务的物体的能力来评估物体检测器。然后根据检测器检测最相关物体的能力而不是检测物体数量最多的物体的能力对检测器进行排名。然而，到目前为止，几乎没有证据表明预测物体的相关性可能有助于提高驾驶任务的安全性和可靠性。本立场文件阐述了一种策略以及部分结果，即 i) 配置和部署成功提取物体相关性知识的物体检测器，以及 ii) 使用此类知识来改进轨迹规划任务。我们表明，给定一个物体检测器，根据相关性对物体进行过滤，结合传统的置信度阈值，可以降低丢失相关物体的风险，降低危险轨迹的可能性，并总体上提高轨迹的质量。]]></description>
      <guid>https://arxiv.org/abs/2406.10232</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:39 GMT</pubDate>
    </item>
    <item>
      <title>基于 YOLOv5 算法的泰卢固语手语识别</title>
      <link>https://arxiv.org/abs/2406.10231</link>
      <description><![CDATA[arXiv:2406.10231v1 公告类型：新
摘要：手语识别 (SLR) 技术在改善听力障碍者的沟通和可访问性方面具有巨大的前景。本文介绍了一种使用 YOLOv5 对象识别框架识别 TSL 手势的新方法。主要目标是创建一种准确且成功的识别 TSL 手势的方法，以便聋人社区可以使用 slr。之后，创建了一个深度学习模型，该模型使用 YOLOv5 识别和分类手势。该模型受益于 YOLOv5 架构的高精度、速度和处理复杂手语特征的能力。利用迁移学习方法，YOLOv5 模型针对 TSL 手势进行了定制。为了获得最佳结果，在训练期间进行了仔细的参数和超参数调整。 YOLOv5-medium 模型的 F1 得分和平均准确率 (mAP) 分别为 90.5% 和 98.1%，以其出色的性能指标脱颖而出，证明了其在泰卢固语手语识别任务中的有效性。令人惊讶的是，该模型在计算复杂度和训练时间之间取得了可接受的平衡，从而产生了这些惊人的结果。由于它提供了令人信服的准确性和效率结合，经过 200 次训练的 YOLOv5-medium 模型成为实际部署的推荐选择。通过严格的测试和验证，评估了系统在各种 TSL 手势和设置中的稳定性和通用性，结果获得了出色的准确性。这项研究通过为 TSL 手势识别提供深度学习和计算机视觉技术的前沿应用，为语言社区未来无障碍技术的进步奠定了基础。它还为手语识别领域提供了深刻的见解和新颖的方法。]]></description>
      <guid>https://arxiv.org/abs/2406.10231</guid>
      <pubDate>Wed, 19 Jun 2024 03:16:38 GMT</pubDate>
    </item>
    </channel>
</rss>