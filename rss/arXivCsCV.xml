<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Wed, 27 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>AnimateMe：通过扩散模型的 4D 面部表情</title>
      <link>https://arxiv.org/abs/2403.17213</link>
      <description><![CDATA[arXiv:2403.17213v1 公告类型：新
摘要：近年来，真实感 3D 头像重建和生成领域受到了广泛关注。然而，制作这样的化身动画仍然具有挑战性。扩散模型的最新进展显着增强了 2D 动画生成模型的能力。在这项工作中，我们直接在 3D 域内利用这些模型来实现可控且高保真的 4D 面部动画。通过整合扩散过程和几何深度学习的优势，我们以一种新颖的方法采用图神经网络 (GNN) 作为去噪扩散模型，直接在网格空间上制定扩散过程并能够生成 3D 面部表情。这有助于通过基于网格扩散的模型生成面部变形。此外，为了确保动画的时间一致性，我们提出了一致的噪声采样方法。通过一系列定量和定性实验，我们证明所提出的方法通过生成高保真极端表达式，优于 4D 表达式合成中的先前工作。此外，我们将我们的方法应用于纹理 4D 面部表情生成，实现了一个简单的扩展，其中涉及对大规模纹理 4D 面部表情数据库进行训练。]]></description>
      <guid>https://arxiv.org/abs/2403.17213</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:20 GMT</pubDate>
    </item>
    <item>
      <title>DiffusionAct：用于一次性面部重演的可控扩散自动编码器</title>
      <link>https://arxiv.org/abs/2403.17217</link>
      <description><![CDATA[arXiv:2403.17217v1 公告类型：新
摘要：视频驱动的神经人脸重演旨在合成逼真的面部图像，成功保留源面部的身份和外观，同时传输目标头部姿势和面部表情。现有的基于 GAN 的方法存在失真和视觉伪影或重建质量差的问题，即背景和一些重要的外观细节（例如发型/颜色、眼镜和配饰）没有忠实地重建。扩散概率模型 (DPM) 的最新进展使得能够生成高质量的逼真图像。为此，在本文中，我们提出了 DiffusionAct，这是一种利用扩散模型的逼真图像生成来执行神经面部重演的新颖方法。具体来说，我们建议控制扩散自动编码器（DiffAE）的语义空间，以便编辑输入图像的面部姿势，定义为头部姿势方向和面部表情。我们的方法允许一次性、自我和跨主题重演，而不需要针对特定​​主题进行微调。我们与最先进的 GAN、StyleGAN2 和基于扩散的方法进行比较，显示出更好或同等的重演性能。]]></description>
      <guid>https://arxiv.org/abs/2403.17217</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:20 GMT</pubDate>
    </item>
    <item>
      <title>神经工程特征的直方图层</title>
      <link>https://arxiv.org/abs/2403.17176</link>
      <description><![CDATA[arXiv:2403.17176v1 公告类型：新
摘要：在计算机视觉文献中，已经开发了许多有效的基于直方图的特征。这些工程特征包括局部二进制模式和边缘直方图描述符等，它们已被证明是各种计算机视觉任务的信息特征。在本文中，我们探讨了是否可以通过嵌入神经网络中的直方图层来学习这些特征，从而在深度学习框架中利用这些特征。通过使用直方图特征，可以使用卷积神经网络的特征图的局部统计来更好地表示数据。我们提出了局部二值模式和边缘直方图描述符的神经版本，它们共同改进了特征表示并执行图像分类。在基准数据集和真实数据集上进行了实验。]]></description>
      <guid>https://arxiv.org/abs/2403.17176</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:19 GMT</pubDate>
    </item>
    <item>
      <title>LOTUS：通过子分区进行规避和弹性后门攻击</title>
      <link>https://arxiv.org/abs/2403.17188</link>
      <description><![CDATA[arXiv:2403.17188v1 公告类型：新
摘要：后门攻击对深度学习应用程序构成重大安全威胁。现有的攻击通常无法躲避已建立的后门检测技术。这种敏感性主要源于这样一个事实：这些攻击通常利用通用触发模式或转换函数，使得触发器可能导致任何输入的错误分类。针对这一点，最近的论文引入了使用通过特殊转换函数制作的特定于样本的隐形触发器的攻击。虽然这些方法在某种程度上设法逃避检测，但它们暴露了现有后门缓解技术的漏洞。为了解决并增强规避性和弹性，我们引入了一种新颖的后门攻击 LOTUS。具体来说，它利用秘密函数将受害者类中的样本分成一组分区，并将唯一的触发器应用于不同的分区。此外，LOTUS还采用了有效的触发聚焦机制，确保只有分区对应的触发才能诱发后门行为。大量的实验结果表明，LOTUS可以在4个数据集和7种模型结构上实现较高的攻击成功率，并有效规避13种后门检测和缓解技术。该代码可从 https://github.com/Megum1/LOTUS 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.17188</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:19 GMT</pubDate>
    </item>
    <item>
      <title>提高腹腔镜解剖分割模型现实世界适用性的策略</title>
      <link>https://arxiv.org/abs/2403.17192</link>
      <description><![CDATA[arXiv:2403.17192v1 公告类型：新
摘要：在腹腔镜成像中准确识别和定位不同大小和外观的解剖结构对于利用计算机视觉技术的潜力进行手术决策支持是必要的。传统上，此类模型的分割性能是使用 IoU 等重叠指标来报告的。然而，训练数据中类别的不平衡和不切实际的表示以及报告指标的次优选择可能会扭曲名义分割性能，从而最终限制临床转化。在这项工作中，我们系统地分析了类特征（即器官大小差异）、训练和测试数据组成（即正例和负例的表示）和建模参数（即前景到背景类权重）的影响八个分割指标：准确度、精确度、召回率、IoU、F1 分数、特异性、豪斯多夫距离和平均对称表面距离。根据我们的发现，我们提出了两种简单而有效的策略来提高腹腔镜手术数据中图像分割模型的实际适用性：（1）在训练过程中包含负例；（2）在分割中调整前景-背景权重根据临床用例，根据感兴趣的特定指标最大化模型性能。]]></description>
      <guid>https://arxiv.org/abs/2403.17192</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:19 GMT</pubDate>
    </item>
    <item>
      <title>视频帧插值基准测试</title>
      <link>https://arxiv.org/abs/2403.17128</link>
      <description><![CDATA[arXiv:2403.17128v1 公告类型：新
摘要：视频帧插值，即在两个或多个给定帧之间合成新帧的任务，正在成为越来越受欢迎的研究目标。然而，目前对帧插值技术的评价并不理想。由于可用的测试数据集过多，并且错误指标的计算不一致，因此在论文之间进行连贯且公平的比较非常具有挑战性。此外，新的测试集已作为方法论文的一部分提出，因此它们无法提供专用基准测试论文的深入评估。另一个严重的缺点是，当给定两个输入帧时，这些测试集违反了线性假设，使得没有预言机就无法解决。因此，我们坚信社区将从我们提议的基准测试文件中受益匪浅。具体来说，我们提出了一个基准，该基准通过利用计算错误度量的提交网站来建立一致的错误度量，通过分析相对于各种每像素属性（例如运动幅度）的插值质量来提供见解，包含一个精心设计的测试集，遵循利用合成数据假设线性，并以一致的方式评估计算效率。]]></description>
      <guid>https://arxiv.org/abs/2403.17128</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:18 GMT</pubDate>
    </item>
    <item>
      <title>Task2Box：用于建模不对称任务关系的框嵌入</title>
      <link>https://arxiv.org/abs/2403.17173</link>
      <description><![CDATA[arXiv:2403.17173v1 公告类型：新
摘要：对任务或数据集之间的关系进行建模和可视化是解决数据集发现、多任务和迁移学习等各种元任务的重要一步。然而，许多关系，例如包含性和可转移性，本质上是不对称的，并且当前的表示和可视化方法（例如，t-SNE 不容易支持这一点）。我们提出了 Task2Box，一种使用框嵌入表示任务的方法 - 轴对齐低维空间中的超矩形——可以通过体积重叠捕获它们之间的不对称关系。我们表明，Task2Box 可以准确预测 ImageNet 和 iNaturalist 数据集中节点之间不可见的层次关系，以及 Taskonomy 基准中任务之间的可转移性。我们还表明从任务表示（例如 CLIP、Task2Vec 或基于属性）估计的框嵌入可以比在相同表示上训练的分类器以及手工设计的不对称距离（例如 KL 散度）更准确地预测未见过的任务之间的关系。表明低维框嵌入可以有效地捕获这些任务关系，并具有可解释的额外优势。我们使用该方法来可视化名为 Hugging Face 的流行数据集托管平台上公开可用的图像分类数据集之间的关系。]]></description>
      <guid>https://arxiv.org/abs/2403.17173</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:18 GMT</pubDate>
    </item>
    <item>
      <title>基于面部标志和时空图卷积网络的参与度测量</title>
      <link>https://arxiv.org/abs/2403.17175</link>
      <description><![CDATA[arXiv:2403.17175v1 公告类型：新
摘要：虚拟学习的参与对于多种因素至关重要，包括学习者满意度、表现和学习计划的遵守情况，但衡量它是一项具有挑战性的任务。因此，人们对利用人工智能和情感计算来大规模地测量自然环境中的参与度产生了很大的兴趣。本文介绍了一种新颖的、隐私保护的视频参与度测量方法。它使用通过 MediaPipe 深度学习解决方案从视频中提取的面部标志，不携带个人身份信息。提取的面部标志被输入时空图卷积网络（ST-GCN），以输出学习者在视频中的参与程度。为了将参与变量的序数性质整合到训练过程中，ST-GCN 在基于迁移学习的新颖序数学习框架中接受训练。两个视频学生参与度测量数据集的实验结果表明，与之前的方法相比，所提出的方法具有优越性，在 EngageNet 数据集上改进了最先进的方法，四级参与度分类精度提高了 3.1%，在线学生参与度数据集的二元参与度分类准确性提高了 1.5%。相对轻量级的 ST-GCN 及其与实时 MediaPipe 深度学习解决方案的集成使得所提出的方法能够部署在虚拟学习平台上并实时测量参与度。]]></description>
      <guid>https://arxiv.org/abs/2403.17175</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:18 GMT</pubDate>
    </item>
    <item>
      <title>SynFog：基于端到端成像仿真的逼真合成雾数据集，用于推进自动驾驶中的真实世界去雾</title>
      <link>https://arxiv.org/abs/2403.17094</link>
      <description><![CDATA[arXiv:2403.17094v1 公告类型：新
摘要：为了推进基于学习的去雾算法的研究，已经开发了各种合成雾数据集。然而，使用大气散射模型 (ASM) 或实时渲染引擎创建的现有数据集通常难以生成准确模拟实际成像过程的逼真雾图像。这种限制阻碍了模型从合成数据到真实数据的有效泛化。在本文中，我们介绍了一种端到端模拟管道，旨在生成逼真的雾图像。该流程全面考虑了整个基于物理的雾天场景成像过程，与现实世界的图像捕获方法紧密结合。基于这个管道，我们提出了一个名为 SynFog 的新合成雾数据集，它具有天空光和主动照明条件，以及三个级别的雾密度。实验结果表明，在应用于现实世界的有雾图像时，与其他模型相比，在 SynFog 上训练的模型在视觉感知和检测精度方面表现出优越的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.17094</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:17 GMT</pubDate>
    </item>
    <item>
      <title>动物头像：从休闲视频中重建可动画的 3D 动物</title>
      <link>https://arxiv.org/abs/2403.17103</link>
      <description><![CDATA[arXiv:2403.17103v1 公告类型：新
摘要：我们提出了一种从单眼视频构建可动画狗头像的方法。这是具有挑战性的，因为动物表现出一系列（不可预测的）非刚性运动并且具有各种外观细节（例如皮毛、斑点、尾巴）。我们开发了一种通过 4D 解决方案链接视频帧的方法，共同解决动物的姿势变化及其外观（规范姿势）。为此，我们通过赋予 SMAL 参数模型连续表面嵌入来显着提高基于模板的形状拟合的质量，这带来了比之前使用的稀疏语义关键点更密集、更强的图像到网格重投影约束信件往来。为了对外观进行建模，我们提出了一种在规范姿势中定义的隐式双网格纹理，但可以使用 SMAL 姿势系数进行变形，然后进行渲染以强制与输入视频帧的光度兼容性。在具有挑战性的 CoP3D 和 APTv2 数据集上，我们展示了优于现有无模板 (RAC) 和基于模板的方法（BARC、BITE）的结果（在姿势估计和预测外观方面）。]]></description>
      <guid>https://arxiv.org/abs/2403.17103</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:17 GMT</pubDate>
    </item>
    <item>
      <title>HEAL-ViT：球形网格上的视觉变压器，用于中期天气预报</title>
      <link>https://arxiv.org/abs/2403.17016</link>
      <description><![CDATA[arXiv:2403.17016v1 公告类型：新
摘要：近年来，各种机器学习架构和技术在生成熟练的中期天气预报方面取得了成功。特别是，基于 Vision Transformer (ViT) 的模型（例如 Pangu-Weather、FuXi）表现出了强大的性能，通过将天气数据视为直线网格上的多通道图像，几乎“开箱即用”。虽然直线网格适用于 2D 图像，但天气数据本质上是球形的，因此在直线网格的极点处严重扭曲，导致使用不成比例的计算来对极点附近的数据进行建模。基于图的方法（例如 GraphCast）不会遇到这个问题，因为它们将经纬度网格映射到球形网格，但通常更占用内存，并且往往需要更多计算资源来进行训练和推理。虽然空间均匀，但球形网格并不容易通过基于 ViT 的模型进行建模，这些模型隐式依赖于直线网格结构。我们提出了 HEAL-ViT，这是一种在球形网格上使用 ViT 模型的新颖架构，从而受益于基于图的模型所享有的空间同质性和 Transformer 利用的高效的基于注意力的机制。 HEAL-ViT 生成的天气预报在关键指标上优于 ECMWF IFS，并且比其他 ML 天气预报模型表现出更好的偏差累积和模糊效果。此外，HEAL-ViT 较低的计算占用量使其对运营用途也很有吸引力，除了 6 小时预测模型之外，可能还需要其他模型来生成所需的全套运营预测。]]></description>
      <guid>https://arxiv.org/abs/2403.17016</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:16 GMT</pubDate>
    </item>
    <item>
      <title>通过细心的特征正则化促进少样本学习</title>
      <link>https://arxiv.org/abs/2403.17025</link>
      <description><![CDATA[arXiv:2403.17025v1 公告类型：新
摘要：基于流形正则化的少样本学习（FSL）旨在通过使用混合因子混合来自不同类别的两个样本来提高有限训练样本的新物体的识别能力。然而，由于线性插值和忽略特定通道的重要性，这种混合操作削弱了特征表示。为了解决这些问题，本文提出了注意力特征正则化（AFR），旨在提高特征的代表性和可辨别性。在我们的方法中，我们首先计算不同类别的语义标签之间的关系，以挑选出用于正则化的相关特征。然后，我们在实例和通道级别设计了两种基于注意力的计算。这些计算使正则化过程能够专注于两个关键方面：通过相关类别中的自适应插值实现特征互补以及对特定特征通道的强调。最后，我们结合这些正则化策略来显着提高分类器的性能。对几个流行的 FSL 基准的实证研究证明了 AFR 的有效性，它提高了新类别的识别准确性，而无需重新训练任何特征提取器，特别是在 1-shot 设置中。此外，所提出的 AFR 可以无缝集成到其他 FSL 方法中，以提高分类性能。]]></description>
      <guid>https://arxiv.org/abs/2403.17025</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:16 GMT</pubDate>
    </item>
    <item>
      <title>通过识别语义方向在 T2I 模型中进行连续的、特定于主题的属性控制</title>
      <link>https://arxiv.org/abs/2403.17064</link>
      <description><![CDATA[arXiv:2403.17064v1 公告类型：新
摘要：近年来，文本到图像（T2I）扩散模型的进步大大提高了生成图像的质量。然而，由于自然语言提示的局限性（例如“人”和“老人”之间不存在连续的中间描述集），实现对属性的细粒度控制仍然是一个挑战。尽管引入了许多方法来增强模型或生成过程以实现这种控制，但不需要固定参考图像的方法仅限于启用全局细粒度属性表达控制或局部于特定对象的粗粒度属性表达控制，而不是两者同时进行。我们表明，常用的标记级 CLIP 文本嵌入存在方向，可以对文本到图像模型中的高级属性进行细粒度的特定于主题的控制。基于这一观察，我们引入了一种有效的免优化和一种基于鲁棒优化的方法来从对比文本提示中识别特定属性的这些方向。我们证明，这些方向可用于以组合方式（控制单个主题的多个属性）对特定主题的属性进行细粒度控制来增强提示文本输入，而无需适应扩散模型。项目页面：https://compvis.github.io/attribute-control。代码可在 https://github.com/CompVis/attribute-control 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.17064</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:16 GMT</pubDate>
    </item>
    <item>
      <title>通过延迟环储层神经网络对事件摄像机数据进行时空处理</title>
      <link>https://arxiv.org/abs/2403.17013</link>
      <description><![CDATA[arXiv:2403.17013v1 公告类型：新
摘要：本文描述了一种用于视频处理的时空模型，特别适用于处理事件摄像机视频。我们建议研究一个由我们之前的延迟循环存储（DLR）神经网络视频处理研究激发的猜想，我们称之为时空猜想（TSC）。 TSC 假设视频信号的时间表示中携带有重要的信息内容，并且机器学习算法将受益于智能处理的空间和时间分量的单独优化。为了验证或反驳 TSC，我们提出了一种视觉马尔可夫模型（VMM），它将视频分解为空间和时间分量，并估计这些分量的互信息（MI）。由于视频互信息的计算复杂且耗时，因此我们使用互信息神经网络来估计互信息的界限。我们的结果表明，与空间分量相比，时间分量具有显着的 MI。这一发现在神经网络文献中经常被忽视。在本文中，我们将利用这一新发现来指导我们设计用于事件摄像机分类的延迟循环储层神经网络，从而使分类精度提高 18%。]]></description>
      <guid>https://arxiv.org/abs/2403.17013</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:15 GMT</pubDate>
    </item>
    <item>
      <title>高光谱数据回归的对比学习</title>
      <link>https://arxiv.org/abs/2403.17014</link>
      <description><![CDATA[arXiv:2403.17014v1 公告类型：新
摘要：对比学习在表示学习尤其是图像分类任务中表现出了巨大的有效性。然而，针对回归任务的研究仍然不足，更具体地说，针对高光谱数据的应用的研究仍然不足。在本文中，我们提出了一个用于高光谱数据回归任务的对比学习框架。为此，我们提供了一系列与增强高光谱数据相关的转换，并研究了回归的对比学习。对合成和真实高光谱数据集的实验表明，所提出的框架和转换显着提高了回归模型的性能，比其他最先进的转换获得了更好的分数。]]></description>
      <guid>https://arxiv.org/abs/2403.17014</guid>
      <pubDate>Thu, 28 Mar 2024 00:57:15 GMT</pubDate>
    </item>
    </channel>
</rss>