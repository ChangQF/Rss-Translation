<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 06 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>FER-YOLO-Mamba：基于选择性状态空间的面部表情检测与分类</title>
      <link>https://arxiv.org/abs/2405.01828</link>
      <description><![CDATA[arXiv:2405.01828v1 公告类型：新
摘要：面部表情识别（FER）在理解人类情绪线索方面发挥着关键作用。然而，基于视觉信息的传统FER方法存在一些局限性，例如预处理、特征提取和多阶段分类程序。这些不仅增加了计算复杂性，而且需要大量的计算资源。考虑到基于卷积神经网络 (CNN) 的 FER 方案经常被证明不足以识别嵌入在面部表情图像中的深层、长距离依赖性，以及 Transformer 固有的二次计算复杂性，本文提出了 FER-YOLO-Mamba 模型，该模型集成了Mamba 和 YOLO 技术的原理，促进面部表情图像识别和定位的高效协调。在 FER-YOLO-Mamba 模型中，我们进一步设计了 FER-YOLO-VSS 双分支模块，它将卷积层在局部特征提取方面的固有优势与状态空间模型（SSM）在揭示长期特征方面的卓越能力结合起来。距离依赖性。据我们所知，这是第一个专为面部表情检测和分类而设计的 Vision Mamba 模型。为了评估所提出的 FER-YOLO-Mamba 模型的性能，我们在两个基准数据集 RAF-DB 和 SFEW 上进行了实验。实验结果表明，FER-YOLO-Mamba模型相比其他模型取得了更好的结果。该代码可从 https://github.com/SwjtuMa/FER-YOLO-Mamba 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.01828</guid>
      <pubDate>Mon, 06 May 2024 06:19:14 GMT</pubDate>
    </item>
    <item>
      <title>使用量子迁移学习检测糖尿病视网膜病变</title>
      <link>https://arxiv.org/abs/2405.01734</link>
      <description><![CDATA[arXiv:2405.01734v1 公告类型：新
摘要：糖尿病视网膜病变（DR）是糖尿病患者常见的并发症，由于视网膜上形成病变，可导致视力损害。在晚期发现 DR 通常会导致不可逆转的失明。眼科医生通过视网膜眼底图像诊断 DR 的传统过程不仅耗时而且昂贵。虽然经典的迁移学习模型已广泛用于计算机辅助 DR 检测，但其高昂的维护成本会阻碍其检测效率。相比之下，量子迁移学习为这一挑战提供了更有效的解决方案。这种方法非常有利，因为它按照启发式原理运行，使其针对任务进行了高度优化。我们提出的方法利用这种混合量子迁移学习技术来检测 DR。为了构建我们的模型，我们利用 Kaggle 上提供的 APTOS 2019 失明检测数据集。我们采用 ResNet-18、ResNet34、ResNet50、ResNet101、ResNet152 和 Inception V3（预训练的经典神经网络）进行初始特征提取。对于分类阶段，我们使用变分量子分类器。我们的混合量子模型显示出显着的结果，ResNet-18 的准确率达到 97%。这表明，当量子计算与量子机器学习集成时，可以以传统计算机单独无法达到的功率和效率水平执行任务。通过利用这些先进技术，我们可以显着改善糖尿病视网膜病变的检测和诊断，从而有可能挽救许多人失明的风险。
  关键词：糖尿病视网膜病变，量子迁移学习，深度学习]]></description>
      <guid>https://arxiv.org/abs/2405.01734</guid>
      <pubDate>Mon, 06 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>改善视觉语言概念瓶颈模型中的概念对齐</title>
      <link>https://arxiv.org/abs/2405.01825</link>
      <description><![CDATA[arXiv:2405.01825v1 公告类型：新
摘要：概念瓶颈模型（CBM）将输入图像映射到人类可理解的高级概念空间，然后根据这些概念进行类别预测。最近的方法通过提示大型语言模型 (LLM) 生成文本概念，然后使用视觉语言模型 (VLM) 获取概念分数来训练 CBM，从而自动化 CBM 的构建。然而，我们希望使用人类专家定义的概念而不是 LLM 生成的概念来构建 CBM，以使它们更值得信赖。在这项工作中，我们对细粒度鸟类分类和动物分类等领域的专家定义概念的 VLM 概念评分的忠实度进行了更仔细的检查。我们的调查表明，冻结的 VLM（例如 CLIP）尽管实现了较高的分类性能，但仍难以将概念与相应的视觉输入正确关联。为了解决这个问题，我们提出了一种新颖的对比半监督（CSS）学习方法，该方法使用一些标记的概念示例来改进 CLIP 模型中的概念对齐（激活真实的视觉概念）。对三个基准数据集的广泛实验表明，我们的方法大大提高了概念准确性和分类准确性，但只需要一小部分人工注释的概念标签。为了进一步提高分类性能，我们还针对细粒度分类问题引入了一种新的类级干预程序，该程序可以识别混淆类并干预其概念空间以减少错误。]]></description>
      <guid>https://arxiv.org/abs/2405.01825</guid>
      <pubDate>Mon, 06 May 2024 06:19:13 GMT</pubDate>
    </item>
    <item>
      <title>通过特征空间增强和迭代学习生成长尾图像</title>
      <link>https://arxiv.org/abs/2405.01705</link>
      <description><![CDATA[arXiv:2405.01705v1 公告类型：新
摘要：在数据分布不良的情况下，图像和多模态机器学习任务的解决非常具有挑战性。特别是，数据可用性和隐私限制加剧了医疗领域的这些障碍。潜在扩散模型拥有最先进的图像生成质量，使其成为解决该问题的主要候选者。然而，仍然有一些关键问题需要解决，例如从代表性不足的类生成数据的困难以及推理过程缓慢。为了缓解这些问题，我们提出了一种基于利用预训练稳定扩散模型的丰富潜在空间的长尾数据图像增强新方法。我们创建一个修改后的可分离潜在空间来混合头类和尾类示例。我们通过底层稀疏嵌入的迭代学习来构建这个空间，并通过 K-NN 方法将其应用于特定于任务的显着性图。代码可在 https://github.com/SugarFreeManatee/Feature-Space-Augmentation-and-Iterated-Learning 获取]]></description>
      <guid>https://arxiv.org/abs/2405.01705</guid>
      <pubDate>Mon, 06 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>通过将深度学习与几何运动模型融合相结合，实现野外零样本单目运动分割</title>
      <link>https://arxiv.org/abs/2405.01723</link>
      <description><![CDATA[arXiv:2405.01723v1 公告类型：新
摘要：在存在未知的相机运动、不同的物体运动和复杂的场景结构的情况下，从移动单目相机检测和分割移动物体具有挑战性。大多数现有方法依赖于单个运动线索来执行运动分割，这在面对不同的复杂环境时通常是不够的。虽然最近一些基于深度学习的方法能够结合多个运动线索来提高准确性，但它们严重依赖于庞大的数据集和广泛的注释，这使得它们不太适应新的场景。为了解决这些限制，我们提出了一种新颖的单目密集分割方法，该方法以零镜头方式实现最先进的运动分割结果。该方法通过对目标提案进行几何模型融合，协同结合了深度学习和几何模型融合方法的优势。实验表明，我们的方法在多个运动分割数据集上取得了有竞争力的结果，甚至在某些基准上超越了一些最先进的监督方法，而无需在任何数据上进行训练。我们还提出了一项消融研究，以展示将不同几何模型组合在一起进行运动分割的有效性，突出我们的几何模型融合策略的价值。]]></description>
      <guid>https://arxiv.org/abs/2405.01723</guid>
      <pubDate>Mon, 06 May 2024 06:19:12 GMT</pubDate>
    </item>
    <item>
      <title>SOAR：使用状态空间模型和可编程梯度进行航空图像小体物体检测的进展</title>
      <link>https://arxiv.org/abs/2405.01699</link>
      <description><![CDATA[arXiv:2405.01699v1 公告类型：新
摘要：由于小尺寸物体固有的数据最少且容易被较大物体和背景噪声遮挡，航空图像中的小物体检测对计算机视觉提出了重大挑战。使用基于变压器的模型的传统方法通常面临由于缺乏专门数据库而产生的限制，这对其在不同方向和尺度的对象上的性能产生不利影响。这强调了对适应性更强、轻量级模型的需求。为此，本文介绍了两种创新方法，可显着增强小型空中物体的检测和分割能力。首先，我们探索在新引入的轻量级 YOLO v9 架构上使用 SAHI 框架，该架构利用可编程梯度信息（PGI）来减少顺序特征提取过程中通常遇到的大量信息丢失。该论文采用 Vision Mamba 模型，该模型结合了位置嵌入以促进精确的位置感知视觉理解，并结合新颖的双向状态空间模型（SSM）来进行有效的视觉上下文建模。该状态空间模型巧妙地利用了 CNN 的线性复杂性和 Transformer 的全局感受野，使其在遥感图像分类中特别有效。我们的实验结果证明了检测精度和处理效率的显着提高，验证了这些方法在不同空中场景中实时小物体检测的适用性。本文还讨论了这些方法如何作为航空物体识别技术未来发展的基础模型。源代码将在此处提供。]]></description>
      <guid>https://arxiv.org/abs/2405.01699</guid>
      <pubDate>Mon, 06 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>使用边界框注释进行主动学习实现低成本细胞图像分割</title>
      <link>https://arxiv.org/abs/2405.01701</link>
      <description><![CDATA[arXiv:2405.01701v1 公告类型：新
摘要：细胞图像分割通常使用完全监督的深度学习方法来实现，该方法严重依赖于大量带注释的训练数据。然而，由于细胞形态的复杂性和对专业知识的要求，细胞图像的像素级注释已成为一项高度劳动密集型的任务。为了解决上述问题，我们提出了一种使用边界框注释进行细胞分割的主动学习框架，大大降低了细胞分割算法的数据注释成本。首先，我们将YOLOv8检测器与分段任意模型（SAM）相结合，生成了盒监督学习方法（记为YOLO-SAM），有效降低了数据标注的复杂度。此外，它还集成到主动学习框架中，该框架采用 MC DropBlock 方法以较少的框注释样本训练分割模型。大量的实验表明，与掩模监督的深度学习方法相比，我们的模型节省了百分之九十以上的数据注释时间。]]></description>
      <guid>https://arxiv.org/abs/2405.01701</guid>
      <pubDate>Mon, 06 May 2024 06:19:11 GMT</pubDate>
    </item>
    <item>
      <title>适应计算病理学的自我监督学习</title>
      <link>https://arxiv.org/abs/2405.01688</link>
      <description><![CDATA[arXiv:2405.01688v1 公告类型：新
摘要：自监督学习（SSL）已成为训练网络的关键技术，可以很好地推广到无需特定任务监督的各种任务。这一特性使得 SSL 成为计算病理学（组织数字化图像的研究）的理想选择，因为目标应用很多，但标记的训练样本通常有限。然而，SSL 算法和模型主要是在自然图像领域开发的，其性能是否可以通过适应特定领域来提高仍然是一个悬而未决的问题。在这项工作中，我们对病理数据的 SSL 修改进行了研究，特别关注 DINOv2 算法。我们根据病理图像的特征提出了替代增强、正则化函数和位置编码。我们评估这些变化对几个基准的影响，以证明定制方法的价值。]]></description>
      <guid>https://arxiv.org/abs/2405.01688</guid>
      <pubDate>Mon, 06 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>自动驾驶中分布外检测的语言增强潜在表示</title>
      <link>https://arxiv.org/abs/2405.01691</link>
      <description><![CDATA[arXiv:2405.01691v1 公告类型：新
摘要：分布外（OOD）检测对于自动驾驶至关重要，可以确定基于学习的组件何时遇到意外输入。传统的检测器通常使用固定设置的编码器模型，因此缺乏有效的人机交互能力。随着大型基础模型的兴起，多模态输入提供了将人类语言作为潜在表示的可能性，从而实现了语言定义的 OOD 检测。在本文中，我们使用多模态模型CLIP编码的图像和文本表示的余弦相似度作为新的表示，以提高用于视觉异常检测的潜在编码的透明度和可控性。我们将我们的方法与现有的预训练编码器进行比较，后者只能产生从用户的角度来看毫无意义的潜在表示。我们对真实驾驶数据的实验表明，基于语言的潜在表示比视觉编码器的传统表示表现更好，并且与标准表示结合时有助于提高检测性能。]]></description>
      <guid>https://arxiv.org/abs/2405.01691</guid>
      <pubDate>Mon, 06 May 2024 06:19:10 GMT</pubDate>
    </item>
    <item>
      <title>S4：跨频谱的自监督感知</title>
      <link>https://arxiv.org/abs/2405.01656</link>
      <description><![CDATA[arXiv:2405.01656v1 公告类型：新
摘要：卫星图像时间序列（SITS）分割对于环境监测、土地覆盖制图和农作物类型分类等许多应用至关重要。然而，由于缺乏丰富的训练数据，SITS 分割的训练模型仍然是一项具有挑战性的任务，这需要细粒度的注释。我们提出了 S4 一种新的自监督预训练方法，该方法通过利用两个新见解显着减少对标记训练数据的需求：（a）卫星捕获频谱不同部分（例如无线电频率和可见频率）的图像。 (b) 卫星图像经过地理注册，可实现细粒度的空间对齐。我们利用这些见解来制定 S4 中的预训练任务。我们还策划了 m2s2-SITS，这是一个未标记、空间对齐、多模式和地理特定 SITS 的大型数据集，可作为 S4 的代表性预训练数据。最后，我们在多个 SITS 分割数据集上评估 S4，并在使用有限的标记数据时证明其与竞争基线的有效性。]]></description>
      <guid>https://arxiv.org/abs/2405.01656</guid>
      <pubDate>Mon, 06 May 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>基于最后一个卷积层输出的高维特征的子空间投影的分布外检测</title>
      <link>https://arxiv.org/abs/2405.01662</link>
      <description><![CDATA[arXiv:2405.01662v1 公告类型：新
摘要：分布外（OOD）检测对于可靠的模式分类至关重要，它可以识别样本是否源自训练分布之外。本文主要关注最终卷积层输出的高维特征，其中包含丰富的图像特征。我们的关键思想是将这些高维特征投影到两个特定的特征子空间中，利用网络线性层的降维能力，并使用预定义均匀分布类质心（PEDCC）损失进行训练。这涉及计算三个投影角度的余弦和特征的范数，从而识别分布内（ID）和OOD数据的独特信息，这有助于OOD检测。在此基础上，我们修改了全连接层之前的批量标准化（BN）和ReLU层，减少了它们对输出特征分布的影响，从而扩大了ID和OOD数据特征之间的分布差距。我们的方法只需要分类网络模型的训练，避免了任何输入预处理或特定 OOD 数据预调整的需要。对多个基准数据集的广泛实验表明，我们的方法提供了最先进的性能。我们的代码可在 https://github.com/Hewell0/ProjOOD 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.01662</guid>
      <pubDate>Mon, 06 May 2024 06:19:09 GMT</pubDate>
    </item>
    <item>
      <title>解释与对象和隐私相关的模型</title>
      <link>https://arxiv.org/abs/2405.01646</link>
      <description><![CDATA[arXiv:2405.01646v1 公告类型：新
摘要：由于内容的多样性和隐私本身的主观性质，在在线共享图像之前准确预测图像是否是隐私是很困难的。在本文中，我们评估隐私模型，该模型使用从图像中提取的对象来确定图像被预测为隐私的原因。为了解释这些模型的决定，我们使用特征归因来识别和量化哪些对象（及其哪些特征）与参考输入（即图像中没有定位对象）的隐私分类更相关，预测为民众。我们表明，人员类别的存在及其基数是隐私决策的主要因素。因此，这些模型大多无法识别描述包含敏感数据、车辆所有权和互联网活动的文档的私人图像，或包含人物的公共图像（例如，户外音乐会或在著名地标旁边的公共空间行走的人）。作为未来基准的基线，我们还设计了两种基于人员存在和基数的策略，并实现了隐私模型的可比分类性能。]]></description>
      <guid>https://arxiv.org/abs/2405.01646</guid>
      <pubDate>Mon, 06 May 2024 06:19:08 GMT</pubDate>
    </item>
    <item>
      <title>您只需要关键补丁：用于稳健医疗诊断的多实例学习框架</title>
      <link>https://arxiv.org/abs/2405.01654</link>
      <description><![CDATA[arXiv:2405.01654v1 公告类型：新
摘要：深度学习模型因其出色的性能而彻底改变了医学图像分析领域。然而，它们对虚假相关性很敏感，通常利用数据集偏差来改善域内数据的结果，但危及它们的泛化能力。在本文中，我们建议通过使用多实例学习 (MIL) 框架来限制这些模型用于达到最终分类的信息量。MIL 强制模型仅使用图像中 (小) 块子集来识别判别区域。这模仿了临床​​程序，其中医疗决策基于局部发现。我们在两个医疗应用上评估了我们的框架：使用皮肤镜检查进行皮肤癌诊断和使用乳房 X 线摄影进行乳腺癌诊断。我们的结果表明，与基线方法相比，仅使用块子集不会影响域内数据的诊断性能。然而，我们的方法对患者人口统计数据的变化更为稳健，同时还提供了有关哪些区域对决策有所贡献的更详细解释。代码可在以下位置获得：https://github.com/diogojpa99/MedicalMultiple-Instance-Learning。]]></description>
      <guid>https://arxiv.org/abs/2405.01654</guid>
      <pubDate>Mon, 06 May 2024 06:19:08 GMT</pubDate>
    </item>
    <item>
      <title>可配置的学习全息术</title>
      <link>https://arxiv.org/abs/2405.01558</link>
      <description><![CDATA[arXiv:2405.01558v1 公告类型：新
摘要：在追求先进的全息显示技术的过程中，我们面临着一个独特而持久的障碍：学习的全息技术在适应各种硬件配置方面缺乏灵活性。
  这是由于现有全息显示器中复杂的光学组件和系统设置存在差异。
  尽管新兴的学习方法已经实现了快速、高质量的全息图生成，但显示硬件的任何改变仍然需要对模型进行重新训练。
  我们的工作引入了一个可配置的学习模型，该模型可以从纯 RGB 2D 图像交互式计算 3D 全息图，用于各种全息显示器。
  该模型可以适应现有全息显示器的预定义硬件参数，例如工作波长、像素间距、传播距离和峰值亮度，而无需重新训练。
  此外，我们的模型适用于各种全息图类型，包括传统的单色和新兴的多色全息图，它们在全息显示器中同时使用多种原色。
  值得注意的是，我们在文献中首次使全息图计算能够依赖于识别学习领域内的深度估计和 3D 全息图合成任务之间的相关性。
  我们通过学生-教师学习策略采用知识蒸馏来简化我们的交互性能模型。
  与最先进的模型相比，速度提高了 2 倍，同时使用不同的硬件配置始终生成高质量的 3D 全息图。]]></description>
      <guid>https://arxiv.org/abs/2405.01558</guid>
      <pubDate>Mon, 06 May 2024 06:19:07 GMT</pubDate>
    </item>
    <item>
      <title>医学、工业及其他领域图像分割中的可解释人工智能 (XAI)：一项调查</title>
      <link>https://arxiv.org/abs/2405.01636</link>
      <description><![CDATA[arXiv:2405.01636v1 公告类型：新
摘要：人工智能（XAI）在计算机视觉领域有着广泛的应用。虽然基于图像分类的可解释性技术引起了极大的关注，但语义分割中的对应技术却相对被忽视了。鉴于图像分割的广泛使用，从医疗到工业部署，这些技术需要系统化的观察。在本文中，我们对语义图像分割中的 XAI 进行了首次全面调查。这项工作重点关注专门为密集预测任务引入的技术，或通过修改现有分类方法对其进行扩展的技术。我们根据应用程序类别和领域以及所使用的评估指标和数据集对文献进行分析和分类。我们还提出了可解释语义分割的分类法，并讨论了潜在的挑战和未来的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2405.01636</guid>
      <pubDate>Mon, 06 May 2024 06:19:07 GMT</pubDate>
    </item>
    </channel>
</rss>