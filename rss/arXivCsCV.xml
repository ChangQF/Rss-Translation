<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 16 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>通过内容感知形状调整进行 3D 形状增强</title>
      <link>https://arxiv.org/abs/2405.09050</link>
      <description><![CDATA[arXiv:2405.09050v1 公告类型：新
摘要：3D 模型深度学习的最新进展推动了生成、检测和场景理解方面的突破。然而，这些算法的有效性取决于大型训练数据集。我们通过引入高效 3D 接缝雕刻 (E3SC) 来应对这一挑战，这是一种基于接缝雕刻的新型 3D 模型增强方法，它仅逐步变形输入模型的一部分，同时确保整体语义不变。实验表明，我们的方法能够在各种类型和风格的输入模型中生成多样化且高质量的增强 3D 形状，比以前的方法取得了显着的改进。定量评估表明，我们的方法有效增强了其他后续 3D 生成算法生成的形状的新颖性和质量。]]></description>
      <guid>https://arxiv.org/abs/2405.09050</guid>
      <pubDate>Thu, 16 May 2024 06:16:04 GMT</pubDate>
    </item>
    <item>
      <title>弱小目标检测与跟踪：一种基于时间能量选择性缩放和轨迹关联的新方法</title>
      <link>https://arxiv.org/abs/2405.09054</link>
      <description><![CDATA[arXiv:2405.09054v1 公告类型：新
摘要：被动光学遥感（PORS）中小目标的检测和跟踪具有广泛的应用。然而，先前提出的大多数方法很少利用目标运动形成的丰富时间特征，导致低信杂比（SCR）目标的检测和跟踪性能较差。本文分析了基于空间特征的难度和基于时间特征实现有效检测的可行性。根据这一分析，我们使用多帧作为检测单元，提出了一种基于时间能量选择性缩放（TESS）的检测方法。具体来说，我们研究了多帧检测单元上的像素形成的强度时间分布（ITP）的组成。对于存在目标的像素，目标穿过该像素会对ITP带来微弱的瞬态扰动，并引入ITP统计特性的变化。我们使用精心设计的函数来放大瞬态扰动，抑制背景和噪声分量，并在多帧检测单元上输出目标的轨迹。随后，为了解决传统阈值分割带来的检测率和误报率之间的矛盾，我们将输出轨迹的时间和空间特征关联起来，提出了一种基于3D Hough变换的轨迹提取方法。最后，我们对目标的轨迹进行建模，并提出一种基于轨迹的多目标跟踪方法。与各种最先进的检测和跟踪方法相比，多个场景的实验证明了我们提出的方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2405.09054</guid>
      <pubDate>Thu, 16 May 2024 06:16:04 GMT</pubDate>
    </item>
    <item>
      <title>从部分标签比例学习整个幻灯片图像分割</title>
      <link>https://arxiv.org/abs/2405.09041</link>
      <description><![CDATA[arXiv:2405.09041v1 公告类型：新
摘要：在本文中，我们利用不完整的标签比例来解决整个幻灯片图像（WSI）中肿瘤亚型的分割问题。具体来说，我们利用“部分”标签比例，它给出了肿瘤亚型之间的比例，但没有给出肿瘤和非肿瘤之间的比例。部分标签比例被病理学家记录为标准诊断信息，因此，我们希望使用它们来实现分割模型，该分割模型可以将每个 WSI 斑块分类为一种肿瘤亚型或非肿瘤亚型。我们将此问题称为“从部分标签比例学习（LPLP）”，并将该问题表述为弱监督学习问题。然后，我们针对这个具有挑战性的问题提出了一种有效的算法，将其分解为两个弱监督学习子问题：多实例学习（MIL）和从标签比例学习（LLP）。这些子问题以端到端的方式得到有效优化。我们的算法的有效性通过在两个 WSI 数据集上进行的实验得到了证明。]]></description>
      <guid>https://arxiv.org/abs/2405.09041</guid>
      <pubDate>Thu, 16 May 2024 06:16:03 GMT</pubDate>
    </item>
    <item>
      <title>AMSNet：AMS 电路的网表数据集</title>
      <link>https://arxiv.org/abs/2405.09045</link>
      <description><![CDATA[arXiv:2405.09045v1 公告类型：新
摘要：当今的模拟/混合信号 (AMS) 集成电路 (IC) 设计需要大量的手动干预。多模态大语言模型 (MLLM) 的出现在各个领域展现了巨大的潜力，表明它们也适用于简化大规模 AMS IC 设计。使用 MLLM 进行自动 AMS 电路生成的瓶颈是缺乏描绘原理图网表关系的综合数据集。因此，我们设计了一种自动技术，将原理图转换为网表，并创建数据集 AMSNet，其中包含晶体管级原理图和相应的 SPICE 格式网表。随着规模的不断扩大，AMSNet 可以极大地促进 AMS 电路设计中 MLLM 应用的探索。我们已经公开了一组初始网表，并将在本文发布后提供我们的网表生成工具和完整数据集。]]></description>
      <guid>https://arxiv.org/abs/2405.09045</guid>
      <pubDate>Thu, 16 May 2024 06:16:03 GMT</pubDate>
    </item>
    <item>
      <title>带有噪声标签的遥感图像上基于动态损耗衰减的鲁棒定向目标检测</title>
      <link>https://arxiv.org/abs/2405.09024</link>
      <description><![CDATA[arXiv:2405.09024v1 公告类型：新
摘要：遥感图像中目标的模糊外观、微小尺度和细粒度类别不可避免地导致检测数据集的类别标签中存在噪声注释。然而，在现代面向遥感目标探测器中，标签噪声的影响和处理尚未得到充分探索。为了解决这个问题，我们提出了一种通过动态损失衰减（DLD）机制的鲁棒面向遥感目标检测方法，其灵感来自于清洁深度神经网络的两阶段“早期学习”和“记忆”学习动态和嘈杂的样本。具体来说，我们首先观察称为 EL 的早期学习阶段的终点，此后模型开始记住会显着降低检测精度的错误标签。其次，在训练指标的指导下，将每个样本的损失按降序排列，并在接下来的epoch中自适应衰减前K个最大的样本（坏样本）的损失。因为这些巨大的损失对于使用错误的标签进行计算具有很高的可信度。实验结果表明，该方法在具有合成类别标签噪声的HRSC2016和DOTA-v1.0/v2.0等多个公共数据集上测试，取得了优异的抗噪声性能。我们的解决方案还获得了2023年国家大数据与计算智能挑战赛“基于亚米级遥感图像的细粒度目标检测”带噪声标签赛道的第二名。]]></description>
      <guid>https://arxiv.org/abs/2405.09024</guid>
      <pubDate>Thu, 16 May 2024 06:16:02 GMT</pubDate>
    </item>
    <item>
      <title>ICAL：用于增强手写数学表达式识别的隐式字符辅助学习</title>
      <link>https://arxiv.org/abs/2405.09032</link>
      <description><![CDATA[arXiv:2405.09032v1 公告类型：新
摘要： 手写数学表达式识别领域已经取得了重大进展，而现有的编码器-解码器方法通常难以对 \LaTeX 中的全局信息进行建模。因此，本文提出了一种新方法——隐式字符辅助学习（ICAL），来挖掘全局表达信息并增强手写数学表达识别。具体来说，我们提出隐式字符构造模块（ICCM）来预测隐式字符序列，并使用融合模块来合并 ICCM 和解码器的输出，从而产生正确的预测。通过建模和利用隐式字符信息，ICAL 可以对手写数学表达式进行更准确和上下文感知的解释。实验结果表明，ICAL 明显超越了最先进的（SOTA）模型，在 CROHME 2014/2016/2019 数据集上的表情识别率（ExpRate）分别提高了 2.21\%/1.75\%/1.28\% ，并在具有挑战性的 HME100k 测试集上取得了令人瞩目的 69.25\%。我们在 GitHub 上提供代码：https://github.com/qingzhenduyu/ICAL]]></description>
      <guid>https://arxiv.org/abs/2405.09032</guid>
      <pubDate>Thu, 16 May 2024 06:16:02 GMT</pubDate>
    </item>
    <item>
      <title>学习可变形物体的对应关系</title>
      <link>https://arxiv.org/abs/2405.08996</link>
      <description><![CDATA[arXiv:2405.08996v1 公告类型：新
摘要：我们通过比较经典方法和基于学习的方法，研究可变形物体（即布料和绳索）的像素对应问题。我们选择布料和绳子是因为它们传统上是最难分析建模的可变形物体，具有较大的配置空间，并且它们在机器人任务中很有意义，例如布料折叠、绳子打结、T恤折叠、窗帘对应问题在机器人技术中受到广泛关注，具有广泛的应用，包括语义抓取、对象跟踪和基于对应构建的操作策略。我们对通过特征匹配进行对应的现有经典方法（包括 SIFT、SURF 和 ORB）以及最近发布的两种基于学习的方法（包括 TimeCycle 和 Dense Object Nets）进行了详尽的调查。我们做出了三个主要贡献：（1）一个用于模拟和渲染可变形物体的合成图像的框架，定性结果证明了我们的模拟域和真实域之间的转移（2）一种新的基于学习的对应方法，扩展了密集对象网络，以及（3） ）对最先进的通信方法进行标准化比较。我们提出的方法提供了一种灵活的通用公式，用于学习非刚性（和刚性）对象的时间和空间连续对应关系。我们报告了所有方法的均方根误差统计数据，并发现密集对象网络的性能优于基线经典对应方法，并且我们提出的密集对象网络的扩展表现类似。]]></description>
      <guid>https://arxiv.org/abs/2405.08996</guid>
      <pubDate>Thu, 16 May 2024 06:16:01 GMT</pubDate>
    </item>
    <item>
      <title>用于参考图像分割的空间语义循环挖掘</title>
      <link>https://arxiv.org/abs/2405.09006</link>
      <description><![CDATA[arXiv:2405.09006v1 公告类型：新
摘要：参考图像分割（RIS）始终需要语言和外观语义来更好地相互理解。尤其是在困难的情况下，这种需求变得更加迫切。为了实现这一目标，现有的工作倾向于采用各种反表示机制来直接沿着主要 RGB 分支反馈语言语义，但这将导致空间中弱挖掘的指称分布和沿通道的非指称语义污染。在本文中，我们提出了空间语义循环挖掘（S\textsuperscript{2}RM）来实现高质量的跨模态融合。它遵循三部曲的工作策略：分布语言特征、空间语义循环共解析和解析语义平衡。在融合过程中，S\textsuperscript{2}RM将首先生成约束弱但分布感知的语言特征，然后将来自一种模态上下文的旋转特征的每行和每列的特征捆绑在一起，以循环关联来自其他模态的特征中包含的相关语义上下文，最后采用自蒸馏权重来权衡不同解析语义的贡献。通过协同解析，S\textsuperscript{2}RM 将信息从生成器上下文的近端和远程切片层传输到已解析上下文的当前切片层，能够更好地建模双向和结构化的全局关系。此外，我们还提出了一种跨尺度抽象语义引导解码器（CASG）来强调所指对象的前景，最终以相对较低的成本集成不同粒度的特征。对当前四个具有挑战性的数据集的广泛实验结果表明，我们提出的方法相对于其他最先进的算法表现良好。]]></description>
      <guid>https://arxiv.org/abs/2405.09006</guid>
      <pubDate>Thu, 16 May 2024 06:16:01 GMT</pubDate>
    </item>
    <item>
      <title>基于期望最大化的多模型 3D 配准理论分析</title>
      <link>https://arxiv.org/abs/2405.08991</link>
      <description><![CDATA[arXiv:2405.08991v1 公告类型：新
摘要：我们对最近提出的基于期望最大化的算法进行了详细的理论分析，该算法用于解决 3D 配准问题的变体，即多模型 3D 配准。尽管已经显示出优越的实证结果，但并没有从理论上证明 EM 方法收敛到基本事实的条件是合理的。在这个项目中，我们的目标是通过创造这样的条件来缩小这一差距。特别是，分析围绕着在整个课程的各种实例中开发和应用的概率尾部边界的使用。这个项目中研究的问题是另一个例子，与课程中看到的不同，尾部边界有助于以概率的方式促进我们对算法的理解。我们提供关于 3D 注册的独立背景材料]]></description>
      <guid>https://arxiv.org/abs/2405.08991</guid>
      <pubDate>Thu, 16 May 2024 06:16:00 GMT</pubDate>
    </item>
    <item>
      <title>使用大视觉语言模型的上下文情感识别</title>
      <link>https://arxiv.org/abs/2405.08992</link>
      <description><![CDATA[arXiv:2405.08992v1 公告类型：新
摘要：“边界框中的人感觉如何？”在现实世界中实现对人的明显情绪的人类水平的识别仍然是计算机视觉中尚未解决的任务。面部表情是不够的：身体姿势、情境知识和常识推理都有助于人类如何执行情感心理理论任务。在本文中，我们研究了最近大型视觉语言模型支持的两种主要方法：1）图像字幕，然后是纯语言法学硕士，2）零样本和微调设置下的视觉语言模型。我们评估了上下文情绪 (EMOTIC) 数据集上的方法，并证明即使在小型数据集上进行微调的视觉语言模型也可以显着优于传统基线。这项工作的结果旨在帮助机器人和智能体在未来进行情感敏感的决策和交互。]]></description>
      <guid>https://arxiv.org/abs/2405.08992</guid>
      <pubDate>Thu, 16 May 2024 06:16:00 GMT</pubDate>
    </item>
    <item>
      <title>用于骨 X 射线分析的深度学习表示的自监督视觉语言对齐</title>
      <link>https://arxiv.org/abs/2405.08932</link>
      <description><![CDATA[arXiv:2405.08932v1 公告类型：新
摘要：本文建议利用骨 X 射线视觉语言预训练与法国报告相结合来解决骨 X 射线照相感兴趣的下游任务。引入了实用的处理流程来匿名和处理法国医疗报告。预训练包括对源自深度模型编码器的视觉和文本嵌入空间进行自我监督对齐。然后，生成的图像编码器用于处理各种下游任务，包括骨关节炎的量化、儿科手腕骨龄的估计、骨折和异常检测。与需要大量人类专家注释的替代方案相比，我们的方法展示了在下游任务上的竞争性能。我们的工作是第一项整合法国报告来塑造专门用于骨 X 射线表示的嵌入空间的研究，利用医院中可用的大量配对图像和报告数据。通过依赖特定语言场景中的通用视觉语言深度模型，它有助于为更广泛的医疗保健应用部署视觉模型。]]></description>
      <guid>https://arxiv.org/abs/2405.08932</guid>
      <pubDate>Thu, 16 May 2024 06:15:59 GMT</pubDate>
    </item>
    <item>
      <title>鸟瞰图到街景：一项调查</title>
      <link>https://arxiv.org/abs/2405.08961</link>
      <description><![CDATA[arXiv:2405.08961v1 公告类型：新
摘要：近年来，街景图像已发展成为地理空间数据收集和城市分析最重要的来源之一，有助于生成有意义的见解并协助决策。由于两个域之间的外观和视点存在显着差异，从相应的卫星图像合成街景图像是一项具有挑战性的任务。在这项研究中，我们筛选了 20 篇最近的研究论文，全面回顾了如何从相应的卫星图像合成街景图像的最新技术。主要发现是：（i）需要新颖的深度学习技术来合成更真实和准确的街景图像； (ii) 需要收集更多数据集供公众使用； (iii)需要研究更具体的评估指标以适当评估生成的图像。我们的结论是，由于应用过时的深度学习技术，最近的文献未能生成详细且多样化的街景图像。]]></description>
      <guid>https://arxiv.org/abs/2405.08961</guid>
      <pubDate>Thu, 16 May 2024 06:15:59 GMT</pubDate>
    </item>
    <item>
      <title>ADA-Track：具有交替检测和关联功能的端到端多摄像头 3D 多目标跟踪</title>
      <link>https://arxiv.org/abs/2405.08909</link>
      <description><![CDATA[arXiv:2405.08909v1 公告类型：新
摘要：许多基于查询的 3D 多对象跟踪 (MOT) 方法采用注意力跟踪范例，利用跟踪查询进行身份一致检测，并利用对象查询进行与身份无关的跟踪生成。然而，注意力跟踪将检测和跟踪查询纠缠在一个嵌入中，用于检测和跟踪任务，这是次优的。其他方法类似于检测跟踪范例，使用解耦的跟踪和检测查询来检测对象，然后进行后续关联。然而，这些方法没有利用检测和关联任务之间的协同作用。结合这两种范式的优势，我们推出了 ADA-Track，这是一种用于多视图相机 3D MOT 的新颖端到端框架。我们引入了一种基于边缘增强交叉注意力、利用外观和几何特征的可学习数据关联模块。此外，我们将此关联模块集成到基于 DETR 的 3D 检测器的解码器层中，从而实现同时类似 DETR 的查询到图像交叉注意用于检测和查询到查询交叉注意用于数据关联。通过堆叠这些解码器层，交替细化检测和关联任务的查询，有效地利用任务依赖性。我们在 nuScenes 数据集上评估我们的方法，并证明我们的方法与之前的两种范式相比的优势。代码可在 https://github.com/dsx0511/ADA-Track 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.08909</guid>
      <pubDate>Thu, 16 May 2024 06:15:58 GMT</pubDate>
    </item>
    <item>
      <title>带有高质量字幕的 CLIP：视觉任务的强大预训练</title>
      <link>https://arxiv.org/abs/2405.08911</link>
      <description><![CDATA[arXiv:2405.08911v1 公告类型：新
摘要：CLIP 模型在零样本分类和检索任务上表现非常出色。但最近的研究表明，CLIP 中学习的表示不太适合密集的预测任务，如对象检测、语义分割或深度估计。最近，引入了 CLIP 模型的多阶段训练方法，以缓解 CLIP 在下游任务上的弱性能。在这项工作中，我们发现，简单地提高图像文本数据集中的字幕质量就可以提高 CLIP 视觉表示的质量，从而显着改善下游密集预测视觉任务。事实上，我们发现具有高质量字幕的 CLIP 预训练可以超越最近的监督、自监督和弱监督预训练方法。我们表明，当使用 ViT-B/16 作为图像编码器的 CLIP 模型在对齐良好的图像-文本对上进行训练时，与最近的最新状态相比，它在语义分割和深度估计任务上获得了 12.1% 更高的 mIoU 和 11.5% 更低的 RMSE。艺术掩模图像建模（MIM）预训练方法，如掩模自动编码器（MAE）。我们发现移动架构也从 CLIP 预训练中受益匪浅。最近的移动视觉架构 MCi2 通过 CLIP 预训练获得了与 Swin-L 相似的性能，在 ImageNet-22k 上针对语义分割任务进行了预训练，同时尺寸缩小了 6.1 倍。此外，我们表明，在对密集预测任务进行微调时，提高字幕质量可带来 10 倍的数据效率。]]></description>
      <guid>https://arxiv.org/abs/2405.08911</guid>
      <pubDate>Thu, 16 May 2024 06:15:58 GMT</pubDate>
    </item>
    <item>
      <title>考虑视频多样性的使用文本语义匹配的语言引导自监督视频摘要</title>
      <link>https://arxiv.org/abs/2405.08890</link>
      <description><![CDATA[arXiv:2405.08890v1 公告类型：新
摘要：当前的视频摘要方法主要依赖于有监督的计算机视觉技术，这需要耗时的手动注释。此外，注释总是主观的，这使得这项任务更具挑战性。为了解决这些问题，我们分析了将视频摘要转换为文本摘要任务并利用大型语言模型（LLM）来提高视频摘要的可行性。本文提出了一种由法学硕士指导的新型自监督视频摘要框架。我们的方法首先为视频帧生成字幕，然后由法学硕士将其合成为文本摘要。随后，我们测量帧标题和文本摘要之间的语义距离。值得注意的是，我们提出了一种新颖的损失函数来根据视频的多样性来优化我们的模型。最后，可以通过选择字幕与文本摘要相似的帧来生成摘要视频。我们的模型取得了与其他最先进方法相媲美的结果，并为视频摘要铺平了一条新途径。]]></description>
      <guid>https://arxiv.org/abs/2405.08890</guid>
      <pubDate>Thu, 16 May 2024 06:15:57 GMT</pubDate>
    </item>
    </channel>
</rss>