<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 07 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>鲁棒联合稀疏视图 CT 重建的隐式神经表示</title>
      <link>https://arxiv.org/abs/2405.02509</link>
      <description><![CDATA[arXiv:2405.02509v1 公告类型：新
摘要：计算机断层扫描（CT）在工业质量控制和医疗诊断中至关重要。稀疏视图 CT 可以减少电离辐射，但由于其采样不足的性质而面临挑战，从而导致不适定的重建问题。隐式神经表征 (INR) 的最新进展在解决稀疏视图 CT 重建方面显示出了希望。认识到 CT 通常涉及扫描相似的对象，我们提出了一种通过使用 INR 联合重建多个对象来提高重建质量的新方法。这种方法可以潜在地利用 INR 的优势和多个对象的统计规律。虽然当前的 INR 联合重建技术主要侧重于通过元初始化加速收敛，但它们并不是专门为提高重建质量而定制的。为了解决这一差距，我们引入了一种新颖的基于 INR 的贝叶斯框架，该框架集成了潜在变量以捕获对象间关系。这些变量在整个优化过程中充当动态参考，从而提高个体重建保真度。我们进行了大量的实验，评估了各种关键因素，例如重建质量、抗过度拟合和概括性，证明了通用数值指标相对于基线的显着改进。这凸显了 CT 重建方法的显着进步。]]></description>
      <guid>https://arxiv.org/abs/2405.02509</guid>
      <pubDate>Tue, 07 May 2024 06:19:27 GMT</pubDate>
    </item>
    <item>
      <title>时空 SwinMAE：基于 Swin Transformer 的时态卫星图像多尺度表示学习器</title>
      <link>https://arxiv.org/abs/2405.02512</link>
      <description><![CDATA[arXiv:2405.02512v1 公告类型：新
摘要： 目前，以大语言模型为代表的基础模型已经取得了巨大的进步，并被应用于包括2D和3D视觉在内的非常广泛的领域。作为基础模型的重要应用领域之一，对地观测引起了人们的关注，并开发了各种方法。当将地球观测视为单个图像捕获时，地球观测图像可以被处理为具有三个或更多通道的图像，并且当它在一个位置具有不同时间戳的多个图像捕获时，时间观测可以被视为一组类似于视频帧或医学扫描切片的连续图像。本文提出了时空 SwinMAE（ST-SwinMAE），这是一种特别关注时空图像处理的表示学习的架构。具体来说，它使用带有 Video Swin Transformer 块的分层屏蔽自动编码器 (MAE)。通过该架构，我们提出了一个名为 Degas 100M 的预训练模型作为地理空间基础模型。此外，我们提出了一种使用 Degas 100M 进行迁移学习的方法，利用 MAE 的预训练编码器和解码器，并在它们之间添加跳跃连接来实现多尺度信息通信，形成一个名为 Spatio-Temporal SwinUNet (ST-SwinUNet) 的架构。与现有最先进的基础模型相比，我们的方法显示出性能的显着提高。具体来说，对于 PhilEO Bench 数据集上的土地覆盖下游任务的迁移学习，与其他地理空间基础模型相比，其准确率平均高出 10.4%。]]></description>
      <guid>https://arxiv.org/abs/2405.02512</guid>
      <pubDate>Tue, 07 May 2024 06:19:27 GMT</pubDate>
    </item>
    <item>
      <title>Rip-NeRF：具有 Ripmap 编码柏拉图实体的抗锯齿辐射场</title>
      <link>https://arxiv.org/abs/2405.02386</link>
      <description><![CDATA[arXiv:2405.02386v1 公告类型：新
摘要：尽管神经辐射场 (NeRF) 取得了重大进展，但渲染仍然可能会出现锯齿和模糊伪影，因为有效且高效地表征锥体铸造过程引起的各向异性区域仍然是一个基本挑战。本文介绍了一种 Ripmap 编码的柏拉图实体表示法，可以精确有效地表征 3D 各向异性区域，从而实现高保真抗锯齿渲染。我们方法的核心是两个关键组件：柏拉图实体投影和 Ripmap 编码。柏拉图实体投影将 3D 空间分解到特定柏拉图实体的无与伦比的面上，以便各向异性 3D 区域可以投影到具有可区分特征的平面上。同时，柏拉图实体的每个面都通过 Ripmap 编码进行编码，Ripmap 编码是通过对可学习特征网格进行各向异性预过滤而构建的，以便能够通过各向异性区域采样精确有效地表征投影各向异性区域。对完善的合成数据集和新捕获的真实世界数据集进行的大量实验表明，我们的 Rip-NeRF 实现了最先进的渲染质量，特别是在重复结构和纹理的精细细节方面表现出色，同时保持相对较快的速度训练次数。]]></description>
      <guid>https://arxiv.org/abs/2405.02386</guid>
      <pubDate>Tue, 07 May 2024 06:19:26 GMT</pubDate>
    </item>
    <item>
      <title>光栅化边缘梯度：以不同的方式处理不连续性</title>
      <link>https://arxiv.org/abs/2405.02508</link>
      <description><![CDATA[arXiv:2405.02508v1 公告类型：新
摘要：计算渲染过程的梯度对于计算机视觉和图形领域的各种应用至关重要。然而，由于不连续性和渲染近似，这些梯度的精确计算具有挑战性，特别是对于基于表面的表示和基于光栅化的渲染。我们提出了一种计算基于光栅化的可微渲染器可见性不连续处的梯度的新方法。我们的方法通过精心设计的近似策略优雅地简化了传统上复杂的问题，从而实现了简单、有效和高性能的解决方案。我们引入了一种新颖的微边缘概念，它使我们能够将光栅化图像视为可微分、连续过程的结果，与固有的不可微分、离散像素光栅化相一致。该技术消除了对前向通道进行渲染近似或其他修改的必要性，从而保留了渲染图像的完整性，这使得它适用于禁止过滤的光栅化掩模、深度和法线图像。利用微边缘简化了不连续处的梯度解释并能够处理几何交叉点，从而提供了优于现有技术的优势。我们展示了我们在动态人体头部场景重建中的方法，展示了对相机图像和分割掩模的有效处理。]]></description>
      <guid>https://arxiv.org/abs/2405.02508</guid>
      <pubDate>Tue, 07 May 2024 06:19:26 GMT</pubDate>
    </item>
    <item>
      <title>Rad4XCNN：一种新的不可知论方法，通过放射组学对 CNN 衍生特征进行事后全局解释</title>
      <link>https://arxiv.org/abs/2405.02334</link>
      <description><![CDATA[arXiv:2405.02334v1 公告类型：新
摘要：近年来，临床决策支持系统（CDSS）中的人工智能（AI）在利用机器学习和深度学习架构方面发挥了关键作用。尽管人工智能模型的能力很有前途，但缺乏透明度和可解释性带来了重大挑战，特别是在可靠性是强制性要求的医疗环境中。在不影响预测准确性的情况下实现透明度仍然是一个关键挑战。本文提出了一种新方法，即 Rad4XCNN，通过放射组学特征固有的可解释性来增强 CNN 衍生特征的预测能力。 Rad4XCNN 与基于显着图的传统方法不同，通过放射组学将可理解的含义与 CNN 衍生的特征相关联，为可视化图之外的解释方法提供了新的视角。我们以乳腺癌分类任务作为案例研究，在超声成像数据集上评估了 Rad4XCNN，包括一个在线数据集和两个用于内部和外部验证的内部数据集。一些关键结果是： i) 与 ViT 导出的特征和放射组学特征相比，CNN 导出的特征保证了更稳健的准确性； ii) 传统的可视化地图解释方法存在一些缺陷； iii) Rad4XCNN 不会为了可解释性而牺牲模型的准确性； iv) Rad4XCNN 提供全局解释见解，使医生能够分析模型输出和结果。此外，我们强调将可解释性集成到人工智能模型中以增强临床实践中的信任和采用的重要性，并强调我们的方法如何减轻与可解释的人工智能方法相关的一些担忧。]]></description>
      <guid>https://arxiv.org/abs/2405.02334</guid>
      <pubDate>Tue, 07 May 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>法学硕士作为数据集分析师：使用大型语言模型发现子群体结构</title>
      <link>https://arxiv.org/abs/2405.02363</link>
      <description><![CDATA[arXiv:2405.02363v1 公告类型：新
摘要：子群体的分布是隐藏在数据集中的重要属性。揭示和分析数据集中的子种群分布可以提供对数据集的全面理解，是有益于各种下游任务的强大工具，包括数据集子种群组织、子种群转移和切片发现。尽管它很重要，但据我们所知，还没有系统地探索数据集的亚群分布的工作。为了解决这一限制并以统一的方式解决所有提到的任务，我们引入了一种新的子总体结构概念来表示、分析和利用数据集中的子总体分布。为了以可解释的方式表征结构，我们提出了使用大语言模型的子群体结构发现（SSD-LLM）框架，该框架利用大语言模型（LLM）的世界知识和指令跟踪功能来对信息丰富的图像标题进行语言分析并总结结构。此外，我们提出了解决下游任务的完整工作流程，称为特定于任务的调整，展示了所发现的结构在一系列与子群相关的任务中的应用，包括数据集子群组织、子群转移和切片发现。此外，我们提出了解决下游任务的完整工作流程，称为特定于任务的调整，展示了所发现的结构在一系列与子群相关的任务中的应用，包括数据集子群组织、子群转移和切片发现。]]></description>
      <guid>https://arxiv.org/abs/2405.02363</guid>
      <pubDate>Tue, 07 May 2024 06:19:25 GMT</pubDate>
    </item>
    <item>
      <title>使用动态场景分析在协作学习环境中进行长期人类参与评估</title>
      <link>https://arxiv.org/abs/2405.02317</link>
      <description><![CDATA[arXiv:2405.02317v1 公告类型：新
摘要：本文开发了数据集和方法来评估学生在现实生活协作学习环境中的参与情况。在协作学习环境中，学生被组织成小组，他们可以在小组内自由互动。因此，学生可以自由移动，从而导致姿势变化较大的问题，移出并重新进入相​​机场景，或背对相机。我们将评估学生参与度的问题表述为两个子问题：（i）针对其他群体的强背景干扰进行学生群体检测，以及（ii）群体内的动态参与者跟踪。使用包含 12,518,250 个学生标签实例、总时长为 21 小时 22 分钟的现实视频的大型独立测试数据集来评估我们提出的学生群体检测方法的性能。所提出的使用多个图像表示的方法在所有视频实例上都表现得与 YOLO 相同或更好。在整个数据集上，所提出的方法取得了 0.85 的 F1 分数，而 YOLO 的 F1 分数为 0.80。在学生群体检测之后，本文介绍了动态参与者跟踪系统的开发，用于通过长视频会话评估学生群体的参与情况。所提出的动态参与者跟踪系统被证明表现非常好，35 个测试视频中只有一个丢失了一名学生。相比之下，最先进的方法无法在 35 个测试视频中的 14 个中跟踪学生。该方法在一组独立的长的、真实的协作视频上实现了 82.3% 的准确率。]]></description>
      <guid>https://arxiv.org/abs/2405.02317</guid>
      <pubDate>Tue, 07 May 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>使用贝叶斯优化和文本到图像模型有效探索图像分类器故障</title>
      <link>https://arxiv.org/abs/2405.02332</link>
      <description><![CDATA[arXiv:2405.02332v1 公告类型：新
摘要：在现实世界中应该谨慎使用图像分类器。在验证集上评估的性能可能无法反映现实世界中的性能。特别是，分类器对于训练期间经常遇到的情况可能表现良好，但对于其他不常见的情况则表现不佳。在这项研究中，我们假设文本到图像生成模型的最新进展使它们对于图像分类器等计算机视觉模型的基准测试很有价值：它们可以生成由导致分类器失败的文本提示调节的图像，从而允许用文本属性。然而，当需要生成大量合成图像时，即需要测试许多不同属性组合时，它们的生成成本就成为问题。我们提出了一种图像分类器基准测试方法，作为交替图像生成、分类器评估和属性选择的迭代过程。该方法有效地探索了最终导致不良行为检测的属性。]]></description>
      <guid>https://arxiv.org/abs/2405.02332</guid>
      <pubDate>Tue, 07 May 2024 06:19:24 GMT</pubDate>
    </item>
    <item>
      <title>在字幕中插入人脸：使用注意力引导合并进行图像字幕制作</title>
      <link>https://arxiv.org/abs/2405.02305</link>
      <description><![CDATA[arXiv:2405.02305v1 公告类型：新
摘要：图像字幕模型广泛用于描述最近和存档的图片，目的是提高其可访问性和检索性。然而，这些方法在检索人名时往往效率低下且存在偏见。在这项工作中，我们引入了 AstroCaptions，这是一个用于图像字幕任务的数据集。该数据集具体包含数千个公众人物，传统模型很难识别这些公众人物。我们还提出了一种新颖的后处理方法，使用可解释的人工智能工具和视觉语言模型的基础功能在标题中插入已识别的人名。使用这种方法获得的结果显示了字幕质量的显着改善以及减少幻觉的潜力。高达 93.2% 的检测到的人物可以插入到图像字幕中，从而提高每个字幕模型的 BLEU、ROUGE、CIDEr 和 METEOR 分数。]]></description>
      <guid>https://arxiv.org/abs/2405.02305</guid>
      <pubDate>Tue, 07 May 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>海洋渔业中的 YOLOv5 与 YOLOv8：平衡类检测和实例计数</title>
      <link>https://arxiv.org/abs/2405.02312</link>
      <description><![CDATA[arXiv:2405.02312v1 公告类型：新
摘要：本文提出了使用 YOLOv5 和 YOLOv8 对三种不同类别（卤虫、囊肿和排泄物）进行目标检测的比较研究。在这项比较研究中，我们从准确度、精确度、召回率等方面分析了这些模型的性能，其中 YOLOv5 在检测卤虫和囊肿方面通常表现更好，具有出色的精确度和准确度。然而，在检测排泄物方面，YOLOv5 面临着显着的挑战和局限性。这表明 YOLOv8 在检测任务中提供了更大的多功能性和适应性，而 YOLOv5 可能会在困难的情况下陷入困境，并且可能需要进一步的微调或专门的训练来增强其性能。结果显示了 YOLOv5 和 YOLOv8 在具有挑战性的海洋环境中检测物体的适用性，对生态研究等应用具有影响。]]></description>
      <guid>https://arxiv.org/abs/2405.02312</guid>
      <pubDate>Tue, 07 May 2024 06:19:23 GMT</pubDate>
    </item>
    <item>
      <title>采用通用投票方案来提高视觉位置识别性能</title>
      <link>https://arxiv.org/abs/2405.02297</link>
      <description><![CDATA[arXiv:2405.02297v1 公告类型：新
摘要：视觉位置识别一直是许多利用不同集成方法来提高 VPR 性能的努力的主题。多进程融合、Fly-Inspired Voting Unit、SwitchHit 或 Switch-Fuse 等想法涉及将不同的 VPR 技术结合在一起，利用不同的策略。然而，许多这些策略常见的一个主要方面是投票。投票是一个非常相关的主题，需要探索它的应用以及对于任何整体 VPR 设置的重要性。本文分析了几种投票方案，以最大限度地提高 VPR 集合的位置检测精度，并确定最佳投票方案以进行选择。我们从政治学和社会学等领域广泛采用的各种投票方案中汲取灵感，通过经验数据可以明显看出，投票方法的选择对结果有很大影响。本文测试了多种投票方案，以展示多个数据集的 VPR 结果的改进。我们的目标是确定是否存在单一的最佳投票方案，或者就像其他研究领域一样，投票技术的选择与其应用和环境相关。我们建议对这些不同的投票方法从最好到最差进行排名，以便更好地进行选择。在以雷达图、PR 曲线的形式展示投票方法性能范围的结果时，PR 曲线展示了性能差异，并使用 McNemar 测试变体的比较方法来确定差异的统计显着性。进行此测试是为了进一步确认结果的可靠性并进行比较，以便更好、更明智地选择投票技术。]]></description>
      <guid>https://arxiv.org/abs/2405.02297</guid>
      <pubDate>Tue, 07 May 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>TFCounter：无需训练的物体计数的抛光宝石</title>
      <link>https://arxiv.org/abs/2405.02301</link>
      <description><![CDATA[arXiv:2405.02301v1 公告类型：新
摘要：物体计数是一项具有挑战性的任务，在安全监控、交通管理和疾病诊断等方面具有广阔的应用前景。现有的对象计数方法面临三重挑战：实现卓越的性能、保持高通用性以及最小化注释成本。我们开发了一种新颖的免训练、与类无关的对象计数器 TFCounter，它通过大规模基础模型中基本元素的级联来实现即时上下文感知。该方法采用具有双重提示系统的迭代计数框架来识别更广泛的形状、外观和大小不同的物体。此外，它还引入了一种创新的上下文感知相似性模块，该模块结合了背景上下文，以提高杂乱场景中的准确性。为了证明跨领域的通用性，我们收集了一个名为 BIKE-1000 的新颖计数数据集，其中包括美团独家的 1000 张共享单车图像。在 FSC-147、CARPK 和 BIKE-1000 数据集上进行的大量实验表明，TFCounter 的性能优于现有领先的免训练方法，并且与经过训练的方法相比，表现出有竞争力的结果。]]></description>
      <guid>https://arxiv.org/abs/2405.02301</guid>
      <pubDate>Tue, 07 May 2024 06:19:22 GMT</pubDate>
    </item>
    <item>
      <title>神经加法图像模型：通过插值解释</title>
      <link>https://arxiv.org/abs/2405.02295</link>
      <description><![CDATA[arXiv:2405.02295v1 公告类型：新
摘要：理解图像如何影响世界、解释图像的语义对各种量的影响以及探索基于图像的预测变化背后的原因是非常困难但又非常有趣的问题。通过采用神经加法模型与扩散自动编码器相结合的整体建模方法，我们可以有效地识别图像效果的潜在隐藏语义，并实现附加表格效果的完全可理解性。我们的方法提供了高度的灵活性，使我们能够全面探索各种图像特征的影响。我们证明所提出的方法可以在消融研究中精确识别复杂的图像效果。为了进一步展示我们提出的模型的实际适用性，我们进行了一项案例研究，研究主机图像中捕获的独特特征和属性如何对 Airbnb 租赁定价产生影响。]]></description>
      <guid>https://arxiv.org/abs/2405.02295</guid>
      <pubDate>Tue, 07 May 2024 06:19:21 GMT</pubDate>
    </item>
    <item>
      <title>用于减轻表征学习中视角扭曲的 M\"obius 变换</title>
      <link>https://arxiv.org/abs/2405.02296</link>
      <description><![CDATA[arXiv:2405.02296v1 公告类型：新
摘要： 透视畸变（PD）导致图像中视觉概念的形状、大小、方向、角度和其他空间关系发生前所未有的变化。精确估计相机的内在和外在参数是一项具有挑战性的任务，它可以防止合成透视失真。缺乏专用训练数据对开发强大的计算机视觉方法构成了关键障碍。此外，畸变校正方法使其他计算机视觉任务成为多步骤方法并且缺乏性能。在这项工作中，我们建议通过对特定系列的 M\&quot;obius 变换采用细粒度参数控制来减轻透视畸变（MPD），以模拟真实世界的畸变，而无需估计相机的内在和外在参数，也不需要实际的畸变此外，我们还提出了一个专用的透视扭曲基准数据集 ImageNet-PD，以针对该新数据集对深度学习模型的鲁棒性进行基准测试。此外，该方法的性能优于现有基准 ImageNet-E 和 ImageNet-X。提高了 ImageNet-PD 的性能，同时在标准数据分布上保持一致的性能。此外，我们的方法在三个受 PD 影响的实际应用中显示出改进的性能：人群计数、鱼眼图像识别和人员重新识别。我们将发布源代码，数据集和促进进一步研究的模型。]]></description>
      <guid>https://arxiv.org/abs/2405.02296</guid>
      <pubDate>Tue, 07 May 2024 06:19:21 GMT</pubDate>
    </item>
    <item>
      <title>基础模型在推进自动驾驶汽车方面的前瞻性作用</title>
      <link>https://arxiv.org/abs/2405.02288</link>
      <description><![CDATA[arXiv:2405.02288v1 公告类型：新
摘要： 随着人工智能的发展和深度学习的突破，大规模基础模型（FM），如GPT、CLIP等，在自然语言处理、计算机视觉等多个领域取得了显着的成果。 FM 在自动驾驶中的应用前景广阔。例如，它们可以有助于增强场景理解和推理。通过对丰富的语言和视觉数据进行预训练，FM可以理解和解释驾驶场景中的各种元素，并提供认知推理，为驾驶决策和规划提供语言和动作命令。此外，FM 可以基于对驾驶场景的理解来增强数据，以提供长尾分布中罕见情况的可行场景，这些情况在日常驾驶和数据收集过程中不太可能遇到。这种增强随后可以提高自动驾驶系统的准确性和可靠性。 FM 应用潜力的另一个证明在于世界模型的开发，以 DREAMER 系列为例，它展示了理解物理定律和动力学的能力。世界模型在自监督学习的范式下从海量数据中学习，可以生成不可见但似是而非的驾驶环境，有助于增强道路使用者行为的预测和驾驶策略的离线训练。在本文中，我们综合了FM在自动驾驶中的应用和未来趋势。通过利用FM的强大能力，我们致力于解决自动驾驶中长尾分布带来的潜在问题，从而提高该领域的整体安全性。]]></description>
      <guid>https://arxiv.org/abs/2405.02288</guid>
      <pubDate>Tue, 07 May 2024 06:19:20 GMT</pubDate>
    </item>
    </channel>
</rss>