<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 10 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>使用计算机视觉和机器学习进行高通量表型分析</title>
      <link>https://arxiv.org/abs/2407.06354</link>
      <description><![CDATA[arXiv:2407.06354v2 公告类型：新
摘要：高通量表型分析是指对植物表型进行无损且有效的评估。近年来，它与机器学习相结合，通过提高处理大型数据集的效率和开发提取特定性状的方法来改进植物表型分析的过程。先前的研究已经开发出通过将深度神经网络与自动相机结合使用来应对这些挑战的方法；然而，所研究的数据集通常不包括物理标签。在本研究中，我们使用了橡树岭国家实验室提供的数据集，其中包含 1,672 张 Populus Trichocarpa 图像，白色标签显示处理（控制或干旱）、块、行、位置和基因型。使用光学字符识别 (OCR) 读取植物上的这些标签，结合使用图像分割技术和机器学习算法进行形态分类，使用机器学习模型根据这些分类预测治疗方案，并使用分析的编码 EXIF 标签查找叶片大小和表型之间的相关性。我们发现我们的 OCR 模型对非空文本提取的准确率为 94.31%，可以将信息准确地放入电子表格中。我们的分类模型识别叶片形状、颜色和棕色斑点程度的平均准确率为 62.82%，识别植物治疗的准确率为 60.08%。最后，我们发现 EXIF 标签中缺少一些关键信息，这些信息阻碍了对叶片大小的评估。还有一些缺失的信息阻碍了对表型和条件之间相关性的评估。但是，未来的研究可以对此进行改进，以便评估这些特征。]]></description>
      <guid>https://arxiv.org/abs/2407.06354</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:59 GMT</pubDate>
    </item>
    <item>
      <title>MiraData：具有长时长和结构化字幕的大规模视频数据集</title>
      <link>https://arxiv.org/abs/2407.06358</link>
      <description><![CDATA[arXiv:2407.06358v1 公告类型：新
摘要：Sora 的高运动强度和长一致性视频对视频生成领域产生了重大影响，引起了前所未有的关注。然而，现有的公开数据集不足以生成类似 Sora 的视频，因为它们主要包含运动强度低且字幕简短的短视频。为了解决这些问题，我们提出了 MiraData，这是一个高质量的视频数据集，在视频时长、字幕细节、运动强度和视觉质量方面都超越了以前的数据集。我们从各种手动选择的来源中整理 MiraData，并精心处理数据以获得语义一致的剪辑。GPT-4V 用于注释结构化字幕，从四个不同的角度提供详细描述以及总结的密集字幕。为了更好地评估视频生成中的时间一致性和运动强度，我们引入了 MiraBench，它通过添加 3D 一致性和基于跟踪的运动强度指标来增强现有基准。 MiraBench 包含 150 个评估提示和 17 个指标，涵盖时间一致性、运动强度、3D 一致性、视觉质量、文本-视频对齐和分布相似性。为了证明 MiraData 的实用性和有效性，我们使用基于 DiT 的视频生成模型 MiraDiT 进行了实验。MiraBench 上的实验结果证明了 MiraData 的优越性，尤其是在运动强度方面。]]></description>
      <guid>https://arxiv.org/abs/2407.06358</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:59 GMT</pubDate>
    </item>
    <item>
      <title>GeoWATCH 用于在异质时间序列卫星图像中检测大型建筑</title>
      <link>https://arxiv.org/abs/2407.06337</link>
      <description><![CDATA[arXiv:2407.06337v1 公告类型：新
摘要：由于时空错位以及分辨率和捕获光谱的差异，从多个传感器学习具有挑战性。为此，我们引入了 GeoWATCH，这是一个灵活的框架，用于在来自多个传感器平台的长序列卫星图像上训练模型，旨在处理图像分类、活动识别、对象检测或对象跟踪任务。我们的系统包括一种基于子图同构的新型部分权重加载机制，允许在多个训练周期内持续训练和修改网络。这使我们能够在很长一段时间内训练一系列模型，我们观察到，随着我们在保持核心主干的同时调整配置，性能得到了提高。]]></description>
      <guid>https://arxiv.org/abs/2407.06337</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:58 GMT</pubDate>
    </item>
    <item>
      <title>驾驶行为预测的无噪声解释</title>
      <link>https://arxiv.org/abs/2407.06339</link>
      <description><![CDATA[arXiv:2407.06339v1 公告类型：新
摘要：虽然注意力机制在基于 Transformer 的架构中在各个人工智能 (AI) 领域取得了长足的进步，但其内部工作原理仍有待探索。现有的可解释方法侧重点不同，但比较片面。它们主要分析注意力机制或基于梯度的归因，而忽略了输入特征值的大小或跳过连接模块。此外，它们不可避免地会带来与模型决策无关的虚假噪声像素归因，从而阻碍人们对斑点可视化结果的信任。因此，我们提出了一种易于实施但有效的方法来弥补这一缺陷：平滑噪声范数注意力 (SNNA)。我们通过变换后的值向量的范数来加权注意力，并用注意力梯度引导标签特定信号，然后随机采样输入扰动并平均相应的梯度以产生无噪声归因。与之前的研究不同，我们不是评估二分类或多分类任务上的解释方法，而是在本研究中探索更复杂的多标签分类场景，即驾驶行为预测任务，并专门为其训练了一个模型。定性和定量评估结果都表明，SNNA 与其他基于 SOTA 注意力的可解释方法相比，在生成更清晰的视觉解释图和对输入像素重要性进行排序方面具有优势。]]></description>
      <guid>https://arxiv.org/abs/2407.06339</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:58 GMT</pubDate>
    </item>
    <item>
      <title>从基于能量的模型角度进一步阐明稳健分类器</title>
      <link>https://arxiv.org/abs/2407.06315</link>
      <description><![CDATA[arXiv:2407.06315v1 公告类型：新
摘要：通过将强大的判别分类器重新解释为基于能量的模型 (EBM)，我们对对抗训练 (AT) 的动态提出了新的看法。我们对 AT 期间能量状况的分析表明，从模型的角度来看，非针对性攻击产生的对抗性图像比原始数据分布更分散（能量更低）。相反，我们观察到针对性攻击的情况正好相反。在彻底分析的基础上，我们提出了新的理论和实践结果，展示了如何解释 AT 能量动态以获得更好的理解：（1）AT 动态由三个阶段控制，稳健过拟合发生在第三阶段，自然能量和对抗能量之间存在巨大差异（2）通过在能量方面重写 TRadeoff 启发的替代损失最小化对抗性防御 (TRADES) 的损失，我们表明 TRADES 通过将自然能量与对抗能量相结合来隐式缓解过度拟合（3）我们通过经验表明，所有最新稳健分类器都在平滑能量格局，并且我们协调了关于理解 AT 和在 EBM 的保护下加权损失函数的各种研究。在严谨证据的推动下，我们提出了加权能量对抗训练 (WEAT)，这是一种新颖的样本加权方案，可在 CIFAR-10 和 SVHN 等多个基准上实现与最新水平相当的稳健准确度，并在 CIFAR-100 和 Tiny-ImageNet 上超越。我们进一步表明，稳健分类器的生成能力的强度和质量各不相同，并提供了一种简单的方法来推动这种能力，使用稳健分类器无需进行生成建模训练即可达到卓越的初始分数 (IS) 和 FID。重现我们结果的代码可在 http://github.com/OmnAI-Lab/Robust-Classifiers-under-the-lens-of-EBM/ 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.06315</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:57 GMT</pubDate>
    </item>
    <item>
      <title>GeoLifeCLEF 2024 中用于多标签分类的图块压缩和嵌入</title>
      <link>https://arxiv.org/abs/2407.06326</link>
      <description><![CDATA[arXiv:2407.06326v1 公告类型：新
摘要：我们与 DS@GT 团队一起探索解决 GeoLifeCLEF 2024 竞赛提出的多标签分类任务的方法，该任务旨在使用空间和时间遥感数据预测特定位置植物物种的存在和不存在。我们的方法通过离散余弦变换 (DCT) 使用频域系数来压缩和预先计算卷积神经网络的原始输入数据。我们还通过局部敏感哈希 (LSH) 研究最近邻域模型进行预测并通过 tile2vec 帮助进行嵌入的自监督对比学习。我们最好的竞赛模型利用地理位置特征，排行榜​​得分为 0.152，最佳赛后得分为 0.161。源代码和模型可在 https://github.com/dsgt-kaggle-clef/geolifeclef-2024 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.06326</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:57 GMT</pubDate>
    </item>
    <item>
      <title>VIMI：通过多模式教学奠定视频生成基础</title>
      <link>https://arxiv.org/abs/2407.06304</link>
      <description><![CDATA[arXiv:2407.06304v1 公告类型：新
摘要：现有的文本到视频传播模型仅依靠纯文本编码器进行预训练。这种限制源于缺乏大规模多模态提示视频数据集，导致缺乏视觉基础并限制了它们在多模态集成中的多功能性和应用。为了解决这个问题，我们通过使用检索方法将上下文示例与给定的文本提示配对来构建大规模多模态提示数据集，然后利用两阶段训练策略在同一模型中实现不同的视频生成任务。在第一阶段，我们提出了一个多模态条件视频生成框架，用于在这些增强数据集上进行预训练，为基础视频生成建立了基础模型。其次，我们在三个视频生成任务上对第一阶段的模型进行微调，并结合多模态指令。这个过程进一步完善了模型处理不同输入和任务的能力，确保多模态信息的无缝集成。经过这两个阶段的训练过程，VIMI 展示了多模态理解能力，根据提供的输入生成了内容丰富且个性化的视频，如图 1 所示。与之前的基于视觉的视频生成方法相比，VIMI 可以合成具有大动作的一致且时间连贯的视频，同时保留语义控制。最后，VIMI 还在 UCF101 基准上实现了最先进的文本转视频生成结果。]]></description>
      <guid>https://arxiv.org/abs/2407.06304</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:56 GMT</pubDate>
    </item>
    <item>
      <title>SweepNet：通过神经清扫器进行无监督学习形状抽象</title>
      <link>https://arxiv.org/abs/2407.06305</link>
      <description><![CDATA[arXiv:2407.06305v1 公告类型：新
摘要：形状抽象是一项重要任务，可在保留基本特征的同时简化复杂的几何结构。扫掠曲面通常存在于人造物体中，它通过有效捕获和表示物体几何形状来协助此过程，从而促进抽象。在本文中，我们介绍了 \papername，这是一种通过扫掠曲面进行形状抽象的新方法。我们提出了一种有效的扫掠曲面参数化方法，利用超椭圆表示轮廓，利用 B 样条曲线表示轴。这种紧凑的表示只需要 14 个浮点数，便于直观和交互式编辑，同时有效地保留形状细节。此外，通过引入可微分神经扫描器和编码器-解码器架构，我们展示了无需监督即可预测扫掠曲面表示的能力。我们通过整篇论文中的几个定量和定性实验展示了我们模型的优越性。我们的代码可在 https://mingrui-zhao.github.io/SweepNet/ 获得]]></description>
      <guid>https://arxiv.org/abs/2407.06305</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:56 GMT</pubDate>
    </item>
    <item>
      <title>使用自监督视觉变换器进行多标签植物物种分类</title>
      <link>https://arxiv.org/abs/2407.06298</link>
      <description><![CDATA[arXiv:2407.06298v1 公告类型：新
摘要：我们为 PlantCLEF 2024 竞赛提出了一种使用自监督视觉变换器 (DINOv2) 的迁移学习方法，重点关注多标签植物物种分类。我们的方法利用基础和微调的 DINOv2 模型来提取广义特征嵌入。我们训练分类器使用这些丰富的嵌入来预测单个图像中的多种植物物种。为了解决大规模数据集的计算挑战，我们使用 Spark 进行分布式数据处理，确保在一组工作者中实现高效的内存管理和处理。我们的数据处理管道将图像转换为图块网格，对每个图块进行分类，并将这些预测聚合为一组合并的概率。我们的结果证明了将迁移学习与高级数据处理技术相结合对于多标签图像分类任务的有效性。我们的代码可在 https://github.com/dsgt-kaggle-clef/plantclef-2024 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.06298</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:55 GMT</pubDate>
    </item>
    <item>
      <title>使用 SAM 和移动窗口方法进行无监督故障检测</title>
      <link>https://arxiv.org/abs/2407.06303</link>
      <description><![CDATA[arXiv:2407.06303v1 公告类型：新 
摘要：工程中的自动故障检测和监控至关重要，但由于需要收集和标记大量有缺陷的样本，因此通常很困难。我们提出了一种使用高端 Segment Anything 模型 (SAM) 和移动窗口方法的无监督方法。SAM 因其准确性和多功能性而在 AI 图像分割社区中获得了认可。但是，在处理某些意外形状（例如阴影和细微的表面不规则性）时，其性能可能不一致。这种限制引发了人们对其在现实世界场景中故障检测适用性的担忧。我们的目标是在不需要微调或标记数据的情况下克服这些挑战。我们的技术将图片分成更小的窗口，然后使用 SAM 进行处理。这通过关注局部细节提高了故障识别的准确性。我们计算分割部分的大小，然后使用聚类技术来发现一致的故障区域，同时滤除噪声。为了进一步提高该方法的稳健性，我们建议在工业环境中添加指数加权移动平均 (EWMA) 技术进行持续监控，这将提高该方法随时间跟踪故障的能力。我们使用一个真实的案例研究将我们的方法与各种成熟的方法进行了比较，其中我们的模型实现了 0.96 的准确率，而第二好的模型的准确率仅为 0.85。我们还使用两个开源数据集比较了我们的方法，其中我们的模型在数据集中实现了一致的 0.86 的准确率，而第二好的模型的准确率仅为 0.53 和 0.54。]]></description>
      <guid>https://arxiv.org/abs/2407.06303</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:55 GMT</pubDate>
    </item>
    <item>
      <title>FairDiff：利用点图像扩散进行公平分割</title>
      <link>https://arxiv.org/abs/2407.06250</link>
      <description><![CDATA[arXiv:2407.06250v1 公告类型：新
摘要：公平性是医学图像分析的一个重要课题，其驱动因素是不同目标群体之间训练数据不平衡的挑战以及社会对公平医疗质量的需求。针对这一问题，我们的研究采用了数据驱动的策略，通过整合合成图像来增强数据平衡。然而，在生成合成图像方面，以前的工作要么缺乏配对标签，要么无法精确控制合成图像的边界与这些标签对齐。为了解决这个问题，我们以联合优化的方式制定了这个问题，其中三个网络朝着经验风险最小化和公平最大化的目标进行了优化。在实施方面，我们的解决方案采用了创新的点图像扩散架构，它利用 3D 点云通过点掩模图像合成管道来改进对掩模边界的控制。该方法在合成扫描激光检眼镜 (SLO) 眼底图像方面的表现明显优于现有技术。通过在训练阶段使用建议的 Equal Scale 方法将合成数据与真实数据相结合，我们的模型与最先进的公平学习模型相比实现了卓越的公平分割性能。代码可在 https://github.com/wenyi-li/FairDiff 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.06250</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:54 GMT</pubDate>
    </item>
    <item>
      <title>SGOR：利用语义和几何信息去除异常值，实现稳健的点云配准</title>
      <link>https://arxiv.org/abs/2407.06297</link>
      <description><![CDATA[arXiv:2407.06297v1 Announce Type: new 
摘要：本文介绍了一种新的离群值去除方法，充分利用几何和语义信息，实现鲁棒的配准。目前基于语义的配准方法仅将语义用于点对点或实例语义对应生成，存在两个问题。首先，这些方法高度依赖于语义的正确性。它们在语义不正确和语义稀疏的场景中表现不佳。其次，语义的使用仅限于对应生成，导致在弱几何场景中性能不佳。为了解决这些问题，一方面，我们提出了基于区域投票的二次地面分割和松散语义一致性。它通过减少对单点语义的依赖来提高对语义正确性的鲁棒性。另一方面，我们提出了用于离群值去除的语义几何一致性，充分利用语义信息并显着提高对应质量。此外，提出了两阶段假设验证，解决了弱几何场景中变换选择不正确的问题。在户外数据集中，我们的方法表现出色，注册召回率提高了 22.5 个百分点，并在各种条件下实现了更好的鲁棒性。我们的代码可用。]]></description>
      <guid>https://arxiv.org/abs/2407.06297</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:54 GMT</pubDate>
    </item>
    <item>
      <title>Poetry2Image：中国古典诗歌生成图像的迭代校正框架</title>
      <link>https://arxiv.org/abs/2407.06196</link>
      <description><![CDATA[arXiv:2407.06196v1 公告类型：新
摘要：在涉及中国古典诗歌的任务中，文本到图像的生成模型经常会遇到关键元素丢失或语义混淆的问题。通过微调模型解决这个问题需要相当大的训练成本。此外，手动提示重新扩散调整需要专业知识。为了解决这个问题，我们提出了Poetry2Image，一个针对中国古典诗歌生成图像的迭代校正框架。Poetry2Image利用外部诗歌数据集建立自动反馈和校正循环，通过图像生成模型和随后由大型语言模型（LLM）建议的重新扩散修改来增强诗歌和图像之间的一致性。使用200句中国古典诗歌的测试集，所提出的方法与五种流行的图像生成模型相结合，平均元素完整度达到70.63%，比直接图像生成提高了25.56%。在语义正确性测试中，我们的方法平均语义一致性达到80.09%。该研究不仅有助于古诗词文化的传播，也为类似非微调方法提升LLM生成能力提供了参考。]]></description>
      <guid>https://arxiv.org/abs/2407.06196</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:53 GMT</pubDate>
    </item>
    <item>
      <title>语义视频对象分割提案中的上下文传播</title>
      <link>https://arxiv.org/abs/2407.06247</link>
      <description><![CDATA[arXiv:2407.06247v1 公告类型：新
摘要：在本文中，我们提出了一种用于学习视频中语义上下文关系的新方法，用于语义对象分割。我们的算法从视频对象提议中得出语义上下文，这些提议对对象的关键演变和对象在时空域上的关系进行编码。这种语义上下文在视频中传播，以估计所有局部超像素对之间的成对上下文，这些成对上下文以成对势的形式集成到条件随机场中，并推断出每个超像素的语义标签。实验表明，与最先进的方法相比，我们的上下文学习和传播模型有效地提高了解决语义视频对象分割中视觉模糊性的鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2407.06247</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:53 GMT</pubDate>
    </item>
    <item>
      <title>更加独特的黑人和女性面孔导致视觉语言模型中的刻板印象增强</title>
      <link>https://arxiv.org/abs/2407.06194</link>
      <description><![CDATA[arXiv:2407.06194v1 公告类型：新
摘要：以 GPT-4V 为例的视觉语言模型 (VLM) 巧妙地整合了文本和视觉模态。这种整合增强了大型语言模型模仿人类感知的能力，使它们能够处理图像输入。然而，尽管 VLM 具有先进的功能，但人们担心 VLM 会继承两种模态的偏见，从而使偏见更加普遍且难以缓解。我们的研究探讨了 VLM 如何在种族和性别方面延续同质性偏见和特征关联。当被提示根据人脸图像撰写故事时，GPT-4V 描述的从属种族和性别群体比主导群体具有更大的同质性，并且依赖于独特但通常是积极的刻板印象。重要的是，VLM 刻板印象是由视觉线索而不是群体成员身份驱动的，因此被评为更典型的黑人和女性面孔会受到更大的刻板印象。这些发现表明，VLM 可能会将与种族和性别群体相关的细微视觉线索与刻板印象联系起来，而这种联系可能很难缓解。我们探讨了这种行为背后的根本原因，讨论了其影响，并强调了在 VLM 逐渐反映人类感知时解决这些偏见的重要性。]]></description>
      <guid>https://arxiv.org/abs/2407.06194</guid>
      <pubDate>Thu, 11 Jul 2024 03:18:52 GMT</pubDate>
    </item>
    </channel>
</rss>