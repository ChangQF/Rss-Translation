<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 02 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>引导端到端驾驶模型中的注意力</title>
      <link>https://arxiv.org/abs/2405.00242</link>
      <description><![CDATA[arXiv:2405.00242v1 公告类型：新
摘要：通过模仿学习训练的基于视觉的端到端驾驶模型可以为自动驾驶提供经济实惠的解决方案。然而，训练这些性能良好的模型通常需要大量数据，同时仍然缺乏明确直观的激活图来揭示这些模型在驾驶时的内部工作原理。在本文中，我们研究如何通过在使用显着语义图的训练过程中添加损失项来引导这些模型的注意力以提高其驾驶质量并获得更直观的激活图。与以前的工作相比，我们的方法不需要在测试期间提供这些显着的语义图，也不需要修改它所应用的模型的架构。我们使用完美和嘈杂的显着语义图进行测试，两者都取得了令人鼓舞的结果，后者的灵感来自于真实数据可能遇到的错误。使用 CIL++ 作为代表性的最先进模型和具有标准基准的 CARLA 模拟器，我们进行了实验，证明我们的方法在训练更好的自动驾驶模型方面的有效性，特别是在数据和计算资源稀缺的情况下。]]></description>
      <guid>https://arxiv.org/abs/2405.00242</guid>
      <pubDate>Thu, 02 May 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>面向真实世界的 HDR 视频重建：大规模基准数据集和两阶段对齐网络</title>
      <link>https://arxiv.org/abs/2405.00244</link>
      <description><![CDATA[arXiv:2405.00244v1 公告类型：新
摘要：作为获取高动态范围（HDR）视频的重要且实用的方法，从交替曝光序列重建 HDR 视频的探索仍然较少，这主要是由于缺乏大规模的真实数据集。现有的方法主要是在合成数据集上进行训练，在真实场景中表现不佳。在这项工作中，为了促进现实世界 HDR 视频重建的发展，我们提出了 Real-HDRV，这是一个用于 HDR 视频重建的大规模现实世界基准数据集，具有多种场景、多种运动模式和高质量标签。具体来说，我们的数据集包含 500 个 LDR-HDR 视频对，包括约 28,000 个 LDR 帧和 4,000 个 HDR 标签，涵盖白天、夜间、室内和室外场景。据我们所知，我们的数据集是最大的现实世界 HDR 视频重建数据集。相应地，我们提出了一种用于 HDR 视频重建的端到端网络，其中设计了一种新颖的两阶段策略来顺序执行对齐。具体来说，第一阶段利用自适应估计的全局偏移量进行全局对齐，降低后续对齐的难度。第二阶段使用自适应可分离卷积在特征级别以从粗到细的方式隐式执行局部对齐。大量的实验表明：（1）在我们的数据集上训练的模型可以比在合成数据集上训练的模型在真实场景中获得更好的性能； (2)我们的方法优于以前最先进的方法。我们的数据集可在 https://github.com/yungsyu99/Real-HDRV 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.00244</guid>
      <pubDate>Thu, 02 May 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>使用语义对齐匹配变压器实现端到端半监督表检测</title>
      <link>https://arxiv.org/abs/2405.00187</link>
      <description><![CDATA[arXiv:2405.00187v1 公告类型：新
摘要：文档图像中的表格检测是文档处理中的一项关键任务，涉及表格的识别和定位。深度学习的最新进展极大地提高了该任务的准确性，但它仍然严重依赖大型标记数据集来进行有效训练。为了克服这一挑战，已经出现了几种半监督方法，通常使用基于 CNN 的检测器和锚点提议以及非极大值抑制 (NMS) 等后处理技术。然而，该领域的最新进展已将焦点转向基于 Transformer 的技术，消除了对 NMS 的需求并强调对象查询和注意机制。之前的研究主要集中在改进基于 Transformer 的检测器的两个关键领域：提高对象查询的质量和优化注意力机制。然而，增加对象查询可能会引入冗余，而对注意力机制的调整可能会增加复杂性。为了解决这些挑战，我们引入了一种采用 SAM-DETR 的半监督方法，这是一种用于对象查询和目标特征之间精确对齐的新颖方法。我们的方法证明了误报的显着减少和表格检测性能的显着增强，特别是在具有不同表格结构的复杂文档中。这项工作在半监督设置中提供了更高效、更准确的表检测。]]></description>
      <guid>https://arxiv.org/abs/2405.00187</guid>
      <pubDate>Thu, 02 May 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>生成人工智能时代的合成图像验证：什么有效，什么还没有</title>
      <link>https://arxiv.org/abs/2405.00196</link>
      <description><![CDATA[arXiv:2405.00196v1 公告类型：新
摘要：在这项工作中，我们概述了合成图像的检测和归因方法，并强调了它们的优点和缺点。我们还指出并讨论了该领域的热点话题，并概述了未来研究的有前景的方向。]]></description>
      <guid>https://arxiv.org/abs/2405.00196</guid>
      <pubDate>Thu, 02 May 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>通过布朗身份扩散的潜在空间探索生成合成人脸数据集</title>
      <link>https://arxiv.org/abs/2405.00228</link>
      <description><![CDATA[arXiv:2405.00228v1 公告类型：新
摘要：人脸识别（FR）模型是在大规模数据集上进行训练的，存在隐私和伦理问题。最近，有人提出使用合成数据来补充或替代真实数据来训练 FR 模型。尽管已经取得了有希望的结果，但仍不清楚生成模型是否可以为此类任务产生足够多样化的数据。在这项工作中，我们引入了一种新方法，其灵感来自受随机布朗力作用的软粒子的物理运动，使我们能够在各种约束下对潜在空间中的身份分布进行采样。有了这个，我们生成了几个人脸数据集，并通过训练 FR 模型对它们进行基准测试，结果表明，用我们的方法生成的数据超过了之前基于 GAN 的数据集的性能，并与最先进的基于扩散的合成方法实现了竞争性能数据集。我们还表明，这种方法可用于减轻生成器训练集的泄漏，并探索生成模型生成超出其范围的数据的能力。]]></description>
      <guid>https://arxiv.org/abs/2405.00228</guid>
      <pubDate>Thu, 02 May 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>从模态有效性的角度重新审视 RGBT 跟踪基准：新的基准、问题和方法</title>
      <link>https://arxiv.org/abs/2405.00168</link>
      <description><![CDATA[arXiv:2405.00168v1 公告类型：新
摘要：RGBT跟踪因其在多模态保证（MMW）场景（例如夜间和恶劣天气）中的鲁棒性而受到越来越多的关注，在这些场景中，依靠单一传感模态无法确保稳定的跟踪结果。然而，现有的基准主要包括在常见场景中收集的视频，其中 RGB 和热红外 (TIR) 信息都具有足够的质量。这使得数据不能代表恶劣的成像条件，导致毫米波场景中的跟踪失败。为了弥补这一差距，我们提出了一个新的基准，MV-RGBT，专门在毫米波场景中捕获。与现有数据集相比，MV-RGBT 包含更多的对象类别和场景，提供了多样化且具有挑战性的基准。此外，对于毫米波场景的恶劣成像条件，提出了一个新问题，即\textit{何时融合}，以刺激此类数据融合策略的发展。我们提出了一种基于专家混合的新方法，即 MoETrack，作为基线融合策略。在MoETrack中，每个专家都会生成独立的跟踪结果以及相应的置信度分数，用于控制融合过程。大量的实验结果证明了 MV-RGBT 在推进 RGBT 跟踪方面的巨大潜力，并得出这样的结论：融合并不总是有益的，尤其是在毫米波场景中。值得注意的是，所提出的 MoETrack 方法不仅在 MV-RGBT 上取得了新的最先进结果，而且在 RGBT234、LasHeR 和 VTUAV 的短期分割（VTUAV-ST）等标准基准上也取得了新的最先进结果。 MV-RGBT的更多信息和MoETrack的源代码将在https://github.com/Zhangyong-Tang/MoETrack发布。]]></description>
      <guid>https://arxiv.org/abs/2405.00168</guid>
      <pubDate>Thu, 02 May 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>揭示什么、为什么和如何：视频异常因果关系理解的综合基准</title>
      <link>https://arxiv.org/abs/2405.00181</link>
      <description><![CDATA[arXiv:2405.00181v1 公告类型：新
摘要：视频异常理解 (VAU) 旨在自动理解视频中的异常事件，从而实现交通监控和工业制造等各种应用。虽然现有的 VAU 基准主要集中在异常检测和定位上，但我们的重点是更实用性，这促使我们提出以下关键问题：“发生了什么异常？”，“为什么会发生？”以及“这个异常事件有多严重？”。为了寻找这些答案，我们提出了一个全面的视频异常因果关系理解基准 (CUVA)。具体来说，所提出的基准的每个实例都涉及三组人工注释，以指示异常的“什么”、“为什么”和“如何”，包括 1) 异常类型、开始和结束时间以及事件描述，2) 异常原因的自然语言解释，以及 3) 反映异常影响的自由文本。此外，我们还引入了 MMEval，这是一种新颖的评估指标，旨在更好地与人类对 CUVA 的偏好保持一致，有助于测量现有的 LLM，以理解视频异常的根本原因和相应的影响。最后，我们提出了一种新颖的基于提示的方法，可作为具有挑战性的 CUVA 的基线方法。我们进行了广泛的实验，以证明我们的评估指标和基于提示的方法的优越性。我们的代码和数据集可在 https://github.com/fesvhtr/CUVA 上找到。]]></description>
      <guid>https://arxiv.org/abs/2405.00181</guid>
      <pubDate>Thu, 02 May 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>使用一半的数据和减少 400 倍的计算量来训练高性能视网膜基础模型</title>
      <link>https://arxiv.org/abs/2405.00117</link>
      <description><![CDATA[arXiv:2405.00117v1 公告类型：新
摘要：人工智能在医学领域拥有巨大潜力，但传统上由于缺乏用于训练模型的海量数据集而受到限制。基础模型，即可以适应具有小数据集的下游任务的预训练模型，可以缓解这个问题。 Moorfields 眼科医院 (MEH) 的研究人员提出了 RETFound-MEH，这是一种视网膜成像的基础模型，经过 900,000 张图像（包括私人医院数据）的训练。最近，提出了数据高效的 DERETFound，它在仅使用 150,000 张公开图像进行训练的情况下提供了可比的性能。然而，这两种模型最初都需要非常大量的资源来训练，并且在下游使用中是资源密集型的。我们提出了一种新颖的令牌重建目标，用于训练 RETFound-Green，这是一种视网膜基础模型，仅使用 75,000 个公开可用的图像和 400 倍的计算量进行训练。我们估计 RETFound-MEH 和 DERETFound 的培训成本分别为 10,000 美元和 14,000 美元，而 RETFound-Green 的培训成本不到 100 美元，同时同样减少了对环境的影响。 RETFound-Green 在下游使用方面也更加高效：下载速度提高了 14 倍，计算向量嵌入的速度提高了 2.7 倍，所需的存储空间减少了 2.6 倍。尽管如此，RETFound-Green 的表现并没有系统性地变差。事实上，它在 14 项任务上表现最佳，而 DERETFound 为 6 项，RETFound-MEH 为 2 项。我们的结果表明 RETFound-Green 是一种非常高效、高性能的视网膜基础模型。我们预计我们的代币重建目标可以扩大规模以获得更高的性能，并应用于视网膜成像以外的其他领域。]]></description>
      <guid>https://arxiv.org/abs/2405.00117</guid>
      <pubDate>Thu, 02 May 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>拓展视野：为长尾胸部 X 射线分类启用混合量子迁移学习</title>
      <link>https://arxiv.org/abs/2405.00156</link>
      <description><![CDATA[arXiv:2405.00156v1 公告类型：新
摘要：由于量子机器学习 (QML) 相对于经典机器学习 (CML) 的理论量子优势，量子机器学习 (QML) 具有改善大规模胸部 X 射线 (CXR) 数据集中罕见但危急疾病的多标签分类的潜力。样本效率和普遍性。虽然之前的文献已经探索了使用 CXR 的 QML，但由于对量子硬件的访问有限以及计算成本高昂的模拟，它主要集中在小数据集的二元分类任务上。为此，我们实现了一个基于 Jax 的框架，该框架能够模拟中型量子位架构，并且与当前软件产品相比，挂钟时间有了显着改进。我们使用大规模 CXR 数据集评估了基于 Jax 的框架在混合量子迁移学习的效率和性能方面的性能，该学习用于跨 8、14 和 19 种疾病标签的长尾分类。与 PyTorch 和 TensorFlow 实现相比，基于 Jax 的框架分别实现了高达 58% 和 95% 的加速。然而，与 CML 相比，QML 的收敛速度较慢，对于 8、14 和 19 CXR 疾病标签的分类，平均 AUROC 分别为 0.70、0.73 和 0.74。相比之下，CML 模型的平均 AUROC 分别为 0.77、0.78 和 0.80。总之，我们的工作通过计算高效的基于 Jax 的框架，为长尾 CXR 分类提供了一种可访问的混合量子迁移学习实现。]]></description>
      <guid>https://arxiv.org/abs/2405.00156</guid>
      <pubDate>Thu, 02 May 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>具有跨模式匹配的自动创意选择</title>
      <link>https://arxiv.org/abs/2405.00029</link>
      <description><![CDATA[arXiv:2405.00029v1 公告类型：新
摘要：应用程序开发人员通过使用应用程序图像创建产品页面并对搜索词进行竞价来宣传他们的应用程序。因此，应用程序图像与搜索词高度相关至关重要。该问题的解决方案需要图像文本匹配模型来预测所选图像和搜索词之间的匹配质量。在这项工作中，我们提出了一种基于微调预训练 LXMERT 模型的新颖方法，将应用程序图像与搜索词进行匹配。我们表明，与 CLIP 模型以及使用 Transformer 模型搜索词和 ResNet 模型搜索图像的基线相比，我们显着提高了匹配精度。我们使用两组标签来评估我们的方法：给定应用程序的广告商关联（图像，搜索词）对，以及对（图像，搜索词）对之间相关性的人工评分。我们的方法在广告商相关的真实数据方面实现了 0.96 AUC 分数，比 Transformer+ResNet 基线和微调 CLIP 模型高出 8% 和 14%。对于人类标记的真实情况，我们的方法达到了 0.95 AUC 分数，比 Transformer+ResNet 基线和微调 CLIP 模型高出 16% 和 17%。]]></description>
      <guid>https://arxiv.org/abs/2405.00029</guid>
      <pubDate>Thu, 02 May 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>SegNet：一种基于分段深度学习的卷积神经网络方法，用于无人机野火检测</title>
      <link>https://arxiv.org/abs/2405.00031</link>
      <description><![CDATA[arXiv:2405.00031v1 公告类型：新
摘要：尽管数据集有限，但这项研究解决了提高无人机（UAV）/无人机图像用于全球野火检测的处理时间和检测能力的紧迫挑战。我们提出了分段神经网络（SegNet）选择方法，专注于减少特征图以提高时间分辨率和准确性，从而显着提高实时野火检测的处理速度和准确性。本文通过提出火、水、烟雾等非晶态物体图像分类的新方向，有助于提高处理速度，实现野火的实时检测能力，提高野火的检测精度，并提高早期野火的检测能力。采用卷积神经网络 (CNN) 进行图像分类，强调减少对深度学习过程至关重要的不相关特征，特别是在火灾检测的实时馈送数据中。鉴于火灾探测中实时馈送数据的复杂性，我们的研究强调图像馈送，凸显了增强实时处理的紧迫性。我们提出的算法通过分割来对抗特征过载，解决对象、颜色和纹理等不同特征带来的挑战。值得注意的是，特征图大小和数据集充足性之间的微妙平衡至关重要。一些研究论文使用较小的图像尺寸，损害了特征的丰富性，因此需要一种新的方法。我们阐明了像素密度在保留基本细节方面的关键作用，特别是对于早期野火检测。通过在训练期间仔细选择滤波器的数量，我们强调了更高的像素密度对于正确的特征选择的重要性。所提出的 SegNet 方法使用无人机飞行获得的真实世界数据集进行严格评估，并与最先进的文献进行比较。]]></description>
      <guid>https://arxiv.org/abs/2405.00031</guid>
      <pubDate>Thu, 02 May 2024 06:18:20 GMT</pubDate>
    </item>
    <item>
      <title>利用预训练的 CNN 在水稻叶部病害分类中进行高效特征提取</title>
      <link>https://arxiv.org/abs/2405.00025</link>
      <description><![CDATA[arXiv:2405.00025v1 公告类型：新
摘要：水稻病害分类是农业研究中的一项关键任务，在这项研究中，我们严格评估了在预训练的卷积神经网络（CNN）中集成特征提取方法的影响。对没有特征提取的基线模型的初步研究显示，ResNet-50 和 ResNet-101 的性能值得称赞，分别达到 91% 和 92% 的准确率。随后集成定向梯度直方图 (HOG) 在各个架构中产生了显着的改进，特别是将 EfficientNet-B7 的准确率从 92% 提升到了令人印象深刻的 97%。相反，局部二进制模式（LBP）的应用表现出更保守的性能增强。此外，采用梯度加权类激活映射 (Grad-CAM) 揭示了 HOG 集成导致了对疾病特定特征的高度关注，证实了观察到的性能增强。视觉表现进一步验证了 HOG 的显着影响，显示由于对受疾病影响的区域的关注，跨时代的准确性明显激增。这些结果强调了特征提取（尤其是 HOG）在细化表示和提高分类准确性方面的关键作用。该研究的显着亮点是采用 HOG 和 Grad-CAM 的 EfficientNet-B7 实现了 97% 的准确率，这是优化基于 CNN 的预训练水稻病害识别系统的显着进步。研究结果提倡将先进的特征提取技术与先进的预训练 CNN 架构进行战略整合，为大幅提高农业环境中基于图像的疾病分类系统的精度和有效性提供了一条有前途的途径。]]></description>
      <guid>https://arxiv.org/abs/2405.00025</guid>
      <pubDate>Thu, 02 May 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>光谱光场成像的多维压缩感知</title>
      <link>https://arxiv.org/abs/2405.00027</link>
      <description><![CDATA[arXiv:2405.00027v1 公告类型：新
摘要：本文考虑了一种压缩多光谱光场相机模型，该模型利用单热光谱编码掩模和微透镜阵列，使用单个单色传感器捕获空间、角度和光谱信息。我们提出了一种模型，采用压缩传感技术从欠采样测量中重建完整的多光谱光场。与之前将光场矢量化为一维信号的工作不同，我们的方法采用 5D 基础和新颖的 5D 测量模型，因此匹配多光谱光场的固有维度。我们从数学和经验上证明了 5D 和 1D 传感模型的等效性，最重要的是，5D 框架实现了几个数量级的快速重建，同时只需要一小部分内存。此外，我们新的多维传感模型为设计高效的视觉数据采集算法和硬件开辟了新的研究方向。]]></description>
      <guid>https://arxiv.org/abs/2405.00027</guid>
      <pubDate>Thu, 02 May 2024 06:18:19 GMT</pubDate>
    </item>
    <item>
      <title>SIMPLOT：通过提炼要点来增强图表问答能力</title>
      <link>https://arxiv.org/abs/2405.00021</link>
      <description><![CDATA[arXiv:2405.00021v1 公告类型：新
摘要：最近，由于视觉语言模型的发展，用逻辑推理解释复杂的图表已成为挑战。先前最先进的 (SOTA) 模型 Deplot 提出了一种端到端方法，该方法利用视觉语言模型将图表转换为表格格式，并利用大型语言模型 (LLM) 进行推理。然而，与自然图像不同，图表包含图表推理所需的基本信息和不相关信息的混合，我们发现此特征会降低图表到表格提取的性能。在本文中，我们介绍了 SIMPLOT，一种旨在仅提取图表推理所需元素的方法。所提出的方法涉及两个步骤：1）训练以模仿一个简单的图，该图仅包含复杂图表中用于表格提取的基本信息，然后 2）根据表格进行推理。我们的模型无需额外的注释或数据集即可实现准确的图表推理，并且其有效性已通过各种实验得到证明。此外，我们提出了一种新颖的提示，解决了最近 SOTA 模型忽略颜色等视觉属性的缺点。我们的源代码可在 https://github.com/sangwu99/Simplot 获取。]]></description>
      <guid>https://arxiv.org/abs/2405.00021</guid>
      <pubDate>Thu, 02 May 2024 06:18:18 GMT</pubDate>
    </item>
    <item>
      <title>彻底改变零售分析：利用人工智能推进库存和客户洞察</title>
      <link>https://arxiv.org/abs/2405.00023</link>
      <description><![CDATA[arXiv:2405.00023v1 公告类型：新
摘要：为了应对零售业面临的重大挑战，包括低效的队列管理、糟糕的需求预测和无效的营销，本文介绍了一种利用尖端机器学习技术的创新方法。我们的目标是创建先进的智能零售分析系统（SRAS），利用这些技术来提高零售效率和客户参与度。为了增强客户跟踪能力，提出了一种集成多个预测模型的新混合架构。在所提出的客户跟踪混合架构的第一阶段，我们使用一组不同的参数对 YOLOV8 算法进行了微调，在各种性能指标上取得了优异的结果。这一微调过程利用了零售环境中的实际监控录像，确保了其实际适用性。在第二阶段，我们探索将两个复杂的对象跟踪模型 BOT-SORT 和 ByteTrack 与 YOLOV8 检测到的标签集成。这种集成对于跟踪商店内的客户路径至关重要，有助于创建准确的访客计数和热图。这些见解对于了解消费者行为和改善商店运营非常宝贵。为了优化库存管理，我们深入研究了各种预测模型，优化其性能并将其与复杂的零售数据模式进行对比。 GRU 模型能够解释具有长期时间依赖性的时间序列数据，始终优于线性回归等其他模型，R2 分数和 mAPE 分别提高了 2.873% 和 29.31%。]]></description>
      <guid>https://arxiv.org/abs/2405.00023</guid>
      <pubDate>Thu, 02 May 2024 06:18:18 GMT</pubDate>
    </item>
    </channel>
</rss>