<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Mon, 25 Dec 2023 03:14:50 GMT</lastBuildDate>
    <item>
      <title>基于自动编码器的人脸验证系统。 （arXiv：2312.14301v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14301</link>
      <description><![CDATA[这项工作的主要目标是提出一种替代方法
旨在减少对标记数据的依赖。我们提出的方法涉及
在人脸图像识别任务中利用自动编码器预训练
两步过程。最初，自动编码器在无监督的环境中进行训练
使用大量未标记的训练数据集的方式。随后，
使用来自的初始化参数来训练深度学习模型
预训练的自动编码器。这个深度学习训练过程是在
监督方式，采用相对有限的标记训练数据集。
在评估阶段，面部图像嵌入作为以下输出生成：
深层神经网络层。我们的训练是在 CelebA 数据集上执行的，
同时使用基准人脸识别数据集进行评估，例如
野外标记面孔 (LFW) 和 YouTube 面孔 (YTF)。实验结果
证明通过用预训练初始化深度神经网络
自动编码器参数达到了与最先进的方法相当的结果。
]]></description>
      <guid>http://arxiv.org/abs/2312.14301</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:50 GMT</pubDate>
    </item>
    <item>
      <title>PlatoNeRF：通过单视图二反射激光雷达对柏拉图洞穴进行 3D 重建。 （arXiv：2312.14239v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14239</link>
      <description><![CDATA[由于模糊性，从单一视图进行 3D 重建具有挑战性
来自单眼线索和缺乏有关遮挡区域的信息。神经
辐射场 (NeRF)，虽然在视图合成和 3D 重建中很受欢迎，
通常依赖于多视图图像。现有的单视图 3D 方法
NeRF 重建依赖于幻觉视图之前的数据
遮挡区域（物理上可能不准确）或观察到的阴影
RGB 相机，在环境光和低反照率下很难检测到
背景。我们建议使用单光子捕获的飞行时间数据
雪崩二极管克服了这些限制。我们的方法模拟两次反弹
NeRF 的光路，使用激光雷达瞬态数据进行监控。经过
利用 NeRF 和激光雷达测量的两次反射光的优点，
我们证明我们可以重建可见和遮挡的几何体，而无需
数据先验或对受控环境照明或场景反照率的依赖。在
此外，我们证明了在实际约束下改进的泛化能力
传感器空间和时间分辨率。我们相信我们的方法是有前途的
随着单光子激光雷达在消费设备上变得无处不在，例如
如手机、平板电脑和耳机。
]]></description>
      <guid>http://arxiv.org/abs/2312.14239</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:49 GMT</pubDate>
    </item>
    <item>
      <title>通过高斯过程模糊效应的细粒度预测模型。 （arXiv：2312.14280v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.14280</link>
      <description><![CDATA[由于复杂因素的存在，时间序列预测是一项具有挑战性的任务
和动态时间依赖性。这可能会导致错误的预测
即使是最好的预测模型。使用更多训练数据是一种方法
提高准确性，但这种来源通常是有限的。相比之下，我们是
通过倡导建立成功的图像生成去噪方法
用于端到端预测和去噪范例。

我们提出了一个端到端的预测模糊去噪预测框架
鼓励预测和去噪之间的分工
楷模。最初的预测模型旨在准确地关注
预测粗粒度行为，而降噪模型则侧重于
通过集成捕获局部模糊的细粒度行为
高斯过程模型。所有三个部分都相互作用以实现最佳的端到端
表现。我们广泛的实验表明我们提出的方法
能够提高几种最先进的预测精度
预测模型以及其他几种去噪方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.14280</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:49 GMT</pubDate>
    </item>
    <item>
      <title>用于突发图像融合和层分离的神经样条场。 （arXiv：2312.14235v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14235</link>
      <description><![CDATA[图像连拍中的每张照片都可以被视为复杂 3D 的样本
场景：视差、漫反射和镜面材质、场景运动的产物，
和光源变化。从堆栈中分解所有这些效果时
未对齐的图像是一项高度病态的任务，传统的
对齐和合并突发管道采取另一个极端：将它们混合成一个
单个图像。在这项工作中，我们提出了一种通用的中间表示：
使用神经网络构建的两层 alpha 合成图像加流模型
样条线字段——经过训练将输入坐标映射到样条线控制的网络
点。我们的方法能够在测试时优化期间联合融合
将突发图像捕捉为一个高分辨率重建并将其分解
分为传输层和阻塞层。然后，通过丢弃障碍物
层，我们可以执行一系列任务，包括看穿遮挡物，
反射抑制和阴影去除。经过复杂合成和验证
我们发现，在野外捕获的情况下，没有任何后处理步骤或学习
先验，我们的泛化模型能够超越现有的专用模型
单图像和多视图障碍物去除方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.14235</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:48 GMT</pubDate>
    </item>
    <item>
      <title>InternVL：扩大视觉基础模型并调整通用视觉语言任务。 （arXiv：2312.14238v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14238</link>
      <description><![CDATA[大型语言模型 (LLM) 的指数级增长开辟了无数
多模式 AGI 系统的可能性。然而，视觉和视觉上的进步
视觉语言基础模型，这也是关键要素
多模态 AGI 尚未跟上法学硕士的步伐。在这项工作中，我们设计了一个
大规模视觉语言基础模型（InternVL），它扩大了
视觉基础模型包含 60 亿个参数并逐步调整
通过大语言模型，使用来自各种网络规模的图像文本数据
来源。该模型可以广泛应用于并实现state-of-the-art
图像级或像素级等视觉感知任务的性能
识别、视觉语言任务，例如零样本图像/视频
分类、零样本图像/视频文本检索，并与法学硕士链接
创建多模式对话系统。我们希望我们的研究能够做出贡献
多模式大型模型的开发。代码和型号可用
位于 https://github.com/OpenGVLab/InternVL。
]]></description>
      <guid>http://arxiv.org/abs/2312.14238</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:48 GMT</pubDate>
    </item>
    <item>
      <title>Parrot 字幕教 CLIP 识别文本。 （arXiv：2312.14232v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14232</link>
      <description><![CDATA[尽管 CLIP 是众多视觉语言的基础模型
在应用程序中，CLIP 存在严重的文本识别偏差。这样的偏见
导致 CLIP 模型“模仿”嵌入图像中的视觉文本，同时
忽视真实的视觉语义。我们发现，在最
热门图文数据集LAION-2B，字幕也密密麻麻鹦鹉学舌（咒语）
嵌入图像中的文本。我们的分析表明，大约 \textbf{50\%} 的
图像中嵌入了视觉文本内容，其 \textbf{90\%}
字幕或多或少地模仿了视觉文本。基于这样的观察，我们
彻底检查 CLIP 模型的不同版本 d 并验证
视觉文本是衡量 LAION 风格的主导因素
这些模型的图像文本相似度。检查这些鹦鹉是否
字幕塑造了文本识别偏差，我们训练了一系列 CLIP 模型
LAION 子集由不同的鹦鹉字幕导向标准管理。我们展示
使用鹦鹉字幕进行训练很容易形成这种偏见，但却会损害
CLIP 模型中预期的视觉语言表示学习。这表明
迫切需要重新审视类 CLIP 模型的设计或
基于 CLIP 分数过滤构建的现有图像文本数据集管理管道。
]]></description>
      <guid>http://arxiv.org/abs/2312.14232</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:47 GMT</pubDate>
    </item>
    <item>
      <title>VCoder：用于多模式大语言模型的多功能视觉编码器。 （arXiv：2312.14233v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14233</link>
      <description><![CDATA[人类拥有非凡的视觉感知能力，能够看到
并理解所见，帮助他们理解视觉世界，
转，道理。多模态大语言模型（MLLM）最近取得了
在视觉语言任务上表现出色，从视觉到语言
从问答和图像字幕到视觉推理和图像
一代。然而，当提示识别或计数（感知）实体时
在给定的图像中，现有的 MLLM 系统会失败。致力于开发一个
准确的 MLLM 系统用于感知和推理，我们建议使用 Versatile
视觉编码器（VCoder）作为多模式法学硕士的感知眼睛。我们喂养
具有分割或深度图等感知模式的 VCoder，改进了
MLLM 的感知能力。其次，我们利用 COCO 的图像和
现成的视觉感知模型的输出来创建我们的 COCO
用于训练和评估 MLLM 的分割文本 (COST) 数据集
物体感知任务。第三，我们引入评估对象的指标
我们的 COST 数据集上 MLLM 的感知能力。最后，我们提供广泛的
实验证据证明 VCoder 改进了对象级感知
超过现有多模式法学硕士（包括 GPT-4V）的技能。我们开源我们的
促进研究的数据集、代码和模型。我们将代码开源于
https://github.com/SHI-Labs/VCoder
]]></description>
      <guid>http://arxiv.org/abs/2312.14233</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:47 GMT</pubDate>
    </item>
    <item>
      <title>DreamDistribution：文本到图像扩散模型的快速分布学习。 （arXiv：2312.14216v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14216</link>
      <description><![CDATA[文本到图像（T2I）扩散模型的普及使得
从文本描述生成高质量图像。然而，生成
具有参考视觉属性的多样化定制图像仍然具有挑战性。
这项工作的重点是更抽象地个性化 T2I 扩散模型
概念或类别级别，从一组参考中调整共性
图像，同时创建具有足够变化的新实例。我们介绍一个
该解决方案允许预训练的 T2I 扩散模型学习一组软
提示，通过对提示进行采样来生成新颖的图像
学习了分布。这些提示提供文本引导编辑功能和
在控制多个变量之间的变化和混合方面具有额外的灵活性
分布。我们还展示了学习到的提示分布的适应性
其他任务，例如文本转 3D。最后我们展示了我们的有效性
通过定量分析的方法，包括自动评估和人工评估
评估。项目网站：https://briannlongzhao.github.io/DreamDistribution
]]></description>
      <guid>http://arxiv.org/abs/2312.14216</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:46 GMT</pubDate>
    </item>
    <item>
      <title>自动增强输入转换以实现高度可转移的针对性攻击。 （arXiv：2312.14218v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14218</link>
      <description><![CDATA[人们普遍认为深度神经网络 (DNN) 容易受到
对抗性例子，其中添加难以察觉的扰动来清洁
通过各种输入转换攻击的示例。然而，这些方法
最初设计用于非目标攻击，但在以下情况下成功率较低
有针对性的攻击。近期定向对抗攻击主要关注
梯度优化，试图找到合适的扰动方向。
然而，他们中很少有人致力于输入转换。在这项工作中，我们
观察目标的 logit/概率之间的正相关性
针对性攻击中的类和多样化的输入转换方法。对此
最后，我们提出了一种新颖的有针对性的对抗性攻击，称为 AutoAugment 输入
转型（AAIT）。 AAIT 不依赖手工制定策略，
从变换空间中搜索最优变换策略
包括各种操作。然后，AAIT 使用以下方法制作对抗性示例
找到了最佳的转型策略来提高对抗性可转移性
有针对性的攻击。在 CIFAR-10 和
ImageNet 兼容数据集表明所提出的 AAIT 优于其他
基于转移的针对性攻击显着。
]]></description>
      <guid>http://arxiv.org/abs/2312.14218</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:46 GMT</pubDate>
    </item>
    <item>
      <title>用于快捷方式删除和生成的基于快速扩散的反事实。 （arXiv：2312.14223v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14223</link>
      <description><![CDATA[捷径学习是当一个模型——例如心脏病分类器——
利用目标标签和虚假快捷方式特征之间的相关性，
例如起搏器，根据快捷方式而不是预测目标标签
真正的歧视性特征。这在医学成像中很常见，其中
治疗和临床注释与疾病标签相关，使它们
预测疾病的简单捷径。我们提出了一种新颖的检测和
通过快速量化潜在快捷方式功能的影响
基于扩散的反事实图像生成，可以综合去除
或添加快捷方式。通过一种新颖的基于修复的修改，我们在空间上进行了限制
无需额外的推理步骤即可进行更改，鼓励删除
空间受限的快捷方式功能，同时确保无快捷方式
反事实在很大程度上保留了其剩余的图像特征。使用
这些，我们评估快捷特征如何影响模型预测。

这是通过我们的第二个贡献实现的：基于高效扩散的
反事实解释方法，推理加速显着
与最先进的图像质量相当。我们在两个大型项目上证实了这一点
胸部 X 射线数据集、皮肤病变数据集和 CelebA。
]]></description>
      <guid>http://arxiv.org/abs/2312.14223</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:46 GMT</pubDate>
    </item>
    <item>
      <title>LLM4VG：视频接地的大型语言模型评估。 （arXiv：2312.14206v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14206</link>
      <description><![CDATA[最近，研究人员试图调查法学硕士在以下方面的能力：
处理视频并提出了几种视频LLM模型。然而，有能力
法学硕士处理视频接地（VG），这是一个重要的时间相关视频
要求模型精确定位开始和结束时间戳的任务
视频中与给定文本查询匹配的时间时刻仍然存在
文献中尚不清楚且未经探索。为了填补这一空白，在本文中，我们
提出LLM4VG基准，系统地评估
视频基础任务上的不同法学硕士。基于我们提出的 LLM4VG，我们
设计大量实验来检查视频上的两组视频 LLM 模型
基础：（i）在文本视频对上训练的视频法学硕士（表示为
VidLLM），以及（ii）与预训练视觉描述模型相结合的法学硕士
例如视频/图像字幕模型。我们提出了快速方法
整合了VG的指令和不同类型的描述
生成器，包括用于直接视觉描述的基于标题的生成器
以及基于 VQA 的信息增强生成器。我们还提供
综合比较各种VidLLM并探讨其影响
视觉模型、法学硕士、提示设计等的不同选择。我们的
实验评估得出两个结论：(i) 现有的 VidLLM 是
离达到令人满意的视频接地性能还很远，并且
应包含更多与时间相关的视频任务，以进一步微调这些任务
模型，以及（ii）法学硕士和视觉模型的结合显示了初步的
视频接地的能力有很大的改进潜力
依靠更可靠的模型和进一步的及时指示指导。
]]></description>
      <guid>http://arxiv.org/abs/2312.14206</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:45 GMT</pubDate>
    </item>
    <item>
      <title>TextFusion：揭示文本语义用于可控图像融合的力量。 （arXiv：2312.14209v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14209</link>
      <description><![CDATA[先进的图像融合方法致力于通过以下方式生成融合结果：
聚合源图像传达的补充信息。
然而，成像源特定表现的差异
场景内容使得设计鲁棒可控的融合变得困难
过程。我们认为这个问题可以通过以下方式得到缓解：
由文本模态传达的更高层次的语义，这应该使我们能够
生成用于不同目的的融合图像，例如可视化和
以可控的方式完成下游任务。这是通过利用
视觉和语言模型构建从粗到细的关联机制
文本和图像信号之间。在关联地图的指引下，
变压器网络中嵌入仿射融合单元来融合文本
和特征级别的视觉模式。作为这项工作的另一个组成部分，
我们建议使用文本注意来调整图像质量评估
融合任务。为了促进拟议的文本指导的实施
融合范式，以及它被更广泛的研究界采用，我们发布了
文本注释图像融合数据集 IVT。大量的实验表明
我们的方法（TextFusion）始终优于传统的基于外观的方法
融合方法。我们的代码和数据集将在该项目上公开
主页。
]]></description>
      <guid>http://arxiv.org/abs/2312.14209</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:45 GMT</pubDate>
    </item>
    <item>
      <title>通过双层数据修剪进行高效架构搜索。 （arXiv：2312.14200v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14200</link>
      <description><![CDATA[提高神经架构搜索（NAS）的效率是一项具有挑战性的工作
但这是一项备受关注的重大任务。以前的作品主要
采用可微架构搜索（DARTS）并改进其搜索
提高搜索效率的策略或模块。最近，一些方法
开始考虑减少数据以提高速度，但它们并不紧密
再加上架构搜索过程，导致次优
表现。为此，这项工作开创了对关键问题的探索
数据集特征对于 DARTS 双层优化的作用，然后
提出了一种针对权重的新颖的双层数据修剪（BDP）范式
DARTS的架构层面，从数据角度提升效率。
具体来说，我们引入了一种新的渐进式数据修剪策略
利用超网预测动态作为度量，逐步剪枝
搜索过程中发现不适合 DARTS 的样本。有效的自动化课程
平衡约束也被集成到BDP中，以抑制潜在的阶级
数据高效算法导致的不平衡。综合评价
在 NAS-Bench-201 搜索空间、DARTS 搜索空间和类似 MobileNet 上
搜索空间验证 BDP 将搜索成本降低了 50% 以上，同时实现
应用于基线 DARTS 时具有卓越的性能。此外，我们还展示了
BDP 可以与先进的 DARTS 变体（例如 PC-DARTS）和谐地集成
和 \b{eta}-DARTS，以最小的速度提供大约 2 倍的加速
性能妥协。
]]></description>
      <guid>http://arxiv.org/abs/2312.14200</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:44 GMT</pubDate>
    </item>
    <item>
      <title>通过展开和征服归因指导更好地可视化网络的决策基础。 （arXiv：2312.14201v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14201</link>
      <description><![CDATA[揭示深度神经网络 (DNN) 的透明度已得到广泛关注
研究描述网络内部结构的决策机制。在
在本文中，我们提出了一个新颖的事后框架，Unfold and Conquer
归因指导（UCAG），增强网络的可解释性
通过对模型的输入特征进行空间审查来做出决策
信心。解决缺少详细描述的现象，UCAG
依次符合图像切片的置信度，从而导致
提供了丰富而清晰的解释。因此，可以
通过保留细节来增强解释的表达能力
辅助输入功能的描述，这些功能通常被
主要有意义的区域。我们进行了大量的评估来验证
多个指标的性能：i) 删除和插入，ii)（基于能量）
指点游戏，以及 iii) 正负密度图。实验性的
结果，包括定性比较，表明我们的方法
优于现有方法，具有清晰详细的性质
解释和适用性。
]]></description>
      <guid>http://arxiv.org/abs/2312.14201</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:44 GMT</pubDate>
    </item>
    <item>
      <title>ZeroShape：基于回归的零样本形状重建。 （arXiv：2312.14198v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.14198</link>
      <description><![CDATA[我们研究单图像零样本 3D 形状重建问题。
最近的作品通过生成建模学习零样本形状重建
3D 资产，但这些模型在训练和训练时的计算成本很高
推理时间。相比之下，解决这个问题的传统方法是
基于回归，其中确定性模型被训练为直接回归
物体形状。这种回归方法具有更高的计算能力
效率高于生成方法。这就提出了一个自然的问题：是
高性能所需的生成建模，或者相反，是
基于回归的方法仍然有竞争力吗？为了回答这个问题，我们设计了一个
基于强回归的模型，称为 ZeroShape，基于收敛
该领域的发现和新颖的见解。我们还策划了一个大型的现实世界
评估基准，使用来自三个不同现实世界 3D 数据集的对象。
这个评估基准更加多样化，数量级比
先前的工作使用什么来定量评估他们的模型，旨在
减少我们领域的评估差异。我们证明 ZeroShape 不仅
实现了优于最先进方法的性能，而且
表现出显着更高的计算和数据效率。
]]></description>
      <guid>http://arxiv.org/abs/2312.14198</guid>
      <pubDate>Mon, 25 Dec 2023 03:14:43 GMT</pubDate>
    </item>
    </channel>
</rss>