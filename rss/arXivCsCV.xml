<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Thu, 11 Jan 2024 06:18:32 GMT</lastBuildDate>
    <item>
      <title>基于不一致的以数据为中心的主动开放集注释。 （arXiv：2401.04923v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2401.04923</link>
      <description><![CDATA[主动学习是一种常用的方法，可以减少标记工作
训练深度神经网络所需的。然而，目前的有效性
主动学习方法受到其封闭世界假设的限制，
假设未标记池中的所有数据都来自一组预定义的已知数据
类。这种假设在实际情况中通常是无效的，因为
未标记数据中可能是未知类，导致活动开集
注释问题。数据中未知类别的存在可以
由于以下原因，显着影响现有主动学习方法的性能
他们带来的不确定性。为了解决这个问题，我们提出了一个小说
以数据为中心的主动学习方法，称为 NEAT，可主动注释
开集数据。 NEAT 旨在标记来自两者池中的已知类别数据
已知和未知类未标记数据。它利用了集群性
标签来识别未标记池中的已知类别并选择
基于一致性标准的这些类别的信息样本
衡量模型预测和局部特征之间的不一致
分配。与最近提出的以学习为中心的方法不同
问题，NEAT 的计算效率更高，并且是以数据为中心的
主动开集注释方法。我们的实验证明 NEAT
取得比最先进的主动学习显着更好的性能
主动开放集注释的方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.04923</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:31 GMT</pubDate>
    </item>
    <item>
      <title>视频中的延迟感知道路异常分割：真实感数据集和新指标。 （arXiv：2401.04942v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04942</link>
      <description><![CDATA[在过去的几年里，道路异常分割在以下领域得到了积极的探索：
学术界和业界日益关注。理由
后面的事情很简单：如果自动驾驶汽车能够在撞上障碍物之前刹车
异常物体，安全性得到提升。然而，这个理由自然而然地称为
对于暂时知情的环境，而现有的方法和基准是
以不切实际的框架方式设计。为了弥合这一差距，我们做出了贡献
第一个用于自动驾驶的视频异常分割数据集。自从
将各种异常物体放置在繁忙的道路上，并在每个
框架既危险又昂贵，我们求助于合成数据。为了改善
为了了解该合成数据集与现实世界应用的相关性，我们训练了一个
以渲染 G 缓冲区为条件的生成对抗网络
照片真实感增强。我们的数据集包含 120,000 个高分辨率
帧速率为 60 FPS，记录于 7 个不同的城镇。作为初始
基准测试，我们使用最新的监督和无监督提供基线
道路异常分割方法。除了传统的，我们还专注于
两个新指标：时间一致性和延迟感知流准确性。我们
相信后者是有价值的，因为它衡量是否存在异常分割
算法可以真正防止汽车在临时通知的情况下发生碰撞
环境。
]]></description>
      <guid>http://arxiv.org/abs/2401.04942</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:31 GMT</pubDate>
    </item>
    <item>
      <title>用于行人轨迹预测的知识感知图转换器。 （arXiv：2401.04872v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04872</link>
      <description><![CDATA[预测行人运动轨迹对于路径规划和
自动驾驶车辆的运动控制。准确预测人群
由于人类运动的不确定性，轨迹的确定具有挑战性
不同的环境。对于训练，最近基于深度学习的预测
方法主要利用轨迹历史和交互等信息
行人之间等等。这会限制预测性能
由于训练数据集之间的差异，跨各种场景
没有被正确纳入。为了克服这个限制，本文
提出了一种图变换器结构来提高预测性能，
捕捉其中包含的各个站点和场景之间的差异
数据集。特别是自注意力机制和领域适应
设计了模块以提高模型的泛化能力。
此外，考虑跨数据集序列的附加指标是
出于培训和绩效评估目的而引入。拟议的
使用流行的方法对框架进行验证并与现有方法进行比较
公共数据集，即 ETH 和 UCY。实验结果表明
改进了我们提出的方案的性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.04872</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:30 GMT</pubDate>
    </item>
    <item>
      <title>SnapCap：高效的快照压缩视频字幕。 （arXiv：2401.04903v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04903</link>
      <description><![CDATA[视频字幕 (VC) 是一项具有挑战性的多模式任务，因为它需要
通过理解各种复杂的视频，用语言描述场景。
对于机器来说，传统的VC遵循
“成像-压缩-解码-然后字幕”管道，其中压缩
是存储和传输的枢轴。然而，在这样的管道中，一些
潜在的缺点是不可避免的，即导致信息冗余
采样过程中效率低下且信息丢失
字幕。为了解决这些问题，在本文中，我们提出了一种新的 VC
直接从压缩测量生成字幕的管道，
可以通过快照压缩传感相机捕获，我们将我们的模型称为
SnapCap。更具体地说，受益于信号模拟，我们有
获得丰富的测量-视频-注释数据对
模型。此外，为了更好地从中提取与语言相关的视觉表示
压缩测量，我们建议通过以下方式从视频中提取知识
一个经过预先训练的 CLIP，具有丰富的语言-视觉关联来指导
学习我们的 SnapCap。为了证明 SnapCap 的有效性，我们
在两个广泛使用的 VC 数据集上进行实验。无论是质量还是
定量结果验证了我们的管道相对于传统管道的优越性
VC 管道。特别是与“重建后的标题”相比
方法，我们的 SnapCap 运行速度至少可以快 3$\times$，并且实现更好
字幕结果。
]]></description>
      <guid>http://arxiv.org/abs/2401.04903</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:30 GMT</pubDate>
    </item>
    <item>
      <title>基于扩散的姿势细化和 3D 人体姿势估计的多假设生成。 （arXiv：2401.04921v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04921</link>
      <description><![CDATA[之前的 3D 人体姿势估计 (3DHPE) 概率模型旨在
通过生成多个假设来提高姿势准确性。然而，大多数
生成的假设与真实姿势有很大偏差。相比
确定性模型中，概率模型的过度不确定性导致
单一假设预测的性能较弱。为了解决这两个问题
挑战，我们提出了一个基于扩散的细化框架，称为 DRPose，
它通过反向扩散和细化确定性模型的输出
对当前位姿实现更合适的多假设预测
通过多步细化和多种噪声进行基准测试。为此，我们
提出可扩展图卷积变换器（SGCT）和姿势细化
用于去噪和细化的模块（PRM）。在 Human3.6M 和
MPI-INF-3DHP 数据集证明我们的方法达到了最先进的水平
单假设和多假设 3DHPE 的性能。代码可在
https://github.com/KHB1698/DRPose。
]]></description>
      <guid>http://arxiv.org/abs/2401.04921</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:30 GMT</pubDate>
    </item>
    <item>
      <title>用于基于零样本草图的图像检索的模态感知表示学习。 （arXiv：2401.04860v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04860</link>
      <description><![CDATA[零样本学习为机器学习模型提供了有效的解决方案
处理看不见的类别，避免详尽的数据收集。零射击
基于草图的图像检索 (ZS-SBIR) 模拟现实场景，其中
收集配对的素描照片样本既困难又昂贵。我们提议写一本小说
通过对比草图和照片来间接对齐它们的框架
通过文本，消除了访问素描照片对的必要性。与
从数据中学习显式模态编码，我们的方法解开
来自特定于模态信息的模态不可知语义，桥接
模态差距并在一个范围内实现有效的跨模态内容检索
联合潜在空间。通过综合实验，我们验证了其功效
在ZS-SBIR上提出的模型，它也可以应用于广义和
细粒度的设置。
]]></description>
      <guid>http://arxiv.org/abs/2401.04860</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:29 GMT</pubDate>
    </item>
    <item>
      <title>CTNeRF：单目视频动态神经辐射场的跨时间变换器。 （arXiv：2401.04861v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04861</link>
      <description><![CDATA[我们工作的目标是从单眼生成高质量的新颖视图
复杂动态场景的视频。先前的方法，例如 DynamicNeRF，具有
通过利用时变动态辐射显示出令人印象深刻的性能
字段。然而，这些方法在准确度方面存在局限性。
对复杂物体的运动进行建模，这可能会导致不准确和模糊
细节效果图。为了解决这个限制，我们提出了一种新方法
它建立在最近的泛化 NeRF 之上，它聚合了附近的视图
到新的观点。然而，此类方法通常仅对以下情况有效：
静态场景。为了克服这一挑战，我们引入了一个运行的模块
在时域和频域上聚合对象的特征
运动。这使我们能够学习框架之间的关系并生成
更高质量的图像。我们的实验证明了显着的改进
超越动态场景数据集上最先进的方法。具体来说，我们的
该方法在准确性和视觉效果方面均优于现有方法
合成视图的质量。
]]></description>
      <guid>http://arxiv.org/abs/2401.04861</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:29 GMT</pubDate>
    </item>
    <item>
      <title>SOS-SLAM：非结构化环境中开放集 SLAM 的分割。 （arXiv：2401.04791v1 [cs.RO]）</title>
      <link>http://arxiv.org/abs/2401.04791</link>
      <description><![CDATA[我们提出了一种新颖的开放集同步本地化框架
非结构化环境中的地图绘制 (SLAM)，使用分段来创建
用于定位的对象地图和对象之间的几何关系。
我们的系统包括 1) 使用零样本的前端映射管道
分割模型从图像中提取对象掩模并跟踪它们
帧来生成基于对象的地图，以及 2) 帧对齐管道
利用对象的几何一致性在地图内高效定位
在各种条件下拍摄的。这种方法被证明更稳健
与传统基于特征的 SLAM 系统相比，照明和外观发生了变化
或全局描述符方法。这是通过评估 SOS-SLAM 建立的
Batvik 季节性数据集，其中包括在沿海地区收集的无人机飞行数据
芬兰南部不同季节和光照条件下的情节。
在不同环境条件下的飞行中，我们的方法实现了
比基准方法更高的召回率，精度为 1.0。 SOS-SLAM 定位
在参考地图中速度比其他基于特征的方法快 14 倍
地图大小不到最紧凑的其他地图大小的 0.4%。什么时候
从不同的角度考虑本地化性能，我们的方法
从相同的角度来看，优于所有基准测试，并且优于来自以下方面的大多数基准测试：
不同的观点。 SOS-SLAM 是一种很有前景的 SLAM 新方法
对照明和外观变化具有鲁棒性的非结构化环境
并且比其他方法的计算效率更高。我们发布我们的
代码和数据集：https://acl.mit.edu/SOS-SLAM/。
]]></description>
      <guid>http://arxiv.org/abs/2401.04791</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:28 GMT</pubDate>
    </item>
    <item>
      <title>使用 CKA 和经验方法改进远程光电体积描记法架构。 （arXiv：2401.04801v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04801</link>
      <description><![CDATA[模型架构细化是深度学习研究中一项具有挑战性的任务
远程光电体积描记法（rPPG）等领域。一座建筑
考虑因素，模型的深度，可能会对
由此产生的性能。在过度配置更多层的 rPPG 模型中
存在不必要的冗余，消除冗余可以导致更快的速度
训练并减少推理时的计算负载。层数太少
模型可能会表现出次优的错误率。我们应用居中内核
与不同深度的 rPPG 架构阵列对齐（CKA），
证明较浅的模型不能学习与
更深的模型，达到一定深度后，添加冗余层
无需显着增加功能。一项实证研究证实
这些发现并展示了如何使用该方法来改进 rPPG
架构。
]]></description>
      <guid>http://arxiv.org/abs/2401.04801</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:28 GMT</pubDate>
    </item>
    <item>
      <title>在不受控制的农场环境中基于高光谱成像的黑莓果实成熟度检测的卷积神经网络集成学习。 （arXiv：2401.04748v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04748</link>
      <description><![CDATA[几十年来，水果成熟度估计模型一直依赖于光谱指数
特征或基于颜色的特征，例如平均值、标准差、偏度、
用于学习水果成熟度特征的颜色时刻和/或直方图。
最近，很少有研究探索使用深度学习技术来
从具有可见成熟度线索的水果图像中提取特征。但是，那
黑莓 (Rubus fruticosus) 果实没有表现出明显且可靠的可见
成熟时的成熟性状，因此对结果造成很大困难
采摘者。成熟的黑莓，对于人眼来说，之前、期间和之后都是黑色的。
后熟。为了解决这一工程应用挑战，本文
提出了一种新颖的多输入卷积神经网络（CNN）集成
用于检测黑莓果实成熟度细微特征的分类器。这
多输入 CNN 由预训练的 16 层视觉几何组创建
在 ImageNet 数据集上训练的深度卷积网络 (VGG16) 模型。这
全连接层针对学习成熟的成熟特征进行了优化
黑莓水果。由此产生的模型作为构建的基础
使用堆栈泛化进行集成的同质集成学习器
集成（SGE）框架。网络的输入是通过
使用可见光和近红外 (VIS-NIR) 光谱滤光片的立体传感器
波长为 700 nm 和 770 nm。通过实验，提出的模型
在未见过的场景中实现了 95.1% 的准确率，在现场实现了 90.2% 的准确率
状况。进一步的实验表明，机器感官是高度和
与人类对黑莓果皮纹理的感觉呈正相关。
]]></description>
      <guid>http://arxiv.org/abs/2401.04748</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:27 GMT</pubDate>
    </item>
    <item>
      <title>DedustNet：基于频率主导的 Swin 变压器的小波网络，用于农业除尘。 （arXiv：2401.04750v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04750</link>
      <description><![CDATA[虽然灰尘显着影响自动化的环境感知
农业机械，现有基于深度学习的灰尘方法
去除需要在这方面进一步研究和改进，以提高
农业自动化农业机械的性能和可靠性。
我们提出了一个端到端的可训练学习网络（DedustNet）来解决
现实世界的农业除尘任务。据我们所知，DedustNet 是
首次在小波网络中使用基于 Swin Transformer 的单元
农业形象喷粉。具体来说，我们提出了频率主导的
通过添加空间特征来块（DWTFormer块和IDWTFormer块）
Swin Transformer 的聚合方案（SFAS）并将其与
小波变换、DWTFormer 块和 IDWTFormer 块，减轻了
Swin Transformer 处理时全局感受野的限制
复杂的尘土飞扬的背景。此外，我们还提出了一个跨层次的信息
融合模块，融合不同层次的特征并有效捕获
全局和远程特征关系。此外，我们还提出了一个扩张的
卷积模块捕获小波引导的上下文信息
多尺度变换，结合了小波的优点
变换和扩张卷积。我们的算法利用深度学习
有效去除图像灰尘同时保留原始图像的技术
结构和纹理特征。与现有最先进技术相比
方法，DedustNet 在以下方面取得了卓越的性能和更可靠的结果
农业图像除尘，为应用提供有力支撑
多尘环境中的农业机械。此外，令人印象深刻的
真实世界模糊数据集和应用程序测试的性能亮点
DedustNet超强的泛化能力与计算机视觉相关
应用性能。
]]></description>
      <guid>http://arxiv.org/abs/2401.04750</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:27 GMT</pubDate>
    </item>
    <item>
      <title>用于功能磁共振成像研究中大脑提取的分段任何模型 (SAM)。 (arXiv:2401.04740v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2401.04740</link>
      <description><![CDATA[从磁共振中提取大脑并去除头骨文物
图像（MRI）是神经影像分析中重要的预处理步骤。那里
有许多工具被开发来处理人类功能磁共振成像图像，其中可能涉及
验证大脑分割结果的手动步骤，使其成为时间
消耗且低效。在本研究中，我们将使用段anything
model (SAM)，Meta[4] 发布的一个免费的神经网络，它具有
在许多通用分割应用中显示出有希望的结果。我们将
通过去除来分析 SAM 用于神经成像大脑分割的效率
头骨文物。实验结果显示出有希望的结果
探索使用自动分割算法进行神经成像，而无需
需要对自定义医学成像数据集进行训练。
]]></description>
      <guid>http://arxiv.org/abs/2401.04740</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vision Transformer 在基于皮肤镜的非侵入性数字系统中进行自动分析的皮肤癌分割和分类。 (arXiv:2401.04746v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2401.04746</link>
      <description><![CDATA[皮肤癌是一个全球性的健康问题，需要早期、准确的诊断
诊断以改善患者的治疗效果。这项研究引入了一项突破性的
皮肤癌分类方法，采用 Vision Transformer
最先进的深度学习架构以其在不同领域的成功而闻名
图像分析任务。精心利用10,015个HAM10000数据集
带注释的皮肤病变图像，模型经过预处理以增强
鲁棒性。适应皮肤癌分类的 Vision Transformer
任务，利用自注意力机制来捕获复杂的空间
依赖性，实现优于传统深度学习的性能
架构。 Segment Anything Model 有助于精确分割癌症
区域，获得高 IOU 和 Dice 系数。大量实验凸显
该模型的霸主地位，特别是基于 Google 的 ViT patch-32 变体，
其准确率达到 96.15%，展示了作为有效工具的潜力
皮肤科医生在皮肤癌诊断方面的贡献，为皮肤癌的进步做出了贡献
皮肤病学实践。
]]></description>
      <guid>http://arxiv.org/abs/2401.04746</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>DiffSHEG：一种基于扩散的实时语音驱动的整体 3D 表达和手势生成方法。 （arXiv：2401.04747v1 [cs.SD]）</title>
      <link>http://arxiv.org/abs/2401.04747</link>
      <description><![CDATA[我们提出了 DiffSHEG，一种基于扩散的语音驱动整体 3D 方法
任意长度的表情和手势生成。虽然之前的作品
专注于单独的共同语音手势或表情生成，联合
同步表情和手势的生成仍然很少被探索。到
解决这个问题，我们基于扩散的协同语音运动生成变压器
实现从表情到手势的单向信息流，
促进改进联合表情手势分布的匹配。
此外，我们引入了一种基于外画的任意采样策略
扩散模型中的长序列生成，提供灵活性和
计算效率。我们的方法提供了一个实用的解决方案
产生由驱动驱动的高质量同步表达和手势生成
演讲。在两个公共数据集上进行评估，我们的方法实现了
在数量和质量上都具有最先进的性能。
此外，一项用户研究证实了 DiffSHEG 相对于之前的技术的优越性
接近。通过实时生成富有表现力和同步的
动议，DiffSHEG 展示了其在各种应用中的潜力
数字人类和实体代理的发展。
]]></description>
      <guid>http://arxiv.org/abs/2401.04747</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>以内容为条件生成风格化的手绘草图。 （arXiv：2401.04739v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.04739</link>
      <description><![CDATA[近年来，手绘草图的识别仍然很流行
任务。但在一些特殊领域如军事领域，徒手
草图很难大规模采样。常见的数据增强和
图像生成技术很难生成具有各种特征的图像
徒手素描风格。因此，识别和分割任务
相关领域有限。在本文中，我们提出了一种新颖的对抗性
生成网络，可以准确生成逼真的手绘草图
具有各种风格。我们探索模型的性能，包括使用
从先验正态分布中随机采样的样式以生成图像
多种写意风格，解开画家风格
从已知的手绘草图生成具有特定风格的图像，以及
生成不在训练集中的未知类别的图像。我们
通过定性和定量评估进一步证明我们的
在视觉质量、内容准确性和风格模仿方面的优势
草图输入法。
]]></description>
      <guid>http://arxiv.org/abs/2401.04739</guid>
      <pubDate>Thu, 11 Jan 2024 06:18:25 GMT</pubDate>
    </item>
    </channel>
</rss>