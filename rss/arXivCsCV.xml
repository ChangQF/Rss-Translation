<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 18 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>迈向全面的多模态感知：引入触摸-语言-视觉数据集</title>
      <link>https://arxiv.org/abs/2403.09813</link>
      <description><![CDATA[arXiv:2403.09813v1 公告类型：新
摘要：触觉为人类和机器人的感知和交互能力提供了至关重要的支持和增强。然而，与触觉相关的多模态研究主要集中在视觉和触觉模态，在语言领域的探索有限。除了词汇之外，句子级描述还包含更丰富的语义。在此基础上，我们通过人机级联协作构建了一个名为 TLV（Touch-Language-Vision）的触摸语言视觉数据集，该数据集具有用于多模式对齐的句子级描述。新数据集用于微调我们提出的轻量级训练框架 TLV-Link（通过对齐链接触摸、语言和视觉），以最小的参数调整 (1%) 实现有效的语义对齐。项目页面：https://xiaoen0.github.io/touch.page/。]]></description>
      <guid>https://arxiv.org/abs/2403.09813</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:55 GMT</pubDate>
    </item>
    <item>
      <title>图像是对齐的致命弱点：利用视觉漏洞越狱多模态大型语言模型</title>
      <link>https://arxiv.org/abs/2403.09792</link>
      <description><![CDATA[arXiv:2403.09792v1 公告类型：新
摘要：在本文中，我们研究了多模态大型语言模型（MLLM）的无害对齐问题。我们对代表性 MLLM 的无害性能进行了系统的实证分析，并揭示了图像输入造成了 MLLM 的对齐漏洞。受此启发，我们提出了一种名为 HADES 的新颖越狱方法，该方法使用精心制作的图像隐藏并放大文本输入中恶意意图的危害性。实验结果表明，HADES可以有效越狱现有的MLLM，LLaVA-1.5的平均攻击成功率(ASR)为90.26%，Gemini Pro Vision的平均攻击成功率为71.60%。我们的代码和数据将公开发布。]]></description>
      <guid>https://arxiv.org/abs/2403.09792</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>2023 年 BOP 挑战赛，关于已见和未见刚性物体的检测、分割和姿态估计</title>
      <link>https://arxiv.org/abs/2403.09799</link>
      <description><![CDATA[arXiv:2403.09799v1 公告类型：新
摘要：我们介绍了 2023 年 BOP 挑战赛的评估方法、数据集和结果，这是一系列公共竞赛中的第五个竞赛，旨在捕捉基于模型的 6D 物体姿态估计（从 RGB/RGB-D 图像中获取最新技术）和相关任务。除了 2022 年的三项任务（基于模型的 2D 检测、2D 分割和训练期间看到的对象的 6D 定位）之外，2023 年的挑战还引入了这些任务的新变体，重点关注训练期间未看到的对象。在新任务中，需要方法在短暂的入门阶段（最多 5 分钟，1 个 GPU）从提供的 3D 对象模型中学习新对象。 2023 年未见物体 6D 定位的最佳方法 (GenFlow) 显着达到了 2020 年可见物体最佳方法 (CosyPose) 的准确度，但速度明显较慢。与 2022 年最佳对应方法 (GDRNPP) 相比，2023 年最佳可见物体方法 (GPose) 实现了适度的精度提升，但运行时间显着提高了 43%。自 2017 年以来，所见物体的 6D 定位精度提高了 50% 以上（从 56.9 AR_C 提高到 85.6 AR_C）。在线评估系统保持开放，可访问：http://bop.felk.cvut.cz/。]]></description>
      <guid>https://arxiv.org/abs/2403.09799</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>论 3D 手势在动作识别中的实用性</title>
      <link>https://arxiv.org/abs/2403.09805</link>
      <description><![CDATA[arXiv:2403.09805v1 公告类型：新
摘要：3D 手势是一种尚未充分探索的动作识别模式。姿势紧凑但信息丰富，可以极大地有利于计算预算有限的应用程序。然而，姿势本身无法完全理解动作，因为它们无法完全捕捉人类与之互动的物体和环境。为了有效地模拟手与物体的交互，我们提出了 HandFormer，一种新型的多模态转换器。 HandFormer 将用于细粒度运动建模的高时间分辨率 3D 手部姿势与用于编码场景语义的稀疏采样 RGB 帧相结合。观察手部姿势的独特特征，我们对手部建模进行时间分解，并通过其短期轨迹来表示每个关节。这种因式分解的姿态表示与稀疏 RGB 样本相结合非常高效并实现了高精度。仅具有手部姿势的 Unimodal HandFormer 的 FLOP 次数比现有的基于骨架的方法要好 5 倍。借助 RGB，我们在 Assembly101 和 H2O 上实现了最先进的新性能，并显着改进了以自我为中心的动作识别。]]></description>
      <guid>https://arxiv.org/abs/2403.09805</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:54 GMT</pubDate>
    </item>
    <item>
      <title>Shapley 价值观驱动的 GenAI 制作内容公平奖励分配框架</title>
      <link>https://arxiv.org/abs/2403.09700</link>
      <description><![CDATA[arXiv:2403.09700v1 公告类型：新
摘要：很明显，目前生成模型的质量已被人类专业人士超越。然而，随着人工智能的进步，这种差距将会缩小，导致那些花费了数年时间来掌握一项技能的个人由于成本高昂而被淘汰，而成本与他们完成一项技能所需的时间有着内在的联系。任务——人工智能可以在几分钟或几秒钟内完成的任务。为了避免未来的社会动荡，即使是现在，我们也必须考虑如何公平评估这些人在训练生成模型方面的贡献，以及如何补偿他们收入的减少或完全损失。在这项工作中，我们提出了一种构建模型开发人员和数据提供者之间协作的方法。为了实现这一目标，我们采用 Shapley 值来量化稳定扩散-v1.5 模型生成的图像中艺术家的贡献，并在他们之间公平分配奖励。]]></description>
      <guid>https://arxiv.org/abs/2403.09700</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>PICNIQ：自然图像质量评估的成对比较</title>
      <link>https://arxiv.org/abs/2403.09746</link>
      <description><![CDATA[arXiv:2403.09746v1 公告类型：新
摘要：盲图像质量评估（BIQA）方法虽然有望实现图像质量评估的自动化，但由于它们依赖于在不同图像中统一应用的通用质量标准，因此在现实场景中往往达不到要求。这种一刀切的方法忽视了图像内容和质量之间的关键感知关系，导致了“域转移”挑战，其中单一质量指标不足以代表各种内容类型。此外，BIQA 技术通常忽略了不同观察者之间人类视觉系统的固有差异。为了应对这些挑战，本文介绍了 PICNIQ，这是一种创新的成对比较框架，旨在通过强调相对而不是绝对的质量评估来绕过传统 BIQA 的局限性。 PICNIQ 专门设计用于评估图像对之间的质量差异。所提出的框架实现了精心设计的深度学习架构、专门的损失函数以及针对稀疏比较设置优化的训练策略。通过采用 TrueSkill 等心理计量缩放算法，PICNIQ 将成对比较转换为合理差异 (JOD) 质量分数，从而提供精细且可解释的图像质量度量。我们使用本文中发布的 PIQ23 数据集的比较矩阵进行研究。我们广泛的实验分析展示了 PICNIQ 的广泛适用性和优于现有模型的性能，凸显了其在 BIQA 领域树立新标准的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.09746</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>一张图像抵得上 1000 个谎言：视觉语言模型上跨提示的对抗性可迁移性</title>
      <link>https://arxiv.org/abs/2403.09766</link>
      <description><![CDATA[arXiv:2403.09766v1 公告类型：新
摘要：与传统的特定任务视觉模型不同，最近的大型 VLM 可以通过简单地使用不同的文本指令（即提示）轻松适应不同的视觉任务。然而，人们对传统特定任务视觉模型的一个众所周知的担忧是，它们可能会被难以察觉的对抗性扰动所误导。此外，相同的对抗性扰动可以欺骗不同的特定任务模型的现象加剧了人们的担忧。鉴于 VLM 依赖提示来适应不同的任务，一个有趣的问题出现了：当给出一千个不同的提示时，单个对抗性图像是否会误导 VLM 的所有预测？这个问题本质上引入了一种关于对抗性可转移性的新观点：交叉提示对抗性可转移性。在这项工作中，我们提出了交叉提示攻击（CroPA）。该方法用可学习的提示更新视觉对抗性扰动，旨在抵消对抗性图像的误导性影响。通过这样做，CroPA 显着提高了对抗性示例跨提示的可转移性。进行了大量的实验来验证 CroPA 与流行的 VLM（包括 Flamingo、BLIP-2 和 InstructBLIP）在各种不同任务中的强大的交叉提示对抗可迁移性。我们的源代码位于 \url{https://github.com/Haochen-Luo/CroPA}。]]></description>
      <guid>https://arxiv.org/abs/2403.09766</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:53 GMT</pubDate>
    </item>
    <item>
      <title>使用 LLM 程序合成和非策划对象数据库生成开放宇宙室内场景</title>
      <link>https://arxiv.org/abs/2403.09675</link>
      <description><![CDATA[arXiv:2403.09675v1 公告类型：新
摘要：我们提出了一种根据文本提示生成室内场景的系统。提示不限于场景描述的固定词汇，生成场景中的对象也不限于一组固定的对象类别——我们将这种设置称为室内场景生成。与大多数先前的室内场景生成工作不同，我们的系统不需要现有 3D 场景的大型训练数据集。相反，它利用预先训练的大型语言模型 (LLM) 中编码的世界知识，以特定领域的布局语言合成程序，描述对象及其之间的空间关系。执行这样的程序会产生约束满足问题的规范，系统使用基于梯度的优化方案来解决该问题，以产生对象位置和方向。为了生成对象几何形状，系统从数据库中检索 3D 网格。与之前使用类别注释的、相互对齐的网格数据库的工作不同，我们开发了一个使用视觉语言模型（VLM）的管道，从未注释的、不一致对齐的网格的海量数据库中检索网格。实验评估表明，对于传统的封闭宇宙场景生成任务，我们的系统优于基于 3D 数据训练的生成模型；它在开放宇宙场景生成方面也优于最近基于 LLM 的布局生成方法。]]></description>
      <guid>https://arxiv.org/abs/2403.09675</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>ViT-MUL：应用于视觉变压器的最新机器去学习方法的基线研究</title>
      <link>https://arxiv.org/abs/2403.09681</link>
      <description><![CDATA[arXiv:2403.09681v1 公告类型：新
摘要：机器去学习（MUL）是机器学习中的一个新兴领域，旨在从训练模型中删除特定训练数据点的学习信息。尽管最近计算机视觉领域的 MUL 研究很活跃，但大部分工作都集中在基于 ResNet 的模型上。鉴于视觉变换器 (ViT) 已成为主要的模型架构，因此专门针对 ViT 定制的 MUL 的详细研究至关重要。在本文中，我们使用最新的 MUL 算法和数据集对 ViT 进行了全面的实验。我们预计我们的实验、消融研究和发现可以提供有价值的见解并激发该领域的进一步研究。]]></description>
      <guid>https://arxiv.org/abs/2403.09681</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>反事实图像编辑</title>
      <link>https://arxiv.org/abs/2403.09683</link>
      <description><![CDATA[arXiv:2403.09683v1 公告类型：新
摘要：反事实图像编辑是生成人工智能中的一项重要任务，它询问如果某些特征不同，图像会是什么样子。目前有关该主题的文献主要关注于改变个体特征，同时对现实世界中这些特征之间的因果关系保持沉默。在本文中，我们使用形式语言形式化了反事实图像编辑任务，通过一种称为增强结构因果模型（ASCM）的特殊类型模型对潜在生成因素和图像之间的因果关系进行建模。其次，我们展示了两个基本的不可能结果：（1）从独立同分布中反事实编辑是不可能的。单独的图像样本及其相应的标签； （2）即使潜在生成因素和图像之间的因果关系可用，也不能提供模型输出的保证。第三，我们提出通过使用一系列新的反事实一致估计量来近似不可识别的反事实分布，来缓解这一具有挑战性的问题。该系列展现了在事实和反事实世界中保留用户关心的特征的理想特性。最后，我们开发了一种有效的算法，利用神经因果模型生成反事实图像。]]></description>
      <guid>https://arxiv.org/abs/2403.09683</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:52 GMT</pubDate>
    </item>
    <item>
      <title>通过定性场景理解和解释实现值得信赖的自动驾驶</title>
      <link>https://arxiv.org/abs/2403.09668</link>
      <description><![CDATA[arXiv:2403.09668v1 公告类型：新
摘要：我们提出了定性可解释图（QXG）：城市交通场景理解的统一符号和定性表示。 QXG 能够使用传感器数据和机器学习模型来解释自动驾驶车辆的环境。它利用时空图和定性约束从原始传感器输入（例如激光雷达和相机数据）中提取场景语义，提供可理解的场景模型。至关重要的是，QXG 可以实时增量构建，使其成为跨各种传感器类型进行车内解释和实时决策的多功能工具。我们的研究展示了 QXG 的变革潜力，特别是在自动驾驶的背景下，它通过将图表与车辆行为联系起来来阐明决策原理。这些解释有多种目的，从通知乘客和提醒弱势道路使用者 (VRU) 到对先前行为进行事后分析。]]></description>
      <guid>https://arxiv.org/abs/2403.09668</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>STREAM：视频生成模型的时空评估和分析指标</title>
      <link>https://arxiv.org/abs/2403.09669</link>
      <description><![CDATA[arXiv:2403.09669v1 公告类型：新
摘要：在各种评估指标的综合指导的支持下，图像生成模型在生成真实且多样化的图像方面取得了重大进展。然而，当前的视频生成模型甚至难以生成短视频剪辑，提供改进见解的工具有限。当前的视频评估指标是通过使用视频嵌入网络切换嵌入来对图像指标进行简单的调整，这可能会低估视频的独特特征。我们的分析表明，广泛使用的 Frechet 视频距离 (FVD) 更强调视频的空间方面而不是时间自然性，并且本质上受到所使用的嵌入网络的输入大小的限制，将其限制为 16 帧。此外，它表现出相当大的不稳定性并且与人类的评估存在差异。为了解决这些限制，我们提出了 STREAM，这是一种新的视频评估指标，专门设计用于独立评估空间和时间方面。该功能可以从多个角度对视频生成模型进行综合分析和评估，不受视频长度的限制。我们提供的分析和实验证据表明，STREAM 为视频的视觉和时间质量提供了有效的评估工具，为视频生成模型的改进领域提供了见解。据我们所知，STREAM是第一个可以单独评估视频的时间和空间方面的评估指标。我们的代码可在 STREAM 上找到。]]></description>
      <guid>https://arxiv.org/abs/2403.09669</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>COMPRER：用于增强医学图像表示的多模态多目标预训练框架</title>
      <link>https://arxiv.org/abs/2403.09672</link>
      <description><![CDATA[arXiv:2403.09672v1 公告类型：新
摘要：多模式人工智能（AI）的巨大进步促进了多种医疗模式的结合，以实现整体健康评估。我们提出了 COMPRER，一种新颖的多模式、多目标预训练框架，可增强医学图像表示、诊断推理和疾病预后。 COMPRER 采用多目标训练框架，其中每个目标向模型引入不同的知识。这包括整合不同成像模式信息的多模态损失；时间上的损失赋予了随着时间的推移辨别模式的能力；医疗措施预测增加了适当的医疗见解；最后，重建损失确保了潜在空间内图像结构的完整性。尽管担心多个目标可能会削弱任务绩效，但我们的研究结果表明，这种组合实际上可以提高某些任务的结果。在这里，我们将该框架应用于眼底图像和颈动脉超声，并通过预测当前和未来的心血管状况来验证我们的下游任务能力。与保留数据的现有模型相比，COMPRER 在评估医疗状况方面取得了更高的曲线下面积 (AUC) 分数。在分布外 (OOD) UK-Biobank 数据集上，与具有更多参数的成熟模型相比，COMPRER 保持了良好的性能，尽管这些模型的训练数据比 COMPRER 多 75 倍。此外，为了更好地评估我们的模型在对比学习中的表现，我们引入了一种新颖的评估指标，可以更深入地了解潜在空间配对的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.09672</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:51 GMT</pubDate>
    </item>
    <item>
      <title>无监督图像到图像转换和 GAN 稳定性</title>
      <link>https://arxiv.org/abs/2403.09646</link>
      <description><![CDATA[arXiv:2403.09646v1 公告类型：新
摘要：图像到图像的转换问题是一个既有趣又具有挑战性的问题，因为它可能对各种其他计算机视觉应用（如着色、修复、分割等）产生潜在的影响。鉴于从一个领域提取模式并将其成功应用于另一个领域所需的高度复杂性，特别是以完全无监督（不配对）的方式，这一问题在过去几年中引起了广泛关注。这是深度生成模型（尤其是生成对抗网络）成功应用的首要问题之一，取得了令人震惊的结果，这些结果实际上对现实世界产生了影响，而不仅仅是理论实力的展示；这些一直主导着 GAN 世界。在这项工作中，我们研究了该领域开创性工作 CycleGAN [1] 的一些失败案例，并假设它们与 GAN 稳定性相关，并提出了两种通用模型来尝试缓解这些问题。我们还得出了最近文献中流传的问题不适定的相同结论。]]></description>
      <guid>https://arxiv.org/abs/2403.09646</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:50 GMT</pubDate>
    </item>
    <item>
      <title>精准农业：使用机器学习和 Sentinel-2 卫星图像绘制作物地图</title>
      <link>https://arxiv.org/abs/2403.09651</link>
      <description><![CDATA[arXiv:2403.09651v1 公告类型：新
摘要：由于气候变化及其变暖效应，粮食安全变得越来越重要。为了支持对农产品不断增长的需求，并尽量减少气候变化和大规模种植的负面影响，精准农业对于作物种植变得越来越重要。这项研究采用深度学习和基于像素的机器学习方法，利用从 Sentinel-2 卫星图像中提取的各种光谱带组合，准确分割薰衣草田以实现精准农业。我们经过微调的最终模型（U-Net 架构）可以实现 0.8324 的 Dice 系数。此外，我们的研究强调了基于像素的方法和 RGB 光谱带组合在此任务中的意想不到的功效。]]></description>
      <guid>https://arxiv.org/abs/2403.09651</guid>
      <pubDate>Mon, 18 Mar 2024 06:17:50 GMT</pubDate>
    </item>
    </channel>
</rss>