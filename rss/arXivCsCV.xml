<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Tue, 06 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>在单个图像中具有对比学习和相机一致性的多裁剪人体网格恢复</title>
      <link>https://arxiv.org/abs/2402.02074</link>
      <description><![CDATA[我们解决了单图像人体网格恢复（HMR）的问题。以前的方法大多基于单一作物。在本文中，我们将单作物 HMR 转变为一种新颖的多作物 HMR 范式。通过移动和缩放原始边界框来多次从图像中裁剪人体在实践中是可行的，易于实现，并且产生的成本可以忽略不计，但立即丰富了可用的视觉细节。以多种作物作为输入，我们设法利用这些作物之间的关系来提取判别性特征并减少相机模糊度。具体来说，（1）我们采用对比学习方案来增强从同一个人的作物中提取的特征之间的相似性。 （2）我们还提出了一种作物感知融合方案，以融合多种作物的特征来回归目标网格。 (3) 我们计算所有输入作物的本地相机，并在本地相机之间建立相机一致性损失，这会奖励我们不那么模糊的相机。基于上述创新，我们提出的方法优于以前的方法，如大量实验所证明的。]]></description>
      <guid>https://arxiv.org/abs/2402.02074</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:03 GMT</pubDate>
    </item>
    <item>
      <title>TCI-Former：用于红外小目标检测的热传导变压器</title>
      <link>https://arxiv.org/abs/2402.02046</link>
      <description><![CDATA[红外小目标探测（ISTD）对国家安全至关重要，已广泛应用于军事领域。 ISTD 旨在从背景中分割小目标像素。大多数ISTD网络专注于设计特征提取块或特征融合模块，但很少从特征图演化的角度描述ISTD过程。在ISTD过程中，网络注意力逐渐向目标领域转移。我们将这个过程抽象为特征图像素通过卷积、池化以及与周围像素的相互作用向目标区域的定向移动，这可以类似于受周围变量和粒子约束的热粒子的运动。鉴于这个类比，我们基于热传导的理论原理提出了热传导启发变压器（TCI-Former）。根据热动力学中的热传导微分方程，推导了图像域中的像素运动微分方程（PMDE），并进一步开发了两个模块：热传导启发注意力模块（TCIA）和热传导边界模块（TCBM）。 TCIA将有限差分法与PMDE相结合，达到数值逼近，从而提取目标身体特征。为了进一步消除边界区域中的错误，TCBM 被设计并通过边界掩模进行监督，以通过精细的边界细节来细化目标身体特征。 IRSTD-1k 和 NUAA-SIRST 上的实验证明了我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2402.02046</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:02 GMT</pubDate>
    </item>
    <item>
      <title>DiffVein：用于指静脉分割和身份验证的统一扩散网络</title>
      <link>https://arxiv.org/abs/2402.02060</link>
      <description><![CDATA[指静脉认证以其高安全性和特异性而受到认可，已成为生物识别研究的焦点。传统方法主要集中于静脉特征提取以进行判别建模，对生成方法的探索有限。由于验证失败，现有方法往往无法通过分割获得真实的静脉图案。为了填补这一空白，我们引入了 DiffVein，这是一种基于统一扩散模型的框架，可同时解决静脉分割和身份验证任务。 DiffVein 由两个专用分支组成：一个用于分割，另一个用于去噪。为了这两个分支之间更好的功能交互，我们引入了两个专门的模块来提高它们的集体性能。第一个是掩模条件模块，将来自分割分支的静脉图案的语义信息合并到去噪过程中。此外，我们还提出了一种语义差异变换器（SD-Former），它采用傅里叶空间自注意力和交叉注意力模块来提取类别嵌入，然后将其输入到分割任务。通过这种方式，我们的框架允许扩散和分割嵌入之间的动态相互作用，因此静脉分割和身份验证任务可以在联合训练中相互告知和增强。为了进一步优化我们的模型，我们引入了傅里叶空间结构相似性（FourierSIM）损失函数，该函数是为提高去噪网络的学习效率而定制的。 USM 和 THU-MVFV3V 数据集上的大量实验证实了 DiffVein 的卓越性能，为静脉分割和身份验证任务树立了新的基准。]]></description>
      <guid>https://arxiv.org/abs/2402.02060</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:02 GMT</pubDate>
    </item>
    <item>
      <title>RIDERS：用于稳健传感的雷达红外深度估计</title>
      <link>https://arxiv.org/abs/2402.02067</link>
      <description><![CDATA[密集深度恢复对于自动驾驶至关重要，是避障、3D 物体检测和局部路径规划的基础要素。雾霾、灰尘、雨雪、黑暗等恶劣天气条件给精确的密集深度估计带来了重大挑战，从而给自动驾驶带来了巨大的安全风险。对于依赖短电磁波传感器的传统深度估计方法（例如可见光谱相机和近红外激光雷达）来说，这些挑战尤其明显，因为它们在此类环境中容易受到衍射噪声和遮挡的影响。为了从根本上解决这个问题，我们提出了一种通过融合毫米波雷达和单目红外热像仪来进行鲁棒的度量深度估计的新方法，它们能够穿透大气颗粒并且不受照明条件的影响。我们提出的雷达-红外融合方法通过三个阶段实现了高精度和精细的密集深度估计，包括具有全局尺度对齐的单目深度预测、通过学习雷达像素对应关系的准密集雷达增强以及使用密集深度的局部尺度细化。比例尺地图学习者。我们的方法通过解决直接融合多模态长波特征所带来的模糊性和错位的挑战，实现了卓越的视觉质量和准确的度量估计。我们评估了我们的方法在 NTU4DRadLM 数据集和我们自行收集的具有挑战性的 ZJU-Multispectrum 数据集上的性能。特别值得注意的是我们提出的方法在烟雾场景中表现出前所未有的鲁棒性。我们的代码将在 \url{https://github.com/MMOCKING/RIDERS} 发布。]]></description>
      <guid>https://arxiv.org/abs/2402.02067</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:02 GMT</pubDate>
    </item>
    <item>
      <title>用于自动驾驶中极端情况检测的多模态增强对象学习器</title>
      <link>https://arxiv.org/abs/2402.02026</link>
      <description><![CDATA[先前的目标检测工作在封闭场景中取得了较高的准确率，但在开放场景中的表现却不尽如人意。具有挑战性的开放世界问题之一是自动驾驶中的极端情况检测。现有的检测器在处理这些情况时遇到了困难，严重依赖视觉外观并且泛化能力较差。在本文中，我们提出了一种解决方案，通过减少已知类和未知类之间的差异，并引入多模态增强的对象概念学习器。利用以视觉为中心和图像文本模式，我们的半监督学习框架将客观性知识传授给学生模型，从而实现类感知检测。我们的方法，用于角点案例检测的多模态增强对象学习器（MENOL），可以以较低的训练成本显着提高新类的召回率。通过仅使用 5100 个标记训练图像在 CODA-val 数据集上实现 76.6% 的 mAR 角点和 79.8% 的 mAR 不可知性，MENOL 的性能分别优于基线 ORE 71.3% 和 60.6%。该代码可在 https://github.com/tryhiseyyysum/MENOL 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.02026</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:01 GMT</pubDate>
    </item>
    <item>
      <title>ScribFormer：Transformer 使 CNN 更好地进行基于 Scribble 的医学图像分割</title>
      <link>https://arxiv.org/abs/2402.02029</link>
      <description><![CDATA[最近的涂鸦监督分割方法通常采用具有编码器-解码器架构的 CNN 框架。尽管具有多种优点，但该框架通常只能捕获具有局部感受野的卷积层的小范围特征依赖性，这使得很难从涂鸦注释提供的有限信息中学习全局形状信息。为了解决这个问题，本文提出了一种新的 CNN-Transformer 混合解决方案，用于涂鸦监督医学图像分割，称为 ScribFormer。所提出的 ScribFormer 模型具有三分支结构，即 CNN 分支、Transformer 分支和注意力引导类激活图 (ACAM) 分支的混合。具体来说，CNN分支与Transformer分支协作，将从CNN学到的局部特征与从Transformer获得的全局表示融合，可以有效克服现有涂鸦监督分割方法的局限性。此外，ACAM分支有助于统一浅层卷积特征和深层卷积特征，以进一步提高模型的性能。对两个公共数据集和一个私有数据集的大量实验表明，我们的 ScribFormer 比最先进的涂鸦监督分割方法具有更优越的性能，并且比完全监督分割方法取得了更好的结果。代码发布于https://github.com/HUANGLIZI/ScribFormer。]]></description>
      <guid>https://arxiv.org/abs/2402.02029</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:01 GMT</pubDate>
    </item>
    <item>
      <title>MLIP：通过发散编码器和知识引导的对比学习增强医学视觉表示</title>
      <link>https://arxiv.org/abs/2402.02045</link>
      <description><![CDATA[带注释数据的稀缺引发了人们对无监督预训练方法的极大兴趣，这些方法利用医疗报告作为医学视觉表示学习的辅助信号。然而，现有研究忽视了医学视觉表示的多粒度性质，并且缺乏合适的对比学习技术来提高模型在不同粒度上的泛化性，导致图像文本信息的利用不足。为了解决这个问题，我们提出了 MLIP，这是一种利用特定领域的医学知识作为指导信号，通过图像文本对比学习将语言信息整合到视觉领域的新颖框架。我们的模型包括使用我们设计的散度编码器进行的全局对比学习、本地令牌知识补丁对齐对比学习以及使用专家知识进行知识引导的类别级对比学习。实验评估揭示了我们的模型在增强图像分类、对象检测和语义分割等任务的传输性能方面的功效。值得注意的是，即使注释数据有限，MLIP 也超越了最先进的方法，凸显了多模态预训练在推进医学表征学习方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.02045</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:01 GMT</pubDate>
    </item>
    <item>
      <title>GenFace：大规模细粒度人脸伪造基准和交叉外观边缘学习</title>
      <link>https://arxiv.org/abs/2402.02003</link>
      <description><![CDATA[真实感生成器的快速发展已经达到了一个关键时刻，真实图像和经过处理的图像之间的差异越来越难以区分。因此，检测数字操纵的基准和先进技术成为一个紧迫的问题。虽然已经有很多公开的人脸伪造数据集，但伪造的人脸大多是使用基于 GAN 的合成技术生成的，不涉及扩散等最新技术。扩散模型生成的图像的多样性和质量得到了显着提高，因此应使用更具挑战性的人脸伪造数据集来评估 SOTA 伪造检测文献。在本文中，我们提出了一个大规模、多样化、细粒度的高保真数据集，即 GenFace，以促进 Deepfake 检测的进步，其中包含由高级生成器（例如扩散生成器）生成的大量伪造人脸。基于模型和有关操作方法和采用的生成器的更详细标签。除了在我们的基准上评估 SOTA 方法之外，我们还设计了一种创新的交叉外观边缘学习 (CAEL) 检测器来捕获多粒度外观和边缘全局表示，并检测有区别的和一般的伪造痕迹。此外，我们设计了一个外观边缘交叉注意（AECA）模块来探索跨两个领域的各种集成。大量的实验结果和可视化结果表明，我们的检测模型在跨生成器、交叉伪造和跨数据集评估等不同设置上都优于现有技术。代码和数据集可在 \url{https://github.com/Jenine-321/GenFace 获取]]></description>
      <guid>https://arxiv.org/abs/2402.02003</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:00 GMT</pubDate>
    </item>
    <item>
      <title>通过流程匹配实现精准知识传递</title>
      <link>https://arxiv.org/abs/2402.02012</link>
      <description><![CDATA[在本文中，我们提出了一种新颖的知识转移框架，该框架引入了用于渐进式知识转换的连续规范化流，并利用多步采样策略来实现精确的知识转移。我们将这个框架命名为流匹配知识转移（FM-KT），它可以与任何形式的基于度量的蒸馏方法（\textit{例如} vanilla KD、DKD、PKD 和 DIST）以及元编码器集成任何可用的架构（\textit{例如 CNN、MLP 和 Transformer）。通过引入随机插值，FM-KD 很容易适应任意噪声表（\textit{e.g.}、VP-ODE、VE-ODE、整流流），以进行归一化流路径估计。我们从理论上证明，FM-KT 的训练目标相当于最小化教师特征图的上限或 logit 负对数似然。此外，FM-KT 可以被视为一种独特的隐式集成方法，可以带来性能提升。通过稍微修改 FM-KT 框架，FM-KT 还可以转变为在线蒸馏框架 OFM-KT，并具有理想的性能提升。通过对 CIFAR-100、ImageNet-1k 和 MS-COCO 数据集进行广泛的实验，我们凭经验验证了我们提出的方法在相关比较方法中的可扩展性和最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.02012</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:00 GMT</pubDate>
    </item>
    <item>
      <title>NeuV-SLAM：RGBD 密集 SLAM 的快速神经多分辨率体素优化</title>
      <link>https://arxiv.org/abs/2402.02020</link>
      <description><![CDATA[我们介绍了 NeuV-SLAM，这是一种基于神经多分辨率体素的新型密集同步定位和建图管道，其特点是超快速收敛和增量扩展能力。该流程利用 RGBD 图像作为输入来构建多分辨率神经体素，实现快速收敛，同时保持稳健的增量场景重建和相机跟踪。我们方法的核心是提出一种新颖的隐式表示，称为 VDF，它将神经符号距离场 (SDF) 体素的实现与 SDF 激活策略结合起来。这种方法需要直接优化锚定在体素内的颜色特征和 SDF 值，从而大大提高场景收敛速度。为了确保获得清晰的边缘轮廓，设计了 SDF 激活，即使在体素分辨率的限制下也能保持示例性场景表示保真度。此外，为了追求以低计算开销推进快速增量扩展，我们开发了 hashMV，一种新颖的基于哈希的多分辨率体素管理结构。该架构得到了战略性设计的体素生成技术的补充，该技术与二维场景先验相协同。我们对 Replica 和 ScanNet 数据集进行的实证评估证实了 NeuV-SLAM 在收敛速度、跟踪精度、场景重建和渲染质量方面的卓越功效。]]></description>
      <guid>https://arxiv.org/abs/2402.02020</guid>
      <pubDate>Tue, 06 Feb 2024 18:15:00 GMT</pubDate>
    </item>
    <item>
      <title>BVI-Lowlight：完全注册的低光视频增强基准数据集</title>
      <link>https://arxiv.org/abs/2402.01970</link>
      <description><![CDATA[低光视频通常会表现出时空不相干噪声，导致各种计算机视觉应用的可视性较差并影响性能。使用现代技术增强此类内容的一项重大挑战是训练数据的稀缺。本文介绍了一种新颖的低光视频数据集，由在两种不同的低光条件下的各种运动场景中捕获的 40 个场景组成，包含真实的噪声和时间伪影。我们使用可编程电动小车提供在正常光线下捕获的完全配准的地面实况数据，然后通过基于图像的后处理对其进行细化，以确保不同光线水平下帧的像素级对齐。本文还对弱光数据集进行了详尽的分析，并展示了我们的数据集在监督学习背景下的广泛性和代表性。我们的实验结果证明了完全配准的视频对在低光视频增强方法开发中的重要性以及综合评估的必要性。我们的数据集可在 DOI:10.21227/mzny-8c77 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.01970</guid>
      <pubDate>Tue, 06 Feb 2024 18:14:59 GMT</pubDate>
    </item>
    <item>
      <title>用于腹腔镜和机器人手术中交互式事件预测的超图变换器 (HGT)</title>
      <link>https://arxiv.org/abs/2402.01974</link>
      <description><![CDATA[了解和预测术中事件和行动对于微创手术期间的术中协助和决策至关重要。通过各种计算方法来自动预测事件、行动和随后的后果，目的是增强外科医生的感知和决策能力。我们提出了一种预测神经网络，能够通过腹内视频理解和预测手术工作流程的关键交互方面，同时灵活利用手术知识图。该方法采用了超图变换器（HGT）结构，将专家知识编码到网络设计中并预测图的隐藏嵌入。我们在已建立的手术数据集和应用程序上验证了我们的方法，包括动作三元组的检测和预测，以及安全批判性观点（CVS）的实现。此外，我们还解决与安全相关的具体任务，例如在未事先实现 CVS 的情况下预测胆囊管或动脉的夹闭。我们的结果证明了我们的方法与非结构化替代方案相比的优越性。]]></description>
      <guid>https://arxiv.org/abs/2402.01974</guid>
      <pubDate>Tue, 06 Feb 2024 18:14:59 GMT</pubDate>
    </item>
    <item>
      <title>SynthCLIP：我们准备好进行全合成 CLIP 培训了吗？</title>
      <link>https://arxiv.org/abs/2402.01832</link>
      <description><![CDATA[我们提出了 SynthCLIP，这是一种使用完全合成的文本图像对来训练 CLIP 模型的新颖框架，与之前依赖真实数据的方法有很大不同。利用最新的文本到图像（TTI）生成网络和大型语言模型（LLM），我们能够生成任何规模的图像和相应标题的合成数据集，而无需人工干预。通过大规模训练，SynthCLIP 的性能可与在真实数据集上训练的 CLIP 模型相媲美。我们还引入了 SynthCI-30M，这是一个包含 3000 万张带字幕图像的纯合成数据集。我们的代码、训练模型和生成的数据发布在 https://github.com/hammoudhasan/SynthCLIP]]></description>
      <guid>https://arxiv.org/abs/2402.01832</guid>
      <pubDate>Tue, 06 Feb 2024 18:14:58 GMT</pubDate>
    </item>
    <item>
      <title>通过概率推理的鲁棒逆向图形</title>
      <link>https://arxiv.org/abs/2402.01915</link>
      <description><![CDATA[在存在雨、雪或雾等损坏的情况下，我们如何从单个图像推断出 3D 场景？直接的域随机化依赖于提前了解腐败家族。在这里，我们提出了一种被称为鲁棒逆向图形（RIG）的贝叶斯方法，它依赖于强大的场景先验和无信息的统一腐败先验，使其适用于广泛的腐败。给定单个图像，RIG 对场景和损坏联合执行后验推理。我们通过训练神经辐射场 (NeRF) 场景先验并使用辅助 NeRF 来表示我们在其上放置无信息先验的损坏来演示这一想法。 RIG 仅在干净数据上进行训练，其性能优于深度估计器和执行点估计而不是完全推理的替代 NeRF 方法。结果适用于许多基于标准化流和扩散模型的场景先验架构。对于后者，我们开发了带有辅助潜在变量的重建引导（ReGAL）——一种扩散调节算法，适用于存在辅助潜在变量（例如腐败）的情况。 RIG 演示了如何在生成任务之外使用场景先验。]]></description>
      <guid>https://arxiv.org/abs/2402.01915</guid>
      <pubDate>Tue, 06 Feb 2024 18:14:58 GMT</pubDate>
    </item>
    <item>
      <title>ConRF：具有条件辐射场的 3D 场景的零样本风格化</title>
      <link>https://arxiv.org/abs/2402.01950</link>
      <description><![CDATA[大多数现有的任意 3D NeRF 风格迁移工作都需要对每种单一风格条件进行重新训练。这项工作旨在利用文本或视觉输入作为条件因素，在 3D 场景中实现零镜头控制的风格化。我们介绍 ConRF，一种零样本风格化的新方法。具体来说，由于 CLIP 特征的模糊性，我们采用了一种转换过程，将 CLIP 特征空间映射到预训练的 VGG 网络的风格空间，然后将 CLIP 多模态知识细化为风格迁移神经辐射场。此外，我们使用 3D 体积表示来执行局部风格转换。通过结合这些操作，ConRF 提供了利用文本或图像作为参考的能力，从而生成具有通过全局或局部风格化增强的新颖视图的序列。我们的实验表明，ConRF 在视觉质量方面优于其他现有的 3D 场景和单文本风格化方法。]]></description>
      <guid>https://arxiv.org/abs/2402.01950</guid>
      <pubDate>Tue, 06 Feb 2024 18:14:58 GMT</pubDate>
    </item>
    </channel>
</rss>