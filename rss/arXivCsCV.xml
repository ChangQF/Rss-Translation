<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 27 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>CLIPose：利用预先训练的视觉语言知识进行类别级物体姿态估计</title>
      <link>https://arxiv.org/abs/2402.15726</link>
      <description><![CDATA[arXiv:2402.15726v1 公告类型：新
摘要：现有的大多数类别级物体姿态估计方法致力于从点云模态学习物体类别信息。然而，由于 3D 数据收集和注释的成本高昂，3D 数据集的规模受到限制。因此，从这些有限的点云样本中提取的类别特征可能并不全面。这促使我们研究是否可以利用其他方式的知识来获取类别信息。受此动机的启发，我们提出了 CLIPose，这是一种新颖的 6D 姿势框架，它采用预先训练的视觉语言模型来更好地学习对象类别信息，可以充分利用图像和文本模态中丰富的语义知识。为了使 3D 编码器更有效地学习特定类别的特征，我们通过多模态对比学习在特征空间中对齐三种模态的表示。除了利用 CLIP 模型的预训练知识之外，我们还期望它对姿态参数更加敏感。因此，我们引入了一种即时调整方法来微调图像编码器，同时将旋转和平移信息合并到文本描述中。 CLIPose 在两个主流基准数据集 REAL275 和 CAMERA25 上实现了最先进的性能，并在推理过程中实时运行 (40FPS)。]]></description>
      <guid>https://arxiv.org/abs/2402.15726</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>智能导演：使用 ChatGPT 的动态视觉合成自动框架</title>
      <link>https://arxiv.org/abs/2402.15746</link>
      <description><![CDATA[arXiv:2402.15746v1 公告类型：新
摘要：随着以TikTok为代表的短视频平台的崛起，用户通过照片和视频表达创意的趋势急剧增加。然而，普通用户缺乏使用专业创作软件制作高质量视频的专业技能。为了满足智能和用户友好的视频创作工具的需求，我们提出了动态视觉合成（DVC）任务，这是一项有趣且具有挑战性的任务，旨在根据用户需求自动集成各种媒体元素并创建讲故事的视频。我们提出了一个智能导演框架，利用 LENS 生成图像和视频帧的描述，并结合 ChatGPT 生成连贯的字幕，同时推荐适当的音乐名称。然后，通过音乐检索得到最匹配的音乐。然后将字幕、图像、视频、音乐等素材进行整合，无缝合成视频。最后，我们应用 AnimeGANv2 进行风格迁移。我们构建了 UCF101-DVC 和个人相册数据集，并通过定性和定量比较以及用户研究验证了我们的框架在解决 DVC 方面的有效性，展示了其巨大的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.15746</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>压缩感知光声投影成像系统的设计、实现与分析</title>
      <link>https://arxiv.org/abs/2402.15750</link>
      <description><![CDATA[arXiv:2402.15750v1 公告类型：新
摘要： 意义：压缩感知（CS）利用特殊的测量设计与强大的数学算法相结合，在保持图像质量的同时减少需要收集的数据量。这几乎与所有成像方式相关，在本文中，我们重点关注具有集成线探测器 (ILD) 的光声投影成像 (PAPI) 中的 CS。
  目标：我们之前的研究涉及相当一般的 CS 测量，其中每个 ILD 都可以对任何测量做出贡献。然而，在现实世界中，CS 测量的设计受到实际限制。在这项研究中，我们的目标是建立一个 CS-PAPI 系统，其中每次测量仅涉及 ILD 的一个子集，并且可以以经济有效的方式实施。
  方法：我们用自主开发的 CS 单元扩展现有的 PAPI。该系统提供了现有恢复理论无法直接应用的结构化CS矩阵。应用随机搜索策略来选择此类中的 CS 测量矩阵，为此我们获得了精确的稀疏恢复。
  结果：我们实施了压缩系数为 4:3 的 CS PAPI 系统，其中对 16 个 ILD 的不同组进行了特定测量。我们通过算法设计最佳 CS 测量，并已证明稀疏 CS 功能。数值实验用于支持我们的结果。
  结论：具有经过验证的稀疏恢复能力的 CS 可以集成到 PAPI 中，并且数值结果支持这种设置。未来的工作将集中于将其应用于实验数据，并利用数据驱动的方法来增强压缩因子并概括信号类别。]]></description>
      <guid>https://arxiv.org/abs/2402.15750</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:44 GMT</pubDate>
    </item>
    <item>
      <title>GiMeFive：迈向可解释的面部情绪分类</title>
      <link>https://arxiv.org/abs/2402.15662</link>
      <description><![CDATA[arXiv:2402.15662v1 公告类型：新
摘要：过去几年，深度卷积神经网络在计算机视觉领域已被证明可以成功识别面部情绪。然而，现有的检测方法并不总是可靠或可解释的，我们在这里提出了带有解释的模型 GiMeFive，即通过层激活和梯度加权类激活映射。我们与最先进的方法进行比较，对六种面部情绪进行分类。实证结果表明，我们的模型在两个面部情绪识别 (FER) 基准和我们聚合的 FER GiMeFive 的准确性方面优于以前的方法。此外，我们还通过现实世界的图像和视频示例以及实时摄像头流来解释我们的工作。我们的代码和补充材料可在 https://github.com/werywjw/SEP-CVDL 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.15662</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:43 GMT</pubDate>
    </item>
    <item>
      <title>用于医学图像配准的通用图像编码器 DINOv2</title>
      <link>https://arxiv.org/abs/2402.15687</link>
      <description><![CDATA[arXiv:2402.15687v1 公告类型：新
摘要：现有的医学图像配准算法依赖于数据集特定的训练或基于局部纹理的特征来对齐图像。如果没有大型特定于模态的训练数据集，前者就无法可靠地实现，而后者缺乏全局语义，因此很容易陷入局部最小值。在本文中，我们提出了一种免训练的可变形图像配准方法 DINO-Reg，利用通用图像编码器 DINOv2 进行图像特征提取。 DINOv2 编码器使用包含自然图像的 ImageNet 数据进行训练。我们使用预训练的 DINOv2，无需任何微调。我们的方法将 DINOv2 编码特征输入离散优化器中，以找到最佳的可变形配准字段。我们进行了一系列实验来了解这种通用图像编码器在图像配准应用中的行为和作用。结合手工制作的特征，我们的方法在最近的 OncoReg 挑战赛中获得了第一名。据我们所知，这是通用视觉基础模型在医学图像配准中的首次应用。]]></description>
      <guid>https://arxiv.org/abs/2402.15687</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:43 GMT</pubDate>
    </item>
    <item>
      <title>通过进化算法进行低频黑盒后门攻击</title>
      <link>https://arxiv.org/abs/2402.15653</link>
      <description><![CDATA[arXiv:2402.15653v1 公告类型：新
摘要：虽然卷积神经网络（CNN）在计算机视觉任务中取得了成功，但它很容易受到后门攻击。此类攻击可能会误导受害者模型，以特定的触发模式做出攻击者选择的预测。到目前为止，现有攻击的触发注入主要局限于空间域。最近的工作利用了在频域中植入特定模式的感知特性，这仅反映了像素域中不可区分的像素级扰动。然而，在黑盒设置中，训练过程的不可访问性往往导致触发器设计更加复杂。现有的频率攻击只是简单地手工调整频谱的大小，在干净数据和中毒数据之间引入异常频率差异，并冒着被图像处理操作（例如有损压缩和过滤）删除的风险。在本文中，我们提出了一种鲁棒的低频黑盒后门攻击（LFBA），它最大限度地干扰频谱的低频分量，同时保持空间空间的感知相似性。我们攻击的关键见解将最佳触发的搜索限制在低频区域，从而实现高攻击效率、针对图像变换防御的鲁棒性以及对偶空间中的隐身性。我们利用模拟退火（SA）（一种进化算法）来优化频率触发的属性，包括操纵频带的数量和每个频率分量的扰动，而不依赖于受害者分类器的知识。对真实世界数据集的大量实验验证了 LFBA 针对图像处理操作和最先进的后门防御的有效性和鲁棒性，以及其在空间和频率空间中固有的隐秘性，使其能够抵抗频率检查。]]></description>
      <guid>https://arxiv.org/abs/2402.15653</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:42 GMT</pubDate>
    </item>
    <item>
      <title>DeepLight：利用多模态遥感数据重建夜间光的高分辨率观测</title>
      <link>https://arxiv.org/abs/2402.15659</link>
      <description><![CDATA[arXiv:2402.15659v1 公告类型：新
摘要：夜间光（NTL）遥感观测是定量评估实现贫困估计、城市可持续发展和碳排放等一系列可持续发展目标（SDG）进展情况的独特指标。然而，现有的 NTL 观测结果往往普遍退化和不一致，限制了其计算可持续发展目标定义的指标的效用。在这项研究中，我们提出了一种使用多模态遥感数据重建高分辨率 NTL 图像的新方法。为了支持这项研究工作，我们引入了 DeepLightMD，这是一个综合数据集，包含来自五个异构传感器的数据，在全国范围内提供精细的空间分辨率和丰富的光谱信息。此外，我们还提出了 DeepLightSR，一种校准感知方法，用于在多模态超分辨率中的空间异构模态数据之间建立桥梁。 DeepLightSR 集成了校准感知对齐、辅助到主多模态融合和辅助嵌入细化，可有效解决空间异质性、融合不同的代表性特征，并增强 8 倍超分辨率 (SR) 任务的性能。大量实验证明了 DeepLightSR 相对于 8 种竞争方法的优越性，PSNR (2.01 dB $ \sim $ 13.25 dB) 和 PIQE (0.49 $ \sim $ 9.32) 的改进证明了这一点。我们的研究结果强调了我们提出的数据集和模型在重建高分辨率 NTL 数据方面的实际意义，支持有效、定量地评估可持续发展目标的进展。]]></description>
      <guid>https://arxiv.org/abs/2402.15659</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:42 GMT</pubDate>
    </item>
    <item>
      <title>抗噪声形状建模研究</title>
      <link>https://arxiv.org/abs/2402.15587</link>
      <description><![CDATA[arXiv:2402.15587v1 公告类型：新
摘要：形状建模是一项具有挑战性的任务，在计算机视觉和医学成像中有许多潜在的应用。文献中有许多形状建模方法，每种方法都有其优点和应用。然而，许多形状建模方法难以处理缺少部分或异常值的形状。在这方面，本文介绍了形状去噪，这是形状建模中的一个基本问题，是许多计算机视觉和医学成像应用的核心，但在文献中尚未得到足够的重视。本文介绍了可用于扰乱形状的六种类型的噪声，以及噪声水平的客观测量方法以及比较其形状去噪能力的方法。最后，论文评估了七种能够完成这项任务的方法，其中六种是基于深度学习的，包括一些生成模型。]]></description>
      <guid>https://arxiv.org/abs/2402.15587</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>DeepSet SimCLR：用于改进病理表示学习的自监督深度集</title>
      <link>https://arxiv.org/abs/2402.15598</link>
      <description><![CDATA[arXiv:2402.15598v1 公告类型：新
摘要：通常，自监督学习在 3D 医疗数据中的应用选择使用成功 2D 网络架构的 3D 变体。尽管这些方法很有前途，但它们对训练的计算要求明显更高，因此降低了这些方法的广泛适用性，远离那些具有适度计算资源的方法。因此，在本文中，我们的目标是通过隐式建模这些数据集固有的 3D 性质来改进标准 2D SSL 算法。我们提出了两种基于强大基线模型的变体，并表明这两种变体在各种下游任务中通常都优于基线。重要的是，与之前针对 3D 医疗数据的 2D 和 3D 方法的工作相比，我们的两项提案在基线上引入的额外开销可以忽略不计，从而提高了这些医疗应用方法的民主化程度。]]></description>
      <guid>https://arxiv.org/abs/2402.15598</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>MambaIR：状态空间模型图像恢复的简单基线</title>
      <link>https://arxiv.org/abs/2402.15648</link>
      <description><![CDATA[arXiv:2402.15648v1 公告类型：新
摘要：近年来，由于现代深度神经网络（例如神经网络）的进步，图像恢复领域取得了巨大进步。卷积神经网络和变压器。然而，由于固有的局部还原偏差或二次计算复杂性，现有的恢复主干通常受到限制。最近，选择性结构化状态空间模型（例如 Mamba）在具有线性复杂性的远程依赖关系建模方面表现出了巨大的潜力，但它在低级计算机视觉中仍处于探索之中。在这项工作中，我们引入了一个简单但强大的基准模型，名为 MambaIR，用于图像恢复。具体来说，我们提出残差状态空间块作为核心组件，它采用卷积和通道注意力来增强普通曼巴的能力。通过这种方式，我们的 MambaIR 利用局部补丁重现先验以及通道交互来生成特定于恢复的特征表示。大量实验证明了我们方法的优越性，例如，使用类似的计算成本但具有全局感受野，MambaIR 的性能比基于 Transformer 的基线 SwinIR 提高了 0.36dB。代码可在 \url{https://github.com/csguoh/MambaIR} 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.15648</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:41 GMT</pubDate>
    </item>
    <item>
      <title>Cohere3D：利用时间一致性进行基于视觉的自动驾驶的无监督表示学习</title>
      <link>https://arxiv.org/abs/2402.15583</link>
      <description><![CDATA[arXiv:2402.15583v1 公告类型：新
摘要：由于图像中缺乏深度线索，多帧输入对于自动驾驶中基于视觉的感知、预测和规划的成功非常重要。如果我们能够识别不同输入帧中的相同实例，那么从不同角度进行观察就可以从 2D 图像输入中恢复 3D 对象状态。然而，自动驾驶场景的动态特性导致相机在不同时间步捕获的每个实例的外观和形状发生显着变化。为此，我们提出了一种新颖的对比学习算法 Cohere3D，用于学习长期输入序列中对距离和视角变化具有鲁棒性的连贯实例表示。学习到的表示有助于下游任务中跨多个输入帧的实例级对应。在预训练阶段，利用激光雷达传感器的原始点云来构建每个实例的长期时间对应关系，这为从基于视觉的鸟瞰图（BEV）中提取实例级表示提供指导特征图。 Cohere3D 鼓励同一实例在不同帧上保持一致的表示，但区分不同实例的表示。我们通过对各种下游感知、预测和规划任务的预训练模型进行微调来评估我们的算法。结果显示数据效率和任务性能都有显着提高。]]></description>
      <guid>https://arxiv.org/abs/2402.15583</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>事件相机的状态空间模型</title>
      <link>https://arxiv.org/abs/2402.15584</link>
      <description><![CDATA[arXiv:2402.15584v1 公告类型：新
摘要：如今，处理事件摄像机数据的最先进的深度神经网络首先将事件的时间窗口转换为密集的网格状输入表示。因此，当部署在比训练时更高的推理频率（即更小的时间窗口）时，它们表现出较差的通用性。我们通过将具有可学习时间尺度参数的状态空间模型（SSM）引入基于事件的视觉来应对这一挑战。这种设计适应不同的频率，而不需要在不同的频率下重新训练网络。此外，我们研究了两种策略来抵消以较高频率部署模型时的混叠效应。我们根据基于 RNN 和 Transformer 架构的现有方法，跨各种基准（包括 Gen1 和 1 Mpx 事件相机数据集）全面评估我们的方法。我们的结果表明，基于 SSM 的模型训练速度提高了 33%，并且在比训练输入更高的频率进行测试时，性能下降也最小。传统 RNN 和 Transformer 模型表现出超过 20 mAP 的性能下降，其中 SSM 下降了 3.31 mAP，凸显了 SSM 在基于事件的视觉任务中的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.15584</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>使用异质教师提炼对抗鲁棒性</title>
      <link>https://arxiv.org/abs/2402.15586</link>
      <description><![CDATA[arXiv:2402.15586v1 公告类型：新
摘要：在错误分类会产生巨大成本的领域（例如自动驾驶汽车或医学成像）部署神经网络分类器之前，必须实现对抗性攻击的弹性。最近的工作表明，可以使用知识蒸馏将鲁棒性从经过对抗性训练的教师转移到学生模型。然而，当前的方法使用单个对抗性和普通教师进行蒸馏，并考虑容易对来自类似对抗性子空间的示例进行错误分类的同质架构（即残差网络）。在这项工作中，我们通过使用异构教师（DARHT）提炼对抗性鲁棒性，开发了一个针对对抗性攻击的防御框架。在 DARHT 中，学生模型在学生-教师特征图中明确表示教师逻辑，并利用表现出低对抗性示例可转移性的多个教师（即，在不同的对抗性示例上表现出高性能）。白盒和黑盒场景中的分类任务实验表明，与 CIFAR-10、CIFAR-100 和 Tiny 中的竞争性对抗训练和蒸馏方法相比，DARHT 实现了最先进的干净且稳健的准确度ImageNet 数据集。与同质和异质教师集的比较表明，利用具有低对抗性示例可转移性的教师可以提高学生模型的稳健性。]]></description>
      <guid>https://arxiv.org/abs/2402.15586</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:40 GMT</pubDate>
    </item>
    <item>
      <title>表结构识别变压器的自监督预训练</title>
      <link>https://arxiv.org/abs/2402.15578</link>
      <description><![CDATA[arXiv:2402.15578v1 公告类型：新
摘要：表格结构识别（TSR）旨在将表格图像转换为机器可读的格式。尽管混合卷积神经网络（CNN）-变压器架构在现有方法中广泛使用，但线性投影变压器由于其简单性和效率而在许多视觉任务中优于混合架构。然而，现有研究表明，直接用线性投影替换 CNN 主干会导致性能明显下降。在这项工作中，我们通过提出一种 TSR 变压器的自监督预训练（SSP）方法来解决这个问题。我们发现，线性投影变换器和混合 CNN 变换器之间的性能差距可以通过 TSR 模型中视觉编码器的 SSP 来缩小。我们进行了可重复的消融研究，并在 https://github.com/poloclub/unitable 开源了我们的代码，以提高透明度、激发创新并促进我们领域内的公平比较，因为表格是表示学习的一种有前景的方式。]]></description>
      <guid>https://arxiv.org/abs/2402.15578</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    <item>
      <title>CI w/o TN：没有任务名称的上下文注入用于程序规划</title>
      <link>https://arxiv.org/abs/2402.15579</link>
      <description><![CDATA[arXiv:2402.15579v1 公告类型：新
摘要：本文探讨了教学视频中程序规划的挑战，其中涉及根据视频中的视觉起点和目标观察创建目标导向的计划。之前的研究通过逐渐弱化的训练监督来解决这个问题，从大量的中间视觉观察或语言指令到任务类监督。然而，随着大型语言模型的出现，即使只给出任务名称，这些模型也可以产生详细的计划。在这项研究中，我们提出了一个更弱的设置，没有任务名称作为监督，目前现有的大型语言模型无法解决这个问题，因为它们需要良好的提示和足够的信息。具体来说，我们假设之前的中间监督可以作为上下文信息，并且我们使用视觉开始和目标观察的标题作为一种更便宜的监督形式。这种方法大大降低了标记成本，因为可以通过大型预训练视觉语言基础模型轻松获得字幕。从技术上讲，我们应用 BLIP 生成字幕作为监督，通过对比学习损失来训练上下文特征。然后，上下文特征被输入生成器以帮助生成计划。我们对两个不同规模的数据集的实验表明，我们的模型可以在多个指标上实现可比的性能，这验证了我们的假设。]]></description>
      <guid>https://arxiv.org/abs/2402.15579</guid>
      <pubDate>Tue, 27 Feb 2024 06:16:39 GMT</pubDate>
    </item>
    </channel>
</rss>