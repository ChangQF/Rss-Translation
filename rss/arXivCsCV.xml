<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Fri, 01 Dec 2023 06:18:46 GMT</lastBuildDate>
    <item>
      <title>PEAN：一种基于扩散的先验增强注意网络，用于场景文本图像超分辨率。 （arXiv：2311.17955v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17955</link>
      <description><![CDATA[场景文本图像超分辨率（STISR）旨在同时提高
低分辨率场景文本图像的分辨率和可读性，因此
提高下游识别任务的性能。两个因素
场景文本图像、语义信息和视觉结构，影响着
识别性能显着。为了减轻这些影响
因素，本文提出了先验增强注意力网络（PEAN）。
具体来说，开发了一个基于扩散的模块来增强文本先验，
从而为 SR 网络生成 SR 图像提供更好的指导
更高的语义准确性。与此同时，拟议的 PEAN 利用了
基于注意力的调制模块可以巧妙地理解场景文本图像
感知图像的局部和全局依赖性，尽管图像的形状
文本。采用多任务学习范式来优化网络，
使模型能够生成清晰的 SR 图像。结果，PEAN 建立
TextZoom 基准测试的新 SOTA 结果。还进行了实验
分析增强文本先验作为改进文本的手段的重要性
SR网络的性能。代码将在以下位置提供：
https://github.com/jdfxzzy/PEAN。
]]></description>
      <guid>http://arxiv.org/abs/2311.17955</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:46 GMT</pubDate>
    </item>
    <item>
      <title>QuadraNet：通过硬件感知二次神经网络提高高阶神经交互效率。 （arXiv：2311.17956v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2311.17956</link>
      <description><![CDATA[面向计算机视觉的神经网络设计的最新进展主要是
通过捕获输入和特征之间的高阶神经交互来驱动。
出现了多种方法来实现这一目标，例如
变形金刚及其变种。然而，这些相互作用产生了巨大的
大量的中间状态和/或强数据依赖性，导致
相当大的内存消耗和计算成本，因此妥协
整体运行时性能。为了应对这一挑战，我们重新思考
具有二次计算的高阶交互式神经网络设计
方法。具体来说，我们提出了 QuadraNet——一种综合模型设计
从神经元重建到结构块并最终到的方法论
整体神经网络的实现。利用二次神经元
固有的高阶优势和专用的计算优化方案，
QuadraNet可以有效实现最优的认知和计算
表现。结合最先进的硬件感知神经架构
搜索和系统集成技术，QuadraNet 也可以很好
在不同的硬件约束设置和部署场景中通用。
实验表明QuadraNet实现了高达1.5$\times$的吞吐量，30%
与相比，内存占用更少，认知性能相似
最先进的高阶方法。
]]></description>
      <guid>http://arxiv.org/abs/2311.17956</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:46 GMT</pubDate>
    </item>
    <item>
      <title>同步视觉和语言：用于参考图像分割的双向令牌掩码自动编码器。 （arXiv：2311.17952v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17952</link>
      <description><![CDATA[参考图像分割（RIS）旨在分割所表达的目标对象
在像素级别的场景中以自然语言表示。最近的各种RIS
模型通过生成上下文实现了最先进的性能
令牌可以有效地对来自预训练编码器的多模态特征进行建模
使用基于变压器的跨模式注意力融合它们。虽然这些方法
将语言特征与图像特征进行匹配，有效识别可能的
目标对象，他们经常难以正确理解上下文
复杂且模棱两可的句子和场景中的信息。为了解决这个问题
问题，我们提出了一种新颖的双向令牌屏蔽自动编码器（BTMAE）
受到屏蔽自动编码器（MAE）的启发。所提出的模型学习上下文
通过重建缺失的特征来实现图像到语言和语言到图像的转换
在令牌级别的图像和语言特征上。换句话说，这
方法涉及图像和图像特征之间的相互补充
语言，重点是使网络能够理解互连的
两种模式之间的深层上下文信息。这种学习方法
增强了RIS在复杂句子和场景中表现的鲁棒性。我们的
BTMAE 在三个流行数据集上实现了最先进的性能，我们
通过各种消融证明所提出方法的有效性
学习。
]]></description>
      <guid>http://arxiv.org/abs/2311.17952</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>重新思考生成式人工智能革命时代的图像编辑检测。 （arXiv：2311.17953v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17953</link>
      <description><![CDATA[生成式人工智能的加速发展显着增强了
生成区域编辑方法的可行性和有效性。这
进化使图像处理变得更容易，从而强化
改变原始图像中传达的信息的风险，甚至
传播错误信息。因此，存在迫切的需求
能够检测编辑后的图像。然而，缺乏
包含经过丰富且先进的编辑的图像的综合数据集
生成区域编辑方法对
相应检测方法的进展。

我们努力通过构建 GRE 数据集来填补这一空缺，
大规模生成区域编辑数据集具有以下优点：
1）真实世界原始图像的集合，重点关注两个经常编辑的图像
场景。 2) 逻辑和模拟编辑管道的集成，
以各种方式利用多个大型模型。 3）包含各种
具有不同架构的编辑方法。 4) 提供全面的
分析任务。我们对提出的三项任务进行了全面的实验：
编辑图像分类、编辑方法属性和编辑区域
本地化，提供不同编辑方法的分析和评估
相关领域的检测方法。我们期望 GRE 数据集能够促进
生成区域编辑领域的进一步研究和探索
检测。
]]></description>
      <guid>http://arxiv.org/abs/2311.17953</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>Transformer 支持的多模式项目嵌入，用于增强电子商务中的图像搜索。 （arXiv：2311.17954v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17954</link>
      <description><![CDATA[在过去的十年中，该领域取得了重大进展
电子商务应用程序的图像搜索。传统的图像到图像检索
仅关注纹理等图像细节的模型往往会忽略
图像中包含有用的语义信息。结果，
检索到的产品可能具有相似的图像细节，但无法满足
用户的搜索目标。此外，使用图像到图像检索模型
包含多个图像的产品会产生重要的在线产品
特征存储开销和复杂的映射实现。在本文中，我们
报告拟议的多模式项目嵌入的设计和部署
模型（MIEM）来解决这些限制。它能够同时利用
关于产品的文本信息和多个图像构建有意义的
产品特点。通过利用图像中的语义信息，MIEM
有效补充图像搜索过程，提高整体
检索结果的准确性。 MIEM已成为Shopee不可分割的一部分
图片搜索平台。自 2023 年 3 月部署以来，已实现
每位用户的点击次数显着增加 9.90%，用户点击量增加 4.23%
Shopee 电商图片搜索功能的每用户订单量
平台。
]]></description>
      <guid>http://arxiv.org/abs/2311.17954</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:45 GMT</pubDate>
    </item>
    <item>
      <title>零样本检索：使用搜索引擎增强预训练模型。 （arXiv：2311.17949v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17949</link>
      <description><![CDATA[大型预训练模型可以显着减少特定任务的数量
解决问题所需的数据，但他们常常无法捕获
开箱即用的特定领域的细微差别。网络可能包含以下信息
需要在任何特定应用中表现出色，但要识别正确的数据
先验是具有挑战性的。本文展示了如何利用最新进展
NLP 和多模态学习通过搜索引擎增强预训练模型
恢复。我们建议在测试时从网络上检索有用的数据
模型不确定的测试用例。与现有的不同
检索增强方法，然后我们更新模型来解决这个问题
潜在的不确定性。我们展示了零样本方面的重大改进
性能，例如准确率显着提高 15 个百分点
斯坦福汽车和鲜花数据集。我们还提供了广泛的实验
探索噪声检索和不同学习策略的影响。
]]></description>
      <guid>http://arxiv.org/abs/2311.17949</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>通过各种骨干和统计匹配的广义大规模数据压缩。 （arXiv：2311.17950v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17950</link>
      <description><![CDATA[SRe2L引入的轻量级“局部-匹配-全局”匹配
成功创建了一个包含全面信息的蒸馏数据集
完整的 224x224 ImageNet-1k。然而，这种片面的做法仅限于
特定的骨干网、层和统计数据，限制了性能的提高
蒸馏数据集的泛化。我们建议充分且多样化
“本地-匹配-全局”匹配比单一匹配更精准、更有效
并能够创建包含更丰富信息的精炼数据集
更好的概括。我们将这种观点称为“广义匹配”
在此提出广义各种骨干网和统计匹配（G-VBSM）
工作，旨在创建一个具有密度的合成数据集，确保
与各个主干网、层和完整数据集的一致性
统计数据。实验证明，G-VBSM 是第一个
在小规模和大规模数据集上获得强大的性能。
具体来说，G-VBSM 在 CIFAR-100 上实现了 38.7% 的性能
128 宽度的 ConvNet，在带有 ResNet18 的 Tiny-ImageNet 上为 47.6%，在完整图像上为 31.4%
224x224 ImageNet-1k with ResNet18，每类图像 (IPC) 10、50 和 10，
分别。这些结果比所有 SOTA 方法高出 3.9%、6.5%、
和 10.1%。
]]></description>
      <guid>http://arxiv.org/abs/2311.17950</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:44 GMT</pubDate>
    </item>
    <item>
      <title>对比视觉-语言对齐使学习者的教学更加高效。 （arXiv：2311.17945v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17945</link>
      <description><![CDATA[我们研究将大语言模型（LLM）扩展为
视觉语言指令遵循模型。这项任务至关重要，但
由于法学硕士仅接受文本形式的培训，因此很难
有效消化视觉形态。为了解决这个问题，现有的方法
通常训练一个视觉适配器来对齐之间的表示
预训练视觉变换器 (ViT) 和生成图像的法学硕士
字幕损失。然而，我们发现生成目标只能
产生视觉和语言的弱对齐，使得对齐
视觉语言模型非常渴望指令微调数据。在这个
论文中，我们提出了同时应用对比对齐和生成对齐的 CG-VLM
目标是有效协调 ViT 和 LLM 的代表性。不同的
从常见对比学习中的图像级别和句子级别对齐
设置，CG-VLM 对齐图像补丁级别特征和文本标记级别
嵌入，然而，由于没有明确的基础，很难实现
标准图像字幕数据集中提供的补丁标记关系。讲话
这个问题，我们建议最大化池化之间的平均相似度
图像补丁特征和文本标记嵌入。广泛的实验
证明所提出的 CG-VLM 产生了强大的视觉语言对齐
并且是一个高效的教学学习者。例如，仅使用 10%
指令调优数据，我们达到了最先进方法的 95% 性能
LLaVA [29] 零样本 ScienceQA-Image 基准测试。
]]></description>
      <guid>http://arxiv.org/abs/2311.17945</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>DreamSync：将文本到图像的生成与图像理解反馈结合起来。 （arXiv：2311.17946v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17946</link>
      <description><![CDATA[尽管文本到图像模型 (T2I) 取得了广泛的成功，但仍然举步维艰
产生既美观又忠实于主题的图像
用户输入的文本。我们引入 DreamSync，一种与模型无关的训练算法
通过设计改进 T2I 模型以忠实于文本输入。梦同步
基于 TIFA 评估框架的最新见解——大
视觉语言模型（VLM）可以有效地识别细粒度的
生成的图像和文本输入之间的差异。 DreamSync 使用这个
在没有任何标记数据的情况下训练 T2I 模型的洞察力；它改进了 T2I 模型
使用自己的世代。首先，它提示模型生成几个
给定输入文本的候选图像。然后，它使用两个 VLM 来选择
最佳一代：测量对齐的视觉问答模型
生成的图像到文本，另一个衡量生成的
审美品质。选择后，我们使用 LoRA 迭代微调 T2I
模型引导其一代走向选定的最佳一代。梦同步
不需要任何额外的人工注释。模型架构变化，或者
强化学习。尽管很简单，DreamSync 仍改进了
两个基于扩散的 T2I 模型的语义对齐和审美吸引力，
多个基准证明了这一点（TIFA +1.7%、DSG1K +2.9%、VILA +3.4%）
审美）和人类评价。
]]></description>
      <guid>http://arxiv.org/abs/2311.17946</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>动作槽：交通场景中多标签原子活动识别的以视觉动作为中心的表示。 （arXiv：2311.17948v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17948</link>
      <description><![CDATA[在本文中，我们研究了多标签原子活动识别。尽管
动作识别取得了显着进展，但识别仍然具有挑战性
由于缺乏对两者的整体理解而导致的原子活动
多个道路使用者的动作及其上下文信息。在本文中，
我们引入了 Action-slot，这是一种基于插槽注意力的方法，可以学习视觉
以动作为中心的表示，捕捉动作和上下文
信息。我们的核心理念是设计能够支付的动作老虎机
关注原子活动发生的区域，而不需要
明确的感知指导。为了进一步提高老虎机注意力，我们引入了
与动作槽竞争的背景槽，有助于训练过程
避免不必要地关注没有活动的背景区域。然而，
现有数据集中不平衡的类别分布阻碍了评估
罕见的活动。为了解决该限制，我们收集了一个合成数据集
称为 TACO，比 OATS 大四倍，具有平衡的特点
原子活动的分布。为了验证我们方法的有效性，
我们针对各种情况进行全面的实验和消融研究
动作识别基线。我们还表明，多标签的性能
现实世界数据集上的原子活动识别可以通过以下方式改进
TACO 上的预训练表示。我们将发布我们的源代码并
数据集。请参阅项目页面上的可视化视频：
https://hcis-lab.github.io/Action-slot/
]]></description>
      <guid>http://arxiv.org/abs/2311.17948</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:43 GMT</pubDate>
    </item>
    <item>
      <title>基于对象（但与类无关）的视频域适应。 （arXiv：2311.17942v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17942</link>
      <description><![CDATA[现有的基于视频的动作识别系统通常需要密集的
存在显着分布的环境中的注释和挣扎
相对于训练数据的变化。视频领域的当前方法
适应通常使用子集上的完全注释数据来微调模型
目标域数据或使用对齐两个域的表示
引导学习或对抗性学习。受到物体的关键作用的启发
在最近的有监督的以对象为中心的动作识别模型中，我们提出
基于对象（但与类无关）的视频域适应 (ODAPT)，一个简单但
使现有的动作识别系统适应新的有效框架
通过利用一组稀疏的框架和与类无关的对象来划分域
目标域中的注释。我们的模型实现了 +6.5 的增长
Epic-Kitchens 中的跨厨房适应能力提高 +3.1
Epic-Kitchens 和 EGTEA 数据集。 ODAPT 是一个通用框架，还可以
与之前的无监督方法相结合，当
与自监督多模态方法 MMSADA 相结合，提升 +1.7
当添加到 Epic-Kitchens 上基于对抗性的方法 TA$^3$N 时。
]]></description>
      <guid>http://arxiv.org/abs/2311.17942</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>LALM：使用语言模型进行长期行动预期。 （arXiv：2311.17944v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17944</link>
      <description><![CDATA[在以自我为中心的情况下，理解人类活动是一项至关重要但复杂的任务
视觉，一个专注于从相机捕捉视觉视角的领域
佩戴者的观点。虽然传统方法严重依赖代表性
在大量视频数据上训练的学习，存在显着的
限制：由于以下原因，获得有效的视频表示被证明具有挑战性
人类活动固有的复杂性和可变性。此外，
完全依赖基于视频的学习可能会限制模型的能力
泛化长尾类和分布外场景。

在这项研究中，我们引入了一种长期行动的新方法
使用语言模型（LALM）进行预测，擅长解决复杂的问题
无需广泛了解即可应对长期活动理解的挑战
训练。我们的方法结合了一个动作识别模型来跟踪之前的动作
动作序列和视觉语言模型来阐明相关
环境细节。通过利用这些过去事件提供的背景，
我们使用大语言设计了一种针对行动预期的提示策略
模型（法学硕士）。此外，我们还实现了最大边际相关性
选择以促进法学硕士的情境学习。我们的实验
结果表明 LALM 在以下方面超越了最先进的方法
Ego4D 基准上的长期行动预期任务。我们进一步
在另外两个基准上验证 LALM，确认其能力
具有不同分类法集的复杂活动的概括。
这些都是无需具体微调即可实现的。
]]></description>
      <guid>http://arxiv.org/abs/2311.17944</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:42 GMT</pubDate>
    </item>
    <item>
      <title>解锁文本到图像扩散模型中的空间理解。 （arXiv：2311.17937v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17937</link>
      <description><![CDATA[我们提出了 CompFuser，一种增强空间图像生成管道
文本到图像生成模型中的理解和属性分配。我们的
管道能够解释定义空间的指令
场景中对象之间的关系，例如“一只灰猫的图像”
橙色狗的左边&#39;，并生成相应的图像。这是
为了向用户提供更多控制尤其重要。计算机融合器
通过解码克服现有文本到图像扩散模型的限制
将多个对象的生成分为迭代步骤：首先生成一个
单个对象，然后通过将其他对象放入其中来编辑图像
指定职位。创建空间理解和训练数据
属性分配我们引入了一个合成数据生成过程，即
利用冻结的大语言模型和冻结的基于布局的扩散
对象放置模型。我们将我们的方法与强大的基线进行比较
表明我们的模型在以下方面优于最先进的图像生成模型
空间理解和属性分配，尽管尺寸小了 3 到 5 倍
在参数中。
]]></description>
      <guid>http://arxiv.org/abs/2311.17937</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>主动开放词汇识别：让智能移动减轻 CLIP 限制。 （arXiv：2311.17938v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17938</link>
      <description><![CDATA[主动识别，允许智能代理探索观察结果
为了获得更好的识别性能，作为各种的先决条件
体现人工智能任务，例如抓取、导航和房间布置。给定
不断变化的环境和众多的对象类别，这是不切实际的
包括培训阶段所有可能的课程。在本文中，我们
旨在促进积极的开放词汇识别，增强实体主体的能力
主动感知和分类任意物体。不过直接采用
最近的开放词汇分类模型，例如对比语言图像
预训练（CLIP）提出了其独特的挑战。具体来说，我们观察到
CLIP的性能很大程度上受到视点和遮挡的影响，
在不受约束的具体感知场景中损害其可靠性。
此外，主体环境中观察的顺序性质
交互需要一种有效的方法来集成功能，
保持开放词汇分类的辨别力。到
为了解决这些问题，我们引入了一种新的主​​动开放词汇代理
认出。所提出的方法利用了框架间和概念间
导航代理移动和融合功能的相似之处，无需依赖
关于特定班级的知识。与基线 CLIP 模型相比，提高了 29.6%
在 ShapeNet 数据集上的准确率，所提出的代理可以达到 53.3% 的准确率
用于开放词汇识别，无需对配备的 CLIP 进行任何微调
模型。使用栖息地模拟器进一步进行了额外的实验
肯定我们方法的有效性。
]]></description>
      <guid>http://arxiv.org/abs/2311.17938</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:41 GMT</pubDate>
    </item>
    <item>
      <title>场景摘要：将场景视频聚类成空间多样化的帧。 （arXiv：2311.17940v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2311.17940</link>
      <description><![CDATA[我们提出场景摘要作为一种新的基于视频的场景理解任务。
它的目的是将一个场景的长视频演练总结为一小部分
场景中空间多样化的帧，这有许多重要的意义
应用，例如监控、房地产和机器人技术。它源于
视频摘要，但重点关注移动中的长且连续的视频
摄像机，而不是更常见的用户编辑的碎片视频剪辑
研究了现有的视频摘要作品。我们对这个任务的解决方案是
名为 SceneSum 的两阶段自监督管道。它的第一阶段使用
聚类来分割视频序列。我们的核心理念是结合视觉
地点识别（VPR）进入该聚类过程以促进空间
多样性。第二阶段需要从每个关键帧中选择一个有代表性的关键帧
集群作为摘要，同时尊重资源限制，例如内存和
磁盘空间限制。此外，如果地面真实图像轨迹是
可用，我们的方法可以很容易地通过监督损失来增强
聚类和关键帧选择。在现实世界中进行了大量的实验
模拟数据集表明我们的方法优于常见的视频摘要
基线 50%
]]></description>
      <guid>http://arxiv.org/abs/2311.17940</guid>
      <pubDate>Fri, 01 Dec 2023 06:18:41 GMT</pubDate>
    </item>
    </channel>
</rss>