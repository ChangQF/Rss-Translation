<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 31 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>SAM-E：利用序列模仿的视觉基础模型进行具身操控</title>
      <link>https://arxiv.org/abs/2405.19586</link>
      <description><![CDATA[arXiv:2405.19586v1 公告类型：新
摘要：在 3D 操作中获得多任务模仿策略对场景理解和动作预测提出了挑战。当前的方法采用 3D 表示和多视图 2D 表示来预测机器人末端执行器的姿势。然而，它们仍然需要大量高质量的机器人轨迹，并且在看不见的任务中泛化有限，在长期推理中执行效率低下。在本文中，我们提出了一种用于机器人操作的新型架构 SAM-E，它利用视觉基础模型进行可泛化的场景理解和序列模仿进行长期动作推理。具体来说，我们采用在大量图像和可提示掩码上预训练的 Segment Anything (SAM) 作为提取任务相关特征的基础模型，并对机器人数据进行参数高效的微调，以更好地理解具体场景。为了解决长远推理问题，我们开发了一种新颖的多通道热图，能够一次性预测动作序列，显著提高执行效率。来自各种指令跟踪任务的实验结果表明，与基线相比，SAM-E 实现了卓越的性能和更高的执行效率，并且还显著提高了对新任务的少样本适应的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2405.19586</guid>
      <pubDate>Fri, 31 May 2024 06:20:28 GMT</pubDate>
    </item>
    <item>
      <title>利用集成和布尔基元改进凸分解</title>
      <link>https://arxiv.org/abs/2405.19569</link>
      <description><![CDATA[arXiv:2405.19569v1 公告类型：新
摘要：用图元（几何上简单的形状，提供简约但准确的结构抽象）来描述场景是一个既定的视觉问题。这是一个困难的拟合问题的良好模型：不同的场景需要不同数量的图元，并且图元之间相互作用强烈，但任何提出的解决方案都可以在推理时进行评估。最先进的方法涉及学习回归程序以预测由固定数量图元组成的起点，然后使用下降方法来细化几何并去除冗余图元。方法通过深度和正常预测以及场景分割的准确性进行评估。本文表明，通过（a）合并少量负图元和（b）在多个不同的回归程序上进行集成，可以获得非常显着的准确性改进。集成是通过细化每个预测的起点，然后通过拟合损失选择最佳点来实现的。在标准数据集上进行的大量实验证实，负基元在大部分图像中都是有用的，并且我们的“先细化再选择”策略优于“先选择再细化”，从而证实了拟合问题非常困难。]]></description>
      <guid>https://arxiv.org/abs/2405.19569</guid>
      <pubDate>Fri, 31 May 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>通过快速扩散反演实现盲图像恢复</title>
      <link>https://arxiv.org/abs/2405.19572</link>
      <description><![CDATA[arXiv:2405.19572v1 公告类型：新
摘要：最近，人们提出了各种方法来解决图像恢复 (IR) 任务，使用预先训练的扩散模型，从而获得最先进的性能。然而，这些方法中的大多数都假设 IR 任务中的退化算子是完全已知的。此外，这些方法的一个共同特点是它们改变扩散采样过程以满足与退化输入图像的一致性。最近发现这种选择不是最优的，会导致恢复的图像偏离数据流形。为了解决这些问题，我们提出了通过快速扩散反演 (BIRD) 进行盲图像恢复，这是一种盲 IR 方法，可联合优化退化模型参数和恢复的图像。为了确保恢复的图像位于数据流形上，我们提出了一种基于预训练扩散模型的新型采样技术。我们方法的一个关键思想是，一旦对初始噪声进行采样，就不要修改反向采样，即不要改变所有中间潜变量。这最终相当于将 IR 任务视为输入噪声空间中的优化问题。此外，为了减轻与反转完全展开的扩散模型相关的计算成本，我们利用这些模型的固有能力，使用较大的时间步长跳过前向扩散过程。我们在几个图像恢复任务上通过实验验证了 BIRD，并表明它在所有这些任务上都实现了最先进的性能。我们的代码可在 https://github.com/hamadichihaoui/BIRD 上找到。]]></description>
      <guid>https://arxiv.org/abs/2405.19572</guid>
      <pubDate>Fri, 31 May 2024 06:20:27 GMT</pubDate>
    </item>
    <item>
      <title>组织背景以探索增量少样本语义分割的潜在类别</title>
      <link>https://arxiv.org/abs/2405.19568</link>
      <description><![CDATA[arXiv:2405.19568v1 公告类型：新
摘要：增量式少样本语义分割（iFSS）的目标是通过少量带注释的图像将预训练的分割模型扩展到新类别，而无需访问旧的训练数据。在增量学习新类别的过程中，旧类别的数据分布将被破坏，导致灾难性遗忘。同时，新类别只有很少的样本，使得模型无法学习新类别的令人满意的表示。针对 iFSS 问题，我们提出了一个名为 OINet 的网络，即背景嵌入空间 \textbf{O}rganization 和原型 \textbf{I}nherit 网络。具体而言，在训练基类时，OINet 使用多个分类头作为背景，并设置多个子类原型来为潜在的新类别保留嵌入空间。在增量学习新类别的过程中，我们提出了一种策略来选择与当前学习的新类别最匹配的子类原型，并使新类别继承所选原型的嵌入空间。此操作允许使用少量样本将新类别注册到嵌入空间中，而不会影响基类的分布。Pascal-VOC 和 COCO 上的结果表明 OINet 达到了新的最佳水平。]]></description>
      <guid>https://arxiv.org/abs/2405.19568</guid>
      <pubDate>Fri, 31 May 2024 06:20:26 GMT</pubDate>
    </item>
    <item>
      <title>MDS-ViTNet：使用 Vision Transformer 改进眼动追踪的显著性预测</title>
      <link>https://arxiv.org/abs/2405.19501</link>
      <description><![CDATA[arXiv:2405.19501v1 公告类型：新
摘要：在本文中，我们提出了一种称为 MDS-ViTNet（视觉变换器网络的多解码器显着性）的新方法，用于增强视觉显着性预测或眼动追踪。这种方法对营销、医学、机器人和零售等不同领域都具有巨大潜力。我们提出了一种利用 Vision Transformer 的网络架构，超越了传统的 ImageNet 主干。该框架采用编码器-解码器结构，编码器利用 Swin 变压器有效地嵌入最重要的特征。该过程涉及迁移学习方法，其中 Vision Transformer 中的层由编码器变压器转换并无缝集成到 CNN 解码器中。这种方法可确保原始输入图像的信息损失最小。解码器采用多解码技术，利用双解码器生成两个不同的注意力图。随后，这些图通过额外的 CNN 模型组合成一个单一的输出。我们训练的模型 MDS-ViTNet 在多个基准测试中取得了最佳结果。我们致力于促进进一步合作，打算将我们的代码、模型和数据集向公众开放。]]></description>
      <guid>https://arxiv.org/abs/2405.19501</guid>
      <pubDate>Fri, 31 May 2024 06:20:25 GMT</pubDate>
    </item>
    <item>
      <title>使用动态生长的子网络树进行终身学习以实现视频对象分割中的域泛化</title>
      <link>https://arxiv.org/abs/2405.19525</link>
      <description><![CDATA[arXiv:2405.19525v1 公告类型：新
摘要：当前最先进的视频对象分割模型使用具有大量标记训练数据集的监督学习取得了巨大成功。但是，这些模型是使用单个源域进行训练的，并使用从同一源域采样的视频进行评估。当使用从不同目标域采样的视频评估这些模型时，由于域泛化能力差，即无法使用传统监督学习同时从多域源中学习，它们的性能会显著下降。在本文中，我们提出了一种动态增长的子网络树 (DGT)，以便有效地从多域源中学习。DGT 使用一种新颖的终身学习技术，使模型能够持续有效地从新域中学习，而不会忘记以前学习过的域。因此，该模型可以推广到域外视频。使用单源域内（传统视频对象分割）、多源域内和多源域外视频对象分割对所提出的工作进行了评估。 DGT 的结果显示，在 DAVIS16 和 DAVIS17 数据集上，单源域内性能分别提高了 0.2% 和 3.5%。然而，当使用域内多源评估 DGT 时，结果显示其性能优于最先进的视频对象分割和其他终身学习技术，F 分数的平均性能提高了 6.9%，灾难性遗忘最小。最后，在域外实验中，DGT 的性能在 1 次和 5 次拍摄中分别比最先进的技术高出 2.7% 和 4%。]]></description>
      <guid>https://arxiv.org/abs/2405.19525</guid>
      <pubDate>Fri, 31 May 2024 06:20:25 GMT</pubDate>
    </item>
    <item>
      <title>MemControl：通过自动参数选择减少医学传播模型中的记忆</title>
      <link>https://arxiv.org/abs/2405.19458</link>
      <description><![CDATA[arXiv:2405.19458v1 公告类型：新
摘要：扩散模型在生成与训练分布非常相似的图像方面表现出非凡的能力。然而，这些模型容易出现训练数据记忆，导致严重的隐私、道德和法律问题，特别是在医学成像等敏感领域。我们假设记忆是由深度模型的过度参数化驱动的，这表明在微调过程中规范模型容量可能是一种有效的缓解策略。参数高效微调 (PEFT) 方法通过有选择地更新特定参数，为容量控制提供了一种有前途的方法。然而，找到平衡生成质量和记忆的可学习参数的最佳子集仍然难以捉摸。为了应对这一挑战，我们提出了一个双层优化框架，该框架利用记忆和生成质量指标作为奖励来指导自动参数选择。我们的框架成功地确定了要更新的最佳参数集，以满足生成-记忆权衡。我们针对医学图像生成这一特定任务进行了实验，通过微调少至 0.019% 的模型参数，我们的实验结果超越了现有的最先进的训练时间缓解策略。此外，我们还表明，通过我们的框架学习到的策略可以跨不同的数据集和领域进行迁移。我们提出的框架可扩展到大型数据集，并且不受奖励函数选择的影响。最后，我们表明，我们的框架可以与现有方法相结合，以进一步缓解记忆。]]></description>
      <guid>https://arxiv.org/abs/2405.19458</guid>
      <pubDate>Fri, 31 May 2024 06:20:24 GMT</pubDate>
    </item>
    <item>
      <title>RAP：使用稀疏相关适配器进行高效的文本视频检索</title>
      <link>https://arxiv.org/abs/2405.19465</link>
      <description><![CDATA[arXiv:2405.19465v1 公告类型：新
摘要：文本视频检索 (TVR) 旨在将相关视频内容与自然语言查询对齐。迄今为止，大多数最先进的 TVR 方法都是基于大规模预训练视觉语言模型（例如 CLIP）进行图像到视频的迁移学习。但是，完全微调这些用于 TVR 的预训练模型会产生非常昂贵的计算成本。为此，我们建议使用稀疏相关适配器 (RAP) 进行高效的文本视频检索，即使用一些参数化层对预训练模型进行微调。为了适应文本视频场景，我们为 RAP 配备了两个不可或缺的特性：时间稀疏性和相关性。具体而言，我们提出了一个低秩调制模块来细化冻结 CLIP 主干中的每幅图像特征，这可以突出视频特征中的显着帧，同时减轻时间冗余。此外，我们引入了一种异步自注意力机制，该机制首先选择响应度最高的视觉块，然后通过可学习的时间和块偏移量增强它们之间的相关性建模。在四个 TVR 数据集上进行的大量实验表明，与完全微调的对应方法和其他参数高效的微调方法相比，RAP 实现了更优或相当的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.19465</guid>
      <pubDate>Fri, 31 May 2024 06:20:24 GMT</pubDate>
    </item>
    <item>
      <title>通过运动平均实现大规模 DSM 配准</title>
      <link>https://arxiv.org/abs/2405.19442</link>
      <description><![CDATA[arXiv:2405.19442v1 公告类型：新
摘要：生成广域数字表面模型 (DSM) 需要注册大量单独的、部分重叠的 DSM。这对典型的注册算法来说是一个具有挑战性的问题，因为当考虑来自这些多个 DSM 的大量观测值时，很容易导致内存溢出。顺序注册算法虽然可以显著减少计算量，但对于小的重叠对尤其容易受到影响，导致大量的误差积累。在这项工作中，我们提出了一种新颖的解决方案，将 DSM 注册任务构建为运动平均问题：成对的 DSM 被注册以构建场景图，其中边缘表示 DSM 之间的相对姿势。具体而言，基于大型 DSM 的网格结构，使用新颖的最近邻搜索方法执行成对注册。我们表明，可以通过具有 O(N) 复杂度的极快运动平均算法优化场景图（N 指图像数量）。对高分辨率卫星 DSM 的评估表明计算量和准确性有显著提高。]]></description>
      <guid>https://arxiv.org/abs/2405.19442</guid>
      <pubDate>Fri, 31 May 2024 06:20:23 GMT</pubDate>
    </item>
    <item>
      <title>FourierMamba：傅里叶学习与状态空间模型的集成，用于图像去雨</title>
      <link>https://arxiv.org/abs/2405.19450</link>
      <description><![CDATA[arXiv:2405.19450v1 公告类型：新
摘要：图像去雨旨在去除雨天图像中的雨纹并恢复清晰的背景。目前，一些采用傅里叶变换的研究已被证明对图像去雨有效，因为它可以作为捕捉雨纹的有效频率先验。然而，尽管图像中存在低频和高频的依赖性，但这些基于傅里叶的方法很少利用不同频率的相关性来结合它们的学习过程，限制了频率信息在图像去雨中的充分利用。相反，最近出现的 Mamba 技术展示了其在各个领域（例如空间、时间）建模相关性的有效性和效率，我们认为将 Mamba 引入其未探索的傅里叶空间以关联不同的频率将有助于改善图像去雨。这促使我们提出一个名为 FourierMamba 的新框架，该框架在傅里叶空间中使用 Mamba 执行图像去雨。由于傅里叶空间中频率顺序的独特排列，FourierMamba 的核心在于对不同频率的扫描编码，其中低高频顺序格式在空间维度（轴上无排列）和通道维度（轴上排列）上的表现不同。因此，我们设计了 FourierMamba，以不同的设计在空间和通道维度上关联傅里叶空间信息。具体而言，在空间维度傅里叶空间中，我们引入锯齿编码来扫描频率以从低频到高频重新排列顺序，从而有序关联频率之间的联系；在频率轴上有序排列的通道维度傅里叶空间中，我们可以直接使用 Mamba 进行频率关联，改善通道信息表示。]]></description>
      <guid>https://arxiv.org/abs/2405.19450</guid>
      <pubDate>Fri, 31 May 2024 06:20:23 GMT</pubDate>
    </item>
    <item>
      <title>扩散策略攻击者：针对基于扩散的策略制定对抗性攻击</title>
      <link>https://arxiv.org/abs/2405.19424</link>
      <description><![CDATA[arXiv:2405.19424v1 公告类型：新
摘要：扩散模型 (DM) 已成为一种有前途的行为克隆 (BC) 方法。基于 DM 的扩散策略 (DP) 将 BC 性能提升到了新的高度，展示了在不同任务中的强大功效，以及其固有的灵活性和易于实施性。尽管越来越多地采用 DP 作为策略生成的基础，但关键的安全问题仍然在很大程度上未被探索。虽然之前的尝试针对深度策略网络，但 DP 使用扩散模型作为策略网络，由于其链式结构和注入的随机性，使用以前的方法对其进行攻击是无效的。在本文中，我们通过引入对抗场景，包括离线和在线攻击以及全局和基于补丁的攻击，对 DP 安全问题进行了全面的检查。我们提出了 DP-Attacker，这是一套可以在所有上述场景中设计有效对抗攻击的算法。我们对各种操作任务中的预训练扩散策略进行攻击。通过大量实验，我们证明了 DP-Attacker 能够显著降低所有场景的 DP 成功率。特别是在离线场景中，DP-Attacker 可以生成适用于所有帧的高度可转移扰动。此外，我们说明了对抗性物理补丁的创建，当应用于环境时，可以有效欺骗模型。视频结果放在：https://sites.google.com/view/diffusion-policy-attacker。]]></description>
      <guid>https://arxiv.org/abs/2405.19424</guid>
      <pubDate>Fri, 31 May 2024 06:20:22 GMT</pubDate>
    </item>
    <item>
      <title>保形递归特征消除</title>
      <link>https://arxiv.org/abs/2405.19429</link>
      <description><![CDATA[arXiv:2405.19429v1 公告类型：新
摘要：与传统统计方法不同，共形预测 (CP) 仅基于数据的可交换性即可确定与单个预测相关的有效和准确的置信水平。我们在此介绍一种利用 CP 框架的新型特征选择方法。我们的提案名为共形递归特征消除 (CRFE)，可识别并递归删除增加数据集不一致性的特征。我们还提出了 CRFE 的自动停止标准，以及一个用于衡量特征子集之间一致性的新指标。通过使用数据的多个分区，将 CRFE 选择与经典的递归特征消除 (RFE) 方法在几个多类数据集上进行比较。结果表明，CRFE 在半数数据集中的表现明显优于 RFE，而在其余数据集中实现了类似的性能。自动停止标准提供了有效和非冗余特征的子集，而无需计算任何分类性能。]]></description>
      <guid>https://arxiv.org/abs/2405.19429</guid>
      <pubDate>Fri, 31 May 2024 06:20:22 GMT</pubDate>
    </item>
    <item>
      <title>在双稳态图像上评估视觉语言模型</title>
      <link>https://arxiv.org/abs/2405.19423</link>
      <description><![CDATA[arXiv:2405.19423v1 公告类型：新
摘要：双稳态图像，也称为模糊图像或可逆图像，呈现的视觉刺激可以在两种不同的解释中看到，尽管观察者不能同时看到。在这项研究中，我们使用双稳态图像对视觉语言模型进行了迄今为止最广泛的检查。我们手动收集了 29 张双稳态图像及其相关标签的数据集，并对其进行了 116 种不同的亮度、色调和旋转操作。我们在六种模型架构中对 12 种不同的模型进行了分类和生成任务评估。我们的研究结果表明，除了 Idefics 系列和 LLaVA1.5-13b 中的模型外，模型中对一种解释的偏好明显高于另一种解释，在图像处理下差异最小，图像旋转几乎没有例外。此外，我们将模型偏好与人类进行了比较，注意到模型没有表现出与人类相同的连续性偏差，并且经常与人类的初始解释不同。我们还研究了提示变化和同义标签使用的影响，发现这些因素对模型解释的影响比图像处理更大，表明语言先验对双稳态图像解释的影响比图像文本训练数据更大。所有代码和数据都是开源的。]]></description>
      <guid>https://arxiv.org/abs/2405.19423</guid>
      <pubDate>Fri, 31 May 2024 06:20:21 GMT</pubDate>
    </item>
    <item>
      <title>视频异常检测十年回顾与展望</title>
      <link>https://arxiv.org/abs/2405.19387</link>
      <description><![CDATA[arXiv:2405.19387v1 公告类型：新
摘要：视频异常检测 (VAD) 在监控、医疗保健和环境监测等不同领域都具有重要意义。虽然许多调查都关注传统的 VAD 方法，但它们往往缺乏对具体方法和新兴趋势的深入探索。本调查探讨了基于深度学习的 VAD，超越了传统的监督训练范式，涵盖了新兴的弱监督、自监督和无监督方法。本综述的一个突出特点是调查了 VAD 范式中的核心挑战，包括大规模数据集、特征提取、学习方法、损失函数、正则化和异常分数预测。此外，本综述还研究了视觉语言模型 (VLM) 作为 VAD 的强大特征提取器。VLM 将视觉数据与视频中的文本描述或口语相结合，从而能够对异常检测至关重要的场景进行细致入微的理解。通过应对这些挑战并提出未来的研究方向，本评论旨在促进开发强大而高效的 VAD 系统，利用 VLM 的功能来增强复杂现实场景中的异常检测。这项全面的分析旨在弥补现有的知识空白，为研究人员提供宝贵的见解，并为塑造 VAD 研究的未来做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2405.19387</guid>
      <pubDate>Fri, 31 May 2024 06:20:20 GMT</pubDate>
    </item>
    <item>
      <title>VisTA-SR：提高农业用低成本热成像摄像机的精度和分辨率</title>
      <link>https://arxiv.org/abs/2405.19413</link>
      <description><![CDATA[arXiv:2405.19413v1 公告类型：新
摘要：热像仪是农业研究的重要工具，因为它们可以非侵入性地测量植物温度，这与重要的光化学、水力和农艺性状有关。利用低成本的热像仪可以降低将热成像引入农业研究和生产的障碍。本文介绍了一种提高用于农业应用的低成本热成像相机的温度精度和图像质量的方法。利用计算机视觉技术（特别是深度学习网络）的进步，我们提出了一种称为 $\textbf{VisTA-SR}$（$\textbf{Vis}$ual \&amp; $\textbf{T}$hermal $\textbf{A}$lignment 和 $\textbf{S}$uper-$\textbf{R}$esolution Enhancement）的方法，该方法结合了 RGB 和热图像以增强低分辨率热像仪的功能。研究包括校准和验证温度测量、获取配对图像数据集以及开发专门用于农业热成像的深度学习网络。我们的研究解决了农业领域图像增强的挑战，并探索了低成本热像仪取代高分辨率工业相机的潜力。实验结果证明了我们的方法在提高温度准确性和图像清晰度方面的有效性，为农业领域更方便、更高效的热成像解决方案铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2405.19413</guid>
      <pubDate>Fri, 31 May 2024 06:20:20 GMT</pubDate>
    </item>
    </channel>
</rss>