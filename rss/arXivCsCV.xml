<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://arxiv.org/rss/</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 05 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>多级特征融合的尺度均衡</title>
      <link>https://arxiv.org/abs/2402.01149</link>
      <description><![CDATA[深度神经网络在各种计算机视觉领域，尤其是语义分割任务中表现出了卓越的性能。他们的成功通常归功于多级特征融合，这使他们能够从图像中理解全局和局部信息。然而，我们发现并行分支的多级特征具有不同的尺度。尺度不平衡是一种普遍且不必要的缺陷，会导致有害的梯度下降，从而降低语义分割的性能。我们发现尺度不平衡是由双线性上采样引起的，这一点得到了理论和经验证据的支持。基于这一观察，我们建议注入尺度均衡器，以在双线性上采样后实现多级特征之间的尺度平衡。我们提出的尺度均衡器易于实现，适用于任何架构，无超参数，无需额外的计算成本即可实现，并保证任何数据集的尺度平衡。实验表明，采用尺度均衡器可以持续改善各种目标数据集（包括 ADE20K、PASCAL VOC 2012 和 Cityscapes）以及各种解码器选择（包括 UPerHead、PSPHead、ASPPPHead、SepASPPHead 和 FCNHead）的 mIoU 指数。]]></description>
      <guid>https://arxiv.org/abs/2402.01149</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:04 GMT</pubDate>
    </item>
    <item>
      <title>具有预测基本原理假设巩固的无源无监督域适应</title>
      <link>https://arxiv.org/abs/2402.01157</link>
      <description><![CDATA[无源无监督域适应（SFUDA）是一项具有挑战性的任务，其中模型需要适应新域，而无需访问目标域标签或源域数据。这项任务的主要困难是模型的预测可能不准确，并且使用这些不准确的预测进行模型自适应可能会导致误导性的结果。为了解决这个问题，本文提出了一种新颖的方法，该方法考虑每个样本的多个预测假设，并研究每个假设背后的基本原理。通过巩固这些假设的基本原理，我们确定了最有可能的正确假设，然后将其用作伪标记集来支持模型适应的半监督学习过程。为了实现最佳性能，我们提出了一个三步适应过程：模型预适应、假设巩固和半监督学习。大量的实验结果表明，我们的方法在 SFUDA 任务中实现了最先进的性能，并且可以轻松集成到现有方法中以提高其性能。这些代码可在 \url{https://github.com/GANPerf/HCPR} 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.01157</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:04 GMT</pubDate>
    </item>
    <item>
      <title>2AFC提示大型多模态模型用于图像质量评估</title>
      <link>https://arxiv.org/abs/2402.01162</link>
      <description><![CDATA[虽然在提高大型多模态模型（LMM）的高级视觉理解和推理能力方面已经进行了大量研究，但其视觉质量评估（IQA）能力的探索相对不足。在这里，我们通过采用两种选择强制选择〜（2AFC）提示来实现这一目标，因为 2AFC 被广泛认为是收集人类视觉质量意见的最可靠方式。随后，可以使用最大后验估计来有效地聚合由特定 LMM 估计的每个图像的全局质量分数。同时，我们引入了三个评估标准：一致性、准确性和相关性，以提供对五个 LMM 的 IQA 能力的全面量化和更深入的了解。大量实验表明，现有的 LMM 在粗粒度质量比较方面表现出显着的 IQA 能力，但在细粒度质量辨别方面还有改进的空间。所提出的数据集揭示了基于 LMM 的 IQA 模型的未来发展。这些代码将在 https://github.com/h4nwei/2AFC-LMMs 上公开发布。]]></description>
      <guid>https://arxiv.org/abs/2402.01162</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:04 GMT</pubDate>
    </item>
    <item>
      <title>只需一个简单的补丁即可进行 AI 生成的图像检测</title>
      <link>https://arxiv.org/abs/2402.01123</link>
      <description><![CDATA[生成模型的最新发展释放了生成超现实假图像的潜力。为了防止恶意使用假图像，人工智能生成的图像检测旨在区分假图像和真实图像。然而，现有方法通常在不同生成器之间的通用性较差。在这项工作中，我们提出了一种名为 SSP 的极其简单的方法，即将单个简单补丁 (SSP) 的噪声模式输入二元分类器，与 GenImage 数据集上的最新方法相比，该方法可以实现 14.6% 的相对改进。我们的 SSP 方法非常稳健且具有通用性，可以作为未来方法的简单且有竞争力的基线。]]></description>
      <guid>https://arxiv.org/abs/2402.01123</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:03 GMT</pubDate>
    </item>
    <item>
      <title>在杂乱的世界中看到对象：视频中运动的计算对象性</title>
      <link>https://arxiv.org/abs/2402.01126</link>
      <description><![CDATA[将我们杂乱的世界中视觉上不相交的表面感知为整个物体，在物理上不同于那些重叠它们的物体，是一种称为客观性的认知现象，它构成了我们视觉感知的基础。它为所有脊椎动物所共有，并在人类出生时就存在，它使得能够以对象为中心的视觉世界表示和推理成为可能。我们提出了一种利用运动线索和时空注意力的对象性计算方法，使用一对监督时空 R(2+1)U-Net。第一个网络检测运动边界，并根据局部前景-背景感对这些边界处的像素进行分类。该运动边界感知 (MBS) 信息与时空对象注意提示一起传递到注意表面感知 (ASP) 模块，该模块通过一系列帧推断所关注对象的形式，并将其“像素”分类为可见或模糊。注意线索的空间形式是灵活的，但它必须松散地跟踪不需要可见的关注对象。我们展示了这种简单但新颖的方法能够在没有对象模型的情况下从现象学推断对象性，并表明即使在模糊和相机抖动的情况下，它也能在杂乱的场景中提供对个体参与对象的强大感知。我们表明，我们的数据多样性和增强可以最大限度地减少偏见并促进转移到真实视频。最后，我们描述了这种计算对象能力如何变得更加复杂，并锚定一个强大的模块化视频对象感知框架。]]></description>
      <guid>https://arxiv.org/abs/2402.01126</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:03 GMT</pubDate>
    </item>
    <item>
      <title>DeepAAT：深度自动化空中三角测量，用于基于无人机的快速测绘</title>
      <link>https://arxiv.org/abs/2402.01134</link>
      <description><![CDATA[自动空中三角测量（AAT）旨在同时恢复图像位姿和重建稀疏点，在对地观测中发挥着关键作用。凭借其在摄影测量领域数十年的丰富研究成果，AAT 已发展成为广泛应用于大规模无人机 (UAV) 测绘的基本流程。尽管取得了进步，经典的 AAT 方法仍然面临效率低和鲁棒性有限等挑战。本文介绍了 DeepAAT，这是一种专门为无人机图像 AAT 设计的深度学习网络。 DeepAAT 考虑图像的空间和光谱特征，增强其解决错误匹配对和准确预测图像姿态的能力。 DeepAAT标志着AAT效率的重大飞跃，确保了场景的全面覆盖和精度。其处理速度比增量 AAT 方法快数百倍，比全局 AAT 方法快数十倍，同时保持相当的重建精度水平。此外，即使在计算资源有限的情况下，DeepAAT 的场景聚类和合并策略也有助于大规模无人机图像的快速定位和姿态确定。实验结果表明，DeepAAT 相对于传统 AAT 方法有了实质性改进，凸显了其在基于无人机的 3D 重建任务的效率和准确性方面的潜力。为了造福摄影测量界，DeepAAT的代码将发布在：https://github.com/WHU-USI3DV/DeepAAT。]]></description>
      <guid>https://arxiv.org/abs/2402.01134</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:03 GMT</pubDate>
    </item>
    <item>
      <title>使用生成对抗网络 (GAN) 创建虚拟试衣间的经济有效的方法</title>
      <link>https://arxiv.org/abs/2402.00994</link>
      <description><![CDATA[世界各地的顾客都希望在购买前看看衣服是否合身。因此，顾客本质上更喜欢实体店购物，这样他们可以在购买前试穿产品。但新冠肺炎疫情发生后，许多卖家要么转向网上购物，要么关闭试衣间，这让购物过程变得犹豫和可疑。购买后的衣服可能不适合买家，这一事实促使我们考虑使用新的人工智能技术，以移动应用程序和使用网页的部署模型的形式创建在线平台或虚拟试衣间（VFR）稍后可以将其嵌入到任何在线商店，在那里他们可以试穿任意数量的布料，而无需亲自尝试。此外，这将为他们的需求节省大量搜索时间。此外，通过使用特殊类型的镜子应用相同的技术，顾客可以更快地试穿，这将减少实体店的拥挤和头痛。另一方面，从企业主的角度来看，该项目将大大增加他们的在线销售额，此外，它还可以避免实体试用问题，从而节省产品质量。这项工作使用的主要方法是应用生成对抗网络（GAN）与图像处理技术相结合，从两个输入图像（人物图像和布料图像）生成一个输出图像。这项工作取得的成果优于文献中最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2402.00994</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:02 GMT</pubDate>
    </item>
    <item>
      <title>mmID：用于人体识别的高分辨率毫米波成像</title>
      <link>https://arxiv.org/abs/2402.00996</link>
      <description><![CDATA[通过射频成像实现准确的人体识别一直是一个持续的挑战，这主要归因于有限的孔径尺寸及其对成像分辨率的影响。现有的成像解决方案通过估计骨骼关节来实现基于深度神经网络的姿势估计、活动识别和人体跟踪等任务。与估计关节相反，本文提出通过使用条件生成对抗网络（cGAN）估计整个人体来提高成像分辨率。为了降低训练复杂性，我们使用多重信号分类 (MUSIC) 算法估计的空间谱作为 cGAN 的输入。我们的系统生成与环境无关的高分辨率图像，可以提取有助于人类识别的独特物理特征。我们使用简单的基于卷积层的分类网络来获得最终的识别结果。从实验结果来看，我们表明经过训练的生成器生成的图像分辨率足够高，足以实现人类识别。我们的发现表明高分辨率准确度与 Kinect 设备的平均轮廓差异为 5%。在不同环境下、在多个测试仪上进行的大量实验表明，我们的系统在未见过的环境下静态人体目标识别的整体测试精度可以达到 93%。]]></description>
      <guid>https://arxiv.org/abs/2402.00996</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:02 GMT</pubDate>
    </item>
    <item>
      <title>人工智能生成的面孔不受种族和性别刻板印象的影响</title>
      <link>https://arxiv.org/abs/2402.01002</link>
      <description><![CDATA[全球数百万人每天都在使用稳定扩散等文本到图像生成人工智能模型。然而，许多人对这些模式如何放大种族和性别刻板印象表示担忧。为了研究这种现象，我们开发了一个分类器来预测任何给定面部图像的种族、性别和年龄组，并表明它实现了最先进的性能。使用这个分类器，我们量化了 6 个种族、2 个性别、5 个年龄组、32 个职业和 8 个属性的稳定扩散偏差。然后，我们提出了优于最先进替代方案的新颖去偏解决方案。此外，我们还研究了稳定扩散将同一种族的个体彼此相似的程度。这一分析揭示了高度的刻板印象，例如，将大多数中东男性描绘成深色皮肤、留着胡须、戴着传统头饰。我们通过提出另一种新颖的解决方案来解决这些限制，该解决方案可以增加跨性别和种族群体的面部多样性。我们的解决方案是开源的并公开提供。]]></description>
      <guid>https://arxiv.org/abs/2402.01002</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:02 GMT</pubDate>
    </item>
    <item>
      <title>IMUGPT 2.0：基于语言的跨模态传输，用于基于传感器的人类活动识别</title>
      <link>https://arxiv.org/abs/2402.01049</link>
      <description><![CDATA[人类活动识别（HAR）领域的主要挑战之一是缺乏大型标记数据集。这阻碍了稳健且可推广的模型的开发。最近，人们探索了可以缓解数据稀缺问题的跨模态传输方法。这些方法将现有数据集从视频等源模态转换为目标模态 (IMU)。随着大语言模型 (LLM) 和文本驱动的运动合成模型等生成式 AI 模型的出现，语言已成为一种有前途的源数据模态，并在 IMUGPT 等概念证明中得到体现。在这项工作中，我们对基于语言的跨模态迁移进行了大规模评估，以确定其对 HAR 的有效性。基于这项研究，我们引入了 IMUGPT 的两个新扩展，增强了其在实际 HAR 应用场景中的使用：一个运动滤波器，能够滤除不相关的运动序列以确保生成的虚拟 IMU 数据的相关性，以及一组衡量指标生成数据的多样性有助于确定何时停止生成虚拟 IMU 数据以进行有效且高效的处理。我们证明，我们的多样性指标可以将生成虚拟 IMU 数据所需的工作量减少至少 50%，这为 IMUGPT 提供了实际用例，而不仅仅是概念证明。]]></description>
      <guid>https://arxiv.org/abs/2402.01049</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:02 GMT</pubDate>
    </item>
    <item>
      <title>FuseFormer：视觉和热图像融合的变压器</title>
      <link>https://arxiv.org/abs/2402.00971</link>
      <description><![CDATA[图像融合是将来自不同传感器的图像组合成包含所有相关信息的单个图像的过程。大多数最先进的图像融合技术都使用深度学习方法来提取有意义的特征；然而，它们主要整合局部特征，而没有考虑图像更广泛的背景。为了克服这一限制，基于 Transformer 的模型已成为一种有前途的解决方案，旨在通过注意机制捕获一般上下文依赖关系。由于图像融合没有基本事实，因此损失函数是基于评估指标构建的，例如结构相似性指数度量（SSIM）。通过这样做，我们对 SSIM 产生了偏差，从而对输入视觉带图像产生了偏差。本研究的目的是提出一种新颖的图像融合方法，以减轻与使用评估指标作为损失函数相关的限制。我们的方法集成了基于变压器的多尺度融合策略，可以熟练地处理本地和全局上下文信息。这种集成不仅细化了图像融合过程的各个组成部分，而且显着增强了该方法的整体效率。我们提出的方法遵循两阶段训练方法，其中自动编码器最初经过训练以在第一阶段提取多个尺度的深层特征。对于第二阶段，我们集成了融合块并改变了如上所述的损失函数。使用卷积神经网络 (CNN) 和 Transformer 的组合来融合多尺度特征。 CNN 用于捕获局部特征，而 Transformer 则处理一般上下文特征的集成。]]></description>
      <guid>https://arxiv.org/abs/2402.00971</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:01 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习增强边缘到阶段框架</title>
      <link>https://arxiv.org/abs/2402.00977</link>
      <description><![CDATA[在条纹投影轮廓术 (FPP) 中，利用有限数量的条纹图案实现稳健且准确的 3D 重建仍然是结构光 3D 成像中的一个挑战。传统方法需要一组条纹图像，但仅使用一个或两个图案会使相位恢复和展开变得复杂。在本研究中，我们引入了 SFNet，这是一种对称融合网络，可将两个条纹图像转换为绝对相位。为了提高输出可靠性，我们的框架通过合并来自与用作输入的频率不同的频率的条纹图像的信息来预测精细相位。这使我们只需两张图像即可实现高精度。比较实验和消融研究验证了我们提出的方法的有效性。数据集和代码可在我们的项目页面 https://wonhoe-kim.github.io/SFNet 上公开访问。]]></description>
      <guid>https://arxiv.org/abs/2402.00977</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:01 GMT</pubDate>
    </item>
    <item>
      <title>YOLinO++：无地图自动潜水通用折线的单次估计</title>
      <link>https://arxiv.org/abs/2402.00989</link>
      <description><![CDATA[在自动驾驶中，高精度地图通常用于支持和补充感知。这些地图的创建成本很高，并且随着交通世界的永久变化而很快就会过时。为了通过传感器数据的检测来支持或替换自动化系统的地图，感知模块必须能够检测地图特征。我们提出了一种神经网络，它遵循 YOLO 的一次性哲学，但设计用于检测图像中的一维结构，例如车道边界。
  我们通过基于中点的线表示和锚定义来扩展以前的想法。这种表示可用于描述车道边界、标记，也可用于描述隐式特征，例如车道中心线。该方法的广泛适用性体现在车道中心线、车道边界以及高速公路和城市地区标记的检测性能上。
  检测多种车道边界，并且可以本质上将其分类为虚线或实线、路缘、道路边界或隐式划界。]]></description>
      <guid>https://arxiv.org/abs/2402.00989</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:01 GMT</pubDate>
    </item>
    <item>
      <title>将 GradCAM 推广到嵌入网络</title>
      <link>https://arxiv.org/abs/2402.00909</link>
      <description><![CDATA[可视化 CNN 是建立信任和解释模型预测的重要组成部分。 CAM 和 GradCAM 等方法在定位负责输出的图像区域方面非常成功，但仅限于分类模型。在本文中，我们提出了一种新方法 EmbeddingCAM，它推广了嵌入网络的 Grad-CAM。我们表明，对于分类网络，EmbeddingCAM 简化为 GradCAM。我们展示了我们的方法在 CUB-200-2011 数据集上的有效性，并对数据集进行了定量和定性分析。]]></description>
      <guid>https://arxiv.org/abs/2402.00909</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:00 GMT</pubDate>
    </item>
    <item>
      <title>MUSTAN：多尺度时间上下文作为鲁棒视频前景分割的注意力</title>
      <link>https://arxiv.org/abs/2402.00918</link>
      <description><![CDATA[视频前景分割（VFS）是一项重要的计算机视觉任务，其目标是从背景中分割运动物体。目前的大多数方法都是基于图像的，即仅依赖于空间线索而忽略运动线索。因此，它们往往会过度拟合训练数据，并且不能很好地推广到域外 (OOD) 分布。为了解决上述问题，先前的工作利用了多种线索，例如光流、背景减法掩模等。然而，拥有带有光流等注释的视频数据是一项具有挑战性的任务。在本文中，我们利用视频数据中的时间信息和空间线索来提高 OOD 性能。然而，挑战在于我们如何以可解释的方式对给定视频数据的时间信息进行建模，从而产生非常明显的差异。因此，我们设计了一种策略，将视频的时间上下文集成到 VFS 的开发中。我们的方法产生了深度学习架构，即 MUSTAN1 和 MUSTAN2，它们基于多尺度时间上下文作为注意力的思想，即帮助我们的模型学习有利于 VFS 的更好的表示。此外，我们引入了一个新的视频数据集，即 VFS 的室内监控数据集（ISD）。它在帧级别上有多个注释，例如前景二进制掩模、深度图和实例语义注释。因此，ISD 可以使其他计算机视觉任务受益。我们验证架构的有效性并将性能与基线进行比较。我们证明所提出的方法明显优于 OOD 基准方法。此外，由于 ISD，MUSTAN2 在 OOD 数据上的某些视频类别上的性能得到显着提高。]]></description>
      <guid>https://arxiv.org/abs/2402.00918</guid>
      <pubDate>Mon, 05 Feb 2024 18:18:00 GMT</pubDate>
    </item>
    </channel>
</rss>