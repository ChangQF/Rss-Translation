<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 09 May 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>检测和细化被大气灰尘遮挡的 HiRISE 图像块</title>
      <link>https://arxiv.org/abs/2405.04722</link>
      <description><![CDATA[arXiv:2405.04722v1 公告类型：新
摘要：HiRISE（高分辨率成像科学实验）是火星勘测轨道飞行器上的一台相机，负责以前所未有的细节拍摄火星表面的大片区域。它可以在几分钟内捕获数百万张令人难以置信的特写图像。然而，火星经常遭受区域和局部沙尘暴的影响，阻碍了数据收集过程和管道，导致工作量和关键飞行时间的损失。手动删除这些图像需要大量的人力。我使用在 Resnet-50 上微调的灰尘图像分类器自动过滤掉这些被大气灰尘遮挡的图像，准确度为 94.05%。为了进一步促进图像的无缝过滤，我设计了一个预测管道，对这些灰尘斑块进行分类和存储。我还使用基于自动编码器的降噪器和分别具有 0.75 和 0.99 SSIM 索引的 Pix2Pix GAN 对部分遮挡的图像进行降噪。]]></description>
      <guid>https://arxiv.org/abs/2405.04722</guid>
      <pubDate>Thu, 09 May 2024 06:18:55 GMT</pubDate>
    </item>
    <item>
      <title>野外多模式重新识别的多合一框架</title>
      <link>https://arxiv.org/abs/2405.04741</link>
      <description><![CDATA[arXiv:2405.04741v1 公告类型：新
摘要：在重新识别（ReID）领域，最近的进展在单模态和跨模态检索任务中都取得了显着的进展。然而，开发一个能够有效处理不同多模态数据（包括 RGB、红外、草图和文本信息）的统一框架仍然是一项挑战。此外，大规模模型的出现在各种视觉任务中显示出良好的性能，但 ReID 的基础模型仍然是空白。为了应对这些挑战，引入了一种新颖的 ReID 多模态学习范式，称为 All-in-One (AIO)，它利用冻结的预训练大模型作为编码器，无需额外的微调即可实现有效的多模态检索。 AIO 中的不同多模态数据被无缝标记到统一空间中，允许模态共享的冻结编码器跨所有模态全面提取身份一致的特征。此外，精心设计的跨模态头部集合旨在指导学习轨迹。 AIO 是执行一体化 ReID 的第一个框架，包含四种常用的模式。跨模态和多模态 ReID 实验表明，AIO 不仅能够熟练处理各种模态数据，而且在具有挑战性的环境中表现出色，在零样本和领域泛化场景中展现出卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.04741</guid>
      <pubDate>Thu, 09 May 2024 06:18:55 GMT</pubDate>
    </item>
    <item>
      <title>TALC：用于多场景文本到视频生成的时间对齐字幕</title>
      <link>https://arxiv.org/abs/2405.04682</link>
      <description><![CDATA[arXiv:2405.04682v1 公告类型：新
摘要：基于扩散的生成建模的最新进展促进了文本到视频（T2V）模型的发展，该模型可以根据文本提示生成高质量视频。大多数 T2V 模型通常会生成单场景视频剪辑，描述实体执行特定动作（例如“小熊猫爬树”）。然而，生成多场景视频是相关的，因为它们在现实世界中无处不在（例如，“小熊猫爬树”，然后是“小熊猫睡在树顶”）。为了从预训练的 T2V 模型生成多场景视频，我们引入了时间对齐字幕（TALC）框架。具体来说，我们增强了 T2V 架构中的文本调节机制，以识别视频场景和场景描述之间的时间对齐。例如，我们用第一个场景描述（例如，“小熊猫爬树”）和第二个场景描述（例如，“小熊猫睡觉”）的表示来调节生成视频的早期和后期场景的视觉特征分别在树的顶部&#39;）。结果，我们表明 T2V 模型可以生成符合多场景文本描述并且视觉上一致（例如实体和背景）的多场景视频。此外，我们使用 TALC 框架使用多场景视频文本数据对预训练的 T2V 模型进行微调。我们表明，TALC 微调模型的总分比基线方法高出 15.5 分，使用人工评估来平均视觉一致性和文本依从性。项目网站为https://talc-mst2v.github.io/。]]></description>
      <guid>https://arxiv.org/abs/2405.04682</guid>
      <pubDate>Thu, 09 May 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>远程扩散</title>
      <link>https://arxiv.org/abs/2405.04717</link>
      <description><![CDATA[arXiv:2405.04717v1 公告类型：新
摘要：我探索了如何调整 Stable Diffusion v1.5 以生成遥感领域特定的卫星和航空图像。认识到现有模型（如 Midjourney 和 Stable Diffusion）的局限性，这些模型主要在自然 RGB 图像上进行训练，缺乏遥感背景，我使用 RSICD 数据集训练了一个损失为 0.2 的稳定扩散模型。我从数据集中加入了描述性标题进行文本调节。此外，我还为土地利用土地分类 (LULC) 任务创建了一个合成数据集，采用了 RAG 和 ChatGPT 的提示技术，并对专门的遥感 LLM 进行了微调。然而，我面临着提示质量和模型性能方面的挑战。我在合成数据集上训练了一个分类模型 (ResNet18)，在 TorchGeo 中实现了 49.48% 的测试准确率，从而创建了基线。通过 FID 分数进行定量评估和来自领域专家的定性反馈，评估了生成的图像和数据集的真实性和质量。尽管进行了大量的微调和数据集迭代，但结果表明图像质量和真实性低于标准，正如高 FID 分数和领域专家评估所表明的那样。这些发现引起了人们对扩散模型在遥感中的潜力的关注，同时也强调了与预训练数据和计算资源不足相关的重大挑战。]]></description>
      <guid>https://arxiv.org/abs/2405.04717</guid>
      <pubDate>Thu, 09 May 2024 06:18:54 GMT</pubDate>
    </item>
    <item>
      <title>雷达场：FMCW 雷达的频率空间神经场景表示</title>
      <link>https://arxiv.org/abs/2405.04662</link>
      <description><![CDATA[arXiv:2405.04662v1 公告类型：新
摘要：神经场作为场景表示被广泛研究，用于再现和新颖生成各种户外场景，包括自动驾驶车辆和机器人必须处理的场景。尽管存在 RGB 和 LiDAR 数据的成功方法，但雷达作为传感方式的神经重建方法在很大程度上尚未得到探索。雷达传感器在毫米波长下工作，对雾和雨中的散射具有鲁棒性，因此为主动和被动光学传感技术提供了补充模式。此外，现有的雷达传感器具有很高的成本效益，并广泛部署在户外运行的机器人和车辆中。我们介绍雷达场 - 一种专为主动雷达成像仪设计的神经场景重建方法。我们的方法将显式的、基于物理的传感器模型与隐式神经几何和反射率模型结合起来，直接合成原始雷达测量结果并提取场景占用率。所提出的方法不依赖于体绘制。相反，我们学习傅立叶频率空间中的场，并使用原始雷达数据进行监督。我们在不同的户外场景中验证了该方法的有效性，包括车辆和基础设施密集的城市场景，以及毫米波长传感特别有利的恶劣天气场景。]]></description>
      <guid>https://arxiv.org/abs/2405.04662</guid>
      <pubDate>Thu, 09 May 2024 06:18:53 GMT</pubDate>
    </item>
    <item>
      <title>TexControl：使用扩散模型生成基于草图的两阶段时尚图像</title>
      <link>https://arxiv.org/abs/2405.04675</link>
      <description><![CDATA[arXiv:2405.04675v1 公告类型：新
摘要：基于深度学习的草图到服装图像生成为时装设计过程提供了初步设计和灵感。然而，由于手绘草图信息稀疏且模糊，因此从手绘生成服装具有挑战性。当前的生成模型可能难以生成详细的纹理信息。在这项工作中，我们提出了 TexControl，这是一个基于草图的时尚生成框架，它使用两阶段流水线生成与草图输入相对应的时尚图像。首先，我们采用 ControlNet 从草图生成时尚图像并保持图像轮廓稳定。然后，我们使用图像到图像的方法优化生成图像的细节纹理并获得最终结果。评估结果表明，TexControl 可以生成具有高质量纹理的时尚图像，就像细粒度图像生成一样。]]></description>
      <guid>https://arxiv.org/abs/2405.04675</guid>
      <pubDate>Thu, 09 May 2024 06:18:53 GMT</pubDate>
    </item>
    <item>
      <title>FRACTAL：用于对不同景观进行 3D 语义分割的超大规模航空激光雷达数据集</title>
      <link>https://arxiv.org/abs/2405.04634</link>
      <description><![CDATA[arXiv:2405.04634v1 公告类型：新
摘要：测绘机构越来越多地采用空中激光雷达扫描（ALS）作为监测领土和支持公共政策的新工具。大规模处理 ALS 数据需要高效的点分类方法，该方法在高度多样化的地区表现良好。为了评估它们，研究人员需要大型带注释的激光雷达数据集，但是，当前的激光雷达基准数据集范围有限，并且通常覆盖单个城市区域。为了弥补这一数据差距，我们提出了 FRench ALS Clouds from TArgeted Landscapes (FRACTAL) 数据集：一个超大规模航空激光雷达数据集，由 100,000 个密集点云组成，具有 7 个语义类别的高质量标签，跨越 250 公里$^ 2 美元。 FRACTAL 基于法国全国开放激光雷达数据。它通过明确集中来自法国五个地区的稀有类别和具有挑战性的景观的采样方案实现了空间和语义多样性。它应该支持开发用于大规模土地监测的 3D 深度学习方法。我们描述源数据的性质、采样工作流程、结果数据集的内容，并使用高性能 3D 神经架构提供分割性能的初步评估。]]></description>
      <guid>https://arxiv.org/abs/2405.04634</guid>
      <pubDate>Thu, 09 May 2024 06:18:52 GMT</pubDate>
    </item>
    <item>
      <title>大鼠图像身体部位分割和关键点检测的自监督方法</title>
      <link>https://arxiv.org/abs/2405.04650</link>
      <description><![CDATA[arXiv:2405.04650v1 公告类型：新
摘要：实例分割支持的单个组件识别和关键点检测对于分析场景中代理的行为至关重要。此类系统可用于监视、自动驾驶汽车，也可用于医学研究，其中实验动物的行为分析用于确认给定药物的后遗症。能够解决上述任务的方法通常需要大量高质量的手工注释数据，这需要时间和金钱来产生。在本文中，我们提出了一种减少对实验室大鼠进行手动标记的需要的方法。为此，我们首先使用基于计算机视觉的方法生成初始注释，然后通过广泛的增强，我们在生成的数据上训练深度神经网络。即使对象被严重遮挡，最终的系统也能够进行实例分割、关键点检测和身体部位分割。]]></description>
      <guid>https://arxiv.org/abs/2405.04650</guid>
      <pubDate>Thu, 09 May 2024 06:18:52 GMT</pubDate>
    </item>
    <item>
      <title>一种具有高概率区域搜索的新型广域多目标检测系统</title>
      <link>https://arxiv.org/abs/2405.04589</link>
      <description><![CDATA[arXiv:2405.04589v1 公告类型：新
摘要： 近年来，广域视觉监控系统已广泛应用于各种工业和交通场景。然而，由于高分辨率成像、高效目标搜索和准确定位的需求所产生的冲突，这些系统在实现多目标检测时面临重大挑战。为了应对这些挑战，本文提出了一种混合系统，其中包含广角相机、高速搜索相机和电流镜。在该系统中，广角相机提供全景图像作为先验信息，这有助于搜索相机捕获目标物体的详细图像。这种集成方法提高了广域视觉检测系统的整体效率和有效性。具体来说，在本研究中，我们引入了一种基于广角相机的方法来生成全景概率图（PPM），以估计目标物体存在的高概率区域。然后，我们提出了一种概率搜索模块，该模块使用 PPM 生成的先验信息来动态调整采样范围，并根据对象检测器计算的不确定性方差来细化目标坐标。最后，PPM 和概率搜索模块的集成产生了一个高效的混合视觉系统，能够实现 120 fps 的多目标搜索和检测。进行了大量的实验来验证系统的有效性和鲁棒性。]]></description>
      <guid>https://arxiv.org/abs/2405.04589</guid>
      <pubDate>Thu, 09 May 2024 06:18:51 GMT</pubDate>
    </item>
    <item>
      <title>肺部健康中的人工智能：跨多个 CT 扫描数据集的检测和诊断模型基准测试</title>
      <link>https://arxiv.org/abs/2405.04605</link>
      <description><![CDATA[arXiv:2405.04605v1 公告类型：新
摘要： 背景：肺癌的高死亡率可以通过早期检测来降低，而肺癌越来越依赖于人工智能（AI）进行诊断成像。然而，人工智能模型的性能取决于用于训练和验证的数据集。方法：本研究利用杜克肺癌筛查数据集 (DLCSD) 开发并验证了 DLCSD-mD 和 LUNA16-mD 模型，包含 2,000 多个 CT 扫描和 3,000 多个注释。这些模型根据内部 DLCSD 以及外部 LUNA16 和 NLST 数据集进行了严格评估，旨在建立基于成像的性能基准。评估的重点是创建标准化评估框架，以促进与广泛使用的数据集进行一致比较，确保模型功效的全面验证。使用自由反应受试者工作特征（FROC）和曲线下面积（AUC）分析评估诊断准确性。结果：在内部 DLCSD 集上，DLCSD-mD 模型的 AUC 为 0.93（95% CI：0.91-0.94），表现出较高的准确性。其性能在外部数据集上保持不变，在 LUNA16 上的 AUC 为 0.97（95% CI：0.96-0.98），在 NLST 上的 AUC 为 0.75（95% CI：0.73-0.76）。同样，LUNA16-mD 模型在其原生数据集上记录的 AUC 为 0.96（95% CI：0.95-0.97），并显示出可转移的诊断性能，在 DLCSD 上的 AUC 为 0.91（95% CI：0.89-0.93），在 DLCSD 上的 AUC 为 0.71（95% CI：0.89-0.93）。 CI：0.70-0.72) NLST。结论：DLCSD-mD 模型在不同数据集上表现出可靠的性能，将 DLCSD 确立为肺癌检测和诊断的稳健基准。通过向公共领域提供我们的模型和代码，我们的目标是加速基于人工智能的诊断工具的开发，并鼓励医疗机器学习 (ML) 领域的可重复性和协作进步。]]></description>
      <guid>https://arxiv.org/abs/2405.04605</guid>
      <pubDate>Thu, 09 May 2024 06:18:51 GMT</pubDate>
    </item>
    <item>
      <title>DiffFinger：通过去噪扩散概率模型推进合成指纹生成</title>
      <link>https://arxiv.org/abs/2405.04538</link>
      <description><![CDATA[arXiv:2405.04538v1 公告类型：新
摘要：本研究探索使用去噪扩散概率模型（DDPM）生成合成指纹图像。收集真实生物识别数据的重大障碍，例如隐私问题和对多样化数据集的需求，强调了现实且多样化的合成生物识别替代方案的必要性。尽管生成对抗网络 (GAN) 在生成逼真的指纹图像方面取得了长足的进步，但它们的局限性促使我们提出 DDPM 作为一种有前途的替代方案。 DDPM 能够生成清晰度和真实度不断提高的图像，同时保持多样性。我们的结果表明，DiffFinger 不仅在质量上与真实的训练集数据竞争，而且还提供了更丰富的生物识别数据集，反映了真实的可变性。这些发现标志着生物识别合成领域取得了有希望的进步，展示了 DDPM 推动指纹识别和认证系统发展的潜力。]]></description>
      <guid>https://arxiv.org/abs/2405.04538</guid>
      <pubDate>Thu, 09 May 2024 06:18:50 GMT</pubDate>
    </item>
    <item>
      <title>ClothPPO：具有观察对齐动作空间的机器人布料操纵的近端策略优化增强框架</title>
      <link>https://arxiv.org/abs/2405.04549</link>
      <description><![CDATA[arXiv:2405.04549v1 公告类型：新
摘要：基于视觉的机器人展开布料最近取得了很大进展。然而，先前的工作主要依赖于价值学习，并没有充分探索基于策略的技术。最近，强化学习在大型语言模型上的成功表明，策略梯度算法可以增强具有巨大动作空间的策略。在本文中，我们介绍了 ClothPPO，这是一个框架，它采用基于 Actor-Critic 架构的策略梯度算法来增强预训练模型，该模型具有与展开衣服任务中的观察一致的巨大 10^6 动作空间。为此，我们将布料操纵问题重新定义为部分可观察的马尔可夫决策过程。采用有监督的预训练阶段来训练我们策略的基线模型。在第二阶段，利用近端策略优化（PPO）来指导观察一致的行动空间内的监督模型。通过优化和更新策略，我们提出的方法增加了在软体操纵任务下衣服展开的表面积。实验结果表明，我们提出的框架可以进一步提高其他最先进方法的展开性能。]]></description>
      <guid>https://arxiv.org/abs/2405.04549</guid>
      <pubDate>Thu, 09 May 2024 06:18:50 GMT</pubDate>
    </item>
    <item>
      <title>当免训练 NAS 遇上 Vision Transformer：神经切线核视角</title>
      <link>https://arxiv.org/abs/2405.04536</link>
      <description><![CDATA[arXiv:2405.04536v1 公告类型：新
摘要：本文研究了神经正切核（NTK）来搜索无需训练的视觉变换器。与之前观察到的基于 NTK 的指标可以有效预测 CNN 在初始化时的性能相比，我们凭经验证明了它们在 ViT 搜索空间中的无效性。我们假设 ViT 中的基本特征学习偏好导致将 NTK 应用于 ViT 的 NAS 无效。我们从理论上和经验上都验证了 NTK 本质上估计了神经网络学习低频信号的能力，完全忽略了高频信号对特征学习的影响。为了解决这个限制，我们提出了一种名为 ViNTK 的新方法，通过集成输入的傅里叶特征，将标准 NTK 推广到高频域。在图像分类和语义分割任务上使用多个 ViT 搜索空间进行的实验表明，与现有最先进的 ViT NAS 相比，我们的方法可以显着加快搜索成本，同时在搜索架构上保持相似的性能。]]></description>
      <guid>https://arxiv.org/abs/2405.04536</guid>
      <pubDate>Thu, 09 May 2024 06:18:49 GMT</pubDate>
    </item>
    <item>
      <title>SO(3) 等变网络的直观多频特征表示</title>
      <link>https://arxiv.org/abs/2405.04537</link>
      <description><![CDATA[arXiv:2405.04537v1 公告类型：新
摘要：3D 视觉算法（例如形状重建）的使用仍然有限，因为它们要求输入处于固定的标准旋转。最近，提出了一种简单的等变网络矢量神经元 (VN)，可以轻松地与最先进的 3D 神经网络 (NN) 架构一起使用。然而，它的性能有限，因为它被设计为仅使用三维特征，这不足以捕获 3D 数据中存在的细节。在本文中，我们引入了一种等变特征表示，用于将 3D 点映射到高维特征空间。我们的特征可以辨别 3D 数据中存在的多个频率，这是为 3D 视觉任务设计富有表现力的特征的关键。我们的表示可以用作 VN 的输入，结果表明，通过我们的特征表示，VN 可以捕获更多细节，克服了其原始论文中提出的限制。]]></description>
      <guid>https://arxiv.org/abs/2405.04537</guid>
      <pubDate>Thu, 09 May 2024 06:18:49 GMT</pubDate>
    </item>
    <item>
      <title>可可植物 CSSVD 检测的图像分类</title>
      <link>https://arxiv.org/abs/2405.04535</link>
      <description><![CDATA[arXiv:2405.04535v1 公告类型：新
摘要：植物内疾病的检测引起了计算机视觉爱好者的广泛关注。尽管在检测许多植物疾病方面取得了进展，但在训练图像分类器来检测与可可植物相关的可可肿芽病毒病（简称 CSSVD）方面仍然存在研究空白。这一差距主要是由于缺乏高质量的标记训练数据造成的。此外，各机构对于分享与 CSSVD 相关的数据一直犹豫不决。为了填补这些空白，我们建议开发图像分类器来检测受 CSSVD 感染的可可植物。我们提出的解决方案基于 VGG16、ResNet50 和 Vision Transformer (ViT)。我们在最近发布且可公开访问的 KaraAgroAI Cocoa 数据集上评估分类器。我们最好的图像分类器基于 ResNet50，仅在 20 个 epoch 上就实现了 95.39\% 的精度、93.75\% 的召回率、94.34\% F1 分数和 94\% 的准确度。与之前的作品相比，召回率提高了 +9.75%。我们的结果表明图像分类器学会识别感染 CSSVD 的可可植物。]]></description>
      <guid>https://arxiv.org/abs/2405.04535</guid>
      <pubDate>Thu, 09 May 2024 06:18:48 GMT</pubDate>
    </item>
    </channel>
</rss>