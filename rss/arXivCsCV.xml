<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Fri, 29 Mar 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>LITA：语言指导的时间本地化助手</title>
      <link>https://arxiv.org/abs/2403.19046</link>
      <description><![CDATA[arXiv:2403.19046v1 公告类型：新
摘要：多模式大语言模型（LLM）取得了巨大进展。最近的工作已将这些模型扩展到视频输入，并具有良好的指令跟随能力。然而，一个重要的缺失部分是时间定位。这些模型无法准确回答“何时？”问题。我们确定了限制其时间定位能力的三个关键方面：（i）时间表示，（ii）架构和（iii）数据。我们通过提出具有以下功能的语言指导时间本地化助手（LITA）来解决这些缺点：（1）我们引入了时间标记，对相对于视频长度的时间戳进行编码，以更好地表示视频中的时间。 (2) 我们在架构中引入 SlowFast 令牌，以精细的时间分辨率捕获时间信息。 (3) 我们强调 LITA 的时间定位数据。除了利用带有时间戳的现有视频数据集之外，我们还提出了一项新任务：推理时间定位（RTL）以及数据集 ActivityNet-RTL，用于学习和评估该任务。推理时间本地化需要视频 LLM 的推理和时间本地化。 LITA 在这项具有挑战性的任务中表现出了出色的性能，几乎使基线的时间平均交并集 (mIoU) 提高了一倍。此外，我们表明，与现有的视频法学硕士相比，我们对时间本地化的重视也大大改善了基于视频的文本生成，包括时间理解相对提高了 36%。代码位于：https://github.com/NVlabs/LITA]]></description>
      <guid>https://arxiv.org/abs/2403.19046</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:05 GMT</pubDate>
    </item>
    <item>
      <title>WALT3D：从延时图像生成真实训练数据，用于重建遮挡下的动态对象</title>
      <link>https://arxiv.org/abs/2403.19022</link>
      <description><![CDATA[arXiv:2403.19022v1 公告类型：新
摘要：当前的 2D 和 3D 对象理解方法在繁忙的城市环境中难以应对严重的遮挡，部分原因是缺乏用于学习遮挡的大规模标记的地面实况注释。在这项工作中，我们介绍了一种新颖的框架，用于使用免费的延时图像自动生成遮挡下动态对象的大型真实数据集。通过利用现成的 2D（边界框、分割、关键点）和 3D（姿势、形状）预测作为伪真实值，自动识别未遮挡的 3D 对象并以剪贴画风格合成到背景中，确保逼真的外观以及物理上准确的遮挡配置。生成的具有伪真实值的剪贴画图像可以有效地训练对遮挡具有鲁棒性的对象重建方法。我们的方法展示了 2D 和 3D 重建的显着改进，特别是在城市场景中物体被严重遮挡的场景中，例如车辆和人。]]></description>
      <guid>https://arxiv.org/abs/2403.19022</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:04 GMT</pubDate>
    </item>
    <item>
      <title>以自我为中心的场景感知人体轨迹预测</title>
      <link>https://arxiv.org/abs/2403.19026</link>
      <description><![CDATA[arXiv:2403.19026v1 公告类型：新
摘要：可穿戴协作机器人可以帮助需要防跌倒帮助或佩戴外骨骼的人类佩戴者。这样的机器人需要能够根据以自我为中心的视觉和周围场景来预测佩戴者的自我运动。在这项工作中，我们利用佩戴在身上的摄像头和传感器来预测人类佩戴者在复杂环境中的轨迹。为了促进自我运动预测的研究，我们收集了一个以用户视角为中心的综合步行场景导航数据集。我们提出了一种预测周围静态场景的人体运动调节的方法。我们的方法利用扩散模型来生成潜在未来轨迹的分布，同时考虑用户对环境的观察。我们引入了一种紧凑的表示来编码用户对周围环境的视觉记忆，以及一种有效的样本生成技术来加速扩散模型的实时推理。我们消除了我们的模型并将其与基线进行比较，结果表明我们的模型在避免碰撞和轨迹模式覆盖的关键指标上优于现有方法。]]></description>
      <guid>https://arxiv.org/abs/2403.19026</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:04 GMT</pubDate>
    </item>
    <item>
      <title>使用 Vision Transformer 检测 X 射线图像中的非法物体</title>
      <link>https://arxiv.org/abs/2403.19043</link>
      <description><![CDATA[arXiv:2403.19043v1 公告类型：新
摘要：非法物体检测是在机场、火车站、地铁和港口等各种高安全场所执行的一项关键任务。每小时检查数千张 X 射线图像的连续而乏味的工作可能会造成精神负担。因此，深度神经网络（DNN）可用于自动化 X 射线图像分析过程，提高效率并减轻安全人员的检查负担。相关文献中通常使用的神经架构是卷积神经网络（CNN），很少使用视觉变换器（ViT）。为了弥补这一差距，本文对 X 射线图像中非法物品检测的相关 ViT 架构进行了全面评估。这项研究利用了 Transformer 和混合主干网（例如 SWIN 和 NextViT）以及检测器（例如 DINO 和 RT-DETR）。结果证明了 DINO Transformer 检测器在低数据情况下的卓越准确性、YOLOv8 令人印象深刻的实时性能以及混合 NextViT 主干的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.19043</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:04 GMT</pubDate>
    </item>
    <item>
      <title>设想 MedCLIP：深入探讨医学视觉语言模型的可解释性</title>
      <link>https://arxiv.org/abs/2403.18996</link>
      <description><![CDATA[arXiv:2403.18996v1 公告类型：新
摘要：面对日常出现的多模态模型，解释深度学习模型变得越来越重要，特别是在医学成像等安全关键领域。然而，缺乏对这些模型的可解释性方法的性能的详细研究正在扩大它们的开发和安全部署之间的差距。在这项工作中，我们分析了各种可解释的人工智能方法在视觉语言模型 MedCLIP 上的性能，以揭开其内部运作的神秘面纱。我们还提供了一种简单的方法来克服这些方法的缺点。我们的工作为医学领域最近著名的 VLM 的可解释性提供了不同的新视角，并且我们的评估方法可推广到其他当前和未来可能的 VLM。]]></description>
      <guid>https://arxiv.org/abs/2403.18996</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:03 GMT</pubDate>
    </item>
    <item>
      <title>用于语言表现认知得分预测的跨域纤维簇形状分析</title>
      <link>https://arxiv.org/abs/2403.19001</link>
      <description><![CDATA[arXiv:2403.19001v1 公告类型：新
摘要：形状在计算机图形学中起着重要作用，提供信息特征来传达物体的形态和功能。大脑成像中的形状分析可以帮助解释人脑的结构和功能相关性。在这项工作中，我们研究了大脑 3D 白质连接的形状及其与人类认知功能的潜在预测关系。我们使用扩散磁共振成像 (dMRI) 纤维束成像将大脑连接重建为 3D 点序列。为了描述每个连接，除了传统的 dMRI 连接和组织微观结构特征之外，我们还提取了 12 个形状描述符。我们引入了一种新颖的框架，形状融合纤维簇变压器（SFFormer），它利用多头交叉注意特征融合模块来预测基于 dMRI 纤维束成像的特定于主题的语言表现。我们在包含 1065 名健康年轻人的大型数据集上评估了该方法的性能。结果表明，基于 Transformer 的 SFFormer 模型及其与形状、微观结构和连接性的帧内/帧内特征融合都提供了丰富的信息，并且它们共同改进了特定学科语言表现分数的预测。总的来说，我们的结果表明大脑连接的形状可以预测人类语言功能。]]></description>
      <guid>https://arxiv.org/abs/2403.19001</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:03 GMT</pubDate>
    </item>
    <item>
      <title>Lift3D：将任何 2D 视觉模型零镜头提升至 3D</title>
      <link>https://arxiv.org/abs/2403.18922</link>
      <description><![CDATA[arXiv:2403.18922v1 公告类型：新
摘要：近年来，在大规模 2D 图像数据集的支持下，用于语义分割、风格迁移或场景编辑等众多任务的 2D 视觉模型出现了爆炸式增长。与此同时，人们对 3D 场景表示重新产生了兴趣，例如来自多视图图像的神经辐射场。然而，与 2D 图像数据集相比，3D 或多视图数据的可用性仍然受到很大限制，因此将 2D 视觉模型扩展到 3D 数据非常理想，但也非常具有挑战性。事实上，将场景编辑等单一 2D 视觉操作扩展到 3D 通常需要专门针对该任务的高度创造性的方法，并且通常需要针对每个场景进行优化。在本文中，我们提出了是否可以提升任何 2D 视觉模型来做出 3D 一致预测的问题。我们对这个问题的回答是肯定的；我们的新 Lift3D 方法训练预测由一些视觉模型（即 DINO 和 CLIP）生成的特征空间上看不见的视图，然后推广到新的视觉算子和任务，例如风格迁移、超分辨率、开放词汇分割和图像着色;对于其中一些任务，没有可比的先前 3D 方法。在许多情况下，我们甚至优于专门针对相关任务的最先进方法。此外，Lift3D 是一种零样本方法，因为它不需要特定于任务的训练，也不需要特定于场景的优化。]]></description>
      <guid>https://arxiv.org/abs/2403.18922</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>TextCrraftor：您的文本编码器可以是图像质量控制器</title>
      <link>https://arxiv.org/abs/2403.18978</link>
      <description><![CDATA[arXiv:2403.18978v1 公告类型：新
摘要：基于扩散的文本到图像生成模型，例如稳定扩散，彻底改变了内容生成领域，使图像编辑和视频合成等领域取得了重大进步。尽管这些模型具有强大的功能，但也有其局限性。合成与输入文本良好对齐的图像仍然具有挑战性，并且需要使用精心设计的提示进行多次运行才能获得满意的结果。为了减轻这些限制，许多研究都试图利用各种技术来微调预先训练的扩散模型，即 UNet。然而，在这些努力中，文本到图像扩散模型训练的一个关键问题在很大程度上仍未得到探索：微调文本编码器以提高文本到图像扩散模型的性能是否可能且可行？我们的研究结果表明，我们可以通过我们提出的微调方法 TextCrraftor 来增强它，从而在定量基准和人类评估方面取得实质性改进，而不是用其他大型语言模型替换稳定扩散中使用的 CLIP 文本编码器。有趣的是，我们的技术还通过不同文本编码器的插值来实现可控图像生成，并通过各种奖励进行微调。我们还证明 TextCraftor 与 UNet 微调是正交的，并且可以结合起来进一步提高生成质量。]]></description>
      <guid>https://arxiv.org/abs/2403.18978</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:02 GMT</pubDate>
    </item>
    <item>
      <title>UniDepth：通用单目度量深度估计</title>
      <link>https://arxiv.org/abs/2403.18913</link>
      <description><![CDATA[arXiv:2403.18913v1 公告类型：新
摘要：准确的单目度量深度估计（MMDE）对于解决 3D 感知和建模中的下游任务至关重要。然而，最近 MMDE 方法的卓越准确性仅限于其训练领域。即使存在适度的域间隙，这些方法也无法推广到看不见的域，这阻碍了它们的实际适用性。我们提出了一种新模型 UniDepth，能够从跨域的单个图像重建度量 3D 场景。与现有的 MMDE 方法不同，UniDepth 在推理时直接从输入图像中预测度量 3D 点，无需任何附加信息，力求提供通用且灵活的 MMDE 解决方案。特别是，UniDepth 实现了一个可自我提示的相机模块，可以预测密集的相机表示以调节深度特征。我们的模型利用伪球形输出表示，它可以解开相机和深度表示。此外，我们提出了一种几何不变性损失，可以促进相机提示的深度特征的不变性。即使与直接在测试域上训练的方法相比，在零样本情况下对十个数据集的彻底评估始终证明了 UniDepth 的卓越性能。代码和模型可在以下位置获取：https://github.com/lpiccinelli-eth/unidepth]]></description>
      <guid>https://arxiv.org/abs/2403.18913</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>PLOT-TAL——通过最佳传输进行快速学习，实现小样本时间动作定位</title>
      <link>https://arxiv.org/abs/2403.18915</link>
      <description><![CDATA[arXiv:2403.18915v1 公告类型：新
摘要：本文介绍了一种在少样本学习中进行时间动作定位（TAL）的新方法。我们的工作解决了传统单提示学习方法的固有局限性，由于无法在现实视频中的不同上下文中进行泛化，这些局限性常常导致过度拟合。认识到视频中摄像机视图、背景和物体的多样性，我们提出了一种通过最佳传输增强的多提示学习框架。这种设计允许模型为每个动作学习一组不同的提示，更有效地捕获一般特征并分配表示以减轻过度拟合的风险。此外，通过采用最佳传输理论，我们有效地将这些提示与动作特征结合起来，优化适应视频数据多方面性质的全面表示。我们的实验证明，在 THUMOS-14 和 EpicKitchens100 标准挑战性数据集上的少样本设置中，动作定位准确性和鲁棒性有了显着提高，突显了我们的多提示最优传输方法在克服传统少样本 TAL 方法挑战方面的有效性。]]></description>
      <guid>https://arxiv.org/abs/2403.18915</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:01 GMT</pubDate>
    </item>
    <item>
      <title>临床领域知识衍生模板改进了气胸分类中的事后 AI 解释</title>
      <link>https://arxiv.org/abs/2403.18871</link>
      <description><![CDATA[arXiv:2403.18871v1 公告类型：新
【摘要】：背景气胸是由于肺与胸壁之间空气异常聚集而引起的一种急性胸部疾病。为了解决通常与深度学习 (DL) 模型相关的不透明性问题，引入了可解释的人工智能 (XAI) 方法来概述与 DL 模型做出的气胸诊断相关的区域。然而，这些解释有时与实际病变区域存在偏差，突出需要进一步改进。方法：我们提出了一种模板引导的方法，将气胸的临床知识纳入 XAI 方法生成的模型解释中，从而提高这些解释的质量。利用放射科医生创建的病灶勾画，我们的方法首先生成一个代表气胸发生潜在区域的模板。然后将该模板叠加在模型解释上，以过滤掉模板边界之外的无关解释。为了验证其功效，我们在解释两个真实数据集中的两个深度学习模型时，对有和没有模板指导的三种 XAI 方法进行了比较分析。结果：所提出的方法在基于三种 XAI 方法、两种深度学习模型和两个数据集构建的十二个基准场景中持续改进了基准 XAI 方法。当比较模型解释和地面真实病变区域时，通过相对于基线性能的性能改进计算出的平均增量百分比在并集交集 (IoU) 中为 97.8%，在骰子相似系数 (DSC) 中为 94.1%。结论：在气胸诊断的背景下，我们提出了一种模板引导的方法来改进人工智能解释。我们预计我们的模板指南将通过整合临床领域专业知识形成一种阐明人工智能模型的新方法。]]></description>
      <guid>https://arxiv.org/abs/2403.18871</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>AIC-UNet：用于稳健多器官分割的解剖学级联 UNet</title>
      <link>https://arxiv.org/abs/2403.18878</link>
      <description><![CDATA[arXiv:2403.18878v1 公告类型：新
摘要：强加关键的解剖特征，例如器官的数量、形状、大小和相对位置，对于建立强大的多器官分割模型至关重要。目前整合解剖学特征的尝试包括通过资源和数据密集型模块（例如自注意力）扩大有效感受野（ERF）大小，或引入器官特异性拓扑正则化器，这可能无法扩展到器官间分割的多器官分割问题。关系也发挥着巨大作用。我们引入了一种新方法，通过使用可学习的解剖学先验条件来调节模型预测，对任何现有的编码器-解码器分割模型施加解剖学约束。更具体地说，给定腹部扫描，编码器的一部分在使用薄板样条（TPS）网格插值与给定输入扫描对齐之前对可学习的空间进行扭曲。然后在解码阶段整合扭曲的先验，以指导模型进行更多解剖学预测。代码可在\hyperlink{https://anonymous.4open.science/r/AIC-UNet-7048}{https://anonymous.4open.science/r/AIC-UNet-7048}获取。]]></description>
      <guid>https://arxiv.org/abs/2403.18878</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>通过量子退火提高多物体跟踪精度</title>
      <link>https://arxiv.org/abs/2403.18908</link>
      <description><![CDATA[arXiv:2403.18908v1 公告类型：新
摘要：多目标跟踪（MOT）是图像识别中的一项关键任务，在平衡处理速度和跟踪精度方面提出了持续的挑战。这项研究引入了一种新颖的方法，该方法利用量子退火（QA）来加快计算速度，同时通过对象跟踪过程的集成来提高跟踪精度。还提出了一种改进匹配集成过程的方法。通过利用 MOT 的顺序性质，本研究通过反向退火 (RA) 进一步增强了跟踪方法。实验验证证实，每个跟踪过程的退火时间仅为 3 $\mu$s，可保持高精度。该方法在实时 MOT 应用中具有巨大潜力，包括用于城市交通灯控制的交通流量测量、自动机器人和车辆的碰撞预测以及工厂批量生产的产品的管理。]]></description>
      <guid>https://arxiv.org/abs/2403.18908</guid>
      <pubDate>Fri, 29 Mar 2024 06:17:00 GMT</pubDate>
    </item>
    <item>
      <title>JEP-KD：基于联合嵌入预测架构的视觉语音识别知识蒸馏</title>
      <link>https://arxiv.org/abs/2403.18843</link>
      <description><![CDATA[arXiv:2403.18843v1 公告类型：新
摘要：由于视觉传达语义信息的固有局限性，视觉语音识别（VSR）任务通常被认为比自动语音识别（ASR）具有较低的理论性能上限。为了缓解这一挑战，本文引入了一种使用联合嵌入预测架构 (JEPA) 的先进知识蒸馏方法，称为 JEP-KD，旨在在模型训练期间更有效地利用音频特征。 JEP-KD 的核心是在嵌入层中包含生成网络，这增强了视频编码器的语义特征提取能力，并使其与来自预训练 ASR 模型编码器的音频特征更加一致。这种方法旨在逐步缩小 VSR 和 ASR 之间的性能差距。此外，还为 JEP-KD 框架建立了全面的多模式、多阶段培训方案，增强了培训过程的稳健性和有效性。实验结果表明，JEP-KD 显着提高了 VSR 模型的性能，并展示了跨不同 VSR 平台的多功能性，表明其在其他多模态任务中具有更广泛应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2403.18843</guid>
      <pubDate>Fri, 29 Mar 2024 06:16:59 GMT</pubDate>
    </item>
    <item>
      <title>SugarcaneNet2024：用于甘蔗病害分类的 LASSO 正则化预训练模型的优化加权平均集成方法</title>
      <link>https://arxiv.org/abs/2403.18870</link>
      <description><![CDATA[arXiv:2403.18870v1 公告类型：新
【摘要】：甘蔗是世界制糖工业的重要作物,其易发生多种病害,对其产量和品质产生重大负面影响。为了有效管理和实施预防措施，必须及时、准确地发现疾病。在这项研究中，我们提出了一种名为 SugarcaneNet2024 的独特模型，该模型优于以前通过叶子图像处理自动快速检测甘蔗病害的方法。我们提出的模型整合了七个定制和 LASSO 正则化预训练模型的优化加权平均集成，特别是 InceptionV3、InceptionResNetV2、DenseNet201、DenseNet169、Xception 和 ResNet152V2。最初，我们添加了三个更密集的层（具有 0.0001 LASSO 正则化）、三个 30% 的 dropout 层以及三个在这些预训练模型底部启用了重规范的批量标准化，以提高性能。这一添加大大提高了甘蔗叶病分类的准确性。随后，对平均集成和个体模型进行了几项比较研究，表明集成技术表现更好。所有修改后的预训练模型的平均集成产生了出色的结果：f1 分数、精度、召回率和准确率分别为 100%、99%、99% 和 99.45%。通过实施与网格搜索相结合的优化加权平均集成技术，性能得到了进一步增强。这种优化的 SugarcaneNet2024 模型在检测甘蔗病害方面表现最好，准确率、精确度、召回率和 F1 分数分别为 99.67%、100%、100% 和 100%。]]></description>
      <guid>https://arxiv.org/abs/2403.18870</guid>
      <pubDate>Fri, 29 Mar 2024 06:16:59 GMT</pubDate>
    </item>
    </channel>
</rss>