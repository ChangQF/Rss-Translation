<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Tue, 12 Dec 2023 06:18:11 GMT</lastBuildDate>
    <item>
      <title>通过相似性驱动的嵌套重要性采样对数据集中的簇计数进行人工循环估计。 （arXiv：2312.05287v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05287</link>
      <description><![CDATA[识别簇的数量是许多数据的初步目标
分析任务。解决这个问题的常见方法是改变数量
聚类算法中的聚类（例如 $k$-means 中的“k”）并选择值
这最能解释数据。然而，计数估计可能不可靠
尤其是当图像相似度较差时。人类对成对的反馈
相似性可用于改进聚类，但现有方法无法做到这一点
不保证准确的计数估计。我们提出了一种生产方法
在给定近似值的情况下对大型数据集中的簇计数进行估计
成对相似性。我们的框架对由成对引导的边缘进行采样
相似性，我们收集人类反馈来构建统计估计
的簇计数。在技​​术方面，我们开发了一个嵌套的
重要性抽样方法，产生（渐近）无偏估计
具有可以指导人类努力的置信区间的聚类计数。
与朴素采样相比，我们的相似性驱动采样产生更多
准确的计数估计和更严格的置信区间。我们评估我们的
基于六个细粒度图像分类数据集的基准方法
在估计的簇数量上实现低错误率
与基线和替代方案相比，人工标记工作显着减少
主动聚类方法。
]]></description>
      <guid>http://arxiv.org/abs/2312.05287</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>MotionCrafter：扩散模型的一次性运动定制。 （arXiv：2312.05288v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05288</link>
      <description><![CDATA[视频的本质在于其动态动作，包括角色
动作、物体运动和摄像机运动。在生成文本到视频的同时
扩散模型最近在创建多样化内容方面取得了进步，
通过文本提示控制特定动作仍然是一个重要的
挑战。主要问题是外观和运动的耦合，通常
导致外观上过度拟合。为了应对这一挑战，我们引入
MotionCrafter，一种新颖的一次性实例引导运动定制方法。
MotionCrafter 采用并行时空架构，注入
参考运动进入基本模型的时间分量，而
空间模块独立调整以进行字符或风格控制。到
为了增强运动和外观的分离，我们提出了一种创新的
双分支运动解缠结方法，包括运动
解缠结损失和外观先验增强策略。期间
训练时，冻结的基础模型有效地提供了外观归一化
将外观与运动分开，从而保留多样性。
全面的定量和定性实验，以及用户
偏好测试，证明 MotionCrafter 可以成功集成
动态运动，同时保持基本模型的连贯性和质量
具有广泛的外观生成功能。代码可在
https://github.com/zyxElsa/MotionCrafter。
]]></description>
      <guid>http://arxiv.org/abs/2312.05288</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:11 GMT</pubDate>
    </item>
    <item>
      <title>Nuvo：用于不规则 3D 表示的神经 UV 映射。 （arXiv：2312.05283v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05283</link>
      <description><![CDATA[现有的 UV 映射算法旨在运行良好的
网格，而不是由最先进的 3D 生成的几何表示
重建和生成技术。因此，应用这些方法
通过神经辐射场和相关技术恢复的体积密度
（或从这些场三角化的网格）产生的纹理图集是
过于碎片化，无法用于视图合成或外观等任务
编辑。我们提出了一种设计用于对几何体进行操作的 UV 映射方法
通过 3D 重建和生成技术产生。而不是计算一个
在网格顶点上定义映射，我们的方法 Nuvo 使用神经场来
表示连续的 UV 映射，并将其优化为有效且
仅适用于可见点集的良好映射，即仅适用于
影响场景美观。我们证明我们的模型对于
不良几何体带来的挑战，以及它产生可编辑的 UV
可以表示详细外观的映射。
]]></description>
      <guid>http://arxiv.org/abs/2312.05283</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>0.1% 的数据让细分变得更薄。 （arXiv：2312.05284v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05284</link>
      <description><![CDATA[Segment 庞大的模型规模和苛刻的计算要求
Anything Model (SAM) 使得部署变得很麻烦
资源受限的设备。现有的 SAM 压缩方法通常
涉及从头开始训练新网络，这是一个具有挑战性的权衡
压缩成本和模型性能之间的关系。为了解决这个问题，这个
论文介绍了 SlimSAM，一种新颖的 SAM 压缩方法，可实现卓越的性能
性能与极低的培训成本。这是通过以下方式实现的
通过统一的修剪-蒸馏有效地重复使用预先训练的 SAM
框架。为了增强原始 SAM 的知识继承，我们采用了
创新的替代瘦身策略，对压缩过程进行分区
进入一个渐进的过程。与之前的修剪技术不同，我们
交替地精心修剪和提炼解耦模型结构
时尚。此外，还提出了一种新的无标签剪枝标准
将剪枝目标与优化目标保持一致，从而提高
修剪后进行后蒸馏。 SlimSAM 带来显着的性能
改进的同时要求培训成本比任何其他方法少 10 倍以上
现有的方法。即使与原始 SAM-H 相比，SlimSAM 也实现了
接近性能，同时将参数数量减少到仅 0.9% (5.7M)，
MAC 降低至 0.8% (21G)，并且仅需要 0.1% (10k) 的 SAM 训练数据。
]]></description>
      <guid>http://arxiv.org/abs/2312.05284</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>连接预训练场景文本检测器的合成世界和现实世界。 （arXiv：2312.05286v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05286</link>
      <description><![CDATA[现有的场景文本检测方法通常依赖于大量的真实数据
为了训练。由于缺乏注释的真实图像，最近的作品
尝试利用大规模标记合成数据（LSD）进行预训练
文本检测器。然而，出现了合成域与真实域的差距，进一步限制了
文本检测器的性能。不同的是，在这项工作中，我们建议
\textbf{FreeReal}，一种真实域对齐的预训练范例，可以实现
LSD 和未标记真实数据 (URD)​​ 的优势互补。
具体来说，为了在预训练中连接真实世界和合成世界，一种新颖的
基于字形的混合机制（GlyphMix）是为文本图像量身定制的。字形混合
描绘合成图像的字符结构并将其嵌入为
类似涂鸦的单元到真实图像上。在不引入真实域漂移的情况下，
GlyphMix 可自由生成带有源自以下注释的真实世界图像
合成标签。此外，当给予免费的细粒度合成标签时，
GlyphMix 可以有效地弥合源自以下方面的语言领域差距：
以英语为主的 LSD 到 URD 的多种语言版本。没有花哨的东西，
FreeReal 的平均收益分别为 4.56\%、3.85\%、3.90\% 和 1.97\%
提高 DBNet、PANet、PSENet 和 FCENet 方法的性能，
分别持续优于以前的预训练方法
四个公共数据集的显着优势。代码即将发布。
]]></description>
      <guid>http://arxiv.org/abs/2312.05286</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:10 GMT</pubDate>
    </item>
    <item>
      <title>使用新颖的时空卷积神经网络进行定量灌注图。 （arXiv：2312.05279v1 [eess.IV]）</title>
      <link>http://arxiv.org/abs/2312.05279</link>
      <description><![CDATA[动态磁化率对比磁共振成像 (DSC-MRI)
广泛用于评估急性缺血性中风以区分可挽救的组织
和梗塞核心。为此，传统方法采用反卷积
众所周知，奇异值分解等技术很容易受到攻击
噪声，可能会扭曲导出的灌注参数。然而，
深度学习技术可以利用它，可以准确地估计
与传统临床方法相比的临床灌注参数。
因此，本研究提出了一种灌注参数估计网络
考虑空间和时间信息，时空网络
（ST-Net），首次。所提出的网络包括一个设计的
物理损失函数进一步增强模型性能。结果
表明网络可以准确估计灌注参数，
包括脑血容量（CBV）、脑血流量（CBF）和时间
残差函数的最大值 (Tmax)。结构相似性指数（SSIM）
CBV、CBF 和 Tmax 参数的平均值分别为 0.952、0.943 和 0.863，
分别。低灌注区域的 DICE 评分达到 0.859，
表现出高度的一致性。所提出的模型还保持时间
效率，接近商业黄金标准的性能
软件。
]]></description>
      <guid>http://arxiv.org/abs/2312.05279</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>X2-Softmax：用于人脸识别的边缘自适应损失函数。 （arXiv：2312.05281v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05281</link>
      <description><![CDATA[学习不同面孔的判别特征是一项重要任务
在人脸识别方面。通过神经网络提取人脸特征，就变成
方便衡量不同人脸图像的相似度，从而使得人脸
识别可能。增强神经网络的人脸特征
可分离性，在训练过程中加入角度裕度很常见
实践。最先进的损失函数 CosFace 和 ArcFace 应用固定
类别权重之间的边距，以增强类别间的分离
面部特征。由于训练集中样本的分布为
不平衡，不同身份之间的相似性是不平等的。所以，
使用不适当的固定角度裕量可能会导致问题
模型难以收敛或者人脸特征没有区分度
足够的。边距是有角度的更符合我们的直觉
适应性，随着类之间角度的增加而增加。在这个
在论文中，我们提出了一种新的角度边缘损失，名为 X2-Softmax。 X2-Softmax损失
具有自适应角度边距，可提供随角度增加而增加的边距
不同阶级之间的角度越来越大。角度自适应余量确保
模型灵活性，有效提高人脸识别效果。我们
在 MS1Mv3 数据集上训练了具有 X2-Softmax 损失的神经网络，并且
在多个评估基准上进行了测试，以证明其有效性和
我们的损失函数的优越性。实验代码和训练模型为
发布于https://github.com/xujiamu123/X2-Softmax/tree/main。
]]></description>
      <guid>http://arxiv.org/abs/2312.05281</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>走向边缘设备学习：在预算约束下选择要更新的神经元的方法。 （arXiv：2312.05282v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.05282</link>
      <description><![CDATA[在极端记忆和高效的设备上学习领域
由于计算限制，成功方法之间仍然存在显着差距。
尽管人们在高效推理方面付出了相当大的努力，但主要的
高效学习的障碍是反向传播的高昂成本。这
经常计算梯度和更新网络参数所需的资源
超出严格限制的内存预算的限制。本文挑战
传统智慧并提出了一系列实验来揭示
存在上级子网。此外，我们暗示了潜在的
微调时通过动态神经元选择策略获得显着收益
一个目标任务。我们的努力扩展到最近动态神经元的适应
Bragagnolo 等人首创的选择策略。 （NEq），揭示了其
在最严格的情况下的有效性。我们的实验表明，在
在平均情况下，NEq 启发的方法相对于随机方法的优越性
选择。这一观察结果为进一步探索提供了一条引人注目的途径
在该领域，突出了设计一类新算法的机会
旨在方便参数更新选择。我们的发现带来了新的
极端条件下设备端学习领域的可能性时代
限制并鼓励追求创新战略，以实现高效、
资源友好型模型微调。
]]></description>
      <guid>http://arxiv.org/abs/2312.05282</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:09 GMT</pubDate>
    </item>
    <item>
      <title>用于海事计算机视觉应用的图像和 AIS 数据融合技术。 （arXiv：2312.05270v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05270</link>
      <description><![CDATA[深度学习目标检测方法，如 YOLOv5，在以下方面非常有效
识别海上船只，但往往缺乏重要的详细信息
实际应用。在本文中，我们通过开发解决了这个问题
将自动识别系统 (AIS) 数据与船舶融合的技术
在图像中检测以创建数据集。这种融合丰富了船舶图像
船舶相关数据，例如类型、尺寸、速度和方向。我们的方法
通过估计，将检测到的船舶与其相应的 AIS 消息关联起来
距离和方位角使用基于单应性的方法，适用于固定
并定期平移摄像机。该技术对于创建
用于水路交通管理、遭遇检测和
监视。我们引入了一个新颖的数据集，其中包含拍摄的图像
各种天气状况及其相应的 AIS 消息。这个数据集
为炼油容器检测算法提供稳定的基线
轨迹预测模型。为了评估我们方法的性能，我们手动
注释了该数据集的一部分。结果显示总体
关联准确度为 74.76 %，关联准确度为固定值
相机利用率达到85.06%。这证明了我们的方法的潜力
创建用于血管检测、姿态估计和自动标记的数据集
管道。
]]></description>
      <guid>http://arxiv.org/abs/2312.05270</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>StableQ：通过文本到图像数据增强数据稀缺量化。 （arXiv：2312.05272v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05272</link>
      <description><![CDATA[虽然低位量化可以实现深度数据的高效存储和推理
神经网络，它常常需要使用训练数据来维持
抵抗量化误差的能力。然而，训练数据经常
受隐私或版权问题的影响。在这项工作中，我们解决了
数据稀缺量化的挑战，其中训练数据的访问是
对于量化目的而言，严重受限或不存在。传统的
方法通常依赖于反转虚拟图像或联合训练
生成模型来生成合成输入样本。然而，这些方法
难以在大规模数据集中准确地重新创建复杂对象，例如
图像网。为了克服这些限制，我们引入了 StableQ，一种新方法
利用先进的文本到图像扩散模型来生成
高分辨率、逼真的合成数据。为了验证产品的质量
生成的数据，我们实现了两种强大的过滤机制。这些机制
旨在选择与内在特征非常相似的图像
实际训练数据的特征。此外，在以下场景中
可用的训练数据有限，我们使用这些数据来指导合成
通过反转文本中嵌入的可学习标记来生成数据的过程
编码器。我们广泛的实验结果表明 StbaleQ 设定了新的
零样本和少样本量化的基准，优于现有的
方法的准确性和效率。
]]></description>
      <guid>http://arxiv.org/abs/2312.05272</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>从目标到源：用于测试时间适应的基于指导的扩散模型。 （arXiv：2312.05274v1 [cs.LG]）</title>
      <link>http://arxiv.org/abs/2312.05274</link>
      <description><![CDATA[最近的测试时间适应（TTA）工作旨在减轻域转移
通过在每个域中重新训练源分类器来解决问题。另一方面，
扩散模型的出现为TTA提供了另一种解决方案，即
直接将测试数据从目标域映射到源域
在源域中预先训练的扩散模型上。源分类器
不需要进行微调。然而，1）语义信息丢失
测试数据到源域以及 2) 源域之间的模型转换
分类器和扩散模型会阻止扩散模型映射
测试数据正确返回源域。在本文中，我们提出了一个
新颖的基于引导的扩散驱动适应（GDDA）来克服数据
转移，让扩散模型找到更好的方式回到源头。
具体而言，我们首先提出细节和全球指导，以更好地保持
测试和源数据的通用语义。这两个指南包括
对比损失和均方误差来减轻信息损失
充分探索扩散模型和测试数据。同时，我们提出一个
分类器感知指导，以减少模型转变引起的偏差，这
可以将源分类器的信息合并到生成过程中
的扩散模型。对三个图像数据集进行了广泛的实验
三个分类器主干表明 GDDA 显着表现更好
比最先进的基线。在 CIFAR-10C、CIFAR-100C 和 ImageNetC 上，
GDDA 平均准确率提高了 11.54\%、19.05\% 和 11.63\%，
分别。与以下方法相比，GDDA 甚至达到了相同的性能
重新训练分类器。该代码可在补充材料中找到。
]]></description>
      <guid>http://arxiv.org/abs/2312.05274</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>3D 复制粘贴：用于单目 3D 检测的物理上合理的对象插入。 （arXiv：2312.05277v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05277</link>
      <description><![CDATA[单目 3D 物体检测的一个主要挑战是多样性有限
以及真实数据集中对象的数量。在增强真实场景的同时
虚拟物体有望提高物体的多样性和数量
对象，由于缺乏有效的 3D 对象插入，它仍然难以捉摸
复杂实拍场景中的方法。在这项工作中，我们研究增强
具有单眼 3D 对象虚拟对象的复杂真实室内场景
检测。主要挑战是自动识别合理的物理
虚拟资产的属性（例如位置、外观、大小等）
杂乱的真实场景。为了应对这一挑战，我们提出了一种物理方法
合理的室内 3D 对象插入方法自动复制虚拟
物体并将它们粘贴到真实场景中。场景中生成的对象有
具有合理物理位置和外观的 3D 边界框。在
特别是，我们的方法首先识别物理上可行的位置和姿势
用于插入的对象，以防止与现有房间布局发生冲突。
随后，它估计插入的空间变化的照明
位置，使虚拟对象能够身临其境地融入到现实中
具有合理外观和投射阴影的原始场景。我们表明我们的
增强方法显着改进了现有的单目 3D 物体模型
并实现了最先进的性能。我们第一次展示
物理上合理的 3D 对象插入，用作生成数据
增强技术，可以显着改善判别能力
下游任务，例如单目 3D 物体检测。项目网站：
https://gyhandy.github.io/3D-Copy-Paste/
]]></description>
      <guid>http://arxiv.org/abs/2312.05277</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:08 GMT</pubDate>
    </item>
    <item>
      <title>非对比计算机断层扫描中自动检测小肾癌。 (arXiv:2312.05258v1 [eess.IV])</title>
      <link>http://arxiv.org/abs/2312.05258</link>
      <description><![CDATA[这项研究引入了肾癌 (RC) 检测的自动化流程
在非对比计算机断层扫描 (NCCT) 中。在我们的管道开发过程中，
我们测试了三个检测模型：形状模型、2D 和 3D 轴向样本
模型。训练 (n=1348) 和测试 (n=64) 数据是从开放收集的
来源（KiTS23、Abdomen1k、CT-ORG）和剑桥大学医院（CUH）。
交叉验证和测试的结果表明，二维轴向样本
模型具有最高的小（$\leq$40mm 直径）RC 检测区域
曲线（AUC）为0.804。我们的管道实现了 61.9\% 的灵敏度和 92.7\%
未见测试数据对小肾癌的特异性。我们的成果很多
比之前自动检测小肾的尝试更准确
NCCT 中的癌症，这是 RC 筛查最有可能的成像方式。这
管道提供了一个有希望的进步，可以实现肾脏筛查
癌症。
]]></description>
      <guid>http://arxiv.org/abs/2312.05258</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>使用隐私兼容功能进行野外多模式群体情绪识别。 （arXiv：2312.05265v1 [cs.AI]）</title>
      <link>http://arxiv.org/abs/2312.05265</link>
      <description><![CDATA[本文探讨了符合隐私的群体级情绪识别
2023 年 EmotiW 挑战赛中的“野外”。团体层面的情感
识别在许多领域都很有用，包括社交机器人、
对话代理、电子辅导和学习分析。这项研究强加
本身仅使用全局特征，避免单个特征，即所有特征
可用于识别或跟踪视频中的人物（面部标志、身体特征）
姿势、音频二值化等）。所提出的多模态模型由
视频和音频分支具有模态之间的交叉注意力。这
video分支基于微调的ViT架构。音频分支
提取梅尔频谱图并通过 CNN 块将其输入变压器
编码器。我们的训练范例包括生成的合成数据集
增加我们的模型对图像中面部表情的敏感度
数据驱动的方式。大量的实验表明了我们的重要性
方法。我们的隐私合规提案在 EmotiW 上表现相当不错
挑战，验证和验证准确率分别为 79.24% 和 75.13%
最佳模型的测试集。值得注意的是，我们的研究结果强调，
通过使用符合隐私要求的功能可以达到此准确度级别
视频上只有5帧均匀分布。
]]></description>
      <guid>http://arxiv.org/abs/2312.05265</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:07 GMT</pubDate>
    </item>
    <item>
      <title>LifelongMemory：利用法学硕士回答以自我为中心的视频中的查询。 （arXiv：2312.05269v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2312.05269</link>
      <description><![CDATA[以自我为中心的视频自然语言查询 (NLQ) 任务涉及本地化
以自我为中心的视频中的时间窗口，提供了对姿势的答案
查询，在构建个性化人工智能助手方面有着广泛的应用。
此任务的先前方法侧重于网络的改进
架构并利用预训练来增强图像和视频功能，
但一直在努力捕捉长期的时间依赖性
视频和繁琐的端到端培训。受最近进展的推动
我们介绍大型语言模型（LLM）和视觉语言模型
LifelongMemory，一种新颖的框架，利用多个预先训练的模型
回答来自大量以自我为中心的视频内容的疑问。我们解决独特的
通过采用预先训练的字幕模型来创建详细的字幕来应对挑战
视频的叙述。然后这些叙述被用来促使冻结法学硕士
生成粗粒度的时间窗口预测，随后
使用预先训练的 NLQ 模型进行细化。实证结果表明我们的
该方法实现了与现有的有监督的端到端竞争的性能
学习方法，强调整合多个预训练的潜力
复杂视觉语言任务中的多模态大语言模型。我们提供一个
对我们的关键设计决策和超参数进行全面分析
管道，提供见解和实用指南。
]]></description>
      <guid>http://arxiv.org/abs/2312.05269</guid>
      <pubDate>Tue, 12 Dec 2023 06:18:07 GMT</pubDate>
    </item>
    </channel>
</rss>