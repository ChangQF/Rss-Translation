<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 03 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Freeview Sketching：基于视图感知的细粒度草图图像检索</title>
      <link>https://arxiv.org/abs/2407.01810</link>
      <description><![CDATA[arXiv:2407.01810v1 公告类型：新
摘要：在本文中，我们深入研究了基于细粒度草图的图像检索 (FG-SBIR) 的复杂动态，解决了一个关键但被忽视的方面——草图创建过程中的视点选择。与通过大量数据集无缝处理不同视图的照片系统不同，草图系统从固定视角收集的数据有限，面临挑战。我们的试点研究采用了预先训练的 FG-SBIR 模型，突出了当查询草图的视点与目标实例不同时系统的困难。有趣的是，一份问卷调查显示，用户渴望自主性，其中相当一部分人倾向于特定于视图的检索。为了解决这个问题，我们提倡一个视图感知系统，无缝地适应与视图无关的任务和特定于视图的任务。我们的第一个贡献克服了数据集的限制，利用了 3D 对象的多视图 2D 投影，从而实现了跨模式视图感知。第二个贡献通过解缠引入了可定制的跨模式功能，从而实现了轻松的模式切换。在标准数据集上进行的大量实验验证了我们方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2407.01810</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:57 GMT</pubDate>
    </item>
    <item>
      <title>CLIP 分歧：语言引导的无监督领域适应</title>
      <link>https://arxiv.org/abs/2407.01842</link>
      <description><![CDATA[arXiv:2407.01842v1 公告类型：新
摘要：无监督域自适应 (UDA) 已成为解决标记源域和未标记目标域之间差异的一种流行解决方案。最近，一些研究工作已经开展，以利用大型视觉语言模型（例如 CLIP），然后对它们进行微调或学习提示，以解决具有挑战性的 UDA 任务。在这项工作中，我们通过直接利用 CLIP 来测量域差异，将方向转向新的方向，并提出了一种新的语言引导的 UDA 方法，称为 CLIP-Div。我们的主要思想是利用 CLIP 1) 通过获得的域无关分布来测量域差异和 2) 使用语言指导校准目标伪标签，以有效减少域差距并提高 UDA 模型的泛化能力。具体而言，我们的主要技术贡献在于提出了两种新颖的语言引导领域分歧测量损失：绝对分歧和相对分歧。这些损失项提供了精确的指导方针，用于将源域和目标域的分布与从 CLIP 得出的​​领域无关分布对齐。此外，我们提出了一种语言引导的伪标签策略来校准目标伪标签。在此基础上，我们表明，进一步实施自我训练可以增强 UDA 模型在目标域上的泛化能力。CLIP-Div 远远超过了最先进的基于 CNN 的方法，在 Office-Home 上实现了 +10.3% 的性能提升，在 Office-31 上实现了 +1.5% 的性能提升，在 VisDA-2017 上实现了 +0.2% 的性能提升，在 DomainNet 上实现了 +24.3% 的性能提升。]]></description>
      <guid>https://arxiv.org/abs/2407.01842</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:57 GMT</pubDate>
    </item>
    <item>
      <title>无标签神经语义图像合成</title>
      <link>https://arxiv.org/abs/2407.01790</link>
      <description><![CDATA[arXiv:2407.01790v1 公告类型：新
摘要：最近的研究表明，在整合空间条件以控制大型、预训练的文本到图像扩散模型方面取得了巨大进展。尽管取得了这些进展，但现有方法使用手工制作的条件输入来描述空间图像内容，这些输入要么在语义上含糊不清（例如边缘），要么需要昂贵的手动注释（例如语义分割）。为了解决这些限制，我们提出了一种新的无标签方式来调节扩散模型，以实现细粒度的空间控制。我们引入了神经语义图像合成的概念，它使用从预训练的基础模型中提取的神经布局作为条件。神经布局的优势在于它们提供了所需图像的丰富描述，包含场景的语义和详细几何形状。我们通过实验表明，与使用昂贵的语义标签图创建的图像相比，通过神经语义图像合成合成的图像实现了相似或更优异的语义类像素级对齐。同时，它们比其他无标签条件选项（例如边缘或深度）能更好地捕捉语义、实例分离和对象方向。此外，我们表明，神经布局条件生成的图像可以有效地增强真实数据，以训练各种感知任务。]]></description>
      <guid>https://arxiv.org/abs/2407.01790</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:56 GMT</pubDate>
    </item>
    <item>
      <title>{\mu}-Bench：显微镜理解的视觉语言基准</title>
      <link>https://arxiv.org/abs/2407.01791</link>
      <description><![CDATA[arXiv:2407.01791v1 公告类型：新
摘要：显微镜技术的最新进展使得细胞生物学和生物医学研究中能够快速生成 TB 级的图像数据。视觉语言模型 (VLM) 为大规模生物图像分析提供了一种有前途的解决方案，提高了研究人员的效率，识别了新的图像生物标记，并加速了假设的生成和科学发现。然而，缺乏标准化、多样化和大规模的视觉语言基准来评估 VLM 在生物图像理解中的感知和认知能力。为了解决这一差距，我们引入了 {\mu}-Bench，这是一个专家策划的基准，涵盖了各种科学学科（生物学、病理学）、显微镜模式（电子、荧光、光）、尺度（亚细胞、细胞、组织）和正常和异常状态的生物体的 22 项生物医学任务。我们在 {\mu}-Bench 上评估了最先进的生物医学、病理学和通用 VLM，发现：i) 当前模型在所有类别上都表现不佳，即使是区分显微镜模式等基本任务也是如此；ii) 当前针对生物医学数据进行微调的专家模型通常比通用模型表现更差；iii) 在特定显微镜领域进行微调可能会导致灾难性的遗忘，侵蚀其基础模型中编码的先前生物医学知识。iv) 微调和预训练模型之间的权重插值为遗忘提供了一种解决方案，并提高了生物医学任务的总体性能。我们根据宽松的许可发布了 {\mu}-Bench，以加速显微镜基础模型的研究和开发。]]></description>
      <guid>https://arxiv.org/abs/2407.01791</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:56 GMT</pubDate>
    </item>
    <item>
      <title>fVDB：用于稀疏、大规模、高性能空间智能的深度学习框架</title>
      <link>https://arxiv.org/abs/2407.01781</link>
      <description><![CDATA[arXiv:2407.01781v1 公告类型：新
摘要：我们提出了 fVDB，这是一种针对大规模 3D 数据进行深度学习的新型 GPU 优化框架。fVDB 提供了一整套可微分原语，用于构建深度学习架构，用于 3D 学习中的常见任务，例如卷积、池化、注意力、光线追踪、网格划分等。
fVDB 同时提供了比现有框架大得多的功能集（原语和运算符），且效率没有损失：我们的运算符的性能可匹敌或超过其他范围较窄的框架。此外，fVDB 可以处理比之前的工作占用空间和空间分辨率更大的数据集，同时在小输入上提供具有竞争力的内存占用。为了实现多功能性和性能的结合，fVDB 依赖于单一的新型 VDB 索引网格加速结构以及几项关键创新，包括 GPU 加速稀疏网格构造、使用张量核的卷积、使用分层数字差分分析仪算法 (HDDA) 的快速光线追踪内核和锯齿状张量。
我们的框架与 PyTorch 完全集成，从而实现了与现有管道的互操作性，并且我们在许多代表性任务上证明了其有效性，例如大规模点云分割、高分辨率 3D 生成建模、无界神经辐射场和大规模点云重建。]]></description>
      <guid>https://arxiv.org/abs/2407.01781</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:55 GMT</pubDate>
    </item>
    <item>
      <title>解决深度视觉模型的一个根本限制：缺乏空间注意力</title>
      <link>https://arxiv.org/abs/2407.01782</link>
      <description><![CDATA[arXiv:2407.01782v1 公告类型：新
摘要：本文的主要目的是强调当前深度学习模型（尤其是视觉模型）的一个重大局限性。与人类视觉不同，人类视觉可以有效地选择必要的视觉区域进行进一步处理，从而实现高速和低能耗，而深度视觉模型可以处理整个图像。在这项工作中，我们从更广泛的角度研究了这个问题，并提出了一种解决方案，可以为下一代更高效的视觉模型铺平道路。基本上，卷积和池化操作被选择性地应用于改变的区域，并将更改图发送到后续层。该图指示需要重复哪些计算。代码可在 https://github.com/aliborji/spatial_attention 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.01782</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:55 GMT</pubDate>
    </item>
    <item>
      <title>数码相机系统中的最佳采样镜头定位</title>
      <link>https://arxiv.org/abs/2407.01789</link>
      <description><![CDATA[arXiv:2407.01789v1 公告类型：新
摘要：在当代成像系统中，实现最佳自动对焦 (AF) 性能取决于精确的镜头定位。大量研究深入研究了在被动、主动和混合自动对焦系统中确定理想镜头位置的算法。本文探讨了在焦点搜索过程中优化镜头位置所必需的数学复杂性和实际考虑因素，解决了自动对焦系统中遇到的总体挑战，例如平衡速度和准确性。此外，本文提出的镜头位置计算可应用于各种对焦算法，包括对焦包围。所提出的方法具有适应性和可扩展性，使其适合集成到各种相机系统中，从智能手机和数码单反相机到显微镜和工业成像设备。]]></description>
      <guid>https://arxiv.org/abs/2407.01789</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:55 GMT</pubDate>
    </item>
    <item>
      <title>表格结构识别中的不确定性量化</title>
      <link>https://arxiv.org/abs/2407.01731</link>
      <description><![CDATA[arXiv:2407.01731v1 公告类型：新
摘要：量化机器学习模型的不确定性是通过检测低置信度预测来减少人工验证工作量的关键步骤。本文提出了一种表格结构识别（TSR）不确定性量化（UQ）的方法。所提出的 UQ 方法建立在称为测试时间增强（TTA）的混合专家方法之上。我们的主要思想是丰富和多样化表格表示，以突出具有高识别不确定性的单元格。为了评估其有效性，我们提出了两种启发式方法来区分高度不确定的细胞和正常细胞，即掩蔽和细胞复杂性量化。掩蔽涉及改变像素强度以判断检测不确定性。细胞复杂性量化通过每个细胞与相邻细胞的拓扑关系来衡量其不确定性。基于标准基准数据集的评估结果表明，所提出的方法可有效量化 TSR 模型中的不确定性。据我们所知，这项研究是首个在 TSR 任务中实现 UQ 的研究。我们的代码和数据可以在以下网址获得：https://github.com/lamps-lab/UQTTA.git。]]></description>
      <guid>https://arxiv.org/abs/2407.01731</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:54 GMT</pubDate>
    </item>
    <item>
      <title>DRAGON：无人机和地面高斯溅射技术用于 3D 建筑物重建</title>
      <link>https://arxiv.org/abs/2407.01761</link>
      <description><![CDATA[arXiv:2407.01761v1 公告类型：新
摘要：从图像数据进行 3D 建筑物重建是从城市规划到侦察等许多应用的重要任务。现代新视图合成 (NVS) 方法（如 NeRF 和 Gaussian Splatting）提供了强大的技术，可用于以无监督的方式从自然 2D 图像开发 3D 模型。这些算法通常需要输入围绕感兴趣场景的训练视图，对于大型建筑物，通常并非所有摄像机高度都可用。特别是，大多数建筑物中最容易获得的大规模摄像机视点位于近地面（例如，使用手机）和空中（无人机）高度。然而，由于无人机和地面图像集之间的视点存在显著差异，相机配准（NVS 算法的必要步骤）失败。在这项工作中，我们提出了一种方法 DRAGON，它可以将无人机和地面建筑物图像作为输入并生成 3D NVS 模型。 DRAGON 的关键见解是，中间海拔图像可以通过 NVS 算法本身在具有感知正则化的迭代过程中推断出来，从而弥合两个海拔之间的视觉特征差距并实现配准。我们使用 Google Earth Studio 编制了 9 个大型建筑场景的半合成数据集，并从数量和质量上证明了与基线策略相比，DRAGON 可以在这个数据集上生成引人注目的渲染图。]]></description>
      <guid>https://arxiv.org/abs/2407.01761</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:54 GMT</pubDate>
    </item>
    <item>
      <title>利用加速计算策略优化 X 射线图像分类学习，实现多类疾病诊断</title>
      <link>https://arxiv.org/abs/2407.01705</link>
      <description><![CDATA[arXiv:2407.01705v1 公告类型：新
摘要：基于 X 射线图像的疾病诊断的关键在于确保在样本中识别疾病的准确性，这项任务充满了挑战，源于假阳性和假阴性的发生。假阳性会带来错误识别不存在疾病的风险，导致误诊和患者护理质量下降。相反，假阴性会造成忽视真正异常的威胁，可能导致治疗和干预的延误，从而导致不良的患者结果。克服这些挑战的紧迫性迫使人们不断努力提高计算框架内 X 射线图像分析算法的精度和可靠性。本研究引入了针对 X 射线图像多类疾病诊断而定制的改进的预训练 ResNet 模型，结合了先进的优化策略来减少训练和推理任务的执行运行时间。主要目标是通过加速实施 PyTorch、CUDA、混合精度训练和学习率调度程序来实现切实的性能改进。虽然结果表明，普通训练和 CUDA 加速训练之间的执行运行时间有显著改善，但各种训练优化模式之间的差异可以忽略不计。这项研究标志着在优化计算方法以减少较大模型的训练执行时间方面取得了重大进展。此外，我们探索了使用 MPI4Py 进行有效并行数据处理的潜力，以便在多个节点上分布梯度下降优化，并利用多处理来加快较大数据集的数据预处理。]]></description>
      <guid>https://arxiv.org/abs/2407.01705</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>VolETA：一次性和少量食物体积估计</title>
      <link>https://arxiv.org/abs/2407.01717</link>
      <description><![CDATA[arXiv:2407.01717v1 公告类型：新
摘要：准确的食物体积估计对于饮食评估、营养跟踪和份量控制应用至关重要。我们提出了 VolETA，这是一种使用 3D 生成技术估计食物体积的复杂方法。我们的方法使用一张或几张 RGBD 图像创建食物对象的缩放 3D 网格。我们首先根据 RGB 图像选择关键帧，然后使用 XMem++ 在 RGB 图像中分割参考对象。同时，使用 PixSfM 技术估计和细化相机位置。分割的食物图像、参考对象和相机姿势组合在一起，形成适合 NeuS2 的数据模型。对参考和食物对象进行独立的网格重建，并使用基于参考对象的 MeshLab 确定缩放因子。此外，深度信息用于通过估计潜在体积范围来微调缩放因子。然后将微调后的缩放因子应用于清洁的食物网格以进行准确的体积测量。类似地，我们将分割后的 RGB 图像输入到 One-2-3-45 模型中，进行一次性食物体积估计，从而生成网格。然后，我们将获得的缩放因子应用于清洁后的食物网格，以进行准确的体积测量。我们的实验表明，我们的方法可以有效解决遮挡、变化的光照条件和复杂的食物几何形状，使用 MTF 数据集实现稳健而准确的体积估计，MAPE 为 10.97%。这种创新方法提高了体积评估的精度，并显著促进了计算营养和饮食监测的进步。]]></description>
      <guid>https://arxiv.org/abs/2407.01717</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>分组离散表示指导以对象为中心的学习</title>
      <link>https://arxiv.org/abs/2407.01726</link>
      <description><![CDATA[arXiv:2407.01726v1 公告类型：新
摘要：与人类将视觉场景视为物体类似，以对象为中心的学习 (OCL) 可以将密集的图像或视频抽象为稀疏的对象级特征。基于 Transformer 的 OCL 可以很好地处理复杂纹理，这是由于离散表示的解码指导，该离散表示是通过使用来自码本的模板特征将图像或视频特征图中的噪声特征离散化而获得的。然而，将特征视为最小单位会忽略它们的组成属性，从而阻碍模型泛化；用自然数对特征进行索引会失去属性级的共性和特征，从而削弱模型收敛的启发式方法。我们提出 \textit{分组离散表示} (GDR) 来解决这些问题，方法是将特征分组为属性并使用元组数字对其进行索引。在跨不同查询初始化、数据集模态和模型架构的大量实验中，GDR 持续提高了收敛性和泛化能力。可视化显示我们的方法有效地捕获了特征中的属性级信息。源代码将在接受后提供。]]></description>
      <guid>https://arxiv.org/abs/2407.01726</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:53 GMT</pubDate>
    </item>
    <item>
      <title>学习频率感知动态变换器以实现一体化图像恢复</title>
      <link>https://arxiv.org/abs/2407.01636</link>
      <description><![CDATA[arXiv:2407.01636v1 公告类型：新 
摘要：这项工作旨在解决一体化图像恢复任务，该任务旨在使用单个模型处理多种类型的退化。主要挑战是从输入的退化图像中提取退化表示，并使用它们来指导模型对特定退化类型的适应。认识到各种退化对不同频带的图像内容的影响不同，我们提出了一种从频率角度进行的新型一体化图像恢复方法，利用先进的视觉变换器。我们的方法由两个主要组件组成：频率感知退化先验学习变换器 (Dformer) 和退化自适应恢复变换器 (Rformer)。Dformer 通过将输入分解为不同的频率分量来捕捉各种退化的基本特征。通过了解退化如何影响这些频率成分，Dformer 可以学习到有效的先验知识，从而有效地指导恢复过程。然后，Rformer 采用降级自适应自注意力模块，在学习到的降级表示的指导下，选择性地关注受影响最大的频率分量。大量实验结果表明，我们的方法在四个代表性恢复任务上优于现有方法，包括去噪、去雨、去雾和去模糊。此外，我们的方法还有利于处理空间变化的降级和看不见的降级水平。]]></description>
      <guid>https://arxiv.org/abs/2407.01636</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:52 GMT</pubDate>
    </item>
    <item>
      <title>SeFlow：自动驾驶中的自监督场景流方法</title>
      <link>https://arxiv.org/abs/2407.01702</link>
      <description><![CDATA[arXiv:2407.01702v1 公告类型：新
摘要：场景流估计可预测连续 LiDAR 扫描中每个点的 3D 运动。这种详细的点级信息可以帮助自动驾驶汽车准确预测和了解周围环境的动态变化。当前最先进的方法需要带注释的数据来训练场景流网络，而标记的成本本质上限制了它们的可扩展性。自监督方法可以克服上述限制，但面临阻碍最佳性能的两个主要挑战：点分布不平衡和无视物体级运动约束。在本文中，我们提出了 SeFlow，这是一种自监督方法，它将有效的动态分类集成到基于学习的场景流管道中。我们证明对静态和动态点进行分类有助于为不同的运动模式设计有针对性的目标函数。我们还强调了内部集群一致性和正确的对象点关联对于改进场景流估计的重要性，特别是在对象细节方面。我们的实时方法在 Argoverse 2 和 Waymo 数据集上的自监督场景流任务中实现了最先进的性能。代码和经过训练的模型权重在 https://github.com/KTH-RPL/SeFlow 上开源。]]></description>
      <guid>https://arxiv.org/abs/2407.01702</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:52 GMT</pubDate>
    </item>
    <item>
      <title>CVPR 2024 WeatherProof 数据集挑战赛技术报告：配对真实数据的语义分割</title>
      <link>https://arxiv.org/abs/2407.01579</link>
      <description><![CDATA[arXiv:2407.01579v1 公告类型：新
摘要：本技术报告介绍了 CVPR&#39;24 UG2 WeatherProof 数据集挑战赛第二届获奖者的实施细节。该挑战赛旨在对来自世界各地的因不同程度的天气而退化的图像进行语义分割。我们通过引入预先训练的大规模视觉基础模型：InternImage 来解决此问题，并使用具有不同噪声级别的图像对其进行训练。此外，我们在训练过程中没有使用其他数据集，而是在最终测试过程中使用密集 CRF 作为后处理。结果，我们以 45.1 mIOU 获得了挑战赛第二名，提交的论文数量少于其他获奖者。]]></description>
      <guid>https://arxiv.org/abs/2407.01579</guid>
      <pubDate>Thu, 04 Jul 2024 03:17:51 GMT</pubDate>
    </item>
    </channel>
</rss>