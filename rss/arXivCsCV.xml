<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 07 Mar 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>DLP-GAN：学习用生成对抗网络绘制现代中国风景照片</title>
      <link>https://arxiv.org/abs/2403.03456</link>
      <description><![CDATA[arXiv:2403.03456v1 公告类型：新
摘要：中国山水画具有独特的艺术风格，其绘画技法无论是色彩的运用还是对物象的写实表现都具有高度的抽象性。以前的方法侧重于从现代照片转移到古代水墨画。然而，将风景画转化为现代照片却很少受到关注。为了解决这些问题，在本文中，我们（1）提出了 DLP-GAN (\textbf{D}raw Modern Chinese \textbf{L}andscape \textbf{P}hotos with \textbf{G}enerative \textbf{A} dversarial \textbf{N}etwork），一种具有新颖的非对称循环映射的无监督跨域图像翻译框架，以及（2）引入基于密集融合模块的生成器来匹配不同的翻译方向。此外，提出了双重一致性损失来平衡模型绘画的现实性和抽象性。这样，我们的模型就可以画出现代意义上的风景照片和素描了。最后，根据我们收集的现代景观和草图数据集，我们将模型生成的图像与其他基准进行比较。包括用户研究在内的大量实验表明，我们的模型优于最先进的方法。]]></description>
      <guid>https://arxiv.org/abs/2403.03456</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:27 GMT</pubDate>
    </item>
    <item>
      <title>Slot Abstractors：迈向可扩展的抽象视觉推理</title>
      <link>https://arxiv.org/abs/2403.03458</link>
      <description><![CDATA[arXiv:2403.03458v1 公告类型：新
摘要：抽象视觉推理是人类特有的能力，可以识别从对象特征中抽象出来的关系模式，并将这些模式系统地推广到看不见的问题。最近的工作通过集成用于提取以对象为中心的表示的基于槽的方法以及用于关系抽象的强归纳偏差，在涉及多对象输入的视觉推理任务中展示了强大的系统泛化。然而，这种方法仅限于包含单个规则的问题，并且无法扩展到包含大量对象的视觉推理问题。最近的其他工作提出了 Abstractors，这是 Transformer 的扩展，它融合了强关系归纳偏差，从而继承了 Transformer 的可扩展性和多头架构，但尚未证明这种方法如何应用于多对象视觉输入。在这里，我们结合了上述方法的优点，提出了 Slot Abstractors，这是一种抽象视觉推理的方法，可以扩展到涉及大量对象及其之间的多种关系的问题。该方法在四个抽象视觉推理任务中展示了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.03458</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:27 GMT</pubDate>
    </item>
    <item>
      <title>HDRFlow：大运动的实时 HDR 视频重建</title>
      <link>https://arxiv.org/abs/2403.03447</link>
      <description><![CDATA[arXiv:2403.03447v1 公告类型：新
摘要：从通过交替曝光捕获的图像序列重建高动态范围（HDR）视频具有挑战性，特别是在存在大型相机或物体运动的情况下。现有方法通常使用光流或注意机制来对齐低动态范围序列以进行消重影。然而，它们通常难以处理大型复杂运动并且计算成本昂贵。为了应对这些挑战，我们提出了一种专为实时 HDR 视频重建而定制的强大且高效的流量估计器，名为 HDRFlow。 HDRFlow具有三种新颖的设计：HDR域对齐损失（HALoss）、具有多尺寸大内核（MLK）的高效流网络以及新的HDR流训练方案。 HALoss 监督我们的流网络来学习面向 HDR 的流，以便在饱和和黑暗区域中进行准确对齐。 MLK 可以以可忽略不计的成本有效地模拟大型运动。此外，我们将合成数据 Sintel 纳入我们的训练数据集中，利用其提供的前向流和我们生成的后向流来监督我们的流网络，从而增强我们在大运动区域的性能。大量实验表明，我们的 HDRFlow 在标准基准测试中优于以前的方法。据我们所知，HDRFlow 是第一个针对交替曝光捕获的视频序列的实时 HDR 视频重建方法，能够以 25 毫秒处理 720p 分辨率输入。]]></description>
      <guid>https://arxiv.org/abs/2403.03447</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>D4C 手套系：通过分布和划线概念解决 RPM 和 Bongard-logo 问题</title>
      <link>https://arxiv.org/abs/2403.03452</link>
      <description><![CDATA[arXiv:2403.03452v1 公告类型：新
摘要：本文介绍了抽象推理领域的重大进展，特别是针对 Raven 渐进矩阵 (RPM) 和 Bongard-Logo 问题。我们首先介绍 D2C，一种重新定义这些领域中的概念边界并弥合高级概念与其低维表示之间的差距的方法。利用这个基础，我们提出了 D3C，一种解决 Bongard-Logo 问题的新颖方法。 D3C 估计图像表示的分布并测量它们的 Sinkhorn 距离，以实现显着的推理准确性。这种创新方法为图像之间的关系提供了新的见解，并推进了抽象推理的最先进水平。为了在不牺牲性能的情况下进一步提高计算效率，我们引入了 D3C-cos。 D3C 的这种变体限制了分布距离，为 RPM 问题提供了计算效率更高的解决方案，同时保持了高精度。此外，我们还推出了 Lico-Net，这是一个集成了 D3C 和 D3C-cos 的 RPM 基线网络。通过估计和约束正则表示的分布，Lico-Net 解决了问题解决和可解释性挑战，实现了最先进的性能。最后，我们用 D4C 扩展了我们的方法，这是一种对抗性方法，与 D2C 相比，它进一步细化了概念边界。 D4C 专为 RPM 和 Bongard-Logo 问题量身定制，在解决抽象推理的挑战方面展示了显着的改进。总的来说，我们的贡献推动了抽象推理领域的发展，为长期存在的问题提供了新的视角和实用的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2403.03452</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:26 GMT</pubDate>
    </item>
    <item>
      <title>LEAD：无源通用域适应的学习分解</title>
      <link>https://arxiv.org/abs/2403.03421</link>
      <description><![CDATA[arXiv:2403.03421v1 公告类型：新
摘要：通用域适应（UniDA）的目标是在存在协变量和标签转移的情况下进行知识转移。最近，出现了无源通用域适应（SF-UniDA），无需访问源数据即可实现UniDA，由于数据保护策略，这往往更加实用。主要挑战在于确定协变量偏移样本是否属于目标私有未知类别。现有的方法要么通过手工设计的阈值处理，要么通过开发耗时的迭代聚类策略来解决这个问题。在本文中，我们提出了一种学习分解（LEAD）的新思想，它将特征解耦为源已知和未知的组件，以识别目标私有数据。从技术上讲，LEAD 最初利用正交分解分析进行特征分解。然后，LEAD 构建实例级决策边界以自适应地识别目标私有数据。跨 UniDA 各种场景的大量实验证明了 LEAD 的有效性和优越性。值得注意的是，在 VisDA 数据集的 OPDA 场景中，LEAD 的整体 H 分数比 GLC 高出 3.5%，并减少了 75% 的导出伪标签决策边界的时间。此外，LEAD 的吸引力还在于它是对大多数现有方法的补充。该代码可从 https://github.com/ispc-lab/LEAD 获取。]]></description>
      <guid>https://arxiv.org/abs/2403.03421</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>理解文本引导图像编辑稳定扩散中的交叉和自注意力</title>
      <link>https://arxiv.org/abs/2403.03431</link>
      <description><![CDATA[arXiv:2403.03431v1 公告类型：新
摘要：诸如稳定扩散之类的深度文本到图像合成（TIS）模型最近在创造性的文本到图像生成方面受到了广泛的欢迎。然而，对于特定领域的场景，免调优的文本引导图像编辑（TIE）对于应用程序开发人员来说更为重要，他们通过在生成过程中操作注意层中的特征组件来修改图像中的对象或对象属性。然而，人们对这些注意力层学到了什么语义以及注意力图的哪些部分有助于图像编辑的成功却知之甚少。在本文中，我们进行了深入的探测分析，并证明稳定扩散中的交叉注意力图通常包含可能导致编辑失败的对象属性信息。相比之下，自注意力图在转换为目标图像期间保留源图像的几何和形状细节方面发挥着至关重要的作用。我们的分析为理解扩散模型中的交叉和自注意力图提供了宝贵的见解。此外，根据我们的发现，我们简化了流行的图像编辑方法，并提出了一种更直接、更稳定、更高效的免调整程序，该程序仅在去噪过程中修改指定注意层的自注意力图。实验结果表明，我们的简化方法在多个数据集上始终优于流行方法的性能。]]></description>
      <guid>https://arxiv.org/abs/2403.03431</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:25 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉和语言导航的基于因果关系的跨模态表示学习</title>
      <link>https://arxiv.org/abs/2403.03405</link>
      <description><![CDATA[arXiv:2403.03405v1 公告类型：新
摘要：视觉和语言导航（VLN）由于其在现实场景中的潜在应用，近年来引起了人们的广泛研究兴趣。然而，现有的 VLN 方法难以解决虚假关联的问题，导致泛化能力较差，可见环境和未见环境之间存在显着的性能差距。在本文中，我们通过提出一个基于因果学习范式的统一框架 CausalVLN 来应对这一挑战，以训练能够学习无偏特征表示的鲁棒导航器。具体来说，我们使用结构化因果模型（SCM）建立了关于 VLN 中视觉和语言混杂因素的合理假设。在此基础上，我们提出了一种基于迭代后门的表示学习（IBRL）方法，该方法允许对混杂因素进行适应性和有效的干预。此外，我们引入了视觉和语言后门因果编码器，以在训练和验证过程中实现多模态的无偏特征表达，从而增强代理在不同环境中泛化的能力。在三个 VLN 数据集（R2R、RxR 和 REVERIE）上的实验展示了我们提出的方法相对于之前最先进方法的优越性。此外，详细的可视化分析证明了 CausalVLN 在显着缩小可见环境和未见环境之间的性能差距方面的有效性，强调了其强大的泛化能力。]]></description>
      <guid>https://arxiv.org/abs/2403.03405</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>东方传统山水画的场景深度估计</title>
      <link>https://arxiv.org/abs/2403.03408</link>
      <description><![CDATA[arXiv:2403.03408v1 公告类型：新
摘要：绘画中的场景深度估计可以简化 3D 雕塑创作的过程，使视障人士能够通过触觉来欣赏绘画。然而，由于东方山水画图像描绘深度的方法独特且保存较差，测量深度极具挑战性。为了解决东方山水画图像的场景深度估计问题，我们提出了一种新颖的框架，该框架由两步图像到图像转换方法和前端基于 CLIP 的图像匹配组成，以预测最佳的真实场景图像与给定的东方山水画形象相匹配。然后，我们对生成的真实场景图像采用预训练的 SOTA 深度估计模型。第一步，CycleGAN 将东方山水画图像转换为伪真实场景图像。我们利用 CLIP 将风景照片图像与东方山水画图像进行语义匹配，以无监督的方式训练 CycleGAN。然后，将伪真实场景图像和东方山水画图像输入到DiffuseIT中，以在第二步中预测最终的真实场景图像。最后，我们使用预训练的深度估计模型（例如 MiDaS）测量生成的真实场景图像的深度。实验结果表明，我们的方法足以预测与东方山水画图像相对应的真实场景图像。据我们所知，这是第一个测量东方山水画图像深度的研究。我们的研究有可能帮助视障人士以多种方式体验绘画。我们将发布我们的代码和结果数据集。]]></description>
      <guid>https://arxiv.org/abs/2403.03408</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:24 GMT</pubDate>
    </item>
    <item>
      <title>多类杂草检测半监督学习框架的性能评估</title>
      <link>https://arxiv.org/abs/2403.03390</link>
      <description><![CDATA[arXiv:2403.03390v1 公告类型：新
摘要：有效的杂草控制对于优化作物产量、提高农产品质量起着至关重要的作用。然而，对除草剂施用的依赖不仅对环境构成严重威胁，而且还促进了抗性杂草的出现。幸运的是，机器学习和深度学习在精准杂草管理方面的最新进展提供了一种可持续的替代方案。尽管取得了很大的进步，但现有的算法主要是基于监督学习方法开发的，这种方法通常需要带有手动标记注释的大规模数据集，这既费时又费力。因此，标签有效的学习方法，特别是半监督学习，在更广泛的计算机视觉领域获得了越来越多的关注，并表现出了有希望的性能。这些方法旨在利用少量标记数据样本和大量未标记样本来开发高性能模型，与在大量标记数据样本上训练的监督学习模型相当。在这项研究中，我们使用两个著名的目标检测框架，即 FCOS 和 Faster-RCNN，评估了用于多类杂草检测的半监督学习框架的有效性。具体来说，我们评估了一个通用的学生-教师框架，该框架具有改进的伪标签生成模块，以便为未标记的数据生成可靠的伪标签。为了增强泛化能力，采用了集成学生网络来促进训练过程。实验结果表明，在 CottonWeedDet3 和 CottonWeedDet12 中，仅用 10% 的标记数据，该方法就能分别达到监督方法约 76% 和 96% 的检测精度。我们提供源代码访问权限，为正在进行的杂草检测及其他领域的半监督学习研究贡献了宝贵的资源。]]></description>
      <guid>https://arxiv.org/abs/2403.03390</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>用于面部动作单元检测的独立于人的表示的对比学习</title>
      <link>https://arxiv.org/abs/2403.03400</link>
      <description><![CDATA[arXiv:2403.03400v1 公告类型：新
摘要：面部动作单元（AU）检测旨在对面部图像中存在的 AU 进行分类，长期以来一直受到 AU 注释不足的困扰。在本文中，我们的目标是通过在对比学习范式中从大量未标记的面部视频中学习 AU 表示来缓解这种数据稀缺问题。我们从两方面制定自监督 AU 表示学习信号：（1）AU 表示应该在短视频剪辑中按帧进行区分； (2) 从不同身份采样但显示相似面部 AU 的面部帧应该具有一致的 AU 表示。为了实现这些目标，我们建议对比学习视频剪辑中的 AU 表示，并设计一种跨身份重建机制来学习与人无关的表示。特别地，我们采用基于边缘的时间对比学习范式来感知由连续输入面部帧组成的剪辑内的时间AU一致性和演化特征。此外，跨身份重建机制有助于推送来自不同身份的面孔，但在潜在嵌入空间中显示类似的 AU。三个公共 AU 数据集上的实验结果表明，学习到的 AU 表示对于 AU 检测具有区分性。我们的方法优于其他对比学习方法，并显着缩小了自监督和监督 AU 检测方法之间的性能差距。]]></description>
      <guid>https://arxiv.org/abs/2403.03400</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:23 GMT</pubDate>
    </item>
    <item>
      <title>通过丰富的监督加强视觉语言预训练</title>
      <link>https://arxiv.org/abs/2403.03346</link>
      <description><![CDATA[arXiv:2403.03346v1 公告类型：新
摘要：我们提出了 ScreenShots（S4）的强监督预训练——一种使用大规模网络屏幕截图渲染数据的视觉语言模型的新颖预训练范例。使用网络屏幕截图可以解锁视觉和文本线索的宝库，而使用图像文本对时则不会出现这些线索。在S4中，我们利用HTML元素固有的树形结构层次结构和空间定位，精心设计了10个具有大规模注释数据的预训练任务。这些任务类似于跨不同领域的下游任务，并且注释的获取成本低廉。我们证明，与当前的屏幕截图预训练目标相比，我们创新的预训练方法显着提高了图像到文本模型在九个不同且流行的下游任务中的性能 - 表检测提高了高达 76.1%，并且至少提高了 1% % 在小部件标题上。]]></description>
      <guid>https://arxiv.org/abs/2403.03346</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>F$^3$Loc：平面图本地化的融合和过滤</title>
      <link>https://arxiv.org/abs/2403.03370</link>
      <description><![CDATA[arXiv:2403.03370v1 公告类型：新
摘要：在本文中，我们提出了一种有效的数据驱动解决方案，用于在平面图内进行自我定位。平面图数据易于获得、长期持久且对视觉外观的变化具有固有的鲁棒性。我们的方法不需要对每个地图和位置进行重新训练，也不需要感兴趣区域的大型图像数据库。我们提出了一种新颖的概率模型，由观察和新颖的时间过滤模块组成。观察模块采用基于光线的高效表示进行内部操作，由单个视图模块和多视图模块组成，用于预测图像的水平深度并融合其结果，以受益于任一方法提供的优势。我们的方法在传统的消费类硬件上运行，并克服了通常需要直立图像的竞争方法的常见限制。我们的完整系统满足实时要求，同时大幅超越最先进的系统。]]></description>
      <guid>https://arxiv.org/abs/2403.03370</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:22 GMT</pubDate>
    </item>
    <item>
      <title>CenterDisks：具有磁盘覆盖的实时实例分段</title>
      <link>https://arxiv.org/abs/2403.03296</link>
      <description><![CDATA[arXiv:2403.03296v1 公告类型：新
摘要：提高实例分割方法的准确性通常是以牺牲速度为代价的。使用更粗糙的表示，我们可以减少参数的数量，从而获得实时掩模。在本文中，我们从集合覆盖问题中获得灵感来预测掩模近似值。给定感兴趣对象的真实二进制掩码作为训练输入，我们的方法学习预测磁盘对这些对象的大致覆盖范围，而无需对其位置或半径进行监督。每个对象由固定数量的不同半径的圆盘表示。在学习阶段，我们将半径视为与标准差成正比，以便计算在一组二维高斯函数而不是磁盘上传播的误差。我们在具有挑战性的数据集上训练和测试了我们的实例分割方法，这些数据集显示了具有各种道路使用者的密集城市环境。我们的方法在 IDD 和 KITTI 数据集上实现了最先进的结果，在单个 RTX 3090 GPU 上的推理时间为 0.040 秒。]]></description>
      <guid>https://arxiv.org/abs/2403.03296</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>通过在合成数据中植入自然图像模式来学习零样本材料状态分割</title>
      <link>https://arxiv.org/abs/2403.03309</link>
      <description><![CDATA[arXiv:2403.03309v1 公告类型：新
摘要：材料及其状态的视觉理解和分割是理解物理世界的基础。材料形成的无限纹理、形状和通常模糊的边界使得这项任务特别难以概括。无论是识别表面的潮湿区域、岩石中的矿物质、植物中的感染区域还是水中的污染，每种物质状态都有其独特的形式。对于神经网络来说，要学习与类别无关的材料分割，有必要首先收集和注释捕获这种复杂性的数据。收集真实世界的图像和手动注释受到成本和手工劳动精度有限的限制。相比之下，合成数据非常准确且几乎免费，但无法复制物质世界的巨大多样性。在这项工作中，我们提出了一种方法来弥合这一关键差距，即将从现实世界图像中提取的模式植入到合成数据中。因此，从自然图像中自动收集的图案用于将材质映射到合成场景中。这种无监督的方法允许生成的数据捕获现实世界的巨大复杂性，同时保持合成数据的精度和规模。我们还提出了第一个与类别无关的材料状态分割的通用基准。基准图像包含各种物质状态的真实世界图像，包括烹饪、食物、岩石、建筑、植物和液体，每种状态都处于不同的状态（湿/干/染色/煮熟/烧毁/磨损/生锈/沉积物/泡沫...）。注释包括具有相似但不相同材料的区域之间的部分相似性，以及仅对完全相同材料状态的点进行硬分割。我们表明，MatSeg 上的网络训练在此任务上显着优于现有的最先进方法。]]></description>
      <guid>https://arxiv.org/abs/2403.03309</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:21 GMT</pubDate>
    </item>
    <item>
      <title>基于 DINOv2 的小镜头医学图像分割自监督学习</title>
      <link>https://arxiv.org/abs/2403.03273</link>
      <description><![CDATA[arXiv:2403.03273v1 公告类型：新
摘要：深度学习模型已成为医学图像分割的基石，但其功效取决于大量手动标记数据集的可用性，并且其对不可预见类别的适应性仍然是一个挑战。少镜头分割（FSS）通过赋予模型从有限的标记示例中学习新类别的能力，提供了一种有前景的解决方案。 FSS 的一种领先方法是 ALPNet，它比较查询图像和少数可用的支持分割图像之间的特征。使用 ALPNet 的一个关键问题是如何设计其特征。在这项工作中，我们深入研究了使用 DINOv2 特征的潜力，DINOv2 是计算机视觉中的基础自我监督学习模型。利用 ALPNet 的优势并利用 DINOv2 的特征提取功能，我们提出了一种新颖的少镜头分割方法，不仅提高了性能，而且为更强大和适应性更强的医学图像分析铺平了道路。]]></description>
      <guid>https://arxiv.org/abs/2403.03273</guid>
      <pubDate>Thu, 07 Mar 2024 06:18:20 GMT</pubDate>
    </item>
    </channel>
</rss>