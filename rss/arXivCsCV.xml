<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://arxiv.org/rss/</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Thu, 01 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>LaneGraph2Seq：通过点边编码和连接增强使用语言模型提取车道拓扑</title>
      <link>https://arxiv.org/abs/2401.17609</link>
      <description><![CDATA[了解道路结构对于自动驾驶至关重要。复杂的道路结构通常使用车道图来描述，其中包括形成有向无环图 (DAG) 的中心线曲线和连接。车道图的准确提取依赖于精确估计 DAG 内的顶点和边缘信息。最近的研究强调了基于 Transformer 的语言模型令人印象深刻的序列预测能力，使它们在图数据编码为序列时能够有效地学习图表示。然而，现有的研究主要集中在显式地对顶点进行建模，而将边缘信息简单地嵌入到网络中。因此，这些方法在车道图提取任务中存在不足。为了解决这个问题，我们引入了 LaneGraph2Seq，一种用于车道图提取的新方法。它利用具有点边编码和连接增强功能的语言模型。我们的序列化策略包括以顶点为中心的深度优先遍历和简洁的基于边缘的分区序列。此外，我们使用无分类器指导与核采样相结合来改善车道连接性。我们在著名数据集 nuScenes 和 Argoverse 2 上验证了我们的方法，展示了一致且令人信服的结果。与泳道图提取中最先进的技术相比，我们的 LaneGraph2Seq 方法表现出卓越的性能。]]></description>
      <guid>https://arxiv.org/abs/2401.17609</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:50 GMT</pubDate>
    </item>
    <item>
      <title>揭示多视角多人关联和跟踪的自我监督力量</title>
      <link>https://arxiv.org/abs/2401.17617</link>
      <description><![CDATA[多视图多人关联与跟踪（MvMHAT）是多人场景视频监控的一个新但重要的问题，旨在随着时间的推移在每个视图中跟踪一群人，以及在不同的视图中识别同一个人与之前的 MOT 和多摄像头 MOT 任务只考虑超时的人体跟踪不同。这样，MvMHAT 的视频需要更复杂的注释，同时包含更多用于自学习的信息。在这项工作中，我们通过自监督学习感知端到端网络来解决这个问题。具体来说，我们建议通过考虑自反性、对称性和传递性三个属性来利用时空自洽原理。除了自然存在的自反性属性之外，我们还基于对称性和传递性的属性设计了自监督学习损失，用于外观特征学习和分配矩阵优化，以随时间和跨视图关联多个人。此外，为了促进 MvMHAT 的研究，我们建立了两个新的大规模基准用于不同算法的网络训练和测试。对所提出的基准进行大量实验验证了我们方法的有效性。我们已向公众发布了基准测试和代码。]]></description>
      <guid>https://arxiv.org/abs/2401.17617</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:50 GMT</pubDate>
    </item>
    <item>
      <title>基于扩散模型的图像空间和频率感知恢复方法</title>
      <link>https://arxiv.org/abs/2401.17629</link>
      <description><![CDATA[扩散模型最近已成为图像恢复（IR）的一个有前途的框架，因为它们能够产生高质量的重建并且与现有方法兼容。解决红外噪声逆问题的现有方法考虑了像素级数据保真度。在本文中，我们提出了 SaFaRI，一种具有高斯噪声的 IR 空间和频率感知扩散模型。我们的模型鼓励图像在空间和频率域上保持数据保真度，从而提高重建质量。我们全面评估了我们的模型在各种噪声逆问题上的性能，包括修复、去噪和超分辨率。我们的全面评估表明，SaFaRI 在 ImageNet 数据集和 FFHQ 数据集上均实现了最先进的性能，在 LPIPS 和 FID 指标方面优于现有的零样本 IR 方法。]]></description>
      <guid>https://arxiv.org/abs/2401.17629</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:50 GMT</pubDate>
    </item>
    <item>
      <title>探索夜间光流的常见外观边界适应</title>
      <link>https://arxiv.org/abs/2401.17642</link>
      <description><![CDATA[我们研究了夜间光流的一项具有挑战性的任务，该任务受到纹理减弱和噪声放大的影响。这些退化削弱了辨别性视觉特征，从而导致无效的运动特征匹配。通常，现有方法采用域适应将知识从输入视觉空间或输出运动空间中的辅助域转移到夜间域。然而，这种直接适应是无效的，因为由于辅助域和夜间域之间的特征表示的内在异构性，存在很大的域差距。为了克服这个问题，我们探索了一个公共潜在空间作为中间桥梁，以加强辅助域和夜间域之间的特征对齐。在这项工作中，我们利用两个辅助的白天和事件域，并提出了一种新颖的夜间光流通用外观边界适应框架。在外观适应中，我们采用本征图像分解将辅助白天图像和夜间图像嵌入到反射率对齐的公共空间中。我们发现两个反射图的运动分布非常相似，这有利于我们一致地将运动外观知识从白天转移到夜间领域。在边界适应中，我们从理论上推导了夜间图像和时空梯度对齐的公共空间内累积事件之间的运动相关公式。我们发现两个时空梯度图的相关性存在显着差异，这有利于我们将边界知识从事件域转移到夜间域。此外，外观适应和边界适应是相互补充的，因为它们可以共同将全局运动和局部边界知识转移到夜间领域。]]></description>
      <guid>https://arxiv.org/abs/2401.17642</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:50 GMT</pubDate>
    </item>
    <item>
      <title>使用深度学习进行局部特征匹配：调查</title>
      <link>https://arxiv.org/abs/2401.17592</link>
      <description><![CDATA[局部特征匹配在计算机视觉领域有着广泛的应用，涵盖图像检索、3D 重建和对象识别等领域。然而，由于视点和照明变化等因素，提高匹配的准确性和鲁棒性仍然存在挑战。近年来，深度学习模型的引入引发了对局部特征匹配技术的广泛探索。这项工作的目标是提供局部特征匹配方法的全面概述。根据检测器的存在，这些方法分为两个关键部分。基于检测器的类别涵盖的模型包括检测然后描述、联合检测和描述、描述然后检测以及基于图的技术。相比之下，无检测器类别包括基于 CNN、基于变换器和基于补丁的方法。我们的研究超越了方法论分析，结合了对流行数据集和指标的评估，以促进最先进技术的定量比较。本文还探讨了局部特征匹配在运动结构、遥感图像配准和医学图像配准等不同领域的实际应用，强调了其在各个领域的多功能性和重要性。最终，我们努力概述该领域当前面临的挑战并提供未来的研究方向，从而为涉及局部特征匹配及其互连领域的研究人员提供参考。]]></description>
      <guid>https://arxiv.org/abs/2401.17592</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:49 GMT</pubDate>
    </item>
    <item>
      <title>用于生成 3D 形状的拓扑感知潜在扩散</title>
      <link>https://arxiv.org/abs/2401.17603</link>
      <description><![CDATA[我们引入了一种新的生成模型，该模型将潜在扩散与持久同源性相结合，以创建具有高度多样性的 3D 形状，并特别强调其拓扑特征。我们的方法涉及将 3D 形状表示为隐式场，然后采用持久同源性来提取拓扑特征，包括贝蒂数和持久性图。形状生成过程由两个步骤组成。最初，我们采用基于 Transformer 的自动编码模块将每个 3D 形状的隐式表示嵌入到一组潜在向量中。随后，我们通过扩散模型浏览学习到的潜在空间。通过策略性地将拓扑特征融入到扩散过程中，我们的生成模块能够生成更丰富的具有不同拓扑结构的 3D 形状。此外，我们的框架非常灵活，支持受各种输入约束的生成任务，包括稀疏和部分点云以及草图。通过修改持久性图，我们可以改变从这些输入模式生成的形状的拓扑。]]></description>
      <guid>https://arxiv.org/abs/2401.17603</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:49 GMT</pubDate>
    </item>
    <item>
      <title>用于提示语音识别的计算和参数高效的多模态融合变压器</title>
      <link>https://arxiv.org/abs/2401.17604</link>
      <description><![CDATA[提示语音（CS）是听障人士使用的一种纯视觉编码方法，它将唇读与几种特定的手形相结合，使口语变得可见。自动 CS 识别 (ACSR) 旨在将语音的视觉线索转录为文本，这可以帮助听力障碍人士有效地进行交流。 CS的视觉信息包含唇读和手势，两者的融合在ACSR中发挥着重要作用。然而，大多数先前的融合方法都难以捕获多模态 CS 数据的长序列输入中存在的全局依赖性。因此，这些方法通常无法学习有助于融合的有效跨模式关系。最近，基于注意力的变压器已成为捕获多模态融合中长序列的全局依赖性的流行想法，但现有的多模态融合变压器在 ACSR 任务中存在识别精度差和计算效率低的问题。为了解决这些问题，我们通过提出一种新颖的令牌重要性感知注意机制（TIAA）来开发一种新颖的计算和参数高效的多模态融合变压器，其中制定了令牌利用率（TUR）以从多模式流。更准确地说，TIAA 首先对每种模态的所有标记上特定模态的细粒度时间依赖性进行建模，然后学习不同模态的重要标记上模态共享的粗粒度时间依赖性的有效跨模态交互。此外，还设计了轻量级门控隐藏投影来控制 TIAA 的特征流。与现有的基于 Transformer 的融合方法和 ACSR 融合方法相比，所得模型名为经济 Cued 语音融合变压器 (EcoCued)，在所有现有 CS 数据集上实现了最先进的性能。]]></description>
      <guid>https://arxiv.org/abs/2401.17604</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:49 GMT</pubDate>
    </item>
    <item>
      <title>任何场景中的任何内容：真实感视频对象插入</title>
      <link>https://arxiv.org/abs/2401.17509</link>
      <description><![CDATA[逼真的视频模拟在从虚拟现实到电影制作的各种应用中显示出巨大的潜力。对于在现实环境中捕获视频不切实际或昂贵的场景尤其如此。视频模拟中的现有方法通常无法准确地模拟照明环境、表示对象几何形状或实现高水平的照片级真实感。在本文中，我们提出了任何场景中的任何内容，这是一种新颖且通用的真实视频模拟框架，可以将任何对象无缝插入到现有的动态视频中，并强调物理真实感。我们提出的总体框架包含三个关键过程：1）将真实的物体集成到给定的场景视频中，并放置适当的位置以确保几何真实感； 2）估计天空和环境光照分布并模拟真实阴影，增强光线真实感； 3）采用风格转换网络来细化最终的视频输出，以最大限度地提高照片真实感。我们通过实验证明 Anything in Any Scene 框架可以生成具有出色的几何真实感、灯光真实感和照片真实感的模拟视频。通过显着缓解与视频数据生成相关的挑战，我们的框架为获取高质量视频提供了高效且经济高效的解决方案。此外，其应用远远超出了视频数据增强的范围，在虚拟现实、视频编辑和各种其他以视频为中心的应用中显示出广阔的潜力。请查看我们的项目网站 https://anythinginanyscene.github.io 以访问我们的项目代码和更多高分辨率视频结果。]]></description>
      <guid>https://arxiv.org/abs/2401.17509</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:48 GMT</pubDate>
    </item>
    <item>
      <title>迈向图像语义和句法序列学习</title>
      <link>https://arxiv.org/abs/2401.17515</link>
      <description><![CDATA[卷积神经网络和视觉转换器在机器感知方面取得了出色的性能，特别是在图像分类方面。尽管这些图像分类器擅长预测图像级类别标签，但它们可能无法区分对象内丢失或移位的部分。因此，他们可能无法检测到对象组合中涉及丢失或混乱的语义信息的损坏图像。相反，人类的感知很容易区分这种腐败。为了弥补这一差距，我们引入了“图像语法”的概念，由“图像语义”和“图像语法”组成，来表示图像的部分或块的语义以及这些部分排列以创建图像的顺序。有意义的对象。为了学习与一类视觉对象/场景相关的图像语法，我们提出了一种弱监督的两阶段方法。在第一阶段，我们使用深度聚类框架，该框架依赖于迭代聚类和特征细化来产生部分语义分割。在第二阶段，我们结合了一个循环双 LSTM 模块来处理一系列语义分割补丁以捕获图像语法。我们的框架经过训练可以推理补丁语义并检测错误的语法。我们对几种语法学习模型在检测补丁损坏方面的性能进行了基准测试。最后，我们在 Celeb 和 SUNRGBD 数据集中验证了我们的框架的功能，并证明它可以在各种语义和句法损坏场景中实现 70% 到 90% 的语法验证准确性。]]></description>
      <guid>https://arxiv.org/abs/2401.17515</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:48 GMT</pubDate>
    </item>
    <item>
      <title>面向任务的扩散模型压缩</title>
      <link>https://arxiv.org/abs/2401.17547</link>
      <description><![CDATA[随着大规模文本到图像（T2I）扩散模型的最新进展产生了显着的高质量图像，各种下游图像到图像（I2I）应用程序也随之出现。尽管这些 I2I 模型取得了令人印象深刻的结果，但它们的实用性却受到模型尺寸大和迭代去噪过程的计算负担的阻碍。在本文中，我们以面向任务的方式探索这些 I2I 模型的压缩潜力，并介绍了一种减少模型大小和时间步数的新方法。通过大量的实验，我们观察到关键的见解，并利用我们的经验知识来开发实用的解决方案，旨在以最小的勘探成本获得接近最佳的结果。我们通过将其应用于 InstructPix2Pix 进行图像编辑和 StableSR 进行图像恢复来验证该方法的有效性。我们的方法实现了令人满意的输出质量，模型占用空间减少了 39.2% 和 56.4%，InstructPix2Pix 和 StableSR 的延迟分别减少了 81.4% 和 68.7%。]]></description>
      <guid>https://arxiv.org/abs/2401.17547</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:48 GMT</pubDate>
    </item>
    <item>
      <title>基于WiFi信道状态信息的穿墙成像</title>
      <link>https://arxiv.org/abs/2401.17417</link>
      <description><![CDATA[这项工作提出了一种在穿墙场景中从 WiFi 通道状态信息 (CSI) 合成图像的开创性方法。利用 WiFi 的优势，例如成本效益、照明不变性和穿墙能力，我们的方法可以对房间边界之外的室内环境进行视觉监控，而无需摄像头。更一般地说，它通过解锁执行基于图像的下游任务（例如视觉活动识别）的选项来提高 WiFi CSI 的可解释性。为了实现从 WiFi CSI 到图像的跨模态转换，我们依赖于适合我们问题具体情况的多模态变分自动编码器 (VAE)。我们通过对架构配置的消融研究和对重建图像的定量/定性评估来广泛评估我们提出的方法。我们的结果证明了我们方法的可行性，并强调了其实际应用的潜力。]]></description>
      <guid>https://arxiv.org/abs/2401.17417</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:47 GMT</pubDate>
    </item>
    <item>
      <title>走向视觉句法理解</title>
      <link>https://arxiv.org/abs/2401.17497</link>
      <description><![CDATA[句法通常在语言学领域进行研究，指的是句子中单词的排列。类似地，图像可以被视为视觉“句子”，而图像的语义部分则充当“单词”。虽然视觉句法理解对人类来说是自然而然的，但探索深度神经网络 (DNN) 是否具备这种推理能力还是很有趣的。为此，我们改变了被称为“不正确”图像的自然图像的语法（例如交换脸部的眼睛和鼻子），以研究 DNN 对此类语法异常的敏感性。通过我们的实验，我们发现了 DNN 的一个有趣的特性，我们观察到最先进的卷积神经网络以及视觉变换器在仅训练正确的图像时无法区分语法正确和不正确的图像。为了解决这个问题并利用 DNN 实现视觉句法理解，我们提出了一个三阶段框架 - (i) 检测图像中的“单词”（或子特征），(ii) 检测到的单词被顺序屏蔽并使用自动编码器重建，（iii）在每个位置比较原始部分和重建部分以确定语法正确性。重建模块使用类似于 BERT 的图像掩码自动编码进行训练，其动机是利用语言模型启发的训练来更好地捕获语法。请注意，我们提出的方法是无监督的，因为不正确的图像仅在测试期间使用，并且正确与不正确的标签从不用于训练。我们在 CelebA 和 AFHQ 数据集上进行实验，分类准确率分别为 92.10% 和 90.89%。值得注意的是，该方法可以很好地推广到 ImageNet 样本，这些样本与 CelebA 和 AFHQ 共享公共类，而无需对其进行显式训练。]]></description>
      <guid>https://arxiv.org/abs/2401.17497</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:47 GMT</pubDate>
    </item>
    <item>
      <title>AdvGPS：用于多智能体感知攻击的对抗性 GPS</title>
      <link>https://arxiv.org/abs/2401.17499</link>
      <description><![CDATA[多智能体感知系统从位于各个智能体上的传感器收集视觉数据，并利用 GPS 信号确定的相对姿势来有效融合信息，从而减轻单智能体感知的局限性，例如遮挡。然而，GPS 信号的精度可能会受到一系列因素的影响，包括无线传输和建筑物等障碍物。鉴于GPS信号在感知融合中的关键作用以及潜在的各种干扰，有必要研究特定的GPS信号是否容易误导多智能体感知系统。为了解决这个问题，我们将该任务定义为对抗性攻击挑战，并引入 \textsc{AdvGPS}，一种能够生成对抗性 GPS 信号的方法，该信号对于系统内的各个代理来说也是隐秘的，从而显着降低了对象检测的准确性。为了提高黑盒场景中这些攻击的成功率，我们引入了三种类型的统计敏感自然差异：基于外观的差异、基于分布的差异和任务感知差异。我们对 OPV2V 数据集进行的广泛实验表明，这些攻击极大地破坏了最先进方法的性能，展示了跨不同基于点云的 3D 检测系统的显着可转移性。这一令人震惊的发现强调了解决多智能体感知系统中的安全影响的迫切需要，从而强调了一个关键的研究领域。]]></description>
      <guid>https://arxiv.org/abs/2401.17499</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:47 GMT</pubDate>
    </item>
    <item>
      <title>YTCommentQA：教学视频中的视频问题解答</title>
      <link>https://arxiv.org/abs/2401.17343</link>
      <description><![CDATA[教学视频提供了各种任务的详细操作指南，观看者经常提出有关内容的问题。解决这些问题对于理解内容至关重要，但立即得到答案却很困难。虽然已经为视频问答（视频 QA）任务开发了许多计算模型，但它们主要针对基于视频内容生成的问题进行训练，旨在从内容中生成答案。然而，在现实情况下，用户可能会提出超出视频信息边界的问题，这凸显了确定视频是否可以提供答案的必要性。由于视频具有多模态性质，视觉和口头信息相互交织，因此判断视频内容是否可以回答问题具有挑战性。为了弥补这一差距，我们提出了 YTCommentQA 数据集，其中包含来自 YouTube 的自然生成的问题，按其可回答性和所需的回答方式（视觉、脚本或两者）进行分类。可回答性分类任务的实验证明了 YTCommentQA 的复杂性，并强调需要理解视频推理中视觉和脚本信息的组合作用。该数据集可在 https://github.com/lgresearch/YTCommentQA 获取。]]></description>
      <guid>https://arxiv.org/abs/2401.17343</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:46 GMT</pubDate>
    </item>
    <item>
      <title>CALM：卷积作为局部混合</title>
      <link>https://arxiv.org/abs/2401.17400</link>
      <description><![CDATA[在本文中，我们证明卷积层的特征图相当于用于图像建模的一种特殊高斯混合的非归一化对数后验。然后我们扩展了模型以驱动不同的特征，并提出了相应的 EM 算法来学习模型。使用这种方法学习卷积权重是高效的，保证收敛，并且不需要监督信息。代码位于：https://github.com/LifanLiang/CALM。]]></description>
      <guid>https://arxiv.org/abs/2401.17400</guid>
      <pubDate>Thu, 01 Feb 2024 18:15:46 GMT</pubDate>
    </item>
    </channel>
</rss>