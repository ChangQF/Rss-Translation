<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 08 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>BVI-RLV：低光视频增强的完全注册数据集和基准</title>
      <link>https://arxiv.org/abs/2407.03535</link>
      <description><![CDATA[arXiv:2407.03535v1 公告类型：新
摘要：低光视频通常会表现出时空不相干噪声，从而影响计算机视觉应用中的可见性和性能。使用深度学习增强此类内容的一个重大挑战是训练数据的稀缺性。本文介绍了一种新颖的低光视频数据集，该数据集由两种不同低光照条件下具有各种运动场景的 40 个场景组成，包含真实噪声和时间伪影。我们使用可编程电动小车在正常光线下捕获完全配准的地面真实数据，并通过基于图像的方法对其进行细化，以实现不同光照水平下的像素级帧对齐。我们基于四种不同的技术提供基准：卷积神经网络、变压器、扩散模型和状态空间模型（mamba）。我们的实验结果证明了完全配准的视频对对于低光视频增强（LLVE）的重要性，综合评估表明使用我们的数据集训练的模型优于使用现有数据集训练的模型。我们的数据集和基准链接可在 https://doi.org/10.21227/mzny-8c77 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2407.03535</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:42 GMT</pubDate>
    </item>
    <item>
      <title>漫画数据集框架：用于检测基准测试的漫画数据集组合</title>
      <link>https://arxiv.org/abs/2407.03540</link>
      <description><![CDATA[arXiv:2407.03540v1 公告类型：新
摘要：漫画作为一种媒介，以独特的方式将文本和图像结合在一起，其风格通常与现实世界的视觉效果不同。在过去的三十年里，漫画的计算研究已经从基本的物体检测发展到更复杂的任务。然而，该领域面临着持续的挑战，例如数据集小、注释不一致、模型权重难以访问以及由于训练/测试分割和指标不同而无法直接比较的结果。为了解决这些问题，我们的目标是标准化跨数据集的注释，将各种漫画风格引入数据集，并建立具有清晰、可复制设置的基准结果。我们提出的漫画数据集框架将数据集注释标准化为通用格式，并通过引入 Comics100（来自数字漫画博物馆的 100 本书的精选集，以统一的格式进行注释以进行检测）来解决漫画的过度代表问题。我们使用漫画数据集框架对各种检测架构进行了基准测试。所有相关代码、模型权重和详细评估流程均可在 https://github.com/emanuelevivoli/cdf 上找到，确保透明度并便于复制。这一举措是改善漫画中物体检测的重大进步，为依赖于精确物体识别的更复杂的计算任务奠定了基础。]]></description>
      <guid>https://arxiv.org/abs/2407.03540</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:42 GMT</pubDate>
    </item>
    <item>
      <title>FlowCon：使用基于流的对比学习进行分布外检测</title>
      <link>https://arxiv.org/abs/2407.03489</link>
      <description><![CDATA[arXiv:2407.03489v1 公告类型：新
摘要：随着深度学习方法在现实世界中的应用不断扩大，识别分布外 (OOD) 数据变得越来越重要。事后方法修改对异常数据进行微调的 softmax 分数或利用中间特征层来识别分布内 (ID) 和 OOD 样本之间的独特模式。其他方法侧重于使用不同的 OOD 样本来学习 ID 和 OOD 之间的差异。然而，这些技术通常依赖于假设的异常样本的质量。基于密度的方法明确地模拟了类条件分布，但这需要较长的训练时间或重新训练分类器。为了解决这些问题，我们引入了一种新的基于密度的 OOD 检测技术 \textit{FlowCon}。我们的主要创新在于有效地将规范化流的属性与监督对比学习相结合，确保稳健的表示学习和易于处理的密度估计。实证评估表明，我们的方法在 ResNet18 和 WideResNet 分类器上预训练的常见视觉数据集（如 CIFAR-10 和 CIFAR-100）上具有增强的性能。我们还使用似然图进行定量分析，使用 UMAP 嵌入进行定性可视化，并证明所提方法在各种 OOD 环境下的稳健性。代码将在决策后开源。]]></description>
      <guid>https://arxiv.org/abs/2407.03489</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:41 GMT</pubDate>
    </item>
    <item>
      <title>使用新型预激活倒置 ResNet 和混合元启发式优化 DenseNet 进行虹膜和掌纹多模态生物特征识别</title>
      <link>https://arxiv.org/abs/2407.03498</link>
      <description><![CDATA[arXiv:2407.03498v1 公告类型：新
摘要：由于人们越来越重视信息安全，生物特征识别技术已广泛融入日常生活。在这个领域，结合多种生物特征的多模态生物识别技术克服了单模态系统中存在的局限性，例如易受欺骗攻击或无法适应随时间的变化。本文提出了一种新型多模态生物特征识别系统，该系统利用虹膜和掌纹模态的深度学习算法。介绍了一种开创性的方法，首先实施新颖的带有 L\&#39;evy 飞行的改进萤火虫算法 (MFALF) 来优化对比度限制自适应直方图均衡 (CLAHE) 算法，从而有效增强图像对比度。随后，通过 ReliefF 和 Moth Flame 优化 (MFOR) 的独特混合进行特征选择以提取信息特征。对于分类，我们采用并行方法，首先引入一种新颖的预激活倒置 ResNet (PIR) 架构，其次，利用元启发式方法与创新的 Johnson 花授粉算法和降雨优化算法的混合来微调基于迁移学习的 DenseNet 架构 (JFPA-ROA) 的学习率和 dropout 参数。最后，实施分数级融合策略来结合两个分类器的输出，提供一个强大而准确的多模态生物特征识别系统。该系统的性能是根据准确度、检测误差权衡 (DET) 曲线、等错误率 (EER) 和总训练时间来评估的。提出的多模态识别架构在 CASIA Palmprint、MMU、BMPD 和 IIT 数据集上进行了测试，实现了 100% 的识别准确率，优于单模态虹膜和掌纹识别方法。]]></description>
      <guid>https://arxiv.org/abs/2407.03498</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:41 GMT</pubDate>
    </item>
    <item>
      <title>基础模型的领域感知微调</title>
      <link>https://arxiv.org/abs/2407.03482</link>
      <description><![CDATA[arXiv:2407.03482v1 公告类型：新
摘要：基础模型 (FM) 彻底改变了计算机视觉，实现了跨不同领域的有效学习。然而，它们在领域转移下的性能尚未得到充分探索。本文通过比较不同的主干架构并引入利用领域相关文本嵌入的新型领域感知组件，研究了 FM 的零样本领域自适应潜力。我们提出了领域自适应规范化，称为 Domino，它在微调期间明确利用领域嵌入，从而使模型具有领域感知能力。最终，Domino 使更强大的计算机视觉模型能够有效地适应各种看不见的领域。]]></description>
      <guid>https://arxiv.org/abs/2407.03482</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:40 GMT</pubDate>
    </item>
    <item>
      <title>Celeb-FBI：基于深度学习方法的人体全身图像和年龄、性别、身高和体重估计的基准数据集</title>
      <link>https://arxiv.org/abs/2407.03486</link>
      <description><![CDATA[arXiv:2407.03486v1 公告类型：新
摘要：监控、识别、图像检索系统和医疗保健领域缺乏全面的数据集，这对研究人员探索新方法和推进这些领域知识提出了重大挑战。此外，在时尚行业分析、人体工程学设计评估、虚拟现实头像创建和运动表现分析等领域，对具有身高、体重、年龄和性别等详细属性的全身图像数据集的需求尤为重要。为了弥补这一差距，我们创建了“Celeb-FBI”数据集，其中包含 7,211 张个人全身图像，并附有身高、年龄、体重和性别的详细信息。在创建数据集之后，我们继续进行预处理阶段，包括图像清理、缩放和合成少数过采样技术 (SMOTE) 的应用。随后，利用这个准备好的数据集，我们采用了三种深度学习方法：卷积神经网络 (CNN)、50 层 ResNet 和 16 层 VGG，用于从人体全身图像中估计身高、体重、年龄和性别。从得到的结果来看，ResNet-50 在系统中表现最佳，年龄准确率为 79.18%，性别准确率为 95.43%，身高准确率为 85.60%，体重准确率为 81.91%。]]></description>
      <guid>https://arxiv.org/abs/2407.03486</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:40 GMT</pubDate>
    </item>
    <item>
      <title>规模精度：按需提供特定领域的数据集</title>
      <link>https://arxiv.org/abs/2407.03463</link>
      <description><![CDATA[arXiv:2407.03463v1 公告类型：新
摘要：在自监督学习 (SSL) 领域，传统观点倾向于使用大量通用领域数据集来预训练稳健的主干。在本文中，我们通过探索是否有可能弥合通用领域数据集和（传统上较小的）领域特定数据集之间的规模以缩小当前的性能差距来挑战这一想法。更具体地说，我们提出了 Precision at Scale (PaS)，这是一种按需自主创建领域特定数据集的新方法。PaS 管道的模块化使得能够利用最先进的基础和生成模型来创建属于任何给定领域的任意给定大小的图像集合，而无需太多人为干预。在两个复杂领域进行的广泛分析证明了 PaS 数据集在多样性、规模和训练视觉变换器和卷积神经网络的有效性方面优于现有的传统领域特定数据集。最值得注意的是，我们证明自动生成的领域特定数据集比 ImageNet-1k 和 ImageNet-21k 等大规模监督数据集具有更好的预训练效果。具体来说，在由 PaS 管道构建的领域特定数据集上训练的模型在所有考虑的领域和分类任务中至少比 ImageNet-1k 预训练主干高出 12%，并且在食品领域表现优于监督 ImageNet-21k 预训练，同时规模却小 12 倍。代码库：https://github.com/jesusmolrdv/Precision-at-Scale/]]></description>
      <guid>https://arxiv.org/abs/2407.03463</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:39 GMT</pubDate>
    </item>
    <item>
      <title>从视频和模拟中学习以行动和推理为中心的图像编辑</title>
      <link>https://arxiv.org/abs/2407.03471</link>
      <description><![CDATA[arXiv:2407.03471v1 公告类型：新
摘要：图像编辑模型应该能够执行各种编辑，从对象替换、更改属性或样式到执行动作或移动，这需要多种形式的推理。当前一般的指令引导编辑模型在以动作和推理为中心的编辑方面存在重大缺陷。对象、属性或风格变化可以从视觉静态数据集中学习。另一方面，以动作和推理为中心的编辑的高质量数据很少，必须来自完全不同的来源，例如物理动态、时间和空间推理。为此，我们精心策划了 AURORA 数据集（动作推理对象属性），这是一组高质量的训练数据，由人工注释并从视频和模拟引擎中策划。我们关注高质量训练数据的一个关键方面：三元组（源图像、提示、目标图像）包含提示描述的单个有意义的视觉变化，即源图像和目标图像之间真正最小的变化。为了证明我们数据集的价值，我们在一个新的专家策划的基准（AURORA-Bench）上评估了一个 AURORA 微调模型，该基准涵盖 8 种不同的编辑任务。经人类评分者评判，我们的模型表现明显优于以前的编辑模型。对于自动评估，我们发现以前的指标存在重要缺陷，并警告不要将其用于语义上困难的编辑任务。相反，我们提出了一个侧重于判别理解的新自动指标。我们希望我们的努力：（1）策划高质量的训练数据集和评估基准，（2）开发关键评估，以及（3）发布最先进的模型，将推动一般图像编辑的进一步发展。]]></description>
      <guid>https://arxiv.org/abs/2407.03471</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:39 GMT</pubDate>
    </item>
    <item>
      <title>DACB-Net：用于皮肤病分类的双注意引导紧凑双线性卷积神经网络</title>
      <link>https://arxiv.org/abs/2407.03439</link>
      <description><![CDATA[arXiv:2407.03439v1 公告类型：新
摘要：本文介绍了三分支双注意引导紧凑双线性 CNN (DACB-Net)，重点是从疾病特定区域学习以提高准确性和对齐性。全局分支补偿丢失的判别特征，为相关裁剪区域生成注意热图 (AHM)。最后，将全局和局部分支的最后池化层连接起来进行微调，为皮肤病诊断带来的挑战提供了全面的解决方案。虽然当前的 CNN 采用随机梯度下降 (SGD) 进行判别特征学习，使用不同的局部图像块对来计算梯度，并在损失中加入调制因子以在训练期间关注复杂数据。然而，这种方法可能导致数据集不平衡、权重调整和过度拟合。提出的解决方案结合了两个监督分支和一个新颖的损失函数来解决这些问题，从而提高了性能和可解释性。该框架集成了数据增强、迁移学习和微调来解决数据不平衡问题，从而提高分类性能并降低计算成本。在 HAM10000 和 ISIC2019 数据集上的模拟证明了该方法的有效性，与最先进的方法相比，准确率提高了 2.59%。]]></description>
      <guid>https://arxiv.org/abs/2407.03439</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:38 GMT</pubDate>
    </item>
    <item>
      <title>具有关键类别目标的 DETR 检测器的 Fisher 感知量化</title>
      <link>https://arxiv.org/abs/2407.03442</link>
      <description><![CDATA[arXiv:2407.03442v1 公告类型：新
摘要：量化对深度学习模型整体性能的影响是一个研究得很好的问题。然而，在更细粒度的层面上理解和减轻其影响仍然缺乏，特别是对于更困难的任务，例如具有分类和回归目标的对象检测。这项工作将任务关键类别子集的性能（即关键类别性能）定义为检测任务的关键但在很大程度上被忽视的细粒度目标。我们分析了类别级粒度上量化的影响，并提出了提高关键类别性能的方法。具体而言，我们发现某些关键类别对量化具有更高的敏感性，并且在量化感知训练（QAT）后容易过度拟合。为了解释这一点，我们利用 Fisher 信息框架提供了它们的性能差距与相应的损失状况之间的理论和经验联系。利用这一证据，我们在关键类别损失景观上应用了 Fisher 感知混合精度量化方案和 Fisher 迹正则化 QAT。所提出的方法改进了基于量化变压器的 DETR 检测器的关键类别指标。在模型较大且类别数量较多的情况下，过度拟合变得更加严重，这些指标甚至更为重要。例如，我们的方法在 COCO Panoptic 数据集中受影响最严重的关键类别上分别使 4 位 DETR-R50 和可变形 DETR 的 mAP 增益达到 10.4% 和 14.5%。]]></description>
      <guid>https://arxiv.org/abs/2407.03442</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:38 GMT</pubDate>
    </item>
    <item>
      <title>视觉问答 (VQA) 的视觉稳健性基准</title>
      <link>https://arxiv.org/abs/2407.03386</link>
      <description><![CDATA[arXiv:2407.03386v1 公告类型：新
摘要：视觉问答 (VQA) 系统在现实世界中部署时能否表现同样出色？或者它们是否容易受到现实损坏的影响，例如图像模糊，这可能会对敏感应用（例如医疗 VQA）造成不利影响？虽然语言或文本稳健性已在 VQA 文献中得到彻底探索，但尚未对 VQA 模型的视觉稳健性进行任何重大研究。我们提出了第一个包含 213,000 张增强图像的大规模基准，挑战多个 VQA 模型的视觉稳健性并评估现实视觉损坏的强度。此外，我们设计了几个稳健性评估指标，这些指标可以汇总为一个统一的指标并根据各种用例进行定制。我们的实验揭示了模型大小、性能和稳健性与视觉损坏之间关系的几点见解。我们的基准强调了在模型开发中需要采取一种平衡的方法，在不损害稳健性的情况下考虑模型性能。]]></description>
      <guid>https://arxiv.org/abs/2407.03386</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:37 GMT</pubDate>
    </item>
    <item>
      <title>提升、溅射、映射：提升基础蒙版以实现无标签语义场景完成</title>
      <link>https://arxiv.org/abs/2407.03425</link>
      <description><![CDATA[arXiv:2407.03425v1 公告类型：新
摘要：部署在城市环境中的自主移动机器人必须具有上下文感知能力，即能够区分不同的语义实体，并且对遮挡具有鲁棒性。当前的方法（如语义场景完成 (SSC)）需要预先枚举类集和昂贵的人工注释，而表示学习方法放宽了这些假设，但对遮挡不具有鲁棒性，并且学习针对辅助任务量身定制的表示。为了解决这些限制，我们提出了 LSMap，这种方法可以从视觉基础模型中提取掩码，以预测整个场景的鸟瞰图 (BEV) 中的连续、开放集语义和高程感知表示，包括动态实体下方的区域和遮挡区域。我们的模型只需要一张 RGBD 图像，不需要人工标签，并且可以实时运行。我们定量地证明了我们的方法在语义和高程场景完成任务上优于从头开始训练的现有模型，并且经过了微调。此外，我们表明，在无监督语义场景完成方面，我们的预训练表示优于现有的视觉基础模型。我们使用 CODa（一个大规模的真实世界城市机器人数据集）评估我们的方法。补充可视化、代码、数据和预训练模型将很快公开发布。]]></description>
      <guid>https://arxiv.org/abs/2407.03425</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:37 GMT</pubDate>
    </item>
    <item>
      <title>Anole：适配多种压缩模型，实现移动设备跨场景预测</title>
      <link>https://arxiv.org/abs/2407.03331</link>
      <description><![CDATA[arXiv:2407.03331v1 公告类型：新 
摘要：新兴的人工智能物联网（AIoT）应用需要使用移动设备上的深度神经网络（DNN）模型进行在线预测。然而，由于设备的移动，不熟悉的测试样本不断出现，严重影响预训练 DNN 的预测精度。此外，不稳定的网络连接需要本地模型推理。在本文中，我们提出了一种轻量级方案 Anole，以应对移动设备上的本地 DNN 模型推理。Anole 的核心思想是首先建立一支紧凑的 DNN 模型大军，然后自适应地选择最适合当前测试样本的模型进行在线推理。关键是自动识别模型友好的场景以训练场景特定的 DNN 模型。为此，我们设计了一种弱监督场景表示学习算法，结合人类启发式方法和分离场景的特征相似性。此外，我们进一步训练模型分类器来预测每个测试样本最适合的场景特定 DNN 模型。我们在不同类型的移动设备上实现了 Anole，并基于无人机 (UAV) 进行了广泛的跟踪驱动和真实世界实验。结果表明，Anole 在预测准确度（高 4.5%）、响应时间（快 33.1%）和功耗（低 45.1%）方面都优于使用多功能大型 DNN 的方法。]]></description>
      <guid>https://arxiv.org/abs/2407.03331</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:36 GMT</pubDate>
    </item>
    <item>
      <title>DDPM-MoCo：通过生成和对比学习推进工业表面缺陷生成和检测</title>
      <link>https://arxiv.org/abs/2407.03332</link>
      <description><![CDATA[arXiv:2407.03332v1 公告类型：新 
摘要：基于深度学习的工业检测任务通常涉及解决两个问题：（1）获取充足有效的数据样本，（2）使用高效便捷的模型训练方法。在本文中，我们介绍了一种新的缺陷生成方法，称为DDPM-MoCo，以解决这些问题。首先，我们利用去噪扩散概率模型（DDPM）生成高质量的缺陷数据样本，克服了模型学习样本数据不足的问题。此外，我们利用无监督学习动量对比模型（MoCo）和增强的批量对比损失函数在未标记数据上训练模型，解决了扩散模型训练期间大规模负样本编码的效率和一致性挑战。实验结果展示了一种用于识别金属表面缺陷的增强视觉检测方法，涵盖了整个过程，从生成未标记样本数据以训练扩散模型，到利用相同的标记样本数据进行下游检测任务。该研究为金属加工行业的视觉检测提供了宝贵的实践见解和应用潜力。]]></description>
      <guid>https://arxiv.org/abs/2407.03332</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:36 GMT</pubDate>
    </item>
    <item>
      <title>使用神经全向距离场实现游戏 AI 的有效可见性近似</title>
      <link>https://arxiv.org/abs/2407.03330</link>
      <description><![CDATA[arXiv:2407.03330v1 公告类型：新
摘要：可见性信息在游戏 AI 应用中至关重要，但基于光线投射的方法的计算成本对实时系统构成了挑战。为了应对这一挑战，我们提出了一种新方法，将分区的游戏场景表示为神经全向距离场 (ODF)，从而允许在不进行光线投射的情况下在位置之间进行可扩展且高效的可见性近似。对于每个感兴趣的位置，我们将其从球面的全向距离数据映射到 UV 平面上。然后，我们使用多分辨率网格和双线性插值特征来编码方向。这使我们能够使用紧凑的多层感知器 (MLP) 重建这些位置的高频方向距离数据，从而确保快速的推理速度。我们通过离线实验和游戏内评估证明了我们方法的有效性。对于游戏内评估，我们在三个不同的场景中与基于光线投射的可见性测试进行了并排比较。使用紧凑型 MLP（128 个神经元和 2 层），我们的方法在这些场景中实现了平均 9.35 倍的冷启动加速和 4.8 倍的热启动加速。此外，与基于光线投射的方法不同，后者的评估时间受场景特征的影响，我们的方法的评估时间保持不变。]]></description>
      <guid>https://arxiv.org/abs/2407.03330</guid>
      <pubDate>Tue, 09 Jul 2024 03:17:35 GMT</pubDate>
    </item>
    </channel>
</rss>