<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Tue, 20 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>基于交叉伪标记利用强弱数据增强策略的半监督医学图像分割方法</title>
      <link>https://arxiv.org/abs/2402.11273</link>
      <description><![CDATA[arXiv:2402.11273v1 公告类型：新
摘要：由于收集过程具有挑战性、标记成本高、信噪比低以及生物医学图像特征复杂，传统的监督学习方法在医学图像分割方面历来遇到一定的限制。本文提出了一种半监督模型DFCPS，创新性地融合了Fixmatch的概念。通过数据增强处理，对未标记数据采用不同的策略，显着增强了模型的性能和通用性。同时，模型设计适当强调伪标签的生成、过滤和细化过程。引入了交叉伪监督的新概念，将一致性学习与自我训练相结合。这使得模型能够从多个角度充分利用伪标签，从而增强训练多样性。使用可公开访问的 Kvasir-SEG 数据集将 DFCPS 模型与基线模型和高级模型进行比较。在包含不同比例的未标记数据的所有四个细分中，我们的模型始终表现出卓越的性能。我们的源代码可在 https://github.com/JustlfC03/DFCPS 获取。]]></description>
      <guid>https://arxiv.org/abs/2402.11273</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:36 GMT</pubDate>
    </item>
    <item>
      <title>用于密集跟踪的密集匹配器</title>
      <link>https://arxiv.org/abs/2402.11287</link>
      <description><![CDATA[arXiv:2402.11287v1 公告类型：新
摘要：光流是各种应用的有用输入，包括 3D 重建、姿态估计、跟踪和运动结构。尽管它很实用，但密集长期跟踪领域，尤其是在宽基线上，尚未得到广泛探索。本文扩展了 MFT 提出的在对数间隔上组合多个光流的概念。我们展示了 MFT 与不同光流网络的兼容性，产生的结果超越了它们各自的性能。此外，我们在 MFT 框架内提出了这些网络的简单而有效的组合。事实证明，这种方法在位置预测精度方面与更复杂的非因果方法相比具有竞争力，凸显了 MFT 在增强长期跟踪应用方面的潜力。]]></description>
      <guid>https://arxiv.org/abs/2402.11287</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:36 GMT</pubDate>
    </item>
    <item>
      <title>DiffPoint：使用基于 ViT 的扩散模型进行单视点和多视点云重建</title>
      <link>https://arxiv.org/abs/2402.11241</link>
      <description><![CDATA[arXiv:2402.11241v1 公告类型：新
摘要：随着 2D 到 3D 重建任务在各种现实场景中受到广泛关注，能够生成高质量的点云变得至关重要。尽管深度学习模型最近在生成点云方面取得了成功，但由于图像和点云之间的差异，在生成高保真结果方面​​仍然存在挑战。虽然视觉变换器 (ViT) 和扩散模型在各种视觉任务中显示出前景，但它们在从图像重建点云方面的优势尚未得到证明。在本文中，我们首先提出了一种名为 DiffPoint 的简洁而强大的架构，它结合了 ViT 和扩散模型来完成点云重建任务。在每个扩散步骤中，我们将噪声点云划分为不规则的斑块。然后，使用将所有输入视为标记（包括时间信息、图像嵌入和噪声补丁）的标准 ViT 主干，我们训练模型以根据输入图像预测目标点。我们在单视图和多视图重建任务上评估 DiffPoint 并取得了最先进的结果。此外，我们引入了一个统一且灵活的特征融合模块，用于聚合来自单个或多个输入图像的图像特征。此外，我们的工作证明了跨语言和图像应用统一架构来改进 3D 重建任务的可行性。]]></description>
      <guid>https://arxiv.org/abs/2402.11241</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:35 GMT</pubDate>
    </item>
    <item>
      <title>CoLLaVO：Crayon 大语言和视觉模型</title>
      <link>https://arxiv.org/abs/2402.11248</link>
      <description><![CDATA[arXiv:2402.11248v1 公告类型：新
摘要：大型语言模型（LLM）和指令调优的巨大成功推动了视觉语言模型（VLM）向多功能通用模型的发展。然而，目前的 VLM 是否真正具备由“图像中有哪些对象？”确定的高质量对象级图像理解能力仍有待探索。或“哪个对象对应于指定的边界框？”。我们的研究结果表明，当前 VLM 的图像理解能力与其在视觉语言（VL）任务上的零样本性能密切相关。这表明优先考虑基本图像理解对于 VLM 出色完成 VL 任务至关重要。为了增强对象级图像理解，我们提出了蜡笔大语言和视觉模型（CoLLaVO），它将指令调整与蜡笔提示相结合，作为一种基于全景彩色图的新视觉提示调整方案。此外，我们提出了一种 Dual QLoRA 的学习策略，以保留对象级图像理解，而不会在视觉指令调整过程中忘记它，从而在零样本众多 VL 基准测试中实现显着飞跃。]]></description>
      <guid>https://arxiv.org/abs/2402.11248</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:35 GMT</pubDate>
    </item>
    <item>
      <title>超越文字描述：理解和定位符合人类意图的开放世界物体</title>
      <link>https://arxiv.org/abs/2402.11265</link>
      <description><![CDATA[arXiv:2402.11265v1 公告类型：新
摘要：视觉接地（VG）旨在定位与给定自然语言表达相匹配的前景实体。以前的经典 VG 任务的数据集和方法主要依赖于给定表达式必须字面上引用目标对象的先验假设，这极大地阻碍了智能体在现实场景中的实际部署。由于用户通常更喜欢提供所需对象的基于意图的表达，而不是涵盖所有细节，因此代理有必要解释意图驱动的指令。因此，在这项工作中，我们在意图驱动的视觉语言（V-L）理解方面更进一步。为了促进经典 VG 向人类意图解释的方向发展，我们提出了一种新的意图驱动视觉基础（IVG）任务，并构建了一个最大规模的 IVG 数据集，名为 IntentionVG，具有自由形式的意图表达。考虑到实际代理需要在各种场景中移动并寻找特定目标来实现接地任务，我们的 IVG 任务和 IntentionVG 数据集考虑了多场景感知和自我中心视图的关键属性。此外，还建立了各种类型的模型作为实现我们的 IVG 任务的基线。对我们的 IntentionVG 数据集和基线进行的大量实验证明了我们的方法在 V-L 领域的必要性和有效性。为了促进这一方向的未来研究，我们新构建的数据集和基线将公开。]]></description>
      <guid>https://arxiv.org/abs/2402.11265</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:35 GMT</pubDate>
    </item>
    <item>
      <title>用于视觉场景理解的语义感知神经辐射场：综合综述</title>
      <link>https://arxiv.org/abs/2402.11141</link>
      <description><![CDATA[arXiv:2402.11141v1 公告类型：新
摘要：这篇综述深入研究了语义感知神经辐射场 (NeRF) 在视觉场景理解中的作用，涵盖了 250 多篇学术论文的分析。它探讨了 NeRF 如何熟练地推断场景中静态和动态对象的 3D 表示。此功能对于生成高质量的新视点、完成缺失的场景细节（修复）、进行全面的场景分割（全景分割）、预测 3D 边界框、编辑 3D 场景以及提取以对象为中心的 3D 模型至关重要。这项研究的一个重要方面是将语义标签作为视点不变函数的应用，它有效地将空间坐标映射到一系列语义标签，从而促进场景内不同对象的识别。总的来说，这项调查强调了语义感知神经辐射场在视觉场景解释中的进展和多样化应用。]]></description>
      <guid>https://arxiv.org/abs/2402.11141</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:34 GMT</pubDate>
    </item>
    <item>
      <title>一种用于轻量级语义分割的多级特征连续聚合的解码方案</title>
      <link>https://arxiv.org/abs/2402.11201</link>
      <description><![CDATA[arXiv:2402.11201v1 公告类型：新
摘要：多尺度架构，包括分层视觉变换器，已普遍应用于高分辨率语义分割，以最小的性能损失处理计算复杂性。在本文中，我们在这方面提出了一种新颖的语义分割解码方案，该方案从具有多尺度架构的编码器中获取多级特征。基于多级视觉变换器的解码方案旨在通过在多级特征的聚合中引入连续的交叉注意力，不仅减少计算开销，而且提高分割精度。此外，提出了一种通过聚合语义增强多级特征的方法。这项工作的重点是从注意力分配的角度保持上下文一致性，并以显着降低的计算成本提高性能。对流行数据集的一组实验证明了所提出的方案在计算成本方面优于最先进的语义分割模型，且不损失准确性，并且广泛的消融研究证明了所提出想法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.11201</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:34 GMT</pubDate>
    </item>
    <item>
      <title>数字取证中的手部生物识别技术</title>
      <link>https://arxiv.org/abs/2402.11206</link>
      <description><![CDATA[arXiv:2402.11206v1 公告类型：新
摘要：数字取证现在是保护数字世界免遭身份盗窃的不可避免的一部分。高阶犯罪、处理海量数据库对于任何智能系统来说确实是非常具有挑战性的问题。生物识别是解决数字取证遇到的问题的更好解决方案。几十年来，许多生物识别特征在法医学中发挥着重要作用。通过手几何验证方法的说明，研究了基于手的模式在取证中的潜在好处和范围。当有效的生物识别证据适当不可用时可以应用；手套损坏、污垢或任何类型的液体都会降低指纹或掌纹的可访问性和可靠性。由于非常大的数据库存在手部特征纯粹唯一性的危机，因此它可能仅与验证相关。一些单模态和多模态手部生物识别技术（例如手几何、掌纹和手静脉）以及多种特征提取、数据库和验证方法已经通过 2D、3D 和红外图像进行了讨论。]]></description>
      <guid>https://arxiv.org/abs/2402.11206</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:34 GMT</pubDate>
    </item>
    <item>
      <title>手写电路图图像的模块化图形提取</title>
      <link>https://arxiv.org/abs/2402.11093</link>
      <description><![CDATA[arXiv:2402.11093v1 公告类型：新
摘要：随着工程数字化的进步，电路图（也称为原理图）通常在计算机辅助工程（CAE）系统中开发和维护，从而允许在下游工程步骤中进行自动验证、模拟和进一步处理。然而，除了印刷的传统原理图之外，手绘电路图今天仍在教育领域中使用，它们是受训者和学生学习绘制此类图表的一种易于访问的方式。此外，由于法律限制，考试中通常使用手绘原理图。为了利用数字电路表示的功能，需要从光栅图形中提取电气图的自动化方法。
  虽然文献中已经提出了各自的方法，但它们通常是在小型或未公开的数据集上进行的。本文描述了在更大的公共数据集上的模块化端到端解决方案，其中评估各个子任务的方法以形成新的基线。这些子任务包括对象检测（针对电子符号和文本）、二元分割（绘图者的笔画与背景）、手写字符识别以及电子符号和文本的方向回归。此外，还提出了计算机视觉图组装和校正算法。所有方法都集成在公开可用的原型中。]]></description>
      <guid>https://arxiv.org/abs/2402.11093</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:33 GMT</pubDate>
    </item>
    <item>
      <title>GIM：从互联网视频中学习通用图像匹配器</title>
      <link>https://arxiv.org/abs/2402.11095</link>
      <description><![CDATA[arXiv:2402.11095v1 公告类型：新
摘要：图像匹配是一个基本的计算机视觉问题。虽然基于学习的方法在现有基准上实现了最先进的性能，但它们对野外图像的泛化能力很差。此类方法通常需要针对不同场景类型训练单独的模型，并且当场景类型预先未知时是不切实际的。根本问题之一是现有数据构建管道的可扩展性有限，这限制了标准图像匹配数据集的多样性。为了解决这个问题，我们提出了 GIM，这是一种自训练框架，用于使用互联网视频（丰富多样的数据源）学习基于任何图像匹配架构的单个可推广模型。给定一个架构，GIM 首先在标准的特定领域数据集上对其进行训练，然后将其与互补匹配方法相结合，在新颖视频的附近帧上创建密集标签。这些标签通过稳健拟合进行过滤，然后通过将它们传播到远处的帧来增强。最终模型是在具有强增强的传播数据上进行训练的。我们还提出了ZEB，第一个用于图像匹配的零样本评估基准。通过混合来自不同领域的数据，ZEB 可以全面评估不同方法的跨领域泛化性能。应用 GIM 持续提高 3 种最先进的图像匹配架构的零样本性能；对于 50 小时的 YouTube 视频，相对零样本性能提高了 8.4%-18.1%。 GIM 还能够泛化到极端的跨域数据，例如投影 3D 点云的鸟瞰图 (BEV) 图像（图 1(c)）。更重要的是，当对各自领域固有的下游任务进行评估时，我们的单一零样本模型始终优于特定领域的基线。视频演示可在 https://www.youtube.com/watch?v=FU_MJLD8LeY 上观看。]]></description>
      <guid>https://arxiv.org/abs/2402.11095</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:33 GMT</pubDate>
    </item>
    <item>
      <title>II-MMR：识别和改进视觉问答中的多模态多跳推理</title>
      <link>https://arxiv.org/abs/2402.11058</link>
      <description><![CDATA[arXiv:2402.11058v1 公告类型：新
摘要：视觉问答（VQA）通常涉及视觉和语言（V＆L）的多种推理场景。然而，大多数先前的 VQA 研究仅侧重于评估模型的整体准确性，而没有在不同的推理案例上对其进行评估。此外，最近的一些研究发现，传统的思想链（CoT）提示无法为 VQA 生成有效的推理，特别是对于需要多跳推理的复杂场景。在本文中，我们提出了 II-MMR，这是一种识别和改进 VQA 中多模态多跳推理的新思想。具体来说，II-MMR 采用带有图像的 VQA 问题，并使用两种新颖的语言提示找到推理路径来得出答案：(i) 答案预测引导的 CoT 提示，或 (ii) 知识三元组引导的提示。然后，II-MMR 分析这条路径，通过估计回答问题所需的跳跃次数和推理类型（即视觉或超视觉）来识别当前 VQA 基准中的不同推理案例。在包括 GQA 和 A-OKVQA 在内的流行基准测试中，II-MMR 观察到，他们的大多数 VQA 问题都很容易回答，只需要“单跳”推理，而只有少数问题需要“多跳”推理。此外，虽然最近的 V&amp;L 模型即使使用传统的 CoT 方法也难以解决如此复杂的多跳推理问题，但 II-MMR 在零样本和微调设置下的所有推理情况下都显示出其有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.11058</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:32 GMT</pubDate>
    </item>
    <item>
      <title>VQAtack：通过预训练模型对视觉问答进行可转移的对抗性攻击</title>
      <link>https://arxiv.org/abs/2402.11083</link>
      <description><![CDATA[arXiv:2402.11083v1 公告类型：新
摘要：视觉问答（VQA）是计算机视觉和自然语言处理领域的一项基本任务。尽管“预训练和微调”学习范式显着提高了 VQA 性能，但这种学习范式的对抗鲁棒性尚未得到探索。在本文中，我们深入研究了一个新问题：使用预训练的多模态源模型创建对抗性图像文本对，然后将它们转移到攻击目标 VQA 模型。相应地，我们提出了一种新颖的 VQAtack 模型，它可以通过设计的模块迭代生成图像和文本扰动：大语言模型（LLM）增强图像攻击和跨模态联合攻击模块。在每次迭代中，LLM 增强的图像攻击模块首先优化基于潜在表示的损失以生成特征级图像扰动。然后，它结合了法学硕士，通过优化设计的屏蔽答案抗恢复损失来进一步增强图像扰动。跨模态联合攻击模块将在特定迭代时触发，依次更新图像和文本扰动。值得注意的是，文本扰动更新基于单词嵌入空间中学习的梯度和基于单词同义词的替换。与最先进的基线相比，两个 VQA 数据集和五个经过验证的模型的实验结果证明了所提出的 VQAtack 在可转移攻击设置中的有效性。这项工作揭示了 VQA 任务的“预训练和微调”范例中的一个重大盲点。源代码将被发布。]]></description>
      <guid>https://arxiv.org/abs/2402.11083</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:32 GMT</pubDate>
    </item>
    <item>
      <title>男性首席执行官和女性助理：通过配对刻板印象测试探讨文本到图像模型中的性别偏见</title>
      <link>https://arxiv.org/abs/2402.11089</link>
      <description><![CDATA[arXiv:2402.11089v1 公告类型：新
摘要：最近的大规模文本到图像（T2I）模型（例如 DALLE-3）在新应用中展现出巨大潜力，但也面临着前所未有的公平性挑战。先前的研究揭示了单人图像生成中的性别偏见，但 T2I 模型应用可能需要同时描绘两个或更多人。这种情况下的潜在偏见尚未得到探索，从而导致使用中与公平相关的风险。为了研究 T2I 模型中性别偏见的这些潜在方面，我们提出了一种新颖的配对刻板印象测试 (PST) 偏见评估框架。 PST 提示模型在同一张图像中生成两个个体。他们被描述为具有两种社会身份，这些身份通常与异性相关。然后可以通过生成图像中性别刻板印象的符合程度来测量偏差。使用 PST，我们从两个角度评估 DALLE-3：性别职业偏见和组织权力偏见。尽管单人世代看似公平甚至反刻板印象，PST 仍然揭示了性别化的职业和权力协会。此外，与单人环境相比，DALLE-3 在 PST 下为具有男性刻板印象的个体生成了明显更多的男性形象。因此，PST 可以有效揭示 DALLE-3 中单人设置无法捕获的潜在性别偏见。我们的研究结果揭示了现代 T2I 模型中性别偏见的复杂模式，进一步凸显了多模式生成系统中关键的公平挑战。]]></description>
      <guid>https://arxiv.org/abs/2402.11089</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:32 GMT</pubDate>
    </item>
    <item>
      <title>遮挡弹性 3D 人体姿势估计</title>
      <link>https://arxiv.org/abs/2402.11036</link>
      <description><![CDATA[arXiv:2402.11036v1 公告类型：新
摘要：遮挡仍然是单摄像头视频序列 3D 身体姿势估计的关键挑战之一。时间一致性已被广泛用于减轻其影响，但文献中的现有算法并未明确对其进行建模。
  在这里，我们通过将变形体表示为时空图来应用这一点。然后，我们引入一个细化网络，该网络对该图执行图卷积以输出 3D 姿势。为了确保对遮挡的鲁棒性，我们使用一组二进制掩码来训练该网络，我们使用这些掩码来禁用一些边缘，就像在 drop-out 技术中一样。
  实际上，我们模拟了一些关节可以隐藏一段时间的事实，并训练网络免受这种影响。与从单相机序列推断姿势的最先进技术相比，我们证明了这种方法的有效性。]]></description>
      <guid>https://arxiv.org/abs/2402.11036</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:31 GMT</pubDate>
    </item>
    <item>
      <title>你在挣扎吗？装配视频中斗争判定的数据集和基线</title>
      <link>https://arxiv.org/abs/2402.11057</link>
      <description><![CDATA[arXiv:2402.11057v1 公告类型：新
摘要：确定人们何时在视频中陷入困境，可以更细致地理解人们的行为，并为构建智能支持视觉界面提供了机会。在本文中，我们提出了一个新的数据集，其中包含三种装配活动和相应的性能基线，用于从视频中确定挣扎。介绍了三个现实世界的解决问题的活动，包括组装管道（Pipes-Struggle）、搭建露营帐篷（Tent-Struggle）和解决河内塔难题（Tower-Struggle）。视频片段的评分为 w.r.t.注释者使用强制选择 4 点量表所感知的斗争水平。除了众包注释之外，每个视频片段均由一位专家注释者进行注释。该数据集是第一个斗争标注数据集，总共包含来自 73 位参与者的 5.1 小时视频和 725,100 帧。我们评估三个决策任务：斗争分类、斗争水平回归和斗争标签分布学习。我们利用几个主流深度神经网络为每项任务提供基线结果，以及消融研究和结果可视化。我们的工作目标是开发辅助系统，该系统可以分析挣扎，在手动活动期间支持用户并鼓励学习，以及其他视频理解能力。]]></description>
      <guid>https://arxiv.org/abs/2402.11057</guid>
      <pubDate>Wed, 21 Feb 2024 03:13:31 GMT</pubDate>
    </item>
    </channel>
</rss>