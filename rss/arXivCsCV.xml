<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://arxiv.org/</link>
    <description>计算机科学 - 计算机视觉和模式识别 (cs.CV) 在 arXiv.org 电子打印存档上更新</description>
    <lastBuildDate>Fri, 05 Jan 2024 03:14:43 GMT</lastBuildDate>
    <item>
      <title>用于对象重新识别的高效云边协同推理。 （arXiv：2401.02041v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.02041</link>
      <description><![CDATA[当前的对象重新识别（ReID）系统遵循集中式
处理范式，即所有计算都在云服务器中进行
边缘设备仅用于捕获和发送图像。作为数量
视频经历了快速升级，这种模式已经变得不切实际了
到有限的计算资源。在这种情况下，ReID系统
应该进行转换以适应云边缘协作处理范例，
这对于提高 ReID 系统的可扩展性和实用性至关重要。
但目前的相关工作缺乏对此问题的研究，使得
ReID 方法的有效适应面临着挑战。因此，我们开创了
ReID系统的云边协作推理框架，特别是
提出一个分布感知相关建模网络（DaCM）
通过学习将想要的图像尽快返回到云服务器
对实例之间的时空相关性进行建模。 DaCM 嵌入了
时空相关性隐式包含在时间戳中
图结构，并且可以应用在云端来调节图的大小
上传窗口和边缘设备上调整图像顺序，
分别。传统ReID方法可以与DaCM无缝结合，
在我们提出的边缘云协作中启用它们的应用
框架。大量的实验表明我们的方法明显减少了
传输开销并显着提高性能。我们将发布
我们的代码和模型。
]]></description>
      <guid>http://arxiv.org/abs/2401.02041</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:43 GMT</pubDate>
    </item>
    <item>
      <title>间谍水印：用于后门攻击的强大的隐形水印。 （arXiv：2401.02031v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.02031</link>
      <description><![CDATA[后门攻击的目的是在面对后门实例时欺骗受害者模型
同时保持其在良性数据上的性能。目前的方法使用手册
模式或特殊扰动作为触发因素，而他们经常忽视
针对数据损坏的稳健性，使后门攻击易于防御
实践。为了解决这个问题，我们提出了一种新颖的后门攻击方法
名为 Spy-Watermark，在面临数据崩溃和
后门防御。其中，我们引入了嵌入在中的可学习水印
图像的潜在域，作为触发器。然后，我们寻找一个
图像解码时可承受崩溃的水印，配合
多次抗倒塌行动，进一步增强我们的抗灾能力
触发防止数据损坏。进行了大量的实验
CIFAR10、GTSRB 和 ImageNet 数据集，证明 Spy-Watermark
在鲁棒性和隐秘性方面超过了十种最先进的方法。
]]></description>
      <guid>http://arxiv.org/abs/2401.02031</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:42 GMT</pubDate>
    </item>
    <item>
      <title>DiffusionEdge：用于清晰边缘检测的扩散概率模型。 （arXiv：2401.02032v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.02032</link>
      <description><![CDATA[受限于编码器-解码器架构，基于学习的边缘检测器
通常很难预测同时满足正确性和准确性的边缘图
脆度。随着扩散概率模型（DPM）最近的成功，
我们发现它特别适合准确和清晰的边缘检测，因为
去噪处理直接应用于原始图像尺寸。
因此，我们提出了第一个用于一般边缘任务的扩散模型
检测，我们称之为 DiffusionEdge。为了避免昂贵的计算
在保留最终性能的同时，我们在潜在资源中应用了DPM
空间并启用经典的交叉熵损失，该损失具有不确定性
像素级直接优化潜在空间中的参数
蒸馏方式。我们还采用解耦架构来加速
去噪过程并提出相应的自适应傅里叶滤波器来调整
特定频率的潜在特征。凭借所有的技术设计，
DiffusionEdge 可以用有限的资源稳定地训练，预测清晰
以及精确的边缘图，而增强策略却少得多。广泛的
四个边缘检测基准的实验证明了
DiffusionEdge 具有正确性和清晰度。在 NYUDv2 数据集上，
与次优相比，我们增加了ODS、OIS（无需后处理）
和AC分别提高了30.2%、28.1%和65.1%。代码：
https://github.com/GuHuangAI/DiffusionEdge。
]]></description>
      <guid>http://arxiv.org/abs/2401.02032</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:42 GMT</pubDate>
    </item>
    <item>
      <title>通过上下文预测改进基于扩散的图像合成。 （arXiv：2401.02015v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.02015</link>
      <description><![CDATA[扩散模型是一类新型的生成模型，并且具有显着的优势
促进了前所未有的质量和多样性的图像生成。现存的
扩散模型主要尝试从损坏的图像中重建输入图像
沿空间轴具有像素级或特征级约束。然而，这样的
基于点的重建可能无法使每个预测像素/特征充分
保留其邻域上下文，损害基于扩散的图像合成。
背景作为自动监管信号的强大来源，已经得到了很好的体现。
研究学习表征。受此启发，我们第一次
提出 ConPreDiff 来改进基于扩散的上下文图像合成
预言。我们明确强化每个点以预测其邻域
上下文（即多步幅特征/标记/像素），上下文解码器位于
训练阶段扩散去噪块结束，并移除解码器
供推论。这样，每个点都可以更好地重建自己
保留其与邻里上下文的语义联系。这个新的
ConPreDiff 的范式可以推广到任意离散和连续
扩散主干，无需在采样过程中引入额外的参数。
对无条件图像生成进行了大量实验，
文本到图像生成和图像修复任务。我们的ConPreDiff
始终优于以前的方法并实现了新的 SOTA 文本到图像
MS-COCO 上的生成结果，零样本 FID 得分为 6.21。
]]></description>
      <guid>http://arxiv.org/abs/2401.02015</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:41 GMT</pubDate>
    </item>
    <item>
      <title>Spikformer V2：使用 SNN 门票加入 ImageNet 上的高精度俱乐部。 （arXiv：2401.02020v1 [cs.NE]）</title>
      <link>http://arxiv.org/abs/2401.02020</link>
      <description><![CDATA[尖峰神经网络 (SNN)，以其生物学上的合理性而闻名
架构，面临性能有限的挑战。自我关注
机制，这是高性能 Transformer 的基石
这也是一种受生物学启发的结构，在现有的 SNN 中是不存在的。对此
最后，我们探索了利用自注意力能力和
SNN 的生物学特性，并提出了一种新颖的尖峰自注意力（SSA）
和尖峰变压器（Spikformer）。 SSA 机制消除了对
softmax 并使用基于尖峰的查询捕获稀疏视觉特征，
键和值。这种无需乘法的稀疏计算使得 SSA
高效节能。此外，我们开发了一个尖峰卷积干
（SCS）具有补充卷积层来增强架构
Spikformer。使用 SCS 增强的 Spikformer 称为 Spikformer
V2。为了训练更大更深的 Spikformer V2，我们引入了开创性的
探索 SNN 中的自监督学习 (SSL)。具体来说，我们
预训练 Spikformer V2，其掩蔽和重建风格的灵感源自
主流自监督 Transformer，然后对 Spikformer V2 进行微调
ImageNet 上的图像分类。大量实验表明
Spikformer V2 优于之前的其他代理训练和 ANN2SNN
方法。 8 层 Spikformer V2 使用 4 倍时间实现了 80.38% 的准确率
步骤，SSL之后，172M 16层Spikformer V2达到了精度
只需 1 个时间步即可达到 81.10%。据我们所知，这是第一个
SNN 在 ImageNet 上达到 80% 以上准确率的时间。代码将是
可在 Spikformer V2 上购买。
]]></description>
      <guid>http://arxiv.org/abs/2401.02020</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:41 GMT</pubDate>
    </item>
    <item>
      <title>AUPIMO：以高速和低容差重新定义视觉异常检测基准。 （arXiv：2401.01984v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01984</link>
      <description><![CDATA[视觉异常检测研究的最新进展见证了 AUROC 和
AUPRO 在 MVTec 和 VisA 等公共基准数据集上的得分趋于一致
朝着完美的回忆，给人的印象是这些基准是
接近解决。然而，高 AUROC 和 AUPRO 分数并不总是反映
定性绩效，限制了这些指标的有效性
现实世界的应用程序。我们认为，由
缺乏适当的评估指标限制了该领域的进展，并且
至关重要的是，我们必须重新审视用于评价我们的评价指标
算法。作为回应，我们引入了每图像重叠（PIMO），这是一种新颖的指标
解决了 AUROC 和 AUPRO 的缺点。 PIMO 保留
现有指标基于召回的性质，但引入了两个区别：
曲线的分配（以及曲线下的相应区域）是针对每个图像的，
它的 X 轴仅依赖于普通图像。测量每张图像的召回率
简化了实例分数索引，并且对嘈杂的注释更加鲁棒。作为
我们证明，它还可以加速计算并允许使用统计数据
测试来比较模型。通过对误报施加低容忍度
正常图像，PIMO 提供增强的模型验证程序和
突出显示跨数据集的性能差异。我们的实验证明
PIMO 提供了实际优势和细致入微的性能见解
重新定义异常检测基准——特别是挑战认知
MVTec AD 和 VisA 数据集已通过当代模型解决。
可在 GitHub 上获取：https://github.com/jpcbertoldo/aupimo。
]]></description>
      <guid>http://arxiv.org/abs/2401.01984</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>GPS-SSL：引导正采样将先验注入自我监督学习。 （arXiv：2401.01990v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01990</link>
      <description><![CDATA[我们提出引导正采样自监督学习 (GPS-SSL)，
将先验知识注入自监督学习（SSL）的通用方法
阳性样本选择。当前的 SSL 方法利用数据增强
（DA）用于生成正样本并结合先验知识 - 一个
不正确或太弱的 DA 会大大降低学习的质量
表示。 GPS-SSL 建议设计一个度量空间，其中
欧几里得距离成为语义关系的有意义的代理。在
在这个空间中，现在可以从最近的位置生成正样本
邻居采样。现在可以将任何先验知识嵌入到该指标中
空间独立于所使用的 DA。从其简单性来看，GPS-SSL 是
适用于任何 SSL 方法，例如SimCLR 或 BYOL。 GPS-SSL 的一个主要优点是
减少定制强大 DA 的压力。例如 GPS-SSL 达到
在 DA 较弱的 Cifar10 上达到 85.58%，而基线仅达到 37.51%。我们
因此，朝着减少 SSL 依赖的目标迈进了一步
DA。我们还表明，即使使用强大的 DA，GPS-SSL 的性能也优于
未充分研究领域的基线。我们评估 GPS-SSL 以及多个
来自不同域的大量下游数据集的基线 SSL 方法
当模型使用强或最小的数据增强时。我们希望 GPS-SSL
将为研究如何将先验知识注入 SSL 开辟新途径
有原则的方式。
]]></description>
      <guid>http://arxiv.org/abs/2401.01990</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:40 GMT</pubDate>
    </item>
    <item>
      <title>Instruct-Imagen：使用多模式指令生成图像。 （arXiv：2401.01952v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01952</link>
      <description><![CDATA[本文提出了 instruct-imagen，一种处理异构图像的模型
生成任务并对未见过的任务进行概括。我们介绍*多式联运
用于图像生成的指令*，阐明范围的任务表示
准确地表达生成意图。它使用自然语言来合并
不同的模式（例如文本、边缘、样式、主题等），使得
丰富的生成意图可以以统一的格式标准化。

然后，我们通过微调预先训练的文本到图像来构建指令图像
具有两阶段框架的扩散模型。首先，我们使用
检索增强训练，以增强模型的能力
外部多模式环境下的生成。随后，我们对
适应需要视觉语言的各种图像生成任务的模型
理解（例如，主题驱动的生成等），每个都与一个配对
封装任务本质的多模式指令。人类评价
各种图像生成数据集表明，instruct-imagen 匹配或
超越了领域内先前的特定任务模型，并展示了有前途的
泛化到看不见的和更复杂的任务。
]]></description>
      <guid>http://arxiv.org/abs/2401.01952</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>FMGS：用于整体 3D 场景理解的嵌入式 3D 高斯泼溅基础模型。 （arXiv：2401.01970v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01970</link>
      <description><![CDATA[精确感知现实 3D 世界的几何和语义属性
物体对于增强现实和机器人的持续发展至关重要
应用程序。为此，我们提出 \algfull{} (\algname{})，其中
将基础模型的视觉语言嵌入纳入 3D 高斯
泼溅（GS）。这项工作的关键贡献是一种有效的方法
重建并表示 3D 视觉语言模型。这是通过以下方式实现的
将基于图像的基础模型生成的特征图提取为
从我们的 3D 模型渲染出来。为了保证高质量的渲染和快速的训练，
我们通过整合 GS 的优势引入了一种新颖的场景表示
和多分辨率哈希编码（MHE）。我们有效的培训程序
还引入了像素对齐损失，使得渲染的特征距离
相同语义实体的接近，遵循像素级语义边界。
我们的结果证明了显着的多视图语义一致性，
促进各种下游任务，击败最先进的方法
$\mathbf{10.2}$% 基于开放词汇语言的对象检测，
尽管如此，我们的推理速度还是快了 $\mathbf{851\times}$。这项研究
探索视觉、语言和 3D 场景表示的交叉点，
为增强不受控制的现实世界中的场景理解铺平道路
环境。我们计划在论文接受后发布代码。
]]></description>
      <guid>http://arxiv.org/abs/2401.01970</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>以法学硕士作为程序员实现真正的零样本组合视觉推理。 （arXiv：2401.01974v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01974</link>
      <description><![CDATA[视觉推理由端到端神经网络主导，其规模可扩展到
数十亿个模型参数和训练示例。然而，即使是最大的
模型在组合推理、泛化、细粒度方面遇到困难
空间和时间推理以及计数。视觉推理大
语言模型（LLM）作为控制器原则上可以解决这些问题
通过分解任务并通过编排一组解决子任务来克服限制
（视觉）工具。最近，这些模型在任务上取得了很好的表现
例如组合视觉问答、视觉基础和视频
时间推理。然而，以目前的形式，这些模型严重
依赖于提示中上下文示例的人体工程学，这些示例通常是
数据集和任务特定，需要高技能的大量劳动力
程序员。在这项工作中，我们提出了一个可以缓解这些问题的框架
通过引入空间和时间抽象例程并利用
少量标记示例自动生成上下文示例，
从而避免人为创建的上下文示例。在一些视觉上
推理任务中，我们表明我们的框架可以带来持续的收益
性能，使法学硕士作为控制器设置更加稳健，并消除了需要
用于上下文示例的人体工程学。
]]></description>
      <guid>http://arxiv.org/abs/2401.01974</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:39 GMT</pubDate>
    </item>
    <item>
      <title>通过掩码特征重建提取时态知识以进行 3D 对象检测。 （arXiv：2401.01918v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01918</link>
      <description><![CDATA[在精度和效率之间取得平衡是一个突出的问题
鸟瞰 (BEV) 3D 物体检测中的挑战。虽然之前
基于摄像头的 BEV 方法通过结合
长期的时间信息，大多数仍然面临低的问题
效率。一种潜在的解决方案是知识蒸馏。现存的
蒸馏方法只关注重建空间特征，而
忽视暂时的知识。为此，我们提出 TempDistiller，一个
时间知识蒸馏器，从老师那里获得长期记忆
当提供有限数量的帧时，检测器。具体来说，一个
通过整合长期时间知识制定重建目标
通过应用于特征教师的自注意力操作。随后，
通过生成器为被屏蔽的学生特征生成新颖的特征。
最终，我们利用这个重建目标来重建学生
特征。此外，我们还探索时态关系知识
输入学生模型的完整帧。我们验证了该方法的有效性
nuScenes 基准上提出的方法。实验结果表明我们
与基线相比，方法获得 +1.6 mAP 和 +1.1 NDS 的增强，
压缩时间后速度提高约 6 FPS
知识和最准确的速度估计。
]]></description>
      <guid>http://arxiv.org/abs/2401.01918</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>来自多个未指定观点的无监督的以对象为中心的学习。 （arXiv：2401.01922v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01922</link>
      <description><![CDATA[视觉场景极其多样，不仅仅是因为有无限的
物体和背景的可能组合也因为
对同一场景的观察可能会随着视点的变化而产生很大差异。
当从多个视点观察多目标视觉场景时，人类可以
从每个视点组合地感知场景，同时实现
跨不同观点的所谓“对象恒常性”，尽管
确切的观点尚不清楚。这种能力对于人类识别
移动时同一物体，并有效地从视觉中学习。这是
设计具有类似能力的模型很有趣。在本文中，我们
考虑一个学习组合场景表示的新问题
多个未指定（即未知且不相关）的观点，而不使用任何
监督并提出一个深度生成模型来分离潜在的
表示分为视点无关部分和视点相关部分
部分来解决这个问题。在推理过程中，潜在表示是
通过整合信息来随机初始化和迭代更新
神经网络的不同观点。具体进行了几个实验
设计的合成数据集表明，所提出的方法可以有效地
从多个未指定的观点学习。
]]></description>
      <guid>http://arxiv.org/abs/2401.01922</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>我们可以仅使用卷积生成真实的手吗？ （arXiv：2401.01951v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01951</link>
      <description><![CDATA[图像生成模型始终无法重建复杂的图像
几何特征，例如人类手和手指中存在的几何特征
近十年来图像生成中一直存在的问题。虽然大步有
通过增加模型大小和多样化训练数据集来实现，这
这个问题在所有模型中仍然普遍存在，从去噪扩散模型到
生成对抗网络（GAN），指出了一个基本缺陷
底层架构。在本文中，我们演示了如何解决这个问题
可以通过增强卷积层的几何能力来缓解
通过为他们提供一个单一的输入通道，结合相关的
$n$维笛卡尔坐标系。我们表明这极大地
提高 GAN 和变分生成的手部和面部图像的质量
自动编码器（VAE）。
]]></description>
      <guid>http://arxiv.org/abs/2401.01951</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:38 GMT</pubDate>
    </item>
    <item>
      <title>对不成对的医学图像文本基础模型的后门攻击：MedCLIP 的试点研究。 （arXiv：2401.01911v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01911</link>
      <description><![CDATA[近年来，基础模型 (FM) 巩固了其作为
深度学习领域的基石进步。通过提取复杂的
来自大量数据集的模式，这些模型始终达到最先进的水平
跨一系列下游任务的结果，所有这些都不需要
广泛的计算资源。值得注意的是，MedCLIP，一种视觉语言
基于对比学习的医学 FM，采用不成对的设计
图文训练。虽然医学领域经常采用不成对的
扩大数据的培训，探索相关的潜在安全问题
这种方法并没有跟上其实际使用的步伐。值得注意的是，
不配对训练固有的增强能力也表明
微小的标签差异可能会导致显着的模型偏差。在这个
研究中，我们将这种标签差异视为后门攻击问题。我们进一步
分析其对整个 FM 供应链中医疗 FM 的影响。我们的
评估主要围绕医学 FM 的象征 MedCLIP
采用不配对策略。我们首先探索
MedCLIP 中的漏洞源自未配对的图像文本匹配，称为
不匹配。 BadMatch 是使用一组适度的错误标记数据来实现的。
随后，我们通过以下方式破坏 MedCLIP 的对比学习：
BadDist 通过在嵌入之间引入 Bad-Distance 来辅助 BadMatch
干净的数据和有毒的数据。此外，结合 BadMatch 和 BadDist，
攻击管道始终能够抵御不同类型的后门攻击
模型设计、数据集和触发器。此外，我们的研究结果表明，当前
防御策略不足以检测这些潜在威胁
医疗 FM 的供应链。
]]></description>
      <guid>http://arxiv.org/abs/2401.01911</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:37 GMT</pubDate>
    </item>
    <item>
      <title>缩短时间步长：通过尖峰神经网络实现低延迟神经形态对象识别。 （arXiv：2401.01912v1 [cs.CV]）</title>
      <link>http://arxiv.org/abs/2401.01912</link>
      <description><![CDATA[使用尖峰神经网络 (SNN) 进行神经形态对象识别是
低功耗神经拟态计算的基石。然而，现有的 SNN 受到了影响
从显着的延迟，利用 10 到 40 个时间步或更多，来识别
神经形态物体。在低延迟时，现有 SNN 的性能为
急剧退化。在这项工作中，我们提出了收缩 SNN (SSNN)
实现低延迟神经形态物体识别而不降低
表现。具体来说，我们通过以下方式减轻了 SNN 中的时间冗余：
将 SNN 分为多个阶段，时间步长逐渐缩小，
这显着减少了推理延迟。在时间步收缩期间，
时间变换器平滑地变换时间尺度并保留
信息最大化。此外，我们添加了多个早期分类器
SNN 在训练过程中减轻代理梯度和梯度之间的不匹配
真实梯度，以及梯度消失/爆炸，因此
消除低延迟时的性能下降。广泛的实验
在神经形态数据集上，CIFAR10-DVS、N-Caltech101 和 DVS-Gesture
研究表明，SSNN 能够将基线精度提高 6.55% ~ 21.41%。
只需 5 个平均时间步长并且无需任何数据增强，SSNN 就能够
在 CIFAR10-DVS 上实现 73.63% 的准确率。这项工作提出了一个
异构时间尺度 SNN 并提供了有关
开发高性能、低延迟的 SNN。
]]></description>
      <guid>http://arxiv.org/abs/2401.01912</guid>
      <pubDate>Fri, 05 Jan 2024 03:14:37 GMT</pubDate>
    </item>
    </channel>
</rss>