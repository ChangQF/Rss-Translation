<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 25 Apr 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>Guided AbsoluteGrad：梯度的大小对解释的定位和显着性很重要</title>
      <link>https://arxiv.org/abs/2404.15564</link>
      <description><![CDATA[arXiv:2404.15564v1 公告类型：新
摘要：本文提出了一种新的基于梯度的 XAI 方法，称为 Guided AbsoluteGrad，用于解释显着性图。我们利用正梯度幅度和负梯度幅度并利用梯度方差来区分降噪的重要区域。我们还引入了一种名为恢复和预测（RCAP）的新颖评估指标，它考虑了解释的定位和视觉噪声水平目标。我们针对这两个目标提出了两个建议，并证明了评估它们的必要性。我们在三个案例研究中使用 RCAP 指标和其他 SOTA 指标，通过七种基于梯度的 XAI 方法评估 Guided AbsoluteGrad：（1）具有 ResNet50 模型的 ImageNet 数据集； (2) 采用 EfficientNet 模型的国际皮肤成像协作组织 (ISIC) 数据集； (3)具有DenseNet161模型的Places365数据集。我们的方法超越了其他基于梯度的方法，通过梯度幅度展示了增强显着性图解释的质量。]]></description>
      <guid>https://arxiv.org/abs/2404.15564</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:17 GMT</pubDate>
    </item>
    <item>
      <title>MiM：用于 3D 医学图像分析的 Mask in Mask 自监督预训练</title>
      <link>https://arxiv.org/abs/2404.15580</link>
      <description><![CDATA[arXiv:2404.15580v1 公告类型：新
摘要：Vision Transformer (ViT) 在用于 3D 医学图像分析的自监督学习 (SSL) 中表现出了卓越的性能。用于特征预训练的掩模自动编码器（MAE）可以进一步释放 ViT 在各种医学视觉任务上的潜力。然而，由于 3D 医学图像的空间尺寸较大且维度更高，MAE 缺乏分层设计可能会阻碍下游任务的性能。在本文中，我们提出了一种新颖的 3D 医学图像 \textit{Mask in Mask (MiM)} 预训练框架，其目的是通过学习不同尺度的分层视觉标记的判别表示来推进 MAE。我们为来自体积的屏蔽输入引入了多个粒度级别，然后在精细和粗略级别上同时重建。此外，跨层对齐机制应用于相邻层体积，以分层强化解剖学相似性。此外，我们采用混合主干来在预训练期间有效地增强分层表示学习。 MiM 在大量可用的 3D 体积图像（即包含各个身体部位的计算机断层扫描（CT）图像）上进行了预训练。对 13 个公共数据集的大量实验证明了 MiM 在器官/病变/肿瘤分割和疾病分类方面优于其他 SSL 方法。我们进一步将 MiM 扩展到超过 10k 卷的大型预训练数据集，表明大规模预训练可以进一步增强下游任务的性能。该改进还得出结论，研究界应该更加关注 3D 医学图像的医疗保健基础模型的预训练数据集的规模。]]></description>
      <guid>https://arxiv.org/abs/2404.15580</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:17 GMT</pubDate>
    </item>
    <item>
      <title>通过硬负采样理解双曲度量学习</title>
      <link>https://arxiv.org/abs/2404.15523</link>
      <description><![CDATA[arXiv:2404.15523v1 公告类型：新
摘要：近年来，将双曲几何方法融入计算机视觉的趋势日益明显。虽然这些方法使用双曲距离测量在各种度量学习任务上实现了最先进的性能，但支持这种卓越性能的基础理论分析仍然没有得到充分利用。在这项研究中，我们研究了将双曲空间整合到度量学习中的效果，特别是在使用对比损失进行训练时。我们发现需要对现有文献中对比损失中的温度影响进行欧几里德空间和双曲空间之间的全面比较。为了解决这一差距，我们进行了广泛的调查，使用结合了欧几里得空间和双曲空间损失的混合目标函数来对视觉变换器（ViT）的结果进行基准测试。此外，我们还对观察到的性能改进进行了理论分析。我们还揭示了双曲度量学习与硬负采样高度相关，为未来的工作提供了见解。这项工作将为理解双曲图像嵌入提供有价值的数据点和经验。为了更多地阐明问题的解决并鼓励进一步研究我们的方法，我们的代码可在线获取（https://github.com/YunYunY/HypMix）。]]></description>
      <guid>https://arxiv.org/abs/2404.15523</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:16 GMT</pubDate>
    </item>
    <item>
      <title>跨时谱图自动编码器（CTSAE）：用于聚类引力波故障的无监督降维</title>
      <link>https://arxiv.org/abs/2404.15552</link>
      <description><![CDATA[arXiv:2404.15552v1 公告类型：新
摘要：激光干涉引力波天文台（LIGO）的进步显着提高了引力波探测的可行性和可靠性。然而，LIGO 的高灵敏度使其容易受到称为毛刺的瞬态噪声的影响，因此需要与真实的引力波信号进行有效区分。传统方法主要采用完全监督或半监督算法来完成故障分类和聚类任务。在未来对主通道和辅助通道的故障进行识别和分类的任务中，构建带有手动标记的真实数据的数据集是不切实际的。此外，故障的模式可能会随着时间而变化，从而产生新的故障，而无需手动标记。为了应对这一挑战，我们引入了跨时谱图自动编码器（CTSAE），这是一种用于引力波故障降维和聚类的开创性无监督方法。 CTSAE 将新颖的四分支自动编码器与卷积神经网络 (CNN) 和视觉变换器 (ViT) 的混合体集成在一起。为了进一步提取跨多分支的特征，我们引入了一种使用 CLS（Class）标记的新颖的多分支融合方法。我们的模型在主通道上的 GravitySpy O3 数据集上进行了训练和评估，与最先进的半监督学习方法相比，在聚类任务中表现出了卓越的性能。据我们所知，CTSAE 代表了第一个专门为 LIGO 数据聚类而定制的无监督方法，标志着引力波研究领域向前迈出了重要一步。本文代码可参见https://github.com/Zod-L/CTSAE]]></description>
      <guid>https://arxiv.org/abs/2404.15552</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:16 GMT</pubDate>
    </item>
    <item>
      <title>CFPFormer：用于分割和检测的类似特征金字塔的 Transformer 解码器</title>
      <link>https://arxiv.org/abs/2404.15451</link>
      <description><![CDATA[arXiv:2404.15451v1 公告类型：新
摘要：特征金字塔已广泛应用于卷积神经网络（CNN）和转换器中，用于医学图像分割和目标检测等任务。然而，目前现有的模型通常侧重于编码器端的 Transformer 来提取特征，解码器的改进可以通过精心设计的架构带来进一步的潜力。我们提出了 CFPFormer，一种集成了特征金字塔和转换器的新型解码器块。具体来说，通过利用补丁嵌入、跨层特征串联和高斯注意力机制，CFPFormer 增强了特征提取能力，同时促进了跨不同任务的泛化。受益于 Transformer 结构和 U 形连接，我们引入的模型能够捕获远程依赖性并有效地对特征图进行上采样。与现有方法相比，我们的模型在检测小物体方面实现了卓越的性能。我们在医学图像分割数据集和对象检测基准（VOC 2007、VOC2012、MS-COCO）上评估 CFPFormer，证明其有效性和多功能性。在 ACDC Post-2017-MICCAI-Challenge 在线测试集上，我们的模型达到了令人印象深刻的准确性，并且与 Synapse 多器官分割数据集中的原始解码器设置相比表现良好。]]></description>
      <guid>https://arxiv.org/abs/2404.15451</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:15 GMT</pubDate>
    </item>
    <item>
      <title>Metric3D v2：用于零样本度量深度和表面法线估计的多功能单目几何基础模型</title>
      <link>https://arxiv.org/abs/2404.15506</link>
      <description><![CDATA[arXiv:2404.15506v1 公告类型：新
摘要：我们引入了 Metric3D v2，这是一种用于从单个图像进行零样本度量深度和表面法线估计的几何基础模型，这对于度量 3D 恢复至关重要。虽然深度和法线在几何上相关且高度互补，但它们提出了不同的挑战。SoTA 单目深度方法通过学习仿射不变深度实现零样本泛化，而仿射不变深度无法恢复真实世界的度量。同时，由于缺乏大规模标记数据，SoTA 法线估计方法的零样本性能有限。为了解决这些问题，我们提出了度量深度估计和表面法线估计的解决方案。对于度量深度估计，我们表明零样本单视图模型的关键在于解决来自各种相机模型和大规模数据训练的度量模糊性。我们提出了一个规范的相机空间变换模块，它明确解决了模糊性问题，并且可以毫不费力地插入现有的单目模型中。对于表面法线估计，我们提出了一个联合深度法线优化模块，从度量深度中提取各种数据知识，使法线估计器能够学习超越法线标签的知识。配备这些模块后，我们的深度法线模型可以使用来自数千种具有不同类型注释的相机型号的超过 1600 万张图像进行稳定训练，从而实现对具有未见过的相机设置的自然图像的零样本泛化。我们的方法能够准确恢复随机收集的互联网图像上的度量 3D 结构，为合理的单图像计量铺平了道路。我们的项目页面位于 https://JUGGHM.github.io/Metric3Dv2。]]></description>
      <guid>https://arxiv.org/abs/2404.15506</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:15 GMT</pubDate>
    </item>
    <item>
      <title>用于半监督合成图像检索的具有大型多模态模型的视觉增量生成器</title>
      <link>https://arxiv.org/abs/2404.15516</link>
      <description><![CDATA[arXiv:2404.15516v1 公告类型：新
摘要：组合图像检索（CIR）是一项基于提供的文本修改检索类似于查询的图像的任务。当前的技术依赖于使用参考图像、文本、目标图像的标记三元组对 CIR 模型进行监督学习。这些特定的三元组不像简单的图像-文本对那样普遍可用，限制了 CIR 的广泛使用及其可扩展性。另一方面，零样本 CIR 可以相对容易地使用图像标题对进行训练，而无需考虑图像与图像的关系，但这种方法往往会产生较低的精度。我们提出了一种新的半监督 CIR 方法，在辅助数据中搜索参考及其相关目标图像，并学习基于大型语言模型的视觉增量生成器（VDG）来生成描述视觉差异（即视觉增量）的文本两者之间。 VDG 配备了流畅的语言知识并且与模型无关，可以生成伪三元组来提高 CIR 模型的性能。我们的方法显着改进了现有的监督学习方法，并在 CIR 基准上取得了最先进的结果。]]></description>
      <guid>https://arxiv.org/abs/2404.15516</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:15 GMT</pubDate>
    </item>
    <item>
      <title>GLoD：在图像生成中组合全局上下文和局部细节</title>
      <link>https://arxiv.org/abs/2404.15447</link>
      <description><![CDATA[arXiv:2404.15447v1 公告类型：新
摘要：扩散模型已经证明了其根据文本提示合成高质量和多样化图像的能力。然而，同时控制全局上下文（例如，对象布局和交互）和局部细节（例如，颜色和情感）仍然是一个重大挑战。这些模型通常无法理解涉及多个对象的复杂描述，并将指定的视觉属性反映到错误的目标或忽略它们。本文提出了全局局部扩散（\textit{GLoD}），这是一种新颖的框架，允许同时控制文本到图像生成中的全局上下文和局部细节，而无需训练或微调。它将多个全局和局部提示分配给相应的层，并合成它们的噪声，以使用预先训练的扩散模型来指导去噪过程。我们的框架支持复杂的全局-局部组合，用局部提示调节全局提示中的对象，同时保留其他未指定的身份。我们的定量和定性评估表明，GLoD 可以有效地生成符合用户提供的对象交互和对象细节的复杂图像。]]></description>
      <guid>https://arxiv.org/abs/2404.15447</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:14 GMT</pubDate>
    </item>
    <item>
      <title>ID-Aligner：通过奖励反馈学习增强保留身份的文本到图像的生成</title>
      <link>https://arxiv.org/abs/2404.15449</link>
      <description><![CDATA[arXiv:2404.15449v1 公告类型：新
摘要：扩散模型的快速发展引发了多样化的应用。身份保留文本到图像生成（ID-T2I）因其广泛的应用场景（例如人工智能肖像和广告）而受到广泛关注。虽然现有的 ID-T2I 方法已经展示了令人印象深刻的结果，但仍然存在一些关键挑战：（1）很难准确地保持参考肖像的身份特征，（2）生成的图像缺乏审美吸引力，特别是在执行身份保留时，以及（3） ）存在无法同时兼容基于LoRA和基于Adapter的方法的限制。为了解决这些问题，我们提出了 \textbf{ID-Aligner}，一个通用反馈学习框架来增强 ID-T2I 性能。为了解决身份特征丢失的问题，我们引入了身份一致性奖励微调，以利用人脸检测和识别模型的反馈来改善生成的身份保留。此外，我们提出身份美学奖励微调，利用人类注释的偏好数据的奖励和自动构建的角色结构生成反馈来提供美学调整信号。得益于其通用反馈微调框架，我们的方法可以轻松应用于 LoRA 和 Adapter 模型，从而实现一致的性能增益。 SD1.5 和 SDXL 扩散模型的大量实验验证了我们方法的有效性。 \textbf{项目页面：\url{https://idaligner.github.io/}}]]></description>
      <guid>https://arxiv.org/abs/2404.15449</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:14 GMT</pubDate>
    </item>
    <item>
      <title>Wiki-LLaVA：多模式法学硕士的分层检索增强生成</title>
      <link>https://arxiv.org/abs/2404.15406</link>
      <description><![CDATA[arXiv:2404.15406v1 公告类型：新
摘要：多模式法学硕士是法学硕士的自然演变，并扩大了其能力，以超越纯文本模式。随着设计新颖架构以及视觉和语言适配器的研究正在进行，在本文中，我们专注于赋予此类模型回答需要外部知识的问题的能力。我们的方法称为 Wiki-LLaVA，旨在集成多模式文档的外部知识源，该知识源通过分层检索管道进行访问。使用这种方法，从外部知识源检索相关段落，并将其用作法学硕士的附加背景，从而增强生成对话的有效性和准确性。我们使用外部数据对专为视觉问答而定制的数据集进行了广泛的实验，并证明了我们方法的适当性。]]></description>
      <guid>https://arxiv.org/abs/2404.15406</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:13 GMT</pubDate>
    </item>
    <item>
      <title>晶圆图缺陷模式的迭代聚类收获</title>
      <link>https://arxiv.org/abs/2404.15436</link>
      <description><![CDATA[arXiv:2404.15436v1 公告类型：新
摘要：晶圆图缺陷图案的无监督聚类具有挑战性，因为某些缺陷图案的外观变化很大。这包括改变晶圆上缺陷区域的形状、位置、密度和旋转。我们提出了一种收集方法，它甚至可以很好地对晶圆图的具有挑战性的缺陷模式进行聚类。我们的方法利用了众所周知的三步过程：特征提取、降维和聚类。我们方法的新颖之处在于迭代地重复降维和聚类，同时根据其轮廓分数在每次迭代中过滤出一个聚类。该方法总体上提高了聚类性能，并且对于困难的缺陷模式特别有用。较低的计算量允许快速评估大型数据集，并可用于支持手动标记工作。我们对文献中的相关方法进行了基准测试，并在现实世界的工业数据集上展示了改进的结果。]]></description>
      <guid>https://arxiv.org/abs/2404.15436</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:13 GMT</pubDate>
    </item>
    <item>
      <title>深度多原型胶囊网络</title>
      <link>https://arxiv.org/abs/2404.15445</link>
      <description><![CDATA[arXiv:2404.15445v1 公告类型：新
摘要：胶囊网络是一种神经网络，它可以识别图像部分并分层形成整体的实例化参数。网络背后的目标是执行逆向计算机图形任务，网络参数是将部分转换为整体的映射权重。胶囊网络在具有高类内或部分内变化的复杂数据中的可训练性具有挑战性。本文提出了一种多原型架构，用于指导胶囊网络来表示图像部分的变化。为此，所提出的方法不是为每个类和部分考虑单个胶囊，而是采用多个胶囊（共组胶囊），捕获一个对象的多个原型。在最后一层，同组胶囊进行竞争，它们的软输出被认为是竞争性交叉熵损失的目标。此外，在中间层中，最活跃的胶囊映射到下一层，并在协同组之间共享权重。因此，由于参数的减少，隐式权重共享使得拥有更深层的胶囊网络层成为可能。在 MNIST、SVHN、C-Cube、CEDAR、MCYT 和 UTSig 数据集上的实验结果表明，所提出的模型在图像分类精度方面优于其他模型。]]></description>
      <guid>https://arxiv.org/abs/2404.15445</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:13 GMT</pubDate>
    </item>
    <item>
      <title>WANDR：意图引导的人体运动生成</title>
      <link>https://arxiv.org/abs/2404.15383</link>
      <description><![CDATA[arXiv:2404.15383v1 公告类型：新
摘要：合成自然的人体运动，使 3D 人体化身能够行走并达到 3D 空间中的任意目标，这对于许多应用来说仍然是一个未解决的问题。现有方法（数据驱动或使用强化学习）在泛化和运动自然性方面受到限制。主要障碍是缺乏将运动与目标达成相结合的训练数据。为了解决这个问题，我们引入了 WANDR，这是一种数据驱动模型，它采用化身的初始姿势和目标的 3D 位置，并生成自然的人体运动，将末端执行器（手腕）放置在目标位置。为了解决这个问题，我们引入了新颖的意图特征，可以驱动丰富的目标导向的运动。意图引导智能体实现目标，并交互式地使生成适应新的情况，而无需定义子目标或整个运动路径。至关重要的是，意图允许对具有目标导向运动和不具有目标导向运动的数据集进行训练。 WANDR 是一种条件变分自动编码器 (c-VAE)，我们使用 AMASS 和 CIRCLE 数据集对其进行训练。我们广泛评估我们的方法，并证明其生成自然和长期运动的能力，以达到 3D 目标并推广到看不见的目标位置。我们的模型和代码可用于研究目的，网址为 wandr.is.tue.mpg.de。]]></description>
      <guid>https://arxiv.org/abs/2404.15383</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:12 GMT</pubDate>
    </item>
    <item>
      <title>组误差差异之和：生物特征验证中偏差评估的严格检查和双度量测量</title>
      <link>https://arxiv.org/abs/2404.15385</link>
      <description><![CDATA[arXiv:2404.15385v1 公告类型：新
摘要：生物识别验证 (BV) 系统通常在不同人口群体之间表现出准确性差异，导致 BV 应用存在偏差。评估和量化这些偏差对于确保 BV 系统的公平性至关重要。然而，BV 中现有的偏差评估指标存在局限性，例如仅关注匹配或不匹配错误率，忽视绩效水平介于最佳和最差绩效水平之间的人口群体的偏差，以及忽略存在偏差的程度。
  本文深入分析了 BV 中当前偏差评估指标的局限性，并通过实验分析证明了它们的上下文适用性、优点和局限性。此外，它还引入了一种新颖的 BV 通用偏差评估方法，即“组误差差之和 (SEDG)”。我们在受控合成数据集上的实验结果证明了使用现有指标和我们自己提出的措施时人口统计偏差量化的有效性。我们讨论了偏差评估指标在一组模拟人口统计偏差场景中的适用性，并提供了基于场景的指标建议。我们的代码可在 \url{https://github.com/alaaobeid/SEDG} 下公开获取。]]></description>
      <guid>https://arxiv.org/abs/2404.15385</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:12 GMT</pubDate>
    </item>
    <item>
      <title>分层混合切片 Wasserstein：异构联合分布的可扩展度量</title>
      <link>https://arxiv.org/abs/2404.15378</link>
      <description><![CDATA[arXiv:2404.15378v1 公告类型：新
摘要：Sliced Wasserstein (SW) 和 Generalized Sliced Wasserstein (GSW) 由于其计算和统计可扩展性而在应用中得到了广泛的应用。然而，SW 和 GSW 仅在同构域上支持的分布之间定义。此限制阻止它们在具有异构联合分布且在多个不同域上支持边缘分布的应用程序中使用。直接在关节域上使用 SW 和 GSW 无法进行有意义的比较，因为它们的同构切片算子，即 Radon 变换 (RT) 和广义 Radon 变换 (GRT) 的表达能力不足以捕获关节支持集的结构。为了解决这个问题，我们提出了两种新的切片算子，即部分广义氡变换（PGRT）和分层混合氡变换（HHRT）。更详细地说，PGRT 是部分 Radon 变换 (PRT) 的推广，它对函数参数的子集进行非线性变换，而 HHRT 是 PRT 和边缘域参数上的多个特定域 PGRT 的组合。通过使用 HHRT，我们将 SW 扩展到分层混合切片 Wasserstein (H2SW) 距离，该距离专为比较异构联合分布而设计。然后我们讨论 H2SW 的拓扑、统计和计算特性。最后，我们展示了 H2SW 在 3D 网格变形、深度 3D 网格自动编码器和数据集比较方面的良好性能。]]></description>
      <guid>https://arxiv.org/abs/2404.15378</guid>
      <pubDate>Thu, 25 Apr 2024 21:15:11 GMT</pubDate>
    </item>
    </channel>
</rss>