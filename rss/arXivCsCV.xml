<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Thu, 22 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>ToDo：令牌下采样以高效生成高分辨率图像</title>
      <link>https://arxiv.org/abs/2402.13573</link>
      <description><![CDATA[arXiv:2402.13573v1 公告类型：新
摘要：注意力机制对于图像扩散模型至关重要，然而，它们的二次计算复杂性限制了我们在合理的时间和内存限制下可以处理的图像大小。本文研究了生成图像模型中密集注意力的重要性，生成图像模型通常包含冗余特征，使其适合稀疏注意力机制。我们提出了一种新颖的免训练方法 ToDo，它依赖于键和值标记的标记下采样，以将稳定扩散推理加速到常见尺寸的 2 倍，以及高分辨率（如 2048x2048）的 4.5 倍或更多。我们证明，我们的方法在平衡有效吞吐量和保真度方面优于以前的方法。]]></description>
      <guid>https://arxiv.org/abs/2402.13573</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:30 GMT</pubDate>
    </item>
    <item>
      <title>EffLoc：用于高效 6 自由度相机重定位的轻量级视觉转换器</title>
      <link>https://arxiv.org/abs/2402.13537</link>
      <description><![CDATA[arXiv:2402.13537v1 公告类型：新
摘要：相机重定位在计算机视觉中至关重要，在 AR、无人机、机器人和自动驾驶中都有应用。它根据图像估计 3D 相机位置和方向 (6-DoF)。与 SLAM 等传统方法不同，最近的进展是使用深度学习进行直接端到端姿态估计。我们提出了 EffLoc，一种用于单图像相机重定位的新型高效视觉转换器。 EffLoc 的分层布局、内存限制的自注意力和前馈层提高了内存效率和通道间通信。我们引入的顺序群体注意力（SGA）模块通过多样化输入特征、减少冗余和扩展模型容量来提高计算效率。 EffLoc 在效率和准确性方面表现出色，优于 AtLoc 和 MapNet 等现有方法。它在大规模户外汽车驾驶场景中蓬勃发展，确保简单性、端到端可训练性，并消除手工制作的损失函数。]]></description>
      <guid>https://arxiv.org/abs/2402.13537</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>用于文本篡改检测和识别的两阶段双路径框架</title>
      <link>https://arxiv.org/abs/2402.13545</link>
      <description><![CDATA[arXiv:2402.13545v1 公告类型：新
摘要：文档篡改检测一直是篡改检测的一个重要方面。在深度学习出现之前，文档篡改检测很困难。我们在基于深度学习的文本篡改检测领域做了一些探索。我们的Ps篡改检测方法包括三个步骤：特征辅助、审核点定位和篡改识别。它涉及分级过滤和分级输出（被篡改/疑似篡改/未篡改）。通过结合人工篡改数据特征，我们模拟和增强了各种场景下的数据样本（添加噪声/替换的裁剪、单个字符/空格替换、涂抹/拼接、亮度/对比度调整等）。辅助功能包括exif/二进制流关键字检索/噪声，用于根据结果进行分支检测。审核点定位使用检测框架并控制高密度和低密度检测的阈值。篡改识别采用双路双流识别网络，具有RGB和ELA流特征提取。通过自相关百分位池化降维后，融合输出经过 vlad 处理，准确率达到 0.804，召回率达到 0.659，精度达到 0.913。]]></description>
      <guid>https://arxiv.org/abs/2402.13545</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>事件感知视频语料库时刻检索</title>
      <link>https://arxiv.org/abs/2402.13566</link>
      <description><![CDATA[arXiv:2402.13566v1 公告类型：新
摘要：视频语料库时刻检索（VCMR）是一种实用的视频检索任务，专注于使用自然语言查询来识别大量未经修剪的视频语料库中的特定时刻。现有的 VCMR 方法通常依赖于帧感知视频检索，计算查询帧和视频帧之间的相似性，以根据最大帧相似性对视频进行排名。然而，这种方法忽略了嵌入在帧之间信息中的语义结构，即事件、人类理解视频的关键要素。受此启发，我们提出了 EventFormer，这是一种明确利用视频中的事件作为视频检索的基本单元的模型。该模型通过事件推理和分层事件编码来提取事件表示。事件推理模块将连续且视觉上相似的帧表示分组为事件，而分层事件编码在帧和事件级别对信息进行编码。我们还引入了锚点多头自注意力，以鼓励 Transformer 捕获视频中相邻内容的相关性。 EventFormer的训练是通过对VCMR的两个子任务进行双分支对比学习和对偶优化来进行的。 TVR、ANetCaps 和 DiDeMo 基准测试的大量实验显示了 EventFormer 在 VCMR 中的有效性和效率，取得了新的最先进的结果。此外，EventFormer 的有效性也在部分相关视频检索任务上得到了验证。]]></description>
      <guid>https://arxiv.org/abs/2402.13566</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:29 GMT</pubDate>
    </item>
    <item>
      <title>SealD-NeRF：通过神经辐射场对动态场景进行交互式像素级编辑</title>
      <link>https://arxiv.org/abs/2402.13510</link>
      <description><![CDATA[arXiv:2402.13510v1 公告类型：新
摘要：隐式神经表示，尤其是神经辐射场 (NeRF) 的广泛采用，凸显了对隐式 3D 模型编辑功能日益增长的需求，这对于场景后处理和 3D 内容创建等任务至关重要。尽管之前在 NeRF 编辑方面做出了努力，但由于编辑灵活性和质量的限制，挑战仍然存在。关键问题是开发一种支持本地编辑以进行实时更新的神经表示。目前的 NeRF 编辑方法提供像素级调整或详细的几何和颜色修改，但大多仅限于静态场景。本文介绍了 SealD-NeRF，它是 Seal-3D 的扩展，用于动态设置中的像素级编辑，专门针对 D-NeRF 网络。它通过将编辑操作映射到特定时间范围、冻结负责动态场景表示的变形网络以及使用师生方法来集成更改，允许跨序列进行一致的编辑。]]></description>
      <guid>https://arxiv.org/abs/2402.13510</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>探索每像素微比特语义图像压缩的极限</title>
      <link>https://arxiv.org/abs/2402.13536</link>
      <description><![CDATA[arXiv:2402.13536v1 公告类型：新
摘要：传统方法（例如 JPEG）通过对结构信息（例如像素值或频率内容）进行操作来执行图像压缩。这些方法对于标准图像尺寸下大约每像素一位 (bpp) 和更高的比特率有效。相比之下，基于文本的语义压缩使用自然语言直接存储概念及其关系，自然语言随着人类的发展而有效地表示这些显着概念。这些方法可以在极低的比特率下运行，忽略位置、大小和方向等结构信息。在这项工作中，我们使用 OpenAI 的 GPT-4V 和 DALL-E3 来探索图像压缩的质量压缩前沿并确定当前技术的局限性。通过引入迭代反射过程来改进解码图像，我们将语义压缩推至低至 100 $\mu$bpp（比 JPEG 小高达 $10,000\times$）。我们进一步假设这个 100 $\mu$bpp 水平代表了标准图像分辨率下语义压缩的软限制。]]></description>
      <guid>https://arxiv.org/abs/2402.13536</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:28 GMT</pubDate>
    </item>
    <item>
      <title>一种基于多级细化策略的特征匹配方法</title>
      <link>https://arxiv.org/abs/2402.13488</link>
      <description><![CDATA[arXiv:2402.13488v1 公告类型：新
摘要：特征匹配是视觉SLAM中的一个基础且关键的过程，而精度一直是特征匹配中的一个具有挑战性的问题。在本文中，基于多级精细匹配策略，我们提出了一种新的特征匹配方法，称为KTGP-ORB。该方法利用特征描述符生成的汉明空间中局部外观的相似性来建立初始对应关系。结合局部图像运动平滑度的约束，利用GMS算法增强初始匹配的精度，最后利用PROSAC算法进行优化匹配，实现基于欧几里得空间中全局灰度信息的精确匹配。实验结果表明，在光照变化和模糊的复杂场景中，KTGP-ORB方法比ORB算法平均降低了29.92%的误差。]]></description>
      <guid>https://arxiv.org/abs/2402.13488</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>对比提示可改善文本到图像扩散模型中的解缠结</title>
      <link>https://arxiv.org/abs/2402.13490</link>
      <description><![CDATA[arXiv:2402.13490v1 公告类型：新
摘要：文本到图像的扩散模型在图像合成方面取得了显着的性能，而文本界面并不总是提供对某些图像因素的细粒度控制。例如，更改文本中的单个标记可能会对图像产生意想不到的影响。本文展示了对无分类器指导的简单修改可以帮助理清文本到图像模型中的图像因素。我们的方法“对比指导”的关键思想是用两个最小标记不同的提示来表征预期因素：正提示描述要合成的图像，基线提示充当解开其他因素的“基线”。对比指导是一种通用方法，我们在三种情况下说明其优点：(1) 指导在对象类上训练的特定领域扩散模型，(2) 获得用于文本到图像生成的连续、类似装备的控制，以及(3) 提高零样本图像编辑器的性能。]]></description>
      <guid>https://arxiv.org/abs/2402.13490</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>通过一致性正则化将量化感知训练推向全精度性能</title>
      <link>https://arxiv.org/abs/2402.13497</link>
      <description><![CDATA[arXiv:2402.13497v1 公告类型：新
摘要：现有的量化感知训练（QAT）方法强烈依赖于完整的标记数据集或知识蒸馏来保证全精度（FP）精度的性能。然而，实证结果表明，与 FP 相比，QAT 的结果仍然较差。一个问题是如何推动 QAT 接近甚至超越 FP 性能。在本文中，我们从一个新的角度解决了这个问题，通过注入邻近数据分布信息来有效提高QAT的泛化性能。我们提出了一种简单、新颖但功能强大的方法，为 QAT 引入一致性正则化 (CR)。具体来说，CR 假设增强样本在潜在特征空间中应该是一致的。我们的方法可以很好地推广到不同的网络架构和各种 QAT 方法。大量的实验表明，我们的方法明显优于当前最先进的 QAT 方法，甚至 FP 方法。]]></description>
      <guid>https://arxiv.org/abs/2402.13497</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:27 GMT</pubDate>
    </item>
    <item>
      <title>使用带有交叉注意力控制的 ControlNet 生成具有本地化描述的布局到图像</title>
      <link>https://arxiv.org/abs/2402.13404</link>
      <description><![CDATA[arXiv:2402.13404v1 公告类型：新
摘要：虽然文本到图像的扩散模型可以从文本描述生成高质量的图像，但它们通常缺乏对生成图像的视觉构成的细粒度控制。最近的一些工作通过训练模型来根据描述所需图像布局的附加输入来调节生成过程来解决这个问题。 ControlNet 可以说是此类方法中最受欢迎的方法，它可以使用各种类型的条件输入（例如分割图）对生成的图像进行高度控制。然而，它仍然缺乏考虑本地化文本描述的能力，这些文本描述指示提示中的哪个短语描述了哪个图像区域。在这项工作中，我们展示了 ControlNet 对于布局到图像任务的局限性，并使其能够使用无需训练的方法来使用本地化描述，该方法在生成过程中修改交叉注意力分数。我们在 ControlNet 的背景下调整和研究了几种现有的交叉注意力控制方法，并确定了在特定条件下导致失败（概念出血）或图像退化的缺点。为了解决这些缺点，我们开发了一种新颖的交叉注意力操纵方法，以在提高控制的同时保持图像质量。提出了针对挑战性案例的定性和定量实验研究，证明了所研究的通用方法的有效性，并显示了所提出的交叉注意控制方法所获得的改进。]]></description>
      <guid>https://arxiv.org/abs/2402.13404</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>使用对比学习的基于无监督学习的对象检测</title>
      <link>https://arxiv.org/abs/2402.13465</link>
      <description><![CDATA[arXiv:2402.13465v1 公告类型：新
摘要：训练基于图像的物体检测器提出了巨大的挑战，因为它不仅需要物体检测的复杂性，而且还需要在潜在多样化和嘈杂的环境中精确定位物体的复杂性。然而，图像的收集本身通常很简单；例如，安装在车辆上的摄像头可以轻松捕获各种现实场景中的大量数据。鉴于此，我们引入了一种通过无监督/自监督学习来训练单级目标检测器的突破性方法。
  我们最先进的方法有可能彻底改变标签流程，大大减少与手动注释相关的时间和成本。此外，它为以前无法​​实现的研究机会铺平了道路，特别是对于缺乏广泛标签的大型、多样化且具有挑战性的数据集。
  与主要针对分类任务的流行无监督学习方法相比，我们的方法面临着对象检测的独特挑战。我们率先提出了图像内对比学习以及图像间对比学习的概念，从而能够获取对象检测所必需的关键位置信息。该方法熟练地学习并表示该位置信息，从而生成信息丰富的热图。我们的结果展示了 \textbf{89.2\%} 的出色准确度，标志着计算机视觉领域无监督对象检测领域相对于随机初始化的约 \textbf{15x} 的重大突破。]]></description>
      <guid>https://arxiv.org/abs/2402.13465</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>基于多尺度时空变换器的不平衡纵向学习用于不规则时间序列图像的青光眼预测</title>
      <link>https://arxiv.org/abs/2402.13475</link>
      <description><![CDATA[arXiv:2402.13475v1 公告类型：新
摘要：青光眼是一种主要的眼部疾病，它会导致进行性视神经纤维损伤和不可逆性失明，困扰着数百万人。青光眼预测很好地解决了潜在患者的早期筛查和干预，有助于防止病情进一步恶化。它利用一系列历史眼底图像来预测未来发生青光眼的可能性。然而，不规则的抽样性质和不平衡的类别分布是疾病预测方法发展的两个挑战。为此，我们引入了基于为序列图像输入量身定制的变压器架构的多尺度时空变压器网络（MST-former），它可以有效地从时间和空间维度上的序列图像中学习代表性语义信息。具体来说，我们采用多尺度结构来提取各种分辨率的特征，这可以很大程度上利用每个图像中编码的丰富空间信息。此外，我们设计了一个时间距离矩阵以非线性方式缩放时间注意力，可以有效处理不规则采样的数据。此外，我们引入了温度控制的平衡 Softmax 交叉熵损失来解决类别不平衡问题。对青光眼预测序列眼底图像 (SIGF) 数据集的大量实验证明了所提出的 MST 前方法的优越性，青光眼预测的 AUC 达到 98.6%。此外，我们的方法在阿尔茨海默病神经影像计划（ADNI）MRI数据集上显示出出色的泛化能力，对轻度认知障碍和阿尔茨海默病预测的准确率达到90.3%，大幅优于对比方法。]]></description>
      <guid>https://arxiv.org/abs/2402.13475</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:26 GMT</pubDate>
    </item>
    <item>
      <title>Aria 日常活动数据集</title>
      <link>https://arxiv.org/abs/2402.13349</link>
      <description><![CDATA[arXiv:2402.13349v1 公告类型：新
摘要：我们提出了 Aria 日常活动 (AEA) 数据集，这是一个使用 Project Aria 眼镜记录的以自我为中心的多模式开放数据集。 AEA 包含由多个佩戴者在五个不同地理位置的室内地点记录的 143 个日常活动序列。每个记录都包含通过 Project Aria 眼镜记录的多模态传感器数据。此外，AEA 还提供机器感知数据，包括高频全局对齐的 3D 轨迹、场景点云、每帧 3D 眼睛注视向量和时间对齐的语音转录。在本文中，我们演示了该数据集支持的一些示例性研究应用，包括神经场景重建和提示分割。 AEA 是一个开源数据集，可以从projectaria.com 下载。我们还提供开源实现以及如何在 Project Aria Tools 中使用数据集的示例。]]></description>
      <guid>https://arxiv.org/abs/2402.13349</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>将显微镜中的无监督学习和监督学习相结合，可以对完整 4H-SiC 晶圆进行缺陷分析</title>
      <link>https://arxiv.org/abs/2402.13353</link>
      <description><![CDATA[arXiv:2402.13353v1 公告类型：新
摘要：检测和分析半导体材料中的各种缺陷类型是了解潜在机制以及定制生产工艺的重要前提。对揭示缺陷的显微镜图像的分析通常需要图像分析任务，例如分割和对象检测。随着实验产生的数据量不断增加，手动处理这些任务变得越来越不可能。在这项工作中，我们结合了各种图像分析和数据挖掘技术，以创建强大且准确的自动化图像分析管道。这样可以提取 KOH 蚀刻的 4H-SiC 晶圆显微图像中所有缺陷的类型和位置，该图像由大约 40,000 个单独图像拼接在一起。]]></description>
      <guid>https://arxiv.org/abs/2402.13353</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:25 GMT</pubDate>
    </item>
    <item>
      <title>使用智能相机进行检查和监控的视觉系统原型</title>
      <link>https://arxiv.org/abs/2402.13306</link>
      <description><![CDATA[arXiv:2402.13306v1 公告类型：新
摘要：本文介绍了一种人工视觉系统原型的设计，用于自动检查和监控传送带上的物体，并使用智能相机 2D BOA-INS。该原型由传送带和基于 Arduino Mega 卡的嵌入式系统组成，用于系统控制，主要外围设备有智能相机、直流电机、光电传感器、LED 照明和指示状态的 LED（良好或良好）。每个评估对象的缺陷）。原型的应用用于教育目的，以便本科生、硕士和文凭学生可以模拟由嵌入式系统控制的连续生产线，并通过视觉系统和个人计算机进行监控来进行质量控制。这允许实现嵌入式系统、人工视觉、人工智能、模式识别、自动控制以及实际过程自动化等主题。]]></description>
      <guid>https://arxiv.org/abs/2402.13306</guid>
      <pubDate>Thu, 22 Feb 2024 06:17:24 GMT</pubDate>
    </item>
    </channel>
</rss>