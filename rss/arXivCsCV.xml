<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Mon, 01 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>PPTFormer：用于无人机分割的伪多视角变换器</title>
      <link>https://arxiv.org/abs/2406.19632</link>
      <description><![CDATA[arXiv:2406.19632v1 公告类型：新
摘要：无人机 (UAV) 在各个领域的崛起需要有效的无人机图像分割，而无人机捕获图像的动态视角使其面临挑战。传统的分割算法无法准确模拟无人机视角的复杂性，而且获取多视角标记数据集的成本过高。为了解决这些问题，我们引入了 PPTFormer，这是一种新颖的 \textbf{P}seudo Multi-\textbf{P}erspective \textbf{T}trans\textbf{former} 网络，它彻底改变了无人机图像分割。我们的方法通过创建伪视角来增强多视角学习，从而避免了对实际多视角数据的需求。 PPTFormer 网络拥有视角分解、新颖的视角原型以及专门的编码器和解码器，它们共同通过伪多视角注意 (PMP Attention) 和融合实现卓越的分割结果。我们的实验表明，PPTFormer 在五个无人机分割数据集上实现了最先进的性能，证实了它能够有效模拟无人机飞行视角并显著提高分割精度。这项工作代表了无人机场景理解的开创性飞跃，并为语义分割的未来发展树立了新的标杆。]]></description>
      <guid>https://arxiv.org/abs/2406.19632</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:17 GMT</pubDate>
    </item>
    <item>
      <title>精度很重要：弱监督语义分割的精度感知集成</title>
      <link>https://arxiv.org/abs/2406.19638</link>
      <description><![CDATA[arXiv:2406.19638v1 公告类型：新
摘要：弱监督语义分割 (WSSS) 采用弱监督（例如图像级标签）来训练分割模型。尽管最近的 WSSS 方法取得了令人瞩目的成就，但我们发现引入具有高平均并集 (mIoU) 的弱标签并不能保证较高的分割性能。现有研究强调了优先考虑精度和降低噪音以提高整体性能的重要性。同样，我们提出了 ORANDNet，一种为 WSSS 量身定制的高级集成方法。ORANDNet 结合了来自两个不同分类器的类激活图 (CAM) 来提高伪掩模 (PM) 的精度。为了进一步减轻 PM 中的小噪音，我们结合了课程学习。这涉及最初使用较小尺寸的图像和相应的 PM 对训练分割模型，逐渐过渡到原始大小的对。通过结合 ResNet-50 和 ViT 的原始 CAM，我们显著提高了单一最佳模型和朴素集成模型的分割性能。我们进一步将集成方法扩展到 AMN（类似 ResNet）和 MCTformer（类似 ViT）模型的 CAM，在高级 WSSS 模型中实现性能优势。它凸显了我们的 ORANDNet 作为 WSSS 模型的最终附加模块的潜力。]]></description>
      <guid>https://arxiv.org/abs/2406.19638</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:17 GMT</pubDate>
    </item>
    <item>
      <title>通过递归多分支融合实现高效事件流超分辨率</title>
      <link>https://arxiv.org/abs/2406.19640</link>
      <description><![CDATA[arXiv:2406.19640v1 公告类型：新
摘要：当前的事件流超分辨率（ESR）方法忽略了事件流中正事件和负事件中存在的冗余和互补信息，采用直接混合方法进行超分辨率，这可能会导致细节丢失和效率低下。为了解决这些问题，我们提出了一种有效的递归多分支信息融合网络（RMFNet），将正事件和负事件分开以提取互补信息，然后相互补充和细化。特别是，我们引入了特征融合模块（FFM）和特征交换模块（FEM）。FFM 旨在融合相邻事件流中的上下文信息，利用正事件和负事件之间的耦合关系来减轻各自分支中噪声的误导。FEM 有效地促进了正分支和负分支之间的信息融合和交换，实现了卓越的局部信息增强和全局信息补充。实验结果表明，我们的方法在合成和真实数据集上实现了超过 17% 和 31% 的改进，同时加速了 2.3 倍。此外，我们在两个下游事件驱动应用程序（即对象识别和视频重建）上评估了我们的方法，取得了优于现有方法的显著成果。我们的代码和补充材料可在 https://github.com/Lqm26/RMFNet 上找到。]]></description>
      <guid>https://arxiv.org/abs/2406.19640</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:17 GMT</pubDate>
    </item>
    <item>
      <title>深度聚类综述：基于先验的视角</title>
      <link>https://arxiv.org/abs/2406.19602</link>
      <description><![CDATA[arXiv:2406.19602v2 公告类型：新
摘要：借助神经网络强大的特征提取能力，深度聚类在分析高维复杂的现实世界数据方面取得了巨大成功。深度聚类方法的性能受到网络结构和学习目标等多种因素的影响。然而，正如本调查所指出的，深度聚类的本质在于先验知识的结合和利用，而现有的研究在很大程度上忽略了这一点。从基于数据结构假设的开创性深度聚类方法到最近基于数据增强不变性的对比聚类方法，深度聚类的发展本质上对应于先验知识的演变。在本调查中，我们通过将深度聚类方法分为六种先验知识类型，对其进行了全面的回顾。我们发现，一般来说，先验创新遵循两种趋势，即 i) 从挖掘到构建，以及 ii) 从内部到外部。此外，我们在五个广泛使用的数据集上提供了基准，并分析了具有不同先验的方法的性能。通过提供新颖的先验知识视角，我们希望这项调查能够提供一些新颖的见解并启发深度聚类社区的未来研究。]]></description>
      <guid>https://arxiv.org/abs/2406.19602</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:16 GMT</pubDate>
    </item>
    <item>
      <title>使用像素移位跟踪实现最佳视频压缩</title>
      <link>https://arxiv.org/abs/2406.19630</link>
      <description><![CDATA[arXiv:2406.19630v1 公告类型：新
摘要：视频约占所有互联网流量的 85%，但视频编码/压缩历来都是使用硬编码规则进行的，这种方法效果很好，但只能达到一定限度。过去几年，我们看到使用基于 ML 模型的视频压缩算法激增，其中许多算法的表现优于几种传统编解码器。这些模型包括使用 ML 方法对视频进行端到端编码，或使用 ML 模型替换传统编解码器中的一些中间步骤以提高这些步骤的效率。
优化视频存储是视频处理的一个重要方面，因此我们提出了一种可能的方法是避免每帧的冗余数据。在本文中，我们想介绍一种在给定视频的后续帧中去除冗余的方法，作为视频压缩的主要方法。我们将这种方法称为使用 Shift (R\textsuperscript2S) 的冗余去除。该方法可以应用于各种机器学习模型算法，使压缩更易于访问和适应。在本研究中，我们利用基于计算机视觉的像素点跟踪方法来识别冗余像素，以便对视频进行编码以实现最佳存储。]]></description>
      <guid>https://arxiv.org/abs/2406.19630</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:16 GMT</pubDate>
    </item>
    <item>
      <title>用于高光谱重建的经济高效的主动照明相机</title>
      <link>https://arxiv.org/abs/2406.19560</link>
      <description><![CDATA[arXiv:2406.19560v1 公告类型：新
摘要：高光谱成像最近越来越受到关注，用于不同的应用，包括农业调查、地面跟踪、遥感等。然而，高成本、大尺寸和复杂的操作过程阻碍了高光谱相机用于各种应用和研究领域。在本文中，我们介绍了一种经济高效、紧凑且易于使用的主动照明相机，它可能使许多应用受益。我们开发了这种相机的功能齐全的原型。为了帮助农业研究，我们测试了我们的相机进行植物根部成像。此外，通过使用参考高光谱相机的数据作为地面实况和我们的相机数据作为输入，训练了一个用于光谱重建的 U-Net 模型。我们展示了我们的相机能够通过典型的 RGB 相机获取更多信息。此外，从多光谱输入重建高光谱数据的能力使我们的设备无需修改即可与为高光谱应用开发的模型和算法兼容。]]></description>
      <guid>https://arxiv.org/abs/2406.19560</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>检测像 Sora 这样的 AI 生成的视频有什么重要意义？</title>
      <link>https://arxiv.org/abs/2406.19568</link>
      <description><![CDATA[arXiv:2406.19568v1 公告类型：新
摘要：基于扩散的视频生成的最新进展已经展示了显着的成果，但合成视频和真实视频之间的差距仍未得到充分探索。在这项研究中，我们从三个基本角度研究了这一差距：外观、运动和几何，将真实世界的视频与最先进的 AI 模型稳定视频扩散生成的视频进行比较。为此，我们使用 3D 卷积网络训练了三个分类器，每个分类器都针对不同的方面：外观的视觉基础模型特征、运动的光流和几何的单眼深度。每个分类器在假视频检测方面都表现出强大的性能，无论是定性和定量。这表明 AI 生成的视频仍然很容易被检测到，并且真实视频和假视频之间仍然存在显着差距。此外，利用 Grad-CAM，我们可以精确定位 AI 生成的视频在外观、运动和几何方面的系统性故障。最后，我们提出了一个集成外观、光流和深度信息的专家组合模型，用于假视频检测，从而增强了鲁棒性和泛化能力。我们的模型能够高精度地检测由 Sora 生成的视频，即使在训练期间没有接触任何 Sora 视频。这表明，真实视频和假视频之间的差距可以推广到各种视频生成模型。项目页面：https://justin-crchang.github.io/3DCNNDetection.github.io/]]></description>
      <guid>https://arxiv.org/abs/2406.19568</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>PathAlign：组织病理学中全切片图像的视觉语言模型</title>
      <link>https://arxiv.org/abs/2406.19578</link>
      <description><![CDATA[arXiv:2406.19578v1 公告类型：新
摘要：组织病理学图像的微观解释是许多重要诊断和治疗决策的基础。虽然视觉语言建模的进步为分析此类图像带来了新的机会，但全幻灯片图像 (WSI) 的千兆像素级大小带来了独特的挑战。此外，病理报告同时突出小区域的关键发现，同时汇总多张幻灯片的解释，这通常使得创建强大的图像文本对变得困难。因此，病理报告仍然是计算病理学中尚未开发的监督来源，大多数努力依赖于感兴趣区域注释或补丁级别的自我监督。在这项工作中，我们使用 WSI 与病理报告中的精选文本配对，开发了一个基于 BLIP-2 框架的视觉语言模型。这使得应用程序能够利用共享的图像-文本嵌入空间，例如用于查找感兴趣案例的文本或图像检索，以及将 WSI 编码器与冻结的大型语言模型 (LLM) 集成，以实现基于 WSI 的生成文本功能，例如报告生成或 AI-in-the-loop 交互。我们利用超过 350,000 个 WSI 和诊断文本对的去识别数据集，涵盖广泛的诊断、程序类型和组织类型。我们展示了病理学家对使用 WSI 嵌入进行文本生成和文本检索的评估，以及 WSI 分类和工作流优先级排序（幻灯片级分类）的结果。病理学家对 78% 的 WSI 的模型生成文本评为准确，没有临床上显着的错误或遗漏。这项工作展示了语言对齐的 WSI 嵌入令人兴奋的潜在能力。]]></description>
      <guid>https://arxiv.org/abs/2406.19578</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:15 GMT</pubDate>
    </item>
    <item>
      <title>人工智能生成的图像检测的健全性检查</title>
      <link>https://arxiv.org/abs/2406.19435</link>
      <description><![CDATA[arXiv:2406.19435v1 Announce Type: new 
摘要：随着生成模型的快速发展，识别AI生成的内容引起了工业界和学术界越来越多的关注。在本文中，我们对“AI生成的图像检测任务是否已经解决”进行了健全性检查。首先，我们展示了Chameleon数据集，其中包含对人类感知真正具有挑战性的AI生成的图像。为了量化现有方法的泛化，我们在Chameleon数据集上评估了9个现成的AI生成图像检测器。经过分析，几乎所有模型都将AI生成的图像归类为真实图像。后来，我们提出了AIDE（具有混合特征的AI生成图像检测器），它利用多个专家同时提取视觉伪影和噪声模式。具体而言，为了捕获高级语义，我们利用CLIP来计算视觉嵌入。这有效地使模型能够根据语义或上下文信息辨别AI生成的图像；其次，我们选择图像中频率最高的块和频率最低的块，并计算低级块特征，旨在通过低级伪像（例如噪声模式、抗锯齿等）检测 AI 生成的图像。在对现有基准（例如 AIGCDetectBenchmark 和 GenImage）进行评估时，AIDE 比最先进的方法分别提高了 3.5% 和 4.6%，并且在我们提出的具有挑战性的 Chameleon 基准上，它也取得了令人鼓舞的结果，尽管检测 AI 生成的图像的这个问题还远未解决。数据集、代码和预训练模型将发布在 https://github.com/shilinyan99/AIDE。]]></description>
      <guid>https://arxiv.org/abs/2406.19435</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>基于立体视觉的机器人，用于 VR 支持的远程监控</title>
      <link>https://arxiv.org/abs/2406.19498</link>
      <description><![CDATA[arXiv:2406.19498v1 公告类型：新
摘要：机器视觉系统在视觉监控系统中发挥着重要作用。借助立体视觉和机器学习，它将能够模仿人类的视觉系统和对环境的行为。在本文中，我们介绍了一种基于立体视觉的 3-DOF 机器人，该机器人将用于使用云服务器和互联网设备远程监控各个地方。3-DOF 机器人将传输类似人类的头部运动，即偏航、俯仰、滚动，并产生 3D 立体视频并实时传输。此视频流通过任何支持 VR 盒的通用互联网设备发送给用户，即智能手机，为用户提供第一人称实时 3D 体验，并将用户的头部运动实时传输给机器人。机器人还将能够使用深度神经网络跟踪移动物体和面部作为目标，这使其成为一个独立的监控机器人。用户将能够选择在空间中监控的特定对象。立体视觉使我们能够跟踪检测到的不同物体的深度信息，并将用于跟踪人类感兴趣的物体及其距离并发送到云端。开发了一个完整的工作原型，展示了基于立体视觉、机器人技术和机器学习的监控系统的功能。]]></description>
      <guid>https://arxiv.org/abs/2406.19498</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>人类感知和视觉色差的色彩模型比较分析</title>
      <link>https://arxiv.org/abs/2406.19520</link>
      <description><![CDATA[arXiv:2406.19520v1 公告类型：新
摘要：颜色是人类体验不可或缺的一部分，影响情绪、决策和感知。本文对各种颜色模型与人类视觉感知的一致性进行了比较分析。该研究评估了 RGB、HSV、HSL、XYZ、CIELAB 和 CIELUV 等颜色模型，以评估它们在准确表示人类感知颜色方面的有效性。我们根据每个模型准确反映视觉颜色差异和与人眼兼容的主导调色板提取的能力对其进行评估。在图像处理中，准确评估颜色差异对于从数字设计到质量控制等应用至关重要。当前的色差指标并不总是与人们看到的颜色相匹配，导致无法准确判断细微差异。了解不同的颜色模型如何与人类视觉感知保持一致对于图像处理、数字媒体和设计中的各种应用至关重要。]]></description>
      <guid>https://arxiv.org/abs/2406.19520</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>加权圆融合：从不同的物体检测结果中整合圆表示</title>
      <link>https://arxiv.org/abs/2406.19540</link>
      <description><![CDATA[arXiv:2406.19540v1 公告类型：新
摘要：最近，在医学成像研究中，使用圆形表示法已成为一种改进球形物体（如肾小球、细胞和细胞核）识别的方法。在传统的基于边界框的物体检测中，结合多种模型的结果可以提高准确性，尤其是在实时处理并不重要的情况下。不幸的是，这种广泛采用的策略并不容易用于组合圆形表示。在本文中，我们提出了加权圆融合 (WCF)，这是一种合并各种圆检测模型预测的简单方法。我们的方法利用与每个提议的边界圆相关的置信度分数来生成平均圆。我们的方法在专有数据集上进行了彻底评估，用于全幻灯片成像 (WSI) 中物体检测中的肾小球检测。研究结果显示，与现有的集成方法相比，性能分别提高了 5%。此外，加权圆融合技术不仅提高了医学图像中目标检测的精度，而且显著减少了误检，为未来病理图像分析的研究和应用提供了一个有希望的方向。]]></description>
      <guid>https://arxiv.org/abs/2406.19540</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:14 GMT</pubDate>
    </item>
    <item>
      <title>使用反射-透射图像对捕捉编织织物</title>
      <link>https://arxiv.org/abs/2406.19398</link>
      <description><![CDATA[arXiv:2406.19398v2 公告类型：新
摘要：数字化机织织物对从数字人到室内设计等许多应用都很有价值。先前的研究介绍了一种轻量级机织织物采集方法，即捕获单个反射图像并使用可微分的几何和阴影模型估算织物参数。估计的织物参数的渲染可以与照片紧密匹配；然而，捕获的反射图像不足以完全表征织物样品的反射率。例如，不同厚度的织物可能具有相似的反射图像，但会导致显着不同的透射。我们建议从两个捕获的图像中恢复机织织物参数：反射和透射。我们方法的核心是一个可微分的双向散射分布函数 (BSDF) 模型，处理反射和透射，包括单次和多次散射。我们提出了一个双层模型，其中单次散射使用与之前工作相同的 SGGX 相位函数，而多次散射使用新的方位不变微薄片定义，我们称之为 ASGGX。这种新的织物 BSDF 模型在反射和透射方面都与真实编织织物非常相似。我们使用一个简单的设置，用手机相机和两个点光源捕捉反射和透射照片，并通过轻量级网络估计织物参数，并进行可微分优化。我们还使用一个简单的解决方案明确地模拟了失焦效果，以更好地匹配薄透镜相机。因此，估计参数的渲染首次可以在反射和透射方面与输入图像一致。本文的代码位于 https://github.com/lxtyin/FabricBTDF-Recovery。]]></description>
      <guid>https://arxiv.org/abs/2406.19398</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>深度卷积神经网络与变分形状紧致性先验相结合用于图像分割</title>
      <link>https://arxiv.org/abs/2406.19400</link>
      <description><![CDATA[arXiv:2406.19400v1 公告类型：新
摘要：形状紧凑性是描述许多图像分割任务中有趣区域的关键几何属性。在本文中，我们提出了两种新算法来解决引入的图像分割问题，该算法结合了形状紧凑性先验。现有的此类问题算法通常存在计算效率低、难以达到局部最小值以及需要微调超参数的问题。为了解决这些问题，我们提出了一种新的优化模型及其等效的原始对偶模型，并引入了一种基于原始对偶阈值动力学 (PD-TD) 的新优化算法。此外，我们放宽了解决方案约束并提出了另一种新的原始对偶软阈值动力学算法 (PD-STD) 以实现卓越的性能。基于对 sigmoid 层的变分解释，提出的 PD-STD 算法可以集成到深度神经网络 (DNN) 中，以强制将紧凑区域作为图像分割结果。与现有的深度学习方法相比，大量实验表明，所提出的算法在数值效率和有效性方面优于最先进的算法，尤其是在应用于流行的 DeepLabV3 和 IrisParseNet 网络时，在嘈杂的 Iris 数据集上具有更高的 IoU、dice 和紧凑性指标。特别是在高噪声图像数据集上，所提出的算法显著提高了 IoU，提高了 20%。]]></description>
      <guid>https://arxiv.org/abs/2406.19400</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:13 GMT</pubDate>
    </item>
    <item>
      <title>YOLOv10 诞生：对《你只看一次》系列的十年全面回顾</title>
      <link>https://arxiv.org/abs/2406.19407</link>
      <description><![CDATA[arXiv:2406.19407v2 公告类型：新 
摘要：本综述系统地研究了 You Only Look Once (YOLO) 物体检测算法从 YOLOv1 到最近发布的 YOLOv10 的进展。本研究采用逆向时间顺序分析，研究了 YOLO 算法引入的进步，从 YOLOv10 开始，然后是 YOLOv9、YOLOv8 和后续版本，以探索每个版本对提高实时物体检测的速度、准确性和计算效率的贡献。该研究强调了 YOLO 在五个关键应用领域的变革性影响：汽车安全、医疗保健、工业制造、监控和农业。通过详细介绍后续 YOLO 版本中的渐进式技术进步，本综述记录了 YOLO 的发展历程，并讨论了每个早期版本中的挑战和局限性。这一演变标志着 YOLO 在未来十年内将与多模态、情境感知和通用人工智能 (AGI) 系统相结合的道路，对未来人工智能驱动应用的发展具有重要意义。]]></description>
      <guid>https://arxiv.org/abs/2406.19407</guid>
      <pubDate>Tue, 02 Jul 2024 03:16:13 GMT</pubDate>
    </item>
    </channel>
</rss>