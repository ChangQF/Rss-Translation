<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>cs.CV 在 arXiv.org 上的更新</title>
    <link>http://rss.arxiv.org/rss/cs.CV</link>
    <description>cs.CV 在 arXiv.org 电子印刷档案上更新。</description>
    <lastBuildDate>Wed, 17 Jul 2024 04:00:00 GMT</lastBuildDate>
    <item>
      <title>用于面部行为理解的表征学习和身份对抗训练</title>
      <link>https://arxiv.org/abs/2407.11243</link>
      <description><![CDATA[arXiv:2407.11243v1 公告类型：新
摘要：面部动作单元 (AU) 检测已引起广泛研究关注，因为 AU 包含复杂的表情信息。在本文中，我们分别解开了 AU 检测中的两个基本因素：数据和主体身份正则化。受基础模型最新进展的启发，我们强调了数据的重要性，并从多个公共资源中收集了一个包含 900 万张面部图像的多样化数据集 Face9M。在 Face9M 上对蒙版自动编码器进行预训练可在 AU 检测和面部表情任务中产生出色的性能。然后，我们表明 AU 数据集中的主体身份为模型提供了一种捷径学习，并导致 AU 预测的次优解决方案。为了解决 AU 任务的这个一般问题，我们提出了身份对抗训练 (IAT)，并证明强大的 IAT 正则化对于学习身份不变特征是必要的。此外，我们阐明了 IAT 的设计空间，并通过经验表明 IAT 绕过了身份捷径学习并产生了更好的解决方案。我们提出的方法，面部蒙版自动编码器 (FMAE) 和 IAT，简单、通用且有效。值得注意的是，提出的 FMAE-IAT 方法在 BP4D (67.1\%)、BP4D+ (66.8\%) 和 DISFA (70.1\%) 数据库上获得了新的最先进的 F1 分数，远远优于以前的工作。我们在 https://github.com/forever208/FMAE-IAT 上发布了代码和模型，这是第一个在 900 万张不同图像上进行预训练的开源面部模型。]]></description>
      <guid>https://arxiv.org/abs/2407.11243</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:31 GMT</pubDate>
    </item>
    <item>
      <title>通过逼真的服装动画实现高质量 3D 运动传输</title>
      <link>https://arxiv.org/abs/2407.11266</link>
      <description><![CDATA[arXiv:2407.11266v1 公告类型：新
摘要：为风格化的角色制作动画以匹配参考运动序列是电影和游戏行业中一项要求很高的任务。现有方法主要关注角色身体的刚性变形，而忽略了由物理动力学驱动的服装局部变形。它们以与身体相同的方式变形服装，导致结果细节有限且不切实际，例如身体服装穿透。相比之下，我们提出了一种新颖的方法，旨在通过逼真的服装动画实现高质量的运动传递。由于现有数据集缺乏生成逼真服装动画所需的注释，我们构建了一个名为 MMDMC 的新数据集，它将来自 MikuMikuDance 社区的风格化角色与现实世界的运动捕捉数据相结合。然后，我们提出了一个数据驱动的管道，通过两个神经变形模块学习解开身体和服装的变形。对于身体部位，我们提出了一个测地线注意模块，以有效地将语义先验纳入骨骼身体变形中，以解决风格化角色的复杂体形。由于服装运动可能与各自的身体关节有显著偏差，我们建议在以历史状态为条件的非线性顶点位移场中对服装变形进行建模。大量实验表明，我们的方法可以为各种类型的服装产生优质的结果。我们的数据集发布在 https://github.com/rongakowang/MMDMC。]]></description>
      <guid>https://arxiv.org/abs/2407.11266</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:31 GMT</pubDate>
    </item>
    <item>
      <title>TLRN：用于大变形图像配准的时间潜在残差网络</title>
      <link>https://arxiv.org/abs/2407.11219</link>
      <description><![CDATA[arXiv:2407.11219v1 公告类型：新
摘要：本文提出了一种新方法，称为时间潜在残差网络 (TLRN)，用于预测时间序列图像配准中的一系列变形场。配准时间序列图像的挑战通常在于发生大运动，尤其是当图像与参考图像有显著差异时（例如，心动周期的开始与峰值拉伸阶段相比）。为了获得准确而稳健的配准结果，我们利用运动连续性的性质并利用连续图像帧中的时间平滑性。我们提出的 TLRN 强调了一个时间残差网络，其中残差块在潜在变形空间中经过精心设计，由时间顺序的初始速度场参数化。我们将一系列随时间变化的残差块视为一个动态训练系统，其中每个块都旨在学习所需变形特征与从先前时间帧累积的当前输入之间的残差函数。我们在合成数据和真实世界电影心脏磁共振 (CMR) 图像视频上验证了 TLRN 的有效性。我们的实验结果表明，与最先进的方法相比，TLRN 能够实现显着提高的配准精度。我们的代码可在 https://github.com/nellie689/TLRN 上公开获取。]]></description>
      <guid>https://arxiv.org/abs/2407.11219</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:30 GMT</pubDate>
    </item>
    <item>
      <title>与 SLAM 方法相比，评估 NeRF 重建的几何精度</title>
      <link>https://arxiv.org/abs/2407.11238</link>
      <description><![CDATA[arXiv:2407.11238v1 公告类型：新
摘要：随着神经辐射场 (NeRF) 实现变得更快、更高效和更准确，它们在现实世界地图绘制任务中的适用性变得更加容易。传统上，3D 地图绘制或场景重建依赖于昂贵的 LiDAR 传感。摄影测量可以执行基于图像的 3D 重建，但计算成本高昂，并且需要极其密集的图像表示才能恢复复杂的几何形状和照片真实感。NeRF 通过在稀疏图像和姿势数据上训练神经网络来执行 3D 场景重建，以较少的输入数据获得优于摄影测量的结果。本文对两种 NeRF 场景重建进行了评估，目的是估计垂直 PVC 圆柱体的直径。其中一个是在商品 iPhone 数据上训练的，另一个是在机器人来源的图像和姿势上训练的。在场景噪声和度量精度方面，将这种神经几何与最先进的激光雷达惯性 SLAM 进行了比较。]]></description>
      <guid>https://arxiv.org/abs/2407.11238</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:30 GMT</pubDate>
    </item>
    <item>
      <title>OpenPSG：通过大型多模式模型生成开放全景场景图</title>
      <link>https://arxiv.org/abs/2407.11213</link>
      <description><![CDATA[arXiv:2407.11213v1 公告类型：新
摘要：全景场景图生成 (PSG) 旨在分割对象并识别它们的关系，从而实现对图像的结构化理解。以前的方法侧重于预测预定义的对象和关系类别，因此限制了它们在开放世界场景中的应用。随着大型多模态模型 (LMM) 的快速发展，开放集对象检测和分割取得了重大进展，但 PSG 中的开放集关系预测仍未被探索。在本文中，我们专注于开放集关系预测任务，并结合预训练的开放集全景分割模型来实现真正的开放集全景场景图生成 (OpenPSG)。我们的 OpenPSG 利用 LMM 以自回归方式实现开放集关系预测。我们引入了一个关系查询转换器来有效地提取对象对的视觉特征并估计它们之间关系的存在。后者可以通过过滤不相关的对来提高预测效率。最后，我们设计了生成和判断指令，以自回归方式在 PSG 中执行开放集关系预测。据我们所知，我们是第一个提出开放集 PSG 任务的人。大量实验表明，我们的方法在开放集关系预测和全景场景图生成方面达到了最先进的性能。代码可在 \url{https://github.com/franciszzj/OpenPSG} 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.11213</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:29 GMT</pubDate>
    </item>
    <item>
      <title>寻找点中的意义：事件相机的弱监督语义分割</title>
      <link>https://arxiv.org/abs/2407.11216</link>
      <description><![CDATA[arXiv:2407.11216v1 公告类型：新
摘要：事件相机擅长捕捉高对比度场景和动态物体，与传统的基于帧的相机相比具有显着优势。尽管积极研究利用事件相机进行语义分割，但为这种具有挑战性的场景生成像素密集语义图仍然需要大量劳动力。作为补救措施，我们提出了 EV-WSSS：一种利用稀疏点注释的基于事件的语义分割的新型弱监督方法。为了充分利用事件数据的时间特性，所提出的框架在 1）原始正向事件数据和 2）较长的反向事件数据之间执行不对称双学生学习，它们分别包含来自过去和未来的互补信息。此外，为了减轻稀疏监督带来的挑战，我们提出了基于类原型的特征级对比学习，并在空间区域和样本级别进行仔细聚合。此外，我们通过在两种学习路径之间交换原型，进一步挖掘双学生学习模型的潜力，从而利用它们的互补优势。通过对各种数据集进行大量实验，包括本文新提供的具有稀疏点注释的 DSEC Night-Point，所提出的方法即使不依赖像素级密集基本事实也能实现显著的分割结果。代码和数据集可在 https://github.com/Chohoonhee/EV-WSSS 上找到。]]></description>
      <guid>https://arxiv.org/abs/2407.11216</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:29 GMT</pubDate>
    </item>
    <item>
      <title>EyeDentify：基于网络摄像头图像的瞳孔直径估计数据集</title>
      <link>https://arxiv.org/abs/2407.11204</link>
      <description><![CDATA[arXiv:2407.11204v1 公告类型：新
摘要：在这项工作中，我们引入了 EyeDentify，这是一个专门为基于网络摄像头图像的瞳孔直径估计而设计的数据集。EyeDentify 解决了瞳孔直径估计数据集不足的问题，瞳孔直径估计是理解生理和心理状态的关键领域，传统上由 Tobii 等高度专业化的传感器系统主导。与这些先进的传感器系统和相关成本不同，网络摄像头图像在实践中更常见。然而，能够使用标准网络摄像头数据估计瞳孔直径的深度学习模型却很少。通过提供裁剪的眼睛图像数据集以及相应的瞳孔直径信息，EyeDentify 能够开发和改进专为装备较差的环境设计的模型，通过使其更易于访问和广泛应用来实现瞳孔直径估计的民主化，从而有助于理解人类活动和支持医疗保健的多个领域。我们的数据集可在 https://vijulshah.github.io/eyedentify/ 获得。]]></description>
      <guid>https://arxiv.org/abs/2407.11204</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:28 GMT</pubDate>
    </item>
    <item>
      <title>无约束开放词汇图像分类：通过 CLIP 反转实现从文本到图像的零样本迁移</title>
      <link>https://arxiv.org/abs/2407.11211</link>
      <description><![CDATA[arXiv:2407.11211v1 公告类型：新
摘要：我们介绍了 NOVIC，这是一种创新的无约束开放词汇图像分类器，它使用自回归变换器以语言形式生成输出分类标签。利用 CLIP 模型的广泛知识，NOVIC 利用嵌入空间实现从纯文本到图像的零样本传输。传统的 CLIP 模型尽管具有开放词汇分类的能力，但需要详尽的潜在类标签提示，将其应用限制在已知内容或上下文的图像上。为了解决这个问题，我们提出了一个“对象解码器”模型，该模型在一个大规模的 92M 目标数据集上进行训练，该数据集由模板化的对象名词集和 LLM 生成的字幕组成，以始终输出相关的对象名词。这有效地反转了 CLIP 文本编码器，并允许直接从图像派生的嵌入向量生成文本对象标签，而无需任何关于图像潜在内容的先验知识。经过训练的解码器在手动和网络整理的数据集以及标准图像分类基准上进行测试，并实现高达 87.5% 的细粒度无提示预测分数，考虑到该模型必须适用于任何可以想象的图像并且不需要任何上下文线索，这是一个很强的结果。]]></description>
      <guid>https://arxiv.org/abs/2407.11211</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:28 GMT</pubDate>
    </item>
    <item>
      <title>基于神经血型分型质量控制的专家感知不确定性估计</title>
      <link>https://arxiv.org/abs/2407.11181</link>
      <description><![CDATA[arXiv:2407.11181v1 公告类型：新
摘要：在医学诊断中，准确的神经模型不确定性估计对于补充第二意见系统至关重要。尽管神经网络集成在这个问题上很熟练，但实际不确定性和预测估计之间仍然存在差距。这里的主要困难是缺乏对示例难度的标签：典型的数据集仅包含地面真实目标标签，使得不确定性估计问题几乎无人监督。我们的新方法通过将专家对案例复杂性的评估整合到神经网络的学习过程中来缩小这一差距，同时利用明确的目标标签和补充复杂性评级。我们验证了我们的血型分型方法，利用新的数据集“BloodyWell”，该数据集独特地增强了带有来自六位医学专家的复杂性评分的标记反应图像。实验表明我们的方法在不确定性预测方面得到了增强，使用专家标签实现了 2.5 倍的改进，使用基于神经的专家共识估计实现了 35% 的性能提升。]]></description>
      <guid>https://arxiv.org/abs/2407.11181</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:27 GMT</pubDate>
    </item>
    <item>
      <title>通过元驱动的视觉提示选择实现高效的上下文医学分割</title>
      <link>https://arxiv.org/abs/2407.11188</link>
      <description><![CDATA[arXiv:2407.11188v1 公告类型：新
摘要：使用大型视觉模型 (LVM) 的上下文学习 (ICL) 通过减少对大量标签的依赖，为医学图像分割提供了一种有希望的途径。然而，LVM 的 ICL 性能高度依赖于视觉提示的选择，并且受到领域转移的影响。虽然现有的利用 LVM 进行医疗任务的工作主要集中在以模型为中心的方法上，例如微调，但我们研究了如何选择好的视觉提示以促进医学领域的推广的正交数据中心视角。在这项工作中，我们通过引入一种新颖的元驱动视觉提示选择机制 (MVPS) 提出了一种标签高效的上下文医学分割方法，其中从元学习框架获得的提示检索器主动选择最佳图像作为提示，以提高模型性能和通用性。经过对 3 种医学成像模式的 8 个数据集和 4 个任务的评估，我们提出的方法在不同场景下都表现出比现有方法一致的优势，提高了计算效率和标签效率。最后，我们展示了 MVPS 是一个灵活、无需微调的模块，可以轻松插入不同的主干并与其他以模型为中心的方法相结合。]]></description>
      <guid>https://arxiv.org/abs/2407.11188</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:27 GMT</pubDate>
    </item>
    <item>
      <title>通过显式聚类平衡实现高效的无监督视觉表征学习</title>
      <link>https://arxiv.org/abs/2407.11168</link>
      <description><![CDATA[arXiv:2407.11168v1 公告类型：新
摘要：自监督学习最近已成为跨模态和模态之间的卓越预训练范式，并取得了显著的成果。特别是在图像领域，组（或集群）区分是最成功的方法之一。然而，这样的框架需要防止严重不平衡的集群分配，以防止崩溃为平凡的解决方案。现有的工作通常通过重新加权集群分配以促进平衡来解决这个问题，或者通过离线操作（例如定期重新聚类）来防止崩溃。然而，前者通常需要较大的批量，这会导致资源需求增加，而后者会引入与大数据集有关的可扩展性问题。在这项工作中，我们提出了 ExCB，这是一个使用新颖的集群平衡方法解决这个问题的框架。ExCB 估计跨批次的集群的相对大小，并通过调整集群分配（与其相对大小成比例）并以在线方式平衡它们。因此，它克服了以前的方法对大批量大小的依赖，并且完全在线，因此可扩展到任何数据集。我们进行了大量实验来评估我们的方法，并证明 ExCB：a) 与以前的工作相比，它实现了最先进的结果，并且资源需求大大减少；b) 完全在线，因此可扩展到大型数据集；c) 即使在非常小的批量大小下也稳定有效。]]></description>
      <guid>https://arxiv.org/abs/2407.11168</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:26 GMT</pubDate>
    </item>
    <item>
      <title>iHuman：基于单目视频的即时动画数字人</title>
      <link>https://arxiv.org/abs/2407.11174</link>
      <description><![CDATA[arXiv:2407.11174v1 公告类型：新 
摘要：个性化的 3D 化身需要可动画化的数字人表示。通过单目视频即时执行此操作可为广大用户和大规模应用提供可扩展性。在本文中，我们提出了一种快速、简单而有效的方法，用于从单目视频创建可动画化的 3D 数字人。我们的方法利用高斯溅射的效率来模拟 3D 几何和外观。然而，我们观察到，单纯地优化高斯溅射会导致不准确的几何形状，从而导致动画效果不佳。这项工作实现并说明了通过高斯溅射进行可动画数字化需要对人体进行精确的 3D 网格类型建模。这是通过开发一种新颖的管道实现的，该管道受益于三个关键方面：(a) 表面位移和颜色球谐函数的隐式建模； (b) 将 3D 高斯函数绑定到身体模板的各个三角面；(c) 一种渲染法线的新技术，然后对其进行辅助监督。我们在三个不同的基准数据集上进行的详尽实验证明了我们的方法在有限的时间设置下取得了最先进的结果。事实上，我们的方法比其最接近的竞争对手快一个数量级（就训练时间而言）。同时，我们在姿势变化的情况下实现了卓越的渲染和 3D 重建性能。]]></description>
      <guid>https://arxiv.org/abs/2407.11174</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:26 GMT</pubDate>
    </item>
    <item>
      <title>将摊销推理与扩散模型相结合，从损坏的图像中学习干净的分布</title>
      <link>https://arxiv.org/abs/2407.11162</link>
      <description><![CDATA[arXiv:2407.11162v1 公告类型：新
摘要：扩散模型 (DM) 已成为解决逆问题的强大生成模型，可以很好地近似真实图像数据的先验分布。通常，扩散模型依赖于大规模干净信号来准确学习地面真实干净图像分布的得分函数。然而，这种对大量干净数据的要求在实际应用中通常是不切实际的，特别是在数据样本获取成本昂贵的领域。为了解决这一限制，在这项工作中，我们引入了 \emph{FlowDiff}，这是一种新颖的联合训练范式，它利用条件规范化流模型来促进损坏数据源上扩散模型的训练。条件规范化流尝试通过新颖的摊销推理机制学习恢复干净图像，从而可以有效地促进扩散模型对损坏数据的训练。另一方面，扩散模型提供了强大的先验，从而提高了图像恢复的质量。因此，流动模型和扩散模型可以相互促进，并表现出强大的经验性能。我们精心设计的实验表明，FlowDiff 可以有效地从各种损坏的数据源（例如嘈杂和模糊的图像）中学习干净的分布。在相同条件下，它始终以显着的优势优于现有基线。此外，我们还研究了学习扩散先验，观察到其在下游计算成像任务（包括修复、去噪和去模糊）中的卓越性能。]]></description>
      <guid>https://arxiv.org/abs/2407.11162</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:25 GMT</pubDate>
    </item>
    <item>
      <title>迈向对抗性鲁棒视觉语言模型：来自设计选择和提示格式化技术的见解</title>
      <link>https://arxiv.org/abs/2407.11121</link>
      <description><![CDATA[arXiv:2407.11121v1 公告类型：新
摘要：视觉语言模型 (VLM) 在研究和实际应用中都出现了激增。然而，随着它们变得越来越普遍，确保它们对对抗性攻击的鲁棒性至关重要。这项工作系统地研究了模型设计选择对 VLM 对抗基于图像的攻击的对抗鲁棒性的影响。此外，我们引入了新颖的、经济有效的方法，通过快速格式化来增强鲁棒性。通过重新表述问题并提出潜在的对抗性扰动，我们证明了模型对强大的基于图像的攻击（例如 Auto-PGD）的鲁棒性有显着改善。我们的研究结果为开发更强大的 VLM 提供了重要指导，特别是对于在安全关键环境中的部署。]]></description>
      <guid>https://arxiv.org/abs/2407.11121</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:24 GMT</pubDate>
    </item>
    <item>
      <title>UFQA：实用指导的指纹照片质量评估</title>
      <link>https://arxiv.org/abs/2407.11141</link>
      <description><![CDATA[arXiv:2407.11141v1 公告类型：新
摘要：使用数码相机和智能手机捕获的指纹（也称为手指照片）的质量评估是生物特征识别系统中的一个难题。随着非接触式生物特征识别模式越来越受到关注，其可靠性也应得到提高。在指纹照片采集过程中，诸如照明、图像对比度、相机角度等许多因素都会引入各种类型的失真，从而可能导致样本无用。当前为使用接触式传感器收集的指纹开发的质量估计方法不适用于指纹照片。我们提出了实用引导指纹照片质量评估 (UFQA)，这是一种自监督双编码器框架，用于学习有意义的特征表示以评估指纹照片质量。通过对质量图进行额外监督，训练质量预测模型来评估指纹照片质量。质量指标是匹配场景中指纹照片效用的预测指标。因此，我们在标记训练数据时采用整体方法，包括指纹照片效用和局部质量。实验结果验证了我们的方法在多个公开的指纹照片数据集上的表现优于广泛使用的指纹质量度量 NF​​IQ2.2 和最先进的图像质量评估算法。]]></description>
      <guid>https://arxiv.org/abs/2407.11141</guid>
      <pubDate>Thu, 18 Jul 2024 03:19:24 GMT</pubDate>
    </item>
    </channel>
</rss>