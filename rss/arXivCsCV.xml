<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
  <channel>
    <title>arXiv.org 上的 cs.CV 更新</title>
    <link>https://rss.arxiv.org/rss</link>
    <description>cs.CV 在 arXiv.org 电子打印档案上更新。</description>
    <lastBuildDate>Mon, 12 Feb 2024 05:00:00 GMT</lastBuildDate>
    <item>
      <title>HeadStudio：使用 3D 高斯泼溅将文本转换为可动画头部头像</title>
      <link>https://arxiv.org/abs/2402.06149</link>
      <description><![CDATA[长期以来，根据文本提示创建数字化身一直是一项令人向往但具有挑战性的任务。尽管在最近的工作中通过 2D 扩散先验获得了有希望的结果，但当前的方法在有效实现高质量和动画化身方面面临着挑战。在本文中，我们提出了 $\textbf{HeadStudio}$，这是一个新颖的框架，它利用 3D 高斯泼溅从文本提示生成逼真的动画头像。我们的方法在语义上驱动 3D 高斯，通过中间 FLAME 表示创建灵活且可实现的外观。具体来说，我们将 FLAME 合并到 3D 表示和分数蒸馏中：1）基于 FLAME 的 3D 高斯泼溅，通过将每个点绑定到 FLAME 网格来驱动 3D 高斯点。 2）基于FLAME的乐谱蒸馏采样，利用基于FLAME的细粒度控制信号从文本提示中指导乐谱蒸馏。大量的实验证明了 HeadStudio 在根据文本提示生成可动画化身、展现视觉上吸引人的外观方面的功效。这些化身能够以 1024 的分辨率渲染高质量实时（$\geq 40$ fps）新颖的视图。它们可以通过现实世界的语音和视频流畅地控制。我们希望 HeadStudio 能够推进数字化身的创作，并且本方法可以广泛应用于各个领域。]]></description>
      <guid>https://arxiv.org/abs/2402.06149</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:12 GMT</pubDate>
    </item>
    <item>
      <title>《俄罗斯方块》：探索交互式分割的稳健性</title>
      <link>https://arxiv.org/abs/2402.06132</link>
      <description><![CDATA[交互式分割方法依赖于用户输入来迭代更新选择掩模。指定感兴趣对象的点击可以说是最简单和直观的交互类型，因此也是交互式分割的最常见选择。然而，交互式分段上下文中的用户点击模式仍未被探索。因此，交互式分割评估策略更多地依赖于直觉和常识，而不是实证研究（例如，假设用户倾向于点击误差最大的区域的中心）。在这项工作中，我们进行了真实的用户研究来调查真实的用户点击模式。这项研究表明，通用评估策略中做出的直观假设可能并不成立。因此，交互式细分模型可能在标准基准测试中显示出高分，但这并不意味着它们在现实世界场景中表现良好。为了评估交互式分割方法的适用性，我们提出了一种新颖的评估策略，可以对模型的性能进行更全面的分析。为此，我们提出了一种通过对交互式分割模型的白盒对抗攻击进行直接优化来查找极端用户输入的方法。根据此类对抗性用户输入的性能，我们评估了交互式分割模型与点击位置的鲁棒性。此外，我们引入了一种新颖的基准来衡量交互式分割的鲁棒性，并报告了对数十个模型的广泛评估的结果。]]></description>
      <guid>https://arxiv.org/abs/2402.06132</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>SIR：室内场景的可分解阴影多视图逆渲染</title>
      <link>https://arxiv.org/abs/2402.06136</link>
      <description><![CDATA[我们提出了 SIR，这是一种使用多视图数据分解可微阴影以在室内场景上进行逆渲染的有效方法，解决了准确分解材质和照明条件的挑战。与之前在复杂照明环境中努力解决阴影保真度的方法不同，我们的方法明确地学习阴影，以增强未知光位置下材料估计的真实感。 SIR 利用摆姿势的 HDR 图像作为输入，采用基于 SDF 的神经辐射场来进行全面的场景表示。然后，SIR 将影子项与三阶段材料估计方法相结合，以提高 SVBRDF 质量。具体来说，SIR 旨在学习可微分阴影，并辅以 BRDF 正则化，以优化逆渲染精度。对合成和真实室内场景的大量实验证明，SIR 在定量指标和定性分析方面均优于现有方法。 SIR 强大的分解能力可实现复杂的编辑功能，例如自由视图重新照明、对象插入和材质替换。]]></description>
      <guid>https://arxiv.org/abs/2402.06136</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:11 GMT</pubDate>
    </item>
    <item>
      <title>ViGoR：通过细粒度奖励模型改善大视觉语言模型的视觉基础</title>
      <link>https://arxiv.org/abs/2402.06118</link>
      <description><![CDATA[通过将自然语言理解以及大型语言模型的生成能力和知识广度与图像感知相结合，最近的大型视觉语言模型（LVLM）在现实世界中表现出了前所未有的推理能力。然而，生成的文本通常会受到视觉输入基础不准确的影响，从而导致错误，例如幻觉不存在的场景元素、丢失场景的重要部分以及推断对象之间不正确的属性和关系。为了解决这些问题，我们引入了一种新颖的框架 ViGoR（通过细粒度奖励模型实现视觉基础），该框架利用细粒度奖励模型来显着增强 LVLM 在预训练基线上的视觉基础。这种改进是通过使用更便宜的人工评估而不是全面监督以及自动化方法来有效实现的。我们通过多个基准的大量指标展示了我们方法的有效性。此外，我们还构建了一个全面且具有挑战性的数据集，专门用于验证 LVLM 的视觉基础能力。最后，我们计划发布由大约 16,000 张图像组成的人工注释，并生成带有细粒度评估的文本对，为社区的相关研究做出贡献。]]></description>
      <guid>https://arxiv.org/abs/2402.06118</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>ContPhy：从视频中学习和推理连续物理概念</title>
      <link>https://arxiv.org/abs/2402.06119</link>
      <description><![CDATA[我们引入了连续体物理数据集（ContPhy），这是一种用于评估机器物理常识的新颖基准。 ContPhy 通过涵盖各种场景中不同物理属性（例如质量和密度）的推理并预测相应的动态，补充了现有的物理推理基准。我们评估了一系列 AI 模型，发现它们仍然难以在 ContPhy 上获得令人满意的性能，这表明当前的 AI 模型仍然缺乏连续体（尤其是软体）的物理常识，并说明了所提出的数据集的价值。我们还引入了一种预言机模型（ContPRO），它将基于粒子的物理动态模型与最近的大型语言模型结合起来，它具有两种模型的优点，精确的动态预测和可解释的推理。 ContPhy 旨在促进不同物理环境中感知和推理的进步，缩小人类和机器智能在理解物理世界方面的鸿沟。项目页面：https://physical-reasoning-project.github.io。]]></description>
      <guid>https://arxiv.org/abs/2402.06119</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:10 GMT</pubDate>
    </item>
    <item>
      <title>用于在线考试作弊检测和定位的多实例学习</title>
      <link>https://arxiv.org/abs/2402.06107</link>
      <description><![CDATA[2019年冠状病毒疫情的蔓延导致许多课程和考试在网上进行。监考系统中的作弊行为检测模型对于保障远程考试的公平性具有举足轻重的作用。然而，作弊行为很少见，大多数研究人员在作弊行为检测任务中没有综合考虑头部姿势、注视角度、身体姿势和背景信息等特征。在本文中，我们开发并提出了 CHEESE，这是一种通过多重 inStance 学习的 CHEating 检测框架。该框架由一个实现弱监督的标签生成器和一个用于学习判别性特征的特征编码器组成。此外，该框架还将3D卷积提取的身体姿势和背景特征与OpenFace 2.0捕获的眼睛注视、头部姿势和面部特征相结合。这些特征通过拼接的方式输入到时空图模块中，分析视频片段中的时空变化，从而检测作弊行为。我们在 UCF-Crime、ShanghaiTech 和 Online Exam Proctoring (OEP) 三个数据集上进行的实验证明了我们的方法与最先进的方法相比的有效性，并获得了 87.58% 的帧级 AUC 分数OEP 数据集。]]></description>
      <guid>https://arxiv.org/abs/2402.06107</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>具有自适应采样运动去模糊功能的空间注意力补丁分层网络</title>
      <link>https://arxiv.org/abs/2402.06117</link>
      <description><![CDATA[本文解决了动态场景的运动去模糊问题。尽管端到端全卷积设计最近在非均匀运动去模糊方面取得了最先进的进展，但它们的性能与复杂性的权衡仍然不是最优的。大多数现有方法通过增加通用卷积层的数量和内核大小来实现大的感受野。在这项工作中，我们提出了一种像素自适应和特征关注设计，用于处理不同空间位置上的大模糊变化，并自适应处理每个测试图像。我们设计了一个内容感知的全局局部过滤模块，该模块不仅考虑全局依赖性，还通过动态利用相邻像素信息来显着提高性能。我们进一步引入了一种像素自适应非均匀采样策略，该策略隐式地发现图像中存在的难以恢复的区域，进而以渐进的方式执行细粒度的细化。与现有技术在去模糊基准上进行的广泛定性和定量比较表明，我们的方法比最先进的去模糊算法表现得更好。]]></description>
      <guid>https://arxiv.org/abs/2402.06117</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:09 GMT</pubDate>
    </item>
    <item>
      <title>语义分割的早期特征融合</title>
      <link>https://arxiv.org/abs/2402.06091</link>
      <description><![CDATA[本文介绍了一种新颖的分割框架，该框架将分类器网络与反向 HRNet 架构集成在一起，以实现高效的图像分割。我们的方法利用以半监督方式预训练的 ResNet-50 主干网络来生成各种尺度的特征图。然后，这些图由反向 HRNet 进行处理，该 HRNet 适合通过 1x1 卷积处理不同的通道维度，以产生最终的分割输出。我们策略性地避免微调主干网络，以最大限度地减少训练期间的内存消耗。我们的方法在多个基准数据集（包括 Mapillary Vistas、Cityscapes、CamVid、COCO 和 PASCAL-VOC2012）上进行了严格测试，采用像素精度和平均交集 (mIoU) 等指标来评估分割性能。结果证明了我们提出的模型在实现高分割精度方面的有效性，表明其在图像分析中的各种应用的潜力。通过在统一框架内利用 ResNet-50 和反向 HRNet 的优势，我们为图像分割的挑战提供了一个强大的解决方案。]]></description>
      <guid>https://arxiv.org/abs/2402.06091</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>CLIP-Loc：基于对象的地图中全球定位的多模式地标协会</title>
      <link>https://arxiv.org/abs/2402.06092</link>
      <description><![CDATA[本文描述了一种使用基于对象的地图和相机图像进行全局定位的多模态数据关联方法。在使用基于对象的地图的全局定位或重新定位中，现有方法通常诉诸于将检测到的对象和具有相同对象类别的地标的所有可能组合进行匹配，然后使用 RANSAC 或强力搜索进行内点提取。由于对应候选的指数增长而导致地标数量增加，这种方法变得不可行。在本文中，我们建议用自然语言描述来标记地标，并使用视觉语言模型（VLM）根据图像观察的概念相似性来提取对应关系。与仅使用对象类别的方法相比，通过利用详细的文本信息，我们的方法可以有效地提取对应关系。通过实验，我们证明所提出的方法与基线方法相比能够以更少的迭代次数实现更准确的全局定位，展示了其效率。]]></description>
      <guid>https://arxiv.org/abs/2402.06092</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>CLR-Face：使用基于分数的扩散模型进行盲脸恢复的条件潜在细化</title>
      <link>https://arxiv.org/abs/2402.06106</link>
      <description><![CDATA[最近的基于生成先验的方法已经显示出有希望的盲脸恢复性能。他们通常将降级图像投影到潜在空间，然后通过单阶段潜在优化或直接从编码解码高质量的面部。生成忠实于输入的细粒度面部细节仍然是一个具有挑战性的问题。大多数现有方法在尝试平衡生成和重建时要么产生​​过于平滑的输出，要么改变身份。这可能归因于潜在空间中质量和分辨率之间的典型权衡。如果潜在空间被高度压缩，则解码输出对降级更稳健，但保真度较差。另一方面，更灵活的潜在空间可以更好地捕捉复杂的面部细节，但使用现有技术很难针对高度退化的面部进行优化。为了解决这些问题，我们在 VQGAN 架构中引入了基于扩散的先验，该架构专注于学习未损坏的潜在嵌入的分布。有了这些知识，我们就可以迭代地在退化的对应物上恢复干净的嵌入条件。此外，为了确保反向扩散轨迹不偏离底层身份，我们训练一个单独的身份恢复网络并使用其输出来约束反向扩散过程。具体来说，使用可学习的潜在掩模，我们将来自面部识别网络的梯度添加到与像素空间中更精细的身份相关细节相关的潜在特征子集，而其他特征保持不变。潜在空间中感知和保真度之间的解开使我们能够实现两全其美。我们对多个真实和合成数据集进行了广泛的评估，以验证我们方法的优越性。]]></description>
      <guid>https://arxiv.org/abs/2402.06106</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:08 GMT</pubDate>
    </item>
    <item>
      <title>结合形状和轮廓特征来改进铣削过程中的刀具磨损监控</title>
      <link>https://arxiv.org/abs/2402.05978</link>
      <description><![CDATA[在本文中，提出了一种基于形状描述符和轮廓描述符组合的新系统，用于按照基于计算机视觉的方法根据磨损程度对铣削过程中的刀片进行分类。为了描述磨损区域形状，我们提出了一种名为 ShapeFeat 的新描述符，并使用 BORCHIZ 方法对其轮廓进行了表征，据我们所知，采用基于计算机视觉的方法实现了刀具磨损监测的最佳性能。结果表明，采用后期融合方法的 BORCHIZ 与 ShapeFeat 的结合显着提高了分类性能，在二元分类（即磨损程度高或低的分类）中获得了 91.44% 的准确率，在三个目标类别（即磨损的高低分类）中获得了 82.90% 的准确率（即磨损分为高、中或低）。这些结果优于单独使用两个描述符获得的结果，使用 ShapeFeat 时，两个和三个类别的准确率分别为 88.70 和 80.67%，使用 B-ORCHIZ 时分别达到 87.06 和 80.24%。这项研究为制造界取得了令人鼓舞的结果，以便根据铣削过程中的磨损情况自动对刀片进行分类。]]></description>
      <guid>https://arxiv.org/abs/2402.05978</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>内存高效的视觉变压器：激活感知的混合等级压缩策略</title>
      <link>https://arxiv.org/abs/2402.06004</link>
      <description><![CDATA[随着视觉变压器 (ViT) 越来越多地在计算机视觉领域树立新的基准，它们在推理引擎上的实际部署往往因其巨大的内存带宽和（片上）内存占用要求而受到阻碍。本文通过引入一种激活感知模型压缩方法来解决这一内存限制，该方法使用不同层的选择性低秩权重张量近似来减少 ViT 的参数数量。关键思想是将权重张量分解为两个参数有效张量的和，同时最小化输入激活与原始权重张量的乘积与输入激活与近似张量和的乘积之间的误差。通过采用有效的逐层误差补偿技术（使用层输出损失的梯度），可以进一步细化该近似值。这些技术的结合取得了优异的结果，同时避免了在优化过程的早期陷入浅局部最小值，并在模型压缩和输出精度之间取得了良好的平衡。值得注意的是，所提出的方法将 DeiT-B 的参数数量显着减少了 60%，并且 ImageNet 数据集上的准确率下降了不到 1%，克服了低秩近似中常见的准确率下降问题。除此之外，所提出的压缩技术可以压缩大型 DeiT/ViT 模型，使其具有与较小的 DeiT/ViT 变体大致相同的模型大小，同时产生高达 1.8% 的精度增益。这些结果凸显了我们方法的有效性，为在内存受限的环境中嵌入 ViT 提供了一种可行的解决方案，同时又不影响其性能。]]></description>
      <guid>https://arxiv.org/abs/2402.06004</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>动画贴纸：通过视频扩散让贴纸变得栩栩如生</title>
      <link>https://arxiv.org/abs/2402.06088</link>
      <description><![CDATA[我们引入动画贴纸，这是一种视频扩散模型，可根据文本提示和静态贴纸图像生成动画。我们的模型建立在最先进的 Emu 文本到图像模型之上，并添加了时间层来建模运动。由于域差距，即视觉和运动风格的差异，在生成自然视频方面表现良好的模型在应用于贴纸时无法再生成生动的视频。为了弥补这一差距，我们采用了两阶段的微调流程：首先使用弱域内数据，然后采用人机交互（HITL）策略，我们将其称为教师集合。它将多名教师的最佳品质提炼成更小的学生模型。我们表明，这种策略允许我们专门针对运动质量的改进，同时保持静态图像的风格。通过推理优化，我们的模型能够在一秒内生成具有高质量、有趣且相关运动的八帧视频。]]></description>
      <guid>https://arxiv.org/abs/2402.06088</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:07 GMT</pubDate>
    </item>
    <item>
      <title>不同深度学习架构下可穿戴设备和单摄像头视频上肢实验室外活动识别的比较研究</title>
      <link>https://arxiv.org/abs/2402.05958</link>
      <description><![CDATA[在临床和研究环境中评估人体活动时，广泛使用计算机视觉解决方案以及最近的高端惯性测量单元 (IMU) 已变得越来越流行。然而，为了提高在实验室外环境中跟踪患者的可行性，有必要使用减少数量的运动采集设备。在这种情况下，有前景的解决方案是基于 IMU 的可穿戴设备和单摄像头系统。此外，需要开发能够在野外识别和消化临床相关数据的机器学习系统，因此确定这些数据的理想输入至关重要。]]></description>
      <guid>https://arxiv.org/abs/2402.05958</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    <item>
      <title>使用基于局部纹理的在线、自动和低成本系统进行刀具磨损监测</title>
      <link>https://arxiv.org/abs/2402.05977</link>
      <description><![CDATA[在这项工作中，我们提出了一种基于计算机视觉和机器学习的在线、低成本、快速的新方法，以根据磨损程度确定边缘仿形铣削过程中使用的切削刀具是否可维修或一次性。我们创建了一个包含 254 张边缘轮廓切割头图像的新数据集，据我们所知，这是第一个具有足够质量用于此目的的公开可用数据集。所有刀片均被分段并裁剪其切削刃，获得 577 个切削刃图像：301 个功能性刀片和 276 个一次性刀片。所提出的方法基于（1）将尖端图像划分为不同区域，称为磨损补丁（WP），（2）使用基于局部二进制模式（LBP）不同变体的纹理描述符将每个区域描述为磨损或可用(3) 根据这些 WP 的状态，确定切削刃（以及刀具）是可使用的还是一次性的。我们提出并评估了五种不同的补丁划分配置。各个 WP 通过具有交集内核的支持向量机 (SVM) 进行分类。 WP 的最佳补丁分割配置和纹理描述符在一次性切割边缘的检测中实现了 90.26% 的准确率。这些结果显示了边缘仿形铣削过程中自动磨损监测的非常有前途的机会。]]></description>
      <guid>https://arxiv.org/abs/2402.05977</guid>
      <pubDate>Mon, 12 Feb 2024 06:17:06 GMT</pubDate>
    </item>
    </channel>
</rss>